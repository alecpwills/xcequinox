{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ad85b03-163f-49a0-8765-55a25b48c224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awills/anaconda3/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting \"B3LYP_WITH_VWN5 = True\" in pyscf_conf.py\n",
      "  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '\n",
      "2024-04-04 21:45:34.072082: W external/xla/xla/service/gpu/nvptx_compiler.cc:742] The NVIDIA driver's CUDA version is 12.0 which is older than the ptxas CUDA version (12.4.99). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "from ase import Atoms\n",
    "from ase.io import read\n",
    "import xcquinox as xce\n",
    "import torch, jax, optax\n",
    "import numpy as np\n",
    "import equinox as eqx\n",
    "import jax.numpy as jnp\n",
    "import pyscfad as psa\n",
    "from pyscfad import dft, scf, gto, df\n",
    "from pyscfad.pbc import scf as scfp\n",
    "from pyscfad.pbc import gto as gtop\n",
    "from pyscfad.pbc import dft as dftp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c512a82-36be-4a46-b7ab-42b784f61c2a",
   "metadata": {},
   "source": [
    "Utility function requiring torch to load old models, but we won't require torch as a prerequisite. Eventually want to have a self-contained folder of translated models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f315b50-8bf3-48ea-997f-fdb45a6fe351",
   "metadata": {},
   "source": [
    "Torch structure for loading old models below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d2f6383-a523-42f0-b1ba-dcd0a8ae3caf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#relevant functions from dpyscfl to see if it can be self-contained here in the notebook\n",
    "#xcdiff has this named XC_L, not X_L. keep for consistency's sake\n",
    "\n",
    "class LOB(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, limit=1.804):\n",
    "        \"\"\" Utility function to squash output to [-1, limit-1] inteval.\n",
    "            Can be used to enforce non-negativity and Lieb-Oxford bound.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        self.limit = limit\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.limit*self.sig(x-np.log(self.limit-1))-1\n",
    "\n",
    "\n",
    "class X_L(torch.nn.Module):\n",
    "    def __init__(self, n_input, n_hidden=16, use=[], device='cpu', ueg_limit=False, lob=1.804, one_e=False):\n",
    "        \"\"\"Local exchange model based on MLP\n",
    "        Receives density descriptors in this order : [rho, s, alpha, nl],\n",
    "        input may be truncated depending on level of approximation\n",
    "\n",
    "        Args:\n",
    "            n_input (int): Input dimensions (LDA: 1, GGA: 2, meta-GGA: 3, ...)\n",
    "            n_hidden (int, optional): Number of hidden nodes (three hidden layers used by default). Defaults to 16.\n",
    "            use (list of ints, optional): Only these indices are used as input to the model (can be used to omit density as input to enforce uniform density scaling). These indices are also used to enforce UEG where the assumed order is [s, alpha, ...].. Defaults to [].\n",
    "            device (str, optional): {'cpu','cuda'}. Defaults to 'cpu'.\n",
    "            ueg_limit (bool, optional): Enforce uniform homoegeneous electron gas limit. Defaults to False.\n",
    "            lob (float, optional): Enforce this value as local Lieb-Oxford bound (don't enforce if set to 0). Defaults to 1.804.\n",
    "            one_e (bool, optional): _description_. Defaults to False.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.ueg_limit = ueg_limit\n",
    "        self.spin_scaling = True\n",
    "        self.lob = lob\n",
    "\n",
    "        if not use:\n",
    "            self.use = torch.Tensor(np.arange(n_input)).long().to(device)\n",
    "        else:\n",
    "            self.use = torch.Tensor(use).long().to(device)\n",
    "        #xcdiff includes double flag on net\n",
    "        self.net =  torch.nn.Sequential(\n",
    "                torch.nn.Linear(n_input, n_hidden),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.Linear(n_hidden, n_hidden),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.Linear(n_hidden, n_hidden),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.Linear(n_hidden, 1),\n",
    "            ).double().to(device)\n",
    "\n",
    "        #to device not declared in xcdiff\n",
    "        self.tanh = torch.nn.Tanh().to(device)\n",
    "        self.lobf = LOB(lob).to(device)\n",
    "        #below declared in xcdiff\n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        self.shift = 1/(1+np.exp(-1e-3))\n",
    "\n",
    "    def forward(self, rho, **kwargs):\n",
    "        \"\"\"Forward pass\n",
    "\n",
    "        Args:\n",
    "            rho (_type_): _description_\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        # print(rho.size, rho.shape, rho.dtype)\n",
    "        # print('x call -- rho shape', rho.shape)\n",
    "        # print('x call -- rho[...,self.use] shape', rho[...,self.use].shape)\n",
    "        squeezed = self.net(rho[...,self.use]).squeeze()\n",
    "        # print('x call -- squeezed shape', squeezed.shape)\n",
    "        # print('x call -- squeezed', squeezed)\n",
    "\n",
    "        if self.ueg_limit:\n",
    "            ueg_lim = rho[...,self.use[0]]\n",
    "            if len(self.use) > 1:\n",
    "                ueg_lim_a = torch.pow(self.tanh(rho[...,self.use[1]]),2)\n",
    "            else:\n",
    "                ueg_lim_a = 0\n",
    "            #below comparison not in xcdiff\n",
    "            if len(self.use) > 2:\n",
    "                ueg_lim_nl = torch.sum(rho[...,self.use[2:]],dim=-1)\n",
    "            else:\n",
    "                ueg_lim_nl = 0\n",
    "        else:\n",
    "            ueg_lim = 1\n",
    "            ueg_lim_a = 0\n",
    "            ueg_lim_nl = 0\n",
    "\n",
    "        if self.lob:\n",
    "            result = self.lobf(squeezed*(ueg_lim + ueg_lim_a + ueg_lim_nl))\n",
    "        else:\n",
    "            result = squeezed*(ueg_lim + ueg_lim_a + ueg_lim_nl)\n",
    "\n",
    "        return result\n",
    "\n",
    "class C_L(torch.nn.Module):\n",
    "    def __init__(self, n_input=2,n_hidden=16, device='cpu', ueg_limit=False, lob=2.0, use = []):\n",
    "        \"\"\"Local correlation model based on MLP\n",
    "        Receives density descriptors in this order : [rho, spinscale, s, alpha, nl]\n",
    "        input may be truncated depending on level of approximation\n",
    "\n",
    "        Args:\n",
    "            n_input (int, optional): Input dimensions (LDA: 2, GGA: 3 , meta-GGA: 4). Defaults to 2.\n",
    "            n_hidden (int, optional): Number of hidden nodes (three hidden layers used by default). Defaults to 16.\n",
    "            device (str, optional): {'cpu','cuda'}. Defaults to 'cpu'.\n",
    "            ueg_limit (bool, optional): Enforce uniform homoegeneous electron gas limit. Defaults to False.\n",
    "            lob (float, optional): Technically Lieb-Oxford bound but used here to enforce non-negativity. Should be kept at 2.0 in most instances. Defaults to 2.0.\n",
    "            use (list of ints, optional): Indices for [s, alpha] (in that order) in input, to determine UEG limit. Defaults to [].\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.spin_scaling = False\n",
    "        self.lob = False\n",
    "        self.ueg_limit = ueg_limit\n",
    "        self.n_input=n_input\n",
    "\n",
    "        if not use:\n",
    "            self.use = torch.Tensor(np.arange(n_input)).long().to(device)\n",
    "        else:\n",
    "            self.use = torch.Tensor(use).long().to(device)\n",
    "        self.net = torch.nn.Sequential(\n",
    "                torch.nn.Linear(n_input, n_hidden),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.Linear(n_hidden, n_hidden),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.Linear(n_hidden, n_hidden),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.Linear(n_hidden, 1),\n",
    "                torch.nn.Softplus()\n",
    "            ).double().to(device)\n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        #self.lob section allows for different values here, default=2. xcdiff doesn't have this,\n",
    "        #assumes 2 always\n",
    "        self.lob = lob\n",
    "        if self.lob:\n",
    "            self.lobf = LOB(self.lob)\n",
    "        else:\n",
    "            self.lob =  1000.0\n",
    "            self.lobf = LOB(self.lob)\n",
    "\n",
    "\n",
    "    def forward(self, rho, **kwargs):\n",
    "        \"\"\"Forward pass in network\n",
    "\n",
    "        Args:\n",
    "            rho (torch.Tensor): density\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        inp = rho\n",
    "        # print(rho.size, rho.shape, rho.dtype)\n",
    "        # print('c call -- rho shape', rho.shape)\n",
    "        # print('c call, rho[...,self.use] shape', rho.shape)\n",
    "        # print('c call, rho[...,self.use]', rho)        \n",
    "        squeezed = -self.net(inp).squeeze()\n",
    "        # print('c call -- squeezed shape', squeezed.shape)\n",
    "        # print('c call -- squeezed', squeezed)\n",
    "        \n",
    "        if self.ueg_limit:\n",
    "            #below not form used in xcdiff\n",
    "#            ueg_lim = rho[...,self.use[0]]\n",
    "            #below form used in xcdiff,\n",
    "            ueg_lim = self.tanh(rho[...,self.use[0]])\n",
    "            if len(self.use) > 1:\n",
    "                ueg_lim_a = torch.pow(self.tanh(rho[...,self.use[1]]),2)\n",
    "            else:\n",
    "                ueg_lim_a = 0\n",
    "            #xcdiff does not include this next comparison\n",
    "            if len(self.use) > 2:\n",
    "                ueg_lim_nl = torch.sum(self.tanh(rho[...,self.use[2:]])**2,dim=-1)\n",
    "            else:\n",
    "                ueg_lim_nl = 0\n",
    "\n",
    "            ueg_factor = ueg_lim + ueg_lim_a + ueg_lim_nl\n",
    "        else:\n",
    "            ueg_factor = 1\n",
    "        #xcdiff below returns the negative of the negative inputs\n",
    "        #lob is sigmoid, so odd function, negatives cancel, so not needed\n",
    "        if self.lob:\n",
    "            return self.lobf(squeezed*ueg_factor)\n",
    "        else:\n",
    "            return squeezed*ueg_factor\n",
    "class LDA_X(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\" UEG exchange\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, rho, **kwargs):\n",
    "        return -3/4*(3/np.pi)**(1/3)*rho**(1/3)\n",
    "params_a_pp     = [1,  1,  1]\n",
    "params_a_alpha1 = [0.21370,  0.20548,  0.11125]\n",
    "params_a_a      = [0.031091, 0.015545, 0.016887]\n",
    "params_a_beta1  = [7.5957, 14.1189, 10.357]\n",
    "params_a_beta2  = [3.5876, 6.1977, 3.6231]\n",
    "params_a_beta3  = [1.6382, 3.3662,  0.88026]\n",
    "params_a_beta4  = [0.49294, 0.62517, 0.49671]\n",
    "params_a_fz20   = 1.709921\n",
    "       \n",
    "class PW_C(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\" UEG correlation, Perdew & Wang\"\"\"\n",
    "        super().__init__()\n",
    "    def forward(self, rs, zeta):\n",
    "        def g_aux(k, rs):\n",
    "            return params_a_beta1[k]*torch.sqrt(rs) + params_a_beta2[k]*rs\\\n",
    "          + params_a_beta3[k]*rs**1.5 + params_a_beta4[k]*rs**(params_a_pp[k] + 1)\n",
    "\n",
    "        def g(k, rs):\n",
    "            return -2*params_a_a[k]*(1 + params_a_alpha1[k]*rs)\\\n",
    "          * torch.log(1 +  1/(2*params_a_a[k]*g_aux(k, rs)))\n",
    "\n",
    "        def f_zeta(zeta):\n",
    "            return ((1+zeta)**(4/3) + (1-zeta)**(4/3) - 2)/(2**(4/3)-2)\n",
    "\n",
    "        def f_pw(rs, zeta):\n",
    "            return g(0, rs) + zeta**4*f_zeta(zeta)*(g(1, rs) - g(0, rs) + g(2, rs)/params_a_fz20)\\\n",
    "          - f_zeta(zeta)*g(2, rs)/params_a_fz20\n",
    "\n",
    "        return f_pw(rs, zeta)\n",
    "\n",
    "class XC(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, grid_models=None, heg_mult=True, pw_mult=True,\n",
    "                    level = 1, exx_a=None, epsilon=1e-8):\n",
    "        \"\"\"Defines the XC functional on a grid\n",
    "\n",
    "        Args:\n",
    "            grid_models (list, optional): list of X_L (local exchange) or C_L (local correlation). Defines the xc-models/enhancement factors. Defaults to None.\n",
    "            heg_mult (bool, optional): Use homoegeneous electron gas exchange (multiplicative if grid_models is not empty). Defaults to True.\n",
    "            pw_mult (bool, optional): Use homoegeneous electron gas correlation (Perdew & Wang). Defaults to True.\n",
    "            level (int, optional): Controls the number of density \"descriptors\" generated. 1: LDA, 2: GGA, 3:meta-GGA, 4: meta-GGA + electrostatic (nonlocal). Defaults to 1.\n",
    "            exx_a (_type_, optional): Exact exchange mixing parameter. Defaults to None.\n",
    "            epsilon (float, optional): Offset to avoid div/0 in calculations. Defaults to 1e-8.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.heg_mult = heg_mult\n",
    "        self.pw_mult = pw_mult\n",
    "        self.grid_coords = None\n",
    "        self.training = True\n",
    "        self.level = level\n",
    "        self.epsilon = epsilon\n",
    "        if level > 3:\n",
    "            print('WARNING: Non-local models highly experimental and likely will not work ')\n",
    "        self.loge = 1e-5\n",
    "        self.s_gam = 1\n",
    "\n",
    "        if heg_mult:\n",
    "            self.heg_model = LDA_X()\n",
    "        if pw_mult:\n",
    "            self.pw_model = PW_C()\n",
    "        self.grid_models = list(grid_models)\n",
    "        if self.grid_models:\n",
    "            self.grid_models = torch.nn.ModuleList(self.grid_models)\n",
    "        self.model_mult = [1 for m in self.grid_models]\n",
    "\n",
    "        if exx_a is not None:\n",
    "            self.exx_a = torch.nn.Parameter(torch.Tensor([exx_a]))\n",
    "            self.exx_a.requires_grad = True\n",
    "        else:\n",
    "            self.exx_a = 0\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Switches self.training flag to False\n",
    "        \"\"\"\n",
    "        self.training=False\n",
    "    def train(self):\n",
    "        \"\"\"Switches self.training flag to True\n",
    "        \"\"\"\n",
    "        self.training=True\n",
    "\n",
    "    def add_model_mult(self, model_mult):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        .. todo:: \n",
    "            Unclear what the purpose of this is\n",
    "\n",
    "        Args:\n",
    "            model_mult (_type_): _description_\n",
    "        \"\"\"\n",
    "        del(self.model_mult)\n",
    "        self.register_buffer('model_mult',torch.Tensor(model_mult))\n",
    "\n",
    "    def add_exx_a(self, exx_a):\n",
    "        \"\"\"Adds exact-exchange mixing parameter after initialization\n",
    "\n",
    "        Args:\n",
    "            exx_a (float): Exchange mixing parameter\n",
    "        \"\"\"\n",
    "        self.exx_a = torch.nn.Parameter(torch.Tensor([exx_a]))\n",
    "        self.exx_a.requires_grad = True\n",
    "\n",
    "    # Density (rho)\n",
    "    def l_1(self, rho):\n",
    "        \"\"\"Level 1 Descriptor -- Creates dimensionless quantity from rho.\n",
    "        Eq. 3 in `base paper <https://link.aps.org/doi/10.1103/PhysRevB.104.L161109>`_\n",
    "\n",
    "        .. math:: x_0 = \\\\rho^{1/3}\n",
    "\n",
    "        Args:\n",
    "            rho (torch.Tensor): density\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: dimensionless density\n",
    "        \"\"\"\n",
    "        return rho**(1/3)\n",
    "\n",
    "    # Reduced density gradient s\n",
    "    def l_2(self, rho, gamma):\n",
    "        \"\"\"Level 2 Descriptor -- Reduced gradient density\n",
    "        Eq. 5 in `base paper <https://link.aps.org/doi/10.1103/PhysRevB.104.L161109>`_\n",
    "\n",
    "        .. math:: x_2=s=\\\\frac{1}{2(3\\\\pi^2)^{1/3}} \\\\frac{|\\\\nabla \\\\rho|}{\\\\rho^{4/3}}\n",
    "\n",
    "        Args:\n",
    "            rho (torch.Tensor): density\n",
    "            gamma (torch.Tensor): squared density gradient\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: reduced density gradient s\n",
    "        \"\"\"\n",
    "        return torch.sqrt(gamma)/(2*(3*np.pi**2)**(1/3)*rho**(4/3)+self.epsilon)\n",
    "\n",
    "    # Reduced kinetic energy density alpha\n",
    "    def l_3(self, rho, gamma, tau):\n",
    "        \"\"\"Level 3 Descriptor -- Reduced kinetic energy density\n",
    "        Eq. 6 in `base paper <https://link.aps.org/doi/10.1103/PhysRevB.104.L161109>`_\n",
    "\n",
    "        .. math:: x_3 = \\\\alpha = \\\\frac{\\\\tau-\\\\tau^W}{\\\\tau^{unif}},\n",
    "\n",
    "        where\n",
    "\n",
    "        .. math:: \\\\tau^W = \\\\frac{|\\\\nabla \\\\rho|^2}{8\\\\rho}, \\\\tau^{unif} = \\\\frac{3}{10} (3\\\\pi^2)^{2/3}\\\\rho^{5/3}.\n",
    "\n",
    "        Args:\n",
    "            rho (torch.Tensor): density\n",
    "            gamma (torch.Tensor): squared density gradient\n",
    "            tau (torch.Tensor): kinetic energy density\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: reduced kinetic energy density\n",
    "        \"\"\"\n",
    "        uniform_factor = (3/10)*(3*np.pi**2)**(2/3)\n",
    "        tw = gamma/(8*(rho+self.epsilon))\n",
    "        #commented is dpyscflite version, uncommented is xcdiff version\n",
    "        #return torch.nn.functional.relu((tau - tw)/(uniform_factor*rho**(5/3)+tw*1e-3 + 1e-12))\n",
    "        return (tau - gamma/(8*(rho+self.epsilon)))/(uniform_factor*rho**(5/3)+self.epsilon)\n",
    "\n",
    "    # Unit-less electrostatic potential\n",
    "    def l_4(self, rho, nl):\n",
    "        \"\"\"Level 4 Descriptor -- Unitless electrostatic potential\n",
    "\n",
    "        .. todo:: Figure out what exactly this part is\n",
    "\n",
    "        Args:\n",
    "            rho (torch.Tensor): density\n",
    "            nl (torch.Tensor): some non-local descriptor\n",
    "\n",
    "        Returns:\n",
    "            torch.nn.functional.relu: _description_\n",
    "        \"\"\"\n",
    "        u = nl[:,:1]/((rho.unsqueeze(-1)**(1/3))*self.nl_ueg[:,:1] + self.epsilon)\n",
    "        wu = nl[:,1:]/((rho.unsqueeze(-1))*self.nl_ueg[:,1:] + self.epsilon)\n",
    "        return torch.nn.functional.relu(torch.cat([u,wu],dim=-1))\n",
    "\n",
    "    def get_descriptors(self, rho0_a, rho0_b, gamma_a, gamma_b, gamma_ab,nl_a,nl_b, tau_a, tau_b, spin_scaling = False):\n",
    "        \"\"\"Creates 'ML-compatible' descriptors from the electron density and its gradients, a & b correspond to spin channels\n",
    "\n",
    "        Args:\n",
    "            rho0_a (torch.Tensor): :math:`\\\\rho` in spin-channel a\n",
    "            rho0_b (torch.Tensor): :math:`\\\\rho` in spin-channel b\n",
    "            gamma_a (torch.Tensor): :math:`|\\\\nabla \\\\rho|^2` in spin-channel a \n",
    "            gamma_b (torch.Tensor): :math:`|\\\\nabla \\\\rho|^2` in spin-channel b\n",
    "            gamma_ab (torch.Tensor): _description_\n",
    "            nl_a (torch.Tensor): _description_\n",
    "            nl_b (torch.Tensor): _description_\n",
    "            tau_a (torch.Tensor): KE density in spin-channel a\n",
    "            tau_b (torch.Tensor): KE density in spin-channel b\n",
    "            spin_scaling (bool, optional): Flag for spin-scaling. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "\n",
    "        if not spin_scaling:\n",
    "            #If no spin-scaling, calculate polarization and use for X1\n",
    "            zeta = (rho0_a - rho0_b)/(rho0_a + rho0_b + self.epsilon)\n",
    "            spinscale = 0.5*((1+zeta)**(4/3) + (1-zeta)**(4/3)) # zeta\n",
    "\n",
    "        if self.level > 0:  #  LDA\n",
    "            if spin_scaling:\n",
    "                descr1 = torch.log(self.l_1(2*rho0_a) + self.loge)\n",
    "                descr2 = torch.log(self.l_1(2*rho0_b) + self.loge)\n",
    "            else:\n",
    "                descr1 = torch.log(self.l_1(rho0_a + rho0_b) + self.loge)# rho\n",
    "                descr2 = torch.log(spinscale) # zeta\n",
    "            descr = torch.cat([descr1.unsqueeze(-1), descr2.unsqueeze(-1)],dim=-1)\n",
    "        if self.level > 1: # GGA\n",
    "            if spin_scaling:\n",
    "                descr3a = self.l_2(2*rho0_a, 4*gamma_a) # s\n",
    "                descr3b = self.l_2(2*rho0_b, 4*gamma_b) # s\n",
    "                descr3 = torch.cat([descr3a.unsqueeze(-1), descr3b.unsqueeze(-1)],dim=-1)\n",
    "                descr3 = (1-torch.exp(-descr3**2/self.s_gam))*torch.log(descr3 + 1)\n",
    "            else:\n",
    "                descr3 = self.l_2(rho0_a + rho0_b, gamma_a + gamma_b + 2*gamma_ab) # s\n",
    "                #line below in xcdiff, not dpyscfl\n",
    "                descr3 = descr3/((1+zeta)**(2/3) + (1-zeta)**2/3)\n",
    "                descr3 = descr3.unsqueeze(-1)\n",
    "                descr3 = (1-torch.exp(-descr3**2/self.s_gam))*torch.log(descr3 + 1)\n",
    "            descr = torch.cat([descr, descr3],dim=-1)\n",
    "        if self.level > 2: # meta-GGA\n",
    "            if spin_scaling:\n",
    "                descr4a = self.l_3(2*rho0_a, 4*gamma_a, 2*tau_a)\n",
    "                descr4b = self.l_3(2*rho0_b, 4*gamma_b, 2*tau_b)\n",
    "                descr4 = torch.cat([descr4a.unsqueeze(-1), descr4b.unsqueeze(-1)],dim=-1)\n",
    "                #below in xcdiff, not dpyscfl\n",
    "                descr4 = descr4**3/(descr4**2+self.epsilon)\n",
    "            else:\n",
    "                descr4 = self.l_3(rho0_a + rho0_b, gamma_a + gamma_b + 2*gamma_ab, tau_a + tau_b)\n",
    "                #next 2 in xcdiff, not dpyscfl\n",
    "                descr4 = 2*descr4/((1+zeta)**(5/3) + (1-zeta)**(5/3))\n",
    "                descr4 = descr4**3/(descr4**2+self.epsilon)\n",
    "\n",
    "                descr4 = descr4.unsqueeze(-1)\n",
    "            descr4 = torch.log((descr4 + 1)/2)\n",
    "            descr = torch.cat([descr, descr4],dim=-1)\n",
    "        if self.level > 3: # meta-GGA + V_estat\n",
    "            if spin_scaling:\n",
    "                descr5a = self.l_4(2*rho0_a, 2*nl_a)\n",
    "                descr5b = self.l_4(2*rho0_b, 2*nl_b)\n",
    "                descr5 = torch.log(torch.stack([descr5a, descr5b],dim=-1) + self.loge)\n",
    "                descr5 = descr5.view(descr5.size()[0],-1)\n",
    "            else:\n",
    "                descr5= torch.log(self.l_4(rho0_a + rho0_b, nl_a + nl_b) + self.loge)\n",
    "\n",
    "            descr = torch.cat([descr, descr5],dim=-1)\n",
    "        if spin_scaling:\n",
    "            print('spin_scaling')\n",
    "            print('descr size -- ', descr.size())\n",
    "            descr = descr.view(descr.size()[0],-1,2).permute(2,0,1)\n",
    "            print('reshaped descr size --', descr.size())\n",
    "        return descr\n",
    "\n",
    "\n",
    "    def forward(self, dm):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            dm (torch.Tensor): density matrix\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        Exc = 0\n",
    "        if self.grid_models or self.heg_mult:\n",
    "            if self.ao_eval.dim()==2:\n",
    "                ao_eval = self.ao_eval.unsqueeze(0)\n",
    "            else:\n",
    "                ao_eval = self.ao_eval\n",
    "\n",
    "            # Create density (and gradients) from atomic orbitals evaluated on grid\n",
    "            # and density matrix\n",
    "            # rho[ijsp]: del_i phi del_j phi dm (s: spin, p: grid point index)\n",
    "            #print(\"FORWARD PASS IN XC. AO_EVAL SHAPE, DM SHAPE: \", ao_eval.shape, dm.shape)\n",
    "            rho = contract('xij,yik,...jk->xy...i', ao_eval, ao_eval, dm)+1e-10\n",
    "            rho0 = rho[0,0]\n",
    "            drho = rho[0,1:4] + rho[1:4,0]\n",
    "            tau = 0.5*(rho[1,1] + rho[2,2] + rho[3,3])\n",
    "\n",
    "            # Non-local electrostatic potential\n",
    "            if self.level > 3:\n",
    "                non_loc = contract('mnQ, QP, Pki, ...mn-> ...ki', self.df_3c, self.df_2c_inv, self.vh_on_grid, dm)\n",
    "            else:\n",
    "                non_loc = torch.zeros_like(tau).unsqueeze(-1)\n",
    "\n",
    "            if dm.dim() == 3: # If unrestricted (open-shell) calculation\n",
    "\n",
    "                # Density\n",
    "                rho0_a = rho0[0]\n",
    "                rho0_b = rho0[1]\n",
    "\n",
    "                # Contracted density gradient\n",
    "                gamma_a, gamma_b = contract('ij,ij->j',drho[:,0],drho[:,0]), contract('ij,ij->j',drho[:,1],drho[:,1])\n",
    "                gamma_ab = contract('ij,ij->j',drho[:,0],drho[:,1])\n",
    "\n",
    "                # Kinetic energy density\n",
    "                tau_a, tau_b = tau\n",
    "\n",
    "                # E.-static\n",
    "                non_loc_a, non_loc_b = non_loc\n",
    "            else:\n",
    "                rho0_a = rho0_b = rho0*0.5\n",
    "                gamma_a=gamma_b=gamma_ab= contract('ij,ij->j',drho[:],drho[:])*0.25\n",
    "                tau_a = tau_b = tau*0.5\n",
    "                non_loc_a=non_loc_b = non_loc*0.5\n",
    "\n",
    "            # xc-energy per unit particle\n",
    "            exc = self.eval_grid_models(torch.cat([rho0_a.unsqueeze(-1),\n",
    "                                                    rho0_b.unsqueeze(-1),\n",
    "                                                    gamma_a.unsqueeze(-1),\n",
    "                                                    gamma_ab.unsqueeze(-1),\n",
    "                                                    gamma_b.unsqueeze(-1),\n",
    "                                                    torch.zeros_like(rho0_a).unsqueeze(-1), #Dummy for laplacian\n",
    "                                                    torch.zeros_like(rho0_a).unsqueeze(-1), #Dummy for laplacian\n",
    "                                                    tau_a.unsqueeze(-1),\n",
    "                                                    tau_b.unsqueeze(-1),\n",
    "                                                    non_loc_a,\n",
    "                                                    non_loc_b],dim=-1))\n",
    "            print('xc call, exc.shape', exc.shape)\n",
    "            #inplace modification throws MulBackwards0 error sometimes?\n",
    "            Exc += torch.sum(((rho0_a + rho0_b)*exc.clone()[:,0])*self.grid_weights)\n",
    "            #Exc = torch.sum(((rho0_a + rho0_b)*exc[:,0])*self.grid_weights)\n",
    "            # try:\n",
    "            #     Exc = torch.sum(((rho0_a + rho0_b)*exc[:,0])*self.grid_weights)\n",
    "            # except:\n",
    "            #     e = sys.exc_info()[0]\n",
    "            #     Exc = torch.sum(((rho0_a + rho0_b)*exc[:,0])*self.grid_weights)\n",
    "            #     print(\"Error detected\")\n",
    "            #     print(e)                \n",
    "\n",
    "        #Below in xcdiff, not in dpyscfl\n",
    "        #However, keep commented out -- self.nxc_models not implemented\n",
    "        #if self.nxc_models:\n",
    "        #    for nxc_model in self.nxc_models:\n",
    "        #        Exc += nxc_model(dm, self.ml_ovlp)\n",
    "\n",
    "        # print('XC.FORWARD: Exc = ', Exc)\n",
    "        \n",
    "        return Exc\n",
    "\n",
    "    def eval_grid_models(self, rho, debug=False):\n",
    "        \"\"\"Evaluates all models stored in self.grid_models along with HEG exchange and correlation\n",
    "\n",
    "\n",
    "        Args:\n",
    "            rho ([list of torch.Tensors]): List with [rho0_a,rho0_b,gamma_a,gamma_ab,gamma_b, dummy for laplacian, dummy for laplacian, tau_a, tau_b, non_loc_a, non_loc_b]\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        Exc = 0\n",
    "        rho0_a = rho[:, 0]\n",
    "        rho0_b = rho[:, 1]\n",
    "        gamma_a = rho[:, 2]\n",
    "        gamma_ab = rho[:, 3]\n",
    "        gamma_b = rho[:, 4]\n",
    "        tau_a = rho[:, 7]\n",
    "        tau_b = rho[:, 8]\n",
    "        nl = rho[:,9:]\n",
    "        nl_size = nl.size()[-1]//2\n",
    "        nl_a = nl[:,:nl_size]\n",
    "        nl_b = nl[:,nl_size:]\n",
    "\n",
    "        C_F= 3/10*(3*np.pi**2)**(2/3)\n",
    "        #in xcdiff, self.meta_local would change below assignments\n",
    "        #not used here\n",
    "        rho0_a_ueg = rho0_a\n",
    "        rho0_b_ueg = rho0_b\n",
    "\n",
    "        zeta = (rho0_a_ueg - rho0_b_ueg)/(rho0_a_ueg + rho0_b_ueg + 1e-8)\n",
    "        rs = (4*np.pi/3*(rho0_a_ueg+rho0_b_ueg + 1e-8))**(-1/3)\n",
    "        rs_a = (4*np.pi/3*(rho0_a_ueg + 1e-8))**(-1/3)\n",
    "        rs_b = (4*np.pi/3*(rho0_b_ueg + 1e-8))**(-1/3)\n",
    "\n",
    "\n",
    "        exc_a = torch.zeros_like(rho0_a)\n",
    "        exc_b = torch.zeros_like(rho0_a)\n",
    "        exc_ab = torch.zeros_like(rho0_a)\n",
    "\n",
    "        if debug:\n",
    "            print('eval_grid_models nan summary:')\n",
    "            print('zeta, rs, rs_a, rs_b, exc_a, exc_b, exc_ab')\n",
    "            print('{}, {}, {}, {}, {}, {}, {}'.format(\n",
    "                torch.isnan(zeta).any().sum(),\n",
    "                torch.isnan(rs).any().sum(),\n",
    "                torch.isnan(rs_a).any().sum(),\n",
    "                torch.isnan(rs_b).any().sum(),\n",
    "                torch.isnan(exc_a).any().sum(),\n",
    "                torch.isnan(exc_b).any().sum(),\n",
    "                torch.isnan(exc_ab).any().sum(),                \n",
    "            ))\n",
    "\n",
    "        descr_method = self.get_descriptors\n",
    "\n",
    "\n",
    "        descr_dict = {}\n",
    "        rho_tot = rho0_a + rho0_b\n",
    "        if self.grid_models:\n",
    "\n",
    "            for grid_model in self.grid_models:\n",
    "                if not grid_model.spin_scaling:\n",
    "                    if not 'c' in descr_dict:\n",
    "                        descr_dict['c'] = descr_method(rho0_a, rho0_b, gamma_a, gamma_b,\n",
    "                                                                         gamma_ab, nl_a, nl_b, tau_a, tau_b, spin_scaling = False)\n",
    "                        descr_dict['c'] = descr_method(rho0_a, rho0_b, gamma_a, gamma_b,\n",
    "                                                                         gamma_ab, nl_a, nl_b, tau_a, tau_b, spin_scaling = False)\n",
    "                    descr = descr_dict['c']\n",
    "                    #print(\"DESCR: \", descr)\n",
    "                    #print(\"DESCR MAX:\", torch.max(descr))\n",
    "                    #print(\"DESCR MIN: \", torch.min(descr))\n",
    "                    #print(\"GRID MODEL: \", grid_model)\n",
    "                    for name, param in grid_model.named_parameters():\n",
    "                        if torch.isnan(param).any():\n",
    "                            print(\"NANS IN NETWORK WEIGHT -- {}\".format(name))\n",
    "                            raise ValueError(\"NaNs in Network Weights.\")\n",
    "\n",
    "                    #Evaluate network with descriptors on grid\n",
    "                    #in xcdiff, edge_index is passed here, not in dpyscfl\n",
    "                    exc = grid_model(descr,\n",
    "                                      grid_coords = self.grid_coords)\n",
    "                    #print(\"EXC GRID_MODEL C: \", exc)\n",
    "\n",
    "                    #Included from xcdiff, 2dim exc -> spin polarized\n",
    "                    if exc.dim() == 2: #If using spin decomposition\n",
    "                        pw_alpha = self.pw_model(rs_a, torch.ones_like(rs_a))\n",
    "                        pw_beta = self.pw_model(rs_b, torch.ones_like(rs_b))\n",
    "                        pw = self.pw_model(rs, zeta)\n",
    "                        ec_alpha = (1 + exc[:,0])*pw_alpha*rho0_a/(rho_tot+1e-8)\n",
    "                        ec_beta =  (1 + exc[:,1])*pw_beta*rho0_b/(rho_tot+1e-8)\n",
    "                        ec_mixed = (1 + exc[:,2])*(pw*rho_tot - pw_alpha*rho0_a - pw_beta*rho0_b)/(rho_tot+1e-8)\n",
    "                        exc_ab = ec_alpha + ec_beta + ec_mixed\n",
    "                    else:\n",
    "                        if self.pw_mult:\n",
    "                            exc_ab += (1 + exc)*self.pw_model(rs, zeta)\n",
    "                        else:\n",
    "                            exc_ab += exc\n",
    "#                    if self.pw_mult:\n",
    "#                        exc_ab += (1 + exc)*self.pw_model(rs, zeta)\n",
    "#                    else:\n",
    "#                        exc_ab += exc\n",
    "                else:\n",
    "                    if not 'x' in descr_dict:\n",
    "                        descr_dict['x'] = descr_method(rho0_a, rho0_b, gamma_a, gamma_b,\n",
    "                                                                         gamma_ab, nl_a, nl_b, tau_a, tau_b, spin_scaling = True)\n",
    "                    descr = descr_dict['x']\n",
    "\n",
    "                    #in xcdiff, edge_index is passed here, not in dpyscfl\n",
    "                    exc = grid_model(descr,\n",
    "                                  grid_coords = self.grid_coords)\n",
    "\n",
    "                    #print(\"EXC GRID_MODEL X: \", exc)\n",
    "\n",
    "                    if self.heg_mult:\n",
    "                        exc_a += (1 + exc[0])*self.heg_model(2*rho0_a_ueg)*(1-self.exx_a)\n",
    "                    else:\n",
    "                        exc_a += exc[0]*(1-self.exx_a)\n",
    "\n",
    "                    if torch.all(rho0_b == torch.zeros_like(rho0_b)): #Otherwise produces NaN's\n",
    "                        exc_b += exc[0]*0\n",
    "                    else:\n",
    "                        if self.heg_mult:\n",
    "                            exc_b += (1 + exc[1])*self.heg_model(2*rho0_b_ueg)*(1-self.exx_a)\n",
    "                        else:\n",
    "                            exc_b += exc[1]*(1-self.exx_a)\n",
    "\n",
    "        else:\n",
    "            if self.heg_mult:\n",
    "                exc_a = self.heg_model(2*rho0_a_ueg)\n",
    "                exc_b = self.heg_model(2*rho0_b_ueg)\n",
    "            if self.pw_mult:\n",
    "                exc_ab = self.pw_model(rs, zeta)\n",
    "\n",
    "\n",
    "        # exc = rho0_a_ueg/rho_tot*exc_a + rho0_b_ueg/rho_tot*exc_b + exc_ab\n",
    "        exc = exc_a * (rho0_a_ueg/ (rho_tot + self.epsilon)) + exc_b*(rho0_b_ueg / (rho_tot + self.epsilon)) + exc_ab\n",
    "        if debug:\n",
    "            print('eval_grid_models nan summary:')\n",
    "            print('zeta, rs, rs_a, rs_b, exc_a, exc_b, exc_ab')\n",
    "            print('{}, {}, {}, {}, {}, {}, {}'.format(\n",
    "                torch.isnan(zeta).any().sum(),\n",
    "                torch.isnan(rs).any().sum(),\n",
    "                torch.isnan(rs_a).any().sum(),\n",
    "                torch.isnan(rs_b).any().sum(),\n",
    "                torch.isnan(exc_a).any().sum(),\n",
    "                torch.isnan(exc_b).any().sum(),\n",
    "                torch.isnan(exc_ab).any().sum(),                \n",
    "            ))\n",
    "\n",
    "        return exc.unsqueeze(-1)\n",
    "class make_rdm1(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Generate one-particle reduced density matrix\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, mo_coeff, mo_occ):\n",
    "        \"\"\"Forward pass calculating one-particle reduced density matrix.\n",
    "\n",
    "        Args:\n",
    "            mo_coeff (torch.Tensor/np.array(?)): Molecular orbital coefficients\n",
    "            mo_occ (torch.Tensor/np.array(?)): Molecular orbital occupation numbers\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor/np.array(?): The RDM1\n",
    "        \"\"\"\n",
    "        if mo_coeff.ndim == 3:\n",
    "            mocc_a = mo_coeff[0, :, mo_occ[0]>0]\n",
    "            mocc_b = mo_coeff[1, :, mo_occ[1]>0]\n",
    "            if torch.sum(mo_occ[1]) > 0:\n",
    "                return torch.stack([contract('ij,jk->ik', mocc_a*mo_occ[0,mo_occ[0]>0], mocc_a.T),\n",
    "                                    contract('ij,jk->ik', mocc_b*mo_occ[1,mo_occ[1]>0], mocc_b.T)],dim=0)\n",
    "            else:\n",
    "                return torch.stack([contract('ij,jk->ik', mocc_a*mo_occ[0,mo_occ[0]>0], mocc_a.T),\n",
    "                                    torch.zeros_like(mo_coeff)[0]],dim=0)\n",
    "        else:\n",
    "            mocc = mo_coeff[:, mo_occ>0]\n",
    "            return contract('ij,jk->ik', mocc*mo_occ[mo_occ>0], mocc.T)\n",
    "\n",
    "class get_rho(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def forward(self, dm, results):\n",
    "        ao_eval = results['ao_eval'][0]\n",
    "        print(\"AO_EVAL, DM SHAPES: {}. {}.\".format(ao_eval.shape, dm.shape))\n",
    "        if dm.ndim == 2:\n",
    "            print(\"2D DM.\")\n",
    "            print(\"RESULTS N_ELEC: \", results['n_elec'])\n",
    "            rho = contract('ij,ik,jk->i',\n",
    "                               ao_eval, ao_eval, dm)\n",
    "        else:\n",
    "            print(\"NON-2D DM\")\n",
    "            rho = contract('ij,ik,xjk->xi',\n",
    "                               ao_eval, ao_eval, dm)\n",
    "        return rho\n",
    "\n",
    "class energy_tot(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Total energy (electron-electron + electron-ion; ion-ion not included)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, dm, hcore, veff):\n",
    "        \"\"\"Tensor contraction to find total electron energy (e-e + e-ion)\n",
    "\n",
    "        Args:\n",
    "            dm (torch.Tensor): Density matrix\n",
    "            hcore (torch.Tensor): Core Hamiltonian\n",
    "            veff (torch.Tensor): Effective Potential\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The electronic energy\n",
    "        \"\"\"\n",
    "        return torch.sum((contract('...ij,ij', dm, hcore) + .5*contract('...ij,...ij', dm, veff))).unsqueeze(0)\n",
    "\n",
    "class get_veff(torch.nn.Module):\n",
    "    def __init__(self, exx=False, model=None, req_grad=False):\n",
    "        \"\"\"Builds the one-electron effective potential (not including local xc-potential)\n",
    "\n",
    "        Args:\n",
    "            exx (bool, optional): Exact exchange flag. Defaults to False.\n",
    "            model (xc-model): Only used for exact exchange mixing parameter. Defaults to None.\n",
    "            df (bool, optional): Use density fitting flag. Defaults to False.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.exx = exx\n",
    "        self.model = model\n",
    "        self.req_grad = req_grad\n",
    "        \n",
    "    def forward(self, dm, eri):\n",
    "        \"\"\"Forward pass if no density fitting\n",
    "\n",
    "        Args:\n",
    "            dm (torch.Tensor): Density matrix\n",
    "            eri (torch.Tensor(?)): Electron repulsion integral tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The \"effective\" potential\n",
    "        \"\"\"\n",
    "        J = contract('...ij,ijkl->...kl',dm, eri)\n",
    "        if self.exx:\n",
    "            K = self.model.exx_a * contract('...ij,ikjl->...kl',dm, eri)\n",
    "        else:\n",
    "            K =  torch.zeros_like(J)\n",
    "\n",
    "        if J.ndim == 3:\n",
    "            return J[0] + J[1] - K\n",
    "        else:\n",
    "            return J-0.5*K\n",
    "    def forward2(self, dm, eri):\n",
    "        ''' reimplementation of hf.dot_eri_dm '''\n",
    "        nao = dm.shape[-1]\n",
    "        if eri.nelement() == nao**4:\n",
    "            vj = contract('...ij,ijkl->...kl',dm, eri)\n",
    "            if self.exx:\n",
    "                vk = self.model.exx_a * contract('...ij,ikjl->...kl',dm, eri)\n",
    "            else:\n",
    "                vk =  torch.zeros_like(vj)\n",
    "    \n",
    "        else:\n",
    "            # raise ValueError('eri elements != nao**4')\n",
    "            vj, vk = scf._vhf.incore(eri.detach().numpy(), dm.detach().numpy(), 0, with_j = True, with_k = self.exx)\n",
    "\n",
    "        if not self.exx:\n",
    "            vk = np.zeros_like(vj)\n",
    "        if vj.ndim == 3:\n",
    "            veff =  vj[0] + vj[1] - vk\n",
    "        else:\n",
    "            veff =  vj-0.5*vk\n",
    "\n",
    "        return torch.tensor(veff, requires_grad=self.req_grad)\n",
    "        # if vj.ndim == 3:\n",
    "        #     return vj[0] + vj[1] - vk\n",
    "        # else:\n",
    "        #     return vj - 0.5*vk    \n",
    "        \n",
    "        \n",
    "\n",
    "def get_veff_np(dm, eri):\n",
    "        \"\"\"Forward pass if no density fitting\n",
    "\n",
    "        Args:\n",
    "            dm (torch.Tensor): Density matrix\n",
    "            eri (torch.Tensor(?)): Electron repulsion integral tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The \"effective\" potential\n",
    "        \"\"\"\n",
    "        J = contract('...ij,ijkl->...kl',dm, eri)\n",
    "        K =  torch.zeros_like(J)\n",
    "        if J.ndim == 3:\n",
    "            return J[0] + J[1] - K\n",
    "        else:\n",
    "            return J-0.5*K\n",
    "def energy_tot_np(dm, hcore, veff):\n",
    "        \"\"\"Tensor contraction to find total electron energy (e-e + e-ion)\n",
    "\n",
    "        Args:\n",
    "            dm (torch.Tensor): Density matrix\n",
    "            hcore (torch.Tensor): Core Hamiltonian\n",
    "            veff (torch.Tensor): Effective Potential\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The electronic energy\n",
    "        \"\"\"\n",
    "        return torch.sum((contract('...ij,ij', dm, hcore) + .5*contract('...ij,...ij', dm, veff))).unsqueeze(0)\n",
    "def make_rdm1_np(mo_coeff, mo_occ):\n",
    "        \"\"\"Forward pass calculating one-particle reduced density matrix.\n",
    "\n",
    "        Args:\n",
    "            mo_coeff (torch.Tensor/np.array(?)): Molecular orbital coefficients\n",
    "            mo_occ (torch.Tensor/np.array(?)): Molecular orbital occupation numbers\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor/np.array(?): The RDM1\n",
    "        \"\"\"\n",
    "        if mo_coeff.ndim == 3:\n",
    "            mocc_a = mo_coeff[0, :, mo_occ[0]>0]\n",
    "            mocc_b = mo_coeff[1, :, mo_occ[1]>0]\n",
    "            if torch.sum(mo_occ[1]) > 0:\n",
    "                return torch.stack([contract('ij,jk->ik', mocc_a*mo_occ[0,mo_occ[0]>0], mocc_a.T),\n",
    "                                    contract('ij,jk->ik', mocc_b*mo_occ[1,mo_occ[1]>0], mocc_b.T)],dim=0)\n",
    "            else:\n",
    "                return torch.stack([contract('ij,jk->ik', mocc_a*mo_occ[0,mo_occ[0]>0], mocc_a.T),\n",
    "                                    torch.zeros_like(mo_coeff)[0]],dim=0)\n",
    "        else:\n",
    "            mocc = mo_coeff[:, mo_occ>0]\n",
    "            return contract('ij,jk->ik', mocc*mo_occ[mo_occ>0], mocc.T)\n",
    "\n",
    "\n",
    "\n",
    "def get_fock(hc, veff):\n",
    "    \"\"\"Get the Fock matrix\n",
    "\n",
    "    Args:\n",
    "        hc (torch.Tensor): Core Hamiltonian\n",
    "        veff (torch.Tensor): Effective Potential\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: hc+veff\n",
    "    \"\"\"\n",
    "    return hc + veff\n",
    "def get_hcore(v, t):\n",
    "    \"\"\" \"Core\" Hamiltionian, includes ion-electron and kinetic contributions\n",
    "\n",
    "    .. math:: H_{core} = T + V_{nuc-elec}\n",
    "\n",
    "    Args:\n",
    "        v (torch.Tensor, np.array): Electron-ion interaction energy\n",
    "        t (torch.Tensor, np.array): Kinetic energy\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: v + t\n",
    "    \"\"\"\n",
    "    return v + t\n",
    "\n",
    "\n",
    "class eig(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Solves generalized eigenvalue problem using Cholesky decomposition\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, h, s_chol):\n",
    "        \"\"\"Solver for generalized eigenvalue problem\n",
    "\n",
    "        .. todo:: torch.symeig is deprecated for torch.linalg.eigh, replace\n",
    "\n",
    "        Args:\n",
    "            h (torch.Tensor): Hamiltionian\n",
    "            s_chol (torch.Tensor): (Inverse) Cholesky decomp. of overlap matrix S\n",
    "                                    s_chol = np.linalg.inv(np.linalg.cholesky(S))\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor, torch.Tensor): Eigenvalues (MO energies), eigenvectors (MO coeffs)\n",
    "        \"\"\"\n",
    "        #e, c = torch.symeig(contract('ij,...jk,kl->...il',s_chol, h, s_chol.T), eigenvectors=True,upper=False)\n",
    "        upper=False\n",
    "        UPLO = \"U\" if upper else \"L\"\n",
    "        e, c = torch.linalg.eigh(contract('ij,...jk,kl->...il',s_chol, h, s_chol.T), UPLO=UPLO)\n",
    "        c = contract('ij,...jk ->...ik',s_chol.T, c.clone())\n",
    "        return e, c\n",
    "torch._C._debug_only_display_vmap_fallback_warnings(True)\n",
    "class SCF(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, alpha=0.8, nsteps=10, xc=None, device='cpu', exx=False):\n",
    "        \"\"\"This class implements the self-consistent field (SCF) equations\n",
    "\n",
    "        Args:\n",
    "            alpha (float, optional): Linear mixing parameter. Defaults to 0.8.\n",
    "            nsteps (int, optional): Number of scf steps. Defaults to 10.\n",
    "            xc (dpyscfl.net.XC, optional): Class containing the exchange-correlation models. Defaults to None.\n",
    "            device (str, optional): {'cpu','cuda'}, which device to use. Defaults to 'cpu'.\n",
    "            exx (bool, optional): Use exact exchange flag. Defaults to False.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.nsteps = nsteps\n",
    "        self.alpha = alpha\n",
    "        self.get_veff = get_veff(exx, xc, req_grad=REQ_GRAD).to(device) # Include Fock (exact) exchange?\n",
    "\n",
    "        self.eig = eig().to(device)\n",
    "        self.energy_tot = energy_tot().to(device)\n",
    "        self.make_rdm1 = make_rdm1().to(device)\n",
    "        self.xc = xc\n",
    "        #ncore parameter used in xcdiff, not here\n",
    "\n",
    "    def forward(self, dm, matrices, sc=True, **kwargs):\n",
    "        \"\"\"Forward pass SCF cycle\n",
    "\n",
    "        Args:\n",
    "            dm (torch.Tensor): Initial density matrix\n",
    "            matrices (dict of torch.Tensors): Contains all other matrices that are considered fixed during SCF calculations (e-integrals etc.)\n",
    "            sc (bool, optional): If True does self-consistent calculations, else single-pass. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            dict of torch.Tensors: results: E, dm, and mo_energies\n",
    "        \"\"\"\n",
    "        dm = dm[0]\n",
    "\n",
    "        # Required matrices\n",
    "        # ===================\n",
    "        # v: Electron-ion pot.\n",
    "        # t: Kinetic\n",
    "        # mo_occ: MO occupations\n",
    "        # e_nuc: Ion-Ion energy contribution\n",
    "        # s: overlap matrix\n",
    "        # s_chol: inverse Cholesky decomposition of overlap matrix\n",
    "        v, t, mo_occ, e_nuc, s, s_chol = [matrices[key][0] for key in \\\n",
    "                                             ['v','t','mo_occ',\n",
    "                                             'e_nuc','s','s_chol']]\n",
    "        hc = get_hcore(v,t)\n",
    "\n",
    "        # Optional matrices\n",
    "        # ====================\n",
    "\n",
    "        # Electron repulsion integrals\n",
    "        eri = matrices.get('eri',[None])[0]\n",
    "\n",
    "        grid_weights = matrices.get('grid_weights',[None])[0]\n",
    "        grid_coords = matrices.get('grid_coords',[None])[0]\n",
    "        #edge index called for here in xcdiff, not here\n",
    "\n",
    "        # Atomic orbitals evaluated on grid\n",
    "        ao_eval = matrices.get('ao_eval',[None])[0]\n",
    "\n",
    "        # Used to restore correct potential after symmetrization:\n",
    "        L = matrices.get('L', [torch.eye(dm.size()[-1])])[0]\n",
    "        scaling = matrices.get('scaling',[torch.ones([dm.size()[-1]]*2)])[0]\n",
    "\n",
    "        # Density fitting integrals\n",
    "        df_2c_inv = matrices.get('df_2c_inv',[None])[0]\n",
    "        df_3c = matrices.get('df_3c',[None])[0]\n",
    "\n",
    "        # Electrostatic potential on grid\n",
    "        vh_on_grid = matrices.get('vh_on_grid',[None])[0]\n",
    "\n",
    "        dm_old = dm\n",
    "\n",
    "        E = []\n",
    "        deltadm = []\n",
    "        nsteps = self.nsteps\n",
    "\n",
    "        # if not self.xc.training:\n",
    "        #     #if not training, backpropagation doesn't happen so don't need derivatives beyond\n",
    "        #     #calculation at a given step\n",
    "        #     create_graph = False\n",
    "        # else:\n",
    "        #     create_graph = True\n",
    "        vvv = kwargs.get('verbose', False)\n",
    "        if vvv:\n",
    "            print('SCF Loop Beginning: {} Steps'.format(nsteps))\n",
    "\n",
    "        # SCF iteration loop\n",
    "        for step in range(nsteps):\n",
    "            #some diis happens here in xcdiff, not implemented here\n",
    "            if vvv:\n",
    "                print('Step {}'.format(step))\n",
    "            alpha = (self.alpha)**(step)+0.3\n",
    "            beta = (1-alpha)\n",
    "            dm = alpha * dm + beta * dm_old\n",
    "\n",
    "            dm_old = dm\n",
    "            if vvv:\n",
    "                print(\"Density Matrix stats: \")\n",
    "                print(\"Mean: \", torch.mean(dm))\n",
    "                print(\"Min/Max: \", torch.min(dm), torch.max(dm))\n",
    "                print(\"Select Indices: dm.flatten()[[0, 5, 10, 100]]\", dm.flatten()[[0,5,10,100]])\n",
    "\n",
    "            if df_3c is not None:\n",
    "                veff = self.get_veff.forward_df(dm, df_3c, df_2c_inv, eri)\n",
    "            elif kwargs.get('erisym_veff', False):\n",
    "                veff = self.get_veff.forward2(dm, eri)\n",
    "            else:\n",
    "                veff = self.get_veff(dm, eri)\n",
    "\n",
    "            if kwargs.get('debug', False):\n",
    "                print('STEP-{}/VEFF: '.format(step), veff)\n",
    "            \n",
    "            if self.xc: #If using xc-functional (not Hartree-Fock)\n",
    "                self.xc.ao_eval = ao_eval\n",
    "                self.xc.grid_weights = grid_weights\n",
    "                self.xc.grid_coords = grid_coords\n",
    "                #edge index, ml_ovlp called for here in xcdiff\n",
    "                if vh_on_grid is not None:\n",
    "                    self.xc.vh_on_grid = vh_on_grid\n",
    "                    self.xc.df_2c_inv = df_2c_inv\n",
    "                    self.xc.df_3c = df_3c\n",
    "\n",
    "                if torch.sum(mo_occ) == 1:   # Otherwise H produces NaNs\n",
    "                    dm[1] = dm.clone()[0]*1e-12\n",
    "                    dm_old[1] = dm.clone()[0]*1e-12\n",
    "\n",
    "                exc = self.xc(dm)\n",
    "\n",
    "                if kwargs.get('debug', False):\n",
    "                    print('STEP-{}/exc: '.format(step), exc)\n",
    "\n",
    "                \n",
    "                # vxc = torch.autograd.functional.jacobian(self.xc, dm, create_graph=True)\n",
    "                vxc = torch.autograd.functional.jacobian(self.xc, dm, create_graph=False,\n",
    "                                                         vectorize=False)\n",
    "                vxc1 = torch.autograd.grad(exc, dm)[0]\n",
    "                print('vxc/vxc1 shapes,', vxc.shape, vxc1.shape)\n",
    "                if kwargs.get('debug', False):\n",
    "                    msize = vxc.element_size() * vxc.nelement()\n",
    "                    msize1 = vxc1.element_size() * vxc1.nelement()\n",
    "                    print('vxc: SHAPE = {}. SIZE = {} KB / {} MB / {} GB'.format(k, vxc.shape, msize/(1000), msize/(1000**2), msize/(1000**3)))\n",
    "                    print('vxc1: SHAPE = {}. SIZE = {} KB / {} MB / {} GB'.format(k, vxc1.shape, msize1/(1000), msize1/(1000**2), msize1/(1000**3)))\n",
    "                    print('|vxc - vxc1|.max(): ', abs(vxc-vxc1).max())\n",
    "                # Restore correct symmetry for vxc\n",
    "                if vxc.dim() > 2:\n",
    "                    vxc = contract('ij,xjk,kl->xil',L,vxc.clone(),L.T)\n",
    "                    vxc = torch.where(scaling.unsqueeze(0) > 0 , vxc.clone(), scaling.unsqueeze(0))\n",
    "                else:\n",
    "                    vxc = torch.mm(L,torch.mm(vxc.clone(),L.T))\n",
    "                    vxc = torch.where(scaling > 0 , vxc.clone(), scaling)\n",
    "\n",
    "                if torch.sum(mo_occ) == 1:   # Otherwise H produces NaNs\n",
    "                    vxc[1] = torch.zeros_like(vxc.clone()[1])\n",
    "\n",
    "                veff += vxc\n",
    "\n",
    "                if kwargs.get('debug', False):\n",
    "                    print('STEP-{}/VEFF+VXC: '.format(step), veff)\n",
    "\n",
    "\n",
    "                #Add random noise to potential to avoid degeneracies in EVs\n",
    "                if self.xc.training:#: and sc:\n",
    "                    if step == 0:\n",
    "                        print(\"Noise generation to avoid potential degeneracies\")\n",
    "                    noise = torch.abs(torch.randn(vxc.size(),device=vxc.device)*1e-4)\n",
    "                    noise = noise + torch.transpose(noise,-1,-2)\n",
    "                    veff = veff.clone() + noise\n",
    "                if kwargs.get('debug', False):\n",
    "                    print('STEP-{}/VEFF+VXC+NOISE: '.format(step), veff)\n",
    "\n",
    "            else:\n",
    "                exc=0\n",
    "                vxc=torch.zeros_like(veff)\n",
    "            f = get_fock(hc, veff)\n",
    "            if kwargs.get('debug', False):\n",
    "                print('STEP-{}/FOCK: '.format(step), f)\n",
    "\n",
    "            mo_e, mo_coeff = self.eig(f, s_chol)\n",
    "            dm = self.make_rdm1(mo_coeff, mo_occ)\n",
    "\n",
    "            # e_tot = self.energy_tot(dm_old, hc, veff-vxc)+ e_nuc + exc\n",
    "            e_tot = self.energy_tot(dm, hc, veff-vxc)+ e_nuc + exc\n",
    "            E.append(e_tot)\n",
    "            if vvv:\n",
    "                print(\"{} Energy: {}\".format(step, e_tot))\n",
    "                print(\"History: {}\".format(E))\n",
    "            if not sc:\n",
    "                break\n",
    "\n",
    "        #in xcdiff, things happen here with mo_occ[:self.ncore], e_ip etc. not implemented here\n",
    "        \n",
    "        results = {'E': torch.cat(E), 'dm':dm, 'mo_energy':mo_e}\n",
    "\n",
    "        return results\n",
    "\n",
    "def get_optimizer(model, path='', hybrid=None, lr=1e-3, l2=1e-6):\n",
    "    if hybrid:\n",
    "            optimizer = torch.optim.Adam(list(model.parameters()) + [model.xc.exx_a],\n",
    "                                    lr=lr, weight_decay=l2)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                    lr=lr, weight_decay=l2)\n",
    "\n",
    "    MIN_RATE = 1e-7\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',\n",
    "                                                            verbose=True, patience=int(10/PRINT_EVERY),\n",
    "                                                            factor=0.1, min_lr=MIN_RATE)\n",
    "\n",
    "    if path:\n",
    "        optimizer.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "499207f4-29fc-41b0-8983-4822d7fc13cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_torch_xc(xctype, pretrain_loc='', hyb_par=0, path='', DEVICE='cpu', ueg_limit=True, meta_x=None, freec=False,\n",
    "            inserts = 0, nhidden = 16):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        xctype (_type_): _description_\n",
    "        pretrain_loc (_type_): _description_\n",
    "        hyb_par (int, optional): _description_. Defaults to 0.\n",
    "        path (str, optional): _description_. Defaults to ''.\n",
    "        DEVICE (str, optional): _description_. Defaults to 'cpu'.\n",
    "        ueg_limit (bool, optional): _description_. Defaults to True.\n",
    "        meta_x (_type_, optional): _description_. Defaults to None.\n",
    "        freec (bool, optional): _description_. Defaults to False.\n",
    "    \"\"\"\n",
    "    print('FREEC', freec)\n",
    "    if xctype == 'GGA':\n",
    "        lob = 1.804 if ueg_limit else 0\n",
    "        x = X_L(device=DEVICE,n_input=1, n_hidden=nhidden, use=[1], lob=lob, ueg_limit=ueg_limit) # PBE_X\n",
    "        c = C_L(device=DEVICE,n_input=3, n_hidden=nhidden, use=[2], ueg_limit=ueg_limit and not freec)\n",
    "        xc_level = 2\n",
    "    elif xctype == 'MGGA':\n",
    "        lob = 1.174 if ueg_limit else 0\n",
    "        x = X_L(device=DEVICE,n_input=2, n_hidden=nhidden, use=[1,2], lob=1.174, ueg_limit=ueg_limit) # PBE_X\n",
    "        c = C_L(device=DEVICE,n_input=4, n_hidden=nhidden, use=[2,3], ueg_limit=ueg_limit and not freec)\n",
    "        xc_level = 3\n",
    "    if pretrain_loc:\n",
    "        print(\"Loading pre-trained models from \" + pretrain_loc)\n",
    "        x.load_state_dict(torch.load(pretrain_loc + '/x'))\n",
    "        c.load_state_dict(torch.load(pretrain_loc + '/c'))\n",
    "    EXX = bool(hyb_par)\n",
    "    EXX_A = hyb_par if hyb_par else None\n",
    "\n",
    "    xc = XC(grid_models=[x, c], heg_mult=True, level=xc_level)\n",
    "    if path:\n",
    "        try:\n",
    "            xcp = torch.load(path, map_location=torch.device('cpu')).xc\n",
    "            xc.load_state_dict(xcp.state_dict())\n",
    "        except AttributeError:\n",
    "            # AttributeError: 'RecursiveScriptModule' object has no attribute 'copy'\n",
    "            #occurs when loading finished xc from xcdiff\n",
    "            xcp = torch.jit.load(path)\n",
    "            xc.load_state_dict(xcp.state_dict())\n",
    "\n",
    "    return xc\n",
    "def get_torch_weights_and_biases(torch_net):\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for nidx, net in enumerate(torch_net):\n",
    "        try:\n",
    "            w = jnp.array(net.weight.data)\n",
    "            b = jnp.array(net.bias.data)\n",
    "            weights.append(w)\n",
    "            biases.append(b)\n",
    "        except:\n",
    "            print('This torch layer is not a Linear model.')\n",
    "            continue\n",
    "    return (weights, biases)\n",
    "    \n",
    "#per https://docs.kidger.site/equinox/tricks/\n",
    "def trunc_init(weight: jax.Array, key: jax.random.PRNGKey) -> jax.Array:\n",
    "    out, in_ = weight.shape\n",
    "    stddev = math.sqrt(1 / in_)\n",
    "    return stddev * jax.random.truncated_normal(key, shape=(out, in_), lower=-2, upper=2)\n",
    "\n",
    "def init_linear_weight(model, seed, new_weights, new_bias):\n",
    "    jax.random.PRNGKey(seed)\n",
    "    is_linear = lambda x: isinstance(x, eqx.nn.Linear)\n",
    "    get_weights = lambda m: [x.weight\n",
    "                           for x in jax.tree_util.tree_leaves(m, is_leaf=is_linear)\n",
    "                           if is_linear(x)]\n",
    "    get_bias = lambda m: [x.bias\n",
    "                           for x in jax.tree_util.tree_leaves(m, is_leaf=is_linear)\n",
    "                           if is_linear(x)]\n",
    "\n",
    "    weights = get_weights(model)\n",
    "    bias = get_bias(model)\n",
    "    new_model = eqx.tree_at(get_weights, model, new_weights)\n",
    "    new_model = eqx.tree_at(get_bias, new_model, new_bias)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a969b2be-584c-4ca5-b46b-d5ad23a8a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jax_exc_func(model, ao_eval, gw):\n",
    "    def ret_func(inp):\n",
    "        return model(inp, ao_eval, gw)\n",
    "    return ret_func\n",
    "\n",
    "def jax_loss_func(loss_func, model, en, ao, gw, eri, mooc, hc, s):\n",
    "    def ret_func(dm):\n",
    "        return loss_func(model, dm, en, ao, gw, eri, mooc, hc, s)\n",
    "    return ret_func\n",
    "\n",
    "# @eqx.filter_jit\n",
    "def jax_dm(dm, eri, vxc_grad_func, mo_occ, hc, s, ogd, alpha0=0.7):\n",
    "    L = jnp.eye(dm.shape[-1])\n",
    "    scaling = jnp.ones([dm.shape[-1]]*2)\n",
    "    dm_old = dm\n",
    "    def true_func(vxc):\n",
    "        vxc.at[1].set(jnp.zeros_like(vxc[1]))\n",
    "        return vxc\n",
    "    def false_func(vxc):\n",
    "        return vxc\n",
    "    alpha = jnp.power(alpha0, 0)+0.3\n",
    "    beta = (1-alpha)\n",
    "    dm = alpha * dm + beta * dm_old\n",
    "    dm_old = dm\n",
    "    veff = xce.utils.get_veff()(dm, eri)\n",
    "    vxc = jax.grad(vxc_grad_func)(dm)\n",
    "    if vxc.ndim > 2:\n",
    "        vxc = jnp.einsum('ij,xjk,kl->xil',L,vxc,L.T)\n",
    "        vxc = jnp.where(jnp.expand_dims(scaling, 0) > 0 , vxc, jnp.expand_dims(scaling,0))\n",
    "    else:\n",
    "        vxc = jnp.matmul(L,jnp.matmul(vxc ,L.T))\n",
    "        vxc = jnp.where(scaling > 0 , vxc, scaling)\n",
    "    \n",
    "    jax.lax.cond(jnp.sum(mo_occ) == 1, true_func, false_func, vxc)\n",
    "    \n",
    "    veff += vxc\n",
    "    f = xce.utils.get_fock()(hc, veff)\n",
    "    mo_e, mo_c = xce.utils.eig()(f+1e-6*jax.random.uniform(key=jax.random.PRNGKey(92017), shape=f.shape), s, ogd)\n",
    "    dm = xce.utils.make_rdm1()(mo_c, mo_occ)\n",
    "    return dm, mo_e, mo_c\n",
    "    \n",
    "# @eqx.filter_grad\n",
    "def e_loss(model, inp_dm, ref_en, ao_eval, grid_weights, *args):\n",
    "    print(f\"e_loss; input stats. inp_dm.shape = {inp_dm.shape}, ref_en = {ref_en}, ao_eval.shape = {ao_eval.shape}, grid_weights.shape = {grid_weights.shape}\")\n",
    "    e_pred = model(inp_dm, ao_eval, grid_weights)\n",
    "    eL = jnp.sqrt( np.mean((e_pred-ref_en)**2))\n",
    "    # print('energy loss', eL)\n",
    "    return eL\n",
    "\n",
    "def holo_loss(model, inp_dm, ref_en, ao_eval, grid_weights, vxc_grad_func, mo_occ, hc, s, eri, ogd, alpha0):\n",
    "    dm, mo_e, mo_c = jax_dm(inp_dm, eri, vxc_grad_func, mo_occ, hc, s, ogd, alpha0)\n",
    "    homo_i = jnp.max(jnp.nonzero(mo_occ, size=dm.shape[0])[0])\n",
    "    homo_e = mo_e[homo_i]\n",
    "    lumo_e = mo_e[homo_i+1]\n",
    "    pred_holo = lumo_e - homo_e\n",
    "    print('pred_holo', pred_holo)\n",
    "    return jnp.sqrt( np.mean ((pred_holo - ref_en)**2))\n",
    "\n",
    "def loop_e_loss(model, inp_dms, ref_ens, ao_evals, grid_weights):\n",
    "    e_preds = []\n",
    "    for idx in range(len(ref_ens)):\n",
    "        ep = model(inp_dms[idx], ao_evals[idx], grid_weights[idx])\n",
    "        e_preds.append(ep)\n",
    "    e_preds = jnp.array(e_preds)\n",
    "    e_refs = jnp.array(ref_ens)\n",
    "    eL = jnp.sqrt( jnp.mean( (e_refs-e_preds)**2))\n",
    "    return eL\n",
    "# @eqx.filter_grad\n",
    "\n",
    "def dm_loss(model, inp_dm, ref_en, ao_eval, gw, eri, mo_occ, hc, s, ogd, *args):\n",
    "    dmp, moe, moc = jax_dm(inp_dm, eri, jax_exc_func(model, ao_eval, gw), mo_occ, hc, s, ogd)\n",
    "    dmL = jnp.sqrt(jnp.sum( (dmp - inp_dm)**2))\n",
    "    return dmL\n",
    "\n",
    "\n",
    "def loop_dm_loss(model, inp_dms, eris, mo_occs, hcs, ss, ao_evals, gws):\n",
    "    dmL = 0\n",
    "    for idx, dm in enumerate(inp_dms):\n",
    "        dmp = jax_dm(inp_dms[idx], eris[idx], jax_exc_func(model, ao_evals[idx], gws[idx]), mo_occs[idx], hcs[idx], ss[idx])\n",
    "        dmL += jnp.mean((dmp - inp_dms[idx])**2)\n",
    "    dmL = jnp.sqrt(dmL)\n",
    "    return dmL\n",
    "    \n",
    "# @eqx.filter_value_and_grad\n",
    "def total_loss(model, inp_dms, ref_ens, ref_holos, ao_evals, grid_weights, eris, mo_occs, hcs, ss, ogd):\n",
    "    # eL = e_loss(model, inp_dms, ref_ens, ao_evals, grid_weights, ogd)\n",
    "    # dmL = dm_loss(model, inp_dms, ref_ens, ao_eval, grid_weights, eris, mo_occs, hcs, ss, ogd)\n",
    "    vxcgf = jax_exc_func(model, ao_eval, grid_weights)\n",
    "    holoL = holo_loss(model, inp_dms, ref_holos, ao_evals, grid_weights, vxcgf, mo_occs, hcs, ss, eris, ogd, alpha0=0.7)\n",
    "    # return jnp.sqrt( eL**2 + holoL**2)\n",
    "    return jnp.sqrt( holoL**2 )\n",
    "\n",
    "def total_loop_loss(model, inp_dms, ref_ens, ao_evals, grid_weights, eris, mo_occs, hcs, ss):\n",
    "    eL = loop_e_loss(model, inp_dms, ref_ens, ao_evals, grid_weights)\n",
    "    dmL = loop_dm_loss(model, inp_dms, eris, mo_occs, hcs, ss, ao_evals, grid_weights)\n",
    "    return jnp.sqrt(eL**2 + dmL**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9d64854-d562-4a95-ba78-205e9f3a98bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update docs, only input =2 ??? for MGGA? holdover from sebastian for some reason\n",
    "xnet = xce.net.eX(n_input = 2, use = [1, 2], ueg_limit=True, lob=1.174)\n",
    "# I guess use default LOB\n",
    "cnet = xce.net.eC(n_input = 4, use = [2, 3], ueg_limit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db599d3e-8a58-4edc-94c6-ee4d78ed8e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FREEC False\n",
      "Loading pre-trained models from /home/awills/Documents/Research/dpyscfl/models/pretrained/scan\n",
      "This torch layer is not a Linear model.\n",
      "This torch layer is not a Linear model.\n",
      "This torch layer is not a Linear model.\n",
      "This torch layer is not a Linear model.\n",
      "This torch layer is not a Linear model.\n",
      "This torch layer is not a Linear model.\n",
      "This torch layer is not a Linear model.\n"
     ]
    }
   ],
   "source": [
    "ptscan = get_torch_xc(xctype='MGGA', pretrain_loc='/home/awills/Documents/Research/dpyscfl/models/pretrained/scan',\n",
    "                nhidden=16)\n",
    "tgms = ptscan.grid_models\n",
    "t_x_w, t_x_b = get_torch_weights_and_biases(tgms[0].net)\n",
    "t_c_w, t_c_b = get_torch_weights_and_biases(tgms[1].net)\n",
    "\n",
    "xnet = init_linear_weight(xnet, seed=92017, new_weights = t_x_w, new_bias = t_x_b)\n",
    "cnet = init_linear_weight(cnet, seed=92017, new_weights = t_c_w, new_bias = t_c_b)\n",
    "gms = [xnet, cnet]\n",
    "xc = xce.xc.eXC(grid_models = gms, level=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa151a52-1a98-4651-9dd5-62c12a76030f",
   "metadata": {},
   "source": [
    "Test molecule with pyscfad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "92697109-fac8-48ae-8282-5b8d556bc694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awills/anaconda3/envs/pyscfad/lib/python3.10/site-packages/pyscf/gto/mole.py:1215: UserWarning: Function mol.dumps drops attribute coords because it is not JSON-serializable\n",
      "  warnings.warn(msg)\n",
      "/home/awills/anaconda3/envs/pyscfad/lib/python3.10/site-packages/pyscf/gto/mole.py:1215: UserWarning: Function mol.dumps drops attribute exp because it is not JSON-serializable\n",
      "  warnings.warn(msg)\n",
      "/home/awills/anaconda3/envs/pyscfad/lib/python3.10/site-packages/pyscf/gto/mole.py:1215: UserWarning: Function mol.dumps drops attribute ctr_coeff because it is not JSON-serializable\n",
      "  warnings.warn(msg)\n",
      "/home/awills/anaconda3/envs/pyscfad/lib/python3.10/site-packages/pyscfad/_src/util.py:108: UserWarning: Not taking derivatives wrt the leaves in the node <class 'pyscfad.dft.rks.VXC'> as none of those was specified.\n",
      "  warnings.warn(f'Not taking derivatives wrt the leaves in '\n"
     ]
    }
   ],
   "source": [
    "trainms = read('/home/awills/Documents/Research2/torch_dpy/subset09_nf/subat_ref_corrected.traj', ':')\n",
    "energies = []\n",
    "dms = []\n",
    "ao_evals = []\n",
    "gws = []\n",
    "eris = []\n",
    "mo_occs = []\n",
    "hcs = []\n",
    "vs = []\n",
    "ts = []\n",
    "ss = []\n",
    "hologaps = []\n",
    "ogds = []\n",
    "for idx, at in enumerate(trainms[1:2]):\n",
    "    name, mol = xce.utils.ase_atoms_to_mol(at, basis='def2tzvpd')\n",
    "    mol.build()\n",
    "    mf = dft.RKS(mol, xc='SCAN')\n",
    "    e_tot = mf.kernel()\n",
    "    dm = mf.make_rdm1()\n",
    "    ao_eval = jnp.array(mf._numint.eval_ao(mol, mf.grids.coords, deriv=2))\n",
    "    energies.append(mf.get_veff().exc)\n",
    "    dms.append(dm)\n",
    "    ogds.append(dm.shape)\n",
    "    ao_evals.append(ao_eval)\n",
    "    gws.append(mf.grids.weights)\n",
    "    ts.append(mol.intor('int1e_kin'))\n",
    "    vs.append(mol.intor('int1e_nuc'))\n",
    "    mo_occs.append(mf.mo_occ)\n",
    "    hcs.append(mf.get_hcore())\n",
    "    eris.append(mol.intor('int2e'))\n",
    "    ss.append(jnp.linalg.inv(jnp.linalg.cholesky(mol.intor('int1e_ovlp'))))\n",
    "    hologaps.append(mf.mo_energy[mf.mo_occ == 0][0] - mf.mo_energy[mf.mo_occ > 1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88b799a6-52a9-4df0-ae84-8ad29ab7bc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(-0.69754768, dtype=float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xc(dms[0], ao_evals[0], gws[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1b9f57ba-81f6-4bdb-93dd-7fa2d54c0869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_caches():\n",
    "    for module_name, module in sys.modules.items():\n",
    "        if module_name.startswith(\"jax\"):\n",
    "            if module_name not in [\"jax.interpreters.partial_eval\"]:\n",
    "                for obj_name in dir(module):\n",
    "                    obj = getattr(module, obj_name)\n",
    "                    if hasattr(obj, \"cache_clear\"):\n",
    "                        try:\n",
    "                            obj.cache_clear()\n",
    "                        except:\n",
    "                            pass\n",
    "    gc.collect()\n",
    "# chosen_loss = loop_e_loss\n",
    "# chosen_loss = total_loop_loss\n",
    "# chosen_loss = e_loss\n",
    "# chosen_loss = dm_loss\n",
    "chosen_loss = total_loss\n",
    "# @eqx.filter_jit\n",
    "do_jit = True\n",
    "\n",
    "def train(model: eqx.Module,\n",
    "    optim: optax.GradientTransformation,\n",
    "    steps: int,\n",
    "    print_every: int,\n",
    "    clear_every: int,\n",
    "    memory_profile: bool):\n",
    "    # Just like earlier: It only makes sense to train the arrays in our model,\n",
    "    # so filter out everything else.\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_array)) \n",
    "    # Always wrap everything -- computing gradients, running the optimiser, updating\n",
    "    # the model -- into a single JIT region. This ensures things run as fast as\n",
    "    # possible.\n",
    "\n",
    "\n",
    "    def make_step(model, opt_state, inp_dm, ref_en, holos, ao_eval, grid_weights, eris, mo_occs, hcs, ss, ogd):\n",
    "        print('loss_value, grads')\n",
    "        # loss_value, grads = eqx.filter_value_and_grad(chosen_loss)(model, inp_dm, ref_en, ao_eval, grid_weights, eris, mo_occs, hcs, ss, ogd)\n",
    "        loss_value, grads = eqx.filter_value_and_grad(chosen_loss)(model, inp_dm, ref_en, holos, ao_eval, grid_weights, eris, mo_occs, hcs, ss, ogd)\n",
    "        print('updates, opt_state')\n",
    "        updates, opt_state = optim.update(grads, opt_state, model)\n",
    "        print('model update')\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return model, opt_state, loss_value\n",
    "    \n",
    "    for step in range(steps):\n",
    "        print('epoch {}'.format(step))\n",
    "        epoch_loss = 0\n",
    "        if step == 0 and do_jit:\n",
    "            fmake_step = eqx.filter_jit(make_step)\n",
    "        elif (step % clear_every) == 0 and (step > 0) and do_jit:\n",
    "            fmake_step = eqx.filter_jit(make_step)\n",
    "        else:\n",
    "            fmake_step = make_step\n",
    "        for idx in range(len(energies)):  \n",
    "            idx = len(energies)-idx-1\n",
    "            print('e {} mol {}/{}'.format(step, idx, len(energies)))\n",
    "            en = energies[idx]\n",
    "            dm = dms[idx]\n",
    "            ao = ao_evals[idx]\n",
    "            ogd = ogds[idx]\n",
    "            print(ao.shape)\n",
    "            gw = gws[idx]\n",
    "            eri = eris[idx]\n",
    "            mooc = mo_occs[idx]\n",
    "            hc = hcs[idx]\n",
    "            s = ss[idx]\n",
    "            holo = hologaps[idx]\n",
    "            e_pred = model(dm, ao, gw)\n",
    "            dmp, mo_e, mo_c = jax_dm(dm, eri, jax_exc_func(model, ao, gw), mooc, hc, s, ogd)\n",
    "            holo_pred = mo_e[mooc == 0][0] - mo_e[mooc > 1][-1]\n",
    "            print('e_pred - e_ref = {}'.format(e_pred-en))\n",
    "            print('dm_pred - dm sum = {}'.format((dmp-dm).sum()))\n",
    "            print('holo_pred - ref_holo = {}'.format(holo_pred-holo))\n",
    "            model, opt_state, train_loss = fmake_step(model, opt_state, dm, en, holo, ao, gw, eri, mooc, hc, s, ogd) \n",
    "            mol_loss = chosen_loss(model, dm, en, holo, ao, gw, eri, mooc, hc, s, ogd).item()\n",
    "            e_pred.block_until_ready()\n",
    "            if memory_profile:\n",
    "                jax.profiler.save_device_memory_profile(f\"memory{step}_{idx}.prof\")\n",
    "\n",
    "            print('mol loss = {}'.format(mol_loss))\n",
    "            epoch_loss += mol_loss\n",
    "            if (step % clear_every) and (step > 0) == 0:\n",
    "                jax_dm._clear_cache()\n",
    "                fmake_step._clear_cache()\n",
    "                equinox.clear_caches()\n",
    "                jax.clear_backends()\n",
    "                jax.clear_caches()\n",
    "                clear_caches()\n",
    "                chosen_loss.clear_cache()\n",
    "                xla._xla_callable.cache_clear()\n",
    "\n",
    "        if (step % print_every) == 0 or (step == steps - 1):\n",
    "            # current_loss = chosen_loss(model, dms, energies, ao_evals, gws, eris, mo_occs, hcs, ss).item()\n",
    "            # current_loss = chosen_loss(model, dm, en, ao, gw, eri, mooc, hc, s).item()\n",
    "            print(\n",
    "                f\"{step=}, epoch_train_loss={epoch_loss}\"\n",
    "                # f\"{step=}, train_loss={current_loss}\"\n",
    "            )\n",
    "        if (step % clear_every) and (step > 0) == 0:\n",
    "            fmake_step._clear_cache()\n",
    "            jax_dm._clear_cache()\n",
    "            equinox.clear_caches()\n",
    "            jax.clear_backends()\n",
    "            jax.clear_caches()\n",
    "            clear_caches()\n",
    "            chosen_loss.clear_cache()\n",
    "            xla._xla_callable.cache_clear()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1674056-745f-4460-85d6-b5c508a2ef2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "e 0 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0003424572856989272\n",
      "dm_pred - dm sum = -0.09088607376748996\n",
      "holo_pred - ref_holo = -0.0001811382552374674\n",
      "loss_value, grads\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo Traced<ShapedArray(float64[])>with<JVPTrace(level=3/0)> with\n",
      "  primal = Traced<ShapedArray(float64[])>with<DynamicJaxprTrace(level=1/0)>\n",
      "  tangent = Traced<ShapedArray(float64[])>with<JaxprTrace(level=2/0)> with\n",
      "    pval = (ShapedArray(float64[]), None)\n",
      "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7faa7c131560>, in_tracers=(Traced<ShapedArray(float64[]):JaxprTrace(level=2/0)>, Traced<ShapedArray(float64[]):JaxprTrace(level=2/0)>), out_tracer_refs=[<weakref at 0x7faa7c1195d0; to 'JaxprTracer' at 0x7faa7c119580>], out_avals=[ShapedArray(float64[])], primitive=pjit, params={'jaxpr': { lambda ; a:f64[] b:f64[]. let c:f64[] = sub a b in (c,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'resource_env': None, 'donated_invars': (False, False), 'name': '<lambda>', 'keep_unused': False, 'inline': True}, effects=set(), source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x7faa77086570>, name_stack=NameStack(stack=(Transform(name='jvp'),))))\n",
      "updates, opt_state\n",
      "model update\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33373024696784953\n",
      "mol loss = 3.8563386670076305e-05\n",
      "step=0, epoch_train_loss=3.8563386670076305e-05\n",
      "epoch 1\n",
      "e 1 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0006626870521753148\n",
      "dm_pred - dm sum = -0.09136238452143158\n",
      "holo_pred - ref_holo = -3.8563386670076305e-05\n",
      "loss_value, grads\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo Traced<ShapedArray(float64[])>with<JVPTrace(level=3/0)> with\n",
      "  primal = Traced<ShapedArray(float64[])>with<DynamicJaxprTrace(level=1/0)>\n",
      "  tangent = Traced<ShapedArray(float64[])>with<JaxprTrace(level=2/0)> with\n",
      "    pval = (ShapedArray(float64[]), None)\n",
      "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7faa77499320>, in_tracers=(Traced<ShapedArray(float64[]):JaxprTrace(level=2/0)>, Traced<ShapedArray(float64[]):JaxprTrace(level=2/0)>), out_tracer_refs=[<weakref at 0x7faa7c21a250; to 'JaxprTracer' at 0x7faa7c21a200>], out_avals=[ShapedArray(float64[])], primitive=pjit, params={'jaxpr': { lambda ; a:f64[] b:f64[]. let c:f64[] = sub a b in (c,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'resource_env': None, 'donated_invars': (False, False), 'name': '<lambda>', 'keep_unused': False, 'inline': True}, effects=set(), source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x7faa7c24dc70>, name_stack=NameStack(stack=(Transform(name='jvp'),))))\n",
      "updates, opt_state\n",
      "model update\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3338730847861171\n",
      "mol loss = 0.00010427443159749528\n",
      "step=1, epoch_train_loss=0.00010427443159749528\n",
      "epoch 2\n",
      "e 2 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.000983479542476573\n",
      "dm_pred - dm sum = -0.09183652202908021\n",
      "holo_pred - ref_holo = 0.00010427443159749528\n",
      "loss_value, grads\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo Traced<ShapedArray(float64[])>with<JVPTrace(level=3/0)> with\n",
      "  primal = Traced<ShapedArray(float64[])>with<DynamicJaxprTrace(level=1/0)>\n",
      "  tangent = Traced<ShapedArray(float64[])>with<JaxprTrace(level=2/0)> with\n",
      "    pval = (ShapedArray(float64[]), None)\n",
      "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7faa77499dd0>, in_tracers=(Traced<ShapedArray(float64[]):JaxprTrace(level=2/0)>, Traced<ShapedArray(float64[]):JaxprTrace(level=2/0)>), out_tracer_refs=[<weakref at 0x7faa7665a7f0; to 'JaxprTracer' at 0x7faa7665a7a0>], out_avals=[ShapedArray(float64[])], primitive=pjit, params={'jaxpr': { lambda ; a:f64[] b:f64[]. let c:f64[] = sub a b in (c,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'resource_env': None, 'donated_invars': (False, False), 'name': '<lambda>', 'keep_unused': False, 'inline': True}, effects=set(), source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x7faa766684f0>, name_stack=NameStack(stack=(Transform(name='jvp'),))))\n",
      "updates, opt_state\n",
      "model update\n"
     ]
    }
   ],
   "source": [
    "m = train(xc, optax.adamw(1e-4), steps=250, print_every=1, clear_every=1, memory_profile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "93e955aa-529a-4a0c-8583-52a9c45197ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.] [-14.26603092 -14.26463934  -1.09180816  -0.52450794  -0.4397367\n",
      "  -0.4397367   -0.39399782  -0.06022901  -0.06022901   0.08870731\n",
      "   0.14128726   0.27233178   0.28895819   0.28898985   0.30133212\n",
      "   0.30133212   0.35393616   0.35393616   0.39494556   0.4825326\n",
      "   0.48267082   0.48952567   0.48952567   0.51522962   0.53951392\n",
      "   0.72012031   0.7208078    0.80602798   0.80602798   1.08150586\n",
      "   1.10496897   1.10504532   1.25532858   1.25532858   1.48594543\n",
      "   1.4861282    1.54890009   1.72721476   1.72721476   1.9802559\n",
      "   2.00327286   2.00327286   2.19298531   2.50156108   2.57656842\n",
      "   2.57656842   2.72569718   3.01045198   3.5792937    3.57929581\n",
      "   3.70628862   3.70628862   3.78633564   3.78633564   4.20792428\n",
      "   4.20792428   4.23703183   4.60767191   4.60781699   4.62271735\n",
      "   4.62271735   4.72966943   4.72977228   5.07072767   5.07084876\n",
      "   5.16331031   5.16331031   5.18905758   6.29446251   6.475644\n",
      "   6.475644     6.81278876  31.65904887  32.40819947]\n"
     ]
    }
   ],
   "source": [
    "mocc = mo_occs[0]\n",
    "moe = mf.mo_energy\n",
    "print(mocc, moe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "70d81aa9-2ff2-46e2-bea0-848b5f1a8ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "homo_i = jnp.max(jnp.nonzero(mocc, size=ogds[0][0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1e562c22-d29b-415f-80d1-d6287c5e64ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(-0.39399782, dtype=float64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moe[homo_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9c87e5-a33b-4706-905b-e50d7113133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ao_eval.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169d6c18-6c41-49fd-b8f1-e8bf494602f4",
   "metadata": {},
   "source": [
    "Create silicon cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecf4efd-e924-4751-93b0-e35b432c3653",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = gtop.Cell()\n",
    "a = 5.43\n",
    "cell.atom = [['Si', [0,0,0]],\n",
    "              ['Si', [a/4,a/4,a/4]]]\n",
    "cell.a = jnp.asarray([[0, a/2, a/2],\n",
    "                     [a/2, 0, a/2],\n",
    "                     [a/2, a/2, 0]])\n",
    "cell.basis = 'gth-szv'\n",
    "cell.pseudo = 'gth-pade'\n",
    "cell.exp_to_discard = 0.1\n",
    "cell.build(trace_lattice_vectors=True)\n",
    "kpts = cell.make_kpts([2,2,2])\n",
    "mf = scfp.KRHF(cell, kpts=kpts)\n",
    "e = mf.kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd40a2a-4f3d-4d32-be14-31ad5e29efb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
