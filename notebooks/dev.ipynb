{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ad85b03-163f-49a0-8765-55a25b48c224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awills/anaconda3/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting \"B3LYP_WITH_VWN5 = True\" in pyscf_conf.py\n",
      "  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '\n",
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "from ase import Atoms\n",
    "from ase.io import read\n",
    "import xcquinox as xce\n",
    "import torch, jax, optax\n",
    "import numpy as np\n",
    "import equinox as eqx\n",
    "import jax.numpy as jnp\n",
    "import pyscfad as psa\n",
    "import os, sys\n",
    "from pyscfad import dft, scf, gto, df\n",
    "from pyscfad.pbc import scf as scfp\n",
    "from pyscfad.pbc import gto as gtop\n",
    "from pyscfad.pbc import dft as dftp\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE']='false'\n",
    "os.environ['XLA_PYTHON_CLIENT_ALLOCATOR'] = 'platform'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c512a82-36be-4a46-b7ab-42b784f61c2a",
   "metadata": {},
   "source": [
    "Utility function requiring torch to load old models, but we won't require torch as a prerequisite. Eventually want to have a self-contained folder of translated models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f315b50-8bf3-48ea-997f-fdb45a6fe351",
   "metadata": {},
   "source": [
    "Torch structure for loading old models below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d2f6383-a523-42f0-b1ba-dcd0a8ae3caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#relevant functions from dpyscfl to see if it can be self-contained here in the notebook\n",
    "#xcdiff has this named XC_L, not X_L. keep for consistency's sake\n",
    "\n",
    "class LOB(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, limit=1.804):\n",
    "        \"\"\" Utility function to squash output to [-1, limit-1] inteval.\n",
    "            Can be used to enforce non-negativity and Lieb-Oxford bound.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        self.limit = limit\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.limit*self.sig(x-np.log(self.limit-1))-1\n",
    "\n",
    "\n",
    "class X_L(torch.nn.Module):\n",
    "    def __init__(self, n_input, n_hidden=16, use=[], device='cpu', ueg_limit=False, lob=1.804, one_e=False):\n",
    "        \"\"\"Local exchange model based on MLP\n",
    "        Receives density descriptors in this order : [rho, s, alpha, nl],\n",
    "        input may be truncated depending on level of approximation\n",
    "\n",
    "        Args:\n",
    "            n_input (int): Input dimensions (LDA: 1, GGA: 2, meta-GGA: 3, ...)\n",
    "            n_hidden (int, optional): Number of hidden nodes (three hidden layers used by default). Defaults to 16.\n",
    "            use (list of ints, optional): Only these indices are used as input to the model (can be used to omit density as input to enforce uniform density scaling). These indices are also used to enforce UEG where the assumed order is [s, alpha, ...].. Defaults to [].\n",
    "            device (str, optional): {'cpu','cuda'}. Defaults to 'cpu'.\n",
    "            ueg_limit (bool, optional): Enforce uniform homoegeneous electron gas limit. Defaults to False.\n",
    "            lob (float, optional): Enforce this value as local Lieb-Oxford bound (don't enforce if set to 0). Defaults to 1.804.\n",
    "            one_e (bool, optional): _description_. Defaults to False.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.ueg_limit = ueg_limit\n",
    "        self.spin_scaling = True\n",
    "        self.lob = lob\n",
    "\n",
    "        if not use:\n",
    "            self.use = torch.Tensor(np.arange(n_input)).long().to(device)\n",
    "        else:\n",
    "            self.use = torch.Tensor(use).long().to(device)\n",
    "        #xcdiff includes double flag on net\n",
    "        self.net =  torch.nn.Sequential(\n",
    "                torch.nn.Linear(n_input, n_hidden),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.Linear(n_hidden, n_hidden),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.Linear(n_hidden, n_hidden),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.Linear(n_hidden, 1),\n",
    "            ).double().to(device)\n",
    "\n",
    "        #to device not declared in xcdiff\n",
    "        self.tanh = torch.nn.Tanh().to(device)\n",
    "        self.lobf = LOB(lob).to(device)\n",
    "        #below declared in xcdiff\n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "        self.shift = 1/(1+np.exp(-1e-3))\n",
    "\n",
    "    def forward(self, rho, **kwargs):\n",
    "        \"\"\"Forward pass\n",
    "\n",
    "        Args:\n",
    "            rho (_type_): _description_\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        # print(rho.size, rho.shape, rho.dtype)\n",
    "        # print('x call -- rho shape', rho.shape)\n",
    "        # print('x call -- rho[...,self.use] shape', rho[...,self.use].shape)\n",
    "        squeezed = self.net(rho[...,self.use]).squeeze()\n",
    "        # print('x call -- squeezed shape', squeezed.shape)\n",
    "        # print('x call -- squeezed', squeezed)\n",
    "\n",
    "        if self.ueg_limit:\n",
    "            ueg_lim = rho[...,self.use[0]]\n",
    "            if len(self.use) > 1:\n",
    "                ueg_lim_a = torch.pow(self.tanh(rho[...,self.use[1]]),2)\n",
    "            else:\n",
    "                ueg_lim_a = 0\n",
    "            #below comparison not in xcdiff\n",
    "            if len(self.use) > 2:\n",
    "                ueg_lim_nl = torch.sum(rho[...,self.use[2:]],dim=-1)\n",
    "            else:\n",
    "                ueg_lim_nl = 0\n",
    "        else:\n",
    "            ueg_lim = 1\n",
    "            ueg_lim_a = 0\n",
    "            ueg_lim_nl = 0\n",
    "\n",
    "        if self.lob:\n",
    "            result = self.lobf(squeezed*(ueg_lim + ueg_lim_a + ueg_lim_nl))\n",
    "        else:\n",
    "            result = squeezed*(ueg_lim + ueg_lim_a + ueg_lim_nl)\n",
    "\n",
    "        return result\n",
    "\n",
    "class C_L(torch.nn.Module):\n",
    "    def __init__(self, n_input=2,n_hidden=16, device='cpu', ueg_limit=False, lob=2.0, use = []):\n",
    "        \"\"\"Local correlation model based on MLP\n",
    "        Receives density descriptors in this order : [rho, spinscale, s, alpha, nl]\n",
    "        input may be truncated depending on level of approximation\n",
    "\n",
    "        Args:\n",
    "            n_input (int, optional): Input dimensions (LDA: 2, GGA: 3 , meta-GGA: 4). Defaults to 2.\n",
    "            n_hidden (int, optional): Number of hidden nodes (three hidden layers used by default). Defaults to 16.\n",
    "            device (str, optional): {'cpu','cuda'}. Defaults to 'cpu'.\n",
    "            ueg_limit (bool, optional): Enforce uniform homoegeneous electron gas limit. Defaults to False.\n",
    "            lob (float, optional): Technically Lieb-Oxford bound but used here to enforce non-negativity. Should be kept at 2.0 in most instances. Defaults to 2.0.\n",
    "            use (list of ints, optional): Indices for [s, alpha] (in that order) in input, to determine UEG limit. Defaults to [].\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.spin_scaling = False\n",
    "        self.lob = False\n",
    "        self.ueg_limit = ueg_limit\n",
    "        self.n_input=n_input\n",
    "\n",
    "        if not use:\n",
    "            self.use = torch.Tensor(np.arange(n_input)).long().to(device)\n",
    "        else:\n",
    "            self.use = torch.Tensor(use).long().to(device)\n",
    "        self.net = torch.nn.Sequential(\n",
    "                torch.nn.Linear(n_input, n_hidden),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.Linear(n_hidden, n_hidden),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.Linear(n_hidden, n_hidden),\n",
    "                torch.nn.GELU(),\n",
    "                torch.nn.Linear(n_hidden, 1),\n",
    "                torch.nn.Softplus()\n",
    "            ).double().to(device)\n",
    "        self.sig = torch.nn.Sigmoid()\n",
    "\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        #self.lob section allows for different values here, default=2. xcdiff doesn't have this,\n",
    "        #assumes 2 always\n",
    "        self.lob = lob\n",
    "        if self.lob:\n",
    "            self.lobf = LOB(self.lob)\n",
    "        else:\n",
    "            self.lob =  1000.0\n",
    "            self.lobf = LOB(self.lob)\n",
    "\n",
    "\n",
    "    def forward(self, rho, **kwargs):\n",
    "        \"\"\"Forward pass in network\n",
    "\n",
    "        Args:\n",
    "            rho (torch.Tensor): density\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        inp = rho\n",
    "        # print(rho.size, rho.shape, rho.dtype)\n",
    "        # print('c call -- rho shape', rho.shape)\n",
    "        # print('c call, rho[...,self.use] shape', rho.shape)\n",
    "        # print('c call, rho[...,self.use]', rho)        \n",
    "        squeezed = -self.net(inp).squeeze()\n",
    "        # print('c call -- squeezed shape', squeezed.shape)\n",
    "        # print('c call -- squeezed', squeezed)\n",
    "        \n",
    "        if self.ueg_limit:\n",
    "            #below not form used in xcdiff\n",
    "#            ueg_lim = rho[...,self.use[0]]\n",
    "            #below form used in xcdiff,\n",
    "            ueg_lim = self.tanh(rho[...,self.use[0]])\n",
    "            if len(self.use) > 1:\n",
    "                ueg_lim_a = torch.pow(self.tanh(rho[...,self.use[1]]),2)\n",
    "            else:\n",
    "                ueg_lim_a = 0\n",
    "            #xcdiff does not include this next comparison\n",
    "            if len(self.use) > 2:\n",
    "                ueg_lim_nl = torch.sum(self.tanh(rho[...,self.use[2:]])**2,dim=-1)\n",
    "            else:\n",
    "                ueg_lim_nl = 0\n",
    "\n",
    "            ueg_factor = ueg_lim + ueg_lim_a + ueg_lim_nl\n",
    "        else:\n",
    "            ueg_factor = 1\n",
    "        #xcdiff below returns the negative of the negative inputs\n",
    "        #lob is sigmoid, so odd function, negatives cancel, so not needed\n",
    "        if self.lob:\n",
    "            return self.lobf(squeezed*ueg_factor)\n",
    "        else:\n",
    "            return squeezed*ueg_factor\n",
    "class LDA_X(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\" UEG exchange\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, rho, **kwargs):\n",
    "        return -3/4*(3/np.pi)**(1/3)*rho**(1/3)\n",
    "params_a_pp     = [1,  1,  1]\n",
    "params_a_alpha1 = [0.21370,  0.20548,  0.11125]\n",
    "params_a_a      = [0.031091, 0.015545, 0.016887]\n",
    "params_a_beta1  = [7.5957, 14.1189, 10.357]\n",
    "params_a_beta2  = [3.5876, 6.1977, 3.6231]\n",
    "params_a_beta3  = [1.6382, 3.3662,  0.88026]\n",
    "params_a_beta4  = [0.49294, 0.62517, 0.49671]\n",
    "params_a_fz20   = 1.709921\n",
    "       \n",
    "class PW_C(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\" UEG correlation, Perdew & Wang\"\"\"\n",
    "        super().__init__()\n",
    "    def forward(self, rs, zeta):\n",
    "        def g_aux(k, rs):\n",
    "            return params_a_beta1[k]*torch.sqrt(rs) + params_a_beta2[k]*rs\\\n",
    "          + params_a_beta3[k]*rs**1.5 + params_a_beta4[k]*rs**(params_a_pp[k] + 1)\n",
    "\n",
    "        def g(k, rs):\n",
    "            return -2*params_a_a[k]*(1 + params_a_alpha1[k]*rs)\\\n",
    "          * torch.log(1 +  1/(2*params_a_a[k]*g_aux(k, rs)))\n",
    "\n",
    "        def f_zeta(zeta):\n",
    "            return ((1+zeta)**(4/3) + (1-zeta)**(4/3) - 2)/(2**(4/3)-2)\n",
    "\n",
    "        def f_pw(rs, zeta):\n",
    "            return g(0, rs) + zeta**4*f_zeta(zeta)*(g(1, rs) - g(0, rs) + g(2, rs)/params_a_fz20)\\\n",
    "          - f_zeta(zeta)*g(2, rs)/params_a_fz20\n",
    "\n",
    "        return f_pw(rs, zeta)\n",
    "\n",
    "class XC(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, grid_models=None, heg_mult=True, pw_mult=True,\n",
    "                    level = 1, exx_a=None, epsilon=1e-8):\n",
    "        \"\"\"Defines the XC functional on a grid\n",
    "\n",
    "        Args:\n",
    "            grid_models (list, optional): list of X_L (local exchange) or C_L (local correlation). Defines the xc-models/enhancement factors. Defaults to None.\n",
    "            heg_mult (bool, optional): Use homoegeneous electron gas exchange (multiplicative if grid_models is not empty). Defaults to True.\n",
    "            pw_mult (bool, optional): Use homoegeneous electron gas correlation (Perdew & Wang). Defaults to True.\n",
    "            level (int, optional): Controls the number of density \"descriptors\" generated. 1: LDA, 2: GGA, 3:meta-GGA, 4: meta-GGA + electrostatic (nonlocal). Defaults to 1.\n",
    "            exx_a (_type_, optional): Exact exchange mixing parameter. Defaults to None.\n",
    "            epsilon (float, optional): Offset to avoid div/0 in calculations. Defaults to 1e-8.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.heg_mult = heg_mult\n",
    "        self.pw_mult = pw_mult\n",
    "        self.grid_coords = None\n",
    "        self.training = True\n",
    "        self.level = level\n",
    "        self.epsilon = epsilon\n",
    "        if level > 3:\n",
    "            print('WARNING: Non-local models highly experimental and likely will not work ')\n",
    "        self.loge = 1e-5\n",
    "        self.s_gam = 1\n",
    "\n",
    "        if heg_mult:\n",
    "            self.heg_model = LDA_X()\n",
    "        if pw_mult:\n",
    "            self.pw_model = PW_C()\n",
    "        self.grid_models = list(grid_models)\n",
    "        if self.grid_models:\n",
    "            self.grid_models = torch.nn.ModuleList(self.grid_models)\n",
    "        self.model_mult = [1 for m in self.grid_models]\n",
    "\n",
    "        if exx_a is not None:\n",
    "            self.exx_a = torch.nn.Parameter(torch.Tensor([exx_a]))\n",
    "            self.exx_a.requires_grad = True\n",
    "        else:\n",
    "            self.exx_a = 0\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Switches self.training flag to False\n",
    "        \"\"\"\n",
    "        self.training=False\n",
    "    def train(self):\n",
    "        \"\"\"Switches self.training flag to True\n",
    "        \"\"\"\n",
    "        self.training=True\n",
    "\n",
    "    def add_model_mult(self, model_mult):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        .. todo:: \n",
    "            Unclear what the purpose of this is\n",
    "\n",
    "        Args:\n",
    "            model_mult (_type_): _description_\n",
    "        \"\"\"\n",
    "        del(self.model_mult)\n",
    "        self.register_buffer('model_mult',torch.Tensor(model_mult))\n",
    "\n",
    "    def add_exx_a(self, exx_a):\n",
    "        \"\"\"Adds exact-exchange mixing parameter after initialization\n",
    "\n",
    "        Args:\n",
    "            exx_a (float): Exchange mixing parameter\n",
    "        \"\"\"\n",
    "        self.exx_a = torch.nn.Parameter(torch.Tensor([exx_a]))\n",
    "        self.exx_a.requires_grad = True\n",
    "\n",
    "    # Density (rho)\n",
    "    def l_1(self, rho):\n",
    "        \"\"\"Level 1 Descriptor -- Creates dimensionless quantity from rho.\n",
    "        Eq. 3 in `base paper <https://link.aps.org/doi/10.1103/PhysRevB.104.L161109>`_\n",
    "\n",
    "        .. math:: x_0 = \\\\rho^{1/3}\n",
    "\n",
    "        Args:\n",
    "            rho (torch.Tensor): density\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: dimensionless density\n",
    "        \"\"\"\n",
    "        return rho**(1/3)\n",
    "\n",
    "    # Reduced density gradient s\n",
    "    def l_2(self, rho, gamma):\n",
    "        \"\"\"Level 2 Descriptor -- Reduced gradient density\n",
    "        Eq. 5 in `base paper <https://link.aps.org/doi/10.1103/PhysRevB.104.L161109>`_\n",
    "\n",
    "        .. math:: x_2=s=\\\\frac{1}{2(3\\\\pi^2)^{1/3}} \\\\frac{|\\\\nabla \\\\rho|}{\\\\rho^{4/3}}\n",
    "\n",
    "        Args:\n",
    "            rho (torch.Tensor): density\n",
    "            gamma (torch.Tensor): squared density gradient\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: reduced density gradient s\n",
    "        \"\"\"\n",
    "        return torch.sqrt(gamma)/(2*(3*np.pi**2)**(1/3)*rho**(4/3)+self.epsilon)\n",
    "\n",
    "    # Reduced kinetic energy density alpha\n",
    "    def l_3(self, rho, gamma, tau):\n",
    "        \"\"\"Level 3 Descriptor -- Reduced kinetic energy density\n",
    "        Eq. 6 in `base paper <https://link.aps.org/doi/10.1103/PhysRevB.104.L161109>`_\n",
    "\n",
    "        .. math:: x_3 = \\\\alpha = \\\\frac{\\\\tau-\\\\tau^W}{\\\\tau^{unif}},\n",
    "\n",
    "        where\n",
    "\n",
    "        .. math:: \\\\tau^W = \\\\frac{|\\\\nabla \\\\rho|^2}{8\\\\rho}, \\\\tau^{unif} = \\\\frac{3}{10} (3\\\\pi^2)^{2/3}\\\\rho^{5/3}.\n",
    "\n",
    "        Args:\n",
    "            rho (torch.Tensor): density\n",
    "            gamma (torch.Tensor): squared density gradient\n",
    "            tau (torch.Tensor): kinetic energy density\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: reduced kinetic energy density\n",
    "        \"\"\"\n",
    "        uniform_factor = (3/10)*(3*np.pi**2)**(2/3)\n",
    "        tw = gamma/(8*(rho+self.epsilon))\n",
    "        #commented is dpyscflite version, uncommented is xcdiff version\n",
    "        #return torch.nn.functional.relu((tau - tw)/(uniform_factor*rho**(5/3)+tw*1e-3 + 1e-12))\n",
    "        return (tau - gamma/(8*(rho+self.epsilon)))/(uniform_factor*rho**(5/3)+self.epsilon)\n",
    "\n",
    "    # Unit-less electrostatic potential\n",
    "    def l_4(self, rho, nl):\n",
    "        \"\"\"Level 4 Descriptor -- Unitless electrostatic potential\n",
    "\n",
    "        .. todo:: Figure out what exactly this part is\n",
    "\n",
    "        Args:\n",
    "            rho (torch.Tensor): density\n",
    "            nl (torch.Tensor): some non-local descriptor\n",
    "\n",
    "        Returns:\n",
    "            torch.nn.functional.relu: _description_\n",
    "        \"\"\"\n",
    "        u = nl[:,:1]/((rho.unsqueeze(-1)**(1/3))*self.nl_ueg[:,:1] + self.epsilon)\n",
    "        wu = nl[:,1:]/((rho.unsqueeze(-1))*self.nl_ueg[:,1:] + self.epsilon)\n",
    "        return torch.nn.functional.relu(torch.cat([u,wu],dim=-1))\n",
    "\n",
    "    def get_descriptors(self, rho0_a, rho0_b, gamma_a, gamma_b, gamma_ab,nl_a,nl_b, tau_a, tau_b, spin_scaling = False):\n",
    "        \"\"\"Creates 'ML-compatible' descriptors from the electron density and its gradients, a & b correspond to spin channels\n",
    "\n",
    "        Args:\n",
    "            rho0_a (torch.Tensor): :math:`\\\\rho` in spin-channel a\n",
    "            rho0_b (torch.Tensor): :math:`\\\\rho` in spin-channel b\n",
    "            gamma_a (torch.Tensor): :math:`|\\\\nabla \\\\rho|^2` in spin-channel a \n",
    "            gamma_b (torch.Tensor): :math:`|\\\\nabla \\\\rho|^2` in spin-channel b\n",
    "            gamma_ab (torch.Tensor): _description_\n",
    "            nl_a (torch.Tensor): _description_\n",
    "            nl_b (torch.Tensor): _description_\n",
    "            tau_a (torch.Tensor): KE density in spin-channel a\n",
    "            tau_b (torch.Tensor): KE density in spin-channel b\n",
    "            spin_scaling (bool, optional): Flag for spin-scaling. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "\n",
    "        if not spin_scaling:\n",
    "            #If no spin-scaling, calculate polarization and use for X1\n",
    "            zeta = (rho0_a - rho0_b)/(rho0_a + rho0_b + self.epsilon)\n",
    "            spinscale = 0.5*((1+zeta)**(4/3) + (1-zeta)**(4/3)) # zeta\n",
    "\n",
    "        if self.level > 0:  #  LDA\n",
    "            if spin_scaling:\n",
    "                descr1 = torch.log(self.l_1(2*rho0_a) + self.loge)\n",
    "                descr2 = torch.log(self.l_1(2*rho0_b) + self.loge)\n",
    "            else:\n",
    "                descr1 = torch.log(self.l_1(rho0_a + rho0_b) + self.loge)# rho\n",
    "                descr2 = torch.log(spinscale) # zeta\n",
    "            descr = torch.cat([descr1.unsqueeze(-1), descr2.unsqueeze(-1)],dim=-1)\n",
    "        if self.level > 1: # GGA\n",
    "            if spin_scaling:\n",
    "                descr3a = self.l_2(2*rho0_a, 4*gamma_a) # s\n",
    "                descr3b = self.l_2(2*rho0_b, 4*gamma_b) # s\n",
    "                descr3 = torch.cat([descr3a.unsqueeze(-1), descr3b.unsqueeze(-1)],dim=-1)\n",
    "                descr3 = (1-torch.exp(-descr3**2/self.s_gam))*torch.log(descr3 + 1)\n",
    "            else:\n",
    "                descr3 = self.l_2(rho0_a + rho0_b, gamma_a + gamma_b + 2*gamma_ab) # s\n",
    "                #line below in xcdiff, not dpyscfl\n",
    "                descr3 = descr3/((1+zeta)**(2/3) + (1-zeta)**2/3)\n",
    "                descr3 = descr3.unsqueeze(-1)\n",
    "                descr3 = (1-torch.exp(-descr3**2/self.s_gam))*torch.log(descr3 + 1)\n",
    "            descr = torch.cat([descr, descr3],dim=-1)\n",
    "        if self.level > 2: # meta-GGA\n",
    "            if spin_scaling:\n",
    "                descr4a = self.l_3(2*rho0_a, 4*gamma_a, 2*tau_a)\n",
    "                descr4b = self.l_3(2*rho0_b, 4*gamma_b, 2*tau_b)\n",
    "                descr4 = torch.cat([descr4a.unsqueeze(-1), descr4b.unsqueeze(-1)],dim=-1)\n",
    "                #below in xcdiff, not dpyscfl\n",
    "                descr4 = descr4**3/(descr4**2+self.epsilon)\n",
    "            else:\n",
    "                descr4 = self.l_3(rho0_a + rho0_b, gamma_a + gamma_b + 2*gamma_ab, tau_a + tau_b)\n",
    "                #next 2 in xcdiff, not dpyscfl\n",
    "                descr4 = 2*descr4/((1+zeta)**(5/3) + (1-zeta)**(5/3))\n",
    "                descr4 = descr4**3/(descr4**2+self.epsilon)\n",
    "\n",
    "                descr4 = descr4.unsqueeze(-1)\n",
    "            descr4 = torch.log((descr4 + 1)/2)\n",
    "            descr = torch.cat([descr, descr4],dim=-1)\n",
    "        if self.level > 3: # meta-GGA + V_estat\n",
    "            if spin_scaling:\n",
    "                descr5a = self.l_4(2*rho0_a, 2*nl_a)\n",
    "                descr5b = self.l_4(2*rho0_b, 2*nl_b)\n",
    "                descr5 = torch.log(torch.stack([descr5a, descr5b],dim=-1) + self.loge)\n",
    "                descr5 = descr5.view(descr5.size()[0],-1)\n",
    "            else:\n",
    "                descr5= torch.log(self.l_4(rho0_a + rho0_b, nl_a + nl_b) + self.loge)\n",
    "\n",
    "            descr = torch.cat([descr, descr5],dim=-1)\n",
    "        if spin_scaling:\n",
    "            print('spin_scaling')\n",
    "            print('descr size -- ', descr.size())\n",
    "            descr = descr.view(descr.size()[0],-1,2).permute(2,0,1)\n",
    "            print('reshaped descr size --', descr.size())\n",
    "        return descr\n",
    "\n",
    "\n",
    "    def forward(self, dm):\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            dm (torch.Tensor): density matrix\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        Exc = 0\n",
    "        if self.grid_models or self.heg_mult:\n",
    "            if self.ao_eval.dim()==2:\n",
    "                ao_eval = self.ao_eval.unsqueeze(0)\n",
    "            else:\n",
    "                ao_eval = self.ao_eval\n",
    "\n",
    "            # Create density (and gradients) from atomic orbitals evaluated on grid\n",
    "            # and density matrix\n",
    "            # rho[ijsp]: del_i phi del_j phi dm (s: spin, p: grid point index)\n",
    "            #print(\"FORWARD PASS IN XC. AO_EVAL SHAPE, DM SHAPE: \", ao_eval.shape, dm.shape)\n",
    "            rho = contract('xij,yik,...jk->xy...i', ao_eval, ao_eval, dm)+1e-10\n",
    "            rho0 = rho[0,0]\n",
    "            drho = rho[0,1:4] + rho[1:4,0]\n",
    "            tau = 0.5*(rho[1,1] + rho[2,2] + rho[3,3])\n",
    "\n",
    "            # Non-local electrostatic potential\n",
    "            if self.level > 3:\n",
    "                non_loc = contract('mnQ, QP, Pki, ...mn-> ...ki', self.df_3c, self.df_2c_inv, self.vh_on_grid, dm)\n",
    "            else:\n",
    "                non_loc = torch.zeros_like(tau).unsqueeze(-1)\n",
    "\n",
    "            if dm.dim() == 3: # If unrestricted (open-shell) calculation\n",
    "\n",
    "                # Density\n",
    "                rho0_a = rho0[0]\n",
    "                rho0_b = rho0[1]\n",
    "\n",
    "                # Contracted density gradient\n",
    "                gamma_a, gamma_b = contract('ij,ij->j',drho[:,0],drho[:,0]), contract('ij,ij->j',drho[:,1],drho[:,1])\n",
    "                gamma_ab = contract('ij,ij->j',drho[:,0],drho[:,1])\n",
    "\n",
    "                # Kinetic energy density\n",
    "                tau_a, tau_b = tau\n",
    "\n",
    "                # E.-static\n",
    "                non_loc_a, non_loc_b = non_loc\n",
    "            else:\n",
    "                rho0_a = rho0_b = rho0*0.5\n",
    "                gamma_a=gamma_b=gamma_ab= contract('ij,ij->j',drho[:],drho[:])*0.25\n",
    "                tau_a = tau_b = tau*0.5\n",
    "                non_loc_a=non_loc_b = non_loc*0.5\n",
    "\n",
    "            # xc-energy per unit particle\n",
    "            exc = self.eval_grid_models(torch.cat([rho0_a.unsqueeze(-1),\n",
    "                                                    rho0_b.unsqueeze(-1),\n",
    "                                                    gamma_a.unsqueeze(-1),\n",
    "                                                    gamma_ab.unsqueeze(-1),\n",
    "                                                    gamma_b.unsqueeze(-1),\n",
    "                                                    torch.zeros_like(rho0_a).unsqueeze(-1), #Dummy for laplacian\n",
    "                                                    torch.zeros_like(rho0_a).unsqueeze(-1), #Dummy for laplacian\n",
    "                                                    tau_a.unsqueeze(-1),\n",
    "                                                    tau_b.unsqueeze(-1),\n",
    "                                                    non_loc_a,\n",
    "                                                    non_loc_b],dim=-1))\n",
    "            print('xc call, exc.shape', exc.shape)\n",
    "            #inplace modification throws MulBackwards0 error sometimes?\n",
    "            Exc += torch.sum(((rho0_a + rho0_b)*exc.clone()[:,0])*self.grid_weights)\n",
    "            #Exc = torch.sum(((rho0_a + rho0_b)*exc[:,0])*self.grid_weights)\n",
    "            # try:\n",
    "            #     Exc = torch.sum(((rho0_a + rho0_b)*exc[:,0])*self.grid_weights)\n",
    "            # except:\n",
    "            #     e = sys.exc_info()[0]\n",
    "            #     Exc = torch.sum(((rho0_a + rho0_b)*exc[:,0])*self.grid_weights)\n",
    "            #     print(\"Error detected\")\n",
    "            #     print(e)                \n",
    "\n",
    "        #Below in xcdiff, not in dpyscfl\n",
    "        #However, keep commented out -- self.nxc_models not implemented\n",
    "        #if self.nxc_models:\n",
    "        #    for nxc_model in self.nxc_models:\n",
    "        #        Exc += nxc_model(dm, self.ml_ovlp)\n",
    "\n",
    "        # print('XC.FORWARD: Exc = ', Exc)\n",
    "        \n",
    "        return Exc\n",
    "\n",
    "    def eval_grid_models(self, rho, debug=False):\n",
    "        \"\"\"Evaluates all models stored in self.grid_models along with HEG exchange and correlation\n",
    "\n",
    "\n",
    "        Args:\n",
    "            rho ([list of torch.Tensors]): List with [rho0_a,rho0_b,gamma_a,gamma_ab,gamma_b, dummy for laplacian, dummy for laplacian, tau_a, tau_b, non_loc_a, non_loc_b]\n",
    "\n",
    "        Returns:\n",
    "            _type_: _description_\n",
    "        \"\"\"\n",
    "        Exc = 0\n",
    "        rho0_a = rho[:, 0]\n",
    "        rho0_b = rho[:, 1]\n",
    "        gamma_a = rho[:, 2]\n",
    "        gamma_ab = rho[:, 3]\n",
    "        gamma_b = rho[:, 4]\n",
    "        tau_a = rho[:, 7]\n",
    "        tau_b = rho[:, 8]\n",
    "        nl = rho[:,9:]\n",
    "        nl_size = nl.size()[-1]//2\n",
    "        nl_a = nl[:,:nl_size]\n",
    "        nl_b = nl[:,nl_size:]\n",
    "\n",
    "        C_F= 3/10*(3*np.pi**2)**(2/3)\n",
    "        #in xcdiff, self.meta_local would change below assignments\n",
    "        #not used here\n",
    "        rho0_a_ueg = rho0_a\n",
    "        rho0_b_ueg = rho0_b\n",
    "\n",
    "        zeta = (rho0_a_ueg - rho0_b_ueg)/(rho0_a_ueg + rho0_b_ueg + 1e-8)\n",
    "        rs = (4*np.pi/3*(rho0_a_ueg+rho0_b_ueg + 1e-8))**(-1/3)\n",
    "        rs_a = (4*np.pi/3*(rho0_a_ueg + 1e-8))**(-1/3)\n",
    "        rs_b = (4*np.pi/3*(rho0_b_ueg + 1e-8))**(-1/3)\n",
    "\n",
    "\n",
    "        exc_a = torch.zeros_like(rho0_a)\n",
    "        exc_b = torch.zeros_like(rho0_a)\n",
    "        exc_ab = torch.zeros_like(rho0_a)\n",
    "\n",
    "        if debug:\n",
    "            print('eval_grid_models nan summary:')\n",
    "            print('zeta, rs, rs_a, rs_b, exc_a, exc_b, exc_ab')\n",
    "            print('{}, {}, {}, {}, {}, {}, {}'.format(\n",
    "                torch.isnan(zeta).any().sum(),\n",
    "                torch.isnan(rs).any().sum(),\n",
    "                torch.isnan(rs_a).any().sum(),\n",
    "                torch.isnan(rs_b).any().sum(),\n",
    "                torch.isnan(exc_a).any().sum(),\n",
    "                torch.isnan(exc_b).any().sum(),\n",
    "                torch.isnan(exc_ab).any().sum(),                \n",
    "            ))\n",
    "\n",
    "        descr_method = self.get_descriptors\n",
    "\n",
    "\n",
    "        descr_dict = {}\n",
    "        rho_tot = rho0_a + rho0_b\n",
    "        if self.grid_models:\n",
    "\n",
    "            for grid_model in self.grid_models:\n",
    "                if not grid_model.spin_scaling:\n",
    "                    if not 'c' in descr_dict:\n",
    "                        descr_dict['c'] = descr_method(rho0_a, rho0_b, gamma_a, gamma_b,\n",
    "                                                                         gamma_ab, nl_a, nl_b, tau_a, tau_b, spin_scaling = False)\n",
    "                        descr_dict['c'] = descr_method(rho0_a, rho0_b, gamma_a, gamma_b,\n",
    "                                                                         gamma_ab, nl_a, nl_b, tau_a, tau_b, spin_scaling = False)\n",
    "                    descr = descr_dict['c']\n",
    "                    #print(\"DESCR: \", descr)\n",
    "                    #print(\"DESCR MAX:\", torch.max(descr))\n",
    "                    #print(\"DESCR MIN: \", torch.min(descr))\n",
    "                    #print(\"GRID MODEL: \", grid_model)\n",
    "                    for name, param in grid_model.named_parameters():\n",
    "                        if torch.isnan(param).any():\n",
    "                            print(\"NANS IN NETWORK WEIGHT -- {}\".format(name))\n",
    "                            raise ValueError(\"NaNs in Network Weights.\")\n",
    "\n",
    "                    #Evaluate network with descriptors on grid\n",
    "                    #in xcdiff, edge_index is passed here, not in dpyscfl\n",
    "                    exc = grid_model(descr,\n",
    "                                      grid_coords = self.grid_coords)\n",
    "                    #print(\"EXC GRID_MODEL C: \", exc)\n",
    "\n",
    "                    #Included from xcdiff, 2dim exc -> spin polarized\n",
    "                    if exc.dim() == 2: #If using spin decomposition\n",
    "                        pw_alpha = self.pw_model(rs_a, torch.ones_like(rs_a))\n",
    "                        pw_beta = self.pw_model(rs_b, torch.ones_like(rs_b))\n",
    "                        pw = self.pw_model(rs, zeta)\n",
    "                        ec_alpha = (1 + exc[:,0])*pw_alpha*rho0_a/(rho_tot+1e-8)\n",
    "                        ec_beta =  (1 + exc[:,1])*pw_beta*rho0_b/(rho_tot+1e-8)\n",
    "                        ec_mixed = (1 + exc[:,2])*(pw*rho_tot - pw_alpha*rho0_a - pw_beta*rho0_b)/(rho_tot+1e-8)\n",
    "                        exc_ab = ec_alpha + ec_beta + ec_mixed\n",
    "                    else:\n",
    "                        if self.pw_mult:\n",
    "                            exc_ab += (1 + exc)*self.pw_model(rs, zeta)\n",
    "                        else:\n",
    "                            exc_ab += exc\n",
    "#                    if self.pw_mult:\n",
    "#                        exc_ab += (1 + exc)*self.pw_model(rs, zeta)\n",
    "#                    else:\n",
    "#                        exc_ab += exc\n",
    "                else:\n",
    "                    if not 'x' in descr_dict:\n",
    "                        descr_dict['x'] = descr_method(rho0_a, rho0_b, gamma_a, gamma_b,\n",
    "                                                                         gamma_ab, nl_a, nl_b, tau_a, tau_b, spin_scaling = True)\n",
    "                    descr = descr_dict['x']\n",
    "\n",
    "                    #in xcdiff, edge_index is passed here, not in dpyscfl\n",
    "                    exc = grid_model(descr,\n",
    "                                  grid_coords = self.grid_coords)\n",
    "\n",
    "                    #print(\"EXC GRID_MODEL X: \", exc)\n",
    "\n",
    "                    if self.heg_mult:\n",
    "                        exc_a += (1 + exc[0])*self.heg_model(2*rho0_a_ueg)*(1-self.exx_a)\n",
    "                    else:\n",
    "                        exc_a += exc[0]*(1-self.exx_a)\n",
    "\n",
    "                    if torch.all(rho0_b == torch.zeros_like(rho0_b)): #Otherwise produces NaN's\n",
    "                        exc_b += exc[0]*0\n",
    "                    else:\n",
    "                        if self.heg_mult:\n",
    "                            exc_b += (1 + exc[1])*self.heg_model(2*rho0_b_ueg)*(1-self.exx_a)\n",
    "                        else:\n",
    "                            exc_b += exc[1]*(1-self.exx_a)\n",
    "\n",
    "        else:\n",
    "            if self.heg_mult:\n",
    "                exc_a = self.heg_model(2*rho0_a_ueg)\n",
    "                exc_b = self.heg_model(2*rho0_b_ueg)\n",
    "            if self.pw_mult:\n",
    "                exc_ab = self.pw_model(rs, zeta)\n",
    "\n",
    "\n",
    "        # exc = rho0_a_ueg/rho_tot*exc_a + rho0_b_ueg/rho_tot*exc_b + exc_ab\n",
    "        exc = exc_a * (rho0_a_ueg/ (rho_tot + self.epsilon)) + exc_b*(rho0_b_ueg / (rho_tot + self.epsilon)) + exc_ab\n",
    "        if debug:\n",
    "            print('eval_grid_models nan summary:')\n",
    "            print('zeta, rs, rs_a, rs_b, exc_a, exc_b, exc_ab')\n",
    "            print('{}, {}, {}, {}, {}, {}, {}'.format(\n",
    "                torch.isnan(zeta).any().sum(),\n",
    "                torch.isnan(rs).any().sum(),\n",
    "                torch.isnan(rs_a).any().sum(),\n",
    "                torch.isnan(rs_b).any().sum(),\n",
    "                torch.isnan(exc_a).any().sum(),\n",
    "                torch.isnan(exc_b).any().sum(),\n",
    "                torch.isnan(exc_ab).any().sum(),                \n",
    "            ))\n",
    "\n",
    "        return exc.unsqueeze(-1)\n",
    "class make_rdm1(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Generate one-particle reduced density matrix\"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, mo_coeff, mo_occ):\n",
    "        \"\"\"Forward pass calculating one-particle reduced density matrix.\n",
    "\n",
    "        Args:\n",
    "            mo_coeff (torch.Tensor/np.array(?)): Molecular orbital coefficients\n",
    "            mo_occ (torch.Tensor/np.array(?)): Molecular orbital occupation numbers\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor/np.array(?): The RDM1\n",
    "        \"\"\"\n",
    "        if mo_coeff.ndim == 3:\n",
    "            mocc_a = mo_coeff[0, :, mo_occ[0]>0]\n",
    "            mocc_b = mo_coeff[1, :, mo_occ[1]>0]\n",
    "            if torch.sum(mo_occ[1]) > 0:\n",
    "                return torch.stack([contract('ij,jk->ik', mocc_a*mo_occ[0,mo_occ[0]>0], mocc_a.T),\n",
    "                                    contract('ij,jk->ik', mocc_b*mo_occ[1,mo_occ[1]>0], mocc_b.T)],dim=0)\n",
    "            else:\n",
    "                return torch.stack([contract('ij,jk->ik', mocc_a*mo_occ[0,mo_occ[0]>0], mocc_a.T),\n",
    "                                    torch.zeros_like(mo_coeff)[0]],dim=0)\n",
    "        else:\n",
    "            mocc = mo_coeff[:, mo_occ>0]\n",
    "            return contract('ij,jk->ik', mocc*mo_occ[mo_occ>0], mocc.T)\n",
    "\n",
    "class get_rho(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "    def forward(self, dm, results):\n",
    "        ao_eval = results['ao_eval'][0]\n",
    "        print(\"AO_EVAL, DM SHAPES: {}. {}.\".format(ao_eval.shape, dm.shape))\n",
    "        if dm.ndim == 2:\n",
    "            print(\"2D DM.\")\n",
    "            print(\"RESULTS N_ELEC: \", results['n_elec'])\n",
    "            rho = contract('ij,ik,jk->i',\n",
    "                               ao_eval, ao_eval, dm)\n",
    "        else:\n",
    "            print(\"NON-2D DM\")\n",
    "            rho = contract('ij,ik,xjk->xi',\n",
    "                               ao_eval, ao_eval, dm)\n",
    "        return rho\n",
    "\n",
    "class energy_tot(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Total energy (electron-electron + electron-ion; ion-ion not included)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, dm, hcore, veff):\n",
    "        \"\"\"Tensor contraction to find total electron energy (e-e + e-ion)\n",
    "\n",
    "        Args:\n",
    "            dm (torch.Tensor): Density matrix\n",
    "            hcore (torch.Tensor): Core Hamiltonian\n",
    "            veff (torch.Tensor): Effective Potential\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The electronic energy\n",
    "        \"\"\"\n",
    "        return torch.sum((contract('...ij,ij', dm, hcore) + .5*contract('...ij,...ij', dm, veff))).unsqueeze(0)\n",
    "\n",
    "class get_veff(torch.nn.Module):\n",
    "    def __init__(self, exx=False, model=None, req_grad=False):\n",
    "        \"\"\"Builds the one-electron effective potential (not including local xc-potential)\n",
    "\n",
    "        Args:\n",
    "            exx (bool, optional): Exact exchange flag. Defaults to False.\n",
    "            model (xc-model): Only used for exact exchange mixing parameter. Defaults to None.\n",
    "            df (bool, optional): Use density fitting flag. Defaults to False.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.exx = exx\n",
    "        self.model = model\n",
    "        self.req_grad = req_grad\n",
    "        \n",
    "    def forward(self, dm, eri):\n",
    "        \"\"\"Forward pass if no density fitting\n",
    "\n",
    "        Args:\n",
    "            dm (torch.Tensor): Density matrix\n",
    "            eri (torch.Tensor(?)): Electron repulsion integral tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The \"effective\" potential\n",
    "        \"\"\"\n",
    "        J = contract('...ij,ijkl->...kl',dm, eri)\n",
    "        if self.exx:\n",
    "            K = self.model.exx_a * contract('...ij,ikjl->...kl',dm, eri)\n",
    "        else:\n",
    "            K =  torch.zeros_like(J)\n",
    "\n",
    "        if J.ndim == 3:\n",
    "            return J[0] + J[1] - K\n",
    "        else:\n",
    "            return J-0.5*K\n",
    "    def forward2(self, dm, eri):\n",
    "        ''' reimplementation of hf.dot_eri_dm '''\n",
    "        nao = dm.shape[-1]\n",
    "        if eri.nelement() == nao**4:\n",
    "            vj = contract('...ij,ijkl->...kl',dm, eri)\n",
    "            if self.exx:\n",
    "                vk = self.model.exx_a * contract('...ij,ikjl->...kl',dm, eri)\n",
    "            else:\n",
    "                vk =  torch.zeros_like(vj)\n",
    "    \n",
    "        else:\n",
    "            # raise ValueError('eri elements != nao**4')\n",
    "            vj, vk = scf._vhf.incore(eri.detach().numpy(), dm.detach().numpy(), 0, with_j = True, with_k = self.exx)\n",
    "\n",
    "        if not self.exx:\n",
    "            vk = np.zeros_like(vj)\n",
    "        if vj.ndim == 3:\n",
    "            veff =  vj[0] + vj[1] - vk\n",
    "        else:\n",
    "            veff =  vj-0.5*vk\n",
    "\n",
    "        return torch.tensor(veff, requires_grad=self.req_grad)\n",
    "        # if vj.ndim == 3:\n",
    "        #     return vj[0] + vj[1] - vk\n",
    "        # else:\n",
    "        #     return vj - 0.5*vk    \n",
    "        \n",
    "        \n",
    "\n",
    "def get_veff_np(dm, eri):\n",
    "        \"\"\"Forward pass if no density fitting\n",
    "\n",
    "        Args:\n",
    "            dm (torch.Tensor): Density matrix\n",
    "            eri (torch.Tensor(?)): Electron repulsion integral tensor\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The \"effective\" potential\n",
    "        \"\"\"\n",
    "        J = contract('...ij,ijkl->...kl',dm, eri)\n",
    "        K =  torch.zeros_like(J)\n",
    "        if J.ndim == 3:\n",
    "            return J[0] + J[1] - K\n",
    "        else:\n",
    "            return J-0.5*K\n",
    "def energy_tot_np(dm, hcore, veff):\n",
    "        \"\"\"Tensor contraction to find total electron energy (e-e + e-ion)\n",
    "\n",
    "        Args:\n",
    "            dm (torch.Tensor): Density matrix\n",
    "            hcore (torch.Tensor): Core Hamiltonian\n",
    "            veff (torch.Tensor): Effective Potential\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The electronic energy\n",
    "        \"\"\"\n",
    "        return torch.sum((contract('...ij,ij', dm, hcore) + .5*contract('...ij,...ij', dm, veff))).unsqueeze(0)\n",
    "def make_rdm1_np(mo_coeff, mo_occ):\n",
    "        \"\"\"Forward pass calculating one-particle reduced density matrix.\n",
    "\n",
    "        Args:\n",
    "            mo_coeff (torch.Tensor/np.array(?)): Molecular orbital coefficients\n",
    "            mo_occ (torch.Tensor/np.array(?)): Molecular orbital occupation numbers\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor/np.array(?): The RDM1\n",
    "        \"\"\"\n",
    "        if mo_coeff.ndim == 3:\n",
    "            mocc_a = mo_coeff[0, :, mo_occ[0]>0]\n",
    "            mocc_b = mo_coeff[1, :, mo_occ[1]>0]\n",
    "            if torch.sum(mo_occ[1]) > 0:\n",
    "                return torch.stack([contract('ij,jk->ik', mocc_a*mo_occ[0,mo_occ[0]>0], mocc_a.T),\n",
    "                                    contract('ij,jk->ik', mocc_b*mo_occ[1,mo_occ[1]>0], mocc_b.T)],dim=0)\n",
    "            else:\n",
    "                return torch.stack([contract('ij,jk->ik', mocc_a*mo_occ[0,mo_occ[0]>0], mocc_a.T),\n",
    "                                    torch.zeros_like(mo_coeff)[0]],dim=0)\n",
    "        else:\n",
    "            mocc = mo_coeff[:, mo_occ>0]\n",
    "            return contract('ij,jk->ik', mocc*mo_occ[mo_occ>0], mocc.T)\n",
    "\n",
    "\n",
    "\n",
    "def get_fock(hc, veff):\n",
    "    \"\"\"Get the Fock matrix\n",
    "\n",
    "    Args:\n",
    "        hc (torch.Tensor): Core Hamiltonian\n",
    "        veff (torch.Tensor): Effective Potential\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: hc+veff\n",
    "    \"\"\"\n",
    "    return hc + veff\n",
    "def get_hcore(v, t):\n",
    "    \"\"\" \"Core\" Hamiltionian, includes ion-electron and kinetic contributions\n",
    "\n",
    "    .. math:: H_{core} = T + V_{nuc-elec}\n",
    "\n",
    "    Args:\n",
    "        v (torch.Tensor, np.array): Electron-ion interaction energy\n",
    "        t (torch.Tensor, np.array): Kinetic energy\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: v + t\n",
    "    \"\"\"\n",
    "    return v + t\n",
    "\n",
    "\n",
    "class eig(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Solves generalized eigenvalue problem using Cholesky decomposition\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, h, s_chol):\n",
    "        \"\"\"Solver for generalized eigenvalue problem\n",
    "\n",
    "        .. todo:: torch.symeig is deprecated for torch.linalg.eigh, replace\n",
    "\n",
    "        Args:\n",
    "            h (torch.Tensor): Hamiltionian\n",
    "            s_chol (torch.Tensor): (Inverse) Cholesky decomp. of overlap matrix S\n",
    "                                    s_chol = np.linalg.inv(np.linalg.cholesky(S))\n",
    "\n",
    "        Returns:\n",
    "            (torch.Tensor, torch.Tensor): Eigenvalues (MO energies), eigenvectors (MO coeffs)\n",
    "        \"\"\"\n",
    "        #e, c = torch.symeig(contract('ij,...jk,kl->...il',s_chol, h, s_chol.T), eigenvectors=True,upper=False)\n",
    "        upper=False\n",
    "        UPLO = \"U\" if upper else \"L\"\n",
    "        e, c = torch.linalg.eigh(contract('ij,...jk,kl->...il',s_chol, h, s_chol.T), UPLO=UPLO)\n",
    "        c = contract('ij,...jk ->...ik',s_chol.T, c.clone())\n",
    "        return e, c\n",
    "torch._C._debug_only_display_vmap_fallback_warnings(True)\n",
    "class SCF(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, alpha=0.8, nsteps=10, xc=None, device='cpu', exx=False):\n",
    "        \"\"\"This class implements the self-consistent field (SCF) equations\n",
    "\n",
    "        Args:\n",
    "            alpha (float, optional): Linear mixing parameter. Defaults to 0.8.\n",
    "            nsteps (int, optional): Number of scf steps. Defaults to 10.\n",
    "            xc (dpyscfl.net.XC, optional): Class containing the exchange-correlation models. Defaults to None.\n",
    "            device (str, optional): {'cpu','cuda'}, which device to use. Defaults to 'cpu'.\n",
    "            exx (bool, optional): Use exact exchange flag. Defaults to False.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.nsteps = nsteps\n",
    "        self.alpha = alpha\n",
    "        self.get_veff = get_veff(exx, xc, req_grad=REQ_GRAD).to(device) # Include Fock (exact) exchange?\n",
    "\n",
    "        self.eig = eig().to(device)\n",
    "        self.energy_tot = energy_tot().to(device)\n",
    "        self.make_rdm1 = make_rdm1().to(device)\n",
    "        self.xc = xc\n",
    "        #ncore parameter used in xcdiff, not here\n",
    "\n",
    "    def forward(self, dm, matrices, sc=True, **kwargs):\n",
    "        \"\"\"Forward pass SCF cycle\n",
    "\n",
    "        Args:\n",
    "            dm (torch.Tensor): Initial density matrix\n",
    "            matrices (dict of torch.Tensors): Contains all other matrices that are considered fixed during SCF calculations (e-integrals etc.)\n",
    "            sc (bool, optional): If True does self-consistent calculations, else single-pass. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            dict of torch.Tensors: results: E, dm, and mo_energies\n",
    "        \"\"\"\n",
    "        dm = dm[0]\n",
    "\n",
    "        # Required matrices\n",
    "        # ===================\n",
    "        # v: Electron-ion pot.\n",
    "        # t: Kinetic\n",
    "        # mo_occ: MO occupations\n",
    "        # e_nuc: Ion-Ion energy contribution\n",
    "        # s: overlap matrix\n",
    "        # s_chol: inverse Cholesky decomposition of overlap matrix\n",
    "        v, t, mo_occ, e_nuc, s, s_chol = [matrices[key][0] for key in \\\n",
    "                                             ['v','t','mo_occ',\n",
    "                                             'e_nuc','s','s_chol']]\n",
    "        hc = get_hcore(v,t)\n",
    "\n",
    "        # Optional matrices\n",
    "        # ====================\n",
    "\n",
    "        # Electron repulsion integrals\n",
    "        eri = matrices.get('eri',[None])[0]\n",
    "\n",
    "        grid_weights = matrices.get('grid_weights',[None])[0]\n",
    "        grid_coords = matrices.get('grid_coords',[None])[0]\n",
    "        #edge index called for here in xcdiff, not here\n",
    "\n",
    "        # Atomic orbitals evaluated on grid\n",
    "        ao_eval = matrices.get('ao_eval',[None])[0]\n",
    "\n",
    "        # Used to restore correct potential after symmetrization:\n",
    "        L = matrices.get('L', [torch.eye(dm.size()[-1])])[0]\n",
    "        scaling = matrices.get('scaling',[torch.ones([dm.size()[-1]]*2)])[0]\n",
    "\n",
    "        # Density fitting integrals\n",
    "        df_2c_inv = matrices.get('df_2c_inv',[None])[0]\n",
    "        df_3c = matrices.get('df_3c',[None])[0]\n",
    "\n",
    "        # Electrostatic potential on grid\n",
    "        vh_on_grid = matrices.get('vh_on_grid',[None])[0]\n",
    "\n",
    "        dm_old = dm\n",
    "\n",
    "        E = []\n",
    "        deltadm = []\n",
    "        nsteps = self.nsteps\n",
    "\n",
    "        # if not self.xc.training:\n",
    "        #     #if not training, backpropagation doesn't happen so don't need derivatives beyond\n",
    "        #     #calculation at a given step\n",
    "        #     create_graph = False\n",
    "        # else:\n",
    "        #     create_graph = True\n",
    "        vvv = kwargs.get('verbose', False)\n",
    "        if vvv:\n",
    "            print('SCF Loop Beginning: {} Steps'.format(nsteps))\n",
    "\n",
    "        # SCF iteration loop\n",
    "        for step in range(nsteps):\n",
    "            #some diis happens here in xcdiff, not implemented here\n",
    "            if vvv:\n",
    "                print('Step {}'.format(step))\n",
    "            alpha = (self.alpha)**(step)+0.3\n",
    "            beta = (1-alpha)\n",
    "            dm = alpha * dm + beta * dm_old\n",
    "\n",
    "            dm_old = dm\n",
    "            if vvv:\n",
    "                print(\"Density Matrix stats: \")\n",
    "                print(\"Mean: \", torch.mean(dm))\n",
    "                print(\"Min/Max: \", torch.min(dm), torch.max(dm))\n",
    "                print(\"Select Indices: dm.flatten()[[0, 5, 10, 100]]\", dm.flatten()[[0,5,10,100]])\n",
    "\n",
    "            if df_3c is not None:\n",
    "                veff = self.get_veff.forward_df(dm, df_3c, df_2c_inv, eri)\n",
    "            elif kwargs.get('erisym_veff', False):\n",
    "                veff = self.get_veff.forward2(dm, eri)\n",
    "            else:\n",
    "                veff = self.get_veff(dm, eri)\n",
    "\n",
    "            if kwargs.get('debug', False):\n",
    "                print('STEP-{}/VEFF: '.format(step), veff)\n",
    "            \n",
    "            if self.xc: #If using xc-functional (not Hartree-Fock)\n",
    "                self.xc.ao_eval = ao_eval\n",
    "                self.xc.grid_weights = grid_weights\n",
    "                self.xc.grid_coords = grid_coords\n",
    "                #edge index, ml_ovlp called for here in xcdiff\n",
    "                if vh_on_grid is not None:\n",
    "                    self.xc.vh_on_grid = vh_on_grid\n",
    "                    self.xc.df_2c_inv = df_2c_inv\n",
    "                    self.xc.df_3c = df_3c\n",
    "\n",
    "                if torch.sum(mo_occ) == 1:   # Otherwise H produces NaNs\n",
    "                    dm[1] = dm.clone()[0]*1e-12\n",
    "                    dm_old[1] = dm.clone()[0]*1e-12\n",
    "\n",
    "                exc = self.xc(dm)\n",
    "\n",
    "                if kwargs.get('debug', False):\n",
    "                    print('STEP-{}/exc: '.format(step), exc)\n",
    "\n",
    "                \n",
    "                # vxc = torch.autograd.functional.jacobian(self.xc, dm, create_graph=True)\n",
    "                vxc = torch.autograd.functional.jacobian(self.xc, dm, create_graph=False,\n",
    "                                                         vectorize=False)\n",
    "                vxc1 = torch.autograd.grad(exc, dm)[0]\n",
    "                print('vxc/vxc1 shapes,', vxc.shape, vxc1.shape)\n",
    "                if kwargs.get('debug', False):\n",
    "                    msize = vxc.element_size() * vxc.nelement()\n",
    "                    msize1 = vxc1.element_size() * vxc1.nelement()\n",
    "                    print('vxc: SHAPE = {}. SIZE = {} KB / {} MB / {} GB'.format(k, vxc.shape, msize/(1000), msize/(1000**2), msize/(1000**3)))\n",
    "                    print('vxc1: SHAPE = {}. SIZE = {} KB / {} MB / {} GB'.format(k, vxc1.shape, msize1/(1000), msize1/(1000**2), msize1/(1000**3)))\n",
    "                    print('|vxc - vxc1|.max(): ', abs(vxc-vxc1).max())\n",
    "                # Restore correct symmetry for vxc\n",
    "                if vxc.dim() > 2:\n",
    "                    vxc = contract('ij,xjk,kl->xil',L,vxc.clone(),L.T)\n",
    "                    vxc = torch.where(scaling.unsqueeze(0) > 0 , vxc.clone(), scaling.unsqueeze(0))\n",
    "                else:\n",
    "                    vxc = torch.mm(L,torch.mm(vxc.clone(),L.T))\n",
    "                    vxc = torch.where(scaling > 0 , vxc.clone(), scaling)\n",
    "\n",
    "                if torch.sum(mo_occ) == 1:   # Otherwise H produces NaNs\n",
    "                    vxc[1] = torch.zeros_like(vxc.clone()[1])\n",
    "\n",
    "                veff += vxc\n",
    "\n",
    "                if kwargs.get('debug', False):\n",
    "                    print('STEP-{}/VEFF+VXC: '.format(step), veff)\n",
    "\n",
    "\n",
    "                #Add random noise to potential to avoid degeneracies in EVs\n",
    "                if self.xc.training:#: and sc:\n",
    "                    if step == 0:\n",
    "                        print(\"Noise generation to avoid potential degeneracies\")\n",
    "                    noise = torch.abs(torch.randn(vxc.size(),device=vxc.device)*1e-4)\n",
    "                    noise = noise + torch.transpose(noise,-1,-2)\n",
    "                    veff = veff.clone() + noise\n",
    "                if kwargs.get('debug', False):\n",
    "                    print('STEP-{}/VEFF+VXC+NOISE: '.format(step), veff)\n",
    "\n",
    "            else:\n",
    "                exc=0\n",
    "                vxc=torch.zeros_like(veff)\n",
    "            f = get_fock(hc, veff)\n",
    "            if kwargs.get('debug', False):\n",
    "                print('STEP-{}/FOCK: '.format(step), f)\n",
    "\n",
    "            mo_e, mo_coeff = self.eig(f, s_chol)\n",
    "            dm = self.make_rdm1(mo_coeff, mo_occ)\n",
    "\n",
    "            # e_tot = self.energy_tot(dm_old, hc, veff-vxc)+ e_nuc + exc\n",
    "            e_tot = self.energy_tot(dm, hc, veff-vxc)+ e_nuc + exc\n",
    "            E.append(e_tot)\n",
    "            if vvv:\n",
    "                print(\"{} Energy: {}\".format(step, e_tot))\n",
    "                print(\"History: {}\".format(E))\n",
    "            if not sc:\n",
    "                break\n",
    "\n",
    "        #in xcdiff, things happen here with mo_occ[:self.ncore], e_ip etc. not implemented here\n",
    "        \n",
    "        results = {'E': torch.cat(E), 'dm':dm, 'mo_energy':mo_e}\n",
    "\n",
    "        return results\n",
    "\n",
    "def get_optimizer(model, path='', hybrid=None, lr=1e-3, l2=1e-6):\n",
    "    if hybrid:\n",
    "            optimizer = torch.optim.Adam(list(model.parameters()) + [model.xc.exx_a],\n",
    "                                    lr=lr, weight_decay=l2)\n",
    "    else:\n",
    "        optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                    lr=lr, weight_decay=l2)\n",
    "\n",
    "    MIN_RATE = 1e-7\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',\n",
    "                                                            verbose=True, patience=int(10/PRINT_EVERY),\n",
    "                                                            factor=0.1, min_lr=MIN_RATE)\n",
    "\n",
    "    if path:\n",
    "        optimizer.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "499207f4-29fc-41b0-8983-4822d7fc13cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_torch_xc(xctype, pretrain_loc='', hyb_par=0, path='', DEVICE='cpu', ueg_limit=True, meta_x=None, freec=False,\n",
    "            inserts = 0, nhidden = 16):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        xctype (_type_): _description_\n",
    "        pretrain_loc (_type_): _description_\n",
    "        hyb_par (int, optional): _description_. Defaults to 0.\n",
    "        path (str, optional): _description_. Defaults to ''.\n",
    "        DEVICE (str, optional): _description_. Defaults to 'cpu'.\n",
    "        ueg_limit (bool, optional): _description_. Defaults to True.\n",
    "        meta_x (_type_, optional): _description_. Defaults to None.\n",
    "        freec (bool, optional): _description_. Defaults to False.\n",
    "    \"\"\"\n",
    "    print('FREEC', freec)\n",
    "    if xctype == 'GGA':\n",
    "        lob = 1.804 if ueg_limit else 0\n",
    "        x = X_L(device=DEVICE,n_input=1, n_hidden=nhidden, use=[1], lob=lob, ueg_limit=ueg_limit) # PBE_X\n",
    "        c = C_L(device=DEVICE,n_input=3, n_hidden=nhidden, use=[2], ueg_limit=ueg_limit and not freec)\n",
    "        xc_level = 2\n",
    "    elif xctype == 'MGGA':\n",
    "        lob = 1.174 if ueg_limit else 0\n",
    "        x = X_L(device=DEVICE,n_input=2, n_hidden=nhidden, use=[1,2], lob=1.174, ueg_limit=ueg_limit) # PBE_X\n",
    "        c = C_L(device=DEVICE,n_input=4, n_hidden=nhidden, use=[2,3], ueg_limit=ueg_limit and not freec)\n",
    "        xc_level = 3\n",
    "    if pretrain_loc:\n",
    "        print(\"Loading pre-trained models from \" + pretrain_loc)\n",
    "        x.load_state_dict(torch.load(pretrain_loc + '/x'))\n",
    "        c.load_state_dict(torch.load(pretrain_loc + '/c'))\n",
    "    EXX = bool(hyb_par)\n",
    "    EXX_A = hyb_par if hyb_par else None\n",
    "\n",
    "    xc = XC(grid_models=[x, c], heg_mult=True, level=xc_level)\n",
    "    if path:\n",
    "        try:\n",
    "            xcp = torch.load(path, map_location=torch.device('cpu')).xc\n",
    "            xc.load_state_dict(xcp.state_dict())\n",
    "        except AttributeError:\n",
    "            # AttributeError: 'RecursiveScriptModule' object has no attribute 'copy'\n",
    "            #occurs when loading finished xc from xcdiff\n",
    "            xcp = torch.jit.load(path)\n",
    "            xc.load_state_dict(xcp.state_dict())\n",
    "\n",
    "    return xc\n",
    "def get_torch_weights_and_biases(torch_net):\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for nidx, net in enumerate(torch_net):\n",
    "        try:\n",
    "            w = jnp.array(net.weight.data)\n",
    "            b = jnp.array(net.bias.data)\n",
    "            weights.append(w)\n",
    "            biases.append(b)\n",
    "        except:\n",
    "            print('This torch layer is not a Linear model.')\n",
    "            continue\n",
    "    return (weights, biases)\n",
    "    \n",
    "#per https://docs.kidger.site/equinox/tricks/\n",
    "def trunc_init(weight: jax.Array, key: jax.random.PRNGKey) -> jax.Array:\n",
    "    out, in_ = weight.shape\n",
    "    stddev = math.sqrt(1 / in_)\n",
    "    return stddev * jax.random.truncated_normal(key, shape=(out, in_), lower=-2, upper=2)\n",
    "\n",
    "def init_linear_weight(model, seed, new_weights, new_bias):\n",
    "    jax.random.PRNGKey(seed)\n",
    "    is_linear = lambda x: isinstance(x, eqx.nn.Linear)\n",
    "    get_weights = lambda m: [x.weight\n",
    "                           for x in jax.tree_util.tree_leaves(m, is_leaf=is_linear)\n",
    "                           if is_linear(x)]\n",
    "    get_bias = lambda m: [x.bias\n",
    "                           for x in jax.tree_util.tree_leaves(m, is_leaf=is_linear)\n",
    "                           if is_linear(x)]\n",
    "\n",
    "    weights = get_weights(model)\n",
    "    bias = get_bias(model)\n",
    "    new_model = eqx.tree_at(get_weights, model, new_weights)\n",
    "    new_model = eqx.tree_at(get_bias, new_model, new_bias)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a969b2be-584c-4ca5-b46b-d5ad23a8a5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jax_exc_func(model, ao_eval, gw):\n",
    "    def ret_func(inp):\n",
    "        return model(inp, ao_eval, gw)\n",
    "    return ret_func\n",
    "\n",
    "def jax_loss_func(loss_func, model, en, ao, gw, eri, mooc, hc, s):\n",
    "    def ret_func(dm):\n",
    "        return loss_func(model, dm, en, ao, gw, eri, mooc, hc, s)\n",
    "    return ret_func\n",
    "\n",
    "# @eqx.filter_jit\n",
    "def jax_dm(dm, eri, vxc_grad_func, mo_occ, hc, s, ogd, alpha0=0.7):\n",
    "    L = jnp.eye(dm.shape[-1])\n",
    "    scaling = jnp.ones([dm.shape[-1]]*2)\n",
    "    dm_old = dm\n",
    "    def true_func(vxc):\n",
    "        vxc.at[1].set(jnp.zeros_like(vxc[1]))\n",
    "        return vxc\n",
    "    def false_func(vxc):\n",
    "        return vxc\n",
    "    alpha = jnp.power(alpha0, 0)+0.3\n",
    "    beta = (1-alpha)\n",
    "    dm = alpha * dm + beta * dm_old\n",
    "    dm_old = dm\n",
    "    veff = xce.utils.get_veff()(dm, eri)\n",
    "    vxc = jax.grad(vxc_grad_func)(dm)\n",
    "    if vxc.ndim > 2:\n",
    "        vxc = jnp.einsum('ij,xjk,kl->xil',L,vxc,L.T)\n",
    "        vxc = jnp.where(jnp.expand_dims(scaling, 0) > 0 , vxc, jnp.expand_dims(scaling,0))\n",
    "    else:\n",
    "        vxc = jnp.matmul(L,jnp.matmul(vxc ,L.T))\n",
    "        vxc = jnp.where(scaling > 0 , vxc, scaling)\n",
    "    \n",
    "    jax.lax.cond(jnp.sum(mo_occ) == 1, true_func, false_func, vxc)\n",
    "    \n",
    "    veff += vxc\n",
    "    f = xce.utils.get_fock()(hc, veff)\n",
    "    mo_e, mo_c = xce.utils.eig()(f+1e-6*jax.random.uniform(key=jax.random.PRNGKey(92017), shape=f.shape), s, ogd)\n",
    "    dm = xce.utils.make_rdm1()(mo_c, mo_occ)\n",
    "    return dm, mo_e, mo_c\n",
    "    \n",
    "# @eqx.filter_grad\n",
    "def e_loss(model, inp_dm, ref_en, ao_eval, grid_weights, *args):\n",
    "    print(f\"e_loss; input stats. inp_dm.shape = {inp_dm.shape}, ref_en = {ref_en}, ao_eval.shape = {ao_eval.shape}, grid_weights.shape = {grid_weights.shape}\")\n",
    "    e_pred = model(inp_dm, ao_eval, grid_weights)\n",
    "    eL = jnp.sqrt( np.mean((e_pred-ref_en)**2))\n",
    "    # print('energy loss', eL)\n",
    "    return eL\n",
    "\n",
    "class E_loss(eqx.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def __call__(self, model, inp_dm, ref_en, ao_eval, grid_weights):\n",
    "\n",
    "        e_pred = model(inp_dm, ao_eval, grid_weights)\n",
    "        eL = jnp.sqrt( jnp.mean((e_pred-ref_en)**2))\n",
    "        return eL\n",
    "\n",
    "def holo_loss(model, inp_dm, ref_en, ao_eval, grid_weights, vxc_grad_func, mo_occ, hc, s, eri, ogd, alpha0):\n",
    "    dm, mo_e, mo_c = jax_dm(inp_dm, eri, vxc_grad_func, mo_occ, hc, s, ogd, alpha0)\n",
    "    homo_i = jnp.max(jnp.nonzero(mo_occ, size=dm.shape[0])[0])\n",
    "    homo_e = mo_e[homo_i]\n",
    "    lumo_e = mo_e[homo_i+1]\n",
    "    pred_holo = lumo_e - homo_e\n",
    "    print('pred_holo', pred_holo)\n",
    "    return jnp.sqrt( np.mean ((pred_holo - ref_en)**2))\n",
    "\n",
    "def loop_e_loss(model, inp_dms, ref_ens, ao_evals, grid_weights):\n",
    "    e_preds = []\n",
    "    for idx in range(len(ref_ens)):\n",
    "        ep = model(inp_dms[idx], ao_evals[idx], grid_weights[idx])\n",
    "        e_preds.append(ep)\n",
    "    e_preds = jnp.array(e_preds)\n",
    "    e_refs = jnp.array(ref_ens)\n",
    "    eL = jnp.sqrt( jnp.mean( (e_refs-e_preds)**2))\n",
    "    return eL\n",
    "# @eqx.filter_grad\n",
    "\n",
    "def dm_loss(model, inp_dm, ref_en, ao_eval, gw, eri, mo_occ, hc, s, ogd, *args):\n",
    "    dmp, moe, moc = jax_dm(inp_dm, eri, jax_exc_func(model, ao_eval, gw), mo_occ, hc, s, ogd)\n",
    "    dmL = jnp.sqrt(jnp.sum( (dmp - inp_dm)**2))\n",
    "    return dmL\n",
    "\n",
    "\n",
    "def loop_dm_loss(model, inp_dms, eris, mo_occs, hcs, ss, ao_evals, gws):\n",
    "    dmL = 0\n",
    "    for idx, dm in enumerate(inp_dms):\n",
    "        dmp = jax_dm(inp_dms[idx], eris[idx], jax_exc_func(model, ao_evals[idx], gws[idx]), mo_occs[idx], hcs[idx], ss[idx])\n",
    "        dmL += jnp.mean((dmp - inp_dms[idx])**2)\n",
    "    dmL = jnp.sqrt(dmL)\n",
    "    return dmL\n",
    "    \n",
    "# @eqx.filter_value_and_grad\n",
    "def total_loss(model, inp_dms, ref_ens, ref_holos, ao_evals, grid_weights, eris, mo_occs, hcs, ss, ogd):\n",
    "    # eL = e_loss(model, inp_dms, ref_ens, ao_evals, grid_weights, ogd)\n",
    "    # dmL = dm_loss(model, inp_dms, ref_ens, ao_eval, grid_weights, eris, mo_occs, hcs, ss, ogd)\n",
    "    vxcgf = jax_exc_func(model, ao_eval, grid_weights)\n",
    "    holoL = holo_loss(model, inp_dms, ref_holos, ao_evals, grid_weights, vxcgf, mo_occs, hcs, ss, eris, ogd, alpha0=0.7)\n",
    "    # return jnp.sqrt( eL**2 + holoL**2)\n",
    "    return jnp.sqrt( holoL**2 )\n",
    "\n",
    "def total_loop_loss(model, inp_dms, ref_ens, ao_evals, grid_weights, eris, mo_occs, hcs, ss):\n",
    "    eL = loop_e_loss(model, inp_dms, ref_ens, ao_evals, grid_weights)\n",
    "    dmL = loop_dm_loss(model, inp_dms, eris, mo_occs, hcs, ss, ao_evals, grid_weights)\n",
    "    return jnp.sqrt(eL**2 + dmL**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9d64854-d562-4a95-ba78-205e9f3a98bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#update docs, only input =2 ??? for MGGA? holdover from sebastian for some reason\n",
    "xnet = xce.net.eX(n_input = 2, use = [1, 2], ueg_limit=True, lob=1.174)\n",
    "# I guess use default LOB\n",
    "cnet = xce.net.eC(n_input = 4, use = [2, 3], ueg_limit=True)\n",
    "blankxc = xce.xc.eXC(grid_models = [xnet, cnet], level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db599d3e-8a58-4edc-94c6-ee4d78ed8e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FREEC False\n",
      "Loading pre-trained models from /home/awills/Documents/Research/dpyscfl/models/pretrained/scan\n",
      "This torch layer is not a Linear model.\n",
      "This torch layer is not a Linear model.\n",
      "This torch layer is not a Linear model.\n",
      "This torch layer is not a Linear model.\n",
      "This torch layer is not a Linear model.\n",
      "This torch layer is not a Linear model.\n",
      "This torch layer is not a Linear model.\n"
     ]
    }
   ],
   "source": [
    "ptscan = get_torch_xc(xctype='MGGA', pretrain_loc='/home/awills/Documents/Research/dpyscfl/models/pretrained/scan',\n",
    "                nhidden=16)\n",
    "tgms = ptscan.grid_models\n",
    "t_x_w, t_x_b = get_torch_weights_and_biases(tgms[0].net)\n",
    "t_c_w, t_c_b = get_torch_weights_and_biases(tgms[1].net)\n",
    "\n",
    "xnet = init_linear_weight(xnet, seed=92017, new_weights = t_x_w, new_bias = t_x_b)\n",
    "cnet = init_linear_weight(cnet, seed=92017, new_weights = t_c_w, new_bias = t_c_b)\n",
    "gms = [xnet, cnet]\n",
    "xc = xce.xc.eXC(grid_models = gms, level=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "776c0275-d16e-4c04-a0f6-6b3e273be21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FREEC False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awills/anaconda3/envs/pyscfad/lib/python3.10/site-packages/torch/serialization.py:1007: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n"
     ]
    }
   ],
   "source": [
    "xcd = get_torch_xc(xctype='MGGA', path='/home/awills/Documents/Research/torch_dpy/models/xcdiff/MODEL_MGGA/xc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7a6a71-e226-4b0c-b69d-f30fe33f83cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be41362b-346a-4109-b197-2ad96aa1d3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-2.15969814e+00, -3.33072779e-01],\n",
       "       [-1.33774353e+00, -6.14078521e-01],\n",
       "       [ 5.30909233e-01, -4.35234039e+00],\n",
       "       [-2.82881466e-01, -1.18875395e+00],\n",
       "       [-1.20849832e+01, -1.49310464e+00],\n",
       "       [-4.79616419e+01, -3.27104576e-01],\n",
       "       [-7.55614816e+00,  1.06021431e-01],\n",
       "       [-8.53568427e-01, -3.04049449e-01],\n",
       "       [-1.81156744e+00, -6.52161338e-01],\n",
       "       [-4.16262999e+00, -2.73711612e+00],\n",
       "       [ 3.95412026e-01, -7.49266771e-02],\n",
       "       [-4.85004866e-01, -2.93522760e-01],\n",
       "       [-4.21466678e+00, -9.35071154e-03],\n",
       "       [-1.09003024e+00,  6.17014809e-02],\n",
       "       [ 5.53233943e-01,  2.69094329e-01],\n",
       "       [-3.36713487e+00, -1.55620977e-02]], dtype=float64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xc.grid_models[0].net.layers[0].weight - blankxc.grid_models[0].net.layers[0].weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ebe944d-de21-47c7-9c1b-265ee137d658",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '/home/awills/Documents/Research/xcquinox/models/pretrained/scan'\n",
    "eqx.tree_serialise_leaves(os.path.join(p, 'xc.eqx'), xc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5be0e747-2c9a-48f9-a7ed-c54ab6666ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadxc = eqx.tree_deserialise_leaves(os.path.join(p, 'xc.eqx'), blankxc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa151a52-1a98-4651-9dd5-62c12a76030f",
   "metadata": {},
   "source": [
    "Test molecule with pyscfad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "92697109-fac8-48ae-8282-5b8d556bc694",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awills/anaconda3/envs/pyscfad/lib/python3.10/site-packages/pyscf/gto/mole.py:1215: UserWarning: Function mol.dumps drops attribute coords because it is not JSON-serializable\n",
      "  warnings.warn(msg)\n",
      "/home/awills/anaconda3/envs/pyscfad/lib/python3.10/site-packages/pyscf/gto/mole.py:1215: UserWarning: Function mol.dumps drops attribute exp because it is not JSON-serializable\n",
      "  warnings.warn(msg)\n",
      "/home/awills/anaconda3/envs/pyscfad/lib/python3.10/site-packages/pyscf/gto/mole.py:1215: UserWarning: Function mol.dumps drops attribute ctr_coeff because it is not JSON-serializable\n",
      "  warnings.warn(msg)\n",
      "/home/awills/anaconda3/envs/pyscfad/lib/python3.10/site-packages/pyscfad/_src/util.py:108: UserWarning: Not taking derivatives wrt the leaves in the node <class 'pyscfad.dft.rks.VXC'> as none of those was specified.\n",
      "  warnings.warn(f'Not taking derivatives wrt the leaves in '\n"
     ]
    }
   ],
   "source": [
    "trainms = read('/home/awills/Documents/Research2/torch_dpy/subset09_nf/subat_ref_corrected.traj', ':')\n",
    "energies = []\n",
    "dms = []\n",
    "ao_evals = []\n",
    "gws = []\n",
    "eris = []\n",
    "mo_occs = []\n",
    "hcs = []\n",
    "vs = []\n",
    "ts = []\n",
    "ss = []\n",
    "hologaps = []\n",
    "ogds = []\n",
    "for idx, at in enumerate(trainms[1:2]):\n",
    "    name, mol = xce.utils.ase_atoms_to_mol(at, basis='def2tzvpd')\n",
    "    mol.build()\n",
    "    mf = dft.RKS(mol, xc='SCAN')\n",
    "    e_tot = mf.kernel()\n",
    "    dm = mf.make_rdm1()\n",
    "    ao_eval = jnp.array(mf._numint.eval_ao(mol, mf.grids.coords, deriv=2))\n",
    "    energies.append(mf.get_veff().exc)\n",
    "    dms.append(dm)\n",
    "    ogds.append(dm.shape)\n",
    "    ao_evals.append(ao_eval)\n",
    "    gws.append(mf.grids.weights)\n",
    "    ts.append(mol.intor('int1e_kin'))\n",
    "    vs.append(mol.intor('int1e_nuc'))\n",
    "    mo_occs.append(mf.mo_occ)\n",
    "    hcs.append(mf.get_hcore())\n",
    "    eris.append(mol.intor('int2e'))\n",
    "    ss.append(jnp.linalg.inv(jnp.linalg.cholesky(mol.intor('int1e_ovlp'))))\n",
    "    hologaps.append(mf.mo_energy[mf.mo_occ == 0][0] - mf.mo_energy[mf.mo_occ > 1][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88b799a6-52a9-4df0-ae84-8ad29ab7bc08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(-0.69754768, dtype=float64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xc(dms[0], ao_evals[0], gws[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "5ea5b5a1-84c5-4e39-8a65-776ae88762e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class xcTrainer(eqx.Module):\n",
    "    model: eqx.Module\n",
    "    optim: optax.GradientTransformation\n",
    "    loss: eqx.Module\n",
    "    steps: int\n",
    "    print_every: int\n",
    "    clear_every: int\n",
    "    memory_profile: bool\n",
    "    verbose: bool\n",
    "    do_jit: bool\n",
    "    opt_state: tuple\n",
    "    \n",
    "    def __init__(self, model, optim, loss, steps=50, print_every=1, clear_every=1, memory_profile=False, verbose=False, do_jit=True):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.loss = loss\n",
    "        self.steps = steps\n",
    "        self.print_every = print_every\n",
    "        self.clear_every = clear_every\n",
    "        self.memory_profile = memory_profile\n",
    "        self.verbose = verbose\n",
    "        self.do_jit = do_jit\n",
    "        self.opt_state = self.optim.init(eqx.filter(self.model, eqx.is_array)) \n",
    "    \n",
    "    # def __post_init__(self, attr, value):\n",
    "    #     object.__setattr__(self, attr, value)\n",
    "    \n",
    "    def clear_caches(self):\n",
    "        for module_name, module in sys.modules.items():\n",
    "            if module_name.startswith(\"jax\"):\n",
    "                if module_name not in [\"jax.interpreters.partial_eval\"]:\n",
    "                    for obj_name in dir(module):\n",
    "                        obj = getattr(module, obj_name)\n",
    "                        if hasattr(obj, \"cache_clear\"):\n",
    "                            try:\n",
    "                                obj.cache_clear()\n",
    "                            except:\n",
    "                                pass\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "    def vprint(self, output):\n",
    "        if self.verbose:\n",
    "            print(output)\n",
    "\n",
    "    def make_step(self, model, opt_state, *args):\n",
    "        self.vprint('loss_value, grads')\n",
    "        loss_value, grads = eqx.filter_value_and_grad(self.loss)(model, *args)\n",
    "        self.vprint('updates, opt_state')\n",
    "        updates, opt_state = self.optim.update(grads, opt_state, model)\n",
    "        self.vprint('model update')\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return model, opt_state, loss_value\n",
    "\n",
    "\n",
    "    def __call__(self, epoch_batch_len, model, *loss_input_lists):\n",
    "        \n",
    "        for step in range(self.steps):\n",
    "            print('Epoch {}'.format(step))\n",
    "            epoch_loss = 0\n",
    "            if step == 0 and self.do_jit:\n",
    "                fmake_step = eqx.filter_jit(self.make_step)\n",
    "            elif (step % self.clear_every) == 0 and (step > 0) and do_jit:\n",
    "                fmake_step = eqx.filter_jit(self.make_step)\n",
    "            else:\n",
    "                fmake_step = self.make_step\n",
    "            if step == 0:\n",
    "                inp_model = self.model\n",
    "                inp_opt_state = self.opt_state\n",
    "            for idx in range(epoch_batch_len):  \n",
    "                print('Epoch {} :: Batch {}/{}'.format(step, idx, epoch_batch_len))\n",
    "\n",
    "                #loops over every iterable in loss_input_lists, selecting one batch's input data\n",
    "                #assumes separate lists, each having inputs for multiple cases in the training set\n",
    "                loss_inputs = [inp[idx] for inp in loss_input_lists]\n",
    "                \n",
    "                this_loss = self.loss(inp_model, *loss_inputs).item()                \n",
    "                inp_model, inp_opt_state, train_loss = fmake_step(inp_model, inp_opt_state, *loss_inputs) \n",
    "\n",
    "                if self.memory_profile:\n",
    "                    e_pred.block_until_ready()\n",
    "                    jax.profiler.save_device_memory_profile(f\"memory{step}_{idx}.prof\")\n",
    "    \n",
    "                print('Batch Loss = {}'.format(this_loss))\n",
    "                epoch_loss += this_loss\n",
    "                if (step % self.clear_every) and (step > 0) == 0:\n",
    "                    fmake_step._clear_cache()\n",
    "                    equinox.clear_caches()\n",
    "                    jax.clear_backends()\n",
    "                    jax.clear_caches()\n",
    "                    self.clear_caches()\n",
    "                    self.loss.clear_cache()\n",
    "                    xla._xla_callable.cache_clear()\n",
    "    \n",
    "            if (step % self.print_every) == 0 or (step == self.steps - 1):\n",
    "                print(\n",
    "                    f\"{step=}, epoch_train_loss={epoch_loss}\"\n",
    "                )\n",
    "            if (step % self.clear_every) and (step > 0):\n",
    "                fmake_step._clear_cache()\n",
    "                equinox.clear_caches()\n",
    "                jax.clear_backends()\n",
    "                jax.clear_caches()\n",
    "                self.clear_caches()\n",
    "                self.loss.clear_cache()\n",
    "                xla._xla_callable.cache_clear()\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b8f9c5a8-788e-4ae6-8d4b-4c624d5a12a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(optax.adamw(1e-4).init(eqx.filter(xc, eqx.is_array)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "74a0fe88-101a-4fa2-bbe3-ff20de530f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = xcTrainer(model=xc, optim=optax.adamw(1e-4), loss = E_loss())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "f895e20f-a2fc-4af1-8eba-93460a41fc3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 0 :: Batch 0/1\n",
      "Batch Loss = 0.0003424572856989272\n",
      "step=0, epoch_train_loss=0.0003424572856989272\n",
      "Epoch 1\n",
      "Epoch 1 :: Batch 0/1\n",
      "Batch Loss = 0.0029619014513926345\n",
      "step=1, epoch_train_loss=0.0029619014513926345\n",
      "Epoch 2\n",
      "Epoch 2 :: Batch 0/1\n",
      "Batch Loss = 0.002782009913946837\n",
      "step=2, epoch_train_loss=0.002782009913946837\n",
      "Epoch 3\n",
      "Epoch 3 :: Batch 0/1\n",
      "Batch Loss = 0.0014471899868890858\n",
      "step=3, epoch_train_loss=0.0014471899868890858\n",
      "Epoch 4\n",
      "Epoch 4 :: Batch 0/1\n",
      "Batch Loss = 0.0004575789342489145\n",
      "step=4, epoch_train_loss=0.0004575789342489145\n",
      "Epoch 5\n",
      "Epoch 5 :: Batch 0/1\n",
      "Batch Loss = 0.0010923289809205983\n",
      "step=5, epoch_train_loss=0.0010923289809205983\n",
      "Epoch 6\n",
      "Epoch 6 :: Batch 0/1\n",
      "Batch Loss = 0.000889729042844678\n",
      "step=6, epoch_train_loss=0.000889729042844678\n",
      "Epoch 7\n",
      "Epoch 7 :: Batch 0/1\n",
      "Batch Loss = 9.479842606374689e-05\n",
      "step=7, epoch_train_loss=9.479842606374689e-05\n",
      "Epoch 8\n",
      "Epoch 8 :: Batch 0/1\n",
      "Batch Loss = 0.0011404489887336666\n",
      "step=8, epoch_train_loss=0.0011404489887336666\n",
      "Epoch 9\n",
      "Epoch 9 :: Batch 0/1\n",
      "Batch Loss = 0.0016352942373369928\n",
      "step=9, epoch_train_loss=0.0016352942373369928\n",
      "Epoch 10\n",
      "Epoch 10 :: Batch 0/1\n",
      "Batch Loss = 0.001546414068895885\n",
      "step=10, epoch_train_loss=0.001546414068895885\n",
      "Epoch 11\n",
      "Epoch 11 :: Batch 0/1\n",
      "Batch Loss = 0.0009888007206360072\n",
      "step=11, epoch_train_loss=0.0009888007206360072\n",
      "Epoch 12\n",
      "Epoch 12 :: Batch 0/1\n",
      "Batch Loss = 4.927930416265269e-05\n",
      "step=12, epoch_train_loss=4.927930416265269e-05\n",
      "Epoch 13\n",
      "Epoch 13 :: Batch 0/1\n",
      "Batch Loss = 0.001205056906991686\n",
      "step=13, epoch_train_loss=0.001205056906991686\n",
      "Epoch 14\n",
      "Epoch 14 :: Batch 0/1\n",
      "Batch Loss = 0.0018695984150696177\n",
      "step=14, epoch_train_loss=0.0018695984150696177\n",
      "Epoch 15\n",
      "Epoch 15 :: Batch 0/1\n",
      "Batch Loss = 0.0020367610966278704\n",
      "step=15, epoch_train_loss=0.0020367610966278704\n",
      "Epoch 16\n",
      "Epoch 16 :: Batch 0/1\n",
      "Batch Loss = 0.0017803523518669095\n",
      "step=16, epoch_train_loss=0.0017803523518669095\n",
      "Epoch 17\n",
      "Epoch 17 :: Batch 0/1\n",
      "Batch Loss = 0.0011601615058083325\n",
      "step=17, epoch_train_loss=0.0011601615058083325\n",
      "Epoch 18\n",
      "Epoch 18 :: Batch 0/1\n",
      "Batch Loss = 0.00022521359798055585\n",
      "step=18, epoch_train_loss=0.00022521359798055585\n",
      "Epoch 19\n",
      "Epoch 19 :: Batch 0/1\n",
      "Batch Loss = 0.0009838711795122634\n",
      "step=19, epoch_train_loss=0.0009838711795122634\n",
      "Epoch 20\n",
      "Epoch 20 :: Batch 0/1\n",
      "Batch Loss = 0.0016800162954364595\n",
      "step=20, epoch_train_loss=0.0016800162954364595\n",
      "Epoch 21\n",
      "Epoch 21 :: Batch 0/1\n",
      "Batch Loss = 0.001926683720835598\n",
      "step=21, epoch_train_loss=0.001926683720835598\n",
      "Epoch 22\n",
      "Epoch 22 :: Batch 0/1\n",
      "Batch Loss = 0.0017788684770163599\n",
      "step=22, epoch_train_loss=0.0017788684770163599\n",
      "Epoch 23\n",
      "Epoch 23 :: Batch 0/1\n",
      "Batch Loss = 0.001284400849170808\n",
      "step=23, epoch_train_loss=0.001284400849170808\n",
      "Epoch 24\n",
      "Epoch 24 :: Batch 0/1\n",
      "Batch Loss = 0.000485010393722618\n",
      "step=24, epoch_train_loss=0.000485010393722618\n",
      "Epoch 25\n",
      "Epoch 25 :: Batch 0/1\n",
      "Batch Loss = 0.0005827965258848877\n",
      "step=25, epoch_train_loss=0.0005827965258848877\n",
      "Epoch 26\n",
      "Epoch 26 :: Batch 0/1\n",
      "Batch Loss = 0.0011837981019144905\n",
      "step=26, epoch_train_loss=0.0011837981019144905\n",
      "Epoch 27\n",
      "Epoch 27 :: Batch 0/1\n",
      "Batch Loss = 0.001371706613646495\n",
      "step=27, epoch_train_loss=0.001371706613646495\n",
      "Epoch 28\n",
      "Epoch 28 :: Batch 0/1\n",
      "Batch Loss = 0.0011929089250291014\n",
      "step=28, epoch_train_loss=0.0011929089250291014\n",
      "Epoch 29\n",
      "Epoch 29 :: Batch 0/1\n",
      "Batch Loss = 0.0006876450086092945\n",
      "step=29, epoch_train_loss=0.0006876450086092945\n",
      "Epoch 30\n",
      "Epoch 30 :: Batch 0/1\n",
      "Batch Loss = 0.00010904220260776754\n",
      "step=30, epoch_train_loss=0.00010904220260776754\n",
      "Epoch 31\n",
      "Epoch 31 :: Batch 0/1\n",
      "Batch Loss = 0.0004804731363527992\n",
      "step=31, epoch_train_loss=0.0004804731363527992\n",
      "Epoch 32\n",
      "Epoch 32 :: Batch 0/1\n",
      "Batch Loss = 0.00047182152274061195\n",
      "step=32, epoch_train_loss=0.00047182152274061195\n",
      "Epoch 33\n",
      "Epoch 33 :: Batch 0/1\n",
      "Batch Loss = 0.00012368021305597665\n",
      "step=33, epoch_train_loss=0.00012368021305597665\n",
      "Epoch 34\n",
      "Epoch 34 :: Batch 0/1\n",
      "Batch Loss = 0.0005274857760699803\n",
      "step=34, epoch_train_loss=0.0005274857760699803\n",
      "Epoch 35\n",
      "Epoch 35 :: Batch 0/1\n",
      "Batch Loss = 0.0007739397816060034\n",
      "step=35, epoch_train_loss=0.0007739397816060034\n",
      "Epoch 36\n",
      "Epoch 36 :: Batch 0/1\n",
      "Batch Loss = 0.0006584802212383778\n",
      "step=36, epoch_train_loss=0.0006584802212383778\n",
      "Epoch 37\n",
      "Epoch 37 :: Batch 0/1\n",
      "Batch Loss = 0.00021872380239074118\n",
      "step=37, epoch_train_loss=0.00021872380239074118\n",
      "Epoch 38\n",
      "Epoch 38 :: Batch 0/1\n",
      "Batch Loss = 0.0005121848725320177\n",
      "step=38, epoch_train_loss=0.0005121848725320177\n",
      "Epoch 39\n",
      "Epoch 39 :: Batch 0/1\n",
      "Batch Loss = 0.0008336131070034725\n",
      "step=39, epoch_train_loss=0.0008336131070034725\n",
      "Epoch 40\n",
      "Epoch 40 :: Batch 0/1\n",
      "Batch Loss = 0.0007873500731534477\n",
      "step=40, epoch_train_loss=0.0007873500731534477\n",
      "Epoch 41\n",
      "Epoch 41 :: Batch 0/1\n",
      "Batch Loss = 0.00041128925717792697\n",
      "step=41, epoch_train_loss=0.00041128925717792697\n",
      "Epoch 42\n",
      "Epoch 42 :: Batch 0/1\n",
      "Batch Loss = 0.0002602506142803662\n",
      "step=42, epoch_train_loss=0.0002602506142803662\n",
      "Epoch 43\n",
      "Epoch 43 :: Batch 0/1\n",
      "Batch Loss = 0.0005306741619062194\n",
      "step=43, epoch_train_loss=0.0005306741619062194\n",
      "Epoch 44\n",
      "Epoch 44 :: Batch 0/1\n",
      "Batch Loss = 0.00044131617088893904\n",
      "step=44, epoch_train_loss=0.00044131617088893904\n",
      "Epoch 45\n",
      "Epoch 45 :: Batch 0/1\n",
      "Batch Loss = 2.870738040350318e-05\n",
      "step=45, epoch_train_loss=2.870738040350318e-05\n",
      "Epoch 46\n",
      "Epoch 46 :: Batch 0/1\n",
      "Batch Loss = 0.0006748058922099176\n",
      "step=46, epoch_train_loss=0.0006748058922099176\n",
      "Epoch 47\n",
      "Epoch 47 :: Batch 0/1\n",
      "Batch Loss = 0.000975359460955616\n",
      "step=47, epoch_train_loss=0.000975359460955616\n",
      "Epoch 48\n",
      "Epoch 48 :: Batch 0/1\n",
      "Batch Loss = 0.0009133719970879639\n",
      "step=48, epoch_train_loss=0.0009133719970879639\n",
      "Epoch 49\n",
      "Epoch 49 :: Batch 0/1\n",
      "Batch Loss = 0.0005256352359328531\n",
      "step=49, epoch_train_loss=0.0005256352359328531\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "eXC(\n",
       "  grid_models=[\n",
       "    eX(\n",
       "      n_input=2,\n",
       "      n_hidden=16,\n",
       "      ueg_limit=True,\n",
       "      spin_scaling=True,\n",
       "      lob=1.174,\n",
       "      use=[1, 2],\n",
       "      net=MLP(\n",
       "        layers=(\n",
       "          Linear(\n",
       "            weight=f64[16,2],\n",
       "            bias=f64[16],\n",
       "            in_features=2,\n",
       "            out_features=16,\n",
       "            use_bias=True\n",
       "          ),\n",
       "          Linear(\n",
       "            weight=f64[16,16],\n",
       "            bias=f64[16],\n",
       "            in_features=16,\n",
       "            out_features=16,\n",
       "            use_bias=True\n",
       "          ),\n",
       "          Linear(\n",
       "            weight=f64[16,16],\n",
       "            bias=f64[16],\n",
       "            in_features=16,\n",
       "            out_features=16,\n",
       "            use_bias=True\n",
       "          ),\n",
       "          Linear(\n",
       "            weight=f64[1,16],\n",
       "            bias=f64[1],\n",
       "            in_features=16,\n",
       "            out_features=1,\n",
       "            use_bias=True\n",
       "          )\n",
       "        ),\n",
       "        activation=<function gelu>,\n",
       "        final_activation=<function <lambda>>,\n",
       "        use_bias=True,\n",
       "        use_final_bias=True,\n",
       "        in_size=2,\n",
       "        out_size=1,\n",
       "        width_size=16,\n",
       "        depth=3\n",
       "      ),\n",
       "      tanh=<wrapped function tanh>,\n",
       "      lobf=LOB(limit=1.174, sig=<wrapped function sigmoid>),\n",
       "      sig=<wrapped function sigmoid>,\n",
       "      shift=f64[],\n",
       "      seed=92017,\n",
       "      depth=3\n",
       "    ),\n",
       "    eC(\n",
       "      n_input=4,\n",
       "      n_hidden=16,\n",
       "      ueg_limit=True,\n",
       "      spin_scaling=False,\n",
       "      lob=2.0,\n",
       "      use=[2, 3],\n",
       "      net=MLP(\n",
       "        layers=(\n",
       "          Linear(\n",
       "            weight=f64[16,4],\n",
       "            bias=f64[16],\n",
       "            in_features=4,\n",
       "            out_features=16,\n",
       "            use_bias=True\n",
       "          ),\n",
       "          Linear(\n",
       "            weight=f64[16,16],\n",
       "            bias=f64[16],\n",
       "            in_features=16,\n",
       "            out_features=16,\n",
       "            use_bias=True\n",
       "          ),\n",
       "          Linear(\n",
       "            weight=f64[16,16],\n",
       "            bias=f64[16],\n",
       "            in_features=16,\n",
       "            out_features=16,\n",
       "            use_bias=True\n",
       "          ),\n",
       "          Linear(\n",
       "            weight=f64[1,16],\n",
       "            bias=f64[1],\n",
       "            in_features=16,\n",
       "            out_features=1,\n",
       "            use_bias=True\n",
       "          )\n",
       "        ),\n",
       "        activation=<function gelu>,\n",
       "        final_activation=<wrapped function softplus>,\n",
       "        use_bias=True,\n",
       "        use_final_bias=True,\n",
       "        in_size=4,\n",
       "        out_size=1,\n",
       "        width_size=16,\n",
       "        depth=3\n",
       "      ),\n",
       "      tanh=<wrapped function tanh>,\n",
       "      lobf=LOB(limit=2.0, sig=<wrapped function sigmoid>),\n",
       "      sig=<wrapped function sigmoid>,\n",
       "      seed=92017,\n",
       "      depth=3\n",
       "    )\n",
       "  ],\n",
       "  heg_mult=True,\n",
       "  pw_mult=True,\n",
       "  level=3,\n",
       "  exx_a=0,\n",
       "  epsilon=1e-08,\n",
       "  loge=1e-05,\n",
       "  s_gam=1,\n",
       "  heg_model=LDA_X(),\n",
       "  pw_model=PW_C(),\n",
       "  model_mult=[1, 1],\n",
       "  debug=False\n",
       ")"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer(1, trainer.model, dms, energies, ao_evals, gws)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f324462e-ab8d-48da-9394-3846b7b3ac29",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9ce6e99c-5cff-47e9-ab5f-fad6da5cb87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func2(*args):\n",
    "    print([i for i in args])\n",
    "def test_func(*args, blen=1):\n",
    "    for idx in range(blen):\n",
    "        #loops over every iterable in args, selecting the idx for it\n",
    "        subls = [i[idx] for i in args]\n",
    "        func2(*subls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d07e1921-f9e0-4d6a-a534-85e675c77898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'd', 'g']\n",
      "['b', 'e', 'h']\n",
      "['c', 'f', 'i']\n"
     ]
    }
   ],
   "source": [
    "test_func(['a','b','c'], ['d','e','f'], ['g','h','i'], blen=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1b9f57ba-81f6-4bdb-93dd-7fa2d54c0869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_caches():\n",
    "    for module_name, module in sys.modules.items():\n",
    "        if module_name.startswith(\"jax\"):\n",
    "            if module_name not in [\"jax.interpreters.partial_eval\"]:\n",
    "                for obj_name in dir(module):\n",
    "                    obj = getattr(module, obj_name)\n",
    "                    if hasattr(obj, \"cache_clear\"):\n",
    "                        try:\n",
    "                            obj.cache_clear()\n",
    "                        except:\n",
    "                            pass\n",
    "    gc.collect()\n",
    "# chosen_loss = loop_e_loss\n",
    "# chosen_loss = total_loop_loss\n",
    "# chosen_loss = e_loss\n",
    "# chosen_loss = dm_loss\n",
    "chosen_loss = total_loss\n",
    "# @eqx.filter_jit\n",
    "do_jit = True\n",
    "\n",
    "def train(model: eqx.Module,\n",
    "    optim: optax.GradientTransformation,\n",
    "    steps: int,\n",
    "    print_every: int,\n",
    "    clear_every: int,\n",
    "    memory_profile: bool):\n",
    "    # Just like earlier: It only makes sense to train the arrays in our model,\n",
    "    # so filter out everything else.\n",
    "    opt_state = optim.init(eqx.filter(model, eqx.is_array)) \n",
    "    # Always wrap everything -- computing gradients, running the optimiser, updating\n",
    "    # the model -- into a single JIT region. This ensures things run as fast as\n",
    "    # possible.\n",
    "\n",
    "\n",
    "    def make_step(model, opt_state, inp_dm, ref_en, holos, ao_eval, grid_weights, eris, mo_occs, hcs, ss, ogd):\n",
    "        print('loss_value, grads')\n",
    "        # loss_value, grads = eqx.filter_value_and_grad(chosen_loss)(model, inp_dm, ref_en, ao_eval, grid_weights, eris, mo_occs, hcs, ss, ogd)\n",
    "        loss_value, grads = eqx.filter_value_and_grad(chosen_loss)(model, inp_dm, ref_en, holos, ao_eval, grid_weights, eris, mo_occs, hcs, ss, ogd)\n",
    "        print('updates, opt_state')\n",
    "        updates, opt_state = optim.update(grads, opt_state, model)\n",
    "        print('model update')\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return model, opt_state, loss_value\n",
    "    \n",
    "    for step in range(steps):\n",
    "        print('epoch {}'.format(step))\n",
    "        epoch_loss = 0\n",
    "        if step == 0 and do_jit:\n",
    "            fmake_step = eqx.filter_jit(make_step)\n",
    "        elif (step % clear_every) == 0 and (step > 0) and do_jit:\n",
    "            fmake_step = eqx.filter_jit(make_step)\n",
    "        else:\n",
    "            fmake_step = make_step\n",
    "        for idx in range(len(energies)):  \n",
    "            idx = len(energies)-idx-1\n",
    "            print('e {} mol {}/{}'.format(step, idx, len(energies)))\n",
    "            en = energies[idx]\n",
    "            dm = dms[idx]\n",
    "            ao = ao_evals[idx]\n",
    "            ogd = ogds[idx]\n",
    "            print(ao.shape)\n",
    "            gw = gws[idx]\n",
    "            eri = eris[idx]\n",
    "            mooc = mo_occs[idx]\n",
    "            hc = hcs[idx]\n",
    "            s = ss[idx]\n",
    "            holo = hologaps[idx]\n",
    "            e_pred = model(dm, ao, gw)\n",
    "            dmp, mo_e, mo_c = jax_dm(dm, eri, jax_exc_func(model, ao, gw), mooc, hc, s, ogd)\n",
    "            holo_pred = mo_e[mooc == 0][0] - mo_e[mooc > 1][-1]\n",
    "            print('e_pred - e_ref = {}'.format(e_pred-en))\n",
    "            print('dm_pred - dm sum = {}'.format((dmp-dm).sum()))\n",
    "            print('holo_pred - ref_holo = {}'.format(holo_pred-holo))\n",
    "            model, opt_state, train_loss = fmake_step(model, opt_state, dm, en, holo, ao, gw, eri, mooc, hc, s, ogd) \n",
    "            mol_loss = chosen_loss(model, dm, en, holo, ao, gw, eri, mooc, hc, s, ogd).item()\n",
    "            e_pred.block_until_ready()\n",
    "            if memory_profile:\n",
    "                jax.profiler.save_device_memory_profile(f\"memory{step}_{idx}.prof\")\n",
    "\n",
    "            print('mol loss = {}'.format(mol_loss))\n",
    "            epoch_loss += mol_loss\n",
    "            if (step % clear_every) and (step > 0) == 0:\n",
    "                jax_dm._clear_cache()\n",
    "                fmake_step._clear_cache()\n",
    "                equinox.clear_caches()\n",
    "                jax.clear_backends()\n",
    "                jax.clear_caches()\n",
    "                clear_caches()\n",
    "                chosen_loss.clear_cache()\n",
    "                xla._xla_callable.cache_clear()\n",
    "\n",
    "        if (step % print_every) == 0 or (step == steps - 1):\n",
    "            # current_loss = chosen_loss(model, dms, energies, ao_evals, gws, eris, mo_occs, hcs, ss).item()\n",
    "            # current_loss = chosen_loss(model, dm, en, ao, gw, eri, mooc, hc, s).item()\n",
    "            print(\n",
    "                f\"{step=}, epoch_train_loss={epoch_loss}\"\n",
    "                # f\"{step=}, train_loss={current_loss}\"\n",
    "            )\n",
    "        if (step % clear_every) and (step > 0) == 0:\n",
    "            fmake_step._clear_cache()\n",
    "            jax_dm._clear_cache()\n",
    "            equinox.clear_caches()\n",
    "            jax.clear_backends()\n",
    "            jax.clear_caches()\n",
    "            clear_caches()\n",
    "            chosen_loss.clear_cache()\n",
    "            xla._xla_callable.cache_clear()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e1674056-745f-4460-85d6-b5c508a2ef2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "e 0 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0003424572856989272\n",
      "dm_pred - dm sum = -0.09088607376748996\n",
      "holo_pred - ref_holo = -0.0001811382552374674\n",
      "loss_value, grads\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo Traced<ShapedArray(float64[])>with<JVPTrace(level=3/0)> with\n",
      "  primal = Traced<ShapedArray(float64[])>with<DynamicJaxprTrace(level=1/0)>\n",
      "  tangent = Traced<ShapedArray(float64[])>with<JaxprTrace(level=2/0)> with\n",
      "    pval = (ShapedArray(float64[]), None)\n",
      "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7faa7c131560>, in_tracers=(Traced<ShapedArray(float64[]):JaxprTrace(level=2/0)>, Traced<ShapedArray(float64[]):JaxprTrace(level=2/0)>), out_tracer_refs=[<weakref at 0x7faa7c1195d0; to 'JaxprTracer' at 0x7faa7c119580>], out_avals=[ShapedArray(float64[])], primitive=pjit, params={'jaxpr': { lambda ; a:f64[] b:f64[]. let c:f64[] = sub a b in (c,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'resource_env': None, 'donated_invars': (False, False), 'name': '<lambda>', 'keep_unused': False, 'inline': True}, effects=set(), source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x7faa77086570>, name_stack=NameStack(stack=(Transform(name='jvp'),))))\n",
      "updates, opt_state\n",
      "model update\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33373024696784953\n",
      "mol loss = 3.8563386670076305e-05\n",
      "step=0, epoch_train_loss=3.8563386670076305e-05\n",
      "epoch 1\n",
      "e 1 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0006626870521753148\n",
      "dm_pred - dm sum = -0.09136238452143158\n",
      "holo_pred - ref_holo = -3.8563386670076305e-05\n",
      "loss_value, grads\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo Traced<ShapedArray(float64[])>with<JVPTrace(level=3/0)> with\n",
      "  primal = Traced<ShapedArray(float64[])>with<DynamicJaxprTrace(level=1/0)>\n",
      "  tangent = Traced<ShapedArray(float64[])>with<JaxprTrace(level=2/0)> with\n",
      "    pval = (ShapedArray(float64[]), None)\n",
      "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7faa77499320>, in_tracers=(Traced<ShapedArray(float64[]):JaxprTrace(level=2/0)>, Traced<ShapedArray(float64[]):JaxprTrace(level=2/0)>), out_tracer_refs=[<weakref at 0x7faa7c21a250; to 'JaxprTracer' at 0x7faa7c21a200>], out_avals=[ShapedArray(float64[])], primitive=pjit, params={'jaxpr': { lambda ; a:f64[] b:f64[]. let c:f64[] = sub a b in (c,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'resource_env': None, 'donated_invars': (False, False), 'name': '<lambda>', 'keep_unused': False, 'inline': True}, effects=set(), source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x7faa7c24dc70>, name_stack=NameStack(stack=(Transform(name='jvp'),))))\n",
      "updates, opt_state\n",
      "model update\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3338730847861171\n",
      "mol loss = 0.00010427443159749528\n",
      "step=1, epoch_train_loss=0.00010427443159749528\n",
      "epoch 2\n",
      "e 2 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.000983479542476573\n",
      "dm_pred - dm sum = -0.09183652202908021\n",
      "holo_pred - ref_holo = 0.00010427443159749528\n",
      "loss_value, grads\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo Traced<ShapedArray(float64[])>with<JVPTrace(level=3/0)> with\n",
      "  primal = Traced<ShapedArray(float64[])>with<DynamicJaxprTrace(level=1/0)>\n",
      "  tangent = Traced<ShapedArray(float64[])>with<JaxprTrace(level=2/0)> with\n",
      "    pval = (ShapedArray(float64[]), None)\n",
      "    recipe = JaxprEqnRecipe(eqn_id=<object object at 0x7faa77499dd0>, in_tracers=(Traced<ShapedArray(float64[]):JaxprTrace(level=2/0)>, Traced<ShapedArray(float64[]):JaxprTrace(level=2/0)>), out_tracer_refs=[<weakref at 0x7faa7665a7f0; to 'JaxprTracer' at 0x7faa7665a7a0>], out_avals=[ShapedArray(float64[])], primitive=pjit, params={'jaxpr': { lambda ; a:f64[] b:f64[]. let c:f64[] = sub a b in (c,) }, 'in_shardings': (UnspecifiedValue, UnspecifiedValue), 'out_shardings': (UnspecifiedValue,), 'resource_env': None, 'donated_invars': (False, False), 'name': '<lambda>', 'keep_unused': False, 'inline': True}, effects=set(), source_info=SourceInfo(traceback=<jaxlib.xla_extension.Traceback object at 0x7faa766684f0>, name_stack=NameStack(stack=(Transform(name='jvp'),))))\n",
      "updates, opt_state\n",
      "model update\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33391035884560627\n",
      "mol loss = 0.00014154849108666268\n",
      "step=2, epoch_train_loss=0.00014154849108666268\n",
      "epoch 3\n",
      "e 3 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0010846424283545986\n",
      "dm_pred - dm sum = -0.09195927685479606\n",
      "holo_pred - ref_holo = 0.00014154849108666268\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33389513152075884\n",
      "mol loss = 0.0001263211662392294\n",
      "step=3, epoch_train_loss=0.0001263211662392294\n",
      "epoch 4\n",
      "e 4 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0010708263547503094\n",
      "dm_pred - dm sum = -0.09190960999110606\n",
      "holo_pred - ref_holo = 0.0001263211662392294\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3338486962989489\n",
      "mol loss = 7.988594442931429e-05\n",
      "step=4, epoch_train_loss=7.988594442931429e-05\n",
      "epoch 5\n",
      "e 5 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0009857086320668174\n",
      "dm_pred - dm sum = -0.09175768765837579\n",
      "holo_pred - ref_holo = 7.988594442931429e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337817192755009\n",
      "mol loss = 1.290892098126939e-05\n",
      "step=5, epoch_train_loss=1.290892098126939e-05\n",
      "epoch 6\n",
      "e 6 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0008517118811912638\n",
      "dm_pred - dm sum = -0.09153809147527434\n",
      "holo_pred - ref_holo = 1.290892098126939e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33370030255945643\n",
      "mol loss = 6.850779506317739e-05\n",
      "step=6, epoch_train_loss=6.850779506317739e-05\n",
      "epoch 7\n",
      "e 7 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0006818056936435113\n",
      "dm_pred - dm sum = -0.0912704407707776\n",
      "holo_pred - ref_holo = -6.850779506317739e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3336582231443018\n",
      "mol loss = 0.00011058721021778295\n",
      "step=7, epoch_train_loss=0.00011058721021778295\n",
      "epoch 8\n",
      "e 8 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0006015140588875312\n",
      "dm_pred - dm sum = -0.0911313179800083\n",
      "holo_pred - ref_holo = -0.00011058721021778295\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33364624188968456\n",
      "mol loss = 0.0001225684648350489\n",
      "step=8, epoch_train_loss=0.0001225684648350489\n",
      "epoch 9\n",
      "e 9 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0005903003156699782\n",
      "dm_pred - dm sum = -0.0910909359664959\n",
      "holo_pred - ref_holo = -0.0001225684648350489\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33365793515585185\n",
      "mol loss = 0.00011087519866775919\n",
      "step=9, epoch_train_loss=0.00011087519866775919\n",
      "epoch 10\n",
      "e 10 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.000633044322130516\n",
      "dm_pred - dm sum = -0.0911279951032106\n",
      "holo_pred - ref_holo = -0.00011087519866775919\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33368866067090186\n",
      "mol loss = 8.014968361774732e-05\n",
      "step=10, epoch_train_loss=8.014968361774732e-05\n",
      "epoch 11\n",
      "e 11 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0007184046368706731\n",
      "dm_pred - dm sum = -0.09122692578365349\n",
      "holo_pred - ref_holo = -8.014968361774732e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337349586261491\n",
      "mol loss = 3.385172837050421e-05\n",
      "step=11, epoch_train_loss=3.385172837050421e-05\n",
      "epoch 12\n",
      "e 12 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0008376949550861212\n",
      "dm_pred - dm sum = -0.09137613728995742\n",
      "holo_pred - ref_holo = -3.385172837050421e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33379418509146785\n",
      "mol loss = 2.53747369482471e-05\n",
      "step=12, epoch_train_loss=2.53747369482471e-05\n",
      "epoch 13\n",
      "e 13 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0009841335817029062\n",
      "dm_pred - dm sum = -0.09156682143481668\n",
      "holo_pred - ref_holo = 2.53747369482471e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3338272494774551\n",
      "mol loss = 5.843912293551634e-05\n",
      "step=13, epoch_train_loss=5.843912293551634e-05\n",
      "epoch 14\n",
      "e 14 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0010714826676991862\n",
      "dm_pred - dm sum = -0.09167519623751909\n",
      "holo_pred - ref_holo = 5.843912293551634e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3338381715378346\n",
      "mol loss = 6.936118331501984e-05\n",
      "step=14, epoch_train_loss=6.936118331501984e-05\n",
      "epoch 15\n",
      "e 15 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0011094018851327547\n",
      "dm_pred - dm sum = -0.09171379517835299\n",
      "holo_pred - ref_holo = 6.936118331501984e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3338302240489397\n",
      "mol loss = 6.141369442008715e-05\n",
      "step=15, epoch_train_loss=6.141369442008715e-05\n",
      "epoch 16\n",
      "e 16 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0011052135609634917\n",
      "dm_pred - dm sum = -0.09169265205452917\n",
      "holo_pred - ref_holo = 6.141369442008715e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3338061026378963\n",
      "mol loss = 3.729228337667756e-05\n",
      "step=16, epoch_train_loss=3.729228337667756e-05\n",
      "epoch 17\n",
      "e 17 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0010646096133317684\n",
      "dm_pred - dm sum = -0.09161992640440839\n",
      "holo_pred - ref_holo = 3.729228337667756e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33376804966932994\n",
      "mol loss = 7.606851896713174e-07\n",
      "step=17, epoch_train_loss=7.606851896713174e-07\n",
      "epoch 18\n",
      "e 18 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0009921013482916408\n",
      "dm_pred - dm sum = -0.09150233523267917\n",
      "holo_pred - ref_holo = -7.606851896713174e-07\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337508848930727\n",
      "mol loss = 1.792546144691043e-05\n",
      "step=18, epoch_train_loss=1.792546144691043e-05\n",
      "epoch 19\n",
      "e 19 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0009646385426105297\n",
      "dm_pred - dm sum = -0.09145069211697518\n",
      "holo_pred - ref_holo = -1.792546144691043e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.333751891554196\n",
      "mol loss = 1.691880032361759e-05\n",
      "step=19, epoch_train_loss=1.691880032361759e-05\n",
      "epoch 20\n",
      "e 20 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0009767071738355781\n",
      "dm_pred - dm sum = -0.09145672868052421\n",
      "holo_pred - ref_holo = -1.691880032361759e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337687801069733\n",
      "mol loss = 3.024754630098769e-08\n",
      "step=20, epoch_train_loss=3.024754630098769e-08\n",
      "epoch 21\n",
      "e 21 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0010232839749857447\n",
      "dm_pred - dm sum = -0.09151327320778663\n",
      "holo_pred - ref_holo = -3.024754630098769e-08\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337996070447111\n",
      "mol loss = 3.079669019151465e-05\n",
      "step=21, epoch_train_loss=3.079669019151465e-05\n",
      "epoch 22\n",
      "e 22 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0010998318155603926\n",
      "dm_pred - dm sum = -0.09161410123851829\n",
      "holo_pred - ref_holo = 3.079669019151465e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3338114106567477\n",
      "mol loss = 4.2600302228079645e-05\n",
      "step=22, epoch_train_loss=4.2600302228079645e-05\n",
      "epoch 23\n",
      "e 23 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0011338904706814645\n",
      "dm_pred - dm sum = -0.09165422120311555\n",
      "holo_pred - ref_holo = 4.2600302228079645e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33380643364445844\n",
      "mol loss = 3.7623289938837345e-05\n",
      "step=23, epoch_train_loss=3.7623289938837345e-05\n",
      "epoch 24\n",
      "e 24 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.001130617013560098\n",
      "dm_pred - dm sum = -0.09164077469669793\n",
      "holo_pred - ref_holo = 3.7623289938837345e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337866336179066\n",
      "mol loss = 1.7823263387006794e-05\n",
      "step=24, epoch_train_loss=1.7823263387006794e-05\n",
      "epoch 25\n",
      "e 25 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.001094207911545908\n",
      "dm_pred - dm sum = -0.09157986187501967\n",
      "holo_pred - ref_holo = 1.7823263387006794e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33375372438523543\n",
      "mol loss = 1.5085969284178713e-05\n",
      "step=25, epoch_train_loss=1.5085969284178713e-05\n",
      "epoch 26\n",
      "e 26 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0010281100575628699\n",
      "dm_pred - dm sum = -0.09147672597346154\n",
      "holo_pred - ref_holo = -1.5085969284178713e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33373943589292177\n",
      "mol loss = 2.937446159784196e-05\n",
      "step=26, epoch_train_loss=2.937446159784196e-05\n",
      "epoch 27\n",
      "e 27 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0010029298332607794\n",
      "dm_pred - dm sum = -0.09143292788511699\n",
      "holo_pred - ref_holo = -2.937446159784196e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337416765733561\n",
      "mol loss = 2.7133781163479753e-05\n",
      "step=27, epoch_train_loss=2.7133781163479753e-05\n",
      "epoch 28\n",
      "e 28 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0010143205098920305\n",
      "dm_pred - dm sum = -0.09144198282388173\n",
      "holo_pred - ref_holo = -2.7133781163479753e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337586218282822\n",
      "mol loss = 1.0188526237409068e-05\n",
      "step=28, epoch_train_loss=1.0188526237409068e-05\n",
      "epoch 29\n",
      "e 29 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0010581796926274478\n",
      "dm_pred - dm sum = -0.09149808376714397\n",
      "holo_pred - ref_holo = -1.0188526237409068e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33378867471153784\n",
      "mol loss = 1.9864357018228862e-05\n",
      "step=29, epoch_train_loss=1.9864357018228862e-05\n",
      "epoch 30\n",
      "e 30 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0011306738109961856\n",
      "dm_pred - dm sum = -0.09159603916568501\n",
      "holo_pred - ref_holo = 1.9864357018228862e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3338007926854354\n",
      "mol loss = 3.198233091578073e-05\n",
      "step=30, epoch_train_loss=3.198233091578073e-05\n",
      "epoch 31\n",
      "e 31 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0011631567507688345\n",
      "dm_pred - dm sum = -0.09163656022577282\n",
      "holo_pred - ref_holo = 3.198233091578073e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33379689550085184\n",
      "mol loss = 2.8085146332235134e-05\n",
      "step=31, epoch_train_loss=2.8085146332235134e-05\n",
      "epoch 32\n",
      "e 32 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0011600873125541256\n",
      "dm_pred - dm sum = -0.09162582791597046\n",
      "holo_pred - ref_holo = 2.8085146332235134e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337786963675847\n",
      "mol loss = 9.886013065119048e-06\n",
      "step=32, epoch_train_loss=9.886013065119048e-06\n",
      "epoch 33\n",
      "e 33 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0011251579671114342\n",
      "dm_pred - dm sum = -0.09156923391283586\n",
      "holo_pred - ref_holo = 9.886013065119048e-06\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337477251164308\n",
      "mol loss = 2.1085238088802694e-05\n",
      "step=33, epoch_train_loss=2.1085238088802694e-05\n",
      "epoch 34\n",
      "e 34 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.001061441301898114\n",
      "dm_pred - dm sum = -0.09147149796620019\n",
      "holo_pred - ref_holo = -2.1085238088802694e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337345366176991\n",
      "mol loss = 3.4273736820522593e-05\n",
      "step=34, epoch_train_loss=3.4273736820522593e-05\n",
      "epoch 35\n",
      "e 35 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0010370637386554193\n",
      "dm_pred - dm sum = -0.09143064165402788\n",
      "holo_pred - ref_holo = -3.4273736820522593e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.333737255310934\n",
      "mol loss = 3.155504358559602e-05\n",
      "step=35, epoch_train_loss=3.155504358559602e-05\n",
      "epoch 36\n",
      "e 36 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0010480982211955592\n",
      "dm_pred - dm sum = -0.09144080350174594\n",
      "holo_pred - ref_holo = -3.155504358559602e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33375422232096413\n",
      "mol loss = 1.4588033555473778e-05\n",
      "step=36, epoch_train_loss=1.4588033555473778e-05\n",
      "epoch 37\n",
      "e 37 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0010907829559361204\n",
      "dm_pred - dm sum = -0.09149666703760322\n",
      "holo_pred - ref_holo = -1.4588033555473778e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337839677486919\n",
      "mol loss = 1.5157394172293781e-05\n",
      "step=37, epoch_train_loss=1.5157394172293781e-05\n",
      "epoch 38\n",
      "e 38 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0011615504001181165\n",
      "dm_pred - dm sum = -0.09159342305633736\n",
      "holo_pred - ref_holo = 1.5157394172293781e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33379621081818606\n",
      "mol loss = 2.7400463666449326e-05\n",
      "step=38, epoch_train_loss=2.7400463666449326e-05\n",
      "epoch 39\n",
      "e 39 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0011933186467576462\n",
      "dm_pred - dm sum = -0.09163405439694858\n",
      "holo_pred - ref_holo = 2.7400463666449326e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337927486514982\n",
      "mol loss = 2.3938296978609497e-05\n",
      "step=39, epoch_train_loss=2.3938296978609497e-05\n",
      "epoch 40\n",
      "e 40 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0011902783036905618\n",
      "dm_pred - dm sum = -0.09162437559681498\n",
      "holo_pred - ref_holo = 2.3938296978609497e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337751997515036\n",
      "mol loss = 6.389396983996765e-06\n",
      "step=40, epoch_train_loss=6.389396983996765e-06\n",
      "epoch 41\n",
      "e 41 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0011559249898152046\n",
      "dm_pred - dm sum = -0.09156950496399538\n",
      "holo_pred - ref_holo = 6.389396983996765e-06\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33374502131736644\n",
      "mol loss = 2.3789037153165715e-05\n",
      "step=41, epoch_train_loss=2.3789037153165715e-05\n",
      "epoch 42\n",
      "e 42 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.001093181391757625\n",
      "dm_pred - dm sum = -0.09147395848266568\n",
      "holo_pred - ref_holo = -2.3789037153165715e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33373228520110354\n",
      "mol loss = 3.65251534160671e-05\n",
      "step=42, epoch_train_loss=3.65251534160671e-05\n",
      "epoch 43\n",
      "e 43 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.001069111353469765\n",
      "dm_pred - dm sum = -0.09143428573087789\n",
      "holo_pred - ref_holo = -3.65251534160671e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33373520092974573\n",
      "mol loss = 3.360942477387274e-05\n",
      "step=43, epoch_train_loss=3.360942477387274e-05\n",
      "epoch 44\n",
      "e 44 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0010799580942428832\n",
      "dm_pred - dm sum = -0.09144486846865166\n",
      "holo_pred - ref_holo = -3.360942477387274e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33375217572538984\n",
      "mol loss = 1.6634629129763656e-05\n",
      "step=44, epoch_train_loss=1.6634629129763656e-05\n",
      "epoch 45\n",
      "e 45 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0011221000039114415\n",
      "dm_pred - dm sum = -0.09150058647237862\n",
      "holo_pred - ref_holo = -1.6634629129763656e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337817909526729\n",
      "mol loss = 1.2980598153300438e-05\n",
      "step=45, epoch_train_loss=1.2980598153300438e-05\n",
      "epoch 46\n",
      "e 46 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0011920793536894791\n",
      "dm_pred - dm sum = -0.0915967847738332\n",
      "holo_pred - ref_holo = 1.2980598153300438e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.333794085310228\n",
      "mol loss = 2.5274955708365354e-05\n",
      "step=46, epoch_train_loss=2.5274955708365354e-05\n",
      "epoch 47\n",
      "e 47 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0012235052052727013\n",
      "dm_pred - dm sum = -0.09163742161031421\n",
      "holo_pred - ref_holo = 2.5274955708365354e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337908057011271\n",
      "mol loss = 2.199534660751734e-05\n",
      "step=47, epoch_train_loss=2.199534660751734e-05\n",
      "epoch 48\n",
      "e 48 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0012204573640826055\n",
      "dm_pred - dm sum = -0.0916281617731027\n",
      "holo_pred - ref_holo = 2.199534660751734e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337735315065062\n",
      "mol loss = 4.7211519865753004e-06\n",
      "step=48, epoch_train_loss=4.7211519865753004e-06\n",
      "epoch 49\n",
      "e 49 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0011863495936221824\n",
      "dm_pred - dm sum = -0.09157401258136626\n",
      "holo_pred - ref_holo = 4.7211519865753004e-06\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33374368968951623\n",
      "mol loss = 2.5120665003375287e-05\n",
      "step=49, epoch_train_loss=2.5120665003375287e-05\n",
      "epoch 50\n",
      "e 50 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.001124040645093416\n",
      "dm_pred - dm sum = -0.09147940703887712\n",
      "holo_pred - ref_holo = -2.5120665003375287e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337311457440679\n",
      "mol loss = 3.766461045168201e-05\n",
      "step=50, epoch_train_loss=3.766461045168201e-05\n",
      "epoch 51\n",
      "e 51 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0011001013457185849\n",
      "dm_pred - dm sum = -0.09144023014073442\n",
      "holo_pred - ref_holo = -3.766461045168201e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33373414459225137\n",
      "mol loss = 3.466576226823781e-05\n",
      "step=51, epoch_train_loss=3.466576226823781e-05\n",
      "epoch 52\n",
      "e 52 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0011108488655668936\n",
      "dm_pred - dm sum = -0.0914509642092522\n",
      "holo_pred - ref_holo = -3.466576226823781e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33375112110913935\n",
      "mol loss = 1.7689245380259067e-05\n",
      "step=52, epoch_train_loss=1.7689245380259067e-05\n",
      "epoch 53\n",
      "e 53 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0011527232745986993\n",
      "dm_pred - dm sum = -0.09150657129150266\n",
      "holo_pred - ref_holo = -1.7689245380259067e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337806782325049\n",
      "mol loss = 1.1867877985294317e-05\n",
      "step=53, epoch_train_loss=1.1867877985294317e-05\n",
      "epoch 54\n",
      "e 54 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0012223148678103257\n",
      "dm_pred - dm sum = -0.09160246290511251\n",
      "holo_pred - ref_holo = 1.1867877985294317e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337929932255865\n",
      "mol loss = 2.4182871066869005e-05\n",
      "step=54, epoch_train_loss=2.4182871066869005e-05\n",
      "epoch 55\n",
      "e 55 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0012535661153734168\n",
      "dm_pred - dm sum = -0.0916430615566158\n",
      "holo_pred - ref_holo = 2.4182871066869005e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33378979169011364\n",
      "mol loss = 2.098133559402804e-05\n",
      "step=55, epoch_train_loss=2.098133559402804e-05\n",
      "epoch 56\n",
      "e 56 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0012505089141878756\n",
      "dm_pred - dm sum = -0.09163396754556485\n",
      "holo_pred - ref_holo = 2.098133559402804e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33377263633380394\n",
      "mol loss = 3.8259792843331475e-06\n",
      "step=56, epoch_train_loss=3.8259792843331475e-06\n",
      "epoch 57\n",
      "e 57 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.001216521557001471\n",
      "dm_pred - dm sum = -0.09158014106530338\n",
      "holo_pred - ref_holo = 3.8259792843331475e-06\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337429411822443\n",
      "mol loss = 2.586917227531682e-05\n",
      "step=57, epoch_train_loss=2.586917227531682e-05\n",
      "epoch 58\n",
      "e 58 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0011544340611617798\n",
      "dm_pred - dm sum = -0.09148598039441648\n",
      "holo_pred - ref_holo = -2.586917227531682e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337304806198421\n",
      "mol loss = 3.8329734677489125e-05\n",
      "step=58, epoch_train_loss=3.8329734677489125e-05\n",
      "epoch 59\n",
      "e 59 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0011305612323617709\n",
      "dm_pred - dm sum = -0.09144702516907771\n",
      "holo_pred - ref_holo = -3.8329734677489125e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337335147116448\n",
      "mol loss = 3.529564287479969e-05\n",
      "step=59, epoch_train_loss=3.529564287479969e-05\n",
      "epoch 60\n",
      "e 60 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0011412546030005188\n",
      "dm_pred - dm sum = -0.0914578013353867\n",
      "holo_pred - ref_holo = -3.529564287479969e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33375049020704994\n",
      "mol loss = 1.8320147469663706e-05\n",
      "step=60, epoch_train_loss=1.8320147469663706e-05\n",
      "epoch 61\n",
      "e 61 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0011829834882579604\n",
      "dm_pred - dm sum = -0.0915133082680584\n",
      "holo_pred - ref_holo = -1.8320147469663706e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337800193505657\n",
      "mol loss = 1.1208996046119246e-05\n",
      "step=61, epoch_train_loss=1.1208996046119246e-05\n",
      "epoch 62\n",
      "e 62 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.001252360755154669\n",
      "dm_pred - dm sum = -0.09160898783961574\n",
      "holo_pred - ref_holo = 1.1208996046119246e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337923418931961\n",
      "mol loss = 2.3531538676513364e-05\n",
      "step=62, epoch_train_loss=2.3531538676513364e-05\n",
      "epoch 63\n",
      "e 63 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0012835133851272218\n",
      "dm_pred - dm sum = -0.09164952798964615\n",
      "holo_pred - ref_holo = 2.3531538676513364e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337891741792829\n",
      "mol loss = 2.0363824763280913e-05\n",
      "step=63, epoch_train_loss=2.0363824763280913e-05\n",
      "epoch 64\n",
      "e 64 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.001280451287684059\n",
      "dm_pred - dm sum = -0.09164049879737698\n",
      "holo_pred - ref_holo = 2.0363824763280913e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33377207163569017\n",
      "mol loss = 3.261281170563457e-06\n",
      "step=64, epoch_train_loss=3.261281170563457e-06\n",
      "epoch 65\n",
      "e 65 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0012465364606040197\n",
      "dm_pred - dm sum = -0.09158683865325898\n",
      "holo_pred - ref_holo = 3.261281170563457e-06\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337424426167895\n",
      "mol loss = 2.636773773012635e-05\n",
      "step=65, epoch_train_loss=2.636773773012635e-05\n",
      "epoch 66\n",
      "e 66 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0011845849760323546\n",
      "dm_pred - dm sum = -0.09149292984522345\n",
      "holo_pred - ref_holo = -2.636773773012635e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33373001920685835\n",
      "mol loss = 3.879114766125458e-05\n",
      "step=66, epoch_train_loss=3.879114766125458e-05\n",
      "epoch 67\n",
      "e 67 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0011607552794625775\n",
      "dm_pred - dm sum = -0.09145408894018345\n",
      "holo_pred - ref_holo = -3.879114766125458e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337330681110091\n",
      "mol loss = 3.574224351049482e-05\n",
      "step=67, epoch_train_loss=3.574224351049482e-05\n",
      "epoch 68\n",
      "e 68 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0011714165483116545\n",
      "dm_pred - dm sum = -0.0914648632704161\n",
      "holo_pred - ref_holo = -3.574224351049482e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337500413082575\n",
      "mol loss = 1.8769046262134825e-05\n",
      "step=68, epoch_train_loss=1.8769046262134825e-05\n",
      "epoch 69\n",
      "e 69 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0012130536254115754\n",
      "dm_pred - dm sum = -0.09152027102812997\n",
      "holo_pred - ref_holo = -1.8769046262134825e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33377955516848296\n",
      "mol loss = 1.0744813963348143e-05\n",
      "step=69, epoch_train_loss=1.0744813963348143e-05\n",
      "epoch 70\n",
      "e 70 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0012822913335615027\n",
      "dm_pred - dm sum = -0.09161577043395427\n",
      "holo_pred - ref_holo = 1.0744813963348143e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33379187959588674\n",
      "mol loss = 2.3069241367135174e-05\n",
      "step=70, epoch_train_loss=2.3069241367135174e-05\n",
      "epoch 71\n",
      "e 71 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0013133792923660792\n",
      "dm_pred - dm sum = -0.09165624163078609\n",
      "holo_pred - ref_holo = 2.3069241367135174e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33378872678211885\n",
      "mol loss = 1.9916427599242503e-05\n",
      "step=71, epoch_train_loss=1.9916427599242503e-05\n",
      "epoch 72\n",
      "e 72 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.001310316128492417\n",
      "dm_pred - dm sum = -0.09164723825720314\n",
      "holo_pred - ref_holo = 1.9916427599242503e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33377164886065125\n",
      "mol loss = 2.838506131641605e-06\n",
      "step=72, epoch_train_loss=2.838506131641605e-06\n",
      "epoch 73\n",
      "e 73 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0012764554569404396\n",
      "dm_pred - dm sum = -0.09159368660113058\n",
      "holo_pred - ref_holo = 2.838506131641605e-06\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33374205160233855\n",
      "mol loss = 2.675875218105528e-05\n",
      "step=73, epoch_train_loss=2.675875218105528e-05\n",
      "epoch 74\n",
      "e 74 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0012146056573758557\n",
      "dm_pred - dm sum = -0.09149995977250142\n",
      "holo_pred - ref_holo = -2.675875218105528e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33372964557601337\n",
      "mol loss = 3.9164778506239895e-05\n",
      "step=74, epoch_train_loss=3.9164778506239895e-05\n",
      "epoch 75\n",
      "e 75 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.001190810650635754\n",
      "dm_pred - dm sum = -0.09146119406288675\n",
      "holo_pred - ref_holo = -3.9164778506239895e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337327005149276\n",
      "mol loss = 3.6109839592013415e-05\n",
      "step=75, epoch_train_loss=3.6109839592013415e-05\n",
      "epoch 76\n",
      "e 76 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0012014501054302684\n",
      "dm_pred - dm sum = -0.09147194917464366\n",
      "holo_pred - ref_holo = -3.6109839592013415e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33374967080151313\n",
      "mol loss = 1.9139553006475296e-05\n",
      "step=76, epoch_train_loss=1.9139553006475296e-05\n",
      "epoch 77\n",
      "e 77 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0012430185213592182\n",
      "dm_pred - dm sum = -0.09152725540872218\n",
      "holo_pred - ref_holo = -1.9139553006475296e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337791747234158\n",
      "mol loss = 1.0364368896176313e-05\n",
      "step=77, epoch_train_loss=1.0364368896176313e-05\n",
      "epoch 78\n",
      "e 78 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.001312148258593382\n",
      "dm_pred - dm sum = -0.09162258177657151\n",
      "holo_pred - ref_holo = 1.0364368896176313e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33379149855949125\n",
      "mol loss = 2.26882049716437e-05\n",
      "step=78, epoch_train_loss=2.26882049716437e-05\n",
      "epoch 79\n",
      "e 79 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0013431862855082244\n",
      "dm_pred - dm sum = -0.09166297823430433\n",
      "holo_pred - ref_holo = 2.26882049716437e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337883525266544\n",
      "mol loss = 1.9542172134789926e-05\n",
      "step=79, epoch_train_loss=1.9542172134789926e-05\n",
      "epoch 80\n",
      "e 80 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0013401242810004987\n",
      "dm_pred - dm sum = -0.09165398681141293\n",
      "holo_pred - ref_holo = 1.9542172134789926e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33377128716655036\n",
      "mol loss = 2.4768120307494534e-06\n",
      "step=80, epoch_train_loss=2.4768120307494534e-06\n",
      "epoch 81\n",
      "e 81 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0013063106645692102\n",
      "dm_pred - dm sum = -0.09160052515299133\n",
      "holo_pred - ref_holo = 2.4768120307494534e-06\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33374170698281563\n",
      "mol loss = 2.71033717039737e-05\n",
      "step=81, epoch_train_loss=2.71033717039737e-05\n",
      "epoch 82\n",
      "e 82 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.001244548754248953\n",
      "dm_pred - dm sum = -0.09150695975260216\n",
      "holo_pred - ref_holo = -2.71033717039737e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33372930987527455\n",
      "mol loss = 3.950047924505817e-05\n",
      "step=82, epoch_train_loss=3.950047924505817e-05\n",
      "epoch 83\n",
      "e 83 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0012207853401164925\n",
      "dm_pred - dm sum = -0.09146825726935678\n",
      "holo_pred - ref_holo = -3.950047924505817e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337323670662105\n",
      "mol loss = 3.6443288309095934e-05\n",
      "step=83, epoch_train_loss=3.6443288309095934e-05\n",
      "epoch 84\n",
      "e 84 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0012314074904100636\n",
      "dm_pred - dm sum = -0.09147898679556671\n",
      "holo_pred - ref_holo = -3.6443288309095934e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33374933413654484\n",
      "mol loss = 1.9476217974767618e-05\n",
      "step=84, epoch_train_loss=1.9476217974767618e-05\n",
      "epoch 85\n",
      "e 85 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0012729168232894494\n",
      "dm_pred - dm sum = -0.09153418828059051\n",
      "holo_pred - ref_holo = -1.9476217974767618e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33377883034909667\n",
      "mol loss = 1.0019994577059599e-05\n",
      "step=85, epoch_train_loss=1.0019994577059599e-05\n",
      "epoch 86\n",
      "e 86 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0013419514215691208\n",
      "dm_pred - dm sum = -0.09162933952628988\n",
      "holo_pred - ref_holo = 1.0019994577059599e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337911525000805\n",
      "mol loss = 2.234214556090519e-05\n",
      "step=86, epoch_train_loss=2.234214556090519e-05\n",
      "epoch 87\n",
      "e 87 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0013729456102300475\n",
      "dm_pred - dm sum = -0.09166965757197804\n",
      "holo_pred - ref_holo = 2.234214556090519e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337880097622087\n",
      "mol loss = 1.9199407689074466e-05\n",
      "step=87, epoch_train_loss=1.9199407689074466e-05\n",
      "epoch 88\n",
      "e 88 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0013698857928670094\n",
      "dm_pred - dm sum = -0.09166067422867381\n",
      "holo_pred - ref_holo = 1.9199407689074466e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337709518116055\n",
      "mol loss = 2.141457085880205e-06\n",
      "step=88, epoch_train_loss=2.141457085880205e-06\n",
      "epoch 89\n",
      "e 89 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0013361164057634767\n",
      "dm_pred - dm sum = -0.09160729935051073\n",
      "holo_pred - ref_holo = 2.141457085880205e-06\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33374138244488566\n",
      "mol loss = 2.7427909633948033e-05\n",
      "step=89, epoch_train_loss=2.7427909633948033e-05\n",
      "epoch 90\n",
      "e 90 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0012744367960078051\n",
      "dm_pred - dm sum = -0.0915138934882288\n",
      "holo_pred - ref_holo = -2.7427909633948033e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33372899063807315\n",
      "mol loss = 3.981971644645421e-05\n",
      "step=90, epoch_train_loss=3.981971644645421e-05\n",
      "epoch 91\n",
      "e 91 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.001250703751651372\n",
      "dm_pred - dm sum = -0.09147525245455823\n",
      "holo_pred - ref_holo = -3.981971644645421e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.33373204844800497\n",
      "mol loss = 3.67619065146374e-05\n",
      "step=91, epoch_train_loss=3.67619065146374e-05\n",
      "epoch 92\n",
      "e 92 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.001261310385105574\n",
      "dm_pred - dm sum = -0.09148595436533\n",
      "holo_pred - ref_holo = -3.67619065146374e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337490121415806\n",
      "mol loss = 1.979821293901507e-05\n",
      "step=92, epoch_train_loss=1.979821293901507e-05\n",
      "epoch 93\n",
      "e 93 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0013027643174439163\n",
      "dm_pred - dm sum = -0.09154104784125575\n",
      "holo_pred - ref_holo = -1.979821293901507e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337785015550837\n",
      "mol loss = 9.691200564099223e-06\n",
      "step=93, epoch_train_loss=9.691200564099223e-06\n",
      "epoch 94\n",
      "e 94 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0013717086276496815\n",
      "dm_pred - dm sum = -0.09163601929580914\n",
      "holo_pred - ref_holo = 9.691200564099223e-06\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337908215304997\n",
      "mol loss = 2.2011175980107822e-05\n",
      "step=94, epoch_train_loss=2.2011175980107822e-05\n",
      "epoch 95\n",
      "e 95 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0014026612844251218\n",
      "dm_pred - dm sum = -0.09167625620536596\n",
      "holo_pred - ref_holo = 2.2011175980107822e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337876805931185\n",
      "mol loss = 1.887023859886705e-05\n",
      "step=95, epoch_train_loss=1.887023859886705e-05\n",
      "epoch 96\n",
      "e 96 mol 0/1\n",
      "(10, 25728, 74)\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "e_pred - e_ref = -0.0013996039931498672\n",
      "dm_pred - dm sum = -0.09166728078108571\n",
      "holo_pred - ref_holo = 1.887023859886705e-05\n",
      "[74] (74,)\n",
      "(74, 74) (74, 74)\n",
      "Spin unpolarized make_rdm1()\n",
      "pred_holo 0.3337706278625369\n",
      "mol loss = 1.8175080173099545e-06\n",
      "step=96, epoch_train_loss=1.8175080173099545e-06\n",
      "epoch 97\n",
      "e 97 mol 0/1\n",
      "(10, 25728, 74)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = train(xc, optax.adamw(1e-4), steps=250, print_every=1, clear_every=1, memory_profile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "93e955aa-529a-4a0c-8583-52a9c45197ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.] [-14.26603092 -14.26463934  -1.09180816  -0.52450794  -0.4397367\n",
      "  -0.4397367   -0.39399782  -0.06022901  -0.06022901   0.08870731\n",
      "   0.14128726   0.27233178   0.28895819   0.28898985   0.30133212\n",
      "   0.30133212   0.35393616   0.35393616   0.39494556   0.4825326\n",
      "   0.48267082   0.48952567   0.48952567   0.51522962   0.53951392\n",
      "   0.72012031   0.7208078    0.80602798   0.80602798   1.08150586\n",
      "   1.10496897   1.10504532   1.25532858   1.25532858   1.48594543\n",
      "   1.4861282    1.54890009   1.72721476   1.72721476   1.9802559\n",
      "   2.00327286   2.00327286   2.19298531   2.50156108   2.57656842\n",
      "   2.57656842   2.72569718   3.01045198   3.5792937    3.57929581\n",
      "   3.70628862   3.70628862   3.78633564   3.78633564   4.20792428\n",
      "   4.20792428   4.23703183   4.60767191   4.60781699   4.62271735\n",
      "   4.62271735   4.72966943   4.72977228   5.07072767   5.07084876\n",
      "   5.16331031   5.16331031   5.18905758   6.29446251   6.475644\n",
      "   6.475644     6.81278876  31.65904887  32.40819947]\n"
     ]
    }
   ],
   "source": [
    "mocc = mo_occs[0]\n",
    "moe = mf.mo_energy\n",
    "print(mocc, moe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "70d81aa9-2ff2-46e2-bea0-848b5f1a8ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "homo_i = jnp.max(jnp.nonzero(mocc, size=ogds[0][0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1e562c22-d29b-415f-80d1-d6287c5e64ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(-0.39399782, dtype=float64)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moe[homo_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9c87e5-a33b-4706-905b-e50d7113133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ao_eval.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169d6c18-6c41-49fd-b8f1-e8bf494602f4",
   "metadata": {},
   "source": [
    "Create silicon cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecf4efd-e924-4751-93b0-e35b432c3653",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell = gtop.Cell()\n",
    "a = 5.43\n",
    "cell.atom = [['Si', [0,0,0]],\n",
    "              ['Si', [a/4,a/4,a/4]]]\n",
    "cell.a = jnp.asarray([[0, a/2, a/2],\n",
    "                     [a/2, 0, a/2],\n",
    "                     [a/2, a/2, 0]])\n",
    "cell.basis = 'gth-szv'\n",
    "cell.pseudo = 'gth-pade'\n",
    "cell.exp_to_discard = 0.1\n",
    "cell.build(trace_lattice_vectors=True)\n",
    "kpts = cell.make_kpts([2,2,2])\n",
    "mf = scfp.KRHF(cell, kpts=kpts)\n",
    "e = mf.kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd40a2a-4f3d-4d32-be14-31ad5e29efb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
