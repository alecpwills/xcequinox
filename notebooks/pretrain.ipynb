{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08b9c7b6-b4c1-4561-8985-c133b7c54e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/awills/anaconda3/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting \"B3LYP_WITH_VWN5 = True\" in pyscf_conf.py\n",
      "  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '\n"
     ]
    }
   ],
   "source": [
    "# import pyscfad\n",
    "# from pyscfad import gto,dft,scf\n",
    "import pyscf\n",
    "from pyscf import gto,dft,scf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff3495a9-9540-4120-92c8-01c0005c99d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import scipy\n",
    "from ase import Atoms\n",
    "from ase.io import read\n",
    "import xcquinox as xce\n",
    "from functools import partial\n",
    "from ase.units import Bohr\n",
    "import os, optax, jax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc862ab5-4c49-465a-acbb-dca9a2ff288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mol(atoms, basis='6-311++G**'):\n",
    "    pos = atoms.positions\n",
    "    spec = atoms.get_chemical_symbols()\n",
    "    mol_input = [[s, p] for s, p in zip(spec, pos)]\n",
    "    try:\n",
    "        mol = gto.Mole(atom=mol_input, basis=atoms.info.get('basis',basis),spin=atoms.info.get('spin',0))\n",
    "    except Exception:\n",
    "        mol = gto.Mole(atom=mol_input, basis=atoms.info.get('basis','STO-3G'),spin=atoms.info.get('spin',0))\n",
    "    return mol \n",
    "\n",
    "def get_rhos(rho, spin):\n",
    "    rho0 = rho[0,0]\n",
    "    drho = rho[0,1:4] + rho[1:4,0]\n",
    "    tau = 0.5*(rho[1,1] + rho[2,2] + rho[3,3])\n",
    "\n",
    "    if spin != 0:\n",
    "        rho0_a = rho0[0]\n",
    "        rho0_b = rho0[1]\n",
    "        gamma_a, gamma_b = jnp.einsum('ij,ij->j',drho[:,0],drho[:,0]), jnp.einsum('ij,ij->j',drho[:,1],drho[:,1])              \n",
    "        gamma_ab = jnp.einsum('ij,ij->j',drho[:,0],drho[:,1])\n",
    "        tau_a, tau_b = tau\n",
    "    else:\n",
    "        rho0_a = rho0_b = rho0*0.5\n",
    "        gamma_a=gamma_b=gamma_ab= jnp.einsum('ij,ij->j',drho[:],drho[:])*0.25\n",
    "        tau_a = tau_b = tau*0.5\n",
    "    return rho0_a, rho0_b, gamma_a, gamma_b, gamma_ab, tau_a, tau_b\n",
    "    \n",
    "def get_data_synth(xc_func, n=100):\n",
    "    def get_rho(s, a):\n",
    "        c0 = 2*(3*np.pi**2)**(1/3)\n",
    "        c1 = 3/10*(3*np.pi**2)**(2/3)\n",
    "        gamma = c0*s\n",
    "        tau = c1*a+c0**2*s**2/8\n",
    "        rho = np.zeros([len(a),6])\n",
    "        rho[:, 1] = gamma\n",
    "        rho[:,-1] = tau\n",
    "        rho[:, 0] = 1\n",
    "        return rho\n",
    "    \n",
    "    s_grid = jnp.concatenate([[0],jnp.exp(jnp.linspace(-10,4,n))])\n",
    "    rho = []\n",
    "    for s in s_grid:\n",
    "        if 'MGGA' in xc_func:\n",
    "            a_grid = jnp.concatenate([jnp.exp(jnp.linspace(jnp.log((s/100)+1e-8),8,n))])\n",
    "        else:\n",
    "            a_grid = jnp.array([0])\n",
    "        rho.append(get_rho(s, a_grid))\n",
    "        \n",
    "    rho = jnp.concatenate(rho)\n",
    "    \n",
    "    fxc =  dft.numint.libxc.eval_xc(xc_func,rho.T, spin=0)[0]/dft.numint.libxc.eval_xc('LDA_X',rho.T, spin=0)[0] -1\n",
    " \n",
    "    rho = jnp.asarray(rho)\n",
    "    \n",
    "    tdrho = xc.get_descriptors(rho[:,0]/2,rho[:,0]/2,(rho[:,1]/2)**2,(rho[:,1]/2)**2,(rho[:,1]/2)**2,rho[:,5]/2,rho[:,5]/2, spin_scaling=True)\n",
    "    \n",
    "\n",
    "\n",
    "    tFxc = torch.from_numpy(fxc)\n",
    "    return tdrho[0], tFxc\n",
    "\n",
    "def get_data(mol, xc_func ,full=False, enhance_spin=False, localnet=None):\n",
    "    print('mol: ', mol.atom)\n",
    "    try:\n",
    "        mf = scf.UKS(mol)\n",
    "    except:\n",
    "        mf = dft.RKS(mol)\n",
    "    mf.xc = 'PBE'\n",
    "    mf.grids.level = 1\n",
    "    mf.kernel()\n",
    "    if not full:\n",
    "        mf.grids.coords = coords\n",
    "        mf.grids.weights = weights\n",
    "    if localnet.spin_scaling:\n",
    "        print('spin scaling, indicates exchange network')\n",
    "        rho_alpha = mf._numint.eval_rho(mol, mf._numint.eval_ao(mol, mf.grids.coords, deriv=2) , mf.make_rdm1()[0], xctype='metaGGA',hermi=True)\n",
    "        rho_beta = mf._numint.eval_rho(mol, mf._numint.eval_ao(mol, mf.grids.coords, deriv=2) , mf.make_rdm1()[1], xctype='metaGGA',hermi=True)\n",
    "        fxc_a =  mf._numint.eval_xc(xc_func,(rho_alpha,rho_alpha*0), spin=1)[0]/mf._numint.eval_xc('LDA_X',(rho_alpha,rho_alpha*0), spin=1)[0] -1\n",
    "        fxc_b =  mf._numint.eval_xc(xc_func,(rho_beta*0,rho_beta), spin=1)[0]/mf._numint.eval_xc('LDA_X',(rho_beta*0,rho_beta), spin=1)[0] -1\n",
    "        print('fxc with xc_func = {} = {}'.format(fxc_a, xc_func))\n",
    "\n",
    "        if mol.spin != 0 and sum(mol.nelec)>1:\n",
    "            rho = jnp.concatenate([rho_alpha, rho_beta])\n",
    "            fxc = jnp.concatenate([fxc_a, fxc_b])\n",
    "        else:\n",
    "            rho = rho_alpha\n",
    "            fxc = fxc_a\n",
    "    else:    \n",
    "        print('no spin scaling, indicates correlation network')\n",
    "        rho_alpha = mf._numint.eval_rho(mol, mf._numint.eval_ao(mol, mf.grids.coords, deriv=2) , mf.make_rdm1()[0], xctype='metaGGA',hermi=True)\n",
    "        rho_beta = mf._numint.eval_rho(mol, mf._numint.eval_ao(mol, mf.grids.coords, deriv=2) , mf.make_rdm1()[1], xctype='metaGGA',hermi=True)\n",
    "        exc = mf._numint.eval_xc(xc_func,(rho_alpha,rho_beta), spin=1)[0]\n",
    "        print('exc with xc_func = {} = {}'.format(exc, xc_func))\n",
    "        fxc = exc/mf._numint.eval_xc('LDA_C_PW',(rho_alpha, rho_beta), spin=1)[0] -1\n",
    "#         fxc = exc\n",
    "        rho = jnp.stack([rho_alpha,rho_beta], axis=-1)\n",
    "    \n",
    "    dm = jnp.array(mf.make_rdm1())\n",
    "    ao_eval = jnp.array(mf._numint.eval_ao(mol, mf.grids.coords, deriv=1))\n",
    "    rho = jnp.einsum('xij,yik,...jk->xy...i', ao_eval, ao_eval, dm)\n",
    "    \n",
    "    if dm.ndim == 3:\n",
    "        rho_filt = (jnp.sum(rho[0,0],axis=0) > 1e-6)\n",
    "    else:\n",
    "        rho_filt = (rho[0,0] > 1e-6)\n",
    "    tdrho = xc.get_descriptors(*get_rhos(rho, spin=1), spin_scaling=localnet.spin_scaling)\n",
    "    \n",
    "#     tdrho = torch.from_numpy(tdrho.detach().numpy().round(8))\n",
    "        \n",
    "    if localnet.spin_scaling:\n",
    "        if mol.spin != 0 and sum(mol.nelec) > 1:\n",
    "            tdrho = jnp.concatenate([tdrho[0],tdrho[1]])\n",
    "            rho_filt = jnp.concatenate([rho_filt]*2)\n",
    "            \n",
    "        else:\n",
    "            tdrho = tdrho[0]\n",
    "    tdrho = tdrho[rho_filt]\n",
    "\n",
    "    tFxc = jnp.array(fxc)[rho_filt]\n",
    "#     tFxc = torch.from_snumpy(fxc)\n",
    "    return tdrho, tFxc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb5b74f8-9bdb-47b4-b639-a2bb0f6bbd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "PRETRAIN_LEVEL = 'MGGA'\n",
    "\n",
    "TRAIN_NET = 'c'\n",
    "\n",
    "REFERENCE_XC = 'PBE0'\n",
    "\n",
    "N_HIDDEN = 16\n",
    "DEPTH = 3\n",
    "if PRETRAIN_LEVEL == 'GGA':\n",
    "    if TRAIN_NET == 'x':\n",
    "        localx = xce.net.eX(n_input=1, n_hidden=N_HIDDEN, use=[1], depth=DEPTH, lob=1.804)\n",
    "    elif TRAIN_NET == 'c':\n",
    "        localc = xce.net.eC(n_input=3, n_hidden=N_HIDDEN, use=[2], depth=DEPTH, ueg_limit=True)\n",
    "elif PRETRAIN_LEVEL == 'MGGA':\n",
    "    if TRAIN_NET == 'x':\n",
    "        localx = xce.net.eX(n_input=2, n_hidden=N_HIDDEN, use=[1, 2], depth=DEPTH, ueg_limit=True, lob=1.174)\n",
    "    elif TRAIN_NET == 'c':\n",
    "        localc = xce.net.eC(n_input=4, n_hidden=N_HIDDEN, depth=DEPTH, use=[2,3], ueg_limit=True)\n",
    "\n",
    "if TRAIN_NET == 'x':\n",
    "    thislocal = localx\n",
    "else:\n",
    "    thislocal = localc\n",
    "ueg = xce.xc.LDA_X()\n",
    "xc = xce.xc.eXC(grid_models=[thislocal], heg_mult=True, level= {'GGA':2, 'MGGA':3, 'NONLOCAL':4}[PRETRAIN_LEVEL])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fc935919-401c-449d-aca9-e423cf20792d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyscf.gto.mole.Mole object at 0x7451a04314b0> [['P', array([0., 0., 0.])]] 1\n",
      "<pyscf.gto.mole.Mole object at 0x7451a0432b60> [['N', array([0., 0., 0.])]] 1\n",
      "<pyscf.gto.mole.Mole object at 0x7451a04324d0> [['H', array([0., 0., 0.])]] 1\n",
      "<pyscf.gto.mole.Mole object at 0x7451a04322f0> [['Li', array([0., 0., 0.])]] 1\n",
      "<pyscf.gto.mole.Mole object at 0x7451a0432830> [['O', array([0., 0., 0.])]] 1\n",
      "<pyscf.gto.mole.Mole object at 0x7451a0433430> [['Cl', array([0., 0., 0.])]] 1\n",
      "<pyscf.gto.mole.Mole object at 0x7451a0431e40> [['Al', array([0., 0., 0.])]] 1\n",
      "<pyscf.gto.mole.Mole object at 0x7451a0433580> [['S', array([0., 0., 0.])]] 1\n",
      "<pyscf.gto.mole.Mole object at 0x7451a0433490> [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]] 2\n",
      "<pyscf.gto.mole.Mole object at 0x7451a0432cb0> [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]] 3\n",
      "<pyscf.gto.mole.Mole object at 0x7451a0430100> [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]] 3\n",
      "<pyscf.gto.mole.Mole object at 0x7451a04339d0> [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]] 2\n",
      "<pyscf.gto.mole.Mole object at 0x7451a0433820> [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]] 2\n",
      "<pyscf.gto.mole.Mole object at 0x7451a0432d70> [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]] 2\n",
      "<pyscf.gto.mole.Mole object at 0x7451a0432710> [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]] 4\n",
      "<pyscf.gto.mole.Mole object at 0x7451a0431b40> [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]] 2\n",
      "<pyscf.gto.mole.Mole object at 0x7451a0431de0> [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]] 2\n",
      "<pyscf.gto.mole.Mole object at 0x7451a0433880> [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]] 2\n",
      "<pyscf.gto.mole.Mole object at 0x7451a04317b0> [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]] 2\n",
      "<pyscf.gto.mole.Mole object at 0x7451a0433fd0> [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]] 4\n",
      "<pyscf.gto.mole.Mole object at 0x7451a0433c70> [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]] 4\n",
      "<pyscf.gto.mole.Mole object at 0x7451a0432590> [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]] 2\n",
      "<pyscf.gto.mole.Mole object at 0x7451a0432a10> [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]] 5\n",
      "<pyscf.gto.mole.Mole object at 0x7451a04304f0> [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]] 3\n",
      "<pyscf.gto.mole.Mole object at 0x7451a04313c0> [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]] 5\n"
     ]
    }
   ],
   "source": [
    "spins = {\n",
    "    'Al': 1,\n",
    "    'B' : 1,\n",
    "    'Li': 1,\n",
    "    'Na': 1,\n",
    "    'Si': 2 ,\n",
    "    'Be':0,\n",
    "    'C': 2,\n",
    "    'Cl': 1,\n",
    "    'F': 1,\n",
    "    'H': 1,\n",
    "    'N': 3,\n",
    "    'O': 2,\n",
    "    'P': 3,\n",
    "    'S': 2\n",
    "}\n",
    "\n",
    "selection = [2, 113, 25, 18, 11, 17, 114, 121, 101, 0, 20, 26, 29, 67, 28, 110, 125, 10, 115, 89, 105, 50]\n",
    "atoms = [read('/home/awills/Documents/Research/ogdpyscf/dpyscf/data/haunschild_g2/g2_97.traj',':')[s] for s in selection]\n",
    "ksr_atoms = atoms\n",
    "if PRETRAIN_LEVEL=='MGGA':\n",
    "    ksr_atoms = ksr_atoms[2:]\n",
    "ksr_atoms = [Atoms('P',info={'spin':3}), Atoms('N', info={'spin':3}), Atoms('H', info={'spin':1}),Atoms('Li', info={'spin':1}), Atoms('O',info={'spin':2}),Atoms('Cl',info={'spin':1}),Atoms('Al',info={'spin':1}), Atoms('S',info={'spin':2})] + ksr_atoms\n",
    "# ksr_atoms = [Atoms('H',info={'spin':1})]\n",
    "mols = [get_mol(atoms) for atoms in ksr_atoms]\n",
    "mols = [i for i in mols if len(i.atom) < 8]\n",
    "for i in mols:\n",
    "    print(i, i.atom, len(i.atom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4f46302-f203-4cbd-afdc-1e8471877717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thislocal.spin_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c09eb552-027d-4bcd-a3b0-7e7bb70c5a20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mol:  [['P', array([0., 0., 0.])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: <pyscf.gto.mole.Mole object at 0x7451a04314b0> must be initialized before calling SCF.\n",
      "Initialize <pyscf.gto.mole.Mole object at 0x7451a04314b0> in UKS object of <class 'pyscf.dft.uks.UKS'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -341.104145992717  <S^2> = 3.7502984  2S+1 = 4.0001492\n",
      "no spin scaling, indicates correlation network\n",
      "exc with xc_func = [-1.02664237e-02 -2.75034982e-03 -4.08130021e-03 ... -7.25655317e+00\n",
      " -7.25655317e+00 -7.25655317e+00] = PBE0\n",
      "mol:  [['N', array([0., 0., 0.])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: <pyscf.gto.mole.Mole object at 0x7451a0432b60> must be initialized before calling SCF.\n",
      "Initialize <pyscf.gto.mole.Mole object at 0x7451a0432b60> in UKS object of <class 'pyscf.dft.uks.UKS'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -54.5289742046674  <S^2> = 3.7524945  2S+1 = 4.0012471\n",
      "no spin scaling, indicates correlation network\n",
      "exc with xc_func = [-5.96866425e-03 -2.51798665e-03 -5.83130694e-04 ... -3.33703592e+00\n",
      " -3.33703592e+00 -3.33703592e+00] = PBE0\n",
      "mol:  [['H', array([0., 0., 0.])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: <pyscf.gto.mole.Mole object at 0x7451a04324d0> must be initialized before calling SCF.\n",
      "Initialize <pyscf.gto.mole.Mole object at 0x7451a04324d0> in UKS object of <class 'pyscf.dft.uks.UKS'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -0.49981298400854  <S^2> = 0.75  2S+1 = 2\n",
      "no spin scaling, indicates correlation network\n",
      "exc with xc_func = [-5.68454275e-03 -2.30635936e-03 -4.18219349e-04 ... -4.97345779e-01\n",
      " -4.97345779e-01 -4.97345779e-01] = PBE0\n",
      "mol:  [['Li', array([0., 0., 0.])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: <pyscf.gto.mole.Mole object at 0x7451a04322f0> must be initialized before calling SCF.\n",
      "Initialize <pyscf.gto.mole.Mole object at 0x7451a04322f0> in UKS object of <class 'pyscf.dft.uks.UKS'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -7.46006188627842  <S^2> = 0.75000049  2S+1 = 2.0000005\n",
      "no spin scaling, indicates correlation network\n",
      "exc with xc_func = [-0.01101625 -0.00387793 -0.00387793 ... -0.02009147 -1.39639206\n",
      " -1.39639206] = PBE0\n",
      "mol:  [['O', array([0., 0., 0.])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: <pyscf.gto.mole.Mole object at 0x7451a0432830> must be initialized before calling SCF.\n",
      "Initialize <pyscf.gto.mole.Mole object at 0x7451a0432830> in UKS object of <class 'pyscf.dft.uks.UKS'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -75.0033795084925  <S^2> = 2.0027447  2S+1 = 3.0018292\n",
      "no spin scaling, indicates correlation network\n",
      "exc with xc_func = [-3.68102742e-03 -1.13732790e-03 -1.57783875e-04 ... -3.81896192e+00\n",
      " -3.81896192e+00 -3.81896192e+00] = PBE0\n",
      "mol:  [['Cl', array([0., 0., 0.])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: <pyscf.gto.mole.Mole object at 0x7451a0433430> must be initialized before calling SCF.\n",
      "Initialize <pyscf.gto.mole.Mole object at 0x7451a0433430> in UKS object of <class 'pyscf.dft.uks.UKS'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -459.95757712076  <S^2> = 0.7516194  2S+1 = 2.0016187\n",
      "no spin scaling, indicates correlation network\n",
      "exc with xc_func = [-2.11982273e-03 -2.48633744e-03 -1.44197115e-03 ... -8.24204296e+00\n",
      " -8.24204296e+00 -8.24204296e+00] = PBE0\n",
      "mol:  [['Al', array([0., 0., 0.])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: <pyscf.gto.mole.Mole object at 0x7451a0431e40> must be initialized before calling SCF.\n",
      "Initialize <pyscf.gto.mole.Mole object at 0x7451a0431e40> in UKS object of <class 'pyscf.dft.uks.UKS'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -242.226561015217  <S^2> = 0.75226414  2S+1 = 2.0022629\n",
      "no spin scaling, indicates correlation network\n",
      "exc with xc_func = [-0.02501281 -0.01759485 -0.0104445  ... -0.00091049 -0.00407204\n",
      " -0.00067045] = PBE0\n",
      "mol:  [['S', array([0., 0., 0.])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: <pyscf.gto.mole.Mole object at 0x7451a0433580> must be initialized before calling SCF.\n",
      "Initialize <pyscf.gto.mole.Mole object at 0x7451a0433580> in UKS object of <class 'pyscf.dft.uks.UKS'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -397.938786804559  <S^2> = 2.0022329  2S+1 = 3.0014882\n",
      "no spin scaling, indicates correlation network\n",
      "exc with xc_func = [-6.01728008e-03 -2.66238833e-03 -2.59666135e-03 ... -7.75011279e+00\n",
      " -7.75011279e+00 -7.75011279e+00] = PBE0\n",
      "mol:  [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: <pyscf.gto.mole.Mole object at 0x7451a0433490> must be initialized before calling SCF.\n",
      "Initialize <pyscf.gto.mole.Mole object at 0x7451a0433490> in UKS object of <class 'pyscf.dft.uks.UKS'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -107.339357395734  <S^2> = 5.3290705e-15  2S+1 = 1\n",
      "no spin scaling, indicates correlation network\n",
      "exc with xc_func = [-0.00214665 -0.00173898 -0.00179938 ... -0.00203991 -0.61562248\n",
      " -0.61562248] = PBE0\n",
      "mol:  [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: <pyscf.gto.mole.Mole object at 0x7451a0432cb0> must be initialized before calling SCF.\n",
      "Initialize <pyscf.gto.mole.Mole object at 0x7451a0432cb0> in UKS object of <class 'pyscf.dft.uks.UKS'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -93.337792446513  <S^2> = 4.0072479e-10  2S+1 = 1\n",
      "no spin scaling, indicates correlation network\n",
      "exc with xc_func = [-1.38296163e-03 -2.66832598e-04 -3.06425201e-04 ... -5.03343606e-01\n",
      " -5.03343606e-01 -5.03343606e-01] = PBE0\n",
      "mol:  [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: <pyscf.gto.mole.Mole object at 0x7451a0430100> must be initialized before calling SCF.\n",
      "Initialize <pyscf.gto.mole.Mole object at 0x7451a0430100> in UKS object of <class 'pyscf.dft.uks.UKS'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -188.456965322844  <S^2> = 7.1054274e-15  2S+1 = 1\n",
      "no spin scaling, indicates correlation network\n",
      "exc with xc_func = [-1.29084098e-03 -1.15920157e-03 -3.32405317e-03 ... -6.61945569e-05\n",
      " -2.84591976e+00 -2.84591976e+00] = PBE0\n",
      "mol:  [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: <pyscf.gto.mole.Mole object at 0x7451a04339d0> must be initialized before calling SCF.\n",
      "Initialize <pyscf.gto.mole.Mole object at 0x7451a04339d0> in UKS object of <class 'pyscf.dft.uks.UKS'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -920.00560888896  <S^2> = 4.938272e-13  2S+1 = 1\n",
      "no spin scaling, indicates correlation network\n",
      "exc with xc_func = [-0.00376872 -0.00297928 -0.00401242 ... -0.00297928 -0.3467079\n",
      " -0.3467079 ] = PBE0\n",
      "mol:  [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: <pyscf.gto.mole.Mole object at 0x7451a0433820> must be initialized before calling SCF.\n",
      "Initialize <pyscf.gto.mole.Mole object at 0x7451a0433820> in UKS object of <class 'pyscf.dft.uks.UKS'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -199.394370591172  <S^2> = 1.1901591e-13  2S+1 = 1\n",
      "no spin scaling, indicates correlation network\n",
      "exc with xc_func = [-1.28783793e-03 -2.82669550e-04 -4.34324741e-03 ... -4.25493449e-01\n",
      " -4.25493449e-01 -4.25493449e-01] = PBE0\n",
      "mol:  [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: <pyscf.gto.mole.Mole object at 0x7451a0432d70> must be initialized before calling SCF.\n",
      "Initialize <pyscf.gto.mole.Mole object at 0x7451a0432d70> in UKS object of <class 'pyscf.dft.uks.UKS'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -150.21489454177  <S^2> = 1.0018599  2S+1 = 2.2377309\n",
      "no spin scaling, indicates correlation network\n",
      "exc with xc_func = [-2.88673908e-03 -9.06450598e-04 -1.28008902e-04 ... -5.16165985e-01\n",
      " -5.16165985e-01 -5.16165985e-01] = PBE0\n",
      "mol:  [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: <pyscf.gto.mole.Mole object at 0x7451a0432710> must be initialized before calling SCF.\n",
      "Initialize <pyscf.gto.mole.Mole object at 0x7451a0432710> in UKS object of <class 'pyscf.dft.uks.UKS'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -77.2435048346375  <S^2> = 1.5099033e-14  2S+1 = 1\n",
      "no spin scaling, indicates correlation network\n",
      "exc with xc_func = [-0.00060514 -0.00060514 -0.0032515  ... -0.00031967 -0.00031865\n",
      " -0.00031865] = PBE0\n",
      "mol:  [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: <pyscf.gto.mole.Mole object at 0x7451a0431b40> must be initialized before calling SCF.\n",
      "Initialize <pyscf.gto.mole.Mole object at 0x7451a0431b40> in UKS object of <class 'pyscf.dft.uks.UKS'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -113.221335689652  <S^2> = 6.6346928e-13  2S+1 = 1\n",
      "no spin scaling, indicates correlation network\n",
      "exc with xc_func = [-0.00219858 -0.00302532 -0.00639712 ... -0.56161386 -0.56161386\n",
      " -0.56161386] = PBE0\n",
      "mol:  [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: <pyscf.gto.mole.Mole object at 0x7451a0431de0> must be initialized before calling SCF.\n",
      "Initialize <pyscf.gto.mole.Mole object at 0x7451a0431de0> in UKS object of <class 'pyscf.dft.uks.UKS'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -460.624592374078  <S^2> = 6.5725203e-14  2S+1 = 1\n",
      "no spin scaling, indicates correlation network\n",
      "exc with xc_func = [-1.16123674e-03 -2.16564032e-04 -2.58966276e-04 ... -2.41956464e+00\n",
      " -2.41956464e+00 -2.41956464e+00] = PBE0\n",
      "mol:  [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: <pyscf.gto.mole.Mole object at 0x7451a0433880> must be initialized before calling SCF.\n",
      "Initialize <pyscf.gto.mole.Mole object at 0x7451a0433880> in UKS object of <class 'pyscf.dft.uks.UKS'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -8.04458854018922  <S^2> = 7.9269924e-14  2S+1 = 1\n",
      "no spin scaling, indicates correlation network\n",
      "exc with xc_func = [-0.00280244 -0.00292882 -0.00292882 ... -0.42329395 -0.42329395\n",
      " -0.42329395] = PBE0\n",
      "mol:  [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: <pyscf.gto.mole.Mole object at 0x7451a04317b0> must be initialized before calling SCF.\n",
      "Initialize <pyscf.gto.mole.Mole object at 0x7451a04317b0> in UKS object of <class 'pyscf.dft.uks.UKS'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -324.340512506578  <S^2> = 1.5857538e-11  2S+1 = 1\n",
      "no spin scaling, indicates correlation network\n",
      "exc with xc_func = [-0.00664309 -0.00664309 -0.00843956 ... -0.00664309 -0.00664309\n",
      " -0.00843956] = PBE0\n",
      "mol:  [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: <pyscf.gto.mole.Mole object at 0x7451a0433fd0> must be initialized before calling SCF.\n",
      "Initialize <pyscf.gto.mole.Mole object at 0x7451a0433fd0> in UKS object of <class 'pyscf.dft.uks.UKS'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -1622.57507845814  <S^2> = 8.2422957e-13  2S+1 = 1\n",
      "no spin scaling, indicates correlation network\n",
      "exc with xc_func = [-9.24417223e-04 -1.96285983e-03 -4.54661166e-03 ... -6.26905723e+00\n",
      " -6.26905723e+00 -6.26905723e+00] = PBE0\n",
      "mol:  [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: <pyscf.gto.mole.Mole object at 0x7451a0433c70> must be initialized before calling SCF.\n",
      "Initialize <pyscf.gto.mole.Mole object at 0x7451a0433c70> in UKS object of <class 'pyscf.dft.uks.UKS'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -342.979728469575  <S^2> = 2.5393021e-11  2S+1 = 1\n",
      "no spin scaling, indicates correlation network\n",
      "exc with xc_func = [-3.12430446e-03 -1.72805187e-03 -3.47626992e-04 ... -1.52422293e+00\n",
      " -1.52422293e+00 -1.52422293e+00] = PBE0\n",
      "mol:  [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: <pyscf.gto.mole.Mole object at 0x7451a0432590> must be initialized before calling SCF.\n",
      "Initialize <pyscf.gto.mole.Mole object at 0x7451a0432590> in UKS object of <class 'pyscf.dft.uks.UKS'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -578.56533666059  <S^2> = 1.0034708  2S+1 = 2.2391702\n",
      "no spin scaling, indicates correlation network\n",
      "exc with xc_func = [-0.00093916 -0.00130488 -0.00130487 ... -0.30880861 -0.30880861\n",
      " -0.30880861] = PBE0\n",
      "mol:  [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: <pyscf.gto.mole.Mole object at 0x7451a0432a10> must be initialized before calling SCF.\n",
      "Initialize <pyscf.gto.mole.Mole object at 0x7451a0432a10> in UKS object of <class 'pyscf.dft.uks.UKS'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -40.4598214864075  <S^2> = 3.170797e-13  2S+1 = 1\n",
      "no spin scaling, indicates correlation network\n",
      "exc with xc_func = [-0.00295239 -0.00121221 -0.00020696 ... -0.00027031 -0.00027031\n",
      " -0.00027031] = PBE0\n",
      "mol:  [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: <pyscf.gto.mole.Mole object at 0x7451a04304f0> must be initialized before calling SCF.\n",
      "Initialize <pyscf.gto.mole.Mole object at 0x7451a04304f0> in UKS object of <class 'pyscf.dft.uks.UKS'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -39.0756147483505  <S^2> = 6.2030381e-12  2S+1 = 1\n",
      "no spin scaling, indicates correlation network\n",
      "exc with xc_func = [-2.63869543e-03 -4.00286742e-04 -1.08589328e-03 ... -9.56249175e-01\n",
      " -9.56249175e-01 -9.56249175e-01] = PBE0\n",
      "mol:  [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: <pyscf.gto.mole.Mole object at 0x7451a04313c0> must be initialized before calling SCF.\n",
      "Initialize <pyscf.gto.mole.Mole object at 0x7451a04313c0> in UKS object of <class 'pyscf.dft.uks.UKS'>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converged SCF energy = -291.719272437819  <S^2> = 1.3146817e-11  2S+1 = 1\n",
      "no spin scaling, indicates correlation network\n",
      "exc with xc_func = [-0.00288016 -0.00118216 -0.00020993 ... -0.0002218  -0.0002218\n",
      " -0.0002218 ] = PBE0\n"
     ]
    }
   ],
   "source": [
    "# data = [get_data(mol, xc_func=ref, full=i<14) for i,mol in enumerate(mols)]\n",
    "ref = 'PBE0'\n",
    "data = [get_data(mol, xc_func=ref,full=True, localnet=thislocal) for i,mol in enumerate(mols)]\n",
    "# \n",
    "# data = [get_data_synth(ref, 100)]\n",
    "tdrho = jnp.concatenate([d[0] for d in data])\n",
    "tFxc = jnp.concatenate([d[1] for d in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7f45ce9-18ef-4535-868c-cf66adc06778",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_filt = ~jnp.any((tdrho != tdrho),axis=-1)\n",
    "\n",
    "tFxc = tFxc[nan_filt]\n",
    "tdrho = tdrho[nan_filt,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27650b8e-1d55-4408-a389-bad72f2ca469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((224145, 4),\n",
       " Array([[-3.55646847e+00,  2.20899060e-01,  2.04142939e+00,\n",
       "          1.43859114e+00],\n",
       "        [-4.04188739e+00,  2.26935426e-01,  2.65887948e+00,\n",
       "          1.38496111e+00],\n",
       "        [-3.20301698e+00,  2.08517375e-01,  1.63659032e+00,\n",
       "          1.14131231e+00],\n",
       "        ...,\n",
       "        [-4.21483060e+00,  9.81437154e-14,  2.82063903e+00,\n",
       "          5.40655126e-01],\n",
       "        [-4.25518732e+00,  4.00346423e-13,  2.87882770e+00,\n",
       "          2.76902720e-01],\n",
       "        [-4.31527649e+00,  6.20392626e-13,  2.81798141e+00,\n",
       "          1.35860915e+00]], dtype=float64))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdrho[::].shape, tdrho[::]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "558c5fc1-a90c-4aff-af01-ca7bf3d06cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpus = jax.devices(backend='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b1ccd3b-745d-4e4b-96bd-8e8983c967b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Epoch 0 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 142.01953322295498\n",
      "0, epoch_train_loss=142.01953322295498\n",
      "Epoch 1\n",
      "Epoch 1 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 141.52423514100153\n",
      "1, epoch_train_loss=141.52423514100153\n",
      "Epoch 2\n",
      "Epoch 2 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 140.93775898579273\n",
      "2, epoch_train_loss=140.93775898579273\n",
      "Epoch 3\n",
      "Epoch 3 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 140.21737178188835\n",
      "3, epoch_train_loss=140.21737178188835\n",
      "Epoch 4\n",
      "Epoch 4 :: Batch 0/1\n",
      "Batch Loss = 139.3136881036083\n",
      "4, epoch_train_loss=139.3136881036083\n",
      "Epoch 5\n",
      "Epoch 5 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 138.16118932122865\n",
      "5, epoch_train_loss=138.16118932122865\n",
      "Epoch 6\n",
      "Epoch 6 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 136.6826233323594\n",
      "6, epoch_train_loss=136.6826233323594\n",
      "Epoch 7\n",
      "Epoch 7 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 134.78788687460062\n",
      "7, epoch_train_loss=134.78788687460062\n",
      "Epoch 8\n",
      "Epoch 8 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 132.37911096797714\n",
      "8, epoch_train_loss=132.37911096797714\n",
      "Epoch 9\n",
      "Epoch 9 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 129.354262105626\n",
      "9, epoch_train_loss=129.354262105626\n",
      "Epoch 10\n",
      "Epoch 10 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 125.61655599566436\n",
      "10, epoch_train_loss=125.61655599566436\n",
      "Epoch 11\n",
      "Epoch 11 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 121.08010315284187\n",
      "11, epoch_train_loss=121.08010315284187\n",
      "Epoch 12\n",
      "Epoch 12 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 115.6694970421447\n",
      "12, epoch_train_loss=115.6694970421447\n",
      "Epoch 13\n",
      "Epoch 13 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 109.32586802565598\n",
      "13, epoch_train_loss=109.32586802565598\n",
      "Epoch 14\n",
      "Epoch 14 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 102.0103293245059\n",
      "14, epoch_train_loss=102.0103293245059\n",
      "Epoch 15\n",
      "Epoch 15 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 93.70808314475755\n",
      "15, epoch_train_loss=93.70808314475755\n",
      "Epoch 16\n",
      "Epoch 16 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 84.44260081666202\n",
      "16, epoch_train_loss=84.44260081666202\n",
      "Epoch 17\n",
      "Epoch 17 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 74.29156860914269\n",
      "17, epoch_train_loss=74.29156860914269\n",
      "Epoch 18\n",
      "Epoch 18 :: Batch 0/1\n",
      "Batch Loss = 63.38767684473789\n",
      "18, epoch_train_loss=63.38767684473789\n",
      "Epoch 19\n",
      "Epoch 19 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 51.972113491236826\n",
      "19, epoch_train_loss=51.972113491236826\n",
      "Epoch 20\n",
      "Epoch 20 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 40.46697595175366\n",
      "20, epoch_train_loss=40.46697595175366\n",
      "Epoch 21\n",
      "Epoch 21 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 29.50917794143992\n",
      "21, epoch_train_loss=29.50917794143992\n",
      "Epoch 22\n",
      "Epoch 22 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 19.95310610270469\n",
      "22, epoch_train_loss=19.95310610270469\n",
      "Epoch 23\n",
      "Epoch 23 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 12.794355444919017\n",
      "23, epoch_train_loss=12.794355444919017\n",
      "Epoch 24\n",
      "Epoch 24 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 9.070226942027253\n",
      "24, epoch_train_loss=9.070226942027253\n",
      "Epoch 25\n",
      "Epoch 25 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 9.593325266358981\n",
      "25, epoch_train_loss=9.593325266358981\n",
      "Epoch 26\n",
      "Epoch 26 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 13.936150284997348\n",
      "26, epoch_train_loss=13.936150284997348\n",
      "Epoch 27\n",
      "Epoch 27 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 19.405037816890903\n",
      "27, epoch_train_loss=19.405037816890903\n",
      "Epoch 28\n",
      "Epoch 28 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 22.574512404975096\n",
      "28, epoch_train_loss=22.574512404975096\n",
      "Epoch 29\n",
      "Epoch 29 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 21.964669938538268\n",
      "29, epoch_train_loss=21.964669938538268\n",
      "Epoch 30\n",
      "Epoch 30 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 18.450993116757395\n",
      "30, epoch_train_loss=18.450993116757395\n",
      "Epoch 31\n",
      "Epoch 31 :: Batch 0/1\n",
      "Batch Loss = 14.015423323540983\n",
      "31, epoch_train_loss=14.015423323540983\n",
      "Epoch 32\n",
      "Epoch 32 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 10.510461477466128\n",
      "32, epoch_train_loss=10.510461477466128\n",
      "Epoch 33\n",
      "Epoch 33 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 8.952402583293074\n",
      "33, epoch_train_loss=8.952402583293074\n",
      "Epoch 34\n",
      "Epoch 34 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 9.169911007891397\n",
      "34, epoch_train_loss=9.169911007891397\n",
      "Epoch 35\n",
      "Epoch 35 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 10.227060895302841\n",
      "35, epoch_train_loss=10.227060895302841\n",
      "Epoch 36\n",
      "Epoch 36 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 11.189091922177269\n",
      "36, epoch_train_loss=11.189091922177269\n",
      "Epoch 37\n",
      "Epoch 37 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 11.51738354812142\n",
      "37, epoch_train_loss=11.51738354812142\n",
      "Epoch 38\n",
      "Epoch 38 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 11.11031736998523\n",
      "38, epoch_train_loss=11.11031736998523\n",
      "Epoch 39\n",
      "Epoch 39 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 10.142906633018372\n",
      "39, epoch_train_loss=10.142906633018372\n",
      "Epoch 40\n",
      "Epoch 40 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 8.909145430876883\n",
      "40, epoch_train_loss=8.909145430876883\n",
      "Epoch 41\n",
      "Epoch 41 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 7.721528636463038\n",
      "41, epoch_train_loss=7.721528636463038\n",
      "Epoch 42\n",
      "Epoch 42 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 6.843084919161483\n",
      "42, epoch_train_loss=6.843084919161483\n",
      "Epoch 43\n",
      "Epoch 43 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 6.424971977348707\n",
      "43, epoch_train_loss=6.424971977348707\n",
      "Epoch 44\n",
      "Epoch 44 :: Batch 0/1\n",
      "Batch Loss = 6.458571877450885\n",
      "44, epoch_train_loss=6.458571877450885\n",
      "Epoch 45\n",
      "Epoch 45 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 6.7772549284648385\n",
      "45, epoch_train_loss=6.7772549284648385\n",
      "Epoch 46\n",
      "Epoch 46 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 7.130108135352308\n",
      "46, epoch_train_loss=7.130108135352308\n",
      "Epoch 47\n",
      "Epoch 47 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 7.298281879134176\n",
      "47, epoch_train_loss=7.298281879134176\n",
      "Epoch 48\n",
      "Epoch 48 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 7.186078002505767\n",
      "48, epoch_train_loss=7.186078002505767\n",
      "Epoch 49\n",
      "Epoch 49 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 6.839310197806178\n",
      "49, epoch_train_loss=6.839310197806178\n",
      "Epoch 50\n",
      "Epoch 50 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 6.395365082935501\n",
      "50, epoch_train_loss=6.395365082935501\n",
      "Epoch 51\n",
      "Epoch 51 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 6.004991330386296\n",
      "51, epoch_train_loss=6.004991330386296\n",
      "Epoch 52\n",
      "Epoch 52 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 5.7681307739371865\n",
      "52, epoch_train_loss=5.7681307739371865\n",
      "Epoch 53\n",
      "Epoch 53 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 5.706312765895923\n",
      "53, epoch_train_loss=5.706312765895923\n",
      "Epoch 54\n",
      "Epoch 54 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 5.772217148433129\n",
      "54, epoch_train_loss=5.772217148433129\n",
      "Epoch 55\n",
      "Epoch 55 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 5.882596899645913\n",
      "55, epoch_train_loss=5.882596899645913\n",
      "Epoch 56\n",
      "Epoch 56 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 5.9563907013926825\n",
      "56, epoch_train_loss=5.9563907013926825\n",
      "Epoch 57\n",
      "Epoch 57 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 5.942775279914039\n",
      "57, epoch_train_loss=5.942775279914039\n",
      "Epoch 58\n",
      "Epoch 58 :: Batch 0/1\n",
      "Batch Loss = 5.832419379409219\n",
      "58, epoch_train_loss=5.832419379409219\n",
      "Epoch 59\n",
      "Epoch 59 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 5.652861462986839\n",
      "59, epoch_train_loss=5.652861462986839\n",
      "Epoch 60\n",
      "Epoch 60 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 5.452866895034855\n",
      "60, epoch_train_loss=5.452866895034855\n",
      "Epoch 61\n",
      "Epoch 61 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 5.282123214181726\n",
      "61, epoch_train_loss=5.282123214181726\n",
      "Epoch 62\n",
      "Epoch 62 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 5.172922207867301\n",
      "62, epoch_train_loss=5.172922207867301\n",
      "Epoch 63\n",
      "Epoch 63 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 5.129790832571869\n",
      "63, epoch_train_loss=5.129790832571869\n",
      "Epoch 64\n",
      "Epoch 64 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 5.130713271230361\n",
      "64, epoch_train_loss=5.130713271230361\n",
      "Epoch 65\n",
      "Epoch 65 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 5.139242380715564\n",
      "65, epoch_train_loss=5.139242380715564\n",
      "Epoch 66\n",
      "Epoch 66 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 5.1218181833102765\n",
      "66, epoch_train_loss=5.1218181833102765\n",
      "Epoch 67\n",
      "Epoch 67 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 5.061978395004324\n",
      "67, epoch_train_loss=5.061978395004324\n",
      "Epoch 68\n",
      "Epoch 68 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 4.965015634266676\n",
      "68, epoch_train_loss=4.965015634266676\n",
      "Epoch 69\n",
      "Epoch 69 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 4.852051773251655\n",
      "69, epoch_train_loss=4.852051773251655\n",
      "Epoch 70\n",
      "Epoch 70 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 4.747946046123675\n",
      "70, epoch_train_loss=4.747946046123675\n",
      "Epoch 71\n",
      "Epoch 71 :: Batch 0/1\n",
      "Batch Loss = 4.669724393885189\n",
      "71, epoch_train_loss=4.669724393885189\n",
      "Epoch 72\n",
      "Epoch 72 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n",
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 4.620655297989225\n",
      "72, epoch_train_loss=4.620655297989225\n",
      "Epoch 73\n",
      "Epoch 73 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 4.591419115039264\n",
      "73, epoch_train_loss=4.591419115039264\n",
      "Epoch 74\n",
      "Epoch 74 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 4.566384867938147\n",
      "74, epoch_train_loss=4.566384867938147\n",
      "Epoch 75\n",
      "Epoch 75 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 4.5313136719703335\n",
      "75, epoch_train_loss=4.5313136719703335\n",
      "Epoch 76\n",
      "Epoch 76 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 4.479052150569882\n",
      "76, epoch_train_loss=4.479052150569882\n",
      "Epoch 77\n",
      "Epoch 77 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 4.411295666917683\n",
      "77, epoch_train_loss=4.411295666917683\n",
      "Epoch 78\n",
      "Epoch 78 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 4.336386907127048\n",
      "78, epoch_train_loss=4.336386907127048\n",
      "Epoch 79\n",
      "Epoch 79 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 4.2646470377744095\n",
      "79, epoch_train_loss=4.2646470377744095\n",
      "Epoch 80\n",
      "Epoch 80 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12030, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Loss = 4.203499225050835\n",
      "80, epoch_train_loss=4.203499225050835\n",
      "Epoch 81\n",
      "Epoch 81 :: Batch 0/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7452a39d66b0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/awills/anaconda3/envs/pyscfad/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PRINT_EVERY=200\n",
    "scheduler = optax.exponential_decay(init_value = 1e-2, transition_begin=50, transition_steps=1000, decay_rate=0.9)\n",
    "# optimizer = optax.adam(learning_rate = 1e-2)\n",
    "optimizer = optax.adam(learning_rate = scheduler)\n",
    "\n",
    "class PT_E_Loss():\n",
    "\n",
    "    def __call__(self, model, inp, ref):\n",
    "\n",
    "        pred = jax.vmap(model.net)(inp)[:, 0]\n",
    "\n",
    "        err = pred-ref\n",
    "\n",
    "        return jnp.mean(jnp.square(err))\n",
    "\n",
    "trainer = xce.train.xcTrainer(model=thislocal, optim=optimizer, steps=100000, loss = PT_E_Loss(), do_jit=True)\n",
    "if TRAIN_NET == 'x':\n",
    "    inp = [tdrho[:, trainer.model.use]]\n",
    "else:\n",
    "    inp = [tdrho]\n",
    "with jax.default_device(cpus[0]):\n",
    "    newm = trainer(1, trainer.model, inp, [tFxc])\n",
    "        \n",
    "\n",
    "# for epoch in range(100000):\n",
    "#     total_loss = 0\n",
    "#     results = thislocal(tdrho[::])\n",
    "#     loss = eloss(results, tFxc[::])\n",
    "#     total_loss += loss.item()\n",
    "#     loss.backward()\n",
    "\n",
    "#     optimizer.step()\n",
    "#     optimizer.zero_grad()\n",
    "#     if epoch%PRINT_EVERY==0:\n",
    "#         print('total loss {:.12f}'.format(total_loss))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a675396-1351-4aa7-a5a4-432373674a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.vmap(thislocal.net)(tdrho[:, [1,2]])[:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cc2779-5f7c-4bde-b2ea-dcc050cae720",
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.vmap(trainer.model.net)(tdrho[:, [1,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1803786d-0d21-4e9f-8be3-fed9839ae1df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
