no change     /gpfs/projects/FernandezGroup/Alec/miniconda3/condabin/conda
no change     /gpfs/projects/FernandezGroup/Alec/miniconda3/bin/conda
no change     /gpfs/projects/FernandezGroup/Alec/miniconda3/bin/conda-env
no change     /gpfs/projects/FernandezGroup/Alec/miniconda3/bin/activate
no change     /gpfs/projects/FernandezGroup/Alec/miniconda3/bin/deactivate
no change     /gpfs/projects/FernandezGroup/Alec/miniconda3/etc/profile.d/conda.sh
no change     /gpfs/projects/FernandezGroup/Alec/miniconda3/etc/fish/conf.d/conda.fish
no change     /gpfs/projects/FernandezGroup/Alec/miniconda3/shell/condabin/Conda.psm1
no change     /gpfs/projects/FernandezGroup/Alec/miniconda3/shell/condabin/conda-hook.ps1
no change     /gpfs/projects/FernandezGroup/Alec/miniconda3/lib/python3.9/site-packages/xontrib/conda.xsh
no change     /gpfs/projects/FernandezGroup/Alec/miniconda3/etc/profile.d/conda.csh
no change     /gpfs/home/jofranklin/.bashrc
No action taken.
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting "B3LYP_WITH_VWN5 = True" in pyscf_conf.py
  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '
Warning: <pyscf.gto.mole.Mole object at 0x7ffea82c7eb0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffea82c7eb0> in UKS object of <class 'pyscf.dft.uks.UKS'>
<pyscf.gto.mole.Mole object at 0x7ffea82c7eb0> [['P', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffea82c5990> [['N', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffea82c4f40> [['H', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffea8451d80> [['Li', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffea8451270> [['O', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffea84514b0> [['Cl', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffea8387e50> [['Al', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffea8265960> [['S', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffea8267760> [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]] 2
<pyscf.gto.mole.Mole object at 0x7ffea82679a0> [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]] 2
<pyscf.gto.mole.Mole object at 0x7ffea8266cb0> [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]] 2
<pyscf.gto.mole.Mole object at 0x7ffea829ac80> [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]] 3
<pyscf.gto.mole.Mole object at 0x7ffea829a5f0> [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]] 3
<pyscf.gto.mole.Mole object at 0x7ffea829a9b0> [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]] 2
<pyscf.gto.mole.Mole object at 0x7ffea8298ac0> [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]] 2
<pyscf.gto.mole.Mole object at 0x7ffea829a7a0> [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]] 2
<pyscf.gto.mole.Mole object at 0x7ffea829a620> [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]] 4
<pyscf.gto.mole.Mole object at 0x7ffea829bbe0> [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]] 2
<pyscf.gto.mole.Mole object at 0x7ffea829b100> [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]] 2
<pyscf.gto.mole.Mole object at 0x7ffea8298070> [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]] 2
<pyscf.gto.mole.Mole object at 0x7ffea829ab00> [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]] 2
<pyscf.gto.mole.Mole object at 0x7ffea829bc10> [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]] 4
<pyscf.gto.mole.Mole object at 0x7ffea805b610> [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]] 4
<pyscf.gto.mole.Mole object at 0x7ffea805a560> [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]] 2
<pyscf.gto.mole.Mole object at 0x7ffea805bfa0> [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]] 5
<pyscf.gto.mole.Mole object at 0x7ffea80590c0> [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]] 3
<pyscf.gto.mole.Mole object at 0x7ffea8059f90> [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]] 5
mol:  [['P', array([0., 0., 0.])]]
converged SCF energy = -341.104145992718  <S^2> = 3.7502984  2S+1 = 4.0001492
Warning: <pyscf.gto.mole.Mole object at 0x7ffea82c5990> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffea82c5990> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00051022 -0.00019156 -0.00051334 ... -0.02830887 -0.02830887
 -0.02830887] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 3)
mol:  [['N', array([0., 0., 0.])]]
converged SCF energy = -54.5289742046674  <S^2> = 3.7524945  2S+1 = 4.0012471
Warning: <pyscf.gto.mole.Mole object at 0x7ffea82c4f40> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffea82c4f40> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5016, 22)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-3.60081838e-04 -1.08775305e-04 -1.31917160e-05 ... -2.74817476e-02
 -2.74817476e-02 -2.74817476e-02] = ,SCAN
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5016, 22)
rho shape (4, 4, 2, 5016)
rho_filt shape: (5016,)
get descriptors tdrho.shape=(5016, 3)
mol:  [['H', array([0., 0., 0.])]]
converged SCF energy = -0.499812984008539  <S^2> = 0.75  2S+1 = 2
Warning: <pyscf.gto.mole.Mole object at 0x7ffea8451d80> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffea8451d80> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 7, 7)
ao.shape (10, 2440, 7)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.92637327e-09 -1.31700807e-07 -9.61527370e-06 ... -7.49400542e-16
 -7.49400542e-16 -7.49400542e-16] = ,SCAN
get_data, dm shape = (2, 7, 7)
ao_eval.shape=(4, 2440, 7)
rho shape (4, 4, 2, 2440)
rho_filt shape: (2440,)
get descriptors tdrho.shape=(2440, 3)
mol:  [['Li', array([0., 0., 0.])]]
converged SCF energy = -7.46006188627842  <S^2> = 0.75000049  2S+1 = 2.0000005
Warning: <pyscf.gto.mole.Mole object at 0x7ffea8451270> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffea8451270> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 4592, 22)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-5.31884219e-04 -2.81911891e-04 -2.81911891e-04 ... -1.27154711e-05
 -2.64861768e-02 -2.64861768e-02] = ,SCAN
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 4592, 22)
rho shape (4, 4, 2, 4592)
rho_filt shape: (4592,)
get descriptors tdrho.shape=(4592, 3)
mol:  [['O', array([0., 0., 0.])]]
converged SCF energy = -75.0033774430142  <S^2> = 2.0027452  2S+1 = 3.0018296
Warning: <pyscf.gto.mole.Mole object at 0x7ffea84514b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffea84514b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5040, 22)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.96773310e-04 -3.63598160e-05 -1.93123529e-06 ... -2.76158570e-02
 -2.76158570e-02 -2.76158570e-02] = ,SCAN
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5040, 22)
rho shape (4, 4, 2, 5040)
rho_filt shape: (5040,)
get descriptors tdrho.shape=(5040, 3)
mol:  [['Cl', array([0., 0., 0.])]]
converged SCF energy = -459.957577123229  <S^2> = 0.75161941  2S+1 = 2.0016188
Warning: <pyscf.gto.mole.Mole object at 0x7ffea8387e50> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffea8387e50> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6152, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.91421889e-04 -2.43214678e-04 -8.28519574e-05 ... -2.84484387e-02
 -2.84484387e-02 -2.84484387e-02] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6152, 30)
rho shape (4, 4, 2, 6152)
rho_filt shape: (6152,)
get descriptors tdrho.shape=(6152, 3)
mol:  [['Al', array([0., 0., 0.])]]
converged SCF energy = -242.226560989111  <S^2> = 0.75226414  2S+1 = 2.0022629
Warning: <pyscf.gto.mole.Mole object at 0x7ffea8265960> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffea8265960> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6088, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.12266318e-03 -1.34614087e-03 -6.89688299e-04 ... -2.71425517e-05
 -1.90183895e-04 -1.52027276e-05] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6088, 30)
rho shape (4, 4, 2, 6088)
rho_filt shape: (6088,)
get descriptors tdrho.shape=(6088, 3)
mol:  [['S', array([0., 0., 0.])]]
converged SCF energy = -397.938786818534  <S^2> = 2.0022329  2S+1 = 3.0014882
Warning: <pyscf.gto.mole.Mole object at 0x7ffea8267760> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffea8267760> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00037527 -0.00017737 -0.00022881 ... -0.02838402 -0.02838402
 -0.02838402] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 3)
mol:  [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]]
converged SCF energy = -1.16580491182912  <S^2> = -4.4408921e-16  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffea82679a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffea82679a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 14, 14)
ao.shape (10, 4776, 14)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.43725660e-05 -1.02204687e-06 -4.05575842e-05 ... -2.36278434e-02
 -2.36278434e-02 -2.36278434e-02] = ,SCAN
get_data, dm shape = (2, 14, 14)
ao_eval.shape=(4, 4776, 14)
rho shape (4, 4, 2, 4776)
rho_filt shape: (4776,)
get descriptors tdrho.shape=(4776, 3)
mol:  [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]]
converged SCF energy = -109.439263799566  <S^2> = 1.9539925e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffea8266cb0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffea8266cb0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9848, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.89629699e-05 -2.76172354e-04 -7.59017288e-05 ... -7.34654212e-06
 -7.34654212e-06 -2.89629699e-05] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9848, 44)
rho shape (4, 4, 2, 9848)
rho_filt shape: (9848,)
get descriptors tdrho.shape=(9848, 3)
mol:  [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]]
converged SCF energy = -107.339357395734  <S^2> = 0  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffea829ac80> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffea829ac80> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9752, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00043469 -0.00024024 -0.00035532 ... -0.00047537 -0.03728133
 -0.03728133] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9752, 44)
rho shape (4, 4, 2, 9752)
rho_filt shape: (9752,)
get descriptors tdrho.shape=(9752, 3)
mol:  [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]]
converged SCF energy = -93.337792446513  <S^2> = 4.0072479e-10  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffea829a5f0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffea829a5f0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 12256, 51)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-9.02468888e-05 -7.92694658e-06 -9.80568469e-06 ... -4.33714150e-02
 -4.33714150e-02 -4.33714150e-02] = ,SCAN
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 12256, 51)
rho shape (4, 4, 2, 12256)
rho_filt shape: (12256,)
get descriptors tdrho.shape=(12256, 3)
mol:  [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]]
converged SCF energy = -188.456965322843  <S^2> = 1.0658141e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffea829a9b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffea829a9b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 66, 66)
ao.shape (10, 14920, 66)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-7.48187338e-05 -6.19475249e-05 -2.61742784e-04 ... -8.70042314e-07
 -2.73391097e-02 -2.73391097e-02] = ,SCAN
get_data, dm shape = (2, 66, 66)
ao_eval.shape=(4, 14920, 66)
rho shape (4, 4, 2, 14920)
rho_filt shape: (14920,)
get descriptors tdrho.shape=(14920, 3)
mol:  [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]]
converged SCF energy = -920.005608888959  <S^2> = 5.0448534e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffea8298ac0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffea8298ac0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12208, 60)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00051559 -0.00027432 -0.00088583 ... -0.00027432 -0.04174728
 -0.04174728] = ,SCAN
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12208, 60)
rho shape (4, 4, 2, 12208)
rho_filt shape: (12208,)
get descriptors tdrho.shape=(12208, 3)
mol:  [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]]
converged SCF energy = -199.394370591172  <S^2> = 1.2079227e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffea829a7a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffea829a7a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9824, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-5.53951178e-05 -5.93507199e-06 -3.10072916e-04 ... -5.94325581e-02
 -5.94325581e-02 -5.94325581e-02] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9824, 44)
rho shape (4, 4, 2, 9824)
rho_filt shape: (9824,)
get descriptors tdrho.shape=(9824, 3)
mol:  [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]]
converged SCF energy = -150.214894506342  <S^2> = 1.0018599  2S+1 = 2.2377309
Warning: <pyscf.gto.mole.Mole object at 0x7ffea829a620> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffea829a620> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9912, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.62902008e-04 -3.05440987e-05 -1.65849117e-06 ... -4.22396765e-02
 -4.22396765e-02 -4.22396765e-02] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9912, 44)
rho shape (4, 4, 2, 9912)
rho_filt shape: (9912,)
get descriptors tdrho.shape=(9912, 3)
mol:  [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]]
converged SCF energy = -77.2435048346374  <S^2> = 7.1054274e-15  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffea829bbe0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffea829bbe0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 58, 58)
ao.shape (10, 15208, 58)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.72190712e-05 -2.72190712e-05 -2.84904833e-04 ... -1.08108260e-05
 -1.03072478e-05 -1.03072478e-05] = ,SCAN
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15208, 58)
rho shape (4, 4, 2, 15208)
rho_filt shape: (15208,)
get descriptors tdrho.shape=(15208, 3)
mol:  [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]]
converged SCF energy = -113.221335689652  <S^2> = 6.5991657e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffea829b100> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffea829b100> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 10040, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00015688 -0.00024669 -0.00068269 ... -0.03791166 -0.03791166
 -0.03791166] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 10040, 44)
rho shape (4, 4, 2, 10040)
rho_filt shape: (10040,)
get descriptors tdrho.shape=(10040, 3)
mol:  [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]]
converged SCF energy = -460.624592374078  <S^2> = 6.2172489e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffea8298070> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffea8298070> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 37, 37)
ao.shape (10, 8552, 37)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-7.28500681e-05 -5.65091132e-06 -7.37932132e-06 ... -4.76689214e-02
 -4.76689214e-02 -4.76689214e-02] = ,SCAN
get_data, dm shape = (2, 37, 37)
ao_eval.shape=(4, 8552, 37)
rho shape (4, 4, 2, 8552)
rho_filt shape: (8552,)
get descriptors tdrho.shape=(8552, 3)
mol:  [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]]
converged SCF energy = -8.04458854018922  <S^2> = 7.7049478e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffea829ab00> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffea829ab00> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 29, 29)
ao.shape (10, 6936, 29)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.0003863  -0.00040095 -0.00040095 ... -0.0213199  -0.0213199
 -0.0213199 ] = ,SCAN
get_data, dm shape = (2, 29, 29)
ao_eval.shape=(4, 6936, 29)
rho shape (4, 4, 2, 6936)
rho_filt shape: (6936,)
get descriptors tdrho.shape=(6936, 3)
mol:  [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]]
converged SCF energy = -324.340512506578  <S^2> = 1.5859314e-11  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffea829bc10> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffea829bc10> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 11536, 60)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00088473 -0.00088473 -0.00116894 ... -0.00088473 -0.00088473
 -0.00116894] = ,SCAN
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 11536, 60)
rho shape (4, 4, 2, 11536)
rho_filt shape: (11536,)
get descriptors tdrho.shape=(11536, 3)
mol:  [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]]
converged SCF energy = -1622.57507845815  <S^2> = 8.56204e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffea805b610> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffea805b610> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 120, 120)
ao.shape (10, 24512, 120)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.91408540e-05 -1.46971271e-04 -1.08734417e-03 ... -2.81566369e-02
 -2.81566369e-02 -2.81566369e-02] = ,SCAN
get_data, dm shape = (2, 120, 120)
ao_eval.shape=(4, 24512, 120)
rho shape (4, 4, 2, 24512)
rho_filt shape: (24512,)
get descriptors tdrho.shape=(24512, 3)
mol:  [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]]
converged SCF energy = -342.979728469575  <S^2> = 2.5403679e-11  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffea805a560> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffea805a560> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 13096, 51)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.39373335e-04 -1.31641332e-04 -1.15950750e-05 ... -7.32416564e-02
 -7.32416564e-02 -7.32416564e-02] = ,SCAN
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 13096, 51)
rho shape (4, 4, 2, 13096)
rho_filt shape: (13096,)
get descriptors tdrho.shape=(13096, 3)
mol:  [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]]
converged SCF energy = -578.56533589715  <S^2> = 1.0034705  2S+1 = 2.2391699
Warning: <pyscf.gto.mole.Mole object at 0x7ffea805bfa0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffea805bfa0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12384, 60)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.84672518e-05 -7.80550752e-05 -7.80527699e-05 ... -2.92531316e-02
 -2.92531316e-02 -2.92531316e-02] = ,SCAN
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12384, 60)
rho shape (4, 4, 2, 12384)
rho_filt shape: (12384,)
get descriptors tdrho.shape=(12384, 3)
mol:  [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]]
converged SCF energy = -40.4598214864076  <S^2> = 3.1619152e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffea80590c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffea80590c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 50, 50)
ao.shape (10, 13936, 50)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.56165538e-04 -7.34744214e-05 -5.30574304e-06 ... -7.93995702e-06
 -7.93995702e-06 -7.93995702e-06] = ,SCAN
get_data, dm shape = (2, 50, 50)
ao_eval.shape=(4, 13936, 50)
rho shape (4, 4, 2, 13936)
rho_filt shape: (13936,)
get descriptors tdrho.shape=(13936, 3)
mol:  [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]]
converged SCF energy = -39.0756147483504  <S^2> = 6.1959327e-12  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffea8059f90> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffea8059f90> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 36, 36)
ao.shape (10, 9656, 36)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.28290753e-04 -1.41305449e-05 -6.13700492e-05 ... -2.47993463e-02
 -2.47993463e-02 -2.47993463e-02] = ,SCAN
get_data, dm shape = (2, 36, 36)
ao_eval.shape=(4, 9656, 36)
rho shape (4, 4, 2, 9656)
rho_filt shape: (9656,)
get descriptors tdrho.shape=(9656, 3)
mol:  [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]]
converged SCF energy = -291.719272437819  <S^2> = 1.3143264e-11  2S+1 = 1
New DM shape: (2, 58, 58)
ao.shape (10, 15256, 58)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.45512011e-04 -7.12775692e-05 -5.48666345e-06 ... -6.02613084e-06
 -6.02613084e-06 -6.02613084e-06] = ,SCAN
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15256, 58)
rho shape (4, 4, 2, 15256)
rho_filt shape: (15256,)
get descriptors tdrho.shape=(15256, 3)
PRE NAN FILT: tFxc.shape=(237019,), tdrho.shape=(237019, 3)
nan_filt_rho.shape=(237019,)
nan_filt_fxc.shape=(237019,)
tFxc.shape=(237019,), tdrho.shape=(237019, 3)
inp[0].shape = (237019, 1)
Epoch 0
Epoch 0 :: Batch 0/1
Batch Loss = 0.5789347061118412
0, epoch_train_loss=0.5789347061118412
Epoch 1
Epoch 1 :: Batch 0/1
Batch Loss = 0.3768205364893897
1, epoch_train_loss=0.3768205364893897
Epoch 2
Epoch 2 :: Batch 0/1
Batch Loss = 0.17747060042500637
2, epoch_train_loss=0.17747060042500637
Epoch 3
Epoch 3 :: Batch 0/1
Batch Loss = 0.034476045579425664
3, epoch_train_loss=0.034476045579425664
Epoch 4
Epoch 4 :: Batch 0/1
Batch Loss = 0.0034202230705574705
4, epoch_train_loss=0.0034202230705574705
Epoch 5
Epoch 5 :: Batch 0/1
Batch Loss = 0.0009572608200704835
5, epoch_train_loss=0.0009572608200704835
Epoch 6
Epoch 6 :: Batch 0/1
Batch Loss = 0.0007504359424002669
6, epoch_train_loss=0.0007504359424002669
Epoch 7
Epoch 7 :: Batch 0/1
Batch Loss = 0.0007323710899640648
7, epoch_train_loss=0.0007323710899640648
Epoch 8
Epoch 8 :: Batch 0/1
Batch Loss = 0.0007310737939850745
8, epoch_train_loss=0.0007310737939850745
Epoch 9
Epoch 9 :: Batch 0/1
Batch Loss = 0.0007309959412234328
9, epoch_train_loss=0.0007309959412234328
Epoch 10
Epoch 10 :: Batch 0/1
Batch Loss = 0.0007309917800276888
10, epoch_train_loss=0.0007309917800276888
Epoch 11
Epoch 11 :: Batch 0/1
Batch Loss = 0.0007309915705617969
11, epoch_train_loss=0.0007309915705617969
Epoch 12
Epoch 12 :: Batch 0/1
Batch Loss = 0.0007309915601765311
12, epoch_train_loss=0.0007309915601765311
Epoch 13
Epoch 13 :: Batch 0/1
Batch Loss = 0.0007309915596507914
13, epoch_train_loss=0.0007309915596507914
Epoch 14
Epoch 14 :: Batch 0/1
Batch Loss = 0.0007309915596228176
14, epoch_train_loss=0.0007309915596228176
Epoch 15
Epoch 15 :: Batch 0/1
Batch Loss = 0.0007309915596212166
15, epoch_train_loss=0.0007309915596212166
Epoch 16
Epoch 16 :: Batch 0/1
Batch Loss = 0.0007309915596211163
16, epoch_train_loss=0.0007309915596211163
Epoch 17
Epoch 17 :: Batch 0/1
Batch Loss = 0.0007309915596211094
17, epoch_train_loss=0.0007309915596211094
Epoch 18
Epoch 18 :: Batch 0/1
Batch Loss = 0.0007309915596211087
18, epoch_train_loss=0.0007309915596211087
Epoch 19
Epoch 19 :: Batch 0/1
Batch Loss = 0.0007309915596211087
19, epoch_train_loss=0.0007309915596211087
Epoch 20
Epoch 20 :: Batch 0/1
Batch Loss = 0.0007309915596211087
20, epoch_train_loss=0.0007309915596211087
Epoch 21
Epoch 21 :: Batch 0/1
Batch Loss = 0.0007309915596211087
21, epoch_train_loss=0.0007309915596211087
Epoch 22
Epoch 22 :: Batch 0/1
Batch Loss = 0.0007309915596211087
22, epoch_train_loss=0.0007309915596211087
Epoch 23
Epoch 23 :: Batch 0/1
Batch Loss = 0.0007309915596211087
23, epoch_train_loss=0.0007309915596211087
Epoch 24
Epoch 24 :: Batch 0/1
Batch Loss = 0.0007309915596211087
24, epoch_train_loss=0.0007309915596211087
Epoch 25
Epoch 25 :: Batch 0/1
Batch Loss = 0.0007309915596211087
25, epoch_train_loss=0.0007309915596211087
Epoch 26
Epoch 26 :: Batch 0/1
Batch Loss = 0.0007309915596211087
26, epoch_train_loss=0.0007309915596211087
Epoch 27
Epoch 27 :: Batch 0/1
Batch Loss = 0.0007309915596211087
27, epoch_train_loss=0.0007309915596211087
Epoch 28
Epoch 28 :: Batch 0/1
Batch Loss = 0.0007309915596211087
28, epoch_train_loss=0.0007309915596211087
Epoch 29
Epoch 29 :: Batch 0/1
Batch Loss = 0.0007309915596211087
29, epoch_train_loss=0.0007309915596211087
Epoch 30
Epoch 30 :: Batch 0/1
Batch Loss = 0.0007309915596211087
30, epoch_train_loss=0.0007309915596211087
Epoch 31
Epoch 31 :: Batch 0/1
Batch Loss = 0.0007309915596211087
31, epoch_train_loss=0.0007309915596211087
Epoch 32
Epoch 32 :: Batch 0/1
Batch Loss = 0.0007309915596211087
32, epoch_train_loss=0.0007309915596211087
Epoch 33
Epoch 33 :: Batch 0/1
Batch Loss = 0.0007309915596211087
33, epoch_train_loss=0.0007309915596211087
Epoch 34
Epoch 34 :: Batch 0/1
Batch Loss = 0.0007309915596211087
34, epoch_train_loss=0.0007309915596211087
Epoch 35
Epoch 35 :: Batch 0/1
Batch Loss = 0.0007309915596211087
35, epoch_train_loss=0.0007309915596211087
Epoch 36
Epoch 36 :: Batch 0/1
Batch Loss = 0.0007309915596211087
36, epoch_train_loss=0.0007309915596211087
Epoch 37
Epoch 37 :: Batch 0/1
Batch Loss = 0.0007309915596211087
37, epoch_train_loss=0.0007309915596211087
Epoch 38
Epoch 38 :: Batch 0/1
Batch Loss = 0.0007309915596211087
38, epoch_train_loss=0.0007309915596211087
Epoch 39
Epoch 39 :: Batch 0/1
Batch Loss = 0.0007309915596211087
39, epoch_train_loss=0.0007309915596211087
Epoch 40
Epoch 40 :: Batch 0/1
Batch Loss = 0.0007309915596211087
40, epoch_train_loss=0.0007309915596211087
Epoch 41
Epoch 41 :: Batch 0/1
Batch Loss = 0.0007309915596211087
41, epoch_train_loss=0.0007309915596211087
Epoch 42
Epoch 42 :: Batch 0/1
Batch Loss = 0.0007309915596211087
42, epoch_train_loss=0.0007309915596211087
Epoch 43
Epoch 43 :: Batch 0/1
Batch Loss = 0.0007309915596211087
43, epoch_train_loss=0.0007309915596211087
Epoch 44
Epoch 44 :: Batch 0/1
Batch Loss = 0.0007309915596211087
44, epoch_train_loss=0.0007309915596211087
Epoch 45
Epoch 45 :: Batch 0/1
Batch Loss = 0.0007309915596211087
45, epoch_train_loss=0.0007309915596211087
Epoch 46
Epoch 46 :: Batch 0/1
Batch Loss = 0.0007309915596211087
46, epoch_train_loss=0.0007309915596211087
Epoch 47
Epoch 47 :: Batch 0/1
Batch Loss = 0.0007309915596211087
47, epoch_train_loss=0.0007309915596211087
Epoch 48
Epoch 48 :: Batch 0/1
Batch Loss = 0.0007309915596211087
48, epoch_train_loss=0.0007309915596211087
Epoch 49
Epoch 49 :: Batch 0/1
Batch Loss = 0.0007309915596211087
49, epoch_train_loss=0.0007309915596211087
Epoch 50
Epoch 50 :: Batch 0/1
Batch Loss = 0.0007309915596211087
50, epoch_train_loss=0.0007309915596211087
Epoch 51
Epoch 51 :: Batch 0/1
Batch Loss = 0.0007309915596211087
51, epoch_train_loss=0.0007309915596211087
Epoch 52
Epoch 52 :: Batch 0/1
Batch Loss = 0.0007309915596211087
52, epoch_train_loss=0.0007309915596211087
Epoch 53
Epoch 53 :: Batch 0/1
Batch Loss = 0.0007309915596211087
53, epoch_train_loss=0.0007309915596211087
Epoch 54
Epoch 54 :: Batch 0/1
Batch Loss = 0.0007309915596211087
54, epoch_train_loss=0.0007309915596211087
Epoch 55
Epoch 55 :: Batch 0/1
Batch Loss = 0.0007309915596211087
55, epoch_train_loss=0.0007309915596211087
Epoch 56
Epoch 56 :: Batch 0/1
Batch Loss = 0.0007309915596211087
56, epoch_train_loss=0.0007309915596211087
Epoch 57
Epoch 57 :: Batch 0/1
Batch Loss = 0.0007309915596211087
57, epoch_train_loss=0.0007309915596211087
Epoch 58
Epoch 58 :: Batch 0/1
Batch Loss = 0.0007309915596211087
58, epoch_train_loss=0.0007309915596211087
Epoch 59
Epoch 59 :: Batch 0/1
Batch Loss = 0.0007309915596211087
59, epoch_train_loss=0.0007309915596211087
Epoch 60
Epoch 60 :: Batch 0/1
Batch Loss = 0.0007309915596211087
60, epoch_train_loss=0.0007309915596211087
Epoch 61
Epoch 61 :: Batch 0/1
Batch Loss = 0.0007309915596211087
61, epoch_train_loss=0.0007309915596211087
Epoch 62
Epoch 62 :: Batch 0/1
Batch Loss = 0.0007309915596211087
62, epoch_train_loss=0.0007309915596211087
Epoch 63
Epoch 63 :: Batch 0/1
Batch Loss = 0.0007309915596211087
63, epoch_train_loss=0.0007309915596211087
Epoch 64
Epoch 64 :: Batch 0/1
Batch Loss = 0.0007309915596211087
64, epoch_train_loss=0.0007309915596211087
Epoch 65
Epoch 65 :: Batch 0/1
Batch Loss = 0.0007309915596211087
65, epoch_train_loss=0.0007309915596211087
Epoch 66
Epoch 66 :: Batch 0/1
Batch Loss = 0.0007309915596211087
66, epoch_train_loss=0.0007309915596211087
Epoch 67
Epoch 67 :: Batch 0/1
Batch Loss = 0.0007309915596211087
67, epoch_train_loss=0.0007309915596211087
Epoch 68
Epoch 68 :: Batch 0/1
Batch Loss = 0.0007309915596211087
68, epoch_train_loss=0.0007309915596211087
Epoch 69
Epoch 69 :: Batch 0/1
Batch Loss = 0.0007309915596211087
69, epoch_train_loss=0.0007309915596211087
Epoch 70
Epoch 70 :: Batch 0/1
Batch Loss = 0.0007309915596211087
70, epoch_train_loss=0.0007309915596211087
Epoch 71
Epoch 71 :: Batch 0/1
Batch Loss = 0.0007309915596211087
71, epoch_train_loss=0.0007309915596211087
Epoch 72
Epoch 72 :: Batch 0/1
Batch Loss = 0.0007309915596211087
72, epoch_train_loss=0.0007309915596211087
Epoch 73
Epoch 73 :: Batch 0/1
Batch Loss = 0.0007309915596211087
73, epoch_train_loss=0.0007309915596211087
Epoch 74
Epoch 74 :: Batch 0/1
Batch Loss = 0.0007309915596211087
74, epoch_train_loss=0.0007309915596211087
Epoch 75
Epoch 75 :: Batch 0/1
Batch Loss = 0.0007309915596211087
75, epoch_train_loss=0.0007309915596211087
Epoch 76
Epoch 76 :: Batch 0/1
Batch Loss = 0.0007309915596211087
76, epoch_train_loss=0.0007309915596211087
Epoch 77
Epoch 77 :: Batch 0/1
Batch Loss = 0.0007309915596211087
77, epoch_train_loss=0.0007309915596211087
Epoch 78
Epoch 78 :: Batch 0/1
Batch Loss = 0.0007309915596211087
78, epoch_train_loss=0.0007309915596211087
Epoch 79
Epoch 79 :: Batch 0/1
Batch Loss = 0.0007309915596211087
79, epoch_train_loss=0.0007309915596211087
Epoch 80
Epoch 80 :: Batch 0/1
Batch Loss = 0.0007309915596211087
80, epoch_train_loss=0.0007309915596211087
Epoch 81
Epoch 81 :: Batch 0/1
Batch Loss = 0.0007309915596211087
81, epoch_train_loss=0.0007309915596211087
Epoch 82
Epoch 82 :: Batch 0/1
Batch Loss = 0.0007309915596211087
82, epoch_train_loss=0.0007309915596211087
Epoch 83
Epoch 83 :: Batch 0/1
Batch Loss = 0.0007309915596211087
83, epoch_train_loss=0.0007309915596211087
Epoch 84
Epoch 84 :: Batch 0/1
Batch Loss = 0.0007309915596211087
84, epoch_train_loss=0.0007309915596211087
Epoch 85
Epoch 85 :: Batch 0/1
Batch Loss = 0.0007309915596211087
85, epoch_train_loss=0.0007309915596211087
Epoch 86
Epoch 86 :: Batch 0/1
Batch Loss = 0.0007309915596211087
86, epoch_train_loss=0.0007309915596211087
Epoch 87
Epoch 87 :: Batch 0/1
Batch Loss = 0.0007309915596211087
87, epoch_train_loss=0.0007309915596211087
Epoch 88
Epoch 88 :: Batch 0/1
Batch Loss = 0.0007309915596211087
88, epoch_train_loss=0.0007309915596211087
Epoch 89
Epoch 89 :: Batch 0/1
Batch Loss = 0.0007309915596211087
89, epoch_train_loss=0.0007309915596211087
Epoch 90
Epoch 90 :: Batch 0/1
Batch Loss = 0.0007309915596211087
90, epoch_train_loss=0.0007309915596211087
Epoch 91
Epoch 91 :: Batch 0/1
Batch Loss = 0.0007309915596211087
91, epoch_train_loss=0.0007309915596211087
Epoch 92
Epoch 92 :: Batch 0/1
Batch Loss = 0.0007309915596211087
92, epoch_train_loss=0.0007309915596211087
Epoch 93
Epoch 93 :: Batch 0/1
Batch Loss = 0.0007309915596211087
93, epoch_train_loss=0.0007309915596211087
Epoch 94
Epoch 94 :: Batch 0/1
Batch Loss = 0.0007309915596211087
94, epoch_train_loss=0.0007309915596211087
Epoch 95
Epoch 95 :: Batch 0/1
Batch Loss = 0.0007309915596211087
95, epoch_train_loss=0.0007309915596211087
Epoch 96
Epoch 96 :: Batch 0/1
Batch Loss = 0.0007309915596211087
96, epoch_train_loss=0.0007309915596211087
Epoch 97
Epoch 97 :: Batch 0/1
Batch Loss = 0.0007309915596211087
97, epoch_train_loss=0.0007309915596211087
Epoch 98
Epoch 98 :: Batch 0/1
Batch Loss = 0.0007309915596211087
98, epoch_train_loss=0.0007309915596211087
Epoch 99
Epoch 99 :: Batch 0/1
Batch Loss = 0.0007309915596211087
99, epoch_train_loss=0.0007309915596211087
Epoch 100
Epoch 100 :: Batch 0/1
Batch Loss = 0.0007309915596211087
100, epoch_train_loss=0.0007309915596211087
Epoch 101
Epoch 101 :: Batch 0/1
Batch Loss = 0.0007309915596211087
101, epoch_train_loss=0.0007309915596211087
Epoch 102
Epoch 102 :: Batch 0/1
Batch Loss = 0.0007309915596211087
102, epoch_train_loss=0.0007309915596211087
Epoch 103
Epoch 103 :: Batch 0/1
Batch Loss = 0.0007309915596211087
103, epoch_train_loss=0.0007309915596211087
Epoch 104
Epoch 104 :: Batch 0/1
Batch Loss = 0.0007309915596211087
104, epoch_train_loss=0.0007309915596211087
Epoch 105
Epoch 105 :: Batch 0/1
Batch Loss = 0.0007309915596211087
105, epoch_train_loss=0.0007309915596211087
Epoch 106
Epoch 106 :: Batch 0/1
Batch Loss = 0.0007309915596211087
106, epoch_train_loss=0.0007309915596211087
Epoch 107
Epoch 107 :: Batch 0/1
Batch Loss = 0.0007309915596211087
107, epoch_train_loss=0.0007309915596211087
Epoch 108
Epoch 108 :: Batch 0/1
Batch Loss = 0.0007309915596211087
108, epoch_train_loss=0.0007309915596211087
Epoch 109
Epoch 109 :: Batch 0/1
Batch Loss = 0.0007309915596211087
109, epoch_train_loss=0.0007309915596211087
Epoch 110
Epoch 110 :: Batch 0/1
Batch Loss = 0.0007309915596211087
110, epoch_train_loss=0.0007309915596211087
Epoch 111
Epoch 111 :: Batch 0/1
Batch Loss = 0.0007309915596211087
111, epoch_train_loss=0.0007309915596211087
Epoch 112
Epoch 112 :: Batch 0/1
Batch Loss = 0.0007309915596211087
112, epoch_train_loss=0.0007309915596211087
Epoch 113
Epoch 113 :: Batch 0/1
Batch Loss = 0.0007309915596211087
113, epoch_train_loss=0.0007309915596211087
Epoch 114
Epoch 114 :: Batch 0/1
Batch Loss = 0.0007309915596211087
114, epoch_train_loss=0.0007309915596211087
Epoch 115
Epoch 115 :: Batch 0/1
Batch Loss = 0.0007309915596211087
115, epoch_train_loss=0.0007309915596211087
Epoch 116
Epoch 116 :: Batch 0/1
Batch Loss = 0.0007309915596211087
116, epoch_train_loss=0.0007309915596211087
Epoch 117
Epoch 117 :: Batch 0/1
Batch Loss = 0.0007309915596211087
117, epoch_train_loss=0.0007309915596211087
Epoch 118
Epoch 118 :: Batch 0/1
Batch Loss = 0.0007309915596211087
118, epoch_train_loss=0.0007309915596211087
Epoch 119
Epoch 119 :: Batch 0/1
Batch Loss = 0.0007309915596211087
119, epoch_train_loss=0.0007309915596211087
Epoch 120
Epoch 120 :: Batch 0/1
Batch Loss = 0.0007309915596211087
120, epoch_train_loss=0.0007309915596211087
Epoch 121
Epoch 121 :: Batch 0/1
Batch Loss = 0.0007309915596211087
121, epoch_train_loss=0.0007309915596211087
Epoch 122
Epoch 122 :: Batch 0/1
Batch Loss = 0.0007309915596211087
122, epoch_train_loss=0.0007309915596211087
Epoch 123
Epoch 123 :: Batch 0/1
Batch Loss = 0.0007309915596211087
123, epoch_train_loss=0.0007309915596211087
Epoch 124
Epoch 124 :: Batch 0/1
Batch Loss = 0.0007309915596211087
124, epoch_train_loss=0.0007309915596211087
Epoch 125
Epoch 125 :: Batch 0/1
Batch Loss = 0.0007309915596211087
125, epoch_train_loss=0.0007309915596211087
Epoch 126
Epoch 126 :: Batch 0/1
Batch Loss = 0.0007309915596211087
126, epoch_train_loss=0.0007309915596211087
Epoch 127
Epoch 127 :: Batch 0/1
Batch Loss = 0.0007309915596211087
127, epoch_train_loss=0.0007309915596211087
Epoch 128
Epoch 128 :: Batch 0/1
Batch Loss = 0.0007309915596211087
128, epoch_train_loss=0.0007309915596211087
Epoch 129
Epoch 129 :: Batch 0/1
Batch Loss = 0.0007309915596211087
129, epoch_train_loss=0.0007309915596211087
Epoch 130
Epoch 130 :: Batch 0/1
Batch Loss = 0.0007309915596211087
130, epoch_train_loss=0.0007309915596211087
Epoch 131
Epoch 131 :: Batch 0/1
Batch Loss = 0.0007309915596211087
131, epoch_train_loss=0.0007309915596211087
Epoch 132
Epoch 132 :: Batch 0/1
Batch Loss = 0.0007309915596211087
132, epoch_train_loss=0.0007309915596211087
Epoch 133
Epoch 133 :: Batch 0/1
Batch Loss = 0.0007309915596211087
133, epoch_train_loss=0.0007309915596211087
Epoch 134
Epoch 134 :: Batch 0/1
Batch Loss = 0.0007309915596211087
134, epoch_train_loss=0.0007309915596211087
Epoch 135
Epoch 135 :: Batch 0/1
Batch Loss = 0.0007309915596211087
135, epoch_train_loss=0.0007309915596211087
Epoch 136
Epoch 136 :: Batch 0/1
Batch Loss = 0.0007309915596211087
136, epoch_train_loss=0.0007309915596211087
Epoch 137
Epoch 137 :: Batch 0/1
Batch Loss = 0.0007309915596211087
137, epoch_train_loss=0.0007309915596211087
Epoch 138
Epoch 138 :: Batch 0/1
Batch Loss = 0.0007309915596211087
138, epoch_train_loss=0.0007309915596211087
Epoch 139
Epoch 139 :: Batch 0/1
Batch Loss = 0.0007309915596211087
139, epoch_train_loss=0.0007309915596211087
Epoch 140
Epoch 140 :: Batch 0/1
Batch Loss = 0.0007309915596211087
140, epoch_train_loss=0.0007309915596211087
Epoch 141
Epoch 141 :: Batch 0/1
Batch Loss = 0.0007309915596211087
141, epoch_train_loss=0.0007309915596211087
Epoch 142
Epoch 142 :: Batch 0/1
Batch Loss = 0.0007309915596211087
142, epoch_train_loss=0.0007309915596211087
Epoch 143
Epoch 143 :: Batch 0/1
Batch Loss = 0.0007309915596211087
143, epoch_train_loss=0.0007309915596211087
Epoch 144
Epoch 144 :: Batch 0/1
Batch Loss = 0.0007309915596211087
144, epoch_train_loss=0.0007309915596211087
Epoch 145
Epoch 145 :: Batch 0/1
Batch Loss = 0.0007309915596211087
145, epoch_train_loss=0.0007309915596211087
Epoch 146
Epoch 146 :: Batch 0/1
Batch Loss = 0.0007309915596211087
146, epoch_train_loss=0.0007309915596211087
Epoch 147
Epoch 147 :: Batch 0/1
Batch Loss = 0.0007309915596211087
147, epoch_train_loss=0.0007309915596211087
Epoch 148
Epoch 148 :: Batch 0/1
Batch Loss = 0.0007309915596211087
148, epoch_train_loss=0.0007309915596211087
Epoch 149
Epoch 149 :: Batch 0/1
Batch Loss = 0.0007309915596211087
149, epoch_train_loss=0.0007309915596211087
Epoch 150
Epoch 150 :: Batch 0/1
Batch Loss = 0.0007309915596211087
150, epoch_train_loss=0.0007309915596211087
Epoch 151
Epoch 151 :: Batch 0/1
Batch Loss = 0.0007309915596211087
151, epoch_train_loss=0.0007309915596211087
Epoch 152
Epoch 152 :: Batch 0/1
Batch Loss = 0.0007309915596211087
152, epoch_train_loss=0.0007309915596211087
Epoch 153
Epoch 153 :: Batch 0/1
Batch Loss = 0.0007309915596211087
153, epoch_train_loss=0.0007309915596211087
Epoch 154
Epoch 154 :: Batch 0/1
Batch Loss = 0.0007309915596211087
154, epoch_train_loss=0.0007309915596211087
Epoch 155
Epoch 155 :: Batch 0/1
Batch Loss = 0.0007309915596211087
155, epoch_train_loss=0.0007309915596211087
Epoch 156
Epoch 156 :: Batch 0/1
Batch Loss = 0.0007309915596211087
156, epoch_train_loss=0.0007309915596211087
Epoch 157
Epoch 157 :: Batch 0/1
Batch Loss = 0.0007309915596211087
157, epoch_train_loss=0.0007309915596211087
Epoch 158
Epoch 158 :: Batch 0/1
Batch Loss = 0.0007309915596211087
158, epoch_train_loss=0.0007309915596211087
Epoch 159
Epoch 159 :: Batch 0/1
Batch Loss = 0.0007309915596211087
159, epoch_train_loss=0.0007309915596211087
Epoch 160
Epoch 160 :: Batch 0/1
Batch Loss = 0.0007309915596211087
160, epoch_train_loss=0.0007309915596211087
Epoch 161
Epoch 161 :: Batch 0/1
Batch Loss = 0.0007309915596211087
161, epoch_train_loss=0.0007309915596211087
Epoch 162
Epoch 162 :: Batch 0/1
Batch Loss = 0.0007309915596211087
162, epoch_train_loss=0.0007309915596211087
Epoch 163
Epoch 163 :: Batch 0/1
Batch Loss = 0.0007309915596211087
163, epoch_train_loss=0.0007309915596211087
Epoch 164
Epoch 164 :: Batch 0/1
Batch Loss = 0.0007309915596211087
164, epoch_train_loss=0.0007309915596211087
Epoch 165
Epoch 165 :: Batch 0/1
Batch Loss = 0.0007309915596211087
165, epoch_train_loss=0.0007309915596211087
Epoch 166
Epoch 166 :: Batch 0/1
Batch Loss = 0.0007309915596211087
166, epoch_train_loss=0.0007309915596211087
Epoch 167
Epoch 167 :: Batch 0/1
Batch Loss = 0.0007309915596211087
167, epoch_train_loss=0.0007309915596211087
Epoch 168
Epoch 168 :: Batch 0/1
Batch Loss = 0.0007309915596211087
168, epoch_train_loss=0.0007309915596211087
Epoch 169
Epoch 169 :: Batch 0/1
Batch Loss = 0.0007309915596211087
169, epoch_train_loss=0.0007309915596211087
Epoch 170
Epoch 170 :: Batch 0/1
Batch Loss = 0.0007309915596211087
170, epoch_train_loss=0.0007309915596211087
Epoch 171
Epoch 171 :: Batch 0/1
Batch Loss = 0.0007309915596211087
171, epoch_train_loss=0.0007309915596211087
Epoch 172
Epoch 172 :: Batch 0/1
Batch Loss = 0.0007309915596211087
172, epoch_train_loss=0.0007309915596211087
Epoch 173
Epoch 173 :: Batch 0/1
Batch Loss = 0.0007309915596211087
173, epoch_train_loss=0.0007309915596211087
Epoch 174
Epoch 174 :: Batch 0/1
Batch Loss = 0.0007309915596211087
174, epoch_train_loss=0.0007309915596211087
Epoch 175
Epoch 175 :: Batch 0/1
Batch Loss = 0.0007309915596211087
175, epoch_train_loss=0.0007309915596211087
Epoch 176
Epoch 176 :: Batch 0/1
Batch Loss = 0.0007309915596211087
176, epoch_train_loss=0.0007309915596211087
Epoch 177
Epoch 177 :: Batch 0/1
Batch Loss = 0.0007309915596211087
177, epoch_train_loss=0.0007309915596211087
Epoch 178
Epoch 178 :: Batch 0/1
Batch Loss = 0.0007309915596211087
178, epoch_train_loss=0.0007309915596211087
Epoch 179
Epoch 179 :: Batch 0/1
Batch Loss = 0.0007309915596211087
179, epoch_train_loss=0.0007309915596211087
Epoch 180
Epoch 180 :: Batch 0/1
Batch Loss = 0.0007309915596211087
180, epoch_train_loss=0.0007309915596211087
Epoch 181
Epoch 181 :: Batch 0/1
Batch Loss = 0.0007309915596211087
181, epoch_train_loss=0.0007309915596211087
Epoch 182
Epoch 182 :: Batch 0/1
Batch Loss = 0.0007309915596211087
182, epoch_train_loss=0.0007309915596211087
Epoch 183
Epoch 183 :: Batch 0/1
Batch Loss = 0.0007309915596211087
183, epoch_train_loss=0.0007309915596211087
Epoch 184
Epoch 184 :: Batch 0/1
Batch Loss = 0.0007309915596211087
184, epoch_train_loss=0.0007309915596211087
Epoch 185
Epoch 185 :: Batch 0/1
Batch Loss = 0.0007309915596211087
185, epoch_train_loss=0.0007309915596211087
Epoch 186
Epoch 186 :: Batch 0/1
Batch Loss = 0.0007309915596211087
186, epoch_train_loss=0.0007309915596211087
Epoch 187
Epoch 187 :: Batch 0/1
Batch Loss = 0.0007309915596211087
187, epoch_train_loss=0.0007309915596211087
Epoch 188
Epoch 188 :: Batch 0/1
Batch Loss = 0.0007309915596211087
188, epoch_train_loss=0.0007309915596211087
Epoch 189
Epoch 189 :: Batch 0/1
Batch Loss = 0.0007309915596211087
189, epoch_train_loss=0.0007309915596211087
Epoch 190
Epoch 190 :: Batch 0/1
Batch Loss = 0.0007309915596211087
190, epoch_train_loss=0.0007309915596211087
Epoch 191
Epoch 191 :: Batch 0/1
Batch Loss = 0.0007309915596211087
191, epoch_train_loss=0.0007309915596211087
Epoch 192
Epoch 192 :: Batch 0/1
Batch Loss = 0.0007309915596211087
192, epoch_train_loss=0.0007309915596211087
Epoch 193
Epoch 193 :: Batch 0/1
Batch Loss = 0.0007309915596211087
193, epoch_train_loss=0.0007309915596211087
Epoch 194
Epoch 194 :: Batch 0/1
Batch Loss = 0.0007309915596211087
194, epoch_train_loss=0.0007309915596211087
Epoch 195
Epoch 195 :: Batch 0/1
Batch Loss = 0.0007309915596211087
195, epoch_train_loss=0.0007309915596211087
Epoch 196
Epoch 196 :: Batch 0/1
Batch Loss = 0.0007309915596211087
196, epoch_train_loss=0.0007309915596211087
Epoch 197
Epoch 197 :: Batch 0/1
Batch Loss = 0.0007309915596211087
197, epoch_train_loss=0.0007309915596211087
Epoch 198
Epoch 198 :: Batch 0/1
Batch Loss = 0.0007309915596211087
198, epoch_train_loss=0.0007309915596211087
Epoch 199
Epoch 199 :: Batch 0/1
Batch Loss = 0.0007309915596211087
199, epoch_train_loss=0.0007309915596211087
Epoch 200
Epoch 200 :: Batch 0/1
Batch Loss = 0.0007309915596211087
200, epoch_train_loss=0.0007309915596211087
Epoch 201
Epoch 201 :: Batch 0/1
Batch Loss = 0.0007309915596211087
201, epoch_train_loss=0.0007309915596211087
Epoch 202
Epoch 202 :: Batch 0/1
Batch Loss = 0.0007309915596211087
202, epoch_train_loss=0.0007309915596211087
Epoch 203
Epoch 203 :: Batch 0/1
Batch Loss = 0.0007309915596211087
203, epoch_train_loss=0.0007309915596211087
Epoch 204
Epoch 204 :: Batch 0/1
Batch Loss = 0.0007309915596211087
204, epoch_train_loss=0.0007309915596211087
Epoch 205
Epoch 205 :: Batch 0/1
Batch Loss = 0.0007309915596211087
205, epoch_train_loss=0.0007309915596211087
Epoch 206
Epoch 206 :: Batch 0/1
Batch Loss = 0.0007309915596211087
206, epoch_train_loss=0.0007309915596211087
Epoch 207
Epoch 207 :: Batch 0/1
Batch Loss = 0.0007309915596211087
207, epoch_train_loss=0.0007309915596211087
Epoch 208
Epoch 208 :: Batch 0/1
Batch Loss = 0.0007309915596211087
208, epoch_train_loss=0.0007309915596211087
Epoch 209
Epoch 209 :: Batch 0/1
Batch Loss = 0.0007309915596211087
209, epoch_train_loss=0.0007309915596211087
Epoch 210
Epoch 210 :: Batch 0/1
Batch Loss = 0.0007309915596211087
210, epoch_train_loss=0.0007309915596211087
Epoch 211
Epoch 211 :: Batch 0/1
Batch Loss = 0.0007309915596211087
211, epoch_train_loss=0.0007309915596211087
Epoch 212
Epoch 212 :: Batch 0/1
Batch Loss = 0.0007309915596211087
212, epoch_train_loss=0.0007309915596211087
Epoch 213
Epoch 213 :: Batch 0/1
Batch Loss = 0.0007309915596211087
213, epoch_train_loss=0.0007309915596211087
Epoch 214
Epoch 214 :: Batch 0/1
Batch Loss = 0.0007309915596211087
214, epoch_train_loss=0.0007309915596211087
Epoch 215
Epoch 215 :: Batch 0/1
Batch Loss = 0.0007309915596211087
215, epoch_train_loss=0.0007309915596211087
Epoch 216
Epoch 216 :: Batch 0/1
Batch Loss = 0.0007309915596211087
216, epoch_train_loss=0.0007309915596211087
Epoch 217
Epoch 217 :: Batch 0/1
Batch Loss = 0.0007309915596211087
217, epoch_train_loss=0.0007309915596211087
Epoch 218
Epoch 218 :: Batch 0/1
Batch Loss = 0.0007309915596211087
218, epoch_train_loss=0.0007309915596211087
Epoch 219
Epoch 219 :: Batch 0/1
Batch Loss = 0.0007309915596211087
219, epoch_train_loss=0.0007309915596211087
Epoch 220
Epoch 220 :: Batch 0/1
Batch Loss = 0.0007309915596211087
220, epoch_train_loss=0.0007309915596211087
Epoch 221
Epoch 221 :: Batch 0/1
Batch Loss = 0.0007309915596211087
221, epoch_train_loss=0.0007309915596211087
Epoch 222
Epoch 222 :: Batch 0/1
Batch Loss = 0.0007309915596211087
222, epoch_train_loss=0.0007309915596211087
Epoch 223
Epoch 223 :: Batch 0/1
Batch Loss = 0.0007309915596211087
223, epoch_train_loss=0.0007309915596211087
Epoch 224
Epoch 224 :: Batch 0/1
Batch Loss = 0.0007309915596211087
224, epoch_train_loss=0.0007309915596211087
Epoch 225
Epoch 225 :: Batch 0/1
Batch Loss = 0.0007309915596211087
225, epoch_train_loss=0.0007309915596211087
Epoch 226
Epoch 226 :: Batch 0/1
Batch Loss = 0.0007309915596211087
226, epoch_train_loss=0.0007309915596211087
Epoch 227
Epoch 227 :: Batch 0/1
Batch Loss = 0.0007309915596211087
227, epoch_train_loss=0.0007309915596211087
Epoch 228
Epoch 228 :: Batch 0/1
Batch Loss = 0.0007309915596211087
228, epoch_train_loss=0.0007309915596211087
Epoch 229
Epoch 229 :: Batch 0/1
Batch Loss = 0.0007309915596211087
229, epoch_train_loss=0.0007309915596211087
Epoch 230
Epoch 230 :: Batch 0/1
Batch Loss = 0.0007309915596211087
230, epoch_train_loss=0.0007309915596211087
Epoch 231
Epoch 231 :: Batch 0/1
Batch Loss = 0.0007309915596211087
231, epoch_train_loss=0.0007309915596211087
Epoch 232
Epoch 232 :: Batch 0/1
Batch Loss = 0.0007309915596211087
232, epoch_train_loss=0.0007309915596211087
Epoch 233
Epoch 233 :: Batch 0/1
Batch Loss = 0.0007309915596211087
233, epoch_train_loss=0.0007309915596211087
Epoch 234
Epoch 234 :: Batch 0/1
Batch Loss = 0.0007309915596211087
234, epoch_train_loss=0.0007309915596211087
Epoch 235
Epoch 235 :: Batch 0/1
Batch Loss = 0.0007309915596211087
235, epoch_train_loss=0.0007309915596211087
Epoch 236
Epoch 236 :: Batch 0/1
Batch Loss = 0.0007309915596211087
236, epoch_train_loss=0.0007309915596211087
Epoch 237
Epoch 237 :: Batch 0/1
Batch Loss = 0.0007309915596211087
237, epoch_train_loss=0.0007309915596211087
Epoch 238
Epoch 238 :: Batch 0/1
Batch Loss = 0.0007309915596211087
238, epoch_train_loss=0.0007309915596211087
Epoch 239
Epoch 239 :: Batch 0/1
Batch Loss = 0.0007309915596211087
239, epoch_train_loss=0.0007309915596211087
Epoch 240
Epoch 240 :: Batch 0/1
Batch Loss = 0.0007309915596211087
240, epoch_train_loss=0.0007309915596211087
Epoch 241
Epoch 241 :: Batch 0/1
Batch Loss = 0.0007309915596211087
241, epoch_train_loss=0.0007309915596211087
Epoch 242
Epoch 242 :: Batch 0/1
Batch Loss = 0.0007309915596211087
242, epoch_train_loss=0.0007309915596211087
Epoch 243
Epoch 243 :: Batch 0/1
Batch Loss = 0.0007309915596211087
243, epoch_train_loss=0.0007309915596211087
Epoch 244
Epoch 244 :: Batch 0/1
Batch Loss = 0.0007309915596211087
244, epoch_train_loss=0.0007309915596211087
Epoch 245
Epoch 245 :: Batch 0/1
Batch Loss = 0.0007309915596211087
245, epoch_train_loss=0.0007309915596211087
Epoch 246
Epoch 246 :: Batch 0/1
Batch Loss = 0.0007309915596211087
246, epoch_train_loss=0.0007309915596211087
Epoch 247
Epoch 247 :: Batch 0/1
Batch Loss = 0.0007309915596211087
247, epoch_train_loss=0.0007309915596211087
Epoch 248
Epoch 248 :: Batch 0/1
Batch Loss = 0.0007309915596211087
248, epoch_train_loss=0.0007309915596211087
Epoch 249
Epoch 249 :: Batch 0/1
Batch Loss = 0.0007309915596211087
249, epoch_train_loss=0.0007309915596211087
Epoch 250
Epoch 250 :: Batch 0/1
Batch Loss = 0.0007309915596211087
250, epoch_train_loss=0.0007309915596211087
Epoch 251
Epoch 251 :: Batch 0/1
Batch Loss = 0.0007309915596211087
251, epoch_train_loss=0.0007309915596211087
Epoch 252
Epoch 252 :: Batch 0/1
Batch Loss = 0.0007309915596211087
252, epoch_train_loss=0.0007309915596211087
Epoch 253
Epoch 253 :: Batch 0/1
Batch Loss = 0.0007309915596211087
253, epoch_train_loss=0.0007309915596211087
Epoch 254
Epoch 254 :: Batch 0/1
Batch Loss = 0.0007309915596211087
254, epoch_train_loss=0.0007309915596211087
Epoch 255
Epoch 255 :: Batch 0/1
Batch Loss = 0.0007309915596211087
255, epoch_train_loss=0.0007309915596211087
Epoch 256
Epoch 256 :: Batch 0/1
Batch Loss = 0.0007309915596211087
256, epoch_train_loss=0.0007309915596211087
Epoch 257
Epoch 257 :: Batch 0/1
Batch Loss = 0.0007309915596211087
257, epoch_train_loss=0.0007309915596211087
Epoch 258
Epoch 258 :: Batch 0/1
Batch Loss = 0.0007309915596211087
258, epoch_train_loss=0.0007309915596211087
Epoch 259
Epoch 259 :: Batch 0/1
Batch Loss = 0.0007309915596211087
259, epoch_train_loss=0.0007309915596211087
Epoch 260
Epoch 260 :: Batch 0/1
Batch Loss = 0.0007309915596211087
260, epoch_train_loss=0.0007309915596211087
Epoch 261
Epoch 261 :: Batch 0/1
Batch Loss = 0.0007309915596211087
261, epoch_train_loss=0.0007309915596211087
Epoch 262
Epoch 262 :: Batch 0/1
Batch Loss = 0.0007309915596211087
262, epoch_train_loss=0.0007309915596211087
Epoch 263
Epoch 263 :: Batch 0/1
Batch Loss = 0.0007309915596211087
263, epoch_train_loss=0.0007309915596211087
Epoch 264
Epoch 264 :: Batch 0/1
Batch Loss = 0.0007309915596211087
264, epoch_train_loss=0.0007309915596211087
Epoch 265
Epoch 265 :: Batch 0/1
Batch Loss = 0.0007309915596211087
265, epoch_train_loss=0.0007309915596211087
Epoch 266
Epoch 266 :: Batch 0/1
Batch Loss = 0.0007309915596211087
266, epoch_train_loss=0.0007309915596211087
Epoch 267
Epoch 267 :: Batch 0/1
Batch Loss = 0.0007309915596211087
267, epoch_train_loss=0.0007309915596211087
Epoch 268
Epoch 268 :: Batch 0/1
Batch Loss = 0.0007309915596211087
268, epoch_train_loss=0.0007309915596211087
Epoch 269
Epoch 269 :: Batch 0/1
Batch Loss = 0.0007309915596211087
269, epoch_train_loss=0.0007309915596211087
Epoch 270
Epoch 270 :: Batch 0/1
Batch Loss = 0.0007309915596211087
270, epoch_train_loss=0.0007309915596211087
Epoch 271
Epoch 271 :: Batch 0/1
Batch Loss = 0.0007309915596211087
271, epoch_train_loss=0.0007309915596211087
Epoch 272
Epoch 272 :: Batch 0/1
Batch Loss = 0.0007309915596211087
272, epoch_train_loss=0.0007309915596211087
Epoch 273
Epoch 273 :: Batch 0/1
Batch Loss = 0.0007309915596211087
273, epoch_train_loss=0.0007309915596211087
Epoch 274
Epoch 274 :: Batch 0/1
Batch Loss = 0.0007309915596211087
274, epoch_train_loss=0.0007309915596211087
Epoch 275
Epoch 275 :: Batch 0/1
Batch Loss = 0.0007309915596211087
275, epoch_train_loss=0.0007309915596211087
Epoch 276
Epoch 276 :: Batch 0/1
Batch Loss = 0.0007309915596211087
276, epoch_train_loss=0.0007309915596211087
Epoch 277
Epoch 277 :: Batch 0/1
Batch Loss = 0.0007309915596211087
277, epoch_train_loss=0.0007309915596211087
Epoch 278
Epoch 278 :: Batch 0/1
Batch Loss = 0.0007309915596211087
278, epoch_train_loss=0.0007309915596211087
Epoch 279
Epoch 279 :: Batch 0/1
Batch Loss = 0.0007309915596211087
279, epoch_train_loss=0.0007309915596211087
Epoch 280
Epoch 280 :: Batch 0/1
Batch Loss = 0.0007309915596211087
280, epoch_train_loss=0.0007309915596211087
Epoch 281
Epoch 281 :: Batch 0/1
Batch Loss = 0.0007309915596211087
281, epoch_train_loss=0.0007309915596211087
Epoch 282
Epoch 282 :: Batch 0/1
Batch Loss = 0.0007309915596211087
282, epoch_train_loss=0.0007309915596211087
Epoch 283
Epoch 283 :: Batch 0/1
Batch Loss = 0.0007309915596211087
283, epoch_train_loss=0.0007309915596211087
Epoch 284
Epoch 284 :: Batch 0/1
Batch Loss = 0.0007309915596211087
284, epoch_train_loss=0.0007309915596211087
Epoch 285
Epoch 285 :: Batch 0/1
Batch Loss = 0.0007309915596211087
285, epoch_train_loss=0.0007309915596211087
Epoch 286
Epoch 286 :: Batch 0/1
Batch Loss = 0.0007309915596211087
286, epoch_train_loss=0.0007309915596211087
Epoch 287
Epoch 287 :: Batch 0/1
Batch Loss = 0.0007309915596211087
287, epoch_train_loss=0.0007309915596211087
Epoch 288
Epoch 288 :: Batch 0/1
Batch Loss = 0.0007309915596211087
288, epoch_train_loss=0.0007309915596211087
Epoch 289
Epoch 289 :: Batch 0/1
Batch Loss = 0.0007309915596211087
289, epoch_train_loss=0.0007309915596211087
Epoch 290
Epoch 290 :: Batch 0/1
Batch Loss = 0.0007309915596211087
290, epoch_train_loss=0.0007309915596211087
Epoch 291
Epoch 291 :: Batch 0/1
Batch Loss = 0.0007309915596211087
291, epoch_train_loss=0.0007309915596211087
Epoch 292
Epoch 292 :: Batch 0/1
Batch Loss = 0.0007309915596211087
292, epoch_train_loss=0.0007309915596211087
Epoch 293
Epoch 293 :: Batch 0/1
Batch Loss = 0.0007309915596211087
293, epoch_train_loss=0.0007309915596211087
Epoch 294
Epoch 294 :: Batch 0/1
Batch Loss = 0.0007309915596211087
294, epoch_train_loss=0.0007309915596211087
Epoch 295
Epoch 295 :: Batch 0/1
Batch Loss = 0.0007309915596211087
295, epoch_train_loss=0.0007309915596211087
Epoch 296
Epoch 296 :: Batch 0/1
Batch Loss = 0.0007309915596211087
296, epoch_train_loss=0.0007309915596211087
Epoch 297
Epoch 297 :: Batch 0/1
Batch Loss = 0.0007309915596211087
297, epoch_train_loss=0.0007309915596211087
Epoch 298
Epoch 298 :: Batch 0/1
Batch Loss = 0.0007309915596211087
298, epoch_train_loss=0.0007309915596211087
Epoch 299
Epoch 299 :: Batch 0/1
Batch Loss = 0.0007309915596211087
299, epoch_train_loss=0.0007309915596211087
Epoch 300
Epoch 300 :: Batch 0/1
Batch Loss = 0.0007309915596211087
300, epoch_train_loss=0.0007309915596211087
Epoch 301
Epoch 301 :: Batch 0/1
Batch Loss = 0.0007309915596211087
301, epoch_train_loss=0.0007309915596211087
Epoch 302
Epoch 302 :: Batch 0/1
Batch Loss = 0.0007309915596211087
302, epoch_train_loss=0.0007309915596211087
Epoch 303
Epoch 303 :: Batch 0/1
Batch Loss = 0.0007309915596211087
303, epoch_train_loss=0.0007309915596211087
Epoch 304
Epoch 304 :: Batch 0/1
Batch Loss = 0.0007309915596211087
304, epoch_train_loss=0.0007309915596211087
Epoch 305
Epoch 305 :: Batch 0/1
Batch Loss = 0.0007309915596211087
305, epoch_train_loss=0.0007309915596211087
Epoch 306
Epoch 306 :: Batch 0/1
Batch Loss = 0.0007309915596211087
306, epoch_train_loss=0.0007309915596211087
Epoch 307
Epoch 307 :: Batch 0/1
Batch Loss = 0.0007309915596211087
307, epoch_train_loss=0.0007309915596211087
Epoch 308
Epoch 308 :: Batch 0/1
Batch Loss = 0.0007309915596211087
308, epoch_train_loss=0.0007309915596211087
Epoch 309
Epoch 309 :: Batch 0/1
Batch Loss = 0.0007309915596211087
309, epoch_train_loss=0.0007309915596211087
Epoch 310
Epoch 310 :: Batch 0/1
Batch Loss = 0.0007309915596211087
310, epoch_train_loss=0.0007309915596211087
Epoch 311
Epoch 311 :: Batch 0/1
Batch Loss = 0.0007309915596211087
311, epoch_train_loss=0.0007309915596211087
Epoch 312
Epoch 312 :: Batch 0/1
Batch Loss = 0.0007309915596211087
312, epoch_train_loss=0.0007309915596211087
Epoch 313
Epoch 313 :: Batch 0/1
Batch Loss = 0.0007309915596211087
313, epoch_train_loss=0.0007309915596211087
Epoch 314
Epoch 314 :: Batch 0/1
Batch Loss = 0.0007309915596211087
314, epoch_train_loss=0.0007309915596211087
Epoch 315
Epoch 315 :: Batch 0/1
Batch Loss = 0.0007309915596211087
315, epoch_train_loss=0.0007309915596211087
Epoch 316
Epoch 316 :: Batch 0/1
Batch Loss = 0.0007309915596211087
316, epoch_train_loss=0.0007309915596211087
Epoch 317
Epoch 317 :: Batch 0/1
Batch Loss = 0.0007309915596211087
317, epoch_train_loss=0.0007309915596211087
Epoch 318
Epoch 318 :: Batch 0/1
Batch Loss = 0.0007309915596211087
318, epoch_train_loss=0.0007309915596211087
Epoch 319
Epoch 319 :: Batch 0/1
Batch Loss = 0.0007309915596211087
319, epoch_train_loss=0.0007309915596211087
Epoch 320
Epoch 320 :: Batch 0/1
Batch Loss = 0.0007309915596211087
320, epoch_train_loss=0.0007309915596211087
Epoch 321
Epoch 321 :: Batch 0/1
Batch Loss = 0.0007309915596211087
321, epoch_train_loss=0.0007309915596211087
Epoch 322
Epoch 322 :: Batch 0/1
Batch Loss = 0.0007309915596211087
322, epoch_train_loss=0.0007309915596211087
Epoch 323
Epoch 323 :: Batch 0/1
Batch Loss = 0.0007309915596211087
323, epoch_train_loss=0.0007309915596211087
Epoch 324
Epoch 324 :: Batch 0/1
Batch Loss = 0.0007309915596211087
324, epoch_train_loss=0.0007309915596211087
Epoch 325
Epoch 325 :: Batch 0/1
Batch Loss = 0.0007309915596211087
325, epoch_train_loss=0.0007309915596211087
Epoch 326
Epoch 326 :: Batch 0/1
Batch Loss = 0.0007309915596211087
326, epoch_train_loss=0.0007309915596211087
Epoch 327
Epoch 327 :: Batch 0/1
Batch Loss = 0.0007309915596211087
327, epoch_train_loss=0.0007309915596211087
Epoch 328
Epoch 328 :: Batch 0/1
Batch Loss = 0.0007309915596211087
328, epoch_train_loss=0.0007309915596211087
Epoch 329
Epoch 329 :: Batch 0/1
Batch Loss = 0.0007309915596211087
329, epoch_train_loss=0.0007309915596211087
Epoch 330
Epoch 330 :: Batch 0/1
Batch Loss = 0.0007309915596211087
330, epoch_train_loss=0.0007309915596211087
Epoch 331
Epoch 331 :: Batch 0/1
Batch Loss = 0.0007309915596211087
331, epoch_train_loss=0.0007309915596211087
Epoch 332
Epoch 332 :: Batch 0/1
Batch Loss = 0.0007309915596211087
332, epoch_train_loss=0.0007309915596211087
Epoch 333
Epoch 333 :: Batch 0/1
Batch Loss = 0.0007309915596211087
333, epoch_train_loss=0.0007309915596211087
Epoch 334
Epoch 334 :: Batch 0/1
Batch Loss = 0.0007309915596211087
334, epoch_train_loss=0.0007309915596211087
Epoch 335
Epoch 335 :: Batch 0/1
Batch Loss = 0.0007309915596211087
335, epoch_train_loss=0.0007309915596211087
Epoch 336
Epoch 336 :: Batch 0/1
Batch Loss = 0.0007309915596211087
336, epoch_train_loss=0.0007309915596211087
Epoch 337
Epoch 337 :: Batch 0/1
Batch Loss = 0.0007309915596211087
337, epoch_train_loss=0.0007309915596211087
Epoch 338
Epoch 338 :: Batch 0/1
Batch Loss = 0.0007309915596211087
338, epoch_train_loss=0.0007309915596211087
Epoch 339
Epoch 339 :: Batch 0/1
Batch Loss = 0.0007309915596211087
339, epoch_train_loss=0.0007309915596211087
Epoch 340
Epoch 340 :: Batch 0/1
Batch Loss = 0.0007309915596211087
340, epoch_train_loss=0.0007309915596211087
Epoch 341
Epoch 341 :: Batch 0/1
Batch Loss = 0.0007309915596211087
341, epoch_train_loss=0.0007309915596211087
Epoch 342
Epoch 342 :: Batch 0/1
Batch Loss = 0.0007309915596211087
342, epoch_train_loss=0.0007309915596211087
Epoch 343
Epoch 343 :: Batch 0/1
Batch Loss = 0.0007309915596211087
343, epoch_train_loss=0.0007309915596211087
Epoch 344
Epoch 344 :: Batch 0/1
Batch Loss = 0.0007309915596211087
344, epoch_train_loss=0.0007309915596211087
Epoch 345
Epoch 345 :: Batch 0/1
Batch Loss = 0.0007309915596211087
345, epoch_train_loss=0.0007309915596211087
Epoch 346
Epoch 346 :: Batch 0/1
Batch Loss = 0.0007309915596211087
346, epoch_train_loss=0.0007309915596211087
Epoch 347
Epoch 347 :: Batch 0/1
Batch Loss = 0.0007309915596211087
347, epoch_train_loss=0.0007309915596211087
Epoch 348
Epoch 348 :: Batch 0/1
Batch Loss = 0.0007309915596211087
348, epoch_train_loss=0.0007309915596211087
Epoch 349
Epoch 349 :: Batch 0/1
Batch Loss = 0.0007309915596211087
349, epoch_train_loss=0.0007309915596211087
Epoch 350
Epoch 350 :: Batch 0/1
Batch Loss = 0.0007309915596211087
350, epoch_train_loss=0.0007309915596211087
Epoch 351
Epoch 351 :: Batch 0/1
Batch Loss = 0.0007309915596211087
351, epoch_train_loss=0.0007309915596211087
Epoch 352
Epoch 352 :: Batch 0/1
Batch Loss = 0.0007309915596211087
352, epoch_train_loss=0.0007309915596211087
Epoch 353
Epoch 353 :: Batch 0/1
Batch Loss = 0.0007309915596211087
353, epoch_train_loss=0.0007309915596211087
Epoch 354
Epoch 354 :: Batch 0/1
Batch Loss = 0.0007309915596211087
354, epoch_train_loss=0.0007309915596211087
Epoch 355
Epoch 355 :: Batch 0/1
Batch Loss = 0.0007309915596211087
355, epoch_train_loss=0.0007309915596211087
Epoch 356
Epoch 356 :: Batch 0/1
Batch Loss = 0.0007309915596211087
356, epoch_train_loss=0.0007309915596211087
Epoch 357
Epoch 357 :: Batch 0/1
Batch Loss = 0.0007309915596211087
357, epoch_train_loss=0.0007309915596211087
Epoch 358
Epoch 358 :: Batch 0/1
Batch Loss = 0.0007309915596211087
358, epoch_train_loss=0.0007309915596211087
Epoch 359
Epoch 359 :: Batch 0/1
Batch Loss = 0.0007309915596211087
359, epoch_train_loss=0.0007309915596211087
Epoch 360
Epoch 360 :: Batch 0/1
Batch Loss = 0.0007309915596211087
360, epoch_train_loss=0.0007309915596211087
Epoch 361
Epoch 361 :: Batch 0/1
Batch Loss = 0.0007309915596211087
361, epoch_train_loss=0.0007309915596211087
Epoch 362
Epoch 362 :: Batch 0/1
Batch Loss = 0.0007309915596211087
362, epoch_train_loss=0.0007309915596211087
Epoch 363
Epoch 363 :: Batch 0/1
Batch Loss = 0.0007309915596211087
363, epoch_train_loss=0.0007309915596211087
Epoch 364
Epoch 364 :: Batch 0/1
Batch Loss = 0.0007309915596211087
364, epoch_train_loss=0.0007309915596211087
Epoch 365
Epoch 365 :: Batch 0/1
Batch Loss = 0.0007309915596211087
365, epoch_train_loss=0.0007309915596211087
Epoch 366
Epoch 366 :: Batch 0/1
Batch Loss = 0.0007309915596211087
366, epoch_train_loss=0.0007309915596211087
Epoch 367
Epoch 367 :: Batch 0/1
Batch Loss = 0.0007309915596211087
367, epoch_train_loss=0.0007309915596211087
Epoch 368
Epoch 368 :: Batch 0/1
Batch Loss = 0.0007309915596211087
368, epoch_train_loss=0.0007309915596211087
Epoch 369
Epoch 369 :: Batch 0/1
Batch Loss = 0.0007309915596211087
369, epoch_train_loss=0.0007309915596211087
Epoch 370
Epoch 370 :: Batch 0/1
Batch Loss = 0.0007309915596211087
370, epoch_train_loss=0.0007309915596211087
Epoch 371
Epoch 371 :: Batch 0/1
Batch Loss = 0.0007309915596211087
371, epoch_train_loss=0.0007309915596211087
Epoch 372
Epoch 372 :: Batch 0/1
Batch Loss = 0.0007309915596211087
372, epoch_train_loss=0.0007309915596211087
Epoch 373
Epoch 373 :: Batch 0/1
Batch Loss = 0.0007309915596211087
373, epoch_train_loss=0.0007309915596211087
Epoch 374
Epoch 374 :: Batch 0/1
Batch Loss = 0.0007309915596211087
374, epoch_train_loss=0.0007309915596211087
Epoch 375
Epoch 375 :: Batch 0/1
Batch Loss = 0.0007309915596211087
375, epoch_train_loss=0.0007309915596211087
Epoch 376
Epoch 376 :: Batch 0/1
Batch Loss = 0.0007309915596211087
376, epoch_train_loss=0.0007309915596211087
Epoch 377
Epoch 377 :: Batch 0/1
Batch Loss = 0.0007309915596211087
377, epoch_train_loss=0.0007309915596211087
Epoch 378
Epoch 378 :: Batch 0/1
Batch Loss = 0.0007309915596211087
378, epoch_train_loss=0.0007309915596211087
Epoch 379
Epoch 379 :: Batch 0/1
Batch Loss = 0.0007309915596211087
379, epoch_train_loss=0.0007309915596211087
Epoch 380
Epoch 380 :: Batch 0/1
Batch Loss = 0.0007309915596211087
380, epoch_train_loss=0.0007309915596211087
Epoch 381
Epoch 381 :: Batch 0/1
Batch Loss = 0.0007309915596211087
381, epoch_train_loss=0.0007309915596211087
Epoch 382
Epoch 382 :: Batch 0/1
Batch Loss = 0.0007309915596211087
382, epoch_train_loss=0.0007309915596211087
Epoch 383
Epoch 383 :: Batch 0/1
Batch Loss = 0.0007309915596211087
383, epoch_train_loss=0.0007309915596211087
Epoch 384
Epoch 384 :: Batch 0/1
Batch Loss = 0.0007309915596211087
384, epoch_train_loss=0.0007309915596211087
Epoch 385
Epoch 385 :: Batch 0/1
Batch Loss = 0.0007309915596211087
385, epoch_train_loss=0.0007309915596211087
Epoch 386
Epoch 386 :: Batch 0/1
Batch Loss = 0.0007309915596211087
386, epoch_train_loss=0.0007309915596211087
Epoch 387
Epoch 387 :: Batch 0/1
Batch Loss = 0.0007309915596211087
387, epoch_train_loss=0.0007309915596211087
Epoch 388
Epoch 388 :: Batch 0/1
Batch Loss = 0.0007309915596211087
388, epoch_train_loss=0.0007309915596211087
Epoch 389
Epoch 389 :: Batch 0/1
Batch Loss = 0.0007309915596211087
389, epoch_train_loss=0.0007309915596211087
Epoch 390
Epoch 390 :: Batch 0/1
Batch Loss = 0.0007309915596211087
390, epoch_train_loss=0.0007309915596211087
Epoch 391
Epoch 391 :: Batch 0/1
Batch Loss = 0.0007309915596211087
391, epoch_train_loss=0.0007309915596211087
Epoch 392
Epoch 392 :: Batch 0/1
Batch Loss = 0.0007309915596211087
392, epoch_train_loss=0.0007309915596211087
Epoch 393
Epoch 393 :: Batch 0/1
Batch Loss = 0.0007309915596211087
393, epoch_train_loss=0.0007309915596211087
Epoch 394
Epoch 394 :: Batch 0/1
Batch Loss = 0.0007309915596211087
394, epoch_train_loss=0.0007309915596211087
Epoch 395
Epoch 395 :: Batch 0/1
Batch Loss = 0.0007309915596211087
395, epoch_train_loss=0.0007309915596211087
Epoch 396
Epoch 396 :: Batch 0/1
Batch Loss = 0.0007309915596211087
396, epoch_train_loss=0.0007309915596211087
Epoch 397
Epoch 397 :: Batch 0/1
Batch Loss = 0.0007309915596211087
397, epoch_train_loss=0.0007309915596211087
Epoch 398
Epoch 398 :: Batch 0/1
Batch Loss = 0.0007309915596211087
398, epoch_train_loss=0.0007309915596211087
Epoch 399
Epoch 399 :: Batch 0/1
Batch Loss = 0.0007309915596211087
399, epoch_train_loss=0.0007309915596211087
Epoch 400
Epoch 400 :: Batch 0/1
Batch Loss = 0.0007309915596211087
400, epoch_train_loss=0.0007309915596211087
Epoch 401
Epoch 401 :: Batch 0/1
Batch Loss = 0.0007309915596211087
401, epoch_train_loss=0.0007309915596211087
Epoch 402
Epoch 402 :: Batch 0/1
Batch Loss = 0.0007309915596211087
402, epoch_train_loss=0.0007309915596211087
Epoch 403
Epoch 403 :: Batch 0/1
Batch Loss = 0.0007309915596211087
403, epoch_train_loss=0.0007309915596211087
Epoch 404
Epoch 404 :: Batch 0/1
Batch Loss = 0.0007309915596211087
404, epoch_train_loss=0.0007309915596211087
Epoch 405
Epoch 405 :: Batch 0/1
Batch Loss = 0.0007309915596211087
405, epoch_train_loss=0.0007309915596211087
Epoch 406
Epoch 406 :: Batch 0/1
Batch Loss = 0.0007309915596211087
406, epoch_train_loss=0.0007309915596211087
Epoch 407
Epoch 407 :: Batch 0/1
Batch Loss = 0.0007309915596211087
407, epoch_train_loss=0.0007309915596211087
Epoch 408
Epoch 408 :: Batch 0/1
Batch Loss = 0.0007309915596211087
408, epoch_train_loss=0.0007309915596211087
Epoch 409
Epoch 409 :: Batch 0/1
Batch Loss = 0.0007309915596211087
409, epoch_train_loss=0.0007309915596211087
Epoch 410
Epoch 410 :: Batch 0/1
Batch Loss = 0.0007309915596211087
410, epoch_train_loss=0.0007309915596211087
Epoch 411
Epoch 411 :: Batch 0/1
Batch Loss = 0.0007309915596211087
411, epoch_train_loss=0.0007309915596211087
Epoch 412
Epoch 412 :: Batch 0/1
Batch Loss = 0.0007309915596211087
412, epoch_train_loss=0.0007309915596211087
Epoch 413
Epoch 413 :: Batch 0/1
Batch Loss = 0.0007309915596211087
413, epoch_train_loss=0.0007309915596211087
Epoch 414
Epoch 414 :: Batch 0/1
Batch Loss = 0.0007309915596211087
414, epoch_train_loss=0.0007309915596211087
Epoch 415
Epoch 415 :: Batch 0/1
Batch Loss = 0.0007309915596211087
415, epoch_train_loss=0.0007309915596211087
Epoch 416
Epoch 416 :: Batch 0/1
Batch Loss = 0.0007309915596211087
416, epoch_train_loss=0.0007309915596211087
Epoch 417
Epoch 417 :: Batch 0/1
Batch Loss = 0.0007309915596211087
417, epoch_train_loss=0.0007309915596211087
Epoch 418
Epoch 418 :: Batch 0/1
Batch Loss = 0.0007309915596211087
418, epoch_train_loss=0.0007309915596211087
Epoch 419
Epoch 419 :: Batch 0/1
Batch Loss = 0.0007309915596211087
419, epoch_train_loss=0.0007309915596211087
Epoch 420
Epoch 420 :: Batch 0/1
Batch Loss = 0.0007309915596211087
420, epoch_train_loss=0.0007309915596211087
Epoch 421
Epoch 421 :: Batch 0/1
Batch Loss = 0.0007309915596211087
421, epoch_train_loss=0.0007309915596211087
Epoch 422
Epoch 422 :: Batch 0/1
Batch Loss = 0.0007309915596211087
422, epoch_train_loss=0.0007309915596211087
Epoch 423
Epoch 423 :: Batch 0/1
Batch Loss = 0.0007309915596211087
423, epoch_train_loss=0.0007309915596211087
Epoch 424
Epoch 424 :: Batch 0/1
Batch Loss = 0.0007309915596211087
424, epoch_train_loss=0.0007309915596211087
Epoch 425
Epoch 425 :: Batch 0/1
Batch Loss = 0.0007309915596211087
425, epoch_train_loss=0.0007309915596211087
Epoch 426
Epoch 426 :: Batch 0/1
Batch Loss = 0.0007309915596211087
426, epoch_train_loss=0.0007309915596211087
Epoch 427
Epoch 427 :: Batch 0/1
Batch Loss = 0.0007309915596211087
427, epoch_train_loss=0.0007309915596211087
Epoch 428
Epoch 428 :: Batch 0/1
Batch Loss = 0.0007309915596211087
428, epoch_train_loss=0.0007309915596211087
Epoch 429
Epoch 429 :: Batch 0/1
Batch Loss = 0.0007309915596211087
429, epoch_train_loss=0.0007309915596211087
Epoch 430
Epoch 430 :: Batch 0/1
Batch Loss = 0.0007309915596211087
430, epoch_train_loss=0.0007309915596211087
Epoch 431
Epoch 431 :: Batch 0/1
Batch Loss = 0.0007309915596211087
431, epoch_train_loss=0.0007309915596211087
Epoch 432
Epoch 432 :: Batch 0/1
Batch Loss = 0.0007309915596211087
432, epoch_train_loss=0.0007309915596211087
Epoch 433
Epoch 433 :: Batch 0/1
Batch Loss = 0.0007309915596211087
433, epoch_train_loss=0.0007309915596211087
Epoch 434
Epoch 434 :: Batch 0/1
Batch Loss = 0.0007309915596211087
434, epoch_train_loss=0.0007309915596211087
Epoch 435
Epoch 435 :: Batch 0/1
Batch Loss = 0.0007309915596211087
435, epoch_train_loss=0.0007309915596211087
Epoch 436
Epoch 436 :: Batch 0/1
Batch Loss = 0.0007309915596211087
436, epoch_train_loss=0.0007309915596211087
Epoch 437
Epoch 437 :: Batch 0/1
Batch Loss = 0.0007309915596211087
437, epoch_train_loss=0.0007309915596211087
Epoch 438
Epoch 438 :: Batch 0/1
Batch Loss = 0.0007309915596211087
438, epoch_train_loss=0.0007309915596211087
Epoch 439
Epoch 439 :: Batch 0/1
Batch Loss = 0.0007309915596211087
439, epoch_train_loss=0.0007309915596211087
Epoch 440
Epoch 440 :: Batch 0/1
Batch Loss = 0.0007309915596211087
440, epoch_train_loss=0.0007309915596211087
Epoch 441
Epoch 441 :: Batch 0/1
Batch Loss = 0.0007309915596211087
441, epoch_train_loss=0.0007309915596211087
Epoch 442
Epoch 442 :: Batch 0/1
Batch Loss = 0.0007309915596211087
442, epoch_train_loss=0.0007309915596211087
Epoch 443
Epoch 443 :: Batch 0/1
Batch Loss = 0.0007309915596211087
443, epoch_train_loss=0.0007309915596211087
Epoch 444
Epoch 444 :: Batch 0/1
Batch Loss = 0.0007309915596211087
444, epoch_train_loss=0.0007309915596211087
Epoch 445
Epoch 445 :: Batch 0/1
Batch Loss = 0.0007309915596211087
445, epoch_train_loss=0.0007309915596211087
Epoch 446
Epoch 446 :: Batch 0/1
Batch Loss = 0.0007309915596211087
446, epoch_train_loss=0.0007309915596211087
Epoch 447
Epoch 447 :: Batch 0/1
Batch Loss = 0.0007309915596211087
447, epoch_train_loss=0.0007309915596211087
Epoch 448
Epoch 448 :: Batch 0/1
Batch Loss = 0.0007309915596211087
448, epoch_train_loss=0.0007309915596211087
Epoch 449
Epoch 449 :: Batch 0/1
Batch Loss = 0.0007309915596211087
449, epoch_train_loss=0.0007309915596211087
Epoch 450
Epoch 450 :: Batch 0/1
Batch Loss = 0.0007309915596211087
450, epoch_train_loss=0.0007309915596211087
Epoch 451
Epoch 451 :: Batch 0/1
Batch Loss = 0.0007309915596211087
451, epoch_train_loss=0.0007309915596211087
Epoch 452
Epoch 452 :: Batch 0/1
Batch Loss = 0.0007309915596211087
452, epoch_train_loss=0.0007309915596211087
Epoch 453
Epoch 453 :: Batch 0/1
Batch Loss = 0.0007309915596211087
453, epoch_train_loss=0.0007309915596211087
Epoch 454
Epoch 454 :: Batch 0/1
Batch Loss = 0.0007309915596211087
454, epoch_train_loss=0.0007309915596211087
Epoch 455
Epoch 455 :: Batch 0/1
Batch Loss = 0.0007309915596211087
455, epoch_train_loss=0.0007309915596211087
Epoch 456
Epoch 456 :: Batch 0/1
Batch Loss = 0.0007309915596211087
456, epoch_train_loss=0.0007309915596211087
Epoch 457
Epoch 457 :: Batch 0/1
Batch Loss = 0.0007309915596211087
457, epoch_train_loss=0.0007309915596211087
Epoch 458
Epoch 458 :: Batch 0/1
Batch Loss = 0.0007309915596211087
458, epoch_train_loss=0.0007309915596211087
Epoch 459
Epoch 459 :: Batch 0/1
Batch Loss = 0.0007309915596211087
459, epoch_train_loss=0.0007309915596211087
Epoch 460
Epoch 460 :: Batch 0/1
Batch Loss = 0.0007309915596211087
460, epoch_train_loss=0.0007309915596211087
Epoch 461
Epoch 461 :: Batch 0/1
Batch Loss = 0.0007309915596211087
461, epoch_train_loss=0.0007309915596211087
Epoch 462
Epoch 462 :: Batch 0/1
Batch Loss = 0.0007309915596211087
462, epoch_train_loss=0.0007309915596211087
Epoch 463
Epoch 463 :: Batch 0/1
Batch Loss = 0.0007309915596211087
463, epoch_train_loss=0.0007309915596211087
Epoch 464
Epoch 464 :: Batch 0/1
Batch Loss = 0.0007309915596211087
464, epoch_train_loss=0.0007309915596211087
Epoch 465
Epoch 465 :: Batch 0/1
Batch Loss = 0.0007309915596211087
465, epoch_train_loss=0.0007309915596211087
Epoch 466
Epoch 466 :: Batch 0/1
Batch Loss = 0.0007309915596211087
466, epoch_train_loss=0.0007309915596211087
Epoch 467
Epoch 467 :: Batch 0/1
Batch Loss = 0.0007309915596211087
467, epoch_train_loss=0.0007309915596211087
Epoch 468
Epoch 468 :: Batch 0/1
Batch Loss = 0.0007309915596211087
468, epoch_train_loss=0.0007309915596211087
Epoch 469
Epoch 469 :: Batch 0/1
Batch Loss = 0.0007309915596211087
469, epoch_train_loss=0.0007309915596211087
Epoch 470
Epoch 470 :: Batch 0/1
Batch Loss = 0.0007309915596211087
470, epoch_train_loss=0.0007309915596211087
Epoch 471
Epoch 471 :: Batch 0/1
Batch Loss = 0.0007309915596211087
471, epoch_train_loss=0.0007309915596211087
Epoch 472
Epoch 472 :: Batch 0/1
Batch Loss = 0.0007309915596211087
472, epoch_train_loss=0.0007309915596211087
Epoch 473
Epoch 473 :: Batch 0/1
Batch Loss = 0.0007309915596211087
473, epoch_train_loss=0.0007309915596211087
Epoch 474
Epoch 474 :: Batch 0/1
Batch Loss = 0.0007309915596211087
474, epoch_train_loss=0.0007309915596211087
Epoch 475
Epoch 475 :: Batch 0/1
Batch Loss = 0.0007309915596211087
475, epoch_train_loss=0.0007309915596211087
Epoch 476
Epoch 476 :: Batch 0/1
Batch Loss = 0.0007309915596211087
476, epoch_train_loss=0.0007309915596211087
Epoch 477
Epoch 477 :: Batch 0/1
Batch Loss = 0.0007309915596211087
477, epoch_train_loss=0.0007309915596211087
Epoch 478
Epoch 478 :: Batch 0/1
Batch Loss = 0.0007309915596211087
478, epoch_train_loss=0.0007309915596211087
Epoch 479
Epoch 479 :: Batch 0/1
Batch Loss = 0.0007309915596211087
479, epoch_train_loss=0.0007309915596211087
Epoch 480
Epoch 480 :: Batch 0/1
Batch Loss = 0.0007309915596211087
480, epoch_train_loss=0.0007309915596211087
Epoch 481
Epoch 481 :: Batch 0/1
Batch Loss = 0.0007309915596211087
481, epoch_train_loss=0.0007309915596211087
Epoch 482
Epoch 482 :: Batch 0/1
Batch Loss = 0.0007309915596211087
482, epoch_train_loss=0.0007309915596211087
Epoch 483
Epoch 483 :: Batch 0/1
Batch Loss = 0.0007309915596211087
483, epoch_train_loss=0.0007309915596211087
Epoch 484
Epoch 484 :: Batch 0/1
Batch Loss = 0.0007309915596211087
484, epoch_train_loss=0.0007309915596211087
Epoch 485
Epoch 485 :: Batch 0/1
Batch Loss = 0.0007309915596211087
485, epoch_train_loss=0.0007309915596211087
Epoch 486
Epoch 486 :: Batch 0/1
Batch Loss = 0.0007309915596211087
486, epoch_train_loss=0.0007309915596211087
Epoch 487
Epoch 487 :: Batch 0/1
Batch Loss = 0.0007309915596211087
487, epoch_train_loss=0.0007309915596211087
Epoch 488
Epoch 488 :: Batch 0/1
Batch Loss = 0.0007309915596211087
488, epoch_train_loss=0.0007309915596211087
Epoch 489
Epoch 489 :: Batch 0/1
Batch Loss = 0.0007309915596211087
489, epoch_train_loss=0.0007309915596211087
Epoch 490
Epoch 490 :: Batch 0/1
Batch Loss = 0.0007309915596211087
490, epoch_train_loss=0.0007309915596211087
Epoch 491
Epoch 491 :: Batch 0/1
Batch Loss = 0.0007309915596211087
491, epoch_train_loss=0.0007309915596211087
Epoch 492
Epoch 492 :: Batch 0/1
Batch Loss = 0.0007309915596211087
492, epoch_train_loss=0.0007309915596211087
Epoch 493
Epoch 493 :: Batch 0/1
Batch Loss = 0.0007309915596211087
493, epoch_train_loss=0.0007309915596211087
Epoch 494
Epoch 494 :: Batch 0/1
Batch Loss = 0.0007309915596211087
494, epoch_train_loss=0.0007309915596211087
Epoch 495
Epoch 495 :: Batch 0/1
Batch Loss = 0.0007309915596211087
495, epoch_train_loss=0.0007309915596211087
Epoch 496
Epoch 496 :: Batch 0/1
Batch Loss = 0.0007309915596211087
496, epoch_train_loss=0.0007309915596211087
Epoch 497
Epoch 497 :: Batch 0/1
Batch Loss = 0.0007309915596211087
497, epoch_train_loss=0.0007309915596211087
Epoch 498
Epoch 498 :: Batch 0/1
Batch Loss = 0.0007309915596211087
498, epoch_train_loss=0.0007309915596211087
Epoch 499
Epoch 499 :: Batch 0/1
Batch Loss = 0.0007309915596211087
499, epoch_train_loss=0.0007309915596211087
Epoch 500
Epoch 500 :: Batch 0/1
Batch Loss = 0.0007309915596211087
500, epoch_train_loss=0.0007309915596211087
Epoch 501
Epoch 501 :: Batch 0/1
Batch Loss = 0.0007309915596211087
501, epoch_train_loss=0.0007309915596211087
Epoch 502
Epoch 502 :: Batch 0/1
Batch Loss = 0.0007309915596211087
502, epoch_train_loss=0.0007309915596211087
Epoch 503
Epoch 503 :: Batch 0/1
Batch Loss = 0.0007309915596211087
503, epoch_train_loss=0.0007309915596211087
Epoch 504
Epoch 504 :: Batch 0/1
Batch Loss = 0.0007309915596211087
504, epoch_train_loss=0.0007309915596211087
Epoch 505
Epoch 505 :: Batch 0/1
Batch Loss = 0.0007309915596211087
505, epoch_train_loss=0.0007309915596211087
Epoch 506
Epoch 506 :: Batch 0/1
Batch Loss = 0.0007309915596211087
506, epoch_train_loss=0.0007309915596211087
Epoch 507
Epoch 507 :: Batch 0/1
Batch Loss = 0.0007309915596211087
507, epoch_train_loss=0.0007309915596211087
Epoch 508
Epoch 508 :: Batch 0/1
Batch Loss = 0.0007309915596211087
508, epoch_train_loss=0.0007309915596211087
Epoch 509
Epoch 509 :: Batch 0/1
Batch Loss = 0.0007309915596211087
509, epoch_train_loss=0.0007309915596211087
Epoch 510
Epoch 510 :: Batch 0/1
Batch Loss = 0.0007309915596211087
510, epoch_train_loss=0.0007309915596211087
Epoch 511
Epoch 511 :: Batch 0/1
Batch Loss = 0.0007309915596211087
511, epoch_train_loss=0.0007309915596211087
Epoch 512
Epoch 512 :: Batch 0/1
Batch Loss = 0.0007309915596211087
512, epoch_train_loss=0.0007309915596211087
Epoch 513
Epoch 513 :: Batch 0/1
Batch Loss = 0.0007309915596211087
513, epoch_train_loss=0.0007309915596211087
Epoch 514
Epoch 514 :: Batch 0/1
Batch Loss = 0.0007309915596211087
514, epoch_train_loss=0.0007309915596211087
Epoch 515
Epoch 515 :: Batch 0/1
Batch Loss = 0.0007309915596211087
515, epoch_train_loss=0.0007309915596211087
Epoch 516
Epoch 516 :: Batch 0/1
Batch Loss = 0.0007309915596211087
516, epoch_train_loss=0.0007309915596211087
Epoch 517
Epoch 517 :: Batch 0/1
Batch Loss = 0.0007309915596211087
517, epoch_train_loss=0.0007309915596211087
Epoch 518
Epoch 518 :: Batch 0/1
Batch Loss = 0.0007309915596211087
518, epoch_train_loss=0.0007309915596211087
Epoch 519
Epoch 519 :: Batch 0/1
Batch Loss = 0.0007309915596211087
519, epoch_train_loss=0.0007309915596211087
Epoch 520
Epoch 520 :: Batch 0/1
Batch Loss = 0.0007309915596211087
520, epoch_train_loss=0.0007309915596211087
Epoch 521
Epoch 521 :: Batch 0/1
Batch Loss = 0.0007309915596211087
521, epoch_train_loss=0.0007309915596211087
Epoch 522
Epoch 522 :: Batch 0/1
Batch Loss = 0.0007309915596211087
522, epoch_train_loss=0.0007309915596211087
Epoch 523
Epoch 523 :: Batch 0/1
Batch Loss = 0.0007309915596211087
523, epoch_train_loss=0.0007309915596211087
Epoch 524
Epoch 524 :: Batch 0/1
Batch Loss = 0.0007309915596211087
524, epoch_train_loss=0.0007309915596211087
Epoch 525
Epoch 525 :: Batch 0/1
Batch Loss = 0.0007309915596211087
525, epoch_train_loss=0.0007309915596211087
Epoch 526
Epoch 526 :: Batch 0/1
Batch Loss = 0.0007309915596211087
526, epoch_train_loss=0.0007309915596211087
Epoch 527
Epoch 527 :: Batch 0/1
Batch Loss = 0.0007309915596211087
527, epoch_train_loss=0.0007309915596211087
Epoch 528
Epoch 528 :: Batch 0/1
Batch Loss = 0.0007309915596211087
528, epoch_train_loss=0.0007309915596211087
Epoch 529
Epoch 529 :: Batch 0/1
Batch Loss = 0.0007309915596211087
529, epoch_train_loss=0.0007309915596211087
Epoch 530
Epoch 530 :: Batch 0/1
Batch Loss = 0.0007309915596211087
530, epoch_train_loss=0.0007309915596211087
Epoch 531
Epoch 531 :: Batch 0/1
Batch Loss = 0.0007309915596211087
531, epoch_train_loss=0.0007309915596211087
Epoch 532
Epoch 532 :: Batch 0/1
Batch Loss = 0.0007309915596211087
532, epoch_train_loss=0.0007309915596211087
Epoch 533
Epoch 533 :: Batch 0/1
Batch Loss = 0.0007309915596211087
533, epoch_train_loss=0.0007309915596211087
Epoch 534
Epoch 534 :: Batch 0/1
Batch Loss = 0.0007309915596211087
534, epoch_train_loss=0.0007309915596211087
Epoch 535
Epoch 535 :: Batch 0/1
Batch Loss = 0.0007309915596211087
535, epoch_train_loss=0.0007309915596211087
Epoch 536
Epoch 536 :: Batch 0/1
Batch Loss = 0.0007309915596211087
536, epoch_train_loss=0.0007309915596211087
Epoch 537
Epoch 537 :: Batch 0/1
Batch Loss = 0.0007309915596211087
537, epoch_train_loss=0.0007309915596211087
Epoch 538
Epoch 538 :: Batch 0/1
Batch Loss = 0.0007309915596211087
538, epoch_train_loss=0.0007309915596211087
Epoch 539
Epoch 539 :: Batch 0/1
Batch Loss = 0.0007309915596211087
539, epoch_train_loss=0.0007309915596211087
Epoch 540
Epoch 540 :: Batch 0/1
Batch Loss = 0.0007309915596211087
540, epoch_train_loss=0.0007309915596211087
Epoch 541
Epoch 541 :: Batch 0/1
Batch Loss = 0.0007309915596211087
541, epoch_train_loss=0.0007309915596211087
Epoch 542
Epoch 542 :: Batch 0/1
Batch Loss = 0.0007309915596211087
542, epoch_train_loss=0.0007309915596211087
Epoch 543
Epoch 543 :: Batch 0/1
Batch Loss = 0.0007309915596211087
543, epoch_train_loss=0.0007309915596211087
Epoch 544
Epoch 544 :: Batch 0/1
Batch Loss = 0.0007309915596211087
544, epoch_train_loss=0.0007309915596211087
Epoch 545
Epoch 545 :: Batch 0/1
Batch Loss = 0.0007309915596211087
545, epoch_train_loss=0.0007309915596211087
Epoch 546
Epoch 546 :: Batch 0/1
Batch Loss = 0.0007309915596211087
546, epoch_train_loss=0.0007309915596211087
Epoch 547
Epoch 547 :: Batch 0/1
Batch Loss = 0.0007309915596211087
547, epoch_train_loss=0.0007309915596211087
Epoch 548
Epoch 548 :: Batch 0/1
Batch Loss = 0.0007309915596211087
548, epoch_train_loss=0.0007309915596211087
Epoch 549
Epoch 549 :: Batch 0/1
Batch Loss = 0.0007309915596211087
549, epoch_train_loss=0.0007309915596211087
Epoch 550
Epoch 550 :: Batch 0/1
Batch Loss = 0.0007309915596211087
550, epoch_train_loss=0.0007309915596211087
Epoch 551
Epoch 551 :: Batch 0/1
Batch Loss = 0.0007309915596211087
551, epoch_train_loss=0.0007309915596211087
Epoch 552
Epoch 552 :: Batch 0/1
Batch Loss = 0.0007309915596211087
552, epoch_train_loss=0.0007309915596211087
Epoch 553
Epoch 553 :: Batch 0/1
Batch Loss = 0.0007309915596211087
553, epoch_train_loss=0.0007309915596211087
Epoch 554
Epoch 554 :: Batch 0/1
Batch Loss = 0.0007309915596211087
554, epoch_train_loss=0.0007309915596211087
Epoch 555
Epoch 555 :: Batch 0/1
Batch Loss = 0.0007309915596211087
555, epoch_train_loss=0.0007309915596211087
Epoch 556
Epoch 556 :: Batch 0/1
Batch Loss = 0.0007309915596211087
556, epoch_train_loss=0.0007309915596211087
Epoch 557
Epoch 557 :: Batch 0/1
Batch Loss = 0.0007309915596211087
557, epoch_train_loss=0.0007309915596211087
Epoch 558
Epoch 558 :: Batch 0/1
Batch Loss = 0.0007309915596211087
558, epoch_train_loss=0.0007309915596211087
Epoch 559
Epoch 559 :: Batch 0/1
Batch Loss = 0.0007309915596211087
559, epoch_train_loss=0.0007309915596211087
Epoch 560
Epoch 560 :: Batch 0/1
Batch Loss = 0.0007309915596211087
560, epoch_train_loss=0.0007309915596211087
Epoch 561
Epoch 561 :: Batch 0/1
Batch Loss = 0.0007309915596211087
561, epoch_train_loss=0.0007309915596211087
Epoch 562
Epoch 562 :: Batch 0/1
Batch Loss = 0.0007309915596211087
562, epoch_train_loss=0.0007309915596211087
Epoch 563
Epoch 563 :: Batch 0/1
Batch Loss = 0.0007309915596211087
563, epoch_train_loss=0.0007309915596211087
Epoch 564
Epoch 564 :: Batch 0/1
Batch Loss = 0.0007309915596211087
564, epoch_train_loss=0.0007309915596211087
Epoch 565
Epoch 565 :: Batch 0/1
Batch Loss = 0.0007309915596211087
565, epoch_train_loss=0.0007309915596211087
Epoch 566
Epoch 566 :: Batch 0/1
Batch Loss = 0.0007309915596211087
566, epoch_train_loss=0.0007309915596211087
Epoch 567
Epoch 567 :: Batch 0/1
Batch Loss = 0.0007309915596211087
567, epoch_train_loss=0.0007309915596211087
Epoch 568
Epoch 568 :: Batch 0/1
Batch Loss = 0.0007309915596211087
568, epoch_train_loss=0.0007309915596211087
Epoch 569
Epoch 569 :: Batch 0/1
Batch Loss = 0.0007309915596211087
569, epoch_train_loss=0.0007309915596211087
Epoch 570
Epoch 570 :: Batch 0/1
Batch Loss = 0.0007309915596211087
570, epoch_train_loss=0.0007309915596211087
Epoch 571
Epoch 571 :: Batch 0/1
Batch Loss = 0.0007309915596211087
571, epoch_train_loss=0.0007309915596211087
Epoch 572
Epoch 572 :: Batch 0/1
Batch Loss = 0.0007309915596211087
572, epoch_train_loss=0.0007309915596211087
Epoch 573
Epoch 573 :: Batch 0/1
Batch Loss = 0.0007309915596211087
573, epoch_train_loss=0.0007309915596211087
Epoch 574
Epoch 574 :: Batch 0/1
Batch Loss = 0.0007309915596211087
574, epoch_train_loss=0.0007309915596211087
Epoch 575
Epoch 575 :: Batch 0/1
Batch Loss = 0.0007309915596211087
575, epoch_train_loss=0.0007309915596211087
Epoch 576
Epoch 576 :: Batch 0/1
Batch Loss = 0.0007309915596211087
576, epoch_train_loss=0.0007309915596211087
Epoch 577
Epoch 577 :: Batch 0/1
Batch Loss = 0.0007309915596211087
577, epoch_train_loss=0.0007309915596211087
Epoch 578
Epoch 578 :: Batch 0/1
Batch Loss = 0.0007309915596211087
578, epoch_train_loss=0.0007309915596211087
Epoch 579
Epoch 579 :: Batch 0/1
Batch Loss = 0.0007309915596211087
579, epoch_train_loss=0.0007309915596211087
Epoch 580
Epoch 580 :: Batch 0/1
Batch Loss = 0.0007309915596211087
580, epoch_train_loss=0.0007309915596211087
Epoch 581
Epoch 581 :: Batch 0/1
Batch Loss = 0.0007309915596211087
581, epoch_train_loss=0.0007309915596211087
Epoch 582
Epoch 582 :: Batch 0/1
Batch Loss = 0.0007309915596211087
582, epoch_train_loss=0.0007309915596211087
Epoch 583
Epoch 583 :: Batch 0/1
Batch Loss = 0.0007309915596211087
583, epoch_train_loss=0.0007309915596211087
Epoch 584
Epoch 584 :: Batch 0/1
Batch Loss = 0.0007309915596211087
584, epoch_train_loss=0.0007309915596211087
Epoch 585
Epoch 585 :: Batch 0/1
Batch Loss = 0.0007309915596211087
585, epoch_train_loss=0.0007309915596211087
Epoch 586
Epoch 586 :: Batch 0/1
Batch Loss = 0.0007309915596211087
586, epoch_train_loss=0.0007309915596211087
Epoch 587
Epoch 587 :: Batch 0/1
Batch Loss = 0.0007309915596211087
587, epoch_train_loss=0.0007309915596211087
Epoch 588
Epoch 588 :: Batch 0/1
Batch Loss = 0.0007309915596211087
588, epoch_train_loss=0.0007309915596211087
Epoch 589
Epoch 589 :: Batch 0/1
Batch Loss = 0.0007309915596211087
589, epoch_train_loss=0.0007309915596211087
Epoch 590
Epoch 590 :: Batch 0/1
Batch Loss = 0.0007309915596211087
590, epoch_train_loss=0.0007309915596211087
Epoch 591
Epoch 591 :: Batch 0/1
Batch Loss = 0.0007309915596211087
591, epoch_train_loss=0.0007309915596211087
Epoch 592
Epoch 592 :: Batch 0/1
Batch Loss = 0.0007309915596211087
592, epoch_train_loss=0.0007309915596211087
Epoch 593
Epoch 593 :: Batch 0/1
Batch Loss = 0.0007309915596211087
593, epoch_train_loss=0.0007309915596211087
Epoch 594
Epoch 594 :: Batch 0/1
Batch Loss = 0.0007309915596211087
594, epoch_train_loss=0.0007309915596211087
Epoch 595
Epoch 595 :: Batch 0/1
Batch Loss = 0.0007309915596211087
595, epoch_train_loss=0.0007309915596211087
Epoch 596
Epoch 596 :: Batch 0/1
Batch Loss = 0.0007309915596211087
596, epoch_train_loss=0.0007309915596211087
Epoch 597
Epoch 597 :: Batch 0/1
Batch Loss = 0.0007309915596211087
597, epoch_train_loss=0.0007309915596211087
Epoch 598
Epoch 598 :: Batch 0/1
Batch Loss = 0.0007309915596211087
598, epoch_train_loss=0.0007309915596211087
Epoch 599
Epoch 599 :: Batch 0/1
Batch Loss = 0.0007309915596211087
599, epoch_train_loss=0.0007309915596211087
Epoch 600
Epoch 600 :: Batch 0/1
Batch Loss = 0.0007309915596211087
600, epoch_train_loss=0.0007309915596211087
Epoch 601
Epoch 601 :: Batch 0/1
Batch Loss = 0.0007309915596211087
601, epoch_train_loss=0.0007309915596211087
Epoch 602
Epoch 602 :: Batch 0/1
Batch Loss = 0.0007309915596211087
602, epoch_train_loss=0.0007309915596211087
Epoch 603
Epoch 603 :: Batch 0/1
Batch Loss = 0.0007309915596211087
603, epoch_train_loss=0.0007309915596211087
Epoch 604
Epoch 604 :: Batch 0/1
Batch Loss = 0.0007309915596211087
604, epoch_train_loss=0.0007309915596211087
Epoch 605
Epoch 605 :: Batch 0/1
Batch Loss = 0.0007309915596211087
605, epoch_train_loss=0.0007309915596211087
Epoch 606
Epoch 606 :: Batch 0/1
Batch Loss = 0.0007309915596211087
606, epoch_train_loss=0.0007309915596211087
Epoch 607
Epoch 607 :: Batch 0/1
Batch Loss = 0.0007309915596211087
607, epoch_train_loss=0.0007309915596211087
Epoch 608
Epoch 608 :: Batch 0/1
Batch Loss = 0.0007309915596211087
608, epoch_train_loss=0.0007309915596211087
Epoch 609
Epoch 609 :: Batch 0/1
Batch Loss = 0.0007309915596211087
609, epoch_train_loss=0.0007309915596211087
Epoch 610
Epoch 610 :: Batch 0/1
Batch Loss = 0.0007309915596211087
610, epoch_train_loss=0.0007309915596211087
Epoch 611
Epoch 611 :: Batch 0/1
Batch Loss = 0.0007309915596211087
611, epoch_train_loss=0.0007309915596211087
Epoch 612
Epoch 612 :: Batch 0/1
Batch Loss = 0.0007309915596211087
612, epoch_train_loss=0.0007309915596211087
Epoch 613
Epoch 613 :: Batch 0/1
Batch Loss = 0.0007309915596211087
613, epoch_train_loss=0.0007309915596211087
Epoch 614
Epoch 614 :: Batch 0/1
Batch Loss = 0.0007309915596211087
614, epoch_train_loss=0.0007309915596211087
Epoch 615
Epoch 615 :: Batch 0/1
Batch Loss = 0.0007309915596211087
615, epoch_train_loss=0.0007309915596211087
Epoch 616
Epoch 616 :: Batch 0/1
Batch Loss = 0.0007309915596211087
616, epoch_train_loss=0.0007309915596211087
Epoch 617
Epoch 617 :: Batch 0/1
Batch Loss = 0.0007309915596211087
617, epoch_train_loss=0.0007309915596211087
Epoch 618
Epoch 618 :: Batch 0/1
Batch Loss = 0.0007309915596211087
618, epoch_train_loss=0.0007309915596211087
Epoch 619
Epoch 619 :: Batch 0/1
Batch Loss = 0.0007309915596211087
619, epoch_train_loss=0.0007309915596211087
Epoch 620
Epoch 620 :: Batch 0/1
Batch Loss = 0.0007309915596211087
620, epoch_train_loss=0.0007309915596211087
Epoch 621
Epoch 621 :: Batch 0/1
Batch Loss = 0.0007309915596211087
621, epoch_train_loss=0.0007309915596211087
Epoch 622
Epoch 622 :: Batch 0/1
Batch Loss = 0.0007309915596211087
622, epoch_train_loss=0.0007309915596211087
Epoch 623
Epoch 623 :: Batch 0/1
Batch Loss = 0.0007309915596211087
623, epoch_train_loss=0.0007309915596211087
Epoch 624
Epoch 624 :: Batch 0/1
Batch Loss = 0.0007309915596211087
624, epoch_train_loss=0.0007309915596211087
Epoch 625
Epoch 625 :: Batch 0/1
Batch Loss = 0.0007309915596211087
625, epoch_train_loss=0.0007309915596211087
Epoch 626
Epoch 626 :: Batch 0/1
Batch Loss = 0.0007309915596211087
626, epoch_train_loss=0.0007309915596211087
Epoch 627
Epoch 627 :: Batch 0/1
Batch Loss = 0.0007309915596211087
627, epoch_train_loss=0.0007309915596211087
Epoch 628
Epoch 628 :: Batch 0/1
Batch Loss = 0.0007309915596211087
628, epoch_train_loss=0.0007309915596211087
Epoch 629
Epoch 629 :: Batch 0/1
Batch Loss = 0.0007309915596211087
629, epoch_train_loss=0.0007309915596211087
Epoch 630
Epoch 630 :: Batch 0/1
Batch Loss = 0.0007309915596211087
630, epoch_train_loss=0.0007309915596211087
Epoch 631
Epoch 631 :: Batch 0/1
Batch Loss = 0.0007309915596211087
631, epoch_train_loss=0.0007309915596211087
Epoch 632
Epoch 632 :: Batch 0/1
Batch Loss = 0.0007309915596211087
632, epoch_train_loss=0.0007309915596211087
Epoch 633
Epoch 633 :: Batch 0/1
Batch Loss = 0.0007309915596211087
633, epoch_train_loss=0.0007309915596211087
Epoch 634
Epoch 634 :: Batch 0/1
Batch Loss = 0.0007309915596211087
634, epoch_train_loss=0.0007309915596211087
Epoch 635
Epoch 635 :: Batch 0/1
Batch Loss = 0.0007309915596211087
635, epoch_train_loss=0.0007309915596211087
Epoch 636
Epoch 636 :: Batch 0/1
Batch Loss = 0.0007309915596211087
636, epoch_train_loss=0.0007309915596211087
Epoch 637
Epoch 637 :: Batch 0/1
Batch Loss = 0.0007309915596211087
637, epoch_train_loss=0.0007309915596211087
Epoch 638
Epoch 638 :: Batch 0/1
Batch Loss = 0.0007309915596211087
638, epoch_train_loss=0.0007309915596211087
Epoch 639
Epoch 639 :: Batch 0/1
Batch Loss = 0.0007309915596211087
639, epoch_train_loss=0.0007309915596211087
Epoch 640
Epoch 640 :: Batch 0/1
Batch Loss = 0.0007309915596211087
640, epoch_train_loss=0.0007309915596211087
Epoch 641
Epoch 641 :: Batch 0/1
Batch Loss = 0.0007309915596211087
641, epoch_train_loss=0.0007309915596211087
Epoch 642
Epoch 642 :: Batch 0/1
Batch Loss = 0.0007309915596211087
642, epoch_train_loss=0.0007309915596211087
Epoch 643
Epoch 643 :: Batch 0/1
Batch Loss = 0.0007309915596211087
643, epoch_train_loss=0.0007309915596211087
Epoch 644
Epoch 644 :: Batch 0/1
Batch Loss = 0.0007309915596211087
644, epoch_train_loss=0.0007309915596211087
Epoch 645
Epoch 645 :: Batch 0/1
Batch Loss = 0.0007309915596211087
645, epoch_train_loss=0.0007309915596211087
Epoch 646
Epoch 646 :: Batch 0/1
Batch Loss = 0.0007309915596211087
646, epoch_train_loss=0.0007309915596211087
Epoch 647
Epoch 647 :: Batch 0/1
Batch Loss = 0.0007309915596211087
647, epoch_train_loss=0.0007309915596211087
Epoch 648
Epoch 648 :: Batch 0/1
Batch Loss = 0.0007309915596211087
648, epoch_train_loss=0.0007309915596211087
Epoch 649
Epoch 649 :: Batch 0/1
Batch Loss = 0.0007309915596211087
649, epoch_train_loss=0.0007309915596211087
Epoch 650
Epoch 650 :: Batch 0/1
Batch Loss = 0.0007309915596211087
650, epoch_train_loss=0.0007309915596211087
Epoch 651
Epoch 651 :: Batch 0/1
Batch Loss = 0.0007309915596211087
651, epoch_train_loss=0.0007309915596211087
Epoch 652
Epoch 652 :: Batch 0/1
Batch Loss = 0.0007309915596211087
652, epoch_train_loss=0.0007309915596211087
Epoch 653
Epoch 653 :: Batch 0/1
Batch Loss = 0.0007309915596211087
653, epoch_train_loss=0.0007309915596211087
Epoch 654
Epoch 654 :: Batch 0/1
Batch Loss = 0.0007309915596211087
654, epoch_train_loss=0.0007309915596211087
Epoch 655
Epoch 655 :: Batch 0/1
Batch Loss = 0.0007309915596211087
655, epoch_train_loss=0.0007309915596211087
Epoch 656
Epoch 656 :: Batch 0/1
Batch Loss = 0.0007309915596211087
656, epoch_train_loss=0.0007309915596211087
Epoch 657
Epoch 657 :: Batch 0/1
Batch Loss = 0.0007309915596211087
657, epoch_train_loss=0.0007309915596211087
Epoch 658
Epoch 658 :: Batch 0/1
Batch Loss = 0.0007309915596211087
658, epoch_train_loss=0.0007309915596211087
Epoch 659
Epoch 659 :: Batch 0/1
Batch Loss = 0.0007309915596211087
659, epoch_train_loss=0.0007309915596211087
Epoch 660
Epoch 660 :: Batch 0/1
Batch Loss = 0.0007309915596211087
660, epoch_train_loss=0.0007309915596211087
Epoch 661
Epoch 661 :: Batch 0/1
Batch Loss = 0.0007309915596211087
661, epoch_train_loss=0.0007309915596211087
Epoch 662
Epoch 662 :: Batch 0/1
Batch Loss = 0.0007309915596211087
662, epoch_train_loss=0.0007309915596211087
Epoch 663
Epoch 663 :: Batch 0/1
Batch Loss = 0.0007309915596211087
663, epoch_train_loss=0.0007309915596211087
Epoch 664
Epoch 664 :: Batch 0/1
Batch Loss = 0.0007309915596211087
664, epoch_train_loss=0.0007309915596211087
Epoch 665
Epoch 665 :: Batch 0/1
Batch Loss = 0.0007309915596211087
665, epoch_train_loss=0.0007309915596211087
Epoch 666
Epoch 666 :: Batch 0/1
Batch Loss = 0.0007309915596211087
666, epoch_train_loss=0.0007309915596211087
Epoch 667
Epoch 667 :: Batch 0/1
Batch Loss = 0.0007309915596211087
667, epoch_train_loss=0.0007309915596211087
Epoch 668
Epoch 668 :: Batch 0/1
Batch Loss = 0.0007309915596211087
668, epoch_train_loss=0.0007309915596211087
Epoch 669
Epoch 669 :: Batch 0/1
Batch Loss = 0.0007309915596211087
669, epoch_train_loss=0.0007309915596211087
Epoch 670
Epoch 670 :: Batch 0/1
Batch Loss = 0.0007309915596211087
670, epoch_train_loss=0.0007309915596211087
Epoch 671
Epoch 671 :: Batch 0/1
Batch Loss = 0.0007309915596211087
671, epoch_train_loss=0.0007309915596211087
Epoch 672
Epoch 672 :: Batch 0/1
Batch Loss = 0.0007309915596211087
672, epoch_train_loss=0.0007309915596211087
Epoch 673
Epoch 673 :: Batch 0/1
Batch Loss = 0.0007309915596211087
673, epoch_train_loss=0.0007309915596211087
Epoch 674
Epoch 674 :: Batch 0/1
Batch Loss = 0.0007309915596211087
674, epoch_train_loss=0.0007309915596211087
Epoch 675
Epoch 675 :: Batch 0/1
Batch Loss = 0.0007309915596211087
675, epoch_train_loss=0.0007309915596211087
Epoch 676
Epoch 676 :: Batch 0/1
Batch Loss = 0.0007309915596211087
676, epoch_train_loss=0.0007309915596211087
Epoch 677
Epoch 677 :: Batch 0/1
Batch Loss = 0.0007309915596211087
677, epoch_train_loss=0.0007309915596211087
Epoch 678
Epoch 678 :: Batch 0/1
Batch Loss = 0.0007309915596211087
678, epoch_train_loss=0.0007309915596211087
Epoch 679
Epoch 679 :: Batch 0/1
Batch Loss = 0.0007309915596211087
679, epoch_train_loss=0.0007309915596211087
Epoch 680
Epoch 680 :: Batch 0/1
Batch Loss = 0.0007309915596211087
680, epoch_train_loss=0.0007309915596211087
Epoch 681
Epoch 681 :: Batch 0/1
Batch Loss = 0.0007309915596211087
681, epoch_train_loss=0.0007309915596211087
Epoch 682
Epoch 682 :: Batch 0/1
Batch Loss = 0.0007309915596211087
682, epoch_train_loss=0.0007309915596211087
Epoch 683
Epoch 683 :: Batch 0/1
Batch Loss = 0.0007309915596211087
683, epoch_train_loss=0.0007309915596211087
Epoch 684
Epoch 684 :: Batch 0/1
Batch Loss = 0.0007309915596211087
684, epoch_train_loss=0.0007309915596211087
Epoch 685
Epoch 685 :: Batch 0/1
Batch Loss = 0.0007309915596211087
685, epoch_train_loss=0.0007309915596211087
Epoch 686
Epoch 686 :: Batch 0/1
Batch Loss = 0.0007309915596211087
686, epoch_train_loss=0.0007309915596211087
Epoch 687
Epoch 687 :: Batch 0/1
Batch Loss = 0.0007309915596211087
687, epoch_train_loss=0.0007309915596211087
Epoch 688
Epoch 688 :: Batch 0/1
Batch Loss = 0.0007309915596211087
688, epoch_train_loss=0.0007309915596211087
Epoch 689
Epoch 689 :: Batch 0/1
Batch Loss = 0.0007309915596211087
689, epoch_train_loss=0.0007309915596211087
Epoch 690
Epoch 690 :: Batch 0/1
Batch Loss = 0.0007309915596211087
690, epoch_train_loss=0.0007309915596211087
Epoch 691
Epoch 691 :: Batch 0/1
Batch Loss = 0.0007309915596211087
691, epoch_train_loss=0.0007309915596211087
Epoch 692
Epoch 692 :: Batch 0/1
Batch Loss = 0.0007309915596211087
692, epoch_train_loss=0.0007309915596211087
Epoch 693
Epoch 693 :: Batch 0/1
Batch Loss = 0.0007309915596211087
693, epoch_train_loss=0.0007309915596211087
Epoch 694
Epoch 694 :: Batch 0/1
Batch Loss = 0.0007309915596211087
694, epoch_train_loss=0.0007309915596211087
Epoch 695
Epoch 695 :: Batch 0/1
Batch Loss = 0.0007309915596211087
695, epoch_train_loss=0.0007309915596211087
Epoch 696
Epoch 696 :: Batch 0/1
Batch Loss = 0.0007309915596211087
696, epoch_train_loss=0.0007309915596211087
Epoch 697
Epoch 697 :: Batch 0/1
Batch Loss = 0.0007309915596211087
697, epoch_train_loss=0.0007309915596211087
Epoch 698
Epoch 698 :: Batch 0/1
Batch Loss = 0.0007309915596211087
698, epoch_train_loss=0.0007309915596211087
Epoch 699
Epoch 699 :: Batch 0/1
Batch Loss = 0.0007309915596211087
699, epoch_train_loss=0.0007309915596211087
Epoch 700
Epoch 700 :: Batch 0/1
Batch Loss = 0.0007309915596211087
700, epoch_train_loss=0.0007309915596211087
Epoch 701
Epoch 701 :: Batch 0/1
Batch Loss = 0.0007309915596211087
701, epoch_train_loss=0.0007309915596211087
Epoch 702
Epoch 702 :: Batch 0/1
Batch Loss = 0.0007309915596211087
702, epoch_train_loss=0.0007309915596211087
Epoch 703
Epoch 703 :: Batch 0/1
Batch Loss = 0.0007309915596211087
703, epoch_train_loss=0.0007309915596211087
Epoch 704
Epoch 704 :: Batch 0/1
Batch Loss = 0.0007309915596211087
704, epoch_train_loss=0.0007309915596211087
Epoch 705
Epoch 705 :: Batch 0/1
Batch Loss = 0.0007309915596211087
705, epoch_train_loss=0.0007309915596211087
Epoch 706
Epoch 706 :: Batch 0/1
Batch Loss = 0.0007309915596211087
706, epoch_train_loss=0.0007309915596211087
Epoch 707
Epoch 707 :: Batch 0/1
Batch Loss = 0.0007309915596211087
707, epoch_train_loss=0.0007309915596211087
Epoch 708
Epoch 708 :: Batch 0/1
Batch Loss = 0.0007309915596211087
708, epoch_train_loss=0.0007309915596211087
Epoch 709
Epoch 709 :: Batch 0/1
Batch Loss = 0.0007309915596211087
709, epoch_train_loss=0.0007309915596211087
Epoch 710
Epoch 710 :: Batch 0/1
Batch Loss = 0.0007309915596211087
710, epoch_train_loss=0.0007309915596211087
Epoch 711
Epoch 711 :: Batch 0/1
Batch Loss = 0.0007309915596211087
711, epoch_train_loss=0.0007309915596211087
Epoch 712
Epoch 712 :: Batch 0/1
Batch Loss = 0.0007309915596211087
712, epoch_train_loss=0.0007309915596211087
Epoch 713
Epoch 713 :: Batch 0/1
Batch Loss = 0.0007309915596211087
713, epoch_train_loss=0.0007309915596211087
Epoch 714
Epoch 714 :: Batch 0/1
Batch Loss = 0.0007309915596211087
714, epoch_train_loss=0.0007309915596211087
Epoch 715
Epoch 715 :: Batch 0/1
Batch Loss = 0.0007309915596211087
715, epoch_train_loss=0.0007309915596211087
Epoch 716
Epoch 716 :: Batch 0/1
Batch Loss = 0.0007309915596211087
716, epoch_train_loss=0.0007309915596211087
Epoch 717
Epoch 717 :: Batch 0/1
Batch Loss = 0.0007309915596211087
717, epoch_train_loss=0.0007309915596211087
Epoch 718
Epoch 718 :: Batch 0/1
Batch Loss = 0.0007309915596211087
718, epoch_train_loss=0.0007309915596211087
Epoch 719
Epoch 719 :: Batch 0/1
Batch Loss = 0.0007309915596211087
719, epoch_train_loss=0.0007309915596211087
Epoch 720
Epoch 720 :: Batch 0/1
Batch Loss = 0.0007309915596211087
720, epoch_train_loss=0.0007309915596211087
Epoch 721
Epoch 721 :: Batch 0/1
Batch Loss = 0.0007309915596211087
721, epoch_train_loss=0.0007309915596211087
Epoch 722
Epoch 722 :: Batch 0/1
Batch Loss = 0.0007309915596211087
722, epoch_train_loss=0.0007309915596211087
Epoch 723
Epoch 723 :: Batch 0/1
Batch Loss = 0.0007309915596211087
723, epoch_train_loss=0.0007309915596211087
Epoch 724
Epoch 724 :: Batch 0/1
Batch Loss = 0.0007309915596211087
724, epoch_train_loss=0.0007309915596211087
Epoch 725
Epoch 725 :: Batch 0/1
Batch Loss = 0.0007309915596211087
725, epoch_train_loss=0.0007309915596211087
Epoch 726
Epoch 726 :: Batch 0/1
Batch Loss = 0.0007309915596211087
726, epoch_train_loss=0.0007309915596211087
Epoch 727
Epoch 727 :: Batch 0/1
Batch Loss = 0.0007309915596211087
727, epoch_train_loss=0.0007309915596211087
Epoch 728
Epoch 728 :: Batch 0/1
Batch Loss = 0.0007309915596211087
728, epoch_train_loss=0.0007309915596211087
Epoch 729
Epoch 729 :: Batch 0/1
Batch Loss = 0.0007309915596211087
729, epoch_train_loss=0.0007309915596211087
Epoch 730
Epoch 730 :: Batch 0/1
Batch Loss = 0.0007309915596211087
730, epoch_train_loss=0.0007309915596211087
Epoch 731
Epoch 731 :: Batch 0/1
Batch Loss = 0.0007309915596211087
731, epoch_train_loss=0.0007309915596211087
Epoch 732
Epoch 732 :: Batch 0/1
Batch Loss = 0.0007309915596211087
732, epoch_train_loss=0.0007309915596211087
Epoch 733
Epoch 733 :: Batch 0/1
Batch Loss = 0.0007309915596211087
733, epoch_train_loss=0.0007309915596211087
Epoch 734
Epoch 734 :: Batch 0/1
Batch Loss = 0.0007309915596211087
734, epoch_train_loss=0.0007309915596211087
Epoch 735
Epoch 735 :: Batch 0/1
Batch Loss = 0.0007309915596211087
735, epoch_train_loss=0.0007309915596211087
Epoch 736
Epoch 736 :: Batch 0/1
Batch Loss = 0.0007309915596211087
736, epoch_train_loss=0.0007309915596211087
Epoch 737
Epoch 737 :: Batch 0/1
Batch Loss = 0.0007309915596211087
737, epoch_train_loss=0.0007309915596211087
Epoch 738
Epoch 738 :: Batch 0/1
Batch Loss = 0.0007309915596211087
738, epoch_train_loss=0.0007309915596211087
Epoch 739
Epoch 739 :: Batch 0/1
Batch Loss = 0.0007309915596211087
739, epoch_train_loss=0.0007309915596211087
Epoch 740
Epoch 740 :: Batch 0/1
Batch Loss = 0.0007309915596211087
740, epoch_train_loss=0.0007309915596211087
Epoch 741
Epoch 741 :: Batch 0/1
Batch Loss = 0.0007309915596211087
741, epoch_train_loss=0.0007309915596211087
Epoch 742
Epoch 742 :: Batch 0/1
Batch Loss = 0.0007309915596211087
742, epoch_train_loss=0.0007309915596211087
Epoch 743
Epoch 743 :: Batch 0/1
Batch Loss = 0.0007309915596211087
743, epoch_train_loss=0.0007309915596211087
Epoch 744
Epoch 744 :: Batch 0/1
Batch Loss = 0.0007309915596211087
744, epoch_train_loss=0.0007309915596211087
Epoch 745
Epoch 745 :: Batch 0/1
Batch Loss = 0.0007309915596211087
745, epoch_train_loss=0.0007309915596211087
Epoch 746
Epoch 746 :: Batch 0/1
Batch Loss = 0.0007309915596211087
746, epoch_train_loss=0.0007309915596211087
Epoch 747
Epoch 747 :: Batch 0/1
Batch Loss = 0.0007309915596211087
747, epoch_train_loss=0.0007309915596211087
Epoch 748
Epoch 748 :: Batch 0/1
Batch Loss = 0.0007309915596211087
748, epoch_train_loss=0.0007309915596211087
Epoch 749
Epoch 749 :: Batch 0/1
Batch Loss = 0.0007309915596211087
749, epoch_train_loss=0.0007309915596211087
Epoch 750
Epoch 750 :: Batch 0/1
Batch Loss = 0.0007309915596211087
750, epoch_train_loss=0.0007309915596211087
Epoch 751
Epoch 751 :: Batch 0/1
Batch Loss = 0.0007309915596211087
751, epoch_train_loss=0.0007309915596211087
Epoch 752
Epoch 752 :: Batch 0/1
Batch Loss = 0.0007309915596211087
752, epoch_train_loss=0.0007309915596211087
Epoch 753
Epoch 753 :: Batch 0/1
Batch Loss = 0.0007309915596211087
753, epoch_train_loss=0.0007309915596211087
Epoch 754
Epoch 754 :: Batch 0/1
Batch Loss = 0.0007309915596211087
754, epoch_train_loss=0.0007309915596211087
Epoch 755
Epoch 755 :: Batch 0/1
Batch Loss = 0.0007309915596211087
755, epoch_train_loss=0.0007309915596211087
Epoch 756
Epoch 756 :: Batch 0/1
Batch Loss = 0.0007309915596211087
756, epoch_train_loss=0.0007309915596211087
Epoch 757
Epoch 757 :: Batch 0/1
Batch Loss = 0.0007309915596211087
757, epoch_train_loss=0.0007309915596211087
Epoch 758
Epoch 758 :: Batch 0/1
Batch Loss = 0.0007309915596211087
758, epoch_train_loss=0.0007309915596211087
Epoch 759
Epoch 759 :: Batch 0/1
Batch Loss = 0.0007309915596211087
759, epoch_train_loss=0.0007309915596211087
Epoch 760
Epoch 760 :: Batch 0/1
Batch Loss = 0.0007309915596211087
760, epoch_train_loss=0.0007309915596211087
Epoch 761
Epoch 761 :: Batch 0/1
Batch Loss = 0.0007309915596211087
761, epoch_train_loss=0.0007309915596211087
Epoch 762
Epoch 762 :: Batch 0/1
Batch Loss = 0.0007309915596211087
762, epoch_train_loss=0.0007309915596211087
Epoch 763
Epoch 763 :: Batch 0/1
Batch Loss = 0.0007309915596211087
763, epoch_train_loss=0.0007309915596211087
Epoch 764
Epoch 764 :: Batch 0/1
Batch Loss = 0.0007309915596211087
764, epoch_train_loss=0.0007309915596211087
Epoch 765
Epoch 765 :: Batch 0/1
Batch Loss = 0.0007309915596211087
765, epoch_train_loss=0.0007309915596211087
Epoch 766
Epoch 766 :: Batch 0/1
Batch Loss = 0.0007309915596211087
766, epoch_train_loss=0.0007309915596211087
Epoch 767
Epoch 767 :: Batch 0/1
Batch Loss = 0.0007309915596211087
767, epoch_train_loss=0.0007309915596211087
Epoch 768
Epoch 768 :: Batch 0/1
Batch Loss = 0.0007309915596211087
768, epoch_train_loss=0.0007309915596211087
Epoch 769
Epoch 769 :: Batch 0/1
Batch Loss = 0.0007309915596211087
769, epoch_train_loss=0.0007309915596211087
Epoch 770
Epoch 770 :: Batch 0/1
Batch Loss = 0.0007309915596211087
770, epoch_train_loss=0.0007309915596211087
Epoch 771
Epoch 771 :: Batch 0/1
Batch Loss = 0.0007309915596211087
771, epoch_train_loss=0.0007309915596211087
Epoch 772
Epoch 772 :: Batch 0/1
Batch Loss = 0.0007309915596211087
772, epoch_train_loss=0.0007309915596211087
Epoch 773
Epoch 773 :: Batch 0/1
Batch Loss = 0.0007309915596211087
773, epoch_train_loss=0.0007309915596211087
Epoch 774
Epoch 774 :: Batch 0/1
Batch Loss = 0.0007309915596211087
774, epoch_train_loss=0.0007309915596211087
Epoch 775
Epoch 775 :: Batch 0/1
Batch Loss = 0.0007309915596211087
775, epoch_train_loss=0.0007309915596211087
Epoch 776
Epoch 776 :: Batch 0/1
Batch Loss = 0.0007309915596211087
776, epoch_train_loss=0.0007309915596211087
Epoch 777
Epoch 777 :: Batch 0/1
Batch Loss = 0.0007309915596211087
777, epoch_train_loss=0.0007309915596211087
Epoch 778
Epoch 778 :: Batch 0/1
Batch Loss = 0.0007309915596211087
778, epoch_train_loss=0.0007309915596211087
Epoch 779
Epoch 779 :: Batch 0/1
Batch Loss = 0.0007309915596211087
779, epoch_train_loss=0.0007309915596211087
Epoch 780
Epoch 780 :: Batch 0/1
Batch Loss = 0.0007309915596211087
780, epoch_train_loss=0.0007309915596211087
Epoch 781
Epoch 781 :: Batch 0/1
Batch Loss = 0.0007309915596211087
781, epoch_train_loss=0.0007309915596211087
Epoch 782
Epoch 782 :: Batch 0/1
Batch Loss = 0.0007309915596211087
782, epoch_train_loss=0.0007309915596211087
Epoch 783
Epoch 783 :: Batch 0/1
Batch Loss = 0.0007309915596211087
783, epoch_train_loss=0.0007309915596211087
Epoch 784
Epoch 784 :: Batch 0/1
Batch Loss = 0.0007309915596211087
784, epoch_train_loss=0.0007309915596211087
Epoch 785
Epoch 785 :: Batch 0/1
Batch Loss = 0.0007309915596211087
785, epoch_train_loss=0.0007309915596211087
Epoch 786
Epoch 786 :: Batch 0/1
Batch Loss = 0.0007309915596211087
786, epoch_train_loss=0.0007309915596211087
Epoch 787
Epoch 787 :: Batch 0/1
Batch Loss = 0.0007309915596211087
787, epoch_train_loss=0.0007309915596211087
Epoch 788
Epoch 788 :: Batch 0/1
Batch Loss = 0.0007309915596211087
788, epoch_train_loss=0.0007309915596211087
Epoch 789
Epoch 789 :: Batch 0/1
Batch Loss = 0.0007309915596211087
789, epoch_train_loss=0.0007309915596211087
Epoch 790
Epoch 790 :: Batch 0/1
Batch Loss = 0.0007309915596211087
790, epoch_train_loss=0.0007309915596211087
Epoch 791
Epoch 791 :: Batch 0/1
Batch Loss = 0.0007309915596211087
791, epoch_train_loss=0.0007309915596211087
Epoch 792
Epoch 792 :: Batch 0/1
Batch Loss = 0.0007309915596211087
792, epoch_train_loss=0.0007309915596211087
Epoch 793
Epoch 793 :: Batch 0/1
Batch Loss = 0.0007309915596211087
793, epoch_train_loss=0.0007309915596211087
Epoch 794
Epoch 794 :: Batch 0/1
Batch Loss = 0.0007309915596211087
794, epoch_train_loss=0.0007309915596211087
Epoch 795
Epoch 795 :: Batch 0/1
Batch Loss = 0.0007309915596211087
795, epoch_train_loss=0.0007309915596211087
Epoch 796
Epoch 796 :: Batch 0/1
Batch Loss = 0.0007309915596211087
796, epoch_train_loss=0.0007309915596211087
Epoch 797
Epoch 797 :: Batch 0/1
Batch Loss = 0.0007309915596211087
797, epoch_train_loss=0.0007309915596211087
Epoch 798
Epoch 798 :: Batch 0/1
Batch Loss = 0.0007309915596211087
798, epoch_train_loss=0.0007309915596211087
Epoch 799
Epoch 799 :: Batch 0/1
Batch Loss = 0.0007309915596211087
799, epoch_train_loss=0.0007309915596211087
Epoch 800
Epoch 800 :: Batch 0/1
Batch Loss = 0.0007309915596211087
800, epoch_train_loss=0.0007309915596211087
Epoch 801
Epoch 801 :: Batch 0/1
Batch Loss = 0.0007309915596211087
801, epoch_train_loss=0.0007309915596211087
Epoch 802
Epoch 802 :: Batch 0/1
Batch Loss = 0.0007309915596211087
802, epoch_train_loss=0.0007309915596211087
Epoch 803
Epoch 803 :: Batch 0/1
Batch Loss = 0.0007309915596211087
803, epoch_train_loss=0.0007309915596211087
Epoch 804
Epoch 804 :: Batch 0/1
Batch Loss = 0.0007309915596211087
804, epoch_train_loss=0.0007309915596211087
Epoch 805
Epoch 805 :: Batch 0/1
Batch Loss = 0.0007309915596211087
805, epoch_train_loss=0.0007309915596211087
Epoch 806
Epoch 806 :: Batch 0/1
Batch Loss = 0.0007309915596211087
806, epoch_train_loss=0.0007309915596211087
Epoch 807
Epoch 807 :: Batch 0/1
Batch Loss = 0.0007309915596211087
807, epoch_train_loss=0.0007309915596211087
Epoch 808
Epoch 808 :: Batch 0/1
Batch Loss = 0.0007309915596211087
808, epoch_train_loss=0.0007309915596211087
Epoch 809
Epoch 809 :: Batch 0/1
Batch Loss = 0.0007309915596211087
809, epoch_train_loss=0.0007309915596211087
Epoch 810
Epoch 810 :: Batch 0/1
Batch Loss = 0.0007309915596211087
810, epoch_train_loss=0.0007309915596211087
Epoch 811
Epoch 811 :: Batch 0/1
Batch Loss = 0.0007309915596211087
811, epoch_train_loss=0.0007309915596211087
Epoch 812
Epoch 812 :: Batch 0/1
Batch Loss = 0.0007309915596211087
812, epoch_train_loss=0.0007309915596211087
Epoch 813
Epoch 813 :: Batch 0/1
Batch Loss = 0.0007309915596211087
813, epoch_train_loss=0.0007309915596211087
Epoch 814
Epoch 814 :: Batch 0/1
Batch Loss = 0.0007309915596211087
814, epoch_train_loss=0.0007309915596211087
Epoch 815
Epoch 815 :: Batch 0/1
Batch Loss = 0.0007309915596211087
815, epoch_train_loss=0.0007309915596211087
Epoch 816
Epoch 816 :: Batch 0/1
Batch Loss = 0.0007309915596211087
816, epoch_train_loss=0.0007309915596211087
Epoch 817
Epoch 817 :: Batch 0/1
Batch Loss = 0.0007309915596211087
817, epoch_train_loss=0.0007309915596211087
Epoch 818
Epoch 818 :: Batch 0/1
Batch Loss = 0.0007309915596211087
818, epoch_train_loss=0.0007309915596211087
Epoch 819
Epoch 819 :: Batch 0/1
Batch Loss = 0.0007309915596211087
819, epoch_train_loss=0.0007309915596211087
Epoch 820
Epoch 820 :: Batch 0/1
Batch Loss = 0.0007309915596211087
820, epoch_train_loss=0.0007309915596211087
Epoch 821
Epoch 821 :: Batch 0/1
Batch Loss = 0.0007309915596211087
821, epoch_train_loss=0.0007309915596211087
Epoch 822
Epoch 822 :: Batch 0/1
Batch Loss = 0.0007309915596211087
822, epoch_train_loss=0.0007309915596211087
Epoch 823
Epoch 823 :: Batch 0/1
Batch Loss = 0.0007309915596211087
823, epoch_train_loss=0.0007309915596211087
Epoch 824
Epoch 824 :: Batch 0/1
Batch Loss = 0.0007309915596211087
824, epoch_train_loss=0.0007309915596211087
Epoch 825
Epoch 825 :: Batch 0/1
Batch Loss = 0.0007309915596211087
825, epoch_train_loss=0.0007309915596211087
Epoch 826
Epoch 826 :: Batch 0/1
Batch Loss = 0.0007309915596211087
826, epoch_train_loss=0.0007309915596211087
Epoch 827
Epoch 827 :: Batch 0/1
Batch Loss = 0.0007309915596211087
827, epoch_train_loss=0.0007309915596211087
Epoch 828
Epoch 828 :: Batch 0/1
Batch Loss = 0.0007309915596211087
828, epoch_train_loss=0.0007309915596211087
Epoch 829
Epoch 829 :: Batch 0/1
Batch Loss = 0.0007309915596211087
829, epoch_train_loss=0.0007309915596211087
Epoch 830
Epoch 830 :: Batch 0/1
Batch Loss = 0.0007309915596211087
830, epoch_train_loss=0.0007309915596211087
Epoch 831
Epoch 831 :: Batch 0/1
Batch Loss = 0.0007309915596211087
831, epoch_train_loss=0.0007309915596211087
Epoch 832
Epoch 832 :: Batch 0/1
Batch Loss = 0.0007309915596211087
832, epoch_train_loss=0.0007309915596211087
Epoch 833
Epoch 833 :: Batch 0/1
Batch Loss = 0.0007309915596211087
833, epoch_train_loss=0.0007309915596211087
Epoch 834
Epoch 834 :: Batch 0/1
Batch Loss = 0.0007309915596211087
834, epoch_train_loss=0.0007309915596211087
Epoch 835
Epoch 835 :: Batch 0/1
Batch Loss = 0.0007309915596211087
835, epoch_train_loss=0.0007309915596211087
Epoch 836
Epoch 836 :: Batch 0/1
Batch Loss = 0.0007309915596211087
836, epoch_train_loss=0.0007309915596211087
Epoch 837
Epoch 837 :: Batch 0/1
Batch Loss = 0.0007309915596211087
837, epoch_train_loss=0.0007309915596211087
Epoch 838
Epoch 838 :: Batch 0/1
Batch Loss = 0.0007309915596211087
838, epoch_train_loss=0.0007309915596211087
Epoch 839
Epoch 839 :: Batch 0/1
Batch Loss = 0.0007309915596211087
839, epoch_train_loss=0.0007309915596211087
Epoch 840
Epoch 840 :: Batch 0/1
Batch Loss = 0.0007309915596211087
840, epoch_train_loss=0.0007309915596211087
Epoch 841
Epoch 841 :: Batch 0/1
Batch Loss = 0.0007309915596211087
841, epoch_train_loss=0.0007309915596211087
Epoch 842
Epoch 842 :: Batch 0/1
Batch Loss = 0.0007309915596211087
842, epoch_train_loss=0.0007309915596211087
Epoch 843
Epoch 843 :: Batch 0/1
Batch Loss = 0.0007309915596211087
843, epoch_train_loss=0.0007309915596211087
Epoch 844
Epoch 844 :: Batch 0/1
Batch Loss = 0.0007309915596211087
844, epoch_train_loss=0.0007309915596211087
Epoch 845
Epoch 845 :: Batch 0/1
Batch Loss = 0.0007309915596211087
845, epoch_train_loss=0.0007309915596211087
Epoch 846
Epoch 846 :: Batch 0/1
Batch Loss = 0.0007309915596211087
846, epoch_train_loss=0.0007309915596211087
Epoch 847
Epoch 847 :: Batch 0/1
Batch Loss = 0.0007309915596211087
847, epoch_train_loss=0.0007309915596211087
Epoch 848
Epoch 848 :: Batch 0/1
Batch Loss = 0.0007309915596211087
848, epoch_train_loss=0.0007309915596211087
Epoch 849
Epoch 849 :: Batch 0/1
Batch Loss = 0.0007309915596211087
849, epoch_train_loss=0.0007309915596211087
Epoch 850
Epoch 850 :: Batch 0/1
Batch Loss = 0.0007309915596211087
850, epoch_train_loss=0.0007309915596211087
Epoch 851
Epoch 851 :: Batch 0/1
Batch Loss = 0.0007309915596211087
851, epoch_train_loss=0.0007309915596211087
Epoch 852
Epoch 852 :: Batch 0/1
Batch Loss = 0.0007309915596211087
852, epoch_train_loss=0.0007309915596211087
Epoch 853
Epoch 853 :: Batch 0/1
Batch Loss = 0.0007309915596211087
853, epoch_train_loss=0.0007309915596211087
Epoch 854
Epoch 854 :: Batch 0/1
Batch Loss = 0.0007309915596211087
854, epoch_train_loss=0.0007309915596211087
Epoch 855
Epoch 855 :: Batch 0/1
Batch Loss = 0.0007309915596211087
855, epoch_train_loss=0.0007309915596211087
Epoch 856
Epoch 856 :: Batch 0/1
Batch Loss = 0.0007309915596211087
856, epoch_train_loss=0.0007309915596211087
Epoch 857
Epoch 857 :: Batch 0/1
Batch Loss = 0.0007309915596211087
857, epoch_train_loss=0.0007309915596211087
Epoch 858
Epoch 858 :: Batch 0/1
Batch Loss = 0.0007309915596211087
858, epoch_train_loss=0.0007309915596211087
Epoch 859
Epoch 859 :: Batch 0/1
Batch Loss = 0.0007309915596211087
859, epoch_train_loss=0.0007309915596211087
Epoch 860
Epoch 860 :: Batch 0/1
Batch Loss = 0.0007309915596211087
860, epoch_train_loss=0.0007309915596211087
Epoch 861
Epoch 861 :: Batch 0/1
Batch Loss = 0.0007309915596211087
861, epoch_train_loss=0.0007309915596211087
Epoch 862
Epoch 862 :: Batch 0/1
Batch Loss = 0.0007309915596211087
862, epoch_train_loss=0.0007309915596211087
Epoch 863
Epoch 863 :: Batch 0/1
Batch Loss = 0.0007309915596211087
863, epoch_train_loss=0.0007309915596211087
Epoch 864
Epoch 864 :: Batch 0/1
Batch Loss = 0.0007309915596211087
864, epoch_train_loss=0.0007309915596211087
Epoch 865
Epoch 865 :: Batch 0/1
Batch Loss = 0.0007309915596211087
865, epoch_train_loss=0.0007309915596211087
Epoch 866
Epoch 866 :: Batch 0/1
Batch Loss = 0.0007309915596211087
866, epoch_train_loss=0.0007309915596211087
Epoch 867
Epoch 867 :: Batch 0/1
Batch Loss = 0.0007309915596211087
867, epoch_train_loss=0.0007309915596211087
Epoch 868
Epoch 868 :: Batch 0/1
Batch Loss = 0.0007309915596211087
868, epoch_train_loss=0.0007309915596211087
Epoch 869
Epoch 869 :: Batch 0/1
Batch Loss = 0.0007309915596211087
869, epoch_train_loss=0.0007309915596211087
Epoch 870
Epoch 870 :: Batch 0/1
Batch Loss = 0.0007309915596211087
870, epoch_train_loss=0.0007309915596211087
Epoch 871
Epoch 871 :: Batch 0/1
Batch Loss = 0.0007309915596211087
871, epoch_train_loss=0.0007309915596211087
Epoch 872
Epoch 872 :: Batch 0/1
Batch Loss = 0.0007309915596211087
872, epoch_train_loss=0.0007309915596211087
Epoch 873
Epoch 873 :: Batch 0/1
Batch Loss = 0.0007309915596211087
873, epoch_train_loss=0.0007309915596211087
Epoch 874
Epoch 874 :: Batch 0/1
Batch Loss = 0.0007309915596211087
874, epoch_train_loss=0.0007309915596211087
Epoch 875
Epoch 875 :: Batch 0/1
Batch Loss = 0.0007309915596211087
875, epoch_train_loss=0.0007309915596211087
Epoch 876
Epoch 876 :: Batch 0/1
Batch Loss = 0.0007309915596211087
876, epoch_train_loss=0.0007309915596211087
Epoch 877
Epoch 877 :: Batch 0/1
Batch Loss = 0.0007309915596211087
877, epoch_train_loss=0.0007309915596211087
Epoch 878
Epoch 878 :: Batch 0/1
Batch Loss = 0.0007309915596211087
878, epoch_train_loss=0.0007309915596211087
Epoch 879
Epoch 879 :: Batch 0/1
Batch Loss = 0.0007309915596211087
879, epoch_train_loss=0.0007309915596211087
Epoch 880
Epoch 880 :: Batch 0/1
Batch Loss = 0.0007309915596211087
880, epoch_train_loss=0.0007309915596211087
Epoch 881
Epoch 881 :: Batch 0/1
Batch Loss = 0.0007309915596211087
881, epoch_train_loss=0.0007309915596211087
Epoch 882
Epoch 882 :: Batch 0/1
Batch Loss = 0.0007309915596211087
882, epoch_train_loss=0.0007309915596211087
Epoch 883
Epoch 883 :: Batch 0/1
Batch Loss = 0.0007309915596211087
883, epoch_train_loss=0.0007309915596211087
Epoch 884
Epoch 884 :: Batch 0/1
Batch Loss = 0.0007309915596211087
884, epoch_train_loss=0.0007309915596211087
Epoch 885
Epoch 885 :: Batch 0/1
Batch Loss = 0.0007309915596211087
885, epoch_train_loss=0.0007309915596211087
Epoch 886
Epoch 886 :: Batch 0/1
Batch Loss = 0.0007309915596211087
886, epoch_train_loss=0.0007309915596211087
Epoch 887
Epoch 887 :: Batch 0/1
Batch Loss = 0.0007309915596211087
887, epoch_train_loss=0.0007309915596211087
Epoch 888
Epoch 888 :: Batch 0/1
Batch Loss = 0.0007309915596211087
888, epoch_train_loss=0.0007309915596211087
Epoch 889
Epoch 889 :: Batch 0/1
Batch Loss = 0.0007309915596211087
889, epoch_train_loss=0.0007309915596211087
Epoch 890
Epoch 890 :: Batch 0/1
Batch Loss = 0.0007309915596211087
890, epoch_train_loss=0.0007309915596211087
Epoch 891
Epoch 891 :: Batch 0/1
Batch Loss = 0.0007309915596211087
891, epoch_train_loss=0.0007309915596211087
Epoch 892
Epoch 892 :: Batch 0/1
Batch Loss = 0.0007309915596211087
892, epoch_train_loss=0.0007309915596211087
Epoch 893
Epoch 893 :: Batch 0/1
Batch Loss = 0.0007309915596211087
893, epoch_train_loss=0.0007309915596211087
Epoch 894
Epoch 894 :: Batch 0/1
Batch Loss = 0.0007309915596211087
894, epoch_train_loss=0.0007309915596211087
Epoch 895
Epoch 895 :: Batch 0/1
Batch Loss = 0.0007309915596211087
895, epoch_train_loss=0.0007309915596211087
Epoch 896
Epoch 896 :: Batch 0/1
Batch Loss = 0.0007309915596211087
896, epoch_train_loss=0.0007309915596211087
Epoch 897
Epoch 897 :: Batch 0/1
Batch Loss = 0.0007309915596211087
897, epoch_train_loss=0.0007309915596211087
Epoch 898
Epoch 898 :: Batch 0/1
Batch Loss = 0.0007309915596211087
898, epoch_train_loss=0.0007309915596211087
Epoch 899
Epoch 899 :: Batch 0/1
Batch Loss = 0.0007309915596211087
899, epoch_train_loss=0.0007309915596211087
Epoch 900
Epoch 900 :: Batch 0/1
Batch Loss = 0.0007309915596211087
900, epoch_train_loss=0.0007309915596211087
Epoch 901
Epoch 901 :: Batch 0/1
Batch Loss = 0.0007309915596211087
901, epoch_train_loss=0.0007309915596211087
Epoch 902
Epoch 902 :: Batch 0/1
Batch Loss = 0.0007309915596211087
902, epoch_train_loss=0.0007309915596211087
Epoch 903
Epoch 903 :: Batch 0/1
Batch Loss = 0.0007309915596211087
903, epoch_train_loss=0.0007309915596211087
Epoch 904
Epoch 904 :: Batch 0/1
Batch Loss = 0.0007309915596211087
904, epoch_train_loss=0.0007309915596211087
Epoch 905
Epoch 905 :: Batch 0/1
Batch Loss = 0.0007309915596211087
905, epoch_train_loss=0.0007309915596211087
Epoch 906
Epoch 906 :: Batch 0/1
Batch Loss = 0.0007309915596211087
906, epoch_train_loss=0.0007309915596211087
Epoch 907
Epoch 907 :: Batch 0/1
Batch Loss = 0.0007309915596211087
907, epoch_train_loss=0.0007309915596211087
Epoch 908
Epoch 908 :: Batch 0/1
Batch Loss = 0.0007309915596211087
908, epoch_train_loss=0.0007309915596211087
Epoch 909
Epoch 909 :: Batch 0/1
Batch Loss = 0.0007309915596211087
909, epoch_train_loss=0.0007309915596211087
Epoch 910
Epoch 910 :: Batch 0/1
Batch Loss = 0.0007309915596211087
910, epoch_train_loss=0.0007309915596211087
Epoch 911
Epoch 911 :: Batch 0/1
Batch Loss = 0.0007309915596211087
911, epoch_train_loss=0.0007309915596211087
Epoch 912
Epoch 912 :: Batch 0/1
Batch Loss = 0.0007309915596211087
912, epoch_train_loss=0.0007309915596211087
Epoch 913
Epoch 913 :: Batch 0/1
Batch Loss = 0.0007309915596211087
913, epoch_train_loss=0.0007309915596211087
Epoch 914
Epoch 914 :: Batch 0/1
Batch Loss = 0.0007309915596211087
914, epoch_train_loss=0.0007309915596211087
Epoch 915
Epoch 915 :: Batch 0/1
Batch Loss = 0.0007309915596211087
915, epoch_train_loss=0.0007309915596211087
Epoch 916
Epoch 916 :: Batch 0/1
Batch Loss = 0.0007309915596211087
916, epoch_train_loss=0.0007309915596211087
Epoch 917
Epoch 917 :: Batch 0/1
Batch Loss = 0.0007309915596211087
917, epoch_train_loss=0.0007309915596211087
Epoch 918
Epoch 918 :: Batch 0/1
Batch Loss = 0.0007309915596211087
918, epoch_train_loss=0.0007309915596211087
Epoch 919
Epoch 919 :: Batch 0/1
Batch Loss = 0.0007309915596211087
919, epoch_train_loss=0.0007309915596211087
Epoch 920
Epoch 920 :: Batch 0/1
Batch Loss = 0.0007309915596211087
920, epoch_train_loss=0.0007309915596211087
Epoch 921
Epoch 921 :: Batch 0/1
Batch Loss = 0.0007309915596211087
921, epoch_train_loss=0.0007309915596211087
Epoch 922
Epoch 922 :: Batch 0/1
Batch Loss = 0.0007309915596211087
922, epoch_train_loss=0.0007309915596211087
Epoch 923
Epoch 923 :: Batch 0/1
Batch Loss = 0.0007309915596211087
923, epoch_train_loss=0.0007309915596211087
Epoch 924
Epoch 924 :: Batch 0/1
Batch Loss = 0.0007309915596211087
924, epoch_train_loss=0.0007309915596211087
Epoch 925
Epoch 925 :: Batch 0/1
Batch Loss = 0.0007309915596211087
925, epoch_train_loss=0.0007309915596211087
Epoch 926
Epoch 926 :: Batch 0/1
Batch Loss = 0.0007309915596211087
926, epoch_train_loss=0.0007309915596211087
Epoch 927
Epoch 927 :: Batch 0/1
Batch Loss = 0.0007309915596211087
927, epoch_train_loss=0.0007309915596211087
Epoch 928
Epoch 928 :: Batch 0/1
Batch Loss = 0.0007309915596211087
928, epoch_train_loss=0.0007309915596211087
Epoch 929
Epoch 929 :: Batch 0/1
Batch Loss = 0.0007309915596211087
929, epoch_train_loss=0.0007309915596211087
Epoch 930
Epoch 930 :: Batch 0/1
Batch Loss = 0.0007309915596211087
930, epoch_train_loss=0.0007309915596211087
Epoch 931
Epoch 931 :: Batch 0/1
Batch Loss = 0.0007309915596211087
931, epoch_train_loss=0.0007309915596211087
Epoch 932
Epoch 932 :: Batch 0/1
Batch Loss = 0.0007309915596211087
932, epoch_train_loss=0.0007309915596211087
Epoch 933
Epoch 933 :: Batch 0/1
Batch Loss = 0.0007309915596211087
933, epoch_train_loss=0.0007309915596211087
Epoch 934
Epoch 934 :: Batch 0/1
Batch Loss = 0.0007309915596211087
934, epoch_train_loss=0.0007309915596211087
Epoch 935
Epoch 935 :: Batch 0/1
Batch Loss = 0.0007309915596211087
935, epoch_train_loss=0.0007309915596211087
Epoch 936
Epoch 936 :: Batch 0/1
Batch Loss = 0.0007309915596211087
936, epoch_train_loss=0.0007309915596211087
Epoch 937
Epoch 937 :: Batch 0/1
Batch Loss = 0.0007309915596211087
937, epoch_train_loss=0.0007309915596211087
Epoch 938
Epoch 938 :: Batch 0/1
Batch Loss = 0.0007309915596211087
938, epoch_train_loss=0.0007309915596211087
Epoch 939
Epoch 939 :: Batch 0/1
Batch Loss = 0.0007309915596211087
939, epoch_train_loss=0.0007309915596211087
Epoch 940
Epoch 940 :: Batch 0/1
Batch Loss = 0.0007309915596211087
940, epoch_train_loss=0.0007309915596211087
Epoch 941
Epoch 941 :: Batch 0/1
Batch Loss = 0.0007309915596211087
941, epoch_train_loss=0.0007309915596211087
Epoch 942
Epoch 942 :: Batch 0/1
Batch Loss = 0.0007309915596211087
942, epoch_train_loss=0.0007309915596211087
Epoch 943
Epoch 943 :: Batch 0/1
Batch Loss = 0.0007309915596211087
943, epoch_train_loss=0.0007309915596211087
Epoch 944
Epoch 944 :: Batch 0/1
Batch Loss = 0.0007309915596211087
944, epoch_train_loss=0.0007309915596211087
Epoch 945
Epoch 945 :: Batch 0/1
Batch Loss = 0.0007309915596211087
945, epoch_train_loss=0.0007309915596211087
Epoch 946
Epoch 946 :: Batch 0/1
Batch Loss = 0.0007309915596211087
946, epoch_train_loss=0.0007309915596211087
Epoch 947
Epoch 947 :: Batch 0/1
Batch Loss = 0.0007309915596211087
947, epoch_train_loss=0.0007309915596211087
Epoch 948
Epoch 948 :: Batch 0/1
Batch Loss = 0.0007309915596211087
948, epoch_train_loss=0.0007309915596211087
Epoch 949
Epoch 949 :: Batch 0/1
Batch Loss = 0.0007309915596211087
949, epoch_train_loss=0.0007309915596211087
Epoch 950
Epoch 950 :: Batch 0/1
Batch Loss = 0.0007309915596211087
950, epoch_train_loss=0.0007309915596211087
Epoch 951
Epoch 951 :: Batch 0/1
Batch Loss = 0.0007309915596211087
951, epoch_train_loss=0.0007309915596211087
Epoch 952
Epoch 952 :: Batch 0/1
Batch Loss = 0.0007309915596211087
952, epoch_train_loss=0.0007309915596211087
Epoch 953
Epoch 953 :: Batch 0/1
Batch Loss = 0.0007309915596211087
953, epoch_train_loss=0.0007309915596211087
Epoch 954
Epoch 954 :: Batch 0/1
Batch Loss = 0.0007309915596211087
954, epoch_train_loss=0.0007309915596211087
Epoch 955
Epoch 955 :: Batch 0/1
Batch Loss = 0.0007309915596211087
955, epoch_train_loss=0.0007309915596211087
Epoch 956
Epoch 956 :: Batch 0/1
Batch Loss = 0.0007309915596211087
956, epoch_train_loss=0.0007309915596211087
Epoch 957
Epoch 957 :: Batch 0/1
Batch Loss = 0.0007309915596211087
957, epoch_train_loss=0.0007309915596211087
Epoch 958
Epoch 958 :: Batch 0/1
Batch Loss = 0.0007309915596211087
958, epoch_train_loss=0.0007309915596211087
Epoch 959
Epoch 959 :: Batch 0/1
Batch Loss = 0.0007309915596211087
959, epoch_train_loss=0.0007309915596211087
Epoch 960
Epoch 960 :: Batch 0/1
Batch Loss = 0.0007309915596211087
960, epoch_train_loss=0.0007309915596211087
Epoch 961
Epoch 961 :: Batch 0/1
Batch Loss = 0.0007309915596211087
961, epoch_train_loss=0.0007309915596211087
Epoch 962
Epoch 962 :: Batch 0/1
Batch Loss = 0.0007309915596211087
962, epoch_train_loss=0.0007309915596211087
Epoch 963
Epoch 963 :: Batch 0/1
Batch Loss = 0.0007309915596211087
963, epoch_train_loss=0.0007309915596211087
Epoch 964
Epoch 964 :: Batch 0/1
Batch Loss = 0.0007309915596211087
964, epoch_train_loss=0.0007309915596211087
Epoch 965
Epoch 965 :: Batch 0/1
Batch Loss = 0.0007309915596211087
965, epoch_train_loss=0.0007309915596211087
Epoch 966
Epoch 966 :: Batch 0/1
Batch Loss = 0.0007309915596211087
966, epoch_train_loss=0.0007309915596211087
Epoch 967
Epoch 967 :: Batch 0/1
Batch Loss = 0.0007309915596211087
967, epoch_train_loss=0.0007309915596211087
Epoch 968
Epoch 968 :: Batch 0/1
Batch Loss = 0.0007309915596211087
968, epoch_train_loss=0.0007309915596211087
Epoch 969
Epoch 969 :: Batch 0/1
Batch Loss = 0.0007309915596211087
969, epoch_train_loss=0.0007309915596211087
Epoch 970
Epoch 970 :: Batch 0/1
Batch Loss = 0.0007309915596211087
970, epoch_train_loss=0.0007309915596211087
Epoch 971
Epoch 971 :: Batch 0/1
Batch Loss = 0.0007309915596211087
971, epoch_train_loss=0.0007309915596211087
Epoch 972
Epoch 972 :: Batch 0/1
Batch Loss = 0.0007309915596211087
972, epoch_train_loss=0.0007309915596211087
Epoch 973
Epoch 973 :: Batch 0/1
Batch Loss = 0.0007309915596211087
973, epoch_train_loss=0.0007309915596211087
Epoch 974
Epoch 974 :: Batch 0/1
Batch Loss = 0.0007309915596211087
974, epoch_train_loss=0.0007309915596211087
Epoch 975
Epoch 975 :: Batch 0/1
Batch Loss = 0.0007309915596211087
975, epoch_train_loss=0.0007309915596211087
Epoch 976
Epoch 976 :: Batch 0/1
Batch Loss = 0.0007309915596211087
976, epoch_train_loss=0.0007309915596211087
Epoch 977
Epoch 977 :: Batch 0/1
Batch Loss = 0.0007309915596211087
977, epoch_train_loss=0.0007309915596211087
Epoch 978
Epoch 978 :: Batch 0/1
Batch Loss = 0.0007309915596211087
978, epoch_train_loss=0.0007309915596211087
Epoch 979
Epoch 979 :: Batch 0/1
Batch Loss = 0.0007309915596211087
979, epoch_train_loss=0.0007309915596211087
Epoch 980
Epoch 980 :: Batch 0/1
Batch Loss = 0.0007309915596211087
980, epoch_train_loss=0.0007309915596211087
Epoch 981
Epoch 981 :: Batch 0/1
Batch Loss = 0.0007309915596211087
981, epoch_train_loss=0.0007309915596211087
Epoch 982
Epoch 982 :: Batch 0/1
Batch Loss = 0.0007309915596211087
982, epoch_train_loss=0.0007309915596211087
Epoch 983
Epoch 983 :: Batch 0/1
Batch Loss = 0.0007309915596211087
983, epoch_train_loss=0.0007309915596211087
Epoch 984
Epoch 984 :: Batch 0/1
Batch Loss = 0.0007309915596211087
984, epoch_train_loss=0.0007309915596211087
Epoch 985
Epoch 985 :: Batch 0/1
Batch Loss = 0.0007309915596211087
985, epoch_train_loss=0.0007309915596211087
Epoch 986
Epoch 986 :: Batch 0/1
Batch Loss = 0.0007309915596211087
986, epoch_train_loss=0.0007309915596211087
Epoch 987
Epoch 987 :: Batch 0/1
Batch Loss = 0.0007309915596211087
987, epoch_train_loss=0.0007309915596211087
Epoch 988
Epoch 988 :: Batch 0/1
Batch Loss = 0.0007309915596211087
988, epoch_train_loss=0.0007309915596211087
Epoch 989
Epoch 989 :: Batch 0/1
Batch Loss = 0.0007309915596211087
989, epoch_train_loss=0.0007309915596211087
Epoch 990
Epoch 990 :: Batch 0/1
Batch Loss = 0.0007309915596211087
990, epoch_train_loss=0.0007309915596211087
Epoch 991
Epoch 991 :: Batch 0/1
Batch Loss = 0.0007309915596211087
991, epoch_train_loss=0.0007309915596211087
Epoch 992
Epoch 992 :: Batch 0/1
Batch Loss = 0.0007309915596211087
992, epoch_train_loss=0.0007309915596211087
Epoch 993
Epoch 993 :: Batch 0/1
Batch Loss = 0.0007309915596211087
993, epoch_train_loss=0.0007309915596211087
Epoch 994
Epoch 994 :: Batch 0/1
Batch Loss = 0.0007309915596211087
994, epoch_train_loss=0.0007309915596211087
Epoch 995
Epoch 995 :: Batch 0/1
Batch Loss = 0.0007309915596211087
995, epoch_train_loss=0.0007309915596211087
Epoch 996
Epoch 996 :: Batch 0/1
Batch Loss = 0.0007309915596211087
996, epoch_train_loss=0.0007309915596211087
Epoch 997
Epoch 997 :: Batch 0/1
Batch Loss = 0.0007309915596211087
997, epoch_train_loss=0.0007309915596211087
Epoch 998
Epoch 998 :: Batch 0/1
Batch Loss = 0.0007309915596211087
998, epoch_train_loss=0.0007309915596211087
Epoch 999
Epoch 999 :: Batch 0/1
Batch Loss = 0.0007309915596211087
999, epoch_train_loss=0.0007309915596211087
Epoch 1000
Epoch 1000 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1000, epoch_train_loss=0.0007309915596211087
Epoch 1001
Epoch 1001 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1001, epoch_train_loss=0.0007309915596211087
Epoch 1002
Epoch 1002 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1002, epoch_train_loss=0.0007309915596211087
Epoch 1003
Epoch 1003 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1003, epoch_train_loss=0.0007309915596211087
Epoch 1004
Epoch 1004 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1004, epoch_train_loss=0.0007309915596211087
Epoch 1005
Epoch 1005 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1005, epoch_train_loss=0.0007309915596211087
Epoch 1006
Epoch 1006 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1006, epoch_train_loss=0.0007309915596211087
Epoch 1007
Epoch 1007 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1007, epoch_train_loss=0.0007309915596211087
Epoch 1008
Epoch 1008 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1008, epoch_train_loss=0.0007309915596211087
Epoch 1009
Epoch 1009 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1009, epoch_train_loss=0.0007309915596211087
Epoch 1010
Epoch 1010 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1010, epoch_train_loss=0.0007309915596211087
Epoch 1011
Epoch 1011 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1011, epoch_train_loss=0.0007309915596211087
Epoch 1012
Epoch 1012 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1012, epoch_train_loss=0.0007309915596211087
Epoch 1013
Epoch 1013 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1013, epoch_train_loss=0.0007309915596211087
Epoch 1014
Epoch 1014 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1014, epoch_train_loss=0.0007309915596211087
Epoch 1015
Epoch 1015 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1015, epoch_train_loss=0.0007309915596211087
Epoch 1016
Epoch 1016 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1016, epoch_train_loss=0.0007309915596211087
Epoch 1017
Epoch 1017 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1017, epoch_train_loss=0.0007309915596211087
Epoch 1018
Epoch 1018 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1018, epoch_train_loss=0.0007309915596211087
Epoch 1019
Epoch 1019 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1019, epoch_train_loss=0.0007309915596211087
Epoch 1020
Epoch 1020 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1020, epoch_train_loss=0.0007309915596211087
Epoch 1021
Epoch 1021 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1021, epoch_train_loss=0.0007309915596211087
Epoch 1022
Epoch 1022 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1022, epoch_train_loss=0.0007309915596211087
Epoch 1023
Epoch 1023 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1023, epoch_train_loss=0.0007309915596211087
Epoch 1024
Epoch 1024 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1024, epoch_train_loss=0.0007309915596211087
Epoch 1025
Epoch 1025 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1025, epoch_train_loss=0.0007309915596211087
Epoch 1026
Epoch 1026 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1026, epoch_train_loss=0.0007309915596211087
Epoch 1027
Epoch 1027 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1027, epoch_train_loss=0.0007309915596211087
Epoch 1028
Epoch 1028 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1028, epoch_train_loss=0.0007309915596211087
Epoch 1029
Epoch 1029 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1029, epoch_train_loss=0.0007309915596211087
Epoch 1030
Epoch 1030 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1030, epoch_train_loss=0.0007309915596211087
Epoch 1031
Epoch 1031 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1031, epoch_train_loss=0.0007309915596211087
Epoch 1032
Epoch 1032 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1032, epoch_train_loss=0.0007309915596211087
Epoch 1033
Epoch 1033 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1033, epoch_train_loss=0.0007309915596211087
Epoch 1034
Epoch 1034 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1034, epoch_train_loss=0.0007309915596211087
Epoch 1035
Epoch 1035 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1035, epoch_train_loss=0.0007309915596211087
Epoch 1036
Epoch 1036 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1036, epoch_train_loss=0.0007309915596211087
Epoch 1037
Epoch 1037 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1037, epoch_train_loss=0.0007309915596211087
Epoch 1038
Epoch 1038 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1038, epoch_train_loss=0.0007309915596211087
Epoch 1039
Epoch 1039 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1039, epoch_train_loss=0.0007309915596211087
Epoch 1040
Epoch 1040 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1040, epoch_train_loss=0.0007309915596211087
Epoch 1041
Epoch 1041 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1041, epoch_train_loss=0.0007309915596211087
Epoch 1042
Epoch 1042 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1042, epoch_train_loss=0.0007309915596211087
Epoch 1043
Epoch 1043 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1043, epoch_train_loss=0.0007309915596211087
Epoch 1044
Epoch 1044 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1044, epoch_train_loss=0.0007309915596211087
Epoch 1045
Epoch 1045 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1045, epoch_train_loss=0.0007309915596211087
Epoch 1046
Epoch 1046 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1046, epoch_train_loss=0.0007309915596211087
Epoch 1047
Epoch 1047 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1047, epoch_train_loss=0.0007309915596211087
Epoch 1048
Epoch 1048 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1048, epoch_train_loss=0.0007309915596211087
Epoch 1049
Epoch 1049 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1049, epoch_train_loss=0.0007309915596211087
Epoch 1050
Epoch 1050 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1050, epoch_train_loss=0.0007309915596211087
Epoch 1051
Epoch 1051 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1051, epoch_train_loss=0.0007309915596211087
Epoch 1052
Epoch 1052 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1052, epoch_train_loss=0.0007309915596211087
Epoch 1053
Epoch 1053 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1053, epoch_train_loss=0.0007309915596211087
Epoch 1054
Epoch 1054 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1054, epoch_train_loss=0.0007309915596211087
Epoch 1055
Epoch 1055 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1055, epoch_train_loss=0.0007309915596211087
Epoch 1056
Epoch 1056 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1056, epoch_train_loss=0.0007309915596211087
Epoch 1057
Epoch 1057 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1057, epoch_train_loss=0.0007309915596211087
Epoch 1058
Epoch 1058 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1058, epoch_train_loss=0.0007309915596211087
Epoch 1059
Epoch 1059 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1059, epoch_train_loss=0.0007309915596211087
Epoch 1060
Epoch 1060 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1060, epoch_train_loss=0.0007309915596211087
Epoch 1061
Epoch 1061 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1061, epoch_train_loss=0.0007309915596211087
Epoch 1062
Epoch 1062 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1062, epoch_train_loss=0.0007309915596211087
Epoch 1063
Epoch 1063 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1063, epoch_train_loss=0.0007309915596211087
Epoch 1064
Epoch 1064 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1064, epoch_train_loss=0.0007309915596211087
Epoch 1065
Epoch 1065 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1065, epoch_train_loss=0.0007309915596211087
Epoch 1066
Epoch 1066 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1066, epoch_train_loss=0.0007309915596211087
Epoch 1067
Epoch 1067 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1067, epoch_train_loss=0.0007309915596211087
Epoch 1068
Epoch 1068 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1068, epoch_train_loss=0.0007309915596211087
Epoch 1069
Epoch 1069 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1069, epoch_train_loss=0.0007309915596211087
Epoch 1070
Epoch 1070 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1070, epoch_train_loss=0.0007309915596211087
Epoch 1071
Epoch 1071 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1071, epoch_train_loss=0.0007309915596211087
Epoch 1072
Epoch 1072 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1072, epoch_train_loss=0.0007309915596211087
Epoch 1073
Epoch 1073 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1073, epoch_train_loss=0.0007309915596211087
Epoch 1074
Epoch 1074 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1074, epoch_train_loss=0.0007309915596211087
Epoch 1075
Epoch 1075 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1075, epoch_train_loss=0.0007309915596211087
Epoch 1076
Epoch 1076 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1076, epoch_train_loss=0.0007309915596211087
Epoch 1077
Epoch 1077 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1077, epoch_train_loss=0.0007309915596211087
Epoch 1078
Epoch 1078 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1078, epoch_train_loss=0.0007309915596211087
Epoch 1079
Epoch 1079 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1079, epoch_train_loss=0.0007309915596211087
Epoch 1080
Epoch 1080 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1080, epoch_train_loss=0.0007309915596211087
Epoch 1081
Epoch 1081 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1081, epoch_train_loss=0.0007309915596211087
Epoch 1082
Epoch 1082 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1082, epoch_train_loss=0.0007309915596211087
Epoch 1083
Epoch 1083 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1083, epoch_train_loss=0.0007309915596211087
Epoch 1084
Epoch 1084 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1084, epoch_train_loss=0.0007309915596211087
Epoch 1085
Epoch 1085 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1085, epoch_train_loss=0.0007309915596211087
Epoch 1086
Epoch 1086 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1086, epoch_train_loss=0.0007309915596211087
Epoch 1087
Epoch 1087 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1087, epoch_train_loss=0.0007309915596211087
Epoch 1088
Epoch 1088 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1088, epoch_train_loss=0.0007309915596211087
Epoch 1089
Epoch 1089 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1089, epoch_train_loss=0.0007309915596211087
Epoch 1090
Epoch 1090 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1090, epoch_train_loss=0.0007309915596211087
Epoch 1091
Epoch 1091 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1091, epoch_train_loss=0.0007309915596211087
Epoch 1092
Epoch 1092 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1092, epoch_train_loss=0.0007309915596211087
Epoch 1093
Epoch 1093 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1093, epoch_train_loss=0.0007309915596211087
Epoch 1094
Epoch 1094 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1094, epoch_train_loss=0.0007309915596211087
Epoch 1095
Epoch 1095 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1095, epoch_train_loss=0.0007309915596211087
Epoch 1096
Epoch 1096 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1096, epoch_train_loss=0.0007309915596211087
Epoch 1097
Epoch 1097 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1097, epoch_train_loss=0.0007309915596211087
Epoch 1098
Epoch 1098 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1098, epoch_train_loss=0.0007309915596211087
Epoch 1099
Epoch 1099 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1099, epoch_train_loss=0.0007309915596211087
Epoch 1100
Epoch 1100 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1100, epoch_train_loss=0.0007309915596211087
Epoch 1101
Epoch 1101 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1101, epoch_train_loss=0.0007309915596211087
Epoch 1102
Epoch 1102 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1102, epoch_train_loss=0.0007309915596211087
Epoch 1103
Epoch 1103 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1103, epoch_train_loss=0.0007309915596211087
Epoch 1104
Epoch 1104 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1104, epoch_train_loss=0.0007309915596211087
Epoch 1105
Epoch 1105 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1105, epoch_train_loss=0.0007309915596211087
Epoch 1106
Epoch 1106 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1106, epoch_train_loss=0.0007309915596211087
Epoch 1107
Epoch 1107 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1107, epoch_train_loss=0.0007309915596211087
Epoch 1108
Epoch 1108 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1108, epoch_train_loss=0.0007309915596211087
Epoch 1109
Epoch 1109 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1109, epoch_train_loss=0.0007309915596211087
Epoch 1110
Epoch 1110 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1110, epoch_train_loss=0.0007309915596211087
Epoch 1111
Epoch 1111 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1111, epoch_train_loss=0.0007309915596211087
Epoch 1112
Epoch 1112 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1112, epoch_train_loss=0.0007309915596211087
Epoch 1113
Epoch 1113 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1113, epoch_train_loss=0.0007309915596211087
Epoch 1114
Epoch 1114 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1114, epoch_train_loss=0.0007309915596211087
Epoch 1115
Epoch 1115 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1115, epoch_train_loss=0.0007309915596211087
Epoch 1116
Epoch 1116 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1116, epoch_train_loss=0.0007309915596211087
Epoch 1117
Epoch 1117 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1117, epoch_train_loss=0.0007309915596211087
Epoch 1118
Epoch 1118 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1118, epoch_train_loss=0.0007309915596211087
Epoch 1119
Epoch 1119 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1119, epoch_train_loss=0.0007309915596211087
Epoch 1120
Epoch 1120 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1120, epoch_train_loss=0.0007309915596211087
Epoch 1121
Epoch 1121 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1121, epoch_train_loss=0.0007309915596211087
Epoch 1122
Epoch 1122 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1122, epoch_train_loss=0.0007309915596211087
Epoch 1123
Epoch 1123 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1123, epoch_train_loss=0.0007309915596211087
Epoch 1124
Epoch 1124 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1124, epoch_train_loss=0.0007309915596211087
Epoch 1125
Epoch 1125 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1125, epoch_train_loss=0.0007309915596211087
Epoch 1126
Epoch 1126 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1126, epoch_train_loss=0.0007309915596211087
Epoch 1127
Epoch 1127 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1127, epoch_train_loss=0.0007309915596211087
Epoch 1128
Epoch 1128 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1128, epoch_train_loss=0.0007309915596211087
Epoch 1129
Epoch 1129 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1129, epoch_train_loss=0.0007309915596211087
Epoch 1130
Epoch 1130 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1130, epoch_train_loss=0.0007309915596211087
Epoch 1131
Epoch 1131 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1131, epoch_train_loss=0.0007309915596211087
Epoch 1132
Epoch 1132 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1132, epoch_train_loss=0.0007309915596211087
Epoch 1133
Epoch 1133 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1133, epoch_train_loss=0.0007309915596211087
Epoch 1134
Epoch 1134 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1134, epoch_train_loss=0.0007309915596211087
Epoch 1135
Epoch 1135 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1135, epoch_train_loss=0.0007309915596211087
Epoch 1136
Epoch 1136 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1136, epoch_train_loss=0.0007309915596211087
Epoch 1137
Epoch 1137 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1137, epoch_train_loss=0.0007309915596211087
Epoch 1138
Epoch 1138 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1138, epoch_train_loss=0.0007309915596211087
Epoch 1139
Epoch 1139 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1139, epoch_train_loss=0.0007309915596211087
Epoch 1140
Epoch 1140 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1140, epoch_train_loss=0.0007309915596211087
Epoch 1141
Epoch 1141 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1141, epoch_train_loss=0.0007309915596211087
Epoch 1142
Epoch 1142 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1142, epoch_train_loss=0.0007309915596211087
Epoch 1143
Epoch 1143 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1143, epoch_train_loss=0.0007309915596211087
Epoch 1144
Epoch 1144 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1144, epoch_train_loss=0.0007309915596211087
Epoch 1145
Epoch 1145 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1145, epoch_train_loss=0.0007309915596211087
Epoch 1146
Epoch 1146 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1146, epoch_train_loss=0.0007309915596211087
Epoch 1147
Epoch 1147 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1147, epoch_train_loss=0.0007309915596211087
Epoch 1148
Epoch 1148 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1148, epoch_train_loss=0.0007309915596211087
Epoch 1149
Epoch 1149 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1149, epoch_train_loss=0.0007309915596211087
Epoch 1150
Epoch 1150 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1150, epoch_train_loss=0.0007309915596211087
Epoch 1151
Epoch 1151 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1151, epoch_train_loss=0.0007309915596211087
Epoch 1152
Epoch 1152 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1152, epoch_train_loss=0.0007309915596211087
Epoch 1153
Epoch 1153 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1153, epoch_train_loss=0.0007309915596211087
Epoch 1154
Epoch 1154 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1154, epoch_train_loss=0.0007309915596211087
Epoch 1155
Epoch 1155 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1155, epoch_train_loss=0.0007309915596211087
Epoch 1156
Epoch 1156 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1156, epoch_train_loss=0.0007309915596211087
Epoch 1157
Epoch 1157 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1157, epoch_train_loss=0.0007309915596211087
Epoch 1158
Epoch 1158 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1158, epoch_train_loss=0.0007309915596211087
Epoch 1159
Epoch 1159 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1159, epoch_train_loss=0.0007309915596211087
Epoch 1160
Epoch 1160 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1160, epoch_train_loss=0.0007309915596211087
Epoch 1161
Epoch 1161 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1161, epoch_train_loss=0.0007309915596211087
Epoch 1162
Epoch 1162 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1162, epoch_train_loss=0.0007309915596211087
Epoch 1163
Epoch 1163 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1163, epoch_train_loss=0.0007309915596211087
Epoch 1164
Epoch 1164 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1164, epoch_train_loss=0.0007309915596211087
Epoch 1165
Epoch 1165 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1165, epoch_train_loss=0.0007309915596211087
Epoch 1166
Epoch 1166 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1166, epoch_train_loss=0.0007309915596211087
Epoch 1167
Epoch 1167 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1167, epoch_train_loss=0.0007309915596211087
Epoch 1168
Epoch 1168 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1168, epoch_train_loss=0.0007309915596211087
Epoch 1169
Epoch 1169 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1169, epoch_train_loss=0.0007309915596211087
Epoch 1170
Epoch 1170 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1170, epoch_train_loss=0.0007309915596211087
Epoch 1171
Epoch 1171 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1171, epoch_train_loss=0.0007309915596211087
Epoch 1172
Epoch 1172 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1172, epoch_train_loss=0.0007309915596211087
Epoch 1173
Epoch 1173 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1173, epoch_train_loss=0.0007309915596211087
Epoch 1174
Epoch 1174 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1174, epoch_train_loss=0.0007309915596211087
Epoch 1175
Epoch 1175 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1175, epoch_train_loss=0.0007309915596211087
Epoch 1176
Epoch 1176 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1176, epoch_train_loss=0.0007309915596211087
Epoch 1177
Epoch 1177 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1177, epoch_train_loss=0.0007309915596211087
Epoch 1178
Epoch 1178 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1178, epoch_train_loss=0.0007309915596211087
Epoch 1179
Epoch 1179 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1179, epoch_train_loss=0.0007309915596211087
Epoch 1180
Epoch 1180 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1180, epoch_train_loss=0.0007309915596211087
Epoch 1181
Epoch 1181 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1181, epoch_train_loss=0.0007309915596211087
Epoch 1182
Epoch 1182 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1182, epoch_train_loss=0.0007309915596211087
Epoch 1183
Epoch 1183 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1183, epoch_train_loss=0.0007309915596211087
Epoch 1184
Epoch 1184 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1184, epoch_train_loss=0.0007309915596211087
Epoch 1185
Epoch 1185 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1185, epoch_train_loss=0.0007309915596211087
Epoch 1186
Epoch 1186 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1186, epoch_train_loss=0.0007309915596211087
Epoch 1187
Epoch 1187 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1187, epoch_train_loss=0.0007309915596211087
Epoch 1188
Epoch 1188 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1188, epoch_train_loss=0.0007309915596211087
Epoch 1189
Epoch 1189 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1189, epoch_train_loss=0.0007309915596211087
Epoch 1190
Epoch 1190 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1190, epoch_train_loss=0.0007309915596211087
Epoch 1191
Epoch 1191 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1191, epoch_train_loss=0.0007309915596211087
Epoch 1192
Epoch 1192 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1192, epoch_train_loss=0.0007309915596211087
Epoch 1193
Epoch 1193 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1193, epoch_train_loss=0.0007309915596211087
Epoch 1194
Epoch 1194 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1194, epoch_train_loss=0.0007309915596211087
Epoch 1195
Epoch 1195 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1195, epoch_train_loss=0.0007309915596211087
Epoch 1196
Epoch 1196 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1196, epoch_train_loss=0.0007309915596211087
Epoch 1197
Epoch 1197 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1197, epoch_train_loss=0.0007309915596211087
Epoch 1198
Epoch 1198 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1198, epoch_train_loss=0.0007309915596211087
Epoch 1199
Epoch 1199 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1199, epoch_train_loss=0.0007309915596211087
Epoch 1200
Epoch 1200 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1200, epoch_train_loss=0.0007309915596211087
Epoch 1201
Epoch 1201 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1201, epoch_train_loss=0.0007309915596211087
Epoch 1202
Epoch 1202 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1202, epoch_train_loss=0.0007309915596211087
Epoch 1203
Epoch 1203 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1203, epoch_train_loss=0.0007309915596211087
Epoch 1204
Epoch 1204 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1204, epoch_train_loss=0.0007309915596211087
Epoch 1205
Epoch 1205 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1205, epoch_train_loss=0.0007309915596211087
Epoch 1206
Epoch 1206 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1206, epoch_train_loss=0.0007309915596211087
Epoch 1207
Epoch 1207 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1207, epoch_train_loss=0.0007309915596211087
Epoch 1208
Epoch 1208 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1208, epoch_train_loss=0.0007309915596211087
Epoch 1209
Epoch 1209 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1209, epoch_train_loss=0.0007309915596211087
Epoch 1210
Epoch 1210 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1210, epoch_train_loss=0.0007309915596211087
Epoch 1211
Epoch 1211 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1211, epoch_train_loss=0.0007309915596211087
Epoch 1212
Epoch 1212 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1212, epoch_train_loss=0.0007309915596211087
Epoch 1213
Epoch 1213 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1213, epoch_train_loss=0.0007309915596211087
Epoch 1214
Epoch 1214 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1214, epoch_train_loss=0.0007309915596211087
Epoch 1215
Epoch 1215 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1215, epoch_train_loss=0.0007309915596211087
Epoch 1216
Epoch 1216 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1216, epoch_train_loss=0.0007309915596211087
Epoch 1217
Epoch 1217 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1217, epoch_train_loss=0.0007309915596211087
Epoch 1218
Epoch 1218 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1218, epoch_train_loss=0.0007309915596211087
Epoch 1219
Epoch 1219 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1219, epoch_train_loss=0.0007309915596211087
Epoch 1220
Epoch 1220 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1220, epoch_train_loss=0.0007309915596211087
Epoch 1221
Epoch 1221 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1221, epoch_train_loss=0.0007309915596211087
Epoch 1222
Epoch 1222 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1222, epoch_train_loss=0.0007309915596211087
Epoch 1223
Epoch 1223 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1223, epoch_train_loss=0.0007309915596211087
Epoch 1224
Epoch 1224 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1224, epoch_train_loss=0.0007309915596211087
Epoch 1225
Epoch 1225 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1225, epoch_train_loss=0.0007309915596211087
Epoch 1226
Epoch 1226 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1226, epoch_train_loss=0.0007309915596211087
Epoch 1227
Epoch 1227 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1227, epoch_train_loss=0.0007309915596211087
Epoch 1228
Epoch 1228 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1228, epoch_train_loss=0.0007309915596211087
Epoch 1229
Epoch 1229 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1229, epoch_train_loss=0.0007309915596211087
Epoch 1230
Epoch 1230 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1230, epoch_train_loss=0.0007309915596211087
Epoch 1231
Epoch 1231 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1231, epoch_train_loss=0.0007309915596211087
Epoch 1232
Epoch 1232 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1232, epoch_train_loss=0.0007309915596211087
Epoch 1233
Epoch 1233 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1233, epoch_train_loss=0.0007309915596211087
Epoch 1234
Epoch 1234 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1234, epoch_train_loss=0.0007309915596211087
Epoch 1235
Epoch 1235 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1235, epoch_train_loss=0.0007309915596211087
Epoch 1236
Epoch 1236 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1236, epoch_train_loss=0.0007309915596211087
Epoch 1237
Epoch 1237 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1237, epoch_train_loss=0.0007309915596211087
Epoch 1238
Epoch 1238 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1238, epoch_train_loss=0.0007309915596211087
Epoch 1239
Epoch 1239 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1239, epoch_train_loss=0.0007309915596211087
Epoch 1240
Epoch 1240 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1240, epoch_train_loss=0.0007309915596211087
Epoch 1241
Epoch 1241 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1241, epoch_train_loss=0.0007309915596211087
Epoch 1242
Epoch 1242 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1242, epoch_train_loss=0.0007309915596211087
Epoch 1243
Epoch 1243 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1243, epoch_train_loss=0.0007309915596211087
Epoch 1244
Epoch 1244 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1244, epoch_train_loss=0.0007309915596211087
Epoch 1245
Epoch 1245 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1245, epoch_train_loss=0.0007309915596211087
Epoch 1246
Epoch 1246 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1246, epoch_train_loss=0.0007309915596211087
Epoch 1247
Epoch 1247 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1247, epoch_train_loss=0.0007309915596211087
Epoch 1248
Epoch 1248 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1248, epoch_train_loss=0.0007309915596211087
Epoch 1249
Epoch 1249 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1249, epoch_train_loss=0.0007309915596211087
Epoch 1250
Epoch 1250 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1250, epoch_train_loss=0.0007309915596211087
Epoch 1251
Epoch 1251 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1251, epoch_train_loss=0.0007309915596211087
Epoch 1252
Epoch 1252 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1252, epoch_train_loss=0.0007309915596211087
Epoch 1253
Epoch 1253 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1253, epoch_train_loss=0.0007309915596211087
Epoch 1254
Epoch 1254 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1254, epoch_train_loss=0.0007309915596211087
Epoch 1255
Epoch 1255 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1255, epoch_train_loss=0.0007309915596211087
Epoch 1256
Epoch 1256 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1256, epoch_train_loss=0.0007309915596211087
Epoch 1257
Epoch 1257 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1257, epoch_train_loss=0.0007309915596211087
Epoch 1258
Epoch 1258 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1258, epoch_train_loss=0.0007309915596211087
Epoch 1259
Epoch 1259 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1259, epoch_train_loss=0.0007309915596211087
Epoch 1260
Epoch 1260 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1260, epoch_train_loss=0.0007309915596211087
Epoch 1261
Epoch 1261 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1261, epoch_train_loss=0.0007309915596211087
Epoch 1262
Epoch 1262 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1262, epoch_train_loss=0.0007309915596211087
Epoch 1263
Epoch 1263 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1263, epoch_train_loss=0.0007309915596211087
Epoch 1264
Epoch 1264 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1264, epoch_train_loss=0.0007309915596211087
Epoch 1265
Epoch 1265 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1265, epoch_train_loss=0.0007309915596211087
Epoch 1266
Epoch 1266 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1266, epoch_train_loss=0.0007309915596211087
Epoch 1267
Epoch 1267 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1267, epoch_train_loss=0.0007309915596211087
Epoch 1268
Epoch 1268 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1268, epoch_train_loss=0.0007309915596211087
Epoch 1269
Epoch 1269 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1269, epoch_train_loss=0.0007309915596211087
Epoch 1270
Epoch 1270 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1270, epoch_train_loss=0.0007309915596211087
Epoch 1271
Epoch 1271 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1271, epoch_train_loss=0.0007309915596211087
Epoch 1272
Epoch 1272 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1272, epoch_train_loss=0.0007309915596211087
Epoch 1273
Epoch 1273 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1273, epoch_train_loss=0.0007309915596211087
Epoch 1274
Epoch 1274 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1274, epoch_train_loss=0.0007309915596211087
Epoch 1275
Epoch 1275 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1275, epoch_train_loss=0.0007309915596211087
Epoch 1276
Epoch 1276 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1276, epoch_train_loss=0.0007309915596211087
Epoch 1277
Epoch 1277 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1277, epoch_train_loss=0.0007309915596211087
Epoch 1278
Epoch 1278 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1278, epoch_train_loss=0.0007309915596211087
Epoch 1279
Epoch 1279 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1279, epoch_train_loss=0.0007309915596211087
Epoch 1280
Epoch 1280 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1280, epoch_train_loss=0.0007309915596211087
Epoch 1281
Epoch 1281 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1281, epoch_train_loss=0.0007309915596211087
Epoch 1282
Epoch 1282 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1282, epoch_train_loss=0.0007309915596211087
Epoch 1283
Epoch 1283 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1283, epoch_train_loss=0.0007309915596211087
Epoch 1284
Epoch 1284 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1284, epoch_train_loss=0.0007309915596211087
Epoch 1285
Epoch 1285 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1285, epoch_train_loss=0.0007309915596211087
Epoch 1286
Epoch 1286 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1286, epoch_train_loss=0.0007309915596211087
Epoch 1287
Epoch 1287 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1287, epoch_train_loss=0.0007309915596211087
Epoch 1288
Epoch 1288 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1288, epoch_train_loss=0.0007309915596211087
Epoch 1289
Epoch 1289 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1289, epoch_train_loss=0.0007309915596211087
Epoch 1290
Epoch 1290 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1290, epoch_train_loss=0.0007309915596211087
Epoch 1291
Epoch 1291 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1291, epoch_train_loss=0.0007309915596211087
Epoch 1292
Epoch 1292 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1292, epoch_train_loss=0.0007309915596211087
Epoch 1293
Epoch 1293 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1293, epoch_train_loss=0.0007309915596211087
Epoch 1294
Epoch 1294 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1294, epoch_train_loss=0.0007309915596211087
Epoch 1295
Epoch 1295 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1295, epoch_train_loss=0.0007309915596211087
Epoch 1296
Epoch 1296 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1296, epoch_train_loss=0.0007309915596211087
Epoch 1297
Epoch 1297 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1297, epoch_train_loss=0.0007309915596211087
Epoch 1298
Epoch 1298 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1298, epoch_train_loss=0.0007309915596211087
Epoch 1299
Epoch 1299 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1299, epoch_train_loss=0.0007309915596211087
Epoch 1300
Epoch 1300 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1300, epoch_train_loss=0.0007309915596211087
Epoch 1301
Epoch 1301 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1301, epoch_train_loss=0.0007309915596211087
Epoch 1302
Epoch 1302 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1302, epoch_train_loss=0.0007309915596211087
Epoch 1303
Epoch 1303 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1303, epoch_train_loss=0.0007309915596211087
Epoch 1304
Epoch 1304 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1304, epoch_train_loss=0.0007309915596211087
Epoch 1305
Epoch 1305 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1305, epoch_train_loss=0.0007309915596211087
Epoch 1306
Epoch 1306 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1306, epoch_train_loss=0.0007309915596211087
Epoch 1307
Epoch 1307 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1307, epoch_train_loss=0.0007309915596211087
Epoch 1308
Epoch 1308 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1308, epoch_train_loss=0.0007309915596211087
Epoch 1309
Epoch 1309 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1309, epoch_train_loss=0.0007309915596211087
Epoch 1310
Epoch 1310 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1310, epoch_train_loss=0.0007309915596211087
Epoch 1311
Epoch 1311 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1311, epoch_train_loss=0.0007309915596211087
Epoch 1312
Epoch 1312 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1312, epoch_train_loss=0.0007309915596211087
Epoch 1313
Epoch 1313 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1313, epoch_train_loss=0.0007309915596211087
Epoch 1314
Epoch 1314 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1314, epoch_train_loss=0.0007309915596211087
Epoch 1315
Epoch 1315 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1315, epoch_train_loss=0.0007309915596211087
Epoch 1316
Epoch 1316 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1316, epoch_train_loss=0.0007309915596211087
Epoch 1317
Epoch 1317 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1317, epoch_train_loss=0.0007309915596211087
Epoch 1318
Epoch 1318 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1318, epoch_train_loss=0.0007309915596211087
Epoch 1319
Epoch 1319 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1319, epoch_train_loss=0.0007309915596211087
Epoch 1320
Epoch 1320 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1320, epoch_train_loss=0.0007309915596211087
Epoch 1321
Epoch 1321 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1321, epoch_train_loss=0.0007309915596211087
Epoch 1322
Epoch 1322 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1322, epoch_train_loss=0.0007309915596211087
Epoch 1323
Epoch 1323 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1323, epoch_train_loss=0.0007309915596211087
Epoch 1324
Epoch 1324 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1324, epoch_train_loss=0.0007309915596211087
Epoch 1325
Epoch 1325 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1325, epoch_train_loss=0.0007309915596211087
Epoch 1326
Epoch 1326 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1326, epoch_train_loss=0.0007309915596211087
Epoch 1327
Epoch 1327 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1327, epoch_train_loss=0.0007309915596211087
Epoch 1328
Epoch 1328 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1328, epoch_train_loss=0.0007309915596211087
Epoch 1329
Epoch 1329 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1329, epoch_train_loss=0.0007309915596211087
Epoch 1330
Epoch 1330 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1330, epoch_train_loss=0.0007309915596211087
Epoch 1331
Epoch 1331 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1331, epoch_train_loss=0.0007309915596211087
Epoch 1332
Epoch 1332 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1332, epoch_train_loss=0.0007309915596211087
Epoch 1333
Epoch 1333 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1333, epoch_train_loss=0.0007309915596211087
Epoch 1334
Epoch 1334 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1334, epoch_train_loss=0.0007309915596211087
Epoch 1335
Epoch 1335 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1335, epoch_train_loss=0.0007309915596211087
Epoch 1336
Epoch 1336 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1336, epoch_train_loss=0.0007309915596211087
Epoch 1337
Epoch 1337 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1337, epoch_train_loss=0.0007309915596211087
Epoch 1338
Epoch 1338 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1338, epoch_train_loss=0.0007309915596211087
Epoch 1339
Epoch 1339 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1339, epoch_train_loss=0.0007309915596211087
Epoch 1340
Epoch 1340 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1340, epoch_train_loss=0.0007309915596211087
Epoch 1341
Epoch 1341 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1341, epoch_train_loss=0.0007309915596211087
Epoch 1342
Epoch 1342 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1342, epoch_train_loss=0.0007309915596211087
Epoch 1343
Epoch 1343 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1343, epoch_train_loss=0.0007309915596211087
Epoch 1344
Epoch 1344 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1344, epoch_train_loss=0.0007309915596211087
Epoch 1345
Epoch 1345 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1345, epoch_train_loss=0.0007309915596211087
Epoch 1346
Epoch 1346 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1346, epoch_train_loss=0.0007309915596211087
Epoch 1347
Epoch 1347 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1347, epoch_train_loss=0.0007309915596211087
Epoch 1348
Epoch 1348 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1348, epoch_train_loss=0.0007309915596211087
Epoch 1349
Epoch 1349 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1349, epoch_train_loss=0.0007309915596211087
Epoch 1350
Epoch 1350 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1350, epoch_train_loss=0.0007309915596211087
Epoch 1351
Epoch 1351 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1351, epoch_train_loss=0.0007309915596211087
Epoch 1352
Epoch 1352 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1352, epoch_train_loss=0.0007309915596211087
Epoch 1353
Epoch 1353 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1353, epoch_train_loss=0.0007309915596211087
Epoch 1354
Epoch 1354 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1354, epoch_train_loss=0.0007309915596211087
Epoch 1355
Epoch 1355 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1355, epoch_train_loss=0.0007309915596211087
Epoch 1356
Epoch 1356 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1356, epoch_train_loss=0.0007309915596211087
Epoch 1357
Epoch 1357 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1357, epoch_train_loss=0.0007309915596211087
Epoch 1358
Epoch 1358 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1358, epoch_train_loss=0.0007309915596211087
Epoch 1359
Epoch 1359 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1359, epoch_train_loss=0.0007309915596211087
Epoch 1360
Epoch 1360 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1360, epoch_train_loss=0.0007309915596211087
Epoch 1361
Epoch 1361 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1361, epoch_train_loss=0.0007309915596211087
Epoch 1362
Epoch 1362 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1362, epoch_train_loss=0.0007309915596211087
Epoch 1363
Epoch 1363 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1363, epoch_train_loss=0.0007309915596211087
Epoch 1364
Epoch 1364 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1364, epoch_train_loss=0.0007309915596211087
Epoch 1365
Epoch 1365 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1365, epoch_train_loss=0.0007309915596211087
Epoch 1366
Epoch 1366 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1366, epoch_train_loss=0.0007309915596211087
Epoch 1367
Epoch 1367 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1367, epoch_train_loss=0.0007309915596211087
Epoch 1368
Epoch 1368 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1368, epoch_train_loss=0.0007309915596211087
Epoch 1369
Epoch 1369 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1369, epoch_train_loss=0.0007309915596211087
Epoch 1370
Epoch 1370 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1370, epoch_train_loss=0.0007309915596211087
Epoch 1371
Epoch 1371 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1371, epoch_train_loss=0.0007309915596211087
Epoch 1372
Epoch 1372 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1372, epoch_train_loss=0.0007309915596211087
Epoch 1373
Epoch 1373 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1373, epoch_train_loss=0.0007309915596211087
Epoch 1374
Epoch 1374 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1374, epoch_train_loss=0.0007309915596211087
Epoch 1375
Epoch 1375 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1375, epoch_train_loss=0.0007309915596211087
Epoch 1376
Epoch 1376 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1376, epoch_train_loss=0.0007309915596211087
Epoch 1377
Epoch 1377 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1377, epoch_train_loss=0.0007309915596211087
Epoch 1378
Epoch 1378 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1378, epoch_train_loss=0.0007309915596211087
Epoch 1379
Epoch 1379 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1379, epoch_train_loss=0.0007309915596211087
Epoch 1380
Epoch 1380 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1380, epoch_train_loss=0.0007309915596211087
Epoch 1381
Epoch 1381 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1381, epoch_train_loss=0.0007309915596211087
Epoch 1382
Epoch 1382 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1382, epoch_train_loss=0.0007309915596211087
Epoch 1383
Epoch 1383 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1383, epoch_train_loss=0.0007309915596211087
Epoch 1384
Epoch 1384 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1384, epoch_train_loss=0.0007309915596211087
Epoch 1385
Epoch 1385 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1385, epoch_train_loss=0.0007309915596211087
Epoch 1386
Epoch 1386 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1386, epoch_train_loss=0.0007309915596211087
Epoch 1387
Epoch 1387 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1387, epoch_train_loss=0.0007309915596211087
Epoch 1388
Epoch 1388 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1388, epoch_train_loss=0.0007309915596211087
Epoch 1389
Epoch 1389 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1389, epoch_train_loss=0.0007309915596211087
Epoch 1390
Epoch 1390 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1390, epoch_train_loss=0.0007309915596211087
Epoch 1391
Epoch 1391 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1391, epoch_train_loss=0.0007309915596211087
Epoch 1392
Epoch 1392 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1392, epoch_train_loss=0.0007309915596211087
Epoch 1393
Epoch 1393 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1393, epoch_train_loss=0.0007309915596211087
Epoch 1394
Epoch 1394 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1394, epoch_train_loss=0.0007309915596211087
Epoch 1395
Epoch 1395 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1395, epoch_train_loss=0.0007309915596211087
Epoch 1396
Epoch 1396 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1396, epoch_train_loss=0.0007309915596211087
Epoch 1397
Epoch 1397 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1397, epoch_train_loss=0.0007309915596211087
Epoch 1398
Epoch 1398 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1398, epoch_train_loss=0.0007309915596211087
Epoch 1399
Epoch 1399 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1399, epoch_train_loss=0.0007309915596211087
Epoch 1400
Epoch 1400 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1400, epoch_train_loss=0.0007309915596211087
Epoch 1401
Epoch 1401 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1401, epoch_train_loss=0.0007309915596211087
Epoch 1402
Epoch 1402 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1402, epoch_train_loss=0.0007309915596211087
Epoch 1403
Epoch 1403 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1403, epoch_train_loss=0.0007309915596211087
Epoch 1404
Epoch 1404 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1404, epoch_train_loss=0.0007309915596211087
Epoch 1405
Epoch 1405 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1405, epoch_train_loss=0.0007309915596211087
Epoch 1406
Epoch 1406 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1406, epoch_train_loss=0.0007309915596211087
Epoch 1407
Epoch 1407 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1407, epoch_train_loss=0.0007309915596211087
Epoch 1408
Epoch 1408 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1408, epoch_train_loss=0.0007309915596211087
Epoch 1409
Epoch 1409 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1409, epoch_train_loss=0.0007309915596211087
Epoch 1410
Epoch 1410 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1410, epoch_train_loss=0.0007309915596211087
Epoch 1411
Epoch 1411 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1411, epoch_train_loss=0.0007309915596211087
Epoch 1412
Epoch 1412 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1412, epoch_train_loss=0.0007309915596211087
Epoch 1413
Epoch 1413 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1413, epoch_train_loss=0.0007309915596211087
Epoch 1414
Epoch 1414 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1414, epoch_train_loss=0.0007309915596211087
Epoch 1415
Epoch 1415 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1415, epoch_train_loss=0.0007309915596211087
Epoch 1416
Epoch 1416 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1416, epoch_train_loss=0.0007309915596211087
Epoch 1417
Epoch 1417 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1417, epoch_train_loss=0.0007309915596211087
Epoch 1418
Epoch 1418 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1418, epoch_train_loss=0.0007309915596211087
Epoch 1419
Epoch 1419 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1419, epoch_train_loss=0.0007309915596211087
Epoch 1420
Epoch 1420 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1420, epoch_train_loss=0.0007309915596211087
Epoch 1421
Epoch 1421 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1421, epoch_train_loss=0.0007309915596211087
Epoch 1422
Epoch 1422 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1422, epoch_train_loss=0.0007309915596211087
Epoch 1423
Epoch 1423 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1423, epoch_train_loss=0.0007309915596211087
Epoch 1424
Epoch 1424 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1424, epoch_train_loss=0.0007309915596211087
Epoch 1425
Epoch 1425 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1425, epoch_train_loss=0.0007309915596211087
Epoch 1426
Epoch 1426 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1426, epoch_train_loss=0.0007309915596211087
Epoch 1427
Epoch 1427 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1427, epoch_train_loss=0.0007309915596211087
Epoch 1428
Epoch 1428 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1428, epoch_train_loss=0.0007309915596211087
Epoch 1429
Epoch 1429 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1429, epoch_train_loss=0.0007309915596211087
Epoch 1430
Epoch 1430 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1430, epoch_train_loss=0.0007309915596211087
Epoch 1431
Epoch 1431 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1431, epoch_train_loss=0.0007309915596211087
Epoch 1432
Epoch 1432 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1432, epoch_train_loss=0.0007309915596211087
Epoch 1433
Epoch 1433 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1433, epoch_train_loss=0.0007309915596211087
Epoch 1434
Epoch 1434 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1434, epoch_train_loss=0.0007309915596211087
Epoch 1435
Epoch 1435 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1435, epoch_train_loss=0.0007309915596211087
Epoch 1436
Epoch 1436 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1436, epoch_train_loss=0.0007309915596211087
Epoch 1437
Epoch 1437 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1437, epoch_train_loss=0.0007309915596211087
Epoch 1438
Epoch 1438 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1438, epoch_train_loss=0.0007309915596211087
Epoch 1439
Epoch 1439 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1439, epoch_train_loss=0.0007309915596211087
Epoch 1440
Epoch 1440 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1440, epoch_train_loss=0.0007309915596211087
Epoch 1441
Epoch 1441 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1441, epoch_train_loss=0.0007309915596211087
Epoch 1442
Epoch 1442 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1442, epoch_train_loss=0.0007309915596211087
Epoch 1443
Epoch 1443 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1443, epoch_train_loss=0.0007309915596211087
Epoch 1444
Epoch 1444 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1444, epoch_train_loss=0.0007309915596211087
Epoch 1445
Epoch 1445 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1445, epoch_train_loss=0.0007309915596211087
Epoch 1446
Epoch 1446 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1446, epoch_train_loss=0.0007309915596211087
Epoch 1447
Epoch 1447 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1447, epoch_train_loss=0.0007309915596211087
Epoch 1448
Epoch 1448 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1448, epoch_train_loss=0.0007309915596211087
Epoch 1449
Epoch 1449 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1449, epoch_train_loss=0.0007309915596211087
Epoch 1450
Epoch 1450 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1450, epoch_train_loss=0.0007309915596211087
Epoch 1451
Epoch 1451 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1451, epoch_train_loss=0.0007309915596211087
Epoch 1452
Epoch 1452 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1452, epoch_train_loss=0.0007309915596211087
Epoch 1453
Epoch 1453 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1453, epoch_train_loss=0.0007309915596211087
Epoch 1454
Epoch 1454 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1454, epoch_train_loss=0.0007309915596211087
Epoch 1455
Epoch 1455 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1455, epoch_train_loss=0.0007309915596211087
Epoch 1456
Epoch 1456 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1456, epoch_train_loss=0.0007309915596211087
Epoch 1457
Epoch 1457 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1457, epoch_train_loss=0.0007309915596211087
Epoch 1458
Epoch 1458 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1458, epoch_train_loss=0.0007309915596211087
Epoch 1459
Epoch 1459 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1459, epoch_train_loss=0.0007309915596211087
Epoch 1460
Epoch 1460 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1460, epoch_train_loss=0.0007309915596211087
Epoch 1461
Epoch 1461 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1461, epoch_train_loss=0.0007309915596211087
Epoch 1462
Epoch 1462 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1462, epoch_train_loss=0.0007309915596211087
Epoch 1463
Epoch 1463 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1463, epoch_train_loss=0.0007309915596211087
Epoch 1464
Epoch 1464 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1464, epoch_train_loss=0.0007309915596211087
Epoch 1465
Epoch 1465 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1465, epoch_train_loss=0.0007309915596211087
Epoch 1466
Epoch 1466 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1466, epoch_train_loss=0.0007309915596211087
Epoch 1467
Epoch 1467 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1467, epoch_train_loss=0.0007309915596211087
Epoch 1468
Epoch 1468 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1468, epoch_train_loss=0.0007309915596211087
Epoch 1469
Epoch 1469 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1469, epoch_train_loss=0.0007309915596211087
Epoch 1470
Epoch 1470 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1470, epoch_train_loss=0.0007309915596211087
Epoch 1471
Epoch 1471 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1471, epoch_train_loss=0.0007309915596211087
Epoch 1472
Epoch 1472 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1472, epoch_train_loss=0.0007309915596211087
Epoch 1473
Epoch 1473 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1473, epoch_train_loss=0.0007309915596211087
Epoch 1474
Epoch 1474 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1474, epoch_train_loss=0.0007309915596211087
Epoch 1475
Epoch 1475 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1475, epoch_train_loss=0.0007309915596211087
Epoch 1476
Epoch 1476 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1476, epoch_train_loss=0.0007309915596211087
Epoch 1477
Epoch 1477 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1477, epoch_train_loss=0.0007309915596211087
Epoch 1478
Epoch 1478 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1478, epoch_train_loss=0.0007309915596211087
Epoch 1479
Epoch 1479 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1479, epoch_train_loss=0.0007309915596211087
Epoch 1480
Epoch 1480 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1480, epoch_train_loss=0.0007309915596211087
Epoch 1481
Epoch 1481 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1481, epoch_train_loss=0.0007309915596211087
Epoch 1482
Epoch 1482 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1482, epoch_train_loss=0.0007309915596211087
Epoch 1483
Epoch 1483 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1483, epoch_train_loss=0.0007309915596211087
Epoch 1484
Epoch 1484 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1484, epoch_train_loss=0.0007309915596211087
Epoch 1485
Epoch 1485 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1485, epoch_train_loss=0.0007309915596211087
Epoch 1486
Epoch 1486 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1486, epoch_train_loss=0.0007309915596211087
Epoch 1487
Epoch 1487 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1487, epoch_train_loss=0.0007309915596211087
Epoch 1488
Epoch 1488 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1488, epoch_train_loss=0.0007309915596211087
Epoch 1489
Epoch 1489 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1489, epoch_train_loss=0.0007309915596211087
Epoch 1490
Epoch 1490 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1490, epoch_train_loss=0.0007309915596211087
Epoch 1491
Epoch 1491 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1491, epoch_train_loss=0.0007309915596211087
Epoch 1492
Epoch 1492 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1492, epoch_train_loss=0.0007309915596211087
Epoch 1493
Epoch 1493 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1493, epoch_train_loss=0.0007309915596211087
Epoch 1494
Epoch 1494 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1494, epoch_train_loss=0.0007309915596211087
Epoch 1495
Epoch 1495 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1495, epoch_train_loss=0.0007309915596211087
Epoch 1496
Epoch 1496 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1496, epoch_train_loss=0.0007309915596211087
Epoch 1497
Epoch 1497 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1497, epoch_train_loss=0.0007309915596211087
Epoch 1498
Epoch 1498 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1498, epoch_train_loss=0.0007309915596211087
Epoch 1499
Epoch 1499 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1499, epoch_train_loss=0.0007309915596211087
Epoch 1500
Epoch 1500 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1500, epoch_train_loss=0.0007309915596211087
Epoch 1501
Epoch 1501 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1501, epoch_train_loss=0.0007309915596211087
Epoch 1502
Epoch 1502 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1502, epoch_train_loss=0.0007309915596211087
Epoch 1503
Epoch 1503 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1503, epoch_train_loss=0.0007309915596211087
Epoch 1504
Epoch 1504 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1504, epoch_train_loss=0.0007309915596211087
Epoch 1505
Epoch 1505 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1505, epoch_train_loss=0.0007309915596211087
Epoch 1506
Epoch 1506 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1506, epoch_train_loss=0.0007309915596211087
Epoch 1507
Epoch 1507 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1507, epoch_train_loss=0.0007309915596211087
Epoch 1508
Epoch 1508 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1508, epoch_train_loss=0.0007309915596211087
Epoch 1509
Epoch 1509 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1509, epoch_train_loss=0.0007309915596211087
Epoch 1510
Epoch 1510 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1510, epoch_train_loss=0.0007309915596211087
Epoch 1511
Epoch 1511 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1511, epoch_train_loss=0.0007309915596211087
Epoch 1512
Epoch 1512 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1512, epoch_train_loss=0.0007309915596211087
Epoch 1513
Epoch 1513 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1513, epoch_train_loss=0.0007309915596211087
Epoch 1514
Epoch 1514 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1514, epoch_train_loss=0.0007309915596211087
Epoch 1515
Epoch 1515 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1515, epoch_train_loss=0.0007309915596211087
Epoch 1516
Epoch 1516 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1516, epoch_train_loss=0.0007309915596211087
Epoch 1517
Epoch 1517 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1517, epoch_train_loss=0.0007309915596211087
Epoch 1518
Epoch 1518 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1518, epoch_train_loss=0.0007309915596211087
Epoch 1519
Epoch 1519 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1519, epoch_train_loss=0.0007309915596211087
Epoch 1520
Epoch 1520 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1520, epoch_train_loss=0.0007309915596211087
Epoch 1521
Epoch 1521 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1521, epoch_train_loss=0.0007309915596211087
Epoch 1522
Epoch 1522 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1522, epoch_train_loss=0.0007309915596211087
Epoch 1523
Epoch 1523 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1523, epoch_train_loss=0.0007309915596211087
Epoch 1524
Epoch 1524 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1524, epoch_train_loss=0.0007309915596211087
Epoch 1525
Epoch 1525 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1525, epoch_train_loss=0.0007309915596211087
Epoch 1526
Epoch 1526 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1526, epoch_train_loss=0.0007309915596211087
Epoch 1527
Epoch 1527 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1527, epoch_train_loss=0.0007309915596211087
Epoch 1528
Epoch 1528 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1528, epoch_train_loss=0.0007309915596211087
Epoch 1529
Epoch 1529 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1529, epoch_train_loss=0.0007309915596211087
Epoch 1530
Epoch 1530 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1530, epoch_train_loss=0.0007309915596211087
Epoch 1531
Epoch 1531 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1531, epoch_train_loss=0.0007309915596211087
Epoch 1532
Epoch 1532 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1532, epoch_train_loss=0.0007309915596211087
Epoch 1533
Epoch 1533 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1533, epoch_train_loss=0.0007309915596211087
Epoch 1534
Epoch 1534 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1534, epoch_train_loss=0.0007309915596211087
Epoch 1535
Epoch 1535 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1535, epoch_train_loss=0.0007309915596211087
Epoch 1536
Epoch 1536 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1536, epoch_train_loss=0.0007309915596211087
Epoch 1537
Epoch 1537 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1537, epoch_train_loss=0.0007309915596211087
Epoch 1538
Epoch 1538 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1538, epoch_train_loss=0.0007309915596211087
Epoch 1539
Epoch 1539 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1539, epoch_train_loss=0.0007309915596211087
Epoch 1540
Epoch 1540 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1540, epoch_train_loss=0.0007309915596211087
Epoch 1541
Epoch 1541 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1541, epoch_train_loss=0.0007309915596211087
Epoch 1542
Epoch 1542 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1542, epoch_train_loss=0.0007309915596211087
Epoch 1543
Epoch 1543 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1543, epoch_train_loss=0.0007309915596211087
Epoch 1544
Epoch 1544 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1544, epoch_train_loss=0.0007309915596211087
Epoch 1545
Epoch 1545 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1545, epoch_train_loss=0.0007309915596211087
Epoch 1546
Epoch 1546 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1546, epoch_train_loss=0.0007309915596211087
Epoch 1547
Epoch 1547 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1547, epoch_train_loss=0.0007309915596211087
Epoch 1548
Epoch 1548 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1548, epoch_train_loss=0.0007309915596211087
Epoch 1549
Epoch 1549 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1549, epoch_train_loss=0.0007309915596211087
Epoch 1550
Epoch 1550 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1550, epoch_train_loss=0.0007309915596211087
Epoch 1551
Epoch 1551 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1551, epoch_train_loss=0.0007309915596211087
Epoch 1552
Epoch 1552 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1552, epoch_train_loss=0.0007309915596211087
Epoch 1553
Epoch 1553 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1553, epoch_train_loss=0.0007309915596211087
Epoch 1554
Epoch 1554 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1554, epoch_train_loss=0.0007309915596211087
Epoch 1555
Epoch 1555 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1555, epoch_train_loss=0.0007309915596211087
Epoch 1556
Epoch 1556 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1556, epoch_train_loss=0.0007309915596211087
Epoch 1557
Epoch 1557 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1557, epoch_train_loss=0.0007309915596211087
Epoch 1558
Epoch 1558 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1558, epoch_train_loss=0.0007309915596211087
Epoch 1559
Epoch 1559 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1559, epoch_train_loss=0.0007309915596211087
Epoch 1560
Epoch 1560 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1560, epoch_train_loss=0.0007309915596211087
Epoch 1561
Epoch 1561 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1561, epoch_train_loss=0.0007309915596211087
Epoch 1562
Epoch 1562 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1562, epoch_train_loss=0.0007309915596211087
Epoch 1563
Epoch 1563 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1563, epoch_train_loss=0.0007309915596211087
Epoch 1564
Epoch 1564 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1564, epoch_train_loss=0.0007309915596211087
Epoch 1565
Epoch 1565 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1565, epoch_train_loss=0.0007309915596211087
Epoch 1566
Epoch 1566 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1566, epoch_train_loss=0.0007309915596211087
Epoch 1567
Epoch 1567 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1567, epoch_train_loss=0.0007309915596211087
Epoch 1568
Epoch 1568 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1568, epoch_train_loss=0.0007309915596211087
Epoch 1569
Epoch 1569 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1569, epoch_train_loss=0.0007309915596211087
Epoch 1570
Epoch 1570 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1570, epoch_train_loss=0.0007309915596211087
Epoch 1571
Epoch 1571 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1571, epoch_train_loss=0.0007309915596211087
Epoch 1572
Epoch 1572 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1572, epoch_train_loss=0.0007309915596211087
Epoch 1573
Epoch 1573 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1573, epoch_train_loss=0.0007309915596211087
Epoch 1574
Epoch 1574 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1574, epoch_train_loss=0.0007309915596211087
Epoch 1575
Epoch 1575 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1575, epoch_train_loss=0.0007309915596211087
Epoch 1576
Epoch 1576 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1576, epoch_train_loss=0.0007309915596211087
Epoch 1577
Epoch 1577 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1577, epoch_train_loss=0.0007309915596211087
Epoch 1578
Epoch 1578 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1578, epoch_train_loss=0.0007309915596211087
Epoch 1579
Epoch 1579 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1579, epoch_train_loss=0.0007309915596211087
Epoch 1580
Epoch 1580 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1580, epoch_train_loss=0.0007309915596211087
Epoch 1581
Epoch 1581 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1581, epoch_train_loss=0.0007309915596211087
Epoch 1582
Epoch 1582 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1582, epoch_train_loss=0.0007309915596211087
Epoch 1583
Epoch 1583 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1583, epoch_train_loss=0.0007309915596211087
Epoch 1584
Epoch 1584 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1584, epoch_train_loss=0.0007309915596211087
Epoch 1585
Epoch 1585 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1585, epoch_train_loss=0.0007309915596211087
Epoch 1586
Epoch 1586 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1586, epoch_train_loss=0.0007309915596211087
Epoch 1587
Epoch 1587 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1587, epoch_train_loss=0.0007309915596211087
Epoch 1588
Epoch 1588 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1588, epoch_train_loss=0.0007309915596211087
Epoch 1589
Epoch 1589 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1589, epoch_train_loss=0.0007309915596211087
Epoch 1590
Epoch 1590 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1590, epoch_train_loss=0.0007309915596211087
Epoch 1591
Epoch 1591 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1591, epoch_train_loss=0.0007309915596211087
Epoch 1592
Epoch 1592 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1592, epoch_train_loss=0.0007309915596211087
Epoch 1593
Epoch 1593 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1593, epoch_train_loss=0.0007309915596211087
Epoch 1594
Epoch 1594 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1594, epoch_train_loss=0.0007309915596211087
Epoch 1595
Epoch 1595 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1595, epoch_train_loss=0.0007309915596211087
Epoch 1596
Epoch 1596 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1596, epoch_train_loss=0.0007309915596211087
Epoch 1597
Epoch 1597 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1597, epoch_train_loss=0.0007309915596211087
Epoch 1598
Epoch 1598 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1598, epoch_train_loss=0.0007309915596211087
Epoch 1599
Epoch 1599 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1599, epoch_train_loss=0.0007309915596211087
Epoch 1600
Epoch 1600 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1600, epoch_train_loss=0.0007309915596211087
Epoch 1601
Epoch 1601 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1601, epoch_train_loss=0.0007309915596211087
Epoch 1602
Epoch 1602 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1602, epoch_train_loss=0.0007309915596211087
Epoch 1603
Epoch 1603 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1603, epoch_train_loss=0.0007309915596211087
Epoch 1604
Epoch 1604 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1604, epoch_train_loss=0.0007309915596211087
Epoch 1605
Epoch 1605 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1605, epoch_train_loss=0.0007309915596211087
Epoch 1606
Epoch 1606 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1606, epoch_train_loss=0.0007309915596211087
Epoch 1607
Epoch 1607 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1607, epoch_train_loss=0.0007309915596211087
Epoch 1608
Epoch 1608 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1608, epoch_train_loss=0.0007309915596211087
Epoch 1609
Epoch 1609 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1609, epoch_train_loss=0.0007309915596211087
Epoch 1610
Epoch 1610 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1610, epoch_train_loss=0.0007309915596211087
Epoch 1611
Epoch 1611 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1611, epoch_train_loss=0.0007309915596211087
Epoch 1612
Epoch 1612 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1612, epoch_train_loss=0.0007309915596211087
Epoch 1613
Epoch 1613 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1613, epoch_train_loss=0.0007309915596211087
Epoch 1614
Epoch 1614 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1614, epoch_train_loss=0.0007309915596211087
Epoch 1615
Epoch 1615 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1615, epoch_train_loss=0.0007309915596211087
Epoch 1616
Epoch 1616 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1616, epoch_train_loss=0.0007309915596211087
Epoch 1617
Epoch 1617 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1617, epoch_train_loss=0.0007309915596211087
Epoch 1618
Epoch 1618 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1618, epoch_train_loss=0.0007309915596211087
Epoch 1619
Epoch 1619 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1619, epoch_train_loss=0.0007309915596211087
Epoch 1620
Epoch 1620 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1620, epoch_train_loss=0.0007309915596211087
Epoch 1621
Epoch 1621 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1621, epoch_train_loss=0.0007309915596211087
Epoch 1622
Epoch 1622 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1622, epoch_train_loss=0.0007309915596211087
Epoch 1623
Epoch 1623 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1623, epoch_train_loss=0.0007309915596211087
Epoch 1624
Epoch 1624 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1624, epoch_train_loss=0.0007309915596211087
Epoch 1625
Epoch 1625 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1625, epoch_train_loss=0.0007309915596211087
Epoch 1626
Epoch 1626 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1626, epoch_train_loss=0.0007309915596211087
Epoch 1627
Epoch 1627 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1627, epoch_train_loss=0.0007309915596211087
Epoch 1628
Epoch 1628 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1628, epoch_train_loss=0.0007309915596211087
Epoch 1629
Epoch 1629 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1629, epoch_train_loss=0.0007309915596211087
Epoch 1630
Epoch 1630 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1630, epoch_train_loss=0.0007309915596211087
Epoch 1631
Epoch 1631 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1631, epoch_train_loss=0.0007309915596211087
Epoch 1632
Epoch 1632 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1632, epoch_train_loss=0.0007309915596211087
Epoch 1633
Epoch 1633 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1633, epoch_train_loss=0.0007309915596211087
Epoch 1634
Epoch 1634 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1634, epoch_train_loss=0.0007309915596211087
Epoch 1635
Epoch 1635 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1635, epoch_train_loss=0.0007309915596211087
Epoch 1636
Epoch 1636 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1636, epoch_train_loss=0.0007309915596211087
Epoch 1637
Epoch 1637 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1637, epoch_train_loss=0.0007309915596211087
Epoch 1638
Epoch 1638 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1638, epoch_train_loss=0.0007309915596211087
Epoch 1639
Epoch 1639 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1639, epoch_train_loss=0.0007309915596211087
Epoch 1640
Epoch 1640 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1640, epoch_train_loss=0.0007309915596211087
Epoch 1641
Epoch 1641 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1641, epoch_train_loss=0.0007309915596211087
Epoch 1642
Epoch 1642 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1642, epoch_train_loss=0.0007309915596211087
Epoch 1643
Epoch 1643 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1643, epoch_train_loss=0.0007309915596211087
Epoch 1644
Epoch 1644 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1644, epoch_train_loss=0.0007309915596211087
Epoch 1645
Epoch 1645 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1645, epoch_train_loss=0.0007309915596211087
Epoch 1646
Epoch 1646 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1646, epoch_train_loss=0.0007309915596211087
Epoch 1647
Epoch 1647 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1647, epoch_train_loss=0.0007309915596211087
Epoch 1648
Epoch 1648 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1648, epoch_train_loss=0.0007309915596211087
Epoch 1649
Epoch 1649 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1649, epoch_train_loss=0.0007309915596211087
Epoch 1650
Epoch 1650 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1650, epoch_train_loss=0.0007309915596211087
Epoch 1651
Epoch 1651 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1651, epoch_train_loss=0.0007309915596211087
Epoch 1652
Epoch 1652 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1652, epoch_train_loss=0.0007309915596211087
Epoch 1653
Epoch 1653 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1653, epoch_train_loss=0.0007309915596211087
Epoch 1654
Epoch 1654 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1654, epoch_train_loss=0.0007309915596211087
Epoch 1655
Epoch 1655 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1655, epoch_train_loss=0.0007309915596211087
Epoch 1656
Epoch 1656 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1656, epoch_train_loss=0.0007309915596211087
Epoch 1657
Epoch 1657 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1657, epoch_train_loss=0.0007309915596211087
Epoch 1658
Epoch 1658 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1658, epoch_train_loss=0.0007309915596211087
Epoch 1659
Epoch 1659 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1659, epoch_train_loss=0.0007309915596211087
Epoch 1660
Epoch 1660 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1660, epoch_train_loss=0.0007309915596211087
Epoch 1661
Epoch 1661 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1661, epoch_train_loss=0.0007309915596211087
Epoch 1662
Epoch 1662 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1662, epoch_train_loss=0.0007309915596211087
Epoch 1663
Epoch 1663 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1663, epoch_train_loss=0.0007309915596211087
Epoch 1664
Epoch 1664 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1664, epoch_train_loss=0.0007309915596211087
Epoch 1665
Epoch 1665 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1665, epoch_train_loss=0.0007309915596211087
Epoch 1666
Epoch 1666 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1666, epoch_train_loss=0.0007309915596211087
Epoch 1667
Epoch 1667 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1667, epoch_train_loss=0.0007309915596211087
Epoch 1668
Epoch 1668 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1668, epoch_train_loss=0.0007309915596211087
Epoch 1669
Epoch 1669 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1669, epoch_train_loss=0.0007309915596211087
Epoch 1670
Epoch 1670 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1670, epoch_train_loss=0.0007309915596211087
Epoch 1671
Epoch 1671 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1671, epoch_train_loss=0.0007309915596211087
Epoch 1672
Epoch 1672 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1672, epoch_train_loss=0.0007309915596211087
Epoch 1673
Epoch 1673 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1673, epoch_train_loss=0.0007309915596211087
Epoch 1674
Epoch 1674 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1674, epoch_train_loss=0.0007309915596211087
Epoch 1675
Epoch 1675 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1675, epoch_train_loss=0.0007309915596211087
Epoch 1676
Epoch 1676 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1676, epoch_train_loss=0.0007309915596211087
Epoch 1677
Epoch 1677 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1677, epoch_train_loss=0.0007309915596211087
Epoch 1678
Epoch 1678 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1678, epoch_train_loss=0.0007309915596211087
Epoch 1679
Epoch 1679 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1679, epoch_train_loss=0.0007309915596211087
Epoch 1680
Epoch 1680 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1680, epoch_train_loss=0.0007309915596211087
Epoch 1681
Epoch 1681 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1681, epoch_train_loss=0.0007309915596211087
Epoch 1682
Epoch 1682 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1682, epoch_train_loss=0.0007309915596211087
Epoch 1683
Epoch 1683 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1683, epoch_train_loss=0.0007309915596211087
Epoch 1684
Epoch 1684 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1684, epoch_train_loss=0.0007309915596211087
Epoch 1685
Epoch 1685 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1685, epoch_train_loss=0.0007309915596211087
Epoch 1686
Epoch 1686 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1686, epoch_train_loss=0.0007309915596211087
Epoch 1687
Epoch 1687 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1687, epoch_train_loss=0.0007309915596211087
Epoch 1688
Epoch 1688 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1688, epoch_train_loss=0.0007309915596211087
Epoch 1689
Epoch 1689 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1689, epoch_train_loss=0.0007309915596211087
Epoch 1690
Epoch 1690 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1690, epoch_train_loss=0.0007309915596211087
Epoch 1691
Epoch 1691 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1691, epoch_train_loss=0.0007309915596211087
Epoch 1692
Epoch 1692 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1692, epoch_train_loss=0.0007309915596211087
Epoch 1693
Epoch 1693 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1693, epoch_train_loss=0.0007309915596211087
Epoch 1694
Epoch 1694 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1694, epoch_train_loss=0.0007309915596211087
Epoch 1695
Epoch 1695 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1695, epoch_train_loss=0.0007309915596211087
Epoch 1696
Epoch 1696 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1696, epoch_train_loss=0.0007309915596211087
Epoch 1697
Epoch 1697 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1697, epoch_train_loss=0.0007309915596211087
Epoch 1698
Epoch 1698 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1698, epoch_train_loss=0.0007309915596211087
Epoch 1699
Epoch 1699 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1699, epoch_train_loss=0.0007309915596211087
Epoch 1700
Epoch 1700 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1700, epoch_train_loss=0.0007309915596211087
Epoch 1701
Epoch 1701 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1701, epoch_train_loss=0.0007309915596211087
Epoch 1702
Epoch 1702 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1702, epoch_train_loss=0.0007309915596211087
Epoch 1703
Epoch 1703 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1703, epoch_train_loss=0.0007309915596211087
Epoch 1704
Epoch 1704 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1704, epoch_train_loss=0.0007309915596211087
Epoch 1705
Epoch 1705 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1705, epoch_train_loss=0.0007309915596211087
Epoch 1706
Epoch 1706 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1706, epoch_train_loss=0.0007309915596211087
Epoch 1707
Epoch 1707 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1707, epoch_train_loss=0.0007309915596211087
Epoch 1708
Epoch 1708 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1708, epoch_train_loss=0.0007309915596211087
Epoch 1709
Epoch 1709 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1709, epoch_train_loss=0.0007309915596211087
Epoch 1710
Epoch 1710 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1710, epoch_train_loss=0.0007309915596211087
Epoch 1711
Epoch 1711 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1711, epoch_train_loss=0.0007309915596211087
Epoch 1712
Epoch 1712 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1712, epoch_train_loss=0.0007309915596211087
Epoch 1713
Epoch 1713 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1713, epoch_train_loss=0.0007309915596211087
Epoch 1714
Epoch 1714 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1714, epoch_train_loss=0.0007309915596211087
Epoch 1715
Epoch 1715 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1715, epoch_train_loss=0.0007309915596211087
Epoch 1716
Epoch 1716 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1716, epoch_train_loss=0.0007309915596211087
Epoch 1717
Epoch 1717 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1717, epoch_train_loss=0.0007309915596211087
Epoch 1718
Epoch 1718 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1718, epoch_train_loss=0.0007309915596211087
Epoch 1719
Epoch 1719 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1719, epoch_train_loss=0.0007309915596211087
Epoch 1720
Epoch 1720 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1720, epoch_train_loss=0.0007309915596211087
Epoch 1721
Epoch 1721 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1721, epoch_train_loss=0.0007309915596211087
Epoch 1722
Epoch 1722 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1722, epoch_train_loss=0.0007309915596211087
Epoch 1723
Epoch 1723 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1723, epoch_train_loss=0.0007309915596211087
Epoch 1724
Epoch 1724 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1724, epoch_train_loss=0.0007309915596211087
Epoch 1725
Epoch 1725 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1725, epoch_train_loss=0.0007309915596211087
Epoch 1726
Epoch 1726 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1726, epoch_train_loss=0.0007309915596211087
Epoch 1727
Epoch 1727 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1727, epoch_train_loss=0.0007309915596211087
Epoch 1728
Epoch 1728 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1728, epoch_train_loss=0.0007309915596211087
Epoch 1729
Epoch 1729 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1729, epoch_train_loss=0.0007309915596211087
Epoch 1730
Epoch 1730 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1730, epoch_train_loss=0.0007309915596211087
Epoch 1731
Epoch 1731 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1731, epoch_train_loss=0.0007309915596211087
Epoch 1732
Epoch 1732 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1732, epoch_train_loss=0.0007309915596211087
Epoch 1733
Epoch 1733 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1733, epoch_train_loss=0.0007309915596211087
Epoch 1734
Epoch 1734 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1734, epoch_train_loss=0.0007309915596211087
Epoch 1735
Epoch 1735 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1735, epoch_train_loss=0.0007309915596211087
Epoch 1736
Epoch 1736 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1736, epoch_train_loss=0.0007309915596211087
Epoch 1737
Epoch 1737 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1737, epoch_train_loss=0.0007309915596211087
Epoch 1738
Epoch 1738 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1738, epoch_train_loss=0.0007309915596211087
Epoch 1739
Epoch 1739 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1739, epoch_train_loss=0.0007309915596211087
Epoch 1740
Epoch 1740 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1740, epoch_train_loss=0.0007309915596211087
Epoch 1741
Epoch 1741 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1741, epoch_train_loss=0.0007309915596211087
Epoch 1742
Epoch 1742 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1742, epoch_train_loss=0.0007309915596211087
Epoch 1743
Epoch 1743 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1743, epoch_train_loss=0.0007309915596211087
Epoch 1744
Epoch 1744 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1744, epoch_train_loss=0.0007309915596211087
Epoch 1745
Epoch 1745 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1745, epoch_train_loss=0.0007309915596211087
Epoch 1746
Epoch 1746 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1746, epoch_train_loss=0.0007309915596211087
Epoch 1747
Epoch 1747 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1747, epoch_train_loss=0.0007309915596211087
Epoch 1748
Epoch 1748 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1748, epoch_train_loss=0.0007309915596211087
Epoch 1749
Epoch 1749 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1749, epoch_train_loss=0.0007309915596211087
Epoch 1750
Epoch 1750 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1750, epoch_train_loss=0.0007309915596211087
Epoch 1751
Epoch 1751 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1751, epoch_train_loss=0.0007309915596211087
Epoch 1752
Epoch 1752 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1752, epoch_train_loss=0.0007309915596211087
Epoch 1753
Epoch 1753 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1753, epoch_train_loss=0.0007309915596211087
Epoch 1754
Epoch 1754 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1754, epoch_train_loss=0.0007309915596211087
Epoch 1755
Epoch 1755 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1755, epoch_train_loss=0.0007309915596211087
Epoch 1756
Epoch 1756 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1756, epoch_train_loss=0.0007309915596211087
Epoch 1757
Epoch 1757 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1757, epoch_train_loss=0.0007309915596211087
Epoch 1758
Epoch 1758 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1758, epoch_train_loss=0.0007309915596211087
Epoch 1759
Epoch 1759 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1759, epoch_train_loss=0.0007309915596211087
Epoch 1760
Epoch 1760 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1760, epoch_train_loss=0.0007309915596211087
Epoch 1761
Epoch 1761 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1761, epoch_train_loss=0.0007309915596211087
Epoch 1762
Epoch 1762 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1762, epoch_train_loss=0.0007309915596211087
Epoch 1763
Epoch 1763 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1763, epoch_train_loss=0.0007309915596211087
Epoch 1764
Epoch 1764 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1764, epoch_train_loss=0.0007309915596211087
Epoch 1765
Epoch 1765 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1765, epoch_train_loss=0.0007309915596211087
Epoch 1766
Epoch 1766 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1766, epoch_train_loss=0.0007309915596211087
Epoch 1767
Epoch 1767 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1767, epoch_train_loss=0.0007309915596211087
Epoch 1768
Epoch 1768 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1768, epoch_train_loss=0.0007309915596211087
Epoch 1769
Epoch 1769 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1769, epoch_train_loss=0.0007309915596211087
Epoch 1770
Epoch 1770 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1770, epoch_train_loss=0.0007309915596211087
Epoch 1771
Epoch 1771 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1771, epoch_train_loss=0.0007309915596211087
Epoch 1772
Epoch 1772 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1772, epoch_train_loss=0.0007309915596211087
Epoch 1773
Epoch 1773 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1773, epoch_train_loss=0.0007309915596211087
Epoch 1774
Epoch 1774 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1774, epoch_train_loss=0.0007309915596211087
Epoch 1775
Epoch 1775 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1775, epoch_train_loss=0.0007309915596211087
Epoch 1776
Epoch 1776 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1776, epoch_train_loss=0.0007309915596211087
Epoch 1777
Epoch 1777 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1777, epoch_train_loss=0.0007309915596211087
Epoch 1778
Epoch 1778 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1778, epoch_train_loss=0.0007309915596211087
Epoch 1779
Epoch 1779 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1779, epoch_train_loss=0.0007309915596211087
Epoch 1780
Epoch 1780 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1780, epoch_train_loss=0.0007309915596211087
Epoch 1781
Epoch 1781 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1781, epoch_train_loss=0.0007309915596211087
Epoch 1782
Epoch 1782 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1782, epoch_train_loss=0.0007309915596211087
Epoch 1783
Epoch 1783 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1783, epoch_train_loss=0.0007309915596211087
Epoch 1784
Epoch 1784 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1784, epoch_train_loss=0.0007309915596211087
Epoch 1785
Epoch 1785 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1785, epoch_train_loss=0.0007309915596211087
Epoch 1786
Epoch 1786 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1786, epoch_train_loss=0.0007309915596211087
Epoch 1787
Epoch 1787 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1787, epoch_train_loss=0.0007309915596211087
Epoch 1788
Epoch 1788 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1788, epoch_train_loss=0.0007309915596211087
Epoch 1789
Epoch 1789 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1789, epoch_train_loss=0.0007309915596211087
Epoch 1790
Epoch 1790 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1790, epoch_train_loss=0.0007309915596211087
Epoch 1791
Epoch 1791 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1791, epoch_train_loss=0.0007309915596211087
Epoch 1792
Epoch 1792 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1792, epoch_train_loss=0.0007309915596211087
Epoch 1793
Epoch 1793 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1793, epoch_train_loss=0.0007309915596211087
Epoch 1794
Epoch 1794 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1794, epoch_train_loss=0.0007309915596211087
Epoch 1795
Epoch 1795 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1795, epoch_train_loss=0.0007309915596211087
Epoch 1796
Epoch 1796 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1796, epoch_train_loss=0.0007309915596211087
Epoch 1797
Epoch 1797 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1797, epoch_train_loss=0.0007309915596211087
Epoch 1798
Epoch 1798 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1798, epoch_train_loss=0.0007309915596211087
Epoch 1799
Epoch 1799 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1799, epoch_train_loss=0.0007309915596211087
Epoch 1800
Epoch 1800 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1800, epoch_train_loss=0.0007309915596211087
Epoch 1801
Epoch 1801 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1801, epoch_train_loss=0.0007309915596211087
Epoch 1802
Epoch 1802 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1802, epoch_train_loss=0.0007309915596211087
Epoch 1803
Epoch 1803 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1803, epoch_train_loss=0.0007309915596211087
Epoch 1804
Epoch 1804 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1804, epoch_train_loss=0.0007309915596211087
Epoch 1805
Epoch 1805 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1805, epoch_train_loss=0.0007309915596211087
Epoch 1806
Epoch 1806 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1806, epoch_train_loss=0.0007309915596211087
Epoch 1807
Epoch 1807 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1807, epoch_train_loss=0.0007309915596211087
Epoch 1808
Epoch 1808 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1808, epoch_train_loss=0.0007309915596211087
Epoch 1809
Epoch 1809 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1809, epoch_train_loss=0.0007309915596211087
Epoch 1810
Epoch 1810 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1810, epoch_train_loss=0.0007309915596211087
Epoch 1811
Epoch 1811 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1811, epoch_train_loss=0.0007309915596211087
Epoch 1812
Epoch 1812 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1812, epoch_train_loss=0.0007309915596211087
Epoch 1813
Epoch 1813 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1813, epoch_train_loss=0.0007309915596211087
Epoch 1814
Epoch 1814 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1814, epoch_train_loss=0.0007309915596211087
Epoch 1815
Epoch 1815 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1815, epoch_train_loss=0.0007309915596211087
Epoch 1816
Epoch 1816 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1816, epoch_train_loss=0.0007309915596211087
Epoch 1817
Epoch 1817 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1817, epoch_train_loss=0.0007309915596211087
Epoch 1818
Epoch 1818 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1818, epoch_train_loss=0.0007309915596211087
Epoch 1819
Epoch 1819 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1819, epoch_train_loss=0.0007309915596211087
Epoch 1820
Epoch 1820 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1820, epoch_train_loss=0.0007309915596211087
Epoch 1821
Epoch 1821 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1821, epoch_train_loss=0.0007309915596211087
Epoch 1822
Epoch 1822 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1822, epoch_train_loss=0.0007309915596211087
Epoch 1823
Epoch 1823 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1823, epoch_train_loss=0.0007309915596211087
Epoch 1824
Epoch 1824 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1824, epoch_train_loss=0.0007309915596211087
Epoch 1825
Epoch 1825 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1825, epoch_train_loss=0.0007309915596211087
Epoch 1826
Epoch 1826 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1826, epoch_train_loss=0.0007309915596211087
Epoch 1827
Epoch 1827 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1827, epoch_train_loss=0.0007309915596211087
Epoch 1828
Epoch 1828 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1828, epoch_train_loss=0.0007309915596211087
Epoch 1829
Epoch 1829 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1829, epoch_train_loss=0.0007309915596211087
Epoch 1830
Epoch 1830 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1830, epoch_train_loss=0.0007309915596211087
Epoch 1831
Epoch 1831 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1831, epoch_train_loss=0.0007309915596211087
Epoch 1832
Epoch 1832 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1832, epoch_train_loss=0.0007309915596211087
Epoch 1833
Epoch 1833 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1833, epoch_train_loss=0.0007309915596211087
Epoch 1834
Epoch 1834 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1834, epoch_train_loss=0.0007309915596211087
Epoch 1835
Epoch 1835 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1835, epoch_train_loss=0.0007309915596211087
Epoch 1836
Epoch 1836 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1836, epoch_train_loss=0.0007309915596211087
Epoch 1837
Epoch 1837 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1837, epoch_train_loss=0.0007309915596211087
Epoch 1838
Epoch 1838 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1838, epoch_train_loss=0.0007309915596211087
Epoch 1839
Epoch 1839 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1839, epoch_train_loss=0.0007309915596211087
Epoch 1840
Epoch 1840 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1840, epoch_train_loss=0.0007309915596211087
Epoch 1841
Epoch 1841 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1841, epoch_train_loss=0.0007309915596211087
Epoch 1842
Epoch 1842 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1842, epoch_train_loss=0.0007309915596211087
Epoch 1843
Epoch 1843 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1843, epoch_train_loss=0.0007309915596211087
Epoch 1844
Epoch 1844 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1844, epoch_train_loss=0.0007309915596211087
Epoch 1845
Epoch 1845 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1845, epoch_train_loss=0.0007309915596211087
Epoch 1846
Epoch 1846 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1846, epoch_train_loss=0.0007309915596211087
Epoch 1847
Epoch 1847 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1847, epoch_train_loss=0.0007309915596211087
Epoch 1848
Epoch 1848 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1848, epoch_train_loss=0.0007309915596211087
Epoch 1849
Epoch 1849 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1849, epoch_train_loss=0.0007309915596211087
Epoch 1850
Epoch 1850 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1850, epoch_train_loss=0.0007309915596211087
Epoch 1851
Epoch 1851 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1851, epoch_train_loss=0.0007309915596211087
Epoch 1852
Epoch 1852 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1852, epoch_train_loss=0.0007309915596211087
Epoch 1853
Epoch 1853 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1853, epoch_train_loss=0.0007309915596211087
Epoch 1854
Epoch 1854 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1854, epoch_train_loss=0.0007309915596211087
Epoch 1855
Epoch 1855 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1855, epoch_train_loss=0.0007309915596211087
Epoch 1856
Epoch 1856 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1856, epoch_train_loss=0.0007309915596211087
Epoch 1857
Epoch 1857 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1857, epoch_train_loss=0.0007309915596211087
Epoch 1858
Epoch 1858 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1858, epoch_train_loss=0.0007309915596211087
Epoch 1859
Epoch 1859 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1859, epoch_train_loss=0.0007309915596211087
Epoch 1860
Epoch 1860 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1860, epoch_train_loss=0.0007309915596211087
Epoch 1861
Epoch 1861 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1861, epoch_train_loss=0.0007309915596211087
Epoch 1862
Epoch 1862 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1862, epoch_train_loss=0.0007309915596211087
Epoch 1863
Epoch 1863 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1863, epoch_train_loss=0.0007309915596211087
Epoch 1864
Epoch 1864 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1864, epoch_train_loss=0.0007309915596211087
Epoch 1865
Epoch 1865 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1865, epoch_train_loss=0.0007309915596211087
Epoch 1866
Epoch 1866 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1866, epoch_train_loss=0.0007309915596211087
Epoch 1867
Epoch 1867 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1867, epoch_train_loss=0.0007309915596211087
Epoch 1868
Epoch 1868 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1868, epoch_train_loss=0.0007309915596211087
Epoch 1869
Epoch 1869 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1869, epoch_train_loss=0.0007309915596211087
Epoch 1870
Epoch 1870 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1870, epoch_train_loss=0.0007309915596211087
Epoch 1871
Epoch 1871 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1871, epoch_train_loss=0.0007309915596211087
Epoch 1872
Epoch 1872 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1872, epoch_train_loss=0.0007309915596211087
Epoch 1873
Epoch 1873 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1873, epoch_train_loss=0.0007309915596211087
Epoch 1874
Epoch 1874 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1874, epoch_train_loss=0.0007309915596211087
Epoch 1875
Epoch 1875 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1875, epoch_train_loss=0.0007309915596211087
Epoch 1876
Epoch 1876 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1876, epoch_train_loss=0.0007309915596211087
Epoch 1877
Epoch 1877 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1877, epoch_train_loss=0.0007309915596211087
Epoch 1878
Epoch 1878 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1878, epoch_train_loss=0.0007309915596211087
Epoch 1879
Epoch 1879 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1879, epoch_train_loss=0.0007309915596211087
Epoch 1880
Epoch 1880 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1880, epoch_train_loss=0.0007309915596211087
Epoch 1881
Epoch 1881 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1881, epoch_train_loss=0.0007309915596211087
Epoch 1882
Epoch 1882 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1882, epoch_train_loss=0.0007309915596211087
Epoch 1883
Epoch 1883 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1883, epoch_train_loss=0.0007309915596211087
Epoch 1884
Epoch 1884 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1884, epoch_train_loss=0.0007309915596211087
Epoch 1885
Epoch 1885 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1885, epoch_train_loss=0.0007309915596211087
Epoch 1886
Epoch 1886 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1886, epoch_train_loss=0.0007309915596211087
Epoch 1887
Epoch 1887 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1887, epoch_train_loss=0.0007309915596211087
Epoch 1888
Epoch 1888 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1888, epoch_train_loss=0.0007309915596211087
Epoch 1889
Epoch 1889 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1889, epoch_train_loss=0.0007309915596211087
Epoch 1890
Epoch 1890 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1890, epoch_train_loss=0.0007309915596211087
Epoch 1891
Epoch 1891 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1891, epoch_train_loss=0.0007309915596211087
Epoch 1892
Epoch 1892 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1892, epoch_train_loss=0.0007309915596211087
Epoch 1893
Epoch 1893 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1893, epoch_train_loss=0.0007309915596211087
Epoch 1894
Epoch 1894 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1894, epoch_train_loss=0.0007309915596211087
Epoch 1895
Epoch 1895 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1895, epoch_train_loss=0.0007309915596211087
Epoch 1896
Epoch 1896 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1896, epoch_train_loss=0.0007309915596211087
Epoch 1897
Epoch 1897 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1897, epoch_train_loss=0.0007309915596211087
Epoch 1898
Epoch 1898 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1898, epoch_train_loss=0.0007309915596211087
Epoch 1899
Epoch 1899 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1899, epoch_train_loss=0.0007309915596211087
Epoch 1900
Epoch 1900 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1900, epoch_train_loss=0.0007309915596211087
Epoch 1901
Epoch 1901 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1901, epoch_train_loss=0.0007309915596211087
Epoch 1902
Epoch 1902 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1902, epoch_train_loss=0.0007309915596211087
Epoch 1903
Epoch 1903 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1903, epoch_train_loss=0.0007309915596211087
Epoch 1904
Epoch 1904 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1904, epoch_train_loss=0.0007309915596211087
Epoch 1905
Epoch 1905 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1905, epoch_train_loss=0.0007309915596211087
Epoch 1906
Epoch 1906 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1906, epoch_train_loss=0.0007309915596211087
Epoch 1907
Epoch 1907 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1907, epoch_train_loss=0.0007309915596211087
Epoch 1908
Epoch 1908 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1908, epoch_train_loss=0.0007309915596211087
Epoch 1909
Epoch 1909 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1909, epoch_train_loss=0.0007309915596211087
Epoch 1910
Epoch 1910 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1910, epoch_train_loss=0.0007309915596211087
Epoch 1911
Epoch 1911 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1911, epoch_train_loss=0.0007309915596211087
Epoch 1912
Epoch 1912 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1912, epoch_train_loss=0.0007309915596211087
Epoch 1913
Epoch 1913 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1913, epoch_train_loss=0.0007309915596211087
Epoch 1914
Epoch 1914 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1914, epoch_train_loss=0.0007309915596211087
Epoch 1915
Epoch 1915 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1915, epoch_train_loss=0.0007309915596211087
Epoch 1916
Epoch 1916 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1916, epoch_train_loss=0.0007309915596211087
Epoch 1917
Epoch 1917 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1917, epoch_train_loss=0.0007309915596211087
Epoch 1918
Epoch 1918 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1918, epoch_train_loss=0.0007309915596211087
Epoch 1919
Epoch 1919 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1919, epoch_train_loss=0.0007309915596211087
Epoch 1920
Epoch 1920 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1920, epoch_train_loss=0.0007309915596211087
Epoch 1921
Epoch 1921 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1921, epoch_train_loss=0.0007309915596211087
Epoch 1922
Epoch 1922 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1922, epoch_train_loss=0.0007309915596211087
Epoch 1923
Epoch 1923 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1923, epoch_train_loss=0.0007309915596211087
Epoch 1924
Epoch 1924 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1924, epoch_train_loss=0.0007309915596211087
Epoch 1925
Epoch 1925 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1925, epoch_train_loss=0.0007309915596211087
Epoch 1926
Epoch 1926 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1926, epoch_train_loss=0.0007309915596211087
Epoch 1927
Epoch 1927 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1927, epoch_train_loss=0.0007309915596211087
Epoch 1928
Epoch 1928 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1928, epoch_train_loss=0.0007309915596211087
Epoch 1929
Epoch 1929 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1929, epoch_train_loss=0.0007309915596211087
Epoch 1930
Epoch 1930 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1930, epoch_train_loss=0.0007309915596211087
Epoch 1931
Epoch 1931 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1931, epoch_train_loss=0.0007309915596211087
Epoch 1932
Epoch 1932 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1932, epoch_train_loss=0.0007309915596211087
Epoch 1933
Epoch 1933 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1933, epoch_train_loss=0.0007309915596211087
Epoch 1934
Epoch 1934 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1934, epoch_train_loss=0.0007309915596211087
Epoch 1935
Epoch 1935 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1935, epoch_train_loss=0.0007309915596211087
Epoch 1936
Epoch 1936 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1936, epoch_train_loss=0.0007309915596211087
Epoch 1937
Epoch 1937 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1937, epoch_train_loss=0.0007309915596211087
Epoch 1938
Epoch 1938 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1938, epoch_train_loss=0.0007309915596211087
Epoch 1939
Epoch 1939 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1939, epoch_train_loss=0.0007309915596211087
Epoch 1940
Epoch 1940 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1940, epoch_train_loss=0.0007309915596211087
Epoch 1941
Epoch 1941 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1941, epoch_train_loss=0.0007309915596211087
Epoch 1942
Epoch 1942 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1942, epoch_train_loss=0.0007309915596211087
Epoch 1943
Epoch 1943 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1943, epoch_train_loss=0.0007309915596211087
Epoch 1944
Epoch 1944 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1944, epoch_train_loss=0.0007309915596211087
Epoch 1945
Epoch 1945 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1945, epoch_train_loss=0.0007309915596211087
Epoch 1946
Epoch 1946 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1946, epoch_train_loss=0.0007309915596211087
Epoch 1947
Epoch 1947 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1947, epoch_train_loss=0.0007309915596211087
Epoch 1948
Epoch 1948 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1948, epoch_train_loss=0.0007309915596211087
Epoch 1949
Epoch 1949 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1949, epoch_train_loss=0.0007309915596211087
Epoch 1950
Epoch 1950 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1950, epoch_train_loss=0.0007309915596211087
Epoch 1951
Epoch 1951 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1951, epoch_train_loss=0.0007309915596211087
Epoch 1952
Epoch 1952 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1952, epoch_train_loss=0.0007309915596211087
Epoch 1953
Epoch 1953 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1953, epoch_train_loss=0.0007309915596211087
Epoch 1954
Epoch 1954 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1954, epoch_train_loss=0.0007309915596211087
Epoch 1955
Epoch 1955 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1955, epoch_train_loss=0.0007309915596211087
Epoch 1956
Epoch 1956 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1956, epoch_train_loss=0.0007309915596211087
Epoch 1957
Epoch 1957 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1957, epoch_train_loss=0.0007309915596211087
Epoch 1958
Epoch 1958 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1958, epoch_train_loss=0.0007309915596211087
Epoch 1959
Epoch 1959 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1959, epoch_train_loss=0.0007309915596211087
Epoch 1960
Epoch 1960 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1960, epoch_train_loss=0.0007309915596211087
Epoch 1961
Epoch 1961 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1961, epoch_train_loss=0.0007309915596211087
Epoch 1962
Epoch 1962 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1962, epoch_train_loss=0.0007309915596211087
Epoch 1963
Epoch 1963 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1963, epoch_train_loss=0.0007309915596211087
Epoch 1964
Epoch 1964 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1964, epoch_train_loss=0.0007309915596211087
Epoch 1965
Epoch 1965 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1965, epoch_train_loss=0.0007309915596211087
Epoch 1966
Epoch 1966 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1966, epoch_train_loss=0.0007309915596211087
Epoch 1967
Epoch 1967 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1967, epoch_train_loss=0.0007309915596211087
Epoch 1968
Epoch 1968 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1968, epoch_train_loss=0.0007309915596211087
Epoch 1969
Epoch 1969 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1969, epoch_train_loss=0.0007309915596211087
Epoch 1970
Epoch 1970 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1970, epoch_train_loss=0.0007309915596211087
Epoch 1971
Epoch 1971 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1971, epoch_train_loss=0.0007309915596211087
Epoch 1972
Epoch 1972 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1972, epoch_train_loss=0.0007309915596211087
Epoch 1973
Epoch 1973 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1973, epoch_train_loss=0.0007309915596211087
Epoch 1974
Epoch 1974 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1974, epoch_train_loss=0.0007309915596211087
Epoch 1975
Epoch 1975 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1975, epoch_train_loss=0.0007309915596211087
Epoch 1976
Epoch 1976 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1976, epoch_train_loss=0.0007309915596211087
Epoch 1977
Epoch 1977 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1977, epoch_train_loss=0.0007309915596211087
Epoch 1978
Epoch 1978 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1978, epoch_train_loss=0.0007309915596211087
Epoch 1979
Epoch 1979 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1979, epoch_train_loss=0.0007309915596211087
Epoch 1980
Epoch 1980 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1980, epoch_train_loss=0.0007309915596211087
Epoch 1981
Epoch 1981 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1981, epoch_train_loss=0.0007309915596211087
Epoch 1982
Epoch 1982 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1982, epoch_train_loss=0.0007309915596211087
Epoch 1983
Epoch 1983 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1983, epoch_train_loss=0.0007309915596211087
Epoch 1984
Epoch 1984 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1984, epoch_train_loss=0.0007309915596211087
Epoch 1985
Epoch 1985 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1985, epoch_train_loss=0.0007309915596211087
Epoch 1986
Epoch 1986 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1986, epoch_train_loss=0.0007309915596211087
Epoch 1987
Epoch 1987 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1987, epoch_train_loss=0.0007309915596211087
Epoch 1988
Epoch 1988 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1988, epoch_train_loss=0.0007309915596211087
Epoch 1989
Epoch 1989 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1989, epoch_train_loss=0.0007309915596211087
Epoch 1990
Epoch 1990 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1990, epoch_train_loss=0.0007309915596211087
Epoch 1991
Epoch 1991 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1991, epoch_train_loss=0.0007309915596211087
Epoch 1992
Epoch 1992 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1992, epoch_train_loss=0.0007309915596211087
Epoch 1993
Epoch 1993 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1993, epoch_train_loss=0.0007309915596211087
Epoch 1994
Epoch 1994 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1994, epoch_train_loss=0.0007309915596211087
Epoch 1995
Epoch 1995 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1995, epoch_train_loss=0.0007309915596211087
Epoch 1996
Epoch 1996 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1996, epoch_train_loss=0.0007309915596211087
Epoch 1997
Epoch 1997 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1997, epoch_train_loss=0.0007309915596211087
Epoch 1998
Epoch 1998 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1998, epoch_train_loss=0.0007309915596211087
Epoch 1999
Epoch 1999 :: Batch 0/1
Batch Loss = 0.0007309915596211087
1999, epoch_train_loss=0.0007309915596211087
Epoch 2000
Epoch 2000 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2000, epoch_train_loss=0.0007309915596211087
Epoch 2001
Epoch 2001 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2001, epoch_train_loss=0.0007309915596211087
Epoch 2002
Epoch 2002 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2002, epoch_train_loss=0.0007309915596211087
Epoch 2003
Epoch 2003 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2003, epoch_train_loss=0.0007309915596211087
Epoch 2004
Epoch 2004 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2004, epoch_train_loss=0.0007309915596211087
Epoch 2005
Epoch 2005 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2005, epoch_train_loss=0.0007309915596211087
Epoch 2006
Epoch 2006 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2006, epoch_train_loss=0.0007309915596211087
Epoch 2007
Epoch 2007 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2007, epoch_train_loss=0.0007309915596211087
Epoch 2008
Epoch 2008 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2008, epoch_train_loss=0.0007309915596211087
Epoch 2009
Epoch 2009 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2009, epoch_train_loss=0.0007309915596211087
Epoch 2010
Epoch 2010 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2010, epoch_train_loss=0.0007309915596211087
Epoch 2011
Epoch 2011 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2011, epoch_train_loss=0.0007309915596211087
Epoch 2012
Epoch 2012 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2012, epoch_train_loss=0.0007309915596211087
Epoch 2013
Epoch 2013 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2013, epoch_train_loss=0.0007309915596211087
Epoch 2014
Epoch 2014 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2014, epoch_train_loss=0.0007309915596211087
Epoch 2015
Epoch 2015 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2015, epoch_train_loss=0.0007309915596211087
Epoch 2016
Epoch 2016 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2016, epoch_train_loss=0.0007309915596211087
Epoch 2017
Epoch 2017 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2017, epoch_train_loss=0.0007309915596211087
Epoch 2018
Epoch 2018 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2018, epoch_train_loss=0.0007309915596211087
Epoch 2019
Epoch 2019 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2019, epoch_train_loss=0.0007309915596211087
Epoch 2020
Epoch 2020 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2020, epoch_train_loss=0.0007309915596211087
Epoch 2021
Epoch 2021 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2021, epoch_train_loss=0.0007309915596211087
Epoch 2022
Epoch 2022 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2022, epoch_train_loss=0.0007309915596211087
Epoch 2023
Epoch 2023 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2023, epoch_train_loss=0.0007309915596211087
Epoch 2024
Epoch 2024 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2024, epoch_train_loss=0.0007309915596211087
Epoch 2025
Epoch 2025 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2025, epoch_train_loss=0.0007309915596211087
Epoch 2026
Epoch 2026 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2026, epoch_train_loss=0.0007309915596211087
Epoch 2027
Epoch 2027 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2027, epoch_train_loss=0.0007309915596211087
Epoch 2028
Epoch 2028 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2028, epoch_train_loss=0.0007309915596211087
Epoch 2029
Epoch 2029 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2029, epoch_train_loss=0.0007309915596211087
Epoch 2030
Epoch 2030 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2030, epoch_train_loss=0.0007309915596211087
Epoch 2031
Epoch 2031 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2031, epoch_train_loss=0.0007309915596211087
Epoch 2032
Epoch 2032 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2032, epoch_train_loss=0.0007309915596211087
Epoch 2033
Epoch 2033 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2033, epoch_train_loss=0.0007309915596211087
Epoch 2034
Epoch 2034 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2034, epoch_train_loss=0.0007309915596211087
Epoch 2035
Epoch 2035 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2035, epoch_train_loss=0.0007309915596211087
Epoch 2036
Epoch 2036 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2036, epoch_train_loss=0.0007309915596211087
Epoch 2037
Epoch 2037 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2037, epoch_train_loss=0.0007309915596211087
Epoch 2038
Epoch 2038 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2038, epoch_train_loss=0.0007309915596211087
Epoch 2039
Epoch 2039 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2039, epoch_train_loss=0.0007309915596211087
Epoch 2040
Epoch 2040 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2040, epoch_train_loss=0.0007309915596211087
Epoch 2041
Epoch 2041 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2041, epoch_train_loss=0.0007309915596211087
Epoch 2042
Epoch 2042 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2042, epoch_train_loss=0.0007309915596211087
Epoch 2043
Epoch 2043 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2043, epoch_train_loss=0.0007309915596211087
Epoch 2044
Epoch 2044 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2044, epoch_train_loss=0.0007309915596211087
Epoch 2045
Epoch 2045 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2045, epoch_train_loss=0.0007309915596211087
Epoch 2046
Epoch 2046 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2046, epoch_train_loss=0.0007309915596211087
Epoch 2047
Epoch 2047 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2047, epoch_train_loss=0.0007309915596211087
Epoch 2048
Epoch 2048 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2048, epoch_train_loss=0.0007309915596211087
Epoch 2049
Epoch 2049 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2049, epoch_train_loss=0.0007309915596211087
Epoch 2050
Epoch 2050 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2050, epoch_train_loss=0.0007309915596211087
Epoch 2051
Epoch 2051 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2051, epoch_train_loss=0.0007309915596211087
Epoch 2052
Epoch 2052 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2052, epoch_train_loss=0.0007309915596211087
Epoch 2053
Epoch 2053 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2053, epoch_train_loss=0.0007309915596211087
Epoch 2054
Epoch 2054 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2054, epoch_train_loss=0.0007309915596211087
Epoch 2055
Epoch 2055 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2055, epoch_train_loss=0.0007309915596211087
Epoch 2056
Epoch 2056 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2056, epoch_train_loss=0.0007309915596211087
Epoch 2057
Epoch 2057 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2057, epoch_train_loss=0.0007309915596211087
Epoch 2058
Epoch 2058 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2058, epoch_train_loss=0.0007309915596211087
Epoch 2059
Epoch 2059 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2059, epoch_train_loss=0.0007309915596211087
Epoch 2060
Epoch 2060 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2060, epoch_train_loss=0.0007309915596211087
Epoch 2061
Epoch 2061 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2061, epoch_train_loss=0.0007309915596211087
Epoch 2062
Epoch 2062 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2062, epoch_train_loss=0.0007309915596211087
Epoch 2063
Epoch 2063 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2063, epoch_train_loss=0.0007309915596211087
Epoch 2064
Epoch 2064 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2064, epoch_train_loss=0.0007309915596211087
Epoch 2065
Epoch 2065 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2065, epoch_train_loss=0.0007309915596211087
Epoch 2066
Epoch 2066 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2066, epoch_train_loss=0.0007309915596211087
Epoch 2067
Epoch 2067 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2067, epoch_train_loss=0.0007309915596211087
Epoch 2068
Epoch 2068 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2068, epoch_train_loss=0.0007309915596211087
Epoch 2069
Epoch 2069 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2069, epoch_train_loss=0.0007309915596211087
Epoch 2070
Epoch 2070 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2070, epoch_train_loss=0.0007309915596211087
Epoch 2071
Epoch 2071 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2071, epoch_train_loss=0.0007309915596211087
Epoch 2072
Epoch 2072 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2072, epoch_train_loss=0.0007309915596211087
Epoch 2073
Epoch 2073 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2073, epoch_train_loss=0.0007309915596211087
Epoch 2074
Epoch 2074 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2074, epoch_train_loss=0.0007309915596211087
Epoch 2075
Epoch 2075 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2075, epoch_train_loss=0.0007309915596211087
Epoch 2076
Epoch 2076 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2076, epoch_train_loss=0.0007309915596211087
Epoch 2077
Epoch 2077 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2077, epoch_train_loss=0.0007309915596211087
Epoch 2078
Epoch 2078 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2078, epoch_train_loss=0.0007309915596211087
Epoch 2079
Epoch 2079 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2079, epoch_train_loss=0.0007309915596211087
Epoch 2080
Epoch 2080 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2080, epoch_train_loss=0.0007309915596211087
Epoch 2081
Epoch 2081 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2081, epoch_train_loss=0.0007309915596211087
Epoch 2082
Epoch 2082 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2082, epoch_train_loss=0.0007309915596211087
Epoch 2083
Epoch 2083 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2083, epoch_train_loss=0.0007309915596211087
Epoch 2084
Epoch 2084 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2084, epoch_train_loss=0.0007309915596211087
Epoch 2085
Epoch 2085 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2085, epoch_train_loss=0.0007309915596211087
Epoch 2086
Epoch 2086 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2086, epoch_train_loss=0.0007309915596211087
Epoch 2087
Epoch 2087 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2087, epoch_train_loss=0.0007309915596211087
Epoch 2088
Epoch 2088 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2088, epoch_train_loss=0.0007309915596211087
Epoch 2089
Epoch 2089 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2089, epoch_train_loss=0.0007309915596211087
Epoch 2090
Epoch 2090 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2090, epoch_train_loss=0.0007309915596211087
Epoch 2091
Epoch 2091 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2091, epoch_train_loss=0.0007309915596211087
Epoch 2092
Epoch 2092 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2092, epoch_train_loss=0.0007309915596211087
Epoch 2093
Epoch 2093 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2093, epoch_train_loss=0.0007309915596211087
Epoch 2094
Epoch 2094 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2094, epoch_train_loss=0.0007309915596211087
Epoch 2095
Epoch 2095 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2095, epoch_train_loss=0.0007309915596211087
Epoch 2096
Epoch 2096 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2096, epoch_train_loss=0.0007309915596211087
Epoch 2097
Epoch 2097 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2097, epoch_train_loss=0.0007309915596211087
Epoch 2098
Epoch 2098 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2098, epoch_train_loss=0.0007309915596211087
Epoch 2099
Epoch 2099 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2099, epoch_train_loss=0.0007309915596211087
Epoch 2100
Epoch 2100 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2100, epoch_train_loss=0.0007309915596211087
Epoch 2101
Epoch 2101 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2101, epoch_train_loss=0.0007309915596211087
Epoch 2102
Epoch 2102 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2102, epoch_train_loss=0.0007309915596211087
Epoch 2103
Epoch 2103 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2103, epoch_train_loss=0.0007309915596211087
Epoch 2104
Epoch 2104 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2104, epoch_train_loss=0.0007309915596211087
Epoch 2105
Epoch 2105 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2105, epoch_train_loss=0.0007309915596211087
Epoch 2106
Epoch 2106 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2106, epoch_train_loss=0.0007309915596211087
Epoch 2107
Epoch 2107 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2107, epoch_train_loss=0.0007309915596211087
Epoch 2108
Epoch 2108 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2108, epoch_train_loss=0.0007309915596211087
Epoch 2109
Epoch 2109 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2109, epoch_train_loss=0.0007309915596211087
Epoch 2110
Epoch 2110 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2110, epoch_train_loss=0.0007309915596211087
Epoch 2111
Epoch 2111 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2111, epoch_train_loss=0.0007309915596211087
Epoch 2112
Epoch 2112 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2112, epoch_train_loss=0.0007309915596211087
Epoch 2113
Epoch 2113 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2113, epoch_train_loss=0.0007309915596211087
Epoch 2114
Epoch 2114 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2114, epoch_train_loss=0.0007309915596211087
Epoch 2115
Epoch 2115 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2115, epoch_train_loss=0.0007309915596211087
Epoch 2116
Epoch 2116 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2116, epoch_train_loss=0.0007309915596211087
Epoch 2117
Epoch 2117 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2117, epoch_train_loss=0.0007309915596211087
Epoch 2118
Epoch 2118 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2118, epoch_train_loss=0.0007309915596211087
Epoch 2119
Epoch 2119 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2119, epoch_train_loss=0.0007309915596211087
Epoch 2120
Epoch 2120 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2120, epoch_train_loss=0.0007309915596211087
Epoch 2121
Epoch 2121 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2121, epoch_train_loss=0.0007309915596211087
Epoch 2122
Epoch 2122 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2122, epoch_train_loss=0.0007309915596211087
Epoch 2123
Epoch 2123 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2123, epoch_train_loss=0.0007309915596211087
Epoch 2124
Epoch 2124 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2124, epoch_train_loss=0.0007309915596211087
Epoch 2125
Epoch 2125 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2125, epoch_train_loss=0.0007309915596211087
Epoch 2126
Epoch 2126 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2126, epoch_train_loss=0.0007309915596211087
Epoch 2127
Epoch 2127 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2127, epoch_train_loss=0.0007309915596211087
Epoch 2128
Epoch 2128 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2128, epoch_train_loss=0.0007309915596211087
Epoch 2129
Epoch 2129 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2129, epoch_train_loss=0.0007309915596211087
Epoch 2130
Epoch 2130 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2130, epoch_train_loss=0.0007309915596211087
Epoch 2131
Epoch 2131 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2131, epoch_train_loss=0.0007309915596211087
Epoch 2132
Epoch 2132 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2132, epoch_train_loss=0.0007309915596211087
Epoch 2133
Epoch 2133 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2133, epoch_train_loss=0.0007309915596211087
Epoch 2134
Epoch 2134 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2134, epoch_train_loss=0.0007309915596211087
Epoch 2135
Epoch 2135 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2135, epoch_train_loss=0.0007309915596211087
Epoch 2136
Epoch 2136 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2136, epoch_train_loss=0.0007309915596211087
Epoch 2137
Epoch 2137 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2137, epoch_train_loss=0.0007309915596211087
Epoch 2138
Epoch 2138 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2138, epoch_train_loss=0.0007309915596211087
Epoch 2139
Epoch 2139 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2139, epoch_train_loss=0.0007309915596211087
Epoch 2140
Epoch 2140 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2140, epoch_train_loss=0.0007309915596211087
Epoch 2141
Epoch 2141 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2141, epoch_train_loss=0.0007309915596211087
Epoch 2142
Epoch 2142 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2142, epoch_train_loss=0.0007309915596211087
Epoch 2143
Epoch 2143 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2143, epoch_train_loss=0.0007309915596211087
Epoch 2144
Epoch 2144 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2144, epoch_train_loss=0.0007309915596211087
Epoch 2145
Epoch 2145 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2145, epoch_train_loss=0.0007309915596211087
Epoch 2146
Epoch 2146 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2146, epoch_train_loss=0.0007309915596211087
Epoch 2147
Epoch 2147 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2147, epoch_train_loss=0.0007309915596211087
Epoch 2148
Epoch 2148 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2148, epoch_train_loss=0.0007309915596211087
Epoch 2149
Epoch 2149 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2149, epoch_train_loss=0.0007309915596211087
Epoch 2150
Epoch 2150 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2150, epoch_train_loss=0.0007309915596211087
Epoch 2151
Epoch 2151 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2151, epoch_train_loss=0.0007309915596211087
Epoch 2152
Epoch 2152 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2152, epoch_train_loss=0.0007309915596211087
Epoch 2153
Epoch 2153 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2153, epoch_train_loss=0.0007309915596211087
Epoch 2154
Epoch 2154 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2154, epoch_train_loss=0.0007309915596211087
Epoch 2155
Epoch 2155 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2155, epoch_train_loss=0.0007309915596211087
Epoch 2156
Epoch 2156 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2156, epoch_train_loss=0.0007309915596211087
Epoch 2157
Epoch 2157 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2157, epoch_train_loss=0.0007309915596211087
Epoch 2158
Epoch 2158 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2158, epoch_train_loss=0.0007309915596211087
Epoch 2159
Epoch 2159 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2159, epoch_train_loss=0.0007309915596211087
Epoch 2160
Epoch 2160 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2160, epoch_train_loss=0.0007309915596211087
Epoch 2161
Epoch 2161 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2161, epoch_train_loss=0.0007309915596211087
Epoch 2162
Epoch 2162 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2162, epoch_train_loss=0.0007309915596211087
Epoch 2163
Epoch 2163 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2163, epoch_train_loss=0.0007309915596211087
Epoch 2164
Epoch 2164 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2164, epoch_train_loss=0.0007309915596211087
Epoch 2165
Epoch 2165 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2165, epoch_train_loss=0.0007309915596211087
Epoch 2166
Epoch 2166 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2166, epoch_train_loss=0.0007309915596211087
Epoch 2167
Epoch 2167 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2167, epoch_train_loss=0.0007309915596211087
Epoch 2168
Epoch 2168 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2168, epoch_train_loss=0.0007309915596211087
Epoch 2169
Epoch 2169 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2169, epoch_train_loss=0.0007309915596211087
Epoch 2170
Epoch 2170 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2170, epoch_train_loss=0.0007309915596211087
Epoch 2171
Epoch 2171 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2171, epoch_train_loss=0.0007309915596211087
Epoch 2172
Epoch 2172 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2172, epoch_train_loss=0.0007309915596211087
Epoch 2173
Epoch 2173 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2173, epoch_train_loss=0.0007309915596211087
Epoch 2174
Epoch 2174 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2174, epoch_train_loss=0.0007309915596211087
Epoch 2175
Epoch 2175 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2175, epoch_train_loss=0.0007309915596211087
Epoch 2176
Epoch 2176 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2176, epoch_train_loss=0.0007309915596211087
Epoch 2177
Epoch 2177 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2177, epoch_train_loss=0.0007309915596211087
Epoch 2178
Epoch 2178 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2178, epoch_train_loss=0.0007309915596211087
Epoch 2179
Epoch 2179 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2179, epoch_train_loss=0.0007309915596211087
Epoch 2180
Epoch 2180 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2180, epoch_train_loss=0.0007309915596211087
Epoch 2181
Epoch 2181 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2181, epoch_train_loss=0.0007309915596211087
Epoch 2182
Epoch 2182 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2182, epoch_train_loss=0.0007309915596211087
Epoch 2183
Epoch 2183 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2183, epoch_train_loss=0.0007309915596211087
Epoch 2184
Epoch 2184 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2184, epoch_train_loss=0.0007309915596211087
Epoch 2185
Epoch 2185 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2185, epoch_train_loss=0.0007309915596211087
Epoch 2186
Epoch 2186 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2186, epoch_train_loss=0.0007309915596211087
Epoch 2187
Epoch 2187 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2187, epoch_train_loss=0.0007309915596211087
Epoch 2188
Epoch 2188 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2188, epoch_train_loss=0.0007309915596211087
Epoch 2189
Epoch 2189 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2189, epoch_train_loss=0.0007309915596211087
Epoch 2190
Epoch 2190 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2190, epoch_train_loss=0.0007309915596211087
Epoch 2191
Epoch 2191 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2191, epoch_train_loss=0.0007309915596211087
Epoch 2192
Epoch 2192 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2192, epoch_train_loss=0.0007309915596211087
Epoch 2193
Epoch 2193 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2193, epoch_train_loss=0.0007309915596211087
Epoch 2194
Epoch 2194 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2194, epoch_train_loss=0.0007309915596211087
Epoch 2195
Epoch 2195 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2195, epoch_train_loss=0.0007309915596211087
Epoch 2196
Epoch 2196 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2196, epoch_train_loss=0.0007309915596211087
Epoch 2197
Epoch 2197 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2197, epoch_train_loss=0.0007309915596211087
Epoch 2198
Epoch 2198 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2198, epoch_train_loss=0.0007309915596211087
Epoch 2199
Epoch 2199 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2199, epoch_train_loss=0.0007309915596211087
Epoch 2200
Epoch 2200 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2200, epoch_train_loss=0.0007309915596211087
Epoch 2201
Epoch 2201 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2201, epoch_train_loss=0.0007309915596211087
Epoch 2202
Epoch 2202 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2202, epoch_train_loss=0.0007309915596211087
Epoch 2203
Epoch 2203 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2203, epoch_train_loss=0.0007309915596211087
Epoch 2204
Epoch 2204 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2204, epoch_train_loss=0.0007309915596211087
Epoch 2205
Epoch 2205 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2205, epoch_train_loss=0.0007309915596211087
Epoch 2206
Epoch 2206 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2206, epoch_train_loss=0.0007309915596211087
Epoch 2207
Epoch 2207 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2207, epoch_train_loss=0.0007309915596211087
Epoch 2208
Epoch 2208 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2208, epoch_train_loss=0.0007309915596211087
Epoch 2209
Epoch 2209 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2209, epoch_train_loss=0.0007309915596211087
Epoch 2210
Epoch 2210 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2210, epoch_train_loss=0.0007309915596211087
Epoch 2211
Epoch 2211 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2211, epoch_train_loss=0.0007309915596211087
Epoch 2212
Epoch 2212 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2212, epoch_train_loss=0.0007309915596211087
Epoch 2213
Epoch 2213 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2213, epoch_train_loss=0.0007309915596211087
Epoch 2214
Epoch 2214 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2214, epoch_train_loss=0.0007309915596211087
Epoch 2215
Epoch 2215 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2215, epoch_train_loss=0.0007309915596211087
Epoch 2216
Epoch 2216 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2216, epoch_train_loss=0.0007309915596211087
Epoch 2217
Epoch 2217 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2217, epoch_train_loss=0.0007309915596211087
Epoch 2218
Epoch 2218 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2218, epoch_train_loss=0.0007309915596211087
Epoch 2219
Epoch 2219 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2219, epoch_train_loss=0.0007309915596211087
Epoch 2220
Epoch 2220 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2220, epoch_train_loss=0.0007309915596211087
Epoch 2221
Epoch 2221 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2221, epoch_train_loss=0.0007309915596211087
Epoch 2222
Epoch 2222 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2222, epoch_train_loss=0.0007309915596211087
Epoch 2223
Epoch 2223 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2223, epoch_train_loss=0.0007309915596211087
Epoch 2224
Epoch 2224 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2224, epoch_train_loss=0.0007309915596211087
Epoch 2225
Epoch 2225 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2225, epoch_train_loss=0.0007309915596211087
Epoch 2226
Epoch 2226 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2226, epoch_train_loss=0.0007309915596211087
Epoch 2227
Epoch 2227 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2227, epoch_train_loss=0.0007309915596211087
Epoch 2228
Epoch 2228 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2228, epoch_train_loss=0.0007309915596211087
Epoch 2229
Epoch 2229 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2229, epoch_train_loss=0.0007309915596211087
Epoch 2230
Epoch 2230 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2230, epoch_train_loss=0.0007309915596211087
Epoch 2231
Epoch 2231 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2231, epoch_train_loss=0.0007309915596211087
Epoch 2232
Epoch 2232 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2232, epoch_train_loss=0.0007309915596211087
Epoch 2233
Epoch 2233 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2233, epoch_train_loss=0.0007309915596211087
Epoch 2234
Epoch 2234 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2234, epoch_train_loss=0.0007309915596211087
Epoch 2235
Epoch 2235 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2235, epoch_train_loss=0.0007309915596211087
Epoch 2236
Epoch 2236 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2236, epoch_train_loss=0.0007309915596211087
Epoch 2237
Epoch 2237 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2237, epoch_train_loss=0.0007309915596211087
Epoch 2238
Epoch 2238 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2238, epoch_train_loss=0.0007309915596211087
Epoch 2239
Epoch 2239 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2239, epoch_train_loss=0.0007309915596211087
Epoch 2240
Epoch 2240 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2240, epoch_train_loss=0.0007309915596211087
Epoch 2241
Epoch 2241 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2241, epoch_train_loss=0.0007309915596211087
Epoch 2242
Epoch 2242 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2242, epoch_train_loss=0.0007309915596211087
Epoch 2243
Epoch 2243 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2243, epoch_train_loss=0.0007309915596211087
Epoch 2244
Epoch 2244 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2244, epoch_train_loss=0.0007309915596211087
Epoch 2245
Epoch 2245 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2245, epoch_train_loss=0.0007309915596211087
Epoch 2246
Epoch 2246 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2246, epoch_train_loss=0.0007309915596211087
Epoch 2247
Epoch 2247 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2247, epoch_train_loss=0.0007309915596211087
Epoch 2248
Epoch 2248 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2248, epoch_train_loss=0.0007309915596211087
Epoch 2249
Epoch 2249 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2249, epoch_train_loss=0.0007309915596211087
Epoch 2250
Epoch 2250 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2250, epoch_train_loss=0.0007309915596211087
Epoch 2251
Epoch 2251 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2251, epoch_train_loss=0.0007309915596211087
Epoch 2252
Epoch 2252 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2252, epoch_train_loss=0.0007309915596211087
Epoch 2253
Epoch 2253 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2253, epoch_train_loss=0.0007309915596211087
Epoch 2254
Epoch 2254 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2254, epoch_train_loss=0.0007309915596211087
Epoch 2255
Epoch 2255 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2255, epoch_train_loss=0.0007309915596211087
Epoch 2256
Epoch 2256 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2256, epoch_train_loss=0.0007309915596211087
Epoch 2257
Epoch 2257 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2257, epoch_train_loss=0.0007309915596211087
Epoch 2258
Epoch 2258 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2258, epoch_train_loss=0.0007309915596211087
Epoch 2259
Epoch 2259 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2259, epoch_train_loss=0.0007309915596211087
Epoch 2260
Epoch 2260 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2260, epoch_train_loss=0.0007309915596211087
Epoch 2261
Epoch 2261 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2261, epoch_train_loss=0.0007309915596211087
Epoch 2262
Epoch 2262 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2262, epoch_train_loss=0.0007309915596211087
Epoch 2263
Epoch 2263 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2263, epoch_train_loss=0.0007309915596211087
Epoch 2264
Epoch 2264 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2264, epoch_train_loss=0.0007309915596211087
Epoch 2265
Epoch 2265 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2265, epoch_train_loss=0.0007309915596211087
Epoch 2266
Epoch 2266 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2266, epoch_train_loss=0.0007309915596211087
Epoch 2267
Epoch 2267 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2267, epoch_train_loss=0.0007309915596211087
Epoch 2268
Epoch 2268 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2268, epoch_train_loss=0.0007309915596211087
Epoch 2269
Epoch 2269 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2269, epoch_train_loss=0.0007309915596211087
Epoch 2270
Epoch 2270 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2270, epoch_train_loss=0.0007309915596211087
Epoch 2271
Epoch 2271 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2271, epoch_train_loss=0.0007309915596211087
Epoch 2272
Epoch 2272 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2272, epoch_train_loss=0.0007309915596211087
Epoch 2273
Epoch 2273 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2273, epoch_train_loss=0.0007309915596211087
Epoch 2274
Epoch 2274 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2274, epoch_train_loss=0.0007309915596211087
Epoch 2275
Epoch 2275 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2275, epoch_train_loss=0.0007309915596211087
Epoch 2276
Epoch 2276 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2276, epoch_train_loss=0.0007309915596211087
Epoch 2277
Epoch 2277 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2277, epoch_train_loss=0.0007309915596211087
Epoch 2278
Epoch 2278 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2278, epoch_train_loss=0.0007309915596211087
Epoch 2279
Epoch 2279 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2279, epoch_train_loss=0.0007309915596211087
Epoch 2280
Epoch 2280 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2280, epoch_train_loss=0.0007309915596211087
Epoch 2281
Epoch 2281 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2281, epoch_train_loss=0.0007309915596211087
Epoch 2282
Epoch 2282 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2282, epoch_train_loss=0.0007309915596211087
Epoch 2283
Epoch 2283 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2283, epoch_train_loss=0.0007309915596211087
Epoch 2284
Epoch 2284 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2284, epoch_train_loss=0.0007309915596211087
Epoch 2285
Epoch 2285 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2285, epoch_train_loss=0.0007309915596211087
Epoch 2286
Epoch 2286 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2286, epoch_train_loss=0.0007309915596211087
Epoch 2287
Epoch 2287 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2287, epoch_train_loss=0.0007309915596211087
Epoch 2288
Epoch 2288 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2288, epoch_train_loss=0.0007309915596211087
Epoch 2289
Epoch 2289 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2289, epoch_train_loss=0.0007309915596211087
Epoch 2290
Epoch 2290 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2290, epoch_train_loss=0.0007309915596211087
Epoch 2291
Epoch 2291 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2291, epoch_train_loss=0.0007309915596211087
Epoch 2292
Epoch 2292 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2292, epoch_train_loss=0.0007309915596211087
Epoch 2293
Epoch 2293 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2293, epoch_train_loss=0.0007309915596211087
Epoch 2294
Epoch 2294 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2294, epoch_train_loss=0.0007309915596211087
Epoch 2295
Epoch 2295 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2295, epoch_train_loss=0.0007309915596211087
Epoch 2296
Epoch 2296 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2296, epoch_train_loss=0.0007309915596211087
Epoch 2297
Epoch 2297 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2297, epoch_train_loss=0.0007309915596211087
Epoch 2298
Epoch 2298 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2298, epoch_train_loss=0.0007309915596211087
Epoch 2299
Epoch 2299 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2299, epoch_train_loss=0.0007309915596211087
Epoch 2300
Epoch 2300 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2300, epoch_train_loss=0.0007309915596211087
Epoch 2301
Epoch 2301 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2301, epoch_train_loss=0.0007309915596211087
Epoch 2302
Epoch 2302 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2302, epoch_train_loss=0.0007309915596211087
Epoch 2303
Epoch 2303 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2303, epoch_train_loss=0.0007309915596211087
Epoch 2304
Epoch 2304 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2304, epoch_train_loss=0.0007309915596211087
Epoch 2305
Epoch 2305 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2305, epoch_train_loss=0.0007309915596211087
Epoch 2306
Epoch 2306 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2306, epoch_train_loss=0.0007309915596211087
Epoch 2307
Epoch 2307 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2307, epoch_train_loss=0.0007309915596211087
Epoch 2308
Epoch 2308 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2308, epoch_train_loss=0.0007309915596211087
Epoch 2309
Epoch 2309 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2309, epoch_train_loss=0.0007309915596211087
Epoch 2310
Epoch 2310 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2310, epoch_train_loss=0.0007309915596211087
Epoch 2311
Epoch 2311 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2311, epoch_train_loss=0.0007309915596211087
Epoch 2312
Epoch 2312 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2312, epoch_train_loss=0.0007309915596211087
Epoch 2313
Epoch 2313 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2313, epoch_train_loss=0.0007309915596211087
Epoch 2314
Epoch 2314 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2314, epoch_train_loss=0.0007309915596211087
Epoch 2315
Epoch 2315 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2315, epoch_train_loss=0.0007309915596211087
Epoch 2316
Epoch 2316 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2316, epoch_train_loss=0.0007309915596211087
Epoch 2317
Epoch 2317 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2317, epoch_train_loss=0.0007309915596211087
Epoch 2318
Epoch 2318 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2318, epoch_train_loss=0.0007309915596211087
Epoch 2319
Epoch 2319 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2319, epoch_train_loss=0.0007309915596211087
Epoch 2320
Epoch 2320 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2320, epoch_train_loss=0.0007309915596211087
Epoch 2321
Epoch 2321 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2321, epoch_train_loss=0.0007309915596211087
Epoch 2322
Epoch 2322 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2322, epoch_train_loss=0.0007309915596211087
Epoch 2323
Epoch 2323 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2323, epoch_train_loss=0.0007309915596211087
Epoch 2324
Epoch 2324 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2324, epoch_train_loss=0.0007309915596211087
Epoch 2325
Epoch 2325 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2325, epoch_train_loss=0.0007309915596211087
Epoch 2326
Epoch 2326 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2326, epoch_train_loss=0.0007309915596211087
Epoch 2327
Epoch 2327 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2327, epoch_train_loss=0.0007309915596211087
Epoch 2328
Epoch 2328 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2328, epoch_train_loss=0.0007309915596211087
Epoch 2329
Epoch 2329 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2329, epoch_train_loss=0.0007309915596211087
Epoch 2330
Epoch 2330 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2330, epoch_train_loss=0.0007309915596211087
Epoch 2331
Epoch 2331 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2331, epoch_train_loss=0.0007309915596211087
Epoch 2332
Epoch 2332 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2332, epoch_train_loss=0.0007309915596211087
Epoch 2333
Epoch 2333 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2333, epoch_train_loss=0.0007309915596211087
Epoch 2334
Epoch 2334 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2334, epoch_train_loss=0.0007309915596211087
Epoch 2335
Epoch 2335 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2335, epoch_train_loss=0.0007309915596211087
Epoch 2336
Epoch 2336 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2336, epoch_train_loss=0.0007309915596211087
Epoch 2337
Epoch 2337 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2337, epoch_train_loss=0.0007309915596211087
Epoch 2338
Epoch 2338 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2338, epoch_train_loss=0.0007309915596211087
Epoch 2339
Epoch 2339 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2339, epoch_train_loss=0.0007309915596211087
Epoch 2340
Epoch 2340 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2340, epoch_train_loss=0.0007309915596211087
Epoch 2341
Epoch 2341 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2341, epoch_train_loss=0.0007309915596211087
Epoch 2342
Epoch 2342 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2342, epoch_train_loss=0.0007309915596211087
Epoch 2343
Epoch 2343 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2343, epoch_train_loss=0.0007309915596211087
Epoch 2344
Epoch 2344 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2344, epoch_train_loss=0.0007309915596211087
Epoch 2345
Epoch 2345 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2345, epoch_train_loss=0.0007309915596211087
Epoch 2346
Epoch 2346 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2346, epoch_train_loss=0.0007309915596211087
Epoch 2347
Epoch 2347 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2347, epoch_train_loss=0.0007309915596211087
Epoch 2348
Epoch 2348 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2348, epoch_train_loss=0.0007309915596211087
Epoch 2349
Epoch 2349 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2349, epoch_train_loss=0.0007309915596211087
Epoch 2350
Epoch 2350 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2350, epoch_train_loss=0.0007309915596211087
Epoch 2351
Epoch 2351 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2351, epoch_train_loss=0.0007309915596211087
Epoch 2352
Epoch 2352 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2352, epoch_train_loss=0.0007309915596211087
Epoch 2353
Epoch 2353 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2353, epoch_train_loss=0.0007309915596211087
Epoch 2354
Epoch 2354 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2354, epoch_train_loss=0.0007309915596211087
Epoch 2355
Epoch 2355 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2355, epoch_train_loss=0.0007309915596211087
Epoch 2356
Epoch 2356 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2356, epoch_train_loss=0.0007309915596211087
Epoch 2357
Epoch 2357 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2357, epoch_train_loss=0.0007309915596211087
Epoch 2358
Epoch 2358 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2358, epoch_train_loss=0.0007309915596211087
Epoch 2359
Epoch 2359 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2359, epoch_train_loss=0.0007309915596211087
Epoch 2360
Epoch 2360 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2360, epoch_train_loss=0.0007309915596211087
Epoch 2361
Epoch 2361 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2361, epoch_train_loss=0.0007309915596211087
Epoch 2362
Epoch 2362 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2362, epoch_train_loss=0.0007309915596211087
Epoch 2363
Epoch 2363 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2363, epoch_train_loss=0.0007309915596211087
Epoch 2364
Epoch 2364 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2364, epoch_train_loss=0.0007309915596211087
Epoch 2365
Epoch 2365 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2365, epoch_train_loss=0.0007309915596211087
Epoch 2366
Epoch 2366 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2366, epoch_train_loss=0.0007309915596211087
Epoch 2367
Epoch 2367 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2367, epoch_train_loss=0.0007309915596211087
Epoch 2368
Epoch 2368 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2368, epoch_train_loss=0.0007309915596211087
Epoch 2369
Epoch 2369 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2369, epoch_train_loss=0.0007309915596211087
Epoch 2370
Epoch 2370 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2370, epoch_train_loss=0.0007309915596211087
Epoch 2371
Epoch 2371 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2371, epoch_train_loss=0.0007309915596211087
Epoch 2372
Epoch 2372 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2372, epoch_train_loss=0.0007309915596211087
Epoch 2373
Epoch 2373 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2373, epoch_train_loss=0.0007309915596211087
Epoch 2374
Epoch 2374 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2374, epoch_train_loss=0.0007309915596211087
Epoch 2375
Epoch 2375 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2375, epoch_train_loss=0.0007309915596211087
Epoch 2376
Epoch 2376 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2376, epoch_train_loss=0.0007309915596211087
Epoch 2377
Epoch 2377 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2377, epoch_train_loss=0.0007309915596211087
Epoch 2378
Epoch 2378 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2378, epoch_train_loss=0.0007309915596211087
Epoch 2379
Epoch 2379 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2379, epoch_train_loss=0.0007309915596211087
Epoch 2380
Epoch 2380 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2380, epoch_train_loss=0.0007309915596211087
Epoch 2381
Epoch 2381 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2381, epoch_train_loss=0.0007309915596211087
Epoch 2382
Epoch 2382 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2382, epoch_train_loss=0.0007309915596211087
Epoch 2383
Epoch 2383 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2383, epoch_train_loss=0.0007309915596211087
Epoch 2384
Epoch 2384 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2384, epoch_train_loss=0.0007309915596211087
Epoch 2385
Epoch 2385 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2385, epoch_train_loss=0.0007309915596211087
Epoch 2386
Epoch 2386 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2386, epoch_train_loss=0.0007309915596211087
Epoch 2387
Epoch 2387 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2387, epoch_train_loss=0.0007309915596211087
Epoch 2388
Epoch 2388 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2388, epoch_train_loss=0.0007309915596211087
Epoch 2389
Epoch 2389 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2389, epoch_train_loss=0.0007309915596211087
Epoch 2390
Epoch 2390 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2390, epoch_train_loss=0.0007309915596211087
Epoch 2391
Epoch 2391 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2391, epoch_train_loss=0.0007309915596211087
Epoch 2392
Epoch 2392 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2392, epoch_train_loss=0.0007309915596211087
Epoch 2393
Epoch 2393 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2393, epoch_train_loss=0.0007309915596211087
Epoch 2394
Epoch 2394 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2394, epoch_train_loss=0.0007309915596211087
Epoch 2395
Epoch 2395 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2395, epoch_train_loss=0.0007309915596211087
Epoch 2396
Epoch 2396 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2396, epoch_train_loss=0.0007309915596211087
Epoch 2397
Epoch 2397 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2397, epoch_train_loss=0.0007309915596211087
Epoch 2398
Epoch 2398 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2398, epoch_train_loss=0.0007309915596211087
Epoch 2399
Epoch 2399 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2399, epoch_train_loss=0.0007309915596211087
Epoch 2400
Epoch 2400 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2400, epoch_train_loss=0.0007309915596211087
Epoch 2401
Epoch 2401 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2401, epoch_train_loss=0.0007309915596211087
Epoch 2402
Epoch 2402 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2402, epoch_train_loss=0.0007309915596211087
Epoch 2403
Epoch 2403 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2403, epoch_train_loss=0.0007309915596211087
Epoch 2404
Epoch 2404 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2404, epoch_train_loss=0.0007309915596211087
Epoch 2405
Epoch 2405 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2405, epoch_train_loss=0.0007309915596211087
Epoch 2406
Epoch 2406 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2406, epoch_train_loss=0.0007309915596211087
Epoch 2407
Epoch 2407 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2407, epoch_train_loss=0.0007309915596211087
Epoch 2408
Epoch 2408 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2408, epoch_train_loss=0.0007309915596211087
Epoch 2409
Epoch 2409 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2409, epoch_train_loss=0.0007309915596211087
Epoch 2410
Epoch 2410 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2410, epoch_train_loss=0.0007309915596211087
Epoch 2411
Epoch 2411 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2411, epoch_train_loss=0.0007309915596211087
Epoch 2412
Epoch 2412 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2412, epoch_train_loss=0.0007309915596211087
Epoch 2413
Epoch 2413 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2413, epoch_train_loss=0.0007309915596211087
Epoch 2414
Epoch 2414 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2414, epoch_train_loss=0.0007309915596211087
Epoch 2415
Epoch 2415 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2415, epoch_train_loss=0.0007309915596211087
Epoch 2416
Epoch 2416 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2416, epoch_train_loss=0.0007309915596211087
Epoch 2417
Epoch 2417 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2417, epoch_train_loss=0.0007309915596211087
Epoch 2418
Epoch 2418 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2418, epoch_train_loss=0.0007309915596211087
Epoch 2419
Epoch 2419 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2419, epoch_train_loss=0.0007309915596211087
Epoch 2420
Epoch 2420 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2420, epoch_train_loss=0.0007309915596211087
Epoch 2421
Epoch 2421 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2421, epoch_train_loss=0.0007309915596211087
Epoch 2422
Epoch 2422 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2422, epoch_train_loss=0.0007309915596211087
Epoch 2423
Epoch 2423 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2423, epoch_train_loss=0.0007309915596211087
Epoch 2424
Epoch 2424 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2424, epoch_train_loss=0.0007309915596211087
Epoch 2425
Epoch 2425 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2425, epoch_train_loss=0.0007309915596211087
Epoch 2426
Epoch 2426 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2426, epoch_train_loss=0.0007309915596211087
Epoch 2427
Epoch 2427 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2427, epoch_train_loss=0.0007309915596211087
Epoch 2428
Epoch 2428 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2428, epoch_train_loss=0.0007309915596211087
Epoch 2429
Epoch 2429 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2429, epoch_train_loss=0.0007309915596211087
Epoch 2430
Epoch 2430 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2430, epoch_train_loss=0.0007309915596211087
Epoch 2431
Epoch 2431 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2431, epoch_train_loss=0.0007309915596211087
Epoch 2432
Epoch 2432 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2432, epoch_train_loss=0.0007309915596211087
Epoch 2433
Epoch 2433 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2433, epoch_train_loss=0.0007309915596211087
Epoch 2434
Epoch 2434 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2434, epoch_train_loss=0.0007309915596211087
Epoch 2435
Epoch 2435 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2435, epoch_train_loss=0.0007309915596211087
Epoch 2436
Epoch 2436 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2436, epoch_train_loss=0.0007309915596211087
Epoch 2437
Epoch 2437 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2437, epoch_train_loss=0.0007309915596211087
Epoch 2438
Epoch 2438 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2438, epoch_train_loss=0.0007309915596211087
Epoch 2439
Epoch 2439 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2439, epoch_train_loss=0.0007309915596211087
Epoch 2440
Epoch 2440 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2440, epoch_train_loss=0.0007309915596211087
Epoch 2441
Epoch 2441 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2441, epoch_train_loss=0.0007309915596211087
Epoch 2442
Epoch 2442 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2442, epoch_train_loss=0.0007309915596211087
Epoch 2443
Epoch 2443 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2443, epoch_train_loss=0.0007309915596211087
Epoch 2444
Epoch 2444 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2444, epoch_train_loss=0.0007309915596211087
Epoch 2445
Epoch 2445 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2445, epoch_train_loss=0.0007309915596211087
Epoch 2446
Epoch 2446 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2446, epoch_train_loss=0.0007309915596211087
Epoch 2447
Epoch 2447 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2447, epoch_train_loss=0.0007309915596211087
Epoch 2448
Epoch 2448 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2448, epoch_train_loss=0.0007309915596211087
Epoch 2449
Epoch 2449 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2449, epoch_train_loss=0.0007309915596211087
Epoch 2450
Epoch 2450 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2450, epoch_train_loss=0.0007309915596211087
Epoch 2451
Epoch 2451 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2451, epoch_train_loss=0.0007309915596211087
Epoch 2452
Epoch 2452 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2452, epoch_train_loss=0.0007309915596211087
Epoch 2453
Epoch 2453 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2453, epoch_train_loss=0.0007309915596211087
Epoch 2454
Epoch 2454 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2454, epoch_train_loss=0.0007309915596211087
Epoch 2455
Epoch 2455 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2455, epoch_train_loss=0.0007309915596211087
Epoch 2456
Epoch 2456 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2456, epoch_train_loss=0.0007309915596211087
Epoch 2457
Epoch 2457 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2457, epoch_train_loss=0.0007309915596211087
Epoch 2458
Epoch 2458 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2458, epoch_train_loss=0.0007309915596211087
Epoch 2459
Epoch 2459 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2459, epoch_train_loss=0.0007309915596211087
Epoch 2460
Epoch 2460 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2460, epoch_train_loss=0.0007309915596211087
Epoch 2461
Epoch 2461 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2461, epoch_train_loss=0.0007309915596211087
Epoch 2462
Epoch 2462 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2462, epoch_train_loss=0.0007309915596211087
Epoch 2463
Epoch 2463 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2463, epoch_train_loss=0.0007309915596211087
Epoch 2464
Epoch 2464 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2464, epoch_train_loss=0.0007309915596211087
Epoch 2465
Epoch 2465 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2465, epoch_train_loss=0.0007309915596211087
Epoch 2466
Epoch 2466 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2466, epoch_train_loss=0.0007309915596211087
Epoch 2467
Epoch 2467 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2467, epoch_train_loss=0.0007309915596211087
Epoch 2468
Epoch 2468 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2468, epoch_train_loss=0.0007309915596211087
Epoch 2469
Epoch 2469 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2469, epoch_train_loss=0.0007309915596211087
Epoch 2470
Epoch 2470 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2470, epoch_train_loss=0.0007309915596211087
Epoch 2471
Epoch 2471 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2471, epoch_train_loss=0.0007309915596211087
Epoch 2472
Epoch 2472 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2472, epoch_train_loss=0.0007309915596211087
Epoch 2473
Epoch 2473 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2473, epoch_train_loss=0.0007309915596211087
Epoch 2474
Epoch 2474 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2474, epoch_train_loss=0.0007309915596211087
Epoch 2475
Epoch 2475 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2475, epoch_train_loss=0.0007309915596211087
Epoch 2476
Epoch 2476 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2476, epoch_train_loss=0.0007309915596211087
Epoch 2477
Epoch 2477 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2477, epoch_train_loss=0.0007309915596211087
Epoch 2478
Epoch 2478 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2478, epoch_train_loss=0.0007309915596211087
Epoch 2479
Epoch 2479 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2479, epoch_train_loss=0.0007309915596211087
Epoch 2480
Epoch 2480 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2480, epoch_train_loss=0.0007309915596211087
Epoch 2481
Epoch 2481 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2481, epoch_train_loss=0.0007309915596211087
Epoch 2482
Epoch 2482 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2482, epoch_train_loss=0.0007309915596211087
Epoch 2483
Epoch 2483 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2483, epoch_train_loss=0.0007309915596211087
Epoch 2484
Epoch 2484 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2484, epoch_train_loss=0.0007309915596211087
Epoch 2485
Epoch 2485 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2485, epoch_train_loss=0.0007309915596211087
Epoch 2486
Epoch 2486 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2486, epoch_train_loss=0.0007309915596211087
Epoch 2487
Epoch 2487 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2487, epoch_train_loss=0.0007309915596211087
Epoch 2488
Epoch 2488 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2488, epoch_train_loss=0.0007309915596211087
Epoch 2489
Epoch 2489 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2489, epoch_train_loss=0.0007309915596211087
Epoch 2490
Epoch 2490 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2490, epoch_train_loss=0.0007309915596211087
Epoch 2491
Epoch 2491 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2491, epoch_train_loss=0.0007309915596211087
Epoch 2492
Epoch 2492 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2492, epoch_train_loss=0.0007309915596211087
Epoch 2493
Epoch 2493 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2493, epoch_train_loss=0.0007309915596211087
Epoch 2494
Epoch 2494 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2494, epoch_train_loss=0.0007309915596211087
Epoch 2495
Epoch 2495 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2495, epoch_train_loss=0.0007309915596211087
Epoch 2496
Epoch 2496 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2496, epoch_train_loss=0.0007309915596211087
Epoch 2497
Epoch 2497 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2497, epoch_train_loss=0.0007309915596211087
Epoch 2498
Epoch 2498 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2498, epoch_train_loss=0.0007309915596211087
Epoch 2499
Epoch 2499 :: Batch 0/1
Batch Loss = 0.0007309915596211087
2499, epoch_train_loss=0.0007309915596211087
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting "B3LYP_WITH_VWN5 = True" in pyscf_conf.py
  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb4441cc0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb4441cc0> in UKS object of <class 'pyscf.dft.uks.UKS'>
<pyscf.gto.mole.Mole object at 0x7ffeb4441cc0> [['P', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb4440b80> [['N', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb4441c90> [['H', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb4249960> [['Li', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb424af50> [['O', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb424bf70> [['Cl', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb4249900> [['Al', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb424ac20> [['S', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb4249420> [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb427eaa0> [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb427f940> [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb427c1c0> [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb427dff0> [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb427e680> [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb427e7a0> [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb427f070> [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb427e380> [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb427d4e0> [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb427e530> [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb427c490> [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb40477f0> [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb4047820> [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb4046aa0> [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb4046b90> [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb4045c60> [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]] 5
<pyscf.gto.mole.Mole object at 0x7ffeb40461d0> [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb4045f90> [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]] 5
mol:  [['P', array([0., 0., 0.])]]
converged SCF energy = -341.104145992717  <S^2> = 3.7502984  2S+1 = 4.0001492
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb4440b80> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb4440b80> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00051022 -0.00019156 -0.00051334 ... -0.02830887 -0.02830887
 -0.02830887] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 3)
mol:  [['N', array([0., 0., 0.])]]
converged SCF energy = -54.5289742046674  <S^2> = 3.7524945  2S+1 = 4.0012471
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb4441c90> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb4441c90> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5016, 22)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-3.60081838e-04 -1.08775305e-04 -1.31917160e-05 ... -2.74817476e-02
 -2.74817476e-02 -2.74817476e-02] = ,SCAN
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5016, 22)
rho shape (4, 4, 2, 5016)
rho_filt shape: (5016,)
get descriptors tdrho.shape=(5016, 3)
mol:  [['H', array([0., 0., 0.])]]
converged SCF energy = -0.49981298400854  <S^2> = 0.75  2S+1 = 2
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb4249960> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb4249960> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 7, 7)
ao.shape (10, 2440, 7)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.92637338e-09 -1.31700807e-07 -9.61527370e-06 ... -7.35522754e-16
 -7.35522754e-16 -7.35522754e-16] = ,SCAN
get_data, dm shape = (2, 7, 7)
ao_eval.shape=(4, 2440, 7)
rho shape (4, 4, 2, 2440)
rho_filt shape: (2440,)
get descriptors tdrho.shape=(2440, 3)
mol:  [['Li', array([0., 0., 0.])]]
converged SCF energy = -7.46006188627842  <S^2> = 0.75000049  2S+1 = 2.0000005
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb424af50> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb424af50> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 4592, 22)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-5.31884219e-04 -2.81911891e-04 -2.81911891e-04 ... -1.27154711e-05
 -2.64861768e-02 -2.64861768e-02] = ,SCAN
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 4592, 22)
rho shape (4, 4, 2, 4592)
rho_filt shape: (4592,)
get descriptors tdrho.shape=(4592, 3)
mol:  [['O', array([0., 0., 0.])]]
converged SCF energy = -75.0033774436575  <S^2> = 2.0027452  2S+1 = 3.0018296
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb424bf70> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb424bf70> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5040, 22)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.96349993e-04 -3.62769681e-05 -1.92664096e-06 ... -2.76158571e-02
 -2.76158571e-02 -2.76158571e-02] = ,SCAN
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5040, 22)
rho shape (4, 4, 2, 5040)
rho_filt shape: (5040,)
get descriptors tdrho.shape=(5040, 3)
mol:  [['Cl', array([0., 0., 0.])]]
converged SCF energy = -459.957577121376  <S^2> = 0.7516194  2S+1 = 2.0016187
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb4249900> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb4249900> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6152, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.99675173e-04 -2.40561352e-04 -8.22173667e-05 ... -2.84484386e-02
 -2.84484386e-02 -2.84484386e-02] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6152, 30)
rho shape (4, 4, 2, 6152)
rho_filt shape: (6152,)
get descriptors tdrho.shape=(6152, 3)
mol:  [['Al', array([0., 0., 0.])]]
converged SCF energy = -242.226560984077  <S^2> = 0.75226414  2S+1 = 2.0022629
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb424ac20> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb424ac20> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6088, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.27863518e-03 -1.45258924e-03 -7.43339504e-04 ... -1.45430453e-05
 -3.16421883e-04 -2.94243654e-05] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6088, 30)
rho shape (4, 4, 2, 6088)
rho_filt shape: (6088,)
get descriptors tdrho.shape=(6088, 3)
mol:  [['S', array([0., 0., 0.])]]
converged SCF energy = -397.938786831136  <S^2> = 2.0022329  2S+1 = 3.0014882
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb4249420> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb4249420> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00021141 -0.00019652 -0.00020074 ... -0.02838402 -0.02838402
 -0.02838402] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 3)
mol:  [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]]
converged SCF energy = -1.16580491182912  <S^2> = -4.4408921e-16  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb427eaa0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb427eaa0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 14, 14)
ao.shape (10, 4776, 14)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.43725660e-05 -1.02204687e-06 -4.05575842e-05 ... -2.36278434e-02
 -2.36278434e-02 -2.36278434e-02] = ,SCAN
get_data, dm shape = (2, 14, 14)
ao_eval.shape=(4, 4776, 14)
rho shape (4, 4, 2, 4776)
rho_filt shape: (4776,)
get descriptors tdrho.shape=(4776, 3)
mol:  [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]]
converged SCF energy = -109.439263799566  <S^2> = 6.2172489e-15  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb427f940> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb427f940> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9848, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.89629699e-05 -2.76172354e-04 -7.59017288e-05 ... -7.34654212e-06
 -7.34654212e-06 -2.89629699e-05] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9848, 44)
rho shape (4, 4, 2, 9848)
rho_filt shape: (9848,)
get descriptors tdrho.shape=(9848, 3)
mol:  [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]]
converged SCF energy = -107.339357395734  <S^2> = 8.8817842e-16  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb427c1c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb427c1c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9752, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00043469 -0.00024024 -0.00035532 ... -0.00047537 -0.03728133
 -0.03728133] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9752, 44)
rho shape (4, 4, 2, 9752)
rho_filt shape: (9752,)
get descriptors tdrho.shape=(9752, 3)
mol:  [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]]
converged SCF energy = -93.3377924465129  <S^2> = 4.00731e-10  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb427dff0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb427dff0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 12256, 51)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-9.02468888e-05 -7.92694658e-06 -9.80568469e-06 ... -4.33714150e-02
 -4.33714150e-02 -4.33714150e-02] = ,SCAN
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 12256, 51)
rho shape (4, 4, 2, 12256)
rho_filt shape: (12256,)
get descriptors tdrho.shape=(12256, 3)
mol:  [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]]
converged SCF energy = -188.456965322844  <S^2> = 1.7763568e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb427e680> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb427e680> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 66, 66)
ao.shape (10, 14920, 66)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-7.48187338e-05 -6.19475249e-05 -2.61742784e-04 ... -8.70042314e-07
 -2.73391097e-02 -2.73391097e-02] = ,SCAN
get_data, dm shape = (2, 66, 66)
ao_eval.shape=(4, 14920, 66)
rho shape (4, 4, 2, 14920)
rho_filt shape: (14920,)
get descriptors tdrho.shape=(14920, 3)
mol:  [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]]
converged SCF energy = -920.00560888896  <S^2> = 5.1159077e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb427e7a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb427e7a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12208, 60)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00051559 -0.00027432 -0.00088583 ... -0.00027432 -0.04174728
 -0.04174728] = ,SCAN
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12208, 60)
rho shape (4, 4, 2, 12208)
rho_filt shape: (12208,)
get descriptors tdrho.shape=(12208, 3)
mol:  [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]]
converged SCF energy = -199.394370591172  <S^2> = 1.2789769e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb427f070> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb427f070> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9824, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-5.53951178e-05 -5.93507199e-06 -3.10072916e-04 ... -5.94325581e-02
 -5.94325581e-02 -5.94325581e-02] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9824, 44)
rho shape (4, 4, 2, 9824)
rho_filt shape: (9824,)
get descriptors tdrho.shape=(9824, 3)
mol:  [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]]
converged SCF energy = -150.214894474723  <S^2> = 1.0018598  2S+1 = 2.2377309
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb427e380> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb427e380> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9912, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.54833157e-04 -2.91899991e-05 -1.59382076e-06 ... -4.22396702e-02
 -4.22396702e-02 -4.22396702e-02] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9912, 44)
rho shape (4, 4, 2, 9912)
rho_filt shape: (9912,)
get descriptors tdrho.shape=(9912, 3)
mol:  [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]]
converged SCF energy = -77.2435048346373  <S^2> = 1.4210855e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb427d4e0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb427d4e0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 58, 58)
ao.shape (10, 15208, 58)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.72190712e-05 -2.72190712e-05 -2.84904833e-04 ... -1.08108260e-05
 -1.03072478e-05 -1.03072478e-05] = ,SCAN
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15208, 58)
rho shape (4, 4, 2, 15208)
rho_filt shape: (15208,)
get descriptors tdrho.shape=(15208, 3)
mol:  [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]]
converged SCF energy = -113.221335689652  <S^2> = 6.5725203e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb427e530> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb427e530> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 10040, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00015688 -0.00024669 -0.00068269 ... -0.03791166 -0.03791166
 -0.03791166] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 10040, 44)
rho shape (4, 4, 2, 10040)
rho_filt shape: (10040,)
get descriptors tdrho.shape=(10040, 3)
mol:  [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]]
converged SCF energy = -460.624592374078  <S^2> = 7.1054274e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb427c490> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb427c490> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 37, 37)
ao.shape (10, 8552, 37)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-7.28500681e-05 -5.65091132e-06 -7.37932132e-06 ... -4.76689214e-02
 -4.76689214e-02 -4.76689214e-02] = ,SCAN
get_data, dm shape = (2, 37, 37)
ao_eval.shape=(4, 8552, 37)
rho shape (4, 4, 2, 8552)
rho_filt shape: (8552,)
get descriptors tdrho.shape=(8552, 3)
mol:  [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]]
converged SCF energy = -8.04458854018922  <S^2> = 7.5273121e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb40477f0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb40477f0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 29, 29)
ao.shape (10, 6936, 29)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.0003863  -0.00040095 -0.00040095 ... -0.0213199  -0.0213199
 -0.0213199 ] = ,SCAN
get_data, dm shape = (2, 29, 29)
ao_eval.shape=(4, 6936, 29)
rho shape (4, 4, 2, 6936)
rho_filt shape: (6936,)
get descriptors tdrho.shape=(6936, 3)
mol:  [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]]
converged SCF energy = -324.340512506579  <S^2> = 1.5864643e-11  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb4047820> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb4047820> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 11536, 60)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00088473 -0.00088473 -0.00116894 ... -0.00088473 -0.00088473
 -0.00116894] = ,SCAN
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 11536, 60)
rho shape (4, 4, 2, 11536)
rho_filt shape: (11536,)
get descriptors tdrho.shape=(11536, 3)
mol:  [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]]
converged SCF energy = -1622.57507845814  <S^2> = 8.2422957e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb4046aa0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb4046aa0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 120, 120)
ao.shape (10, 24512, 120)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.91408540e-05 -1.46971271e-04 -1.08734417e-03 ... -2.81566369e-02
 -2.81566369e-02 -2.81566369e-02] = ,SCAN
get_data, dm shape = (2, 120, 120)
ao_eval.shape=(4, 24512, 120)
rho shape (4, 4, 2, 24512)
rho_filt shape: (24512,)
get descriptors tdrho.shape=(24512, 3)
mol:  [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]]
converged SCF energy = -342.979728469575  <S^2> = 2.5393021e-11  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb4046b90> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb4046b90> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 13096, 51)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.39373335e-04 -1.31641332e-04 -1.15950750e-05 ... -7.32416564e-02
 -7.32416564e-02 -7.32416564e-02] = ,SCAN
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 13096, 51)
rho shape (4, 4, 2, 13096)
rho_filt shape: (13096,)
get descriptors tdrho.shape=(13096, 3)
mol:  [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]]
converged SCF energy = -578.565335809471  <S^2> = 1.0034705  2S+1 = 2.2391699
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb4045c60> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb4045c60> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12384, 60)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.84669638e-05 -7.80543664e-05 -7.80532451e-05 ... -2.92531283e-02
 -2.92531283e-02 -2.92531283e-02] = ,SCAN
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12384, 60)
rho shape (4, 4, 2, 12384)
rho_filt shape: (12384,)
get descriptors tdrho.shape=(12384, 3)
mol:  [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]]
converged SCF energy = -40.4598214864074  <S^2> = 3.2596148e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb40461d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb40461d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 50, 50)
ao.shape (10, 13936, 50)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.56165538e-04 -7.34744214e-05 -5.30574304e-06 ... -7.93995702e-06
 -7.93995702e-06 -7.93995702e-06] = ,SCAN
get_data, dm shape = (2, 50, 50)
ao_eval.shape=(4, 13936, 50)
rho shape (4, 4, 2, 13936)
rho_filt shape: (13936,)
get descriptors tdrho.shape=(13936, 3)
mol:  [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]]
converged SCF energy = -39.0756147483504  <S^2> = 6.197709e-12  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb4045f90> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb4045f90> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 36, 36)
ao.shape (10, 9656, 36)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.28290753e-04 -1.41305449e-05 -6.13700492e-05 ... -2.47993463e-02
 -2.47993463e-02 -2.47993463e-02] = ,SCAN
get_data, dm shape = (2, 36, 36)
ao_eval.shape=(4, 9656, 36)
rho shape (4, 4, 2, 9656)
rho_filt shape: (9656,)
get descriptors tdrho.shape=(9656, 3)
mol:  [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]]
converged SCF energy = -291.719272437819  <S^2> = 1.3152146e-11  2S+1 = 1
New DM shape: (2, 58, 58)
ao.shape (10, 15256, 58)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.45512011e-04 -7.12775692e-05 -5.48666345e-06 ... -6.02613084e-06
 -6.02613084e-06 -6.02613084e-06] = ,SCAN
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15256, 58)
rho shape (4, 4, 2, 15256)
rho_filt shape: (15256,)
get descriptors tdrho.shape=(15256, 3)
PRE NAN FILT: tFxc.shape=(237017,), tdrho.shape=(237017, 3)
nan_filt_rho.shape=(237017,)
nan_filt_fxc.shape=(237017,)
tFxc.shape=(237017,), tdrho.shape=(237017, 3)
inp[0].shape = (237017, 1)
Epoch 0
Epoch 0 :: Batch 0/1
Batch Loss = 0.36212405958689936
0, epoch_train_loss=0.36212405958689936
Epoch 1
Epoch 1 :: Batch 0/1
Batch Loss = 0.26082956202803864
1, epoch_train_loss=0.26082956202803864
Epoch 2
Epoch 2 :: Batch 0/1
Batch Loss = 0.18558620669854045
2, epoch_train_loss=0.18558620669854045
Epoch 3
Epoch 3 :: Batch 0/1
Batch Loss = 0.156978522904377
3, epoch_train_loss=0.156978522904377
Epoch 4
Epoch 4 :: Batch 0/1
Batch Loss = 0.11529328527484409
4, epoch_train_loss=0.11529328527484409
Epoch 5
Epoch 5 :: Batch 0/1
Batch Loss = 0.06913313768390632
5, epoch_train_loss=0.06913313768390632
Epoch 6
Epoch 6 :: Batch 0/1
Batch Loss = 0.03736759077465918
6, epoch_train_loss=0.03736759077465918
Epoch 7
Epoch 7 :: Batch 0/1
Batch Loss = 0.018344580633009408
7, epoch_train_loss=0.018344580633009408
Epoch 8
Epoch 8 :: Batch 0/1
Batch Loss = 0.007870144887061096
8, epoch_train_loss=0.007870144887061096
Epoch 9
Epoch 9 :: Batch 0/1
Batch Loss = 0.0032086096602774493
9, epoch_train_loss=0.0032086096602774493
Epoch 10
Epoch 10 :: Batch 0/1
Batch Loss = 0.0015459982325409
10, epoch_train_loss=0.0015459982325409
Epoch 11
Epoch 11 :: Batch 0/1
Batch Loss = 0.0010080764858590796
11, epoch_train_loss=0.0010080764858590796
Epoch 12
Epoch 12 :: Batch 0/1
Batch Loss = 0.0008301312331216635
12, epoch_train_loss=0.0008301312331216635
Epoch 13
Epoch 13 :: Batch 0/1
Batch Loss = 0.0007679676056614487
13, epoch_train_loss=0.0007679676056614487
Epoch 14
Epoch 14 :: Batch 0/1
Batch Loss = 0.0007453213803913686
14, epoch_train_loss=0.0007453213803913686
Epoch 15
Epoch 15 :: Batch 0/1
Batch Loss = 0.0007367960426020213
15, epoch_train_loss=0.0007367960426020213
Epoch 16
Epoch 16 :: Batch 0/1
Batch Loss = 0.0007334741181987054
16, epoch_train_loss=0.0007334741181987054
Epoch 17
Epoch 17 :: Batch 0/1
Batch Loss = 0.0007321261399759491
17, epoch_train_loss=0.0007321261399759491
Epoch 18
Epoch 18 :: Batch 0/1
Batch Loss = 0.0007315531398898828
18, epoch_train_loss=0.0007315531398898828
Epoch 19
Epoch 19 :: Batch 0/1
Batch Loss = 0.000731296979106716
19, epoch_train_loss=0.000731296979106716
Epoch 20
Epoch 20 :: Batch 0/1
Batch Loss = 0.0007311763327550747
20, epoch_train_loss=0.0007311763327550747
Epoch 21
Epoch 21 :: Batch 0/1
Batch Loss = 0.0007311164753860055
21, epoch_train_loss=0.0007311164753860055
Epoch 22
Epoch 22 :: Batch 0/1
Batch Loss = 0.0007310852370572952
22, epoch_train_loss=0.0007310852370572952
Epoch 23
Epoch 23 :: Batch 0/1
Batch Loss = 0.0007310681294952723
23, epoch_train_loss=0.0007310681294952723
Epoch 24
Epoch 24 :: Batch 0/1
Batch Loss = 0.0007310583269687622
24, epoch_train_loss=0.0007310583269687622
Epoch 25
Epoch 25 :: Batch 0/1
Batch Loss = 0.0007310524691920869
25, epoch_train_loss=0.0007310524691920869
Epoch 26
Epoch 26 :: Batch 0/1
Batch Loss = 0.0007310488306272562
26, epoch_train_loss=0.0007310488306272562
Epoch 27
Epoch 27 :: Batch 0/1
Batch Loss = 0.0007310464890476179
27, epoch_train_loss=0.0007310464890476179
Epoch 28
Epoch 28 :: Batch 0/1
Batch Loss = 0.000731044932696838
28, epoch_train_loss=0.000731044932696838
Epoch 29
Epoch 29 :: Batch 0/1
Batch Loss = 0.0007310438674614575
29, epoch_train_loss=0.0007310438674614575
Epoch 30
Epoch 30 :: Batch 0/1
Batch Loss = 0.000731043118713111
30, epoch_train_loss=0.000731043118713111
Epoch 31
Epoch 31 :: Batch 0/1
Batch Loss = 0.0007310425795903684
31, epoch_train_loss=0.0007310425795903684
Epoch 32
Epoch 32 :: Batch 0/1
Batch Loss = 0.000731042182850651
32, epoch_train_loss=0.000731042182850651
Epoch 33
Epoch 33 :: Batch 0/1
Batch Loss = 0.0007310418850771439
33, epoch_train_loss=0.0007310418850771439
Epoch 34
Epoch 34 :: Batch 0/1
Batch Loss = 0.0007310416575619764
34, epoch_train_loss=0.0007310416575619764
Epoch 35
Epoch 35 :: Batch 0/1
Batch Loss = 0.0007310414809016753
35, epoch_train_loss=0.0007310414809016753
Epoch 36
Epoch 36 :: Batch 0/1
Batch Loss = 0.0007310413417128959
36, epoch_train_loss=0.0007310413417128959
Epoch 37
Epoch 37 :: Batch 0/1
Batch Loss = 0.0007310412305899654
37, epoch_train_loss=0.0007310412305899654
Epoch 38
Epoch 38 :: Batch 0/1
Batch Loss = 0.0007310411408070415
38, epoch_train_loss=0.0007310411408070415
Epoch 39
Epoch 39 :: Batch 0/1
Batch Loss = 0.0007310410674766901
39, epoch_train_loss=0.0007310410674766901
Epoch 40
Epoch 40 :: Batch 0/1
Batch Loss = 0.0007310410069940566
40, epoch_train_loss=0.0007310410069940566
Epoch 41
Epoch 41 :: Batch 0/1
Batch Loss = 0.0007310409566632138
41, epoch_train_loss=0.0007310409566632138
Epoch 42
Epoch 42 :: Batch 0/1
Batch Loss = 0.000731040914441841
42, epoch_train_loss=0.000731040914441841
Epoch 43
Epoch 43 :: Batch 0/1
Batch Loss = 0.0007310408787640738
43, epoch_train_loss=0.0007310408787640738
Epoch 44
Epoch 44 :: Batch 0/1
Batch Loss = 0.0007310408484158202
44, epoch_train_loss=0.0007310408484158202
Epoch 45
Epoch 45 :: Batch 0/1
Batch Loss = 0.0007310408224458212
45, epoch_train_loss=0.0007310408224458212
Epoch 46
Epoch 46 :: Batch 0/1
Batch Loss = 0.0007310408001014071
46, epoch_train_loss=0.0007310408001014071
Epoch 47
Epoch 47 :: Batch 0/1
Batch Loss = 0.0007310407807815391
47, epoch_train_loss=0.0007310407807815391
Epoch 48
Epoch 48 :: Batch 0/1
Batch Loss = 0.0007310407640021053
48, epoch_train_loss=0.0007310407640021053
Epoch 49
Epoch 49 :: Batch 0/1
Batch Loss = 0.0007310407493699999
49, epoch_train_loss=0.0007310407493699999
Epoch 50
Epoch 50 :: Batch 0/1
Batch Loss = 0.0007310407365635776
50, epoch_train_loss=0.0007310407365635776
Epoch 51
Epoch 51 :: Batch 0/1
Batch Loss = 0.0007310407253177689
51, epoch_train_loss=0.0007310407253177689
Epoch 52
Epoch 52 :: Batch 0/1
Batch Loss = 0.0007310407154130724
52, epoch_train_loss=0.0007310407154130724
Epoch 53
Epoch 53 :: Batch 0/1
Batch Loss = 0.0007310407066657571
53, epoch_train_loss=0.0007310407066657571
Epoch 54
Epoch 54 :: Batch 0/1
Batch Loss = 0.0007310406989215061
54, epoch_train_loss=0.0007310406989215061
Epoch 55
Epoch 55 :: Batch 0/1
Batch Loss = 0.0007310406920499787
55, epoch_train_loss=0.0007310406920499787
Epoch 56
Epoch 56 :: Batch 0/1
Batch Loss = 0.0007310406859405017
56, epoch_train_loss=0.0007310406859405017
Epoch 57
Epoch 57 :: Batch 0/1
Batch Loss = 0.0007310406804986372
57, epoch_train_loss=0.0007310406804986372
Epoch 58
Epoch 58 :: Batch 0/1
Batch Loss = 0.0007310406756434253
58, epoch_train_loss=0.0007310406756434253
Epoch 59
Epoch 59 :: Batch 0/1
Batch Loss = 0.0007310406713051567
59, epoch_train_loss=0.0007310406713051567
Epoch 60
Epoch 60 :: Batch 0/1
Batch Loss = 0.000731040667423566
60, epoch_train_loss=0.000731040667423566
Epoch 61
Epoch 61 :: Batch 0/1
Batch Loss = 0.000731040663946353
61, epoch_train_loss=0.000731040663946353
Epoch 62
Epoch 62 :: Batch 0/1
Batch Loss = 0.0007310406608279697
62, epoch_train_loss=0.0007310406608279697
Epoch 63
Epoch 63 :: Batch 0/1
Batch Loss = 0.0007310406580286163
63, epoch_train_loss=0.0007310406580286163
Epoch 64
Epoch 64 :: Batch 0/1
Batch Loss = 0.0007310406555134093
64, epoch_train_loss=0.0007310406555134093
Epoch 65
Epoch 65 :: Batch 0/1
Batch Loss = 0.0007310406532516874
65, epoch_train_loss=0.0007310406532516874
Epoch 66
Epoch 66 :: Batch 0/1
Batch Loss = 0.0007310406512164292
66, epoch_train_loss=0.0007310406512164292
Epoch 67
Epoch 67 :: Batch 0/1
Batch Loss = 0.0007310406493837628
67, epoch_train_loss=0.0007310406493837628
Epoch 68
Epoch 68 :: Batch 0/1
Batch Loss = 0.0007310406477325516
68, epoch_train_loss=0.0007310406477325516
Epoch 69
Epoch 69 :: Batch 0/1
Batch Loss = 0.0007310406462440414
69, epoch_train_loss=0.0007310406462440414
Epoch 70
Epoch 70 :: Batch 0/1
Batch Loss = 0.0007310406449015615
70, epoch_train_loss=0.0007310406449015615
Epoch 71
Epoch 71 :: Batch 0/1
Batch Loss = 0.0007310406436902652
71, epoch_train_loss=0.0007310406436902652
Epoch 72
Epoch 72 :: Batch 0/1
Batch Loss = 0.0007310406425969116
72, epoch_train_loss=0.0007310406425969116
Epoch 73
Epoch 73 :: Batch 0/1
Batch Loss = 0.0007310406416096728
73, epoch_train_loss=0.0007310406416096728
Epoch 74
Epoch 74 :: Batch 0/1
Batch Loss = 0.0007310406407179691
74, epoch_train_loss=0.0007310406407179691
Epoch 75
Epoch 75 :: Batch 0/1
Batch Loss = 0.0007310406399123265
75, epoch_train_loss=0.0007310406399123265
Epoch 76
Epoch 76 :: Batch 0/1
Batch Loss = 0.0007310406391842519
76, epoch_train_loss=0.0007310406391842519
Epoch 77
Epoch 77 :: Batch 0/1
Batch Loss = 0.0007310406385261222
77, epoch_train_loss=0.0007310406385261222
Epoch 78
Epoch 78 :: Batch 0/1
Batch Loss = 0.0007310406379310913
78, epoch_train_loss=0.0007310406379310913
Epoch 79
Epoch 79 :: Batch 0/1
Batch Loss = 0.0007310406373930035
79, epoch_train_loss=0.0007310406373930035
Epoch 80
Epoch 80 :: Batch 0/1
Batch Loss = 0.0007310406369063214
80, epoch_train_loss=0.0007310406369063214
Epoch 81
Epoch 81 :: Batch 0/1
Batch Loss = 0.0007310406364660598
81, epoch_train_loss=0.0007310406364660598
Epoch 82
Epoch 82 :: Batch 0/1
Batch Loss = 0.0007310406360677279
82, epoch_train_loss=0.0007310406360677279
Epoch 83
Epoch 83 :: Batch 0/1
Batch Loss = 0.0007310406357072778
83, epoch_train_loss=0.0007310406357072778
Epoch 84
Epoch 84 :: Batch 0/1
Batch Loss = 0.00073104063538106
84, epoch_train_loss=0.00073104063538106
Epoch 85
Epoch 85 :: Batch 0/1
Batch Loss = 0.0007310406350857812
85, epoch_train_loss=0.0007310406350857812
Epoch 86
Epoch 86 :: Batch 0/1
Batch Loss = 0.0007310406348184703
86, epoch_train_loss=0.0007310406348184703
Epoch 87
Epoch 87 :: Batch 0/1
Batch Loss = 0.0007310406345764439
87, epoch_train_loss=0.0007310406345764439
Epoch 88
Epoch 88 :: Batch 0/1
Batch Loss = 0.0007310406343572791
88, epoch_train_loss=0.0007310406343572791
Epoch 89
Epoch 89 :: Batch 0/1
Batch Loss = 0.0007310406341587872
89, epoch_train_loss=0.0007310406341587872
Epoch 90
Epoch 90 :: Batch 0/1
Batch Loss = 0.0007310406339789906
90, epoch_train_loss=0.0007310406339789906
Epoch 91
Epoch 91 :: Batch 0/1
Batch Loss = 0.0007310406338161018
91, epoch_train_loss=0.0007310406338161018
Epoch 92
Epoch 92 :: Batch 0/1
Batch Loss = 0.000731040633668505
92, epoch_train_loss=0.000731040633668505
Epoch 93
Epoch 93 :: Batch 0/1
Batch Loss = 0.0007310406335347397
93, epoch_train_loss=0.0007310406335347397
Epoch 94
Epoch 94 :: Batch 0/1
Batch Loss = 0.000731040633413485
94, epoch_train_loss=0.000731040633413485
Epoch 95
Epoch 95 :: Batch 0/1
Batch Loss = 0.0007310406333035466
95, epoch_train_loss=0.0007310406333035466
Epoch 96
Epoch 96 :: Batch 0/1
Batch Loss = 0.0007310406332038444
96, epoch_train_loss=0.0007310406332038444
Epoch 97
Epoch 97 :: Batch 0/1
Batch Loss = 0.0007310406331134006
97, epoch_train_loss=0.0007310406331134006
Epoch 98
Epoch 98 :: Batch 0/1
Batch Loss = 0.0007310406330313321
98, epoch_train_loss=0.0007310406330313321
Epoch 99
Epoch 99 :: Batch 0/1
Batch Loss = 0.000731040632956839
99, epoch_train_loss=0.000731040632956839
Epoch 100
Epoch 100 :: Batch 0/1
Batch Loss = 0.0007310406328891979
100, epoch_train_loss=0.0007310406328891979
Epoch 101
Epoch 101 :: Batch 0/1
Batch Loss = 0.0007310406328277545
101, epoch_train_loss=0.0007310406328277545
Epoch 102
Epoch 102 :: Batch 0/1
Batch Loss = 0.0007310406327719169
102, epoch_train_loss=0.0007310406327719169
Epoch 103
Epoch 103 :: Batch 0/1
Batch Loss = 0.0007310406327211489
103, epoch_train_loss=0.0007310406327211489
Epoch 104
Epoch 104 :: Batch 0/1
Batch Loss = 0.0007310406326749662
104, epoch_train_loss=0.0007310406326749662
Epoch 105
Epoch 105 :: Batch 0/1
Batch Loss = 0.0007310406326329302
105, epoch_train_loss=0.0007310406326329302
Epoch 106
Epoch 106 :: Batch 0/1
Batch Loss = 0.0007310406325946441
106, epoch_train_loss=0.0007310406325946441
Epoch 107
Epoch 107 :: Batch 0/1
Batch Loss = 0.0007310406325597487
107, epoch_train_loss=0.0007310406325597487
Epoch 108
Epoch 108 :: Batch 0/1
Batch Loss = 0.0007310406325279194
108, epoch_train_loss=0.0007310406325279194
Epoch 109
Epoch 109 :: Batch 0/1
Batch Loss = 0.0007310406324988624
109, epoch_train_loss=0.0007310406324988624
Epoch 110
Epoch 110 :: Batch 0/1
Batch Loss = 0.0007310406324723115
110, epoch_train_loss=0.0007310406324723115
Epoch 111
Epoch 111 :: Batch 0/1
Batch Loss = 0.0007310406324480262
111, epoch_train_loss=0.0007310406324480262
Epoch 112
Epoch 112 :: Batch 0/1
Batch Loss = 0.0007310406324257888
112, epoch_train_loss=0.0007310406324257888
Epoch 113
Epoch 113 :: Batch 0/1
Batch Loss = 0.0007310406324054026
113, epoch_train_loss=0.0007310406324054026
Epoch 114
Epoch 114 :: Batch 0/1
Batch Loss = 0.0007310406323866893
114, epoch_train_loss=0.0007310406323866893
Epoch 115
Epoch 115 :: Batch 0/1
Batch Loss = 0.0007310406323694879
115, epoch_train_loss=0.0007310406323694879
Epoch 116
Epoch 116 :: Batch 0/1
Batch Loss = 0.0007310406323536525
116, epoch_train_loss=0.0007310406323536525
Epoch 117
Epoch 117 :: Batch 0/1
Batch Loss = 0.0007310406323390511
117, epoch_train_loss=0.0007310406323390511
Epoch 118
Epoch 118 :: Batch 0/1
Batch Loss = 0.0007310406323255645
118, epoch_train_loss=0.0007310406323255645
Epoch 119
Epoch 119 :: Batch 0/1
Batch Loss = 0.0007310406323130849
119, epoch_train_loss=0.0007310406323130849
Epoch 120
Epoch 120 :: Batch 0/1
Batch Loss = 0.0007310406323015145
120, epoch_train_loss=0.0007310406323015145
Epoch 121
Epoch 121 :: Batch 0/1
Batch Loss = 0.0007310406322907652
121, epoch_train_loss=0.0007310406322907652
Epoch 122
Epoch 122 :: Batch 0/1
Batch Loss = 0.0007310406322807571
122, epoch_train_loss=0.0007310406322807571
Epoch 123
Epoch 123 :: Batch 0/1
Batch Loss = 0.0007310406322714179
123, epoch_train_loss=0.0007310406322714179
Epoch 124
Epoch 124 :: Batch 0/1
Batch Loss = 0.0007310406322626824
124, epoch_train_loss=0.0007310406322626824
Epoch 125
Epoch 125 :: Batch 0/1
Batch Loss = 0.0007310406322544916
125, epoch_train_loss=0.0007310406322544916
Epoch 126
Epoch 126 :: Batch 0/1
Batch Loss = 0.0007310406322467919
126, epoch_train_loss=0.0007310406322467919
Epoch 127
Epoch 127 :: Batch 0/1
Batch Loss = 0.0007310406322395351
127, epoch_train_loss=0.0007310406322395351
Epoch 128
Epoch 128 :: Batch 0/1
Batch Loss = 0.0007310406322326777
128, epoch_train_loss=0.0007310406322326777
Epoch 129
Epoch 129 :: Batch 0/1
Batch Loss = 0.0007310406322261801
129, epoch_train_loss=0.0007310406322261801
Epoch 130
Epoch 130 :: Batch 0/1
Batch Loss = 0.0007310406322200065
130, epoch_train_loss=0.0007310406322200065
Epoch 131
Epoch 131 :: Batch 0/1
Batch Loss = 0.0007310406322141243
131, epoch_train_loss=0.0007310406322141243
Epoch 132
Epoch 132 :: Batch 0/1
Batch Loss = 0.000731040632208505
132, epoch_train_loss=0.000731040632208505
Epoch 133
Epoch 133 :: Batch 0/1
Batch Loss = 0.0007310406322031221
133, epoch_train_loss=0.0007310406322031221
Epoch 134
Epoch 134 :: Batch 0/1
Batch Loss = 0.0007310406321979515
134, epoch_train_loss=0.0007310406321979515
Epoch 135
Epoch 135 :: Batch 0/1
Batch Loss = 0.0007310406321929718
135, epoch_train_loss=0.0007310406321929718
Epoch 136
Epoch 136 :: Batch 0/1
Batch Loss = 0.0007310406321881633
136, epoch_train_loss=0.0007310406321881633
Epoch 137
Epoch 137 :: Batch 0/1
Batch Loss = 0.0007310406321835086
137, epoch_train_loss=0.0007310406321835086
Epoch 138
Epoch 138 :: Batch 0/1
Batch Loss = 0.0007310406321789916
138, epoch_train_loss=0.0007310406321789916
Epoch 139
Epoch 139 :: Batch 0/1
Batch Loss = 0.0007310406321745982
139, epoch_train_loss=0.0007310406321745982
Epoch 140
Epoch 140 :: Batch 0/1
Batch Loss = 0.0007310406321703154
140, epoch_train_loss=0.0007310406321703154
Epoch 141
Epoch 141 :: Batch 0/1
Batch Loss = 0.0007310406321661311
141, epoch_train_loss=0.0007310406321661311
Epoch 142
Epoch 142 :: Batch 0/1
Batch Loss = 0.0007310406321620349
142, epoch_train_loss=0.0007310406321620349
Epoch 143
Epoch 143 :: Batch 0/1
Batch Loss = 0.0007310406321580172
143, epoch_train_loss=0.0007310406321580172
Epoch 144
Epoch 144 :: Batch 0/1
Batch Loss = 0.0007310406321540694
144, epoch_train_loss=0.0007310406321540694
Epoch 145
Epoch 145 :: Batch 0/1
Batch Loss = 0.0007310406321501835
145, epoch_train_loss=0.0007310406321501835
Epoch 146
Epoch 146 :: Batch 0/1
Batch Loss = 0.0007310406321463526
146, epoch_train_loss=0.0007310406321463526
Epoch 147
Epoch 147 :: Batch 0/1
Batch Loss = 0.0007310406321425702
147, epoch_train_loss=0.0007310406321425702
Epoch 148
Epoch 148 :: Batch 0/1
Batch Loss = 0.0007310406321388306
148, epoch_train_loss=0.0007310406321388306
Epoch 149
Epoch 149 :: Batch 0/1
Batch Loss = 0.0007310406321351285
149, epoch_train_loss=0.0007310406321351285
Epoch 150
Epoch 150 :: Batch 0/1
Batch Loss = 0.0007310406321314593
150, epoch_train_loss=0.0007310406321314593
Epoch 151
Epoch 151 :: Batch 0/1
Batch Loss = 0.0007310406321278187
151, epoch_train_loss=0.0007310406321278187
Epoch 152
Epoch 152 :: Batch 0/1
Batch Loss = 0.0007310406321242029
152, epoch_train_loss=0.0007310406321242029
Epoch 153
Epoch 153 :: Batch 0/1
Batch Loss = 0.0007310406321206083
153, epoch_train_loss=0.0007310406321206083
Epoch 154
Epoch 154 :: Batch 0/1
Batch Loss = 0.000731040632117032
154, epoch_train_loss=0.000731040632117032
Epoch 155
Epoch 155 :: Batch 0/1
Batch Loss = 0.000731040632113471
155, epoch_train_loss=0.000731040632113471
Epoch 156
Epoch 156 :: Batch 0/1
Batch Loss = 0.0007310406321099227
156, epoch_train_loss=0.0007310406321099227
Epoch 157
Epoch 157 :: Batch 0/1
Batch Loss = 0.000731040632106385
157, epoch_train_loss=0.000731040632106385
Epoch 158
Epoch 158 :: Batch 0/1
Batch Loss = 0.0007310406321028557
158, epoch_train_loss=0.0007310406321028557
Epoch 159
Epoch 159 :: Batch 0/1
Batch Loss = 0.0007310406320993329
159, epoch_train_loss=0.0007310406320993329
Epoch 160
Epoch 160 :: Batch 0/1
Batch Loss = 0.0007310406320958151
160, epoch_train_loss=0.0007310406320958151
Epoch 161
Epoch 161 :: Batch 0/1
Batch Loss = 0.0007310406320923006
161, epoch_train_loss=0.0007310406320923006
Epoch 162
Epoch 162 :: Batch 0/1
Batch Loss = 0.0007310406320887879
162, epoch_train_loss=0.0007310406320887879
Epoch 163
Epoch 163 :: Batch 0/1
Batch Loss = 0.0007310406320852762
163, epoch_train_loss=0.0007310406320852762
Epoch 164
Epoch 164 :: Batch 0/1
Batch Loss = 0.0007310406320817638
164, epoch_train_loss=0.0007310406320817638
Epoch 165
Epoch 165 :: Batch 0/1
Batch Loss = 0.0007310406320782503
165, epoch_train_loss=0.0007310406320782503
Epoch 166
Epoch 166 :: Batch 0/1
Batch Loss = 0.0007310406320747343
166, epoch_train_loss=0.0007310406320747343
Epoch 167
Epoch 167 :: Batch 0/1
Batch Loss = 0.0007310406320712153
167, epoch_train_loss=0.0007310406320712153
Epoch 168
Epoch 168 :: Batch 0/1
Batch Loss = 0.0007310406320676923
168, epoch_train_loss=0.0007310406320676923
Epoch 169
Epoch 169 :: Batch 0/1
Batch Loss = 0.000731040632064165
169, epoch_train_loss=0.000731040632064165
Epoch 170
Epoch 170 :: Batch 0/1
Batch Loss = 0.0007310406320606325
170, epoch_train_loss=0.0007310406320606325
Epoch 171
Epoch 171 :: Batch 0/1
Batch Loss = 0.0007310406320570943
171, epoch_train_loss=0.0007310406320570943
Epoch 172
Epoch 172 :: Batch 0/1
Batch Loss = 0.0007310406320535501
172, epoch_train_loss=0.0007310406320535501
Epoch 173
Epoch 173 :: Batch 0/1
Batch Loss = 0.0007310406320499992
173, epoch_train_loss=0.0007310406320499992
Epoch 174
Epoch 174 :: Batch 0/1
Batch Loss = 0.0007310406320464414
174, epoch_train_loss=0.0007310406320464414
Epoch 175
Epoch 175 :: Batch 0/1
Batch Loss = 0.0007310406320428763
175, epoch_train_loss=0.0007310406320428763
Epoch 176
Epoch 176 :: Batch 0/1
Batch Loss = 0.0007310406320393036
176, epoch_train_loss=0.0007310406320393036
Epoch 177
Epoch 177 :: Batch 0/1
Batch Loss = 0.0007310406320357231
177, epoch_train_loss=0.0007310406320357231
Epoch 178
Epoch 178 :: Batch 0/1
Batch Loss = 0.0007310406320321344
178, epoch_train_loss=0.0007310406320321344
Epoch 179
Epoch 179 :: Batch 0/1
Batch Loss = 0.0007310406320285373
179, epoch_train_loss=0.0007310406320285373
Epoch 180
Epoch 180 :: Batch 0/1
Batch Loss = 0.0007310406320249318
180, epoch_train_loss=0.0007310406320249318
Epoch 181
Epoch 181 :: Batch 0/1
Batch Loss = 0.0007310406320213175
181, epoch_train_loss=0.0007310406320213175
Epoch 182
Epoch 182 :: Batch 0/1
Batch Loss = 0.0007310406320176943
182, epoch_train_loss=0.0007310406320176943
Epoch 183
Epoch 183 :: Batch 0/1
Batch Loss = 0.0007310406320140619
183, epoch_train_loss=0.0007310406320140619
Epoch 184
Epoch 184 :: Batch 0/1
Batch Loss = 0.0007310406320104205
184, epoch_train_loss=0.0007310406320104205
Epoch 185
Epoch 185 :: Batch 0/1
Batch Loss = 0.0007310406320067699
185, epoch_train_loss=0.0007310406320067699
Epoch 186
Epoch 186 :: Batch 0/1
Batch Loss = 0.0007310406320031098
186, epoch_train_loss=0.0007310406320031098
Epoch 187
Epoch 187 :: Batch 0/1
Batch Loss = 0.0007310406319994404
187, epoch_train_loss=0.0007310406319994404
Epoch 188
Epoch 188 :: Batch 0/1
Batch Loss = 0.0007310406319957612
188, epoch_train_loss=0.0007310406319957612
Epoch 189
Epoch 189 :: Batch 0/1
Batch Loss = 0.0007310406319920726
189, epoch_train_loss=0.0007310406319920726
Epoch 190
Epoch 190 :: Batch 0/1
Batch Loss = 0.0007310406319883744
190, epoch_train_loss=0.0007310406319883744
Epoch 191
Epoch 191 :: Batch 0/1
Batch Loss = 0.0007310406319846664
191, epoch_train_loss=0.0007310406319846664
Epoch 192
Epoch 192 :: Batch 0/1
Batch Loss = 0.0007310406319809486
192, epoch_train_loss=0.0007310406319809486
Epoch 193
Epoch 193 :: Batch 0/1
Batch Loss = 0.000731040631977221
193, epoch_train_loss=0.000731040631977221
Epoch 194
Epoch 194 :: Batch 0/1
Batch Loss = 0.0007310406319734836
194, epoch_train_loss=0.0007310406319734836
Epoch 195
Epoch 195 :: Batch 0/1
Batch Loss = 0.0007310406319697363
195, epoch_train_loss=0.0007310406319697363
Epoch 196
Epoch 196 :: Batch 0/1
Batch Loss = 0.0007310406319659793
196, epoch_train_loss=0.0007310406319659793
Epoch 197
Epoch 197 :: Batch 0/1
Batch Loss = 0.0007310406319622121
197, epoch_train_loss=0.0007310406319622121
Epoch 198
Epoch 198 :: Batch 0/1
Batch Loss = 0.0007310406319584353
198, epoch_train_loss=0.0007310406319584353
Epoch 199
Epoch 199 :: Batch 0/1
Batch Loss = 0.0007310406319546484
199, epoch_train_loss=0.0007310406319546484
Epoch 200
Epoch 200 :: Batch 0/1
Batch Loss = 0.0007310406319508517
200, epoch_train_loss=0.0007310406319508517
Epoch 201
Epoch 201 :: Batch 0/1
Batch Loss = 0.000731040631947045
201, epoch_train_loss=0.000731040631947045
Epoch 202
Epoch 202 :: Batch 0/1
Batch Loss = 0.0007310406319432285
202, epoch_train_loss=0.0007310406319432285
Epoch 203
Epoch 203 :: Batch 0/1
Batch Loss = 0.0007310406319394018
203, epoch_train_loss=0.0007310406319394018
Epoch 204
Epoch 204 :: Batch 0/1
Batch Loss = 0.0007310406319355655
204, epoch_train_loss=0.0007310406319355655
Epoch 205
Epoch 205 :: Batch 0/1
Batch Loss = 0.0007310406319317189
205, epoch_train_loss=0.0007310406319317189
Epoch 206
Epoch 206 :: Batch 0/1
Batch Loss = 0.0007310406319278625
206, epoch_train_loss=0.0007310406319278625
Epoch 207
Epoch 207 :: Batch 0/1
Batch Loss = 0.0007310406319239962
207, epoch_train_loss=0.0007310406319239962
Epoch 208
Epoch 208 :: Batch 0/1
Batch Loss = 0.0007310406319201201
208, epoch_train_loss=0.0007310406319201201
Epoch 209
Epoch 209 :: Batch 0/1
Batch Loss = 0.0007310406319162339
209, epoch_train_loss=0.0007310406319162339
Epoch 210
Epoch 210 :: Batch 0/1
Batch Loss = 0.000731040631912338
210, epoch_train_loss=0.000731040631912338
Epoch 211
Epoch 211 :: Batch 0/1
Batch Loss = 0.0007310406319084319
211, epoch_train_loss=0.0007310406319084319
Epoch 212
Epoch 212 :: Batch 0/1
Batch Loss = 0.0007310406319045162
212, epoch_train_loss=0.0007310406319045162
Epoch 213
Epoch 213 :: Batch 0/1
Batch Loss = 0.0007310406319005904
213, epoch_train_loss=0.0007310406319005904
Epoch 214
Epoch 214 :: Batch 0/1
Batch Loss = 0.000731040631896655
214, epoch_train_loss=0.000731040631896655
Epoch 215
Epoch 215 :: Batch 0/1
Batch Loss = 0.0007310406318927096
215, epoch_train_loss=0.0007310406318927096
Epoch 216
Epoch 216 :: Batch 0/1
Batch Loss = 0.0007310406318887544
216, epoch_train_loss=0.0007310406318887544
Epoch 217
Epoch 217 :: Batch 0/1
Batch Loss = 0.0007310406318847891
217, epoch_train_loss=0.0007310406318847891
Epoch 218
Epoch 218 :: Batch 0/1
Batch Loss = 0.0007310406318808143
218, epoch_train_loss=0.0007310406318808143
Epoch 219
Epoch 219 :: Batch 0/1
Batch Loss = 0.0007310406318768297
219, epoch_train_loss=0.0007310406318768297
Epoch 220
Epoch 220 :: Batch 0/1
Batch Loss = 0.0007310406318728352
220, epoch_train_loss=0.0007310406318728352
Epoch 221
Epoch 221 :: Batch 0/1
Batch Loss = 0.0007310406318688309
221, epoch_train_loss=0.0007310406318688309
Epoch 222
Epoch 222 :: Batch 0/1
Batch Loss = 0.000731040631864817
222, epoch_train_loss=0.000731040631864817
Epoch 223
Epoch 223 :: Batch 0/1
Batch Loss = 0.0007310406318607931
223, epoch_train_loss=0.0007310406318607931
Epoch 224
Epoch 224 :: Batch 0/1
Batch Loss = 0.0007310406318567598
224, epoch_train_loss=0.0007310406318567598
Epoch 225
Epoch 225 :: Batch 0/1
Batch Loss = 0.0007310406318527166
225, epoch_train_loss=0.0007310406318527166
Epoch 226
Epoch 226 :: Batch 0/1
Batch Loss = 0.0007310406318486637
226, epoch_train_loss=0.0007310406318486637
Epoch 227
Epoch 227 :: Batch 0/1
Batch Loss = 0.0007310406318446011
227, epoch_train_loss=0.0007310406318446011
Epoch 228
Epoch 228 :: Batch 0/1
Batch Loss = 0.000731040631840529
228, epoch_train_loss=0.000731040631840529
Epoch 229
Epoch 229 :: Batch 0/1
Batch Loss = 0.000731040631836447
229, epoch_train_loss=0.000731040631836447
Epoch 230
Epoch 230 :: Batch 0/1
Batch Loss = 0.0007310406318323554
230, epoch_train_loss=0.0007310406318323554
Epoch 231
Epoch 231 :: Batch 0/1
Batch Loss = 0.0007310406318282543
231, epoch_train_loss=0.0007310406318282543
Epoch 232
Epoch 232 :: Batch 0/1
Batch Loss = 0.0007310406318241436
232, epoch_train_loss=0.0007310406318241436
Epoch 233
Epoch 233 :: Batch 0/1
Batch Loss = 0.0007310406318200231
233, epoch_train_loss=0.0007310406318200231
Epoch 234
Epoch 234 :: Batch 0/1
Batch Loss = 0.0007310406318158932
234, epoch_train_loss=0.0007310406318158932
Epoch 235
Epoch 235 :: Batch 0/1
Batch Loss = 0.0007310406318117536
235, epoch_train_loss=0.0007310406318117536
Epoch 236
Epoch 236 :: Batch 0/1
Batch Loss = 0.0007310406318076047
236, epoch_train_loss=0.0007310406318076047
Epoch 237
Epoch 237 :: Batch 0/1
Batch Loss = 0.0007310406318034461
237, epoch_train_loss=0.0007310406318034461
Epoch 238
Epoch 238 :: Batch 0/1
Batch Loss = 0.000731040631799278
238, epoch_train_loss=0.000731040631799278
Epoch 239
Epoch 239 :: Batch 0/1
Batch Loss = 0.0007310406317951005
239, epoch_train_loss=0.0007310406317951005
Epoch 240
Epoch 240 :: Batch 0/1
Batch Loss = 0.0007310406317909133
240, epoch_train_loss=0.0007310406317909133
Epoch 241
Epoch 241 :: Batch 0/1
Batch Loss = 0.0007310406317867168
241, epoch_train_loss=0.0007310406317867168
Epoch 242
Epoch 242 :: Batch 0/1
Batch Loss = 0.0007310406317825106
242, epoch_train_loss=0.0007310406317825106
Epoch 243
Epoch 243 :: Batch 0/1
Batch Loss = 0.000731040631778295
243, epoch_train_loss=0.000731040631778295
Epoch 244
Epoch 244 :: Batch 0/1
Batch Loss = 0.0007310406317740701
244, epoch_train_loss=0.0007310406317740701
Epoch 245
Epoch 245 :: Batch 0/1
Batch Loss = 0.0007310406317698357
245, epoch_train_loss=0.0007310406317698357
Epoch 246
Epoch 246 :: Batch 0/1
Batch Loss = 0.000731040631765592
246, epoch_train_loss=0.000731040631765592
Epoch 247
Epoch 247 :: Batch 0/1
Batch Loss = 0.0007310406317613388
247, epoch_train_loss=0.0007310406317613388
Epoch 248
Epoch 248 :: Batch 0/1
Batch Loss = 0.0007310406317570763
248, epoch_train_loss=0.0007310406317570763
Epoch 249
Epoch 249 :: Batch 0/1
Batch Loss = 0.0007310406317528044
249, epoch_train_loss=0.0007310406317528044
Epoch 250
Epoch 250 :: Batch 0/1
Batch Loss = 0.0007310406317485231
250, epoch_train_loss=0.0007310406317485231
Epoch 251
Epoch 251 :: Batch 0/1
Batch Loss = 0.0007310406317442325
251, epoch_train_loss=0.0007310406317442325
Epoch 252
Epoch 252 :: Batch 0/1
Batch Loss = 0.0007310406317399327
252, epoch_train_loss=0.0007310406317399327
Epoch 253
Epoch 253 :: Batch 0/1
Batch Loss = 0.0007310406317356234
253, epoch_train_loss=0.0007310406317356234
Epoch 254
Epoch 254 :: Batch 0/1
Batch Loss = 0.0007310406317313049
254, epoch_train_loss=0.0007310406317313049
Epoch 255
Epoch 255 :: Batch 0/1
Batch Loss = 0.0007310406317269771
255, epoch_train_loss=0.0007310406317269771
Epoch 256
Epoch 256 :: Batch 0/1
Batch Loss = 0.0007310406317226401
256, epoch_train_loss=0.0007310406317226401
Epoch 257
Epoch 257 :: Batch 0/1
Batch Loss = 0.0007310406317182939
257, epoch_train_loss=0.0007310406317182939
Epoch 258
Epoch 258 :: Batch 0/1
Batch Loss = 0.0007310406317139382
258, epoch_train_loss=0.0007310406317139382
Epoch 259
Epoch 259 :: Batch 0/1
Batch Loss = 0.0007310406317095734
259, epoch_train_loss=0.0007310406317095734
Epoch 260
Epoch 260 :: Batch 0/1
Batch Loss = 0.0007310406317051994
260, epoch_train_loss=0.0007310406317051994
Epoch 261
Epoch 261 :: Batch 0/1
Batch Loss = 0.0007310406317008162
261, epoch_train_loss=0.0007310406317008162
Epoch 262
Epoch 262 :: Batch 0/1
Batch Loss = 0.000731040631696424
262, epoch_train_loss=0.000731040631696424
Epoch 263
Epoch 263 :: Batch 0/1
Batch Loss = 0.0007310406316920221
263, epoch_train_loss=0.0007310406316920221
Epoch 264
Epoch 264 :: Batch 0/1
Batch Loss = 0.0007310406316876115
264, epoch_train_loss=0.0007310406316876115
Epoch 265
Epoch 265 :: Batch 0/1
Batch Loss = 0.0007310406316831917
265, epoch_train_loss=0.0007310406316831917
Epoch 266
Epoch 266 :: Batch 0/1
Batch Loss = 0.0007310406316787626
266, epoch_train_loss=0.0007310406316787626
Epoch 267
Epoch 267 :: Batch 0/1
Batch Loss = 0.0007310406316743245
267, epoch_train_loss=0.0007310406316743245
Epoch 268
Epoch 268 :: Batch 0/1
Batch Loss = 0.0007310406316698771
268, epoch_train_loss=0.0007310406316698771
Epoch 269
Epoch 269 :: Batch 0/1
Batch Loss = 0.0007310406316654209
269, epoch_train_loss=0.0007310406316654209
Epoch 270
Epoch 270 :: Batch 0/1
Batch Loss = 0.0007310406316609555
270, epoch_train_loss=0.0007310406316609555
Epoch 271
Epoch 271 :: Batch 0/1
Batch Loss = 0.0007310406316564809
271, epoch_train_loss=0.0007310406316564809
Epoch 272
Epoch 272 :: Batch 0/1
Batch Loss = 0.0007310406316519973
272, epoch_train_loss=0.0007310406316519973
Epoch 273
Epoch 273 :: Batch 0/1
Batch Loss = 0.0007310406316475048
273, epoch_train_loss=0.0007310406316475048
Epoch 274
Epoch 274 :: Batch 0/1
Batch Loss = 0.0007310406316430031
274, epoch_train_loss=0.0007310406316430031
Epoch 275
Epoch 275 :: Batch 0/1
Batch Loss = 0.0007310406316384925
275, epoch_train_loss=0.0007310406316384925
Epoch 276
Epoch 276 :: Batch 0/1
Batch Loss = 0.0007310406316339726
276, epoch_train_loss=0.0007310406316339726
Epoch 277
Epoch 277 :: Batch 0/1
Batch Loss = 0.0007310406316294439
277, epoch_train_loss=0.0007310406316294439
Epoch 278
Epoch 278 :: Batch 0/1
Batch Loss = 0.0007310406316249062
278, epoch_train_loss=0.0007310406316249062
Epoch 279
Epoch 279 :: Batch 0/1
Batch Loss = 0.0007310406316203595
279, epoch_train_loss=0.0007310406316203595
Epoch 280
Epoch 280 :: Batch 0/1
Batch Loss = 0.000731040631615804
280, epoch_train_loss=0.000731040631615804
Epoch 281
Epoch 281 :: Batch 0/1
Batch Loss = 0.0007310406316112393
281, epoch_train_loss=0.0007310406316112393
Epoch 282
Epoch 282 :: Batch 0/1
Batch Loss = 0.0007310406316066657
282, epoch_train_loss=0.0007310406316066657
Epoch 283
Epoch 283 :: Batch 0/1
Batch Loss = 0.0007310406316020831
283, epoch_train_loss=0.0007310406316020831
Epoch 284
Epoch 284 :: Batch 0/1
Batch Loss = 0.0007310406315974918
284, epoch_train_loss=0.0007310406315974918
Epoch 285
Epoch 285 :: Batch 0/1
Batch Loss = 0.0007310406315928915
285, epoch_train_loss=0.0007310406315928915
Epoch 286
Epoch 286 :: Batch 0/1
Batch Loss = 0.0007310406315882823
286, epoch_train_loss=0.0007310406315882823
Epoch 287
Epoch 287 :: Batch 0/1
Batch Loss = 0.0007310406315836642
287, epoch_train_loss=0.0007310406315836642
Epoch 288
Epoch 288 :: Batch 0/1
Batch Loss = 0.0007310406315790372
288, epoch_train_loss=0.0007310406315790372
Epoch 289
Epoch 289 :: Batch 0/1
Batch Loss = 0.0007310406315744013
289, epoch_train_loss=0.0007310406315744013
Epoch 290
Epoch 290 :: Batch 0/1
Batch Loss = 0.0007310406315697566
290, epoch_train_loss=0.0007310406315697566
Epoch 291
Epoch 291 :: Batch 0/1
Batch Loss = 0.000731040631565103
291, epoch_train_loss=0.000731040631565103
Epoch 292
Epoch 292 :: Batch 0/1
Batch Loss = 0.0007310406315604406
292, epoch_train_loss=0.0007310406315604406
Epoch 293
Epoch 293 :: Batch 0/1
Batch Loss = 0.0007310406315557693
293, epoch_train_loss=0.0007310406315557693
Epoch 294
Epoch 294 :: Batch 0/1
Batch Loss = 0.0007310406315510894
294, epoch_train_loss=0.0007310406315510894
Epoch 295
Epoch 295 :: Batch 0/1
Batch Loss = 0.0007310406315464006
295, epoch_train_loss=0.0007310406315464006
Epoch 296
Epoch 296 :: Batch 0/1
Batch Loss = 0.000731040631541703
296, epoch_train_loss=0.000731040631541703
Epoch 297
Epoch 297 :: Batch 0/1
Batch Loss = 0.0007310406315369966
297, epoch_train_loss=0.0007310406315369966
Epoch 298
Epoch 298 :: Batch 0/1
Batch Loss = 0.0007310406315322813
298, epoch_train_loss=0.0007310406315322813
Epoch 299
Epoch 299 :: Batch 0/1
Batch Loss = 0.0007310406315275575
299, epoch_train_loss=0.0007310406315275575
Epoch 300
Epoch 300 :: Batch 0/1
Batch Loss = 0.0007310406315228249
300, epoch_train_loss=0.0007310406315228249
Epoch 301
Epoch 301 :: Batch 0/1
Batch Loss = 0.0007310406315180835
301, epoch_train_loss=0.0007310406315180835
Epoch 302
Epoch 302 :: Batch 0/1
Batch Loss = 0.0007310406315133331
302, epoch_train_loss=0.0007310406315133331
Epoch 303
Epoch 303 :: Batch 0/1
Batch Loss = 0.0007310406315085742
303, epoch_train_loss=0.0007310406315085742
Epoch 304
Epoch 304 :: Batch 0/1
Batch Loss = 0.000731040631503807
304, epoch_train_loss=0.000731040631503807
Epoch 305
Epoch 305 :: Batch 0/1
Batch Loss = 0.0007310406314990306
305, epoch_train_loss=0.0007310406314990306
Epoch 306
Epoch 306 :: Batch 0/1
Batch Loss = 0.0007310406314942456
306, epoch_train_loss=0.0007310406314942456
Epoch 307
Epoch 307 :: Batch 0/1
Batch Loss = 0.000731040631489452
307, epoch_train_loss=0.000731040631489452
Epoch 308
Epoch 308 :: Batch 0/1
Batch Loss = 0.0007310406314846497
308, epoch_train_loss=0.0007310406314846497
Epoch 309
Epoch 309 :: Batch 0/1
Batch Loss = 0.0007310406314798388
309, epoch_train_loss=0.0007310406314798388
Epoch 310
Epoch 310 :: Batch 0/1
Batch Loss = 0.0007310406314750192
310, epoch_train_loss=0.0007310406314750192
Epoch 311
Epoch 311 :: Batch 0/1
Batch Loss = 0.0007310406314701909
311, epoch_train_loss=0.0007310406314701909
Epoch 312
Epoch 312 :: Batch 0/1
Batch Loss = 0.0007310406314653539
312, epoch_train_loss=0.0007310406314653539
Epoch 313
Epoch 313 :: Batch 0/1
Batch Loss = 0.0007310406314605084
313, epoch_train_loss=0.0007310406314605084
Epoch 314
Epoch 314 :: Batch 0/1
Batch Loss = 0.0007310406314556544
314, epoch_train_loss=0.0007310406314556544
Epoch 315
Epoch 315 :: Batch 0/1
Batch Loss = 0.0007310406314507917
315, epoch_train_loss=0.0007310406314507917
Epoch 316
Epoch 316 :: Batch 0/1
Batch Loss = 0.0007310406314459203
316, epoch_train_loss=0.0007310406314459203
Epoch 317
Epoch 317 :: Batch 0/1
Batch Loss = 0.0007310406314410405
317, epoch_train_loss=0.0007310406314410405
Epoch 318
Epoch 318 :: Batch 0/1
Batch Loss = 0.0007310406314361519
318, epoch_train_loss=0.0007310406314361519
Epoch 319
Epoch 319 :: Batch 0/1
Batch Loss = 0.000731040631431255
319, epoch_train_loss=0.000731040631431255
Epoch 320
Epoch 320 :: Batch 0/1
Batch Loss = 0.0007310406314263493
320, epoch_train_loss=0.0007310406314263493
Epoch 321
Epoch 321 :: Batch 0/1
Batch Loss = 0.0007310406314214353
321, epoch_train_loss=0.0007310406314214353
Epoch 322
Epoch 322 :: Batch 0/1
Batch Loss = 0.0007310406314165125
322, epoch_train_loss=0.0007310406314165125
Epoch 323
Epoch 323 :: Batch 0/1
Batch Loss = 0.0007310406314115814
323, epoch_train_loss=0.0007310406314115814
Epoch 324
Epoch 324 :: Batch 0/1
Batch Loss = 0.0007310406314066417
324, epoch_train_loss=0.0007310406314066417
Epoch 325
Epoch 325 :: Batch 0/1
Batch Loss = 0.0007310406314016935
325, epoch_train_loss=0.0007310406314016935
Epoch 326
Epoch 326 :: Batch 0/1
Batch Loss = 0.0007310406313967367
326, epoch_train_loss=0.0007310406313967367
Epoch 327
Epoch 327 :: Batch 0/1
Batch Loss = 0.0007310406313917716
327, epoch_train_loss=0.0007310406313917716
Epoch 328
Epoch 328 :: Batch 0/1
Batch Loss = 0.0007310406313867981
328, epoch_train_loss=0.0007310406313867981
Epoch 329
Epoch 329 :: Batch 0/1
Batch Loss = 0.0007310406313818158
329, epoch_train_loss=0.0007310406313818158
Epoch 330
Epoch 330 :: Batch 0/1
Batch Loss = 0.0007310406313768251
330, epoch_train_loss=0.0007310406313768251
Epoch 331
Epoch 331 :: Batch 0/1
Batch Loss = 0.000731040631371826
331, epoch_train_loss=0.000731040631371826
Epoch 332
Epoch 332 :: Batch 0/1
Batch Loss = 0.0007310406313668186
332, epoch_train_loss=0.0007310406313668186
Epoch 333
Epoch 333 :: Batch 0/1
Batch Loss = 0.0007310406313618026
333, epoch_train_loss=0.0007310406313618026
Epoch 334
Epoch 334 :: Batch 0/1
Batch Loss = 0.0007310406313567781
334, epoch_train_loss=0.0007310406313567781
Epoch 335
Epoch 335 :: Batch 0/1
Batch Loss = 0.0007310406313517454
335, epoch_train_loss=0.0007310406313517454
Epoch 336
Epoch 336 :: Batch 0/1
Batch Loss = 0.0007310406313467042
336, epoch_train_loss=0.0007310406313467042
Epoch 337
Epoch 337 :: Batch 0/1
Batch Loss = 0.0007310406313416545
337, epoch_train_loss=0.0007310406313416545
Epoch 338
Epoch 338 :: Batch 0/1
Batch Loss = 0.0007310406313365966
338, epoch_train_loss=0.0007310406313365966
Epoch 339
Epoch 339 :: Batch 0/1
Batch Loss = 0.00073104063133153
339, epoch_train_loss=0.00073104063133153
Epoch 340
Epoch 340 :: Batch 0/1
Batch Loss = 0.0007310406313264553
340, epoch_train_loss=0.0007310406313264553
Epoch 341
Epoch 341 :: Batch 0/1
Batch Loss = 0.0007310406313213721
341, epoch_train_loss=0.0007310406313213721
Epoch 342
Epoch 342 :: Batch 0/1
Batch Loss = 0.0007310406313162806
342, epoch_train_loss=0.0007310406313162806
Epoch 343
Epoch 343 :: Batch 0/1
Batch Loss = 0.0007310406313111807
343, epoch_train_loss=0.0007310406313111807
Epoch 344
Epoch 344 :: Batch 0/1
Batch Loss = 0.0007310406313060725
344, epoch_train_loss=0.0007310406313060725
Epoch 345
Epoch 345 :: Batch 0/1
Batch Loss = 0.0007310406313009559
345, epoch_train_loss=0.0007310406313009559
Epoch 346
Epoch 346 :: Batch 0/1
Batch Loss = 0.0007310406312958311
346, epoch_train_loss=0.0007310406312958311
Epoch 347
Epoch 347 :: Batch 0/1
Batch Loss = 0.0007310406312906977
347, epoch_train_loss=0.0007310406312906977
Epoch 348
Epoch 348 :: Batch 0/1
Batch Loss = 0.0007310406312855563
348, epoch_train_loss=0.0007310406312855563
Epoch 349
Epoch 349 :: Batch 0/1
Batch Loss = 0.0007310406312804064
349, epoch_train_loss=0.0007310406312804064
Epoch 350
Epoch 350 :: Batch 0/1
Batch Loss = 0.0007310406312752483
350, epoch_train_loss=0.0007310406312752483
Epoch 351
Epoch 351 :: Batch 0/1
Batch Loss = 0.0007310406312700819
351, epoch_train_loss=0.0007310406312700819
Epoch 352
Epoch 352 :: Batch 0/1
Batch Loss = 0.0007310406312649072
352, epoch_train_loss=0.0007310406312649072
Epoch 353
Epoch 353 :: Batch 0/1
Batch Loss = 0.0007310406312597241
353, epoch_train_loss=0.0007310406312597241
Epoch 354
Epoch 354 :: Batch 0/1
Batch Loss = 0.0007310406312545329
354, epoch_train_loss=0.0007310406312545329
Epoch 355
Epoch 355 :: Batch 0/1
Batch Loss = 0.0007310406312493334
355, epoch_train_loss=0.0007310406312493334
Epoch 356
Epoch 356 :: Batch 0/1
Batch Loss = 0.0007310406312441256
356, epoch_train_loss=0.0007310406312441256
Epoch 357
Epoch 357 :: Batch 0/1
Batch Loss = 0.0007310406312389097
357, epoch_train_loss=0.0007310406312389097
Epoch 358
Epoch 358 :: Batch 0/1
Batch Loss = 0.0007310406312336854
358, epoch_train_loss=0.0007310406312336854
Epoch 359
Epoch 359 :: Batch 0/1
Batch Loss = 0.0007310406312284529
359, epoch_train_loss=0.0007310406312284529
Epoch 360
Epoch 360 :: Batch 0/1
Batch Loss = 0.0007310406312232122
360, epoch_train_loss=0.0007310406312232122
Epoch 361
Epoch 361 :: Batch 0/1
Batch Loss = 0.0007310406312179634
361, epoch_train_loss=0.0007310406312179634
Epoch 362
Epoch 362 :: Batch 0/1
Batch Loss = 0.0007310406312127062
362, epoch_train_loss=0.0007310406312127062
Epoch 363
Epoch 363 :: Batch 0/1
Batch Loss = 0.0007310406312074409
363, epoch_train_loss=0.0007310406312074409
Epoch 364
Epoch 364 :: Batch 0/1
Batch Loss = 0.0007310406312021673
364, epoch_train_loss=0.0007310406312021673
Epoch 365
Epoch 365 :: Batch 0/1
Batch Loss = 0.0007310406311968857
365, epoch_train_loss=0.0007310406311968857
Epoch 366
Epoch 366 :: Batch 0/1
Batch Loss = 0.0007310406311915959
366, epoch_train_loss=0.0007310406311915959
Epoch 367
Epoch 367 :: Batch 0/1
Batch Loss = 0.0007310406311862979
367, epoch_train_loss=0.0007310406311862979
Epoch 368
Epoch 368 :: Batch 0/1
Batch Loss = 0.0007310406311809917
368, epoch_train_loss=0.0007310406311809917
Epoch 369
Epoch 369 :: Batch 0/1
Batch Loss = 0.0007310406311756772
369, epoch_train_loss=0.0007310406311756772
Epoch 370
Epoch 370 :: Batch 0/1
Batch Loss = 0.0007310406311703548
370, epoch_train_loss=0.0007310406311703548
Epoch 371
Epoch 371 :: Batch 0/1
Batch Loss = 0.0007310406311650241
371, epoch_train_loss=0.0007310406311650241
Epoch 372
Epoch 372 :: Batch 0/1
Batch Loss = 0.0007310406311596852
372, epoch_train_loss=0.0007310406311596852
Epoch 373
Epoch 373 :: Batch 0/1
Batch Loss = 0.0007310406311543384
373, epoch_train_loss=0.0007310406311543384
Epoch 374
Epoch 374 :: Batch 0/1
Batch Loss = 0.0007310406311489834
374, epoch_train_loss=0.0007310406311489834
Epoch 375
Epoch 375 :: Batch 0/1
Batch Loss = 0.0007310406311436202
375, epoch_train_loss=0.0007310406311436202
Epoch 376
Epoch 376 :: Batch 0/1
Batch Loss = 0.0007310406311382489
376, epoch_train_loss=0.0007310406311382489
Epoch 377
Epoch 377 :: Batch 0/1
Batch Loss = 0.0007310406311328696
377, epoch_train_loss=0.0007310406311328696
Epoch 378
Epoch 378 :: Batch 0/1
Batch Loss = 0.0007310406311274822
378, epoch_train_loss=0.0007310406311274822
Epoch 379
Epoch 379 :: Batch 0/1
Batch Loss = 0.0007310406311220866
379, epoch_train_loss=0.0007310406311220866
Epoch 380
Epoch 380 :: Batch 0/1
Batch Loss = 0.0007310406311166831
380, epoch_train_loss=0.0007310406311166831
Epoch 381
Epoch 381 :: Batch 0/1
Batch Loss = 0.0007310406311112715
381, epoch_train_loss=0.0007310406311112715
Epoch 382
Epoch 382 :: Batch 0/1
Batch Loss = 0.0007310406311058517
382, epoch_train_loss=0.0007310406311058517
Epoch 383
Epoch 383 :: Batch 0/1
Batch Loss = 0.000731040631100424
383, epoch_train_loss=0.000731040631100424
Epoch 384
Epoch 384 :: Batch 0/1
Batch Loss = 0.0007310406310949881
384, epoch_train_loss=0.0007310406310949881
Epoch 385
Epoch 385 :: Batch 0/1
Batch Loss = 0.0007310406310895441
385, epoch_train_loss=0.0007310406310895441
Epoch 386
Epoch 386 :: Batch 0/1
Batch Loss = 0.0007310406310840923
386, epoch_train_loss=0.0007310406310840923
Epoch 387
Epoch 387 :: Batch 0/1
Batch Loss = 0.0007310406310786322
387, epoch_train_loss=0.0007310406310786322
Epoch 388
Epoch 388 :: Batch 0/1
Batch Loss = 0.0007310406310731643
388, epoch_train_loss=0.0007310406310731643
Epoch 389
Epoch 389 :: Batch 0/1
Batch Loss = 0.0007310406310676883
389, epoch_train_loss=0.0007310406310676883
Epoch 390
Epoch 390 :: Batch 0/1
Batch Loss = 0.0007310406310622042
390, epoch_train_loss=0.0007310406310622042
Epoch 391
Epoch 391 :: Batch 0/1
Batch Loss = 0.0007310406310567123
391, epoch_train_loss=0.0007310406310567123
Epoch 392
Epoch 392 :: Batch 0/1
Batch Loss = 0.000731040631051212
392, epoch_train_loss=0.000731040631051212
Epoch 393
Epoch 393 :: Batch 0/1
Batch Loss = 0.0007310406310457042
393, epoch_train_loss=0.0007310406310457042
Epoch 394
Epoch 394 :: Batch 0/1
Batch Loss = 0.0007310406310401881
394, epoch_train_loss=0.0007310406310401881
Epoch 395
Epoch 395 :: Batch 0/1
Batch Loss = 0.0007310406310346642
395, epoch_train_loss=0.0007310406310346642
Epoch 396
Epoch 396 :: Batch 0/1
Batch Loss = 0.0007310406310291322
396, epoch_train_loss=0.0007310406310291322
Epoch 397
Epoch 397 :: Batch 0/1
Batch Loss = 0.0007310406310235922
397, epoch_train_loss=0.0007310406310235922
Epoch 398
Epoch 398 :: Batch 0/1
Batch Loss = 0.0007310406310180444
398, epoch_train_loss=0.0007310406310180444
Epoch 399
Epoch 399 :: Batch 0/1
Batch Loss = 0.0007310406310124886
399, epoch_train_loss=0.0007310406310124886
Epoch 400
Epoch 400 :: Batch 0/1
Batch Loss = 0.0007310406310069248
400, epoch_train_loss=0.0007310406310069248
Epoch 401
Epoch 401 :: Batch 0/1
Batch Loss = 0.0007310406310013529
401, epoch_train_loss=0.0007310406310013529
Epoch 402
Epoch 402 :: Batch 0/1
Batch Loss = 0.0007310406309957733
402, epoch_train_loss=0.0007310406309957733
Epoch 403
Epoch 403 :: Batch 0/1
Batch Loss = 0.0007310406309901858
403, epoch_train_loss=0.0007310406309901858
Epoch 404
Epoch 404 :: Batch 0/1
Batch Loss = 0.0007310406309845903
404, epoch_train_loss=0.0007310406309845903
Epoch 405
Epoch 405 :: Batch 0/1
Batch Loss = 0.0007310406309789868
405, epoch_train_loss=0.0007310406309789868
Epoch 406
Epoch 406 :: Batch 0/1
Batch Loss = 0.0007310406309733755
406, epoch_train_loss=0.0007310406309733755
Epoch 407
Epoch 407 :: Batch 0/1
Batch Loss = 0.0007310406309677561
407, epoch_train_loss=0.0007310406309677561
Epoch 408
Epoch 408 :: Batch 0/1
Batch Loss = 0.000731040630962129
408, epoch_train_loss=0.000731040630962129
Epoch 409
Epoch 409 :: Batch 0/1
Batch Loss = 0.0007310406309564939
409, epoch_train_loss=0.0007310406309564939
Epoch 410
Epoch 410 :: Batch 0/1
Batch Loss = 0.0007310406309508509
410, epoch_train_loss=0.0007310406309508509
Epoch 411
Epoch 411 :: Batch 0/1
Batch Loss = 0.0007310406309452002
411, epoch_train_loss=0.0007310406309452002
Epoch 412
Epoch 412 :: Batch 0/1
Batch Loss = 0.0007310406309395414
412, epoch_train_loss=0.0007310406309395414
Epoch 413
Epoch 413 :: Batch 0/1
Batch Loss = 0.0007310406309338748
413, epoch_train_loss=0.0007310406309338748
Epoch 414
Epoch 414 :: Batch 0/1
Batch Loss = 0.0007310406309282004
414, epoch_train_loss=0.0007310406309282004
Epoch 415
Epoch 415 :: Batch 0/1
Batch Loss = 0.0007310406309225182
415, epoch_train_loss=0.0007310406309225182
Epoch 416
Epoch 416 :: Batch 0/1
Batch Loss = 0.0007310406309168281
416, epoch_train_loss=0.0007310406309168281
Epoch 417
Epoch 417 :: Batch 0/1
Batch Loss = 0.00073104063091113
417, epoch_train_loss=0.00073104063091113
Epoch 418
Epoch 418 :: Batch 0/1
Batch Loss = 0.0007310406309054241
418, epoch_train_loss=0.0007310406309054241
Epoch 419
Epoch 419 :: Batch 0/1
Batch Loss = 0.0007310406308997105
419, epoch_train_loss=0.0007310406308997105
Epoch 420
Epoch 420 :: Batch 0/1
Batch Loss = 0.0007310406308939889
420, epoch_train_loss=0.0007310406308939889
Epoch 421
Epoch 421 :: Batch 0/1
Batch Loss = 0.0007310406308882596
421, epoch_train_loss=0.0007310406308882596
Epoch 422
Epoch 422 :: Batch 0/1
Batch Loss = 0.0007310406308825225
422, epoch_train_loss=0.0007310406308825225
Epoch 423
Epoch 423 :: Batch 0/1
Batch Loss = 0.0007310406308767774
423, epoch_train_loss=0.0007310406308767774
Epoch 424
Epoch 424 :: Batch 0/1
Batch Loss = 0.0007310406308710247
424, epoch_train_loss=0.0007310406308710247
Epoch 425
Epoch 425 :: Batch 0/1
Batch Loss = 0.0007310406308652641
425, epoch_train_loss=0.0007310406308652641
Epoch 426
Epoch 426 :: Batch 0/1
Batch Loss = 0.0007310406308594957
426, epoch_train_loss=0.0007310406308594957
Epoch 427
Epoch 427 :: Batch 0/1
Batch Loss = 0.0007310406308537196
427, epoch_train_loss=0.0007310406308537196
Epoch 428
Epoch 428 :: Batch 0/1
Batch Loss = 0.0007310406308479356
428, epoch_train_loss=0.0007310406308479356
Epoch 429
Epoch 429 :: Batch 0/1
Batch Loss = 0.0007310406308421439
429, epoch_train_loss=0.0007310406308421439
Epoch 430
Epoch 430 :: Batch 0/1
Batch Loss = 0.0007310406308363444
430, epoch_train_loss=0.0007310406308363444
Epoch 431
Epoch 431 :: Batch 0/1
Batch Loss = 0.0007310406308305371
431, epoch_train_loss=0.0007310406308305371
Epoch 432
Epoch 432 :: Batch 0/1
Batch Loss = 0.0007310406308247221
432, epoch_train_loss=0.0007310406308247221
Epoch 433
Epoch 433 :: Batch 0/1
Batch Loss = 0.0007310406308188993
433, epoch_train_loss=0.0007310406308188993
Epoch 434
Epoch 434 :: Batch 0/1
Batch Loss = 0.0007310406308130686
434, epoch_train_loss=0.0007310406308130686
Epoch 435
Epoch 435 :: Batch 0/1
Batch Loss = 0.0007310406308072303
435, epoch_train_loss=0.0007310406308072303
Epoch 436
Epoch 436 :: Batch 0/1
Batch Loss = 0.0007310406308013843
436, epoch_train_loss=0.0007310406308013843
Epoch 437
Epoch 437 :: Batch 0/1
Batch Loss = 0.0007310406307955306
437, epoch_train_loss=0.0007310406307955306
Epoch 438
Epoch 438 :: Batch 0/1
Batch Loss = 0.0007310406307896691
438, epoch_train_loss=0.0007310406307896691
Epoch 439
Epoch 439 :: Batch 0/1
Batch Loss = 0.0007310406307837997
439, epoch_train_loss=0.0007310406307837997
Epoch 440
Epoch 440 :: Batch 0/1
Batch Loss = 0.0007310406307779227
440, epoch_train_loss=0.0007310406307779227
Epoch 441
Epoch 441 :: Batch 0/1
Batch Loss = 0.0007310406307720381
441, epoch_train_loss=0.0007310406307720381
Epoch 442
Epoch 442 :: Batch 0/1
Batch Loss = 0.0007310406307661458
442, epoch_train_loss=0.0007310406307661458
Epoch 443
Epoch 443 :: Batch 0/1
Batch Loss = 0.0007310406307602457
443, epoch_train_loss=0.0007310406307602457
Epoch 444
Epoch 444 :: Batch 0/1
Batch Loss = 0.0007310406307543378
444, epoch_train_loss=0.0007310406307543378
Epoch 445
Epoch 445 :: Batch 0/1
Batch Loss = 0.0007310406307484224
445, epoch_train_loss=0.0007310406307484224
Epoch 446
Epoch 446 :: Batch 0/1
Batch Loss = 0.0007310406307424991
446, epoch_train_loss=0.0007310406307424991
Epoch 447
Epoch 447 :: Batch 0/1
Batch Loss = 0.0007310406307365682
447, epoch_train_loss=0.0007310406307365682
Epoch 448
Epoch 448 :: Batch 0/1
Batch Loss = 0.0007310406307306297
448, epoch_train_loss=0.0007310406307306297
Epoch 449
Epoch 449 :: Batch 0/1
Batch Loss = 0.0007310406307246835
449, epoch_train_loss=0.0007310406307246835
Epoch 450
Epoch 450 :: Batch 0/1
Batch Loss = 0.0007310406307187295
450, epoch_train_loss=0.0007310406307187295
Epoch 451
Epoch 451 :: Batch 0/1
Batch Loss = 0.000731040630712768
451, epoch_train_loss=0.000731040630712768
Epoch 452
Epoch 452 :: Batch 0/1
Batch Loss = 0.0007310406307067987
452, epoch_train_loss=0.0007310406307067987
Epoch 453
Epoch 453 :: Batch 0/1
Batch Loss = 0.0007310406307008217
453, epoch_train_loss=0.0007310406307008217
Epoch 454
Epoch 454 :: Batch 0/1
Batch Loss = 0.0007310406306948373
454, epoch_train_loss=0.0007310406306948373
Epoch 455
Epoch 455 :: Batch 0/1
Batch Loss = 0.0007310406306888452
455, epoch_train_loss=0.0007310406306888452
Epoch 456
Epoch 456 :: Batch 0/1
Batch Loss = 0.0007310406306828453
456, epoch_train_loss=0.0007310406306828453
Epoch 457
Epoch 457 :: Batch 0/1
Batch Loss = 0.000731040630676838
457, epoch_train_loss=0.000731040630676838
Epoch 458
Epoch 458 :: Batch 0/1
Batch Loss = 0.0007310406306708227
458, epoch_train_loss=0.0007310406306708227
Epoch 459
Epoch 459 :: Batch 0/1
Batch Loss = 0.0007310406306648
459, epoch_train_loss=0.0007310406306648
Epoch 460
Epoch 460 :: Batch 0/1
Batch Loss = 0.0007310406306587695
460, epoch_train_loss=0.0007310406306587695
Epoch 461
Epoch 461 :: Batch 0/1
Batch Loss = 0.0007310406306527317
461, epoch_train_loss=0.0007310406306527317
Epoch 462
Epoch 462 :: Batch 0/1
Batch Loss = 0.0007310406306466861
462, epoch_train_loss=0.0007310406306466861
Epoch 463
Epoch 463 :: Batch 0/1
Batch Loss = 0.000731040630640633
463, epoch_train_loss=0.000731040630640633
Epoch 464
Epoch 464 :: Batch 0/1
Batch Loss = 0.000731040630634572
464, epoch_train_loss=0.000731040630634572
Epoch 465
Epoch 465 :: Batch 0/1
Batch Loss = 0.0007310406306285036
465, epoch_train_loss=0.0007310406306285036
Epoch 466
Epoch 466 :: Batch 0/1
Batch Loss = 0.0007310406306224277
466, epoch_train_loss=0.0007310406306224277
Epoch 467
Epoch 467 :: Batch 0/1
Batch Loss = 0.000731040630616344
467, epoch_train_loss=0.000731040630616344
Epoch 468
Epoch 468 :: Batch 0/1
Batch Loss = 0.0007310406306102529
468, epoch_train_loss=0.0007310406306102529
Epoch 469
Epoch 469 :: Batch 0/1
Batch Loss = 0.0007310406306041544
469, epoch_train_loss=0.0007310406306041544
Epoch 470
Epoch 470 :: Batch 0/1
Batch Loss = 0.0007310406305980478
470, epoch_train_loss=0.0007310406305980478
Epoch 471
Epoch 471 :: Batch 0/1
Batch Loss = 0.0007310406305919341
471, epoch_train_loss=0.0007310406305919341
Epoch 472
Epoch 472 :: Batch 0/1
Batch Loss = 0.0007310406305858126
472, epoch_train_loss=0.0007310406305858126
Epoch 473
Epoch 473 :: Batch 0/1
Batch Loss = 0.0007310406305796836
473, epoch_train_loss=0.0007310406305796836
Epoch 474
Epoch 474 :: Batch 0/1
Batch Loss = 0.0007310406305735469
474, epoch_train_loss=0.0007310406305735469
Epoch 475
Epoch 475 :: Batch 0/1
Batch Loss = 0.0007310406305674027
475, epoch_train_loss=0.0007310406305674027
Epoch 476
Epoch 476 :: Batch 0/1
Batch Loss = 0.0007310406305612512
476, epoch_train_loss=0.0007310406305612512
Epoch 477
Epoch 477 :: Batch 0/1
Batch Loss = 0.0007310406305550919
477, epoch_train_loss=0.0007310406305550919
Epoch 478
Epoch 478 :: Batch 0/1
Batch Loss = 0.0007310406305489252
478, epoch_train_loss=0.0007310406305489252
Epoch 479
Epoch 479 :: Batch 0/1
Batch Loss = 0.0007310406305427509
479, epoch_train_loss=0.0007310406305427509
Epoch 480
Epoch 480 :: Batch 0/1
Batch Loss = 0.000731040630536569
480, epoch_train_loss=0.000731040630536569
Epoch 481
Epoch 481 :: Batch 0/1
Batch Loss = 0.0007310406305303797
481, epoch_train_loss=0.0007310406305303797
Epoch 482
Epoch 482 :: Batch 0/1
Batch Loss = 0.0007310406305241829
482, epoch_train_loss=0.0007310406305241829
Epoch 483
Epoch 483 :: Batch 0/1
Batch Loss = 0.0007310406305179785
483, epoch_train_loss=0.0007310406305179785
Epoch 484
Epoch 484 :: Batch 0/1
Batch Loss = 0.0007310406305117666
484, epoch_train_loss=0.0007310406305117666
Epoch 485
Epoch 485 :: Batch 0/1
Batch Loss = 0.0007310406305055472
485, epoch_train_loss=0.0007310406305055472
Epoch 486
Epoch 486 :: Batch 0/1
Batch Loss = 0.0007310406304993201
486, epoch_train_loss=0.0007310406304993201
Epoch 487
Epoch 487 :: Batch 0/1
Batch Loss = 0.0007310406304930857
487, epoch_train_loss=0.0007310406304930857
Epoch 488
Epoch 488 :: Batch 0/1
Batch Loss = 0.0007310406304868438
488, epoch_train_loss=0.0007310406304868438
Epoch 489
Epoch 489 :: Batch 0/1
Batch Loss = 0.0007310406304805945
489, epoch_train_loss=0.0007310406304805945
Epoch 490
Epoch 490 :: Batch 0/1
Batch Loss = 0.0007310406304743376
490, epoch_train_loss=0.0007310406304743376
Epoch 491
Epoch 491 :: Batch 0/1
Batch Loss = 0.0007310406304680731
491, epoch_train_loss=0.0007310406304680731
Epoch 492
Epoch 492 :: Batch 0/1
Batch Loss = 0.0007310406304618013
492, epoch_train_loss=0.0007310406304618013
Epoch 493
Epoch 493 :: Batch 0/1
Batch Loss = 0.0007310406304555218
493, epoch_train_loss=0.0007310406304555218
Epoch 494
Epoch 494 :: Batch 0/1
Batch Loss = 0.0007310406304492352
494, epoch_train_loss=0.0007310406304492352
Epoch 495
Epoch 495 :: Batch 0/1
Batch Loss = 0.000731040630442941
495, epoch_train_loss=0.000731040630442941
Epoch 496
Epoch 496 :: Batch 0/1
Batch Loss = 0.0007310406304366391
496, epoch_train_loss=0.0007310406304366391
Epoch 497
Epoch 497 :: Batch 0/1
Batch Loss = 0.0007310406304303299
497, epoch_train_loss=0.0007310406304303299
Epoch 498
Epoch 498 :: Batch 0/1
Batch Loss = 0.0007310406304240132
498, epoch_train_loss=0.0007310406304240132
Epoch 499
Epoch 499 :: Batch 0/1
Batch Loss = 0.0007310406304176892
499, epoch_train_loss=0.0007310406304176892
Epoch 500
Epoch 500 :: Batch 0/1
Batch Loss = 0.0007310406304113576
500, epoch_train_loss=0.0007310406304113576
Epoch 501
Epoch 501 :: Batch 0/1
Batch Loss = 0.0007310406304050186
501, epoch_train_loss=0.0007310406304050186
Epoch 502
Epoch 502 :: Batch 0/1
Batch Loss = 0.0007310406303986722
502, epoch_train_loss=0.0007310406303986722
Epoch 503
Epoch 503 :: Batch 0/1
Batch Loss = 0.0007310406303923184
503, epoch_train_loss=0.0007310406303923184
Epoch 504
Epoch 504 :: Batch 0/1
Batch Loss = 0.0007310406303859569
504, epoch_train_loss=0.0007310406303859569
Epoch 505
Epoch 505 :: Batch 0/1
Batch Loss = 0.0007310406303795883
505, epoch_train_loss=0.0007310406303795883
Epoch 506
Epoch 506 :: Batch 0/1
Batch Loss = 0.0007310406303732121
506, epoch_train_loss=0.0007310406303732121
Epoch 507
Epoch 507 :: Batch 0/1
Batch Loss = 0.0007310406303668286
507, epoch_train_loss=0.0007310406303668286
Epoch 508
Epoch 508 :: Batch 0/1
Batch Loss = 0.0007310406303604375
508, epoch_train_loss=0.0007310406303604375
Epoch 509
Epoch 509 :: Batch 0/1
Batch Loss = 0.0007310406303540391
509, epoch_train_loss=0.0007310406303540391
Epoch 510
Epoch 510 :: Batch 0/1
Batch Loss = 0.0007310406303476334
510, epoch_train_loss=0.0007310406303476334
Epoch 511
Epoch 511 :: Batch 0/1
Batch Loss = 0.00073104063034122
511, epoch_train_loss=0.00073104063034122
Epoch 512
Epoch 512 :: Batch 0/1
Batch Loss = 0.0007310406303347996
512, epoch_train_loss=0.0007310406303347996
Epoch 513
Epoch 513 :: Batch 0/1
Batch Loss = 0.0007310406303283716
513, epoch_train_loss=0.0007310406303283716
Epoch 514
Epoch 514 :: Batch 0/1
Batch Loss = 0.0007310406303219362
514, epoch_train_loss=0.0007310406303219362
Epoch 515
Epoch 515 :: Batch 0/1
Batch Loss = 0.0007310406303154935
515, epoch_train_loss=0.0007310406303154935
Epoch 516
Epoch 516 :: Batch 0/1
Batch Loss = 0.0007310406303090433
516, epoch_train_loss=0.0007310406303090433
Epoch 517
Epoch 517 :: Batch 0/1
Batch Loss = 0.0007310406303025857
517, epoch_train_loss=0.0007310406303025857
Epoch 518
Epoch 518 :: Batch 0/1
Batch Loss = 0.0007310406302961208
518, epoch_train_loss=0.0007310406302961208
Epoch 519
Epoch 519 :: Batch 0/1
Batch Loss = 0.0007310406302896486
519, epoch_train_loss=0.0007310406302896486
Epoch 520
Epoch 520 :: Batch 0/1
Batch Loss = 0.0007310406302831689
520, epoch_train_loss=0.0007310406302831689
Epoch 521
Epoch 521 :: Batch 0/1
Batch Loss = 0.0007310406302766819
521, epoch_train_loss=0.0007310406302766819
Epoch 522
Epoch 522 :: Batch 0/1
Batch Loss = 0.0007310406302701875
522, epoch_train_loss=0.0007310406302701875
Epoch 523
Epoch 523 :: Batch 0/1
Batch Loss = 0.0007310406302636858
523, epoch_train_loss=0.0007310406302636858
Epoch 524
Epoch 524 :: Batch 0/1
Batch Loss = 0.0007310406302571768
524, epoch_train_loss=0.0007310406302571768
Epoch 525
Epoch 525 :: Batch 0/1
Batch Loss = 0.0007310406302506601
525, epoch_train_loss=0.0007310406302506601
Epoch 526
Epoch 526 :: Batch 0/1
Batch Loss = 0.0007310406302441364
526, epoch_train_loss=0.0007310406302441364
Epoch 527
Epoch 527 :: Batch 0/1
Batch Loss = 0.0007310406302376053
527, epoch_train_loss=0.0007310406302376053
Epoch 528
Epoch 528 :: Batch 0/1
Batch Loss = 0.0007310406302310668
528, epoch_train_loss=0.0007310406302310668
Epoch 529
Epoch 529 :: Batch 0/1
Batch Loss = 0.000731040630224521
529, epoch_train_loss=0.000731040630224521
Epoch 530
Epoch 530 :: Batch 0/1
Batch Loss = 0.000731040630217968
530, epoch_train_loss=0.000731040630217968
Epoch 531
Epoch 531 :: Batch 0/1
Batch Loss = 0.0007310406302114074
531, epoch_train_loss=0.0007310406302114074
Epoch 532
Epoch 532 :: Batch 0/1
Batch Loss = 0.0007310406302048398
532, epoch_train_loss=0.0007310406302048398
Epoch 533
Epoch 533 :: Batch 0/1
Batch Loss = 0.0007310406301982645
533, epoch_train_loss=0.0007310406301982645
Epoch 534
Epoch 534 :: Batch 0/1
Batch Loss = 0.000731040630191682
534, epoch_train_loss=0.000731040630191682
Epoch 535
Epoch 535 :: Batch 0/1
Batch Loss = 0.0007310406301850923
535, epoch_train_loss=0.0007310406301850923
Epoch 536
Epoch 536 :: Batch 0/1
Batch Loss = 0.0007310406301784953
536, epoch_train_loss=0.0007310406301784953
Epoch 537
Epoch 537 :: Batch 0/1
Batch Loss = 0.0007310406301718909
537, epoch_train_loss=0.0007310406301718909
Epoch 538
Epoch 538 :: Batch 0/1
Batch Loss = 0.0007310406301652792
538, epoch_train_loss=0.0007310406301652792
Epoch 539
Epoch 539 :: Batch 0/1
Batch Loss = 0.0007310406301586602
539, epoch_train_loss=0.0007310406301586602
Epoch 540
Epoch 540 :: Batch 0/1
Batch Loss = 0.000731040630152034
540, epoch_train_loss=0.000731040630152034
Epoch 541
Epoch 541 :: Batch 0/1
Batch Loss = 0.0007310406301454006
541, epoch_train_loss=0.0007310406301454006
Epoch 542
Epoch 542 :: Batch 0/1
Batch Loss = 0.0007310406301387597
542, epoch_train_loss=0.0007310406301387597
Epoch 543
Epoch 543 :: Batch 0/1
Batch Loss = 0.0007310406301321115
543, epoch_train_loss=0.0007310406301321115
Epoch 544
Epoch 544 :: Batch 0/1
Batch Loss = 0.000731040630125456
544, epoch_train_loss=0.000731040630125456
Epoch 545
Epoch 545 :: Batch 0/1
Batch Loss = 0.0007310406301187934
545, epoch_train_loss=0.0007310406301187934
Epoch 546
Epoch 546 :: Batch 0/1
Batch Loss = 0.0007310406301121234
546, epoch_train_loss=0.0007310406301121234
Epoch 547
Epoch 547 :: Batch 0/1
Batch Loss = 0.0007310406301054462
547, epoch_train_loss=0.0007310406301054462
Epoch 548
Epoch 548 :: Batch 0/1
Batch Loss = 0.0007310406300987617
548, epoch_train_loss=0.0007310406300987617
Epoch 549
Epoch 549 :: Batch 0/1
Batch Loss = 0.0007310406300920698
549, epoch_train_loss=0.0007310406300920698
Epoch 550
Epoch 550 :: Batch 0/1
Batch Loss = 0.0007310406300853709
550, epoch_train_loss=0.0007310406300853709
Epoch 551
Epoch 551 :: Batch 0/1
Batch Loss = 0.0007310406300786646
551, epoch_train_loss=0.0007310406300786646
Epoch 552
Epoch 552 :: Batch 0/1
Batch Loss = 0.0007310406300719511
552, epoch_train_loss=0.0007310406300719511
Epoch 553
Epoch 553 :: Batch 0/1
Batch Loss = 0.0007310406300652303
553, epoch_train_loss=0.0007310406300652303
Epoch 554
Epoch 554 :: Batch 0/1
Batch Loss = 0.0007310406300585022
554, epoch_train_loss=0.0007310406300585022
Epoch 555
Epoch 555 :: Batch 0/1
Batch Loss = 0.0007310406300517668
555, epoch_train_loss=0.0007310406300517668
Epoch 556
Epoch 556 :: Batch 0/1
Batch Loss = 0.0007310406300450243
556, epoch_train_loss=0.0007310406300450243
Epoch 557
Epoch 557 :: Batch 0/1
Batch Loss = 0.0007310406300382746
557, epoch_train_loss=0.0007310406300382746
Epoch 558
Epoch 558 :: Batch 0/1
Batch Loss = 0.0007310406300315176
558, epoch_train_loss=0.0007310406300315176
Epoch 559
Epoch 559 :: Batch 0/1
Batch Loss = 0.0007310406300247533
559, epoch_train_loss=0.0007310406300247533
Epoch 560
Epoch 560 :: Batch 0/1
Batch Loss = 0.0007310406300179819
560, epoch_train_loss=0.0007310406300179819
Epoch 561
Epoch 561 :: Batch 0/1
Batch Loss = 0.0007310406300112032
561, epoch_train_loss=0.0007310406300112032
Epoch 562
Epoch 562 :: Batch 0/1
Batch Loss = 0.0007310406300044173
562, epoch_train_loss=0.0007310406300044173
Epoch 563
Epoch 563 :: Batch 0/1
Batch Loss = 0.0007310406299976241
563, epoch_train_loss=0.0007310406299976241
Epoch 564
Epoch 564 :: Batch 0/1
Batch Loss = 0.0007310406299908237
564, epoch_train_loss=0.0007310406299908237
Epoch 565
Epoch 565 :: Batch 0/1
Batch Loss = 0.000731040629984016
565, epoch_train_loss=0.000731040629984016
Epoch 566
Epoch 566 :: Batch 0/1
Batch Loss = 0.0007310406299772012
566, epoch_train_loss=0.0007310406299772012
Epoch 567
Epoch 567 :: Batch 0/1
Batch Loss = 0.0007310406299703793
567, epoch_train_loss=0.0007310406299703793
Epoch 568
Epoch 568 :: Batch 0/1
Batch Loss = 0.0007310406299635502
568, epoch_train_loss=0.0007310406299635502
Epoch 569
Epoch 569 :: Batch 0/1
Batch Loss = 0.0007310406299567137
569, epoch_train_loss=0.0007310406299567137
Epoch 570
Epoch 570 :: Batch 0/1
Batch Loss = 0.0007310406299498701
570, epoch_train_loss=0.0007310406299498701
Epoch 571
Epoch 571 :: Batch 0/1
Batch Loss = 0.0007310406299430195
571, epoch_train_loss=0.0007310406299430195
Epoch 572
Epoch 572 :: Batch 0/1
Batch Loss = 0.0007310406299361614
572, epoch_train_loss=0.0007310406299361614
Epoch 573
Epoch 573 :: Batch 0/1
Batch Loss = 0.0007310406299292963
573, epoch_train_loss=0.0007310406299292963
Epoch 574
Epoch 574 :: Batch 0/1
Batch Loss = 0.000731040629922424
574, epoch_train_loss=0.000731040629922424
Epoch 575
Epoch 575 :: Batch 0/1
Batch Loss = 0.0007310406299155443
575, epoch_train_loss=0.0007310406299155443
Epoch 576
Epoch 576 :: Batch 0/1
Batch Loss = 0.0007310406299086576
576, epoch_train_loss=0.0007310406299086576
Epoch 577
Epoch 577 :: Batch 0/1
Batch Loss = 0.0007310406299017637
577, epoch_train_loss=0.0007310406299017637
Epoch 578
Epoch 578 :: Batch 0/1
Batch Loss = 0.0007310406298948627
578, epoch_train_loss=0.0007310406298948627
Epoch 579
Epoch 579 :: Batch 0/1
Batch Loss = 0.0007310406298879544
579, epoch_train_loss=0.0007310406298879544
Epoch 580
Epoch 580 :: Batch 0/1
Batch Loss = 0.0007310406298810389
580, epoch_train_loss=0.0007310406298810389
Epoch 581
Epoch 581 :: Batch 0/1
Batch Loss = 0.0007310406298741164
581, epoch_train_loss=0.0007310406298741164
Epoch 582
Epoch 582 :: Batch 0/1
Batch Loss = 0.0007310406298671867
582, epoch_train_loss=0.0007310406298671867
Epoch 583
Epoch 583 :: Batch 0/1
Batch Loss = 0.0007310406298602498
583, epoch_train_loss=0.0007310406298602498
Epoch 584
Epoch 584 :: Batch 0/1
Batch Loss = 0.0007310406298533058
584, epoch_train_loss=0.0007310406298533058
Epoch 585
Epoch 585 :: Batch 0/1
Batch Loss = 0.0007310406298463546
585, epoch_train_loss=0.0007310406298463546
Epoch 586
Epoch 586 :: Batch 0/1
Batch Loss = 0.0007310406298393963
586, epoch_train_loss=0.0007310406298393963
Epoch 587
Epoch 587 :: Batch 0/1
Batch Loss = 0.0007310406298324308
587, epoch_train_loss=0.0007310406298324308
Epoch 588
Epoch 588 :: Batch 0/1
Batch Loss = 0.0007310406298254581
588, epoch_train_loss=0.0007310406298254581
Epoch 589
Epoch 589 :: Batch 0/1
Batch Loss = 0.0007310406298184784
589, epoch_train_loss=0.0007310406298184784
Epoch 590
Epoch 590 :: Batch 0/1
Batch Loss = 0.0007310406298114914
590, epoch_train_loss=0.0007310406298114914
Epoch 591
Epoch 591 :: Batch 0/1
Batch Loss = 0.0007310406298044973
591, epoch_train_loss=0.0007310406298044973
Epoch 592
Epoch 592 :: Batch 0/1
Batch Loss = 0.0007310406297974961
592, epoch_train_loss=0.0007310406297974961
Epoch 593
Epoch 593 :: Batch 0/1
Batch Loss = 0.0007310406297904878
593, epoch_train_loss=0.0007310406297904878
Epoch 594
Epoch 594 :: Batch 0/1
Batch Loss = 0.0007310406297834724
594, epoch_train_loss=0.0007310406297834724
Epoch 595
Epoch 595 :: Batch 0/1
Batch Loss = 0.0007310406297764499
595, epoch_train_loss=0.0007310406297764499
Epoch 596
Epoch 596 :: Batch 0/1
Batch Loss = 0.0007310406297694202
596, epoch_train_loss=0.0007310406297694202
Epoch 597
Epoch 597 :: Batch 0/1
Batch Loss = 0.0007310406297623834
597, epoch_train_loss=0.0007310406297623834
Epoch 598
Epoch 598 :: Batch 0/1
Batch Loss = 0.0007310406297553395
598, epoch_train_loss=0.0007310406297553395
Epoch 599
Epoch 599 :: Batch 0/1
Batch Loss = 0.0007310406297482885
599, epoch_train_loss=0.0007310406297482885
Epoch 600
Epoch 600 :: Batch 0/1
Batch Loss = 0.0007310406297412304
600, epoch_train_loss=0.0007310406297412304
Epoch 601
Epoch 601 :: Batch 0/1
Batch Loss = 0.0007310406297341652
601, epoch_train_loss=0.0007310406297341652
Epoch 602
Epoch 602 :: Batch 0/1
Batch Loss = 0.0007310406297270927
602, epoch_train_loss=0.0007310406297270927
Epoch 603
Epoch 603 :: Batch 0/1
Batch Loss = 0.0007310406297200133
603, epoch_train_loss=0.0007310406297200133
Epoch 604
Epoch 604 :: Batch 0/1
Batch Loss = 0.0007310406297129266
604, epoch_train_loss=0.0007310406297129266
Epoch 605
Epoch 605 :: Batch 0/1
Batch Loss = 0.0007310406297058331
605, epoch_train_loss=0.0007310406297058331
Epoch 606
Epoch 606 :: Batch 0/1
Batch Loss = 0.0007310406296987324
606, epoch_train_loss=0.0007310406296987324
Epoch 607
Epoch 607 :: Batch 0/1
Batch Loss = 0.0007310406296916245
607, epoch_train_loss=0.0007310406296916245
Epoch 608
Epoch 608 :: Batch 0/1
Batch Loss = 0.0007310406296845096
608, epoch_train_loss=0.0007310406296845096
Epoch 609
Epoch 609 :: Batch 0/1
Batch Loss = 0.0007310406296773877
609, epoch_train_loss=0.0007310406296773877
Epoch 610
Epoch 610 :: Batch 0/1
Batch Loss = 0.0007310406296702585
610, epoch_train_loss=0.0007310406296702585
Epoch 611
Epoch 611 :: Batch 0/1
Batch Loss = 0.0007310406296631224
611, epoch_train_loss=0.0007310406296631224
Epoch 612
Epoch 612 :: Batch 0/1
Batch Loss = 0.0007310406296559791
612, epoch_train_loss=0.0007310406296559791
Epoch 613
Epoch 613 :: Batch 0/1
Batch Loss = 0.0007310406296488289
613, epoch_train_loss=0.0007310406296488289
Epoch 614
Epoch 614 :: Batch 0/1
Batch Loss = 0.0007310406296416715
614, epoch_train_loss=0.0007310406296416715
Epoch 615
Epoch 615 :: Batch 0/1
Batch Loss = 0.000731040629634507
615, epoch_train_loss=0.000731040629634507
Epoch 616
Epoch 616 :: Batch 0/1
Batch Loss = 0.0007310406296273356
616, epoch_train_loss=0.0007310406296273356
Epoch 617
Epoch 617 :: Batch 0/1
Batch Loss = 0.000731040629620157
617, epoch_train_loss=0.000731040629620157
Epoch 618
Epoch 618 :: Batch 0/1
Batch Loss = 0.0007310406296129713
618, epoch_train_loss=0.0007310406296129713
Epoch 619
Epoch 619 :: Batch 0/1
Batch Loss = 0.0007310406296057785
619, epoch_train_loss=0.0007310406296057785
Epoch 620
Epoch 620 :: Batch 0/1
Batch Loss = 0.0007310406295985789
620, epoch_train_loss=0.0007310406295985789
Epoch 621
Epoch 621 :: Batch 0/1
Batch Loss = 0.0007310406295913721
621, epoch_train_loss=0.0007310406295913721
Epoch 622
Epoch 622 :: Batch 0/1
Batch Loss = 0.0007310406295841582
622, epoch_train_loss=0.0007310406295841582
Epoch 623
Epoch 623 :: Batch 0/1
Batch Loss = 0.0007310406295769373
623, epoch_train_loss=0.0007310406295769373
Epoch 624
Epoch 624 :: Batch 0/1
Batch Loss = 0.0007310406295697093
624, epoch_train_loss=0.0007310406295697093
Epoch 625
Epoch 625 :: Batch 0/1
Batch Loss = 0.0007310406295624744
625, epoch_train_loss=0.0007310406295624744
Epoch 626
Epoch 626 :: Batch 0/1
Batch Loss = 0.0007310406295552322
626, epoch_train_loss=0.0007310406295552322
Epoch 627
Epoch 627 :: Batch 0/1
Batch Loss = 0.0007310406295479832
627, epoch_train_loss=0.0007310406295479832
Epoch 628
Epoch 628 :: Batch 0/1
Batch Loss = 0.0007310406295407271
628, epoch_train_loss=0.0007310406295407271
Epoch 629
Epoch 629 :: Batch 0/1
Batch Loss = 0.000731040629533464
629, epoch_train_loss=0.000731040629533464
Epoch 630
Epoch 630 :: Batch 0/1
Batch Loss = 0.0007310406295261938
630, epoch_train_loss=0.0007310406295261938
Epoch 631
Epoch 631 :: Batch 0/1
Batch Loss = 0.0007310406295189167
631, epoch_train_loss=0.0007310406295189167
Epoch 632
Epoch 632 :: Batch 0/1
Batch Loss = 0.0007310406295116324
632, epoch_train_loss=0.0007310406295116324
Epoch 633
Epoch 633 :: Batch 0/1
Batch Loss = 0.0007310406295043412
633, epoch_train_loss=0.0007310406295043412
Epoch 634
Epoch 634 :: Batch 0/1
Batch Loss = 0.0007310406294970429
634, epoch_train_loss=0.0007310406294970429
Epoch 635
Epoch 635 :: Batch 0/1
Batch Loss = 0.0007310406294897376
635, epoch_train_loss=0.0007310406294897376
Epoch 636
Epoch 636 :: Batch 0/1
Batch Loss = 0.0007310406294824254
636, epoch_train_loss=0.0007310406294824254
Epoch 637
Epoch 637 :: Batch 0/1
Batch Loss = 0.0007310406294751061
637, epoch_train_loss=0.0007310406294751061
Epoch 638
Epoch 638 :: Batch 0/1
Batch Loss = 0.0007310406294677797
638, epoch_train_loss=0.0007310406294677797
Epoch 639
Epoch 639 :: Batch 0/1
Batch Loss = 0.0007310406294604464
639, epoch_train_loss=0.0007310406294604464
Epoch 640
Epoch 640 :: Batch 0/1
Batch Loss = 0.0007310406294531061
640, epoch_train_loss=0.0007310406294531061
Epoch 641
Epoch 641 :: Batch 0/1
Batch Loss = 0.0007310406294457587
641, epoch_train_loss=0.0007310406294457587
Epoch 642
Epoch 642 :: Batch 0/1
Batch Loss = 0.0007310406294384045
642, epoch_train_loss=0.0007310406294384045
Epoch 643
Epoch 643 :: Batch 0/1
Batch Loss = 0.0007310406294310431
643, epoch_train_loss=0.0007310406294310431
Epoch 644
Epoch 644 :: Batch 0/1
Batch Loss = 0.0007310406294236748
644, epoch_train_loss=0.0007310406294236748
Epoch 645
Epoch 645 :: Batch 0/1
Batch Loss = 0.0007310406294162995
645, epoch_train_loss=0.0007310406294162995
Epoch 646
Epoch 646 :: Batch 0/1
Batch Loss = 0.0007310406294089172
646, epoch_train_loss=0.0007310406294089172
Epoch 647
Epoch 647 :: Batch 0/1
Batch Loss = 0.0007310406294015277
647, epoch_train_loss=0.0007310406294015277
Epoch 648
Epoch 648 :: Batch 0/1
Batch Loss = 0.0007310406293941316
648, epoch_train_loss=0.0007310406293941316
Epoch 649
Epoch 649 :: Batch 0/1
Batch Loss = 0.0007310406293867282
649, epoch_train_loss=0.0007310406293867282
Epoch 650
Epoch 650 :: Batch 0/1
Batch Loss = 0.000731040629379318
650, epoch_train_loss=0.000731040629379318
Epoch 651
Epoch 651 :: Batch 0/1
Batch Loss = 0.0007310406293719008
651, epoch_train_loss=0.0007310406293719008
Epoch 652
Epoch 652 :: Batch 0/1
Batch Loss = 0.0007310406293644765
652, epoch_train_loss=0.0007310406293644765
Epoch 653
Epoch 653 :: Batch 0/1
Batch Loss = 0.0007310406293570455
653, epoch_train_loss=0.0007310406293570455
Epoch 654
Epoch 654 :: Batch 0/1
Batch Loss = 0.0007310406293496072
654, epoch_train_loss=0.0007310406293496072
Epoch 655
Epoch 655 :: Batch 0/1
Batch Loss = 0.0007310406293421622
655, epoch_train_loss=0.0007310406293421622
Epoch 656
Epoch 656 :: Batch 0/1
Batch Loss = 0.00073104062933471
656, epoch_train_loss=0.00073104062933471
Epoch 657
Epoch 657 :: Batch 0/1
Batch Loss = 0.000731040629327251
657, epoch_train_loss=0.000731040629327251
Epoch 658
Epoch 658 :: Batch 0/1
Batch Loss = 0.0007310406293197849
658, epoch_train_loss=0.0007310406293197849
Epoch 659
Epoch 659 :: Batch 0/1
Batch Loss = 0.000731040629312312
659, epoch_train_loss=0.000731040629312312
Epoch 660
Epoch 660 :: Batch 0/1
Batch Loss = 0.0007310406293048322
660, epoch_train_loss=0.0007310406293048322
Epoch 661
Epoch 661 :: Batch 0/1
Batch Loss = 0.0007310406292973451
661, epoch_train_loss=0.0007310406292973451
Epoch 662
Epoch 662 :: Batch 0/1
Batch Loss = 0.0007310406292898513
662, epoch_train_loss=0.0007310406292898513
Epoch 663
Epoch 663 :: Batch 0/1
Batch Loss = 0.0007310406292823504
663, epoch_train_loss=0.0007310406292823504
Epoch 664
Epoch 664 :: Batch 0/1
Batch Loss = 0.0007310406292748428
664, epoch_train_loss=0.0007310406292748428
Epoch 665
Epoch 665 :: Batch 0/1
Batch Loss = 0.000731040629267328
665, epoch_train_loss=0.000731040629267328
Epoch 666
Epoch 666 :: Batch 0/1
Batch Loss = 0.0007310406292598063
666, epoch_train_loss=0.0007310406292598063
Epoch 667
Epoch 667 :: Batch 0/1
Batch Loss = 0.0007310406292522779
667, epoch_train_loss=0.0007310406292522779
Epoch 668
Epoch 668 :: Batch 0/1
Batch Loss = 0.0007310406292447423
668, epoch_train_loss=0.0007310406292447423
Epoch 669
Epoch 669 :: Batch 0/1
Batch Loss = 0.0007310406292371999
669, epoch_train_loss=0.0007310406292371999
Epoch 670
Epoch 670 :: Batch 0/1
Batch Loss = 0.0007310406292296504
670, epoch_train_loss=0.0007310406292296504
Epoch 671
Epoch 671 :: Batch 0/1
Batch Loss = 0.000731040629222094
671, epoch_train_loss=0.000731040629222094
Epoch 672
Epoch 672 :: Batch 0/1
Batch Loss = 0.0007310406292145307
672, epoch_train_loss=0.0007310406292145307
Epoch 673
Epoch 673 :: Batch 0/1
Batch Loss = 0.0007310406292069606
673, epoch_train_loss=0.0007310406292069606
Epoch 674
Epoch 674 :: Batch 0/1
Batch Loss = 0.0007310406291993834
674, epoch_train_loss=0.0007310406291993834
Epoch 675
Epoch 675 :: Batch 0/1
Batch Loss = 0.0007310406291917993
675, epoch_train_loss=0.0007310406291917993
Epoch 676
Epoch 676 :: Batch 0/1
Batch Loss = 0.0007310406291842084
676, epoch_train_loss=0.0007310406291842084
Epoch 677
Epoch 677 :: Batch 0/1
Batch Loss = 0.0007310406291766105
677, epoch_train_loss=0.0007310406291766105
Epoch 678
Epoch 678 :: Batch 0/1
Batch Loss = 0.0007310406291690058
678, epoch_train_loss=0.0007310406291690058
Epoch 679
Epoch 679 :: Batch 0/1
Batch Loss = 0.0007310406291613938
679, epoch_train_loss=0.0007310406291613938
Epoch 680
Epoch 680 :: Batch 0/1
Batch Loss = 0.0007310406291537754
680, epoch_train_loss=0.0007310406291537754
Epoch 681
Epoch 681 :: Batch 0/1
Batch Loss = 0.0007310406291461496
681, epoch_train_loss=0.0007310406291461496
Epoch 682
Epoch 682 :: Batch 0/1
Batch Loss = 0.0007310406291385174
682, epoch_train_loss=0.0007310406291385174
Epoch 683
Epoch 683 :: Batch 0/1
Batch Loss = 0.0007310406291308779
683, epoch_train_loss=0.0007310406291308779
Epoch 684
Epoch 684 :: Batch 0/1
Batch Loss = 0.0007310406291232317
684, epoch_train_loss=0.0007310406291232317
Epoch 685
Epoch 685 :: Batch 0/1
Batch Loss = 0.0007310406291155784
685, epoch_train_loss=0.0007310406291155784
Epoch 686
Epoch 686 :: Batch 0/1
Batch Loss = 0.0007310406291079184
686, epoch_train_loss=0.0007310406291079184
Epoch 687
Epoch 687 :: Batch 0/1
Batch Loss = 0.0007310406291002514
687, epoch_train_loss=0.0007310406291002514
Epoch 688
Epoch 688 :: Batch 0/1
Batch Loss = 0.0007310406290925775
688, epoch_train_loss=0.0007310406290925775
Epoch 689
Epoch 689 :: Batch 0/1
Batch Loss = 0.0007310406290848967
689, epoch_train_loss=0.0007310406290848967
Epoch 690
Epoch 690 :: Batch 0/1
Batch Loss = 0.0007310406290772089
690, epoch_train_loss=0.0007310406290772089
Epoch 691
Epoch 691 :: Batch 0/1
Batch Loss = 0.0007310406290695144
691, epoch_train_loss=0.0007310406290695144
Epoch 692
Epoch 692 :: Batch 0/1
Batch Loss = 0.0007310406290618129
692, epoch_train_loss=0.0007310406290618129
Epoch 693
Epoch 693 :: Batch 0/1
Batch Loss = 0.0007310406290541045
693, epoch_train_loss=0.0007310406290541045
Epoch 694
Epoch 694 :: Batch 0/1
Batch Loss = 0.0007310406290463892
694, epoch_train_loss=0.0007310406290463892
Epoch 695
Epoch 695 :: Batch 0/1
Batch Loss = 0.0007310406290386672
695, epoch_train_loss=0.0007310406290386672
Epoch 696
Epoch 696 :: Batch 0/1
Batch Loss = 0.0007310406290309382
696, epoch_train_loss=0.0007310406290309382
Epoch 697
Epoch 697 :: Batch 0/1
Batch Loss = 0.0007310406290232023
697, epoch_train_loss=0.0007310406290232023
Epoch 698
Epoch 698 :: Batch 0/1
Batch Loss = 0.0007310406290154595
698, epoch_train_loss=0.0007310406290154595
Epoch 699
Epoch 699 :: Batch 0/1
Batch Loss = 0.0007310406290077099
699, epoch_train_loss=0.0007310406290077099
Epoch 700
Epoch 700 :: Batch 0/1
Batch Loss = 0.0007310406289999533
700, epoch_train_loss=0.0007310406289999533
Epoch 701
Epoch 701 :: Batch 0/1
Batch Loss = 0.0007310406289921899
701, epoch_train_loss=0.0007310406289921899
Epoch 702
Epoch 702 :: Batch 0/1
Batch Loss = 0.0007310406289844196
702, epoch_train_loss=0.0007310406289844196
Epoch 703
Epoch 703 :: Batch 0/1
Batch Loss = 0.0007310406289766425
703, epoch_train_loss=0.0007310406289766425
Epoch 704
Epoch 704 :: Batch 0/1
Batch Loss = 0.0007310406289688586
704, epoch_train_loss=0.0007310406289688586
Epoch 705
Epoch 705 :: Batch 0/1
Batch Loss = 0.0007310406289610677
705, epoch_train_loss=0.0007310406289610677
Epoch 706
Epoch 706 :: Batch 0/1
Batch Loss = 0.0007310406289532701
706, epoch_train_loss=0.0007310406289532701
Epoch 707
Epoch 707 :: Batch 0/1
Batch Loss = 0.0007310406289454654
707, epoch_train_loss=0.0007310406289454654
Epoch 708
Epoch 708 :: Batch 0/1
Batch Loss = 0.000731040628937654
708, epoch_train_loss=0.000731040628937654
Epoch 709
Epoch 709 :: Batch 0/1
Batch Loss = 0.0007310406289298357
709, epoch_train_loss=0.0007310406289298357
Epoch 710
Epoch 710 :: Batch 0/1
Batch Loss = 0.0007310406289220106
710, epoch_train_loss=0.0007310406289220106
Epoch 711
Epoch 711 :: Batch 0/1
Batch Loss = 0.0007310406289141785
711, epoch_train_loss=0.0007310406289141785
Epoch 712
Epoch 712 :: Batch 0/1
Batch Loss = 0.0007310406289063397
712, epoch_train_loss=0.0007310406289063397
Epoch 713
Epoch 713 :: Batch 0/1
Batch Loss = 0.0007310406288984939
713, epoch_train_loss=0.0007310406288984939
Epoch 714
Epoch 714 :: Batch 0/1
Batch Loss = 0.0007310406288906415
714, epoch_train_loss=0.0007310406288906415
Epoch 715
Epoch 715 :: Batch 0/1
Batch Loss = 0.0007310406288827821
715, epoch_train_loss=0.0007310406288827821
Epoch 716
Epoch 716 :: Batch 0/1
Batch Loss = 0.0007310406288749158
716, epoch_train_loss=0.0007310406288749158
Epoch 717
Epoch 717 :: Batch 0/1
Batch Loss = 0.0007310406288670426
717, epoch_train_loss=0.0007310406288670426
Epoch 718
Epoch 718 :: Batch 0/1
Batch Loss = 0.0007310406288591628
718, epoch_train_loss=0.0007310406288591628
Epoch 719
Epoch 719 :: Batch 0/1
Batch Loss = 0.000731040628851276
719, epoch_train_loss=0.000731040628851276
Epoch 720
Epoch 720 :: Batch 0/1
Batch Loss = 0.0007310406288433823
720, epoch_train_loss=0.0007310406288433823
Epoch 721
Epoch 721 :: Batch 0/1
Batch Loss = 0.0007310406288354819
721, epoch_train_loss=0.0007310406288354819
Epoch 722
Epoch 722 :: Batch 0/1
Batch Loss = 0.0007310406288275747
722, epoch_train_loss=0.0007310406288275747
Epoch 723
Epoch 723 :: Batch 0/1
Batch Loss = 0.0007310406288196607
723, epoch_train_loss=0.0007310406288196607
Epoch 724
Epoch 724 :: Batch 0/1
Batch Loss = 0.0007310406288117397
724, epoch_train_loss=0.0007310406288117397
Epoch 725
Epoch 725 :: Batch 0/1
Batch Loss = 0.0007310406288038119
725, epoch_train_loss=0.0007310406288038119
Epoch 726
Epoch 726 :: Batch 0/1
Batch Loss = 0.0007310406287958773
726, epoch_train_loss=0.0007310406287958773
Epoch 727
Epoch 727 :: Batch 0/1
Batch Loss = 0.0007310406287879361
727, epoch_train_loss=0.0007310406287879361
Epoch 728
Epoch 728 :: Batch 0/1
Batch Loss = 0.0007310406287799878
728, epoch_train_loss=0.0007310406287799878
Epoch 729
Epoch 729 :: Batch 0/1
Batch Loss = 0.0007310406287720328
729, epoch_train_loss=0.0007310406287720328
Epoch 730
Epoch 730 :: Batch 0/1
Batch Loss = 0.0007310406287640708
730, epoch_train_loss=0.0007310406287640708
Epoch 731
Epoch 731 :: Batch 0/1
Batch Loss = 0.0007310406287561023
731, epoch_train_loss=0.0007310406287561023
Epoch 732
Epoch 732 :: Batch 0/1
Batch Loss = 0.0007310406287481268
732, epoch_train_loss=0.0007310406287481268
Epoch 733
Epoch 733 :: Batch 0/1
Batch Loss = 0.0007310406287401444
733, epoch_train_loss=0.0007310406287401444
Epoch 734
Epoch 734 :: Batch 0/1
Batch Loss = 0.0007310406287321553
734, epoch_train_loss=0.0007310406287321553
Epoch 735
Epoch 735 :: Batch 0/1
Batch Loss = 0.0007310406287241594
735, epoch_train_loss=0.0007310406287241594
Epoch 736
Epoch 736 :: Batch 0/1
Batch Loss = 0.0007310406287161567
736, epoch_train_loss=0.0007310406287161567
Epoch 737
Epoch 737 :: Batch 0/1
Batch Loss = 0.0007310406287081471
737, epoch_train_loss=0.0007310406287081471
Epoch 738
Epoch 738 :: Batch 0/1
Batch Loss = 0.0007310406287001309
738, epoch_train_loss=0.0007310406287001309
Epoch 739
Epoch 739 :: Batch 0/1
Batch Loss = 0.0007310406286921077
739, epoch_train_loss=0.0007310406286921077
Epoch 740
Epoch 740 :: Batch 0/1
Batch Loss = 0.0007310406286840777
740, epoch_train_loss=0.0007310406286840777
Epoch 741
Epoch 741 :: Batch 0/1
Batch Loss = 0.000731040628676041
741, epoch_train_loss=0.000731040628676041
Epoch 742
Epoch 742 :: Batch 0/1
Batch Loss = 0.0007310406286679975
742, epoch_train_loss=0.0007310406286679975
Epoch 743
Epoch 743 :: Batch 0/1
Batch Loss = 0.0007310406286599471
743, epoch_train_loss=0.0007310406286599471
Epoch 744
Epoch 744 :: Batch 0/1
Batch Loss = 0.00073104062865189
744, epoch_train_loss=0.00073104062865189
Epoch 745
Epoch 745 :: Batch 0/1
Batch Loss = 0.0007310406286438261
745, epoch_train_loss=0.0007310406286438261
Epoch 746
Epoch 746 :: Batch 0/1
Batch Loss = 0.0007310406286357554
746, epoch_train_loss=0.0007310406286357554
Epoch 747
Epoch 747 :: Batch 0/1
Batch Loss = 0.0007310406286276779
747, epoch_train_loss=0.0007310406286276779
Epoch 748
Epoch 748 :: Batch 0/1
Batch Loss = 0.0007310406286195936
748, epoch_train_loss=0.0007310406286195936
Epoch 749
Epoch 749 :: Batch 0/1
Batch Loss = 0.0007310406286115026
749, epoch_train_loss=0.0007310406286115026
Epoch 750
Epoch 750 :: Batch 0/1
Batch Loss = 0.0007310406286034048
750, epoch_train_loss=0.0007310406286034048
Epoch 751
Epoch 751 :: Batch 0/1
Batch Loss = 0.0007310406285953
751, epoch_train_loss=0.0007310406285953
Epoch 752
Epoch 752 :: Batch 0/1
Batch Loss = 0.0007310406285871887
752, epoch_train_loss=0.0007310406285871887
Epoch 753
Epoch 753 :: Batch 0/1
Batch Loss = 0.0007310406285790705
753, epoch_train_loss=0.0007310406285790705
Epoch 754
Epoch 754 :: Batch 0/1
Batch Loss = 0.0007310406285709454
754, epoch_train_loss=0.0007310406285709454
Epoch 755
Epoch 755 :: Batch 0/1
Batch Loss = 0.0007310406285628137
755, epoch_train_loss=0.0007310406285628137
Epoch 756
Epoch 756 :: Batch 0/1
Batch Loss = 0.0007310406285546753
756, epoch_train_loss=0.0007310406285546753
Epoch 757
Epoch 757 :: Batch 0/1
Batch Loss = 0.00073104062854653
757, epoch_train_loss=0.00073104062854653
Epoch 758
Epoch 758 :: Batch 0/1
Batch Loss = 0.0007310406285383779
758, epoch_train_loss=0.0007310406285383779
Epoch 759
Epoch 759 :: Batch 0/1
Batch Loss = 0.0007310406285302192
759, epoch_train_loss=0.0007310406285302192
Epoch 760
Epoch 760 :: Batch 0/1
Batch Loss = 0.0007310406285220536
760, epoch_train_loss=0.0007310406285220536
Epoch 761
Epoch 761 :: Batch 0/1
Batch Loss = 0.0007310406285138811
761, epoch_train_loss=0.0007310406285138811
Epoch 762
Epoch 762 :: Batch 0/1
Batch Loss = 0.000731040628505702
762, epoch_train_loss=0.000731040628505702
Epoch 763
Epoch 763 :: Batch 0/1
Batch Loss = 0.0007310406284975162
763, epoch_train_loss=0.0007310406284975162
Epoch 764
Epoch 764 :: Batch 0/1
Batch Loss = 0.0007310406284893235
764, epoch_train_loss=0.0007310406284893235
Epoch 765
Epoch 765 :: Batch 0/1
Batch Loss = 0.0007310406284811241
765, epoch_train_loss=0.0007310406284811241
Epoch 766
Epoch 766 :: Batch 0/1
Batch Loss = 0.0007310406284729179
766, epoch_train_loss=0.0007310406284729179
Epoch 767
Epoch 767 :: Batch 0/1
Batch Loss = 0.0007310406284647049
767, epoch_train_loss=0.0007310406284647049
Epoch 768
Epoch 768 :: Batch 0/1
Batch Loss = 0.0007310406284564853
768, epoch_train_loss=0.0007310406284564853
Epoch 769
Epoch 769 :: Batch 0/1
Batch Loss = 0.000731040628448259
769, epoch_train_loss=0.000731040628448259
Epoch 770
Epoch 770 :: Batch 0/1
Batch Loss = 0.0007310406284400257
770, epoch_train_loss=0.0007310406284400257
Epoch 771
Epoch 771 :: Batch 0/1
Batch Loss = 0.0007310406284317859
771, epoch_train_loss=0.0007310406284317859
Epoch 772
Epoch 772 :: Batch 0/1
Batch Loss = 0.0007310406284235391
772, epoch_train_loss=0.0007310406284235391
Epoch 773
Epoch 773 :: Batch 0/1
Batch Loss = 0.0007310406284152857
773, epoch_train_loss=0.0007310406284152857
Epoch 774
Epoch 774 :: Batch 0/1
Batch Loss = 0.0007310406284070255
774, epoch_train_loss=0.0007310406284070255
Epoch 775
Epoch 775 :: Batch 0/1
Batch Loss = 0.0007310406283987586
775, epoch_train_loss=0.0007310406283987586
Epoch 776
Epoch 776 :: Batch 0/1
Batch Loss = 0.000731040628390485
776, epoch_train_loss=0.000731040628390485
Epoch 777
Epoch 777 :: Batch 0/1
Batch Loss = 0.0007310406283822045
777, epoch_train_loss=0.0007310406283822045
Epoch 778
Epoch 778 :: Batch 0/1
Batch Loss = 0.0007310406283739174
778, epoch_train_loss=0.0007310406283739174
Epoch 779
Epoch 779 :: Batch 0/1
Batch Loss = 0.0007310406283656237
779, epoch_train_loss=0.0007310406283656237
Epoch 780
Epoch 780 :: Batch 0/1
Batch Loss = 0.000731040628357323
780, epoch_train_loss=0.000731040628357323
Epoch 781
Epoch 781 :: Batch 0/1
Batch Loss = 0.0007310406283490156
781, epoch_train_loss=0.0007310406283490156
Epoch 782
Epoch 782 :: Batch 0/1
Batch Loss = 0.0007310406283407016
782, epoch_train_loss=0.0007310406283407016
Epoch 783
Epoch 783 :: Batch 0/1
Batch Loss = 0.0007310406283323808
783, epoch_train_loss=0.0007310406283323808
Epoch 784
Epoch 784 :: Batch 0/1
Batch Loss = 0.0007310406283240532
784, epoch_train_loss=0.0007310406283240532
Epoch 785
Epoch 785 :: Batch 0/1
Batch Loss = 0.000731040628315719
785, epoch_train_loss=0.000731040628315719
Epoch 786
Epoch 786 :: Batch 0/1
Batch Loss = 0.000731040628307378
786, epoch_train_loss=0.000731040628307378
Epoch 787
Epoch 787 :: Batch 0/1
Batch Loss = 0.0007310406282990303
787, epoch_train_loss=0.0007310406282990303
Epoch 788
Epoch 788 :: Batch 0/1
Batch Loss = 0.0007310406282906759
788, epoch_train_loss=0.0007310406282906759
Epoch 789
Epoch 789 :: Batch 0/1
Batch Loss = 0.0007310406282823147
789, epoch_train_loss=0.0007310406282823147
Epoch 790
Epoch 790 :: Batch 0/1
Batch Loss = 0.0007310406282739468
790, epoch_train_loss=0.0007310406282739468
Epoch 791
Epoch 791 :: Batch 0/1
Batch Loss = 0.0007310406282655722
791, epoch_train_loss=0.0007310406282655722
Epoch 792
Epoch 792 :: Batch 0/1
Batch Loss = 0.000731040628257191
792, epoch_train_loss=0.000731040628257191
Epoch 793
Epoch 793 :: Batch 0/1
Batch Loss = 0.0007310406282488029
793, epoch_train_loss=0.0007310406282488029
Epoch 794
Epoch 794 :: Batch 0/1
Batch Loss = 0.0007310406282404081
794, epoch_train_loss=0.0007310406282404081
Epoch 795
Epoch 795 :: Batch 0/1
Batch Loss = 0.0007310406282320067
795, epoch_train_loss=0.0007310406282320067
Epoch 796
Epoch 796 :: Batch 0/1
Batch Loss = 0.0007310406282235984
796, epoch_train_loss=0.0007310406282235984
Epoch 797
Epoch 797 :: Batch 0/1
Batch Loss = 0.0007310406282151836
797, epoch_train_loss=0.0007310406282151836
Epoch 798
Epoch 798 :: Batch 0/1
Batch Loss = 0.0007310406282067621
798, epoch_train_loss=0.0007310406282067621
Epoch 799
Epoch 799 :: Batch 0/1
Batch Loss = 0.0007310406281983337
799, epoch_train_loss=0.0007310406281983337
Epoch 800
Epoch 800 :: Batch 0/1
Batch Loss = 0.0007310406281898987
800, epoch_train_loss=0.0007310406281898987
Epoch 801
Epoch 801 :: Batch 0/1
Batch Loss = 0.000731040628181457
801, epoch_train_loss=0.000731040628181457
Epoch 802
Epoch 802 :: Batch 0/1
Batch Loss = 0.0007310406281730086
802, epoch_train_loss=0.0007310406281730086
Epoch 803
Epoch 803 :: Batch 0/1
Batch Loss = 0.0007310406281645534
803, epoch_train_loss=0.0007310406281645534
Epoch 804
Epoch 804 :: Batch 0/1
Batch Loss = 0.0007310406281560916
804, epoch_train_loss=0.0007310406281560916
Epoch 805
Epoch 805 :: Batch 0/1
Batch Loss = 0.0007310406281476231
805, epoch_train_loss=0.0007310406281476231
Epoch 806
Epoch 806 :: Batch 0/1
Batch Loss = 0.0007310406281391479
806, epoch_train_loss=0.0007310406281391479
Epoch 807
Epoch 807 :: Batch 0/1
Batch Loss = 0.0007310406281306659
807, epoch_train_loss=0.0007310406281306659
Epoch 808
Epoch 808 :: Batch 0/1
Batch Loss = 0.0007310406281221774
808, epoch_train_loss=0.0007310406281221774
Epoch 809
Epoch 809 :: Batch 0/1
Batch Loss = 0.0007310406281136822
809, epoch_train_loss=0.0007310406281136822
Epoch 810
Epoch 810 :: Batch 0/1
Batch Loss = 0.00073104062810518
810, epoch_train_loss=0.00073104062810518
Epoch 811
Epoch 811 :: Batch 0/1
Batch Loss = 0.0007310406280966714
811, epoch_train_loss=0.0007310406280966714
Epoch 812
Epoch 812 :: Batch 0/1
Batch Loss = 0.000731040628088156
812, epoch_train_loss=0.000731040628088156
Epoch 813
Epoch 813 :: Batch 0/1
Batch Loss = 0.0007310406280796338
813, epoch_train_loss=0.0007310406280796338
Epoch 814
Epoch 814 :: Batch 0/1
Batch Loss = 0.0007310406280711051
814, epoch_train_loss=0.0007310406280711051
Epoch 815
Epoch 815 :: Batch 0/1
Batch Loss = 0.0007310406280625697
815, epoch_train_loss=0.0007310406280625697
Epoch 816
Epoch 816 :: Batch 0/1
Batch Loss = 0.0007310406280540275
816, epoch_train_loss=0.0007310406280540275
Epoch 817
Epoch 817 :: Batch 0/1
Batch Loss = 0.0007310406280454788
817, epoch_train_loss=0.0007310406280454788
Epoch 818
Epoch 818 :: Batch 0/1
Batch Loss = 0.0007310406280369233
818, epoch_train_loss=0.0007310406280369233
Epoch 819
Epoch 819 :: Batch 0/1
Batch Loss = 0.000731040628028361
819, epoch_train_loss=0.000731040628028361
Epoch 820
Epoch 820 :: Batch 0/1
Batch Loss = 0.0007310406280197923
820, epoch_train_loss=0.0007310406280197923
Epoch 821
Epoch 821 :: Batch 0/1
Batch Loss = 0.0007310406280112167
821, epoch_train_loss=0.0007310406280112167
Epoch 822
Epoch 822 :: Batch 0/1
Batch Loss = 0.0007310406280026347
822, epoch_train_loss=0.0007310406280026347
Epoch 823
Epoch 823 :: Batch 0/1
Batch Loss = 0.0007310406279940457
823, epoch_train_loss=0.0007310406279940457
Epoch 824
Epoch 824 :: Batch 0/1
Batch Loss = 0.0007310406279854502
824, epoch_train_loss=0.0007310406279854502
Epoch 825
Epoch 825 :: Batch 0/1
Batch Loss = 0.0007310406279768479
825, epoch_train_loss=0.0007310406279768479
Epoch 826
Epoch 826 :: Batch 0/1
Batch Loss = 0.000731040627968239
826, epoch_train_loss=0.000731040627968239
Epoch 827
Epoch 827 :: Batch 0/1
Batch Loss = 0.0007310406279596234
827, epoch_train_loss=0.0007310406279596234
Epoch 828
Epoch 828 :: Batch 0/1
Batch Loss = 0.0007310406279510012
828, epoch_train_loss=0.0007310406279510012
Epoch 829
Epoch 829 :: Batch 0/1
Batch Loss = 0.0007310406279423722
829, epoch_train_loss=0.0007310406279423722
Epoch 830
Epoch 830 :: Batch 0/1
Batch Loss = 0.0007310406279337367
830, epoch_train_loss=0.0007310406279337367
Epoch 831
Epoch 831 :: Batch 0/1
Batch Loss = 0.0007310406279250945
831, epoch_train_loss=0.0007310406279250945
Epoch 832
Epoch 832 :: Batch 0/1
Batch Loss = 0.0007310406279164455
832, epoch_train_loss=0.0007310406279164455
Epoch 833
Epoch 833 :: Batch 0/1
Batch Loss = 0.00073104062790779
833, epoch_train_loss=0.00073104062790779
Epoch 834
Epoch 834 :: Batch 0/1
Batch Loss = 0.0007310406278991278
834, epoch_train_loss=0.0007310406278991278
Epoch 835
Epoch 835 :: Batch 0/1
Batch Loss = 0.0007310406278904589
835, epoch_train_loss=0.0007310406278904589
Epoch 836
Epoch 836 :: Batch 0/1
Batch Loss = 0.0007310406278817833
836, epoch_train_loss=0.0007310406278817833
Epoch 837
Epoch 837 :: Batch 0/1
Batch Loss = 0.000731040627873101
837, epoch_train_loss=0.000731040627873101
Epoch 838
Epoch 838 :: Batch 0/1
Batch Loss = 0.0007310406278644122
838, epoch_train_loss=0.0007310406278644122
Epoch 839
Epoch 839 :: Batch 0/1
Batch Loss = 0.0007310406278557167
839, epoch_train_loss=0.0007310406278557167
Epoch 840
Epoch 840 :: Batch 0/1
Batch Loss = 0.0007310406278470146
840, epoch_train_loss=0.0007310406278470146
Epoch 841
Epoch 841 :: Batch 0/1
Batch Loss = 0.0007310406278383058
841, epoch_train_loss=0.0007310406278383058
Epoch 842
Epoch 842 :: Batch 0/1
Batch Loss = 0.0007310406278295903
842, epoch_train_loss=0.0007310406278295903
Epoch 843
Epoch 843 :: Batch 0/1
Batch Loss = 0.0007310406278208682
843, epoch_train_loss=0.0007310406278208682
Epoch 844
Epoch 844 :: Batch 0/1
Batch Loss = 0.0007310406278121395
844, epoch_train_loss=0.0007310406278121395
Epoch 845
Epoch 845 :: Batch 0/1
Batch Loss = 0.0007310406278034041
845, epoch_train_loss=0.0007310406278034041
Epoch 846
Epoch 846 :: Batch 0/1
Batch Loss = 0.0007310406277946619
846, epoch_train_loss=0.0007310406277946619
Epoch 847
Epoch 847 :: Batch 0/1
Batch Loss = 0.0007310406277859133
847, epoch_train_loss=0.0007310406277859133
Epoch 848
Epoch 848 :: Batch 0/1
Batch Loss = 0.0007310406277771578
848, epoch_train_loss=0.0007310406277771578
Epoch 849
Epoch 849 :: Batch 0/1
Batch Loss = 0.0007310406277683959
849, epoch_train_loss=0.0007310406277683959
Epoch 850
Epoch 850 :: Batch 0/1
Batch Loss = 0.0007310406277596272
850, epoch_train_loss=0.0007310406277596272
Epoch 851
Epoch 851 :: Batch 0/1
Batch Loss = 0.000731040627750852
851, epoch_train_loss=0.000731040627750852
Epoch 852
Epoch 852 :: Batch 0/1
Batch Loss = 0.00073104062774207
852, epoch_train_loss=0.00073104062774207
Epoch 853
Epoch 853 :: Batch 0/1
Batch Loss = 0.0007310406277332814
853, epoch_train_loss=0.0007310406277332814
Epoch 854
Epoch 854 :: Batch 0/1
Batch Loss = 0.0007310406277244863
854, epoch_train_loss=0.0007310406277244863
Epoch 855
Epoch 855 :: Batch 0/1
Batch Loss = 0.0007310406277156845
855, epoch_train_loss=0.0007310406277156845
Epoch 856
Epoch 856 :: Batch 0/1
Batch Loss = 0.0007310406277068759
856, epoch_train_loss=0.0007310406277068759
Epoch 857
Epoch 857 :: Batch 0/1
Batch Loss = 0.0007310406276980609
857, epoch_train_loss=0.0007310406276980609
Epoch 858
Epoch 858 :: Batch 0/1
Batch Loss = 0.0007310406276892391
858, epoch_train_loss=0.0007310406276892391
Epoch 859
Epoch 859 :: Batch 0/1
Batch Loss = 0.0007310406276804108
859, epoch_train_loss=0.0007310406276804108
Epoch 860
Epoch 860 :: Batch 0/1
Batch Loss = 0.0007310406276715758
860, epoch_train_loss=0.0007310406276715758
Epoch 861
Epoch 861 :: Batch 0/1
Batch Loss = 0.0007310406276627343
861, epoch_train_loss=0.0007310406276627343
Epoch 862
Epoch 862 :: Batch 0/1
Batch Loss = 0.000731040627653886
862, epoch_train_loss=0.000731040627653886
Epoch 863
Epoch 863 :: Batch 0/1
Batch Loss = 0.0007310406276450311
863, epoch_train_loss=0.0007310406276450311
Epoch 864
Epoch 864 :: Batch 0/1
Batch Loss = 0.0007310406276361697
864, epoch_train_loss=0.0007310406276361697
Epoch 865
Epoch 865 :: Batch 0/1
Batch Loss = 0.0007310406276273014
865, epoch_train_loss=0.0007310406276273014
Epoch 866
Epoch 866 :: Batch 0/1
Batch Loss = 0.0007310406276184269
866, epoch_train_loss=0.0007310406276184269
Epoch 867
Epoch 867 :: Batch 0/1
Batch Loss = 0.0007310406276095455
867, epoch_train_loss=0.0007310406276095455
Epoch 868
Epoch 868 :: Batch 0/1
Batch Loss = 0.0007310406276006574
868, epoch_train_loss=0.0007310406276006574
Epoch 869
Epoch 869 :: Batch 0/1
Batch Loss = 0.0007310406275917627
869, epoch_train_loss=0.0007310406275917627
Epoch 870
Epoch 870 :: Batch 0/1
Batch Loss = 0.0007310406275828616
870, epoch_train_loss=0.0007310406275828616
Epoch 871
Epoch 871 :: Batch 0/1
Batch Loss = 0.0007310406275739538
871, epoch_train_loss=0.0007310406275739538
Epoch 872
Epoch 872 :: Batch 0/1
Batch Loss = 0.0007310406275650394
872, epoch_train_loss=0.0007310406275650394
Epoch 873
Epoch 873 :: Batch 0/1
Batch Loss = 0.0007310406275561183
873, epoch_train_loss=0.0007310406275561183
Epoch 874
Epoch 874 :: Batch 0/1
Batch Loss = 0.0007310406275471906
874, epoch_train_loss=0.0007310406275471906
Epoch 875
Epoch 875 :: Batch 0/1
Batch Loss = 0.0007310406275382564
875, epoch_train_loss=0.0007310406275382564
Epoch 876
Epoch 876 :: Batch 0/1
Batch Loss = 0.0007310406275293154
876, epoch_train_loss=0.0007310406275293154
Epoch 877
Epoch 877 :: Batch 0/1
Batch Loss = 0.0007310406275203678
877, epoch_train_loss=0.0007310406275203678
Epoch 878
Epoch 878 :: Batch 0/1
Batch Loss = 0.0007310406275114138
878, epoch_train_loss=0.0007310406275114138
Epoch 879
Epoch 879 :: Batch 0/1
Batch Loss = 0.0007310406275024531
879, epoch_train_loss=0.0007310406275024531
Epoch 880
Epoch 880 :: Batch 0/1
Batch Loss = 0.0007310406274934856
880, epoch_train_loss=0.0007310406274934856
Epoch 881
Epoch 881 :: Batch 0/1
Batch Loss = 0.0007310406274845117
881, epoch_train_loss=0.0007310406274845117
Epoch 882
Epoch 882 :: Batch 0/1
Batch Loss = 0.0007310406274755311
882, epoch_train_loss=0.0007310406274755311
Epoch 883
Epoch 883 :: Batch 0/1
Batch Loss = 0.000731040627466544
883, epoch_train_loss=0.000731040627466544
Epoch 884
Epoch 884 :: Batch 0/1
Batch Loss = 0.0007310406274575502
884, epoch_train_loss=0.0007310406274575502
Epoch 885
Epoch 885 :: Batch 0/1
Batch Loss = 0.0007310406274485498
885, epoch_train_loss=0.0007310406274485498
Epoch 886
Epoch 886 :: Batch 0/1
Batch Loss = 0.0007310406274395429
886, epoch_train_loss=0.0007310406274395429
Epoch 887
Epoch 887 :: Batch 0/1
Batch Loss = 0.0007310406274305292
887, epoch_train_loss=0.0007310406274305292
Epoch 888
Epoch 888 :: Batch 0/1
Batch Loss = 0.0007310406274215091
888, epoch_train_loss=0.0007310406274215091
Epoch 889
Epoch 889 :: Batch 0/1
Batch Loss = 0.0007310406274124824
889, epoch_train_loss=0.0007310406274124824
Epoch 890
Epoch 890 :: Batch 0/1
Batch Loss = 0.0007310406274034489
890, epoch_train_loss=0.0007310406274034489
Epoch 891
Epoch 891 :: Batch 0/1
Batch Loss = 0.000731040627394409
891, epoch_train_loss=0.000731040627394409
Epoch 892
Epoch 892 :: Batch 0/1
Batch Loss = 0.0007310406273853626
892, epoch_train_loss=0.0007310406273853626
Epoch 893
Epoch 893 :: Batch 0/1
Batch Loss = 0.0007310406273763094
893, epoch_train_loss=0.0007310406273763094
Epoch 894
Epoch 894 :: Batch 0/1
Batch Loss = 0.0007310406273672496
894, epoch_train_loss=0.0007310406273672496
Epoch 895
Epoch 895 :: Batch 0/1
Batch Loss = 0.0007310406273581833
895, epoch_train_loss=0.0007310406273581833
Epoch 896
Epoch 896 :: Batch 0/1
Batch Loss = 0.0007310406273491103
896, epoch_train_loss=0.0007310406273491103
Epoch 897
Epoch 897 :: Batch 0/1
Batch Loss = 0.0007310406273400309
897, epoch_train_loss=0.0007310406273400309
Epoch 898
Epoch 898 :: Batch 0/1
Batch Loss = 0.0007310406273309448
898, epoch_train_loss=0.0007310406273309448
Epoch 899
Epoch 899 :: Batch 0/1
Batch Loss = 0.0007310406273218521
899, epoch_train_loss=0.0007310406273218521
Epoch 900
Epoch 900 :: Batch 0/1
Batch Loss = 0.0007310406273127528
900, epoch_train_loss=0.0007310406273127528
Epoch 901
Epoch 901 :: Batch 0/1
Batch Loss = 0.000731040627303647
901, epoch_train_loss=0.000731040627303647
Epoch 902
Epoch 902 :: Batch 0/1
Batch Loss = 0.0007310406272945346
902, epoch_train_loss=0.0007310406272945346
Epoch 903
Epoch 903 :: Batch 0/1
Batch Loss = 0.0007310406272854155
903, epoch_train_loss=0.0007310406272854155
Epoch 904
Epoch 904 :: Batch 0/1
Batch Loss = 0.0007310406272762899
904, epoch_train_loss=0.0007310406272762899
Epoch 905
Epoch 905 :: Batch 0/1
Batch Loss = 0.0007310406272671578
905, epoch_train_loss=0.0007310406272671578
Epoch 906
Epoch 906 :: Batch 0/1
Batch Loss = 0.0007310406272580189
906, epoch_train_loss=0.0007310406272580189
Epoch 907
Epoch 907 :: Batch 0/1
Batch Loss = 0.0007310406272488737
907, epoch_train_loss=0.0007310406272488737
Epoch 908
Epoch 908 :: Batch 0/1
Batch Loss = 0.0007310406272397217
908, epoch_train_loss=0.0007310406272397217
Epoch 909
Epoch 909 :: Batch 0/1
Batch Loss = 0.0007310406272305631
909, epoch_train_loss=0.0007310406272305631
Epoch 910
Epoch 910 :: Batch 0/1
Batch Loss = 0.000731040627221398
910, epoch_train_loss=0.000731040627221398
Epoch 911
Epoch 911 :: Batch 0/1
Batch Loss = 0.0007310406272122266
911, epoch_train_loss=0.0007310406272122266
Epoch 912
Epoch 912 :: Batch 0/1
Batch Loss = 0.0007310406272030483
912, epoch_train_loss=0.0007310406272030483
Epoch 913
Epoch 913 :: Batch 0/1
Batch Loss = 0.0007310406271938635
913, epoch_train_loss=0.0007310406271938635
Epoch 914
Epoch 914 :: Batch 0/1
Batch Loss = 0.0007310406271846721
914, epoch_train_loss=0.0007310406271846721
Epoch 915
Epoch 915 :: Batch 0/1
Batch Loss = 0.0007310406271754741
915, epoch_train_loss=0.0007310406271754741
Epoch 916
Epoch 916 :: Batch 0/1
Batch Loss = 0.0007310406271662696
916, epoch_train_loss=0.0007310406271662696
Epoch 917
Epoch 917 :: Batch 0/1
Batch Loss = 0.0007310406271570586
917, epoch_train_loss=0.0007310406271570586
Epoch 918
Epoch 918 :: Batch 0/1
Batch Loss = 0.0007310406271478409
918, epoch_train_loss=0.0007310406271478409
Epoch 919
Epoch 919 :: Batch 0/1
Batch Loss = 0.0007310406271386167
919, epoch_train_loss=0.0007310406271386167
Epoch 920
Epoch 920 :: Batch 0/1
Batch Loss = 0.0007310406271293858
920, epoch_train_loss=0.0007310406271293858
Epoch 921
Epoch 921 :: Batch 0/1
Batch Loss = 0.0007310406271201485
921, epoch_train_loss=0.0007310406271201485
Epoch 922
Epoch 922 :: Batch 0/1
Batch Loss = 0.0007310406271109047
922, epoch_train_loss=0.0007310406271109047
Epoch 923
Epoch 923 :: Batch 0/1
Batch Loss = 0.0007310406271016541
923, epoch_train_loss=0.0007310406271016541
Epoch 924
Epoch 924 :: Batch 0/1
Batch Loss = 0.0007310406270923971
924, epoch_train_loss=0.0007310406270923971
Epoch 925
Epoch 925 :: Batch 0/1
Batch Loss = 0.0007310406270831334
925, epoch_train_loss=0.0007310406270831334
Epoch 926
Epoch 926 :: Batch 0/1
Batch Loss = 0.0007310406270738633
926, epoch_train_loss=0.0007310406270738633
Epoch 927
Epoch 927 :: Batch 0/1
Batch Loss = 0.0007310406270645866
927, epoch_train_loss=0.0007310406270645866
Epoch 928
Epoch 928 :: Batch 0/1
Batch Loss = 0.0007310406270553032
928, epoch_train_loss=0.0007310406270553032
Epoch 929
Epoch 929 :: Batch 0/1
Batch Loss = 0.0007310406270460135
929, epoch_train_loss=0.0007310406270460135
Epoch 930
Epoch 930 :: Batch 0/1
Batch Loss = 0.0007310406270367171
930, epoch_train_loss=0.0007310406270367171
Epoch 931
Epoch 931 :: Batch 0/1
Batch Loss = 0.000731040627027414
931, epoch_train_loss=0.000731040627027414
Epoch 932
Epoch 932 :: Batch 0/1
Batch Loss = 0.0007310406270181046
932, epoch_train_loss=0.0007310406270181046
Epoch 933
Epoch 933 :: Batch 0/1
Batch Loss = 0.0007310406270087885
933, epoch_train_loss=0.0007310406270087885
Epoch 934
Epoch 934 :: Batch 0/1
Batch Loss = 0.0007310406269994659
934, epoch_train_loss=0.0007310406269994659
Epoch 935
Epoch 935 :: Batch 0/1
Batch Loss = 0.0007310406269901366
935, epoch_train_loss=0.0007310406269901366
Epoch 936
Epoch 936 :: Batch 0/1
Batch Loss = 0.000731040626980801
936, epoch_train_loss=0.000731040626980801
Epoch 937
Epoch 937 :: Batch 0/1
Batch Loss = 0.0007310406269714587
937, epoch_train_loss=0.0007310406269714587
Epoch 938
Epoch 938 :: Batch 0/1
Batch Loss = 0.0007310406269621099
938, epoch_train_loss=0.0007310406269621099
Epoch 939
Epoch 939 :: Batch 0/1
Batch Loss = 0.0007310406269527544
939, epoch_train_loss=0.0007310406269527544
Epoch 940
Epoch 940 :: Batch 0/1
Batch Loss = 0.0007310406269433926
940, epoch_train_loss=0.0007310406269433926
Epoch 941
Epoch 941 :: Batch 0/1
Batch Loss = 0.0007310406269340241
941, epoch_train_loss=0.0007310406269340241
Epoch 942
Epoch 942 :: Batch 0/1
Batch Loss = 0.000731040626924649
942, epoch_train_loss=0.000731040626924649
Epoch 943
Epoch 943 :: Batch 0/1
Batch Loss = 0.0007310406269152676
943, epoch_train_loss=0.0007310406269152676
Epoch 944
Epoch 944 :: Batch 0/1
Batch Loss = 0.0007310406269058794
944, epoch_train_loss=0.0007310406269058794
Epoch 945
Epoch 945 :: Batch 0/1
Batch Loss = 0.0007310406268964848
945, epoch_train_loss=0.0007310406268964848
Epoch 946
Epoch 946 :: Batch 0/1
Batch Loss = 0.0007310406268870834
946, epoch_train_loss=0.0007310406268870834
Epoch 947
Epoch 947 :: Batch 0/1
Batch Loss = 0.0007310406268776759
947, epoch_train_loss=0.0007310406268776759
Epoch 948
Epoch 948 :: Batch 0/1
Batch Loss = 0.0007310406268682615
948, epoch_train_loss=0.0007310406268682615
Epoch 949
Epoch 949 :: Batch 0/1
Batch Loss = 0.0007310406268588407
949, epoch_train_loss=0.0007310406268588407
Epoch 950
Epoch 950 :: Batch 0/1
Batch Loss = 0.0007310406268494134
950, epoch_train_loss=0.0007310406268494134
Epoch 951
Epoch 951 :: Batch 0/1
Batch Loss = 0.0007310406268399795
951, epoch_train_loss=0.0007310406268399795
Epoch 952
Epoch 952 :: Batch 0/1
Batch Loss = 0.0007310406268305389
952, epoch_train_loss=0.0007310406268305389
Epoch 953
Epoch 953 :: Batch 0/1
Batch Loss = 0.0007310406268210921
953, epoch_train_loss=0.0007310406268210921
Epoch 954
Epoch 954 :: Batch 0/1
Batch Loss = 0.0007310406268116385
954, epoch_train_loss=0.0007310406268116385
Epoch 955
Epoch 955 :: Batch 0/1
Batch Loss = 0.0007310406268021785
955, epoch_train_loss=0.0007310406268021785
Epoch 956
Epoch 956 :: Batch 0/1
Batch Loss = 0.0007310406267927119
956, epoch_train_loss=0.0007310406267927119
Epoch 957
Epoch 957 :: Batch 0/1
Batch Loss = 0.0007310406267832387
957, epoch_train_loss=0.0007310406267832387
Epoch 958
Epoch 958 :: Batch 0/1
Batch Loss = 0.0007310406267737592
958, epoch_train_loss=0.0007310406267737592
Epoch 959
Epoch 959 :: Batch 0/1
Batch Loss = 0.0007310406267642729
959, epoch_train_loss=0.0007310406267642729
Epoch 960
Epoch 960 :: Batch 0/1
Batch Loss = 0.0007310406267547803
960, epoch_train_loss=0.0007310406267547803
Epoch 961
Epoch 961 :: Batch 0/1
Batch Loss = 0.0007310406267452811
961, epoch_train_loss=0.0007310406267452811
Epoch 962
Epoch 962 :: Batch 0/1
Batch Loss = 0.0007310406267357752
962, epoch_train_loss=0.0007310406267357752
Epoch 963
Epoch 963 :: Batch 0/1
Batch Loss = 0.000731040626726263
963, epoch_train_loss=0.000731040626726263
Epoch 964
Epoch 964 :: Batch 0/1
Batch Loss = 0.0007310406267167441
964, epoch_train_loss=0.0007310406267167441
Epoch 965
Epoch 965 :: Batch 0/1
Batch Loss = 0.0007310406267072189
965, epoch_train_loss=0.0007310406267072189
Epoch 966
Epoch 966 :: Batch 0/1
Batch Loss = 0.0007310406266976869
966, epoch_train_loss=0.0007310406266976869
Epoch 967
Epoch 967 :: Batch 0/1
Batch Loss = 0.0007310406266881484
967, epoch_train_loss=0.0007310406266881484
Epoch 968
Epoch 968 :: Batch 0/1
Batch Loss = 0.0007310406266786036
968, epoch_train_loss=0.0007310406266786036
Epoch 969
Epoch 969 :: Batch 0/1
Batch Loss = 0.000731040626669052
969, epoch_train_loss=0.000731040626669052
Epoch 970
Epoch 970 :: Batch 0/1
Batch Loss = 0.0007310406266594942
970, epoch_train_loss=0.0007310406266594942
Epoch 971
Epoch 971 :: Batch 0/1
Batch Loss = 0.0007310406266499296
971, epoch_train_loss=0.0007310406266499296
Epoch 972
Epoch 972 :: Batch 0/1
Batch Loss = 0.0007310406266403585
972, epoch_train_loss=0.0007310406266403585
Epoch 973
Epoch 973 :: Batch 0/1
Batch Loss = 0.0007310406266307811
973, epoch_train_loss=0.0007310406266307811
Epoch 974
Epoch 974 :: Batch 0/1
Batch Loss = 0.000731040626621197
974, epoch_train_loss=0.000731040626621197
Epoch 975
Epoch 975 :: Batch 0/1
Batch Loss = 0.0007310406266116064
975, epoch_train_loss=0.0007310406266116064
Epoch 976
Epoch 976 :: Batch 0/1
Batch Loss = 0.0007310406266020093
976, epoch_train_loss=0.0007310406266020093
Epoch 977
Epoch 977 :: Batch 0/1
Batch Loss = 0.0007310406265924057
977, epoch_train_loss=0.0007310406265924057
Epoch 978
Epoch 978 :: Batch 0/1
Batch Loss = 0.0007310406265827956
978, epoch_train_loss=0.0007310406265827956
Epoch 979
Epoch 979 :: Batch 0/1
Batch Loss = 0.0007310406265731789
979, epoch_train_loss=0.0007310406265731789
Epoch 980
Epoch 980 :: Batch 0/1
Batch Loss = 0.0007310406265635557
980, epoch_train_loss=0.0007310406265635557
Epoch 981
Epoch 981 :: Batch 0/1
Batch Loss = 0.0007310406265539261
981, epoch_train_loss=0.0007310406265539261
Epoch 982
Epoch 982 :: Batch 0/1
Batch Loss = 0.0007310406265442899
982, epoch_train_loss=0.0007310406265442899
Epoch 983
Epoch 983 :: Batch 0/1
Batch Loss = 0.0007310406265346472
983, epoch_train_loss=0.0007310406265346472
Epoch 984
Epoch 984 :: Batch 0/1
Batch Loss = 0.0007310406265249979
984, epoch_train_loss=0.0007310406265249979
Epoch 985
Epoch 985 :: Batch 0/1
Batch Loss = 0.0007310406265153422
985, epoch_train_loss=0.0007310406265153422
Epoch 986
Epoch 986 :: Batch 0/1
Batch Loss = 0.00073104062650568
986, epoch_train_loss=0.00073104062650568
Epoch 987
Epoch 987 :: Batch 0/1
Batch Loss = 0.0007310406264960113
987, epoch_train_loss=0.0007310406264960113
Epoch 988
Epoch 988 :: Batch 0/1
Batch Loss = 0.000731040626486336
988, epoch_train_loss=0.000731040626486336
Epoch 989
Epoch 989 :: Batch 0/1
Batch Loss = 0.0007310406264766543
989, epoch_train_loss=0.0007310406264766543
Epoch 990
Epoch 990 :: Batch 0/1
Batch Loss = 0.0007310406264669659
990, epoch_train_loss=0.0007310406264669659
Epoch 991
Epoch 991 :: Batch 0/1
Batch Loss = 0.0007310406264572712
991, epoch_train_loss=0.0007310406264572712
Epoch 992
Epoch 992 :: Batch 0/1
Batch Loss = 0.0007310406264475699
992, epoch_train_loss=0.0007310406264475699
Epoch 993
Epoch 993 :: Batch 0/1
Batch Loss = 0.0007310406264378619
993, epoch_train_loss=0.0007310406264378619
Epoch 994
Epoch 994 :: Batch 0/1
Batch Loss = 0.0007310406264281477
994, epoch_train_loss=0.0007310406264281477
Epoch 995
Epoch 995 :: Batch 0/1
Batch Loss = 0.000731040626418427
995, epoch_train_loss=0.000731040626418427
Epoch 996
Epoch 996 :: Batch 0/1
Batch Loss = 0.0007310406264086997
996, epoch_train_loss=0.0007310406264086997
Epoch 997
Epoch 997 :: Batch 0/1
Batch Loss = 0.0007310406263989659
997, epoch_train_loss=0.0007310406263989659
Epoch 998
Epoch 998 :: Batch 0/1
Batch Loss = 0.0007310406263892255
998, epoch_train_loss=0.0007310406263892255
Epoch 999
Epoch 999 :: Batch 0/1
Batch Loss = 0.0007310406263794787
999, epoch_train_loss=0.0007310406263794787
Epoch 1000
Epoch 1000 :: Batch 0/1
Batch Loss = 0.0007310406263697254
1000, epoch_train_loss=0.0007310406263697254
Epoch 1001
Epoch 1001 :: Batch 0/1
Batch Loss = 0.0007310406263599657
1001, epoch_train_loss=0.0007310406263599657
Epoch 1002
Epoch 1002 :: Batch 0/1
Batch Loss = 0.0007310406263501994
1002, epoch_train_loss=0.0007310406263501994
Epoch 1003
Epoch 1003 :: Batch 0/1
Batch Loss = 0.0007310406263404265
1003, epoch_train_loss=0.0007310406263404265
Epoch 1004
Epoch 1004 :: Batch 0/1
Batch Loss = 0.0007310406263306471
1004, epoch_train_loss=0.0007310406263306471
Epoch 1005
Epoch 1005 :: Batch 0/1
Batch Loss = 0.0007310406263208615
1005, epoch_train_loss=0.0007310406263208615
Epoch 1006
Epoch 1006 :: Batch 0/1
Batch Loss = 0.0007310406263110693
1006, epoch_train_loss=0.0007310406263110693
Epoch 1007
Epoch 1007 :: Batch 0/1
Batch Loss = 0.0007310406263012704
1007, epoch_train_loss=0.0007310406263012704
Epoch 1008
Epoch 1008 :: Batch 0/1
Batch Loss = 0.0007310406262914649
1008, epoch_train_loss=0.0007310406262914649
Epoch 1009
Epoch 1009 :: Batch 0/1
Batch Loss = 0.0007310406262816533
1009, epoch_train_loss=0.0007310406262816533
Epoch 1010
Epoch 1010 :: Batch 0/1
Batch Loss = 0.000731040626271835
1010, epoch_train_loss=0.000731040626271835
Epoch 1011
Epoch 1011 :: Batch 0/1
Batch Loss = 0.0007310406262620102
1011, epoch_train_loss=0.0007310406262620102
Epoch 1012
Epoch 1012 :: Batch 0/1
Batch Loss = 0.0007310406262521789
1012, epoch_train_loss=0.0007310406262521789
Epoch 1013
Epoch 1013 :: Batch 0/1
Batch Loss = 0.0007310406262423413
1013, epoch_train_loss=0.0007310406262423413
Epoch 1014
Epoch 1014 :: Batch 0/1
Batch Loss = 0.0007310406262324971
1014, epoch_train_loss=0.0007310406262324971
Epoch 1015
Epoch 1015 :: Batch 0/1
Batch Loss = 0.0007310406262226463
1015, epoch_train_loss=0.0007310406262226463
Epoch 1016
Epoch 1016 :: Batch 0/1
Batch Loss = 0.0007310406262127892
1016, epoch_train_loss=0.0007310406262127892
Epoch 1017
Epoch 1017 :: Batch 0/1
Batch Loss = 0.0007310406262029253
1017, epoch_train_loss=0.0007310406262029253
Epoch 1018
Epoch 1018 :: Batch 0/1
Batch Loss = 0.0007310406261930552
1018, epoch_train_loss=0.0007310406261930552
Epoch 1019
Epoch 1019 :: Batch 0/1
Batch Loss = 0.0007310406261831786
1019, epoch_train_loss=0.0007310406261831786
Epoch 1020
Epoch 1020 :: Batch 0/1
Batch Loss = 0.0007310406261732954
1020, epoch_train_loss=0.0007310406261732954
Epoch 1021
Epoch 1021 :: Batch 0/1
Batch Loss = 0.0007310406261634058
1021, epoch_train_loss=0.0007310406261634058
Epoch 1022
Epoch 1022 :: Batch 0/1
Batch Loss = 0.0007310406261535095
1022, epoch_train_loss=0.0007310406261535095
Epoch 1023
Epoch 1023 :: Batch 0/1
Batch Loss = 0.000731040626143607
1023, epoch_train_loss=0.000731040626143607
Epoch 1024
Epoch 1024 :: Batch 0/1
Batch Loss = 0.0007310406261336979
1024, epoch_train_loss=0.0007310406261336979
Epoch 1025
Epoch 1025 :: Batch 0/1
Batch Loss = 0.0007310406261237823
1025, epoch_train_loss=0.0007310406261237823
Epoch 1026
Epoch 1026 :: Batch 0/1
Batch Loss = 0.0007310406261138603
1026, epoch_train_loss=0.0007310406261138603
Epoch 1027
Epoch 1027 :: Batch 0/1
Batch Loss = 0.0007310406261039316
1027, epoch_train_loss=0.0007310406261039316
Epoch 1028
Epoch 1028 :: Batch 0/1
Batch Loss = 0.0007310406260939967
1028, epoch_train_loss=0.0007310406260939967
Epoch 1029
Epoch 1029 :: Batch 0/1
Batch Loss = 0.0007310406260840552
1029, epoch_train_loss=0.0007310406260840552
Epoch 1030
Epoch 1030 :: Batch 0/1
Batch Loss = 0.0007310406260741071
1030, epoch_train_loss=0.0007310406260741071
Epoch 1031
Epoch 1031 :: Batch 0/1
Batch Loss = 0.0007310406260641527
1031, epoch_train_loss=0.0007310406260641527
Epoch 1032
Epoch 1032 :: Batch 0/1
Batch Loss = 0.0007310406260541917
1032, epoch_train_loss=0.0007310406260541917
Epoch 1033
Epoch 1033 :: Batch 0/1
Batch Loss = 0.0007310406260442243
1033, epoch_train_loss=0.0007310406260442243
Epoch 1034
Epoch 1034 :: Batch 0/1
Batch Loss = 0.0007310406260342503
1034, epoch_train_loss=0.0007310406260342503
Epoch 1035
Epoch 1035 :: Batch 0/1
Batch Loss = 0.0007310406260242701
1035, epoch_train_loss=0.0007310406260242701
Epoch 1036
Epoch 1036 :: Batch 0/1
Batch Loss = 0.0007310406260142832
1036, epoch_train_loss=0.0007310406260142832
Epoch 1037
Epoch 1037 :: Batch 0/1
Batch Loss = 0.0007310406260042898
1037, epoch_train_loss=0.0007310406260042898
Epoch 1038
Epoch 1038 :: Batch 0/1
Batch Loss = 0.0007310406259942902
1038, epoch_train_loss=0.0007310406259942902
Epoch 1039
Epoch 1039 :: Batch 0/1
Batch Loss = 0.0007310406259842838
1039, epoch_train_loss=0.0007310406259842838
Epoch 1040
Epoch 1040 :: Batch 0/1
Batch Loss = 0.000731040625974271
1040, epoch_train_loss=0.000731040625974271
Epoch 1041
Epoch 1041 :: Batch 0/1
Batch Loss = 0.0007310406259642519
1041, epoch_train_loss=0.0007310406259642519
Epoch 1042
Epoch 1042 :: Batch 0/1
Batch Loss = 0.0007310406259542262
1042, epoch_train_loss=0.0007310406259542262
Epoch 1043
Epoch 1043 :: Batch 0/1
Batch Loss = 0.000731040625944194
1043, epoch_train_loss=0.000731040625944194
Epoch 1044
Epoch 1044 :: Batch 0/1
Batch Loss = 0.0007310406259341554
1044, epoch_train_loss=0.0007310406259341554
Epoch 1045
Epoch 1045 :: Batch 0/1
Batch Loss = 0.0007310406259241104
1045, epoch_train_loss=0.0007310406259241104
Epoch 1046
Epoch 1046 :: Batch 0/1
Batch Loss = 0.0007310406259140587
1046, epoch_train_loss=0.0007310406259140587
Epoch 1047
Epoch 1047 :: Batch 0/1
Batch Loss = 0.0007310406259040007
1047, epoch_train_loss=0.0007310406259040007
Epoch 1048
Epoch 1048 :: Batch 0/1
Batch Loss = 0.0007310406258939361
1048, epoch_train_loss=0.0007310406258939361
Epoch 1049
Epoch 1049 :: Batch 0/1
Batch Loss = 0.0007310406258838652
1049, epoch_train_loss=0.0007310406258838652
Epoch 1050
Epoch 1050 :: Batch 0/1
Batch Loss = 0.0007310406258737877
1050, epoch_train_loss=0.0007310406258737877
Epoch 1051
Epoch 1051 :: Batch 0/1
Batch Loss = 0.0007310406258637038
1051, epoch_train_loss=0.0007310406258637038
Epoch 1052
Epoch 1052 :: Batch 0/1
Batch Loss = 0.0007310406258536134
1052, epoch_train_loss=0.0007310406258536134
Epoch 1053
Epoch 1053 :: Batch 0/1
Batch Loss = 0.0007310406258435165
1053, epoch_train_loss=0.0007310406258435165
Epoch 1054
Epoch 1054 :: Batch 0/1
Batch Loss = 0.0007310406258334133
1054, epoch_train_loss=0.0007310406258334133
Epoch 1055
Epoch 1055 :: Batch 0/1
Batch Loss = 0.0007310406258233035
1055, epoch_train_loss=0.0007310406258233035
Epoch 1056
Epoch 1056 :: Batch 0/1
Batch Loss = 0.0007310406258131874
1056, epoch_train_loss=0.0007310406258131874
Epoch 1057
Epoch 1057 :: Batch 0/1
Batch Loss = 0.0007310406258030646
1057, epoch_train_loss=0.0007310406258030646
Epoch 1058
Epoch 1058 :: Batch 0/1
Batch Loss = 0.0007310406257929355
1058, epoch_train_loss=0.0007310406257929355
Epoch 1059
Epoch 1059 :: Batch 0/1
Batch Loss = 0.0007310406257827999
1059, epoch_train_loss=0.0007310406257827999
Epoch 1060
Epoch 1060 :: Batch 0/1
Batch Loss = 0.0007310406257726577
1060, epoch_train_loss=0.0007310406257726577
Epoch 1061
Epoch 1061 :: Batch 0/1
Batch Loss = 0.0007310406257625092
1061, epoch_train_loss=0.0007310406257625092
Epoch 1062
Epoch 1062 :: Batch 0/1
Batch Loss = 0.0007310406257523542
1062, epoch_train_loss=0.0007310406257523542
Epoch 1063
Epoch 1063 :: Batch 0/1
Batch Loss = 0.0007310406257421928
1063, epoch_train_loss=0.0007310406257421928
Epoch 1064
Epoch 1064 :: Batch 0/1
Batch Loss = 0.0007310406257320249
1064, epoch_train_loss=0.0007310406257320249
Epoch 1065
Epoch 1065 :: Batch 0/1
Batch Loss = 0.0007310406257218504
1065, epoch_train_loss=0.0007310406257218504
Epoch 1066
Epoch 1066 :: Batch 0/1
Batch Loss = 0.0007310406257116696
1066, epoch_train_loss=0.0007310406257116696
Epoch 1067
Epoch 1067 :: Batch 0/1
Batch Loss = 0.0007310406257014825
1067, epoch_train_loss=0.0007310406257014825
Epoch 1068
Epoch 1068 :: Batch 0/1
Batch Loss = 0.0007310406256912885
1068, epoch_train_loss=0.0007310406256912885
Epoch 1069
Epoch 1069 :: Batch 0/1
Batch Loss = 0.0007310406256810884
1069, epoch_train_loss=0.0007310406256810884
Epoch 1070
Epoch 1070 :: Batch 0/1
Batch Loss = 0.0007310406256708817
1070, epoch_train_loss=0.0007310406256708817
Epoch 1071
Epoch 1071 :: Batch 0/1
Batch Loss = 0.0007310406256606686
1071, epoch_train_loss=0.0007310406256606686
Epoch 1072
Epoch 1072 :: Batch 0/1
Batch Loss = 0.0007310406256504491
1072, epoch_train_loss=0.0007310406256504491
Epoch 1073
Epoch 1073 :: Batch 0/1
Batch Loss = 0.0007310406256402231
1073, epoch_train_loss=0.0007310406256402231
Epoch 1074
Epoch 1074 :: Batch 0/1
Batch Loss = 0.0007310406256299905
1074, epoch_train_loss=0.0007310406256299905
Epoch 1075
Epoch 1075 :: Batch 0/1
Batch Loss = 0.0007310406256197516
1075, epoch_train_loss=0.0007310406256197516
Epoch 1076
Epoch 1076 :: Batch 0/1
Batch Loss = 0.0007310406256095063
1076, epoch_train_loss=0.0007310406256095063
Epoch 1077
Epoch 1077 :: Batch 0/1
Batch Loss = 0.0007310406255992543
1077, epoch_train_loss=0.0007310406255992543
Epoch 1078
Epoch 1078 :: Batch 0/1
Batch Loss = 0.0007310406255889962
1078, epoch_train_loss=0.0007310406255889962
Epoch 1079
Epoch 1079 :: Batch 0/1
Batch Loss = 0.0007310406255787315
1079, epoch_train_loss=0.0007310406255787315
Epoch 1080
Epoch 1080 :: Batch 0/1
Batch Loss = 0.0007310406255684604
1080, epoch_train_loss=0.0007310406255684604
Epoch 1081
Epoch 1081 :: Batch 0/1
Batch Loss = 0.0007310406255581827
1081, epoch_train_loss=0.0007310406255581827
Epoch 1082
Epoch 1082 :: Batch 0/1
Batch Loss = 0.0007310406255478987
1082, epoch_train_loss=0.0007310406255478987
Epoch 1083
Epoch 1083 :: Batch 0/1
Batch Loss = 0.0007310406255376081
1083, epoch_train_loss=0.0007310406255376081
Epoch 1084
Epoch 1084 :: Batch 0/1
Batch Loss = 0.0007310406255273111
1084, epoch_train_loss=0.0007310406255273111
Epoch 1085
Epoch 1085 :: Batch 0/1
Batch Loss = 0.0007310406255170077
1085, epoch_train_loss=0.0007310406255170077
Epoch 1086
Epoch 1086 :: Batch 0/1
Batch Loss = 0.0007310406255066978
1086, epoch_train_loss=0.0007310406255066978
Epoch 1087
Epoch 1087 :: Batch 0/1
Batch Loss = 0.0007310406254963813
1087, epoch_train_loss=0.0007310406254963813
Epoch 1088
Epoch 1088 :: Batch 0/1
Batch Loss = 0.0007310406254860586
1088, epoch_train_loss=0.0007310406254860586
Epoch 1089
Epoch 1089 :: Batch 0/1
Batch Loss = 0.0007310406254757296
1089, epoch_train_loss=0.0007310406254757296
Epoch 1090
Epoch 1090 :: Batch 0/1
Batch Loss = 0.0007310406254653939
1090, epoch_train_loss=0.0007310406254653939
Epoch 1091
Epoch 1091 :: Batch 0/1
Batch Loss = 0.0007310406254550518
1091, epoch_train_loss=0.0007310406254550518
Epoch 1092
Epoch 1092 :: Batch 0/1
Batch Loss = 0.0007310406254447033
1092, epoch_train_loss=0.0007310406254447033
Epoch 1093
Epoch 1093 :: Batch 0/1
Batch Loss = 0.0007310406254343483
1093, epoch_train_loss=0.0007310406254343483
Epoch 1094
Epoch 1094 :: Batch 0/1
Batch Loss = 0.0007310406254239869
1094, epoch_train_loss=0.0007310406254239869
Epoch 1095
Epoch 1095 :: Batch 0/1
Batch Loss = 0.000731040625413619
1095, epoch_train_loss=0.000731040625413619
Epoch 1096
Epoch 1096 :: Batch 0/1
Batch Loss = 0.0007310406254032447
1096, epoch_train_loss=0.0007310406254032447
Epoch 1097
Epoch 1097 :: Batch 0/1
Batch Loss = 0.000731040625392864
1097, epoch_train_loss=0.000731040625392864
Epoch 1098
Epoch 1098 :: Batch 0/1
Batch Loss = 0.0007310406253824768
1098, epoch_train_loss=0.0007310406253824768
Epoch 1099
Epoch 1099 :: Batch 0/1
Batch Loss = 0.0007310406253720832
1099, epoch_train_loss=0.0007310406253720832
Epoch 1100
Epoch 1100 :: Batch 0/1
Batch Loss = 0.0007310406253616832
1100, epoch_train_loss=0.0007310406253616832
Epoch 1101
Epoch 1101 :: Batch 0/1
Batch Loss = 0.0007310406253512765
1101, epoch_train_loss=0.0007310406253512765
Epoch 1102
Epoch 1102 :: Batch 0/1
Batch Loss = 0.0007310406253408637
1102, epoch_train_loss=0.0007310406253408637
Epoch 1103
Epoch 1103 :: Batch 0/1
Batch Loss = 0.0007310406253304443
1103, epoch_train_loss=0.0007310406253304443
Epoch 1104
Epoch 1104 :: Batch 0/1
Batch Loss = 0.0007310406253200186
1104, epoch_train_loss=0.0007310406253200186
Epoch 1105
Epoch 1105 :: Batch 0/1
Batch Loss = 0.0007310406253095862
1105, epoch_train_loss=0.0007310406253095862
Epoch 1106
Epoch 1106 :: Batch 0/1
Batch Loss = 0.0007310406252991477
1106, epoch_train_loss=0.0007310406252991477
Epoch 1107
Epoch 1107 :: Batch 0/1
Batch Loss = 0.0007310406252887024
1107, epoch_train_loss=0.0007310406252887024
Epoch 1108
Epoch 1108 :: Batch 0/1
Batch Loss = 0.0007310406252782509
1108, epoch_train_loss=0.0007310406252782509
Epoch 1109
Epoch 1109 :: Batch 0/1
Batch Loss = 0.000731040625267793
1109, epoch_train_loss=0.000731040625267793
Epoch 1110
Epoch 1110 :: Batch 0/1
Batch Loss = 0.0007310406252573285
1110, epoch_train_loss=0.0007310406252573285
Epoch 1111
Epoch 1111 :: Batch 0/1
Batch Loss = 0.0007310406252468576
1111, epoch_train_loss=0.0007310406252468576
Epoch 1112
Epoch 1112 :: Batch 0/1
Batch Loss = 0.0007310406252363802
1112, epoch_train_loss=0.0007310406252363802
Epoch 1113
Epoch 1113 :: Batch 0/1
Batch Loss = 0.0007310406252258966
1113, epoch_train_loss=0.0007310406252258966
Epoch 1114
Epoch 1114 :: Batch 0/1
Batch Loss = 0.0007310406252154065
1114, epoch_train_loss=0.0007310406252154065
Epoch 1115
Epoch 1115 :: Batch 0/1
Batch Loss = 0.0007310406252049099
1115, epoch_train_loss=0.0007310406252049099
Epoch 1116
Epoch 1116 :: Batch 0/1
Batch Loss = 0.000731040625194407
1116, epoch_train_loss=0.000731040625194407
Epoch 1117
Epoch 1117 :: Batch 0/1
Batch Loss = 0.0007310406251838973
1117, epoch_train_loss=0.0007310406251838973
Epoch 1118
Epoch 1118 :: Batch 0/1
Batch Loss = 0.0007310406251733817
1118, epoch_train_loss=0.0007310406251733817
Epoch 1119
Epoch 1119 :: Batch 0/1
Batch Loss = 0.0007310406251628594
1119, epoch_train_loss=0.0007310406251628594
Epoch 1120
Epoch 1120 :: Batch 0/1
Batch Loss = 0.0007310406251523305
1120, epoch_train_loss=0.0007310406251523305
Epoch 1121
Epoch 1121 :: Batch 0/1
Batch Loss = 0.0007310406251417954
1121, epoch_train_loss=0.0007310406251417954
Epoch 1122
Epoch 1122 :: Batch 0/1
Batch Loss = 0.0007310406251312538
1122, epoch_train_loss=0.0007310406251312538
Epoch 1123
Epoch 1123 :: Batch 0/1
Batch Loss = 0.0007310406251207057
1123, epoch_train_loss=0.0007310406251207057
Epoch 1124
Epoch 1124 :: Batch 0/1
Batch Loss = 0.0007310406251101513
1124, epoch_train_loss=0.0007310406251101513
Epoch 1125
Epoch 1125 :: Batch 0/1
Batch Loss = 0.0007310406250995904
1125, epoch_train_loss=0.0007310406250995904
Epoch 1126
Epoch 1126 :: Batch 0/1
Batch Loss = 0.0007310406250890232
1126, epoch_train_loss=0.0007310406250890232
Epoch 1127
Epoch 1127 :: Batch 0/1
Batch Loss = 0.0007310406250784495
1127, epoch_train_loss=0.0007310406250784495
Epoch 1128
Epoch 1128 :: Batch 0/1
Batch Loss = 0.0007310406250678692
1128, epoch_train_loss=0.0007310406250678692
Epoch 1129
Epoch 1129 :: Batch 0/1
Batch Loss = 0.0007310406250572827
1129, epoch_train_loss=0.0007310406250572827
Epoch 1130
Epoch 1130 :: Batch 0/1
Batch Loss = 0.0007310406250466896
1130, epoch_train_loss=0.0007310406250466896
Epoch 1131
Epoch 1131 :: Batch 0/1
Batch Loss = 0.0007310406250360902
1131, epoch_train_loss=0.0007310406250360902
Epoch 1132
Epoch 1132 :: Batch 0/1
Batch Loss = 0.0007310406250254844
1132, epoch_train_loss=0.0007310406250254844
Epoch 1133
Epoch 1133 :: Batch 0/1
Batch Loss = 0.0007310406250148722
1133, epoch_train_loss=0.0007310406250148722
Epoch 1134
Epoch 1134 :: Batch 0/1
Batch Loss = 0.0007310406250042534
1134, epoch_train_loss=0.0007310406250042534
Epoch 1135
Epoch 1135 :: Batch 0/1
Batch Loss = 0.0007310406249936283
1135, epoch_train_loss=0.0007310406249936283
Epoch 1136
Epoch 1136 :: Batch 0/1
Batch Loss = 0.0007310406249829966
1136, epoch_train_loss=0.0007310406249829966
Epoch 1137
Epoch 1137 :: Batch 0/1
Batch Loss = 0.0007310406249723589
1137, epoch_train_loss=0.0007310406249723589
Epoch 1138
Epoch 1138 :: Batch 0/1
Batch Loss = 0.0007310406249617144
1138, epoch_train_loss=0.0007310406249617144
Epoch 1139
Epoch 1139 :: Batch 0/1
Batch Loss = 0.0007310406249510636
1139, epoch_train_loss=0.0007310406249510636
Epoch 1140
Epoch 1140 :: Batch 0/1
Batch Loss = 0.0007310406249404064
1140, epoch_train_loss=0.0007310406249404064
Epoch 1141
Epoch 1141 :: Batch 0/1
Batch Loss = 0.0007310406249297428
1141, epoch_train_loss=0.0007310406249297428
Epoch 1142
Epoch 1142 :: Batch 0/1
Batch Loss = 0.0007310406249190728
1142, epoch_train_loss=0.0007310406249190728
Epoch 1143
Epoch 1143 :: Batch 0/1
Batch Loss = 0.0007310406249083963
1143, epoch_train_loss=0.0007310406249083963
Epoch 1144
Epoch 1144 :: Batch 0/1
Batch Loss = 0.0007310406248977133
1144, epoch_train_loss=0.0007310406248977133
Epoch 1145
Epoch 1145 :: Batch 0/1
Batch Loss = 0.000731040624887024
1145, epoch_train_loss=0.000731040624887024
Epoch 1146
Epoch 1146 :: Batch 0/1
Batch Loss = 0.0007310406248763283
1146, epoch_train_loss=0.0007310406248763283
Epoch 1147
Epoch 1147 :: Batch 0/1
Batch Loss = 0.0007310406248656261
1147, epoch_train_loss=0.0007310406248656261
Epoch 1148
Epoch 1148 :: Batch 0/1
Batch Loss = 0.0007310406248549176
1148, epoch_train_loss=0.0007310406248549176
Epoch 1149
Epoch 1149 :: Batch 0/1
Batch Loss = 0.0007310406248442026
1149, epoch_train_loss=0.0007310406248442026
Epoch 1150
Epoch 1150 :: Batch 0/1
Batch Loss = 0.0007310406248334811
1150, epoch_train_loss=0.0007310406248334811
Epoch 1151
Epoch 1151 :: Batch 0/1
Batch Loss = 0.0007310406248227534
1151, epoch_train_loss=0.0007310406248227534
Epoch 1152
Epoch 1152 :: Batch 0/1
Batch Loss = 0.0007310406248120191
1152, epoch_train_loss=0.0007310406248120191
Epoch 1153
Epoch 1153 :: Batch 0/1
Batch Loss = 0.0007310406248012784
1153, epoch_train_loss=0.0007310406248012784
Epoch 1154
Epoch 1154 :: Batch 0/1
Batch Loss = 0.0007310406247905315
1154, epoch_train_loss=0.0007310406247905315
Epoch 1155
Epoch 1155 :: Batch 0/1
Batch Loss = 0.0007310406247797779
1155, epoch_train_loss=0.0007310406247797779
Epoch 1156
Epoch 1156 :: Batch 0/1
Batch Loss = 0.0007310406247690181
1156, epoch_train_loss=0.0007310406247690181
Epoch 1157
Epoch 1157 :: Batch 0/1
Batch Loss = 0.0007310406247582518
1157, epoch_train_loss=0.0007310406247582518
Epoch 1158
Epoch 1158 :: Batch 0/1
Batch Loss = 0.0007310406247474791
1158, epoch_train_loss=0.0007310406247474791
Epoch 1159
Epoch 1159 :: Batch 0/1
Batch Loss = 0.0007310406247366998
1159, epoch_train_loss=0.0007310406247366998
Epoch 1160
Epoch 1160 :: Batch 0/1
Batch Loss = 0.0007310406247259143
1160, epoch_train_loss=0.0007310406247259143
Epoch 1161
Epoch 1161 :: Batch 0/1
Batch Loss = 0.0007310406247151225
1161, epoch_train_loss=0.0007310406247151225
Epoch 1162
Epoch 1162 :: Batch 0/1
Batch Loss = 0.000731040624704324
1162, epoch_train_loss=0.000731040624704324
Epoch 1163
Epoch 1163 :: Batch 0/1
Batch Loss = 0.0007310406246935192
1163, epoch_train_loss=0.0007310406246935192
Epoch 1164
Epoch 1164 :: Batch 0/1
Batch Loss = 0.0007310406246827081
1164, epoch_train_loss=0.0007310406246827081
Epoch 1165
Epoch 1165 :: Batch 0/1
Batch Loss = 0.0007310406246718905
1165, epoch_train_loss=0.0007310406246718905
Epoch 1166
Epoch 1166 :: Batch 0/1
Batch Loss = 0.0007310406246610665
1166, epoch_train_loss=0.0007310406246610665
Epoch 1167
Epoch 1167 :: Batch 0/1
Batch Loss = 0.0007310406246502361
1167, epoch_train_loss=0.0007310406246502361
Epoch 1168
Epoch 1168 :: Batch 0/1
Batch Loss = 0.0007310406246393993
1168, epoch_train_loss=0.0007310406246393993
Epoch 1169
Epoch 1169 :: Batch 0/1
Batch Loss = 0.000731040624628556
1169, epoch_train_loss=0.000731040624628556
Epoch 1170
Epoch 1170 :: Batch 0/1
Batch Loss = 0.0007310406246177064
1170, epoch_train_loss=0.0007310406246177064
Epoch 1171
Epoch 1171 :: Batch 0/1
Batch Loss = 0.0007310406246068504
1171, epoch_train_loss=0.0007310406246068504
Epoch 1172
Epoch 1172 :: Batch 0/1
Batch Loss = 0.000731040624595988
1172, epoch_train_loss=0.000731040624595988
Epoch 1173
Epoch 1173 :: Batch 0/1
Batch Loss = 0.0007310406245851192
1173, epoch_train_loss=0.0007310406245851192
Epoch 1174
Epoch 1174 :: Batch 0/1
Batch Loss = 0.0007310406245742439
1174, epoch_train_loss=0.0007310406245742439
Epoch 1175
Epoch 1175 :: Batch 0/1
Batch Loss = 0.0007310406245633621
1175, epoch_train_loss=0.0007310406245633621
Epoch 1176
Epoch 1176 :: Batch 0/1
Batch Loss = 0.000731040624552474
1176, epoch_train_loss=0.000731040624552474
Epoch 1177
Epoch 1177 :: Batch 0/1
Batch Loss = 0.0007310406245415794
1177, epoch_train_loss=0.0007310406245415794
Epoch 1178
Epoch 1178 :: Batch 0/1
Batch Loss = 0.0007310406245306787
1178, epoch_train_loss=0.0007310406245306787
Epoch 1179
Epoch 1179 :: Batch 0/1
Batch Loss = 0.0007310406245197714
1179, epoch_train_loss=0.0007310406245197714
Epoch 1180
Epoch 1180 :: Batch 0/1
Batch Loss = 0.0007310406245088578
1180, epoch_train_loss=0.0007310406245088578
Epoch 1181
Epoch 1181 :: Batch 0/1
Batch Loss = 0.0007310406244979377
1181, epoch_train_loss=0.0007310406244979377
Epoch 1182
Epoch 1182 :: Batch 0/1
Batch Loss = 0.0007310406244870111
1182, epoch_train_loss=0.0007310406244870111
Epoch 1183
Epoch 1183 :: Batch 0/1
Batch Loss = 0.0007310406244760782
1183, epoch_train_loss=0.0007310406244760782
Epoch 1184
Epoch 1184 :: Batch 0/1
Batch Loss = 0.000731040624465139
1184, epoch_train_loss=0.000731040624465139
Epoch 1185
Epoch 1185 :: Batch 0/1
Batch Loss = 0.0007310406244541932
1185, epoch_train_loss=0.0007310406244541932
Epoch 1186
Epoch 1186 :: Batch 0/1
Batch Loss = 0.0007310406244432412
1186, epoch_train_loss=0.0007310406244432412
Epoch 1187
Epoch 1187 :: Batch 0/1
Batch Loss = 0.0007310406244322828
1187, epoch_train_loss=0.0007310406244322828
Epoch 1188
Epoch 1188 :: Batch 0/1
Batch Loss = 0.0007310406244213178
1188, epoch_train_loss=0.0007310406244213178
Epoch 1189
Epoch 1189 :: Batch 0/1
Batch Loss = 0.0007310406244103464
1189, epoch_train_loss=0.0007310406244103464
Epoch 1190
Epoch 1190 :: Batch 0/1
Batch Loss = 0.0007310406243993687
1190, epoch_train_loss=0.0007310406243993687
Epoch 1191
Epoch 1191 :: Batch 0/1
Batch Loss = 0.0007310406243883845
1191, epoch_train_loss=0.0007310406243883845
Epoch 1192
Epoch 1192 :: Batch 0/1
Batch Loss = 0.0007310406243773942
1192, epoch_train_loss=0.0007310406243773942
Epoch 1193
Epoch 1193 :: Batch 0/1
Batch Loss = 0.0007310406243663972
1193, epoch_train_loss=0.0007310406243663972
Epoch 1194
Epoch 1194 :: Batch 0/1
Batch Loss = 0.000731040624355394
1194, epoch_train_loss=0.000731040624355394
Epoch 1195
Epoch 1195 :: Batch 0/1
Batch Loss = 0.0007310406243443841
1195, epoch_train_loss=0.0007310406243443841
Epoch 1196
Epoch 1196 :: Batch 0/1
Batch Loss = 0.000731040624333368
1196, epoch_train_loss=0.000731040624333368
Epoch 1197
Epoch 1197 :: Batch 0/1
Batch Loss = 0.0007310406243223456
1197, epoch_train_loss=0.0007310406243223456
Epoch 1198
Epoch 1198 :: Batch 0/1
Batch Loss = 0.0007310406243113166
1198, epoch_train_loss=0.0007310406243113166
Epoch 1199
Epoch 1199 :: Batch 0/1
Batch Loss = 0.0007310406243002814
1199, epoch_train_loss=0.0007310406243002814
Epoch 1200
Epoch 1200 :: Batch 0/1
Batch Loss = 0.0007310406242892396
1200, epoch_train_loss=0.0007310406242892396
Epoch 1201
Epoch 1201 :: Batch 0/1
Batch Loss = 0.0007310406242781915
1201, epoch_train_loss=0.0007310406242781915
Epoch 1202
Epoch 1202 :: Batch 0/1
Batch Loss = 0.0007310406242671371
1202, epoch_train_loss=0.0007310406242671371
Epoch 1203
Epoch 1203 :: Batch 0/1
Batch Loss = 0.0007310406242560762
1203, epoch_train_loss=0.0007310406242560762
Epoch 1204
Epoch 1204 :: Batch 0/1
Batch Loss = 0.000731040624245009
1204, epoch_train_loss=0.000731040624245009
Epoch 1205
Epoch 1205 :: Batch 0/1
Batch Loss = 0.0007310406242339352
1205, epoch_train_loss=0.0007310406242339352
Epoch 1206
Epoch 1206 :: Batch 0/1
Batch Loss = 0.0007310406242228551
1206, epoch_train_loss=0.0007310406242228551
Epoch 1207
Epoch 1207 :: Batch 0/1
Batch Loss = 0.0007310406242117686
1207, epoch_train_loss=0.0007310406242117686
Epoch 1208
Epoch 1208 :: Batch 0/1
Batch Loss = 0.0007310406242006758
1208, epoch_train_loss=0.0007310406242006758
Epoch 1209
Epoch 1209 :: Batch 0/1
Batch Loss = 0.0007310406241895765
1209, epoch_train_loss=0.0007310406241895765
Epoch 1210
Epoch 1210 :: Batch 0/1
Batch Loss = 0.0007310406241784709
1210, epoch_train_loss=0.0007310406241784709
Epoch 1211
Epoch 1211 :: Batch 0/1
Batch Loss = 0.0007310406241673588
1211, epoch_train_loss=0.0007310406241673588
Epoch 1212
Epoch 1212 :: Batch 0/1
Batch Loss = 0.0007310406241562404
1212, epoch_train_loss=0.0007310406241562404
Epoch 1213
Epoch 1213 :: Batch 0/1
Batch Loss = 0.0007310406241451158
1213, epoch_train_loss=0.0007310406241451158
Epoch 1214
Epoch 1214 :: Batch 0/1
Batch Loss = 0.0007310406241339846
1214, epoch_train_loss=0.0007310406241339846
Epoch 1215
Epoch 1215 :: Batch 0/1
Batch Loss = 0.0007310406241228469
1215, epoch_train_loss=0.0007310406241228469
Epoch 1216
Epoch 1216 :: Batch 0/1
Batch Loss = 0.0007310406241117028
1216, epoch_train_loss=0.0007310406241117028
Epoch 1217
Epoch 1217 :: Batch 0/1
Batch Loss = 0.0007310406241005525
1217, epoch_train_loss=0.0007310406241005525
Epoch 1218
Epoch 1218 :: Batch 0/1
Batch Loss = 0.0007310406240893958
1218, epoch_train_loss=0.0007310406240893958
Epoch 1219
Epoch 1219 :: Batch 0/1
Batch Loss = 0.0007310406240782326
1219, epoch_train_loss=0.0007310406240782326
Epoch 1220
Epoch 1220 :: Batch 0/1
Batch Loss = 0.0007310406240670631
1220, epoch_train_loss=0.0007310406240670631
Epoch 1221
Epoch 1221 :: Batch 0/1
Batch Loss = 0.0007310406240558871
1221, epoch_train_loss=0.0007310406240558871
Epoch 1222
Epoch 1222 :: Batch 0/1
Batch Loss = 0.0007310406240447047
1222, epoch_train_loss=0.0007310406240447047
Epoch 1223
Epoch 1223 :: Batch 0/1
Batch Loss = 0.000731040624033516
1223, epoch_train_loss=0.000731040624033516
Epoch 1224
Epoch 1224 :: Batch 0/1
Batch Loss = 0.0007310406240223209
1224, epoch_train_loss=0.0007310406240223209
Epoch 1225
Epoch 1225 :: Batch 0/1
Batch Loss = 0.0007310406240111193
1225, epoch_train_loss=0.0007310406240111193
Epoch 1226
Epoch 1226 :: Batch 0/1
Batch Loss = 0.0007310406239999115
1226, epoch_train_loss=0.0007310406239999115
Epoch 1227
Epoch 1227 :: Batch 0/1
Batch Loss = 0.0007310406239886972
1227, epoch_train_loss=0.0007310406239886972
Epoch 1228
Epoch 1228 :: Batch 0/1
Batch Loss = 0.0007310406239774764
1228, epoch_train_loss=0.0007310406239774764
Epoch 1229
Epoch 1229 :: Batch 0/1
Batch Loss = 0.0007310406239662495
1229, epoch_train_loss=0.0007310406239662495
Epoch 1230
Epoch 1230 :: Batch 0/1
Batch Loss = 0.000731040623955016
1230, epoch_train_loss=0.000731040623955016
Epoch 1231
Epoch 1231 :: Batch 0/1
Batch Loss = 0.0007310406239437762
1231, epoch_train_loss=0.0007310406239437762
Epoch 1232
Epoch 1232 :: Batch 0/1
Batch Loss = 0.0007310406239325301
1232, epoch_train_loss=0.0007310406239325301
Epoch 1233
Epoch 1233 :: Batch 0/1
Batch Loss = 0.0007310406239212773
1233, epoch_train_loss=0.0007310406239212773
Epoch 1234
Epoch 1234 :: Batch 0/1
Batch Loss = 0.0007310406239100183
1234, epoch_train_loss=0.0007310406239100183
Epoch 1235
Epoch 1235 :: Batch 0/1
Batch Loss = 0.0007310406238987529
1235, epoch_train_loss=0.0007310406238987529
Epoch 1236
Epoch 1236 :: Batch 0/1
Batch Loss = 0.0007310406238874812
1236, epoch_train_loss=0.0007310406238874812
Epoch 1237
Epoch 1237 :: Batch 0/1
Batch Loss = 0.0007310406238762032
1237, epoch_train_loss=0.0007310406238762032
Epoch 1238
Epoch 1238 :: Batch 0/1
Batch Loss = 0.0007310406238649185
1238, epoch_train_loss=0.0007310406238649185
Epoch 1239
Epoch 1239 :: Batch 0/1
Batch Loss = 0.0007310406238536276
1239, epoch_train_loss=0.0007310406238536276
Epoch 1240
Epoch 1240 :: Batch 0/1
Batch Loss = 0.0007310406238423304
1240, epoch_train_loss=0.0007310406238423304
Epoch 1241
Epoch 1241 :: Batch 0/1
Batch Loss = 0.0007310406238310265
1241, epoch_train_loss=0.0007310406238310265
Epoch 1242
Epoch 1242 :: Batch 0/1
Batch Loss = 0.0007310406238197166
1242, epoch_train_loss=0.0007310406238197166
Epoch 1243
Epoch 1243 :: Batch 0/1
Batch Loss = 0.0007310406238084002
1243, epoch_train_loss=0.0007310406238084002
Epoch 1244
Epoch 1244 :: Batch 0/1
Batch Loss = 0.0007310406237970774
1244, epoch_train_loss=0.0007310406237970774
Epoch 1245
Epoch 1245 :: Batch 0/1
Batch Loss = 0.0007310406237857482
1245, epoch_train_loss=0.0007310406237857482
Epoch 1246
Epoch 1246 :: Batch 0/1
Batch Loss = 0.0007310406237744126
1246, epoch_train_loss=0.0007310406237744126
Epoch 1247
Epoch 1247 :: Batch 0/1
Batch Loss = 0.0007310406237630706
1247, epoch_train_loss=0.0007310406237630706
Epoch 1248
Epoch 1248 :: Batch 0/1
Batch Loss = 0.0007310406237517222
1248, epoch_train_loss=0.0007310406237517222
Epoch 1249
Epoch 1249 :: Batch 0/1
Batch Loss = 0.0007310406237403675
1249, epoch_train_loss=0.0007310406237403675
Epoch 1250
Epoch 1250 :: Batch 0/1
Batch Loss = 0.0007310406237290065
1250, epoch_train_loss=0.0007310406237290065
Epoch 1251
Epoch 1251 :: Batch 0/1
Batch Loss = 0.0007310406237176389
1251, epoch_train_loss=0.0007310406237176389
Epoch 1252
Epoch 1252 :: Batch 0/1
Batch Loss = 0.000731040623706265
1252, epoch_train_loss=0.000731040623706265
Epoch 1253
Epoch 1253 :: Batch 0/1
Batch Loss = 0.0007310406236948847
1253, epoch_train_loss=0.0007310406236948847
Epoch 1254
Epoch 1254 :: Batch 0/1
Batch Loss = 0.000731040623683498
1254, epoch_train_loss=0.000731040623683498
Epoch 1255
Epoch 1255 :: Batch 0/1
Batch Loss = 0.000731040623672105
1255, epoch_train_loss=0.000731040623672105
Epoch 1256
Epoch 1256 :: Batch 0/1
Batch Loss = 0.0007310406236607056
1256, epoch_train_loss=0.0007310406236607056
Epoch 1257
Epoch 1257 :: Batch 0/1
Batch Loss = 0.0007310406236492999
1257, epoch_train_loss=0.0007310406236492999
Epoch 1258
Epoch 1258 :: Batch 0/1
Batch Loss = 0.0007310406236378877
1258, epoch_train_loss=0.0007310406236378877
Epoch 1259
Epoch 1259 :: Batch 0/1
Batch Loss = 0.0007310406236264691
1259, epoch_train_loss=0.0007310406236264691
Epoch 1260
Epoch 1260 :: Batch 0/1
Batch Loss = 0.0007310406236150442
1260, epoch_train_loss=0.0007310406236150442
Epoch 1261
Epoch 1261 :: Batch 0/1
Batch Loss = 0.0007310406236036128
1261, epoch_train_loss=0.0007310406236036128
Epoch 1262
Epoch 1262 :: Batch 0/1
Batch Loss = 0.0007310406235921753
1262, epoch_train_loss=0.0007310406235921753
Epoch 1263
Epoch 1263 :: Batch 0/1
Batch Loss = 0.0007310406235807313
1263, epoch_train_loss=0.0007310406235807313
Epoch 1264
Epoch 1264 :: Batch 0/1
Batch Loss = 0.0007310406235692809
1264, epoch_train_loss=0.0007310406235692809
Epoch 1265
Epoch 1265 :: Batch 0/1
Batch Loss = 0.000731040623557824
1265, epoch_train_loss=0.000731040623557824
Epoch 1266
Epoch 1266 :: Batch 0/1
Batch Loss = 0.0007310406235463609
1266, epoch_train_loss=0.0007310406235463609
Epoch 1267
Epoch 1267 :: Batch 0/1
Batch Loss = 0.0007310406235348912
1267, epoch_train_loss=0.0007310406235348912
Epoch 1268
Epoch 1268 :: Batch 0/1
Batch Loss = 0.0007310406235234154
1268, epoch_train_loss=0.0007310406235234154
Epoch 1269
Epoch 1269 :: Batch 0/1
Batch Loss = 0.000731040623511933
1269, epoch_train_loss=0.000731040623511933
Epoch 1270
Epoch 1270 :: Batch 0/1
Batch Loss = 0.0007310406235004444
1270, epoch_train_loss=0.0007310406235004444
Epoch 1271
Epoch 1271 :: Batch 0/1
Batch Loss = 0.0007310406234889492
1271, epoch_train_loss=0.0007310406234889492
Epoch 1272
Epoch 1272 :: Batch 0/1
Batch Loss = 0.0007310406234774479
1272, epoch_train_loss=0.0007310406234774479
Epoch 1273
Epoch 1273 :: Batch 0/1
Batch Loss = 0.00073104062346594
1273, epoch_train_loss=0.00073104062346594
Epoch 1274
Epoch 1274 :: Batch 0/1
Batch Loss = 0.0007310406234544259
1274, epoch_train_loss=0.0007310406234544259
Epoch 1275
Epoch 1275 :: Batch 0/1
Batch Loss = 0.0007310406234429052
1275, epoch_train_loss=0.0007310406234429052
Epoch 1276
Epoch 1276 :: Batch 0/1
Batch Loss = 0.0007310406234313783
1276, epoch_train_loss=0.0007310406234313783
Epoch 1277
Epoch 1277 :: Batch 0/1
Batch Loss = 0.000731040623419845
1277, epoch_train_loss=0.000731040623419845
Epoch 1278
Epoch 1278 :: Batch 0/1
Batch Loss = 0.0007310406234083054
1278, epoch_train_loss=0.0007310406234083054
Epoch 1279
Epoch 1279 :: Batch 0/1
Batch Loss = 0.0007310406233967593
1279, epoch_train_loss=0.0007310406233967593
Epoch 1280
Epoch 1280 :: Batch 0/1
Batch Loss = 0.0007310406233852069
1280, epoch_train_loss=0.0007310406233852069
Epoch 1281
Epoch 1281 :: Batch 0/1
Batch Loss = 0.000731040623373648
1281, epoch_train_loss=0.000731040623373648
Epoch 1282
Epoch 1282 :: Batch 0/1
Batch Loss = 0.0007310406233620828
1282, epoch_train_loss=0.0007310406233620828
Epoch 1283
Epoch 1283 :: Batch 0/1
Batch Loss = 0.0007310406233505114
1283, epoch_train_loss=0.0007310406233505114
Epoch 1284
Epoch 1284 :: Batch 0/1
Batch Loss = 0.0007310406233389335
1284, epoch_train_loss=0.0007310406233389335
Epoch 1285
Epoch 1285 :: Batch 0/1
Batch Loss = 0.0007310406233273491
1285, epoch_train_loss=0.0007310406233273491
Epoch 1286
Epoch 1286 :: Batch 0/1
Batch Loss = 0.0007310406233157585
1286, epoch_train_loss=0.0007310406233157585
Epoch 1287
Epoch 1287 :: Batch 0/1
Batch Loss = 0.0007310406233041616
1287, epoch_train_loss=0.0007310406233041616
Epoch 1288
Epoch 1288 :: Batch 0/1
Batch Loss = 0.0007310406232925579
1288, epoch_train_loss=0.0007310406232925579
Epoch 1289
Epoch 1289 :: Batch 0/1
Batch Loss = 0.0007310406232809483
1289, epoch_train_loss=0.0007310406232809483
Epoch 1290
Epoch 1290 :: Batch 0/1
Batch Loss = 0.000731040623269332
1290, epoch_train_loss=0.000731040623269332
Epoch 1291
Epoch 1291 :: Batch 0/1
Batch Loss = 0.0007310406232577096
1291, epoch_train_loss=0.0007310406232577096
Epoch 1292
Epoch 1292 :: Batch 0/1
Batch Loss = 0.0007310406232460807
1292, epoch_train_loss=0.0007310406232460807
Epoch 1293
Epoch 1293 :: Batch 0/1
Batch Loss = 0.0007310406232344455
1293, epoch_train_loss=0.0007310406232344455
Epoch 1294
Epoch 1294 :: Batch 0/1
Batch Loss = 0.0007310406232228038
1294, epoch_train_loss=0.0007310406232228038
Epoch 1295
Epoch 1295 :: Batch 0/1
Batch Loss = 0.0007310406232111558
1295, epoch_train_loss=0.0007310406232111558
Epoch 1296
Epoch 1296 :: Batch 0/1
Batch Loss = 0.0007310406231995014
1296, epoch_train_loss=0.0007310406231995014
Epoch 1297
Epoch 1297 :: Batch 0/1
Batch Loss = 0.0007310406231878407
1297, epoch_train_loss=0.0007310406231878407
Epoch 1298
Epoch 1298 :: Batch 0/1
Batch Loss = 0.0007310406231761735
1298, epoch_train_loss=0.0007310406231761735
Epoch 1299
Epoch 1299 :: Batch 0/1
Batch Loss = 0.0007310406231645001
1299, epoch_train_loss=0.0007310406231645001
Epoch 1300
Epoch 1300 :: Batch 0/1
Batch Loss = 0.0007310406231528202
1300, epoch_train_loss=0.0007310406231528202
Epoch 1301
Epoch 1301 :: Batch 0/1
Batch Loss = 0.0007310406231411341
1301, epoch_train_loss=0.0007310406231411341
Epoch 1302
Epoch 1302 :: Batch 0/1
Batch Loss = 0.0007310406231294416
1302, epoch_train_loss=0.0007310406231294416
Epoch 1303
Epoch 1303 :: Batch 0/1
Batch Loss = 0.0007310406231177425
1303, epoch_train_loss=0.0007310406231177425
Epoch 1304
Epoch 1304 :: Batch 0/1
Batch Loss = 0.0007310406231060372
1304, epoch_train_loss=0.0007310406231060372
Epoch 1305
Epoch 1305 :: Batch 0/1
Batch Loss = 0.0007310406230943256
1305, epoch_train_loss=0.0007310406230943256
Epoch 1306
Epoch 1306 :: Batch 0/1
Batch Loss = 0.0007310406230826076
1306, epoch_train_loss=0.0007310406230826076
Epoch 1307
Epoch 1307 :: Batch 0/1
Batch Loss = 0.000731040623070883
1307, epoch_train_loss=0.000731040623070883
Epoch 1308
Epoch 1308 :: Batch 0/1
Batch Loss = 0.0007310406230591522
1308, epoch_train_loss=0.0007310406230591522
Epoch 1309
Epoch 1309 :: Batch 0/1
Batch Loss = 0.0007310406230474152
1309, epoch_train_loss=0.0007310406230474152
Epoch 1310
Epoch 1310 :: Batch 0/1
Batch Loss = 0.0007310406230356717
1310, epoch_train_loss=0.0007310406230356717
Epoch 1311
Epoch 1311 :: Batch 0/1
Batch Loss = 0.0007310406230239218
1311, epoch_train_loss=0.0007310406230239218
Epoch 1312
Epoch 1312 :: Batch 0/1
Batch Loss = 0.0007310406230121656
1312, epoch_train_loss=0.0007310406230121656
Epoch 1313
Epoch 1313 :: Batch 0/1
Batch Loss = 0.0007310406230004028
1313, epoch_train_loss=0.0007310406230004028
Epoch 1314
Epoch 1314 :: Batch 0/1
Batch Loss = 0.000731040622988634
1314, epoch_train_loss=0.000731040622988634
Epoch 1315
Epoch 1315 :: Batch 0/1
Batch Loss = 0.0007310406229768585
1315, epoch_train_loss=0.0007310406229768585
Epoch 1316
Epoch 1316 :: Batch 0/1
Batch Loss = 0.000731040622965077
1316, epoch_train_loss=0.000731040622965077
Epoch 1317
Epoch 1317 :: Batch 0/1
Batch Loss = 0.0007310406229532887
1317, epoch_train_loss=0.0007310406229532887
Epoch 1318
Epoch 1318 :: Batch 0/1
Batch Loss = 0.0007310406229414944
1318, epoch_train_loss=0.0007310406229414944
Epoch 1319
Epoch 1319 :: Batch 0/1
Batch Loss = 0.0007310406229296936
1319, epoch_train_loss=0.0007310406229296936
Epoch 1320
Epoch 1320 :: Batch 0/1
Batch Loss = 0.0007310406229178865
1320, epoch_train_loss=0.0007310406229178865
Epoch 1321
Epoch 1321 :: Batch 0/1
Batch Loss = 0.0007310406229060728
1321, epoch_train_loss=0.0007310406229060728
Epoch 1322
Epoch 1322 :: Batch 0/1
Batch Loss = 0.000731040622894253
1322, epoch_train_loss=0.000731040622894253
Epoch 1323
Epoch 1323 :: Batch 0/1
Batch Loss = 0.0007310406228824267
1323, epoch_train_loss=0.0007310406228824267
Epoch 1324
Epoch 1324 :: Batch 0/1
Batch Loss = 0.0007310406228705943
1324, epoch_train_loss=0.0007310406228705943
Epoch 1325
Epoch 1325 :: Batch 0/1
Batch Loss = 0.0007310406228587551
1325, epoch_train_loss=0.0007310406228587551
Epoch 1326
Epoch 1326 :: Batch 0/1
Batch Loss = 0.0007310406228469099
1326, epoch_train_loss=0.0007310406228469099
Epoch 1327
Epoch 1327 :: Batch 0/1
Batch Loss = 0.0007310406228350581
1327, epoch_train_loss=0.0007310406228350581
Epoch 1328
Epoch 1328 :: Batch 0/1
Batch Loss = 0.0007310406228232
1328, epoch_train_loss=0.0007310406228232
Epoch 1329
Epoch 1329 :: Batch 0/1
Batch Loss = 0.0007310406228113355
1329, epoch_train_loss=0.0007310406228113355
Epoch 1330
Epoch 1330 :: Batch 0/1
Batch Loss = 0.0007310406227994646
1330, epoch_train_loss=0.0007310406227994646
Epoch 1331
Epoch 1331 :: Batch 0/1
Batch Loss = 0.0007310406227875876
1331, epoch_train_loss=0.0007310406227875876
Epoch 1332
Epoch 1332 :: Batch 0/1
Batch Loss = 0.0007310406227757041
1332, epoch_train_loss=0.0007310406227757041
Epoch 1333
Epoch 1333 :: Batch 0/1
Batch Loss = 0.0007310406227638142
1333, epoch_train_loss=0.0007310406227638142
Epoch 1334
Epoch 1334 :: Batch 0/1
Batch Loss = 0.000731040622751918
1334, epoch_train_loss=0.000731040622751918
Epoch 1335
Epoch 1335 :: Batch 0/1
Batch Loss = 0.0007310406227400153
1335, epoch_train_loss=0.0007310406227400153
Epoch 1336
Epoch 1336 :: Batch 0/1
Batch Loss = 0.0007310406227281064
1336, epoch_train_loss=0.0007310406227281064
Epoch 1337
Epoch 1337 :: Batch 0/1
Batch Loss = 0.0007310406227161911
1337, epoch_train_loss=0.0007310406227161911
Epoch 1338
Epoch 1338 :: Batch 0/1
Batch Loss = 0.0007310406227042693
1338, epoch_train_loss=0.0007310406227042693
Epoch 1339
Epoch 1339 :: Batch 0/1
Batch Loss = 0.0007310406226923413
1339, epoch_train_loss=0.0007310406226923413
Epoch 1340
Epoch 1340 :: Batch 0/1
Batch Loss = 0.0007310406226804069
1340, epoch_train_loss=0.0007310406226804069
Epoch 1341
Epoch 1341 :: Batch 0/1
Batch Loss = 0.0007310406226684662
1341, epoch_train_loss=0.0007310406226684662
Epoch 1342
Epoch 1342 :: Batch 0/1
Batch Loss = 0.000731040622656519
1342, epoch_train_loss=0.000731040622656519
Epoch 1343
Epoch 1343 :: Batch 0/1
Batch Loss = 0.0007310406226445655
1343, epoch_train_loss=0.0007310406226445655
Epoch 1344
Epoch 1344 :: Batch 0/1
Batch Loss = 0.0007310406226326057
1344, epoch_train_loss=0.0007310406226326057
Epoch 1345
Epoch 1345 :: Batch 0/1
Batch Loss = 0.0007310406226206395
1345, epoch_train_loss=0.0007310406226206395
Epoch 1346
Epoch 1346 :: Batch 0/1
Batch Loss = 0.0007310406226086667
1346, epoch_train_loss=0.0007310406226086667
Epoch 1347
Epoch 1347 :: Batch 0/1
Batch Loss = 0.000731040622596688
1347, epoch_train_loss=0.000731040622596688
Epoch 1348
Epoch 1348 :: Batch 0/1
Batch Loss = 0.0007310406225847026
1348, epoch_train_loss=0.0007310406225847026
Epoch 1349
Epoch 1349 :: Batch 0/1
Batch Loss = 0.0007310406225727111
1349, epoch_train_loss=0.0007310406225727111
Epoch 1350
Epoch 1350 :: Batch 0/1
Batch Loss = 0.0007310406225607131
1350, epoch_train_loss=0.0007310406225607131
Epoch 1351
Epoch 1351 :: Batch 0/1
Batch Loss = 0.0007310406225487087
1351, epoch_train_loss=0.0007310406225487087
Epoch 1352
Epoch 1352 :: Batch 0/1
Batch Loss = 0.0007310406225366979
1352, epoch_train_loss=0.0007310406225366979
Epoch 1353
Epoch 1353 :: Batch 0/1
Batch Loss = 0.0007310406225246809
1353, epoch_train_loss=0.0007310406225246809
Epoch 1354
Epoch 1354 :: Batch 0/1
Batch Loss = 0.0007310406225126575
1354, epoch_train_loss=0.0007310406225126575
Epoch 1355
Epoch 1355 :: Batch 0/1
Batch Loss = 0.0007310406225006276
1355, epoch_train_loss=0.0007310406225006276
Epoch 1356
Epoch 1356 :: Batch 0/1
Batch Loss = 0.0007310406224885915
1356, epoch_train_loss=0.0007310406224885915
Epoch 1357
Epoch 1357 :: Batch 0/1
Batch Loss = 0.000731040622476549
1357, epoch_train_loss=0.000731040622476549
Epoch 1358
Epoch 1358 :: Batch 0/1
Batch Loss = 0.0007310406224645002
1358, epoch_train_loss=0.0007310406224645002
Epoch 1359
Epoch 1359 :: Batch 0/1
Batch Loss = 0.0007310406224524449
1359, epoch_train_loss=0.0007310406224524449
Epoch 1360
Epoch 1360 :: Batch 0/1
Batch Loss = 0.0007310406224403835
1360, epoch_train_loss=0.0007310406224403835
Epoch 1361
Epoch 1361 :: Batch 0/1
Batch Loss = 0.0007310406224283155
1361, epoch_train_loss=0.0007310406224283155
Epoch 1362
Epoch 1362 :: Batch 0/1
Batch Loss = 0.0007310406224162411
1362, epoch_train_loss=0.0007310406224162411
Epoch 1363
Epoch 1363 :: Batch 0/1
Batch Loss = 0.0007310406224041605
1363, epoch_train_loss=0.0007310406224041605
Epoch 1364
Epoch 1364 :: Batch 0/1
Batch Loss = 0.0007310406223920736
1364, epoch_train_loss=0.0007310406223920736
Epoch 1365
Epoch 1365 :: Batch 0/1
Batch Loss = 0.0007310406223799802
1365, epoch_train_loss=0.0007310406223799802
Epoch 1366
Epoch 1366 :: Batch 0/1
Batch Loss = 0.0007310406223678803
1366, epoch_train_loss=0.0007310406223678803
Epoch 1367
Epoch 1367 :: Batch 0/1
Batch Loss = 0.0007310406223557744
1367, epoch_train_loss=0.0007310406223557744
Epoch 1368
Epoch 1368 :: Batch 0/1
Batch Loss = 0.0007310406223436619
1368, epoch_train_loss=0.0007310406223436619
Epoch 1369
Epoch 1369 :: Batch 0/1
Batch Loss = 0.0007310406223315433
1369, epoch_train_loss=0.0007310406223315433
Epoch 1370
Epoch 1370 :: Batch 0/1
Batch Loss = 0.0007310406223194181
1370, epoch_train_loss=0.0007310406223194181
Epoch 1371
Epoch 1371 :: Batch 0/1
Batch Loss = 0.0007310406223072866
1371, epoch_train_loss=0.0007310406223072866
Epoch 1372
Epoch 1372 :: Batch 0/1
Batch Loss = 0.0007310406222951486
1372, epoch_train_loss=0.0007310406222951486
Epoch 1373
Epoch 1373 :: Batch 0/1
Batch Loss = 0.0007310406222830044
1373, epoch_train_loss=0.0007310406222830044
Epoch 1374
Epoch 1374 :: Batch 0/1
Batch Loss = 0.0007310406222708541
1374, epoch_train_loss=0.0007310406222708541
Epoch 1375
Epoch 1375 :: Batch 0/1
Batch Loss = 0.000731040622258697
1375, epoch_train_loss=0.000731040622258697
Epoch 1376
Epoch 1376 :: Batch 0/1
Batch Loss = 0.0007310406222465338
1376, epoch_train_loss=0.0007310406222465338
Epoch 1377
Epoch 1377 :: Batch 0/1
Batch Loss = 0.0007310406222343642
1377, epoch_train_loss=0.0007310406222343642
Epoch 1378
Epoch 1378 :: Batch 0/1
Batch Loss = 0.0007310406222221882
1378, epoch_train_loss=0.0007310406222221882
Epoch 1379
Epoch 1379 :: Batch 0/1
Batch Loss = 0.0007310406222100059
1379, epoch_train_loss=0.0007310406222100059
Epoch 1380
Epoch 1380 :: Batch 0/1
Batch Loss = 0.0007310406221978171
1380, epoch_train_loss=0.0007310406221978171
Epoch 1381
Epoch 1381 :: Batch 0/1
Batch Loss = 0.0007310406221856221
1381, epoch_train_loss=0.0007310406221856221
Epoch 1382
Epoch 1382 :: Batch 0/1
Batch Loss = 0.0007310406221734208
1382, epoch_train_loss=0.0007310406221734208
Epoch 1383
Epoch 1383 :: Batch 0/1
Batch Loss = 0.000731040622161213
1383, epoch_train_loss=0.000731040622161213
Epoch 1384
Epoch 1384 :: Batch 0/1
Batch Loss = 0.000731040622148999
1384, epoch_train_loss=0.000731040622148999
Epoch 1385
Epoch 1385 :: Batch 0/1
Batch Loss = 0.0007310406221367784
1385, epoch_train_loss=0.0007310406221367784
Epoch 1386
Epoch 1386 :: Batch 0/1
Batch Loss = 0.0007310406221245517
1386, epoch_train_loss=0.0007310406221245517
Epoch 1387
Epoch 1387 :: Batch 0/1
Batch Loss = 0.0007310406221123185
1387, epoch_train_loss=0.0007310406221123185
Epoch 1388
Epoch 1388 :: Batch 0/1
Batch Loss = 0.0007310406221000791
1388, epoch_train_loss=0.0007310406221000791
Epoch 1389
Epoch 1389 :: Batch 0/1
Batch Loss = 0.0007310406220878333
1389, epoch_train_loss=0.0007310406220878333
Epoch 1390
Epoch 1390 :: Batch 0/1
Batch Loss = 0.000731040622075581
1390, epoch_train_loss=0.000731040622075581
Epoch 1391
Epoch 1391 :: Batch 0/1
Batch Loss = 0.0007310406220633226
1391, epoch_train_loss=0.0007310406220633226
Epoch 1392
Epoch 1392 :: Batch 0/1
Batch Loss = 0.0007310406220510575
1392, epoch_train_loss=0.0007310406220510575
Epoch 1393
Epoch 1393 :: Batch 0/1
Batch Loss = 0.0007310406220387862
1393, epoch_train_loss=0.0007310406220387862
Epoch 1394
Epoch 1394 :: Batch 0/1
Batch Loss = 0.0007310406220265087
1394, epoch_train_loss=0.0007310406220265087
Epoch 1395
Epoch 1395 :: Batch 0/1
Batch Loss = 0.0007310406220142247
1395, epoch_train_loss=0.0007310406220142247
Epoch 1396
Epoch 1396 :: Batch 0/1
Batch Loss = 0.0007310406220019344
1396, epoch_train_loss=0.0007310406220019344
Epoch 1397
Epoch 1397 :: Batch 0/1
Batch Loss = 0.0007310406219896378
1397, epoch_train_loss=0.0007310406219896378
Epoch 1398
Epoch 1398 :: Batch 0/1
Batch Loss = 0.0007310406219773347
1398, epoch_train_loss=0.0007310406219773347
Epoch 1399
Epoch 1399 :: Batch 0/1
Batch Loss = 0.0007310406219650253
1399, epoch_train_loss=0.0007310406219650253
Epoch 1400
Epoch 1400 :: Batch 0/1
Batch Loss = 0.0007310406219527096
1400, epoch_train_loss=0.0007310406219527096
Epoch 1401
Epoch 1401 :: Batch 0/1
Batch Loss = 0.0007310406219403877
1401, epoch_train_loss=0.0007310406219403877
Epoch 1402
Epoch 1402 :: Batch 0/1
Batch Loss = 0.000731040621928059
1402, epoch_train_loss=0.000731040621928059
Epoch 1403
Epoch 1403 :: Batch 0/1
Batch Loss = 0.0007310406219157244
1403, epoch_train_loss=0.0007310406219157244
Epoch 1404
Epoch 1404 :: Batch 0/1
Batch Loss = 0.0007310406219033834
1404, epoch_train_loss=0.0007310406219033834
Epoch 1405
Epoch 1405 :: Batch 0/1
Batch Loss = 0.0007310406218910359
1405, epoch_train_loss=0.0007310406218910359
Epoch 1406
Epoch 1406 :: Batch 0/1
Batch Loss = 0.000731040621878682
1406, epoch_train_loss=0.000731040621878682
Epoch 1407
Epoch 1407 :: Batch 0/1
Batch Loss = 0.0007310406218663219
1407, epoch_train_loss=0.0007310406218663219
Epoch 1408
Epoch 1408 :: Batch 0/1
Batch Loss = 0.0007310406218539553
1408, epoch_train_loss=0.0007310406218539553
Epoch 1409
Epoch 1409 :: Batch 0/1
Batch Loss = 0.0007310406218415824
1409, epoch_train_loss=0.0007310406218415824
Epoch 1410
Epoch 1410 :: Batch 0/1
Batch Loss = 0.0007310406218292032
1410, epoch_train_loss=0.0007310406218292032
Epoch 1411
Epoch 1411 :: Batch 0/1
Batch Loss = 0.0007310406218168176
1411, epoch_train_loss=0.0007310406218168176
Epoch 1412
Epoch 1412 :: Batch 0/1
Batch Loss = 0.0007310406218044258
1412, epoch_train_loss=0.0007310406218044258
Epoch 1413
Epoch 1413 :: Batch 0/1
Batch Loss = 0.0007310406217920277
1413, epoch_train_loss=0.0007310406217920277
Epoch 1414
Epoch 1414 :: Batch 0/1
Batch Loss = 0.0007310406217796229
1414, epoch_train_loss=0.0007310406217796229
Epoch 1415
Epoch 1415 :: Batch 0/1
Batch Loss = 0.000731040621767212
1415, epoch_train_loss=0.000731040621767212
Epoch 1416
Epoch 1416 :: Batch 0/1
Batch Loss = 0.0007310406217547948
1416, epoch_train_loss=0.0007310406217547948
Epoch 1417
Epoch 1417 :: Batch 0/1
Batch Loss = 0.0007310406217423709
1417, epoch_train_loss=0.0007310406217423709
Epoch 1418
Epoch 1418 :: Batch 0/1
Batch Loss = 0.0007310406217299409
1418, epoch_train_loss=0.0007310406217299409
Epoch 1419
Epoch 1419 :: Batch 0/1
Batch Loss = 0.0007310406217175047
1419, epoch_train_loss=0.0007310406217175047
Epoch 1420
Epoch 1420 :: Batch 0/1
Batch Loss = 0.000731040621705062
1420, epoch_train_loss=0.000731040621705062
Epoch 1421
Epoch 1421 :: Batch 0/1
Batch Loss = 0.0007310406216926128
1421, epoch_train_loss=0.0007310406216926128
Epoch 1422
Epoch 1422 :: Batch 0/1
Batch Loss = 0.0007310406216801575
1422, epoch_train_loss=0.0007310406216801575
Epoch 1423
Epoch 1423 :: Batch 0/1
Batch Loss = 0.0007310406216676958
1423, epoch_train_loss=0.0007310406216676958
Epoch 1424
Epoch 1424 :: Batch 0/1
Batch Loss = 0.0007310406216552276
1424, epoch_train_loss=0.0007310406216552276
Epoch 1425
Epoch 1425 :: Batch 0/1
Batch Loss = 0.0007310406216427532
1425, epoch_train_loss=0.0007310406216427532
Epoch 1426
Epoch 1426 :: Batch 0/1
Batch Loss = 0.0007310406216302725
1426, epoch_train_loss=0.0007310406216302725
Epoch 1427
Epoch 1427 :: Batch 0/1
Batch Loss = 0.0007310406216177853
1427, epoch_train_loss=0.0007310406216177853
Epoch 1428
Epoch 1428 :: Batch 0/1
Batch Loss = 0.0007310406216052918
1428, epoch_train_loss=0.0007310406216052918
Epoch 1429
Epoch 1429 :: Batch 0/1
Batch Loss = 0.000731040621592792
1429, epoch_train_loss=0.000731040621592792
Epoch 1430
Epoch 1430 :: Batch 0/1
Batch Loss = 0.0007310406215802858
1430, epoch_train_loss=0.0007310406215802858
Epoch 1431
Epoch 1431 :: Batch 0/1
Batch Loss = 0.0007310406215677735
1431, epoch_train_loss=0.0007310406215677735
Epoch 1432
Epoch 1432 :: Batch 0/1
Batch Loss = 0.0007310406215552544
1432, epoch_train_loss=0.0007310406215552544
Epoch 1433
Epoch 1433 :: Batch 0/1
Batch Loss = 0.0007310406215427293
1433, epoch_train_loss=0.0007310406215427293
Epoch 1434
Epoch 1434 :: Batch 0/1
Batch Loss = 0.0007310406215301976
1434, epoch_train_loss=0.0007310406215301976
Epoch 1435
Epoch 1435 :: Batch 0/1
Batch Loss = 0.0007310406215176598
1435, epoch_train_loss=0.0007310406215176598
Epoch 1436
Epoch 1436 :: Batch 0/1
Batch Loss = 0.0007310406215051156
1436, epoch_train_loss=0.0007310406215051156
Epoch 1437
Epoch 1437 :: Batch 0/1
Batch Loss = 0.000731040621492565
1437, epoch_train_loss=0.000731040621492565
Epoch 1438
Epoch 1438 :: Batch 0/1
Batch Loss = 0.0007310406214800079
1438, epoch_train_loss=0.0007310406214800079
Epoch 1439
Epoch 1439 :: Batch 0/1
Batch Loss = 0.0007310406214674446
1439, epoch_train_loss=0.0007310406214674446
Epoch 1440
Epoch 1440 :: Batch 0/1
Batch Loss = 0.0007310406214548752
1440, epoch_train_loss=0.0007310406214548752
Epoch 1441
Epoch 1441 :: Batch 0/1
Batch Loss = 0.0007310406214422991
1441, epoch_train_loss=0.0007310406214422991
Epoch 1442
Epoch 1442 :: Batch 0/1
Batch Loss = 0.0007310406214297167
1442, epoch_train_loss=0.0007310406214297167
Epoch 1443
Epoch 1443 :: Batch 0/1
Batch Loss = 0.000731040621417128
1443, epoch_train_loss=0.000731040621417128
Epoch 1444
Epoch 1444 :: Batch 0/1
Batch Loss = 0.0007310406214045332
1444, epoch_train_loss=0.0007310406214045332
Epoch 1445
Epoch 1445 :: Batch 0/1
Batch Loss = 0.0007310406213919317
1445, epoch_train_loss=0.0007310406213919317
Epoch 1446
Epoch 1446 :: Batch 0/1
Batch Loss = 0.000731040621379324
1446, epoch_train_loss=0.000731040621379324
Epoch 1447
Epoch 1447 :: Batch 0/1
Batch Loss = 0.0007310406213667099
1447, epoch_train_loss=0.0007310406213667099
Epoch 1448
Epoch 1448 :: Batch 0/1
Batch Loss = 0.0007310406213540895
1448, epoch_train_loss=0.0007310406213540895
Epoch 1449
Epoch 1449 :: Batch 0/1
Batch Loss = 0.000731040621341463
1449, epoch_train_loss=0.000731040621341463
Epoch 1450
Epoch 1450 :: Batch 0/1
Batch Loss = 0.0007310406213288297
1450, epoch_train_loss=0.0007310406213288297
Epoch 1451
Epoch 1451 :: Batch 0/1
Batch Loss = 0.0007310406213161903
1451, epoch_train_loss=0.0007310406213161903
Epoch 1452
Epoch 1452 :: Batch 0/1
Batch Loss = 0.0007310406213035446
1452, epoch_train_loss=0.0007310406213035446
Epoch 1453
Epoch 1453 :: Batch 0/1
Batch Loss = 0.0007310406212908924
1453, epoch_train_loss=0.0007310406212908924
Epoch 1454
Epoch 1454 :: Batch 0/1
Batch Loss = 0.0007310406212782339
1454, epoch_train_loss=0.0007310406212782339
Epoch 1455
Epoch 1455 :: Batch 0/1
Batch Loss = 0.0007310406212655692
1455, epoch_train_loss=0.0007310406212655692
Epoch 1456
Epoch 1456 :: Batch 0/1
Batch Loss = 0.0007310406212528979
1456, epoch_train_loss=0.0007310406212528979
Epoch 1457
Epoch 1457 :: Batch 0/1
Batch Loss = 0.0007310406212402205
1457, epoch_train_loss=0.0007310406212402205
Epoch 1458
Epoch 1458 :: Batch 0/1
Batch Loss = 0.0007310406212275367
1458, epoch_train_loss=0.0007310406212275367
Epoch 1459
Epoch 1459 :: Batch 0/1
Batch Loss = 0.0007310406212148465
1459, epoch_train_loss=0.0007310406212148465
Epoch 1460
Epoch 1460 :: Batch 0/1
Batch Loss = 0.0007310406212021499
1460, epoch_train_loss=0.0007310406212021499
Epoch 1461
Epoch 1461 :: Batch 0/1
Batch Loss = 0.0007310406211894472
1461, epoch_train_loss=0.0007310406211894472
Epoch 1462
Epoch 1462 :: Batch 0/1
Batch Loss = 0.0007310406211767379
1462, epoch_train_loss=0.0007310406211767379
Epoch 1463
Epoch 1463 :: Batch 0/1
Batch Loss = 0.0007310406211640223
1463, epoch_train_loss=0.0007310406211640223
Epoch 1464
Epoch 1464 :: Batch 0/1
Batch Loss = 0.0007310406211513003
1464, epoch_train_loss=0.0007310406211513003
Epoch 1465
Epoch 1465 :: Batch 0/1
Batch Loss = 0.0007310406211385721
1465, epoch_train_loss=0.0007310406211385721
Epoch 1466
Epoch 1466 :: Batch 0/1
Batch Loss = 0.0007310406211258376
1466, epoch_train_loss=0.0007310406211258376
Epoch 1467
Epoch 1467 :: Batch 0/1
Batch Loss = 0.0007310406211130967
1467, epoch_train_loss=0.0007310406211130967
Epoch 1468
Epoch 1468 :: Batch 0/1
Batch Loss = 0.0007310406211003495
1468, epoch_train_loss=0.0007310406211003495
Epoch 1469
Epoch 1469 :: Batch 0/1
Batch Loss = 0.0007310406210875957
1469, epoch_train_loss=0.0007310406210875957
Epoch 1470
Epoch 1470 :: Batch 0/1
Batch Loss = 0.0007310406210748358
1470, epoch_train_loss=0.0007310406210748358
Epoch 1471
Epoch 1471 :: Batch 0/1
Batch Loss = 0.0007310406210620695
1471, epoch_train_loss=0.0007310406210620695
Epoch 1472
Epoch 1472 :: Batch 0/1
Batch Loss = 0.0007310406210492969
1472, epoch_train_loss=0.0007310406210492969
Epoch 1473
Epoch 1473 :: Batch 0/1
Batch Loss = 0.000731040621036518
1473, epoch_train_loss=0.000731040621036518
Epoch 1474
Epoch 1474 :: Batch 0/1
Batch Loss = 0.0007310406210237326
1474, epoch_train_loss=0.0007310406210237326
Epoch 1475
Epoch 1475 :: Batch 0/1
Batch Loss = 0.0007310406210109409
1475, epoch_train_loss=0.0007310406210109409
Epoch 1476
Epoch 1476 :: Batch 0/1
Batch Loss = 0.000731040620998143
1476, epoch_train_loss=0.000731040620998143
Epoch 1477
Epoch 1477 :: Batch 0/1
Batch Loss = 0.0007310406209853385
1477, epoch_train_loss=0.0007310406209853385
Epoch 1478
Epoch 1478 :: Batch 0/1
Batch Loss = 0.0007310406209725278
1478, epoch_train_loss=0.0007310406209725278
Epoch 1479
Epoch 1479 :: Batch 0/1
Batch Loss = 0.0007310406209597108
1479, epoch_train_loss=0.0007310406209597108
Epoch 1480
Epoch 1480 :: Batch 0/1
Batch Loss = 0.0007310406209468873
1480, epoch_train_loss=0.0007310406209468873
Epoch 1481
Epoch 1481 :: Batch 0/1
Batch Loss = 0.0007310406209340577
1481, epoch_train_loss=0.0007310406209340577
Epoch 1482
Epoch 1482 :: Batch 0/1
Batch Loss = 0.0007310406209212216
1482, epoch_train_loss=0.0007310406209212216
Epoch 1483
Epoch 1483 :: Batch 0/1
Batch Loss = 0.0007310406209083793
1483, epoch_train_loss=0.0007310406209083793
Epoch 1484
Epoch 1484 :: Batch 0/1
Batch Loss = 0.0007310406208955305
1484, epoch_train_loss=0.0007310406208955305
Epoch 1485
Epoch 1485 :: Batch 0/1
Batch Loss = 0.0007310406208826753
1485, epoch_train_loss=0.0007310406208826753
Epoch 1486
Epoch 1486 :: Batch 0/1
Batch Loss = 0.0007310406208698139
1486, epoch_train_loss=0.0007310406208698139
Epoch 1487
Epoch 1487 :: Batch 0/1
Batch Loss = 0.0007310406208569461
1487, epoch_train_loss=0.0007310406208569461
Epoch 1488
Epoch 1488 :: Batch 0/1
Batch Loss = 0.0007310406208440722
1488, epoch_train_loss=0.0007310406208440722
Epoch 1489
Epoch 1489 :: Batch 0/1
Batch Loss = 0.0007310406208311917
1489, epoch_train_loss=0.0007310406208311917
Epoch 1490
Epoch 1490 :: Batch 0/1
Batch Loss = 0.0007310406208183048
1490, epoch_train_loss=0.0007310406208183048
Epoch 1491
Epoch 1491 :: Batch 0/1
Batch Loss = 0.0007310406208054118
1491, epoch_train_loss=0.0007310406208054118
Epoch 1492
Epoch 1492 :: Batch 0/1
Batch Loss = 0.0007310406207925121
1492, epoch_train_loss=0.0007310406207925121
Epoch 1493
Epoch 1493 :: Batch 0/1
Batch Loss = 0.0007310406207796065
1493, epoch_train_loss=0.0007310406207796065
Epoch 1494
Epoch 1494 :: Batch 0/1
Batch Loss = 0.0007310406207666944
1494, epoch_train_loss=0.0007310406207666944
Epoch 1495
Epoch 1495 :: Batch 0/1
Batch Loss = 0.0007310406207537757
1495, epoch_train_loss=0.0007310406207537757
Epoch 1496
Epoch 1496 :: Batch 0/1
Batch Loss = 0.0007310406207408509
1496, epoch_train_loss=0.0007310406207408509
Epoch 1497
Epoch 1497 :: Batch 0/1
Batch Loss = 0.0007310406207279198
1497, epoch_train_loss=0.0007310406207279198
Epoch 1498
Epoch 1498 :: Batch 0/1
Batch Loss = 0.0007310406207149823
1498, epoch_train_loss=0.0007310406207149823
Epoch 1499
Epoch 1499 :: Batch 0/1
Batch Loss = 0.0007310406207020385
1499, epoch_train_loss=0.0007310406207020385
Epoch 1500
Epoch 1500 :: Batch 0/1
Batch Loss = 0.0007310406206890882
1500, epoch_train_loss=0.0007310406206890882
Epoch 1501
Epoch 1501 :: Batch 0/1
Batch Loss = 0.0007310406206761317
1501, epoch_train_loss=0.0007310406206761317
Epoch 1502
Epoch 1502 :: Batch 0/1
Batch Loss = 0.0007310406206631688
1502, epoch_train_loss=0.0007310406206631688
Epoch 1503
Epoch 1503 :: Batch 0/1
Batch Loss = 0.0007310406206501996
1503, epoch_train_loss=0.0007310406206501996
Epoch 1504
Epoch 1504 :: Batch 0/1
Batch Loss = 0.000731040620637224
1504, epoch_train_loss=0.000731040620637224
Epoch 1505
Epoch 1505 :: Batch 0/1
Batch Loss = 0.0007310406206242422
1505, epoch_train_loss=0.0007310406206242422
Epoch 1506
Epoch 1506 :: Batch 0/1
Batch Loss = 0.0007310406206112539
1506, epoch_train_loss=0.0007310406206112539
Epoch 1507
Epoch 1507 :: Batch 0/1
Batch Loss = 0.0007310406205982593
1507, epoch_train_loss=0.0007310406205982593
Epoch 1508
Epoch 1508 :: Batch 0/1
Batch Loss = 0.0007310406205852584
1508, epoch_train_loss=0.0007310406205852584
Epoch 1509
Epoch 1509 :: Batch 0/1
Batch Loss = 0.0007310406205722511
1509, epoch_train_loss=0.0007310406205722511
Epoch 1510
Epoch 1510 :: Batch 0/1
Batch Loss = 0.0007310406205592376
1510, epoch_train_loss=0.0007310406205592376
Epoch 1511
Epoch 1511 :: Batch 0/1
Batch Loss = 0.0007310406205462177
1511, epoch_train_loss=0.0007310406205462177
Epoch 1512
Epoch 1512 :: Batch 0/1
Batch Loss = 0.0007310406205331913
1512, epoch_train_loss=0.0007310406205331913
Epoch 1513
Epoch 1513 :: Batch 0/1
Batch Loss = 0.0007310406205201587
1513, epoch_train_loss=0.0007310406205201587
Epoch 1514
Epoch 1514 :: Batch 0/1
Batch Loss = 0.0007310406205071197
1514, epoch_train_loss=0.0007310406205071197
Epoch 1515
Epoch 1515 :: Batch 0/1
Batch Loss = 0.0007310406204940745
1515, epoch_train_loss=0.0007310406204940745
Epoch 1516
Epoch 1516 :: Batch 0/1
Batch Loss = 0.0007310406204810229
1516, epoch_train_loss=0.0007310406204810229
Epoch 1517
Epoch 1517 :: Batch 0/1
Batch Loss = 0.0007310406204679651
1517, epoch_train_loss=0.0007310406204679651
Epoch 1518
Epoch 1518 :: Batch 0/1
Batch Loss = 0.0007310406204549006
1518, epoch_train_loss=0.0007310406204549006
Epoch 1519
Epoch 1519 :: Batch 0/1
Batch Loss = 0.0007310406204418299
1519, epoch_train_loss=0.0007310406204418299
Epoch 1520
Epoch 1520 :: Batch 0/1
Batch Loss = 0.0007310406204287529
1520, epoch_train_loss=0.0007310406204287529
Epoch 1521
Epoch 1521 :: Batch 0/1
Batch Loss = 0.0007310406204156697
1521, epoch_train_loss=0.0007310406204156697
Epoch 1522
Epoch 1522 :: Batch 0/1
Batch Loss = 0.00073104062040258
1522, epoch_train_loss=0.00073104062040258
Epoch 1523
Epoch 1523 :: Batch 0/1
Batch Loss = 0.000731040620389484
1523, epoch_train_loss=0.000731040620389484
Epoch 1524
Epoch 1524 :: Batch 0/1
Batch Loss = 0.0007310406203763817
1524, epoch_train_loss=0.0007310406203763817
Epoch 1525
Epoch 1525 :: Batch 0/1
Batch Loss = 0.000731040620363273
1525, epoch_train_loss=0.000731040620363273
Epoch 1526
Epoch 1526 :: Batch 0/1
Batch Loss = 0.0007310406203501579
1526, epoch_train_loss=0.0007310406203501579
Epoch 1527
Epoch 1527 :: Batch 0/1
Batch Loss = 0.0007310406203370367
1527, epoch_train_loss=0.0007310406203370367
Epoch 1528
Epoch 1528 :: Batch 0/1
Batch Loss = 0.000731040620323909
1528, epoch_train_loss=0.000731040620323909
Epoch 1529
Epoch 1529 :: Batch 0/1
Batch Loss = 0.0007310406203107749
1529, epoch_train_loss=0.0007310406203107749
Epoch 1530
Epoch 1530 :: Batch 0/1
Batch Loss = 0.0007310406202976345
1530, epoch_train_loss=0.0007310406202976345
Epoch 1531
Epoch 1531 :: Batch 0/1
Batch Loss = 0.0007310406202844879
1531, epoch_train_loss=0.0007310406202844879
Epoch 1532
Epoch 1532 :: Batch 0/1
Batch Loss = 0.0007310406202713348
1532, epoch_train_loss=0.0007310406202713348
Epoch 1533
Epoch 1533 :: Batch 0/1
Batch Loss = 0.0007310406202581753
1533, epoch_train_loss=0.0007310406202581753
Epoch 1534
Epoch 1534 :: Batch 0/1
Batch Loss = 0.0007310406202450097
1534, epoch_train_loss=0.0007310406202450097
Epoch 1535
Epoch 1535 :: Batch 0/1
Batch Loss = 0.0007310406202318376
1535, epoch_train_loss=0.0007310406202318376
Epoch 1536
Epoch 1536 :: Batch 0/1
Batch Loss = 0.0007310406202186592
1536, epoch_train_loss=0.0007310406202186592
Epoch 1537
Epoch 1537 :: Batch 0/1
Batch Loss = 0.0007310406202054745
1537, epoch_train_loss=0.0007310406202054745
Epoch 1538
Epoch 1538 :: Batch 0/1
Batch Loss = 0.0007310406201922835
1538, epoch_train_loss=0.0007310406201922835
Epoch 1539
Epoch 1539 :: Batch 0/1
Batch Loss = 0.0007310406201790859
1539, epoch_train_loss=0.0007310406201790859
Epoch 1540
Epoch 1540 :: Batch 0/1
Batch Loss = 0.0007310406201658823
1540, epoch_train_loss=0.0007310406201658823
Epoch 1541
Epoch 1541 :: Batch 0/1
Batch Loss = 0.0007310406201526721
1541, epoch_train_loss=0.0007310406201526721
Epoch 1542
Epoch 1542 :: Batch 0/1
Batch Loss = 0.0007310406201394558
1542, epoch_train_loss=0.0007310406201394558
Epoch 1543
Epoch 1543 :: Batch 0/1
Batch Loss = 0.000731040620126233
1543, epoch_train_loss=0.000731040620126233
Epoch 1544
Epoch 1544 :: Batch 0/1
Batch Loss = 0.0007310406201130039
1544, epoch_train_loss=0.0007310406201130039
Epoch 1545
Epoch 1545 :: Batch 0/1
Batch Loss = 0.0007310406200997684
1545, epoch_train_loss=0.0007310406200997684
Epoch 1546
Epoch 1546 :: Batch 0/1
Batch Loss = 0.0007310406200865267
1546, epoch_train_loss=0.0007310406200865267
Epoch 1547
Epoch 1547 :: Batch 0/1
Batch Loss = 0.0007310406200732786
1547, epoch_train_loss=0.0007310406200732786
Epoch 1548
Epoch 1548 :: Batch 0/1
Batch Loss = 0.0007310406200600241
1548, epoch_train_loss=0.0007310406200600241
Epoch 1549
Epoch 1549 :: Batch 0/1
Batch Loss = 0.0007310406200467634
1549, epoch_train_loss=0.0007310406200467634
Epoch 1550
Epoch 1550 :: Batch 0/1
Batch Loss = 0.0007310406200334961
1550, epoch_train_loss=0.0007310406200334961
Epoch 1551
Epoch 1551 :: Batch 0/1
Batch Loss = 0.0007310406200202228
1551, epoch_train_loss=0.0007310406200202228
Epoch 1552
Epoch 1552 :: Batch 0/1
Batch Loss = 0.000731040620006943
1552, epoch_train_loss=0.000731040620006943
Epoch 1553
Epoch 1553 :: Batch 0/1
Batch Loss = 0.0007310406199936567
1553, epoch_train_loss=0.0007310406199936567
Epoch 1554
Epoch 1554 :: Batch 0/1
Batch Loss = 0.0007310406199803643
1554, epoch_train_loss=0.0007310406199803643
Epoch 1555
Epoch 1555 :: Batch 0/1
Batch Loss = 0.0007310406199670656
1555, epoch_train_loss=0.0007310406199670656
Epoch 1556
Epoch 1556 :: Batch 0/1
Batch Loss = 0.0007310406199537604
1556, epoch_train_loss=0.0007310406199537604
Epoch 1557
Epoch 1557 :: Batch 0/1
Batch Loss = 0.000731040619940449
1557, epoch_train_loss=0.000731040619940449
Epoch 1558
Epoch 1558 :: Batch 0/1
Batch Loss = 0.0007310406199271312
1558, epoch_train_loss=0.0007310406199271312
Epoch 1559
Epoch 1559 :: Batch 0/1
Batch Loss = 0.000731040619913807
1559, epoch_train_loss=0.000731040619913807
Epoch 1560
Epoch 1560 :: Batch 0/1
Batch Loss = 0.0007310406199004765
1560, epoch_train_loss=0.0007310406199004765
Epoch 1561
Epoch 1561 :: Batch 0/1
Batch Loss = 0.0007310406198871397
1561, epoch_train_loss=0.0007310406198871397
Epoch 1562
Epoch 1562 :: Batch 0/1
Batch Loss = 0.0007310406198737965
1562, epoch_train_loss=0.0007310406198737965
Epoch 1563
Epoch 1563 :: Batch 0/1
Batch Loss = 0.000731040619860447
1563, epoch_train_loss=0.000731040619860447
Epoch 1564
Epoch 1564 :: Batch 0/1
Batch Loss = 0.0007310406198470912
1564, epoch_train_loss=0.0007310406198470912
Epoch 1565
Epoch 1565 :: Batch 0/1
Batch Loss = 0.0007310406198337291
1565, epoch_train_loss=0.0007310406198337291
Epoch 1566
Epoch 1566 :: Batch 0/1
Batch Loss = 0.0007310406198203606
1566, epoch_train_loss=0.0007310406198203606
Epoch 1567
Epoch 1567 :: Batch 0/1
Batch Loss = 0.0007310406198069857
1567, epoch_train_loss=0.0007310406198069857
Epoch 1568
Epoch 1568 :: Batch 0/1
Batch Loss = 0.0007310406197936046
1568, epoch_train_loss=0.0007310406197936046
Epoch 1569
Epoch 1569 :: Batch 0/1
Batch Loss = 0.000731040619780217
1569, epoch_train_loss=0.000731040619780217
Epoch 1570
Epoch 1570 :: Batch 0/1
Batch Loss = 0.0007310406197668232
1570, epoch_train_loss=0.0007310406197668232
Epoch 1571
Epoch 1571 :: Batch 0/1
Batch Loss = 0.000731040619753423
1571, epoch_train_loss=0.000731040619753423
Epoch 1572
Epoch 1572 :: Batch 0/1
Batch Loss = 0.0007310406197400163
1572, epoch_train_loss=0.0007310406197400163
Epoch 1573
Epoch 1573 :: Batch 0/1
Batch Loss = 0.0007310406197266036
1573, epoch_train_loss=0.0007310406197266036
Epoch 1574
Epoch 1574 :: Batch 0/1
Batch Loss = 0.0007310406197131844
1574, epoch_train_loss=0.0007310406197131844
Epoch 1575
Epoch 1575 :: Batch 0/1
Batch Loss = 0.0007310406196997589
1575, epoch_train_loss=0.0007310406196997589
Epoch 1576
Epoch 1576 :: Batch 0/1
Batch Loss = 0.0007310406196863271
1576, epoch_train_loss=0.0007310406196863271
Epoch 1577
Epoch 1577 :: Batch 0/1
Batch Loss = 0.0007310406196728887
1577, epoch_train_loss=0.0007310406196728887
Epoch 1578
Epoch 1578 :: Batch 0/1
Batch Loss = 0.0007310406196594443
1578, epoch_train_loss=0.0007310406196594443
Epoch 1579
Epoch 1579 :: Batch 0/1
Batch Loss = 0.0007310406196459936
1579, epoch_train_loss=0.0007310406196459936
Epoch 1580
Epoch 1580 :: Batch 0/1
Batch Loss = 0.0007310406196325362
1580, epoch_train_loss=0.0007310406196325362
Epoch 1581
Epoch 1581 :: Batch 0/1
Batch Loss = 0.0007310406196190727
1581, epoch_train_loss=0.0007310406196190727
Epoch 1582
Epoch 1582 :: Batch 0/1
Batch Loss = 0.0007310406196056028
1582, epoch_train_loss=0.0007310406196056028
Epoch 1583
Epoch 1583 :: Batch 0/1
Batch Loss = 0.0007310406195921266
1583, epoch_train_loss=0.0007310406195921266
Epoch 1584
Epoch 1584 :: Batch 0/1
Batch Loss = 0.0007310406195786441
1584, epoch_train_loss=0.0007310406195786441
Epoch 1585
Epoch 1585 :: Batch 0/1
Batch Loss = 0.000731040619565155
1585, epoch_train_loss=0.000731040619565155
Epoch 1586
Epoch 1586 :: Batch 0/1
Batch Loss = 0.0007310406195516599
1586, epoch_train_loss=0.0007310406195516599
Epoch 1587
Epoch 1587 :: Batch 0/1
Batch Loss = 0.0007310406195381583
1587, epoch_train_loss=0.0007310406195381583
Epoch 1588
Epoch 1588 :: Batch 0/1
Batch Loss = 0.0007310406195246504
1588, epoch_train_loss=0.0007310406195246504
Epoch 1589
Epoch 1589 :: Batch 0/1
Batch Loss = 0.0007310406195111363
1589, epoch_train_loss=0.0007310406195111363
Epoch 1590
Epoch 1590 :: Batch 0/1
Batch Loss = 0.0007310406194976156
1590, epoch_train_loss=0.0007310406194976156
Epoch 1591
Epoch 1591 :: Batch 0/1
Batch Loss = 0.0007310406194840889
1591, epoch_train_loss=0.0007310406194840889
Epoch 1592
Epoch 1592 :: Batch 0/1
Batch Loss = 0.0007310406194705554
1592, epoch_train_loss=0.0007310406194705554
Epoch 1593
Epoch 1593 :: Batch 0/1
Batch Loss = 0.0007310406194570159
1593, epoch_train_loss=0.0007310406194570159
Epoch 1594
Epoch 1594 :: Batch 0/1
Batch Loss = 0.00073104061944347
1594, epoch_train_loss=0.00073104061944347
Epoch 1595
Epoch 1595 :: Batch 0/1
Batch Loss = 0.0007310406194299179
1595, epoch_train_loss=0.0007310406194299179
Epoch 1596
Epoch 1596 :: Batch 0/1
Batch Loss = 0.0007310406194163593
1596, epoch_train_loss=0.0007310406194163593
Epoch 1597
Epoch 1597 :: Batch 0/1
Batch Loss = 0.0007310406194027943
1597, epoch_train_loss=0.0007310406194027943
Epoch 1598
Epoch 1598 :: Batch 0/1
Batch Loss = 0.0007310406193892231
1598, epoch_train_loss=0.0007310406193892231
Epoch 1599
Epoch 1599 :: Batch 0/1
Batch Loss = 0.0007310406193756455
1599, epoch_train_loss=0.0007310406193756455
Epoch 1600
Epoch 1600 :: Batch 0/1
Batch Loss = 0.0007310406193620616
1600, epoch_train_loss=0.0007310406193620616
Epoch 1601
Epoch 1601 :: Batch 0/1
Batch Loss = 0.0007310406193484715
1601, epoch_train_loss=0.0007310406193484715
Epoch 1602
Epoch 1602 :: Batch 0/1
Batch Loss = 0.0007310406193348749
1602, epoch_train_loss=0.0007310406193348749
Epoch 1603
Epoch 1603 :: Batch 0/1
Batch Loss = 0.000731040619321272
1603, epoch_train_loss=0.000731040619321272
Epoch 1604
Epoch 1604 :: Batch 0/1
Batch Loss = 0.0007310406193076627
1604, epoch_train_loss=0.0007310406193076627
Epoch 1605
Epoch 1605 :: Batch 0/1
Batch Loss = 0.0007310406192940472
1605, epoch_train_loss=0.0007310406192940472
Epoch 1606
Epoch 1606 :: Batch 0/1
Batch Loss = 0.0007310406192804252
1606, epoch_train_loss=0.0007310406192804252
Epoch 1607
Epoch 1607 :: Batch 0/1
Batch Loss = 0.000731040619266797
1607, epoch_train_loss=0.000731040619266797
Epoch 1608
Epoch 1608 :: Batch 0/1
Batch Loss = 0.0007310406192531624
1608, epoch_train_loss=0.0007310406192531624
Epoch 1609
Epoch 1609 :: Batch 0/1
Batch Loss = 0.0007310406192395215
1609, epoch_train_loss=0.0007310406192395215
Epoch 1610
Epoch 1610 :: Batch 0/1
Batch Loss = 0.0007310406192258742
1610, epoch_train_loss=0.0007310406192258742
Epoch 1611
Epoch 1611 :: Batch 0/1
Batch Loss = 0.0007310406192122206
1611, epoch_train_loss=0.0007310406192122206
Epoch 1612
Epoch 1612 :: Batch 0/1
Batch Loss = 0.0007310406191985608
1612, epoch_train_loss=0.0007310406191985608
Epoch 1613
Epoch 1613 :: Batch 0/1
Batch Loss = 0.0007310406191848943
1613, epoch_train_loss=0.0007310406191848943
Epoch 1614
Epoch 1614 :: Batch 0/1
Batch Loss = 0.0007310406191712219
1614, epoch_train_loss=0.0007310406191712219
Epoch 1615
Epoch 1615 :: Batch 0/1
Batch Loss = 0.0007310406191575429
1615, epoch_train_loss=0.0007310406191575429
Epoch 1616
Epoch 1616 :: Batch 0/1
Batch Loss = 0.0007310406191438576
1616, epoch_train_loss=0.0007310406191438576
Epoch 1617
Epoch 1617 :: Batch 0/1
Batch Loss = 0.000731040619130166
1617, epoch_train_loss=0.000731040619130166
Epoch 1618
Epoch 1618 :: Batch 0/1
Batch Loss = 0.0007310406191164681
1618, epoch_train_loss=0.0007310406191164681
Epoch 1619
Epoch 1619 :: Batch 0/1
Batch Loss = 0.0007310406191027639
1619, epoch_train_loss=0.0007310406191027639
Epoch 1620
Epoch 1620 :: Batch 0/1
Batch Loss = 0.0007310406190890532
1620, epoch_train_loss=0.0007310406190890532
Epoch 1621
Epoch 1621 :: Batch 0/1
Batch Loss = 0.0007310406190753363
1621, epoch_train_loss=0.0007310406190753363
Epoch 1622
Epoch 1622 :: Batch 0/1
Batch Loss = 0.000731040619061613
1622, epoch_train_loss=0.000731040619061613
Epoch 1623
Epoch 1623 :: Batch 0/1
Batch Loss = 0.0007310406190478835
1623, epoch_train_loss=0.0007310406190478835
Epoch 1624
Epoch 1624 :: Batch 0/1
Batch Loss = 0.0007310406190341475
1624, epoch_train_loss=0.0007310406190341475
Epoch 1625
Epoch 1625 :: Batch 0/1
Batch Loss = 0.0007310406190204053
1625, epoch_train_loss=0.0007310406190204053
Epoch 1626
Epoch 1626 :: Batch 0/1
Batch Loss = 0.0007310406190066567
1626, epoch_train_loss=0.0007310406190066567
Epoch 1627
Epoch 1627 :: Batch 0/1
Batch Loss = 0.0007310406189929018
1627, epoch_train_loss=0.0007310406189929018
Epoch 1628
Epoch 1628 :: Batch 0/1
Batch Loss = 0.0007310406189791405
1628, epoch_train_loss=0.0007310406189791405
Epoch 1629
Epoch 1629 :: Batch 0/1
Batch Loss = 0.0007310406189653728
1629, epoch_train_loss=0.0007310406189653728
Epoch 1630
Epoch 1630 :: Batch 0/1
Batch Loss = 0.0007310406189515988
1630, epoch_train_loss=0.0007310406189515988
Epoch 1631
Epoch 1631 :: Batch 0/1
Batch Loss = 0.0007310406189378186
1631, epoch_train_loss=0.0007310406189378186
Epoch 1632
Epoch 1632 :: Batch 0/1
Batch Loss = 0.000731040618924032
1632, epoch_train_loss=0.000731040618924032
Epoch 1633
Epoch 1633 :: Batch 0/1
Batch Loss = 0.0007310406189102392
1633, epoch_train_loss=0.0007310406189102392
Epoch 1634
Epoch 1634 :: Batch 0/1
Batch Loss = 0.0007310406188964398
1634, epoch_train_loss=0.0007310406188964398
Epoch 1635
Epoch 1635 :: Batch 0/1
Batch Loss = 0.0007310406188826342
1635, epoch_train_loss=0.0007310406188826342
Epoch 1636
Epoch 1636 :: Batch 0/1
Batch Loss = 0.0007310406188688223
1636, epoch_train_loss=0.0007310406188688223
Epoch 1637
Epoch 1637 :: Batch 0/1
Batch Loss = 0.0007310406188550041
1637, epoch_train_loss=0.0007310406188550041
Epoch 1638
Epoch 1638 :: Batch 0/1
Batch Loss = 0.0007310406188411795
1638, epoch_train_loss=0.0007310406188411795
Epoch 1639
Epoch 1639 :: Batch 0/1
Batch Loss = 0.0007310406188273485
1639, epoch_train_loss=0.0007310406188273485
Epoch 1640
Epoch 1640 :: Batch 0/1
Batch Loss = 0.0007310406188135113
1640, epoch_train_loss=0.0007310406188135113
Epoch 1641
Epoch 1641 :: Batch 0/1
Batch Loss = 0.0007310406187996675
1641, epoch_train_loss=0.0007310406187996675
Epoch 1642
Epoch 1642 :: Batch 0/1
Batch Loss = 0.0007310406187858177
1642, epoch_train_loss=0.0007310406187858177
Epoch 1643
Epoch 1643 :: Batch 0/1
Batch Loss = 0.0007310406187719613
1643, epoch_train_loss=0.0007310406187719613
Epoch 1644
Epoch 1644 :: Batch 0/1
Batch Loss = 0.0007310406187580988
1644, epoch_train_loss=0.0007310406187580988
Epoch 1645
Epoch 1645 :: Batch 0/1
Batch Loss = 0.0007310406187442299
1645, epoch_train_loss=0.0007310406187442299
Epoch 1646
Epoch 1646 :: Batch 0/1
Batch Loss = 0.0007310406187303546
1646, epoch_train_loss=0.0007310406187303546
Epoch 1647
Epoch 1647 :: Batch 0/1
Batch Loss = 0.0007310406187164729
1647, epoch_train_loss=0.0007310406187164729
Epoch 1648
Epoch 1648 :: Batch 0/1
Batch Loss = 0.0007310406187025849
1648, epoch_train_loss=0.0007310406187025849
Epoch 1649
Epoch 1649 :: Batch 0/1
Batch Loss = 0.0007310406186886907
1649, epoch_train_loss=0.0007310406186886907
Epoch 1650
Epoch 1650 :: Batch 0/1
Batch Loss = 0.0007310406186747901
1650, epoch_train_loss=0.0007310406186747901
Epoch 1651
Epoch 1651 :: Batch 0/1
Batch Loss = 0.0007310406186608831
1651, epoch_train_loss=0.0007310406186608831
Epoch 1652
Epoch 1652 :: Batch 0/1
Batch Loss = 0.0007310406186469698
1652, epoch_train_loss=0.0007310406186469698
Epoch 1653
Epoch 1653 :: Batch 0/1
Batch Loss = 0.0007310406186330502
1653, epoch_train_loss=0.0007310406186330502
Epoch 1654
Epoch 1654 :: Batch 0/1
Batch Loss = 0.0007310406186191244
1654, epoch_train_loss=0.0007310406186191244
Epoch 1655
Epoch 1655 :: Batch 0/1
Batch Loss = 0.0007310406186051921
1655, epoch_train_loss=0.0007310406186051921
Epoch 1656
Epoch 1656 :: Batch 0/1
Batch Loss = 0.0007310406185912535
1656, epoch_train_loss=0.0007310406185912535
Epoch 1657
Epoch 1657 :: Batch 0/1
Batch Loss = 0.0007310406185773086
1657, epoch_train_loss=0.0007310406185773086
Epoch 1658
Epoch 1658 :: Batch 0/1
Batch Loss = 0.0007310406185633572
1658, epoch_train_loss=0.0007310406185633572
Epoch 1659
Epoch 1659 :: Batch 0/1
Batch Loss = 0.0007310406185493996
1659, epoch_train_loss=0.0007310406185493996
Epoch 1660
Epoch 1660 :: Batch 0/1
Batch Loss = 0.0007310406185354356
1660, epoch_train_loss=0.0007310406185354356
Epoch 1661
Epoch 1661 :: Batch 0/1
Batch Loss = 0.0007310406185214653
1661, epoch_train_loss=0.0007310406185214653
Epoch 1662
Epoch 1662 :: Batch 0/1
Batch Loss = 0.0007310406185074888
1662, epoch_train_loss=0.0007310406185074888
Epoch 1663
Epoch 1663 :: Batch 0/1
Batch Loss = 0.0007310406184935058
1663, epoch_train_loss=0.0007310406184935058
Epoch 1664
Epoch 1664 :: Batch 0/1
Batch Loss = 0.0007310406184795167
1664, epoch_train_loss=0.0007310406184795167
Epoch 1665
Epoch 1665 :: Batch 0/1
Batch Loss = 0.000731040618465521
1665, epoch_train_loss=0.000731040618465521
Epoch 1666
Epoch 1666 :: Batch 0/1
Batch Loss = 0.0007310406184515191
1666, epoch_train_loss=0.0007310406184515191
Epoch 1667
Epoch 1667 :: Batch 0/1
Batch Loss = 0.0007310406184375107
1667, epoch_train_loss=0.0007310406184375107
Epoch 1668
Epoch 1668 :: Batch 0/1
Batch Loss = 0.0007310406184234962
1668, epoch_train_loss=0.0007310406184234962
Epoch 1669
Epoch 1669 :: Batch 0/1
Batch Loss = 0.0007310406184094753
1669, epoch_train_loss=0.0007310406184094753
Epoch 1670
Epoch 1670 :: Batch 0/1
Batch Loss = 0.0007310406183954479
1670, epoch_train_loss=0.0007310406183954479
Epoch 1671
Epoch 1671 :: Batch 0/1
Batch Loss = 0.0007310406183814144
1671, epoch_train_loss=0.0007310406183814144
Epoch 1672
Epoch 1672 :: Batch 0/1
Batch Loss = 0.0007310406183673744
1672, epoch_train_loss=0.0007310406183673744
Epoch 1673
Epoch 1673 :: Batch 0/1
Batch Loss = 0.0007310406183533283
1673, epoch_train_loss=0.0007310406183533283
Epoch 1674
Epoch 1674 :: Batch 0/1
Batch Loss = 0.0007310406183392756
1674, epoch_train_loss=0.0007310406183392756
Epoch 1675
Epoch 1675 :: Batch 0/1
Batch Loss = 0.0007310406183252167
1675, epoch_train_loss=0.0007310406183252167
Epoch 1676
Epoch 1676 :: Batch 0/1
Batch Loss = 0.0007310406183111514
1676, epoch_train_loss=0.0007310406183111514
Epoch 1677
Epoch 1677 :: Batch 0/1
Batch Loss = 0.0007310406182970797
1677, epoch_train_loss=0.0007310406182970797
Epoch 1678
Epoch 1678 :: Batch 0/1
Batch Loss = 0.0007310406182830018
1678, epoch_train_loss=0.0007310406182830018
Epoch 1679
Epoch 1679 :: Batch 0/1
Batch Loss = 0.0007310406182689176
1679, epoch_train_loss=0.0007310406182689176
Epoch 1680
Epoch 1680 :: Batch 0/1
Batch Loss = 0.0007310406182548269
1680, epoch_train_loss=0.0007310406182548269
Epoch 1681
Epoch 1681 :: Batch 0/1
Batch Loss = 0.0007310406182407301
1681, epoch_train_loss=0.0007310406182407301
Epoch 1682
Epoch 1682 :: Batch 0/1
Batch Loss = 0.0007310406182266267
1682, epoch_train_loss=0.0007310406182266267
Epoch 1683
Epoch 1683 :: Batch 0/1
Batch Loss = 0.0007310406182125172
1683, epoch_train_loss=0.0007310406182125172
Epoch 1684
Epoch 1684 :: Batch 0/1
Batch Loss = 0.0007310406181984013
1684, epoch_train_loss=0.0007310406181984013
Epoch 1685
Epoch 1685 :: Batch 0/1
Batch Loss = 0.000731040618184279
1685, epoch_train_loss=0.000731040618184279
Epoch 1686
Epoch 1686 :: Batch 0/1
Batch Loss = 0.0007310406181701505
1686, epoch_train_loss=0.0007310406181701505
Epoch 1687
Epoch 1687 :: Batch 0/1
Batch Loss = 0.0007310406181560154
1687, epoch_train_loss=0.0007310406181560154
Epoch 1688
Epoch 1688 :: Batch 0/1
Batch Loss = 0.0007310406181418742
1688, epoch_train_loss=0.0007310406181418742
Epoch 1689
Epoch 1689 :: Batch 0/1
Batch Loss = 0.0007310406181277266
1689, epoch_train_loss=0.0007310406181277266
Epoch 1690
Epoch 1690 :: Batch 0/1
Batch Loss = 0.0007310406181135727
1690, epoch_train_loss=0.0007310406181135727
Epoch 1691
Epoch 1691 :: Batch 0/1
Batch Loss = 0.0007310406180994124
1691, epoch_train_loss=0.0007310406180994124
Epoch 1692
Epoch 1692 :: Batch 0/1
Batch Loss = 0.0007310406180852459
1692, epoch_train_loss=0.0007310406180852459
Epoch 1693
Epoch 1693 :: Batch 0/1
Batch Loss = 0.0007310406180710729
1693, epoch_train_loss=0.0007310406180710729
Epoch 1694
Epoch 1694 :: Batch 0/1
Batch Loss = 0.0007310406180568936
1694, epoch_train_loss=0.0007310406180568936
Epoch 1695
Epoch 1695 :: Batch 0/1
Batch Loss = 0.000731040618042708
1695, epoch_train_loss=0.000731040618042708
Epoch 1696
Epoch 1696 :: Batch 0/1
Batch Loss = 0.0007310406180285161
1696, epoch_train_loss=0.0007310406180285161
Epoch 1697
Epoch 1697 :: Batch 0/1
Batch Loss = 0.0007310406180143179
1697, epoch_train_loss=0.0007310406180143179
Epoch 1698
Epoch 1698 :: Batch 0/1
Batch Loss = 0.0007310406180001133
1698, epoch_train_loss=0.0007310406180001133
Epoch 1699
Epoch 1699 :: Batch 0/1
Batch Loss = 0.0007310406179859022
1699, epoch_train_loss=0.0007310406179859022
Epoch 1700
Epoch 1700 :: Batch 0/1
Batch Loss = 0.0007310406179716852
1700, epoch_train_loss=0.0007310406179716852
Epoch 1701
Epoch 1701 :: Batch 0/1
Batch Loss = 0.0007310406179574614
1701, epoch_train_loss=0.0007310406179574614
Epoch 1702
Epoch 1702 :: Batch 0/1
Batch Loss = 0.0007310406179432316
1702, epoch_train_loss=0.0007310406179432316
Epoch 1703
Epoch 1703 :: Batch 0/1
Batch Loss = 0.0007310406179289953
1703, epoch_train_loss=0.0007310406179289953
Epoch 1704
Epoch 1704 :: Batch 0/1
Batch Loss = 0.0007310406179147529
1704, epoch_train_loss=0.0007310406179147529
Epoch 1705
Epoch 1705 :: Batch 0/1
Batch Loss = 0.0007310406179005038
1705, epoch_train_loss=0.0007310406179005038
Epoch 1706
Epoch 1706 :: Batch 0/1
Batch Loss = 0.0007310406178862486
1706, epoch_train_loss=0.0007310406178862486
Epoch 1707
Epoch 1707 :: Batch 0/1
Batch Loss = 0.000731040617871987
1707, epoch_train_loss=0.000731040617871987
Epoch 1708
Epoch 1708 :: Batch 0/1
Batch Loss = 0.0007310406178577192
1708, epoch_train_loss=0.0007310406178577192
Epoch 1709
Epoch 1709 :: Batch 0/1
Batch Loss = 0.0007310406178434449
1709, epoch_train_loss=0.0007310406178434449
Epoch 1710
Epoch 1710 :: Batch 0/1
Batch Loss = 0.0007310406178291643
1710, epoch_train_loss=0.0007310406178291643
Epoch 1711
Epoch 1711 :: Batch 0/1
Batch Loss = 0.0007310406178148775
1711, epoch_train_loss=0.0007310406178148775
Epoch 1712
Epoch 1712 :: Batch 0/1
Batch Loss = 0.0007310406178005842
1712, epoch_train_loss=0.0007310406178005842
Epoch 1713
Epoch 1713 :: Batch 0/1
Batch Loss = 0.0007310406177862847
1713, epoch_train_loss=0.0007310406177862847
Epoch 1714
Epoch 1714 :: Batch 0/1
Batch Loss = 0.0007310406177719786
1714, epoch_train_loss=0.0007310406177719786
Epoch 1715
Epoch 1715 :: Batch 0/1
Batch Loss = 0.0007310406177576665
1715, epoch_train_loss=0.0007310406177576665
Epoch 1716
Epoch 1716 :: Batch 0/1
Batch Loss = 0.0007310406177433479
1716, epoch_train_loss=0.0007310406177433479
Epoch 1717
Epoch 1717 :: Batch 0/1
Batch Loss = 0.000731040617729023
1717, epoch_train_loss=0.000731040617729023
Epoch 1718
Epoch 1718 :: Batch 0/1
Batch Loss = 0.0007310406177146918
1718, epoch_train_loss=0.0007310406177146918
Epoch 1719
Epoch 1719 :: Batch 0/1
Batch Loss = 0.0007310406177003543
1719, epoch_train_loss=0.0007310406177003543
Epoch 1720
Epoch 1720 :: Batch 0/1
Batch Loss = 0.0007310406176860102
1720, epoch_train_loss=0.0007310406176860102
Epoch 1721
Epoch 1721 :: Batch 0/1
Batch Loss = 0.0007310406176716601
1721, epoch_train_loss=0.0007310406176716601
Epoch 1722
Epoch 1722 :: Batch 0/1
Batch Loss = 0.0007310406176573034
1722, epoch_train_loss=0.0007310406176573034
Epoch 1723
Epoch 1723 :: Batch 0/1
Batch Loss = 0.0007310406176429406
1723, epoch_train_loss=0.0007310406176429406
Epoch 1724
Epoch 1724 :: Batch 0/1
Batch Loss = 0.0007310406176285714
1724, epoch_train_loss=0.0007310406176285714
Epoch 1725
Epoch 1725 :: Batch 0/1
Batch Loss = 0.0007310406176141958
1725, epoch_train_loss=0.0007310406176141958
Epoch 1726
Epoch 1726 :: Batch 0/1
Batch Loss = 0.0007310406175998139
1726, epoch_train_loss=0.0007310406175998139
Epoch 1727
Epoch 1727 :: Batch 0/1
Batch Loss = 0.0007310406175854257
1727, epoch_train_loss=0.0007310406175854257
Epoch 1728
Epoch 1728 :: Batch 0/1
Batch Loss = 0.0007310406175710311
1728, epoch_train_loss=0.0007310406175710311
Epoch 1729
Epoch 1729 :: Batch 0/1
Batch Loss = 0.0007310406175566301
1729, epoch_train_loss=0.0007310406175566301
Epoch 1730
Epoch 1730 :: Batch 0/1
Batch Loss = 0.0007310406175422229
1730, epoch_train_loss=0.0007310406175422229
Epoch 1731
Epoch 1731 :: Batch 0/1
Batch Loss = 0.0007310406175278096
1731, epoch_train_loss=0.0007310406175278096
Epoch 1732
Epoch 1732 :: Batch 0/1
Batch Loss = 0.0007310406175133896
1732, epoch_train_loss=0.0007310406175133896
Epoch 1733
Epoch 1733 :: Batch 0/1
Batch Loss = 0.0007310406174989634
1733, epoch_train_loss=0.0007310406174989634
Epoch 1734
Epoch 1734 :: Batch 0/1
Batch Loss = 0.0007310406174845308
1734, epoch_train_loss=0.0007310406174845308
Epoch 1735
Epoch 1735 :: Batch 0/1
Batch Loss = 0.0007310406174700921
1735, epoch_train_loss=0.0007310406174700921
Epoch 1736
Epoch 1736 :: Batch 0/1
Batch Loss = 0.0007310406174556468
1736, epoch_train_loss=0.0007310406174556468
Epoch 1737
Epoch 1737 :: Batch 0/1
Batch Loss = 0.0007310406174411953
1737, epoch_train_loss=0.0007310406174411953
Epoch 1738
Epoch 1738 :: Batch 0/1
Batch Loss = 0.0007310406174267374
1738, epoch_train_loss=0.0007310406174267374
Epoch 1739
Epoch 1739 :: Batch 0/1
Batch Loss = 0.0007310406174122731
1739, epoch_train_loss=0.0007310406174122731
Epoch 1740
Epoch 1740 :: Batch 0/1
Batch Loss = 0.0007310406173978026
1740, epoch_train_loss=0.0007310406173978026
Epoch 1741
Epoch 1741 :: Batch 0/1
Batch Loss = 0.0007310406173833256
1741, epoch_train_loss=0.0007310406173833256
Epoch 1742
Epoch 1742 :: Batch 0/1
Batch Loss = 0.0007310406173688425
1742, epoch_train_loss=0.0007310406173688425
Epoch 1743
Epoch 1743 :: Batch 0/1
Batch Loss = 0.000731040617354353
1743, epoch_train_loss=0.000731040617354353
Epoch 1744
Epoch 1744 :: Batch 0/1
Batch Loss = 0.0007310406173398572
1744, epoch_train_loss=0.0007310406173398572
Epoch 1745
Epoch 1745 :: Batch 0/1
Batch Loss = 0.0007310406173253549
1745, epoch_train_loss=0.0007310406173253549
Epoch 1746
Epoch 1746 :: Batch 0/1
Batch Loss = 0.0007310406173108463
1746, epoch_train_loss=0.0007310406173108463
Epoch 1747
Epoch 1747 :: Batch 0/1
Batch Loss = 0.0007310406172963314
1747, epoch_train_loss=0.0007310406172963314
Epoch 1748
Epoch 1748 :: Batch 0/1
Batch Loss = 0.0007310406172818103
1748, epoch_train_loss=0.0007310406172818103
Epoch 1749
Epoch 1749 :: Batch 0/1
Batch Loss = 0.0007310406172672827
1749, epoch_train_loss=0.0007310406172672827
Epoch 1750
Epoch 1750 :: Batch 0/1
Batch Loss = 0.000731040617252749
1750, epoch_train_loss=0.000731040617252749
Epoch 1751
Epoch 1751 :: Batch 0/1
Batch Loss = 0.0007310406172382088
1751, epoch_train_loss=0.0007310406172382088
Epoch 1752
Epoch 1752 :: Batch 0/1
Batch Loss = 0.0007310406172236622
1752, epoch_train_loss=0.0007310406172236622
Epoch 1753
Epoch 1753 :: Batch 0/1
Batch Loss = 0.0007310406172091094
1753, epoch_train_loss=0.0007310406172091094
Epoch 1754
Epoch 1754 :: Batch 0/1
Batch Loss = 0.0007310406171945501
1754, epoch_train_loss=0.0007310406171945501
Epoch 1755
Epoch 1755 :: Batch 0/1
Batch Loss = 0.0007310406171799846
1755, epoch_train_loss=0.0007310406171799846
Epoch 1756
Epoch 1756 :: Batch 0/1
Batch Loss = 0.0007310406171654128
1756, epoch_train_loss=0.0007310406171654128
Epoch 1757
Epoch 1757 :: Batch 0/1
Batch Loss = 0.0007310406171508345
1757, epoch_train_loss=0.0007310406171508345
Epoch 1758
Epoch 1758 :: Batch 0/1
Batch Loss = 0.0007310406171362501
1758, epoch_train_loss=0.0007310406171362501
Epoch 1759
Epoch 1759 :: Batch 0/1
Batch Loss = 0.0007310406171216593
1759, epoch_train_loss=0.0007310406171216593
Epoch 1760
Epoch 1760 :: Batch 0/1
Batch Loss = 0.0007310406171070621
1760, epoch_train_loss=0.0007310406171070621
Epoch 1761
Epoch 1761 :: Batch 0/1
Batch Loss = 0.0007310406170924586
1761, epoch_train_loss=0.0007310406170924586
Epoch 1762
Epoch 1762 :: Batch 0/1
Batch Loss = 0.0007310406170778486
1762, epoch_train_loss=0.0007310406170778486
Epoch 1763
Epoch 1763 :: Batch 0/1
Batch Loss = 0.0007310406170632325
1763, epoch_train_loss=0.0007310406170632325
Epoch 1764
Epoch 1764 :: Batch 0/1
Batch Loss = 0.00073104061704861
1764, epoch_train_loss=0.00073104061704861
Epoch 1765
Epoch 1765 :: Batch 0/1
Batch Loss = 0.0007310406170339812
1765, epoch_train_loss=0.0007310406170339812
Epoch 1766
Epoch 1766 :: Batch 0/1
Batch Loss = 0.0007310406170193461
1766, epoch_train_loss=0.0007310406170193461
Epoch 1767
Epoch 1767 :: Batch 0/1
Batch Loss = 0.0007310406170047046
1767, epoch_train_loss=0.0007310406170047046
Epoch 1768
Epoch 1768 :: Batch 0/1
Batch Loss = 0.0007310406169900568
1768, epoch_train_loss=0.0007310406169900568
Epoch 1769
Epoch 1769 :: Batch 0/1
Batch Loss = 0.0007310406169754026
1769, epoch_train_loss=0.0007310406169754026
Epoch 1770
Epoch 1770 :: Batch 0/1
Batch Loss = 0.0007310406169607421
1770, epoch_train_loss=0.0007310406169607421
Epoch 1771
Epoch 1771 :: Batch 0/1
Batch Loss = 0.0007310406169460751
1771, epoch_train_loss=0.0007310406169460751
Epoch 1772
Epoch 1772 :: Batch 0/1
Batch Loss = 0.0007310406169314021
1772, epoch_train_loss=0.0007310406169314021
Epoch 1773
Epoch 1773 :: Batch 0/1
Batch Loss = 0.0007310406169167225
1773, epoch_train_loss=0.0007310406169167225
Epoch 1774
Epoch 1774 :: Batch 0/1
Batch Loss = 0.0007310406169020367
1774, epoch_train_loss=0.0007310406169020367
Epoch 1775
Epoch 1775 :: Batch 0/1
Batch Loss = 0.0007310406168873446
1775, epoch_train_loss=0.0007310406168873446
Epoch 1776
Epoch 1776 :: Batch 0/1
Batch Loss = 0.000731040616872646
1776, epoch_train_loss=0.000731040616872646
Epoch 1777
Epoch 1777 :: Batch 0/1
Batch Loss = 0.0007310406168579413
1777, epoch_train_loss=0.0007310406168579413
Epoch 1778
Epoch 1778 :: Batch 0/1
Batch Loss = 0.0007310406168432301
1778, epoch_train_loss=0.0007310406168432301
Epoch 1779
Epoch 1779 :: Batch 0/1
Batch Loss = 0.0007310406168285127
1779, epoch_train_loss=0.0007310406168285127
Epoch 1780
Epoch 1780 :: Batch 0/1
Batch Loss = 0.0007310406168137889
1780, epoch_train_loss=0.0007310406168137889
Epoch 1781
Epoch 1781 :: Batch 0/1
Batch Loss = 0.0007310406167990587
1781, epoch_train_loss=0.0007310406167990587
Epoch 1782
Epoch 1782 :: Batch 0/1
Batch Loss = 0.0007310406167843223
1782, epoch_train_loss=0.0007310406167843223
Epoch 1783
Epoch 1783 :: Batch 0/1
Batch Loss = 0.0007310406167695794
1783, epoch_train_loss=0.0007310406167695794
Epoch 1784
Epoch 1784 :: Batch 0/1
Batch Loss = 0.0007310406167548303
1784, epoch_train_loss=0.0007310406167548303
Epoch 1785
Epoch 1785 :: Batch 0/1
Batch Loss = 0.0007310406167400748
1785, epoch_train_loss=0.0007310406167400748
Epoch 1786
Epoch 1786 :: Batch 0/1
Batch Loss = 0.000731040616725313
1786, epoch_train_loss=0.000731040616725313
Epoch 1787
Epoch 1787 :: Batch 0/1
Batch Loss = 0.0007310406167105449
1787, epoch_train_loss=0.0007310406167105449
Epoch 1788
Epoch 1788 :: Batch 0/1
Batch Loss = 0.0007310406166957705
1788, epoch_train_loss=0.0007310406166957705
Epoch 1789
Epoch 1789 :: Batch 0/1
Batch Loss = 0.0007310406166809897
1789, epoch_train_loss=0.0007310406166809897
Epoch 1790
Epoch 1790 :: Batch 0/1
Batch Loss = 0.0007310406166662026
1790, epoch_train_loss=0.0007310406166662026
Epoch 1791
Epoch 1791 :: Batch 0/1
Batch Loss = 0.0007310406166514092
1791, epoch_train_loss=0.0007310406166514092
Epoch 1792
Epoch 1792 :: Batch 0/1
Batch Loss = 0.0007310406166366093
1792, epoch_train_loss=0.0007310406166366093
Epoch 1793
Epoch 1793 :: Batch 0/1
Batch Loss = 0.0007310406166218032
1793, epoch_train_loss=0.0007310406166218032
Epoch 1794
Epoch 1794 :: Batch 0/1
Batch Loss = 0.0007310406166069908
1794, epoch_train_loss=0.0007310406166069908
Epoch 1795
Epoch 1795 :: Batch 0/1
Batch Loss = 0.0007310406165921719
1795, epoch_train_loss=0.0007310406165921719
Epoch 1796
Epoch 1796 :: Batch 0/1
Batch Loss = 0.0007310406165773469
1796, epoch_train_loss=0.0007310406165773469
Epoch 1797
Epoch 1797 :: Batch 0/1
Batch Loss = 0.0007310406165625154
1797, epoch_train_loss=0.0007310406165625154
Epoch 1798
Epoch 1798 :: Batch 0/1
Batch Loss = 0.0007310406165476777
1798, epoch_train_loss=0.0007310406165476777
Epoch 1799
Epoch 1799 :: Batch 0/1
Batch Loss = 0.0007310406165328334
1799, epoch_train_loss=0.0007310406165328334
Epoch 1800
Epoch 1800 :: Batch 0/1
Batch Loss = 0.0007310406165179831
1800, epoch_train_loss=0.0007310406165179831
Epoch 1801
Epoch 1801 :: Batch 0/1
Batch Loss = 0.0007310406165031263
1801, epoch_train_loss=0.0007310406165031263
Epoch 1802
Epoch 1802 :: Batch 0/1
Batch Loss = 0.0007310406164882632
1802, epoch_train_loss=0.0007310406164882632
Epoch 1803
Epoch 1803 :: Batch 0/1
Batch Loss = 0.0007310406164733937
1803, epoch_train_loss=0.0007310406164733937
Epoch 1804
Epoch 1804 :: Batch 0/1
Batch Loss = 0.000731040616458518
1804, epoch_train_loss=0.000731040616458518
Epoch 1805
Epoch 1805 :: Batch 0/1
Batch Loss = 0.0007310406164436359
1805, epoch_train_loss=0.0007310406164436359
Epoch 1806
Epoch 1806 :: Batch 0/1
Batch Loss = 0.0007310406164287474
1806, epoch_train_loss=0.0007310406164287474
Epoch 1807
Epoch 1807 :: Batch 0/1
Batch Loss = 0.0007310406164138527
1807, epoch_train_loss=0.0007310406164138527
Epoch 1808
Epoch 1808 :: Batch 0/1
Batch Loss = 0.0007310406163989517
1808, epoch_train_loss=0.0007310406163989517
Epoch 1809
Epoch 1809 :: Batch 0/1
Batch Loss = 0.0007310406163840441
1809, epoch_train_loss=0.0007310406163840441
Epoch 1810
Epoch 1810 :: Batch 0/1
Batch Loss = 0.0007310406163691304
1810, epoch_train_loss=0.0007310406163691304
Epoch 1811
Epoch 1811 :: Batch 0/1
Batch Loss = 0.0007310406163542103
1811, epoch_train_loss=0.0007310406163542103
Epoch 1812
Epoch 1812 :: Batch 0/1
Batch Loss = 0.0007310406163392839
1812, epoch_train_loss=0.0007310406163392839
Epoch 1813
Epoch 1813 :: Batch 0/1
Batch Loss = 0.0007310406163243512
1813, epoch_train_loss=0.0007310406163243512
Epoch 1814
Epoch 1814 :: Batch 0/1
Batch Loss = 0.0007310406163094122
1814, epoch_train_loss=0.0007310406163094122
Epoch 1815
Epoch 1815 :: Batch 0/1
Batch Loss = 0.0007310406162944668
1815, epoch_train_loss=0.0007310406162944668
Epoch 1816
Epoch 1816 :: Batch 0/1
Batch Loss = 0.0007310406162795149
1816, epoch_train_loss=0.0007310406162795149
Epoch 1817
Epoch 1817 :: Batch 0/1
Batch Loss = 0.0007310406162645568
1817, epoch_train_loss=0.0007310406162645568
Epoch 1818
Epoch 1818 :: Batch 0/1
Batch Loss = 0.0007310406162495924
1818, epoch_train_loss=0.0007310406162495924
Epoch 1819
Epoch 1819 :: Batch 0/1
Batch Loss = 0.0007310406162346218
1819, epoch_train_loss=0.0007310406162346218
Epoch 1820
Epoch 1820 :: Batch 0/1
Batch Loss = 0.0007310406162196446
1820, epoch_train_loss=0.0007310406162196446
Epoch 1821
Epoch 1821 :: Batch 0/1
Batch Loss = 0.0007310406162046614
1821, epoch_train_loss=0.0007310406162046614
Epoch 1822
Epoch 1822 :: Batch 0/1
Batch Loss = 0.0007310406161896716
1822, epoch_train_loss=0.0007310406161896716
Epoch 1823
Epoch 1823 :: Batch 0/1
Batch Loss = 0.0007310406161746755
1823, epoch_train_loss=0.0007310406161746755
Epoch 1824
Epoch 1824 :: Batch 0/1
Batch Loss = 0.0007310406161596732
1824, epoch_train_loss=0.0007310406161596732
Epoch 1825
Epoch 1825 :: Batch 0/1
Batch Loss = 0.0007310406161446643
1825, epoch_train_loss=0.0007310406161446643
Epoch 1826
Epoch 1826 :: Batch 0/1
Batch Loss = 0.0007310406161296492
1826, epoch_train_loss=0.0007310406161296492
Epoch 1827
Epoch 1827 :: Batch 0/1
Batch Loss = 0.0007310406161146278
1827, epoch_train_loss=0.0007310406161146278
Epoch 1828
Epoch 1828 :: Batch 0/1
Batch Loss = 0.0007310406160996002
1828, epoch_train_loss=0.0007310406160996002
Epoch 1829
Epoch 1829 :: Batch 0/1
Batch Loss = 0.0007310406160845661
1829, epoch_train_loss=0.0007310406160845661
Epoch 1830
Epoch 1830 :: Batch 0/1
Batch Loss = 0.0007310406160695257
1830, epoch_train_loss=0.0007310406160695257
Epoch 1831
Epoch 1831 :: Batch 0/1
Batch Loss = 0.0007310406160544791
1831, epoch_train_loss=0.0007310406160544791
Epoch 1832
Epoch 1832 :: Batch 0/1
Batch Loss = 0.000731040616039426
1832, epoch_train_loss=0.000731040616039426
Epoch 1833
Epoch 1833 :: Batch 0/1
Batch Loss = 0.0007310406160243667
1833, epoch_train_loss=0.0007310406160243667
Epoch 1834
Epoch 1834 :: Batch 0/1
Batch Loss = 0.0007310406160093009
1834, epoch_train_loss=0.0007310406160093009
Epoch 1835
Epoch 1835 :: Batch 0/1
Batch Loss = 0.0007310406159942289
1835, epoch_train_loss=0.0007310406159942289
Epoch 1836
Epoch 1836 :: Batch 0/1
Batch Loss = 0.0007310406159791507
1836, epoch_train_loss=0.0007310406159791507
Epoch 1837
Epoch 1837 :: Batch 0/1
Batch Loss = 0.000731040615964066
1837, epoch_train_loss=0.000731040615964066
Epoch 1838
Epoch 1838 :: Batch 0/1
Batch Loss = 0.0007310406159489748
1838, epoch_train_loss=0.0007310406159489748
Epoch 1839
Epoch 1839 :: Batch 0/1
Batch Loss = 0.0007310406159338775
1839, epoch_train_loss=0.0007310406159338775
Epoch 1840
Epoch 1840 :: Batch 0/1
Batch Loss = 0.0007310406159187737
1840, epoch_train_loss=0.0007310406159187737
Epoch 1841
Epoch 1841 :: Batch 0/1
Batch Loss = 0.0007310406159036636
1841, epoch_train_loss=0.0007310406159036636
Epoch 1842
Epoch 1842 :: Batch 0/1
Batch Loss = 0.0007310406158885475
1842, epoch_train_loss=0.0007310406158885475
Epoch 1843
Epoch 1843 :: Batch 0/1
Batch Loss = 0.0007310406158734247
1843, epoch_train_loss=0.0007310406158734247
Epoch 1844
Epoch 1844 :: Batch 0/1
Batch Loss = 0.0007310406158582958
1844, epoch_train_loss=0.0007310406158582958
Epoch 1845
Epoch 1845 :: Batch 0/1
Batch Loss = 0.0007310406158431604
1845, epoch_train_loss=0.0007310406158431604
Epoch 1846
Epoch 1846 :: Batch 0/1
Batch Loss = 0.0007310406158280186
1846, epoch_train_loss=0.0007310406158280186
Epoch 1847
Epoch 1847 :: Batch 0/1
Batch Loss = 0.0007310406158128706
1847, epoch_train_loss=0.0007310406158128706
Epoch 1848
Epoch 1848 :: Batch 0/1
Batch Loss = 0.0007310406157977164
1848, epoch_train_loss=0.0007310406157977164
Epoch 1849
Epoch 1849 :: Batch 0/1
Batch Loss = 0.0007310406157825558
1849, epoch_train_loss=0.0007310406157825558
Epoch 1850
Epoch 1850 :: Batch 0/1
Batch Loss = 0.0007310406157673888
1850, epoch_train_loss=0.0007310406157673888
Epoch 1851
Epoch 1851 :: Batch 0/1
Batch Loss = 0.0007310406157522154
1851, epoch_train_loss=0.0007310406157522154
Epoch 1852
Epoch 1852 :: Batch 0/1
Batch Loss = 0.0007310406157370357
1852, epoch_train_loss=0.0007310406157370357
Epoch 1853
Epoch 1853 :: Batch 0/1
Batch Loss = 0.0007310406157218498
1853, epoch_train_loss=0.0007310406157218498
Epoch 1854
Epoch 1854 :: Batch 0/1
Batch Loss = 0.0007310406157066574
1854, epoch_train_loss=0.0007310406157066574
Epoch 1855
Epoch 1855 :: Batch 0/1
Batch Loss = 0.0007310406156914587
1855, epoch_train_loss=0.0007310406156914587
Epoch 1856
Epoch 1856 :: Batch 0/1
Batch Loss = 0.0007310406156762538
1856, epoch_train_loss=0.0007310406156762538
Epoch 1857
Epoch 1857 :: Batch 0/1
Batch Loss = 0.0007310406156610424
1857, epoch_train_loss=0.0007310406156610424
Epoch 1858
Epoch 1858 :: Batch 0/1
Batch Loss = 0.0007310406156458249
1858, epoch_train_loss=0.0007310406156458249
Epoch 1859
Epoch 1859 :: Batch 0/1
Batch Loss = 0.0007310406156306008
1859, epoch_train_loss=0.0007310406156306008
Epoch 1860
Epoch 1860 :: Batch 0/1
Batch Loss = 0.0007310406156153705
1860, epoch_train_loss=0.0007310406156153705
Epoch 1861
Epoch 1861 :: Batch 0/1
Batch Loss = 0.000731040615600134
1861, epoch_train_loss=0.000731040615600134
Epoch 1862
Epoch 1862 :: Batch 0/1
Batch Loss = 0.000731040615584891
1862, epoch_train_loss=0.000731040615584891
Epoch 1863
Epoch 1863 :: Batch 0/1
Batch Loss = 0.0007310406155696416
1863, epoch_train_loss=0.0007310406155696416
Epoch 1864
Epoch 1864 :: Batch 0/1
Batch Loss = 0.000731040615554386
1864, epoch_train_loss=0.000731040615554386
Epoch 1865
Epoch 1865 :: Batch 0/1
Batch Loss = 0.000731040615539124
1865, epoch_train_loss=0.000731040615539124
Epoch 1866
Epoch 1866 :: Batch 0/1
Batch Loss = 0.0007310406155238557
1866, epoch_train_loss=0.0007310406155238557
Epoch 1867
Epoch 1867 :: Batch 0/1
Batch Loss = 0.0007310406155085811
1867, epoch_train_loss=0.0007310406155085811
Epoch 1868
Epoch 1868 :: Batch 0/1
Batch Loss = 0.0007310406154933002
1868, epoch_train_loss=0.0007310406154933002
Epoch 1869
Epoch 1869 :: Batch 0/1
Batch Loss = 0.0007310406154780129
1869, epoch_train_loss=0.0007310406154780129
Epoch 1870
Epoch 1870 :: Batch 0/1
Batch Loss = 0.0007310406154627192
1870, epoch_train_loss=0.0007310406154627192
Epoch 1871
Epoch 1871 :: Batch 0/1
Batch Loss = 0.0007310406154474193
1871, epoch_train_loss=0.0007310406154474193
Epoch 1872
Epoch 1872 :: Batch 0/1
Batch Loss = 0.000731040615432113
1872, epoch_train_loss=0.000731040615432113
Epoch 1873
Epoch 1873 :: Batch 0/1
Batch Loss = 0.0007310406154168005
1873, epoch_train_loss=0.0007310406154168005
Epoch 1874
Epoch 1874 :: Batch 0/1
Batch Loss = 0.0007310406154014814
1874, epoch_train_loss=0.0007310406154014814
Epoch 1875
Epoch 1875 :: Batch 0/1
Batch Loss = 0.0007310406153861563
1875, epoch_train_loss=0.0007310406153861563
Epoch 1876
Epoch 1876 :: Batch 0/1
Batch Loss = 0.0007310406153708246
1876, epoch_train_loss=0.0007310406153708246
Epoch 1877
Epoch 1877 :: Batch 0/1
Batch Loss = 0.0007310406153554867
1877, epoch_train_loss=0.0007310406153554867
Epoch 1878
Epoch 1878 :: Batch 0/1
Batch Loss = 0.0007310406153401425
1878, epoch_train_loss=0.0007310406153401425
Epoch 1879
Epoch 1879 :: Batch 0/1
Batch Loss = 0.0007310406153247918
1879, epoch_train_loss=0.0007310406153247918
Epoch 1880
Epoch 1880 :: Batch 0/1
Batch Loss = 0.000731040615309435
1880, epoch_train_loss=0.000731040615309435
Epoch 1881
Epoch 1881 :: Batch 0/1
Batch Loss = 0.0007310406152940717
1881, epoch_train_loss=0.0007310406152940717
Epoch 1882
Epoch 1882 :: Batch 0/1
Batch Loss = 0.0007310406152787022
1882, epoch_train_loss=0.0007310406152787022
Epoch 1883
Epoch 1883 :: Batch 0/1
Batch Loss = 0.0007310406152633261
1883, epoch_train_loss=0.0007310406152633261
Epoch 1884
Epoch 1884 :: Batch 0/1
Batch Loss = 0.0007310406152479439
1884, epoch_train_loss=0.0007310406152479439
Epoch 1885
Epoch 1885 :: Batch 0/1
Batch Loss = 0.0007310406152325554
1885, epoch_train_loss=0.0007310406152325554
Epoch 1886
Epoch 1886 :: Batch 0/1
Batch Loss = 0.0007310406152171605
1886, epoch_train_loss=0.0007310406152171605
Epoch 1887
Epoch 1887 :: Batch 0/1
Batch Loss = 0.0007310406152017592
1887, epoch_train_loss=0.0007310406152017592
Epoch 1888
Epoch 1888 :: Batch 0/1
Batch Loss = 0.0007310406151863517
1888, epoch_train_loss=0.0007310406151863517
Epoch 1889
Epoch 1889 :: Batch 0/1
Batch Loss = 0.0007310406151709378
1889, epoch_train_loss=0.0007310406151709378
Epoch 1890
Epoch 1890 :: Batch 0/1
Batch Loss = 0.0007310406151555175
1890, epoch_train_loss=0.0007310406151555175
Epoch 1891
Epoch 1891 :: Batch 0/1
Batch Loss = 0.0007310406151400911
1891, epoch_train_loss=0.0007310406151400911
Epoch 1892
Epoch 1892 :: Batch 0/1
Batch Loss = 0.0007310406151246582
1892, epoch_train_loss=0.0007310406151246582
Epoch 1893
Epoch 1893 :: Batch 0/1
Batch Loss = 0.0007310406151092191
1893, epoch_train_loss=0.0007310406151092191
Epoch 1894
Epoch 1894 :: Batch 0/1
Batch Loss = 0.0007310406150937733
1894, epoch_train_loss=0.0007310406150937733
Epoch 1895
Epoch 1895 :: Batch 0/1
Batch Loss = 0.0007310406150783216
1895, epoch_train_loss=0.0007310406150783216
Epoch 1896
Epoch 1896 :: Batch 0/1
Batch Loss = 0.0007310406150628633
1896, epoch_train_loss=0.0007310406150628633
Epoch 1897
Epoch 1897 :: Batch 0/1
Batch Loss = 0.0007310406150473987
1897, epoch_train_loss=0.0007310406150473987
Epoch 1898
Epoch 1898 :: Batch 0/1
Batch Loss = 0.0007310406150319278
1898, epoch_train_loss=0.0007310406150319278
Epoch 1899
Epoch 1899 :: Batch 0/1
Batch Loss = 0.0007310406150164507
1899, epoch_train_loss=0.0007310406150164507
Epoch 1900
Epoch 1900 :: Batch 0/1
Batch Loss = 0.0007310406150009672
1900, epoch_train_loss=0.0007310406150009672
Epoch 1901
Epoch 1901 :: Batch 0/1
Batch Loss = 0.0007310406149854773
1901, epoch_train_loss=0.0007310406149854773
Epoch 1902
Epoch 1902 :: Batch 0/1
Batch Loss = 0.0007310406149699812
1902, epoch_train_loss=0.0007310406149699812
Epoch 1903
Epoch 1903 :: Batch 0/1
Batch Loss = 0.0007310406149544786
1903, epoch_train_loss=0.0007310406149544786
Epoch 1904
Epoch 1904 :: Batch 0/1
Batch Loss = 0.0007310406149389698
1904, epoch_train_loss=0.0007310406149389698
Epoch 1905
Epoch 1905 :: Batch 0/1
Batch Loss = 0.0007310406149234546
1905, epoch_train_loss=0.0007310406149234546
Epoch 1906
Epoch 1906 :: Batch 0/1
Batch Loss = 0.0007310406149079331
1906, epoch_train_loss=0.0007310406149079331
Epoch 1907
Epoch 1907 :: Batch 0/1
Batch Loss = 0.0007310406148924052
1907, epoch_train_loss=0.0007310406148924052
Epoch 1908
Epoch 1908 :: Batch 0/1
Batch Loss = 0.0007310406148768712
1908, epoch_train_loss=0.0007310406148768712
Epoch 1909
Epoch 1909 :: Batch 0/1
Batch Loss = 0.0007310406148613307
1909, epoch_train_loss=0.0007310406148613307
Epoch 1910
Epoch 1910 :: Batch 0/1
Batch Loss = 0.0007310406148457837
1910, epoch_train_loss=0.0007310406148457837
Epoch 1911
Epoch 1911 :: Batch 0/1
Batch Loss = 0.0007310406148302306
1911, epoch_train_loss=0.0007310406148302306
Epoch 1912
Epoch 1912 :: Batch 0/1
Batch Loss = 0.000731040614814671
1912, epoch_train_loss=0.000731040614814671
Epoch 1913
Epoch 1913 :: Batch 0/1
Batch Loss = 0.0007310406147991054
1913, epoch_train_loss=0.0007310406147991054
Epoch 1914
Epoch 1914 :: Batch 0/1
Batch Loss = 0.0007310406147835332
1914, epoch_train_loss=0.0007310406147835332
Epoch 1915
Epoch 1915 :: Batch 0/1
Batch Loss = 0.0007310406147679546
1915, epoch_train_loss=0.0007310406147679546
Epoch 1916
Epoch 1916 :: Batch 0/1
Batch Loss = 0.00073104061475237
1916, epoch_train_loss=0.00073104061475237
Epoch 1917
Epoch 1917 :: Batch 0/1
Batch Loss = 0.0007310406147367788
1917, epoch_train_loss=0.0007310406147367788
Epoch 1918
Epoch 1918 :: Batch 0/1
Batch Loss = 0.0007310406147211814
1918, epoch_train_loss=0.0007310406147211814
Epoch 1919
Epoch 1919 :: Batch 0/1
Batch Loss = 0.0007310406147055775
1919, epoch_train_loss=0.0007310406147055775
Epoch 1920
Epoch 1920 :: Batch 0/1
Batch Loss = 0.0007310406146899675
1920, epoch_train_loss=0.0007310406146899675
Epoch 1921
Epoch 1921 :: Batch 0/1
Batch Loss = 0.0007310406146743511
1921, epoch_train_loss=0.0007310406146743511
Epoch 1922
Epoch 1922 :: Batch 0/1
Batch Loss = 0.0007310406146587282
1922, epoch_train_loss=0.0007310406146587282
Epoch 1923
Epoch 1923 :: Batch 0/1
Batch Loss = 0.000731040614643099
1923, epoch_train_loss=0.000731040614643099
Epoch 1924
Epoch 1924 :: Batch 0/1
Batch Loss = 0.0007310406146274636
1924, epoch_train_loss=0.0007310406146274636
Epoch 1925
Epoch 1925 :: Batch 0/1
Batch Loss = 0.0007310406146118219
1925, epoch_train_loss=0.0007310406146118219
Epoch 1926
Epoch 1926 :: Batch 0/1
Batch Loss = 0.0007310406145961737
1926, epoch_train_loss=0.0007310406145961737
Epoch 1927
Epoch 1927 :: Batch 0/1
Batch Loss = 0.0007310406145805194
1927, epoch_train_loss=0.0007310406145805194
Epoch 1928
Epoch 1928 :: Batch 0/1
Batch Loss = 0.0007310406145648586
1928, epoch_train_loss=0.0007310406145648586
Epoch 1929
Epoch 1929 :: Batch 0/1
Batch Loss = 0.0007310406145491916
1929, epoch_train_loss=0.0007310406145491916
Epoch 1930
Epoch 1930 :: Batch 0/1
Batch Loss = 0.000731040614533518
1930, epoch_train_loss=0.000731040614533518
Epoch 1931
Epoch 1931 :: Batch 0/1
Batch Loss = 0.0007310406145178383
1931, epoch_train_loss=0.0007310406145178383
Epoch 1932
Epoch 1932 :: Batch 0/1
Batch Loss = 0.0007310406145021523
1932, epoch_train_loss=0.0007310406145021523
Epoch 1933
Epoch 1933 :: Batch 0/1
Batch Loss = 0.0007310406144864598
1933, epoch_train_loss=0.0007310406144864598
Epoch 1934
Epoch 1934 :: Batch 0/1
Batch Loss = 0.0007310406144707611
1934, epoch_train_loss=0.0007310406144707611
Epoch 1935
Epoch 1935 :: Batch 0/1
Batch Loss = 0.000731040614455056
1935, epoch_train_loss=0.000731040614455056
Epoch 1936
Epoch 1936 :: Batch 0/1
Batch Loss = 0.0007310406144393446
1936, epoch_train_loss=0.0007310406144393446
Epoch 1937
Epoch 1937 :: Batch 0/1
Batch Loss = 0.000731040614423627
1937, epoch_train_loss=0.000731040614423627
Epoch 1938
Epoch 1938 :: Batch 0/1
Batch Loss = 0.0007310406144079029
1938, epoch_train_loss=0.0007310406144079029
Epoch 1939
Epoch 1939 :: Batch 0/1
Batch Loss = 0.0007310406143921725
1939, epoch_train_loss=0.0007310406143921725
Epoch 1940
Epoch 1940 :: Batch 0/1
Batch Loss = 0.0007310406143764358
1940, epoch_train_loss=0.0007310406143764358
Epoch 1941
Epoch 1941 :: Batch 0/1
Batch Loss = 0.0007310406143606927
1941, epoch_train_loss=0.0007310406143606927
Epoch 1942
Epoch 1942 :: Batch 0/1
Batch Loss = 0.0007310406143449434
1942, epoch_train_loss=0.0007310406143449434
Epoch 1943
Epoch 1943 :: Batch 0/1
Batch Loss = 0.0007310406143291877
1943, epoch_train_loss=0.0007310406143291877
Epoch 1944
Epoch 1944 :: Batch 0/1
Batch Loss = 0.0007310406143134256
1944, epoch_train_loss=0.0007310406143134256
Epoch 1945
Epoch 1945 :: Batch 0/1
Batch Loss = 0.0007310406142976573
1945, epoch_train_loss=0.0007310406142976573
Epoch 1946
Epoch 1946 :: Batch 0/1
Batch Loss = 0.0007310406142818826
1946, epoch_train_loss=0.0007310406142818826
Epoch 1947
Epoch 1947 :: Batch 0/1
Batch Loss = 0.0007310406142661016
1947, epoch_train_loss=0.0007310406142661016
Epoch 1948
Epoch 1948 :: Batch 0/1
Batch Loss = 0.0007310406142503143
1948, epoch_train_loss=0.0007310406142503143
Epoch 1949
Epoch 1949 :: Batch 0/1
Batch Loss = 0.0007310406142345205
1949, epoch_train_loss=0.0007310406142345205
Epoch 1950
Epoch 1950 :: Batch 0/1
Batch Loss = 0.0007310406142187207
1950, epoch_train_loss=0.0007310406142187207
Epoch 1951
Epoch 1951 :: Batch 0/1
Batch Loss = 0.0007310406142029143
1951, epoch_train_loss=0.0007310406142029143
Epoch 1952
Epoch 1952 :: Batch 0/1
Batch Loss = 0.0007310406141871016
1952, epoch_train_loss=0.0007310406141871016
Epoch 1953
Epoch 1953 :: Batch 0/1
Batch Loss = 0.0007310406141712826
1953, epoch_train_loss=0.0007310406141712826
Epoch 1954
Epoch 1954 :: Batch 0/1
Batch Loss = 0.0007310406141554573
1954, epoch_train_loss=0.0007310406141554573
Epoch 1955
Epoch 1955 :: Batch 0/1
Batch Loss = 0.0007310406141396257
1955, epoch_train_loss=0.0007310406141396257
Epoch 1956
Epoch 1956 :: Batch 0/1
Batch Loss = 0.0007310406141237877
1956, epoch_train_loss=0.0007310406141237877
Epoch 1957
Epoch 1957 :: Batch 0/1
Batch Loss = 0.0007310406141079434
1957, epoch_train_loss=0.0007310406141079434
Epoch 1958
Epoch 1958 :: Batch 0/1
Batch Loss = 0.0007310406140920927
1958, epoch_train_loss=0.0007310406140920927
Epoch 1959
Epoch 1959 :: Batch 0/1
Batch Loss = 0.0007310406140762358
1959, epoch_train_loss=0.0007310406140762358
Epoch 1960
Epoch 1960 :: Batch 0/1
Batch Loss = 0.0007310406140603725
1960, epoch_train_loss=0.0007310406140603725
Epoch 1961
Epoch 1961 :: Batch 0/1
Batch Loss = 0.0007310406140445029
1961, epoch_train_loss=0.0007310406140445029
Epoch 1962
Epoch 1962 :: Batch 0/1
Batch Loss = 0.000731040614028627
1962, epoch_train_loss=0.000731040614028627
Epoch 1963
Epoch 1963 :: Batch 0/1
Batch Loss = 0.0007310406140127447
1963, epoch_train_loss=0.0007310406140127447
Epoch 1964
Epoch 1964 :: Batch 0/1
Batch Loss = 0.000731040613996856
1964, epoch_train_loss=0.000731040613996856
Epoch 1965
Epoch 1965 :: Batch 0/1
Batch Loss = 0.0007310406139809611
1965, epoch_train_loss=0.0007310406139809611
Epoch 1966
Epoch 1966 :: Batch 0/1
Batch Loss = 0.0007310406139650599
1966, epoch_train_loss=0.0007310406139650599
Epoch 1967
Epoch 1967 :: Batch 0/1
Batch Loss = 0.0007310406139491522
1967, epoch_train_loss=0.0007310406139491522
Epoch 1968
Epoch 1968 :: Batch 0/1
Batch Loss = 0.0007310406139332383
1968, epoch_train_loss=0.0007310406139332383
Epoch 1969
Epoch 1969 :: Batch 0/1
Batch Loss = 0.0007310406139173182
1969, epoch_train_loss=0.0007310406139173182
Epoch 1970
Epoch 1970 :: Batch 0/1
Batch Loss = 0.0007310406139013915
1970, epoch_train_loss=0.0007310406139013915
Epoch 1971
Epoch 1971 :: Batch 0/1
Batch Loss = 0.0007310406138854586
1971, epoch_train_loss=0.0007310406138854586
Epoch 1972
Epoch 1972 :: Batch 0/1
Batch Loss = 0.0007310406138695194
1972, epoch_train_loss=0.0007310406138695194
Epoch 1973
Epoch 1973 :: Batch 0/1
Batch Loss = 0.000731040613853574
1973, epoch_train_loss=0.000731040613853574
Epoch 1974
Epoch 1974 :: Batch 0/1
Batch Loss = 0.000731040613837622
1974, epoch_train_loss=0.000731040613837622
Epoch 1975
Epoch 1975 :: Batch 0/1
Batch Loss = 0.0007310406138216638
1975, epoch_train_loss=0.0007310406138216638
Epoch 1976
Epoch 1976 :: Batch 0/1
Batch Loss = 0.0007310406138056992
1976, epoch_train_loss=0.0007310406138056992
Epoch 1977
Epoch 1977 :: Batch 0/1
Batch Loss = 0.0007310406137897283
1977, epoch_train_loss=0.0007310406137897283
Epoch 1978
Epoch 1978 :: Batch 0/1
Batch Loss = 0.0007310406137737511
1978, epoch_train_loss=0.0007310406137737511
Epoch 1979
Epoch 1979 :: Batch 0/1
Batch Loss = 0.0007310406137577677
1979, epoch_train_loss=0.0007310406137577677
Epoch 1980
Epoch 1980 :: Batch 0/1
Batch Loss = 0.0007310406137417777
1980, epoch_train_loss=0.0007310406137417777
Epoch 1981
Epoch 1981 :: Batch 0/1
Batch Loss = 0.0007310406137257815
1981, epoch_train_loss=0.0007310406137257815
Epoch 1982
Epoch 1982 :: Batch 0/1
Batch Loss = 0.0007310406137097791
1982, epoch_train_loss=0.0007310406137097791
Epoch 1983
Epoch 1983 :: Batch 0/1
Batch Loss = 0.0007310406136937702
1983, epoch_train_loss=0.0007310406136937702
Epoch 1984
Epoch 1984 :: Batch 0/1
Batch Loss = 0.0007310406136777551
1984, epoch_train_loss=0.0007310406136777551
Epoch 1985
Epoch 1985 :: Batch 0/1
Batch Loss = 0.0007310406136617336
1985, epoch_train_loss=0.0007310406136617336
Epoch 1986
Epoch 1986 :: Batch 0/1
Batch Loss = 0.0007310406136457057
1986, epoch_train_loss=0.0007310406136457057
Epoch 1987
Epoch 1987 :: Batch 0/1
Batch Loss = 0.0007310406136296717
1987, epoch_train_loss=0.0007310406136296717
Epoch 1988
Epoch 1988 :: Batch 0/1
Batch Loss = 0.0007310406136136311
1988, epoch_train_loss=0.0007310406136136311
Epoch 1989
Epoch 1989 :: Batch 0/1
Batch Loss = 0.0007310406135975844
1989, epoch_train_loss=0.0007310406135975844
Epoch 1990
Epoch 1990 :: Batch 0/1
Batch Loss = 0.0007310406135815313
1990, epoch_train_loss=0.0007310406135815313
Epoch 1991
Epoch 1991 :: Batch 0/1
Batch Loss = 0.0007310406135654717
1991, epoch_train_loss=0.0007310406135654717
Epoch 1992
Epoch 1992 :: Batch 0/1
Batch Loss = 0.000731040613549406
1992, epoch_train_loss=0.000731040613549406
Epoch 1993
Epoch 1993 :: Batch 0/1
Batch Loss = 0.0007310406135333339
1993, epoch_train_loss=0.0007310406135333339
Epoch 1994
Epoch 1994 :: Batch 0/1
Batch Loss = 0.0007310406135172553
1994, epoch_train_loss=0.0007310406135172553
Epoch 1995
Epoch 1995 :: Batch 0/1
Batch Loss = 0.0007310406135011707
1995, epoch_train_loss=0.0007310406135011707
Epoch 1996
Epoch 1996 :: Batch 0/1
Batch Loss = 0.0007310406134850794
1996, epoch_train_loss=0.0007310406134850794
Epoch 1997
Epoch 1997 :: Batch 0/1
Batch Loss = 0.0007310406134689822
1997, epoch_train_loss=0.0007310406134689822
Epoch 1998
Epoch 1998 :: Batch 0/1
Batch Loss = 0.0007310406134528784
1998, epoch_train_loss=0.0007310406134528784
Epoch 1999
Epoch 1999 :: Batch 0/1
Batch Loss = 0.0007310406134367684
1999, epoch_train_loss=0.0007310406134367684
Epoch 2000
Epoch 2000 :: Batch 0/1
Batch Loss = 0.0007310406134206519
2000, epoch_train_loss=0.0007310406134206519
Epoch 2001
Epoch 2001 :: Batch 0/1
Batch Loss = 0.0007310406134045292
2001, epoch_train_loss=0.0007310406134045292
Epoch 2002
Epoch 2002 :: Batch 0/1
Batch Loss = 0.0007310406133884
2002, epoch_train_loss=0.0007310406133884
Epoch 2003
Epoch 2003 :: Batch 0/1
Batch Loss = 0.0007310406133722646
2003, epoch_train_loss=0.0007310406133722646
Epoch 2004
Epoch 2004 :: Batch 0/1
Batch Loss = 0.0007310406133561229
2004, epoch_train_loss=0.0007310406133561229
Epoch 2005
Epoch 2005 :: Batch 0/1
Batch Loss = 0.0007310406133399749
2005, epoch_train_loss=0.0007310406133399749
Epoch 2006
Epoch 2006 :: Batch 0/1
Batch Loss = 0.0007310406133238207
2006, epoch_train_loss=0.0007310406133238207
Epoch 2007
Epoch 2007 :: Batch 0/1
Batch Loss = 0.0007310406133076598
2007, epoch_train_loss=0.0007310406133076598
Epoch 2008
Epoch 2008 :: Batch 0/1
Batch Loss = 0.0007310406132914928
2008, epoch_train_loss=0.0007310406132914928
Epoch 2009
Epoch 2009 :: Batch 0/1
Batch Loss = 0.0007310406132753195
2009, epoch_train_loss=0.0007310406132753195
Epoch 2010
Epoch 2010 :: Batch 0/1
Batch Loss = 0.0007310406132591398
2010, epoch_train_loss=0.0007310406132591398
Epoch 2011
Epoch 2011 :: Batch 0/1
Batch Loss = 0.0007310406132429539
2011, epoch_train_loss=0.0007310406132429539
Epoch 2012
Epoch 2012 :: Batch 0/1
Batch Loss = 0.0007310406132267615
2012, epoch_train_loss=0.0007310406132267615
Epoch 2013
Epoch 2013 :: Batch 0/1
Batch Loss = 0.0007310406132105629
2013, epoch_train_loss=0.0007310406132105629
Epoch 2014
Epoch 2014 :: Batch 0/1
Batch Loss = 0.000731040613194358
2014, epoch_train_loss=0.000731040613194358
Epoch 2015
Epoch 2015 :: Batch 0/1
Batch Loss = 0.0007310406131781467
2015, epoch_train_loss=0.0007310406131781467
Epoch 2016
Epoch 2016 :: Batch 0/1
Batch Loss = 0.000731040613161929
2016, epoch_train_loss=0.000731040613161929
Epoch 2017
Epoch 2017 :: Batch 0/1
Batch Loss = 0.000731040613145705
2017, epoch_train_loss=0.000731040613145705
Epoch 2018
Epoch 2018 :: Batch 0/1
Batch Loss = 0.0007310406131294748
2018, epoch_train_loss=0.0007310406131294748
Epoch 2019
Epoch 2019 :: Batch 0/1
Batch Loss = 0.0007310406131132381
2019, epoch_train_loss=0.0007310406131132381
Epoch 2020
Epoch 2020 :: Batch 0/1
Batch Loss = 0.0007310406130969951
2020, epoch_train_loss=0.0007310406130969951
Epoch 2021
Epoch 2021 :: Batch 0/1
Batch Loss = 0.000731040613080746
2021, epoch_train_loss=0.000731040613080746
Epoch 2022
Epoch 2022 :: Batch 0/1
Batch Loss = 0.0007310406130644904
2022, epoch_train_loss=0.0007310406130644904
Epoch 2023
Epoch 2023 :: Batch 0/1
Batch Loss = 0.0007310406130482285
2023, epoch_train_loss=0.0007310406130482285
Epoch 2024
Epoch 2024 :: Batch 0/1
Batch Loss = 0.0007310406130319601
2024, epoch_train_loss=0.0007310406130319601
Epoch 2025
Epoch 2025 :: Batch 0/1
Batch Loss = 0.0007310406130156856
2025, epoch_train_loss=0.0007310406130156856
Epoch 2026
Epoch 2026 :: Batch 0/1
Batch Loss = 0.0007310406129994048
2026, epoch_train_loss=0.0007310406129994048
Epoch 2027
Epoch 2027 :: Batch 0/1
Batch Loss = 0.0007310406129831176
2027, epoch_train_loss=0.0007310406129831176
Epoch 2028
Epoch 2028 :: Batch 0/1
Batch Loss = 0.0007310406129668239
2028, epoch_train_loss=0.0007310406129668239
Epoch 2029
Epoch 2029 :: Batch 0/1
Batch Loss = 0.0007310406129505242
2029, epoch_train_loss=0.0007310406129505242
Epoch 2030
Epoch 2030 :: Batch 0/1
Batch Loss = 0.0007310406129342179
2030, epoch_train_loss=0.0007310406129342179
Epoch 2031
Epoch 2031 :: Batch 0/1
Batch Loss = 0.0007310406129179053
2031, epoch_train_loss=0.0007310406129179053
Epoch 2032
Epoch 2032 :: Batch 0/1
Batch Loss = 0.0007310406129015865
2032, epoch_train_loss=0.0007310406129015865
Epoch 2033
Epoch 2033 :: Batch 0/1
Batch Loss = 0.0007310406128852613
2033, epoch_train_loss=0.0007310406128852613
Epoch 2034
Epoch 2034 :: Batch 0/1
Batch Loss = 0.0007310406128689298
2034, epoch_train_loss=0.0007310406128689298
Epoch 2035
Epoch 2035 :: Batch 0/1
Batch Loss = 0.000731040612852592
2035, epoch_train_loss=0.000731040612852592
Epoch 2036
Epoch 2036 :: Batch 0/1
Batch Loss = 0.0007310406128362478
2036, epoch_train_loss=0.0007310406128362478
Epoch 2037
Epoch 2037 :: Batch 0/1
Batch Loss = 0.0007310406128198973
2037, epoch_train_loss=0.0007310406128198973
Epoch 2038
Epoch 2038 :: Batch 0/1
Batch Loss = 0.0007310406128035406
2038, epoch_train_loss=0.0007310406128035406
Epoch 2039
Epoch 2039 :: Batch 0/1
Batch Loss = 0.0007310406127871774
2039, epoch_train_loss=0.0007310406127871774
Epoch 2040
Epoch 2040 :: Batch 0/1
Batch Loss = 0.000731040612770808
2040, epoch_train_loss=0.000731040612770808
Epoch 2041
Epoch 2041 :: Batch 0/1
Batch Loss = 0.0007310406127544322
2041, epoch_train_loss=0.0007310406127544322
Epoch 2042
Epoch 2042 :: Batch 0/1
Batch Loss = 0.0007310406127380502
2042, epoch_train_loss=0.0007310406127380502
Epoch 2043
Epoch 2043 :: Batch 0/1
Batch Loss = 0.0007310406127216617
2043, epoch_train_loss=0.0007310406127216617
Epoch 2044
Epoch 2044 :: Batch 0/1
Batch Loss = 0.0007310406127052669
2044, epoch_train_loss=0.0007310406127052669
Epoch 2045
Epoch 2045 :: Batch 0/1
Batch Loss = 0.0007310406126888658
2045, epoch_train_loss=0.0007310406126888658
Epoch 2046
Epoch 2046 :: Batch 0/1
Batch Loss = 0.0007310406126724584
2046, epoch_train_loss=0.0007310406126724584
Epoch 2047
Epoch 2047 :: Batch 0/1
Batch Loss = 0.0007310406126560448
2047, epoch_train_loss=0.0007310406126560448
Epoch 2048
Epoch 2048 :: Batch 0/1
Batch Loss = 0.0007310406126396245
2048, epoch_train_loss=0.0007310406126396245
Epoch 2049
Epoch 2049 :: Batch 0/1
Batch Loss = 0.0007310406126231983
2049, epoch_train_loss=0.0007310406126231983
Epoch 2050
Epoch 2050 :: Batch 0/1
Batch Loss = 0.0007310406126067655
2050, epoch_train_loss=0.0007310406126067655
Epoch 2051
Epoch 2051 :: Batch 0/1
Batch Loss = 0.0007310406125903265
2051, epoch_train_loss=0.0007310406125903265
Epoch 2052
Epoch 2052 :: Batch 0/1
Batch Loss = 0.0007310406125738812
2052, epoch_train_loss=0.0007310406125738812
Epoch 2053
Epoch 2053 :: Batch 0/1
Batch Loss = 0.0007310406125574294
2053, epoch_train_loss=0.0007310406125574294
Epoch 2054
Epoch 2054 :: Batch 0/1
Batch Loss = 0.0007310406125409715
2054, epoch_train_loss=0.0007310406125409715
Epoch 2055
Epoch 2055 :: Batch 0/1
Batch Loss = 0.0007310406125245073
2055, epoch_train_loss=0.0007310406125245073
Epoch 2056
Epoch 2056 :: Batch 0/1
Batch Loss = 0.0007310406125080366
2056, epoch_train_loss=0.0007310406125080366
Epoch 2057
Epoch 2057 :: Batch 0/1
Batch Loss = 0.0007310406124915596
2057, epoch_train_loss=0.0007310406124915596
Epoch 2058
Epoch 2058 :: Batch 0/1
Batch Loss = 0.0007310406124750763
2058, epoch_train_loss=0.0007310406124750763
Epoch 2059
Epoch 2059 :: Batch 0/1
Batch Loss = 0.0007310406124585866
2059, epoch_train_loss=0.0007310406124585866
Epoch 2060
Epoch 2060 :: Batch 0/1
Batch Loss = 0.0007310406124420908
2060, epoch_train_loss=0.0007310406124420908
Epoch 2061
Epoch 2061 :: Batch 0/1
Batch Loss = 0.0007310406124255885
2061, epoch_train_loss=0.0007310406124255885
Epoch 2062
Epoch 2062 :: Batch 0/1
Batch Loss = 0.0007310406124090799
2062, epoch_train_loss=0.0007310406124090799
Epoch 2063
Epoch 2063 :: Batch 0/1
Batch Loss = 0.0007310406123925649
2063, epoch_train_loss=0.0007310406123925649
Epoch 2064
Epoch 2064 :: Batch 0/1
Batch Loss = 0.0007310406123760437
2064, epoch_train_loss=0.0007310406123760437
Epoch 2065
Epoch 2065 :: Batch 0/1
Batch Loss = 0.0007310406123595163
2065, epoch_train_loss=0.0007310406123595163
Epoch 2066
Epoch 2066 :: Batch 0/1
Batch Loss = 0.0007310406123429823
2066, epoch_train_loss=0.0007310406123429823
Epoch 2067
Epoch 2067 :: Batch 0/1
Batch Loss = 0.0007310406123264421
2067, epoch_train_loss=0.0007310406123264421
Epoch 2068
Epoch 2068 :: Batch 0/1
Batch Loss = 0.0007310406123098956
2068, epoch_train_loss=0.0007310406123098956
Epoch 2069
Epoch 2069 :: Batch 0/1
Batch Loss = 0.0007310406122933426
2069, epoch_train_loss=0.0007310406122933426
Epoch 2070
Epoch 2070 :: Batch 0/1
Batch Loss = 0.0007310406122767834
2070, epoch_train_loss=0.0007310406122767834
Epoch 2071
Epoch 2071 :: Batch 0/1
Batch Loss = 0.000731040612260218
2071, epoch_train_loss=0.000731040612260218
Epoch 2072
Epoch 2072 :: Batch 0/1
Batch Loss = 0.000731040612243646
2072, epoch_train_loss=0.000731040612243646
Epoch 2073
Epoch 2073 :: Batch 0/1
Batch Loss = 0.000731040612227068
2073, epoch_train_loss=0.000731040612227068
Epoch 2074
Epoch 2074 :: Batch 0/1
Batch Loss = 0.0007310406122104836
2074, epoch_train_loss=0.0007310406122104836
Epoch 2075
Epoch 2075 :: Batch 0/1
Batch Loss = 0.0007310406121938928
2075, epoch_train_loss=0.0007310406121938928
Epoch 2076
Epoch 2076 :: Batch 0/1
Batch Loss = 0.0007310406121772957
2076, epoch_train_loss=0.0007310406121772957
Epoch 2077
Epoch 2077 :: Batch 0/1
Batch Loss = 0.0007310406121606922
2077, epoch_train_loss=0.0007310406121606922
Epoch 2078
Epoch 2078 :: Batch 0/1
Batch Loss = 0.0007310406121440824
2078, epoch_train_loss=0.0007310406121440824
Epoch 2079
Epoch 2079 :: Batch 0/1
Batch Loss = 0.0007310406121274664
2079, epoch_train_loss=0.0007310406121274664
Epoch 2080
Epoch 2080 :: Batch 0/1
Batch Loss = 0.000731040612110844
2080, epoch_train_loss=0.000731040612110844
Epoch 2081
Epoch 2081 :: Batch 0/1
Batch Loss = 0.0007310406120942152
2081, epoch_train_loss=0.0007310406120942152
Epoch 2082
Epoch 2082 :: Batch 0/1
Batch Loss = 0.0007310406120775801
2082, epoch_train_loss=0.0007310406120775801
Epoch 2083
Epoch 2083 :: Batch 0/1
Batch Loss = 0.0007310406120609388
2083, epoch_train_loss=0.0007310406120609388
Epoch 2084
Epoch 2084 :: Batch 0/1
Batch Loss = 0.0007310406120442911
2084, epoch_train_loss=0.0007310406120442911
Epoch 2085
Epoch 2085 :: Batch 0/1
Batch Loss = 0.0007310406120276371
2085, epoch_train_loss=0.0007310406120276371
Epoch 2086
Epoch 2086 :: Batch 0/1
Batch Loss = 0.0007310406120109768
2086, epoch_train_loss=0.0007310406120109768
Epoch 2087
Epoch 2087 :: Batch 0/1
Batch Loss = 0.0007310406119943099
2087, epoch_train_loss=0.0007310406119943099
Epoch 2088
Epoch 2088 :: Batch 0/1
Batch Loss = 0.000731040611977637
2088, epoch_train_loss=0.000731040611977637
Epoch 2089
Epoch 2089 :: Batch 0/1
Batch Loss = 0.0007310406119609577
2089, epoch_train_loss=0.0007310406119609577
Epoch 2090
Epoch 2090 :: Batch 0/1
Batch Loss = 0.0007310406119442721
2090, epoch_train_loss=0.0007310406119442721
Epoch 2091
Epoch 2091 :: Batch 0/1
Batch Loss = 0.0007310406119275802
2091, epoch_train_loss=0.0007310406119275802
Epoch 2092
Epoch 2092 :: Batch 0/1
Batch Loss = 0.0007310406119108818
2092, epoch_train_loss=0.0007310406119108818
Epoch 2093
Epoch 2093 :: Batch 0/1
Batch Loss = 0.0007310406118941772
2093, epoch_train_loss=0.0007310406118941772
Epoch 2094
Epoch 2094 :: Batch 0/1
Batch Loss = 0.0007310406118774664
2094, epoch_train_loss=0.0007310406118774664
Epoch 2095
Epoch 2095 :: Batch 0/1
Batch Loss = 0.0007310406118607491
2095, epoch_train_loss=0.0007310406118607491
Epoch 2096
Epoch 2096 :: Batch 0/1
Batch Loss = 0.0007310406118440255
2096, epoch_train_loss=0.0007310406118440255
Epoch 2097
Epoch 2097 :: Batch 0/1
Batch Loss = 0.0007310406118272956
2097, epoch_train_loss=0.0007310406118272956
Epoch 2098
Epoch 2098 :: Batch 0/1
Batch Loss = 0.0007310406118105594
2098, epoch_train_loss=0.0007310406118105594
Epoch 2099
Epoch 2099 :: Batch 0/1
Batch Loss = 0.000731040611793817
2099, epoch_train_loss=0.000731040611793817
Epoch 2100
Epoch 2100 :: Batch 0/1
Batch Loss = 0.000731040611777068
2100, epoch_train_loss=0.000731040611777068
Epoch 2101
Epoch 2101 :: Batch 0/1
Batch Loss = 0.0007310406117603128
2101, epoch_train_loss=0.0007310406117603128
Epoch 2102
Epoch 2102 :: Batch 0/1
Batch Loss = 0.0007310406117435513
2102, epoch_train_loss=0.0007310406117435513
Epoch 2103
Epoch 2103 :: Batch 0/1
Batch Loss = 0.0007310406117267837
2103, epoch_train_loss=0.0007310406117267837
Epoch 2104
Epoch 2104 :: Batch 0/1
Batch Loss = 0.0007310406117100094
2104, epoch_train_loss=0.0007310406117100094
Epoch 2105
Epoch 2105 :: Batch 0/1
Batch Loss = 0.000731040611693229
2105, epoch_train_loss=0.000731040611693229
Epoch 2106
Epoch 2106 :: Batch 0/1
Batch Loss = 0.0007310406116764422
2106, epoch_train_loss=0.0007310406116764422
Epoch 2107
Epoch 2107 :: Batch 0/1
Batch Loss = 0.0007310406116596492
2107, epoch_train_loss=0.0007310406116596492
Epoch 2108
Epoch 2108 :: Batch 0/1
Batch Loss = 0.0007310406116428497
2108, epoch_train_loss=0.0007310406116428497
Epoch 2109
Epoch 2109 :: Batch 0/1
Batch Loss = 0.0007310406116260439
2109, epoch_train_loss=0.0007310406116260439
Epoch 2110
Epoch 2110 :: Batch 0/1
Batch Loss = 0.000731040611609232
2110, epoch_train_loss=0.000731040611609232
Epoch 2111
Epoch 2111 :: Batch 0/1
Batch Loss = 0.0007310406115924135
2111, epoch_train_loss=0.0007310406115924135
Epoch 2112
Epoch 2112 :: Batch 0/1
Batch Loss = 0.0007310406115755889
2112, epoch_train_loss=0.0007310406115755889
Epoch 2113
Epoch 2113 :: Batch 0/1
Batch Loss = 0.0007310406115587578
2113, epoch_train_loss=0.0007310406115587578
Epoch 2114
Epoch 2114 :: Batch 0/1
Batch Loss = 0.0007310406115419206
2114, epoch_train_loss=0.0007310406115419206
Epoch 2115
Epoch 2115 :: Batch 0/1
Batch Loss = 0.0007310406115250767
2115, epoch_train_loss=0.0007310406115250767
Epoch 2116
Epoch 2116 :: Batch 0/1
Batch Loss = 0.0007310406115082268
2116, epoch_train_loss=0.0007310406115082268
Epoch 2117
Epoch 2117 :: Batch 0/1
Batch Loss = 0.0007310406114913704
2117, epoch_train_loss=0.0007310406114913704
Epoch 2118
Epoch 2118 :: Batch 0/1
Batch Loss = 0.0007310406114745079
2118, epoch_train_loss=0.0007310406114745079
Epoch 2119
Epoch 2119 :: Batch 0/1
Batch Loss = 0.0007310406114576389
2119, epoch_train_loss=0.0007310406114576389
Epoch 2120
Epoch 2120 :: Batch 0/1
Batch Loss = 0.0007310406114407638
2120, epoch_train_loss=0.0007310406114407638
Epoch 2121
Epoch 2121 :: Batch 0/1
Batch Loss = 0.0007310406114238821
2121, epoch_train_loss=0.0007310406114238821
Epoch 2122
Epoch 2122 :: Batch 0/1
Batch Loss = 0.0007310406114069942
2122, epoch_train_loss=0.0007310406114069942
Epoch 2123
Epoch 2123 :: Batch 0/1
Batch Loss = 0.0007310406113901
2123, epoch_train_loss=0.0007310406113901
Epoch 2124
Epoch 2124 :: Batch 0/1
Batch Loss = 0.0007310406113731995
2124, epoch_train_loss=0.0007310406113731995
Epoch 2125
Epoch 2125 :: Batch 0/1
Batch Loss = 0.0007310406113562926
2125, epoch_train_loss=0.0007310406113562926
Epoch 2126
Epoch 2126 :: Batch 0/1
Batch Loss = 0.0007310406113393794
2126, epoch_train_loss=0.0007310406113393794
Epoch 2127
Epoch 2127 :: Batch 0/1
Batch Loss = 0.00073104061132246
2127, epoch_train_loss=0.00073104061132246
Epoch 2128
Epoch 2128 :: Batch 0/1
Batch Loss = 0.0007310406113055342
2128, epoch_train_loss=0.0007310406113055342
Epoch 2129
Epoch 2129 :: Batch 0/1
Batch Loss = 0.0007310406112886021
2129, epoch_train_loss=0.0007310406112886021
Epoch 2130
Epoch 2130 :: Batch 0/1
Batch Loss = 0.0007310406112716635
2130, epoch_train_loss=0.0007310406112716635
Epoch 2131
Epoch 2131 :: Batch 0/1
Batch Loss = 0.0007310406112547188
2131, epoch_train_loss=0.0007310406112547188
Epoch 2132
Epoch 2132 :: Batch 0/1
Batch Loss = 0.0007310406112377677
2132, epoch_train_loss=0.0007310406112377677
Epoch 2133
Epoch 2133 :: Batch 0/1
Batch Loss = 0.0007310406112208104
2133, epoch_train_loss=0.0007310406112208104
Epoch 2134
Epoch 2134 :: Batch 0/1
Batch Loss = 0.0007310406112038465
2134, epoch_train_loss=0.0007310406112038465
Epoch 2135
Epoch 2135 :: Batch 0/1
Batch Loss = 0.0007310406111868766
2135, epoch_train_loss=0.0007310406111868766
Epoch 2136
Epoch 2136 :: Batch 0/1
Batch Loss = 0.0007310406111699002
2136, epoch_train_loss=0.0007310406111699002
Epoch 2137
Epoch 2137 :: Batch 0/1
Batch Loss = 0.0007310406111529175
2137, epoch_train_loss=0.0007310406111529175
Epoch 2138
Epoch 2138 :: Batch 0/1
Batch Loss = 0.0007310406111359286
2138, epoch_train_loss=0.0007310406111359286
Epoch 2139
Epoch 2139 :: Batch 0/1
Batch Loss = 0.0007310406111189332
2139, epoch_train_loss=0.0007310406111189332
Epoch 2140
Epoch 2140 :: Batch 0/1
Batch Loss = 0.0007310406111019315
2140, epoch_train_loss=0.0007310406111019315
Epoch 2141
Epoch 2141 :: Batch 0/1
Batch Loss = 0.0007310406110849236
2141, epoch_train_loss=0.0007310406110849236
Epoch 2142
Epoch 2142 :: Batch 0/1
Batch Loss = 0.0007310406110679093
2142, epoch_train_loss=0.0007310406110679093
Epoch 2143
Epoch 2143 :: Batch 0/1
Batch Loss = 0.0007310406110508888
2143, epoch_train_loss=0.0007310406110508888
Epoch 2144
Epoch 2144 :: Batch 0/1
Batch Loss = 0.0007310406110338618
2144, epoch_train_loss=0.0007310406110338618
Epoch 2145
Epoch 2145 :: Batch 0/1
Batch Loss = 0.0007310406110168286
2145, epoch_train_loss=0.0007310406110168286
Epoch 2146
Epoch 2146 :: Batch 0/1
Batch Loss = 0.000731040610999789
2146, epoch_train_loss=0.000731040610999789
Epoch 2147
Epoch 2147 :: Batch 0/1
Batch Loss = 0.0007310406109827432
2147, epoch_train_loss=0.0007310406109827432
Epoch 2148
Epoch 2148 :: Batch 0/1
Batch Loss = 0.0007310406109656912
2148, epoch_train_loss=0.0007310406109656912
Epoch 2149
Epoch 2149 :: Batch 0/1
Batch Loss = 0.0007310406109486326
2149, epoch_train_loss=0.0007310406109486326
Epoch 2150
Epoch 2150 :: Batch 0/1
Batch Loss = 0.0007310406109315678
2150, epoch_train_loss=0.0007310406109315678
Epoch 2151
Epoch 2151 :: Batch 0/1
Batch Loss = 0.0007310406109144966
2151, epoch_train_loss=0.0007310406109144966
Epoch 2152
Epoch 2152 :: Batch 0/1
Batch Loss = 0.0007310406108974192
2152, epoch_train_loss=0.0007310406108974192
Epoch 2153
Epoch 2153 :: Batch 0/1
Batch Loss = 0.0007310406108803355
2153, epoch_train_loss=0.0007310406108803355
Epoch 2154
Epoch 2154 :: Batch 0/1
Batch Loss = 0.0007310406108632454
2154, epoch_train_loss=0.0007310406108632454
Epoch 2155
Epoch 2155 :: Batch 0/1
Batch Loss = 0.0007310406108461491
2155, epoch_train_loss=0.0007310406108461491
Epoch 2156
Epoch 2156 :: Batch 0/1
Batch Loss = 0.0007310406108290463
2156, epoch_train_loss=0.0007310406108290463
Epoch 2157
Epoch 2157 :: Batch 0/1
Batch Loss = 0.0007310406108119373
2157, epoch_train_loss=0.0007310406108119373
Epoch 2158
Epoch 2158 :: Batch 0/1
Batch Loss = 0.0007310406107948219
2158, epoch_train_loss=0.0007310406107948219
Epoch 2159
Epoch 2159 :: Batch 0/1
Batch Loss = 0.0007310406107777002
2159, epoch_train_loss=0.0007310406107777002
Epoch 2160
Epoch 2160 :: Batch 0/1
Batch Loss = 0.0007310406107605724
2160, epoch_train_loss=0.0007310406107605724
Epoch 2161
Epoch 2161 :: Batch 0/1
Batch Loss = 0.0007310406107434381
2161, epoch_train_loss=0.0007310406107434381
Epoch 2162
Epoch 2162 :: Batch 0/1
Batch Loss = 0.0007310406107262975
2162, epoch_train_loss=0.0007310406107262975
Epoch 2163
Epoch 2163 :: Batch 0/1
Batch Loss = 0.0007310406107091505
2163, epoch_train_loss=0.0007310406107091505
Epoch 2164
Epoch 2164 :: Batch 0/1
Batch Loss = 0.0007310406106919973
2164, epoch_train_loss=0.0007310406106919973
Epoch 2165
Epoch 2165 :: Batch 0/1
Batch Loss = 0.0007310406106748378
2165, epoch_train_loss=0.0007310406106748378
Epoch 2166
Epoch 2166 :: Batch 0/1
Batch Loss = 0.0007310406106576719
2166, epoch_train_loss=0.0007310406106576719
Epoch 2167
Epoch 2167 :: Batch 0/1
Batch Loss = 0.0007310406106404996
2167, epoch_train_loss=0.0007310406106404996
Epoch 2168
Epoch 2168 :: Batch 0/1
Batch Loss = 0.0007310406106233212
2168, epoch_train_loss=0.0007310406106233212
Epoch 2169
Epoch 2169 :: Batch 0/1
Batch Loss = 0.0007310406106061366
2169, epoch_train_loss=0.0007310406106061366
Epoch 2170
Epoch 2170 :: Batch 0/1
Batch Loss = 0.0007310406105889454
2170, epoch_train_loss=0.0007310406105889454
Epoch 2171
Epoch 2171 :: Batch 0/1
Batch Loss = 0.000731040610571748
2171, epoch_train_loss=0.000731040610571748
Epoch 2172
Epoch 2172 :: Batch 0/1
Batch Loss = 0.0007310406105545442
2172, epoch_train_loss=0.0007310406105545442
Epoch 2173
Epoch 2173 :: Batch 0/1
Batch Loss = 0.0007310406105373341
2173, epoch_train_loss=0.0007310406105373341
Epoch 2174
Epoch 2174 :: Batch 0/1
Batch Loss = 0.0007310406105201177
2174, epoch_train_loss=0.0007310406105201177
Epoch 2175
Epoch 2175 :: Batch 0/1
Batch Loss = 0.0007310406105028951
2175, epoch_train_loss=0.0007310406105028951
Epoch 2176
Epoch 2176 :: Batch 0/1
Batch Loss = 0.0007310406104856659
2176, epoch_train_loss=0.0007310406104856659
Epoch 2177
Epoch 2177 :: Batch 0/1
Batch Loss = 0.0007310406104684307
2177, epoch_train_loss=0.0007310406104684307
Epoch 2178
Epoch 2178 :: Batch 0/1
Batch Loss = 0.0007310406104511891
2178, epoch_train_loss=0.0007310406104511891
Epoch 2179
Epoch 2179 :: Batch 0/1
Batch Loss = 0.0007310406104339412
2179, epoch_train_loss=0.0007310406104339412
Epoch 2180
Epoch 2180 :: Batch 0/1
Batch Loss = 0.0007310406104166869
2180, epoch_train_loss=0.0007310406104166869
Epoch 2181
Epoch 2181 :: Batch 0/1
Batch Loss = 0.0007310406103994263
2181, epoch_train_loss=0.0007310406103994263
Epoch 2182
Epoch 2182 :: Batch 0/1
Batch Loss = 0.0007310406103821594
2182, epoch_train_loss=0.0007310406103821594
Epoch 2183
Epoch 2183 :: Batch 0/1
Batch Loss = 0.0007310406103648862
2183, epoch_train_loss=0.0007310406103648862
Epoch 2184
Epoch 2184 :: Batch 0/1
Batch Loss = 0.0007310406103476066
2184, epoch_train_loss=0.0007310406103476066
Epoch 2185
Epoch 2185 :: Batch 0/1
Batch Loss = 0.0007310406103303209
2185, epoch_train_loss=0.0007310406103303209
Epoch 2186
Epoch 2186 :: Batch 0/1
Batch Loss = 0.0007310406103130288
2186, epoch_train_loss=0.0007310406103130288
Epoch 2187
Epoch 2187 :: Batch 0/1
Batch Loss = 0.0007310406102957302
2187, epoch_train_loss=0.0007310406102957302
Epoch 2188
Epoch 2188 :: Batch 0/1
Batch Loss = 0.0007310406102784255
2188, epoch_train_loss=0.0007310406102784255
Epoch 2189
Epoch 2189 :: Batch 0/1
Batch Loss = 0.0007310406102611143
2189, epoch_train_loss=0.0007310406102611143
Epoch 2190
Epoch 2190 :: Batch 0/1
Batch Loss = 0.000731040610243797
2190, epoch_train_loss=0.000731040610243797
Epoch 2191
Epoch 2191 :: Batch 0/1
Batch Loss = 0.0007310406102264733
2191, epoch_train_loss=0.0007310406102264733
Epoch 2192
Epoch 2192 :: Batch 0/1
Batch Loss = 0.0007310406102091433
2192, epoch_train_loss=0.0007310406102091433
Epoch 2193
Epoch 2193 :: Batch 0/1
Batch Loss = 0.000731040610191807
2193, epoch_train_loss=0.000731040610191807
Epoch 2194
Epoch 2194 :: Batch 0/1
Batch Loss = 0.0007310406101744643
2194, epoch_train_loss=0.0007310406101744643
Epoch 2195
Epoch 2195 :: Batch 0/1
Batch Loss = 0.0007310406101571153
2195, epoch_train_loss=0.0007310406101571153
Epoch 2196
Epoch 2196 :: Batch 0/1
Batch Loss = 0.0007310406101397602
2196, epoch_train_loss=0.0007310406101397602
Epoch 2197
Epoch 2197 :: Batch 0/1
Batch Loss = 0.0007310406101223985
2197, epoch_train_loss=0.0007310406101223985
Epoch 2198
Epoch 2198 :: Batch 0/1
Batch Loss = 0.0007310406101050307
2198, epoch_train_loss=0.0007310406101050307
Epoch 2199
Epoch 2199 :: Batch 0/1
Batch Loss = 0.0007310406100876565
2199, epoch_train_loss=0.0007310406100876565
Epoch 2200
Epoch 2200 :: Batch 0/1
Batch Loss = 0.000731040610070276
2200, epoch_train_loss=0.000731040610070276
Epoch 2201
Epoch 2201 :: Batch 0/1
Batch Loss = 0.0007310406100528891
2201, epoch_train_loss=0.0007310406100528891
Epoch 2202
Epoch 2202 :: Batch 0/1
Batch Loss = 0.0007310406100354959
2202, epoch_train_loss=0.0007310406100354959
Epoch 2203
Epoch 2203 :: Batch 0/1
Batch Loss = 0.0007310406100180967
2203, epoch_train_loss=0.0007310406100180967
Epoch 2204
Epoch 2204 :: Batch 0/1
Batch Loss = 0.0007310406100006909
2204, epoch_train_loss=0.0007310406100006909
Epoch 2205
Epoch 2205 :: Batch 0/1
Batch Loss = 0.0007310406099832788
2205, epoch_train_loss=0.0007310406099832788
Epoch 2206
Epoch 2206 :: Batch 0/1
Batch Loss = 0.0007310406099658604
2206, epoch_train_loss=0.0007310406099658604
Epoch 2207
Epoch 2207 :: Batch 0/1
Batch Loss = 0.0007310406099484357
2207, epoch_train_loss=0.0007310406099484357
Epoch 2208
Epoch 2208 :: Batch 0/1
Batch Loss = 0.0007310406099310048
2208, epoch_train_loss=0.0007310406099310048
Epoch 2209
Epoch 2209 :: Batch 0/1
Batch Loss = 0.0007310406099135674
2209, epoch_train_loss=0.0007310406099135674
Epoch 2210
Epoch 2210 :: Batch 0/1
Batch Loss = 0.0007310406098961238
2210, epoch_train_loss=0.0007310406098961238
Epoch 2211
Epoch 2211 :: Batch 0/1
Batch Loss = 0.0007310406098786739
2211, epoch_train_loss=0.0007310406098786739
Epoch 2212
Epoch 2212 :: Batch 0/1
Batch Loss = 0.0007310406098612177
2212, epoch_train_loss=0.0007310406098612177
Epoch 2213
Epoch 2213 :: Batch 0/1
Batch Loss = 0.0007310406098437551
2213, epoch_train_loss=0.0007310406098437551
Epoch 2214
Epoch 2214 :: Batch 0/1
Batch Loss = 0.0007310406098262863
2214, epoch_train_loss=0.0007310406098262863
Epoch 2215
Epoch 2215 :: Batch 0/1
Batch Loss = 0.000731040609808811
2215, epoch_train_loss=0.000731040609808811
Epoch 2216
Epoch 2216 :: Batch 0/1
Batch Loss = 0.0007310406097913295
2216, epoch_train_loss=0.0007310406097913295
Epoch 2217
Epoch 2217 :: Batch 0/1
Batch Loss = 0.000731040609773842
2217, epoch_train_loss=0.000731040609773842
Epoch 2218
Epoch 2218 :: Batch 0/1
Batch Loss = 0.0007310406097563477
2218, epoch_train_loss=0.0007310406097563477
Epoch 2219
Epoch 2219 :: Batch 0/1
Batch Loss = 0.0007310406097388474
2219, epoch_train_loss=0.0007310406097388474
Epoch 2220
Epoch 2220 :: Batch 0/1
Batch Loss = 0.0007310406097213405
2220, epoch_train_loss=0.0007310406097213405
Epoch 2221
Epoch 2221 :: Batch 0/1
Batch Loss = 0.0007310406097038276
2221, epoch_train_loss=0.0007310406097038276
Epoch 2222
Epoch 2222 :: Batch 0/1
Batch Loss = 0.0007310406096863082
2222, epoch_train_loss=0.0007310406096863082
Epoch 2223
Epoch 2223 :: Batch 0/1
Batch Loss = 0.0007310406096687827
2223, epoch_train_loss=0.0007310406096687827
Epoch 2224
Epoch 2224 :: Batch 0/1
Batch Loss = 0.0007310406096512507
2224, epoch_train_loss=0.0007310406096512507
Epoch 2225
Epoch 2225 :: Batch 0/1
Batch Loss = 0.0007310406096337125
2225, epoch_train_loss=0.0007310406096337125
Epoch 2226
Epoch 2226 :: Batch 0/1
Batch Loss = 0.000731040609616168
2226, epoch_train_loss=0.000731040609616168
Epoch 2227
Epoch 2227 :: Batch 0/1
Batch Loss = 0.000731040609598617
2227, epoch_train_loss=0.000731040609598617
Epoch 2228
Epoch 2228 :: Batch 0/1
Batch Loss = 0.0007310406095810599
2228, epoch_train_loss=0.0007310406095810599
Epoch 2229
Epoch 2229 :: Batch 0/1
Batch Loss = 0.0007310406095634963
2229, epoch_train_loss=0.0007310406095634963
Epoch 2230
Epoch 2230 :: Batch 0/1
Batch Loss = 0.0007310406095459267
2230, epoch_train_loss=0.0007310406095459267
Epoch 2231
Epoch 2231 :: Batch 0/1
Batch Loss = 0.0007310406095283506
2231, epoch_train_loss=0.0007310406095283506
Epoch 2232
Epoch 2232 :: Batch 0/1
Batch Loss = 0.0007310406095107682
2232, epoch_train_loss=0.0007310406095107682
Epoch 2233
Epoch 2233 :: Batch 0/1
Batch Loss = 0.0007310406094931794
2233, epoch_train_loss=0.0007310406094931794
Epoch 2234
Epoch 2234 :: Batch 0/1
Batch Loss = 0.0007310406094755846
2234, epoch_train_loss=0.0007310406094755846
Epoch 2235
Epoch 2235 :: Batch 0/1
Batch Loss = 0.0007310406094579831
2235, epoch_train_loss=0.0007310406094579831
Epoch 2236
Epoch 2236 :: Batch 0/1
Batch Loss = 0.0007310406094403755
2236, epoch_train_loss=0.0007310406094403755
Epoch 2237
Epoch 2237 :: Batch 0/1
Batch Loss = 0.0007310406094227616
2237, epoch_train_loss=0.0007310406094227616
Epoch 2238
Epoch 2238 :: Batch 0/1
Batch Loss = 0.0007310406094051414
2238, epoch_train_loss=0.0007310406094051414
Epoch 2239
Epoch 2239 :: Batch 0/1
Batch Loss = 0.0007310406093875147
2239, epoch_train_loss=0.0007310406093875147
Epoch 2240
Epoch 2240 :: Batch 0/1
Batch Loss = 0.000731040609369882
2240, epoch_train_loss=0.000731040609369882
Epoch 2241
Epoch 2241 :: Batch 0/1
Batch Loss = 0.0007310406093522427
2241, epoch_train_loss=0.0007310406093522427
Epoch 2242
Epoch 2242 :: Batch 0/1
Batch Loss = 0.0007310406093345973
2242, epoch_train_loss=0.0007310406093345973
Epoch 2243
Epoch 2243 :: Batch 0/1
Batch Loss = 0.0007310406093169455
2243, epoch_train_loss=0.0007310406093169455
Epoch 2244
Epoch 2244 :: Batch 0/1
Batch Loss = 0.0007310406092992874
2244, epoch_train_loss=0.0007310406092992874
Epoch 2245
Epoch 2245 :: Batch 0/1
Batch Loss = 0.0007310406092816231
2245, epoch_train_loss=0.0007310406092816231
Epoch 2246
Epoch 2246 :: Batch 0/1
Batch Loss = 0.0007310406092639525
2246, epoch_train_loss=0.0007310406092639525
Epoch 2247
Epoch 2247 :: Batch 0/1
Batch Loss = 0.0007310406092462755
2247, epoch_train_loss=0.0007310406092462755
Epoch 2248
Epoch 2248 :: Batch 0/1
Batch Loss = 0.0007310406092285921
2248, epoch_train_loss=0.0007310406092285921
Epoch 2249
Epoch 2249 :: Batch 0/1
Batch Loss = 0.0007310406092109025
2249, epoch_train_loss=0.0007310406092109025
Epoch 2250
Epoch 2250 :: Batch 0/1
Batch Loss = 0.0007310406091932066
2250, epoch_train_loss=0.0007310406091932066
Epoch 2251
Epoch 2251 :: Batch 0/1
Batch Loss = 0.0007310406091755045
2251, epoch_train_loss=0.0007310406091755045
Epoch 2252
Epoch 2252 :: Batch 0/1
Batch Loss = 0.000731040609157796
2252, epoch_train_loss=0.000731040609157796
Epoch 2253
Epoch 2253 :: Batch 0/1
Batch Loss = 0.0007310406091400811
2253, epoch_train_loss=0.0007310406091400811
Epoch 2254
Epoch 2254 :: Batch 0/1
Batch Loss = 0.0007310406091223599
2254, epoch_train_loss=0.0007310406091223599
Epoch 2255
Epoch 2255 :: Batch 0/1
Batch Loss = 0.0007310406091046326
2255, epoch_train_loss=0.0007310406091046326
Epoch 2256
Epoch 2256 :: Batch 0/1
Batch Loss = 0.0007310406090868989
2256, epoch_train_loss=0.0007310406090868989
Epoch 2257
Epoch 2257 :: Batch 0/1
Batch Loss = 0.0007310406090691588
2257, epoch_train_loss=0.0007310406090691588
Epoch 2258
Epoch 2258 :: Batch 0/1
Batch Loss = 0.0007310406090514125
2258, epoch_train_loss=0.0007310406090514125
Epoch 2259
Epoch 2259 :: Batch 0/1
Batch Loss = 0.0007310406090336598
2259, epoch_train_loss=0.0007310406090336598
Epoch 2260
Epoch 2260 :: Batch 0/1
Batch Loss = 0.0007310406090159009
2260, epoch_train_loss=0.0007310406090159009
Epoch 2261
Epoch 2261 :: Batch 0/1
Batch Loss = 0.0007310406089981357
2261, epoch_train_loss=0.0007310406089981357
Epoch 2262
Epoch 2262 :: Batch 0/1
Batch Loss = 0.0007310406089803643
2262, epoch_train_loss=0.0007310406089803643
Epoch 2263
Epoch 2263 :: Batch 0/1
Batch Loss = 0.0007310406089625863
2263, epoch_train_loss=0.0007310406089625863
Epoch 2264
Epoch 2264 :: Batch 0/1
Batch Loss = 0.0007310406089448022
2264, epoch_train_loss=0.0007310406089448022
Epoch 2265
Epoch 2265 :: Batch 0/1
Batch Loss = 0.0007310406089270118
2265, epoch_train_loss=0.0007310406089270118
Epoch 2266
Epoch 2266 :: Batch 0/1
Batch Loss = 0.000731040608909215
2266, epoch_train_loss=0.000731040608909215
Epoch 2267
Epoch 2267 :: Batch 0/1
Batch Loss = 0.0007310406088914119
2267, epoch_train_loss=0.0007310406088914119
Epoch 2268
Epoch 2268 :: Batch 0/1
Batch Loss = 0.0007310406088736027
2268, epoch_train_loss=0.0007310406088736027
Epoch 2269
Epoch 2269 :: Batch 0/1
Batch Loss = 0.0007310406088557869
2269, epoch_train_loss=0.0007310406088557869
Epoch 2270
Epoch 2270 :: Batch 0/1
Batch Loss = 0.000731040608837965
2270, epoch_train_loss=0.000731040608837965
Epoch 2271
Epoch 2271 :: Batch 0/1
Batch Loss = 0.0007310406088201367
2271, epoch_train_loss=0.0007310406088201367
Epoch 2272
Epoch 2272 :: Batch 0/1
Batch Loss = 0.0007310406088023022
2272, epoch_train_loss=0.0007310406088023022
Epoch 2273
Epoch 2273 :: Batch 0/1
Batch Loss = 0.0007310406087844613
2273, epoch_train_loss=0.0007310406087844613
Epoch 2274
Epoch 2274 :: Batch 0/1
Batch Loss = 0.0007310406087666141
2274, epoch_train_loss=0.0007310406087666141
Epoch 2275
Epoch 2275 :: Batch 0/1
Batch Loss = 0.0007310406087487607
2275, epoch_train_loss=0.0007310406087487607
Epoch 2276
Epoch 2276 :: Batch 0/1
Batch Loss = 0.0007310406087309009
2276, epoch_train_loss=0.0007310406087309009
Epoch 2277
Epoch 2277 :: Batch 0/1
Batch Loss = 0.0007310406087130349
2277, epoch_train_loss=0.0007310406087130349
Epoch 2278
Epoch 2278 :: Batch 0/1
Batch Loss = 0.0007310406086951625
2278, epoch_train_loss=0.0007310406086951625
Epoch 2279
Epoch 2279 :: Batch 0/1
Batch Loss = 0.0007310406086772838
2279, epoch_train_loss=0.0007310406086772838
Epoch 2280
Epoch 2280 :: Batch 0/1
Batch Loss = 0.0007310406086593989
2280, epoch_train_loss=0.0007310406086593989
Epoch 2281
Epoch 2281 :: Batch 0/1
Batch Loss = 0.0007310406086415076
2281, epoch_train_loss=0.0007310406086415076
Epoch 2282
Epoch 2282 :: Batch 0/1
Batch Loss = 0.00073104060862361
2282, epoch_train_loss=0.00073104060862361
Epoch 2283
Epoch 2283 :: Batch 0/1
Batch Loss = 0.0007310406086057062
2283, epoch_train_loss=0.0007310406086057062
Epoch 2284
Epoch 2284 :: Batch 0/1
Batch Loss = 0.000731040608587796
2284, epoch_train_loss=0.000731040608587796
Epoch 2285
Epoch 2285 :: Batch 0/1
Batch Loss = 0.0007310406085698796
2285, epoch_train_loss=0.0007310406085698796
Epoch 2286
Epoch 2286 :: Batch 0/1
Batch Loss = 0.000731040608551957
2286, epoch_train_loss=0.000731040608551957
Epoch 2287
Epoch 2287 :: Batch 0/1
Batch Loss = 0.0007310406085340278
2287, epoch_train_loss=0.0007310406085340278
Epoch 2288
Epoch 2288 :: Batch 0/1
Batch Loss = 0.0007310406085160924
2288, epoch_train_loss=0.0007310406085160924
Epoch 2289
Epoch 2289 :: Batch 0/1
Batch Loss = 0.0007310406084981508
2289, epoch_train_loss=0.0007310406084981508
Epoch 2290
Epoch 2290 :: Batch 0/1
Batch Loss = 0.0007310406084802028
2290, epoch_train_loss=0.0007310406084802028
Epoch 2291
Epoch 2291 :: Batch 0/1
Batch Loss = 0.0007310406084622487
2291, epoch_train_loss=0.0007310406084622487
Epoch 2292
Epoch 2292 :: Batch 0/1
Batch Loss = 0.0007310406084442882
2292, epoch_train_loss=0.0007310406084442882
Epoch 2293
Epoch 2293 :: Batch 0/1
Batch Loss = 0.0007310406084263213
2293, epoch_train_loss=0.0007310406084263213
Epoch 2294
Epoch 2294 :: Batch 0/1
Batch Loss = 0.0007310406084083482
2294, epoch_train_loss=0.0007310406084083482
Epoch 2295
Epoch 2295 :: Batch 0/1
Batch Loss = 0.0007310406083903688
2295, epoch_train_loss=0.0007310406083903688
Epoch 2296
Epoch 2296 :: Batch 0/1
Batch Loss = 0.000731040608372383
2296, epoch_train_loss=0.000731040608372383
Epoch 2297
Epoch 2297 :: Batch 0/1
Batch Loss = 0.0007310406083543909
2297, epoch_train_loss=0.0007310406083543909
Epoch 2298
Epoch 2298 :: Batch 0/1
Batch Loss = 0.0007310406083363927
2298, epoch_train_loss=0.0007310406083363927
Epoch 2299
Epoch 2299 :: Batch 0/1
Batch Loss = 0.0007310406083183879
2299, epoch_train_loss=0.0007310406083183879
Epoch 2300
Epoch 2300 :: Batch 0/1
Batch Loss = 0.000731040608300377
2300, epoch_train_loss=0.000731040608300377
Epoch 2301
Epoch 2301 :: Batch 0/1
Batch Loss = 0.0007310406082823599
2301, epoch_train_loss=0.0007310406082823599
Epoch 2302
Epoch 2302 :: Batch 0/1
Batch Loss = 0.0007310406082643363
2302, epoch_train_loss=0.0007310406082643363
Epoch 2303
Epoch 2303 :: Batch 0/1
Batch Loss = 0.0007310406082463067
2303, epoch_train_loss=0.0007310406082463067
Epoch 2304
Epoch 2304 :: Batch 0/1
Batch Loss = 0.0007310406082282706
2304, epoch_train_loss=0.0007310406082282706
Epoch 2305
Epoch 2305 :: Batch 0/1
Batch Loss = 0.000731040608210228
2305, epoch_train_loss=0.000731040608210228
Epoch 2306
Epoch 2306 :: Batch 0/1
Batch Loss = 0.0007310406081921793
2306, epoch_train_loss=0.0007310406081921793
Epoch 2307
Epoch 2307 :: Batch 0/1
Batch Loss = 0.0007310406081741244
2307, epoch_train_loss=0.0007310406081741244
Epoch 2308
Epoch 2308 :: Batch 0/1
Batch Loss = 0.0007310406081560632
2308, epoch_train_loss=0.0007310406081560632
Epoch 2309
Epoch 2309 :: Batch 0/1
Batch Loss = 0.0007310406081379955
2309, epoch_train_loss=0.0007310406081379955
Epoch 2310
Epoch 2310 :: Batch 0/1
Batch Loss = 0.0007310406081199218
2310, epoch_train_loss=0.0007310406081199218
Epoch 2311
Epoch 2311 :: Batch 0/1
Batch Loss = 0.0007310406081018415
2311, epoch_train_loss=0.0007310406081018415
Epoch 2312
Epoch 2312 :: Batch 0/1
Batch Loss = 0.0007310406080837552
2312, epoch_train_loss=0.0007310406080837552
Epoch 2313
Epoch 2313 :: Batch 0/1
Batch Loss = 0.0007310406080656624
2313, epoch_train_loss=0.0007310406080656624
Epoch 2314
Epoch 2314 :: Batch 0/1
Batch Loss = 0.0007310406080475633
2314, epoch_train_loss=0.0007310406080475633
Epoch 2315
Epoch 2315 :: Batch 0/1
Batch Loss = 0.0007310406080294581
2315, epoch_train_loss=0.0007310406080294581
Epoch 2316
Epoch 2316 :: Batch 0/1
Batch Loss = 0.0007310406080113465
2316, epoch_train_loss=0.0007310406080113465
Epoch 2317
Epoch 2317 :: Batch 0/1
Batch Loss = 0.0007310406079932284
2317, epoch_train_loss=0.0007310406079932284
Epoch 2318
Epoch 2318 :: Batch 0/1
Batch Loss = 0.0007310406079751044
2318, epoch_train_loss=0.0007310406079751044
Epoch 2319
Epoch 2319 :: Batch 0/1
Batch Loss = 0.0007310406079569738
2319, epoch_train_loss=0.0007310406079569738
Epoch 2320
Epoch 2320 :: Batch 0/1
Batch Loss = 0.000731040607938837
2320, epoch_train_loss=0.000731040607938837
Epoch 2321
Epoch 2321 :: Batch 0/1
Batch Loss = 0.0007310406079206939
2321, epoch_train_loss=0.0007310406079206939
Epoch 2322
Epoch 2322 :: Batch 0/1
Batch Loss = 0.0007310406079025445
2322, epoch_train_loss=0.0007310406079025445
Epoch 2323
Epoch 2323 :: Batch 0/1
Batch Loss = 0.0007310406078843888
2323, epoch_train_loss=0.0007310406078843888
Epoch 2324
Epoch 2324 :: Batch 0/1
Batch Loss = 0.0007310406078662268
2324, epoch_train_loss=0.0007310406078662268
Epoch 2325
Epoch 2325 :: Batch 0/1
Batch Loss = 0.0007310406078480586
2325, epoch_train_loss=0.0007310406078480586
Epoch 2326
Epoch 2326 :: Batch 0/1
Batch Loss = 0.0007310406078298842
2326, epoch_train_loss=0.0007310406078298842
Epoch 2327
Epoch 2327 :: Batch 0/1
Batch Loss = 0.0007310406078117033
2327, epoch_train_loss=0.0007310406078117033
Epoch 2328
Epoch 2328 :: Batch 0/1
Batch Loss = 0.0007310406077935161
2328, epoch_train_loss=0.0007310406077935161
Epoch 2329
Epoch 2329 :: Batch 0/1
Batch Loss = 0.0007310406077753227
2329, epoch_train_loss=0.0007310406077753227
Epoch 2330
Epoch 2330 :: Batch 0/1
Batch Loss = 0.000731040607757123
2330, epoch_train_loss=0.000731040607757123
Epoch 2331
Epoch 2331 :: Batch 0/1
Batch Loss = 0.000731040607738917
2331, epoch_train_loss=0.000731040607738917
Epoch 2332
Epoch 2332 :: Batch 0/1
Batch Loss = 0.0007310406077207048
2332, epoch_train_loss=0.0007310406077207048
Epoch 2333
Epoch 2333 :: Batch 0/1
Batch Loss = 0.0007310406077024862
2333, epoch_train_loss=0.0007310406077024862
Epoch 2334
Epoch 2334 :: Batch 0/1
Batch Loss = 0.0007310406076842613
2334, epoch_train_loss=0.0007310406076842613
Epoch 2335
Epoch 2335 :: Batch 0/1
Batch Loss = 0.0007310406076660301
2335, epoch_train_loss=0.0007310406076660301
Epoch 2336
Epoch 2336 :: Batch 0/1
Batch Loss = 0.0007310406076477927
2336, epoch_train_loss=0.0007310406076477927
Epoch 2337
Epoch 2337 :: Batch 0/1
Batch Loss = 0.000731040607629549
2337, epoch_train_loss=0.000731040607629549
Epoch 2338
Epoch 2338 :: Batch 0/1
Batch Loss = 0.000731040607611299
2338, epoch_train_loss=0.000731040607611299
Epoch 2339
Epoch 2339 :: Batch 0/1
Batch Loss = 0.0007310406075930427
2339, epoch_train_loss=0.0007310406075930427
Epoch 2340
Epoch 2340 :: Batch 0/1
Batch Loss = 0.0007310406075747801
2340, epoch_train_loss=0.0007310406075747801
Epoch 2341
Epoch 2341 :: Batch 0/1
Batch Loss = 0.0007310406075565111
2341, epoch_train_loss=0.0007310406075565111
Epoch 2342
Epoch 2342 :: Batch 0/1
Batch Loss = 0.0007310406075382359
2342, epoch_train_loss=0.0007310406075382359
Epoch 2343
Epoch 2343 :: Batch 0/1
Batch Loss = 0.0007310406075199546
2343, epoch_train_loss=0.0007310406075199546
Epoch 2344
Epoch 2344 :: Batch 0/1
Batch Loss = 0.0007310406075016667
2344, epoch_train_loss=0.0007310406075016667
Epoch 2345
Epoch 2345 :: Batch 0/1
Batch Loss = 0.0007310406074833728
2345, epoch_train_loss=0.0007310406074833728
Epoch 2346
Epoch 2346 :: Batch 0/1
Batch Loss = 0.0007310406074650725
2346, epoch_train_loss=0.0007310406074650725
Epoch 2347
Epoch 2347 :: Batch 0/1
Batch Loss = 0.0007310406074467659
2347, epoch_train_loss=0.0007310406074467659
Epoch 2348
Epoch 2348 :: Batch 0/1
Batch Loss = 0.000731040607428453
2348, epoch_train_loss=0.000731040607428453
Epoch 2349
Epoch 2349 :: Batch 0/1
Batch Loss = 0.0007310406074101338
2349, epoch_train_loss=0.0007310406074101338
Epoch 2350
Epoch 2350 :: Batch 0/1
Batch Loss = 0.0007310406073918083
2350, epoch_train_loss=0.0007310406073918083
Epoch 2351
Epoch 2351 :: Batch 0/1
Batch Loss = 0.0007310406073734766
2351, epoch_train_loss=0.0007310406073734766
Epoch 2352
Epoch 2352 :: Batch 0/1
Batch Loss = 0.0007310406073551386
2352, epoch_train_loss=0.0007310406073551386
Epoch 2353
Epoch 2353 :: Batch 0/1
Batch Loss = 0.0007310406073367942
2353, epoch_train_loss=0.0007310406073367942
Epoch 2354
Epoch 2354 :: Batch 0/1
Batch Loss = 0.0007310406073184435
2354, epoch_train_loss=0.0007310406073184435
Epoch 2355
Epoch 2355 :: Batch 0/1
Batch Loss = 0.0007310406073000867
2355, epoch_train_loss=0.0007310406073000867
Epoch 2356
Epoch 2356 :: Batch 0/1
Batch Loss = 0.0007310406072817235
2356, epoch_train_loss=0.0007310406072817235
Epoch 2357
Epoch 2357 :: Batch 0/1
Batch Loss = 0.000731040607263354
2357, epoch_train_loss=0.000731040607263354
Epoch 2358
Epoch 2358 :: Batch 0/1
Batch Loss = 0.0007310406072449783
2358, epoch_train_loss=0.0007310406072449783
Epoch 2359
Epoch 2359 :: Batch 0/1
Batch Loss = 0.0007310406072265961
2359, epoch_train_loss=0.0007310406072265961
Epoch 2360
Epoch 2360 :: Batch 0/1
Batch Loss = 0.0007310406072082079
2360, epoch_train_loss=0.0007310406072082079
Epoch 2361
Epoch 2361 :: Batch 0/1
Batch Loss = 0.0007310406071898134
2361, epoch_train_loss=0.0007310406071898134
Epoch 2362
Epoch 2362 :: Batch 0/1
Batch Loss = 0.0007310406071714124
2362, epoch_train_loss=0.0007310406071714124
Epoch 2363
Epoch 2363 :: Batch 0/1
Batch Loss = 0.0007310406071530052
2363, epoch_train_loss=0.0007310406071530052
Epoch 2364
Epoch 2364 :: Batch 0/1
Batch Loss = 0.0007310406071345917
2364, epoch_train_loss=0.0007310406071345917
Epoch 2365
Epoch 2365 :: Batch 0/1
Batch Loss = 0.0007310406071161721
2365, epoch_train_loss=0.0007310406071161721
Epoch 2366
Epoch 2366 :: Batch 0/1
Batch Loss = 0.0007310406070977462
2366, epoch_train_loss=0.0007310406070977462
Epoch 2367
Epoch 2367 :: Batch 0/1
Batch Loss = 0.0007310406070793138
2367, epoch_train_loss=0.0007310406070793138
Epoch 2368
Epoch 2368 :: Batch 0/1
Batch Loss = 0.0007310406070608752
2368, epoch_train_loss=0.0007310406070608752
Epoch 2369
Epoch 2369 :: Batch 0/1
Batch Loss = 0.0007310406070424304
2369, epoch_train_loss=0.0007310406070424304
Epoch 2370
Epoch 2370 :: Batch 0/1
Batch Loss = 0.0007310406070239792
2370, epoch_train_loss=0.0007310406070239792
Epoch 2371
Epoch 2371 :: Batch 0/1
Batch Loss = 0.0007310406070055217
2371, epoch_train_loss=0.0007310406070055217
Epoch 2372
Epoch 2372 :: Batch 0/1
Batch Loss = 0.000731040606987058
2372, epoch_train_loss=0.000731040606987058
Epoch 2373
Epoch 2373 :: Batch 0/1
Batch Loss = 0.0007310406069685881
2373, epoch_train_loss=0.0007310406069685881
Epoch 2374
Epoch 2374 :: Batch 0/1
Batch Loss = 0.0007310406069501119
2374, epoch_train_loss=0.0007310406069501119
Epoch 2375
Epoch 2375 :: Batch 0/1
Batch Loss = 0.0007310406069316293
2375, epoch_train_loss=0.0007310406069316293
Epoch 2376
Epoch 2376 :: Batch 0/1
Batch Loss = 0.0007310406069131404
2376, epoch_train_loss=0.0007310406069131404
Epoch 2377
Epoch 2377 :: Batch 0/1
Batch Loss = 0.0007310406068946453
2377, epoch_train_loss=0.0007310406068946453
Epoch 2378
Epoch 2378 :: Batch 0/1
Batch Loss = 0.0007310406068761439
2378, epoch_train_loss=0.0007310406068761439
Epoch 2379
Epoch 2379 :: Batch 0/1
Batch Loss = 0.0007310406068576363
2379, epoch_train_loss=0.0007310406068576363
Epoch 2380
Epoch 2380 :: Batch 0/1
Batch Loss = 0.0007310406068391222
2380, epoch_train_loss=0.0007310406068391222
Epoch 2381
Epoch 2381 :: Batch 0/1
Batch Loss = 0.0007310406068206021
2381, epoch_train_loss=0.0007310406068206021
Epoch 2382
Epoch 2382 :: Batch 0/1
Batch Loss = 0.0007310406068020756
2382, epoch_train_loss=0.0007310406068020756
Epoch 2383
Epoch 2383 :: Batch 0/1
Batch Loss = 0.0007310406067835429
2383, epoch_train_loss=0.0007310406067835429
Epoch 2384
Epoch 2384 :: Batch 0/1
Batch Loss = 0.0007310406067650038
2384, epoch_train_loss=0.0007310406067650038
Epoch 2385
Epoch 2385 :: Batch 0/1
Batch Loss = 0.0007310406067464584
2385, epoch_train_loss=0.0007310406067464584
Epoch 2386
Epoch 2386 :: Batch 0/1
Batch Loss = 0.0007310406067279067
2386, epoch_train_loss=0.0007310406067279067
Epoch 2387
Epoch 2387 :: Batch 0/1
Batch Loss = 0.0007310406067093489
2387, epoch_train_loss=0.0007310406067093489
Epoch 2388
Epoch 2388 :: Batch 0/1
Batch Loss = 0.0007310406066907848
2388, epoch_train_loss=0.0007310406066907848
Epoch 2389
Epoch 2389 :: Batch 0/1
Batch Loss = 0.0007310406066722143
2389, epoch_train_loss=0.0007310406066722143
Epoch 2390
Epoch 2390 :: Batch 0/1
Batch Loss = 0.0007310406066536377
2390, epoch_train_loss=0.0007310406066536377
Epoch 2391
Epoch 2391 :: Batch 0/1
Batch Loss = 0.0007310406066350545
2391, epoch_train_loss=0.0007310406066350545
Epoch 2392
Epoch 2392 :: Batch 0/1
Batch Loss = 0.0007310406066164651
2392, epoch_train_loss=0.0007310406066164651
Epoch 2393
Epoch 2393 :: Batch 0/1
Batch Loss = 0.0007310406065978696
2393, epoch_train_loss=0.0007310406065978696
Epoch 2394
Epoch 2394 :: Batch 0/1
Batch Loss = 0.0007310406065792678
2394, epoch_train_loss=0.0007310406065792678
Epoch 2395
Epoch 2395 :: Batch 0/1
Batch Loss = 0.0007310406065606597
2395, epoch_train_loss=0.0007310406065606597
Epoch 2396
Epoch 2396 :: Batch 0/1
Batch Loss = 0.0007310406065420453
2396, epoch_train_loss=0.0007310406065420453
Epoch 2397
Epoch 2397 :: Batch 0/1
Batch Loss = 0.0007310406065234247
2397, epoch_train_loss=0.0007310406065234247
Epoch 2398
Epoch 2398 :: Batch 0/1
Batch Loss = 0.0007310406065047976
2398, epoch_train_loss=0.0007310406065047976
Epoch 2399
Epoch 2399 :: Batch 0/1
Batch Loss = 0.0007310406064861645
2399, epoch_train_loss=0.0007310406064861645
Epoch 2400
Epoch 2400 :: Batch 0/1
Batch Loss = 0.000731040606467525
2400, epoch_train_loss=0.000731040606467525
Epoch 2401
Epoch 2401 :: Batch 0/1
Batch Loss = 0.0007310406064488793
2401, epoch_train_loss=0.0007310406064488793
Epoch 2402
Epoch 2402 :: Batch 0/1
Batch Loss = 0.0007310406064302273
2402, epoch_train_loss=0.0007310406064302273
Epoch 2403
Epoch 2403 :: Batch 0/1
Batch Loss = 0.0007310406064115689
2403, epoch_train_loss=0.0007310406064115689
Epoch 2404
Epoch 2404 :: Batch 0/1
Batch Loss = 0.0007310406063929044
2404, epoch_train_loss=0.0007310406063929044
Epoch 2405
Epoch 2405 :: Batch 0/1
Batch Loss = 0.0007310406063742336
2405, epoch_train_loss=0.0007310406063742336
Epoch 2406
Epoch 2406 :: Batch 0/1
Batch Loss = 0.0007310406063555564
2406, epoch_train_loss=0.0007310406063555564
Epoch 2407
Epoch 2407 :: Batch 0/1
Batch Loss = 0.0007310406063368729
2407, epoch_train_loss=0.0007310406063368729
Epoch 2408
Epoch 2408 :: Batch 0/1
Batch Loss = 0.0007310406063181832
2408, epoch_train_loss=0.0007310406063181832
Epoch 2409
Epoch 2409 :: Batch 0/1
Batch Loss = 0.0007310406062994873
2409, epoch_train_loss=0.0007310406062994873
Epoch 2410
Epoch 2410 :: Batch 0/1
Batch Loss = 0.0007310406062807852
2410, epoch_train_loss=0.0007310406062807852
Epoch 2411
Epoch 2411 :: Batch 0/1
Batch Loss = 0.0007310406062620766
2411, epoch_train_loss=0.0007310406062620766
Epoch 2412
Epoch 2412 :: Batch 0/1
Batch Loss = 0.0007310406062433619
2412, epoch_train_loss=0.0007310406062433619
Epoch 2413
Epoch 2413 :: Batch 0/1
Batch Loss = 0.0007310406062246408
2413, epoch_train_loss=0.0007310406062246408
Epoch 2414
Epoch 2414 :: Batch 0/1
Batch Loss = 0.0007310406062059136
2414, epoch_train_loss=0.0007310406062059136
Epoch 2415
Epoch 2415 :: Batch 0/1
Batch Loss = 0.00073104060618718
2415, epoch_train_loss=0.00073104060618718
Epoch 2416
Epoch 2416 :: Batch 0/1
Batch Loss = 0.0007310406061684401
2416, epoch_train_loss=0.0007310406061684401
Epoch 2417
Epoch 2417 :: Batch 0/1
Batch Loss = 0.000731040606149694
2417, epoch_train_loss=0.000731040606149694
Epoch 2418
Epoch 2418 :: Batch 0/1
Batch Loss = 0.0007310406061309417
2418, epoch_train_loss=0.0007310406061309417
Epoch 2419
Epoch 2419 :: Batch 0/1
Batch Loss = 0.000731040606112183
2419, epoch_train_loss=0.000731040606112183
Epoch 2420
Epoch 2420 :: Batch 0/1
Batch Loss = 0.000731040606093418
2420, epoch_train_loss=0.000731040606093418
Epoch 2421
Epoch 2421 :: Batch 0/1
Batch Loss = 0.0007310406060746469
2421, epoch_train_loss=0.0007310406060746469
Epoch 2422
Epoch 2422 :: Batch 0/1
Batch Loss = 0.0007310406060558693
2422, epoch_train_loss=0.0007310406060558693
Epoch 2423
Epoch 2423 :: Batch 0/1
Batch Loss = 0.0007310406060370857
2423, epoch_train_loss=0.0007310406060370857
Epoch 2424
Epoch 2424 :: Batch 0/1
Batch Loss = 0.0007310406060182956
2424, epoch_train_loss=0.0007310406060182956
Epoch 2425
Epoch 2425 :: Batch 0/1
Batch Loss = 0.0007310406059994992
2425, epoch_train_loss=0.0007310406059994992
Epoch 2426
Epoch 2426 :: Batch 0/1
Batch Loss = 0.0007310406059806969
2426, epoch_train_loss=0.0007310406059806969
Epoch 2427
Epoch 2427 :: Batch 0/1
Batch Loss = 0.0007310406059618881
2427, epoch_train_loss=0.0007310406059618881
Epoch 2428
Epoch 2428 :: Batch 0/1
Batch Loss = 0.0007310406059430729
2428, epoch_train_loss=0.0007310406059430729
Epoch 2429
Epoch 2429 :: Batch 0/1
Batch Loss = 0.0007310406059242517
2429, epoch_train_loss=0.0007310406059242517
Epoch 2430
Epoch 2430 :: Batch 0/1
Batch Loss = 0.000731040605905424
2430, epoch_train_loss=0.000731040605905424
Epoch 2431
Epoch 2431 :: Batch 0/1
Batch Loss = 0.0007310406058865902
2431, epoch_train_loss=0.0007310406058865902
Epoch 2432
Epoch 2432 :: Batch 0/1
Batch Loss = 0.00073104060586775
2432, epoch_train_loss=0.00073104060586775
Epoch 2433
Epoch 2433 :: Batch 0/1
Batch Loss = 0.0007310406058489036
2433, epoch_train_loss=0.0007310406058489036
Epoch 2434
Epoch 2434 :: Batch 0/1
Batch Loss = 0.000731040605830051
2434, epoch_train_loss=0.000731040605830051
Epoch 2435
Epoch 2435 :: Batch 0/1
Batch Loss = 0.000731040605811192
2435, epoch_train_loss=0.000731040605811192
Epoch 2436
Epoch 2436 :: Batch 0/1
Batch Loss = 0.0007310406057923269
2436, epoch_train_loss=0.0007310406057923269
Epoch 2437
Epoch 2437 :: Batch 0/1
Batch Loss = 0.0007310406057734555
2437, epoch_train_loss=0.0007310406057734555
Epoch 2438
Epoch 2438 :: Batch 0/1
Batch Loss = 0.0007310406057545776
2438, epoch_train_loss=0.0007310406057545776
Epoch 2439
Epoch 2439 :: Batch 0/1
Batch Loss = 0.0007310406057356937
2439, epoch_train_loss=0.0007310406057356937
Epoch 2440
Epoch 2440 :: Batch 0/1
Batch Loss = 0.0007310406057168034
2440, epoch_train_loss=0.0007310406057168034
Epoch 2441
Epoch 2441 :: Batch 0/1
Batch Loss = 0.0007310406056979068
2441, epoch_train_loss=0.0007310406056979068
Epoch 2442
Epoch 2442 :: Batch 0/1
Batch Loss = 0.0007310406056790041
2442, epoch_train_loss=0.0007310406056790041
Epoch 2443
Epoch 2443 :: Batch 0/1
Batch Loss = 0.0007310406056600951
2443, epoch_train_loss=0.0007310406056600951
Epoch 2444
Epoch 2444 :: Batch 0/1
Batch Loss = 0.0007310406056411798
2444, epoch_train_loss=0.0007310406056411798
Epoch 2445
Epoch 2445 :: Batch 0/1
Batch Loss = 0.0007310406056222581
2445, epoch_train_loss=0.0007310406056222581
Epoch 2446
Epoch 2446 :: Batch 0/1
Batch Loss = 0.0007310406056033303
2446, epoch_train_loss=0.0007310406056033303
Epoch 2447
Epoch 2447 :: Batch 0/1
Batch Loss = 0.0007310406055843962
2447, epoch_train_loss=0.0007310406055843962
Epoch 2448
Epoch 2448 :: Batch 0/1
Batch Loss = 0.0007310406055654558
2448, epoch_train_loss=0.0007310406055654558
Epoch 2449
Epoch 2449 :: Batch 0/1
Batch Loss = 0.0007310406055465092
2449, epoch_train_loss=0.0007310406055465092
Epoch 2450
Epoch 2450 :: Batch 0/1
Batch Loss = 0.0007310406055275563
2450, epoch_train_loss=0.0007310406055275563
Epoch 2451
Epoch 2451 :: Batch 0/1
Batch Loss = 0.0007310406055085972
2451, epoch_train_loss=0.0007310406055085972
Epoch 2452
Epoch 2452 :: Batch 0/1
Batch Loss = 0.0007310406054896317
2452, epoch_train_loss=0.0007310406054896317
Epoch 2453
Epoch 2453 :: Batch 0/1
Batch Loss = 0.0007310406054706602
2453, epoch_train_loss=0.0007310406054706602
Epoch 2454
Epoch 2454 :: Batch 0/1
Batch Loss = 0.0007310406054516822
2454, epoch_train_loss=0.0007310406054516822
Epoch 2455
Epoch 2455 :: Batch 0/1
Batch Loss = 0.0007310406054326981
2455, epoch_train_loss=0.0007310406054326981
Epoch 2456
Epoch 2456 :: Batch 0/1
Batch Loss = 0.0007310406054137076
2456, epoch_train_loss=0.0007310406054137076
Epoch 2457
Epoch 2457 :: Batch 0/1
Batch Loss = 0.0007310406053947108
2457, epoch_train_loss=0.0007310406053947108
Epoch 2458
Epoch 2458 :: Batch 0/1
Batch Loss = 0.0007310406053757079
2458, epoch_train_loss=0.0007310406053757079
Epoch 2459
Epoch 2459 :: Batch 0/1
Batch Loss = 0.0007310406053566988
2459, epoch_train_loss=0.0007310406053566988
Epoch 2460
Epoch 2460 :: Batch 0/1
Batch Loss = 0.0007310406053376833
2460, epoch_train_loss=0.0007310406053376833
Epoch 2461
Epoch 2461 :: Batch 0/1
Batch Loss = 0.0007310406053186616
2461, epoch_train_loss=0.0007310406053186616
Epoch 2462
Epoch 2462 :: Batch 0/1
Batch Loss = 0.0007310406052996335
2462, epoch_train_loss=0.0007310406052996335
Epoch 2463
Epoch 2463 :: Batch 0/1
Batch Loss = 0.0007310406052805993
2463, epoch_train_loss=0.0007310406052805993
Epoch 2464
Epoch 2464 :: Batch 0/1
Batch Loss = 0.0007310406052615587
2464, epoch_train_loss=0.0007310406052615587
Epoch 2465
Epoch 2465 :: Batch 0/1
Batch Loss = 0.000731040605242512
2465, epoch_train_loss=0.000731040605242512
Epoch 2466
Epoch 2466 :: Batch 0/1
Batch Loss = 0.0007310406052234588
2466, epoch_train_loss=0.0007310406052234588
Epoch 2467
Epoch 2467 :: Batch 0/1
Batch Loss = 0.0007310406052043997
2467, epoch_train_loss=0.0007310406052043997
Epoch 2468
Epoch 2468 :: Batch 0/1
Batch Loss = 0.0007310406051853342
2468, epoch_train_loss=0.0007310406051853342
Epoch 2469
Epoch 2469 :: Batch 0/1
Batch Loss = 0.0007310406051662623
2469, epoch_train_loss=0.0007310406051662623
Epoch 2470
Epoch 2470 :: Batch 0/1
Batch Loss = 0.0007310406051471844
2470, epoch_train_loss=0.0007310406051471844
Epoch 2471
Epoch 2471 :: Batch 0/1
Batch Loss = 0.0007310406051280999
2471, epoch_train_loss=0.0007310406051280999
Epoch 2472
Epoch 2472 :: Batch 0/1
Batch Loss = 0.0007310406051090094
2472, epoch_train_loss=0.0007310406051090094
Epoch 2473
Epoch 2473 :: Batch 0/1
Batch Loss = 0.0007310406050899126
2473, epoch_train_loss=0.0007310406050899126
Epoch 2474
Epoch 2474 :: Batch 0/1
Batch Loss = 0.0007310406050708097
2474, epoch_train_loss=0.0007310406050708097
Epoch 2475
Epoch 2475 :: Batch 0/1
Batch Loss = 0.0007310406050517004
2475, epoch_train_loss=0.0007310406050517004
Epoch 2476
Epoch 2476 :: Batch 0/1
Batch Loss = 0.0007310406050325848
2476, epoch_train_loss=0.0007310406050325848
Epoch 2477
Epoch 2477 :: Batch 0/1
Batch Loss = 0.0007310406050134629
2477, epoch_train_loss=0.0007310406050134629
Epoch 2478
Epoch 2478 :: Batch 0/1
Batch Loss = 0.0007310406049943349
2478, epoch_train_loss=0.0007310406049943349
Epoch 2479
Epoch 2479 :: Batch 0/1
Batch Loss = 0.0007310406049752004
2479, epoch_train_loss=0.0007310406049752004
Epoch 2480
Epoch 2480 :: Batch 0/1
Batch Loss = 0.0007310406049560599
2480, epoch_train_loss=0.0007310406049560599
Epoch 2481
Epoch 2481 :: Batch 0/1
Batch Loss = 0.0007310406049369131
2481, epoch_train_loss=0.0007310406049369131
Epoch 2482
Epoch 2482 :: Batch 0/1
Batch Loss = 0.00073104060491776
2482, epoch_train_loss=0.00073104060491776
Epoch 2483
Epoch 2483 :: Batch 0/1
Batch Loss = 0.0007310406048986006
2483, epoch_train_loss=0.0007310406048986006
Epoch 2484
Epoch 2484 :: Batch 0/1
Batch Loss = 0.000731040604879435
2484, epoch_train_loss=0.000731040604879435
Epoch 2485
Epoch 2485 :: Batch 0/1
Batch Loss = 0.0007310406048602632
2485, epoch_train_loss=0.0007310406048602632
Epoch 2486
Epoch 2486 :: Batch 0/1
Batch Loss = 0.0007310406048410851
2486, epoch_train_loss=0.0007310406048410851
Epoch 2487
Epoch 2487 :: Batch 0/1
Batch Loss = 0.0007310406048219009
2487, epoch_train_loss=0.0007310406048219009
Epoch 2488
Epoch 2488 :: Batch 0/1
Batch Loss = 0.0007310406048027103
2488, epoch_train_loss=0.0007310406048027103
Epoch 2489
Epoch 2489 :: Batch 0/1
Batch Loss = 0.0007310406047835134
2489, epoch_train_loss=0.0007310406047835134
Epoch 2490
Epoch 2490 :: Batch 0/1
Batch Loss = 0.0007310406047643103
2490, epoch_train_loss=0.0007310406047643103
Epoch 2491
Epoch 2491 :: Batch 0/1
Batch Loss = 0.000731040604745101
2491, epoch_train_loss=0.000731040604745101
Epoch 2492
Epoch 2492 :: Batch 0/1
Batch Loss = 0.0007310406047258854
2492, epoch_train_loss=0.0007310406047258854
Epoch 2493
Epoch 2493 :: Batch 0/1
Batch Loss = 0.0007310406047066635
2493, epoch_train_loss=0.0007310406047066635
Epoch 2494
Epoch 2494 :: Batch 0/1
Batch Loss = 0.0007310406046874355
2494, epoch_train_loss=0.0007310406046874355
Epoch 2495
Epoch 2495 :: Batch 0/1
Batch Loss = 0.000731040604668201
2495, epoch_train_loss=0.000731040604668201
Epoch 2496
Epoch 2496 :: Batch 0/1
Batch Loss = 0.0007310406046489605
2496, epoch_train_loss=0.0007310406046489605
Epoch 2497
Epoch 2497 :: Batch 0/1
Batch Loss = 0.0007310406046297138
2497, epoch_train_loss=0.0007310406046297138
Epoch 2498
Epoch 2498 :: Batch 0/1
Batch Loss = 0.0007310406046104606
2498, epoch_train_loss=0.0007310406046104606
Epoch 2499
Epoch 2499 :: Batch 0/1
Batch Loss = 0.0007310406045912014
2499, epoch_train_loss=0.0007310406045912014
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting "B3LYP_WITH_VWN5 = True" in pyscf_conf.py
  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0108760> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0108760> in UKS object of <class 'pyscf.dft.uks.UKS'>
<pyscf.gto.mole.Mole object at 0x7ffeb0108760> [['P', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb01084c0> [['N', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb010aaa0> [['H', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb01092a0> [['Li', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb0109090> [['O', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb01088b0> [['Cl', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb0109720> [['Al', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb0109690> [['S', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb0109540> [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb0109b10> [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb010a140> [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb0109bd0> [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb010a590> [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb010a5c0> [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb0109fc0> [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb010a860> [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb010a770> [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb010a6e0> [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb010aa10> [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb010ac20> [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb010a9b0> [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb010aec0> [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb010aef0> [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb010ab60> [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb010b0a0> [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]] 5
<pyscf.gto.mole.Mole object at 0x7ffeb010b550> [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb010b2e0> [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]] 5
mol:  [['P', array([0., 0., 0.])]]
converged SCF energy = -341.104145992717  <S^2> = 3.7502984  2S+1 = 4.0001492
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01084c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01084c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00051022 -0.00019156 -0.00051334 ... -0.02830887 -0.02830887
 -0.02830887] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 3)
mol:  [['N', array([0., 0., 0.])]]
converged SCF energy = -54.5289742046675  <S^2> = 3.7524945  2S+1 = 4.0012471
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb010aaa0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb010aaa0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5016, 22)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-3.60081838e-04 -1.08775305e-04 -1.31917160e-05 ... -2.74817476e-02
 -2.74817476e-02 -2.74817476e-02] = ,SCAN
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5016, 22)
rho shape (4, 4, 2, 5016)
rho_filt shape: (5016,)
get descriptors tdrho.shape=(5016, 3)
mol:  [['H', array([0., 0., 0.])]]
converged SCF energy = -0.499812984008539  <S^2> = 0.75  2S+1 = 2
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01092a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01092a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 7, 7)
ao.shape (10, 2440, 7)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.92637355e-09 -1.31700807e-07 -9.61527370e-06 ... -7.42461648e-16
 -7.42461648e-16 -7.42461648e-16] = ,SCAN
get_data, dm shape = (2, 7, 7)
ao_eval.shape=(4, 2440, 7)
rho shape (4, 4, 2, 2440)
rho_filt shape: (2440,)
get descriptors tdrho.shape=(2440, 3)
mol:  [['Li', array([0., 0., 0.])]]
converged SCF energy = -7.46006188627842  <S^2> = 0.75000049  2S+1 = 2.0000005
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0109090> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0109090> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 4592, 22)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-5.31884219e-04 -2.81911891e-04 -2.81911891e-04 ... -1.27154711e-05
 -2.64861768e-02 -2.64861768e-02] = ,SCAN
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 4592, 22)
rho shape (4, 4, 2, 4592)
rho_filt shape: (4592,)
get descriptors tdrho.shape=(4592, 3)
mol:  [['O', array([0., 0., 0.])]]
converged SCF energy = -75.0033802888117  <S^2> = 2.0027444  2S+1 = 3.001829
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01088b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01088b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5040, 22)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.71974592e-04 -3.15576081e-05 -1.66682493e-06 ... -2.76158574e-02
 -2.76158574e-02 -2.76158574e-02] = ,SCAN
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5040, 22)
rho shape (4, 4, 2, 5040)
rho_filt shape: (5040,)
get descriptors tdrho.shape=(5040, 3)
mol:  [['Cl', array([0., 0., 0.])]]
converged SCF energy = -459.957577121304  <S^2> = 0.75161941  2S+1 = 2.0016188
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0109720> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0109720> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6152, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.63651212e-04 -2.52730141e-04 -8.48597695e-05 ... -2.84484392e-02
 -2.84484392e-02 -2.84484392e-02] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6152, 30)
rho shape (4, 4, 2, 6152)
rho_filt shape: (6152,)
get descriptors tdrho.shape=(6152, 3)
mol:  [['Al', array([0., 0., 0.])]]
converged SCF energy = -242.22656101461  <S^2> = 0.75226414  2S+1 = 2.0022629
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0109690> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0109690> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6088, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.33229364e-03 -1.44803374e-03 -7.31764588e-04 ... -2.61613475e-05
 -2.69586998e-04 -2.25274487e-05] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6088, 30)
rho shape (4, 4, 2, 6088)
rho_filt shape: (6088,)
get descriptors tdrho.shape=(6088, 3)
mol:  [['S', array([0., 0., 0.])]]
converged SCF energy = -397.93878682895  <S^2> = 2.0022329  2S+1 = 3.0014882
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0109540> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0109540> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00019084 -0.00019773 -0.00020105 ... -0.02838402 -0.02838402
 -0.02838402] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 3)
mol:  [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]]
converged SCF energy = -1.16580491182912  <S^2> = -4.4408921e-16  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0109b10> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0109b10> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 14, 14)
ao.shape (10, 4776, 14)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.43725660e-05 -1.02204687e-06 -4.05575842e-05 ... -2.36278434e-02
 -2.36278434e-02 -2.36278434e-02] = ,SCAN
get_data, dm shape = (2, 14, 14)
ao_eval.shape=(4, 4776, 14)
rho shape (4, 4, 2, 4776)
rho_filt shape: (4776,)
get descriptors tdrho.shape=(4776, 3)
mol:  [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]]
converged SCF energy = -109.439263799566  <S^2> = 1.5987212e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb010a140> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb010a140> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9848, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.89629699e-05 -2.76172354e-04 -7.59017288e-05 ... -7.34654212e-06
 -7.34654212e-06 -2.89629699e-05] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9848, 44)
rho shape (4, 4, 2, 9848)
rho_filt shape: (9848,)
get descriptors tdrho.shape=(9848, 3)
mol:  [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]]
converged SCF energy = -107.339357395734  <S^2> = 5.3290705e-15  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0109bd0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0109bd0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9752, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00043469 -0.00024024 -0.00035532 ... -0.00047537 -0.03728133
 -0.03728133] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9752, 44)
rho shape (4, 4, 2, 9752)
rho_filt shape: (9752,)
get descriptors tdrho.shape=(9752, 3)
mol:  [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]]
converged SCF energy = -93.3377924465132  <S^2> = 4.0073189e-10  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb010a590> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb010a590> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 12256, 51)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-9.02468888e-05 -7.92694658e-06 -9.80568469e-06 ... -4.33714150e-02
 -4.33714150e-02 -4.33714150e-02] = ,SCAN
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 12256, 51)
rho shape (4, 4, 2, 12256)
rho_filt shape: (12256,)
get descriptors tdrho.shape=(12256, 3)
mol:  [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]]
converged SCF energy = -188.456965322844  <S^2> = 2.1316282e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb010a5c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb010a5c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 66, 66)
ao.shape (10, 14920, 66)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-7.48187338e-05 -6.19475249e-05 -2.61742784e-04 ... -8.70042314e-07
 -2.73391097e-02 -2.73391097e-02] = ,SCAN
get_data, dm shape = (2, 66, 66)
ao_eval.shape=(4, 14920, 66)
rho shape (4, 4, 2, 14920)
rho_filt shape: (14920,)
get descriptors tdrho.shape=(14920, 3)
mol:  [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]]
converged SCF energy = -920.00560888896  <S^2> = 4.938272e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0109fc0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0109fc0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12208, 60)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00051559 -0.00027432 -0.00088583 ... -0.00027432 -0.04174728
 -0.04174728] = ,SCAN
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12208, 60)
rho shape (4, 4, 2, 12208)
rho_filt shape: (12208,)
get descriptors tdrho.shape=(12208, 3)
mol:  [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]]
converged SCF energy = -199.394370591172  <S^2> = 1.1723955e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb010a860> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb010a860> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9824, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-5.53951178e-05 -5.93507199e-06 -3.10072916e-04 ... -5.94325581e-02
 -5.94325581e-02 -5.94325581e-02] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9824, 44)
rho shape (4, 4, 2, 9824)
rho_filt shape: (9824,)
get descriptors tdrho.shape=(9824, 3)
mol:  [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]]
converged SCF energy = -150.214894508027  <S^2> = 1.0018599  2S+1 = 2.2377309
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb010a770> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb010a770> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9912, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.78730354e-04 -3.34007677e-05 -1.80694834e-06 ... -4.22396886e-02
 -4.22396886e-02 -4.22396886e-02] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9912, 44)
rho shape (4, 4, 2, 9912)
rho_filt shape: (9912,)
get descriptors tdrho.shape=(9912, 3)
mol:  [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]]
converged SCF energy = -77.2435048346374  <S^2> = 8.8817842e-15  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb010a6e0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb010a6e0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 58, 58)
ao.shape (10, 15208, 58)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.72190712e-05 -2.72190712e-05 -2.84904833e-04 ... -1.08108260e-05
 -1.03072478e-05 -1.03072478e-05] = ,SCAN
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15208, 58)
rho shape (4, 4, 2, 15208)
rho_filt shape: (15208,)
get descriptors tdrho.shape=(15208, 3)
mol:  [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]]
converged SCF energy = -113.221335689652  <S^2> = 6.6524564e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb010aa10> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb010aa10> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 10040, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00015688 -0.00024669 -0.00068269 ... -0.03791166 -0.03791166
 -0.03791166] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 10040, 44)
rho shape (4, 4, 2, 10040)
rho_filt shape: (10040,)
get descriptors tdrho.shape=(10040, 3)
mol:  [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]]
converged SCF energy = -460.624592374077  <S^2> = 6.2172489e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb010ac20> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb010ac20> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 37, 37)
ao.shape (10, 8552, 37)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-7.28500681e-05 -5.65091132e-06 -7.37932132e-06 ... -4.76689214e-02
 -4.76689214e-02 -4.76689214e-02] = ,SCAN
get_data, dm shape = (2, 37, 37)
ao_eval.shape=(4, 8552, 37)
rho shape (4, 4, 2, 8552)
rho_filt shape: (8552,)
get descriptors tdrho.shape=(8552, 3)
mol:  [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]]
converged SCF energy = -8.04458854018921  <S^2> = 7.9269924e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb010a9b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb010a9b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 29, 29)
ao.shape (10, 6936, 29)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.0003863  -0.00040095 -0.00040095 ... -0.0213199  -0.0213199
 -0.0213199 ] = ,SCAN
get_data, dm shape = (2, 29, 29)
ao_eval.shape=(4, 6936, 29)
rho shape (4, 4, 2, 6936)
rho_filt shape: (6936,)
get descriptors tdrho.shape=(6936, 3)
mol:  [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]]
converged SCF energy = -324.340512506578  <S^2> = 1.5855761e-11  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb010aec0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb010aec0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 11536, 60)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00088473 -0.00088473 -0.00116894 ... -0.00088473 -0.00088473
 -0.00116894] = ,SCAN
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 11536, 60)
rho shape (4, 4, 2, 11536)
rho_filt shape: (11536,)
get descriptors tdrho.shape=(11536, 3)
mol:  [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]]
converged SCF energy = -1622.57507845814  <S^2> = 8.3844043e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb010aef0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb010aef0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 120, 120)
ao.shape (10, 24512, 120)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.91408540e-05 -1.46971271e-04 -1.08734417e-03 ... -2.81566369e-02
 -2.81566369e-02 -2.81566369e-02] = ,SCAN
get_data, dm shape = (2, 120, 120)
ao_eval.shape=(4, 24512, 120)
rho shape (4, 4, 2, 24512)
rho_filt shape: (24512,)
get descriptors tdrho.shape=(24512, 3)
mol:  [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]]
converged SCF energy = -342.979728469576  <S^2> = 2.5387692e-11  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb010ab60> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb010ab60> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 13096, 51)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.39373335e-04 -1.31641332e-04 -1.15950750e-05 ... -7.32416564e-02
 -7.32416564e-02 -7.32416564e-02] = ,SCAN
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 13096, 51)
rho shape (4, 4, 2, 13096)
rho_filt shape: (13096,)
get descriptors tdrho.shape=(13096, 3)
mol:  [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]]
converged SCF energy = -578.565336187411  <S^2> = 1.0034705  2S+1 = 2.2391699
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb010b0a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb010b0a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12384, 60)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.84685156e-05 -7.80527215e-05 -7.80562600e-05 ... -2.92531397e-02
 -2.92531397e-02 -2.92531397e-02] = ,SCAN
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12384, 60)
rho shape (4, 4, 2, 12384)
rho_filt shape: (12384,)
get descriptors tdrho.shape=(12384, 3)
mol:  [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]]
converged SCF energy = -40.4598214864076  <S^2> = 3.2418512e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb010b550> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb010b550> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 50, 50)
ao.shape (10, 13936, 50)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.56165538e-04 -7.34744214e-05 -5.30574304e-06 ... -7.93995702e-06
 -7.93995702e-06 -7.93995702e-06] = ,SCAN
get_data, dm shape = (2, 50, 50)
ao_eval.shape=(4, 13936, 50)
rho shape (4, 4, 2, 13936)
rho_filt shape: (13936,)
get descriptors tdrho.shape=(13936, 3)
mol:  [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]]
converged SCF energy = -39.0756147483505  <S^2> = 6.1963767e-12  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb010b2e0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb010b2e0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 36, 36)
ao.shape (10, 9656, 36)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.28290753e-04 -1.41305449e-05 -6.13700492e-05 ... -2.47993463e-02
 -2.47993463e-02 -2.47993463e-02] = ,SCAN
get_data, dm shape = (2, 36, 36)
ao_eval.shape=(4, 9656, 36)
rho shape (4, 4, 2, 9656)
rho_filt shape: (9656,)
get descriptors tdrho.shape=(9656, 3)
mol:  [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]]
converged SCF energy = -291.719272437819  <S^2> = 1.3148593e-11  2S+1 = 1
New DM shape: (2, 58, 58)
ao.shape (10, 15256, 58)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.45512011e-04 -7.12775692e-05 -5.48666345e-06 ... -6.02613084e-06
 -6.02613084e-06 -6.02613084e-06] = ,SCAN
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15256, 58)
rho shape (4, 4, 2, 15256)
rho_filt shape: (15256,)
get descriptors tdrho.shape=(15256, 3)
PRE NAN FILT: tFxc.shape=(237011,), tdrho.shape=(237011, 3)
nan_filt_rho.shape=(237011,)
nan_filt_fxc.shape=(237011,)
tFxc.shape=(237011,), tdrho.shape=(237011, 3)
inp[0].shape = (237011, 3)
Epoch 0
Epoch 0 :: Batch 0/1
Batch Loss = 0.5581664023011771
0, epoch_train_loss=0.5581664023011771
Epoch 1
Epoch 1 :: Batch 0/1
Batch Loss = 0.40088727503472404
1, epoch_train_loss=0.40088727503472404
Epoch 2
Epoch 2 :: Batch 0/1
Batch Loss = 0.24726945542771767
2, epoch_train_loss=0.24726945542771767
Epoch 3
Epoch 3 :: Batch 0/1
Batch Loss = 0.09426160921812418
3, epoch_train_loss=0.09426160921812418
Epoch 4
Epoch 4 :: Batch 0/1
Batch Loss = 0.018300081582746198
4, epoch_train_loss=0.018300081582746198
Epoch 5
Epoch 5 :: Batch 0/1
Batch Loss = 0.002879028600311885
5, epoch_train_loss=0.002879028600311885
Epoch 6
Epoch 6 :: Batch 0/1
Batch Loss = 0.0010205214362664675
6, epoch_train_loss=0.0010205214362664675
Epoch 7
Epoch 7 :: Batch 0/1
Batch Loss = 0.0007730171494533
7, epoch_train_loss=0.0007730171494533
Epoch 8
Epoch 8 :: Batch 0/1
Batch Loss = 0.0007367251308865191
8, epoch_train_loss=0.0007367251308865191
Epoch 9
Epoch 9 :: Batch 0/1
Batch Loss = 0.0007317048863758095
9, epoch_train_loss=0.0007317048863758095
Epoch 10
Epoch 10 :: Batch 0/1
Batch Loss = 0.000731066167519583
10, epoch_train_loss=0.000731066167519583
Epoch 11
Epoch 11 :: Batch 0/1
Batch Loss = 0.0007309893068857513
11, epoch_train_loss=0.0007309893068857513
Epoch 12
Epoch 12 :: Batch 0/1
Batch Loss = 0.0007309802368578819
12, epoch_train_loss=0.0007309802368578819
Epoch 13
Epoch 13 :: Batch 0/1
Batch Loss = 0.0007309791527052411
13, epoch_train_loss=0.0007309791527052411
Epoch 14
Epoch 14 :: Batch 0/1
Batch Loss = 0.0007309790179872698
14, epoch_train_loss=0.0007309790179872698
Epoch 15
Epoch 15 :: Batch 0/1
Batch Loss = 0.0007309790002434414
15, epoch_train_loss=0.0007309790002434414
Epoch 16
Epoch 16 :: Batch 0/1
Batch Loss = 0.0007309789977321864
16, epoch_train_loss=0.0007309789977321864
Epoch 17
Epoch 17 :: Batch 0/1
Batch Loss = 0.0007309789973468262
17, epoch_train_loss=0.0007309789973468262
Epoch 18
Epoch 18 :: Batch 0/1
Batch Loss = 0.0007309789972823593
18, epoch_train_loss=0.0007309789972823593
Epoch 19
Epoch 19 :: Batch 0/1
Batch Loss = 0.000730978997270569
19, epoch_train_loss=0.000730978997270569
Epoch 20
Epoch 20 :: Batch 0/1
Batch Loss = 0.0007309789972682098
20, epoch_train_loss=0.0007309789972682098
Epoch 21
Epoch 21 :: Batch 0/1
Batch Loss = 0.0007309789972676934
21, epoch_train_loss=0.0007309789972676934
Epoch 22
Epoch 22 :: Batch 0/1
Batch Loss = 0.0007309789972675699
22, epoch_train_loss=0.0007309789972675699
Epoch 23
Epoch 23 :: Batch 0/1
Batch Loss = 0.0007309789972675379
23, epoch_train_loss=0.0007309789972675379
Epoch 24
Epoch 24 :: Batch 0/1
Batch Loss = 0.0007309789972675287
24, epoch_train_loss=0.0007309789972675287
Epoch 25
Epoch 25 :: Batch 0/1
Batch Loss = 0.000730978997267526
25, epoch_train_loss=0.000730978997267526
Epoch 26
Epoch 26 :: Batch 0/1
Batch Loss = 0.0007309789972675251
26, epoch_train_loss=0.0007309789972675251
Epoch 27
Epoch 27 :: Batch 0/1
Batch Loss = 0.0007309789972675247
27, epoch_train_loss=0.0007309789972675247
Epoch 28
Epoch 28 :: Batch 0/1
Batch Loss = 0.0007309789972675246
28, epoch_train_loss=0.0007309789972675246
Epoch 29
Epoch 29 :: Batch 0/1
Batch Loss = 0.0007309789972675246
29, epoch_train_loss=0.0007309789972675246
Epoch 30
Epoch 30 :: Batch 0/1
Batch Loss = 0.0007309789972675246
30, epoch_train_loss=0.0007309789972675246
Epoch 31
Epoch 31 :: Batch 0/1
Batch Loss = 0.0007309789972675246
31, epoch_train_loss=0.0007309789972675246
Epoch 32
Epoch 32 :: Batch 0/1
Batch Loss = 0.0007309789972675246
32, epoch_train_loss=0.0007309789972675246
Epoch 33
Epoch 33 :: Batch 0/1
Batch Loss = 0.0007309789972675246
33, epoch_train_loss=0.0007309789972675246
Epoch 34
Epoch 34 :: Batch 0/1
Batch Loss = 0.0007309789972675246
34, epoch_train_loss=0.0007309789972675246
Epoch 35
Epoch 35 :: Batch 0/1
Batch Loss = 0.0007309789972675246
35, epoch_train_loss=0.0007309789972675246
Epoch 36
Epoch 36 :: Batch 0/1
Batch Loss = 0.0007309789972675246
36, epoch_train_loss=0.0007309789972675246
Epoch 37
Epoch 37 :: Batch 0/1
Batch Loss = 0.0007309789972675246
37, epoch_train_loss=0.0007309789972675246
Epoch 38
Epoch 38 :: Batch 0/1
Batch Loss = 0.0007309789972675246
38, epoch_train_loss=0.0007309789972675246
Epoch 39
Epoch 39 :: Batch 0/1
Batch Loss = 0.0007309789972675246
39, epoch_train_loss=0.0007309789972675246
Epoch 40
Epoch 40 :: Batch 0/1
Batch Loss = 0.0007309789972675246
40, epoch_train_loss=0.0007309789972675246
Epoch 41
Epoch 41 :: Batch 0/1
Batch Loss = 0.0007309789972675246
41, epoch_train_loss=0.0007309789972675246
Epoch 42
Epoch 42 :: Batch 0/1
Batch Loss = 0.0007309789972675246
42, epoch_train_loss=0.0007309789972675246
Epoch 43
Epoch 43 :: Batch 0/1
Batch Loss = 0.0007309789972675246
43, epoch_train_loss=0.0007309789972675246
Epoch 44
Epoch 44 :: Batch 0/1
Batch Loss = 0.0007309789972675246
44, epoch_train_loss=0.0007309789972675246
Epoch 45
Epoch 45 :: Batch 0/1
Batch Loss = 0.0007309789972675246
45, epoch_train_loss=0.0007309789972675246
Epoch 46
Epoch 46 :: Batch 0/1
Batch Loss = 0.0007309789972675246
46, epoch_train_loss=0.0007309789972675246
Epoch 47
Epoch 47 :: Batch 0/1
Batch Loss = 0.0007309789972675246
47, epoch_train_loss=0.0007309789972675246
Epoch 48
Epoch 48 :: Batch 0/1
Batch Loss = 0.0007309789972675246
48, epoch_train_loss=0.0007309789972675246
Epoch 49
Epoch 49 :: Batch 0/1
Batch Loss = 0.0007309789972675246
49, epoch_train_loss=0.0007309789972675246
Epoch 50
Epoch 50 :: Batch 0/1
Batch Loss = 0.0007309789972675246
50, epoch_train_loss=0.0007309789972675246
Epoch 51
Epoch 51 :: Batch 0/1
Batch Loss = 0.0007309789972675246
51, epoch_train_loss=0.0007309789972675246
Epoch 52
Epoch 52 :: Batch 0/1
Batch Loss = 0.0007309789972675246
52, epoch_train_loss=0.0007309789972675246
Epoch 53
Epoch 53 :: Batch 0/1
Batch Loss = 0.0007309789972675246
53, epoch_train_loss=0.0007309789972675246
Epoch 54
Epoch 54 :: Batch 0/1
Batch Loss = 0.0007309789972675246
54, epoch_train_loss=0.0007309789972675246
Epoch 55
Epoch 55 :: Batch 0/1
Batch Loss = 0.0007309789972675246
55, epoch_train_loss=0.0007309789972675246
Epoch 56
Epoch 56 :: Batch 0/1
Batch Loss = 0.0007309789972675246
56, epoch_train_loss=0.0007309789972675246
Epoch 57
Epoch 57 :: Batch 0/1
Batch Loss = 0.0007309789972675246
57, epoch_train_loss=0.0007309789972675246
Epoch 58
Epoch 58 :: Batch 0/1
Batch Loss = 0.0007309789972675246
58, epoch_train_loss=0.0007309789972675246
Epoch 59
Epoch 59 :: Batch 0/1
Batch Loss = 0.0007309789972675246
59, epoch_train_loss=0.0007309789972675246
Epoch 60
Epoch 60 :: Batch 0/1
Batch Loss = 0.0007309789972675246
60, epoch_train_loss=0.0007309789972675246
Epoch 61
Epoch 61 :: Batch 0/1
Batch Loss = 0.0007309789972675246
61, epoch_train_loss=0.0007309789972675246
Epoch 62
Epoch 62 :: Batch 0/1
Batch Loss = 0.0007309789972675246
62, epoch_train_loss=0.0007309789972675246
Epoch 63
Epoch 63 :: Batch 0/1
Batch Loss = 0.0007309789972675246
63, epoch_train_loss=0.0007309789972675246
Epoch 64
Epoch 64 :: Batch 0/1
Batch Loss = 0.0007309789972675246
64, epoch_train_loss=0.0007309789972675246
Epoch 65
Epoch 65 :: Batch 0/1
Batch Loss = 0.0007309789972675246
65, epoch_train_loss=0.0007309789972675246
Epoch 66
Epoch 66 :: Batch 0/1
Batch Loss = 0.0007309789972675246
66, epoch_train_loss=0.0007309789972675246
Epoch 67
Epoch 67 :: Batch 0/1
Batch Loss = 0.0007309789972675246
67, epoch_train_loss=0.0007309789972675246
Epoch 68
Epoch 68 :: Batch 0/1
Batch Loss = 0.0007309789972675246
68, epoch_train_loss=0.0007309789972675246
Epoch 69
Epoch 69 :: Batch 0/1
Batch Loss = 0.0007309789972675246
69, epoch_train_loss=0.0007309789972675246
Epoch 70
Epoch 70 :: Batch 0/1
Batch Loss = 0.0007309789972675246
70, epoch_train_loss=0.0007309789972675246
Epoch 71
Epoch 71 :: Batch 0/1
Batch Loss = 0.0007309789972675246
71, epoch_train_loss=0.0007309789972675246
Epoch 72
Epoch 72 :: Batch 0/1
Batch Loss = 0.0007309789972675246
72, epoch_train_loss=0.0007309789972675246
Epoch 73
Epoch 73 :: Batch 0/1
Batch Loss = 0.0007309789972675246
73, epoch_train_loss=0.0007309789972675246
Epoch 74
Epoch 74 :: Batch 0/1
Batch Loss = 0.0007309789972675246
74, epoch_train_loss=0.0007309789972675246
Epoch 75
Epoch 75 :: Batch 0/1
Batch Loss = 0.0007309789972675246
75, epoch_train_loss=0.0007309789972675246
Epoch 76
Epoch 76 :: Batch 0/1
Batch Loss = 0.0007309789972675246
76, epoch_train_loss=0.0007309789972675246
Epoch 77
Epoch 77 :: Batch 0/1
Batch Loss = 0.0007309789972675246
77, epoch_train_loss=0.0007309789972675246
Epoch 78
Epoch 78 :: Batch 0/1
Batch Loss = 0.0007309789972675246
78, epoch_train_loss=0.0007309789972675246
Epoch 79
Epoch 79 :: Batch 0/1
Batch Loss = 0.0007309789972675246
79, epoch_train_loss=0.0007309789972675246
Epoch 80
Epoch 80 :: Batch 0/1
Batch Loss = 0.0007309789972675246
80, epoch_train_loss=0.0007309789972675246
Epoch 81
Epoch 81 :: Batch 0/1
Batch Loss = 0.0007309789972675246
81, epoch_train_loss=0.0007309789972675246
Epoch 82
Epoch 82 :: Batch 0/1
Batch Loss = 0.0007309789972675246
82, epoch_train_loss=0.0007309789972675246
Epoch 83
Epoch 83 :: Batch 0/1
Batch Loss = 0.0007309789972675246
83, epoch_train_loss=0.0007309789972675246
Epoch 84
Epoch 84 :: Batch 0/1
Batch Loss = 0.0007309789972675246
84, epoch_train_loss=0.0007309789972675246
Epoch 85
Epoch 85 :: Batch 0/1
Batch Loss = 0.0007309789972675246
85, epoch_train_loss=0.0007309789972675246
Epoch 86
Epoch 86 :: Batch 0/1
Batch Loss = 0.0007309789972675246
86, epoch_train_loss=0.0007309789972675246
Epoch 87
Epoch 87 :: Batch 0/1
Batch Loss = 0.0007309789972675246
87, epoch_train_loss=0.0007309789972675246
Epoch 88
Epoch 88 :: Batch 0/1
Batch Loss = 0.0007309789972675246
88, epoch_train_loss=0.0007309789972675246
Epoch 89
Epoch 89 :: Batch 0/1
Batch Loss = 0.0007309789972675246
89, epoch_train_loss=0.0007309789972675246
Epoch 90
Epoch 90 :: Batch 0/1
Batch Loss = 0.0007309789972675246
90, epoch_train_loss=0.0007309789972675246
Epoch 91
Epoch 91 :: Batch 0/1
Batch Loss = 0.0007309789972675246
91, epoch_train_loss=0.0007309789972675246
Epoch 92
Epoch 92 :: Batch 0/1
Batch Loss = 0.0007309789972675246
92, epoch_train_loss=0.0007309789972675246
Epoch 93
Epoch 93 :: Batch 0/1
Batch Loss = 0.0007309789972675246
93, epoch_train_loss=0.0007309789972675246
Epoch 94
Epoch 94 :: Batch 0/1
Batch Loss = 0.0007309789972675246
94, epoch_train_loss=0.0007309789972675246
Epoch 95
Epoch 95 :: Batch 0/1
Batch Loss = 0.0007309789972675246
95, epoch_train_loss=0.0007309789972675246
Epoch 96
Epoch 96 :: Batch 0/1
Batch Loss = 0.0007309789972675246
96, epoch_train_loss=0.0007309789972675246
Epoch 97
Epoch 97 :: Batch 0/1
Batch Loss = 0.0007309789972675246
97, epoch_train_loss=0.0007309789972675246
Epoch 98
Epoch 98 :: Batch 0/1
Batch Loss = 0.0007309789972675246
98, epoch_train_loss=0.0007309789972675246
Epoch 99
Epoch 99 :: Batch 0/1
Batch Loss = 0.0007309789972675246
99, epoch_train_loss=0.0007309789972675246
Epoch 100
Epoch 100 :: Batch 0/1
Batch Loss = 0.0007309789972675246
100, epoch_train_loss=0.0007309789972675246
Epoch 101
Epoch 101 :: Batch 0/1
Batch Loss = 0.0007309789972675246
101, epoch_train_loss=0.0007309789972675246
Epoch 102
Epoch 102 :: Batch 0/1
Batch Loss = 0.0007309789972675246
102, epoch_train_loss=0.0007309789972675246
Epoch 103
Epoch 103 :: Batch 0/1
Batch Loss = 0.0007309789972675246
103, epoch_train_loss=0.0007309789972675246
Epoch 104
Epoch 104 :: Batch 0/1
Batch Loss = 0.0007309789972675246
104, epoch_train_loss=0.0007309789972675246
Epoch 105
Epoch 105 :: Batch 0/1
Batch Loss = 0.0007309789972675246
105, epoch_train_loss=0.0007309789972675246
Epoch 106
Epoch 106 :: Batch 0/1
Batch Loss = 0.0007309789972675246
106, epoch_train_loss=0.0007309789972675246
Epoch 107
Epoch 107 :: Batch 0/1
Batch Loss = 0.0007309789972675246
107, epoch_train_loss=0.0007309789972675246
Epoch 108
Epoch 108 :: Batch 0/1
Batch Loss = 0.0007309789972675246
108, epoch_train_loss=0.0007309789972675246
Epoch 109
Epoch 109 :: Batch 0/1
Batch Loss = 0.0007309789972675246
109, epoch_train_loss=0.0007309789972675246
Epoch 110
Epoch 110 :: Batch 0/1
Batch Loss = 0.0007309789972675246
110, epoch_train_loss=0.0007309789972675246
Epoch 111
Epoch 111 :: Batch 0/1
Batch Loss = 0.0007309789972675246
111, epoch_train_loss=0.0007309789972675246
Epoch 112
Epoch 112 :: Batch 0/1
Batch Loss = 0.0007309789972675246
112, epoch_train_loss=0.0007309789972675246
Epoch 113
Epoch 113 :: Batch 0/1
Batch Loss = 0.0007309789972675246
113, epoch_train_loss=0.0007309789972675246
Epoch 114
Epoch 114 :: Batch 0/1
Batch Loss = 0.0007309789972675246
114, epoch_train_loss=0.0007309789972675246
Epoch 115
Epoch 115 :: Batch 0/1
Batch Loss = 0.0007309789972675246
115, epoch_train_loss=0.0007309789972675246
Epoch 116
Epoch 116 :: Batch 0/1
Batch Loss = 0.0007309789972675246
116, epoch_train_loss=0.0007309789972675246
Epoch 117
Epoch 117 :: Batch 0/1
Batch Loss = 0.0007309789972675246
117, epoch_train_loss=0.0007309789972675246
Epoch 118
Epoch 118 :: Batch 0/1
Batch Loss = 0.0007309789972675246
118, epoch_train_loss=0.0007309789972675246
Epoch 119
Epoch 119 :: Batch 0/1
Batch Loss = 0.0007309789972675246
119, epoch_train_loss=0.0007309789972675246
Epoch 120
Epoch 120 :: Batch 0/1
Batch Loss = 0.0007309789972675246
120, epoch_train_loss=0.0007309789972675246
Epoch 121
Epoch 121 :: Batch 0/1
Batch Loss = 0.0007309789972675246
121, epoch_train_loss=0.0007309789972675246
Epoch 122
Epoch 122 :: Batch 0/1
Batch Loss = 0.0007309789972675246
122, epoch_train_loss=0.0007309789972675246
Epoch 123
Epoch 123 :: Batch 0/1
Batch Loss = 0.0007309789972675246
123, epoch_train_loss=0.0007309789972675246
Epoch 124
Epoch 124 :: Batch 0/1
Batch Loss = 0.0007309789972675246
124, epoch_train_loss=0.0007309789972675246
Epoch 125
Epoch 125 :: Batch 0/1
Batch Loss = 0.0007309789972675246
125, epoch_train_loss=0.0007309789972675246
Epoch 126
Epoch 126 :: Batch 0/1
Batch Loss = 0.0007309789972675246
126, epoch_train_loss=0.0007309789972675246
Epoch 127
Epoch 127 :: Batch 0/1
Batch Loss = 0.0007309789972675246
127, epoch_train_loss=0.0007309789972675246
Epoch 128
Epoch 128 :: Batch 0/1
Batch Loss = 0.0007309789972675246
128, epoch_train_loss=0.0007309789972675246
Epoch 129
Epoch 129 :: Batch 0/1
Batch Loss = 0.0007309789972675246
129, epoch_train_loss=0.0007309789972675246
Epoch 130
Epoch 130 :: Batch 0/1
Batch Loss = 0.0007309789972675246
130, epoch_train_loss=0.0007309789972675246
Epoch 131
Epoch 131 :: Batch 0/1
Batch Loss = 0.0007309789972675246
131, epoch_train_loss=0.0007309789972675246
Epoch 132
Epoch 132 :: Batch 0/1
Batch Loss = 0.0007309789972675246
132, epoch_train_loss=0.0007309789972675246
Epoch 133
Epoch 133 :: Batch 0/1
Batch Loss = 0.0007309789972675246
133, epoch_train_loss=0.0007309789972675246
Epoch 134
Epoch 134 :: Batch 0/1
Batch Loss = 0.0007309789972675246
134, epoch_train_loss=0.0007309789972675246
Epoch 135
Epoch 135 :: Batch 0/1
Batch Loss = 0.0007309789972675246
135, epoch_train_loss=0.0007309789972675246
Epoch 136
Epoch 136 :: Batch 0/1
Batch Loss = 0.0007309789972675246
136, epoch_train_loss=0.0007309789972675246
Epoch 137
Epoch 137 :: Batch 0/1
Batch Loss = 0.0007309789972675246
137, epoch_train_loss=0.0007309789972675246
Epoch 138
Epoch 138 :: Batch 0/1
Batch Loss = 0.0007309789972675246
138, epoch_train_loss=0.0007309789972675246
Epoch 139
Epoch 139 :: Batch 0/1
Batch Loss = 0.0007309789972675246
139, epoch_train_loss=0.0007309789972675246
Epoch 140
Epoch 140 :: Batch 0/1
Batch Loss = 0.0007309789972675246
140, epoch_train_loss=0.0007309789972675246
Epoch 141
Epoch 141 :: Batch 0/1
Batch Loss = 0.0007309789972675246
141, epoch_train_loss=0.0007309789972675246
Epoch 142
Epoch 142 :: Batch 0/1
Batch Loss = 0.0007309789972675246
142, epoch_train_loss=0.0007309789972675246
Epoch 143
Epoch 143 :: Batch 0/1
Batch Loss = 0.0007309789972675246
143, epoch_train_loss=0.0007309789972675246
Epoch 144
Epoch 144 :: Batch 0/1
Batch Loss = 0.0007309789972675246
144, epoch_train_loss=0.0007309789972675246
Epoch 145
Epoch 145 :: Batch 0/1
Batch Loss = 0.0007309789972675246
145, epoch_train_loss=0.0007309789972675246
Epoch 146
Epoch 146 :: Batch 0/1
Batch Loss = 0.0007309789972675246
146, epoch_train_loss=0.0007309789972675246
Epoch 147
Epoch 147 :: Batch 0/1
Batch Loss = 0.0007309789972675246
147, epoch_train_loss=0.0007309789972675246
Epoch 148
Epoch 148 :: Batch 0/1
Batch Loss = 0.0007309789972675246
148, epoch_train_loss=0.0007309789972675246
Epoch 149
Epoch 149 :: Batch 0/1
Batch Loss = 0.0007309789972675246
149, epoch_train_loss=0.0007309789972675246
Epoch 150
Epoch 150 :: Batch 0/1
Batch Loss = 0.0007309789972675246
150, epoch_train_loss=0.0007309789972675246
Epoch 151
Epoch 151 :: Batch 0/1
Batch Loss = 0.0007309789972675246
151, epoch_train_loss=0.0007309789972675246
Epoch 152
Epoch 152 :: Batch 0/1
Batch Loss = 0.0007309789972675246
152, epoch_train_loss=0.0007309789972675246
Epoch 153
Epoch 153 :: Batch 0/1
Batch Loss = 0.0007309789972675246
153, epoch_train_loss=0.0007309789972675246
Epoch 154
Epoch 154 :: Batch 0/1
Batch Loss = 0.0007309789972675246
154, epoch_train_loss=0.0007309789972675246
Epoch 155
Epoch 155 :: Batch 0/1
Batch Loss = 0.0007309789972675246
155, epoch_train_loss=0.0007309789972675246
Epoch 156
Epoch 156 :: Batch 0/1
Batch Loss = 0.0007309789972675246
156, epoch_train_loss=0.0007309789972675246
Epoch 157
Epoch 157 :: Batch 0/1
Batch Loss = 0.0007309789972675246
157, epoch_train_loss=0.0007309789972675246
Epoch 158
Epoch 158 :: Batch 0/1
Batch Loss = 0.0007309789972675246
158, epoch_train_loss=0.0007309789972675246
Epoch 159
Epoch 159 :: Batch 0/1
Batch Loss = 0.0007309789972675246
159, epoch_train_loss=0.0007309789972675246
Epoch 160
Epoch 160 :: Batch 0/1
Batch Loss = 0.0007309789972675246
160, epoch_train_loss=0.0007309789972675246
Epoch 161
Epoch 161 :: Batch 0/1
Batch Loss = 0.0007309789972675246
161, epoch_train_loss=0.0007309789972675246
Epoch 162
Epoch 162 :: Batch 0/1
Batch Loss = 0.0007309789972675246
162, epoch_train_loss=0.0007309789972675246
Epoch 163
Epoch 163 :: Batch 0/1
Batch Loss = 0.0007309789972675246
163, epoch_train_loss=0.0007309789972675246
Epoch 164
Epoch 164 :: Batch 0/1
Batch Loss = 0.0007309789972675246
164, epoch_train_loss=0.0007309789972675246
Epoch 165
Epoch 165 :: Batch 0/1
Batch Loss = 0.0007309789972675246
165, epoch_train_loss=0.0007309789972675246
Epoch 166
Epoch 166 :: Batch 0/1
Batch Loss = 0.0007309789972675246
166, epoch_train_loss=0.0007309789972675246
Epoch 167
Epoch 167 :: Batch 0/1
Batch Loss = 0.0007309789972675246
167, epoch_train_loss=0.0007309789972675246
Epoch 168
Epoch 168 :: Batch 0/1
Batch Loss = 0.0007309789972675246
168, epoch_train_loss=0.0007309789972675246
Epoch 169
Epoch 169 :: Batch 0/1
Batch Loss = 0.0007309789972675246
169, epoch_train_loss=0.0007309789972675246
Epoch 170
Epoch 170 :: Batch 0/1
Batch Loss = 0.0007309789972675246
170, epoch_train_loss=0.0007309789972675246
Epoch 171
Epoch 171 :: Batch 0/1
Batch Loss = 0.0007309789972675246
171, epoch_train_loss=0.0007309789972675246
Epoch 172
Epoch 172 :: Batch 0/1
Batch Loss = 0.0007309789972675246
172, epoch_train_loss=0.0007309789972675246
Epoch 173
Epoch 173 :: Batch 0/1
Batch Loss = 0.0007309789972675246
173, epoch_train_loss=0.0007309789972675246
Epoch 174
Epoch 174 :: Batch 0/1
Batch Loss = 0.0007309789972675246
174, epoch_train_loss=0.0007309789972675246
Epoch 175
Epoch 175 :: Batch 0/1
Batch Loss = 0.0007309789972675246
175, epoch_train_loss=0.0007309789972675246
Epoch 176
Epoch 176 :: Batch 0/1
Batch Loss = 0.0007309789972675246
176, epoch_train_loss=0.0007309789972675246
Epoch 177
Epoch 177 :: Batch 0/1
Batch Loss = 0.0007309789972675246
177, epoch_train_loss=0.0007309789972675246
Epoch 178
Epoch 178 :: Batch 0/1
Batch Loss = 0.0007309789972675246
178, epoch_train_loss=0.0007309789972675246
Epoch 179
Epoch 179 :: Batch 0/1
Batch Loss = 0.0007309789972675246
179, epoch_train_loss=0.0007309789972675246
Epoch 180
Epoch 180 :: Batch 0/1
Batch Loss = 0.0007309789972675246
180, epoch_train_loss=0.0007309789972675246
Epoch 181
Epoch 181 :: Batch 0/1
Batch Loss = 0.0007309789972675246
181, epoch_train_loss=0.0007309789972675246
Epoch 182
Epoch 182 :: Batch 0/1
Batch Loss = 0.0007309789972675246
182, epoch_train_loss=0.0007309789972675246
Epoch 183
Epoch 183 :: Batch 0/1
Batch Loss = 0.0007309789972675246
183, epoch_train_loss=0.0007309789972675246
Epoch 184
Epoch 184 :: Batch 0/1
Batch Loss = 0.0007309789972675246
184, epoch_train_loss=0.0007309789972675246
Epoch 185
Epoch 185 :: Batch 0/1
Batch Loss = 0.0007309789972675246
185, epoch_train_loss=0.0007309789972675246
Epoch 186
Epoch 186 :: Batch 0/1
Batch Loss = 0.0007309789972675246
186, epoch_train_loss=0.0007309789972675246
Epoch 187
Epoch 187 :: Batch 0/1
Batch Loss = 0.0007309789972675246
187, epoch_train_loss=0.0007309789972675246
Epoch 188
Epoch 188 :: Batch 0/1
Batch Loss = 0.0007309789972675246
188, epoch_train_loss=0.0007309789972675246
Epoch 189
Epoch 189 :: Batch 0/1
Batch Loss = 0.0007309789972675246
189, epoch_train_loss=0.0007309789972675246
Epoch 190
Epoch 190 :: Batch 0/1
Batch Loss = 0.0007309789972675246
190, epoch_train_loss=0.0007309789972675246
Epoch 191
Epoch 191 :: Batch 0/1
Batch Loss = 0.0007309789972675246
191, epoch_train_loss=0.0007309789972675246
Epoch 192
Epoch 192 :: Batch 0/1
Batch Loss = 0.0007309789972675246
192, epoch_train_loss=0.0007309789972675246
Epoch 193
Epoch 193 :: Batch 0/1
Batch Loss = 0.0007309789972675246
193, epoch_train_loss=0.0007309789972675246
Epoch 194
Epoch 194 :: Batch 0/1
Batch Loss = 0.0007309789972675246
194, epoch_train_loss=0.0007309789972675246
Epoch 195
Epoch 195 :: Batch 0/1
Batch Loss = 0.0007309789972675246
195, epoch_train_loss=0.0007309789972675246
Epoch 196
Epoch 196 :: Batch 0/1
Batch Loss = 0.0007309789972675246
196, epoch_train_loss=0.0007309789972675246
Epoch 197
Epoch 197 :: Batch 0/1
Batch Loss = 0.0007309789972675246
197, epoch_train_loss=0.0007309789972675246
Epoch 198
Epoch 198 :: Batch 0/1
Batch Loss = 0.0007309789972675246
198, epoch_train_loss=0.0007309789972675246
Epoch 199
Epoch 199 :: Batch 0/1
Batch Loss = 0.0007309789972675246
199, epoch_train_loss=0.0007309789972675246
Epoch 200
Epoch 200 :: Batch 0/1
Batch Loss = 0.0007309789972675246
200, epoch_train_loss=0.0007309789972675246
Epoch 201
Epoch 201 :: Batch 0/1
Batch Loss = 0.0007309789972675246
201, epoch_train_loss=0.0007309789972675246
Epoch 202
Epoch 202 :: Batch 0/1
Batch Loss = 0.0007309789972675246
202, epoch_train_loss=0.0007309789972675246
Epoch 203
Epoch 203 :: Batch 0/1
Batch Loss = 0.0007309789972675246
203, epoch_train_loss=0.0007309789972675246
Epoch 204
Epoch 204 :: Batch 0/1
Batch Loss = 0.0007309789972675246
204, epoch_train_loss=0.0007309789972675246
Epoch 205
Epoch 205 :: Batch 0/1
Batch Loss = 0.0007309789972675246
205, epoch_train_loss=0.0007309789972675246
Epoch 206
Epoch 206 :: Batch 0/1
Batch Loss = 0.0007309789972675246
206, epoch_train_loss=0.0007309789972675246
Epoch 207
Epoch 207 :: Batch 0/1
Batch Loss = 0.0007309789972675246
207, epoch_train_loss=0.0007309789972675246
Epoch 208
Epoch 208 :: Batch 0/1
Batch Loss = 0.0007309789972675246
208, epoch_train_loss=0.0007309789972675246
Epoch 209
Epoch 209 :: Batch 0/1
Batch Loss = 0.0007309789972675246
209, epoch_train_loss=0.0007309789972675246
Epoch 210
Epoch 210 :: Batch 0/1
Batch Loss = 0.0007309789972675246
210, epoch_train_loss=0.0007309789972675246
Epoch 211
Epoch 211 :: Batch 0/1
Batch Loss = 0.0007309789972675246
211, epoch_train_loss=0.0007309789972675246
Epoch 212
Epoch 212 :: Batch 0/1
Batch Loss = 0.0007309789972675246
212, epoch_train_loss=0.0007309789972675246
Epoch 213
Epoch 213 :: Batch 0/1
Batch Loss = 0.0007309789972675246
213, epoch_train_loss=0.0007309789972675246
Epoch 214
Epoch 214 :: Batch 0/1
Batch Loss = 0.0007309789972675246
214, epoch_train_loss=0.0007309789972675246
Epoch 215
Epoch 215 :: Batch 0/1
Batch Loss = 0.0007309789972675246
215, epoch_train_loss=0.0007309789972675246
Epoch 216
Epoch 216 :: Batch 0/1
Batch Loss = 0.0007309789972675246
216, epoch_train_loss=0.0007309789972675246
Epoch 217
Epoch 217 :: Batch 0/1
Batch Loss = 0.0007309789972675246
217, epoch_train_loss=0.0007309789972675246
Epoch 218
Epoch 218 :: Batch 0/1
Batch Loss = 0.0007309789972675246
218, epoch_train_loss=0.0007309789972675246
Epoch 219
Epoch 219 :: Batch 0/1
Batch Loss = 0.0007309789972675246
219, epoch_train_loss=0.0007309789972675246
Epoch 220
Epoch 220 :: Batch 0/1
Batch Loss = 0.0007309789972675246
220, epoch_train_loss=0.0007309789972675246
Epoch 221
Epoch 221 :: Batch 0/1
Batch Loss = 0.0007309789972675246
221, epoch_train_loss=0.0007309789972675246
Epoch 222
Epoch 222 :: Batch 0/1
Batch Loss = 0.0007309789972675246
222, epoch_train_loss=0.0007309789972675246
Epoch 223
Epoch 223 :: Batch 0/1
Batch Loss = 0.0007309789972675246
223, epoch_train_loss=0.0007309789972675246
Epoch 224
Epoch 224 :: Batch 0/1
Batch Loss = 0.0007309789972675246
224, epoch_train_loss=0.0007309789972675246
Epoch 225
Epoch 225 :: Batch 0/1
Batch Loss = 0.0007309789972675246
225, epoch_train_loss=0.0007309789972675246
Epoch 226
Epoch 226 :: Batch 0/1
Batch Loss = 0.0007309789972675246
226, epoch_train_loss=0.0007309789972675246
Epoch 227
Epoch 227 :: Batch 0/1
Batch Loss = 0.0007309789972675246
227, epoch_train_loss=0.0007309789972675246
Epoch 228
Epoch 228 :: Batch 0/1
Batch Loss = 0.0007309789972675246
228, epoch_train_loss=0.0007309789972675246
Epoch 229
Epoch 229 :: Batch 0/1
Batch Loss = 0.0007309789972675246
229, epoch_train_loss=0.0007309789972675246
Epoch 230
Epoch 230 :: Batch 0/1
Batch Loss = 0.0007309789972675246
230, epoch_train_loss=0.0007309789972675246
Epoch 231
Epoch 231 :: Batch 0/1
Batch Loss = 0.0007309789972675246
231, epoch_train_loss=0.0007309789972675246
Epoch 232
Epoch 232 :: Batch 0/1
Batch Loss = 0.0007309789972675246
232, epoch_train_loss=0.0007309789972675246
Epoch 233
Epoch 233 :: Batch 0/1
Batch Loss = 0.0007309789972675246
233, epoch_train_loss=0.0007309789972675246
Epoch 234
Epoch 234 :: Batch 0/1
Batch Loss = 0.0007309789972675246
234, epoch_train_loss=0.0007309789972675246
Epoch 235
Epoch 235 :: Batch 0/1
Batch Loss = 0.0007309789972675246
235, epoch_train_loss=0.0007309789972675246
Epoch 236
Epoch 236 :: Batch 0/1
Batch Loss = 0.0007309789972675246
236, epoch_train_loss=0.0007309789972675246
Epoch 237
Epoch 237 :: Batch 0/1
Batch Loss = 0.0007309789972675246
237, epoch_train_loss=0.0007309789972675246
Epoch 238
Epoch 238 :: Batch 0/1
Batch Loss = 0.0007309789972675246
238, epoch_train_loss=0.0007309789972675246
Epoch 239
Epoch 239 :: Batch 0/1
Batch Loss = 0.0007309789972675246
239, epoch_train_loss=0.0007309789972675246
Epoch 240
Epoch 240 :: Batch 0/1
Batch Loss = 0.0007309789972675246
240, epoch_train_loss=0.0007309789972675246
Epoch 241
Epoch 241 :: Batch 0/1
Batch Loss = 0.0007309789972675246
241, epoch_train_loss=0.0007309789972675246
Epoch 242
Epoch 242 :: Batch 0/1
Batch Loss = 0.0007309789972675246
242, epoch_train_loss=0.0007309789972675246
Epoch 243
Epoch 243 :: Batch 0/1
Batch Loss = 0.0007309789972675246
243, epoch_train_loss=0.0007309789972675246
Epoch 244
Epoch 244 :: Batch 0/1
Batch Loss = 0.0007309789972675246
244, epoch_train_loss=0.0007309789972675246
Epoch 245
Epoch 245 :: Batch 0/1
Batch Loss = 0.0007309789972675246
245, epoch_train_loss=0.0007309789972675246
Epoch 246
Epoch 246 :: Batch 0/1
Batch Loss = 0.0007309789972675246
246, epoch_train_loss=0.0007309789972675246
Epoch 247
Epoch 247 :: Batch 0/1
Batch Loss = 0.0007309789972675246
247, epoch_train_loss=0.0007309789972675246
Epoch 248
Epoch 248 :: Batch 0/1
Batch Loss = 0.0007309789972675246
248, epoch_train_loss=0.0007309789972675246
Epoch 249
Epoch 249 :: Batch 0/1
Batch Loss = 0.0007309789972675246
249, epoch_train_loss=0.0007309789972675246
Epoch 250
Epoch 250 :: Batch 0/1
Batch Loss = 0.0007309789972675246
250, epoch_train_loss=0.0007309789972675246
Epoch 251
Epoch 251 :: Batch 0/1
Batch Loss = 0.0007309789972675246
251, epoch_train_loss=0.0007309789972675246
Epoch 252
Epoch 252 :: Batch 0/1
Batch Loss = 0.0007309789972675246
252, epoch_train_loss=0.0007309789972675246
Epoch 253
Epoch 253 :: Batch 0/1
Batch Loss = 0.0007309789972675246
253, epoch_train_loss=0.0007309789972675246
Epoch 254
Epoch 254 :: Batch 0/1
Batch Loss = 0.0007309789972675246
254, epoch_train_loss=0.0007309789972675246
Epoch 255
Epoch 255 :: Batch 0/1
Batch Loss = 0.0007309789972675246
255, epoch_train_loss=0.0007309789972675246
Epoch 256
Epoch 256 :: Batch 0/1
Batch Loss = 0.0007309789972675246
256, epoch_train_loss=0.0007309789972675246
Epoch 257
Epoch 257 :: Batch 0/1
Batch Loss = 0.0007309789972675246
257, epoch_train_loss=0.0007309789972675246
Epoch 258
Epoch 258 :: Batch 0/1
Batch Loss = 0.0007309789972675246
258, epoch_train_loss=0.0007309789972675246
Epoch 259
Epoch 259 :: Batch 0/1
Batch Loss = 0.0007309789972675246
259, epoch_train_loss=0.0007309789972675246
Epoch 260
Epoch 260 :: Batch 0/1
Batch Loss = 0.0007309789972675246
260, epoch_train_loss=0.0007309789972675246
Epoch 261
Epoch 261 :: Batch 0/1
Batch Loss = 0.0007309789972675246
261, epoch_train_loss=0.0007309789972675246
Epoch 262
Epoch 262 :: Batch 0/1
Batch Loss = 0.0007309789972675246
262, epoch_train_loss=0.0007309789972675246
Epoch 263
Epoch 263 :: Batch 0/1
Batch Loss = 0.0007309789972675246
263, epoch_train_loss=0.0007309789972675246
Epoch 264
Epoch 264 :: Batch 0/1
Batch Loss = 0.0007309789972675246
264, epoch_train_loss=0.0007309789972675246
Epoch 265
Epoch 265 :: Batch 0/1
Batch Loss = 0.0007309789972675246
265, epoch_train_loss=0.0007309789972675246
Epoch 266
Epoch 266 :: Batch 0/1
Batch Loss = 0.0007309789972675246
266, epoch_train_loss=0.0007309789972675246
Epoch 267
Epoch 267 :: Batch 0/1
Batch Loss = 0.0007309789972675246
267, epoch_train_loss=0.0007309789972675246
Epoch 268
Epoch 268 :: Batch 0/1
Batch Loss = 0.0007309789972675246
268, epoch_train_loss=0.0007309789972675246
Epoch 269
Epoch 269 :: Batch 0/1
Batch Loss = 0.0007309789972675246
269, epoch_train_loss=0.0007309789972675246
Epoch 270
Epoch 270 :: Batch 0/1
Batch Loss = 0.0007309789972675246
270, epoch_train_loss=0.0007309789972675246
Epoch 271
Epoch 271 :: Batch 0/1
Batch Loss = 0.0007309789972675246
271, epoch_train_loss=0.0007309789972675246
Epoch 272
Epoch 272 :: Batch 0/1
Batch Loss = 0.0007309789972675246
272, epoch_train_loss=0.0007309789972675246
Epoch 273
Epoch 273 :: Batch 0/1
Batch Loss = 0.0007309789972675246
273, epoch_train_loss=0.0007309789972675246
Epoch 274
Epoch 274 :: Batch 0/1
Batch Loss = 0.0007309789972675246
274, epoch_train_loss=0.0007309789972675246
Epoch 275
Epoch 275 :: Batch 0/1
Batch Loss = 0.0007309789972675246
275, epoch_train_loss=0.0007309789972675246
Epoch 276
Epoch 276 :: Batch 0/1
Batch Loss = 0.0007309789972675246
276, epoch_train_loss=0.0007309789972675246
Epoch 277
Epoch 277 :: Batch 0/1
Batch Loss = 0.0007309789972675246
277, epoch_train_loss=0.0007309789972675246
Epoch 278
Epoch 278 :: Batch 0/1
Batch Loss = 0.0007309789972675246
278, epoch_train_loss=0.0007309789972675246
Epoch 279
Epoch 279 :: Batch 0/1
Batch Loss = 0.0007309789972675246
279, epoch_train_loss=0.0007309789972675246
Epoch 280
Epoch 280 :: Batch 0/1
Batch Loss = 0.0007309789972675246
280, epoch_train_loss=0.0007309789972675246
Epoch 281
Epoch 281 :: Batch 0/1
Batch Loss = 0.0007309789972675246
281, epoch_train_loss=0.0007309789972675246
Epoch 282
Epoch 282 :: Batch 0/1
Batch Loss = 0.0007309789972675246
282, epoch_train_loss=0.0007309789972675246
Epoch 283
Epoch 283 :: Batch 0/1
Batch Loss = 0.0007309789972675246
283, epoch_train_loss=0.0007309789972675246
Epoch 284
Epoch 284 :: Batch 0/1
Batch Loss = 0.0007309789972675246
284, epoch_train_loss=0.0007309789972675246
Epoch 285
Epoch 285 :: Batch 0/1
Batch Loss = 0.0007309789972675246
285, epoch_train_loss=0.0007309789972675246
Epoch 286
Epoch 286 :: Batch 0/1
Batch Loss = 0.0007309789972675246
286, epoch_train_loss=0.0007309789972675246
Epoch 287
Epoch 287 :: Batch 0/1
Batch Loss = 0.0007309789972675246
287, epoch_train_loss=0.0007309789972675246
Epoch 288
Epoch 288 :: Batch 0/1
Batch Loss = 0.0007309789972675246
288, epoch_train_loss=0.0007309789972675246
Epoch 289
Epoch 289 :: Batch 0/1
Batch Loss = 0.0007309789972675246
289, epoch_train_loss=0.0007309789972675246
Epoch 290
Epoch 290 :: Batch 0/1
Batch Loss = 0.0007309789972675246
290, epoch_train_loss=0.0007309789972675246
Epoch 291
Epoch 291 :: Batch 0/1
Batch Loss = 0.0007309789972675246
291, epoch_train_loss=0.0007309789972675246
Epoch 292
Epoch 292 :: Batch 0/1
Batch Loss = 0.0007309789972675246
292, epoch_train_loss=0.0007309789972675246
Epoch 293
Epoch 293 :: Batch 0/1
Batch Loss = 0.0007309789972675246
293, epoch_train_loss=0.0007309789972675246
Epoch 294
Epoch 294 :: Batch 0/1
Batch Loss = 0.0007309789972675246
294, epoch_train_loss=0.0007309789972675246
Epoch 295
Epoch 295 :: Batch 0/1
Batch Loss = 0.0007309789972675246
295, epoch_train_loss=0.0007309789972675246
Epoch 296
Epoch 296 :: Batch 0/1
Batch Loss = 0.0007309789972675246
296, epoch_train_loss=0.0007309789972675246
Epoch 297
Epoch 297 :: Batch 0/1
Batch Loss = 0.0007309789972675246
297, epoch_train_loss=0.0007309789972675246
Epoch 298
Epoch 298 :: Batch 0/1
Batch Loss = 0.0007309789972675246
298, epoch_train_loss=0.0007309789972675246
Epoch 299
Epoch 299 :: Batch 0/1
Batch Loss = 0.0007309789972675246
299, epoch_train_loss=0.0007309789972675246
Epoch 300
Epoch 300 :: Batch 0/1
Batch Loss = 0.0007309789972675246
300, epoch_train_loss=0.0007309789972675246
Epoch 301
Epoch 301 :: Batch 0/1
Batch Loss = 0.0007309789972675246
301, epoch_train_loss=0.0007309789972675246
Epoch 302
Epoch 302 :: Batch 0/1
Batch Loss = 0.0007309789972675246
302, epoch_train_loss=0.0007309789972675246
Epoch 303
Epoch 303 :: Batch 0/1
Batch Loss = 0.0007309789972675246
303, epoch_train_loss=0.0007309789972675246
Epoch 304
Epoch 304 :: Batch 0/1
Batch Loss = 0.0007309789972675246
304, epoch_train_loss=0.0007309789972675246
Epoch 305
Epoch 305 :: Batch 0/1
Batch Loss = 0.0007309789972675246
305, epoch_train_loss=0.0007309789972675246
Epoch 306
Epoch 306 :: Batch 0/1
Batch Loss = 0.0007309789972675246
306, epoch_train_loss=0.0007309789972675246
Epoch 307
Epoch 307 :: Batch 0/1
Batch Loss = 0.0007309789972675246
307, epoch_train_loss=0.0007309789972675246
Epoch 308
Epoch 308 :: Batch 0/1
Batch Loss = 0.0007309789972675246
308, epoch_train_loss=0.0007309789972675246
Epoch 309
Epoch 309 :: Batch 0/1
Batch Loss = 0.0007309789972675246
309, epoch_train_loss=0.0007309789972675246
Epoch 310
Epoch 310 :: Batch 0/1
Batch Loss = 0.0007309789972675246
310, epoch_train_loss=0.0007309789972675246
Epoch 311
Epoch 311 :: Batch 0/1
Batch Loss = 0.0007309789972675246
311, epoch_train_loss=0.0007309789972675246
Epoch 312
Epoch 312 :: Batch 0/1
Batch Loss = 0.0007309789972675246
312, epoch_train_loss=0.0007309789972675246
Epoch 313
Epoch 313 :: Batch 0/1
Batch Loss = 0.0007309789972675246
313, epoch_train_loss=0.0007309789972675246
Epoch 314
Epoch 314 :: Batch 0/1
Batch Loss = 0.0007309789972675246
314, epoch_train_loss=0.0007309789972675246
Epoch 315
Epoch 315 :: Batch 0/1
Batch Loss = 0.0007309789972675246
315, epoch_train_loss=0.0007309789972675246
Epoch 316
Epoch 316 :: Batch 0/1
Batch Loss = 0.0007309789972675246
316, epoch_train_loss=0.0007309789972675246
Epoch 317
Epoch 317 :: Batch 0/1
Batch Loss = 0.0007309789972675246
317, epoch_train_loss=0.0007309789972675246
Epoch 318
Epoch 318 :: Batch 0/1
Batch Loss = 0.0007309789972675246
318, epoch_train_loss=0.0007309789972675246
Epoch 319
Epoch 319 :: Batch 0/1
Batch Loss = 0.0007309789972675246
319, epoch_train_loss=0.0007309789972675246
Epoch 320
Epoch 320 :: Batch 0/1
Batch Loss = 0.0007309789972675246
320, epoch_train_loss=0.0007309789972675246
Epoch 321
Epoch 321 :: Batch 0/1
Batch Loss = 0.0007309789972675246
321, epoch_train_loss=0.0007309789972675246
Epoch 322
Epoch 322 :: Batch 0/1
Batch Loss = 0.0007309789972675246
322, epoch_train_loss=0.0007309789972675246
Epoch 323
Epoch 323 :: Batch 0/1
Batch Loss = 0.0007309789972675246
323, epoch_train_loss=0.0007309789972675246
Epoch 324
Epoch 324 :: Batch 0/1
Batch Loss = 0.0007309789972675246
324, epoch_train_loss=0.0007309789972675246
Epoch 325
Epoch 325 :: Batch 0/1
Batch Loss = 0.0007309789972675246
325, epoch_train_loss=0.0007309789972675246
Epoch 326
Epoch 326 :: Batch 0/1
Batch Loss = 0.0007309789972675246
326, epoch_train_loss=0.0007309789972675246
Epoch 327
Epoch 327 :: Batch 0/1
Batch Loss = 0.0007309789972675246
327, epoch_train_loss=0.0007309789972675246
Epoch 328
Epoch 328 :: Batch 0/1
Batch Loss = 0.0007309789972675246
328, epoch_train_loss=0.0007309789972675246
Epoch 329
Epoch 329 :: Batch 0/1
Batch Loss = 0.0007309789972675246
329, epoch_train_loss=0.0007309789972675246
Epoch 330
Epoch 330 :: Batch 0/1
Batch Loss = 0.0007309789972675246
330, epoch_train_loss=0.0007309789972675246
Epoch 331
Epoch 331 :: Batch 0/1
Batch Loss = 0.0007309789972675246
331, epoch_train_loss=0.0007309789972675246
Epoch 332
Epoch 332 :: Batch 0/1
Batch Loss = 0.0007309789972675246
332, epoch_train_loss=0.0007309789972675246
Epoch 333
Epoch 333 :: Batch 0/1
Batch Loss = 0.0007309789972675246
333, epoch_train_loss=0.0007309789972675246
Epoch 334
Epoch 334 :: Batch 0/1
Batch Loss = 0.0007309789972675246
334, epoch_train_loss=0.0007309789972675246
Epoch 335
Epoch 335 :: Batch 0/1
Batch Loss = 0.0007309789972675246
335, epoch_train_loss=0.0007309789972675246
Epoch 336
Epoch 336 :: Batch 0/1
Batch Loss = 0.0007309789972675246
336, epoch_train_loss=0.0007309789972675246
Epoch 337
Epoch 337 :: Batch 0/1
Batch Loss = 0.0007309789972675246
337, epoch_train_loss=0.0007309789972675246
Epoch 338
Epoch 338 :: Batch 0/1
Batch Loss = 0.0007309789972675246
338, epoch_train_loss=0.0007309789972675246
Epoch 339
Epoch 339 :: Batch 0/1
Batch Loss = 0.0007309789972675246
339, epoch_train_loss=0.0007309789972675246
Epoch 340
Epoch 340 :: Batch 0/1
Batch Loss = 0.0007309789972675246
340, epoch_train_loss=0.0007309789972675246
Epoch 341
Epoch 341 :: Batch 0/1
Batch Loss = 0.0007309789972675246
341, epoch_train_loss=0.0007309789972675246
Epoch 342
Epoch 342 :: Batch 0/1
Batch Loss = 0.0007309789972675246
342, epoch_train_loss=0.0007309789972675246
Epoch 343
Epoch 343 :: Batch 0/1
Batch Loss = 0.0007309789972675246
343, epoch_train_loss=0.0007309789972675246
Epoch 344
Epoch 344 :: Batch 0/1
Batch Loss = 0.0007309789972675246
344, epoch_train_loss=0.0007309789972675246
Epoch 345
Epoch 345 :: Batch 0/1
Batch Loss = 0.0007309789972675246
345, epoch_train_loss=0.0007309789972675246
Epoch 346
Epoch 346 :: Batch 0/1
Batch Loss = 0.0007309789972675246
346, epoch_train_loss=0.0007309789972675246
Epoch 347
Epoch 347 :: Batch 0/1
Batch Loss = 0.0007309789972675246
347, epoch_train_loss=0.0007309789972675246
Epoch 348
Epoch 348 :: Batch 0/1
Batch Loss = 0.0007309789972675246
348, epoch_train_loss=0.0007309789972675246
Epoch 349
Epoch 349 :: Batch 0/1
Batch Loss = 0.0007309789972675246
349, epoch_train_loss=0.0007309789972675246
Epoch 350
Epoch 350 :: Batch 0/1
Batch Loss = 0.0007309789972675246
350, epoch_train_loss=0.0007309789972675246
Epoch 351
Epoch 351 :: Batch 0/1
Batch Loss = 0.0007309789972675246
351, epoch_train_loss=0.0007309789972675246
Epoch 352
Epoch 352 :: Batch 0/1
Batch Loss = 0.0007309789972675246
352, epoch_train_loss=0.0007309789972675246
Epoch 353
Epoch 353 :: Batch 0/1
Batch Loss = 0.0007309789972675246
353, epoch_train_loss=0.0007309789972675246
Epoch 354
Epoch 354 :: Batch 0/1
Batch Loss = 0.0007309789972675246
354, epoch_train_loss=0.0007309789972675246
Epoch 355
Epoch 355 :: Batch 0/1
Batch Loss = 0.0007309789972675246
355, epoch_train_loss=0.0007309789972675246
Epoch 356
Epoch 356 :: Batch 0/1
Batch Loss = 0.0007309789972675246
356, epoch_train_loss=0.0007309789972675246
Epoch 357
Epoch 357 :: Batch 0/1
Batch Loss = 0.0007309789972675246
357, epoch_train_loss=0.0007309789972675246
Epoch 358
Epoch 358 :: Batch 0/1
Batch Loss = 0.0007309789972675246
358, epoch_train_loss=0.0007309789972675246
Epoch 359
Epoch 359 :: Batch 0/1
Batch Loss = 0.0007309789972675246
359, epoch_train_loss=0.0007309789972675246
Epoch 360
Epoch 360 :: Batch 0/1
Batch Loss = 0.0007309789972675246
360, epoch_train_loss=0.0007309789972675246
Epoch 361
Epoch 361 :: Batch 0/1
Batch Loss = 0.0007309789972675246
361, epoch_train_loss=0.0007309789972675246
Epoch 362
Epoch 362 :: Batch 0/1
Batch Loss = 0.0007309789972675246
362, epoch_train_loss=0.0007309789972675246
Epoch 363
Epoch 363 :: Batch 0/1
Batch Loss = 0.0007309789972675246
363, epoch_train_loss=0.0007309789972675246
Epoch 364
Epoch 364 :: Batch 0/1
Batch Loss = 0.0007309789972675246
364, epoch_train_loss=0.0007309789972675246
Epoch 365
Epoch 365 :: Batch 0/1
Batch Loss = 0.0007309789972675246
365, epoch_train_loss=0.0007309789972675246
Epoch 366
Epoch 366 :: Batch 0/1
Batch Loss = 0.0007309789972675246
366, epoch_train_loss=0.0007309789972675246
Epoch 367
Epoch 367 :: Batch 0/1
Batch Loss = 0.0007309789972675246
367, epoch_train_loss=0.0007309789972675246
Epoch 368
Epoch 368 :: Batch 0/1
Batch Loss = 0.0007309789972675246
368, epoch_train_loss=0.0007309789972675246
Epoch 369
Epoch 369 :: Batch 0/1
Batch Loss = 0.0007309789972675246
369, epoch_train_loss=0.0007309789972675246
Epoch 370
Epoch 370 :: Batch 0/1
Batch Loss = 0.0007309789972675246
370, epoch_train_loss=0.0007309789972675246
Epoch 371
Epoch 371 :: Batch 0/1
Batch Loss = 0.0007309789972675246
371, epoch_train_loss=0.0007309789972675246
Epoch 372
Epoch 372 :: Batch 0/1
Batch Loss = 0.0007309789972675246
372, epoch_train_loss=0.0007309789972675246
Epoch 373
Epoch 373 :: Batch 0/1
Batch Loss = 0.0007309789972675246
373, epoch_train_loss=0.0007309789972675246
Epoch 374
Epoch 374 :: Batch 0/1
Batch Loss = 0.0007309789972675246
374, epoch_train_loss=0.0007309789972675246
Epoch 375
Epoch 375 :: Batch 0/1
Batch Loss = 0.0007309789972675246
375, epoch_train_loss=0.0007309789972675246
Epoch 376
Epoch 376 :: Batch 0/1
Batch Loss = 0.0007309789972675246
376, epoch_train_loss=0.0007309789972675246
Epoch 377
Epoch 377 :: Batch 0/1
Batch Loss = 0.0007309789972675246
377, epoch_train_loss=0.0007309789972675246
Epoch 378
Epoch 378 :: Batch 0/1
Batch Loss = 0.0007309789972675246
378, epoch_train_loss=0.0007309789972675246
Epoch 379
Epoch 379 :: Batch 0/1
Batch Loss = 0.0007309789972675246
379, epoch_train_loss=0.0007309789972675246
Epoch 380
Epoch 380 :: Batch 0/1
Batch Loss = 0.0007309789972675246
380, epoch_train_loss=0.0007309789972675246
Epoch 381
Epoch 381 :: Batch 0/1
Batch Loss = 0.0007309789972675246
381, epoch_train_loss=0.0007309789972675246
Epoch 382
Epoch 382 :: Batch 0/1
Batch Loss = 0.0007309789972675246
382, epoch_train_loss=0.0007309789972675246
Epoch 383
Epoch 383 :: Batch 0/1
Batch Loss = 0.0007309789972675246
383, epoch_train_loss=0.0007309789972675246
Epoch 384
Epoch 384 :: Batch 0/1
Batch Loss = 0.0007309789972675246
384, epoch_train_loss=0.0007309789972675246
Epoch 385
Epoch 385 :: Batch 0/1
Batch Loss = 0.0007309789972675246
385, epoch_train_loss=0.0007309789972675246
Epoch 386
Epoch 386 :: Batch 0/1
Batch Loss = 0.0007309789972675246
386, epoch_train_loss=0.0007309789972675246
Epoch 387
Epoch 387 :: Batch 0/1
Batch Loss = 0.0007309789972675246
387, epoch_train_loss=0.0007309789972675246
Epoch 388
Epoch 388 :: Batch 0/1
Batch Loss = 0.0007309789972675246
388, epoch_train_loss=0.0007309789972675246
Epoch 389
Epoch 389 :: Batch 0/1
Batch Loss = 0.0007309789972675246
389, epoch_train_loss=0.0007309789972675246
Epoch 390
Epoch 390 :: Batch 0/1
Batch Loss = 0.0007309789972675246
390, epoch_train_loss=0.0007309789972675246
Epoch 391
Epoch 391 :: Batch 0/1
Batch Loss = 0.0007309789972675246
391, epoch_train_loss=0.0007309789972675246
Epoch 392
Epoch 392 :: Batch 0/1
Batch Loss = 0.0007309789972675246
392, epoch_train_loss=0.0007309789972675246
Epoch 393
Epoch 393 :: Batch 0/1
Batch Loss = 0.0007309789972675246
393, epoch_train_loss=0.0007309789972675246
Epoch 394
Epoch 394 :: Batch 0/1
Batch Loss = 0.0007309789972675246
394, epoch_train_loss=0.0007309789972675246
Epoch 395
Epoch 395 :: Batch 0/1
Batch Loss = 0.0007309789972675246
395, epoch_train_loss=0.0007309789972675246
Epoch 396
Epoch 396 :: Batch 0/1
Batch Loss = 0.0007309789972675246
396, epoch_train_loss=0.0007309789972675246
Epoch 397
Epoch 397 :: Batch 0/1
Batch Loss = 0.0007309789972675246
397, epoch_train_loss=0.0007309789972675246
Epoch 398
Epoch 398 :: Batch 0/1
Batch Loss = 0.0007309789972675246
398, epoch_train_loss=0.0007309789972675246
Epoch 399
Epoch 399 :: Batch 0/1
Batch Loss = 0.0007309789972675246
399, epoch_train_loss=0.0007309789972675246
Epoch 400
Epoch 400 :: Batch 0/1
Batch Loss = 0.0007309789972675246
400, epoch_train_loss=0.0007309789972675246
Epoch 401
Epoch 401 :: Batch 0/1
Batch Loss = 0.0007309789972675246
401, epoch_train_loss=0.0007309789972675246
Epoch 402
Epoch 402 :: Batch 0/1
Batch Loss = 0.0007309789972675246
402, epoch_train_loss=0.0007309789972675246
Epoch 403
Epoch 403 :: Batch 0/1
Batch Loss = 0.0007309789972675246
403, epoch_train_loss=0.0007309789972675246
Epoch 404
Epoch 404 :: Batch 0/1
Batch Loss = 0.0007309789972675246
404, epoch_train_loss=0.0007309789972675246
Epoch 405
Epoch 405 :: Batch 0/1
Batch Loss = 0.0007309789972675246
405, epoch_train_loss=0.0007309789972675246
Epoch 406
Epoch 406 :: Batch 0/1
Batch Loss = 0.0007309789972675246
406, epoch_train_loss=0.0007309789972675246
Epoch 407
Epoch 407 :: Batch 0/1
Batch Loss = 0.0007309789972675246
407, epoch_train_loss=0.0007309789972675246
Epoch 408
Epoch 408 :: Batch 0/1
Batch Loss = 0.0007309789972675246
408, epoch_train_loss=0.0007309789972675246
Epoch 409
Epoch 409 :: Batch 0/1
Batch Loss = 0.0007309789972675246
409, epoch_train_loss=0.0007309789972675246
Epoch 410
Epoch 410 :: Batch 0/1
Batch Loss = 0.0007309789972675246
410, epoch_train_loss=0.0007309789972675246
Epoch 411
Epoch 411 :: Batch 0/1
Batch Loss = 0.0007309789972675246
411, epoch_train_loss=0.0007309789972675246
Epoch 412
Epoch 412 :: Batch 0/1
Batch Loss = 0.0007309789972675246
412, epoch_train_loss=0.0007309789972675246
Epoch 413
Epoch 413 :: Batch 0/1
Batch Loss = 0.0007309789972675246
413, epoch_train_loss=0.0007309789972675246
Epoch 414
Epoch 414 :: Batch 0/1
Batch Loss = 0.0007309789972675246
414, epoch_train_loss=0.0007309789972675246
Epoch 415
Epoch 415 :: Batch 0/1
Batch Loss = 0.0007309789972675246
415, epoch_train_loss=0.0007309789972675246
Epoch 416
Epoch 416 :: Batch 0/1
Batch Loss = 0.0007309789972675246
416, epoch_train_loss=0.0007309789972675246
Epoch 417
Epoch 417 :: Batch 0/1
Batch Loss = 0.0007309789972675246
417, epoch_train_loss=0.0007309789972675246
Epoch 418
Epoch 418 :: Batch 0/1
Batch Loss = 0.0007309789972675246
418, epoch_train_loss=0.0007309789972675246
Epoch 419
Epoch 419 :: Batch 0/1
Batch Loss = 0.0007309789972675246
419, epoch_train_loss=0.0007309789972675246
Epoch 420
Epoch 420 :: Batch 0/1
Batch Loss = 0.0007309789972675246
420, epoch_train_loss=0.0007309789972675246
Epoch 421
Epoch 421 :: Batch 0/1
Batch Loss = 0.0007309789972675246
421, epoch_train_loss=0.0007309789972675246
Epoch 422
Epoch 422 :: Batch 0/1
Batch Loss = 0.0007309789972675246
422, epoch_train_loss=0.0007309789972675246
Epoch 423
Epoch 423 :: Batch 0/1
Batch Loss = 0.0007309789972675246
423, epoch_train_loss=0.0007309789972675246
Epoch 424
Epoch 424 :: Batch 0/1
Batch Loss = 0.0007309789972675246
424, epoch_train_loss=0.0007309789972675246
Epoch 425
Epoch 425 :: Batch 0/1
Batch Loss = 0.0007309789972675246
425, epoch_train_loss=0.0007309789972675246
Epoch 426
Epoch 426 :: Batch 0/1
Batch Loss = 0.0007309789972675246
426, epoch_train_loss=0.0007309789972675246
Epoch 427
Epoch 427 :: Batch 0/1
Batch Loss = 0.0007309789972675246
427, epoch_train_loss=0.0007309789972675246
Epoch 428
Epoch 428 :: Batch 0/1
Batch Loss = 0.0007309789972675246
428, epoch_train_loss=0.0007309789972675246
Epoch 429
Epoch 429 :: Batch 0/1
Batch Loss = 0.0007309789972675246
429, epoch_train_loss=0.0007309789972675246
Epoch 430
Epoch 430 :: Batch 0/1
Batch Loss = 0.0007309789972675246
430, epoch_train_loss=0.0007309789972675246
Epoch 431
Epoch 431 :: Batch 0/1
Batch Loss = 0.0007309789972675246
431, epoch_train_loss=0.0007309789972675246
Epoch 432
Epoch 432 :: Batch 0/1
Batch Loss = 0.0007309789972675246
432, epoch_train_loss=0.0007309789972675246
Epoch 433
Epoch 433 :: Batch 0/1
Batch Loss = 0.0007309789972675246
433, epoch_train_loss=0.0007309789972675246
Epoch 434
Epoch 434 :: Batch 0/1
Batch Loss = 0.0007309789972675246
434, epoch_train_loss=0.0007309789972675246
Epoch 435
Epoch 435 :: Batch 0/1
Batch Loss = 0.0007309789972675246
435, epoch_train_loss=0.0007309789972675246
Epoch 436
Epoch 436 :: Batch 0/1
Batch Loss = 0.0007309789972675246
436, epoch_train_loss=0.0007309789972675246
Epoch 437
Epoch 437 :: Batch 0/1
Batch Loss = 0.0007309789972675246
437, epoch_train_loss=0.0007309789972675246
Epoch 438
Epoch 438 :: Batch 0/1
Batch Loss = 0.0007309789972675246
438, epoch_train_loss=0.0007309789972675246
Epoch 439
Epoch 439 :: Batch 0/1
Batch Loss = 0.0007309789972675246
439, epoch_train_loss=0.0007309789972675246
Epoch 440
Epoch 440 :: Batch 0/1
Batch Loss = 0.0007309789972675246
440, epoch_train_loss=0.0007309789972675246
Epoch 441
Epoch 441 :: Batch 0/1
Batch Loss = 0.0007309789972675246
441, epoch_train_loss=0.0007309789972675246
Epoch 442
Epoch 442 :: Batch 0/1
Batch Loss = 0.0007309789972675246
442, epoch_train_loss=0.0007309789972675246
Epoch 443
Epoch 443 :: Batch 0/1
Batch Loss = 0.0007309789972675246
443, epoch_train_loss=0.0007309789972675246
Epoch 444
Epoch 444 :: Batch 0/1
Batch Loss = 0.0007309789972675246
444, epoch_train_loss=0.0007309789972675246
Epoch 445
Epoch 445 :: Batch 0/1
Batch Loss = 0.0007309789972675246
445, epoch_train_loss=0.0007309789972675246
Epoch 446
Epoch 446 :: Batch 0/1
Batch Loss = 0.0007309789972675246
446, epoch_train_loss=0.0007309789972675246
Epoch 447
Epoch 447 :: Batch 0/1
Batch Loss = 0.0007309789972675246
447, epoch_train_loss=0.0007309789972675246
Epoch 448
Epoch 448 :: Batch 0/1
Batch Loss = 0.0007309789972675246
448, epoch_train_loss=0.0007309789972675246
Epoch 449
Epoch 449 :: Batch 0/1
Batch Loss = 0.0007309789972675246
449, epoch_train_loss=0.0007309789972675246
Epoch 450
Epoch 450 :: Batch 0/1
Batch Loss = 0.0007309789972675246
450, epoch_train_loss=0.0007309789972675246
Epoch 451
Epoch 451 :: Batch 0/1
Batch Loss = 0.0007309789972675246
451, epoch_train_loss=0.0007309789972675246
Epoch 452
Epoch 452 :: Batch 0/1
Batch Loss = 0.0007309789972675246
452, epoch_train_loss=0.0007309789972675246
Epoch 453
Epoch 453 :: Batch 0/1
Batch Loss = 0.0007309789972675246
453, epoch_train_loss=0.0007309789972675246
Epoch 454
Epoch 454 :: Batch 0/1
Batch Loss = 0.0007309789972675246
454, epoch_train_loss=0.0007309789972675246
Epoch 455
Epoch 455 :: Batch 0/1
Batch Loss = 0.0007309789972675246
455, epoch_train_loss=0.0007309789972675246
Epoch 456
Epoch 456 :: Batch 0/1
Batch Loss = 0.0007309789972675246
456, epoch_train_loss=0.0007309789972675246
Epoch 457
Epoch 457 :: Batch 0/1
Batch Loss = 0.0007309789972675246
457, epoch_train_loss=0.0007309789972675246
Epoch 458
Epoch 458 :: Batch 0/1
Batch Loss = 0.0007309789972675246
458, epoch_train_loss=0.0007309789972675246
Epoch 459
Epoch 459 :: Batch 0/1
Batch Loss = 0.0007309789972675246
459, epoch_train_loss=0.0007309789972675246
Epoch 460
Epoch 460 :: Batch 0/1
Batch Loss = 0.0007309789972675246
460, epoch_train_loss=0.0007309789972675246
Epoch 461
Epoch 461 :: Batch 0/1
Batch Loss = 0.0007309789972675246
461, epoch_train_loss=0.0007309789972675246
Epoch 462
Epoch 462 :: Batch 0/1
Batch Loss = 0.0007309789972675246
462, epoch_train_loss=0.0007309789972675246
Epoch 463
Epoch 463 :: Batch 0/1
Batch Loss = 0.0007309789972675246
463, epoch_train_loss=0.0007309789972675246
Epoch 464
Epoch 464 :: Batch 0/1
Batch Loss = 0.0007309789972675246
464, epoch_train_loss=0.0007309789972675246
Epoch 465
Epoch 465 :: Batch 0/1
Batch Loss = 0.0007309789972675246
465, epoch_train_loss=0.0007309789972675246
Epoch 466
Epoch 466 :: Batch 0/1
Batch Loss = 0.0007309789972675246
466, epoch_train_loss=0.0007309789972675246
Epoch 467
Epoch 467 :: Batch 0/1
Batch Loss = 0.0007309789972675246
467, epoch_train_loss=0.0007309789972675246
Epoch 468
Epoch 468 :: Batch 0/1
Batch Loss = 0.0007309789972675246
468, epoch_train_loss=0.0007309789972675246
Epoch 469
Epoch 469 :: Batch 0/1
Batch Loss = 0.0007309789972675246
469, epoch_train_loss=0.0007309789972675246
Epoch 470
Epoch 470 :: Batch 0/1
Batch Loss = 0.0007309789972675246
470, epoch_train_loss=0.0007309789972675246
Epoch 471
Epoch 471 :: Batch 0/1
Batch Loss = 0.0007309789972675246
471, epoch_train_loss=0.0007309789972675246
Epoch 472
Epoch 472 :: Batch 0/1
Batch Loss = 0.0007309789972675246
472, epoch_train_loss=0.0007309789972675246
Epoch 473
Epoch 473 :: Batch 0/1
Batch Loss = 0.0007309789972675246
473, epoch_train_loss=0.0007309789972675246
Epoch 474
Epoch 474 :: Batch 0/1
Batch Loss = 0.0007309789972675246
474, epoch_train_loss=0.0007309789972675246
Epoch 475
Epoch 475 :: Batch 0/1
Batch Loss = 0.0007309789972675246
475, epoch_train_loss=0.0007309789972675246
Epoch 476
Epoch 476 :: Batch 0/1
Batch Loss = 0.0007309789972675246
476, epoch_train_loss=0.0007309789972675246
Epoch 477
Epoch 477 :: Batch 0/1
Batch Loss = 0.0007309789972675246
477, epoch_train_loss=0.0007309789972675246
Epoch 478
Epoch 478 :: Batch 0/1
Batch Loss = 0.0007309789972675246
478, epoch_train_loss=0.0007309789972675246
Epoch 479
Epoch 479 :: Batch 0/1
Batch Loss = 0.0007309789972675246
479, epoch_train_loss=0.0007309789972675246
Epoch 480
Epoch 480 :: Batch 0/1
Batch Loss = 0.0007309789972675246
480, epoch_train_loss=0.0007309789972675246
Epoch 481
Epoch 481 :: Batch 0/1
Batch Loss = 0.0007309789972675246
481, epoch_train_loss=0.0007309789972675246
Epoch 482
Epoch 482 :: Batch 0/1
Batch Loss = 0.0007309789972675246
482, epoch_train_loss=0.0007309789972675246
Epoch 483
Epoch 483 :: Batch 0/1
Batch Loss = 0.0007309789972675246
483, epoch_train_loss=0.0007309789972675246
Epoch 484
Epoch 484 :: Batch 0/1
Batch Loss = 0.0007309789972675246
484, epoch_train_loss=0.0007309789972675246
Epoch 485
Epoch 485 :: Batch 0/1
Batch Loss = 0.0007309789972675246
485, epoch_train_loss=0.0007309789972675246
Epoch 486
Epoch 486 :: Batch 0/1
Batch Loss = 0.0007309789972675246
486, epoch_train_loss=0.0007309789972675246
Epoch 487
Epoch 487 :: Batch 0/1
Batch Loss = 0.0007309789972675246
487, epoch_train_loss=0.0007309789972675246
Epoch 488
Epoch 488 :: Batch 0/1
Batch Loss = 0.0007309789972675246
488, epoch_train_loss=0.0007309789972675246
Epoch 489
Epoch 489 :: Batch 0/1
Batch Loss = 0.0007309789972675246
489, epoch_train_loss=0.0007309789972675246
Epoch 490
Epoch 490 :: Batch 0/1
Batch Loss = 0.0007309789972675246
490, epoch_train_loss=0.0007309789972675246
Epoch 491
Epoch 491 :: Batch 0/1
Batch Loss = 0.0007309789972675246
491, epoch_train_loss=0.0007309789972675246
Epoch 492
Epoch 492 :: Batch 0/1
Batch Loss = 0.0007309789972675246
492, epoch_train_loss=0.0007309789972675246
Epoch 493
Epoch 493 :: Batch 0/1
Batch Loss = 0.0007309789972675246
493, epoch_train_loss=0.0007309789972675246
Epoch 494
Epoch 494 :: Batch 0/1
Batch Loss = 0.0007309789972675246
494, epoch_train_loss=0.0007309789972675246
Epoch 495
Epoch 495 :: Batch 0/1
Batch Loss = 0.0007309789972675246
495, epoch_train_loss=0.0007309789972675246
Epoch 496
Epoch 496 :: Batch 0/1
Batch Loss = 0.0007309789972675246
496, epoch_train_loss=0.0007309789972675246
Epoch 497
Epoch 497 :: Batch 0/1
Batch Loss = 0.0007309789972675246
497, epoch_train_loss=0.0007309789972675246
Epoch 498
Epoch 498 :: Batch 0/1
Batch Loss = 0.0007309789972675246
498, epoch_train_loss=0.0007309789972675246
Epoch 499
Epoch 499 :: Batch 0/1
Batch Loss = 0.0007309789972675246
499, epoch_train_loss=0.0007309789972675246
Epoch 500
Epoch 500 :: Batch 0/1
Batch Loss = 0.0007309789972675246
500, epoch_train_loss=0.0007309789972675246
Epoch 501
Epoch 501 :: Batch 0/1
Batch Loss = 0.0007309789972675246
501, epoch_train_loss=0.0007309789972675246
Epoch 502
Epoch 502 :: Batch 0/1
Batch Loss = 0.0007309789972675246
502, epoch_train_loss=0.0007309789972675246
Epoch 503
Epoch 503 :: Batch 0/1
Batch Loss = 0.0007309789972675246
503, epoch_train_loss=0.0007309789972675246
Epoch 504
Epoch 504 :: Batch 0/1
Batch Loss = 0.0007309789972675246
504, epoch_train_loss=0.0007309789972675246
Epoch 505
Epoch 505 :: Batch 0/1
Batch Loss = 0.0007309789972675246
505, epoch_train_loss=0.0007309789972675246
Epoch 506
Epoch 506 :: Batch 0/1
Batch Loss = 0.0007309789972675246
506, epoch_train_loss=0.0007309789972675246
Epoch 507
Epoch 507 :: Batch 0/1
Batch Loss = 0.0007309789972675246
507, epoch_train_loss=0.0007309789972675246
Epoch 508
Epoch 508 :: Batch 0/1
Batch Loss = 0.0007309789972675246
508, epoch_train_loss=0.0007309789972675246
Epoch 509
Epoch 509 :: Batch 0/1
Batch Loss = 0.0007309789972675246
509, epoch_train_loss=0.0007309789972675246
Epoch 510
Epoch 510 :: Batch 0/1
Batch Loss = 0.0007309789972675246
510, epoch_train_loss=0.0007309789972675246
Epoch 511
Epoch 511 :: Batch 0/1
Batch Loss = 0.0007309789972675246
511, epoch_train_loss=0.0007309789972675246
Epoch 512
Epoch 512 :: Batch 0/1
Batch Loss = 0.0007309789972675246
512, epoch_train_loss=0.0007309789972675246
Epoch 513
Epoch 513 :: Batch 0/1
Batch Loss = 0.0007309789972675246
513, epoch_train_loss=0.0007309789972675246
Epoch 514
Epoch 514 :: Batch 0/1
Batch Loss = 0.0007309789972675246
514, epoch_train_loss=0.0007309789972675246
Epoch 515
Epoch 515 :: Batch 0/1
Batch Loss = 0.0007309789972675246
515, epoch_train_loss=0.0007309789972675246
Epoch 516
Epoch 516 :: Batch 0/1
Batch Loss = 0.0007309789972675246
516, epoch_train_loss=0.0007309789972675246
Epoch 517
Epoch 517 :: Batch 0/1
Batch Loss = 0.0007309789972675246
517, epoch_train_loss=0.0007309789972675246
Epoch 518
Epoch 518 :: Batch 0/1
Batch Loss = 0.0007309789972675246
518, epoch_train_loss=0.0007309789972675246
Epoch 519
Epoch 519 :: Batch 0/1
Batch Loss = 0.0007309789972675246
519, epoch_train_loss=0.0007309789972675246
Epoch 520
Epoch 520 :: Batch 0/1
Batch Loss = 0.0007309789972675246
520, epoch_train_loss=0.0007309789972675246
Epoch 521
Epoch 521 :: Batch 0/1
Batch Loss = 0.0007309789972675246
521, epoch_train_loss=0.0007309789972675246
Epoch 522
Epoch 522 :: Batch 0/1
Batch Loss = 0.0007309789972675246
522, epoch_train_loss=0.0007309789972675246
Epoch 523
Epoch 523 :: Batch 0/1
Batch Loss = 0.0007309789972675246
523, epoch_train_loss=0.0007309789972675246
Epoch 524
Epoch 524 :: Batch 0/1
Batch Loss = 0.0007309789972675246
524, epoch_train_loss=0.0007309789972675246
Epoch 525
Epoch 525 :: Batch 0/1
Batch Loss = 0.0007309789972675246
525, epoch_train_loss=0.0007309789972675246
Epoch 526
Epoch 526 :: Batch 0/1
Batch Loss = 0.0007309789972675246
526, epoch_train_loss=0.0007309789972675246
Epoch 527
Epoch 527 :: Batch 0/1
Batch Loss = 0.0007309789972675246
527, epoch_train_loss=0.0007309789972675246
Epoch 528
Epoch 528 :: Batch 0/1
Batch Loss = 0.0007309789972675246
528, epoch_train_loss=0.0007309789972675246
Epoch 529
Epoch 529 :: Batch 0/1
Batch Loss = 0.0007309789972675246
529, epoch_train_loss=0.0007309789972675246
Epoch 530
Epoch 530 :: Batch 0/1
Batch Loss = 0.0007309789972675246
530, epoch_train_loss=0.0007309789972675246
Epoch 531
Epoch 531 :: Batch 0/1
Batch Loss = 0.0007309789972675246
531, epoch_train_loss=0.0007309789972675246
Epoch 532
Epoch 532 :: Batch 0/1
Batch Loss = 0.0007309789972675246
532, epoch_train_loss=0.0007309789972675246
Epoch 533
Epoch 533 :: Batch 0/1
Batch Loss = 0.0007309789972675246
533, epoch_train_loss=0.0007309789972675246
Epoch 534
Epoch 534 :: Batch 0/1
Batch Loss = 0.0007309789972675246
534, epoch_train_loss=0.0007309789972675246
Epoch 535
Epoch 535 :: Batch 0/1
Batch Loss = 0.0007309789972675246
535, epoch_train_loss=0.0007309789972675246
Epoch 536
Epoch 536 :: Batch 0/1
Batch Loss = 0.0007309789972675246
536, epoch_train_loss=0.0007309789972675246
Epoch 537
Epoch 537 :: Batch 0/1
Batch Loss = 0.0007309789972675246
537, epoch_train_loss=0.0007309789972675246
Epoch 538
Epoch 538 :: Batch 0/1
Batch Loss = 0.0007309789972675246
538, epoch_train_loss=0.0007309789972675246
Epoch 539
Epoch 539 :: Batch 0/1
Batch Loss = 0.0007309789972675246
539, epoch_train_loss=0.0007309789972675246
Epoch 540
Epoch 540 :: Batch 0/1
Batch Loss = 0.0007309789972675246
540, epoch_train_loss=0.0007309789972675246
Epoch 541
Epoch 541 :: Batch 0/1
Batch Loss = 0.0007309789972675246
541, epoch_train_loss=0.0007309789972675246
Epoch 542
Epoch 542 :: Batch 0/1
Batch Loss = 0.0007309789972675246
542, epoch_train_loss=0.0007309789972675246
Epoch 543
Epoch 543 :: Batch 0/1
Batch Loss = 0.0007309789972675246
543, epoch_train_loss=0.0007309789972675246
Epoch 544
Epoch 544 :: Batch 0/1
Batch Loss = 0.0007309789972675246
544, epoch_train_loss=0.0007309789972675246
Epoch 545
Epoch 545 :: Batch 0/1
Batch Loss = 0.0007309789972675246
545, epoch_train_loss=0.0007309789972675246
Epoch 546
Epoch 546 :: Batch 0/1
Batch Loss = 0.0007309789972675246
546, epoch_train_loss=0.0007309789972675246
Epoch 547
Epoch 547 :: Batch 0/1
Batch Loss = 0.0007309789972675246
547, epoch_train_loss=0.0007309789972675246
Epoch 548
Epoch 548 :: Batch 0/1
Batch Loss = 0.0007309789972675246
548, epoch_train_loss=0.0007309789972675246
Epoch 549
Epoch 549 :: Batch 0/1
Batch Loss = 0.0007309789972675246
549, epoch_train_loss=0.0007309789972675246
Epoch 550
Epoch 550 :: Batch 0/1
Batch Loss = 0.0007309789972675246
550, epoch_train_loss=0.0007309789972675246
Epoch 551
Epoch 551 :: Batch 0/1
Batch Loss = 0.0007309789972675246
551, epoch_train_loss=0.0007309789972675246
Epoch 552
Epoch 552 :: Batch 0/1
Batch Loss = 0.0007309789972675246
552, epoch_train_loss=0.0007309789972675246
Epoch 553
Epoch 553 :: Batch 0/1
Batch Loss = 0.0007309789972675246
553, epoch_train_loss=0.0007309789972675246
Epoch 554
Epoch 554 :: Batch 0/1
Batch Loss = 0.0007309789972675246
554, epoch_train_loss=0.0007309789972675246
Epoch 555
Epoch 555 :: Batch 0/1
Batch Loss = 0.0007309789972675246
555, epoch_train_loss=0.0007309789972675246
Epoch 556
Epoch 556 :: Batch 0/1
Batch Loss = 0.0007309789972675246
556, epoch_train_loss=0.0007309789972675246
Epoch 557
Epoch 557 :: Batch 0/1
Batch Loss = 0.0007309789972675246
557, epoch_train_loss=0.0007309789972675246
Epoch 558
Epoch 558 :: Batch 0/1
Batch Loss = 0.0007309789972675246
558, epoch_train_loss=0.0007309789972675246
Epoch 559
Epoch 559 :: Batch 0/1
Batch Loss = 0.0007309789972675246
559, epoch_train_loss=0.0007309789972675246
Epoch 560
Epoch 560 :: Batch 0/1
Batch Loss = 0.0007309789972675246
560, epoch_train_loss=0.0007309789972675246
Epoch 561
Epoch 561 :: Batch 0/1
Batch Loss = 0.0007309789972675246
561, epoch_train_loss=0.0007309789972675246
Epoch 562
Epoch 562 :: Batch 0/1
Batch Loss = 0.0007309789972675246
562, epoch_train_loss=0.0007309789972675246
Epoch 563
Epoch 563 :: Batch 0/1
Batch Loss = 0.0007309789972675246
563, epoch_train_loss=0.0007309789972675246
Epoch 564
Epoch 564 :: Batch 0/1
Batch Loss = 0.0007309789972675246
564, epoch_train_loss=0.0007309789972675246
Epoch 565
Epoch 565 :: Batch 0/1
Batch Loss = 0.0007309789972675246
565, epoch_train_loss=0.0007309789972675246
Epoch 566
Epoch 566 :: Batch 0/1
Batch Loss = 0.0007309789972675246
566, epoch_train_loss=0.0007309789972675246
Epoch 567
Epoch 567 :: Batch 0/1
Batch Loss = 0.0007309789972675246
567, epoch_train_loss=0.0007309789972675246
Epoch 568
Epoch 568 :: Batch 0/1
Batch Loss = 0.0007309789972675246
568, epoch_train_loss=0.0007309789972675246
Epoch 569
Epoch 569 :: Batch 0/1
Batch Loss = 0.0007309789972675246
569, epoch_train_loss=0.0007309789972675246
Epoch 570
Epoch 570 :: Batch 0/1
Batch Loss = 0.0007309789972675246
570, epoch_train_loss=0.0007309789972675246
Epoch 571
Epoch 571 :: Batch 0/1
Batch Loss = 0.0007309789972675246
571, epoch_train_loss=0.0007309789972675246
Epoch 572
Epoch 572 :: Batch 0/1
Batch Loss = 0.0007309789972675246
572, epoch_train_loss=0.0007309789972675246
Epoch 573
Epoch 573 :: Batch 0/1
Batch Loss = 0.0007309789972675246
573, epoch_train_loss=0.0007309789972675246
Epoch 574
Epoch 574 :: Batch 0/1
Batch Loss = 0.0007309789972675246
574, epoch_train_loss=0.0007309789972675246
Epoch 575
Epoch 575 :: Batch 0/1
Batch Loss = 0.0007309789972675246
575, epoch_train_loss=0.0007309789972675246
Epoch 576
Epoch 576 :: Batch 0/1
Batch Loss = 0.0007309789972675246
576, epoch_train_loss=0.0007309789972675246
Epoch 577
Epoch 577 :: Batch 0/1
Batch Loss = 0.0007309789972675246
577, epoch_train_loss=0.0007309789972675246
Epoch 578
Epoch 578 :: Batch 0/1
Batch Loss = 0.0007309789972675246
578, epoch_train_loss=0.0007309789972675246
Epoch 579
Epoch 579 :: Batch 0/1
Batch Loss = 0.0007309789972675246
579, epoch_train_loss=0.0007309789972675246
Epoch 580
Epoch 580 :: Batch 0/1
Batch Loss = 0.0007309789972675246
580, epoch_train_loss=0.0007309789972675246
Epoch 581
Epoch 581 :: Batch 0/1
Batch Loss = 0.0007309789972675246
581, epoch_train_loss=0.0007309789972675246
Epoch 582
Epoch 582 :: Batch 0/1
Batch Loss = 0.0007309789972675246
582, epoch_train_loss=0.0007309789972675246
Epoch 583
Epoch 583 :: Batch 0/1
Batch Loss = 0.0007309789972675246
583, epoch_train_loss=0.0007309789972675246
Epoch 584
Epoch 584 :: Batch 0/1
Batch Loss = 0.0007309789972675246
584, epoch_train_loss=0.0007309789972675246
Epoch 585
Epoch 585 :: Batch 0/1
Batch Loss = 0.0007309789972675246
585, epoch_train_loss=0.0007309789972675246
Epoch 586
Epoch 586 :: Batch 0/1
Batch Loss = 0.0007309789972675246
586, epoch_train_loss=0.0007309789972675246
Epoch 587
Epoch 587 :: Batch 0/1
Batch Loss = 0.0007309789972675246
587, epoch_train_loss=0.0007309789972675246
Epoch 588
Epoch 588 :: Batch 0/1
Batch Loss = 0.0007309789972675246
588, epoch_train_loss=0.0007309789972675246
Epoch 589
Epoch 589 :: Batch 0/1
Batch Loss = 0.0007309789972675246
589, epoch_train_loss=0.0007309789972675246
Epoch 590
Epoch 590 :: Batch 0/1
Batch Loss = 0.0007309789972675246
590, epoch_train_loss=0.0007309789972675246
Epoch 591
Epoch 591 :: Batch 0/1
Batch Loss = 0.0007309789972675246
591, epoch_train_loss=0.0007309789972675246
Epoch 592
Epoch 592 :: Batch 0/1
Batch Loss = 0.0007309789972675246
592, epoch_train_loss=0.0007309789972675246
Epoch 593
Epoch 593 :: Batch 0/1
Batch Loss = 0.0007309789972675246
593, epoch_train_loss=0.0007309789972675246
Epoch 594
Epoch 594 :: Batch 0/1
Batch Loss = 0.0007309789972675246
594, epoch_train_loss=0.0007309789972675246
Epoch 595
Epoch 595 :: Batch 0/1
Batch Loss = 0.0007309789972675246
595, epoch_train_loss=0.0007309789972675246
Epoch 596
Epoch 596 :: Batch 0/1
Batch Loss = 0.0007309789972675246
596, epoch_train_loss=0.0007309789972675246
Epoch 597
Epoch 597 :: Batch 0/1
Batch Loss = 0.0007309789972675246
597, epoch_train_loss=0.0007309789972675246
Epoch 598
Epoch 598 :: Batch 0/1
Batch Loss = 0.0007309789972675246
598, epoch_train_loss=0.0007309789972675246
Epoch 599
Epoch 599 :: Batch 0/1
Batch Loss = 0.0007309789972675246
599, epoch_train_loss=0.0007309789972675246
Epoch 600
Epoch 600 :: Batch 0/1
Batch Loss = 0.0007309789972675246
600, epoch_train_loss=0.0007309789972675246
Epoch 601
Epoch 601 :: Batch 0/1
Batch Loss = 0.0007309789972675246
601, epoch_train_loss=0.0007309789972675246
Epoch 602
Epoch 602 :: Batch 0/1
Batch Loss = 0.0007309789972675246
602, epoch_train_loss=0.0007309789972675246
Epoch 603
Epoch 603 :: Batch 0/1
Batch Loss = 0.0007309789972675246
603, epoch_train_loss=0.0007309789972675246
Epoch 604
Epoch 604 :: Batch 0/1
Batch Loss = 0.0007309789972675246
604, epoch_train_loss=0.0007309789972675246
Epoch 605
Epoch 605 :: Batch 0/1
Batch Loss = 0.0007309789972675246
605, epoch_train_loss=0.0007309789972675246
Epoch 606
Epoch 606 :: Batch 0/1
Batch Loss = 0.0007309789972675246
606, epoch_train_loss=0.0007309789972675246
Epoch 607
Epoch 607 :: Batch 0/1
Batch Loss = 0.0007309789972675246
607, epoch_train_loss=0.0007309789972675246
Epoch 608
Epoch 608 :: Batch 0/1
Batch Loss = 0.0007309789972675246
608, epoch_train_loss=0.0007309789972675246
Epoch 609
Epoch 609 :: Batch 0/1
Batch Loss = 0.0007309789972675246
609, epoch_train_loss=0.0007309789972675246
Epoch 610
Epoch 610 :: Batch 0/1
Batch Loss = 0.0007309789972675246
610, epoch_train_loss=0.0007309789972675246
Epoch 611
Epoch 611 :: Batch 0/1
Batch Loss = 0.0007309789972675246
611, epoch_train_loss=0.0007309789972675246
Epoch 612
Epoch 612 :: Batch 0/1
Batch Loss = 0.0007309789972675246
612, epoch_train_loss=0.0007309789972675246
Epoch 613
Epoch 613 :: Batch 0/1
Batch Loss = 0.0007309789972675246
613, epoch_train_loss=0.0007309789972675246
Epoch 614
Epoch 614 :: Batch 0/1
Batch Loss = 0.0007309789972675246
614, epoch_train_loss=0.0007309789972675246
Epoch 615
Epoch 615 :: Batch 0/1
Batch Loss = 0.0007309789972675246
615, epoch_train_loss=0.0007309789972675246
Epoch 616
Epoch 616 :: Batch 0/1
Batch Loss = 0.0007309789972675246
616, epoch_train_loss=0.0007309789972675246
Epoch 617
Epoch 617 :: Batch 0/1
Batch Loss = 0.0007309789972675246
617, epoch_train_loss=0.0007309789972675246
Epoch 618
Epoch 618 :: Batch 0/1
Batch Loss = 0.0007309789972675246
618, epoch_train_loss=0.0007309789972675246
Epoch 619
Epoch 619 :: Batch 0/1
Batch Loss = 0.0007309789972675246
619, epoch_train_loss=0.0007309789972675246
Epoch 620
Epoch 620 :: Batch 0/1
Batch Loss = 0.0007309789972675246
620, epoch_train_loss=0.0007309789972675246
Epoch 621
Epoch 621 :: Batch 0/1
Batch Loss = 0.0007309789972675246
621, epoch_train_loss=0.0007309789972675246
Epoch 622
Epoch 622 :: Batch 0/1
Batch Loss = 0.0007309789972675246
622, epoch_train_loss=0.0007309789972675246
Epoch 623
Epoch 623 :: Batch 0/1
Batch Loss = 0.0007309789972675246
623, epoch_train_loss=0.0007309789972675246
Epoch 624
Epoch 624 :: Batch 0/1
Batch Loss = 0.0007309789972675246
624, epoch_train_loss=0.0007309789972675246
Epoch 625
Epoch 625 :: Batch 0/1
Batch Loss = 0.0007309789972675246
625, epoch_train_loss=0.0007309789972675246
Epoch 626
Epoch 626 :: Batch 0/1
Batch Loss = 0.0007309789972675246
626, epoch_train_loss=0.0007309789972675246
Epoch 627
Epoch 627 :: Batch 0/1
Batch Loss = 0.0007309789972675246
627, epoch_train_loss=0.0007309789972675246
Epoch 628
Epoch 628 :: Batch 0/1
Batch Loss = 0.0007309789972675246
628, epoch_train_loss=0.0007309789972675246
Epoch 629
Epoch 629 :: Batch 0/1
Batch Loss = 0.0007309789972675246
629, epoch_train_loss=0.0007309789972675246
Epoch 630
Epoch 630 :: Batch 0/1
Batch Loss = 0.0007309789972675246
630, epoch_train_loss=0.0007309789972675246
Epoch 631
Epoch 631 :: Batch 0/1
Batch Loss = 0.0007309789972675246
631, epoch_train_loss=0.0007309789972675246
Epoch 632
Epoch 632 :: Batch 0/1
Batch Loss = 0.0007309789972675246
632, epoch_train_loss=0.0007309789972675246
Epoch 633
Epoch 633 :: Batch 0/1
Batch Loss = 0.0007309789972675246
633, epoch_train_loss=0.0007309789972675246
Epoch 634
Epoch 634 :: Batch 0/1
Batch Loss = 0.0007309789972675246
634, epoch_train_loss=0.0007309789972675246
Epoch 635
Epoch 635 :: Batch 0/1
Batch Loss = 0.0007309789972675246
635, epoch_train_loss=0.0007309789972675246
Epoch 636
Epoch 636 :: Batch 0/1
Batch Loss = 0.0007309789972675246
636, epoch_train_loss=0.0007309789972675246
Epoch 637
Epoch 637 :: Batch 0/1
Batch Loss = 0.0007309789972675246
637, epoch_train_loss=0.0007309789972675246
Epoch 638
Epoch 638 :: Batch 0/1
Batch Loss = 0.0007309789972675246
638, epoch_train_loss=0.0007309789972675246
Epoch 639
Epoch 639 :: Batch 0/1
Batch Loss = 0.0007309789972675246
639, epoch_train_loss=0.0007309789972675246
Epoch 640
Epoch 640 :: Batch 0/1
Batch Loss = 0.0007309789972675246
640, epoch_train_loss=0.0007309789972675246
Epoch 641
Epoch 641 :: Batch 0/1
Batch Loss = 0.0007309789972675246
641, epoch_train_loss=0.0007309789972675246
Epoch 642
Epoch 642 :: Batch 0/1
Batch Loss = 0.0007309789972675246
642, epoch_train_loss=0.0007309789972675246
Epoch 643
Epoch 643 :: Batch 0/1
Batch Loss = 0.0007309789972675246
643, epoch_train_loss=0.0007309789972675246
Epoch 644
Epoch 644 :: Batch 0/1
Batch Loss = 0.0007309789972675246
644, epoch_train_loss=0.0007309789972675246
Epoch 645
Epoch 645 :: Batch 0/1
Batch Loss = 0.0007309789972675246
645, epoch_train_loss=0.0007309789972675246
Epoch 646
Epoch 646 :: Batch 0/1
Batch Loss = 0.0007309789972675246
646, epoch_train_loss=0.0007309789972675246
Epoch 647
Epoch 647 :: Batch 0/1
Batch Loss = 0.0007309789972675246
647, epoch_train_loss=0.0007309789972675246
Epoch 648
Epoch 648 :: Batch 0/1
Batch Loss = 0.0007309789972675246
648, epoch_train_loss=0.0007309789972675246
Epoch 649
Epoch 649 :: Batch 0/1
Batch Loss = 0.0007309789972675246
649, epoch_train_loss=0.0007309789972675246
Epoch 650
Epoch 650 :: Batch 0/1
Batch Loss = 0.0007309789972675246
650, epoch_train_loss=0.0007309789972675246
Epoch 651
Epoch 651 :: Batch 0/1
Batch Loss = 0.0007309789972675246
651, epoch_train_loss=0.0007309789972675246
Epoch 652
Epoch 652 :: Batch 0/1
Batch Loss = 0.0007309789972675246
652, epoch_train_loss=0.0007309789972675246
Epoch 653
Epoch 653 :: Batch 0/1
Batch Loss = 0.0007309789972675246
653, epoch_train_loss=0.0007309789972675246
Epoch 654
Epoch 654 :: Batch 0/1
Batch Loss = 0.0007309789972675246
654, epoch_train_loss=0.0007309789972675246
Epoch 655
Epoch 655 :: Batch 0/1
Batch Loss = 0.0007309789972675246
655, epoch_train_loss=0.0007309789972675246
Epoch 656
Epoch 656 :: Batch 0/1
Batch Loss = 0.0007309789972675246
656, epoch_train_loss=0.0007309789972675246
Epoch 657
Epoch 657 :: Batch 0/1
Batch Loss = 0.0007309789972675246
657, epoch_train_loss=0.0007309789972675246
Epoch 658
Epoch 658 :: Batch 0/1
Batch Loss = 0.0007309789972675246
658, epoch_train_loss=0.0007309789972675246
Epoch 659
Epoch 659 :: Batch 0/1
Batch Loss = 0.0007309789972675246
659, epoch_train_loss=0.0007309789972675246
Epoch 660
Epoch 660 :: Batch 0/1
Batch Loss = 0.0007309789972675246
660, epoch_train_loss=0.0007309789972675246
Epoch 661
Epoch 661 :: Batch 0/1
Batch Loss = 0.0007309789972675246
661, epoch_train_loss=0.0007309789972675246
Epoch 662
Epoch 662 :: Batch 0/1
Batch Loss = 0.0007309789972675246
662, epoch_train_loss=0.0007309789972675246
Epoch 663
Epoch 663 :: Batch 0/1
Batch Loss = 0.0007309789972675246
663, epoch_train_loss=0.0007309789972675246
Epoch 664
Epoch 664 :: Batch 0/1
Batch Loss = 0.0007309789972675246
664, epoch_train_loss=0.0007309789972675246
Epoch 665
Epoch 665 :: Batch 0/1
Batch Loss = 0.0007309789972675246
665, epoch_train_loss=0.0007309789972675246
Epoch 666
Epoch 666 :: Batch 0/1
Batch Loss = 0.0007309789972675246
666, epoch_train_loss=0.0007309789972675246
Epoch 667
Epoch 667 :: Batch 0/1
Batch Loss = 0.0007309789972675246
667, epoch_train_loss=0.0007309789972675246
Epoch 668
Epoch 668 :: Batch 0/1
Batch Loss = 0.0007309789972675246
668, epoch_train_loss=0.0007309789972675246
Epoch 669
Epoch 669 :: Batch 0/1
Batch Loss = 0.0007309789972675246
669, epoch_train_loss=0.0007309789972675246
Epoch 670
Epoch 670 :: Batch 0/1
Batch Loss = 0.0007309789972675246
670, epoch_train_loss=0.0007309789972675246
Epoch 671
Epoch 671 :: Batch 0/1
Batch Loss = 0.0007309789972675246
671, epoch_train_loss=0.0007309789972675246
Epoch 672
Epoch 672 :: Batch 0/1
Batch Loss = 0.0007309789972675246
672, epoch_train_loss=0.0007309789972675246
Epoch 673
Epoch 673 :: Batch 0/1
Batch Loss = 0.0007309789972675246
673, epoch_train_loss=0.0007309789972675246
Epoch 674
Epoch 674 :: Batch 0/1
Batch Loss = 0.0007309789972675246
674, epoch_train_loss=0.0007309789972675246
Epoch 675
Epoch 675 :: Batch 0/1
Batch Loss = 0.0007309789972675246
675, epoch_train_loss=0.0007309789972675246
Epoch 676
Epoch 676 :: Batch 0/1
Batch Loss = 0.0007309789972675246
676, epoch_train_loss=0.0007309789972675246
Epoch 677
Epoch 677 :: Batch 0/1
Batch Loss = 0.0007309789972675246
677, epoch_train_loss=0.0007309789972675246
Epoch 678
Epoch 678 :: Batch 0/1
Batch Loss = 0.0007309789972675246
678, epoch_train_loss=0.0007309789972675246
Epoch 679
Epoch 679 :: Batch 0/1
Batch Loss = 0.0007309789972675246
679, epoch_train_loss=0.0007309789972675246
Epoch 680
Epoch 680 :: Batch 0/1
Batch Loss = 0.0007309789972675246
680, epoch_train_loss=0.0007309789972675246
Epoch 681
Epoch 681 :: Batch 0/1
Batch Loss = 0.0007309789972675246
681, epoch_train_loss=0.0007309789972675246
Epoch 682
Epoch 682 :: Batch 0/1
Batch Loss = 0.0007309789972675246
682, epoch_train_loss=0.0007309789972675246
Epoch 683
Epoch 683 :: Batch 0/1
Batch Loss = 0.0007309789972675246
683, epoch_train_loss=0.0007309789972675246
Epoch 684
Epoch 684 :: Batch 0/1
Batch Loss = 0.0007309789972675246
684, epoch_train_loss=0.0007309789972675246
Epoch 685
Epoch 685 :: Batch 0/1
Batch Loss = 0.0007309789972675246
685, epoch_train_loss=0.0007309789972675246
Epoch 686
Epoch 686 :: Batch 0/1
Batch Loss = 0.0007309789972675246
686, epoch_train_loss=0.0007309789972675246
Epoch 687
Epoch 687 :: Batch 0/1
Batch Loss = 0.0007309789972675246
687, epoch_train_loss=0.0007309789972675246
Epoch 688
Epoch 688 :: Batch 0/1
Batch Loss = 0.0007309789972675246
688, epoch_train_loss=0.0007309789972675246
Epoch 689
Epoch 689 :: Batch 0/1
Batch Loss = 0.0007309789972675246
689, epoch_train_loss=0.0007309789972675246
Epoch 690
Epoch 690 :: Batch 0/1
Batch Loss = 0.0007309789972675246
690, epoch_train_loss=0.0007309789972675246
Epoch 691
Epoch 691 :: Batch 0/1
Batch Loss = 0.0007309789972675246
691, epoch_train_loss=0.0007309789972675246
Epoch 692
Epoch 692 :: Batch 0/1
Batch Loss = 0.0007309789972675246
692, epoch_train_loss=0.0007309789972675246
Epoch 693
Epoch 693 :: Batch 0/1
Batch Loss = 0.0007309789972675246
693, epoch_train_loss=0.0007309789972675246
Epoch 694
Epoch 694 :: Batch 0/1
Batch Loss = 0.0007309789972675246
694, epoch_train_loss=0.0007309789972675246
Epoch 695
Epoch 695 :: Batch 0/1
Batch Loss = 0.0007309789972675246
695, epoch_train_loss=0.0007309789972675246
Epoch 696
Epoch 696 :: Batch 0/1
Batch Loss = 0.0007309789972675246
696, epoch_train_loss=0.0007309789972675246
Epoch 697
Epoch 697 :: Batch 0/1
Batch Loss = 0.0007309789972675246
697, epoch_train_loss=0.0007309789972675246
Epoch 698
Epoch 698 :: Batch 0/1
Batch Loss = 0.0007309789972675246
698, epoch_train_loss=0.0007309789972675246
Epoch 699
Epoch 699 :: Batch 0/1
Batch Loss = 0.0007309789972675246
699, epoch_train_loss=0.0007309789972675246
Epoch 700
Epoch 700 :: Batch 0/1
Batch Loss = 0.0007309789972675246
700, epoch_train_loss=0.0007309789972675246
Epoch 701
Epoch 701 :: Batch 0/1
Batch Loss = 0.0007309789972675246
701, epoch_train_loss=0.0007309789972675246
Epoch 702
Epoch 702 :: Batch 0/1
Batch Loss = 0.0007309789972675246
702, epoch_train_loss=0.0007309789972675246
Epoch 703
Epoch 703 :: Batch 0/1
Batch Loss = 0.0007309789972675246
703, epoch_train_loss=0.0007309789972675246
Epoch 704
Epoch 704 :: Batch 0/1
Batch Loss = 0.0007309789972675246
704, epoch_train_loss=0.0007309789972675246
Epoch 705
Epoch 705 :: Batch 0/1
Batch Loss = 0.0007309789972675246
705, epoch_train_loss=0.0007309789972675246
Epoch 706
Epoch 706 :: Batch 0/1
Batch Loss = 0.0007309789972675246
706, epoch_train_loss=0.0007309789972675246
Epoch 707
Epoch 707 :: Batch 0/1
Batch Loss = 0.0007309789972675246
707, epoch_train_loss=0.0007309789972675246
Epoch 708
Epoch 708 :: Batch 0/1
Batch Loss = 0.0007309789972675246
708, epoch_train_loss=0.0007309789972675246
Epoch 709
Epoch 709 :: Batch 0/1
Batch Loss = 0.0007309789972675246
709, epoch_train_loss=0.0007309789972675246
Epoch 710
Epoch 710 :: Batch 0/1
Batch Loss = 0.0007309789972675246
710, epoch_train_loss=0.0007309789972675246
Epoch 711
Epoch 711 :: Batch 0/1
Batch Loss = 0.0007309789972675246
711, epoch_train_loss=0.0007309789972675246
Epoch 712
Epoch 712 :: Batch 0/1
Batch Loss = 0.0007309789972675246
712, epoch_train_loss=0.0007309789972675246
Epoch 713
Epoch 713 :: Batch 0/1
Batch Loss = 0.0007309789972675246
713, epoch_train_loss=0.0007309789972675246
Epoch 714
Epoch 714 :: Batch 0/1
Batch Loss = 0.0007309789972675246
714, epoch_train_loss=0.0007309789972675246
Epoch 715
Epoch 715 :: Batch 0/1
Batch Loss = 0.0007309789972675246
715, epoch_train_loss=0.0007309789972675246
Epoch 716
Epoch 716 :: Batch 0/1
Batch Loss = 0.0007309789972675246
716, epoch_train_loss=0.0007309789972675246
Epoch 717
Epoch 717 :: Batch 0/1
Batch Loss = 0.0007309789972675246
717, epoch_train_loss=0.0007309789972675246
Epoch 718
Epoch 718 :: Batch 0/1
Batch Loss = 0.0007309789972675246
718, epoch_train_loss=0.0007309789972675246
Epoch 719
Epoch 719 :: Batch 0/1
Batch Loss = 0.0007309789972675246
719, epoch_train_loss=0.0007309789972675246
Epoch 720
Epoch 720 :: Batch 0/1
Batch Loss = 0.0007309789972675246
720, epoch_train_loss=0.0007309789972675246
Epoch 721
Epoch 721 :: Batch 0/1
Batch Loss = 0.0007309789972675246
721, epoch_train_loss=0.0007309789972675246
Epoch 722
Epoch 722 :: Batch 0/1
Batch Loss = 0.0007309789972675246
722, epoch_train_loss=0.0007309789972675246
Epoch 723
Epoch 723 :: Batch 0/1
Batch Loss = 0.0007309789972675246
723, epoch_train_loss=0.0007309789972675246
Epoch 724
Epoch 724 :: Batch 0/1
Batch Loss = 0.0007309789972675246
724, epoch_train_loss=0.0007309789972675246
Epoch 725
Epoch 725 :: Batch 0/1
Batch Loss = 0.0007309789972675246
725, epoch_train_loss=0.0007309789972675246
Epoch 726
Epoch 726 :: Batch 0/1
Batch Loss = 0.0007309789972675246
726, epoch_train_loss=0.0007309789972675246
Epoch 727
Epoch 727 :: Batch 0/1
Batch Loss = 0.0007309789972675246
727, epoch_train_loss=0.0007309789972675246
Epoch 728
Epoch 728 :: Batch 0/1
Batch Loss = 0.0007309789972675246
728, epoch_train_loss=0.0007309789972675246
Epoch 729
Epoch 729 :: Batch 0/1
Batch Loss = 0.0007309789972675246
729, epoch_train_loss=0.0007309789972675246
Epoch 730
Epoch 730 :: Batch 0/1
Batch Loss = 0.0007309789972675246
730, epoch_train_loss=0.0007309789972675246
Epoch 731
Epoch 731 :: Batch 0/1
Batch Loss = 0.0007309789972675246
731, epoch_train_loss=0.0007309789972675246
Epoch 732
Epoch 732 :: Batch 0/1
Batch Loss = 0.0007309789972675246
732, epoch_train_loss=0.0007309789972675246
Epoch 733
Epoch 733 :: Batch 0/1
Batch Loss = 0.0007309789972675246
733, epoch_train_loss=0.0007309789972675246
Epoch 734
Epoch 734 :: Batch 0/1
Batch Loss = 0.0007309789972675246
734, epoch_train_loss=0.0007309789972675246
Epoch 735
Epoch 735 :: Batch 0/1
Batch Loss = 0.0007309789972675246
735, epoch_train_loss=0.0007309789972675246
Epoch 736
Epoch 736 :: Batch 0/1
Batch Loss = 0.0007309789972675246
736, epoch_train_loss=0.0007309789972675246
Epoch 737
Epoch 737 :: Batch 0/1
Batch Loss = 0.0007309789972675246
737, epoch_train_loss=0.0007309789972675246
Epoch 738
Epoch 738 :: Batch 0/1
Batch Loss = 0.0007309789972675246
738, epoch_train_loss=0.0007309789972675246
Epoch 739
Epoch 739 :: Batch 0/1
Batch Loss = 0.0007309789972675246
739, epoch_train_loss=0.0007309789972675246
Epoch 740
Epoch 740 :: Batch 0/1
Batch Loss = 0.0007309789972675246
740, epoch_train_loss=0.0007309789972675246
Epoch 741
Epoch 741 :: Batch 0/1
Batch Loss = 0.0007309789972675246
741, epoch_train_loss=0.0007309789972675246
Epoch 742
Epoch 742 :: Batch 0/1
Batch Loss = 0.0007309789972675246
742, epoch_train_loss=0.0007309789972675246
Epoch 743
Epoch 743 :: Batch 0/1
Batch Loss = 0.0007309789972675246
743, epoch_train_loss=0.0007309789972675246
Epoch 744
Epoch 744 :: Batch 0/1
Batch Loss = 0.0007309789972675246
744, epoch_train_loss=0.0007309789972675246
Epoch 745
Epoch 745 :: Batch 0/1
Batch Loss = 0.0007309789972675246
745, epoch_train_loss=0.0007309789972675246
Epoch 746
Epoch 746 :: Batch 0/1
Batch Loss = 0.0007309789972675246
746, epoch_train_loss=0.0007309789972675246
Epoch 747
Epoch 747 :: Batch 0/1
Batch Loss = 0.0007309789972675246
747, epoch_train_loss=0.0007309789972675246
Epoch 748
Epoch 748 :: Batch 0/1
Batch Loss = 0.0007309789972675246
748, epoch_train_loss=0.0007309789972675246
Epoch 749
Epoch 749 :: Batch 0/1
Batch Loss = 0.0007309789972675246
749, epoch_train_loss=0.0007309789972675246
Epoch 750
Epoch 750 :: Batch 0/1
Batch Loss = 0.0007309789972675246
750, epoch_train_loss=0.0007309789972675246
Epoch 751
Epoch 751 :: Batch 0/1
Batch Loss = 0.0007309789972675246
751, epoch_train_loss=0.0007309789972675246
Epoch 752
Epoch 752 :: Batch 0/1
Batch Loss = 0.0007309789972675246
752, epoch_train_loss=0.0007309789972675246
Epoch 753
Epoch 753 :: Batch 0/1
Batch Loss = 0.0007309789972675246
753, epoch_train_loss=0.0007309789972675246
Epoch 754
Epoch 754 :: Batch 0/1
Batch Loss = 0.0007309789972675246
754, epoch_train_loss=0.0007309789972675246
Epoch 755
Epoch 755 :: Batch 0/1
Batch Loss = 0.0007309789972675246
755, epoch_train_loss=0.0007309789972675246
Epoch 756
Epoch 756 :: Batch 0/1
Batch Loss = 0.0007309789972675246
756, epoch_train_loss=0.0007309789972675246
Epoch 757
Epoch 757 :: Batch 0/1
Batch Loss = 0.0007309789972675246
757, epoch_train_loss=0.0007309789972675246
Epoch 758
Epoch 758 :: Batch 0/1
Batch Loss = 0.0007309789972675246
758, epoch_train_loss=0.0007309789972675246
Epoch 759
Epoch 759 :: Batch 0/1
Batch Loss = 0.0007309789972675246
759, epoch_train_loss=0.0007309789972675246
Epoch 760
Epoch 760 :: Batch 0/1
Batch Loss = 0.0007309789972675246
760, epoch_train_loss=0.0007309789972675246
Epoch 761
Epoch 761 :: Batch 0/1
Batch Loss = 0.0007309789972675246
761, epoch_train_loss=0.0007309789972675246
Epoch 762
Epoch 762 :: Batch 0/1
Batch Loss = 0.0007309789972675246
762, epoch_train_loss=0.0007309789972675246
Epoch 763
Epoch 763 :: Batch 0/1
Batch Loss = 0.0007309789972675246
763, epoch_train_loss=0.0007309789972675246
Epoch 764
Epoch 764 :: Batch 0/1
Batch Loss = 0.0007309789972675246
764, epoch_train_loss=0.0007309789972675246
Epoch 765
Epoch 765 :: Batch 0/1
Batch Loss = 0.0007309789972675246
765, epoch_train_loss=0.0007309789972675246
Epoch 766
Epoch 766 :: Batch 0/1
Batch Loss = 0.0007309789972675246
766, epoch_train_loss=0.0007309789972675246
Epoch 767
Epoch 767 :: Batch 0/1
Batch Loss = 0.0007309789972675246
767, epoch_train_loss=0.0007309789972675246
Epoch 768
Epoch 768 :: Batch 0/1
Batch Loss = 0.0007309789972675246
768, epoch_train_loss=0.0007309789972675246
Epoch 769
Epoch 769 :: Batch 0/1
Batch Loss = 0.0007309789972675246
769, epoch_train_loss=0.0007309789972675246
Epoch 770
Epoch 770 :: Batch 0/1
Batch Loss = 0.0007309789972675246
770, epoch_train_loss=0.0007309789972675246
Epoch 771
Epoch 771 :: Batch 0/1
Batch Loss = 0.0007309789972675246
771, epoch_train_loss=0.0007309789972675246
Epoch 772
Epoch 772 :: Batch 0/1
Batch Loss = 0.0007309789972675246
772, epoch_train_loss=0.0007309789972675246
Epoch 773
Epoch 773 :: Batch 0/1
Batch Loss = 0.0007309789972675246
773, epoch_train_loss=0.0007309789972675246
Epoch 774
Epoch 774 :: Batch 0/1
Batch Loss = 0.0007309789972675246
774, epoch_train_loss=0.0007309789972675246
Epoch 775
Epoch 775 :: Batch 0/1
Batch Loss = 0.0007309789972675246
775, epoch_train_loss=0.0007309789972675246
Epoch 776
Epoch 776 :: Batch 0/1
Batch Loss = 0.0007309789972675246
776, epoch_train_loss=0.0007309789972675246
Epoch 777
Epoch 777 :: Batch 0/1
Batch Loss = 0.0007309789972675246
777, epoch_train_loss=0.0007309789972675246
Epoch 778
Epoch 778 :: Batch 0/1
Batch Loss = 0.0007309789972675246
778, epoch_train_loss=0.0007309789972675246
Epoch 779
Epoch 779 :: Batch 0/1
Batch Loss = 0.0007309789972675246
779, epoch_train_loss=0.0007309789972675246
Epoch 780
Epoch 780 :: Batch 0/1
Batch Loss = 0.0007309789972675246
780, epoch_train_loss=0.0007309789972675246
Epoch 781
Epoch 781 :: Batch 0/1
Batch Loss = 0.0007309789972675246
781, epoch_train_loss=0.0007309789972675246
Epoch 782
Epoch 782 :: Batch 0/1
Batch Loss = 0.0007309789972675246
782, epoch_train_loss=0.0007309789972675246
Epoch 783
Epoch 783 :: Batch 0/1
Batch Loss = 0.0007309789972675246
783, epoch_train_loss=0.0007309789972675246
Epoch 784
Epoch 784 :: Batch 0/1
Batch Loss = 0.0007309789972675246
784, epoch_train_loss=0.0007309789972675246
Epoch 785
Epoch 785 :: Batch 0/1
Batch Loss = 0.0007309789972675246
785, epoch_train_loss=0.0007309789972675246
Epoch 786
Epoch 786 :: Batch 0/1
Batch Loss = 0.0007309789972675246
786, epoch_train_loss=0.0007309789972675246
Epoch 787
Epoch 787 :: Batch 0/1
Batch Loss = 0.0007309789972675246
787, epoch_train_loss=0.0007309789972675246
Epoch 788
Epoch 788 :: Batch 0/1
Batch Loss = 0.0007309789972675246
788, epoch_train_loss=0.0007309789972675246
Epoch 789
Epoch 789 :: Batch 0/1
Batch Loss = 0.0007309789972675246
789, epoch_train_loss=0.0007309789972675246
Epoch 790
Epoch 790 :: Batch 0/1
Batch Loss = 0.0007309789972675246
790, epoch_train_loss=0.0007309789972675246
Epoch 791
Epoch 791 :: Batch 0/1
Batch Loss = 0.0007309789972675246
791, epoch_train_loss=0.0007309789972675246
Epoch 792
Epoch 792 :: Batch 0/1
Batch Loss = 0.0007309789972675246
792, epoch_train_loss=0.0007309789972675246
Epoch 793
Epoch 793 :: Batch 0/1
Batch Loss = 0.0007309789972675246
793, epoch_train_loss=0.0007309789972675246
Epoch 794
Epoch 794 :: Batch 0/1
Batch Loss = 0.0007309789972675246
794, epoch_train_loss=0.0007309789972675246
Epoch 795
Epoch 795 :: Batch 0/1
Batch Loss = 0.0007309789972675246
795, epoch_train_loss=0.0007309789972675246
Epoch 796
Epoch 796 :: Batch 0/1
Batch Loss = 0.0007309789972675246
796, epoch_train_loss=0.0007309789972675246
Epoch 797
Epoch 797 :: Batch 0/1
Batch Loss = 0.0007309789972675246
797, epoch_train_loss=0.0007309789972675246
Epoch 798
Epoch 798 :: Batch 0/1
Batch Loss = 0.0007309789972675246
798, epoch_train_loss=0.0007309789972675246
Epoch 799
Epoch 799 :: Batch 0/1
Batch Loss = 0.0007309789972675246
799, epoch_train_loss=0.0007309789972675246
Epoch 800
Epoch 800 :: Batch 0/1
Batch Loss = 0.0007309789972675246
800, epoch_train_loss=0.0007309789972675246
Epoch 801
Epoch 801 :: Batch 0/1
Batch Loss = 0.0007309789972675246
801, epoch_train_loss=0.0007309789972675246
Epoch 802
Epoch 802 :: Batch 0/1
Batch Loss = 0.0007309789972675246
802, epoch_train_loss=0.0007309789972675246
Epoch 803
Epoch 803 :: Batch 0/1
Batch Loss = 0.0007309789972675246
803, epoch_train_loss=0.0007309789972675246
Epoch 804
Epoch 804 :: Batch 0/1
Batch Loss = 0.0007309789972675246
804, epoch_train_loss=0.0007309789972675246
Epoch 805
Epoch 805 :: Batch 0/1
Batch Loss = 0.0007309789972675246
805, epoch_train_loss=0.0007309789972675246
Epoch 806
Epoch 806 :: Batch 0/1
Batch Loss = 0.0007309789972675246
806, epoch_train_loss=0.0007309789972675246
Epoch 807
Epoch 807 :: Batch 0/1
Batch Loss = 0.0007309789972675246
807, epoch_train_loss=0.0007309789972675246
Epoch 808
Epoch 808 :: Batch 0/1
Batch Loss = 0.0007309789972675246
808, epoch_train_loss=0.0007309789972675246
Epoch 809
Epoch 809 :: Batch 0/1
Batch Loss = 0.0007309789972675246
809, epoch_train_loss=0.0007309789972675246
Epoch 810
Epoch 810 :: Batch 0/1
Batch Loss = 0.0007309789972675246
810, epoch_train_loss=0.0007309789972675246
Epoch 811
Epoch 811 :: Batch 0/1
Batch Loss = 0.0007309789972675246
811, epoch_train_loss=0.0007309789972675246
Epoch 812
Epoch 812 :: Batch 0/1
Batch Loss = 0.0007309789972675246
812, epoch_train_loss=0.0007309789972675246
Epoch 813
Epoch 813 :: Batch 0/1
Batch Loss = 0.0007309789972675246
813, epoch_train_loss=0.0007309789972675246
Epoch 814
Epoch 814 :: Batch 0/1
Batch Loss = 0.0007309789972675246
814, epoch_train_loss=0.0007309789972675246
Epoch 815
Epoch 815 :: Batch 0/1
Batch Loss = 0.0007309789972675246
815, epoch_train_loss=0.0007309789972675246
Epoch 816
Epoch 816 :: Batch 0/1
Batch Loss = 0.0007309789972675246
816, epoch_train_loss=0.0007309789972675246
Epoch 817
Epoch 817 :: Batch 0/1
Batch Loss = 0.0007309789972675246
817, epoch_train_loss=0.0007309789972675246
Epoch 818
Epoch 818 :: Batch 0/1
Batch Loss = 0.0007309789972675246
818, epoch_train_loss=0.0007309789972675246
Epoch 819
Epoch 819 :: Batch 0/1
Batch Loss = 0.0007309789972675246
819, epoch_train_loss=0.0007309789972675246
Epoch 820
Epoch 820 :: Batch 0/1
Batch Loss = 0.0007309789972675246
820, epoch_train_loss=0.0007309789972675246
Epoch 821
Epoch 821 :: Batch 0/1
Batch Loss = 0.0007309789972675246
821, epoch_train_loss=0.0007309789972675246
Epoch 822
Epoch 822 :: Batch 0/1
Batch Loss = 0.0007309789972675246
822, epoch_train_loss=0.0007309789972675246
Epoch 823
Epoch 823 :: Batch 0/1
Batch Loss = 0.0007309789972675246
823, epoch_train_loss=0.0007309789972675246
Epoch 824
Epoch 824 :: Batch 0/1
Batch Loss = 0.0007309789972675246
824, epoch_train_loss=0.0007309789972675246
Epoch 825
Epoch 825 :: Batch 0/1
Batch Loss = 0.0007309789972675246
825, epoch_train_loss=0.0007309789972675246
Epoch 826
Epoch 826 :: Batch 0/1
Batch Loss = 0.0007309789972675246
826, epoch_train_loss=0.0007309789972675246
Epoch 827
Epoch 827 :: Batch 0/1
Batch Loss = 0.0007309789972675246
827, epoch_train_loss=0.0007309789972675246
Epoch 828
Epoch 828 :: Batch 0/1
Batch Loss = 0.0007309789972675246
828, epoch_train_loss=0.0007309789972675246
Epoch 829
Epoch 829 :: Batch 0/1
Batch Loss = 0.0007309789972675246
829, epoch_train_loss=0.0007309789972675246
Epoch 830
Epoch 830 :: Batch 0/1
Batch Loss = 0.0007309789972675246
830, epoch_train_loss=0.0007309789972675246
Epoch 831
Epoch 831 :: Batch 0/1
Batch Loss = 0.0007309789972675246
831, epoch_train_loss=0.0007309789972675246
Epoch 832
Epoch 832 :: Batch 0/1
Batch Loss = 0.0007309789972675246
832, epoch_train_loss=0.0007309789972675246
Epoch 833
Epoch 833 :: Batch 0/1
Batch Loss = 0.0007309789972675246
833, epoch_train_loss=0.0007309789972675246
Epoch 834
Epoch 834 :: Batch 0/1
Batch Loss = 0.0007309789972675246
834, epoch_train_loss=0.0007309789972675246
Epoch 835
Epoch 835 :: Batch 0/1
Batch Loss = 0.0007309789972675246
835, epoch_train_loss=0.0007309789972675246
Epoch 836
Epoch 836 :: Batch 0/1
Batch Loss = 0.0007309789972675246
836, epoch_train_loss=0.0007309789972675246
Epoch 837
Epoch 837 :: Batch 0/1
Batch Loss = 0.0007309789972675246
837, epoch_train_loss=0.0007309789972675246
Epoch 838
Epoch 838 :: Batch 0/1
Batch Loss = 0.0007309789972675246
838, epoch_train_loss=0.0007309789972675246
Epoch 839
Epoch 839 :: Batch 0/1
Batch Loss = 0.0007309789972675246
839, epoch_train_loss=0.0007309789972675246
Epoch 840
Epoch 840 :: Batch 0/1
Batch Loss = 0.0007309789972675246
840, epoch_train_loss=0.0007309789972675246
Epoch 841
Epoch 841 :: Batch 0/1
Batch Loss = 0.0007309789972675246
841, epoch_train_loss=0.0007309789972675246
Epoch 842
Epoch 842 :: Batch 0/1
Batch Loss = 0.0007309789972675246
842, epoch_train_loss=0.0007309789972675246
Epoch 843
Epoch 843 :: Batch 0/1
Batch Loss = 0.0007309789972675246
843, epoch_train_loss=0.0007309789972675246
Epoch 844
Epoch 844 :: Batch 0/1
Batch Loss = 0.0007309789972675246
844, epoch_train_loss=0.0007309789972675246
Epoch 845
Epoch 845 :: Batch 0/1
Batch Loss = 0.0007309789972675246
845, epoch_train_loss=0.0007309789972675246
Epoch 846
Epoch 846 :: Batch 0/1
Batch Loss = 0.0007309789972675246
846, epoch_train_loss=0.0007309789972675246
Epoch 847
Epoch 847 :: Batch 0/1
Batch Loss = 0.0007309789972675246
847, epoch_train_loss=0.0007309789972675246
Epoch 848
Epoch 848 :: Batch 0/1
Batch Loss = 0.0007309789972675246
848, epoch_train_loss=0.0007309789972675246
Epoch 849
Epoch 849 :: Batch 0/1
Batch Loss = 0.0007309789972675246
849, epoch_train_loss=0.0007309789972675246
Epoch 850
Epoch 850 :: Batch 0/1
Batch Loss = 0.0007309789972675246
850, epoch_train_loss=0.0007309789972675246
Epoch 851
Epoch 851 :: Batch 0/1
Batch Loss = 0.0007309789972675246
851, epoch_train_loss=0.0007309789972675246
Epoch 852
Epoch 852 :: Batch 0/1
Batch Loss = 0.0007309789972675246
852, epoch_train_loss=0.0007309789972675246
Epoch 853
Epoch 853 :: Batch 0/1
Batch Loss = 0.0007309789972675246
853, epoch_train_loss=0.0007309789972675246
Epoch 854
Epoch 854 :: Batch 0/1
Batch Loss = 0.0007309789972675246
854, epoch_train_loss=0.0007309789972675246
Epoch 855
Epoch 855 :: Batch 0/1
Batch Loss = 0.0007309789972675246
855, epoch_train_loss=0.0007309789972675246
Epoch 856
Epoch 856 :: Batch 0/1
Batch Loss = 0.0007309789972675246
856, epoch_train_loss=0.0007309789972675246
Epoch 857
Epoch 857 :: Batch 0/1
Batch Loss = 0.0007309789972675246
857, epoch_train_loss=0.0007309789972675246
Epoch 858
Epoch 858 :: Batch 0/1
Batch Loss = 0.0007309789972675246
858, epoch_train_loss=0.0007309789972675246
Epoch 859
Epoch 859 :: Batch 0/1
Batch Loss = 0.0007309789972675246
859, epoch_train_loss=0.0007309789972675246
Epoch 860
Epoch 860 :: Batch 0/1
Batch Loss = 0.0007309789972675246
860, epoch_train_loss=0.0007309789972675246
Epoch 861
Epoch 861 :: Batch 0/1
Batch Loss = 0.0007309789972675246
861, epoch_train_loss=0.0007309789972675246
Epoch 862
Epoch 862 :: Batch 0/1
Batch Loss = 0.0007309789972675246
862, epoch_train_loss=0.0007309789972675246
Epoch 863
Epoch 863 :: Batch 0/1
Batch Loss = 0.0007309789972675246
863, epoch_train_loss=0.0007309789972675246
Epoch 864
Epoch 864 :: Batch 0/1
Batch Loss = 0.0007309789972675246
864, epoch_train_loss=0.0007309789972675246
Epoch 865
Epoch 865 :: Batch 0/1
Batch Loss = 0.0007309789972675246
865, epoch_train_loss=0.0007309789972675246
Epoch 866
Epoch 866 :: Batch 0/1
Batch Loss = 0.0007309789972675246
866, epoch_train_loss=0.0007309789972675246
Epoch 867
Epoch 867 :: Batch 0/1
Batch Loss = 0.0007309789972675246
867, epoch_train_loss=0.0007309789972675246
Epoch 868
Epoch 868 :: Batch 0/1
Batch Loss = 0.0007309789972675246
868, epoch_train_loss=0.0007309789972675246
Epoch 869
Epoch 869 :: Batch 0/1
Batch Loss = 0.0007309789972675246
869, epoch_train_loss=0.0007309789972675246
Epoch 870
Epoch 870 :: Batch 0/1
Batch Loss = 0.0007309789972675246
870, epoch_train_loss=0.0007309789972675246
Epoch 871
Epoch 871 :: Batch 0/1
Batch Loss = 0.0007309789972675246
871, epoch_train_loss=0.0007309789972675246
Epoch 872
Epoch 872 :: Batch 0/1
Batch Loss = 0.0007309789972675246
872, epoch_train_loss=0.0007309789972675246
Epoch 873
Epoch 873 :: Batch 0/1
Batch Loss = 0.0007309789972675246
873, epoch_train_loss=0.0007309789972675246
Epoch 874
Epoch 874 :: Batch 0/1
Batch Loss = 0.0007309789972675246
874, epoch_train_loss=0.0007309789972675246
Epoch 875
Epoch 875 :: Batch 0/1
Batch Loss = 0.0007309789972675246
875, epoch_train_loss=0.0007309789972675246
Epoch 876
Epoch 876 :: Batch 0/1
Batch Loss = 0.0007309789972675246
876, epoch_train_loss=0.0007309789972675246
Epoch 877
Epoch 877 :: Batch 0/1
Batch Loss = 0.0007309789972675246
877, epoch_train_loss=0.0007309789972675246
Epoch 878
Epoch 878 :: Batch 0/1
Batch Loss = 0.0007309789972675246
878, epoch_train_loss=0.0007309789972675246
Epoch 879
Epoch 879 :: Batch 0/1
Batch Loss = 0.0007309789972675246
879, epoch_train_loss=0.0007309789972675246
Epoch 880
Epoch 880 :: Batch 0/1
Batch Loss = 0.0007309789972675246
880, epoch_train_loss=0.0007309789972675246
Epoch 881
Epoch 881 :: Batch 0/1
Batch Loss = 0.0007309789972675246
881, epoch_train_loss=0.0007309789972675246
Epoch 882
Epoch 882 :: Batch 0/1
Batch Loss = 0.0007309789972675246
882, epoch_train_loss=0.0007309789972675246
Epoch 883
Epoch 883 :: Batch 0/1
Batch Loss = 0.0007309789972675246
883, epoch_train_loss=0.0007309789972675246
Epoch 884
Epoch 884 :: Batch 0/1
Batch Loss = 0.0007309789972675246
884, epoch_train_loss=0.0007309789972675246
Epoch 885
Epoch 885 :: Batch 0/1
Batch Loss = 0.0007309789972675246
885, epoch_train_loss=0.0007309789972675246
Epoch 886
Epoch 886 :: Batch 0/1
Batch Loss = 0.0007309789972675246
886, epoch_train_loss=0.0007309789972675246
Epoch 887
Epoch 887 :: Batch 0/1
Batch Loss = 0.0007309789972675246
887, epoch_train_loss=0.0007309789972675246
Epoch 888
Epoch 888 :: Batch 0/1
Batch Loss = 0.0007309789972675246
888, epoch_train_loss=0.0007309789972675246
Epoch 889
Epoch 889 :: Batch 0/1
Batch Loss = 0.0007309789972675246
889, epoch_train_loss=0.0007309789972675246
Epoch 890
Epoch 890 :: Batch 0/1
Batch Loss = 0.0007309789972675246
890, epoch_train_loss=0.0007309789972675246
Epoch 891
Epoch 891 :: Batch 0/1
Batch Loss = 0.0007309789972675246
891, epoch_train_loss=0.0007309789972675246
Epoch 892
Epoch 892 :: Batch 0/1
Batch Loss = 0.0007309789972675246
892, epoch_train_loss=0.0007309789972675246
Epoch 893
Epoch 893 :: Batch 0/1
Batch Loss = 0.0007309789972675246
893, epoch_train_loss=0.0007309789972675246
Epoch 894
Epoch 894 :: Batch 0/1
Batch Loss = 0.0007309789972675246
894, epoch_train_loss=0.0007309789972675246
Epoch 895
Epoch 895 :: Batch 0/1
Batch Loss = 0.0007309789972675246
895, epoch_train_loss=0.0007309789972675246
Epoch 896
Epoch 896 :: Batch 0/1
Batch Loss = 0.0007309789972675246
896, epoch_train_loss=0.0007309789972675246
Epoch 897
Epoch 897 :: Batch 0/1
Batch Loss = 0.0007309789972675246
897, epoch_train_loss=0.0007309789972675246
Epoch 898
Epoch 898 :: Batch 0/1
Batch Loss = 0.0007309789972675246
898, epoch_train_loss=0.0007309789972675246
Epoch 899
Epoch 899 :: Batch 0/1
Batch Loss = 0.0007309789972675246
899, epoch_train_loss=0.0007309789972675246
Epoch 900
Epoch 900 :: Batch 0/1
Batch Loss = 0.0007309789972675246
900, epoch_train_loss=0.0007309789972675246
Epoch 901
Epoch 901 :: Batch 0/1
Batch Loss = 0.0007309789972675246
901, epoch_train_loss=0.0007309789972675246
Epoch 902
Epoch 902 :: Batch 0/1
Batch Loss = 0.0007309789972675246
902, epoch_train_loss=0.0007309789972675246
Epoch 903
Epoch 903 :: Batch 0/1
Batch Loss = 0.0007309789972675246
903, epoch_train_loss=0.0007309789972675246
Epoch 904
Epoch 904 :: Batch 0/1
Batch Loss = 0.0007309789972675246
904, epoch_train_loss=0.0007309789972675246
Epoch 905
Epoch 905 :: Batch 0/1
Batch Loss = 0.0007309789972675246
905, epoch_train_loss=0.0007309789972675246
Epoch 906
Epoch 906 :: Batch 0/1
Batch Loss = 0.0007309789972675246
906, epoch_train_loss=0.0007309789972675246
Epoch 907
Epoch 907 :: Batch 0/1
Batch Loss = 0.0007309789972675246
907, epoch_train_loss=0.0007309789972675246
Epoch 908
Epoch 908 :: Batch 0/1
Batch Loss = 0.0007309789972675246
908, epoch_train_loss=0.0007309789972675246
Epoch 909
Epoch 909 :: Batch 0/1
Batch Loss = 0.0007309789972675246
909, epoch_train_loss=0.0007309789972675246
Epoch 910
Epoch 910 :: Batch 0/1
Batch Loss = 0.0007309789972675246
910, epoch_train_loss=0.0007309789972675246
Epoch 911
Epoch 911 :: Batch 0/1
Batch Loss = 0.0007309789972675246
911, epoch_train_loss=0.0007309789972675246
Epoch 912
Epoch 912 :: Batch 0/1
Batch Loss = 0.0007309789972675246
912, epoch_train_loss=0.0007309789972675246
Epoch 913
Epoch 913 :: Batch 0/1
Batch Loss = 0.0007309789972675246
913, epoch_train_loss=0.0007309789972675246
Epoch 914
Epoch 914 :: Batch 0/1
Batch Loss = 0.0007309789972675246
914, epoch_train_loss=0.0007309789972675246
Epoch 915
Epoch 915 :: Batch 0/1
Batch Loss = 0.0007309789972675246
915, epoch_train_loss=0.0007309789972675246
Epoch 916
Epoch 916 :: Batch 0/1
Batch Loss = 0.0007309789972675246
916, epoch_train_loss=0.0007309789972675246
Epoch 917
Epoch 917 :: Batch 0/1
Batch Loss = 0.0007309789972675246
917, epoch_train_loss=0.0007309789972675246
Epoch 918
Epoch 918 :: Batch 0/1
Batch Loss = 0.0007309789972675246
918, epoch_train_loss=0.0007309789972675246
Epoch 919
Epoch 919 :: Batch 0/1
Batch Loss = 0.0007309789972675246
919, epoch_train_loss=0.0007309789972675246
Epoch 920
Epoch 920 :: Batch 0/1
Batch Loss = 0.0007309789972675246
920, epoch_train_loss=0.0007309789972675246
Epoch 921
Epoch 921 :: Batch 0/1
Batch Loss = 0.0007309789972675246
921, epoch_train_loss=0.0007309789972675246
Epoch 922
Epoch 922 :: Batch 0/1
Batch Loss = 0.0007309789972675246
922, epoch_train_loss=0.0007309789972675246
Epoch 923
Epoch 923 :: Batch 0/1
Batch Loss = 0.0007309789972675246
923, epoch_train_loss=0.0007309789972675246
Epoch 924
Epoch 924 :: Batch 0/1
Batch Loss = 0.0007309789972675246
924, epoch_train_loss=0.0007309789972675246
Epoch 925
Epoch 925 :: Batch 0/1
Batch Loss = 0.0007309789972675246
925, epoch_train_loss=0.0007309789972675246
Epoch 926
Epoch 926 :: Batch 0/1
Batch Loss = 0.0007309789972675246
926, epoch_train_loss=0.0007309789972675246
Epoch 927
Epoch 927 :: Batch 0/1
Batch Loss = 0.0007309789972675246
927, epoch_train_loss=0.0007309789972675246
Epoch 928
Epoch 928 :: Batch 0/1
Batch Loss = 0.0007309789972675246
928, epoch_train_loss=0.0007309789972675246
Epoch 929
Epoch 929 :: Batch 0/1
Batch Loss = 0.0007309789972675246
929, epoch_train_loss=0.0007309789972675246
Epoch 930
Epoch 930 :: Batch 0/1
Batch Loss = 0.0007309789972675246
930, epoch_train_loss=0.0007309789972675246
Epoch 931
Epoch 931 :: Batch 0/1
Batch Loss = 0.0007309789972675246
931, epoch_train_loss=0.0007309789972675246
Epoch 932
Epoch 932 :: Batch 0/1
Batch Loss = 0.0007309789972675246
932, epoch_train_loss=0.0007309789972675246
Epoch 933
Epoch 933 :: Batch 0/1
Batch Loss = 0.0007309789972675246
933, epoch_train_loss=0.0007309789972675246
Epoch 934
Epoch 934 :: Batch 0/1
Batch Loss = 0.0007309789972675246
934, epoch_train_loss=0.0007309789972675246
Epoch 935
Epoch 935 :: Batch 0/1
Batch Loss = 0.0007309789972675246
935, epoch_train_loss=0.0007309789972675246
Epoch 936
Epoch 936 :: Batch 0/1
Batch Loss = 0.0007309789972675246
936, epoch_train_loss=0.0007309789972675246
Epoch 937
Epoch 937 :: Batch 0/1
Batch Loss = 0.0007309789972675246
937, epoch_train_loss=0.0007309789972675246
Epoch 938
Epoch 938 :: Batch 0/1
Batch Loss = 0.0007309789972675246
938, epoch_train_loss=0.0007309789972675246
Epoch 939
Epoch 939 :: Batch 0/1
Batch Loss = 0.0007309789972675246
939, epoch_train_loss=0.0007309789972675246
Epoch 940
Epoch 940 :: Batch 0/1
Batch Loss = 0.0007309789972675246
940, epoch_train_loss=0.0007309789972675246
Epoch 941
Epoch 941 :: Batch 0/1
Batch Loss = 0.0007309789972675246
941, epoch_train_loss=0.0007309789972675246
Epoch 942
Epoch 942 :: Batch 0/1
Batch Loss = 0.0007309789972675246
942, epoch_train_loss=0.0007309789972675246
Epoch 943
Epoch 943 :: Batch 0/1
Batch Loss = 0.0007309789972675246
943, epoch_train_loss=0.0007309789972675246
Epoch 944
Epoch 944 :: Batch 0/1
Batch Loss = 0.0007309789972675246
944, epoch_train_loss=0.0007309789972675246
Epoch 945
Epoch 945 :: Batch 0/1
Batch Loss = 0.0007309789972675246
945, epoch_train_loss=0.0007309789972675246
Epoch 946
Epoch 946 :: Batch 0/1
Batch Loss = 0.0007309789972675246
946, epoch_train_loss=0.0007309789972675246
Epoch 947
Epoch 947 :: Batch 0/1
Batch Loss = 0.0007309789972675246
947, epoch_train_loss=0.0007309789972675246
Epoch 948
Epoch 948 :: Batch 0/1
Batch Loss = 0.0007309789972675246
948, epoch_train_loss=0.0007309789972675246
Epoch 949
Epoch 949 :: Batch 0/1
Batch Loss = 0.0007309789972675246
949, epoch_train_loss=0.0007309789972675246
Epoch 950
Epoch 950 :: Batch 0/1
Batch Loss = 0.0007309789972675246
950, epoch_train_loss=0.0007309789972675246
Epoch 951
Epoch 951 :: Batch 0/1
Batch Loss = 0.0007309789972675246
951, epoch_train_loss=0.0007309789972675246
Epoch 952
Epoch 952 :: Batch 0/1
Batch Loss = 0.0007309789972675246
952, epoch_train_loss=0.0007309789972675246
Epoch 953
Epoch 953 :: Batch 0/1
Batch Loss = 0.0007309789972675246
953, epoch_train_loss=0.0007309789972675246
Epoch 954
Epoch 954 :: Batch 0/1
Batch Loss = 0.0007309789972675246
954, epoch_train_loss=0.0007309789972675246
Epoch 955
Epoch 955 :: Batch 0/1
Batch Loss = 0.0007309789972675246
955, epoch_train_loss=0.0007309789972675246
Epoch 956
Epoch 956 :: Batch 0/1
Batch Loss = 0.0007309789972675246
956, epoch_train_loss=0.0007309789972675246
Epoch 957
Epoch 957 :: Batch 0/1
Batch Loss = 0.0007309789972675246
957, epoch_train_loss=0.0007309789972675246
Epoch 958
Epoch 958 :: Batch 0/1
Batch Loss = 0.0007309789972675246
958, epoch_train_loss=0.0007309789972675246
Epoch 959
Epoch 959 :: Batch 0/1
Batch Loss = 0.0007309789972675246
959, epoch_train_loss=0.0007309789972675246
Epoch 960
Epoch 960 :: Batch 0/1
Batch Loss = 0.0007309789972675246
960, epoch_train_loss=0.0007309789972675246
Epoch 961
Epoch 961 :: Batch 0/1
Batch Loss = 0.0007309789972675246
961, epoch_train_loss=0.0007309789972675246
Epoch 962
Epoch 962 :: Batch 0/1
Batch Loss = 0.0007309789972675246
962, epoch_train_loss=0.0007309789972675246
Epoch 963
Epoch 963 :: Batch 0/1
Batch Loss = 0.0007309789972675246
963, epoch_train_loss=0.0007309789972675246
Epoch 964
Epoch 964 :: Batch 0/1
Batch Loss = 0.0007309789972675246
964, epoch_train_loss=0.0007309789972675246
Epoch 965
Epoch 965 :: Batch 0/1
Batch Loss = 0.0007309789972675246
965, epoch_train_loss=0.0007309789972675246
Epoch 966
Epoch 966 :: Batch 0/1
Batch Loss = 0.0007309789972675246
966, epoch_train_loss=0.0007309789972675246
Epoch 967
Epoch 967 :: Batch 0/1
Batch Loss = 0.0007309789972675246
967, epoch_train_loss=0.0007309789972675246
Epoch 968
Epoch 968 :: Batch 0/1
Batch Loss = 0.0007309789972675246
968, epoch_train_loss=0.0007309789972675246
Epoch 969
Epoch 969 :: Batch 0/1
Batch Loss = 0.0007309789972675246
969, epoch_train_loss=0.0007309789972675246
Epoch 970
Epoch 970 :: Batch 0/1
Batch Loss = 0.0007309789972675246
970, epoch_train_loss=0.0007309789972675246
Epoch 971
Epoch 971 :: Batch 0/1
Batch Loss = 0.0007309789972675246
971, epoch_train_loss=0.0007309789972675246
Epoch 972
Epoch 972 :: Batch 0/1
Batch Loss = 0.0007309789972675246
972, epoch_train_loss=0.0007309789972675246
Epoch 973
Epoch 973 :: Batch 0/1
Batch Loss = 0.0007309789972675246
973, epoch_train_loss=0.0007309789972675246
Epoch 974
Epoch 974 :: Batch 0/1
Batch Loss = 0.0007309789972675246
974, epoch_train_loss=0.0007309789972675246
Epoch 975
Epoch 975 :: Batch 0/1
Batch Loss = 0.0007309789972675246
975, epoch_train_loss=0.0007309789972675246
Epoch 976
Epoch 976 :: Batch 0/1
Batch Loss = 0.0007309789972675246
976, epoch_train_loss=0.0007309789972675246
Epoch 977
Epoch 977 :: Batch 0/1
Batch Loss = 0.0007309789972675246
977, epoch_train_loss=0.0007309789972675246
Epoch 978
Epoch 978 :: Batch 0/1
Batch Loss = 0.0007309789972675246
978, epoch_train_loss=0.0007309789972675246
Epoch 979
Epoch 979 :: Batch 0/1
Batch Loss = 0.0007309789972675246
979, epoch_train_loss=0.0007309789972675246
Epoch 980
Epoch 980 :: Batch 0/1
Batch Loss = 0.0007309789972675246
980, epoch_train_loss=0.0007309789972675246
Epoch 981
Epoch 981 :: Batch 0/1
Batch Loss = 0.0007309789972675246
981, epoch_train_loss=0.0007309789972675246
Epoch 982
Epoch 982 :: Batch 0/1
Batch Loss = 0.0007309789972675246
982, epoch_train_loss=0.0007309789972675246
Epoch 983
Epoch 983 :: Batch 0/1
Batch Loss = 0.0007309789972675246
983, epoch_train_loss=0.0007309789972675246
Epoch 984
Epoch 984 :: Batch 0/1
Batch Loss = 0.0007309789972675246
984, epoch_train_loss=0.0007309789972675246
Epoch 985
Epoch 985 :: Batch 0/1
Batch Loss = 0.0007309789972675246
985, epoch_train_loss=0.0007309789972675246
Epoch 986
Epoch 986 :: Batch 0/1
Batch Loss = 0.0007309789972675246
986, epoch_train_loss=0.0007309789972675246
Epoch 987
Epoch 987 :: Batch 0/1
Batch Loss = 0.0007309789972675246
987, epoch_train_loss=0.0007309789972675246
Epoch 988
Epoch 988 :: Batch 0/1
Batch Loss = 0.0007309789972675246
988, epoch_train_loss=0.0007309789972675246
Epoch 989
Epoch 989 :: Batch 0/1
Batch Loss = 0.0007309789972675246
989, epoch_train_loss=0.0007309789972675246
Epoch 990
Epoch 990 :: Batch 0/1
Batch Loss = 0.0007309789972675246
990, epoch_train_loss=0.0007309789972675246
Epoch 991
Epoch 991 :: Batch 0/1
Batch Loss = 0.0007309789972675246
991, epoch_train_loss=0.0007309789972675246
Epoch 992
Epoch 992 :: Batch 0/1
Batch Loss = 0.0007309789972675246
992, epoch_train_loss=0.0007309789972675246
Epoch 993
Epoch 993 :: Batch 0/1
Batch Loss = 0.0007309789972675246
993, epoch_train_loss=0.0007309789972675246
Epoch 994
Epoch 994 :: Batch 0/1
Batch Loss = 0.0007309789972675246
994, epoch_train_loss=0.0007309789972675246
Epoch 995
Epoch 995 :: Batch 0/1
Batch Loss = 0.0007309789972675246
995, epoch_train_loss=0.0007309789972675246
Epoch 996
Epoch 996 :: Batch 0/1
Batch Loss = 0.0007309789972675246
996, epoch_train_loss=0.0007309789972675246
Epoch 997
Epoch 997 :: Batch 0/1
Batch Loss = 0.0007309789972675246
997, epoch_train_loss=0.0007309789972675246
Epoch 998
Epoch 998 :: Batch 0/1
Batch Loss = 0.0007309789972675246
998, epoch_train_loss=0.0007309789972675246
Epoch 999
Epoch 999 :: Batch 0/1
Batch Loss = 0.0007309789972675246
999, epoch_train_loss=0.0007309789972675246
Epoch 1000
Epoch 1000 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1000, epoch_train_loss=0.0007309789972675246
Epoch 1001
Epoch 1001 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1001, epoch_train_loss=0.0007309789972675246
Epoch 1002
Epoch 1002 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1002, epoch_train_loss=0.0007309789972675246
Epoch 1003
Epoch 1003 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1003, epoch_train_loss=0.0007309789972675246
Epoch 1004
Epoch 1004 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1004, epoch_train_loss=0.0007309789972675246
Epoch 1005
Epoch 1005 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1005, epoch_train_loss=0.0007309789972675246
Epoch 1006
Epoch 1006 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1006, epoch_train_loss=0.0007309789972675246
Epoch 1007
Epoch 1007 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1007, epoch_train_loss=0.0007309789972675246
Epoch 1008
Epoch 1008 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1008, epoch_train_loss=0.0007309789972675246
Epoch 1009
Epoch 1009 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1009, epoch_train_loss=0.0007309789972675246
Epoch 1010
Epoch 1010 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1010, epoch_train_loss=0.0007309789972675246
Epoch 1011
Epoch 1011 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1011, epoch_train_loss=0.0007309789972675246
Epoch 1012
Epoch 1012 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1012, epoch_train_loss=0.0007309789972675246
Epoch 1013
Epoch 1013 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1013, epoch_train_loss=0.0007309789972675246
Epoch 1014
Epoch 1014 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1014, epoch_train_loss=0.0007309789972675246
Epoch 1015
Epoch 1015 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1015, epoch_train_loss=0.0007309789972675246
Epoch 1016
Epoch 1016 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1016, epoch_train_loss=0.0007309789972675246
Epoch 1017
Epoch 1017 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1017, epoch_train_loss=0.0007309789972675246
Epoch 1018
Epoch 1018 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1018, epoch_train_loss=0.0007309789972675246
Epoch 1019
Epoch 1019 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1019, epoch_train_loss=0.0007309789972675246
Epoch 1020
Epoch 1020 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1020, epoch_train_loss=0.0007309789972675246
Epoch 1021
Epoch 1021 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1021, epoch_train_loss=0.0007309789972675246
Epoch 1022
Epoch 1022 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1022, epoch_train_loss=0.0007309789972675246
Epoch 1023
Epoch 1023 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1023, epoch_train_loss=0.0007309789972675246
Epoch 1024
Epoch 1024 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1024, epoch_train_loss=0.0007309789972675246
Epoch 1025
Epoch 1025 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1025, epoch_train_loss=0.0007309789972675246
Epoch 1026
Epoch 1026 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1026, epoch_train_loss=0.0007309789972675246
Epoch 1027
Epoch 1027 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1027, epoch_train_loss=0.0007309789972675246
Epoch 1028
Epoch 1028 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1028, epoch_train_loss=0.0007309789972675246
Epoch 1029
Epoch 1029 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1029, epoch_train_loss=0.0007309789972675246
Epoch 1030
Epoch 1030 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1030, epoch_train_loss=0.0007309789972675246
Epoch 1031
Epoch 1031 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1031, epoch_train_loss=0.0007309789972675246
Epoch 1032
Epoch 1032 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1032, epoch_train_loss=0.0007309789972675246
Epoch 1033
Epoch 1033 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1033, epoch_train_loss=0.0007309789972675246
Epoch 1034
Epoch 1034 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1034, epoch_train_loss=0.0007309789972675246
Epoch 1035
Epoch 1035 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1035, epoch_train_loss=0.0007309789972675246
Epoch 1036
Epoch 1036 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1036, epoch_train_loss=0.0007309789972675246
Epoch 1037
Epoch 1037 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1037, epoch_train_loss=0.0007309789972675246
Epoch 1038
Epoch 1038 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1038, epoch_train_loss=0.0007309789972675246
Epoch 1039
Epoch 1039 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1039, epoch_train_loss=0.0007309789972675246
Epoch 1040
Epoch 1040 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1040, epoch_train_loss=0.0007309789972675246
Epoch 1041
Epoch 1041 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1041, epoch_train_loss=0.0007309789972675246
Epoch 1042
Epoch 1042 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1042, epoch_train_loss=0.0007309789972675246
Epoch 1043
Epoch 1043 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1043, epoch_train_loss=0.0007309789972675246
Epoch 1044
Epoch 1044 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1044, epoch_train_loss=0.0007309789972675246
Epoch 1045
Epoch 1045 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1045, epoch_train_loss=0.0007309789972675246
Epoch 1046
Epoch 1046 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1046, epoch_train_loss=0.0007309789972675246
Epoch 1047
Epoch 1047 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1047, epoch_train_loss=0.0007309789972675246
Epoch 1048
Epoch 1048 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1048, epoch_train_loss=0.0007309789972675246
Epoch 1049
Epoch 1049 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1049, epoch_train_loss=0.0007309789972675246
Epoch 1050
Epoch 1050 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1050, epoch_train_loss=0.0007309789972675246
Epoch 1051
Epoch 1051 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1051, epoch_train_loss=0.0007309789972675246
Epoch 1052
Epoch 1052 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1052, epoch_train_loss=0.0007309789972675246
Epoch 1053
Epoch 1053 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1053, epoch_train_loss=0.0007309789972675246
Epoch 1054
Epoch 1054 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1054, epoch_train_loss=0.0007309789972675246
Epoch 1055
Epoch 1055 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1055, epoch_train_loss=0.0007309789972675246
Epoch 1056
Epoch 1056 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1056, epoch_train_loss=0.0007309789972675246
Epoch 1057
Epoch 1057 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1057, epoch_train_loss=0.0007309789972675246
Epoch 1058
Epoch 1058 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1058, epoch_train_loss=0.0007309789972675246
Epoch 1059
Epoch 1059 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1059, epoch_train_loss=0.0007309789972675246
Epoch 1060
Epoch 1060 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1060, epoch_train_loss=0.0007309789972675246
Epoch 1061
Epoch 1061 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1061, epoch_train_loss=0.0007309789972675246
Epoch 1062
Epoch 1062 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1062, epoch_train_loss=0.0007309789972675246
Epoch 1063
Epoch 1063 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1063, epoch_train_loss=0.0007309789972675246
Epoch 1064
Epoch 1064 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1064, epoch_train_loss=0.0007309789972675246
Epoch 1065
Epoch 1065 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1065, epoch_train_loss=0.0007309789972675246
Epoch 1066
Epoch 1066 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1066, epoch_train_loss=0.0007309789972675246
Epoch 1067
Epoch 1067 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1067, epoch_train_loss=0.0007309789972675246
Epoch 1068
Epoch 1068 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1068, epoch_train_loss=0.0007309789972675246
Epoch 1069
Epoch 1069 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1069, epoch_train_loss=0.0007309789972675246
Epoch 1070
Epoch 1070 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1070, epoch_train_loss=0.0007309789972675246
Epoch 1071
Epoch 1071 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1071, epoch_train_loss=0.0007309789972675246
Epoch 1072
Epoch 1072 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1072, epoch_train_loss=0.0007309789972675246
Epoch 1073
Epoch 1073 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1073, epoch_train_loss=0.0007309789972675246
Epoch 1074
Epoch 1074 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1074, epoch_train_loss=0.0007309789972675246
Epoch 1075
Epoch 1075 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1075, epoch_train_loss=0.0007309789972675246
Epoch 1076
Epoch 1076 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1076, epoch_train_loss=0.0007309789972675246
Epoch 1077
Epoch 1077 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1077, epoch_train_loss=0.0007309789972675246
Epoch 1078
Epoch 1078 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1078, epoch_train_loss=0.0007309789972675246
Epoch 1079
Epoch 1079 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1079, epoch_train_loss=0.0007309789972675246
Epoch 1080
Epoch 1080 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1080, epoch_train_loss=0.0007309789972675246
Epoch 1081
Epoch 1081 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1081, epoch_train_loss=0.0007309789972675246
Epoch 1082
Epoch 1082 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1082, epoch_train_loss=0.0007309789972675246
Epoch 1083
Epoch 1083 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1083, epoch_train_loss=0.0007309789972675246
Epoch 1084
Epoch 1084 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1084, epoch_train_loss=0.0007309789972675246
Epoch 1085
Epoch 1085 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1085, epoch_train_loss=0.0007309789972675246
Epoch 1086
Epoch 1086 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1086, epoch_train_loss=0.0007309789972675246
Epoch 1087
Epoch 1087 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1087, epoch_train_loss=0.0007309789972675246
Epoch 1088
Epoch 1088 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1088, epoch_train_loss=0.0007309789972675246
Epoch 1089
Epoch 1089 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1089, epoch_train_loss=0.0007309789972675246
Epoch 1090
Epoch 1090 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1090, epoch_train_loss=0.0007309789972675246
Epoch 1091
Epoch 1091 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1091, epoch_train_loss=0.0007309789972675246
Epoch 1092
Epoch 1092 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1092, epoch_train_loss=0.0007309789972675246
Epoch 1093
Epoch 1093 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1093, epoch_train_loss=0.0007309789972675246
Epoch 1094
Epoch 1094 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1094, epoch_train_loss=0.0007309789972675246
Epoch 1095
Epoch 1095 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1095, epoch_train_loss=0.0007309789972675246
Epoch 1096
Epoch 1096 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1096, epoch_train_loss=0.0007309789972675246
Epoch 1097
Epoch 1097 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1097, epoch_train_loss=0.0007309789972675246
Epoch 1098
Epoch 1098 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1098, epoch_train_loss=0.0007309789972675246
Epoch 1099
Epoch 1099 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1099, epoch_train_loss=0.0007309789972675246
Epoch 1100
Epoch 1100 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1100, epoch_train_loss=0.0007309789972675246
Epoch 1101
Epoch 1101 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1101, epoch_train_loss=0.0007309789972675246
Epoch 1102
Epoch 1102 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1102, epoch_train_loss=0.0007309789972675246
Epoch 1103
Epoch 1103 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1103, epoch_train_loss=0.0007309789972675246
Epoch 1104
Epoch 1104 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1104, epoch_train_loss=0.0007309789972675246
Epoch 1105
Epoch 1105 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1105, epoch_train_loss=0.0007309789972675246
Epoch 1106
Epoch 1106 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1106, epoch_train_loss=0.0007309789972675246
Epoch 1107
Epoch 1107 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1107, epoch_train_loss=0.0007309789972675246
Epoch 1108
Epoch 1108 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1108, epoch_train_loss=0.0007309789972675246
Epoch 1109
Epoch 1109 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1109, epoch_train_loss=0.0007309789972675246
Epoch 1110
Epoch 1110 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1110, epoch_train_loss=0.0007309789972675246
Epoch 1111
Epoch 1111 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1111, epoch_train_loss=0.0007309789972675246
Epoch 1112
Epoch 1112 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1112, epoch_train_loss=0.0007309789972675246
Epoch 1113
Epoch 1113 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1113, epoch_train_loss=0.0007309789972675246
Epoch 1114
Epoch 1114 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1114, epoch_train_loss=0.0007309789972675246
Epoch 1115
Epoch 1115 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1115, epoch_train_loss=0.0007309789972675246
Epoch 1116
Epoch 1116 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1116, epoch_train_loss=0.0007309789972675246
Epoch 1117
Epoch 1117 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1117, epoch_train_loss=0.0007309789972675246
Epoch 1118
Epoch 1118 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1118, epoch_train_loss=0.0007309789972675246
Epoch 1119
Epoch 1119 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1119, epoch_train_loss=0.0007309789972675246
Epoch 1120
Epoch 1120 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1120, epoch_train_loss=0.0007309789972675246
Epoch 1121
Epoch 1121 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1121, epoch_train_loss=0.0007309789972675246
Epoch 1122
Epoch 1122 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1122, epoch_train_loss=0.0007309789972675246
Epoch 1123
Epoch 1123 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1123, epoch_train_loss=0.0007309789972675246
Epoch 1124
Epoch 1124 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1124, epoch_train_loss=0.0007309789972675246
Epoch 1125
Epoch 1125 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1125, epoch_train_loss=0.0007309789972675246
Epoch 1126
Epoch 1126 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1126, epoch_train_loss=0.0007309789972675246
Epoch 1127
Epoch 1127 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1127, epoch_train_loss=0.0007309789972675246
Epoch 1128
Epoch 1128 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1128, epoch_train_loss=0.0007309789972675246
Epoch 1129
Epoch 1129 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1129, epoch_train_loss=0.0007309789972675246
Epoch 1130
Epoch 1130 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1130, epoch_train_loss=0.0007309789972675246
Epoch 1131
Epoch 1131 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1131, epoch_train_loss=0.0007309789972675246
Epoch 1132
Epoch 1132 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1132, epoch_train_loss=0.0007309789972675246
Epoch 1133
Epoch 1133 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1133, epoch_train_loss=0.0007309789972675246
Epoch 1134
Epoch 1134 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1134, epoch_train_loss=0.0007309789972675246
Epoch 1135
Epoch 1135 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1135, epoch_train_loss=0.0007309789972675246
Epoch 1136
Epoch 1136 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1136, epoch_train_loss=0.0007309789972675246
Epoch 1137
Epoch 1137 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1137, epoch_train_loss=0.0007309789972675246
Epoch 1138
Epoch 1138 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1138, epoch_train_loss=0.0007309789972675246
Epoch 1139
Epoch 1139 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1139, epoch_train_loss=0.0007309789972675246
Epoch 1140
Epoch 1140 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1140, epoch_train_loss=0.0007309789972675246
Epoch 1141
Epoch 1141 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1141, epoch_train_loss=0.0007309789972675246
Epoch 1142
Epoch 1142 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1142, epoch_train_loss=0.0007309789972675246
Epoch 1143
Epoch 1143 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1143, epoch_train_loss=0.0007309789972675246
Epoch 1144
Epoch 1144 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1144, epoch_train_loss=0.0007309789972675246
Epoch 1145
Epoch 1145 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1145, epoch_train_loss=0.0007309789972675246
Epoch 1146
Epoch 1146 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1146, epoch_train_loss=0.0007309789972675246
Epoch 1147
Epoch 1147 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1147, epoch_train_loss=0.0007309789972675246
Epoch 1148
Epoch 1148 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1148, epoch_train_loss=0.0007309789972675246
Epoch 1149
Epoch 1149 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1149, epoch_train_loss=0.0007309789972675246
Epoch 1150
Epoch 1150 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1150, epoch_train_loss=0.0007309789972675246
Epoch 1151
Epoch 1151 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1151, epoch_train_loss=0.0007309789972675246
Epoch 1152
Epoch 1152 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1152, epoch_train_loss=0.0007309789972675246
Epoch 1153
Epoch 1153 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1153, epoch_train_loss=0.0007309789972675246
Epoch 1154
Epoch 1154 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1154, epoch_train_loss=0.0007309789972675246
Epoch 1155
Epoch 1155 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1155, epoch_train_loss=0.0007309789972675246
Epoch 1156
Epoch 1156 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1156, epoch_train_loss=0.0007309789972675246
Epoch 1157
Epoch 1157 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1157, epoch_train_loss=0.0007309789972675246
Epoch 1158
Epoch 1158 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1158, epoch_train_loss=0.0007309789972675246
Epoch 1159
Epoch 1159 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1159, epoch_train_loss=0.0007309789972675246
Epoch 1160
Epoch 1160 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1160, epoch_train_loss=0.0007309789972675246
Epoch 1161
Epoch 1161 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1161, epoch_train_loss=0.0007309789972675246
Epoch 1162
Epoch 1162 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1162, epoch_train_loss=0.0007309789972675246
Epoch 1163
Epoch 1163 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1163, epoch_train_loss=0.0007309789972675246
Epoch 1164
Epoch 1164 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1164, epoch_train_loss=0.0007309789972675246
Epoch 1165
Epoch 1165 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1165, epoch_train_loss=0.0007309789972675246
Epoch 1166
Epoch 1166 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1166, epoch_train_loss=0.0007309789972675246
Epoch 1167
Epoch 1167 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1167, epoch_train_loss=0.0007309789972675246
Epoch 1168
Epoch 1168 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1168, epoch_train_loss=0.0007309789972675246
Epoch 1169
Epoch 1169 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1169, epoch_train_loss=0.0007309789972675246
Epoch 1170
Epoch 1170 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1170, epoch_train_loss=0.0007309789972675246
Epoch 1171
Epoch 1171 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1171, epoch_train_loss=0.0007309789972675246
Epoch 1172
Epoch 1172 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1172, epoch_train_loss=0.0007309789972675246
Epoch 1173
Epoch 1173 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1173, epoch_train_loss=0.0007309789972675246
Epoch 1174
Epoch 1174 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1174, epoch_train_loss=0.0007309789972675246
Epoch 1175
Epoch 1175 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1175, epoch_train_loss=0.0007309789972675246
Epoch 1176
Epoch 1176 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1176, epoch_train_loss=0.0007309789972675246
Epoch 1177
Epoch 1177 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1177, epoch_train_loss=0.0007309789972675246
Epoch 1178
Epoch 1178 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1178, epoch_train_loss=0.0007309789972675246
Epoch 1179
Epoch 1179 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1179, epoch_train_loss=0.0007309789972675246
Epoch 1180
Epoch 1180 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1180, epoch_train_loss=0.0007309789972675246
Epoch 1181
Epoch 1181 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1181, epoch_train_loss=0.0007309789972675246
Epoch 1182
Epoch 1182 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1182, epoch_train_loss=0.0007309789972675246
Epoch 1183
Epoch 1183 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1183, epoch_train_loss=0.0007309789972675246
Epoch 1184
Epoch 1184 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1184, epoch_train_loss=0.0007309789972675246
Epoch 1185
Epoch 1185 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1185, epoch_train_loss=0.0007309789972675246
Epoch 1186
Epoch 1186 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1186, epoch_train_loss=0.0007309789972675246
Epoch 1187
Epoch 1187 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1187, epoch_train_loss=0.0007309789972675246
Epoch 1188
Epoch 1188 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1188, epoch_train_loss=0.0007309789972675246
Epoch 1189
Epoch 1189 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1189, epoch_train_loss=0.0007309789972675246
Epoch 1190
Epoch 1190 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1190, epoch_train_loss=0.0007309789972675246
Epoch 1191
Epoch 1191 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1191, epoch_train_loss=0.0007309789972675246
Epoch 1192
Epoch 1192 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1192, epoch_train_loss=0.0007309789972675246
Epoch 1193
Epoch 1193 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1193, epoch_train_loss=0.0007309789972675246
Epoch 1194
Epoch 1194 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1194, epoch_train_loss=0.0007309789972675246
Epoch 1195
Epoch 1195 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1195, epoch_train_loss=0.0007309789972675246
Epoch 1196
Epoch 1196 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1196, epoch_train_loss=0.0007309789972675246
Epoch 1197
Epoch 1197 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1197, epoch_train_loss=0.0007309789972675246
Epoch 1198
Epoch 1198 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1198, epoch_train_loss=0.0007309789972675246
Epoch 1199
Epoch 1199 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1199, epoch_train_loss=0.0007309789972675246
Epoch 1200
Epoch 1200 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1200, epoch_train_loss=0.0007309789972675246
Epoch 1201
Epoch 1201 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1201, epoch_train_loss=0.0007309789972675246
Epoch 1202
Epoch 1202 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1202, epoch_train_loss=0.0007309789972675246
Epoch 1203
Epoch 1203 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1203, epoch_train_loss=0.0007309789972675246
Epoch 1204
Epoch 1204 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1204, epoch_train_loss=0.0007309789972675246
Epoch 1205
Epoch 1205 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1205, epoch_train_loss=0.0007309789972675246
Epoch 1206
Epoch 1206 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1206, epoch_train_loss=0.0007309789972675246
Epoch 1207
Epoch 1207 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1207, epoch_train_loss=0.0007309789972675246
Epoch 1208
Epoch 1208 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1208, epoch_train_loss=0.0007309789972675246
Epoch 1209
Epoch 1209 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1209, epoch_train_loss=0.0007309789972675246
Epoch 1210
Epoch 1210 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1210, epoch_train_loss=0.0007309789972675246
Epoch 1211
Epoch 1211 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1211, epoch_train_loss=0.0007309789972675246
Epoch 1212
Epoch 1212 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1212, epoch_train_loss=0.0007309789972675246
Epoch 1213
Epoch 1213 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1213, epoch_train_loss=0.0007309789972675246
Epoch 1214
Epoch 1214 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1214, epoch_train_loss=0.0007309789972675246
Epoch 1215
Epoch 1215 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1215, epoch_train_loss=0.0007309789972675246
Epoch 1216
Epoch 1216 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1216, epoch_train_loss=0.0007309789972675246
Epoch 1217
Epoch 1217 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1217, epoch_train_loss=0.0007309789972675246
Epoch 1218
Epoch 1218 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1218, epoch_train_loss=0.0007309789972675246
Epoch 1219
Epoch 1219 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1219, epoch_train_loss=0.0007309789972675246
Epoch 1220
Epoch 1220 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1220, epoch_train_loss=0.0007309789972675246
Epoch 1221
Epoch 1221 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1221, epoch_train_loss=0.0007309789972675246
Epoch 1222
Epoch 1222 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1222, epoch_train_loss=0.0007309789972675246
Epoch 1223
Epoch 1223 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1223, epoch_train_loss=0.0007309789972675246
Epoch 1224
Epoch 1224 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1224, epoch_train_loss=0.0007309789972675246
Epoch 1225
Epoch 1225 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1225, epoch_train_loss=0.0007309789972675246
Epoch 1226
Epoch 1226 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1226, epoch_train_loss=0.0007309789972675246
Epoch 1227
Epoch 1227 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1227, epoch_train_loss=0.0007309789972675246
Epoch 1228
Epoch 1228 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1228, epoch_train_loss=0.0007309789972675246
Epoch 1229
Epoch 1229 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1229, epoch_train_loss=0.0007309789972675246
Epoch 1230
Epoch 1230 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1230, epoch_train_loss=0.0007309789972675246
Epoch 1231
Epoch 1231 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1231, epoch_train_loss=0.0007309789972675246
Epoch 1232
Epoch 1232 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1232, epoch_train_loss=0.0007309789972675246
Epoch 1233
Epoch 1233 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1233, epoch_train_loss=0.0007309789972675246
Epoch 1234
Epoch 1234 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1234, epoch_train_loss=0.0007309789972675246
Epoch 1235
Epoch 1235 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1235, epoch_train_loss=0.0007309789972675246
Epoch 1236
Epoch 1236 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1236, epoch_train_loss=0.0007309789972675246
Epoch 1237
Epoch 1237 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1237, epoch_train_loss=0.0007309789972675246
Epoch 1238
Epoch 1238 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1238, epoch_train_loss=0.0007309789972675246
Epoch 1239
Epoch 1239 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1239, epoch_train_loss=0.0007309789972675246
Epoch 1240
Epoch 1240 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1240, epoch_train_loss=0.0007309789972675246
Epoch 1241
Epoch 1241 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1241, epoch_train_loss=0.0007309789972675246
Epoch 1242
Epoch 1242 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1242, epoch_train_loss=0.0007309789972675246
Epoch 1243
Epoch 1243 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1243, epoch_train_loss=0.0007309789972675246
Epoch 1244
Epoch 1244 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1244, epoch_train_loss=0.0007309789972675246
Epoch 1245
Epoch 1245 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1245, epoch_train_loss=0.0007309789972675246
Epoch 1246
Epoch 1246 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1246, epoch_train_loss=0.0007309789972675246
Epoch 1247
Epoch 1247 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1247, epoch_train_loss=0.0007309789972675246
Epoch 1248
Epoch 1248 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1248, epoch_train_loss=0.0007309789972675246
Epoch 1249
Epoch 1249 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1249, epoch_train_loss=0.0007309789972675246
Epoch 1250
Epoch 1250 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1250, epoch_train_loss=0.0007309789972675246
Epoch 1251
Epoch 1251 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1251, epoch_train_loss=0.0007309789972675246
Epoch 1252
Epoch 1252 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1252, epoch_train_loss=0.0007309789972675246
Epoch 1253
Epoch 1253 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1253, epoch_train_loss=0.0007309789972675246
Epoch 1254
Epoch 1254 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1254, epoch_train_loss=0.0007309789972675246
Epoch 1255
Epoch 1255 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1255, epoch_train_loss=0.0007309789972675246
Epoch 1256
Epoch 1256 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1256, epoch_train_loss=0.0007309789972675246
Epoch 1257
Epoch 1257 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1257, epoch_train_loss=0.0007309789972675246
Epoch 1258
Epoch 1258 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1258, epoch_train_loss=0.0007309789972675246
Epoch 1259
Epoch 1259 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1259, epoch_train_loss=0.0007309789972675246
Epoch 1260
Epoch 1260 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1260, epoch_train_loss=0.0007309789972675246
Epoch 1261
Epoch 1261 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1261, epoch_train_loss=0.0007309789972675246
Epoch 1262
Epoch 1262 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1262, epoch_train_loss=0.0007309789972675246
Epoch 1263
Epoch 1263 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1263, epoch_train_loss=0.0007309789972675246
Epoch 1264
Epoch 1264 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1264, epoch_train_loss=0.0007309789972675246
Epoch 1265
Epoch 1265 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1265, epoch_train_loss=0.0007309789972675246
Epoch 1266
Epoch 1266 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1266, epoch_train_loss=0.0007309789972675246
Epoch 1267
Epoch 1267 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1267, epoch_train_loss=0.0007309789972675246
Epoch 1268
Epoch 1268 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1268, epoch_train_loss=0.0007309789972675246
Epoch 1269
Epoch 1269 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1269, epoch_train_loss=0.0007309789972675246
Epoch 1270
Epoch 1270 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1270, epoch_train_loss=0.0007309789972675246
Epoch 1271
Epoch 1271 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1271, epoch_train_loss=0.0007309789972675246
Epoch 1272
Epoch 1272 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1272, epoch_train_loss=0.0007309789972675246
Epoch 1273
Epoch 1273 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1273, epoch_train_loss=0.0007309789972675246
Epoch 1274
Epoch 1274 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1274, epoch_train_loss=0.0007309789972675246
Epoch 1275
Epoch 1275 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1275, epoch_train_loss=0.0007309789972675246
Epoch 1276
Epoch 1276 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1276, epoch_train_loss=0.0007309789972675246
Epoch 1277
Epoch 1277 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1277, epoch_train_loss=0.0007309789972675246
Epoch 1278
Epoch 1278 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1278, epoch_train_loss=0.0007309789972675246
Epoch 1279
Epoch 1279 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1279, epoch_train_loss=0.0007309789972675246
Epoch 1280
Epoch 1280 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1280, epoch_train_loss=0.0007309789972675246
Epoch 1281
Epoch 1281 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1281, epoch_train_loss=0.0007309789972675246
Epoch 1282
Epoch 1282 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1282, epoch_train_loss=0.0007309789972675246
Epoch 1283
Epoch 1283 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1283, epoch_train_loss=0.0007309789972675246
Epoch 1284
Epoch 1284 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1284, epoch_train_loss=0.0007309789972675246
Epoch 1285
Epoch 1285 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1285, epoch_train_loss=0.0007309789972675246
Epoch 1286
Epoch 1286 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1286, epoch_train_loss=0.0007309789972675246
Epoch 1287
Epoch 1287 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1287, epoch_train_loss=0.0007309789972675246
Epoch 1288
Epoch 1288 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1288, epoch_train_loss=0.0007309789972675246
Epoch 1289
Epoch 1289 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1289, epoch_train_loss=0.0007309789972675246
Epoch 1290
Epoch 1290 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1290, epoch_train_loss=0.0007309789972675246
Epoch 1291
Epoch 1291 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1291, epoch_train_loss=0.0007309789972675246
Epoch 1292
Epoch 1292 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1292, epoch_train_loss=0.0007309789972675246
Epoch 1293
Epoch 1293 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1293, epoch_train_loss=0.0007309789972675246
Epoch 1294
Epoch 1294 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1294, epoch_train_loss=0.0007309789972675246
Epoch 1295
Epoch 1295 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1295, epoch_train_loss=0.0007309789972675246
Epoch 1296
Epoch 1296 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1296, epoch_train_loss=0.0007309789972675246
Epoch 1297
Epoch 1297 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1297, epoch_train_loss=0.0007309789972675246
Epoch 1298
Epoch 1298 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1298, epoch_train_loss=0.0007309789972675246
Epoch 1299
Epoch 1299 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1299, epoch_train_loss=0.0007309789972675246
Epoch 1300
Epoch 1300 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1300, epoch_train_loss=0.0007309789972675246
Epoch 1301
Epoch 1301 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1301, epoch_train_loss=0.0007309789972675246
Epoch 1302
Epoch 1302 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1302, epoch_train_loss=0.0007309789972675246
Epoch 1303
Epoch 1303 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1303, epoch_train_loss=0.0007309789972675246
Epoch 1304
Epoch 1304 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1304, epoch_train_loss=0.0007309789972675246
Epoch 1305
Epoch 1305 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1305, epoch_train_loss=0.0007309789972675246
Epoch 1306
Epoch 1306 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1306, epoch_train_loss=0.0007309789972675246
Epoch 1307
Epoch 1307 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1307, epoch_train_loss=0.0007309789972675246
Epoch 1308
Epoch 1308 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1308, epoch_train_loss=0.0007309789972675246
Epoch 1309
Epoch 1309 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1309, epoch_train_loss=0.0007309789972675246
Epoch 1310
Epoch 1310 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1310, epoch_train_loss=0.0007309789972675246
Epoch 1311
Epoch 1311 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1311, epoch_train_loss=0.0007309789972675246
Epoch 1312
Epoch 1312 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1312, epoch_train_loss=0.0007309789972675246
Epoch 1313
Epoch 1313 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1313, epoch_train_loss=0.0007309789972675246
Epoch 1314
Epoch 1314 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1314, epoch_train_loss=0.0007309789972675246
Epoch 1315
Epoch 1315 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1315, epoch_train_loss=0.0007309789972675246
Epoch 1316
Epoch 1316 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1316, epoch_train_loss=0.0007309789972675246
Epoch 1317
Epoch 1317 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1317, epoch_train_loss=0.0007309789972675246
Epoch 1318
Epoch 1318 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1318, epoch_train_loss=0.0007309789972675246
Epoch 1319
Epoch 1319 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1319, epoch_train_loss=0.0007309789972675246
Epoch 1320
Epoch 1320 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1320, epoch_train_loss=0.0007309789972675246
Epoch 1321
Epoch 1321 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1321, epoch_train_loss=0.0007309789972675246
Epoch 1322
Epoch 1322 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1322, epoch_train_loss=0.0007309789972675246
Epoch 1323
Epoch 1323 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1323, epoch_train_loss=0.0007309789972675246
Epoch 1324
Epoch 1324 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1324, epoch_train_loss=0.0007309789972675246
Epoch 1325
Epoch 1325 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1325, epoch_train_loss=0.0007309789972675246
Epoch 1326
Epoch 1326 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1326, epoch_train_loss=0.0007309789972675246
Epoch 1327
Epoch 1327 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1327, epoch_train_loss=0.0007309789972675246
Epoch 1328
Epoch 1328 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1328, epoch_train_loss=0.0007309789972675246
Epoch 1329
Epoch 1329 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1329, epoch_train_loss=0.0007309789972675246
Epoch 1330
Epoch 1330 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1330, epoch_train_loss=0.0007309789972675246
Epoch 1331
Epoch 1331 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1331, epoch_train_loss=0.0007309789972675246
Epoch 1332
Epoch 1332 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1332, epoch_train_loss=0.0007309789972675246
Epoch 1333
Epoch 1333 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1333, epoch_train_loss=0.0007309789972675246
Epoch 1334
Epoch 1334 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1334, epoch_train_loss=0.0007309789972675246
Epoch 1335
Epoch 1335 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1335, epoch_train_loss=0.0007309789972675246
Epoch 1336
Epoch 1336 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1336, epoch_train_loss=0.0007309789972675246
Epoch 1337
Epoch 1337 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1337, epoch_train_loss=0.0007309789972675246
Epoch 1338
Epoch 1338 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1338, epoch_train_loss=0.0007309789972675246
Epoch 1339
Epoch 1339 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1339, epoch_train_loss=0.0007309789972675246
Epoch 1340
Epoch 1340 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1340, epoch_train_loss=0.0007309789972675246
Epoch 1341
Epoch 1341 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1341, epoch_train_loss=0.0007309789972675246
Epoch 1342
Epoch 1342 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1342, epoch_train_loss=0.0007309789972675246
Epoch 1343
Epoch 1343 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1343, epoch_train_loss=0.0007309789972675246
Epoch 1344
Epoch 1344 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1344, epoch_train_loss=0.0007309789972675246
Epoch 1345
Epoch 1345 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1345, epoch_train_loss=0.0007309789972675246
Epoch 1346
Epoch 1346 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1346, epoch_train_loss=0.0007309789972675246
Epoch 1347
Epoch 1347 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1347, epoch_train_loss=0.0007309789972675246
Epoch 1348
Epoch 1348 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1348, epoch_train_loss=0.0007309789972675246
Epoch 1349
Epoch 1349 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1349, epoch_train_loss=0.0007309789972675246
Epoch 1350
Epoch 1350 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1350, epoch_train_loss=0.0007309789972675246
Epoch 1351
Epoch 1351 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1351, epoch_train_loss=0.0007309789972675246
Epoch 1352
Epoch 1352 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1352, epoch_train_loss=0.0007309789972675246
Epoch 1353
Epoch 1353 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1353, epoch_train_loss=0.0007309789972675246
Epoch 1354
Epoch 1354 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1354, epoch_train_loss=0.0007309789972675246
Epoch 1355
Epoch 1355 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1355, epoch_train_loss=0.0007309789972675246
Epoch 1356
Epoch 1356 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1356, epoch_train_loss=0.0007309789972675246
Epoch 1357
Epoch 1357 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1357, epoch_train_loss=0.0007309789972675246
Epoch 1358
Epoch 1358 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1358, epoch_train_loss=0.0007309789972675246
Epoch 1359
Epoch 1359 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1359, epoch_train_loss=0.0007309789972675246
Epoch 1360
Epoch 1360 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1360, epoch_train_loss=0.0007309789972675246
Epoch 1361
Epoch 1361 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1361, epoch_train_loss=0.0007309789972675246
Epoch 1362
Epoch 1362 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1362, epoch_train_loss=0.0007309789972675246
Epoch 1363
Epoch 1363 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1363, epoch_train_loss=0.0007309789972675246
Epoch 1364
Epoch 1364 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1364, epoch_train_loss=0.0007309789972675246
Epoch 1365
Epoch 1365 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1365, epoch_train_loss=0.0007309789972675246
Epoch 1366
Epoch 1366 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1366, epoch_train_loss=0.0007309789972675246
Epoch 1367
Epoch 1367 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1367, epoch_train_loss=0.0007309789972675246
Epoch 1368
Epoch 1368 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1368, epoch_train_loss=0.0007309789972675246
Epoch 1369
Epoch 1369 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1369, epoch_train_loss=0.0007309789972675246
Epoch 1370
Epoch 1370 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1370, epoch_train_loss=0.0007309789972675246
Epoch 1371
Epoch 1371 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1371, epoch_train_loss=0.0007309789972675246
Epoch 1372
Epoch 1372 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1372, epoch_train_loss=0.0007309789972675246
Epoch 1373
Epoch 1373 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1373, epoch_train_loss=0.0007309789972675246
Epoch 1374
Epoch 1374 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1374, epoch_train_loss=0.0007309789972675246
Epoch 1375
Epoch 1375 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1375, epoch_train_loss=0.0007309789972675246
Epoch 1376
Epoch 1376 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1376, epoch_train_loss=0.0007309789972675246
Epoch 1377
Epoch 1377 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1377, epoch_train_loss=0.0007309789972675246
Epoch 1378
Epoch 1378 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1378, epoch_train_loss=0.0007309789972675246
Epoch 1379
Epoch 1379 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1379, epoch_train_loss=0.0007309789972675246
Epoch 1380
Epoch 1380 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1380, epoch_train_loss=0.0007309789972675246
Epoch 1381
Epoch 1381 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1381, epoch_train_loss=0.0007309789972675246
Epoch 1382
Epoch 1382 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1382, epoch_train_loss=0.0007309789972675246
Epoch 1383
Epoch 1383 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1383, epoch_train_loss=0.0007309789972675246
Epoch 1384
Epoch 1384 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1384, epoch_train_loss=0.0007309789972675246
Epoch 1385
Epoch 1385 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1385, epoch_train_loss=0.0007309789972675246
Epoch 1386
Epoch 1386 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1386, epoch_train_loss=0.0007309789972675246
Epoch 1387
Epoch 1387 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1387, epoch_train_loss=0.0007309789972675246
Epoch 1388
Epoch 1388 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1388, epoch_train_loss=0.0007309789972675246
Epoch 1389
Epoch 1389 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1389, epoch_train_loss=0.0007309789972675246
Epoch 1390
Epoch 1390 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1390, epoch_train_loss=0.0007309789972675246
Epoch 1391
Epoch 1391 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1391, epoch_train_loss=0.0007309789972675246
Epoch 1392
Epoch 1392 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1392, epoch_train_loss=0.0007309789972675246
Epoch 1393
Epoch 1393 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1393, epoch_train_loss=0.0007309789972675246
Epoch 1394
Epoch 1394 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1394, epoch_train_loss=0.0007309789972675246
Epoch 1395
Epoch 1395 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1395, epoch_train_loss=0.0007309789972675246
Epoch 1396
Epoch 1396 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1396, epoch_train_loss=0.0007309789972675246
Epoch 1397
Epoch 1397 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1397, epoch_train_loss=0.0007309789972675246
Epoch 1398
Epoch 1398 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1398, epoch_train_loss=0.0007309789972675246
Epoch 1399
Epoch 1399 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1399, epoch_train_loss=0.0007309789972675246
Epoch 1400
Epoch 1400 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1400, epoch_train_loss=0.0007309789972675246
Epoch 1401
Epoch 1401 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1401, epoch_train_loss=0.0007309789972675246
Epoch 1402
Epoch 1402 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1402, epoch_train_loss=0.0007309789972675246
Epoch 1403
Epoch 1403 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1403, epoch_train_loss=0.0007309789972675246
Epoch 1404
Epoch 1404 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1404, epoch_train_loss=0.0007309789972675246
Epoch 1405
Epoch 1405 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1405, epoch_train_loss=0.0007309789972675246
Epoch 1406
Epoch 1406 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1406, epoch_train_loss=0.0007309789972675246
Epoch 1407
Epoch 1407 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1407, epoch_train_loss=0.0007309789972675246
Epoch 1408
Epoch 1408 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1408, epoch_train_loss=0.0007309789972675246
Epoch 1409
Epoch 1409 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1409, epoch_train_loss=0.0007309789972675246
Epoch 1410
Epoch 1410 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1410, epoch_train_loss=0.0007309789972675246
Epoch 1411
Epoch 1411 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1411, epoch_train_loss=0.0007309789972675246
Epoch 1412
Epoch 1412 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1412, epoch_train_loss=0.0007309789972675246
Epoch 1413
Epoch 1413 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1413, epoch_train_loss=0.0007309789972675246
Epoch 1414
Epoch 1414 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1414, epoch_train_loss=0.0007309789972675246
Epoch 1415
Epoch 1415 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1415, epoch_train_loss=0.0007309789972675246
Epoch 1416
Epoch 1416 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1416, epoch_train_loss=0.0007309789972675246
Epoch 1417
Epoch 1417 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1417, epoch_train_loss=0.0007309789972675246
Epoch 1418
Epoch 1418 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1418, epoch_train_loss=0.0007309789972675246
Epoch 1419
Epoch 1419 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1419, epoch_train_loss=0.0007309789972675246
Epoch 1420
Epoch 1420 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1420, epoch_train_loss=0.0007309789972675246
Epoch 1421
Epoch 1421 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1421, epoch_train_loss=0.0007309789972675246
Epoch 1422
Epoch 1422 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1422, epoch_train_loss=0.0007309789972675246
Epoch 1423
Epoch 1423 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1423, epoch_train_loss=0.0007309789972675246
Epoch 1424
Epoch 1424 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1424, epoch_train_loss=0.0007309789972675246
Epoch 1425
Epoch 1425 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1425, epoch_train_loss=0.0007309789972675246
Epoch 1426
Epoch 1426 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1426, epoch_train_loss=0.0007309789972675246
Epoch 1427
Epoch 1427 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1427, epoch_train_loss=0.0007309789972675246
Epoch 1428
Epoch 1428 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1428, epoch_train_loss=0.0007309789972675246
Epoch 1429
Epoch 1429 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1429, epoch_train_loss=0.0007309789972675246
Epoch 1430
Epoch 1430 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1430, epoch_train_loss=0.0007309789972675246
Epoch 1431
Epoch 1431 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1431, epoch_train_loss=0.0007309789972675246
Epoch 1432
Epoch 1432 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1432, epoch_train_loss=0.0007309789972675246
Epoch 1433
Epoch 1433 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1433, epoch_train_loss=0.0007309789972675246
Epoch 1434
Epoch 1434 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1434, epoch_train_loss=0.0007309789972675246
Epoch 1435
Epoch 1435 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1435, epoch_train_loss=0.0007309789972675246
Epoch 1436
Epoch 1436 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1436, epoch_train_loss=0.0007309789972675246
Epoch 1437
Epoch 1437 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1437, epoch_train_loss=0.0007309789972675246
Epoch 1438
Epoch 1438 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1438, epoch_train_loss=0.0007309789972675246
Epoch 1439
Epoch 1439 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1439, epoch_train_loss=0.0007309789972675246
Epoch 1440
Epoch 1440 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1440, epoch_train_loss=0.0007309789972675246
Epoch 1441
Epoch 1441 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1441, epoch_train_loss=0.0007309789972675246
Epoch 1442
Epoch 1442 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1442, epoch_train_loss=0.0007309789972675246
Epoch 1443
Epoch 1443 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1443, epoch_train_loss=0.0007309789972675246
Epoch 1444
Epoch 1444 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1444, epoch_train_loss=0.0007309789972675246
Epoch 1445
Epoch 1445 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1445, epoch_train_loss=0.0007309789972675246
Epoch 1446
Epoch 1446 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1446, epoch_train_loss=0.0007309789972675246
Epoch 1447
Epoch 1447 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1447, epoch_train_loss=0.0007309789972675246
Epoch 1448
Epoch 1448 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1448, epoch_train_loss=0.0007309789972675246
Epoch 1449
Epoch 1449 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1449, epoch_train_loss=0.0007309789972675246
Epoch 1450
Epoch 1450 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1450, epoch_train_loss=0.0007309789972675246
Epoch 1451
Epoch 1451 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1451, epoch_train_loss=0.0007309789972675246
Epoch 1452
Epoch 1452 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1452, epoch_train_loss=0.0007309789972675246
Epoch 1453
Epoch 1453 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1453, epoch_train_loss=0.0007309789972675246
Epoch 1454
Epoch 1454 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1454, epoch_train_loss=0.0007309789972675246
Epoch 1455
Epoch 1455 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1455, epoch_train_loss=0.0007309789972675246
Epoch 1456
Epoch 1456 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1456, epoch_train_loss=0.0007309789972675246
Epoch 1457
Epoch 1457 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1457, epoch_train_loss=0.0007309789972675246
Epoch 1458
Epoch 1458 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1458, epoch_train_loss=0.0007309789972675246
Epoch 1459
Epoch 1459 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1459, epoch_train_loss=0.0007309789972675246
Epoch 1460
Epoch 1460 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1460, epoch_train_loss=0.0007309789972675246
Epoch 1461
Epoch 1461 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1461, epoch_train_loss=0.0007309789972675246
Epoch 1462
Epoch 1462 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1462, epoch_train_loss=0.0007309789972675246
Epoch 1463
Epoch 1463 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1463, epoch_train_loss=0.0007309789972675246
Epoch 1464
Epoch 1464 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1464, epoch_train_loss=0.0007309789972675246
Epoch 1465
Epoch 1465 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1465, epoch_train_loss=0.0007309789972675246
Epoch 1466
Epoch 1466 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1466, epoch_train_loss=0.0007309789972675246
Epoch 1467
Epoch 1467 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1467, epoch_train_loss=0.0007309789972675246
Epoch 1468
Epoch 1468 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1468, epoch_train_loss=0.0007309789972675246
Epoch 1469
Epoch 1469 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1469, epoch_train_loss=0.0007309789972675246
Epoch 1470
Epoch 1470 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1470, epoch_train_loss=0.0007309789972675246
Epoch 1471
Epoch 1471 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1471, epoch_train_loss=0.0007309789972675246
Epoch 1472
Epoch 1472 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1472, epoch_train_loss=0.0007309789972675246
Epoch 1473
Epoch 1473 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1473, epoch_train_loss=0.0007309789972675246
Epoch 1474
Epoch 1474 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1474, epoch_train_loss=0.0007309789972675246
Epoch 1475
Epoch 1475 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1475, epoch_train_loss=0.0007309789972675246
Epoch 1476
Epoch 1476 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1476, epoch_train_loss=0.0007309789972675246
Epoch 1477
Epoch 1477 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1477, epoch_train_loss=0.0007309789972675246
Epoch 1478
Epoch 1478 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1478, epoch_train_loss=0.0007309789972675246
Epoch 1479
Epoch 1479 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1479, epoch_train_loss=0.0007309789972675246
Epoch 1480
Epoch 1480 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1480, epoch_train_loss=0.0007309789972675246
Epoch 1481
Epoch 1481 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1481, epoch_train_loss=0.0007309789972675246
Epoch 1482
Epoch 1482 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1482, epoch_train_loss=0.0007309789972675246
Epoch 1483
Epoch 1483 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1483, epoch_train_loss=0.0007309789972675246
Epoch 1484
Epoch 1484 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1484, epoch_train_loss=0.0007309789972675246
Epoch 1485
Epoch 1485 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1485, epoch_train_loss=0.0007309789972675246
Epoch 1486
Epoch 1486 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1486, epoch_train_loss=0.0007309789972675246
Epoch 1487
Epoch 1487 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1487, epoch_train_loss=0.0007309789972675246
Epoch 1488
Epoch 1488 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1488, epoch_train_loss=0.0007309789972675246
Epoch 1489
Epoch 1489 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1489, epoch_train_loss=0.0007309789972675246
Epoch 1490
Epoch 1490 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1490, epoch_train_loss=0.0007309789972675246
Epoch 1491
Epoch 1491 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1491, epoch_train_loss=0.0007309789972675246
Epoch 1492
Epoch 1492 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1492, epoch_train_loss=0.0007309789972675246
Epoch 1493
Epoch 1493 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1493, epoch_train_loss=0.0007309789972675246
Epoch 1494
Epoch 1494 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1494, epoch_train_loss=0.0007309789972675246
Epoch 1495
Epoch 1495 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1495, epoch_train_loss=0.0007309789972675246
Epoch 1496
Epoch 1496 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1496, epoch_train_loss=0.0007309789972675246
Epoch 1497
Epoch 1497 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1497, epoch_train_loss=0.0007309789972675246
Epoch 1498
Epoch 1498 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1498, epoch_train_loss=0.0007309789972675246
Epoch 1499
Epoch 1499 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1499, epoch_train_loss=0.0007309789972675246
Epoch 1500
Epoch 1500 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1500, epoch_train_loss=0.0007309789972675246
Epoch 1501
Epoch 1501 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1501, epoch_train_loss=0.0007309789972675246
Epoch 1502
Epoch 1502 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1502, epoch_train_loss=0.0007309789972675246
Epoch 1503
Epoch 1503 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1503, epoch_train_loss=0.0007309789972675246
Epoch 1504
Epoch 1504 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1504, epoch_train_loss=0.0007309789972675246
Epoch 1505
Epoch 1505 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1505, epoch_train_loss=0.0007309789972675246
Epoch 1506
Epoch 1506 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1506, epoch_train_loss=0.0007309789972675246
Epoch 1507
Epoch 1507 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1507, epoch_train_loss=0.0007309789972675246
Epoch 1508
Epoch 1508 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1508, epoch_train_loss=0.0007309789972675246
Epoch 1509
Epoch 1509 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1509, epoch_train_loss=0.0007309789972675246
Epoch 1510
Epoch 1510 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1510, epoch_train_loss=0.0007309789972675246
Epoch 1511
Epoch 1511 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1511, epoch_train_loss=0.0007309789972675246
Epoch 1512
Epoch 1512 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1512, epoch_train_loss=0.0007309789972675246
Epoch 1513
Epoch 1513 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1513, epoch_train_loss=0.0007309789972675246
Epoch 1514
Epoch 1514 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1514, epoch_train_loss=0.0007309789972675246
Epoch 1515
Epoch 1515 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1515, epoch_train_loss=0.0007309789972675246
Epoch 1516
Epoch 1516 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1516, epoch_train_loss=0.0007309789972675246
Epoch 1517
Epoch 1517 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1517, epoch_train_loss=0.0007309789972675246
Epoch 1518
Epoch 1518 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1518, epoch_train_loss=0.0007309789972675246
Epoch 1519
Epoch 1519 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1519, epoch_train_loss=0.0007309789972675246
Epoch 1520
Epoch 1520 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1520, epoch_train_loss=0.0007309789972675246
Epoch 1521
Epoch 1521 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1521, epoch_train_loss=0.0007309789972675246
Epoch 1522
Epoch 1522 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1522, epoch_train_loss=0.0007309789972675246
Epoch 1523
Epoch 1523 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1523, epoch_train_loss=0.0007309789972675246
Epoch 1524
Epoch 1524 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1524, epoch_train_loss=0.0007309789972675246
Epoch 1525
Epoch 1525 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1525, epoch_train_loss=0.0007309789972675246
Epoch 1526
Epoch 1526 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1526, epoch_train_loss=0.0007309789972675246
Epoch 1527
Epoch 1527 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1527, epoch_train_loss=0.0007309789972675246
Epoch 1528
Epoch 1528 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1528, epoch_train_loss=0.0007309789972675246
Epoch 1529
Epoch 1529 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1529, epoch_train_loss=0.0007309789972675246
Epoch 1530
Epoch 1530 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1530, epoch_train_loss=0.0007309789972675246
Epoch 1531
Epoch 1531 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1531, epoch_train_loss=0.0007309789972675246
Epoch 1532
Epoch 1532 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1532, epoch_train_loss=0.0007309789972675246
Epoch 1533
Epoch 1533 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1533, epoch_train_loss=0.0007309789972675246
Epoch 1534
Epoch 1534 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1534, epoch_train_loss=0.0007309789972675246
Epoch 1535
Epoch 1535 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1535, epoch_train_loss=0.0007309789972675246
Epoch 1536
Epoch 1536 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1536, epoch_train_loss=0.0007309789972675246
Epoch 1537
Epoch 1537 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1537, epoch_train_loss=0.0007309789972675246
Epoch 1538
Epoch 1538 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1538, epoch_train_loss=0.0007309789972675246
Epoch 1539
Epoch 1539 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1539, epoch_train_loss=0.0007309789972675246
Epoch 1540
Epoch 1540 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1540, epoch_train_loss=0.0007309789972675246
Epoch 1541
Epoch 1541 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1541, epoch_train_loss=0.0007309789972675246
Epoch 1542
Epoch 1542 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1542, epoch_train_loss=0.0007309789972675246
Epoch 1543
Epoch 1543 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1543, epoch_train_loss=0.0007309789972675246
Epoch 1544
Epoch 1544 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1544, epoch_train_loss=0.0007309789972675246
Epoch 1545
Epoch 1545 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1545, epoch_train_loss=0.0007309789972675246
Epoch 1546
Epoch 1546 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1546, epoch_train_loss=0.0007309789972675246
Epoch 1547
Epoch 1547 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1547, epoch_train_loss=0.0007309789972675246
Epoch 1548
Epoch 1548 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1548, epoch_train_loss=0.0007309789972675246
Epoch 1549
Epoch 1549 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1549, epoch_train_loss=0.0007309789972675246
Epoch 1550
Epoch 1550 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1550, epoch_train_loss=0.0007309789972675246
Epoch 1551
Epoch 1551 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1551, epoch_train_loss=0.0007309789972675246
Epoch 1552
Epoch 1552 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1552, epoch_train_loss=0.0007309789972675246
Epoch 1553
Epoch 1553 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1553, epoch_train_loss=0.0007309789972675246
Epoch 1554
Epoch 1554 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1554, epoch_train_loss=0.0007309789972675246
Epoch 1555
Epoch 1555 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1555, epoch_train_loss=0.0007309789972675246
Epoch 1556
Epoch 1556 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1556, epoch_train_loss=0.0007309789972675246
Epoch 1557
Epoch 1557 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1557, epoch_train_loss=0.0007309789972675246
Epoch 1558
Epoch 1558 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1558, epoch_train_loss=0.0007309789972675246
Epoch 1559
Epoch 1559 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1559, epoch_train_loss=0.0007309789972675246
Epoch 1560
Epoch 1560 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1560, epoch_train_loss=0.0007309789972675246
Epoch 1561
Epoch 1561 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1561, epoch_train_loss=0.0007309789972675246
Epoch 1562
Epoch 1562 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1562, epoch_train_loss=0.0007309789972675246
Epoch 1563
Epoch 1563 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1563, epoch_train_loss=0.0007309789972675246
Epoch 1564
Epoch 1564 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1564, epoch_train_loss=0.0007309789972675246
Epoch 1565
Epoch 1565 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1565, epoch_train_loss=0.0007309789972675246
Epoch 1566
Epoch 1566 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1566, epoch_train_loss=0.0007309789972675246
Epoch 1567
Epoch 1567 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1567, epoch_train_loss=0.0007309789972675246
Epoch 1568
Epoch 1568 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1568, epoch_train_loss=0.0007309789972675246
Epoch 1569
Epoch 1569 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1569, epoch_train_loss=0.0007309789972675246
Epoch 1570
Epoch 1570 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1570, epoch_train_loss=0.0007309789972675246
Epoch 1571
Epoch 1571 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1571, epoch_train_loss=0.0007309789972675246
Epoch 1572
Epoch 1572 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1572, epoch_train_loss=0.0007309789972675246
Epoch 1573
Epoch 1573 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1573, epoch_train_loss=0.0007309789972675246
Epoch 1574
Epoch 1574 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1574, epoch_train_loss=0.0007309789972675246
Epoch 1575
Epoch 1575 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1575, epoch_train_loss=0.0007309789972675246
Epoch 1576
Epoch 1576 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1576, epoch_train_loss=0.0007309789972675246
Epoch 1577
Epoch 1577 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1577, epoch_train_loss=0.0007309789972675246
Epoch 1578
Epoch 1578 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1578, epoch_train_loss=0.0007309789972675246
Epoch 1579
Epoch 1579 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1579, epoch_train_loss=0.0007309789972675246
Epoch 1580
Epoch 1580 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1580, epoch_train_loss=0.0007309789972675246
Epoch 1581
Epoch 1581 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1581, epoch_train_loss=0.0007309789972675246
Epoch 1582
Epoch 1582 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1582, epoch_train_loss=0.0007309789972675246
Epoch 1583
Epoch 1583 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1583, epoch_train_loss=0.0007309789972675246
Epoch 1584
Epoch 1584 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1584, epoch_train_loss=0.0007309789972675246
Epoch 1585
Epoch 1585 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1585, epoch_train_loss=0.0007309789972675246
Epoch 1586
Epoch 1586 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1586, epoch_train_loss=0.0007309789972675246
Epoch 1587
Epoch 1587 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1587, epoch_train_loss=0.0007309789972675246
Epoch 1588
Epoch 1588 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1588, epoch_train_loss=0.0007309789972675246
Epoch 1589
Epoch 1589 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1589, epoch_train_loss=0.0007309789972675246
Epoch 1590
Epoch 1590 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1590, epoch_train_loss=0.0007309789972675246
Epoch 1591
Epoch 1591 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1591, epoch_train_loss=0.0007309789972675246
Epoch 1592
Epoch 1592 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1592, epoch_train_loss=0.0007309789972675246
Epoch 1593
Epoch 1593 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1593, epoch_train_loss=0.0007309789972675246
Epoch 1594
Epoch 1594 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1594, epoch_train_loss=0.0007309789972675246
Epoch 1595
Epoch 1595 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1595, epoch_train_loss=0.0007309789972675246
Epoch 1596
Epoch 1596 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1596, epoch_train_loss=0.0007309789972675246
Epoch 1597
Epoch 1597 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1597, epoch_train_loss=0.0007309789972675246
Epoch 1598
Epoch 1598 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1598, epoch_train_loss=0.0007309789972675246
Epoch 1599
Epoch 1599 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1599, epoch_train_loss=0.0007309789972675246
Epoch 1600
Epoch 1600 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1600, epoch_train_loss=0.0007309789972675246
Epoch 1601
Epoch 1601 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1601, epoch_train_loss=0.0007309789972675246
Epoch 1602
Epoch 1602 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1602, epoch_train_loss=0.0007309789972675246
Epoch 1603
Epoch 1603 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1603, epoch_train_loss=0.0007309789972675246
Epoch 1604
Epoch 1604 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1604, epoch_train_loss=0.0007309789972675246
Epoch 1605
Epoch 1605 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1605, epoch_train_loss=0.0007309789972675246
Epoch 1606
Epoch 1606 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1606, epoch_train_loss=0.0007309789972675246
Epoch 1607
Epoch 1607 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1607, epoch_train_loss=0.0007309789972675246
Epoch 1608
Epoch 1608 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1608, epoch_train_loss=0.0007309789972675246
Epoch 1609
Epoch 1609 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1609, epoch_train_loss=0.0007309789972675246
Epoch 1610
Epoch 1610 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1610, epoch_train_loss=0.0007309789972675246
Epoch 1611
Epoch 1611 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1611, epoch_train_loss=0.0007309789972675246
Epoch 1612
Epoch 1612 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1612, epoch_train_loss=0.0007309789972675246
Epoch 1613
Epoch 1613 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1613, epoch_train_loss=0.0007309789972675246
Epoch 1614
Epoch 1614 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1614, epoch_train_loss=0.0007309789972675246
Epoch 1615
Epoch 1615 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1615, epoch_train_loss=0.0007309789972675246
Epoch 1616
Epoch 1616 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1616, epoch_train_loss=0.0007309789972675246
Epoch 1617
Epoch 1617 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1617, epoch_train_loss=0.0007309789972675246
Epoch 1618
Epoch 1618 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1618, epoch_train_loss=0.0007309789972675246
Epoch 1619
Epoch 1619 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1619, epoch_train_loss=0.0007309789972675246
Epoch 1620
Epoch 1620 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1620, epoch_train_loss=0.0007309789972675246
Epoch 1621
Epoch 1621 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1621, epoch_train_loss=0.0007309789972675246
Epoch 1622
Epoch 1622 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1622, epoch_train_loss=0.0007309789972675246
Epoch 1623
Epoch 1623 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1623, epoch_train_loss=0.0007309789972675246
Epoch 1624
Epoch 1624 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1624, epoch_train_loss=0.0007309789972675246
Epoch 1625
Epoch 1625 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1625, epoch_train_loss=0.0007309789972675246
Epoch 1626
Epoch 1626 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1626, epoch_train_loss=0.0007309789972675246
Epoch 1627
Epoch 1627 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1627, epoch_train_loss=0.0007309789972675246
Epoch 1628
Epoch 1628 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1628, epoch_train_loss=0.0007309789972675246
Epoch 1629
Epoch 1629 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1629, epoch_train_loss=0.0007309789972675246
Epoch 1630
Epoch 1630 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1630, epoch_train_loss=0.0007309789972675246
Epoch 1631
Epoch 1631 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1631, epoch_train_loss=0.0007309789972675246
Epoch 1632
Epoch 1632 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1632, epoch_train_loss=0.0007309789972675246
Epoch 1633
Epoch 1633 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1633, epoch_train_loss=0.0007309789972675246
Epoch 1634
Epoch 1634 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1634, epoch_train_loss=0.0007309789972675246
Epoch 1635
Epoch 1635 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1635, epoch_train_loss=0.0007309789972675246
Epoch 1636
Epoch 1636 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1636, epoch_train_loss=0.0007309789972675246
Epoch 1637
Epoch 1637 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1637, epoch_train_loss=0.0007309789972675246
Epoch 1638
Epoch 1638 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1638, epoch_train_loss=0.0007309789972675246
Epoch 1639
Epoch 1639 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1639, epoch_train_loss=0.0007309789972675246
Epoch 1640
Epoch 1640 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1640, epoch_train_loss=0.0007309789972675246
Epoch 1641
Epoch 1641 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1641, epoch_train_loss=0.0007309789972675246
Epoch 1642
Epoch 1642 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1642, epoch_train_loss=0.0007309789972675246
Epoch 1643
Epoch 1643 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1643, epoch_train_loss=0.0007309789972675246
Epoch 1644
Epoch 1644 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1644, epoch_train_loss=0.0007309789972675246
Epoch 1645
Epoch 1645 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1645, epoch_train_loss=0.0007309789972675246
Epoch 1646
Epoch 1646 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1646, epoch_train_loss=0.0007309789972675246
Epoch 1647
Epoch 1647 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1647, epoch_train_loss=0.0007309789972675246
Epoch 1648
Epoch 1648 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1648, epoch_train_loss=0.0007309789972675246
Epoch 1649
Epoch 1649 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1649, epoch_train_loss=0.0007309789972675246
Epoch 1650
Epoch 1650 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1650, epoch_train_loss=0.0007309789972675246
Epoch 1651
Epoch 1651 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1651, epoch_train_loss=0.0007309789972675246
Epoch 1652
Epoch 1652 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1652, epoch_train_loss=0.0007309789972675246
Epoch 1653
Epoch 1653 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1653, epoch_train_loss=0.0007309789972675246
Epoch 1654
Epoch 1654 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1654, epoch_train_loss=0.0007309789972675246
Epoch 1655
Epoch 1655 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1655, epoch_train_loss=0.0007309789972675246
Epoch 1656
Epoch 1656 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1656, epoch_train_loss=0.0007309789972675246
Epoch 1657
Epoch 1657 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1657, epoch_train_loss=0.0007309789972675246
Epoch 1658
Epoch 1658 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1658, epoch_train_loss=0.0007309789972675246
Epoch 1659
Epoch 1659 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1659, epoch_train_loss=0.0007309789972675246
Epoch 1660
Epoch 1660 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1660, epoch_train_loss=0.0007309789972675246
Epoch 1661
Epoch 1661 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1661, epoch_train_loss=0.0007309789972675246
Epoch 1662
Epoch 1662 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1662, epoch_train_loss=0.0007309789972675246
Epoch 1663
Epoch 1663 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1663, epoch_train_loss=0.0007309789972675246
Epoch 1664
Epoch 1664 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1664, epoch_train_loss=0.0007309789972675246
Epoch 1665
Epoch 1665 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1665, epoch_train_loss=0.0007309789972675246
Epoch 1666
Epoch 1666 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1666, epoch_train_loss=0.0007309789972675246
Epoch 1667
Epoch 1667 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1667, epoch_train_loss=0.0007309789972675246
Epoch 1668
Epoch 1668 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1668, epoch_train_loss=0.0007309789972675246
Epoch 1669
Epoch 1669 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1669, epoch_train_loss=0.0007309789972675246
Epoch 1670
Epoch 1670 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1670, epoch_train_loss=0.0007309789972675246
Epoch 1671
Epoch 1671 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1671, epoch_train_loss=0.0007309789972675246
Epoch 1672
Epoch 1672 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1672, epoch_train_loss=0.0007309789972675246
Epoch 1673
Epoch 1673 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1673, epoch_train_loss=0.0007309789972675246
Epoch 1674
Epoch 1674 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1674, epoch_train_loss=0.0007309789972675246
Epoch 1675
Epoch 1675 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1675, epoch_train_loss=0.0007309789972675246
Epoch 1676
Epoch 1676 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1676, epoch_train_loss=0.0007309789972675246
Epoch 1677
Epoch 1677 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1677, epoch_train_loss=0.0007309789972675246
Epoch 1678
Epoch 1678 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1678, epoch_train_loss=0.0007309789972675246
Epoch 1679
Epoch 1679 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1679, epoch_train_loss=0.0007309789972675246
Epoch 1680
Epoch 1680 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1680, epoch_train_loss=0.0007309789972675246
Epoch 1681
Epoch 1681 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1681, epoch_train_loss=0.0007309789972675246
Epoch 1682
Epoch 1682 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1682, epoch_train_loss=0.0007309789972675246
Epoch 1683
Epoch 1683 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1683, epoch_train_loss=0.0007309789972675246
Epoch 1684
Epoch 1684 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1684, epoch_train_loss=0.0007309789972675246
Epoch 1685
Epoch 1685 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1685, epoch_train_loss=0.0007309789972675246
Epoch 1686
Epoch 1686 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1686, epoch_train_loss=0.0007309789972675246
Epoch 1687
Epoch 1687 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1687, epoch_train_loss=0.0007309789972675246
Epoch 1688
Epoch 1688 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1688, epoch_train_loss=0.0007309789972675246
Epoch 1689
Epoch 1689 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1689, epoch_train_loss=0.0007309789972675246
Epoch 1690
Epoch 1690 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1690, epoch_train_loss=0.0007309789972675246
Epoch 1691
Epoch 1691 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1691, epoch_train_loss=0.0007309789972675246
Epoch 1692
Epoch 1692 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1692, epoch_train_loss=0.0007309789972675246
Epoch 1693
Epoch 1693 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1693, epoch_train_loss=0.0007309789972675246
Epoch 1694
Epoch 1694 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1694, epoch_train_loss=0.0007309789972675246
Epoch 1695
Epoch 1695 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1695, epoch_train_loss=0.0007309789972675246
Epoch 1696
Epoch 1696 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1696, epoch_train_loss=0.0007309789972675246
Epoch 1697
Epoch 1697 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1697, epoch_train_loss=0.0007309789972675246
Epoch 1698
Epoch 1698 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1698, epoch_train_loss=0.0007309789972675246
Epoch 1699
Epoch 1699 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1699, epoch_train_loss=0.0007309789972675246
Epoch 1700
Epoch 1700 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1700, epoch_train_loss=0.0007309789972675246
Epoch 1701
Epoch 1701 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1701, epoch_train_loss=0.0007309789972675246
Epoch 1702
Epoch 1702 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1702, epoch_train_loss=0.0007309789972675246
Epoch 1703
Epoch 1703 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1703, epoch_train_loss=0.0007309789972675246
Epoch 1704
Epoch 1704 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1704, epoch_train_loss=0.0007309789972675246
Epoch 1705
Epoch 1705 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1705, epoch_train_loss=0.0007309789972675246
Epoch 1706
Epoch 1706 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1706, epoch_train_loss=0.0007309789972675246
Epoch 1707
Epoch 1707 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1707, epoch_train_loss=0.0007309789972675246
Epoch 1708
Epoch 1708 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1708, epoch_train_loss=0.0007309789972675246
Epoch 1709
Epoch 1709 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1709, epoch_train_loss=0.0007309789972675246
Epoch 1710
Epoch 1710 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1710, epoch_train_loss=0.0007309789972675246
Epoch 1711
Epoch 1711 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1711, epoch_train_loss=0.0007309789972675246
Epoch 1712
Epoch 1712 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1712, epoch_train_loss=0.0007309789972675246
Epoch 1713
Epoch 1713 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1713, epoch_train_loss=0.0007309789972675246
Epoch 1714
Epoch 1714 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1714, epoch_train_loss=0.0007309789972675246
Epoch 1715
Epoch 1715 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1715, epoch_train_loss=0.0007309789972675246
Epoch 1716
Epoch 1716 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1716, epoch_train_loss=0.0007309789972675246
Epoch 1717
Epoch 1717 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1717, epoch_train_loss=0.0007309789972675246
Epoch 1718
Epoch 1718 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1718, epoch_train_loss=0.0007309789972675246
Epoch 1719
Epoch 1719 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1719, epoch_train_loss=0.0007309789972675246
Epoch 1720
Epoch 1720 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1720, epoch_train_loss=0.0007309789972675246
Epoch 1721
Epoch 1721 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1721, epoch_train_loss=0.0007309789972675246
Epoch 1722
Epoch 1722 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1722, epoch_train_loss=0.0007309789972675246
Epoch 1723
Epoch 1723 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1723, epoch_train_loss=0.0007309789972675246
Epoch 1724
Epoch 1724 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1724, epoch_train_loss=0.0007309789972675246
Epoch 1725
Epoch 1725 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1725, epoch_train_loss=0.0007309789972675246
Epoch 1726
Epoch 1726 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1726, epoch_train_loss=0.0007309789972675246
Epoch 1727
Epoch 1727 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1727, epoch_train_loss=0.0007309789972675246
Epoch 1728
Epoch 1728 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1728, epoch_train_loss=0.0007309789972675246
Epoch 1729
Epoch 1729 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1729, epoch_train_loss=0.0007309789972675246
Epoch 1730
Epoch 1730 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1730, epoch_train_loss=0.0007309789972675246
Epoch 1731
Epoch 1731 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1731, epoch_train_loss=0.0007309789972675246
Epoch 1732
Epoch 1732 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1732, epoch_train_loss=0.0007309789972675246
Epoch 1733
Epoch 1733 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1733, epoch_train_loss=0.0007309789972675246
Epoch 1734
Epoch 1734 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1734, epoch_train_loss=0.0007309789972675246
Epoch 1735
Epoch 1735 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1735, epoch_train_loss=0.0007309789972675246
Epoch 1736
Epoch 1736 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1736, epoch_train_loss=0.0007309789972675246
Epoch 1737
Epoch 1737 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1737, epoch_train_loss=0.0007309789972675246
Epoch 1738
Epoch 1738 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1738, epoch_train_loss=0.0007309789972675246
Epoch 1739
Epoch 1739 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1739, epoch_train_loss=0.0007309789972675246
Epoch 1740
Epoch 1740 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1740, epoch_train_loss=0.0007309789972675246
Epoch 1741
Epoch 1741 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1741, epoch_train_loss=0.0007309789972675246
Epoch 1742
Epoch 1742 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1742, epoch_train_loss=0.0007309789972675246
Epoch 1743
Epoch 1743 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1743, epoch_train_loss=0.0007309789972675246
Epoch 1744
Epoch 1744 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1744, epoch_train_loss=0.0007309789972675246
Epoch 1745
Epoch 1745 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1745, epoch_train_loss=0.0007309789972675246
Epoch 1746
Epoch 1746 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1746, epoch_train_loss=0.0007309789972675246
Epoch 1747
Epoch 1747 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1747, epoch_train_loss=0.0007309789972675246
Epoch 1748
Epoch 1748 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1748, epoch_train_loss=0.0007309789972675246
Epoch 1749
Epoch 1749 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1749, epoch_train_loss=0.0007309789972675246
Epoch 1750
Epoch 1750 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1750, epoch_train_loss=0.0007309789972675246
Epoch 1751
Epoch 1751 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1751, epoch_train_loss=0.0007309789972675246
Epoch 1752
Epoch 1752 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1752, epoch_train_loss=0.0007309789972675246
Epoch 1753
Epoch 1753 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1753, epoch_train_loss=0.0007309789972675246
Epoch 1754
Epoch 1754 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1754, epoch_train_loss=0.0007309789972675246
Epoch 1755
Epoch 1755 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1755, epoch_train_loss=0.0007309789972675246
Epoch 1756
Epoch 1756 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1756, epoch_train_loss=0.0007309789972675246
Epoch 1757
Epoch 1757 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1757, epoch_train_loss=0.0007309789972675246
Epoch 1758
Epoch 1758 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1758, epoch_train_loss=0.0007309789972675246
Epoch 1759
Epoch 1759 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1759, epoch_train_loss=0.0007309789972675246
Epoch 1760
Epoch 1760 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1760, epoch_train_loss=0.0007309789972675246
Epoch 1761
Epoch 1761 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1761, epoch_train_loss=0.0007309789972675246
Epoch 1762
Epoch 1762 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1762, epoch_train_loss=0.0007309789972675246
Epoch 1763
Epoch 1763 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1763, epoch_train_loss=0.0007309789972675246
Epoch 1764
Epoch 1764 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1764, epoch_train_loss=0.0007309789972675246
Epoch 1765
Epoch 1765 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1765, epoch_train_loss=0.0007309789972675246
Epoch 1766
Epoch 1766 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1766, epoch_train_loss=0.0007309789972675246
Epoch 1767
Epoch 1767 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1767, epoch_train_loss=0.0007309789972675246
Epoch 1768
Epoch 1768 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1768, epoch_train_loss=0.0007309789972675246
Epoch 1769
Epoch 1769 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1769, epoch_train_loss=0.0007309789972675246
Epoch 1770
Epoch 1770 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1770, epoch_train_loss=0.0007309789972675246
Epoch 1771
Epoch 1771 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1771, epoch_train_loss=0.0007309789972675246
Epoch 1772
Epoch 1772 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1772, epoch_train_loss=0.0007309789972675246
Epoch 1773
Epoch 1773 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1773, epoch_train_loss=0.0007309789972675246
Epoch 1774
Epoch 1774 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1774, epoch_train_loss=0.0007309789972675246
Epoch 1775
Epoch 1775 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1775, epoch_train_loss=0.0007309789972675246
Epoch 1776
Epoch 1776 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1776, epoch_train_loss=0.0007309789972675246
Epoch 1777
Epoch 1777 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1777, epoch_train_loss=0.0007309789972675246
Epoch 1778
Epoch 1778 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1778, epoch_train_loss=0.0007309789972675246
Epoch 1779
Epoch 1779 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1779, epoch_train_loss=0.0007309789972675246
Epoch 1780
Epoch 1780 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1780, epoch_train_loss=0.0007309789972675246
Epoch 1781
Epoch 1781 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1781, epoch_train_loss=0.0007309789972675246
Epoch 1782
Epoch 1782 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1782, epoch_train_loss=0.0007309789972675246
Epoch 1783
Epoch 1783 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1783, epoch_train_loss=0.0007309789972675246
Epoch 1784
Epoch 1784 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1784, epoch_train_loss=0.0007309789972675246
Epoch 1785
Epoch 1785 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1785, epoch_train_loss=0.0007309789972675246
Epoch 1786
Epoch 1786 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1786, epoch_train_loss=0.0007309789972675246
Epoch 1787
Epoch 1787 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1787, epoch_train_loss=0.0007309789972675246
Epoch 1788
Epoch 1788 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1788, epoch_train_loss=0.0007309789972675246
Epoch 1789
Epoch 1789 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1789, epoch_train_loss=0.0007309789972675246
Epoch 1790
Epoch 1790 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1790, epoch_train_loss=0.0007309789972675246
Epoch 1791
Epoch 1791 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1791, epoch_train_loss=0.0007309789972675246
Epoch 1792
Epoch 1792 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1792, epoch_train_loss=0.0007309789972675246
Epoch 1793
Epoch 1793 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1793, epoch_train_loss=0.0007309789972675246
Epoch 1794
Epoch 1794 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1794, epoch_train_loss=0.0007309789972675246
Epoch 1795
Epoch 1795 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1795, epoch_train_loss=0.0007309789972675246
Epoch 1796
Epoch 1796 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1796, epoch_train_loss=0.0007309789972675246
Epoch 1797
Epoch 1797 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1797, epoch_train_loss=0.0007309789972675246
Epoch 1798
Epoch 1798 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1798, epoch_train_loss=0.0007309789972675246
Epoch 1799
Epoch 1799 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1799, epoch_train_loss=0.0007309789972675246
Epoch 1800
Epoch 1800 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1800, epoch_train_loss=0.0007309789972675246
Epoch 1801
Epoch 1801 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1801, epoch_train_loss=0.0007309789972675246
Epoch 1802
Epoch 1802 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1802, epoch_train_loss=0.0007309789972675246
Epoch 1803
Epoch 1803 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1803, epoch_train_loss=0.0007309789972675246
Epoch 1804
Epoch 1804 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1804, epoch_train_loss=0.0007309789972675246
Epoch 1805
Epoch 1805 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1805, epoch_train_loss=0.0007309789972675246
Epoch 1806
Epoch 1806 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1806, epoch_train_loss=0.0007309789972675246
Epoch 1807
Epoch 1807 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1807, epoch_train_loss=0.0007309789972675246
Epoch 1808
Epoch 1808 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1808, epoch_train_loss=0.0007309789972675246
Epoch 1809
Epoch 1809 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1809, epoch_train_loss=0.0007309789972675246
Epoch 1810
Epoch 1810 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1810, epoch_train_loss=0.0007309789972675246
Epoch 1811
Epoch 1811 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1811, epoch_train_loss=0.0007309789972675246
Epoch 1812
Epoch 1812 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1812, epoch_train_loss=0.0007309789972675246
Epoch 1813
Epoch 1813 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1813, epoch_train_loss=0.0007309789972675246
Epoch 1814
Epoch 1814 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1814, epoch_train_loss=0.0007309789972675246
Epoch 1815
Epoch 1815 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1815, epoch_train_loss=0.0007309789972675246
Epoch 1816
Epoch 1816 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1816, epoch_train_loss=0.0007309789972675246
Epoch 1817
Epoch 1817 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1817, epoch_train_loss=0.0007309789972675246
Epoch 1818
Epoch 1818 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1818, epoch_train_loss=0.0007309789972675246
Epoch 1819
Epoch 1819 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1819, epoch_train_loss=0.0007309789972675246
Epoch 1820
Epoch 1820 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1820, epoch_train_loss=0.0007309789972675246
Epoch 1821
Epoch 1821 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1821, epoch_train_loss=0.0007309789972675246
Epoch 1822
Epoch 1822 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1822, epoch_train_loss=0.0007309789972675246
Epoch 1823
Epoch 1823 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1823, epoch_train_loss=0.0007309789972675246
Epoch 1824
Epoch 1824 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1824, epoch_train_loss=0.0007309789972675246
Epoch 1825
Epoch 1825 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1825, epoch_train_loss=0.0007309789972675246
Epoch 1826
Epoch 1826 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1826, epoch_train_loss=0.0007309789972675246
Epoch 1827
Epoch 1827 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1827, epoch_train_loss=0.0007309789972675246
Epoch 1828
Epoch 1828 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1828, epoch_train_loss=0.0007309789972675246
Epoch 1829
Epoch 1829 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1829, epoch_train_loss=0.0007309789972675246
Epoch 1830
Epoch 1830 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1830, epoch_train_loss=0.0007309789972675246
Epoch 1831
Epoch 1831 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1831, epoch_train_loss=0.0007309789972675246
Epoch 1832
Epoch 1832 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1832, epoch_train_loss=0.0007309789972675246
Epoch 1833
Epoch 1833 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1833, epoch_train_loss=0.0007309789972675246
Epoch 1834
Epoch 1834 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1834, epoch_train_loss=0.0007309789972675246
Epoch 1835
Epoch 1835 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1835, epoch_train_loss=0.0007309789972675246
Epoch 1836
Epoch 1836 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1836, epoch_train_loss=0.0007309789972675246
Epoch 1837
Epoch 1837 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1837, epoch_train_loss=0.0007309789972675246
Epoch 1838
Epoch 1838 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1838, epoch_train_loss=0.0007309789972675246
Epoch 1839
Epoch 1839 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1839, epoch_train_loss=0.0007309789972675246
Epoch 1840
Epoch 1840 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1840, epoch_train_loss=0.0007309789972675246
Epoch 1841
Epoch 1841 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1841, epoch_train_loss=0.0007309789972675246
Epoch 1842
Epoch 1842 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1842, epoch_train_loss=0.0007309789972675246
Epoch 1843
Epoch 1843 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1843, epoch_train_loss=0.0007309789972675246
Epoch 1844
Epoch 1844 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1844, epoch_train_loss=0.0007309789972675246
Epoch 1845
Epoch 1845 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1845, epoch_train_loss=0.0007309789972675246
Epoch 1846
Epoch 1846 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1846, epoch_train_loss=0.0007309789972675246
Epoch 1847
Epoch 1847 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1847, epoch_train_loss=0.0007309789972675246
Epoch 1848
Epoch 1848 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1848, epoch_train_loss=0.0007309789972675246
Epoch 1849
Epoch 1849 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1849, epoch_train_loss=0.0007309789972675246
Epoch 1850
Epoch 1850 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1850, epoch_train_loss=0.0007309789972675246
Epoch 1851
Epoch 1851 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1851, epoch_train_loss=0.0007309789972675246
Epoch 1852
Epoch 1852 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1852, epoch_train_loss=0.0007309789972675246
Epoch 1853
Epoch 1853 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1853, epoch_train_loss=0.0007309789972675246
Epoch 1854
Epoch 1854 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1854, epoch_train_loss=0.0007309789972675246
Epoch 1855
Epoch 1855 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1855, epoch_train_loss=0.0007309789972675246
Epoch 1856
Epoch 1856 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1856, epoch_train_loss=0.0007309789972675246
Epoch 1857
Epoch 1857 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1857, epoch_train_loss=0.0007309789972675246
Epoch 1858
Epoch 1858 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1858, epoch_train_loss=0.0007309789972675246
Epoch 1859
Epoch 1859 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1859, epoch_train_loss=0.0007309789972675246
Epoch 1860
Epoch 1860 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1860, epoch_train_loss=0.0007309789972675246
Epoch 1861
Epoch 1861 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1861, epoch_train_loss=0.0007309789972675246
Epoch 1862
Epoch 1862 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1862, epoch_train_loss=0.0007309789972675246
Epoch 1863
Epoch 1863 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1863, epoch_train_loss=0.0007309789972675246
Epoch 1864
Epoch 1864 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1864, epoch_train_loss=0.0007309789972675246
Epoch 1865
Epoch 1865 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1865, epoch_train_loss=0.0007309789972675246
Epoch 1866
Epoch 1866 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1866, epoch_train_loss=0.0007309789972675246
Epoch 1867
Epoch 1867 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1867, epoch_train_loss=0.0007309789972675246
Epoch 1868
Epoch 1868 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1868, epoch_train_loss=0.0007309789972675246
Epoch 1869
Epoch 1869 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1869, epoch_train_loss=0.0007309789972675246
Epoch 1870
Epoch 1870 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1870, epoch_train_loss=0.0007309789972675246
Epoch 1871
Epoch 1871 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1871, epoch_train_loss=0.0007309789972675246
Epoch 1872
Epoch 1872 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1872, epoch_train_loss=0.0007309789972675246
Epoch 1873
Epoch 1873 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1873, epoch_train_loss=0.0007309789972675246
Epoch 1874
Epoch 1874 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1874, epoch_train_loss=0.0007309789972675246
Epoch 1875
Epoch 1875 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1875, epoch_train_loss=0.0007309789972675246
Epoch 1876
Epoch 1876 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1876, epoch_train_loss=0.0007309789972675246
Epoch 1877
Epoch 1877 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1877, epoch_train_loss=0.0007309789972675246
Epoch 1878
Epoch 1878 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1878, epoch_train_loss=0.0007309789972675246
Epoch 1879
Epoch 1879 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1879, epoch_train_loss=0.0007309789972675246
Epoch 1880
Epoch 1880 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1880, epoch_train_loss=0.0007309789972675246
Epoch 1881
Epoch 1881 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1881, epoch_train_loss=0.0007309789972675246
Epoch 1882
Epoch 1882 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1882, epoch_train_loss=0.0007309789972675246
Epoch 1883
Epoch 1883 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1883, epoch_train_loss=0.0007309789972675246
Epoch 1884
Epoch 1884 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1884, epoch_train_loss=0.0007309789972675246
Epoch 1885
Epoch 1885 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1885, epoch_train_loss=0.0007309789972675246
Epoch 1886
Epoch 1886 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1886, epoch_train_loss=0.0007309789972675246
Epoch 1887
Epoch 1887 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1887, epoch_train_loss=0.0007309789972675246
Epoch 1888
Epoch 1888 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1888, epoch_train_loss=0.0007309789972675246
Epoch 1889
Epoch 1889 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1889, epoch_train_loss=0.0007309789972675246
Epoch 1890
Epoch 1890 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1890, epoch_train_loss=0.0007309789972675246
Epoch 1891
Epoch 1891 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1891, epoch_train_loss=0.0007309789972675246
Epoch 1892
Epoch 1892 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1892, epoch_train_loss=0.0007309789972675246
Epoch 1893
Epoch 1893 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1893, epoch_train_loss=0.0007309789972675246
Epoch 1894
Epoch 1894 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1894, epoch_train_loss=0.0007309789972675246
Epoch 1895
Epoch 1895 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1895, epoch_train_loss=0.0007309789972675246
Epoch 1896
Epoch 1896 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1896, epoch_train_loss=0.0007309789972675246
Epoch 1897
Epoch 1897 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1897, epoch_train_loss=0.0007309789972675246
Epoch 1898
Epoch 1898 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1898, epoch_train_loss=0.0007309789972675246
Epoch 1899
Epoch 1899 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1899, epoch_train_loss=0.0007309789972675246
Epoch 1900
Epoch 1900 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1900, epoch_train_loss=0.0007309789972675246
Epoch 1901
Epoch 1901 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1901, epoch_train_loss=0.0007309789972675246
Epoch 1902
Epoch 1902 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1902, epoch_train_loss=0.0007309789972675246
Epoch 1903
Epoch 1903 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1903, epoch_train_loss=0.0007309789972675246
Epoch 1904
Epoch 1904 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1904, epoch_train_loss=0.0007309789972675246
Epoch 1905
Epoch 1905 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1905, epoch_train_loss=0.0007309789972675246
Epoch 1906
Epoch 1906 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1906, epoch_train_loss=0.0007309789972675246
Epoch 1907
Epoch 1907 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1907, epoch_train_loss=0.0007309789972675246
Epoch 1908
Epoch 1908 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1908, epoch_train_loss=0.0007309789972675246
Epoch 1909
Epoch 1909 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1909, epoch_train_loss=0.0007309789972675246
Epoch 1910
Epoch 1910 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1910, epoch_train_loss=0.0007309789972675246
Epoch 1911
Epoch 1911 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1911, epoch_train_loss=0.0007309789972675246
Epoch 1912
Epoch 1912 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1912, epoch_train_loss=0.0007309789972675246
Epoch 1913
Epoch 1913 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1913, epoch_train_loss=0.0007309789972675246
Epoch 1914
Epoch 1914 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1914, epoch_train_loss=0.0007309789972675246
Epoch 1915
Epoch 1915 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1915, epoch_train_loss=0.0007309789972675246
Epoch 1916
Epoch 1916 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1916, epoch_train_loss=0.0007309789972675246
Epoch 1917
Epoch 1917 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1917, epoch_train_loss=0.0007309789972675246
Epoch 1918
Epoch 1918 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1918, epoch_train_loss=0.0007309789972675246
Epoch 1919
Epoch 1919 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1919, epoch_train_loss=0.0007309789972675246
Epoch 1920
Epoch 1920 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1920, epoch_train_loss=0.0007309789972675246
Epoch 1921
Epoch 1921 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1921, epoch_train_loss=0.0007309789972675246
Epoch 1922
Epoch 1922 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1922, epoch_train_loss=0.0007309789972675246
Epoch 1923
Epoch 1923 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1923, epoch_train_loss=0.0007309789972675246
Epoch 1924
Epoch 1924 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1924, epoch_train_loss=0.0007309789972675246
Epoch 1925
Epoch 1925 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1925, epoch_train_loss=0.0007309789972675246
Epoch 1926
Epoch 1926 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1926, epoch_train_loss=0.0007309789972675246
Epoch 1927
Epoch 1927 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1927, epoch_train_loss=0.0007309789972675246
Epoch 1928
Epoch 1928 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1928, epoch_train_loss=0.0007309789972675246
Epoch 1929
Epoch 1929 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1929, epoch_train_loss=0.0007309789972675246
Epoch 1930
Epoch 1930 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1930, epoch_train_loss=0.0007309789972675246
Epoch 1931
Epoch 1931 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1931, epoch_train_loss=0.0007309789972675246
Epoch 1932
Epoch 1932 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1932, epoch_train_loss=0.0007309789972675246
Epoch 1933
Epoch 1933 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1933, epoch_train_loss=0.0007309789972675246
Epoch 1934
Epoch 1934 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1934, epoch_train_loss=0.0007309789972675246
Epoch 1935
Epoch 1935 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1935, epoch_train_loss=0.0007309789972675246
Epoch 1936
Epoch 1936 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1936, epoch_train_loss=0.0007309789972675246
Epoch 1937
Epoch 1937 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1937, epoch_train_loss=0.0007309789972675246
Epoch 1938
Epoch 1938 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1938, epoch_train_loss=0.0007309789972675246
Epoch 1939
Epoch 1939 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1939, epoch_train_loss=0.0007309789972675246
Epoch 1940
Epoch 1940 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1940, epoch_train_loss=0.0007309789972675246
Epoch 1941
Epoch 1941 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1941, epoch_train_loss=0.0007309789972675246
Epoch 1942
Epoch 1942 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1942, epoch_train_loss=0.0007309789972675246
Epoch 1943
Epoch 1943 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1943, epoch_train_loss=0.0007309789972675246
Epoch 1944
Epoch 1944 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1944, epoch_train_loss=0.0007309789972675246
Epoch 1945
Epoch 1945 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1945, epoch_train_loss=0.0007309789972675246
Epoch 1946
Epoch 1946 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1946, epoch_train_loss=0.0007309789972675246
Epoch 1947
Epoch 1947 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1947, epoch_train_loss=0.0007309789972675246
Epoch 1948
Epoch 1948 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1948, epoch_train_loss=0.0007309789972675246
Epoch 1949
Epoch 1949 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1949, epoch_train_loss=0.0007309789972675246
Epoch 1950
Epoch 1950 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1950, epoch_train_loss=0.0007309789972675246
Epoch 1951
Epoch 1951 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1951, epoch_train_loss=0.0007309789972675246
Epoch 1952
Epoch 1952 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1952, epoch_train_loss=0.0007309789972675246
Epoch 1953
Epoch 1953 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1953, epoch_train_loss=0.0007309789972675246
Epoch 1954
Epoch 1954 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1954, epoch_train_loss=0.0007309789972675246
Epoch 1955
Epoch 1955 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1955, epoch_train_loss=0.0007309789972675246
Epoch 1956
Epoch 1956 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1956, epoch_train_loss=0.0007309789972675246
Epoch 1957
Epoch 1957 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1957, epoch_train_loss=0.0007309789972675246
Epoch 1958
Epoch 1958 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1958, epoch_train_loss=0.0007309789972675246
Epoch 1959
Epoch 1959 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1959, epoch_train_loss=0.0007309789972675246
Epoch 1960
Epoch 1960 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1960, epoch_train_loss=0.0007309789972675246
Epoch 1961
Epoch 1961 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1961, epoch_train_loss=0.0007309789972675246
Epoch 1962
Epoch 1962 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1962, epoch_train_loss=0.0007309789972675246
Epoch 1963
Epoch 1963 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1963, epoch_train_loss=0.0007309789972675246
Epoch 1964
Epoch 1964 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1964, epoch_train_loss=0.0007309789972675246
Epoch 1965
Epoch 1965 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1965, epoch_train_loss=0.0007309789972675246
Epoch 1966
Epoch 1966 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1966, epoch_train_loss=0.0007309789972675246
Epoch 1967
Epoch 1967 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1967, epoch_train_loss=0.0007309789972675246
Epoch 1968
Epoch 1968 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1968, epoch_train_loss=0.0007309789972675246
Epoch 1969
Epoch 1969 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1969, epoch_train_loss=0.0007309789972675246
Epoch 1970
Epoch 1970 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1970, epoch_train_loss=0.0007309789972675246
Epoch 1971
Epoch 1971 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1971, epoch_train_loss=0.0007309789972675246
Epoch 1972
Epoch 1972 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1972, epoch_train_loss=0.0007309789972675246
Epoch 1973
Epoch 1973 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1973, epoch_train_loss=0.0007309789972675246
Epoch 1974
Epoch 1974 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1974, epoch_train_loss=0.0007309789972675246
Epoch 1975
Epoch 1975 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1975, epoch_train_loss=0.0007309789972675246
Epoch 1976
Epoch 1976 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1976, epoch_train_loss=0.0007309789972675246
Epoch 1977
Epoch 1977 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1977, epoch_train_loss=0.0007309789972675246
Epoch 1978
Epoch 1978 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1978, epoch_train_loss=0.0007309789972675246
Epoch 1979
Epoch 1979 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1979, epoch_train_loss=0.0007309789972675246
Epoch 1980
Epoch 1980 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1980, epoch_train_loss=0.0007309789972675246
Epoch 1981
Epoch 1981 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1981, epoch_train_loss=0.0007309789972675246
Epoch 1982
Epoch 1982 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1982, epoch_train_loss=0.0007309789972675246
Epoch 1983
Epoch 1983 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1983, epoch_train_loss=0.0007309789972675246
Epoch 1984
Epoch 1984 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1984, epoch_train_loss=0.0007309789972675246
Epoch 1985
Epoch 1985 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1985, epoch_train_loss=0.0007309789972675246
Epoch 1986
Epoch 1986 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1986, epoch_train_loss=0.0007309789972675246
Epoch 1987
Epoch 1987 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1987, epoch_train_loss=0.0007309789972675246
Epoch 1988
Epoch 1988 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1988, epoch_train_loss=0.0007309789972675246
Epoch 1989
Epoch 1989 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1989, epoch_train_loss=0.0007309789972675246
Epoch 1990
Epoch 1990 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1990, epoch_train_loss=0.0007309789972675246
Epoch 1991
Epoch 1991 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1991, epoch_train_loss=0.0007309789972675246
Epoch 1992
Epoch 1992 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1992, epoch_train_loss=0.0007309789972675246
Epoch 1993
Epoch 1993 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1993, epoch_train_loss=0.0007309789972675246
Epoch 1994
Epoch 1994 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1994, epoch_train_loss=0.0007309789972675246
Epoch 1995
Epoch 1995 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1995, epoch_train_loss=0.0007309789972675246
Epoch 1996
Epoch 1996 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1996, epoch_train_loss=0.0007309789972675246
Epoch 1997
Epoch 1997 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1997, epoch_train_loss=0.0007309789972675246
Epoch 1998
Epoch 1998 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1998, epoch_train_loss=0.0007309789972675246
Epoch 1999
Epoch 1999 :: Batch 0/1
Batch Loss = 0.0007309789972675246
1999, epoch_train_loss=0.0007309789972675246
Epoch 2000
Epoch 2000 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2000, epoch_train_loss=0.0007309789972675246
Epoch 2001
Epoch 2001 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2001, epoch_train_loss=0.0007309789972675246
Epoch 2002
Epoch 2002 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2002, epoch_train_loss=0.0007309789972675246
Epoch 2003
Epoch 2003 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2003, epoch_train_loss=0.0007309789972675246
Epoch 2004
Epoch 2004 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2004, epoch_train_loss=0.0007309789972675246
Epoch 2005
Epoch 2005 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2005, epoch_train_loss=0.0007309789972675246
Epoch 2006
Epoch 2006 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2006, epoch_train_loss=0.0007309789972675246
Epoch 2007
Epoch 2007 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2007, epoch_train_loss=0.0007309789972675246
Epoch 2008
Epoch 2008 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2008, epoch_train_loss=0.0007309789972675246
Epoch 2009
Epoch 2009 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2009, epoch_train_loss=0.0007309789972675246
Epoch 2010
Epoch 2010 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2010, epoch_train_loss=0.0007309789972675246
Epoch 2011
Epoch 2011 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2011, epoch_train_loss=0.0007309789972675246
Epoch 2012
Epoch 2012 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2012, epoch_train_loss=0.0007309789972675246
Epoch 2013
Epoch 2013 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2013, epoch_train_loss=0.0007309789972675246
Epoch 2014
Epoch 2014 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2014, epoch_train_loss=0.0007309789972675246
Epoch 2015
Epoch 2015 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2015, epoch_train_loss=0.0007309789972675246
Epoch 2016
Epoch 2016 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2016, epoch_train_loss=0.0007309789972675246
Epoch 2017
Epoch 2017 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2017, epoch_train_loss=0.0007309789972675246
Epoch 2018
Epoch 2018 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2018, epoch_train_loss=0.0007309789972675246
Epoch 2019
Epoch 2019 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2019, epoch_train_loss=0.0007309789972675246
Epoch 2020
Epoch 2020 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2020, epoch_train_loss=0.0007309789972675246
Epoch 2021
Epoch 2021 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2021, epoch_train_loss=0.0007309789972675246
Epoch 2022
Epoch 2022 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2022, epoch_train_loss=0.0007309789972675246
Epoch 2023
Epoch 2023 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2023, epoch_train_loss=0.0007309789972675246
Epoch 2024
Epoch 2024 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2024, epoch_train_loss=0.0007309789972675246
Epoch 2025
Epoch 2025 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2025, epoch_train_loss=0.0007309789972675246
Epoch 2026
Epoch 2026 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2026, epoch_train_loss=0.0007309789972675246
Epoch 2027
Epoch 2027 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2027, epoch_train_loss=0.0007309789972675246
Epoch 2028
Epoch 2028 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2028, epoch_train_loss=0.0007309789972675246
Epoch 2029
Epoch 2029 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2029, epoch_train_loss=0.0007309789972675246
Epoch 2030
Epoch 2030 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2030, epoch_train_loss=0.0007309789972675246
Epoch 2031
Epoch 2031 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2031, epoch_train_loss=0.0007309789972675246
Epoch 2032
Epoch 2032 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2032, epoch_train_loss=0.0007309789972675246
Epoch 2033
Epoch 2033 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2033, epoch_train_loss=0.0007309789972675246
Epoch 2034
Epoch 2034 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2034, epoch_train_loss=0.0007309789972675246
Epoch 2035
Epoch 2035 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2035, epoch_train_loss=0.0007309789972675246
Epoch 2036
Epoch 2036 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2036, epoch_train_loss=0.0007309789972675246
Epoch 2037
Epoch 2037 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2037, epoch_train_loss=0.0007309789972675246
Epoch 2038
Epoch 2038 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2038, epoch_train_loss=0.0007309789972675246
Epoch 2039
Epoch 2039 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2039, epoch_train_loss=0.0007309789972675246
Epoch 2040
Epoch 2040 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2040, epoch_train_loss=0.0007309789972675246
Epoch 2041
Epoch 2041 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2041, epoch_train_loss=0.0007309789972675246
Epoch 2042
Epoch 2042 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2042, epoch_train_loss=0.0007309789972675246
Epoch 2043
Epoch 2043 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2043, epoch_train_loss=0.0007309789972675246
Epoch 2044
Epoch 2044 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2044, epoch_train_loss=0.0007309789972675246
Epoch 2045
Epoch 2045 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2045, epoch_train_loss=0.0007309789972675246
Epoch 2046
Epoch 2046 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2046, epoch_train_loss=0.0007309789972675246
Epoch 2047
Epoch 2047 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2047, epoch_train_loss=0.0007309789972675246
Epoch 2048
Epoch 2048 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2048, epoch_train_loss=0.0007309789972675246
Epoch 2049
Epoch 2049 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2049, epoch_train_loss=0.0007309789972675246
Epoch 2050
Epoch 2050 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2050, epoch_train_loss=0.0007309789972675246
Epoch 2051
Epoch 2051 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2051, epoch_train_loss=0.0007309789972675246
Epoch 2052
Epoch 2052 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2052, epoch_train_loss=0.0007309789972675246
Epoch 2053
Epoch 2053 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2053, epoch_train_loss=0.0007309789972675246
Epoch 2054
Epoch 2054 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2054, epoch_train_loss=0.0007309789972675246
Epoch 2055
Epoch 2055 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2055, epoch_train_loss=0.0007309789972675246
Epoch 2056
Epoch 2056 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2056, epoch_train_loss=0.0007309789972675246
Epoch 2057
Epoch 2057 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2057, epoch_train_loss=0.0007309789972675246
Epoch 2058
Epoch 2058 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2058, epoch_train_loss=0.0007309789972675246
Epoch 2059
Epoch 2059 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2059, epoch_train_loss=0.0007309789972675246
Epoch 2060
Epoch 2060 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2060, epoch_train_loss=0.0007309789972675246
Epoch 2061
Epoch 2061 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2061, epoch_train_loss=0.0007309789972675246
Epoch 2062
Epoch 2062 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2062, epoch_train_loss=0.0007309789972675246
Epoch 2063
Epoch 2063 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2063, epoch_train_loss=0.0007309789972675246
Epoch 2064
Epoch 2064 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2064, epoch_train_loss=0.0007309789972675246
Epoch 2065
Epoch 2065 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2065, epoch_train_loss=0.0007309789972675246
Epoch 2066
Epoch 2066 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2066, epoch_train_loss=0.0007309789972675246
Epoch 2067
Epoch 2067 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2067, epoch_train_loss=0.0007309789972675246
Epoch 2068
Epoch 2068 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2068, epoch_train_loss=0.0007309789972675246
Epoch 2069
Epoch 2069 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2069, epoch_train_loss=0.0007309789972675246
Epoch 2070
Epoch 2070 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2070, epoch_train_loss=0.0007309789972675246
Epoch 2071
Epoch 2071 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2071, epoch_train_loss=0.0007309789972675246
Epoch 2072
Epoch 2072 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2072, epoch_train_loss=0.0007309789972675246
Epoch 2073
Epoch 2073 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2073, epoch_train_loss=0.0007309789972675246
Epoch 2074
Epoch 2074 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2074, epoch_train_loss=0.0007309789972675246
Epoch 2075
Epoch 2075 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2075, epoch_train_loss=0.0007309789972675246
Epoch 2076
Epoch 2076 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2076, epoch_train_loss=0.0007309789972675246
Epoch 2077
Epoch 2077 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2077, epoch_train_loss=0.0007309789972675246
Epoch 2078
Epoch 2078 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2078, epoch_train_loss=0.0007309789972675246
Epoch 2079
Epoch 2079 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2079, epoch_train_loss=0.0007309789972675246
Epoch 2080
Epoch 2080 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2080, epoch_train_loss=0.0007309789972675246
Epoch 2081
Epoch 2081 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2081, epoch_train_loss=0.0007309789972675246
Epoch 2082
Epoch 2082 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2082, epoch_train_loss=0.0007309789972675246
Epoch 2083
Epoch 2083 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2083, epoch_train_loss=0.0007309789972675246
Epoch 2084
Epoch 2084 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2084, epoch_train_loss=0.0007309789972675246
Epoch 2085
Epoch 2085 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2085, epoch_train_loss=0.0007309789972675246
Epoch 2086
Epoch 2086 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2086, epoch_train_loss=0.0007309789972675246
Epoch 2087
Epoch 2087 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2087, epoch_train_loss=0.0007309789972675246
Epoch 2088
Epoch 2088 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2088, epoch_train_loss=0.0007309789972675246
Epoch 2089
Epoch 2089 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2089, epoch_train_loss=0.0007309789972675246
Epoch 2090
Epoch 2090 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2090, epoch_train_loss=0.0007309789972675246
Epoch 2091
Epoch 2091 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2091, epoch_train_loss=0.0007309789972675246
Epoch 2092
Epoch 2092 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2092, epoch_train_loss=0.0007309789972675246
Epoch 2093
Epoch 2093 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2093, epoch_train_loss=0.0007309789972675246
Epoch 2094
Epoch 2094 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2094, epoch_train_loss=0.0007309789972675246
Epoch 2095
Epoch 2095 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2095, epoch_train_loss=0.0007309789972675246
Epoch 2096
Epoch 2096 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2096, epoch_train_loss=0.0007309789972675246
Epoch 2097
Epoch 2097 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2097, epoch_train_loss=0.0007309789972675246
Epoch 2098
Epoch 2098 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2098, epoch_train_loss=0.0007309789972675246
Epoch 2099
Epoch 2099 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2099, epoch_train_loss=0.0007309789972675246
Epoch 2100
Epoch 2100 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2100, epoch_train_loss=0.0007309789972675246
Epoch 2101
Epoch 2101 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2101, epoch_train_loss=0.0007309789972675246
Epoch 2102
Epoch 2102 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2102, epoch_train_loss=0.0007309789972675246
Epoch 2103
Epoch 2103 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2103, epoch_train_loss=0.0007309789972675246
Epoch 2104
Epoch 2104 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2104, epoch_train_loss=0.0007309789972675246
Epoch 2105
Epoch 2105 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2105, epoch_train_loss=0.0007309789972675246
Epoch 2106
Epoch 2106 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2106, epoch_train_loss=0.0007309789972675246
Epoch 2107
Epoch 2107 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2107, epoch_train_loss=0.0007309789972675246
Epoch 2108
Epoch 2108 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2108, epoch_train_loss=0.0007309789972675246
Epoch 2109
Epoch 2109 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2109, epoch_train_loss=0.0007309789972675246
Epoch 2110
Epoch 2110 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2110, epoch_train_loss=0.0007309789972675246
Epoch 2111
Epoch 2111 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2111, epoch_train_loss=0.0007309789972675246
Epoch 2112
Epoch 2112 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2112, epoch_train_loss=0.0007309789972675246
Epoch 2113
Epoch 2113 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2113, epoch_train_loss=0.0007309789972675246
Epoch 2114
Epoch 2114 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2114, epoch_train_loss=0.0007309789972675246
Epoch 2115
Epoch 2115 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2115, epoch_train_loss=0.0007309789972675246
Epoch 2116
Epoch 2116 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2116, epoch_train_loss=0.0007309789972675246
Epoch 2117
Epoch 2117 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2117, epoch_train_loss=0.0007309789972675246
Epoch 2118
Epoch 2118 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2118, epoch_train_loss=0.0007309789972675246
Epoch 2119
Epoch 2119 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2119, epoch_train_loss=0.0007309789972675246
Epoch 2120
Epoch 2120 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2120, epoch_train_loss=0.0007309789972675246
Epoch 2121
Epoch 2121 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2121, epoch_train_loss=0.0007309789972675246
Epoch 2122
Epoch 2122 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2122, epoch_train_loss=0.0007309789972675246
Epoch 2123
Epoch 2123 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2123, epoch_train_loss=0.0007309789972675246
Epoch 2124
Epoch 2124 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2124, epoch_train_loss=0.0007309789972675246
Epoch 2125
Epoch 2125 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2125, epoch_train_loss=0.0007309789972675246
Epoch 2126
Epoch 2126 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2126, epoch_train_loss=0.0007309789972675246
Epoch 2127
Epoch 2127 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2127, epoch_train_loss=0.0007309789972675246
Epoch 2128
Epoch 2128 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2128, epoch_train_loss=0.0007309789972675246
Epoch 2129
Epoch 2129 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2129, epoch_train_loss=0.0007309789972675246
Epoch 2130
Epoch 2130 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2130, epoch_train_loss=0.0007309789972675246
Epoch 2131
Epoch 2131 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2131, epoch_train_loss=0.0007309789972675246
Epoch 2132
Epoch 2132 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2132, epoch_train_loss=0.0007309789972675246
Epoch 2133
Epoch 2133 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2133, epoch_train_loss=0.0007309789972675246
Epoch 2134
Epoch 2134 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2134, epoch_train_loss=0.0007309789972675246
Epoch 2135
Epoch 2135 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2135, epoch_train_loss=0.0007309789972675246
Epoch 2136
Epoch 2136 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2136, epoch_train_loss=0.0007309789972675246
Epoch 2137
Epoch 2137 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2137, epoch_train_loss=0.0007309789972675246
Epoch 2138
Epoch 2138 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2138, epoch_train_loss=0.0007309789972675246
Epoch 2139
Epoch 2139 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2139, epoch_train_loss=0.0007309789972675246
Epoch 2140
Epoch 2140 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2140, epoch_train_loss=0.0007309789972675246
Epoch 2141
Epoch 2141 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2141, epoch_train_loss=0.0007309789972675246
Epoch 2142
Epoch 2142 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2142, epoch_train_loss=0.0007309789972675246
Epoch 2143
Epoch 2143 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2143, epoch_train_loss=0.0007309789972675246
Epoch 2144
Epoch 2144 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2144, epoch_train_loss=0.0007309789972675246
Epoch 2145
Epoch 2145 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2145, epoch_train_loss=0.0007309789972675246
Epoch 2146
Epoch 2146 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2146, epoch_train_loss=0.0007309789972675246
Epoch 2147
Epoch 2147 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2147, epoch_train_loss=0.0007309789972675246
Epoch 2148
Epoch 2148 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2148, epoch_train_loss=0.0007309789972675246
Epoch 2149
Epoch 2149 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2149, epoch_train_loss=0.0007309789972675246
Epoch 2150
Epoch 2150 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2150, epoch_train_loss=0.0007309789972675246
Epoch 2151
Epoch 2151 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2151, epoch_train_loss=0.0007309789972675246
Epoch 2152
Epoch 2152 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2152, epoch_train_loss=0.0007309789972675246
Epoch 2153
Epoch 2153 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2153, epoch_train_loss=0.0007309789972675246
Epoch 2154
Epoch 2154 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2154, epoch_train_loss=0.0007309789972675246
Epoch 2155
Epoch 2155 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2155, epoch_train_loss=0.0007309789972675246
Epoch 2156
Epoch 2156 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2156, epoch_train_loss=0.0007309789972675246
Epoch 2157
Epoch 2157 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2157, epoch_train_loss=0.0007309789972675246
Epoch 2158
Epoch 2158 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2158, epoch_train_loss=0.0007309789972675246
Epoch 2159
Epoch 2159 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2159, epoch_train_loss=0.0007309789972675246
Epoch 2160
Epoch 2160 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2160, epoch_train_loss=0.0007309789972675246
Epoch 2161
Epoch 2161 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2161, epoch_train_loss=0.0007309789972675246
Epoch 2162
Epoch 2162 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2162, epoch_train_loss=0.0007309789972675246
Epoch 2163
Epoch 2163 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2163, epoch_train_loss=0.0007309789972675246
Epoch 2164
Epoch 2164 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2164, epoch_train_loss=0.0007309789972675246
Epoch 2165
Epoch 2165 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2165, epoch_train_loss=0.0007309789972675246
Epoch 2166
Epoch 2166 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2166, epoch_train_loss=0.0007309789972675246
Epoch 2167
Epoch 2167 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2167, epoch_train_loss=0.0007309789972675246
Epoch 2168
Epoch 2168 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2168, epoch_train_loss=0.0007309789972675246
Epoch 2169
Epoch 2169 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2169, epoch_train_loss=0.0007309789972675246
Epoch 2170
Epoch 2170 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2170, epoch_train_loss=0.0007309789972675246
Epoch 2171
Epoch 2171 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2171, epoch_train_loss=0.0007309789972675246
Epoch 2172
Epoch 2172 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2172, epoch_train_loss=0.0007309789972675246
Epoch 2173
Epoch 2173 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2173, epoch_train_loss=0.0007309789972675246
Epoch 2174
Epoch 2174 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2174, epoch_train_loss=0.0007309789972675246
Epoch 2175
Epoch 2175 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2175, epoch_train_loss=0.0007309789972675246
Epoch 2176
Epoch 2176 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2176, epoch_train_loss=0.0007309789972675246
Epoch 2177
Epoch 2177 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2177, epoch_train_loss=0.0007309789972675246
Epoch 2178
Epoch 2178 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2178, epoch_train_loss=0.0007309789972675246
Epoch 2179
Epoch 2179 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2179, epoch_train_loss=0.0007309789972675246
Epoch 2180
Epoch 2180 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2180, epoch_train_loss=0.0007309789972675246
Epoch 2181
Epoch 2181 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2181, epoch_train_loss=0.0007309789972675246
Epoch 2182
Epoch 2182 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2182, epoch_train_loss=0.0007309789972675246
Epoch 2183
Epoch 2183 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2183, epoch_train_loss=0.0007309789972675246
Epoch 2184
Epoch 2184 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2184, epoch_train_loss=0.0007309789972675246
Epoch 2185
Epoch 2185 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2185, epoch_train_loss=0.0007309789972675246
Epoch 2186
Epoch 2186 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2186, epoch_train_loss=0.0007309789972675246
Epoch 2187
Epoch 2187 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2187, epoch_train_loss=0.0007309789972675246
Epoch 2188
Epoch 2188 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2188, epoch_train_loss=0.0007309789972675246
Epoch 2189
Epoch 2189 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2189, epoch_train_loss=0.0007309789972675246
Epoch 2190
Epoch 2190 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2190, epoch_train_loss=0.0007309789972675246
Epoch 2191
Epoch 2191 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2191, epoch_train_loss=0.0007309789972675246
Epoch 2192
Epoch 2192 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2192, epoch_train_loss=0.0007309789972675246
Epoch 2193
Epoch 2193 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2193, epoch_train_loss=0.0007309789972675246
Epoch 2194
Epoch 2194 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2194, epoch_train_loss=0.0007309789972675246
Epoch 2195
Epoch 2195 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2195, epoch_train_loss=0.0007309789972675246
Epoch 2196
Epoch 2196 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2196, epoch_train_loss=0.0007309789972675246
Epoch 2197
Epoch 2197 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2197, epoch_train_loss=0.0007309789972675246
Epoch 2198
Epoch 2198 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2198, epoch_train_loss=0.0007309789972675246
Epoch 2199
Epoch 2199 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2199, epoch_train_loss=0.0007309789972675246
Epoch 2200
Epoch 2200 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2200, epoch_train_loss=0.0007309789972675246
Epoch 2201
Epoch 2201 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2201, epoch_train_loss=0.0007309789972675246
Epoch 2202
Epoch 2202 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2202, epoch_train_loss=0.0007309789972675246
Epoch 2203
Epoch 2203 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2203, epoch_train_loss=0.0007309789972675246
Epoch 2204
Epoch 2204 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2204, epoch_train_loss=0.0007309789972675246
Epoch 2205
Epoch 2205 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2205, epoch_train_loss=0.0007309789972675246
Epoch 2206
Epoch 2206 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2206, epoch_train_loss=0.0007309789972675246
Epoch 2207
Epoch 2207 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2207, epoch_train_loss=0.0007309789972675246
Epoch 2208
Epoch 2208 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2208, epoch_train_loss=0.0007309789972675246
Epoch 2209
Epoch 2209 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2209, epoch_train_loss=0.0007309789972675246
Epoch 2210
Epoch 2210 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2210, epoch_train_loss=0.0007309789972675246
Epoch 2211
Epoch 2211 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2211, epoch_train_loss=0.0007309789972675246
Epoch 2212
Epoch 2212 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2212, epoch_train_loss=0.0007309789972675246
Epoch 2213
Epoch 2213 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2213, epoch_train_loss=0.0007309789972675246
Epoch 2214
Epoch 2214 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2214, epoch_train_loss=0.0007309789972675246
Epoch 2215
Epoch 2215 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2215, epoch_train_loss=0.0007309789972675246
Epoch 2216
Epoch 2216 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2216, epoch_train_loss=0.0007309789972675246
Epoch 2217
Epoch 2217 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2217, epoch_train_loss=0.0007309789972675246
Epoch 2218
Epoch 2218 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2218, epoch_train_loss=0.0007309789972675246
Epoch 2219
Epoch 2219 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2219, epoch_train_loss=0.0007309789972675246
Epoch 2220
Epoch 2220 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2220, epoch_train_loss=0.0007309789972675246
Epoch 2221
Epoch 2221 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2221, epoch_train_loss=0.0007309789972675246
Epoch 2222
Epoch 2222 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2222, epoch_train_loss=0.0007309789972675246
Epoch 2223
Epoch 2223 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2223, epoch_train_loss=0.0007309789972675246
Epoch 2224
Epoch 2224 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2224, epoch_train_loss=0.0007309789972675246
Epoch 2225
Epoch 2225 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2225, epoch_train_loss=0.0007309789972675246
Epoch 2226
Epoch 2226 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2226, epoch_train_loss=0.0007309789972675246
Epoch 2227
Epoch 2227 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2227, epoch_train_loss=0.0007309789972675246
Epoch 2228
Epoch 2228 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2228, epoch_train_loss=0.0007309789972675246
Epoch 2229
Epoch 2229 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2229, epoch_train_loss=0.0007309789972675246
Epoch 2230
Epoch 2230 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2230, epoch_train_loss=0.0007309789972675246
Epoch 2231
Epoch 2231 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2231, epoch_train_loss=0.0007309789972675246
Epoch 2232
Epoch 2232 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2232, epoch_train_loss=0.0007309789972675246
Epoch 2233
Epoch 2233 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2233, epoch_train_loss=0.0007309789972675246
Epoch 2234
Epoch 2234 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2234, epoch_train_loss=0.0007309789972675246
Epoch 2235
Epoch 2235 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2235, epoch_train_loss=0.0007309789972675246
Epoch 2236
Epoch 2236 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2236, epoch_train_loss=0.0007309789972675246
Epoch 2237
Epoch 2237 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2237, epoch_train_loss=0.0007309789972675246
Epoch 2238
Epoch 2238 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2238, epoch_train_loss=0.0007309789972675246
Epoch 2239
Epoch 2239 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2239, epoch_train_loss=0.0007309789972675246
Epoch 2240
Epoch 2240 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2240, epoch_train_loss=0.0007309789972675246
Epoch 2241
Epoch 2241 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2241, epoch_train_loss=0.0007309789972675246
Epoch 2242
Epoch 2242 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2242, epoch_train_loss=0.0007309789972675246
Epoch 2243
Epoch 2243 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2243, epoch_train_loss=0.0007309789972675246
Epoch 2244
Epoch 2244 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2244, epoch_train_loss=0.0007309789972675246
Epoch 2245
Epoch 2245 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2245, epoch_train_loss=0.0007309789972675246
Epoch 2246
Epoch 2246 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2246, epoch_train_loss=0.0007309789972675246
Epoch 2247
Epoch 2247 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2247, epoch_train_loss=0.0007309789972675246
Epoch 2248
Epoch 2248 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2248, epoch_train_loss=0.0007309789972675246
Epoch 2249
Epoch 2249 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2249, epoch_train_loss=0.0007309789972675246
Epoch 2250
Epoch 2250 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2250, epoch_train_loss=0.0007309789972675246
Epoch 2251
Epoch 2251 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2251, epoch_train_loss=0.0007309789972675246
Epoch 2252
Epoch 2252 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2252, epoch_train_loss=0.0007309789972675246
Epoch 2253
Epoch 2253 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2253, epoch_train_loss=0.0007309789972675246
Epoch 2254
Epoch 2254 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2254, epoch_train_loss=0.0007309789972675246
Epoch 2255
Epoch 2255 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2255, epoch_train_loss=0.0007309789972675246
Epoch 2256
Epoch 2256 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2256, epoch_train_loss=0.0007309789972675246
Epoch 2257
Epoch 2257 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2257, epoch_train_loss=0.0007309789972675246
Epoch 2258
Epoch 2258 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2258, epoch_train_loss=0.0007309789972675246
Epoch 2259
Epoch 2259 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2259, epoch_train_loss=0.0007309789972675246
Epoch 2260
Epoch 2260 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2260, epoch_train_loss=0.0007309789972675246
Epoch 2261
Epoch 2261 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2261, epoch_train_loss=0.0007309789972675246
Epoch 2262
Epoch 2262 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2262, epoch_train_loss=0.0007309789972675246
Epoch 2263
Epoch 2263 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2263, epoch_train_loss=0.0007309789972675246
Epoch 2264
Epoch 2264 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2264, epoch_train_loss=0.0007309789972675246
Epoch 2265
Epoch 2265 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2265, epoch_train_loss=0.0007309789972675246
Epoch 2266
Epoch 2266 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2266, epoch_train_loss=0.0007309789972675246
Epoch 2267
Epoch 2267 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2267, epoch_train_loss=0.0007309789972675246
Epoch 2268
Epoch 2268 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2268, epoch_train_loss=0.0007309789972675246
Epoch 2269
Epoch 2269 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2269, epoch_train_loss=0.0007309789972675246
Epoch 2270
Epoch 2270 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2270, epoch_train_loss=0.0007309789972675246
Epoch 2271
Epoch 2271 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2271, epoch_train_loss=0.0007309789972675246
Epoch 2272
Epoch 2272 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2272, epoch_train_loss=0.0007309789972675246
Epoch 2273
Epoch 2273 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2273, epoch_train_loss=0.0007309789972675246
Epoch 2274
Epoch 2274 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2274, epoch_train_loss=0.0007309789972675246
Epoch 2275
Epoch 2275 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2275, epoch_train_loss=0.0007309789972675246
Epoch 2276
Epoch 2276 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2276, epoch_train_loss=0.0007309789972675246
Epoch 2277
Epoch 2277 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2277, epoch_train_loss=0.0007309789972675246
Epoch 2278
Epoch 2278 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2278, epoch_train_loss=0.0007309789972675246
Epoch 2279
Epoch 2279 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2279, epoch_train_loss=0.0007309789972675246
Epoch 2280
Epoch 2280 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2280, epoch_train_loss=0.0007309789972675246
Epoch 2281
Epoch 2281 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2281, epoch_train_loss=0.0007309789972675246
Epoch 2282
Epoch 2282 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2282, epoch_train_loss=0.0007309789972675246
Epoch 2283
Epoch 2283 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2283, epoch_train_loss=0.0007309789972675246
Epoch 2284
Epoch 2284 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2284, epoch_train_loss=0.0007309789972675246
Epoch 2285
Epoch 2285 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2285, epoch_train_loss=0.0007309789972675246
Epoch 2286
Epoch 2286 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2286, epoch_train_loss=0.0007309789972675246
Epoch 2287
Epoch 2287 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2287, epoch_train_loss=0.0007309789972675246
Epoch 2288
Epoch 2288 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2288, epoch_train_loss=0.0007309789972675246
Epoch 2289
Epoch 2289 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2289, epoch_train_loss=0.0007309789972675246
Epoch 2290
Epoch 2290 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2290, epoch_train_loss=0.0007309789972675246
Epoch 2291
Epoch 2291 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2291, epoch_train_loss=0.0007309789972675246
Epoch 2292
Epoch 2292 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2292, epoch_train_loss=0.0007309789972675246
Epoch 2293
Epoch 2293 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2293, epoch_train_loss=0.0007309789972675246
Epoch 2294
Epoch 2294 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2294, epoch_train_loss=0.0007309789972675246
Epoch 2295
Epoch 2295 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2295, epoch_train_loss=0.0007309789972675246
Epoch 2296
Epoch 2296 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2296, epoch_train_loss=0.0007309789972675246
Epoch 2297
Epoch 2297 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2297, epoch_train_loss=0.0007309789972675246
Epoch 2298
Epoch 2298 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2298, epoch_train_loss=0.0007309789972675246
Epoch 2299
Epoch 2299 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2299, epoch_train_loss=0.0007309789972675246
Epoch 2300
Epoch 2300 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2300, epoch_train_loss=0.0007309789972675246
Epoch 2301
Epoch 2301 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2301, epoch_train_loss=0.0007309789972675246
Epoch 2302
Epoch 2302 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2302, epoch_train_loss=0.0007309789972675246
Epoch 2303
Epoch 2303 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2303, epoch_train_loss=0.0007309789972675246
Epoch 2304
Epoch 2304 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2304, epoch_train_loss=0.0007309789972675246
Epoch 2305
Epoch 2305 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2305, epoch_train_loss=0.0007309789972675246
Epoch 2306
Epoch 2306 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2306, epoch_train_loss=0.0007309789972675246
Epoch 2307
Epoch 2307 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2307, epoch_train_loss=0.0007309789972675246
Epoch 2308
Epoch 2308 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2308, epoch_train_loss=0.0007309789972675246
Epoch 2309
Epoch 2309 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2309, epoch_train_loss=0.0007309789972675246
Epoch 2310
Epoch 2310 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2310, epoch_train_loss=0.0007309789972675246
Epoch 2311
Epoch 2311 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2311, epoch_train_loss=0.0007309789972675246
Epoch 2312
Epoch 2312 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2312, epoch_train_loss=0.0007309789972675246
Epoch 2313
Epoch 2313 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2313, epoch_train_loss=0.0007309789972675246
Epoch 2314
Epoch 2314 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2314, epoch_train_loss=0.0007309789972675246
Epoch 2315
Epoch 2315 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2315, epoch_train_loss=0.0007309789972675246
Epoch 2316
Epoch 2316 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2316, epoch_train_loss=0.0007309789972675246
Epoch 2317
Epoch 2317 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2317, epoch_train_loss=0.0007309789972675246
Epoch 2318
Epoch 2318 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2318, epoch_train_loss=0.0007309789972675246
Epoch 2319
Epoch 2319 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2319, epoch_train_loss=0.0007309789972675246
Epoch 2320
Epoch 2320 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2320, epoch_train_loss=0.0007309789972675246
Epoch 2321
Epoch 2321 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2321, epoch_train_loss=0.0007309789972675246
Epoch 2322
Epoch 2322 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2322, epoch_train_loss=0.0007309789972675246
Epoch 2323
Epoch 2323 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2323, epoch_train_loss=0.0007309789972675246
Epoch 2324
Epoch 2324 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2324, epoch_train_loss=0.0007309789972675246
Epoch 2325
Epoch 2325 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2325, epoch_train_loss=0.0007309789972675246
Epoch 2326
Epoch 2326 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2326, epoch_train_loss=0.0007309789972675246
Epoch 2327
Epoch 2327 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2327, epoch_train_loss=0.0007309789972675246
Epoch 2328
Epoch 2328 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2328, epoch_train_loss=0.0007309789972675246
Epoch 2329
Epoch 2329 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2329, epoch_train_loss=0.0007309789972675246
Epoch 2330
Epoch 2330 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2330, epoch_train_loss=0.0007309789972675246
Epoch 2331
Epoch 2331 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2331, epoch_train_loss=0.0007309789972675246
Epoch 2332
Epoch 2332 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2332, epoch_train_loss=0.0007309789972675246
Epoch 2333
Epoch 2333 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2333, epoch_train_loss=0.0007309789972675246
Epoch 2334
Epoch 2334 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2334, epoch_train_loss=0.0007309789972675246
Epoch 2335
Epoch 2335 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2335, epoch_train_loss=0.0007309789972675246
Epoch 2336
Epoch 2336 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2336, epoch_train_loss=0.0007309789972675246
Epoch 2337
Epoch 2337 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2337, epoch_train_loss=0.0007309789972675246
Epoch 2338
Epoch 2338 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2338, epoch_train_loss=0.0007309789972675246
Epoch 2339
Epoch 2339 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2339, epoch_train_loss=0.0007309789972675246
Epoch 2340
Epoch 2340 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2340, epoch_train_loss=0.0007309789972675246
Epoch 2341
Epoch 2341 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2341, epoch_train_loss=0.0007309789972675246
Epoch 2342
Epoch 2342 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2342, epoch_train_loss=0.0007309789972675246
Epoch 2343
Epoch 2343 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2343, epoch_train_loss=0.0007309789972675246
Epoch 2344
Epoch 2344 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2344, epoch_train_loss=0.0007309789972675246
Epoch 2345
Epoch 2345 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2345, epoch_train_loss=0.0007309789972675246
Epoch 2346
Epoch 2346 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2346, epoch_train_loss=0.0007309789972675246
Epoch 2347
Epoch 2347 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2347, epoch_train_loss=0.0007309789972675246
Epoch 2348
Epoch 2348 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2348, epoch_train_loss=0.0007309789972675246
Epoch 2349
Epoch 2349 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2349, epoch_train_loss=0.0007309789972675246
Epoch 2350
Epoch 2350 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2350, epoch_train_loss=0.0007309789972675246
Epoch 2351
Epoch 2351 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2351, epoch_train_loss=0.0007309789972675246
Epoch 2352
Epoch 2352 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2352, epoch_train_loss=0.0007309789972675246
Epoch 2353
Epoch 2353 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2353, epoch_train_loss=0.0007309789972675246
Epoch 2354
Epoch 2354 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2354, epoch_train_loss=0.0007309789972675246
Epoch 2355
Epoch 2355 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2355, epoch_train_loss=0.0007309789972675246
Epoch 2356
Epoch 2356 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2356, epoch_train_loss=0.0007309789972675246
Epoch 2357
Epoch 2357 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2357, epoch_train_loss=0.0007309789972675246
Epoch 2358
Epoch 2358 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2358, epoch_train_loss=0.0007309789972675246
Epoch 2359
Epoch 2359 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2359, epoch_train_loss=0.0007309789972675246
Epoch 2360
Epoch 2360 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2360, epoch_train_loss=0.0007309789972675246
Epoch 2361
Epoch 2361 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2361, epoch_train_loss=0.0007309789972675246
Epoch 2362
Epoch 2362 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2362, epoch_train_loss=0.0007309789972675246
Epoch 2363
Epoch 2363 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2363, epoch_train_loss=0.0007309789972675246
Epoch 2364
Epoch 2364 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2364, epoch_train_loss=0.0007309789972675246
Epoch 2365
Epoch 2365 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2365, epoch_train_loss=0.0007309789972675246
Epoch 2366
Epoch 2366 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2366, epoch_train_loss=0.0007309789972675246
Epoch 2367
Epoch 2367 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2367, epoch_train_loss=0.0007309789972675246
Epoch 2368
Epoch 2368 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2368, epoch_train_loss=0.0007309789972675246
Epoch 2369
Epoch 2369 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2369, epoch_train_loss=0.0007309789972675246
Epoch 2370
Epoch 2370 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2370, epoch_train_loss=0.0007309789972675246
Epoch 2371
Epoch 2371 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2371, epoch_train_loss=0.0007309789972675246
Epoch 2372
Epoch 2372 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2372, epoch_train_loss=0.0007309789972675246
Epoch 2373
Epoch 2373 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2373, epoch_train_loss=0.0007309789972675246
Epoch 2374
Epoch 2374 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2374, epoch_train_loss=0.0007309789972675246
Epoch 2375
Epoch 2375 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2375, epoch_train_loss=0.0007309789972675246
Epoch 2376
Epoch 2376 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2376, epoch_train_loss=0.0007309789972675246
Epoch 2377
Epoch 2377 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2377, epoch_train_loss=0.0007309789972675246
Epoch 2378
Epoch 2378 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2378, epoch_train_loss=0.0007309789972675246
Epoch 2379
Epoch 2379 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2379, epoch_train_loss=0.0007309789972675246
Epoch 2380
Epoch 2380 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2380, epoch_train_loss=0.0007309789972675246
Epoch 2381
Epoch 2381 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2381, epoch_train_loss=0.0007309789972675246
Epoch 2382
Epoch 2382 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2382, epoch_train_loss=0.0007309789972675246
Epoch 2383
Epoch 2383 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2383, epoch_train_loss=0.0007309789972675246
Epoch 2384
Epoch 2384 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2384, epoch_train_loss=0.0007309789972675246
Epoch 2385
Epoch 2385 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2385, epoch_train_loss=0.0007309789972675246
Epoch 2386
Epoch 2386 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2386, epoch_train_loss=0.0007309789972675246
Epoch 2387
Epoch 2387 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2387, epoch_train_loss=0.0007309789972675246
Epoch 2388
Epoch 2388 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2388, epoch_train_loss=0.0007309789972675246
Epoch 2389
Epoch 2389 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2389, epoch_train_loss=0.0007309789972675246
Epoch 2390
Epoch 2390 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2390, epoch_train_loss=0.0007309789972675246
Epoch 2391
Epoch 2391 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2391, epoch_train_loss=0.0007309789972675246
Epoch 2392
Epoch 2392 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2392, epoch_train_loss=0.0007309789972675246
Epoch 2393
Epoch 2393 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2393, epoch_train_loss=0.0007309789972675246
Epoch 2394
Epoch 2394 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2394, epoch_train_loss=0.0007309789972675246
Epoch 2395
Epoch 2395 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2395, epoch_train_loss=0.0007309789972675246
Epoch 2396
Epoch 2396 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2396, epoch_train_loss=0.0007309789972675246
Epoch 2397
Epoch 2397 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2397, epoch_train_loss=0.0007309789972675246
Epoch 2398
Epoch 2398 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2398, epoch_train_loss=0.0007309789972675246
Epoch 2399
Epoch 2399 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2399, epoch_train_loss=0.0007309789972675246
Epoch 2400
Epoch 2400 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2400, epoch_train_loss=0.0007309789972675246
Epoch 2401
Epoch 2401 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2401, epoch_train_loss=0.0007309789972675246
Epoch 2402
Epoch 2402 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2402, epoch_train_loss=0.0007309789972675246
Epoch 2403
Epoch 2403 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2403, epoch_train_loss=0.0007309789972675246
Epoch 2404
Epoch 2404 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2404, epoch_train_loss=0.0007309789972675246
Epoch 2405
Epoch 2405 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2405, epoch_train_loss=0.0007309789972675246
Epoch 2406
Epoch 2406 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2406, epoch_train_loss=0.0007309789972675246
Epoch 2407
Epoch 2407 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2407, epoch_train_loss=0.0007309789972675246
Epoch 2408
Epoch 2408 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2408, epoch_train_loss=0.0007309789972675246
Epoch 2409
Epoch 2409 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2409, epoch_train_loss=0.0007309789972675246
Epoch 2410
Epoch 2410 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2410, epoch_train_loss=0.0007309789972675246
Epoch 2411
Epoch 2411 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2411, epoch_train_loss=0.0007309789972675246
Epoch 2412
Epoch 2412 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2412, epoch_train_loss=0.0007309789972675246
Epoch 2413
Epoch 2413 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2413, epoch_train_loss=0.0007309789972675246
Epoch 2414
Epoch 2414 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2414, epoch_train_loss=0.0007309789972675246
Epoch 2415
Epoch 2415 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2415, epoch_train_loss=0.0007309789972675246
Epoch 2416
Epoch 2416 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2416, epoch_train_loss=0.0007309789972675246
Epoch 2417
Epoch 2417 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2417, epoch_train_loss=0.0007309789972675246
Epoch 2418
Epoch 2418 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2418, epoch_train_loss=0.0007309789972675246
Epoch 2419
Epoch 2419 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2419, epoch_train_loss=0.0007309789972675246
Epoch 2420
Epoch 2420 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2420, epoch_train_loss=0.0007309789972675246
Epoch 2421
Epoch 2421 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2421, epoch_train_loss=0.0007309789972675246
Epoch 2422
Epoch 2422 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2422, epoch_train_loss=0.0007309789972675246
Epoch 2423
Epoch 2423 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2423, epoch_train_loss=0.0007309789972675246
Epoch 2424
Epoch 2424 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2424, epoch_train_loss=0.0007309789972675246
Epoch 2425
Epoch 2425 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2425, epoch_train_loss=0.0007309789972675246
Epoch 2426
Epoch 2426 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2426, epoch_train_loss=0.0007309789972675246
Epoch 2427
Epoch 2427 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2427, epoch_train_loss=0.0007309789972675246
Epoch 2428
Epoch 2428 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2428, epoch_train_loss=0.0007309789972675246
Epoch 2429
Epoch 2429 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2429, epoch_train_loss=0.0007309789972675246
Epoch 2430
Epoch 2430 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2430, epoch_train_loss=0.0007309789972675246
Epoch 2431
Epoch 2431 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2431, epoch_train_loss=0.0007309789972675246
Epoch 2432
Epoch 2432 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2432, epoch_train_loss=0.0007309789972675246
Epoch 2433
Epoch 2433 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2433, epoch_train_loss=0.0007309789972675246
Epoch 2434
Epoch 2434 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2434, epoch_train_loss=0.0007309789972675246
Epoch 2435
Epoch 2435 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2435, epoch_train_loss=0.0007309789972675246
Epoch 2436
Epoch 2436 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2436, epoch_train_loss=0.0007309789972675246
Epoch 2437
Epoch 2437 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2437, epoch_train_loss=0.0007309789972675246
Epoch 2438
Epoch 2438 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2438, epoch_train_loss=0.0007309789972675246
Epoch 2439
Epoch 2439 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2439, epoch_train_loss=0.0007309789972675246
Epoch 2440
Epoch 2440 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2440, epoch_train_loss=0.0007309789972675246
Epoch 2441
Epoch 2441 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2441, epoch_train_loss=0.0007309789972675246
Epoch 2442
Epoch 2442 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2442, epoch_train_loss=0.0007309789972675246
Epoch 2443
Epoch 2443 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2443, epoch_train_loss=0.0007309789972675246
Epoch 2444
Epoch 2444 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2444, epoch_train_loss=0.0007309789972675246
Epoch 2445
Epoch 2445 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2445, epoch_train_loss=0.0007309789972675246
Epoch 2446
Epoch 2446 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2446, epoch_train_loss=0.0007309789972675246
Epoch 2447
Epoch 2447 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2447, epoch_train_loss=0.0007309789972675246
Epoch 2448
Epoch 2448 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2448, epoch_train_loss=0.0007309789972675246
Epoch 2449
Epoch 2449 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2449, epoch_train_loss=0.0007309789972675246
Epoch 2450
Epoch 2450 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2450, epoch_train_loss=0.0007309789972675246
Epoch 2451
Epoch 2451 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2451, epoch_train_loss=0.0007309789972675246
Epoch 2452
Epoch 2452 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2452, epoch_train_loss=0.0007309789972675246
Epoch 2453
Epoch 2453 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2453, epoch_train_loss=0.0007309789972675246
Epoch 2454
Epoch 2454 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2454, epoch_train_loss=0.0007309789972675246
Epoch 2455
Epoch 2455 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2455, epoch_train_loss=0.0007309789972675246
Epoch 2456
Epoch 2456 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2456, epoch_train_loss=0.0007309789972675246
Epoch 2457
Epoch 2457 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2457, epoch_train_loss=0.0007309789972675246
Epoch 2458
Epoch 2458 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2458, epoch_train_loss=0.0007309789972675246
Epoch 2459
Epoch 2459 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2459, epoch_train_loss=0.0007309789972675246
Epoch 2460
Epoch 2460 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2460, epoch_train_loss=0.0007309789972675246
Epoch 2461
Epoch 2461 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2461, epoch_train_loss=0.0007309789972675246
Epoch 2462
Epoch 2462 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2462, epoch_train_loss=0.0007309789972675246
Epoch 2463
Epoch 2463 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2463, epoch_train_loss=0.0007309789972675246
Epoch 2464
Epoch 2464 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2464, epoch_train_loss=0.0007309789972675246
Epoch 2465
Epoch 2465 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2465, epoch_train_loss=0.0007309789972675246
Epoch 2466
Epoch 2466 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2466, epoch_train_loss=0.0007309789972675246
Epoch 2467
Epoch 2467 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2467, epoch_train_loss=0.0007309789972675246
Epoch 2468
Epoch 2468 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2468, epoch_train_loss=0.0007309789972675246
Epoch 2469
Epoch 2469 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2469, epoch_train_loss=0.0007309789972675246
Epoch 2470
Epoch 2470 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2470, epoch_train_loss=0.0007309789972675246
Epoch 2471
Epoch 2471 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2471, epoch_train_loss=0.0007309789972675246
Epoch 2472
Epoch 2472 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2472, epoch_train_loss=0.0007309789972675246
Epoch 2473
Epoch 2473 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2473, epoch_train_loss=0.0007309789972675246
Epoch 2474
Epoch 2474 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2474, epoch_train_loss=0.0007309789972675246
Epoch 2475
Epoch 2475 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2475, epoch_train_loss=0.0007309789972675246
Epoch 2476
Epoch 2476 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2476, epoch_train_loss=0.0007309789972675246
Epoch 2477
Epoch 2477 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2477, epoch_train_loss=0.0007309789972675246
Epoch 2478
Epoch 2478 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2478, epoch_train_loss=0.0007309789972675246
Epoch 2479
Epoch 2479 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2479, epoch_train_loss=0.0007309789972675246
Epoch 2480
Epoch 2480 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2480, epoch_train_loss=0.0007309789972675246
Epoch 2481
Epoch 2481 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2481, epoch_train_loss=0.0007309789972675246
Epoch 2482
Epoch 2482 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2482, epoch_train_loss=0.0007309789972675246
Epoch 2483
Epoch 2483 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2483, epoch_train_loss=0.0007309789972675246
Epoch 2484
Epoch 2484 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2484, epoch_train_loss=0.0007309789972675246
Epoch 2485
Epoch 2485 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2485, epoch_train_loss=0.0007309789972675246
Epoch 2486
Epoch 2486 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2486, epoch_train_loss=0.0007309789972675246
Epoch 2487
Epoch 2487 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2487, epoch_train_loss=0.0007309789972675246
Epoch 2488
Epoch 2488 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2488, epoch_train_loss=0.0007309789972675246
Epoch 2489
Epoch 2489 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2489, epoch_train_loss=0.0007309789972675246
Epoch 2490
Epoch 2490 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2490, epoch_train_loss=0.0007309789972675246
Epoch 2491
Epoch 2491 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2491, epoch_train_loss=0.0007309789972675246
Epoch 2492
Epoch 2492 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2492, epoch_train_loss=0.0007309789972675246
Epoch 2493
Epoch 2493 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2493, epoch_train_loss=0.0007309789972675246
Epoch 2494
Epoch 2494 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2494, epoch_train_loss=0.0007309789972675246
Epoch 2495
Epoch 2495 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2495, epoch_train_loss=0.0007309789972675246
Epoch 2496
Epoch 2496 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2496, epoch_train_loss=0.0007309789972675246
Epoch 2497
Epoch 2497 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2497, epoch_train_loss=0.0007309789972675246
Epoch 2498
Epoch 2498 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2498, epoch_train_loss=0.0007309789972675246
Epoch 2499
Epoch 2499 :: Batch 0/1
Batch Loss = 0.0007309789972675246
2499, epoch_train_loss=0.0007309789972675246
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting "B3LYP_WITH_VWN5 = True" in pyscf_conf.py
  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0ea950> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0ea950> in UKS object of <class 'pyscf.dft.uks.UKS'>
<pyscf.gto.mole.Mole object at 0x7ffeac0ea950> [['P', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac0ea8f0> [['N', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac0e83d0> [['H', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac0ea9b0> [['Li', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac0e8670> [['O', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac0e8fa0> [['Cl', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac0e91b0> [['Al', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac0e87c0> [['S', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac0e9630> [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0e95a0> [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0e9450> [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0e9a20> [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeac0ea050> [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeac0e9ae0> [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0ea4a0> [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0ea4d0> [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0e9ed0> [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeac0ea770> [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0ea680> [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0ea5f0> [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0ea920> [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0eab30> [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeac0ea8c0> [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeac0eadd0> [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0eaa70> [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]] 5
<pyscf.gto.mole.Mole object at 0x7ffeac0eaf20> [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeac0eb250> [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]] 5
mol:  [['P', array([0., 0., 0.])]]
converged SCF energy = -341.104145992717  <S^2> = 3.7502984  2S+1 = 4.0001492
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0ea8f0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0ea8f0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00051022 -0.00019156 -0.00051334 ... -0.02830887 -0.02830887
 -0.02830887] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 3)
mol:  [['N', array([0., 0., 0.])]]
converged SCF energy = -54.5289742046675  <S^2> = 3.7524945  2S+1 = 4.0012471
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0e83d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0e83d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5016, 22)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-3.60081838e-04 -1.08775305e-04 -1.31917160e-05 ... -2.74817476e-02
 -2.74817476e-02 -2.74817476e-02] = ,SCAN
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5016, 22)
rho shape (4, 4, 2, 5016)
rho_filt shape: (5016,)
get descriptors tdrho.shape=(5016, 3)
mol:  [['H', array([0., 0., 0.])]]
converged SCF energy = -0.499812984008539  <S^2> = 0.75  2S+1 = 2
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0ea9b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0ea9b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 7, 7)
ao.shape (10, 2440, 7)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.92637310e-09 -1.31700807e-07 -9.61527370e-06 ... -7.49400542e-16
 -7.49400542e-16 -7.49400542e-16] = ,SCAN
get_data, dm shape = (2, 7, 7)
ao_eval.shape=(4, 2440, 7)
rho shape (4, 4, 2, 2440)
rho_filt shape: (2440,)
get descriptors tdrho.shape=(2440, 3)
mol:  [['Li', array([0., 0., 0.])]]
converged SCF energy = -7.46006188627842  <S^2> = 0.75000049  2S+1 = 2.0000005
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0e8670> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0e8670> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 4592, 22)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-5.31884219e-04 -2.81911891e-04 -2.81911891e-04 ... -1.27154711e-05
 -2.64861768e-02 -2.64861768e-02] = ,SCAN
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 4592, 22)
rho shape (4, 4, 2, 4592)
rho_filt shape: (4592,)
get descriptors tdrho.shape=(4592, 3)
mol:  [['O', array([0., 0., 0.])]]
converged SCF energy = -75.0033771319123  <S^2> = 2.0027452  2S+1 = 3.0018296
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0e8fa0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0e8fa0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5040, 22)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.04592535e-04 -1.93746627e-05 -1.03530428e-06 ... -2.76158561e-02
 -2.76158561e-02 -2.76158561e-02] = ,SCAN
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5040, 22)
rho shape (4, 4, 2, 5040)
rho_filt shape: (5040,)
get descriptors tdrho.shape=(5040, 3)
mol:  [['Cl', array([0., 0., 0.])]]
converged SCF energy = -459.957577120489  <S^2> = 0.7516194  2S+1 = 2.0016187
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0e91b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0e91b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6152, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.96840340e-04 -2.41462945e-04 -8.24371181e-05 ... -2.84484387e-02
 -2.84484387e-02 -2.84484387e-02] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6152, 30)
rho shape (4, 4, 2, 6152)
rho_filt shape: (6152,)
get descriptors tdrho.shape=(6152, 3)
mol:  [['Al', array([0., 0., 0.])]]
converged SCF energy = -242.226560989222  <S^2> = 0.75226414  2S+1 = 2.0022629
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0e87c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0e87c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6088, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.12430400e-03 -1.34730791e-03 -6.90281808e-04 ... -2.71611201e-05
 -1.90435860e-04 -1.52226957e-05] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6088, 30)
rho shape (4, 4, 2, 6088)
rho_filt shape: (6088,)
get descriptors tdrho.shape=(6088, 3)
mol:  [['S', array([0., 0., 0.])]]
converged SCF energy = -397.938786807705  <S^2> = 2.0022329  2S+1 = 3.0014882
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0e9630> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0e9630> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00039854 -0.00017275 -0.00023834 ... -0.02838402 -0.02838402
 -0.02838402] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 3)
mol:  [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]]
converged SCF energy = -1.16580491182913  <S^2> = -8.8817842e-16  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0e95a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0e95a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 14, 14)
ao.shape (10, 4776, 14)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.43725660e-05 -1.02204686e-06 -4.05575842e-05 ... -2.36278434e-02
 -2.36278434e-02 -2.36278434e-02] = ,SCAN
get_data, dm shape = (2, 14, 14)
ao_eval.shape=(4, 4776, 14)
rho shape (4, 4, 2, 4776)
rho_filt shape: (4776,)
get descriptors tdrho.shape=(4776, 3)
mol:  [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]]
converged SCF energy = -109.439263799566  <S^2> = 1.0658141e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0e9450> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0e9450> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9848, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.89629699e-05 -2.76172354e-04 -7.59017288e-05 ... -7.34654212e-06
 -7.34654212e-06 -2.89629699e-05] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9848, 44)
rho shape (4, 4, 2, 9848)
rho_filt shape: (9848,)
get descriptors tdrho.shape=(9848, 3)
mol:  [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]]
converged SCF energy = -107.339357395734  <S^2> = 1.7763568e-15  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0e9a20> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0e9a20> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9752, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00043469 -0.00024024 -0.00035532 ... -0.00047537 -0.03728133
 -0.03728133] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9752, 44)
rho shape (4, 4, 2, 9752)
rho_filt shape: (9752,)
get descriptors tdrho.shape=(9752, 3)
mol:  [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]]
converged SCF energy = -93.337792446513  <S^2> = 4.0072745e-10  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0ea050> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0ea050> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 12256, 51)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-9.02468888e-05 -7.92694658e-06 -9.80568469e-06 ... -4.33714150e-02
 -4.33714150e-02 -4.33714150e-02] = ,SCAN
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 12256, 51)
rho shape (4, 4, 2, 12256)
rho_filt shape: (12256,)
get descriptors tdrho.shape=(12256, 3)
mol:  [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]]
converged SCF energy = -188.456965322843  <S^2> = 1.7763568e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0e9ae0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0e9ae0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 66, 66)
ao.shape (10, 14920, 66)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-7.48187338e-05 -6.19475249e-05 -2.61742784e-04 ... -8.70042314e-07
 -2.73391097e-02 -2.73391097e-02] = ,SCAN
get_data, dm shape = (2, 66, 66)
ao_eval.shape=(4, 14920, 66)
rho shape (4, 4, 2, 14920)
rho_filt shape: (14920,)
get descriptors tdrho.shape=(14920, 3)
mol:  [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]]
converged SCF energy = -920.005608888959  <S^2> = 4.9737992e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0ea4a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0ea4a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12208, 60)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00051559 -0.00027432 -0.00088583 ... -0.00027432 -0.04174728
 -0.04174728] = ,SCAN
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12208, 60)
rho shape (4, 4, 2, 12208)
rho_filt shape: (12208,)
get descriptors tdrho.shape=(12208, 3)
mol:  [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]]
converged SCF energy = -199.394370591172  <S^2> = 1.1546319e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0ea4d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0ea4d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9824, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-5.53951178e-05 -5.93507199e-06 -3.10072916e-04 ... -5.94325581e-02
 -5.94325581e-02 -5.94325581e-02] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9824, 44)
rho shape (4, 4, 2, 9824)
rho_filt shape: (9824,)
get descriptors tdrho.shape=(9824, 3)
mol:  [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]]
converged SCF energy = -150.21489448715  <S^2> = 1.0018599  2S+1 = 2.2377309
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0e9ed0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0e9ed0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9912, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.56050702e-04 -2.93826342e-05 -1.60238025e-06 ... -4.22396712e-02
 -4.22396712e-02 -4.22396712e-02] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9912, 44)
rho shape (4, 4, 2, 9912)
rho_filt shape: (9912,)
get descriptors tdrho.shape=(9912, 3)
mol:  [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]]
converged SCF energy = -77.2435048346373  <S^2> = 1.3322676e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0ea770> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0ea770> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 58, 58)
ao.shape (10, 15208, 58)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.72190712e-05 -2.72190712e-05 -2.84904833e-04 ... -1.08108260e-05
 -1.03072478e-05 -1.03072478e-05] = ,SCAN
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15208, 58)
rho shape (4, 4, 2, 15208)
rho_filt shape: (15208,)
get descriptors tdrho.shape=(15208, 3)
mol:  [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]]
converged SCF energy = -113.221335689652  <S^2> = 6.6346928e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0ea680> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0ea680> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 10040, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00015688 -0.00024669 -0.00068269 ... -0.03791166 -0.03791166
 -0.03791166] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 10040, 44)
rho shape (4, 4, 2, 10040)
rho_filt shape: (10040,)
get descriptors tdrho.shape=(10040, 3)
mol:  [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]]
converged SCF energy = -460.624592374078  <S^2> = 6.3948846e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0ea5f0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0ea5f0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 37, 37)
ao.shape (10, 8552, 37)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-7.28500681e-05 -5.65091132e-06 -7.37932132e-06 ... -4.76689214e-02
 -4.76689214e-02 -4.76689214e-02] = ,SCAN
get_data, dm shape = (2, 37, 37)
ao_eval.shape=(4, 8552, 37)
rho shape (4, 4, 2, 8552)
rho_filt shape: (8552,)
get descriptors tdrho.shape=(8552, 3)
mol:  [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]]
converged SCF energy = -8.04458854018922  <S^2> = 7.4384943e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0ea920> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0ea920> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 29, 29)
ao.shape (10, 6936, 29)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.0003863  -0.00040095 -0.00040095 ... -0.0213199  -0.0213199
 -0.0213199 ] = ,SCAN
get_data, dm shape = (2, 29, 29)
ao_eval.shape=(4, 6936, 29)
rho shape (4, 4, 2, 6936)
rho_filt shape: (6936,)
get descriptors tdrho.shape=(6936, 3)
mol:  [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]]
converged SCF energy = -324.340512506578  <S^2> = 1.5859314e-11  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0eab30> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0eab30> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 11536, 60)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00088473 -0.00088473 -0.00116894 ... -0.00088473 -0.00088473
 -0.00116894] = ,SCAN
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 11536, 60)
rho shape (4, 4, 2, 11536)
rho_filt shape: (11536,)
get descriptors tdrho.shape=(11536, 3)
mol:  [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]]
converged SCF energy = -1622.57507845814  <S^2> = 8.31335e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0ea8c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0ea8c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 120, 120)
ao.shape (10, 24512, 120)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.91408540e-05 -1.46971271e-04 -1.08734417e-03 ... -2.81566369e-02
 -2.81566369e-02 -2.81566369e-02] = ,SCAN
get_data, dm shape = (2, 120, 120)
ao_eval.shape=(4, 24512, 120)
rho shape (4, 4, 2, 24512)
rho_filt shape: (24512,)
get descriptors tdrho.shape=(24512, 3)
mol:  [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]]
converged SCF energy = -342.979728469574  <S^2> = 2.5389468e-11  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0eadd0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0eadd0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 13096, 51)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.39373335e-04 -1.31641332e-04 -1.15950750e-05 ... -7.32416564e-02
 -7.32416564e-02 -7.32416564e-02] = ,SCAN
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 13096, 51)
rho shape (4, 4, 2, 13096)
rho_filt shape: (13096,)
get descriptors tdrho.shape=(13096, 3)
mol:  [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]]
converged SCF energy = -578.565336133527  <S^2> = 1.0034705  2S+1 = 2.2391699
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0eaa70> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0eaa70> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12384, 60)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.84683071e-05 -7.80528266e-05 -7.80563243e-05 ... -2.92531360e-02
 -2.92531360e-02 -2.92531360e-02] = ,SCAN
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12384, 60)
rho shape (4, 4, 2, 12384)
rho_filt shape: (12384,)
get descriptors tdrho.shape=(12384, 3)
mol:  [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]]
converged SCF energy = -40.4598214864075  <S^2> = 3.2152059e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0eaf20> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0eaf20> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 50, 50)
ao.shape (10, 13936, 50)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.56165538e-04 -7.34744214e-05 -5.30574304e-06 ... -7.93995702e-06
 -7.93995702e-06 -7.93995702e-06] = ,SCAN
get_data, dm shape = (2, 50, 50)
ao_eval.shape=(4, 13936, 50)
rho shape (4, 4, 2, 13936)
rho_filt shape: (13936,)
get descriptors tdrho.shape=(13936, 3)
mol:  [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]]
converged SCF energy = -39.0756147483504  <S^2> = 6.202594e-12  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0eb250> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0eb250> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 36, 36)
ao.shape (10, 9656, 36)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.28290753e-04 -1.41305449e-05 -6.13700492e-05 ... -2.47993463e-02
 -2.47993463e-02 -2.47993463e-02] = ,SCAN
get_data, dm shape = (2, 36, 36)
ao_eval.shape=(4, 9656, 36)
rho shape (4, 4, 2, 9656)
rho_filt shape: (9656,)
get descriptors tdrho.shape=(9656, 3)
mol:  [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]]
converged SCF energy = -291.719272437819  <S^2> = 1.3152146e-11  2S+1 = 1
New DM shape: (2, 58, 58)
ao.shape (10, 15256, 58)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.45512011e-04 -7.12775692e-05 -5.48666345e-06 ... -6.02613084e-06
 -6.02613084e-06 -6.02613084e-06] = ,SCAN
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15256, 58)
rho shape (4, 4, 2, 15256)
rho_filt shape: (15256,)
get descriptors tdrho.shape=(15256, 3)
PRE NAN FILT: tFxc.shape=(237021,), tdrho.shape=(237021, 3)
nan_filt_rho.shape=(237021,)
nan_filt_fxc.shape=(237021,)
tFxc.shape=(237021,), tdrho.shape=(237021, 3)
inp[0].shape = (237021, 3)
Epoch 0
Epoch 0 :: Batch 0/1
Batch Loss = 0.3728621146944484
0, epoch_train_loss=0.3728621146944484
Epoch 1
Epoch 1 :: Batch 0/1
Batch Loss = 0.2855422337871976
1, epoch_train_loss=0.2855422337871976
Epoch 2
Epoch 2 :: Batch 0/1
Batch Loss = 0.2118083009855103
2, epoch_train_loss=0.2118083009855103
Epoch 3
Epoch 3 :: Batch 0/1
Batch Loss = 0.16880768616960695
3, epoch_train_loss=0.16880768616960695
Epoch 4
Epoch 4 :: Batch 0/1
Batch Loss = 0.13646048725186966
4, epoch_train_loss=0.13646048725186966
Epoch 5
Epoch 5 :: Batch 0/1
Batch Loss = 0.09986100284786377
5, epoch_train_loss=0.09986100284786377
Epoch 6
Epoch 6 :: Batch 0/1
Batch Loss = 0.06969580515970862
6, epoch_train_loss=0.06969580515970862
Epoch 7
Epoch 7 :: Batch 0/1
Batch Loss = 0.04911163452127574
7, epoch_train_loss=0.04911163452127574
Epoch 8
Epoch 8 :: Batch 0/1
Batch Loss = 0.033884070767576276
8, epoch_train_loss=0.033884070767576276
Epoch 9
Epoch 9 :: Batch 0/1
Batch Loss = 0.021788526632113503
9, epoch_train_loss=0.021788526632113503
Epoch 10
Epoch 10 :: Batch 0/1
Batch Loss = 0.012729201799961681
10, epoch_train_loss=0.012729201799961681
Epoch 11
Epoch 11 :: Batch 0/1
Batch Loss = 0.006791944930947394
11, epoch_train_loss=0.006791944930947394
Epoch 12
Epoch 12 :: Batch 0/1
Batch Loss = 0.003477952800438089
12, epoch_train_loss=0.003477952800438089
Epoch 13
Epoch 13 :: Batch 0/1
Batch Loss = 0.0018909198933812284
13, epoch_train_loss=0.0018909198933812284
Epoch 14
Epoch 14 :: Batch 0/1
Batch Loss = 0.0012103678980823556
14, epoch_train_loss=0.0012103678980823556
Epoch 15
Epoch 15 :: Batch 0/1
Batch Loss = 0.0009326224415143844
15, epoch_train_loss=0.0009326224415143844
Epoch 16
Epoch 16 :: Batch 0/1
Batch Loss = 0.0008181355179019865
16, epoch_train_loss=0.0008181355179019865
Epoch 17
Epoch 17 :: Batch 0/1
Batch Loss = 0.0007697715046912293
17, epoch_train_loss=0.0007697715046912293
Epoch 18
Epoch 18 :: Batch 0/1
Batch Loss = 0.0007489494181056799
18, epoch_train_loss=0.0007489494181056799
Epoch 19
Epoch 19 :: Batch 0/1
Batch Loss = 0.0007397423970973714
19, epoch_train_loss=0.0007397423970973714
Epoch 20
Epoch 20 :: Batch 0/1
Batch Loss = 0.0007355055616904177
20, epoch_train_loss=0.0007355055616904177
Epoch 21
Epoch 21 :: Batch 0/1
Batch Loss = 0.0007334572793993467
21, epoch_train_loss=0.0007334572793993467
Epoch 22
Epoch 22 :: Batch 0/1
Batch Loss = 0.0007324132283472668
22, epoch_train_loss=0.0007324132283472668
Epoch 23
Epoch 23 :: Batch 0/1
Batch Loss = 0.0007318527003148666
23, epoch_train_loss=0.0007318527003148666
Epoch 24
Epoch 24 :: Batch 0/1
Batch Loss = 0.0007315368730540877
24, epoch_train_loss=0.0007315368730540877
Epoch 25
Epoch 25 :: Batch 0/1
Batch Loss = 0.0007313509659730517
25, epoch_train_loss=0.0007313509659730517
Epoch 26
Epoch 26 :: Batch 0/1
Batch Loss = 0.0007312371609434509
26, epoch_train_loss=0.0007312371609434509
Epoch 27
Epoch 27 :: Batch 0/1
Batch Loss = 0.0007311650061778319
27, epoch_train_loss=0.0007311650061778319
Epoch 28
Epoch 28 :: Batch 0/1
Batch Loss = 0.0007311177929216755
28, epoch_train_loss=0.0007311177929216755
Epoch 29
Epoch 29 :: Batch 0/1
Batch Loss = 0.0007310860069794465
29, epoch_train_loss=0.0007310860069794465
Epoch 30
Epoch 30 :: Batch 0/1
Batch Loss = 0.0007310640467702071
30, epoch_train_loss=0.0007310640467702071
Epoch 31
Epoch 31 :: Batch 0/1
Batch Loss = 0.0007310485132530573
31, epoch_train_loss=0.0007310485132530573
Epoch 32
Epoch 32 :: Batch 0/1
Batch Loss = 0.0007310372866041618
32, epoch_train_loss=0.0007310372866041618
Epoch 33
Epoch 33 :: Batch 0/1
Batch Loss = 0.000731029011308726
33, epoch_train_loss=0.000731029011308726
Epoch 34
Epoch 34 :: Batch 0/1
Batch Loss = 0.0007310228004616602
34, epoch_train_loss=0.0007310228004616602
Epoch 35
Epoch 35 :: Batch 0/1
Batch Loss = 0.0007310180613538694
35, epoch_train_loss=0.0007310180613538694
Epoch 36
Epoch 36 :: Batch 0/1
Batch Loss = 0.0007310143900483471
36, epoch_train_loss=0.0007310143900483471
Epoch 37
Epoch 37 :: Batch 0/1
Batch Loss = 0.0007310115062054749
37, epoch_train_loss=0.0007310115062054749
Epoch 38
Epoch 38 :: Batch 0/1
Batch Loss = 0.0007310092119428843
38, epoch_train_loss=0.0007310092119428843
Epoch 39
Epoch 39 :: Batch 0/1
Batch Loss = 0.0007310073653592628
39, epoch_train_loss=0.0007310073653592628
Epoch 40
Epoch 40 :: Batch 0/1
Batch Loss = 0.0007310058631851865
40, epoch_train_loss=0.0007310058631851865
Epoch 41
Epoch 41 :: Batch 0/1
Batch Loss = 0.0007310046292213193
41, epoch_train_loss=0.0007310046292213193
Epoch 42
Epoch 42 :: Batch 0/1
Batch Loss = 0.000731003606510685
42, epoch_train_loss=0.000731003606510685
Epoch 43
Epoch 43 :: Batch 0/1
Batch Loss = 0.0007310027519597913
43, epoch_train_loss=0.0007310027519597913
Epoch 44
Epoch 44 :: Batch 0/1
Batch Loss = 0.0007310020325905464
44, epoch_train_loss=0.0007310020325905464
Epoch 45
Epoch 45 :: Batch 0/1
Batch Loss = 0.0007310014228939694
45, epoch_train_loss=0.0007310014228939694
Epoch 46
Epoch 46 :: Batch 0/1
Batch Loss = 0.0007310009029384985
46, epoch_train_loss=0.0007310009029384985
Epoch 47
Epoch 47 :: Batch 0/1
Batch Loss = 0.0007310004570017967
47, epoch_train_loss=0.0007310004570017967
Epoch 48
Epoch 48 :: Batch 0/1
Batch Loss = 0.0007310000725701759
48, epoch_train_loss=0.0007310000725701759
Epoch 49
Epoch 49 :: Batch 0/1
Batch Loss = 0.0007309997395991658
49, epoch_train_loss=0.0007309997395991658
Epoch 50
Epoch 50 :: Batch 0/1
Batch Loss = 0.000730999449961627
50, epoch_train_loss=0.000730999449961627
Epoch 51
Epoch 51 :: Batch 0/1
Batch Loss = 0.000730999197031952
51, epoch_train_loss=0.000730999197031952
Epoch 52
Epoch 52 :: Batch 0/1
Batch Loss = 0.0007309989753793567
52, epoch_train_loss=0.0007309989753793567
Epoch 53
Epoch 53 :: Batch 0/1
Batch Loss = 0.0007309987805043147
53, epoch_train_loss=0.0007309987805043147
Epoch 54
Epoch 54 :: Batch 0/1
Batch Loss = 0.0007309986086638014
54, epoch_train_loss=0.0007309986086638014
Epoch 55
Epoch 55 :: Batch 0/1
Batch Loss = 0.0007309984567253183
55, epoch_train_loss=0.0007309984567253183
Epoch 56
Epoch 56 :: Batch 0/1
Batch Loss = 0.0007309983220523378
56, epoch_train_loss=0.0007309983220523378
Epoch 57
Epoch 57 :: Batch 0/1
Batch Loss = 0.0007309982024138719
57, epoch_train_loss=0.0007309982024138719
Epoch 58
Epoch 58 :: Batch 0/1
Batch Loss = 0.0007309980959125558
58, epoch_train_loss=0.0007309980959125558
Epoch 59
Epoch 59 :: Batch 0/1
Batch Loss = 0.0007309980009270218
59, epoch_train_loss=0.0007309980009270218
Epoch 60
Epoch 60 :: Batch 0/1
Batch Loss = 0.0007309979160655722
60, epoch_train_loss=0.0007309979160655722
Epoch 61
Epoch 61 :: Batch 0/1
Batch Loss = 0.0007309978401286121
61, epoch_train_loss=0.0007309978401286121
Epoch 62
Epoch 62 :: Batch 0/1
Batch Loss = 0.0007309977720780785
62, epoch_train_loss=0.0007309977720780785
Epoch 63
Epoch 63 :: Batch 0/1
Batch Loss = 0.0007309977110123488
63, epoch_train_loss=0.0007309977110123488
Epoch 64
Epoch 64 :: Batch 0/1
Batch Loss = 0.0007309976561456303
64, epoch_train_loss=0.0007309976561456303
Epoch 65
Epoch 65 :: Batch 0/1
Batch Loss = 0.0007309976067908741
65, epoch_train_loss=0.0007309976067908741
Epoch 66
Epoch 66 :: Batch 0/1
Batch Loss = 0.0007309975623455841
66, epoch_train_loss=0.0007309975623455841
Epoch 67
Epoch 67 :: Batch 0/1
Batch Loss = 0.0007309975222799369
67, epoch_train_loss=0.0007309975222799369
Epoch 68
Epoch 68 :: Batch 0/1
Batch Loss = 0.0007309974861268218
68, epoch_train_loss=0.0007309974861268218
Epoch 69
Epoch 69 :: Batch 0/1
Batch Loss = 0.0007309974534734386
69, epoch_train_loss=0.0007309974534734386
Epoch 70
Epoch 70 :: Batch 0/1
Batch Loss = 0.0007309974239541807
70, epoch_train_loss=0.0007309974239541807
Epoch 71
Epoch 71 :: Batch 0/1
Batch Loss = 0.0007309973972445908
71, epoch_train_loss=0.0007309973972445908
Epoch 72
Epoch 72 :: Batch 0/1
Batch Loss = 0.0007309973730561877
72, epoch_train_loss=0.0007309973730561877
Epoch 73
Epoch 73 :: Batch 0/1
Batch Loss = 0.0007309973511320389
73, epoch_train_loss=0.0007309973511320389
Epoch 74
Epoch 74 :: Batch 0/1
Batch Loss = 0.0007309973312429574
74, epoch_train_loss=0.0007309973312429574
Epoch 75
Epoch 75 :: Batch 0/1
Batch Loss = 0.0007309973131842076
75, epoch_train_loss=0.0007309973131842076
Epoch 76
Epoch 76 :: Batch 0/1
Batch Loss = 0.0007309972967726577
76, epoch_train_loss=0.0007309972967726577
Epoch 77
Epoch 77 :: Batch 0/1
Batch Loss = 0.0007309972818443072
77, epoch_train_loss=0.0007309972818443072
Epoch 78
Epoch 78 :: Batch 0/1
Batch Loss = 0.000730997268252128
78, epoch_train_loss=0.000730997268252128
Epoch 79
Epoch 79 :: Batch 0/1
Batch Loss = 0.000730997255864177
79, epoch_train_loss=0.000730997255864177
Epoch 80
Epoch 80 :: Batch 0/1
Batch Loss = 0.0007309972445619412
80, epoch_train_loss=0.0007309972445619412
Epoch 81
Epoch 81 :: Batch 0/1
Batch Loss = 0.0007309972342388878
81, epoch_train_loss=0.0007309972342388878
Epoch 82
Epoch 82 :: Batch 0/1
Batch Loss = 0.00073099722479918
82, epoch_train_loss=0.00073099722479918
Epoch 83
Epoch 83 :: Batch 0/1
Batch Loss = 0.0007309972161565481
83, epoch_train_loss=0.0007309972161565481
Epoch 84
Epoch 84 :: Batch 0/1
Batch Loss = 0.0007309972082332869
84, epoch_train_loss=0.0007309972082332869
Epoch 85
Epoch 85 :: Batch 0/1
Batch Loss = 0.0007309972009593686
85, epoch_train_loss=0.0007309972009593686
Epoch 86
Epoch 86 :: Batch 0/1
Batch Loss = 0.0007309971942716546
86, epoch_train_loss=0.0007309971942716546
Epoch 87
Epoch 87 :: Batch 0/1
Batch Loss = 0.0007309971881131928
87, epoch_train_loss=0.0007309971881131928
Epoch 88
Epoch 88 :: Batch 0/1
Batch Loss = 0.0007309971824325915
88, epoch_train_loss=0.0007309971824325915
Epoch 89
Epoch 89 :: Batch 0/1
Batch Loss = 0.0007309971771834613
89, epoch_train_loss=0.0007309971771834613
Epoch 90
Epoch 90 :: Batch 0/1
Batch Loss = 0.0007309971723239155
90, epoch_train_loss=0.0007309971723239155
Epoch 91
Epoch 91 :: Batch 0/1
Batch Loss = 0.0007309971678161228
91, epoch_train_loss=0.0007309971678161228
Epoch 92
Epoch 92 :: Batch 0/1
Batch Loss = 0.0007309971636259066
92, epoch_train_loss=0.0007309971636259066
Epoch 93
Epoch 93 :: Batch 0/1
Batch Loss = 0.0007309971597223855
93, epoch_train_loss=0.0007309971597223855
Epoch 94
Epoch 94 :: Batch 0/1
Batch Loss = 0.0007309971560776524
94, epoch_train_loss=0.0007309971560776524
Epoch 95
Epoch 95 :: Batch 0/1
Batch Loss = 0.0007309971526664807
95, epoch_train_loss=0.0007309971526664807
Epoch 96
Epoch 96 :: Batch 0/1
Batch Loss = 0.0007309971494660671
96, epoch_train_loss=0.0007309971494660671
Epoch 97
Epoch 97 :: Batch 0/1
Batch Loss = 0.0007309971464557948
97, epoch_train_loss=0.0007309971464557948
Epoch 98
Epoch 98 :: Batch 0/1
Batch Loss = 0.0007309971436170224
98, epoch_train_loss=0.0007309971436170224
Epoch 99
Epoch 99 :: Batch 0/1
Batch Loss = 0.0007309971409328941
99, epoch_train_loss=0.0007309971409328941
Epoch 100
Epoch 100 :: Batch 0/1
Batch Loss = 0.0007309971383881665
100, epoch_train_loss=0.0007309971383881665
Epoch 101
Epoch 101 :: Batch 0/1
Batch Loss = 0.000730997135969055
101, epoch_train_loss=0.000730997135969055
Epoch 102
Epoch 102 :: Batch 0/1
Batch Loss = 0.0007309971336630925
102, epoch_train_loss=0.0007309971336630925
Epoch 103
Epoch 103 :: Batch 0/1
Batch Loss = 0.0007309971314590046
103, epoch_train_loss=0.0007309971314590046
Epoch 104
Epoch 104 :: Batch 0/1
Batch Loss = 0.0007309971293465944
104, epoch_train_loss=0.0007309971293465944
Epoch 105
Epoch 105 :: Batch 0/1
Batch Loss = 0.0007309971273166398
105, epoch_train_loss=0.0007309971273166398
Epoch 106
Epoch 106 :: Batch 0/1
Batch Loss = 0.0007309971253608009
106, epoch_train_loss=0.0007309971253608009
Epoch 107
Epoch 107 :: Batch 0/1
Batch Loss = 0.0007309971234715349
107, epoch_train_loss=0.0007309971234715349
Epoch 108
Epoch 108 :: Batch 0/1
Batch Loss = 0.0007309971216420209
108, epoch_train_loss=0.0007309971216420209
Epoch 109
Epoch 109 :: Batch 0/1
Batch Loss = 0.0007309971198660903
109, epoch_train_loss=0.0007309971198660903
Epoch 110
Epoch 110 :: Batch 0/1
Batch Loss = 0.000730997118138166
110, epoch_train_loss=0.000730997118138166
Epoch 111
Epoch 111 :: Batch 0/1
Batch Loss = 0.0007309971164532039
111, epoch_train_loss=0.0007309971164532039
Epoch 112
Epoch 112 :: Batch 0/1
Batch Loss = 0.0007309971148066446
112, epoch_train_loss=0.0007309971148066446
Epoch 113
Epoch 113 :: Batch 0/1
Batch Loss = 0.0007309971131943652
113, epoch_train_loss=0.0007309971131943652
Epoch 114
Epoch 114 :: Batch 0/1
Batch Loss = 0.0007309971116126389
114, epoch_train_loss=0.0007309971116126389
Epoch 115
Epoch 115 :: Batch 0/1
Batch Loss = 0.0007309971100580969
115, epoch_train_loss=0.0007309971100580969
Epoch 116
Epoch 116 :: Batch 0/1
Batch Loss = 0.0007309971085276942
116, epoch_train_loss=0.0007309971085276942
Epoch 117
Epoch 117 :: Batch 0/1
Batch Loss = 0.0007309971070186791
117, epoch_train_loss=0.0007309971070186791
Epoch 118
Epoch 118 :: Batch 0/1
Batch Loss = 0.0007309971055285651
118, epoch_train_loss=0.0007309971055285651
Epoch 119
Epoch 119 :: Batch 0/1
Batch Loss = 0.0007309971040551054
119, epoch_train_loss=0.0007309971040551054
Epoch 120
Epoch 120 :: Batch 0/1
Batch Loss = 0.0007309971025962703
120, epoch_train_loss=0.0007309971025962703
Epoch 121
Epoch 121 :: Batch 0/1
Batch Loss = 0.0007309971011502269
121, epoch_train_loss=0.0007309971011502269
Epoch 122
Epoch 122 :: Batch 0/1
Batch Loss = 0.0007309970997153194
122, epoch_train_loss=0.0007309970997153194
Epoch 123
Epoch 123 :: Batch 0/1
Batch Loss = 0.000730997098290053
123, epoch_train_loss=0.000730997098290053
Epoch 124
Epoch 124 :: Batch 0/1
Batch Loss = 0.0007309970968730781
124, epoch_train_loss=0.0007309970968730781
Epoch 125
Epoch 125 :: Batch 0/1
Batch Loss = 0.0007309970954631769
125, epoch_train_loss=0.0007309970954631769
Epoch 126
Epoch 126 :: Batch 0/1
Batch Loss = 0.0007309970940592502
126, epoch_train_loss=0.0007309970940592502
Epoch 127
Epoch 127 :: Batch 0/1
Batch Loss = 0.0007309970926603063
127, epoch_train_loss=0.0007309970926603063
Epoch 128
Epoch 128 :: Batch 0/1
Batch Loss = 0.0007309970912654516
128, epoch_train_loss=0.0007309970912654516
Epoch 129
Epoch 129 :: Batch 0/1
Batch Loss = 0.0007309970898738794
129, epoch_train_loss=0.0007309970898738794
Epoch 130
Epoch 130 :: Batch 0/1
Batch Loss = 0.0007309970884848633
130, epoch_train_loss=0.0007309970884848633
Epoch 131
Epoch 131 :: Batch 0/1
Batch Loss = 0.0007309970870977485
131, epoch_train_loss=0.0007309970870977485
Epoch 132
Epoch 132 :: Batch 0/1
Batch Loss = 0.0007309970857119458
132, epoch_train_loss=0.0007309970857119458
Epoch 133
Epoch 133 :: Batch 0/1
Batch Loss = 0.0007309970843269239
133, epoch_train_loss=0.0007309970843269239
Epoch 134
Epoch 134 :: Batch 0/1
Batch Loss = 0.0007309970829422059
134, epoch_train_loss=0.0007309970829422059
Epoch 135
Epoch 135 :: Batch 0/1
Batch Loss = 0.0007309970815573619
135, epoch_train_loss=0.0007309970815573619
Epoch 136
Epoch 136 :: Batch 0/1
Batch Loss = 0.0007309970801720061
136, epoch_train_loss=0.0007309970801720061
Epoch 137
Epoch 137 :: Batch 0/1
Batch Loss = 0.0007309970787857918
137, epoch_train_loss=0.0007309970787857918
Epoch 138
Epoch 138 :: Batch 0/1
Batch Loss = 0.000730997077398408
138, epoch_train_loss=0.000730997077398408
Epoch 139
Epoch 139 :: Batch 0/1
Batch Loss = 0.0007309970760095755
139, epoch_train_loss=0.0007309970760095755
Epoch 140
Epoch 140 :: Batch 0/1
Batch Loss = 0.0007309970746190443
140, epoch_train_loss=0.0007309970746190443
Epoch 141
Epoch 141 :: Batch 0/1
Batch Loss = 0.0007309970732265904
141, epoch_train_loss=0.0007309970732265904
Epoch 142
Epoch 142 :: Batch 0/1
Batch Loss = 0.0007309970718320142
142, epoch_train_loss=0.0007309970718320142
Epoch 143
Epoch 143 :: Batch 0/1
Batch Loss = 0.0007309970704351366
143, epoch_train_loss=0.0007309970704351366
Epoch 144
Epoch 144 :: Batch 0/1
Batch Loss = 0.0007309970690357984
144, epoch_train_loss=0.0007309970690357984
Epoch 145
Epoch 145 :: Batch 0/1
Batch Loss = 0.0007309970676338574
145, epoch_train_loss=0.0007309970676338574
Epoch 146
Epoch 146 :: Batch 0/1
Batch Loss = 0.0007309970662291879
146, epoch_train_loss=0.0007309970662291879
Epoch 147
Epoch 147 :: Batch 0/1
Batch Loss = 0.0007309970648216778
147, epoch_train_loss=0.0007309970648216778
Epoch 148
Epoch 148 :: Batch 0/1
Batch Loss = 0.000730997063411228
148, epoch_train_loss=0.000730997063411228
Epoch 149
Epoch 149 :: Batch 0/1
Batch Loss = 0.0007309970619977515
149, epoch_train_loss=0.0007309970619977515
Epoch 150
Epoch 150 :: Batch 0/1
Batch Loss = 0.0007309970605811711
150, epoch_train_loss=0.0007309970605811711
Epoch 151
Epoch 151 :: Batch 0/1
Batch Loss = 0.0007309970591614192
151, epoch_train_loss=0.0007309970591614192
Epoch 152
Epoch 152 :: Batch 0/1
Batch Loss = 0.0007309970577384372
152, epoch_train_loss=0.0007309970577384372
Epoch 153
Epoch 153 :: Batch 0/1
Batch Loss = 0.0007309970563121739
153, epoch_train_loss=0.0007309970563121739
Epoch 154
Epoch 154 :: Batch 0/1
Batch Loss = 0.0007309970548825849
154, epoch_train_loss=0.0007309970548825849
Epoch 155
Epoch 155 :: Batch 0/1
Batch Loss = 0.0007309970534496324
155, epoch_train_loss=0.0007309970534496324
Epoch 156
Epoch 156 :: Batch 0/1
Batch Loss = 0.0007309970520132838
156, epoch_train_loss=0.0007309970520132838
Epoch 157
Epoch 157 :: Batch 0/1
Batch Loss = 0.0007309970505735122
157, epoch_train_loss=0.0007309970505735122
Epoch 158
Epoch 158 :: Batch 0/1
Batch Loss = 0.0007309970491302947
158, epoch_train_loss=0.0007309970491302947
Epoch 159
Epoch 159 :: Batch 0/1
Batch Loss = 0.0007309970476836131
159, epoch_train_loss=0.0007309970476836131
Epoch 160
Epoch 160 :: Batch 0/1
Batch Loss = 0.0007309970462334523
160, epoch_train_loss=0.0007309970462334523
Epoch 161
Epoch 161 :: Batch 0/1
Batch Loss = 0.0007309970447798013
161, epoch_train_loss=0.0007309970447798013
Epoch 162
Epoch 162 :: Batch 0/1
Batch Loss = 0.0007309970433226516
162, epoch_train_loss=0.0007309970433226516
Epoch 163
Epoch 163 :: Batch 0/1
Batch Loss = 0.000730997041861997
163, epoch_train_loss=0.000730997041861997
Epoch 164
Epoch 164 :: Batch 0/1
Batch Loss = 0.000730997040397835
164, epoch_train_loss=0.000730997040397835
Epoch 165
Epoch 165 :: Batch 0/1
Batch Loss = 0.0007309970389301642
165, epoch_train_loss=0.0007309970389301642
Epoch 166
Epoch 166 :: Batch 0/1
Batch Loss = 0.0007309970374589856
166, epoch_train_loss=0.0007309970374589856
Epoch 167
Epoch 167 :: Batch 0/1
Batch Loss = 0.000730997035984302
167, epoch_train_loss=0.000730997035984302
Epoch 168
Epoch 168 :: Batch 0/1
Batch Loss = 0.0007309970345061175
168, epoch_train_loss=0.0007309970345061175
Epoch 169
Epoch 169 :: Batch 0/1
Batch Loss = 0.000730997033024438
169, epoch_train_loss=0.000730997033024438
Epoch 170
Epoch 170 :: Batch 0/1
Batch Loss = 0.0007309970315392706
170, epoch_train_loss=0.0007309970315392706
Epoch 171
Epoch 171 :: Batch 0/1
Batch Loss = 0.0007309970300506233
171, epoch_train_loss=0.0007309970300506233
Epoch 172
Epoch 172 :: Batch 0/1
Batch Loss = 0.0007309970285585054
172, epoch_train_loss=0.0007309970285585054
Epoch 173
Epoch 173 :: Batch 0/1
Batch Loss = 0.0007309970270629268
173, epoch_train_loss=0.0007309970270629268
Epoch 174
Epoch 174 :: Batch 0/1
Batch Loss = 0.0007309970255638985
174, epoch_train_loss=0.0007309970255638985
Epoch 175
Epoch 175 :: Batch 0/1
Batch Loss = 0.0007309970240614323
175, epoch_train_loss=0.0007309970240614323
Epoch 176
Epoch 176 :: Batch 0/1
Batch Loss = 0.0007309970225555402
176, epoch_train_loss=0.0007309970225555402
Epoch 177
Epoch 177 :: Batch 0/1
Batch Loss = 0.0007309970210462352
177, epoch_train_loss=0.0007309970210462352
Epoch 178
Epoch 178 :: Batch 0/1
Batch Loss = 0.0007309970195335303
178, epoch_train_loss=0.0007309970195335303
Epoch 179
Epoch 179 :: Batch 0/1
Batch Loss = 0.0007309970180174398
179, epoch_train_loss=0.0007309970180174398
Epoch 180
Epoch 180 :: Batch 0/1
Batch Loss = 0.0007309970164979777
180, epoch_train_loss=0.0007309970164979777
Epoch 181
Epoch 181 :: Batch 0/1
Batch Loss = 0.0007309970149751584
181, epoch_train_loss=0.0007309970149751584
Epoch 182
Epoch 182 :: Batch 0/1
Batch Loss = 0.0007309970134489968
182, epoch_train_loss=0.0007309970134489968
Epoch 183
Epoch 183 :: Batch 0/1
Batch Loss = 0.0007309970119195083
183, epoch_train_loss=0.0007309970119195083
Epoch 184
Epoch 184 :: Batch 0/1
Batch Loss = 0.0007309970103867078
184, epoch_train_loss=0.0007309970103867078
Epoch 185
Epoch 185 :: Batch 0/1
Batch Loss = 0.0007309970088506109
185, epoch_train_loss=0.0007309970088506109
Epoch 186
Epoch 186 :: Batch 0/1
Batch Loss = 0.0007309970073112336
186, epoch_train_loss=0.0007309970073112336
Epoch 187
Epoch 187 :: Batch 0/1
Batch Loss = 0.0007309970057685916
187, epoch_train_loss=0.0007309970057685916
Epoch 188
Epoch 188 :: Batch 0/1
Batch Loss = 0.0007309970042227007
188, epoch_train_loss=0.0007309970042227007
Epoch 189
Epoch 189 :: Batch 0/1
Batch Loss = 0.0007309970026735772
189, epoch_train_loss=0.0007309970026735772
Epoch 190
Epoch 190 :: Batch 0/1
Batch Loss = 0.0007309970011212372
190, epoch_train_loss=0.0007309970011212372
Epoch 191
Epoch 191 :: Batch 0/1
Batch Loss = 0.0007309969995656967
191, epoch_train_loss=0.0007309969995656967
Epoch 192
Epoch 192 :: Batch 0/1
Batch Loss = 0.0007309969980069723
192, epoch_train_loss=0.0007309969980069723
Epoch 193
Epoch 193 :: Batch 0/1
Batch Loss = 0.0007309969964450802
193, epoch_train_loss=0.0007309969964450802
Epoch 194
Epoch 194 :: Batch 0/1
Batch Loss = 0.000730996994880037
194, epoch_train_loss=0.000730996994880037
Epoch 195
Epoch 195 :: Batch 0/1
Batch Loss = 0.0007309969933118585
195, epoch_train_loss=0.0007309969933118585
Epoch 196
Epoch 196 :: Batch 0/1
Batch Loss = 0.0007309969917405613
196, epoch_train_loss=0.0007309969917405613
Epoch 197
Epoch 197 :: Batch 0/1
Batch Loss = 0.0007309969901661619
197, epoch_train_loss=0.0007309969901661619
Epoch 198
Epoch 198 :: Batch 0/1
Batch Loss = 0.0007309969885886766
198, epoch_train_loss=0.0007309969885886766
Epoch 199
Epoch 199 :: Batch 0/1
Batch Loss = 0.0007309969870081216
199, epoch_train_loss=0.0007309969870081216
Epoch 200
Epoch 200 :: Batch 0/1
Batch Loss = 0.0007309969854245133
200, epoch_train_loss=0.0007309969854245133
Epoch 201
Epoch 201 :: Batch 0/1
Batch Loss = 0.000730996983837868
201, epoch_train_loss=0.000730996983837868
Epoch 202
Epoch 202 :: Batch 0/1
Batch Loss = 0.0007309969822482019
202, epoch_train_loss=0.0007309969822482019
Epoch 203
Epoch 203 :: Batch 0/1
Batch Loss = 0.0007309969806555311
203, epoch_train_loss=0.0007309969806555311
Epoch 204
Epoch 204 :: Batch 0/1
Batch Loss = 0.0007309969790598718
204, epoch_train_loss=0.0007309969790598718
Epoch 205
Epoch 205 :: Batch 0/1
Batch Loss = 0.0007309969774612401
205, epoch_train_loss=0.0007309969774612401
Epoch 206
Epoch 206 :: Batch 0/1
Batch Loss = 0.0007309969758596521
206, epoch_train_loss=0.0007309969758596521
Epoch 207
Epoch 207 :: Batch 0/1
Batch Loss = 0.0007309969742551241
207, epoch_train_loss=0.0007309969742551241
Epoch 208
Epoch 208 :: Batch 0/1
Batch Loss = 0.0007309969726476715
208, epoch_train_loss=0.0007309969726476715
Epoch 209
Epoch 209 :: Batch 0/1
Batch Loss = 0.0007309969710373107
209, epoch_train_loss=0.0007309969710373107
Epoch 210
Epoch 210 :: Batch 0/1
Batch Loss = 0.0007309969694240573
210, epoch_train_loss=0.0007309969694240573
Epoch 211
Epoch 211 :: Batch 0/1
Batch Loss = 0.000730996967807927
211, epoch_train_loss=0.000730996967807927
Epoch 212
Epoch 212 :: Batch 0/1
Batch Loss = 0.0007309969661889357
212, epoch_train_loss=0.0007309969661889357
Epoch 213
Epoch 213 :: Batch 0/1
Batch Loss = 0.0007309969645670993
213, epoch_train_loss=0.0007309969645670993
Epoch 214
Epoch 214 :: Batch 0/1
Batch Loss = 0.0007309969629424327
214, epoch_train_loss=0.0007309969629424327
Epoch 215
Epoch 215 :: Batch 0/1
Batch Loss = 0.0007309969613149522
215, epoch_train_loss=0.0007309969613149522
Epoch 216
Epoch 216 :: Batch 0/1
Batch Loss = 0.0007309969596846726
216, epoch_train_loss=0.0007309969596846726
Epoch 217
Epoch 217 :: Batch 0/1
Batch Loss = 0.00073099695805161
217, epoch_train_loss=0.00073099695805161
Epoch 218
Epoch 218 :: Batch 0/1
Batch Loss = 0.0007309969564157793
218, epoch_train_loss=0.0007309969564157793
Epoch 219
Epoch 219 :: Batch 0/1
Batch Loss = 0.0007309969547771957
219, epoch_train_loss=0.0007309969547771957
Epoch 220
Epoch 220 :: Batch 0/1
Batch Loss = 0.0007309969531358745
220, epoch_train_loss=0.0007309969531358745
Epoch 221
Epoch 221 :: Batch 0/1
Batch Loss = 0.0007309969514918313
221, epoch_train_loss=0.0007309969514918313
Epoch 222
Epoch 222 :: Batch 0/1
Batch Loss = 0.0007309969498450807
222, epoch_train_loss=0.0007309969498450807
Epoch 223
Epoch 223 :: Batch 0/1
Batch Loss = 0.0007309969481956376
223, epoch_train_loss=0.0007309969481956376
Epoch 224
Epoch 224 :: Batch 0/1
Batch Loss = 0.0007309969465435174
224, epoch_train_loss=0.0007309969465435174
Epoch 225
Epoch 225 :: Batch 0/1
Batch Loss = 0.0007309969448887349
225, epoch_train_loss=0.0007309969448887349
Epoch 226
Epoch 226 :: Batch 0/1
Batch Loss = 0.0007309969432313048
226, epoch_train_loss=0.0007309969432313048
Epoch 227
Epoch 227 :: Batch 0/1
Batch Loss = 0.0007309969415712418
227, epoch_train_loss=0.0007309969415712418
Epoch 228
Epoch 228 :: Batch 0/1
Batch Loss = 0.0007309969399085604
228, epoch_train_loss=0.0007309969399085604
Epoch 229
Epoch 229 :: Batch 0/1
Batch Loss = 0.0007309969382432757
229, epoch_train_loss=0.0007309969382432757
Epoch 230
Epoch 230 :: Batch 0/1
Batch Loss = 0.0007309969365754022
230, epoch_train_loss=0.0007309969365754022
Epoch 231
Epoch 231 :: Batch 0/1
Batch Loss = 0.0007309969349049543
231, epoch_train_loss=0.0007309969349049543
Epoch 232
Epoch 232 :: Batch 0/1
Batch Loss = 0.0007309969332319461
232, epoch_train_loss=0.0007309969332319461
Epoch 233
Epoch 233 :: Batch 0/1
Batch Loss = 0.0007309969315563924
233, epoch_train_loss=0.0007309969315563924
Epoch 234
Epoch 234 :: Batch 0/1
Batch Loss = 0.0007309969298783074
234, epoch_train_loss=0.0007309969298783074
Epoch 235
Epoch 235 :: Batch 0/1
Batch Loss = 0.0007309969281977055
235, epoch_train_loss=0.0007309969281977055
Epoch 236
Epoch 236 :: Batch 0/1
Batch Loss = 0.0007309969265146005
236, epoch_train_loss=0.0007309969265146005
Epoch 237
Epoch 237 :: Batch 0/1
Batch Loss = 0.0007309969248290068
237, epoch_train_loss=0.0007309969248290068
Epoch 238
Epoch 238 :: Batch 0/1
Batch Loss = 0.0007309969231409383
238, epoch_train_loss=0.0007309969231409383
Epoch 239
Epoch 239 :: Batch 0/1
Batch Loss = 0.0007309969214504093
239, epoch_train_loss=0.0007309969214504093
Epoch 240
Epoch 240 :: Batch 0/1
Batch Loss = 0.0007309969197574332
240, epoch_train_loss=0.0007309969197574332
Epoch 241
Epoch 241 :: Batch 0/1
Batch Loss = 0.0007309969180620245
241, epoch_train_loss=0.0007309969180620245
Epoch 242
Epoch 242 :: Batch 0/1
Batch Loss = 0.0007309969163641967
242, epoch_train_loss=0.0007309969163641967
Epoch 243
Epoch 243 :: Batch 0/1
Batch Loss = 0.0007309969146639635
243, epoch_train_loss=0.0007309969146639635
Epoch 244
Epoch 244 :: Batch 0/1
Batch Loss = 0.0007309969129613387
244, epoch_train_loss=0.0007309969129613387
Epoch 245
Epoch 245 :: Batch 0/1
Batch Loss = 0.000730996911256336
245, epoch_train_loss=0.000730996911256336
Epoch 246
Epoch 246 :: Batch 0/1
Batch Loss = 0.0007309969095489688
246, epoch_train_loss=0.0007309969095489688
Epoch 247
Epoch 247 :: Batch 0/1
Batch Loss = 0.0007309969078392505
247, epoch_train_loss=0.0007309969078392505
Epoch 248
Epoch 248 :: Batch 0/1
Batch Loss = 0.000730996906127195
248, epoch_train_loss=0.000730996906127195
Epoch 249
Epoch 249 :: Batch 0/1
Batch Loss = 0.0007309969044128156
249, epoch_train_loss=0.0007309969044128156
Epoch 250
Epoch 250 :: Batch 0/1
Batch Loss = 0.0007309969026961253
250, epoch_train_loss=0.0007309969026961253
Epoch 251
Epoch 251 :: Batch 0/1
Batch Loss = 0.0007309969009771378
251, epoch_train_loss=0.0007309969009771378
Epoch 252
Epoch 252 :: Batch 0/1
Batch Loss = 0.0007309968992558662
252, epoch_train_loss=0.0007309968992558662
Epoch 253
Epoch 253 :: Batch 0/1
Batch Loss = 0.0007309968975323237
253, epoch_train_loss=0.0007309968975323237
Epoch 254
Epoch 254 :: Batch 0/1
Batch Loss = 0.0007309968958065233
254, epoch_train_loss=0.0007309968958065233
Epoch 255
Epoch 255 :: Batch 0/1
Batch Loss = 0.0007309968940784782
255, epoch_train_loss=0.0007309968940784782
Epoch 256
Epoch 256 :: Batch 0/1
Batch Loss = 0.0007309968923482012
256, epoch_train_loss=0.0007309968923482012
Epoch 257
Epoch 257 :: Batch 0/1
Batch Loss = 0.0007309968906157055
257, epoch_train_loss=0.0007309968906157055
Epoch 258
Epoch 258 :: Batch 0/1
Batch Loss = 0.000730996888881004
258, epoch_train_loss=0.000730996888881004
Epoch 259
Epoch 259 :: Batch 0/1
Batch Loss = 0.0007309968871441094
259, epoch_train_loss=0.0007309968871441094
Epoch 260
Epoch 260 :: Batch 0/1
Batch Loss = 0.0007309968854050346
260, epoch_train_loss=0.0007309968854050346
Epoch 261
Epoch 261 :: Batch 0/1
Batch Loss = 0.0007309968836637923
261, epoch_train_loss=0.0007309968836637923
Epoch 262
Epoch 262 :: Batch 0/1
Batch Loss = 0.0007309968819203953
262, epoch_train_loss=0.0007309968819203953
Epoch 263
Epoch 263 :: Batch 0/1
Batch Loss = 0.0007309968801748562
263, epoch_train_loss=0.0007309968801748562
Epoch 264
Epoch 264 :: Batch 0/1
Batch Loss = 0.0007309968784271875
264, epoch_train_loss=0.0007309968784271875
Epoch 265
Epoch 265 :: Batch 0/1
Batch Loss = 0.0007309968766774018
265, epoch_train_loss=0.0007309968766774018
Epoch 266
Epoch 266 :: Batch 0/1
Batch Loss = 0.0007309968749255115
266, epoch_train_loss=0.0007309968749255115
Epoch 267
Epoch 267 :: Batch 0/1
Batch Loss = 0.0007309968731715294
267, epoch_train_loss=0.0007309968731715294
Epoch 268
Epoch 268 :: Batch 0/1
Batch Loss = 0.0007309968714154675
268, epoch_train_loss=0.0007309968714154675
Epoch 269
Epoch 269 :: Batch 0/1
Batch Loss = 0.000730996869657338
269, epoch_train_loss=0.000730996869657338
Epoch 270
Epoch 270 :: Batch 0/1
Batch Loss = 0.0007309968678971537
270, epoch_train_loss=0.0007309968678971537
Epoch 271
Epoch 271 :: Batch 0/1
Batch Loss = 0.0007309968661349265
271, epoch_train_loss=0.0007309968661349265
Epoch 272
Epoch 272 :: Batch 0/1
Batch Loss = 0.0007309968643706686
272, epoch_train_loss=0.0007309968643706686
Epoch 273
Epoch 273 :: Batch 0/1
Batch Loss = 0.0007309968626043925
273, epoch_train_loss=0.0007309968626043925
Epoch 274
Epoch 274 :: Batch 0/1
Batch Loss = 0.0007309968608361096
274, epoch_train_loss=0.0007309968608361096
Epoch 275
Epoch 275 :: Batch 0/1
Batch Loss = 0.0007309968590658325
275, epoch_train_loss=0.0007309968590658325
Epoch 276
Epoch 276 :: Batch 0/1
Batch Loss = 0.000730996857293573
276, epoch_train_loss=0.000730996857293573
Epoch 277
Epoch 277 :: Batch 0/1
Batch Loss = 0.0007309968555193429
277, epoch_train_loss=0.0007309968555193429
Epoch 278
Epoch 278 :: Batch 0/1
Batch Loss = 0.0007309968537431547
278, epoch_train_loss=0.0007309968537431547
Epoch 279
Epoch 279 :: Batch 0/1
Batch Loss = 0.0007309968519650195
279, epoch_train_loss=0.0007309968519650195
Epoch 280
Epoch 280 :: Batch 0/1
Batch Loss = 0.0007309968501849497
280, epoch_train_loss=0.0007309968501849497
Epoch 281
Epoch 281 :: Batch 0/1
Batch Loss = 0.0007309968484029565
281, epoch_train_loss=0.0007309968484029565
Epoch 282
Epoch 282 :: Batch 0/1
Batch Loss = 0.0007309968466190522
282, epoch_train_loss=0.0007309968466190522
Epoch 283
Epoch 283 :: Batch 0/1
Batch Loss = 0.0007309968448332481
283, epoch_train_loss=0.0007309968448332481
Epoch 284
Epoch 284 :: Batch 0/1
Batch Loss = 0.0007309968430455556
284, epoch_train_loss=0.0007309968430455556
Epoch 285
Epoch 285 :: Batch 0/1
Batch Loss = 0.0007309968412559871
285, epoch_train_loss=0.0007309968412559871
Epoch 286
Epoch 286 :: Batch 0/1
Batch Loss = 0.0007309968394645532
286, epoch_train_loss=0.0007309968394645532
Epoch 287
Epoch 287 :: Batch 0/1
Batch Loss = 0.000730996837671266
287, epoch_train_loss=0.000730996837671266
Epoch 288
Epoch 288 :: Batch 0/1
Batch Loss = 0.0007309968358761368
288, epoch_train_loss=0.0007309968358761368
Epoch 289
Epoch 289 :: Batch 0/1
Batch Loss = 0.0007309968340791767
289, epoch_train_loss=0.0007309968340791767
Epoch 290
Epoch 290 :: Batch 0/1
Batch Loss = 0.0007309968322803974
290, epoch_train_loss=0.0007309968322803974
Epoch 291
Epoch 291 :: Batch 0/1
Batch Loss = 0.0007309968304798101
291, epoch_train_loss=0.0007309968304798101
Epoch 292
Epoch 292 :: Batch 0/1
Batch Loss = 0.0007309968286774263
292, epoch_train_loss=0.0007309968286774263
Epoch 293
Epoch 293 :: Batch 0/1
Batch Loss = 0.000730996826873257
293, epoch_train_loss=0.000730996826873257
Epoch 294
Epoch 294 :: Batch 0/1
Batch Loss = 0.0007309968250673131
294, epoch_train_loss=0.0007309968250673131
Epoch 295
Epoch 295 :: Batch 0/1
Batch Loss = 0.000730996823259606
295, epoch_train_loss=0.000730996823259606
Epoch 296
Epoch 296 :: Batch 0/1
Batch Loss = 0.000730996821450147
296, epoch_train_loss=0.000730996821450147
Epoch 297
Epoch 297 :: Batch 0/1
Batch Loss = 0.0007309968196389469
297, epoch_train_loss=0.0007309968196389469
Epoch 298
Epoch 298 :: Batch 0/1
Batch Loss = 0.000730996817826017
298, epoch_train_loss=0.000730996817826017
Epoch 299
Epoch 299 :: Batch 0/1
Batch Loss = 0.0007309968160113677
299, epoch_train_loss=0.0007309968160113677
Epoch 300
Epoch 300 :: Batch 0/1
Batch Loss = 0.0007309968141950104
300, epoch_train_loss=0.0007309968141950104
Epoch 301
Epoch 301 :: Batch 0/1
Batch Loss = 0.0007309968123769558
301, epoch_train_loss=0.0007309968123769558
Epoch 302
Epoch 302 :: Batch 0/1
Batch Loss = 0.0007309968105572149
302, epoch_train_loss=0.0007309968105572149
Epoch 303
Epoch 303 :: Batch 0/1
Batch Loss = 0.0007309968087357983
303, epoch_train_loss=0.0007309968087357983
Epoch 304
Epoch 304 :: Batch 0/1
Batch Loss = 0.000730996806912717
304, epoch_train_loss=0.000730996806912717
Epoch 305
Epoch 305 :: Batch 0/1
Batch Loss = 0.0007309968050879817
305, epoch_train_loss=0.0007309968050879817
Epoch 306
Epoch 306 :: Batch 0/1
Batch Loss = 0.0007309968032616028
306, epoch_train_loss=0.0007309968032616028
Epoch 307
Epoch 307 :: Batch 0/1
Batch Loss = 0.000730996801433591
307, epoch_train_loss=0.000730996801433591
Epoch 308
Epoch 308 :: Batch 0/1
Batch Loss = 0.0007309967996039574
308, epoch_train_loss=0.0007309967996039574
Epoch 309
Epoch 309 :: Batch 0/1
Batch Loss = 0.000730996797772712
309, epoch_train_loss=0.000730996797772712
Epoch 310
Epoch 310 :: Batch 0/1
Batch Loss = 0.0007309967959398656
310, epoch_train_loss=0.0007309967959398656
Epoch 311
Epoch 311 :: Batch 0/1
Batch Loss = 0.0007309967941054285
311, epoch_train_loss=0.0007309967941054285
Epoch 312
Epoch 312 :: Batch 0/1
Batch Loss = 0.0007309967922694114
312, epoch_train_loss=0.0007309967922694114
Epoch 313
Epoch 313 :: Batch 0/1
Batch Loss = 0.0007309967904318244
313, epoch_train_loss=0.0007309967904318244
Epoch 314
Epoch 314 :: Batch 0/1
Batch Loss = 0.0007309967885926781
314, epoch_train_loss=0.0007309967885926781
Epoch 315
Epoch 315 :: Batch 0/1
Batch Loss = 0.000730996786751983
315, epoch_train_loss=0.000730996786751983
Epoch 316
Epoch 316 :: Batch 0/1
Batch Loss = 0.0007309967849097489
316, epoch_train_loss=0.0007309967849097489
Epoch 317
Epoch 317 :: Batch 0/1
Batch Loss = 0.0007309967830659863
317, epoch_train_loss=0.0007309967830659863
Epoch 318
Epoch 318 :: Batch 0/1
Batch Loss = 0.0007309967812207055
318, epoch_train_loss=0.0007309967812207055
Epoch 319
Epoch 319 :: Batch 0/1
Batch Loss = 0.0007309967793739168
319, epoch_train_loss=0.0007309967793739168
Epoch 320
Epoch 320 :: Batch 0/1
Batch Loss = 0.0007309967775256299
320, epoch_train_loss=0.0007309967775256299
Epoch 321
Epoch 321 :: Batch 0/1
Batch Loss = 0.0007309967756758553
321, epoch_train_loss=0.0007309967756758553
Epoch 322
Epoch 322 :: Batch 0/1
Batch Loss = 0.000730996773824603
322, epoch_train_loss=0.000730996773824603
Epoch 323
Epoch 323 :: Batch 0/1
Batch Loss = 0.0007309967719718828
323, epoch_train_loss=0.0007309967719718828
Epoch 324
Epoch 324 :: Batch 0/1
Batch Loss = 0.000730996770117705
324, epoch_train_loss=0.000730996770117705
Epoch 325
Epoch 325 :: Batch 0/1
Batch Loss = 0.0007309967682620794
325, epoch_train_loss=0.0007309967682620794
Epoch 326
Epoch 326 :: Batch 0/1
Batch Loss = 0.000730996766405016
326, epoch_train_loss=0.000730996766405016
Epoch 327
Epoch 327 :: Batch 0/1
Batch Loss = 0.0007309967645465246
327, epoch_train_loss=0.0007309967645465246
Epoch 328
Epoch 328 :: Batch 0/1
Batch Loss = 0.000730996762686615
328, epoch_train_loss=0.000730996762686615
Epoch 329
Epoch 329 :: Batch 0/1
Batch Loss = 0.0007309967608252971
329, epoch_train_loss=0.0007309967608252971
Epoch 330
Epoch 330 :: Batch 0/1
Batch Loss = 0.0007309967589625809
330, epoch_train_loss=0.0007309967589625809
Epoch 331
Epoch 331 :: Batch 0/1
Batch Loss = 0.0007309967570984756
331, epoch_train_loss=0.0007309967570984756
Epoch 332
Epoch 332 :: Batch 0/1
Batch Loss = 0.0007309967552329914
332, epoch_train_loss=0.0007309967552329914
Epoch 333
Epoch 333 :: Batch 0/1
Batch Loss = 0.0007309967533661378
333, epoch_train_loss=0.0007309967533661378
Epoch 334
Epoch 334 :: Batch 0/1
Batch Loss = 0.0007309967514979245
334, epoch_train_loss=0.0007309967514979245
Epoch 335
Epoch 335 :: Batch 0/1
Batch Loss = 0.0007309967496283607
335, epoch_train_loss=0.0007309967496283607
Epoch 336
Epoch 336 :: Batch 0/1
Batch Loss = 0.0007309967477574567
336, epoch_train_loss=0.0007309967477574567
Epoch 337
Epoch 337 :: Batch 0/1
Batch Loss = 0.0007309967458852215
337, epoch_train_loss=0.0007309967458852215
Epoch 338
Epoch 338 :: Batch 0/1
Batch Loss = 0.0007309967440116647
338, epoch_train_loss=0.0007309967440116647
Epoch 339
Epoch 339 :: Batch 0/1
Batch Loss = 0.0007309967421367958
339, epoch_train_loss=0.0007309967421367958
Epoch 340
Epoch 340 :: Batch 0/1
Batch Loss = 0.0007309967402606244
340, epoch_train_loss=0.0007309967402606244
Epoch 341
Epoch 341 :: Batch 0/1
Batch Loss = 0.0007309967383831597
341, epoch_train_loss=0.0007309967383831597
Epoch 342
Epoch 342 :: Batch 0/1
Batch Loss = 0.0007309967365044112
342, epoch_train_loss=0.0007309967365044112
Epoch 343
Epoch 343 :: Batch 0/1
Batch Loss = 0.000730996734624388
343, epoch_train_loss=0.000730996734624388
Epoch 344
Epoch 344 :: Batch 0/1
Batch Loss = 0.0007309967327430996
344, epoch_train_loss=0.0007309967327430996
Epoch 345
Epoch 345 :: Batch 0/1
Batch Loss = 0.000730996730860555
345, epoch_train_loss=0.000730996730860555
Epoch 346
Epoch 346 :: Batch 0/1
Batch Loss = 0.0007309967289767641
346, epoch_train_loss=0.0007309967289767641
Epoch 347
Epoch 347 :: Batch 0/1
Batch Loss = 0.0007309967270917353
347, epoch_train_loss=0.0007309967270917353
Epoch 348
Epoch 348 :: Batch 0/1
Batch Loss = 0.0007309967252054782
348, epoch_train_loss=0.0007309967252054782
Epoch 349
Epoch 349 :: Batch 0/1
Batch Loss = 0.0007309967233180017
349, epoch_train_loss=0.0007309967233180017
Epoch 350
Epoch 350 :: Batch 0/1
Batch Loss = 0.0007309967214293152
350, epoch_train_loss=0.0007309967214293152
Epoch 351
Epoch 351 :: Batch 0/1
Batch Loss = 0.0007309967195394275
351, epoch_train_loss=0.0007309967195394275
Epoch 352
Epoch 352 :: Batch 0/1
Batch Loss = 0.0007309967176483479
352, epoch_train_loss=0.0007309967176483479
Epoch 353
Epoch 353 :: Batch 0/1
Batch Loss = 0.0007309967157560854
353, epoch_train_loss=0.0007309967157560854
Epoch 354
Epoch 354 :: Batch 0/1
Batch Loss = 0.0007309967138626485
354, epoch_train_loss=0.0007309967138626485
Epoch 355
Epoch 355 :: Batch 0/1
Batch Loss = 0.0007309967119680465
355, epoch_train_loss=0.0007309967119680465
Epoch 356
Epoch 356 :: Batch 0/1
Batch Loss = 0.0007309967100722882
356, epoch_train_loss=0.0007309967100722882
Epoch 357
Epoch 357 :: Batch 0/1
Batch Loss = 0.0007309967081753826
357, epoch_train_loss=0.0007309967081753826
Epoch 358
Epoch 358 :: Batch 0/1
Batch Loss = 0.0007309967062773387
358, epoch_train_loss=0.0007309967062773387
Epoch 359
Epoch 359 :: Batch 0/1
Batch Loss = 0.0007309967043781649
359, epoch_train_loss=0.0007309967043781649
Epoch 360
Epoch 360 :: Batch 0/1
Batch Loss = 0.0007309967024778702
360, epoch_train_loss=0.0007309967024778702
Epoch 361
Epoch 361 :: Batch 0/1
Batch Loss = 0.0007309967005764633
361, epoch_train_loss=0.0007309967005764633
Epoch 362
Epoch 362 :: Batch 0/1
Batch Loss = 0.0007309966986739531
362, epoch_train_loss=0.0007309966986739531
Epoch 363
Epoch 363 :: Batch 0/1
Batch Loss = 0.0007309966967703483
363, epoch_train_loss=0.0007309966967703483
Epoch 364
Epoch 364 :: Batch 0/1
Batch Loss = 0.0007309966948656573
364, epoch_train_loss=0.0007309966948656573
Epoch 365
Epoch 365 :: Batch 0/1
Batch Loss = 0.0007309966929598887
365, epoch_train_loss=0.0007309966929598887
Epoch 366
Epoch 366 :: Batch 0/1
Batch Loss = 0.0007309966910530515
366, epoch_train_loss=0.0007309966910530515
Epoch 367
Epoch 367 :: Batch 0/1
Batch Loss = 0.0007309966891451538
367, epoch_train_loss=0.0007309966891451538
Epoch 368
Epoch 368 :: Batch 0/1
Batch Loss = 0.0007309966872362045
368, epoch_train_loss=0.0007309966872362045
Epoch 369
Epoch 369 :: Batch 0/1
Batch Loss = 0.0007309966853262119
369, epoch_train_loss=0.0007309966853262119
Epoch 370
Epoch 370 :: Batch 0/1
Batch Loss = 0.0007309966834151848
370, epoch_train_loss=0.0007309966834151848
Epoch 371
Epoch 371 :: Batch 0/1
Batch Loss = 0.0007309966815031312
371, epoch_train_loss=0.0007309966815031312
Epoch 372
Epoch 372 :: Batch 0/1
Batch Loss = 0.0007309966795900597
372, epoch_train_loss=0.0007309966795900597
Epoch 373
Epoch 373 :: Batch 0/1
Batch Loss = 0.0007309966776759787
373, epoch_train_loss=0.0007309966776759787
Epoch 374
Epoch 374 :: Batch 0/1
Batch Loss = 0.0007309966757608966
374, epoch_train_loss=0.0007309966757608966
Epoch 375
Epoch 375 :: Batch 0/1
Batch Loss = 0.0007309966738448219
375, epoch_train_loss=0.0007309966738448219
Epoch 376
Epoch 376 :: Batch 0/1
Batch Loss = 0.0007309966719277624
376, epoch_train_loss=0.0007309966719277624
Epoch 377
Epoch 377 :: Batch 0/1
Batch Loss = 0.000730996670009727
377, epoch_train_loss=0.000730996670009727
Epoch 378
Epoch 378 :: Batch 0/1
Batch Loss = 0.0007309966680907234
378, epoch_train_loss=0.0007309966680907234
Epoch 379
Epoch 379 :: Batch 0/1
Batch Loss = 0.0007309966661707601
379, epoch_train_loss=0.0007309966661707601
Epoch 380
Epoch 380 :: Batch 0/1
Batch Loss = 0.0007309966642498453
380, epoch_train_loss=0.0007309966642498453
Epoch 381
Epoch 381 :: Batch 0/1
Batch Loss = 0.000730996662327987
381, epoch_train_loss=0.000730996662327987
Epoch 382
Epoch 382 :: Batch 0/1
Batch Loss = 0.0007309966604051935
382, epoch_train_loss=0.0007309966604051935
Epoch 383
Epoch 383 :: Batch 0/1
Batch Loss = 0.0007309966584814727
383, epoch_train_loss=0.0007309966584814727
Epoch 384
Epoch 384 :: Batch 0/1
Batch Loss = 0.0007309966565568328
384, epoch_train_loss=0.0007309966565568328
Epoch 385
Epoch 385 :: Batch 0/1
Batch Loss = 0.0007309966546312819
385, epoch_train_loss=0.0007309966546312819
Epoch 386
Epoch 386 :: Batch 0/1
Batch Loss = 0.0007309966527048279
386, epoch_train_loss=0.0007309966527048279
Epoch 387
Epoch 387 :: Batch 0/1
Batch Loss = 0.0007309966507774789
387, epoch_train_loss=0.0007309966507774789
Epoch 388
Epoch 388 :: Batch 0/1
Batch Loss = 0.0007309966488492429
388, epoch_train_loss=0.0007309966488492429
Epoch 389
Epoch 389 :: Batch 0/1
Batch Loss = 0.0007309966469201277
389, epoch_train_loss=0.0007309966469201277
Epoch 390
Epoch 390 :: Batch 0/1
Batch Loss = 0.0007309966449901414
390, epoch_train_loss=0.0007309966449901414
Epoch 391
Epoch 391 :: Batch 0/1
Batch Loss = 0.0007309966430592914
391, epoch_train_loss=0.0007309966430592914
Epoch 392
Epoch 392 :: Batch 0/1
Batch Loss = 0.0007309966411275862
392, epoch_train_loss=0.0007309966411275862
Epoch 393
Epoch 393 :: Batch 0/1
Batch Loss = 0.000730996639195033
393, epoch_train_loss=0.000730996639195033
Epoch 394
Epoch 394 :: Batch 0/1
Batch Loss = 0.0007309966372616402
394, epoch_train_loss=0.0007309966372616402
Epoch 395
Epoch 395 :: Batch 0/1
Batch Loss = 0.0007309966353274152
395, epoch_train_loss=0.0007309966353274152
Epoch 396
Epoch 396 :: Batch 0/1
Batch Loss = 0.0007309966333923656
396, epoch_train_loss=0.0007309966333923656
Epoch 397
Epoch 397 :: Batch 0/1
Batch Loss = 0.0007309966314564994
397, epoch_train_loss=0.0007309966314564994
Epoch 398
Epoch 398 :: Batch 0/1
Batch Loss = 0.0007309966295198241
398, epoch_train_loss=0.0007309966295198241
Epoch 399
Epoch 399 :: Batch 0/1
Batch Loss = 0.0007309966275823477
399, epoch_train_loss=0.0007309966275823477
Epoch 400
Epoch 400 :: Batch 0/1
Batch Loss = 0.0007309966256440775
400, epoch_train_loss=0.0007309966256440775
Epoch 401
Epoch 401 :: Batch 0/1
Batch Loss = 0.0007309966237050213
401, epoch_train_loss=0.0007309966237050213
Epoch 402
Epoch 402 :: Batch 0/1
Batch Loss = 0.0007309966217651866
402, epoch_train_loss=0.0007309966217651866
Epoch 403
Epoch 403 :: Batch 0/1
Batch Loss = 0.0007309966198245808
403, epoch_train_loss=0.0007309966198245808
Epoch 404
Epoch 404 :: Batch 0/1
Batch Loss = 0.0007309966178832116
404, epoch_train_loss=0.0007309966178832116
Epoch 405
Epoch 405 :: Batch 0/1
Batch Loss = 0.0007309966159410866
405, epoch_train_loss=0.0007309966159410866
Epoch 406
Epoch 406 :: Batch 0/1
Batch Loss = 0.0007309966139982132
406, epoch_train_loss=0.0007309966139982132
Epoch 407
Epoch 407 :: Batch 0/1
Batch Loss = 0.0007309966120545986
407, epoch_train_loss=0.0007309966120545986
Epoch 408
Epoch 408 :: Batch 0/1
Batch Loss = 0.0007309966101102509
408, epoch_train_loss=0.0007309966101102509
Epoch 409
Epoch 409 :: Batch 0/1
Batch Loss = 0.0007309966081651767
409, epoch_train_loss=0.0007309966081651767
Epoch 410
Epoch 410 :: Batch 0/1
Batch Loss = 0.0007309966062193839
410, epoch_train_loss=0.0007309966062193839
Epoch 411
Epoch 411 :: Batch 0/1
Batch Loss = 0.0007309966042728796
411, epoch_train_loss=0.0007309966042728796
Epoch 412
Epoch 412 :: Batch 0/1
Batch Loss = 0.000730996602325671
412, epoch_train_loss=0.000730996602325671
Epoch 413
Epoch 413 :: Batch 0/1
Batch Loss = 0.0007309966003777658
413, epoch_train_loss=0.0007309966003777658
Epoch 414
Epoch 414 :: Batch 0/1
Batch Loss = 0.0007309965984291708
414, epoch_train_loss=0.0007309965984291708
Epoch 415
Epoch 415 :: Batch 0/1
Batch Loss = 0.0007309965964798936
415, epoch_train_loss=0.0007309965964798936
Epoch 416
Epoch 416 :: Batch 0/1
Batch Loss = 0.0007309965945299415
416, epoch_train_loss=0.0007309965945299415
Epoch 417
Epoch 417 :: Batch 0/1
Batch Loss = 0.0007309965925793214
417, epoch_train_loss=0.0007309965925793214
Epoch 418
Epoch 418 :: Batch 0/1
Batch Loss = 0.0007309965906280406
418, epoch_train_loss=0.0007309965906280406
Epoch 419
Epoch 419 :: Batch 0/1
Batch Loss = 0.000730996588676106
419, epoch_train_loss=0.000730996588676106
Epoch 420
Epoch 420 :: Batch 0/1
Batch Loss = 0.0007309965867235254
420, epoch_train_loss=0.0007309965867235254
Epoch 421
Epoch 421 :: Batch 0/1
Batch Loss = 0.0007309965847703053
421, epoch_train_loss=0.0007309965847703053
Epoch 422
Epoch 422 :: Batch 0/1
Batch Loss = 0.000730996582816453
422, epoch_train_loss=0.000730996582816453
Epoch 423
Epoch 423 :: Batch 0/1
Batch Loss = 0.0007309965808619754
423, epoch_train_loss=0.0007309965808619754
Epoch 424
Epoch 424 :: Batch 0/1
Batch Loss = 0.0007309965789068797
424, epoch_train_loss=0.0007309965789068797
Epoch 425
Epoch 425 :: Batch 0/1
Batch Loss = 0.0007309965769511727
425, epoch_train_loss=0.0007309965769511727
Epoch 426
Epoch 426 :: Batch 0/1
Batch Loss = 0.0007309965749948615
426, epoch_train_loss=0.0007309965749948615
Epoch 427
Epoch 427 :: Batch 0/1
Batch Loss = 0.0007309965730379532
427, epoch_train_loss=0.0007309965730379532
Epoch 428
Epoch 428 :: Batch 0/1
Batch Loss = 0.0007309965710804543
428, epoch_train_loss=0.0007309965710804543
Epoch 429
Epoch 429 :: Batch 0/1
Batch Loss = 0.000730996569122372
429, epoch_train_loss=0.000730996569122372
Epoch 430
Epoch 430 :: Batch 0/1
Batch Loss = 0.0007309965671637132
430, epoch_train_loss=0.0007309965671637132
Epoch 431
Epoch 431 :: Batch 0/1
Batch Loss = 0.0007309965652044845
431, epoch_train_loss=0.0007309965652044845
Epoch 432
Epoch 432 :: Batch 0/1
Batch Loss = 0.000730996563244693
432, epoch_train_loss=0.000730996563244693
Epoch 433
Epoch 433 :: Batch 0/1
Batch Loss = 0.0007309965612843455
433, epoch_train_loss=0.0007309965612843455
Epoch 434
Epoch 434 :: Batch 0/1
Batch Loss = 0.0007309965593234486
434, epoch_train_loss=0.0007309965593234486
Epoch 435
Epoch 435 :: Batch 0/1
Batch Loss = 0.0007309965573620091
435, epoch_train_loss=0.0007309965573620091
Epoch 436
Epoch 436 :: Batch 0/1
Batch Loss = 0.000730996555400034
436, epoch_train_loss=0.000730996555400034
Epoch 437
Epoch 437 :: Batch 0/1
Batch Loss = 0.0007309965534375298
437, epoch_train_loss=0.0007309965534375298
Epoch 438
Epoch 438 :: Batch 0/1
Batch Loss = 0.0007309965514745031
438, epoch_train_loss=0.0007309965514745031
Epoch 439
Epoch 439 :: Batch 0/1
Batch Loss = 0.0007309965495109609
439, epoch_train_loss=0.0007309965495109609
Epoch 440
Epoch 440 :: Batch 0/1
Batch Loss = 0.0007309965475469095
440, epoch_train_loss=0.0007309965475469095
Epoch 441
Epoch 441 :: Batch 0/1
Batch Loss = 0.0007309965455823557
441, epoch_train_loss=0.0007309965455823557
Epoch 442
Epoch 442 :: Batch 0/1
Batch Loss = 0.0007309965436173059
442, epoch_train_loss=0.0007309965436173059
Epoch 443
Epoch 443 :: Batch 0/1
Batch Loss = 0.0007309965416517668
443, epoch_train_loss=0.0007309965416517668
Epoch 444
Epoch 444 :: Batch 0/1
Batch Loss = 0.0007309965396857448
444, epoch_train_loss=0.0007309965396857448
Epoch 445
Epoch 445 :: Batch 0/1
Batch Loss = 0.0007309965377192467
445, epoch_train_loss=0.0007309965377192467
Epoch 446
Epoch 446 :: Batch 0/1
Batch Loss = 0.0007309965357522788
446, epoch_train_loss=0.0007309965357522788
Epoch 447
Epoch 447 :: Batch 0/1
Batch Loss = 0.0007309965337848477
447, epoch_train_loss=0.0007309965337848477
Epoch 448
Epoch 448 :: Batch 0/1
Batch Loss = 0.0007309965318169599
448, epoch_train_loss=0.0007309965318169599
Epoch 449
Epoch 449 :: Batch 0/1
Batch Loss = 0.0007309965298486219
449, epoch_train_loss=0.0007309965298486219
Epoch 450
Epoch 450 :: Batch 0/1
Batch Loss = 0.0007309965278798398
450, epoch_train_loss=0.0007309965278798398
Epoch 451
Epoch 451 :: Batch 0/1
Batch Loss = 0.0007309965259106201
451, epoch_train_loss=0.0007309965259106201
Epoch 452
Epoch 452 :: Batch 0/1
Batch Loss = 0.0007309965239409693
452, epoch_train_loss=0.0007309965239409693
Epoch 453
Epoch 453 :: Batch 0/1
Batch Loss = 0.0007309965219708936
453, epoch_train_loss=0.0007309965219708936
Epoch 454
Epoch 454 :: Batch 0/1
Batch Loss = 0.0007309965200003994
454, epoch_train_loss=0.0007309965200003994
Epoch 455
Epoch 455 :: Batch 0/1
Batch Loss = 0.0007309965180294932
455, epoch_train_loss=0.0007309965180294932
Epoch 456
Epoch 456 :: Batch 0/1
Batch Loss = 0.000730996516058181
456, epoch_train_loss=0.000730996516058181
Epoch 457
Epoch 457 :: Batch 0/1
Batch Loss = 0.0007309965140864691
457, epoch_train_loss=0.0007309965140864691
Epoch 458
Epoch 458 :: Batch 0/1
Batch Loss = 0.0007309965121143638
458, epoch_train_loss=0.0007309965121143638
Epoch 459
Epoch 459 :: Batch 0/1
Batch Loss = 0.0007309965101418714
459, epoch_train_loss=0.0007309965101418714
Epoch 460
Epoch 460 :: Batch 0/1
Batch Loss = 0.0007309965081689979
460, epoch_train_loss=0.0007309965081689979
Epoch 461
Epoch 461 :: Batch 0/1
Batch Loss = 0.0007309965061957494
461, epoch_train_loss=0.0007309965061957494
Epoch 462
Epoch 462 :: Batch 0/1
Batch Loss = 0.0007309965042221324
462, epoch_train_loss=0.0007309965042221324
Epoch 463
Epoch 463 :: Batch 0/1
Batch Loss = 0.0007309965022481527
463, epoch_train_loss=0.0007309965022481527
Epoch 464
Epoch 464 :: Batch 0/1
Batch Loss = 0.0007309965002738169
464, epoch_train_loss=0.0007309965002738169
Epoch 465
Epoch 465 :: Batch 0/1
Batch Loss = 0.0007309964982991306
465, epoch_train_loss=0.0007309964982991306
Epoch 466
Epoch 466 :: Batch 0/1
Batch Loss = 0.0007309964963240997
466, epoch_train_loss=0.0007309964963240997
Epoch 467
Epoch 467 :: Batch 0/1
Batch Loss = 0.0007309964943487307
467, epoch_train_loss=0.0007309964943487307
Epoch 468
Epoch 468 :: Batch 0/1
Batch Loss = 0.0007309964923730296
468, epoch_train_loss=0.0007309964923730296
Epoch 469
Epoch 469 :: Batch 0/1
Batch Loss = 0.0007309964903970021
469, epoch_train_loss=0.0007309964903970021
Epoch 470
Epoch 470 :: Batch 0/1
Batch Loss = 0.0007309964884206543
470, epoch_train_loss=0.0007309964884206543
Epoch 471
Epoch 471 :: Batch 0/1
Batch Loss = 0.0007309964864439924
471, epoch_train_loss=0.0007309964864439924
Epoch 472
Epoch 472 :: Batch 0/1
Batch Loss = 0.000730996484467022
472, epoch_train_loss=0.000730996484467022
Epoch 473
Epoch 473 :: Batch 0/1
Batch Loss = 0.0007309964824897493
473, epoch_train_loss=0.0007309964824897493
Epoch 474
Epoch 474 :: Batch 0/1
Batch Loss = 0.0007309964805121799
474, epoch_train_loss=0.0007309964805121799
Epoch 475
Epoch 475 :: Batch 0/1
Batch Loss = 0.00073099647853432
475, epoch_train_loss=0.00073099647853432
Epoch 476
Epoch 476 :: Batch 0/1
Batch Loss = 0.000730996476556175
476, epoch_train_loss=0.000730996476556175
Epoch 477
Epoch 477 :: Batch 0/1
Batch Loss = 0.000730996474577751
477, epoch_train_loss=0.000730996474577751
Epoch 478
Epoch 478 :: Batch 0/1
Batch Loss = 0.0007309964725990541
478, epoch_train_loss=0.0007309964725990541
Epoch 479
Epoch 479 :: Batch 0/1
Batch Loss = 0.0007309964706200897
479, epoch_train_loss=0.0007309964706200897
Epoch 480
Epoch 480 :: Batch 0/1
Batch Loss = 0.0007309964686408636
480, epoch_train_loss=0.0007309964686408636
Epoch 481
Epoch 481 :: Batch 0/1
Batch Loss = 0.0007309964666613817
481, epoch_train_loss=0.0007309964666613817
Epoch 482
Epoch 482 :: Batch 0/1
Batch Loss = 0.0007309964646816496
482, epoch_train_loss=0.0007309964646816496
Epoch 483
Epoch 483 :: Batch 0/1
Batch Loss = 0.0007309964627016732
483, epoch_train_loss=0.0007309964627016732
Epoch 484
Epoch 484 :: Batch 0/1
Batch Loss = 0.000730996460721458
484, epoch_train_loss=0.000730996460721458
Epoch 485
Epoch 485 :: Batch 0/1
Batch Loss = 0.00073099645874101
485, epoch_train_loss=0.00073099645874101
Epoch 486
Epoch 486 :: Batch 0/1
Batch Loss = 0.0007309964567603342
486, epoch_train_loss=0.0007309964567603342
Epoch 487
Epoch 487 :: Batch 0/1
Batch Loss = 0.0007309964547794368
487, epoch_train_loss=0.0007309964547794368
Epoch 488
Epoch 488 :: Batch 0/1
Batch Loss = 0.0007309964527983232
488, epoch_train_loss=0.0007309964527983232
Epoch 489
Epoch 489 :: Batch 0/1
Batch Loss = 0.0007309964508169991
489, epoch_train_loss=0.0007309964508169991
Epoch 490
Epoch 490 :: Batch 0/1
Batch Loss = 0.0007309964488354698
490, epoch_train_loss=0.0007309964488354698
Epoch 491
Epoch 491 :: Batch 0/1
Batch Loss = 0.0007309964468537411
491, epoch_train_loss=0.0007309964468537411
Epoch 492
Epoch 492 :: Batch 0/1
Batch Loss = 0.0007309964448718187
492, epoch_train_loss=0.0007309964448718187
Epoch 493
Epoch 493 :: Batch 0/1
Batch Loss = 0.0007309964428897077
493, epoch_train_loss=0.0007309964428897077
Epoch 494
Epoch 494 :: Batch 0/1
Batch Loss = 0.0007309964409074138
494, epoch_train_loss=0.0007309964409074138
Epoch 495
Epoch 495 :: Batch 0/1
Batch Loss = 0.0007309964389249424
495, epoch_train_loss=0.0007309964389249424
Epoch 496
Epoch 496 :: Batch 0/1
Batch Loss = 0.0007309964369422992
496, epoch_train_loss=0.0007309964369422992
Epoch 497
Epoch 497 :: Batch 0/1
Batch Loss = 0.0007309964349594891
497, epoch_train_loss=0.0007309964349594891
Epoch 498
Epoch 498 :: Batch 0/1
Batch Loss = 0.0007309964329765182
498, epoch_train_loss=0.0007309964329765182
Epoch 499
Epoch 499 :: Batch 0/1
Batch Loss = 0.0007309964309933914
499, epoch_train_loss=0.0007309964309933914
Epoch 500
Epoch 500 :: Batch 0/1
Batch Loss = 0.000730996429010114
500, epoch_train_loss=0.000730996429010114
Epoch 501
Epoch 501 :: Batch 0/1
Batch Loss = 0.0007309964270266918
501, epoch_train_loss=0.0007309964270266918
Epoch 502
Epoch 502 :: Batch 0/1
Batch Loss = 0.00073099642504313
502, epoch_train_loss=0.00073099642504313
Epoch 503
Epoch 503 :: Batch 0/1
Batch Loss = 0.0007309964230594335
503, epoch_train_loss=0.0007309964230594335
Epoch 504
Epoch 504 :: Batch 0/1
Batch Loss = 0.0007309964210756081
504, epoch_train_loss=0.0007309964210756081
Epoch 505
Epoch 505 :: Batch 0/1
Batch Loss = 0.0007309964190916592
505, epoch_train_loss=0.0007309964190916592
Epoch 506
Epoch 506 :: Batch 0/1
Batch Loss = 0.0007309964171075911
506, epoch_train_loss=0.0007309964171075911
Epoch 507
Epoch 507 :: Batch 0/1
Batch Loss = 0.0007309964151234104
507, epoch_train_loss=0.0007309964151234104
Epoch 508
Epoch 508 :: Batch 0/1
Batch Loss = 0.0007309964131391213
508, epoch_train_loss=0.0007309964131391213
Epoch 509
Epoch 509 :: Batch 0/1
Batch Loss = 0.0007309964111547294
509, epoch_train_loss=0.0007309964111547294
Epoch 510
Epoch 510 :: Batch 0/1
Batch Loss = 0.0007309964091702397
510, epoch_train_loss=0.0007309964091702397
Epoch 511
Epoch 511 :: Batch 0/1
Batch Loss = 0.0007309964071856575
511, epoch_train_loss=0.0007309964071856575
Epoch 512
Epoch 512 :: Batch 0/1
Batch Loss = 0.000730996405200988
512, epoch_train_loss=0.000730996405200988
Epoch 513
Epoch 513 :: Batch 0/1
Batch Loss = 0.0007309964032162364
513, epoch_train_loss=0.0007309964032162364
Epoch 514
Epoch 514 :: Batch 0/1
Batch Loss = 0.0007309964012314076
514, epoch_train_loss=0.0007309964012314076
Epoch 515
Epoch 515 :: Batch 0/1
Batch Loss = 0.0007309963992465067
515, epoch_train_loss=0.0007309963992465067
Epoch 516
Epoch 516 :: Batch 0/1
Batch Loss = 0.000730996397261539
516, epoch_train_loss=0.000730996397261539
Epoch 517
Epoch 517 :: Batch 0/1
Batch Loss = 0.0007309963952765093
517, epoch_train_loss=0.0007309963952765093
Epoch 518
Epoch 518 :: Batch 0/1
Batch Loss = 0.0007309963932914229
518, epoch_train_loss=0.0007309963932914229
Epoch 519
Epoch 519 :: Batch 0/1
Batch Loss = 0.0007309963913062842
519, epoch_train_loss=0.0007309963913062842
Epoch 520
Epoch 520 :: Batch 0/1
Batch Loss = 0.000730996389321099
520, epoch_train_loss=0.000730996389321099
Epoch 521
Epoch 521 :: Batch 0/1
Batch Loss = 0.0007309963873358719
521, epoch_train_loss=0.0007309963873358719
Epoch 522
Epoch 522 :: Batch 0/1
Batch Loss = 0.000730996385350608
522, epoch_train_loss=0.000730996385350608
Epoch 523
Epoch 523 :: Batch 0/1
Batch Loss = 0.0007309963833653122
523, epoch_train_loss=0.0007309963833653122
Epoch 524
Epoch 524 :: Batch 0/1
Batch Loss = 0.0007309963813799893
524, epoch_train_loss=0.0007309963813799893
Epoch 525
Epoch 525 :: Batch 0/1
Batch Loss = 0.0007309963793946441
525, epoch_train_loss=0.0007309963793946441
Epoch 526
Epoch 526 :: Batch 0/1
Batch Loss = 0.0007309963774092819
526, epoch_train_loss=0.0007309963774092819
Epoch 527
Epoch 527 :: Batch 0/1
Batch Loss = 0.0007309963754239073
527, epoch_train_loss=0.0007309963754239073
Epoch 528
Epoch 528 :: Batch 0/1
Batch Loss = 0.0007309963734385253
528, epoch_train_loss=0.0007309963734385253
Epoch 529
Epoch 529 :: Batch 0/1
Batch Loss = 0.0007309963714531405
529, epoch_train_loss=0.0007309963714531405
Epoch 530
Epoch 530 :: Batch 0/1
Batch Loss = 0.0007309963694677581
530, epoch_train_loss=0.0007309963694677581
Epoch 531
Epoch 531 :: Batch 0/1
Batch Loss = 0.0007309963674823829
531, epoch_train_loss=0.0007309963674823829
Epoch 532
Epoch 532 :: Batch 0/1
Batch Loss = 0.000730996365497019
532, epoch_train_loss=0.000730996365497019
Epoch 533
Epoch 533 :: Batch 0/1
Batch Loss = 0.0007309963635116719
533, epoch_train_loss=0.0007309963635116719
Epoch 534
Epoch 534 :: Batch 0/1
Batch Loss = 0.0007309963615263463
534, epoch_train_loss=0.0007309963615263463
Epoch 535
Epoch 535 :: Batch 0/1
Batch Loss = 0.0007309963595410464
535, epoch_train_loss=0.0007309963595410464
Epoch 536
Epoch 536 :: Batch 0/1
Batch Loss = 0.0007309963575557773
536, epoch_train_loss=0.0007309963575557773
Epoch 537
Epoch 537 :: Batch 0/1
Batch Loss = 0.000730996355570544
537, epoch_train_loss=0.000730996355570544
Epoch 538
Epoch 538 :: Batch 0/1
Batch Loss = 0.0007309963535853505
538, epoch_train_loss=0.0007309963535853505
Epoch 539
Epoch 539 :: Batch 0/1
Batch Loss = 0.000730996351600202
539, epoch_train_loss=0.000730996351600202
Epoch 540
Epoch 540 :: Batch 0/1
Batch Loss = 0.000730996349615103
540, epoch_train_loss=0.000730996349615103
Epoch 541
Epoch 541 :: Batch 0/1
Batch Loss = 0.000730996347630058
541, epoch_train_loss=0.000730996347630058
Epoch 542
Epoch 542 :: Batch 0/1
Batch Loss = 0.0007309963456450716
542, epoch_train_loss=0.0007309963456450716
Epoch 543
Epoch 543 :: Batch 0/1
Batch Loss = 0.0007309963436601487
543, epoch_train_loss=0.0007309963436601487
Epoch 544
Epoch 544 :: Batch 0/1
Batch Loss = 0.0007309963416752937
544, epoch_train_loss=0.0007309963416752937
Epoch 545
Epoch 545 :: Batch 0/1
Batch Loss = 0.000730996339690511
545, epoch_train_loss=0.000730996339690511
Epoch 546
Epoch 546 :: Batch 0/1
Batch Loss = 0.0007309963377058054
546, epoch_train_loss=0.0007309963377058054
Epoch 547
Epoch 547 :: Batch 0/1
Batch Loss = 0.0007309963357211815
547, epoch_train_loss=0.0007309963357211815
Epoch 548
Epoch 548 :: Batch 0/1
Batch Loss = 0.0007309963337366435
548, epoch_train_loss=0.0007309963337366435
Epoch 549
Epoch 549 :: Batch 0/1
Batch Loss = 0.000730996331752196
549, epoch_train_loss=0.000730996331752196
Epoch 550
Epoch 550 :: Batch 0/1
Batch Loss = 0.0007309963297678435
550, epoch_train_loss=0.0007309963297678435
Epoch 551
Epoch 551 :: Batch 0/1
Batch Loss = 0.0007309963277835906
551, epoch_train_loss=0.0007309963277835906
Epoch 552
Epoch 552 :: Batch 0/1
Batch Loss = 0.0007309963257994418
552, epoch_train_loss=0.0007309963257994418
Epoch 553
Epoch 553 :: Batch 0/1
Batch Loss = 0.0007309963238154012
553, epoch_train_loss=0.0007309963238154012
Epoch 554
Epoch 554 :: Batch 0/1
Batch Loss = 0.0007309963218314734
554, epoch_train_loss=0.0007309963218314734
Epoch 555
Epoch 555 :: Batch 0/1
Batch Loss = 0.0007309963198476627
555, epoch_train_loss=0.0007309963198476627
Epoch 556
Epoch 556 :: Batch 0/1
Batch Loss = 0.0007309963178639738
556, epoch_train_loss=0.0007309963178639738
Epoch 557
Epoch 557 :: Batch 0/1
Batch Loss = 0.0007309963158804108
557, epoch_train_loss=0.0007309963158804108
Epoch 558
Epoch 558 :: Batch 0/1
Batch Loss = 0.000730996313896978
558, epoch_train_loss=0.000730996313896978
Epoch 559
Epoch 559 :: Batch 0/1
Batch Loss = 0.0007309963119136798
559, epoch_train_loss=0.0007309963119136798
Epoch 560
Epoch 560 :: Batch 0/1
Batch Loss = 0.0007309963099305206
560, epoch_train_loss=0.0007309963099305206
Epoch 561
Epoch 561 :: Batch 0/1
Batch Loss = 0.0007309963079475047
561, epoch_train_loss=0.0007309963079475047
Epoch 562
Epoch 562 :: Batch 0/1
Batch Loss = 0.0007309963059646363
562, epoch_train_loss=0.0007309963059646363
Epoch 563
Epoch 563 :: Batch 0/1
Batch Loss = 0.0007309963039819195
563, epoch_train_loss=0.0007309963039819195
Epoch 564
Epoch 564 :: Batch 0/1
Batch Loss = 0.0007309963019993589
564, epoch_train_loss=0.0007309963019993589
Epoch 565
Epoch 565 :: Batch 0/1
Batch Loss = 0.0007309963000169586
565, epoch_train_loss=0.0007309963000169586
Epoch 566
Epoch 566 :: Batch 0/1
Batch Loss = 0.0007309962980347228
566, epoch_train_loss=0.0007309962980347228
Epoch 567
Epoch 567 :: Batch 0/1
Batch Loss = 0.0007309962960526556
567, epoch_train_loss=0.0007309962960526556
Epoch 568
Epoch 568 :: Batch 0/1
Batch Loss = 0.0007309962940707616
568, epoch_train_loss=0.0007309962940707616
Epoch 569
Epoch 569 :: Batch 0/1
Batch Loss = 0.0007309962920890445
569, epoch_train_loss=0.0007309962920890445
Epoch 570
Epoch 570 :: Batch 0/1
Batch Loss = 0.0007309962901075087
570, epoch_train_loss=0.0007309962901075087
Epoch 571
Epoch 571 :: Batch 0/1
Batch Loss = 0.0007309962881261582
571, epoch_train_loss=0.0007309962881261582
Epoch 572
Epoch 572 :: Batch 0/1
Batch Loss = 0.0007309962861449972
572, epoch_train_loss=0.0007309962861449972
Epoch 573
Epoch 573 :: Batch 0/1
Batch Loss = 0.0007309962841640297
573, epoch_train_loss=0.0007309962841640297
Epoch 574
Epoch 574 :: Batch 0/1
Batch Loss = 0.0007309962821832601
574, epoch_train_loss=0.0007309962821832601
Epoch 575
Epoch 575 :: Batch 0/1
Batch Loss = 0.0007309962802026922
575, epoch_train_loss=0.0007309962802026922
Epoch 576
Epoch 576 :: Batch 0/1
Batch Loss = 0.0007309962782223301
576, epoch_train_loss=0.0007309962782223301
Epoch 577
Epoch 577 :: Batch 0/1
Batch Loss = 0.0007309962762421781
577, epoch_train_loss=0.0007309962762421781
Epoch 578
Epoch 578 :: Batch 0/1
Batch Loss = 0.0007309962742622399
578, epoch_train_loss=0.0007309962742622399
Epoch 579
Epoch 579 :: Batch 0/1
Batch Loss = 0.0007309962722825197
579, epoch_train_loss=0.0007309962722825197
Epoch 580
Epoch 580 :: Batch 0/1
Batch Loss = 0.0007309962703030216
580, epoch_train_loss=0.0007309962703030216
Epoch 581
Epoch 581 :: Batch 0/1
Batch Loss = 0.0007309962683237494
581, epoch_train_loss=0.0007309962683237494
Epoch 582
Epoch 582 :: Batch 0/1
Batch Loss = 0.000730996266344707
582, epoch_train_loss=0.000730996266344707
Epoch 583
Epoch 583 :: Batch 0/1
Batch Loss = 0.0007309962643658987
583, epoch_train_loss=0.0007309962643658987
Epoch 584
Epoch 584 :: Batch 0/1
Batch Loss = 0.000730996262387328
584, epoch_train_loss=0.000730996262387328
Epoch 585
Epoch 585 :: Batch 0/1
Batch Loss = 0.0007309962604089993
585, epoch_train_loss=0.0007309962604089993
Epoch 586
Epoch 586 :: Batch 0/1
Batch Loss = 0.0007309962584309163
586, epoch_train_loss=0.0007309962584309163
Epoch 587
Epoch 587 :: Batch 0/1
Batch Loss = 0.0007309962564530827
587, epoch_train_loss=0.0007309962564530827
Epoch 588
Epoch 588 :: Batch 0/1
Batch Loss = 0.0007309962544755026
588, epoch_train_loss=0.0007309962544755026
Epoch 589
Epoch 589 :: Batch 0/1
Batch Loss = 0.0007309962524981798
589, epoch_train_loss=0.0007309962524981798
Epoch 590
Epoch 590 :: Batch 0/1
Batch Loss = 0.0007309962505211183
590, epoch_train_loss=0.0007309962505211183
Epoch 591
Epoch 591 :: Batch 0/1
Batch Loss = 0.0007309962485443217
591, epoch_train_loss=0.0007309962485443217
Epoch 592
Epoch 592 :: Batch 0/1
Batch Loss = 0.0007309962465677939
592, epoch_train_loss=0.0007309962465677939
Epoch 593
Epoch 593 :: Batch 0/1
Batch Loss = 0.0007309962445915388
593, epoch_train_loss=0.0007309962445915388
Epoch 594
Epoch 594 :: Batch 0/1
Batch Loss = 0.0007309962426155603
594, epoch_train_loss=0.0007309962426155603
Epoch 595
Epoch 595 :: Batch 0/1
Batch Loss = 0.0007309962406398617
595, epoch_train_loss=0.0007309962406398617
Epoch 596
Epoch 596 :: Batch 0/1
Batch Loss = 0.0007309962386644472
596, epoch_train_loss=0.0007309962386644472
Epoch 597
Epoch 597 :: Batch 0/1
Batch Loss = 0.0007309962366893204
597, epoch_train_loss=0.0007309962366893204
Epoch 598
Epoch 598 :: Batch 0/1
Batch Loss = 0.0007309962347144852
598, epoch_train_loss=0.0007309962347144852
Epoch 599
Epoch 599 :: Batch 0/1
Batch Loss = 0.000730996232739945
599, epoch_train_loss=0.000730996232739945
Epoch 600
Epoch 600 :: Batch 0/1
Batch Loss = 0.0007309962307657039
600, epoch_train_loss=0.0007309962307657039
Epoch 601
Epoch 601 :: Batch 0/1
Batch Loss = 0.0007309962287917653
601, epoch_train_loss=0.0007309962287917653
Epoch 602
Epoch 602 :: Batch 0/1
Batch Loss = 0.0007309962268181329
602, epoch_train_loss=0.0007309962268181329
Epoch 603
Epoch 603 :: Batch 0/1
Batch Loss = 0.0007309962248448107
603, epoch_train_loss=0.0007309962248448107
Epoch 604
Epoch 604 :: Batch 0/1
Batch Loss = 0.000730996222871802
604, epoch_train_loss=0.000730996222871802
Epoch 605
Epoch 605 :: Batch 0/1
Batch Loss = 0.0007309962208991102
605, epoch_train_loss=0.0007309962208991102
Epoch 606
Epoch 606 :: Batch 0/1
Batch Loss = 0.0007309962189267397
606, epoch_train_loss=0.0007309962189267397
Epoch 607
Epoch 607 :: Batch 0/1
Batch Loss = 0.0007309962169546936
607, epoch_train_loss=0.0007309962169546936
Epoch 608
Epoch 608 :: Batch 0/1
Batch Loss = 0.0007309962149829755
608, epoch_train_loss=0.0007309962149829755
Epoch 609
Epoch 609 :: Batch 0/1
Batch Loss = 0.000730996213011589
609, epoch_train_loss=0.000730996213011589
Epoch 610
Epoch 610 :: Batch 0/1
Batch Loss = 0.0007309962110405375
610, epoch_train_loss=0.0007309962110405375
Epoch 611
Epoch 611 :: Batch 0/1
Batch Loss = 0.0007309962090698251
611, epoch_train_loss=0.0007309962090698251
Epoch 612
Epoch 612 :: Batch 0/1
Batch Loss = 0.0007309962070994549
612, epoch_train_loss=0.0007309962070994549
Epoch 613
Epoch 613 :: Batch 0/1
Batch Loss = 0.0007309962051294303
613, epoch_train_loss=0.0007309962051294303
Epoch 614
Epoch 614 :: Batch 0/1
Batch Loss = 0.0007309962031597553
614, epoch_train_loss=0.0007309962031597553
Epoch 615
Epoch 615 :: Batch 0/1
Batch Loss = 0.000730996201190433
615, epoch_train_loss=0.000730996201190433
Epoch 616
Epoch 616 :: Batch 0/1
Batch Loss = 0.0007309961992214672
616, epoch_train_loss=0.0007309961992214672
Epoch 617
Epoch 617 :: Batch 0/1
Batch Loss = 0.0007309961972528609
617, epoch_train_loss=0.0007309961972528609
Epoch 618
Epoch 618 :: Batch 0/1
Batch Loss = 0.0007309961952846178
618, epoch_train_loss=0.0007309961952846178
Epoch 619
Epoch 619 :: Batch 0/1
Batch Loss = 0.0007309961933167417
619, epoch_train_loss=0.0007309961933167417
Epoch 620
Epoch 620 :: Batch 0/1
Batch Loss = 0.0007309961913492354
620, epoch_train_loss=0.0007309961913492354
Epoch 621
Epoch 621 :: Batch 0/1
Batch Loss = 0.0007309961893821027
621, epoch_train_loss=0.0007309961893821027
Epoch 622
Epoch 622 :: Batch 0/1
Batch Loss = 0.000730996187415347
622, epoch_train_loss=0.000730996187415347
Epoch 623
Epoch 623 :: Batch 0/1
Batch Loss = 0.0007309961854489715
623, epoch_train_loss=0.0007309961854489715
Epoch 624
Epoch 624 :: Batch 0/1
Batch Loss = 0.0007309961834829796
624, epoch_train_loss=0.0007309961834829796
Epoch 625
Epoch 625 :: Batch 0/1
Batch Loss = 0.000730996181517375
625, epoch_train_loss=0.000730996181517375
Epoch 626
Epoch 626 :: Batch 0/1
Batch Loss = 0.0007309961795521606
626, epoch_train_loss=0.0007309961795521606
Epoch 627
Epoch 627 :: Batch 0/1
Batch Loss = 0.0007309961775873399
627, epoch_train_loss=0.0007309961775873399
Epoch 628
Epoch 628 :: Batch 0/1
Batch Loss = 0.0007309961756229164
628, epoch_train_loss=0.0007309961756229164
Epoch 629
Epoch 629 :: Batch 0/1
Batch Loss = 0.0007309961736588933
629, epoch_train_loss=0.0007309961736588933
Epoch 630
Epoch 630 :: Batch 0/1
Batch Loss = 0.0007309961716952737
630, epoch_train_loss=0.0007309961716952737
Epoch 631
Epoch 631 :: Batch 0/1
Batch Loss = 0.000730996169732061
631, epoch_train_loss=0.000730996169732061
Epoch 632
Epoch 632 :: Batch 0/1
Batch Loss = 0.0007309961677692586
632, epoch_train_loss=0.0007309961677692586
Epoch 633
Epoch 633 :: Batch 0/1
Batch Loss = 0.0007309961658068696
633, epoch_train_loss=0.0007309961658068696
Epoch 634
Epoch 634 :: Batch 0/1
Batch Loss = 0.0007309961638448973
634, epoch_train_loss=0.0007309961638448973
Epoch 635
Epoch 635 :: Batch 0/1
Batch Loss = 0.0007309961618833449
635, epoch_train_loss=0.0007309961618833449
Epoch 636
Epoch 636 :: Batch 0/1
Batch Loss = 0.0007309961599222156
636, epoch_train_loss=0.0007309961599222156
Epoch 637
Epoch 637 :: Batch 0/1
Batch Loss = 0.0007309961579615126
637, epoch_train_loss=0.0007309961579615126
Epoch 638
Epoch 638 :: Batch 0/1
Batch Loss = 0.0007309961560012393
638, epoch_train_loss=0.0007309961560012393
Epoch 639
Epoch 639 :: Batch 0/1
Batch Loss = 0.0007309961540413987
639, epoch_train_loss=0.0007309961540413987
Epoch 640
Epoch 640 :: Batch 0/1
Batch Loss = 0.0007309961520819938
640, epoch_train_loss=0.0007309961520819938
Epoch 641
Epoch 641 :: Batch 0/1
Batch Loss = 0.0007309961501230282
641, epoch_train_loss=0.0007309961501230282
Epoch 642
Epoch 642 :: Batch 0/1
Batch Loss = 0.0007309961481645046
642, epoch_train_loss=0.0007309961481645046
Epoch 643
Epoch 643 :: Batch 0/1
Batch Loss = 0.0007309961462064262
643, epoch_train_loss=0.0007309961462064262
Epoch 644
Epoch 644 :: Batch 0/1
Batch Loss = 0.0007309961442487962
644, epoch_train_loss=0.0007309961442487962
Epoch 645
Epoch 645 :: Batch 0/1
Batch Loss = 0.0007309961422916179
645, epoch_train_loss=0.0007309961422916179
Epoch 646
Epoch 646 :: Batch 0/1
Batch Loss = 0.0007309961403348939
646, epoch_train_loss=0.0007309961403348939
Epoch 647
Epoch 647 :: Batch 0/1
Batch Loss = 0.0007309961383786278
647, epoch_train_loss=0.0007309961383786278
Epoch 648
Epoch 648 :: Batch 0/1
Batch Loss = 0.0007309961364228225
648, epoch_train_loss=0.0007309961364228225
Epoch 649
Epoch 649 :: Batch 0/1
Batch Loss = 0.0007309961344674808
649, epoch_train_loss=0.0007309961344674808
Epoch 650
Epoch 650 :: Batch 0/1
Batch Loss = 0.0007309961325126061
650, epoch_train_loss=0.0007309961325126061
Epoch 651
Epoch 651 :: Batch 0/1
Batch Loss = 0.0007309961305582009
651, epoch_train_loss=0.0007309961305582009
Epoch 652
Epoch 652 :: Batch 0/1
Batch Loss = 0.0007309961286042687
652, epoch_train_loss=0.0007309961286042687
Epoch 653
Epoch 653 :: Batch 0/1
Batch Loss = 0.0007309961266508124
653, epoch_train_loss=0.0007309961266508124
Epoch 654
Epoch 654 :: Batch 0/1
Batch Loss = 0.0007309961246978349
654, epoch_train_loss=0.0007309961246978349
Epoch 655
Epoch 655 :: Batch 0/1
Batch Loss = 0.0007309961227453394
655, epoch_train_loss=0.0007309961227453394
Epoch 656
Epoch 656 :: Batch 0/1
Batch Loss = 0.0007309961207933285
656, epoch_train_loss=0.0007309961207933285
Epoch 657
Epoch 657 :: Batch 0/1
Batch Loss = 0.0007309961188418055
657, epoch_train_loss=0.0007309961188418055
Epoch 658
Epoch 658 :: Batch 0/1
Batch Loss = 0.0007309961168907729
658, epoch_train_loss=0.0007309961168907729
Epoch 659
Epoch 659 :: Batch 0/1
Batch Loss = 0.000730996114940234
659, epoch_train_loss=0.000730996114940234
Epoch 660
Epoch 660 :: Batch 0/1
Batch Loss = 0.0007309961129901918
660, epoch_train_loss=0.0007309961129901918
Epoch 661
Epoch 661 :: Batch 0/1
Batch Loss = 0.0007309961110406489
661, epoch_train_loss=0.0007309961110406489
Epoch 662
Epoch 662 :: Batch 0/1
Batch Loss = 0.0007309961090916083
662, epoch_train_loss=0.0007309961090916083
Epoch 663
Epoch 663 :: Batch 0/1
Batch Loss = 0.0007309961071430728
663, epoch_train_loss=0.0007309961071430728
Epoch 664
Epoch 664 :: Batch 0/1
Batch Loss = 0.0007309961051950455
664, epoch_train_loss=0.0007309961051950455
Epoch 665
Epoch 665 :: Batch 0/1
Batch Loss = 0.000730996103247529
665, epoch_train_loss=0.000730996103247529
Epoch 666
Epoch 666 :: Batch 0/1
Batch Loss = 0.0007309961013005266
666, epoch_train_loss=0.0007309961013005266
Epoch 667
Epoch 667 :: Batch 0/1
Batch Loss = 0.0007309960993540405
667, epoch_train_loss=0.0007309960993540405
Epoch 668
Epoch 668 :: Batch 0/1
Batch Loss = 0.0007309960974080739
668, epoch_train_loss=0.0007309960974080739
Epoch 669
Epoch 669 :: Batch 0/1
Batch Loss = 0.0007309960954626294
669, epoch_train_loss=0.0007309960954626294
Epoch 670
Epoch 670 :: Batch 0/1
Batch Loss = 0.00073099609351771
670, epoch_train_loss=0.00073099609351771
Epoch 671
Epoch 671 :: Batch 0/1
Batch Loss = 0.0007309960915733185
671, epoch_train_loss=0.0007309960915733185
Epoch 672
Epoch 672 :: Batch 0/1
Batch Loss = 0.0007309960896294575
672, epoch_train_loss=0.0007309960896294575
Epoch 673
Epoch 673 :: Batch 0/1
Batch Loss = 0.0007309960876861298
673, epoch_train_loss=0.0007309960876861298
Epoch 674
Epoch 674 :: Batch 0/1
Batch Loss = 0.0007309960857433381
674, epoch_train_loss=0.0007309960857433381
Epoch 675
Epoch 675 :: Batch 0/1
Batch Loss = 0.0007309960838010852
675, epoch_train_loss=0.0007309960838010852
Epoch 676
Epoch 676 :: Batch 0/1
Batch Loss = 0.0007309960818593741
676, epoch_train_loss=0.0007309960818593741
Epoch 677
Epoch 677 :: Batch 0/1
Batch Loss = 0.0007309960799182069
677, epoch_train_loss=0.0007309960799182069
Epoch 678
Epoch 678 :: Batch 0/1
Batch Loss = 0.000730996077977587
678, epoch_train_loss=0.000730996077977587
Epoch 679
Epoch 679 :: Batch 0/1
Batch Loss = 0.0007309960760375165
679, epoch_train_loss=0.0007309960760375165
Epoch 680
Epoch 680 :: Batch 0/1
Batch Loss = 0.0007309960740979986
680, epoch_train_loss=0.0007309960740979986
Epoch 681
Epoch 681 :: Batch 0/1
Batch Loss = 0.0007309960721590354
681, epoch_train_loss=0.0007309960721590354
Epoch 682
Epoch 682 :: Batch 0/1
Batch Loss = 0.0007309960702206301
682, epoch_train_loss=0.0007309960702206301
Epoch 683
Epoch 683 :: Batch 0/1
Batch Loss = 0.000730996068282785
683, epoch_train_loss=0.000730996068282785
Epoch 684
Epoch 684 :: Batch 0/1
Batch Loss = 0.0007309960663455028
684, epoch_train_loss=0.0007309960663455028
Epoch 685
Epoch 685 :: Batch 0/1
Batch Loss = 0.0007309960644087863
685, epoch_train_loss=0.0007309960644087863
Epoch 686
Epoch 686 :: Batch 0/1
Batch Loss = 0.0007309960624726379
686, epoch_train_loss=0.0007309960624726379
Epoch 687
Epoch 687 :: Batch 0/1
Batch Loss = 0.0007309960605370602
687, epoch_train_loss=0.0007309960605370602
Epoch 688
Epoch 688 :: Batch 0/1
Batch Loss = 0.000730996058602056
688, epoch_train_loss=0.000730996058602056
Epoch 689
Epoch 689 :: Batch 0/1
Batch Loss = 0.0007309960566676278
689, epoch_train_loss=0.0007309960566676278
Epoch 690
Epoch 690 :: Batch 0/1
Batch Loss = 0.000730996054733778
690, epoch_train_loss=0.000730996054733778
Epoch 691
Epoch 691 :: Batch 0/1
Batch Loss = 0.0007309960528005093
691, epoch_train_loss=0.0007309960528005093
Epoch 692
Epoch 692 :: Batch 0/1
Batch Loss = 0.0007309960508678243
692, epoch_train_loss=0.0007309960508678243
Epoch 693
Epoch 693 :: Batch 0/1
Batch Loss = 0.0007309960489357253
693, epoch_train_loss=0.0007309960489357253
Epoch 694
Epoch 694 :: Batch 0/1
Batch Loss = 0.000730996047004215
694, epoch_train_loss=0.000730996047004215
Epoch 695
Epoch 695 :: Batch 0/1
Batch Loss = 0.0007309960450732959
695, epoch_train_loss=0.0007309960450732959
Epoch 696
Epoch 696 :: Batch 0/1
Batch Loss = 0.0007309960431429705
696, epoch_train_loss=0.0007309960431429705
Epoch 697
Epoch 697 :: Batch 0/1
Batch Loss = 0.0007309960412132414
697, epoch_train_loss=0.0007309960412132414
Epoch 698
Epoch 698 :: Batch 0/1
Batch Loss = 0.0007309960392841107
698, epoch_train_loss=0.0007309960392841107
Epoch 699
Epoch 699 :: Batch 0/1
Batch Loss = 0.0007309960373555813
699, epoch_train_loss=0.0007309960373555813
Epoch 700
Epoch 700 :: Batch 0/1
Batch Loss = 0.0007309960354276555
700, epoch_train_loss=0.0007309960354276555
Epoch 701
Epoch 701 :: Batch 0/1
Batch Loss = 0.0007309960335003357
701, epoch_train_loss=0.0007309960335003357
Epoch 702
Epoch 702 :: Batch 0/1
Batch Loss = 0.0007309960315736244
702, epoch_train_loss=0.0007309960315736244
Epoch 703
Epoch 703 :: Batch 0/1
Batch Loss = 0.0007309960296475239
703, epoch_train_loss=0.0007309960296475239
Epoch 704
Epoch 704 :: Batch 0/1
Batch Loss = 0.0007309960277220369
704, epoch_train_loss=0.0007309960277220369
Epoch 705
Epoch 705 :: Batch 0/1
Batch Loss = 0.0007309960257971653
705, epoch_train_loss=0.0007309960257971653
Epoch 706
Epoch 706 :: Batch 0/1
Batch Loss = 0.0007309960238729121
706, epoch_train_loss=0.0007309960238729121
Epoch 707
Epoch 707 :: Batch 0/1
Batch Loss = 0.0007309960219492793
707, epoch_train_loss=0.0007309960219492793
Epoch 708
Epoch 708 :: Batch 0/1
Batch Loss = 0.0007309960200262695
708, epoch_train_loss=0.0007309960200262695
Epoch 709
Epoch 709 :: Batch 0/1
Batch Loss = 0.0007309960181038849
709, epoch_train_loss=0.0007309960181038849
Epoch 710
Epoch 710 :: Batch 0/1
Batch Loss = 0.0007309960161821276
710, epoch_train_loss=0.0007309960161821276
Epoch 711
Epoch 711 :: Batch 0/1
Batch Loss = 0.0007309960142610006
711, epoch_train_loss=0.0007309960142610006
Epoch 712
Epoch 712 :: Batch 0/1
Batch Loss = 0.0007309960123405057
712, epoch_train_loss=0.0007309960123405057
Epoch 713
Epoch 713 :: Batch 0/1
Batch Loss = 0.0007309960104206453
713, epoch_train_loss=0.0007309960104206453
Epoch 714
Epoch 714 :: Batch 0/1
Batch Loss = 0.0007309960085014219
714, epoch_train_loss=0.0007309960085014219
Epoch 715
Epoch 715 :: Batch 0/1
Batch Loss = 0.0007309960065828378
715, epoch_train_loss=0.0007309960065828378
Epoch 716
Epoch 716 :: Batch 0/1
Batch Loss = 0.0007309960046648952
716, epoch_train_loss=0.0007309960046648952
Epoch 717
Epoch 717 :: Batch 0/1
Batch Loss = 0.0007309960027475962
717, epoch_train_loss=0.0007309960027475962
Epoch 718
Epoch 718 :: Batch 0/1
Batch Loss = 0.0007309960008309432
718, epoch_train_loss=0.0007309960008309432
Epoch 719
Epoch 719 :: Batch 0/1
Batch Loss = 0.0007309959989149387
719, epoch_train_loss=0.0007309959989149387
Epoch 720
Epoch 720 :: Batch 0/1
Batch Loss = 0.0007309959969995846
720, epoch_train_loss=0.0007309959969995846
Epoch 721
Epoch 721 :: Batch 0/1
Batch Loss = 0.0007309959950848834
721, epoch_train_loss=0.0007309959950848834
Epoch 722
Epoch 722 :: Batch 0/1
Batch Loss = 0.0007309959931708369
722, epoch_train_loss=0.0007309959931708369
Epoch 723
Epoch 723 :: Batch 0/1
Batch Loss = 0.0007309959912574479
723, epoch_train_loss=0.0007309959912574479
Epoch 724
Epoch 724 :: Batch 0/1
Batch Loss = 0.0007309959893447183
724, epoch_train_loss=0.0007309959893447183
Epoch 725
Epoch 725 :: Batch 0/1
Batch Loss = 0.0007309959874326504
725, epoch_train_loss=0.0007309959874326504
Epoch 726
Epoch 726 :: Batch 0/1
Batch Loss = 0.0007309959855212463
726, epoch_train_loss=0.0007309959855212463
Epoch 727
Epoch 727 :: Batch 0/1
Batch Loss = 0.0007309959836105082
727, epoch_train_loss=0.0007309959836105082
Epoch 728
Epoch 728 :: Batch 0/1
Batch Loss = 0.0007309959817004384
728, epoch_train_loss=0.0007309959817004384
Epoch 729
Epoch 729 :: Batch 0/1
Batch Loss = 0.0007309959797910386
729, epoch_train_loss=0.0007309959797910386
Epoch 730
Epoch 730 :: Batch 0/1
Batch Loss = 0.0007309959778823116
730, epoch_train_loss=0.0007309959778823116
Epoch 731
Epoch 731 :: Batch 0/1
Batch Loss = 0.000730995975974259
731, epoch_train_loss=0.000730995975974259
Epoch 732
Epoch 732 :: Batch 0/1
Batch Loss = 0.0007309959740668834
732, epoch_train_loss=0.0007309959740668834
Epoch 733
Epoch 733 :: Batch 0/1
Batch Loss = 0.0007309959721601868
733, epoch_train_loss=0.0007309959721601868
Epoch 734
Epoch 734 :: Batch 0/1
Batch Loss = 0.0007309959702541709
734, epoch_train_loss=0.0007309959702541709
Epoch 735
Epoch 735 :: Batch 0/1
Batch Loss = 0.000730995968348838
735, epoch_train_loss=0.000730995968348838
Epoch 736
Epoch 736 :: Batch 0/1
Batch Loss = 0.0007309959664441903
736, epoch_train_loss=0.0007309959664441903
Epoch 737
Epoch 737 :: Batch 0/1
Batch Loss = 0.0007309959645402299
737, epoch_train_loss=0.0007309959645402299
Epoch 738
Epoch 738 :: Batch 0/1
Batch Loss = 0.000730995962636959
738, epoch_train_loss=0.000730995962636959
Epoch 739
Epoch 739 :: Batch 0/1
Batch Loss = 0.0007309959607343794
739, epoch_train_loss=0.0007309959607343794
Epoch 740
Epoch 740 :: Batch 0/1
Batch Loss = 0.0007309959588324931
740, epoch_train_loss=0.0007309959588324931
Epoch 741
Epoch 741 :: Batch 0/1
Batch Loss = 0.0007309959569313027
741, epoch_train_loss=0.0007309959569313027
Epoch 742
Epoch 742 :: Batch 0/1
Batch Loss = 0.0007309959550308098
742, epoch_train_loss=0.0007309959550308098
Epoch 743
Epoch 743 :: Batch 0/1
Batch Loss = 0.0007309959531310161
743, epoch_train_loss=0.0007309959531310161
Epoch 744
Epoch 744 :: Batch 0/1
Batch Loss = 0.0007309959512319242
744, epoch_train_loss=0.0007309959512319242
Epoch 745
Epoch 745 :: Batch 0/1
Batch Loss = 0.0007309959493335358
745, epoch_train_loss=0.0007309959493335358
Epoch 746
Epoch 746 :: Batch 0/1
Batch Loss = 0.0007309959474358529
746, epoch_train_loss=0.0007309959474358529
Epoch 747
Epoch 747 :: Batch 0/1
Batch Loss = 0.0007309959455388776
747, epoch_train_loss=0.0007309959455388776
Epoch 748
Epoch 748 :: Batch 0/1
Batch Loss = 0.0007309959436426117
748, epoch_train_loss=0.0007309959436426117
Epoch 749
Epoch 749 :: Batch 0/1
Batch Loss = 0.0007309959417470573
749, epoch_train_loss=0.0007309959417470573
Epoch 750
Epoch 750 :: Batch 0/1
Batch Loss = 0.0007309959398522165
750, epoch_train_loss=0.0007309959398522165
Epoch 751
Epoch 751 :: Batch 0/1
Batch Loss = 0.0007309959379580909
751, epoch_train_loss=0.0007309959379580909
Epoch 752
Epoch 752 :: Batch 0/1
Batch Loss = 0.0007309959360646829
752, epoch_train_loss=0.0007309959360646829
Epoch 753
Epoch 753 :: Batch 0/1
Batch Loss = 0.0007309959341719941
753, epoch_train_loss=0.0007309959341719941
Epoch 754
Epoch 754 :: Batch 0/1
Batch Loss = 0.0007309959322800265
754, epoch_train_loss=0.0007309959322800265
Epoch 755
Epoch 755 :: Batch 0/1
Batch Loss = 0.000730995930388782
755, epoch_train_loss=0.000730995930388782
Epoch 756
Epoch 756 :: Batch 0/1
Batch Loss = 0.0007309959284982625
756, epoch_train_loss=0.0007309959284982625
Epoch 757
Epoch 757 :: Batch 0/1
Batch Loss = 0.0007309959266084698
757, epoch_train_loss=0.0007309959266084698
Epoch 758
Epoch 758 :: Batch 0/1
Batch Loss = 0.0007309959247194061
758, epoch_train_loss=0.0007309959247194061
Epoch 759
Epoch 759 :: Batch 0/1
Batch Loss = 0.0007309959228310731
759, epoch_train_loss=0.0007309959228310731
Epoch 760
Epoch 760 :: Batch 0/1
Batch Loss = 0.0007309959209434724
760, epoch_train_loss=0.0007309959209434724
Epoch 761
Epoch 761 :: Batch 0/1
Batch Loss = 0.0007309959190566061
761, epoch_train_loss=0.0007309959190566061
Epoch 762
Epoch 762 :: Batch 0/1
Batch Loss = 0.0007309959171704761
762, epoch_train_loss=0.0007309959171704761
Epoch 763
Epoch 763 :: Batch 0/1
Batch Loss = 0.0007309959152850843
763, epoch_train_loss=0.0007309959152850843
Epoch 764
Epoch 764 :: Batch 0/1
Batch Loss = 0.0007309959134004323
764, epoch_train_loss=0.0007309959134004323
Epoch 765
Epoch 765 :: Batch 0/1
Batch Loss = 0.0007309959115165221
765, epoch_train_loss=0.0007309959115165221
Epoch 766
Epoch 766 :: Batch 0/1
Batch Loss = 0.0007309959096333556
766, epoch_train_loss=0.0007309959096333556
Epoch 767
Epoch 767 :: Batch 0/1
Batch Loss = 0.0007309959077509344
767, epoch_train_loss=0.0007309959077509344
Epoch 768
Epoch 768 :: Batch 0/1
Batch Loss = 0.0007309959058692602
768, epoch_train_loss=0.0007309959058692602
Epoch 769
Epoch 769 :: Batch 0/1
Batch Loss = 0.000730995903988335
769, epoch_train_loss=0.000730995903988335
Epoch 770
Epoch 770 :: Batch 0/1
Batch Loss = 0.0007309959021081605
770, epoch_train_loss=0.0007309959021081605
Epoch 771
Epoch 771 :: Batch 0/1
Batch Loss = 0.0007309959002287388
771, epoch_train_loss=0.0007309959002287388
Epoch 772
Epoch 772 :: Batch 0/1
Batch Loss = 0.0007309958983500709
772, epoch_train_loss=0.0007309958983500709
Epoch 773
Epoch 773 :: Batch 0/1
Batch Loss = 0.0007309958964721591
773, epoch_train_loss=0.0007309958964721591
Epoch 774
Epoch 774 :: Batch 0/1
Batch Loss = 0.0007309958945950051
774, epoch_train_loss=0.0007309958945950051
Epoch 775
Epoch 775 :: Batch 0/1
Batch Loss = 0.0007309958927186106
775, epoch_train_loss=0.0007309958927186106
Epoch 776
Epoch 776 :: Batch 0/1
Batch Loss = 0.0007309958908429775
776, epoch_train_loss=0.0007309958908429775
Epoch 777
Epoch 777 :: Batch 0/1
Batch Loss = 0.0007309958889681072
777, epoch_train_loss=0.0007309958889681072
Epoch 778
Epoch 778 :: Batch 0/1
Batch Loss = 0.0007309958870940014
778, epoch_train_loss=0.0007309958870940014
Epoch 779
Epoch 779 :: Batch 0/1
Batch Loss = 0.000730995885220662
779, epoch_train_loss=0.000730995885220662
Epoch 780
Epoch 780 :: Batch 0/1
Batch Loss = 0.0007309958833480907
780, epoch_train_loss=0.0007309958833480907
Epoch 781
Epoch 781 :: Batch 0/1
Batch Loss = 0.0007309958814762889
781, epoch_train_loss=0.0007309958814762889
Epoch 782
Epoch 782 :: Batch 0/1
Batch Loss = 0.0007309958796052587
782, epoch_train_loss=0.0007309958796052587
Epoch 783
Epoch 783 :: Batch 0/1
Batch Loss = 0.0007309958777350013
783, epoch_train_loss=0.0007309958777350013
Epoch 784
Epoch 784 :: Batch 0/1
Batch Loss = 0.0007309958758655191
784, epoch_train_loss=0.0007309958758655191
Epoch 785
Epoch 785 :: Batch 0/1
Batch Loss = 0.0007309958739968129
785, epoch_train_loss=0.0007309958739968129
Epoch 786
Epoch 786 :: Batch 0/1
Batch Loss = 0.000730995872128885
786, epoch_train_loss=0.000730995872128885
Epoch 787
Epoch 787 :: Batch 0/1
Batch Loss = 0.0007309958702617364
787, epoch_train_loss=0.0007309958702617364
Epoch 788
Epoch 788 :: Batch 0/1
Batch Loss = 0.0007309958683953693
788, epoch_train_loss=0.0007309958683953693
Epoch 789
Epoch 789 :: Batch 0/1
Batch Loss = 0.000730995866529785
789, epoch_train_loss=0.000730995866529785
Epoch 790
Epoch 790 :: Batch 0/1
Batch Loss = 0.0007309958646649854
790, epoch_train_loss=0.0007309958646649854
Epoch 791
Epoch 791 :: Batch 0/1
Batch Loss = 0.0007309958628009715
791, epoch_train_loss=0.0007309958628009715
Epoch 792
Epoch 792 :: Batch 0/1
Batch Loss = 0.0007309958609377456
792, epoch_train_loss=0.0007309958609377456
Epoch 793
Epoch 793 :: Batch 0/1
Batch Loss = 0.0007309958590753088
793, epoch_train_loss=0.0007309958590753088
Epoch 794
Epoch 794 :: Batch 0/1
Batch Loss = 0.0007309958572136629
794, epoch_train_loss=0.0007309958572136629
Epoch 795
Epoch 795 :: Batch 0/1
Batch Loss = 0.0007309958553528095
795, epoch_train_loss=0.0007309958553528095
Epoch 796
Epoch 796 :: Batch 0/1
Batch Loss = 0.0007309958534927501
796, epoch_train_loss=0.0007309958534927501
Epoch 797
Epoch 797 :: Batch 0/1
Batch Loss = 0.0007309958516334861
797, epoch_train_loss=0.0007309958516334861
Epoch 798
Epoch 798 :: Batch 0/1
Batch Loss = 0.0007309958497750191
798, epoch_train_loss=0.0007309958497750191
Epoch 799
Epoch 799 :: Batch 0/1
Batch Loss = 0.0007309958479173508
799, epoch_train_loss=0.0007309958479173508
Epoch 800
Epoch 800 :: Batch 0/1
Batch Loss = 0.0007309958460604829
800, epoch_train_loss=0.0007309958460604829
Epoch 801
Epoch 801 :: Batch 0/1
Batch Loss = 0.0007309958442044165
801, epoch_train_loss=0.0007309958442044165
Epoch 802
Epoch 802 :: Batch 0/1
Batch Loss = 0.0007309958423491533
802, epoch_train_loss=0.0007309958423491533
Epoch 803
Epoch 803 :: Batch 0/1
Batch Loss = 0.0007309958404946946
803, epoch_train_loss=0.0007309958404946946
Epoch 804
Epoch 804 :: Batch 0/1
Batch Loss = 0.0007309958386410423
804, epoch_train_loss=0.0007309958386410423
Epoch 805
Epoch 805 :: Batch 0/1
Batch Loss = 0.0007309958367881975
805, epoch_train_loss=0.0007309958367881975
Epoch 806
Epoch 806 :: Batch 0/1
Batch Loss = 0.0007309958349361621
806, epoch_train_loss=0.0007309958349361621
Epoch 807
Epoch 807 :: Batch 0/1
Batch Loss = 0.0007309958330849371
807, epoch_train_loss=0.0007309958330849371
Epoch 808
Epoch 808 :: Batch 0/1
Batch Loss = 0.0007309958312345244
808, epoch_train_loss=0.0007309958312345244
Epoch 809
Epoch 809 :: Batch 0/1
Batch Loss = 0.0007309958293849249
809, epoch_train_loss=0.0007309958293849249
Epoch 810
Epoch 810 :: Batch 0/1
Batch Loss = 0.0007309958275361407
810, epoch_train_loss=0.0007309958275361407
Epoch 811
Epoch 811 :: Batch 0/1
Batch Loss = 0.0007309958256881727
811, epoch_train_loss=0.0007309958256881727
Epoch 812
Epoch 812 :: Batch 0/1
Batch Loss = 0.0007309958238410228
812, epoch_train_loss=0.0007309958238410228
Epoch 813
Epoch 813 :: Batch 0/1
Batch Loss = 0.0007309958219946921
813, epoch_train_loss=0.0007309958219946921
Epoch 814
Epoch 814 :: Batch 0/1
Batch Loss = 0.0007309958201491821
814, epoch_train_loss=0.0007309958201491821
Epoch 815
Epoch 815 :: Batch 0/1
Batch Loss = 0.0007309958183044942
815, epoch_train_loss=0.0007309958183044942
Epoch 816
Epoch 816 :: Batch 0/1
Batch Loss = 0.00073099581646063
816, epoch_train_loss=0.00073099581646063
Epoch 817
Epoch 817 :: Batch 0/1
Batch Loss = 0.0007309958146175906
817, epoch_train_loss=0.0007309958146175906
Epoch 818
Epoch 818 :: Batch 0/1
Batch Loss = 0.0007309958127753776
818, epoch_train_loss=0.0007309958127753776
Epoch 819
Epoch 819 :: Batch 0/1
Batch Loss = 0.0007309958109339924
819, epoch_train_loss=0.0007309958109339924
Epoch 820
Epoch 820 :: Batch 0/1
Batch Loss = 0.000730995809093436
820, epoch_train_loss=0.000730995809093436
Epoch 821
Epoch 821 :: Batch 0/1
Batch Loss = 0.0007309958072537103
821, epoch_train_loss=0.0007309958072537103
Epoch 822
Epoch 822 :: Batch 0/1
Batch Loss = 0.0007309958054148163
822, epoch_train_loss=0.0007309958054148163
Epoch 823
Epoch 823 :: Batch 0/1
Batch Loss = 0.0007309958035767554
823, epoch_train_loss=0.0007309958035767554
Epoch 824
Epoch 824 :: Batch 0/1
Batch Loss = 0.0007309958017395293
824, epoch_train_loss=0.0007309958017395293
Epoch 825
Epoch 825 :: Batch 0/1
Batch Loss = 0.0007309957999031388
825, epoch_train_loss=0.0007309957999031388
Epoch 826
Epoch 826 :: Batch 0/1
Batch Loss = 0.0007309957980675854
826, epoch_train_loss=0.0007309957980675854
Epoch 827
Epoch 827 :: Batch 0/1
Batch Loss = 0.0007309957962328706
827, epoch_train_loss=0.0007309957962328706
Epoch 828
Epoch 828 :: Batch 0/1
Batch Loss = 0.0007309957943989956
828, epoch_train_loss=0.0007309957943989956
Epoch 829
Epoch 829 :: Batch 0/1
Batch Loss = 0.0007309957925659619
829, epoch_train_loss=0.0007309957925659619
Epoch 830
Epoch 830 :: Batch 0/1
Batch Loss = 0.0007309957907337704
830, epoch_train_loss=0.0007309957907337704
Epoch 831
Epoch 831 :: Batch 0/1
Batch Loss = 0.0007309957889024226
831, epoch_train_loss=0.0007309957889024226
Epoch 832
Epoch 832 :: Batch 0/1
Batch Loss = 0.0007309957870719197
832, epoch_train_loss=0.0007309957870719197
Epoch 833
Epoch 833 :: Batch 0/1
Batch Loss = 0.0007309957852422633
833, epoch_train_loss=0.0007309957852422633
Epoch 834
Epoch 834 :: Batch 0/1
Batch Loss = 0.0007309957834134542
834, epoch_train_loss=0.0007309957834134542
Epoch 835
Epoch 835 :: Batch 0/1
Batch Loss = 0.0007309957815854942
835, epoch_train_loss=0.0007309957815854942
Epoch 836
Epoch 836 :: Batch 0/1
Batch Loss = 0.000730995779758384
836, epoch_train_loss=0.000730995779758384
Epoch 837
Epoch 837 :: Batch 0/1
Batch Loss = 0.0007309957779321252
837, epoch_train_loss=0.0007309957779321252
Epoch 838
Epoch 838 :: Batch 0/1
Batch Loss = 0.0007309957761067191
838, epoch_train_loss=0.0007309957761067191
Epoch 839
Epoch 839 :: Batch 0/1
Batch Loss = 0.0007309957742821667
839, epoch_train_loss=0.0007309957742821667
Epoch 840
Epoch 840 :: Batch 0/1
Batch Loss = 0.0007309957724584694
840, epoch_train_loss=0.0007309957724584694
Epoch 841
Epoch 841 :: Batch 0/1
Batch Loss = 0.0007309957706356284
841, epoch_train_loss=0.0007309957706356284
Epoch 842
Epoch 842 :: Batch 0/1
Batch Loss = 0.0007309957688136447
842, epoch_train_loss=0.0007309957688136447
Epoch 843
Epoch 843 :: Batch 0/1
Batch Loss = 0.0007309957669925198
843, epoch_train_loss=0.0007309957669925198
Epoch 844
Epoch 844 :: Batch 0/1
Batch Loss = 0.0007309957651722547
844, epoch_train_loss=0.0007309957651722547
Epoch 845
Epoch 845 :: Batch 0/1
Batch Loss = 0.0007309957633528506
845, epoch_train_loss=0.0007309957633528506
Epoch 846
Epoch 846 :: Batch 0/1
Batch Loss = 0.000730995761534309
846, epoch_train_loss=0.000730995761534309
Epoch 847
Epoch 847 :: Batch 0/1
Batch Loss = 0.0007309957597166309
847, epoch_train_loss=0.0007309957597166309
Epoch 848
Epoch 848 :: Batch 0/1
Batch Loss = 0.0007309957578998175
848, epoch_train_loss=0.0007309957578998175
Epoch 849
Epoch 849 :: Batch 0/1
Batch Loss = 0.0007309957560838695
849, epoch_train_loss=0.0007309957560838695
Epoch 850
Epoch 850 :: Batch 0/1
Batch Loss = 0.0007309957542687887
850, epoch_train_loss=0.0007309957542687887
Epoch 851
Epoch 851 :: Batch 0/1
Batch Loss = 0.0007309957524545757
851, epoch_train_loss=0.0007309957524545757
Epoch 852
Epoch 852 :: Batch 0/1
Batch Loss = 0.0007309957506412322
852, epoch_train_loss=0.0007309957506412322
Epoch 853
Epoch 853 :: Batch 0/1
Batch Loss = 0.0007309957488287591
853, epoch_train_loss=0.0007309957488287591
Epoch 854
Epoch 854 :: Batch 0/1
Batch Loss = 0.0007309957470171575
854, epoch_train_loss=0.0007309957470171575
Epoch 855
Epoch 855 :: Batch 0/1
Batch Loss = 0.0007309957452064284
855, epoch_train_loss=0.0007309957452064284
Epoch 856
Epoch 856 :: Batch 0/1
Batch Loss = 0.0007309957433965733
856, epoch_train_loss=0.0007309957433965733
Epoch 857
Epoch 857 :: Batch 0/1
Batch Loss = 0.0007309957415875928
857, epoch_train_loss=0.0007309957415875928
Epoch 858
Epoch 858 :: Batch 0/1
Batch Loss = 0.0007309957397794886
858, epoch_train_loss=0.0007309957397794886
Epoch 859
Epoch 859 :: Batch 0/1
Batch Loss = 0.0007309957379722613
859, epoch_train_loss=0.0007309957379722613
Epoch 860
Epoch 860 :: Batch 0/1
Batch Loss = 0.0007309957361659123
860, epoch_train_loss=0.0007309957361659123
Epoch 861
Epoch 861 :: Batch 0/1
Batch Loss = 0.0007309957343604425
861, epoch_train_loss=0.0007309957343604425
Epoch 862
Epoch 862 :: Batch 0/1
Batch Loss = 0.000730995732555853
862, epoch_train_loss=0.000730995732555853
Epoch 863
Epoch 863 :: Batch 0/1
Batch Loss = 0.0007309957307521449
863, epoch_train_loss=0.0007309957307521449
Epoch 864
Epoch 864 :: Batch 0/1
Batch Loss = 0.0007309957289493194
864, epoch_train_loss=0.0007309957289493194
Epoch 865
Epoch 865 :: Batch 0/1
Batch Loss = 0.0007309957271473775
865, epoch_train_loss=0.0007309957271473775
Epoch 866
Epoch 866 :: Batch 0/1
Batch Loss = 0.0007309957253463202
866, epoch_train_loss=0.0007309957253463202
Epoch 867
Epoch 867 :: Batch 0/1
Batch Loss = 0.0007309957235461484
867, epoch_train_loss=0.0007309957235461484
Epoch 868
Epoch 868 :: Batch 0/1
Batch Loss = 0.0007309957217468635
868, epoch_train_loss=0.0007309957217468635
Epoch 869
Epoch 869 :: Batch 0/1
Batch Loss = 0.0007309957199484661
869, epoch_train_loss=0.0007309957199484661
Epoch 870
Epoch 870 :: Batch 0/1
Batch Loss = 0.0007309957181509577
870, epoch_train_loss=0.0007309957181509577
Epoch 871
Epoch 871 :: Batch 0/1
Batch Loss = 0.0007309957163543389
871, epoch_train_loss=0.0007309957163543389
Epoch 872
Epoch 872 :: Batch 0/1
Batch Loss = 0.0007309957145586111
872, epoch_train_loss=0.0007309957145586111
Epoch 873
Epoch 873 :: Batch 0/1
Batch Loss = 0.0007309957127637749
873, epoch_train_loss=0.0007309957127637749
Epoch 874
Epoch 874 :: Batch 0/1
Batch Loss = 0.0007309957109698315
874, epoch_train_loss=0.0007309957109698315
Epoch 875
Epoch 875 :: Batch 0/1
Batch Loss = 0.000730995709176782
875, epoch_train_loss=0.000730995709176782
Epoch 876
Epoch 876 :: Batch 0/1
Batch Loss = 0.0007309957073846273
876, epoch_train_loss=0.0007309957073846273
Epoch 877
Epoch 877 :: Batch 0/1
Batch Loss = 0.0007309957055933685
877, epoch_train_loss=0.0007309957055933685
Epoch 878
Epoch 878 :: Batch 0/1
Batch Loss = 0.0007309957038030065
878, epoch_train_loss=0.0007309957038030065
Epoch 879
Epoch 879 :: Batch 0/1
Batch Loss = 0.000730995702013542
879, epoch_train_loss=0.000730995702013542
Epoch 880
Epoch 880 :: Batch 0/1
Batch Loss = 0.0007309957002249764
880, epoch_train_loss=0.0007309957002249764
Epoch 881
Epoch 881 :: Batch 0/1
Batch Loss = 0.0007309956984373104
881, epoch_train_loss=0.0007309956984373104
Epoch 882
Epoch 882 :: Batch 0/1
Batch Loss = 0.000730995696650545
882, epoch_train_loss=0.000730995696650545
Epoch 883
Epoch 883 :: Batch 0/1
Batch Loss = 0.0007309956948646812
883, epoch_train_loss=0.0007309956948646812
Epoch 884
Epoch 884 :: Batch 0/1
Batch Loss = 0.0007309956930797199
884, epoch_train_loss=0.0007309956930797199
Epoch 885
Epoch 885 :: Batch 0/1
Batch Loss = 0.0007309956912956621
885, epoch_train_loss=0.0007309956912956621
Epoch 886
Epoch 886 :: Batch 0/1
Batch Loss = 0.0007309956895125086
886, epoch_train_loss=0.0007309956895125086
Epoch 887
Epoch 887 :: Batch 0/1
Batch Loss = 0.0007309956877302603
887, epoch_train_loss=0.0007309956877302603
Epoch 888
Epoch 888 :: Batch 0/1
Batch Loss = 0.0007309956859489183
888, epoch_train_loss=0.0007309956859489183
Epoch 889
Epoch 889 :: Batch 0/1
Batch Loss = 0.0007309956841684836
889, epoch_train_loss=0.0007309956841684836
Epoch 890
Epoch 890 :: Batch 0/1
Batch Loss = 0.0007309956823889569
890, epoch_train_loss=0.0007309956823889569
Epoch 891
Epoch 891 :: Batch 0/1
Batch Loss = 0.0007309956806103389
891, epoch_train_loss=0.0007309956806103389
Epoch 892
Epoch 892 :: Batch 0/1
Batch Loss = 0.0007309956788326309
892, epoch_train_loss=0.0007309956788326309
Epoch 893
Epoch 893 :: Batch 0/1
Batch Loss = 0.0007309956770558335
893, epoch_train_loss=0.0007309956770558335
Epoch 894
Epoch 894 :: Batch 0/1
Batch Loss = 0.0007309956752799479
894, epoch_train_loss=0.0007309956752799479
Epoch 895
Epoch 895 :: Batch 0/1
Batch Loss = 0.0007309956735049745
895, epoch_train_loss=0.0007309956735049745
Epoch 896
Epoch 896 :: Batch 0/1
Batch Loss = 0.0007309956717309147
896, epoch_train_loss=0.0007309956717309147
Epoch 897
Epoch 897 :: Batch 0/1
Batch Loss = 0.000730995669957769
897, epoch_train_loss=0.000730995669957769
Epoch 898
Epoch 898 :: Batch 0/1
Batch Loss = 0.0007309956681855382
898, epoch_train_loss=0.0007309956681855382
Epoch 899
Epoch 899 :: Batch 0/1
Batch Loss = 0.0007309956664142234
899, epoch_train_loss=0.0007309956664142234
Epoch 900
Epoch 900 :: Batch 0/1
Batch Loss = 0.0007309956646438256
900, epoch_train_loss=0.0007309956646438256
Epoch 901
Epoch 901 :: Batch 0/1
Batch Loss = 0.0007309956628743451
901, epoch_train_loss=0.0007309956628743451
Epoch 902
Epoch 902 :: Batch 0/1
Batch Loss = 0.0007309956611057833
902, epoch_train_loss=0.0007309956611057833
Epoch 903
Epoch 903 :: Batch 0/1
Batch Loss = 0.0007309956593381406
903, epoch_train_loss=0.0007309956593381406
Epoch 904
Epoch 904 :: Batch 0/1
Batch Loss = 0.0007309956575714182
904, epoch_train_loss=0.0007309956575714182
Epoch 905
Epoch 905 :: Batch 0/1
Batch Loss = 0.0007309956558056165
905, epoch_train_loss=0.0007309956558056165
Epoch 906
Epoch 906 :: Batch 0/1
Batch Loss = 0.0007309956540407366
906, epoch_train_loss=0.0007309956540407366
Epoch 907
Epoch 907 :: Batch 0/1
Batch Loss = 0.0007309956522767792
907, epoch_train_loss=0.0007309956522767792
Epoch 908
Epoch 908 :: Batch 0/1
Batch Loss = 0.000730995650513745
908, epoch_train_loss=0.000730995650513745
Epoch 909
Epoch 909 :: Batch 0/1
Batch Loss = 0.0007309956487516351
909, epoch_train_loss=0.0007309956487516351
Epoch 910
Epoch 910 :: Batch 0/1
Batch Loss = 0.0007309956469904502
910, epoch_train_loss=0.0007309956469904502
Epoch 911
Epoch 911 :: Batch 0/1
Batch Loss = 0.0007309956452301908
911, epoch_train_loss=0.0007309956452301908
Epoch 912
Epoch 912 :: Batch 0/1
Batch Loss = 0.0007309956434708581
912, epoch_train_loss=0.0007309956434708581
Epoch 913
Epoch 913 :: Batch 0/1
Batch Loss = 0.0007309956417124525
913, epoch_train_loss=0.0007309956417124525
Epoch 914
Epoch 914 :: Batch 0/1
Batch Loss = 0.0007309956399549751
914, epoch_train_loss=0.0007309956399549751
Epoch 915
Epoch 915 :: Batch 0/1
Batch Loss = 0.0007309956381984265
915, epoch_train_loss=0.0007309956381984265
Epoch 916
Epoch 916 :: Batch 0/1
Batch Loss = 0.0007309956364428073
916, epoch_train_loss=0.0007309956364428073
Epoch 917
Epoch 917 :: Batch 0/1
Batch Loss = 0.0007309956346881185
917, epoch_train_loss=0.0007309956346881185
Epoch 918
Epoch 918 :: Batch 0/1
Batch Loss = 0.0007309956329343606
918, epoch_train_loss=0.0007309956329343606
Epoch 919
Epoch 919 :: Batch 0/1
Batch Loss = 0.0007309956311815348
919, epoch_train_loss=0.0007309956311815348
Epoch 920
Epoch 920 :: Batch 0/1
Batch Loss = 0.0007309956294296411
920, epoch_train_loss=0.0007309956294296411
Epoch 921
Epoch 921 :: Batch 0/1
Batch Loss = 0.000730995627678681
921, epoch_train_loss=0.000730995627678681
Epoch 922
Epoch 922 :: Batch 0/1
Batch Loss = 0.0007309956259286548
922, epoch_train_loss=0.0007309956259286548
Epoch 923
Epoch 923 :: Batch 0/1
Batch Loss = 0.0007309956241795632
923, epoch_train_loss=0.0007309956241795632
Epoch 924
Epoch 924 :: Batch 0/1
Batch Loss = 0.0007309956224314069
924, epoch_train_loss=0.0007309956224314069
Epoch 925
Epoch 925 :: Batch 0/1
Batch Loss = 0.0007309956206841869
925, epoch_train_loss=0.0007309956206841869
Epoch 926
Epoch 926 :: Batch 0/1
Batch Loss = 0.0007309956189379039
926, epoch_train_loss=0.0007309956189379039
Epoch 927
Epoch 927 :: Batch 0/1
Batch Loss = 0.000730995617192558
927, epoch_train_loss=0.000730995617192558
Epoch 928
Epoch 928 :: Batch 0/1
Batch Loss = 0.0007309956154481507
928, epoch_train_loss=0.0007309956154481507
Epoch 929
Epoch 929 :: Batch 0/1
Batch Loss = 0.0007309956137046819
929, epoch_train_loss=0.0007309956137046819
Epoch 930
Epoch 930 :: Batch 0/1
Batch Loss = 0.000730995611962153
930, epoch_train_loss=0.000730995611962153
Epoch 931
Epoch 931 :: Batch 0/1
Batch Loss = 0.0007309956102205642
931, epoch_train_loss=0.0007309956102205642
Epoch 932
Epoch 932 :: Batch 0/1
Batch Loss = 0.0007309956084799163
932, epoch_train_loss=0.0007309956084799163
Epoch 933
Epoch 933 :: Batch 0/1
Batch Loss = 0.00073099560674021
933, epoch_train_loss=0.00073099560674021
Epoch 934
Epoch 934 :: Batch 0/1
Batch Loss = 0.000730995605001446
934, epoch_train_loss=0.000730995605001446
Epoch 935
Epoch 935 :: Batch 0/1
Batch Loss = 0.0007309956032636248
935, epoch_train_loss=0.0007309956032636248
Epoch 936
Epoch 936 :: Batch 0/1
Batch Loss = 0.0007309956015267473
936, epoch_train_loss=0.0007309956015267473
Epoch 937
Epoch 937 :: Batch 0/1
Batch Loss = 0.000730995599790814
937, epoch_train_loss=0.000730995599790814
Epoch 938
Epoch 938 :: Batch 0/1
Batch Loss = 0.0007309955980558255
938, epoch_train_loss=0.0007309955980558255
Epoch 939
Epoch 939 :: Batch 0/1
Batch Loss = 0.0007309955963217827
939, epoch_train_loss=0.0007309955963217827
Epoch 940
Epoch 940 :: Batch 0/1
Batch Loss = 0.0007309955945886858
940, epoch_train_loss=0.0007309955945886858
Epoch 941
Epoch 941 :: Batch 0/1
Batch Loss = 0.0007309955928565357
941, epoch_train_loss=0.0007309955928565357
Epoch 942
Epoch 942 :: Batch 0/1
Batch Loss = 0.0007309955911253329
942, epoch_train_loss=0.0007309955911253329
Epoch 943
Epoch 943 :: Batch 0/1
Batch Loss = 0.0007309955893950781
943, epoch_train_loss=0.0007309955893950781
Epoch 944
Epoch 944 :: Batch 0/1
Batch Loss = 0.000730995587665772
944, epoch_train_loss=0.000730995587665772
Epoch 945
Epoch 945 :: Batch 0/1
Batch Loss = 0.0007309955859374149
945, epoch_train_loss=0.0007309955859374149
Epoch 946
Epoch 946 :: Batch 0/1
Batch Loss = 0.0007309955842100078
946, epoch_train_loss=0.0007309955842100078
Epoch 947
Epoch 947 :: Batch 0/1
Batch Loss = 0.000730995582483551
947, epoch_train_loss=0.000730995582483551
Epoch 948
Epoch 948 :: Batch 0/1
Batch Loss = 0.0007309955807580452
948, epoch_train_loss=0.0007309955807580452
Epoch 949
Epoch 949 :: Batch 0/1
Batch Loss = 0.000730995579033491
949, epoch_train_loss=0.000730995579033491
Epoch 950
Epoch 950 :: Batch 0/1
Batch Loss = 0.000730995577309889
950, epoch_train_loss=0.000730995577309889
Epoch 951
Epoch 951 :: Batch 0/1
Batch Loss = 0.0007309955755872396
951, epoch_train_loss=0.0007309955755872396
Epoch 952
Epoch 952 :: Batch 0/1
Batch Loss = 0.0007309955738655434
952, epoch_train_loss=0.0007309955738655434
Epoch 953
Epoch 953 :: Batch 0/1
Batch Loss = 0.0007309955721448015
953, epoch_train_loss=0.0007309955721448015
Epoch 954
Epoch 954 :: Batch 0/1
Batch Loss = 0.0007309955704250138
954, epoch_train_loss=0.0007309955704250138
Epoch 955
Epoch 955 :: Batch 0/1
Batch Loss = 0.000730995568706181
955, epoch_train_loss=0.000730995568706181
Epoch 956
Epoch 956 :: Batch 0/1
Batch Loss = 0.0007309955669883038
956, epoch_train_loss=0.0007309955669883038
Epoch 957
Epoch 957 :: Batch 0/1
Batch Loss = 0.0007309955652713826
957, epoch_train_loss=0.0007309955652713826
Epoch 958
Epoch 958 :: Batch 0/1
Batch Loss = 0.0007309955635554182
958, epoch_train_loss=0.0007309955635554182
Epoch 959
Epoch 959 :: Batch 0/1
Batch Loss = 0.0007309955618404108
959, epoch_train_loss=0.0007309955618404108
Epoch 960
Epoch 960 :: Batch 0/1
Batch Loss = 0.0007309955601263612
960, epoch_train_loss=0.0007309955601263612
Epoch 961
Epoch 961 :: Batch 0/1
Batch Loss = 0.0007309955584132696
961, epoch_train_loss=0.0007309955584132696
Epoch 962
Epoch 962 :: Batch 0/1
Batch Loss = 0.000730995556701137
962, epoch_train_loss=0.000730995556701137
Epoch 963
Epoch 963 :: Batch 0/1
Batch Loss = 0.0007309955549899636
963, epoch_train_loss=0.0007309955549899636
Epoch 964
Epoch 964 :: Batch 0/1
Batch Loss = 0.00073099555327975
964, epoch_train_loss=0.00073099555327975
Epoch 965
Epoch 965 :: Batch 0/1
Batch Loss = 0.0007309955515704968
965, epoch_train_loss=0.0007309955515704968
Epoch 966
Epoch 966 :: Batch 0/1
Batch Loss = 0.0007309955498622043
966, epoch_train_loss=0.0007309955498622043
Epoch 967
Epoch 967 :: Batch 0/1
Batch Loss = 0.0007309955481548733
967, epoch_train_loss=0.0007309955481548733
Epoch 968
Epoch 968 :: Batch 0/1
Batch Loss = 0.0007309955464485037
968, epoch_train_loss=0.0007309955464485037
Epoch 969
Epoch 969 :: Batch 0/1
Batch Loss = 0.0007309955447430967
969, epoch_train_loss=0.0007309955447430967
Epoch 970
Epoch 970 :: Batch 0/1
Batch Loss = 0.0007309955430386523
970, epoch_train_loss=0.0007309955430386523
Epoch 971
Epoch 971 :: Batch 0/1
Batch Loss = 0.0007309955413351714
971, epoch_train_loss=0.0007309955413351714
Epoch 972
Epoch 972 :: Batch 0/1
Batch Loss = 0.0007309955396326538
972, epoch_train_loss=0.0007309955396326538
Epoch 973
Epoch 973 :: Batch 0/1
Batch Loss = 0.0007309955379311009
973, epoch_train_loss=0.0007309955379311009
Epoch 974
Epoch 974 :: Batch 0/1
Batch Loss = 0.0007309955362305124
974, epoch_train_loss=0.0007309955362305124
Epoch 975
Epoch 975 :: Batch 0/1
Batch Loss = 0.0007309955345308891
975, epoch_train_loss=0.0007309955345308891
Epoch 976
Epoch 976 :: Batch 0/1
Batch Loss = 0.0007309955328322313
976, epoch_train_loss=0.0007309955328322313
Epoch 977
Epoch 977 :: Batch 0/1
Batch Loss = 0.0007309955311345396
977, epoch_train_loss=0.0007309955311345396
Epoch 978
Epoch 978 :: Batch 0/1
Batch Loss = 0.0007309955294378145
978, epoch_train_loss=0.0007309955294378145
Epoch 979
Epoch 979 :: Batch 0/1
Batch Loss = 0.0007309955277420562
979, epoch_train_loss=0.0007309955277420562
Epoch 980
Epoch 980 :: Batch 0/1
Batch Loss = 0.0007309955260472654
980, epoch_train_loss=0.0007309955260472654
Epoch 981
Epoch 981 :: Batch 0/1
Batch Loss = 0.0007309955243534426
981, epoch_train_loss=0.0007309955243534426
Epoch 982
Epoch 982 :: Batch 0/1
Batch Loss = 0.0007309955226605879
982, epoch_train_loss=0.0007309955226605879
Epoch 983
Epoch 983 :: Batch 0/1
Batch Loss = 0.0007309955209687019
983, epoch_train_loss=0.0007309955209687019
Epoch 984
Epoch 984 :: Batch 0/1
Batch Loss = 0.0007309955192777852
984, epoch_train_loss=0.0007309955192777852
Epoch 985
Epoch 985 :: Batch 0/1
Batch Loss = 0.0007309955175878378
985, epoch_train_loss=0.0007309955175878378
Epoch 986
Epoch 986 :: Batch 0/1
Batch Loss = 0.0007309955158988605
986, epoch_train_loss=0.0007309955158988605
Epoch 987
Epoch 987 :: Batch 0/1
Batch Loss = 0.0007309955142108536
987, epoch_train_loss=0.0007309955142108536
Epoch 988
Epoch 988 :: Batch 0/1
Batch Loss = 0.0007309955125238172
988, epoch_train_loss=0.0007309955125238172
Epoch 989
Epoch 989 :: Batch 0/1
Batch Loss = 0.0007309955108377524
989, epoch_train_loss=0.0007309955108377524
Epoch 990
Epoch 990 :: Batch 0/1
Batch Loss = 0.000730995509152659
990, epoch_train_loss=0.000730995509152659
Epoch 991
Epoch 991 :: Batch 0/1
Batch Loss = 0.0007309955074685374
991, epoch_train_loss=0.0007309955074685374
Epoch 992
Epoch 992 :: Batch 0/1
Batch Loss = 0.0007309955057853884
992, epoch_train_loss=0.0007309955057853884
Epoch 993
Epoch 993 :: Batch 0/1
Batch Loss = 0.0007309955041032121
993, epoch_train_loss=0.0007309955041032121
Epoch 994
Epoch 994 :: Batch 0/1
Batch Loss = 0.000730995502422009
994, epoch_train_loss=0.000730995502422009
Epoch 995
Epoch 995 :: Batch 0/1
Batch Loss = 0.0007309955007417793
995, epoch_train_loss=0.0007309955007417793
Epoch 996
Epoch 996 :: Batch 0/1
Batch Loss = 0.0007309954990625235
996, epoch_train_loss=0.0007309954990625235
Epoch 997
Epoch 997 :: Batch 0/1
Batch Loss = 0.000730995497384242
997, epoch_train_loss=0.000730995497384242
Epoch 998
Epoch 998 :: Batch 0/1
Batch Loss = 0.000730995495706935
998, epoch_train_loss=0.000730995495706935
Epoch 999
Epoch 999 :: Batch 0/1
Batch Loss = 0.0007309954940306031
999, epoch_train_loss=0.0007309954940306031
Epoch 1000
Epoch 1000 :: Batch 0/1
Batch Loss = 0.0007309954923552465
1000, epoch_train_loss=0.0007309954923552465
Epoch 1001
Epoch 1001 :: Batch 0/1
Batch Loss = 0.0007309954906808659
1001, epoch_train_loss=0.0007309954906808659
Epoch 1002
Epoch 1002 :: Batch 0/1
Batch Loss = 0.0007309954890074607
1002, epoch_train_loss=0.0007309954890074607
Epoch 1003
Epoch 1003 :: Batch 0/1
Batch Loss = 0.0007309954873350325
1003, epoch_train_loss=0.0007309954873350325
Epoch 1004
Epoch 1004 :: Batch 0/1
Batch Loss = 0.0007309954856635808
1004, epoch_train_loss=0.0007309954856635808
Epoch 1005
Epoch 1005 :: Batch 0/1
Batch Loss = 0.0007309954839931061
1005, epoch_train_loss=0.0007309954839931061
Epoch 1006
Epoch 1006 :: Batch 0/1
Batch Loss = 0.0007309954823236088
1006, epoch_train_loss=0.0007309954823236088
Epoch 1007
Epoch 1007 :: Batch 0/1
Batch Loss = 0.0007309954806550893
1007, epoch_train_loss=0.0007309954806550893
Epoch 1008
Epoch 1008 :: Batch 0/1
Batch Loss = 0.000730995478987548
1008, epoch_train_loss=0.000730995478987548
Epoch 1009
Epoch 1009 :: Batch 0/1
Batch Loss = 0.0007309954773209847
1009, epoch_train_loss=0.0007309954773209847
Epoch 1010
Epoch 1010 :: Batch 0/1
Batch Loss = 0.0007309954756554004
1010, epoch_train_loss=0.0007309954756554004
Epoch 1011
Epoch 1011 :: Batch 0/1
Batch Loss = 0.000730995473990795
1011, epoch_train_loss=0.000730995473990795
Epoch 1012
Epoch 1012 :: Batch 0/1
Batch Loss = 0.0007309954723271689
1012, epoch_train_loss=0.0007309954723271689
Epoch 1013
Epoch 1013 :: Batch 0/1
Batch Loss = 0.0007309954706645225
1013, epoch_train_loss=0.0007309954706645225
Epoch 1014
Epoch 1014 :: Batch 0/1
Batch Loss = 0.000730995469002856
1014, epoch_train_loss=0.000730995469002856
Epoch 1015
Epoch 1015 :: Batch 0/1
Batch Loss = 0.0007309954673421698
1015, epoch_train_loss=0.0007309954673421698
Epoch 1016
Epoch 1016 :: Batch 0/1
Batch Loss = 0.000730995465682464
1016, epoch_train_loss=0.000730995465682464
Epoch 1017
Epoch 1017 :: Batch 0/1
Batch Loss = 0.0007309954640237388
1017, epoch_train_loss=0.0007309954640237388
Epoch 1018
Epoch 1018 :: Batch 0/1
Batch Loss = 0.0007309954623659951
1018, epoch_train_loss=0.0007309954623659951
Epoch 1019
Epoch 1019 :: Batch 0/1
Batch Loss = 0.0007309954607092326
1019, epoch_train_loss=0.0007309954607092326
Epoch 1020
Epoch 1020 :: Batch 0/1
Batch Loss = 0.0007309954590534518
1020, epoch_train_loss=0.0007309954590534518
Epoch 1021
Epoch 1021 :: Batch 0/1
Batch Loss = 0.0007309954573986529
1021, epoch_train_loss=0.0007309954573986529
Epoch 1022
Epoch 1022 :: Batch 0/1
Batch Loss = 0.0007309954557448362
1022, epoch_train_loss=0.0007309954557448362
Epoch 1023
Epoch 1023 :: Batch 0/1
Batch Loss = 0.000730995454092002
1023, epoch_train_loss=0.000730995454092002
Epoch 1024
Epoch 1024 :: Batch 0/1
Batch Loss = 0.0007309954524401504
1024, epoch_train_loss=0.0007309954524401504
Epoch 1025
Epoch 1025 :: Batch 0/1
Batch Loss = 0.000730995450789282
1025, epoch_train_loss=0.000730995450789282
Epoch 1026
Epoch 1026 :: Batch 0/1
Batch Loss = 0.0007309954491393966
1026, epoch_train_loss=0.0007309954491393966
Epoch 1027
Epoch 1027 :: Batch 0/1
Batch Loss = 0.0007309954474904947
1027, epoch_train_loss=0.0007309954474904947
Epoch 1028
Epoch 1028 :: Batch 0/1
Batch Loss = 0.0007309954458425766
1028, epoch_train_loss=0.0007309954458425766
Epoch 1029
Epoch 1029 :: Batch 0/1
Batch Loss = 0.0007309954441956426
1029, epoch_train_loss=0.0007309954441956426
Epoch 1030
Epoch 1030 :: Batch 0/1
Batch Loss = 0.0007309954425496928
1030, epoch_train_loss=0.0007309954425496928
Epoch 1031
Epoch 1031 :: Batch 0/1
Batch Loss = 0.0007309954409047272
1031, epoch_train_loss=0.0007309954409047272
Epoch 1032
Epoch 1032 :: Batch 0/1
Batch Loss = 0.0007309954392607466
1032, epoch_train_loss=0.0007309954392607466
Epoch 1033
Epoch 1033 :: Batch 0/1
Batch Loss = 0.0007309954376177509
1033, epoch_train_loss=0.0007309954376177509
Epoch 1034
Epoch 1034 :: Batch 0/1
Batch Loss = 0.0007309954359757401
1034, epoch_train_loss=0.0007309954359757401
Epoch 1035
Epoch 1035 :: Batch 0/1
Batch Loss = 0.000730995434334715
1035, epoch_train_loss=0.000730995434334715
Epoch 1036
Epoch 1036 :: Batch 0/1
Batch Loss = 0.0007309954326946755
1036, epoch_train_loss=0.0007309954326946755
Epoch 1037
Epoch 1037 :: Batch 0/1
Batch Loss = 0.0007309954310556214
1037, epoch_train_loss=0.0007309954310556214
Epoch 1038
Epoch 1038 :: Batch 0/1
Batch Loss = 0.0007309954294175539
1038, epoch_train_loss=0.0007309954294175539
Epoch 1039
Epoch 1039 :: Batch 0/1
Batch Loss = 0.0007309954277804722
1039, epoch_train_loss=0.0007309954277804722
Epoch 1040
Epoch 1040 :: Batch 0/1
Batch Loss = 0.0007309954261443771
1040, epoch_train_loss=0.0007309954261443771
Epoch 1041
Epoch 1041 :: Batch 0/1
Batch Loss = 0.0007309954245092685
1041, epoch_train_loss=0.0007309954245092685
Epoch 1042
Epoch 1042 :: Batch 0/1
Batch Loss = 0.0007309954228751469
1042, epoch_train_loss=0.0007309954228751469
Epoch 1043
Epoch 1043 :: Batch 0/1
Batch Loss = 0.0007309954212420122
1043, epoch_train_loss=0.0007309954212420122
Epoch 1044
Epoch 1044 :: Batch 0/1
Batch Loss = 0.0007309954196098649
1044, epoch_train_loss=0.0007309954196098649
Epoch 1045
Epoch 1045 :: Batch 0/1
Batch Loss = 0.0007309954179787049
1045, epoch_train_loss=0.0007309954179787049
Epoch 1046
Epoch 1046 :: Batch 0/1
Batch Loss = 0.0007309954163485326
1046, epoch_train_loss=0.0007309954163485326
Epoch 1047
Epoch 1047 :: Batch 0/1
Batch Loss = 0.0007309954147193479
1047, epoch_train_loss=0.0007309954147193479
Epoch 1048
Epoch 1048 :: Batch 0/1
Batch Loss = 0.0007309954130911511
1048, epoch_train_loss=0.0007309954130911511
Epoch 1049
Epoch 1049 :: Batch 0/1
Batch Loss = 0.0007309954114639426
1049, epoch_train_loss=0.0007309954114639426
Epoch 1050
Epoch 1050 :: Batch 0/1
Batch Loss = 0.0007309954098377224
1050, epoch_train_loss=0.0007309954098377224
Epoch 1051
Epoch 1051 :: Batch 0/1
Batch Loss = 0.0007309954082124905
1051, epoch_train_loss=0.0007309954082124905
Epoch 1052
Epoch 1052 :: Batch 0/1
Batch Loss = 0.0007309954065882472
1052, epoch_train_loss=0.0007309954065882472
Epoch 1053
Epoch 1053 :: Batch 0/1
Batch Loss = 0.000730995404964993
1053, epoch_train_loss=0.000730995404964993
Epoch 1054
Epoch 1054 :: Batch 0/1
Batch Loss = 0.0007309954033427274
1054, epoch_train_loss=0.0007309954033427274
Epoch 1055
Epoch 1055 :: Batch 0/1
Batch Loss = 0.000730995401721451
1055, epoch_train_loss=0.000730995401721451
Epoch 1056
Epoch 1056 :: Batch 0/1
Batch Loss = 0.0007309954001011638
1056, epoch_train_loss=0.0007309954001011638
Epoch 1057
Epoch 1057 :: Batch 0/1
Batch Loss = 0.000730995398481866
1057, epoch_train_loss=0.000730995398481866
Epoch 1058
Epoch 1058 :: Batch 0/1
Batch Loss = 0.000730995396863558
1058, epoch_train_loss=0.000730995396863558
Epoch 1059
Epoch 1059 :: Batch 0/1
Batch Loss = 0.0007309953952462394
1059, epoch_train_loss=0.0007309953952462394
Epoch 1060
Epoch 1060 :: Batch 0/1
Batch Loss = 0.0007309953936299108
1060, epoch_train_loss=0.0007309953936299108
Epoch 1061
Epoch 1061 :: Batch 0/1
Batch Loss = 0.0007309953920145719
1061, epoch_train_loss=0.0007309953920145719
Epoch 1062
Epoch 1062 :: Batch 0/1
Batch Loss = 0.0007309953904002231
1062, epoch_train_loss=0.0007309953904002231
Epoch 1063
Epoch 1063 :: Batch 0/1
Batch Loss = 0.0007309953887868645
1063, epoch_train_loss=0.0007309953887868645
Epoch 1064
Epoch 1064 :: Batch 0/1
Batch Loss = 0.0007309953871744964
1064, epoch_train_loss=0.0007309953871744964
Epoch 1065
Epoch 1065 :: Batch 0/1
Batch Loss = 0.0007309953855631188
1065, epoch_train_loss=0.0007309953855631188
Epoch 1066
Epoch 1066 :: Batch 0/1
Batch Loss = 0.0007309953839527315
1066, epoch_train_loss=0.0007309953839527315
Epoch 1067
Epoch 1067 :: Batch 0/1
Batch Loss = 0.000730995382343335
1067, epoch_train_loss=0.000730995382343335
Epoch 1068
Epoch 1068 :: Batch 0/1
Batch Loss = 0.0007309953807349291
1068, epoch_train_loss=0.0007309953807349291
Epoch 1069
Epoch 1069 :: Batch 0/1
Batch Loss = 0.0007309953791275142
1069, epoch_train_loss=0.0007309953791275142
Epoch 1070
Epoch 1070 :: Batch 0/1
Batch Loss = 0.0007309953775210903
1070, epoch_train_loss=0.0007309953775210903
Epoch 1071
Epoch 1071 :: Batch 0/1
Batch Loss = 0.0007309953759156575
1071, epoch_train_loss=0.0007309953759156575
Epoch 1072
Epoch 1072 :: Batch 0/1
Batch Loss = 0.0007309953743112158
1072, epoch_train_loss=0.0007309953743112158
Epoch 1073
Epoch 1073 :: Batch 0/1
Batch Loss = 0.0007309953727077656
1073, epoch_train_loss=0.0007309953727077656
Epoch 1074
Epoch 1074 :: Batch 0/1
Batch Loss = 0.0007309953711053068
1074, epoch_train_loss=0.0007309953711053068
Epoch 1075
Epoch 1075 :: Batch 0/1
Batch Loss = 0.0007309953695038393
1075, epoch_train_loss=0.0007309953695038393
Epoch 1076
Epoch 1076 :: Batch 0/1
Batch Loss = 0.0007309953679033633
1076, epoch_train_loss=0.0007309953679033633
Epoch 1077
Epoch 1077 :: Batch 0/1
Batch Loss = 0.0007309953663038791
1077, epoch_train_loss=0.0007309953663038791
Epoch 1078
Epoch 1078 :: Batch 0/1
Batch Loss = 0.0007309953647053865
1078, epoch_train_loss=0.0007309953647053865
Epoch 1079
Epoch 1079 :: Batch 0/1
Batch Loss = 0.0007309953631078857
1079, epoch_train_loss=0.0007309953631078857
Epoch 1080
Epoch 1080 :: Batch 0/1
Batch Loss = 0.0007309953615113768
1080, epoch_train_loss=0.0007309953615113768
Epoch 1081
Epoch 1081 :: Batch 0/1
Batch Loss = 0.0007309953599158596
1081, epoch_train_loss=0.0007309953599158596
Epoch 1082
Epoch 1082 :: Batch 0/1
Batch Loss = 0.0007309953583213345
1082, epoch_train_loss=0.0007309953583213345
Epoch 1083
Epoch 1083 :: Batch 0/1
Batch Loss = 0.0007309953567278017
1083, epoch_train_loss=0.0007309953567278017
Epoch 1084
Epoch 1084 :: Batch 0/1
Batch Loss = 0.000730995355135261
1084, epoch_train_loss=0.000730995355135261
Epoch 1085
Epoch 1085 :: Batch 0/1
Batch Loss = 0.0007309953535437123
1085, epoch_train_loss=0.0007309953535437123
Epoch 1086
Epoch 1086 :: Batch 0/1
Batch Loss = 0.000730995351953156
1086, epoch_train_loss=0.000730995351953156
Epoch 1087
Epoch 1087 :: Batch 0/1
Batch Loss = 0.0007309953503635919
1087, epoch_train_loss=0.0007309953503635919
Epoch 1088
Epoch 1088 :: Batch 0/1
Batch Loss = 0.0007309953487750202
1088, epoch_train_loss=0.0007309953487750202
Epoch 1089
Epoch 1089 :: Batch 0/1
Batch Loss = 0.0007309953471874407
1089, epoch_train_loss=0.0007309953471874407
Epoch 1090
Epoch 1090 :: Batch 0/1
Batch Loss = 0.0007309953456008538
1090, epoch_train_loss=0.0007309953456008538
Epoch 1091
Epoch 1091 :: Batch 0/1
Batch Loss = 0.0007309953440152593
1091, epoch_train_loss=0.0007309953440152593
Epoch 1092
Epoch 1092 :: Batch 0/1
Batch Loss = 0.0007309953424306573
1092, epoch_train_loss=0.0007309953424306573
Epoch 1093
Epoch 1093 :: Batch 0/1
Batch Loss = 0.000730995340847048
1093, epoch_train_loss=0.000730995340847048
Epoch 1094
Epoch 1094 :: Batch 0/1
Batch Loss = 0.0007309953392644312
1094, epoch_train_loss=0.0007309953392644312
Epoch 1095
Epoch 1095 :: Batch 0/1
Batch Loss = 0.000730995337682807
1095, epoch_train_loss=0.000730995337682807
Epoch 1096
Epoch 1096 :: Batch 0/1
Batch Loss = 0.0007309953361021753
1096, epoch_train_loss=0.0007309953361021753
Epoch 1097
Epoch 1097 :: Batch 0/1
Batch Loss = 0.0007309953345225361
1097, epoch_train_loss=0.0007309953345225361
Epoch 1098
Epoch 1098 :: Batch 0/1
Batch Loss = 0.0007309953329438899
1098, epoch_train_loss=0.0007309953329438899
Epoch 1099
Epoch 1099 :: Batch 0/1
Batch Loss = 0.0007309953313662363
1099, epoch_train_loss=0.0007309953313662363
Epoch 1100
Epoch 1100 :: Batch 0/1
Batch Loss = 0.0007309953297895752
1100, epoch_train_loss=0.0007309953297895752
Epoch 1101
Epoch 1101 :: Batch 0/1
Batch Loss = 0.0007309953282139071
1101, epoch_train_loss=0.0007309953282139071
Epoch 1102
Epoch 1102 :: Batch 0/1
Batch Loss = 0.0007309953266392316
1102, epoch_train_loss=0.0007309953266392316
Epoch 1103
Epoch 1103 :: Batch 0/1
Batch Loss = 0.0007309953250655488
1103, epoch_train_loss=0.0007309953250655488
Epoch 1104
Epoch 1104 :: Batch 0/1
Batch Loss = 0.0007309953234928589
1104, epoch_train_loss=0.0007309953234928589
Epoch 1105
Epoch 1105 :: Batch 0/1
Batch Loss = 0.0007309953219211615
1105, epoch_train_loss=0.0007309953219211615
Epoch 1106
Epoch 1106 :: Batch 0/1
Batch Loss = 0.0007309953203504569
1106, epoch_train_loss=0.0007309953203504569
Epoch 1107
Epoch 1107 :: Batch 0/1
Batch Loss = 0.0007309953187807451
1107, epoch_train_loss=0.0007309953187807451
Epoch 1108
Epoch 1108 :: Batch 0/1
Batch Loss = 0.0007309953172120261
1108, epoch_train_loss=0.0007309953172120261
Epoch 1109
Epoch 1109 :: Batch 0/1
Batch Loss = 0.0007309953156442996
1109, epoch_train_loss=0.0007309953156442996
Epoch 1110
Epoch 1110 :: Batch 0/1
Batch Loss = 0.0007309953140775656
1110, epoch_train_loss=0.0007309953140775656
Epoch 1111
Epoch 1111 :: Batch 0/1
Batch Loss = 0.0007309953125118245
1111, epoch_train_loss=0.0007309953125118245
Epoch 1112
Epoch 1112 :: Batch 0/1
Batch Loss = 0.0007309953109470763
1112, epoch_train_loss=0.0007309953109470763
Epoch 1113
Epoch 1113 :: Batch 0/1
Batch Loss = 0.0007309953093833203
1113, epoch_train_loss=0.0007309953093833203
Epoch 1114
Epoch 1114 :: Batch 0/1
Batch Loss = 0.0007309953078205571
1114, epoch_train_loss=0.0007309953078205571
Epoch 1115
Epoch 1115 :: Batch 0/1
Batch Loss = 0.0007309953062587866
1115, epoch_train_loss=0.0007309953062587866
Epoch 1116
Epoch 1116 :: Batch 0/1
Batch Loss = 0.0007309953046980084
1116, epoch_train_loss=0.0007309953046980084
Epoch 1117
Epoch 1117 :: Batch 0/1
Batch Loss = 0.0007309953031382228
1117, epoch_train_loss=0.0007309953031382228
Epoch 1118
Epoch 1118 :: Batch 0/1
Batch Loss = 0.0007309953015794297
1118, epoch_train_loss=0.0007309953015794297
Epoch 1119
Epoch 1119 :: Batch 0/1
Batch Loss = 0.0007309953000216293
1119, epoch_train_loss=0.0007309953000216293
Epoch 1120
Epoch 1120 :: Batch 0/1
Batch Loss = 0.0007309952984648212
1120, epoch_train_loss=0.0007309952984648212
Epoch 1121
Epoch 1121 :: Batch 0/1
Batch Loss = 0.0007309952969090052
1121, epoch_train_loss=0.0007309952969090052
Epoch 1122
Epoch 1122 :: Batch 0/1
Batch Loss = 0.0007309952953541819
1122, epoch_train_loss=0.0007309952953541819
Epoch 1123
Epoch 1123 :: Batch 0/1
Batch Loss = 0.0007309952938003506
1123, epoch_train_loss=0.0007309952938003506
Epoch 1124
Epoch 1124 :: Batch 0/1
Batch Loss = 0.0007309952922475116
1124, epoch_train_loss=0.0007309952922475116
Epoch 1125
Epoch 1125 :: Batch 0/1
Batch Loss = 0.0007309952906956647
1125, epoch_train_loss=0.0007309952906956647
Epoch 1126
Epoch 1126 :: Batch 0/1
Batch Loss = 0.00073099528914481
1126, epoch_train_loss=0.00073099528914481
Epoch 1127
Epoch 1127 :: Batch 0/1
Batch Loss = 0.0007309952875949472
1127, epoch_train_loss=0.0007309952875949472
Epoch 1128
Epoch 1128 :: Batch 0/1
Batch Loss = 0.0007309952860460765
1128, epoch_train_loss=0.0007309952860460765
Epoch 1129
Epoch 1129 :: Batch 0/1
Batch Loss = 0.0007309952844981978
1129, epoch_train_loss=0.0007309952844981978
Epoch 1130
Epoch 1130 :: Batch 0/1
Batch Loss = 0.0007309952829513108
1130, epoch_train_loss=0.0007309952829513108
Epoch 1131
Epoch 1131 :: Batch 0/1
Batch Loss = 0.0007309952814054157
1131, epoch_train_loss=0.0007309952814054157
Epoch 1132
Epoch 1132 :: Batch 0/1
Batch Loss = 0.0007309952798605124
1132, epoch_train_loss=0.0007309952798605124
Epoch 1133
Epoch 1133 :: Batch 0/1
Batch Loss = 0.0007309952783166008
1133, epoch_train_loss=0.0007309952783166008
Epoch 1134
Epoch 1134 :: Batch 0/1
Batch Loss = 0.0007309952767736805
1134, epoch_train_loss=0.0007309952767736805
Epoch 1135
Epoch 1135 :: Batch 0/1
Batch Loss = 0.0007309952752317517
1135, epoch_train_loss=0.0007309952752317517
Epoch 1136
Epoch 1136 :: Batch 0/1
Batch Loss = 0.0007309952736908145
1136, epoch_train_loss=0.0007309952736908145
Epoch 1137
Epoch 1137 :: Batch 0/1
Batch Loss = 0.0007309952721508687
1137, epoch_train_loss=0.0007309952721508687
Epoch 1138
Epoch 1138 :: Batch 0/1
Batch Loss = 0.000730995270611914
1138, epoch_train_loss=0.000730995270611914
Epoch 1139
Epoch 1139 :: Batch 0/1
Batch Loss = 0.0007309952690739504
1139, epoch_train_loss=0.0007309952690739504
Epoch 1140
Epoch 1140 :: Batch 0/1
Batch Loss = 0.0007309952675369779
1140, epoch_train_loss=0.0007309952675369779
Epoch 1141
Epoch 1141 :: Batch 0/1
Batch Loss = 0.0007309952660009967
1141, epoch_train_loss=0.0007309952660009967
Epoch 1142
Epoch 1142 :: Batch 0/1
Batch Loss = 0.000730995264466006
1142, epoch_train_loss=0.000730995264466006
Epoch 1143
Epoch 1143 :: Batch 0/1
Batch Loss = 0.0007309952629320064
1143, epoch_train_loss=0.0007309952629320064
Epoch 1144
Epoch 1144 :: Batch 0/1
Batch Loss = 0.0007309952613989972
1144, epoch_train_loss=0.0007309952613989972
Epoch 1145
Epoch 1145 :: Batch 0/1
Batch Loss = 0.0007309952598669785
1145, epoch_train_loss=0.0007309952598669785
Epoch 1146
Epoch 1146 :: Batch 0/1
Batch Loss = 0.0007309952583359506
1146, epoch_train_loss=0.0007309952583359506
Epoch 1147
Epoch 1147 :: Batch 0/1
Batch Loss = 0.0007309952568059129
1147, epoch_train_loss=0.0007309952568059129
Epoch 1148
Epoch 1148 :: Batch 0/1
Batch Loss = 0.0007309952552768656
1148, epoch_train_loss=0.0007309952552768656
Epoch 1149
Epoch 1149 :: Batch 0/1
Batch Loss = 0.0007309952537488082
1149, epoch_train_loss=0.0007309952537488082
Epoch 1150
Epoch 1150 :: Batch 0/1
Batch Loss = 0.0007309952522217411
1150, epoch_train_loss=0.0007309952522217411
Epoch 1151
Epoch 1151 :: Batch 0/1
Batch Loss = 0.0007309952506956638
1151, epoch_train_loss=0.0007309952506956638
Epoch 1152
Epoch 1152 :: Batch 0/1
Batch Loss = 0.0007309952491705763
1152, epoch_train_loss=0.0007309952491705763
Epoch 1153
Epoch 1153 :: Batch 0/1
Batch Loss = 0.0007309952476464784
1153, epoch_train_loss=0.0007309952476464784
Epoch 1154
Epoch 1154 :: Batch 0/1
Batch Loss = 0.0007309952461233703
1154, epoch_train_loss=0.0007309952461233703
Epoch 1155
Epoch 1155 :: Batch 0/1
Batch Loss = 0.0007309952446012515
1155, epoch_train_loss=0.0007309952446012515
Epoch 1156
Epoch 1156 :: Batch 0/1
Batch Loss = 0.0007309952430801218
1156, epoch_train_loss=0.0007309952430801218
Epoch 1157
Epoch 1157 :: Batch 0/1
Batch Loss = 0.0007309952415599813
1157, epoch_train_loss=0.0007309952415599813
Epoch 1158
Epoch 1158 :: Batch 0/1
Batch Loss = 0.0007309952400408302
1158, epoch_train_loss=0.0007309952400408302
Epoch 1159
Epoch 1159 :: Batch 0/1
Batch Loss = 0.0007309952385226678
1159, epoch_train_loss=0.0007309952385226678
Epoch 1160
Epoch 1160 :: Batch 0/1
Batch Loss = 0.0007309952370054942
1160, epoch_train_loss=0.0007309952370054942
Epoch 1161
Epoch 1161 :: Batch 0/1
Batch Loss = 0.0007309952354893092
1161, epoch_train_loss=0.0007309952354893092
Epoch 1162
Epoch 1162 :: Batch 0/1
Batch Loss = 0.0007309952339741127
1162, epoch_train_loss=0.0007309952339741127
Epoch 1163
Epoch 1163 :: Batch 0/1
Batch Loss = 0.0007309952324599044
1163, epoch_train_loss=0.0007309952324599044
Epoch 1164
Epoch 1164 :: Batch 0/1
Batch Loss = 0.0007309952309466845
1164, epoch_train_loss=0.0007309952309466845
Epoch 1165
Epoch 1165 :: Batch 0/1
Batch Loss = 0.0007309952294344525
1165, epoch_train_loss=0.0007309952294344525
Epoch 1166
Epoch 1166 :: Batch 0/1
Batch Loss = 0.0007309952279232085
1166, epoch_train_loss=0.0007309952279232085
Epoch 1167
Epoch 1167 :: Batch 0/1
Batch Loss = 0.0007309952264129522
1167, epoch_train_loss=0.0007309952264129522
Epoch 1168
Epoch 1168 :: Batch 0/1
Batch Loss = 0.0007309952249036836
1168, epoch_train_loss=0.0007309952249036836
Epoch 1169
Epoch 1169 :: Batch 0/1
Batch Loss = 0.0007309952233954025
1169, epoch_train_loss=0.0007309952233954025
Epoch 1170
Epoch 1170 :: Batch 0/1
Batch Loss = 0.0007309952218881086
1170, epoch_train_loss=0.0007309952218881086
Epoch 1171
Epoch 1171 :: Batch 0/1
Batch Loss = 0.0007309952203818016
1171, epoch_train_loss=0.0007309952203818016
Epoch 1172
Epoch 1172 :: Batch 0/1
Batch Loss = 0.0007309952188764817
1172, epoch_train_loss=0.0007309952188764817
Epoch 1173
Epoch 1173 :: Batch 0/1
Batch Loss = 0.0007309952173721488
1173, epoch_train_loss=0.0007309952173721488
Epoch 1174
Epoch 1174 :: Batch 0/1
Batch Loss = 0.0007309952158688023
1174, epoch_train_loss=0.0007309952158688023
Epoch 1175
Epoch 1175 :: Batch 0/1
Batch Loss = 0.0007309952143664425
1175, epoch_train_loss=0.0007309952143664425
Epoch 1176
Epoch 1176 :: Batch 0/1
Batch Loss = 0.0007309952128650687
1176, epoch_train_loss=0.0007309952128650687
Epoch 1177
Epoch 1177 :: Batch 0/1
Batch Loss = 0.0007309952113646812
1177, epoch_train_loss=0.0007309952113646812
Epoch 1178
Epoch 1178 :: Batch 0/1
Batch Loss = 0.0007309952098652795
1178, epoch_train_loss=0.0007309952098652795
Epoch 1179
Epoch 1179 :: Batch 0/1
Batch Loss = 0.0007309952083668636
1179, epoch_train_loss=0.0007309952083668636
Epoch 1180
Epoch 1180 :: Batch 0/1
Batch Loss = 0.0007309952068694335
1180, epoch_train_loss=0.0007309952068694335
Epoch 1181
Epoch 1181 :: Batch 0/1
Batch Loss = 0.0007309952053729886
1181, epoch_train_loss=0.0007309952053729886
Epoch 1182
Epoch 1182 :: Batch 0/1
Batch Loss = 0.0007309952038775288
1182, epoch_train_loss=0.0007309952038775288
Epoch 1183
Epoch 1183 :: Batch 0/1
Batch Loss = 0.0007309952023830542
1183, epoch_train_loss=0.0007309952023830542
Epoch 1184
Epoch 1184 :: Batch 0/1
Batch Loss = 0.0007309952008895645
1184, epoch_train_loss=0.0007309952008895645
Epoch 1185
Epoch 1185 :: Batch 0/1
Batch Loss = 0.0007309951993970591
1185, epoch_train_loss=0.0007309951993970591
Epoch 1186
Epoch 1186 :: Batch 0/1
Batch Loss = 0.0007309951979055385
1186, epoch_train_loss=0.0007309951979055385
Epoch 1187
Epoch 1187 :: Batch 0/1
Batch Loss = 0.0007309951964150022
1187, epoch_train_loss=0.0007309951964150022
Epoch 1188
Epoch 1188 :: Batch 0/1
Batch Loss = 0.0007309951949254497
1188, epoch_train_loss=0.0007309951949254497
Epoch 1189
Epoch 1189 :: Batch 0/1
Batch Loss = 0.0007309951934368813
1189, epoch_train_loss=0.0007309951934368813
Epoch 1190
Epoch 1190 :: Batch 0/1
Batch Loss = 0.0007309951919492963
1190, epoch_train_loss=0.0007309951919492963
Epoch 1191
Epoch 1191 :: Batch 0/1
Batch Loss = 0.000730995190462695
1191, epoch_train_loss=0.000730995190462695
Epoch 1192
Epoch 1192 :: Batch 0/1
Batch Loss = 0.0007309951889770771
1192, epoch_train_loss=0.0007309951889770771
Epoch 1193
Epoch 1193 :: Batch 0/1
Batch Loss = 0.0007309951874924419
1193, epoch_train_loss=0.0007309951874924419
Epoch 1194
Epoch 1194 :: Batch 0/1
Batch Loss = 0.0007309951860087899
1194, epoch_train_loss=0.0007309951860087899
Epoch 1195
Epoch 1195 :: Batch 0/1
Batch Loss = 0.0007309951845261202
1195, epoch_train_loss=0.0007309951845261202
Epoch 1196
Epoch 1196 :: Batch 0/1
Batch Loss = 0.0007309951830444333
1196, epoch_train_loss=0.0007309951830444333
Epoch 1197
Epoch 1197 :: Batch 0/1
Batch Loss = 0.0007309951815637286
1197, epoch_train_loss=0.0007309951815637286
Epoch 1198
Epoch 1198 :: Batch 0/1
Batch Loss = 0.0007309951800840057
1198, epoch_train_loss=0.0007309951800840057
Epoch 1199
Epoch 1199 :: Batch 0/1
Batch Loss = 0.0007309951786052646
1199, epoch_train_loss=0.0007309951786052646
Epoch 1200
Epoch 1200 :: Batch 0/1
Batch Loss = 0.0007309951771275053
1200, epoch_train_loss=0.0007309951771275053
Epoch 1201
Epoch 1201 :: Batch 0/1
Batch Loss = 0.000730995175650727
1201, epoch_train_loss=0.000730995175650727
Epoch 1202
Epoch 1202 :: Batch 0/1
Batch Loss = 0.0007309951741749302
1202, epoch_train_loss=0.0007309951741749302
Epoch 1203
Epoch 1203 :: Batch 0/1
Batch Loss = 0.0007309951727001142
1203, epoch_train_loss=0.0007309951727001142
Epoch 1204
Epoch 1204 :: Batch 0/1
Batch Loss = 0.0007309951712262787
1204, epoch_train_loss=0.0007309951712262787
Epoch 1205
Epoch 1205 :: Batch 0/1
Batch Loss = 0.0007309951697534238
1205, epoch_train_loss=0.0007309951697534238
Epoch 1206
Epoch 1206 :: Batch 0/1
Batch Loss = 0.0007309951682815492
1206, epoch_train_loss=0.0007309951682815492
Epoch 1207
Epoch 1207 :: Batch 0/1
Batch Loss = 0.0007309951668106545
1207, epoch_train_loss=0.0007309951668106545
Epoch 1208
Epoch 1208 :: Batch 0/1
Batch Loss = 0.0007309951653407395
1208, epoch_train_loss=0.0007309951653407395
Epoch 1209
Epoch 1209 :: Batch 0/1
Batch Loss = 0.0007309951638718043
1209, epoch_train_loss=0.0007309951638718043
Epoch 1210
Epoch 1210 :: Batch 0/1
Batch Loss = 0.0007309951624038481
1210, epoch_train_loss=0.0007309951624038481
Epoch 1211
Epoch 1211 :: Batch 0/1
Batch Loss = 0.0007309951609368709
1211, epoch_train_loss=0.0007309951609368709
Epoch 1212
Epoch 1212 :: Batch 0/1
Batch Loss = 0.0007309951594708726
1212, epoch_train_loss=0.0007309951594708726
Epoch 1213
Epoch 1213 :: Batch 0/1
Batch Loss = 0.0007309951580058529
1213, epoch_train_loss=0.0007309951580058529
Epoch 1214
Epoch 1214 :: Batch 0/1
Batch Loss = 0.0007309951565418115
1214, epoch_train_loss=0.0007309951565418115
Epoch 1215
Epoch 1215 :: Batch 0/1
Batch Loss = 0.0007309951550787481
1215, epoch_train_loss=0.0007309951550787481
Epoch 1216
Epoch 1216 :: Batch 0/1
Batch Loss = 0.0007309951536166627
1216, epoch_train_loss=0.0007309951536166627
Epoch 1217
Epoch 1217 :: Batch 0/1
Batch Loss = 0.0007309951521555548
1217, epoch_train_loss=0.0007309951521555548
Epoch 1218
Epoch 1218 :: Batch 0/1
Batch Loss = 0.0007309951506954241
1218, epoch_train_loss=0.0007309951506954241
Epoch 1219
Epoch 1219 :: Batch 0/1
Batch Loss = 0.0007309951492362706
1219, epoch_train_loss=0.0007309951492362706
Epoch 1220
Epoch 1220 :: Batch 0/1
Batch Loss = 0.0007309951477780937
1220, epoch_train_loss=0.0007309951477780937
Epoch 1221
Epoch 1221 :: Batch 0/1
Batch Loss = 0.0007309951463208937
1221, epoch_train_loss=0.0007309951463208937
Epoch 1222
Epoch 1222 :: Batch 0/1
Batch Loss = 0.0007309951448646699
1222, epoch_train_loss=0.0007309951448646699
Epoch 1223
Epoch 1223 :: Batch 0/1
Batch Loss = 0.0007309951434094219
1223, epoch_train_loss=0.0007309951434094219
Epoch 1224
Epoch 1224 :: Batch 0/1
Batch Loss = 0.0007309951419551498
1224, epoch_train_loss=0.0007309951419551498
Epoch 1225
Epoch 1225 :: Batch 0/1
Batch Loss = 0.0007309951405018533
1225, epoch_train_loss=0.0007309951405018533
Epoch 1226
Epoch 1226 :: Batch 0/1
Batch Loss = 0.0007309951390495319
1226, epoch_train_loss=0.0007309951390495319
Epoch 1227
Epoch 1227 :: Batch 0/1
Batch Loss = 0.0007309951375981857
1227, epoch_train_loss=0.0007309951375981857
Epoch 1228
Epoch 1228 :: Batch 0/1
Batch Loss = 0.0007309951361478141
1228, epoch_train_loss=0.0007309951361478141
Epoch 1229
Epoch 1229 :: Batch 0/1
Batch Loss = 0.0007309951346984168
1229, epoch_train_loss=0.0007309951346984168
Epoch 1230
Epoch 1230 :: Batch 0/1
Batch Loss = 0.000730995133249994
1230, epoch_train_loss=0.000730995133249994
Epoch 1231
Epoch 1231 :: Batch 0/1
Batch Loss = 0.0007309951318025448
1231, epoch_train_loss=0.0007309951318025448
Epoch 1232
Epoch 1232 :: Batch 0/1
Batch Loss = 0.0007309951303560695
1232, epoch_train_loss=0.0007309951303560695
Epoch 1233
Epoch 1233 :: Batch 0/1
Batch Loss = 0.0007309951289105675
1233, epoch_train_loss=0.0007309951289105675
Epoch 1234
Epoch 1234 :: Batch 0/1
Batch Loss = 0.0007309951274660384
1234, epoch_train_loss=0.0007309951274660384
Epoch 1235
Epoch 1235 :: Batch 0/1
Batch Loss = 0.0007309951260224822
1235, epoch_train_loss=0.0007309951260224822
Epoch 1236
Epoch 1236 :: Batch 0/1
Batch Loss = 0.0007309951245798985
1236, epoch_train_loss=0.0007309951245798985
Epoch 1237
Epoch 1237 :: Batch 0/1
Batch Loss = 0.000730995123138287
1237, epoch_train_loss=0.000730995123138287
Epoch 1238
Epoch 1238 :: Batch 0/1
Batch Loss = 0.0007309951216976474
1238, epoch_train_loss=0.0007309951216976474
Epoch 1239
Epoch 1239 :: Batch 0/1
Batch Loss = 0.0007309951202579796
1239, epoch_train_loss=0.0007309951202579796
Epoch 1240
Epoch 1240 :: Batch 0/1
Batch Loss = 0.0007309951188192831
1240, epoch_train_loss=0.0007309951188192831
Epoch 1241
Epoch 1241 :: Batch 0/1
Batch Loss = 0.0007309951173815577
1241, epoch_train_loss=0.0007309951173815577
Epoch 1242
Epoch 1242 :: Batch 0/1
Batch Loss = 0.000730995115944803
1242, epoch_train_loss=0.000730995115944803
Epoch 1243
Epoch 1243 :: Batch 0/1
Batch Loss = 0.0007309951145090188
1243, epoch_train_loss=0.0007309951145090188
Epoch 1244
Epoch 1244 :: Batch 0/1
Batch Loss = 0.000730995113074205
1244, epoch_train_loss=0.000730995113074205
Epoch 1245
Epoch 1245 :: Batch 0/1
Batch Loss = 0.0007309951116403608
1245, epoch_train_loss=0.0007309951116403608
Epoch 1246
Epoch 1246 :: Batch 0/1
Batch Loss = 0.0007309951102074864
1246, epoch_train_loss=0.0007309951102074864
Epoch 1247
Epoch 1247 :: Batch 0/1
Batch Loss = 0.0007309951087755812
1247, epoch_train_loss=0.0007309951087755812
Epoch 1248
Epoch 1248 :: Batch 0/1
Batch Loss = 0.0007309951073446452
1248, epoch_train_loss=0.0007309951073446452
Epoch 1249
Epoch 1249 :: Batch 0/1
Batch Loss = 0.0007309951059146778
1249, epoch_train_loss=0.0007309951059146778
Epoch 1250
Epoch 1250 :: Batch 0/1
Batch Loss = 0.0007309951044856786
1250, epoch_train_loss=0.0007309951044856786
Epoch 1251
Epoch 1251 :: Batch 0/1
Batch Loss = 0.0007309951030576478
1251, epoch_train_loss=0.0007309951030576478
Epoch 1252
Epoch 1252 :: Batch 0/1
Batch Loss = 0.0007309951016305847
1252, epoch_train_loss=0.0007309951016305847
Epoch 1253
Epoch 1253 :: Batch 0/1
Batch Loss = 0.0007309951002044893
1253, epoch_train_loss=0.0007309951002044893
Epoch 1254
Epoch 1254 :: Batch 0/1
Batch Loss = 0.0007309950987793607
1254, epoch_train_loss=0.0007309950987793607
Epoch 1255
Epoch 1255 :: Batch 0/1
Batch Loss = 0.0007309950973551991
1255, epoch_train_loss=0.0007309950973551991
Epoch 1256
Epoch 1256 :: Batch 0/1
Batch Loss = 0.000730995095932004
1256, epoch_train_loss=0.000730995095932004
Epoch 1257
Epoch 1257 :: Batch 0/1
Batch Loss = 0.0007309950945097752
1257, epoch_train_loss=0.0007309950945097752
Epoch 1258
Epoch 1258 :: Batch 0/1
Batch Loss = 0.0007309950930885123
1258, epoch_train_loss=0.0007309950930885123
Epoch 1259
Epoch 1259 :: Batch 0/1
Batch Loss = 0.000730995091668215
1259, epoch_train_loss=0.000730995091668215
Epoch 1260
Epoch 1260 :: Batch 0/1
Batch Loss = 0.0007309950902488831
1260, epoch_train_loss=0.0007309950902488831
Epoch 1261
Epoch 1261 :: Batch 0/1
Batch Loss = 0.0007309950888305161
1261, epoch_train_loss=0.0007309950888305161
Epoch 1262
Epoch 1262 :: Batch 0/1
Batch Loss = 0.0007309950874131138
1262, epoch_train_loss=0.0007309950874131138
Epoch 1263
Epoch 1263 :: Batch 0/1
Batch Loss = 0.0007309950859966758
1263, epoch_train_loss=0.0007309950859966758
Epoch 1264
Epoch 1264 :: Batch 0/1
Batch Loss = 0.0007309950845812018
1264, epoch_train_loss=0.0007309950845812018
Epoch 1265
Epoch 1265 :: Batch 0/1
Batch Loss = 0.0007309950831666915
1265, epoch_train_loss=0.0007309950831666915
Epoch 1266
Epoch 1266 :: Batch 0/1
Batch Loss = 0.0007309950817531445
1266, epoch_train_loss=0.0007309950817531445
Epoch 1267
Epoch 1267 :: Batch 0/1
Batch Loss = 0.0007309950803405605
1267, epoch_train_loss=0.0007309950803405605
Epoch 1268
Epoch 1268 :: Batch 0/1
Batch Loss = 0.0007309950789289392
1268, epoch_train_loss=0.0007309950789289392
Epoch 1269
Epoch 1269 :: Batch 0/1
Batch Loss = 0.0007309950775182804
1269, epoch_train_loss=0.0007309950775182804
Epoch 1270
Epoch 1270 :: Batch 0/1
Batch Loss = 0.0007309950761085834
1270, epoch_train_loss=0.0007309950761085834
Epoch 1271
Epoch 1271 :: Batch 0/1
Batch Loss = 0.0007309950746998483
1271, epoch_train_loss=0.0007309950746998483
Epoch 1272
Epoch 1272 :: Batch 0/1
Batch Loss = 0.0007309950732920745
1272, epoch_train_loss=0.0007309950732920745
Epoch 1273
Epoch 1273 :: Batch 0/1
Batch Loss = 0.0007309950718852616
1273, epoch_train_loss=0.0007309950718852616
Epoch 1274
Epoch 1274 :: Batch 0/1
Batch Loss = 0.0007309950704794095
1274, epoch_train_loss=0.0007309950704794095
Epoch 1275
Epoch 1275 :: Batch 0/1
Batch Loss = 0.0007309950690745178
1275, epoch_train_loss=0.0007309950690745178
Epoch 1276
Epoch 1276 :: Batch 0/1
Batch Loss = 0.0007309950676705863
1276, epoch_train_loss=0.0007309950676705863
Epoch 1277
Epoch 1277 :: Batch 0/1
Batch Loss = 0.0007309950662676144
1277, epoch_train_loss=0.0007309950662676144
Epoch 1278
Epoch 1278 :: Batch 0/1
Batch Loss = 0.0007309950648656016
1278, epoch_train_loss=0.0007309950648656016
Epoch 1279
Epoch 1279 :: Batch 0/1
Batch Loss = 0.0007309950634645479
1279, epoch_train_loss=0.0007309950634645479
Epoch 1280
Epoch 1280 :: Batch 0/1
Batch Loss = 0.0007309950620644528
1280, epoch_train_loss=0.0007309950620644528
Epoch 1281
Epoch 1281 :: Batch 0/1
Batch Loss = 0.000730995060665316
1281, epoch_train_loss=0.000730995060665316
Epoch 1282
Epoch 1282 :: Batch 0/1
Batch Loss = 0.0007309950592671371
1282, epoch_train_loss=0.0007309950592671371
Epoch 1283
Epoch 1283 :: Batch 0/1
Batch Loss = 0.0007309950578699159
1283, epoch_train_loss=0.0007309950578699159
Epoch 1284
Epoch 1284 :: Batch 0/1
Batch Loss = 0.0007309950564736518
1284, epoch_train_loss=0.0007309950564736518
Epoch 1285
Epoch 1285 :: Batch 0/1
Batch Loss = 0.0007309950550783448
1285, epoch_train_loss=0.0007309950550783448
Epoch 1286
Epoch 1286 :: Batch 0/1
Batch Loss = 0.0007309950536839943
1286, epoch_train_loss=0.0007309950536839943
Epoch 1287
Epoch 1287 :: Batch 0/1
Batch Loss = 0.0007309950522905998
1287, epoch_train_loss=0.0007309950522905998
Epoch 1288
Epoch 1288 :: Batch 0/1
Batch Loss = 0.0007309950508981612
1288, epoch_train_loss=0.0007309950508981612
Epoch 1289
Epoch 1289 :: Batch 0/1
Batch Loss = 0.0007309950495066782
1289, epoch_train_loss=0.0007309950495066782
Epoch 1290
Epoch 1290 :: Batch 0/1
Batch Loss = 0.0007309950481161501
1290, epoch_train_loss=0.0007309950481161501
Epoch 1291
Epoch 1291 :: Batch 0/1
Batch Loss = 0.0007309950467265768
1291, epoch_train_loss=0.0007309950467265768
Epoch 1292
Epoch 1292 :: Batch 0/1
Batch Loss = 0.000730995045337958
1292, epoch_train_loss=0.000730995045337958
Epoch 1293
Epoch 1293 :: Batch 0/1
Batch Loss = 0.0007309950439502931
1293, epoch_train_loss=0.0007309950439502931
Epoch 1294
Epoch 1294 :: Batch 0/1
Batch Loss = 0.0007309950425635821
1294, epoch_train_loss=0.0007309950425635821
Epoch 1295
Epoch 1295 :: Batch 0/1
Batch Loss = 0.0007309950411778242
1295, epoch_train_loss=0.0007309950411778242
Epoch 1296
Epoch 1296 :: Batch 0/1
Batch Loss = 0.0007309950397930194
1296, epoch_train_loss=0.0007309950397930194
Epoch 1297
Epoch 1297 :: Batch 0/1
Batch Loss = 0.0007309950384091669
1297, epoch_train_loss=0.0007309950384091669
Epoch 1298
Epoch 1298 :: Batch 0/1
Batch Loss = 0.0007309950370262667
1298, epoch_train_loss=0.0007309950370262667
Epoch 1299
Epoch 1299 :: Batch 0/1
Batch Loss = 0.0007309950356443186
1299, epoch_train_loss=0.0007309950356443186
Epoch 1300
Epoch 1300 :: Batch 0/1
Batch Loss = 0.0007309950342633218
1300, epoch_train_loss=0.0007309950342633218
Epoch 1301
Epoch 1301 :: Batch 0/1
Batch Loss = 0.0007309950328832761
1301, epoch_train_loss=0.0007309950328832761
Epoch 1302
Epoch 1302 :: Batch 0/1
Batch Loss = 0.000730995031504181
1302, epoch_train_loss=0.000730995031504181
Epoch 1303
Epoch 1303 :: Batch 0/1
Batch Loss = 0.0007309950301260363
1303, epoch_train_loss=0.0007309950301260363
Epoch 1304
Epoch 1304 :: Batch 0/1
Batch Loss = 0.0007309950287488415
1304, epoch_train_loss=0.0007309950287488415
Epoch 1305
Epoch 1305 :: Batch 0/1
Batch Loss = 0.0007309950273725965
1305, epoch_train_loss=0.0007309950273725965
Epoch 1306
Epoch 1306 :: Batch 0/1
Batch Loss = 0.0007309950259973006
1306, epoch_train_loss=0.0007309950259973006
Epoch 1307
Epoch 1307 :: Batch 0/1
Batch Loss = 0.0007309950246229536
1307, epoch_train_loss=0.0007309950246229536
Epoch 1308
Epoch 1308 :: Batch 0/1
Batch Loss = 0.000730995023249555
1308, epoch_train_loss=0.000730995023249555
Epoch 1309
Epoch 1309 :: Batch 0/1
Batch Loss = 0.0007309950218771045
1309, epoch_train_loss=0.0007309950218771045
Epoch 1310
Epoch 1310 :: Batch 0/1
Batch Loss = 0.0007309950205056019
1310, epoch_train_loss=0.0007309950205056019
Epoch 1311
Epoch 1311 :: Batch 0/1
Batch Loss = 0.0007309950191350462
1311, epoch_train_loss=0.0007309950191350462
Epoch 1312
Epoch 1312 :: Batch 0/1
Batch Loss = 0.0007309950177654375
1312, epoch_train_loss=0.0007309950177654375
Epoch 1313
Epoch 1313 :: Batch 0/1
Batch Loss = 0.0007309950163967756
1313, epoch_train_loss=0.0007309950163967756
Epoch 1314
Epoch 1314 :: Batch 0/1
Batch Loss = 0.0007309950150290597
1314, epoch_train_loss=0.0007309950150290597
Epoch 1315
Epoch 1315 :: Batch 0/1
Batch Loss = 0.0007309950136622895
1315, epoch_train_loss=0.0007309950136622895
Epoch 1316
Epoch 1316 :: Batch 0/1
Batch Loss = 0.0007309950122964649
1316, epoch_train_loss=0.0007309950122964649
Epoch 1317
Epoch 1317 :: Batch 0/1
Batch Loss = 0.000730995010931585
1317, epoch_train_loss=0.000730995010931585
Epoch 1318
Epoch 1318 :: Batch 0/1
Batch Loss = 0.00073099500956765
1318, epoch_train_loss=0.00073099500956765
Epoch 1319
Epoch 1319 :: Batch 0/1
Batch Loss = 0.0007309950082046591
1319, epoch_train_loss=0.0007309950082046591
Epoch 1320
Epoch 1320 :: Batch 0/1
Batch Loss = 0.0007309950068426119
1320, epoch_train_loss=0.0007309950068426119
Epoch 1321
Epoch 1321 :: Batch 0/1
Batch Loss = 0.0007309950054815082
1321, epoch_train_loss=0.0007309950054815082
Epoch 1322
Epoch 1322 :: Batch 0/1
Batch Loss = 0.0007309950041213474
1322, epoch_train_loss=0.0007309950041213474
Epoch 1323
Epoch 1323 :: Batch 0/1
Batch Loss = 0.0007309950027621294
1323, epoch_train_loss=0.0007309950027621294
Epoch 1324
Epoch 1324 :: Batch 0/1
Batch Loss = 0.0007309950014038535
1324, epoch_train_loss=0.0007309950014038535
Epoch 1325
Epoch 1325 :: Batch 0/1
Batch Loss = 0.0007309950000465196
1325, epoch_train_loss=0.0007309950000465196
Epoch 1326
Epoch 1326 :: Batch 0/1
Batch Loss = 0.0007309949986901269
1326, epoch_train_loss=0.0007309949986901269
Epoch 1327
Epoch 1327 :: Batch 0/1
Batch Loss = 0.0007309949973346754
1327, epoch_train_loss=0.0007309949973346754
Epoch 1328
Epoch 1328 :: Batch 0/1
Batch Loss = 0.0007309949959801644
1328, epoch_train_loss=0.0007309949959801644
Epoch 1329
Epoch 1329 :: Batch 0/1
Batch Loss = 0.0007309949946265937
1329, epoch_train_loss=0.0007309949946265937
Epoch 1330
Epoch 1330 :: Batch 0/1
Batch Loss = 0.0007309949932739628
1330, epoch_train_loss=0.0007309949932739628
Epoch 1331
Epoch 1331 :: Batch 0/1
Batch Loss = 0.0007309949919222714
1331, epoch_train_loss=0.0007309949919222714
Epoch 1332
Epoch 1332 :: Batch 0/1
Batch Loss = 0.000730994990571519
1332, epoch_train_loss=0.000730994990571519
Epoch 1333
Epoch 1333 :: Batch 0/1
Batch Loss = 0.0007309949892217052
1333, epoch_train_loss=0.0007309949892217052
Epoch 1334
Epoch 1334 :: Batch 0/1
Batch Loss = 0.0007309949878728295
1334, epoch_train_loss=0.0007309949878728295
Epoch 1335
Epoch 1335 :: Batch 0/1
Batch Loss = 0.0007309949865248916
1335, epoch_train_loss=0.0007309949865248916
Epoch 1336
Epoch 1336 :: Batch 0/1
Batch Loss = 0.0007309949851778913
1336, epoch_train_loss=0.0007309949851778913
Epoch 1337
Epoch 1337 :: Batch 0/1
Batch Loss = 0.0007309949838318277
1337, epoch_train_loss=0.0007309949838318277
Epoch 1338
Epoch 1338 :: Batch 0/1
Batch Loss = 0.0007309949824867008
1338, epoch_train_loss=0.0007309949824867008
Epoch 1339
Epoch 1339 :: Batch 0/1
Batch Loss = 0.0007309949811425101
1339, epoch_train_loss=0.0007309949811425101
Epoch 1340
Epoch 1340 :: Batch 0/1
Batch Loss = 0.0007309949797992552
1340, epoch_train_loss=0.0007309949797992552
Epoch 1341
Epoch 1341 :: Batch 0/1
Batch Loss = 0.0007309949784569355
1341, epoch_train_loss=0.0007309949784569355
Epoch 1342
Epoch 1342 :: Batch 0/1
Batch Loss = 0.0007309949771155506
1342, epoch_train_loss=0.0007309949771155506
Epoch 1343
Epoch 1343 :: Batch 0/1
Batch Loss = 0.0007309949757751004
1343, epoch_train_loss=0.0007309949757751004
Epoch 1344
Epoch 1344 :: Batch 0/1
Batch Loss = 0.000730994974435584
1344, epoch_train_loss=0.000730994974435584
Epoch 1345
Epoch 1345 :: Batch 0/1
Batch Loss = 0.0007309949730970015
1345, epoch_train_loss=0.0007309949730970015
Epoch 1346
Epoch 1346 :: Batch 0/1
Batch Loss = 0.0007309949717593521
1346, epoch_train_loss=0.0007309949717593521
Epoch 1347
Epoch 1347 :: Batch 0/1
Batch Loss = 0.0007309949704226356
1347, epoch_train_loss=0.0007309949704226356
Epoch 1348
Epoch 1348 :: Batch 0/1
Batch Loss = 0.0007309949690868515
1348, epoch_train_loss=0.0007309949690868515
Epoch 1349
Epoch 1349 :: Batch 0/1
Batch Loss = 0.0007309949677519992
1349, epoch_train_loss=0.0007309949677519992
Epoch 1350
Epoch 1350 :: Batch 0/1
Batch Loss = 0.0007309949664180786
1350, epoch_train_loss=0.0007309949664180786
Epoch 1351
Epoch 1351 :: Batch 0/1
Batch Loss = 0.000730994965085089
1351, epoch_train_loss=0.000730994965085089
Epoch 1352
Epoch 1352 :: Batch 0/1
Batch Loss = 0.0007309949637530303
1352, epoch_train_loss=0.0007309949637530303
Epoch 1353
Epoch 1353 :: Batch 0/1
Batch Loss = 0.0007309949624219016
1353, epoch_train_loss=0.0007309949624219016
Epoch 1354
Epoch 1354 :: Batch 0/1
Batch Loss = 0.0007309949610917031
1354, epoch_train_loss=0.0007309949610917031
Epoch 1355
Epoch 1355 :: Batch 0/1
Batch Loss = 0.0007309949597624337
1355, epoch_train_loss=0.0007309949597624337
Epoch 1356
Epoch 1356 :: Batch 0/1
Batch Loss = 0.0007309949584340934
1356, epoch_train_loss=0.0007309949584340934
Epoch 1357
Epoch 1357 :: Batch 0/1
Batch Loss = 0.0007309949571066817
1357, epoch_train_loss=0.0007309949571066817
Epoch 1358
Epoch 1358 :: Batch 0/1
Batch Loss = 0.000730994955780198
1358, epoch_train_loss=0.000730994955780198
Epoch 1359
Epoch 1359 :: Batch 0/1
Batch Loss = 0.0007309949544546421
1359, epoch_train_loss=0.0007309949544546421
Epoch 1360
Epoch 1360 :: Batch 0/1
Batch Loss = 0.0007309949531300135
1360, epoch_train_loss=0.0007309949531300135
Epoch 1361
Epoch 1361 :: Batch 0/1
Batch Loss = 0.0007309949518063116
1361, epoch_train_loss=0.0007309949518063116
Epoch 1362
Epoch 1362 :: Batch 0/1
Batch Loss = 0.0007309949504835363
1362, epoch_train_loss=0.0007309949504835363
Epoch 1363
Epoch 1363 :: Batch 0/1
Batch Loss = 0.0007309949491616867
1363, epoch_train_loss=0.0007309949491616867
Epoch 1364
Epoch 1364 :: Batch 0/1
Batch Loss = 0.0007309949478407628
1364, epoch_train_loss=0.0007309949478407628
Epoch 1365
Epoch 1365 :: Batch 0/1
Batch Loss = 0.0007309949465207641
1365, epoch_train_loss=0.0007309949465207641
Epoch 1366
Epoch 1366 :: Batch 0/1
Batch Loss = 0.0007309949452016899
1366, epoch_train_loss=0.0007309949452016899
Epoch 1367
Epoch 1367 :: Batch 0/1
Batch Loss = 0.00073099494388354
1367, epoch_train_loss=0.00073099494388354
Epoch 1368
Epoch 1368 :: Batch 0/1
Batch Loss = 0.0007309949425663137
1368, epoch_train_loss=0.0007309949425663137
Epoch 1369
Epoch 1369 :: Batch 0/1
Batch Loss = 0.000730994941250011
1369, epoch_train_loss=0.000730994941250011
Epoch 1370
Epoch 1370 :: Batch 0/1
Batch Loss = 0.0007309949399346311
1370, epoch_train_loss=0.0007309949399346311
Epoch 1371
Epoch 1371 :: Batch 0/1
Batch Loss = 0.0007309949386201736
1371, epoch_train_loss=0.0007309949386201736
Epoch 1372
Epoch 1372 :: Batch 0/1
Batch Loss = 0.0007309949373066381
1372, epoch_train_loss=0.0007309949373066381
Epoch 1373
Epoch 1373 :: Batch 0/1
Batch Loss = 0.0007309949359940244
1373, epoch_train_loss=0.0007309949359940244
Epoch 1374
Epoch 1374 :: Batch 0/1
Batch Loss = 0.0007309949346823315
1374, epoch_train_loss=0.0007309949346823315
Epoch 1375
Epoch 1375 :: Batch 0/1
Batch Loss = 0.0007309949333715596
1375, epoch_train_loss=0.0007309949333715596
Epoch 1376
Epoch 1376 :: Batch 0/1
Batch Loss = 0.0007309949320617077
1376, epoch_train_loss=0.0007309949320617077
Epoch 1377
Epoch 1377 :: Batch 0/1
Batch Loss = 0.0007309949307527757
1377, epoch_train_loss=0.0007309949307527757
Epoch 1378
Epoch 1378 :: Batch 0/1
Batch Loss = 0.000730994929444763
1378, epoch_train_loss=0.000730994929444763
Epoch 1379
Epoch 1379 :: Batch 0/1
Batch Loss = 0.0007309949281376693
1379, epoch_train_loss=0.0007309949281376693
Epoch 1380
Epoch 1380 :: Batch 0/1
Batch Loss = 0.000730994926831494
1380, epoch_train_loss=0.000730994926831494
Epoch 1381
Epoch 1381 :: Batch 0/1
Batch Loss = 0.0007309949255262368
1381, epoch_train_loss=0.0007309949255262368
Epoch 1382
Epoch 1382 :: Batch 0/1
Batch Loss = 0.000730994924221897
1382, epoch_train_loss=0.000730994924221897
Epoch 1383
Epoch 1383 :: Batch 0/1
Batch Loss = 0.0007309949229184744
1383, epoch_train_loss=0.0007309949229184744
Epoch 1384
Epoch 1384 :: Batch 0/1
Batch Loss = 0.0007309949216159684
1384, epoch_train_loss=0.0007309949216159684
Epoch 1385
Epoch 1385 :: Batch 0/1
Batch Loss = 0.0007309949203143785
1385, epoch_train_loss=0.0007309949203143785
Epoch 1386
Epoch 1386 :: Batch 0/1
Batch Loss = 0.0007309949190137047
1386, epoch_train_loss=0.0007309949190137047
Epoch 1387
Epoch 1387 :: Batch 0/1
Batch Loss = 0.0007309949177139461
1387, epoch_train_loss=0.0007309949177139461
Epoch 1388
Epoch 1388 :: Batch 0/1
Batch Loss = 0.0007309949164151021
1388, epoch_train_loss=0.0007309949164151021
Epoch 1389
Epoch 1389 :: Batch 0/1
Batch Loss = 0.0007309949151171726
1389, epoch_train_loss=0.0007309949151171726
Epoch 1390
Epoch 1390 :: Batch 0/1
Batch Loss = 0.0007309949138201571
1390, epoch_train_loss=0.0007309949138201571
Epoch 1391
Epoch 1391 :: Batch 0/1
Batch Loss = 0.0007309949125240549
1391, epoch_train_loss=0.0007309949125240549
Epoch 1392
Epoch 1392 :: Batch 0/1
Batch Loss = 0.000730994911228866
1392, epoch_train_loss=0.000730994911228866
Epoch 1393
Epoch 1393 :: Batch 0/1
Batch Loss = 0.0007309949099345894
1393, epoch_train_loss=0.0007309949099345894
Epoch 1394
Epoch 1394 :: Batch 0/1
Batch Loss = 0.0007309949086412249
1394, epoch_train_loss=0.0007309949086412249
Epoch 1395
Epoch 1395 :: Batch 0/1
Batch Loss = 0.000730994907348772
1395, epoch_train_loss=0.000730994907348772
Epoch 1396
Epoch 1396 :: Batch 0/1
Batch Loss = 0.0007309949060572304
1396, epoch_train_loss=0.0007309949060572304
Epoch 1397
Epoch 1397 :: Batch 0/1
Batch Loss = 0.0007309949047665995
1397, epoch_train_loss=0.0007309949047665995
Epoch 1398
Epoch 1398 :: Batch 0/1
Batch Loss = 0.0007309949034768789
1398, epoch_train_loss=0.0007309949034768789
Epoch 1399
Epoch 1399 :: Batch 0/1
Batch Loss = 0.000730994902188068
1399, epoch_train_loss=0.000730994902188068
Epoch 1400
Epoch 1400 :: Batch 0/1
Batch Loss = 0.0007309949009001666
1400, epoch_train_loss=0.0007309949009001666
Epoch 1401
Epoch 1401 :: Batch 0/1
Batch Loss = 0.0007309948996131739
1401, epoch_train_loss=0.0007309948996131739
Epoch 1402
Epoch 1402 :: Batch 0/1
Batch Loss = 0.0007309948983270897
1402, epoch_train_loss=0.0007309948983270897
Epoch 1403
Epoch 1403 :: Batch 0/1
Batch Loss = 0.0007309948970419134
1403, epoch_train_loss=0.0007309948970419134
Epoch 1404
Epoch 1404 :: Batch 0/1
Batch Loss = 0.0007309948957576446
1404, epoch_train_loss=0.0007309948957576446
Epoch 1405
Epoch 1405 :: Batch 0/1
Batch Loss = 0.0007309948944742828
1405, epoch_train_loss=0.0007309948944742828
Epoch 1406
Epoch 1406 :: Batch 0/1
Batch Loss = 0.0007309948931918275
1406, epoch_train_loss=0.0007309948931918275
Epoch 1407
Epoch 1407 :: Batch 0/1
Batch Loss = 0.0007309948919102783
1407, epoch_train_loss=0.0007309948919102783
Epoch 1408
Epoch 1408 :: Batch 0/1
Batch Loss = 0.0007309948906296347
1408, epoch_train_loss=0.0007309948906296347
Epoch 1409
Epoch 1409 :: Batch 0/1
Batch Loss = 0.0007309948893498961
1409, epoch_train_loss=0.0007309948893498961
Epoch 1410
Epoch 1410 :: Batch 0/1
Batch Loss = 0.0007309948880710624
1410, epoch_train_loss=0.0007309948880710624
Epoch 1411
Epoch 1411 :: Batch 0/1
Batch Loss = 0.0007309948867931329
1411, epoch_train_loss=0.0007309948867931329
Epoch 1412
Epoch 1412 :: Batch 0/1
Batch Loss = 0.000730994885516107
1412, epoch_train_loss=0.000730994885516107
Epoch 1413
Epoch 1413 :: Batch 0/1
Batch Loss = 0.0007309948842399843
1413, epoch_train_loss=0.0007309948842399843
Epoch 1414
Epoch 1414 :: Batch 0/1
Batch Loss = 0.0007309948829647643
1414, epoch_train_loss=0.0007309948829647643
Epoch 1415
Epoch 1415 :: Batch 0/1
Batch Loss = 0.0007309948816904469
1415, epoch_train_loss=0.0007309948816904469
Epoch 1416
Epoch 1416 :: Batch 0/1
Batch Loss = 0.0007309948804170312
1416, epoch_train_loss=0.0007309948804170312
Epoch 1417
Epoch 1417 :: Batch 0/1
Batch Loss = 0.0007309948791445167
1417, epoch_train_loss=0.0007309948791445167
Epoch 1418
Epoch 1418 :: Batch 0/1
Batch Loss = 0.0007309948778729034
1418, epoch_train_loss=0.0007309948778729034
Epoch 1419
Epoch 1419 :: Batch 0/1
Batch Loss = 0.0007309948766021903
1419, epoch_train_loss=0.0007309948766021903
Epoch 1420
Epoch 1420 :: Batch 0/1
Batch Loss = 0.0007309948753323771
1420, epoch_train_loss=0.0007309948753323771
Epoch 1421
Epoch 1421 :: Batch 0/1
Batch Loss = 0.0007309948740634634
1421, epoch_train_loss=0.0007309948740634634
Epoch 1422
Epoch 1422 :: Batch 0/1
Batch Loss = 0.0007309948727954488
1422, epoch_train_loss=0.0007309948727954488
Epoch 1423
Epoch 1423 :: Batch 0/1
Batch Loss = 0.0007309948715283326
1423, epoch_train_loss=0.0007309948715283326
Epoch 1424
Epoch 1424 :: Batch 0/1
Batch Loss = 0.0007309948702621144
1424, epoch_train_loss=0.0007309948702621144
Epoch 1425
Epoch 1425 :: Batch 0/1
Batch Loss = 0.0007309948689967938
1425, epoch_train_loss=0.0007309948689967938
Epoch 1426
Epoch 1426 :: Batch 0/1
Batch Loss = 0.0007309948677323702
1426, epoch_train_loss=0.0007309948677323702
Epoch 1427
Epoch 1427 :: Batch 0/1
Batch Loss = 0.0007309948664688433
1427, epoch_train_loss=0.0007309948664688433
Epoch 1428
Epoch 1428 :: Batch 0/1
Batch Loss = 0.0007309948652062123
1428, epoch_train_loss=0.0007309948652062123
Epoch 1429
Epoch 1429 :: Batch 0/1
Batch Loss = 0.0007309948639444772
1429, epoch_train_loss=0.0007309948639444772
Epoch 1430
Epoch 1430 :: Batch 0/1
Batch Loss = 0.000730994862683637
1430, epoch_train_loss=0.000730994862683637
Epoch 1431
Epoch 1431 :: Batch 0/1
Batch Loss = 0.0007309948614236915
1431, epoch_train_loss=0.0007309948614236915
Epoch 1432
Epoch 1432 :: Batch 0/1
Batch Loss = 0.0007309948601646403
1432, epoch_train_loss=0.0007309948601646403
Epoch 1433
Epoch 1433 :: Batch 0/1
Batch Loss = 0.0007309948589064826
1433, epoch_train_loss=0.0007309948589064826
Epoch 1434
Epoch 1434 :: Batch 0/1
Batch Loss = 0.0007309948576492184
1434, epoch_train_loss=0.0007309948576492184
Epoch 1435
Epoch 1435 :: Batch 0/1
Batch Loss = 0.0007309948563928467
1435, epoch_train_loss=0.0007309948563928467
Epoch 1436
Epoch 1436 :: Batch 0/1
Batch Loss = 0.0007309948551373674
1436, epoch_train_loss=0.0007309948551373674
Epoch 1437
Epoch 1437 :: Batch 0/1
Batch Loss = 0.0007309948538827797
1437, epoch_train_loss=0.0007309948538827797
Epoch 1438
Epoch 1438 :: Batch 0/1
Batch Loss = 0.0007309948526290832
1438, epoch_train_loss=0.0007309948526290832
Epoch 1439
Epoch 1439 :: Batch 0/1
Batch Loss = 0.0007309948513762776
1439, epoch_train_loss=0.0007309948513762776
Epoch 1440
Epoch 1440 :: Batch 0/1
Batch Loss = 0.0007309948501243622
1440, epoch_train_loss=0.0007309948501243622
Epoch 1441
Epoch 1441 :: Batch 0/1
Batch Loss = 0.0007309948488733368
1441, epoch_train_loss=0.0007309948488733368
Epoch 1442
Epoch 1442 :: Batch 0/1
Batch Loss = 0.0007309948476232006
1442, epoch_train_loss=0.0007309948476232006
Epoch 1443
Epoch 1443 :: Batch 0/1
Batch Loss = 0.0007309948463739531
1443, epoch_train_loss=0.0007309948463739531
Epoch 1444
Epoch 1444 :: Batch 0/1
Batch Loss = 0.000730994845125594
1444, epoch_train_loss=0.000730994845125594
Epoch 1445
Epoch 1445 :: Batch 0/1
Batch Loss = 0.0007309948438781227
1445, epoch_train_loss=0.0007309948438781227
Epoch 1446
Epoch 1446 :: Batch 0/1
Batch Loss = 0.000730994842631539
1446, epoch_train_loss=0.000730994842631539
Epoch 1447
Epoch 1447 :: Batch 0/1
Batch Loss = 0.0007309948413858421
1447, epoch_train_loss=0.0007309948413858421
Epoch 1448
Epoch 1448 :: Batch 0/1
Batch Loss = 0.0007309948401410314
1448, epoch_train_loss=0.0007309948401410314
Epoch 1449
Epoch 1449 :: Batch 0/1
Batch Loss = 0.0007309948388971068
1449, epoch_train_loss=0.0007309948388971068
Epoch 1450
Epoch 1450 :: Batch 0/1
Batch Loss = 0.0007309948376540675
1450, epoch_train_loss=0.0007309948376540675
Epoch 1451
Epoch 1451 :: Batch 0/1
Batch Loss = 0.0007309948364119131
1451, epoch_train_loss=0.0007309948364119131
Epoch 1452
Epoch 1452 :: Batch 0/1
Batch Loss = 0.0007309948351706429
1452, epoch_train_loss=0.0007309948351706429
Epoch 1453
Epoch 1453 :: Batch 0/1
Batch Loss = 0.0007309948339302568
1453, epoch_train_loss=0.0007309948339302568
Epoch 1454
Epoch 1454 :: Batch 0/1
Batch Loss = 0.000730994832690754
1454, epoch_train_loss=0.000730994832690754
Epoch 1455
Epoch 1455 :: Batch 0/1
Batch Loss = 0.0007309948314521345
1455, epoch_train_loss=0.0007309948314521345
Epoch 1456
Epoch 1456 :: Batch 0/1
Batch Loss = 0.000730994830214397
1456, epoch_train_loss=0.000730994830214397
Epoch 1457
Epoch 1457 :: Batch 0/1
Batch Loss = 0.0007309948289775413
1457, epoch_train_loss=0.0007309948289775413
Epoch 1458
Epoch 1458 :: Batch 0/1
Batch Loss = 0.0007309948277415675
1458, epoch_train_loss=0.0007309948277415675
Epoch 1459
Epoch 1459 :: Batch 0/1
Batch Loss = 0.0007309948265064744
1459, epoch_train_loss=0.0007309948265064744
Epoch 1460
Epoch 1460 :: Batch 0/1
Batch Loss = 0.0007309948252722619
1460, epoch_train_loss=0.0007309948252722619
Epoch 1461
Epoch 1461 :: Batch 0/1
Batch Loss = 0.0007309948240389291
1461, epoch_train_loss=0.0007309948240389291
Epoch 1462
Epoch 1462 :: Batch 0/1
Batch Loss = 0.0007309948228064758
1462, epoch_train_loss=0.0007309948228064758
Epoch 1463
Epoch 1463 :: Batch 0/1
Batch Loss = 0.0007309948215749015
1463, epoch_train_loss=0.0007309948215749015
Epoch 1464
Epoch 1464 :: Batch 0/1
Batch Loss = 0.0007309948203442058
1464, epoch_train_loss=0.0007309948203442058
Epoch 1465
Epoch 1465 :: Batch 0/1
Batch Loss = 0.0007309948191143879
1465, epoch_train_loss=0.0007309948191143879
Epoch 1466
Epoch 1466 :: Batch 0/1
Batch Loss = 0.0007309948178854474
1466, epoch_train_loss=0.0007309948178854474
Epoch 1467
Epoch 1467 :: Batch 0/1
Batch Loss = 0.0007309948166573841
1467, epoch_train_loss=0.0007309948166573841
Epoch 1468
Epoch 1468 :: Batch 0/1
Batch Loss = 0.0007309948154301969
1468, epoch_train_loss=0.0007309948154301969
Epoch 1469
Epoch 1469 :: Batch 0/1
Batch Loss = 0.0007309948142038857
1469, epoch_train_loss=0.0007309948142038857
Epoch 1470
Epoch 1470 :: Batch 0/1
Batch Loss = 0.00073099481297845
1470, epoch_train_loss=0.00073099481297845
Epoch 1471
Epoch 1471 :: Batch 0/1
Batch Loss = 0.0007309948117538893
1471, epoch_train_loss=0.0007309948117538893
Epoch 1472
Epoch 1472 :: Batch 0/1
Batch Loss = 0.0007309948105302029
1472, epoch_train_loss=0.0007309948105302029
Epoch 1473
Epoch 1473 :: Batch 0/1
Batch Loss = 0.0007309948093073905
1473, epoch_train_loss=0.0007309948093073905
Epoch 1474
Epoch 1474 :: Batch 0/1
Batch Loss = 0.0007309948080854515
1474, epoch_train_loss=0.0007309948080854515
Epoch 1475
Epoch 1475 :: Batch 0/1
Batch Loss = 0.0007309948068643854
1475, epoch_train_loss=0.0007309948068643854
Epoch 1476
Epoch 1476 :: Batch 0/1
Batch Loss = 0.0007309948056441919
1476, epoch_train_loss=0.0007309948056441919
Epoch 1477
Epoch 1477 :: Batch 0/1
Batch Loss = 0.0007309948044248704
1477, epoch_train_loss=0.0007309948044248704
Epoch 1478
Epoch 1478 :: Batch 0/1
Batch Loss = 0.0007309948032064199
1478, epoch_train_loss=0.0007309948032064199
Epoch 1479
Epoch 1479 :: Batch 0/1
Batch Loss = 0.0007309948019888406
1479, epoch_train_loss=0.0007309948019888406
Epoch 1480
Epoch 1480 :: Batch 0/1
Batch Loss = 0.0007309948007721315
1480, epoch_train_loss=0.0007309948007721315
Epoch 1481
Epoch 1481 :: Batch 0/1
Batch Loss = 0.0007309947995562925
1481, epoch_train_loss=0.0007309947995562925
Epoch 1482
Epoch 1482 :: Batch 0/1
Batch Loss = 0.0007309947983413227
1482, epoch_train_loss=0.0007309947983413227
Epoch 1483
Epoch 1483 :: Batch 0/1
Batch Loss = 0.0007309947971272218
1483, epoch_train_loss=0.0007309947971272218
Epoch 1484
Epoch 1484 :: Batch 0/1
Batch Loss = 0.0007309947959139893
1484, epoch_train_loss=0.0007309947959139893
Epoch 1485
Epoch 1485 :: Batch 0/1
Batch Loss = 0.0007309947947016247
1485, epoch_train_loss=0.0007309947947016247
Epoch 1486
Epoch 1486 :: Batch 0/1
Batch Loss = 0.0007309947934901274
1486, epoch_train_loss=0.0007309947934901274
Epoch 1487
Epoch 1487 :: Batch 0/1
Batch Loss = 0.000730994792279497
1487, epoch_train_loss=0.000730994792279497
Epoch 1488
Epoch 1488 :: Batch 0/1
Batch Loss = 0.000730994791069733
1488, epoch_train_loss=0.000730994791069733
Epoch 1489
Epoch 1489 :: Batch 0/1
Batch Loss = 0.0007309947898608348
1489, epoch_train_loss=0.0007309947898608348
Epoch 1490
Epoch 1490 :: Batch 0/1
Batch Loss = 0.0007309947886528019
1490, epoch_train_loss=0.0007309947886528019
Epoch 1491
Epoch 1491 :: Batch 0/1
Batch Loss = 0.0007309947874456339
1491, epoch_train_loss=0.0007309947874456339
Epoch 1492
Epoch 1492 :: Batch 0/1
Batch Loss = 0.00073099478623933
1492, epoch_train_loss=0.00073099478623933
Epoch 1493
Epoch 1493 :: Batch 0/1
Batch Loss = 0.0007309947850338899
1493, epoch_train_loss=0.0007309947850338899
Epoch 1494
Epoch 1494 :: Batch 0/1
Batch Loss = 0.0007309947838293132
1494, epoch_train_loss=0.0007309947838293132
Epoch 1495
Epoch 1495 :: Batch 0/1
Batch Loss = 0.0007309947826255992
1495, epoch_train_loss=0.0007309947826255992
Epoch 1496
Epoch 1496 :: Batch 0/1
Batch Loss = 0.0007309947814227474
1496, epoch_train_loss=0.0007309947814227474
Epoch 1497
Epoch 1497 :: Batch 0/1
Batch Loss = 0.0007309947802207575
1497, epoch_train_loss=0.0007309947802207575
Epoch 1498
Epoch 1498 :: Batch 0/1
Batch Loss = 0.0007309947790196288
1498, epoch_train_loss=0.0007309947790196288
Epoch 1499
Epoch 1499 :: Batch 0/1
Batch Loss = 0.0007309947778193607
1499, epoch_train_loss=0.0007309947778193607
Epoch 1500
Epoch 1500 :: Batch 0/1
Batch Loss = 0.0007309947766199528
1500, epoch_train_loss=0.0007309947766199528
Epoch 1501
Epoch 1501 :: Batch 0/1
Batch Loss = 0.0007309947754214046
1501, epoch_train_loss=0.0007309947754214046
Epoch 1502
Epoch 1502 :: Batch 0/1
Batch Loss = 0.0007309947742237156
1502, epoch_train_loss=0.0007309947742237156
Epoch 1503
Epoch 1503 :: Batch 0/1
Batch Loss = 0.0007309947730268855
1503, epoch_train_loss=0.0007309947730268855
Epoch 1504
Epoch 1504 :: Batch 0/1
Batch Loss = 0.0007309947718309133
1504, epoch_train_loss=0.0007309947718309133
Epoch 1505
Epoch 1505 :: Batch 0/1
Batch Loss = 0.0007309947706357987
1505, epoch_train_loss=0.0007309947706357987
Epoch 1506
Epoch 1506 :: Batch 0/1
Batch Loss = 0.0007309947694415411
1506, epoch_train_loss=0.0007309947694415411
Epoch 1507
Epoch 1507 :: Batch 0/1
Batch Loss = 0.0007309947682481405
1507, epoch_train_loss=0.0007309947682481405
Epoch 1508
Epoch 1508 :: Batch 0/1
Batch Loss = 0.0007309947670555958
1508, epoch_train_loss=0.0007309947670555958
Epoch 1509
Epoch 1509 :: Batch 0/1
Batch Loss = 0.0007309947658639067
1509, epoch_train_loss=0.0007309947658639067
Epoch 1510
Epoch 1510 :: Batch 0/1
Batch Loss = 0.0007309947646730724
1510, epoch_train_loss=0.0007309947646730724
Epoch 1511
Epoch 1511 :: Batch 0/1
Batch Loss = 0.0007309947634830928
1511, epoch_train_loss=0.0007309947634830928
Epoch 1512
Epoch 1512 :: Batch 0/1
Batch Loss = 0.0007309947622939673
1512, epoch_train_loss=0.0007309947622939673
Epoch 1513
Epoch 1513 :: Batch 0/1
Batch Loss = 0.0007309947611056954
1513, epoch_train_loss=0.0007309947611056954
Epoch 1514
Epoch 1514 :: Batch 0/1
Batch Loss = 0.0007309947599182762
1514, epoch_train_loss=0.0007309947599182762
Epoch 1515
Epoch 1515 :: Batch 0/1
Batch Loss = 0.0007309947587317097
1515, epoch_train_loss=0.0007309947587317097
Epoch 1516
Epoch 1516 :: Batch 0/1
Batch Loss = 0.000730994757545995
1516, epoch_train_loss=0.000730994757545995
Epoch 1517
Epoch 1517 :: Batch 0/1
Batch Loss = 0.0007309947563611318
1517, epoch_train_loss=0.0007309947563611318
Epoch 1518
Epoch 1518 :: Batch 0/1
Batch Loss = 0.0007309947551771195
1518, epoch_train_loss=0.0007309947551771195
Epoch 1519
Epoch 1519 :: Batch 0/1
Batch Loss = 0.0007309947539939575
1519, epoch_train_loss=0.0007309947539939575
Epoch 1520
Epoch 1520 :: Batch 0/1
Batch Loss = 0.0007309947528116456
1520, epoch_train_loss=0.0007309947528116456
Epoch 1521
Epoch 1521 :: Batch 0/1
Batch Loss = 0.0007309947516301828
1521, epoch_train_loss=0.0007309947516301828
Epoch 1522
Epoch 1522 :: Batch 0/1
Batch Loss = 0.000730994750449569
1522, epoch_train_loss=0.000730994750449569
Epoch 1523
Epoch 1523 :: Batch 0/1
Batch Loss = 0.0007309947492698034
1523, epoch_train_loss=0.0007309947492698034
Epoch 1524
Epoch 1524 :: Batch 0/1
Batch Loss = 0.0007309947480908857
1524, epoch_train_loss=0.0007309947480908857
Epoch 1525
Epoch 1525 :: Batch 0/1
Batch Loss = 0.0007309947469128152
1525, epoch_train_loss=0.0007309947469128152
Epoch 1526
Epoch 1526 :: Batch 0/1
Batch Loss = 0.0007309947457355917
1526, epoch_train_loss=0.0007309947457355917
Epoch 1527
Epoch 1527 :: Batch 0/1
Batch Loss = 0.0007309947445592141
1527, epoch_train_loss=0.0007309947445592141
Epoch 1528
Epoch 1528 :: Batch 0/1
Batch Loss = 0.0007309947433836822
1528, epoch_train_loss=0.0007309947433836822
Epoch 1529
Epoch 1529 :: Batch 0/1
Batch Loss = 0.0007309947422089955
1529, epoch_train_loss=0.0007309947422089955
Epoch 1530
Epoch 1530 :: Batch 0/1
Batch Loss = 0.0007309947410351536
1530, epoch_train_loss=0.0007309947410351536
Epoch 1531
Epoch 1531 :: Batch 0/1
Batch Loss = 0.0007309947398621558
1531, epoch_train_loss=0.0007309947398621558
Epoch 1532
Epoch 1532 :: Batch 0/1
Batch Loss = 0.0007309947386900014
1532, epoch_train_loss=0.0007309947386900014
Epoch 1533
Epoch 1533 :: Batch 0/1
Batch Loss = 0.0007309947375186905
1533, epoch_train_loss=0.0007309947375186905
Epoch 1534
Epoch 1534 :: Batch 0/1
Batch Loss = 0.000730994736348222
1534, epoch_train_loss=0.000730994736348222
Epoch 1535
Epoch 1535 :: Batch 0/1
Batch Loss = 0.0007309947351785954
1535, epoch_train_loss=0.0007309947351785954
Epoch 1536
Epoch 1536 :: Batch 0/1
Batch Loss = 0.0007309947340098105
1536, epoch_train_loss=0.0007309947340098105
Epoch 1537
Epoch 1537 :: Batch 0/1
Batch Loss = 0.0007309947328418666
1537, epoch_train_loss=0.0007309947328418666
Epoch 1538
Epoch 1538 :: Batch 0/1
Batch Loss = 0.0007309947316747632
1538, epoch_train_loss=0.0007309947316747632
Epoch 1539
Epoch 1539 :: Batch 0/1
Batch Loss = 0.0007309947305084997
1539, epoch_train_loss=0.0007309947305084997
Epoch 1540
Epoch 1540 :: Batch 0/1
Batch Loss = 0.0007309947293430756
1540, epoch_train_loss=0.0007309947293430756
Epoch 1541
Epoch 1541 :: Batch 0/1
Batch Loss = 0.0007309947281784905
1541, epoch_train_loss=0.0007309947281784905
Epoch 1542
Epoch 1542 :: Batch 0/1
Batch Loss = 0.0007309947270147437
1542, epoch_train_loss=0.0007309947270147437
Epoch 1543
Epoch 1543 :: Batch 0/1
Batch Loss = 0.0007309947258518347
1543, epoch_train_loss=0.0007309947258518347
Epoch 1544
Epoch 1544 :: Batch 0/1
Batch Loss = 0.0007309947246897632
1544, epoch_train_loss=0.0007309947246897632
Epoch 1545
Epoch 1545 :: Batch 0/1
Batch Loss = 0.0007309947235285283
1545, epoch_train_loss=0.0007309947235285283
Epoch 1546
Epoch 1546 :: Batch 0/1
Batch Loss = 0.0007309947223681296
1546, epoch_train_loss=0.0007309947223681296
Epoch 1547
Epoch 1547 :: Batch 0/1
Batch Loss = 0.0007309947212085669
1547, epoch_train_loss=0.0007309947212085669
Epoch 1548
Epoch 1548 :: Batch 0/1
Batch Loss = 0.0007309947200498395
1548, epoch_train_loss=0.0007309947200498395
Epoch 1549
Epoch 1549 :: Batch 0/1
Batch Loss = 0.0007309947188919466
1549, epoch_train_loss=0.0007309947188919466
Epoch 1550
Epoch 1550 :: Batch 0/1
Batch Loss = 0.0007309947177348879
1550, epoch_train_loss=0.0007309947177348879
Epoch 1551
Epoch 1551 :: Batch 0/1
Batch Loss = 0.000730994716578663
1551, epoch_train_loss=0.000730994716578663
Epoch 1552
Epoch 1552 :: Batch 0/1
Batch Loss = 0.0007309947154232712
1552, epoch_train_loss=0.0007309947154232712
Epoch 1553
Epoch 1553 :: Batch 0/1
Batch Loss = 0.0007309947142687118
1553, epoch_train_loss=0.0007309947142687118
Epoch 1554
Epoch 1554 :: Batch 0/1
Batch Loss = 0.0007309947131149846
1554, epoch_train_loss=0.0007309947131149846
Epoch 1555
Epoch 1555 :: Batch 0/1
Batch Loss = 0.0007309947119620889
1555, epoch_train_loss=0.0007309947119620889
Epoch 1556
Epoch 1556 :: Batch 0/1
Batch Loss = 0.0007309947108100241
1556, epoch_train_loss=0.0007309947108100241
Epoch 1557
Epoch 1557 :: Batch 0/1
Batch Loss = 0.0007309947096587901
1557, epoch_train_loss=0.0007309947096587901
Epoch 1558
Epoch 1558 :: Batch 0/1
Batch Loss = 0.0007309947085083858
1558, epoch_train_loss=0.0007309947085083858
Epoch 1559
Epoch 1559 :: Batch 0/1
Batch Loss = 0.000730994707358811
1559, epoch_train_loss=0.000730994707358811
Epoch 1560
Epoch 1560 :: Batch 0/1
Batch Loss = 0.000730994706210065
1560, epoch_train_loss=0.000730994706210065
Epoch 1561
Epoch 1561 :: Batch 0/1
Batch Loss = 0.0007309947050621473
1561, epoch_train_loss=0.0007309947050621473
Epoch 1562
Epoch 1562 :: Batch 0/1
Batch Loss = 0.0007309947039150575
1562, epoch_train_loss=0.0007309947039150575
Epoch 1563
Epoch 1563 :: Batch 0/1
Batch Loss = 0.0007309947027687953
1563, epoch_train_loss=0.0007309947027687953
Epoch 1564
Epoch 1564 :: Batch 0/1
Batch Loss = 0.0007309947016233597
1564, epoch_train_loss=0.0007309947016233597
Epoch 1565
Epoch 1565 :: Batch 0/1
Batch Loss = 0.0007309947004787503
1565, epoch_train_loss=0.0007309947004787503
Epoch 1566
Epoch 1566 :: Batch 0/1
Batch Loss = 0.0007309946993349668
1566, epoch_train_loss=0.0007309946993349668
Epoch 1567
Epoch 1567 :: Batch 0/1
Batch Loss = 0.0007309946981920082
1567, epoch_train_loss=0.0007309946981920082
Epoch 1568
Epoch 1568 :: Batch 0/1
Batch Loss = 0.0007309946970498745
1568, epoch_train_loss=0.0007309946970498745
Epoch 1569
Epoch 1569 :: Batch 0/1
Batch Loss = 0.0007309946959085648
1569, epoch_train_loss=0.0007309946959085648
Epoch 1570
Epoch 1570 :: Batch 0/1
Batch Loss = 0.0007309946947680788
1570, epoch_train_loss=0.0007309946947680788
Epoch 1571
Epoch 1571 :: Batch 0/1
Batch Loss = 0.0007309946936284158
1571, epoch_train_loss=0.0007309946936284158
Epoch 1572
Epoch 1572 :: Batch 0/1
Batch Loss = 0.0007309946924895753
1572, epoch_train_loss=0.0007309946924895753
Epoch 1573
Epoch 1573 :: Batch 0/1
Batch Loss = 0.000730994691351557
1573, epoch_train_loss=0.000730994691351557
Epoch 1574
Epoch 1574 :: Batch 0/1
Batch Loss = 0.0007309946902143602
1574, epoch_train_loss=0.0007309946902143602
Epoch 1575
Epoch 1575 :: Batch 0/1
Batch Loss = 0.0007309946890779841
1575, epoch_train_loss=0.0007309946890779841
Epoch 1576
Epoch 1576 :: Batch 0/1
Batch Loss = 0.0007309946879424286
1576, epoch_train_loss=0.0007309946879424286
Epoch 1577
Epoch 1577 :: Batch 0/1
Batch Loss = 0.0007309946868076929
1577, epoch_train_loss=0.0007309946868076929
Epoch 1578
Epoch 1578 :: Batch 0/1
Batch Loss = 0.0007309946856737764
1578, epoch_train_loss=0.0007309946856737764
Epoch 1579
Epoch 1579 :: Batch 0/1
Batch Loss = 0.0007309946845406789
1579, epoch_train_loss=0.0007309946845406789
Epoch 1580
Epoch 1580 :: Batch 0/1
Batch Loss = 0.0007309946834083999
1580, epoch_train_loss=0.0007309946834083999
Epoch 1581
Epoch 1581 :: Batch 0/1
Batch Loss = 0.0007309946822769383
1581, epoch_train_loss=0.0007309946822769383
Epoch 1582
Epoch 1582 :: Batch 0/1
Batch Loss = 0.0007309946811462943
1582, epoch_train_loss=0.0007309946811462943
Epoch 1583
Epoch 1583 :: Batch 0/1
Batch Loss = 0.0007309946800164667
1583, epoch_train_loss=0.0007309946800164667
Epoch 1584
Epoch 1584 :: Batch 0/1
Batch Loss = 0.0007309946788874553
1584, epoch_train_loss=0.0007309946788874553
Epoch 1585
Epoch 1585 :: Batch 0/1
Batch Loss = 0.0007309946777592596
1585, epoch_train_loss=0.0007309946777592596
Epoch 1586
Epoch 1586 :: Batch 0/1
Batch Loss = 0.0007309946766318791
1586, epoch_train_loss=0.0007309946766318791
Epoch 1587
Epoch 1587 :: Batch 0/1
Batch Loss = 0.0007309946755053129
1587, epoch_train_loss=0.0007309946755053129
Epoch 1588
Epoch 1588 :: Batch 0/1
Batch Loss = 0.0007309946743795609
1588, epoch_train_loss=0.0007309946743795609
Epoch 1589
Epoch 1589 :: Batch 0/1
Batch Loss = 0.0007309946732546226
1589, epoch_train_loss=0.0007309946732546226
Epoch 1590
Epoch 1590 :: Batch 0/1
Batch Loss = 0.0007309946721304971
1590, epoch_train_loss=0.0007309946721304971
Epoch 1591
Epoch 1591 :: Batch 0/1
Batch Loss = 0.000730994671007184
1591, epoch_train_loss=0.000730994671007184
Epoch 1592
Epoch 1592 :: Batch 0/1
Batch Loss = 0.0007309946698846827
1592, epoch_train_loss=0.0007309946698846827
Epoch 1593
Epoch 1593 :: Batch 0/1
Batch Loss = 0.0007309946687629929
1593, epoch_train_loss=0.0007309946687629929
Epoch 1594
Epoch 1594 :: Batch 0/1
Batch Loss = 0.000730994667642114
1594, epoch_train_loss=0.000730994667642114
Epoch 1595
Epoch 1595 :: Batch 0/1
Batch Loss = 0.0007309946665220455
1595, epoch_train_loss=0.0007309946665220455
Epoch 1596
Epoch 1596 :: Batch 0/1
Batch Loss = 0.0007309946654027866
1596, epoch_train_loss=0.0007309946654027866
Epoch 1597
Epoch 1597 :: Batch 0/1
Batch Loss = 0.0007309946642843368
1597, epoch_train_loss=0.0007309946642843368
Epoch 1598
Epoch 1598 :: Batch 0/1
Batch Loss = 0.000730994663166696
1598, epoch_train_loss=0.000730994663166696
Epoch 1599
Epoch 1599 :: Batch 0/1
Batch Loss = 0.0007309946620498632
1599, epoch_train_loss=0.0007309946620498632
Epoch 1600
Epoch 1600 :: Batch 0/1
Batch Loss = 0.000730994660933838
1600, epoch_train_loss=0.000730994660933838
Epoch 1601
Epoch 1601 :: Batch 0/1
Batch Loss = 0.0007309946598186199
1601, epoch_train_loss=0.0007309946598186199
Epoch 1602
Epoch 1602 :: Batch 0/1
Batch Loss = 0.0007309946587042085
1602, epoch_train_loss=0.0007309946587042085
Epoch 1603
Epoch 1603 :: Batch 0/1
Batch Loss = 0.0007309946575906032
1603, epoch_train_loss=0.0007309946575906032
Epoch 1604
Epoch 1604 :: Batch 0/1
Batch Loss = 0.0007309946564778032
1604, epoch_train_loss=0.0007309946564778032
Epoch 1605
Epoch 1605 :: Batch 0/1
Batch Loss = 0.0007309946553658085
1605, epoch_train_loss=0.0007309946553658085
Epoch 1606
Epoch 1606 :: Batch 0/1
Batch Loss = 0.0007309946542546178
1606, epoch_train_loss=0.0007309946542546178
Epoch 1607
Epoch 1607 :: Batch 0/1
Batch Loss = 0.0007309946531442313
1607, epoch_train_loss=0.0007309946531442313
Epoch 1608
Epoch 1608 :: Batch 0/1
Batch Loss = 0.0007309946520346481
1608, epoch_train_loss=0.0007309946520346481
Epoch 1609
Epoch 1609 :: Batch 0/1
Batch Loss = 0.0007309946509258677
1609, epoch_train_loss=0.0007309946509258677
Epoch 1610
Epoch 1610 :: Batch 0/1
Batch Loss = 0.0007309946498178895
1610, epoch_train_loss=0.0007309946498178895
Epoch 1611
Epoch 1611 :: Batch 0/1
Batch Loss = 0.0007309946487107133
1611, epoch_train_loss=0.0007309946487107133
Epoch 1612
Epoch 1612 :: Batch 0/1
Batch Loss = 0.0007309946476043381
1612, epoch_train_loss=0.0007309946476043381
Epoch 1613
Epoch 1613 :: Batch 0/1
Batch Loss = 0.0007309946464987637
1613, epoch_train_loss=0.0007309946464987637
Epoch 1614
Epoch 1614 :: Batch 0/1
Batch Loss = 0.0007309946453939892
1614, epoch_train_loss=0.0007309946453939892
Epoch 1615
Epoch 1615 :: Batch 0/1
Batch Loss = 0.0007309946442900146
1615, epoch_train_loss=0.0007309946442900146
Epoch 1616
Epoch 1616 :: Batch 0/1
Batch Loss = 0.000730994643186839
1616, epoch_train_loss=0.000730994643186839
Epoch 1617
Epoch 1617 :: Batch 0/1
Batch Loss = 0.000730994642084462
1617, epoch_train_loss=0.000730994642084462
Epoch 1618
Epoch 1618 :: Batch 0/1
Batch Loss = 0.0007309946409828832
1618, epoch_train_loss=0.0007309946409828832
Epoch 1619
Epoch 1619 :: Batch 0/1
Batch Loss = 0.0007309946398821015
1619, epoch_train_loss=0.0007309946398821015
Epoch 1620
Epoch 1620 :: Batch 0/1
Batch Loss = 0.0007309946387821169
1620, epoch_train_loss=0.0007309946387821169
Epoch 1621
Epoch 1621 :: Batch 0/1
Batch Loss = 0.0007309946376829288
1621, epoch_train_loss=0.0007309946376829288
Epoch 1622
Epoch 1622 :: Batch 0/1
Batch Loss = 0.0007309946365845365
1622, epoch_train_loss=0.0007309946365845365
Epoch 1623
Epoch 1623 :: Batch 0/1
Batch Loss = 0.0007309946354869395
1623, epoch_train_loss=0.0007309946354869395
Epoch 1624
Epoch 1624 :: Batch 0/1
Batch Loss = 0.0007309946343901374
1624, epoch_train_loss=0.0007309946343901374
Epoch 1625
Epoch 1625 :: Batch 0/1
Batch Loss = 0.0007309946332941296
1625, epoch_train_loss=0.0007309946332941296
Epoch 1626
Epoch 1626 :: Batch 0/1
Batch Loss = 0.0007309946321989154
1626, epoch_train_loss=0.0007309946321989154
Epoch 1627
Epoch 1627 :: Batch 0/1
Batch Loss = 0.0007309946311044945
1627, epoch_train_loss=0.0007309946311044945
Epoch 1628
Epoch 1628 :: Batch 0/1
Batch Loss = 0.0007309946300108664
1628, epoch_train_loss=0.0007309946300108664
Epoch 1629
Epoch 1629 :: Batch 0/1
Batch Loss = 0.0007309946289180301
1629, epoch_train_loss=0.0007309946289180301
Epoch 1630
Epoch 1630 :: Batch 0/1
Batch Loss = 0.0007309946278259856
1630, epoch_train_loss=0.0007309946278259856
Epoch 1631
Epoch 1631 :: Batch 0/1
Batch Loss = 0.000730994626734732
1631, epoch_train_loss=0.000730994626734732
Epoch 1632
Epoch 1632 :: Batch 0/1
Batch Loss = 0.000730994625644269
1632, epoch_train_loss=0.000730994625644269
Epoch 1633
Epoch 1633 :: Batch 0/1
Batch Loss = 0.0007309946245545962
1633, epoch_train_loss=0.0007309946245545962
Epoch 1634
Epoch 1634 :: Batch 0/1
Batch Loss = 0.0007309946234657124
1634, epoch_train_loss=0.0007309946234657124
Epoch 1635
Epoch 1635 :: Batch 0/1
Batch Loss = 0.000730994622377618
1635, epoch_train_loss=0.000730994622377618
Epoch 1636
Epoch 1636 :: Batch 0/1
Batch Loss = 0.0007309946212903116
1636, epoch_train_loss=0.0007309946212903116
Epoch 1637
Epoch 1637 :: Batch 0/1
Batch Loss = 0.0007309946202037933
1637, epoch_train_loss=0.0007309946202037933
Epoch 1638
Epoch 1638 :: Batch 0/1
Batch Loss = 0.0007309946191180622
1638, epoch_train_loss=0.0007309946191180622
Epoch 1639
Epoch 1639 :: Batch 0/1
Batch Loss = 0.0007309946180331177
1639, epoch_train_loss=0.0007309946180331177
Epoch 1640
Epoch 1640 :: Batch 0/1
Batch Loss = 0.0007309946169489596
1640, epoch_train_loss=0.0007309946169489596
Epoch 1641
Epoch 1641 :: Batch 0/1
Batch Loss = 0.0007309946158655873
1641, epoch_train_loss=0.0007309946158655873
Epoch 1642
Epoch 1642 :: Batch 0/1
Batch Loss = 0.0007309946147830002
1642, epoch_train_loss=0.0007309946147830002
Epoch 1643
Epoch 1643 :: Batch 0/1
Batch Loss = 0.0007309946137011974
1643, epoch_train_loss=0.0007309946137011974
Epoch 1644
Epoch 1644 :: Batch 0/1
Batch Loss = 0.0007309946126201789
1644, epoch_train_loss=0.0007309946126201789
Epoch 1645
Epoch 1645 :: Batch 0/1
Batch Loss = 0.000730994611539944
1645, epoch_train_loss=0.000730994611539944
Epoch 1646
Epoch 1646 :: Batch 0/1
Batch Loss = 0.000730994610460492
1646, epoch_train_loss=0.000730994610460492
Epoch 1647
Epoch 1647 :: Batch 0/1
Batch Loss = 0.0007309946093818227
1647, epoch_train_loss=0.0007309946093818227
Epoch 1648
Epoch 1648 :: Batch 0/1
Batch Loss = 0.000730994608303935
1648, epoch_train_loss=0.000730994608303935
Epoch 1649
Epoch 1649 :: Batch 0/1
Batch Loss = 0.0007309946072268288
1649, epoch_train_loss=0.0007309946072268288
Epoch 1650
Epoch 1650 :: Batch 0/1
Batch Loss = 0.0007309946061505037
1650, epoch_train_loss=0.0007309946061505037
Epoch 1651
Epoch 1651 :: Batch 0/1
Batch Loss = 0.0007309946050749588
1651, epoch_train_loss=0.0007309946050749588
Epoch 1652
Epoch 1652 :: Batch 0/1
Batch Loss = 0.0007309946040001937
1652, epoch_train_loss=0.0007309946040001937
Epoch 1653
Epoch 1653 :: Batch 0/1
Batch Loss = 0.0007309946029262079
1653, epoch_train_loss=0.0007309946029262079
Epoch 1654
Epoch 1654 :: Batch 0/1
Batch Loss = 0.0007309946018530008
1654, epoch_train_loss=0.0007309946018530008
Epoch 1655
Epoch 1655 :: Batch 0/1
Batch Loss = 0.0007309946007805719
1655, epoch_train_loss=0.0007309946007805719
Epoch 1656
Epoch 1656 :: Batch 0/1
Batch Loss = 0.0007309945997089208
1656, epoch_train_loss=0.0007309945997089208
Epoch 1657
Epoch 1657 :: Batch 0/1
Batch Loss = 0.0007309945986380466
1657, epoch_train_loss=0.0007309945986380466
Epoch 1658
Epoch 1658 :: Batch 0/1
Batch Loss = 0.0007309945975679491
1658, epoch_train_loss=0.0007309945975679491
Epoch 1659
Epoch 1659 :: Batch 0/1
Batch Loss = 0.0007309945964986277
1659, epoch_train_loss=0.0007309945964986277
Epoch 1660
Epoch 1660 :: Batch 0/1
Batch Loss = 0.0007309945954300817
1660, epoch_train_loss=0.0007309945954300817
Epoch 1661
Epoch 1661 :: Batch 0/1
Batch Loss = 0.0007309945943623107
1661, epoch_train_loss=0.0007309945943623107
Epoch 1662
Epoch 1662 :: Batch 0/1
Batch Loss = 0.0007309945932953141
1662, epoch_train_loss=0.0007309945932953141
Epoch 1663
Epoch 1663 :: Batch 0/1
Batch Loss = 0.0007309945922290916
1663, epoch_train_loss=0.0007309945922290916
Epoch 1664
Epoch 1664 :: Batch 0/1
Batch Loss = 0.0007309945911636425
1664, epoch_train_loss=0.0007309945911636425
Epoch 1665
Epoch 1665 :: Batch 0/1
Batch Loss = 0.000730994590098966
1665, epoch_train_loss=0.000730994590098966
Epoch 1666
Epoch 1666 :: Batch 0/1
Batch Loss = 0.0007309945890350618
1666, epoch_train_loss=0.0007309945890350618
Epoch 1667
Epoch 1667 :: Batch 0/1
Batch Loss = 0.0007309945879719295
1667, epoch_train_loss=0.0007309945879719295
Epoch 1668
Epoch 1668 :: Batch 0/1
Batch Loss = 0.0007309945869095683
1668, epoch_train_loss=0.0007309945869095683
Epoch 1669
Epoch 1669 :: Batch 0/1
Batch Loss = 0.0007309945858479778
1669, epoch_train_loss=0.0007309945858479778
Epoch 1670
Epoch 1670 :: Batch 0/1
Batch Loss = 0.0007309945847871577
1670, epoch_train_loss=0.0007309945847871577
Epoch 1671
Epoch 1671 :: Batch 0/1
Batch Loss = 0.0007309945837271069
1671, epoch_train_loss=0.0007309945837271069
Epoch 1672
Epoch 1672 :: Batch 0/1
Batch Loss = 0.0007309945826678253
1672, epoch_train_loss=0.0007309945826678253
Epoch 1673
Epoch 1673 :: Batch 0/1
Batch Loss = 0.0007309945816093123
1673, epoch_train_loss=0.0007309945816093123
Epoch 1674
Epoch 1674 :: Batch 0/1
Batch Loss = 0.0007309945805515674
1674, epoch_train_loss=0.0007309945805515674
Epoch 1675
Epoch 1675 :: Batch 0/1
Batch Loss = 0.0007309945794945899
1675, epoch_train_loss=0.0007309945794945899
Epoch 1676
Epoch 1676 :: Batch 0/1
Batch Loss = 0.0007309945784383794
1676, epoch_train_loss=0.0007309945784383794
Epoch 1677
Epoch 1677 :: Batch 0/1
Batch Loss = 0.0007309945773829352
1677, epoch_train_loss=0.0007309945773829352
Epoch 1678
Epoch 1678 :: Batch 0/1
Batch Loss = 0.0007309945763282569
1678, epoch_train_loss=0.0007309945763282569
Epoch 1679
Epoch 1679 :: Batch 0/1
Batch Loss = 0.000730994575274344
1679, epoch_train_loss=0.000730994575274344
Epoch 1680
Epoch 1680 :: Batch 0/1
Batch Loss = 0.0007309945742211958
1680, epoch_train_loss=0.0007309945742211958
Epoch 1681
Epoch 1681 :: Batch 0/1
Batch Loss = 0.000730994573168812
1681, epoch_train_loss=0.000730994573168812
Epoch 1682
Epoch 1682 :: Batch 0/1
Batch Loss = 0.0007309945721171918
1682, epoch_train_loss=0.0007309945721171918
Epoch 1683
Epoch 1683 :: Batch 0/1
Batch Loss = 0.0007309945710663347
1683, epoch_train_loss=0.0007309945710663347
Epoch 1684
Epoch 1684 :: Batch 0/1
Batch Loss = 0.0007309945700162406
1684, epoch_train_loss=0.0007309945700162406
Epoch 1685
Epoch 1685 :: Batch 0/1
Batch Loss = 0.0007309945689669084
1685, epoch_train_loss=0.0007309945689669084
Epoch 1686
Epoch 1686 :: Batch 0/1
Batch Loss = 0.0007309945679183378
1686, epoch_train_loss=0.0007309945679183378
Epoch 1687
Epoch 1687 :: Batch 0/1
Batch Loss = 0.0007309945668705283
1687, epoch_train_loss=0.0007309945668705283
Epoch 1688
Epoch 1688 :: Batch 0/1
Batch Loss = 0.0007309945658234793
1688, epoch_train_loss=0.0007309945658234793
Epoch 1689
Epoch 1689 :: Batch 0/1
Batch Loss = 0.0007309945647771903
1689, epoch_train_loss=0.0007309945647771903
Epoch 1690
Epoch 1690 :: Batch 0/1
Batch Loss = 0.0007309945637316607
1690, epoch_train_loss=0.0007309945637316607
Epoch 1691
Epoch 1691 :: Batch 0/1
Batch Loss = 0.0007309945626868901
1691, epoch_train_loss=0.0007309945626868901
Epoch 1692
Epoch 1692 :: Batch 0/1
Batch Loss = 0.0007309945616428779
1692, epoch_train_loss=0.0007309945616428779
Epoch 1693
Epoch 1693 :: Batch 0/1
Batch Loss = 0.0007309945605996236
1693, epoch_train_loss=0.0007309945605996236
Epoch 1694
Epoch 1694 :: Batch 0/1
Batch Loss = 0.0007309945595571265
1694, epoch_train_loss=0.0007309945595571265
Epoch 1695
Epoch 1695 :: Batch 0/1
Batch Loss = 0.0007309945585153861
1695, epoch_train_loss=0.0007309945585153861
Epoch 1696
Epoch 1696 :: Batch 0/1
Batch Loss = 0.000730994557474402
1696, epoch_train_loss=0.000730994557474402
Epoch 1697
Epoch 1697 :: Batch 0/1
Batch Loss = 0.0007309945564341738
1697, epoch_train_loss=0.0007309945564341738
Epoch 1698
Epoch 1698 :: Batch 0/1
Batch Loss = 0.0007309945553947008
1698, epoch_train_loss=0.0007309945553947008
Epoch 1699
Epoch 1699 :: Batch 0/1
Batch Loss = 0.0007309945543559822
1699, epoch_train_loss=0.0007309945543559822
Epoch 1700
Epoch 1700 :: Batch 0/1
Batch Loss = 0.0007309945533180179
1700, epoch_train_loss=0.0007309945533180179
Epoch 1701
Epoch 1701 :: Batch 0/1
Batch Loss = 0.000730994552280807
1701, epoch_train_loss=0.000730994552280807
Epoch 1702
Epoch 1702 :: Batch 0/1
Batch Loss = 0.0007309945512443491
1702, epoch_train_loss=0.0007309945512443491
Epoch 1703
Epoch 1703 :: Batch 0/1
Batch Loss = 0.000730994550208644
1703, epoch_train_loss=0.000730994550208644
Epoch 1704
Epoch 1704 :: Batch 0/1
Batch Loss = 0.0007309945491736906
1704, epoch_train_loss=0.0007309945491736906
Epoch 1705
Epoch 1705 :: Batch 0/1
Batch Loss = 0.0007309945481394887
1705, epoch_train_loss=0.0007309945481394887
Epoch 1706
Epoch 1706 :: Batch 0/1
Batch Loss = 0.0007309945471060378
1706, epoch_train_loss=0.0007309945471060378
Epoch 1707
Epoch 1707 :: Batch 0/1
Batch Loss = 0.0007309945460733373
1707, epoch_train_loss=0.0007309945460733373
Epoch 1708
Epoch 1708 :: Batch 0/1
Batch Loss = 0.0007309945450413865
1708, epoch_train_loss=0.0007309945450413865
Epoch 1709
Epoch 1709 :: Batch 0/1
Batch Loss = 0.000730994544010185
1709, epoch_train_loss=0.000730994544010185
Epoch 1710
Epoch 1710 :: Batch 0/1
Batch Loss = 0.0007309945429797325
1710, epoch_train_loss=0.0007309945429797325
Epoch 1711
Epoch 1711 :: Batch 0/1
Batch Loss = 0.000730994541950028
1711, epoch_train_loss=0.000730994541950028
Epoch 1712
Epoch 1712 :: Batch 0/1
Batch Loss = 0.0007309945409210714
1712, epoch_train_loss=0.0007309945409210714
Epoch 1713
Epoch 1713 :: Batch 0/1
Batch Loss = 0.0007309945398928619
1713, epoch_train_loss=0.0007309945398928619
Epoch 1714
Epoch 1714 :: Batch 0/1
Batch Loss = 0.0007309945388653988
1714, epoch_train_loss=0.0007309945388653988
Epoch 1715
Epoch 1715 :: Batch 0/1
Batch Loss = 0.0007309945378386821
1715, epoch_train_loss=0.0007309945378386821
Epoch 1716
Epoch 1716 :: Batch 0/1
Batch Loss = 0.000730994536812711
1716, epoch_train_loss=0.000730994536812711
Epoch 1717
Epoch 1717 :: Batch 0/1
Batch Loss = 0.0007309945357874848
1717, epoch_train_loss=0.0007309945357874848
Epoch 1718
Epoch 1718 :: Batch 0/1
Batch Loss = 0.0007309945347630032
1718, epoch_train_loss=0.0007309945347630032
Epoch 1719
Epoch 1719 :: Batch 0/1
Batch Loss = 0.0007309945337392655
1719, epoch_train_loss=0.0007309945337392655
Epoch 1720
Epoch 1720 :: Batch 0/1
Batch Loss = 0.0007309945327162713
1720, epoch_train_loss=0.0007309945327162713
Epoch 1721
Epoch 1721 :: Batch 0/1
Batch Loss = 0.00073099453169402
1721, epoch_train_loss=0.00073099453169402
Epoch 1722
Epoch 1722 :: Batch 0/1
Batch Loss = 0.0007309945306725111
1722, epoch_train_loss=0.0007309945306725111
Epoch 1723
Epoch 1723 :: Batch 0/1
Batch Loss = 0.000730994529651744
1723, epoch_train_loss=0.000730994529651744
Epoch 1724
Epoch 1724 :: Batch 0/1
Batch Loss = 0.0007309945286317184
1724, epoch_train_loss=0.0007309945286317184
Epoch 1725
Epoch 1725 :: Batch 0/1
Batch Loss = 0.0007309945276124335
1725, epoch_train_loss=0.0007309945276124335
Epoch 1726
Epoch 1726 :: Batch 0/1
Batch Loss = 0.0007309945265938887
1726, epoch_train_loss=0.0007309945265938887
Epoch 1727
Epoch 1727 :: Batch 0/1
Batch Loss = 0.0007309945255760837
1727, epoch_train_loss=0.0007309945255760837
Epoch 1728
Epoch 1728 :: Batch 0/1
Batch Loss = 0.000730994524559018
1728, epoch_train_loss=0.000730994524559018
Epoch 1729
Epoch 1729 :: Batch 0/1
Batch Loss = 0.0007309945235426909
1729, epoch_train_loss=0.0007309945235426909
Epoch 1730
Epoch 1730 :: Batch 0/1
Batch Loss = 0.0007309945225271019
1730, epoch_train_loss=0.0007309945225271019
Epoch 1731
Epoch 1731 :: Batch 0/1
Batch Loss = 0.0007309945215122506
1731, epoch_train_loss=0.0007309945215122506
Epoch 1732
Epoch 1732 :: Batch 0/1
Batch Loss = 0.0007309945204981363
1732, epoch_train_loss=0.0007309945204981363
Epoch 1733
Epoch 1733 :: Batch 0/1
Batch Loss = 0.0007309945194847586
1733, epoch_train_loss=0.0007309945194847586
Epoch 1734
Epoch 1734 :: Batch 0/1
Batch Loss = 0.0007309945184721168
1734, epoch_train_loss=0.0007309945184721168
Epoch 1735
Epoch 1735 :: Batch 0/1
Batch Loss = 0.0007309945174602105
1735, epoch_train_loss=0.0007309945174602105
Epoch 1736
Epoch 1736 :: Batch 0/1
Batch Loss = 0.0007309945164490391
1736, epoch_train_loss=0.0007309945164490391
Epoch 1737
Epoch 1737 :: Batch 0/1
Batch Loss = 0.0007309945154386022
1737, epoch_train_loss=0.0007309945154386022
Epoch 1738
Epoch 1738 :: Batch 0/1
Batch Loss = 0.000730994514428899
1738, epoch_train_loss=0.000730994514428899
Epoch 1739
Epoch 1739 :: Batch 0/1
Batch Loss = 0.0007309945134199294
1739, epoch_train_loss=0.0007309945134199294
Epoch 1740
Epoch 1740 :: Batch 0/1
Batch Loss = 0.0007309945124116927
1740, epoch_train_loss=0.0007309945124116927
Epoch 1741
Epoch 1741 :: Batch 0/1
Batch Loss = 0.0007309945114041881
1741, epoch_train_loss=0.0007309945114041881
Epoch 1742
Epoch 1742 :: Batch 0/1
Batch Loss = 0.0007309945103974153
1742, epoch_train_loss=0.0007309945103974153
Epoch 1743
Epoch 1743 :: Batch 0/1
Batch Loss = 0.0007309945093913738
1743, epoch_train_loss=0.0007309945093913738
Epoch 1744
Epoch 1744 :: Batch 0/1
Batch Loss = 0.000730994508386063
1744, epoch_train_loss=0.000730994508386063
Epoch 1745
Epoch 1745 :: Batch 0/1
Batch Loss = 0.0007309945073814822
1745, epoch_train_loss=0.0007309945073814822
Epoch 1746
Epoch 1746 :: Batch 0/1
Batch Loss = 0.0007309945063776312
1746, epoch_train_loss=0.0007309945063776312
Epoch 1747
Epoch 1747 :: Batch 0/1
Batch Loss = 0.0007309945053745092
1747, epoch_train_loss=0.0007309945053745092
Epoch 1748
Epoch 1748 :: Batch 0/1
Batch Loss = 0.0007309945043721159
1748, epoch_train_loss=0.0007309945043721159
Epoch 1749
Epoch 1749 :: Batch 0/1
Batch Loss = 0.0007309945033704506
1749, epoch_train_loss=0.0007309945033704506
Epoch 1750
Epoch 1750 :: Batch 0/1
Batch Loss = 0.000730994502369513
1750, epoch_train_loss=0.000730994502369513
Epoch 1751
Epoch 1751 :: Batch 0/1
Batch Loss = 0.0007309945013693022
1751, epoch_train_loss=0.0007309945013693022
Epoch 1752
Epoch 1752 :: Batch 0/1
Batch Loss = 0.0007309945003698181
1752, epoch_train_loss=0.0007309945003698181
Epoch 1753
Epoch 1753 :: Batch 0/1
Batch Loss = 0.0007309944993710597
1753, epoch_train_loss=0.0007309944993710597
Epoch 1754
Epoch 1754 :: Batch 0/1
Batch Loss = 0.0007309944983730268
1754, epoch_train_loss=0.0007309944983730268
Epoch 1755
Epoch 1755 :: Batch 0/1
Batch Loss = 0.0007309944973757187
1755, epoch_train_loss=0.0007309944973757187
Epoch 1756
Epoch 1756 :: Batch 0/1
Batch Loss = 0.0007309944963791351
1756, epoch_train_loss=0.0007309944963791351
Epoch 1757
Epoch 1757 :: Batch 0/1
Batch Loss = 0.0007309944953832752
1757, epoch_train_loss=0.0007309944953832752
Epoch 1758
Epoch 1758 :: Batch 0/1
Batch Loss = 0.0007309944943881387
1758, epoch_train_loss=0.0007309944943881387
Epoch 1759
Epoch 1759 :: Batch 0/1
Batch Loss = 0.000730994493393725
1759, epoch_train_loss=0.000730994493393725
Epoch 1760
Epoch 1760 :: Batch 0/1
Batch Loss = 0.0007309944924000336
1760, epoch_train_loss=0.0007309944924000336
Epoch 1761
Epoch 1761 :: Batch 0/1
Batch Loss = 0.0007309944914070638
1761, epoch_train_loss=0.0007309944914070638
Epoch 1762
Epoch 1762 :: Batch 0/1
Batch Loss = 0.0007309944904148153
1762, epoch_train_loss=0.0007309944904148153
Epoch 1763
Epoch 1763 :: Batch 0/1
Batch Loss = 0.0007309944894232876
1763, epoch_train_loss=0.0007309944894232876
Epoch 1764
Epoch 1764 :: Batch 0/1
Batch Loss = 0.0007309944884324797
1764, epoch_train_loss=0.0007309944884324797
Epoch 1765
Epoch 1765 :: Batch 0/1
Batch Loss = 0.0007309944874423918
1765, epoch_train_loss=0.0007309944874423918
Epoch 1766
Epoch 1766 :: Batch 0/1
Batch Loss = 0.0007309944864530227
1766, epoch_train_loss=0.0007309944864530227
Epoch 1767
Epoch 1767 :: Batch 0/1
Batch Loss = 0.0007309944854643724
1767, epoch_train_loss=0.0007309944854643724
Epoch 1768
Epoch 1768 :: Batch 0/1
Batch Loss = 0.0007309944844764399
1768, epoch_train_loss=0.0007309944844764399
Epoch 1769
Epoch 1769 :: Batch 0/1
Batch Loss = 0.000730994483489225
1769, epoch_train_loss=0.000730994483489225
Epoch 1770
Epoch 1770 :: Batch 0/1
Batch Loss = 0.000730994482502727
1770, epoch_train_loss=0.000730994482502727
Epoch 1771
Epoch 1771 :: Batch 0/1
Batch Loss = 0.0007309944815169457
1771, epoch_train_loss=0.0007309944815169457
Epoch 1772
Epoch 1772 :: Batch 0/1
Batch Loss = 0.0007309944805318801
1772, epoch_train_loss=0.0007309944805318801
Epoch 1773
Epoch 1773 :: Batch 0/1
Batch Loss = 0.0007309944795475302
1773, epoch_train_loss=0.0007309944795475302
Epoch 1774
Epoch 1774 :: Batch 0/1
Batch Loss = 0.0007309944785638949
1774, epoch_train_loss=0.0007309944785638949
Epoch 1775
Epoch 1775 :: Batch 0/1
Batch Loss = 0.0007309944775809739
1775, epoch_train_loss=0.0007309944775809739
Epoch 1776
Epoch 1776 :: Batch 0/1
Batch Loss = 0.000730994476598767
1776, epoch_train_loss=0.000730994476598767
Epoch 1777
Epoch 1777 :: Batch 0/1
Batch Loss = 0.0007309944756172732
1777, epoch_train_loss=0.0007309944756172732
Epoch 1778
Epoch 1778 :: Batch 0/1
Batch Loss = 0.0007309944746364924
1778, epoch_train_loss=0.0007309944746364924
Epoch 1779
Epoch 1779 :: Batch 0/1
Batch Loss = 0.0007309944736564237
1779, epoch_train_loss=0.0007309944736564237
Epoch 1780
Epoch 1780 :: Batch 0/1
Batch Loss = 0.0007309944726770669
1780, epoch_train_loss=0.0007309944726770669
Epoch 1781
Epoch 1781 :: Batch 0/1
Batch Loss = 0.0007309944716984212
1781, epoch_train_loss=0.0007309944716984212
Epoch 1782
Epoch 1782 :: Batch 0/1
Batch Loss = 0.0007309944707204862
1782, epoch_train_loss=0.0007309944707204862
Epoch 1783
Epoch 1783 :: Batch 0/1
Batch Loss = 0.0007309944697432614
1783, epoch_train_loss=0.0007309944697432614
Epoch 1784
Epoch 1784 :: Batch 0/1
Batch Loss = 0.0007309944687667461
1784, epoch_train_loss=0.0007309944687667461
Epoch 1785
Epoch 1785 :: Batch 0/1
Batch Loss = 0.0007309944677909401
1785, epoch_train_loss=0.0007309944677909401
Epoch 1786
Epoch 1786 :: Batch 0/1
Batch Loss = 0.0007309944668158426
1786, epoch_train_loss=0.0007309944668158426
Epoch 1787
Epoch 1787 :: Batch 0/1
Batch Loss = 0.0007309944658414531
1787, epoch_train_loss=0.0007309944658414531
Epoch 1788
Epoch 1788 :: Batch 0/1
Batch Loss = 0.0007309944648677712
1788, epoch_train_loss=0.0007309944648677712
Epoch 1789
Epoch 1789 :: Batch 0/1
Batch Loss = 0.0007309944638947964
1789, epoch_train_loss=0.0007309944638947964
Epoch 1790
Epoch 1790 :: Batch 0/1
Batch Loss = 0.0007309944629225279
1790, epoch_train_loss=0.0007309944629225279
Epoch 1791
Epoch 1791 :: Batch 0/1
Batch Loss = 0.0007309944619509658
1791, epoch_train_loss=0.0007309944619509658
Epoch 1792
Epoch 1792 :: Batch 0/1
Batch Loss = 0.0007309944609801088
1792, epoch_train_loss=0.0007309944609801088
Epoch 1793
Epoch 1793 :: Batch 0/1
Batch Loss = 0.0007309944600099569
1793, epoch_train_loss=0.0007309944600099569
Epoch 1794
Epoch 1794 :: Batch 0/1
Batch Loss = 0.0007309944590405094
1794, epoch_train_loss=0.0007309944590405094
Epoch 1795
Epoch 1795 :: Batch 0/1
Batch Loss = 0.0007309944580717656
1795, epoch_train_loss=0.0007309944580717656
Epoch 1796
Epoch 1796 :: Batch 0/1
Batch Loss = 0.0007309944571037254
1796, epoch_train_loss=0.0007309944571037254
Epoch 1797
Epoch 1797 :: Batch 0/1
Batch Loss = 0.0007309944561363879
1797, epoch_train_loss=0.0007309944561363879
Epoch 1798
Epoch 1798 :: Batch 0/1
Batch Loss = 0.0007309944551697529
1798, epoch_train_loss=0.0007309944551697529
Epoch 1799
Epoch 1799 :: Batch 0/1
Batch Loss = 0.0007309944542038196
1799, epoch_train_loss=0.0007309944542038196
Epoch 1800
Epoch 1800 :: Batch 0/1
Batch Loss = 0.0007309944532385875
1800, epoch_train_loss=0.0007309944532385875
Epoch 1801
Epoch 1801 :: Batch 0/1
Batch Loss = 0.0007309944522740564
1801, epoch_train_loss=0.0007309944522740564
Epoch 1802
Epoch 1802 :: Batch 0/1
Batch Loss = 0.0007309944513102253
1802, epoch_train_loss=0.0007309944513102253
Epoch 1803
Epoch 1803 :: Batch 0/1
Batch Loss = 0.000730994450347094
1803, epoch_train_loss=0.000730994450347094
Epoch 1804
Epoch 1804 :: Batch 0/1
Batch Loss = 0.000730994449384662
1804, epoch_train_loss=0.000730994449384662
Epoch 1805
Epoch 1805 :: Batch 0/1
Batch Loss = 0.0007309944484229286
1805, epoch_train_loss=0.0007309944484229286
Epoch 1806
Epoch 1806 :: Batch 0/1
Batch Loss = 0.0007309944474618935
1806, epoch_train_loss=0.0007309944474618935
Epoch 1807
Epoch 1807 :: Batch 0/1
Batch Loss = 0.0007309944465015559
1807, epoch_train_loss=0.0007309944465015559
Epoch 1808
Epoch 1808 :: Batch 0/1
Batch Loss = 0.0007309944455419155
1808, epoch_train_loss=0.0007309944455419155
Epoch 1809
Epoch 1809 :: Batch 0/1
Batch Loss = 0.000730994444582972
1809, epoch_train_loss=0.000730994444582972
Epoch 1810
Epoch 1810 :: Batch 0/1
Batch Loss = 0.0007309944436247241
1810, epoch_train_loss=0.0007309944436247241
Epoch 1811
Epoch 1811 :: Batch 0/1
Batch Loss = 0.000730994442667172
1811, epoch_train_loss=0.000730994442667172
Epoch 1812
Epoch 1812 :: Batch 0/1
Batch Loss = 0.0007309944417103149
1812, epoch_train_loss=0.0007309944417103149
Epoch 1813
Epoch 1813 :: Batch 0/1
Batch Loss = 0.0007309944407541523
1813, epoch_train_loss=0.0007309944407541523
Epoch 1814
Epoch 1814 :: Batch 0/1
Batch Loss = 0.0007309944397986838
1814, epoch_train_loss=0.0007309944397986838
Epoch 1815
Epoch 1815 :: Batch 0/1
Batch Loss = 0.0007309944388439089
1815, epoch_train_loss=0.0007309944388439089
Epoch 1816
Epoch 1816 :: Batch 0/1
Batch Loss = 0.0007309944378898267
1816, epoch_train_loss=0.0007309944378898267
Epoch 1817
Epoch 1817 :: Batch 0/1
Batch Loss = 0.0007309944369364372
1817, epoch_train_loss=0.0007309944369364372
Epoch 1818
Epoch 1818 :: Batch 0/1
Batch Loss = 0.0007309944359837396
1818, epoch_train_loss=0.0007309944359837396
Epoch 1819
Epoch 1819 :: Batch 0/1
Batch Loss = 0.0007309944350317334
1819, epoch_train_loss=0.0007309944350317334
Epoch 1820
Epoch 1820 :: Batch 0/1
Batch Loss = 0.0007309944340804181
1820, epoch_train_loss=0.0007309944340804181
Epoch 1821
Epoch 1821 :: Batch 0/1
Batch Loss = 0.0007309944331297932
1821, epoch_train_loss=0.0007309944331297932
Epoch 1822
Epoch 1822 :: Batch 0/1
Batch Loss = 0.0007309944321798582
1822, epoch_train_loss=0.0007309944321798582
Epoch 1823
Epoch 1823 :: Batch 0/1
Batch Loss = 0.0007309944312306125
1823, epoch_train_loss=0.0007309944312306125
Epoch 1824
Epoch 1824 :: Batch 0/1
Batch Loss = 0.0007309944302820559
1824, epoch_train_loss=0.0007309944302820559
Epoch 1825
Epoch 1825 :: Batch 0/1
Batch Loss = 0.0007309944293341875
1825, epoch_train_loss=0.0007309944293341875
Epoch 1826
Epoch 1826 :: Batch 0/1
Batch Loss = 0.0007309944283870068
1826, epoch_train_loss=0.0007309944283870068
Epoch 1827
Epoch 1827 :: Batch 0/1
Batch Loss = 0.0007309944274405134
1827, epoch_train_loss=0.0007309944274405134
Epoch 1828
Epoch 1828 :: Batch 0/1
Batch Loss = 0.0007309944264947068
1828, epoch_train_loss=0.0007309944264947068
Epoch 1829
Epoch 1829 :: Batch 0/1
Batch Loss = 0.0007309944255495868
1829, epoch_train_loss=0.0007309944255495868
Epoch 1830
Epoch 1830 :: Batch 0/1
Batch Loss = 0.000730994424605152
1830, epoch_train_loss=0.000730994424605152
Epoch 1831
Epoch 1831 :: Batch 0/1
Batch Loss = 0.0007309944236614028
1831, epoch_train_loss=0.0007309944236614028
Epoch 1832
Epoch 1832 :: Batch 0/1
Batch Loss = 0.0007309944227183383
1832, epoch_train_loss=0.0007309944227183383
Epoch 1833
Epoch 1833 :: Batch 0/1
Batch Loss = 0.000730994421775958
1833, epoch_train_loss=0.000730994421775958
Epoch 1834
Epoch 1834 :: Batch 0/1
Batch Loss = 0.0007309944208342613
1834, epoch_train_loss=0.0007309944208342613
Epoch 1835
Epoch 1835 :: Batch 0/1
Batch Loss = 0.000730994419893248
1835, epoch_train_loss=0.000730994419893248
Epoch 1836
Epoch 1836 :: Batch 0/1
Batch Loss = 0.0007309944189529171
1836, epoch_train_loss=0.0007309944189529171
Epoch 1837
Epoch 1837 :: Batch 0/1
Batch Loss = 0.0007309944180132684
1837, epoch_train_loss=0.0007309944180132684
Epoch 1838
Epoch 1838 :: Batch 0/1
Batch Loss = 0.0007309944170743013
1838, epoch_train_loss=0.0007309944170743013
Epoch 1839
Epoch 1839 :: Batch 0/1
Batch Loss = 0.0007309944161360155
1839, epoch_train_loss=0.0007309944161360155
Epoch 1840
Epoch 1840 :: Batch 0/1
Batch Loss = 0.0007309944151984103
1840, epoch_train_loss=0.0007309944151984103
Epoch 1841
Epoch 1841 :: Batch 0/1
Batch Loss = 0.0007309944142614854
1841, epoch_train_loss=0.0007309944142614854
Epoch 1842
Epoch 1842 :: Batch 0/1
Batch Loss = 0.0007309944133252397
1842, epoch_train_loss=0.0007309944133252397
Epoch 1843
Epoch 1843 :: Batch 0/1
Batch Loss = 0.0007309944123896735
1843, epoch_train_loss=0.0007309944123896735
Epoch 1844
Epoch 1844 :: Batch 0/1
Batch Loss = 0.0007309944114547854
1844, epoch_train_loss=0.0007309944114547854
Epoch 1845
Epoch 1845 :: Batch 0/1
Batch Loss = 0.0007309944105205758
1845, epoch_train_loss=0.0007309944105205758
Epoch 1846
Epoch 1846 :: Batch 0/1
Batch Loss = 0.0007309944095870433
1846, epoch_train_loss=0.0007309944095870433
Epoch 1847
Epoch 1847 :: Batch 0/1
Batch Loss = 0.0007309944086541881
1847, epoch_train_loss=0.0007309944086541881
Epoch 1848
Epoch 1848 :: Batch 0/1
Batch Loss = 0.0007309944077220094
1848, epoch_train_loss=0.0007309944077220094
Epoch 1849
Epoch 1849 :: Batch 0/1
Batch Loss = 0.0007309944067905067
1849, epoch_train_loss=0.0007309944067905067
Epoch 1850
Epoch 1850 :: Batch 0/1
Batch Loss = 0.0007309944058596796
1850, epoch_train_loss=0.0007309944058596796
Epoch 1851
Epoch 1851 :: Batch 0/1
Batch Loss = 0.0007309944049295274
1851, epoch_train_loss=0.0007309944049295274
Epoch 1852
Epoch 1852 :: Batch 0/1
Batch Loss = 0.0007309944040000496
1852, epoch_train_loss=0.0007309944040000496
Epoch 1853
Epoch 1853 :: Batch 0/1
Batch Loss = 0.000730994403071246
1853, epoch_train_loss=0.000730994403071246
Epoch 1854
Epoch 1854 :: Batch 0/1
Batch Loss = 0.0007309944021431156
1854, epoch_train_loss=0.0007309944021431156
Epoch 1855
Epoch 1855 :: Batch 0/1
Batch Loss = 0.0007309944012156582
1855, epoch_train_loss=0.0007309944012156582
Epoch 1856
Epoch 1856 :: Batch 0/1
Batch Loss = 0.0007309944002888733
1856, epoch_train_loss=0.0007309944002888733
Epoch 1857
Epoch 1857 :: Batch 0/1
Batch Loss = 0.0007309943993627603
1857, epoch_train_loss=0.0007309943993627603
Epoch 1858
Epoch 1858 :: Batch 0/1
Batch Loss = 0.0007309943984373187
1858, epoch_train_loss=0.0007309943984373187
Epoch 1859
Epoch 1859 :: Batch 0/1
Batch Loss = 0.0007309943975125482
1859, epoch_train_loss=0.0007309943975125482
Epoch 1860
Epoch 1860 :: Batch 0/1
Batch Loss = 0.000730994396588448
1860, epoch_train_loss=0.000730994396588448
Epoch 1861
Epoch 1861 :: Batch 0/1
Batch Loss = 0.0007309943956650178
1861, epoch_train_loss=0.0007309943956650178
Epoch 1862
Epoch 1862 :: Batch 0/1
Batch Loss = 0.0007309943947422567
1862, epoch_train_loss=0.0007309943947422567
Epoch 1863
Epoch 1863 :: Batch 0/1
Batch Loss = 0.0007309943938201647
1863, epoch_train_loss=0.0007309943938201647
Epoch 1864
Epoch 1864 :: Batch 0/1
Batch Loss = 0.0007309943928987411
1864, epoch_train_loss=0.0007309943928987411
Epoch 1865
Epoch 1865 :: Batch 0/1
Batch Loss = 0.0007309943919779853
1865, epoch_train_loss=0.0007309943919779853
Epoch 1866
Epoch 1866 :: Batch 0/1
Batch Loss = 0.0007309943910578968
1866, epoch_train_loss=0.0007309943910578968
Epoch 1867
Epoch 1867 :: Batch 0/1
Batch Loss = 0.0007309943901384753
1867, epoch_train_loss=0.0007309943901384753
Epoch 1868
Epoch 1868 :: Batch 0/1
Batch Loss = 0.00073099438921972
1868, epoch_train_loss=0.00073099438921972
Epoch 1869
Epoch 1869 :: Batch 0/1
Batch Loss = 0.0007309943883016308
1869, epoch_train_loss=0.0007309943883016308
Epoch 1870
Epoch 1870 :: Batch 0/1
Batch Loss = 0.0007309943873842067
1870, epoch_train_loss=0.0007309943873842067
Epoch 1871
Epoch 1871 :: Batch 0/1
Batch Loss = 0.0007309943864674475
1871, epoch_train_loss=0.0007309943864674475
Epoch 1872
Epoch 1872 :: Batch 0/1
Batch Loss = 0.0007309943855513526
1872, epoch_train_loss=0.0007309943855513526
Epoch 1873
Epoch 1873 :: Batch 0/1
Batch Loss = 0.0007309943846359215
1873, epoch_train_loss=0.0007309943846359215
Epoch 1874
Epoch 1874 :: Batch 0/1
Batch Loss = 0.0007309943837211538
1874, epoch_train_loss=0.0007309943837211538
Epoch 1875
Epoch 1875 :: Batch 0/1
Batch Loss = 0.000730994382807049
1875, epoch_train_loss=0.000730994382807049
Epoch 1876
Epoch 1876 :: Batch 0/1
Batch Loss = 0.0007309943818936063
1876, epoch_train_loss=0.0007309943818936063
Epoch 1877
Epoch 1877 :: Batch 0/1
Batch Loss = 0.0007309943809808253
1877, epoch_train_loss=0.0007309943809808253
Epoch 1878
Epoch 1878 :: Batch 0/1
Batch Loss = 0.0007309943800687059
1878, epoch_train_loss=0.0007309943800687059
Epoch 1879
Epoch 1879 :: Batch 0/1
Batch Loss = 0.000730994379157247
1879, epoch_train_loss=0.000730994379157247
Epoch 1880
Epoch 1880 :: Batch 0/1
Batch Loss = 0.0007309943782464487
1880, epoch_train_loss=0.0007309943782464487
Epoch 1881
Epoch 1881 :: Batch 0/1
Batch Loss = 0.00073099437733631
1881, epoch_train_loss=0.00073099437733631
Epoch 1882
Epoch 1882 :: Batch 0/1
Batch Loss = 0.0007309943764268307
1882, epoch_train_loss=0.0007309943764268307
Epoch 1883
Epoch 1883 :: Batch 0/1
Batch Loss = 0.0007309943755180101
1883, epoch_train_loss=0.0007309943755180101
Epoch 1884
Epoch 1884 :: Batch 0/1
Batch Loss = 0.0007309943746098476
1884, epoch_train_loss=0.0007309943746098476
Epoch 1885
Epoch 1885 :: Batch 0/1
Batch Loss = 0.0007309943737023432
1885, epoch_train_loss=0.0007309943737023432
Epoch 1886
Epoch 1886 :: Batch 0/1
Batch Loss = 0.0007309943727954959
1886, epoch_train_loss=0.0007309943727954959
Epoch 1887
Epoch 1887 :: Batch 0/1
Batch Loss = 0.0007309943718893055
1887, epoch_train_loss=0.0007309943718893055
Epoch 1888
Epoch 1888 :: Batch 0/1
Batch Loss = 0.0007309943709837712
1888, epoch_train_loss=0.0007309943709837712
Epoch 1889
Epoch 1889 :: Batch 0/1
Batch Loss = 0.0007309943700788927
1889, epoch_train_loss=0.0007309943700788927
Epoch 1890
Epoch 1890 :: Batch 0/1
Batch Loss = 0.0007309943691746694
1890, epoch_train_loss=0.0007309943691746694
Epoch 1891
Epoch 1891 :: Batch 0/1
Batch Loss = 0.0007309943682711011
1891, epoch_train_loss=0.0007309943682711011
Epoch 1892
Epoch 1892 :: Batch 0/1
Batch Loss = 0.0007309943673681869
1892, epoch_train_loss=0.0007309943673681869
Epoch 1893
Epoch 1893 :: Batch 0/1
Batch Loss = 0.0007309943664659264
1893, epoch_train_loss=0.0007309943664659264
Epoch 1894
Epoch 1894 :: Batch 0/1
Batch Loss = 0.0007309943655643193
1894, epoch_train_loss=0.0007309943655643193
Epoch 1895
Epoch 1895 :: Batch 0/1
Batch Loss = 0.000730994364663365
1895, epoch_train_loss=0.000730994364663365
Epoch 1896
Epoch 1896 :: Batch 0/1
Batch Loss = 0.0007309943637630628
1896, epoch_train_loss=0.0007309943637630628
Epoch 1897
Epoch 1897 :: Batch 0/1
Batch Loss = 0.0007309943628634124
1897, epoch_train_loss=0.0007309943628634124
Epoch 1898
Epoch 1898 :: Batch 0/1
Batch Loss = 0.0007309943619644133
1898, epoch_train_loss=0.0007309943619644133
Epoch 1899
Epoch 1899 :: Batch 0/1
Batch Loss = 0.000730994361066065
1899, epoch_train_loss=0.000730994361066065
Epoch 1900
Epoch 1900 :: Batch 0/1
Batch Loss = 0.000730994360168367
1900, epoch_train_loss=0.000730994360168367
Epoch 1901
Epoch 1901 :: Batch 0/1
Batch Loss = 0.0007309943592713186
1901, epoch_train_loss=0.0007309943592713186
Epoch 1902
Epoch 1902 :: Batch 0/1
Batch Loss = 0.0007309943583749196
1902, epoch_train_loss=0.0007309943583749196
Epoch 1903
Epoch 1903 :: Batch 0/1
Batch Loss = 0.0007309943574791695
1903, epoch_train_loss=0.0007309943574791695
Epoch 1904
Epoch 1904 :: Batch 0/1
Batch Loss = 0.0007309943565840674
1904, epoch_train_loss=0.0007309943565840674
Epoch 1905
Epoch 1905 :: Batch 0/1
Batch Loss = 0.0007309943556896133
1905, epoch_train_loss=0.0007309943556896133
Epoch 1906
Epoch 1906 :: Batch 0/1
Batch Loss = 0.0007309943547958063
1906, epoch_train_loss=0.0007309943547958063
Epoch 1907
Epoch 1907 :: Batch 0/1
Batch Loss = 0.0007309943539026461
1907, epoch_train_loss=0.0007309943539026461
Epoch 1908
Epoch 1908 :: Batch 0/1
Batch Loss = 0.0007309943530101322
1908, epoch_train_loss=0.0007309943530101322
Epoch 1909
Epoch 1909 :: Batch 0/1
Batch Loss = 0.0007309943521182641
1909, epoch_train_loss=0.0007309943521182641
Epoch 1910
Epoch 1910 :: Batch 0/1
Batch Loss = 0.0007309943512270414
1910, epoch_train_loss=0.0007309943512270414
Epoch 1911
Epoch 1911 :: Batch 0/1
Batch Loss = 0.0007309943503364633
1911, epoch_train_loss=0.0007309943503364633
Epoch 1912
Epoch 1912 :: Batch 0/1
Batch Loss = 0.0007309943494465298
1912, epoch_train_loss=0.0007309943494465298
Epoch 1913
Epoch 1913 :: Batch 0/1
Batch Loss = 0.0007309943485572398
1913, epoch_train_loss=0.0007309943485572398
Epoch 1914
Epoch 1914 :: Batch 0/1
Batch Loss = 0.0007309943476685932
1914, epoch_train_loss=0.0007309943476685932
Epoch 1915
Epoch 1915 :: Batch 0/1
Batch Loss = 0.0007309943467805896
1915, epoch_train_loss=0.0007309943467805896
Epoch 1916
Epoch 1916 :: Batch 0/1
Batch Loss = 0.000730994345893228
1916, epoch_train_loss=0.000730994345893228
Epoch 1917
Epoch 1917 :: Batch 0/1
Batch Loss = 0.0007309943450065085
1917, epoch_train_loss=0.0007309943450065085
Epoch 1918
Epoch 1918 :: Batch 0/1
Batch Loss = 0.0007309943441204302
1918, epoch_train_loss=0.0007309943441204302
Epoch 1919
Epoch 1919 :: Batch 0/1
Batch Loss = 0.0007309943432349926
1919, epoch_train_loss=0.0007309943432349926
Epoch 1920
Epoch 1920 :: Batch 0/1
Batch Loss = 0.0007309943423501955
1920, epoch_train_loss=0.0007309943423501955
Epoch 1921
Epoch 1921 :: Batch 0/1
Batch Loss = 0.0007309943414660383
1921, epoch_train_loss=0.0007309943414660383
Epoch 1922
Epoch 1922 :: Batch 0/1
Batch Loss = 0.0007309943405825204
1922, epoch_train_loss=0.0007309943405825204
Epoch 1923
Epoch 1923 :: Batch 0/1
Batch Loss = 0.0007309943396996414
1923, epoch_train_loss=0.0007309943396996414
Epoch 1924
Epoch 1924 :: Batch 0/1
Batch Loss = 0.0007309943388174006
1924, epoch_train_loss=0.0007309943388174006
Epoch 1925
Epoch 1925 :: Batch 0/1
Batch Loss = 0.0007309943379357978
1925, epoch_train_loss=0.0007309943379357978
Epoch 1926
Epoch 1926 :: Batch 0/1
Batch Loss = 0.0007309943370548324
1926, epoch_train_loss=0.0007309943370548324
Epoch 1927
Epoch 1927 :: Batch 0/1
Batch Loss = 0.0007309943361745039
1927, epoch_train_loss=0.0007309943361745039
Epoch 1928
Epoch 1928 :: Batch 0/1
Batch Loss = 0.0007309943352948118
1928, epoch_train_loss=0.0007309943352948118
Epoch 1929
Epoch 1929 :: Batch 0/1
Batch Loss = 0.0007309943344157555
1929, epoch_train_loss=0.0007309943344157555
Epoch 1930
Epoch 1930 :: Batch 0/1
Batch Loss = 0.0007309943335373347
1930, epoch_train_loss=0.0007309943335373347
Epoch 1931
Epoch 1931 :: Batch 0/1
Batch Loss = 0.0007309943326595489
1931, epoch_train_loss=0.0007309943326595489
Epoch 1932
Epoch 1932 :: Batch 0/1
Batch Loss = 0.0007309943317823973
1932, epoch_train_loss=0.0007309943317823973
Epoch 1933
Epoch 1933 :: Batch 0/1
Batch Loss = 0.0007309943309058797
1933, epoch_train_loss=0.0007309943309058797
Epoch 1934
Epoch 1934 :: Batch 0/1
Batch Loss = 0.0007309943300299957
1934, epoch_train_loss=0.0007309943300299957
Epoch 1935
Epoch 1935 :: Batch 0/1
Batch Loss = 0.0007309943291547447
1935, epoch_train_loss=0.0007309943291547447
Epoch 1936
Epoch 1936 :: Batch 0/1
Batch Loss = 0.0007309943282801261
1936, epoch_train_loss=0.0007309943282801261
Epoch 1937
Epoch 1937 :: Batch 0/1
Batch Loss = 0.0007309943274061395
1937, epoch_train_loss=0.0007309943274061395
Epoch 1938
Epoch 1938 :: Batch 0/1
Batch Loss = 0.0007309943265327843
1938, epoch_train_loss=0.0007309943265327843
Epoch 1939
Epoch 1939 :: Batch 0/1
Batch Loss = 0.0007309943256600602
1939, epoch_train_loss=0.0007309943256600602
Epoch 1940
Epoch 1940 :: Batch 0/1
Batch Loss = 0.0007309943247879668
1940, epoch_train_loss=0.0007309943247879668
Epoch 1941
Epoch 1941 :: Batch 0/1
Batch Loss = 0.0007309943239165032
1941, epoch_train_loss=0.0007309943239165032
Epoch 1942
Epoch 1942 :: Batch 0/1
Batch Loss = 0.0007309943230456692
1942, epoch_train_loss=0.0007309943230456692
Epoch 1943
Epoch 1943 :: Batch 0/1
Batch Loss = 0.0007309943221754643
1943, epoch_train_loss=0.0007309943221754643
Epoch 1944
Epoch 1944 :: Batch 0/1
Batch Loss = 0.0007309943213058879
1944, epoch_train_loss=0.0007309943213058879
Epoch 1945
Epoch 1945 :: Batch 0/1
Batch Loss = 0.0007309943204369397
1945, epoch_train_loss=0.0007309943204369397
Epoch 1946
Epoch 1946 :: Batch 0/1
Batch Loss = 0.000730994319568619
1946, epoch_train_loss=0.000730994319568619
Epoch 1947
Epoch 1947 :: Batch 0/1
Batch Loss = 0.0007309943187009252
1947, epoch_train_loss=0.0007309943187009252
Epoch 1948
Epoch 1948 :: Batch 0/1
Batch Loss = 0.0007309943178338583
1948, epoch_train_loss=0.0007309943178338583
Epoch 1949
Epoch 1949 :: Batch 0/1
Batch Loss = 0.0007309943169674176
1949, epoch_train_loss=0.0007309943169674176
Epoch 1950
Epoch 1950 :: Batch 0/1
Batch Loss = 0.0007309943161016024
1950, epoch_train_loss=0.0007309943161016024
Epoch 1951
Epoch 1951 :: Batch 0/1
Batch Loss = 0.0007309943152364123
1951, epoch_train_loss=0.0007309943152364123
Epoch 1952
Epoch 1952 :: Batch 0/1
Batch Loss = 0.000730994314371847
1952, epoch_train_loss=0.000730994314371847
Epoch 1953
Epoch 1953 :: Batch 0/1
Batch Loss = 0.0007309943135079059
1953, epoch_train_loss=0.0007309943135079059
Epoch 1954
Epoch 1954 :: Batch 0/1
Batch Loss = 0.0007309943126445885
1954, epoch_train_loss=0.0007309943126445885
Epoch 1955
Epoch 1955 :: Batch 0/1
Batch Loss = 0.0007309943117818942
1955, epoch_train_loss=0.0007309943117818942
Epoch 1956
Epoch 1956 :: Batch 0/1
Batch Loss = 0.0007309943109198228
1956, epoch_train_loss=0.0007309943109198228
Epoch 1957
Epoch 1957 :: Batch 0/1
Batch Loss = 0.0007309943100583735
1957, epoch_train_loss=0.0007309943100583735
Epoch 1958
Epoch 1958 :: Batch 0/1
Batch Loss = 0.0007309943091975463
1958, epoch_train_loss=0.0007309943091975463
Epoch 1959
Epoch 1959 :: Batch 0/1
Batch Loss = 0.00073099430833734
1959, epoch_train_loss=0.00073099430833734
Epoch 1960
Epoch 1960 :: Batch 0/1
Batch Loss = 0.0007309943074777546
1960, epoch_train_loss=0.0007309943074777546
Epoch 1961
Epoch 1961 :: Batch 0/1
Batch Loss = 0.0007309943066187897
1961, epoch_train_loss=0.0007309943066187897
Epoch 1962
Epoch 1962 :: Batch 0/1
Batch Loss = 0.0007309943057604445
1962, epoch_train_loss=0.0007309943057604445
Epoch 1963
Epoch 1963 :: Batch 0/1
Batch Loss = 0.0007309943049027187
1963, epoch_train_loss=0.0007309943049027187
Epoch 1964
Epoch 1964 :: Batch 0/1
Batch Loss = 0.0007309943040456118
1964, epoch_train_loss=0.0007309943040456118
Epoch 1965
Epoch 1965 :: Batch 0/1
Batch Loss = 0.0007309943031891233
1965, epoch_train_loss=0.0007309943031891233
Epoch 1966
Epoch 1966 :: Batch 0/1
Batch Loss = 0.0007309943023332527
1966, epoch_train_loss=0.0007309943023332527
Epoch 1967
Epoch 1967 :: Batch 0/1
Batch Loss = 0.0007309943014779995
1967, epoch_train_loss=0.0007309943014779995
Epoch 1968
Epoch 1968 :: Batch 0/1
Batch Loss = 0.0007309943006233634
1968, epoch_train_loss=0.0007309943006233634
Epoch 1969
Epoch 1969 :: Batch 0/1
Batch Loss = 0.0007309942997693436
1969, epoch_train_loss=0.0007309942997693436
Epoch 1970
Epoch 1970 :: Batch 0/1
Batch Loss = 0.0007309942989159399
1970, epoch_train_loss=0.0007309942989159399
Epoch 1971
Epoch 1971 :: Batch 0/1
Batch Loss = 0.0007309942980631516
1971, epoch_train_loss=0.0007309942980631516
Epoch 1972
Epoch 1972 :: Batch 0/1
Batch Loss = 0.0007309942972109784
1972, epoch_train_loss=0.0007309942972109784
Epoch 1973
Epoch 1973 :: Batch 0/1
Batch Loss = 0.0007309942963594196
1973, epoch_train_loss=0.0007309942963594196
Epoch 1974
Epoch 1974 :: Batch 0/1
Batch Loss = 0.0007309942955084752
1974, epoch_train_loss=0.0007309942955084752
Epoch 1975
Epoch 1975 :: Batch 0/1
Batch Loss = 0.0007309942946581442
1975, epoch_train_loss=0.0007309942946581442
Epoch 1976
Epoch 1976 :: Batch 0/1
Batch Loss = 0.0007309942938084263
1976, epoch_train_loss=0.0007309942938084263
Epoch 1977
Epoch 1977 :: Batch 0/1
Batch Loss = 0.0007309942929593211
1977, epoch_train_loss=0.0007309942929593211
Epoch 1978
Epoch 1978 :: Batch 0/1
Batch Loss = 0.0007309942921108279
1978, epoch_train_loss=0.0007309942921108279
Epoch 1979
Epoch 1979 :: Batch 0/1
Batch Loss = 0.0007309942912629466
1979, epoch_train_loss=0.0007309942912629466
Epoch 1980
Epoch 1980 :: Batch 0/1
Batch Loss = 0.0007309942904156764
1980, epoch_train_loss=0.0007309942904156764
Epoch 1981
Epoch 1981 :: Batch 0/1
Batch Loss = 0.000730994289569017
1981, epoch_train_loss=0.000730994289569017
Epoch 1982
Epoch 1982 :: Batch 0/1
Batch Loss = 0.0007309942887229677
1982, epoch_train_loss=0.0007309942887229677
Epoch 1983
Epoch 1983 :: Batch 0/1
Batch Loss = 0.0007309942878775282
1983, epoch_train_loss=0.0007309942878775282
Epoch 1984
Epoch 1984 :: Batch 0/1
Batch Loss = 0.0007309942870326982
1984, epoch_train_loss=0.0007309942870326982
Epoch 1985
Epoch 1985 :: Batch 0/1
Batch Loss = 0.0007309942861884769
1985, epoch_train_loss=0.0007309942861884769
Epoch 1986
Epoch 1986 :: Batch 0/1
Batch Loss = 0.0007309942853448638
1986, epoch_train_loss=0.0007309942853448638
Epoch 1987
Epoch 1987 :: Batch 0/1
Batch Loss = 0.0007309942845018589
1987, epoch_train_loss=0.0007309942845018589
Epoch 1988
Epoch 1988 :: Batch 0/1
Batch Loss = 0.000730994283659461
1988, epoch_train_loss=0.000730994283659461
Epoch 1989
Epoch 1989 :: Batch 0/1
Batch Loss = 0.0007309942828176703
1989, epoch_train_loss=0.0007309942828176703
Epoch 1990
Epoch 1990 :: Batch 0/1
Batch Loss = 0.0007309942819764861
1990, epoch_train_loss=0.0007309942819764861
Epoch 1991
Epoch 1991 :: Batch 0/1
Batch Loss = 0.0007309942811359076
1991, epoch_train_loss=0.0007309942811359076
Epoch 1992
Epoch 1992 :: Batch 0/1
Batch Loss = 0.0007309942802959347
1992, epoch_train_loss=0.0007309942802959347
Epoch 1993
Epoch 1993 :: Batch 0/1
Batch Loss = 0.0007309942794565668
1993, epoch_train_loss=0.0007309942794565668
Epoch 1994
Epoch 1994 :: Batch 0/1
Batch Loss = 0.0007309942786178036
1994, epoch_train_loss=0.0007309942786178036
Epoch 1995
Epoch 1995 :: Batch 0/1
Batch Loss = 0.0007309942777796443
1995, epoch_train_loss=0.0007309942777796443
Epoch 1996
Epoch 1996 :: Batch 0/1
Batch Loss = 0.0007309942769420887
1996, epoch_train_loss=0.0007309942769420887
Epoch 1997
Epoch 1997 :: Batch 0/1
Batch Loss = 0.0007309942761051362
1997, epoch_train_loss=0.0007309942761051362
Epoch 1998
Epoch 1998 :: Batch 0/1
Batch Loss = 0.0007309942752687864
1998, epoch_train_loss=0.0007309942752687864
Epoch 1999
Epoch 1999 :: Batch 0/1
Batch Loss = 0.0007309942744330388
1999, epoch_train_loss=0.0007309942744330388
Epoch 2000
Epoch 2000 :: Batch 0/1
Batch Loss = 0.0007309942735978927
2000, epoch_train_loss=0.0007309942735978927
Epoch 2001
Epoch 2001 :: Batch 0/1
Batch Loss = 0.000730994272763348
2001, epoch_train_loss=0.000730994272763348
Epoch 2002
Epoch 2002 :: Batch 0/1
Batch Loss = 0.000730994271929404
2002, epoch_train_loss=0.000730994271929404
Epoch 2003
Epoch 2003 :: Batch 0/1
Batch Loss = 0.0007309942710960605
2003, epoch_train_loss=0.0007309942710960605
Epoch 2004
Epoch 2004 :: Batch 0/1
Batch Loss = 0.0007309942702633167
2004, epoch_train_loss=0.0007309942702633167
Epoch 2005
Epoch 2005 :: Batch 0/1
Batch Loss = 0.0007309942694311721
2005, epoch_train_loss=0.0007309942694311721
Epoch 2006
Epoch 2006 :: Batch 0/1
Batch Loss = 0.0007309942685996266
2006, epoch_train_loss=0.0007309942685996266
Epoch 2007
Epoch 2007 :: Batch 0/1
Batch Loss = 0.0007309942677686794
2007, epoch_train_loss=0.0007309942677686794
Epoch 2008
Epoch 2008 :: Batch 0/1
Batch Loss = 0.0007309942669383304
2008, epoch_train_loss=0.0007309942669383304
Epoch 2009
Epoch 2009 :: Batch 0/1
Batch Loss = 0.0007309942661085784
2009, epoch_train_loss=0.0007309942661085784
Epoch 2010
Epoch 2010 :: Batch 0/1
Batch Loss = 0.0007309942652794239
2010, epoch_train_loss=0.0007309942652794239
Epoch 2011
Epoch 2011 :: Batch 0/1
Batch Loss = 0.0007309942644508657
2011, epoch_train_loss=0.0007309942644508657
Epoch 2012
Epoch 2012 :: Batch 0/1
Batch Loss = 0.0007309942636229036
2012, epoch_train_loss=0.0007309942636229036
Epoch 2013
Epoch 2013 :: Batch 0/1
Batch Loss = 0.0007309942627955371
2013, epoch_train_loss=0.0007309942627955371
Epoch 2014
Epoch 2014 :: Batch 0/1
Batch Loss = 0.0007309942619687657
2014, epoch_train_loss=0.0007309942619687657
Epoch 2015
Epoch 2015 :: Batch 0/1
Batch Loss = 0.0007309942611425891
2015, epoch_train_loss=0.0007309942611425891
Epoch 2016
Epoch 2016 :: Batch 0/1
Batch Loss = 0.0007309942603170066
2016, epoch_train_loss=0.0007309942603170066
Epoch 2017
Epoch 2017 :: Batch 0/1
Batch Loss = 0.0007309942594920178
2017, epoch_train_loss=0.0007309942594920178
Epoch 2018
Epoch 2018 :: Batch 0/1
Batch Loss = 0.0007309942586676222
2018, epoch_train_loss=0.0007309942586676222
Epoch 2019
Epoch 2019 :: Batch 0/1
Batch Loss = 0.0007309942578438195
2019, epoch_train_loss=0.0007309942578438195
Epoch 2020
Epoch 2020 :: Batch 0/1
Batch Loss = 0.0007309942570206091
2020, epoch_train_loss=0.0007309942570206091
Epoch 2021
Epoch 2021 :: Batch 0/1
Batch Loss = 0.0007309942561979907
2021, epoch_train_loss=0.0007309942561979907
Epoch 2022
Epoch 2022 :: Batch 0/1
Batch Loss = 0.0007309942553759636
2022, epoch_train_loss=0.0007309942553759636
Epoch 2023
Epoch 2023 :: Batch 0/1
Batch Loss = 0.0007309942545545274
2023, epoch_train_loss=0.0007309942545545274
Epoch 2024
Epoch 2024 :: Batch 0/1
Batch Loss = 0.000730994253733682
2024, epoch_train_loss=0.000730994253733682
Epoch 2025
Epoch 2025 :: Batch 0/1
Batch Loss = 0.0007309942529134262
2025, epoch_train_loss=0.0007309942529134262
Epoch 2026
Epoch 2026 :: Batch 0/1
Batch Loss = 0.0007309942520937603
2026, epoch_train_loss=0.0007309942520937603
Epoch 2027
Epoch 2027 :: Batch 0/1
Batch Loss = 0.0007309942512746832
2027, epoch_train_loss=0.0007309942512746832
Epoch 2028
Epoch 2028 :: Batch 0/1
Batch Loss = 0.000730994250456195
2028, epoch_train_loss=0.000730994250456195
Epoch 2029
Epoch 2029 :: Batch 0/1
Batch Loss = 0.0007309942496382945
2029, epoch_train_loss=0.0007309942496382945
Epoch 2030
Epoch 2030 :: Batch 0/1
Batch Loss = 0.0007309942488209819
2030, epoch_train_loss=0.0007309942488209819
Epoch 2031
Epoch 2031 :: Batch 0/1
Batch Loss = 0.0007309942480042567
2031, epoch_train_loss=0.0007309942480042567
Epoch 2032
Epoch 2032 :: Batch 0/1
Batch Loss = 0.0007309942471881182
2032, epoch_train_loss=0.0007309942471881182
Epoch 2033
Epoch 2033 :: Batch 0/1
Batch Loss = 0.0007309942463725659
2033, epoch_train_loss=0.0007309942463725659
Epoch 2034
Epoch 2034 :: Batch 0/1
Batch Loss = 0.0007309942455575995
2034, epoch_train_loss=0.0007309942455575995
Epoch 2035
Epoch 2035 :: Batch 0/1
Batch Loss = 0.0007309942447432184
2035, epoch_train_loss=0.0007309942447432184
Epoch 2036
Epoch 2036 :: Batch 0/1
Batch Loss = 0.0007309942439294223
2036, epoch_train_loss=0.0007309942439294223
Epoch 2037
Epoch 2037 :: Batch 0/1
Batch Loss = 0.0007309942431162108
2037, epoch_train_loss=0.0007309942431162108
Epoch 2038
Epoch 2038 :: Batch 0/1
Batch Loss = 0.0007309942423035832
2038, epoch_train_loss=0.0007309942423035832
Epoch 2039
Epoch 2039 :: Batch 0/1
Batch Loss = 0.000730994241491539
2039, epoch_train_loss=0.000730994241491539
Epoch 2040
Epoch 2040 :: Batch 0/1
Batch Loss = 0.0007309942406800778
2040, epoch_train_loss=0.0007309942406800778
Epoch 2041
Epoch 2041 :: Batch 0/1
Batch Loss = 0.0007309942398691995
2041, epoch_train_loss=0.0007309942398691995
Epoch 2042
Epoch 2042 :: Batch 0/1
Batch Loss = 0.0007309942390589032
2042, epoch_train_loss=0.0007309942390589032
Epoch 2043
Epoch 2043 :: Batch 0/1
Batch Loss = 0.0007309942382491886
2043, epoch_train_loss=0.0007309942382491886
Epoch 2044
Epoch 2044 :: Batch 0/1
Batch Loss = 0.0007309942374400552
2044, epoch_train_loss=0.0007309942374400552
Epoch 2045
Epoch 2045 :: Batch 0/1
Batch Loss = 0.0007309942366315028
2045, epoch_train_loss=0.0007309942366315028
Epoch 2046
Epoch 2046 :: Batch 0/1
Batch Loss = 0.0007309942358235305
2046, epoch_train_loss=0.0007309942358235305
Epoch 2047
Epoch 2047 :: Batch 0/1
Batch Loss = 0.0007309942350161383
2047, epoch_train_loss=0.0007309942350161383
Epoch 2048
Epoch 2048 :: Batch 0/1
Batch Loss = 0.0007309942342093252
2048, epoch_train_loss=0.0007309942342093252
Epoch 2049
Epoch 2049 :: Batch 0/1
Batch Loss = 0.0007309942334030912
2049, epoch_train_loss=0.0007309942334030912
Epoch 2050
Epoch 2050 :: Batch 0/1
Batch Loss = 0.0007309942325974357
2050, epoch_train_loss=0.0007309942325974357
Epoch 2051
Epoch 2051 :: Batch 0/1
Batch Loss = 0.0007309942317923582
2051, epoch_train_loss=0.0007309942317923582
Epoch 2052
Epoch 2052 :: Batch 0/1
Batch Loss = 0.0007309942309878584
2052, epoch_train_loss=0.0007309942309878584
Epoch 2053
Epoch 2053 :: Batch 0/1
Batch Loss = 0.0007309942301839356
2053, epoch_train_loss=0.0007309942301839356
Epoch 2054
Epoch 2054 :: Batch 0/1
Batch Loss = 0.0007309942293805895
2054, epoch_train_loss=0.0007309942293805895
Epoch 2055
Epoch 2055 :: Batch 0/1
Batch Loss = 0.0007309942285778197
2055, epoch_train_loss=0.0007309942285778197
Epoch 2056
Epoch 2056 :: Batch 0/1
Batch Loss = 0.0007309942277756255
2056, epoch_train_loss=0.0007309942277756255
Epoch 2057
Epoch 2057 :: Batch 0/1
Batch Loss = 0.0007309942269740065
2057, epoch_train_loss=0.0007309942269740065
Epoch 2058
Epoch 2058 :: Batch 0/1
Batch Loss = 0.0007309942261729624
2058, epoch_train_loss=0.0007309942261729624
Epoch 2059
Epoch 2059 :: Batch 0/1
Batch Loss = 0.0007309942253724929
2059, epoch_train_loss=0.0007309942253724929
Epoch 2060
Epoch 2060 :: Batch 0/1
Batch Loss = 0.0007309942245725972
2060, epoch_train_loss=0.0007309942245725972
Epoch 2061
Epoch 2061 :: Batch 0/1
Batch Loss = 0.000730994223773275
2061, epoch_train_loss=0.000730994223773275
Epoch 2062
Epoch 2062 :: Batch 0/1
Batch Loss = 0.0007309942229745258
2062, epoch_train_loss=0.0007309942229745258
Epoch 2063
Epoch 2063 :: Batch 0/1
Batch Loss = 0.0007309942221763493
2063, epoch_train_loss=0.0007309942221763493
Epoch 2064
Epoch 2064 :: Batch 0/1
Batch Loss = 0.0007309942213787447
2064, epoch_train_loss=0.0007309942213787447
Epoch 2065
Epoch 2065 :: Batch 0/1
Batch Loss = 0.0007309942205817119
2065, epoch_train_loss=0.0007309942205817119
Epoch 2066
Epoch 2066 :: Batch 0/1
Batch Loss = 0.0007309942197852502
2066, epoch_train_loss=0.0007309942197852502
Epoch 2067
Epoch 2067 :: Batch 0/1
Batch Loss = 0.0007309942189893594
2067, epoch_train_loss=0.0007309942189893594
Epoch 2068
Epoch 2068 :: Batch 0/1
Batch Loss = 0.0007309942181940389
2068, epoch_train_loss=0.0007309942181940389
Epoch 2069
Epoch 2069 :: Batch 0/1
Batch Loss = 0.0007309942173992882
2069, epoch_train_loss=0.0007309942173992882
Epoch 2070
Epoch 2070 :: Batch 0/1
Batch Loss = 0.0007309942166051069
2070, epoch_train_loss=0.0007309942166051069
Epoch 2071
Epoch 2071 :: Batch 0/1
Batch Loss = 0.0007309942158114947
2071, epoch_train_loss=0.0007309942158114947
Epoch 2072
Epoch 2072 :: Batch 0/1
Batch Loss = 0.000730994215018451
2072, epoch_train_loss=0.000730994215018451
Epoch 2073
Epoch 2073 :: Batch 0/1
Batch Loss = 0.0007309942142259753
2073, epoch_train_loss=0.0007309942142259753
Epoch 2074
Epoch 2074 :: Batch 0/1
Batch Loss = 0.0007309942134340671
2074, epoch_train_loss=0.0007309942134340671
Epoch 2075
Epoch 2075 :: Batch 0/1
Batch Loss = 0.000730994212642726
2075, epoch_train_loss=0.000730994212642726
Epoch 2076
Epoch 2076 :: Batch 0/1
Batch Loss = 0.0007309942118519519
2076, epoch_train_loss=0.0007309942118519519
Epoch 2077
Epoch 2077 :: Batch 0/1
Batch Loss = 0.0007309942110617437
2077, epoch_train_loss=0.0007309942110617437
Epoch 2078
Epoch 2078 :: Batch 0/1
Batch Loss = 0.0007309942102721015
2078, epoch_train_loss=0.0007309942102721015
Epoch 2079
Epoch 2079 :: Batch 0/1
Batch Loss = 0.0007309942094830247
2079, epoch_train_loss=0.0007309942094830247
Epoch 2080
Epoch 2080 :: Batch 0/1
Batch Loss = 0.0007309942086945127
2080, epoch_train_loss=0.0007309942086945127
Epoch 2081
Epoch 2081 :: Batch 0/1
Batch Loss = 0.0007309942079065652
2081, epoch_train_loss=0.0007309942079065652
Epoch 2082
Epoch 2082 :: Batch 0/1
Batch Loss = 0.0007309942071191818
2082, epoch_train_loss=0.0007309942071191818
Epoch 2083
Epoch 2083 :: Batch 0/1
Batch Loss = 0.0007309942063323619
2083, epoch_train_loss=0.0007309942063323619
Epoch 2084
Epoch 2084 :: Batch 0/1
Batch Loss = 0.0007309942055461051
2084, epoch_train_loss=0.0007309942055461051
Epoch 2085
Epoch 2085 :: Batch 0/1
Batch Loss = 0.000730994204760411
2085, epoch_train_loss=0.000730994204760411
Epoch 2086
Epoch 2086 :: Batch 0/1
Batch Loss = 0.0007309942039752792
2086, epoch_train_loss=0.0007309942039752792
Epoch 2087
Epoch 2087 :: Batch 0/1
Batch Loss = 0.0007309942031907092
2087, epoch_train_loss=0.0007309942031907092
Epoch 2088
Epoch 2088 :: Batch 0/1
Batch Loss = 0.0007309942024067005
2088, epoch_train_loss=0.0007309942024067005
Epoch 2089
Epoch 2089 :: Batch 0/1
Batch Loss = 0.0007309942016232525
2089, epoch_train_loss=0.0007309942016232525
Epoch 2090
Epoch 2090 :: Batch 0/1
Batch Loss = 0.0007309942008403652
2090, epoch_train_loss=0.0007309942008403652
Epoch 2091
Epoch 2091 :: Batch 0/1
Batch Loss = 0.0007309942000580377
2091, epoch_train_loss=0.0007309942000580377
Epoch 2092
Epoch 2092 :: Batch 0/1
Batch Loss = 0.0007309941992762697
2092, epoch_train_loss=0.0007309941992762697
Epoch 2093
Epoch 2093 :: Batch 0/1
Batch Loss = 0.0007309941984950612
2093, epoch_train_loss=0.0007309941984950612
Epoch 2094
Epoch 2094 :: Batch 0/1
Batch Loss = 0.000730994197714411
2094, epoch_train_loss=0.000730994197714411
Epoch 2095
Epoch 2095 :: Batch 0/1
Batch Loss = 0.0007309941969343192
2095, epoch_train_loss=0.0007309941969343192
Epoch 2096
Epoch 2096 :: Batch 0/1
Batch Loss = 0.0007309941961547851
2096, epoch_train_loss=0.0007309941961547851
Epoch 2097
Epoch 2097 :: Batch 0/1
Batch Loss = 0.0007309941953758084
2097, epoch_train_loss=0.0007309941953758084
Epoch 2098
Epoch 2098 :: Batch 0/1
Batch Loss = 0.0007309941945973886
2098, epoch_train_loss=0.0007309941945973886
Epoch 2099
Epoch 2099 :: Batch 0/1
Batch Loss = 0.0007309941938195252
2099, epoch_train_loss=0.0007309941938195252
Epoch 2100
Epoch 2100 :: Batch 0/1
Batch Loss = 0.0007309941930422178
2100, epoch_train_loss=0.0007309941930422178
Epoch 2101
Epoch 2101 :: Batch 0/1
Batch Loss = 0.000730994192265466
2101, epoch_train_loss=0.000730994192265466
Epoch 2102
Epoch 2102 :: Batch 0/1
Batch Loss = 0.0007309941914892694
2102, epoch_train_loss=0.0007309941914892694
Epoch 2103
Epoch 2103 :: Batch 0/1
Batch Loss = 0.0007309941907136274
2103, epoch_train_loss=0.0007309941907136274
Epoch 2104
Epoch 2104 :: Batch 0/1
Batch Loss = 0.0007309941899385397
2104, epoch_train_loss=0.0007309941899385397
Epoch 2105
Epoch 2105 :: Batch 0/1
Batch Loss = 0.0007309941891640059
2105, epoch_train_loss=0.0007309941891640059
Epoch 2106
Epoch 2106 :: Batch 0/1
Batch Loss = 0.0007309941883900253
2106, epoch_train_loss=0.0007309941883900253
Epoch 2107
Epoch 2107 :: Batch 0/1
Batch Loss = 0.0007309941876165977
2107, epoch_train_loss=0.0007309941876165977
Epoch 2108
Epoch 2108 :: Batch 0/1
Batch Loss = 0.0007309941868437225
2108, epoch_train_loss=0.0007309941868437225
Epoch 2109
Epoch 2109 :: Batch 0/1
Batch Loss = 0.0007309941860713994
2109, epoch_train_loss=0.0007309941860713994
Epoch 2110
Epoch 2110 :: Batch 0/1
Batch Loss = 0.0007309941852996277
2110, epoch_train_loss=0.0007309941852996277
Epoch 2111
Epoch 2111 :: Batch 0/1
Batch Loss = 0.0007309941845284075
2111, epoch_train_loss=0.0007309941845284075
Epoch 2112
Epoch 2112 :: Batch 0/1
Batch Loss = 0.000730994183757738
2112, epoch_train_loss=0.000730994183757738
Epoch 2113
Epoch 2113 :: Batch 0/1
Batch Loss = 0.0007309941829876185
2113, epoch_train_loss=0.0007309941829876185
Epoch 2114
Epoch 2114 :: Batch 0/1
Batch Loss = 0.000730994182218049
2114, epoch_train_loss=0.000730994182218049
Epoch 2115
Epoch 2115 :: Batch 0/1
Batch Loss = 0.0007309941814490289
2115, epoch_train_loss=0.0007309941814490289
Epoch 2116
Epoch 2116 :: Batch 0/1
Batch Loss = 0.0007309941806805579
2116, epoch_train_loss=0.0007309941806805579
Epoch 2117
Epoch 2117 :: Batch 0/1
Batch Loss = 0.0007309941799126354
2117, epoch_train_loss=0.0007309941799126354
Epoch 2118
Epoch 2118 :: Batch 0/1
Batch Loss = 0.0007309941791452608
2118, epoch_train_loss=0.0007309941791452608
Epoch 2119
Epoch 2119 :: Batch 0/1
Batch Loss = 0.000730994178378434
2119, epoch_train_loss=0.000730994178378434
Epoch 2120
Epoch 2120 :: Batch 0/1
Batch Loss = 0.0007309941776121544
2120, epoch_train_loss=0.0007309941776121544
Epoch 2121
Epoch 2121 :: Batch 0/1
Batch Loss = 0.0007309941768464216
2121, epoch_train_loss=0.0007309941768464216
Epoch 2122
Epoch 2122 :: Batch 0/1
Batch Loss = 0.0007309941760812352
2122, epoch_train_loss=0.0007309941760812352
Epoch 2123
Epoch 2123 :: Batch 0/1
Batch Loss = 0.0007309941753165947
2123, epoch_train_loss=0.0007309941753165947
Epoch 2124
Epoch 2124 :: Batch 0/1
Batch Loss = 0.0007309941745524996
2124, epoch_train_loss=0.0007309941745524996
Epoch 2125
Epoch 2125 :: Batch 0/1
Batch Loss = 0.0007309941737889496
2125, epoch_train_loss=0.0007309941737889496
Epoch 2126
Epoch 2126 :: Batch 0/1
Batch Loss = 0.0007309941730259444
2126, epoch_train_loss=0.0007309941730259444
Epoch 2127
Epoch 2127 :: Batch 0/1
Batch Loss = 0.0007309941722634832
2127, epoch_train_loss=0.0007309941722634832
Epoch 2128
Epoch 2128 :: Batch 0/1
Batch Loss = 0.0007309941715015659
2128, epoch_train_loss=0.0007309941715015659
Epoch 2129
Epoch 2129 :: Batch 0/1
Batch Loss = 0.000730994170740192
2129, epoch_train_loss=0.000730994170740192
Epoch 2130
Epoch 2130 :: Batch 0/1
Batch Loss = 0.0007309941699793606
2130, epoch_train_loss=0.0007309941699793606
Epoch 2131
Epoch 2131 :: Batch 0/1
Batch Loss = 0.0007309941692190718
2131, epoch_train_loss=0.0007309941692190718
Epoch 2132
Epoch 2132 :: Batch 0/1
Batch Loss = 0.0007309941684593252
2132, epoch_train_loss=0.0007309941684593252
Epoch 2133
Epoch 2133 :: Batch 0/1
Batch Loss = 0.0007309941677001201
2133, epoch_train_loss=0.0007309941677001201
Epoch 2134
Epoch 2134 :: Batch 0/1
Batch Loss = 0.0007309941669414559
2134, epoch_train_loss=0.0007309941669414559
Epoch 2135
Epoch 2135 :: Batch 0/1
Batch Loss = 0.0007309941661833328
2135, epoch_train_loss=0.0007309941661833328
Epoch 2136
Epoch 2136 :: Batch 0/1
Batch Loss = 0.0007309941654257499
2136, epoch_train_loss=0.0007309941654257499
Epoch 2137
Epoch 2137 :: Batch 0/1
Batch Loss = 0.0007309941646687067
2137, epoch_train_loss=0.0007309941646687067
Epoch 2138
Epoch 2138 :: Batch 0/1
Batch Loss = 0.0007309941639122033
2138, epoch_train_loss=0.0007309941639122033
Epoch 2139
Epoch 2139 :: Batch 0/1
Batch Loss = 0.0007309941631562386
2139, epoch_train_loss=0.0007309941631562386
Epoch 2140
Epoch 2140 :: Batch 0/1
Batch Loss = 0.0007309941624008126
2140, epoch_train_loss=0.0007309941624008126
Epoch 2141
Epoch 2141 :: Batch 0/1
Batch Loss = 0.0007309941616459247
2141, epoch_train_loss=0.0007309941616459247
Epoch 2142
Epoch 2142 :: Batch 0/1
Batch Loss = 0.0007309941608915744
2142, epoch_train_loss=0.0007309941608915744
Epoch 2143
Epoch 2143 :: Batch 0/1
Batch Loss = 0.0007309941601377615
2143, epoch_train_loss=0.0007309941601377615
Epoch 2144
Epoch 2144 :: Batch 0/1
Batch Loss = 0.0007309941593844853
2144, epoch_train_loss=0.0007309941593844853
Epoch 2145
Epoch 2145 :: Batch 0/1
Batch Loss = 0.0007309941586317458
2145, epoch_train_loss=0.0007309941586317458
Epoch 2146
Epoch 2146 :: Batch 0/1
Batch Loss = 0.0007309941578795421
2146, epoch_train_loss=0.0007309941578795421
Epoch 2147
Epoch 2147 :: Batch 0/1
Batch Loss = 0.0007309941571278742
2147, epoch_train_loss=0.0007309941571278742
Epoch 2148
Epoch 2148 :: Batch 0/1
Batch Loss = 0.0007309941563767412
2148, epoch_train_loss=0.0007309941563767412
Epoch 2149
Epoch 2149 :: Batch 0/1
Batch Loss = 0.0007309941556261429
2149, epoch_train_loss=0.0007309941556261429
Epoch 2150
Epoch 2150 :: Batch 0/1
Batch Loss = 0.000730994154876079
2150, epoch_train_loss=0.000730994154876079
Epoch 2151
Epoch 2151 :: Batch 0/1
Batch Loss = 0.0007309941541265488
2151, epoch_train_loss=0.0007309941541265488
Epoch 2152
Epoch 2152 :: Batch 0/1
Batch Loss = 0.0007309941533775522
2152, epoch_train_loss=0.0007309941533775522
Epoch 2153
Epoch 2153 :: Batch 0/1
Batch Loss = 0.0007309941526290886
2153, epoch_train_loss=0.0007309941526290886
Epoch 2154
Epoch 2154 :: Batch 0/1
Batch Loss = 0.0007309941518811575
2154, epoch_train_loss=0.0007309941518811575
Epoch 2155
Epoch 2155 :: Batch 0/1
Batch Loss = 0.0007309941511337585
2155, epoch_train_loss=0.0007309941511337585
Epoch 2156
Epoch 2156 :: Batch 0/1
Batch Loss = 0.0007309941503868914
2156, epoch_train_loss=0.0007309941503868914
Epoch 2157
Epoch 2157 :: Batch 0/1
Batch Loss = 0.0007309941496405557
2157, epoch_train_loss=0.0007309941496405557
Epoch 2158
Epoch 2158 :: Batch 0/1
Batch Loss = 0.0007309941488947506
2158, epoch_train_loss=0.0007309941488947506
Epoch 2159
Epoch 2159 :: Batch 0/1
Batch Loss = 0.0007309941481494762
2159, epoch_train_loss=0.0007309941481494762
Epoch 2160
Epoch 2160 :: Batch 0/1
Batch Loss = 0.0007309941474047317
2160, epoch_train_loss=0.0007309941474047317
Epoch 2161
Epoch 2161 :: Batch 0/1
Batch Loss = 0.0007309941466605168
2161, epoch_train_loss=0.0007309941466605168
Epoch 2162
Epoch 2162 :: Batch 0/1
Batch Loss = 0.0007309941459168311
2162, epoch_train_loss=0.0007309941459168311
Epoch 2163
Epoch 2163 :: Batch 0/1
Batch Loss = 0.0007309941451736743
2163, epoch_train_loss=0.0007309941451736743
Epoch 2164
Epoch 2164 :: Batch 0/1
Batch Loss = 0.0007309941444310457
2164, epoch_train_loss=0.0007309941444310457
Epoch 2165
Epoch 2165 :: Batch 0/1
Batch Loss = 0.0007309941436889451
2165, epoch_train_loss=0.0007309941436889451
Epoch 2166
Epoch 2166 :: Batch 0/1
Batch Loss = 0.0007309941429473718
2166, epoch_train_loss=0.0007309941429473718
Epoch 2167
Epoch 2167 :: Batch 0/1
Batch Loss = 0.0007309941422063259
2167, epoch_train_loss=0.0007309941422063259
Epoch 2168
Epoch 2168 :: Batch 0/1
Batch Loss = 0.0007309941414658065
2168, epoch_train_loss=0.0007309941414658065
Epoch 2169
Epoch 2169 :: Batch 0/1
Batch Loss = 0.0007309941407258133
2169, epoch_train_loss=0.0007309941407258133
Epoch 2170
Epoch 2170 :: Batch 0/1
Batch Loss = 0.000730994139986346
2170, epoch_train_loss=0.000730994139986346
Epoch 2171
Epoch 2171 :: Batch 0/1
Batch Loss = 0.000730994139247404
2171, epoch_train_loss=0.000730994139247404
Epoch 2172
Epoch 2172 :: Batch 0/1
Batch Loss = 0.000730994138508987
2172, epoch_train_loss=0.000730994138508987
Epoch 2173
Epoch 2173 :: Batch 0/1
Batch Loss = 0.0007309941377710946
2173, epoch_train_loss=0.0007309941377710946
Epoch 2174
Epoch 2174 :: Batch 0/1
Batch Loss = 0.0007309941370337262
2174, epoch_train_loss=0.0007309941370337262
Epoch 2175
Epoch 2175 :: Batch 0/1
Batch Loss = 0.0007309941362968817
2175, epoch_train_loss=0.0007309941362968817
Epoch 2176
Epoch 2176 :: Batch 0/1
Batch Loss = 0.0007309941355605604
2176, epoch_train_loss=0.0007309941355605604
Epoch 2177
Epoch 2177 :: Batch 0/1
Batch Loss = 0.0007309941348247619
2177, epoch_train_loss=0.0007309941348247619
Epoch 2178
Epoch 2178 :: Batch 0/1
Batch Loss = 0.000730994134089486
2178, epoch_train_loss=0.000730994134089486
Epoch 2179
Epoch 2179 :: Batch 0/1
Batch Loss = 0.0007309941333547321
2179, epoch_train_loss=0.0007309941333547321
Epoch 2180
Epoch 2180 :: Batch 0/1
Batch Loss = 0.0007309941326204998
2180, epoch_train_loss=0.0007309941326204998
Epoch 2181
Epoch 2181 :: Batch 0/1
Batch Loss = 0.0007309941318867887
2181, epoch_train_loss=0.0007309941318867887
Epoch 2182
Epoch 2182 :: Batch 0/1
Batch Loss = 0.0007309941311535984
2182, epoch_train_loss=0.0007309941311535984
Epoch 2183
Epoch 2183 :: Batch 0/1
Batch Loss = 0.0007309941304209284
2183, epoch_train_loss=0.0007309941304209284
Epoch 2184
Epoch 2184 :: Batch 0/1
Batch Loss = 0.0007309941296887785
2184, epoch_train_loss=0.0007309941296887785
Epoch 2185
Epoch 2185 :: Batch 0/1
Batch Loss = 0.000730994128957148
2185, epoch_train_loss=0.000730994128957148
Epoch 2186
Epoch 2186 :: Batch 0/1
Batch Loss = 0.0007309941282260368
2186, epoch_train_loss=0.0007309941282260368
Epoch 2187
Epoch 2187 :: Batch 0/1
Batch Loss = 0.0007309941274954441
2187, epoch_train_loss=0.0007309941274954441
Epoch 2188
Epoch 2188 :: Batch 0/1
Batch Loss = 0.0007309941267653698
2188, epoch_train_loss=0.0007309941267653698
Epoch 2189
Epoch 2189 :: Batch 0/1
Batch Loss = 0.0007309941260358133
2189, epoch_train_loss=0.0007309941260358133
Epoch 2190
Epoch 2190 :: Batch 0/1
Batch Loss = 0.0007309941253067745
2190, epoch_train_loss=0.0007309941253067745
Epoch 2191
Epoch 2191 :: Batch 0/1
Batch Loss = 0.0007309941245782524
2191, epoch_train_loss=0.0007309941245782524
Epoch 2192
Epoch 2192 :: Batch 0/1
Batch Loss = 0.000730994123850247
2192, epoch_train_loss=0.000730994123850247
Epoch 2193
Epoch 2193 :: Batch 0/1
Batch Loss = 0.0007309941231227581
2193, epoch_train_loss=0.0007309941231227581
Epoch 2194
Epoch 2194 :: Batch 0/1
Batch Loss = 0.0007309941223957847
2194, epoch_train_loss=0.0007309941223957847
Epoch 2195
Epoch 2195 :: Batch 0/1
Batch Loss = 0.0007309941216693269
2195, epoch_train_loss=0.0007309941216693269
Epoch 2196
Epoch 2196 :: Batch 0/1
Batch Loss = 0.0007309941209433841
2196, epoch_train_loss=0.0007309941209433841
Epoch 2197
Epoch 2197 :: Batch 0/1
Batch Loss = 0.0007309941202179557
2197, epoch_train_loss=0.0007309941202179557
Epoch 2198
Epoch 2198 :: Batch 0/1
Batch Loss = 0.0007309941194930415
2198, epoch_train_loss=0.0007309941194930415
Epoch 2199
Epoch 2199 :: Batch 0/1
Batch Loss = 0.0007309941187686411
2199, epoch_train_loss=0.0007309941187686411
Epoch 2200
Epoch 2200 :: Batch 0/1
Batch Loss = 0.000730994118044754
2200, epoch_train_loss=0.000730994118044754
Epoch 2201
Epoch 2201 :: Batch 0/1
Batch Loss = 0.0007309941173213798
2201, epoch_train_loss=0.0007309941173213798
Epoch 2202
Epoch 2202 :: Batch 0/1
Batch Loss = 0.0007309941165985182
2202, epoch_train_loss=0.0007309941165985182
Epoch 2203
Epoch 2203 :: Batch 0/1
Batch Loss = 0.0007309941158761685
2203, epoch_train_loss=0.0007309941158761685
Epoch 2204
Epoch 2204 :: Batch 0/1
Batch Loss = 0.0007309941151543306
2204, epoch_train_loss=0.0007309941151543306
Epoch 2205
Epoch 2205 :: Batch 0/1
Batch Loss = 0.000730994114433004
2205, epoch_train_loss=0.000730994114433004
Epoch 2206
Epoch 2206 :: Batch 0/1
Batch Loss = 0.0007309941137121882
2206, epoch_train_loss=0.0007309941137121882
Epoch 2207
Epoch 2207 :: Batch 0/1
Batch Loss = 0.000730994112991883
2207, epoch_train_loss=0.000730994112991883
Epoch 2208
Epoch 2208 :: Batch 0/1
Batch Loss = 0.0007309941122720877
2208, epoch_train_loss=0.0007309941122720877
Epoch 2209
Epoch 2209 :: Batch 0/1
Batch Loss = 0.000730994111552802
2209, epoch_train_loss=0.000730994111552802
Epoch 2210
Epoch 2210 :: Batch 0/1
Batch Loss = 0.0007309941108340258
2210, epoch_train_loss=0.0007309941108340258
Epoch 2211
Epoch 2211 :: Batch 0/1
Batch Loss = 0.0007309941101157582
2211, epoch_train_loss=0.0007309941101157582
Epoch 2212
Epoch 2212 :: Batch 0/1
Batch Loss = 0.000730994109397999
2212, epoch_train_loss=0.000730994109397999
Epoch 2213
Epoch 2213 :: Batch 0/1
Batch Loss = 0.000730994108680748
2213, epoch_train_loss=0.000730994108680748
Epoch 2214
Epoch 2214 :: Batch 0/1
Batch Loss = 0.0007309941079640045
2214, epoch_train_loss=0.0007309941079640045
Epoch 2215
Epoch 2215 :: Batch 0/1
Batch Loss = 0.0007309941072477682
2215, epoch_train_loss=0.0007309941072477682
Epoch 2216
Epoch 2216 :: Batch 0/1
Batch Loss = 0.0007309941065320387
2216, epoch_train_loss=0.0007309941065320387
Epoch 2217
Epoch 2217 :: Batch 0/1
Batch Loss = 0.0007309941058168154
2217, epoch_train_loss=0.0007309941058168154
Epoch 2218
Epoch 2218 :: Batch 0/1
Batch Loss = 0.0007309941051020981
2218, epoch_train_loss=0.0007309941051020981
Epoch 2219
Epoch 2219 :: Batch 0/1
Batch Loss = 0.0007309941043878866
2219, epoch_train_loss=0.0007309941043878866
Epoch 2220
Epoch 2220 :: Batch 0/1
Batch Loss = 0.0007309941036741802
2220, epoch_train_loss=0.0007309941036741802
Epoch 2221
Epoch 2221 :: Batch 0/1
Batch Loss = 0.0007309941029609785
2221, epoch_train_loss=0.0007309941029609785
Epoch 2222
Epoch 2222 :: Batch 0/1
Batch Loss = 0.0007309941022482812
2222, epoch_train_loss=0.0007309941022482812
Epoch 2223
Epoch 2223 :: Batch 0/1
Batch Loss = 0.0007309941015360878
2223, epoch_train_loss=0.0007309941015360878
Epoch 2224
Epoch 2224 :: Batch 0/1
Batch Loss = 0.0007309941008243979
2224, epoch_train_loss=0.0007309941008243979
Epoch 2225
Epoch 2225 :: Batch 0/1
Batch Loss = 0.0007309941001132112
2225, epoch_train_loss=0.0007309941001132112
Epoch 2226
Epoch 2226 :: Batch 0/1
Batch Loss = 0.0007309940994025272
2226, epoch_train_loss=0.0007309940994025272
Epoch 2227
Epoch 2227 :: Batch 0/1
Batch Loss = 0.0007309940986923456
2227, epoch_train_loss=0.0007309940986923456
Epoch 2228
Epoch 2228 :: Batch 0/1
Batch Loss = 0.000730994097982666
2228, epoch_train_loss=0.000730994097982666
Epoch 2229
Epoch 2229 :: Batch 0/1
Batch Loss = 0.0007309940972734878
2229, epoch_train_loss=0.0007309940972734878
Epoch 2230
Epoch 2230 :: Batch 0/1
Batch Loss = 0.0007309940965648107
2230, epoch_train_loss=0.0007309940965648107
Epoch 2231
Epoch 2231 :: Batch 0/1
Batch Loss = 0.0007309940958566345
2231, epoch_train_loss=0.0007309940958566345
Epoch 2232
Epoch 2232 :: Batch 0/1
Batch Loss = 0.0007309940951489585
2232, epoch_train_loss=0.0007309940951489585
Epoch 2233
Epoch 2233 :: Batch 0/1
Batch Loss = 0.0007309940944417824
2233, epoch_train_loss=0.0007309940944417824
Epoch 2234
Epoch 2234 :: Batch 0/1
Batch Loss = 0.0007309940937351058
2234, epoch_train_loss=0.0007309940937351058
Epoch 2235
Epoch 2235 :: Batch 0/1
Batch Loss = 0.0007309940930289284
2235, epoch_train_loss=0.0007309940930289284
Epoch 2236
Epoch 2236 :: Batch 0/1
Batch Loss = 0.0007309940923232497
2236, epoch_train_loss=0.0007309940923232497
Epoch 2237
Epoch 2237 :: Batch 0/1
Batch Loss = 0.0007309940916180691
2237, epoch_train_loss=0.0007309940916180691
Epoch 2238
Epoch 2238 :: Batch 0/1
Batch Loss = 0.0007309940909133867
2238, epoch_train_loss=0.0007309940909133867
Epoch 2239
Epoch 2239 :: Batch 0/1
Batch Loss = 0.0007309940902092018
2239, epoch_train_loss=0.0007309940902092018
Epoch 2240
Epoch 2240 :: Batch 0/1
Batch Loss = 0.0007309940895055139
2240, epoch_train_loss=0.0007309940895055139
Epoch 2241
Epoch 2241 :: Batch 0/1
Batch Loss = 0.0007309940888023228
2241, epoch_train_loss=0.0007309940888023228
Epoch 2242
Epoch 2242 :: Batch 0/1
Batch Loss = 0.000730994088099628
2242, epoch_train_loss=0.000730994088099628
Epoch 2243
Epoch 2243 :: Batch 0/1
Batch Loss = 0.0007309940873974291
2243, epoch_train_loss=0.0007309940873974291
Epoch 2244
Epoch 2244 :: Batch 0/1
Batch Loss = 0.0007309940866957257
2244, epoch_train_loss=0.0007309940866957257
Epoch 2245
Epoch 2245 :: Batch 0/1
Batch Loss = 0.0007309940859945175
2245, epoch_train_loss=0.0007309940859945175
Epoch 2246
Epoch 2246 :: Batch 0/1
Batch Loss = 0.000730994085293804
2246, epoch_train_loss=0.000730994085293804
Epoch 2247
Epoch 2247 :: Batch 0/1
Batch Loss = 0.0007309940845935847
2247, epoch_train_loss=0.0007309940845935847
Epoch 2248
Epoch 2248 :: Batch 0/1
Batch Loss = 0.0007309940838938595
2248, epoch_train_loss=0.0007309940838938595
Epoch 2249
Epoch 2249 :: Batch 0/1
Batch Loss = 0.0007309940831946278
2249, epoch_train_loss=0.0007309940831946278
Epoch 2250
Epoch 2250 :: Batch 0/1
Batch Loss = 0.0007309940824958891
2250, epoch_train_loss=0.0007309940824958891
Epoch 2251
Epoch 2251 :: Batch 0/1
Batch Loss = 0.0007309940817976433
2251, epoch_train_loss=0.0007309940817976433
Epoch 2252
Epoch 2252 :: Batch 0/1
Batch Loss = 0.0007309940810998899
2252, epoch_train_loss=0.0007309940810998899
Epoch 2253
Epoch 2253 :: Batch 0/1
Batch Loss = 0.0007309940804026282
2253, epoch_train_loss=0.0007309940804026282
Epoch 2254
Epoch 2254 :: Batch 0/1
Batch Loss = 0.0007309940797058585
2254, epoch_train_loss=0.0007309940797058585
Epoch 2255
Epoch 2255 :: Batch 0/1
Batch Loss = 0.0007309940790095796
2255, epoch_train_loss=0.0007309940790095796
Epoch 2256
Epoch 2256 :: Batch 0/1
Batch Loss = 0.0007309940783137915
2256, epoch_train_loss=0.0007309940783137915
Epoch 2257
Epoch 2257 :: Batch 0/1
Batch Loss = 0.0007309940776184938
2257, epoch_train_loss=0.0007309940776184938
Epoch 2258
Epoch 2258 :: Batch 0/1
Batch Loss = 0.0007309940769236862
2258, epoch_train_loss=0.0007309940769236862
Epoch 2259
Epoch 2259 :: Batch 0/1
Batch Loss = 0.000730994076229368
2259, epoch_train_loss=0.000730994076229368
Epoch 2260
Epoch 2260 :: Batch 0/1
Batch Loss = 0.0007309940755355391
2260, epoch_train_loss=0.0007309940755355391
Epoch 2261
Epoch 2261 :: Batch 0/1
Batch Loss = 0.0007309940748421992
2261, epoch_train_loss=0.0007309940748421992
Epoch 2262
Epoch 2262 :: Batch 0/1
Batch Loss = 0.0007309940741493474
2262, epoch_train_loss=0.0007309940741493474
Epoch 2263
Epoch 2263 :: Batch 0/1
Batch Loss = 0.0007309940734569837
2263, epoch_train_loss=0.0007309940734569837
Epoch 2264
Epoch 2264 :: Batch 0/1
Batch Loss = 0.0007309940727651077
2264, epoch_train_loss=0.0007309940727651077
Epoch 2265
Epoch 2265 :: Batch 0/1
Batch Loss = 0.0007309940720737188
2265, epoch_train_loss=0.0007309940720737188
Epoch 2266
Epoch 2266 :: Batch 0/1
Batch Loss = 0.000730994071382817
2266, epoch_train_loss=0.000730994071382817
Epoch 2267
Epoch 2267 :: Batch 0/1
Batch Loss = 0.0007309940706924014
2267, epoch_train_loss=0.0007309940706924014
Epoch 2268
Epoch 2268 :: Batch 0/1
Batch Loss = 0.000730994070002472
2268, epoch_train_loss=0.000730994070002472
Epoch 2269
Epoch 2269 :: Batch 0/1
Batch Loss = 0.0007309940693130282
2269, epoch_train_loss=0.0007309940693130282
Epoch 2270
Epoch 2270 :: Batch 0/1
Batch Loss = 0.0007309940686240696
2270, epoch_train_loss=0.0007309940686240696
Epoch 2271
Epoch 2271 :: Batch 0/1
Batch Loss = 0.000730994067935596
2271, epoch_train_loss=0.000730994067935596
Epoch 2272
Epoch 2272 :: Batch 0/1
Batch Loss = 0.0007309940672476069
2272, epoch_train_loss=0.0007309940672476069
Epoch 2273
Epoch 2273 :: Batch 0/1
Batch Loss = 0.0007309940665601017
2273, epoch_train_loss=0.0007309940665601017
Epoch 2274
Epoch 2274 :: Batch 0/1
Batch Loss = 0.0007309940658730805
2274, epoch_train_loss=0.0007309940658730805
Epoch 2275
Epoch 2275 :: Batch 0/1
Batch Loss = 0.0007309940651865426
2275, epoch_train_loss=0.0007309940651865426
Epoch 2276
Epoch 2276 :: Batch 0/1
Batch Loss = 0.0007309940645004874
2276, epoch_train_loss=0.0007309940645004874
Epoch 2277
Epoch 2277 :: Batch 0/1
Batch Loss = 0.000730994063814915
2277, epoch_train_loss=0.000730994063814915
Epoch 2278
Epoch 2278 :: Batch 0/1
Batch Loss = 0.0007309940631298247
2278, epoch_train_loss=0.0007309940631298247
Epoch 2279
Epoch 2279 :: Batch 0/1
Batch Loss = 0.0007309940624452163
2279, epoch_train_loss=0.0007309940624452163
Epoch 2280
Epoch 2280 :: Batch 0/1
Batch Loss = 0.0007309940617610889
2280, epoch_train_loss=0.0007309940617610889
Epoch 2281
Epoch 2281 :: Batch 0/1
Batch Loss = 0.0007309940610774428
2281, epoch_train_loss=0.0007309940610774428
Epoch 2282
Epoch 2282 :: Batch 0/1
Batch Loss = 0.0007309940603942774
2282, epoch_train_loss=0.0007309940603942774
Epoch 2283
Epoch 2283 :: Batch 0/1
Batch Loss = 0.000730994059711592
2283, epoch_train_loss=0.000730994059711592
Epoch 2284
Epoch 2284 :: Batch 0/1
Batch Loss = 0.0007309940590293866
2284, epoch_train_loss=0.0007309940590293866
Epoch 2285
Epoch 2285 :: Batch 0/1
Batch Loss = 0.0007309940583476606
2285, epoch_train_loss=0.0007309940583476606
Epoch 2286
Epoch 2286 :: Batch 0/1
Batch Loss = 0.0007309940576664137
2286, epoch_train_loss=0.0007309940576664137
Epoch 2287
Epoch 2287 :: Batch 0/1
Batch Loss = 0.0007309940569856455
2287, epoch_train_loss=0.0007309940569856455
Epoch 2288
Epoch 2288 :: Batch 0/1
Batch Loss = 0.0007309940563053556
2288, epoch_train_loss=0.0007309940563053556
Epoch 2289
Epoch 2289 :: Batch 0/1
Batch Loss = 0.0007309940556255436
2289, epoch_train_loss=0.0007309940556255436
Epoch 2290
Epoch 2290 :: Batch 0/1
Batch Loss = 0.0007309940549462092
2290, epoch_train_loss=0.0007309940549462092
Epoch 2291
Epoch 2291 :: Batch 0/1
Batch Loss = 0.0007309940542673518
2291, epoch_train_loss=0.0007309940542673518
Epoch 2292
Epoch 2292 :: Batch 0/1
Batch Loss = 0.0007309940535889714
2292, epoch_train_loss=0.0007309940535889714
Epoch 2293
Epoch 2293 :: Batch 0/1
Batch Loss = 0.0007309940529110671
2293, epoch_train_loss=0.0007309940529110671
Epoch 2294
Epoch 2294 :: Batch 0/1
Batch Loss = 0.000730994052233639
2294, epoch_train_loss=0.000730994052233639
Epoch 2295
Epoch 2295 :: Batch 0/1
Batch Loss = 0.0007309940515566864
2295, epoch_train_loss=0.0007309940515566864
Epoch 2296
Epoch 2296 :: Batch 0/1
Batch Loss = 0.0007309940508802091
2296, epoch_train_loss=0.0007309940508802091
Epoch 2297
Epoch 2297 :: Batch 0/1
Batch Loss = 0.0007309940502042067
2297, epoch_train_loss=0.0007309940502042067
Epoch 2298
Epoch 2298 :: Batch 0/1
Batch Loss = 0.0007309940495286786
2298, epoch_train_loss=0.0007309940495286786
Epoch 2299
Epoch 2299 :: Batch 0/1
Batch Loss = 0.0007309940488536246
2299, epoch_train_loss=0.0007309940488536246
Epoch 2300
Epoch 2300 :: Batch 0/1
Batch Loss = 0.0007309940481790446
2300, epoch_train_loss=0.0007309940481790446
Epoch 2301
Epoch 2301 :: Batch 0/1
Batch Loss = 0.0007309940475049376
2301, epoch_train_loss=0.0007309940475049376
Epoch 2302
Epoch 2302 :: Batch 0/1
Batch Loss = 0.0007309940468313037
2302, epoch_train_loss=0.0007309940468313037
Epoch 2303
Epoch 2303 :: Batch 0/1
Batch Loss = 0.0007309940461581424
2303, epoch_train_loss=0.0007309940461581424
Epoch 2304
Epoch 2304 :: Batch 0/1
Batch Loss = 0.0007309940454854532
2304, epoch_train_loss=0.0007309940454854532
Epoch 2305
Epoch 2305 :: Batch 0/1
Batch Loss = 0.0007309940448132358
2305, epoch_train_loss=0.0007309940448132358
Epoch 2306
Epoch 2306 :: Batch 0/1
Batch Loss = 0.00073099404414149
2306, epoch_train_loss=0.00073099404414149
Epoch 2307
Epoch 2307 :: Batch 0/1
Batch Loss = 0.0007309940434702152
2307, epoch_train_loss=0.0007309940434702152
Epoch 2308
Epoch 2308 :: Batch 0/1
Batch Loss = 0.0007309940427994109
2308, epoch_train_loss=0.0007309940427994109
Epoch 2309
Epoch 2309 :: Batch 0/1
Batch Loss = 0.0007309940421290771
2309, epoch_train_loss=0.0007309940421290771
Epoch 2310
Epoch 2310 :: Batch 0/1
Batch Loss = 0.0007309940414592132
2310, epoch_train_loss=0.0007309940414592132
Epoch 2311
Epoch 2311 :: Batch 0/1
Batch Loss = 0.0007309940407898187
2311, epoch_train_loss=0.0007309940407898187
Epoch 2312
Epoch 2312 :: Batch 0/1
Batch Loss = 0.0007309940401208936
2312, epoch_train_loss=0.0007309940401208936
Epoch 2313
Epoch 2313 :: Batch 0/1
Batch Loss = 0.0007309940394524371
2313, epoch_train_loss=0.0007309940394524371
Epoch 2314
Epoch 2314 :: Batch 0/1
Batch Loss = 0.000730994038784449
2314, epoch_train_loss=0.000730994038784449
Epoch 2315
Epoch 2315 :: Batch 0/1
Batch Loss = 0.000730994038116929
2315, epoch_train_loss=0.000730994038116929
Epoch 2316
Epoch 2316 :: Batch 0/1
Batch Loss = 0.0007309940374498765
2316, epoch_train_loss=0.0007309940374498765
Epoch 2317
Epoch 2317 :: Batch 0/1
Batch Loss = 0.0007309940367832915
2317, epoch_train_loss=0.0007309940367832915
Epoch 2318
Epoch 2318 :: Batch 0/1
Batch Loss = 0.0007309940361171731
2318, epoch_train_loss=0.0007309940361171731
Epoch 2319
Epoch 2319 :: Batch 0/1
Batch Loss = 0.0007309940354515214
2319, epoch_train_loss=0.0007309940354515214
Epoch 2320
Epoch 2320 :: Batch 0/1
Batch Loss = 0.0007309940347863358
2320, epoch_train_loss=0.0007309940347863358
Epoch 2321
Epoch 2321 :: Batch 0/1
Batch Loss = 0.0007309940341216161
2321, epoch_train_loss=0.0007309940341216161
Epoch 2322
Epoch 2322 :: Batch 0/1
Batch Loss = 0.0007309940334573618
2322, epoch_train_loss=0.0007309940334573618
Epoch 2323
Epoch 2323 :: Batch 0/1
Batch Loss = 0.0007309940327935722
2323, epoch_train_loss=0.0007309940327935722
Epoch 2324
Epoch 2324 :: Batch 0/1
Batch Loss = 0.0007309940321302477
2324, epoch_train_loss=0.0007309940321302477
Epoch 2325
Epoch 2325 :: Batch 0/1
Batch Loss = 0.0007309940314673872
2325, epoch_train_loss=0.0007309940314673872
Epoch 2326
Epoch 2326 :: Batch 0/1
Batch Loss = 0.0007309940308049908
2326, epoch_train_loss=0.0007309940308049908
Epoch 2327
Epoch 2327 :: Batch 0/1
Batch Loss = 0.0007309940301430576
2327, epoch_train_loss=0.0007309940301430576
Epoch 2328
Epoch 2328 :: Batch 0/1
Batch Loss = 0.0007309940294815879
2328, epoch_train_loss=0.0007309940294815879
Epoch 2329
Epoch 2329 :: Batch 0/1
Batch Loss = 0.0007309940288205808
2329, epoch_train_loss=0.0007309940288205808
Epoch 2330
Epoch 2330 :: Batch 0/1
Batch Loss = 0.0007309940281600362
2330, epoch_train_loss=0.0007309940281600362
Epoch 2331
Epoch 2331 :: Batch 0/1
Batch Loss = 0.0007309940274999537
2331, epoch_train_loss=0.0007309940274999537
Epoch 2332
Epoch 2332 :: Batch 0/1
Batch Loss = 0.0007309940268403327
2332, epoch_train_loss=0.0007309940268403327
Epoch 2333
Epoch 2333 :: Batch 0/1
Batch Loss = 0.000730994026181173
2333, epoch_train_loss=0.000730994026181173
Epoch 2334
Epoch 2334 :: Batch 0/1
Batch Loss = 0.0007309940255224743
2334, epoch_train_loss=0.0007309940255224743
Epoch 2335
Epoch 2335 :: Batch 0/1
Batch Loss = 0.0007309940248642361
2335, epoch_train_loss=0.0007309940248642361
Epoch 2336
Epoch 2336 :: Batch 0/1
Batch Loss = 0.0007309940242064582
2336, epoch_train_loss=0.0007309940242064582
Epoch 2337
Epoch 2337 :: Batch 0/1
Batch Loss = 0.0007309940235491401
2337, epoch_train_loss=0.0007309940235491401
Epoch 2338
Epoch 2338 :: Batch 0/1
Batch Loss = 0.0007309940228922813
2338, epoch_train_loss=0.0007309940228922813
Epoch 2339
Epoch 2339 :: Batch 0/1
Batch Loss = 0.0007309940222358816
2339, epoch_train_loss=0.0007309940222358816
Epoch 2340
Epoch 2340 :: Batch 0/1
Batch Loss = 0.0007309940215799409
2340, epoch_train_loss=0.0007309940215799409
Epoch 2341
Epoch 2341 :: Batch 0/1
Batch Loss = 0.0007309940209244581
2341, epoch_train_loss=0.0007309940209244581
Epoch 2342
Epoch 2342 :: Batch 0/1
Batch Loss = 0.0007309940202694336
2342, epoch_train_loss=0.0007309940202694336
Epoch 2343
Epoch 2343 :: Batch 0/1
Batch Loss = 0.0007309940196148666
2343, epoch_train_loss=0.0007309940196148666
Epoch 2344
Epoch 2344 :: Batch 0/1
Batch Loss = 0.0007309940189607568
2344, epoch_train_loss=0.0007309940189607568
Epoch 2345
Epoch 2345 :: Batch 0/1
Batch Loss = 0.0007309940183071039
2345, epoch_train_loss=0.0007309940183071039
Epoch 2346
Epoch 2346 :: Batch 0/1
Batch Loss = 0.0007309940176539074
2346, epoch_train_loss=0.0007309940176539074
Epoch 2347
Epoch 2347 :: Batch 0/1
Batch Loss = 0.000730994017001167
2347, epoch_train_loss=0.000730994017001167
Epoch 2348
Epoch 2348 :: Batch 0/1
Batch Loss = 0.0007309940163488826
2348, epoch_train_loss=0.0007309940163488826
Epoch 2349
Epoch 2349 :: Batch 0/1
Batch Loss = 0.0007309940156970535
2349, epoch_train_loss=0.0007309940156970535
Epoch 2350
Epoch 2350 :: Batch 0/1
Batch Loss = 0.0007309940150456793
2350, epoch_train_loss=0.0007309940150456793
Epoch 2351
Epoch 2351 :: Batch 0/1
Batch Loss = 0.00073099401439476
2351, epoch_train_loss=0.00073099401439476
Epoch 2352
Epoch 2352 :: Batch 0/1
Batch Loss = 0.0007309940137442948
2352, epoch_train_loss=0.0007309940137442948
Epoch 2353
Epoch 2353 :: Batch 0/1
Batch Loss = 0.0007309940130942837
2353, epoch_train_loss=0.0007309940130942837
Epoch 2354
Epoch 2354 :: Batch 0/1
Batch Loss = 0.000730994012444726
2354, epoch_train_loss=0.000730994012444726
Epoch 2355
Epoch 2355 :: Batch 0/1
Batch Loss = 0.0007309940117956216
2355, epoch_train_loss=0.0007309940117956216
Epoch 2356
Epoch 2356 :: Batch 0/1
Batch Loss = 0.00073099401114697
2356, epoch_train_loss=0.00073099401114697
Epoch 2357
Epoch 2357 :: Batch 0/1
Batch Loss = 0.0007309940104987709
2357, epoch_train_loss=0.0007309940104987709
Epoch 2358
Epoch 2358 :: Batch 0/1
Batch Loss = 0.0007309940098510239
2358, epoch_train_loss=0.0007309940098510239
Epoch 2359
Epoch 2359 :: Batch 0/1
Batch Loss = 0.0007309940092037286
2359, epoch_train_loss=0.0007309940092037286
Epoch 2360
Epoch 2360 :: Batch 0/1
Batch Loss = 0.0007309940085568847
2360, epoch_train_loss=0.0007309940085568847
Epoch 2361
Epoch 2361 :: Batch 0/1
Batch Loss = 0.0007309940079104919
2361, epoch_train_loss=0.0007309940079104919
Epoch 2362
Epoch 2362 :: Batch 0/1
Batch Loss = 0.0007309940072645496
2362, epoch_train_loss=0.0007309940072645496
Epoch 2363
Epoch 2363 :: Batch 0/1
Batch Loss = 0.0007309940066190577
2363, epoch_train_loss=0.0007309940066190577
Epoch 2364
Epoch 2364 :: Batch 0/1
Batch Loss = 0.0007309940059740157
2364, epoch_train_loss=0.0007309940059740157
Epoch 2365
Epoch 2365 :: Batch 0/1
Batch Loss = 0.0007309940053294233
2365, epoch_train_loss=0.0007309940053294233
Epoch 2366
Epoch 2366 :: Batch 0/1
Batch Loss = 0.0007309940046852802
2366, epoch_train_loss=0.0007309940046852802
Epoch 2367
Epoch 2367 :: Batch 0/1
Batch Loss = 0.0007309940040415858
2367, epoch_train_loss=0.0007309940040415858
Epoch 2368
Epoch 2368 :: Batch 0/1
Batch Loss = 0.00073099400339834
2368, epoch_train_loss=0.00073099400339834
Epoch 2369
Epoch 2369 :: Batch 0/1
Batch Loss = 0.0007309940027555423
2369, epoch_train_loss=0.0007309940027555423
Epoch 2370
Epoch 2370 :: Batch 0/1
Batch Loss = 0.0007309940021131923
2370, epoch_train_loss=0.0007309940021131923
Epoch 2371
Epoch 2371 :: Batch 0/1
Batch Loss = 0.0007309940014712897
2371, epoch_train_loss=0.0007309940014712897
Epoch 2372
Epoch 2372 :: Batch 0/1
Batch Loss = 0.0007309940008298344
2372, epoch_train_loss=0.0007309940008298344
Epoch 2373
Epoch 2373 :: Batch 0/1
Batch Loss = 0.0007309940001888254
2373, epoch_train_loss=0.0007309940001888254
Epoch 2374
Epoch 2374 :: Batch 0/1
Batch Loss = 0.0007309939995482629
2374, epoch_train_loss=0.0007309939995482629
Epoch 2375
Epoch 2375 :: Batch 0/1
Batch Loss = 0.0007309939989081464
2375, epoch_train_loss=0.0007309939989081464
Epoch 2376
Epoch 2376 :: Batch 0/1
Batch Loss = 0.0007309939982684756
2376, epoch_train_loss=0.0007309939982684756
Epoch 2377
Epoch 2377 :: Batch 0/1
Batch Loss = 0.0007309939976292497
2377, epoch_train_loss=0.0007309939976292497
Epoch 2378
Epoch 2378 :: Batch 0/1
Batch Loss = 0.000730993996990469
2378, epoch_train_loss=0.000730993996990469
Epoch 2379
Epoch 2379 :: Batch 0/1
Batch Loss = 0.0007309939963521326
2379, epoch_train_loss=0.0007309939963521326
Epoch 2380
Epoch 2380 :: Batch 0/1
Batch Loss = 0.0007309939957142406
2380, epoch_train_loss=0.0007309939957142406
Epoch 2381
Epoch 2381 :: Batch 0/1
Batch Loss = 0.0007309939950767924
2381, epoch_train_loss=0.0007309939950767924
Epoch 2382
Epoch 2382 :: Batch 0/1
Batch Loss = 0.0007309939944397877
2382, epoch_train_loss=0.0007309939944397877
Epoch 2383
Epoch 2383 :: Batch 0/1
Batch Loss = 0.0007309939938032259
2383, epoch_train_loss=0.0007309939938032259
Epoch 2384
Epoch 2384 :: Batch 0/1
Batch Loss = 0.0007309939931671069
2384, epoch_train_loss=0.0007309939931671069
Epoch 2385
Epoch 2385 :: Batch 0/1
Batch Loss = 0.0007309939925314305
2385, epoch_train_loss=0.0007309939925314305
Epoch 2386
Epoch 2386 :: Batch 0/1
Batch Loss = 0.0007309939918961959
2386, epoch_train_loss=0.0007309939918961959
Epoch 2387
Epoch 2387 :: Batch 0/1
Batch Loss = 0.000730993991261403
2387, epoch_train_loss=0.000730993991261403
Epoch 2388
Epoch 2388 :: Batch 0/1
Batch Loss = 0.0007309939906270516
2388, epoch_train_loss=0.0007309939906270516
Epoch 2389
Epoch 2389 :: Batch 0/1
Batch Loss = 0.000730993989993141
2389, epoch_train_loss=0.000730993989993141
Epoch 2390
Epoch 2390 :: Batch 0/1
Batch Loss = 0.0007309939893596711
2390, epoch_train_loss=0.0007309939893596711
Epoch 2391
Epoch 2391 :: Batch 0/1
Batch Loss = 0.0007309939887266416
2391, epoch_train_loss=0.0007309939887266416
Epoch 2392
Epoch 2392 :: Batch 0/1
Batch Loss = 0.0007309939880940518
2392, epoch_train_loss=0.0007309939880940518
Epoch 2393
Epoch 2393 :: Batch 0/1
Batch Loss = 0.0007309939874619015
2393, epoch_train_loss=0.0007309939874619015
Epoch 2394
Epoch 2394 :: Batch 0/1
Batch Loss = 0.0007309939868301907
2394, epoch_train_loss=0.0007309939868301907
Epoch 2395
Epoch 2395 :: Batch 0/1
Batch Loss = 0.0007309939861989186
2395, epoch_train_loss=0.0007309939861989186
Epoch 2396
Epoch 2396 :: Batch 0/1
Batch Loss = 0.0007309939855680849
2396, epoch_train_loss=0.0007309939855680849
Epoch 2397
Epoch 2397 :: Batch 0/1
Batch Loss = 0.0007309939849376896
2397, epoch_train_loss=0.0007309939849376896
Epoch 2398
Epoch 2398 :: Batch 0/1
Batch Loss = 0.0007309939843077318
2398, epoch_train_loss=0.0007309939843077318
Epoch 2399
Epoch 2399 :: Batch 0/1
Batch Loss = 0.0007309939836782115
2399, epoch_train_loss=0.0007309939836782115
Epoch 2400
Epoch 2400 :: Batch 0/1
Batch Loss = 0.0007309939830491283
2400, epoch_train_loss=0.0007309939830491283
Epoch 2401
Epoch 2401 :: Batch 0/1
Batch Loss = 0.0007309939824204819
2401, epoch_train_loss=0.0007309939824204819
Epoch 2402
Epoch 2402 :: Batch 0/1
Batch Loss = 0.0007309939817922717
2402, epoch_train_loss=0.0007309939817922717
Epoch 2403
Epoch 2403 :: Batch 0/1
Batch Loss = 0.0007309939811644977
2403, epoch_train_loss=0.0007309939811644977
Epoch 2404
Epoch 2404 :: Batch 0/1
Batch Loss = 0.0007309939805371595
2404, epoch_train_loss=0.0007309939805371595
Epoch 2405
Epoch 2405 :: Batch 0/1
Batch Loss = 0.0007309939799102565
2405, epoch_train_loss=0.0007309939799102565
Epoch 2406
Epoch 2406 :: Batch 0/1
Batch Loss = 0.0007309939792837883
2406, epoch_train_loss=0.0007309939792837883
Epoch 2407
Epoch 2407 :: Batch 0/1
Batch Loss = 0.000730993978657755
2407, epoch_train_loss=0.000730993978657755
Epoch 2408
Epoch 2408 :: Batch 0/1
Batch Loss = 0.0007309939780321559
2408, epoch_train_loss=0.0007309939780321559
Epoch 2409
Epoch 2409 :: Batch 0/1
Batch Loss = 0.0007309939774069907
2409, epoch_train_loss=0.0007309939774069907
Epoch 2410
Epoch 2410 :: Batch 0/1
Batch Loss = 0.0007309939767822591
2410, epoch_train_loss=0.0007309939767822591
Epoch 2411
Epoch 2411 :: Batch 0/1
Batch Loss = 0.0007309939761579608
2411, epoch_train_loss=0.0007309939761579608
Epoch 2412
Epoch 2412 :: Batch 0/1
Batch Loss = 0.0007309939755340952
2412, epoch_train_loss=0.0007309939755340952
Epoch 2413
Epoch 2413 :: Batch 0/1
Batch Loss = 0.0007309939749106624
2413, epoch_train_loss=0.0007309939749106624
Epoch 2414
Epoch 2414 :: Batch 0/1
Batch Loss = 0.0007309939742876616
2414, epoch_train_loss=0.0007309939742876616
Epoch 2415
Epoch 2415 :: Batch 0/1
Batch Loss = 0.0007309939736650929
2415, epoch_train_loss=0.0007309939736650929
Epoch 2416
Epoch 2416 :: Batch 0/1
Batch Loss = 0.0007309939730429552
2416, epoch_train_loss=0.0007309939730429552
Epoch 2417
Epoch 2417 :: Batch 0/1
Batch Loss = 0.0007309939724212491
2417, epoch_train_loss=0.0007309939724212491
Epoch 2418
Epoch 2418 :: Batch 0/1
Batch Loss = 0.0007309939717999737
2418, epoch_train_loss=0.0007309939717999737
Epoch 2419
Epoch 2419 :: Batch 0/1
Batch Loss = 0.0007309939711791287
2419, epoch_train_loss=0.0007309939711791287
Epoch 2420
Epoch 2420 :: Batch 0/1
Batch Loss = 0.0007309939705587138
2420, epoch_train_loss=0.0007309939705587138
Epoch 2421
Epoch 2421 :: Batch 0/1
Batch Loss = 0.0007309939699387288
2421, epoch_train_loss=0.0007309939699387288
Epoch 2422
Epoch 2422 :: Batch 0/1
Batch Loss = 0.0007309939693191733
2422, epoch_train_loss=0.0007309939693191733
Epoch 2423
Epoch 2423 :: Batch 0/1
Batch Loss = 0.0007309939687000465
2423, epoch_train_loss=0.0007309939687000465
Epoch 2424
Epoch 2424 :: Batch 0/1
Batch Loss = 0.0007309939680813487
2424, epoch_train_loss=0.0007309939680813487
Epoch 2425
Epoch 2425 :: Batch 0/1
Batch Loss = 0.0007309939674630793
2425, epoch_train_loss=0.0007309939674630793
Epoch 2426
Epoch 2426 :: Batch 0/1
Batch Loss = 0.0007309939668452378
2426, epoch_train_loss=0.0007309939668452378
Epoch 2427
Epoch 2427 :: Batch 0/1
Batch Loss = 0.0007309939662278241
2427, epoch_train_loss=0.0007309939662278241
Epoch 2428
Epoch 2428 :: Batch 0/1
Batch Loss = 0.0007309939656108377
2428, epoch_train_loss=0.0007309939656108377
Epoch 2429
Epoch 2429 :: Batch 0/1
Batch Loss = 0.0007309939649942782
2429, epoch_train_loss=0.0007309939649942782
Epoch 2430
Epoch 2430 :: Batch 0/1
Batch Loss = 0.0007309939643781454
2430, epoch_train_loss=0.0007309939643781454
Epoch 2431
Epoch 2431 :: Batch 0/1
Batch Loss = 0.0007309939637624391
2431, epoch_train_loss=0.0007309939637624391
Epoch 2432
Epoch 2432 :: Batch 0/1
Batch Loss = 0.0007309939631471588
2432, epoch_train_loss=0.0007309939631471588
Epoch 2433
Epoch 2433 :: Batch 0/1
Batch Loss = 0.000730993962532304
2433, epoch_train_loss=0.000730993962532304
Epoch 2434
Epoch 2434 :: Batch 0/1
Batch Loss = 0.0007309939619178744
2434, epoch_train_loss=0.0007309939619178744
Epoch 2435
Epoch 2435 :: Batch 0/1
Batch Loss = 0.0007309939613038699
2435, epoch_train_loss=0.0007309939613038699
Epoch 2436
Epoch 2436 :: Batch 0/1
Batch Loss = 0.00073099396069029
2436, epoch_train_loss=0.00073099396069029
Epoch 2437
Epoch 2437 :: Batch 0/1
Batch Loss = 0.0007309939600771345
2437, epoch_train_loss=0.0007309939600771345
Epoch 2438
Epoch 2438 :: Batch 0/1
Batch Loss = 0.0007309939594644028
2438, epoch_train_loss=0.0007309939594644028
Epoch 2439
Epoch 2439 :: Batch 0/1
Batch Loss = 0.0007309939588520946
2439, epoch_train_loss=0.0007309939588520946
Epoch 2440
Epoch 2440 :: Batch 0/1
Batch Loss = 0.0007309939582402097
2440, epoch_train_loss=0.0007309939582402097
Epoch 2441
Epoch 2441 :: Batch 0/1
Batch Loss = 0.0007309939576287478
2441, epoch_train_loss=0.0007309939576287478
Epoch 2442
Epoch 2442 :: Batch 0/1
Batch Loss = 0.0007309939570177084
2442, epoch_train_loss=0.0007309939570177084
Epoch 2443
Epoch 2443 :: Batch 0/1
Batch Loss = 0.0007309939564070914
2443, epoch_train_loss=0.0007309939564070914
Epoch 2444
Epoch 2444 :: Batch 0/1
Batch Loss = 0.0007309939557968962
2444, epoch_train_loss=0.0007309939557968962
Epoch 2445
Epoch 2445 :: Batch 0/1
Batch Loss = 0.0007309939551871224
2445, epoch_train_loss=0.0007309939551871224
Epoch 2446
Epoch 2446 :: Batch 0/1
Batch Loss = 0.0007309939545777699
2446, epoch_train_loss=0.0007309939545777699
Epoch 2447
Epoch 2447 :: Batch 0/1
Batch Loss = 0.0007309939539688384
2447, epoch_train_loss=0.0007309939539688384
Epoch 2448
Epoch 2448 :: Batch 0/1
Batch Loss = 0.0007309939533603274
2448, epoch_train_loss=0.0007309939533603274
Epoch 2449
Epoch 2449 :: Batch 0/1
Batch Loss = 0.0007309939527522365
2449, epoch_train_loss=0.0007309939527522365
Epoch 2450
Epoch 2450 :: Batch 0/1
Batch Loss = 0.0007309939521445654
2450, epoch_train_loss=0.0007309939521445654
Epoch 2451
Epoch 2451 :: Batch 0/1
Batch Loss = 0.000730993951537314
2451, epoch_train_loss=0.000730993951537314
Epoch 2452
Epoch 2452 :: Batch 0/1
Batch Loss = 0.0007309939509304817
2452, epoch_train_loss=0.0007309939509304817
Epoch 2453
Epoch 2453 :: Batch 0/1
Batch Loss = 0.0007309939503240683
2453, epoch_train_loss=0.0007309939503240683
Epoch 2454
Epoch 2454 :: Batch 0/1
Batch Loss = 0.0007309939497180734
2454, epoch_train_loss=0.0007309939497180734
Epoch 2455
Epoch 2455 :: Batch 0/1
Batch Loss = 0.0007309939491124968
2455, epoch_train_loss=0.0007309939491124968
Epoch 2456
Epoch 2456 :: Batch 0/1
Batch Loss = 0.0007309939485073379
2456, epoch_train_loss=0.0007309939485073379
Epoch 2457
Epoch 2457 :: Batch 0/1
Batch Loss = 0.0007309939479025964
2457, epoch_train_loss=0.0007309939479025964
Epoch 2458
Epoch 2458 :: Batch 0/1
Batch Loss = 0.0007309939472982723
2458, epoch_train_loss=0.0007309939472982723
Epoch 2459
Epoch 2459 :: Batch 0/1
Batch Loss = 0.0007309939466943649
2459, epoch_train_loss=0.0007309939466943649
Epoch 2460
Epoch 2460 :: Batch 0/1
Batch Loss = 0.000730993946090874
2460, epoch_train_loss=0.000730993946090874
Epoch 2461
Epoch 2461 :: Batch 0/1
Batch Loss = 0.0007309939454877994
2461, epoch_train_loss=0.0007309939454877994
Epoch 2462
Epoch 2462 :: Batch 0/1
Batch Loss = 0.0007309939448851405
2462, epoch_train_loss=0.0007309939448851405
Epoch 2463
Epoch 2463 :: Batch 0/1
Batch Loss = 0.0007309939442828973
2463, epoch_train_loss=0.0007309939442828973
Epoch 2464
Epoch 2464 :: Batch 0/1
Batch Loss = 0.0007309939436810689
2464, epoch_train_loss=0.0007309939436810689
Epoch 2465
Epoch 2465 :: Batch 0/1
Batch Loss = 0.0007309939430796557
2465, epoch_train_loss=0.0007309939430796557
Epoch 2466
Epoch 2466 :: Batch 0/1
Batch Loss = 0.0007309939424786568
2466, epoch_train_loss=0.0007309939424786568
Epoch 2467
Epoch 2467 :: Batch 0/1
Batch Loss = 0.0007309939418780723
2467, epoch_train_loss=0.0007309939418780723
Epoch 2468
Epoch 2468 :: Batch 0/1
Batch Loss = 0.0007309939412779011
2468, epoch_train_loss=0.0007309939412779011
Epoch 2469
Epoch 2469 :: Batch 0/1
Batch Loss = 0.000730993940678144
2469, epoch_train_loss=0.000730993940678144
Epoch 2470
Epoch 2470 :: Batch 0/1
Batch Loss = 0.0007309939400787997
2470, epoch_train_loss=0.0007309939400787997
Epoch 2471
Epoch 2471 :: Batch 0/1
Batch Loss = 0.0007309939394798683
2471, epoch_train_loss=0.0007309939394798683
Epoch 2472
Epoch 2472 :: Batch 0/1
Batch Loss = 0.0007309939388813496
2472, epoch_train_loss=0.0007309939388813496
Epoch 2473
Epoch 2473 :: Batch 0/1
Batch Loss = 0.0007309939382832429
2473, epoch_train_loss=0.0007309939382832429
Epoch 2474
Epoch 2474 :: Batch 0/1
Batch Loss = 0.0007309939376855482
2474, epoch_train_loss=0.0007309939376855482
Epoch 2475
Epoch 2475 :: Batch 0/1
Batch Loss = 0.0007309939370882648
2475, epoch_train_loss=0.0007309939370882648
Epoch 2476
Epoch 2476 :: Batch 0/1
Batch Loss = 0.0007309939364913929
2476, epoch_train_loss=0.0007309939364913929
Epoch 2477
Epoch 2477 :: Batch 0/1
Batch Loss = 0.0007309939358949315
2477, epoch_train_loss=0.0007309939358949315
Epoch 2478
Epoch 2478 :: Batch 0/1
Batch Loss = 0.000730993935298881
2478, epoch_train_loss=0.000730993935298881
Epoch 2479
Epoch 2479 :: Batch 0/1
Batch Loss = 0.0007309939347032405
2479, epoch_train_loss=0.0007309939347032405
Epoch 2480
Epoch 2480 :: Batch 0/1
Batch Loss = 0.0007309939341080098
2480, epoch_train_loss=0.0007309939341080098
Epoch 2481
Epoch 2481 :: Batch 0/1
Batch Loss = 0.0007309939335131888
2481, epoch_train_loss=0.0007309939335131888
Epoch 2482
Epoch 2482 :: Batch 0/1
Batch Loss = 0.0007309939329187767
2482, epoch_train_loss=0.0007309939329187767
Epoch 2483
Epoch 2483 :: Batch 0/1
Batch Loss = 0.0007309939323247738
2483, epoch_train_loss=0.0007309939323247738
Epoch 2484
Epoch 2484 :: Batch 0/1
Batch Loss = 0.0007309939317311794
2484, epoch_train_loss=0.0007309939317311794
Epoch 2485
Epoch 2485 :: Batch 0/1
Batch Loss = 0.0007309939311379932
2485, epoch_train_loss=0.0007309939311379932
Epoch 2486
Epoch 2486 :: Batch 0/1
Batch Loss = 0.0007309939305452149
2486, epoch_train_loss=0.0007309939305452149
Epoch 2487
Epoch 2487 :: Batch 0/1
Batch Loss = 0.0007309939299528441
2487, epoch_train_loss=0.0007309939299528441
Epoch 2488
Epoch 2488 :: Batch 0/1
Batch Loss = 0.0007309939293608804
2488, epoch_train_loss=0.0007309939293608804
Epoch 2489
Epoch 2489 :: Batch 0/1
Batch Loss = 0.0007309939287693238
2489, epoch_train_loss=0.0007309939287693238
Epoch 2490
Epoch 2490 :: Batch 0/1
Batch Loss = 0.0007309939281781738
2490, epoch_train_loss=0.0007309939281781738
Epoch 2491
Epoch 2491 :: Batch 0/1
Batch Loss = 0.0007309939275874301
2491, epoch_train_loss=0.0007309939275874301
Epoch 2492
Epoch 2492 :: Batch 0/1
Batch Loss = 0.0007309939269970922
2492, epoch_train_loss=0.0007309939269970922
Epoch 2493
Epoch 2493 :: Batch 0/1
Batch Loss = 0.0007309939264071603
2493, epoch_train_loss=0.0007309939264071603
Epoch 2494
Epoch 2494 :: Batch 0/1
Batch Loss = 0.0007309939258176333
2494, epoch_train_loss=0.0007309939258176333
Epoch 2495
Epoch 2495 :: Batch 0/1
Batch Loss = 0.0007309939252285113
2495, epoch_train_loss=0.0007309939252285113
Epoch 2496
Epoch 2496 :: Batch 0/1
Batch Loss = 0.0007309939246397941
2496, epoch_train_loss=0.0007309939246397941
Epoch 2497
Epoch 2497 :: Batch 0/1
Batch Loss = 0.0007309939240514812
2497, epoch_train_loss=0.0007309939240514812
Epoch 2498
Epoch 2498 :: Batch 0/1
Batch Loss = 0.0007309939234635721
2498, epoch_train_loss=0.0007309939234635721
Epoch 2499
Epoch 2499 :: Batch 0/1
Batch Loss = 0.0007309939228760667
2499, epoch_train_loss=0.0007309939228760667
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting "B3LYP_WITH_VWN5 = True" in pyscf_conf.py
  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820f340> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820f340> in UKS object of <class 'pyscf.dft.uks.UKS'>
<pyscf.gto.mole.Mole object at 0x7ffec820f340> [['P', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec820d2a0> [['N', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec820fbe0> [['H', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec820d480> [['Li', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec820e6e0> [['O', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec820cc70> [['Cl', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec820fb50> [['Al', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec820d0c0> [['S', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec820c250> [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec820e350> [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec820d690> [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec820ef80> [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]] 3
<pyscf.gto.mole.Mole object at 0x7ffec820d450> [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]] 3
<pyscf.gto.mole.Mole object at 0x7ffec820fb80> [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec820ed40> [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec820c1c0> [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec820e4a0> [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]] 4
<pyscf.gto.mole.Mole object at 0x7ffec820f8b0> [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec820e4d0> [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec820fa60> [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec820cbb0> [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec820ff10> [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]] 4
<pyscf.gto.mole.Mole object at 0x7ffec820dd80> [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]] 4
<pyscf.gto.mole.Mole object at 0x7ffec820dae0> [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec820c700> [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]] 5
<pyscf.gto.mole.Mole object at 0x7ffec820cc10> [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]] 3
<pyscf.gto.mole.Mole object at 0x7ffec820dc30> [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]] 5
mol:  [['P', array([0., 0., 0.])]]
converged SCF energy = -341.104145992717  <S^2> = 3.7502984  2S+1 = 4.0001492
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820d2a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820d2a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-3.48053232e-03 -9.22981806e-04 -2.09507924e-03 ... -1.11294850e+01
 -1.11294850e+01 -1.11294850e+01] = SCAN,
rho_a.shape=(6, 6320), rho_b.shape=(6, 6320)
fxc_a.shape=(6320,), fxc_b.shape=(6320,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12640), fxc.shape=(12640,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(2, 6320, 2)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12640, 2)
rho_filt.shape=(12640,)
mol:  [['N', array([0., 0., 0.])]]
converged SCF energy = -54.5289742046675  <S^2> = 3.7524945  2S+1 = 4.0012471
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820fbe0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820fbe0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5016, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.10256797e-03 -5.98013179e-04 -6.71209617e-05 ... -5.03581543e+00
 -5.03581543e+00 -5.03581543e+00] = SCAN,
rho_a.shape=(6, 5016), rho_b.shape=(6, 5016)
fxc_a.shape=(5016,), fxc_b.shape=(5016,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 10032), fxc.shape=(10032,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5016, 22)
rho shape (4, 4, 2, 5016)
rho_filt shape: (5016,)
get descriptors tdrho.shape=(2, 5016, 2)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(10032, 2)
rho_filt.shape=(10032,)
mol:  [['H', array([0., 0., 0.])]]
converged SCF energy = -0.499812984008539  <S^2> = 0.75  2S+1 = 2
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820d480> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820d480> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 7, 7)
ao.shape (10, 2440, 7)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.59173730e-03 -7.67300285e-04 -6.09330303e-05 ... -7.27736608e-01
 -7.27736608e-01 -7.27736608e-01] = SCAN,
rho_a.shape=(6, 2440), rho_b.shape=(6, 2440)
fxc_a.shape=(2440,), fxc_b.shape=(2440,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 2440), fxc.shape=(2440,)
get_data, dm shape = (2, 7, 7)
ao_eval.shape=(4, 2440, 7)
rho shape (4, 4, 2, 2440)
rho_filt shape: (2440,)
get descriptors tdrho.shape=(2, 2440, 2)
mol:  [['Li', array([0., 0., 0.])]]
converged SCF energy = -7.46006188627842  <S^2> = 0.75000049  2S+1 = 2.0000005
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820e6e0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820e6e0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 4592, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-6.71507910e-03 -1.45299376e-03 -1.45299376e-03 ... -1.46930969e-02
 -2.05021258e+00 -2.05021258e+00] = SCAN,
rho_a.shape=(6, 4592), rho_b.shape=(6, 4592)
fxc_a.shape=(4592,), fxc_b.shape=(4592,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 9184), fxc.shape=(9184,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 4592, 22)
rho shape (4, 4, 2, 4592)
rho_filt shape: (4592,)
get descriptors tdrho.shape=(2, 4592, 2)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(9184, 2)
rho_filt.shape=(9184,)
mol:  [['O', array([0., 0., 0.])]]
converged SCF energy = -75.003383488575  <S^2> = 2.0027437  2S+1 = 3.0018286
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820cc70> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820cc70> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5040, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.10003615e-04 -1.41439129e-04 -7.12423491e-06 ... -5.78388625e+00
 -5.78388625e+00 -5.78388625e+00] = SCAN,
rho_a.shape=(6, 5040), rho_b.shape=(6, 5040)
fxc_a.shape=(5040,), fxc_b.shape=(5040,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 10080), fxc.shape=(10080,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5040, 22)
rho shape (4, 4, 2, 5040)
rho_filt shape: (5040,)
get descriptors tdrho.shape=(2, 5040, 2)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(10080, 2)
rho_filt.shape=(10080,)
mol:  [['Cl', array([0., 0., 0.])]]
converged SCF energy = -459.957577120834  <S^2> = 0.75161941  2S+1 = 2.0016188
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820fb50> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820fb50> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6152, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.04138993e-03 -1.04126500e-03 -3.70769624e-04 ... -1.26646370e+01
 -1.26646370e+01 -1.26646370e+01] = SCAN,
rho_a.shape=(6, 6152), rho_b.shape=(6, 6152)
fxc_a.shape=(6152,), fxc_b.shape=(6152,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12304), fxc.shape=(12304,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6152, 30)
rho shape (4, 4, 2, 6152)
rho_filt shape: (6152,)
get descriptors tdrho.shape=(2, 6152, 2)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12304, 2)
rho_filt.shape=(12304,)
mol:  [['Al', array([0., 0., 0.])]]
converged SCF energy = -242.22656098921  <S^2> = 0.75226414  2S+1 = 2.0022629
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820d0c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820d0c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6088, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.39521696e-02 -8.69306060e-03 -4.30027045e-03 ... -1.39756529e-04
 -1.04859832e-03 -7.75066292e-05] = SCAN,
rho_a.shape=(6, 6088), rho_b.shape=(6, 6088)
fxc_a.shape=(6088,), fxc_b.shape=(6088,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12176), fxc.shape=(12176,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6088, 30)
rho shape (4, 4, 2, 6088)
rho_filt shape: (6088,)
get descriptors tdrho.shape=(2, 6088, 2)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12176, 2)
rho_filt.shape=(12176,)
mol:  [['S', array([0., 0., 0.])]]
converged SCF energy = -397.938786807398  <S^2> = 2.0022329  2S+1 = 3.0014882
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820c250> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820c250> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.23872019e-03 -9.01909626e-04 -9.92388620e-04 ... -1.18982463e+01
 -1.18982463e+01 -1.18982463e+01] = SCAN,
rho_a.shape=(6, 6320), rho_b.shape=(6, 6320)
fxc_a.shape=(6320,), fxc_b.shape=(6320,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12640), fxc.shape=(12640,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(2, 6320, 2)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12640, 2)
rho_filt.shape=(12640,)
mol:  [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]]
converged SCF energy = -1.16580491182912  <S^2> = -4.4408921e-16  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820e350> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820e350> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 14, 14)
ao.shape (10, 4776, 14)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.31556415e-04 -9.72662915e-06 -3.66768501e-04 ... -5.54165573e-01
 -5.54165573e-01 -5.54165573e-01] = SCAN,
rho_a.shape=(6, 4776), rho_b.shape=(6, 4776)
fxc_a.shape=(4776,), fxc_b.shape=(4776,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 4776), fxc.shape=(4776,)
get_data, dm shape = (2, 14, 14)
ao_eval.shape=(4, 4776, 14)
rho shape (4, 4, 2, 4776)
rho_filt shape: (4776,)
get descriptors tdrho.shape=(2, 4776, 2)
mol:  [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]]
converged SCF energy = -109.439263799566  <S^2> = 1.5987212e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820d690> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820d690> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9848, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-9.68469910e-05 -9.84742277e-04 -2.59676148e-04 ... -2.39626668e-05
 -2.39626668e-05 -9.68469910e-05] = SCAN,
rho_a.shape=(6, 9848), rho_b.shape=(6, 9848)
fxc_a.shape=(9848,), fxc_b.shape=(9848,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9848), fxc.shape=(9848,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9848, 44)
rho shape (4, 4, 2, 9848)
rho_filt shape: (9848,)
get descriptors tdrho.shape=(2, 9848, 2)
mol:  [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]]
converged SCF energy = -107.339357395734  <S^2> = 4.4408921e-15  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820ef80> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820ef80> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9752, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.04987750e-03 -6.68953858e-04 -8.57556270e-04 ... -1.07485583e-03
 -8.01425698e-01 -8.01425698e-01] = SCAN,
rho_a.shape=(6, 9752), rho_b.shape=(6, 9752)
fxc_a.shape=(9752,), fxc_b.shape=(9752,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9752), fxc.shape=(9752,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9752, 44)
rho shape (4, 4, 2, 9752)
rho_filt shape: (9752,)
get descriptors tdrho.shape=(2, 9752, 2)
mol:  [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]]
converged SCF energy = -93.3377924465129  <S^2> = 4.0072479e-10  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820d450> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820d450> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 12256, 51)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.97917285e-04 -2.54412366e-05 -3.15182243e-05 ... -6.37386500e-01
 -6.37386500e-01 -6.37386500e-01] = SCAN,
rho_a.shape=(6, 12256), rho_b.shape=(6, 12256)
fxc_a.shape=(12256,), fxc_b.shape=(12256,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12256), fxc.shape=(12256,)
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 12256, 51)
rho shape (4, 4, 2, 12256)
rho_filt shape: (12256,)
get descriptors tdrho.shape=(2, 12256, 2)
mol:  [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]]
converged SCF energy = -188.456965322843  <S^2> = 1.0658141e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820fb80> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820fb80> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 66, 66)
ao.shape (10, 14920, 66)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.50217115e-04 -2.07520066e-04 -9.23619896e-04 ... -2.74295208e-06
 -4.27559894e+00 -4.27559894e+00] = SCAN,
rho_a.shape=(6, 14920), rho_b.shape=(6, 14920)
fxc_a.shape=(14920,), fxc_b.shape=(14920,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 14920), fxc.shape=(14920,)
get_data, dm shape = (2, 66, 66)
ao_eval.shape=(4, 14920, 66)
rho shape (4, 4, 2, 14920)
rho_filt shape: (14920,)
get descriptors tdrho.shape=(2, 14920, 2)
mol:  [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]]
converged SCF energy = -920.005608888959  <S^2> = 4.9737992e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820ed40> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820ed40> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12208, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00155834 -0.00091688 -0.00215831 ... -0.00091688 -0.41618507
 -0.41618507] = SCAN,
rho_a.shape=(6, 12208), rho_b.shape=(6, 12208)
fxc_a.shape=(12208,), fxc_b.shape=(12208,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12208), fxc.shape=(12208,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12208, 60)
rho shape (4, 4, 2, 12208)
rho_filt shape: (12208,)
get descriptors tdrho.shape=(2, 12208, 2)
mol:  [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]]
converged SCF energy = -199.394370591172  <S^2> = 1.1546319e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820c1c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820c1c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9824, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.92948614e-04 -1.95198688e-05 -1.16699802e-03 ... -4.89378340e-01
 -4.89378340e-01 -4.89378340e-01] = SCAN,
rho_a.shape=(6, 9824), rho_b.shape=(6, 9824)
fxc_a.shape=(9824,), fxc_b.shape=(9824,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9824), fxc.shape=(9824,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9824, 44)
rho shape (4, 4, 2, 9824)
rho_filt shape: (9824,)
get descriptors tdrho.shape=(2, 9824, 2)
mol:  [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]]
converged SCF energy = -150.214894405799  <S^2> = 1.0018598  2S+1 = 2.2377308
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820e4a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820e4a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9912, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-3.12999834e-04 -5.28727802e-05 -2.62764780e-06 ... -6.59150659e-01
 -6.59150659e-01 -6.59150659e-01] = SCAN,
rho_a.shape=(6, 9912), rho_b.shape=(6, 9912)
fxc_a.shape=(9912,), fxc_b.shape=(9912,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9912), fxc.shape=(9912,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9912, 44)
rho shape (4, 4, 2, 9912)
rho_filt shape: (9912,)
get descriptors tdrho.shape=(2, 9912, 2)
mol:  [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]]
converged SCF energy = -77.2435048346373  <S^2> = 8.8817842e-15  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820f8b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820f8b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 58, 58)
ao.shape (10, 15208, 58)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.83270456e-05 -8.83270456e-05 -9.75839850e-04 ... -3.46719667e-05
 -3.31708644e-05 -3.31708644e-05] = SCAN,
rho_a.shape=(6, 15208), rho_b.shape=(6, 15208)
fxc_a.shape=(15208,), fxc_b.shape=(15208,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 15208), fxc.shape=(15208,)
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15208, 58)
rho shape (4, 4, 2, 15208)
rho_filt shape: (15208,)
get descriptors tdrho.shape=(2, 15208, 2)
mol:  [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]]
converged SCF energy = -113.221335689652  <S^2> = 6.5814021e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820e4d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820e4d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 10040, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-5.37000578e-04 -8.55494549e-04 -2.46853288e-03 ... -7.34251999e-01
 -7.34251999e-01 -7.34251999e-01] = SCAN,
rho_a.shape=(6, 10040), rho_b.shape=(6, 10040)
fxc_a.shape=(10040,), fxc_b.shape=(10040,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 10040), fxc.shape=(10040,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 10040, 44)
rho shape (4, 4, 2, 10040)
rho_filt shape: (10040,)
get descriptors tdrho.shape=(2, 10040, 2)
mol:  [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]]
converged SCF energy = -460.624592374078  <S^2> = 7.283063e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820fa60> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820fa60> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 37, 37)
ao.shape (10, 8552, 37)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.38161177e-04 -1.81188367e-05 -2.37300299e-05 ... -2.83738108e+00
 -2.83738108e+00 -2.83738108e+00] = SCAN,
rho_a.shape=(6, 8552), rho_b.shape=(6, 8552)
fxc_a.shape=(8552,), fxc_b.shape=(8552,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 8552), fxc.shape=(8552,)
get_data, dm shape = (2, 37, 37)
ao_eval.shape=(4, 8552, 37)
rho shape (4, 4, 2, 8552)
rho_filt shape: (8552,)
get descriptors tdrho.shape=(2, 8552, 2)
mol:  [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]]
converged SCF energy = -8.04458854018923  <S^2> = 7.5051076e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820cbb0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820cbb0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 29, 29)
ao.shape (10, 6936, 29)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00113445 -0.00118624 -0.00118624 ... -0.48434639 -0.48434639
 -0.48434639] = SCAN,
rho_a.shape=(6, 6936), rho_b.shape=(6, 6936)
fxc_a.shape=(6936,), fxc_b.shape=(6936,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 6936), fxc.shape=(6936,)
get_data, dm shape = (2, 29, 29)
ao_eval.shape=(4, 6936, 29)
rho shape (4, 4, 2, 6936)
rho_filt shape: (6936,)
get descriptors tdrho.shape=(2, 6936, 2)
mol:  [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]]
converged SCF energy = -324.340512506579  <S^2> = 1.5853985e-11  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820ff10> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820ff10> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 11536, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00297935 -0.00297935 -0.00407089 ... -0.00297935 -0.00297935
 -0.00407089] = SCAN,
rho_a.shape=(6, 11536), rho_b.shape=(6, 11536)
fxc_a.shape=(11536,), fxc_b.shape=(11536,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 11536), fxc.shape=(11536,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 11536, 60)
rho shape (4, 4, 2, 11536)
rho_filt shape: (11536,)
get descriptors tdrho.shape=(2, 11536, 2)
mol:  [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]]
converged SCF energy = -1622.57507845814  <S^2> = 8.3844043e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820dd80> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820dd80> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 120, 120)
ao.shape (10, 24512, 120)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.61400996e-04 -4.90484900e-04 -2.56451718e-03 ... -9.59296113e+00
 -9.59296113e+00 -9.59296113e+00] = SCAN,
rho_a.shape=(6, 24512), rho_b.shape=(6, 24512)
fxc_a.shape=(24512,), fxc_b.shape=(24512,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 24512), fxc.shape=(24512,)
get_data, dm shape = (2, 120, 120)
ao_eval.shape=(4, 24512, 120)
rho shape (4, 4, 2, 24512)
rho_filt shape: (24512,)
get descriptors tdrho.shape=(2, 24512, 2)
mol:  [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]]
converged SCF energy = -342.979728469574  <S^2> = 2.5387692e-11  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820dae0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820dae0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 13096, 51)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.28637920e-03 -4.32383300e-04 -3.74057272e-05 ... -1.91722770e+00
 -1.91722770e+00 -1.91722770e+00] = SCAN,
rho_a.shape=(6, 13096), rho_b.shape=(6, 13096)
fxc_a.shape=(13096,), fxc_b.shape=(13096,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 13096), fxc.shape=(13096,)
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 13096, 51)
rho shape (4, 4, 2, 13096)
rho_filt shape: (13096,)
get descriptors tdrho.shape=(2, 13096, 2)
mol:  [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]]
converged SCF energy = -578.565335952491  <S^2> = 1.0034705  2S+1 = 2.2391699
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820c700> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820c700> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12384, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.59396665e-04 -2.59124431e-04 -2.59711371e-04 ... -3.86944059e-01
 -3.86944059e-01 -3.86944059e-01] = SCAN,
rho_a.shape=(6, 12384), rho_b.shape=(6, 12384)
fxc_a.shape=(12384,), fxc_b.shape=(12384,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12384), fxc.shape=(12384,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12384, 60)
rho shape (4, 4, 2, 12384)
rho_filt shape: (12384,)
get descriptors tdrho.shape=(2, 12384, 2)
mol:  [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]]
converged SCF energy = -40.4598214864077  <S^2> = 3.1885605e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820cc10> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820cc10> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 50, 50)
ao.shape (10, 13936, 50)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.68439986e-04 -2.42462569e-04 -1.69927031e-05 ... -2.55230307e-05
 -2.55230307e-05 -2.55230307e-05] = SCAN,
rho_a.shape=(6, 13936), rho_b.shape=(6, 13936)
fxc_a.shape=(13936,), fxc_b.shape=(13936,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 13936), fxc.shape=(13936,)
get_data, dm shape = (2, 50, 50)
ao_eval.shape=(4, 13936, 50)
rho shape (4, 4, 2, 13936)
rho_filt shape: (13936,)
get descriptors tdrho.shape=(2, 13936, 2)
mol:  [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]]
converged SCF energy = -39.0756147483504  <S^2> = 6.2030381e-12  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820dc30> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820dc30> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 36, 36)
ao.shape (10, 9656, 36)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-7.67688751e-04 -4.57393214e-05 -2.02834191e-04 ... -1.14928924e+00
 -1.14928924e+00 -1.14928924e+00] = SCAN,
rho_a.shape=(6, 9656), rho_b.shape=(6, 9656)
fxc_a.shape=(9656,), fxc_b.shape=(9656,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9656), fxc.shape=(9656,)
get_data, dm shape = (2, 36, 36)
ao_eval.shape=(4, 9656, 36)
rho shape (4, 4, 2, 9656)
rho_filt shape: (9656,)
get descriptors tdrho.shape=(2, 9656, 2)
mol:  [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]]
converged SCF energy = -291.719272437819  <S^2> = 1.3145041e-11  2S+1 = 1
New DM shape: (2, 58, 58)
ao.shape (10, 15256, 58)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.33850535e-04 -2.34903029e-04 -1.75623665e-05 ... -1.92891112e-05
 -1.92891112e-05 -1.92891112e-05] = SCAN,
rho_a.shape=(6, 15256), rho_b.shape=(6, 15256)
fxc_a.shape=(15256,), fxc_b.shape=(15256,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 15256), fxc.shape=(15256,)
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15256, 58)
rho shape (4, 4, 2, 15256)
rho_filt shape: (15256,)
get descriptors tdrho.shape=(2, 15256, 2)
localnet.spin_scaling: concatenating the data
first data shape = (10940, 2)
concatenated: tdrho.shape=(271699, 2)
PRE NAN FILT: tFxc.shape=(271699,), tdrho.shape=(271699, 2)
nan_filt_rho.shape=(271699,)
nan_filt_fxc.shape=(271699,)
tFxc.shape=(271699,), tdrho.shape=(271699, 2)
inp[0].shape = (271699, 1)
Epoch 0
Epoch 0 :: Batch 0/1
Batch Loss = 5.567271014159502
0, epoch_train_loss=5.567271014159502
Epoch 1
Epoch 1 :: Batch 0/1
Batch Loss = 5.033966692050044
1, epoch_train_loss=5.033966692050044
Epoch 2
Epoch 2 :: Batch 0/1
Batch Loss = 4.554753529649199
2, epoch_train_loss=4.554753529649199
Epoch 3
Epoch 3 :: Batch 0/1
Batch Loss = 4.201209685588494
3, epoch_train_loss=4.201209685588494
Epoch 4
Epoch 4 :: Batch 0/1
Batch Loss = 3.91174199659232
4, epoch_train_loss=3.91174199659232
Epoch 5
Epoch 5 :: Batch 0/1
Batch Loss = 3.8392053985037813
5, epoch_train_loss=3.8392053985037813
Epoch 6
Epoch 6 :: Batch 0/1
Batch Loss = 3.6700268683554174
6, epoch_train_loss=3.6700268683554174
Epoch 7
Epoch 7 :: Batch 0/1
Batch Loss = 3.561274822676202
7, epoch_train_loss=3.561274822676202
Epoch 8
Epoch 8 :: Batch 0/1
Batch Loss = 3.5457740908368667
8, epoch_train_loss=3.5457740908368667
Epoch 9
Epoch 9 :: Batch 0/1
Batch Loss = 3.552604378458118
9, epoch_train_loss=3.552604378458118
Epoch 10
Epoch 10 :: Batch 0/1
Batch Loss = 3.5511483854107406
10, epoch_train_loss=3.5511483854107406
Epoch 11
Epoch 11 :: Batch 0/1
Batch Loss = 3.5276234272632303
11, epoch_train_loss=3.5276234272632303
Epoch 12
Epoch 12 :: Batch 0/1
Batch Loss = 3.510823663669471
12, epoch_train_loss=3.510823663669471
Epoch 13
Epoch 13 :: Batch 0/1
Batch Loss = 3.506294761161934
13, epoch_train_loss=3.506294761161934
Epoch 14
Epoch 14 :: Batch 0/1
Batch Loss = 3.517204787808884
14, epoch_train_loss=3.517204787808884
Epoch 15
Epoch 15 :: Batch 0/1
Batch Loss = 3.527530427345127
15, epoch_train_loss=3.527530427345127
Epoch 16
Epoch 16 :: Batch 0/1
Batch Loss = 3.5107639840218514
16, epoch_train_loss=3.5107639840218514
Epoch 17
Epoch 17 :: Batch 0/1
Batch Loss = 3.4962301986078956
17, epoch_train_loss=3.4962301986078956
Epoch 18
Epoch 18 :: Batch 0/1
Batch Loss = 3.4984574884542505
18, epoch_train_loss=3.4984574884542505
Epoch 19
Epoch 19 :: Batch 0/1
Batch Loss = 3.4913129785402073
19, epoch_train_loss=3.4913129785402073
Epoch 20
Epoch 20 :: Batch 0/1
Batch Loss = 3.4973915087019445
20, epoch_train_loss=3.4973915087019445
Epoch 21
Epoch 21 :: Batch 0/1
Batch Loss = 3.4955498708507537
21, epoch_train_loss=3.4955498708507537
Epoch 22
Epoch 22 :: Batch 0/1
Batch Loss = 3.493021032700775
22, epoch_train_loss=3.493021032700775
Epoch 23
Epoch 23 :: Batch 0/1
Batch Loss = 3.4865046086062903
23, epoch_train_loss=3.4865046086062903
Epoch 24
Epoch 24 :: Batch 0/1
Batch Loss = 3.481423997727751
24, epoch_train_loss=3.481423997727751
Epoch 25
Epoch 25 :: Batch 0/1
Batch Loss = 3.4775037345901882
25, epoch_train_loss=3.4775037345901882
Epoch 26
Epoch 26 :: Batch 0/1
Batch Loss = 3.475930720519389
26, epoch_train_loss=3.475930720519389
Epoch 27
Epoch 27 :: Batch 0/1
Batch Loss = 3.4756331377055747
27, epoch_train_loss=3.4756331377055747
Epoch 28
Epoch 28 :: Batch 0/1
Batch Loss = 3.4756282837262997
28, epoch_train_loss=3.4756282837262997
Epoch 29
Epoch 29 :: Batch 0/1
Batch Loss = 3.4760846646568364
29, epoch_train_loss=3.4760846646568364
Epoch 30
Epoch 30 :: Batch 0/1
Batch Loss = 3.475261516220646
30, epoch_train_loss=3.475261516220646
Epoch 31
Epoch 31 :: Batch 0/1
Batch Loss = 3.474786216922938
31, epoch_train_loss=3.474786216922938
Epoch 32
Epoch 32 :: Batch 0/1
Batch Loss = 3.4732294098153074
32, epoch_train_loss=3.4732294098153074
Epoch 33
Epoch 33 :: Batch 0/1
Batch Loss = 3.472554342275369
33, epoch_train_loss=3.472554342275369
Epoch 34
Epoch 34 :: Batch 0/1
Batch Loss = 3.4716903944039585
34, epoch_train_loss=3.4716903944039585
Epoch 35
Epoch 35 :: Batch 0/1
Batch Loss = 3.471560242713435
35, epoch_train_loss=3.471560242713435
Epoch 36
Epoch 36 :: Batch 0/1
Batch Loss = 3.471644013257491
36, epoch_train_loss=3.471644013257491
Epoch 37
Epoch 37 :: Batch 0/1
Batch Loss = 3.471567506746978
37, epoch_train_loss=3.471567506746978
Epoch 38
Epoch 38 :: Batch 0/1
Batch Loss = 3.4716663597791864
38, epoch_train_loss=3.4716663597791864
Epoch 39
Epoch 39 :: Batch 0/1
Batch Loss = 3.4708508562283638
39, epoch_train_loss=3.4708508562283638
Epoch 40
Epoch 40 :: Batch 0/1
Batch Loss = 3.470485801512792
40, epoch_train_loss=3.470485801512792
Epoch 41
Epoch 41 :: Batch 0/1
Batch Loss = 3.469273158967615
41, epoch_train_loss=3.469273158967615
Epoch 42
Epoch 42 :: Batch 0/1
Batch Loss = 3.4689114884276417
42, epoch_train_loss=3.4689114884276417
Epoch 43
Epoch 43 :: Batch 0/1
Batch Loss = 3.4679939455910507
43, epoch_train_loss=3.4679939455910507
Epoch 44
Epoch 44 :: Batch 0/1
Batch Loss = 3.4679838249288446
44, epoch_train_loss=3.4679838249288446
Epoch 45
Epoch 45 :: Batch 0/1
Batch Loss = 3.46746728204302
45, epoch_train_loss=3.46746728204302
Epoch 46
Epoch 46 :: Batch 0/1
Batch Loss = 3.467445122697242
46, epoch_train_loss=3.467445122697242
Epoch 47
Epoch 47 :: Batch 0/1
Batch Loss = 3.4669208661448883
47, epoch_train_loss=3.4669208661448883
Epoch 48
Epoch 48 :: Batch 0/1
Batch Loss = 3.4665612914175243
48, epoch_train_loss=3.4665612914175243
Epoch 49
Epoch 49 :: Batch 0/1
Batch Loss = 3.466007915290197
49, epoch_train_loss=3.466007915290197
Epoch 50
Epoch 50 :: Batch 0/1
Batch Loss = 3.465496369084703
50, epoch_train_loss=3.465496369084703
Epoch 51
Epoch 51 :: Batch 0/1
Batch Loss = 3.465162745495587
51, epoch_train_loss=3.465162745495587
Epoch 52
Epoch 52 :: Batch 0/1
Batch Loss = 3.4647633237882633
52, epoch_train_loss=3.4647633237882633
Epoch 53
Epoch 53 :: Batch 0/1
Batch Loss = 3.4646593348434065
53, epoch_train_loss=3.4646593348434065
Epoch 54
Epoch 54 :: Batch 0/1
Batch Loss = 3.464299194065655
54, epoch_train_loss=3.464299194065655
Epoch 55
Epoch 55 :: Batch 0/1
Batch Loss = 3.464174314401578
55, epoch_train_loss=3.464174314401578
Epoch 56
Epoch 56 :: Batch 0/1
Batch Loss = 3.4637219709849214
56, epoch_train_loss=3.4637219709849214
Epoch 57
Epoch 57 :: Batch 0/1
Batch Loss = 3.4635155734034826
57, epoch_train_loss=3.4635155734034826
Epoch 58
Epoch 58 :: Batch 0/1
Batch Loss = 3.4631175317793432
58, epoch_train_loss=3.4631175317793432
Epoch 59
Epoch 59 :: Batch 0/1
Batch Loss = 3.4629657930651856
59, epoch_train_loss=3.4629657930651856
Epoch 60
Epoch 60 :: Batch 0/1
Batch Loss = 3.4627618328833485
60, epoch_train_loss=3.4627618328833485
Epoch 61
Epoch 61 :: Batch 0/1
Batch Loss = 3.4626234555237825
61, epoch_train_loss=3.4626234555237825
Epoch 62
Epoch 62 :: Batch 0/1
Batch Loss = 3.4624998059592778
62, epoch_train_loss=3.4624998059592778
Epoch 63
Epoch 63 :: Batch 0/1
Batch Loss = 3.4622557058491483
63, epoch_train_loss=3.4622557058491483
Epoch 64
Epoch 64 :: Batch 0/1
Batch Loss = 3.462112457819477
64, epoch_train_loss=3.462112457819477
Epoch 65
Epoch 65 :: Batch 0/1
Batch Loss = 3.461860884081048
65, epoch_train_loss=3.461860884081048
Epoch 66
Epoch 66 :: Batch 0/1
Batch Loss = 3.4617382626621693
66, epoch_train_loss=3.4617382626621693
Epoch 67
Epoch 67 :: Batch 0/1
Batch Loss = 3.4616013685634965
67, epoch_train_loss=3.4616013685634965
Epoch 68
Epoch 68 :: Batch 0/1
Batch Loss = 3.4614851920372494
68, epoch_train_loss=3.4614851920372494
Epoch 69
Epoch 69 :: Batch 0/1
Batch Loss = 3.4614080516751913
69, epoch_train_loss=3.4614080516751913
Epoch 70
Epoch 70 :: Batch 0/1
Batch Loss = 3.461259638644584
70, epoch_train_loss=3.461259638644584
Epoch 71
Epoch 71 :: Batch 0/1
Batch Loss = 3.4611755889711113
71, epoch_train_loss=3.4611755889711113
Epoch 72
Epoch 72 :: Batch 0/1
Batch Loss = 3.4610562927610373
72, epoch_train_loss=3.4610562927610373
Epoch 73
Epoch 73 :: Batch 0/1
Batch Loss = 3.4609711670679975
73, epoch_train_loss=3.4609711670679975
Epoch 74
Epoch 74 :: Batch 0/1
Batch Loss = 3.46091707963073
74, epoch_train_loss=3.46091707963073
Epoch 75
Epoch 75 :: Batch 0/1
Batch Loss = 3.460830629730286
75, epoch_train_loss=3.460830629730286
Epoch 76
Epoch 76 :: Batch 0/1
Batch Loss = 3.46078150471407
76, epoch_train_loss=3.46078150471407
Epoch 77
Epoch 77 :: Batch 0/1
Batch Loss = 3.4607080909987245
77, epoch_train_loss=3.4607080909987245
Epoch 78
Epoch 78 :: Batch 0/1
Batch Loss = 3.4606404281841265
78, epoch_train_loss=3.4606404281841265
Epoch 79
Epoch 79 :: Batch 0/1
Batch Loss = 3.460604613477986
79, epoch_train_loss=3.460604613477986
Epoch 80
Epoch 80 :: Batch 0/1
Batch Loss = 3.460553356457929
80, epoch_train_loss=3.460553356457929
Epoch 81
Epoch 81 :: Batch 0/1
Batch Loss = 3.4605245300368215
81, epoch_train_loss=3.4605245300368215
Epoch 82
Epoch 82 :: Batch 0/1
Batch Loss = 3.460489518887231
82, epoch_train_loss=3.460489518887231
Epoch 83
Epoch 83 :: Batch 0/1
Batch Loss = 3.460437679615914
83, epoch_train_loss=3.460437679615914
Epoch 84
Epoch 84 :: Batch 0/1
Batch Loss = 3.460401527251782
84, epoch_train_loss=3.460401527251782
Epoch 85
Epoch 85 :: Batch 0/1
Batch Loss = 3.4603572762075423
85, epoch_train_loss=3.4603572762075423
Epoch 86
Epoch 86 :: Batch 0/1
Batch Loss = 3.4603197175524487
86, epoch_train_loss=3.4603197175524487
Epoch 87
Epoch 87 :: Batch 0/1
Batch Loss = 3.460295332615819
87, epoch_train_loss=3.460295332615819
Epoch 88
Epoch 88 :: Batch 0/1
Batch Loss = 3.4602598549984624
88, epoch_train_loss=3.4602598549984624
Epoch 89
Epoch 89 :: Batch 0/1
Batch Loss = 3.460227633003775
89, epoch_train_loss=3.460227633003775
Epoch 90
Epoch 90 :: Batch 0/1
Batch Loss = 3.460199637202744
90, epoch_train_loss=3.460199637202744
Epoch 91
Epoch 91 :: Batch 0/1
Batch Loss = 3.460165979585223
91, epoch_train_loss=3.460165979585223
Epoch 92
Epoch 92 :: Batch 0/1
Batch Loss = 3.4601406518833513
92, epoch_train_loss=3.4601406518833513
Epoch 93
Epoch 93 :: Batch 0/1
Batch Loss = 3.4601193266504815
93, epoch_train_loss=3.4601193266504815
Epoch 94
Epoch 94 :: Batch 0/1
Batch Loss = 3.460092062707344
94, epoch_train_loss=3.460092062707344
Epoch 95
Epoch 95 :: Batch 0/1
Batch Loss = 3.460068057938978
95, epoch_train_loss=3.460068057938978
Epoch 96
Epoch 96 :: Batch 0/1
Batch Loss = 3.4600461447032917
96, epoch_train_loss=3.4600461447032917
Epoch 97
Epoch 97 :: Batch 0/1
Batch Loss = 3.4600221813308902
97, epoch_train_loss=3.4600221813308902
Epoch 98
Epoch 98 :: Batch 0/1
Batch Loss = 3.460003203250659
98, epoch_train_loss=3.460003203250659
Epoch 99
Epoch 99 :: Batch 0/1
Batch Loss = 3.459986854267819
99, epoch_train_loss=3.459986854267819
Epoch 100
Epoch 100 :: Batch 0/1
Batch Loss = 3.459966780050675
100, epoch_train_loss=3.459966780050675
Epoch 101
Epoch 101 :: Batch 0/1
Batch Loss = 3.459947048948622
101, epoch_train_loss=3.459947048948622
Epoch 102
Epoch 102 :: Batch 0/1
Batch Loss = 3.4599295337492575
102, epoch_train_loss=3.4599295337492575
Epoch 103
Epoch 103 :: Batch 0/1
Batch Loss = 3.459911272838145
103, epoch_train_loss=3.459911272838145
Epoch 104
Epoch 104 :: Batch 0/1
Batch Loss = 3.4598941845340594
104, epoch_train_loss=3.4598941845340594
Epoch 105
Epoch 105 :: Batch 0/1
Batch Loss = 3.4598795661487616
105, epoch_train_loss=3.4598795661487616
Epoch 106
Epoch 106 :: Batch 0/1
Batch Loss = 3.459864003707069
106, epoch_train_loss=3.459864003707069
Epoch 107
Epoch 107 :: Batch 0/1
Batch Loss = 3.459847403411579
107, epoch_train_loss=3.459847403411579
Epoch 108
Epoch 108 :: Batch 0/1
Batch Loss = 3.459832503308471
108, epoch_train_loss=3.459832503308471
Epoch 109
Epoch 109 :: Batch 0/1
Batch Loss = 3.4598186231412162
109, epoch_train_loss=3.4598186231412162
Epoch 110
Epoch 110 :: Batch 0/1
Batch Loss = 3.4598042081428044
110, epoch_train_loss=3.4598042081428044
Epoch 111
Epoch 111 :: Batch 0/1
Batch Loss = 3.4597900939150477
111, epoch_train_loss=3.4597900939150477
Epoch 112
Epoch 112 :: Batch 0/1
Batch Loss = 3.4597766593649366
112, epoch_train_loss=3.4597766593649366
Epoch 113
Epoch 113 :: Batch 0/1
Batch Loss = 3.459762880214806
113, epoch_train_loss=3.459762880214806
Epoch 114
Epoch 114 :: Batch 0/1
Batch Loss = 3.4597490093713184
114, epoch_train_loss=3.4597490093713184
Epoch 115
Epoch 115 :: Batch 0/1
Batch Loss = 3.4597360116232383
115, epoch_train_loss=3.4597360116232383
Epoch 116
Epoch 116 :: Batch 0/1
Batch Loss = 3.4597232991340543
116, epoch_train_loss=3.4597232991340543
Epoch 117
Epoch 117 :: Batch 0/1
Batch Loss = 3.4597098438981866
117, epoch_train_loss=3.4597098438981866
Epoch 118
Epoch 118 :: Batch 0/1
Batch Loss = 3.459696071503007
118, epoch_train_loss=3.459696071503007
Epoch 119
Epoch 119 :: Batch 0/1
Batch Loss = 3.459682766480857
119, epoch_train_loss=3.459682766480857
Epoch 120
Epoch 120 :: Batch 0/1
Batch Loss = 3.4596697384519737
120, epoch_train_loss=3.4596697384519737
Epoch 121
Epoch 121 :: Batch 0/1
Batch Loss = 3.459656540799046
121, epoch_train_loss=3.459656540799046
Epoch 122
Epoch 122 :: Batch 0/1
Batch Loss = 3.4596432567242887
122, epoch_train_loss=3.4596432567242887
Epoch 123
Epoch 123 :: Batch 0/1
Batch Loss = 3.4596301335589783
123, epoch_train_loss=3.4596301335589783
Epoch 124
Epoch 124 :: Batch 0/1
Batch Loss = 3.459617069428556
124, epoch_train_loss=3.459617069428556
Epoch 125
Epoch 125 :: Batch 0/1
Batch Loss = 3.4596039282951168
125, epoch_train_loss=3.4596039282951168
Epoch 126
Epoch 126 :: Batch 0/1
Batch Loss = 3.459590845546122
126, epoch_train_loss=3.459590845546122
Epoch 127
Epoch 127 :: Batch 0/1
Batch Loss = 3.459577935160129
127, epoch_train_loss=3.459577935160129
Epoch 128
Epoch 128 :: Batch 0/1
Batch Loss = 3.459565084078558
128, epoch_train_loss=3.459565084078558
Epoch 129
Epoch 129 :: Batch 0/1
Batch Loss = 3.459552166889497
129, epoch_train_loss=3.459552166889497
Epoch 130
Epoch 130 :: Batch 0/1
Batch Loss = 3.4595392402222607
130, epoch_train_loss=3.4595392402222607
Epoch 131
Epoch 131 :: Batch 0/1
Batch Loss = 3.4595264448354195
131, epoch_train_loss=3.4595264448354195
Epoch 132
Epoch 132 :: Batch 0/1
Batch Loss = 3.4595137630937987
132, epoch_train_loss=3.4595137630937987
Epoch 133
Epoch 133 :: Batch 0/1
Batch Loss = 3.459501065823662
133, epoch_train_loss=3.459501065823662
Epoch 134
Epoch 134 :: Batch 0/1
Batch Loss = 3.4594883051194145
134, epoch_train_loss=3.4594883051194145
Epoch 135
Epoch 135 :: Batch 0/1
Batch Loss = 3.4594755568382913
135, epoch_train_loss=3.4594755568382913
Epoch 136
Epoch 136 :: Batch 0/1
Batch Loss = 3.4594629109840165
136, epoch_train_loss=3.4594629109840165
Epoch 137
Epoch 137 :: Batch 0/1
Batch Loss = 3.459450354397928
137, epoch_train_loss=3.459450354397928
Epoch 138
Epoch 138 :: Batch 0/1
Batch Loss = 3.4594378089514377
138, epoch_train_loss=3.4594378089514377
Epoch 139
Epoch 139 :: Batch 0/1
Batch Loss = 3.459425233807301
139, epoch_train_loss=3.459425233807301
Epoch 140
Epoch 140 :: Batch 0/1
Batch Loss = 3.459412649100169
140, epoch_train_loss=3.459412649100169
Epoch 141
Epoch 141 :: Batch 0/1
Batch Loss = 3.4594000956092272
141, epoch_train_loss=3.4594000956092272
Epoch 142
Epoch 142 :: Batch 0/1
Batch Loss = 3.4593875768309004
142, epoch_train_loss=3.4593875768309004
Epoch 143
Epoch 143 :: Batch 0/1
Batch Loss = 3.4593750717263707
143, epoch_train_loss=3.4593750717263707
Epoch 144
Epoch 144 :: Batch 0/1
Batch Loss = 3.4593625637134915
144, epoch_train_loss=3.4593625637134915
Epoch 145
Epoch 145 :: Batch 0/1
Batch Loss = 3.459350048764141
145, epoch_train_loss=3.459350048764141
Epoch 146
Epoch 146 :: Batch 0/1
Batch Loss = 3.459337522773098
146, epoch_train_loss=3.459337522773098
Epoch 147
Epoch 147 :: Batch 0/1
Batch Loss = 3.4593249738638683
147, epoch_train_loss=3.4593249738638683
Epoch 148
Epoch 148 :: Batch 0/1
Batch Loss = 3.459312397813187
148, epoch_train_loss=3.459312397813187
Epoch 149
Epoch 149 :: Batch 0/1
Batch Loss = 3.4592998039486527
149, epoch_train_loss=3.4592998039486527
Epoch 150
Epoch 150 :: Batch 0/1
Batch Loss = 3.459287204409581
150, epoch_train_loss=3.459287204409581
Epoch 151
Epoch 151 :: Batch 0/1
Batch Loss = 3.459274596140076
151, epoch_train_loss=3.459274596140076
Epoch 152
Epoch 152 :: Batch 0/1
Batch Loss = 3.4592619610024706
152, epoch_train_loss=3.4592619610024706
Epoch 153
Epoch 153 :: Batch 0/1
Batch Loss = 3.4592492847601033
153, epoch_train_loss=3.4592492847601033
Epoch 154
Epoch 154 :: Batch 0/1
Batch Loss = 3.4592365654841215
154, epoch_train_loss=3.4592365654841215
Epoch 155
Epoch 155 :: Batch 0/1
Batch Loss = 3.4592238075664214
155, epoch_train_loss=3.4592238075664214
Epoch 156
Epoch 156 :: Batch 0/1
Batch Loss = 3.4592110086096888
156, epoch_train_loss=3.4592110086096888
Epoch 157
Epoch 157 :: Batch 0/1
Batch Loss = 3.4591981596183126
157, epoch_train_loss=3.4591981596183126
Epoch 158
Epoch 158 :: Batch 0/1
Batch Loss = 3.4591852541893346
158, epoch_train_loss=3.4591852541893346
Epoch 159
Epoch 159 :: Batch 0/1
Batch Loss = 3.4591722921713637
159, epoch_train_loss=3.4591722921713637
Epoch 160
Epoch 160 :: Batch 0/1
Batch Loss = 3.459159273327129
160, epoch_train_loss=3.459159273327129
Epoch 161
Epoch 161 :: Batch 0/1
Batch Loss = 3.4591461916463566
161, epoch_train_loss=3.4591461916463566
Epoch 162
Epoch 162 :: Batch 0/1
Batch Loss = 3.45913303837556
162, epoch_train_loss=3.45913303837556
Epoch 163
Epoch 163 :: Batch 0/1
Batch Loss = 3.459119808253561
163, epoch_train_loss=3.459119808253561
Epoch 164
Epoch 164 :: Batch 0/1
Batch Loss = 3.4591065002771235
164, epoch_train_loss=3.4591065002771235
Epoch 165
Epoch 165 :: Batch 0/1
Batch Loss = 3.4590931117117916
165, epoch_train_loss=3.4590931117117916
Epoch 166
Epoch 166 :: Batch 0/1
Batch Loss = 3.4590796365441507
166, epoch_train_loss=3.4590796365441507
Epoch 167
Epoch 167 :: Batch 0/1
Batch Loss = 3.459066068248173
167, epoch_train_loss=3.459066068248173
Epoch 168
Epoch 168 :: Batch 0/1
Batch Loss = 3.4590524057811067
168, epoch_train_loss=3.4590524057811067
Epoch 169
Epoch 169 :: Batch 0/1
Batch Loss = 3.459038653519477
169, epoch_train_loss=3.459038653519477
Epoch 170
Epoch 170 :: Batch 0/1
Batch Loss = 3.4590248252101214
170, epoch_train_loss=3.4590248252101214
Epoch 171
Epoch 171 :: Batch 0/1
Batch Loss = 3.4590109545830607
171, epoch_train_loss=3.4590109545830607
Epoch 172
Epoch 172 :: Batch 0/1
Batch Loss = 3.4589971326968474
172, epoch_train_loss=3.4589971326968474
Epoch 173
Epoch 173 :: Batch 0/1
Batch Loss = 3.4589835845014227
173, epoch_train_loss=3.4589835845014227
Epoch 174
Epoch 174 :: Batch 0/1
Batch Loss = 3.4589708641347254
174, epoch_train_loss=3.4589708641347254
Epoch 175
Epoch 175 :: Batch 0/1
Batch Loss = 3.4589603989776503
175, epoch_train_loss=3.4589603989776503
Epoch 176
Epoch 176 :: Batch 0/1
Batch Loss = 3.4589557256225225
176, epoch_train_loss=3.4589557256225225
Epoch 177
Epoch 177 :: Batch 0/1
Batch Loss = 3.4589668116177363
177, epoch_train_loss=3.4589668116177363
Epoch 178
Epoch 178 :: Batch 0/1
Batch Loss = 3.459018050369182
178, epoch_train_loss=3.459018050369182
Epoch 179
Epoch 179 :: Batch 0/1
Batch Loss = 3.459187025737627
179, epoch_train_loss=3.459187025737627
Epoch 180
Epoch 180 :: Batch 0/1
Batch Loss = 3.459644265595714
180, epoch_train_loss=3.459644265595714
Epoch 181
Epoch 181 :: Batch 0/1
Batch Loss = 3.4610452126059674
181, epoch_train_loss=3.4610452126059674
Epoch 182
Epoch 182 :: Batch 0/1
Batch Loss = 3.4641701211886575
182, epoch_train_loss=3.4641701211886575
Epoch 183
Epoch 183 :: Batch 0/1
Batch Loss = 3.4734483304315193
183, epoch_train_loss=3.4734483304315193
Epoch 184
Epoch 184 :: Batch 0/1
Batch Loss = 3.478973730635806
184, epoch_train_loss=3.478973730635806
Epoch 185
Epoch 185 :: Batch 0/1
Batch Loss = 3.486949680434764
185, epoch_train_loss=3.486949680434764
Epoch 186
Epoch 186 :: Batch 0/1
Batch Loss = 3.463068474279467
186, epoch_train_loss=3.463068474279467
Epoch 187
Epoch 187 :: Batch 0/1
Batch Loss = 3.466079140776727
187, epoch_train_loss=3.466079140776727
Epoch 188
Epoch 188 :: Batch 0/1
Batch Loss = 3.4806431665329236
188, epoch_train_loss=3.4806431665329236
Epoch 189
Epoch 189 :: Batch 0/1
Batch Loss = 3.4614767257644585
189, epoch_train_loss=3.4614767257644585
Epoch 190
Epoch 190 :: Batch 0/1
Batch Loss = 3.4660002008930197
190, epoch_train_loss=3.4660002008930197
Epoch 191
Epoch 191 :: Batch 0/1
Batch Loss = 3.478406173274009
191, epoch_train_loss=3.478406173274009
Epoch 192
Epoch 192 :: Batch 0/1
Batch Loss = 3.4599018888888353
192, epoch_train_loss=3.4599018888888353
Epoch 193
Epoch 193 :: Batch 0/1
Batch Loss = 3.4695978000548973
193, epoch_train_loss=3.4695978000548973
Epoch 194
Epoch 194 :: Batch 0/1
Batch Loss = 3.474939509849418
194, epoch_train_loss=3.474939509849418
Epoch 195
Epoch 195 :: Batch 0/1
Batch Loss = 3.4600636230653414
195, epoch_train_loss=3.4600636230653414
Epoch 196
Epoch 196 :: Batch 0/1
Batch Loss = 3.4779222062919932
196, epoch_train_loss=3.4779222062919932
Epoch 197
Epoch 197 :: Batch 0/1
Batch Loss = 3.4681550887591897
197, epoch_train_loss=3.4681550887591897
Epoch 198
Epoch 198 :: Batch 0/1
Batch Loss = 3.4650766448166506
198, epoch_train_loss=3.4650766448166506
Epoch 199
Epoch 199 :: Batch 0/1
Batch Loss = 3.474242462804398
199, epoch_train_loss=3.474242462804398
Epoch 200
Epoch 200 :: Batch 0/1
Batch Loss = 3.4594360665076462
200, epoch_train_loss=3.4594360665076462
Epoch 201
Epoch 201 :: Batch 0/1
Batch Loss = 3.467549230104572
201, epoch_train_loss=3.467549230104572
Epoch 202
Epoch 202 :: Batch 0/1
Batch Loss = 3.4615668377908246
202, epoch_train_loss=3.4615668377908246
Epoch 203
Epoch 203 :: Batch 0/1
Batch Loss = 3.4619641782575408
203, epoch_train_loss=3.4619641782575408
Epoch 204
Epoch 204 :: Batch 0/1
Batch Loss = 3.4644045219705726
204, epoch_train_loss=3.4644045219705726
Epoch 205
Epoch 205 :: Batch 0/1
Batch Loss = 3.4595814618248246
205, epoch_train_loss=3.4595814618248246
Epoch 206
Epoch 206 :: Batch 0/1
Batch Loss = 3.465163470466433
206, epoch_train_loss=3.465163470466433
Epoch 207
Epoch 207 :: Batch 0/1
Batch Loss = 3.4589088491185698
207, epoch_train_loss=3.4589088491185698
Epoch 208
Epoch 208 :: Batch 0/1
Batch Loss = 3.462759045331886
208, epoch_train_loss=3.462759045331886
Epoch 209
Epoch 209 :: Batch 0/1
Batch Loss = 3.4594926955574254
209, epoch_train_loss=3.4594926955574254
Epoch 210
Epoch 210 :: Batch 0/1
Batch Loss = 3.4607822187939674
210, epoch_train_loss=3.4607822187939674
Epoch 211
Epoch 211 :: Batch 0/1
Batch Loss = 3.4607303375262357
211, epoch_train_loss=3.4607303375262357
Epoch 212
Epoch 212 :: Batch 0/1
Batch Loss = 3.459379630270226
212, epoch_train_loss=3.459379630270226
Epoch 213
Epoch 213 :: Batch 0/1
Batch Loss = 3.4611345409181355
213, epoch_train_loss=3.4611345409181355
Epoch 214
Epoch 214 :: Batch 0/1
Batch Loss = 3.4585099501492573
214, epoch_train_loss=3.4585099501492573
Epoch 215
Epoch 215 :: Batch 0/1
Batch Loss = 3.46061795985847
215, epoch_train_loss=3.46061795985847
Epoch 216
Epoch 216 :: Batch 0/1
Batch Loss = 3.4584586012199723
216, epoch_train_loss=3.4584586012199723
Epoch 217
Epoch 217 :: Batch 0/1
Batch Loss = 3.460094722792887
217, epoch_train_loss=3.460094722792887
Epoch 218
Epoch 218 :: Batch 0/1
Batch Loss = 3.4588277123297293
218, epoch_train_loss=3.4588277123297293
Epoch 219
Epoch 219 :: Batch 0/1
Batch Loss = 3.459255301590659
219, epoch_train_loss=3.459255301590659
Epoch 220
Epoch 220 :: Batch 0/1
Batch Loss = 3.459122476911904
220, epoch_train_loss=3.459122476911904
Epoch 221
Epoch 221 :: Batch 0/1
Batch Loss = 3.4585500710324615
221, epoch_train_loss=3.4585500710324615
Epoch 222
Epoch 222 :: Batch 0/1
Batch Loss = 3.4592342944769925
222, epoch_train_loss=3.4592342944769925
Epoch 223
Epoch 223 :: Batch 0/1
Batch Loss = 3.45826649692898
223, epoch_train_loss=3.45826649692898
Epoch 224
Epoch 224 :: Batch 0/1
Batch Loss = 3.4591386413551914
224, epoch_train_loss=3.4591386413551914
Epoch 225
Epoch 225 :: Batch 0/1
Batch Loss = 3.458229981705903
225, epoch_train_loss=3.458229981705903
Epoch 226
Epoch 226 :: Batch 0/1
Batch Loss = 3.4587469339453345
226, epoch_train_loss=3.4587469339453345
Epoch 227
Epoch 227 :: Batch 0/1
Batch Loss = 3.4583110280765905
227, epoch_train_loss=3.4583110280765905
Epoch 228
Epoch 228 :: Batch 0/1
Batch Loss = 3.458359016599062
228, epoch_train_loss=3.458359016599062
Epoch 229
Epoch 229 :: Batch 0/1
Batch Loss = 3.458415245308936
229, epoch_train_loss=3.458415245308936
Epoch 230
Epoch 230 :: Batch 0/1
Batch Loss = 3.4580717199048308
230, epoch_train_loss=3.4580717199048308
Epoch 231
Epoch 231 :: Batch 0/1
Batch Loss = 3.458390541017801
231, epoch_train_loss=3.458390541017801
Epoch 232
Epoch 232 :: Batch 0/1
Batch Loss = 3.4579143703373463
232, epoch_train_loss=3.4579143703373463
Epoch 233
Epoch 233 :: Batch 0/1
Batch Loss = 3.458202977462501
233, epoch_train_loss=3.458202977462501
Epoch 234
Epoch 234 :: Batch 0/1
Batch Loss = 3.457872303213631
234, epoch_train_loss=3.457872303213631
Epoch 235
Epoch 235 :: Batch 0/1
Batch Loss = 3.4579644706362815
235, epoch_train_loss=3.4579644706362815
Epoch 236
Epoch 236 :: Batch 0/1
Batch Loss = 3.457868334395533
236, epoch_train_loss=3.457868334395533
Epoch 237
Epoch 237 :: Batch 0/1
Batch Loss = 3.4577285963598783
237, epoch_train_loss=3.4577285963598783
Epoch 238
Epoch 238 :: Batch 0/1
Batch Loss = 3.4578107643127654
238, epoch_train_loss=3.4578107643127654
Epoch 239
Epoch 239 :: Batch 0/1
Batch Loss = 3.457550529430111
239, epoch_train_loss=3.457550529430111
Epoch 240
Epoch 240 :: Batch 0/1
Batch Loss = 3.457654909761742
240, epoch_train_loss=3.457654909761742
Epoch 241
Epoch 241 :: Batch 0/1
Batch Loss = 3.457440979619785
241, epoch_train_loss=3.457440979619785
Epoch 242
Epoch 242 :: Batch 0/1
Batch Loss = 3.4574446299345314
242, epoch_train_loss=3.4574446299345314
Epoch 243
Epoch 243 :: Batch 0/1
Batch Loss = 3.457347915973784
243, epoch_train_loss=3.457347915973784
Epoch 244
Epoch 244 :: Batch 0/1
Batch Loss = 3.457217730982399
244, epoch_train_loss=3.457217730982399
Epoch 245
Epoch 245 :: Batch 0/1
Batch Loss = 3.457210336294239
245, epoch_train_loss=3.457210336294239
Epoch 246
Epoch 246 :: Batch 0/1
Batch Loss = 3.4570159052895035
246, epoch_train_loss=3.4570159052895035
Epoch 247
Epoch 247 :: Batch 0/1
Batch Loss = 3.4569958447555518
247, epoch_train_loss=3.4569958447555518
Epoch 248
Epoch 248 :: Batch 0/1
Batch Loss = 3.456834813103662
248, epoch_train_loss=3.456834813103662
Epoch 249
Epoch 249 :: Batch 0/1
Batch Loss = 3.4567302862029092
249, epoch_train_loss=3.4567302862029092
Epoch 250
Epoch 250 :: Batch 0/1
Batch Loss = 3.4566296916280947
250, epoch_train_loss=3.4566296916280947
Epoch 251
Epoch 251 :: Batch 0/1
Batch Loss = 3.456447322003528
251, epoch_train_loss=3.456447322003528
Epoch 252
Epoch 252 :: Batch 0/1
Batch Loss = 3.4563563675194233
252, epoch_train_loss=3.4563563675194233
Epoch 253
Epoch 253 :: Batch 0/1
Batch Loss = 3.4561572410046573
253, epoch_train_loss=3.4561572410046573
Epoch 254
Epoch 254 :: Batch 0/1
Batch Loss = 3.456002194748305
254, epoch_train_loss=3.456002194748305
Epoch 255
Epoch 255 :: Batch 0/1
Batch Loss = 3.455826236848909
255, epoch_train_loss=3.455826236848909
Epoch 256
Epoch 256 :: Batch 0/1
Batch Loss = 3.4555922773356986
256, epoch_train_loss=3.4555922773356986
Epoch 257
Epoch 257 :: Batch 0/1
Batch Loss = 3.4553997452597573
257, epoch_train_loss=3.4553997452597573
Epoch 258
Epoch 258 :: Batch 0/1
Batch Loss = 3.4551304912678367
258, epoch_train_loss=3.4551304912678367
Epoch 259
Epoch 259 :: Batch 0/1
Batch Loss = 3.4548584419630988
259, epoch_train_loss=3.4548584419630988
Epoch 260
Epoch 260 :: Batch 0/1
Batch Loss = 3.4545670154883257
260, epoch_train_loss=3.4545670154883257
Epoch 261
Epoch 261 :: Batch 0/1
Batch Loss = 3.45420320446073
261, epoch_train_loss=3.45420320446073
Epoch 262
Epoch 262 :: Batch 0/1
Batch Loss = 3.453834960762408
262, epoch_train_loss=3.453834960762408
Epoch 263
Epoch 263 :: Batch 0/1
Batch Loss = 3.4534007297251605
263, epoch_train_loss=3.4534007297251605
Epoch 264
Epoch 264 :: Batch 0/1
Batch Loss = 3.45289522701535
264, epoch_train_loss=3.45289522701535
Epoch 265
Epoch 265 :: Batch 0/1
Batch Loss = 3.452349499952116
265, epoch_train_loss=3.452349499952116
Epoch 266
Epoch 266 :: Batch 0/1
Batch Loss = 3.4517007174233965
266, epoch_train_loss=3.4517007174233965
Epoch 267
Epoch 267 :: Batch 0/1
Batch Loss = 3.4509469911403623
267, epoch_train_loss=3.4509469911403623
Epoch 268
Epoch 268 :: Batch 0/1
Batch Loss = 3.4500944493695767
268, epoch_train_loss=3.4500944493695767
Epoch 269
Epoch 269 :: Batch 0/1
Batch Loss = 3.44908192268674
269, epoch_train_loss=3.44908192268674
Epoch 270
Epoch 270 :: Batch 0/1
Batch Loss = 3.447878626928705
270, epoch_train_loss=3.447878626928705
Epoch 271
Epoch 271 :: Batch 0/1
Batch Loss = 3.4464644315057336
271, epoch_train_loss=3.4464644315057336
Epoch 272
Epoch 272 :: Batch 0/1
Batch Loss = 3.444775142595779
272, epoch_train_loss=3.444775142595779
Epoch 273
Epoch 273 :: Batch 0/1
Batch Loss = 3.442728233769181
273, epoch_train_loss=3.442728233769181
Epoch 274
Epoch 274 :: Batch 0/1
Batch Loss = 3.440222301225743
274, epoch_train_loss=3.440222301225743
Epoch 275
Epoch 275 :: Batch 0/1
Batch Loss = 3.4371470211549107
275, epoch_train_loss=3.4371470211549107
Epoch 276
Epoch 276 :: Batch 0/1
Batch Loss = 3.4333309292665772
276, epoch_train_loss=3.4333309292665772
Epoch 277
Epoch 277 :: Batch 0/1
Batch Loss = 3.428573324913708
277, epoch_train_loss=3.428573324913708
Epoch 278
Epoch 278 :: Batch 0/1
Batch Loss = 3.422621409422181
278, epoch_train_loss=3.422621409422181
Epoch 279
Epoch 279 :: Batch 0/1
Batch Loss = 3.41575327586203
279, epoch_train_loss=3.41575327586203
Epoch 280
Epoch 280 :: Batch 0/1
Batch Loss = 3.4166722147670785
280, epoch_train_loss=3.4166722147670785
Epoch 281
Epoch 281 :: Batch 0/1
Batch Loss = 3.5556427794925516
281, epoch_train_loss=3.5556427794925516
Epoch 282
Epoch 282 :: Batch 0/1
Batch Loss = 3.6656401405365266
282, epoch_train_loss=3.6656401405365266
Epoch 283
Epoch 283 :: Batch 0/1
Batch Loss = 3.6199032887908302
283, epoch_train_loss=3.6199032887908302
Epoch 284
Epoch 284 :: Batch 0/1
Batch Loss = 3.702628175869159
284, epoch_train_loss=3.702628175869159
Epoch 285
Epoch 285 :: Batch 0/1
Batch Loss = 3.5515863508125074
285, epoch_train_loss=3.5515863508125074
Epoch 286
Epoch 286 :: Batch 0/1
Batch Loss = 3.543764819114883
286, epoch_train_loss=3.543764819114883
Epoch 287
Epoch 287 :: Batch 0/1
Batch Loss = 3.568089190762147
287, epoch_train_loss=3.568089190762147
Epoch 288
Epoch 288 :: Batch 0/1
Batch Loss = 3.520447707041251
288, epoch_train_loss=3.520447707041251
Epoch 289
Epoch 289 :: Batch 0/1
Batch Loss = 3.5846313217171657
289, epoch_train_loss=3.5846313217171657
Epoch 290
Epoch 290 :: Batch 0/1
Batch Loss = 3.4874343903122926
290, epoch_train_loss=3.4874343903122926
Epoch 291
Epoch 291 :: Batch 0/1
Batch Loss = 3.592418740795372
291, epoch_train_loss=3.592418740795372
Epoch 292
Epoch 292 :: Batch 0/1
Batch Loss = 3.5428851038581324
292, epoch_train_loss=3.5428851038581324
Epoch 293
Epoch 293 :: Batch 0/1
Batch Loss = 3.5468919640444225
293, epoch_train_loss=3.5468919640444225
Epoch 294
Epoch 294 :: Batch 0/1
Batch Loss = 3.510869418177905
294, epoch_train_loss=3.510869418177905
Epoch 295
Epoch 295 :: Batch 0/1
Batch Loss = 3.4892850018784913
295, epoch_train_loss=3.4892850018784913
Epoch 296
Epoch 296 :: Batch 0/1
Batch Loss = 3.5187599758583277
296, epoch_train_loss=3.5187599758583277
Epoch 297
Epoch 297 :: Batch 0/1
Batch Loss = 3.4712389425608974
297, epoch_train_loss=3.4712389425608974
Epoch 298
Epoch 298 :: Batch 0/1
Batch Loss = 3.503350726470524
298, epoch_train_loss=3.503350726470524
Epoch 299
Epoch 299 :: Batch 0/1
Batch Loss = 3.4871462721049546
299, epoch_train_loss=3.4871462721049546
Epoch 300
Epoch 300 :: Batch 0/1
Batch Loss = 3.4713383598938132
300, epoch_train_loss=3.4713383598938132
Epoch 301
Epoch 301 :: Batch 0/1
Batch Loss = 3.4911845165415634
301, epoch_train_loss=3.4911845165415634
Epoch 302
Epoch 302 :: Batch 0/1
Batch Loss = 3.4739619237047648
302, epoch_train_loss=3.4739619237047648
Epoch 303
Epoch 303 :: Batch 0/1
Batch Loss = 3.4712638116998438
303, epoch_train_loss=3.4712638116998438
Epoch 304
Epoch 304 :: Batch 0/1
Batch Loss = 3.475987054672457
304, epoch_train_loss=3.475987054672457
Epoch 305
Epoch 305 :: Batch 0/1
Batch Loss = 3.4697386781162156
305, epoch_train_loss=3.4697386781162156
Epoch 306
Epoch 306 :: Batch 0/1
Batch Loss = 3.4716211464740496
306, epoch_train_loss=3.4716211464740496
Epoch 307
Epoch 307 :: Batch 0/1
Batch Loss = 3.464737812231614
307, epoch_train_loss=3.464737812231614
Epoch 308
Epoch 308 :: Batch 0/1
Batch Loss = 3.4640899349196563
308, epoch_train_loss=3.4640899349196563
Epoch 309
Epoch 309 :: Batch 0/1
Batch Loss = 3.4711496238137385
309, epoch_train_loss=3.4711496238137385
Epoch 310
Epoch 310 :: Batch 0/1
Batch Loss = 3.4637961313634733
310, epoch_train_loss=3.4637961313634733
Epoch 311
Epoch 311 :: Batch 0/1
Batch Loss = 3.464426378923939
311, epoch_train_loss=3.464426378923939
Epoch 312
Epoch 312 :: Batch 0/1
Batch Loss = 3.4627489820337067
312, epoch_train_loss=3.4627489820337067
Epoch 313
Epoch 313 :: Batch 0/1
Batch Loss = 3.4631811566976674
313, epoch_train_loss=3.4631811566976674
Epoch 314
Epoch 314 :: Batch 0/1
Batch Loss = 3.4649404003958426
314, epoch_train_loss=3.4649404003958426
Epoch 315
Epoch 315 :: Batch 0/1
Batch Loss = 3.4608383405610303
315, epoch_train_loss=3.4608383405610303
Epoch 316
Epoch 316 :: Batch 0/1
Batch Loss = 3.461188724756344
316, epoch_train_loss=3.461188724756344
Epoch 317
Epoch 317 :: Batch 0/1
Batch Loss = 3.461496035158385
317, epoch_train_loss=3.461496035158385
Epoch 318
Epoch 318 :: Batch 0/1
Batch Loss = 3.460444828872667
318, epoch_train_loss=3.460444828872667
Epoch 319
Epoch 319 :: Batch 0/1
Batch Loss = 3.4618906444282946
319, epoch_train_loss=3.4618906444282946
Epoch 320
Epoch 320 :: Batch 0/1
Batch Loss = 3.459725049470188
320, epoch_train_loss=3.459725049470188
Epoch 321
Epoch 321 :: Batch 0/1
Batch Loss = 3.459725452881173
321, epoch_train_loss=3.459725452881173
Epoch 322
Epoch 322 :: Batch 0/1
Batch Loss = 3.4599820214720136
322, epoch_train_loss=3.4599820214720136
Epoch 323
Epoch 323 :: Batch 0/1
Batch Loss = 3.4596551126591173
323, epoch_train_loss=3.4596551126591173
Epoch 324
Epoch 324 :: Batch 0/1
Batch Loss = 3.460305763838902
324, epoch_train_loss=3.460305763838902
Epoch 325
Epoch 325 :: Batch 0/1
Batch Loss = 3.459025517526149
325, epoch_train_loss=3.459025517526149
Epoch 326
Epoch 326 :: Batch 0/1
Batch Loss = 3.458869589855678
326, epoch_train_loss=3.458869589855678
Epoch 327
Epoch 327 :: Batch 0/1
Batch Loss = 3.4589573779845106
327, epoch_train_loss=3.4589573779845106
Epoch 328
Epoch 328 :: Batch 0/1
Batch Loss = 3.458659324179138
328, epoch_train_loss=3.458659324179138
Epoch 329
Epoch 329 :: Batch 0/1
Batch Loss = 3.4589786931440174
329, epoch_train_loss=3.4589786931440174
Epoch 330
Epoch 330 :: Batch 0/1
Batch Loss = 3.458081273811359
330, epoch_train_loss=3.458081273811359
Epoch 331
Epoch 331 :: Batch 0/1
Batch Loss = 3.458034953487186
331, epoch_train_loss=3.458034953487186
Epoch 332
Epoch 332 :: Batch 0/1
Batch Loss = 3.4580386064817947
332, epoch_train_loss=3.4580386064817947
Epoch 333
Epoch 333 :: Batch 0/1
Batch Loss = 3.457887307271145
333, epoch_train_loss=3.457887307271145
Epoch 334
Epoch 334 :: Batch 0/1
Batch Loss = 3.4579550615344883
334, epoch_train_loss=3.4579550615344883
Epoch 335
Epoch 335 :: Batch 0/1
Batch Loss = 3.4573084217121224
335, epoch_train_loss=3.4573084217121224
Epoch 336
Epoch 336 :: Batch 0/1
Batch Loss = 3.4572817006999648
336, epoch_train_loss=3.4572817006999648
Epoch 337
Epoch 337 :: Batch 0/1
Batch Loss = 3.4571369330977433
337, epoch_train_loss=3.4571369330977433
Epoch 338
Epoch 338 :: Batch 0/1
Batch Loss = 3.4569969947060577
338, epoch_train_loss=3.4569969947060577
Epoch 339
Epoch 339 :: Batch 0/1
Batch Loss = 3.45685427849759
339, epoch_train_loss=3.45685427849759
Epoch 340
Epoch 340 :: Batch 0/1
Batch Loss = 3.4564020493416505
340, epoch_train_loss=3.4564020493416505
Epoch 341
Epoch 341 :: Batch 0/1
Batch Loss = 3.456385321863815
341, epoch_train_loss=3.456385321863815
Epoch 342
Epoch 342 :: Batch 0/1
Batch Loss = 3.456175755543016
342, epoch_train_loss=3.456175755543016
Epoch 343
Epoch 343 :: Batch 0/1
Batch Loss = 3.4560349321774466
343, epoch_train_loss=3.4560349321774466
Epoch 344
Epoch 344 :: Batch 0/1
Batch Loss = 3.4558174284731633
344, epoch_train_loss=3.4558174284731633
Epoch 345
Epoch 345 :: Batch 0/1
Batch Loss = 3.4555058164685
345, epoch_train_loss=3.4555058164685
Epoch 346
Epoch 346 :: Batch 0/1
Batch Loss = 3.455415699615327
346, epoch_train_loss=3.455415699615327
Epoch 347
Epoch 347 :: Batch 0/1
Batch Loss = 3.455155932577544
347, epoch_train_loss=3.455155932577544
Epoch 348
Epoch 348 :: Batch 0/1
Batch Loss = 3.4549808426674224
348, epoch_train_loss=3.4549808426674224
Epoch 349
Epoch 349 :: Batch 0/1
Batch Loss = 3.454709961879707
349, epoch_train_loss=3.454709961879707
Epoch 350
Epoch 350 :: Batch 0/1
Batch Loss = 3.4544587488428813
350, epoch_train_loss=3.4544587488428813
Epoch 351
Epoch 351 :: Batch 0/1
Batch Loss = 3.454287556424604
351, epoch_train_loss=3.454287556424604
Epoch 352
Epoch 352 :: Batch 0/1
Batch Loss = 3.454006123962227
352, epoch_train_loss=3.454006123962227
Epoch 353
Epoch 353 :: Batch 0/1
Batch Loss = 3.4537916338006327
353, epoch_train_loss=3.4537916338006327
Epoch 354
Epoch 354 :: Batch 0/1
Batch Loss = 3.4534987930619487
354, epoch_train_loss=3.4534987930619487
Epoch 355
Epoch 355 :: Batch 0/1
Batch Loss = 3.4532560762040894
355, epoch_train_loss=3.4532560762040894
Epoch 356
Epoch 356 :: Batch 0/1
Batch Loss = 3.4529872975962803
356, epoch_train_loss=3.4529872975962803
Epoch 357
Epoch 357 :: Batch 0/1
Batch Loss = 3.452676829347256
357, epoch_train_loss=3.452676829347256
Epoch 358
Epoch 358 :: Batch 0/1
Batch Loss = 3.4524001747469963
358, epoch_train_loss=3.4524001747469963
Epoch 359
Epoch 359 :: Batch 0/1
Batch Loss = 3.4520762088095927
359, epoch_train_loss=3.4520762088095927
Epoch 360
Epoch 360 :: Batch 0/1
Batch Loss = 3.4517849654246566
360, epoch_train_loss=3.4517849654246566
Epoch 361
Epoch 361 :: Batch 0/1
Batch Loss = 3.4514362305136994
361, epoch_train_loss=3.4514362305136994
Epoch 362
Epoch 362 :: Batch 0/1
Batch Loss = 3.4510938050675506
362, epoch_train_loss=3.4510938050675506
Epoch 363
Epoch 363 :: Batch 0/1
Batch Loss = 3.450747340099263
363, epoch_train_loss=3.450747340099263
Epoch 364
Epoch 364 :: Batch 0/1
Batch Loss = 3.450376190484644
364, epoch_train_loss=3.450376190484644
Epoch 365
Epoch 365 :: Batch 0/1
Batch Loss = 3.449998726104063
365, epoch_train_loss=3.449998726104063
Epoch 366
Epoch 366 :: Batch 0/1
Batch Loss = 3.4495777377776995
366, epoch_train_loss=3.4495777377776995
Epoch 367
Epoch 367 :: Batch 0/1
Batch Loss = 3.4491715140226047
367, epoch_train_loss=3.4491715140226047
Epoch 368
Epoch 368 :: Batch 0/1
Batch Loss = 3.4487359070599526
368, epoch_train_loss=3.4487359070599526
Epoch 369
Epoch 369 :: Batch 0/1
Batch Loss = 3.448285747199912
369, epoch_train_loss=3.448285747199912
Epoch 370
Epoch 370 :: Batch 0/1
Batch Loss = 3.4478072683215295
370, epoch_train_loss=3.4478072683215295
Epoch 371
Epoch 371 :: Batch 0/1
Batch Loss = 3.4473092249715696
371, epoch_train_loss=3.4473092249715696
Epoch 372
Epoch 372 :: Batch 0/1
Batch Loss = 3.4468004720993863
372, epoch_train_loss=3.4468004720993863
Epoch 373
Epoch 373 :: Batch 0/1
Batch Loss = 3.446253708652938
373, epoch_train_loss=3.446253708652938
Epoch 374
Epoch 374 :: Batch 0/1
Batch Loss = 3.4456888035342366
374, epoch_train_loss=3.4456888035342366
Epoch 375
Epoch 375 :: Batch 0/1
Batch Loss = 3.4450915186777022
375, epoch_train_loss=3.4450915186777022
Epoch 376
Epoch 376 :: Batch 0/1
Batch Loss = 3.4444759675750665
376, epoch_train_loss=3.4444759675750665
Epoch 377
Epoch 377 :: Batch 0/1
Batch Loss = 3.4438231524295753
377, epoch_train_loss=3.4438231524295753
Epoch 378
Epoch 378 :: Batch 0/1
Batch Loss = 3.443140176768581
378, epoch_train_loss=3.443140176768581
Epoch 379
Epoch 379 :: Batch 0/1
Batch Loss = 3.4424270709153126
379, epoch_train_loss=3.4424270709153126
Epoch 380
Epoch 380 :: Batch 0/1
Batch Loss = 3.4416758354596158
380, epoch_train_loss=3.4416758354596158
Epoch 381
Epoch 381 :: Batch 0/1
Batch Loss = 3.4408884724372677
381, epoch_train_loss=3.4408884724372677
Epoch 382
Epoch 382 :: Batch 0/1
Batch Loss = 3.4400585828121524
382, epoch_train_loss=3.4400585828121524
Epoch 383
Epoch 383 :: Batch 0/1
Batch Loss = 3.4391939980402118
383, epoch_train_loss=3.4391939980402118
Epoch 384
Epoch 384 :: Batch 0/1
Batch Loss = 3.4382798681732605
384, epoch_train_loss=3.4382798681732605
Epoch 385
Epoch 385 :: Batch 0/1
Batch Loss = 3.4373204031800513
385, epoch_train_loss=3.4373204031800513
Epoch 386
Epoch 386 :: Batch 0/1
Batch Loss = 3.4363111215888438
386, epoch_train_loss=3.4363111215888438
Epoch 387
Epoch 387 :: Batch 0/1
Batch Loss = 3.435251981157757
387, epoch_train_loss=3.435251981157757
Epoch 388
Epoch 388 :: Batch 0/1
Batch Loss = 3.4341352394156486
388, epoch_train_loss=3.4341352394156486
Epoch 389
Epoch 389 :: Batch 0/1
Batch Loss = 3.4329611818522894
389, epoch_train_loss=3.4329611818522894
Epoch 390
Epoch 390 :: Batch 0/1
Batch Loss = 3.431727180857247
390, epoch_train_loss=3.431727180857247
Epoch 391
Epoch 391 :: Batch 0/1
Batch Loss = 3.430427447790271
391, epoch_train_loss=3.430427447790271
Epoch 392
Epoch 392 :: Batch 0/1
Batch Loss = 3.4290617511776826
392, epoch_train_loss=3.4290617511776826
Epoch 393
Epoch 393 :: Batch 0/1
Batch Loss = 3.4276250083373205
393, epoch_train_loss=3.4276250083373205
Epoch 394
Epoch 394 :: Batch 0/1
Batch Loss = 3.426114304688921
394, epoch_train_loss=3.426114304688921
Epoch 395
Epoch 395 :: Batch 0/1
Batch Loss = 3.4245245178831496
395, epoch_train_loss=3.4245245178831496
Epoch 396
Epoch 396 :: Batch 0/1
Batch Loss = 3.4228555012467914
396, epoch_train_loss=3.4228555012467914
Epoch 397
Epoch 397 :: Batch 0/1
Batch Loss = 3.4210991671619517
397, epoch_train_loss=3.4210991671619517
Epoch 398
Epoch 398 :: Batch 0/1
Batch Loss = 3.419254488360622
398, epoch_train_loss=3.419254488360622
Epoch 399
Epoch 399 :: Batch 0/1
Batch Loss = 3.4173162777262984
399, epoch_train_loss=3.4173162777262984
Epoch 400
Epoch 400 :: Batch 0/1
Batch Loss = 3.415280682133457
400, epoch_train_loss=3.415280682133457
Epoch 401
Epoch 401 :: Batch 0/1
Batch Loss = 3.4131426750741074
401, epoch_train_loss=3.4131426750741074
Epoch 402
Epoch 402 :: Batch 0/1
Batch Loss = 3.4108966675770787
402, epoch_train_loss=3.4108966675770787
Epoch 403
Epoch 403 :: Batch 0/1
Batch Loss = 3.408533068370355
403, epoch_train_loss=3.408533068370355
Epoch 404
Epoch 404 :: Batch 0/1
Batch Loss = 3.4060441177020597
404, epoch_train_loss=3.4060441177020597
Epoch 405
Epoch 405 :: Batch 0/1
Batch Loss = 3.4034629718502147
405, epoch_train_loss=3.4034629718502147
Epoch 406
Epoch 406 :: Batch 0/1
Batch Loss = 3.4008464229932707
406, epoch_train_loss=3.4008464229932707
Epoch 407
Epoch 407 :: Batch 0/1
Batch Loss = 3.3980699278330553
407, epoch_train_loss=3.3980699278330553
Epoch 408
Epoch 408 :: Batch 0/1
Batch Loss = 3.3950956363284903
408, epoch_train_loss=3.3950956363284903
Epoch 409
Epoch 409 :: Batch 0/1
Batch Loss = 3.3921177322219487
409, epoch_train_loss=3.3921177322219487
Epoch 410
Epoch 410 :: Batch 0/1
Batch Loss = 3.388909632836247
410, epoch_train_loss=3.388909632836247
Epoch 411
Epoch 411 :: Batch 0/1
Batch Loss = 3.385665681062479
411, epoch_train_loss=3.385665681062479
Epoch 412
Epoch 412 :: Batch 0/1
Batch Loss = 3.382212078481077
412, epoch_train_loss=3.382212078481077
Epoch 413
Epoch 413 :: Batch 0/1
Batch Loss = 3.3786655602322915
413, epoch_train_loss=3.3786655602322915
Epoch 414
Epoch 414 :: Batch 0/1
Batch Loss = 3.374997927988175
414, epoch_train_loss=3.374997927988175
Epoch 415
Epoch 415 :: Batch 0/1
Batch Loss = 3.3712018934842445
415, epoch_train_loss=3.3712018934842445
Epoch 416
Epoch 416 :: Batch 0/1
Batch Loss = 3.3677226830575946
416, epoch_train_loss=3.3677226830575946
Epoch 417
Epoch 417 :: Batch 0/1
Batch Loss = 3.3636196383624735
417, epoch_train_loss=3.3636196383624735
Epoch 418
Epoch 418 :: Batch 0/1
Batch Loss = 3.359720412440052
418, epoch_train_loss=3.359720412440052
Epoch 419
Epoch 419 :: Batch 0/1
Batch Loss = 3.3550562689550008
419, epoch_train_loss=3.3550562689550008
Epoch 420
Epoch 420 :: Batch 0/1
Batch Loss = 3.3508716602074395
420, epoch_train_loss=3.3508716602074395
Epoch 421
Epoch 421 :: Batch 0/1
Batch Loss = 3.3469737375629594
421, epoch_train_loss=3.3469737375629594
Epoch 422
Epoch 422 :: Batch 0/1
Batch Loss = 3.348653400513251
422, epoch_train_loss=3.348653400513251
Epoch 423
Epoch 423 :: Batch 0/1
Batch Loss = 3.361744227606192
423, epoch_train_loss=3.361744227606192
Epoch 424
Epoch 424 :: Batch 0/1
Batch Loss = 3.3689380704151874
424, epoch_train_loss=3.3689380704151874
Epoch 425
Epoch 425 :: Batch 0/1
Batch Loss = 3.363661467417886
425, epoch_train_loss=3.363661467417886
Epoch 426
Epoch 426 :: Batch 0/1
Batch Loss = 3.3981984176557645
426, epoch_train_loss=3.3981984176557645
Epoch 427
Epoch 427 :: Batch 0/1
Batch Loss = 3.347704831607871
427, epoch_train_loss=3.347704831607871
Epoch 428
Epoch 428 :: Batch 0/1
Batch Loss = 3.418601649116677
428, epoch_train_loss=3.418601649116677
Epoch 429
Epoch 429 :: Batch 0/1
Batch Loss = 3.4726425869927016
429, epoch_train_loss=3.4726425869927016
Epoch 430
Epoch 430 :: Batch 0/1
Batch Loss = 3.396517144434203
430, epoch_train_loss=3.396517144434203
Epoch 431
Epoch 431 :: Batch 0/1
Batch Loss = 3.484557272459629
431, epoch_train_loss=3.484557272459629
Epoch 432
Epoch 432 :: Batch 0/1
Batch Loss = 3.3306523426226393
432, epoch_train_loss=3.3306523426226393
Epoch 433
Epoch 433 :: Batch 0/1
Batch Loss = 3.3804875616594012
433, epoch_train_loss=3.3804875616594012
Epoch 434
Epoch 434 :: Batch 0/1
Batch Loss = 3.305652393248711
434, epoch_train_loss=3.305652393248711
Epoch 435
Epoch 435 :: Batch 0/1
Batch Loss = 3.3459641544955034
435, epoch_train_loss=3.3459641544955034
Epoch 436
Epoch 436 :: Batch 0/1
Batch Loss = 3.330218845315353
436, epoch_train_loss=3.330218845315353
Epoch 437
Epoch 437 :: Batch 0/1
Batch Loss = 3.303872278665384
437, epoch_train_loss=3.303872278665384
Epoch 438
Epoch 438 :: Batch 0/1
Batch Loss = 3.3251575457369476
438, epoch_train_loss=3.3251575457369476
Epoch 439
Epoch 439 :: Batch 0/1
Batch Loss = 3.26419903903666
439, epoch_train_loss=3.26419903903666
Epoch 440
Epoch 440 :: Batch 0/1
Batch Loss = 3.3048996036247056
440, epoch_train_loss=3.3048996036247056
Epoch 441
Epoch 441 :: Batch 0/1
Batch Loss = 3.2480714842743246
441, epoch_train_loss=3.2480714842743246
Epoch 442
Epoch 442 :: Batch 0/1
Batch Loss = 3.2564199125670594
442, epoch_train_loss=3.2564199125670594
Epoch 443
Epoch 443 :: Batch 0/1
Batch Loss = 3.2523890608258386
443, epoch_train_loss=3.2523890608258386
Epoch 444
Epoch 444 :: Batch 0/1
Batch Loss = 3.212996494463951
444, epoch_train_loss=3.212996494463951
Epoch 445
Epoch 445 :: Batch 0/1
Batch Loss = 3.2258086241641415
445, epoch_train_loss=3.2258086241641415
Epoch 446
Epoch 446 :: Batch 0/1
Batch Loss = 3.196810452843141
446, epoch_train_loss=3.196810452843141
Epoch 447
Epoch 447 :: Batch 0/1
Batch Loss = 3.1842547352958688
447, epoch_train_loss=3.1842547352958688
Epoch 448
Epoch 448 :: Batch 0/1
Batch Loss = 3.1946101158115505
448, epoch_train_loss=3.1946101158115505
Epoch 449
Epoch 449 :: Batch 0/1
Batch Loss = 3.1704373943313895
449, epoch_train_loss=3.1704373943313895
Epoch 450
Epoch 450 :: Batch 0/1
Batch Loss = 3.16325523727404
450, epoch_train_loss=3.16325523727404
Epoch 451
Epoch 451 :: Batch 0/1
Batch Loss = 3.1663254366935476
451, epoch_train_loss=3.1663254366935476
Epoch 452
Epoch 452 :: Batch 0/1
Batch Loss = 3.169293611339685
452, epoch_train_loss=3.169293611339685
Epoch 453
Epoch 453 :: Batch 0/1
Batch Loss = 3.1857750141677315
453, epoch_train_loss=3.1857750141677315
Epoch 454
Epoch 454 :: Batch 0/1
Batch Loss = 3.1937095627267422
454, epoch_train_loss=3.1937095627267422
Epoch 455
Epoch 455 :: Batch 0/1
Batch Loss = 3.236709477578313
455, epoch_train_loss=3.236709477578313
Epoch 456
Epoch 456 :: Batch 0/1
Batch Loss = 3.2362895914201286
456, epoch_train_loss=3.2362895914201286
Epoch 457
Epoch 457 :: Batch 0/1
Batch Loss = 3.2109096469443803
457, epoch_train_loss=3.2109096469443803
Epoch 458
Epoch 458 :: Batch 0/1
Batch Loss = 3.148729697825042
458, epoch_train_loss=3.148729697825042
Epoch 459
Epoch 459 :: Batch 0/1
Batch Loss = 3.2159935111938123
459, epoch_train_loss=3.2159935111938123
Epoch 460
Epoch 460 :: Batch 0/1
Batch Loss = 3.2299628063759083
460, epoch_train_loss=3.2299628063759083
Epoch 461
Epoch 461 :: Batch 0/1
Batch Loss = 3.164252309217977
461, epoch_train_loss=3.164252309217977
Epoch 462
Epoch 462 :: Batch 0/1
Batch Loss = 3.2726022840052194
462, epoch_train_loss=3.2726022840052194
Epoch 463
Epoch 463 :: Batch 0/1
Batch Loss = 3.1687988547076316
463, epoch_train_loss=3.1687988547076316
Epoch 464
Epoch 464 :: Batch 0/1
Batch Loss = 3.1938190670531807
464, epoch_train_loss=3.1938190670531807
Epoch 465
Epoch 465 :: Batch 0/1
Batch Loss = 3.191657978678836
465, epoch_train_loss=3.191657978678836
Epoch 466
Epoch 466 :: Batch 0/1
Batch Loss = 3.1502359330452028
466, epoch_train_loss=3.1502359330452028
Epoch 467
Epoch 467 :: Batch 0/1
Batch Loss = 3.191165286073919
467, epoch_train_loss=3.191165286073919
Epoch 468
Epoch 468 :: Batch 0/1
Batch Loss = 3.1467859950704193
468, epoch_train_loss=3.1467859950704193
Epoch 469
Epoch 469 :: Batch 0/1
Batch Loss = 3.191618195968966
469, epoch_train_loss=3.191618195968966
Epoch 470
Epoch 470 :: Batch 0/1
Batch Loss = 3.147830089659755
470, epoch_train_loss=3.147830089659755
Epoch 471
Epoch 471 :: Batch 0/1
Batch Loss = 3.1732238015007748
471, epoch_train_loss=3.1732238015007748
Epoch 472
Epoch 472 :: Batch 0/1
Batch Loss = 3.1458494196603746
472, epoch_train_loss=3.1458494196603746
Epoch 473
Epoch 473 :: Batch 0/1
Batch Loss = 3.163838488082161
473, epoch_train_loss=3.163838488082161
Epoch 474
Epoch 474 :: Batch 0/1
Batch Loss = 3.1512762098427034
474, epoch_train_loss=3.1512762098427034
Epoch 475
Epoch 475 :: Batch 0/1
Batch Loss = 3.1532092289244296
475, epoch_train_loss=3.1532092289244296
Epoch 476
Epoch 476 :: Batch 0/1
Batch Loss = 3.1518207421444067
476, epoch_train_loss=3.1518207421444067
Epoch 477
Epoch 477 :: Batch 0/1
Batch Loss = 3.1414642895372755
477, epoch_train_loss=3.1414642895372755
Epoch 478
Epoch 478 :: Batch 0/1
Batch Loss = 3.1527363797489856
478, epoch_train_loss=3.1527363797489856
Epoch 479
Epoch 479 :: Batch 0/1
Batch Loss = 3.1356497362381495
479, epoch_train_loss=3.1356497362381495
Epoch 480
Epoch 480 :: Batch 0/1
Batch Loss = 3.1500409702390506
480, epoch_train_loss=3.1500409702390506
Epoch 481
Epoch 481 :: Batch 0/1
Batch Loss = 3.1356018847446894
481, epoch_train_loss=3.1356018847446894
Epoch 482
Epoch 482 :: Batch 0/1
Batch Loss = 3.139878126349864
482, epoch_train_loss=3.139878126349864
Epoch 483
Epoch 483 :: Batch 0/1
Batch Loss = 3.1389170057440516
483, epoch_train_loss=3.1389170057440516
Epoch 484
Epoch 484 :: Batch 0/1
Batch Loss = 3.1308301835374084
484, epoch_train_loss=3.1308301835374084
Epoch 485
Epoch 485 :: Batch 0/1
Batch Loss = 3.1382255951696774
485, epoch_train_loss=3.1382255951696774
Epoch 486
Epoch 486 :: Batch 0/1
Batch Loss = 3.128788535336591
486, epoch_train_loss=3.128788535336591
Epoch 487
Epoch 487 :: Batch 0/1
Batch Loss = 3.1309280093743808
487, epoch_train_loss=3.1309280093743808
Epoch 488
Epoch 488 :: Batch 0/1
Batch Loss = 3.1317549910527873
488, epoch_train_loss=3.1317549910527873
Epoch 489
Epoch 489 :: Batch 0/1
Batch Loss = 3.1244441930084585
489, epoch_train_loss=3.1244441930084585
Epoch 490
Epoch 490 :: Batch 0/1
Batch Loss = 3.1288909789073864
490, epoch_train_loss=3.1288909789073864
Epoch 491
Epoch 491 :: Batch 0/1
Batch Loss = 3.1258646532492445
491, epoch_train_loss=3.1258646532492445
Epoch 492
Epoch 492 :: Batch 0/1
Batch Loss = 3.121988104618956
492, epoch_train_loss=3.121988104618956
Epoch 493
Epoch 493 :: Batch 0/1
Batch Loss = 3.1254480920855503
493, epoch_train_loss=3.1254480920855503
Epoch 494
Epoch 494 :: Batch 0/1
Batch Loss = 3.1218195674454976
494, epoch_train_loss=3.1218195674454976
Epoch 495
Epoch 495 :: Batch 0/1
Batch Loss = 3.1200553480110536
495, epoch_train_loss=3.1200553480110536
Epoch 496
Epoch 496 :: Batch 0/1
Batch Loss = 3.122221958093827
496, epoch_train_loss=3.122221958093827
Epoch 497
Epoch 497 :: Batch 0/1
Batch Loss = 3.1191184146511985
497, epoch_train_loss=3.1191184146511985
Epoch 498
Epoch 498 :: Batch 0/1
Batch Loss = 3.1181580703474148
498, epoch_train_loss=3.1181580703474148
Epoch 499
Epoch 499 :: Batch 0/1
Batch Loss = 3.1196436241496706
499, epoch_train_loss=3.1196436241496706
Epoch 500
Epoch 500 :: Batch 0/1
Batch Loss = 3.117263364430537
500, epoch_train_loss=3.117263364430537
Epoch 501
Epoch 501 :: Batch 0/1
Batch Loss = 3.1164735107516517
501, epoch_train_loss=3.1164735107516517
Epoch 502
Epoch 502 :: Batch 0/1
Batch Loss = 3.117639921693506
502, epoch_train_loss=3.117639921693506
Epoch 503
Epoch 503 :: Batch 0/1
Batch Loss = 3.1160334925118724
503, epoch_train_loss=3.1160334925118724
Epoch 504
Epoch 504 :: Batch 0/1
Batch Loss = 3.115162409626759
504, epoch_train_loss=3.115162409626759
Epoch 505
Epoch 505 :: Batch 0/1
Batch Loss = 3.1159633796175195
505, epoch_train_loss=3.1159633796175195
Epoch 506
Epoch 506 :: Batch 0/1
Batch Loss = 3.115164825862669
506, epoch_train_loss=3.115164825862669
Epoch 507
Epoch 507 :: Batch 0/1
Batch Loss = 3.114218598079701
507, epoch_train_loss=3.114218598079701
Epoch 508
Epoch 508 :: Batch 0/1
Batch Loss = 3.1147562059688743
508, epoch_train_loss=3.1147562059688743
Epoch 509
Epoch 509 :: Batch 0/1
Batch Loss = 3.1144616452564153
509, epoch_train_loss=3.1144616452564153
Epoch 510
Epoch 510 :: Batch 0/1
Batch Loss = 3.1136035248688403
510, epoch_train_loss=3.1136035248688403
Epoch 511
Epoch 511 :: Batch 0/1
Batch Loss = 3.113807894201838
511, epoch_train_loss=3.113807894201838
Epoch 512
Epoch 512 :: Batch 0/1
Batch Loss = 3.113864750274634
512, epoch_train_loss=3.113864750274634
Epoch 513
Epoch 513 :: Batch 0/1
Batch Loss = 3.113252209394331
513, epoch_train_loss=3.113252209394331
Epoch 514
Epoch 514 :: Batch 0/1
Batch Loss = 3.1130872726887837
514, epoch_train_loss=3.1130872726887837
Epoch 515
Epoch 515 :: Batch 0/1
Batch Loss = 3.113286931615556
515, epoch_train_loss=3.113286931615556
Epoch 516
Epoch 516 :: Batch 0/1
Batch Loss = 3.1129950036472294
516, epoch_train_loss=3.1129950036472294
Epoch 517
Epoch 517 :: Batch 0/1
Batch Loss = 3.11259377637575
517, epoch_train_loss=3.11259377637575
Epoch 518
Epoch 518 :: Batch 0/1
Batch Loss = 3.112688297273908
518, epoch_train_loss=3.112688297273908
Epoch 519
Epoch 519 :: Batch 0/1
Batch Loss = 3.1126711792822808
519, epoch_train_loss=3.1126711792822808
Epoch 520
Epoch 520 :: Batch 0/1
Batch Loss = 3.1122820925589907
520, epoch_train_loss=3.1122820925589907
Epoch 521
Epoch 521 :: Batch 0/1
Batch Loss = 3.112138873231316
521, epoch_train_loss=3.112138873231316
Epoch 522
Epoch 522 :: Batch 0/1
Batch Loss = 3.112187017693199
522, epoch_train_loss=3.112187017693199
Epoch 523
Epoch 523 :: Batch 0/1
Batch Loss = 3.112001028334292
523, epoch_train_loss=3.112001028334292
Epoch 524
Epoch 524 :: Batch 0/1
Batch Loss = 3.11173113233487
524, epoch_train_loss=3.11173113233487
Epoch 525
Epoch 525 :: Batch 0/1
Batch Loss = 3.1116271389534247
525, epoch_train_loss=3.1116271389534247
Epoch 526
Epoch 526 :: Batch 0/1
Batch Loss = 3.111594659773695
526, epoch_train_loss=3.111594659773695
Epoch 527
Epoch 527 :: Batch 0/1
Batch Loss = 3.1114301803631843
527, epoch_train_loss=3.1114301803631843
Epoch 528
Epoch 528 :: Batch 0/1
Batch Loss = 3.1111845310034307
528, epoch_train_loss=3.1111845310034307
Epoch 529
Epoch 529 :: Batch 0/1
Batch Loss = 3.1110679235462593
529, epoch_train_loss=3.1110679235462593
Epoch 530
Epoch 530 :: Batch 0/1
Batch Loss = 3.111017163509144
530, epoch_train_loss=3.111017163509144
Epoch 531
Epoch 531 :: Batch 0/1
Batch Loss = 3.1108555698435487
531, epoch_train_loss=3.1108555698435487
Epoch 532
Epoch 532 :: Batch 0/1
Batch Loss = 3.110641436189192
532, epoch_train_loss=3.110641436189192
Epoch 533
Epoch 533 :: Batch 0/1
Batch Loss = 3.1105015787699952
533, epoch_train_loss=3.1105015787699952
Epoch 534
Epoch 534 :: Batch 0/1
Batch Loss = 3.1104188688146834
534, epoch_train_loss=3.1104188688146834
Epoch 535
Epoch 535 :: Batch 0/1
Batch Loss = 3.110296296259706
535, epoch_train_loss=3.110296296259706
Epoch 536
Epoch 536 :: Batch 0/1
Batch Loss = 3.1101184796051227
536, epoch_train_loss=3.1101184796051227
Epoch 537
Epoch 537 :: Batch 0/1
Batch Loss = 3.1099517039975115
537, epoch_train_loss=3.1099517039975115
Epoch 538
Epoch 538 :: Batch 0/1
Batch Loss = 3.109828719974228
538, epoch_train_loss=3.109828719974228
Epoch 539
Epoch 539 :: Batch 0/1
Batch Loss = 3.1097261713806255
539, epoch_train_loss=3.1097261713806255
Epoch 540
Epoch 540 :: Batch 0/1
Batch Loss = 3.1095986491837255
540, epoch_train_loss=3.1095986491837255
Epoch 541
Epoch 541 :: Batch 0/1
Batch Loss = 3.1094378346272857
541, epoch_train_loss=3.1094378346272857
Epoch 542
Epoch 542 :: Batch 0/1
Batch Loss = 3.1092832895094538
542, epoch_train_loss=3.1092832895094538
Epoch 543
Epoch 543 :: Batch 0/1
Batch Loss = 3.1091574477349044
543, epoch_train_loss=3.1091574477349044
Epoch 544
Epoch 544 :: Batch 0/1
Batch Loss = 3.1090467888404096
544, epoch_train_loss=3.1090467888404096
Epoch 545
Epoch 545 :: Batch 0/1
Batch Loss = 3.1089271312102715
545, epoch_train_loss=3.1089271312102715
Epoch 546
Epoch 546 :: Batch 0/1
Batch Loss = 3.108790633919357
546, epoch_train_loss=3.108790633919357
Epoch 547
Epoch 547 :: Batch 0/1
Batch Loss = 3.1086470819956915
547, epoch_train_loss=3.1086470819956915
Epoch 548
Epoch 548 :: Batch 0/1
Batch Loss = 3.108506408076427
548, epoch_train_loss=3.108506408076427
Epoch 549
Epoch 549 :: Batch 0/1
Batch Loss = 3.1083765801172705
549, epoch_train_loss=3.1083765801172705
Epoch 550
Epoch 550 :: Batch 0/1
Batch Loss = 3.108256035001029
550, epoch_train_loss=3.108256035001029
Epoch 551
Epoch 551 :: Batch 0/1
Batch Loss = 3.108138432491011
551, epoch_train_loss=3.108138432491011
Epoch 552
Epoch 552 :: Batch 0/1
Batch Loss = 3.108017395242236
552, epoch_train_loss=3.108017395242236
Epoch 553
Epoch 553 :: Batch 0/1
Batch Loss = 3.1078887144932783
553, epoch_train_loss=3.1078887144932783
Epoch 554
Epoch 554 :: Batch 0/1
Batch Loss = 3.107756278517123
554, epoch_train_loss=3.107756278517123
Epoch 555
Epoch 555 :: Batch 0/1
Batch Loss = 3.1076220892143387
555, epoch_train_loss=3.1076220892143387
Epoch 556
Epoch 556 :: Batch 0/1
Batch Loss = 3.1074884169623243
556, epoch_train_loss=3.1074884169623243
Epoch 557
Epoch 557 :: Batch 0/1
Batch Loss = 3.1073552992191704
557, epoch_train_loss=3.1073552992191704
Epoch 558
Epoch 558 :: Batch 0/1
Batch Loss = 3.1072235863870383
558, epoch_train_loss=3.1072235863870383
Epoch 559
Epoch 559 :: Batch 0/1
Batch Loss = 3.1070935012248566
559, epoch_train_loss=3.1070935012248566
Epoch 560
Epoch 560 :: Batch 0/1
Batch Loss = 3.106964295605083
560, epoch_train_loss=3.106964295605083
Epoch 561
Epoch 561 :: Batch 0/1
Batch Loss = 3.1068356246437707
561, epoch_train_loss=3.1068356246437707
Epoch 562
Epoch 562 :: Batch 0/1
Batch Loss = 3.1067069538223056
562, epoch_train_loss=3.1067069538223056
Epoch 563
Epoch 563 :: Batch 0/1
Batch Loss = 3.106578836837561
563, epoch_train_loss=3.106578836837561
Epoch 564
Epoch 564 :: Batch 0/1
Batch Loss = 3.10645241041714
564, epoch_train_loss=3.10645241041714
Epoch 565
Epoch 565 :: Batch 0/1
Batch Loss = 3.106329984707955
565, epoch_train_loss=3.106329984707955
Epoch 566
Epoch 566 :: Batch 0/1
Batch Loss = 3.1062154476594994
566, epoch_train_loss=3.1062154476594994
Epoch 567
Epoch 567 :: Batch 0/1
Batch Loss = 3.106120562400054
567, epoch_train_loss=3.106120562400054
Epoch 568
Epoch 568 :: Batch 0/1
Batch Loss = 3.1060655977962823
568, epoch_train_loss=3.1060655977962823
Epoch 569
Epoch 569 :: Batch 0/1
Batch Loss = 3.1061223395115105
569, epoch_train_loss=3.1061223395115105
Epoch 570
Epoch 570 :: Batch 0/1
Batch Loss = 3.1063968496485344
570, epoch_train_loss=3.1063968496485344
Epoch 571
Epoch 571 :: Batch 0/1
Batch Loss = 3.107379103404943
571, epoch_train_loss=3.107379103404943
Epoch 572
Epoch 572 :: Batch 0/1
Batch Loss = 3.1094477097510085
572, epoch_train_loss=3.1094477097510085
Epoch 573
Epoch 573 :: Batch 0/1
Batch Loss = 3.115985328547973
573, epoch_train_loss=3.115985328547973
Epoch 574
Epoch 574 :: Batch 0/1
Batch Loss = 3.123920904817779
574, epoch_train_loss=3.123920904817779
Epoch 575
Epoch 575 :: Batch 0/1
Batch Loss = 3.149356628281913
575, epoch_train_loss=3.149356628281913
Epoch 576
Epoch 576 :: Batch 0/1
Batch Loss = 3.141474047375501
576, epoch_train_loss=3.141474047375501
Epoch 577
Epoch 577 :: Batch 0/1
Batch Loss = 3.143719730814791
577, epoch_train_loss=3.143719730814791
Epoch 578
Epoch 578 :: Batch 0/1
Batch Loss = 3.1110626760493902
578, epoch_train_loss=3.1110626760493902
Epoch 579
Epoch 579 :: Batch 0/1
Batch Loss = 3.1166101905996966
579, epoch_train_loss=3.1166101905996966
Epoch 580
Epoch 580 :: Batch 0/1
Batch Loss = 3.1507446130376278
580, epoch_train_loss=3.1507446130376278
Epoch 581
Epoch 581 :: Batch 0/1
Batch Loss = 3.1340310623906946
581, epoch_train_loss=3.1340310623906946
Epoch 582
Epoch 582 :: Batch 0/1
Batch Loss = 3.125277669937686
582, epoch_train_loss=3.125277669937686
Epoch 583
Epoch 583 :: Batch 0/1
Batch Loss = 3.1065130838901722
583, epoch_train_loss=3.1065130838901722
Epoch 584
Epoch 584 :: Batch 0/1
Batch Loss = 3.1100607135185756
584, epoch_train_loss=3.1100607135185756
Epoch 585
Epoch 585 :: Batch 0/1
Batch Loss = 3.123946672225293
585, epoch_train_loss=3.123946672225293
Epoch 586
Epoch 586 :: Batch 0/1
Batch Loss = 3.1242784295492667
586, epoch_train_loss=3.1242784295492667
Epoch 587
Epoch 587 :: Batch 0/1
Batch Loss = 3.132997293466753
587, epoch_train_loss=3.132997293466753
Epoch 588
Epoch 588 :: Batch 0/1
Batch Loss = 3.107677301608162
588, epoch_train_loss=3.107677301608162
Epoch 589
Epoch 589 :: Batch 0/1
Batch Loss = 3.1132492783529924
589, epoch_train_loss=3.1132492783529924
Epoch 590
Epoch 590 :: Batch 0/1
Batch Loss = 3.1306160368334965
590, epoch_train_loss=3.1306160368334965
Epoch 591
Epoch 591 :: Batch 0/1
Batch Loss = 3.111773765134122
591, epoch_train_loss=3.111773765134122
Epoch 592
Epoch 592 :: Batch 0/1
Batch Loss = 3.1080397883839543
592, epoch_train_loss=3.1080397883839543
Epoch 593
Epoch 593 :: Batch 0/1
Batch Loss = 3.1121323252113693
593, epoch_train_loss=3.1121323252113693
Epoch 594
Epoch 594 :: Batch 0/1
Batch Loss = 3.122473347561251
594, epoch_train_loss=3.122473347561251
Epoch 595
Epoch 595 :: Batch 0/1
Batch Loss = 3.123325944452702
595, epoch_train_loss=3.123325944452702
Epoch 596
Epoch 596 :: Batch 0/1
Batch Loss = 3.109562921016429
596, epoch_train_loss=3.109562921016429
Epoch 597
Epoch 597 :: Batch 0/1
Batch Loss = 3.107590679723637
597, epoch_train_loss=3.107590679723637
Epoch 598
Epoch 598 :: Batch 0/1
Batch Loss = 3.10911476911707
598, epoch_train_loss=3.10911476911707
Epoch 599
Epoch 599 :: Batch 0/1
Batch Loss = 3.1135148920255937
599, epoch_train_loss=3.1135148920255937
Epoch 600
Epoch 600 :: Batch 0/1
Batch Loss = 3.1090192863422463
600, epoch_train_loss=3.1090192863422463
Epoch 601
Epoch 601 :: Batch 0/1
Batch Loss = 3.1058542904710125
601, epoch_train_loss=3.1058542904710125
Epoch 602
Epoch 602 :: Batch 0/1
Batch Loss = 3.1050536872242733
602, epoch_train_loss=3.1050536872242733
Epoch 603
Epoch 603 :: Batch 0/1
Batch Loss = 3.110568313595242
603, epoch_train_loss=3.110568313595242
Epoch 604
Epoch 604 :: Batch 0/1
Batch Loss = 3.1096505391228098
604, epoch_train_loss=3.1096505391228098
Epoch 605
Epoch 605 :: Batch 0/1
Batch Loss = 3.107019623050346
605, epoch_train_loss=3.107019623050346
Epoch 606
Epoch 606 :: Batch 0/1
Batch Loss = 3.103445944040049
606, epoch_train_loss=3.103445944040049
Epoch 607
Epoch 607 :: Batch 0/1
Batch Loss = 3.103458914112929
607, epoch_train_loss=3.103458914112929
Epoch 608
Epoch 608 :: Batch 0/1
Batch Loss = 3.1067033833930915
608, epoch_train_loss=3.1067033833930915
Epoch 609
Epoch 609 :: Batch 0/1
Batch Loss = 3.105488941788442
609, epoch_train_loss=3.105488941788442
Epoch 610
Epoch 610 :: Batch 0/1
Batch Loss = 3.1042297686009
610, epoch_train_loss=3.1042297686009
Epoch 611
Epoch 611 :: Batch 0/1
Batch Loss = 3.101829781972063
611, epoch_train_loss=3.101829781972063
Epoch 612
Epoch 612 :: Batch 0/1
Batch Loss = 3.1028065396698623
612, epoch_train_loss=3.1028065396698623
Epoch 613
Epoch 613 :: Batch 0/1
Batch Loss = 3.1035700774485013
613, epoch_train_loss=3.1035700774485013
Epoch 614
Epoch 614 :: Batch 0/1
Batch Loss = 3.104538355267055
614, epoch_train_loss=3.104538355267055
Epoch 615
Epoch 615 :: Batch 0/1
Batch Loss = 3.1038176601936085
615, epoch_train_loss=3.1038176601936085
Epoch 616
Epoch 616 :: Batch 0/1
Batch Loss = 3.1026839413307066
616, epoch_train_loss=3.1026839413307066
Epoch 617
Epoch 617 :: Batch 0/1
Batch Loss = 3.101053813362522
617, epoch_train_loss=3.101053813362522
Epoch 618
Epoch 618 :: Batch 0/1
Batch Loss = 3.101183674577364
618, epoch_train_loss=3.101183674577364
Epoch 619
Epoch 619 :: Batch 0/1
Batch Loss = 3.1013705767427204
619, epoch_train_loss=3.1013705767427204
Epoch 620
Epoch 620 :: Batch 0/1
Batch Loss = 3.1023159909666
620, epoch_train_loss=3.1023159909666
Epoch 621
Epoch 621 :: Batch 0/1
Batch Loss = 3.10194952133075
621, epoch_train_loss=3.10194952133075
Epoch 622
Epoch 622 :: Batch 0/1
Batch Loss = 3.1018197439515074
622, epoch_train_loss=3.1018197439515074
Epoch 623
Epoch 623 :: Batch 0/1
Batch Loss = 3.1009362136601926
623, epoch_train_loss=3.1009362136601926
Epoch 624
Epoch 624 :: Batch 0/1
Batch Loss = 3.10037381334259
624, epoch_train_loss=3.10037381334259
Epoch 625
Epoch 625 :: Batch 0/1
Batch Loss = 3.0998461550910377
625, epoch_train_loss=3.0998461550910377
Epoch 626
Epoch 626 :: Batch 0/1
Batch Loss = 3.0996095572512274
626, epoch_train_loss=3.0996095572512274
Epoch 627
Epoch 627 :: Batch 0/1
Batch Loss = 3.0996310361470343
627, epoch_train_loss=3.0996310361470343
Epoch 628
Epoch 628 :: Batch 0/1
Batch Loss = 3.099669655927873
628, epoch_train_loss=3.099669655927873
Epoch 629
Epoch 629 :: Batch 0/1
Batch Loss = 3.099926766455531
629, epoch_train_loss=3.099926766455531
Epoch 630
Epoch 630 :: Batch 0/1
Batch Loss = 3.0999855758819437
630, epoch_train_loss=3.0999855758819437
Epoch 631
Epoch 631 :: Batch 0/1
Batch Loss = 3.1004380277013253
631, epoch_train_loss=3.1004380277013253
Epoch 632
Epoch 632 :: Batch 0/1
Batch Loss = 3.1004639614825114
632, epoch_train_loss=3.1004639614825114
Epoch 633
Epoch 633 :: Batch 0/1
Batch Loss = 3.10116335983065
633, epoch_train_loss=3.10116335983065
Epoch 634
Epoch 634 :: Batch 0/1
Batch Loss = 3.101311583023089
634, epoch_train_loss=3.101311583023089
Epoch 635
Epoch 635 :: Batch 0/1
Batch Loss = 3.1026346122491155
635, epoch_train_loss=3.1026346122491155
Epoch 636
Epoch 636 :: Batch 0/1
Batch Loss = 3.102693926390722
636, epoch_train_loss=3.102693926390722
Epoch 637
Epoch 637 :: Batch 0/1
Batch Loss = 3.1047714962200956
637, epoch_train_loss=3.1047714962200956
Epoch 638
Epoch 638 :: Batch 0/1
Batch Loss = 3.1045799691908496
638, epoch_train_loss=3.1045799691908496
Epoch 639
Epoch 639 :: Batch 0/1
Batch Loss = 3.10752055853084
639, epoch_train_loss=3.10752055853084
Epoch 640
Epoch 640 :: Batch 0/1
Batch Loss = 3.106036201897745
640, epoch_train_loss=3.106036201897745
Epoch 641
Epoch 641 :: Batch 0/1
Batch Loss = 3.108723070227682
641, epoch_train_loss=3.108723070227682
Epoch 642
Epoch 642 :: Batch 0/1
Batch Loss = 3.105642873659449
642, epoch_train_loss=3.105642873659449
Epoch 643
Epoch 643 :: Batch 0/1
Batch Loss = 3.1061523082763745
643, epoch_train_loss=3.1061523082763745
Epoch 644
Epoch 644 :: Batch 0/1
Batch Loss = 3.102587144439794
644, epoch_train_loss=3.102587144439794
Epoch 645
Epoch 645 :: Batch 0/1
Batch Loss = 3.1016398026250225
645, epoch_train_loss=3.1016398026250225
Epoch 646
Epoch 646 :: Batch 0/1
Batch Loss = 3.099704903090031
646, epoch_train_loss=3.099704903090031
Epoch 647
Epoch 647 :: Batch 0/1
Batch Loss = 3.099292410775467
647, epoch_train_loss=3.099292410775467
Epoch 648
Epoch 648 :: Batch 0/1
Batch Loss = 3.098808946772817
648, epoch_train_loss=3.098808946772817
Epoch 649
Epoch 649 :: Batch 0/1
Batch Loss = 3.099524890676224
649, epoch_train_loss=3.099524890676224
Epoch 650
Epoch 650 :: Batch 0/1
Batch Loss = 3.1000447257609993
650, epoch_train_loss=3.1000447257609993
Epoch 651
Epoch 651 :: Batch 0/1
Batch Loss = 3.1027844586874265
651, epoch_train_loss=3.1027844586874265
Epoch 652
Epoch 652 :: Batch 0/1
Batch Loss = 3.103807281420193
652, epoch_train_loss=3.103807281420193
Epoch 653
Epoch 653 :: Batch 0/1
Batch Loss = 3.1091691914624753
653, epoch_train_loss=3.1091691914624753
Epoch 654
Epoch 654 :: Batch 0/1
Batch Loss = 3.1067618832341775
654, epoch_train_loss=3.1067618832341775
Epoch 655
Epoch 655 :: Batch 0/1
Batch Loss = 3.109622948914963
655, epoch_train_loss=3.109622948914963
Epoch 656
Epoch 656 :: Batch 0/1
Batch Loss = 3.1035204073521196
656, epoch_train_loss=3.1035204073521196
Epoch 657
Epoch 657 :: Batch 0/1
Batch Loss = 3.101524927053267
657, epoch_train_loss=3.101524927053267
Epoch 658
Epoch 658 :: Batch 0/1
Batch Loss = 3.098086916596279
658, epoch_train_loss=3.098086916596279
Epoch 659
Epoch 659 :: Batch 0/1
Batch Loss = 3.0967992737708467
659, epoch_train_loss=3.0967992737708467
Epoch 660
Epoch 660 :: Batch 0/1
Batch Loss = 3.0959513461240213
660, epoch_train_loss=3.0959513461240213
Epoch 661
Epoch 661 :: Batch 0/1
Batch Loss = 3.0956926833847205
661, epoch_train_loss=3.0956926833847205
Epoch 662
Epoch 662 :: Batch 0/1
Batch Loss = 3.095579472268434
662, epoch_train_loss=3.095579472268434
Epoch 663
Epoch 663 :: Batch 0/1
Batch Loss = 3.095943235618037
663, epoch_train_loss=3.095943235618037
Epoch 664
Epoch 664 :: Batch 0/1
Batch Loss = 3.09648800633233
664, epoch_train_loss=3.09648800633233
Epoch 665
Epoch 665 :: Batch 0/1
Batch Loss = 3.0982834977750913
665, epoch_train_loss=3.0982834977750913
Epoch 666
Epoch 666 :: Batch 0/1
Batch Loss = 3.1000503092446197
666, epoch_train_loss=3.1000503092446197
Epoch 667
Epoch 667 :: Batch 0/1
Batch Loss = 3.1067132659018957
667, epoch_train_loss=3.1067132659018957
Epoch 668
Epoch 668 :: Batch 0/1
Batch Loss = 3.1093412986584985
668, epoch_train_loss=3.1093412986584985
Epoch 669
Epoch 669 :: Batch 0/1
Batch Loss = 3.1256202190198104
669, epoch_train_loss=3.1256202190198104
Epoch 670
Epoch 670 :: Batch 0/1
Batch Loss = 3.111743831646098
670, epoch_train_loss=3.111743831646098
Epoch 671
Epoch 671 :: Batch 0/1
Batch Loss = 3.1076062645156526
671, epoch_train_loss=3.1076062645156526
Epoch 672
Epoch 672 :: Batch 0/1
Batch Loss = 3.0962278326969184
672, epoch_train_loss=3.0962278326969184
Epoch 673
Epoch 673 :: Batch 0/1
Batch Loss = 3.0938239670710663
673, epoch_train_loss=3.0938239670710663
Epoch 674
Epoch 674 :: Batch 0/1
Batch Loss = 3.0997394613605764
674, epoch_train_loss=3.0997394613605764
Epoch 675
Epoch 675 :: Batch 0/1
Batch Loss = 3.112591080331852
675, epoch_train_loss=3.112591080331852
Epoch 676
Epoch 676 :: Batch 0/1
Batch Loss = 3.1567193403128755
676, epoch_train_loss=3.1567193403128755
Epoch 677
Epoch 677 :: Batch 0/1
Batch Loss = 3.11937947517108
677, epoch_train_loss=3.11937947517108
Epoch 678
Epoch 678 :: Batch 0/1
Batch Loss = 3.103879661412074
678, epoch_train_loss=3.103879661412074
Epoch 679
Epoch 679 :: Batch 0/1
Batch Loss = 3.0998213946218205
679, epoch_train_loss=3.0998213946218205
Epoch 680
Epoch 680 :: Batch 0/1
Batch Loss = 3.114151249957158
680, epoch_train_loss=3.114151249957158
Epoch 681
Epoch 681 :: Batch 0/1
Batch Loss = 3.1444413254641033
681, epoch_train_loss=3.1444413254641033
Epoch 682
Epoch 682 :: Batch 0/1
Batch Loss = 3.112998402349506
682, epoch_train_loss=3.112998402349506
Epoch 683
Epoch 683 :: Batch 0/1
Batch Loss = 3.1038282391102703
683, epoch_train_loss=3.1038282391102703
Epoch 684
Epoch 684 :: Batch 0/1
Batch Loss = 3.0936518658311103
684, epoch_train_loss=3.0936518658311103
Epoch 685
Epoch 685 :: Batch 0/1
Batch Loss = 3.0995434660533236
685, epoch_train_loss=3.0995434660533236
Epoch 686
Epoch 686 :: Batch 0/1
Batch Loss = 3.112509493370141
686, epoch_train_loss=3.112509493370141
Epoch 687
Epoch 687 :: Batch 0/1
Batch Loss = 3.1013325622842784
687, epoch_train_loss=3.1013325622842784
Epoch 688
Epoch 688 :: Batch 0/1
Batch Loss = 3.0953487614807007
688, epoch_train_loss=3.0953487614807007
Epoch 689
Epoch 689 :: Batch 0/1
Batch Loss = 3.0935157244852602
689, epoch_train_loss=3.0935157244852602
Epoch 690
Epoch 690 :: Batch 0/1
Batch Loss = 3.099568932066651
690, epoch_train_loss=3.099568932066651
Epoch 691
Epoch 691 :: Batch 0/1
Batch Loss = 3.106885588000578
691, epoch_train_loss=3.106885588000578
Epoch 692
Epoch 692 :: Batch 0/1
Batch Loss = 3.102945497156075
692, epoch_train_loss=3.102945497156075
Epoch 693
Epoch 693 :: Batch 0/1
Batch Loss = 3.1011337188276524
693, epoch_train_loss=3.1011337188276524
Epoch 694
Epoch 694 :: Batch 0/1
Batch Loss = 3.093182860743924
694, epoch_train_loss=3.093182860743924
Epoch 695
Epoch 695 :: Batch 0/1
Batch Loss = 3.094895642538236
695, epoch_train_loss=3.094895642538236
Epoch 696
Epoch 696 :: Batch 0/1
Batch Loss = 3.099963058392613
696, epoch_train_loss=3.099963058392613
Epoch 697
Epoch 697 :: Batch 0/1
Batch Loss = 3.100949990787553
697, epoch_train_loss=3.100949990787553
Epoch 698
Epoch 698 :: Batch 0/1
Batch Loss = 3.1003113059544227
698, epoch_train_loss=3.1003113059544227
Epoch 699
Epoch 699 :: Batch 0/1
Batch Loss = 3.0950591784040795
699, epoch_train_loss=3.0950591784040795
Epoch 700
Epoch 700 :: Batch 0/1
Batch Loss = 3.0925602944919066
700, epoch_train_loss=3.0925602944919066
Epoch 701
Epoch 701 :: Batch 0/1
Batch Loss = 3.09076774253838
701, epoch_train_loss=3.09076774253838
Epoch 702
Epoch 702 :: Batch 0/1
Batch Loss = 3.0929663880186253
702, epoch_train_loss=3.0929663880186253
Epoch 703
Epoch 703 :: Batch 0/1
Batch Loss = 3.0953511161070657
703, epoch_train_loss=3.0953511161070657
Epoch 704
Epoch 704 :: Batch 0/1
Batch Loss = 3.096556114262444
704, epoch_train_loss=3.096556114262444
Epoch 705
Epoch 705 :: Batch 0/1
Batch Loss = 3.0972294877924735
705, epoch_train_loss=3.0972294877924735
Epoch 706
Epoch 706 :: Batch 0/1
Batch Loss = 3.0941448343478335
706, epoch_train_loss=3.0941448343478335
Epoch 707
Epoch 707 :: Batch 0/1
Batch Loss = 3.092213500690371
707, epoch_train_loss=3.092213500690371
Epoch 708
Epoch 708 :: Batch 0/1
Batch Loss = 3.089986128911655
708, epoch_train_loss=3.089986128911655
Epoch 709
Epoch 709 :: Batch 0/1
Batch Loss = 3.090189532568369
709, epoch_train_loss=3.090189532568369
Epoch 710
Epoch 710 :: Batch 0/1
Batch Loss = 3.090468640914838
710, epoch_train_loss=3.090468640914838
Epoch 711
Epoch 711 :: Batch 0/1
Batch Loss = 3.0924452039491372
711, epoch_train_loss=3.0924452039491372
Epoch 712
Epoch 712 :: Batch 0/1
Batch Loss = 3.0953652751130014
712, epoch_train_loss=3.0953652751130014
Epoch 713
Epoch 713 :: Batch 0/1
Batch Loss = 3.096513038988608
713, epoch_train_loss=3.096513038988608
Epoch 714
Epoch 714 :: Batch 0/1
Batch Loss = 3.101195303782174
714, epoch_train_loss=3.101195303782174
Epoch 715
Epoch 715 :: Batch 0/1
Batch Loss = 3.0962674661806107
715, epoch_train_loss=3.0962674661806107
Epoch 716
Epoch 716 :: Batch 0/1
Batch Loss = 3.094865227915794
716, epoch_train_loss=3.094865227915794
Epoch 717
Epoch 717 :: Batch 0/1
Batch Loss = 3.090958068369141
717, epoch_train_loss=3.090958068369141
Epoch 718
Epoch 718 :: Batch 0/1
Batch Loss = 3.089834203522338
718, epoch_train_loss=3.089834203522338
Epoch 719
Epoch 719 :: Batch 0/1
Batch Loss = 3.0885763747060464
719, epoch_train_loss=3.0885763747060464
Epoch 720
Epoch 720 :: Batch 0/1
Batch Loss = 3.0883782541864058
720, epoch_train_loss=3.0883782541864058
Epoch 721
Epoch 721 :: Batch 0/1
Batch Loss = 3.08818105918135
721, epoch_train_loss=3.08818105918135
Epoch 722
Epoch 722 :: Batch 0/1
Batch Loss = 3.088091893268652
722, epoch_train_loss=3.088091893268652
Epoch 723
Epoch 723 :: Batch 0/1
Batch Loss = 3.0885283389999656
723, epoch_train_loss=3.0885283389999656
Epoch 724
Epoch 724 :: Batch 0/1
Batch Loss = 3.0888566360113763
724, epoch_train_loss=3.0888566360113763
Epoch 725
Epoch 725 :: Batch 0/1
Batch Loss = 3.0904770297092137
725, epoch_train_loss=3.0904770297092137
Epoch 726
Epoch 726 :: Batch 0/1
Batch Loss = 3.091910372422253
726, epoch_train_loss=3.091910372422253
Epoch 727
Epoch 727 :: Batch 0/1
Batch Loss = 3.098084308000486
727, epoch_train_loss=3.098084308000486
Epoch 728
Epoch 728 :: Batch 0/1
Batch Loss = 3.1016815096423764
728, epoch_train_loss=3.1016815096423764
Epoch 729
Epoch 729 :: Batch 0/1
Batch Loss = 3.1198295580217903
729, epoch_train_loss=3.1198295580217903
Epoch 730
Epoch 730 :: Batch 0/1
Batch Loss = 3.102227631524312
730, epoch_train_loss=3.102227631524312
Epoch 731
Epoch 731 :: Batch 0/1
Batch Loss = 3.096113677257765
731, epoch_train_loss=3.096113677257765
Epoch 732
Epoch 732 :: Batch 0/1
Batch Loss = 3.087896274833011
732, epoch_train_loss=3.087896274833011
Epoch 733
Epoch 733 :: Batch 0/1
Batch Loss = 3.0878128670149785
733, epoch_train_loss=3.0878128670149785
Epoch 734
Epoch 734 :: Batch 0/1
Batch Loss = 3.096128644014073
734, epoch_train_loss=3.096128644014073
Epoch 735
Epoch 735 :: Batch 0/1
Batch Loss = 3.1086705656174822
735, epoch_train_loss=3.1086705656174822
Epoch 736
Epoch 736 :: Batch 0/1
Batch Loss = 3.154213859869428
736, epoch_train_loss=3.154213859869428
Epoch 737
Epoch 737 :: Batch 0/1
Batch Loss = 3.0990691395474363
737, epoch_train_loss=3.0990691395474363
Epoch 738
Epoch 738 :: Batch 0/1
Batch Loss = 3.100125712723996
738, epoch_train_loss=3.100125712723996
Epoch 739
Epoch 739 :: Batch 0/1
Batch Loss = 3.133788622683012
739, epoch_train_loss=3.133788622683012
Epoch 740
Epoch 740 :: Batch 0/1
Batch Loss = 3.0958266735547983
740, epoch_train_loss=3.0958266735547983
Epoch 741
Epoch 741 :: Batch 0/1
Batch Loss = 3.098781110570194
741, epoch_train_loss=3.098781110570194
Epoch 742
Epoch 742 :: Batch 0/1
Batch Loss = 3.150939674002243
742, epoch_train_loss=3.150939674002243
Epoch 743
Epoch 743 :: Batch 0/1
Batch Loss = 3.1097570394054186
743, epoch_train_loss=3.1097570394054186
Epoch 744
Epoch 744 :: Batch 0/1
Batch Loss = 3.0886235772887156
744, epoch_train_loss=3.0886235772887156
Epoch 745
Epoch 745 :: Batch 0/1
Batch Loss = 3.0969644802054557
745, epoch_train_loss=3.0969644802054557
Epoch 746
Epoch 746 :: Batch 0/1
Batch Loss = 3.1024894591074688
746, epoch_train_loss=3.1024894591074688
Epoch 747
Epoch 747 :: Batch 0/1
Batch Loss = 3.111076824508473
747, epoch_train_loss=3.111076824508473
Epoch 748
Epoch 748 :: Batch 0/1
Batch Loss = 3.088728045603238
748, epoch_train_loss=3.088728045603238
Epoch 749
Epoch 749 :: Batch 0/1
Batch Loss = 3.0981450661096783
749, epoch_train_loss=3.0981450661096783
Epoch 750
Epoch 750 :: Batch 0/1
Batch Loss = 3.1292461898445008
750, epoch_train_loss=3.1292461898445008
Epoch 751
Epoch 751 :: Batch 0/1
Batch Loss = 3.0920035799966556
751, epoch_train_loss=3.0920035799966556
Epoch 752
Epoch 752 :: Batch 0/1
Batch Loss = 3.1043262204879323
752, epoch_train_loss=3.1043262204879323
Epoch 753
Epoch 753 :: Batch 0/1
Batch Loss = 3.1483299853840876
753, epoch_train_loss=3.1483299853840876
Epoch 754
Epoch 754 :: Batch 0/1
Batch Loss = 3.0919641296046714
754, epoch_train_loss=3.0919641296046714
Epoch 755
Epoch 755 :: Batch 0/1
Batch Loss = 3.161129534887185
755, epoch_train_loss=3.161129534887185
Epoch 756
Epoch 756 :: Batch 0/1
Batch Loss = 3.2713081862766273
756, epoch_train_loss=3.2713081862766273
Epoch 757
Epoch 757 :: Batch 0/1
Batch Loss = 3.2160409416676847
757, epoch_train_loss=3.2160409416676847
Epoch 758
Epoch 758 :: Batch 0/1
Batch Loss = 3.1808107406400783
758, epoch_train_loss=3.1808107406400783
Epoch 759
Epoch 759 :: Batch 0/1
Batch Loss = 3.2649206179652266
759, epoch_train_loss=3.2649206179652266
Epoch 760
Epoch 760 :: Batch 0/1
Batch Loss = 3.2340378048749385
760, epoch_train_loss=3.2340378048749385
Epoch 761
Epoch 761 :: Batch 0/1
Batch Loss = 3.291882611806031
761, epoch_train_loss=3.291882611806031
Epoch 762
Epoch 762 :: Batch 0/1
Batch Loss = 3.196544042346138
762, epoch_train_loss=3.196544042346138
Epoch 763
Epoch 763 :: Batch 0/1
Batch Loss = 3.262882462702711
763, epoch_train_loss=3.262882462702711
Epoch 764
Epoch 764 :: Batch 0/1
Batch Loss = 3.1626015361720334
764, epoch_train_loss=3.1626015361720334
Epoch 765
Epoch 765 :: Batch 0/1
Batch Loss = 3.2199925588076814
765, epoch_train_loss=3.2199925588076814
Epoch 766
Epoch 766 :: Batch 0/1
Batch Loss = 3.1233449844715877
766, epoch_train_loss=3.1233449844715877
Epoch 767
Epoch 767 :: Batch 0/1
Batch Loss = 3.2578112779733814
767, epoch_train_loss=3.2578112779733814
Epoch 768
Epoch 768 :: Batch 0/1
Batch Loss = 3.148762588979647
768, epoch_train_loss=3.148762588979647
Epoch 769
Epoch 769 :: Batch 0/1
Batch Loss = 3.2365741230787215
769, epoch_train_loss=3.2365741230787215
Epoch 770
Epoch 770 :: Batch 0/1
Batch Loss = 3.1216000088375693
770, epoch_train_loss=3.1216000088375693
Epoch 771
Epoch 771 :: Batch 0/1
Batch Loss = 3.226569412277497
771, epoch_train_loss=3.226569412277497
Epoch 772
Epoch 772 :: Batch 0/1
Batch Loss = 3.1220858589759755
772, epoch_train_loss=3.1220858589759755
Epoch 773
Epoch 773 :: Batch 0/1
Batch Loss = 3.178384036892448
773, epoch_train_loss=3.178384036892448
Epoch 774
Epoch 774 :: Batch 0/1
Batch Loss = 3.1299022032414383
774, epoch_train_loss=3.1299022032414383
Epoch 775
Epoch 775 :: Batch 0/1
Batch Loss = 3.1584742628158398
775, epoch_train_loss=3.1584742628158398
Epoch 776
Epoch 776 :: Batch 0/1
Batch Loss = 3.100679683590397
776, epoch_train_loss=3.100679683590397
Epoch 777
Epoch 777 :: Batch 0/1
Batch Loss = 3.125500250099099
777, epoch_train_loss=3.125500250099099
Epoch 778
Epoch 778 :: Batch 0/1
Batch Loss = 3.1090484555663194
778, epoch_train_loss=3.1090484555663194
Epoch 779
Epoch 779 :: Batch 0/1
Batch Loss = 3.123798758260294
779, epoch_train_loss=3.123798758260294
Epoch 780
Epoch 780 :: Batch 0/1
Batch Loss = 3.0957894281363267
780, epoch_train_loss=3.0957894281363267
Epoch 781
Epoch 781 :: Batch 0/1
Batch Loss = 3.1109430373800206
781, epoch_train_loss=3.1109430373800206
Epoch 782
Epoch 782 :: Batch 0/1
Batch Loss = 3.0964644094165736
782, epoch_train_loss=3.0964644094165736
Epoch 783
Epoch 783 :: Batch 0/1
Batch Loss = 3.113811145925805
783, epoch_train_loss=3.113811145925805
Epoch 784
Epoch 784 :: Batch 0/1
Batch Loss = 3.094221111901352
784, epoch_train_loss=3.094221111901352
Epoch 785
Epoch 785 :: Batch 0/1
Batch Loss = 3.104203408814312
785, epoch_train_loss=3.104203408814312
Epoch 786
Epoch 786 :: Batch 0/1
Batch Loss = 3.092222972847484
786, epoch_train_loss=3.092222972847484
Epoch 787
Epoch 787 :: Batch 0/1
Batch Loss = 3.101313552309936
787, epoch_train_loss=3.101313552309936
Epoch 788
Epoch 788 :: Batch 0/1
Batch Loss = 3.0949714710940643
788, epoch_train_loss=3.0949714710940643
Epoch 789
Epoch 789 :: Batch 0/1
Batch Loss = 3.0971964979067383
789, epoch_train_loss=3.0971964979067383
Epoch 790
Epoch 790 :: Batch 0/1
Batch Loss = 3.0937406247630372
790, epoch_train_loss=3.0937406247630372
Epoch 791
Epoch 791 :: Batch 0/1
Batch Loss = 3.0906678987421636
791, epoch_train_loss=3.0906678987421636
Epoch 792
Epoch 792 :: Batch 0/1
Batch Loss = 3.095650110729188
792, epoch_train_loss=3.095650110729188
Epoch 793
Epoch 793 :: Batch 0/1
Batch Loss = 3.0890098514287665
793, epoch_train_loss=3.0890098514287665
Epoch 794
Epoch 794 :: Batch 0/1
Batch Loss = 3.096382223606265
794, epoch_train_loss=3.096382223606265
Epoch 795
Epoch 795 :: Batch 0/1
Batch Loss = 3.088807015587095
795, epoch_train_loss=3.088807015587095
Epoch 796
Epoch 796 :: Batch 0/1
Batch Loss = 3.0918662999026143
796, epoch_train_loss=3.0918662999026143
Epoch 797
Epoch 797 :: Batch 0/1
Batch Loss = 3.0897494983647165
797, epoch_train_loss=3.0897494983647165
Epoch 798
Epoch 798 :: Batch 0/1
Batch Loss = 3.0883778326584364
798, epoch_train_loss=3.0883778326584364
Epoch 799
Epoch 799 :: Batch 0/1
Batch Loss = 3.090420204792442
799, epoch_train_loss=3.090420204792442
Epoch 800
Epoch 800 :: Batch 0/1
Batch Loss = 3.0865858506515784
800, epoch_train_loss=3.0865858506515784
Epoch 801
Epoch 801 :: Batch 0/1
Batch Loss = 3.091235218023588
801, epoch_train_loss=3.091235218023588
Epoch 802
Epoch 802 :: Batch 0/1
Batch Loss = 3.0872938855076373
802, epoch_train_loss=3.0872938855076373
Epoch 803
Epoch 803 :: Batch 0/1
Batch Loss = 3.087670342766677
803, epoch_train_loss=3.087670342766677
Epoch 804
Epoch 804 :: Batch 0/1
Batch Loss = 3.0887066926951654
804, epoch_train_loss=3.0887066926951654
Epoch 805
Epoch 805 :: Batch 0/1
Batch Loss = 3.0856819789558325
805, epoch_train_loss=3.0856819789558325
Epoch 806
Epoch 806 :: Batch 0/1
Batch Loss = 3.0875486667300494
806, epoch_train_loss=3.0875486667300494
Epoch 807
Epoch 807 :: Batch 0/1
Batch Loss = 3.0855925002176496
807, epoch_train_loss=3.0855925002176496
Epoch 808
Epoch 808 :: Batch 0/1
Batch Loss = 3.086365631898314
808, epoch_train_loss=3.086365631898314
Epoch 809
Epoch 809 :: Batch 0/1
Batch Loss = 3.0863938908808857
809, epoch_train_loss=3.0863938908808857
Epoch 810
Epoch 810 :: Batch 0/1
Batch Loss = 3.0848631961305877
810, epoch_train_loss=3.0848631961305877
Epoch 811
Epoch 811 :: Batch 0/1
Batch Loss = 3.086355169866022
811, epoch_train_loss=3.086355169866022
Epoch 812
Epoch 812 :: Batch 0/1
Batch Loss = 3.084966425505767
812, epoch_train_loss=3.084966425505767
Epoch 813
Epoch 813 :: Batch 0/1
Batch Loss = 3.08490250902716
813, epoch_train_loss=3.08490250902716
Epoch 814
Epoch 814 :: Batch 0/1
Batch Loss = 3.0851162165187196
814, epoch_train_loss=3.0851162165187196
Epoch 815
Epoch 815 :: Batch 0/1
Batch Loss = 3.0839880948559486
815, epoch_train_loss=3.0839880948559486
Epoch 816
Epoch 816 :: Batch 0/1
Batch Loss = 3.084609656240889
816, epoch_train_loss=3.084609656240889
Epoch 817
Epoch 817 :: Batch 0/1
Batch Loss = 3.0838485671149805
817, epoch_train_loss=3.0838485671149805
Epoch 818
Epoch 818 :: Batch 0/1
Batch Loss = 3.0835867129807055
818, epoch_train_loss=3.0835867129807055
Epoch 819
Epoch 819 :: Batch 0/1
Batch Loss = 3.083895824237894
819, epoch_train_loss=3.083895824237894
Epoch 820
Epoch 820 :: Batch 0/1
Batch Loss = 3.0831074498422173
820, epoch_train_loss=3.0831074498422173
Epoch 821
Epoch 821 :: Batch 0/1
Batch Loss = 3.0833678683207215
821, epoch_train_loss=3.0833678683207215
Epoch 822
Epoch 822 :: Batch 0/1
Batch Loss = 3.0830949817931805
822, epoch_train_loss=3.0830949817931805
Epoch 823
Epoch 823 :: Batch 0/1
Batch Loss = 3.0826583629744637
823, epoch_train_loss=3.0826583629744637
Epoch 824
Epoch 824 :: Batch 0/1
Batch Loss = 3.082926422397272
824, epoch_train_loss=3.082926422397272
Epoch 825
Epoch 825 :: Batch 0/1
Batch Loss = 3.0823945763305782
825, epoch_train_loss=3.0823945763305782
Epoch 826
Epoch 826 :: Batch 0/1
Batch Loss = 3.082275522736173
826, epoch_train_loss=3.082275522736173
Epoch 827
Epoch 827 :: Batch 0/1
Batch Loss = 3.082318911749374
827, epoch_train_loss=3.082318911749374
Epoch 828
Epoch 828 :: Batch 0/1
Batch Loss = 3.081831033008338
828, epoch_train_loss=3.081831033008338
Epoch 829
Epoch 829 :: Batch 0/1
Batch Loss = 3.081845489511437
829, epoch_train_loss=3.081845489511437
Epoch 830
Epoch 830 :: Batch 0/1
Batch Loss = 3.0816925009879372
830, epoch_train_loss=3.0816925009879372
Epoch 831
Epoch 831 :: Batch 0/1
Batch Loss = 3.0813543523618026
831, epoch_train_loss=3.0813543523618026
Epoch 832
Epoch 832 :: Batch 0/1
Batch Loss = 3.0813681521756515
832, epoch_train_loss=3.0813681521756515
Epoch 833
Epoch 833 :: Batch 0/1
Batch Loss = 3.0811203771865254
833, epoch_train_loss=3.0811203771865254
Epoch 834
Epoch 834 :: Batch 0/1
Batch Loss = 3.0809071604634144
834, epoch_train_loss=3.0809071604634144
Epoch 835
Epoch 835 :: Batch 0/1
Batch Loss = 3.0808693396263322
835, epoch_train_loss=3.0808693396263322
Epoch 836
Epoch 836 :: Batch 0/1
Batch Loss = 3.0806031529189357
836, epoch_train_loss=3.0806031529189357
Epoch 837
Epoch 837 :: Batch 0/1
Batch Loss = 3.080455428264154
837, epoch_train_loss=3.080455428264154
Epoch 838
Epoch 838 :: Batch 0/1
Batch Loss = 3.0803697162304027
838, epoch_train_loss=3.0803697162304027
Epoch 839
Epoch 839 :: Batch 0/1
Batch Loss = 3.080120483743855
839, epoch_train_loss=3.080120483743855
Epoch 840
Epoch 840 :: Batch 0/1
Batch Loss = 3.079988965101949
840, epoch_train_loss=3.079988965101949
Epoch 841
Epoch 841 :: Batch 0/1
Batch Loss = 3.07986858560511
841, epoch_train_loss=3.07986858560511
Epoch 842
Epoch 842 :: Batch 0/1
Batch Loss = 3.079647130052056
842, epoch_train_loss=3.079647130052056
Epoch 843
Epoch 843 :: Batch 0/1
Batch Loss = 3.0795195538873292
843, epoch_train_loss=3.0795195538873292
Epoch 844
Epoch 844 :: Batch 0/1
Batch Loss = 3.0793775424597
844, epoch_train_loss=3.0793775424597
Epoch 845
Epoch 845 :: Batch 0/1
Batch Loss = 3.0791727713230346
845, epoch_train_loss=3.0791727713230346
Epoch 846
Epoch 846 :: Batch 0/1
Batch Loss = 3.07904241824949
846, epoch_train_loss=3.07904241824949
Epoch 847
Epoch 847 :: Batch 0/1
Batch Loss = 3.0788966870996455
847, epoch_train_loss=3.0788966870996455
Epoch 848
Epoch 848 :: Batch 0/1
Batch Loss = 3.0787005320183165
848, epoch_train_loss=3.0787005320183165
Epoch 849
Epoch 849 :: Batch 0/1
Batch Loss = 3.0785601295922027
849, epoch_train_loss=3.0785601295922027
Epoch 850
Epoch 850 :: Batch 0/1
Batch Loss = 3.0784184035963973
850, epoch_train_loss=3.0784184035963973
Epoch 851
Epoch 851 :: Batch 0/1
Batch Loss = 3.0782305003136208
851, epoch_train_loss=3.0782305003136208
Epoch 852
Epoch 852 :: Batch 0/1
Batch Loss = 3.078078789588158
852, epoch_train_loss=3.078078789588158
Epoch 853
Epoch 853 :: Batch 0/1
Batch Loss = 3.077940536772111
853, epoch_train_loss=3.077940536772111
Epoch 854
Epoch 854 :: Batch 0/1
Batch Loss = 3.0777622256334234
854, epoch_train_loss=3.0777622256334234
Epoch 855
Epoch 855 :: Batch 0/1
Batch Loss = 3.0775994015743287
855, epoch_train_loss=3.0775994015743287
Epoch 856
Epoch 856 :: Batch 0/1
Batch Loss = 3.077460158121718
856, epoch_train_loss=3.077460158121718
Epoch 857
Epoch 857 :: Batch 0/1
Batch Loss = 3.0772937551067896
857, epoch_train_loss=3.0772937551067896
Epoch 858
Epoch 858 :: Batch 0/1
Batch Loss = 3.0771244957927992
858, epoch_train_loss=3.0771244957927992
Epoch 859
Epoch 859 :: Batch 0/1
Batch Loss = 3.076977960873928
859, epoch_train_loss=3.076977960873928
Epoch 860
Epoch 860 :: Batch 0/1
Batch Loss = 3.076822414917957
860, epoch_train_loss=3.076822414917957
Epoch 861
Epoch 861 :: Batch 0/1
Batch Loss = 3.0766536785744227
861, epoch_train_loss=3.0766536785744227
Epoch 862
Epoch 862 :: Batch 0/1
Batch Loss = 3.0764962779971134
862, epoch_train_loss=3.0764962779971134
Epoch 863
Epoch 863 :: Batch 0/1
Batch Loss = 3.0763456472757307
863, epoch_train_loss=3.0763456472757307
Epoch 864
Epoch 864 :: Batch 0/1
Batch Loss = 3.0761836373421643
864, epoch_train_loss=3.0761836373421643
Epoch 865
Epoch 865 :: Batch 0/1
Batch Loss = 3.076018943021365
865, epoch_train_loss=3.076018943021365
Epoch 866
Epoch 866 :: Batch 0/1
Batch Loss = 3.075864682305824
866, epoch_train_loss=3.075864682305824
Epoch 867
Epoch 867 :: Batch 0/1
Batch Loss = 3.0757090242500498
867, epoch_train_loss=3.0757090242500498
Epoch 868
Epoch 868 :: Batch 0/1
Batch Loss = 3.0755454583138735
868, epoch_train_loss=3.0755454583138735
Epoch 869
Epoch 869 :: Batch 0/1
Batch Loss = 3.075384074929182
869, epoch_train_loss=3.075384074929182
Epoch 870
Epoch 870 :: Batch 0/1
Batch Loss = 3.07522791241244
870, epoch_train_loss=3.07522791241244
Epoch 871
Epoch 871 :: Batch 0/1
Batch Loss = 3.0750698789096265
871, epoch_train_loss=3.0750698789096265
Epoch 872
Epoch 872 :: Batch 0/1
Batch Loss = 3.074907103991694
872, epoch_train_loss=3.074907103991694
Epoch 873
Epoch 873 :: Batch 0/1
Batch Loss = 3.074745612856482
873, epoch_train_loss=3.074745612856482
Epoch 874
Epoch 874 :: Batch 0/1
Batch Loss = 3.0745876112005743
874, epoch_train_loss=3.0745876112005743
Epoch 875
Epoch 875 :: Batch 0/1
Batch Loss = 3.0744287341772822
875, epoch_train_loss=3.0744287341772822
Epoch 876
Epoch 876 :: Batch 0/1
Batch Loss = 3.0742662423728646
876, epoch_train_loss=3.0742662423728646
Epoch 877
Epoch 877 :: Batch 0/1
Batch Loss = 3.0741037016590664
877, epoch_train_loss=3.0741037016590664
Epoch 878
Epoch 878 :: Batch 0/1
Batch Loss = 3.073943963227038
878, epoch_train_loss=3.073943963227038
Epoch 879
Epoch 879 :: Batch 0/1
Batch Loss = 3.073784024961076
879, epoch_train_loss=3.073784024961076
Epoch 880
Epoch 880 :: Batch 0/1
Batch Loss = 3.073621855340609
880, epoch_train_loss=3.073621855340609
Epoch 881
Epoch 881 :: Batch 0/1
Batch Loss = 3.073458641064729
881, epoch_train_loss=3.073458641064729
Epoch 882
Epoch 882 :: Batch 0/1
Batch Loss = 3.0732965408602344
882, epoch_train_loss=3.0732965408602344
Epoch 883
Epoch 883 :: Batch 0/1
Batch Loss = 3.0731351134361766
883, epoch_train_loss=3.0731351134361766
Epoch 884
Epoch 884 :: Batch 0/1
Batch Loss = 3.072973155573442
884, epoch_train_loss=3.072973155573442
Epoch 885
Epoch 885 :: Batch 0/1
Batch Loss = 3.0728097360519397
885, epoch_train_loss=3.0728097360519397
Epoch 886
Epoch 886 :: Batch 0/1
Batch Loss = 3.072645707385745
886, epoch_train_loss=3.072645707385745
Epoch 887
Epoch 887 :: Batch 0/1
Batch Loss = 3.0724821808539784
887, epoch_train_loss=3.0724821808539784
Epoch 888
Epoch 888 :: Batch 0/1
Batch Loss = 3.0723190710641806
888, epoch_train_loss=3.0723190710641806
Epoch 889
Epoch 889 :: Batch 0/1
Batch Loss = 3.072155447961369
889, epoch_train_loss=3.072155447961369
Epoch 890
Epoch 890 :: Batch 0/1
Batch Loss = 3.0719910338438257
890, epoch_train_loss=3.0719910338438257
Epoch 891
Epoch 891 :: Batch 0/1
Batch Loss = 3.0718259268260706
891, epoch_train_loss=3.0718259268260706
Epoch 892
Epoch 892 :: Batch 0/1
Batch Loss = 3.0716605764545797
892, epoch_train_loss=3.0716605764545797
Epoch 893
Epoch 893 :: Batch 0/1
Batch Loss = 3.071495325837393
893, epoch_train_loss=3.071495325837393
Epoch 894
Epoch 894 :: Batch 0/1
Batch Loss = 3.071330096977074
894, epoch_train_loss=3.071330096977074
Epoch 895
Epoch 895 :: Batch 0/1
Batch Loss = 3.071164427412157
895, epoch_train_loss=3.071164427412157
Epoch 896
Epoch 896 :: Batch 0/1
Batch Loss = 3.070998276945766
896, epoch_train_loss=3.070998276945766
Epoch 897
Epoch 897 :: Batch 0/1
Batch Loss = 3.070831536582686
897, epoch_train_loss=3.070831536582686
Epoch 898
Epoch 898 :: Batch 0/1
Batch Loss = 3.070664363636198
898, epoch_train_loss=3.070664363636198
Epoch 899
Epoch 899 :: Batch 0/1
Batch Loss = 3.0704968949184366
899, epoch_train_loss=3.0704968949184366
Epoch 900
Epoch 900 :: Batch 0/1
Batch Loss = 3.0703292322884312
900, epoch_train_loss=3.0703292322884312
Epoch 901
Epoch 901 :: Batch 0/1
Batch Loss = 3.070161311176457
901, epoch_train_loss=3.070161311176457
Epoch 902
Epoch 902 :: Batch 0/1
Batch Loss = 3.06999312329344
902, epoch_train_loss=3.06999312329344
Epoch 903
Epoch 903 :: Batch 0/1
Batch Loss = 3.069824644003825
903, epoch_train_loss=3.069824644003825
Epoch 904
Epoch 904 :: Batch 0/1
Batch Loss = 3.0696557652089735
904, epoch_train_loss=3.0696557652089735
Epoch 905
Epoch 905 :: Batch 0/1
Batch Loss = 3.069486559460464
905, epoch_train_loss=3.069486559460464
Epoch 906
Epoch 906 :: Batch 0/1
Batch Loss = 3.0693169761501937
906, epoch_train_loss=3.0693169761501937
Epoch 907
Epoch 907 :: Batch 0/1
Batch Loss = 3.0691470650699686
907, epoch_train_loss=3.0691470650699686
Epoch 908
Epoch 908 :: Batch 0/1
Batch Loss = 3.0689768814557628
908, epoch_train_loss=3.0689768814557628
Epoch 909
Epoch 909 :: Batch 0/1
Batch Loss = 3.0688065059028156
909, epoch_train_loss=3.0688065059028156
Epoch 910
Epoch 910 :: Batch 0/1
Batch Loss = 3.068636072661238
910, epoch_train_loss=3.068636072661238
Epoch 911
Epoch 911 :: Batch 0/1
Batch Loss = 3.068465935486609
911, epoch_train_loss=3.068465935486609
Epoch 912
Epoch 912 :: Batch 0/1
Batch Loss = 3.06829673598379
912, epoch_train_loss=3.06829673598379
Epoch 913
Epoch 913 :: Batch 0/1
Batch Loss = 3.068129828715219
913, epoch_train_loss=3.068129828715219
Epoch 914
Epoch 914 :: Batch 0/1
Batch Loss = 3.067968789950159
914, epoch_train_loss=3.067968789950159
Epoch 915
Epoch 915 :: Batch 0/1
Batch Loss = 3.0678201364298805
915, epoch_train_loss=3.0678201364298805
Epoch 916
Epoch 916 :: Batch 0/1
Batch Loss = 3.0677057711974376
916, epoch_train_loss=3.0677057711974376
Epoch 917
Epoch 917 :: Batch 0/1
Batch Loss = 3.067660517135525
917, epoch_train_loss=3.067660517135525
Epoch 918
Epoch 918 :: Batch 0/1
Batch Loss = 3.067843923336699
918, epoch_train_loss=3.067843923336699
Epoch 919
Epoch 919 :: Batch 0/1
Batch Loss = 3.0684204617733792
919, epoch_train_loss=3.0684204617733792
Epoch 920
Epoch 920 :: Batch 0/1
Batch Loss = 3.070760180537539
920, epoch_train_loss=3.070760180537539
Epoch 921
Epoch 921 :: Batch 0/1
Batch Loss = 3.0744235720239566
921, epoch_train_loss=3.0744235720239566
Epoch 922
Epoch 922 :: Batch 0/1
Batch Loss = 3.090779836026448
922, epoch_train_loss=3.090779836026448
Epoch 923
Epoch 923 :: Batch 0/1
Batch Loss = 3.087210843544558
923, epoch_train_loss=3.087210843544558
Epoch 924
Epoch 924 :: Batch 0/1
Batch Loss = 3.1072488726769394
924, epoch_train_loss=3.1072488726769394
Epoch 925
Epoch 925 :: Batch 0/1
Batch Loss = 3.0754901755143575
925, epoch_train_loss=3.0754901755143575
Epoch 926
Epoch 926 :: Batch 0/1
Batch Loss = 3.067419743869107
926, epoch_train_loss=3.067419743869107
Epoch 927
Epoch 927 :: Batch 0/1
Batch Loss = 3.081042420652094
927, epoch_train_loss=3.081042420652094
Epoch 928
Epoch 928 :: Batch 0/1
Batch Loss = 3.0958581849644844
928, epoch_train_loss=3.0958581849644844
Epoch 929
Epoch 929 :: Batch 0/1
Batch Loss = 3.164924122973243
929, epoch_train_loss=3.164924122973243
Epoch 930
Epoch 930 :: Batch 0/1
Batch Loss = 3.0770014542680384
930, epoch_train_loss=3.0770014542680384
Epoch 931
Epoch 931 :: Batch 0/1
Batch Loss = 3.3518204787860784
931, epoch_train_loss=3.3518204787860784
Epoch 932
Epoch 932 :: Batch 0/1
Batch Loss = 3.475409124407612
932, epoch_train_loss=3.475409124407612
Epoch 933
Epoch 933 :: Batch 0/1
Batch Loss = 3.445073171765956
933, epoch_train_loss=3.445073171765956
Epoch 934
Epoch 934 :: Batch 0/1
Batch Loss = 3.3810446108525607
934, epoch_train_loss=3.3810446108525607
Epoch 935
Epoch 935 :: Batch 0/1
Batch Loss = 3.354914093460198
935, epoch_train_loss=3.354914093460198
Epoch 936
Epoch 936 :: Batch 0/1
Batch Loss = 3.358293421246916
936, epoch_train_loss=3.358293421246916
Epoch 937
Epoch 937 :: Batch 0/1
Batch Loss = 3.355798484648521
937, epoch_train_loss=3.355798484648521
Epoch 938
Epoch 938 :: Batch 0/1
Batch Loss = 3.3353880266984737
938, epoch_train_loss=3.3353880266984737
Epoch 939
Epoch 939 :: Batch 0/1
Batch Loss = 3.2994681091207654
939, epoch_train_loss=3.2994681091207654
Epoch 940
Epoch 940 :: Batch 0/1
Batch Loss = 3.290113658845807
940, epoch_train_loss=3.290113658845807
Epoch 941
Epoch 941 :: Batch 0/1
Batch Loss = 3.2635783779802088
941, epoch_train_loss=3.2635783779802088
Epoch 942
Epoch 942 :: Batch 0/1
Batch Loss = 3.263916860598218
942, epoch_train_loss=3.263916860598218
Epoch 943
Epoch 943 :: Batch 0/1
Batch Loss = 3.2361825229119283
943, epoch_train_loss=3.2361825229119283
Epoch 944
Epoch 944 :: Batch 0/1
Batch Loss = 3.219036816107707
944, epoch_train_loss=3.219036816107707
Epoch 945
Epoch 945 :: Batch 0/1
Batch Loss = 3.239325941517015
945, epoch_train_loss=3.239325941517015
Epoch 946
Epoch 946 :: Batch 0/1
Batch Loss = 3.261379221117852
946, epoch_train_loss=3.261379221117852
Epoch 947
Epoch 947 :: Batch 0/1
Batch Loss = 3.204252164797122
947, epoch_train_loss=3.204252164797122
Epoch 948
Epoch 948 :: Batch 0/1
Batch Loss = 3.390816466228615
948, epoch_train_loss=3.390816466228615
Epoch 949
Epoch 949 :: Batch 0/1
Batch Loss = 3.461241213845184
949, epoch_train_loss=3.461241213845184
Epoch 950
Epoch 950 :: Batch 0/1
Batch Loss = 3.5497383117280426
950, epoch_train_loss=3.5497383117280426
Epoch 951
Epoch 951 :: Batch 0/1
Batch Loss = 3.3287569962682024
951, epoch_train_loss=3.3287569962682024
Epoch 952
Epoch 952 :: Batch 0/1
Batch Loss = 3.304651037918439
952, epoch_train_loss=3.304651037918439
Epoch 953
Epoch 953 :: Batch 0/1
Batch Loss = 3.2698001427903716
953, epoch_train_loss=3.2698001427903716
Epoch 954
Epoch 954 :: Batch 0/1
Batch Loss = 3.278775030740857
954, epoch_train_loss=3.278775030740857
Epoch 955
Epoch 955 :: Batch 0/1
Batch Loss = 3.225817396092813
955, epoch_train_loss=3.225817396092813
Epoch 956
Epoch 956 :: Batch 0/1
Batch Loss = 3.2824189504514436
956, epoch_train_loss=3.2824189504514436
Epoch 957
Epoch 957 :: Batch 0/1
Batch Loss = 3.2425199179912383
957, epoch_train_loss=3.2425199179912383
Epoch 958
Epoch 958 :: Batch 0/1
Batch Loss = 3.2704483043069863
958, epoch_train_loss=3.2704483043069863
Epoch 959
Epoch 959 :: Batch 0/1
Batch Loss = 3.233246734420666
959, epoch_train_loss=3.233246734420666
Epoch 960
Epoch 960 :: Batch 0/1
Batch Loss = 3.2619912363308265
960, epoch_train_loss=3.2619912363308265
Epoch 961
Epoch 961 :: Batch 0/1
Batch Loss = 3.2347918460342875
961, epoch_train_loss=3.2347918460342875
Epoch 962
Epoch 962 :: Batch 0/1
Batch Loss = 3.23445642799792
962, epoch_train_loss=3.23445642799792
Epoch 963
Epoch 963 :: Batch 0/1
Batch Loss = 3.2435016207608833
963, epoch_train_loss=3.2435016207608833
Epoch 964
Epoch 964 :: Batch 0/1
Batch Loss = 3.243809608294352
964, epoch_train_loss=3.243809608294352
Epoch 965
Epoch 965 :: Batch 0/1
Batch Loss = 3.231557999853378
965, epoch_train_loss=3.231557999853378
Epoch 966
Epoch 966 :: Batch 0/1
Batch Loss = 3.2154418825622324
966, epoch_train_loss=3.2154418825622324
Epoch 967
Epoch 967 :: Batch 0/1
Batch Loss = 3.2291163373607956
967, epoch_train_loss=3.2291163373607956
Epoch 968
Epoch 968 :: Batch 0/1
Batch Loss = 3.217238854475335
968, epoch_train_loss=3.217238854475335
Epoch 969
Epoch 969 :: Batch 0/1
Batch Loss = 3.213915079435348
969, epoch_train_loss=3.213915079435348
Epoch 970
Epoch 970 :: Batch 0/1
Batch Loss = 3.2039306191775783
970, epoch_train_loss=3.2039306191775783
Epoch 971
Epoch 971 :: Batch 0/1
Batch Loss = 3.2108455473120823
971, epoch_train_loss=3.2108455473120823
Epoch 972
Epoch 972 :: Batch 0/1
Batch Loss = 3.195912105761637
972, epoch_train_loss=3.195912105761637
Epoch 973
Epoch 973 :: Batch 0/1
Batch Loss = 3.197044228923691
973, epoch_train_loss=3.197044228923691
Epoch 974
Epoch 974 :: Batch 0/1
Batch Loss = 3.1974854106258306
974, epoch_train_loss=3.1974854106258306
Epoch 975
Epoch 975 :: Batch 0/1
Batch Loss = 3.1973703963761495
975, epoch_train_loss=3.1973703963761495
Epoch 976
Epoch 976 :: Batch 0/1
Batch Loss = 3.19313612948639
976, epoch_train_loss=3.19313612948639
Epoch 977
Epoch 977 :: Batch 0/1
Batch Loss = 3.1940892985369507
977, epoch_train_loss=3.1940892985369507
Epoch 978
Epoch 978 :: Batch 0/1
Batch Loss = 3.1950081699849253
978, epoch_train_loss=3.1950081699849253
Epoch 979
Epoch 979 :: Batch 0/1
Batch Loss = 3.191404089974157
979, epoch_train_loss=3.191404089974157
Epoch 980
Epoch 980 :: Batch 0/1
Batch Loss = 3.188654663203393
980, epoch_train_loss=3.188654663203393
Epoch 981
Epoch 981 :: Batch 0/1
Batch Loss = 3.1886870849242794
981, epoch_train_loss=3.1886870849242794
Epoch 982
Epoch 982 :: Batch 0/1
Batch Loss = 3.188713768330895
982, epoch_train_loss=3.188713768330895
Epoch 983
Epoch 983 :: Batch 0/1
Batch Loss = 3.186559591103321
983, epoch_train_loss=3.186559591103321
Epoch 984
Epoch 984 :: Batch 0/1
Batch Loss = 3.187773426012395
984, epoch_train_loss=3.187773426012395
Epoch 985
Epoch 985 :: Batch 0/1
Batch Loss = 3.1864743735789878
985, epoch_train_loss=3.1864743735789878
Epoch 986
Epoch 986 :: Batch 0/1
Batch Loss = 3.1850239634387
986, epoch_train_loss=3.1850239634387
Epoch 987
Epoch 987 :: Batch 0/1
Batch Loss = 3.185864549184055
987, epoch_train_loss=3.185864549184055
Epoch 988
Epoch 988 :: Batch 0/1
Batch Loss = 3.1826134175739145
988, epoch_train_loss=3.1826134175739145
Epoch 989
Epoch 989 :: Batch 0/1
Batch Loss = 3.1815003417795915
989, epoch_train_loss=3.1815003417795915
Epoch 990
Epoch 990 :: Batch 0/1
Batch Loss = 3.181145346472763
990, epoch_train_loss=3.181145346472763
Epoch 991
Epoch 991 :: Batch 0/1
Batch Loss = 3.179266373946308
991, epoch_train_loss=3.179266373946308
Epoch 992
Epoch 992 :: Batch 0/1
Batch Loss = 3.1790485628969307
992, epoch_train_loss=3.1790485628969307
Epoch 993
Epoch 993 :: Batch 0/1
Batch Loss = 3.1778210898850827
993, epoch_train_loss=3.1778210898850827
Epoch 994
Epoch 994 :: Batch 0/1
Batch Loss = 3.1767536739228794
994, epoch_train_loss=3.1767536739228794
Epoch 995
Epoch 995 :: Batch 0/1
Batch Loss = 3.1768458974908906
995, epoch_train_loss=3.1768458974908906
Epoch 996
Epoch 996 :: Batch 0/1
Batch Loss = 3.1759589250545894
996, epoch_train_loss=3.1759589250545894
Epoch 997
Epoch 997 :: Batch 0/1
Batch Loss = 3.1747710161944704
997, epoch_train_loss=3.1747710161944704
Epoch 998
Epoch 998 :: Batch 0/1
Batch Loss = 3.174074955826755
998, epoch_train_loss=3.174074955826755
Epoch 999
Epoch 999 :: Batch 0/1
Batch Loss = 3.173079365477706
999, epoch_train_loss=3.173079365477706
Epoch 1000
Epoch 1000 :: Batch 0/1
Batch Loss = 3.1727440024498827
1000, epoch_train_loss=3.1727440024498827
Epoch 1001
Epoch 1001 :: Batch 0/1
Batch Loss = 3.1722719585860717
1001, epoch_train_loss=3.1722719585860717
Epoch 1002
Epoch 1002 :: Batch 0/1
Batch Loss = 3.171346955533847
1002, epoch_train_loss=3.171346955533847
Epoch 1003
Epoch 1003 :: Batch 0/1
Batch Loss = 3.170991733250367
1003, epoch_train_loss=3.170991733250367
Epoch 1004
Epoch 1004 :: Batch 0/1
Batch Loss = 3.1702748970829866
1004, epoch_train_loss=3.1702748970829866
Epoch 1005
Epoch 1005 :: Batch 0/1
Batch Loss = 3.169806558400292
1005, epoch_train_loss=3.169806558400292
Epoch 1006
Epoch 1006 :: Batch 0/1
Batch Loss = 3.169267023372512
1006, epoch_train_loss=3.169267023372512
Epoch 1007
Epoch 1007 :: Batch 0/1
Batch Loss = 3.1683053010899163
1007, epoch_train_loss=3.1683053010899163
Epoch 1008
Epoch 1008 :: Batch 0/1
Batch Loss = 3.167877965866028
1008, epoch_train_loss=3.167877965866028
Epoch 1009
Epoch 1009 :: Batch 0/1
Batch Loss = 3.1671089348683923
1009, epoch_train_loss=3.1671089348683923
Epoch 1010
Epoch 1010 :: Batch 0/1
Batch Loss = 3.1666016173560934
1010, epoch_train_loss=3.1666016173560934
Epoch 1011
Epoch 1011 :: Batch 0/1
Batch Loss = 3.1660465400867834
1011, epoch_train_loss=3.1660465400867834
Epoch 1012
Epoch 1012 :: Batch 0/1
Batch Loss = 3.165517124535239
1012, epoch_train_loss=3.165517124535239
Epoch 1013
Epoch 1013 :: Batch 0/1
Batch Loss = 3.1650163074398256
1013, epoch_train_loss=3.1650163074398256
Epoch 1014
Epoch 1014 :: Batch 0/1
Batch Loss = 3.1644096806745456
1014, epoch_train_loss=3.1644096806745456
Epoch 1015
Epoch 1015 :: Batch 0/1
Batch Loss = 3.164059710384782
1015, epoch_train_loss=3.164059710384782
Epoch 1016
Epoch 1016 :: Batch 0/1
Batch Loss = 3.163541773135235
1016, epoch_train_loss=3.163541773135235
Epoch 1017
Epoch 1017 :: Batch 0/1
Batch Loss = 3.163177394226988
1017, epoch_train_loss=3.163177394226988
Epoch 1018
Epoch 1018 :: Batch 0/1
Batch Loss = 3.1626199434395623
1018, epoch_train_loss=3.1626199434395623
Epoch 1019
Epoch 1019 :: Batch 0/1
Batch Loss = 3.162170954935187
1019, epoch_train_loss=3.162170954935187
Epoch 1020
Epoch 1020 :: Batch 0/1
Batch Loss = 3.161667526049853
1020, epoch_train_loss=3.161667526049853
Epoch 1021
Epoch 1021 :: Batch 0/1
Batch Loss = 3.1612666611135185
1021, epoch_train_loss=3.1612666611135185
Epoch 1022
Epoch 1022 :: Batch 0/1
Batch Loss = 3.1608396302383523
1022, epoch_train_loss=3.1608396302383523
Epoch 1023
Epoch 1023 :: Batch 0/1
Batch Loss = 3.160405871399853
1023, epoch_train_loss=3.160405871399853
Epoch 1024
Epoch 1024 :: Batch 0/1
Batch Loss = 3.1599528943770547
1024, epoch_train_loss=3.1599528943770547
Epoch 1025
Epoch 1025 :: Batch 0/1
Batch Loss = 3.1594970119219274
1025, epoch_train_loss=3.1594970119219274
Epoch 1026
Epoch 1026 :: Batch 0/1
Batch Loss = 3.1590480084900308
1026, epoch_train_loss=3.1590480084900308
Epoch 1027
Epoch 1027 :: Batch 0/1
Batch Loss = 3.158569256570545
1027, epoch_train_loss=3.158569256570545
Epoch 1028
Epoch 1028 :: Batch 0/1
Batch Loss = 3.1581140929147518
1028, epoch_train_loss=3.1581140929147518
Epoch 1029
Epoch 1029 :: Batch 0/1
Batch Loss = 3.1576545981200317
1029, epoch_train_loss=3.1576545981200317
Epoch 1030
Epoch 1030 :: Batch 0/1
Batch Loss = 3.1572186339259694
1030, epoch_train_loss=3.1572186339259694
Epoch 1031
Epoch 1031 :: Batch 0/1
Batch Loss = 3.1567818416331233
1031, epoch_train_loss=3.1567818416331233
Epoch 1032
Epoch 1032 :: Batch 0/1
Batch Loss = 3.1563661873101703
1032, epoch_train_loss=3.1563661873101703
Epoch 1033
Epoch 1033 :: Batch 0/1
Batch Loss = 3.1559559247413698
1033, epoch_train_loss=3.1559559247413698
Epoch 1034
Epoch 1034 :: Batch 0/1
Batch Loss = 3.155556853399855
1034, epoch_train_loss=3.155556853399855
Epoch 1035
Epoch 1035 :: Batch 0/1
Batch Loss = 3.1551626434086035
1035, epoch_train_loss=3.1551626434086035
Epoch 1036
Epoch 1036 :: Batch 0/1
Batch Loss = 3.154769573242515
1036, epoch_train_loss=3.154769573242515
Epoch 1037
Epoch 1037 :: Batch 0/1
Batch Loss = 3.1543993058273188
1037, epoch_train_loss=3.1543993058273188
Epoch 1038
Epoch 1038 :: Batch 0/1
Batch Loss = 3.154040056292191
1038, epoch_train_loss=3.154040056292191
Epoch 1039
Epoch 1039 :: Batch 0/1
Batch Loss = 3.1537327493790017
1039, epoch_train_loss=3.1537327493790017
Epoch 1040
Epoch 1040 :: Batch 0/1
Batch Loss = 3.153474162453398
1040, epoch_train_loss=3.153474162453398
Epoch 1041
Epoch 1041 :: Batch 0/1
Batch Loss = 3.1534142761823776
1041, epoch_train_loss=3.1534142761823776
Epoch 1042
Epoch 1042 :: Batch 0/1
Batch Loss = 3.1536591185150034
1042, epoch_train_loss=3.1536591185150034
Epoch 1043
Epoch 1043 :: Batch 0/1
Batch Loss = 3.155027863693777
1043, epoch_train_loss=3.155027863693777
Epoch 1044
Epoch 1044 :: Batch 0/1
Batch Loss = 3.157801514574105
1044, epoch_train_loss=3.157801514574105
Epoch 1045
Epoch 1045 :: Batch 0/1
Batch Loss = 3.1665433357441617
1045, epoch_train_loss=3.1665433357441617
Epoch 1046
Epoch 1046 :: Batch 0/1
Batch Loss = 3.1675021473315543
1046, epoch_train_loss=3.1675021473315543
Epoch 1047
Epoch 1047 :: Batch 0/1
Batch Loss = 3.1749366790306226
1047, epoch_train_loss=3.1749366790306226
Epoch 1048
Epoch 1048 :: Batch 0/1
Batch Loss = 3.1543595766905295
1048, epoch_train_loss=3.1543595766905295
Epoch 1049
Epoch 1049 :: Batch 0/1
Batch Loss = 3.1523266828333627
1049, epoch_train_loss=3.1523266828333627
Epoch 1050
Epoch 1050 :: Batch 0/1
Batch Loss = 3.163757708149272
1050, epoch_train_loss=3.163757708149272
Epoch 1051
Epoch 1051 :: Batch 0/1
Batch Loss = 3.1562619132355967
1051, epoch_train_loss=3.1562619132355967
Epoch 1052
Epoch 1052 :: Batch 0/1
Batch Loss = 3.149977393886372
1052, epoch_train_loss=3.149977393886372
Epoch 1053
Epoch 1053 :: Batch 0/1
Batch Loss = 3.152681965131018
1053, epoch_train_loss=3.152681965131018
Epoch 1054
Epoch 1054 :: Batch 0/1
Batch Loss = 3.1545181391653943
1054, epoch_train_loss=3.1545181391653943
Epoch 1055
Epoch 1055 :: Batch 0/1
Batch Loss = 3.1515365276831777
1055, epoch_train_loss=3.1515365276831777
Epoch 1056
Epoch 1056 :: Batch 0/1
Batch Loss = 3.1489034030507357
1056, epoch_train_loss=3.1489034030507357
Epoch 1057
Epoch 1057 :: Batch 0/1
Batch Loss = 3.1516869614554244
1057, epoch_train_loss=3.1516869614554244
Epoch 1058
Epoch 1058 :: Batch 0/1
Batch Loss = 3.152890151104454
1058, epoch_train_loss=3.152890151104454
Epoch 1059
Epoch 1059 :: Batch 0/1
Batch Loss = 3.1485526274302025
1059, epoch_train_loss=3.1485526274302025
Epoch 1060
Epoch 1060 :: Batch 0/1
Batch Loss = 3.148651117806339
1060, epoch_train_loss=3.148651117806339
Epoch 1061
Epoch 1061 :: Batch 0/1
Batch Loss = 3.151531880354595
1061, epoch_train_loss=3.151531880354595
Epoch 1062
Epoch 1062 :: Batch 0/1
Batch Loss = 3.1494946909652763
1062, epoch_train_loss=3.1494946909652763
Epoch 1063
Epoch 1063 :: Batch 0/1
Batch Loss = 3.146912274879499
1063, epoch_train_loss=3.146912274879499
Epoch 1064
Epoch 1064 :: Batch 0/1
Batch Loss = 3.1476192489040784
1064, epoch_train_loss=3.1476192489040784
Epoch 1065
Epoch 1065 :: Batch 0/1
Batch Loss = 3.1489243217430536
1065, epoch_train_loss=3.1489243217430536
Epoch 1066
Epoch 1066 :: Batch 0/1
Batch Loss = 3.148528955605674
1066, epoch_train_loss=3.148528955605674
Epoch 1067
Epoch 1067 :: Batch 0/1
Batch Loss = 3.146257105299256
1067, epoch_train_loss=3.146257105299256
Epoch 1068
Epoch 1068 :: Batch 0/1
Batch Loss = 3.146071041040677
1068, epoch_train_loss=3.146071041040677
Epoch 1069
Epoch 1069 :: Batch 0/1
Batch Loss = 3.1473996116865877
1069, epoch_train_loss=3.1473996116865877
Epoch 1070
Epoch 1070 :: Batch 0/1
Batch Loss = 3.1470317756318966
1070, epoch_train_loss=3.1470317756318966
Epoch 1071
Epoch 1071 :: Batch 0/1
Batch Loss = 3.1458626675027648
1071, epoch_train_loss=3.1458626675027648
Epoch 1072
Epoch 1072 :: Batch 0/1
Batch Loss = 3.144837672826941
1072, epoch_train_loss=3.144837672826941
Epoch 1073
Epoch 1073 :: Batch 0/1
Batch Loss = 3.145056670917766
1073, epoch_train_loss=3.145056670917766
Epoch 1074
Epoch 1074 :: Batch 0/1
Batch Loss = 3.145678434339993
1074, epoch_train_loss=3.145678434339993
Epoch 1075
Epoch 1075 :: Batch 0/1
Batch Loss = 3.1452643288362383
1075, epoch_train_loss=3.1452643288362383
Epoch 1076
Epoch 1076 :: Batch 0/1
Batch Loss = 3.144447992934654
1076, epoch_train_loss=3.144447992934654
Epoch 1077
Epoch 1077 :: Batch 0/1
Batch Loss = 3.14380797388795
1077, epoch_train_loss=3.14380797388795
Epoch 1078
Epoch 1078 :: Batch 0/1
Batch Loss = 3.143867148349524
1078, epoch_train_loss=3.143867148349524
Epoch 1079
Epoch 1079 :: Batch 0/1
Batch Loss = 3.1442214824658423
1079, epoch_train_loss=3.1442214824658423
Epoch 1080
Epoch 1080 :: Batch 0/1
Batch Loss = 3.1439733129674385
1080, epoch_train_loss=3.1439733129674385
Epoch 1081
Epoch 1081 :: Batch 0/1
Batch Loss = 3.1434608074321586
1081, epoch_train_loss=3.1434608074321586
Epoch 1082
Epoch 1082 :: Batch 0/1
Batch Loss = 3.1429336973737216
1082, epoch_train_loss=3.1429336973737216
Epoch 1083
Epoch 1083 :: Batch 0/1
Batch Loss = 3.1427373568952954
1083, epoch_train_loss=3.1427373568952954
Epoch 1084
Epoch 1084 :: Batch 0/1
Batch Loss = 3.1428303958216084
1084, epoch_train_loss=3.1428303958216084
Epoch 1085
Epoch 1085 :: Batch 0/1
Batch Loss = 3.1428751309103284
1085, epoch_train_loss=3.1428751309103284
Epoch 1086
Epoch 1086 :: Batch 0/1
Batch Loss = 3.142756404361275
1086, epoch_train_loss=3.142756404361275
Epoch 1087
Epoch 1087 :: Batch 0/1
Batch Loss = 3.1423879187276245
1087, epoch_train_loss=3.1423879187276245
Epoch 1088
Epoch 1088 :: Batch 0/1
Batch Loss = 3.1420433098152643
1088, epoch_train_loss=3.1420433098152643
Epoch 1089
Epoch 1089 :: Batch 0/1
Batch Loss = 3.141719842408993
1089, epoch_train_loss=3.141719842408993
Epoch 1090
Epoch 1090 :: Batch 0/1
Batch Loss = 3.1415150942065355
1090, epoch_train_loss=3.1415150942065355
Epoch 1091
Epoch 1091 :: Batch 0/1
Batch Loss = 3.1414234856610745
1091, epoch_train_loss=3.1414234856610745
Epoch 1092
Epoch 1092 :: Batch 0/1
Batch Loss = 3.1413858627915974
1092, epoch_train_loss=3.1413858627915974
Epoch 1093
Epoch 1093 :: Batch 0/1
Batch Loss = 3.141403108784516
1093, epoch_train_loss=3.141403108784516
Epoch 1094
Epoch 1094 :: Batch 0/1
Batch Loss = 3.141407959536372
1094, epoch_train_loss=3.141407959536372
Epoch 1095
Epoch 1095 :: Batch 0/1
Batch Loss = 3.141540539422743
1095, epoch_train_loss=3.141540539422743
Epoch 1096
Epoch 1096 :: Batch 0/1
Batch Loss = 3.1415849926761337
1096, epoch_train_loss=3.1415849926761337
Epoch 1097
Epoch 1097 :: Batch 0/1
Batch Loss = 3.1419661536701544
1097, epoch_train_loss=3.1419661536701544
Epoch 1098
Epoch 1098 :: Batch 0/1
Batch Loss = 3.1420676505340976
1098, epoch_train_loss=3.1420676505340976
Epoch 1099
Epoch 1099 :: Batch 0/1
Batch Loss = 3.142928137172961
1099, epoch_train_loss=3.142928137172961
Epoch 1100
Epoch 1100 :: Batch 0/1
Batch Loss = 3.1428291410533045
1100, epoch_train_loss=3.1428291410533045
Epoch 1101
Epoch 1101 :: Batch 0/1
Batch Loss = 3.1438386248760404
1101, epoch_train_loss=3.1438386248760404
Epoch 1102
Epoch 1102 :: Batch 0/1
Batch Loss = 3.1429442067388393
1102, epoch_train_loss=3.1429442067388393
Epoch 1103
Epoch 1103 :: Batch 0/1
Batch Loss = 3.1427836780278904
1103, epoch_train_loss=3.1427836780278904
Epoch 1104
Epoch 1104 :: Batch 0/1
Batch Loss = 3.1413054526913564
1104, epoch_train_loss=3.1413054526913564
Epoch 1105
Epoch 1105 :: Batch 0/1
Batch Loss = 3.140219318069232
1105, epoch_train_loss=3.140219318069232
Epoch 1106
Epoch 1106 :: Batch 0/1
Batch Loss = 3.1394711054715256
1106, epoch_train_loss=3.1394711054715256
Epoch 1107
Epoch 1107 :: Batch 0/1
Batch Loss = 3.1393386436290953
1107, epoch_train_loss=3.1393386436290953
Epoch 1108
Epoch 1108 :: Batch 0/1
Batch Loss = 3.139673470833246
1108, epoch_train_loss=3.139673470833246
Epoch 1109
Epoch 1109 :: Batch 0/1
Batch Loss = 3.1400617755144635
1109, epoch_train_loss=3.1400617755144635
Epoch 1110
Epoch 1110 :: Batch 0/1
Batch Loss = 3.1404485014901016
1110, epoch_train_loss=3.1404485014901016
Epoch 1111
Epoch 1111 :: Batch 0/1
Batch Loss = 3.1403178817104527
1111, epoch_train_loss=3.1403178817104527
Epoch 1112
Epoch 1112 :: Batch 0/1
Batch Loss = 3.1401587234353414
1112, epoch_train_loss=3.1401587234353414
Epoch 1113
Epoch 1113 :: Batch 0/1
Batch Loss = 3.1396413556022127
1113, epoch_train_loss=3.1396413556022127
Epoch 1114
Epoch 1114 :: Batch 0/1
Batch Loss = 3.139233052760299
1114, epoch_train_loss=3.139233052760299
Epoch 1115
Epoch 1115 :: Batch 0/1
Batch Loss = 3.1387777286912555
1115, epoch_train_loss=3.1387777286912555
Epoch 1116
Epoch 1116 :: Batch 0/1
Batch Loss = 3.1384579081733137
1116, epoch_train_loss=3.1384579081733137
Epoch 1117
Epoch 1117 :: Batch 0/1
Batch Loss = 3.138186657478477
1117, epoch_train_loss=3.138186657478477
Epoch 1118
Epoch 1118 :: Batch 0/1
Batch Loss = 3.1379883128999824
1118, epoch_train_loss=3.1379883128999824
Epoch 1119
Epoch 1119 :: Batch 0/1
Batch Loss = 3.1378216131694643
1119, epoch_train_loss=3.1378216131694643
Epoch 1120
Epoch 1120 :: Batch 0/1
Batch Loss = 3.137680663063434
1120, epoch_train_loss=3.137680663063434
Epoch 1121
Epoch 1121 :: Batch 0/1
Batch Loss = 3.1375514997962743
1121, epoch_train_loss=3.1375514997962743
Epoch 1122
Epoch 1122 :: Batch 0/1
Batch Loss = 3.137430005048995
1122, epoch_train_loss=3.137430005048995
Epoch 1123
Epoch 1123 :: Batch 0/1
Batch Loss = 3.1373143291937713
1123, epoch_train_loss=3.1373143291937713
Epoch 1124
Epoch 1124 :: Batch 0/1
Batch Loss = 3.1372059959970944
1124, epoch_train_loss=3.1372059959970944
Epoch 1125
Epoch 1125 :: Batch 0/1
Batch Loss = 3.13710978769809
1125, epoch_train_loss=3.13710978769809
Epoch 1126
Epoch 1126 :: Batch 0/1
Batch Loss = 3.137039019513557
1126, epoch_train_loss=3.137039019513557
Epoch 1127
Epoch 1127 :: Batch 0/1
Batch Loss = 3.137027557479439
1127, epoch_train_loss=3.137027557479439
Epoch 1128
Epoch 1128 :: Batch 0/1
Batch Loss = 3.137167720206938
1128, epoch_train_loss=3.137167720206938
Epoch 1129
Epoch 1129 :: Batch 0/1
Batch Loss = 3.137701820516639
1129, epoch_train_loss=3.137701820516639
Epoch 1130
Epoch 1130 :: Batch 0/1
Batch Loss = 3.139311072219223
1130, epoch_train_loss=3.139311072219223
Epoch 1131
Epoch 1131 :: Batch 0/1
Batch Loss = 3.1439191596459675
1131, epoch_train_loss=3.1439191596459675
Epoch 1132
Epoch 1132 :: Batch 0/1
Batch Loss = 3.1563672647310725
1132, epoch_train_loss=3.1563672647310725
Epoch 1133
Epoch 1133 :: Batch 0/1
Batch Loss = 3.1880137928624066
1133, epoch_train_loss=3.1880137928624066
Epoch 1134
Epoch 1134 :: Batch 0/1
Batch Loss = 3.2359952324062227
1134, epoch_train_loss=3.2359952324062227
Epoch 1135
Epoch 1135 :: Batch 0/1
Batch Loss = 3.2531199559295305
1135, epoch_train_loss=3.2531199559295305
Epoch 1136
Epoch 1136 :: Batch 0/1
Batch Loss = 3.1902315446950227
1136, epoch_train_loss=3.1902315446950227
Epoch 1137
Epoch 1137 :: Batch 0/1
Batch Loss = 3.1626259524414317
1137, epoch_train_loss=3.1626259524414317
Epoch 1138
Epoch 1138 :: Batch 0/1
Batch Loss = 3.2389488252729546
1138, epoch_train_loss=3.2389488252729546
Epoch 1139
Epoch 1139 :: Batch 0/1
Batch Loss = 3.198891577039611
1139, epoch_train_loss=3.198891577039611
Epoch 1140
Epoch 1140 :: Batch 0/1
Batch Loss = 3.2770976859935055
1140, epoch_train_loss=3.2770976859935055
Epoch 1141
Epoch 1141 :: Batch 0/1
Batch Loss = 3.203093600533282
1141, epoch_train_loss=3.203093600533282
Epoch 1142
Epoch 1142 :: Batch 0/1
Batch Loss = 3.347440551331589
1142, epoch_train_loss=3.347440551331589
Epoch 1143
Epoch 1143 :: Batch 0/1
Batch Loss = 3.465905449237418
1143, epoch_train_loss=3.465905449237418
Epoch 1144
Epoch 1144 :: Batch 0/1
Batch Loss = 3.7886766043292193
1144, epoch_train_loss=3.7886766043292193
Epoch 1145
Epoch 1145 :: Batch 0/1
Batch Loss = 3.3746774026955215
1145, epoch_train_loss=3.3746774026955215
Epoch 1146
Epoch 1146 :: Batch 0/1
Batch Loss = 3.493896900251812
1146, epoch_train_loss=3.493896900251812
Epoch 1147
Epoch 1147 :: Batch 0/1
Batch Loss = 3.404263161662812
1147, epoch_train_loss=3.404263161662812
Epoch 1148
Epoch 1148 :: Batch 0/1
Batch Loss = 3.5139112547411915
1148, epoch_train_loss=3.5139112547411915
Epoch 1149
Epoch 1149 :: Batch 0/1
Batch Loss = 3.332921025424062
1149, epoch_train_loss=3.332921025424062
Epoch 1150
Epoch 1150 :: Batch 0/1
Batch Loss = 3.430780796088761
1150, epoch_train_loss=3.430780796088761
Epoch 1151
Epoch 1151 :: Batch 0/1
Batch Loss = 3.307090426212196
1151, epoch_train_loss=3.307090426212196
Epoch 1152
Epoch 1152 :: Batch 0/1
Batch Loss = 3.316074888483301
1152, epoch_train_loss=3.316074888483301
Epoch 1153
Epoch 1153 :: Batch 0/1
Batch Loss = 3.3064560569436034
1153, epoch_train_loss=3.3064560569436034
Epoch 1154
Epoch 1154 :: Batch 0/1
Batch Loss = 3.249365178514675
1154, epoch_train_loss=3.249365178514675
Epoch 1155
Epoch 1155 :: Batch 0/1
Batch Loss = 3.300859018176818
1155, epoch_train_loss=3.300859018176818
Epoch 1156
Epoch 1156 :: Batch 0/1
Batch Loss = 3.220052186365071
1156, epoch_train_loss=3.220052186365071
Epoch 1157
Epoch 1157 :: Batch 0/1
Batch Loss = 3.301471225176823
1157, epoch_train_loss=3.301471225176823
Epoch 1158
Epoch 1158 :: Batch 0/1
Batch Loss = 3.1987379631249846
1158, epoch_train_loss=3.1987379631249846
Epoch 1159
Epoch 1159 :: Batch 0/1
Batch Loss = 3.225921187457191
1159, epoch_train_loss=3.225921187457191
Epoch 1160
Epoch 1160 :: Batch 0/1
Batch Loss = 3.2197491453340796
1160, epoch_train_loss=3.2197491453340796
Epoch 1161
Epoch 1161 :: Batch 0/1
Batch Loss = 3.2015390001675623
1161, epoch_train_loss=3.2015390001675623
Epoch 1162
Epoch 1162 :: Batch 0/1
Batch Loss = 3.1866140029097614
1162, epoch_train_loss=3.1866140029097614
Epoch 1163
Epoch 1163 :: Batch 0/1
Batch Loss = 3.171657121722577
1163, epoch_train_loss=3.171657121722577
Epoch 1164
Epoch 1164 :: Batch 0/1
Batch Loss = 3.17842615681795
1164, epoch_train_loss=3.17842615681795
Epoch 1165
Epoch 1165 :: Batch 0/1
Batch Loss = 3.1745901185727563
1165, epoch_train_loss=3.1745901185727563
Epoch 1166
Epoch 1166 :: Batch 0/1
Batch Loss = 3.168437600676051
1166, epoch_train_loss=3.168437600676051
Epoch 1167
Epoch 1167 :: Batch 0/1
Batch Loss = 3.1630346019023046
1167, epoch_train_loss=3.1630346019023046
Epoch 1168
Epoch 1168 :: Batch 0/1
Batch Loss = 3.164119840054021
1168, epoch_train_loss=3.164119840054021
Epoch 1169
Epoch 1169 :: Batch 0/1
Batch Loss = 3.1665425839926082
1169, epoch_train_loss=3.1665425839926082
Epoch 1170
Epoch 1170 :: Batch 0/1
Batch Loss = 3.153193401905777
1170, epoch_train_loss=3.153193401905777
Epoch 1171
Epoch 1171 :: Batch 0/1
Batch Loss = 3.1571947672631477
1171, epoch_train_loss=3.1571947672631477
Epoch 1172
Epoch 1172 :: Batch 0/1
Batch Loss = 3.1578358753638276
1172, epoch_train_loss=3.1578358753638276
Epoch 1173
Epoch 1173 :: Batch 0/1
Batch Loss = 3.1578180014560258
1173, epoch_train_loss=3.1578180014560258
Epoch 1174
Epoch 1174 :: Batch 0/1
Batch Loss = 3.1489487017713964
1174, epoch_train_loss=3.1489487017713964
Epoch 1175
Epoch 1175 :: Batch 0/1
Batch Loss = 3.1424787976575925
1175, epoch_train_loss=3.1424787976575925
Epoch 1176
Epoch 1176 :: Batch 0/1
Batch Loss = 3.147882707750554
1176, epoch_train_loss=3.147882707750554
Epoch 1177
Epoch 1177 :: Batch 0/1
Batch Loss = 3.1416920291994974
1177, epoch_train_loss=3.1416920291994974
Epoch 1178
Epoch 1178 :: Batch 0/1
Batch Loss = 3.134441376654926
1178, epoch_train_loss=3.134441376654926
Epoch 1179
Epoch 1179 :: Batch 0/1
Batch Loss = 3.1342998933606974
1179, epoch_train_loss=3.1342998933606974
Epoch 1180
Epoch 1180 :: Batch 0/1
Batch Loss = 3.1341878016638645
1180, epoch_train_loss=3.1341878016638645
Epoch 1181
Epoch 1181 :: Batch 0/1
Batch Loss = 3.123991063650859
1181, epoch_train_loss=3.123991063650859
Epoch 1182
Epoch 1182 :: Batch 0/1
Batch Loss = 3.128140627418681
1182, epoch_train_loss=3.128140627418681
Epoch 1183
Epoch 1183 :: Batch 0/1
Batch Loss = 3.1204145291270273
1183, epoch_train_loss=3.1204145291270273
Epoch 1184
Epoch 1184 :: Batch 0/1
Batch Loss = 3.121274362233333
1184, epoch_train_loss=3.121274362233333
Epoch 1185
Epoch 1185 :: Batch 0/1
Batch Loss = 3.1166476007681094
1185, epoch_train_loss=3.1166476007681094
Epoch 1186
Epoch 1186 :: Batch 0/1
Batch Loss = 3.1163848790962585
1186, epoch_train_loss=3.1163848790962585
Epoch 1187
Epoch 1187 :: Batch 0/1
Batch Loss = 3.1132704156452107
1187, epoch_train_loss=3.1132704156452107
Epoch 1188
Epoch 1188 :: Batch 0/1
Batch Loss = 3.1124380546277126
1188, epoch_train_loss=3.1124380546277126
Epoch 1189
Epoch 1189 :: Batch 0/1
Batch Loss = 3.10974065184284
1189, epoch_train_loss=3.10974065184284
Epoch 1190
Epoch 1190 :: Batch 0/1
Batch Loss = 3.1103261515572087
1190, epoch_train_loss=3.1103261515572087
Epoch 1191
Epoch 1191 :: Batch 0/1
Batch Loss = 3.10551431324493
1191, epoch_train_loss=3.10551431324493
Epoch 1192
Epoch 1192 :: Batch 0/1
Batch Loss = 3.108951331864334
1192, epoch_train_loss=3.108951331864334
Epoch 1193
Epoch 1193 :: Batch 0/1
Batch Loss = 3.1028371402880452
1193, epoch_train_loss=3.1028371402880452
Epoch 1194
Epoch 1194 :: Batch 0/1
Batch Loss = 3.1052092898082635
1194, epoch_train_loss=3.1052092898082635
Epoch 1195
Epoch 1195 :: Batch 0/1
Batch Loss = 3.1034098231997898
1195, epoch_train_loss=3.1034098231997898
Epoch 1196
Epoch 1196 :: Batch 0/1
Batch Loss = 3.0998787518794106
1196, epoch_train_loss=3.0998787518794106
Epoch 1197
Epoch 1197 :: Batch 0/1
Batch Loss = 3.1024146300926096
1197, epoch_train_loss=3.1024146300926096
Epoch 1198
Epoch 1198 :: Batch 0/1
Batch Loss = 3.099690141976436
1198, epoch_train_loss=3.099690141976436
Epoch 1199
Epoch 1199 :: Batch 0/1
Batch Loss = 3.0967071658742307
1199, epoch_train_loss=3.0967071658742307
Epoch 1200
Epoch 1200 :: Batch 0/1
Batch Loss = 3.098284648039422
1200, epoch_train_loss=3.098284648039422
Epoch 1201
Epoch 1201 :: Batch 0/1
Batch Loss = 3.0981082236561597
1201, epoch_train_loss=3.0981082236561597
Epoch 1202
Epoch 1202 :: Batch 0/1
Batch Loss = 3.093576344277685
1202, epoch_train_loss=3.093576344277685
Epoch 1203
Epoch 1203 :: Batch 0/1
Batch Loss = 3.0927828700400473
1203, epoch_train_loss=3.0927828700400473
Epoch 1204
Epoch 1204 :: Batch 0/1
Batch Loss = 3.095066396320565
1204, epoch_train_loss=3.095066396320565
Epoch 1205
Epoch 1205 :: Batch 0/1
Batch Loss = 3.093535369188553
1205, epoch_train_loss=3.093535369188553
Epoch 1206
Epoch 1206 :: Batch 0/1
Batch Loss = 3.090898892008944
1206, epoch_train_loss=3.090898892008944
Epoch 1207
Epoch 1207 :: Batch 0/1
Batch Loss = 3.0882522513279436
1207, epoch_train_loss=3.0882522513279436
Epoch 1208
Epoch 1208 :: Batch 0/1
Batch Loss = 3.0873064148123395
1208, epoch_train_loss=3.0873064148123395
Epoch 1209
Epoch 1209 :: Batch 0/1
Batch Loss = 3.0881356678544085
1209, epoch_train_loss=3.0881356678544085
Epoch 1210
Epoch 1210 :: Batch 0/1
Batch Loss = 3.0895976219490002
1210, epoch_train_loss=3.0895976219490002
Epoch 1211
Epoch 1211 :: Batch 0/1
Batch Loss = 3.094217550755813
1211, epoch_train_loss=3.094217550755813
Epoch 1212
Epoch 1212 :: Batch 0/1
Batch Loss = 3.0983205846346915
1212, epoch_train_loss=3.0983205846346915
Epoch 1213
Epoch 1213 :: Batch 0/1
Batch Loss = 3.1126120965357025
1213, epoch_train_loss=3.1126120965357025
Epoch 1214
Epoch 1214 :: Batch 0/1
Batch Loss = 3.1049106353328955
1214, epoch_train_loss=3.1049106353328955
Epoch 1215
Epoch 1215 :: Batch 0/1
Batch Loss = 3.1097165941392677
1215, epoch_train_loss=3.1097165941392677
Epoch 1216
Epoch 1216 :: Batch 0/1
Batch Loss = 3.0951250320424233
1216, epoch_train_loss=3.0951250320424233
Epoch 1217
Epoch 1217 :: Batch 0/1
Batch Loss = 3.091573675838486
1217, epoch_train_loss=3.091573675838486
Epoch 1218
Epoch 1218 :: Batch 0/1
Batch Loss = 3.0872407717451376
1218, epoch_train_loss=3.0872407717451376
Epoch 1219
Epoch 1219 :: Batch 0/1
Batch Loss = 3.0887908670639477
1219, epoch_train_loss=3.0887908670639477
Epoch 1220
Epoch 1220 :: Batch 0/1
Batch Loss = 3.0906362283591644
1220, epoch_train_loss=3.0906362283591644
Epoch 1221
Epoch 1221 :: Batch 0/1
Batch Loss = 3.1047345661228474
1221, epoch_train_loss=3.1047345661228474
Epoch 1222
Epoch 1222 :: Batch 0/1
Batch Loss = 3.1074605862404554
1222, epoch_train_loss=3.1074605862404554
Epoch 1223
Epoch 1223 :: Batch 0/1
Batch Loss = 3.135524901188844
1223, epoch_train_loss=3.135524901188844
Epoch 1224
Epoch 1224 :: Batch 0/1
Batch Loss = 3.088500893136785
1224, epoch_train_loss=3.088500893136785
Epoch 1225
Epoch 1225 :: Batch 0/1
Batch Loss = 3.0768413552683396
1225, epoch_train_loss=3.0768413552683396
Epoch 1226
Epoch 1226 :: Batch 0/1
Batch Loss = 3.080191046367396
1226, epoch_train_loss=3.080191046367396
Epoch 1227
Epoch 1227 :: Batch 0/1
Batch Loss = 3.0971550387915316
1227, epoch_train_loss=3.0971550387915316
Epoch 1228
Epoch 1228 :: Batch 0/1
Batch Loss = 3.1478070215528287
1228, epoch_train_loss=3.1478070215528287
Epoch 1229
Epoch 1229 :: Batch 0/1
Batch Loss = 3.082255431575423
1229, epoch_train_loss=3.082255431575423
Epoch 1230
Epoch 1230 :: Batch 0/1
Batch Loss = 3.081498268305355
1230, epoch_train_loss=3.081498268305355
Epoch 1231
Epoch 1231 :: Batch 0/1
Batch Loss = 3.1310948625212602
1231, epoch_train_loss=3.1310948625212602
Epoch 1232
Epoch 1232 :: Batch 0/1
Batch Loss = 3.1022898254146574
1232, epoch_train_loss=3.1022898254146574
Epoch 1233
Epoch 1233 :: Batch 0/1
Batch Loss = 3.098888777331856
1233, epoch_train_loss=3.098888777331856
Epoch 1234
Epoch 1234 :: Batch 0/1
Batch Loss = 3.0823207759496247
1234, epoch_train_loss=3.0823207759496247
Epoch 1235
Epoch 1235 :: Batch 0/1
Batch Loss = 3.07811734311627
1235, epoch_train_loss=3.07811734311627
Epoch 1236
Epoch 1236 :: Batch 0/1
Batch Loss = 3.0758371991603486
1236, epoch_train_loss=3.0758371991603486
Epoch 1237
Epoch 1237 :: Batch 0/1
Batch Loss = 3.0747951135184084
1237, epoch_train_loss=3.0747951135184084
Epoch 1238
Epoch 1238 :: Batch 0/1
Batch Loss = 3.0749750098385737
1238, epoch_train_loss=3.0749750098385737
Epoch 1239
Epoch 1239 :: Batch 0/1
Batch Loss = 3.078267948982475
1239, epoch_train_loss=3.078267948982475
Epoch 1240
Epoch 1240 :: Batch 0/1
Batch Loss = 3.0850927576124145
1240, epoch_train_loss=3.0850927576124145
Epoch 1241
Epoch 1241 :: Batch 0/1
Batch Loss = 3.1149491431706764
1241, epoch_train_loss=3.1149491431706764
Epoch 1242
Epoch 1242 :: Batch 0/1
Batch Loss = 3.1023061078435483
1242, epoch_train_loss=3.1023061078435483
Epoch 1243
Epoch 1243 :: Batch 0/1
Batch Loss = 3.128062589732211
1243, epoch_train_loss=3.128062589732211
Epoch 1244
Epoch 1244 :: Batch 0/1
Batch Loss = 3.0792988696325048
1244, epoch_train_loss=3.0792988696325048
Epoch 1245
Epoch 1245 :: Batch 0/1
Batch Loss = 3.069994909876205
1245, epoch_train_loss=3.069994909876205
Epoch 1246
Epoch 1246 :: Batch 0/1
Batch Loss = 3.075610282773503
1246, epoch_train_loss=3.075610282773503
Epoch 1247
Epoch 1247 :: Batch 0/1
Batch Loss = 3.098160657770485
1247, epoch_train_loss=3.098160657770485
Epoch 1248
Epoch 1248 :: Batch 0/1
Batch Loss = 3.172582432703096
1248, epoch_train_loss=3.172582432703096
Epoch 1249
Epoch 1249 :: Batch 0/1
Batch Loss = 3.074901473057192
1249, epoch_train_loss=3.074901473057192
Epoch 1250
Epoch 1250 :: Batch 0/1
Batch Loss = 3.334230110960085
1250, epoch_train_loss=3.334230110960085
Epoch 1251
Epoch 1251 :: Batch 0/1
Batch Loss = 3.300147559257097
1251, epoch_train_loss=3.300147559257097
Epoch 1252
Epoch 1252 :: Batch 0/1
Batch Loss = 3.3766301189039813
1252, epoch_train_loss=3.3766301189039813
Epoch 1253
Epoch 1253 :: Batch 0/1
Batch Loss = 3.4242346352158153
1253, epoch_train_loss=3.4242346352158153
Epoch 1254
Epoch 1254 :: Batch 0/1
Batch Loss = 3.7026173548070442
1254, epoch_train_loss=3.7026173548070442
Epoch 1255
Epoch 1255 :: Batch 0/1
Batch Loss = 3.7114721727732656
1255, epoch_train_loss=3.7114721727732656
Epoch 1256
Epoch 1256 :: Batch 0/1
Batch Loss = 3.4461515742489075
1256, epoch_train_loss=3.4461515742489075
Epoch 1257
Epoch 1257 :: Batch 0/1
Batch Loss = 3.555902489504899
1257, epoch_train_loss=3.555902489504899
Epoch 1258
Epoch 1258 :: Batch 0/1
Batch Loss = 3.342206718017408
1258, epoch_train_loss=3.342206718017408
Epoch 1259
Epoch 1259 :: Batch 0/1
Batch Loss = 3.4398275025176437
1259, epoch_train_loss=3.4398275025176437
Epoch 1260
Epoch 1260 :: Batch 0/1
Batch Loss = 3.3176698748221587
1260, epoch_train_loss=3.3176698748221587
Epoch 1261
Epoch 1261 :: Batch 0/1
Batch Loss = 3.336977553677505
1261, epoch_train_loss=3.336977553677505
Epoch 1262
Epoch 1262 :: Batch 0/1
Batch Loss = 3.3480883402343586
1262, epoch_train_loss=3.3480883402343586
Epoch 1263
Epoch 1263 :: Batch 0/1
Batch Loss = 3.316420139906445
1263, epoch_train_loss=3.316420139906445
Epoch 1264
Epoch 1264 :: Batch 0/1
Batch Loss = 3.2816812100364503
1264, epoch_train_loss=3.2816812100364503
Epoch 1265
Epoch 1265 :: Batch 0/1
Batch Loss = 3.270799594225154
1265, epoch_train_loss=3.270799594225154
Epoch 1266
Epoch 1266 :: Batch 0/1
Batch Loss = 3.2374289263774427
1266, epoch_train_loss=3.2374289263774427
Epoch 1267
Epoch 1267 :: Batch 0/1
Batch Loss = 3.2659885507368283
1267, epoch_train_loss=3.2659885507368283
Epoch 1268
Epoch 1268 :: Batch 0/1
Batch Loss = 3.196421854313649
1268, epoch_train_loss=3.196421854313649
Epoch 1269
Epoch 1269 :: Batch 0/1
Batch Loss = 3.238344660656857
1269, epoch_train_loss=3.238344660656857
Epoch 1270
Epoch 1270 :: Batch 0/1
Batch Loss = 3.2339652388998297
1270, epoch_train_loss=3.2339652388998297
Epoch 1271
Epoch 1271 :: Batch 0/1
Batch Loss = 3.212063152315401
1271, epoch_train_loss=3.212063152315401
Epoch 1272
Epoch 1272 :: Batch 0/1
Batch Loss = 3.211724300396411
1272, epoch_train_loss=3.211724300396411
Epoch 1273
Epoch 1273 :: Batch 0/1
Batch Loss = 3.198460230482375
1273, epoch_train_loss=3.198460230482375
Epoch 1274
Epoch 1274 :: Batch 0/1
Batch Loss = 3.211040915676205
1274, epoch_train_loss=3.211040915676205
Epoch 1275
Epoch 1275 :: Batch 0/1
Batch Loss = 3.1805030546076316
1275, epoch_train_loss=3.1805030546076316
Epoch 1276
Epoch 1276 :: Batch 0/1
Batch Loss = 3.2023271687280914
1276, epoch_train_loss=3.2023271687280914
Epoch 1277
Epoch 1277 :: Batch 0/1
Batch Loss = 3.20128172685121
1277, epoch_train_loss=3.20128172685121
Epoch 1278
Epoch 1278 :: Batch 0/1
Batch Loss = 3.1770199094755256
1278, epoch_train_loss=3.1770199094755256
Epoch 1279
Epoch 1279 :: Batch 0/1
Batch Loss = 3.187077890900957
1279, epoch_train_loss=3.187077890900957
Epoch 1280
Epoch 1280 :: Batch 0/1
Batch Loss = 3.1857159280514344
1280, epoch_train_loss=3.1857159280514344
Epoch 1281
Epoch 1281 :: Batch 0/1
Batch Loss = 3.1768687781474823
1281, epoch_train_loss=3.1768687781474823
Epoch 1282
Epoch 1282 :: Batch 0/1
Batch Loss = 3.1743349276559267
1282, epoch_train_loss=3.1743349276559267
Epoch 1283
Epoch 1283 :: Batch 0/1
Batch Loss = 3.1754166542040587
1283, epoch_train_loss=3.1754166542040587
Epoch 1284
Epoch 1284 :: Batch 0/1
Batch Loss = 3.1780442901281267
1284, epoch_train_loss=3.1780442901281267
Epoch 1285
Epoch 1285 :: Batch 0/1
Batch Loss = 3.1650876418960525
1285, epoch_train_loss=3.1650876418960525
Epoch 1286
Epoch 1286 :: Batch 0/1
Batch Loss = 3.1675101259243204
1286, epoch_train_loss=3.1675101259243204
Epoch 1287
Epoch 1287 :: Batch 0/1
Batch Loss = 3.168492646213124
1287, epoch_train_loss=3.168492646213124
Epoch 1288
Epoch 1288 :: Batch 0/1
Batch Loss = 3.165839776642005
1288, epoch_train_loss=3.165839776642005
Epoch 1289
Epoch 1289 :: Batch 0/1
Batch Loss = 3.161682511650266
1289, epoch_train_loss=3.161682511650266
Epoch 1290
Epoch 1290 :: Batch 0/1
Batch Loss = 3.160274689353282
1290, epoch_train_loss=3.160274689353282
Epoch 1291
Epoch 1291 :: Batch 0/1
Batch Loss = 3.1634297053373572
1291, epoch_train_loss=3.1634297053373572
Epoch 1292
Epoch 1292 :: Batch 0/1
Batch Loss = 3.1584317340151777
1292, epoch_train_loss=3.1584317340151777
Epoch 1293
Epoch 1293 :: Batch 0/1
Batch Loss = 3.157947865118902
1293, epoch_train_loss=3.157947865118902
Epoch 1294
Epoch 1294 :: Batch 0/1
Batch Loss = 3.156275775353002
1294, epoch_train_loss=3.156275775353002
Epoch 1295
Epoch 1295 :: Batch 0/1
Batch Loss = 3.157753695626135
1295, epoch_train_loss=3.157753695626135
Epoch 1296
Epoch 1296 :: Batch 0/1
Batch Loss = 3.156084486821747
1296, epoch_train_loss=3.156084486821747
Epoch 1297
Epoch 1297 :: Batch 0/1
Batch Loss = 3.1534850382209507
1297, epoch_train_loss=3.1534850382209507
Epoch 1298
Epoch 1298 :: Batch 0/1
Batch Loss = 3.1545415713025995
1298, epoch_train_loss=3.1545415713025995
Epoch 1299
Epoch 1299 :: Batch 0/1
Batch Loss = 3.153180821108449
1299, epoch_train_loss=3.153180821108449
Epoch 1300
Epoch 1300 :: Batch 0/1
Batch Loss = 3.1533532590835667
1300, epoch_train_loss=3.1533532590835667
Epoch 1301
Epoch 1301 :: Batch 0/1
Batch Loss = 3.1509279714451863
1301, epoch_train_loss=3.1509279714451863
Epoch 1302
Epoch 1302 :: Batch 0/1
Batch Loss = 3.151409520765094
1302, epoch_train_loss=3.151409520765094
Epoch 1303
Epoch 1303 :: Batch 0/1
Batch Loss = 3.150586527423798
1303, epoch_train_loss=3.150586527423798
Epoch 1304
Epoch 1304 :: Batch 0/1
Batch Loss = 3.1503541230943353
1304, epoch_train_loss=3.1503541230943353
Epoch 1305
Epoch 1305 :: Batch 0/1
Batch Loss = 3.149083641762012
1305, epoch_train_loss=3.149083641762012
Epoch 1306
Epoch 1306 :: Batch 0/1
Batch Loss = 3.148648487827548
1306, epoch_train_loss=3.148648487827548
Epoch 1307
Epoch 1307 :: Batch 0/1
Batch Loss = 3.148442906351144
1307, epoch_train_loss=3.148442906351144
Epoch 1308
Epoch 1308 :: Batch 0/1
Batch Loss = 3.147692012723502
1308, epoch_train_loss=3.147692012723502
Epoch 1309
Epoch 1309 :: Batch 0/1
Batch Loss = 3.147391127087048
1309, epoch_train_loss=3.147391127087048
Epoch 1310
Epoch 1310 :: Batch 0/1
Batch Loss = 3.146608649837929
1310, epoch_train_loss=3.146608649837929
Epoch 1311
Epoch 1311 :: Batch 0/1
Batch Loss = 3.1466558655847026
1311, epoch_train_loss=3.1466558655847026
Epoch 1312
Epoch 1312 :: Batch 0/1
Batch Loss = 3.1458951759107117
1312, epoch_train_loss=3.1458951759107117
Epoch 1313
Epoch 1313 :: Batch 0/1
Batch Loss = 3.1457637457016303
1313, epoch_train_loss=3.1457637457016303
Epoch 1314
Epoch 1314 :: Batch 0/1
Batch Loss = 3.145053743278634
1314, epoch_train_loss=3.145053743278634
Epoch 1315
Epoch 1315 :: Batch 0/1
Batch Loss = 3.145014984998964
1315, epoch_train_loss=3.145014984998964
Epoch 1316
Epoch 1316 :: Batch 0/1
Batch Loss = 3.144370433419383
1316, epoch_train_loss=3.144370433419383
Epoch 1317
Epoch 1317 :: Batch 0/1
Batch Loss = 3.1442855878662264
1317, epoch_train_loss=3.1442855878662264
Epoch 1318
Epoch 1318 :: Batch 0/1
Batch Loss = 3.1437601019111185
1318, epoch_train_loss=3.1437601019111185
Epoch 1319
Epoch 1319 :: Batch 0/1
Batch Loss = 3.1436254292017787
1319, epoch_train_loss=3.1436254292017787
Epoch 1320
Epoch 1320 :: Batch 0/1
Batch Loss = 3.143111556974366
1320, epoch_train_loss=3.143111556974366
Epoch 1321
Epoch 1321 :: Batch 0/1
Batch Loss = 3.1429614110197797
1321, epoch_train_loss=3.1429614110197797
Epoch 1322
Epoch 1322 :: Batch 0/1
Batch Loss = 3.142561431143354
1322, epoch_train_loss=3.142561431143354
Epoch 1323
Epoch 1323 :: Batch 0/1
Batch Loss = 3.1423798558037164
1323, epoch_train_loss=3.1423798558037164
Epoch 1324
Epoch 1324 :: Batch 0/1
Batch Loss = 3.1419825943505795
1324, epoch_train_loss=3.1419825943505795
Epoch 1325
Epoch 1325 :: Batch 0/1
Batch Loss = 3.141840490022673
1325, epoch_train_loss=3.141840490022673
Epoch 1326
Epoch 1326 :: Batch 0/1
Batch Loss = 3.1415400761737664
1326, epoch_train_loss=3.1415400761737664
Epoch 1327
Epoch 1327 :: Batch 0/1
Batch Loss = 3.1413590454420652
1327, epoch_train_loss=3.1413590454420652
Epoch 1328
Epoch 1328 :: Batch 0/1
Batch Loss = 3.141024037944675
1328, epoch_train_loss=3.141024037944675
Epoch 1329
Epoch 1329 :: Batch 0/1
Batch Loss = 3.140876885542979
1329, epoch_train_loss=3.140876885542979
Epoch 1330
Epoch 1330 :: Batch 0/1
Batch Loss = 3.140626179398937
1330, epoch_train_loss=3.140626179398937
Epoch 1331
Epoch 1331 :: Batch 0/1
Batch Loss = 3.1404620252667175
1331, epoch_train_loss=3.1404620252667175
Epoch 1332
Epoch 1332 :: Batch 0/1
Batch Loss = 3.1401836461784423
1332, epoch_train_loss=3.1401836461784423
Epoch 1333
Epoch 1333 :: Batch 0/1
Batch Loss = 3.14005086274317
1333, epoch_train_loss=3.14005086274317
Epoch 1334
Epoch 1334 :: Batch 0/1
Batch Loss = 3.1398321568167185
1334, epoch_train_loss=3.1398321568167185
Epoch 1335
Epoch 1335 :: Batch 0/1
Batch Loss = 3.139675948448861
1335, epoch_train_loss=3.139675948448861
Epoch 1336
Epoch 1336 :: Batch 0/1
Batch Loss = 3.139432922114008
1336, epoch_train_loss=3.139432922114008
Epoch 1337
Epoch 1337 :: Batch 0/1
Batch Loss = 3.1392985892511653
1337, epoch_train_loss=3.1392985892511653
Epoch 1338
Epoch 1338 :: Batch 0/1
Batch Loss = 3.139110925520557
1338, epoch_train_loss=3.139110925520557
Epoch 1339
Epoch 1339 :: Batch 0/1
Batch Loss = 3.1389611814925664
1339, epoch_train_loss=3.1389611814925664
Epoch 1340
Epoch 1340 :: Batch 0/1
Batch Loss = 3.1387643190820733
1340, epoch_train_loss=3.1387643190820733
Epoch 1341
Epoch 1341 :: Batch 0/1
Batch Loss = 3.1386288640320505
1341, epoch_train_loss=3.1386288640320505
Epoch 1342
Epoch 1342 :: Batch 0/1
Batch Loss = 3.13847106292812
1342, epoch_train_loss=3.13847106292812
Epoch 1343
Epoch 1343 :: Batch 0/1
Batch Loss = 3.1383192887015134
1343, epoch_train_loss=3.1383192887015134
Epoch 1344
Epoch 1344 :: Batch 0/1
Batch Loss = 3.138159317508124
1344, epoch_train_loss=3.138159317508124
Epoch 1345
Epoch 1345 :: Batch 0/1
Batch Loss = 3.138020842374877
1345, epoch_train_loss=3.138020842374877
Epoch 1346
Epoch 1346 :: Batch 0/1
Batch Loss = 3.1378906344160793
1346, epoch_train_loss=3.1378906344160793
Epoch 1347
Epoch 1347 :: Batch 0/1
Batch Loss = 3.1377417812545985
1347, epoch_train_loss=3.1377417812545985
Epoch 1348
Epoch 1348 :: Batch 0/1
Batch Loss = 3.1376098803577843
1348, epoch_train_loss=3.1376098803577843
Epoch 1349
Epoch 1349 :: Batch 0/1
Batch Loss = 3.137472955265771
1349, epoch_train_loss=3.137472955265771
Epoch 1350
Epoch 1350 :: Batch 0/1
Batch Loss = 3.1373560925214874
1350, epoch_train_loss=3.1373560925214874
Epoch 1351
Epoch 1351 :: Batch 0/1
Batch Loss = 3.137216415288537
1351, epoch_train_loss=3.137216415288537
Epoch 1352
Epoch 1352 :: Batch 0/1
Batch Loss = 3.137096969998795
1352, epoch_train_loss=3.137096969998795
Epoch 1353
Epoch 1353 :: Batch 0/1
Batch Loss = 3.1369722921026932
1353, epoch_train_loss=3.1369722921026932
Epoch 1354
Epoch 1354 :: Batch 0/1
Batch Loss = 3.1368588368688393
1354, epoch_train_loss=3.1368588368688393
Epoch 1355
Epoch 1355 :: Batch 0/1
Batch Loss = 3.1367364806429534
1355, epoch_train_loss=3.1367364806429534
Epoch 1356
Epoch 1356 :: Batch 0/1
Batch Loss = 3.136620393428529
1356, epoch_train_loss=3.136620393428529
Epoch 1357
Epoch 1357 :: Batch 0/1
Batch Loss = 3.136510546761584
1357, epoch_train_loss=3.136510546761584
Epoch 1358
Epoch 1358 :: Batch 0/1
Batch Loss = 3.1363980594418304
1358, epoch_train_loss=3.1363980594418304
Epoch 1359
Epoch 1359 :: Batch 0/1
Batch Loss = 3.1362907705615695
1359, epoch_train_loss=3.1362907705615695
Epoch 1360
Epoch 1360 :: Batch 0/1
Batch Loss = 3.1361801010232746
1360, epoch_train_loss=3.1361801010232746
Epoch 1361
Epoch 1361 :: Batch 0/1
Batch Loss = 3.136079340528676
1361, epoch_train_loss=3.136079340528676
Epoch 1362
Epoch 1362 :: Batch 0/1
Batch Loss = 3.1359734959037544
1362, epoch_train_loss=3.1359734959037544
Epoch 1363
Epoch 1363 :: Batch 0/1
Batch Loss = 3.135872372793711
1363, epoch_train_loss=3.135872372793711
Epoch 1364
Epoch 1364 :: Batch 0/1
Batch Loss = 3.1357715225963037
1364, epoch_train_loss=3.1357715225963037
Epoch 1365
Epoch 1365 :: Batch 0/1
Batch Loss = 3.135673359653197
1365, epoch_train_loss=3.135673359653197
Epoch 1366
Epoch 1366 :: Batch 0/1
Batch Loss = 3.1355772601010687
1366, epoch_train_loss=3.1355772601010687
Epoch 1367
Epoch 1367 :: Batch 0/1
Batch Loss = 3.135479702262681
1367, epoch_train_loss=3.135479702262681
Epoch 1368
Epoch 1368 :: Batch 0/1
Batch Loss = 3.1353869786949633
1368, epoch_train_loss=3.1353869786949633
Epoch 1369
Epoch 1369 :: Batch 0/1
Batch Loss = 3.1352928345366506
1369, epoch_train_loss=3.1352928345366506
Epoch 1370
Epoch 1370 :: Batch 0/1
Batch Loss = 3.1352019177654853
1370, epoch_train_loss=3.1352019177654853
Epoch 1371
Epoch 1371 :: Batch 0/1
Batch Loss = 3.135111008534144
1371, epoch_train_loss=3.135111008534144
Epoch 1372
Epoch 1372 :: Batch 0/1
Batch Loss = 3.1350218649855472
1372, epoch_train_loss=3.1350218649855472
Epoch 1373
Epoch 1373 :: Batch 0/1
Batch Loss = 3.1349345065233516
1373, epoch_train_loss=3.1349345065233516
Epoch 1374
Epoch 1374 :: Batch 0/1
Batch Loss = 3.1348468079273695
1374, epoch_train_loss=3.1348468079273695
Epoch 1375
Epoch 1375 :: Batch 0/1
Batch Loss = 3.13476181189765
1375, epoch_train_loss=3.13476181189765
Epoch 1376
Epoch 1376 :: Batch 0/1
Batch Loss = 3.1346765197111406
1376, epoch_train_loss=3.1346765197111406
Epoch 1377
Epoch 1377 :: Batch 0/1
Batch Loss = 3.1345929745915044
1377, epoch_train_loss=3.1345929745915044
Epoch 1378
Epoch 1378 :: Batch 0/1
Batch Loss = 3.134510229734724
1378, epoch_train_loss=3.134510229734724
Epoch 1379
Epoch 1379 :: Batch 0/1
Batch Loss = 3.1344282433720116
1379, epoch_train_loss=3.1344282433720116
Epoch 1380
Epoch 1380 :: Batch 0/1
Batch Loss = 3.1343476930657492
1380, epoch_train_loss=3.1343476930657492
Epoch 1381
Epoch 1381 :: Batch 0/1
Batch Loss = 3.134267014052551
1381, epoch_train_loss=3.134267014052551
Epoch 1382
Epoch 1382 :: Batch 0/1
Batch Loss = 3.1341879723795065
1382, epoch_train_loss=3.1341879723795065
Epoch 1383
Epoch 1383 :: Batch 0/1
Batch Loss = 3.1341093769545774
1383, epoch_train_loss=3.1341093769545774
Epoch 1384
Epoch 1384 :: Batch 0/1
Batch Loss = 3.134031342838332
1384, epoch_train_loss=3.134031342838332
Epoch 1385
Epoch 1385 :: Batch 0/1
Batch Loss = 3.1339543072320835
1385, epoch_train_loss=3.1339543072320835
Epoch 1386
Epoch 1386 :: Batch 0/1
Batch Loss = 3.1338776337372414
1386, epoch_train_loss=3.1338776337372414
Epoch 1387
Epoch 1387 :: Batch 0/1
Batch Loss = 3.1338019633839176
1387, epoch_train_loss=3.1338019633839176
Epoch 1388
Epoch 1388 :: Batch 0/1
Batch Loss = 3.1337264173127495
1388, epoch_train_loss=3.1337264173127495
Epoch 1389
Epoch 1389 :: Batch 0/1
Batch Loss = 3.1336514705131266
1389, epoch_train_loss=3.1336514705131266
Epoch 1390
Epoch 1390 :: Batch 0/1
Batch Loss = 3.1335772942463964
1390, epoch_train_loss=3.1335772942463964
Epoch 1391
Epoch 1391 :: Batch 0/1
Batch Loss = 3.1335031833722606
1391, epoch_train_loss=3.1335031833722606
Epoch 1392
Epoch 1392 :: Batch 0/1
Batch Loss = 3.133429617537064
1392, epoch_train_loss=3.133429617537064
Epoch 1393
Epoch 1393 :: Batch 0/1
Batch Loss = 3.1333563136511624
1393, epoch_train_loss=3.1333563136511624
Epoch 1394
Epoch 1394 :: Batch 0/1
Batch Loss = 3.1332832951461245
1394, epoch_train_loss=3.1332832951461245
Epoch 1395
Epoch 1395 :: Batch 0/1
Batch Loss = 3.1332105484335737
1395, epoch_train_loss=3.1332105484335737
Epoch 1396
Epoch 1396 :: Batch 0/1
Batch Loss = 3.133137751608733
1396, epoch_train_loss=3.133137751608733
Epoch 1397
Epoch 1397 :: Batch 0/1
Batch Loss = 3.133065236445159
1397, epoch_train_loss=3.133065236445159
Epoch 1398
Epoch 1398 :: Batch 0/1
Batch Loss = 3.132992731639223
1398, epoch_train_loss=3.132992731639223
Epoch 1399
Epoch 1399 :: Batch 0/1
Batch Loss = 3.1329200811470934
1399, epoch_train_loss=3.1329200811470934
Epoch 1400
Epoch 1400 :: Batch 0/1
Batch Loss = 3.1328474261634076
1400, epoch_train_loss=3.1328474261634076
Epoch 1401
Epoch 1401 :: Batch 0/1
Batch Loss = 3.132774522711923
1401, epoch_train_loss=3.132774522711923
Epoch 1402
Epoch 1402 :: Batch 0/1
Batch Loss = 3.1327014048701742
1402, epoch_train_loss=3.1327014048701742
Epoch 1403
Epoch 1403 :: Batch 0/1
Batch Loss = 3.13262797197457
1403, epoch_train_loss=3.13262797197457
Epoch 1404
Epoch 1404 :: Batch 0/1
Batch Loss = 3.132554063686913
1404, epoch_train_loss=3.132554063686913
Epoch 1405
Epoch 1405 :: Batch 0/1
Batch Loss = 3.132479773796452
1405, epoch_train_loss=3.132479773796452
Epoch 1406
Epoch 1406 :: Batch 0/1
Batch Loss = 3.132404925129881
1406, epoch_train_loss=3.132404925129881
Epoch 1407
Epoch 1407 :: Batch 0/1
Batch Loss = 3.132329429604632
1407, epoch_train_loss=3.132329429604632
Epoch 1408
Epoch 1408 :: Batch 0/1
Batch Loss = 3.132253350757305
1408, epoch_train_loss=3.132253350757305
Epoch 1409
Epoch 1409 :: Batch 0/1
Batch Loss = 3.1321765826203514
1409, epoch_train_loss=3.1321765826203514
Epoch 1410
Epoch 1410 :: Batch 0/1
Batch Loss = 3.1320991265834985
1410, epoch_train_loss=3.1320991265834985
Epoch 1411
Epoch 1411 :: Batch 0/1
Batch Loss = 3.132021021445823
1411, epoch_train_loss=3.132021021445823
Epoch 1412
Epoch 1412 :: Batch 0/1
Batch Loss = 3.1319422720998276
1412, epoch_train_loss=3.1319422720998276
Epoch 1413
Epoch 1413 :: Batch 0/1
Batch Loss = 3.131862993812677
1413, epoch_train_loss=3.131862993812677
Epoch 1414
Epoch 1414 :: Batch 0/1
Batch Loss = 3.131783301503418
1414, epoch_train_loss=3.131783301503418
Epoch 1415
Epoch 1415 :: Batch 0/1
Batch Loss = 3.1317033645536094
1415, epoch_train_loss=3.1317033645536094
Epoch 1416
Epoch 1416 :: Batch 0/1
Batch Loss = 3.1316234576251847
1416, epoch_train_loss=3.1316234576251847
Epoch 1417
Epoch 1417 :: Batch 0/1
Batch Loss = 3.131543843015063
1417, epoch_train_loss=3.131543843015063
Epoch 1418
Epoch 1418 :: Batch 0/1
Batch Loss = 3.131464838258806
1418, epoch_train_loss=3.131464838258806
Epoch 1419
Epoch 1419 :: Batch 0/1
Batch Loss = 3.131386814491891
1419, epoch_train_loss=3.131386814491891
Epoch 1420
Epoch 1420 :: Batch 0/1
Batch Loss = 3.131310099431064
1420, epoch_train_loss=3.131310099431064
Epoch 1421
Epoch 1421 :: Batch 0/1
Batch Loss = 3.1312350224633674
1421, epoch_train_loss=3.1312350224633674
Epoch 1422
Epoch 1422 :: Batch 0/1
Batch Loss = 3.1311618821726395
1422, epoch_train_loss=3.1311618821726395
Epoch 1423
Epoch 1423 :: Batch 0/1
Batch Loss = 3.1310908479382844
1423, epoch_train_loss=3.1310908479382844
Epoch 1424
Epoch 1424 :: Batch 0/1
Batch Loss = 3.1310219799636987
1424, epoch_train_loss=3.1310219799636987
Epoch 1425
Epoch 1425 :: Batch 0/1
Batch Loss = 3.130955220622549
1425, epoch_train_loss=3.130955220622549
Epoch 1426
Epoch 1426 :: Batch 0/1
Batch Loss = 3.130890349338839
1426, epoch_train_loss=3.130890349338839
Epoch 1427
Epoch 1427 :: Batch 0/1
Batch Loss = 3.130827039101004
1427, epoch_train_loss=3.130827039101004
Epoch 1428
Epoch 1428 :: Batch 0/1
Batch Loss = 3.1307649026207347
1428, epoch_train_loss=3.1307649026207347
Epoch 1429
Epoch 1429 :: Batch 0/1
Batch Loss = 3.1307035027248937
1429, epoch_train_loss=3.1307035027248937
Epoch 1430
Epoch 1430 :: Batch 0/1
Batch Loss = 3.130642426528047
1430, epoch_train_loss=3.130642426528047
Epoch 1431
Epoch 1431 :: Batch 0/1
Batch Loss = 3.1305813362208657
1431, epoch_train_loss=3.1305813362208657
Epoch 1432
Epoch 1432 :: Batch 0/1
Batch Loss = 3.130519989405301
1432, epoch_train_loss=3.130519989405301
Epoch 1433
Epoch 1433 :: Batch 0/1
Batch Loss = 3.130458274239974
1433, epoch_train_loss=3.130458274239974
Epoch 1434
Epoch 1434 :: Batch 0/1
Batch Loss = 3.1303961871420283
1434, epoch_train_loss=3.1303961871420283
Epoch 1435
Epoch 1435 :: Batch 0/1
Batch Loss = 3.130333797282583
1435, epoch_train_loss=3.130333797282583
Epoch 1436
Epoch 1436 :: Batch 0/1
Batch Loss = 3.1302712318852426
1436, epoch_train_loss=3.1302712318852426
Epoch 1437
Epoch 1437 :: Batch 0/1
Batch Loss = 3.1302086291837052
1437, epoch_train_loss=3.1302086291837052
Epoch 1438
Epoch 1438 :: Batch 0/1
Batch Loss = 3.1301461083849453
1438, epoch_train_loss=3.1301461083849453
Epoch 1439
Epoch 1439 :: Batch 0/1
Batch Loss = 3.1300837732628213
1439, epoch_train_loss=3.1300837732628213
Epoch 1440
Epoch 1440 :: Batch 0/1
Batch Loss = 3.1300217009345372
1440, epoch_train_loss=3.1300217009345372
Epoch 1441
Epoch 1441 :: Batch 0/1
Batch Loss = 3.129959939247658
1441, epoch_train_loss=3.129959939247658
Epoch 1442
Epoch 1442 :: Batch 0/1
Batch Loss = 3.129898523922461
1442, epoch_train_loss=3.129898523922461
Epoch 1443
Epoch 1443 :: Batch 0/1
Batch Loss = 3.12983748326919
1443, epoch_train_loss=3.12983748326919
Epoch 1444
Epoch 1444 :: Batch 0/1
Batch Loss = 3.1297768370842634
1444, epoch_train_loss=3.1297768370842634
Epoch 1445
Epoch 1445 :: Batch 0/1
Batch Loss = 3.1297166003990706
1445, epoch_train_loss=3.1297166003990706
Epoch 1446
Epoch 1446 :: Batch 0/1
Batch Loss = 3.129656780919307
1446, epoch_train_loss=3.129656780919307
Epoch 1447
Epoch 1447 :: Batch 0/1
Batch Loss = 3.1295973735756863
1447, epoch_train_loss=3.1295973735756863
Epoch 1448
Epoch 1448 :: Batch 0/1
Batch Loss = 3.129538357239167
1448, epoch_train_loss=3.129538357239167
Epoch 1449
Epoch 1449 :: Batch 0/1
Batch Loss = 3.1294796948038544
1449, epoch_train_loss=3.1294796948038544
Epoch 1450
Epoch 1450 :: Batch 0/1
Batch Loss = 3.129421337529311
1450, epoch_train_loss=3.129421337529311
Epoch 1451
Epoch 1451 :: Batch 0/1
Batch Loss = 3.1293632281818966
1451, epoch_train_loss=3.1293632281818966
Epoch 1452
Epoch 1452 :: Batch 0/1
Batch Loss = 3.129305309100961
1452, epoch_train_loss=3.129305309100961
Epoch 1453
Epoch 1453 :: Batch 0/1
Batch Loss = 3.129247532543355
1453, epoch_train_loss=3.129247532543355
Epoch 1454
Epoch 1454 :: Batch 0/1
Batch Loss = 3.1291898624434165
1454, epoch_train_loss=3.1291898624434165
Epoch 1455
Epoch 1455 :: Batch 0/1
Batch Loss = 3.129132278110207
1455, epoch_train_loss=3.129132278110207
Epoch 1456
Epoch 1456 :: Batch 0/1
Batch Loss = 3.129074777722535
1456, epoch_train_loss=3.129074777722535
Epoch 1457
Epoch 1457 :: Batch 0/1
Batch Loss = 3.1290173719927155
1457, epoch_train_loss=3.1290173719927155
Epoch 1458
Epoch 1458 :: Batch 0/1
Batch Loss = 3.1289600799440915
1458, epoch_train_loss=3.1289600799440915
Epoch 1459
Epoch 1459 :: Batch 0/1
Batch Loss = 3.128902926338028
1459, epoch_train_loss=3.128902926338028
Epoch 1460
Epoch 1460 :: Batch 0/1
Batch Loss = 3.128845934286783
1460, epoch_train_loss=3.128845934286783
Epoch 1461
Epoch 1461 :: Batch 0/1
Batch Loss = 3.128789121278856
1461, epoch_train_loss=3.128789121278856
Epoch 1462
Epoch 1462 :: Batch 0/1
Batch Loss = 3.1287324981744367
1462, epoch_train_loss=3.1287324981744367
Epoch 1463
Epoch 1463 :: Batch 0/1
Batch Loss = 3.1286760676712335
1463, epoch_train_loss=3.1286760676712335
Epoch 1464
Epoch 1464 :: Batch 0/1
Batch Loss = 3.1286198252517634
1464, epoch_train_loss=3.1286198252517634
Epoch 1465
Epoch 1465 :: Batch 0/1
Batch Loss = 3.128563760615914
1465, epoch_train_loss=3.128563760615914
Epoch 1466
Epoch 1466 :: Batch 0/1
Batch Loss = 3.128507859565151
1466, epoch_train_loss=3.128507859565151
Epoch 1467
Epoch 1467 :: Batch 0/1
Batch Loss = 3.128452106917468
1467, epoch_train_loss=3.128452106917468
Epoch 1468
Epoch 1468 :: Batch 0/1
Batch Loss = 3.1283964869751206
1468, epoch_train_loss=3.1283964869751206
Epoch 1469
Epoch 1469 :: Batch 0/1
Batch Loss = 3.1283409846415515
1469, epoch_train_loss=3.1283409846415515
Epoch 1470
Epoch 1470 :: Batch 0/1
Batch Loss = 3.1282855872221225
1470, epoch_train_loss=3.1282855872221225
Epoch 1471
Epoch 1471 :: Batch 0/1
Batch Loss = 3.128230283423505
1471, epoch_train_loss=3.128230283423505
Epoch 1472
Epoch 1472 :: Batch 0/1
Batch Loss = 3.128175063528249
1472, epoch_train_loss=3.128175063528249
Epoch 1473
Epoch 1473 :: Batch 0/1
Batch Loss = 3.128119920047486
1473, epoch_train_loss=3.128119920047486
Epoch 1474
Epoch 1474 :: Batch 0/1
Batch Loss = 3.1280648466537193
1474, epoch_train_loss=3.1280648466537193
Epoch 1475
Epoch 1475 :: Batch 0/1
Batch Loss = 3.12800983852485
1475, epoch_train_loss=3.12800983852485
Epoch 1476
Epoch 1476 :: Batch 0/1
Batch Loss = 3.127954891983045
1476, epoch_train_loss=3.127954891983045
Epoch 1477
Epoch 1477 :: Batch 0/1
Batch Loss = 3.1279000039161673
1477, epoch_train_loss=3.1279000039161673
Epoch 1478
Epoch 1478 :: Batch 0/1
Batch Loss = 3.1278451722461993
1478, epoch_train_loss=3.1278451722461993
Epoch 1479
Epoch 1479 :: Batch 0/1
Batch Loss = 3.127790394951637
1479, epoch_train_loss=3.127790394951637
Epoch 1480
Epoch 1480 :: Batch 0/1
Batch Loss = 3.1277356697609178
1480, epoch_train_loss=3.1277356697609178
Epoch 1481
Epoch 1481 :: Batch 0/1
Batch Loss = 3.127680994271289
1481, epoch_train_loss=3.127680994271289
Epoch 1482
Epoch 1482 :: Batch 0/1
Batch Loss = 3.12762636495611
1482, epoch_train_loss=3.12762636495611
Epoch 1483
Epoch 1483 :: Batch 0/1
Batch Loss = 3.1275717774451657
1483, epoch_train_loss=3.1275717774451657
Epoch 1484
Epoch 1484 :: Batch 0/1
Batch Loss = 3.1275172264035573
1484, epoch_train_loss=3.1275172264035573
Epoch 1485
Epoch 1485 :: Batch 0/1
Batch Loss = 3.1274627053427713
1485, epoch_train_loss=3.1274627053427713
Epoch 1486
Epoch 1486 :: Batch 0/1
Batch Loss = 3.1274082074295997
1486, epoch_train_loss=3.1274082074295997
Epoch 1487
Epoch 1487 :: Batch 0/1
Batch Loss = 3.1273537252890855
1487, epoch_train_loss=3.1273537252890855
Epoch 1488
Epoch 1488 :: Batch 0/1
Batch Loss = 3.1272992513969378
1488, epoch_train_loss=3.1272992513969378
Epoch 1489
Epoch 1489 :: Batch 0/1
Batch Loss = 3.1272447786430626
1489, epoch_train_loss=3.1272447786430626
Epoch 1490
Epoch 1490 :: Batch 0/1
Batch Loss = 3.1271903001080443
1490, epoch_train_loss=3.1271903001080443
Epoch 1491
Epoch 1491 :: Batch 0/1
Batch Loss = 3.1271358095111235
1491, epoch_train_loss=3.1271358095111235
Epoch 1492
Epoch 1492 :: Batch 0/1
Batch Loss = 3.1270813010630594
1492, epoch_train_loss=3.1270813010630594
Epoch 1493
Epoch 1493 :: Batch 0/1
Batch Loss = 3.1270267692730003
1493, epoch_train_loss=3.1270267692730003
Epoch 1494
Epoch 1494 :: Batch 0/1
Batch Loss = 3.1269722090883945
1494, epoch_train_loss=3.1269722090883945
Epoch 1495
Epoch 1495 :: Batch 0/1
Batch Loss = 3.126917615353176
1495, epoch_train_loss=3.126917615353176
Epoch 1496
Epoch 1496 :: Batch 0/1
Batch Loss = 3.1268629828153336
1496, epoch_train_loss=3.1268629828153336
Epoch 1497
Epoch 1497 :: Batch 0/1
Batch Loss = 3.126808305994487
1497, epoch_train_loss=3.126808305994487
Epoch 1498
Epoch 1498 :: Batch 0/1
Batch Loss = 3.1267535789123024
1498, epoch_train_loss=3.1267535789123024
Epoch 1499
Epoch 1499 :: Batch 0/1
Batch Loss = 3.126698795347478
1499, epoch_train_loss=3.126698795347478
Epoch 1500
Epoch 1500 :: Batch 0/1
Batch Loss = 3.1266439486244293
1500, epoch_train_loss=3.1266439486244293
Epoch 1501
Epoch 1501 :: Batch 0/1
Batch Loss = 3.126589031760315
1501, epoch_train_loss=3.126589031760315
Epoch 1502
Epoch 1502 :: Batch 0/1
Batch Loss = 3.126534037615989
1502, epoch_train_loss=3.126534037615989
Epoch 1503
Epoch 1503 :: Batch 0/1
Batch Loss = 3.1264789588022874
1503, epoch_train_loss=3.1264789588022874
Epoch 1504
Epoch 1504 :: Batch 0/1
Batch Loss = 3.126423787980826
1504, epoch_train_loss=3.126423787980826
Epoch 1505
Epoch 1505 :: Batch 0/1
Batch Loss = 3.1263685177439107
1505, epoch_train_loss=3.1263685177439107
Epoch 1506
Epoch 1506 :: Batch 0/1
Batch Loss = 3.126313140714451
1506, epoch_train_loss=3.126313140714451
Epoch 1507
Epoch 1507 :: Batch 0/1
Batch Loss = 3.126257649600961
1507, epoch_train_loss=3.126257649600961
Epoch 1508
Epoch 1508 :: Batch 0/1
Batch Loss = 3.1262020370289285
1508, epoch_train_loss=3.1262020370289285
Epoch 1509
Epoch 1509 :: Batch 0/1
Batch Loss = 3.126146295697769
1509, epoch_train_loss=3.126146295697769
Epoch 1510
Epoch 1510 :: Batch 0/1
Batch Loss = 3.1260904181941984
1510, epoch_train_loss=3.1260904181941984
Epoch 1511
Epoch 1511 :: Batch 0/1
Batch Loss = 3.1260343970316633
1511, epoch_train_loss=3.1260343970316633
Epoch 1512
Epoch 1512 :: Batch 0/1
Batch Loss = 3.125978224645129
1512, epoch_train_loss=3.125978224645129
Epoch 1513
Epoch 1513 :: Batch 0/1
Batch Loss = 3.1259218932741453
1513, epoch_train_loss=3.1259218932741453
Epoch 1514
Epoch 1514 :: Batch 0/1
Batch Loss = 3.1258653950593946
1514, epoch_train_loss=3.1258653950593946
Epoch 1515
Epoch 1515 :: Batch 0/1
Batch Loss = 3.1258087219335935
1515, epoch_train_loss=3.1258087219335935
Epoch 1516
Epoch 1516 :: Batch 0/1
Batch Loss = 3.1257518656745584
1516, epoch_train_loss=3.1257518656745584
Epoch 1517
Epoch 1517 :: Batch 0/1
Batch Loss = 3.125694817920024
1517, epoch_train_loss=3.125694817920024
Epoch 1518
Epoch 1518 :: Batch 0/1
Batch Loss = 3.1256375701262282
1518, epoch_train_loss=3.1256375701262282
Epoch 1519
Epoch 1519 :: Batch 0/1
Batch Loss = 3.1255801136767465
1519, epoch_train_loss=3.1255801136767465
Epoch 1520
Epoch 1520 :: Batch 0/1
Batch Loss = 3.1255224397917316
1520, epoch_train_loss=3.1255224397917316
Epoch 1521
Epoch 1521 :: Batch 0/1
Batch Loss = 3.1254645396436485
1521, epoch_train_loss=3.1254645396436485
Epoch 1522
Epoch 1522 :: Batch 0/1
Batch Loss = 3.12540640429869
1522, epoch_train_loss=3.12540640429869
Epoch 1523
Epoch 1523 :: Batch 0/1
Batch Loss = 3.125348024763144
1523, epoch_train_loss=3.125348024763144
Epoch 1524
Epoch 1524 :: Batch 0/1
Batch Loss = 3.1252893919941283
1524, epoch_train_loss=3.1252893919941283
Epoch 1525
Epoch 1525 :: Batch 0/1
Batch Loss = 3.125230496843916
1525, epoch_train_loss=3.125230496843916
Epoch 1526
Epoch 1526 :: Batch 0/1
Batch Loss = 3.1251713301293282
1526, epoch_train_loss=3.1251713301293282
Epoch 1527
Epoch 1527 :: Batch 0/1
Batch Loss = 3.1251118825364745
1527, epoch_train_loss=3.1251118825364745
Epoch 1528
Epoch 1528 :: Batch 0/1
Batch Loss = 3.1250521446961956
1528, epoch_train_loss=3.1250521446961956
Epoch 1529
Epoch 1529 :: Batch 0/1
Batch Loss = 3.1249921071183397
1529, epoch_train_loss=3.1249921071183397
Epoch 1530
Epoch 1530 :: Batch 0/1
Batch Loss = 3.124931760230366
1530, epoch_train_loss=3.124931760230366
Epoch 1531
Epoch 1531 :: Batch 0/1
Batch Loss = 3.124871094386369
1531, epoch_train_loss=3.124871094386369
Epoch 1532
Epoch 1532 :: Batch 0/1
Batch Loss = 3.1248100998415533
1532, epoch_train_loss=3.1248100998415533
Epoch 1533
Epoch 1533 :: Batch 0/1
Batch Loss = 3.12474876683677
1533, epoch_train_loss=3.12474876683677
Epoch 1534
Epoch 1534 :: Batch 0/1
Batch Loss = 3.1246870855340583
1534, epoch_train_loss=3.1246870855340583
Epoch 1535
Epoch 1535 :: Batch 0/1
Batch Loss = 3.1246250461123006
1535, epoch_train_loss=3.1246250461123006
Epoch 1536
Epoch 1536 :: Batch 0/1
Batch Loss = 3.124562638712479
1536, epoch_train_loss=3.124562638712479
Epoch 1537
Epoch 1537 :: Batch 0/1
Batch Loss = 3.124499853484335
1537, epoch_train_loss=3.124499853484335
Epoch 1538
Epoch 1538 :: Batch 0/1
Batch Loss = 3.1244366805830963
1538, epoch_train_loss=3.1244366805830963
Epoch 1539
Epoch 1539 :: Batch 0/1
Batch Loss = 3.1243731101566468
1539, epoch_train_loss=3.1243731101566468
Epoch 1540
Epoch 1540 :: Batch 0/1
Batch Loss = 3.1243091323729364
1540, epoch_train_loss=3.1243091323729364
Epoch 1541
Epoch 1541 :: Batch 0/1
Batch Loss = 3.1242447373985263
1541, epoch_train_loss=3.1242447373985263
Epoch 1542
Epoch 1542 :: Batch 0/1
Batch Loss = 3.1241799154242025
1542, epoch_train_loss=3.1241799154242025
Epoch 1543
Epoch 1543 :: Batch 0/1
Batch Loss = 3.1241146566453897
1543, epoch_train_loss=3.1241146566453897
Epoch 1544
Epoch 1544 :: Batch 0/1
Batch Loss = 3.1240489513216367
1544, epoch_train_loss=3.1240489513216367
Epoch 1545
Epoch 1545 :: Batch 0/1
Batch Loss = 3.1239827897226013
1545, epoch_train_loss=3.1239827897226013
Epoch 1546
Epoch 1546 :: Batch 0/1
Batch Loss = 3.1239161622091243
1546, epoch_train_loss=3.1239161622091243
Epoch 1547
Epoch 1547 :: Batch 0/1
Batch Loss = 3.123849059194608
1547, epoch_train_loss=3.123849059194608
Epoch 1548
Epoch 1548 :: Batch 0/1
Batch Loss = 3.1237814711912484
1548, epoch_train_loss=3.1237814711912484
Epoch 1549
Epoch 1549 :: Batch 0/1
Batch Loss = 3.123713388785049
1549, epoch_train_loss=3.123713388785049
Epoch 1550
Epoch 1550 :: Batch 0/1
Batch Loss = 3.1236448026868864
1550, epoch_train_loss=3.1236448026868864
Epoch 1551
Epoch 1551 :: Batch 0/1
Batch Loss = 3.12357570369335
1551, epoch_train_loss=3.12357570369335
Epoch 1552
Epoch 1552 :: Batch 0/1
Batch Loss = 3.1235060827214096
1552, epoch_train_loss=3.1235060827214096
Epoch 1553
Epoch 1553 :: Batch 0/1
Batch Loss = 3.123435930790982
1553, epoch_train_loss=3.123435930790982
Epoch 1554
Epoch 1554 :: Batch 0/1
Batch Loss = 3.123365239075185
1554, epoch_train_loss=3.123365239075185
Epoch 1555
Epoch 1555 :: Batch 0/1
Batch Loss = 3.1232939988253308
1555, epoch_train_loss=3.1232939988253308
Epoch 1556
Epoch 1556 :: Batch 0/1
Batch Loss = 3.123222201471441
1556, epoch_train_loss=3.123222201471441
Epoch 1557
Epoch 1557 :: Batch 0/1
Batch Loss = 3.123149838545868
1557, epoch_train_loss=3.123149838545868
Epoch 1558
Epoch 1558 :: Batch 0/1
Batch Loss = 3.1230769017813
1558, epoch_train_loss=3.1230769017813
Epoch 1559
Epoch 1559 :: Batch 0/1
Batch Loss = 3.1230033829768598
1559, epoch_train_loss=3.1230033829768598
Epoch 1560
Epoch 1560 :: Batch 0/1
Batch Loss = 3.122929274200989
1560, epoch_train_loss=3.122929274200989
Epoch 1561
Epoch 1561 :: Batch 0/1
Batch Loss = 3.1228545675275936
1561, epoch_train_loss=3.1228545675275936
Epoch 1562
Epoch 1562 :: Batch 0/1
Batch Loss = 3.122779255402862
1562, epoch_train_loss=3.122779255402862
Epoch 1563
Epoch 1563 :: Batch 0/1
Batch Loss = 3.122703330124878
1563, epoch_train_loss=3.122703330124878
Epoch 1564
Epoch 1564 :: Batch 0/1
Batch Loss = 3.122626784568325
1564, epoch_train_loss=3.122626784568325
Epoch 1565
Epoch 1565 :: Batch 0/1
Batch Loss = 3.1225496111607276
1565, epoch_train_loss=3.1225496111607276
Epoch 1566
Epoch 1566 :: Batch 0/1
Batch Loss = 3.1224718034258356
1566, epoch_train_loss=3.1224718034258356
Epoch 1567
Epoch 1567 :: Batch 0/1
Batch Loss = 3.122393353783978
1567, epoch_train_loss=3.122393353783978
Epoch 1568
Epoch 1568 :: Batch 0/1
Batch Loss = 3.122314257098384
1568, epoch_train_loss=3.122314257098384
Epoch 1569
Epoch 1569 :: Batch 0/1
Batch Loss = 3.1222345060082692
1569, epoch_train_loss=3.1222345060082692
Epoch 1570
Epoch 1570 :: Batch 0/1
Batch Loss = 3.1221540998987813
1570, epoch_train_loss=3.1221540998987813
Epoch 1571
Epoch 1571 :: Batch 0/1
Batch Loss = 3.1220730358916735
1571, epoch_train_loss=3.1220730358916735
Epoch 1572
Epoch 1572 :: Batch 0/1
Batch Loss = 3.121991336201541
1572, epoch_train_loss=3.121991336201541
Epoch 1573
Epoch 1573 :: Batch 0/1
Batch Loss = 3.1219090409408037
1573, epoch_train_loss=3.1219090409408037
Epoch 1574
Epoch 1574 :: Batch 0/1
Batch Loss = 3.121826322267799
1574, epoch_train_loss=3.121826322267799
Epoch 1575
Epoch 1575 :: Batch 0/1
Batch Loss = 3.1217435891445433
1575, epoch_train_loss=3.1217435891445433
Epoch 1576
Epoch 1576 :: Batch 0/1
Batch Loss = 3.121662174571041
1576, epoch_train_loss=3.121662174571041
Epoch 1577
Epoch 1577 :: Batch 0/1
Batch Loss = 3.1215856919461964
1577, epoch_train_loss=3.1215856919461964
Epoch 1578
Epoch 1578 :: Batch 0/1
Batch Loss = 3.1215255850676145
1578, epoch_train_loss=3.1215255850676145
Epoch 1579
Epoch 1579 :: Batch 0/1
Batch Loss = 3.1215146316254163
1579, epoch_train_loss=3.1215146316254163
Epoch 1580
Epoch 1580 :: Batch 0/1
Batch Loss = 3.1216620142521894
1580, epoch_train_loss=3.1216620142521894
Epoch 1581
Epoch 1581 :: Batch 0/1
Batch Loss = 3.1222755008263543
1581, epoch_train_loss=3.1222755008263543
Epoch 1582
Epoch 1582 :: Batch 0/1
Batch Loss = 3.124515320867963
1582, epoch_train_loss=3.124515320867963
Epoch 1583
Epoch 1583 :: Batch 0/1
Batch Loss = 3.131056880587155
1583, epoch_train_loss=3.131056880587155
Epoch 1584
Epoch 1584 :: Batch 0/1
Batch Loss = 3.154857080530827
1584, epoch_train_loss=3.154857080530827
Epoch 1585
Epoch 1585 :: Batch 0/1
Batch Loss = 3.198039790441136
1585, epoch_train_loss=3.198039790441136
Epoch 1586
Epoch 1586 :: Batch 0/1
Batch Loss = 3.34052290731311
1586, epoch_train_loss=3.34052290731311
Epoch 1587
Epoch 1587 :: Batch 0/1
Batch Loss = 3.212568149292798
1587, epoch_train_loss=3.212568149292798
Epoch 1588
Epoch 1588 :: Batch 0/1
Batch Loss = 3.145165117088352
1588, epoch_train_loss=3.145165117088352
Epoch 1589
Epoch 1589 :: Batch 0/1
Batch Loss = 3.1287085514224526
1589, epoch_train_loss=3.1287085514224526
Epoch 1590
Epoch 1590 :: Batch 0/1
Batch Loss = 3.1761350410685365
1590, epoch_train_loss=3.1761350410685365
Epoch 1591
Epoch 1591 :: Batch 0/1
Batch Loss = 3.2092114772992613
1591, epoch_train_loss=3.2092114772992613
Epoch 1592
Epoch 1592 :: Batch 0/1
Batch Loss = 3.1270594675927934
1592, epoch_train_loss=3.1270594675927934
Epoch 1593
Epoch 1593 :: Batch 0/1
Batch Loss = 3.1835125143458067
1593, epoch_train_loss=3.1835125143458067
Epoch 1594
Epoch 1594 :: Batch 0/1
Batch Loss = 3.242284738453131
1594, epoch_train_loss=3.242284738453131
Epoch 1595
Epoch 1595 :: Batch 0/1
Batch Loss = 3.122665754456007
1595, epoch_train_loss=3.122665754456007
Epoch 1596
Epoch 1596 :: Batch 0/1
Batch Loss = 3.2531449251533777
1596, epoch_train_loss=3.2531449251533777
Epoch 1597
Epoch 1597 :: Batch 0/1
Batch Loss = 3.238878096735285
1597, epoch_train_loss=3.238878096735285
Epoch 1598
Epoch 1598 :: Batch 0/1
Batch Loss = 3.180606034436449
1598, epoch_train_loss=3.180606034436449
Epoch 1599
Epoch 1599 :: Batch 0/1
Batch Loss = 3.2740743404279673
1599, epoch_train_loss=3.2740743404279673
Epoch 1600
Epoch 1600 :: Batch 0/1
Batch Loss = 3.1309665802604245
1600, epoch_train_loss=3.1309665802604245
Epoch 1601
Epoch 1601 :: Batch 0/1
Batch Loss = 3.2034477603969305
1601, epoch_train_loss=3.2034477603969305
Epoch 1602
Epoch 1602 :: Batch 0/1
Batch Loss = 3.1256749853455577
1602, epoch_train_loss=3.1256749853455577
Epoch 1603
Epoch 1603 :: Batch 0/1
Batch Loss = 3.192667828784536
1603, epoch_train_loss=3.192667828784536
Epoch 1604
Epoch 1604 :: Batch 0/1
Batch Loss = 3.1453825325832296
1604, epoch_train_loss=3.1453825325832296
Epoch 1605
Epoch 1605 :: Batch 0/1
Batch Loss = 3.169499013289619
1605, epoch_train_loss=3.169499013289619
Epoch 1606
Epoch 1606 :: Batch 0/1
Batch Loss = 3.134625358112834
1606, epoch_train_loss=3.134625358112834
Epoch 1607
Epoch 1607 :: Batch 0/1
Batch Loss = 3.1591063126504686
1607, epoch_train_loss=3.1591063126504686
Epoch 1608
Epoch 1608 :: Batch 0/1
Batch Loss = 3.137170881641693
1608, epoch_train_loss=3.137170881641693
Epoch 1609
Epoch 1609 :: Batch 0/1
Batch Loss = 3.154642952730106
1609, epoch_train_loss=3.154642952730106
Epoch 1610
Epoch 1610 :: Batch 0/1
Batch Loss = 3.127619125533726
1610, epoch_train_loss=3.127619125533726
Epoch 1611
Epoch 1611 :: Batch 0/1
Batch Loss = 3.152923805258112
1611, epoch_train_loss=3.152923805258112
Epoch 1612
Epoch 1612 :: Batch 0/1
Batch Loss = 3.126551698680039
1612, epoch_train_loss=3.126551698680039
Epoch 1613
Epoch 1613 :: Batch 0/1
Batch Loss = 3.1474431049804816
1613, epoch_train_loss=3.1474431049804816
Epoch 1614
Epoch 1614 :: Batch 0/1
Batch Loss = 3.123745457517303
1614, epoch_train_loss=3.123745457517303
Epoch 1615
Epoch 1615 :: Batch 0/1
Batch Loss = 3.1446199204974064
1615, epoch_train_loss=3.1446199204974064
Epoch 1616
Epoch 1616 :: Batch 0/1
Batch Loss = 3.124219578408214
1616, epoch_train_loss=3.124219578408214
Epoch 1617
Epoch 1617 :: Batch 0/1
Batch Loss = 3.1382356772058517
1617, epoch_train_loss=3.1382356772058517
Epoch 1618
Epoch 1618 :: Batch 0/1
Batch Loss = 3.1264272586259008
1618, epoch_train_loss=3.1264272586259008
Epoch 1619
Epoch 1619 :: Batch 0/1
Batch Loss = 3.1332844035995433
1619, epoch_train_loss=3.1332844035995433
Epoch 1620
Epoch 1620 :: Batch 0/1
Batch Loss = 3.1273703096655656
1620, epoch_train_loss=3.1273703096655656
Epoch 1621
Epoch 1621 :: Batch 0/1
Batch Loss = 3.129119530395189
1621, epoch_train_loss=3.129119530395189
Epoch 1622
Epoch 1622 :: Batch 0/1
Batch Loss = 3.129757386991352
1622, epoch_train_loss=3.129757386991352
Epoch 1623
Epoch 1623 :: Batch 0/1
Batch Loss = 3.125126181499514
1623, epoch_train_loss=3.125126181499514
Epoch 1624
Epoch 1624 :: Batch 0/1
Batch Loss = 3.1301010671912466
1624, epoch_train_loss=3.1301010671912466
Epoch 1625
Epoch 1625 :: Batch 0/1
Batch Loss = 3.1237295041893383
1625, epoch_train_loss=3.1237295041893383
Epoch 1626
Epoch 1626 :: Batch 0/1
Batch Loss = 3.1291339043348474
1626, epoch_train_loss=3.1291339043348474
Epoch 1627
Epoch 1627 :: Batch 0/1
Batch Loss = 3.123247009314844
1627, epoch_train_loss=3.123247009314844
Epoch 1628
Epoch 1628 :: Batch 0/1
Batch Loss = 3.1279091632277938
1628, epoch_train_loss=3.1279091632277938
Epoch 1629
Epoch 1629 :: Batch 0/1
Batch Loss = 3.1233578075231625
1629, epoch_train_loss=3.1233578075231625
Epoch 1630
Epoch 1630 :: Batch 0/1
Batch Loss = 3.1260280822171653
1630, epoch_train_loss=3.1260280822171653
Epoch 1631
Epoch 1631 :: Batch 0/1
Batch Loss = 3.123959717459018
1631, epoch_train_loss=3.123959717459018
Epoch 1632
Epoch 1632 :: Batch 0/1
Batch Loss = 3.124375198529556
1632, epoch_train_loss=3.124375198529556
Epoch 1633
Epoch 1633 :: Batch 0/1
Batch Loss = 3.1241865618777322
1633, epoch_train_loss=3.1241865618777322
Epoch 1634
Epoch 1634 :: Batch 0/1
Batch Loss = 3.1232578522515557
1634, epoch_train_loss=3.1232578522515557
Epoch 1635
Epoch 1635 :: Batch 0/1
Batch Loss = 3.124190607151375
1635, epoch_train_loss=3.124190607151375
Epoch 1636
Epoch 1636 :: Batch 0/1
Batch Loss = 3.1223259544251496
1636, epoch_train_loss=3.1223259544251496
Epoch 1637
Epoch 1637 :: Batch 0/1
Batch Loss = 3.1238951506486843
1637, epoch_train_loss=3.1238951506486843
Epoch 1638
Epoch 1638 :: Batch 0/1
Batch Loss = 3.1219013985371245
1638, epoch_train_loss=3.1219013985371245
Epoch 1639
Epoch 1639 :: Batch 0/1
Batch Loss = 3.123348049665957
1639, epoch_train_loss=3.123348049665957
Epoch 1640
Epoch 1640 :: Batch 0/1
Batch Loss = 3.12165306984247
1640, epoch_train_loss=3.12165306984247
Epoch 1641
Epoch 1641 :: Batch 0/1
Batch Loss = 3.122756024704912
1641, epoch_train_loss=3.122756024704912
Epoch 1642
Epoch 1642 :: Batch 0/1
Batch Loss = 3.121446088440967
1642, epoch_train_loss=3.121446088440967
Epoch 1643
Epoch 1643 :: Batch 0/1
Batch Loss = 3.1221320400423793
1643, epoch_train_loss=3.1221320400423793
Epoch 1644
Epoch 1644 :: Batch 0/1
Batch Loss = 3.1213165704450314
1644, epoch_train_loss=3.1213165704450314
Epoch 1645
Epoch 1645 :: Batch 0/1
Batch Loss = 3.1215813208288923
1645, epoch_train_loss=3.1215813208288923
Epoch 1646
Epoch 1646 :: Batch 0/1
Batch Loss = 3.121112785829388
1646, epoch_train_loss=3.121112785829388
Epoch 1647
Epoch 1647 :: Batch 0/1
Batch Loss = 3.121093246244645
1647, epoch_train_loss=3.121093246244645
Epoch 1648
Epoch 1648 :: Batch 0/1
Batch Loss = 3.120885321591438
1648, epoch_train_loss=3.120885321591438
Epoch 1649
Epoch 1649 :: Batch 0/1
Batch Loss = 3.120661654311361
1649, epoch_train_loss=3.120661654311361
Epoch 1650
Epoch 1650 :: Batch 0/1
Batch Loss = 3.12061964208541
1650, epoch_train_loss=3.12061964208541
Epoch 1651
Epoch 1651 :: Batch 0/1
Batch Loss = 3.1202965919051877
1651, epoch_train_loss=3.1202965919051877
Epoch 1652
Epoch 1652 :: Batch 0/1
Batch Loss = 3.120326848171607
1652, epoch_train_loss=3.120326848171607
Epoch 1653
Epoch 1653 :: Batch 0/1
Batch Loss = 3.119944382311575
1653, epoch_train_loss=3.119944382311575
Epoch 1654
Epoch 1654 :: Batch 0/1
Batch Loss = 3.1200096874799175
1654, epoch_train_loss=3.1200096874799175
Epoch 1655
Epoch 1655 :: Batch 0/1
Batch Loss = 3.1196298342694893
1655, epoch_train_loss=3.1196298342694893
Epoch 1656
Epoch 1656 :: Batch 0/1
Batch Loss = 3.119689222361333
1656, epoch_train_loss=3.119689222361333
Epoch 1657
Epoch 1657 :: Batch 0/1
Batch Loss = 3.119317692814264
1657, epoch_train_loss=3.119317692814264
Epoch 1658
Epoch 1658 :: Batch 0/1
Batch Loss = 3.1193592769125256
1658, epoch_train_loss=3.1193592769125256
Epoch 1659
Epoch 1659 :: Batch 0/1
Batch Loss = 3.1190127653898605
1659, epoch_train_loss=3.1190127653898605
Epoch 1660
Epoch 1660 :: Batch 0/1
Batch Loss = 3.119022882578045
1660, epoch_train_loss=3.119022882578045
Epoch 1661
Epoch 1661 :: Batch 0/1
Batch Loss = 3.1187055911643533
1661, epoch_train_loss=3.1187055911643533
Epoch 1662
Epoch 1662 :: Batch 0/1
Batch Loss = 3.118688589597308
1662, epoch_train_loss=3.118688589597308
Epoch 1663
Epoch 1663 :: Batch 0/1
Batch Loss = 3.1183933790738116
1663, epoch_train_loss=3.1183933790738116
Epoch 1664
Epoch 1664 :: Batch 0/1
Batch Loss = 3.118345727175971
1664, epoch_train_loss=3.118345727175971
Epoch 1665
Epoch 1665 :: Batch 0/1
Batch Loss = 3.1180720100816397
1665, epoch_train_loss=3.1180720100816397
Epoch 1666
Epoch 1666 :: Batch 0/1
Batch Loss = 3.117998852211621
1666, epoch_train_loss=3.117998852211621
Epoch 1667
Epoch 1667 :: Batch 0/1
Batch Loss = 3.1177450148893677
1667, epoch_train_loss=3.1177450148893677
Epoch 1668
Epoch 1668 :: Batch 0/1
Batch Loss = 3.117644491959044
1668, epoch_train_loss=3.117644491959044
Epoch 1669
Epoch 1669 :: Batch 0/1
Batch Loss = 3.117403899263913
1669, epoch_train_loss=3.117403899263913
Epoch 1670
Epoch 1670 :: Batch 0/1
Batch Loss = 3.1172785974576223
1670, epoch_train_loss=3.1172785974576223
Epoch 1671
Epoch 1671 :: Batch 0/1
Batch Loss = 3.1170513271848743
1671, epoch_train_loss=3.1170513271848743
Epoch 1672
Epoch 1672 :: Batch 0/1
Batch Loss = 3.116901918427536
1672, epoch_train_loss=3.116901918427536
Epoch 1673
Epoch 1673 :: Batch 0/1
Batch Loss = 3.1166844655733357
1673, epoch_train_loss=3.1166844655733357
Epoch 1674
Epoch 1674 :: Batch 0/1
Batch Loss = 3.1165117066313237
1674, epoch_train_loss=3.1165117066313237
Epoch 1675
Epoch 1675 :: Batch 0/1
Batch Loss = 3.1163017310677326
1675, epoch_train_loss=3.1163017310677326
Epoch 1676
Epoch 1676 :: Batch 0/1
Batch Loss = 3.116107964657981
1676, epoch_train_loss=3.116107964657981
Epoch 1677
Epoch 1677 :: Batch 0/1
Batch Loss = 3.1159025174705897
1677, epoch_train_loss=3.1159025174705897
Epoch 1678
Epoch 1678 :: Batch 0/1
Batch Loss = 3.115689911762
1678, epoch_train_loss=3.115689911762
Epoch 1679
Epoch 1679 :: Batch 0/1
Batch Loss = 3.1154879744574595
1679, epoch_train_loss=3.1154879744574595
Epoch 1680
Epoch 1680 :: Batch 0/1
Batch Loss = 3.1152596247422233
1680, epoch_train_loss=3.1152596247422233
Epoch 1681
Epoch 1681 :: Batch 0/1
Batch Loss = 3.1150573153930745
1681, epoch_train_loss=3.1150573153930745
Epoch 1682
Epoch 1682 :: Batch 0/1
Batch Loss = 3.114819286128377
1682, epoch_train_loss=3.114819286128377
Epoch 1683
Epoch 1683 :: Batch 0/1
Batch Loss = 3.114615640570282
1683, epoch_train_loss=3.114615640570282
Epoch 1684
Epoch 1684 :: Batch 0/1
Batch Loss = 3.114373890230317
1684, epoch_train_loss=3.114373890230317
Epoch 1685
Epoch 1685 :: Batch 0/1
Batch Loss = 3.1141661210392173
1685, epoch_train_loss=3.1141661210392173
Epoch 1686
Epoch 1686 :: Batch 0/1
Batch Loss = 3.1139280122280955
1686, epoch_train_loss=3.1139280122280955
Epoch 1687
Epoch 1687 :: Batch 0/1
Batch Loss = 3.1137167139951383
1687, epoch_train_loss=3.1137167139951383
Epoch 1688
Epoch 1688 :: Batch 0/1
Batch Loss = 3.1134881892291095
1688, epoch_train_loss=3.1134881892291095
Epoch 1689
Epoch 1689 :: Batch 0/1
Batch Loss = 3.113274466172283
1689, epoch_train_loss=3.113274466172283
Epoch 1690
Epoch 1690 :: Batch 0/1
Batch Loss = 3.113060754572893
1690, epoch_train_loss=3.113060754572893
Epoch 1691
Epoch 1691 :: Batch 0/1
Batch Loss = 3.1128490140479084
1691, epoch_train_loss=3.1128490140479084
Epoch 1692
Epoch 1692 :: Batch 0/1
Batch Loss = 3.1126505603250028
1692, epoch_train_loss=3.1126505603250028
Epoch 1693
Epoch 1693 :: Batch 0/1
Batch Loss = 3.112447019612402
1693, epoch_train_loss=3.112447019612402
Epoch 1694
Epoch 1694 :: Batch 0/1
Batch Loss = 3.1122634754002982
1694, epoch_train_loss=3.1122634754002982
Epoch 1695
Epoch 1695 :: Batch 0/1
Batch Loss = 3.1120739951464236
1695, epoch_train_loss=3.1120739951464236
Epoch 1696
Epoch 1696 :: Batch 0/1
Batch Loss = 3.111902286953102
1696, epoch_train_loss=3.111902286953102
Epoch 1697
Epoch 1697 :: Batch 0/1
Batch Loss = 3.1117312119045266
1697, epoch_train_loss=3.1117312119045266
Epoch 1698
Epoch 1698 :: Batch 0/1
Batch Loss = 3.111569665843311
1698, epoch_train_loss=3.111569665843311
Epoch 1699
Epoch 1699 :: Batch 0/1
Batch Loss = 3.1114163530277357
1699, epoch_train_loss=3.1114163530277357
Epoch 1700
Epoch 1700 :: Batch 0/1
Batch Loss = 3.1112647730612277
1700, epoch_train_loss=3.1112647730612277
Epoch 1701
Epoch 1701 :: Batch 0/1
Batch Loss = 3.111125241954619
1701, epoch_train_loss=3.111125241954619
Epoch 1702
Epoch 1702 :: Batch 0/1
Batch Loss = 3.1109842564374874
1702, epoch_train_loss=3.1109842564374874
Epoch 1703
Epoch 1703 :: Batch 0/1
Batch Loss = 3.11085234696406
1703, epoch_train_loss=3.11085234696406
Epoch 1704
Epoch 1704 :: Batch 0/1
Batch Loss = 3.1107218092005966
1704, epoch_train_loss=3.1107218092005966
Epoch 1705
Epoch 1705 :: Batch 0/1
Batch Loss = 3.1105935182952558
1705, epoch_train_loss=3.1105935182952558
Epoch 1706
Epoch 1706 :: Batch 0/1
Batch Loss = 3.1104697843745663
1706, epoch_train_loss=3.1104697843745663
Epoch 1707
Epoch 1707 :: Batch 0/1
Batch Loss = 3.110343940189292
1707, epoch_train_loss=3.110343940189292
Epoch 1708
Epoch 1708 :: Batch 0/1
Batch Loss = 3.1102221659588714
1708, epoch_train_loss=3.1102221659588714
Epoch 1709
Epoch 1709 :: Batch 0/1
Batch Loss = 3.1100986800955797
1709, epoch_train_loss=3.1100986800955797
Epoch 1710
Epoch 1710 :: Batch 0/1
Batch Loss = 3.1099753201536324
1710, epoch_train_loss=3.1099753201536324
Epoch 1711
Epoch 1711 :: Batch 0/1
Batch Loss = 3.109852877064715
1711, epoch_train_loss=3.109852877064715
Epoch 1712
Epoch 1712 :: Batch 0/1
Batch Loss = 3.1097276048433384
1712, epoch_train_loss=3.1097276048433384
Epoch 1713
Epoch 1713 :: Batch 0/1
Batch Loss = 3.109603444908093
1713, epoch_train_loss=3.109603444908093
Epoch 1714
Epoch 1714 :: Batch 0/1
Batch Loss = 3.109477432068486
1714, epoch_train_loss=3.109477432068486
Epoch 1715
Epoch 1715 :: Batch 0/1
Batch Loss = 3.1093502438976492
1715, epoch_train_loss=3.1093502438976492
Epoch 1716
Epoch 1716 :: Batch 0/1
Batch Loss = 3.109223284203286
1716, epoch_train_loss=3.109223284203286
Epoch 1717
Epoch 1717 :: Batch 0/1
Batch Loss = 3.1090941515539927
1717, epoch_train_loss=3.1090941515539927
Epoch 1718
Epoch 1718 :: Batch 0/1
Batch Loss = 3.1089651603072865
1718, epoch_train_loss=3.1089651603072865
Epoch 1719
Epoch 1719 :: Batch 0/1
Batch Loss = 3.1088354547834562
1719, epoch_train_loss=3.1088354547834562
Epoch 1720
Epoch 1720 :: Batch 0/1
Batch Loss = 3.1087045825905815
1720, epoch_train_loss=3.1087045825905815
Epoch 1721
Epoch 1721 :: Batch 0/1
Batch Loss = 3.1085741757242014
1721, epoch_train_loss=3.1085741757242014
Epoch 1722
Epoch 1722 :: Batch 0/1
Batch Loss = 3.1084428282790393
1722, epoch_train_loss=3.1084428282790393
Epoch 1723
Epoch 1723 :: Batch 0/1
Batch Loss = 3.108311238662466
1723, epoch_train_loss=3.108311238662466
Epoch 1724
Epoch 1724 :: Batch 0/1
Batch Loss = 3.108179951882572
1724, epoch_train_loss=3.108179951882572
Epoch 1725
Epoch 1725 :: Batch 0/1
Batch Loss = 3.1080479288507727
1725, epoch_train_loss=3.1080479288507727
Epoch 1726
Epoch 1726 :: Batch 0/1
Batch Loss = 3.107916098081296
1726, epoch_train_loss=3.107916098081296
Epoch 1727
Epoch 1727 :: Batch 0/1
Batch Loss = 3.107784316468997
1727, epoch_train_loss=3.107784316468997
Epoch 1728
Epoch 1728 :: Batch 0/1
Batch Loss = 3.1076520199641924
1728, epoch_train_loss=3.1076520199641924
Epoch 1729
Epoch 1729 :: Batch 0/1
Batch Loss = 3.107519948696493
1729, epoch_train_loss=3.107519948696493
Epoch 1730
Epoch 1730 :: Batch 0/1
Batch Loss = 3.107387707044582
1730, epoch_train_loss=3.107387707044582
Epoch 1731
Epoch 1731 :: Batch 0/1
Batch Loss = 3.107255045478854
1731, epoch_train_loss=3.107255045478854
Epoch 1732
Epoch 1732 :: Batch 0/1
Batch Loss = 3.107122441079384
1732, epoch_train_loss=3.107122441079384
Epoch 1733
Epoch 1733 :: Batch 0/1
Batch Loss = 3.1069894906156756
1733, epoch_train_loss=3.1069894906156756
Epoch 1734
Epoch 1734 :: Batch 0/1
Batch Loss = 3.1068560826716407
1734, epoch_train_loss=3.1068560826716407
Epoch 1735
Epoch 1735 :: Batch 0/1
Batch Loss = 3.1067225238689313
1735, epoch_train_loss=3.1067225238689313
Epoch 1736
Epoch 1736 :: Batch 0/1
Batch Loss = 3.106588480846885
1736, epoch_train_loss=3.106588480846885
Epoch 1737
Epoch 1737 :: Batch 0/1
Batch Loss = 3.106453885331891
1737, epoch_train_loss=3.106453885331891
Epoch 1738
Epoch 1738 :: Batch 0/1
Batch Loss = 3.106318946394522
1738, epoch_train_loss=3.106318946394522
Epoch 1739
Epoch 1739 :: Batch 0/1
Batch Loss = 3.106183427087756
1739, epoch_train_loss=3.106183427087756
Epoch 1740
Epoch 1740 :: Batch 0/1
Batch Loss = 3.1060472541519535
1740, epoch_train_loss=3.1060472541519535
Epoch 1741
Epoch 1741 :: Batch 0/1
Batch Loss = 3.1059105926142707
1741, epoch_train_loss=3.1059105926142707
Epoch 1742
Epoch 1742 :: Batch 0/1
Batch Loss = 3.1057732906046853
1742, epoch_train_loss=3.1057732906046853
Epoch 1743
Epoch 1743 :: Batch 0/1
Batch Loss = 3.10563525869359
1743, epoch_train_loss=3.10563525869359
Epoch 1744
Epoch 1744 :: Batch 0/1
Batch Loss = 3.1054966306492275
1744, epoch_train_loss=3.1054966306492275
Epoch 1745
Epoch 1745 :: Batch 0/1
Batch Loss = 3.1053573337206255
1745, epoch_train_loss=3.1053573337206255
Epoch 1746
Epoch 1746 :: Batch 0/1
Batch Loss = 3.1052172654689496
1746, epoch_train_loss=3.1052172654689496
Epoch 1747
Epoch 1747 :: Batch 0/1
Batch Loss = 3.1050765239225124
1747, epoch_train_loss=3.1050765239225124
Epoch 1748
Epoch 1748 :: Batch 0/1
Batch Loss = 3.104935098668727
1748, epoch_train_loss=3.104935098668727
Epoch 1749
Epoch 1749 :: Batch 0/1
Batch Loss = 3.104792892466324
1749, epoch_train_loss=3.104792892466324
Epoch 1750
Epoch 1750 :: Batch 0/1
Batch Loss = 3.104649954557477
1750, epoch_train_loss=3.104649954557477
Epoch 1751
Epoch 1751 :: Batch 0/1
Batch Loss = 3.104506313917662
1751, epoch_train_loss=3.104506313917662
Epoch 1752
Epoch 1752 :: Batch 0/1
Batch Loss = 3.104361905112854
1752, epoch_train_loss=3.104361905112854
Epoch 1753
Epoch 1753 :: Batch 0/1
Batch Loss = 3.1042167256548376
1753, epoch_train_loss=3.1042167256548376
Epoch 1754
Epoch 1754 :: Batch 0/1
Batch Loss = 3.104070804477997
1754, epoch_train_loss=3.104070804477997
Epoch 1755
Epoch 1755 :: Batch 0/1
Batch Loss = 3.103924128066313
1755, epoch_train_loss=3.103924128066313
Epoch 1756
Epoch 1756 :: Batch 0/1
Batch Loss = 3.103776666485569
1756, epoch_train_loss=3.103776666485569
Epoch 1757
Epoch 1757 :: Batch 0/1
Batch Loss = 3.10362841417203
1757, epoch_train_loss=3.10362841417203
Epoch 1758
Epoch 1758 :: Batch 0/1
Batch Loss = 3.103479393109926
1758, epoch_train_loss=3.103479393109926
Epoch 1759
Epoch 1759 :: Batch 0/1
Batch Loss = 3.103329583177122
1759, epoch_train_loss=3.103329583177122
Epoch 1760
Epoch 1760 :: Batch 0/1
Batch Loss = 3.1031789488464447
1760, epoch_train_loss=3.1031789488464447
Epoch 1761
Epoch 1761 :: Batch 0/1
Batch Loss = 3.1030275055362253
1761, epoch_train_loss=3.1030275055362253
Epoch 1762
Epoch 1762 :: Batch 0/1
Batch Loss = 3.1028752509294395
1762, epoch_train_loss=3.1028752509294395
Epoch 1763
Epoch 1763 :: Batch 0/1
Batch Loss = 3.1027221605804054
1763, epoch_train_loss=3.1027221605804054
Epoch 1764
Epoch 1764 :: Batch 0/1
Batch Loss = 3.10256822397179
1764, epoch_train_loss=3.10256822397179
Epoch 1765
Epoch 1765 :: Batch 0/1
Batch Loss = 3.102413429399459
1765, epoch_train_loss=3.102413429399459
Epoch 1766
Epoch 1766 :: Batch 0/1
Batch Loss = 3.1022577800420734
1766, epoch_train_loss=3.1022577800420734
Epoch 1767
Epoch 1767 :: Batch 0/1
Batch Loss = 3.1021012631074645
1767, epoch_train_loss=3.1021012631074645
Epoch 1768
Epoch 1768 :: Batch 0/1
Batch Loss = 3.1019438539114716
1768, epoch_train_loss=3.1019438539114716
Epoch 1769
Epoch 1769 :: Batch 0/1
Batch Loss = 3.101785552870687
1769, epoch_train_loss=3.101785552870687
Epoch 1770
Epoch 1770 :: Batch 0/1
Batch Loss = 3.10162635010955
1770, epoch_train_loss=3.10162635010955
Epoch 1771
Epoch 1771 :: Batch 0/1
Batch Loss = 3.1014662389070096
1771, epoch_train_loss=3.1014662389070096
Epoch 1772
Epoch 1772 :: Batch 0/1
Batch Loss = 3.1013052093575704
1772, epoch_train_loss=3.1013052093575704
Epoch 1773
Epoch 1773 :: Batch 0/1
Batch Loss = 3.101143243150837
1773, epoch_train_loss=3.101143243150837
Epoch 1774
Epoch 1774 :: Batch 0/1
Batch Loss = 3.1009803404484586
1774, epoch_train_loss=3.1009803404484586
Epoch 1775
Epoch 1775 :: Batch 0/1
Batch Loss = 3.1008164900740938
1775, epoch_train_loss=3.1008164900740938
Epoch 1776
Epoch 1776 :: Batch 0/1
Batch Loss = 3.1006516861805347
1776, epoch_train_loss=3.1006516861805347
Epoch 1777
Epoch 1777 :: Batch 0/1
Batch Loss = 3.1004859212350295
1777, epoch_train_loss=3.1004859212350295
Epoch 1778
Epoch 1778 :: Batch 0/1
Batch Loss = 3.100319180432851
1778, epoch_train_loss=3.100319180432851
Epoch 1779
Epoch 1779 :: Batch 0/1
Batch Loss = 3.100151461598128
1779, epoch_train_loss=3.100151461598128
Epoch 1780
Epoch 1780 :: Batch 0/1
Batch Loss = 3.0999827514645304
1780, epoch_train_loss=3.0999827514645304
Epoch 1781
Epoch 1781 :: Batch 0/1
Batch Loss = 3.099813047607843
1781, epoch_train_loss=3.099813047607843
Epoch 1782
Epoch 1782 :: Batch 0/1
Batch Loss = 3.099642340086694
1782, epoch_train_loss=3.099642340086694
Epoch 1783
Epoch 1783 :: Batch 0/1
Batch Loss = 3.0994706196738906
1783, epoch_train_loss=3.0994706196738906
Epoch 1784
Epoch 1784 :: Batch 0/1
Batch Loss = 3.0992978804749325
1784, epoch_train_loss=3.0992978804749325
Epoch 1785
Epoch 1785 :: Batch 0/1
Batch Loss = 3.099124109248279
1785, epoch_train_loss=3.099124109248279
Epoch 1786
Epoch 1786 :: Batch 0/1
Batch Loss = 3.0989493029883626
1786, epoch_train_loss=3.0989493029883626
Epoch 1787
Epoch 1787 :: Batch 0/1
Batch Loss = 3.098773447074167
1787, epoch_train_loss=3.098773447074167
Epoch 1788
Epoch 1788 :: Batch 0/1
Batch Loss = 3.0985965389987653
1788, epoch_train_loss=3.0985965389987653
Epoch 1789
Epoch 1789 :: Batch 0/1
Batch Loss = 3.0984185646686098
1789, epoch_train_loss=3.0984185646686098
Epoch 1790
Epoch 1790 :: Batch 0/1
Batch Loss = 3.098239520030223
1790, epoch_train_loss=3.098239520030223
Epoch 1791
Epoch 1791 :: Batch 0/1
Batch Loss = 3.0980593919185333
1791, epoch_train_loss=3.0980593919185333
Epoch 1792
Epoch 1792 :: Batch 0/1
Batch Loss = 3.097878175120212
1792, epoch_train_loss=3.097878175120212
Epoch 1793
Epoch 1793 :: Batch 0/1
Batch Loss = 3.0976958572218005
1793, epoch_train_loss=3.0976958572218005
Epoch 1794
Epoch 1794 :: Batch 0/1
Batch Loss = 3.097512432162365
1794, epoch_train_loss=3.097512432162365
Epoch 1795
Epoch 1795 :: Batch 0/1
Batch Loss = 3.0973278877512613
1795, epoch_train_loss=3.0973278877512613
Epoch 1796
Epoch 1796 :: Batch 0/1
Batch Loss = 3.0971422186277886
1796, epoch_train_loss=3.0971422186277886
Epoch 1797
Epoch 1797 :: Batch 0/1
Batch Loss = 3.096955411467377
1797, epoch_train_loss=3.096955411467377
Epoch 1798
Epoch 1798 :: Batch 0/1
Batch Loss = 3.0967674635050533
1798, epoch_train_loss=3.0967674635050533
Epoch 1799
Epoch 1799 :: Batch 0/1
Batch Loss = 3.096578359400582
1799, epoch_train_loss=3.096578359400582
Epoch 1800
Epoch 1800 :: Batch 0/1
Batch Loss = 3.0963881058865352
1800, epoch_train_loss=3.0963881058865352
Epoch 1801
Epoch 1801 :: Batch 0/1
Batch Loss = 3.0961966879773954
1801, epoch_train_loss=3.0961966879773954
Epoch 1802
Epoch 1802 :: Batch 0/1
Batch Loss = 3.096004155986714
1802, epoch_train_loss=3.096004155986714
Epoch 1803
Epoch 1803 :: Batch 0/1
Batch Loss = 3.095810552034136
1803, epoch_train_loss=3.095810552034136
Epoch 1804
Epoch 1804 :: Batch 0/1
Batch Loss = 3.095616221702028
1804, epoch_train_loss=3.095616221702028
Epoch 1805
Epoch 1805 :: Batch 0/1
Batch Loss = 3.095421926887069
1805, epoch_train_loss=3.095421926887069
Epoch 1806
Epoch 1806 :: Batch 0/1
Batch Loss = 3.0952307659952
1806, epoch_train_loss=3.0952307659952
Epoch 1807
Epoch 1807 :: Batch 0/1
Batch Loss = 3.095052082155313
1807, epoch_train_loss=3.095052082155313
Epoch 1808
Epoch 1808 :: Batch 0/1
Batch Loss = 3.094920967252302
1808, epoch_train_loss=3.094920967252302
Epoch 1809
Epoch 1809 :: Batch 0/1
Batch Loss = 3.0949576676523236
1809, epoch_train_loss=3.0949576676523236
Epoch 1810
Epoch 1810 :: Batch 0/1
Batch Loss = 3.095639385410191
1810, epoch_train_loss=3.095639385410191
Epoch 1811
Epoch 1811 :: Batch 0/1
Batch Loss = 3.0986470950091722
1811, epoch_train_loss=3.0986470950091722
Epoch 1812
Epoch 1812 :: Batch 0/1
Batch Loss = 3.111505340569465
1812, epoch_train_loss=3.111505340569465
Epoch 1813
Epoch 1813 :: Batch 0/1
Batch Loss = 3.154712539035171
1813, epoch_train_loss=3.154712539035171
Epoch 1814
Epoch 1814 :: Batch 0/1
Batch Loss = 3.325860632742594
1814, epoch_train_loss=3.325860632742594
Epoch 1815
Epoch 1815 :: Batch 0/1
Batch Loss = 3.4112721110809265
1815, epoch_train_loss=3.4112721110809265
Epoch 1816
Epoch 1816 :: Batch 0/1
Batch Loss = 3.4890645861270175
1816, epoch_train_loss=3.4890645861270175
Epoch 1817
Epoch 1817 :: Batch 0/1
Batch Loss = 3.1227471306017285
1817, epoch_train_loss=3.1227471306017285
Epoch 1818
Epoch 1818 :: Batch 0/1
Batch Loss = 3.5822620893914165
1818, epoch_train_loss=3.5822620893914165
Epoch 1819
Epoch 1819 :: Batch 0/1
Batch Loss = 3.419337754338576
1819, epoch_train_loss=3.419337754338576
Epoch 1820
Epoch 1820 :: Batch 0/1
Batch Loss = 3.457194996150315
1820, epoch_train_loss=3.457194996150315
Epoch 1821
Epoch 1821 :: Batch 0/1
Batch Loss = 3.1840997580204267
1821, epoch_train_loss=3.1840997580204267
Epoch 1822
Epoch 1822 :: Batch 0/1
Batch Loss = 3.47967941395305
1822, epoch_train_loss=3.47967941395305
Epoch 1823
Epoch 1823 :: Batch 0/1
Batch Loss = 3.167584540752423
1823, epoch_train_loss=3.167584540752423
Epoch 1824
Epoch 1824 :: Batch 0/1
Batch Loss = 3.3353152722525876
1824, epoch_train_loss=3.3353152722525876
Epoch 1825
Epoch 1825 :: Batch 0/1
Batch Loss = 3.2629435609189903
1825, epoch_train_loss=3.2629435609189903
Epoch 1826
Epoch 1826 :: Batch 0/1
Batch Loss = 3.1844685955038776
1826, epoch_train_loss=3.1844685955038776
Epoch 1827
Epoch 1827 :: Batch 0/1
Batch Loss = 3.2511165977026737
1827, epoch_train_loss=3.2511165977026737
Epoch 1828
Epoch 1828 :: Batch 0/1
Batch Loss = 3.2008910981775984
1828, epoch_train_loss=3.2008910981775984
Epoch 1829
Epoch 1829 :: Batch 0/1
Batch Loss = 3.16963817655164
1829, epoch_train_loss=3.16963817655164
Epoch 1830
Epoch 1830 :: Batch 0/1
Batch Loss = 3.220139727963335
1830, epoch_train_loss=3.220139727963335
Epoch 1831
Epoch 1831 :: Batch 0/1
Batch Loss = 3.1940285645315796
1831, epoch_train_loss=3.1940285645315796
Epoch 1832
Epoch 1832 :: Batch 0/1
Batch Loss = 3.136718161018664
1832, epoch_train_loss=3.136718161018664
Epoch 1833
Epoch 1833 :: Batch 0/1
Batch Loss = 3.1936563754586804
1833, epoch_train_loss=3.1936563754586804
Epoch 1834
Epoch 1834 :: Batch 0/1
Batch Loss = 3.163963068492573
1834, epoch_train_loss=3.163963068492573
Epoch 1835
Epoch 1835 :: Batch 0/1
Batch Loss = 3.1225991969260147
1835, epoch_train_loss=3.1225991969260147
Epoch 1836
Epoch 1836 :: Batch 0/1
Batch Loss = 3.1665263446785765
1836, epoch_train_loss=3.1665263446785765
Epoch 1837
Epoch 1837 :: Batch 0/1
Batch Loss = 3.1571572675401143
1837, epoch_train_loss=3.1571572675401143
Epoch 1838
Epoch 1838 :: Batch 0/1
Batch Loss = 3.116551022730402
1838, epoch_train_loss=3.116551022730402
Epoch 1839
Epoch 1839 :: Batch 0/1
Batch Loss = 3.1425687179614585
1839, epoch_train_loss=3.1425687179614585
Epoch 1840
Epoch 1840 :: Batch 0/1
Batch Loss = 3.1456765256156785
1840, epoch_train_loss=3.1456765256156785
Epoch 1841
Epoch 1841 :: Batch 0/1
Batch Loss = 3.1111179890803426
1841, epoch_train_loss=3.1111179890803426
Epoch 1842
Epoch 1842 :: Batch 0/1
Batch Loss = 3.1263454748228243
1842, epoch_train_loss=3.1263454748228243
Epoch 1843
Epoch 1843 :: Batch 0/1
Batch Loss = 3.134604278977269
1843, epoch_train_loss=3.134604278977269
Epoch 1844
Epoch 1844 :: Batch 0/1
Batch Loss = 3.116555136839413
1844, epoch_train_loss=3.116555136839413
Epoch 1845
Epoch 1845 :: Batch 0/1
Batch Loss = 3.116722228952204
1845, epoch_train_loss=3.116722228952204
Epoch 1846
Epoch 1846 :: Batch 0/1
Batch Loss = 3.128233671019238
1846, epoch_train_loss=3.128233671019238
Epoch 1847
Epoch 1847 :: Batch 0/1
Batch Loss = 3.1158846408381753
1847, epoch_train_loss=3.1158846408381753
Epoch 1848
Epoch 1848 :: Batch 0/1
Batch Loss = 3.1109509920059204
1848, epoch_train_loss=3.1109509920059204
Epoch 1849
Epoch 1849 :: Batch 0/1
Batch Loss = 3.120584789900008
1849, epoch_train_loss=3.120584789900008
Epoch 1850
Epoch 1850 :: Batch 0/1
Batch Loss = 3.1157971945338025
1850, epoch_train_loss=3.1157971945338025
Epoch 1851
Epoch 1851 :: Batch 0/1
Batch Loss = 3.1107562898582857
1851, epoch_train_loss=3.1107562898582857
Epoch 1852
Epoch 1852 :: Batch 0/1
Batch Loss = 3.1156695155640457
1852, epoch_train_loss=3.1156695155640457
Epoch 1853
Epoch 1853 :: Batch 0/1
Batch Loss = 3.1134738839835947
1853, epoch_train_loss=3.1134738839835947
Epoch 1854
Epoch 1854 :: Batch 0/1
Batch Loss = 3.10881282812958
1854, epoch_train_loss=3.10881282812958
Epoch 1855
Epoch 1855 :: Batch 0/1
Batch Loss = 3.1093259352844256
1855, epoch_train_loss=3.1093259352844256
Epoch 1856
Epoch 1856 :: Batch 0/1
Batch Loss = 3.1104218039747877
1856, epoch_train_loss=3.1104218039747877
Epoch 1857
Epoch 1857 :: Batch 0/1
Batch Loss = 3.1069574362879178
1857, epoch_train_loss=3.1069574362879178
Epoch 1858
Epoch 1858 :: Batch 0/1
Batch Loss = 3.106618419176882
1858, epoch_train_loss=3.106618419176882
Epoch 1859
Epoch 1859 :: Batch 0/1
Batch Loss = 3.107827270662368
1859, epoch_train_loss=3.107827270662368
Epoch 1860
Epoch 1860 :: Batch 0/1
Batch Loss = 3.1050268075459115
1860, epoch_train_loss=3.1050268075459115
Epoch 1861
Epoch 1861 :: Batch 0/1
Batch Loss = 3.1043410957639206
1861, epoch_train_loss=3.1043410957639206
Epoch 1862
Epoch 1862 :: Batch 0/1
Batch Loss = 3.1044818158450265
1862, epoch_train_loss=3.1044818158450265
Epoch 1863
Epoch 1863 :: Batch 0/1
Batch Loss = 3.1035797576125903
1863, epoch_train_loss=3.1035797576125903
Epoch 1864
Epoch 1864 :: Batch 0/1
Batch Loss = 3.102481428707322
1864, epoch_train_loss=3.102481428707322
Epoch 1865
Epoch 1865 :: Batch 0/1
Batch Loss = 3.1030630561425694
1865, epoch_train_loss=3.1030630561425694
Epoch 1866
Epoch 1866 :: Batch 0/1
Batch Loss = 3.102322495812954
1866, epoch_train_loss=3.102322495812954
Epoch 1867
Epoch 1867 :: Batch 0/1
Batch Loss = 3.1013281253614693
1867, epoch_train_loss=3.1013281253614693
Epoch 1868
Epoch 1868 :: Batch 0/1
Batch Loss = 3.1014452215813577
1868, epoch_train_loss=3.1014452215813577
Epoch 1869
Epoch 1869 :: Batch 0/1
Batch Loss = 3.101199397874504
1869, epoch_train_loss=3.101199397874504
Epoch 1870
Epoch 1870 :: Batch 0/1
Batch Loss = 3.1004661679033045
1870, epoch_train_loss=3.1004661679033045
Epoch 1871
Epoch 1871 :: Batch 0/1
Batch Loss = 3.1002385311641154
1871, epoch_train_loss=3.1002385311641154
Epoch 1872
Epoch 1872 :: Batch 0/1
Batch Loss = 3.10030007714699
1872, epoch_train_loss=3.10030007714699
Epoch 1873
Epoch 1873 :: Batch 0/1
Batch Loss = 3.09944460527349
1873, epoch_train_loss=3.09944460527349
Epoch 1874
Epoch 1874 :: Batch 0/1
Batch Loss = 3.099240391125648
1874, epoch_train_loss=3.099240391125648
Epoch 1875
Epoch 1875 :: Batch 0/1
Batch Loss = 3.0991504468736366
1875, epoch_train_loss=3.0991504468736366
Epoch 1876
Epoch 1876 :: Batch 0/1
Batch Loss = 3.098662370272777
1876, epoch_train_loss=3.098662370272777
Epoch 1877
Epoch 1877 :: Batch 0/1
Batch Loss = 3.0981280957759565
1877, epoch_train_loss=3.0981280957759565
Epoch 1878
Epoch 1878 :: Batch 0/1
Batch Loss = 3.0981051012627945
1878, epoch_train_loss=3.0981051012627945
Epoch 1879
Epoch 1879 :: Batch 0/1
Batch Loss = 3.097551272485967
1879, epoch_train_loss=3.097551272485967
Epoch 1880
Epoch 1880 :: Batch 0/1
Batch Loss = 3.0971151533685326
1880, epoch_train_loss=3.0971151533685326
Epoch 1881
Epoch 1881 :: Batch 0/1
Batch Loss = 3.096988717293618
1881, epoch_train_loss=3.096988717293618
Epoch 1882
Epoch 1882 :: Batch 0/1
Batch Loss = 3.0967066712436697
1882, epoch_train_loss=3.0967066712436697
Epoch 1883
Epoch 1883 :: Batch 0/1
Batch Loss = 3.096140428527618
1883, epoch_train_loss=3.096140428527618
Epoch 1884
Epoch 1884 :: Batch 0/1
Batch Loss = 3.0959913581496705
1884, epoch_train_loss=3.0959913581496705
Epoch 1885
Epoch 1885 :: Batch 0/1
Batch Loss = 3.095663744789882
1885, epoch_train_loss=3.095663744789882
Epoch 1886
Epoch 1886 :: Batch 0/1
Batch Loss = 3.0952117948070414
1886, epoch_train_loss=3.0952117948070414
Epoch 1887
Epoch 1887 :: Batch 0/1
Batch Loss = 3.0949932968386418
1887, epoch_train_loss=3.0949932968386418
Epoch 1888
Epoch 1888 :: Batch 0/1
Batch Loss = 3.0948119516337522
1888, epoch_train_loss=3.0948119516337522
Epoch 1889
Epoch 1889 :: Batch 0/1
Batch Loss = 3.094349332797992
1889, epoch_train_loss=3.094349332797992
Epoch 1890
Epoch 1890 :: Batch 0/1
Batch Loss = 3.094103270490626
1890, epoch_train_loss=3.094103270490626
Epoch 1891
Epoch 1891 :: Batch 0/1
Batch Loss = 3.0938646884227614
1891, epoch_train_loss=3.0938646884227614
Epoch 1892
Epoch 1892 :: Batch 0/1
Batch Loss = 3.09347642309759
1892, epoch_train_loss=3.09347642309759
Epoch 1893
Epoch 1893 :: Batch 0/1
Batch Loss = 3.093211475448782
1893, epoch_train_loss=3.093211475448782
Epoch 1894
Epoch 1894 :: Batch 0/1
Batch Loss = 3.0930131044381373
1894, epoch_train_loss=3.0930131044381373
Epoch 1895
Epoch 1895 :: Batch 0/1
Batch Loss = 3.0926529605007875
1895, epoch_train_loss=3.0926529605007875
Epoch 1896
Epoch 1896 :: Batch 0/1
Batch Loss = 3.0923473901815433
1896, epoch_train_loss=3.0923473901815433
Epoch 1897
Epoch 1897 :: Batch 0/1
Batch Loss = 3.0921209113317003
1897, epoch_train_loss=3.0921209113317003
Epoch 1898
Epoch 1898 :: Batch 0/1
Batch Loss = 3.0917655395362846
1898, epoch_train_loss=3.0917655395362846
Epoch 1899
Epoch 1899 :: Batch 0/1
Batch Loss = 3.091477362406664
1899, epoch_train_loss=3.091477362406664
Epoch 1900
Epoch 1900 :: Batch 0/1
Batch Loss = 3.091231957399081
1900, epoch_train_loss=3.091231957399081
Epoch 1901
Epoch 1901 :: Batch 0/1
Batch Loss = 3.090912172739765
1901, epoch_train_loss=3.090912172739765
Epoch 1902
Epoch 1902 :: Batch 0/1
Batch Loss = 3.0905971407203032
1902, epoch_train_loss=3.0905971407203032
Epoch 1903
Epoch 1903 :: Batch 0/1
Batch Loss = 3.0903332843725524
1903, epoch_train_loss=3.0903332843725524
Epoch 1904
Epoch 1904 :: Batch 0/1
Batch Loss = 3.090016399590317
1904, epoch_train_loss=3.090016399590317
Epoch 1905
Epoch 1905 :: Batch 0/1
Batch Loss = 3.089704453823605
1905, epoch_train_loss=3.089704453823605
Epoch 1906
Epoch 1906 :: Batch 0/1
Batch Loss = 3.0894378857851876
1906, epoch_train_loss=3.0894378857851876
Epoch 1907
Epoch 1907 :: Batch 0/1
Batch Loss = 3.0891221120726287
1907, epoch_train_loss=3.0891221120726287
Epoch 1908
Epoch 1908 :: Batch 0/1
Batch Loss = 3.0888125533918735
1908, epoch_train_loss=3.0888125533918735
Epoch 1909
Epoch 1909 :: Batch 0/1
Batch Loss = 3.0885250442757224
1909, epoch_train_loss=3.0885250442757224
Epoch 1910
Epoch 1910 :: Batch 0/1
Batch Loss = 3.088213179810521
1910, epoch_train_loss=3.088213179810521
Epoch 1911
Epoch 1911 :: Batch 0/1
Batch Loss = 3.0879069418730234
1911, epoch_train_loss=3.0879069418730234
Epoch 1912
Epoch 1912 :: Batch 0/1
Batch Loss = 3.087610385676298
1912, epoch_train_loss=3.087610385676298
Epoch 1913
Epoch 1913 :: Batch 0/1
Batch Loss = 3.0873027376548707
1913, epoch_train_loss=3.0873027376548707
Epoch 1914
Epoch 1914 :: Batch 0/1
Batch Loss = 3.086991916760284
1914, epoch_train_loss=3.086991916760284
Epoch 1915
Epoch 1915 :: Batch 0/1
Batch Loss = 3.08668636910411
1915, epoch_train_loss=3.08668636910411
Epoch 1916
Epoch 1916 :: Batch 0/1
Batch Loss = 3.0863783158151774
1916, epoch_train_loss=3.0863783158151774
Epoch 1917
Epoch 1917 :: Batch 0/1
Batch Loss = 3.0860652678661133
1917, epoch_train_loss=3.0860652678661133
Epoch 1918
Epoch 1918 :: Batch 0/1
Batch Loss = 3.085754819493515
1918, epoch_train_loss=3.085754819493515
Epoch 1919
Epoch 1919 :: Batch 0/1
Batch Loss = 3.085443106708847
1919, epoch_train_loss=3.085443106708847
Epoch 1920
Epoch 1920 :: Batch 0/1
Batch Loss = 3.085125433523278
1920, epoch_train_loss=3.085125433523278
Epoch 1921
Epoch 1921 :: Batch 0/1
Batch Loss = 3.08480916560017
1921, epoch_train_loss=3.08480916560017
Epoch 1922
Epoch 1922 :: Batch 0/1
Batch Loss = 3.0844942076665345
1922, epoch_train_loss=3.0844942076665345
Epoch 1923
Epoch 1923 :: Batch 0/1
Batch Loss = 3.084173762679679
1923, epoch_train_loss=3.084173762679679
Epoch 1924
Epoch 1924 :: Batch 0/1
Batch Loss = 3.0838525051781085
1924, epoch_train_loss=3.0838525051781085
Epoch 1925
Epoch 1925 :: Batch 0/1
Batch Loss = 3.083532578185674
1925, epoch_train_loss=3.083532578185674
Epoch 1926
Epoch 1926 :: Batch 0/1
Batch Loss = 3.08320727984859
1926, epoch_train_loss=3.08320727984859
Epoch 1927
Epoch 1927 :: Batch 0/1
Batch Loss = 3.082880830978707
1927, epoch_train_loss=3.082880830978707
Epoch 1928
Epoch 1928 :: Batch 0/1
Batch Loss = 3.082556858875202
1928, epoch_train_loss=3.082556858875202
Epoch 1929
Epoch 1929 :: Batch 0/1
Batch Loss = 3.08222799201872
1929, epoch_train_loss=3.08222799201872
Epoch 1930
Epoch 1930 :: Batch 0/1
Batch Loss = 3.0818969639769835
1930, epoch_train_loss=3.0818969639769835
Epoch 1931
Epoch 1931 :: Batch 0/1
Batch Loss = 3.0815675978213144
1931, epoch_train_loss=3.0815675978213144
Epoch 1932
Epoch 1932 :: Batch 0/1
Batch Loss = 3.0812340651532395
1932, epoch_train_loss=3.0812340651532395
Epoch 1933
Epoch 1933 :: Batch 0/1
Batch Loss = 3.0808988031345867
1933, epoch_train_loss=3.0808988031345867
Epoch 1934
Epoch 1934 :: Batch 0/1
Batch Loss = 3.0805646356612386
1934, epoch_train_loss=3.0805646356612386
Epoch 1935
Epoch 1935 :: Batch 0/1
Batch Loss = 3.0802270037823636
1935, epoch_train_loss=3.0802270037823636
Epoch 1936
Epoch 1936 :: Batch 0/1
Batch Loss = 3.0798877946327323
1936, epoch_train_loss=3.0798877946327323
Epoch 1937
Epoch 1937 :: Batch 0/1
Batch Loss = 3.079548082578963
1937, epoch_train_loss=3.079548082578963
Epoch 1938
Epoch 1938 :: Batch 0/1
Batch Loss = 3.0792051447609516
1938, epoch_train_loss=3.0792051447609516
Epoch 1939
Epoch 1939 :: Batch 0/1
Batch Loss = 3.0788615958776093
1939, epoch_train_loss=3.0788615958776093
Epoch 1940
Epoch 1940 :: Batch 0/1
Batch Loss = 3.078517351300191
1940, epoch_train_loss=3.078517351300191
Epoch 1941
Epoch 1941 :: Batch 0/1
Batch Loss = 3.0781698915109517
1941, epoch_train_loss=3.0781698915109517
Epoch 1942
Epoch 1942 :: Batch 0/1
Batch Loss = 3.077821361920247
1942, epoch_train_loss=3.077821361920247
Epoch 1943
Epoch 1943 :: Batch 0/1
Batch Loss = 3.0774717827340647
1943, epoch_train_loss=3.0774717827340647
Epoch 1944
Epoch 1944 :: Batch 0/1
Batch Loss = 3.0771197484608854
1944, epoch_train_loss=3.0771197484608854
Epoch 1945
Epoch 1945 :: Batch 0/1
Batch Loss = 3.0767666590490834
1945, epoch_train_loss=3.0767666590490834
Epoch 1946
Epoch 1946 :: Batch 0/1
Batch Loss = 3.076412033590417
1946, epoch_train_loss=3.076412033590417
Epoch 1947
Epoch 1947 :: Batch 0/1
Batch Loss = 3.0760550624849956
1947, epoch_train_loss=3.0760550624849956
Epoch 1948
Epoch 1948 :: Batch 0/1
Batch Loss = 3.0756968962213147
1948, epoch_train_loss=3.0756968962213147
Epoch 1949
Epoch 1949 :: Batch 0/1
Batch Loss = 3.075337039267355
1949, epoch_train_loss=3.075337039267355
Epoch 1950
Epoch 1950 :: Batch 0/1
Batch Loss = 3.074975301398143
1950, epoch_train_loss=3.074975301398143
Epoch 1951
Epoch 1951 :: Batch 0/1
Batch Loss = 3.0746122283896127
1951, epoch_train_loss=3.0746122283896127
Epoch 1952
Epoch 1952 :: Batch 0/1
Batch Loss = 3.074247441433551
1952, epoch_train_loss=3.074247441433551
Epoch 1953
Epoch 1953 :: Batch 0/1
Batch Loss = 3.07388095043477
1953, epoch_train_loss=3.07388095043477
Epoch 1954
Epoch 1954 :: Batch 0/1
Batch Loss = 3.0735128976757355
1954, epoch_train_loss=3.0735128976757355
Epoch 1955
Epoch 1955 :: Batch 0/1
Batch Loss = 3.073143398802113
1955, epoch_train_loss=3.073143398802113
Epoch 1956
Epoch 1956 :: Batch 0/1
Batch Loss = 3.072772440456621
1956, epoch_train_loss=3.072772440456621
Epoch 1957
Epoch 1957 :: Batch 0/1
Batch Loss = 3.072400913539248
1957, epoch_train_loss=3.072400913539248
Epoch 1958
Epoch 1958 :: Batch 0/1
Batch Loss = 3.0720300878022457
1958, epoch_train_loss=3.0720300878022457
Epoch 1959
Epoch 1959 :: Batch 0/1
Batch Loss = 3.071665154372641
1959, epoch_train_loss=3.071665154372641
Epoch 1960
Epoch 1960 :: Batch 0/1
Batch Loss = 3.071320785294938
1960, epoch_train_loss=3.071320785294938
Epoch 1961
Epoch 1961 :: Batch 0/1
Batch Loss = 3.071048258975827
1961, epoch_train_loss=3.071048258975827
Epoch 1962
Epoch 1962 :: Batch 0/1
Batch Loss = 3.0710331647453346
1962, epoch_train_loss=3.0710331647453346
Epoch 1963
Epoch 1963 :: Batch 0/1
Batch Loss = 3.0718880139186653
1963, epoch_train_loss=3.0718880139186653
Epoch 1964
Epoch 1964 :: Batch 0/1
Batch Loss = 3.0761483785660726
1964, epoch_train_loss=3.0761483785660726
Epoch 1965
Epoch 1965 :: Batch 0/1
Batch Loss = 3.087398197224351
1965, epoch_train_loss=3.087398197224351
Epoch 1966
Epoch 1966 :: Batch 0/1
Batch Loss = 3.1137468778949455
1966, epoch_train_loss=3.1137468778949455
Epoch 1967
Epoch 1967 :: Batch 0/1
Batch Loss = 3.077856463675071
1967, epoch_train_loss=3.077856463675071
Epoch 1968
Epoch 1968 :: Batch 0/1
Batch Loss = 3.06941493400693
1968, epoch_train_loss=3.06941493400693
Epoch 1969
Epoch 1969 :: Batch 0/1
Batch Loss = 3.0726656972192994
1969, epoch_train_loss=3.0726656972192994
Epoch 1970
Epoch 1970 :: Batch 0/1
Batch Loss = 3.087145178444931
1970, epoch_train_loss=3.087145178444931
Epoch 1971
Epoch 1971 :: Batch 0/1
Batch Loss = 3.1191324274055554
1971, epoch_train_loss=3.1191324274055554
Epoch 1972
Epoch 1972 :: Batch 0/1
Batch Loss = 3.0695289290307697
1972, epoch_train_loss=3.0695289290307697
Epoch 1973
Epoch 1973 :: Batch 0/1
Batch Loss = 3.0888657326134434
1973, epoch_train_loss=3.0888657326134434
Epoch 1974
Epoch 1974 :: Batch 0/1
Batch Loss = 3.1493572758798147
1974, epoch_train_loss=3.1493572758798147
Epoch 1975
Epoch 1975 :: Batch 0/1
Batch Loss = 3.099628943824174
1975, epoch_train_loss=3.099628943824174
Epoch 1976
Epoch 1976 :: Batch 0/1
Batch Loss = 3.273269353849612
1976, epoch_train_loss=3.273269353849612
Epoch 1977
Epoch 1977 :: Batch 0/1
Batch Loss = 3.1964862281878137
1977, epoch_train_loss=3.1964862281878137
Epoch 1978
Epoch 1978 :: Batch 0/1
Batch Loss = 3.2664602631548916
1978, epoch_train_loss=3.2664602631548916
Epoch 1979
Epoch 1979 :: Batch 0/1
Batch Loss = 3.2553016262435035
1979, epoch_train_loss=3.2553016262435035
Epoch 1980
Epoch 1980 :: Batch 0/1
Batch Loss = 3.2173262222911165
1980, epoch_train_loss=3.2173262222911165
Epoch 1981
Epoch 1981 :: Batch 0/1
Batch Loss = 3.186291017668358
1981, epoch_train_loss=3.186291017668358
Epoch 1982
Epoch 1982 :: Batch 0/1
Batch Loss = 3.191714298797409
1982, epoch_train_loss=3.191714298797409
Epoch 1983
Epoch 1983 :: Batch 0/1
Batch Loss = 3.2106642766606606
1983, epoch_train_loss=3.2106642766606606
Epoch 1984
Epoch 1984 :: Batch 0/1
Batch Loss = 3.217694150270862
1984, epoch_train_loss=3.217694150270862
Epoch 1985
Epoch 1985 :: Batch 0/1
Batch Loss = 3.208666168633702
1985, epoch_train_loss=3.208666168633702
Epoch 1986
Epoch 1986 :: Batch 0/1
Batch Loss = 3.195086687615416
1986, epoch_train_loss=3.195086687615416
Epoch 1987
Epoch 1987 :: Batch 0/1
Batch Loss = 3.196295412195062
1987, epoch_train_loss=3.196295412195062
Epoch 1988
Epoch 1988 :: Batch 0/1
Batch Loss = 3.2026318989255005
1988, epoch_train_loss=3.2026318989255005
Epoch 1989
Epoch 1989 :: Batch 0/1
Batch Loss = 3.194402006828934
1989, epoch_train_loss=3.194402006828934
Epoch 1990
Epoch 1990 :: Batch 0/1
Batch Loss = 3.1729946605612085
1990, epoch_train_loss=3.1729946605612085
Epoch 1991
Epoch 1991 :: Batch 0/1
Batch Loss = 3.15432207862032
1991, epoch_train_loss=3.15432207862032
Epoch 1992
Epoch 1992 :: Batch 0/1
Batch Loss = 3.154830834924407
1992, epoch_train_loss=3.154830834924407
Epoch 1993
Epoch 1993 :: Batch 0/1
Batch Loss = 3.16108517227761
1993, epoch_train_loss=3.16108517227761
Epoch 1994
Epoch 1994 :: Batch 0/1
Batch Loss = 3.1524282373377686
1994, epoch_train_loss=3.1524282373377686
Epoch 1995
Epoch 1995 :: Batch 0/1
Batch Loss = 3.138786430263145
1995, epoch_train_loss=3.138786430263145
Epoch 1996
Epoch 1996 :: Batch 0/1
Batch Loss = 3.1378415165591593
1996, epoch_train_loss=3.1378415165591593
Epoch 1997
Epoch 1997 :: Batch 0/1
Batch Loss = 3.1439423630326173
1997, epoch_train_loss=3.1439423630326173
Epoch 1998
Epoch 1998 :: Batch 0/1
Batch Loss = 3.143767460171549
1998, epoch_train_loss=3.143767460171549
Epoch 1999
Epoch 1999 :: Batch 0/1
Batch Loss = 3.1421809632589626
1999, epoch_train_loss=3.1421809632589626
Epoch 2000
Epoch 2000 :: Batch 0/1
Batch Loss = 3.1445399794556215
2000, epoch_train_loss=3.1445399794556215
Epoch 2001
Epoch 2001 :: Batch 0/1
Batch Loss = 3.149292093589645
2001, epoch_train_loss=3.149292093589645
Epoch 2002
Epoch 2002 :: Batch 0/1
Batch Loss = 3.148894475739479
2002, epoch_train_loss=3.148894475739479
Epoch 2003
Epoch 2003 :: Batch 0/1
Batch Loss = 3.1428503776749714
2003, epoch_train_loss=3.1428503776749714
Epoch 2004
Epoch 2004 :: Batch 0/1
Batch Loss = 3.141001803666704
2004, epoch_train_loss=3.141001803666704
Epoch 2005
Epoch 2005 :: Batch 0/1
Batch Loss = 3.1432952239074305
2005, epoch_train_loss=3.1432952239074305
Epoch 2006
Epoch 2006 :: Batch 0/1
Batch Loss = 3.140648991388782
2006, epoch_train_loss=3.140648991388782
Epoch 2007
Epoch 2007 :: Batch 0/1
Batch Loss = 3.1362860517292357
2007, epoch_train_loss=3.1362860517292357
Epoch 2008
Epoch 2008 :: Batch 0/1
Batch Loss = 3.1352786463924325
2008, epoch_train_loss=3.1352786463924325
Epoch 2009
Epoch 2009 :: Batch 0/1
Batch Loss = 3.1357428987643465
2009, epoch_train_loss=3.1357428987643465
Epoch 2010
Epoch 2010 :: Batch 0/1
Batch Loss = 3.1343717786674476
2010, epoch_train_loss=3.1343717786674476
Epoch 2011
Epoch 2011 :: Batch 0/1
Batch Loss = 3.132521852825488
2011, epoch_train_loss=3.132521852825488
Epoch 2012
Epoch 2012 :: Batch 0/1
Batch Loss = 3.1332382084592494
2012, epoch_train_loss=3.1332382084592494
Epoch 2013
Epoch 2013 :: Batch 0/1
Batch Loss = 3.134743518285255
2013, epoch_train_loss=3.134743518285255
Epoch 2014
Epoch 2014 :: Batch 0/1
Batch Loss = 3.1343703431836247
2014, epoch_train_loss=3.1343703431836247
Epoch 2015
Epoch 2015 :: Batch 0/1
Batch Loss = 3.133742668970757
2015, epoch_train_loss=3.133742668970757
Epoch 2016
Epoch 2016 :: Batch 0/1
Batch Loss = 3.133230720712498
2016, epoch_train_loss=3.133230720712498
Epoch 2017
Epoch 2017 :: Batch 0/1
Batch Loss = 3.1337432760042545
2017, epoch_train_loss=3.1337432760042545
Epoch 2018
Epoch 2018 :: Batch 0/1
Batch Loss = 3.1338130467286818
2018, epoch_train_loss=3.1338130467286818
Epoch 2019
Epoch 2019 :: Batch 0/1
Batch Loss = 3.132768512562301
2019, epoch_train_loss=3.132768512562301
Epoch 2020
Epoch 2020 :: Batch 0/1
Batch Loss = 3.1322093100096393
2020, epoch_train_loss=3.1322093100096393
Epoch 2021
Epoch 2021 :: Batch 0/1
Batch Loss = 3.1321226195246994
2021, epoch_train_loss=3.1321226195246994
Epoch 2022
Epoch 2022 :: Batch 0/1
Batch Loss = 3.131610062910503
2022, epoch_train_loss=3.131610062910503
Epoch 2023
Epoch 2023 :: Batch 0/1
Batch Loss = 3.1306704297361283
2023, epoch_train_loss=3.1306704297361283
Epoch 2024
Epoch 2024 :: Batch 0/1
Batch Loss = 3.130071433122291
2024, epoch_train_loss=3.130071433122291
Epoch 2025
Epoch 2025 :: Batch 0/1
Batch Loss = 3.1303717729242444
2025, epoch_train_loss=3.1303717729242444
Epoch 2026
Epoch 2026 :: Batch 0/1
Batch Loss = 3.13020192182831
2026, epoch_train_loss=3.13020192182831
Epoch 2027
Epoch 2027 :: Batch 0/1
Batch Loss = 3.1296553544228676
2027, epoch_train_loss=3.1296553544228676
Epoch 2028
Epoch 2028 :: Batch 0/1
Batch Loss = 3.129478978352112
2028, epoch_train_loss=3.129478978352112
Epoch 2029
Epoch 2029 :: Batch 0/1
Batch Loss = 3.1295401467029715
2029, epoch_train_loss=3.1295401467029715
Epoch 2030
Epoch 2030 :: Batch 0/1
Batch Loss = 3.1294383342730967
2030, epoch_train_loss=3.1294383342730967
Epoch 2031
Epoch 2031 :: Batch 0/1
Batch Loss = 3.1291853713991533
2031, epoch_train_loss=3.1291853713991533
Epoch 2032
Epoch 2032 :: Batch 0/1
Batch Loss = 3.129021102148361
2032, epoch_train_loss=3.129021102148361
Epoch 2033
Epoch 2033 :: Batch 0/1
Batch Loss = 3.1290271771297733
2033, epoch_train_loss=3.1290271771297733
Epoch 2034
Epoch 2034 :: Batch 0/1
Batch Loss = 3.1287759743122927
2034, epoch_train_loss=3.1287759743122927
Epoch 2035
Epoch 2035 :: Batch 0/1
Batch Loss = 3.1284570628167865
2035, epoch_train_loss=3.1284570628167865
Epoch 2036
Epoch 2036 :: Batch 0/1
Batch Loss = 3.128194042955026
2036, epoch_train_loss=3.128194042955026
Epoch 2037
Epoch 2037 :: Batch 0/1
Batch Loss = 3.1280216565653918
2037, epoch_train_loss=3.1280216565653918
Epoch 2038
Epoch 2038 :: Batch 0/1
Batch Loss = 3.1278854897390604
2038, epoch_train_loss=3.1278854897390604
Epoch 2039
Epoch 2039 :: Batch 0/1
Batch Loss = 3.1276255813531777
2039, epoch_train_loss=3.1276255813531777
Epoch 2040
Epoch 2040 :: Batch 0/1
Batch Loss = 3.127414720077811
2040, epoch_train_loss=3.127414720077811
Epoch 2041
Epoch 2041 :: Batch 0/1
Batch Loss = 3.1273084474387085
2041, epoch_train_loss=3.1273084474387085
Epoch 2042
Epoch 2042 :: Batch 0/1
Batch Loss = 3.1271769496174113
2042, epoch_train_loss=3.1271769496174113
Epoch 2043
Epoch 2043 :: Batch 0/1
Batch Loss = 3.1270058120666486
2043, epoch_train_loss=3.1270058120666486
Epoch 2044
Epoch 2044 :: Batch 0/1
Batch Loss = 3.126857465325502
2044, epoch_train_loss=3.126857465325502
Epoch 2045
Epoch 2045 :: Batch 0/1
Batch Loss = 3.126779535760393
2045, epoch_train_loss=3.126779535760393
Epoch 2046
Epoch 2046 :: Batch 0/1
Batch Loss = 3.126677707125997
2046, epoch_train_loss=3.126677707125997
Epoch 2047
Epoch 2047 :: Batch 0/1
Batch Loss = 3.1264792325880846
2047, epoch_train_loss=3.1264792325880846
Epoch 2048
Epoch 2048 :: Batch 0/1
Batch Loss = 3.1263289086152293
2048, epoch_train_loss=3.1263289086152293
Epoch 2049
Epoch 2049 :: Batch 0/1
Batch Loss = 3.1262106109689864
2049, epoch_train_loss=3.1262106109689864
Epoch 2050
Epoch 2050 :: Batch 0/1
Batch Loss = 3.126071869817987
2050, epoch_train_loss=3.126071869817987
Epoch 2051
Epoch 2051 :: Batch 0/1
Batch Loss = 3.125903375340311
2051, epoch_train_loss=3.125903375340311
Epoch 2052
Epoch 2052 :: Batch 0/1
Batch Loss = 3.125750539800956
2052, epoch_train_loss=3.125750539800956
Epoch 2053
Epoch 2053 :: Batch 0/1
Batch Loss = 3.1256283197705836
2053, epoch_train_loss=3.1256283197705836
Epoch 2054
Epoch 2054 :: Batch 0/1
Batch Loss = 3.125488208798823
2054, epoch_train_loss=3.125488208798823
Epoch 2055
Epoch 2055 :: Batch 0/1
Batch Loss = 3.125336226672977
2055, epoch_train_loss=3.125336226672977
Epoch 2056
Epoch 2056 :: Batch 0/1
Batch Loss = 3.125212858921142
2056, epoch_train_loss=3.125212858921142
Epoch 2057
Epoch 2057 :: Batch 0/1
Batch Loss = 3.1250995447239793
2057, epoch_train_loss=3.1250995447239793
Epoch 2058
Epoch 2058 :: Batch 0/1
Batch Loss = 3.1249811426625778
2058, epoch_train_loss=3.1249811426625778
Epoch 2059
Epoch 2059 :: Batch 0/1
Batch Loss = 3.12485196405784
2059, epoch_train_loss=3.12485196405784
Epoch 2060
Epoch 2060 :: Batch 0/1
Batch Loss = 3.1247249380157163
2060, epoch_train_loss=3.1247249380157163
Epoch 2061
Epoch 2061 :: Batch 0/1
Batch Loss = 3.1246107423895295
2061, epoch_train_loss=3.1246107423895295
Epoch 2062
Epoch 2062 :: Batch 0/1
Batch Loss = 3.1244875644632746
2062, epoch_train_loss=3.1244875644632746
Epoch 2063
Epoch 2063 :: Batch 0/1
Batch Loss = 3.1243612405847614
2063, epoch_train_loss=3.1243612405847614
Epoch 2064
Epoch 2064 :: Batch 0/1
Batch Loss = 3.1242368869639394
2064, epoch_train_loss=3.1242368869639394
Epoch 2065
Epoch 2065 :: Batch 0/1
Batch Loss = 3.1241173038543377
2065, epoch_train_loss=3.1241173038543377
Epoch 2066
Epoch 2066 :: Batch 0/1
Batch Loss = 3.1239956766629837
2066, epoch_train_loss=3.1239956766629837
Epoch 2067
Epoch 2067 :: Batch 0/1
Batch Loss = 3.1238681408034865
2067, epoch_train_loss=3.1238681408034865
Epoch 2068
Epoch 2068 :: Batch 0/1
Batch Loss = 3.1237477226707324
2068, epoch_train_loss=3.1237477226707324
Epoch 2069
Epoch 2069 :: Batch 0/1
Batch Loss = 3.123634838541762
2069, epoch_train_loss=3.123634838541762
Epoch 2070
Epoch 2070 :: Batch 0/1
Batch Loss = 3.1235181991773917
2070, epoch_train_loss=3.1235181991773917
Epoch 2071
Epoch 2071 :: Batch 0/1
Batch Loss = 3.1233996580196908
2071, epoch_train_loss=3.1233996580196908
Epoch 2072
Epoch 2072 :: Batch 0/1
Batch Loss = 3.123284924627719
2072, epoch_train_loss=3.123284924627719
Epoch 2073
Epoch 2073 :: Batch 0/1
Batch Loss = 3.123173275789276
2073, epoch_train_loss=3.123173275789276
Epoch 2074
Epoch 2074 :: Batch 0/1
Batch Loss = 3.1230593269528315
2074, epoch_train_loss=3.1230593269528315
Epoch 2075
Epoch 2075 :: Batch 0/1
Batch Loss = 3.1229431907254357
2075, epoch_train_loss=3.1229431907254357
Epoch 2076
Epoch 2076 :: Batch 0/1
Batch Loss = 3.122831459593146
2076, epoch_train_loss=3.122831459593146
Epoch 2077
Epoch 2077 :: Batch 0/1
Batch Loss = 3.122719422817585
2077, epoch_train_loss=3.122719422817585
Epoch 2078
Epoch 2078 :: Batch 0/1
Batch Loss = 3.1226050669503986
2078, epoch_train_loss=3.1226050669503986
Epoch 2079
Epoch 2079 :: Batch 0/1
Batch Loss = 3.1224912946621166
2079, epoch_train_loss=3.1224912946621166
Epoch 2080
Epoch 2080 :: Batch 0/1
Batch Loss = 3.122379622932343
2080, epoch_train_loss=3.122379622932343
Epoch 2081
Epoch 2081 :: Batch 0/1
Batch Loss = 3.122269275577311
2081, epoch_train_loss=3.122269275577311
Epoch 2082
Epoch 2082 :: Batch 0/1
Batch Loss = 3.122157826942275
2082, epoch_train_loss=3.122157826942275
Epoch 2083
Epoch 2083 :: Batch 0/1
Batch Loss = 3.122047222670004
2083, epoch_train_loss=3.122047222670004
Epoch 2084
Epoch 2084 :: Batch 0/1
Batch Loss = 3.1219381475377816
2084, epoch_train_loss=3.1219381475377816
Epoch 2085
Epoch 2085 :: Batch 0/1
Batch Loss = 3.1218292820552143
2085, epoch_train_loss=3.1218292820552143
Epoch 2086
Epoch 2086 :: Batch 0/1
Batch Loss = 3.121720546650768
2086, epoch_train_loss=3.121720546650768
Epoch 2087
Epoch 2087 :: Batch 0/1
Batch Loss = 3.121612355420266
2087, epoch_train_loss=3.121612355420266
Epoch 2088
Epoch 2088 :: Batch 0/1
Batch Loss = 3.121504926532252
2088, epoch_train_loss=3.121504926532252
Epoch 2089
Epoch 2089 :: Batch 0/1
Batch Loss = 3.1213976708628515
2089, epoch_train_loss=3.1213976708628515
Epoch 2090
Epoch 2090 :: Batch 0/1
Batch Loss = 3.121289899464756
2090, epoch_train_loss=3.121289899464756
Epoch 2091
Epoch 2091 :: Batch 0/1
Batch Loss = 3.1211824851218832
2091, epoch_train_loss=3.1211824851218832
Epoch 2092
Epoch 2092 :: Batch 0/1
Batch Loss = 3.1210758099300464
2092, epoch_train_loss=3.1210758099300464
Epoch 2093
Epoch 2093 :: Batch 0/1
Batch Loss = 3.120969315562418
2093, epoch_train_loss=3.120969315562418
Epoch 2094
Epoch 2094 :: Batch 0/1
Batch Loss = 3.120862887129829
2094, epoch_train_loss=3.120862887129829
Epoch 2095
Epoch 2095 :: Batch 0/1
Batch Loss = 3.120756683165139
2095, epoch_train_loss=3.120756683165139
Epoch 2096
Epoch 2096 :: Batch 0/1
Batch Loss = 3.120651104640524
2096, epoch_train_loss=3.120651104640524
Epoch 2097
Epoch 2097 :: Batch 0/1
Batch Loss = 3.1205455547607484
2097, epoch_train_loss=3.1205455547607484
Epoch 2098
Epoch 2098 :: Batch 0/1
Batch Loss = 3.120439874903073
2098, epoch_train_loss=3.120439874903073
Epoch 2099
Epoch 2099 :: Batch 0/1
Batch Loss = 3.1203346166610864
2099, epoch_train_loss=3.1203346166610864
Epoch 2100
Epoch 2100 :: Batch 0/1
Batch Loss = 3.1202294867567493
2100, epoch_train_loss=3.1202294867567493
Epoch 2101
Epoch 2101 :: Batch 0/1
Batch Loss = 3.120124173255738
2101, epoch_train_loss=3.120124173255738
Epoch 2102
Epoch 2102 :: Batch 0/1
Batch Loss = 3.120018624982539
2102, epoch_train_loss=3.120018624982539
Epoch 2103
Epoch 2103 :: Batch 0/1
Batch Loss = 3.1199131035886682
2103, epoch_train_loss=3.1199131035886682
Epoch 2104
Epoch 2104 :: Batch 0/1
Batch Loss = 3.1198074915781593
2104, epoch_train_loss=3.1198074915781593
Epoch 2105
Epoch 2105 :: Batch 0/1
Batch Loss = 3.119701423736072
2105, epoch_train_loss=3.119701423736072
Epoch 2106
Epoch 2106 :: Batch 0/1
Batch Loss = 3.119595069829328
2106, epoch_train_loss=3.119595069829328
Epoch 2107
Epoch 2107 :: Batch 0/1
Batch Loss = 3.119488432572521
2107, epoch_train_loss=3.119488432572521
Epoch 2108
Epoch 2108 :: Batch 0/1
Batch Loss = 3.119381307392096
2108, epoch_train_loss=3.119381307392096
Epoch 2109
Epoch 2109 :: Batch 0/1
Batch Loss = 3.1192735815773047
2109, epoch_train_loss=3.1192735815773047
Epoch 2110
Epoch 2110 :: Batch 0/1
Batch Loss = 3.119165240420121
2110, epoch_train_loss=3.119165240420121
Epoch 2111
Epoch 2111 :: Batch 0/1
Batch Loss = 3.1190562203085315
2111, epoch_train_loss=3.1190562203085315
Epoch 2112
Epoch 2112 :: Batch 0/1
Batch Loss = 3.1189462569468445
2112, epoch_train_loss=3.1189462569468445
Epoch 2113
Epoch 2113 :: Batch 0/1
Batch Loss = 3.118835188328703
2113, epoch_train_loss=3.118835188328703
Epoch 2114
Epoch 2114 :: Batch 0/1
Batch Loss = 3.1187229528479956
2114, epoch_train_loss=3.1187229528479956
Epoch 2115
Epoch 2115 :: Batch 0/1
Batch Loss = 3.118609377129195
2115, epoch_train_loss=3.118609377129195
Epoch 2116
Epoch 2116 :: Batch 0/1
Batch Loss = 3.118494246248643
2116, epoch_train_loss=3.118494246248643
Epoch 2117
Epoch 2117 :: Batch 0/1
Batch Loss = 3.1183773607254723
2117, epoch_train_loss=3.1183773607254723
Epoch 2118
Epoch 2118 :: Batch 0/1
Batch Loss = 3.1182585284523734
2118, epoch_train_loss=3.1182585284523734
Epoch 2119
Epoch 2119 :: Batch 0/1
Batch Loss = 3.118137488527756
2119, epoch_train_loss=3.118137488527756
Epoch 2120
Epoch 2120 :: Batch 0/1
Batch Loss = 3.118013902902136
2120, epoch_train_loss=3.118013902902136
Epoch 2121
Epoch 2121 :: Batch 0/1
Batch Loss = 3.117887471368447
2121, epoch_train_loss=3.117887471368447
Epoch 2122
Epoch 2122 :: Batch 0/1
Batch Loss = 3.1177578727420565
2122, epoch_train_loss=3.1177578727420565
Epoch 2123
Epoch 2123 :: Batch 0/1
Batch Loss = 3.11762467316632
2123, epoch_train_loss=3.11762467316632
Epoch 2124
Epoch 2124 :: Batch 0/1
Batch Loss = 3.117487385634844
2124, epoch_train_loss=3.117487385634844
Epoch 2125
Epoch 2125 :: Batch 0/1
Batch Loss = 3.117345489853886
2125, epoch_train_loss=3.117345489853886
Epoch 2126
Epoch 2126 :: Batch 0/1
Batch Loss = 3.11719839437733
2126, epoch_train_loss=3.11719839437733
Epoch 2127
Epoch 2127 :: Batch 0/1
Batch Loss = 3.1170453763509194
2127, epoch_train_loss=3.1170453763509194
Epoch 2128
Epoch 2128 :: Batch 0/1
Batch Loss = 3.116885621216828
2128, epoch_train_loss=3.116885621216828
Epoch 2129
Epoch 2129 :: Batch 0/1
Batch Loss = 3.1167182441733305
2129, epoch_train_loss=3.1167182441733305
Epoch 2130
Epoch 2130 :: Batch 0/1
Batch Loss = 3.116542183981892
2130, epoch_train_loss=3.116542183981892
Epoch 2131
Epoch 2131 :: Batch 0/1
Batch Loss = 3.116356199575399
2131, epoch_train_loss=3.116356199575399
Epoch 2132
Epoch 2132 :: Batch 0/1
Batch Loss = 3.1161588861933547
2132, epoch_train_loss=3.1161588861933547
Epoch 2133
Epoch 2133 :: Batch 0/1
Batch Loss = 3.1159486055694687
2133, epoch_train_loss=3.1159486055694687
Epoch 2134
Epoch 2134 :: Batch 0/1
Batch Loss = 3.1157234272678433
2134, epoch_train_loss=3.1157234272678433
Epoch 2135
Epoch 2135 :: Batch 0/1
Batch Loss = 3.1154811191678946
2135, epoch_train_loss=3.1154811191678946
Epoch 2136
Epoch 2136 :: Batch 0/1
Batch Loss = 3.11521915018131
2136, epoch_train_loss=3.11521915018131
Epoch 2137
Epoch 2137 :: Batch 0/1
Batch Loss = 3.1149346152674884
2137, epoch_train_loss=3.1149346152674884
Epoch 2138
Epoch 2138 :: Batch 0/1
Batch Loss = 3.1146242063216416
2138, epoch_train_loss=3.1146242063216416
Epoch 2139
Epoch 2139 :: Batch 0/1
Batch Loss = 3.1142842498651175
2139, epoch_train_loss=3.1142842498651175
Epoch 2140
Epoch 2140 :: Batch 0/1
Batch Loss = 3.113910716820493
2140, epoch_train_loss=3.113910716820493
Epoch 2141
Epoch 2141 :: Batch 0/1
Batch Loss = 3.113499322076157
2141, epoch_train_loss=3.113499322076157
Epoch 2142
Epoch 2142 :: Batch 0/1
Batch Loss = 3.1130457644840743
2142, epoch_train_loss=3.1130457644840743
Epoch 2143
Epoch 2143 :: Batch 0/1
Batch Loss = 3.112546063161312
2143, epoch_train_loss=3.112546063161312
Epoch 2144
Epoch 2144 :: Batch 0/1
Batch Loss = 3.1119969971924153
2144, epoch_train_loss=3.1119969971924153
Epoch 2145
Epoch 2145 :: Batch 0/1
Batch Loss = 3.1113967012593133
2145, epoch_train_loss=3.1113967012593133
Epoch 2146
Epoch 2146 :: Batch 0/1
Batch Loss = 3.1107453604691266
2146, epoch_train_loss=3.1107453604691266
Epoch 2147
Epoch 2147 :: Batch 0/1
Batch Loss = 3.110045824854163
2147, epoch_train_loss=3.110045824854163
Epoch 2148
Epoch 2148 :: Batch 0/1
Batch Loss = 3.1093040855116287
2148, epoch_train_loss=3.1093040855116287
Epoch 2149
Epoch 2149 :: Batch 0/1
Batch Loss = 3.108529463356214
2149, epoch_train_loss=3.108529463356214
Epoch 2150
Epoch 2150 :: Batch 0/1
Batch Loss = 3.1077342494050026
2150, epoch_train_loss=3.1077342494050026
Epoch 2151
Epoch 2151 :: Batch 0/1
Batch Loss = 3.1069328108328396
2151, epoch_train_loss=3.1069328108328396
Epoch 2152
Epoch 2152 :: Batch 0/1
Batch Loss = 3.106140375189946
2152, epoch_train_loss=3.106140375189946
Epoch 2153
Epoch 2153 :: Batch 0/1
Batch Loss = 3.105371724602251
2153, epoch_train_loss=3.105371724602251
Epoch 2154
Epoch 2154 :: Batch 0/1
Batch Loss = 3.1046400927103477
2154, epoch_train_loss=3.1046400927103477
Epoch 2155
Epoch 2155 :: Batch 0/1
Batch Loss = 3.10395645121886
2155, epoch_train_loss=3.10395645121886
Epoch 2156
Epoch 2156 :: Batch 0/1
Batch Loss = 3.1033291089516535
2156, epoch_train_loss=3.1033291089516535
Epoch 2157
Epoch 2157 :: Batch 0/1
Batch Loss = 3.102763536751943
2157, epoch_train_loss=3.102763536751943
Epoch 2158
Epoch 2158 :: Batch 0/1
Batch Loss = 3.102262410987698
2158, epoch_train_loss=3.102262410987698
Epoch 2159
Epoch 2159 :: Batch 0/1
Batch Loss = 3.1018257529835855
2159, epoch_train_loss=3.1018257529835855
Epoch 2160
Epoch 2160 :: Batch 0/1
Batch Loss = 3.1014510710667453
2160, epoch_train_loss=3.1014510710667453
Epoch 2161
Epoch 2161 :: Batch 0/1
Batch Loss = 3.1011335539714633
2161, epoch_train_loss=3.1011335539714633
Epoch 2162
Epoch 2162 :: Batch 0/1
Batch Loss = 3.1008663945134614
2162, epoch_train_loss=3.1008663945134614
Epoch 2163
Epoch 2163 :: Batch 0/1
Batch Loss = 3.10064129550557
2163, epoch_train_loss=3.10064129550557
Epoch 2164
Epoch 2164 :: Batch 0/1
Batch Loss = 3.1004491414643764
2164, epoch_train_loss=3.1004491414643764
Epoch 2165
Epoch 2165 :: Batch 0/1
Batch Loss = 3.100280747910896
2165, epoch_train_loss=3.100280747910896
Epoch 2166
Epoch 2166 :: Batch 0/1
Batch Loss = 3.1001275470617666
2166, epoch_train_loss=3.1001275470617666
Epoch 2167
Epoch 2167 :: Batch 0/1
Batch Loss = 3.0999820992872436
2167, epoch_train_loss=3.0999820992872436
Epoch 2168
Epoch 2168 :: Batch 0/1
Batch Loss = 3.099838386982251
2168, epoch_train_loss=3.099838386982251
Epoch 2169
Epoch 2169 :: Batch 0/1
Batch Loss = 3.0996918745781885
2169, epoch_train_loss=3.0996918745781885
Epoch 2170
Epoch 2170 :: Batch 0/1
Batch Loss = 3.099539373584259
2170, epoch_train_loss=3.099539373584259
Epoch 2171
Epoch 2171 :: Batch 0/1
Batch Loss = 3.0993787988383983
2171, epoch_train_loss=3.0993787988383983
Epoch 2172
Epoch 2172 :: Batch 0/1
Batch Loss = 3.0992089040181714
2172, epoch_train_loss=3.0992089040181714
Epoch 2173
Epoch 2173 :: Batch 0/1
Batch Loss = 3.0990290609566564
2173, epoch_train_loss=3.0990290609566564
Epoch 2174
Epoch 2174 :: Batch 0/1
Batch Loss = 3.098839112968592
2174, epoch_train_loss=3.098839112968592
Epoch 2175
Epoch 2175 :: Batch 0/1
Batch Loss = 3.098639290275241
2175, epoch_train_loss=3.098639290275241
Epoch 2176
Epoch 2176 :: Batch 0/1
Batch Loss = 3.0984301468439313
2176, epoch_train_loss=3.0984301468439313
Epoch 2177
Epoch 2177 :: Batch 0/1
Batch Loss = 3.098212484403691
2177, epoch_train_loss=3.098212484403691
Epoch 2178
Epoch 2178 :: Batch 0/1
Batch Loss = 3.0979872422866053
2178, epoch_train_loss=3.0979872422866053
Epoch 2179
Epoch 2179 :: Batch 0/1
Batch Loss = 3.0977553519967604
2179, epoch_train_loss=3.0977553519967604
Epoch 2180
Epoch 2180 :: Batch 0/1
Batch Loss = 3.097517576339294
2180, epoch_train_loss=3.097517576339294
Epoch 2181
Epoch 2181 :: Batch 0/1
Batch Loss = 3.097274356081905
2181, epoch_train_loss=3.097274356081905
Epoch 2182
Epoch 2182 :: Batch 0/1
Batch Loss = 3.09702568595755
2182, epoch_train_loss=3.09702568595755
Epoch 2183
Epoch 2183 :: Batch 0/1
Batch Loss = 3.0967710274926694
2183, epoch_train_loss=3.0967710274926694
Epoch 2184
Epoch 2184 :: Batch 0/1
Batch Loss = 3.0965092489202437
2184, epoch_train_loss=3.0965092489202437
Epoch 2185
Epoch 2185 :: Batch 0/1
Batch Loss = 3.0962385677144066
2185, epoch_train_loss=3.0962385677144066
Epoch 2186
Epoch 2186 :: Batch 0/1
Batch Loss = 3.095956462557106
2186, epoch_train_loss=3.095956462557106
Epoch 2187
Epoch 2187 :: Batch 0/1
Batch Loss = 3.0956595213470885
2187, epoch_train_loss=3.0956595213470885
Epoch 2188
Epoch 2188 :: Batch 0/1
Batch Loss = 3.095343193749286
2188, epoch_train_loss=3.095343193749286
Epoch 2189
Epoch 2189 :: Batch 0/1
Batch Loss = 3.0950014185988746
2189, epoch_train_loss=3.0950014185988746
Epoch 2190
Epoch 2190 :: Batch 0/1
Batch Loss = 3.0946260934303607
2190, epoch_train_loss=3.0946260934303607
Epoch 2191
Epoch 2191 :: Batch 0/1
Batch Loss = 3.0942063452502606
2191, epoch_train_loss=3.0942063452502606
Epoch 2192
Epoch 2192 :: Batch 0/1
Batch Loss = 3.0937275612993687
2192, epoch_train_loss=3.0937275612993687
Epoch 2193
Epoch 2193 :: Batch 0/1
Batch Loss = 3.0931701746314406
2193, epoch_train_loss=3.0931701746314406
Epoch 2194
Epoch 2194 :: Batch 0/1
Batch Loss = 3.092508327943934
2194, epoch_train_loss=3.092508327943934
Epoch 2195
Epoch 2195 :: Batch 0/1
Batch Loss = 3.091708866463782
2195, epoch_train_loss=3.091708866463782
Epoch 2196
Epoch 2196 :: Batch 0/1
Batch Loss = 3.0907317653290716
2196, epoch_train_loss=3.0907317653290716
Epoch 2197
Epoch 2197 :: Batch 0/1
Batch Loss = 3.0895340220823124
2197, epoch_train_loss=3.0895340220823124
Epoch 2198
Epoch 2198 :: Batch 0/1
Batch Loss = 3.0880795410561888
2198, epoch_train_loss=3.0880795410561888
Epoch 2199
Epoch 2199 :: Batch 0/1
Batch Loss = 3.0863558830761586
2199, epoch_train_loss=3.0863558830761586
Epoch 2200
Epoch 2200 :: Batch 0/1
Batch Loss = 3.084393777886724
2200, epoch_train_loss=3.084393777886724
Epoch 2201
Epoch 2201 :: Batch 0/1
Batch Loss = 3.0822791444938846
2201, epoch_train_loss=3.0822791444938846
Epoch 2202
Epoch 2202 :: Batch 0/1
Batch Loss = 3.080145464261938
2202, epoch_train_loss=3.080145464261938
Epoch 2203
Epoch 2203 :: Batch 0/1
Batch Loss = 3.0781436204258887
2203, epoch_train_loss=3.0781436204258887
Epoch 2204
Epoch 2204 :: Batch 0/1
Batch Loss = 3.0763999003778837
2204, epoch_train_loss=3.0763999003778837
Epoch 2205
Epoch 2205 :: Batch 0/1
Batch Loss = 3.0749758184512346
2205, epoch_train_loss=3.0749758184512346
Epoch 2206
Epoch 2206 :: Batch 0/1
Batch Loss = 3.073843554831985
2206, epoch_train_loss=3.073843554831985
Epoch 2207
Epoch 2207 :: Batch 0/1
Batch Loss = 3.072887237851339
2207, epoch_train_loss=3.072887237851339
Epoch 2208
Epoch 2208 :: Batch 0/1
Batch Loss = 3.0719321203927796
2208, epoch_train_loss=3.0719321203927796
Epoch 2209
Epoch 2209 :: Batch 0/1
Batch Loss = 3.0707969462320794
2209, epoch_train_loss=3.0707969462320794
Epoch 2210
Epoch 2210 :: Batch 0/1
Batch Loss = 3.069351079366571
2210, epoch_train_loss=3.069351079366571
Epoch 2211
Epoch 2211 :: Batch 0/1
Batch Loss = 3.067559516812744
2211, epoch_train_loss=3.067559516812744
Epoch 2212
Epoch 2212 :: Batch 0/1
Batch Loss = 3.0654946750055743
2212, epoch_train_loss=3.0654946750055743
Epoch 2213
Epoch 2213 :: Batch 0/1
Batch Loss = 3.0633373270904563
2213, epoch_train_loss=3.0633373270904563
Epoch 2214
Epoch 2214 :: Batch 0/1
Batch Loss = 3.0613647897928313
2214, epoch_train_loss=3.0613647897928313
Epoch 2215
Epoch 2215 :: Batch 0/1
Batch Loss = 3.0607855038962914
2215, epoch_train_loss=3.0607855038962914
Epoch 2216
Epoch 2216 :: Batch 0/1
Batch Loss = 3.0757013428368136
2216, epoch_train_loss=3.0757013428368136
Epoch 2217
Epoch 2217 :: Batch 0/1
Batch Loss = 3.200267461054871
2217, epoch_train_loss=3.200267461054871
Epoch 2218
Epoch 2218 :: Batch 0/1
Batch Loss = 3.3077906396539056
2218, epoch_train_loss=3.3077906396539056
Epoch 2219
Epoch 2219 :: Batch 0/1
Batch Loss = 3.3736696143852276
2219, epoch_train_loss=3.3736696143852276
Epoch 2220
Epoch 2220 :: Batch 0/1
Batch Loss = 3.2373806025901324
2220, epoch_train_loss=3.2373806025901324
Epoch 2221
Epoch 2221 :: Batch 0/1
Batch Loss = 3.357675969248286
2221, epoch_train_loss=3.357675969248286
Epoch 2222
Epoch 2222 :: Batch 0/1
Batch Loss = 3.444666297214622
2222, epoch_train_loss=3.444666297214622
Epoch 2223
Epoch 2223 :: Batch 0/1
Batch Loss = 3.198119294593726
2223, epoch_train_loss=3.198119294593726
Epoch 2224
Epoch 2224 :: Batch 0/1
Batch Loss = 3.3887431790195457
2224, epoch_train_loss=3.3887431790195457
Epoch 2225
Epoch 2225 :: Batch 0/1
Batch Loss = 3.356520670653451
2225, epoch_train_loss=3.356520670653451
Epoch 2226
Epoch 2226 :: Batch 0/1
Batch Loss = 3.290759528895685
2226, epoch_train_loss=3.290759528895685
Epoch 2227
Epoch 2227 :: Batch 0/1
Batch Loss = 3.2832679296276037
2227, epoch_train_loss=3.2832679296276037
Epoch 2228
Epoch 2228 :: Batch 0/1
Batch Loss = 3.210779655936573
2228, epoch_train_loss=3.210779655936573
Epoch 2229
Epoch 2229 :: Batch 0/1
Batch Loss = 3.1731426859539855
2229, epoch_train_loss=3.1731426859539855
Epoch 2230
Epoch 2230 :: Batch 0/1
Batch Loss = 3.3127371241581502
2230, epoch_train_loss=3.3127371241581502
Epoch 2231
Epoch 2231 :: Batch 0/1
Batch Loss = 3.141207055374597
2231, epoch_train_loss=3.141207055374597
Epoch 2232
Epoch 2232 :: Batch 0/1
Batch Loss = 3.2802931639472694
2232, epoch_train_loss=3.2802931639472694
Epoch 2233
Epoch 2233 :: Batch 0/1
Batch Loss = 3.1955521272370335
2233, epoch_train_loss=3.1955521272370335
Epoch 2234
Epoch 2234 :: Batch 0/1
Batch Loss = 3.223795938947367
2234, epoch_train_loss=3.223795938947367
Epoch 2235
Epoch 2235 :: Batch 0/1
Batch Loss = 3.202364419522577
2235, epoch_train_loss=3.202364419522577
Epoch 2236
Epoch 2236 :: Batch 0/1
Batch Loss = 3.1450210069629487
2236, epoch_train_loss=3.1450210069629487
Epoch 2237
Epoch 2237 :: Batch 0/1
Batch Loss = 3.1978930204149285
2237, epoch_train_loss=3.1978930204149285
Epoch 2238
Epoch 2238 :: Batch 0/1
Batch Loss = 3.189133077829482
2238, epoch_train_loss=3.189133077829482
Epoch 2239
Epoch 2239 :: Batch 0/1
Batch Loss = 3.1543695814808608
2239, epoch_train_loss=3.1543695814808608
Epoch 2240
Epoch 2240 :: Batch 0/1
Batch Loss = 3.1580363119555526
2240, epoch_train_loss=3.1580363119555526
Epoch 2241
Epoch 2241 :: Batch 0/1
Batch Loss = 3.149404218438592
2241, epoch_train_loss=3.149404218438592
Epoch 2242
Epoch 2242 :: Batch 0/1
Batch Loss = 3.148722863058253
2242, epoch_train_loss=3.148722863058253
Epoch 2243
Epoch 2243 :: Batch 0/1
Batch Loss = 3.1576959652256407
2243, epoch_train_loss=3.1576959652256407
Epoch 2244
Epoch 2244 :: Batch 0/1
Batch Loss = 3.1342439037869694
2244, epoch_train_loss=3.1342439037869694
Epoch 2245
Epoch 2245 :: Batch 0/1
Batch Loss = 3.1513871360806953
2245, epoch_train_loss=3.1513871360806953
Epoch 2246
Epoch 2246 :: Batch 0/1
Batch Loss = 3.1192796749390435
2246, epoch_train_loss=3.1192796749390435
Epoch 2247
Epoch 2247 :: Batch 0/1
Batch Loss = 3.1335967643867804
2247, epoch_train_loss=3.1335967643867804
Epoch 2248
Epoch 2248 :: Batch 0/1
Batch Loss = 3.1238422165935438
2248, epoch_train_loss=3.1238422165935438
Epoch 2249
Epoch 2249 :: Batch 0/1
Batch Loss = 3.1202639462504305
2249, epoch_train_loss=3.1202639462504305
Epoch 2250
Epoch 2250 :: Batch 0/1
Batch Loss = 3.1316557035872643
2250, epoch_train_loss=3.1316557035872643
Epoch 2251
Epoch 2251 :: Batch 0/1
Batch Loss = 3.104127965747286
2251, epoch_train_loss=3.104127965747286
Epoch 2252
Epoch 2252 :: Batch 0/1
Batch Loss = 3.1105520509547215
2252, epoch_train_loss=3.1105520509547215
Epoch 2253
Epoch 2253 :: Batch 0/1
Batch Loss = 3.099392836247803
2253, epoch_train_loss=3.099392836247803
Epoch 2254
Epoch 2254 :: Batch 0/1
Batch Loss = 3.1055319754130113
2254, epoch_train_loss=3.1055319754130113
Epoch 2255
Epoch 2255 :: Batch 0/1
Batch Loss = 3.0937490078349543
2255, epoch_train_loss=3.0937490078349543
Epoch 2256
Epoch 2256 :: Batch 0/1
Batch Loss = 3.0973531943439503
2256, epoch_train_loss=3.0973531943439503
Epoch 2257
Epoch 2257 :: Batch 0/1
Batch Loss = 3.0930332163021155
2257, epoch_train_loss=3.0930332163021155
Epoch 2258
Epoch 2258 :: Batch 0/1
Batch Loss = 3.0843645773346684
2258, epoch_train_loss=3.0843645773346684
Epoch 2259
Epoch 2259 :: Batch 0/1
Batch Loss = 3.0867557930663962
2259, epoch_train_loss=3.0867557930663962
Epoch 2260
Epoch 2260 :: Batch 0/1
Batch Loss = 3.0832935111941233
2260, epoch_train_loss=3.0832935111941233
Epoch 2261
Epoch 2261 :: Batch 0/1
Batch Loss = 3.0862123912509087
2261, epoch_train_loss=3.0862123912509087
Epoch 2262
Epoch 2262 :: Batch 0/1
Batch Loss = 3.0817736464499808
2262, epoch_train_loss=3.0817736464499808
Epoch 2263
Epoch 2263 :: Batch 0/1
Batch Loss = 3.0827436854016628
2263, epoch_train_loss=3.0827436854016628
Epoch 2264
Epoch 2264 :: Batch 0/1
Batch Loss = 3.08222354012761
2264, epoch_train_loss=3.08222354012761
Epoch 2265
Epoch 2265 :: Batch 0/1
Batch Loss = 3.0758931268509535
2265, epoch_train_loss=3.0758931268509535
Epoch 2266
Epoch 2266 :: Batch 0/1
Batch Loss = 3.0782862267797193
2266, epoch_train_loss=3.0782862267797193
Epoch 2267
Epoch 2267 :: Batch 0/1
Batch Loss = 3.0771591614514264
2267, epoch_train_loss=3.0771591614514264
Epoch 2268
Epoch 2268 :: Batch 0/1
Batch Loss = 3.0737520088137917
2268, epoch_train_loss=3.0737520088137917
Epoch 2269
Epoch 2269 :: Batch 0/1
Batch Loss = 3.0754777399090294
2269, epoch_train_loss=3.0754777399090294
Epoch 2270
Epoch 2270 :: Batch 0/1
Batch Loss = 3.0747990876865816
2270, epoch_train_loss=3.0747990876865816
Epoch 2271
Epoch 2271 :: Batch 0/1
Batch Loss = 3.071318607102003
2271, epoch_train_loss=3.071318607102003
Epoch 2272
Epoch 2272 :: Batch 0/1
Batch Loss = 3.0704981970027463
2272, epoch_train_loss=3.0704981970027463
Epoch 2273
Epoch 2273 :: Batch 0/1
Batch Loss = 3.0709374957963806
2273, epoch_train_loss=3.0709374957963806
Epoch 2274
Epoch 2274 :: Batch 0/1
Batch Loss = 3.0701879888547614
2274, epoch_train_loss=3.0701879888547614
Epoch 2275
Epoch 2275 :: Batch 0/1
Batch Loss = 3.0683468531982636
2275, epoch_train_loss=3.0683468531982636
Epoch 2276
Epoch 2276 :: Batch 0/1
Batch Loss = 3.0673337500889444
2276, epoch_train_loss=3.0673337500889444
Epoch 2277
Epoch 2277 :: Batch 0/1
Batch Loss = 3.067329605728954
2277, epoch_train_loss=3.067329605728954
Epoch 2278
Epoch 2278 :: Batch 0/1
Batch Loss = 3.067004071560954
2278, epoch_train_loss=3.067004071560954
Epoch 2279
Epoch 2279 :: Batch 0/1
Batch Loss = 3.0657568614842825
2279, epoch_train_loss=3.0657568614842825
Epoch 2280
Epoch 2280 :: Batch 0/1
Batch Loss = 3.063781423045002
2280, epoch_train_loss=3.063781423045002
Epoch 2281
Epoch 2281 :: Batch 0/1
Batch Loss = 3.0623612684705868
2281, epoch_train_loss=3.0623612684705868
Epoch 2282
Epoch 2282 :: Batch 0/1
Batch Loss = 3.0610487826930086
2282, epoch_train_loss=3.0610487826930086
Epoch 2283
Epoch 2283 :: Batch 0/1
Batch Loss = 3.059953008020878
2283, epoch_train_loss=3.059953008020878
Epoch 2284
Epoch 2284 :: Batch 0/1
Batch Loss = 3.0598483928193883
2284, epoch_train_loss=3.0598483928193883
Epoch 2285
Epoch 2285 :: Batch 0/1
Batch Loss = 3.0638653073212847
2285, epoch_train_loss=3.0638653073212847
Epoch 2286
Epoch 2286 :: Batch 0/1
Batch Loss = 3.078205907919191
2286, epoch_train_loss=3.078205907919191
Epoch 2287
Epoch 2287 :: Batch 0/1
Batch Loss = 3.119108871784223
2287, epoch_train_loss=3.119108871784223
Epoch 2288
Epoch 2288 :: Batch 0/1
Batch Loss = 3.0557302408129376
2288, epoch_train_loss=3.0557302408129376
Epoch 2289
Epoch 2289 :: Batch 0/1
Batch Loss = 3.1074152454960164
2289, epoch_train_loss=3.1074152454960164
Epoch 2290
Epoch 2290 :: Batch 0/1
Batch Loss = 3.1658816723195624
2290, epoch_train_loss=3.1658816723195624
Epoch 2291
Epoch 2291 :: Batch 0/1
Batch Loss = 3.1637075192390123
2291, epoch_train_loss=3.1637075192390123
Epoch 2292
Epoch 2292 :: Batch 0/1
Batch Loss = 3.072656174799363
2292, epoch_train_loss=3.072656174799363
Epoch 2293
Epoch 2293 :: Batch 0/1
Batch Loss = 3.081743136209352
2293, epoch_train_loss=3.081743136209352
Epoch 2294
Epoch 2294 :: Batch 0/1
Batch Loss = 3.1479526760645236
2294, epoch_train_loss=3.1479526760645236
Epoch 2295
Epoch 2295 :: Batch 0/1
Batch Loss = 3.1416623212639894
2295, epoch_train_loss=3.1416623212639894
Epoch 2296
Epoch 2296 :: Batch 0/1
Batch Loss = 3.079906532075567
2296, epoch_train_loss=3.079906532075567
Epoch 2297
Epoch 2297 :: Batch 0/1
Batch Loss = 3.0770054518754724
2297, epoch_train_loss=3.0770054518754724
Epoch 2298
Epoch 2298 :: Batch 0/1
Batch Loss = 3.1203064370576366
2298, epoch_train_loss=3.1203064370576366
Epoch 2299
Epoch 2299 :: Batch 0/1
Batch Loss = 3.0929661360260954
2299, epoch_train_loss=3.0929661360260954
Epoch 2300
Epoch 2300 :: Batch 0/1
Batch Loss = 3.165020222639849
2300, epoch_train_loss=3.165020222639849
Epoch 2301
Epoch 2301 :: Batch 0/1
Batch Loss = 3.066299841133985
2301, epoch_train_loss=3.066299841133985
Epoch 2302
Epoch 2302 :: Batch 0/1
Batch Loss = 3.1004217610778246
2302, epoch_train_loss=3.1004217610778246
Epoch 2303
Epoch 2303 :: Batch 0/1
Batch Loss = 3.070294867289288
2303, epoch_train_loss=3.070294867289288
Epoch 2304
Epoch 2304 :: Batch 0/1
Batch Loss = 3.0665158932993943
2304, epoch_train_loss=3.0665158932993943
Epoch 2305
Epoch 2305 :: Batch 0/1
Batch Loss = 3.0886401540642634
2305, epoch_train_loss=3.0886401540642634
Epoch 2306
Epoch 2306 :: Batch 0/1
Batch Loss = 3.0611560953407784
2306, epoch_train_loss=3.0611560953407784
Epoch 2307
Epoch 2307 :: Batch 0/1
Batch Loss = 3.075198800656904
2307, epoch_train_loss=3.075198800656904
Epoch 2308
Epoch 2308 :: Batch 0/1
Batch Loss = 3.1005758443226044
2308, epoch_train_loss=3.1005758443226044
Epoch 2309
Epoch 2309 :: Batch 0/1
Batch Loss = 3.058520795317607
2309, epoch_train_loss=3.058520795317607
Epoch 2310
Epoch 2310 :: Batch 0/1
Batch Loss = 3.1188212289211785
2310, epoch_train_loss=3.1188212289211785
Epoch 2311
Epoch 2311 :: Batch 0/1
Batch Loss = 3.119478938170595
2311, epoch_train_loss=3.119478938170595
Epoch 2312
Epoch 2312 :: Batch 0/1
Batch Loss = 3.092451539212258
2312, epoch_train_loss=3.092451539212258
Epoch 2313
Epoch 2313 :: Batch 0/1
Batch Loss = 3.193610950375515
2313, epoch_train_loss=3.193610950375515
Epoch 2314
Epoch 2314 :: Batch 0/1
Batch Loss = 3.074122489192108
2314, epoch_train_loss=3.074122489192108
Epoch 2315
Epoch 2315 :: Batch 0/1
Batch Loss = 3.091041479655129
2315, epoch_train_loss=3.091041479655129
Epoch 2316
Epoch 2316 :: Batch 0/1
Batch Loss = 3.1092644690903124
2316, epoch_train_loss=3.1092644690903124
Epoch 2317
Epoch 2317 :: Batch 0/1
Batch Loss = 3.0701197487649723
2317, epoch_train_loss=3.0701197487649723
Epoch 2318
Epoch 2318 :: Batch 0/1
Batch Loss = 3.0609704973782605
2318, epoch_train_loss=3.0609704973782605
Epoch 2319
Epoch 2319 :: Batch 0/1
Batch Loss = 3.104727954916574
2319, epoch_train_loss=3.104727954916574
Epoch 2320
Epoch 2320 :: Batch 0/1
Batch Loss = 3.097493093645643
2320, epoch_train_loss=3.097493093645643
Epoch 2321
Epoch 2321 :: Batch 0/1
Batch Loss = 3.071963052813831
2321, epoch_train_loss=3.071963052813831
Epoch 2322
Epoch 2322 :: Batch 0/1
Batch Loss = 3.1600979721211018
2322, epoch_train_loss=3.1600979721211018
Epoch 2323
Epoch 2323 :: Batch 0/1
Batch Loss = 3.0649962634026022
2323, epoch_train_loss=3.0649962634026022
Epoch 2324
Epoch 2324 :: Batch 0/1
Batch Loss = 3.0817890063329885
2324, epoch_train_loss=3.0817890063329885
Epoch 2325
Epoch 2325 :: Batch 0/1
Batch Loss = 3.103971947202364
2325, epoch_train_loss=3.103971947202364
Epoch 2326
Epoch 2326 :: Batch 0/1
Batch Loss = 3.061198960584682
2326, epoch_train_loss=3.061198960584682
Epoch 2327
Epoch 2327 :: Batch 0/1
Batch Loss = 3.059378841722125
2327, epoch_train_loss=3.059378841722125
Epoch 2328
Epoch 2328 :: Batch 0/1
Batch Loss = 3.092138979282939
2328, epoch_train_loss=3.092138979282939
Epoch 2329
Epoch 2329 :: Batch 0/1
Batch Loss = 3.078938651759787
2329, epoch_train_loss=3.078938651759787
Epoch 2330
Epoch 2330 :: Batch 0/1
Batch Loss = 3.0517401812646314
2330, epoch_train_loss=3.0517401812646314
Epoch 2331
Epoch 2331 :: Batch 0/1
Batch Loss = 3.085688374547696
2331, epoch_train_loss=3.085688374547696
Epoch 2332
Epoch 2332 :: Batch 0/1
Batch Loss = 3.0967209663326862
2332, epoch_train_loss=3.0967209663326862
Epoch 2333
Epoch 2333 :: Batch 0/1
Batch Loss = 3.0504333350464488
2333, epoch_train_loss=3.0504333350464488
Epoch 2334
Epoch 2334 :: Batch 0/1
Batch Loss = 3.1098604898685567
2334, epoch_train_loss=3.1098604898685567
Epoch 2335
Epoch 2335 :: Batch 0/1
Batch Loss = 3.099338041322951
2335, epoch_train_loss=3.099338041322951
Epoch 2336
Epoch 2336 :: Batch 0/1
Batch Loss = 3.0590116883751786
2336, epoch_train_loss=3.0590116883751786
Epoch 2337
Epoch 2337 :: Batch 0/1
Batch Loss = 3.1682944176186068
2337, epoch_train_loss=3.1682944176186068
Epoch 2338
Epoch 2338 :: Batch 0/1
Batch Loss = 3.11759440000256
2338, epoch_train_loss=3.11759440000256
Epoch 2339
Epoch 2339 :: Batch 0/1
Batch Loss = 3.126898787096084
2339, epoch_train_loss=3.126898787096084
Epoch 2340
Epoch 2340 :: Batch 0/1
Batch Loss = 3.0822415561135945
2340, epoch_train_loss=3.0822415561135945
Epoch 2341
Epoch 2341 :: Batch 0/1
Batch Loss = 3.054954781190276
2341, epoch_train_loss=3.054954781190276
Epoch 2342
Epoch 2342 :: Batch 0/1
Batch Loss = 3.1026555334445027
2342, epoch_train_loss=3.1026555334445027
Epoch 2343
Epoch 2343 :: Batch 0/1
Batch Loss = 3.051459101494579
2343, epoch_train_loss=3.051459101494579
Epoch 2344
Epoch 2344 :: Batch 0/1
Batch Loss = 3.1196358989764628
2344, epoch_train_loss=3.1196358989764628
Epoch 2345
Epoch 2345 :: Batch 0/1
Batch Loss = 3.084649832902726
2345, epoch_train_loss=3.084649832902726
Epoch 2346
Epoch 2346 :: Batch 0/1
Batch Loss = 3.0651112211999765
2346, epoch_train_loss=3.0651112211999765
Epoch 2347
Epoch 2347 :: Batch 0/1
Batch Loss = 3.1428237071263925
2347, epoch_train_loss=3.1428237071263925
Epoch 2348
Epoch 2348 :: Batch 0/1
Batch Loss = 3.0599360684378887
2348, epoch_train_loss=3.0599360684378887
Epoch 2349
Epoch 2349 :: Batch 0/1
Batch Loss = 3.0699938295479847
2349, epoch_train_loss=3.0699938295479847
Epoch 2350
Epoch 2350 :: Batch 0/1
Batch Loss = 3.1102981847222626
2350, epoch_train_loss=3.1102981847222626
Epoch 2351
Epoch 2351 :: Batch 0/1
Batch Loss = 3.0534959433021602
2351, epoch_train_loss=3.0534959433021602
Epoch 2352
Epoch 2352 :: Batch 0/1
Batch Loss = 3.0616703727258177
2352, epoch_train_loss=3.0616703727258177
Epoch 2353
Epoch 2353 :: Batch 0/1
Batch Loss = 3.094158894885753
2353, epoch_train_loss=3.094158894885753
Epoch 2354
Epoch 2354 :: Batch 0/1
Batch Loss = 3.0588651812569414
2354, epoch_train_loss=3.0588651812569414
Epoch 2355
Epoch 2355 :: Batch 0/1
Batch Loss = 3.0503127091890856
2355, epoch_train_loss=3.0503127091890856
Epoch 2356
Epoch 2356 :: Batch 0/1
Batch Loss = 3.080414345752212
2356, epoch_train_loss=3.080414345752212
Epoch 2357
Epoch 2357 :: Batch 0/1
Batch Loss = 3.070621426207533
2357, epoch_train_loss=3.070621426207533
Epoch 2358
Epoch 2358 :: Batch 0/1
Batch Loss = 3.0460253768508947
2358, epoch_train_loss=3.0460253768508947
Epoch 2359
Epoch 2359 :: Batch 0/1
Batch Loss = 3.0672757542820857
2359, epoch_train_loss=3.0672757542820857
Epoch 2360
Epoch 2360 :: Batch 0/1
Batch Loss = 3.07893750252017
2360, epoch_train_loss=3.07893750252017
Epoch 2361
Epoch 2361 :: Batch 0/1
Batch Loss = 3.046286997886481
2361, epoch_train_loss=3.046286997886481
Epoch 2362
Epoch 2362 :: Batch 0/1
Batch Loss = 3.061336328215837
2362, epoch_train_loss=3.061336328215837
Epoch 2363
Epoch 2363 :: Batch 0/1
Batch Loss = 3.081430499002815
2363, epoch_train_loss=3.081430499002815
Epoch 2364
Epoch 2364 :: Batch 0/1
Batch Loss = 3.0463075229469396
2364, epoch_train_loss=3.0463075229469396
Epoch 2365
Epoch 2365 :: Batch 0/1
Batch Loss = 3.0615150818840964
2365, epoch_train_loss=3.0615150818840964
Epoch 2366
Epoch 2366 :: Batch 0/1
Batch Loss = 3.0829866271419224
2366, epoch_train_loss=3.0829866271419224
Epoch 2367
Epoch 2367 :: Batch 0/1
Batch Loss = 3.045304361051634
2367, epoch_train_loss=3.045304361051634
Epoch 2368
Epoch 2368 :: Batch 0/1
Batch Loss = 3.066348667917083
2368, epoch_train_loss=3.066348667917083
Epoch 2369
Epoch 2369 :: Batch 0/1
Batch Loss = 3.085013969786914
2369, epoch_train_loss=3.085013969786914
Epoch 2370
Epoch 2370 :: Batch 0/1
Batch Loss = 3.0439171437837897
2370, epoch_train_loss=3.0439171437837897
Epoch 2371
Epoch 2371 :: Batch 0/1
Batch Loss = 3.0740486253099886
2371, epoch_train_loss=3.0740486253099886
Epoch 2372
Epoch 2372 :: Batch 0/1
Batch Loss = 3.0846395822151913
2372, epoch_train_loss=3.0846395822151913
Epoch 2373
Epoch 2373 :: Batch 0/1
Batch Loss = 3.043097399050613
2373, epoch_train_loss=3.043097399050613
Epoch 2374
Epoch 2374 :: Batch 0/1
Batch Loss = 3.0800783884549365
2374, epoch_train_loss=3.0800783884549365
Epoch 2375
Epoch 2375 :: Batch 0/1
Batch Loss = 3.081775379118305
2375, epoch_train_loss=3.081775379118305
Epoch 2376
Epoch 2376 :: Batch 0/1
Batch Loss = 3.0426235240425297
2376, epoch_train_loss=3.0426235240425297
Epoch 2377
Epoch 2377 :: Batch 0/1
Batch Loss = 3.081281555976581
2377, epoch_train_loss=3.081281555976581
Epoch 2378
Epoch 2378 :: Batch 0/1
Batch Loss = 3.078975158538922
2378, epoch_train_loss=3.078975158538922
Epoch 2379
Epoch 2379 :: Batch 0/1
Batch Loss = 3.0421731588037004
2379, epoch_train_loss=3.0421731588037004
Epoch 2380
Epoch 2380 :: Batch 0/1
Batch Loss = 3.079779208357378
2380, epoch_train_loss=3.079779208357378
Epoch 2381
Epoch 2381 :: Batch 0/1
Batch Loss = 3.0773824543773496
2381, epoch_train_loss=3.0773824543773496
Epoch 2382
Epoch 2382 :: Batch 0/1
Batch Loss = 3.041562485114464
2382, epoch_train_loss=3.041562485114464
Epoch 2383
Epoch 2383 :: Batch 0/1
Batch Loss = 3.0760047801513095
2383, epoch_train_loss=3.0760047801513095
Epoch 2384
Epoch 2384 :: Batch 0/1
Batch Loss = 3.0771141187640576
2384, epoch_train_loss=3.0771141187640576
Epoch 2385
Epoch 2385 :: Batch 0/1
Batch Loss = 3.040999962114098
2385, epoch_train_loss=3.040999962114098
Epoch 2386
Epoch 2386 :: Batch 0/1
Batch Loss = 3.0711409725630996
2386, epoch_train_loss=3.0711409725630996
Epoch 2387
Epoch 2387 :: Batch 0/1
Batch Loss = 3.0779138286613197
2387, epoch_train_loss=3.0779138286613197
Epoch 2388
Epoch 2388 :: Batch 0/1
Batch Loss = 3.0406422521163345
2388, epoch_train_loss=3.0406422521163345
Epoch 2389
Epoch 2389 :: Batch 0/1
Batch Loss = 3.0660529776289818
2389, epoch_train_loss=3.0660529776289818
Epoch 2390
Epoch 2390 :: Batch 0/1
Batch Loss = 3.0789581579405207
2390, epoch_train_loss=3.0789581579405207
Epoch 2391
Epoch 2391 :: Batch 0/1
Batch Loss = 3.0405335050189866
2391, epoch_train_loss=3.0405335050189866
Epoch 2392
Epoch 2392 :: Batch 0/1
Batch Loss = 3.06136967889287
2392, epoch_train_loss=3.06136967889287
Epoch 2393
Epoch 2393 :: Batch 0/1
Batch Loss = 3.0795871151385477
2393, epoch_train_loss=3.0795871151385477
Epoch 2394
Epoch 2394 :: Batch 0/1
Batch Loss = 3.040865849809927
2394, epoch_train_loss=3.040865849809927
Epoch 2395
Epoch 2395 :: Batch 0/1
Batch Loss = 3.0555257330628733
2395, epoch_train_loss=3.0555257330628733
Epoch 2396
Epoch 2396 :: Batch 0/1
Batch Loss = 3.078886766054583
2396, epoch_train_loss=3.078886766054583
Epoch 2397
Epoch 2397 :: Batch 0/1
Batch Loss = 3.042562850832384
2397, epoch_train_loss=3.042562850832384
Epoch 2398
Epoch 2398 :: Batch 0/1
Batch Loss = 3.0472264063231975
2398, epoch_train_loss=3.0472264063231975
Epoch 2399
Epoch 2399 :: Batch 0/1
Batch Loss = 3.073536263571458
2399, epoch_train_loss=3.073536263571458
Epoch 2400
Epoch 2400 :: Batch 0/1
Batch Loss = 3.048979854098616
2400, epoch_train_loss=3.048979854098616
Epoch 2401
Epoch 2401 :: Batch 0/1
Batch Loss = 3.0386549186955705
2401, epoch_train_loss=3.0386549186955705
Epoch 2402
Epoch 2402 :: Batch 0/1
Batch Loss = 3.054753374271787
2402, epoch_train_loss=3.054753374271787
Epoch 2403
Epoch 2403 :: Batch 0/1
Batch Loss = 3.0617093914807003
2403, epoch_train_loss=3.0617093914807003
Epoch 2404
Epoch 2404 :: Batch 0/1
Batch Loss = 3.0437604838641974
2404, epoch_train_loss=3.0437604838641974
Epoch 2405
Epoch 2405 :: Batch 0/1
Batch Loss = 3.0376135068231447
2405, epoch_train_loss=3.0376135068231447
Epoch 2406
Epoch 2406 :: Batch 0/1
Batch Loss = 3.0470928479355193
2406, epoch_train_loss=3.0470928479355193
Epoch 2407
Epoch 2407 :: Batch 0/1
Batch Loss = 3.055709837433187
2407, epoch_train_loss=3.055709837433187
Epoch 2408
Epoch 2408 :: Batch 0/1
Batch Loss = 3.047132389563436
2408, epoch_train_loss=3.047132389563436
Epoch 2409
Epoch 2409 :: Batch 0/1
Batch Loss = 3.0375247375813177
2409, epoch_train_loss=3.0375247375813177
Epoch 2410
Epoch 2410 :: Batch 0/1
Batch Loss = 3.0375475885041157
2410, epoch_train_loss=3.0375475885041157
Epoch 2411
Epoch 2411 :: Batch 0/1
Batch Loss = 3.044609925533767
2411, epoch_train_loss=3.044609925533767
Epoch 2412
Epoch 2412 :: Batch 0/1
Batch Loss = 3.0493704751143786
2412, epoch_train_loss=3.0493704751143786
Epoch 2413
Epoch 2413 :: Batch 0/1
Batch Loss = 3.045437842305723
2413, epoch_train_loss=3.045437842305723
Epoch 2414
Epoch 2414 :: Batch 0/1
Batch Loss = 3.0382225242210796
2414, epoch_train_loss=3.0382225242210796
Epoch 2415
Epoch 2415 :: Batch 0/1
Batch Loss = 3.0351439700389666
2415, epoch_train_loss=3.0351439700389666
Epoch 2416
Epoch 2416 :: Batch 0/1
Batch Loss = 3.0362280715385395
2416, epoch_train_loss=3.0362280715385395
Epoch 2417
Epoch 2417 :: Batch 0/1
Batch Loss = 3.0400352586197688
2417, epoch_train_loss=3.0400352586197688
Epoch 2418
Epoch 2418 :: Batch 0/1
Batch Loss = 3.0442361357283936
2418, epoch_train_loss=3.0442361357283936
Epoch 2419
Epoch 2419 :: Batch 0/1
Batch Loss = 3.0439209523607955
2419, epoch_train_loss=3.0439209523607955
Epoch 2420
Epoch 2420 :: Batch 0/1
Batch Loss = 3.0408865765621544
2420, epoch_train_loss=3.0408865765621544
Epoch 2421
Epoch 2421 :: Batch 0/1
Batch Loss = 3.0369159181958363
2421, epoch_train_loss=3.0369159181958363
Epoch 2422
Epoch 2422 :: Batch 0/1
Batch Loss = 3.034526251859071
2422, epoch_train_loss=3.034526251859071
Epoch 2423
Epoch 2423 :: Batch 0/1
Batch Loss = 3.033345998013543
2423, epoch_train_loss=3.033345998013543
Epoch 2424
Epoch 2424 :: Batch 0/1
Batch Loss = 3.0329542939057736
2424, epoch_train_loss=3.0329542939057736
Epoch 2425
Epoch 2425 :: Batch 0/1
Batch Loss = 3.0331053304028472
2425, epoch_train_loss=3.0331053304028472
Epoch 2426
Epoch 2426 :: Batch 0/1
Batch Loss = 3.033836526234562
2426, epoch_train_loss=3.033836526234562
Epoch 2427
Epoch 2427 :: Batch 0/1
Batch Loss = 3.0355475253128312
2427, epoch_train_loss=3.0355475253128312
Epoch 2428
Epoch 2428 :: Batch 0/1
Batch Loss = 3.038348047419418
2428, epoch_train_loss=3.038348047419418
Epoch 2429
Epoch 2429 :: Batch 0/1
Batch Loss = 3.0430074651396986
2429, epoch_train_loss=3.0430074651396986
Epoch 2430
Epoch 2430 :: Batch 0/1
Batch Loss = 3.0451312855374355
2430, epoch_train_loss=3.0451312855374355
Epoch 2431
Epoch 2431 :: Batch 0/1
Batch Loss = 3.0462835438868576
2431, epoch_train_loss=3.0462835438868576
Epoch 2432
Epoch 2432 :: Batch 0/1
Batch Loss = 3.0417547496317083
2432, epoch_train_loss=3.0417547496317083
Epoch 2433
Epoch 2433 :: Batch 0/1
Batch Loss = 3.039133809265133
2433, epoch_train_loss=3.039133809265133
Epoch 2434
Epoch 2434 :: Batch 0/1
Batch Loss = 3.036828394312786
2434, epoch_train_loss=3.036828394312786
Epoch 2435
Epoch 2435 :: Batch 0/1
Batch Loss = 3.0369816731894224
2435, epoch_train_loss=3.0369816731894224
Epoch 2436
Epoch 2436 :: Batch 0/1
Batch Loss = 3.0379426862505254
2436, epoch_train_loss=3.0379426862505254
Epoch 2437
Epoch 2437 :: Batch 0/1
Batch Loss = 3.0417994476506425
2437, epoch_train_loss=3.0417994476506425
Epoch 2438
Epoch 2438 :: Batch 0/1
Batch Loss = 3.0441459691536705
2438, epoch_train_loss=3.0441459691536705
Epoch 2439
Epoch 2439 :: Batch 0/1
Batch Loss = 3.0479700471959523
2439, epoch_train_loss=3.0479700471959523
Epoch 2440
Epoch 2440 :: Batch 0/1
Batch Loss = 3.0436795134082253
2440, epoch_train_loss=3.0436795134082253
Epoch 2441
Epoch 2441 :: Batch 0/1
Batch Loss = 3.041926128363018
2441, epoch_train_loss=3.041926128363018
Epoch 2442
Epoch 2442 :: Batch 0/1
Batch Loss = 3.0385981157624156
2442, epoch_train_loss=3.0385981157624156
Epoch 2443
Epoch 2443 :: Batch 0/1
Batch Loss = 3.0392827323973797
2443, epoch_train_loss=3.0392827323973797
Epoch 2444
Epoch 2444 :: Batch 0/1
Batch Loss = 3.0397961512169003
2444, epoch_train_loss=3.0397961512169003
Epoch 2445
Epoch 2445 :: Batch 0/1
Batch Loss = 3.0443547711988455
2445, epoch_train_loss=3.0443547711988455
Epoch 2446
Epoch 2446 :: Batch 0/1
Batch Loss = 3.044170792986063
2446, epoch_train_loss=3.044170792986063
Epoch 2447
Epoch 2447 :: Batch 0/1
Batch Loss = 3.046689690214388
2447, epoch_train_loss=3.046689690214388
Epoch 2448
Epoch 2448 :: Batch 0/1
Batch Loss = 3.0414980205912485
2448, epoch_train_loss=3.0414980205912485
Epoch 2449
Epoch 2449 :: Batch 0/1
Batch Loss = 3.040907110339353
2449, epoch_train_loss=3.040907110339353
Epoch 2450
Epoch 2450 :: Batch 0/1
Batch Loss = 3.0385226823457483
2450, epoch_train_loss=3.0385226823457483
Epoch 2451
Epoch 2451 :: Batch 0/1
Batch Loss = 3.0411541731947915
2451, epoch_train_loss=3.0411541731947915
Epoch 2452
Epoch 2452 :: Batch 0/1
Batch Loss = 3.0412507394203154
2452, epoch_train_loss=3.0412507394203154
Epoch 2453
Epoch 2453 :: Batch 0/1
Batch Loss = 3.04597945097045
2453, epoch_train_loss=3.04597945097045
Epoch 2454
Epoch 2454 :: Batch 0/1
Batch Loss = 3.042315788052358
2454, epoch_train_loss=3.042315788052358
Epoch 2455
Epoch 2455 :: Batch 0/1
Batch Loss = 3.043247993079912
2455, epoch_train_loss=3.043247993079912
Epoch 2456
Epoch 2456 :: Batch 0/1
Batch Loss = 3.03888033548169
2456, epoch_train_loss=3.03888033548169
Epoch 2457
Epoch 2457 :: Batch 0/1
Batch Loss = 3.040270539502048
2457, epoch_train_loss=3.040270539502048
Epoch 2458
Epoch 2458 :: Batch 0/1
Batch Loss = 3.0388985626542304
2458, epoch_train_loss=3.0388985626542304
Epoch 2459
Epoch 2459 :: Batch 0/1
Batch Loss = 3.0431989216695143
2459, epoch_train_loss=3.0431989216695143
Epoch 2460
Epoch 2460 :: Batch 0/1
Batch Loss = 3.041039529508045
2460, epoch_train_loss=3.041039529508045
Epoch 2461
Epoch 2461 :: Batch 0/1
Batch Loss = 3.044125440469007
2461, epoch_train_loss=3.044125440469007
Epoch 2462
Epoch 2462 :: Batch 0/1
Batch Loss = 3.0391803305400735
2462, epoch_train_loss=3.0391803305400735
Epoch 2463
Epoch 2463 :: Batch 0/1
Batch Loss = 3.04039788225734
2463, epoch_train_loss=3.04039788225734
Epoch 2464
Epoch 2464 :: Batch 0/1
Batch Loss = 3.037518122330385
2464, epoch_train_loss=3.037518122330385
Epoch 2465
Epoch 2465 :: Batch 0/1
Batch Loss = 3.0409498692932257
2465, epoch_train_loss=3.0409498692932257
Epoch 2466
Epoch 2466 :: Batch 0/1
Batch Loss = 3.039065938556368
2466, epoch_train_loss=3.039065938556368
Epoch 2467
Epoch 2467 :: Batch 0/1
Batch Loss = 3.043293687475381
2467, epoch_train_loss=3.043293687475381
Epoch 2468
Epoch 2468 :: Batch 0/1
Batch Loss = 3.03878225385687
2468, epoch_train_loss=3.03878225385687
Epoch 2469
Epoch 2469 :: Batch 0/1
Batch Loss = 3.040762889515517
2469, epoch_train_loss=3.040762889515517
Epoch 2470
Epoch 2470 :: Batch 0/1
Batch Loss = 3.0367584210990337
2470, epoch_train_loss=3.0367584210990337
Epoch 2471
Epoch 2471 :: Batch 0/1
Batch Loss = 3.0396026174524895
2471, epoch_train_loss=3.0396026174524895
Epoch 2472
Epoch 2472 :: Batch 0/1
Batch Loss = 3.037202612261122
2472, epoch_train_loss=3.037202612261122
Epoch 2473
Epoch 2473 :: Batch 0/1
Batch Loss = 3.0415993881909644
2473, epoch_train_loss=3.0415993881909644
Epoch 2474
Epoch 2474 :: Batch 0/1
Batch Loss = 3.037728485158763
2474, epoch_train_loss=3.037728485158763
Epoch 2475
Epoch 2475 :: Batch 0/1
Batch Loss = 3.0407715333087646
2475, epoch_train_loss=3.0407715333087646
Epoch 2476
Epoch 2476 :: Batch 0/1
Batch Loss = 3.0361490824248385
2476, epoch_train_loss=3.0361490824248385
Epoch 2477
Epoch 2477 :: Batch 0/1
Batch Loss = 3.0388375585144107
2477, epoch_train_loss=3.0388375585144107
Epoch 2478
Epoch 2478 :: Batch 0/1
Batch Loss = 3.035678895690137
2478, epoch_train_loss=3.035678895690137
Epoch 2479
Epoch 2479 :: Batch 0/1
Batch Loss = 3.0398465309142773
2479, epoch_train_loss=3.0398465309142773
Epoch 2480
Epoch 2480 :: Batch 0/1
Batch Loss = 3.03632042777867
2480, epoch_train_loss=3.03632042777867
Epoch 2481
Epoch 2481 :: Batch 0/1
Batch Loss = 3.040198557637046
2481, epoch_train_loss=3.040198557637046
Epoch 2482
Epoch 2482 :: Batch 0/1
Batch Loss = 3.035400394780027
2482, epoch_train_loss=3.035400394780027
Epoch 2483
Epoch 2483 :: Batch 0/1
Batch Loss = 3.0383307004351203
2483, epoch_train_loss=3.0383307004351203
Epoch 2484
Epoch 2484 :: Batch 0/1
Batch Loss = 3.0344597566571174
2484, epoch_train_loss=3.0344597566571174
Epoch 2485
Epoch 2485 :: Batch 0/1
Batch Loss = 3.0383856032929093
2485, epoch_train_loss=3.0383856032929093
Epoch 2486
Epoch 2486 :: Batch 0/1
Batch Loss = 3.034834105256578
2486, epoch_train_loss=3.034834105256578
Epoch 2487
Epoch 2487 :: Batch 0/1
Batch Loss = 3.0391746212188595
2487, epoch_train_loss=3.0391746212188595
Epoch 2488
Epoch 2488 :: Batch 0/1
Batch Loss = 3.0344350663384874
2488, epoch_train_loss=3.0344350663384874
Epoch 2489
Epoch 2489 :: Batch 0/1
Batch Loss = 3.037839124417453
2489, epoch_train_loss=3.037839124417453
Epoch 2490
Epoch 2490 :: Batch 0/1
Batch Loss = 3.0334084539705013
2490, epoch_train_loss=3.0334084539705013
Epoch 2491
Epoch 2491 :: Batch 0/1
Batch Loss = 3.03723357995965
2491, epoch_train_loss=3.03723357995965
Epoch 2492
Epoch 2492 :: Batch 0/1
Batch Loss = 3.0334111864879767
2492, epoch_train_loss=3.0334111864879767
Epoch 2493
Epoch 2493 :: Batch 0/1
Batch Loss = 3.037942108006114
2493, epoch_train_loss=3.037942108006114
Epoch 2494
Epoch 2494 :: Batch 0/1
Batch Loss = 3.033296557228655
2494, epoch_train_loss=3.033296557228655
Epoch 2495
Epoch 2495 :: Batch 0/1
Batch Loss = 3.0372154316822773
2495, epoch_train_loss=3.0372154316822773
Epoch 2496
Epoch 2496 :: Batch 0/1
Batch Loss = 3.0324021860175274
2496, epoch_train_loss=3.0324021860175274
Epoch 2497
Epoch 2497 :: Batch 0/1
Batch Loss = 3.036289874364374
2497, epoch_train_loss=3.036289874364374
Epoch 2498
Epoch 2498 :: Batch 0/1
Batch Loss = 3.0320970926692135
2498, epoch_train_loss=3.0320970926692135
Epoch 2499
Epoch 2499 :: Batch 0/1
Batch Loss = 3.036690217227669
2499, epoch_train_loss=3.036690217227669
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting "B3LYP_WITH_VWN5 = True" in pyscf_conf.py
  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01fbc40> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01fbc40> in UKS object of <class 'pyscf.dft.uks.UKS'>
<pyscf.gto.mole.Mole object at 0x7ffeb01fbc40> [['P', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb01fafe0> [['N', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb01fb910> [['H', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb01fbd00> [['Li', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb01fbf70> [['O', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb01fbee0> [['Cl', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb01f8f10> [['Al', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb01fb010> [['S', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb01fb1c0> [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb01fb400> [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb01f8850> [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb01f86d0> [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb01f82e0> [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb01f89a0> [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb01f9210> [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb01f8070> [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb01f9270> [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb01f9180> [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb01f8b80> [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb01f9de0> [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb01f9a50> [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb01f97b0> [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb01fae00> [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb01faa70> [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb01faa10> [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]] 5
<pyscf.gto.mole.Mole object at 0x7ffeb01f81c0> [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb01f8040> [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]] 5
mol:  [['P', array([0., 0., 0.])]]
converged SCF energy = -341.104145992717  <S^2> = 3.7502984  2S+1 = 4.0001492
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01fafe0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01fafe0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-3.48053232e-03 -9.22981806e-04 -2.09507924e-03 ... -1.11294850e+01
 -1.11294850e+01 -1.11294850e+01] = SCAN,
rho_a.shape=(6, 6320), rho_b.shape=(6, 6320)
fxc_a.shape=(6320,), fxc_b.shape=(6320,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12640), fxc.shape=(12640,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(2, 6320, 2)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12640, 2)
rho_filt.shape=(12640,)
mol:  [['N', array([0., 0., 0.])]]
converged SCF energy = -54.5289742046674  <S^2> = 3.7524945  2S+1 = 4.0012471
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01fb910> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01fb910> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5016, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.10256797e-03 -5.98013179e-04 -6.71209617e-05 ... -5.03581543e+00
 -5.03581543e+00 -5.03581543e+00] = SCAN,
rho_a.shape=(6, 5016), rho_b.shape=(6, 5016)
fxc_a.shape=(5016,), fxc_b.shape=(5016,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 10032), fxc.shape=(10032,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5016, 22)
rho shape (4, 4, 2, 5016)
rho_filt shape: (5016,)
get descriptors tdrho.shape=(2, 5016, 2)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(10032, 2)
rho_filt.shape=(10032,)
mol:  [['H', array([0., 0., 0.])]]
converged SCF energy = -0.499812984008539  <S^2> = 0.75  2S+1 = 2
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01fbd00> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01fbd00> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 7, 7)
ao.shape (10, 2440, 7)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.59173730e-03 -7.67300285e-04 -6.09330303e-05 ... -7.27736608e-01
 -7.27736608e-01 -7.27736608e-01] = SCAN,
rho_a.shape=(6, 2440), rho_b.shape=(6, 2440)
fxc_a.shape=(2440,), fxc_b.shape=(2440,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 2440), fxc.shape=(2440,)
get_data, dm shape = (2, 7, 7)
ao_eval.shape=(4, 2440, 7)
rho shape (4, 4, 2, 2440)
rho_filt shape: (2440,)
get descriptors tdrho.shape=(2, 2440, 2)
mol:  [['Li', array([0., 0., 0.])]]
converged SCF energy = -7.46006188627842  <S^2> = 0.75000049  2S+1 = 2.0000005
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01fbf70> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01fbf70> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 4592, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-6.71507910e-03 -1.45299376e-03 -1.45299376e-03 ... -1.46930969e-02
 -2.05021258e+00 -2.05021258e+00] = SCAN,
rho_a.shape=(6, 4592), rho_b.shape=(6, 4592)
fxc_a.shape=(4592,), fxc_b.shape=(4592,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 9184), fxc.shape=(9184,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 4592, 22)
rho shape (4, 4, 2, 4592)
rho_filt shape: (4592,)
get descriptors tdrho.shape=(2, 4592, 2)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(9184, 2)
rho_filt.shape=(9184,)
mol:  [['O', array([0., 0., 0.])]]
converged SCF energy = -75.0033779898163  <S^2> = 2.0027451  2S+1 = 3.0018295
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01fbee0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01fbee0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5040, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-7.82967892e-04 -1.36569375e-04 -6.87281809e-06 ... -5.78388653e+00
 -5.78388653e+00 -5.78388653e+00] = SCAN,
rho_a.shape=(6, 5040), rho_b.shape=(6, 5040)
fxc_a.shape=(5040,), fxc_b.shape=(5040,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 10080), fxc.shape=(10080,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5040, 22)
rho shape (4, 4, 2, 5040)
rho_filt shape: (5040,)
get descriptors tdrho.shape=(2, 5040, 2)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(10080, 2)
rho_filt.shape=(10080,)
mol:  [['Cl', array([0., 0., 0.])]]
converged SCF energy = -459.957577125462  <S^2> = 0.75161942  2S+1 = 2.0016188
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f8f10> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f8f10> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6152, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-5.04351627e-04 -9.37831486e-04 -3.26514884e-04 ... -1.26646370e+01
 -1.26646370e+01 -1.26646370e+01] = SCAN,
rho_a.shape=(6, 6152), rho_b.shape=(6, 6152)
fxc_a.shape=(6152,), fxc_b.shape=(6152,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12304), fxc.shape=(12304,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6152, 30)
rho shape (4, 4, 2, 6152)
rho_filt shape: (6152,)
get descriptors tdrho.shape=(2, 6152, 2)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12304, 2)
rho_filt.shape=(12304,)
mol:  [['Al', array([0., 0., 0.])]]
converged SCF energy = -242.226560996606  <S^2> = 0.75226414  2S+1 = 2.0022629
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01fb010> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01fb010> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6088, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.02206819 -0.01402453 -0.00706429 ... -0.00012851 -0.0014074
 -0.00010362] = SCAN,
rho_a.shape=(6, 6088), rho_b.shape=(6, 6088)
fxc_a.shape=(6088,), fxc_b.shape=(6088,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12176), fxc.shape=(12176,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6088, 30)
rho shape (4, 4, 2, 6088)
rho_filt shape: (6088,)
get descriptors tdrho.shape=(2, 6088, 2)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12176, 2)
rho_filt.shape=(12176,)
mol:  [['S', array([0., 0., 0.])]]
converged SCF energy = -397.938786815927  <S^2> = 2.0022329  2S+1 = 3.0014882
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01fb1c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01fb1c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.08992216e-03 -9.66918801e-04 -1.00386860e-03 ... -1.18982463e+01
 -1.18982463e+01 -1.18982463e+01] = SCAN,
rho_a.shape=(6, 6320), rho_b.shape=(6, 6320)
fxc_a.shape=(6320,), fxc_b.shape=(6320,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12640), fxc.shape=(12640,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(2, 6320, 2)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12640, 2)
rho_filt.shape=(12640,)
mol:  [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]]
converged SCF energy = -1.16580491182912  <S^2> = 0  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01fb400> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01fb400> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 14, 14)
ao.shape (10, 4776, 14)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.31556415e-04 -9.72662915e-06 -3.66768501e-04 ... -5.54165573e-01
 -5.54165573e-01 -5.54165573e-01] = SCAN,
rho_a.shape=(6, 4776), rho_b.shape=(6, 4776)
fxc_a.shape=(4776,), fxc_b.shape=(4776,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 4776), fxc.shape=(4776,)
get_data, dm shape = (2, 14, 14)
ao_eval.shape=(4, 4776, 14)
rho shape (4, 4, 2, 4776)
rho_filt shape: (4776,)
get descriptors tdrho.shape=(2, 4776, 2)
mol:  [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]]
converged SCF energy = -109.439263799566  <S^2> = 8.8817842e-15  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f8850> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f8850> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9848, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-9.68469910e-05 -9.84742277e-04 -2.59676148e-04 ... -2.39626668e-05
 -2.39626668e-05 -9.68469910e-05] = SCAN,
rho_a.shape=(6, 9848), rho_b.shape=(6, 9848)
fxc_a.shape=(9848,), fxc_b.shape=(9848,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9848), fxc.shape=(9848,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9848, 44)
rho shape (4, 4, 2, 9848)
rho_filt shape: (9848,)
get descriptors tdrho.shape=(2, 9848, 2)
mol:  [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]]
converged SCF energy = -107.339357395734  <S^2> = 2.6645353e-15  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f86d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f86d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9752, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.04987750e-03 -6.68953858e-04 -8.57556270e-04 ... -1.07485583e-03
 -8.01425698e-01 -8.01425698e-01] = SCAN,
rho_a.shape=(6, 9752), rho_b.shape=(6, 9752)
fxc_a.shape=(9752,), fxc_b.shape=(9752,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9752), fxc.shape=(9752,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9752, 44)
rho shape (4, 4, 2, 9752)
rho_filt shape: (9752,)
get descriptors tdrho.shape=(2, 9752, 2)
mol:  [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]]
converged SCF energy = -93.3377924465133  <S^2> = 4.0073012e-10  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f82e0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f82e0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 12256, 51)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.97917285e-04 -2.54412366e-05 -3.15182243e-05 ... -6.37386500e-01
 -6.37386500e-01 -6.37386500e-01] = SCAN,
rho_a.shape=(6, 12256), rho_b.shape=(6, 12256)
fxc_a.shape=(12256,), fxc_b.shape=(12256,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12256), fxc.shape=(12256,)
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 12256, 51)
rho shape (4, 4, 2, 12256)
rho_filt shape: (12256,)
get descriptors tdrho.shape=(2, 12256, 2)
mol:  [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]]
converged SCF energy = -188.456965322844  <S^2> = 1.2434498e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f89a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f89a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 66, 66)
ao.shape (10, 14920, 66)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.50217115e-04 -2.07520066e-04 -9.23619896e-04 ... -2.74295208e-06
 -4.27559894e+00 -4.27559894e+00] = SCAN,
rho_a.shape=(6, 14920), rho_b.shape=(6, 14920)
fxc_a.shape=(14920,), fxc_b.shape=(14920,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 14920), fxc.shape=(14920,)
get_data, dm shape = (2, 66, 66)
ao_eval.shape=(4, 14920, 66)
rho shape (4, 4, 2, 14920)
rho_filt shape: (14920,)
get descriptors tdrho.shape=(2, 14920, 2)
mol:  [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]]
converged SCF energy = -920.00560888896  <S^2> = 5.0093263e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f9210> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f9210> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12208, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00155834 -0.00091688 -0.00215831 ... -0.00091688 -0.41618507
 -0.41618507] = SCAN,
rho_a.shape=(6, 12208), rho_b.shape=(6, 12208)
fxc_a.shape=(12208,), fxc_b.shape=(12208,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12208), fxc.shape=(12208,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12208, 60)
rho shape (4, 4, 2, 12208)
rho_filt shape: (12208,)
get descriptors tdrho.shape=(2, 12208, 2)
mol:  [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]]
converged SCF energy = -199.394370591172  <S^2> = 1.1546319e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f8070> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f8070> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9824, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.92948614e-04 -1.95198688e-05 -1.16699802e-03 ... -4.89378340e-01
 -4.89378340e-01 -4.89378340e-01] = SCAN,
rho_a.shape=(6, 9824), rho_b.shape=(6, 9824)
fxc_a.shape=(9824,), fxc_b.shape=(9824,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9824), fxc.shape=(9824,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9824, 44)
rho shape (4, 4, 2, 9824)
rho_filt shape: (9824,)
get descriptors tdrho.shape=(2, 9824, 2)
mol:  [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]]
converged SCF energy = -150.214894466814  <S^2> = 1.0018598  2S+1 = 2.2377309
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f9270> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f9270> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9912, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-3.38737750e-04 -5.76881261e-05 -2.88355696e-06 ... -6.59150650e-01
 -6.59150650e-01 -6.59150650e-01] = SCAN,
rho_a.shape=(6, 9912), rho_b.shape=(6, 9912)
fxc_a.shape=(9912,), fxc_b.shape=(9912,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9912), fxc.shape=(9912,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9912, 44)
rho shape (4, 4, 2, 9912)
rho_filt shape: (9912,)
get descriptors tdrho.shape=(2, 9912, 2)
mol:  [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]]
converged SCF energy = -77.2435048346374  <S^2> = 8.8817842e-15  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f9180> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f9180> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 58, 58)
ao.shape (10, 15208, 58)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.83270456e-05 -8.83270456e-05 -9.75839850e-04 ... -3.46719667e-05
 -3.31708644e-05 -3.31708644e-05] = SCAN,
rho_a.shape=(6, 15208), rho_b.shape=(6, 15208)
fxc_a.shape=(15208,), fxc_b.shape=(15208,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 15208), fxc.shape=(15208,)
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15208, 58)
rho shape (4, 4, 2, 15208)
rho_filt shape: (15208,)
get descriptors tdrho.shape=(2, 15208, 2)
mol:  [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]]
converged SCF energy = -113.221335689652  <S^2> = 6.6613381e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f8b80> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f8b80> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 10040, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-5.37000578e-04 -8.55494549e-04 -2.46853288e-03 ... -7.34251999e-01
 -7.34251999e-01 -7.34251999e-01] = SCAN,
rho_a.shape=(6, 10040), rho_b.shape=(6, 10040)
fxc_a.shape=(10040,), fxc_b.shape=(10040,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 10040), fxc.shape=(10040,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 10040, 44)
rho shape (4, 4, 2, 10040)
rho_filt shape: (10040,)
get descriptors tdrho.shape=(2, 10040, 2)
mol:  [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]]
converged SCF energy = -460.624592374078  <S^2> = 6.5725203e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f9de0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f9de0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 37, 37)
ao.shape (10, 8552, 37)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.38161177e-04 -1.81188367e-05 -2.37300299e-05 ... -2.83738108e+00
 -2.83738108e+00 -2.83738108e+00] = SCAN,
rho_a.shape=(6, 8552), rho_b.shape=(6, 8552)
fxc_a.shape=(8552,), fxc_b.shape=(8552,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 8552), fxc.shape=(8552,)
get_data, dm shape = (2, 37, 37)
ao_eval.shape=(4, 8552, 37)
rho shape (4, 4, 2, 8552)
rho_filt shape: (8552,)
get descriptors tdrho.shape=(2, 8552, 2)
mol:  [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]]
converged SCF energy = -8.04458854018922  <S^2> = 7.8825835e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f9a50> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f9a50> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 29, 29)
ao.shape (10, 6936, 29)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00113445 -0.00118624 -0.00118624 ... -0.48434639 -0.48434639
 -0.48434639] = SCAN,
rho_a.shape=(6, 6936), rho_b.shape=(6, 6936)
fxc_a.shape=(6936,), fxc_b.shape=(6936,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 6936), fxc.shape=(6936,)
get_data, dm shape = (2, 29, 29)
ao_eval.shape=(4, 6936, 29)
rho shape (4, 4, 2, 6936)
rho_filt shape: (6936,)
get descriptors tdrho.shape=(2, 6936, 2)
mol:  [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]]
converged SCF energy = -324.340512506578  <S^2> = 1.5866419e-11  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f97b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f97b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 11536, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00297935 -0.00297935 -0.00407089 ... -0.00297935 -0.00297935
 -0.00407089] = SCAN,
rho_a.shape=(6, 11536), rho_b.shape=(6, 11536)
fxc_a.shape=(11536,), fxc_b.shape=(11536,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 11536), fxc.shape=(11536,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 11536, 60)
rho shape (4, 4, 2, 11536)
rho_filt shape: (11536,)
get descriptors tdrho.shape=(2, 11536, 2)
mol:  [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]]
converged SCF energy = -1622.57507845815  <S^2> = 8.2422957e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01fae00> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01fae00> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 120, 120)
ao.shape (10, 24512, 120)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.61400996e-04 -4.90484900e-04 -2.56451718e-03 ... -9.59296113e+00
 -9.59296113e+00 -9.59296113e+00] = SCAN,
rho_a.shape=(6, 24512), rho_b.shape=(6, 24512)
fxc_a.shape=(24512,), fxc_b.shape=(24512,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 24512), fxc.shape=(24512,)
get_data, dm shape = (2, 120, 120)
ao_eval.shape=(4, 24512, 120)
rho shape (4, 4, 2, 24512)
rho_filt shape: (24512,)
get descriptors tdrho.shape=(2, 24512, 2)
mol:  [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]]
converged SCF energy = -342.979728469575  <S^2> = 2.5393021e-11  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01faa70> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01faa70> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 13096, 51)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.28637920e-03 -4.32383300e-04 -3.74057272e-05 ... -1.91722770e+00
 -1.91722770e+00 -1.91722770e+00] = SCAN,
rho_a.shape=(6, 13096), rho_b.shape=(6, 13096)
fxc_a.shape=(13096,), fxc_b.shape=(13096,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 13096), fxc.shape=(13096,)
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 13096, 51)
rho shape (4, 4, 2, 13096)
rho_filt shape: (13096,)
get descriptors tdrho.shape=(2, 13096, 2)
mol:  [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]]
converged SCF energy = -578.565336532766  <S^2> = 1.0034706  2S+1 = 2.23917
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01faa10> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01faa10> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12384, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.59860160e-04 -2.60557229e-04 -2.59209644e-04 ... -3.86943883e-01
 -3.86943883e-01 -3.86943883e-01] = SCAN,
rho_a.shape=(6, 12384), rho_b.shape=(6, 12384)
fxc_a.shape=(12384,), fxc_b.shape=(12384,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12384), fxc.shape=(12384,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12384, 60)
rho shape (4, 4, 2, 12384)
rho_filt shape: (12384,)
get descriptors tdrho.shape=(2, 12384, 2)
mol:  [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]]
converged SCF energy = -40.4598214864076  <S^2> = 3.2063241e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f81c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f81c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 50, 50)
ao.shape (10, 13936, 50)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.68439986e-04 -2.42462569e-04 -1.69927031e-05 ... -2.55230307e-05
 -2.55230307e-05 -2.55230307e-05] = SCAN,
rho_a.shape=(6, 13936), rho_b.shape=(6, 13936)
fxc_a.shape=(13936,), fxc_b.shape=(13936,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 13936), fxc.shape=(13936,)
get_data, dm shape = (2, 50, 50)
ao_eval.shape=(4, 13936, 50)
rho shape (4, 4, 2, 13936)
rho_filt shape: (13936,)
get descriptors tdrho.shape=(2, 13936, 2)
mol:  [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]]
converged SCF energy = -39.0756147483504  <S^2> = 6.1985972e-12  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f8040> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f8040> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 36, 36)
ao.shape (10, 9656, 36)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-7.67688751e-04 -4.57393214e-05 -2.02834191e-04 ... -1.14928924e+00
 -1.14928924e+00 -1.14928924e+00] = SCAN,
rho_a.shape=(6, 9656), rho_b.shape=(6, 9656)
fxc_a.shape=(9656,), fxc_b.shape=(9656,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9656), fxc.shape=(9656,)
get_data, dm shape = (2, 36, 36)
ao_eval.shape=(4, 9656, 36)
rho shape (4, 4, 2, 9656)
rho_filt shape: (9656,)
get descriptors tdrho.shape=(2, 9656, 2)
mol:  [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]]
converged SCF energy = -291.719272437819  <S^2> = 1.3155699e-11  2S+1 = 1
New DM shape: (2, 58, 58)
ao.shape (10, 15256, 58)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.33850535e-04 -2.34903029e-04 -1.75623665e-05 ... -1.92891112e-05
 -1.92891112e-05 -1.92891112e-05] = SCAN,
rho_a.shape=(6, 15256), rho_b.shape=(6, 15256)
fxc_a.shape=(15256,), fxc_b.shape=(15256,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 15256), fxc.shape=(15256,)
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15256, 58)
rho shape (4, 4, 2, 15256)
rho_filt shape: (15256,)
get descriptors tdrho.shape=(2, 15256, 2)
localnet.spin_scaling: concatenating the data
first data shape = (10940, 2)
concatenated: tdrho.shape=(271683, 2)
PRE NAN FILT: tFxc.shape=(271683,), tdrho.shape=(271683, 2)
nan_filt_rho.shape=(271683,)
nan_filt_fxc.shape=(271683,)
tFxc.shape=(271683,), tdrho.shape=(271683, 2)
inp[0].shape = (271683, 1)
Epoch 0
Epoch 0 :: Batch 0/1
Batch Loss = 5.007322712275149
0, epoch_train_loss=5.007322712275149
Epoch 1
Epoch 1 :: Batch 0/1
Batch Loss = 4.725931369358813
1, epoch_train_loss=4.725931369358813
Epoch 2
Epoch 2 :: Batch 0/1
Batch Loss = 4.468119621708431
2, epoch_train_loss=4.468119621708431
Epoch 3
Epoch 3 :: Batch 0/1
Batch Loss = 4.555171173724723
3, epoch_train_loss=4.555171173724723
Epoch 4
Epoch 4 :: Batch 0/1
Batch Loss = 4.352907316237606
4, epoch_train_loss=4.352907316237606
Epoch 5
Epoch 5 :: Batch 0/1
Batch Loss = 4.340992423860173
5, epoch_train_loss=4.340992423860173
Epoch 6
Epoch 6 :: Batch 0/1
Batch Loss = 4.323959301602664
6, epoch_train_loss=4.323959301602664
Epoch 7
Epoch 7 :: Batch 0/1
Batch Loss = 4.245180346194352
7, epoch_train_loss=4.245180346194352
Epoch 8
Epoch 8 :: Batch 0/1
Batch Loss = 4.058470793709646
8, epoch_train_loss=4.058470793709646
Epoch 9
Epoch 9 :: Batch 0/1
Batch Loss = 3.8770248975133663
9, epoch_train_loss=3.8770248975133663
Epoch 10
Epoch 10 :: Batch 0/1
Batch Loss = 3.735937102757979
10, epoch_train_loss=3.735937102757979
Epoch 11
Epoch 11 :: Batch 0/1
Batch Loss = 3.5929874591399455
11, epoch_train_loss=3.5929874591399455
Epoch 12
Epoch 12 :: Batch 0/1
Batch Loss = 3.612843023024947
12, epoch_train_loss=3.612843023024947
Epoch 13
Epoch 13 :: Batch 0/1
Batch Loss = 3.573591386372523
13, epoch_train_loss=3.573591386372523
Epoch 14
Epoch 14 :: Batch 0/1
Batch Loss = 3.5462790806631324
14, epoch_train_loss=3.5462790806631324
Epoch 15
Epoch 15 :: Batch 0/1
Batch Loss = 3.5070768073313796
15, epoch_train_loss=3.5070768073313796
Epoch 16
Epoch 16 :: Batch 0/1
Batch Loss = 3.527329011025727
16, epoch_train_loss=3.527329011025727
Epoch 17
Epoch 17 :: Batch 0/1
Batch Loss = 3.5227714451867653
17, epoch_train_loss=3.5227714451867653
Epoch 18
Epoch 18 :: Batch 0/1
Batch Loss = 3.54826053927043
18, epoch_train_loss=3.54826053927043
Epoch 19
Epoch 19 :: Batch 0/1
Batch Loss = 3.5384083891304927
19, epoch_train_loss=3.5384083891304927
Epoch 20
Epoch 20 :: Batch 0/1
Batch Loss = 3.543765708188258
20, epoch_train_loss=3.543765708188258
Epoch 21
Epoch 21 :: Batch 0/1
Batch Loss = 3.529327982508311
21, epoch_train_loss=3.529327982508311
Epoch 22
Epoch 22 :: Batch 0/1
Batch Loss = 3.526204823857636
22, epoch_train_loss=3.526204823857636
Epoch 23
Epoch 23 :: Batch 0/1
Batch Loss = 3.519303526278334
23, epoch_train_loss=3.519303526278334
Epoch 24
Epoch 24 :: Batch 0/1
Batch Loss = 3.508146747059195
24, epoch_train_loss=3.508146747059195
Epoch 25
Epoch 25 :: Batch 0/1
Batch Loss = 3.5117778767808754
25, epoch_train_loss=3.5117778767808754
Epoch 26
Epoch 26 :: Batch 0/1
Batch Loss = 3.4961474897780085
26, epoch_train_loss=3.4961474897780085
Epoch 27
Epoch 27 :: Batch 0/1
Batch Loss = 3.498380252970597
27, epoch_train_loss=3.498380252970597
Epoch 28
Epoch 28 :: Batch 0/1
Batch Loss = 3.483909074909587
28, epoch_train_loss=3.483909074909587
Epoch 29
Epoch 29 :: Batch 0/1
Batch Loss = 3.478809945447218
29, epoch_train_loss=3.478809945447218
Epoch 30
Epoch 30 :: Batch 0/1
Batch Loss = 3.4815014165202194
30, epoch_train_loss=3.4815014165202194
Epoch 31
Epoch 31 :: Batch 0/1
Batch Loss = 3.4808288896997364
31, epoch_train_loss=3.4808288896997364
Epoch 32
Epoch 32 :: Batch 0/1
Batch Loss = 3.48765514149603
32, epoch_train_loss=3.48765514149603
Epoch 33
Epoch 33 :: Batch 0/1
Batch Loss = 3.4898383316933717
33, epoch_train_loss=3.4898383316933717
Epoch 34
Epoch 34 :: Batch 0/1
Batch Loss = 3.4828713572526677
34, epoch_train_loss=3.4828713572526677
Epoch 35
Epoch 35 :: Batch 0/1
Batch Loss = 3.4799288889932463
35, epoch_train_loss=3.4799288889932463
Epoch 36
Epoch 36 :: Batch 0/1
Batch Loss = 3.4802724883241734
36, epoch_train_loss=3.4802724883241734
Epoch 37
Epoch 37 :: Batch 0/1
Batch Loss = 3.478158388789075
37, epoch_train_loss=3.478158388789075
Epoch 38
Epoch 38 :: Batch 0/1
Batch Loss = 3.479708646612321
38, epoch_train_loss=3.479708646612321
Epoch 39
Epoch 39 :: Batch 0/1
Batch Loss = 3.4787920593292196
39, epoch_train_loss=3.4787920593292196
Epoch 40
Epoch 40 :: Batch 0/1
Batch Loss = 3.4734988480416082
40, epoch_train_loss=3.4734988480416082
Epoch 41
Epoch 41 :: Batch 0/1
Batch Loss = 3.472410855764886
41, epoch_train_loss=3.472410855764886
Epoch 42
Epoch 42 :: Batch 0/1
Batch Loss = 3.470878646711614
42, epoch_train_loss=3.470878646711614
Epoch 43
Epoch 43 :: Batch 0/1
Batch Loss = 3.4705617614490154
43, epoch_train_loss=3.4705617614490154
Epoch 44
Epoch 44 :: Batch 0/1
Batch Loss = 3.472665682525324
44, epoch_train_loss=3.472665682525324
Epoch 45
Epoch 45 :: Batch 0/1
Batch Loss = 3.4713122399656084
45, epoch_train_loss=3.4713122399656084
Epoch 46
Epoch 46 :: Batch 0/1
Batch Loss = 3.471173910500866
46, epoch_train_loss=3.471173910500866
Epoch 47
Epoch 47 :: Batch 0/1
Batch Loss = 3.469537223422625
47, epoch_train_loss=3.469537223422625
Epoch 48
Epoch 48 :: Batch 0/1
Batch Loss = 3.467571276476427
48, epoch_train_loss=3.467571276476427
Epoch 49
Epoch 49 :: Batch 0/1
Batch Loss = 3.467638428422523
49, epoch_train_loss=3.467638428422523
Epoch 50
Epoch 50 :: Batch 0/1
Batch Loss = 3.4664844104549903
50, epoch_train_loss=3.4664844104549903
Epoch 51
Epoch 51 :: Batch 0/1
Batch Loss = 3.467180409383873
51, epoch_train_loss=3.467180409383873
Epoch 52
Epoch 52 :: Batch 0/1
Batch Loss = 3.46592894909796
52, epoch_train_loss=3.46592894909796
Epoch 53
Epoch 53 :: Batch 0/1
Batch Loss = 3.4651446344399774
53, epoch_train_loss=3.4651446344399774
Epoch 54
Epoch 54 :: Batch 0/1
Batch Loss = 3.464299262617409
54, epoch_train_loss=3.464299262617409
Epoch 55
Epoch 55 :: Batch 0/1
Batch Loss = 3.4638250972855156
55, epoch_train_loss=3.4638250972855156
Epoch 56
Epoch 56 :: Batch 0/1
Batch Loss = 3.4644683499391173
56, epoch_train_loss=3.4644683499391173
Epoch 57
Epoch 57 :: Batch 0/1
Batch Loss = 3.464131414102608
57, epoch_train_loss=3.464131414102608
Epoch 58
Epoch 58 :: Batch 0/1
Batch Loss = 3.464203249560176
58, epoch_train_loss=3.464203249560176
Epoch 59
Epoch 59 :: Batch 0/1
Batch Loss = 3.4630970508344125
59, epoch_train_loss=3.4630970508344125
Epoch 60
Epoch 60 :: Batch 0/1
Batch Loss = 3.462751623241384
60, epoch_train_loss=3.462751623241384
Epoch 61
Epoch 61 :: Batch 0/1
Batch Loss = 3.462298320018333
61, epoch_train_loss=3.462298320018333
Epoch 62
Epoch 62 :: Batch 0/1
Batch Loss = 3.4623980191928863
62, epoch_train_loss=3.4623980191928863
Epoch 63
Epoch 63 :: Batch 0/1
Batch Loss = 3.462258876592279
63, epoch_train_loss=3.462258876592279
Epoch 64
Epoch 64 :: Batch 0/1
Batch Loss = 3.4619964404370633
64, epoch_train_loss=3.4619964404370633
Epoch 65
Epoch 65 :: Batch 0/1
Batch Loss = 3.461763534877499
65, epoch_train_loss=3.461763534877499
Epoch 66
Epoch 66 :: Batch 0/1
Batch Loss = 3.461663867728888
66, epoch_train_loss=3.461663867728888
Epoch 67
Epoch 67 :: Batch 0/1
Batch Loss = 3.4618220492040184
67, epoch_train_loss=3.4618220492040184
Epoch 68
Epoch 68 :: Batch 0/1
Batch Loss = 3.4618272702460424
68, epoch_train_loss=3.4618272702460424
Epoch 69
Epoch 69 :: Batch 0/1
Batch Loss = 3.4617727017379076
69, epoch_train_loss=3.4617727017379076
Epoch 70
Epoch 70 :: Batch 0/1
Batch Loss = 3.4614878634500466
70, epoch_train_loss=3.4614878634500466
Epoch 71
Epoch 71 :: Batch 0/1
Batch Loss = 3.4613850903491667
71, epoch_train_loss=3.4613850903491667
Epoch 72
Epoch 72 :: Batch 0/1
Batch Loss = 3.461361346592083
72, epoch_train_loss=3.461361346592083
Epoch 73
Epoch 73 :: Batch 0/1
Batch Loss = 3.461505281602975
73, epoch_train_loss=3.461505281602975
Epoch 74
Epoch 74 :: Batch 0/1
Batch Loss = 3.461509674371552
74, epoch_train_loss=3.461509674371552
Epoch 75
Epoch 75 :: Batch 0/1
Batch Loss = 3.461453146380113
75, epoch_train_loss=3.461453146380113
Epoch 76
Epoch 76 :: Batch 0/1
Batch Loss = 3.4613593711690336
76, epoch_train_loss=3.4613593711690336
Epoch 77
Epoch 77 :: Batch 0/1
Batch Loss = 3.4613441932642304
77, epoch_train_loss=3.4613441932642304
Epoch 78
Epoch 78 :: Batch 0/1
Batch Loss = 3.461359095771189
78, epoch_train_loss=3.461359095771189
Epoch 79
Epoch 79 :: Batch 0/1
Batch Loss = 3.4612998503126855
79, epoch_train_loss=3.4612998503126855
Epoch 80
Epoch 80 :: Batch 0/1
Batch Loss = 3.461228574211181
80, epoch_train_loss=3.461228574211181
Epoch 81
Epoch 81 :: Batch 0/1
Batch Loss = 3.461121418169396
81, epoch_train_loss=3.461121418169396
Epoch 82
Epoch 82 :: Batch 0/1
Batch Loss = 3.4611489355540566
82, epoch_train_loss=3.4611489355540566
Epoch 83
Epoch 83 :: Batch 0/1
Batch Loss = 3.4611338109493066
83, epoch_train_loss=3.4611338109493066
Epoch 84
Epoch 84 :: Batch 0/1
Batch Loss = 3.461168509714028
84, epoch_train_loss=3.461168509714028
Epoch 85
Epoch 85 :: Batch 0/1
Batch Loss = 3.461083678441317
85, epoch_train_loss=3.461083678441317
Epoch 86
Epoch 86 :: Batch 0/1
Batch Loss = 3.4610652671762927
86, epoch_train_loss=3.4610652671762927
Epoch 87
Epoch 87 :: Batch 0/1
Batch Loss = 3.4610293367068246
87, epoch_train_loss=3.4610293367068246
Epoch 88
Epoch 88 :: Batch 0/1
Batch Loss = 3.461042905587468
88, epoch_train_loss=3.461042905587468
Epoch 89
Epoch 89 :: Batch 0/1
Batch Loss = 3.461033227039697
89, epoch_train_loss=3.461033227039697
Epoch 90
Epoch 90 :: Batch 0/1
Batch Loss = 3.460998160172275
90, epoch_train_loss=3.460998160172275
Epoch 91
Epoch 91 :: Batch 0/1
Batch Loss = 3.4609892247430434
91, epoch_train_loss=3.4609892247430434
Epoch 92
Epoch 92 :: Batch 0/1
Batch Loss = 3.4609682511094713
92, epoch_train_loss=3.4609682511094713
Epoch 93
Epoch 93 :: Batch 0/1
Batch Loss = 3.4609867621270896
93, epoch_train_loss=3.4609867621270896
Epoch 94
Epoch 94 :: Batch 0/1
Batch Loss = 3.460960458268864
94, epoch_train_loss=3.460960458268864
Epoch 95
Epoch 95 :: Batch 0/1
Batch Loss = 3.4609411606463656
95, epoch_train_loss=3.4609411606463656
Epoch 96
Epoch 96 :: Batch 0/1
Batch Loss = 3.4609152453379832
96, epoch_train_loss=3.4609152453379832
Epoch 97
Epoch 97 :: Batch 0/1
Batch Loss = 3.460904742803658
97, epoch_train_loss=3.460904742803658
Epoch 98
Epoch 98 :: Batch 0/1
Batch Loss = 3.4609122618065946
98, epoch_train_loss=3.4609122618065946
Epoch 99
Epoch 99 :: Batch 0/1
Batch Loss = 3.4608982685398595
99, epoch_train_loss=3.4608982685398595
Epoch 100
Epoch 100 :: Batch 0/1
Batch Loss = 3.4608945452361857
100, epoch_train_loss=3.4608945452361857
Epoch 101
Epoch 101 :: Batch 0/1
Batch Loss = 3.4608805612412636
101, epoch_train_loss=3.4608805612412636
Epoch 102
Epoch 102 :: Batch 0/1
Batch Loss = 3.460881050142096
102, epoch_train_loss=3.460881050142096
Epoch 103
Epoch 103 :: Batch 0/1
Batch Loss = 3.4608820411560433
103, epoch_train_loss=3.4608820411560433
Epoch 104
Epoch 104 :: Batch 0/1
Batch Loss = 3.460870457718894
104, epoch_train_loss=3.460870457718894
Epoch 105
Epoch 105 :: Batch 0/1
Batch Loss = 3.4608646971638612
105, epoch_train_loss=3.4608646971638612
Epoch 106
Epoch 106 :: Batch 0/1
Batch Loss = 3.4608540319313725
106, epoch_train_loss=3.4608540319313725
Epoch 107
Epoch 107 :: Batch 0/1
Batch Loss = 3.460854065623817
107, epoch_train_loss=3.460854065623817
Epoch 108
Epoch 108 :: Batch 0/1
Batch Loss = 3.4608520218010446
108, epoch_train_loss=3.4608520218010446
Epoch 109
Epoch 109 :: Batch 0/1
Batch Loss = 3.4608422582582463
109, epoch_train_loss=3.4608422582582463
Epoch 110
Epoch 110 :: Batch 0/1
Batch Loss = 3.4608359413820713
110, epoch_train_loss=3.4608359413820713
Epoch 111
Epoch 111 :: Batch 0/1
Batch Loss = 3.460826521379151
111, epoch_train_loss=3.460826521379151
Epoch 112
Epoch 112 :: Batch 0/1
Batch Loss = 3.4608222505520265
112, epoch_train_loss=3.4608222505520265
Epoch 113
Epoch 113 :: Batch 0/1
Batch Loss = 3.4608170231618605
113, epoch_train_loss=3.4608170231618605
Epoch 114
Epoch 114 :: Batch 0/1
Batch Loss = 3.460807358425429
114, epoch_train_loss=3.460807358425429
Epoch 115
Epoch 115 :: Batch 0/1
Batch Loss = 3.4608019844077615
115, epoch_train_loss=3.4608019844077615
Epoch 116
Epoch 116 :: Batch 0/1
Batch Loss = 3.460796303480262
116, epoch_train_loss=3.460796303480262
Epoch 117
Epoch 117 :: Batch 0/1
Batch Loss = 3.46079177092659
117, epoch_train_loss=3.46079177092659
Epoch 118
Epoch 118 :: Batch 0/1
Batch Loss = 3.4607873363123685
118, epoch_train_loss=3.4607873363123685
Epoch 119
Epoch 119 :: Batch 0/1
Batch Loss = 3.4607792416326992
119, epoch_train_loss=3.4607792416326992
Epoch 120
Epoch 120 :: Batch 0/1
Batch Loss = 3.460773375820659
120, epoch_train_loss=3.460773375820659
Epoch 121
Epoch 121 :: Batch 0/1
Batch Loss = 3.460768741467981
121, epoch_train_loss=3.460768741467981
Epoch 122
Epoch 122 :: Batch 0/1
Batch Loss = 3.4607630328990333
122, epoch_train_loss=3.4607630328990333
Epoch 123
Epoch 123 :: Batch 0/1
Batch Loss = 3.460758160828723
123, epoch_train_loss=3.460758160828723
Epoch 124
Epoch 124 :: Batch 0/1
Batch Loss = 3.4607523151141684
124, epoch_train_loss=3.4607523151141684
Epoch 125
Epoch 125 :: Batch 0/1
Batch Loss = 3.4607466820280646
125, epoch_train_loss=3.4607466820280646
Epoch 126
Epoch 126 :: Batch 0/1
Batch Loss = 3.460742611529605
126, epoch_train_loss=3.460742611529605
Epoch 127
Epoch 127 :: Batch 0/1
Batch Loss = 3.4607373200961153
127, epoch_train_loss=3.4607373200961153
Epoch 128
Epoch 128 :: Batch 0/1
Batch Loss = 3.460731490956447
128, epoch_train_loss=3.460731490956447
Epoch 129
Epoch 129 :: Batch 0/1
Batch Loss = 3.4607265738692905
129, epoch_train_loss=3.4607265738692905
Epoch 130
Epoch 130 :: Batch 0/1
Batch Loss = 3.460721595971433
130, epoch_train_loss=3.460721595971433
Epoch 131
Epoch 131 :: Batch 0/1
Batch Loss = 3.4607169414714685
131, epoch_train_loss=3.4607169414714685
Epoch 132
Epoch 132 :: Batch 0/1
Batch Loss = 3.4607124487850776
132, epoch_train_loss=3.4607124487850776
Epoch 133
Epoch 133 :: Batch 0/1
Batch Loss = 3.4607071825075275
133, epoch_train_loss=3.4607071825075275
Epoch 134
Epoch 134 :: Batch 0/1
Batch Loss = 3.4607021365186266
134, epoch_train_loss=3.4607021365186266
Epoch 135
Epoch 135 :: Batch 0/1
Batch Loss = 3.4606976387139383
135, epoch_train_loss=3.4606976387139383
Epoch 136
Epoch 136 :: Batch 0/1
Batch Loss = 3.460692654504116
136, epoch_train_loss=3.460692654504116
Epoch 137
Epoch 137 :: Batch 0/1
Batch Loss = 3.4606874493056754
137, epoch_train_loss=3.4606874493056754
Epoch 138
Epoch 138 :: Batch 0/1
Batch Loss = 3.460682565665862
138, epoch_train_loss=3.460682565665862
Epoch 139
Epoch 139 :: Batch 0/1
Batch Loss = 3.4606776602105946
139, epoch_train_loss=3.4606776602105946
Epoch 140
Epoch 140 :: Batch 0/1
Batch Loss = 3.4606727255736502
140, epoch_train_loss=3.4606727255736502
Epoch 141
Epoch 141 :: Batch 0/1
Batch Loss = 3.460667860073061
141, epoch_train_loss=3.460667860073061
Epoch 142
Epoch 142 :: Batch 0/1
Batch Loss = 3.46066278329692
142, epoch_train_loss=3.46066278329692
Epoch 143
Epoch 143 :: Batch 0/1
Batch Loss = 3.4606576527698345
143, epoch_train_loss=3.4606576527698345
Epoch 144
Epoch 144 :: Batch 0/1
Batch Loss = 3.460652787423323
144, epoch_train_loss=3.460652787423323
Epoch 145
Epoch 145 :: Batch 0/1
Batch Loss = 3.4606479271636017
145, epoch_train_loss=3.4606479271636017
Epoch 146
Epoch 146 :: Batch 0/1
Batch Loss = 3.460642861056636
146, epoch_train_loss=3.460642861056636
Epoch 147
Epoch 147 :: Batch 0/1
Batch Loss = 3.4606378497644887
147, epoch_train_loss=3.4606378497644887
Epoch 148
Epoch 148 :: Batch 0/1
Batch Loss = 3.460632986732035
148, epoch_train_loss=3.460632986732035
Epoch 149
Epoch 149 :: Batch 0/1
Batch Loss = 3.460628045499017
149, epoch_train_loss=3.460628045499017
Epoch 150
Epoch 150 :: Batch 0/1
Batch Loss = 3.460622983546495
150, epoch_train_loss=3.460622983546495
Epoch 151
Epoch 151 :: Batch 0/1
Batch Loss = 3.4606179548221054
151, epoch_train_loss=3.4606179548221054
Epoch 152
Epoch 152 :: Batch 0/1
Batch Loss = 3.46061296697881
152, epoch_train_loss=3.46061296697881
Epoch 153
Epoch 153 :: Batch 0/1
Batch Loss = 3.4606079418503994
153, epoch_train_loss=3.4606079418503994
Epoch 154
Epoch 154 :: Batch 0/1
Batch Loss = 3.46060289824217
154, epoch_train_loss=3.46060289824217
Epoch 155
Epoch 155 :: Batch 0/1
Batch Loss = 3.4605978486804805
155, epoch_train_loss=3.4605978486804805
Epoch 156
Epoch 156 :: Batch 0/1
Batch Loss = 3.460592759307752
156, epoch_train_loss=3.460592759307752
Epoch 157
Epoch 157 :: Batch 0/1
Batch Loss = 3.460587639612673
157, epoch_train_loss=3.460587639612673
Epoch 158
Epoch 158 :: Batch 0/1
Batch Loss = 3.460582513293513
158, epoch_train_loss=3.460582513293513
Epoch 159
Epoch 159 :: Batch 0/1
Batch Loss = 3.4605773662772887
159, epoch_train_loss=3.4605773662772887
Epoch 160
Epoch 160 :: Batch 0/1
Batch Loss = 3.460572175612133
160, epoch_train_loss=3.460572175612133
Epoch 161
Epoch 161 :: Batch 0/1
Batch Loss = 3.460566952317764
161, epoch_train_loss=3.460566952317764
Epoch 162
Epoch 162 :: Batch 0/1
Batch Loss = 3.460561721224339
162, epoch_train_loss=3.460561721224339
Epoch 163
Epoch 163 :: Batch 0/1
Batch Loss = 3.460556460015034
163, epoch_train_loss=3.460556460015034
Epoch 164
Epoch 164 :: Batch 0/1
Batch Loss = 3.4605511383800165
164, epoch_train_loss=3.4605511383800165
Epoch 165
Epoch 165 :: Batch 0/1
Batch Loss = 3.460545771039578
165, epoch_train_loss=3.460545771039578
Epoch 166
Epoch 166 :: Batch 0/1
Batch Loss = 3.460540376517682
166, epoch_train_loss=3.460540376517682
Epoch 167
Epoch 167 :: Batch 0/1
Batch Loss = 3.460534952730459
167, epoch_train_loss=3.460534952730459
Epoch 168
Epoch 168 :: Batch 0/1
Batch Loss = 3.4605294818747447
168, epoch_train_loss=3.4605294818747447
Epoch 169
Epoch 169 :: Batch 0/1
Batch Loss = 3.4605239555575658
169, epoch_train_loss=3.4605239555575658
Epoch 170
Epoch 170 :: Batch 0/1
Batch Loss = 3.460518385949347
170, epoch_train_loss=3.460518385949347
Epoch 171
Epoch 171 :: Batch 0/1
Batch Loss = 3.4605127703224903
171, epoch_train_loss=3.4605127703224903
Epoch 172
Epoch 172 :: Batch 0/1
Batch Loss = 3.460507101229044
172, epoch_train_loss=3.460507101229044
Epoch 173
Epoch 173 :: Batch 0/1
Batch Loss = 3.4605013760475964
173, epoch_train_loss=3.4605013760475964
Epoch 174
Epoch 174 :: Batch 0/1
Batch Loss = 3.460495591764708
174, epoch_train_loss=3.460495591764708
Epoch 175
Epoch 175 :: Batch 0/1
Batch Loss = 3.460489750128574
175, epoch_train_loss=3.460489750128574
Epoch 176
Epoch 176 :: Batch 0/1
Batch Loss = 3.4604838443457027
176, epoch_train_loss=3.4604838443457027
Epoch 177
Epoch 177 :: Batch 0/1
Batch Loss = 3.4604778739192104
177, epoch_train_loss=3.4604778739192104
Epoch 178
Epoch 178 :: Batch 0/1
Batch Loss = 3.460471838864654
178, epoch_train_loss=3.460471838864654
Epoch 179
Epoch 179 :: Batch 0/1
Batch Loss = 3.460465730696823
179, epoch_train_loss=3.460465730696823
Epoch 180
Epoch 180 :: Batch 0/1
Batch Loss = 3.4604595455535985
180, epoch_train_loss=3.4604595455535985
Epoch 181
Epoch 181 :: Batch 0/1
Batch Loss = 3.460453278125439
181, epoch_train_loss=3.460453278125439
Epoch 182
Epoch 182 :: Batch 0/1
Batch Loss = 3.4604469326396425
182, epoch_train_loss=3.4604469326396425
Epoch 183
Epoch 183 :: Batch 0/1
Batch Loss = 3.4604405062519974
183, epoch_train_loss=3.4604405062519974
Epoch 184
Epoch 184 :: Batch 0/1
Batch Loss = 3.4604339907777875
184, epoch_train_loss=3.4604339907777875
Epoch 185
Epoch 185 :: Batch 0/1
Batch Loss = 3.4604273810270647
185, epoch_train_loss=3.4604273810270647
Epoch 186
Epoch 186 :: Batch 0/1
Batch Loss = 3.4604206709196728
186, epoch_train_loss=3.4604206709196728
Epoch 187
Epoch 187 :: Batch 0/1
Batch Loss = 3.4604138610793065
187, epoch_train_loss=3.4604138610793065
Epoch 188
Epoch 188 :: Batch 0/1
Batch Loss = 3.460406943341556
188, epoch_train_loss=3.460406943341556
Epoch 189
Epoch 189 :: Batch 0/1
Batch Loss = 3.4603999145669895
189, epoch_train_loss=3.4603999145669895
Epoch 190
Epoch 190 :: Batch 0/1
Batch Loss = 3.4603927687449096
190, epoch_train_loss=3.4603927687449096
Epoch 191
Epoch 191 :: Batch 0/1
Batch Loss = 3.4603855026549475
191, epoch_train_loss=3.4603855026549475
Epoch 192
Epoch 192 :: Batch 0/1
Batch Loss = 3.460378110125574
192, epoch_train_loss=3.460378110125574
Epoch 193
Epoch 193 :: Batch 0/1
Batch Loss = 3.4603705832001954
193, epoch_train_loss=3.4603705832001954
Epoch 194
Epoch 194 :: Batch 0/1
Batch Loss = 3.4603629174780814
194, epoch_train_loss=3.4603629174780814
Epoch 195
Epoch 195 :: Batch 0/1
Batch Loss = 3.460355104961436
195, epoch_train_loss=3.460355104961436
Epoch 196
Epoch 196 :: Batch 0/1
Batch Loss = 3.4603471416709586
196, epoch_train_loss=3.4603471416709586
Epoch 197
Epoch 197 :: Batch 0/1
Batch Loss = 3.4603390158460137
197, epoch_train_loss=3.4603390158460137
Epoch 198
Epoch 198 :: Batch 0/1
Batch Loss = 3.4603307240817496
198, epoch_train_loss=3.4603307240817496
Epoch 199
Epoch 199 :: Batch 0/1
Batch Loss = 3.4603222540791525
199, epoch_train_loss=3.4603222540791525
Epoch 200
Epoch 200 :: Batch 0/1
Batch Loss = 3.460313604555494
200, epoch_train_loss=3.460313604555494
Epoch 201
Epoch 201 :: Batch 0/1
Batch Loss = 3.4603047600494485
201, epoch_train_loss=3.4603047600494485
Epoch 202
Epoch 202 :: Batch 0/1
Batch Loss = 3.4602957246875397
202, epoch_train_loss=3.4602957246875397
Epoch 203
Epoch 203 :: Batch 0/1
Batch Loss = 3.4602864835753
203, epoch_train_loss=3.4602864835753
Epoch 204
Epoch 204 :: Batch 0/1
Batch Loss = 3.460277062930482
204, epoch_train_loss=3.460277062930482
Epoch 205
Epoch 205 :: Batch 0/1
Batch Loss = 3.4602674679242744
205, epoch_train_loss=3.4602674679242744
Epoch 206
Epoch 206 :: Batch 0/1
Batch Loss = 3.460257827088614
206, epoch_train_loss=3.460257827088614
Epoch 207
Epoch 207 :: Batch 0/1
Batch Loss = 3.4602483242595024
207, epoch_train_loss=3.4602483242595024
Epoch 208
Epoch 208 :: Batch 0/1
Batch Loss = 3.460239688384952
208, epoch_train_loss=3.460239688384952
Epoch 209
Epoch 209 :: Batch 0/1
Batch Loss = 3.460233460893505
209, epoch_train_loss=3.460233460893505
Epoch 210
Epoch 210 :: Batch 0/1
Batch Loss = 3.460234627480026
210, epoch_train_loss=3.460234627480026
Epoch 211
Epoch 211 :: Batch 0/1
Batch Loss = 3.460255565572842
211, epoch_train_loss=3.460255565572842
Epoch 212
Epoch 212 :: Batch 0/1
Batch Loss = 3.4603369312545715
212, epoch_train_loss=3.4603369312545715
Epoch 213
Epoch 213 :: Batch 0/1
Batch Loss = 3.4605828089515427
213, epoch_train_loss=3.4605828089515427
Epoch 214
Epoch 214 :: Batch 0/1
Batch Loss = 3.4613780965518766
214, epoch_train_loss=3.4613780965518766
Epoch 215
Epoch 215 :: Batch 0/1
Batch Loss = 3.463531332660586
215, epoch_train_loss=3.463531332660586
Epoch 216
Epoch 216 :: Batch 0/1
Batch Loss = 3.4706991669955913
216, epoch_train_loss=3.4706991669955913
Epoch 217
Epoch 217 :: Batch 0/1
Batch Loss = 3.4818734867223182
217, epoch_train_loss=3.4818734867223182
Epoch 218
Epoch 218 :: Batch 0/1
Batch Loss = 3.5082991171218807
218, epoch_train_loss=3.5082991171218807
Epoch 219
Epoch 219 :: Batch 0/1
Batch Loss = 3.4735465250298057
219, epoch_train_loss=3.4735465250298057
Epoch 220
Epoch 220 :: Batch 0/1
Batch Loss = 3.4609646496135795
220, epoch_train_loss=3.4609646496135795
Epoch 221
Epoch 221 :: Batch 0/1
Batch Loss = 3.475037225424407
221, epoch_train_loss=3.475037225424407
Epoch 222
Epoch 222 :: Batch 0/1
Batch Loss = 3.465904635149995
222, epoch_train_loss=3.465904635149995
Epoch 223
Epoch 223 :: Batch 0/1
Batch Loss = 3.461776676377158
223, epoch_train_loss=3.461776676377158
Epoch 224
Epoch 224 :: Batch 0/1
Batch Loss = 3.468891159284167
224, epoch_train_loss=3.468891159284167
Epoch 225
Epoch 225 :: Batch 0/1
Batch Loss = 3.461210573999223
225, epoch_train_loss=3.461210573999223
Epoch 226
Epoch 226 :: Batch 0/1
Batch Loss = 3.4649100315828316
226, epoch_train_loss=3.4649100315828316
Epoch 227
Epoch 227 :: Batch 0/1
Batch Loss = 3.4656875850036166
227, epoch_train_loss=3.4656875850036166
Epoch 228
Epoch 228 :: Batch 0/1
Batch Loss = 3.461077152449312
228, epoch_train_loss=3.461077152449312
Epoch 229
Epoch 229 :: Batch 0/1
Batch Loss = 3.4666591378497302
229, epoch_train_loss=3.4666591378497302
Epoch 230
Epoch 230 :: Batch 0/1
Batch Loss = 3.461879409543762
230, epoch_train_loss=3.461879409543762
Epoch 231
Epoch 231 :: Batch 0/1
Batch Loss = 3.4629696279526594
231, epoch_train_loss=3.4629696279526594
Epoch 232
Epoch 232 :: Batch 0/1
Batch Loss = 3.4636326331981393
232, epoch_train_loss=3.4636326331981393
Epoch 233
Epoch 233 :: Batch 0/1
Batch Loss = 3.460550986513873
233, epoch_train_loss=3.460550986513873
Epoch 234
Epoch 234 :: Batch 0/1
Batch Loss = 3.4633606786632725
234, epoch_train_loss=3.4633606786632725
Epoch 235
Epoch 235 :: Batch 0/1
Batch Loss = 3.4605195582967974
235, epoch_train_loss=3.4605195582967974
Epoch 236
Epoch 236 :: Batch 0/1
Batch Loss = 3.4622042363537906
236, epoch_train_loss=3.4622042363537906
Epoch 237
Epoch 237 :: Batch 0/1
Batch Loss = 3.4614737090658796
237, epoch_train_loss=3.4614737090658796
Epoch 238
Epoch 238 :: Batch 0/1
Batch Loss = 3.4607571919530806
238, epoch_train_loss=3.4607571919530806
Epoch 239
Epoch 239 :: Batch 0/1
Batch Loss = 3.4620025846331415
239, epoch_train_loss=3.4620025846331415
Epoch 240
Epoch 240 :: Batch 0/1
Batch Loss = 3.4601373839739877
240, epoch_train_loss=3.4601373839739877
Epoch 241
Epoch 241 :: Batch 0/1
Batch Loss = 3.4614723360941806
241, epoch_train_loss=3.4614723360941806
Epoch 242
Epoch 242 :: Batch 0/1
Batch Loss = 3.46040600035756
242, epoch_train_loss=3.46040600035756
Epoch 243
Epoch 243 :: Batch 0/1
Batch Loss = 3.460666893953141
243, epoch_train_loss=3.460666893953141
Epoch 244
Epoch 244 :: Batch 0/1
Batch Loss = 3.4608560846254144
244, epoch_train_loss=3.4608560846254144
Epoch 245
Epoch 245 :: Batch 0/1
Batch Loss = 3.460118835581647
245, epoch_train_loss=3.460118835581647
Epoch 246
Epoch 246 :: Batch 0/1
Batch Loss = 3.460946026378722
246, epoch_train_loss=3.460946026378722
Epoch 247
Epoch 247 :: Batch 0/1
Batch Loss = 3.460054019889899
247, epoch_train_loss=3.460054019889899
Epoch 248
Epoch 248 :: Batch 0/1
Batch Loss = 3.4605245002944085
248, epoch_train_loss=3.4605245002944085
Epoch 249
Epoch 249 :: Batch 0/1
Batch Loss = 3.4602422103210633
249, epoch_train_loss=3.4602422103210633
Epoch 250
Epoch 250 :: Batch 0/1
Batch Loss = 3.4600510954202996
250, epoch_train_loss=3.4600510954202996
Epoch 251
Epoch 251 :: Batch 0/1
Batch Loss = 3.460351467509535
251, epoch_train_loss=3.460351467509535
Epoch 252
Epoch 252 :: Batch 0/1
Batch Loss = 3.459834931000647
252, epoch_train_loss=3.459834931000647
Epoch 253
Epoch 253 :: Batch 0/1
Batch Loss = 3.4602161222553103
253, epoch_train_loss=3.4602161222553103
Epoch 254
Epoch 254 :: Batch 0/1
Batch Loss = 3.459868250038606
254, epoch_train_loss=3.459868250038606
Epoch 255
Epoch 255 :: Batch 0/1
Batch Loss = 3.4598789888087564
255, epoch_train_loss=3.4598789888087564
Epoch 256
Epoch 256 :: Batch 0/1
Batch Loss = 3.4599193427236052
256, epoch_train_loss=3.4599193427236052
Epoch 257
Epoch 257 :: Batch 0/1
Batch Loss = 3.4596234545893663
257, epoch_train_loss=3.4596234545893663
Epoch 258
Epoch 258 :: Batch 0/1
Batch Loss = 3.45981724226443
258, epoch_train_loss=3.45981724226443
Epoch 259
Epoch 259 :: Batch 0/1
Batch Loss = 3.459538212761431
259, epoch_train_loss=3.459538212761431
Epoch 260
Epoch 260 :: Batch 0/1
Batch Loss = 3.4595947489211456
260, epoch_train_loss=3.4595947489211456
Epoch 261
Epoch 261 :: Batch 0/1
Batch Loss = 3.459512931384978
261, epoch_train_loss=3.459512931384978
Epoch 262
Epoch 262 :: Batch 0/1
Batch Loss = 3.4593590856812093
262, epoch_train_loss=3.4593590856812093
Epoch 263
Epoch 263 :: Batch 0/1
Batch Loss = 3.459420424527558
263, epoch_train_loss=3.459420424527558
Epoch 264
Epoch 264 :: Batch 0/1
Batch Loss = 3.459198253900498
264, epoch_train_loss=3.459198253900498
Epoch 265
Epoch 265 :: Batch 0/1
Batch Loss = 3.459214900085859
265, epoch_train_loss=3.459214900085859
Epoch 266
Epoch 266 :: Batch 0/1
Batch Loss = 3.459080817628907
266, epoch_train_loss=3.459080817628907
Epoch 267
Epoch 267 :: Batch 0/1
Batch Loss = 3.458956834177448
267, epoch_train_loss=3.458956834177448
Epoch 268
Epoch 268 :: Batch 0/1
Batch Loss = 3.4589174336051123
268, epoch_train_loss=3.4589174336051123
Epoch 269
Epoch 269 :: Batch 0/1
Batch Loss = 3.4587166604167723
269, epoch_train_loss=3.4587166604167723
Epoch 270
Epoch 270 :: Batch 0/1
Batch Loss = 3.4586570502337373
270, epoch_train_loss=3.4586570502337373
Epoch 271
Epoch 271 :: Batch 0/1
Batch Loss = 3.4584891500117365
271, epoch_train_loss=3.4584891500117365
Epoch 272
Epoch 272 :: Batch 0/1
Batch Loss = 3.4583205898699805
272, epoch_train_loss=3.4583205898699805
Epoch 273
Epoch 273 :: Batch 0/1
Batch Loss = 3.4581978178593924
273, epoch_train_loss=3.4581978178593924
Epoch 274
Epoch 274 :: Batch 0/1
Batch Loss = 3.4579599197556328
274, epoch_train_loss=3.4579599197556328
Epoch 275
Epoch 275 :: Batch 0/1
Batch Loss = 3.457785929819231
275, epoch_train_loss=3.457785929819231
Epoch 276
Epoch 276 :: Batch 0/1
Batch Loss = 3.457559131342105
276, epoch_train_loss=3.457559131342105
Epoch 277
Epoch 277 :: Batch 0/1
Batch Loss = 3.457272860061667
277, epoch_train_loss=3.457272860061667
Epoch 278
Epoch 278 :: Batch 0/1
Batch Loss = 3.4570212496014547
278, epoch_train_loss=3.4570212496014547
Epoch 279
Epoch 279 :: Batch 0/1
Batch Loss = 3.4566810859046173
279, epoch_train_loss=3.4566810859046173
Epoch 280
Epoch 280 :: Batch 0/1
Batch Loss = 3.456297334531731
280, epoch_train_loss=3.456297334531731
Epoch 281
Epoch 281 :: Batch 0/1
Batch Loss = 3.455905543010752
281, epoch_train_loss=3.455905543010752
Epoch 282
Epoch 282 :: Batch 0/1
Batch Loss = 3.455411275292426
282, epoch_train_loss=3.455411275292426
Epoch 283
Epoch 283 :: Batch 0/1
Batch Loss = 3.454835130016231
283, epoch_train_loss=3.454835130016231
Epoch 284
Epoch 284 :: Batch 0/1
Batch Loss = 3.45420096230081
284, epoch_train_loss=3.45420096230081
Epoch 285
Epoch 285 :: Batch 0/1
Batch Loss = 3.4534532237518856
285, epoch_train_loss=3.4534532237518856
Epoch 286
Epoch 286 :: Batch 0/1
Batch Loss = 3.45255920092649
286, epoch_train_loss=3.45255920092649
Epoch 287
Epoch 287 :: Batch 0/1
Batch Loss = 3.4514817166123484
287, epoch_train_loss=3.4514817166123484
Epoch 288
Epoch 288 :: Batch 0/1
Batch Loss = 3.450212002662079
288, epoch_train_loss=3.450212002662079
Epoch 289
Epoch 289 :: Batch 0/1
Batch Loss = 3.448707661950026
289, epoch_train_loss=3.448707661950026
Epoch 290
Epoch 290 :: Batch 0/1
Batch Loss = 3.4471764442914736
290, epoch_train_loss=3.4471764442914736
Epoch 291
Epoch 291 :: Batch 0/1
Batch Loss = 3.448148008658797
291, epoch_train_loss=3.448148008658797
Epoch 292
Epoch 292 :: Batch 0/1
Batch Loss = 3.4818972557025165
292, epoch_train_loss=3.4818972557025165
Epoch 293
Epoch 293 :: Batch 0/1
Batch Loss = 3.634186649148055
293, epoch_train_loss=3.634186649148055
Epoch 294
Epoch 294 :: Batch 0/1
Batch Loss = 3.684786505215952
294, epoch_train_loss=3.684786505215952
Epoch 295
Epoch 295 :: Batch 0/1
Batch Loss = 3.7622093276977884
295, epoch_train_loss=3.7622093276977884
Epoch 296
Epoch 296 :: Batch 0/1
Batch Loss = 3.608786305577716
296, epoch_train_loss=3.608786305577716
Epoch 297
Epoch 297 :: Batch 0/1
Batch Loss = 3.537548118128851
297, epoch_train_loss=3.537548118128851
Epoch 298
Epoch 298 :: Batch 0/1
Batch Loss = 3.555085249552687
298, epoch_train_loss=3.555085249552687
Epoch 299
Epoch 299 :: Batch 0/1
Batch Loss = 3.542878453513595
299, epoch_train_loss=3.542878453513595
Epoch 300
Epoch 300 :: Batch 0/1
Batch Loss = 3.5220723379384746
300, epoch_train_loss=3.5220723379384746
Epoch 301
Epoch 301 :: Batch 0/1
Batch Loss = 3.5307568422442066
301, epoch_train_loss=3.5307568422442066
Epoch 302
Epoch 302 :: Batch 0/1
Batch Loss = 3.5166046147235037
302, epoch_train_loss=3.5166046147235037
Epoch 303
Epoch 303 :: Batch 0/1
Batch Loss = 3.4867335039912644
303, epoch_train_loss=3.4867335039912644
Epoch 304
Epoch 304 :: Batch 0/1
Batch Loss = 3.529329894646821
304, epoch_train_loss=3.529329894646821
Epoch 305
Epoch 305 :: Batch 0/1
Batch Loss = 3.4920351955204785
305, epoch_train_loss=3.4920351955204785
Epoch 306
Epoch 306 :: Batch 0/1
Batch Loss = 3.502921108584798
306, epoch_train_loss=3.502921108584798
Epoch 307
Epoch 307 :: Batch 0/1
Batch Loss = 3.5031975885206896
307, epoch_train_loss=3.5031975885206896
Epoch 308
Epoch 308 :: Batch 0/1
Batch Loss = 3.4843986651877326
308, epoch_train_loss=3.4843986651877326
Epoch 309
Epoch 309 :: Batch 0/1
Batch Loss = 3.484225600987753
309, epoch_train_loss=3.484225600987753
Epoch 310
Epoch 310 :: Batch 0/1
Batch Loss = 3.495851080580093
310, epoch_train_loss=3.495851080580093
Epoch 311
Epoch 311 :: Batch 0/1
Batch Loss = 3.4859185918540927
311, epoch_train_loss=3.4859185918540927
Epoch 312
Epoch 312 :: Batch 0/1
Batch Loss = 3.478455670122929
312, epoch_train_loss=3.478455670122929
Epoch 313
Epoch 313 :: Batch 0/1
Batch Loss = 3.4815778937360005
313, epoch_train_loss=3.4815778937360005
Epoch 314
Epoch 314 :: Batch 0/1
Batch Loss = 3.487702187436508
314, epoch_train_loss=3.487702187436508
Epoch 315
Epoch 315 :: Batch 0/1
Batch Loss = 3.481804755033523
315, epoch_train_loss=3.481804755033523
Epoch 316
Epoch 316 :: Batch 0/1
Batch Loss = 3.4750316314481267
316, epoch_train_loss=3.4750316314481267
Epoch 317
Epoch 317 :: Batch 0/1
Batch Loss = 3.4779070982404514
317, epoch_train_loss=3.4779070982404514
Epoch 318
Epoch 318 :: Batch 0/1
Batch Loss = 3.4796936265764216
318, epoch_train_loss=3.4796936265764216
Epoch 319
Epoch 319 :: Batch 0/1
Batch Loss = 3.475547977361535
319, epoch_train_loss=3.475547977361535
Epoch 320
Epoch 320 :: Batch 0/1
Batch Loss = 3.470685236003202
320, epoch_train_loss=3.470685236003202
Epoch 321
Epoch 321 :: Batch 0/1
Batch Loss = 3.47291636640255
321, epoch_train_loss=3.47291636640255
Epoch 322
Epoch 322 :: Batch 0/1
Batch Loss = 3.4744826035959817
322, epoch_train_loss=3.4744826035959817
Epoch 323
Epoch 323 :: Batch 0/1
Batch Loss = 3.4709506691412684
323, epoch_train_loss=3.4709506691412684
Epoch 324
Epoch 324 :: Batch 0/1
Batch Loss = 3.468337971336467
324, epoch_train_loss=3.468337971336467
Epoch 325
Epoch 325 :: Batch 0/1
Batch Loss = 3.468917654358473
325, epoch_train_loss=3.468917654358473
Epoch 326
Epoch 326 :: Batch 0/1
Batch Loss = 3.470148052920897
326, epoch_train_loss=3.470148052920897
Epoch 327
Epoch 327 :: Batch 0/1
Batch Loss = 3.467662019815485
327, epoch_train_loss=3.467662019815485
Epoch 328
Epoch 328 :: Batch 0/1
Batch Loss = 3.465209749957859
328, epoch_train_loss=3.465209749957859
Epoch 329
Epoch 329 :: Batch 0/1
Batch Loss = 3.4663311225356375
329, epoch_train_loss=3.4663311225356375
Epoch 330
Epoch 330 :: Batch 0/1
Batch Loss = 3.466118880705338
330, epoch_train_loss=3.466118880705338
Epoch 331
Epoch 331 :: Batch 0/1
Batch Loss = 3.464648000976578
331, epoch_train_loss=3.464648000976578
Epoch 332
Epoch 332 :: Batch 0/1
Batch Loss = 3.463375693810385
332, epoch_train_loss=3.463375693810385
Epoch 333
Epoch 333 :: Batch 0/1
Batch Loss = 3.4637235737125738
333, epoch_train_loss=3.4637235737125738
Epoch 334
Epoch 334 :: Batch 0/1
Batch Loss = 3.46424031737177
334, epoch_train_loss=3.46424031737177
Epoch 335
Epoch 335 :: Batch 0/1
Batch Loss = 3.462800257524589
335, epoch_train_loss=3.462800257524589
Epoch 336
Epoch 336 :: Batch 0/1
Batch Loss = 3.4622811544023553
336, epoch_train_loss=3.4622811544023553
Epoch 337
Epoch 337 :: Batch 0/1
Batch Loss = 3.46237103035612
337, epoch_train_loss=3.46237103035612
Epoch 338
Epoch 338 :: Batch 0/1
Batch Loss = 3.4627025900623134
338, epoch_train_loss=3.4627025900623134
Epoch 339
Epoch 339 :: Batch 0/1
Batch Loss = 3.4618445707783096
339, epoch_train_loss=3.4618445707783096
Epoch 340
Epoch 340 :: Batch 0/1
Batch Loss = 3.4614979300258835
340, epoch_train_loss=3.4614979300258835
Epoch 341
Epoch 341 :: Batch 0/1
Batch Loss = 3.4616780567950514
341, epoch_train_loss=3.4616780567950514
Epoch 342
Epoch 342 :: Batch 0/1
Batch Loss = 3.4619160020315007
342, epoch_train_loss=3.4619160020315007
Epoch 343
Epoch 343 :: Batch 0/1
Batch Loss = 3.461323982667124
343, epoch_train_loss=3.461323982667124
Epoch 344
Epoch 344 :: Batch 0/1
Batch Loss = 3.4610664689514596
344, epoch_train_loss=3.4610664689514596
Epoch 345
Epoch 345 :: Batch 0/1
Batch Loss = 3.4611706114748078
345, epoch_train_loss=3.4611706114748078
Epoch 346
Epoch 346 :: Batch 0/1
Batch Loss = 3.4612117055520044
346, epoch_train_loss=3.4612117055520044
Epoch 347
Epoch 347 :: Batch 0/1
Batch Loss = 3.460853940843136
347, epoch_train_loss=3.460853940843136
Epoch 348
Epoch 348 :: Batch 0/1
Batch Loss = 3.4607316875062306
348, epoch_train_loss=3.4607316875062306
Epoch 349
Epoch 349 :: Batch 0/1
Batch Loss = 3.460833910664094
349, epoch_train_loss=3.460833910664094
Epoch 350
Epoch 350 :: Batch 0/1
Batch Loss = 3.4608285976267514
350, epoch_train_loss=3.4608285976267514
Epoch 351
Epoch 351 :: Batch 0/1
Batch Loss = 3.4605539776522365
351, epoch_train_loss=3.4605539776522365
Epoch 352
Epoch 352 :: Batch 0/1
Batch Loss = 3.4604543681213973
352, epoch_train_loss=3.4604543681213973
Epoch 353
Epoch 353 :: Batch 0/1
Batch Loss = 3.460518721974499
353, epoch_train_loss=3.460518721974499
Epoch 354
Epoch 354 :: Batch 0/1
Batch Loss = 3.460494191198601
354, epoch_train_loss=3.460494191198601
Epoch 355
Epoch 355 :: Batch 0/1
Batch Loss = 3.4603278115877254
355, epoch_train_loss=3.4603278115877254
Epoch 356
Epoch 356 :: Batch 0/1
Batch Loss = 3.4602823914325396
356, epoch_train_loss=3.4602823914325396
Epoch 357
Epoch 357 :: Batch 0/1
Batch Loss = 3.460318218722061
357, epoch_train_loss=3.460318218722061
Epoch 358
Epoch 358 :: Batch 0/1
Batch Loss = 3.460245355791084
358, epoch_train_loss=3.460245355791084
Epoch 359
Epoch 359 :: Batch 0/1
Batch Loss = 3.4600960341574623
359, epoch_train_loss=3.4600960341574623
Epoch 360
Epoch 360 :: Batch 0/1
Batch Loss = 3.4600488637739444
360, epoch_train_loss=3.4600488637739444
Epoch 361
Epoch 361 :: Batch 0/1
Batch Loss = 3.4600439301218056
361, epoch_train_loss=3.4600439301218056
Epoch 362
Epoch 362 :: Batch 0/1
Batch Loss = 3.4599642271497486
362, epoch_train_loss=3.4599642271497486
Epoch 363
Epoch 363 :: Batch 0/1
Batch Loss = 3.459856747871366
363, epoch_train_loss=3.459856747871366
Epoch 364
Epoch 364 :: Batch 0/1
Batch Loss = 3.4598182002591136
364, epoch_train_loss=3.4598182002591136
Epoch 365
Epoch 365 :: Batch 0/1
Batch Loss = 3.4597829540586944
365, epoch_train_loss=3.4597829540586944
Epoch 366
Epoch 366 :: Batch 0/1
Batch Loss = 3.459691393052243
366, epoch_train_loss=3.459691393052243
Epoch 367
Epoch 367 :: Batch 0/1
Batch Loss = 3.4596002399272248
367, epoch_train_loss=3.4596002399272248
Epoch 368
Epoch 368 :: Batch 0/1
Batch Loss = 3.4595688530716107
368, epoch_train_loss=3.4595688530716107
Epoch 369
Epoch 369 :: Batch 0/1
Batch Loss = 3.4595249726678063
369, epoch_train_loss=3.4595249726678063
Epoch 370
Epoch 370 :: Batch 0/1
Batch Loss = 3.459447770551383
370, epoch_train_loss=3.459447770551383
Epoch 371
Epoch 371 :: Batch 0/1
Batch Loss = 3.459379549376373
371, epoch_train_loss=3.459379549376373
Epoch 372
Epoch 372 :: Batch 0/1
Batch Loss = 3.459351303040458
372, epoch_train_loss=3.459351303040458
Epoch 373
Epoch 373 :: Batch 0/1
Batch Loss = 3.459292778786534
373, epoch_train_loss=3.459292778786534
Epoch 374
Epoch 374 :: Batch 0/1
Batch Loss = 3.4592244762331403
374, epoch_train_loss=3.4592244762331403
Epoch 375
Epoch 375 :: Batch 0/1
Batch Loss = 3.459170368242019
375, epoch_train_loss=3.459170368242019
Epoch 376
Epoch 376 :: Batch 0/1
Batch Loss = 3.4591371030102915
376, epoch_train_loss=3.4591371030102915
Epoch 377
Epoch 377 :: Batch 0/1
Batch Loss = 3.4590745521103745
377, epoch_train_loss=3.4590745521103745
Epoch 378
Epoch 378 :: Batch 0/1
Batch Loss = 3.459016723998861
378, epoch_train_loss=3.459016723998861
Epoch 379
Epoch 379 :: Batch 0/1
Batch Loss = 3.4589693972772806
379, epoch_train_loss=3.4589693972772806
Epoch 380
Epoch 380 :: Batch 0/1
Batch Loss = 3.458921163325461
380, epoch_train_loss=3.458921163325461
Epoch 381
Epoch 381 :: Batch 0/1
Batch Loss = 3.4588568129765167
381, epoch_train_loss=3.4588568129765167
Epoch 382
Epoch 382 :: Batch 0/1
Batch Loss = 3.4588002850281065
382, epoch_train_loss=3.4588002850281065
Epoch 383
Epoch 383 :: Batch 0/1
Batch Loss = 3.4587536887068366
383, epoch_train_loss=3.4587536887068366
Epoch 384
Epoch 384 :: Batch 0/1
Batch Loss = 3.4586938298165264
384, epoch_train_loss=3.4586938298165264
Epoch 385
Epoch 385 :: Batch 0/1
Batch Loss = 3.4586332588886153
385, epoch_train_loss=3.4586332588886153
Epoch 386
Epoch 386 :: Batch 0/1
Batch Loss = 3.4585774158568054
386, epoch_train_loss=3.4585774158568054
Epoch 387
Epoch 387 :: Batch 0/1
Batch Loss = 3.4585217711219856
387, epoch_train_loss=3.4585217711219856
Epoch 388
Epoch 388 :: Batch 0/1
Batch Loss = 3.4584581162857853
388, epoch_train_loss=3.4584581162857853
Epoch 389
Epoch 389 :: Batch 0/1
Batch Loss = 3.4583956944624092
389, epoch_train_loss=3.4583956944624092
Epoch 390
Epoch 390 :: Batch 0/1
Batch Loss = 3.4583394947579613
390, epoch_train_loss=3.4583394947579613
Epoch 391
Epoch 391 :: Batch 0/1
Batch Loss = 3.458275675303649
391, epoch_train_loss=3.458275675303649
Epoch 392
Epoch 392 :: Batch 0/1
Batch Loss = 3.4582090000336394
392, epoch_train_loss=3.4582090000336394
Epoch 393
Epoch 393 :: Batch 0/1
Batch Loss = 3.458146963468151
393, epoch_train_loss=3.458146963468151
Epoch 394
Epoch 394 :: Batch 0/1
Batch Loss = 3.458081471853088
394, epoch_train_loss=3.458081471853088
Epoch 395
Epoch 395 :: Batch 0/1
Batch Loss = 3.458011754940321
395, epoch_train_loss=3.458011754940321
Epoch 396
Epoch 396 :: Batch 0/1
Batch Loss = 3.457944251486373
396, epoch_train_loss=3.457944251486373
Epoch 397
Epoch 397 :: Batch 0/1
Batch Loss = 3.4578765154932345
397, epoch_train_loss=3.4578765154932345
Epoch 398
Epoch 398 :: Batch 0/1
Batch Loss = 3.4578042229454433
398, epoch_train_loss=3.4578042229454433
Epoch 399
Epoch 399 :: Batch 0/1
Batch Loss = 3.4577310878197767
399, epoch_train_loss=3.4577310878197767
Epoch 400
Epoch 400 :: Batch 0/1
Batch Loss = 3.4576586893480714
400, epoch_train_loss=3.4576586893480714
Epoch 401
Epoch 401 :: Batch 0/1
Batch Loss = 3.4575829322186076
401, epoch_train_loss=3.4575829322186076
Epoch 402
Epoch 402 :: Batch 0/1
Batch Loss = 3.4575042673140035
402, epoch_train_loss=3.4575042673140035
Epoch 403
Epoch 403 :: Batch 0/1
Batch Loss = 3.457425965298816
403, epoch_train_loss=3.457425965298816
Epoch 404
Epoch 404 :: Batch 0/1
Batch Loss = 3.4573460617992846
404, epoch_train_loss=3.4573460617992846
Epoch 405
Epoch 405 :: Batch 0/1
Batch Loss = 3.45726214368181
405, epoch_train_loss=3.45726214368181
Epoch 406
Epoch 406 :: Batch 0/1
Batch Loss = 3.4571759777831654
406, epoch_train_loss=3.4571759777831654
Epoch 407
Epoch 407 :: Batch 0/1
Batch Loss = 3.457088294529006
407, epoch_train_loss=3.457088294529006
Epoch 408
Epoch 408 :: Batch 0/1
Batch Loss = 3.4569972458574534
408, epoch_train_loss=3.4569972458574534
Epoch 409
Epoch 409 :: Batch 0/1
Batch Loss = 3.456903444846692
409, epoch_train_loss=3.456903444846692
Epoch 410
Epoch 410 :: Batch 0/1
Batch Loss = 3.456807677880811
410, epoch_train_loss=3.456807677880811
Epoch 411
Epoch 411 :: Batch 0/1
Batch Loss = 3.4567083218095878
411, epoch_train_loss=3.4567083218095878
Epoch 412
Epoch 412 :: Batch 0/1
Batch Loss = 3.45660549908206
412, epoch_train_loss=3.45660549908206
Epoch 413
Epoch 413 :: Batch 0/1
Batch Loss = 3.456500741829821
413, epoch_train_loss=3.456500741829821
Epoch 414
Epoch 414 :: Batch 0/1
Batch Loss = 3.456395438232018
414, epoch_train_loss=3.456395438232018
Epoch 415
Epoch 415 :: Batch 0/1
Batch Loss = 3.4562959095513794
415, epoch_train_loss=3.4562959095513794
Epoch 416
Epoch 416 :: Batch 0/1
Batch Loss = 3.456229657720748
416, epoch_train_loss=3.456229657720748
Epoch 417
Epoch 417 :: Batch 0/1
Batch Loss = 3.4563039040783567
417, epoch_train_loss=3.4563039040783567
Epoch 418
Epoch 418 :: Batch 0/1
Batch Loss = 3.456912166521648
418, epoch_train_loss=3.456912166521648
Epoch 419
Epoch 419 :: Batch 0/1
Batch Loss = 3.459483513611296
419, epoch_train_loss=3.459483513611296
Epoch 420
Epoch 420 :: Batch 0/1
Batch Loss = 3.4646758314050756
420, epoch_train_loss=3.4646758314050756
Epoch 421
Epoch 421 :: Batch 0/1
Batch Loss = 3.466557896109587
421, epoch_train_loss=3.466557896109587
Epoch 422
Epoch 422 :: Batch 0/1
Batch Loss = 3.458293046528261
422, epoch_train_loss=3.458293046528261
Epoch 423
Epoch 423 :: Batch 0/1
Batch Loss = 3.4569771519216332
423, epoch_train_loss=3.4569771519216332
Epoch 424
Epoch 424 :: Batch 0/1
Batch Loss = 3.4614217859532266
424, epoch_train_loss=3.4614217859532266
Epoch 425
Epoch 425 :: Batch 0/1
Batch Loss = 3.45594557599543
425, epoch_train_loss=3.45594557599543
Epoch 426
Epoch 426 :: Batch 0/1
Batch Loss = 3.4579210084685936
426, epoch_train_loss=3.4579210084685936
Epoch 427
Epoch 427 :: Batch 0/1
Batch Loss = 3.457300116270288
427, epoch_train_loss=3.457300116270288
Epoch 428
Epoch 428 :: Batch 0/1
Batch Loss = 3.4552868320635106
428, epoch_train_loss=3.4552868320635106
Epoch 429
Epoch 429 :: Batch 0/1
Batch Loss = 3.457308872304219
429, epoch_train_loss=3.457308872304219
Epoch 430
Epoch 430 :: Batch 0/1
Batch Loss = 3.4543901121935794
430, epoch_train_loss=3.4543901121935794
Epoch 431
Epoch 431 :: Batch 0/1
Batch Loss = 3.456451969877829
431, epoch_train_loss=3.456451969877829
Epoch 432
Epoch 432 :: Batch 0/1
Batch Loss = 3.4543633021748064
432, epoch_train_loss=3.4543633021748064
Epoch 433
Epoch 433 :: Batch 0/1
Batch Loss = 3.4548715532613277
433, epoch_train_loss=3.4548715532613277
Epoch 434
Epoch 434 :: Batch 0/1
Batch Loss = 3.4543961318314205
434, epoch_train_loss=3.4543961318314205
Epoch 435
Epoch 435 :: Batch 0/1
Batch Loss = 3.453443267137927
435, epoch_train_loss=3.453443267137927
Epoch 436
Epoch 436 :: Batch 0/1
Batch Loss = 3.4540328263434286
436, epoch_train_loss=3.4540328263434286
Epoch 437
Epoch 437 :: Batch 0/1
Batch Loss = 3.4524501933731706
437, epoch_train_loss=3.4524501933731706
Epoch 438
Epoch 438 :: Batch 0/1
Batch Loss = 3.453192439040684
438, epoch_train_loss=3.453192439040684
Epoch 439
Epoch 439 :: Batch 0/1
Batch Loss = 3.4517981929813493
439, epoch_train_loss=3.4517981929813493
Epoch 440
Epoch 440 :: Batch 0/1
Batch Loss = 3.4519445811094833
440, epoch_train_loss=3.4519445811094833
Epoch 441
Epoch 441 :: Batch 0/1
Batch Loss = 3.4512387173338315
441, epoch_train_loss=3.4512387173338315
Epoch 442
Epoch 442 :: Batch 0/1
Batch Loss = 3.450565841394119
442, epoch_train_loss=3.450565841394119
Epoch 443
Epoch 443 :: Batch 0/1
Batch Loss = 3.4504552037021265
443, epoch_train_loss=3.4504552037021265
Epoch 444
Epoch 444 :: Batch 0/1
Batch Loss = 3.449240797160528
444, epoch_train_loss=3.449240797160528
Epoch 445
Epoch 445 :: Batch 0/1
Batch Loss = 3.449209244300915
445, epoch_train_loss=3.449209244300915
Epoch 446
Epoch 446 :: Batch 0/1
Batch Loss = 3.4481959320748174
446, epoch_train_loss=3.4481959320748174
Epoch 447
Epoch 447 :: Batch 0/1
Batch Loss = 3.447394280594696
447, epoch_train_loss=3.447394280594696
Epoch 448
Epoch 448 :: Batch 0/1
Batch Loss = 3.446939710999653
448, epoch_train_loss=3.446939710999653
Epoch 449
Epoch 449 :: Batch 0/1
Batch Loss = 3.4456038265037683
449, epoch_train_loss=3.4456038265037683
Epoch 450
Epoch 450 :: Batch 0/1
Batch Loss = 3.4448359147109415
450, epoch_train_loss=3.4448359147109415
Epoch 451
Epoch 451 :: Batch 0/1
Batch Loss = 3.443881120402403
451, epoch_train_loss=3.443881120402403
Epoch 452
Epoch 452 :: Batch 0/1
Batch Loss = 3.4423937190845484
452, epoch_train_loss=3.4423937190845484
Epoch 453
Epoch 453 :: Batch 0/1
Batch Loss = 3.441314567958872
453, epoch_train_loss=3.441314567958872
Epoch 454
Epoch 454 :: Batch 0/1
Batch Loss = 3.4399910524444195
454, epoch_train_loss=3.4399910524444195
Epoch 455
Epoch 455 :: Batch 0/1
Batch Loss = 3.438214144303009
455, epoch_train_loss=3.438214144303009
Epoch 456
Epoch 456 :: Batch 0/1
Batch Loss = 3.4365966402807833
456, epoch_train_loss=3.4365966402807833
Epoch 457
Epoch 457 :: Batch 0/1
Batch Loss = 3.434945831999068
457, epoch_train_loss=3.434945831999068
Epoch 458
Epoch 458 :: Batch 0/1
Batch Loss = 3.4328599396196418
458, epoch_train_loss=3.4328599396196418
Epoch 459
Epoch 459 :: Batch 0/1
Batch Loss = 3.4305561230317774
459, epoch_train_loss=3.4305561230317774
Epoch 460
Epoch 460 :: Batch 0/1
Batch Loss = 3.4282968321052043
460, epoch_train_loss=3.4282968321052043
Epoch 461
Epoch 461 :: Batch 0/1
Batch Loss = 3.4259804287906386
461, epoch_train_loss=3.4259804287906386
Epoch 462
Epoch 462 :: Batch 0/1
Batch Loss = 3.423554698031941
462, epoch_train_loss=3.423554698031941
Epoch 463
Epoch 463 :: Batch 0/1
Batch Loss = 3.4211918556084506
463, epoch_train_loss=3.4211918556084506
Epoch 464
Epoch 464 :: Batch 0/1
Batch Loss = 3.4194211033167066
464, epoch_train_loss=3.4194211033167066
Epoch 465
Epoch 465 :: Batch 0/1
Batch Loss = 3.4223576460659086
465, epoch_train_loss=3.4223576460659086
Epoch 466
Epoch 466 :: Batch 0/1
Batch Loss = 3.4351207362957585
466, epoch_train_loss=3.4351207362957585
Epoch 467
Epoch 467 :: Batch 0/1
Batch Loss = 3.4861444852651267
467, epoch_train_loss=3.4861444852651267
Epoch 468
Epoch 468 :: Batch 0/1
Batch Loss = 3.4558612523167107
468, epoch_train_loss=3.4558612523167107
Epoch 469
Epoch 469 :: Batch 0/1
Batch Loss = 3.4653073750309877
469, epoch_train_loss=3.4653073750309877
Epoch 470
Epoch 470 :: Batch 0/1
Batch Loss = 3.4131480437852404
470, epoch_train_loss=3.4131480437852404
Epoch 471
Epoch 471 :: Batch 0/1
Batch Loss = 3.6695330399292057
471, epoch_train_loss=3.6695330399292057
Epoch 472
Epoch 472 :: Batch 0/1
Batch Loss = 3.6580260281119337
472, epoch_train_loss=3.6580260281119337
Epoch 473
Epoch 473 :: Batch 0/1
Batch Loss = 3.631997171683586
473, epoch_train_loss=3.631997171683586
Epoch 474
Epoch 474 :: Batch 0/1
Batch Loss = 3.9648268460627016
474, epoch_train_loss=3.9648268460627016
Epoch 475
Epoch 475 :: Batch 0/1
Batch Loss = 3.794750565082176
475, epoch_train_loss=3.794750565082176
Epoch 476
Epoch 476 :: Batch 0/1
Batch Loss = 3.596232386372983
476, epoch_train_loss=3.596232386372983
Epoch 477
Epoch 477 :: Batch 0/1
Batch Loss = 3.8329757490733956
477, epoch_train_loss=3.8329757490733956
Epoch 478
Epoch 478 :: Batch 0/1
Batch Loss = 3.9071958295051474
478, epoch_train_loss=3.9071958295051474
Epoch 479
Epoch 479 :: Batch 0/1
Batch Loss = 3.830580415192252
479, epoch_train_loss=3.830580415192252
Epoch 480
Epoch 480 :: Batch 0/1
Batch Loss = 3.6865281179847504
480, epoch_train_loss=3.6865281179847504
Epoch 481
Epoch 481 :: Batch 0/1
Batch Loss = 3.737797742805704
481, epoch_train_loss=3.737797742805704
Epoch 482
Epoch 482 :: Batch 0/1
Batch Loss = 3.567125745584261
482, epoch_train_loss=3.567125745584261
Epoch 483
Epoch 483 :: Batch 0/1
Batch Loss = 3.6090009787916713
483, epoch_train_loss=3.6090009787916713
Epoch 484
Epoch 484 :: Batch 0/1
Batch Loss = 3.550996428655788
484, epoch_train_loss=3.550996428655788
Epoch 485
Epoch 485 :: Batch 0/1
Batch Loss = 3.7016222858685275
485, epoch_train_loss=3.7016222858685275
Epoch 486
Epoch 486 :: Batch 0/1
Batch Loss = 3.49979537207917
486, epoch_train_loss=3.49979537207917
Epoch 487
Epoch 487 :: Batch 0/1
Batch Loss = 3.5932068943590356
487, epoch_train_loss=3.5932068943590356
Epoch 488
Epoch 488 :: Batch 0/1
Batch Loss = 3.4786080130275345
488, epoch_train_loss=3.4786080130275345
Epoch 489
Epoch 489 :: Batch 0/1
Batch Loss = 3.580541063593241
489, epoch_train_loss=3.580541063593241
Epoch 490
Epoch 490 :: Batch 0/1
Batch Loss = 3.501203728805
490, epoch_train_loss=3.501203728805
Epoch 491
Epoch 491 :: Batch 0/1
Batch Loss = 3.553227756437489
491, epoch_train_loss=3.553227756437489
Epoch 492
Epoch 492 :: Batch 0/1
Batch Loss = 3.4911962559380596
492, epoch_train_loss=3.4911962559380596
Epoch 493
Epoch 493 :: Batch 0/1
Batch Loss = 3.504009054337825
493, epoch_train_loss=3.504009054337825
Epoch 494
Epoch 494 :: Batch 0/1
Batch Loss = 3.5006268332065247
494, epoch_train_loss=3.5006268332065247
Epoch 495
Epoch 495 :: Batch 0/1
Batch Loss = 3.4914634941072804
495, epoch_train_loss=3.4914634941072804
Epoch 496
Epoch 496 :: Batch 0/1
Batch Loss = 3.5163279326218166
496, epoch_train_loss=3.5163279326218166
Epoch 497
Epoch 497 :: Batch 0/1
Batch Loss = 3.475901786199841
497, epoch_train_loss=3.475901786199841
Epoch 498
Epoch 498 :: Batch 0/1
Batch Loss = 3.502720164522465
498, epoch_train_loss=3.502720164522465
Epoch 499
Epoch 499 :: Batch 0/1
Batch Loss = 3.471311008552075
499, epoch_train_loss=3.471311008552075
Epoch 500
Epoch 500 :: Batch 0/1
Batch Loss = 3.4875557809276274
500, epoch_train_loss=3.4875557809276274
Epoch 501
Epoch 501 :: Batch 0/1
Batch Loss = 3.48770784041176
501, epoch_train_loss=3.48770784041176
Epoch 502
Epoch 502 :: Batch 0/1
Batch Loss = 3.473002438894389
502, epoch_train_loss=3.473002438894389
Epoch 503
Epoch 503 :: Batch 0/1
Batch Loss = 3.485376651375775
503, epoch_train_loss=3.485376651375775
Epoch 504
Epoch 504 :: Batch 0/1
Batch Loss = 3.468025842999998
504, epoch_train_loss=3.468025842999998
Epoch 505
Epoch 505 :: Batch 0/1
Batch Loss = 3.473919184721204
505, epoch_train_loss=3.473919184721204
Epoch 506
Epoch 506 :: Batch 0/1
Batch Loss = 3.477166588913152
506, epoch_train_loss=3.477166588913152
Epoch 507
Epoch 507 :: Batch 0/1
Batch Loss = 3.468957164524569
507, epoch_train_loss=3.468957164524569
Epoch 508
Epoch 508 :: Batch 0/1
Batch Loss = 3.477403206828191
508, epoch_train_loss=3.477403206828191
Epoch 509
Epoch 509 :: Batch 0/1
Batch Loss = 3.465065871412775
509, epoch_train_loss=3.465065871412775
Epoch 510
Epoch 510 :: Batch 0/1
Batch Loss = 3.4696808160286015
510, epoch_train_loss=3.4696808160286015
Epoch 511
Epoch 511 :: Batch 0/1
Batch Loss = 3.469330795142517
511, epoch_train_loss=3.469330795142517
Epoch 512
Epoch 512 :: Batch 0/1
Batch Loss = 3.4653822522485007
512, epoch_train_loss=3.4653822522485007
Epoch 513
Epoch 513 :: Batch 0/1
Batch Loss = 3.4704503728673326
513, epoch_train_loss=3.4704503728673326
Epoch 514
Epoch 514 :: Batch 0/1
Batch Loss = 3.464242727946543
514, epoch_train_loss=3.464242727946543
Epoch 515
Epoch 515 :: Batch 0/1
Batch Loss = 3.465176554064167
515, epoch_train_loss=3.465176554064167
Epoch 516
Epoch 516 :: Batch 0/1
Batch Loss = 3.4660887124521254
516, epoch_train_loss=3.4660887124521254
Epoch 517
Epoch 517 :: Batch 0/1
Batch Loss = 3.463310562991324
517, epoch_train_loss=3.463310562991324
Epoch 518
Epoch 518 :: Batch 0/1
Batch Loss = 3.4670055162258167
518, epoch_train_loss=3.4670055162258167
Epoch 519
Epoch 519 :: Batch 0/1
Batch Loss = 3.4632099234925757
519, epoch_train_loss=3.4632099234925757
Epoch 520
Epoch 520 :: Batch 0/1
Batch Loss = 3.463857948527304
520, epoch_train_loss=3.463857948527304
Epoch 521
Epoch 521 :: Batch 0/1
Batch Loss = 3.4634959976829376
521, epoch_train_loss=3.4634959976829376
Epoch 522
Epoch 522 :: Batch 0/1
Batch Loss = 3.4620686117194777
522, epoch_train_loss=3.4620686117194777
Epoch 523
Epoch 523 :: Batch 0/1
Batch Loss = 3.4642210367575474
523, epoch_train_loss=3.4642210367575474
Epoch 524
Epoch 524 :: Batch 0/1
Batch Loss = 3.462016663120821
524, epoch_train_loss=3.462016663120821
Epoch 525
Epoch 525 :: Batch 0/1
Batch Loss = 3.462758123340894
525, epoch_train_loss=3.462758123340894
Epoch 526
Epoch 526 :: Batch 0/1
Batch Loss = 3.4620910555229503
526, epoch_train_loss=3.4620910555229503
Epoch 527
Epoch 527 :: Batch 0/1
Batch Loss = 3.461403619066516
527, epoch_train_loss=3.461403619066516
Epoch 528
Epoch 528 :: Batch 0/1
Batch Loss = 3.462533293788943
528, epoch_train_loss=3.462533293788943
Epoch 529
Epoch 529 :: Batch 0/1
Batch Loss = 3.4612057607717412
529, epoch_train_loss=3.4612057607717412
Epoch 530
Epoch 530 :: Batch 0/1
Batch Loss = 3.4619770384030075
530, epoch_train_loss=3.4619770384030075
Epoch 531
Epoch 531 :: Batch 0/1
Batch Loss = 3.460944638195085
531, epoch_train_loss=3.460944638195085
Epoch 532
Epoch 532 :: Batch 0/1
Batch Loss = 3.4610208177048785
532, epoch_train_loss=3.4610208177048785
Epoch 533
Epoch 533 :: Batch 0/1
Batch Loss = 3.4612137224734263
533, epoch_train_loss=3.4612137224734263
Epoch 534
Epoch 534 :: Batch 0/1
Batch Loss = 3.4607363471214163
534, epoch_train_loss=3.4607363471214163
Epoch 535
Epoch 535 :: Batch 0/1
Batch Loss = 3.46127649929058
535, epoch_train_loss=3.46127649929058
Epoch 536
Epoch 536 :: Batch 0/1
Batch Loss = 3.460530438201626
536, epoch_train_loss=3.460530438201626
Epoch 537
Epoch 537 :: Batch 0/1
Batch Loss = 3.4608191139571605
537, epoch_train_loss=3.4608191139571605
Epoch 538
Epoch 538 :: Batch 0/1
Batch Loss = 3.460579079408116
538, epoch_train_loss=3.460579079408116
Epoch 539
Epoch 539 :: Batch 0/1
Batch Loss = 3.460565869623061
539, epoch_train_loss=3.460565869623061
Epoch 540
Epoch 540 :: Batch 0/1
Batch Loss = 3.4607226915220535
540, epoch_train_loss=3.4607226915220535
Epoch 541
Epoch 541 :: Batch 0/1
Batch Loss = 3.460335203775419
541, epoch_train_loss=3.460335203775419
Epoch 542
Epoch 542 :: Batch 0/1
Batch Loss = 3.4605673075540375
542, epoch_train_loss=3.4605673075540375
Epoch 543
Epoch 543 :: Batch 0/1
Batch Loss = 3.460229340130649
543, epoch_train_loss=3.460229340130649
Epoch 544
Epoch 544 :: Batch 0/1
Batch Loss = 3.4604469374643423
544, epoch_train_loss=3.4604469374643423
Epoch 545
Epoch 545 :: Batch 0/1
Batch Loss = 3.4603408935548674
545, epoch_train_loss=3.4603408935548674
Epoch 546
Epoch 546 :: Batch 0/1
Batch Loss = 3.460317909956645
546, epoch_train_loss=3.460317909956645
Epoch 547
Epoch 547 :: Batch 0/1
Batch Loss = 3.4603275612050557
547, epoch_train_loss=3.4603275612050557
Epoch 548
Epoch 548 :: Batch 0/1
Batch Loss = 3.460150847324926
548, epoch_train_loss=3.460150847324926
Epoch 549
Epoch 549 :: Batch 0/1
Batch Loss = 3.4602794732315414
549, epoch_train_loss=3.4602794732315414
Epoch 550
Epoch 550 :: Batch 0/1
Batch Loss = 3.4601276371624627
550, epoch_train_loss=3.4601276371624627
Epoch 551
Epoch 551 :: Batch 0/1
Batch Loss = 3.4602335805010576
551, epoch_train_loss=3.4602335805010576
Epoch 552
Epoch 552 :: Batch 0/1
Batch Loss = 3.4601111852576847
552, epoch_train_loss=3.4601111852576847
Epoch 553
Epoch 553 :: Batch 0/1
Batch Loss = 3.4601212455182546
553, epoch_train_loss=3.4601212455182546
Epoch 554
Epoch 554 :: Batch 0/1
Batch Loss = 3.460095054224874
554, epoch_train_loss=3.460095054224874
Epoch 555
Epoch 555 :: Batch 0/1
Batch Loss = 3.46006142766826
555, epoch_train_loss=3.46006142766826
Epoch 556
Epoch 556 :: Batch 0/1
Batch Loss = 3.4601014166311472
556, epoch_train_loss=3.4601014166311472
Epoch 557
Epoch 557 :: Batch 0/1
Batch Loss = 3.460016780195863
557, epoch_train_loss=3.460016780195863
Epoch 558
Epoch 558 :: Batch 0/1
Batch Loss = 3.4600502945570533
558, epoch_train_loss=3.4600502945570533
Epoch 559
Epoch 559 :: Batch 0/1
Batch Loss = 3.4599697676845467
559, epoch_train_loss=3.4599697676845467
Epoch 560
Epoch 560 :: Batch 0/1
Batch Loss = 3.4600013558497515
560, epoch_train_loss=3.4600013558497515
Epoch 561
Epoch 561 :: Batch 0/1
Batch Loss = 3.4599632002460567
561, epoch_train_loss=3.4599632002460567
Epoch 562
Epoch 562 :: Batch 0/1
Batch Loss = 3.4599636202107473
562, epoch_train_loss=3.4599636202107473
Epoch 563
Epoch 563 :: Batch 0/1
Batch Loss = 3.4599439455319034
563, epoch_train_loss=3.4599439455319034
Epoch 564
Epoch 564 :: Batch 0/1
Batch Loss = 3.4599131081736636
564, epoch_train_loss=3.4599131081736636
Epoch 565
Epoch 565 :: Batch 0/1
Batch Loss = 3.4599176952570554
565, epoch_train_loss=3.4599176952570554
Epoch 566
Epoch 566 :: Batch 0/1
Batch Loss = 3.4598828462200273
566, epoch_train_loss=3.4598828462200273
Epoch 567
Epoch 567 :: Batch 0/1
Batch Loss = 3.4598943457195843
567, epoch_train_loss=3.4598943457195843
Epoch 568
Epoch 568 :: Batch 0/1
Batch Loss = 3.459854768581282
568, epoch_train_loss=3.459854768581282
Epoch 569
Epoch 569 :: Batch 0/1
Batch Loss = 3.459856298548439
569, epoch_train_loss=3.459856298548439
Epoch 570
Epoch 570 :: Batch 0/1
Batch Loss = 3.459827385668547
570, epoch_train_loss=3.459827385668547
Epoch 571
Epoch 571 :: Batch 0/1
Batch Loss = 3.4598246827195065
571, epoch_train_loss=3.4598246827195065
Epoch 572
Epoch 572 :: Batch 0/1
Batch Loss = 3.4598096847124626
572, epoch_train_loss=3.4598096847124626
Epoch 573
Epoch 573 :: Batch 0/1
Batch Loss = 3.459795232769386
573, epoch_train_loss=3.459795232769386
Epoch 574
Epoch 574 :: Batch 0/1
Batch Loss = 3.4597842822212757
574, epoch_train_loss=3.4597842822212757
Epoch 575
Epoch 575 :: Batch 0/1
Batch Loss = 3.4597628599900188
575, epoch_train_loss=3.4597628599900188
Epoch 576
Epoch 576 :: Batch 0/1
Batch Loss = 3.459758429070477
576, epoch_train_loss=3.459758429070477
Epoch 577
Epoch 577 :: Batch 0/1
Batch Loss = 3.459738248853882
577, epoch_train_loss=3.459738248853882
Epoch 578
Epoch 578 :: Batch 0/1
Batch Loss = 3.4597340604463613
578, epoch_train_loss=3.4597340604463613
Epoch 579
Epoch 579 :: Batch 0/1
Batch Loss = 3.459713641992537
579, epoch_train_loss=3.459713641992537
Epoch 580
Epoch 580 :: Batch 0/1
Batch Loss = 3.4597060271616855
580, epoch_train_loss=3.4597060271616855
Epoch 581
Epoch 581 :: Batch 0/1
Batch Loss = 3.459689818778158
581, epoch_train_loss=3.459689818778158
Epoch 582
Epoch 582 :: Batch 0/1
Batch Loss = 3.4596807504904517
582, epoch_train_loss=3.4596807504904517
Epoch 583
Epoch 583 :: Batch 0/1
Batch Loss = 3.459668055729736
583, epoch_train_loss=3.459668055729736
Epoch 584
Epoch 584 :: Batch 0/1
Batch Loss = 3.459655152553867
584, epoch_train_loss=3.459655152553867
Epoch 585
Epoch 585 :: Batch 0/1
Batch Loss = 3.4596437474506825
585, epoch_train_loss=3.4596437474506825
Epoch 586
Epoch 586 :: Batch 0/1
Batch Loss = 3.459629527582761
586, epoch_train_loss=3.459629527582761
Epoch 587
Epoch 587 :: Batch 0/1
Batch Loss = 3.459620476458465
587, epoch_train_loss=3.459620476458465
Epoch 588
Epoch 588 :: Batch 0/1
Batch Loss = 3.4596064567320224
588, epoch_train_loss=3.4596064567320224
Epoch 589
Epoch 589 :: Batch 0/1
Batch Loss = 3.4595972310897336
589, epoch_train_loss=3.4595972310897336
Epoch 590
Epoch 590 :: Batch 0/1
Batch Loss = 3.4595828177273087
590, epoch_train_loss=3.4595828177273087
Epoch 591
Epoch 591 :: Batch 0/1
Batch Loss = 3.459572924804648
591, epoch_train_loss=3.459572924804648
Epoch 592
Epoch 592 :: Batch 0/1
Batch Loss = 3.459559747953431
592, epoch_train_loss=3.459559747953431
Epoch 593
Epoch 593 :: Batch 0/1
Batch Loss = 3.459549576048556
593, epoch_train_loss=3.459549576048556
Epoch 594
Epoch 594 :: Batch 0/1
Batch Loss = 3.4595372627893672
594, epoch_train_loss=3.4595372627893672
Epoch 595
Epoch 595 :: Batch 0/1
Batch Loss = 3.4595260489074997
595, epoch_train_loss=3.4595260489074997
Epoch 596
Epoch 596 :: Batch 0/1
Batch Loss = 3.4595143997303013
596, epoch_train_loss=3.4595143997303013
Epoch 597
Epoch 597 :: Batch 0/1
Batch Loss = 3.4595029897198586
597, epoch_train_loss=3.4595029897198586
Epoch 598
Epoch 598 :: Batch 0/1
Batch Loss = 3.4594922622749977
598, epoch_train_loss=3.4594922622749977
Epoch 599
Epoch 599 :: Batch 0/1
Batch Loss = 3.4594806439290386
599, epoch_train_loss=3.4594806439290386
Epoch 600
Epoch 600 :: Batch 0/1
Batch Loss = 3.459470012545785
600, epoch_train_loss=3.459470012545785
Epoch 601
Epoch 601 :: Batch 0/1
Batch Loss = 3.4594581913545537
601, epoch_train_loss=3.4594581913545537
Epoch 602
Epoch 602 :: Batch 0/1
Batch Loss = 3.4594478056195337
602, epoch_train_loss=3.4594478056195337
Epoch 603
Epoch 603 :: Batch 0/1
Batch Loss = 3.4594363595820936
603, epoch_train_loss=3.4594363595820936
Epoch 604
Epoch 604 :: Batch 0/1
Batch Loss = 3.459426141041551
604, epoch_train_loss=3.459426141041551
Epoch 605
Epoch 605 :: Batch 0/1
Batch Loss = 3.4594148709569863
605, epoch_train_loss=3.4594148709569863
Epoch 606
Epoch 606 :: Batch 0/1
Batch Loss = 3.459404556623595
606, epoch_train_loss=3.459404556623595
Epoch 607
Epoch 607 :: Batch 0/1
Batch Loss = 3.4593935867345573
607, epoch_train_loss=3.4593935867345573
Epoch 608
Epoch 608 :: Batch 0/1
Batch Loss = 3.45938337608187
608, epoch_train_loss=3.45938337608187
Epoch 609
Epoch 609 :: Batch 0/1
Batch Loss = 3.459372755998772
609, epoch_train_loss=3.459372755998772
Epoch 610
Epoch 610 :: Batch 0/1
Batch Loss = 3.4593625180735708
610, epoch_train_loss=3.4593625180735708
Epoch 611
Epoch 611 :: Batch 0/1
Batch Loss = 3.459352079647801
611, epoch_train_loss=3.459352079647801
Epoch 612
Epoch 612 :: Batch 0/1
Batch Loss = 3.459341882749813
612, epoch_train_loss=3.459341882749813
Epoch 613
Epoch 613 :: Batch 0/1
Batch Loss = 3.459331749169481
613, epoch_train_loss=3.459331749169481
Epoch 614
Epoch 614 :: Batch 0/1
Batch Loss = 3.459321697973688
614, epoch_train_loss=3.459321697973688
Epoch 615
Epoch 615 :: Batch 0/1
Batch Loss = 3.459311760154801
615, epoch_train_loss=3.459311760154801
Epoch 616
Epoch 616 :: Batch 0/1
Batch Loss = 3.4593017697455326
616, epoch_train_loss=3.4593017697455326
Epoch 617
Epoch 617 :: Batch 0/1
Batch Loss = 3.4592919912283038
617, epoch_train_loss=3.4592919912283038
Epoch 618
Epoch 618 :: Batch 0/1
Batch Loss = 3.459282163344976
618, epoch_train_loss=3.459282163344976
Epoch 619
Epoch 619 :: Batch 0/1
Batch Loss = 3.4592725830991085
619, epoch_train_loss=3.4592725830991085
Epoch 620
Epoch 620 :: Batch 0/1
Batch Loss = 3.4592629027678115
620, epoch_train_loss=3.4592629027678115
Epoch 621
Epoch 621 :: Batch 0/1
Batch Loss = 3.459253447640148
621, epoch_train_loss=3.459253447640148
Epoch 622
Epoch 622 :: Batch 0/1
Batch Loss = 3.459243909412582
622, epoch_train_loss=3.459243909412582
Epoch 623
Epoch 623 :: Batch 0/1
Batch Loss = 3.4592346111328487
623, epoch_train_loss=3.4592346111328487
Epoch 624
Epoch 624 :: Batch 0/1
Batch Loss = 3.459225252604957
624, epoch_train_loss=3.459225252604957
Epoch 625
Epoch 625 :: Batch 0/1
Batch Loss = 3.4592160935086507
625, epoch_train_loss=3.4592160935086507
Epoch 626
Epoch 626 :: Batch 0/1
Batch Loss = 3.459206876472845
626, epoch_train_loss=3.459206876472845
Epoch 627
Epoch 627 :: Batch 0/1
Batch Loss = 3.4591978406930313
627, epoch_train_loss=3.4591978406930313
Epoch 628
Epoch 628 :: Batch 0/1
Batch Loss = 3.459188785942396
628, epoch_train_loss=3.459188785942396
Epoch 629
Epoch 629 :: Batch 0/1
Batch Loss = 3.4591798968502
629, epoch_train_loss=3.4591798968502
Epoch 630
Epoch 630 :: Batch 0/1
Batch Loss = 3.459170996205262
630, epoch_train_loss=3.459170996205262
Epoch 631
Epoch 631 :: Batch 0/1
Batch Loss = 3.4591622268257844
631, epoch_train_loss=3.4591622268257844
Epoch 632
Epoch 632 :: Batch 0/1
Batch Loss = 3.4591534612616535
632, epoch_train_loss=3.4591534612616535
Epoch 633
Epoch 633 :: Batch 0/1
Batch Loss = 3.4591448196714003
633, epoch_train_loss=3.4591448196714003
Epoch 634
Epoch 634 :: Batch 0/1
Batch Loss = 3.4591361997276273
634, epoch_train_loss=3.4591361997276273
Epoch 635
Epoch 635 :: Batch 0/1
Batch Loss = 3.459127684916967
635, epoch_train_loss=3.459127684916967
Epoch 636
Epoch 636 :: Batch 0/1
Batch Loss = 3.4591191945769713
636, epoch_train_loss=3.4591191945769713
Epoch 637
Epoch 637 :: Batch 0/1
Batch Loss = 3.4591107974707773
637, epoch_train_loss=3.4591107974707773
Epoch 638
Epoch 638 :: Batch 0/1
Batch Loss = 3.4591024362633056
638, epoch_train_loss=3.4591024362633056
Epoch 639
Epoch 639 :: Batch 0/1
Batch Loss = 3.45909416140844
639, epoch_train_loss=3.45909416140844
Epoch 640
Epoch 640 :: Batch 0/1
Batch Loss = 3.4590859235463154
640, epoch_train_loss=3.4590859235463154
Epoch 641
Epoch 641 :: Batch 0/1
Batch Loss = 3.4590777588468797
641, epoch_train_loss=3.4590777588468797
Epoch 642
Epoch 642 :: Batch 0/1
Batch Loss = 3.459069631741584
642, epoch_train_loss=3.459069631741584
Epoch 643
Epoch 643 :: Batch 0/1
Batch Loss = 3.459061574031043
643, epoch_train_loss=3.459061574031043
Epoch 644
Epoch 644 :: Batch 0/1
Batch Loss = 3.459053557966211
644, epoch_train_loss=3.459053557966211
Epoch 645
Epoch 645 :: Batch 0/1
Batch Loss = 3.4590456045953584
645, epoch_train_loss=3.4590456045953584
Epoch 646
Epoch 646 :: Batch 0/1
Batch Loss = 3.459037689086338
646, epoch_train_loss=3.459037689086338
Epoch 647
Epoch 647 :: Batch 0/1
Batch Loss = 3.459029829717287
647, epoch_train_loss=3.459029829717287
Epoch 648
Epoch 648 :: Batch 0/1
Batch Loss = 3.4590220088888546
648, epoch_train_loss=3.4590220088888546
Epoch 649
Epoch 649 :: Batch 0/1
Batch Loss = 3.4590142408119045
649, epoch_train_loss=3.4590142408119045
Epoch 650
Epoch 650 :: Batch 0/1
Batch Loss = 3.459006508195171
650, epoch_train_loss=3.459006508195171
Epoch 651
Epoch 651 :: Batch 0/1
Batch Loss = 3.45899882184825
651, epoch_train_loss=3.45899882184825
Epoch 652
Epoch 652 :: Batch 0/1
Batch Loss = 3.458991168731487
652, epoch_train_loss=3.458991168731487
Epoch 653
Epoch 653 :: Batch 0/1
Batch Loss = 3.4589835589925557
653, epoch_train_loss=3.4589835589925557
Epoch 654
Epoch 654 :: Batch 0/1
Batch Loss = 3.45897598070228
654, epoch_train_loss=3.45897598070228
Epoch 655
Epoch 655 :: Batch 0/1
Batch Loss = 3.4589684412135178
655, epoch_train_loss=3.4589684412135178
Epoch 656
Epoch 656 :: Batch 0/1
Batch Loss = 3.458960929786258
656, epoch_train_loss=3.458960929786258
Epoch 657
Epoch 657 :: Batch 0/1
Batch Loss = 3.458953452902279
657, epoch_train_loss=3.458953452902279
Epoch 658
Epoch 658 :: Batch 0/1
Batch Loss = 3.4589460015316837
658, epoch_train_loss=3.4589460015316837
Epoch 659
Epoch 659 :: Batch 0/1
Batch Loss = 3.4589385812813087
659, epoch_train_loss=3.4589385812813087
Epoch 660
Epoch 660 :: Batch 0/1
Batch Loss = 3.4589311837920547
660, epoch_train_loss=3.4589311837920547
Epoch 661
Epoch 661 :: Batch 0/1
Batch Loss = 3.458923813096866
661, epoch_train_loss=3.458923813096866
Epoch 662
Epoch 662 :: Batch 0/1
Batch Loss = 3.4589164618189545
662, epoch_train_loss=3.4589164618189545
Epoch 663
Epoch 663 :: Batch 0/1
Batch Loss = 3.4589091339322717
663, epoch_train_loss=3.4589091339322717
Epoch 664
Epoch 664 :: Batch 0/1
Batch Loss = 3.458901823236843
664, epoch_train_loss=3.458901823236843
Epoch 665
Epoch 665 :: Batch 0/1
Batch Loss = 3.4588945323106697
665, epoch_train_loss=3.4588945323106697
Epoch 666
Epoch 666 :: Batch 0/1
Batch Loss = 3.45888725525502
666, epoch_train_loss=3.45888725525502
Epoch 667
Epoch 667 :: Batch 0/1
Batch Loss = 3.4588799943745507
667, epoch_train_loss=3.4588799943745507
Epoch 668
Epoch 668 :: Batch 0/1
Batch Loss = 3.4588727450578824
668, epoch_train_loss=3.4588727450578824
Epoch 669
Epoch 669 :: Batch 0/1
Batch Loss = 3.458865508771708
669, epoch_train_loss=3.458865508771708
Epoch 670
Epoch 670 :: Batch 0/1
Batch Loss = 3.4588582811339226
670, epoch_train_loss=3.4588582811339226
Epoch 671
Epoch 671 :: Batch 0/1
Batch Loss = 3.45885106302934
671, epoch_train_loss=3.45885106302934
Epoch 672
Epoch 672 :: Batch 0/1
Batch Loss = 3.4588438509907005
672, epoch_train_loss=3.4588438509907005
Epoch 673
Epoch 673 :: Batch 0/1
Batch Loss = 3.458836645423058
673, epoch_train_loss=3.458836645423058
Epoch 674
Epoch 674 :: Batch 0/1
Batch Loss = 3.458829443351129
674, epoch_train_loss=3.458829443351129
Epoch 675
Epoch 675 :: Batch 0/1
Batch Loss = 3.45882224470562
675, epoch_train_loss=3.45882224470562
Epoch 676
Epoch 676 :: Batch 0/1
Batch Loss = 3.458815047004314
676, epoch_train_loss=3.458815047004314
Epoch 677
Epoch 677 :: Batch 0/1
Batch Loss = 3.458807849692625
677, epoch_train_loss=3.458807849692625
Epoch 678
Epoch 678 :: Batch 0/1
Batch Loss = 3.458800650776446
678, epoch_train_loss=3.458800650776446
Epoch 679
Epoch 679 :: Batch 0/1
Batch Loss = 3.458793449482566
679, epoch_train_loss=3.458793449482566
Epoch 680
Epoch 680 :: Batch 0/1
Batch Loss = 3.4587862441106667
680, epoch_train_loss=3.4587862441106667
Epoch 681
Epoch 681 :: Batch 0/1
Batch Loss = 3.4587790334535216
681, epoch_train_loss=3.4587790334535216
Epoch 682
Epoch 682 :: Batch 0/1
Batch Loss = 3.4587718161539365
682, epoch_train_loss=3.4587718161539365
Epoch 683
Epoch 683 :: Batch 0/1
Batch Loss = 3.4587645910030456
683, epoch_train_loss=3.4587645910030456
Epoch 684
Epoch 684 :: Batch 0/1
Batch Loss = 3.458757356855793
684, epoch_train_loss=3.458757356855793
Epoch 685
Epoch 685 :: Batch 0/1
Batch Loss = 3.4587501122165834
685, epoch_train_loss=3.4587501122165834
Epoch 686
Epoch 686 :: Batch 0/1
Batch Loss = 3.458742856061321
686, epoch_train_loss=3.458742856061321
Epoch 687
Epoch 687 :: Batch 0/1
Batch Loss = 3.4587355869355316
687, epoch_train_loss=3.4587355869355316
Epoch 688
Epoch 688 :: Batch 0/1
Batch Loss = 3.4587283039044436
688, epoch_train_loss=3.4587283039044436
Epoch 689
Epoch 689 :: Batch 0/1
Batch Loss = 3.458721005417097
689, epoch_train_loss=3.458721005417097
Epoch 690
Epoch 690 :: Batch 0/1
Batch Loss = 3.458713690561741
690, epoch_train_loss=3.458713690561741
Epoch 691
Epoch 691 :: Batch 0/1
Batch Loss = 3.4587063578573267
691, epoch_train_loss=3.4587063578573267
Epoch 692
Epoch 692 :: Batch 0/1
Batch Loss = 3.4586990063679623
692, epoch_train_loss=3.4586990063679623
Epoch 693
Epoch 693 :: Batch 0/1
Batch Loss = 3.45869163464596
693, epoch_train_loss=3.45869163464596
Epoch 694
Epoch 694 :: Batch 0/1
Batch Loss = 3.458684241750077
694, epoch_train_loss=3.458684241750077
Epoch 695
Epoch 695 :: Batch 0/1
Batch Loss = 3.458676826314625
695, epoch_train_loss=3.458676826314625
Epoch 696
Epoch 696 :: Batch 0/1
Batch Loss = 3.4586693872912204
696, epoch_train_loss=3.4586693872912204
Epoch 697
Epoch 697 :: Batch 0/1
Batch Loss = 3.458661923374091
697, epoch_train_loss=3.458661923374091
Epoch 698
Epoch 698 :: Batch 0/1
Batch Loss = 3.4586544335065064
698, epoch_train_loss=3.4586544335065064
Epoch 699
Epoch 699 :: Batch 0/1
Batch Loss = 3.4586469164571443
699, epoch_train_loss=3.4586469164571443
Epoch 700
Epoch 700 :: Batch 0/1
Batch Loss = 3.4586393710615035
700, epoch_train_loss=3.4586393710615035
Epoch 701
Epoch 701 :: Batch 0/1
Batch Loss = 3.458631796131444
701, epoch_train_loss=3.458631796131444
Epoch 702
Epoch 702 :: Batch 0/1
Batch Loss = 3.4586241905025497
702, epoch_train_loss=3.4586241905025497
Epoch 703
Epoch 703 :: Batch 0/1
Batch Loss = 3.4586165530292208
703, epoch_train_loss=3.4586165530292208
Epoch 704
Epoch 704 :: Batch 0/1
Batch Loss = 3.458608882480212
704, epoch_train_loss=3.458608882480212
Epoch 705
Epoch 705 :: Batch 0/1
Batch Loss = 3.4586011777241383
705, epoch_train_loss=3.4586011777241383
Epoch 706
Epoch 706 :: Batch 0/1
Batch Loss = 3.4585934375349425
706, epoch_train_loss=3.4585934375349425
Epoch 707
Epoch 707 :: Batch 0/1
Batch Loss = 3.4585856607702268
707, epoch_train_loss=3.4585856607702268
Epoch 708
Epoch 708 :: Batch 0/1
Batch Loss = 3.458577846174604
708, epoch_train_loss=3.458577846174604
Epoch 709
Epoch 709 :: Batch 0/1
Batch Loss = 3.4585699925990596
709, epoch_train_loss=3.4585699925990596
Epoch 710
Epoch 710 :: Batch 0/1
Batch Loss = 3.4585620987987493
710, epoch_train_loss=3.4585620987987493
Epoch 711
Epoch 711 :: Batch 0/1
Batch Loss = 3.4585541635815304
711, epoch_train_loss=3.4585541635815304
Epoch 712
Epoch 712 :: Batch 0/1
Batch Loss = 3.458546185694476
712, epoch_train_loss=3.458546185694476
Epoch 713
Epoch 713 :: Batch 0/1
Batch Loss = 3.4585381639344397
713, epoch_train_loss=3.4585381639344397
Epoch 714
Epoch 714 :: Batch 0/1
Batch Loss = 3.4585300970537207
714, epoch_train_loss=3.4585300970537207
Epoch 715
Epoch 715 :: Batch 0/1
Batch Loss = 3.458521983793156
715, epoch_train_loss=3.458521983793156
Epoch 716
Epoch 716 :: Batch 0/1
Batch Loss = 3.458513822893097
716, epoch_train_loss=3.458513822893097
Epoch 717
Epoch 717 :: Batch 0/1
Batch Loss = 3.458505613080316
717, epoch_train_loss=3.458505613080316
Epoch 718
Epoch 718 :: Batch 0/1
Batch Loss = 3.4584973530771594
718, epoch_train_loss=3.4584973530771594
Epoch 719
Epoch 719 :: Batch 0/1
Batch Loss = 3.4584890415624003
719, epoch_train_loss=3.4584890415624003
Epoch 720
Epoch 720 :: Batch 0/1
Batch Loss = 3.4584806772388315
720, epoch_train_loss=3.4584806772388315
Epoch 721
Epoch 721 :: Batch 0/1
Batch Loss = 3.458472258770884
721, epoch_train_loss=3.458472258770884
Epoch 722
Epoch 722 :: Batch 0/1
Batch Loss = 3.4584637848224484
722, epoch_train_loss=3.4584637848224484
Epoch 723
Epoch 723 :: Batch 0/1
Batch Loss = 3.4584552540214926
723, epoch_train_loss=3.4584552540214926
Epoch 724
Epoch 724 :: Batch 0/1
Batch Loss = 3.458446665006191
724, epoch_train_loss=3.458446665006191
Epoch 725
Epoch 725 :: Batch 0/1
Batch Loss = 3.4584380163799766
725, epoch_train_loss=3.4584380163799766
Epoch 726
Epoch 726 :: Batch 0/1
Batch Loss = 3.458429306727905
726, epoch_train_loss=3.458429306727905
Epoch 727
Epoch 727 :: Batch 0/1
Batch Loss = 3.4584205346174306
727, epoch_train_loss=3.4584205346174306
Epoch 728
Epoch 728 :: Batch 0/1
Batch Loss = 3.4584116986030153
728, epoch_train_loss=3.4584116986030153
Epoch 729
Epoch 729 :: Batch 0/1
Batch Loss = 3.458402797213583
729, epoch_train_loss=3.458402797213583
Epoch 730
Epoch 730 :: Batch 0/1
Batch Loss = 3.458393828947434
730, epoch_train_loss=3.458393828947434
Epoch 731
Epoch 731 :: Batch 0/1
Batch Loss = 3.4583847922959507
731, epoch_train_loss=3.4583847922959507
Epoch 732
Epoch 732 :: Batch 0/1
Batch Loss = 3.4583756857208523
732, epoch_train_loss=3.4583756857208523
Epoch 733
Epoch 733 :: Batch 0/1
Batch Loss = 3.4583665076589822
733, epoch_train_loss=3.4583665076589822
Epoch 734
Epoch 734 :: Batch 0/1
Batch Loss = 3.4583572565161336
734, epoch_train_loss=3.4583572565161336
Epoch 735
Epoch 735 :: Batch 0/1
Batch Loss = 3.4583479306846856
735, epoch_train_loss=3.4583479306846856
Epoch 736
Epoch 736 :: Batch 0/1
Batch Loss = 3.4583385285240262
736, epoch_train_loss=3.4583385285240262
Epoch 737
Epoch 737 :: Batch 0/1
Batch Loss = 3.4583290483599916
737, epoch_train_loss=3.4583290483599916
Epoch 738
Epoch 738 :: Batch 0/1
Batch Loss = 3.458319488494752
738, epoch_train_loss=3.458319488494752
Epoch 739
Epoch 739 :: Batch 0/1
Batch Loss = 3.458309847202152
739, epoch_train_loss=3.458309847202152
Epoch 740
Epoch 740 :: Batch 0/1
Batch Loss = 3.4583001227228736
740, epoch_train_loss=3.4583001227228736
Epoch 741
Epoch 741 :: Batch 0/1
Batch Loss = 3.458290313259006
741, epoch_train_loss=3.458290313259006
Epoch 742
Epoch 742 :: Batch 0/1
Batch Loss = 3.45828041698989
742, epoch_train_loss=3.45828041698989
Epoch 743
Epoch 743 :: Batch 0/1
Batch Loss = 3.4582704320561817
743, epoch_train_loss=3.4582704320561817
Epoch 744
Epoch 744 :: Batch 0/1
Batch Loss = 3.458260356559912
744, epoch_train_loss=3.458260356559912
Epoch 745
Epoch 745 :: Batch 0/1
Batch Loss = 3.4582501885659
745, epoch_train_loss=3.4582501885659
Epoch 746
Epoch 746 :: Batch 0/1
Batch Loss = 3.458239926106894
746, epoch_train_loss=3.458239926106894
Epoch 747
Epoch 747 :: Batch 0/1
Batch Loss = 3.4582295671723102
747, epoch_train_loss=3.4582295671723102
Epoch 748
Epoch 748 :: Batch 0/1
Batch Loss = 3.458219109706641
748, epoch_train_loss=3.458219109706641
Epoch 749
Epoch 749 :: Batch 0/1
Batch Loss = 3.4582085516172127
749, epoch_train_loss=3.4582085516172127
Epoch 750
Epoch 750 :: Batch 0/1
Batch Loss = 3.458197890769619
750, epoch_train_loss=3.458197890769619
Epoch 751
Epoch 751 :: Batch 0/1
Batch Loss = 3.4581871249789105
751, epoch_train_loss=3.4581871249789105
Epoch 752
Epoch 752 :: Batch 0/1
Batch Loss = 3.4581762520129264
752, epoch_train_loss=3.4581762520129264
Epoch 753
Epoch 753 :: Batch 0/1
Batch Loss = 3.4581652695969716
753, epoch_train_loss=3.4581652695969716
Epoch 754
Epoch 754 :: Batch 0/1
Batch Loss = 3.4581541754042817
754, epoch_train_loss=3.4581541754042817
Epoch 755
Epoch 755 :: Batch 0/1
Batch Loss = 3.4581429670528365
755, epoch_train_loss=3.4581429670528365
Epoch 756
Epoch 756 :: Batch 0/1
Batch Loss = 3.4581316421102057
756, epoch_train_loss=3.4581316421102057
Epoch 757
Epoch 757 :: Batch 0/1
Batch Loss = 3.458120198091607
757, epoch_train_loss=3.458120198091607
Epoch 758
Epoch 758 :: Batch 0/1
Batch Loss = 3.4581086324519044
758, epoch_train_loss=3.4581086324519044
Epoch 759
Epoch 759 :: Batch 0/1
Batch Loss = 3.458096942585491
759, epoch_train_loss=3.458096942585491
Epoch 760
Epoch 760 :: Batch 0/1
Batch Loss = 3.4580851258298213
760, epoch_train_loss=3.4580851258298213
Epoch 761
Epoch 761 :: Batch 0/1
Batch Loss = 3.4580731794606767
761, epoch_train_loss=3.4580731794606767
Epoch 762
Epoch 762 :: Batch 0/1
Batch Loss = 3.458061100684544
762, epoch_train_loss=3.458061100684544
Epoch 763
Epoch 763 :: Batch 0/1
Batch Loss = 3.4580488866425894
763, epoch_train_loss=3.4580488866425894
Epoch 764
Epoch 764 :: Batch 0/1
Batch Loss = 3.45803653440982
764, epoch_train_loss=3.45803653440982
Epoch 765
Epoch 765 :: Batch 0/1
Batch Loss = 3.4580240409868943
765, epoch_train_loss=3.4580240409868943
Epoch 766
Epoch 766 :: Batch 0/1
Batch Loss = 3.458011403298753
766, epoch_train_loss=3.458011403298753
Epoch 767
Epoch 767 :: Batch 0/1
Batch Loss = 3.4579986181967586
767, epoch_train_loss=3.4579986181967586
Epoch 768
Epoch 768 :: Batch 0/1
Batch Loss = 3.4579856824553747
768, epoch_train_loss=3.4579856824553747
Epoch 769
Epoch 769 :: Batch 0/1
Batch Loss = 3.4579725927633014
769, epoch_train_loss=3.4579725927633014
Epoch 770
Epoch 770 :: Batch 0/1
Batch Loss = 3.4579593457250883
770, epoch_train_loss=3.4579593457250883
Epoch 771
Epoch 771 :: Batch 0/1
Batch Loss = 3.457945937863123
771, epoch_train_loss=3.457945937863123
Epoch 772
Epoch 772 :: Batch 0/1
Batch Loss = 3.457932365605432
772, epoch_train_loss=3.457932365605432
Epoch 773
Epoch 773 :: Batch 0/1
Batch Loss = 3.4579186252873613
773, epoch_train_loss=3.4579186252873613
Epoch 774
Epoch 774 :: Batch 0/1
Batch Loss = 3.4579047131491794
774, epoch_train_loss=3.4579047131491794
Epoch 775
Epoch 775 :: Batch 0/1
Batch Loss = 3.457890625334293
775, epoch_train_loss=3.457890625334293
Epoch 776
Epoch 776 :: Batch 0/1
Batch Loss = 3.457876357879522
776, epoch_train_loss=3.457876357879522
Epoch 777
Epoch 777 :: Batch 0/1
Batch Loss = 3.457861906715449
777, epoch_train_loss=3.457861906715449
Epoch 778
Epoch 778 :: Batch 0/1
Batch Loss = 3.4578472676666463
778, epoch_train_loss=3.4578472676666463
Epoch 779
Epoch 779 :: Batch 0/1
Batch Loss = 3.45783243644351
779, epoch_train_loss=3.45783243644351
Epoch 780
Epoch 780 :: Batch 0/1
Batch Loss = 3.4578174086355453
780, epoch_train_loss=3.4578174086355453
Epoch 781
Epoch 781 :: Batch 0/1
Batch Loss = 3.457802179714993
781, epoch_train_loss=3.457802179714993
Epoch 782
Epoch 782 :: Batch 0/1
Batch Loss = 3.4577867450303637
782, epoch_train_loss=3.4577867450303637
Epoch 783
Epoch 783 :: Batch 0/1
Batch Loss = 3.4577710997959197
783, epoch_train_loss=3.4577710997959197
Epoch 784
Epoch 784 :: Batch 0/1
Batch Loss = 3.4577552390956283
784, epoch_train_loss=3.4577552390956283
Epoch 785
Epoch 785 :: Batch 0/1
Batch Loss = 3.4577391578749586
785, epoch_train_loss=3.4577391578749586
Epoch 786
Epoch 786 :: Batch 0/1
Batch Loss = 3.457722850935856
786, epoch_train_loss=3.457722850935856
Epoch 787
Epoch 787 :: Batch 0/1
Batch Loss = 3.4577063129315184
787, epoch_train_loss=3.4577063129315184
Epoch 788
Epoch 788 :: Batch 0/1
Batch Loss = 3.457689538366152
788, epoch_train_loss=3.457689538366152
Epoch 789
Epoch 789 :: Batch 0/1
Batch Loss = 3.4576725215834605
789, epoch_train_loss=3.4576725215834605
Epoch 790
Epoch 790 :: Batch 0/1
Batch Loss = 3.457655256763061
790, epoch_train_loss=3.457655256763061
Epoch 791
Epoch 791 :: Batch 0/1
Batch Loss = 3.4576377379170666
791, epoch_train_loss=3.4576377379170666
Epoch 792
Epoch 792 :: Batch 0/1
Batch Loss = 3.4576199588847927
792, epoch_train_loss=3.4576199588847927
Epoch 793
Epoch 793 :: Batch 0/1
Batch Loss = 3.4576019133234497
793, epoch_train_loss=3.4576019133234497
Epoch 794
Epoch 794 :: Batch 0/1
Batch Loss = 3.4575835947033973
794, epoch_train_loss=3.4575835947033973
Epoch 795
Epoch 795 :: Batch 0/1
Batch Loss = 3.457564996304483
795, epoch_train_loss=3.457564996304483
Epoch 796
Epoch 796 :: Batch 0/1
Batch Loss = 3.4575461112068697
796, epoch_train_loss=3.4575461112068697
Epoch 797
Epoch 797 :: Batch 0/1
Batch Loss = 3.4575269322831153
797, epoch_train_loss=3.4575269322831153
Epoch 798
Epoch 798 :: Batch 0/1
Batch Loss = 3.457507452194321
798, epoch_train_loss=3.457507452194321
Epoch 799
Epoch 799 :: Batch 0/1
Batch Loss = 3.4574876633824343
799, epoch_train_loss=3.4574876633824343
Epoch 800
Epoch 800 :: Batch 0/1
Batch Loss = 3.4574675580600753
800, epoch_train_loss=3.4574675580600753
Epoch 801
Epoch 801 :: Batch 0/1
Batch Loss = 3.457447128204776
801, epoch_train_loss=3.457447128204776
Epoch 802
Epoch 802 :: Batch 0/1
Batch Loss = 3.4574263655526885
802, epoch_train_loss=3.4574263655526885
Epoch 803
Epoch 803 :: Batch 0/1
Batch Loss = 3.457405261586046
803, epoch_train_loss=3.457405261586046
Epoch 804
Epoch 804 :: Batch 0/1
Batch Loss = 3.457383807531107
804, epoch_train_loss=3.457383807531107
Epoch 805
Epoch 805 :: Batch 0/1
Batch Loss = 3.457361994340617
805, epoch_train_loss=3.457361994340617
Epoch 806
Epoch 806 :: Batch 0/1
Batch Loss = 3.4573398126948764
806, epoch_train_loss=3.4573398126948764
Epoch 807
Epoch 807 :: Batch 0/1
Batch Loss = 3.4573172529840055
807, epoch_train_loss=3.4573172529840055
Epoch 808
Epoch 808 :: Batch 0/1
Batch Loss = 3.45729430530047
808, epoch_train_loss=3.45729430530047
Epoch 809
Epoch 809 :: Batch 0/1
Batch Loss = 3.457270959434816
809, epoch_train_loss=3.457270959434816
Epoch 810
Epoch 810 :: Batch 0/1
Batch Loss = 3.4572472048577128
810, epoch_train_loss=3.4572472048577128
Epoch 811
Epoch 811 :: Batch 0/1
Batch Loss = 3.457223030710219
811, epoch_train_loss=3.457223030710219
Epoch 812
Epoch 812 :: Batch 0/1
Batch Loss = 3.457198425800168
812, epoch_train_loss=3.457198425800168
Epoch 813
Epoch 813 :: Batch 0/1
Batch Loss = 3.457173378583366
813, epoch_train_loss=3.457173378583366
Epoch 814
Epoch 814 :: Batch 0/1
Batch Loss = 3.4571478771512383
814, epoch_train_loss=3.4571478771512383
Epoch 815
Epoch 815 :: Batch 0/1
Batch Loss = 3.4571219092248624
815, epoch_train_loss=3.4571219092248624
Epoch 816
Epoch 816 :: Batch 0/1
Batch Loss = 3.4570954621439274
816, epoch_train_loss=3.4570954621439274
Epoch 817
Epoch 817 :: Batch 0/1
Batch Loss = 3.457068522840135
817, epoch_train_loss=3.457068522840135
Epoch 818
Epoch 818 :: Batch 0/1
Batch Loss = 3.4570410778378338
818, epoch_train_loss=3.4570410778378338
Epoch 819
Epoch 819 :: Batch 0/1
Batch Loss = 3.457013113239088
819, epoch_train_loss=3.457013113239088
Epoch 820
Epoch 820 :: Batch 0/1
Batch Loss = 3.4569846147012675
820, epoch_train_loss=3.4569846147012675
Epoch 821
Epoch 821 :: Batch 0/1
Batch Loss = 3.456955567432018
821, epoch_train_loss=3.456955567432018
Epoch 822
Epoch 822 :: Batch 0/1
Batch Loss = 3.456925956167046
822, epoch_train_loss=3.456925956167046
Epoch 823
Epoch 823 :: Batch 0/1
Batch Loss = 3.4568957651597723
823, epoch_train_loss=3.4568957651597723
Epoch 824
Epoch 824 :: Batch 0/1
Batch Loss = 3.4568649781654583
824, epoch_train_loss=3.4568649781654583
Epoch 825
Epoch 825 :: Batch 0/1
Batch Loss = 3.4568335784196687
825, epoch_train_loss=3.4568335784196687
Epoch 826
Epoch 826 :: Batch 0/1
Batch Loss = 3.4568015486277153
826, epoch_train_loss=3.4568015486277153
Epoch 827
Epoch 827 :: Batch 0/1
Batch Loss = 3.456768870944053
827, epoch_train_loss=3.456768870944053
Epoch 828
Epoch 828 :: Batch 0/1
Batch Loss = 3.4567355269587607
828, epoch_train_loss=3.4567355269587607
Epoch 829
Epoch 829 :: Batch 0/1
Batch Loss = 3.4567014976717556
829, epoch_train_loss=3.4567014976717556
Epoch 830
Epoch 830 :: Batch 0/1
Batch Loss = 3.456666763480203
830, epoch_train_loss=3.456666763480203
Epoch 831
Epoch 831 :: Batch 0/1
Batch Loss = 3.456631304157798
831, epoch_train_loss=3.456631304157798
Epoch 832
Epoch 832 :: Batch 0/1
Batch Loss = 3.456595098835756
832, epoch_train_loss=3.456595098835756
Epoch 833
Epoch 833 :: Batch 0/1
Batch Loss = 3.456558125983869
833, epoch_train_loss=3.456558125983869
Epoch 834
Epoch 834 :: Batch 0/1
Batch Loss = 3.4565203633784667
834, epoch_train_loss=3.4565203633784667
Epoch 835
Epoch 835 :: Batch 0/1
Batch Loss = 3.4564817881021144
835, epoch_train_loss=3.4564817881021144
Epoch 836
Epoch 836 :: Batch 0/1
Batch Loss = 3.4564423764970105
836, epoch_train_loss=3.4564423764970105
Epoch 837
Epoch 837 :: Batch 0/1
Batch Loss = 3.4564021041655804
837, epoch_train_loss=3.4564021041655804
Epoch 838
Epoch 838 :: Batch 0/1
Batch Loss = 3.456360945923863
838, epoch_train_loss=3.456360945923863
Epoch 839
Epoch 839 :: Batch 0/1
Batch Loss = 3.456318875801256
839, epoch_train_loss=3.456318875801256
Epoch 840
Epoch 840 :: Batch 0/1
Batch Loss = 3.456275866989111
840, epoch_train_loss=3.456275866989111
Epoch 841
Epoch 841 :: Batch 0/1
Batch Loss = 3.456231891841608
841, epoch_train_loss=3.456231891841608
Epoch 842
Epoch 842 :: Batch 0/1
Batch Loss = 3.4561869218217147
842, epoch_train_loss=3.4561869218217147
Epoch 843
Epoch 843 :: Batch 0/1
Batch Loss = 3.456140927495279
843, epoch_train_loss=3.456140927495279
Epoch 844
Epoch 844 :: Batch 0/1
Batch Loss = 3.4560938784886153
844, epoch_train_loss=3.4560938784886153
Epoch 845
Epoch 845 :: Batch 0/1
Batch Loss = 3.4560457434607423
845, epoch_train_loss=3.4560457434607423
Epoch 846
Epoch 846 :: Batch 0/1
Batch Loss = 3.4559964900693294
846, epoch_train_loss=3.4559964900693294
Epoch 847
Epoch 847 :: Batch 0/1
Batch Loss = 3.455946084936368
847, epoch_train_loss=3.455946084936368
Epoch 848
Epoch 848 :: Batch 0/1
Batch Loss = 3.4558944936114204
848, epoch_train_loss=3.4558944936114204
Epoch 849
Epoch 849 :: Batch 0/1
Batch Loss = 3.455841680526775
849, epoch_train_loss=3.455841680526775
Epoch 850
Epoch 850 :: Batch 0/1
Batch Loss = 3.45578760896725
850, epoch_train_loss=3.45578760896725
Epoch 851
Epoch 851 :: Batch 0/1
Batch Loss = 3.4557322410084024
851, epoch_train_loss=3.4557322410084024
Epoch 852
Epoch 852 :: Batch 0/1
Batch Loss = 3.4556755374823744
852, epoch_train_loss=3.4556755374823744
Epoch 853
Epoch 853 :: Batch 0/1
Batch Loss = 3.45561745791107
853, epoch_train_loss=3.45561745791107
Epoch 854
Epoch 854 :: Batch 0/1
Batch Loss = 3.455557960446873
854, epoch_train_loss=3.455557960446873
Epoch 855
Epoch 855 :: Batch 0/1
Batch Loss = 3.4554970018203126
855, epoch_train_loss=3.4554970018203126
Epoch 856
Epoch 856 :: Batch 0/1
Batch Loss = 3.4554345372498854
856, epoch_train_loss=3.4554345372498854
Epoch 857
Epoch 857 :: Batch 0/1
Batch Loss = 3.4553705203822376
857, epoch_train_loss=3.4553705203822376
Epoch 858
Epoch 858 :: Batch 0/1
Batch Loss = 3.455304903189631
858, epoch_train_loss=3.455304903189631
Epoch 859
Epoch 859 :: Batch 0/1
Batch Loss = 3.4552376358721637
859, epoch_train_loss=3.4552376358721637
Epoch 860
Epoch 860 :: Batch 0/1
Batch Loss = 3.4551686667660997
860, epoch_train_loss=3.4551686667660997
Epoch 861
Epoch 861 :: Batch 0/1
Batch Loss = 3.455097942202991
861, epoch_train_loss=3.455097942202991
Epoch 862
Epoch 862 :: Batch 0/1
Batch Loss = 3.455025406392613
862, epoch_train_loss=3.455025406392613
Epoch 863
Epoch 863 :: Batch 0/1
Batch Loss = 3.4549510012770908
863, epoch_train_loss=3.4549510012770908
Epoch 864
Epoch 864 :: Batch 0/1
Batch Loss = 3.454874666350364
864, epoch_train_loss=3.454874666350364
Epoch 865
Epoch 865 :: Batch 0/1
Batch Loss = 3.4547963385008336
865, epoch_train_loss=3.4547963385008336
Epoch 866
Epoch 866 :: Batch 0/1
Batch Loss = 3.454715951789208
866, epoch_train_loss=3.454715951789208
Epoch 867
Epoch 867 :: Batch 0/1
Batch Loss = 3.454633437246271
867, epoch_train_loss=3.454633437246271
Epoch 868
Epoch 868 :: Batch 0/1
Batch Loss = 3.454548722625519
868, epoch_train_loss=3.454548722625519
Epoch 869
Epoch 869 :: Batch 0/1
Batch Loss = 3.4544617321452735
869, epoch_train_loss=3.4544617321452735
Epoch 870
Epoch 870 :: Batch 0/1
Batch Loss = 3.454372386177944
870, epoch_train_loss=3.454372386177944
Epoch 871
Epoch 871 :: Batch 0/1
Batch Loss = 3.4542806009527705
871, epoch_train_loss=3.4542806009527705
Epoch 872
Epoch 872 :: Batch 0/1
Batch Loss = 3.4541862882041636
872, epoch_train_loss=3.4541862882041636
Epoch 873
Epoch 873 :: Batch 0/1
Batch Loss = 3.454089354803187
873, epoch_train_loss=3.454089354803187
Epoch 874
Epoch 874 :: Batch 0/1
Batch Loss = 3.453989702320374
874, epoch_train_loss=3.453989702320374
Epoch 875
Epoch 875 :: Batch 0/1
Batch Loss = 3.4538872266378675
875, epoch_train_loss=3.4538872266378675
Epoch 876
Epoch 876 :: Batch 0/1
Batch Loss = 3.4537818174193404
876, epoch_train_loss=3.4537818174193404
Epoch 877
Epoch 877 :: Batch 0/1
Batch Loss = 3.4536733576272436
877, epoch_train_loss=3.4536733576272436
Epoch 878
Epoch 878 :: Batch 0/1
Batch Loss = 3.453561722961865
878, epoch_train_loss=3.453561722961865
Epoch 879
Epoch 879 :: Batch 0/1
Batch Loss = 3.4534467812655354
879, epoch_train_loss=3.4534467812655354
Epoch 880
Epoch 880 :: Batch 0/1
Batch Loss = 3.453328391882525
880, epoch_train_loss=3.453328391882525
Epoch 881
Epoch 881 :: Batch 0/1
Batch Loss = 3.4532064049701443
881, epoch_train_loss=3.4532064049701443
Epoch 882
Epoch 882 :: Batch 0/1
Batch Loss = 3.453080660757686
882, epoch_train_loss=3.453080660757686
Epoch 883
Epoch 883 :: Batch 0/1
Batch Loss = 3.452950988749255
883, epoch_train_loss=3.452950988749255
Epoch 884
Epoch 884 :: Batch 0/1
Batch Loss = 3.4528172068641796
884, epoch_train_loss=3.4528172068641796
Epoch 885
Epoch 885 :: Batch 0/1
Batch Loss = 3.452679120509979
885, epoch_train_loss=3.452679120509979
Epoch 886
Epoch 886 :: Batch 0/1
Batch Loss = 3.452536521569718
886, epoch_train_loss=3.452536521569718
Epoch 887
Epoch 887 :: Batch 0/1
Batch Loss = 3.452389187350206
887, epoch_train_loss=3.452389187350206
Epoch 888
Epoch 888 :: Batch 0/1
Batch Loss = 3.4522368793745986
888, epoch_train_loss=3.4522368793745986
Epoch 889
Epoch 889 :: Batch 0/1
Batch Loss = 3.452079342116454
889, epoch_train_loss=3.452079342116454
Epoch 890
Epoch 890 :: Batch 0/1
Batch Loss = 3.4519163016113317
890, epoch_train_loss=3.4519163016113317
Epoch 891
Epoch 891 :: Batch 0/1
Batch Loss = 3.4517474639321786
891, epoch_train_loss=3.4517474639321786
Epoch 892
Epoch 892 :: Batch 0/1
Batch Loss = 3.451572513580016
892, epoch_train_loss=3.451572513580016
Epoch 893
Epoch 893 :: Batch 0/1
Batch Loss = 3.451391111648784
893, epoch_train_loss=3.451391111648784
Epoch 894
Epoch 894 :: Batch 0/1
Batch Loss = 3.451202893862723
894, epoch_train_loss=3.451202893862723
Epoch 895
Epoch 895 :: Batch 0/1
Batch Loss = 3.451007468481092
895, epoch_train_loss=3.451007468481092
Epoch 896
Epoch 896 :: Batch 0/1
Batch Loss = 3.450804413925709
896, epoch_train_loss=3.450804413925709
Epoch 897
Epoch 897 :: Batch 0/1
Batch Loss = 3.4505932762413822
897, epoch_train_loss=3.4505932762413822
Epoch 898
Epoch 898 :: Batch 0/1
Batch Loss = 3.450373566370296
898, epoch_train_loss=3.450373566370296
Epoch 899
Epoch 899 :: Batch 0/1
Batch Loss = 3.450144757165542
899, epoch_train_loss=3.450144757165542
Epoch 900
Epoch 900 :: Batch 0/1
Batch Loss = 3.449906280124486
900, epoch_train_loss=3.449906280124486
Epoch 901
Epoch 901 :: Batch 0/1
Batch Loss = 3.4496575219021772
901, epoch_train_loss=3.4496575219021772
Epoch 902
Epoch 902 :: Batch 0/1
Batch Loss = 3.4493978206297315
902, epoch_train_loss=3.4493978206297315
Epoch 903
Epoch 903 :: Batch 0/1
Batch Loss = 3.449126461845891
903, epoch_train_loss=3.449126461845891
Epoch 904
Epoch 904 :: Batch 0/1
Batch Loss = 3.448842674341502
904, epoch_train_loss=3.448842674341502
Epoch 905
Epoch 905 :: Batch 0/1
Batch Loss = 3.4485456256137494
905, epoch_train_loss=3.4485456256137494
Epoch 906
Epoch 906 :: Batch 0/1
Batch Loss = 3.4482344172736332
906, epoch_train_loss=3.4482344172736332
Epoch 907
Epoch 907 :: Batch 0/1
Batch Loss = 3.447908080221548
907, epoch_train_loss=3.447908080221548
Epoch 908
Epoch 908 :: Batch 0/1
Batch Loss = 3.447565569791695
908, epoch_train_loss=3.447565569791695
Epoch 909
Epoch 909 :: Batch 0/1
Batch Loss = 3.4472057608055744
909, epoch_train_loss=3.4472057608055744
Epoch 910
Epoch 910 :: Batch 0/1
Batch Loss = 3.446827442920427
910, epoch_train_loss=3.446827442920427
Epoch 911
Epoch 911 :: Batch 0/1
Batch Loss = 3.4464293162114945
911, epoch_train_loss=3.4464293162114945
Epoch 912
Epoch 912 :: Batch 0/1
Batch Loss = 3.4460099871704184
912, epoch_train_loss=3.4460099871704184
Epoch 913
Epoch 913 :: Batch 0/1
Batch Loss = 3.445567965659551
913, epoch_train_loss=3.445567965659551
Epoch 914
Epoch 914 :: Batch 0/1
Batch Loss = 3.445101662882487
914, epoch_train_loss=3.445101662882487
Epoch 915
Epoch 915 :: Batch 0/1
Batch Loss = 3.444609390724393
915, epoch_train_loss=3.444609390724393
Epoch 916
Epoch 916 :: Batch 0/1
Batch Loss = 3.4440893632354608
916, epoch_train_loss=3.4440893632354608
Epoch 917
Epoch 917 :: Batch 0/1
Batch Loss = 3.4435397004211654
917, epoch_train_loss=3.4435397004211654
Epoch 918
Epoch 918 :: Batch 0/1
Batch Loss = 3.4429584350637628
918, epoch_train_loss=3.4429584350637628
Epoch 919
Epoch 919 :: Batch 0/1
Batch Loss = 3.4423435231140784
919, epoch_train_loss=3.4423435231140784
Epoch 920
Epoch 920 :: Batch 0/1
Batch Loss = 3.441692858195019
920, epoch_train_loss=3.441692858195019
Epoch 921
Epoch 921 :: Batch 0/1
Batch Loss = 3.4410042906635097
921, epoch_train_loss=3.4410042906635097
Epoch 922
Epoch 922 :: Batch 0/1
Batch Loss = 3.4402756514843356
922, epoch_train_loss=3.4402756514843356
Epoch 923
Epoch 923 :: Batch 0/1
Batch Loss = 3.439504780860464
923, epoch_train_loss=3.439504780860464
Epoch 924
Epoch 924 :: Batch 0/1
Batch Loss = 3.438689561108535
924, epoch_train_loss=3.438689561108535
Epoch 925
Epoch 925 :: Batch 0/1
Batch Loss = 3.4378279526674866
925, epoch_train_loss=3.4378279526674866
Epoch 926
Epoch 926 :: Batch 0/1
Batch Loss = 3.4369180313847587
926, epoch_train_loss=3.4369180313847587
Epoch 927
Epoch 927 :: Batch 0/1
Batch Loss = 3.435958024410224
927, epoch_train_loss=3.435958024410224
Epoch 928
Epoch 928 :: Batch 0/1
Batch Loss = 3.4349463412161905
928, epoch_train_loss=3.4349463412161905
Epoch 929
Epoch 929 :: Batch 0/1
Batch Loss = 3.433881595572545
929, epoch_train_loss=3.433881595572545
Epoch 930
Epoch 930 :: Batch 0/1
Batch Loss = 3.4327626145126686
930, epoch_train_loss=3.4327626145126686
Epoch 931
Epoch 931 :: Batch 0/1
Batch Loss = 3.431588429501083
931, epoch_train_loss=3.431588429501083
Epoch 932
Epoch 932 :: Batch 0/1
Batch Loss = 3.430358247427604
932, epoch_train_loss=3.430358247427604
Epoch 933
Epoch 933 :: Batch 0/1
Batch Loss = 3.4290714005020257
933, epoch_train_loss=3.4290714005020257
Epoch 934
Epoch 934 :: Batch 0/1
Batch Loss = 3.427727274724169
934, epoch_train_loss=3.427727274724169
Epoch 935
Epoch 935 :: Batch 0/1
Batch Loss = 3.426325222109903
935, epoch_train_loss=3.426325222109903
Epoch 936
Epoch 936 :: Batch 0/1
Batch Loss = 3.4248644582689014
936, epoch_train_loss=3.4248644582689014
Epoch 937
Epoch 937 :: Batch 0/1
Batch Loss = 3.4233439566987216
937, epoch_train_loss=3.4233439566987216
Epoch 938
Epoch 938 :: Batch 0/1
Batch Loss = 3.4217623196980296
938, epoch_train_loss=3.4217623196980296
Epoch 939
Epoch 939 :: Batch 0/1
Batch Loss = 3.420117692011528
939, epoch_train_loss=3.420117692011528
Epoch 940
Epoch 940 :: Batch 0/1
Batch Loss = 3.4184075019049796
940, epoch_train_loss=3.4184075019049796
Epoch 941
Epoch 941 :: Batch 0/1
Batch Loss = 3.4166288281177186
941, epoch_train_loss=3.4166288281177186
Epoch 942
Epoch 942 :: Batch 0/1
Batch Loss = 3.414778364391753
942, epoch_train_loss=3.414778364391753
Epoch 943
Epoch 943 :: Batch 0/1
Batch Loss = 3.412886391982353
943, epoch_train_loss=3.412886391982353
Epoch 944
Epoch 944 :: Batch 0/1
Batch Loss = 3.411513108536133
944, epoch_train_loss=3.411513108536133
Epoch 945
Epoch 945 :: Batch 0/1
Batch Loss = 3.4213741202870422
945, epoch_train_loss=3.4213741202870422
Epoch 946
Epoch 946 :: Batch 0/1
Batch Loss = 3.481764041781436
946, epoch_train_loss=3.481764041781436
Epoch 947
Epoch 947 :: Batch 0/1
Batch Loss = 3.6063655804872847
947, epoch_train_loss=3.6063655804872847
Epoch 948
Epoch 948 :: Batch 0/1
Batch Loss = 3.697407855832237
948, epoch_train_loss=3.697407855832237
Epoch 949
Epoch 949 :: Batch 0/1
Batch Loss = 3.4314631722922098
949, epoch_train_loss=3.4314631722922098
Epoch 950
Epoch 950 :: Batch 0/1
Batch Loss = 3.601986455087197
950, epoch_train_loss=3.601986455087197
Epoch 951
Epoch 951 :: Batch 0/1
Batch Loss = 3.5020913290989175
951, epoch_train_loss=3.5020913290989175
Epoch 952
Epoch 952 :: Batch 0/1
Batch Loss = 3.5307494607830265
952, epoch_train_loss=3.5307494607830265
Epoch 953
Epoch 953 :: Batch 0/1
Batch Loss = 3.4669511227312357
953, epoch_train_loss=3.4669511227312357
Epoch 954
Epoch 954 :: Batch 0/1
Batch Loss = 3.5129587533366045
954, epoch_train_loss=3.5129587533366045
Epoch 955
Epoch 955 :: Batch 0/1
Batch Loss = 3.4366236805594927
955, epoch_train_loss=3.4366236805594927
Epoch 956
Epoch 956 :: Batch 0/1
Batch Loss = 3.4981511543390655
956, epoch_train_loss=3.4981511543390655
Epoch 957
Epoch 957 :: Batch 0/1
Batch Loss = 3.429133753528415
957, epoch_train_loss=3.429133753528415
Epoch 958
Epoch 958 :: Batch 0/1
Batch Loss = 3.4651780924373456
958, epoch_train_loss=3.4651780924373456
Epoch 959
Epoch 959 :: Batch 0/1
Batch Loss = 3.440707003750926
959, epoch_train_loss=3.440707003750926
Epoch 960
Epoch 960 :: Batch 0/1
Batch Loss = 3.424362488607202
960, epoch_train_loss=3.424362488607202
Epoch 961
Epoch 961 :: Batch 0/1
Batch Loss = 3.4483154928273794
961, epoch_train_loss=3.4483154928273794
Epoch 962
Epoch 962 :: Batch 0/1
Batch Loss = 3.409654964609612
962, epoch_train_loss=3.409654964609612
Epoch 963
Epoch 963 :: Batch 0/1
Batch Loss = 3.42886332909592
963, epoch_train_loss=3.42886332909592
Epoch 964
Epoch 964 :: Batch 0/1
Batch Loss = 3.415390398087023
964, epoch_train_loss=3.415390398087023
Epoch 965
Epoch 965 :: Batch 0/1
Batch Loss = 3.402621288867108
965, epoch_train_loss=3.402621288867108
Epoch 966
Epoch 966 :: Batch 0/1
Batch Loss = 3.4167548698337886
966, epoch_train_loss=3.4167548698337886
Epoch 967
Epoch 967 :: Batch 0/1
Batch Loss = 3.396821065289483
967, epoch_train_loss=3.396821065289483
Epoch 968
Epoch 968 :: Batch 0/1
Batch Loss = 3.3913427589807745
968, epoch_train_loss=3.3913427589807745
Epoch 969
Epoch 969 :: Batch 0/1
Batch Loss = 3.392787425049588
969, epoch_train_loss=3.392787425049588
Epoch 970
Epoch 970 :: Batch 0/1
Batch Loss = 3.3752225665227247
970, epoch_train_loss=3.3752225665227247
Epoch 971
Epoch 971 :: Batch 0/1
Batch Loss = 3.3824636785217868
971, epoch_train_loss=3.3824636785217868
Epoch 972
Epoch 972 :: Batch 0/1
Batch Loss = 3.3703059749423856
972, epoch_train_loss=3.3703059749423856
Epoch 973
Epoch 973 :: Batch 0/1
Batch Loss = 3.3596804307801293
973, epoch_train_loss=3.3596804307801293
Epoch 974
Epoch 974 :: Batch 0/1
Batch Loss = 3.358699391898243
974, epoch_train_loss=3.358699391898243
Epoch 975
Epoch 975 :: Batch 0/1
Batch Loss = 3.346435524624123
975, epoch_train_loss=3.346435524624123
Epoch 976
Epoch 976 :: Batch 0/1
Batch Loss = 3.345376561565116
976, epoch_train_loss=3.345376561565116
Epoch 977
Epoch 977 :: Batch 0/1
Batch Loss = 3.333348944445203
977, epoch_train_loss=3.333348944445203
Epoch 978
Epoch 978 :: Batch 0/1
Batch Loss = 3.3274415723051862
978, epoch_train_loss=3.3274415723051862
Epoch 979
Epoch 979 :: Batch 0/1
Batch Loss = 3.3206339021326823
979, epoch_train_loss=3.3206339021326823
Epoch 980
Epoch 980 :: Batch 0/1
Batch Loss = 3.3089326005109383
980, epoch_train_loss=3.3089326005109383
Epoch 981
Epoch 981 :: Batch 0/1
Batch Loss = 3.306265356601297
981, epoch_train_loss=3.306265356601297
Epoch 982
Epoch 982 :: Batch 0/1
Batch Loss = 3.2941778905957366
982, epoch_train_loss=3.2941778905957366
Epoch 983
Epoch 983 :: Batch 0/1
Batch Loss = 3.2897519468879026
983, epoch_train_loss=3.2897519468879026
Epoch 984
Epoch 984 :: Batch 0/1
Batch Loss = 3.2783572375817354
984, epoch_train_loss=3.2783572375817354
Epoch 985
Epoch 985 :: Batch 0/1
Batch Loss = 3.274726303907039
985, epoch_train_loss=3.274726303907039
Epoch 986
Epoch 986 :: Batch 0/1
Batch Loss = 3.263902031558067
986, epoch_train_loss=3.263902031558067
Epoch 987
Epoch 987 :: Batch 0/1
Batch Loss = 3.2601225651070878
987, epoch_train_loss=3.2601225651070878
Epoch 988
Epoch 988 :: Batch 0/1
Batch Loss = 3.2513740655371666
988, epoch_train_loss=3.2513740655371666
Epoch 989
Epoch 989 :: Batch 0/1
Batch Loss = 3.245065517988256
989, epoch_train_loss=3.245065517988256
Epoch 990
Epoch 990 :: Batch 0/1
Batch Loss = 3.2409201035345414
990, epoch_train_loss=3.2409201035345414
Epoch 991
Epoch 991 :: Batch 0/1
Batch Loss = 3.233823911287612
991, epoch_train_loss=3.233823911287612
Epoch 992
Epoch 992 :: Batch 0/1
Batch Loss = 3.227874221452866
992, epoch_train_loss=3.227874221452866
Epoch 993
Epoch 993 :: Batch 0/1
Batch Loss = 3.2270710836505763
993, epoch_train_loss=3.2270710836505763
Epoch 994
Epoch 994 :: Batch 0/1
Batch Loss = 3.22126322262008
994, epoch_train_loss=3.22126322262008
Epoch 995
Epoch 995 :: Batch 0/1
Batch Loss = 3.215338652775539
995, epoch_train_loss=3.215338652775539
Epoch 996
Epoch 996 :: Batch 0/1
Batch Loss = 3.21316405903403
996, epoch_train_loss=3.21316405903403
Epoch 997
Epoch 997 :: Batch 0/1
Batch Loss = 3.212149647321792
997, epoch_train_loss=3.212149647321792
Epoch 998
Epoch 998 :: Batch 0/1
Batch Loss = 3.2129210660485668
998, epoch_train_loss=3.2129210660485668
Epoch 999
Epoch 999 :: Batch 0/1
Batch Loss = 3.218667784374083
999, epoch_train_loss=3.218667784374083
Epoch 1000
Epoch 1000 :: Batch 0/1
Batch Loss = 3.233224464852494
1000, epoch_train_loss=3.233224464852494
Epoch 1001
Epoch 1001 :: Batch 0/1
Batch Loss = 3.2609612175995046
1001, epoch_train_loss=3.2609612175995046
Epoch 1002
Epoch 1002 :: Batch 0/1
Batch Loss = 3.2779328342379355
1002, epoch_train_loss=3.2779328342379355
Epoch 1003
Epoch 1003 :: Batch 0/1
Batch Loss = 3.271201670075859
1003, epoch_train_loss=3.271201670075859
Epoch 1004
Epoch 1004 :: Batch 0/1
Batch Loss = 3.2191399566571977
1004, epoch_train_loss=3.2191399566571977
Epoch 1005
Epoch 1005 :: Batch 0/1
Batch Loss = 3.200316891154578
1005, epoch_train_loss=3.200316891154578
Epoch 1006
Epoch 1006 :: Batch 0/1
Batch Loss = 3.220673253058622
1006, epoch_train_loss=3.220673253058622
Epoch 1007
Epoch 1007 :: Batch 0/1
Batch Loss = 3.2469123342868227
1007, epoch_train_loss=3.2469123342868227
Epoch 1008
Epoch 1008 :: Batch 0/1
Batch Loss = 3.232357871942881
1008, epoch_train_loss=3.232357871942881
Epoch 1009
Epoch 1009 :: Batch 0/1
Batch Loss = 3.1993529099779505
1009, epoch_train_loss=3.1993529099779505
Epoch 1010
Epoch 1010 :: Batch 0/1
Batch Loss = 3.1983893628539986
1010, epoch_train_loss=3.1983893628539986
Epoch 1011
Epoch 1011 :: Batch 0/1
Batch Loss = 3.217556702336346
1011, epoch_train_loss=3.217556702336346
Epoch 1012
Epoch 1012 :: Batch 0/1
Batch Loss = 3.2140797321472174
1012, epoch_train_loss=3.2140797321472174
Epoch 1013
Epoch 1013 :: Batch 0/1
Batch Loss = 3.1933981401454927
1013, epoch_train_loss=3.1933981401454927
Epoch 1014
Epoch 1014 :: Batch 0/1
Batch Loss = 3.1949509801413463
1014, epoch_train_loss=3.1949509801413463
Epoch 1015
Epoch 1015 :: Batch 0/1
Batch Loss = 3.208069438486704
1015, epoch_train_loss=3.208069438486704
Epoch 1016
Epoch 1016 :: Batch 0/1
Batch Loss = 3.200726629784094
1016, epoch_train_loss=3.200726629784094
Epoch 1017
Epoch 1017 :: Batch 0/1
Batch Loss = 3.188595650363019
1017, epoch_train_loss=3.188595650363019
Epoch 1018
Epoch 1018 :: Batch 0/1
Batch Loss = 3.1935097603086646
1018, epoch_train_loss=3.1935097603086646
Epoch 1019
Epoch 1019 :: Batch 0/1
Batch Loss = 3.1997408015912563
1019, epoch_train_loss=3.1997408015912563
Epoch 1020
Epoch 1020 :: Batch 0/1
Batch Loss = 3.1913084041042024
1020, epoch_train_loss=3.1913084041042024
Epoch 1021
Epoch 1021 :: Batch 0/1
Batch Loss = 3.1856498673223244
1021, epoch_train_loss=3.1856498673223244
Epoch 1022
Epoch 1022 :: Batch 0/1
Batch Loss = 3.1909201494915065
1022, epoch_train_loss=3.1909201494915065
Epoch 1023
Epoch 1023 :: Batch 0/1
Batch Loss = 3.192620985051016
1023, epoch_train_loss=3.192620985051016
Epoch 1024
Epoch 1024 :: Batch 0/1
Batch Loss = 3.1860984863947444
1024, epoch_train_loss=3.1860984863947444
Epoch 1025
Epoch 1025 :: Batch 0/1
Batch Loss = 3.182504823566133
1025, epoch_train_loss=3.182504823566133
Epoch 1026
Epoch 1026 :: Batch 0/1
Batch Loss = 3.1861084748582704
1026, epoch_train_loss=3.1861084748582704
Epoch 1027
Epoch 1027 :: Batch 0/1
Batch Loss = 3.187746752094781
1027, epoch_train_loss=3.187746752094781
Epoch 1028
Epoch 1028 :: Batch 0/1
Batch Loss = 3.1832565241668016
1028, epoch_train_loss=3.1832565241668016
Epoch 1029
Epoch 1029 :: Batch 0/1
Batch Loss = 3.179463128820237
1029, epoch_train_loss=3.179463128820237
Epoch 1030
Epoch 1030 :: Batch 0/1
Batch Loss = 3.1805102376254446
1030, epoch_train_loss=3.1805102376254446
Epoch 1031
Epoch 1031 :: Batch 0/1
Batch Loss = 3.1827022836050425
1031, epoch_train_loss=3.1827022836050425
Epoch 1032
Epoch 1032 :: Batch 0/1
Batch Loss = 3.182376008382036
1032, epoch_train_loss=3.182376008382036
Epoch 1033
Epoch 1033 :: Batch 0/1
Batch Loss = 3.178960056954841
1033, epoch_train_loss=3.178960056954841
Epoch 1034
Epoch 1034 :: Batch 0/1
Batch Loss = 3.1763702556353577
1034, epoch_train_loss=3.1763702556353577
Epoch 1035
Epoch 1035 :: Batch 0/1
Batch Loss = 3.176163919803092
1035, epoch_train_loss=3.176163919803092
Epoch 1036
Epoch 1036 :: Batch 0/1
Batch Loss = 3.1772819761813835
1036, epoch_train_loss=3.1772819761813835
Epoch 1037
Epoch 1037 :: Batch 0/1
Batch Loss = 3.1783534081528297
1037, epoch_train_loss=3.1783534081528297
Epoch 1038
Epoch 1038 :: Batch 0/1
Batch Loss = 3.17771418816014
1038, epoch_train_loss=3.17771418816014
Epoch 1039
Epoch 1039 :: Batch 0/1
Batch Loss = 3.1764690167528533
1039, epoch_train_loss=3.1764690167528533
Epoch 1040
Epoch 1040 :: Batch 0/1
Batch Loss = 3.174356912566251
1040, epoch_train_loss=3.174356912566251
Epoch 1041
Epoch 1041 :: Batch 0/1
Batch Loss = 3.172668634910752
1041, epoch_train_loss=3.172668634910752
Epoch 1042
Epoch 1042 :: Batch 0/1
Batch Loss = 3.171419362031535
1042, epoch_train_loss=3.171419362031535
Epoch 1043
Epoch 1043 :: Batch 0/1
Batch Loss = 3.170618841107961
1043, epoch_train_loss=3.170618841107961
Epoch 1044
Epoch 1044 :: Batch 0/1
Batch Loss = 3.17019652864355
1044, epoch_train_loss=3.17019652864355
Epoch 1045
Epoch 1045 :: Batch 0/1
Batch Loss = 3.1700319512741593
1045, epoch_train_loss=3.1700319512741593
Epoch 1046
Epoch 1046 :: Batch 0/1
Batch Loss = 3.170248037240788
1046, epoch_train_loss=3.170248037240788
Epoch 1047
Epoch 1047 :: Batch 0/1
Batch Loss = 3.1707989982146447
1047, epoch_train_loss=3.1707989982146447
Epoch 1048
Epoch 1048 :: Batch 0/1
Batch Loss = 3.1727293988927805
1048, epoch_train_loss=3.1727293988927805
Epoch 1049
Epoch 1049 :: Batch 0/1
Batch Loss = 3.1752766105077614
1049, epoch_train_loss=3.1752766105077614
Epoch 1050
Epoch 1050 :: Batch 0/1
Batch Loss = 3.1829192764029655
1050, epoch_train_loss=3.1829192764029655
Epoch 1051
Epoch 1051 :: Batch 0/1
Batch Loss = 3.187726021957647
1051, epoch_train_loss=3.187726021957647
Epoch 1052
Epoch 1052 :: Batch 0/1
Batch Loss = 3.2049312064167412
1052, epoch_train_loss=3.2049312064167412
Epoch 1053
Epoch 1053 :: Batch 0/1
Batch Loss = 3.1957553869326984
1053, epoch_train_loss=3.1957553869326984
Epoch 1054
Epoch 1054 :: Batch 0/1
Batch Loss = 3.1947206465275313
1054, epoch_train_loss=3.1947206465275313
Epoch 1055
Epoch 1055 :: Batch 0/1
Batch Loss = 3.174301881817224
1055, epoch_train_loss=3.174301881817224
Epoch 1056
Epoch 1056 :: Batch 0/1
Batch Loss = 3.163930219202851
1056, epoch_train_loss=3.163930219202851
Epoch 1057
Epoch 1057 :: Batch 0/1
Batch Loss = 3.162116725181424
1057, epoch_train_loss=3.162116725181424
Epoch 1058
Epoch 1058 :: Batch 0/1
Batch Loss = 3.1674576615812673
1058, epoch_train_loss=3.1674576615812673
Epoch 1059
Epoch 1059 :: Batch 0/1
Batch Loss = 3.176188064717722
1059, epoch_train_loss=3.176188064717722
Epoch 1060
Epoch 1060 :: Batch 0/1
Batch Loss = 3.1734357155740778
1060, epoch_train_loss=3.1734357155740778
Epoch 1061
Epoch 1061 :: Batch 0/1
Batch Loss = 3.1689333803434896
1061, epoch_train_loss=3.1689333803434896
Epoch 1062
Epoch 1062 :: Batch 0/1
Batch Loss = 3.1605924982223983
1062, epoch_train_loss=3.1605924982223983
Epoch 1063
Epoch 1063 :: Batch 0/1
Batch Loss = 3.157507962647901
1063, epoch_train_loss=3.157507962647901
Epoch 1064
Epoch 1064 :: Batch 0/1
Batch Loss = 3.159487742358137
1064, epoch_train_loss=3.159487742358137
Epoch 1065
Epoch 1065 :: Batch 0/1
Batch Loss = 3.1626903827746413
1065, epoch_train_loss=3.1626903827746413
Epoch 1066
Epoch 1066 :: Batch 0/1
Batch Loss = 3.1656765604824164
1066, epoch_train_loss=3.1656765604824164
Epoch 1067
Epoch 1067 :: Batch 0/1
Batch Loss = 3.1621602395751682
1067, epoch_train_loss=3.1621602395751682
Epoch 1068
Epoch 1068 :: Batch 0/1
Batch Loss = 3.1582003989789484
1068, epoch_train_loss=3.1582003989789484
Epoch 1069
Epoch 1069 :: Batch 0/1
Batch Loss = 3.1542433976074924
1069, epoch_train_loss=3.1542433976074924
Epoch 1070
Epoch 1070 :: Batch 0/1
Batch Loss = 3.1529273689383777
1070, epoch_train_loss=3.1529273689383777
Epoch 1071
Epoch 1071 :: Batch 0/1
Batch Loss = 3.153880073635242
1071, epoch_train_loss=3.153880073635242
Epoch 1072
Epoch 1072 :: Batch 0/1
Batch Loss = 3.155246266394806
1072, epoch_train_loss=3.155246266394806
Epoch 1073
Epoch 1073 :: Batch 0/1
Batch Loss = 3.1566559888177763
1073, epoch_train_loss=3.1566559888177763
Epoch 1074
Epoch 1074 :: Batch 0/1
Batch Loss = 3.1554006934454013
1074, epoch_train_loss=3.1554006934454013
Epoch 1075
Epoch 1075 :: Batch 0/1
Batch Loss = 3.153820939007699
1075, epoch_train_loss=3.153820939007699
Epoch 1076
Epoch 1076 :: Batch 0/1
Batch Loss = 3.151011482628687
1076, epoch_train_loss=3.151011482628687
Epoch 1077
Epoch 1077 :: Batch 0/1
Batch Loss = 3.149054691642549
1077, epoch_train_loss=3.149054691642549
Epoch 1078
Epoch 1078 :: Batch 0/1
Batch Loss = 3.1479843161244525
1078, epoch_train_loss=3.1479843161244525
Epoch 1079
Epoch 1079 :: Batch 0/1
Batch Loss = 3.1477751739333195
1079, epoch_train_loss=3.1477751739333195
Epoch 1080
Epoch 1080 :: Batch 0/1
Batch Loss = 3.148145789255417
1080, epoch_train_loss=3.148145789255417
Epoch 1081
Epoch 1081 :: Batch 0/1
Batch Loss = 3.1484803240662873
1081, epoch_train_loss=3.1484803240662873
Epoch 1082
Epoch 1082 :: Batch 0/1
Batch Loss = 3.14881609045762
1082, epoch_train_loss=3.14881609045762
Epoch 1083
Epoch 1083 :: Batch 0/1
Batch Loss = 3.1481829450880903
1083, epoch_train_loss=3.1481829450880903
Epoch 1084
Epoch 1084 :: Batch 0/1
Batch Loss = 3.147511602638153
1084, epoch_train_loss=3.147511602638153
Epoch 1085
Epoch 1085 :: Batch 0/1
Batch Loss = 3.1461704271585456
1085, epoch_train_loss=3.1461704271585456
Epoch 1086
Epoch 1086 :: Batch 0/1
Batch Loss = 3.144978173680293
1086, epoch_train_loss=3.144978173680293
Epoch 1087
Epoch 1087 :: Batch 0/1
Batch Loss = 3.1438525129418586
1087, epoch_train_loss=3.1438525129418586
Epoch 1088
Epoch 1088 :: Batch 0/1
Batch Loss = 3.1430921693839644
1088, epoch_train_loss=3.1430921693839644
Epoch 1089
Epoch 1089 :: Batch 0/1
Batch Loss = 3.142678279971744
1089, epoch_train_loss=3.142678279971744
Epoch 1090
Epoch 1090 :: Batch 0/1
Batch Loss = 3.142521611833225
1090, epoch_train_loss=3.142521611833225
Epoch 1091
Epoch 1091 :: Batch 0/1
Batch Loss = 3.1424664924473538
1091, epoch_train_loss=3.1424664924473538
Epoch 1092
Epoch 1092 :: Batch 0/1
Batch Loss = 3.1422982831599726
1092, epoch_train_loss=3.1422982831599726
Epoch 1093
Epoch 1093 :: Batch 0/1
Batch Loss = 3.142026204707607
1093, epoch_train_loss=3.142026204707607
Epoch 1094
Epoch 1094 :: Batch 0/1
Batch Loss = 3.14149716448132
1094, epoch_train_loss=3.14149716448132
Epoch 1095
Epoch 1095 :: Batch 0/1
Batch Loss = 3.1409126117330812
1095, epoch_train_loss=3.1409126117330812
Epoch 1096
Epoch 1096 :: Batch 0/1
Batch Loss = 3.140299930035326
1096, epoch_train_loss=3.140299930035326
Epoch 1097
Epoch 1097 :: Batch 0/1
Batch Loss = 3.1397990848565636
1097, epoch_train_loss=3.1397990848565636
Epoch 1098
Epoch 1098 :: Batch 0/1
Batch Loss = 3.1394336370333815
1098, epoch_train_loss=3.1394336370333815
Epoch 1099
Epoch 1099 :: Batch 0/1
Batch Loss = 3.1391918568255233
1099, epoch_train_loss=3.1391918568255233
Epoch 1100
Epoch 1100 :: Batch 0/1
Batch Loss = 3.1390226070268064
1100, epoch_train_loss=3.1390226070268064
Epoch 1101
Epoch 1101 :: Batch 0/1
Batch Loss = 3.138855381020323
1101, epoch_train_loss=3.138855381020323
Epoch 1102
Epoch 1102 :: Batch 0/1
Batch Loss = 3.138655000181683
1102, epoch_train_loss=3.138655000181683
Epoch 1103
Epoch 1103 :: Batch 0/1
Batch Loss = 3.138375269896585
1103, epoch_train_loss=3.138375269896585
Epoch 1104
Epoch 1104 :: Batch 0/1
Batch Loss = 3.138061818020721
1104, epoch_train_loss=3.138061818020721
Epoch 1105
Epoch 1105 :: Batch 0/1
Batch Loss = 3.137708218735333
1105, epoch_train_loss=3.137708218735333
Epoch 1106
Epoch 1106 :: Batch 0/1
Batch Loss = 3.1373637731554047
1106, epoch_train_loss=3.1373637731554047
Epoch 1107
Epoch 1107 :: Batch 0/1
Batch Loss = 3.1370430035632135
1107, epoch_train_loss=3.1370430035632135
Epoch 1108
Epoch 1108 :: Batch 0/1
Batch Loss = 3.1367587428790644
1108, epoch_train_loss=3.1367587428790644
Epoch 1109
Epoch 1109 :: Batch 0/1
Batch Loss = 3.136509215868794
1109, epoch_train_loss=3.136509215868794
Epoch 1110
Epoch 1110 :: Batch 0/1
Batch Loss = 3.1362846122454915
1110, epoch_train_loss=3.1362846122454915
Epoch 1111
Epoch 1111 :: Batch 0/1
Batch Loss = 3.1360735209898647
1111, epoch_train_loss=3.1360735209898647
Epoch 1112
Epoch 1112 :: Batch 0/1
Batch Loss = 3.135860746922077
1112, epoch_train_loss=3.135860746922077
Epoch 1113
Epoch 1113 :: Batch 0/1
Batch Loss = 3.1356410128508823
1113, epoch_train_loss=3.1356410128508823
Epoch 1114
Epoch 1114 :: Batch 0/1
Batch Loss = 3.135400223739216
1114, epoch_train_loss=3.135400223739216
Epoch 1115
Epoch 1115 :: Batch 0/1
Batch Loss = 3.1351453743855267
1115, epoch_train_loss=3.1351453743855267
Epoch 1116
Epoch 1116 :: Batch 0/1
Batch Loss = 3.1348632095804567
1116, epoch_train_loss=3.1348632095804567
Epoch 1117
Epoch 1117 :: Batch 0/1
Batch Loss = 3.134566266096383
1117, epoch_train_loss=3.134566266096383
Epoch 1118
Epoch 1118 :: Batch 0/1
Batch Loss = 3.134241561464544
1118, epoch_train_loss=3.134241561464544
Epoch 1119
Epoch 1119 :: Batch 0/1
Batch Loss = 3.133901652974253
1119, epoch_train_loss=3.133901652974253
Epoch 1120
Epoch 1120 :: Batch 0/1
Batch Loss = 3.133535537806499
1120, epoch_train_loss=3.133535537806499
Epoch 1121
Epoch 1121 :: Batch 0/1
Batch Loss = 3.133151432933733
1121, epoch_train_loss=3.133151432933733
Epoch 1122
Epoch 1122 :: Batch 0/1
Batch Loss = 3.132740830745058
1122, epoch_train_loss=3.132740830745058
Epoch 1123
Epoch 1123 :: Batch 0/1
Batch Loss = 3.132309063518459
1123, epoch_train_loss=3.132309063518459
Epoch 1124
Epoch 1124 :: Batch 0/1
Batch Loss = 3.1318502210150867
1124, epoch_train_loss=3.1318502210150867
Epoch 1125
Epoch 1125 :: Batch 0/1
Batch Loss = 3.1313671888230137
1125, epoch_train_loss=3.1313671888230137
Epoch 1126
Epoch 1126 :: Batch 0/1
Batch Loss = 3.1308569761348917
1126, epoch_train_loss=3.1308569761348917
Epoch 1127
Epoch 1127 :: Batch 0/1
Batch Loss = 3.1303224289906857
1127, epoch_train_loss=3.1303224289906857
Epoch 1128
Epoch 1128 :: Batch 0/1
Batch Loss = 3.1297627301755724
1128, epoch_train_loss=3.1297627301755724
Epoch 1129
Epoch 1129 :: Batch 0/1
Batch Loss = 3.1291817064059497
1129, epoch_train_loss=3.1291817064059497
Epoch 1130
Epoch 1130 :: Batch 0/1
Batch Loss = 3.128580937648447
1130, epoch_train_loss=3.128580937648447
Epoch 1131
Epoch 1131 :: Batch 0/1
Batch Loss = 3.127966052304373
1131, epoch_train_loss=3.127966052304373
Epoch 1132
Epoch 1132 :: Batch 0/1
Batch Loss = 3.1273409322367125
1132, epoch_train_loss=3.1273409322367125
Epoch 1133
Epoch 1133 :: Batch 0/1
Batch Loss = 3.1267165234094003
1133, epoch_train_loss=3.1267165234094003
Epoch 1134
Epoch 1134 :: Batch 0/1
Batch Loss = 3.126101594397117
1134, epoch_train_loss=3.126101594397117
Epoch 1135
Epoch 1135 :: Batch 0/1
Batch Loss = 3.1255279586851787
1135, epoch_train_loss=3.1255279586851787
Epoch 1136
Epoch 1136 :: Batch 0/1
Batch Loss = 3.1250205624635945
1136, epoch_train_loss=3.1250205624635945
Epoch 1137
Epoch 1137 :: Batch 0/1
Batch Loss = 3.124709175129435
1137, epoch_train_loss=3.124709175129435
Epoch 1138
Epoch 1138 :: Batch 0/1
Batch Loss = 3.1246714109914735
1138, epoch_train_loss=3.1246714109914735
Epoch 1139
Epoch 1139 :: Batch 0/1
Batch Loss = 3.1255633649362697
1139, epoch_train_loss=3.1255633649362697
Epoch 1140
Epoch 1140 :: Batch 0/1
Batch Loss = 3.1274146824231805
1140, epoch_train_loss=3.1274146824231805
Epoch 1141
Epoch 1141 :: Batch 0/1
Batch Loss = 3.13382502019301
1141, epoch_train_loss=3.13382502019301
Epoch 1142
Epoch 1142 :: Batch 0/1
Batch Loss = 3.1406832381834167
1142, epoch_train_loss=3.1406832381834167
Epoch 1143
Epoch 1143 :: Batch 0/1
Batch Loss = 3.1637810288162482
1143, epoch_train_loss=3.1637810288162482
Epoch 1144
Epoch 1144 :: Batch 0/1
Batch Loss = 3.1577205405284188
1144, epoch_train_loss=3.1577205405284188
Epoch 1145
Epoch 1145 :: Batch 0/1
Batch Loss = 3.162191028898289
1145, epoch_train_loss=3.162191028898289
Epoch 1146
Epoch 1146 :: Batch 0/1
Batch Loss = 3.13081737944441
1146, epoch_train_loss=3.13081737944441
Epoch 1147
Epoch 1147 :: Batch 0/1
Batch Loss = 3.1181314348819273
1147, epoch_train_loss=3.1181314348819273
Epoch 1148
Epoch 1148 :: Batch 0/1
Batch Loss = 3.1238669838181177
1148, epoch_train_loss=3.1238669838181177
Epoch 1149
Epoch 1149 :: Batch 0/1
Batch Loss = 3.1345619602775283
1149, epoch_train_loss=3.1345619602775283
Epoch 1150
Epoch 1150 :: Batch 0/1
Batch Loss = 3.1436739093062798
1150, epoch_train_loss=3.1436739093062798
Epoch 1151
Epoch 1151 :: Batch 0/1
Batch Loss = 3.1237039770720774
1151, epoch_train_loss=3.1237039770720774
Epoch 1152
Epoch 1152 :: Batch 0/1
Batch Loss = 3.1152757426686986
1152, epoch_train_loss=3.1152757426686986
Epoch 1153
Epoch 1153 :: Batch 0/1
Batch Loss = 3.122846480253213
1153, epoch_train_loss=3.122846480253213
Epoch 1154
Epoch 1154 :: Batch 0/1
Batch Loss = 3.1292449527775803
1154, epoch_train_loss=3.1292449527775803
Epoch 1155
Epoch 1155 :: Batch 0/1
Batch Loss = 3.1305137155761735
1155, epoch_train_loss=3.1305137155761735
Epoch 1156
Epoch 1156 :: Batch 0/1
Batch Loss = 3.1149293511222123
1156, epoch_train_loss=3.1149293511222123
Epoch 1157
Epoch 1157 :: Batch 0/1
Batch Loss = 3.113911220493741
1157, epoch_train_loss=3.113911220493741
Epoch 1158
Epoch 1158 :: Batch 0/1
Batch Loss = 3.1234499965643154
1158, epoch_train_loss=3.1234499965643154
Epoch 1159
Epoch 1159 :: Batch 0/1
Batch Loss = 3.1245782497690233
1159, epoch_train_loss=3.1245782497690233
Epoch 1160
Epoch 1160 :: Batch 0/1
Batch Loss = 3.1187090533876365
1160, epoch_train_loss=3.1187090533876365
Epoch 1161
Epoch 1161 :: Batch 0/1
Batch Loss = 3.1108871738074493
1161, epoch_train_loss=3.1108871738074493
Epoch 1162
Epoch 1162 :: Batch 0/1
Batch Loss = 3.111324593476948
1162, epoch_train_loss=3.111324593476948
Epoch 1163
Epoch 1163 :: Batch 0/1
Batch Loss = 3.118200463650476
1163, epoch_train_loss=3.118200463650476
Epoch 1164
Epoch 1164 :: Batch 0/1
Batch Loss = 3.116564838691679
1164, epoch_train_loss=3.116564838691679
Epoch 1165
Epoch 1165 :: Batch 0/1
Batch Loss = 3.11361563711964
1165, epoch_train_loss=3.11361563711964
Epoch 1166
Epoch 1166 :: Batch 0/1
Batch Loss = 3.108570272053572
1166, epoch_train_loss=3.108570272053572
Epoch 1167
Epoch 1167 :: Batch 0/1
Batch Loss = 3.1071185332089986
1167, epoch_train_loss=3.1071185332089986
Epoch 1168
Epoch 1168 :: Batch 0/1
Batch Loss = 3.1082224999721655
1168, epoch_train_loss=3.1082224999721655
Epoch 1169
Epoch 1169 :: Batch 0/1
Batch Loss = 3.1099710072958153
1169, epoch_train_loss=3.1099710072958153
Epoch 1170
Epoch 1170 :: Batch 0/1
Batch Loss = 3.1120229616087656
1170, epoch_train_loss=3.1120229616087656
Epoch 1171
Epoch 1171 :: Batch 0/1
Batch Loss = 3.110043608008415
1171, epoch_train_loss=3.110043608008415
Epoch 1172
Epoch 1172 :: Batch 0/1
Batch Loss = 3.1080680465093176
1172, epoch_train_loss=3.1080680465093176
Epoch 1173
Epoch 1173 :: Batch 0/1
Batch Loss = 3.1056597632201353
1173, epoch_train_loss=3.1056597632201353
Epoch 1174
Epoch 1174 :: Batch 0/1
Batch Loss = 3.1041652176343275
1174, epoch_train_loss=3.1041652176343275
Epoch 1175
Epoch 1175 :: Batch 0/1
Batch Loss = 3.1032506161182254
1175, epoch_train_loss=3.1032506161182254
Epoch 1176
Epoch 1176 :: Batch 0/1
Batch Loss = 3.102452706664586
1176, epoch_train_loss=3.102452706664586
Epoch 1177
Epoch 1177 :: Batch 0/1
Batch Loss = 3.1019524174755295
1177, epoch_train_loss=3.1019524174755295
Epoch 1178
Epoch 1178 :: Batch 0/1
Batch Loss = 3.1014115676284493
1178, epoch_train_loss=3.1014115676284493
Epoch 1179
Epoch 1179 :: Batch 0/1
Batch Loss = 3.1009679418392673
1179, epoch_train_loss=3.1009679418392673
Epoch 1180
Epoch 1180 :: Batch 0/1
Batch Loss = 3.10076447152772
1180, epoch_train_loss=3.10076447152772
Epoch 1181
Epoch 1181 :: Batch 0/1
Batch Loss = 3.101779982972959
1181, epoch_train_loss=3.101779982972959
Epoch 1182
Epoch 1182 :: Batch 0/1
Batch Loss = 3.1092741652999787
1182, epoch_train_loss=3.1092741652999787
Epoch 1183
Epoch 1183 :: Batch 0/1
Batch Loss = 3.1595645276401356
1183, epoch_train_loss=3.1595645276401356
Epoch 1184
Epoch 1184 :: Batch 0/1
Batch Loss = 3.1739632523945644
1184, epoch_train_loss=3.1739632523945644
Epoch 1185
Epoch 1185 :: Batch 0/1
Batch Loss = 3.3059588071792976
1185, epoch_train_loss=3.3059588071792976
Epoch 1186
Epoch 1186 :: Batch 0/1
Batch Loss = 3.206284738006663
1186, epoch_train_loss=3.206284738006663
Epoch 1187
Epoch 1187 :: Batch 0/1
Batch Loss = 3.2263427288433246
1187, epoch_train_loss=3.2263427288433246
Epoch 1188
Epoch 1188 :: Batch 0/1
Batch Loss = 3.1803716452663253
1188, epoch_train_loss=3.1803716452663253
Epoch 1189
Epoch 1189 :: Batch 0/1
Batch Loss = 3.167235719736527
1189, epoch_train_loss=3.167235719736527
Epoch 1190
Epoch 1190 :: Batch 0/1
Batch Loss = 3.1825678531321056
1190, epoch_train_loss=3.1825678531321056
Epoch 1191
Epoch 1191 :: Batch 0/1
Batch Loss = 3.1388781278899835
1191, epoch_train_loss=3.1388781278899835
Epoch 1192
Epoch 1192 :: Batch 0/1
Batch Loss = 3.165835192154303
1192, epoch_train_loss=3.165835192154303
Epoch 1193
Epoch 1193 :: Batch 0/1
Batch Loss = 3.1270991492051263
1193, epoch_train_loss=3.1270991492051263
Epoch 1194
Epoch 1194 :: Batch 0/1
Batch Loss = 3.1355094746245946
1194, epoch_train_loss=3.1355094746245946
Epoch 1195
Epoch 1195 :: Batch 0/1
Batch Loss = 3.146403155579898
1195, epoch_train_loss=3.146403155579898
Epoch 1196
Epoch 1196 :: Batch 0/1
Batch Loss = 3.136773723847204
1196, epoch_train_loss=3.136773723847204
Epoch 1197
Epoch 1197 :: Batch 0/1
Batch Loss = 3.1368327725885514
1197, epoch_train_loss=3.1368327725885514
Epoch 1198
Epoch 1198 :: Batch 0/1
Batch Loss = 3.1138688991758223
1198, epoch_train_loss=3.1138688991758223
Epoch 1199
Epoch 1199 :: Batch 0/1
Batch Loss = 3.139426667478912
1199, epoch_train_loss=3.139426667478912
Epoch 1200
Epoch 1200 :: Batch 0/1
Batch Loss = 3.1098474311957807
1200, epoch_train_loss=3.1098474311957807
Epoch 1201
Epoch 1201 :: Batch 0/1
Batch Loss = 3.1334778854430394
1201, epoch_train_loss=3.1334778854430394
Epoch 1202
Epoch 1202 :: Batch 0/1
Batch Loss = 3.1194441981338983
1202, epoch_train_loss=3.1194441981338983
Epoch 1203
Epoch 1203 :: Batch 0/1
Batch Loss = 3.116925652437877
1203, epoch_train_loss=3.116925652437877
Epoch 1204
Epoch 1204 :: Batch 0/1
Batch Loss = 3.1289115105907523
1204, epoch_train_loss=3.1289115105907523
Epoch 1205
Epoch 1205 :: Batch 0/1
Batch Loss = 3.1076511644736735
1205, epoch_train_loss=3.1076511644736735
Epoch 1206
Epoch 1206 :: Batch 0/1
Batch Loss = 3.120365536627168
1206, epoch_train_loss=3.120365536627168
Epoch 1207
Epoch 1207 :: Batch 0/1
Batch Loss = 3.107135065567896
1207, epoch_train_loss=3.107135065567896
Epoch 1208
Epoch 1208 :: Batch 0/1
Batch Loss = 3.113521668337286
1208, epoch_train_loss=3.113521668337286
Epoch 1209
Epoch 1209 :: Batch 0/1
Batch Loss = 3.1168261449963928
1209, epoch_train_loss=3.1168261449963928
Epoch 1210
Epoch 1210 :: Batch 0/1
Batch Loss = 3.103506942751995
1210, epoch_train_loss=3.103506942751995
Epoch 1211
Epoch 1211 :: Batch 0/1
Batch Loss = 3.1166021085774043
1211, epoch_train_loss=3.1166021085774043
Epoch 1212
Epoch 1212 :: Batch 0/1
Batch Loss = 3.1091559990770774
1212, epoch_train_loss=3.1091559990770774
Epoch 1213
Epoch 1213 :: Batch 0/1
Batch Loss = 3.103613741594107
1213, epoch_train_loss=3.103613741594107
Epoch 1214
Epoch 1214 :: Batch 0/1
Batch Loss = 3.118821382730923
1214, epoch_train_loss=3.118821382730923
Epoch 1215
Epoch 1215 :: Batch 0/1
Batch Loss = 3.111707487872558
1215, epoch_train_loss=3.111707487872558
Epoch 1216
Epoch 1216 :: Batch 0/1
Batch Loss = 3.1022442262141285
1216, epoch_train_loss=3.1022442262141285
Epoch 1217
Epoch 1217 :: Batch 0/1
Batch Loss = 3.122527143877946
1217, epoch_train_loss=3.122527143877946
Epoch 1218
Epoch 1218 :: Batch 0/1
Batch Loss = 3.128526934620698
1218, epoch_train_loss=3.128526934620698
Epoch 1219
Epoch 1219 :: Batch 0/1
Batch Loss = 3.097592380817833
1219, epoch_train_loss=3.097592380817833
Epoch 1220
Epoch 1220 :: Batch 0/1
Batch Loss = 3.1586796764439025
1220, epoch_train_loss=3.1586796764439025
Epoch 1221
Epoch 1221 :: Batch 0/1
Batch Loss = 3.2258125592409153
1221, epoch_train_loss=3.2258125592409153
Epoch 1222
Epoch 1222 :: Batch 0/1
Batch Loss = 3.206409219222261
1222, epoch_train_loss=3.206409219222261
Epoch 1223
Epoch 1223 :: Batch 0/1
Batch Loss = 3.1514278179339743
1223, epoch_train_loss=3.1514278179339743
Epoch 1224
Epoch 1224 :: Batch 0/1
Batch Loss = 3.2134510328671197
1224, epoch_train_loss=3.2134510328671197
Epoch 1225
Epoch 1225 :: Batch 0/1
Batch Loss = 3.2532167430244745
1225, epoch_train_loss=3.2532167430244745
Epoch 1226
Epoch 1226 :: Batch 0/1
Batch Loss = 3.224417617512074
1226, epoch_train_loss=3.224417617512074
Epoch 1227
Epoch 1227 :: Batch 0/1
Batch Loss = 3.1823772218963664
1227, epoch_train_loss=3.1823772218963664
Epoch 1228
Epoch 1228 :: Batch 0/1
Batch Loss = 3.229870050926792
1228, epoch_train_loss=3.229870050926792
Epoch 1229
Epoch 1229 :: Batch 0/1
Batch Loss = 3.12898424729485
1229, epoch_train_loss=3.12898424729485
Epoch 1230
Epoch 1230 :: Batch 0/1
Batch Loss = 3.221963778005218
1230, epoch_train_loss=3.221963778005218
Epoch 1231
Epoch 1231 :: Batch 0/1
Batch Loss = 3.1341886256746774
1231, epoch_train_loss=3.1341886256746774
Epoch 1232
Epoch 1232 :: Batch 0/1
Batch Loss = 3.1754221676545167
1232, epoch_train_loss=3.1754221676545167
Epoch 1233
Epoch 1233 :: Batch 0/1
Batch Loss = 3.1472613674824563
1233, epoch_train_loss=3.1472613674824563
Epoch 1234
Epoch 1234 :: Batch 0/1
Batch Loss = 3.1625283405794407
1234, epoch_train_loss=3.1625283405794407
Epoch 1235
Epoch 1235 :: Batch 0/1
Batch Loss = 3.133225847002716
1235, epoch_train_loss=3.133225847002716
Epoch 1236
Epoch 1236 :: Batch 0/1
Batch Loss = 3.1480285390236036
1236, epoch_train_loss=3.1480285390236036
Epoch 1237
Epoch 1237 :: Batch 0/1
Batch Loss = 3.149371774719465
1237, epoch_train_loss=3.149371774719465
Epoch 1238
Epoch 1238 :: Batch 0/1
Batch Loss = 3.120707778050776
1238, epoch_train_loss=3.120707778050776
Epoch 1239
Epoch 1239 :: Batch 0/1
Batch Loss = 3.145290057690471
1239, epoch_train_loss=3.145290057690471
Epoch 1240
Epoch 1240 :: Batch 0/1
Batch Loss = 3.123184108903351
1240, epoch_train_loss=3.123184108903351
Epoch 1241
Epoch 1241 :: Batch 0/1
Batch Loss = 3.130172942981581
1241, epoch_train_loss=3.130172942981581
Epoch 1242
Epoch 1242 :: Batch 0/1
Batch Loss = 3.1186587621121196
1242, epoch_train_loss=3.1186587621121196
Epoch 1243
Epoch 1243 :: Batch 0/1
Batch Loss = 3.131323986778815
1243, epoch_train_loss=3.131323986778815
Epoch 1244
Epoch 1244 :: Batch 0/1
Batch Loss = 3.120137986246822
1244, epoch_train_loss=3.120137986246822
Epoch 1245
Epoch 1245 :: Batch 0/1
Batch Loss = 3.1163436766992687
1245, epoch_train_loss=3.1163436766992687
Epoch 1246
Epoch 1246 :: Batch 0/1
Batch Loss = 3.12438732968506
1246, epoch_train_loss=3.12438732968506
Epoch 1247
Epoch 1247 :: Batch 0/1
Batch Loss = 3.1106925274048396
1247, epoch_train_loss=3.1106925274048396
Epoch 1248
Epoch 1248 :: Batch 0/1
Batch Loss = 3.11666508864025
1248, epoch_train_loss=3.11666508864025
Epoch 1249
Epoch 1249 :: Batch 0/1
Batch Loss = 3.1104018286092683
1249, epoch_train_loss=3.1104018286092683
Epoch 1250
Epoch 1250 :: Batch 0/1
Batch Loss = 3.1147353019582162
1250, epoch_train_loss=3.1147353019582162
Epoch 1251
Epoch 1251 :: Batch 0/1
Batch Loss = 3.1053887899982247
1251, epoch_train_loss=3.1053887899982247
Epoch 1252
Epoch 1252 :: Batch 0/1
Batch Loss = 3.113278265456126
1252, epoch_train_loss=3.113278265456126
Epoch 1253
Epoch 1253 :: Batch 0/1
Batch Loss = 3.1031884890359476
1253, epoch_train_loss=3.1031884890359476
Epoch 1254
Epoch 1254 :: Batch 0/1
Batch Loss = 3.108394997835762
1254, epoch_train_loss=3.108394997835762
Epoch 1255
Epoch 1255 :: Batch 0/1
Batch Loss = 3.105653101739237
1255, epoch_train_loss=3.105653101739237
Epoch 1256
Epoch 1256 :: Batch 0/1
Batch Loss = 3.1040381754372643
1256, epoch_train_loss=3.1040381754372643
Epoch 1257
Epoch 1257 :: Batch 0/1
Batch Loss = 3.1040586907138956
1257, epoch_train_loss=3.1040586907138956
Epoch 1258
Epoch 1258 :: Batch 0/1
Batch Loss = 3.0991695731880853
1258, epoch_train_loss=3.0991695731880853
Epoch 1259
Epoch 1259 :: Batch 0/1
Batch Loss = 3.101988216350737
1259, epoch_train_loss=3.101988216350737
Epoch 1260
Epoch 1260 :: Batch 0/1
Batch Loss = 3.095809251657454
1260, epoch_train_loss=3.095809251657454
Epoch 1261
Epoch 1261 :: Batch 0/1
Batch Loss = 3.099258905162042
1261, epoch_train_loss=3.099258905162042
Epoch 1262
Epoch 1262 :: Batch 0/1
Batch Loss = 3.1004432539243005
1262, epoch_train_loss=3.1004432539243005
Epoch 1263
Epoch 1263 :: Batch 0/1
Batch Loss = 3.0935246806842076
1263, epoch_train_loss=3.0935246806842076
Epoch 1264
Epoch 1264 :: Batch 0/1
Batch Loss = 3.0993638210594967
1264, epoch_train_loss=3.0993638210594967
Epoch 1265
Epoch 1265 :: Batch 0/1
Batch Loss = 3.1006358956721676
1265, epoch_train_loss=3.1006358956721676
Epoch 1266
Epoch 1266 :: Batch 0/1
Batch Loss = 3.091222604657889
1266, epoch_train_loss=3.091222604657889
Epoch 1267
Epoch 1267 :: Batch 0/1
Batch Loss = 3.100049528697141
1267, epoch_train_loss=3.100049528697141
Epoch 1268
Epoch 1268 :: Batch 0/1
Batch Loss = 3.115472120318894
1268, epoch_train_loss=3.115472120318894
Epoch 1269
Epoch 1269 :: Batch 0/1
Batch Loss = 3.0915512026592737
1269, epoch_train_loss=3.0915512026592737
Epoch 1270
Epoch 1270 :: Batch 0/1
Batch Loss = 3.0983575596691426
1270, epoch_train_loss=3.0983575596691426
Epoch 1271
Epoch 1271 :: Batch 0/1
Batch Loss = 3.1248275910276626
1271, epoch_train_loss=3.1248275910276626
Epoch 1272
Epoch 1272 :: Batch 0/1
Batch Loss = 3.0900603400774793
1272, epoch_train_loss=3.0900603400774793
Epoch 1273
Epoch 1273 :: Batch 0/1
Batch Loss = 3.100415239878503
1273, epoch_train_loss=3.100415239878503
Epoch 1274
Epoch 1274 :: Batch 0/1
Batch Loss = 3.1488882899291286
1274, epoch_train_loss=3.1488882899291286
Epoch 1275
Epoch 1275 :: Batch 0/1
Batch Loss = 3.0886323268711324
1275, epoch_train_loss=3.0886323268711324
Epoch 1276
Epoch 1276 :: Batch 0/1
Batch Loss = 3.178289462561752
1276, epoch_train_loss=3.178289462561752
Epoch 1277
Epoch 1277 :: Batch 0/1
Batch Loss = 3.2980944182132323
1277, epoch_train_loss=3.2980944182132323
Epoch 1278
Epoch 1278 :: Batch 0/1
Batch Loss = 3.3424290389378783
1278, epoch_train_loss=3.3424290389378783
Epoch 1279
Epoch 1279 :: Batch 0/1
Batch Loss = 3.27118544056946
1279, epoch_train_loss=3.27118544056946
Epoch 1280
Epoch 1280 :: Batch 0/1
Batch Loss = 3.2380476631459105
1280, epoch_train_loss=3.2380476631459105
Epoch 1281
Epoch 1281 :: Batch 0/1
Batch Loss = 3.1825364280432926
1281, epoch_train_loss=3.1825364280432926
Epoch 1282
Epoch 1282 :: Batch 0/1
Batch Loss = 3.2301107688282724
1282, epoch_train_loss=3.2301107688282724
Epoch 1283
Epoch 1283 :: Batch 0/1
Batch Loss = 3.2467527885110226
1283, epoch_train_loss=3.2467527885110226
Epoch 1284
Epoch 1284 :: Batch 0/1
Batch Loss = 3.17240908861051
1284, epoch_train_loss=3.17240908861051
Epoch 1285
Epoch 1285 :: Batch 0/1
Batch Loss = 3.1502649005854337
1285, epoch_train_loss=3.1502649005854337
Epoch 1286
Epoch 1286 :: Batch 0/1
Batch Loss = 3.222880761768881
1286, epoch_train_loss=3.222880761768881
Epoch 1287
Epoch 1287 :: Batch 0/1
Batch Loss = 3.137052143778913
1287, epoch_train_loss=3.137052143778913
Epoch 1288
Epoch 1288 :: Batch 0/1
Batch Loss = 3.1353219593521886
1288, epoch_train_loss=3.1353219593521886
Epoch 1289
Epoch 1289 :: Batch 0/1
Batch Loss = 3.15129100299123
1289, epoch_train_loss=3.15129100299123
Epoch 1290
Epoch 1290 :: Batch 0/1
Batch Loss = 3.1671263442110917
1290, epoch_train_loss=3.1671263442110917
Epoch 1291
Epoch 1291 :: Batch 0/1
Batch Loss = 3.1264510881088228
1291, epoch_train_loss=3.1264510881088228
Epoch 1292
Epoch 1292 :: Batch 0/1
Batch Loss = 3.1402154029722973
1292, epoch_train_loss=3.1402154029722973
Epoch 1293
Epoch 1293 :: Batch 0/1
Batch Loss = 3.150790678650439
1293, epoch_train_loss=3.150790678650439
Epoch 1294
Epoch 1294 :: Batch 0/1
Batch Loss = 3.144412556079731
1294, epoch_train_loss=3.144412556079731
Epoch 1295
Epoch 1295 :: Batch 0/1
Batch Loss = 3.1133103910970474
1295, epoch_train_loss=3.1133103910970474
Epoch 1296
Epoch 1296 :: Batch 0/1
Batch Loss = 3.1313055697763423
1296, epoch_train_loss=3.1313055697763423
Epoch 1297
Epoch 1297 :: Batch 0/1
Batch Loss = 3.1403211728880454
1297, epoch_train_loss=3.1403211728880454
Epoch 1298
Epoch 1298 :: Batch 0/1
Batch Loss = 3.1091798123370915
1298, epoch_train_loss=3.1091798123370915
Epoch 1299
Epoch 1299 :: Batch 0/1
Batch Loss = 3.1081946788686454
1299, epoch_train_loss=3.1081946788686454
Epoch 1300
Epoch 1300 :: Batch 0/1
Batch Loss = 3.112895114246473
1300, epoch_train_loss=3.112895114246473
Epoch 1301
Epoch 1301 :: Batch 0/1
Batch Loss = 3.117887787239444
1301, epoch_train_loss=3.117887787239444
Epoch 1302
Epoch 1302 :: Batch 0/1
Batch Loss = 3.1009268114882227
1302, epoch_train_loss=3.1009268114882227
Epoch 1303
Epoch 1303 :: Batch 0/1
Batch Loss = 3.1026890612823697
1303, epoch_train_loss=3.1026890612823697
Epoch 1304
Epoch 1304 :: Batch 0/1
Batch Loss = 3.1075534035159413
1304, epoch_train_loss=3.1075534035159413
Epoch 1305
Epoch 1305 :: Batch 0/1
Batch Loss = 3.0995668448544333
1305, epoch_train_loss=3.0995668448544333
Epoch 1306
Epoch 1306 :: Batch 0/1
Batch Loss = 3.097257226432904
1306, epoch_train_loss=3.097257226432904
Epoch 1307
Epoch 1307 :: Batch 0/1
Batch Loss = 3.096110858332098
1307, epoch_train_loss=3.096110858332098
Epoch 1308
Epoch 1308 :: Batch 0/1
Batch Loss = 3.0953411101457022
1308, epoch_train_loss=3.0953411101457022
Epoch 1309
Epoch 1309 :: Batch 0/1
Batch Loss = 3.093001163253031
1309, epoch_train_loss=3.093001163253031
Epoch 1310
Epoch 1310 :: Batch 0/1
Batch Loss = 3.091276840337134
1310, epoch_train_loss=3.091276840337134
Epoch 1311
Epoch 1311 :: Batch 0/1
Batch Loss = 3.0911694502452907
1311, epoch_train_loss=3.0911694502452907
Epoch 1312
Epoch 1312 :: Batch 0/1
Batch Loss = 3.0900949674968388
1312, epoch_train_loss=3.0900949674968388
Epoch 1313
Epoch 1313 :: Batch 0/1
Batch Loss = 3.0880553899535195
1313, epoch_train_loss=3.0880553899535195
Epoch 1314
Epoch 1314 :: Batch 0/1
Batch Loss = 3.087251038650725
1314, epoch_train_loss=3.087251038650725
Epoch 1315
Epoch 1315 :: Batch 0/1
Batch Loss = 3.0843981859799303
1315, epoch_train_loss=3.0843981859799303
Epoch 1316
Epoch 1316 :: Batch 0/1
Batch Loss = 3.0835617823927506
1316, epoch_train_loss=3.0835617823927506
Epoch 1317
Epoch 1317 :: Batch 0/1
Batch Loss = 3.0892825945581075
1317, epoch_train_loss=3.0892825945581075
Epoch 1318
Epoch 1318 :: Batch 0/1
Batch Loss = 3.0952054760977283
1318, epoch_train_loss=3.0952054760977283
Epoch 1319
Epoch 1319 :: Batch 0/1
Batch Loss = 3.1200748485245517
1319, epoch_train_loss=3.1200748485245517
Epoch 1320
Epoch 1320 :: Batch 0/1
Batch Loss = 3.110168050307732
1320, epoch_train_loss=3.110168050307732
Epoch 1321
Epoch 1321 :: Batch 0/1
Batch Loss = 3.141805875562467
1321, epoch_train_loss=3.141805875562467
Epoch 1322
Epoch 1322 :: Batch 0/1
Batch Loss = 3.081675818645423
1322, epoch_train_loss=3.081675818645423
Epoch 1323
Epoch 1323 :: Batch 0/1
Batch Loss = 3.114428250734919
1323, epoch_train_loss=3.114428250734919
Epoch 1324
Epoch 1324 :: Batch 0/1
Batch Loss = 3.2487384116642715
1324, epoch_train_loss=3.2487384116642715
Epoch 1325
Epoch 1325 :: Batch 0/1
Batch Loss = 3.2198990771935527
1325, epoch_train_loss=3.2198990771935527
Epoch 1326
Epoch 1326 :: Batch 0/1
Batch Loss = 3.1165207963211063
1326, epoch_train_loss=3.1165207963211063
Epoch 1327
Epoch 1327 :: Batch 0/1
Batch Loss = 3.10854451390845
1327, epoch_train_loss=3.10854451390845
Epoch 1328
Epoch 1328 :: Batch 0/1
Batch Loss = 3.156612728135568
1328, epoch_train_loss=3.156612728135568
Epoch 1329
Epoch 1329 :: Batch 0/1
Batch Loss = 3.1390941731447586
1329, epoch_train_loss=3.1390941731447586
Epoch 1330
Epoch 1330 :: Batch 0/1
Batch Loss = 3.1624258041316513
1330, epoch_train_loss=3.1624258041316513
Epoch 1331
Epoch 1331 :: Batch 0/1
Batch Loss = 3.1377187823251305
1331, epoch_train_loss=3.1377187823251305
Epoch 1332
Epoch 1332 :: Batch 0/1
Batch Loss = 3.153013462228058
1332, epoch_train_loss=3.153013462228058
Epoch 1333
Epoch 1333 :: Batch 0/1
Batch Loss = 3.111108094779334
1333, epoch_train_loss=3.111108094779334
Epoch 1334
Epoch 1334 :: Batch 0/1
Batch Loss = 3.175934197285331
1334, epoch_train_loss=3.175934197285331
Epoch 1335
Epoch 1335 :: Batch 0/1
Batch Loss = 3.122616958868486
1335, epoch_train_loss=3.122616958868486
Epoch 1336
Epoch 1336 :: Batch 0/1
Batch Loss = 3.168988671443494
1336, epoch_train_loss=3.168988671443494
Epoch 1337
Epoch 1337 :: Batch 0/1
Batch Loss = 3.115722352951717
1337, epoch_train_loss=3.115722352951717
Epoch 1338
Epoch 1338 :: Batch 0/1
Batch Loss = 3.1438328013353196
1338, epoch_train_loss=3.1438328013353196
Epoch 1339
Epoch 1339 :: Batch 0/1
Batch Loss = 3.1062681547008566
1339, epoch_train_loss=3.1062681547008566
Epoch 1340
Epoch 1340 :: Batch 0/1
Batch Loss = 3.1171781135745107
1340, epoch_train_loss=3.1171781135745107
Epoch 1341
Epoch 1341 :: Batch 0/1
Batch Loss = 3.126087708329429
1341, epoch_train_loss=3.126087708329429
Epoch 1342
Epoch 1342 :: Batch 0/1
Batch Loss = 3.0980204052962432
1342, epoch_train_loss=3.0980204052962432
Epoch 1343
Epoch 1343 :: Batch 0/1
Batch Loss = 3.1305888903112753
1343, epoch_train_loss=3.1305888903112753
Epoch 1344
Epoch 1344 :: Batch 0/1
Batch Loss = 3.0967706236689234
1344, epoch_train_loss=3.0967706236689234
Epoch 1345
Epoch 1345 :: Batch 0/1
Batch Loss = 3.1176225844406575
1345, epoch_train_loss=3.1176225844406575
Epoch 1346
Epoch 1346 :: Batch 0/1
Batch Loss = 3.100743552377515
1346, epoch_train_loss=3.100743552377515
Epoch 1347
Epoch 1347 :: Batch 0/1
Batch Loss = 3.1111910221963086
1347, epoch_train_loss=3.1111910221963086
Epoch 1348
Epoch 1348 :: Batch 0/1
Batch Loss = 3.0967233295406182
1348, epoch_train_loss=3.0967233295406182
Epoch 1349
Epoch 1349 :: Batch 0/1
Batch Loss = 3.104129167910783
1349, epoch_train_loss=3.104129167910783
Epoch 1350
Epoch 1350 :: Batch 0/1
Batch Loss = 3.098325689142902
1350, epoch_train_loss=3.098325689142902
Epoch 1351
Epoch 1351 :: Batch 0/1
Batch Loss = 3.1002909211748353
1351, epoch_train_loss=3.1002909211748353
Epoch 1352
Epoch 1352 :: Batch 0/1
Batch Loss = 3.0916096737970853
1352, epoch_train_loss=3.0916096737970853
Epoch 1353
Epoch 1353 :: Batch 0/1
Batch Loss = 3.096685143589497
1353, epoch_train_loss=3.096685143589497
Epoch 1354
Epoch 1354 :: Batch 0/1
Batch Loss = 3.089880699988813
1354, epoch_train_loss=3.089880699988813
Epoch 1355
Epoch 1355 :: Batch 0/1
Batch Loss = 3.0955678076576225
1355, epoch_train_loss=3.0955678076576225
Epoch 1356
Epoch 1356 :: Batch 0/1
Batch Loss = 3.0856933581783785
1356, epoch_train_loss=3.0856933581783785
Epoch 1357
Epoch 1357 :: Batch 0/1
Batch Loss = 3.0895722025928736
1357, epoch_train_loss=3.0895722025928736
Epoch 1358
Epoch 1358 :: Batch 0/1
Batch Loss = 3.0846071852764494
1358, epoch_train_loss=3.0846071852764494
Epoch 1359
Epoch 1359 :: Batch 0/1
Batch Loss = 3.0834020681884837
1359, epoch_train_loss=3.0834020681884837
Epoch 1360
Epoch 1360 :: Batch 0/1
Batch Loss = 3.0855081709973655
1360, epoch_train_loss=3.0855081709973655
Epoch 1361
Epoch 1361 :: Batch 0/1
Batch Loss = 3.0794994034457055
1361, epoch_train_loss=3.0794994034457055
Epoch 1362
Epoch 1362 :: Batch 0/1
Batch Loss = 3.0827857109293193
1362, epoch_train_loss=3.0827857109293193
Epoch 1363
Epoch 1363 :: Batch 0/1
Batch Loss = 3.0847574336891967
1363, epoch_train_loss=3.0847574336891967
Epoch 1364
Epoch 1364 :: Batch 0/1
Batch Loss = 3.0764281915232914
1364, epoch_train_loss=3.0764281915232914
Epoch 1365
Epoch 1365 :: Batch 0/1
Batch Loss = 3.0793434386092478
1365, epoch_train_loss=3.0793434386092478
Epoch 1366
Epoch 1366 :: Batch 0/1
Batch Loss = 3.087855818549234
1366, epoch_train_loss=3.087855818549234
Epoch 1367
Epoch 1367 :: Batch 0/1
Batch Loss = 3.0784136258162933
1367, epoch_train_loss=3.0784136258162933
Epoch 1368
Epoch 1368 :: Batch 0/1
Batch Loss = 3.072873972627749
1368, epoch_train_loss=3.072873972627749
Epoch 1369
Epoch 1369 :: Batch 0/1
Batch Loss = 3.0717377182229155
1369, epoch_train_loss=3.0717377182229155
Epoch 1370
Epoch 1370 :: Batch 0/1
Batch Loss = 3.074823446809631
1370, epoch_train_loss=3.074823446809631
Epoch 1371
Epoch 1371 :: Batch 0/1
Batch Loss = 3.0855960005895056
1371, epoch_train_loss=3.0855960005895056
Epoch 1372
Epoch 1372 :: Batch 0/1
Batch Loss = 3.093519987941648
1372, epoch_train_loss=3.093519987941648
Epoch 1373
Epoch 1373 :: Batch 0/1
Batch Loss = 3.1346410700714293
1373, epoch_train_loss=3.1346410700714293
Epoch 1374
Epoch 1374 :: Batch 0/1
Batch Loss = 3.072525118259043
1374, epoch_train_loss=3.072525118259043
Epoch 1375
Epoch 1375 :: Batch 0/1
Batch Loss = 3.0846082359128197
1375, epoch_train_loss=3.0846082359128197
Epoch 1376
Epoch 1376 :: Batch 0/1
Batch Loss = 3.1729073170853237
1376, epoch_train_loss=3.1729073170853237
Epoch 1377
Epoch 1377 :: Batch 0/1
Batch Loss = 3.072317990381326
1377, epoch_train_loss=3.072317990381326
Epoch 1378
Epoch 1378 :: Batch 0/1
Batch Loss = 3.3731686579570335
1378, epoch_train_loss=3.3731686579570335
Epoch 1379
Epoch 1379 :: Batch 0/1
Batch Loss = 3.390467258518589
1379, epoch_train_loss=3.390467258518589
Epoch 1380
Epoch 1380 :: Batch 0/1
Batch Loss = 3.54633777658401
1380, epoch_train_loss=3.54633777658401
Epoch 1381
Epoch 1381 :: Batch 0/1
Batch Loss = 3.5265274826135697
1381, epoch_train_loss=3.5265274826135697
Epoch 1382
Epoch 1382 :: Batch 0/1
Batch Loss = 3.5309336676963894
1382, epoch_train_loss=3.5309336676963894
Epoch 1383
Epoch 1383 :: Batch 0/1
Batch Loss = 3.5384351300693653
1383, epoch_train_loss=3.5384351300693653
Epoch 1384
Epoch 1384 :: Batch 0/1
Batch Loss = 3.521367573296315
1384, epoch_train_loss=3.521367573296315
Epoch 1385
Epoch 1385 :: Batch 0/1
Batch Loss = 3.486278157316588
1385, epoch_train_loss=3.486278157316588
Epoch 1386
Epoch 1386 :: Batch 0/1
Batch Loss = 3.4249029826754773
1386, epoch_train_loss=3.4249029826754773
Epoch 1387
Epoch 1387 :: Batch 0/1
Batch Loss = 3.4342449852423154
1387, epoch_train_loss=3.4342449852423154
Epoch 1388
Epoch 1388 :: Batch 0/1
Batch Loss = 3.444869967159702
1388, epoch_train_loss=3.444869967159702
Epoch 1389
Epoch 1389 :: Batch 0/1
Batch Loss = 3.3947515186319555
1389, epoch_train_loss=3.3947515186319555
Epoch 1390
Epoch 1390 :: Batch 0/1
Batch Loss = 3.368593091352289
1390, epoch_train_loss=3.368593091352289
Epoch 1391
Epoch 1391 :: Batch 0/1
Batch Loss = 3.3769667677792197
1391, epoch_train_loss=3.3769667677792197
Epoch 1392
Epoch 1392 :: Batch 0/1
Batch Loss = 3.3537019583363263
1392, epoch_train_loss=3.3537019583363263
Epoch 1393
Epoch 1393 :: Batch 0/1
Batch Loss = 3.335680611712134
1393, epoch_train_loss=3.335680611712134
Epoch 1394
Epoch 1394 :: Batch 0/1
Batch Loss = 3.301242109426777
1394, epoch_train_loss=3.301242109426777
Epoch 1395
Epoch 1395 :: Batch 0/1
Batch Loss = 3.314445013451327
1395, epoch_train_loss=3.314445013451327
Epoch 1396
Epoch 1396 :: Batch 0/1
Batch Loss = 3.2753589733048862
1396, epoch_train_loss=3.2753589733048862
Epoch 1397
Epoch 1397 :: Batch 0/1
Batch Loss = 3.273254800243555
1397, epoch_train_loss=3.273254800243555
Epoch 1398
Epoch 1398 :: Batch 0/1
Batch Loss = 3.2956313071068792
1398, epoch_train_loss=3.2956313071068792
Epoch 1399
Epoch 1399 :: Batch 0/1
Batch Loss = 3.278744634879053
1399, epoch_train_loss=3.278744634879053
Epoch 1400
Epoch 1400 :: Batch 0/1
Batch Loss = 3.2408316181656516
1400, epoch_train_loss=3.2408316181656516
Epoch 1401
Epoch 1401 :: Batch 0/1
Batch Loss = 3.2436925118224753
1401, epoch_train_loss=3.2436925118224753
Epoch 1402
Epoch 1402 :: Batch 0/1
Batch Loss = 3.2336380780905425
1402, epoch_train_loss=3.2336380780905425
Epoch 1403
Epoch 1403 :: Batch 0/1
Batch Loss = 3.1975727714557447
1403, epoch_train_loss=3.1975727714557447
Epoch 1404
Epoch 1404 :: Batch 0/1
Batch Loss = 3.1911210260685166
1404, epoch_train_loss=3.1911210260685166
Epoch 1405
Epoch 1405 :: Batch 0/1
Batch Loss = 3.180331302147794
1405, epoch_train_loss=3.180331302147794
Epoch 1406
Epoch 1406 :: Batch 0/1
Batch Loss = 3.1524165982702503
1406, epoch_train_loss=3.1524165982702503
Epoch 1407
Epoch 1407 :: Batch 0/1
Batch Loss = 3.145016160237099
1407, epoch_train_loss=3.145016160237099
Epoch 1408
Epoch 1408 :: Batch 0/1
Batch Loss = 3.1415796031125196
1408, epoch_train_loss=3.1415796031125196
Epoch 1409
Epoch 1409 :: Batch 0/1
Batch Loss = 3.137891474437223
1409, epoch_train_loss=3.137891474437223
Epoch 1410
Epoch 1410 :: Batch 0/1
Batch Loss = 3.128162971834584
1410, epoch_train_loss=3.128162971834584
Epoch 1411
Epoch 1411 :: Batch 0/1
Batch Loss = 3.1298632782898594
1411, epoch_train_loss=3.1298632782898594
Epoch 1412
Epoch 1412 :: Batch 0/1
Batch Loss = 3.1443070661740653
1412, epoch_train_loss=3.1443070661740653
Epoch 1413
Epoch 1413 :: Batch 0/1
Batch Loss = 3.2025406560619527
1413, epoch_train_loss=3.2025406560619527
Epoch 1414
Epoch 1414 :: Batch 0/1
Batch Loss = 3.322083406253259
1414, epoch_train_loss=3.322083406253259
Epoch 1415
Epoch 1415 :: Batch 0/1
Batch Loss = 3.4456480189805947
1415, epoch_train_loss=3.4456480189805947
Epoch 1416
Epoch 1416 :: Batch 0/1
Batch Loss = 3.1260365265459407
1416, epoch_train_loss=3.1260365265459407
Epoch 1417
Epoch 1417 :: Batch 0/1
Batch Loss = 3.390877045501431
1417, epoch_train_loss=3.390877045501431
Epoch 1418
Epoch 1418 :: Batch 0/1
Batch Loss = 3.137160553932848
1418, epoch_train_loss=3.137160553932848
Epoch 1419
Epoch 1419 :: Batch 0/1
Batch Loss = 3.2172991997421754
1419, epoch_train_loss=3.2172991997421754
Epoch 1420
Epoch 1420 :: Batch 0/1
Batch Loss = 3.2290723623026443
1420, epoch_train_loss=3.2290723623026443
Epoch 1421
Epoch 1421 :: Batch 0/1
Batch Loss = 3.248416893991707
1421, epoch_train_loss=3.248416893991707
Epoch 1422
Epoch 1422 :: Batch 0/1
Batch Loss = 3.2163969444283405
1422, epoch_train_loss=3.2163969444283405
Epoch 1423
Epoch 1423 :: Batch 0/1
Batch Loss = 3.182548268464173
1423, epoch_train_loss=3.182548268464173
Epoch 1424
Epoch 1424 :: Batch 0/1
Batch Loss = 3.203012785023465
1424, epoch_train_loss=3.203012785023465
Epoch 1425
Epoch 1425 :: Batch 0/1
Batch Loss = 3.1999407337602537
1425, epoch_train_loss=3.1999407337602537
Epoch 1426
Epoch 1426 :: Batch 0/1
Batch Loss = 3.154230217998265
1426, epoch_train_loss=3.154230217998265
Epoch 1427
Epoch 1427 :: Batch 0/1
Batch Loss = 3.137416081382031
1427, epoch_train_loss=3.137416081382031
Epoch 1428
Epoch 1428 :: Batch 0/1
Batch Loss = 3.147871140664414
1428, epoch_train_loss=3.147871140664414
Epoch 1429
Epoch 1429 :: Batch 0/1
Batch Loss = 3.155475570693611
1429, epoch_train_loss=3.155475570693611
Epoch 1430
Epoch 1430 :: Batch 0/1
Batch Loss = 3.143637408709453
1430, epoch_train_loss=3.143637408709453
Epoch 1431
Epoch 1431 :: Batch 0/1
Batch Loss = 3.1286666348348127
1431, epoch_train_loss=3.1286666348348127
Epoch 1432
Epoch 1432 :: Batch 0/1
Batch Loss = 3.1253437243544924
1432, epoch_train_loss=3.1253437243544924
Epoch 1433
Epoch 1433 :: Batch 0/1
Batch Loss = 3.1333929752789746
1433, epoch_train_loss=3.1333929752789746
Epoch 1434
Epoch 1434 :: Batch 0/1
Batch Loss = 3.1351628073098783
1434, epoch_train_loss=3.1351628073098783
Epoch 1435
Epoch 1435 :: Batch 0/1
Batch Loss = 3.123080712621569
1435, epoch_train_loss=3.123080712621569
Epoch 1436
Epoch 1436 :: Batch 0/1
Batch Loss = 3.119432103280137
1436, epoch_train_loss=3.119432103280137
Epoch 1437
Epoch 1437 :: Batch 0/1
Batch Loss = 3.125978055361063
1437, epoch_train_loss=3.125978055361063
Epoch 1438
Epoch 1438 :: Batch 0/1
Batch Loss = 3.1280519500406294
1438, epoch_train_loss=3.1280519500406294
Epoch 1439
Epoch 1439 :: Batch 0/1
Batch Loss = 3.1199675156113615
1439, epoch_train_loss=3.1199675156113615
Epoch 1440
Epoch 1440 :: Batch 0/1
Batch Loss = 3.114814289907641
1440, epoch_train_loss=3.114814289907641
Epoch 1441
Epoch 1441 :: Batch 0/1
Batch Loss = 3.1185846478378605
1441, epoch_train_loss=3.1185846478378605
Epoch 1442
Epoch 1442 :: Batch 0/1
Batch Loss = 3.1213574454990094
1442, epoch_train_loss=3.1213574454990094
Epoch 1443
Epoch 1443 :: Batch 0/1
Batch Loss = 3.118393006664902
1443, epoch_train_loss=3.118393006664902
Epoch 1444
Epoch 1444 :: Batch 0/1
Batch Loss = 3.1154458280098782
1444, epoch_train_loss=3.1154458280098782
Epoch 1445
Epoch 1445 :: Batch 0/1
Batch Loss = 3.115955325212527
1445, epoch_train_loss=3.115955325212527
Epoch 1446
Epoch 1446 :: Batch 0/1
Batch Loss = 3.1163856980442994
1446, epoch_train_loss=3.1163856980442994
Epoch 1447
Epoch 1447 :: Batch 0/1
Batch Loss = 3.115879372070465
1447, epoch_train_loss=3.115879372070465
Epoch 1448
Epoch 1448 :: Batch 0/1
Batch Loss = 3.1156529845075602
1448, epoch_train_loss=3.1156529845075602
Epoch 1449
Epoch 1449 :: Batch 0/1
Batch Loss = 3.1135544743263903
1449, epoch_train_loss=3.1135544743263903
Epoch 1450
Epoch 1450 :: Batch 0/1
Batch Loss = 3.11293520484284
1450, epoch_train_loss=3.11293520484284
Epoch 1451
Epoch 1451 :: Batch 0/1
Batch Loss = 3.113836821038838
1451, epoch_train_loss=3.113836821038838
Epoch 1452
Epoch 1452 :: Batch 0/1
Batch Loss = 3.114362025212679
1452, epoch_train_loss=3.114362025212679
Epoch 1453
Epoch 1453 :: Batch 0/1
Batch Loss = 3.1131061496418053
1453, epoch_train_loss=3.1131061496418053
Epoch 1454
Epoch 1454 :: Batch 0/1
Batch Loss = 3.1115654449160486
1454, epoch_train_loss=3.1115654449160486
Epoch 1455
Epoch 1455 :: Batch 0/1
Batch Loss = 3.1117608455433445
1455, epoch_train_loss=3.1117608455433445
Epoch 1456
Epoch 1456 :: Batch 0/1
Batch Loss = 3.112569654656542
1456, epoch_train_loss=3.112569654656542
Epoch 1457
Epoch 1457 :: Batch 0/1
Batch Loss = 3.111710685928824
1457, epoch_train_loss=3.111710685928824
Epoch 1458
Epoch 1458 :: Batch 0/1
Batch Loss = 3.1110405671928443
1458, epoch_train_loss=3.1110405671928443
Epoch 1459
Epoch 1459 :: Batch 0/1
Batch Loss = 3.111105812150824
1459, epoch_train_loss=3.111105812150824
Epoch 1460
Epoch 1460 :: Batch 0/1
Batch Loss = 3.1109173927600686
1460, epoch_train_loss=3.1109173927600686
Epoch 1461
Epoch 1461 :: Batch 0/1
Batch Loss = 3.1107279578758273
1461, epoch_train_loss=3.1107279578758273
Epoch 1462
Epoch 1462 :: Batch 0/1
Batch Loss = 3.110533755091962
1462, epoch_train_loss=3.110533755091962
Epoch 1463
Epoch 1463 :: Batch 0/1
Batch Loss = 3.1100590939191353
1463, epoch_train_loss=3.1100590939191353
Epoch 1464
Epoch 1464 :: Batch 0/1
Batch Loss = 3.109821222822004
1464, epoch_train_loss=3.109821222822004
Epoch 1465
Epoch 1465 :: Batch 0/1
Batch Loss = 3.1100472181028107
1465, epoch_train_loss=3.1100472181028107
Epoch 1466
Epoch 1466 :: Batch 0/1
Batch Loss = 3.109933567720143
1466, epoch_train_loss=3.109933567720143
Epoch 1467
Epoch 1467 :: Batch 0/1
Batch Loss = 3.109359687961638
1467, epoch_train_loss=3.109359687961638
Epoch 1468
Epoch 1468 :: Batch 0/1
Batch Loss = 3.109196123776468
1468, epoch_train_loss=3.109196123776468
Epoch 1469
Epoch 1469 :: Batch 0/1
Batch Loss = 3.1093336581923103
1469, epoch_train_loss=3.1093336581923103
Epoch 1470
Epoch 1470 :: Batch 0/1
Batch Loss = 3.109174399006109
1470, epoch_train_loss=3.109174399006109
Epoch 1471
Epoch 1471 :: Batch 0/1
Batch Loss = 3.1088512545185294
1471, epoch_train_loss=3.1088512545185294
Epoch 1472
Epoch 1472 :: Batch 0/1
Batch Loss = 3.108658669470896
1472, epoch_train_loss=3.108658669470896
Epoch 1473
Epoch 1473 :: Batch 0/1
Batch Loss = 3.1085545486899573
1473, epoch_train_loss=3.1085545486899573
Epoch 1474
Epoch 1474 :: Batch 0/1
Batch Loss = 3.1084526971384374
1474, epoch_train_loss=3.1084526971384374
Epoch 1475
Epoch 1475 :: Batch 0/1
Batch Loss = 3.108366332075509
1475, epoch_train_loss=3.108366332075509
Epoch 1476
Epoch 1476 :: Batch 0/1
Batch Loss = 3.108137860702353
1476, epoch_train_loss=3.108137860702353
Epoch 1477
Epoch 1477 :: Batch 0/1
Batch Loss = 3.107923660590828
1477, epoch_train_loss=3.107923660590828
Epoch 1478
Epoch 1478 :: Batch 0/1
Batch Loss = 3.1078717902731388
1478, epoch_train_loss=3.1078717902731388
Epoch 1479
Epoch 1479 :: Batch 0/1
Batch Loss = 3.1078204717439117
1479, epoch_train_loss=3.1078204717439117
Epoch 1480
Epoch 1480 :: Batch 0/1
Batch Loss = 3.1076322562607395
1480, epoch_train_loss=3.1076322562607395
Epoch 1481
Epoch 1481 :: Batch 0/1
Batch Loss = 3.1074419483000066
1481, epoch_train_loss=3.1074419483000066
Epoch 1482
Epoch 1482 :: Batch 0/1
Batch Loss = 3.1073575646900053
1482, epoch_train_loss=3.1073575646900053
Epoch 1483
Epoch 1483 :: Batch 0/1
Batch Loss = 3.1072517645725193
1483, epoch_train_loss=3.1072517645725193
Epoch 1484
Epoch 1484 :: Batch 0/1
Batch Loss = 3.1071059155983978
1484, epoch_train_loss=3.1071059155983978
Epoch 1485
Epoch 1485 :: Batch 0/1
Batch Loss = 3.106978488684413
1485, epoch_train_loss=3.106978488684413
Epoch 1486
Epoch 1486 :: Batch 0/1
Batch Loss = 3.1068548096761366
1486, epoch_train_loss=3.1068548096761366
Epoch 1487
Epoch 1487 :: Batch 0/1
Batch Loss = 3.106726942747823
1487, epoch_train_loss=3.106726942747823
Epoch 1488
Epoch 1488 :: Batch 0/1
Batch Loss = 3.1066235308859524
1488, epoch_train_loss=3.1066235308859524
Epoch 1489
Epoch 1489 :: Batch 0/1
Batch Loss = 3.1065057174045725
1489, epoch_train_loss=3.1065057174045725
Epoch 1490
Epoch 1490 :: Batch 0/1
Batch Loss = 3.106363281536054
1490, epoch_train_loss=3.106363281536054
Epoch 1491
Epoch 1491 :: Batch 0/1
Batch Loss = 3.106260913866945
1491, epoch_train_loss=3.106260913866945
Epoch 1492
Epoch 1492 :: Batch 0/1
Batch Loss = 3.106173198804355
1492, epoch_train_loss=3.106173198804355
Epoch 1493
Epoch 1493 :: Batch 0/1
Batch Loss = 3.106048408494483
1493, epoch_train_loss=3.106048408494483
Epoch 1494
Epoch 1494 :: Batch 0/1
Batch Loss = 3.1059193843588013
1494, epoch_train_loss=3.1059193843588013
Epoch 1495
Epoch 1495 :: Batch 0/1
Batch Loss = 3.1058199155863777
1495, epoch_train_loss=3.1058199155863777
Epoch 1496
Epoch 1496 :: Batch 0/1
Batch Loss = 3.1057170054786036
1496, epoch_train_loss=3.1057170054786036
Epoch 1497
Epoch 1497 :: Batch 0/1
Batch Loss = 3.1056057804872266
1497, epoch_train_loss=3.1056057804872266
Epoch 1498
Epoch 1498 :: Batch 0/1
Batch Loss = 3.105497387356423
1498, epoch_train_loss=3.105497387356423
Epoch 1499
Epoch 1499 :: Batch 0/1
Batch Loss = 3.105385534027718
1499, epoch_train_loss=3.105385534027718
Epoch 1500
Epoch 1500 :: Batch 0/1
Batch Loss = 3.105281400323509
1500, epoch_train_loss=3.105281400323509
Epoch 1501
Epoch 1501 :: Batch 0/1
Batch Loss = 3.105188113339887
1501, epoch_train_loss=3.105188113339887
Epoch 1502
Epoch 1502 :: Batch 0/1
Batch Loss = 3.105085028175471
1502, epoch_train_loss=3.105085028175471
Epoch 1503
Epoch 1503 :: Batch 0/1
Batch Loss = 3.1049734085916323
1503, epoch_train_loss=3.1049734085916323
Epoch 1504
Epoch 1504 :: Batch 0/1
Batch Loss = 3.1048756150812475
1504, epoch_train_loss=3.1048756150812475
Epoch 1505
Epoch 1505 :: Batch 0/1
Batch Loss = 3.1047823592431185
1505, epoch_train_loss=3.1047823592431185
Epoch 1506
Epoch 1506 :: Batch 0/1
Batch Loss = 3.1046825184763125
1506, epoch_train_loss=3.1046825184763125
Epoch 1507
Epoch 1507 :: Batch 0/1
Batch Loss = 3.104582972368348
1507, epoch_train_loss=3.104582972368348
Epoch 1508
Epoch 1508 :: Batch 0/1
Batch Loss = 3.1044870724199276
1508, epoch_train_loss=3.1044870724199276
Epoch 1509
Epoch 1509 :: Batch 0/1
Batch Loss = 3.104391147756912
1509, epoch_train_loss=3.104391147756912
Epoch 1510
Epoch 1510 :: Batch 0/1
Batch Loss = 3.104297189894711
1510, epoch_train_loss=3.104297189894711
Epoch 1511
Epoch 1511 :: Batch 0/1
Batch Loss = 3.1042043084942357
1511, epoch_train_loss=3.1042043084942357
Epoch 1512
Epoch 1512 :: Batch 0/1
Batch Loss = 3.1041091597764794
1512, epoch_train_loss=3.1041091597764794
Epoch 1513
Epoch 1513 :: Batch 0/1
Batch Loss = 3.1040174414005195
1513, epoch_train_loss=3.1040174414005195
Epoch 1514
Epoch 1514 :: Batch 0/1
Batch Loss = 3.1039291368771793
1514, epoch_train_loss=3.1039291368771793
Epoch 1515
Epoch 1515 :: Batch 0/1
Batch Loss = 3.1038381305239984
1515, epoch_train_loss=3.1038381305239984
Epoch 1516
Epoch 1516 :: Batch 0/1
Batch Loss = 3.1037462653442525
1516, epoch_train_loss=3.1037462653442525
Epoch 1517
Epoch 1517 :: Batch 0/1
Batch Loss = 3.1036584028965404
1517, epoch_train_loss=3.1036584028965404
Epoch 1518
Epoch 1518 :: Batch 0/1
Batch Loss = 3.1035712223632164
1518, epoch_train_loss=3.1035712223632164
Epoch 1519
Epoch 1519 :: Batch 0/1
Batch Loss = 3.103483393243999
1519, epoch_train_loss=3.103483393243999
Epoch 1520
Epoch 1520 :: Batch 0/1
Batch Loss = 3.1033964366163724
1520, epoch_train_loss=3.1033964366163724
Epoch 1521
Epoch 1521 :: Batch 0/1
Batch Loss = 3.1033097672576955
1521, epoch_train_loss=3.1033097672576955
Epoch 1522
Epoch 1522 :: Batch 0/1
Batch Loss = 3.1032243011084133
1522, epoch_train_loss=3.1032243011084133
Epoch 1523
Epoch 1523 :: Batch 0/1
Batch Loss = 3.1031404989177185
1523, epoch_train_loss=3.1031404989177185
Epoch 1524
Epoch 1524 :: Batch 0/1
Batch Loss = 3.1030561550013913
1524, epoch_train_loss=3.1030561550013913
Epoch 1525
Epoch 1525 :: Batch 0/1
Batch Loss = 3.1029716111210535
1525, epoch_train_loss=3.1029716111210535
Epoch 1526
Epoch 1526 :: Batch 0/1
Batch Loss = 3.1028890410726646
1526, epoch_train_loss=3.1028890410726646
Epoch 1527
Epoch 1527 :: Batch 0/1
Batch Loss = 3.10280708933798
1527, epoch_train_loss=3.10280708933798
Epoch 1528
Epoch 1528 :: Batch 0/1
Batch Loss = 3.1027249307032014
1528, epoch_train_loss=3.1027249307032014
Epoch 1529
Epoch 1529 :: Batch 0/1
Batch Loss = 3.1026434698308116
1529, epoch_train_loss=3.1026434698308116
Epoch 1530
Epoch 1530 :: Batch 0/1
Batch Loss = 3.102562811751706
1530, epoch_train_loss=3.102562811751706
Epoch 1531
Epoch 1531 :: Batch 0/1
Batch Loss = 3.1024826540838757
1531, epoch_train_loss=3.1024826540838757
Epoch 1532
Epoch 1532 :: Batch 0/1
Batch Loss = 3.102403268305739
1532, epoch_train_loss=3.102403268305739
Epoch 1533
Epoch 1533 :: Batch 0/1
Batch Loss = 3.1023241976031937
1533, epoch_train_loss=3.1023241976031937
Epoch 1534
Epoch 1534 :: Batch 0/1
Batch Loss = 3.102245493450789
1534, epoch_train_loss=3.102245493450789
Epoch 1535
Epoch 1535 :: Batch 0/1
Batch Loss = 3.1021677902089033
1535, epoch_train_loss=3.1021677902089033
Epoch 1536
Epoch 1536 :: Batch 0/1
Batch Loss = 3.1020906992058137
1536, epoch_train_loss=3.1020906992058137
Epoch 1537
Epoch 1537 :: Batch 0/1
Batch Loss = 3.102013775231685
1537, epoch_train_loss=3.102013775231685
Epoch 1538
Epoch 1538 :: Batch 0/1
Batch Loss = 3.1019375594732304
1538, epoch_train_loss=3.1019375594732304
Epoch 1539
Epoch 1539 :: Batch 0/1
Batch Loss = 3.10186214892639
1539, epoch_train_loss=3.10186214892639
Epoch 1540
Epoch 1540 :: Batch 0/1
Batch Loss = 3.1017871670629797
1540, epoch_train_loss=3.1017871670629797
Epoch 1541
Epoch 1541 :: Batch 0/1
Batch Loss = 3.1017127590418023
1541, epoch_train_loss=3.1017127590418023
Epoch 1542
Epoch 1542 :: Batch 0/1
Batch Loss = 3.101638900399273
1542, epoch_train_loss=3.101638900399273
Epoch 1543
Epoch 1543 :: Batch 0/1
Batch Loss = 3.101565608197864
1543, epoch_train_loss=3.101565608197864
Epoch 1544
Epoch 1544 :: Batch 0/1
Batch Loss = 3.1014930671020062
1544, epoch_train_loss=3.1014930671020062
Epoch 1545
Epoch 1545 :: Batch 0/1
Batch Loss = 3.101421074505603
1545, epoch_train_loss=3.101421074505603
Epoch 1546
Epoch 1546 :: Batch 0/1
Batch Loss = 3.1013494895222067
1546, epoch_train_loss=3.1013494895222067
Epoch 1547
Epoch 1547 :: Batch 0/1
Batch Loss = 3.101278609691417
1547, epoch_train_loss=3.101278609691417
Epoch 1548
Epoch 1548 :: Batch 0/1
Batch Loss = 3.1012083739520504
1548, epoch_train_loss=3.1012083739520504
Epoch 1549
Epoch 1549 :: Batch 0/1
Batch Loss = 3.1011386473371565
1549, epoch_train_loss=3.1011386473371565
Epoch 1550
Epoch 1550 :: Batch 0/1
Batch Loss = 3.1010695026607493
1550, epoch_train_loss=3.1010695026607493
Epoch 1551
Epoch 1551 :: Batch 0/1
Batch Loss = 3.1010009449321094
1551, epoch_train_loss=3.1010009449321094
Epoch 1552
Epoch 1552 :: Batch 0/1
Batch Loss = 3.1009329590281203
1552, epoch_train_loss=3.1009329590281203
Epoch 1553
Epoch 1553 :: Batch 0/1
Batch Loss = 3.1008655944141013
1553, epoch_train_loss=3.1008655944141013
Epoch 1554
Epoch 1554 :: Batch 0/1
Batch Loss = 3.1007987627682407
1554, epoch_train_loss=3.1007987627682407
Epoch 1555
Epoch 1555 :: Batch 0/1
Batch Loss = 3.10073246346299
1555, epoch_train_loss=3.10073246346299
Epoch 1556
Epoch 1556 :: Batch 0/1
Batch Loss = 3.10066677439428
1556, epoch_train_loss=3.10066677439428
Epoch 1557
Epoch 1557 :: Batch 0/1
Batch Loss = 3.1006016341260385
1557, epoch_train_loss=3.1006016341260385
Epoch 1558
Epoch 1558 :: Batch 0/1
Batch Loss = 3.1005369903260434
1558, epoch_train_loss=3.1005369903260434
Epoch 1559
Epoch 1559 :: Batch 0/1
Batch Loss = 3.1004728988239125
1559, epoch_train_loss=3.1004728988239125
Epoch 1560
Epoch 1560 :: Batch 0/1
Batch Loss = 3.1004093439818554
1560, epoch_train_loss=3.1004093439818554
Epoch 1561
Epoch 1561 :: Batch 0/1
Batch Loss = 3.100346305729125
1561, epoch_train_loss=3.100346305729125
Epoch 1562
Epoch 1562 :: Batch 0/1
Batch Loss = 3.1002837826992873
1562, epoch_train_loss=3.1002837826992873
Epoch 1563
Epoch 1563 :: Batch 0/1
Batch Loss = 3.1002217375583663
1563, epoch_train_loss=3.1002217375583663
Epoch 1564
Epoch 1564 :: Batch 0/1
Batch Loss = 3.1001601852716254
1564, epoch_train_loss=3.1001601852716254
Epoch 1565
Epoch 1565 :: Batch 0/1
Batch Loss = 3.100099136954246
1565, epoch_train_loss=3.100099136954246
Epoch 1566
Epoch 1566 :: Batch 0/1
Batch Loss = 3.1000385404163238
1566, epoch_train_loss=3.1000385404163238
Epoch 1567
Epoch 1567 :: Batch 0/1
Batch Loss = 3.0999783920190214
1567, epoch_train_loss=3.0999783920190214
Epoch 1568
Epoch 1568 :: Batch 0/1
Batch Loss = 3.09991870263403
1568, epoch_train_loss=3.09991870263403
Epoch 1569
Epoch 1569 :: Batch 0/1
Batch Loss = 3.099859450364652
1569, epoch_train_loss=3.099859450364652
Epoch 1570
Epoch 1570 :: Batch 0/1
Batch Loss = 3.099800630147752
1570, epoch_train_loss=3.099800630147752
Epoch 1571
Epoch 1571 :: Batch 0/1
Batch Loss = 3.0997422308485167
1571, epoch_train_loss=3.0997422308485167
Epoch 1572
Epoch 1572 :: Batch 0/1
Batch Loss = 3.099684236789982
1572, epoch_train_loss=3.099684236789982
Epoch 1573
Epoch 1573 :: Batch 0/1
Batch Loss = 3.0996266501457352
1573, epoch_train_loss=3.0996266501457352
Epoch 1574
Epoch 1574 :: Batch 0/1
Batch Loss = 3.099569457024937
1574, epoch_train_loss=3.099569457024937
Epoch 1575
Epoch 1575 :: Batch 0/1
Batch Loss = 3.0995126395470476
1575, epoch_train_loss=3.0995126395470476
Epoch 1576
Epoch 1576 :: Batch 0/1
Batch Loss = 3.0994562010213667
1576, epoch_train_loss=3.0994562010213667
Epoch 1577
Epoch 1577 :: Batch 0/1
Batch Loss = 3.099400133526123
1577, epoch_train_loss=3.099400133526123
Epoch 1578
Epoch 1578 :: Batch 0/1
Batch Loss = 3.0993444219479143
1578, epoch_train_loss=3.0993444219479143
Epoch 1579
Epoch 1579 :: Batch 0/1
Batch Loss = 3.099289062703547
1579, epoch_train_loss=3.099289062703547
Epoch 1580
Epoch 1580 :: Batch 0/1
Batch Loss = 3.099234047377035
1580, epoch_train_loss=3.099234047377035
Epoch 1581
Epoch 1581 :: Batch 0/1
Batch Loss = 3.0991793685831124
1581, epoch_train_loss=3.0991793685831124
Epoch 1582
Epoch 1582 :: Batch 0/1
Batch Loss = 3.0991250199059404
1582, epoch_train_loss=3.0991250199059404
Epoch 1583
Epoch 1583 :: Batch 0/1
Batch Loss = 3.099070985566431
1583, epoch_train_loss=3.099070985566431
Epoch 1584
Epoch 1584 :: Batch 0/1
Batch Loss = 3.0990172584479354
1584, epoch_train_loss=3.0990172584479354
Epoch 1585
Epoch 1585 :: Batch 0/1
Batch Loss = 3.0989638330975766
1585, epoch_train_loss=3.0989638330975766
Epoch 1586
Epoch 1586 :: Batch 0/1
Batch Loss = 3.0989106941461633
1586, epoch_train_loss=3.0989106941461633
Epoch 1587
Epoch 1587 :: Batch 0/1
Batch Loss = 3.098857829052725
1587, epoch_train_loss=3.098857829052725
Epoch 1588
Epoch 1588 :: Batch 0/1
Batch Loss = 3.09880522458114
1588, epoch_train_loss=3.09880522458114
Epoch 1589
Epoch 1589 :: Batch 0/1
Batch Loss = 3.0987528650562215
1589, epoch_train_loss=3.0987528650562215
Epoch 1590
Epoch 1590 :: Batch 0/1
Batch Loss = 3.0987007356036145
1590, epoch_train_loss=3.0987007356036145
Epoch 1591
Epoch 1591 :: Batch 0/1
Batch Loss = 3.0986488170180726
1591, epoch_train_loss=3.0986488170180726
Epoch 1592
Epoch 1592 :: Batch 0/1
Batch Loss = 3.0985970897899486
1592, epoch_train_loss=3.0985970897899486
Epoch 1593
Epoch 1593 :: Batch 0/1
Batch Loss = 3.098545537130741
1593, epoch_train_loss=3.098545537130741
Epoch 1594
Epoch 1594 :: Batch 0/1
Batch Loss = 3.0984941383575597
1594, epoch_train_loss=3.0984941383575597
Epoch 1595
Epoch 1595 :: Batch 0/1
Batch Loss = 3.0984428721098976
1595, epoch_train_loss=3.0984428721098976
Epoch 1596
Epoch 1596 :: Batch 0/1
Batch Loss = 3.0983917181708893
1596, epoch_train_loss=3.0983917181708893
Epoch 1597
Epoch 1597 :: Batch 0/1
Batch Loss = 3.0983406545662135
1597, epoch_train_loss=3.0983406545662135
Epoch 1598
Epoch 1598 :: Batch 0/1
Batch Loss = 3.0982896598121545
1598, epoch_train_loss=3.0982896598121545
Epoch 1599
Epoch 1599 :: Batch 0/1
Batch Loss = 3.0982387127857858
1599, epoch_train_loss=3.0982387127857858
Epoch 1600
Epoch 1600 :: Batch 0/1
Batch Loss = 3.0981877921185843
1600, epoch_train_loss=3.0981877921185843
Epoch 1601
Epoch 1601 :: Batch 0/1
Batch Loss = 3.098136878775749
1601, epoch_train_loss=3.098136878775749
Epoch 1602
Epoch 1602 :: Batch 0/1
Batch Loss = 3.0980859528263736
1602, epoch_train_loss=3.0980859528263736
Epoch 1603
Epoch 1603 :: Batch 0/1
Batch Loss = 3.098034994507215
1603, epoch_train_loss=3.098034994507215
Epoch 1604
Epoch 1604 :: Batch 0/1
Batch Loss = 3.097983986196485
1604, epoch_train_loss=3.097983986196485
Epoch 1605
Epoch 1605 :: Batch 0/1
Batch Loss = 3.097932910561821
1605, epoch_train_loss=3.097932910561821
Epoch 1606
Epoch 1606 :: Batch 0/1
Batch Loss = 3.0978817508058305
1606, epoch_train_loss=3.0978817508058305
Epoch 1607
Epoch 1607 :: Batch 0/1
Batch Loss = 3.0978304910152548
1607, epoch_train_loss=3.0978304910152548
Epoch 1608
Epoch 1608 :: Batch 0/1
Batch Loss = 3.0977791160164805
1608, epoch_train_loss=3.0977791160164805
Epoch 1609
Epoch 1609 :: Batch 0/1
Batch Loss = 3.0977276124774926
1609, epoch_train_loss=3.0977276124774926
Epoch 1610
Epoch 1610 :: Batch 0/1
Batch Loss = 3.0976759675633696
1610, epoch_train_loss=3.0976759675633696
Epoch 1611
Epoch 1611 :: Batch 0/1
Batch Loss = 3.0976241686755204
1611, epoch_train_loss=3.0976241686755204
Epoch 1612
Epoch 1612 :: Batch 0/1
Batch Loss = 3.0975722044558216
1612, epoch_train_loss=3.0975722044558216
Epoch 1613
Epoch 1613 :: Batch 0/1
Batch Loss = 3.097520064099003
1613, epoch_train_loss=3.097520064099003
Epoch 1614
Epoch 1614 :: Batch 0/1
Batch Loss = 3.0974677372897386
1614, epoch_train_loss=3.0974677372897386
Epoch 1615
Epoch 1615 :: Batch 0/1
Batch Loss = 3.0974152145888425
1615, epoch_train_loss=3.0974152145888425
Epoch 1616
Epoch 1616 :: Batch 0/1
Batch Loss = 3.097362487104453
1616, epoch_train_loss=3.097362487104453
Epoch 1617
Epoch 1617 :: Batch 0/1
Batch Loss = 3.0973095467181926
1617, epoch_train_loss=3.0973095467181926
Epoch 1618
Epoch 1618 :: Batch 0/1
Batch Loss = 3.0972563856418116
1618, epoch_train_loss=3.0972563856418116
Epoch 1619
Epoch 1619 :: Batch 0/1
Batch Loss = 3.097202996340041
1619, epoch_train_loss=3.097202996340041
Epoch 1620
Epoch 1620 :: Batch 0/1
Batch Loss = 3.0971493721482624
1620, epoch_train_loss=3.0971493721482624
Epoch 1621
Epoch 1621 :: Batch 0/1
Batch Loss = 3.0970955066748123
1621, epoch_train_loss=3.0970955066748123
Epoch 1622
Epoch 1622 :: Batch 0/1
Batch Loss = 3.0970413937634733
1622, epoch_train_loss=3.0970413937634733
Epoch 1623
Epoch 1623 :: Batch 0/1
Batch Loss = 3.0969870277076788
1623, epoch_train_loss=3.0969870277076788
Epoch 1624
Epoch 1624 :: Batch 0/1
Batch Loss = 3.096932403176457
1624, epoch_train_loss=3.096932403176457
Epoch 1625
Epoch 1625 :: Batch 0/1
Batch Loss = 3.096877515330041
1625, epoch_train_loss=3.096877515330041
Epoch 1626
Epoch 1626 :: Batch 0/1
Batch Loss = 3.096822359605318
1626, epoch_train_loss=3.096822359605318
Epoch 1627
Epoch 1627 :: Batch 0/1
Batch Loss = 3.096766931755758
1627, epoch_train_loss=3.096766931755758
Epoch 1628
Epoch 1628 :: Batch 0/1
Batch Loss = 3.0967112279523312
1628, epoch_train_loss=3.0967112279523312
Epoch 1629
Epoch 1629 :: Batch 0/1
Batch Loss = 3.0966552445735944
1629, epoch_train_loss=3.0966552445735944
Epoch 1630
Epoch 1630 :: Batch 0/1
Batch Loss = 3.0965989783377
1630, epoch_train_loss=3.0965989783377
Epoch 1631
Epoch 1631 :: Batch 0/1
Batch Loss = 3.096542426469672
1631, epoch_train_loss=3.096542426469672
Epoch 1632
Epoch 1632 :: Batch 0/1
Batch Loss = 3.096485586530567
1632, epoch_train_loss=3.096485586530567
Epoch 1633
Epoch 1633 :: Batch 0/1
Batch Loss = 3.0964284564140026
1633, epoch_train_loss=3.0964284564140026
Epoch 1634
Epoch 1634 :: Batch 0/1
Batch Loss = 3.0963710343082953
1634, epoch_train_loss=3.0963710343082953
Epoch 1635
Epoch 1635 :: Batch 0/1
Batch Loss = 3.0963133188128364
1635, epoch_train_loss=3.0963133188128364
Epoch 1636
Epoch 1636 :: Batch 0/1
Batch Loss = 3.09625530895701
1636, epoch_train_loss=3.09625530895701
Epoch 1637
Epoch 1637 :: Batch 0/1
Batch Loss = 3.0961970040813522
1637, epoch_train_loss=3.0961970040813522
Epoch 1638
Epoch 1638 :: Batch 0/1
Batch Loss = 3.096138403897405
1638, epoch_train_loss=3.096138403897405
Epoch 1639
Epoch 1639 :: Batch 0/1
Batch Loss = 3.0960795084821093
1639, epoch_train_loss=3.0960795084821093
Epoch 1640
Epoch 1640 :: Batch 0/1
Batch Loss = 3.0960203182736326
1640, epoch_train_loss=3.0960203182736326
Epoch 1641
Epoch 1641 :: Batch 0/1
Batch Loss = 3.095960834136903
1641, epoch_train_loss=3.095960834136903
Epoch 1642
Epoch 1642 :: Batch 0/1
Batch Loss = 3.0959010572903325
1642, epoch_train_loss=3.0959010572903325
Epoch 1643
Epoch 1643 :: Batch 0/1
Batch Loss = 3.0958409892738517
1643, epoch_train_loss=3.0958409892738517
Epoch 1644
Epoch 1644 :: Batch 0/1
Batch Loss = 3.0957806319397863
1644, epoch_train_loss=3.0957806319397863
Epoch 1645
Epoch 1645 :: Batch 0/1
Batch Loss = 3.095719987481746
1645, epoch_train_loss=3.095719987481746
Epoch 1646
Epoch 1646 :: Batch 0/1
Batch Loss = 3.095659058468909
1646, epoch_train_loss=3.095659058468909
Epoch 1647
Epoch 1647 :: Batch 0/1
Batch Loss = 3.0955978477835386
1647, epoch_train_loss=3.0955978477835386
Epoch 1648
Epoch 1648 :: Batch 0/1
Batch Loss = 3.09553635860555
1648, epoch_train_loss=3.09553635860555
Epoch 1649
Epoch 1649 :: Batch 0/1
Batch Loss = 3.0954745943931683
1649, epoch_train_loss=3.0954745943931683
Epoch 1650
Epoch 1650 :: Batch 0/1
Batch Loss = 3.0954125588583823
1650, epoch_train_loss=3.0954125588583823
Epoch 1651
Epoch 1651 :: Batch 0/1
Batch Loss = 3.0953502559707275
1651, epoch_train_loss=3.0953502559707275
Epoch 1652
Epoch 1652 :: Batch 0/1
Batch Loss = 3.095287689951567
1652, epoch_train_loss=3.095287689951567
Epoch 1653
Epoch 1653 :: Batch 0/1
Batch Loss = 3.095224865201967
1653, epoch_train_loss=3.095224865201967
Epoch 1654
Epoch 1654 :: Batch 0/1
Batch Loss = 3.0951617863110568
1654, epoch_train_loss=3.0951617863110568
Epoch 1655
Epoch 1655 :: Batch 0/1
Batch Loss = 3.0950984580430068
1655, epoch_train_loss=3.0950984580430068
Epoch 1656
Epoch 1656 :: Batch 0/1
Batch Loss = 3.0950348853565894
1656, epoch_train_loss=3.0950348853565894
Epoch 1657
Epoch 1657 :: Batch 0/1
Batch Loss = 3.0949710733364255
1657, epoch_train_loss=3.0949710733364255
Epoch 1658
Epoch 1658 :: Batch 0/1
Batch Loss = 3.0949070271626598
1658, epoch_train_loss=3.0949070271626598
Epoch 1659
Epoch 1659 :: Batch 0/1
Batch Loss = 3.0948427520875224
1659, epoch_train_loss=3.0948427520875224
Epoch 1660
Epoch 1660 :: Batch 0/1
Batch Loss = 3.094778253418607
1660, epoch_train_loss=3.094778253418607
Epoch 1661
Epoch 1661 :: Batch 0/1
Batch Loss = 3.094713536521299
1661, epoch_train_loss=3.094713536521299
Epoch 1662
Epoch 1662 :: Batch 0/1
Batch Loss = 3.094648606777184
1662, epoch_train_loss=3.094648606777184
Epoch 1663
Epoch 1663 :: Batch 0/1
Batch Loss = 3.094583469581516
1663, epoch_train_loss=3.094583469581516
Epoch 1664
Epoch 1664 :: Batch 0/1
Batch Loss = 3.094518130291707
1664, epoch_train_loss=3.094518130291707
Epoch 1665
Epoch 1665 :: Batch 0/1
Batch Loss = 3.0944525942136245
1665, epoch_train_loss=3.0944525942136245
Epoch 1666
Epoch 1666 :: Batch 0/1
Batch Loss = 3.0943868666110808
1666, epoch_train_loss=3.0943868666110808
Epoch 1667
Epoch 1667 :: Batch 0/1
Batch Loss = 3.0943209526683173
1667, epoch_train_loss=3.0943209526683173
Epoch 1668
Epoch 1668 :: Batch 0/1
Batch Loss = 3.0942548574428765
1668, epoch_train_loss=3.0942548574428765
Epoch 1669
Epoch 1669 :: Batch 0/1
Batch Loss = 3.0941885858809535
1669, epoch_train_loss=3.0941885858809535
Epoch 1670
Epoch 1670 :: Batch 0/1
Batch Loss = 3.094122142793428
1670, epoch_train_loss=3.094122142793428
Epoch 1671
Epoch 1671 :: Batch 0/1
Batch Loss = 3.0940555328565242
1671, epoch_train_loss=3.0940555328565242
Epoch 1672
Epoch 1672 :: Batch 0/1
Batch Loss = 3.09398876059157
1672, epoch_train_loss=3.09398876059157
Epoch 1673
Epoch 1673 :: Batch 0/1
Batch Loss = 3.0939218303310727
1673, epoch_train_loss=3.0939218303310727
Epoch 1674
Epoch 1674 :: Batch 0/1
Batch Loss = 3.0938547462214263
1674, epoch_train_loss=3.0938547462214263
Epoch 1675
Epoch 1675 :: Batch 0/1
Batch Loss = 3.09378751220005
1675, epoch_train_loss=3.09378751220005
Epoch 1676
Epoch 1676 :: Batch 0/1
Batch Loss = 3.0937201320115157
1676, epoch_train_loss=3.0937201320115157
Epoch 1677
Epoch 1677 :: Batch 0/1
Batch Loss = 3.093652609183401
1677, epoch_train_loss=3.093652609183401
Epoch 1678
Epoch 1678 :: Batch 0/1
Batch Loss = 3.093584947013725
1678, epoch_train_loss=3.093584947013725
Epoch 1679
Epoch 1679 :: Batch 0/1
Batch Loss = 3.093517148577367
1679, epoch_train_loss=3.093517148577367
Epoch 1680
Epoch 1680 :: Batch 0/1
Batch Loss = 3.093449216702568
1680, epoch_train_loss=3.093449216702568
Epoch 1681
Epoch 1681 :: Batch 0/1
Batch Loss = 3.093381154000246
1681, epoch_train_loss=3.093381154000246
Epoch 1682
Epoch 1682 :: Batch 0/1
Batch Loss = 3.093312962820548
1682, epoch_train_loss=3.093312962820548
Epoch 1683
Epoch 1683 :: Batch 0/1
Batch Loss = 3.093244645270782
1683, epoch_train_loss=3.093244645270782
Epoch 1684
Epoch 1684 :: Batch 0/1
Batch Loss = 3.093176203208649
1684, epoch_train_loss=3.093176203208649
Epoch 1685
Epoch 1685 :: Batch 0/1
Batch Loss = 3.0931076382492777
1685, epoch_train_loss=3.0931076382492777
Epoch 1686
Epoch 1686 :: Batch 0/1
Batch Loss = 3.093038951763646
1686, epoch_train_loss=3.093038951763646
Epoch 1687
Epoch 1687 :: Batch 0/1
Batch Loss = 3.0929701448695037
1687, epoch_train_loss=3.0929701448695037
Epoch 1688
Epoch 1688 :: Batch 0/1
Batch Loss = 3.0929012184327846
1688, epoch_train_loss=3.0929012184327846
Epoch 1689
Epoch 1689 :: Batch 0/1
Batch Loss = 3.0928321730746315
1689, epoch_train_loss=3.0928321730746315
Epoch 1690
Epoch 1690 :: Batch 0/1
Batch Loss = 3.092763009173018
1690, epoch_train_loss=3.092763009173018
Epoch 1691
Epoch 1691 :: Batch 0/1
Batch Loss = 3.0926937268637986
1691, epoch_train_loss=3.0926937268637986
Epoch 1692
Epoch 1692 :: Batch 0/1
Batch Loss = 3.0926243260385964
1692, epoch_train_loss=3.0926243260385964
Epoch 1693
Epoch 1693 :: Batch 0/1
Batch Loss = 3.0925548063681827
1693, epoch_train_loss=3.0925548063681827
Epoch 1694
Epoch 1694 :: Batch 0/1
Batch Loss = 3.0924851672686327
1694, epoch_train_loss=3.0924851672686327
Epoch 1695
Epoch 1695 :: Batch 0/1
Batch Loss = 3.0924154079454333
1695, epoch_train_loss=3.0924154079454333
Epoch 1696
Epoch 1696 :: Batch 0/1
Batch Loss = 3.092345527370533
1696, epoch_train_loss=3.092345527370533
Epoch 1697
Epoch 1697 :: Batch 0/1
Batch Loss = 3.092275524291294
1697, epoch_train_loss=3.092275524291294
Epoch 1698
Epoch 1698 :: Batch 0/1
Batch Loss = 3.092205397239239
1698, epoch_train_loss=3.092205397239239
Epoch 1699
Epoch 1699 :: Batch 0/1
Batch Loss = 3.0921351445299354
1699, epoch_train_loss=3.0921351445299354
Epoch 1700
Epoch 1700 :: Batch 0/1
Batch Loss = 3.092064764283726
1700, epoch_train_loss=3.092064764283726
Epoch 1701
Epoch 1701 :: Batch 0/1
Batch Loss = 3.0919942544083128
1701, epoch_train_loss=3.0919942544083128
Epoch 1702
Epoch 1702 :: Batch 0/1
Batch Loss = 3.0919236126245675
1702, epoch_train_loss=3.0919236126245675
Epoch 1703
Epoch 1703 :: Batch 0/1
Batch Loss = 3.0918528364553084
1703, epoch_train_loss=3.0918528364553084
Epoch 1704
Epoch 1704 :: Batch 0/1
Batch Loss = 3.091781923232774
1704, epoch_train_loss=3.091781923232774
Epoch 1705
Epoch 1705 :: Batch 0/1
Batch Loss = 3.0917108701192864
1705, epoch_train_loss=3.0917108701192864
Epoch 1706
Epoch 1706 :: Batch 0/1
Batch Loss = 3.0916396740896808
1706, epoch_train_loss=3.0916396740896808
Epoch 1707
Epoch 1707 :: Batch 0/1
Batch Loss = 3.091568331961814
1707, epoch_train_loss=3.091568331961814
Epoch 1708
Epoch 1708 :: Batch 0/1
Batch Loss = 3.091496840384809
1708, epoch_train_loss=3.091496840384809
Epoch 1709
Epoch 1709 :: Batch 0/1
Batch Loss = 3.0914251958440695
1709, epoch_train_loss=3.0914251958440695
Epoch 1710
Epoch 1710 :: Batch 0/1
Batch Loss = 3.0913533946842415
1710, epoch_train_loss=3.0913533946842415
Epoch 1711
Epoch 1711 :: Batch 0/1
Batch Loss = 3.0912814330906633
1711, epoch_train_loss=3.0912814330906633
Epoch 1712
Epoch 1712 :: Batch 0/1
Batch Loss = 3.0912093071126914
1712, epoch_train_loss=3.0912093071126914
Epoch 1713
Epoch 1713 :: Batch 0/1
Batch Loss = 3.0911370126701874
1713, epoch_train_loss=3.0911370126701874
Epoch 1714
Epoch 1714 :: Batch 0/1
Batch Loss = 3.0910645455399846
1714, epoch_train_loss=3.0910645455399846
Epoch 1715
Epoch 1715 :: Batch 0/1
Batch Loss = 3.0909919013783296
1715, epoch_train_loss=3.0909919013783296
Epoch 1716
Epoch 1716 :: Batch 0/1
Batch Loss = 3.0909190757263727
1716, epoch_train_loss=3.0909190757263727
Epoch 1717
Epoch 1717 :: Batch 0/1
Batch Loss = 3.090846063996595
1717, epoch_train_loss=3.090846063996595
Epoch 1718
Epoch 1718 :: Batch 0/1
Batch Loss = 3.090772861494961
1718, epoch_train_loss=3.090772861494961
Epoch 1719
Epoch 1719 :: Batch 0/1
Batch Loss = 3.0906994634249796
1719, epoch_train_loss=3.0906994634249796
Epoch 1720
Epoch 1720 :: Batch 0/1
Batch Loss = 3.090625864873308
1720, epoch_train_loss=3.090625864873308
Epoch 1721
Epoch 1721 :: Batch 0/1
Batch Loss = 3.090552060831554
1721, epoch_train_loss=3.090552060831554
Epoch 1722
Epoch 1722 :: Batch 0/1
Batch Loss = 3.0904780461932777
1722, epoch_train_loss=3.0904780461932777
Epoch 1723
Epoch 1723 :: Batch 0/1
Batch Loss = 3.0904038157623086
1723, epoch_train_loss=3.0904038157623086
Epoch 1724
Epoch 1724 :: Batch 0/1
Batch Loss = 3.090329364232234
1724, epoch_train_loss=3.090329364232234
Epoch 1725
Epoch 1725 :: Batch 0/1
Batch Loss = 3.0902546862298927
1725, epoch_train_loss=3.0902546862298927
Epoch 1726
Epoch 1726 :: Batch 0/1
Batch Loss = 3.0901797762753143
1726, epoch_train_loss=3.0901797762753143
Epoch 1727
Epoch 1727 :: Batch 0/1
Batch Loss = 3.090104628807071
1727, epoch_train_loss=3.090104628807071
Epoch 1728
Epoch 1728 :: Batch 0/1
Batch Loss = 3.090029238176854
1728, epoch_train_loss=3.090029238176854
Epoch 1729
Epoch 1729 :: Batch 0/1
Batch Loss = 3.0899535986504
1729, epoch_train_loss=3.0899535986504
Epoch 1730
Epoch 1730 :: Batch 0/1
Batch Loss = 3.0898777044076025
1730, epoch_train_loss=3.0898777044076025
Epoch 1731
Epoch 1731 :: Batch 0/1
Batch Loss = 3.089801549541932
1731, epoch_train_loss=3.089801549541932
Epoch 1732
Epoch 1732 :: Batch 0/1
Batch Loss = 3.089725128060386
1732, epoch_train_loss=3.089725128060386
Epoch 1733
Epoch 1733 :: Batch 0/1
Batch Loss = 3.0896484338824695
1733, epoch_train_loss=3.0896484338824695
Epoch 1734
Epoch 1734 :: Batch 0/1
Batch Loss = 3.0895714608386395
1734, epoch_train_loss=3.0895714608386395
Epoch 1735
Epoch 1735 :: Batch 0/1
Batch Loss = 3.0894942026684675
1735, epoch_train_loss=3.0894942026684675
Epoch 1736
Epoch 1736 :: Batch 0/1
Batch Loss = 3.089416653012193
1736, epoch_train_loss=3.089416653012193
Epoch 1737
Epoch 1737 :: Batch 0/1
Batch Loss = 3.089338805427614
1737, epoch_train_loss=3.089338805427614
Epoch 1738
Epoch 1738 :: Batch 0/1
Batch Loss = 3.089260653369047
1738, epoch_train_loss=3.089260653369047
Epoch 1739
Epoch 1739 :: Batch 0/1
Batch Loss = 3.089182190190491
1739, epoch_train_loss=3.089182190190491
Epoch 1740
Epoch 1740 :: Batch 0/1
Batch Loss = 3.0891034091427594
1740, epoch_train_loss=3.0891034091427594
Epoch 1741
Epoch 1741 :: Batch 0/1
Batch Loss = 3.0890243033640994
1741, epoch_train_loss=3.0890243033640994
Epoch 1742
Epoch 1742 :: Batch 0/1
Batch Loss = 3.0889448658962806
1742, epoch_train_loss=3.0889448658962806
Epoch 1743
Epoch 1743 :: Batch 0/1
Batch Loss = 3.0888650896624146
1743, epoch_train_loss=3.0888650896624146
Epoch 1744
Epoch 1744 :: Batch 0/1
Batch Loss = 3.0887849674702865
1744, epoch_train_loss=3.0887849674702865
Epoch 1745
Epoch 1745 :: Batch 0/1
Batch Loss = 3.0887044920035693
1745, epoch_train_loss=3.0887044920035693
Epoch 1746
Epoch 1746 :: Batch 0/1
Batch Loss = 3.088623655838906
1746, epoch_train_loss=3.088623655838906
Epoch 1747
Epoch 1747 :: Batch 0/1
Batch Loss = 3.0885424514178825
1747, epoch_train_loss=3.0885424514178825
Epoch 1748
Epoch 1748 :: Batch 0/1
Batch Loss = 3.088460871070996
1748, epoch_train_loss=3.088460871070996
Epoch 1749
Epoch 1749 :: Batch 0/1
Batch Loss = 3.0883789069907617
1749, epoch_train_loss=3.0883789069907617
Epoch 1750
Epoch 1750 :: Batch 0/1
Batch Loss = 3.088296551258034
1750, epoch_train_loss=3.088296551258034
Epoch 1751
Epoch 1751 :: Batch 0/1
Batch Loss = 3.08821379582315
1751, epoch_train_loss=3.08821379582315
Epoch 1752
Epoch 1752 :: Batch 0/1
Batch Loss = 3.0881306325070836
1752, epoch_train_loss=3.0881306325070836
Epoch 1753
Epoch 1753 :: Batch 0/1
Batch Loss = 3.088047053024044
1753, epoch_train_loss=3.088047053024044
Epoch 1754
Epoch 1754 :: Batch 0/1
Batch Loss = 3.08796304895879
1754, epoch_train_loss=3.08796304895879
Epoch 1755
Epoch 1755 :: Batch 0/1
Batch Loss = 3.0878786117919064
1755, epoch_train_loss=3.0878786117919064
Epoch 1756
Epoch 1756 :: Batch 0/1
Batch Loss = 3.087793732899541
1756, epoch_train_loss=3.087793732899541
Epoch 1757
Epoch 1757 :: Batch 0/1
Batch Loss = 3.0877084035684255
1757, epoch_train_loss=3.0877084035684255
Epoch 1758
Epoch 1758 :: Batch 0/1
Batch Loss = 3.0876226149856008
1758, epoch_train_loss=3.0876226149856008
Epoch 1759
Epoch 1759 :: Batch 0/1
Batch Loss = 3.087536358277181
1759, epoch_train_loss=3.087536358277181
Epoch 1760
Epoch 1760 :: Batch 0/1
Batch Loss = 3.0874496244938134
1760, epoch_train_loss=3.0874496244938134
Epoch 1761
Epoch 1761 :: Batch 0/1
Batch Loss = 3.087362404652723
1761, epoch_train_loss=3.087362404652723
Epoch 1762
Epoch 1762 :: Batch 0/1
Batch Loss = 3.0872746897271894
1762, epoch_train_loss=3.0872746897271894
Epoch 1763
Epoch 1763 :: Batch 0/1
Batch Loss = 3.087186470685627
1763, epoch_train_loss=3.087186470685627
Epoch 1764
Epoch 1764 :: Batch 0/1
Batch Loss = 3.0870977385050407
1764, epoch_train_loss=3.0870977385050407
Epoch 1765
Epoch 1765 :: Batch 0/1
Batch Loss = 3.0870084841983876
1765, epoch_train_loss=3.0870084841983876
Epoch 1766
Epoch 1766 :: Batch 0/1
Batch Loss = 3.086918698817072
1766, epoch_train_loss=3.086918698817072
Epoch 1767
Epoch 1767 :: Batch 0/1
Batch Loss = 3.0868283734944253
1767, epoch_train_loss=3.0868283734944253
Epoch 1768
Epoch 1768 :: Batch 0/1
Batch Loss = 3.08673749946596
1768, epoch_train_loss=3.08673749946596
Epoch 1769
Epoch 1769 :: Batch 0/1
Batch Loss = 3.086646068089913
1769, epoch_train_loss=3.086646068089913
Epoch 1770
Epoch 1770 :: Batch 0/1
Batch Loss = 3.086554070879766
1770, epoch_train_loss=3.086554070879766
Epoch 1771
Epoch 1771 :: Batch 0/1
Batch Loss = 3.0864614995174673
1771, epoch_train_loss=3.0864614995174673
Epoch 1772
Epoch 1772 :: Batch 0/1
Batch Loss = 3.0863683458978413
1772, epoch_train_loss=3.0863683458978413
Epoch 1773
Epoch 1773 :: Batch 0/1
Batch Loss = 3.08627460211616
1773, epoch_train_loss=3.08627460211616
Epoch 1774
Epoch 1774 :: Batch 0/1
Batch Loss = 3.0861802605533946
1774, epoch_train_loss=3.0861802605533946
Epoch 1775
Epoch 1775 :: Batch 0/1
Batch Loss = 3.086085313781287
1775, epoch_train_loss=3.086085313781287
Epoch 1776
Epoch 1776 :: Batch 0/1
Batch Loss = 3.0859897547960573
1776, epoch_train_loss=3.0859897547960573
Epoch 1777
Epoch 1777 :: Batch 0/1
Batch Loss = 3.0858935766313422
1777, epoch_train_loss=3.0858935766313422
Epoch 1778
Epoch 1778 :: Batch 0/1
Batch Loss = 3.085796773195921
1778, epoch_train_loss=3.085796773195921
Epoch 1779
Epoch 1779 :: Batch 0/1
Batch Loss = 3.085699337860188
1779, epoch_train_loss=3.085699337860188
Epoch 1780
Epoch 1780 :: Batch 0/1
Batch Loss = 3.0856012672836193
1780, epoch_train_loss=3.0856012672836193
Epoch 1781
Epoch 1781 :: Batch 0/1
Batch Loss = 3.085502557979316
1781, epoch_train_loss=3.085502557979316
Epoch 1782
Epoch 1782 :: Batch 0/1
Batch Loss = 3.0854032346467894
1782, epoch_train_loss=3.0854032346467894
Epoch 1783
Epoch 1783 :: Batch 0/1
Batch Loss = 3.085303394666425
1783, epoch_train_loss=3.085303394666425
Epoch 1784
Epoch 1784 :: Batch 0/1
Batch Loss = 3.0852036523294655
1784, epoch_train_loss=3.0852036523294655
Epoch 1785
Epoch 1785 :: Batch 0/1
Batch Loss = 3.0851070395474642
1785, epoch_train_loss=3.0851070395474642
Epoch 1786
Epoch 1786 :: Batch 0/1
Batch Loss = 3.0850307984589653
1786, epoch_train_loss=3.0850307984589653
Epoch 1787
Epoch 1787 :: Batch 0/1
Batch Loss = 3.085070604833515
1787, epoch_train_loss=3.085070604833515
Epoch 1788
Epoch 1788 :: Batch 0/1
Batch Loss = 3.08580369426206
1788, epoch_train_loss=3.08580369426206
Epoch 1789
Epoch 1789 :: Batch 0/1
Batch Loss = 3.0903838550602667
1789, epoch_train_loss=3.0903838550602667
Epoch 1790
Epoch 1790 :: Batch 0/1
Batch Loss = 3.11619176730683
1790, epoch_train_loss=3.11619176730683
Epoch 1791
Epoch 1791 :: Batch 0/1
Batch Loss = 3.1830793904659442
1791, epoch_train_loss=3.1830793904659442
Epoch 1792
Epoch 1792 :: Batch 0/1
Batch Loss = 3.2577304120410284
1792, epoch_train_loss=3.2577304120410284
Epoch 1793
Epoch 1793 :: Batch 0/1
Batch Loss = 3.1457267859322684
1793, epoch_train_loss=3.1457267859322684
Epoch 1794
Epoch 1794 :: Batch 0/1
Batch Loss = 3.1086210349654873
1794, epoch_train_loss=3.1086210349654873
Epoch 1795
Epoch 1795 :: Batch 0/1
Batch Loss = 3.1725311912872556
1795, epoch_train_loss=3.1725311912872556
Epoch 1796
Epoch 1796 :: Batch 0/1
Batch Loss = 3.1048850910927976
1796, epoch_train_loss=3.1048850910927976
Epoch 1797
Epoch 1797 :: Batch 0/1
Batch Loss = 3.1078961153002864
1797, epoch_train_loss=3.1078961153002864
Epoch 1798
Epoch 1798 :: Batch 0/1
Batch Loss = 3.1436705817730943
1798, epoch_train_loss=3.1436705817730943
Epoch 1799
Epoch 1799 :: Batch 0/1
Batch Loss = 3.0902789650906173
1799, epoch_train_loss=3.0902789650906173
Epoch 1800
Epoch 1800 :: Batch 0/1
Batch Loss = 3.1285110081776195
1800, epoch_train_loss=3.1285110081776195
Epoch 1801
Epoch 1801 :: Batch 0/1
Batch Loss = 3.110505130376311
1801, epoch_train_loss=3.110505130376311
Epoch 1802
Epoch 1802 :: Batch 0/1
Batch Loss = 3.100785644412625
1802, epoch_train_loss=3.100785644412625
Epoch 1803
Epoch 1803 :: Batch 0/1
Batch Loss = 3.1188596295576363
1803, epoch_train_loss=3.1188596295576363
Epoch 1804
Epoch 1804 :: Batch 0/1
Batch Loss = 3.090685330063521
1804, epoch_train_loss=3.090685330063521
Epoch 1805
Epoch 1805 :: Batch 0/1
Batch Loss = 3.114910487138329
1805, epoch_train_loss=3.114910487138329
Epoch 1806
Epoch 1806 :: Batch 0/1
Batch Loss = 3.0909553347907646
1806, epoch_train_loss=3.0909553347907646
Epoch 1807
Epoch 1807 :: Batch 0/1
Batch Loss = 3.105068566147664
1807, epoch_train_loss=3.105068566147664
Epoch 1808
Epoch 1808 :: Batch 0/1
Batch Loss = 3.0970672454000825
1808, epoch_train_loss=3.0970672454000825
Epoch 1809
Epoch 1809 :: Batch 0/1
Batch Loss = 3.095771398396094
1809, epoch_train_loss=3.095771398396094
Epoch 1810
Epoch 1810 :: Batch 0/1
Batch Loss = 3.100342570446161
1810, epoch_train_loss=3.100342570446161
Epoch 1811
Epoch 1811 :: Batch 0/1
Batch Loss = 3.0888041244089823
1811, epoch_train_loss=3.0888041244089823
Epoch 1812
Epoch 1812 :: Batch 0/1
Batch Loss = 3.09951658273925
1812, epoch_train_loss=3.09951658273925
Epoch 1813
Epoch 1813 :: Batch 0/1
Batch Loss = 3.087990713796421
1813, epoch_train_loss=3.087990713796421
Epoch 1814
Epoch 1814 :: Batch 0/1
Batch Loss = 3.096204386949215
1814, epoch_train_loss=3.096204386949215
Epoch 1815
Epoch 1815 :: Batch 0/1
Batch Loss = 3.089945639253666
1815, epoch_train_loss=3.089945639253666
Epoch 1816
Epoch 1816 :: Batch 0/1
Batch Loss = 3.0903641946673037
1816, epoch_train_loss=3.0903641946673037
Epoch 1817
Epoch 1817 :: Batch 0/1
Batch Loss = 3.0923285897105517
1817, epoch_train_loss=3.0923285897105517
Epoch 1818
Epoch 1818 :: Batch 0/1
Batch Loss = 3.086439564583068
1818, epoch_train_loss=3.086439564583068
Epoch 1819
Epoch 1819 :: Batch 0/1
Batch Loss = 3.0920350651758466
1819, epoch_train_loss=3.0920350651758466
Epoch 1820
Epoch 1820 :: Batch 0/1
Batch Loss = 3.086196552818383
1820, epoch_train_loss=3.086196552818383
Epoch 1821
Epoch 1821 :: Batch 0/1
Batch Loss = 3.0892896134987025
1821, epoch_train_loss=3.0892896134987025
Epoch 1822
Epoch 1822 :: Batch 0/1
Batch Loss = 3.0879157210529447
1822, epoch_train_loss=3.0879157210529447
Epoch 1823
Epoch 1823 :: Batch 0/1
Batch Loss = 3.086504723386561
1823, epoch_train_loss=3.086504723386561
Epoch 1824
Epoch 1824 :: Batch 0/1
Batch Loss = 3.088851794285329
1824, epoch_train_loss=3.088851794285329
Epoch 1825
Epoch 1825 :: Batch 0/1
Batch Loss = 3.085292274528261
1825, epoch_train_loss=3.085292274528261
Epoch 1826
Epoch 1826 :: Batch 0/1
Batch Loss = 3.0878371034313385
1826, epoch_train_loss=3.0878371034313385
Epoch 1827
Epoch 1827 :: Batch 0/1
Batch Loss = 3.086088386110379
1827, epoch_train_loss=3.086088386110379
Epoch 1828
Epoch 1828 :: Batch 0/1
Batch Loss = 3.086011163931027
1828, epoch_train_loss=3.086011163931027
Epoch 1829
Epoch 1829 :: Batch 0/1
Batch Loss = 3.0868901501748334
1829, epoch_train_loss=3.0868901501748334
Epoch 1830
Epoch 1830 :: Batch 0/1
Batch Loss = 3.08488190466427
1830, epoch_train_loss=3.08488190466427
Epoch 1831
Epoch 1831 :: Batch 0/1
Batch Loss = 3.086446264278327
1831, epoch_train_loss=3.086446264278327
Epoch 1832
Epoch 1832 :: Batch 0/1
Batch Loss = 3.085168367615068
1832, epoch_train_loss=3.085168367615068
Epoch 1833
Epoch 1833 :: Batch 0/1
Batch Loss = 3.0852477729619254
1833, epoch_train_loss=3.0852477729619254
Epoch 1834
Epoch 1834 :: Batch 0/1
Batch Loss = 3.0856765755255378
1834, epoch_train_loss=3.0856765755255378
Epoch 1835
Epoch 1835 :: Batch 0/1
Batch Loss = 3.0844586893642627
1835, epoch_train_loss=3.0844586893642627
Epoch 1836
Epoch 1836 :: Batch 0/1
Batch Loss = 3.0853405415331165
1836, epoch_train_loss=3.0853405415331165
Epoch 1837
Epoch 1837 :: Batch 0/1
Batch Loss = 3.0845977377481737
1837, epoch_train_loss=3.0845977377481737
Epoch 1838
Epoch 1838 :: Batch 0/1
Batch Loss = 3.084404333005136
1838, epoch_train_loss=3.084404333005136
Epoch 1839
Epoch 1839 :: Batch 0/1
Batch Loss = 3.0847716704182826
1839, epoch_train_loss=3.0847716704182826
Epoch 1840
Epoch 1840 :: Batch 0/1
Batch Loss = 3.0839454114259968
1840, epoch_train_loss=3.0839454114259968
Epoch 1841
Epoch 1841 :: Batch 0/1
Batch Loss = 3.0843165448298957
1841, epoch_train_loss=3.0843165448298957
Epoch 1842
Epoch 1842 :: Batch 0/1
Batch Loss = 3.0840255336397013
1842, epoch_train_loss=3.0840255336397013
Epoch 1843
Epoch 1843 :: Batch 0/1
Batch Loss = 3.083648988569664
1843, epoch_train_loss=3.083648988569664
Epoch 1844
Epoch 1844 :: Batch 0/1
Batch Loss = 3.0839479340870484
1844, epoch_train_loss=3.0839479340870484
Epoch 1845
Epoch 1845 :: Batch 0/1
Batch Loss = 3.083444823782402
1845, epoch_train_loss=3.083444823782402
Epoch 1846
Epoch 1846 :: Batch 0/1
Batch Loss = 3.0834402434672965
1846, epoch_train_loss=3.0834402434672965
Epoch 1847
Epoch 1847 :: Batch 0/1
Batch Loss = 3.0834634554844356
1847, epoch_train_loss=3.0834634554844356
Epoch 1848
Epoch 1848 :: Batch 0/1
Batch Loss = 3.083043067396274
1848, epoch_train_loss=3.083043067396274
Epoch 1849
Epoch 1849 :: Batch 0/1
Batch Loss = 3.0831740962296053
1849, epoch_train_loss=3.0831740962296053
Epoch 1850
Epoch 1850 :: Batch 0/1
Batch Loss = 3.0829756595394455
1850, epoch_train_loss=3.0829756595394455
Epoch 1851
Epoch 1851 :: Batch 0/1
Batch Loss = 3.082739415066937
1851, epoch_train_loss=3.082739415066937
Epoch 1852
Epoch 1852 :: Batch 0/1
Batch Loss = 3.082823064573505
1852, epoch_train_loss=3.082823064573505
Epoch 1853
Epoch 1853 :: Batch 0/1
Batch Loss = 3.082550080664909
1853, epoch_train_loss=3.082550080664909
Epoch 1854
Epoch 1854 :: Batch 0/1
Batch Loss = 3.0824509915773235
1854, epoch_train_loss=3.0824509915773235
Epoch 1855
Epoch 1855 :: Batch 0/1
Batch Loss = 3.082442826192595
1855, epoch_train_loss=3.082442826192595
Epoch 1856
Epoch 1856 :: Batch 0/1
Batch Loss = 3.08218446614605
1856, epoch_train_loss=3.08218446614605
Epoch 1857
Epoch 1857 :: Batch 0/1
Batch Loss = 3.082137254307232
1857, epoch_train_loss=3.082137254307232
Epoch 1858
Epoch 1858 :: Batch 0/1
Batch Loss = 3.0820592334698977
1858, epoch_train_loss=3.0820592334698977
Epoch 1859
Epoch 1859 :: Batch 0/1
Batch Loss = 3.08184262242735
1859, epoch_train_loss=3.08184262242735
Epoch 1860
Epoch 1860 :: Batch 0/1
Batch Loss = 3.081805077767445
1860, epoch_train_loss=3.081805077767445
Epoch 1861
Epoch 1861 :: Batch 0/1
Batch Loss = 3.081683253684231
1861, epoch_train_loss=3.081683253684231
Epoch 1862
Epoch 1862 :: Batch 0/1
Batch Loss = 3.08150581123099
1862, epoch_train_loss=3.08150581123099
Epoch 1863
Epoch 1863 :: Batch 0/1
Batch Loss = 3.0814509327388158
1863, epoch_train_loss=3.0814509327388158
Epoch 1864
Epoch 1864 :: Batch 0/1
Batch Loss = 3.0813172995641533
1864, epoch_train_loss=3.0813172995641533
Epoch 1865
Epoch 1865 :: Batch 0/1
Batch Loss = 3.0811581363458997
1865, epoch_train_loss=3.0811581363458997
Epoch 1866
Epoch 1866 :: Batch 0/1
Batch Loss = 3.081086317125498
1866, epoch_train_loss=3.081086317125498
Epoch 1867
Epoch 1867 :: Batch 0/1
Batch Loss = 3.0809551674537934
1867, epoch_train_loss=3.0809551674537934
Epoch 1868
Epoch 1868 :: Batch 0/1
Batch Loss = 3.0808036039853106
1868, epoch_train_loss=3.0808036039853106
Epoch 1869
Epoch 1869 :: Batch 0/1
Batch Loss = 3.0807189978896594
1869, epoch_train_loss=3.0807189978896594
Epoch 1870
Epoch 1870 :: Batch 0/1
Batch Loss = 3.0805899340100846
1870, epoch_train_loss=3.0805899340100846
Epoch 1871
Epoch 1871 :: Batch 0/1
Batch Loss = 3.080442269042331
1871, epoch_train_loss=3.080442269042331
Epoch 1872
Epoch 1872 :: Batch 0/1
Batch Loss = 3.080344736334757
1872, epoch_train_loss=3.080344736334757
Epoch 1873
Epoch 1873 :: Batch 0/1
Batch Loss = 3.0802203709014684
1873, epoch_train_loss=3.0802203709014684
Epoch 1874
Epoch 1874 :: Batch 0/1
Batch Loss = 3.080075158505702
1874, epoch_train_loss=3.080075158505702
Epoch 1875
Epoch 1875 :: Batch 0/1
Batch Loss = 3.079964556819305
1875, epoch_train_loss=3.079964556819305
Epoch 1876
Epoch 1876 :: Batch 0/1
Batch Loss = 3.0798456615812486
1876, epoch_train_loss=3.0798456615812486
Epoch 1877
Epoch 1877 :: Batch 0/1
Batch Loss = 3.079702092657336
1877, epoch_train_loss=3.079702092657336
Epoch 1878
Epoch 1878 :: Batch 0/1
Batch Loss = 3.079578737465807
1878, epoch_train_loss=3.079578737465807
Epoch 1879
Epoch 1879 :: Batch 0/1
Batch Loss = 3.079461795268032
1879, epoch_train_loss=3.079461795268032
Epoch 1880
Epoch 1880 :: Batch 0/1
Batch Loss = 3.0793216741021996
1880, epoch_train_loss=3.0793216741021996
Epoch 1881
Epoch 1881 :: Batch 0/1
Batch Loss = 3.079188212194653
1881, epoch_train_loss=3.079188212194653
Epoch 1882
Epoch 1882 :: Batch 0/1
Batch Loss = 3.07906680966661
1882, epoch_train_loss=3.07906680966661
Epoch 1883
Epoch 1883 :: Batch 0/1
Batch Loss = 3.0789329102211784
1883, epoch_train_loss=3.0789329102211784
Epoch 1884
Epoch 1884 :: Batch 0/1
Batch Loss = 3.078791999748582
1884, epoch_train_loss=3.078791999748582
Epoch 1885
Epoch 1885 :: Batch 0/1
Batch Loss = 3.078662356085674
1885, epoch_train_loss=3.078662356085674
Epoch 1886
Epoch 1886 :: Batch 0/1
Batch Loss = 3.078531827752541
1886, epoch_train_loss=3.078531827752541
Epoch 1887
Epoch 1887 :: Batch 0/1
Batch Loss = 3.0783901064896018
1887, epoch_train_loss=3.0783901064896018
Epoch 1888
Epoch 1888 :: Batch 0/1
Batch Loss = 3.078250817347536
1888, epoch_train_loss=3.078250817347536
Epoch 1889
Epoch 1889 :: Batch 0/1
Batch Loss = 3.078117309322047
1889, epoch_train_loss=3.078117309322047
Epoch 1890
Epoch 1890 :: Batch 0/1
Batch Loss = 3.0779784670840775
1890, epoch_train_loss=3.0779784670840775
Epoch 1891
Epoch 1891 :: Batch 0/1
Batch Loss = 3.0778336523230116
1891, epoch_train_loss=3.0778336523230116
Epoch 1892
Epoch 1892 :: Batch 0/1
Batch Loss = 3.0776919917560845
1892, epoch_train_loss=3.0776919917560845
Epoch 1893
Epoch 1893 :: Batch 0/1
Batch Loss = 3.0775525477035073
1893, epoch_train_loss=3.0775525477035073
Epoch 1894
Epoch 1894 :: Batch 0/1
Batch Loss = 3.0774080758477265
1894, epoch_train_loss=3.0774080758477265
Epoch 1895
Epoch 1895 :: Batch 0/1
Batch Loss = 3.0772600919348507
1895, epoch_train_loss=3.0772600919348507
Epoch 1896
Epoch 1896 :: Batch 0/1
Batch Loss = 3.077113843616727
1896, epoch_train_loss=3.077113843616727
Epoch 1897
Epoch 1897 :: Batch 0/1
Batch Loss = 3.0769685440060988
1897, epoch_train_loss=3.0769685440060988
Epoch 1898
Epoch 1898 :: Batch 0/1
Batch Loss = 3.076819547588788
1898, epoch_train_loss=3.076819547588788
Epoch 1899
Epoch 1899 :: Batch 0/1
Batch Loss = 3.076667536955894
1899, epoch_train_loss=3.076667536955894
Epoch 1900
Epoch 1900 :: Batch 0/1
Batch Loss = 3.076515794224071
1900, epoch_train_loss=3.076515794224071
Epoch 1901
Epoch 1901 :: Batch 0/1
Batch Loss = 3.0763644652664595
1901, epoch_train_loss=3.0763644652664595
Epoch 1902
Epoch 1902 :: Batch 0/1
Batch Loss = 3.0762110371113276
1902, epoch_train_loss=3.0762110371113276
Epoch 1903
Epoch 1903 :: Batch 0/1
Batch Loss = 3.076054511620368
1903, epoch_train_loss=3.076054511620368
Epoch 1904
Epoch 1904 :: Batch 0/1
Batch Loss = 3.0758968363625137
1904, epoch_train_loss=3.0758968363625137
Epoch 1905
Epoch 1905 :: Batch 0/1
Batch Loss = 3.075739022726679
1905, epoch_train_loss=3.075739022726679
Epoch 1906
Epoch 1906 :: Batch 0/1
Batch Loss = 3.0755802674614987
1906, epoch_train_loss=3.0755802674614987
Epoch 1907
Epoch 1907 :: Batch 0/1
Batch Loss = 3.0754191898791032
1907, epoch_train_loss=3.0754191898791032
Epoch 1908
Epoch 1908 :: Batch 0/1
Batch Loss = 3.0752558781496258
1908, epoch_train_loss=3.0752558781496258
Epoch 1909
Epoch 1909 :: Batch 0/1
Batch Loss = 3.075091258361707
1909, epoch_train_loss=3.075091258361707
Epoch 1910
Epoch 1910 :: Batch 0/1
Batch Loss = 3.0749258137364084
1910, epoch_train_loss=3.0749258137364084
Epoch 1911
Epoch 1911 :: Batch 0/1
Batch Loss = 3.0747590844252404
1911, epoch_train_loss=3.0747590844252404
Epoch 1912
Epoch 1912 :: Batch 0/1
Batch Loss = 3.074590449149475
1912, epoch_train_loss=3.074590449149475
Epoch 1913
Epoch 1913 :: Batch 0/1
Batch Loss = 3.0744196749969386
1913, epoch_train_loss=3.0744196749969386
Epoch 1914
Epoch 1914 :: Batch 0/1
Batch Loss = 3.074247066440819
1914, epoch_train_loss=3.074247066440819
Epoch 1915
Epoch 1915 :: Batch 0/1
Batch Loss = 3.0740729772864794
1915, epoch_train_loss=3.0740729772864794
Epoch 1916
Epoch 1916 :: Batch 0/1
Batch Loss = 3.0738974679202933
1916, epoch_train_loss=3.0738974679202933
Epoch 1917
Epoch 1917 :: Batch 0/1
Batch Loss = 3.0737203538829867
1917, epoch_train_loss=3.0737203538829867
Epoch 1918
Epoch 1918 :: Batch 0/1
Batch Loss = 3.0735412909172353
1918, epoch_train_loss=3.0735412909172353
Epoch 1919
Epoch 1919 :: Batch 0/1
Batch Loss = 3.0733601957457073
1919, epoch_train_loss=3.0733601957457073
Epoch 1920
Epoch 1920 :: Batch 0/1
Batch Loss = 3.073177008749366
1920, epoch_train_loss=3.073177008749366
Epoch 1921
Epoch 1921 :: Batch 0/1
Batch Loss = 3.072991844777186
1921, epoch_train_loss=3.072991844777186
Epoch 1922
Epoch 1922 :: Batch 0/1
Batch Loss = 3.0728047036890587
1922, epoch_train_loss=3.0728047036890587
Epoch 1923
Epoch 1923 :: Batch 0/1
Batch Loss = 3.072615631818127
1923, epoch_train_loss=3.072615631818127
Epoch 1924
Epoch 1924 :: Batch 0/1
Batch Loss = 3.072424559044346
1924, epoch_train_loss=3.072424559044346
Epoch 1925
Epoch 1925 :: Batch 0/1
Batch Loss = 3.072231417484071
1925, epoch_train_loss=3.072231417484071
Epoch 1926
Epoch 1926 :: Batch 0/1
Batch Loss = 3.0720361247121737
1926, epoch_train_loss=3.0720361247121737
Epoch 1927
Epoch 1927 :: Batch 0/1
Batch Loss = 3.07183861451542
1927, epoch_train_loss=3.07183861451542
Epoch 1928
Epoch 1928 :: Batch 0/1
Batch Loss = 3.071638863128338
1928, epoch_train_loss=3.071638863128338
Epoch 1929
Epoch 1929 :: Batch 0/1
Batch Loss = 3.0714368109832955
1929, epoch_train_loss=3.0714368109832955
Epoch 1930
Epoch 1930 :: Batch 0/1
Batch Loss = 3.0712324981239885
1930, epoch_train_loss=3.0712324981239885
Epoch 1931
Epoch 1931 :: Batch 0/1
Batch Loss = 3.071025955826087
1931, epoch_train_loss=3.071025955826087
Epoch 1932
Epoch 1932 :: Batch 0/1
Batch Loss = 3.0708174032047086
1932, epoch_train_loss=3.0708174032047086
Epoch 1933
Epoch 1933 :: Batch 0/1
Batch Loss = 3.0706072694700155
1933, epoch_train_loss=3.0706072694700155
Epoch 1934
Epoch 1934 :: Batch 0/1
Batch Loss = 3.070396649105433
1934, epoch_train_loss=3.070396649105433
Epoch 1935
Epoch 1935 :: Batch 0/1
Batch Loss = 3.070188155572278
1935, epoch_train_loss=3.070188155572278
Epoch 1936
Epoch 1936 :: Batch 0/1
Batch Loss = 3.069987972523958
1936, epoch_train_loss=3.069987972523958
Epoch 1937
Epoch 1937 :: Batch 0/1
Batch Loss = 3.069812360198827
1937, epoch_train_loss=3.069812360198827
Epoch 1938
Epoch 1938 :: Batch 0/1
Batch Loss = 3.0697016497970977
1938, epoch_train_loss=3.0697016497970977
Epoch 1939
Epoch 1939 :: Batch 0/1
Batch Loss = 3.0697718622477113
1939, epoch_train_loss=3.0697718622477113
Epoch 1940
Epoch 1940 :: Batch 0/1
Batch Loss = 3.0703238000850654
1940, epoch_train_loss=3.0703238000850654
Epoch 1941
Epoch 1941 :: Batch 0/1
Batch Loss = 3.0723134424085905
1941, epoch_train_loss=3.0723134424085905
Epoch 1942
Epoch 1942 :: Batch 0/1
Batch Loss = 3.0781077904237333
1942, epoch_train_loss=3.0781077904237333
Epoch 1943
Epoch 1943 :: Batch 0/1
Batch Loss = 3.095836244939712
1943, epoch_train_loss=3.095836244939712
Epoch 1944
Epoch 1944 :: Batch 0/1
Batch Loss = 3.1353701681208954
1944, epoch_train_loss=3.1353701681208954
Epoch 1945
Epoch 1945 :: Batch 0/1
Batch Loss = 3.2238863118505243
1945, epoch_train_loss=3.2238863118505243
Epoch 1946
Epoch 1946 :: Batch 0/1
Batch Loss = 3.2124733086491513
1946, epoch_train_loss=3.2124733086491513
Epoch 1947
Epoch 1947 :: Batch 0/1
Batch Loss = 3.1378521428443404
1947, epoch_train_loss=3.1378521428443404
Epoch 1948
Epoch 1948 :: Batch 0/1
Batch Loss = 3.077379943470875
1948, epoch_train_loss=3.077379943470875
Epoch 1949
Epoch 1949 :: Batch 0/1
Batch Loss = 3.16778485740647
1949, epoch_train_loss=3.16778485740647
Epoch 1950
Epoch 1950 :: Batch 0/1
Batch Loss = 3.200970758942364
1950, epoch_train_loss=3.200970758942364
Epoch 1951
Epoch 1951 :: Batch 0/1
Batch Loss = 3.093042466994525
1951, epoch_train_loss=3.093042466994525
Epoch 1952
Epoch 1952 :: Batch 0/1
Batch Loss = 3.2171867105220073
1952, epoch_train_loss=3.2171867105220073
Epoch 1953
Epoch 1953 :: Batch 0/1
Batch Loss = 3.162532729933082
1953, epoch_train_loss=3.162532729933082
Epoch 1954
Epoch 1954 :: Batch 0/1
Batch Loss = 3.118950126574144
1954, epoch_train_loss=3.118950126574144
Epoch 1955
Epoch 1955 :: Batch 0/1
Batch Loss = 3.170572746341841
1955, epoch_train_loss=3.170572746341841
Epoch 1956
Epoch 1956 :: Batch 0/1
Batch Loss = 3.0970549431586805
1956, epoch_train_loss=3.0970549431586805
Epoch 1957
Epoch 1957 :: Batch 0/1
Batch Loss = 3.1543528746217118
1957, epoch_train_loss=3.1543528746217118
Epoch 1958
Epoch 1958 :: Batch 0/1
Batch Loss = 3.0855685354542555
1958, epoch_train_loss=3.0855685354542555
Epoch 1959
Epoch 1959 :: Batch 0/1
Batch Loss = 3.1439588612745437
1959, epoch_train_loss=3.1439588612745437
Epoch 1960
Epoch 1960 :: Batch 0/1
Batch Loss = 3.086383238269157
1960, epoch_train_loss=3.086383238269157
Epoch 1961
Epoch 1961 :: Batch 0/1
Batch Loss = 3.1164362175081384
1961, epoch_train_loss=3.1164362175081384
Epoch 1962
Epoch 1962 :: Batch 0/1
Batch Loss = 3.092831464268301
1962, epoch_train_loss=3.092831464268301
Epoch 1963
Epoch 1963 :: Batch 0/1
Batch Loss = 3.0916099087574667
1963, epoch_train_loss=3.0916099087574667
Epoch 1964
Epoch 1964 :: Batch 0/1
Batch Loss = 3.1072249568562866
1964, epoch_train_loss=3.1072249568562866
Epoch 1965
Epoch 1965 :: Batch 0/1
Batch Loss = 3.0857291969809366
1965, epoch_train_loss=3.0857291969809366
Epoch 1966
Epoch 1966 :: Batch 0/1
Batch Loss = 3.1007550733747093
1966, epoch_train_loss=3.1007550733747093
Epoch 1967
Epoch 1967 :: Batch 0/1
Batch Loss = 3.0837938082325325
1967, epoch_train_loss=3.0837938082325325
Epoch 1968
Epoch 1968 :: Batch 0/1
Batch Loss = 3.082854910677022
1968, epoch_train_loss=3.082854910677022
Epoch 1969
Epoch 1969 :: Batch 0/1
Batch Loss = 3.0876101484159877
1969, epoch_train_loss=3.0876101484159877
Epoch 1970
Epoch 1970 :: Batch 0/1
Batch Loss = 3.0758309420990133
1970, epoch_train_loss=3.0758309420990133
Epoch 1971
Epoch 1971 :: Batch 0/1
Batch Loss = 3.0845756568121816
1971, epoch_train_loss=3.0845756568121816
Epoch 1972
Epoch 1972 :: Batch 0/1
Batch Loss = 3.075558069489972
1972, epoch_train_loss=3.075558069489972
Epoch 1973
Epoch 1973 :: Batch 0/1
Batch Loss = 3.076477295284232
1973, epoch_train_loss=3.076477295284232
Epoch 1974
Epoch 1974 :: Batch 0/1
Batch Loss = 3.0800519937591213
1974, epoch_train_loss=3.0800519937591213
Epoch 1975
Epoch 1975 :: Batch 0/1
Batch Loss = 3.0737307888900234
1975, epoch_train_loss=3.0737307888900234
Epoch 1976
Epoch 1976 :: Batch 0/1
Batch Loss = 3.0770670997454035
1976, epoch_train_loss=3.0770670997454035
Epoch 1977
Epoch 1977 :: Batch 0/1
Batch Loss = 3.0717701841113487
1977, epoch_train_loss=3.0717701841113487
Epoch 1978
Epoch 1978 :: Batch 0/1
Batch Loss = 3.0725993462612955
1978, epoch_train_loss=3.0725993462612955
Epoch 1979
Epoch 1979 :: Batch 0/1
Batch Loss = 3.0727746772644027
1979, epoch_train_loss=3.0727746772644027
Epoch 1980
Epoch 1980 :: Batch 0/1
Batch Loss = 3.0703130647087407
1980, epoch_train_loss=3.0703130647087407
Epoch 1981
Epoch 1981 :: Batch 0/1
Batch Loss = 3.0731195516879595
1981, epoch_train_loss=3.0731195516879595
Epoch 1982
Epoch 1982 :: Batch 0/1
Batch Loss = 3.06805755706562
1982, epoch_train_loss=3.06805755706562
Epoch 1983
Epoch 1983 :: Batch 0/1
Batch Loss = 3.0709730918642038
1983, epoch_train_loss=3.0709730918642038
Epoch 1984
Epoch 1984 :: Batch 0/1
Batch Loss = 3.0696700104485326
1984, epoch_train_loss=3.0696700104485326
Epoch 1985
Epoch 1985 :: Batch 0/1
Batch Loss = 3.067593923086518
1985, epoch_train_loss=3.067593923086518
Epoch 1986
Epoch 1986 :: Batch 0/1
Batch Loss = 3.0704681884240816
1986, epoch_train_loss=3.0704681884240816
Epoch 1987
Epoch 1987 :: Batch 0/1
Batch Loss = 3.0658069914326056
1987, epoch_train_loss=3.0658069914326056
Epoch 1988
Epoch 1988 :: Batch 0/1
Batch Loss = 3.0677982044575414
1988, epoch_train_loss=3.0677982044575414
Epoch 1989
Epoch 1989 :: Batch 0/1
Batch Loss = 3.0667330013108547
1989, epoch_train_loss=3.0667330013108547
Epoch 1990
Epoch 1990 :: Batch 0/1
Batch Loss = 3.0650556568115213
1990, epoch_train_loss=3.0650556568115213
Epoch 1991
Epoch 1991 :: Batch 0/1
Batch Loss = 3.066516674111139
1991, epoch_train_loss=3.066516674111139
Epoch 1992
Epoch 1992 :: Batch 0/1
Batch Loss = 3.0645938338859047
1992, epoch_train_loss=3.0645938338859047
Epoch 1993
Epoch 1993 :: Batch 0/1
Batch Loss = 3.0648047505867764
1993, epoch_train_loss=3.0648047505867764
Epoch 1994
Epoch 1994 :: Batch 0/1
Batch Loss = 3.0643187440591046
1994, epoch_train_loss=3.0643187440591046
Epoch 1995
Epoch 1995 :: Batch 0/1
Batch Loss = 3.063876879986393
1995, epoch_train_loss=3.063876879986393
Epoch 1996
Epoch 1996 :: Batch 0/1
Batch Loss = 3.0631719028023716
1996, epoch_train_loss=3.0631719028023716
Epoch 1997
Epoch 1997 :: Batch 0/1
Batch Loss = 3.0632339126330574
1997, epoch_train_loss=3.0632339126330574
Epoch 1998
Epoch 1998 :: Batch 0/1
Batch Loss = 3.062503648905848
1998, epoch_train_loss=3.062503648905848
Epoch 1999
Epoch 1999 :: Batch 0/1
Batch Loss = 3.0618904549479193
1999, epoch_train_loss=3.0618904549479193
Epoch 2000
Epoch 2000 :: Batch 0/1
Batch Loss = 3.0622317012160956
2000, epoch_train_loss=3.0622317012160956
Epoch 2001
Epoch 2001 :: Batch 0/1
Batch Loss = 3.0609063423238805
2001, epoch_train_loss=3.0609063423238805
Epoch 2002
Epoch 2002 :: Batch 0/1
Batch Loss = 3.0609811025739044
2002, epoch_train_loss=3.0609811025739044
Epoch 2003
Epoch 2003 :: Batch 0/1
Batch Loss = 3.060554969483783
2003, epoch_train_loss=3.060554969483783
Epoch 2004
Epoch 2004 :: Batch 0/1
Batch Loss = 3.0600695193193754
2004, epoch_train_loss=3.0600695193193754
Epoch 2005
Epoch 2005 :: Batch 0/1
Batch Loss = 3.059565261677107
2005, epoch_train_loss=3.059565261677107
Epoch 2006
Epoch 2006 :: Batch 0/1
Batch Loss = 3.0594082974472614
2006, epoch_train_loss=3.0594082974472614
Epoch 2007
Epoch 2007 :: Batch 0/1
Batch Loss = 3.0589663158585467
2007, epoch_train_loss=3.0589663158585467
Epoch 2008
Epoch 2008 :: Batch 0/1
Batch Loss = 3.058285364631469
2008, epoch_train_loss=3.058285364631469
Epoch 2009
Epoch 2009 :: Batch 0/1
Batch Loss = 3.058233004506764
2009, epoch_train_loss=3.058233004506764
Epoch 2010
Epoch 2010 :: Batch 0/1
Batch Loss = 3.057676119082517
2010, epoch_train_loss=3.057676119082517
Epoch 2011
Epoch 2011 :: Batch 0/1
Batch Loss = 3.0572870641684147
2011, epoch_train_loss=3.0572870641684147
Epoch 2012
Epoch 2012 :: Batch 0/1
Batch Loss = 3.0566985887731604
2012, epoch_train_loss=3.0566985887731604
Epoch 2013
Epoch 2013 :: Batch 0/1
Batch Loss = 3.0565500372012187
2013, epoch_train_loss=3.0565500372012187
Epoch 2014
Epoch 2014 :: Batch 0/1
Batch Loss = 3.056071067930893
2014, epoch_train_loss=3.056071067930893
Epoch 2015
Epoch 2015 :: Batch 0/1
Batch Loss = 3.055596095745208
2015, epoch_train_loss=3.055596095745208
Epoch 2016
Epoch 2016 :: Batch 0/1
Batch Loss = 3.0551221453675126
2016, epoch_train_loss=3.0551221453675126
Epoch 2017
Epoch 2017 :: Batch 0/1
Batch Loss = 3.0547990023802507
2017, epoch_train_loss=3.0547990023802507
Epoch 2018
Epoch 2018 :: Batch 0/1
Batch Loss = 3.0544272759398683
2018, epoch_train_loss=3.0544272759398683
Epoch 2019
Epoch 2019 :: Batch 0/1
Batch Loss = 3.053923295315381
2019, epoch_train_loss=3.053923295315381
Epoch 2020
Epoch 2020 :: Batch 0/1
Batch Loss = 3.053443166130547
2020, epoch_train_loss=3.053443166130547
Epoch 2021
Epoch 2021 :: Batch 0/1
Batch Loss = 3.052973885829493
2021, epoch_train_loss=3.052973885829493
Epoch 2022
Epoch 2022 :: Batch 0/1
Batch Loss = 3.0526283549748
2022, epoch_train_loss=3.0526283549748
Epoch 2023
Epoch 2023 :: Batch 0/1
Batch Loss = 3.0521769958020504
2023, epoch_train_loss=3.0521769958020504
Epoch 2024
Epoch 2024 :: Batch 0/1
Batch Loss = 3.051744121216331
2024, epoch_train_loss=3.051744121216331
Epoch 2025
Epoch 2025 :: Batch 0/1
Batch Loss = 3.0512050645280806
2025, epoch_train_loss=3.0512050645280806
Epoch 2026
Epoch 2026 :: Batch 0/1
Batch Loss = 3.0507431366399436
2026, epoch_train_loss=3.0507431366399436
Epoch 2027
Epoch 2027 :: Batch 0/1
Batch Loss = 3.0502533759972454
2027, epoch_train_loss=3.0502533759972454
Epoch 2028
Epoch 2028 :: Batch 0/1
Batch Loss = 3.04983615789197
2028, epoch_train_loss=3.04983615789197
Epoch 2029
Epoch 2029 :: Batch 0/1
Batch Loss = 3.049377057989102
2029, epoch_train_loss=3.049377057989102
Epoch 2030
Epoch 2030 :: Batch 0/1
Batch Loss = 3.048943888829991
2030, epoch_train_loss=3.048943888829991
Epoch 2031
Epoch 2031 :: Batch 0/1
Batch Loss = 3.0484722476672745
2031, epoch_train_loss=3.0484722476672745
Epoch 2032
Epoch 2032 :: Batch 0/1
Batch Loss = 3.048005249488214
2032, epoch_train_loss=3.048005249488214
Epoch 2033
Epoch 2033 :: Batch 0/1
Batch Loss = 3.047532445489551
2033, epoch_train_loss=3.047532445489551
Epoch 2034
Epoch 2034 :: Batch 0/1
Batch Loss = 3.047072071934683
2034, epoch_train_loss=3.047072071934683
Epoch 2035
Epoch 2035 :: Batch 0/1
Batch Loss = 3.046682977528525
2035, epoch_train_loss=3.046682977528525
Epoch 2036
Epoch 2036 :: Batch 0/1
Batch Loss = 3.0463451358190126
2036, epoch_train_loss=3.0463451358190126
Epoch 2037
Epoch 2037 :: Batch 0/1
Batch Loss = 3.046329511822881
2037, epoch_train_loss=3.046329511822881
Epoch 2038
Epoch 2038 :: Batch 0/1
Batch Loss = 3.0467159945626605
2038, epoch_train_loss=3.0467159945626605
Epoch 2039
Epoch 2039 :: Batch 0/1
Batch Loss = 3.049141238973814
2039, epoch_train_loss=3.049141238973814
Epoch 2040
Epoch 2040 :: Batch 0/1
Batch Loss = 3.0530289683862737
2040, epoch_train_loss=3.0530289683862737
Epoch 2041
Epoch 2041 :: Batch 0/1
Batch Loss = 3.0702157066640194
2041, epoch_train_loss=3.0702157066640194
Epoch 2042
Epoch 2042 :: Batch 0/1
Batch Loss = 3.072105409593887
2042, epoch_train_loss=3.072105409593887
Epoch 2043
Epoch 2043 :: Batch 0/1
Batch Loss = 3.0998218980657084
2043, epoch_train_loss=3.0998218980657084
Epoch 2044
Epoch 2044 :: Batch 0/1
Batch Loss = 3.054908992314514
2044, epoch_train_loss=3.054908992314514
Epoch 2045
Epoch 2045 :: Batch 0/1
Batch Loss = 3.0512523968002387
2045, epoch_train_loss=3.0512523968002387
Epoch 2046
Epoch 2046 :: Batch 0/1
Batch Loss = 3.0867332473082856
2046, epoch_train_loss=3.0867332473082856
Epoch 2047
Epoch 2047 :: Batch 0/1
Batch Loss = 3.0852590898442895
2047, epoch_train_loss=3.0852590898442895
Epoch 2048
Epoch 2048 :: Batch 0/1
Batch Loss = 3.0857091121134697
2048, epoch_train_loss=3.0857091121134697
Epoch 2049
Epoch 2049 :: Batch 0/1
Batch Loss = 3.050613922047291
2049, epoch_train_loss=3.050613922047291
Epoch 2050
Epoch 2050 :: Batch 0/1
Batch Loss = 3.0438477882711825
2050, epoch_train_loss=3.0438477882711825
Epoch 2051
Epoch 2051 :: Batch 0/1
Batch Loss = 3.0525187300991248
2051, epoch_train_loss=3.0525187300991248
Epoch 2052
Epoch 2052 :: Batch 0/1
Batch Loss = 3.061692524134039
2052, epoch_train_loss=3.061692524134039
Epoch 2053
Epoch 2053 :: Batch 0/1
Batch Loss = 3.0909121923248786
2053, epoch_train_loss=3.0909121923248786
Epoch 2054
Epoch 2054 :: Batch 0/1
Batch Loss = 3.0575134216831947
2054, epoch_train_loss=3.0575134216831947
Epoch 2055
Epoch 2055 :: Batch 0/1
Batch Loss = 3.043412283591773
2055, epoch_train_loss=3.043412283591773
Epoch 2056
Epoch 2056 :: Batch 0/1
Batch Loss = 3.0392123930032877
2056, epoch_train_loss=3.0392123930032877
Epoch 2057
Epoch 2057 :: Batch 0/1
Batch Loss = 3.047836449352128
2057, epoch_train_loss=3.047836449352128
Epoch 2058
Epoch 2058 :: Batch 0/1
Batch Loss = 3.066730224763739
2058, epoch_train_loss=3.066730224763739
Epoch 2059
Epoch 2059 :: Batch 0/1
Batch Loss = 3.078242163601599
2059, epoch_train_loss=3.078242163601599
Epoch 2060
Epoch 2060 :: Batch 0/1
Batch Loss = 3.140918358183288
2060, epoch_train_loss=3.140918358183288
Epoch 2061
Epoch 2061 :: Batch 0/1
Batch Loss = 3.056499795113318
2061, epoch_train_loss=3.056499795113318
Epoch 2062
Epoch 2062 :: Batch 0/1
Batch Loss = 3.1694998604820515
2062, epoch_train_loss=3.1694998604820515
Epoch 2063
Epoch 2063 :: Batch 0/1
Batch Loss = 3.2090358050202537
2063, epoch_train_loss=3.2090358050202537
Epoch 2064
Epoch 2064 :: Batch 0/1
Batch Loss = 3.2164499637761743
2064, epoch_train_loss=3.2164499637761743
Epoch 2065
Epoch 2065 :: Batch 0/1
Batch Loss = 3.147700440288132
2065, epoch_train_loss=3.147700440288132
Epoch 2066
Epoch 2066 :: Batch 0/1
Batch Loss = 3.2105998396715756
2066, epoch_train_loss=3.2105998396715756
Epoch 2067
Epoch 2067 :: Batch 0/1
Batch Loss = 3.100759986046903
2067, epoch_train_loss=3.100759986046903
Epoch 2068
Epoch 2068 :: Batch 0/1
Batch Loss = 3.0951800261807234
2068, epoch_train_loss=3.0951800261807234
Epoch 2069
Epoch 2069 :: Batch 0/1
Batch Loss = 3.1053484621158085
2069, epoch_train_loss=3.1053484621158085
Epoch 2070
Epoch 2070 :: Batch 0/1
Batch Loss = 3.1430042097311413
2070, epoch_train_loss=3.1430042097311413
Epoch 2071
Epoch 2071 :: Batch 0/1
Batch Loss = 3.1642331376453234
2071, epoch_train_loss=3.1642331376453234
Epoch 2072
Epoch 2072 :: Batch 0/1
Batch Loss = 3.2846250759364146
2072, epoch_train_loss=3.2846250759364146
Epoch 2073
Epoch 2073 :: Batch 0/1
Batch Loss = 3.230916913296608
2073, epoch_train_loss=3.230916913296608
Epoch 2074
Epoch 2074 :: Batch 0/1
Batch Loss = 3.2565780939000466
2074, epoch_train_loss=3.2565780939000466
Epoch 2075
Epoch 2075 :: Batch 0/1
Batch Loss = 3.3104534935759666
2075, epoch_train_loss=3.3104534935759666
Epoch 2076
Epoch 2076 :: Batch 0/1
Batch Loss = 3.2619629403592514
2076, epoch_train_loss=3.2619629403592514
Epoch 2077
Epoch 2077 :: Batch 0/1
Batch Loss = 3.197189305797951
2077, epoch_train_loss=3.197189305797951
Epoch 2078
Epoch 2078 :: Batch 0/1
Batch Loss = 3.19968891183447
2078, epoch_train_loss=3.19968891183447
Epoch 2079
Epoch 2079 :: Batch 0/1
Batch Loss = 3.1912925072983644
2079, epoch_train_loss=3.1912925072983644
Epoch 2080
Epoch 2080 :: Batch 0/1
Batch Loss = 3.142613880521055
2080, epoch_train_loss=3.142613880521055
Epoch 2081
Epoch 2081 :: Batch 0/1
Batch Loss = 3.1342180921523175
2081, epoch_train_loss=3.1342180921523175
Epoch 2082
Epoch 2082 :: Batch 0/1
Batch Loss = 3.1583085210552273
2082, epoch_train_loss=3.1583085210552273
Epoch 2083
Epoch 2083 :: Batch 0/1
Batch Loss = 3.130363512411725
2083, epoch_train_loss=3.130363512411725
Epoch 2084
Epoch 2084 :: Batch 0/1
Batch Loss = 3.123689367151704
2084, epoch_train_loss=3.123689367151704
Epoch 2085
Epoch 2085 :: Batch 0/1
Batch Loss = 3.1272493515047537
2085, epoch_train_loss=3.1272493515047537
Epoch 2086
Epoch 2086 :: Batch 0/1
Batch Loss = 3.0962874948292223
2086, epoch_train_loss=3.0962874948292223
Epoch 2087
Epoch 2087 :: Batch 0/1
Batch Loss = 3.074372735279283
2087, epoch_train_loss=3.074372735279283
Epoch 2088
Epoch 2088 :: Batch 0/1
Batch Loss = 3.077656361477288
2088, epoch_train_loss=3.077656361477288
Epoch 2089
Epoch 2089 :: Batch 0/1
Batch Loss = 3.0847503103166947
2089, epoch_train_loss=3.0847503103166947
Epoch 2090
Epoch 2090 :: Batch 0/1
Batch Loss = 3.0905207718926175
2090, epoch_train_loss=3.0905207718926175
Epoch 2091
Epoch 2091 :: Batch 0/1
Batch Loss = 3.072494061586811
2091, epoch_train_loss=3.072494061586811
Epoch 2092
Epoch 2092 :: Batch 0/1
Batch Loss = 3.0872834812751986
2092, epoch_train_loss=3.0872834812751986
Epoch 2093
Epoch 2093 :: Batch 0/1
Batch Loss = 3.067828634805493
2093, epoch_train_loss=3.067828634805493
Epoch 2094
Epoch 2094 :: Batch 0/1
Batch Loss = 3.0854559178316308
2094, epoch_train_loss=3.0854559178316308
Epoch 2095
Epoch 2095 :: Batch 0/1
Batch Loss = 3.094158298842706
2095, epoch_train_loss=3.094158298842706
Epoch 2096
Epoch 2096 :: Batch 0/1
Batch Loss = 3.0581943332156936
2096, epoch_train_loss=3.0581943332156936
Epoch 2097
Epoch 2097 :: Batch 0/1
Batch Loss = 3.0945058532743603
2097, epoch_train_loss=3.0945058532743603
Epoch 2098
Epoch 2098 :: Batch 0/1
Batch Loss = 3.1167639387426638
2098, epoch_train_loss=3.1167639387426638
Epoch 2099
Epoch 2099 :: Batch 0/1
Batch Loss = 3.0677350387218567
2099, epoch_train_loss=3.0677350387218567
Epoch 2100
Epoch 2100 :: Batch 0/1
Batch Loss = 3.132030196721148
2100, epoch_train_loss=3.132030196721148
Epoch 2101
Epoch 2101 :: Batch 0/1
Batch Loss = 3.063523960008561
2101, epoch_train_loss=3.063523960008561
Epoch 2102
Epoch 2102 :: Batch 0/1
Batch Loss = 3.0961977673294996
2102, epoch_train_loss=3.0961977673294996
Epoch 2103
Epoch 2103 :: Batch 0/1
Batch Loss = 3.0612542564816256
2103, epoch_train_loss=3.0612542564816256
Epoch 2104
Epoch 2104 :: Batch 0/1
Batch Loss = 3.096983320307911
2104, epoch_train_loss=3.096983320307911
Epoch 2105
Epoch 2105 :: Batch 0/1
Batch Loss = 3.0603951294355562
2105, epoch_train_loss=3.0603951294355562
Epoch 2106
Epoch 2106 :: Batch 0/1
Batch Loss = 3.084576114508045
2106, epoch_train_loss=3.084576114508045
Epoch 2107
Epoch 2107 :: Batch 0/1
Batch Loss = 3.0587564575907185
2107, epoch_train_loss=3.0587564575907185
Epoch 2108
Epoch 2108 :: Batch 0/1
Batch Loss = 3.084612651016566
2108, epoch_train_loss=3.084612651016566
Epoch 2109
Epoch 2109 :: Batch 0/1
Batch Loss = 3.0513193514623547
2109, epoch_train_loss=3.0513193514623547
Epoch 2110
Epoch 2110 :: Batch 0/1
Batch Loss = 3.0707493833511936
2110, epoch_train_loss=3.0707493833511936
Epoch 2111
Epoch 2111 :: Batch 0/1
Batch Loss = 3.049511237618349
2111, epoch_train_loss=3.049511237618349
Epoch 2112
Epoch 2112 :: Batch 0/1
Batch Loss = 3.05848802007574
2112, epoch_train_loss=3.05848802007574
Epoch 2113
Epoch 2113 :: Batch 0/1
Batch Loss = 3.059473756632054
2113, epoch_train_loss=3.059473756632054
Epoch 2114
Epoch 2114 :: Batch 0/1
Batch Loss = 3.048018346265565
2114, epoch_train_loss=3.048018346265565
Epoch 2115
Epoch 2115 :: Batch 0/1
Batch Loss = 3.0671005868054073
2115, epoch_train_loss=3.0671005868054073
Epoch 2116
Epoch 2116 :: Batch 0/1
Batch Loss = 3.0524264496519353
2116, epoch_train_loss=3.0524264496519353
Epoch 2117
Epoch 2117 :: Batch 0/1
Batch Loss = 3.051442273421989
2117, epoch_train_loss=3.051442273421989
Epoch 2118
Epoch 2118 :: Batch 0/1
Batch Loss = 3.0635382131309363
2118, epoch_train_loss=3.0635382131309363
Epoch 2119
Epoch 2119 :: Batch 0/1
Batch Loss = 3.0485570000714306
2119, epoch_train_loss=3.0485570000714306
Epoch 2120
Epoch 2120 :: Batch 0/1
Batch Loss = 3.046632475028088
2120, epoch_train_loss=3.046632475028088
Epoch 2121
Epoch 2121 :: Batch 0/1
Batch Loss = 3.0576842032538023
2121, epoch_train_loss=3.0576842032538023
Epoch 2122
Epoch 2122 :: Batch 0/1
Batch Loss = 3.048217335971288
2122, epoch_train_loss=3.048217335971288
Epoch 2123
Epoch 2123 :: Batch 0/1
Batch Loss = 3.041208519667556
2123, epoch_train_loss=3.041208519667556
Epoch 2124
Epoch 2124 :: Batch 0/1
Batch Loss = 3.0576492419611365
2124, epoch_train_loss=3.0576492419611365
Epoch 2125
Epoch 2125 :: Batch 0/1
Batch Loss = 3.056450618333516
2125, epoch_train_loss=3.056450618333516
Epoch 2126
Epoch 2126 :: Batch 0/1
Batch Loss = 3.039448953751719
2126, epoch_train_loss=3.039448953751719
Epoch 2127
Epoch 2127 :: Batch 0/1
Batch Loss = 3.0676796969851856
2127, epoch_train_loss=3.0676796969851856
Epoch 2128
Epoch 2128 :: Batch 0/1
Batch Loss = 3.066495078694997
2128, epoch_train_loss=3.066495078694997
Epoch 2129
Epoch 2129 :: Batch 0/1
Batch Loss = 3.0460883154901484
2129, epoch_train_loss=3.0460883154901484
Epoch 2130
Epoch 2130 :: Batch 0/1
Batch Loss = 3.084777357608359
2130, epoch_train_loss=3.084777357608359
Epoch 2131
Epoch 2131 :: Batch 0/1
Batch Loss = 3.0568224553840935
2131, epoch_train_loss=3.0568224553840935
Epoch 2132
Epoch 2132 :: Batch 0/1
Batch Loss = 3.0600538979827436
2132, epoch_train_loss=3.0600538979827436
Epoch 2133
Epoch 2133 :: Batch 0/1
Batch Loss = 3.059154896702651
2133, epoch_train_loss=3.059154896702651
Epoch 2134
Epoch 2134 :: Batch 0/1
Batch Loss = 3.0381689108825762
2134, epoch_train_loss=3.0381689108825762
Epoch 2135
Epoch 2135 :: Batch 0/1
Batch Loss = 3.05491781107098
2135, epoch_train_loss=3.05491781107098
Epoch 2136
Epoch 2136 :: Batch 0/1
Batch Loss = 3.0342305274701467
2136, epoch_train_loss=3.0342305274701467
Epoch 2137
Epoch 2137 :: Batch 0/1
Batch Loss = 3.0549058396159756
2137, epoch_train_loss=3.0549058396159756
Epoch 2138
Epoch 2138 :: Batch 0/1
Batch Loss = 3.056559380179578
2138, epoch_train_loss=3.056559380179578
Epoch 2139
Epoch 2139 :: Batch 0/1
Batch Loss = 3.0363103446556687
2139, epoch_train_loss=3.0363103446556687
Epoch 2140
Epoch 2140 :: Batch 0/1
Batch Loss = 3.0736817941331824
2140, epoch_train_loss=3.0736817941331824
Epoch 2141
Epoch 2141 :: Batch 0/1
Batch Loss = 3.066534381079936
2141, epoch_train_loss=3.066534381079936
Epoch 2142
Epoch 2142 :: Batch 0/1
Batch Loss = 3.0460283474438032
2142, epoch_train_loss=3.0460283474438032
Epoch 2143
Epoch 2143 :: Batch 0/1
Batch Loss = 3.089857295355875
2143, epoch_train_loss=3.089857295355875
Epoch 2144
Epoch 2144 :: Batch 0/1
Batch Loss = 3.053582286694339
2144, epoch_train_loss=3.053582286694339
Epoch 2145
Epoch 2145 :: Batch 0/1
Batch Loss = 3.0593390848676068
2145, epoch_train_loss=3.0593390848676068
Epoch 2146
Epoch 2146 :: Batch 0/1
Batch Loss = 3.056374012595685
2146, epoch_train_loss=3.056374012595685
Epoch 2147
Epoch 2147 :: Batch 0/1
Batch Loss = 3.032249790719463
2147, epoch_train_loss=3.032249790719463
Epoch 2148
Epoch 2148 :: Batch 0/1
Batch Loss = 3.051090162496624
2148, epoch_train_loss=3.051090162496624
Epoch 2149
Epoch 2149 :: Batch 0/1
Batch Loss = 3.0291141021082204
2149, epoch_train_loss=3.0291141021082204
Epoch 2150
Epoch 2150 :: Batch 0/1
Batch Loss = 3.0433519817365697
2150, epoch_train_loss=3.0433519817365697
Epoch 2151
Epoch 2151 :: Batch 0/1
Batch Loss = 3.056095422122056
2151, epoch_train_loss=3.056095422122056
Epoch 2152
Epoch 2152 :: Batch 0/1
Batch Loss = 3.0276292803759532
2152, epoch_train_loss=3.0276292803759532
Epoch 2153
Epoch 2153 :: Batch 0/1
Batch Loss = 3.0649814350432663
2153, epoch_train_loss=3.0649814350432663
Epoch 2154
Epoch 2154 :: Batch 0/1
Batch Loss = 3.0759110676714236
2154, epoch_train_loss=3.0759110676714236
Epoch 2155
Epoch 2155 :: Batch 0/1
Batch Loss = 3.0395285506953855
2155, epoch_train_loss=3.0395285506953855
Epoch 2156
Epoch 2156 :: Batch 0/1
Batch Loss = 3.10702249983247
2156, epoch_train_loss=3.10702249983247
Epoch 2157
Epoch 2157 :: Batch 0/1
Batch Loss = 3.0625859272811926
2157, epoch_train_loss=3.0625859272811926
Epoch 2158
Epoch 2158 :: Batch 0/1
Batch Loss = 3.0730700345311606
2158, epoch_train_loss=3.0730700345311606
Epoch 2159
Epoch 2159 :: Batch 0/1
Batch Loss = 3.050178404576958
2159, epoch_train_loss=3.050178404576958
Epoch 2160
Epoch 2160 :: Batch 0/1
Batch Loss = 3.053550522765787
2160, epoch_train_loss=3.053550522765787
Epoch 2161
Epoch 2161 :: Batch 0/1
Batch Loss = 3.0599673804029535
2161, epoch_train_loss=3.0599673804029535
Epoch 2162
Epoch 2162 :: Batch 0/1
Batch Loss = 3.044221911569445
2162, epoch_train_loss=3.044221911569445
Epoch 2163
Epoch 2163 :: Batch 0/1
Batch Loss = 3.07469631116193
2163, epoch_train_loss=3.07469631116193
Epoch 2164
Epoch 2164 :: Batch 0/1
Batch Loss = 3.030142690902419
2164, epoch_train_loss=3.030142690902419
Epoch 2165
Epoch 2165 :: Batch 0/1
Batch Loss = 3.0491971091290626
2165, epoch_train_loss=3.0491971091290626
Epoch 2166
Epoch 2166 :: Batch 0/1
Batch Loss = 3.035456019829956
2166, epoch_train_loss=3.035456019829956
Epoch 2167
Epoch 2167 :: Batch 0/1
Batch Loss = 3.027929284774016
2167, epoch_train_loss=3.027929284774016
Epoch 2168
Epoch 2168 :: Batch 0/1
Batch Loss = 3.0430841806101836
2168, epoch_train_loss=3.0430841806101836
Epoch 2169
Epoch 2169 :: Batch 0/1
Batch Loss = 3.0229767537985044
2169, epoch_train_loss=3.0229767537985044
Epoch 2170
Epoch 2170 :: Batch 0/1
Batch Loss = 3.037971389126358
2170, epoch_train_loss=3.037971389126358
Epoch 2171
Epoch 2171 :: Batch 0/1
Batch Loss = 3.04433896759953
2171, epoch_train_loss=3.04433896759953
Epoch 2172
Epoch 2172 :: Batch 0/1
Batch Loss = 3.0210546570983987
2172, epoch_train_loss=3.0210546570983987
Epoch 2173
Epoch 2173 :: Batch 0/1
Batch Loss = 3.0536983057132074
2173, epoch_train_loss=3.0536983057132074
Epoch 2174
Epoch 2174 :: Batch 0/1
Batch Loss = 3.0657039336057528
2174, epoch_train_loss=3.0657039336057528
Epoch 2175
Epoch 2175 :: Batch 0/1
Batch Loss = 3.025498860354554
2175, epoch_train_loss=3.025498860354554
Epoch 2176
Epoch 2176 :: Batch 0/1
Batch Loss = 3.1027075714426826
2176, epoch_train_loss=3.1027075714426826
Epoch 2177
Epoch 2177 :: Batch 0/1
Batch Loss = 3.0875181313749978
2177, epoch_train_loss=3.0875181313749978
Epoch 2178
Epoch 2178 :: Batch 0/1
Batch Loss = 3.076001182166369
2178, epoch_train_loss=3.076001182166369
Epoch 2179
Epoch 2179 :: Batch 0/1
Batch Loss = 3.067417075292067
2179, epoch_train_loss=3.067417075292067
Epoch 2180
Epoch 2180 :: Batch 0/1
Batch Loss = 3.0448816942021315
2180, epoch_train_loss=3.0448816942021315
Epoch 2181
Epoch 2181 :: Batch 0/1
Batch Loss = 3.0658925364695016
2181, epoch_train_loss=3.0658925364695016
Epoch 2182
Epoch 2182 :: Batch 0/1
Batch Loss = 3.043228070102411
2182, epoch_train_loss=3.043228070102411
Epoch 2183
Epoch 2183 :: Batch 0/1
Batch Loss = 3.0749237178634403
2183, epoch_train_loss=3.0749237178634403
Epoch 2184
Epoch 2184 :: Batch 0/1
Batch Loss = 3.022232717076307
2184, epoch_train_loss=3.022232717076307
Epoch 2185
Epoch 2185 :: Batch 0/1
Batch Loss = 3.05829801607891
2185, epoch_train_loss=3.05829801607891
Epoch 2186
Epoch 2186 :: Batch 0/1
Batch Loss = 3.0221152974590573
2186, epoch_train_loss=3.0221152974590573
Epoch 2187
Epoch 2187 :: Batch 0/1
Batch Loss = 3.0476900271202134
2187, epoch_train_loss=3.0476900271202134
Epoch 2188
Epoch 2188 :: Batch 0/1
Batch Loss = 3.0418533066109825
2188, epoch_train_loss=3.0418533066109825
Epoch 2189
Epoch 2189 :: Batch 0/1
Batch Loss = 3.024734331839249
2189, epoch_train_loss=3.024734331839249
Epoch 2190
Epoch 2190 :: Batch 0/1
Batch Loss = 3.0560535296838767
2190, epoch_train_loss=3.0560535296838767
Epoch 2191
Epoch 2191 :: Batch 0/1
Batch Loss = 3.0253090829077895
2191, epoch_train_loss=3.0253090829077895
Epoch 2192
Epoch 2192 :: Batch 0/1
Batch Loss = 3.0306798116067535
2192, epoch_train_loss=3.0306798116067535
Epoch 2193
Epoch 2193 :: Batch 0/1
Batch Loss = 3.0362737801062103
2193, epoch_train_loss=3.0362737801062103
Epoch 2194
Epoch 2194 :: Batch 0/1
Batch Loss = 3.0142234320170127
2194, epoch_train_loss=3.0142234320170127
Epoch 2195
Epoch 2195 :: Batch 0/1
Batch Loss = 3.022839955917044
2195, epoch_train_loss=3.022839955917044
Epoch 2196
Epoch 2196 :: Batch 0/1
Batch Loss = 3.024903308326139
2196, epoch_train_loss=3.024903308326139
Epoch 2197
Epoch 2197 :: Batch 0/1
Batch Loss = 3.014028474531179
2197, epoch_train_loss=3.014028474531179
Epoch 2198
Epoch 2198 :: Batch 0/1
Batch Loss = 3.0109683610134685
2198, epoch_train_loss=3.0109683610134685
Epoch 2199
Epoch 2199 :: Batch 0/1
Batch Loss = 3.020182839175617
2199, epoch_train_loss=3.020182839175617
Epoch 2200
Epoch 2200 :: Batch 0/1
Batch Loss = 3.025756990659805
2200, epoch_train_loss=3.025756990659805
Epoch 2201
Epoch 2201 :: Batch 0/1
Batch Loss = 3.0086941746820037
2201, epoch_train_loss=3.0086941746820037
Epoch 2202
Epoch 2202 :: Batch 0/1
Batch Loss = 3.0103556793285065
2202, epoch_train_loss=3.0103556793285065
Epoch 2203
Epoch 2203 :: Batch 0/1
Batch Loss = 3.023410737791958
2203, epoch_train_loss=3.023410737791958
Epoch 2204
Epoch 2204 :: Batch 0/1
Batch Loss = 3.0110990646169946
2204, epoch_train_loss=3.0110990646169946
Epoch 2205
Epoch 2205 :: Batch 0/1
Batch Loss = 3.0044886543959977
2205, epoch_train_loss=3.0044886543959977
Epoch 2206
Epoch 2206 :: Batch 0/1
Batch Loss = 3.0084432862917225
2206, epoch_train_loss=3.0084432862917225
Epoch 2207
Epoch 2207 :: Batch 0/1
Batch Loss = 3.011233164960175
2207, epoch_train_loss=3.011233164960175
Epoch 2208
Epoch 2208 :: Batch 0/1
Batch Loss = 3.012715073401863
2208, epoch_train_loss=3.012715073401863
Epoch 2209
Epoch 2209 :: Batch 0/1
Batch Loss = 3.0047016025011364
2209, epoch_train_loss=3.0047016025011364
Epoch 2210
Epoch 2210 :: Batch 0/1
Batch Loss = 3.001182161162063
2210, epoch_train_loss=3.001182161162063
Epoch 2211
Epoch 2211 :: Batch 0/1
Batch Loss = 3.002207282919562
2211, epoch_train_loss=3.002207282919562
Epoch 2212
Epoch 2212 :: Batch 0/1
Batch Loss = 3.0053853076963395
2212, epoch_train_loss=3.0053853076963395
Epoch 2213
Epoch 2213 :: Batch 0/1
Batch Loss = 3.012459642352589
2213, epoch_train_loss=3.012459642352589
Epoch 2214
Epoch 2214 :: Batch 0/1
Batch Loss = 3.0099119420296105
2214, epoch_train_loss=3.0099119420296105
Epoch 2215
Epoch 2215 :: Batch 0/1
Batch Loss = 3.012980758170782
2215, epoch_train_loss=3.012980758170782
Epoch 2216
Epoch 2216 :: Batch 0/1
Batch Loss = 3.0041791095665404
2216, epoch_train_loss=3.0041791095665404
Epoch 2217
Epoch 2217 :: Batch 0/1
Batch Loss = 3.000127260895917
2217, epoch_train_loss=3.000127260895917
Epoch 2218
Epoch 2218 :: Batch 0/1
Batch Loss = 2.997111754321408
2218, epoch_train_loss=2.997111754321408
Epoch 2219
Epoch 2219 :: Batch 0/1
Batch Loss = 2.9961026877385053
2219, epoch_train_loss=2.9961026877385053
Epoch 2220
Epoch 2220 :: Batch 0/1
Batch Loss = 2.9961157552902677
2220, epoch_train_loss=2.9961157552902677
Epoch 2221
Epoch 2221 :: Batch 0/1
Batch Loss = 2.997825618548275
2221, epoch_train_loss=2.997825618548275
Epoch 2222
Epoch 2222 :: Batch 0/1
Batch Loss = 3.0053447491649834
2222, epoch_train_loss=3.0053447491649834
Epoch 2223
Epoch 2223 :: Batch 0/1
Batch Loss = 3.016965716110275
2223, epoch_train_loss=3.016965716110275
Epoch 2224
Epoch 2224 :: Batch 0/1
Batch Loss = 3.069538138460769
2224, epoch_train_loss=3.069538138460769
Epoch 2225
Epoch 2225 :: Batch 0/1
Batch Loss = 3.006602430531248
2225, epoch_train_loss=3.006602430531248
Epoch 2226
Epoch 2226 :: Batch 0/1
Batch Loss = 2.9966654265114148
2226, epoch_train_loss=2.9966654265114148
Epoch 2227
Epoch 2227 :: Batch 0/1
Batch Loss = 3.0150212053433845
2227, epoch_train_loss=3.0150212053433845
Epoch 2228
Epoch 2228 :: Batch 0/1
Batch Loss = 3.02718547545065
2228, epoch_train_loss=3.02718547545065
Epoch 2229
Epoch 2229 :: Batch 0/1
Batch Loss = 3.073362242587098
2229, epoch_train_loss=3.073362242587098
Epoch 2230
Epoch 2230 :: Batch 0/1
Batch Loss = 2.9966104965763636
2230, epoch_train_loss=2.9966104965763636
Epoch 2231
Epoch 2231 :: Batch 0/1
Batch Loss = 3.049978431860567
2231, epoch_train_loss=3.049978431860567
Epoch 2232
Epoch 2232 :: Batch 0/1
Batch Loss = 3.1706790069406954
2232, epoch_train_loss=3.1706790069406954
Epoch 2233
Epoch 2233 :: Batch 0/1
Batch Loss = 3.090559809824126
2233, epoch_train_loss=3.090559809824126
Epoch 2234
Epoch 2234 :: Batch 0/1
Batch Loss = 3.1898400733938983
2234, epoch_train_loss=3.1898400733938983
Epoch 2235
Epoch 2235 :: Batch 0/1
Batch Loss = 3.048461515925897
2235, epoch_train_loss=3.048461515925897
Epoch 2236
Epoch 2236 :: Batch 0/1
Batch Loss = 3.121285181466649
2236, epoch_train_loss=3.121285181466649
Epoch 2237
Epoch 2237 :: Batch 0/1
Batch Loss = 3.05558317881489
2237, epoch_train_loss=3.05558317881489
Epoch 2238
Epoch 2238 :: Batch 0/1
Batch Loss = 3.091085436616582
2238, epoch_train_loss=3.091085436616582
Epoch 2239
Epoch 2239 :: Batch 0/1
Batch Loss = 3.074796071696953
2239, epoch_train_loss=3.074796071696953
Epoch 2240
Epoch 2240 :: Batch 0/1
Batch Loss = 3.059424828552285
2240, epoch_train_loss=3.059424828552285
Epoch 2241
Epoch 2241 :: Batch 0/1
Batch Loss = 3.059444265744832
2241, epoch_train_loss=3.059444265744832
Epoch 2242
Epoch 2242 :: Batch 0/1
Batch Loss = 3.0586548743461903
2242, epoch_train_loss=3.0586548743461903
Epoch 2243
Epoch 2243 :: Batch 0/1
Batch Loss = 3.0682255982312276
2243, epoch_train_loss=3.0682255982312276
Epoch 2244
Epoch 2244 :: Batch 0/1
Batch Loss = 3.0318817922783405
2244, epoch_train_loss=3.0318817922783405
Epoch 2245
Epoch 2245 :: Batch 0/1
Batch Loss = 3.0550704919842553
2245, epoch_train_loss=3.0550704919842553
Epoch 2246
Epoch 2246 :: Batch 0/1
Batch Loss = 3.0387996020886683
2246, epoch_train_loss=3.0387996020886683
Epoch 2247
Epoch 2247 :: Batch 0/1
Batch Loss = 3.0338161059816557
2247, epoch_train_loss=3.0338161059816557
Epoch 2248
Epoch 2248 :: Batch 0/1
Batch Loss = 3.0386717477201923
2248, epoch_train_loss=3.0386717477201923
Epoch 2249
Epoch 2249 :: Batch 0/1
Batch Loss = 3.014377232006394
2249, epoch_train_loss=3.014377232006394
Epoch 2250
Epoch 2250 :: Batch 0/1
Batch Loss = 3.050430111280119
2250, epoch_train_loss=3.050430111280119
Epoch 2251
Epoch 2251 :: Batch 0/1
Batch Loss = 3.0452737392377687
2251, epoch_train_loss=3.0452737392377687
Epoch 2252
Epoch 2252 :: Batch 0/1
Batch Loss = 3.0178998178753913
2252, epoch_train_loss=3.0178998178753913
Epoch 2253
Epoch 2253 :: Batch 0/1
Batch Loss = 3.0277979202243874
2253, epoch_train_loss=3.0277979202243874
Epoch 2254
Epoch 2254 :: Batch 0/1
Batch Loss = 3.0558655480278394
2254, epoch_train_loss=3.0558655480278394
Epoch 2255
Epoch 2255 :: Batch 0/1
Batch Loss = 3.0082484796773663
2255, epoch_train_loss=3.0082484796773663
Epoch 2256
Epoch 2256 :: Batch 0/1
Batch Loss = 3.017882652900637
2256, epoch_train_loss=3.017882652900637
Epoch 2257
Epoch 2257 :: Batch 0/1
Batch Loss = 3.052849215196116
2257, epoch_train_loss=3.052849215196116
Epoch 2258
Epoch 2258 :: Batch 0/1
Batch Loss = 2.9992827280070777
2258, epoch_train_loss=2.9992827280070777
Epoch 2259
Epoch 2259 :: Batch 0/1
Batch Loss = 3.052213360348145
2259, epoch_train_loss=3.052213360348145
Epoch 2260
Epoch 2260 :: Batch 0/1
Batch Loss = 3.077552470040505
2260, epoch_train_loss=3.077552470040505
Epoch 2261
Epoch 2261 :: Batch 0/1
Batch Loss = 3.020683704513923
2261, epoch_train_loss=3.020683704513923
Epoch 2262
Epoch 2262 :: Batch 0/1
Batch Loss = 3.1237091111492186
2262, epoch_train_loss=3.1237091111492186
Epoch 2263
Epoch 2263 :: Batch 0/1
Batch Loss = 3.0206697365374757
2263, epoch_train_loss=3.0206697365374757
Epoch 2264
Epoch 2264 :: Batch 0/1
Batch Loss = 3.063523458465
2264, epoch_train_loss=3.063523458465
Epoch 2265
Epoch 2265 :: Batch 0/1
Batch Loss = 3.020214432853823
2265, epoch_train_loss=3.020214432853823
Epoch 2266
Epoch 2266 :: Batch 0/1
Batch Loss = 3.0480537019505585
2266, epoch_train_loss=3.0480537019505585
Epoch 2267
Epoch 2267 :: Batch 0/1
Batch Loss = 3.018193608654161
2267, epoch_train_loss=3.018193608654161
Epoch 2268
Epoch 2268 :: Batch 0/1
Batch Loss = 3.0256710091307086
2268, epoch_train_loss=3.0256710091307086
Epoch 2269
Epoch 2269 :: Batch 0/1
Batch Loss = 3.0333163963514513
2269, epoch_train_loss=3.0333163963514513
Epoch 2270
Epoch 2270 :: Batch 0/1
Batch Loss = 2.99934354902905
2270, epoch_train_loss=2.99934354902905
Epoch 2271
Epoch 2271 :: Batch 0/1
Batch Loss = 3.026843256398935
2271, epoch_train_loss=3.026843256398935
Epoch 2272
Epoch 2272 :: Batch 0/1
Batch Loss = 3.0188906097825856
2272, epoch_train_loss=3.0188906097825856
Epoch 2273
Epoch 2273 :: Batch 0/1
Batch Loss = 2.9967325847294113
2273, epoch_train_loss=2.9967325847294113
Epoch 2274
Epoch 2274 :: Batch 0/1
Batch Loss = 3.003966408729742
2274, epoch_train_loss=3.003966408729742
Epoch 2275
Epoch 2275 :: Batch 0/1
Batch Loss = 3.017798319500268
2275, epoch_train_loss=3.017798319500268
Epoch 2276
Epoch 2276 :: Batch 0/1
Batch Loss = 3.0159664872470966
2276, epoch_train_loss=3.0159664872470966
Epoch 2277
Epoch 2277 :: Batch 0/1
Batch Loss = 2.9963176372123232
2277, epoch_train_loss=2.9963176372123232
Epoch 2278
Epoch 2278 :: Batch 0/1
Batch Loss = 2.9983721182651006
2278, epoch_train_loss=2.9983721182651006
Epoch 2279
Epoch 2279 :: Batch 0/1
Batch Loss = 3.0155133649062495
2279, epoch_train_loss=3.0155133649062495
Epoch 2280
Epoch 2280 :: Batch 0/1
Batch Loss = 3.0123524047804535
2280, epoch_train_loss=3.0123524047804535
Epoch 2281
Epoch 2281 :: Batch 0/1
Batch Loss = 3.0042408809262335
2281, epoch_train_loss=3.0042408809262335
Epoch 2282
Epoch 2282 :: Batch 0/1
Batch Loss = 2.9923138556257105
2282, epoch_train_loss=2.9923138556257105
Epoch 2283
Epoch 2283 :: Batch 0/1
Batch Loss = 2.9915258650836987
2283, epoch_train_loss=2.9915258650836987
Epoch 2284
Epoch 2284 :: Batch 0/1
Batch Loss = 2.999080906207227
2284, epoch_train_loss=2.999080906207227
Epoch 2285
Epoch 2285 :: Batch 0/1
Batch Loss = 3.0021174261062984
2285, epoch_train_loss=3.0021174261062984
Epoch 2286
Epoch 2286 :: Batch 0/1
Batch Loss = 3.0025872118294523
2286, epoch_train_loss=3.0025872118294523
Epoch 2287
Epoch 2287 :: Batch 0/1
Batch Loss = 2.993341991662158
2287, epoch_train_loss=2.993341991662158
Epoch 2288
Epoch 2288 :: Batch 0/1
Batch Loss = 2.9887793338232305
2288, epoch_train_loss=2.9887793338232305
Epoch 2289
Epoch 2289 :: Batch 0/1
Batch Loss = 2.9889953440514105
2289, epoch_train_loss=2.9889953440514105
Epoch 2290
Epoch 2290 :: Batch 0/1
Batch Loss = 2.9928606418682477
2290, epoch_train_loss=2.9928606418682477
Epoch 2291
Epoch 2291 :: Batch 0/1
Batch Loss = 3.0025672598987208
2291, epoch_train_loss=3.0025672598987208
Epoch 2292
Epoch 2292 :: Batch 0/1
Batch Loss = 3.0079780917936083
2292, epoch_train_loss=3.0079780917936083
Epoch 2293
Epoch 2293 :: Batch 0/1
Batch Loss = 3.0315724778490973
2293, epoch_train_loss=3.0315724778490973
Epoch 2294
Epoch 2294 :: Batch 0/1
Batch Loss = 3.0046880006603414
2294, epoch_train_loss=3.0046880006603414
Epoch 2295
Epoch 2295 :: Batch 0/1
Batch Loss = 2.9963502360887637
2295, epoch_train_loss=2.9963502360887637
Epoch 2296
Epoch 2296 :: Batch 0/1
Batch Loss = 2.9884550249638426
2296, epoch_train_loss=2.9884550249638426
Epoch 2297
Epoch 2297 :: Batch 0/1
Batch Loss = 2.9857317680201008
2297, epoch_train_loss=2.9857317680201008
Epoch 2298
Epoch 2298 :: Batch 0/1
Batch Loss = 2.9852118297099444
2298, epoch_train_loss=2.9852118297099444
Epoch 2299
Epoch 2299 :: Batch 0/1
Batch Loss = 2.988262151579654
2299, epoch_train_loss=2.988262151579654
Epoch 2300
Epoch 2300 :: Batch 0/1
Batch Loss = 3.0024477287126925
2300, epoch_train_loss=3.0024477287126925
Epoch 2301
Epoch 2301 :: Batch 0/1
Batch Loss = 3.028175385512081
2301, epoch_train_loss=3.028175385512081
Epoch 2302
Epoch 2302 :: Batch 0/1
Batch Loss = 3.1338128477648026
2302, epoch_train_loss=3.1338128477648026
Epoch 2303
Epoch 2303 :: Batch 0/1
Batch Loss = 2.989841822973427
2303, epoch_train_loss=2.989841822973427
Epoch 2304
Epoch 2304 :: Batch 0/1
Batch Loss = 3.2132698237907804
2304, epoch_train_loss=3.2132698237907804
Epoch 2305
Epoch 2305 :: Batch 0/1
Batch Loss = 3.2195413553422196
2305, epoch_train_loss=3.2195413553422196
Epoch 2306
Epoch 2306 :: Batch 0/1
Batch Loss = 3.234721922995575
2306, epoch_train_loss=3.234721922995575
Epoch 2307
Epoch 2307 :: Batch 0/1
Batch Loss = 3.216970533272126
2307, epoch_train_loss=3.216970533272126
Epoch 2308
Epoch 2308 :: Batch 0/1
Batch Loss = 3.2526136535208874
2308, epoch_train_loss=3.2526136535208874
Epoch 2309
Epoch 2309 :: Batch 0/1
Batch Loss = 3.2132060807836242
2309, epoch_train_loss=3.2132060807836242
Epoch 2310
Epoch 2310 :: Batch 0/1
Batch Loss = 3.1975900892070612
2310, epoch_train_loss=3.1975900892070612
Epoch 2311
Epoch 2311 :: Batch 0/1
Batch Loss = 3.200319547036091
2311, epoch_train_loss=3.200319547036091
Epoch 2312
Epoch 2312 :: Batch 0/1
Batch Loss = 3.1958942770583145
2312, epoch_train_loss=3.1958942770583145
Epoch 2313
Epoch 2313 :: Batch 0/1
Batch Loss = 3.1855238921217444
2313, epoch_train_loss=3.1855238921217444
Epoch 2314
Epoch 2314 :: Batch 0/1
Batch Loss = 3.1640815224359513
2314, epoch_train_loss=3.1640815224359513
Epoch 2315
Epoch 2315 :: Batch 0/1
Batch Loss = 3.1239961536308747
2315, epoch_train_loss=3.1239961536308747
Epoch 2316
Epoch 2316 :: Batch 0/1
Batch Loss = 3.1012383675831625
2316, epoch_train_loss=3.1012383675831625
Epoch 2317
Epoch 2317 :: Batch 0/1
Batch Loss = 3.0883068054766243
2317, epoch_train_loss=3.0883068054766243
Epoch 2318
Epoch 2318 :: Batch 0/1
Batch Loss = 3.073830651067919
2318, epoch_train_loss=3.073830651067919
Epoch 2319
Epoch 2319 :: Batch 0/1
Batch Loss = 3.034850156403907
2319, epoch_train_loss=3.034850156403907
Epoch 2320
Epoch 2320 :: Batch 0/1
Batch Loss = 3.0897883730346423
2320, epoch_train_loss=3.0897883730346423
Epoch 2321
Epoch 2321 :: Batch 0/1
Batch Loss = 3.0567138133462666
2321, epoch_train_loss=3.0567138133462666
Epoch 2322
Epoch 2322 :: Batch 0/1
Batch Loss = 3.029560519883216
2322, epoch_train_loss=3.029560519883216
Epoch 2323
Epoch 2323 :: Batch 0/1
Batch Loss = 3.0568582996500915
2323, epoch_train_loss=3.0568582996500915
Epoch 2324
Epoch 2324 :: Batch 0/1
Batch Loss = 3.1140968275676255
2324, epoch_train_loss=3.1140968275676255
Epoch 2325
Epoch 2325 :: Batch 0/1
Batch Loss = 3.167936916824303
2325, epoch_train_loss=3.167936916824303
Epoch 2326
Epoch 2326 :: Batch 0/1
Batch Loss = 3.034890190205924
2326, epoch_train_loss=3.034890190205924
Epoch 2327
Epoch 2327 :: Batch 0/1
Batch Loss = 3.227648164604693
2327, epoch_train_loss=3.227648164604693
Epoch 2328
Epoch 2328 :: Batch 0/1
Batch Loss = 3.0980205493675985
2328, epoch_train_loss=3.0980205493675985
Epoch 2329
Epoch 2329 :: Batch 0/1
Batch Loss = 3.1628467840508687
2329, epoch_train_loss=3.1628467840508687
Epoch 2330
Epoch 2330 :: Batch 0/1
Batch Loss = 3.084709227718816
2330, epoch_train_loss=3.084709227718816
Epoch 2331
Epoch 2331 :: Batch 0/1
Batch Loss = 3.100087977612231
2331, epoch_train_loss=3.100087977612231
Epoch 2332
Epoch 2332 :: Batch 0/1
Batch Loss = 3.123259529071969
2332, epoch_train_loss=3.123259529071969
Epoch 2333
Epoch 2333 :: Batch 0/1
Batch Loss = 3.0816819333265175
2333, epoch_train_loss=3.0816819333265175
Epoch 2334
Epoch 2334 :: Batch 0/1
Batch Loss = 3.058278115037004
2334, epoch_train_loss=3.058278115037004
Epoch 2335
Epoch 2335 :: Batch 0/1
Batch Loss = 3.1008525372099087
2335, epoch_train_loss=3.1008525372099087
Epoch 2336
Epoch 2336 :: Batch 0/1
Batch Loss = 3.0606286303815344
2336, epoch_train_loss=3.0606286303815344
Epoch 2337
Epoch 2337 :: Batch 0/1
Batch Loss = 3.0773228980070004
2337, epoch_train_loss=3.0773228980070004
Epoch 2338
Epoch 2338 :: Batch 0/1
Batch Loss = 3.029775687151822
2338, epoch_train_loss=3.029775687151822
Epoch 2339
Epoch 2339 :: Batch 0/1
Batch Loss = 3.0514228145906785
2339, epoch_train_loss=3.0514228145906785
Epoch 2340
Epoch 2340 :: Batch 0/1
Batch Loss = 3.0165283364812066
2340, epoch_train_loss=3.0165283364812066
Epoch 2341
Epoch 2341 :: Batch 0/1
Batch Loss = 3.0514700478977095
2341, epoch_train_loss=3.0514700478977095
Epoch 2342
Epoch 2342 :: Batch 0/1
Batch Loss = 3.0172409640873346
2342, epoch_train_loss=3.0172409640873346
Epoch 2343
Epoch 2343 :: Batch 0/1
Batch Loss = 3.0462684895599255
2343, epoch_train_loss=3.0462684895599255
Epoch 2344
Epoch 2344 :: Batch 0/1
Batch Loss = 3.0186793043096447
2344, epoch_train_loss=3.0186793043096447
Epoch 2345
Epoch 2345 :: Batch 0/1
Batch Loss = 3.037114006812418
2345, epoch_train_loss=3.037114006812418
Epoch 2346
Epoch 2346 :: Batch 0/1
Batch Loss = 3.0345727261974944
2346, epoch_train_loss=3.0345727261974944
Epoch 2347
Epoch 2347 :: Batch 0/1
Batch Loss = 3.0124029509031307
2347, epoch_train_loss=3.0124029509031307
Epoch 2348
Epoch 2348 :: Batch 0/1
Batch Loss = 3.0509367728537593
2348, epoch_train_loss=3.0509367728537593
Epoch 2349
Epoch 2349 :: Batch 0/1
Batch Loss = 3.0436283124202266
2349, epoch_train_loss=3.0436283124202266
Epoch 2350
Epoch 2350 :: Batch 0/1
Batch Loss = 3.0078414858370395
2350, epoch_train_loss=3.0078414858370395
Epoch 2351
Epoch 2351 :: Batch 0/1
Batch Loss = 3.070292177324185
2351, epoch_train_loss=3.070292177324185
Epoch 2352
Epoch 2352 :: Batch 0/1
Batch Loss = 3.0453466339743107
2352, epoch_train_loss=3.0453466339743107
Epoch 2353
Epoch 2353 :: Batch 0/1
Batch Loss = 3.015079818002644
2353, epoch_train_loss=3.015079818002644
Epoch 2354
Epoch 2354 :: Batch 0/1
Batch Loss = 3.062017118704693
2354, epoch_train_loss=3.062017118704693
Epoch 2355
Epoch 2355 :: Batch 0/1
Batch Loss = 3.0056455690775237
2355, epoch_train_loss=3.0056455690775237
Epoch 2356
Epoch 2356 :: Batch 0/1
Batch Loss = 3.0430930298631518
2356, epoch_train_loss=3.0430930298631518
Epoch 2357
Epoch 2357 :: Batch 0/1
Batch Loss = 3.0124058875408632
2357, epoch_train_loss=3.0124058875408632
Epoch 2358
Epoch 2358 :: Batch 0/1
Batch Loss = 3.033875235371923
2358, epoch_train_loss=3.033875235371923
Epoch 2359
Epoch 2359 :: Batch 0/1
Batch Loss = 3.0144014257158696
2359, epoch_train_loss=3.0144014257158696
Epoch 2360
Epoch 2360 :: Batch 0/1
Batch Loss = 3.0218729286494614
2360, epoch_train_loss=3.0218729286494614
Epoch 2361
Epoch 2361 :: Batch 0/1
Batch Loss = 3.0119374884536256
2361, epoch_train_loss=3.0119374884536256
Epoch 2362
Epoch 2362 :: Batch 0/1
Batch Loss = 3.0211178846471554
2362, epoch_train_loss=3.0211178846471554
Epoch 2363
Epoch 2363 :: Batch 0/1
Batch Loss = 3.0005248416083625
2363, epoch_train_loss=3.0005248416083625
Epoch 2364
Epoch 2364 :: Batch 0/1
Batch Loss = 3.0210355031438763
2364, epoch_train_loss=3.0210355031438763
Epoch 2365
Epoch 2365 :: Batch 0/1
Batch Loss = 2.992851863743994
2365, epoch_train_loss=2.992851863743994
Epoch 2366
Epoch 2366 :: Batch 0/1
Batch Loss = 3.012553204194706
2366, epoch_train_loss=3.012553204194706
Epoch 2367
Epoch 2367 :: Batch 0/1
Batch Loss = 3.015562342106355
2367, epoch_train_loss=3.015562342106355
Epoch 2368
Epoch 2368 :: Batch 0/1
Batch Loss = 2.994719970479409
2368, epoch_train_loss=2.994719970479409
Epoch 2369
Epoch 2369 :: Batch 0/1
Batch Loss = 3.0304733199031633
2369, epoch_train_loss=3.0304733199031633
Epoch 2370
Epoch 2370 :: Batch 0/1
Batch Loss = 3.0274523990716893
2370, epoch_train_loss=3.0274523990716893
Epoch 2371
Epoch 2371 :: Batch 0/1
Batch Loss = 2.9971150943279374
2371, epoch_train_loss=2.9971150943279374
Epoch 2372
Epoch 2372 :: Batch 0/1
Batch Loss = 3.06065111093964
2372, epoch_train_loss=3.06065111093964
Epoch 2373
Epoch 2373 :: Batch 0/1
Batch Loss = 3.050488950587975
2373, epoch_train_loss=3.050488950587975
Epoch 2374
Epoch 2374 :: Batch 0/1
Batch Loss = 3.0132645464644847
2374, epoch_train_loss=3.0132645464644847
Epoch 2375
Epoch 2375 :: Batch 0/1
Batch Loss = 3.090098426811299
2375, epoch_train_loss=3.090098426811299
Epoch 2376
Epoch 2376 :: Batch 0/1
Batch Loss = 3.0019403320547324
2376, epoch_train_loss=3.0019403320547324
Epoch 2377
Epoch 2377 :: Batch 0/1
Batch Loss = 3.0394933811968263
2377, epoch_train_loss=3.0394933811968263
Epoch 2378
Epoch 2378 :: Batch 0/1
Batch Loss = 3.001121307587963
2378, epoch_train_loss=3.001121307587963
Epoch 2379
Epoch 2379 :: Batch 0/1
Batch Loss = 3.0318462742352774
2379, epoch_train_loss=3.0318462742352774
Epoch 2380
Epoch 2380 :: Batch 0/1
Batch Loss = 3.000977737298096
2380, epoch_train_loss=3.000977737298096
Epoch 2381
Epoch 2381 :: Batch 0/1
Batch Loss = 3.0250059760191417
2381, epoch_train_loss=3.0250059760191417
Epoch 2382
Epoch 2382 :: Batch 0/1
Batch Loss = 2.9972233691101575
2382, epoch_train_loss=2.9972233691101575
Epoch 2383
Epoch 2383 :: Batch 0/1
Batch Loss = 3.0200785320820325
2383, epoch_train_loss=3.0200785320820325
Epoch 2384
Epoch 2384 :: Batch 0/1
Batch Loss = 2.9952037418101116
2384, epoch_train_loss=2.9952037418101116
Epoch 2385
Epoch 2385 :: Batch 0/1
Batch Loss = 3.011007238318365
2385, epoch_train_loss=3.011007238318365
Epoch 2386
Epoch 2386 :: Batch 0/1
Batch Loss = 2.996318268425441
2386, epoch_train_loss=2.996318268425441
Epoch 2387
Epoch 2387 :: Batch 0/1
Batch Loss = 2.9957281580512656
2387, epoch_train_loss=2.9957281580512656
Epoch 2388
Epoch 2388 :: Batch 0/1
Batch Loss = 3.0057841400619143
2388, epoch_train_loss=3.0057841400619143
Epoch 2389
Epoch 2389 :: Batch 0/1
Batch Loss = 2.987679410129487
2389, epoch_train_loss=2.987679410129487
Epoch 2390
Epoch 2390 :: Batch 0/1
Batch Loss = 3.0016084787108746
2390, epoch_train_loss=3.0016084787108746
Epoch 2391
Epoch 2391 :: Batch 0/1
Batch Loss = 3.0069041911648693
2391, epoch_train_loss=3.0069041911648693
Epoch 2392
Epoch 2392 :: Batch 0/1
Batch Loss = 2.9861598741591413
2392, epoch_train_loss=2.9861598741591413
Epoch 2393
Epoch 2393 :: Batch 0/1
Batch Loss = 3.0055090078655495
2393, epoch_train_loss=3.0055090078655495
Epoch 2394
Epoch 2394 :: Batch 0/1
Batch Loss = 3.0180739434682593
2394, epoch_train_loss=3.0180739434682593
Epoch 2395
Epoch 2395 :: Batch 0/1
Batch Loss = 2.984457122806011
2395, epoch_train_loss=2.984457122806011
Epoch 2396
Epoch 2396 :: Batch 0/1
Batch Loss = 3.011646557821955
2396, epoch_train_loss=3.011646557821955
Epoch 2397
Epoch 2397 :: Batch 0/1
Batch Loss = 3.0371951774382
2397, epoch_train_loss=3.0371951774382
Epoch 2398
Epoch 2398 :: Batch 0/1
Batch Loss = 2.9843047822834103
2398, epoch_train_loss=2.9843047822834103
Epoch 2399
Epoch 2399 :: Batch 0/1
Batch Loss = 3.052848263463362
2399, epoch_train_loss=3.052848263463362
Epoch 2400
Epoch 2400 :: Batch 0/1
Batch Loss = 3.0562790651204903
2400, epoch_train_loss=3.0562790651204903
Epoch 2401
Epoch 2401 :: Batch 0/1
Batch Loss = 3.013622802116589
2401, epoch_train_loss=3.013622802116589
Epoch 2402
Epoch 2402 :: Batch 0/1
Batch Loss = 3.0839365808904398
2402, epoch_train_loss=3.0839365808904398
Epoch 2403
Epoch 2403 :: Batch 0/1
Batch Loss = 2.993186550690119
2403, epoch_train_loss=2.993186550690119
Epoch 2404
Epoch 2404 :: Batch 0/1
Batch Loss = 3.051487835387515
2404, epoch_train_loss=3.051487835387515
Epoch 2405
Epoch 2405 :: Batch 0/1
Batch Loss = 3.000191846596917
2405, epoch_train_loss=3.000191846596917
Epoch 2406
Epoch 2406 :: Batch 0/1
Batch Loss = 3.0401358746136604
2406, epoch_train_loss=3.0401358746136604
Epoch 2407
Epoch 2407 :: Batch 0/1
Batch Loss = 3.0078838282874467
2407, epoch_train_loss=3.0078838282874467
Epoch 2408
Epoch 2408 :: Batch 0/1
Batch Loss = 3.024364830767574
2408, epoch_train_loss=3.024364830767574
Epoch 2409
Epoch 2409 :: Batch 0/1
Batch Loss = 3.006594411411158
2409, epoch_train_loss=3.006594411411158
Epoch 2410
Epoch 2410 :: Batch 0/1
Batch Loss = 3.0192785013999526
2410, epoch_train_loss=3.0192785013999526
Epoch 2411
Epoch 2411 :: Batch 0/1
Batch Loss = 3.004683984939363
2411, epoch_train_loss=3.004683984939363
Epoch 2412
Epoch 2412 :: Batch 0/1
Batch Loss = 3.014980177377746
2412, epoch_train_loss=3.014980177377746
Epoch 2413
Epoch 2413 :: Batch 0/1
Batch Loss = 2.994827267806722
2413, epoch_train_loss=2.994827267806722
Epoch 2414
Epoch 2414 :: Batch 0/1
Batch Loss = 3.019201440128109
2414, epoch_train_loss=3.019201440128109
Epoch 2415
Epoch 2415 :: Batch 0/1
Batch Loss = 2.989227009423516
2415, epoch_train_loss=2.989227009423516
Epoch 2416
Epoch 2416 :: Batch 0/1
Batch Loss = 3.016488803754443
2416, epoch_train_loss=3.016488803754443
Epoch 2417
Epoch 2417 :: Batch 0/1
Batch Loss = 2.987953796180181
2417, epoch_train_loss=2.987953796180181
Epoch 2418
Epoch 2418 :: Batch 0/1
Batch Loss = 3.0038787702718786
2418, epoch_train_loss=3.0038787702718786
Epoch 2419
Epoch 2419 :: Batch 0/1
Batch Loss = 3.0010113507579703
2419, epoch_train_loss=3.0010113507579703
Epoch 2420
Epoch 2420 :: Batch 0/1
Batch Loss = 2.9859858787532043
2420, epoch_train_loss=2.9859858787532043
Epoch 2421
Epoch 2421 :: Batch 0/1
Batch Loss = 3.007520631957938
2421, epoch_train_loss=3.007520631957938
Epoch 2422
Epoch 2422 :: Batch 0/1
Batch Loss = 2.991336433857082
2422, epoch_train_loss=2.991336433857082
Epoch 2423
Epoch 2423 :: Batch 0/1
Batch Loss = 2.984758952251604
2423, epoch_train_loss=2.984758952251604
Epoch 2424
Epoch 2424 :: Batch 0/1
Batch Loss = 3.0012341504068663
2424, epoch_train_loss=3.0012341504068663
Epoch 2425
Epoch 2425 :: Batch 0/1
Batch Loss = 2.9873224598017183
2425, epoch_train_loss=2.9873224598017183
Epoch 2426
Epoch 2426 :: Batch 0/1
Batch Loss = 2.982098439085546
2426, epoch_train_loss=2.982098439085546
Epoch 2427
Epoch 2427 :: Batch 0/1
Batch Loss = 2.9967148944527726
2427, epoch_train_loss=2.9967148944527726
Epoch 2428
Epoch 2428 :: Batch 0/1
Batch Loss = 2.9909808149449413
2428, epoch_train_loss=2.9909808149449413
Epoch 2429
Epoch 2429 :: Batch 0/1
Batch Loss = 2.9789952255825836
2429, epoch_train_loss=2.9789952255825836
Epoch 2430
Epoch 2430 :: Batch 0/1
Batch Loss = 2.9943521980802883
2430, epoch_train_loss=2.9943521980802883
Epoch 2431
Epoch 2431 :: Batch 0/1
Batch Loss = 2.9991556009436398
2431, epoch_train_loss=2.9991556009436398
Epoch 2432
Epoch 2432 :: Batch 0/1
Batch Loss = 2.9783831212403107
2432, epoch_train_loss=2.9783831212403107
Epoch 2433
Epoch 2433 :: Batch 0/1
Batch Loss = 2.9904714463782853
2433, epoch_train_loss=2.9904714463782853
Epoch 2434
Epoch 2434 :: Batch 0/1
Batch Loss = 3.0092609064117157
2434, epoch_train_loss=3.0092609064117157
Epoch 2435
Epoch 2435 :: Batch 0/1
Batch Loss = 2.9801102606301604
2435, epoch_train_loss=2.9801102606301604
Epoch 2436
Epoch 2436 :: Batch 0/1
Batch Loss = 2.9846713385306813
2436, epoch_train_loss=2.9846713385306813
Epoch 2437
Epoch 2437 :: Batch 0/1
Batch Loss = 3.008580007062328
2437, epoch_train_loss=3.008580007062328
Epoch 2438
Epoch 2438 :: Batch 0/1
Batch Loss = 2.983529119056767
2438, epoch_train_loss=2.983529119056767
Epoch 2439
Epoch 2439 :: Batch 0/1
Batch Loss = 2.976986668577335
2439, epoch_train_loss=2.976986668577335
Epoch 2440
Epoch 2440 :: Batch 0/1
Batch Loss = 2.990579376855046
2440, epoch_train_loss=2.990579376855046
Epoch 2441
Epoch 2441 :: Batch 0/1
Batch Loss = 2.9900424887529087
2441, epoch_train_loss=2.9900424887529087
Epoch 2442
Epoch 2442 :: Batch 0/1
Batch Loss = 2.9845481875875173
2442, epoch_train_loss=2.9845481875875173
Epoch 2443
Epoch 2443 :: Batch 0/1
Batch Loss = 2.9753850349427733
2443, epoch_train_loss=2.9753850349427733
Epoch 2444
Epoch 2444 :: Batch 0/1
Batch Loss = 2.976813513612598
2444, epoch_train_loss=2.976813513612598
Epoch 2445
Epoch 2445 :: Batch 0/1
Batch Loss = 2.9857814616174507
2445, epoch_train_loss=2.9857814616174507
Epoch 2446
Epoch 2446 :: Batch 0/1
Batch Loss = 2.986132393424963
2446, epoch_train_loss=2.986132393424963
Epoch 2447
Epoch 2447 :: Batch 0/1
Batch Loss = 2.984688492280026
2447, epoch_train_loss=2.984688492280026
Epoch 2448
Epoch 2448 :: Batch 0/1
Batch Loss = 2.9762741414908676
2448, epoch_train_loss=2.9762741414908676
Epoch 2449
Epoch 2449 :: Batch 0/1
Batch Loss = 2.9737013427550836
2449, epoch_train_loss=2.9737013427550836
Epoch 2450
Epoch 2450 :: Batch 0/1
Batch Loss = 2.976369043639139
2450, epoch_train_loss=2.976369043639139
Epoch 2451
Epoch 2451 :: Batch 0/1
Batch Loss = 2.9799418596508422
2451, epoch_train_loss=2.9799418596508422
Epoch 2452
Epoch 2452 :: Batch 0/1
Batch Loss = 2.983016668488862
2452, epoch_train_loss=2.983016668488862
Epoch 2453
Epoch 2453 :: Batch 0/1
Batch Loss = 2.9788255216235417
2453, epoch_train_loss=2.9788255216235417
Epoch 2454
Epoch 2454 :: Batch 0/1
Batch Loss = 2.975635426300276
2454, epoch_train_loss=2.975635426300276
Epoch 2455
Epoch 2455 :: Batch 0/1
Batch Loss = 2.972905504978627
2455, epoch_train_loss=2.972905504978627
Epoch 2456
Epoch 2456 :: Batch 0/1
Batch Loss = 2.9723681545154728
2456, epoch_train_loss=2.9723681545154728
Epoch 2457
Epoch 2457 :: Batch 0/1
Batch Loss = 2.9735749934448177
2457, epoch_train_loss=2.9735749934448177
Epoch 2458
Epoch 2458 :: Batch 0/1
Batch Loss = 2.9757830511773427
2458, epoch_train_loss=2.9757830511773427
Epoch 2459
Epoch 2459 :: Batch 0/1
Batch Loss = 2.980189012187932
2459, epoch_train_loss=2.980189012187932
Epoch 2460
Epoch 2460 :: Batch 0/1
Batch Loss = 2.982152128061384
2460, epoch_train_loss=2.982152128061384
Epoch 2461
Epoch 2461 :: Batch 0/1
Batch Loss = 2.988816544405569
2461, epoch_train_loss=2.988816544405569
Epoch 2462
Epoch 2462 :: Batch 0/1
Batch Loss = 2.9843442748363187
2462, epoch_train_loss=2.9843442748363187
Epoch 2463
Epoch 2463 :: Batch 0/1
Batch Loss = 2.9848615970066423
2463, epoch_train_loss=2.9848615970066423
Epoch 2464
Epoch 2464 :: Batch 0/1
Batch Loss = 2.9779760392519217
2464, epoch_train_loss=2.9779760392519217
Epoch 2465
Epoch 2465 :: Batch 0/1
Batch Loss = 2.974679501944313
2465, epoch_train_loss=2.974679501944313
Epoch 2466
Epoch 2466 :: Batch 0/1
Batch Loss = 2.9718681071984787
2466, epoch_train_loss=2.9718681071984787
Epoch 2467
Epoch 2467 :: Batch 0/1
Batch Loss = 2.970695589842057
2467, epoch_train_loss=2.970695589842057
Epoch 2468
Epoch 2468 :: Batch 0/1
Batch Loss = 2.970674941230517
2468, epoch_train_loss=2.970674941230517
Epoch 2469
Epoch 2469 :: Batch 0/1
Batch Loss = 2.9715865917016395
2469, epoch_train_loss=2.9715865917016395
Epoch 2470
Epoch 2470 :: Batch 0/1
Batch Loss = 2.9742281342818657
2470, epoch_train_loss=2.9742281342818657
Epoch 2471
Epoch 2471 :: Batch 0/1
Batch Loss = 2.978938574097307
2471, epoch_train_loss=2.978938574097307
Epoch 2472
Epoch 2472 :: Batch 0/1
Batch Loss = 2.9939754269005956
2472, epoch_train_loss=2.9939754269005956
Epoch 2473
Epoch 2473 :: Batch 0/1
Batch Loss = 2.999611222938948
2473, epoch_train_loss=2.999611222938948
Epoch 2474
Epoch 2474 :: Batch 0/1
Batch Loss = 3.024661399137519
2474, epoch_train_loss=3.024661399137519
Epoch 2475
Epoch 2475 :: Batch 0/1
Batch Loss = 2.9835720068889593
2475, epoch_train_loss=2.9835720068889593
Epoch 2476
Epoch 2476 :: Batch 0/1
Batch Loss = 2.971215992397652
2476, epoch_train_loss=2.971215992397652
Epoch 2477
Epoch 2477 :: Batch 0/1
Batch Loss = 2.9792212427673355
2477, epoch_train_loss=2.9792212427673355
Epoch 2478
Epoch 2478 :: Batch 0/1
Batch Loss = 2.99177452312598
2478, epoch_train_loss=2.99177452312598
Epoch 2479
Epoch 2479 :: Batch 0/1
Batch Loss = 3.0052230508741897
2479, epoch_train_loss=3.0052230508741897
Epoch 2480
Epoch 2480 :: Batch 0/1
Batch Loss = 2.9805953350826373
2480, epoch_train_loss=2.9805953350826373
Epoch 2481
Epoch 2481 :: Batch 0/1
Batch Loss = 2.9706757300865663
2481, epoch_train_loss=2.9706757300865663
Epoch 2482
Epoch 2482 :: Batch 0/1
Batch Loss = 2.97645365252193
2482, epoch_train_loss=2.97645365252193
Epoch 2483
Epoch 2483 :: Batch 0/1
Batch Loss = 2.987540884680912
2483, epoch_train_loss=2.987540884680912
Epoch 2484
Epoch 2484 :: Batch 0/1
Batch Loss = 2.99492779052589
2484, epoch_train_loss=2.99492779052589
Epoch 2485
Epoch 2485 :: Batch 0/1
Batch Loss = 2.9792135380187386
2485, epoch_train_loss=2.9792135380187386
Epoch 2486
Epoch 2486 :: Batch 0/1
Batch Loss = 2.970728676990036
2486, epoch_train_loss=2.970728676990036
Epoch 2487
Epoch 2487 :: Batch 0/1
Batch Loss = 2.973042366845978
2487, epoch_train_loss=2.973042366845978
Epoch 2488
Epoch 2488 :: Batch 0/1
Batch Loss = 2.9803943813116875
2488, epoch_train_loss=2.9803943813116875
Epoch 2489
Epoch 2489 :: Batch 0/1
Batch Loss = 2.9844765542187743
2489, epoch_train_loss=2.9844765542187743
Epoch 2490
Epoch 2490 :: Batch 0/1
Batch Loss = 2.976496530203235
2490, epoch_train_loss=2.976496530203235
Epoch 2491
Epoch 2491 :: Batch 0/1
Batch Loss = 2.9704900085325656
2491, epoch_train_loss=2.9704900085325656
Epoch 2492
Epoch 2492 :: Batch 0/1
Batch Loss = 2.969693704857018
2492, epoch_train_loss=2.969693704857018
Epoch 2493
Epoch 2493 :: Batch 0/1
Batch Loss = 2.9732272901800902
2493, epoch_train_loss=2.9732272901800902
Epoch 2494
Epoch 2494 :: Batch 0/1
Batch Loss = 2.9789314410277505
2494, epoch_train_loss=2.9789314410277505
Epoch 2495
Epoch 2495 :: Batch 0/1
Batch Loss = 2.9804259495992915
2495, epoch_train_loss=2.9804259495992915
Epoch 2496
Epoch 2496 :: Batch 0/1
Batch Loss = 2.9810871439671898
2496, epoch_train_loss=2.9810871439671898
Epoch 2497
Epoch 2497 :: Batch 0/1
Batch Loss = 2.9758584590981854
2497, epoch_train_loss=2.9758584590981854
Epoch 2498
Epoch 2498 :: Batch 0/1
Batch Loss = 2.9716845240978813
2498, epoch_train_loss=2.9716845240978813
Epoch 2499
Epoch 2499 :: Batch 0/1
Batch Loss = 2.9684784806123794
2499, epoch_train_loss=2.9684784806123794
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting "B3LYP_WITH_VWN5 = True" in pyscf_conf.py
  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '
Warning: <pyscf.gto.mole.Mole object at 0x7ffef00426e0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef00426e0> in UKS object of <class 'pyscf.dft.uks.UKS'>
<pyscf.gto.mole.Mole object at 0x7ffef00426e0> [['P', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffef0042d70> [['N', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffef00433d0> [['H', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffef0042a40> [['Li', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffef00424d0> [['O', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffef0042110> [['Cl', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffef0042650> [['Al', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffef0042200> [['S', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffef00417e0> [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef0041a80> [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef0041a20> [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef0040190> [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]] 3
<pyscf.gto.mole.Mole object at 0x7ffef0040af0> [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]] 3
<pyscf.gto.mole.Mole object at 0x7ffef0040d60> [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef0040e20> [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef00418a0> [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef0040a60> [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]] 4
<pyscf.gto.mole.Mole object at 0x7ffef00432b0> [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef0043070> [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef00435b0> [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef0041f60> [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef0042b60> [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]] 4
<pyscf.gto.mole.Mole object at 0x7ffef0043280> [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]] 4
<pyscf.gto.mole.Mole object at 0x7ffef0043940> [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef0043430> [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]] 5
<pyscf.gto.mole.Mole object at 0x7ffef0043d30> [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]] 3
<pyscf.gto.mole.Mole object at 0x7ffef0043d60> [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]] 5
mol:  [['P', array([0., 0., 0.])]]
converged SCF energy = -341.104145992717  <S^2> = 3.7502984  2S+1 = 4.0001492
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0042d70> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0042d70> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-3.47389956e-03 -8.82676818e-04 -2.08411238e-03 ... -1.11301603e+01
 -1.11301603e+01 -1.11301603e+01] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 3)
mol:  [['N', array([0., 0., 0.])]]
converged SCF energy = -54.5289742046675  <S^2> = 3.7524945  2S+1 = 4.0012471
Warning: <pyscf.gto.mole.Mole object at 0x7ffef00433d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef00433d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5016, 22)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.07670570e-03 -5.92340671e-04 -6.66573372e-05 ... -5.03679786e+00
 -5.03679786e+00 -5.03679786e+00] = SCAN,
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5016, 22)
rho shape (4, 4, 2, 5016)
rho_filt shape: (5016,)
get descriptors tdrho.shape=(5016, 3)
mol:  [['H', array([0., 0., 0.])]]
converged SCF energy = -0.49981298400854  <S^2> = 0.75  2S+1 = 2
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0042a40> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0042a40> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 7, 7)
ao.shape (10, 2440, 7)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.59173730e-03 -7.67300285e-04 -6.09330303e-05 ... -7.27736608e-01
 -7.27736608e-01 -7.27736608e-01] = SCAN,
get_data, dm shape = (2, 7, 7)
ao_eval.shape=(4, 2440, 7)
rho shape (4, 4, 2, 2440)
rho_filt shape: (2440,)
get descriptors tdrho.shape=(2440, 3)
mol:  [['Li', array([0., 0., 0.])]]
converged SCF energy = -7.46006188627842  <S^2> = 0.75000049  2S+1 = 2.0000005
Warning: <pyscf.gto.mole.Mole object at 0x7ffef00424d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef00424d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 4592, 22)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-6.71503005e-03 -1.44519923e-03 -1.44519923e-03 ... -1.46899070e-02
 -2.03947707e+00 -2.03947707e+00] = SCAN,
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 4592, 22)
rho shape (4, 4, 2, 4592)
rho_filt shape: (4592,)
get descriptors tdrho.shape=(4592, 3)
mol:  [['O', array([0., 0., 0.])]]
converged SCF energy = -75.003377443081  <S^2> = 2.0027453  2S+1 = 3.0018296
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0042110> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0042110> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5040, 22)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-7.50108809e-04 -1.24504028e-04 -6.21637250e-06 ... -5.78449381e+00
 -5.78449381e+00 -5.78449381e+00] = SCAN,
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5040, 22)
rho shape (4, 4, 2, 5040)
rho_filt shape: (5040,)
get descriptors tdrho.shape=(5040, 3)
mol:  [['Cl', array([0., 0., 0.])]]
converged SCF energy = -459.957577121376  <S^2> = 0.7516194  2S+1 = 2.0016187
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0042650> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0042650> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6152, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.06046800e-03 -9.79646678e-04 -3.40075764e-04 ... -1.26648275e+01
 -1.26648275e+01 -1.26648275e+01] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6152, 30)
rho shape (4, 4, 2, 6152)
rho_filt shape: (6152,)
get descriptors tdrho.shape=(6152, 3)
mol:  [['Al', array([0., 0., 0.])]]
converged SCF energy = -242.226560983335  <S^2> = 0.75226414  2S+1 = 2.0022629
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0042200> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0042200> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6088, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-0.02415715 -0.01560148 -0.00795622 ... -0.000139   -0.00176837
 -0.00012769] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6088, 30)
rho shape (4, 4, 2, 6088)
rho_filt shape: (6088,)
get descriptors tdrho.shape=(6088, 3)
mol:  [['S', array([0., 0., 0.])]]
converged SCF energy = -397.938786821141  <S^2> = 2.0022329  2S+1 = 3.0014882
Warning: <pyscf.gto.mole.Mole object at 0x7ffef00417e0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef00417e0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.08470554e-03 -9.04965301e-04 -9.58520064e-04 ... -1.18986567e+01
 -1.18986567e+01 -1.18986567e+01] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 3)
mol:  [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]]
converged SCF energy = -1.16580491182912  <S^2> = -4.4408921e-16  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0041a80> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0041a80> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 14, 14)
ao.shape (10, 4776, 14)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.31557088e-04 -9.73828620e-06 -3.66768667e-04 ... -5.54165574e-01
 -5.54165574e-01 -5.54165574e-01] = SCAN,
get_data, dm shape = (2, 14, 14)
ao_eval.shape=(4, 4776, 14)
rho shape (4, 4, 2, 4776)
rho_filt shape: (4776,)
get descriptors tdrho.shape=(4776, 3)
mol:  [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]]
converged SCF energy = -109.439263799566  <S^2> = 1.0658141e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0041a20> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0041a20> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9848, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-9.68474977e-05 -9.84742592e-04 -2.59676393e-04 ... -2.39645778e-05
 -2.39645778e-05 -9.68474977e-05] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9848, 44)
rho shape (4, 4, 2, 9848)
rho_filt shape: (9848,)
get descriptors tdrho.shape=(9848, 3)
mol:  [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]]
converged SCF energy = -107.339357395734  <S^2> = -3.5527137e-15  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0040190> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0040190> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9752, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.04987770e-03 -6.68954111e-04 -8.57556562e-04 ... -1.07485605e-03
 -8.01425702e-01 -8.01425702e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9752, 44)
rho shape (4, 4, 2, 9752)
rho_filt shape: (9752,)
get descriptors tdrho.shape=(9752, 3)
mol:  [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]]
converged SCF energy = -93.3377924465132  <S^2> = 4.0072834e-10  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0040af0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0040af0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 12256, 51)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.97917639e-04 -2.54437615e-05 -3.15202008e-05 ... -6.37386388e-01
 -6.37386388e-01 -6.37386388e-01] = SCAN,
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 12256, 51)
rho shape (4, 4, 2, 12256)
rho_filt shape: (12256,)
get descriptors tdrho.shape=(12256, 3)
mol:  [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]]
converged SCF energy = -188.456965322844  <S^2> = 2.3092639e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0040d60> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0040d60> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 66, 66)
ao.shape (10, 14920, 66)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.50217343e-04 -2.07520331e-04 -9.23619961e-04 ... -2.76182455e-06
 -4.27559894e+00 -4.27559894e+00] = SCAN,
get_data, dm shape = (2, 66, 66)
ao_eval.shape=(4, 14920, 66)
rho shape (4, 4, 2, 14920)
rho_filt shape: (14920,)
get descriptors tdrho.shape=(14920, 3)
mol:  [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]]
converged SCF energy = -920.005608888959  <S^2> = 5.0093263e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0040e20> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0040e20> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12208, 60)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-0.00155834 -0.00091688 -0.00215831 ... -0.00091688 -0.41618506
 -0.41618506] = SCAN,
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12208, 60)
rho shape (4, 4, 2, 12208)
rho_filt shape: (12208,)
get descriptors tdrho.shape=(12208, 3)
mol:  [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]]
converged SCF energy = -199.394370591172  <S^2> = 1.1901591e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef00418a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef00418a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9824, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.92948752e-04 -1.95215890e-05 -1.16699780e-03 ... -4.89378326e-01
 -4.89378326e-01 -4.89378326e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9824, 44)
rho shape (4, 4, 2, 9824)
rho_filt shape: (9824,)
get descriptors tdrho.shape=(9824, 3)
mol:  [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]]
converged SCF energy = -150.214894541876  <S^2> = 1.0018599  2S+1 = 2.2377309
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0040a60> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0040a60> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9912, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-6.48686938e-04 -1.14848257e-04 -5.90272062e-06 ... -6.59150583e-01
 -6.59150583e-01 -6.59150583e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9912, 44)
rho shape (4, 4, 2, 9912)
rho_filt shape: (9912,)
get descriptors tdrho.shape=(9912, 3)
mol:  [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]]
converged SCF energy = -77.2435048346375  <S^2> = 8.8817842e-15  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef00432b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef00432b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 58, 58)
ao.shape (10, 15208, 58)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-8.83278187e-05 -8.83278187e-05 -9.75839793e-04 ... -3.46740731e-05
 -3.31729009e-05 -3.31729009e-05] = SCAN,
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15208, 58)
rho shape (4, 4, 2, 15208)
rho_filt shape: (15208,)
get descriptors tdrho.shape=(15208, 3)
mol:  [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]]
converged SCF energy = -113.221335689652  <S^2> = 6.5281114e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0043070> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0043070> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 10040, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-5.37000596e-04 -8.55494373e-04 -2.46853248e-03 ... -7.34251993e-01
 -7.34251993e-01 -7.34251993e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 10040, 44)
rho shape (4, 4, 2, 10040)
rho_filt shape: (10040,)
get descriptors tdrho.shape=(10040, 3)
mol:  [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]]
converged SCF energy = -460.624592374078  <S^2> = 6.2172489e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef00435b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef00435b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 37, 37)
ao.shape (10, 8552, 37)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.38161478e-04 -1.81223966e-05 -2.37327566e-05 ... -2.83738108e+00
 -2.83738108e+00 -2.83738108e+00] = SCAN,
get_data, dm shape = (2, 37, 37)
ao_eval.shape=(4, 8552, 37)
rho shape (4, 4, 2, 8552)
rho_filt shape: (8552,)
get descriptors tdrho.shape=(8552, 3)
mol:  [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]]
converged SCF energy = -8.04458854018922  <S^2> = 7.7049478e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0041f60> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0041f60> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 29, 29)
ao.shape (10, 6936, 29)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-0.00113445 -0.00118624 -0.00118624 ... -0.48434639 -0.48434639
 -0.48434639] = SCAN,
get_data, dm shape = (2, 29, 29)
ao_eval.shape=(4, 6936, 29)
rho shape (4, 4, 2, 6936)
rho_filt shape: (6936,)
get descriptors tdrho.shape=(6936, 3)
mol:  [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]]
converged SCF energy = -324.340512506578  <S^2> = 1.5868196e-11  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0042b60> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0042b60> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 11536, 60)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-0.00297936 -0.00297936 -0.00407091 ... -0.00297936 -0.00297936
 -0.00407091] = SCAN,
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 11536, 60)
rho shape (4, 4, 2, 11536)
rho_filt shape: (11536,)
get descriptors tdrho.shape=(11536, 3)
mol:  [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]]
converged SCF energy = -1622.57507845815  <S^2> = 8.31335e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0043280> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0043280> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 120, 120)
ao.shape (10, 24512, 120)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.61401455e-04 -4.90485117e-04 -2.56451688e-03 ... -9.59296114e+00
 -9.59296114e+00 -9.59296114e+00] = SCAN,
get_data, dm shape = (2, 120, 120)
ao_eval.shape=(4, 24512, 120)
rho shape (4, 4, 2, 24512)
rho_filt shape: (24512,)
get descriptors tdrho.shape=(24512, 3)
mol:  [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]]
converged SCF energy = -342.979728469575  <S^2> = 2.5387692e-11  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0043940> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0043940> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 13096, 51)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.28637187e-03 -4.32380890e-04 -3.74072638e-05 ... -1.91722763e+00
 -1.91722763e+00 -1.91722763e+00] = SCAN,
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 13096, 51)
rho shape (4, 4, 2, 13096)
rho_filt shape: (13096,)
get descriptors tdrho.shape=(13096, 3)
mol:  [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]]
converged SCF energy = -578.565336647371  <S^2> = 1.0034707  2S+1 = 2.2391701
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0043430> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0043430> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12384, 60)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.60118839e-04 -2.60152115e-04 -2.60145915e-04 ... -3.86943823e-01
 -3.86943823e-01 -3.86943823e-01] = SCAN,
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12384, 60)
rho shape (4, 4, 2, 12384)
rho_filt shape: (12384,)
get descriptors tdrho.shape=(12384, 3)
mol:  [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]]
converged SCF energy = -40.4598214864075  <S^2> = 3.1974423e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0043d30> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0043d30> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 50, 50)
ao.shape (10, 13936, 50)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-8.68439856e-04 -2.42462783e-04 -1.69965237e-05 ... -2.55256081e-05
 -2.55256081e-05 -2.55256081e-05] = SCAN,
get_data, dm shape = (2, 50, 50)
ao_eval.shape=(4, 13936, 50)
rho shape (4, 4, 2, 13936)
rho_filt shape: (13936,)
get descriptors tdrho.shape=(13936, 3)
mol:  [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]]
converged SCF energy = -39.0756147483503  <S^2> = 6.2017058e-12  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0043d60> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0043d60> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 36, 36)
ao.shape (10, 9656, 36)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-7.67691257e-04 -4.57409182e-05 -2.02835243e-04 ... -1.14928928e+00
 -1.14928928e+00 -1.14928928e+00] = SCAN,
get_data, dm shape = (2, 36, 36)
ao_eval.shape=(4, 9656, 36)
rho shape (4, 4, 2, 9656)
rho_filt shape: (9656,)
get descriptors tdrho.shape=(9656, 3)
mol:  [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]]
converged SCF energy = -291.719272437818  <S^2> = 1.3153922e-11  2S+1 = 1
New DM shape: (2, 58, 58)
ao.shape (10, 15256, 58)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-8.33847724e-04 -2.34902391e-04 -1.75660753e-05 ... -1.92925750e-05
 -1.92925750e-05 -1.92925750e-05] = SCAN,
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15256, 58)
rho shape (4, 4, 2, 15256)
rho_filt shape: (15256,)
get descriptors tdrho.shape=(15256, 3)
PRE NAN FILT: tFxc.shape=(237015,), tdrho.shape=(237015, 3)
nan_filt_rho.shape=(237015,)
nan_filt_fxc.shape=(237015,)
tFxc.shape=(237015,), tdrho.shape=(237015, 3)
inp[0].shape = (237015, 3)
Epoch 0
Epoch 0 :: Batch 0/1
Batch Loss = 5.264168430749761
0, epoch_train_loss=5.264168430749761
Epoch 1
Epoch 1 :: Batch 0/1
Batch Loss = 4.842611944604443
1, epoch_train_loss=4.842611944604443
Epoch 2
Epoch 2 :: Batch 0/1
Batch Loss = 4.260537727536809
2, epoch_train_loss=4.260537727536809
Epoch 3
Epoch 3 :: Batch 0/1
Batch Loss = 3.287587854363113
3, epoch_train_loss=3.287587854363113
Epoch 4
Epoch 4 :: Batch 0/1
Batch Loss = 1.9199244450613997
4, epoch_train_loss=1.9199244450613997
Epoch 5
Epoch 5 :: Batch 0/1
Batch Loss = 1.1158059126402138
5, epoch_train_loss=1.1158059126402138
Epoch 6
Epoch 6 :: Batch 0/1
Batch Loss = 1.8611285689622656
6, epoch_train_loss=1.8611285689622656
Epoch 7
Epoch 7 :: Batch 0/1
Batch Loss = 0.9068584823954158
7, epoch_train_loss=0.9068584823954158
Epoch 8
Epoch 8 :: Batch 0/1
Batch Loss = 0.46517238455863585
8, epoch_train_loss=0.46517238455863585
Epoch 9
Epoch 9 :: Batch 0/1
Batch Loss = 0.8194326463523506
9, epoch_train_loss=0.8194326463523506
Epoch 10
Epoch 10 :: Batch 0/1
Batch Loss = 0.8013306457988211
10, epoch_train_loss=0.8013306457988211
Epoch 11
Epoch 11 :: Batch 0/1
Batch Loss = 0.4861076527408097
11, epoch_train_loss=0.4861076527408097
Epoch 12
Epoch 12 :: Batch 0/1
Batch Loss = 0.23691950274132825
12, epoch_train_loss=0.23691950274132825
Epoch 13
Epoch 13 :: Batch 0/1
Batch Loss = 0.2752623086448307
13, epoch_train_loss=0.2752623086448307
Epoch 14
Epoch 14 :: Batch 0/1
Batch Loss = 0.43282922053774014
14, epoch_train_loss=0.43282922053774014
Epoch 15
Epoch 15 :: Batch 0/1
Batch Loss = 0.3327205661050184
15, epoch_train_loss=0.3327205661050184
Epoch 16
Epoch 16 :: Batch 0/1
Batch Loss = 0.18177770928355802
16, epoch_train_loss=0.18177770928355802
Epoch 17
Epoch 17 :: Batch 0/1
Batch Loss = 0.2183379112253501
17, epoch_train_loss=0.2183379112253501
Epoch 18
Epoch 18 :: Batch 0/1
Batch Loss = 0.2877038170341054
18, epoch_train_loss=0.2877038170341054
Epoch 19
Epoch 19 :: Batch 0/1
Batch Loss = 0.2504436233664692
19, epoch_train_loss=0.2504436233664692
Epoch 20
Epoch 20 :: Batch 0/1
Batch Loss = 0.15884962764334876
20, epoch_train_loss=0.15884962764334876
Epoch 21
Epoch 21 :: Batch 0/1
Batch Loss = 0.12971795794529217
21, epoch_train_loss=0.12971795794529217
Epoch 22
Epoch 22 :: Batch 0/1
Batch Loss = 0.16801183863796376
22, epoch_train_loss=0.16801183863796376
Epoch 23
Epoch 23 :: Batch 0/1
Batch Loss = 0.15697890533632658
23, epoch_train_loss=0.15697890533632658
Epoch 24
Epoch 24 :: Batch 0/1
Batch Loss = 0.0978436717223136
24, epoch_train_loss=0.0978436717223136
Epoch 25
Epoch 25 :: Batch 0/1
Batch Loss = 0.09269149188862225
25, epoch_train_loss=0.09269149188862225
Epoch 26
Epoch 26 :: Batch 0/1
Batch Loss = 0.10929786859063678
26, epoch_train_loss=0.10929786859063678
Epoch 27
Epoch 27 :: Batch 0/1
Batch Loss = 0.08491006164007338
27, epoch_train_loss=0.08491006164007338
Epoch 28
Epoch 28 :: Batch 0/1
Batch Loss = 0.06297634187157407
28, epoch_train_loss=0.06297634187157407
Epoch 29
Epoch 29 :: Batch 0/1
Batch Loss = 0.0641360975610738
29, epoch_train_loss=0.0641360975610738
Epoch 30
Epoch 30 :: Batch 0/1
Batch Loss = 0.06551933550155421
30, epoch_train_loss=0.06551933550155421
Epoch 31
Epoch 31 :: Batch 0/1
Batch Loss = 0.05256026552777695
31, epoch_train_loss=0.05256026552777695
Epoch 32
Epoch 32 :: Batch 0/1
Batch Loss = 0.04966140065035392
32, epoch_train_loss=0.04966140065035392
Epoch 33
Epoch 33 :: Batch 0/1
Batch Loss = 0.04923275431507563
33, epoch_train_loss=0.04923275431507563
Epoch 34
Epoch 34 :: Batch 0/1
Batch Loss = 0.0372179039019891
34, epoch_train_loss=0.0372179039019891
Epoch 35
Epoch 35 :: Batch 0/1
Batch Loss = 0.041236076349237295
35, epoch_train_loss=0.041236076349237295
Epoch 36
Epoch 36 :: Batch 0/1
Batch Loss = 0.040124755953567605
36, epoch_train_loss=0.040124755953567605
Epoch 37
Epoch 37 :: Batch 0/1
Batch Loss = 0.028120065986711924
37, epoch_train_loss=0.028120065986711924
Epoch 38
Epoch 38 :: Batch 0/1
Batch Loss = 0.026660643804528678
38, epoch_train_loss=0.026660643804528678
Epoch 39
Epoch 39 :: Batch 0/1
Batch Loss = 0.032530367021770645
39, epoch_train_loss=0.032530367021770645
Epoch 40
Epoch 40 :: Batch 0/1
Batch Loss = 0.028962472676195124
40, epoch_train_loss=0.028962472676195124
Epoch 41
Epoch 41 :: Batch 0/1
Batch Loss = 0.019705632560211083
41, epoch_train_loss=0.019705632560211083
Epoch 42
Epoch 42 :: Batch 0/1
Batch Loss = 0.022542999737530812
42, epoch_train_loss=0.022542999737530812
Epoch 43
Epoch 43 :: Batch 0/1
Batch Loss = 0.025191067940182253
43, epoch_train_loss=0.025191067940182253
Epoch 44
Epoch 44 :: Batch 0/1
Batch Loss = 0.015759911376571827
44, epoch_train_loss=0.015759911376571827
Epoch 45
Epoch 45 :: Batch 0/1
Batch Loss = 0.013651682772940174
45, epoch_train_loss=0.013651682772940174
Epoch 46
Epoch 46 :: Batch 0/1
Batch Loss = 0.017282922052917298
46, epoch_train_loss=0.017282922052917298
Epoch 47
Epoch 47 :: Batch 0/1
Batch Loss = 0.014624243681895419
47, epoch_train_loss=0.014624243681895419
Epoch 48
Epoch 48 :: Batch 0/1
Batch Loss = 0.012675937488693716
48, epoch_train_loss=0.012675937488693716
Epoch 49
Epoch 49 :: Batch 0/1
Batch Loss = 0.011971975188403657
49, epoch_train_loss=0.011971975188403657
Epoch 50
Epoch 50 :: Batch 0/1
Batch Loss = 0.011775303399553377
50, epoch_train_loss=0.011775303399553377
Epoch 51
Epoch 51 :: Batch 0/1
Batch Loss = 0.013148078182381572
51, epoch_train_loss=0.013148078182381572
Epoch 52
Epoch 52 :: Batch 0/1
Batch Loss = 0.010179074942669909
52, epoch_train_loss=0.010179074942669909
Epoch 53
Epoch 53 :: Batch 0/1
Batch Loss = 0.009163656936372347
53, epoch_train_loss=0.009163656936372347
Epoch 54
Epoch 54 :: Batch 0/1
Batch Loss = 0.011714456783138831
54, epoch_train_loss=0.011714456783138831
Epoch 55
Epoch 55 :: Batch 0/1
Batch Loss = 0.008957871781227974
55, epoch_train_loss=0.008957871781227974
Epoch 56
Epoch 56 :: Batch 0/1
Batch Loss = 0.0076245620459814995
56, epoch_train_loss=0.0076245620459814995
Epoch 57
Epoch 57 :: Batch 0/1
Batch Loss = 0.008729032546093678
57, epoch_train_loss=0.008729032546093678
Epoch 58
Epoch 58 :: Batch 0/1
Batch Loss = 0.008172442397735949
58, epoch_train_loss=0.008172442397735949
Epoch 59
Epoch 59 :: Batch 0/1
Batch Loss = 0.007677612069090301
59, epoch_train_loss=0.007677612069090301
Epoch 60
Epoch 60 :: Batch 0/1
Batch Loss = 0.006424251548927065
60, epoch_train_loss=0.006424251548927065
Epoch 61
Epoch 61 :: Batch 0/1
Batch Loss = 0.006726874779509375
61, epoch_train_loss=0.006726874779509375
Epoch 62
Epoch 62 :: Batch 0/1
Batch Loss = 0.007400901982000552
62, epoch_train_loss=0.007400901982000552
Epoch 63
Epoch 63 :: Batch 0/1
Batch Loss = 0.005798124001216757
63, epoch_train_loss=0.005798124001216757
Epoch 64
Epoch 64 :: Batch 0/1
Batch Loss = 0.005881218473708902
64, epoch_train_loss=0.005881218473708902
Epoch 65
Epoch 65 :: Batch 0/1
Batch Loss = 0.0062777409925462956
65, epoch_train_loss=0.0062777409925462956
Epoch 66
Epoch 66 :: Batch 0/1
Batch Loss = 0.00564356408862438
66, epoch_train_loss=0.00564356408862438
Epoch 67
Epoch 67 :: Batch 0/1
Batch Loss = 0.005309046000708101
67, epoch_train_loss=0.005309046000708101
Epoch 68
Epoch 68 :: Batch 0/1
Batch Loss = 0.004861280208348285
68, epoch_train_loss=0.004861280208348285
Epoch 69
Epoch 69 :: Batch 0/1
Batch Loss = 0.005372515346634793
69, epoch_train_loss=0.005372515346634793
Epoch 70
Epoch 70 :: Batch 0/1
Batch Loss = 0.004799488123500118
70, epoch_train_loss=0.004799488123500118
Epoch 71
Epoch 71 :: Batch 0/1
Batch Loss = 0.004345128897236065
71, epoch_train_loss=0.004345128897236065
Epoch 72
Epoch 72 :: Batch 0/1
Batch Loss = 0.00480895717802759
72, epoch_train_loss=0.00480895717802759
Epoch 73
Epoch 73 :: Batch 0/1
Batch Loss = 0.004411857697913576
73, epoch_train_loss=0.004411857697913576
Epoch 74
Epoch 74 :: Batch 0/1
Batch Loss = 0.004246086452653641
74, epoch_train_loss=0.004246086452653641
Epoch 75
Epoch 75 :: Batch 0/1
Batch Loss = 0.004162073255441237
75, epoch_train_loss=0.004162073255441237
Epoch 76
Epoch 76 :: Batch 0/1
Batch Loss = 0.004263656992201127
76, epoch_train_loss=0.004263656992201127
Epoch 77
Epoch 77 :: Batch 0/1
Batch Loss = 0.00395783405182726
77, epoch_train_loss=0.00395783405182726
Epoch 78
Epoch 78 :: Batch 0/1
Batch Loss = 0.0037883774628827546
78, epoch_train_loss=0.0037883774628827546
Epoch 79
Epoch 79 :: Batch 0/1
Batch Loss = 0.004018765182683092
79, epoch_train_loss=0.004018765182683092
Epoch 80
Epoch 80 :: Batch 0/1
Batch Loss = 0.003657718874449423
80, epoch_train_loss=0.003657718874449423
Epoch 81
Epoch 81 :: Batch 0/1
Batch Loss = 0.003648785999509069
81, epoch_train_loss=0.003648785999509069
Epoch 82
Epoch 82 :: Batch 0/1
Batch Loss = 0.0036304712349884403
82, epoch_train_loss=0.0036304712349884403
Epoch 83
Epoch 83 :: Batch 0/1
Batch Loss = 0.0036017256764745587
83, epoch_train_loss=0.0036017256764745587
Epoch 84
Epoch 84 :: Batch 0/1
Batch Loss = 0.003439650743977725
84, epoch_train_loss=0.003439650743977725
Epoch 85
Epoch 85 :: Batch 0/1
Batch Loss = 0.0034987340926678303
85, epoch_train_loss=0.0034987340926678303
Epoch 86
Epoch 86 :: Batch 0/1
Batch Loss = 0.0034589458124062327
86, epoch_train_loss=0.0034589458124062327
Epoch 87
Epoch 87 :: Batch 0/1
Batch Loss = 0.0032894687473175284
87, epoch_train_loss=0.0032894687473175284
Epoch 88
Epoch 88 :: Batch 0/1
Batch Loss = 0.0033786012708796305
88, epoch_train_loss=0.0033786012708796305
Epoch 89
Epoch 89 :: Batch 0/1
Batch Loss = 0.0032877759334124785
89, epoch_train_loss=0.0032877759334124785
Epoch 90
Epoch 90 :: Batch 0/1
Batch Loss = 0.0032432164502961386
90, epoch_train_loss=0.0032432164502961386
Epoch 91
Epoch 91 :: Batch 0/1
Batch Loss = 0.003216466485964104
91, epoch_train_loss=0.003216466485964104
Epoch 92
Epoch 92 :: Batch 0/1
Batch Loss = 0.0032326871364667647
92, epoch_train_loss=0.0032326871364667647
Epoch 93
Epoch 93 :: Batch 0/1
Batch Loss = 0.003124145354219701
93, epoch_train_loss=0.003124145354219701
Epoch 94
Epoch 94 :: Batch 0/1
Batch Loss = 0.003168505443701392
94, epoch_train_loss=0.003168505443701392
Epoch 95
Epoch 95 :: Batch 0/1
Batch Loss = 0.0031219981261837123
95, epoch_train_loss=0.0031219981261837123
Epoch 96
Epoch 96 :: Batch 0/1
Batch Loss = 0.0030779608280918033
96, epoch_train_loss=0.0030779608280918033
Epoch 97
Epoch 97 :: Batch 0/1
Batch Loss = 0.003084333598488895
97, epoch_train_loss=0.003084333598488895
Epoch 98
Epoch 98 :: Batch 0/1
Batch Loss = 0.0030518526149054784
98, epoch_train_loss=0.0030518526149054784
Epoch 99
Epoch 99 :: Batch 0/1
Batch Loss = 0.0030133181086865186
99, epoch_train_loss=0.0030133181086865186
Epoch 100
Epoch 100 :: Batch 0/1
Batch Loss = 0.003020015061210859
100, epoch_train_loss=0.003020015061210859
Epoch 101
Epoch 101 :: Batch 0/1
Batch Loss = 0.0029916956215732685
101, epoch_train_loss=0.0029916956215732685
Epoch 102
Epoch 102 :: Batch 0/1
Batch Loss = 0.002967208779264172
102, epoch_train_loss=0.002967208779264172
Epoch 103
Epoch 103 :: Batch 0/1
Batch Loss = 0.0029753543498933547
103, epoch_train_loss=0.0029753543498933547
Epoch 104
Epoch 104 :: Batch 0/1
Batch Loss = 0.0029345964385568126
104, epoch_train_loss=0.0029345964385568126
Epoch 105
Epoch 105 :: Batch 0/1
Batch Loss = 0.0029338059123324646
105, epoch_train_loss=0.0029338059123324646
Epoch 106
Epoch 106 :: Batch 0/1
Batch Loss = 0.00292092417302796
106, epoch_train_loss=0.00292092417302796
Epoch 107
Epoch 107 :: Batch 0/1
Batch Loss = 0.0028988180152809974
107, epoch_train_loss=0.0028988180152809974
Epoch 108
Epoch 108 :: Batch 0/1
Batch Loss = 0.0028934892609085937
108, epoch_train_loss=0.0028934892609085937
Epoch 109
Epoch 109 :: Batch 0/1
Batch Loss = 0.002881343330291263
109, epoch_train_loss=0.002881343330291263
Epoch 110
Epoch 110 :: Batch 0/1
Batch Loss = 0.0028623890569313467
110, epoch_train_loss=0.0028623890569313467
Epoch 111
Epoch 111 :: Batch 0/1
Batch Loss = 0.00286430872401898
111, epoch_train_loss=0.00286430872401898
Epoch 112
Epoch 112 :: Batch 0/1
Batch Loss = 0.002843192579372503
112, epoch_train_loss=0.002843192579372503
Epoch 113
Epoch 113 :: Batch 0/1
Batch Loss = 0.002839670658595629
113, epoch_train_loss=0.002839670658595629
Epoch 114
Epoch 114 :: Batch 0/1
Batch Loss = 0.0028301194914717737
114, epoch_train_loss=0.0028301194914717737
Epoch 115
Epoch 115 :: Batch 0/1
Batch Loss = 0.00281595107133103
115, epoch_train_loss=0.00281595107133103
Epoch 116
Epoch 116 :: Batch 0/1
Batch Loss = 0.0028129602951132266
116, epoch_train_loss=0.0028129602951132266
Epoch 117
Epoch 117 :: Batch 0/1
Batch Loss = 0.0028001338046006415
117, epoch_train_loss=0.0028001338046006415
Epoch 118
Epoch 118 :: Batch 0/1
Batch Loss = 0.002794159865011015
118, epoch_train_loss=0.002794159865011015
Epoch 119
Epoch 119 :: Batch 0/1
Batch Loss = 0.0027871873490029806
119, epoch_train_loss=0.0027871873490029806
Epoch 120
Epoch 120 :: Batch 0/1
Batch Loss = 0.002777342189407005
120, epoch_train_loss=0.002777342189407005
Epoch 121
Epoch 121 :: Batch 0/1
Batch Loss = 0.0027725398120596752
121, epoch_train_loss=0.0027725398120596752
Epoch 122
Epoch 122 :: Batch 0/1
Batch Loss = 0.002764784308278445
122, epoch_train_loss=0.002764784308278445
Epoch 123
Epoch 123 :: Batch 0/1
Batch Loss = 0.0027569708481949935
123, epoch_train_loss=0.0027569708481949935
Epoch 124
Epoch 124 :: Batch 0/1
Batch Loss = 0.002753038616642414
124, epoch_train_loss=0.002753038616642414
Epoch 125
Epoch 125 :: Batch 0/1
Batch Loss = 0.0027428695045296817
125, epoch_train_loss=0.0027428695045296817
Epoch 126
Epoch 126 :: Batch 0/1
Batch Loss = 0.0027407096369176363
126, epoch_train_loss=0.0027407096369176363
Epoch 127
Epoch 127 :: Batch 0/1
Batch Loss = 0.002731559306549914
127, epoch_train_loss=0.002731559306549914
Epoch 128
Epoch 128 :: Batch 0/1
Batch Loss = 0.002728143694126682
128, epoch_train_loss=0.002728143694126682
Epoch 129
Epoch 129 :: Batch 0/1
Batch Loss = 0.0027218075255783635
129, epoch_train_loss=0.0027218075255783635
Epoch 130
Epoch 130 :: Batch 0/1
Batch Loss = 0.002716125474030905
130, epoch_train_loss=0.002716125474030905
Epoch 131
Epoch 131 :: Batch 0/1
Batch Loss = 0.0027119570337696
131, epoch_train_loss=0.0027119570337696
Epoch 132
Epoch 132 :: Batch 0/1
Batch Loss = 0.002705338431185671
132, epoch_train_loss=0.002705338431185671
Epoch 133
Epoch 133 :: Batch 0/1
Batch Loss = 0.002702068207841463
133, epoch_train_loss=0.002702068207841463
Epoch 134
Epoch 134 :: Batch 0/1
Batch Loss = 0.0026958135153776703
134, epoch_train_loss=0.0026958135153776703
Epoch 135
Epoch 135 :: Batch 0/1
Batch Loss = 0.0026921363352725904
135, epoch_train_loss=0.0026921363352725904
Epoch 136
Epoch 136 :: Batch 0/1
Batch Loss = 0.002687052479553805
136, epoch_train_loss=0.002687052479553805
Epoch 137
Epoch 137 :: Batch 0/1
Batch Loss = 0.002682768968484091
137, epoch_train_loss=0.002682768968484091
Epoch 138
Epoch 138 :: Batch 0/1
Batch Loss = 0.002678685982355489
138, epoch_train_loss=0.002678685982355489
Epoch 139
Epoch 139 :: Batch 0/1
Batch Loss = 0.002673898525147225
139, epoch_train_loss=0.002673898525147225
Epoch 140
Epoch 140 :: Batch 0/1
Batch Loss = 0.0026703784844537607
140, epoch_train_loss=0.0026703784844537607
Epoch 141
Epoch 141 :: Batch 0/1
Batch Loss = 0.0026656160456125164
141, epoch_train_loss=0.0026656160456125164
Epoch 142
Epoch 142 :: Batch 0/1
Batch Loss = 0.0026622558844503004
142, epoch_train_loss=0.0026622558844503004
Epoch 143
Epoch 143 :: Batch 0/1
Batch Loss = 0.0026579225223336334
143, epoch_train_loss=0.0026579225223336334
Epoch 144
Epoch 144 :: Batch 0/1
Batch Loss = 0.0026544112574252605
144, epoch_train_loss=0.0026544112574252605
Epoch 145
Epoch 145 :: Batch 0/1
Batch Loss = 0.002650611308777834
145, epoch_train_loss=0.002650611308777834
Epoch 146
Epoch 146 :: Batch 0/1
Batch Loss = 0.0026468454522611674
146, epoch_train_loss=0.0026468454522611674
Epoch 147
Epoch 147 :: Batch 0/1
Batch Loss = 0.0026435619311543797
147, epoch_train_loss=0.0026435619311543797
Epoch 148
Epoch 148 :: Batch 0/1
Batch Loss = 0.0026396917300710895
148, epoch_train_loss=0.0026396917300710895
Epoch 149
Epoch 149 :: Batch 0/1
Batch Loss = 0.0026366452667872923
149, epoch_train_loss=0.0026366452667872923
Epoch 150
Epoch 150 :: Batch 0/1
Batch Loss = 0.002632918111133411
150, epoch_train_loss=0.002632918111133411
Epoch 151
Epoch 151 :: Batch 0/1
Batch Loss = 0.002629919572918571
151, epoch_train_loss=0.002629919572918571
Epoch 152
Epoch 152 :: Batch 0/1
Batch Loss = 0.0026265270208998136
152, epoch_train_loss=0.0026265270208998136
Epoch 153
Epoch 153 :: Batch 0/1
Batch Loss = 0.00262343696897739
153, epoch_train_loss=0.00262343696897739
Epoch 154
Epoch 154 :: Batch 0/1
Batch Loss = 0.0026203529077554995
154, epoch_train_loss=0.0026203529077554995
Epoch 155
Epoch 155 :: Batch 0/1
Batch Loss = 0.0026172432358509143
155, epoch_train_loss=0.0026172432358509143
Epoch 156
Epoch 156 :: Batch 0/1
Batch Loss = 0.0026143361367327437
156, epoch_train_loss=0.0026143361367327437
Epoch 157
Epoch 157 :: Batch 0/1
Batch Loss = 0.0026113463410200447
157, epoch_train_loss=0.0026113463410200447
Epoch 158
Epoch 158 :: Batch 0/1
Batch Loss = 0.0026085163408892486
158, epoch_train_loss=0.0026085163408892486
Epoch 159
Epoch 159 :: Batch 0/1
Batch Loss = 0.0026056811693686934
159, epoch_train_loss=0.0026056811693686934
Epoch 160
Epoch 160 :: Batch 0/1
Batch Loss = 0.0026029266116815897
160, epoch_train_loss=0.0026029266116815897
Epoch 161
Epoch 161 :: Batch 0/1
Batch Loss = 0.0026001969152414325
161, epoch_train_loss=0.0026001969152414325
Epoch 162
Epoch 162 :: Batch 0/1
Batch Loss = 0.0025975752384686714
162, epoch_train_loss=0.0025975752384686714
Epoch 163
Epoch 163 :: Batch 0/1
Batch Loss = 0.0025948862809778922
163, epoch_train_loss=0.0025948862809778922
Epoch 164
Epoch 164 :: Batch 0/1
Batch Loss = 0.0025923955774586897
164, epoch_train_loss=0.0025923955774586897
Epoch 165
Epoch 165 :: Batch 0/1
Batch Loss = 0.002589787794098142
165, epoch_train_loss=0.002589787794098142
Epoch 166
Epoch 166 :: Batch 0/1
Batch Loss = 0.002587374128428188
166, epoch_train_loss=0.002587374128428188
Epoch 167
Epoch 167 :: Batch 0/1
Batch Loss = 0.0025848938630940147
167, epoch_train_loss=0.0025848938630940147
Epoch 168
Epoch 168 :: Batch 0/1
Batch Loss = 0.0025825229082568077
168, epoch_train_loss=0.0025825229082568077
Epoch 169
Epoch 169 :: Batch 0/1
Batch Loss = 0.002580156101195065
169, epoch_train_loss=0.002580156101195065
Epoch 170
Epoch 170 :: Batch 0/1
Batch Loss = 0.002577854010627267
170, epoch_train_loss=0.002577854010627267
Epoch 171
Epoch 171 :: Batch 0/1
Batch Loss = 0.002575558066559009
171, epoch_train_loss=0.002575558066559009
Epoch 172
Epoch 172 :: Batch 0/1
Batch Loss = 0.0025733502150724853
172, epoch_train_loss=0.0025733502150724853
Epoch 173
Epoch 173 :: Batch 0/1
Batch Loss = 0.0025711148504106414
173, epoch_train_loss=0.0025711148504106414
Epoch 174
Epoch 174 :: Batch 0/1
Batch Loss = 0.002568978765832513
174, epoch_train_loss=0.002568978765832513
Epoch 175
Epoch 175 :: Batch 0/1
Batch Loss = 0.002566831982566889
175, epoch_train_loss=0.002566831982566889
Epoch 176
Epoch 176 :: Batch 0/1
Batch Loss = 0.0025647443023302094
176, epoch_train_loss=0.0025647443023302094
Epoch 177
Epoch 177 :: Batch 0/1
Batch Loss = 0.002562676705264384
177, epoch_train_loss=0.002562676705264384
Epoch 178
Epoch 178 :: Batch 0/1
Batch Loss = 0.0025606499169149814
178, epoch_train_loss=0.0025606499169149814
Epoch 179
Epoch 179 :: Batch 0/1
Batch Loss = 0.002558638544858864
179, epoch_train_loss=0.002558638544858864
Epoch 180
Epoch 180 :: Batch 0/1
Batch Loss = 0.002556683154172338
180, epoch_train_loss=0.002556683154172338
Epoch 181
Epoch 181 :: Batch 0/1
Batch Loss = 0.0025547267561671907
181, epoch_train_loss=0.0025547267561671907
Epoch 182
Epoch 182 :: Batch 0/1
Batch Loss = 0.0025528252703539444
182, epoch_train_loss=0.0025528252703539444
Epoch 183
Epoch 183 :: Batch 0/1
Batch Loss = 0.0025509341561705846
183, epoch_train_loss=0.0025509341561705846
Epoch 184
Epoch 184 :: Batch 0/1
Batch Loss = 0.0025490791805192307
184, epoch_train_loss=0.0025490791805192307
Epoch 185
Epoch 185 :: Batch 0/1
Batch Loss = 0.0025472433960919065
185, epoch_train_loss=0.0025472433960919065
Epoch 186
Epoch 186 :: Batch 0/1
Batch Loss = 0.0025454419686410635
186, epoch_train_loss=0.0025454419686410635
Epoch 187
Epoch 187 :: Batch 0/1
Batch Loss = 0.0025436510908546854
187, epoch_train_loss=0.0025436510908546854
Epoch 188
Epoch 188 :: Batch 0/1
Batch Loss = 0.002541900670871501
188, epoch_train_loss=0.002541900670871501
Epoch 189
Epoch 189 :: Batch 0/1
Batch Loss = 0.0025401600915272438
189, epoch_train_loss=0.0025401600915272438
Epoch 190
Epoch 190 :: Batch 0/1
Batch Loss = 0.002538449336096916
190, epoch_train_loss=0.002538449336096916
Epoch 191
Epoch 191 :: Batch 0/1
Batch Loss = 0.0025367573296770757
191, epoch_train_loss=0.0025367573296770757
Epoch 192
Epoch 192 :: Batch 0/1
Batch Loss = 0.002535089206520361
192, epoch_train_loss=0.002535089206520361
Epoch 193
Epoch 193 :: Batch 0/1
Batch Loss = 0.0025334368283017605
193, epoch_train_loss=0.0025334368283017605
Epoch 194
Epoch 194 :: Batch 0/1
Batch Loss = 0.002531810724787885
194, epoch_train_loss=0.002531810724787885
Epoch 195
Epoch 195 :: Batch 0/1
Batch Loss = 0.0025301992103315273
195, epoch_train_loss=0.0025301992103315273
Epoch 196
Epoch 196 :: Batch 0/1
Batch Loss = 0.002528608204031608
196, epoch_train_loss=0.002528608204031608
Epoch 197
Epoch 197 :: Batch 0/1
Batch Loss = 0.0025270370362772522
197, epoch_train_loss=0.0025270370362772522
Epoch 198
Epoch 198 :: Batch 0/1
Batch Loss = 0.002525481624816246
198, epoch_train_loss=0.002525481624816246
Epoch 199
Epoch 199 :: Batch 0/1
Batch Loss = 0.002523944531351222
199, epoch_train_loss=0.002523944531351222
Epoch 200
Epoch 200 :: Batch 0/1
Batch Loss = 0.002522424198419668
200, epoch_train_loss=0.002522424198419668
Epoch 201
Epoch 201 :: Batch 0/1
Batch Loss = 0.002520921108989294
201, epoch_train_loss=0.002520921108989294
Epoch 202
Epoch 202 :: Batch 0/1
Batch Loss = 0.0025194317200584347
202, epoch_train_loss=0.0025194317200584347
Epoch 203
Epoch 203 :: Batch 0/1
Batch Loss = 0.002517961256426785
203, epoch_train_loss=0.002517961256426785
Epoch 204
Epoch 204 :: Batch 0/1
Batch Loss = 0.0025165031471669932
204, epoch_train_loss=0.0025165031471669932
Epoch 205
Epoch 205 :: Batch 0/1
Batch Loss = 0.002515061309592394
205, epoch_train_loss=0.002515061309592394
Epoch 206
Epoch 206 :: Batch 0/1
Batch Loss = 0.002513633253620751
206, epoch_train_loss=0.002513633253620751
Epoch 207
Epoch 207 :: Batch 0/1
Batch Loss = 0.0025122199805008285
207, epoch_train_loss=0.0025122199805008285
Epoch 208
Epoch 208 :: Batch 0/1
Batch Loss = 0.002510819169245551
208, epoch_train_loss=0.002510819169245551
Epoch 209
Epoch 209 :: Batch 0/1
Batch Loss = 0.0025094326257510003
209, epoch_train_loss=0.0025094326257510003
Epoch 210
Epoch 210 :: Batch 0/1
Batch Loss = 0.00250805926072434
210, epoch_train_loss=0.00250805926072434
Epoch 211
Epoch 211 :: Batch 0/1
Batch Loss = 0.002506697678136566
211, epoch_train_loss=0.002506697678136566
Epoch 212
Epoch 212 :: Batch 0/1
Batch Loss = 0.002505349418629606
212, epoch_train_loss=0.002505349418629606
Epoch 213
Epoch 213 :: Batch 0/1
Batch Loss = 0.0025040127821182568
213, epoch_train_loss=0.0025040127821182568
Epoch 214
Epoch 214 :: Batch 0/1
Batch Loss = 0.0025026883782024566
214, epoch_train_loss=0.0025026883782024566
Epoch 215
Epoch 215 :: Batch 0/1
Batch Loss = 0.002501374902634027
215, epoch_train_loss=0.002501374902634027
Epoch 216
Epoch 216 :: Batch 0/1
Batch Loss = 0.0025000735473042217
216, epoch_train_loss=0.0025000735473042217
Epoch 217
Epoch 217 :: Batch 0/1
Batch Loss = 0.002498782816938359
217, epoch_train_loss=0.002498782816938359
Epoch 218
Epoch 218 :: Batch 0/1
Batch Loss = 0.0024975029006900447
218, epoch_train_loss=0.0024975029006900447
Epoch 219
Epoch 219 :: Batch 0/1
Batch Loss = 0.002496233874189214
219, epoch_train_loss=0.002496233874189214
Epoch 220
Epoch 220 :: Batch 0/1
Batch Loss = 0.002494975021176445
220, epoch_train_loss=0.002494975021176445
Epoch 221
Epoch 221 :: Batch 0/1
Batch Loss = 0.0024937264876614827
221, epoch_train_loss=0.0024937264876614827
Epoch 222
Epoch 222 :: Batch 0/1
Batch Loss = 0.0024924876652814166
222, epoch_train_loss=0.0024924876652814166
Epoch 223
Epoch 223 :: Batch 0/1
Batch Loss = 0.0024912590220606433
223, epoch_train_loss=0.0024912590220606433
Epoch 224
Epoch 224 :: Batch 0/1
Batch Loss = 0.0024900396806826425
224, epoch_train_loss=0.0024900396806826425
Epoch 225
Epoch 225 :: Batch 0/1
Batch Loss = 0.002488829783092623
225, epoch_train_loss=0.002488829783092623
Epoch 226
Epoch 226 :: Batch 0/1
Batch Loss = 0.0024876292107767466
226, epoch_train_loss=0.0024876292107767466
Epoch 227
Epoch 227 :: Batch 0/1
Batch Loss = 0.002486437612061102
227, epoch_train_loss=0.002486437612061102
Epoch 228
Epoch 228 :: Batch 0/1
Batch Loss = 0.002485254976467691
228, epoch_train_loss=0.002485254976467691
Epoch 229
Epoch 229 :: Batch 0/1
Batch Loss = 0.002484080906232134
229, epoch_train_loss=0.002484080906232134
Epoch 230
Epoch 230 :: Batch 0/1
Batch Loss = 0.0024829156098486894
230, epoch_train_loss=0.0024829156098486894
Epoch 231
Epoch 231 :: Batch 0/1
Batch Loss = 0.002481758643384842
231, epoch_train_loss=0.002481758643384842
Epoch 232
Epoch 232 :: Batch 0/1
Batch Loss = 0.002480609904764928
232, epoch_train_loss=0.002480609904764928
Epoch 233
Epoch 233 :: Batch 0/1
Batch Loss = 0.0024794693513062187
233, epoch_train_loss=0.0024794693513062187
Epoch 234
Epoch 234 :: Batch 0/1
Batch Loss = 0.002478336754086419
234, epoch_train_loss=0.002478336754086419
Epoch 235
Epoch 235 :: Batch 0/1
Batch Loss = 0.0024772120702564694
235, epoch_train_loss=0.0024772120702564694
Epoch 236
Epoch 236 :: Batch 0/1
Batch Loss = 0.00247609500595372
236, epoch_train_loss=0.00247609500595372
Epoch 237
Epoch 237 :: Batch 0/1
Batch Loss = 0.0024749856031970594
237, epoch_train_loss=0.0024749856031970594
Epoch 238
Epoch 238 :: Batch 0/1
Batch Loss = 0.0024738836913659958
238, epoch_train_loss=0.0024738836913659958
Epoch 239
Epoch 239 :: Batch 0/1
Batch Loss = 0.002472789072677137
239, epoch_train_loss=0.002472789072677137
Epoch 240
Epoch 240 :: Batch 0/1
Batch Loss = 0.00247170171332857
240, epoch_train_loss=0.00247170171332857
Epoch 241
Epoch 241 :: Batch 0/1
Batch Loss = 0.002470621441232413
241, epoch_train_loss=0.002470621441232413
Epoch 242
Epoch 242 :: Batch 0/1
Batch Loss = 0.002469548210572619
242, epoch_train_loss=0.002469548210572619
Epoch 243
Epoch 243 :: Batch 0/1
Batch Loss = 0.002468481844532698
243, epoch_train_loss=0.002468481844532698
Epoch 244
Epoch 244 :: Batch 0/1
Batch Loss = 0.00246742223440551
244, epoch_train_loss=0.00246742223440551
Epoch 245
Epoch 245 :: Batch 0/1
Batch Loss = 0.002466369329190861
245, epoch_train_loss=0.002466369329190861
Epoch 246
Epoch 246 :: Batch 0/1
Batch Loss = 0.0024653229665506224
246, epoch_train_loss=0.0024653229665506224
Epoch 247
Epoch 247 :: Batch 0/1
Batch Loss = 0.0024642830796410293
247, epoch_train_loss=0.0024642830796410293
Epoch 248
Epoch 248 :: Batch 0/1
Batch Loss = 0.0024632495426091167
248, epoch_train_loss=0.0024632495426091167
Epoch 249
Epoch 249 :: Batch 0/1
Batch Loss = 0.0024622222561325826
249, epoch_train_loss=0.0024622222561325826
Epoch 250
Epoch 250 :: Batch 0/1
Batch Loss = 0.002461201155797538
250, epoch_train_loss=0.002461201155797538
Epoch 251
Epoch 251 :: Batch 0/1
Batch Loss = 0.0024601861085543765
251, epoch_train_loss=0.0024601861085543765
Epoch 252
Epoch 252 :: Batch 0/1
Batch Loss = 0.002459177034935745
252, epoch_train_loss=0.002459177034935745
Epoch 253
Epoch 253 :: Batch 0/1
Batch Loss = 0.0024581738378878794
253, epoch_train_loss=0.0024581738378878794
Epoch 254
Epoch 254 :: Batch 0/1
Batch Loss = 0.0024571764266094896
254, epoch_train_loss=0.0024571764266094896
Epoch 255
Epoch 255 :: Batch 0/1
Batch Loss = 0.0024561847266248026
255, epoch_train_loss=0.0024561847266248026
Epoch 256
Epoch 256 :: Batch 0/1
Batch Loss = 0.0024551986220426774
256, epoch_train_loss=0.0024551986220426774
Epoch 257
Epoch 257 :: Batch 0/1
Batch Loss = 0.0024542180385235704
257, epoch_train_loss=0.0024542180385235704
Epoch 258
Epoch 258 :: Batch 0/1
Batch Loss = 0.0024532428958243023
258, epoch_train_loss=0.0024532428958243023
Epoch 259
Epoch 259 :: Batch 0/1
Batch Loss = 0.0024522730965332823
259, epoch_train_loss=0.0024522730965332823
Epoch 260
Epoch 260 :: Batch 0/1
Batch Loss = 0.002451308570978259
260, epoch_train_loss=0.002451308570978259
Epoch 261
Epoch 261 :: Batch 0/1
Batch Loss = 0.0024503492278591093
261, epoch_train_loss=0.0024503492278591093
Epoch 262
Epoch 262 :: Batch 0/1
Batch Loss = 0.002449394982533134
262, epoch_train_loss=0.002449394982533134
Epoch 263
Epoch 263 :: Batch 0/1
Batch Loss = 0.002448445765691789
263, epoch_train_loss=0.002448445765691789
Epoch 264
Epoch 264 :: Batch 0/1
Batch Loss = 0.002447501493914168
264, epoch_train_loss=0.002447501493914168
Epoch 265
Epoch 265 :: Batch 0/1
Batch Loss = 0.0024465620907987654
265, epoch_train_loss=0.0024465620907987654
Epoch 266
Epoch 266 :: Batch 0/1
Batch Loss = 0.0024456274796685407
266, epoch_train_loss=0.0024456274796685407
Epoch 267
Epoch 267 :: Batch 0/1
Batch Loss = 0.0024446975822156264
267, epoch_train_loss=0.0024446975822156264
Epoch 268
Epoch 268 :: Batch 0/1
Batch Loss = 0.0024437723278563244
268, epoch_train_loss=0.0024437723278563244
Epoch 269
Epoch 269 :: Batch 0/1
Batch Loss = 0.0024428516424538755
269, epoch_train_loss=0.0024428516424538755
Epoch 270
Epoch 270 :: Batch 0/1
Batch Loss = 0.0024419354539223542
270, epoch_train_loss=0.0024419354539223542
Epoch 271
Epoch 271 :: Batch 0/1
Batch Loss = 0.0024410236926128296
271, epoch_train_loss=0.0024410236926128296
Epoch 272
Epoch 272 :: Batch 0/1
Batch Loss = 0.002440116284745117
272, epoch_train_loss=0.002440116284745117
Epoch 273
Epoch 273 :: Batch 0/1
Batch Loss = 0.0024392131628424847
273, epoch_train_loss=0.0024392131628424847
Epoch 274
Epoch 274 :: Batch 0/1
Batch Loss = 0.002438314261547817
274, epoch_train_loss=0.002438314261547817
Epoch 275
Epoch 275 :: Batch 0/1
Batch Loss = 0.0024374195104877533
275, epoch_train_loss=0.0024374195104877533
Epoch 276
Epoch 276 :: Batch 0/1
Batch Loss = 0.002436528846533721
276, epoch_train_loss=0.002436528846533721
Epoch 277
Epoch 277 :: Batch 0/1
Batch Loss = 0.0024356422053430004
277, epoch_train_loss=0.0024356422053430004
Epoch 278
Epoch 278 :: Batch 0/1
Batch Loss = 0.002434759519414316
278, epoch_train_loss=0.002434759519414316
Epoch 279
Epoch 279 :: Batch 0/1
Batch Loss = 0.0024338807289329663
279, epoch_train_loss=0.0024338807289329663
Epoch 280
Epoch 280 :: Batch 0/1
Batch Loss = 0.002433005772042164
280, epoch_train_loss=0.002433005772042164
Epoch 281
Epoch 281 :: Batch 0/1
Batch Loss = 0.0024321345861762125
281, epoch_train_loss=0.0024321345861762125
Epoch 282
Epoch 282 :: Batch 0/1
Batch Loss = 0.0024312671135349145
282, epoch_train_loss=0.0024312671135349145
Epoch 283
Epoch 283 :: Batch 0/1
Batch Loss = 0.0024304032948231683
283, epoch_train_loss=0.0024304032948231683
Epoch 284
Epoch 284 :: Batch 0/1
Batch Loss = 0.002429543070616938
284, epoch_train_loss=0.002429543070616938
Epoch 285
Epoch 285 :: Batch 0/1
Batch Loss = 0.0024286863853559613
285, epoch_train_loss=0.0024286863853559613
Epoch 286
Epoch 286 :: Batch 0/1
Batch Loss = 0.002427833183543582
286, epoch_train_loss=0.002427833183543582
Epoch 287
Epoch 287 :: Batch 0/1
Batch Loss = 0.002426983408643376
287, epoch_train_loss=0.002426983408643376
Epoch 288
Epoch 288 :: Batch 0/1
Batch Loss = 0.0024261370083698253
288, epoch_train_loss=0.0024261370083698253
Epoch 289
Epoch 289 :: Batch 0/1
Batch Loss = 0.0024252939294155474
289, epoch_train_loss=0.0024252939294155474
Epoch 290
Epoch 290 :: Batch 0/1
Batch Loss = 0.002424454118728338
290, epoch_train_loss=0.002424454118728338
Epoch 291
Epoch 291 :: Batch 0/1
Batch Loss = 0.002423617526209478
291, epoch_train_loss=0.002423617526209478
Epoch 292
Epoch 292 :: Batch 0/1
Batch Loss = 0.002422784101449756
292, epoch_train_loss=0.002422784101449756
Epoch 293
Epoch 293 :: Batch 0/1
Batch Loss = 0.0024219537944205677
293, epoch_train_loss=0.0024219537944205677
Epoch 294
Epoch 294 :: Batch 0/1
Batch Loss = 0.002421126557147961
294, epoch_train_loss=0.002421126557147961
Epoch 295
Epoch 295 :: Batch 0/1
Batch Loss = 0.0024203023423416898
295, epoch_train_loss=0.0024203023423416898
Epoch 296
Epoch 296 :: Batch 0/1
Batch Loss = 0.002419481102290627
296, epoch_train_loss=0.002419481102290627
Epoch 297
Epoch 297 :: Batch 0/1
Batch Loss = 0.002418662792277649
297, epoch_train_loss=0.002418662792277649
Epoch 298
Epoch 298 :: Batch 0/1
Batch Loss = 0.0024178473671013843
298, epoch_train_loss=0.0024178473671013843
Epoch 299
Epoch 299 :: Batch 0/1
Batch Loss = 0.0024170347822661084
299, epoch_train_loss=0.0024170347822661084
Epoch 300
Epoch 300 :: Batch 0/1
Batch Loss = 0.0024162249950641685
300, epoch_train_loss=0.0024162249950641685
Epoch 301
Epoch 301 :: Batch 0/1
Batch Loss = 0.002415417963227585
301, epoch_train_loss=0.002415417963227585
Epoch 302
Epoch 302 :: Batch 0/1
Batch Loss = 0.002414613644706935
302, epoch_train_loss=0.002414613644706935
Epoch 303
Epoch 303 :: Batch 0/1
Batch Loss = 0.0024138119991628506
303, epoch_train_loss=0.0024138119991628506
Epoch 304
Epoch 304 :: Batch 0/1
Batch Loss = 0.0024130129867322996
304, epoch_train_loss=0.0024130129867322996
Epoch 305
Epoch 305 :: Batch 0/1
Batch Loss = 0.0024122165678877043
305, epoch_train_loss=0.0024122165678877043
Epoch 306
Epoch 306 :: Batch 0/1
Batch Loss = 0.002411422704655624
306, epoch_train_loss=0.002411422704655624
Epoch 307
Epoch 307 :: Batch 0/1
Batch Loss = 0.002410631359207805
307, epoch_train_loss=0.002410631359207805
Epoch 308
Epoch 308 :: Batch 0/1
Batch Loss = 0.0024098424948000075
308, epoch_train_loss=0.0024098424948000075
Epoch 309
Epoch 309 :: Batch 0/1
Batch Loss = 0.0024090560753541705
309, epoch_train_loss=0.0024090560753541705
Epoch 310
Epoch 310 :: Batch 0/1
Batch Loss = 0.002408272065692093
310, epoch_train_loss=0.002408272065692093
Epoch 311
Epoch 311 :: Batch 0/1
Batch Loss = 0.0024074904307056196
311, epoch_train_loss=0.0024074904307056196
Epoch 312
Epoch 312 :: Batch 0/1
Batch Loss = 0.0024067111372825085
312, epoch_train_loss=0.0024067111372825085
Epoch 313
Epoch 313 :: Batch 0/1
Batch Loss = 0.0024059341513618703
313, epoch_train_loss=0.0024059341513618703
Epoch 314
Epoch 314 :: Batch 0/1
Batch Loss = 0.00240515944134021
314, epoch_train_loss=0.00240515944134021
Epoch 315
Epoch 315 :: Batch 0/1
Batch Loss = 0.0024043869748322268
315, epoch_train_loss=0.0024043869748322268
Epoch 316
Epoch 316 :: Batch 0/1
Batch Loss = 0.0024036167218728075
316, epoch_train_loss=0.0024036167218728075
Epoch 317
Epoch 317 :: Batch 0/1
Batch Loss = 0.002402848651241443
317, epoch_train_loss=0.002402848651241443
Epoch 318
Epoch 318 :: Batch 0/1
Batch Loss = 0.002402082736800354
318, epoch_train_loss=0.002402082736800354
Epoch 319
Epoch 319 :: Batch 0/1
Batch Loss = 0.002401318949347422
319, epoch_train_loss=0.002401318949347422
Epoch 320
Epoch 320 :: Batch 0/1
Batch Loss = 0.0024005572708633302
320, epoch_train_loss=0.0024005572708633302
Epoch 321
Epoch 321 :: Batch 0/1
Batch Loss = 0.0023997976820879126
321, epoch_train_loss=0.0023997976820879126
Epoch 322
Epoch 322 :: Batch 0/1
Batch Loss = 0.0023990401930833872
322, epoch_train_loss=0.0023990401930833872
Epoch 323
Epoch 323 :: Batch 0/1
Batch Loss = 0.0023982848327865755
323, epoch_train_loss=0.0023982848327865755
Epoch 324
Epoch 324 :: Batch 0/1
Batch Loss = 0.002397531737039308
324, epoch_train_loss=0.002397531737039308
Epoch 325
Epoch 325 :: Batch 0/1
Batch Loss = 0.002396781191775653
325, epoch_train_loss=0.002396781191775653
Epoch 326
Epoch 326 :: Batch 0/1
Batch Loss = 0.0023960339711566847
326, epoch_train_loss=0.0023960339711566847
Epoch 327
Epoch 327 :: Batch 0/1
Batch Loss = 0.0023952918036646196
327, epoch_train_loss=0.0023952918036646196
Epoch 328
Epoch 328 :: Batch 0/1
Batch Loss = 0.002394559020517567
328, epoch_train_loss=0.002394559020517567
Epoch 329
Epoch 329 :: Batch 0/1
Batch Loss = 0.0023938458971869488
329, epoch_train_loss=0.0023938458971869488
Epoch 330
Epoch 330 :: Batch 0/1
Batch Loss = 0.0023931781463881986
330, epoch_train_loss=0.0023931781463881986
Epoch 331
Epoch 331 :: Batch 0/1
Batch Loss = 0.002392619780129241
331, epoch_train_loss=0.002392619780129241
Epoch 332
Epoch 332 :: Batch 0/1
Batch Loss = 0.00239233392565616
332, epoch_train_loss=0.00239233392565616
Epoch 333
Epoch 333 :: Batch 0/1
Batch Loss = 0.0023927440240703005
333, epoch_train_loss=0.0023927440240703005
Epoch 334
Epoch 334 :: Batch 0/1
Batch Loss = 0.002394952111399022
334, epoch_train_loss=0.002394952111399022
Epoch 335
Epoch 335 :: Batch 0/1
Batch Loss = 0.0024019435863707316
335, epoch_train_loss=0.0024019435863707316
Epoch 336
Epoch 336 :: Batch 0/1
Batch Loss = 0.002421595020878287
336, epoch_train_loss=0.002421595020878287
Epoch 337
Epoch 337 :: Batch 0/1
Batch Loss = 0.0024764055360754138
337, epoch_train_loss=0.0024764055360754138
Epoch 338
Epoch 338 :: Batch 0/1
Batch Loss = 0.0026250539224395948
338, epoch_train_loss=0.0026250539224395948
Epoch 339
Epoch 339 :: Batch 0/1
Batch Loss = 0.0030490730078724247
339, epoch_train_loss=0.0030490730078724247
Epoch 340
Epoch 340 :: Batch 0/1
Batch Loss = 0.004178394697914092
340, epoch_train_loss=0.004178394697914092
Epoch 341
Epoch 341 :: Batch 0/1
Batch Loss = 0.007523474862247664
341, epoch_train_loss=0.007523474862247664
Epoch 342
Epoch 342 :: Batch 0/1
Batch Loss = 0.01515992109371771
342, epoch_train_loss=0.01515992109371771
Epoch 343
Epoch 343 :: Batch 0/1
Batch Loss = 0.03606035529386668
343, epoch_train_loss=0.03606035529386668
Epoch 344
Epoch 344 :: Batch 0/1
Batch Loss = 0.05063967104737191
344, epoch_train_loss=0.05063967104737191
Epoch 345
Epoch 345 :: Batch 0/1
Batch Loss = 0.06574242655694222
345, epoch_train_loss=0.06574242655694222
Epoch 346
Epoch 346 :: Batch 0/1
Batch Loss = 0.013489986844202141
346, epoch_train_loss=0.013489986844202141
Epoch 347
Epoch 347 :: Batch 0/1
Batch Loss = 0.009504978816089698
347, epoch_train_loss=0.009504978816089698
Epoch 348
Epoch 348 :: Batch 0/1
Batch Loss = 0.03712060974138437
348, epoch_train_loss=0.03712060974138437
Epoch 349
Epoch 349 :: Batch 0/1
Batch Loss = 0.010005401519978595
349, epoch_train_loss=0.010005401519978595
Epoch 350
Epoch 350 :: Batch 0/1
Batch Loss = 0.009071742048097034
350, epoch_train_loss=0.009071742048097034
Epoch 351
Epoch 351 :: Batch 0/1
Batch Loss = 0.023586457551757232
351, epoch_train_loss=0.023586457551757232
Epoch 352
Epoch 352 :: Batch 0/1
Batch Loss = 0.003208052856148783
352, epoch_train_loss=0.003208052856148783
Epoch 353
Epoch 353 :: Batch 0/1
Batch Loss = 0.016632167895024773
353, epoch_train_loss=0.016632167895024773
Epoch 354
Epoch 354 :: Batch 0/1
Batch Loss = 0.014057156327332093
354, epoch_train_loss=0.014057156327332093
Epoch 355
Epoch 355 :: Batch 0/1
Batch Loss = 0.005727318674631837
355, epoch_train_loss=0.005727318674631837
Epoch 356
Epoch 356 :: Batch 0/1
Batch Loss = 0.0179441012256849
356, epoch_train_loss=0.0179441012256849
Epoch 357
Epoch 357 :: Batch 0/1
Batch Loss = 0.0037263745865075597
357, epoch_train_loss=0.0037263745865075597
Epoch 358
Epoch 358 :: Batch 0/1
Batch Loss = 0.011869409542308465
358, epoch_train_loss=0.011869409542308465
Epoch 359
Epoch 359 :: Batch 0/1
Batch Loss = 0.005995181063118494
359, epoch_train_loss=0.005995181063118494
Epoch 360
Epoch 360 :: Batch 0/1
Batch Loss = 0.00700632675969634
360, epoch_train_loss=0.00700632675969634
Epoch 361
Epoch 361 :: Batch 0/1
Batch Loss = 0.00854768948321894
361, epoch_train_loss=0.00854768948321894
Epoch 362
Epoch 362 :: Batch 0/1
Batch Loss = 0.004456467736862928
362, epoch_train_loss=0.004456467736862928
Epoch 363
Epoch 363 :: Batch 0/1
Batch Loss = 0.009233940944738454
363, epoch_train_loss=0.009233940944738454
Epoch 364
Epoch 364 :: Batch 0/1
Batch Loss = 0.003248733034699689
364, epoch_train_loss=0.003248733034699689
Epoch 365
Epoch 365 :: Batch 0/1
Batch Loss = 0.007867507437366139
365, epoch_train_loss=0.007867507437366139
Epoch 366
Epoch 366 :: Batch 0/1
Batch Loss = 0.003438309122333203
366, epoch_train_loss=0.003438309122333203
Epoch 367
Epoch 367 :: Batch 0/1
Batch Loss = 0.006535784025489164
367, epoch_train_loss=0.006535784025489164
Epoch 368
Epoch 368 :: Batch 0/1
Batch Loss = 0.004295807550528917
368, epoch_train_loss=0.004295807550528917
Epoch 369
Epoch 369 :: Batch 0/1
Batch Loss = 0.004934099766645576
369, epoch_train_loss=0.004934099766645576
Epoch 370
Epoch 370 :: Batch 0/1
Batch Loss = 0.004818673768999113
370, epoch_train_loss=0.004818673768999113
Epoch 371
Epoch 371 :: Batch 0/1
Batch Loss = 0.003632536471710466
371, epoch_train_loss=0.003632536471710466
Epoch 372
Epoch 372 :: Batch 0/1
Batch Loss = 0.005029467266580505
372, epoch_train_loss=0.005029467266580505
Epoch 373
Epoch 373 :: Batch 0/1
Batch Loss = 0.0030525949595377904
373, epoch_train_loss=0.0030525949595377904
Epoch 374
Epoch 374 :: Batch 0/1
Batch Loss = 0.004885177522226478
374, epoch_train_loss=0.004885177522226478
Epoch 375
Epoch 375 :: Batch 0/1
Batch Loss = 0.002940512487384111
375, epoch_train_loss=0.002940512487384111
Epoch 376
Epoch 376 :: Batch 0/1
Batch Loss = 0.0041952070364086845
376, epoch_train_loss=0.0041952070364086845
Epoch 377
Epoch 377 :: Batch 0/1
Batch Loss = 0.0031578074406231967
377, epoch_train_loss=0.0031578074406231967
Epoch 378
Epoch 378 :: Batch 0/1
Batch Loss = 0.0034624373881098234
378, epoch_train_loss=0.0034624373881098234
Epoch 379
Epoch 379 :: Batch 0/1
Batch Loss = 0.0034896651584397885
379, epoch_train_loss=0.0034896651584397885
Epoch 380
Epoch 380 :: Batch 0/1
Batch Loss = 0.0028919407083704204
380, epoch_train_loss=0.0028919407083704204
Epoch 381
Epoch 381 :: Batch 0/1
Batch Loss = 0.0036039976227197662
381, epoch_train_loss=0.0036039976227197662
Epoch 382
Epoch 382 :: Batch 0/1
Batch Loss = 0.002644765203834388
382, epoch_train_loss=0.002644765203834388
Epoch 383
Epoch 383 :: Batch 0/1
Batch Loss = 0.003355157414289927
383, epoch_train_loss=0.003355157414289927
Epoch 384
Epoch 384 :: Batch 0/1
Batch Loss = 0.002733575473222179
384, epoch_train_loss=0.002733575473222179
Epoch 385
Epoch 385 :: Batch 0/1
Batch Loss = 0.002961365614449891
385, epoch_train_loss=0.002961365614449891
Epoch 386
Epoch 386 :: Batch 0/1
Batch Loss = 0.0029447295901413387
386, epoch_train_loss=0.0029447295901413387
Epoch 387
Epoch 387 :: Batch 0/1
Batch Loss = 0.0026270896232553336
387, epoch_train_loss=0.0026270896232553336
Epoch 388
Epoch 388 :: Batch 0/1
Batch Loss = 0.003015739369939336
388, epoch_train_loss=0.003015739369939336
Epoch 389
Epoch 389 :: Batch 0/1
Batch Loss = 0.002527249823239593
389, epoch_train_loss=0.002527249823239593
Epoch 390
Epoch 390 :: Batch 0/1
Batch Loss = 0.002840542060563554
390, epoch_train_loss=0.002840542060563554
Epoch 391
Epoch 391 :: Batch 0/1
Batch Loss = 0.0026292510022686666
391, epoch_train_loss=0.0026292510022686666
Epoch 392
Epoch 392 :: Batch 0/1
Batch Loss = 0.002595592335815504
392, epoch_train_loss=0.002595592335815504
Epoch 393
Epoch 393 :: Batch 0/1
Batch Loss = 0.0027314885220148175
393, epoch_train_loss=0.0027314885220148175
Epoch 394
Epoch 394 :: Batch 0/1
Batch Loss = 0.0024683996483057624
394, epoch_train_loss=0.0024683996483057624
Epoch 395
Epoch 395 :: Batch 0/1
Batch Loss = 0.0026890369698043177
395, epoch_train_loss=0.0026890369698043177
Epoch 396
Epoch 396 :: Batch 0/1
Batch Loss = 0.0025094394543384206
396, epoch_train_loss=0.0025094394543384206
Epoch 397
Epoch 397 :: Batch 0/1
Batch Loss = 0.0025349997372069283
397, epoch_train_loss=0.0025349997372069283
Epoch 398
Epoch 398 :: Batch 0/1
Batch Loss = 0.0025880845807126967
398, epoch_train_loss=0.0025880845807126967
Epoch 399
Epoch 399 :: Batch 0/1
Batch Loss = 0.0024362821168223015
399, epoch_train_loss=0.0024362821168223015
Epoch 400
Epoch 400 :: Batch 0/1
Batch Loss = 0.0025666938326824435
400, epoch_train_loss=0.0025666938326824435
Epoch 401
Epoch 401 :: Batch 0/1
Batch Loss = 0.002457194481286233
401, epoch_train_loss=0.002457194481286233
Epoch 402
Epoch 402 :: Batch 0/1
Batch Loss = 0.002470892671900676
402, epoch_train_loss=0.002470892671900676
Epoch 403
Epoch 403 :: Batch 0/1
Batch Loss = 0.002507715207392791
403, epoch_train_loss=0.002507715207392791
Epoch 404
Epoch 404 :: Batch 0/1
Batch Loss = 0.002412433175696386
404, epoch_train_loss=0.002412433175696386
Epoch 405
Epoch 405 :: Batch 0/1
Batch Loss = 0.002490512787489886
405, epoch_train_loss=0.002490512787489886
Epoch 406
Epoch 406 :: Batch 0/1
Batch Loss = 0.002433245801290372
406, epoch_train_loss=0.002433245801290372
Epoch 407
Epoch 407 :: Batch 0/1
Batch Loss = 0.002424791612525799
407, epoch_train_loss=0.002424791612525799
Epoch 408
Epoch 408 :: Batch 0/1
Batch Loss = 0.002460972692001785
408, epoch_train_loss=0.002460972692001785
Epoch 409
Epoch 409 :: Batch 0/1
Batch Loss = 0.00239764561834565
409, epoch_train_loss=0.00239764561834565
Epoch 410
Epoch 410 :: Batch 0/1
Batch Loss = 0.002435141454401
410, epoch_train_loss=0.002435141454401
Epoch 411
Epoch 411 :: Batch 0/1
Batch Loss = 0.002418492849063808
411, epoch_train_loss=0.002418492849063808
Epoch 412
Epoch 412 :: Batch 0/1
Batch Loss = 0.0023941204157283506
412, epoch_train_loss=0.0023941204157283506
Epoch 413
Epoch 413 :: Batch 0/1
Batch Loss = 0.002425781741295165
413, epoch_train_loss=0.002425781741295165
Epoch 414
Epoch 414 :: Batch 0/1
Batch Loss = 0.0023897570114156997
414, epoch_train_loss=0.0023897570114156997
Epoch 415
Epoch 415 :: Batch 0/1
Batch Loss = 0.0023991934848858615
415, epoch_train_loss=0.0023991934848858615
Epoch 416
Epoch 416 :: Batch 0/1
Batch Loss = 0.0024047816265078347
416, epoch_train_loss=0.0024047816265078347
Epoch 417
Epoch 417 :: Batch 0/1
Batch Loss = 0.0023790745190175786
417, epoch_train_loss=0.0023790745190175786
Epoch 418
Epoch 418 :: Batch 0/1
Batch Loss = 0.00239784078391586
418, epoch_train_loss=0.00239784078391586
Epoch 419
Epoch 419 :: Batch 0/1
Batch Loss = 0.002385563670532011
419, epoch_train_loss=0.002385563670532011
Epoch 420
Epoch 420 :: Batch 0/1
Batch Loss = 0.0023769840982411044
420, epoch_train_loss=0.0023769840982411044
Epoch 421
Epoch 421 :: Batch 0/1
Batch Loss = 0.0023898299884276297
421, epoch_train_loss=0.0023898299884276297
Epoch 422
Epoch 422 :: Batch 0/1
Batch Loss = 0.0023730717957104403
422, epoch_train_loss=0.0023730717957104403
Epoch 423
Epoch 423 :: Batch 0/1
Batch Loss = 0.002376344651148799
423, epoch_train_loss=0.002376344651148799
Epoch 424
Epoch 424 :: Batch 0/1
Batch Loss = 0.002379263562697272
424, epoch_train_loss=0.002379263562697272
Epoch 425
Epoch 425 :: Batch 0/1
Batch Loss = 0.0023667593244224494
425, epoch_train_loss=0.0023667593244224494
Epoch 426
Epoch 426 :: Batch 0/1
Batch Loss = 0.0023736700956251996
426, epoch_train_loss=0.0023736700956251996
Epoch 427
Epoch 427 :: Batch 0/1
Batch Loss = 0.0023699139571056985
427, epoch_train_loss=0.0023699139571056985
Epoch 428
Epoch 428 :: Batch 0/1
Batch Loss = 0.002363498600273056
428, epoch_train_loss=0.002363498600273056
Epoch 429
Epoch 429 :: Batch 0/1
Batch Loss = 0.0023692366224024744
429, epoch_train_loss=0.0023692366224024744
Epoch 430
Epoch 430 :: Batch 0/1
Batch Loss = 0.0023629043214882755
430, epoch_train_loss=0.0023629043214882755
Epoch 431
Epoch 431 :: Batch 0/1
Batch Loss = 0.0023611546578095827
431, epoch_train_loss=0.0023611546578095827
Epoch 432
Epoch 432 :: Batch 0/1
Batch Loss = 0.0023640552356110314
432, epoch_train_loss=0.0023640552356110314
Epoch 433
Epoch 433 :: Batch 0/1
Batch Loss = 0.0023579879564935013
433, epoch_train_loss=0.0023579879564935013
Epoch 434
Epoch 434 :: Batch 0/1
Batch Loss = 0.0023586816721121455
434, epoch_train_loss=0.0023586816721121455
Epoch 435
Epoch 435 :: Batch 0/1
Batch Loss = 0.0023590794500147993
435, epoch_train_loss=0.0023590794500147993
Epoch 436
Epoch 436 :: Batch 0/1
Batch Loss = 0.0023544507509881906
436, epoch_train_loss=0.0023544507509881906
Epoch 437
Epoch 437 :: Batch 0/1
Batch Loss = 0.0023558501166754775
437, epoch_train_loss=0.0023558501166754775
Epoch 438
Epoch 438 :: Batch 0/1
Batch Loss = 0.002354727671668521
438, epoch_train_loss=0.002354727671668521
Epoch 439
Epoch 439 :: Batch 0/1
Batch Loss = 0.0023515634977297065
439, epoch_train_loss=0.0023515634977297065
Epoch 440
Epoch 440 :: Batch 0/1
Batch Loss = 0.002352823207897507
440, epoch_train_loss=0.002352823207897507
Epoch 441
Epoch 441 :: Batch 0/1
Batch Loss = 0.002350981748899202
441, epoch_train_loss=0.002350981748899202
Epoch 442
Epoch 442 :: Batch 0/1
Batch Loss = 0.0023489880580074896
442, epoch_train_loss=0.0023489880580074896
Epoch 443
Epoch 443 :: Batch 0/1
Batch Loss = 0.0023497359774756143
443, epoch_train_loss=0.0023497359774756143
Epoch 444
Epoch 444 :: Batch 0/1
Batch Loss = 0.0023477719548977496
444, epoch_train_loss=0.0023477719548977496
Epoch 445
Epoch 445 :: Batch 0/1
Batch Loss = 0.0023464802986820554
445, epoch_train_loss=0.0023464802986820554
Epoch 446
Epoch 446 :: Batch 0/1
Batch Loss = 0.002346767752327485
446, epoch_train_loss=0.002346767752327485
Epoch 447
Epoch 447 :: Batch 0/1
Batch Loss = 0.0023449100082733047
447, epoch_train_loss=0.0023449100082733047
Epoch 448
Epoch 448 :: Batch 0/1
Batch Loss = 0.0023440318710418154
448, epoch_train_loss=0.0023440318710418154
Epoch 449
Epoch 449 :: Batch 0/1
Batch Loss = 0.002343937423364047
449, epoch_train_loss=0.002343937423364047
Epoch 450
Epoch 450 :: Batch 0/1
Batch Loss = 0.0023423005857094396
450, epoch_train_loss=0.0023423005857094396
Epoch 451
Epoch 451 :: Batch 0/1
Batch Loss = 0.0023416023250740926
451, epoch_train_loss=0.0023416023250740926
Epoch 452
Epoch 452 :: Batch 0/1
Batch Loss = 0.0023412659223552563
452, epoch_train_loss=0.0023412659223552563
Epoch 453
Epoch 453 :: Batch 0/1
Batch Loss = 0.002339843611997516
453, epoch_train_loss=0.002339843611997516
Epoch 454
Epoch 454 :: Batch 0/1
Batch Loss = 0.0023392143287454367
454, epoch_train_loss=0.0023392143287454367
Epoch 455
Epoch 455 :: Batch 0/1
Batch Loss = 0.002338733386187465
455, epoch_train_loss=0.002338733386187465
Epoch 456
Epoch 456 :: Batch 0/1
Batch Loss = 0.0023374903393868
456, epoch_train_loss=0.0023374903393868
Epoch 457
Epoch 457 :: Batch 0/1
Batch Loss = 0.002336876861456545
457, epoch_train_loss=0.002336876861456545
Epoch 458
Epoch 458 :: Batch 0/1
Batch Loss = 0.0023363133236262114
458, epoch_train_loss=0.0023363133236262114
Epoch 459
Epoch 459 :: Batch 0/1
Batch Loss = 0.002335219150082598
459, epoch_train_loss=0.002335219150082598
Epoch 460
Epoch 460 :: Batch 0/1
Batch Loss = 0.0023345893756333207
460, epoch_train_loss=0.0023345893756333207
Epoch 461
Epoch 461 :: Batch 0/1
Batch Loss = 0.002333992487087833
461, epoch_train_loss=0.002333992487087833
Epoch 462
Epoch 462 :: Batch 0/1
Batch Loss = 0.00233300409753023
462, epoch_train_loss=0.00233300409753023
Epoch 463
Epoch 463 :: Batch 0/1
Batch Loss = 0.0023323583908369887
463, epoch_train_loss=0.0023323583908369887
Epoch 464
Epoch 464 :: Batch 0/1
Batch Loss = 0.002331744111511598
464, epoch_train_loss=0.002331744111511598
Epoch 465
Epoch 465 :: Batch 0/1
Batch Loss = 0.0023308425126716595
465, epoch_train_loss=0.0023308425126716595
Epoch 466
Epoch 466 :: Batch 0/1
Batch Loss = 0.0023301782938644788
466, epoch_train_loss=0.0023301782938644788
Epoch 467
Epoch 467 :: Batch 0/1
Batch Loss = 0.00232956059611506
467, epoch_train_loss=0.00232956059611506
Epoch 468
Epoch 468 :: Batch 0/1
Batch Loss = 0.0023287257646675634
468, epoch_train_loss=0.0023287257646675634
Epoch 469
Epoch 469 :: Batch 0/1
Batch Loss = 0.0023280477676498133
469, epoch_train_loss=0.0023280477676498133
Epoch 470
Epoch 470 :: Batch 0/1
Batch Loss = 0.00232743131037507
470, epoch_train_loss=0.00232743131037507
Epoch 471
Epoch 471 :: Batch 0/1
Batch Loss = 0.002326649274093208
471, epoch_train_loss=0.002326649274093208
Epoch 472
Epoch 472 :: Batch 0/1
Batch Loss = 0.0023259642642818974
472, epoch_train_loss=0.0023259642642818974
Epoch 473
Epoch 473 :: Batch 0/1
Batch Loss = 0.0023253471206958185
473, epoch_train_loss=0.0023253471206958185
Epoch 474
Epoch 474 :: Batch 0/1
Batch Loss = 0.0023246102021967007
474, epoch_train_loss=0.0023246102021967007
Epoch 475
Epoch 475 :: Batch 0/1
Batch Loss = 0.0023239211957370707
475, epoch_train_loss=0.0023239211957370707
Epoch 476
Epoch 476 :: Batch 0/1
Batch Loss = 0.0023233040949046965
476, epoch_train_loss=0.0023233040949046965
Epoch 477
Epoch 477 :: Batch 0/1
Batch Loss = 0.0023226035638017344
477, epoch_train_loss=0.0023226035638017344
Epoch 478
Epoch 478 :: Batch 0/1
Batch Loss = 0.0023219172124380053
478, epoch_train_loss=0.0023219172124380053
Epoch 479
Epoch 479 :: Batch 0/1
Batch Loss = 0.0023212978786241348
479, epoch_train_loss=0.0023212978786241348
Epoch 480
Epoch 480 :: Batch 0/1
Batch Loss = 0.0023206275204543463
480, epoch_train_loss=0.0023206275204543463
Epoch 481
Epoch 481 :: Batch 0/1
Batch Loss = 0.0023199488686270206
481, epoch_train_loss=0.0023199488686270206
Epoch 482
Epoch 482 :: Batch 0/1
Batch Loss = 0.002319326012713202
482, epoch_train_loss=0.002319326012713202
Epoch 483
Epoch 483 :: Batch 0/1
Batch Loss = 0.002318680059761509
483, epoch_train_loss=0.002318680059761509
Epoch 484
Epoch 484 :: Batch 0/1
Batch Loss = 0.002318012679852375
484, epoch_train_loss=0.002318012679852375
Epoch 485
Epoch 485 :: Batch 0/1
Batch Loss = 0.0023173870149424997
485, epoch_train_loss=0.0023173870149424997
Epoch 486
Epoch 486 :: Batch 0/1
Batch Loss = 0.0023167584988559556
486, epoch_train_loss=0.0023167584988559556
Epoch 487
Epoch 487 :: Batch 0/1
Batch Loss = 0.0023161059130747245
487, epoch_train_loss=0.0023161059130747245
Epoch 488
Epoch 488 :: Batch 0/1
Batch Loss = 0.002315478676266003
488, epoch_train_loss=0.002315478676266003
Epoch 489
Epoch 489 :: Batch 0/1
Batch Loss = 0.002314862175461148
489, epoch_train_loss=0.002314862175461148
Epoch 490
Epoch 490 :: Batch 0/1
Batch Loss = 0.0023142256959636375
490, epoch_train_loss=0.0023142256959636375
Epoch 491
Epoch 491 :: Batch 0/1
Batch Loss = 0.0023135997118033293
491, epoch_train_loss=0.0023135997118033293
Epoch 492
Epoch 492 :: Batch 0/1
Batch Loss = 0.002312990629088588
492, epoch_train_loss=0.002312990629088588
Epoch 493
Epoch 493 :: Batch 0/1
Batch Loss = 0.00231236954450388
493, epoch_train_loss=0.00231236954450388
Epoch 494
Epoch 494 :: Batch 0/1
Batch Loss = 0.00231174853039175
494, epoch_train_loss=0.00231174853039175
Epoch 495
Epoch 495 :: Batch 0/1
Batch Loss = 0.0023111435521551453
495, epoch_train_loss=0.0023111435521551453
Epoch 496
Epoch 496 :: Batch 0/1
Batch Loss = 0.002310535851171465
496, epoch_train_loss=0.002310535851171465
Epoch 497
Epoch 497 :: Batch 0/1
Batch Loss = 0.0023099227989202623
497, epoch_train_loss=0.0023099227989202623
Epoch 498
Epoch 498 :: Batch 0/1
Batch Loss = 0.0023093209011184894
498, epoch_train_loss=0.0023093209011184894
Epoch 499
Epoch 499 :: Batch 0/1
Batch Loss = 0.0023087233828559257
499, epoch_train_loss=0.0023087233828559257
Epoch 500
Epoch 500 :: Batch 0/1
Batch Loss = 0.002308120289530301
500, epoch_train_loss=0.002308120289530301
Epoch 501
Epoch 501 :: Batch 0/1
Batch Loss = 0.002307522296994452
501, epoch_train_loss=0.002307522296994452
Epoch 502
Epoch 502 :: Batch 0/1
Batch Loss = 0.0023069317934494705
502, epoch_train_loss=0.0023069317934494705
Epoch 503
Epoch 503 :: Batch 0/1
Batch Loss = 0.0023063390367893863
503, epoch_train_loss=0.0023063390367893863
Epoch 504
Epoch 504 :: Batch 0/1
Batch Loss = 0.0023057466528954144
504, epoch_train_loss=0.0023057466528954144
Epoch 505
Epoch 505 :: Batch 0/1
Batch Loss = 0.0023051612189111618
505, epoch_train_loss=0.0023051612189111618
Epoch 506
Epoch 506 :: Batch 0/1
Batch Loss = 0.0023045775032203607
506, epoch_train_loss=0.0023045775032203607
Epoch 507
Epoch 507 :: Batch 0/1
Batch Loss = 0.0023039924691114
507, epoch_train_loss=0.0023039924691114
Epoch 508
Epoch 508 :: Batch 0/1
Batch Loss = 0.0023034116452498622
508, epoch_train_loss=0.0023034116452498622
Epoch 509
Epoch 509 :: Batch 0/1
Batch Loss = 0.0023028349792998544
509, epoch_train_loss=0.0023028349792998544
Epoch 510
Epoch 510 :: Batch 0/1
Batch Loss = 0.002302258048295505
510, epoch_train_loss=0.002302258048295505
Epoch 511
Epoch 511 :: Batch 0/1
Batch Loss = 0.0023016825762995205
511, epoch_train_loss=0.0023016825762995205
Epoch 512
Epoch 512 :: Batch 0/1
Batch Loss = 0.0023011114352450603
512, epoch_train_loss=0.0023011114352450603
Epoch 513
Epoch 513 :: Batch 0/1
Batch Loss = 0.002300542055280686
513, epoch_train_loss=0.002300542055280686
Epoch 514
Epoch 514 :: Batch 0/1
Batch Loss = 0.002299973021807358
514, epoch_train_loss=0.002299973021807358
Epoch 515
Epoch 515 :: Batch 0/1
Batch Loss = 0.0022994068996154654
515, epoch_train_loss=0.0022994068996154654
Epoch 516
Epoch 516 :: Batch 0/1
Batch Loss = 0.0022988438352476563
516, epoch_train_loss=0.0022988438352476563
Epoch 517
Epoch 517 :: Batch 0/1
Batch Loss = 0.002298281701278502
517, epoch_train_loss=0.002298281701278502
Epoch 518
Epoch 518 :: Batch 0/1
Batch Loss = 0.0022977210125116666
518, epoch_train_loss=0.0022977210125116666
Epoch 519
Epoch 519 :: Batch 0/1
Batch Loss = 0.002297163282657789
519, epoch_train_loss=0.002297163282657789
Epoch 520
Epoch 520 :: Batch 0/1
Batch Loss = 0.0022966075852712477
520, epoch_train_loss=0.0022966075852712477
Epoch 521
Epoch 521 :: Batch 0/1
Batch Loss = 0.002296052934974286
521, epoch_train_loss=0.002296052934974286
Epoch 522
Epoch 522 :: Batch 0/1
Batch Loss = 0.0022955003223584397
522, epoch_train_loss=0.0022955003223584397
Epoch 523
Epoch 523 :: Batch 0/1
Batch Loss = 0.0022949502210798662
523, epoch_train_loss=0.0022949502210798662
Epoch 524
Epoch 524 :: Batch 0/1
Batch Loss = 0.0022944016886809067
524, epoch_train_loss=0.0022944016886809067
Epoch 525
Epoch 525 :: Batch 0/1
Batch Loss = 0.0022938545245680913
525, epoch_train_loss=0.0022938545245680913
Epoch 526
Epoch 526 :: Batch 0/1
Batch Loss = 0.0022933095133604365
526, epoch_train_loss=0.0022933095133604365
Epoch 527
Epoch 527 :: Batch 0/1
Batch Loss = 0.0022927665966492954
527, epoch_train_loss=0.0022927665966492954
Epoch 528
Epoch 528 :: Batch 0/1
Batch Loss = 0.0022922251189319053
528, epoch_train_loss=0.0022922251189319053
Epoch 529
Epoch 529 :: Batch 0/1
Batch Loss = 0.002291685239917719
529, epoch_train_loss=0.002291685239917719
Epoch 530
Epoch 530 :: Batch 0/1
Batch Loss = 0.0022911474092282558
530, epoch_train_loss=0.0022911474092282558
Epoch 531
Epoch 531 :: Batch 0/1
Batch Loss = 0.0022906113983437106
531, epoch_train_loss=0.0022906113983437106
Epoch 532
Epoch 532 :: Batch 0/1
Batch Loss = 0.0022900768503525083
532, epoch_train_loss=0.0022900768503525083
Epoch 533
Epoch 533 :: Batch 0/1
Batch Loss = 0.0022895439747461026
533, epoch_train_loss=0.0022895439747461026
Epoch 534
Epoch 534 :: Batch 0/1
Batch Loss = 0.002289013006086717
534, epoch_train_loss=0.002289013006086717
Epoch 535
Epoch 535 :: Batch 0/1
Batch Loss = 0.0022884837129096298
535, epoch_train_loss=0.0022884837129096298
Epoch 536
Epoch 536 :: Batch 0/1
Batch Loss = 0.0022879559124255873
536, epoch_train_loss=0.0022879559124255873
Epoch 537
Epoch 537 :: Batch 0/1
Batch Loss = 0.0022874297842745794
537, epoch_train_loss=0.0022874297842745794
Epoch 538
Epoch 538 :: Batch 0/1
Batch Loss = 0.002286905433213736
538, epoch_train_loss=0.002286905433213736
Epoch 539
Epoch 539 :: Batch 0/1
Batch Loss = 0.002286382681868855
539, epoch_train_loss=0.002286382681868855
Epoch 540
Epoch 540 :: Batch 0/1
Batch Loss = 0.0022858614372125233
540, epoch_train_loss=0.0022858614372125233
Epoch 541
Epoch 541 :: Batch 0/1
Batch Loss = 0.002285341823685126
541, epoch_train_loss=0.002285341823685126
Epoch 542
Epoch 542 :: Batch 0/1
Batch Loss = 0.0022848238878133385
542, epoch_train_loss=0.0022848238878133385
Epoch 543
Epoch 543 :: Batch 0/1
Batch Loss = 0.0022843075086065538
543, epoch_train_loss=0.0022843075086065538
Epoch 544
Epoch 544 :: Batch 0/1
Batch Loss = 0.0022837926281870824
544, epoch_train_loss=0.0022837926281870824
Epoch 545
Epoch 545 :: Batch 0/1
Batch Loss = 0.002283279327726892
545, epoch_train_loss=0.002283279327726892
Epoch 546
Epoch 546 :: Batch 0/1
Batch Loss = 0.0022827676324672897
546, epoch_train_loss=0.0022827676324672897
Epoch 547
Epoch 547 :: Batch 0/1
Batch Loss = 0.0022822574582662372
547, epoch_train_loss=0.0022822574582662372
Epoch 548
Epoch 548 :: Batch 0/1
Batch Loss = 0.002281748767662206
548, epoch_train_loss=0.002281748767662206
Epoch 549
Epoch 549 :: Batch 0/1
Batch Loss = 0.0022812416053518314
549, epoch_train_loss=0.0022812416053518314
Epoch 550
Epoch 550 :: Batch 0/1
Batch Loss = 0.0022807359890046138
550, epoch_train_loss=0.0022807359890046138
Epoch 551
Epoch 551 :: Batch 0/1
Batch Loss = 0.0022802318667623604
551, epoch_train_loss=0.0022802318667623604
Epoch 552
Epoch 552 :: Batch 0/1
Batch Loss = 0.0022797292032184146
552, epoch_train_loss=0.0022797292032184146
Epoch 553
Epoch 553 :: Batch 0/1
Batch Loss = 0.0022792280237840614
553, epoch_train_loss=0.0022792280237840614
Epoch 554
Epoch 554 :: Batch 0/1
Batch Loss = 0.002278728341856184
554, epoch_train_loss=0.002278728341856184
Epoch 555
Epoch 555 :: Batch 0/1
Batch Loss = 0.0022782301252899507
555, epoch_train_loss=0.0022782301252899507
Epoch 556
Epoch 556 :: Batch 0/1
Batch Loss = 0.0022777333456594933
556, epoch_train_loss=0.0022777333456594933
Epoch 557
Epoch 557 :: Batch 0/1
Batch Loss = 0.002277238009582513
557, epoch_train_loss=0.002277238009582513
Epoch 558
Epoch 558 :: Batch 0/1
Batch Loss = 0.002276744129484128
558, epoch_train_loss=0.002276744129484128
Epoch 559
Epoch 559 :: Batch 0/1
Batch Loss = 0.002276251687979613
559, epoch_train_loss=0.002276251687979613
Epoch 560
Epoch 560 :: Batch 0/1
Batch Loss = 0.0022757606597008186
560, epoch_train_loss=0.0022757606597008186
Epoch 561
Epoch 561 :: Batch 0/1
Batch Loss = 0.0022752710426022864
561, epoch_train_loss=0.0022752710426022864
Epoch 562
Epoch 562 :: Batch 0/1
Batch Loss = 0.002274782843538894
562, epoch_train_loss=0.002274782843538894
Epoch 563
Epoch 563 :: Batch 0/1
Batch Loss = 0.0022742960551255997
563, epoch_train_loss=0.0022742960551255997
Epoch 564
Epoch 564 :: Batch 0/1
Batch Loss = 0.0022738106588613646
564, epoch_train_loss=0.0022738106588613646
Epoch 565
Epoch 565 :: Batch 0/1
Batch Loss = 0.002273326645237453
565, epoch_train_loss=0.002273326645237453
Epoch 566
Epoch 566 :: Batch 0/1
Batch Loss = 0.0022728440170054903
566, epoch_train_loss=0.0022728440170054903
Epoch 567
Epoch 567 :: Batch 0/1
Batch Loss = 0.0022723627716717564
567, epoch_train_loss=0.0022723627716717564
Epoch 568
Epoch 568 :: Batch 0/1
Batch Loss = 0.002271882897100527
568, epoch_train_loss=0.002271882897100527
Epoch 569
Epoch 569 :: Batch 0/1
Batch Loss = 0.002271404382474414
569, epoch_train_loss=0.002271404382474414
Epoch 570
Epoch 570 :: Batch 0/1
Batch Loss = 0.00227092722454205
570, epoch_train_loss=0.00227092722454205
Epoch 571
Epoch 571 :: Batch 0/1
Batch Loss = 0.0022704514227470645
571, epoch_train_loss=0.0022704514227470645
Epoch 572
Epoch 572 :: Batch 0/1
Batch Loss = 0.0022699769705029193
572, epoch_train_loss=0.0022699769705029193
Epoch 573
Epoch 573 :: Batch 0/1
Batch Loss = 0.002269503857721035
573, epoch_train_loss=0.002269503857721035
Epoch 574
Epoch 574 :: Batch 0/1
Batch Loss = 0.0022690320784459353
574, epoch_train_loss=0.0022690320784459353
Epoch 575
Epoch 575 :: Batch 0/1
Batch Loss = 0.0022685616301927193
575, epoch_train_loss=0.0022685616301927193
Epoch 576
Epoch 576 :: Batch 0/1
Batch Loss = 0.0022680925097840242
576, epoch_train_loss=0.0022680925097840242
Epoch 577
Epoch 577 :: Batch 0/1
Batch Loss = 0.002267624710353926
577, epoch_train_loss=0.002267624710353926
Epoch 578
Epoch 578 :: Batch 0/1
Batch Loss = 0.002267158224586079
578, epoch_train_loss=0.002267158224586079
Epoch 579
Epoch 579 :: Batch 0/1
Batch Loss = 0.002266693048383399
579, epoch_train_loss=0.002266693048383399
Epoch 580
Epoch 580 :: Batch 0/1
Batch Loss = 0.002266229178650713
580, epoch_train_loss=0.002266229178650713
Epoch 581
Epoch 581 :: Batch 0/1
Batch Loss = 0.002265766611279557
581, epoch_train_loss=0.002265766611279557
Epoch 582
Epoch 582 :: Batch 0/1
Batch Loss = 0.002265305340454139
582, epoch_train_loss=0.002265305340454139
Epoch 583
Epoch 583 :: Batch 0/1
Batch Loss = 0.002264845360508232
583, epoch_train_loss=0.002264845360508232
Epoch 584
Epoch 584 :: Batch 0/1
Batch Loss = 0.0022643866677352657
584, epoch_train_loss=0.0022643866677352657
Epoch 585
Epoch 585 :: Batch 0/1
Batch Loss = 0.0022639292587839357
585, epoch_train_loss=0.0022639292587839357
Epoch 586
Epoch 586 :: Batch 0/1
Batch Loss = 0.0022634731295746334
586, epoch_train_loss=0.0022634731295746334
Epoch 587
Epoch 587 :: Batch 0/1
Batch Loss = 0.002263018275220396
587, epoch_train_loss=0.002263018275220396
Epoch 588
Epoch 588 :: Batch 0/1
Batch Loss = 0.002262564691031382
588, epoch_train_loss=0.002262564691031382
Epoch 589
Epoch 589 :: Batch 0/1
Batch Loss = 0.0022621123735172654
589, epoch_train_loss=0.0022621123735172654
Epoch 590
Epoch 590 :: Batch 0/1
Batch Loss = 0.0022616613192512794
590, epoch_train_loss=0.0022616613192512794
Epoch 591
Epoch 591 :: Batch 0/1
Batch Loss = 0.0022612115245258697
591, epoch_train_loss=0.0022612115245258697
Epoch 592
Epoch 592 :: Batch 0/1
Batch Loss = 0.0022607629851979626
592, epoch_train_loss=0.0022607629851979626
Epoch 593
Epoch 593 :: Batch 0/1
Batch Loss = 0.0022603156971220183
593, epoch_train_loss=0.0022603156971220183
Epoch 594
Epoch 594 :: Batch 0/1
Batch Loss = 0.0022598696569101286
594, epoch_train_loss=0.0022598696569101286
Epoch 595
Epoch 595 :: Batch 0/1
Batch Loss = 0.0022594248612120478
595, epoch_train_loss=0.0022594248612120478
Epoch 596
Epoch 596 :: Batch 0/1
Batch Loss = 0.0022589813066767276
596, epoch_train_loss=0.0022589813066767276
Epoch 597
Epoch 597 :: Batch 0/1
Batch Loss = 0.0022585389896605866
597, epoch_train_loss=0.0022585389896605866
Epoch 598
Epoch 598 :: Batch 0/1
Batch Loss = 0.0022580979064691893
598, epoch_train_loss=0.0022580979064691893
Epoch 599
Epoch 599 :: Batch 0/1
Batch Loss = 0.0022576580537877135
599, epoch_train_loss=0.0022576580537877135
Epoch 600
Epoch 600 :: Batch 0/1
Batch Loss = 0.002257219428422036
600, epoch_train_loss=0.002257219428422036
Epoch 601
Epoch 601 :: Batch 0/1
Batch Loss = 0.002256782027267844
601, epoch_train_loss=0.002256782027267844
Epoch 602
Epoch 602 :: Batch 0/1
Batch Loss = 0.0022563458470070993
602, epoch_train_loss=0.0022563458470070993
Epoch 603
Epoch 603 :: Batch 0/1
Batch Loss = 0.0022559108842873463
603, epoch_train_loss=0.0022559108842873463
Epoch 604
Epoch 604 :: Batch 0/1
Batch Loss = 0.0022554771359431175
604, epoch_train_loss=0.0022554771359431175
Epoch 605
Epoch 605 :: Batch 0/1
Batch Loss = 0.0022550445988045063
605, epoch_train_loss=0.0022550445988045063
Epoch 606
Epoch 606 :: Batch 0/1
Batch Loss = 0.0022546132699332116
606, epoch_train_loss=0.0022546132699332116
Epoch 607
Epoch 607 :: Batch 0/1
Batch Loss = 0.0022541831462764084
607, epoch_train_loss=0.0022541831462764084
Epoch 608
Epoch 608 :: Batch 0/1
Batch Loss = 0.0022537542247033538
608, epoch_train_loss=0.0022537542247033538
Epoch 609
Epoch 609 :: Batch 0/1
Batch Loss = 0.0022533265021958434
609, epoch_train_loss=0.0022533265021958434
Epoch 610
Epoch 610 :: Batch 0/1
Batch Loss = 0.0022528999756671847
610, epoch_train_loss=0.0022528999756671847
Epoch 611
Epoch 611 :: Batch 0/1
Batch Loss = 0.0022524746422598546
611, epoch_train_loss=0.0022524746422598546
Epoch 612
Epoch 612 :: Batch 0/1
Batch Loss = 0.002252050499000455
612, epoch_train_loss=0.002252050499000455
Epoch 613
Epoch 613 :: Batch 0/1
Batch Loss = 0.0022516275429682207
613, epoch_train_loss=0.0022516275429682207
Epoch 614
Epoch 614 :: Batch 0/1
Batch Loss = 0.002251205771258386
614, epoch_train_loss=0.002251205771258386
Epoch 615
Epoch 615 :: Batch 0/1
Batch Loss = 0.0022507851809003333
615, epoch_train_loss=0.0022507851809003333
Epoch 616
Epoch 616 :: Batch 0/1
Batch Loss = 0.0022503657689745766
616, epoch_train_loss=0.0022503657689745766
Epoch 617
Epoch 617 :: Batch 0/1
Batch Loss = 0.0022499475325734883
617, epoch_train_loss=0.0022499475325734883
Epoch 618
Epoch 618 :: Batch 0/1
Batch Loss = 0.0022495304688520024
618, epoch_train_loss=0.0022495304688520024
Epoch 619
Epoch 619 :: Batch 0/1
Batch Loss = 0.0022491145749210318
619, epoch_train_loss=0.0022491145749210318
Epoch 620
Epoch 620 :: Batch 0/1
Batch Loss = 0.0022486998479218415
620, epoch_train_loss=0.0022486998479218415
Epoch 621
Epoch 621 :: Batch 0/1
Batch Loss = 0.0022482862849390233
621, epoch_train_loss=0.0022482862849390233
Epoch 622
Epoch 622 :: Batch 0/1
Batch Loss = 0.002247873883055345
622, epoch_train_loss=0.002247873883055345
Epoch 623
Epoch 623 :: Batch 0/1
Batch Loss = 0.0022474626394079975
623, epoch_train_loss=0.0022474626394079975
Epoch 624
Epoch 624 :: Batch 0/1
Batch Loss = 0.0022470525510954607
624, epoch_train_loss=0.0022470525510954607
Epoch 625
Epoch 625 :: Batch 0/1
Batch Loss = 0.0022466436152682625
625, epoch_train_loss=0.0022466436152682625
Epoch 626
Epoch 626 :: Batch 0/1
Batch Loss = 0.002246235828984837
626, epoch_train_loss=0.002246235828984837
Epoch 627
Epoch 627 :: Batch 0/1
Batch Loss = 0.0022458291893209326
627, epoch_train_loss=0.0022458291893209326
Epoch 628
Epoch 628 :: Batch 0/1
Batch Loss = 0.0022454236933514664
628, epoch_train_loss=0.0022454236933514664
Epoch 629
Epoch 629 :: Batch 0/1
Batch Loss = 0.0022450193381187335
629, epoch_train_loss=0.0022450193381187335
Epoch 630
Epoch 630 :: Batch 0/1
Batch Loss = 0.002244616120687689
630, epoch_train_loss=0.002244616120687689
Epoch 631
Epoch 631 :: Batch 0/1
Batch Loss = 0.0022442140380801003
631, epoch_train_loss=0.0022442140380801003
Epoch 632
Epoch 632 :: Batch 0/1
Batch Loss = 0.0022438130873227015
632, epoch_train_loss=0.0022438130873227015
Epoch 633
Epoch 633 :: Batch 0/1
Batch Loss = 0.0022434132654096397
633, epoch_train_loss=0.0022434132654096397
Epoch 634
Epoch 634 :: Batch 0/1
Batch Loss = 0.0022430145693140293
634, epoch_train_loss=0.0022430145693140293
Epoch 635
Epoch 635 :: Batch 0/1
Batch Loss = 0.002242616995992573
635, epoch_train_loss=0.002242616995992573
Epoch 636
Epoch 636 :: Batch 0/1
Batch Loss = 0.0022422205423383917
636, epoch_train_loss=0.0022422205423383917
Epoch 637
Epoch 637 :: Batch 0/1
Batch Loss = 0.0022418252053310822
637, epoch_train_loss=0.0022418252053310822
Epoch 638
Epoch 638 :: Batch 0/1
Batch Loss = 0.002241430981785495
638, epoch_train_loss=0.002241430981785495
Epoch 639
Epoch 639 :: Batch 0/1
Batch Loss = 0.0022410378686023615
639, epoch_train_loss=0.0022410378686023615
Epoch 640
Epoch 640 :: Batch 0/1
Batch Loss = 0.0022406458626105254
640, epoch_train_loss=0.0022406458626105254
Epoch 641
Epoch 641 :: Batch 0/1
Batch Loss = 0.002240254960618506
641, epoch_train_loss=0.002240254960618506
Epoch 642
Epoch 642 :: Batch 0/1
Batch Loss = 0.0022398651593741607
642, epoch_train_loss=0.0022398651593741607
Epoch 643
Epoch 643 :: Batch 0/1
Batch Loss = 0.0022394764556517392
643, epoch_train_loss=0.0022394764556517392
Epoch 644
Epoch 644 :: Batch 0/1
Batch Loss = 0.0022390888461715957
644, epoch_train_loss=0.0022390888461715957
Epoch 645
Epoch 645 :: Batch 0/1
Batch Loss = 0.002238702327583709
645, epoch_train_loss=0.002238702327583709
Epoch 646
Epoch 646 :: Batch 0/1
Batch Loss = 0.002238316896578382
646, epoch_train_loss=0.002238316896578382
Epoch 647
Epoch 647 :: Batch 0/1
Batch Loss = 0.0022379325497703184
647, epoch_train_loss=0.0022379325497703184
Epoch 648
Epoch 648 :: Batch 0/1
Batch Loss = 0.0022375492837238596
648, epoch_train_loss=0.0022375492837238596
Epoch 649
Epoch 649 :: Batch 0/1
Batch Loss = 0.002237167095018537
649, epoch_train_loss=0.002237167095018537
Epoch 650
Epoch 650 :: Batch 0/1
Batch Loss = 0.002236785980146068
650, epoch_train_loss=0.002236785980146068
Epoch 651
Epoch 651 :: Batch 0/1
Batch Loss = 0.0022364059355888454
651, epoch_train_loss=0.0022364059355888454
Epoch 652
Epoch 652 :: Batch 0/1
Batch Loss = 0.0022360269578226904
652, epoch_train_loss=0.0022360269578226904
Epoch 653
Epoch 653 :: Batch 0/1
Batch Loss = 0.0022356490432303916
653, epoch_train_loss=0.0022356490432303916
Epoch 654
Epoch 654 :: Batch 0/1
Batch Loss = 0.002235272188210777
654, epoch_train_loss=0.002235272188210777
Epoch 655
Epoch 655 :: Batch 0/1
Batch Loss = 0.0022348963891025554
655, epoch_train_loss=0.0022348963891025554
Epoch 656
Epoch 656 :: Batch 0/1
Batch Loss = 0.002234521642169727
656, epoch_train_loss=0.002234521642169727
Epoch 657
Epoch 657 :: Batch 0/1
Batch Loss = 0.0022341479436747053
657, epoch_train_loss=0.0022341479436747053
Epoch 658
Epoch 658 :: Batch 0/1
Batch Loss = 0.0022337752898337836
658, epoch_train_loss=0.0022337752898337836
Epoch 659
Epoch 659 :: Batch 0/1
Batch Loss = 0.002233403676859465
659, epoch_train_loss=0.002233403676859465
Epoch 660
Epoch 660 :: Batch 0/1
Batch Loss = 0.002233033100861439
660, epoch_train_loss=0.002233033100861439
Epoch 661
Epoch 661 :: Batch 0/1
Batch Loss = 0.0022326635579439677
661, epoch_train_loss=0.0022326635579439677
Epoch 662
Epoch 662 :: Batch 0/1
Batch Loss = 0.0022322950441949424
662, epoch_train_loss=0.0022322950441949424
Epoch 663
Epoch 663 :: Batch 0/1
Batch Loss = 0.0022319275555806547
663, epoch_train_loss=0.0022319275555806547
Epoch 664
Epoch 664 :: Batch 0/1
Batch Loss = 0.002231561088138101
664, epoch_train_loss=0.002231561088138101
Epoch 665
Epoch 665 :: Batch 0/1
Batch Loss = 0.0022311956377800935
665, epoch_train_loss=0.0022311956377800935
Epoch 666
Epoch 666 :: Batch 0/1
Batch Loss = 0.002230831200435081
666, epoch_train_loss=0.002230831200435081
Epoch 667
Epoch 667 :: Batch 0/1
Batch Loss = 0.002230467771906647
667, epoch_train_loss=0.002230467771906647
Epoch 668
Epoch 668 :: Batch 0/1
Batch Loss = 0.0022301053480709818
668, epoch_train_loss=0.0022301053480709818
Epoch 669
Epoch 669 :: Batch 0/1
Batch Loss = 0.0022297439246502584
669, epoch_train_loss=0.0022297439246502584
Epoch 670
Epoch 670 :: Batch 0/1
Batch Loss = 0.002229383497440285
670, epoch_train_loss=0.002229383497440285
Epoch 671
Epoch 671 :: Batch 0/1
Batch Loss = 0.0022290240621098824
671, epoch_train_loss=0.0022290240621098824
Epoch 672
Epoch 672 :: Batch 0/1
Batch Loss = 0.0022286656143191926
672, epoch_train_loss=0.0022286656143191926
Epoch 673
Epoch 673 :: Batch 0/1
Batch Loss = 0.00222830814965675
673, epoch_train_loss=0.00222830814965675
Epoch 674
Epoch 674 :: Batch 0/1
Batch Loss = 0.002227951663757886
674, epoch_train_loss=0.002227951663757886
Epoch 675
Epoch 675 :: Batch 0/1
Batch Loss = 0.0022275961521036488
675, epoch_train_loss=0.0022275961521036488
Epoch 676
Epoch 676 :: Batch 0/1
Batch Loss = 0.0022272416102503646
676, epoch_train_loss=0.0022272416102503646
Epoch 677
Epoch 677 :: Batch 0/1
Batch Loss = 0.00222688803360022
677, epoch_train_loss=0.00222688803360022
Epoch 678
Epoch 678 :: Batch 0/1
Batch Loss = 0.002226535417604746
678, epoch_train_loss=0.002226535417604746
Epoch 679
Epoch 679 :: Batch 0/1
Batch Loss = 0.00222618375764235
679, epoch_train_loss=0.00222618375764235
Epoch 680
Epoch 680 :: Batch 0/1
Batch Loss = 0.002225833049061806
680, epoch_train_loss=0.002225833049061806
Epoch 681
Epoch 681 :: Batch 0/1
Batch Loss = 0.0022254832871375792
681, epoch_train_loss=0.0022254832871375792
Epoch 682
Epoch 682 :: Batch 0/1
Batch Loss = 0.0022251344671716925
682, epoch_train_loss=0.0022251344671716925
Epoch 683
Epoch 683 :: Batch 0/1
Batch Loss = 0.0022247865843893736
683, epoch_train_loss=0.0022247865843893736
Epoch 684
Epoch 684 :: Batch 0/1
Batch Loss = 0.0022244396339948765
684, epoch_train_loss=0.0022244396339948765
Epoch 685
Epoch 685 :: Batch 0/1
Batch Loss = 0.002224093611138688
685, epoch_train_loss=0.002224093611138688
Epoch 686
Epoch 686 :: Batch 0/1
Batch Loss = 0.0022237485109321167
686, epoch_train_loss=0.0022237485109321167
Epoch 687
Epoch 687 :: Batch 0/1
Batch Loss = 0.002223404328479064
687, epoch_train_loss=0.002223404328479064
Epoch 688
Epoch 688 :: Batch 0/1
Batch Loss = 0.002223061058831484
688, epoch_train_loss=0.002223061058831484
Epoch 689
Epoch 689 :: Batch 0/1
Batch Loss = 0.0022227186970240247
689, epoch_train_loss=0.0022227186970240247
Epoch 690
Epoch 690 :: Batch 0/1
Batch Loss = 0.002222377238090104
690, epoch_train_loss=0.002222377238090104
Epoch 691
Epoch 691 :: Batch 0/1
Batch Loss = 0.0022220366769550577
691, epoch_train_loss=0.0022220366769550577
Epoch 692
Epoch 692 :: Batch 0/1
Batch Loss = 0.0022216970086710195
692, epoch_train_loss=0.0022216970086710195
Epoch 693
Epoch 693 :: Batch 0/1
Batch Loss = 0.0022213582281507918
693, epoch_train_loss=0.0022213582281507918
Epoch 694
Epoch 694 :: Batch 0/1
Batch Loss = 0.00222102033052388
694, epoch_train_loss=0.00222102033052388
Epoch 695
Epoch 695 :: Batch 0/1
Batch Loss = 0.0022206833109080543
695, epoch_train_loss=0.0022206833109080543
Epoch 696
Epoch 696 :: Batch 0/1
Batch Loss = 0.0022203471648794904
696, epoch_train_loss=0.0022203471648794904
Epoch 697
Epoch 697 :: Batch 0/1
Batch Loss = 0.0022200118883403345
697, epoch_train_loss=0.0022200118883403345
Epoch 698
Epoch 698 :: Batch 0/1
Batch Loss = 0.0022196774786706276
698, epoch_train_loss=0.0022196774786706276
Epoch 699
Epoch 699 :: Batch 0/1
Batch Loss = 0.0022193439349979943
699, epoch_train_loss=0.0022193439349979943
Epoch 700
Epoch 700 :: Batch 0/1
Batch Loss = 0.00221901126165349
700, epoch_train_loss=0.00221901126165349
Epoch 701
Epoch 701 :: Batch 0/1
Batch Loss = 0.0022186794709237573
701, epoch_train_loss=0.0022186794709237573
Epoch 702
Epoch 702 :: Batch 0/1
Batch Loss = 0.0022183485945205363
702, epoch_train_loss=0.0022183485945205363
Epoch 703
Epoch 703 :: Batch 0/1
Batch Loss = 0.0022180186993415105
703, epoch_train_loss=0.0022180186993415105
Epoch 704
Epoch 704 :: Batch 0/1
Batch Loss = 0.002217689930780014
704, epoch_train_loss=0.002217689930780014
Epoch 705
Epoch 705 :: Batch 0/1
Batch Loss = 0.0022173625892459464
705, epoch_train_loss=0.0022173625892459464
Epoch 706
Epoch 706 :: Batch 0/1
Batch Loss = 0.0022170373117418453
706, epoch_train_loss=0.0022170373117418453
Epoch 707
Epoch 707 :: Batch 0/1
Batch Loss = 0.0022167154351844464
707, epoch_train_loss=0.0022167154351844464
Epoch 708
Epoch 708 :: Batch 0/1
Batch Loss = 0.0022163998105908136
708, epoch_train_loss=0.0022163998105908136
Epoch 709
Epoch 709 :: Batch 0/1
Batch Loss = 0.0022160965523888958
709, epoch_train_loss=0.0022160965523888958
Epoch 710
Epoch 710 :: Batch 0/1
Batch Loss = 0.0022158188733747944
710, epoch_train_loss=0.0022158188733747944
Epoch 711
Epoch 711 :: Batch 0/1
Batch Loss = 0.0022155957694438774
711, epoch_train_loss=0.0022155957694438774
Epoch 712
Epoch 712 :: Batch 0/1
Batch Loss = 0.0022154907621575364
712, epoch_train_loss=0.0022154907621575364
Epoch 713
Epoch 713 :: Batch 0/1
Batch Loss = 0.0022156467733729316
713, epoch_train_loss=0.0022156467733729316
Epoch 714
Epoch 714 :: Batch 0/1
Batch Loss = 0.002216380134836214
714, epoch_train_loss=0.002216380134836214
Epoch 715
Epoch 715 :: Batch 0/1
Batch Loss = 0.0022184242766443425
715, epoch_train_loss=0.0022184242766443425
Epoch 716
Epoch 716 :: Batch 0/1
Batch Loss = 0.0022234050615161163
716, epoch_train_loss=0.0022234050615161163
Epoch 717
Epoch 717 :: Batch 0/1
Batch Loss = 0.002235252925779334
717, epoch_train_loss=0.002235252925779334
Epoch 718
Epoch 718 :: Batch 0/1
Batch Loss = 0.002262524767486638
718, epoch_train_loss=0.002262524767486638
Epoch 719
Epoch 719 :: Batch 0/1
Batch Loss = 0.002327353190521142
719, epoch_train_loss=0.002327353190521142
Epoch 720
Epoch 720 :: Batch 0/1
Batch Loss = 0.0024749227134362155
720, epoch_train_loss=0.0024749227134362155
Epoch 721
Epoch 721 :: Batch 0/1
Batch Loss = 0.0028374171103047203
721, epoch_train_loss=0.0028374171103047203
Epoch 722
Epoch 722 :: Batch 0/1
Batch Loss = 0.0036385204394815303
722, epoch_train_loss=0.0036385204394815303
Epoch 723
Epoch 723 :: Batch 0/1
Batch Loss = 0.005713264675193052
723, epoch_train_loss=0.005713264675193052
Epoch 724
Epoch 724 :: Batch 0/1
Batch Loss = 0.009775184422219061
724, epoch_train_loss=0.009775184422219061
Epoch 725
Epoch 725 :: Batch 0/1
Batch Loss = 0.020853594866916864
725, epoch_train_loss=0.020853594866916864
Epoch 726
Epoch 726 :: Batch 0/1
Batch Loss = 0.03348155721977079
726, epoch_train_loss=0.03348155721977079
Epoch 727
Epoch 727 :: Batch 0/1
Batch Loss = 0.06580231812818269
727, epoch_train_loss=0.06580231812818269
Epoch 728
Epoch 728 :: Batch 0/1
Batch Loss = 0.04545237117827233
728, epoch_train_loss=0.04545237117827233
Epoch 729
Epoch 729 :: Batch 0/1
Batch Loss = 0.028022283637432
729, epoch_train_loss=0.028022283637432
Epoch 730
Epoch 730 :: Batch 0/1
Batch Loss = 0.0031255901371061616
730, epoch_train_loss=0.0031255901371061616
Epoch 731
Epoch 731 :: Batch 0/1
Batch Loss = 0.01390215304637881
731, epoch_train_loss=0.01390215304637881
Epoch 732
Epoch 732 :: Batch 0/1
Batch Loss = 0.03548901662790046
732, epoch_train_loss=0.03548901662790046
Epoch 733
Epoch 733 :: Batch 0/1
Batch Loss = 0.012256177399300147
733, epoch_train_loss=0.012256177399300147
Epoch 734
Epoch 734 :: Batch 0/1
Batch Loss = 0.0034609495946481966
734, epoch_train_loss=0.0034609495946481966
Epoch 735
Epoch 735 :: Batch 0/1
Batch Loss = 0.016670627390984877
735, epoch_train_loss=0.016670627390984877
Epoch 736
Epoch 736 :: Batch 0/1
Batch Loss = 0.01056451144213071
736, epoch_train_loss=0.01056451144213071
Epoch 737
Epoch 737 :: Batch 0/1
Batch Loss = 0.0026045381205628423
737, epoch_train_loss=0.0026045381205628423
Epoch 738
Epoch 738 :: Batch 0/1
Batch Loss = 0.008662950051649961
738, epoch_train_loss=0.008662950051649961
Epoch 739
Epoch 739 :: Batch 0/1
Batch Loss = 0.007663529983526354
739, epoch_train_loss=0.007663529983526354
Epoch 740
Epoch 740 :: Batch 0/1
Batch Loss = 0.0025680878163807357
740, epoch_train_loss=0.0025680878163807357
Epoch 741
Epoch 741 :: Batch 0/1
Batch Loss = 0.005881387929928564
741, epoch_train_loss=0.005881387929928564
Epoch 742
Epoch 742 :: Batch 0/1
Batch Loss = 0.005453457129236932
742, epoch_train_loss=0.005453457129236932
Epoch 743
Epoch 743 :: Batch 0/1
Batch Loss = 0.002672296834582386
743, epoch_train_loss=0.002672296834582386
Epoch 744
Epoch 744 :: Batch 0/1
Batch Loss = 0.0047746758536326515
744, epoch_train_loss=0.0047746758536326515
Epoch 745
Epoch 745 :: Batch 0/1
Batch Loss = 0.004189255840669483
745, epoch_train_loss=0.004189255840669483
Epoch 746
Epoch 746 :: Batch 0/1
Batch Loss = 0.0026779202346171147
746, epoch_train_loss=0.0026779202346171147
Epoch 747
Epoch 747 :: Batch 0/1
Batch Loss = 0.004242564915557719
747, epoch_train_loss=0.004242564915557719
Epoch 748
Epoch 748 :: Batch 0/1
Batch Loss = 0.003413566720235297
748, epoch_train_loss=0.003413566720235297
Epoch 749
Epoch 749 :: Batch 0/1
Batch Loss = 0.002753603614355082
749, epoch_train_loss=0.002753603614355082
Epoch 750
Epoch 750 :: Batch 0/1
Batch Loss = 0.0038025347433658903
750, epoch_train_loss=0.0038025347433658903
Epoch 751
Epoch 751 :: Batch 0/1
Batch Loss = 0.002928281963078587
751, epoch_train_loss=0.002928281963078587
Epoch 752
Epoch 752 :: Batch 0/1
Batch Loss = 0.00283834059938891
752, epoch_train_loss=0.00283834059938891
Epoch 753
Epoch 753 :: Batch 0/1
Batch Loss = 0.003425450254249431
753, epoch_train_loss=0.003425450254249431
Epoch 754
Epoch 754 :: Batch 0/1
Batch Loss = 0.0026803713697216075
754, epoch_train_loss=0.0026803713697216075
Epoch 755
Epoch 755 :: Batch 0/1
Batch Loss = 0.002818283815608675
755, epoch_train_loss=0.002818283815608675
Epoch 756
Epoch 756 :: Batch 0/1
Batch Loss = 0.0031559620306053416
756, epoch_train_loss=0.0031559620306053416
Epoch 757
Epoch 757 :: Batch 0/1
Batch Loss = 0.0025315119113734244
757, epoch_train_loss=0.0025315119113734244
Epoch 758
Epoch 758 :: Batch 0/1
Batch Loss = 0.002779281733840015
758, epoch_train_loss=0.002779281733840015
Epoch 759
Epoch 759 :: Batch 0/1
Batch Loss = 0.0029235494336010545
759, epoch_train_loss=0.0029235494336010545
Epoch 760
Epoch 760 :: Batch 0/1
Batch Loss = 0.00246322921015791
760, epoch_train_loss=0.00246322921015791
Epoch 761
Epoch 761 :: Batch 0/1
Batch Loss = 0.002700561645844831
761, epoch_train_loss=0.002700561645844831
Epoch 762
Epoch 762 :: Batch 0/1
Batch Loss = 0.0027549819396358146
762, epoch_train_loss=0.0027549819396358146
Epoch 763
Epoch 763 :: Batch 0/1
Batch Loss = 0.002413958513940789
763, epoch_train_loss=0.002413958513940789
Epoch 764
Epoch 764 :: Batch 0/1
Batch Loss = 0.0026012913436016236
764, epoch_train_loss=0.0026012913436016236
Epoch 765
Epoch 765 :: Batch 0/1
Batch Loss = 0.002644637274739348
765, epoch_train_loss=0.002644637274739348
Epoch 766
Epoch 766 :: Batch 0/1
Batch Loss = 0.002365646687339413
766, epoch_train_loss=0.002365646687339413
Epoch 767
Epoch 767 :: Batch 0/1
Batch Loss = 0.002514600799585241
767, epoch_train_loss=0.002514600799585241
Epoch 768
Epoch 768 :: Batch 0/1
Batch Loss = 0.002554646015061332
768, epoch_train_loss=0.002554646015061332
Epoch 769
Epoch 769 :: Batch 0/1
Batch Loss = 0.0023395743340908107
769, epoch_train_loss=0.0023395743340908107
Epoch 770
Epoch 770 :: Batch 0/1
Batch Loss = 0.002434514197801812
770, epoch_train_loss=0.002434514197801812
Epoch 771
Epoch 771 :: Batch 0/1
Batch Loss = 0.002487917688246002
771, epoch_train_loss=0.002487917688246002
Epoch 772
Epoch 772 :: Batch 0/1
Batch Loss = 0.002325863084876677
772, epoch_train_loss=0.002325863084876677
Epoch 773
Epoch 773 :: Batch 0/1
Batch Loss = 0.002365702381374205
773, epoch_train_loss=0.002365702381374205
Epoch 774
Epoch 774 :: Batch 0/1
Batch Loss = 0.0024356596314865516
774, epoch_train_loss=0.0024356596314865516
Epoch 775
Epoch 775 :: Batch 0/1
Batch Loss = 0.0023152111434514497
775, epoch_train_loss=0.0023152111434514497
Epoch 776
Epoch 776 :: Batch 0/1
Batch Loss = 0.002318773994243018
776, epoch_train_loss=0.002318773994243018
Epoch 777
Epoch 777 :: Batch 0/1
Batch Loss = 0.0023872458878928314
777, epoch_train_loss=0.0023872458878928314
Epoch 778
Epoch 778 :: Batch 0/1
Batch Loss = 0.0023117270283832176
778, epoch_train_loss=0.0023117270283832176
Epoch 779
Epoch 779 :: Batch 0/1
Batch Loss = 0.0022865372519161294
779, epoch_train_loss=0.0022865372519161294
Epoch 780
Epoch 780 :: Batch 0/1
Batch Loss = 0.0023430500429236453
780, epoch_train_loss=0.0023430500429236453
Epoch 781
Epoch 781 :: Batch 0/1
Batch Loss = 0.002310541025716627
781, epoch_train_loss=0.002310541025716627
Epoch 782
Epoch 782 :: Batch 0/1
Batch Loss = 0.002267194798899215
782, epoch_train_loss=0.002267194798899215
Epoch 783
Epoch 783 :: Batch 0/1
Batch Loss = 0.0023054717096976625
783, epoch_train_loss=0.0023054717096976625
Epoch 784
Epoch 784 :: Batch 0/1
Batch Loss = 0.002303356364201441
784, epoch_train_loss=0.002303356364201441
Epoch 785
Epoch 785 :: Batch 0/1
Batch Loss = 0.0022608349292580106
785, epoch_train_loss=0.0022608349292580106
Epoch 786
Epoch 786 :: Batch 0/1
Batch Loss = 0.002274844250362737
786, epoch_train_loss=0.002274844250362737
Epoch 787
Epoch 787 :: Batch 0/1
Batch Loss = 0.0022903872181519523
787, epoch_train_loss=0.0022903872181519523
Epoch 788
Epoch 788 :: Batch 0/1
Batch Loss = 0.002261881075843242
788, epoch_train_loss=0.002261881075843242
Epoch 789
Epoch 789 :: Batch 0/1
Batch Loss = 0.0022539885146829913
789, epoch_train_loss=0.0022539885146829913
Epoch 790
Epoch 790 :: Batch 0/1
Batch Loss = 0.002273542778490861
790, epoch_train_loss=0.002273542778490861
Epoch 791
Epoch 791 :: Batch 0/1
Batch Loss = 0.0022627107912724354
791, epoch_train_loss=0.0022627107912724354
Epoch 792
Epoch 792 :: Batch 0/1
Batch Loss = 0.002244763196061089
792, epoch_train_loss=0.002244763196061089
Epoch 793
Epoch 793 :: Batch 0/1
Batch Loss = 0.0022559540088564114
793, epoch_train_loss=0.0022559540088564114
Epoch 794
Epoch 794 :: Batch 0/1
Batch Loss = 0.0022591218828074294
794, epoch_train_loss=0.0022591218828074294
Epoch 795
Epoch 795 :: Batch 0/1
Batch Loss = 0.002243376076171339
795, epoch_train_loss=0.002243376076171339
Epoch 796
Epoch 796 :: Batch 0/1
Batch Loss = 0.002242044676785584
796, epoch_train_loss=0.002242044676785584
Epoch 797
Epoch 797 :: Batch 0/1
Batch Loss = 0.0022506317317104
797, epoch_train_loss=0.0022506317317104
Epoch 798
Epoch 798 :: Batch 0/1
Batch Loss = 0.0022436241220113134
798, epoch_train_loss=0.0022436241220113134
Epoch 799
Epoch 799 :: Batch 0/1
Batch Loss = 0.0022349241592513986
799, epoch_train_loss=0.0022349241592513986
Epoch 800
Epoch 800 :: Batch 0/1
Batch Loss = 0.002240138553278867
800, epoch_train_loss=0.002240138553278867
Epoch 801
Epoch 801 :: Batch 0/1
Batch Loss = 0.002241212751799328
801, epoch_train_loss=0.002241212751799328
Epoch 802
Epoch 802 :: Batch 0/1
Batch Loss = 0.0022331709009381267
802, epoch_train_loss=0.0022331709009381267
Epoch 803
Epoch 803 :: Batch 0/1
Batch Loss = 0.002231627186578915
803, epoch_train_loss=0.002231627186578915
Epoch 804
Epoch 804 :: Batch 0/1
Batch Loss = 0.002235408897912496
804, epoch_train_loss=0.002235408897912496
Epoch 805
Epoch 805 :: Batch 0/1
Batch Loss = 0.0022324943752615564
805, epoch_train_loss=0.0022324943752615564
Epoch 806
Epoch 806 :: Batch 0/1
Batch Loss = 0.002227271604165866
806, epoch_train_loss=0.002227271604165866
Epoch 807
Epoch 807 :: Batch 0/1
Batch Loss = 0.0022286065553816393
807, epoch_train_loss=0.0022286065553816393
Epoch 808
Epoch 808 :: Batch 0/1
Batch Loss = 0.0022297558533410184
808, epoch_train_loss=0.0022297558533410184
Epoch 809
Epoch 809 :: Batch 0/1
Batch Loss = 0.002225827690202595
809, epoch_train_loss=0.002225827690202595
Epoch 810
Epoch 810 :: Batch 0/1
Batch Loss = 0.0022236400409034796
810, epoch_train_loss=0.0022236400409034796
Epoch 811
Epoch 811 :: Batch 0/1
Batch Loss = 0.002225045782924466
811, epoch_train_loss=0.002225045782924466
Epoch 812
Epoch 812 :: Batch 0/1
Batch Loss = 0.0022243767233757175
812, epoch_train_loss=0.0022243767233757175
Epoch 813
Epoch 813 :: Batch 0/1
Batch Loss = 0.002221269628584515
813, epoch_train_loss=0.002221269628584515
Epoch 814
Epoch 814 :: Batch 0/1
Batch Loss = 0.0022205699784289767
814, epoch_train_loss=0.0022205699784289767
Epoch 815
Epoch 815 :: Batch 0/1
Batch Loss = 0.002221374524269228
815, epoch_train_loss=0.002221374524269228
Epoch 816
Epoch 816 :: Batch 0/1
Batch Loss = 0.002219901667555709
816, epoch_train_loss=0.002219901667555709
Epoch 817
Epoch 817 :: Batch 0/1
Batch Loss = 0.0022178479755417093
817, epoch_train_loss=0.0022178479755417093
Epoch 818
Epoch 818 :: Batch 0/1
Batch Loss = 0.0022177132324871815
818, epoch_train_loss=0.0022177132324871815
Epoch 819
Epoch 819 :: Batch 0/1
Batch Loss = 0.002217792249447182
819, epoch_train_loss=0.002217792249447182
Epoch 820
Epoch 820 :: Batch 0/1
Batch Loss = 0.002216328737654338
820, epoch_train_loss=0.002216328737654338
Epoch 821
Epoch 821 :: Batch 0/1
Batch Loss = 0.002214954603372844
821, epoch_train_loss=0.002214954603372844
Epoch 822
Epoch 822 :: Batch 0/1
Batch Loss = 0.0022149127350334337
822, epoch_train_loss=0.0022149127350334337
Epoch 823
Epoch 823 :: Batch 0/1
Batch Loss = 0.0022145856881810237
823, epoch_train_loss=0.0022145856881810237
Epoch 824
Epoch 824 :: Batch 0/1
Batch Loss = 0.0022132902502546344
824, epoch_train_loss=0.0022132902502546344
Epoch 825
Epoch 825 :: Batch 0/1
Batch Loss = 0.0022123799413189763
825, epoch_train_loss=0.0022123799413189763
Epoch 826
Epoch 826 :: Batch 0/1
Batch Loss = 0.0022122117084192156
826, epoch_train_loss=0.0022122117084192156
Epoch 827
Epoch 827 :: Batch 0/1
Batch Loss = 0.0022117248205863007
827, epoch_train_loss=0.0022117248205863007
Epoch 828
Epoch 828 :: Batch 0/1
Batch Loss = 0.002210666037646926
828, epoch_train_loss=0.002210666037646926
Epoch 829
Epoch 829 :: Batch 0/1
Batch Loss = 0.0022099540278343022
829, epoch_train_loss=0.0022099540278343022
Epoch 830
Epoch 830 :: Batch 0/1
Batch Loss = 0.002209708073009218
830, epoch_train_loss=0.002209708073009218
Epoch 831
Epoch 831 :: Batch 0/1
Batch Loss = 0.0022091604869324535
831, epoch_train_loss=0.0022091604869324535
Epoch 832
Epoch 832 :: Batch 0/1
Batch Loss = 0.0022082960714792208
832, epoch_train_loss=0.0022082960714792208
Epoch 833
Epoch 833 :: Batch 0/1
Batch Loss = 0.002207692370319118
833, epoch_train_loss=0.002207692370319118
Epoch 834
Epoch 834 :: Batch 0/1
Batch Loss = 0.0022073628580310117
834, epoch_train_loss=0.0022073628580310117
Epoch 835
Epoch 835 :: Batch 0/1
Batch Loss = 0.0022068399045232765
835, epoch_train_loss=0.0022068399045232765
Epoch 836
Epoch 836 :: Batch 0/1
Batch Loss = 0.002206102852485096
836, epoch_train_loss=0.002206102852485096
Epoch 837
Epoch 837 :: Batch 0/1
Batch Loss = 0.0022055516643243174
837, epoch_train_loss=0.0022055516643243174
Epoch 838
Epoch 838 :: Batch 0/1
Batch Loss = 0.0022051825345012384
838, epoch_train_loss=0.0022051825345012384
Epoch 839
Epoch 839 :: Batch 0/1
Batch Loss = 0.0022046830158354075
839, epoch_train_loss=0.0022046830158354075
Epoch 840
Epoch 840 :: Batch 0/1
Batch Loss = 0.0022040509174380218
840, epoch_train_loss=0.0022040509174380218
Epoch 841
Epoch 841 :: Batch 0/1
Batch Loss = 0.0022035250689793513
841, epoch_train_loss=0.0022035250689793513
Epoch 842
Epoch 842 :: Batch 0/1
Batch Loss = 0.002203131621544867
842, epoch_train_loss=0.002203131621544867
Epoch 843
Epoch 843 :: Batch 0/1
Batch Loss = 0.0022026682098628693
843, epoch_train_loss=0.0022026682098628693
Epoch 844
Epoch 844 :: Batch 0/1
Batch Loss = 0.0022021039546249346
844, epoch_train_loss=0.0022021039546249346
Epoch 845
Epoch 845 :: Batch 0/1
Batch Loss = 0.0022016036440026003
845, epoch_train_loss=0.0022016036440026003
Epoch 846
Epoch 846 :: Batch 0/1
Batch Loss = 0.002201195630564176
846, epoch_train_loss=0.002201195630564176
Epoch 847
Epoch 847 :: Batch 0/1
Batch Loss = 0.00220075896707766
847, epoch_train_loss=0.00220075896707766
Epoch 848
Epoch 848 :: Batch 0/1
Batch Loss = 0.0022002539958338426
848, epoch_train_loss=0.0022002539958338426
Epoch 849
Epoch 849 :: Batch 0/1
Batch Loss = 0.0021997709312438006
849, epoch_train_loss=0.0021997709312438006
Epoch 850
Epoch 850 :: Batch 0/1
Batch Loss = 0.0021993575869527714
850, epoch_train_loss=0.0021993575869527714
Epoch 851
Epoch 851 :: Batch 0/1
Batch Loss = 0.0021989420953097446
851, epoch_train_loss=0.0021989420953097446
Epoch 852
Epoch 852 :: Batch 0/1
Batch Loss = 0.0021984792351416745
852, epoch_train_loss=0.0021984792351416745
Epoch 853
Epoch 853 :: Batch 0/1
Batch Loss = 0.0021980183644613608
853, epoch_train_loss=0.0021980183644613608
Epoch 854
Epoch 854 :: Batch 0/1
Batch Loss = 0.0021976010869311185
854, epoch_train_loss=0.0021976010869311185
Epoch 855
Epoch 855 :: Batch 0/1
Batch Loss = 0.002197199384135725
855, epoch_train_loss=0.002197199384135725
Epoch 856
Epoch 856 :: Batch 0/1
Batch Loss = 0.0021967709457863765
856, epoch_train_loss=0.0021967709457863765
Epoch 857
Epoch 857 :: Batch 0/1
Batch Loss = 0.0021963302982292944
857, epoch_train_loss=0.0021963302982292944
Epoch 858
Epoch 858 :: Batch 0/1
Batch Loss = 0.002195915990484026
858, epoch_train_loss=0.002195915990484026
Epoch 859
Epoch 859 :: Batch 0/1
Batch Loss = 0.00219552182030694
859, epoch_train_loss=0.00219552182030694
Epoch 860
Epoch 860 :: Batch 0/1
Batch Loss = 0.0021951173539720824
860, epoch_train_loss=0.0021951173539720824
Epoch 861
Epoch 861 :: Batch 0/1
Batch Loss = 0.0021946995169689737
861, epoch_train_loss=0.0021946995169689737
Epoch 862
Epoch 862 :: Batch 0/1
Batch Loss = 0.0021942908722908153
862, epoch_train_loss=0.0021942908722908153
Epoch 863
Epoch 863 :: Batch 0/1
Batch Loss = 0.002193901120814846
863, epoch_train_loss=0.002193901120814846
Epoch 864
Epoch 864 :: Batch 0/1
Batch Loss = 0.0021935132517488976
864, epoch_train_loss=0.0021935132517488976
Epoch 865
Epoch 865 :: Batch 0/1
Batch Loss = 0.002193115197933901
865, epoch_train_loss=0.002193115197933901
Epoch 866
Epoch 866 :: Batch 0/1
Batch Loss = 0.002192717278960945
866, epoch_train_loss=0.002192717278960945
Epoch 867
Epoch 867 :: Batch 0/1
Batch Loss = 0.002192330987936837
867, epoch_train_loss=0.002192330987936837
Epoch 868
Epoch 868 :: Batch 0/1
Batch Loss = 0.002191952624725464
868, epoch_train_loss=0.002191952624725464
Epoch 869
Epoch 869 :: Batch 0/1
Batch Loss = 0.0021915713171487784
869, epoch_train_loss=0.0021915713171487784
Epoch 870
Epoch 870 :: Batch 0/1
Batch Loss = 0.0021911857715339925
870, epoch_train_loss=0.0021911857715339925
Epoch 871
Epoch 871 :: Batch 0/1
Batch Loss = 0.0021908049207242145
871, epoch_train_loss=0.0021908049207242145
Epoch 872
Epoch 872 :: Batch 0/1
Batch Loss = 0.002190432307811582
872, epoch_train_loss=0.002190432307811582
Epoch 873
Epoch 873 :: Batch 0/1
Batch Loss = 0.0021900620458255625
873, epoch_train_loss=0.0021900620458255625
Epoch 874
Epoch 874 :: Batch 0/1
Batch Loss = 0.002189689318778132
874, epoch_train_loss=0.002189689318778132
Epoch 875
Epoch 875 :: Batch 0/1
Batch Loss = 0.0021893164337140826
875, epoch_train_loss=0.0021893164337140826
Epoch 876
Epoch 876 :: Batch 0/1
Batch Loss = 0.002188948351314821
876, epoch_train_loss=0.002188948351314821
Epoch 877
Epoch 877 :: Batch 0/1
Batch Loss = 0.0021885851395307835
877, epoch_train_loss=0.0021885851395307835
Epoch 878
Epoch 878 :: Batch 0/1
Batch Loss = 0.002188222566401648
878, epoch_train_loss=0.002188222566401648
Epoch 879
Epoch 879 :: Batch 0/1
Batch Loss = 0.002187858987700222
879, epoch_train_loss=0.002187858987700222
Epoch 880
Epoch 880 :: Batch 0/1
Batch Loss = 0.002187496769071676
880, epoch_train_loss=0.002187496769071676
Epoch 881
Epoch 881 :: Batch 0/1
Batch Loss = 0.002187138179000932
881, epoch_train_loss=0.002187138179000932
Epoch 882
Epoch 882 :: Batch 0/1
Batch Loss = 0.002186782543426724
882, epoch_train_loss=0.002186782543426724
Epoch 883
Epoch 883 :: Batch 0/1
Batch Loss = 0.0021864274205839257
883, epoch_train_loss=0.0021864274205839257
Epoch 884
Epoch 884 :: Batch 0/1
Batch Loss = 0.002186072178581373
884, epoch_train_loss=0.002186072178581373
Epoch 885
Epoch 885 :: Batch 0/1
Batch Loss = 0.0021857185087264014
885, epoch_train_loss=0.0021857185087264014
Epoch 886
Epoch 886 :: Batch 0/1
Batch Loss = 0.002185367511434107
886, epoch_train_loss=0.002185367511434107
Epoch 887
Epoch 887 :: Batch 0/1
Batch Loss = 0.0021850184714327725
887, epoch_train_loss=0.0021850184714327725
Epoch 888
Epoch 888 :: Batch 0/1
Batch Loss = 0.00218467009451925
888, epoch_train_loss=0.00218467009451925
Epoch 889
Epoch 889 :: Batch 0/1
Batch Loss = 0.002184322094158611
889, epoch_train_loss=0.002184322094158611
Epoch 890
Epoch 890 :: Batch 0/1
Batch Loss = 0.0021839754101645086
890, epoch_train_loss=0.0021839754101645086
Epoch 891
Epoch 891 :: Batch 0/1
Batch Loss = 0.0021836306901198733
891, epoch_train_loss=0.0021836306901198733
Epoch 892
Epoch 892 :: Batch 0/1
Batch Loss = 0.0021832874836043683
892, epoch_train_loss=0.0021832874836043683
Epoch 893
Epoch 893 :: Batch 0/1
Batch Loss = 0.0021829450496103388
893, epoch_train_loss=0.0021829450496103388
Epoch 894
Epoch 894 :: Batch 0/1
Batch Loss = 0.0021826031835019345
894, epoch_train_loss=0.0021826031835019345
Epoch 895
Epoch 895 :: Batch 0/1
Batch Loss = 0.0021822623434574694
895, epoch_train_loss=0.0021822623434574694
Epoch 896
Epoch 896 :: Batch 0/1
Batch Loss = 0.002181922964941827
896, epoch_train_loss=0.002181922964941827
Epoch 897
Epoch 897 :: Batch 0/1
Batch Loss = 0.002181584843905694
897, epoch_train_loss=0.002181584843905694
Epoch 898
Epoch 898 :: Batch 0/1
Batch Loss = 0.0021812475238875513
898, epoch_train_loss=0.0021812475238875513
Epoch 899
Epoch 899 :: Batch 0/1
Batch Loss = 0.0021809108353785284
899, epoch_train_loss=0.0021809108353785284
Epoch 900
Epoch 900 :: Batch 0/1
Batch Loss = 0.002180574951592601
900, epoch_train_loss=0.002180574951592601
Epoch 901
Epoch 901 :: Batch 0/1
Batch Loss = 0.0021802401409731745
901, epoch_train_loss=0.0021802401409731745
Epoch 902
Epoch 902 :: Batch 0/1
Batch Loss = 0.002179906397459718
902, epoch_train_loss=0.002179906397459718
Epoch 903
Epoch 903 :: Batch 0/1
Batch Loss = 0.00217957345498947
903, epoch_train_loss=0.00217957345498947
Epoch 904
Epoch 904 :: Batch 0/1
Batch Loss = 0.002179241137986573
904, epoch_train_loss=0.002179241137986573
Epoch 905
Epoch 905 :: Batch 0/1
Batch Loss = 0.00217890948448714
905, epoch_train_loss=0.00217890948448714
Epoch 906
Epoch 906 :: Batch 0/1
Batch Loss = 0.0021785786321191487
906, epoch_train_loss=0.0021785786321191487
Epoch 907
Epoch 907 :: Batch 0/1
Batch Loss = 0.002178248643023207
907, epoch_train_loss=0.002178248643023207
Epoch 908
Epoch 908 :: Batch 0/1
Batch Loss = 0.002177919414047663
908, epoch_train_loss=0.002177919414047663
Epoch 909
Epoch 909 :: Batch 0/1
Batch Loss = 0.0021775907991848113
909, epoch_train_loss=0.0021775907991848113
Epoch 910
Epoch 910 :: Batch 0/1
Batch Loss = 0.002177262752716493
910, epoch_train_loss=0.002177262752716493
Epoch 911
Epoch 911 :: Batch 0/1
Batch Loss = 0.00217693532298177
911, epoch_train_loss=0.00217693532298177
Epoch 912
Epoch 912 :: Batch 0/1
Batch Loss = 0.0021766085689980917
912, epoch_train_loss=0.0021766085689980917
Epoch 913
Epoch 913 :: Batch 0/1
Batch Loss = 0.002176282481277477
913, epoch_train_loss=0.002176282481277477
Epoch 914
Epoch 914 :: Batch 0/1
Batch Loss = 0.002175956978599133
914, epoch_train_loss=0.002175956978599133
Epoch 915
Epoch 915 :: Batch 0/1
Batch Loss = 0.0021756319897232586
915, epoch_train_loss=0.0021756319897232586
Epoch 916
Epoch 916 :: Batch 0/1
Batch Loss = 0.002175307502077546
916, epoch_train_loss=0.002175307502077546
Epoch 917
Epoch 917 :: Batch 0/1
Batch Loss = 0.0021749835376451555
917, epoch_train_loss=0.0021749835376451555
Epoch 918
Epoch 918 :: Batch 0/1
Batch Loss = 0.0021746601155436395
918, epoch_train_loss=0.0021746601155436395
Epoch 919
Epoch 919 :: Batch 0/1
Batch Loss = 0.002174337217254037
919, epoch_train_loss=0.002174337217254037
Epoch 920
Epoch 920 :: Batch 0/1
Batch Loss = 0.0021740147914119825
920, epoch_train_loss=0.0021740147914119825
Epoch 921
Epoch 921 :: Batch 0/1
Batch Loss = 0.0021736927994190406
921, epoch_train_loss=0.0021736927994190406
Epoch 922
Epoch 922 :: Batch 0/1
Batch Loss = 0.0021733712299994235
922, epoch_train_loss=0.0021733712299994235
Epoch 923
Epoch 923 :: Batch 0/1
Batch Loss = 0.002173050087120863
923, epoch_train_loss=0.002173050087120863
Epoch 924
Epoch 924 :: Batch 0/1
Batch Loss = 0.0021727293750829955
924, epoch_train_loss=0.0021727293750829955
Epoch 925
Epoch 925 :: Batch 0/1
Batch Loss = 0.002172409078988412
925, epoch_train_loss=0.002172409078988412
Epoch 926
Epoch 926 :: Batch 0/1
Batch Loss = 0.0021720891684246747
926, epoch_train_loss=0.0021720891684246747
Epoch 927
Epoch 927 :: Batch 0/1
Batch Loss = 0.0021717696175063425
927, epoch_train_loss=0.0021717696175063425
Epoch 928
Epoch 928 :: Batch 0/1
Batch Loss = 0.0021714504118565603
928, epoch_train_loss=0.0021714504118565603
Epoch 929
Epoch 929 :: Batch 0/1
Batch Loss = 0.002171131546606149
929, epoch_train_loss=0.002171131546606149
Epoch 930
Epoch 930 :: Batch 0/1
Batch Loss = 0.0021708130195525767
930, epoch_train_loss=0.0021708130195525767
Epoch 931
Epoch 931 :: Batch 0/1
Batch Loss = 0.002170494820937778
931, epoch_train_loss=0.002170494820937778
Epoch 932
Epoch 932 :: Batch 0/1
Batch Loss = 0.0021701769328237954
932, epoch_train_loss=0.0021701769328237954
Epoch 933
Epoch 933 :: Batch 0/1
Batch Loss = 0.002169859336441534
933, epoch_train_loss=0.002169859336441534
Epoch 934
Epoch 934 :: Batch 0/1
Batch Loss = 0.002169542016721017
934, epoch_train_loss=0.002169542016721017
Epoch 935
Epoch 935 :: Batch 0/1
Batch Loss = 0.002169224964039793
935, epoch_train_loss=0.002169224964039793
Epoch 936
Epoch 936 :: Batch 0/1
Batch Loss = 0.0021689081726324923
936, epoch_train_loss=0.0021689081726324923
Epoch 937
Epoch 937 :: Batch 0/1
Batch Loss = 0.0021685916352481064
937, epoch_train_loss=0.0021685916352481064
Epoch 938
Epoch 938 :: Batch 0/1
Batch Loss = 0.0021682753410056372
938, epoch_train_loss=0.0021682753410056372
Epoch 939
Epoch 939 :: Batch 0/1
Batch Loss = 0.0021679592769787946
939, epoch_train_loss=0.0021679592769787946
Epoch 940
Epoch 940 :: Batch 0/1
Batch Loss = 0.0021676434298620187
940, epoch_train_loss=0.0021676434298620187
Epoch 941
Epoch 941 :: Batch 0/1
Batch Loss = 0.0021673277884567458
941, epoch_train_loss=0.0021673277884567458
Epoch 942
Epoch 942 :: Batch 0/1
Batch Loss = 0.00216701234449939
942, epoch_train_loss=0.00216701234449939
Epoch 943
Epoch 943 :: Batch 0/1
Batch Loss = 0.0021666970904717694
943, epoch_train_loss=0.0021666970904717694
Epoch 944
Epoch 944 :: Batch 0/1
Batch Loss = 0.002166382018643228
944, epoch_train_loss=0.002166382018643228
Epoch 945
Epoch 945 :: Batch 0/1
Batch Loss = 0.002166067120201131
945, epoch_train_loss=0.002166067120201131
Epoch 946
Epoch 946 :: Batch 0/1
Batch Loss = 0.002165752385085948
946, epoch_train_loss=0.002165752385085948
Epoch 947
Epoch 947 :: Batch 0/1
Batch Loss = 0.002165437803240875
947, epoch_train_loss=0.002165437803240875
Epoch 948
Epoch 948 :: Batch 0/1
Batch Loss = 0.0021651233655036873
948, epoch_train_loss=0.0021651233655036873
Epoch 949
Epoch 949 :: Batch 0/1
Batch Loss = 0.002164809063683171
949, epoch_train_loss=0.002164809063683171
Epoch 950
Epoch 950 :: Batch 0/1
Batch Loss = 0.0021644948903067345
950, epoch_train_loss=0.0021644948903067345
Epoch 951
Epoch 951 :: Batch 0/1
Batch Loss = 0.0021641808383297246
951, epoch_train_loss=0.0021641808383297246
Epoch 952
Epoch 952 :: Batch 0/1
Batch Loss = 0.0021638669003185514
952, epoch_train_loss=0.0021638669003185514
Epoch 953
Epoch 953 :: Batch 0/1
Batch Loss = 0.002163553068418038
953, epoch_train_loss=0.002163553068418038
Epoch 954
Epoch 954 :: Batch 0/1
Batch Loss = 0.0021632393347289797
954, epoch_train_loss=0.0021632393347289797
Epoch 955
Epoch 955 :: Batch 0/1
Batch Loss = 0.002162925691358588
955, epoch_train_loss=0.002162925691358588
Epoch 956
Epoch 956 :: Batch 0/1
Batch Loss = 0.0021626121308730454
956, epoch_train_loss=0.0021626121308730454
Epoch 957
Epoch 957 :: Batch 0/1
Batch Loss = 0.0021622986463932957
957, epoch_train_loss=0.0021622986463932957
Epoch 958
Epoch 958 :: Batch 0/1
Batch Loss = 0.002161985231268445
958, epoch_train_loss=0.002161985231268445
Epoch 959
Epoch 959 :: Batch 0/1
Batch Loss = 0.002161671879004414
959, epoch_train_loss=0.002161671879004414
Epoch 960
Epoch 960 :: Batch 0/1
Batch Loss = 0.002161358583151213
960, epoch_train_loss=0.002161358583151213
Epoch 961
Epoch 961 :: Batch 0/1
Batch Loss = 0.0021610453372578856
961, epoch_train_loss=0.0021610453372578856
Epoch 962
Epoch 962 :: Batch 0/1
Batch Loss = 0.0021607321347946623
962, epoch_train_loss=0.0021607321347946623
Epoch 963
Epoch 963 :: Batch 0/1
Batch Loss = 0.0021604189693783865
963, epoch_train_loss=0.0021604189693783865
Epoch 964
Epoch 964 :: Batch 0/1
Batch Loss = 0.0021601058347523014
964, epoch_train_loss=0.0021601058347523014
Epoch 965
Epoch 965 :: Batch 0/1
Batch Loss = 0.0021597927248361804
965, epoch_train_loss=0.0021597927248361804
Epoch 966
Epoch 966 :: Batch 0/1
Batch Loss = 0.002159479633774282
966, epoch_train_loss=0.002159479633774282
Epoch 967
Epoch 967 :: Batch 0/1
Batch Loss = 0.0021591665558792056
967, epoch_train_loss=0.0021591665558792056
Epoch 968
Epoch 968 :: Batch 0/1
Batch Loss = 0.0021588534855260925
968, epoch_train_loss=0.0021588534855260925
Epoch 969
Epoch 969 :: Batch 0/1
Batch Loss = 0.002158540417272882
969, epoch_train_loss=0.002158540417272882
Epoch 970
Epoch 970 :: Batch 0/1
Batch Loss = 0.002158227345642649
970, epoch_train_loss=0.002158227345642649
Epoch 971
Epoch 971 :: Batch 0/1
Batch Loss = 0.0021579142652833434
971, epoch_train_loss=0.0021579142652833434
Epoch 972
Epoch 972 :: Batch 0/1
Batch Loss = 0.002157601170896978
972, epoch_train_loss=0.002157601170896978
Epoch 973
Epoch 973 :: Batch 0/1
Batch Loss = 0.0021572880572765024
973, epoch_train_loss=0.0021572880572765024
Epoch 974
Epoch 974 :: Batch 0/1
Batch Loss = 0.0021569749192734797
974, epoch_train_loss=0.0021569749192734797
Epoch 975
Epoch 975 :: Batch 0/1
Batch Loss = 0.0021566617518977717
975, epoch_train_loss=0.0021566617518977717
Epoch 976
Epoch 976 :: Batch 0/1
Batch Loss = 0.002156348550267198
976, epoch_train_loss=0.002156348550267198
Epoch 977
Epoch 977 :: Batch 0/1
Batch Loss = 0.0021560353095547657
977, epoch_train_loss=0.0021560353095547657
Epoch 978
Epoch 978 :: Batch 0/1
Batch Loss = 0.0021557220250622566
978, epoch_train_loss=0.0021557220250622566
Epoch 979
Epoch 979 :: Batch 0/1
Batch Loss = 0.0021554086921544927
979, epoch_train_loss=0.0021554086921544927
Epoch 980
Epoch 980 :: Batch 0/1
Batch Loss = 0.002155095306234184
980, epoch_train_loss=0.002155095306234184
Epoch 981
Epoch 981 :: Batch 0/1
Batch Loss = 0.0021547818628276866
981, epoch_train_loss=0.0021547818628276866
Epoch 982
Epoch 982 :: Batch 0/1
Batch Loss = 0.002154468357523666
982, epoch_train_loss=0.002154468357523666
Epoch 983
Epoch 983 :: Batch 0/1
Batch Loss = 0.0021541547859442495
983, epoch_train_loss=0.0021541547859442495
Epoch 984
Epoch 984 :: Batch 0/1
Batch Loss = 0.0021538411438186463
984, epoch_train_loss=0.0021538411438186463
Epoch 985
Epoch 985 :: Batch 0/1
Batch Loss = 0.0021535274269061525
985, epoch_train_loss=0.0021535274269061525
Epoch 986
Epoch 986 :: Batch 0/1
Batch Loss = 0.0021532136310543136
986, epoch_train_loss=0.0021532136310543136
Epoch 987
Epoch 987 :: Batch 0/1
Batch Loss = 0.0021528997521954103
987, epoch_train_loss=0.0021528997521954103
Epoch 988
Epoch 988 :: Batch 0/1
Batch Loss = 0.0021525857862900156
988, epoch_train_loss=0.0021525857862900156
Epoch 989
Epoch 989 :: Batch 0/1
Batch Loss = 0.002152271729401345
989, epoch_train_loss=0.002152271729401345
Epoch 990
Epoch 990 :: Batch 0/1
Batch Loss = 0.0021519575776041576
990, epoch_train_loss=0.0021519575776041576
Epoch 991
Epoch 991 :: Batch 0/1
Batch Loss = 0.0021516433270608475
991, epoch_train_loss=0.0021516433270608475
Epoch 992
Epoch 992 :: Batch 0/1
Batch Loss = 0.002151328974013057
992, epoch_train_loss=0.002151328974013057
Epoch 993
Epoch 993 :: Batch 0/1
Batch Loss = 0.0021510145147000826
993, epoch_train_loss=0.0021510145147000826
Epoch 994
Epoch 994 :: Batch 0/1
Batch Loss = 0.002150699945473497
994, epoch_train_loss=0.002150699945473497
Epoch 995
Epoch 995 :: Batch 0/1
Batch Loss = 0.0021503852626565617
995, epoch_train_loss=0.0021503852626565617
Epoch 996
Epoch 996 :: Batch 0/1
Batch Loss = 0.002150070462724941
996, epoch_train_loss=0.002150070462724941
Epoch 997
Epoch 997 :: Batch 0/1
Batch Loss = 0.0021497555421240108
997, epoch_train_loss=0.0021497555421240108
Epoch 998
Epoch 998 :: Batch 0/1
Batch Loss = 0.0021494404973711246
998, epoch_train_loss=0.0021494404973711246
Epoch 999
Epoch 999 :: Batch 0/1
Batch Loss = 0.00214912532502328
999, epoch_train_loss=0.00214912532502328
Epoch 1000
Epoch 1000 :: Batch 0/1
Batch Loss = 0.002148810021693091
1000, epoch_train_loss=0.002148810021693091
Epoch 1001
Epoch 1001 :: Batch 0/1
Batch Loss = 0.002148494584053351
1001, epoch_train_loss=0.002148494584053351
Epoch 1002
Epoch 1002 :: Batch 0/1
Batch Loss = 0.0021481790087536576
1002, epoch_train_loss=0.0021481790087536576
Epoch 1003
Epoch 1003 :: Batch 0/1
Batch Loss = 0.002147863292572427
1003, epoch_train_loss=0.002147863292572427
Epoch 1004
Epoch 1004 :: Batch 0/1
Batch Loss = 0.0021475474322357095
1004, epoch_train_loss=0.0021475474322357095
Epoch 1005
Epoch 1005 :: Batch 0/1
Batch Loss = 0.0021472314245997337
1005, epoch_train_loss=0.0021472314245997337
Epoch 1006
Epoch 1006 :: Batch 0/1
Batch Loss = 0.002146915266472749
1006, epoch_train_loss=0.002146915266472749
Epoch 1007
Epoch 1007 :: Batch 0/1
Batch Loss = 0.002146598954783948
1007, epoch_train_loss=0.002146598954783948
Epoch 1008
Epoch 1008 :: Batch 0/1
Batch Loss = 0.0021462824864187913
1008, epoch_train_loss=0.0021462824864187913
Epoch 1009
Epoch 1009 :: Batch 0/1
Batch Loss = 0.0021459658583556167
1009, epoch_train_loss=0.0021459658583556167
Epoch 1010
Epoch 1010 :: Batch 0/1
Batch Loss = 0.0021456490676079855
1010, epoch_train_loss=0.0021456490676079855
Epoch 1011
Epoch 1011 :: Batch 0/1
Batch Loss = 0.0021453321112092946
1011, epoch_train_loss=0.0021453321112092946
Epoch 1012
Epoch 1012 :: Batch 0/1
Batch Loss = 0.002145014986270207
1012, epoch_train_loss=0.002145014986270207
Epoch 1013
Epoch 1013 :: Batch 0/1
Batch Loss = 0.0021446976899213268
1013, epoch_train_loss=0.0021446976899213268
Epoch 1014
Epoch 1014 :: Batch 0/1
Batch Loss = 0.002144380219415579
1014, epoch_train_loss=0.002144380219415579
Epoch 1015
Epoch 1015 :: Batch 0/1
Batch Loss = 0.002144062572025632
1015, epoch_train_loss=0.002144062572025632
Epoch 1016
Epoch 1016 :: Batch 0/1
Batch Loss = 0.0021437447452926084
1016, epoch_train_loss=0.0021437447452926084
Epoch 1017
Epoch 1017 :: Batch 0/1
Batch Loss = 0.002143426736849631
1017, epoch_train_loss=0.002143426736849631
Epoch 1018
Epoch 1018 :: Batch 0/1
Batch Loss = 0.0021431085448810456
1018, epoch_train_loss=0.0021431085448810456
Epoch 1019
Epoch 1019 :: Batch 0/1
Batch Loss = 0.002142790167932859
1019, epoch_train_loss=0.002142790167932859
Epoch 1020
Epoch 1020 :: Batch 0/1
Batch Loss = 0.0021424716058187038
1020, epoch_train_loss=0.0021424716058187038
Epoch 1021
Epoch 1021 :: Batch 0/1
Batch Loss = 0.0021421528598159897
1021, epoch_train_loss=0.0021421528598159897
Epoch 1022
Epoch 1022 :: Batch 0/1
Batch Loss = 0.0021418339344620625
1022, epoch_train_loss=0.0021418339344620625
Epoch 1023
Epoch 1023 :: Batch 0/1
Batch Loss = 0.0021415148391765153
1023, epoch_train_loss=0.0021415148391765153
Epoch 1024
Epoch 1024 :: Batch 0/1
Batch Loss = 0.0021411955930739214
1024, epoch_train_loss=0.0021411955930739214
Epoch 1025
Epoch 1025 :: Batch 0/1
Batch Loss = 0.0021408762311759624
1025, epoch_train_loss=0.0021408762311759624
Epoch 1026
Epoch 1026 :: Batch 0/1
Batch Loss = 0.002140556819059492
1026, epoch_train_loss=0.002140556819059492
Epoch 1027
Epoch 1027 :: Batch 0/1
Batch Loss = 0.002140237475451827
1027, epoch_train_loss=0.002140237475451827
Epoch 1028
Epoch 1028 :: Batch 0/1
Batch Loss = 0.0021399184198262313
1028, epoch_train_loss=0.0021399184198262313
Epoch 1029
Epoch 1029 :: Batch 0/1
Batch Loss = 0.0021396000544833885
1029, epoch_train_loss=0.0021396000544833885
Epoch 1030
Epoch 1030 :: Batch 0/1
Batch Loss = 0.002139283126870697
1030, epoch_train_loss=0.002139283126870697
Epoch 1031
Epoch 1031 :: Batch 0/1
Batch Loss = 0.0021389690332632594
1031, epoch_train_loss=0.0021389690332632594
Epoch 1032
Epoch 1032 :: Batch 0/1
Batch Loss = 0.002138660396680985
1032, epoch_train_loss=0.002138660396680985
Epoch 1033
Epoch 1033 :: Batch 0/1
Batch Loss = 0.0021383622186697646
1033, epoch_train_loss=0.0021383622186697646
Epoch 1034
Epoch 1034 :: Batch 0/1
Batch Loss = 0.002138084007398204
1034, epoch_train_loss=0.002138084007398204
Epoch 1035
Epoch 1035 :: Batch 0/1
Batch Loss = 0.002137844297754333
1035, epoch_train_loss=0.002137844297754333
Epoch 1036
Epoch 1036 :: Batch 0/1
Batch Loss = 0.002137678681555408
1036, epoch_train_loss=0.002137678681555408
Epoch 1037
Epoch 1037 :: Batch 0/1
Batch Loss = 0.002137658293366078
1037, epoch_train_loss=0.002137658293366078
Epoch 1038
Epoch 1038 :: Batch 0/1
Batch Loss = 0.0021379204794558493
1038, epoch_train_loss=0.0021379204794558493
Epoch 1039
Epoch 1039 :: Batch 0/1
Batch Loss = 0.002138748338280583
1039, epoch_train_loss=0.002138748338280583
Epoch 1040
Epoch 1040 :: Batch 0/1
Batch Loss = 0.002140686509783967
1040, epoch_train_loss=0.002140686509783967
Epoch 1041
Epoch 1041 :: Batch 0/1
Batch Loss = 0.002144906245857585
1041, epoch_train_loss=0.002144906245857585
Epoch 1042
Epoch 1042 :: Batch 0/1
Batch Loss = 0.002153613494084828
1042, epoch_train_loss=0.002153613494084828
Epoch 1043
Epoch 1043 :: Batch 0/1
Batch Loss = 0.0021718703042283545
1043, epoch_train_loss=0.0021718703042283545
Epoch 1044
Epoch 1044 :: Batch 0/1
Batch Loss = 0.002208685685350083
1044, epoch_train_loss=0.002208685685350083
Epoch 1045
Epoch 1045 :: Batch 0/1
Batch Loss = 0.002287103190848292
1045, epoch_train_loss=0.002287103190848292
Epoch 1046
Epoch 1046 :: Batch 0/1
Batch Loss = 0.002443046504490824
1046, epoch_train_loss=0.002443046504490824
Epoch 1047
Epoch 1047 :: Batch 0/1
Batch Loss = 0.0027881570608190966
1047, epoch_train_loss=0.0027881570608190966
Epoch 1048
Epoch 1048 :: Batch 0/1
Batch Loss = 0.0034483902480590377
1048, epoch_train_loss=0.0034483902480590377
Epoch 1049
Epoch 1049 :: Batch 0/1
Batch Loss = 0.00499905761753441
1049, epoch_train_loss=0.00499905761753441
Epoch 1050
Epoch 1050 :: Batch 0/1
Batch Loss = 0.007630953788559568
1050, epoch_train_loss=0.007630953788559568
Epoch 1051
Epoch 1051 :: Batch 0/1
Batch Loss = 0.014301743752646973
1051, epoch_train_loss=0.014301743752646973
Epoch 1052
Epoch 1052 :: Batch 0/1
Batch Loss = 0.021824573699819338
1052, epoch_train_loss=0.021824573699819338
Epoch 1053
Epoch 1053 :: Batch 0/1
Batch Loss = 0.04193355352491649
1053, epoch_train_loss=0.04193355352491649
Epoch 1054
Epoch 1054 :: Batch 0/1
Batch Loss = 0.04018673064331579
1054, epoch_train_loss=0.04018673064331579
Epoch 1055
Epoch 1055 :: Batch 0/1
Batch Loss = 0.04612677464738705
1055, epoch_train_loss=0.04612677464738705
Epoch 1056
Epoch 1056 :: Batch 0/1
Batch Loss = 0.015903084659844094
1056, epoch_train_loss=0.015903084659844094
Epoch 1057
Epoch 1057 :: Batch 0/1
Batch Loss = 0.002519858650271485
1057, epoch_train_loss=0.002519858650271485
Epoch 1058
Epoch 1058 :: Batch 0/1
Batch Loss = 0.009432643397206357
1058, epoch_train_loss=0.009432643397206357
Epoch 1059
Epoch 1059 :: Batch 0/1
Batch Loss = 0.019837994188092355
1059, epoch_train_loss=0.019837994188092355
Epoch 1060
Epoch 1060 :: Batch 0/1
Batch Loss = 0.023743820224113112
1060, epoch_train_loss=0.023743820224113112
Epoch 1061
Epoch 1061 :: Batch 0/1
Batch Loss = 0.006789226697133328
1061, epoch_train_loss=0.006789226697133328
Epoch 1062
Epoch 1062 :: Batch 0/1
Batch Loss = 0.004003859217174397
1062, epoch_train_loss=0.004003859217174397
Epoch 1063
Epoch 1063 :: Batch 0/1
Batch Loss = 0.013991333318745875
1063, epoch_train_loss=0.013991333318745875
Epoch 1064
Epoch 1064 :: Batch 0/1
Batch Loss = 0.01121536250222564
1064, epoch_train_loss=0.01121536250222564
Epoch 1065
Epoch 1065 :: Batch 0/1
Batch Loss = 0.0038480565946136095
1065, epoch_train_loss=0.0038480565946136095
Epoch 1066
Epoch 1066 :: Batch 0/1
Batch Loss = 0.003738829918218031
1066, epoch_train_loss=0.003738829918218031
Epoch 1067
Epoch 1067 :: Batch 0/1
Batch Loss = 0.008584062135466957
1067, epoch_train_loss=0.008584062135466957
Epoch 1068
Epoch 1068 :: Batch 0/1
Batch Loss = 0.007854637592531518
1068, epoch_train_loss=0.007854637592531518
Epoch 1069
Epoch 1069 :: Batch 0/1
Batch Loss = 0.0026064755916773096
1069, epoch_train_loss=0.0026064755916773096
Epoch 1070
Epoch 1070 :: Batch 0/1
Batch Loss = 0.005052196204812585
1070, epoch_train_loss=0.005052196204812585
Epoch 1071
Epoch 1071 :: Batch 0/1
Batch Loss = 0.008243637732059245
1071, epoch_train_loss=0.008243637732059245
Epoch 1072
Epoch 1072 :: Batch 0/1
Batch Loss = 0.003826561773859244
1072, epoch_train_loss=0.003826561773859244
Epoch 1073
Epoch 1073 :: Batch 0/1
Batch Loss = 0.0029065906893263518
1073, epoch_train_loss=0.0029065906893263518
Epoch 1074
Epoch 1074 :: Batch 0/1
Batch Loss = 0.006049509974310252
1074, epoch_train_loss=0.006049509974310252
Epoch 1075
Epoch 1075 :: Batch 0/1
Batch Loss = 0.00444466957565251
1075, epoch_train_loss=0.00444466957565251
Epoch 1076
Epoch 1076 :: Batch 0/1
Batch Loss = 0.002410874764602912
1076, epoch_train_loss=0.002410874764602912
Epoch 1077
Epoch 1077 :: Batch 0/1
Batch Loss = 0.0040094948903827655
1077, epoch_train_loss=0.0040094948903827655
Epoch 1078
Epoch 1078 :: Batch 0/1
Batch Loss = 0.004307698806997755
1078, epoch_train_loss=0.004307698806997755
Epoch 1079
Epoch 1079 :: Batch 0/1
Batch Loss = 0.002653915890795209
1079, epoch_train_loss=0.002653915890795209
Epoch 1080
Epoch 1080 :: Batch 0/1
Batch Loss = 0.002806673548628702
1080, epoch_train_loss=0.002806673548628702
Epoch 1081
Epoch 1081 :: Batch 0/1
Batch Loss = 0.0038425464266253887
1081, epoch_train_loss=0.0038425464266253887
Epoch 1082
Epoch 1082 :: Batch 0/1
Batch Loss = 0.003110071349169938
1082, epoch_train_loss=0.003110071349169938
Epoch 1083
Epoch 1083 :: Batch 0/1
Batch Loss = 0.0023602834492566097
1083, epoch_train_loss=0.0023602834492566097
Epoch 1084
Epoch 1084 :: Batch 0/1
Batch Loss = 0.0032019935235898652
1084, epoch_train_loss=0.0032019935235898652
Epoch 1085
Epoch 1085 :: Batch 0/1
Batch Loss = 0.003306924335201781
1085, epoch_train_loss=0.003306924335201781
Epoch 1086
Epoch 1086 :: Batch 0/1
Batch Loss = 0.0023839559834792513
1086, epoch_train_loss=0.0023839559834792513
Epoch 1087
Epoch 1087 :: Batch 0/1
Batch Loss = 0.00265780046357178
1087, epoch_train_loss=0.00265780046357178
Epoch 1088
Epoch 1088 :: Batch 0/1
Batch Loss = 0.003177521655010163
1088, epoch_train_loss=0.003177521655010163
Epoch 1089
Epoch 1089 :: Batch 0/1
Batch Loss = 0.0025663425166873983
1089, epoch_train_loss=0.0025663425166873983
Epoch 1090
Epoch 1090 :: Batch 0/1
Batch Loss = 0.002325460296173729
1090, epoch_train_loss=0.002325460296173729
Epoch 1091
Epoch 1091 :: Batch 0/1
Batch Loss = 0.002803100268808618
1091, epoch_train_loss=0.002803100268808618
Epoch 1092
Epoch 1092 :: Batch 0/1
Batch Loss = 0.0026834082746754894
1092, epoch_train_loss=0.0026834082746754894
Epoch 1093
Epoch 1093 :: Batch 0/1
Batch Loss = 0.0022700215784567926
1093, epoch_train_loss=0.0022700215784567926
Epoch 1094
Epoch 1094 :: Batch 0/1
Batch Loss = 0.002429925392781301
1094, epoch_train_loss=0.002429925392781301
Epoch 1095
Epoch 1095 :: Batch 0/1
Batch Loss = 0.0026347849392882013
1095, epoch_train_loss=0.0026347849392882013
Epoch 1096
Epoch 1096 :: Batch 0/1
Batch Loss = 0.002384232083829501
1096, epoch_train_loss=0.002384232083829501
Epoch 1097
Epoch 1097 :: Batch 0/1
Batch Loss = 0.002238352136850619
1097, epoch_train_loss=0.002238352136850619
Epoch 1098
Epoch 1098 :: Batch 0/1
Batch Loss = 0.0024463732022902834
1098, epoch_train_loss=0.0024463732022902834
Epoch 1099
Epoch 1099 :: Batch 0/1
Batch Loss = 0.002463495469464106
1099, epoch_train_loss=0.002463495469464106
Epoch 1100
Epoch 1100 :: Batch 0/1
Batch Loss = 0.0022426172591664397
1100, epoch_train_loss=0.0022426172591664397
Epoch 1101
Epoch 1101 :: Batch 0/1
Batch Loss = 0.0022609272869217696
1101, epoch_train_loss=0.0022609272869217696
Epoch 1102
Epoch 1102 :: Batch 0/1
Batch Loss = 0.0024024583631923227
1102, epoch_train_loss=0.0024024583631923227
Epoch 1103
Epoch 1103 :: Batch 0/1
Batch Loss = 0.002316235443203872
1103, epoch_train_loss=0.002316235443203872
Epoch 1104
Epoch 1104 :: Batch 0/1
Batch Loss = 0.0021948545430304856
1104, epoch_train_loss=0.0021948545430304856
Epoch 1105
Epoch 1105 :: Batch 0/1
Batch Loss = 0.002260435571913651
1105, epoch_train_loss=0.002260435571913651
Epoch 1106
Epoch 1106 :: Batch 0/1
Batch Loss = 0.002320400586746288
1106, epoch_train_loss=0.002320400586746288
Epoch 1107
Epoch 1107 :: Batch 0/1
Batch Loss = 0.0022368635556778253
1107, epoch_train_loss=0.0022368635556778253
Epoch 1108
Epoch 1108 :: Batch 0/1
Batch Loss = 0.0021806867263581367
1108, epoch_train_loss=0.0021806867263581367
Epoch 1109
Epoch 1109 :: Batch 0/1
Batch Loss = 0.0022410739357999047
1109, epoch_train_loss=0.0022410739357999047
Epoch 1110
Epoch 1110 :: Batch 0/1
Batch Loss = 0.002264508719514469
1110, epoch_train_loss=0.002264508719514469
Epoch 1111
Epoch 1111 :: Batch 0/1
Batch Loss = 0.002197010299542486
1111, epoch_train_loss=0.002197010299542486
Epoch 1112
Epoch 1112 :: Batch 0/1
Batch Loss = 0.0021727285959480917
1112, epoch_train_loss=0.0021727285959480917
Epoch 1113
Epoch 1113 :: Batch 0/1
Batch Loss = 0.0022164100347625616
1113, epoch_train_loss=0.0022164100347625616
Epoch 1114
Epoch 1114 :: Batch 0/1
Batch Loss = 0.0022204216728243104
1114, epoch_train_loss=0.0022204216728243104
Epoch 1115
Epoch 1115 :: Batch 0/1
Batch Loss = 0.002175471756932758
1115, epoch_train_loss=0.002175471756932758
Epoch 1116
Epoch 1116 :: Batch 0/1
Batch Loss = 0.002163934057044396
1116, epoch_train_loss=0.002163934057044396
Epoch 1117
Epoch 1117 :: Batch 0/1
Batch Loss = 0.002192267636738587
1117, epoch_train_loss=0.002192267636738587
Epoch 1118
Epoch 1118 :: Batch 0/1
Batch Loss = 0.0021936693268252536
1118, epoch_train_loss=0.0021936693268252536
Epoch 1119
Epoch 1119 :: Batch 0/1
Batch Loss = 0.00216278595372064
1119, epoch_train_loss=0.00216278595372064
Epoch 1120
Epoch 1120 :: Batch 0/1
Batch Loss = 0.0021549961035457563
1120, epoch_train_loss=0.0021549961035457563
Epoch 1121
Epoch 1121 :: Batch 0/1
Batch Loss = 0.002173514435902278
1121, epoch_train_loss=0.002173514435902278
Epoch 1122
Epoch 1122 :: Batch 0/1
Batch Loss = 0.0021744036326535577
1122, epoch_train_loss=0.0021744036326535577
Epoch 1123
Epoch 1123 :: Batch 0/1
Batch Loss = 0.0021542330237184866
1123, epoch_train_loss=0.0021542330237184866
Epoch 1124
Epoch 1124 :: Batch 0/1
Batch Loss = 0.0021470482207353557
1124, epoch_train_loss=0.0021470482207353557
Epoch 1125
Epoch 1125 :: Batch 0/1
Batch Loss = 0.0021584855954803993
1125, epoch_train_loss=0.0021584855954803993
Epoch 1126
Epoch 1126 :: Batch 0/1
Batch Loss = 0.0021611709772293816
1126, epoch_train_loss=0.0021611709772293816
Epoch 1127
Epoch 1127 :: Batch 0/1
Batch Loss = 0.002147840644239683
1127, epoch_train_loss=0.002147840644239683
Epoch 1128
Epoch 1128 :: Batch 0/1
Batch Loss = 0.0021407485702899225
1128, epoch_train_loss=0.0021407485702899225
Epoch 1129
Epoch 1129 :: Batch 0/1
Batch Loss = 0.0021473264456517197
1129, epoch_train_loss=0.0021473264456517197
Epoch 1130
Epoch 1130 :: Batch 0/1
Batch Loss = 0.0021505018618377327
1130, epoch_train_loss=0.0021505018618377327
Epoch 1131
Epoch 1131 :: Batch 0/1
Batch Loss = 0.0021426201733903676
1131, epoch_train_loss=0.0021426201733903676
Epoch 1132
Epoch 1132 :: Batch 0/1
Batch Loss = 0.002135973928504098
1132, epoch_train_loss=0.002135973928504098
Epoch 1133
Epoch 1133 :: Batch 0/1
Batch Loss = 0.002138610003528821
1133, epoch_train_loss=0.002138610003528821
Epoch 1134
Epoch 1134 :: Batch 0/1
Batch Loss = 0.002141878318945687
1134, epoch_train_loss=0.002141878318945687
Epoch 1135
Epoch 1135 :: Batch 0/1
Batch Loss = 0.002137936866657986
1135, epoch_train_loss=0.002137936866657986
Epoch 1136
Epoch 1136 :: Batch 0/1
Batch Loss = 0.002132296997557887
1136, epoch_train_loss=0.002132296997557887
Epoch 1137
Epoch 1137 :: Batch 0/1
Batch Loss = 0.0021321079205993956
1137, epoch_train_loss=0.0021321079205993956
Epoch 1138
Epoch 1138 :: Batch 0/1
Batch Loss = 0.0021346152045517695
1138, epoch_train_loss=0.0021346152045517695
Epoch 1139
Epoch 1139 :: Batch 0/1
Batch Loss = 0.0021334506197240844
1139, epoch_train_loss=0.0021334506197240844
Epoch 1140
Epoch 1140 :: Batch 0/1
Batch Loss = 0.002129251039652598
1140, epoch_train_loss=0.002129251039652598
Epoch 1141
Epoch 1141 :: Batch 0/1
Batch Loss = 0.0021273960444550153
1141, epoch_train_loss=0.0021273960444550153
Epoch 1142
Epoch 1142 :: Batch 0/1
Batch Loss = 0.0021286571597657214
1142, epoch_train_loss=0.0021286571597657214
Epoch 1143
Epoch 1143 :: Batch 0/1
Batch Loss = 0.0021288770734220475
1143, epoch_train_loss=0.0021288770734220475
Epoch 1144
Epoch 1144 :: Batch 0/1
Batch Loss = 0.002126369825228328
1144, epoch_train_loss=0.002126369825228328
Epoch 1145
Epoch 1145 :: Batch 0/1
Batch Loss = 0.0021239636061379666
1145, epoch_train_loss=0.0021239636061379666
Epoch 1146
Epoch 1146 :: Batch 0/1
Batch Loss = 0.0021238474540472138
1146, epoch_train_loss=0.0021238474540472138
Epoch 1147
Epoch 1147 :: Batch 0/1
Batch Loss = 0.002124365267959816
1147, epoch_train_loss=0.002124365267959816
Epoch 1148
Epoch 1148 :: Batch 0/1
Batch Loss = 0.0021232840717360845
1148, epoch_train_loss=0.0021232840717360845
Epoch 1149
Epoch 1149 :: Batch 0/1
Batch Loss = 0.002121235805829297
1149, epoch_train_loss=0.002121235805829297
Epoch 1150
Epoch 1150 :: Batch 0/1
Batch Loss = 0.002120141927127775
1150, epoch_train_loss=0.002120141927127775
Epoch 1151
Epoch 1151 :: Batch 0/1
Batch Loss = 0.002120201284062706
1151, epoch_train_loss=0.002120201284062706
Epoch 1152
Epoch 1152 :: Batch 0/1
Batch Loss = 0.0021199537376216456
1152, epoch_train_loss=0.0021199537376216456
Epoch 1153
Epoch 1153 :: Batch 0/1
Batch Loss = 0.0021186725726127517
1153, epoch_train_loss=0.0021186725726127517
Epoch 1154
Epoch 1154 :: Batch 0/1
Batch Loss = 0.0021172664199334166
1154, epoch_train_loss=0.0021172664199334166
Epoch 1155
Epoch 1155 :: Batch 0/1
Batch Loss = 0.0021166796581629237
1155, epoch_train_loss=0.0021166796581629237
Epoch 1156
Epoch 1156 :: Batch 0/1
Batch Loss = 0.002116537850991462
1156, epoch_train_loss=0.002116537850991462
Epoch 1157
Epoch 1157 :: Batch 0/1
Batch Loss = 0.002115927292697054
1157, epoch_train_loss=0.002115927292697054
Epoch 1158
Epoch 1158 :: Batch 0/1
Batch Loss = 0.0021147875004369305
1158, epoch_train_loss=0.0021147875004369305
Epoch 1159
Epoch 1159 :: Batch 0/1
Batch Loss = 0.0021138180638020526
1159, epoch_train_loss=0.0021138180638020526
Epoch 1160
Epoch 1160 :: Batch 0/1
Batch Loss = 0.0021133508947920082
1160, epoch_train_loss=0.0021133508947920082
Epoch 1161
Epoch 1161 :: Batch 0/1
Batch Loss = 0.002112992119326081
1161, epoch_train_loss=0.002112992119326081
Epoch 1162
Epoch 1162 :: Batch 0/1
Batch Loss = 0.0021123008455036533
1162, epoch_train_loss=0.0021123008455036533
Epoch 1163
Epoch 1163 :: Batch 0/1
Batch Loss = 0.00211136567363979
1163, epoch_train_loss=0.00211136567363979
Epoch 1164
Epoch 1164 :: Batch 0/1
Batch Loss = 0.0021105952812717063
1164, epoch_train_loss=0.0021105952812717063
Epoch 1165
Epoch 1165 :: Batch 0/1
Batch Loss = 0.0021101181241878135
1165, epoch_train_loss=0.0021101181241878135
Epoch 1166
Epoch 1166 :: Batch 0/1
Batch Loss = 0.0021096603848228973
1166, epoch_train_loss=0.0021096603848228973
Epoch 1167
Epoch 1167 :: Batch 0/1
Batch Loss = 0.002108984596235557
1167, epoch_train_loss=0.002108984596235557
Epoch 1168
Epoch 1168 :: Batch 0/1
Batch Loss = 0.002108192590231608
1168, epoch_train_loss=0.002108192590231608
Epoch 1169
Epoch 1169 :: Batch 0/1
Batch Loss = 0.002107521031739103
1169, epoch_train_loss=0.002107521031739103
Epoch 1170
Epoch 1170 :: Batch 0/1
Batch Loss = 0.0021070113600874804
1170, epoch_train_loss=0.0021070113600874804
Epoch 1171
Epoch 1171 :: Batch 0/1
Batch Loss = 0.0021065052975143307
1171, epoch_train_loss=0.0021065052975143307
Epoch 1172
Epoch 1172 :: Batch 0/1
Batch Loss = 0.0021058775637509907
1172, epoch_train_loss=0.0021058775637509907
Epoch 1173
Epoch 1173 :: Batch 0/1
Batch Loss = 0.0021051799996717807
1173, epoch_train_loss=0.0021051799996717807
Epoch 1174
Epoch 1174 :: Batch 0/1
Batch Loss = 0.0021045476456798165
1174, epoch_train_loss=0.0021045476456798165
Epoch 1175
Epoch 1175 :: Batch 0/1
Batch Loss = 0.0021040165627107263
1175, epoch_train_loss=0.0021040165627107263
Epoch 1176
Epoch 1176 :: Batch 0/1
Batch Loss = 0.0021034963100455923
1176, epoch_train_loss=0.0021034963100455923
Epoch 1177
Epoch 1177 :: Batch 0/1
Batch Loss = 0.002102907068419959
1177, epoch_train_loss=0.002102907068419959
Epoch 1178
Epoch 1178 :: Batch 0/1
Batch Loss = 0.00210227225218402
1178, epoch_train_loss=0.00210227225218402
Epoch 1179
Epoch 1179 :: Batch 0/1
Batch Loss = 0.0021016668762119345
1179, epoch_train_loss=0.0021016668762119345
Epoch 1180
Epoch 1180 :: Batch 0/1
Batch Loss = 0.0021011208083264715
1180, epoch_train_loss=0.0021011208083264715
Epoch 1181
Epoch 1181 :: Batch 0/1
Batch Loss = 0.002100594933999234
1181, epoch_train_loss=0.002100594933999234
Epoch 1182
Epoch 1182 :: Batch 0/1
Batch Loss = 0.002100037835079683
1182, epoch_train_loss=0.002100037835079683
Epoch 1183
Epoch 1183 :: Batch 0/1
Batch Loss = 0.0020994462144858727
1183, epoch_train_loss=0.0020994462144858727
Epoch 1184
Epoch 1184 :: Batch 0/1
Batch Loss = 0.002098860858642937
1184, epoch_train_loss=0.002098860858642937
Epoch 1185
Epoch 1185 :: Batch 0/1
Batch Loss = 0.002098310172003301
1185, epoch_train_loss=0.002098310172003301
Epoch 1186
Epoch 1186 :: Batch 0/1
Batch Loss = 0.002097780995525628
1186, epoch_train_loss=0.002097780995525628
Epoch 1187
Epoch 1187 :: Batch 0/1
Batch Loss = 0.0020972429152271094
1187, epoch_train_loss=0.0020972429152271094
Epoch 1188
Epoch 1188 :: Batch 0/1
Batch Loss = 0.0020966839797069976
1188, epoch_train_loss=0.0020966839797069976
Epoch 1189
Epoch 1189 :: Batch 0/1
Batch Loss = 0.0020961189207128286
1189, epoch_train_loss=0.0020961189207128286
Epoch 1190
Epoch 1190 :: Batch 0/1
Batch Loss = 0.002095568832825534
1190, epoch_train_loss=0.002095568832825534
Epoch 1191
Epoch 1191 :: Batch 0/1
Batch Loss = 0.0020950376105896788
1191, epoch_train_loss=0.0020950376105896788
Epoch 1192
Epoch 1192 :: Batch 0/1
Batch Loss = 0.0020945104122196056
1192, epoch_train_loss=0.0020945104122196056
Epoch 1193
Epoch 1193 :: Batch 0/1
Batch Loss = 0.00209397350870404
1193, epoch_train_loss=0.00209397350870404
Epoch 1194
Epoch 1194 :: Batch 0/1
Batch Loss = 0.002093428230365468
1194, epoch_train_loss=0.002093428230365468
Epoch 1195
Epoch 1195 :: Batch 0/1
Batch Loss = 0.002092885722795149
1195, epoch_train_loss=0.002092885722795149
Epoch 1196
Epoch 1196 :: Batch 0/1
Batch Loss = 0.0020923540074660927
1196, epoch_train_loss=0.0020923540074660927
Epoch 1197
Epoch 1197 :: Batch 0/1
Batch Loss = 0.0020918309701652643
1197, epoch_train_loss=0.0020918309701652643
Epoch 1198
Epoch 1198 :: Batch 0/1
Batch Loss = 0.0020913078977863194
1198, epoch_train_loss=0.0020913078977863194
Epoch 1199
Epoch 1199 :: Batch 0/1
Batch Loss = 0.0020907793957954802
1199, epoch_train_loss=0.0020907793957954802
Epoch 1200
Epoch 1200 :: Batch 0/1
Batch Loss = 0.0020902482278432268
1200, epoch_train_loss=0.0020902482278432268
Epoch 1201
Epoch 1201 :: Batch 0/1
Batch Loss = 0.002089720627849452
1201, epoch_train_loss=0.002089720627849452
Epoch 1202
Epoch 1202 :: Batch 0/1
Batch Loss = 0.00208919953357855
1202, epoch_train_loss=0.00208919953357855
Epoch 1203
Epoch 1203 :: Batch 0/1
Batch Loss = 0.0020886828073266687
1203, epoch_train_loss=0.0020886828073266687
Epoch 1204
Epoch 1204 :: Batch 0/1
Batch Loss = 0.0020881660813493533
1204, epoch_train_loss=0.0020881660813493533
Epoch 1205
Epoch 1205 :: Batch 0/1
Batch Loss = 0.002087647006774197
1205, epoch_train_loss=0.002087647006774197
Epoch 1206
Epoch 1206 :: Batch 0/1
Batch Loss = 0.00208712722741715
1206, epoch_train_loss=0.00208712722741715
Epoch 1207
Epoch 1207 :: Batch 0/1
Batch Loss = 0.0020866099278116844
1207, epoch_train_loss=0.0020866099278116844
Epoch 1208
Epoch 1208 :: Batch 0/1
Batch Loss = 0.002086096507964308
1208, epoch_train_loss=0.002086096507964308
Epoch 1209
Epoch 1209 :: Batch 0/1
Batch Loss = 0.0020855858642767305
1209, epoch_train_loss=0.0020855858642767305
Epoch 1210
Epoch 1210 :: Batch 0/1
Batch Loss = 0.002085075874841368
1210, epoch_train_loss=0.002085075874841368
Epoch 1211
Epoch 1211 :: Batch 0/1
Batch Loss = 0.002084565239291583
1211, epoch_train_loss=0.002084565239291583
Epoch 1212
Epoch 1212 :: Batch 0/1
Batch Loss = 0.002084054472075927
1212, epoch_train_loss=0.002084054472075927
Epoch 1213
Epoch 1213 :: Batch 0/1
Batch Loss = 0.002083545136464352
1213, epoch_train_loss=0.002083545136464352
Epoch 1214
Epoch 1214 :: Batch 0/1
Batch Loss = 0.0020830381765814467
1214, epoch_train_loss=0.0020830381765814467
Epoch 1215
Epoch 1215 :: Batch 0/1
Batch Loss = 0.002082533309671363
1215, epoch_train_loss=0.002082533309671363
Epoch 1216
Epoch 1216 :: Batch 0/1
Batch Loss = 0.002082029549197373
1216, epoch_train_loss=0.002082029549197373
Epoch 1217
Epoch 1217 :: Batch 0/1
Batch Loss = 0.0020815260387503478
1217, epoch_train_loss=0.0020815260387503478
Epoch 1218
Epoch 1218 :: Batch 0/1
Batch Loss = 0.0020810226883484183
1218, epoch_train_loss=0.0020810226883484183
Epoch 1219
Epoch 1219 :: Batch 0/1
Batch Loss = 0.002080520132149408
1219, epoch_train_loss=0.002080520132149408
Epoch 1220
Epoch 1220 :: Batch 0/1
Batch Loss = 0.002080019011769027
1220, epoch_train_loss=0.002080019011769027
Epoch 1221
Epoch 1221 :: Batch 0/1
Batch Loss = 0.0020795194639064785
1221, epoch_train_loss=0.0020795194639064785
Epoch 1222
Epoch 1222 :: Batch 0/1
Batch Loss = 0.0020790211610781217
1222, epoch_train_loss=0.0020790211610781217
Epoch 1223
Epoch 1223 :: Batch 0/1
Batch Loss = 0.002078523612316329
1223, epoch_train_loss=0.002078523612316329
Epoch 1224
Epoch 1224 :: Batch 0/1
Batch Loss = 0.002078026524777653
1224, epoch_train_loss=0.002078026524777653
Epoch 1225
Epoch 1225 :: Batch 0/1
Batch Loss = 0.002077529993048491
1225, epoch_train_loss=0.002077529993048491
Epoch 1226
Epoch 1226 :: Batch 0/1
Batch Loss = 0.0020770343216728036
1226, epoch_train_loss=0.0020770343216728036
Epoch 1227
Epoch 1227 :: Batch 0/1
Batch Loss = 0.002076539741482117
1227, epoch_train_loss=0.002076539741482117
Epoch 1228
Epoch 1228 :: Batch 0/1
Batch Loss = 0.002076046282151391
1228, epoch_train_loss=0.002076046282151391
Epoch 1229
Epoch 1229 :: Batch 0/1
Batch Loss = 0.002075553775549256
1229, epoch_train_loss=0.002075553775549256
Epoch 1230
Epoch 1230 :: Batch 0/1
Batch Loss = 0.0020750619891748705
1230, epoch_train_loss=0.0020750619891748705
Epoch 1231
Epoch 1231 :: Batch 0/1
Batch Loss = 0.002074570799506504
1231, epoch_train_loss=0.002074570799506504
Epoch 1232
Epoch 1232 :: Batch 0/1
Batch Loss = 0.0020740802396942717
1232, epoch_train_loss=0.0020740802396942717
Epoch 1233
Epoch 1233 :: Batch 0/1
Batch Loss = 0.0020735904229800517
1233, epoch_train_loss=0.0020735904229800517
Epoch 1234
Epoch 1234 :: Batch 0/1
Batch Loss = 0.002073101458399133
1234, epoch_train_loss=0.002073101458399133
Epoch 1235
Epoch 1235 :: Batch 0/1
Batch Loss = 0.0020726133765277734
1235, epoch_train_loss=0.0020726133765277734
Epoch 1236
Epoch 1236 :: Batch 0/1
Batch Loss = 0.0020721261124929235
1236, epoch_train_loss=0.0020721261124929235
Epoch 1237
Epoch 1237 :: Batch 0/1
Batch Loss = 0.002071639567894488
1237, epoch_train_loss=0.002071639567894488
Epoch 1238
Epoch 1238 :: Batch 0/1
Batch Loss = 0.002071153671401844
1238, epoch_train_loss=0.002071153671401844
Epoch 1239
Epoch 1239 :: Batch 0/1
Batch Loss = 0.0020706683993833882
1239, epoch_train_loss=0.0020706683993833882
Epoch 1240
Epoch 1240 :: Batch 0/1
Batch Loss = 0.002070183779765209
1240, epoch_train_loss=0.002070183779765209
Epoch 1241
Epoch 1241 :: Batch 0/1
Batch Loss = 0.0020696998635831958
1241, epoch_train_loss=0.0020696998635831958
Epoch 1242
Epoch 1242 :: Batch 0/1
Batch Loss = 0.0020692166789858827
1242, epoch_train_loss=0.0020692166789858827
Epoch 1243
Epoch 1243 :: Batch 0/1
Batch Loss = 0.002068734220157997
1243, epoch_train_loss=0.002068734220157997
Epoch 1244
Epoch 1244 :: Batch 0/1
Batch Loss = 0.002068252457725784
1244, epoch_train_loss=0.002068252457725784
Epoch 1245
Epoch 1245 :: Batch 0/1
Batch Loss = 0.0020677713500106675
1245, epoch_train_loss=0.0020677713500106675
Epoch 1246
Epoch 1246 :: Batch 0/1
Batch Loss = 0.0020672908645166257
1246, epoch_train_loss=0.0020672908645166257
Epoch 1247
Epoch 1247 :: Batch 0/1
Batch Loss = 0.0020668109932058558
1247, epoch_train_loss=0.0020668109932058558
Epoch 1248
Epoch 1248 :: Batch 0/1
Batch Loss = 0.00206633174239867
1248, epoch_train_loss=0.00206633174239867
Epoch 1249
Epoch 1249 :: Batch 0/1
Batch Loss = 0.002065853124942077
1249, epoch_train_loss=0.002065853124942077
Epoch 1250
Epoch 1250 :: Batch 0/1
Batch Loss = 0.002065375152672351
1250, epoch_train_loss=0.002065375152672351
Epoch 1251
Epoch 1251 :: Batch 0/1
Batch Loss = 0.0020648978259577796
1251, epoch_train_loss=0.0020648978259577796
Epoch 1252
Epoch 1252 :: Batch 0/1
Batch Loss = 0.002064421133676572
1252, epoch_train_loss=0.002064421133676572
Epoch 1253
Epoch 1253 :: Batch 0/1
Batch Loss = 0.0020639450609343936
1253, epoch_train_loss=0.0020639450609343936
Epoch 1254
Epoch 1254 :: Batch 0/1
Batch Loss = 0.0020634695928654413
1254, epoch_train_loss=0.0020634695928654413
Epoch 1255
Epoch 1255 :: Batch 0/1
Batch Loss = 0.0020629947185287563
1255, epoch_train_loss=0.0020629947185287563
Epoch 1256
Epoch 1256 :: Batch 0/1
Batch Loss = 0.0020625204346455565
1256, epoch_train_loss=0.0020625204346455565
Epoch 1257
Epoch 1257 :: Batch 0/1
Batch Loss = 0.0020620467417966286
1257, epoch_train_loss=0.0020620467417966286
Epoch 1258
Epoch 1258 :: Batch 0/1
Batch Loss = 0.0020615736425836835
1258, epoch_train_loss=0.0020615736425836835
Epoch 1259
Epoch 1259 :: Batch 0/1
Batch Loss = 0.002061101139333287
1259, epoch_train_loss=0.002061101139333287
Epoch 1260
Epoch 1260 :: Batch 0/1
Batch Loss = 0.002060629231687431
1260, epoch_train_loss=0.002060629231687431
Epoch 1261
Epoch 1261 :: Batch 0/1
Batch Loss = 0.0020601579160088113
1261, epoch_train_loss=0.0020601579160088113
Epoch 1262
Epoch 1262 :: Batch 0/1
Batch Loss = 0.002059687187349535
1262, epoch_train_loss=0.002059687187349535
Epoch 1263
Epoch 1263 :: Batch 0/1
Batch Loss = 0.002059217040229573
1263, epoch_train_loss=0.002059217040229573
Epoch 1264
Epoch 1264 :: Batch 0/1
Batch Loss = 0.0020587474694776535
1264, epoch_train_loss=0.0020587474694776535
Epoch 1265
Epoch 1265 :: Batch 0/1
Batch Loss = 0.0020582784719933587
1265, epoch_train_loss=0.0020582784719933587
Epoch 1266
Epoch 1266 :: Batch 0/1
Batch Loss = 0.002057810046098319
1266, epoch_train_loss=0.002057810046098319
Epoch 1267
Epoch 1267 :: Batch 0/1
Batch Loss = 0.002057342191277683
1267, epoch_train_loss=0.002057342191277683
Epoch 1268
Epoch 1268 :: Batch 0/1
Batch Loss = 0.002056874907627796
1268, epoch_train_loss=0.002056874907627796
Epoch 1269
Epoch 1269 :: Batch 0/1
Batch Loss = 0.0020564081956669336
1269, epoch_train_loss=0.0020564081956669336
Epoch 1270
Epoch 1270 :: Batch 0/1
Batch Loss = 0.0020559420550569827
1270, epoch_train_loss=0.0020559420550569827
Epoch 1271
Epoch 1271 :: Batch 0/1
Batch Loss = 0.0020554764854209106
1271, epoch_train_loss=0.0020554764854209106
Epoch 1272
Epoch 1272 :: Batch 0/1
Batch Loss = 0.0020550114860765066
1272, epoch_train_loss=0.0020550114860765066
Epoch 1273
Epoch 1273 :: Batch 0/1
Batch Loss = 0.002054547055943369
1273, epoch_train_loss=0.002054547055943369
Epoch 1274
Epoch 1274 :: Batch 0/1
Batch Loss = 0.002054083194044535
1274, epoch_train_loss=0.002054083194044535
Epoch 1275
Epoch 1275 :: Batch 0/1
Batch Loss = 0.002053619899647443
1275, epoch_train_loss=0.002053619899647443
Epoch 1276
Epoch 1276 :: Batch 0/1
Batch Loss = 0.002053157172195468
1276, epoch_train_loss=0.002053157172195468
Epoch 1277
Epoch 1277 :: Batch 0/1
Batch Loss = 0.0020526950115329
1277, epoch_train_loss=0.0020526950115329
Epoch 1278
Epoch 1278 :: Batch 0/1
Batch Loss = 0.002052233418092984
1278, epoch_train_loss=0.002052233418092984
Epoch 1279
Epoch 1279 :: Batch 0/1
Batch Loss = 0.0020517723922135376
1279, epoch_train_loss=0.0020517723922135376
Epoch 1280
Epoch 1280 :: Batch 0/1
Batch Loss = 0.002051311934764952
1280, epoch_train_loss=0.002051311934764952
Epoch 1281
Epoch 1281 :: Batch 0/1
Batch Loss = 0.0020508520466770716
1281, epoch_train_loss=0.0020508520466770716
Epoch 1282
Epoch 1282 :: Batch 0/1
Batch Loss = 0.002050392728900035
1282, epoch_train_loss=0.002050392728900035
Epoch 1283
Epoch 1283 :: Batch 0/1
Batch Loss = 0.002049933982569661
1283, epoch_train_loss=0.002049933982569661
Epoch 1284
Epoch 1284 :: Batch 0/1
Batch Loss = 0.002049475808882521
1284, epoch_train_loss=0.002049475808882521
Epoch 1285
Epoch 1285 :: Batch 0/1
Batch Loss = 0.002049018209043874
1285, epoch_train_loss=0.002049018209043874
Epoch 1286
Epoch 1286 :: Batch 0/1
Batch Loss = 0.002048561184384945
1286, epoch_train_loss=0.002048561184384945
Epoch 1287
Epoch 1287 :: Batch 0/1
Batch Loss = 0.0020481047362850015
1287, epoch_train_loss=0.0020481047362850015
Epoch 1288
Epoch 1288 :: Batch 0/1
Batch Loss = 0.0020476488662311125
1288, epoch_train_loss=0.0020476488662311125
Epoch 1289
Epoch 1289 :: Batch 0/1
Batch Loss = 0.0020471935758811766
1289, epoch_train_loss=0.0020471935758811766
Epoch 1290
Epoch 1290 :: Batch 0/1
Batch Loss = 0.002046738866921219
1290, epoch_train_loss=0.002046738866921219
Epoch 1291
Epoch 1291 :: Batch 0/1
Batch Loss = 0.0020462847412093346
1291, epoch_train_loss=0.0020462847412093346
Epoch 1292
Epoch 1292 :: Batch 0/1
Batch Loss = 0.0020458312006757483
1292, epoch_train_loss=0.0020458312006757483
Epoch 1293
Epoch 1293 :: Batch 0/1
Batch Loss = 0.0020453782473772446
1293, epoch_train_loss=0.0020453782473772446
Epoch 1294
Epoch 1294 :: Batch 0/1
Batch Loss = 0.0020449258834884017
1294, epoch_train_loss=0.0020449258834884017
Epoch 1295
Epoch 1295 :: Batch 0/1
Batch Loss = 0.002044474111239752
1295, epoch_train_loss=0.002044474111239752
Epoch 1296
Epoch 1296 :: Batch 0/1
Batch Loss = 0.002044022932972083
1296, epoch_train_loss=0.002044022932972083
Epoch 1297
Epoch 1297 :: Batch 0/1
Batch Loss = 0.00204357235116049
1297, epoch_train_loss=0.00204357235116049
Epoch 1298
Epoch 1298 :: Batch 0/1
Batch Loss = 0.002043122368296215
1298, epoch_train_loss=0.002043122368296215
Epoch 1299
Epoch 1299 :: Batch 0/1
Batch Loss = 0.002042672987078718
1299, epoch_train_loss=0.002042672987078718
Epoch 1300
Epoch 1300 :: Batch 0/1
Batch Loss = 0.002042224210107661
1300, epoch_train_loss=0.002042224210107661
Epoch 1301
Epoch 1301 :: Batch 0/1
Batch Loss = 0.00204177604022873
1301, epoch_train_loss=0.00204177604022873
Epoch 1302
Epoch 1302 :: Batch 0/1
Batch Loss = 0.002041328480312761
1302, epoch_train_loss=0.002041328480312761
Epoch 1303
Epoch 1303 :: Batch 0/1
Batch Loss = 0.002040881533399025
1303, epoch_train_loss=0.002040881533399025
Epoch 1304
Epoch 1304 :: Batch 0/1
Batch Loss = 0.0020404352024553482
1304, epoch_train_loss=0.0020404352024553482
Epoch 1305
Epoch 1305 :: Batch 0/1
Batch Loss = 0.002039989490762818
1305, epoch_train_loss=0.002039989490762818
Epoch 1306
Epoch 1306 :: Batch 0/1
Batch Loss = 0.002039544401595759
1306, epoch_train_loss=0.002039544401595759
Epoch 1307
Epoch 1307 :: Batch 0/1
Batch Loss = 0.002039099938514856
1307, epoch_train_loss=0.002039099938514856
Epoch 1308
Epoch 1308 :: Batch 0/1
Batch Loss = 0.00203865610521866
1308, epoch_train_loss=0.00203865610521866
Epoch 1309
Epoch 1309 :: Batch 0/1
Batch Loss = 0.0020382129057724485
1309, epoch_train_loss=0.0020382129057724485
Epoch 1310
Epoch 1310 :: Batch 0/1
Batch Loss = 0.002037770344698624
1310, epoch_train_loss=0.002037770344698624
Epoch 1311
Epoch 1311 :: Batch 0/1
Batch Loss = 0.0020373284271653267
1311, epoch_train_loss=0.0020373284271653267
Epoch 1312
Epoch 1312 :: Batch 0/1
Batch Loss = 0.0020368871592677824
1312, epoch_train_loss=0.0020368871592677824
Epoch 1313
Epoch 1313 :: Batch 0/1
Batch Loss = 0.00203644654889215
1313, epoch_train_loss=0.00203644654889215
Epoch 1314
Epoch 1314 :: Batch 0/1
Batch Loss = 0.002036006606014403
1314, epoch_train_loss=0.002036006606014403
Epoch 1315
Epoch 1315 :: Batch 0/1
Batch Loss = 0.002035567344696846
1315, epoch_train_loss=0.002035567344696846
Epoch 1316
Epoch 1316 :: Batch 0/1
Batch Loss = 0.002035128784980087
1316, epoch_train_loss=0.002035128784980087
Epoch 1317
Epoch 1317 :: Batch 0/1
Batch Loss = 0.002034690956838624
1317, epoch_train_loss=0.002034690956838624
Epoch 1318
Epoch 1318 :: Batch 0/1
Batch Loss = 0.002034253906219584
1318, epoch_train_loss=0.002034253906219584
Epoch 1319
Epoch 1319 :: Batch 0/1
Batch Loss = 0.0020338177057633196
1319, epoch_train_loss=0.0020338177057633196
Epoch 1320
Epoch 1320 :: Batch 0/1
Batch Loss = 0.0020333824718693255
1320, epoch_train_loss=0.0020333824718693255
Epoch 1321
Epoch 1321 :: Batch 0/1
Batch Loss = 0.00203294839422566
1321, epoch_train_loss=0.00203294839422566
Epoch 1322
Epoch 1322 :: Batch 0/1
Batch Loss = 0.0020325157873412763
1322, epoch_train_loss=0.0020325157873412763
Epoch 1323
Epoch 1323 :: Batch 0/1
Batch Loss = 0.002032085174143032
1323, epoch_train_loss=0.002032085174143032
Epoch 1324
Epoch 1324 :: Batch 0/1
Batch Loss = 0.002031657443555482
1324, epoch_train_loss=0.002031657443555482
Epoch 1325
Epoch 1325 :: Batch 0/1
Batch Loss = 0.002031234097345664
1325, epoch_train_loss=0.002031234097345664
Epoch 1326
Epoch 1326 :: Batch 0/1
Batch Loss = 0.002030817745548902
1326, epoch_train_loss=0.002030817745548902
Epoch 1327
Epoch 1327 :: Batch 0/1
Batch Loss = 0.002030412853621007
1327, epoch_train_loss=0.002030412853621007
Epoch 1328
Epoch 1328 :: Batch 0/1
Batch Loss = 0.0020300273581322645
1328, epoch_train_loss=0.0020300273581322645
Epoch 1329
Epoch 1329 :: Batch 0/1
Batch Loss = 0.0020296749627132
1329, epoch_train_loss=0.0020296749627132
Epoch 1330
Epoch 1330 :: Batch 0/1
Batch Loss = 0.0020293806359080735
1330, epoch_train_loss=0.0020293806359080735
Epoch 1331
Epoch 1331 :: Batch 0/1
Batch Loss = 0.002029187678870386
1331, epoch_train_loss=0.002029187678870386
Epoch 1332
Epoch 1332 :: Batch 0/1
Batch Loss = 0.0020291773797373736
1332, epoch_train_loss=0.0020291773797373736
Epoch 1333
Epoch 1333 :: Batch 0/1
Batch Loss = 0.0020294902409497472
1333, epoch_train_loss=0.0020294902409497472
Epoch 1334
Epoch 1334 :: Batch 0/1
Batch Loss = 0.0020304004991066487
1334, epoch_train_loss=0.0020304004991066487
Epoch 1335
Epoch 1335 :: Batch 0/1
Batch Loss = 0.002032374395223442
1335, epoch_train_loss=0.002032374395223442
Epoch 1336
Epoch 1336 :: Batch 0/1
Batch Loss = 0.0020363733701363696
1336, epoch_train_loss=0.0020363733701363696
Epoch 1337
Epoch 1337 :: Batch 0/1
Batch Loss = 0.0020439681819880997
1337, epoch_train_loss=0.0020439681819880997
Epoch 1338
Epoch 1338 :: Batch 0/1
Batch Loss = 0.0020586792584978563
1338, epoch_train_loss=0.0020586792584978563
Epoch 1339
Epoch 1339 :: Batch 0/1
Batch Loss = 0.002085800431766735
1339, epoch_train_loss=0.002085800431766735
Epoch 1340
Epoch 1340 :: Batch 0/1
Batch Loss = 0.002138898509844436
1340, epoch_train_loss=0.002138898509844436
Epoch 1341
Epoch 1341 :: Batch 0/1
Batch Loss = 0.0022352120399731926
1341, epoch_train_loss=0.0022352120399731926
Epoch 1342
Epoch 1342 :: Batch 0/1
Batch Loss = 0.002430274772641784
1342, epoch_train_loss=0.002430274772641784
Epoch 1343
Epoch 1343 :: Batch 0/1
Batch Loss = 0.0027728624940199153
1343, epoch_train_loss=0.0027728624940199153
Epoch 1344
Epoch 1344 :: Batch 0/1
Batch Loss = 0.003505219734898366
1344, epoch_train_loss=0.003505219734898366
Epoch 1345
Epoch 1345 :: Batch 0/1
Batch Loss = 0.00468862073366473
1345, epoch_train_loss=0.00468862073366473
Epoch 1346
Epoch 1346 :: Batch 0/1
Batch Loss = 0.007419197050615724
1346, epoch_train_loss=0.007419197050615724
Epoch 1347
Epoch 1347 :: Batch 0/1
Batch Loss = 0.010929358567869975
1347, epoch_train_loss=0.010929358567869975
Epoch 1348
Epoch 1348 :: Batch 0/1
Batch Loss = 0.01980005833069474
1348, epoch_train_loss=0.01980005833069474
Epoch 1349
Epoch 1349 :: Batch 0/1
Batch Loss = 0.024867166048472916
1349, epoch_train_loss=0.024867166048472916
Epoch 1350
Epoch 1350 :: Batch 0/1
Batch Loss = 0.039918933423699995
1350, epoch_train_loss=0.039918933423699995
Epoch 1351
Epoch 1351 :: Batch 0/1
Batch Loss = 0.029163241783095813
1351, epoch_train_loss=0.029163241783095813
Epoch 1352
Epoch 1352 :: Batch 0/1
Batch Loss = 0.022657105445077694
1352, epoch_train_loss=0.022657105445077694
Epoch 1353
Epoch 1353 :: Batch 0/1
Batch Loss = 0.006508876234956117
1353, epoch_train_loss=0.006508876234956117
Epoch 1354
Epoch 1354 :: Batch 0/1
Batch Loss = 0.002572207024577513
1354, epoch_train_loss=0.002572207024577513
Epoch 1355
Epoch 1355 :: Batch 0/1
Batch Loss = 0.009652199061716011
1355, epoch_train_loss=0.009652199061716011
Epoch 1356
Epoch 1356 :: Batch 0/1
Batch Loss = 0.015259753624766661
1356, epoch_train_loss=0.015259753624766661
Epoch 1357
Epoch 1357 :: Batch 0/1
Batch Loss = 0.01696005971138415
1357, epoch_train_loss=0.01696005971138415
Epoch 1358
Epoch 1358 :: Batch 0/1
Batch Loss = 0.006606198613729897
1358, epoch_train_loss=0.006606198613729897
Epoch 1359
Epoch 1359 :: Batch 0/1
Batch Loss = 0.0023202645798056096
1359, epoch_train_loss=0.0023202645798056096
Epoch 1360
Epoch 1360 :: Batch 0/1
Batch Loss = 0.00654368479120551
1360, epoch_train_loss=0.00654368479120551
Epoch 1361
Epoch 1361 :: Batch 0/1
Batch Loss = 0.009822615140445782
1361, epoch_train_loss=0.009822615140445782
Epoch 1362
Epoch 1362 :: Batch 0/1
Batch Loss = 0.008359409590139357
1362, epoch_train_loss=0.008359409590139357
Epoch 1363
Epoch 1363 :: Batch 0/1
Batch Loss = 0.002893673991908959
1363, epoch_train_loss=0.002893673991908959
Epoch 1364
Epoch 1364 :: Batch 0/1
Batch Loss = 0.003143509753123645
1364, epoch_train_loss=0.003143509753123645
Epoch 1365
Epoch 1365 :: Batch 0/1
Batch Loss = 0.0069756184681421494
1365, epoch_train_loss=0.0069756184681421494
Epoch 1366
Epoch 1366 :: Batch 0/1
Batch Loss = 0.006286651886872792
1366, epoch_train_loss=0.006286651886872792
Epoch 1367
Epoch 1367 :: Batch 0/1
Batch Loss = 0.003366870255662386
1367, epoch_train_loss=0.003366870255662386
Epoch 1368
Epoch 1368 :: Batch 0/1
Batch Loss = 0.0023181807299998045
1368, epoch_train_loss=0.0023181807299998045
Epoch 1369
Epoch 1369 :: Batch 0/1
Batch Loss = 0.004299808529153861
1369, epoch_train_loss=0.004299808529153861
Epoch 1370
Epoch 1370 :: Batch 0/1
Batch Loss = 0.005375855051330397
1370, epoch_train_loss=0.005375855051330397
Epoch 1371
Epoch 1371 :: Batch 0/1
Batch Loss = 0.003202587071346962
1371, epoch_train_loss=0.003202587071346962
Epoch 1372
Epoch 1372 :: Batch 0/1
Batch Loss = 0.0022147083138483134
1372, epoch_train_loss=0.0022147083138483134
Epoch 1373
Epoch 1373 :: Batch 0/1
Batch Loss = 0.003573582927801305
1373, epoch_train_loss=0.003573582927801305
Epoch 1374
Epoch 1374 :: Batch 0/1
Batch Loss = 0.0040443768913268514
1374, epoch_train_loss=0.0040443768913268514
Epoch 1375
Epoch 1375 :: Batch 0/1
Batch Loss = 0.002927891173548454
1375, epoch_train_loss=0.002927891173548454
Epoch 1376
Epoch 1376 :: Batch 0/1
Batch Loss = 0.0021533788047064526
1376, epoch_train_loss=0.0021533788047064526
Epoch 1377
Epoch 1377 :: Batch 0/1
Batch Loss = 0.0029371350051074826
1377, epoch_train_loss=0.0029371350051074826
Epoch 1378
Epoch 1378 :: Batch 0/1
Batch Loss = 0.003497892349656823
1378, epoch_train_loss=0.003497892349656823
Epoch 1379
Epoch 1379 :: Batch 0/1
Batch Loss = 0.002650846617012545
1379, epoch_train_loss=0.002650846617012545
Epoch 1380
Epoch 1380 :: Batch 0/1
Batch Loss = 0.002149744273349456
1380, epoch_train_loss=0.002149744273349456
Epoch 1381
Epoch 1381 :: Batch 0/1
Batch Loss = 0.0026769045292431965
1381, epoch_train_loss=0.0026769045292431965
Epoch 1382
Epoch 1382 :: Batch 0/1
Batch Loss = 0.002958553253614855
1382, epoch_train_loss=0.002958553253614855
Epoch 1383
Epoch 1383 :: Batch 0/1
Batch Loss = 0.0024989881114779993
1383, epoch_train_loss=0.0024989881114779993
Epoch 1384
Epoch 1384 :: Batch 0/1
Batch Loss = 0.0021208579701449824
1384, epoch_train_loss=0.0021208579701449824
Epoch 1385
Epoch 1385 :: Batch 0/1
Batch Loss = 0.0024177815349068076
1385, epoch_train_loss=0.0024177815349068076
Epoch 1386
Epoch 1386 :: Batch 0/1
Batch Loss = 0.0026984982138992613
1386, epoch_train_loss=0.0026984982138992613
Epoch 1387
Epoch 1387 :: Batch 0/1
Batch Loss = 0.0023865938681298642
1387, epoch_train_loss=0.0023865938681298642
Epoch 1388
Epoch 1388 :: Batch 0/1
Batch Loss = 0.0021030815130081825
1388, epoch_train_loss=0.0021030815130081825
Epoch 1389
Epoch 1389 :: Batch 0/1
Batch Loss = 0.0022513152026413523
1389, epoch_train_loss=0.0022513152026413523
Epoch 1390
Epoch 1390 :: Batch 0/1
Batch Loss = 0.002448703529785067
1390, epoch_train_loss=0.002448703529785067
Epoch 1391
Epoch 1391 :: Batch 0/1
Batch Loss = 0.0023298483725538812
1391, epoch_train_loss=0.0023298483725538812
Epoch 1392
Epoch 1392 :: Batch 0/1
Batch Loss = 0.0021005309145845576
1392, epoch_train_loss=0.0021005309145845576
Epoch 1393
Epoch 1393 :: Batch 0/1
Batch Loss = 0.0021317926301311256
1393, epoch_train_loss=0.0021317926301311256
Epoch 1394
Epoch 1394 :: Batch 0/1
Batch Loss = 0.0023001703546842323
1394, epoch_train_loss=0.0023001703546842323
Epoch 1395
Epoch 1395 :: Batch 0/1
Batch Loss = 0.002270199069843084
1395, epoch_train_loss=0.002270199069843084
Epoch 1396
Epoch 1396 :: Batch 0/1
Batch Loss = 0.002107692806318767
1396, epoch_train_loss=0.002107692806318767
Epoch 1397
Epoch 1397 :: Batch 0/1
Batch Loss = 0.002064927963273498
1397, epoch_train_loss=0.002064927963273498
Epoch 1398
Epoch 1398 :: Batch 0/1
Batch Loss = 0.002166320532065716
1398, epoch_train_loss=0.002166320532065716
Epoch 1399
Epoch 1399 :: Batch 0/1
Batch Loss = 0.002208585761983342
1399, epoch_train_loss=0.002208585761983342
Epoch 1400
Epoch 1400 :: Batch 0/1
Batch Loss = 0.002116671629228988
1400, epoch_train_loss=0.002116671629228988
Epoch 1401
Epoch 1401 :: Batch 0/1
Batch Loss = 0.002045603032462519
1401, epoch_train_loss=0.002045603032462519
Epoch 1402
Epoch 1402 :: Batch 0/1
Batch Loss = 0.002080481574890656
1402, epoch_train_loss=0.002080481574890656
Epoch 1403
Epoch 1403 :: Batch 0/1
Batch Loss = 0.0021362168403113698
1403, epoch_train_loss=0.0021362168403113698
Epoch 1404
Epoch 1404 :: Batch 0/1
Batch Loss = 0.002117246266956354
1404, epoch_train_loss=0.002117246266956354
Epoch 1405
Epoch 1405 :: Batch 0/1
Batch Loss = 0.0020517278145898293
1405, epoch_train_loss=0.0020517278145898293
Epoch 1406
Epoch 1406 :: Batch 0/1
Batch Loss = 0.0020349769804965945
1406, epoch_train_loss=0.0020349769804965945
Epoch 1407
Epoch 1407 :: Batch 0/1
Batch Loss = 0.002074209598180442
1407, epoch_train_loss=0.002074209598180442
Epoch 1408
Epoch 1408 :: Batch 0/1
Batch Loss = 0.0020923839535504215
1408, epoch_train_loss=0.0020923839535504215
Epoch 1409
Epoch 1409 :: Batch 0/1
Batch Loss = 0.002061751889225044
1409, epoch_train_loss=0.002061751889225044
Epoch 1410
Epoch 1410 :: Batch 0/1
Batch Loss = 0.002026720766592674
1410, epoch_train_loss=0.002026720766592674
Epoch 1411
Epoch 1411 :: Batch 0/1
Batch Loss = 0.002031094385953095
1411, epoch_train_loss=0.002031094385953095
Epoch 1412
Epoch 1412 :: Batch 0/1
Batch Loss = 0.0020551209617216334
1412, epoch_train_loss=0.0020551209617216334
Epoch 1413
Epoch 1413 :: Batch 0/1
Batch Loss = 0.002057714939907716
1413, epoch_train_loss=0.002057714939907716
Epoch 1414
Epoch 1414 :: Batch 0/1
Batch Loss = 0.002035431868184426
1414, epoch_train_loss=0.002035431868184426
Epoch 1415
Epoch 1415 :: Batch 0/1
Batch Loss = 0.002017189567690205
1415, epoch_train_loss=0.002017189567690205
Epoch 1416
Epoch 1416 :: Batch 0/1
Batch Loss = 0.002022756541325096
1416, epoch_train_loss=0.002022756541325096
Epoch 1417
Epoch 1417 :: Batch 0/1
Batch Loss = 0.0020369396969133523
1417, epoch_train_loss=0.0020369396969133523
Epoch 1418
Epoch 1418 :: Batch 0/1
Batch Loss = 0.002035939208373482
1418, epoch_train_loss=0.002035939208373482
Epoch 1419
Epoch 1419 :: Batch 0/1
Batch Loss = 0.0020212513640148126
1419, epoch_train_loss=0.0020212513640148126
Epoch 1420
Epoch 1420 :: Batch 0/1
Batch Loss = 0.002011243107263474
1420, epoch_train_loss=0.002011243107263474
Epoch 1421
Epoch 1421 :: Batch 0/1
Batch Loss = 0.0020145712800801172
1421, epoch_train_loss=0.0020145712800801172
Epoch 1422
Epoch 1422 :: Batch 0/1
Batch Loss = 0.0020224323854200373
1422, epoch_train_loss=0.0020224323854200373
Epoch 1423
Epoch 1423 :: Batch 0/1
Batch Loss = 0.0020221368166754654
1423, epoch_train_loss=0.0020221368166754654
Epoch 1424
Epoch 1424 :: Batch 0/1
Batch Loss = 0.0020136641624518077
1424, epoch_train_loss=0.0020136641624518077
Epoch 1425
Epoch 1425 :: Batch 0/1
Batch Loss = 0.002006552624143805
1425, epoch_train_loss=0.002006552624143805
Epoch 1426
Epoch 1426 :: Batch 0/1
Batch Loss = 0.0020075753811963147
1426, epoch_train_loss=0.0020075753811963147
Epoch 1427
Epoch 1427 :: Batch 0/1
Batch Loss = 0.0020121994618487967
1427, epoch_train_loss=0.0020121994618487967
Epoch 1428
Epoch 1428 :: Batch 0/1
Batch Loss = 0.002012733458082733
1428, epoch_train_loss=0.002012733458082733
Epoch 1429
Epoch 1429 :: Batch 0/1
Batch Loss = 0.002008076048884725
1429, epoch_train_loss=0.002008076048884725
Epoch 1430
Epoch 1430 :: Batch 0/1
Batch Loss = 0.002003073866743089
1430, epoch_train_loss=0.002003073866743089
Epoch 1431
Epoch 1431 :: Batch 0/1
Batch Loss = 0.002002095881306429
1431, epoch_train_loss=0.002002095881306429
Epoch 1432
Epoch 1432 :: Batch 0/1
Batch Loss = 0.00200437261701248
1432, epoch_train_loss=0.00200437261701248
Epoch 1433
Epoch 1433 :: Batch 0/1
Batch Loss = 0.0020056276589182753
1433, epoch_train_loss=0.0020056276589182753
Epoch 1434
Epoch 1434 :: Batch 0/1
Batch Loss = 0.002003547480266812
1434, epoch_train_loss=0.002003547480266812
Epoch 1435
Epoch 1435 :: Batch 0/1
Batch Loss = 0.0020000646146622588
1435, epoch_train_loss=0.0020000646146622588
Epoch 1436
Epoch 1436 :: Batch 0/1
Batch Loss = 0.001998252974351365
1436, epoch_train_loss=0.001998252974351365
Epoch 1437
Epoch 1437 :: Batch 0/1
Batch Loss = 0.001998770871050895
1437, epoch_train_loss=0.001998770871050895
Epoch 1438
Epoch 1438 :: Batch 0/1
Batch Loss = 0.001999758790299081
1438, epoch_train_loss=0.001999758790299081
Epoch 1439
Epoch 1439 :: Batch 0/1
Batch Loss = 0.0019993422626037247
1439, epoch_train_loss=0.0019993422626037247
Epoch 1440
Epoch 1440 :: Batch 0/1
Batch Loss = 0.0019973444477771754
1440, epoch_train_loss=0.0019973444477771754
Epoch 1441
Epoch 1441 :: Batch 0/1
Batch Loss = 0.001995395557641433
1441, epoch_train_loss=0.001995395557641433
Epoch 1442
Epoch 1442 :: Batch 0/1
Batch Loss = 0.0019947279618109707
1442, epoch_train_loss=0.0019947279618109707
Epoch 1443
Epoch 1443 :: Batch 0/1
Batch Loss = 0.0019950505484077454
1443, epoch_train_loss=0.0019950505484077454
Epoch 1444
Epoch 1444 :: Batch 0/1
Batch Loss = 0.0019951848932072648
1444, epoch_train_loss=0.0019951848932072648
Epoch 1445
Epoch 1445 :: Batch 0/1
Batch Loss = 0.0019944212228718526
1445, epoch_train_loss=0.0019944212228718526
Epoch 1446
Epoch 1446 :: Batch 0/1
Batch Loss = 0.001993041355027201
1446, epoch_train_loss=0.001993041355027201
Epoch 1447
Epoch 1447 :: Batch 0/1
Batch Loss = 0.001991883196519765
1447, epoch_train_loss=0.001991883196519765
Epoch 1448
Epoch 1448 :: Batch 0/1
Batch Loss = 0.001991447078997128
1448, epoch_train_loss=0.001991447078997128
Epoch 1449
Epoch 1449 :: Batch 0/1
Batch Loss = 0.001991441569556444
1449, epoch_train_loss=0.001991441569556444
Epoch 1450
Epoch 1450 :: Batch 0/1
Batch Loss = 0.0019912521674290183
1450, epoch_train_loss=0.0019912521674290183
Epoch 1451
Epoch 1451 :: Batch 0/1
Batch Loss = 0.001990575619538114
1451, epoch_train_loss=0.001990575619538114
Epoch 1452
Epoch 1452 :: Batch 0/1
Batch Loss = 0.0019896135595332152
1452, epoch_train_loss=0.0019896135595332152
Epoch 1453
Epoch 1453 :: Batch 0/1
Batch Loss = 0.0019887939486790984
1453, epoch_train_loss=0.0019887939486790984
Epoch 1454
Epoch 1454 :: Batch 0/1
Batch Loss = 0.0019883569372278324
1454, epoch_train_loss=0.0019883569372278324
Epoch 1455
Epoch 1455 :: Batch 0/1
Batch Loss = 0.001988145480972209
1455, epoch_train_loss=0.001988145480972209
Epoch 1456
Epoch 1456 :: Batch 0/1
Batch Loss = 0.001987859014504186
1456, epoch_train_loss=0.001987859014504186
Epoch 1457
Epoch 1457 :: Batch 0/1
Batch Loss = 0.0019873250567572056
1457, epoch_train_loss=0.0019873250567572056
Epoch 1458
Epoch 1458 :: Batch 0/1
Batch Loss = 0.0019866273788891418
1458, epoch_train_loss=0.0019866273788891418
Epoch 1459
Epoch 1459 :: Batch 0/1
Batch Loss = 0.0019859704242451036
1459, epoch_train_loss=0.0019859704242451036
Epoch 1460
Epoch 1460 :: Batch 0/1
Batch Loss = 0.0019854989071803678
1460, epoch_train_loss=0.0019854989071803678
Epoch 1461
Epoch 1461 :: Batch 0/1
Batch Loss = 0.0019851722712660707
1461, epoch_train_loss=0.0019851722712660707
Epoch 1462
Epoch 1462 :: Batch 0/1
Batch Loss = 0.0019848517798717867
1462, epoch_train_loss=0.0019848517798717867
Epoch 1463
Epoch 1463 :: Batch 0/1
Batch Loss = 0.00198442855023996
1463, epoch_train_loss=0.00198442855023996
Epoch 1464
Epoch 1464 :: Batch 0/1
Batch Loss = 0.0019838952102148154
1464, epoch_train_loss=0.0019838952102148154
Epoch 1465
Epoch 1465 :: Batch 0/1
Batch Loss = 0.001983341350531572
1465, epoch_train_loss=0.001983341350531572
Epoch 1466
Epoch 1466 :: Batch 0/1
Batch Loss = 0.0019828610898299185
1466, epoch_train_loss=0.0019828610898299185
Epoch 1467
Epoch 1467 :: Batch 0/1
Batch Loss = 0.0019824735292402677
1467, epoch_train_loss=0.0019824735292402677
Epoch 1468
Epoch 1468 :: Batch 0/1
Batch Loss = 0.0019821266445900487
1468, epoch_train_loss=0.0019821266445900487
Epoch 1469
Epoch 1469 :: Batch 0/1
Batch Loss = 0.001981757804501691
1469, epoch_train_loss=0.001981757804501691
Epoch 1470
Epoch 1470 :: Batch 0/1
Batch Loss = 0.001981331560427665
1470, epoch_train_loss=0.001981331560427665
Epoch 1471
Epoch 1471 :: Batch 0/1
Batch Loss = 0.001980866393832048
1471, epoch_train_loss=0.001980866393832048
Epoch 1472
Epoch 1472 :: Batch 0/1
Batch Loss = 0.001980409672543139
1472, epoch_train_loss=0.001980409672543139
Epoch 1473
Epoch 1473 :: Batch 0/1
Batch Loss = 0.0019799948789904448
1473, epoch_train_loss=0.0019799948789904448
Epoch 1474
Epoch 1474 :: Batch 0/1
Batch Loss = 0.001979620480226146
1474, epoch_train_loss=0.001979620480226146
Epoch 1475
Epoch 1475 :: Batch 0/1
Batch Loss = 0.0019792640868673984
1475, epoch_train_loss=0.0019792640868673984
Epoch 1476
Epoch 1476 :: Batch 0/1
Batch Loss = 0.00197889527680797
1476, epoch_train_loss=0.00197889527680797
Epoch 1477
Epoch 1477 :: Batch 0/1
Batch Loss = 0.0019785006554669336
1477, epoch_train_loss=0.0019785006554669336
Epoch 1478
Epoch 1478 :: Batch 0/1
Batch Loss = 0.001978090251680665
1478, epoch_train_loss=0.001978090251680665
Epoch 1479
Epoch 1479 :: Batch 0/1
Batch Loss = 0.00197768443731503
1479, epoch_train_loss=0.00197768443731503
Epoch 1480
Epoch 1480 :: Batch 0/1
Batch Loss = 0.0019772974914770507
1480, epoch_train_loss=0.0019772974914770507
Epoch 1481
Epoch 1481 :: Batch 0/1
Batch Loss = 0.0019769317729443022
1481, epoch_train_loss=0.0019769317729443022
Epoch 1482
Epoch 1482 :: Batch 0/1
Batch Loss = 0.001976577646302012
1482, epoch_train_loss=0.001976577646302012
Epoch 1483
Epoch 1483 :: Batch 0/1
Batch Loss = 0.0019762216791231924
1483, epoch_train_loss=0.0019762216791231924
Epoch 1484
Epoch 1484 :: Batch 0/1
Batch Loss = 0.001975856956240758
1484, epoch_train_loss=0.001975856956240758
Epoch 1485
Epoch 1485 :: Batch 0/1
Batch Loss = 0.001975484004185293
1485, epoch_train_loss=0.001975484004185293
Epoch 1486
Epoch 1486 :: Batch 0/1
Batch Loss = 0.001975110127181213
1486, epoch_train_loss=0.001975110127181213
Epoch 1487
Epoch 1487 :: Batch 0/1
Batch Loss = 0.001974743256161772
1487, epoch_train_loss=0.001974743256161772
Epoch 1488
Epoch 1488 :: Batch 0/1
Batch Loss = 0.001974387105622182
1488, epoch_train_loss=0.001974387105622182
Epoch 1489
Epoch 1489 :: Batch 0/1
Batch Loss = 0.0019740396180915514
1489, epoch_train_loss=0.0019740396180915514
Epoch 1490
Epoch 1490 :: Batch 0/1
Batch Loss = 0.001973696428685885
1490, epoch_train_loss=0.001973696428685885
Epoch 1491
Epoch 1491 :: Batch 0/1
Batch Loss = 0.0019733524460421874
1491, epoch_train_loss=0.0019733524460421874
Epoch 1492
Epoch 1492 :: Batch 0/1
Batch Loss = 0.0019730054592296937
1492, epoch_train_loss=0.0019730054592296937
Epoch 1493
Epoch 1493 :: Batch 0/1
Batch Loss = 0.0019726566621473384
1493, epoch_train_loss=0.0019726566621473384
Epoch 1494
Epoch 1494 :: Batch 0/1
Batch Loss = 0.00197230881116532
1494, epoch_train_loss=0.00197230881116532
Epoch 1495
Epoch 1495 :: Batch 0/1
Batch Loss = 0.0019719645853078296
1495, epoch_train_loss=0.0019719645853078296
Epoch 1496
Epoch 1496 :: Batch 0/1
Batch Loss = 0.0019716257002222472
1496, epoch_train_loss=0.0019716257002222472
Epoch 1497
Epoch 1497 :: Batch 0/1
Batch Loss = 0.001971291661102662
1497, epoch_train_loss=0.001971291661102662
Epoch 1498
Epoch 1498 :: Batch 0/1
Batch Loss = 0.001970960575253981
1498, epoch_train_loss=0.001970960575253981
Epoch 1499
Epoch 1499 :: Batch 0/1
Batch Loss = 0.0019706307316259123
1499, epoch_train_loss=0.0019706307316259123
Epoch 1500
Epoch 1500 :: Batch 0/1
Batch Loss = 0.0019703009647770754
1500, epoch_train_loss=0.0019703009647770754
Epoch 1501
Epoch 1501 :: Batch 0/1
Batch Loss = 0.001969971073842756
1501, epoch_train_loss=0.001969971073842756
Epoch 1502
Epoch 1502 :: Batch 0/1
Batch Loss = 0.0019696417968610843
1502, epoch_train_loss=0.0019696417968610843
Epoch 1503
Epoch 1503 :: Batch 0/1
Batch Loss = 0.0019693142034224705
1503, epoch_train_loss=0.0019693142034224705
Epoch 1504
Epoch 1504 :: Batch 0/1
Batch Loss = 0.0019689890773219696
1504, epoch_train_loss=0.0019689890773219696
Epoch 1505
Epoch 1505 :: Batch 0/1
Batch Loss = 0.0019686668097797016
1505, epoch_train_loss=0.0019686668097797016
Epoch 1506
Epoch 1506 :: Batch 0/1
Batch Loss = 0.0019683470759433563
1506, epoch_train_loss=0.0019683470759433563
Epoch 1507
Epoch 1507 :: Batch 0/1
Batch Loss = 0.00196802931650805
1507, epoch_train_loss=0.00196802931650805
Epoch 1508
Epoch 1508 :: Batch 0/1
Batch Loss = 0.001967712946365072
1508, epoch_train_loss=0.001967712946365072
Epoch 1509
Epoch 1509 :: Batch 0/1
Batch Loss = 0.0019673975212670786
1509, epoch_train_loss=0.0019673975212670786
Epoch 1510
Epoch 1510 :: Batch 0/1
Batch Loss = 0.0019670828429827437
1510, epoch_train_loss=0.0019670828429827437
Epoch 1511
Epoch 1511 :: Batch 0/1
Batch Loss = 0.001966769102570485
1511, epoch_train_loss=0.001966769102570485
Epoch 1512
Epoch 1512 :: Batch 0/1
Batch Loss = 0.001966456533229117
1512, epoch_train_loss=0.001966456533229117
Epoch 1513
Epoch 1513 :: Batch 0/1
Batch Loss = 0.0019661453997647928
1513, epoch_train_loss=0.0019661453997647928
Epoch 1514
Epoch 1514 :: Batch 0/1
Batch Loss = 0.0019658359102576674
1514, epoch_train_loss=0.0019658359102576674
Epoch 1515
Epoch 1515 :: Batch 0/1
Batch Loss = 0.0019655280921806696
1515, epoch_train_loss=0.0019655280921806696
Epoch 1516
Epoch 1516 :: Batch 0/1
Batch Loss = 0.0019652218482709934
1516, epoch_train_loss=0.0019652218482709934
Epoch 1517
Epoch 1517 :: Batch 0/1
Batch Loss = 0.0019649170323052583
1517, epoch_train_loss=0.0019649170323052583
Epoch 1518
Epoch 1518 :: Batch 0/1
Batch Loss = 0.0019646134609274655
1518, epoch_train_loss=0.0019646134609274655
Epoch 1519
Epoch 1519 :: Batch 0/1
Batch Loss = 0.001964310968768182
1519, epoch_train_loss=0.001964310968768182
Epoch 1520
Epoch 1520 :: Batch 0/1
Batch Loss = 0.0019640094910409354
1520, epoch_train_loss=0.0019640094910409354
Epoch 1521
Epoch 1521 :: Batch 0/1
Batch Loss = 0.0019637089828090392
1521, epoch_train_loss=0.0019637089828090392
Epoch 1522
Epoch 1522 :: Batch 0/1
Batch Loss = 0.001963409475811928
1522, epoch_train_loss=0.001963409475811928
Epoch 1523
Epoch 1523 :: Batch 0/1
Batch Loss = 0.001963111012664506
1523, epoch_train_loss=0.001963111012664506
Epoch 1524
Epoch 1524 :: Batch 0/1
Batch Loss = 0.0019628136370530022
1524, epoch_train_loss=0.0019628136370530022
Epoch 1525
Epoch 1525 :: Batch 0/1
Batch Loss = 0.001962517375170744
1525, epoch_train_loss=0.001962517375170744
Epoch 1526
Epoch 1526 :: Batch 0/1
Batch Loss = 0.001962222250255906
1526, epoch_train_loss=0.001962222250255906
Epoch 1527
Epoch 1527 :: Batch 0/1
Batch Loss = 0.001961928233442565
1527, epoch_train_loss=0.001961928233442565
Epoch 1528
Epoch 1528 :: Batch 0/1
Batch Loss = 0.001961635294573483
1528, epoch_train_loss=0.001961635294573483
Epoch 1529
Epoch 1529 :: Batch 0/1
Batch Loss = 0.0019613433975770975
1529, epoch_train_loss=0.0019613433975770975
Epoch 1530
Epoch 1530 :: Batch 0/1
Batch Loss = 0.001961052494940119
1530, epoch_train_loss=0.001961052494940119
Epoch 1531
Epoch 1531 :: Batch 0/1
Batch Loss = 0.0019607625448478253
1531, epoch_train_loss=0.0019607625448478253
Epoch 1532
Epoch 1532 :: Batch 0/1
Batch Loss = 0.001960473514510039
1532, epoch_train_loss=0.001960473514510039
Epoch 1533
Epoch 1533 :: Batch 0/1
Batch Loss = 0.0019601853785243973
1533, epoch_train_loss=0.0019601853785243973
Epoch 1534
Epoch 1534 :: Batch 0/1
Batch Loss = 0.0019598981172358892
1534, epoch_train_loss=0.0019598981172358892
Epoch 1535
Epoch 1535 :: Batch 0/1
Batch Loss = 0.0019596117210641806
1535, epoch_train_loss=0.0019596117210641806
Epoch 1536
Epoch 1536 :: Batch 0/1
Batch Loss = 0.0019593261743468416
1536, epoch_train_loss=0.0019593261743468416
Epoch 1537
Epoch 1537 :: Batch 0/1
Batch Loss = 0.0019590414747544514
1537, epoch_train_loss=0.0019590414747544514
Epoch 1538
Epoch 1538 :: Batch 0/1
Batch Loss = 0.0019587576141483275
1538, epoch_train_loss=0.0019587576141483275
Epoch 1539
Epoch 1539 :: Batch 0/1
Batch Loss = 0.001958474584412856
1539, epoch_train_loss=0.001958474584412856
Epoch 1540
Epoch 1540 :: Batch 0/1
Batch Loss = 0.0019581923742210093
1540, epoch_train_loss=0.0019581923742210093
Epoch 1541
Epoch 1541 :: Batch 0/1
Batch Loss = 0.001957910977778031
1541, epoch_train_loss=0.001957910977778031
Epoch 1542
Epoch 1542 :: Batch 0/1
Batch Loss = 0.0019576303802598875
1542, epoch_train_loss=0.0019576303802598875
Epoch 1543
Epoch 1543 :: Batch 0/1
Batch Loss = 0.001957350571734577
1543, epoch_train_loss=0.001957350571734577
Epoch 1544
Epoch 1544 :: Batch 0/1
Batch Loss = 0.0019570715385622962
1544, epoch_train_loss=0.0019570715385622962
Epoch 1545
Epoch 1545 :: Batch 0/1
Batch Loss = 0.001956793268856948
1545, epoch_train_loss=0.001956793268856948
Epoch 1546
Epoch 1546 :: Batch 0/1
Batch Loss = 0.0019565157503782108
1546, epoch_train_loss=0.0019565157503782108
Epoch 1547
Epoch 1547 :: Batch 0/1
Batch Loss = 0.0019562389717316616
1547, epoch_train_loss=0.0019562389717316616
Epoch 1548
Epoch 1548 :: Batch 0/1
Batch Loss = 0.0019559629194479053
1548, epoch_train_loss=0.0019559629194479053
Epoch 1549
Epoch 1549 :: Batch 0/1
Batch Loss = 0.001955687584007821
1549, epoch_train_loss=0.001955687584007821
Epoch 1550
Epoch 1550 :: Batch 0/1
Batch Loss = 0.001955412954671863
1550, epoch_train_loss=0.001955412954671863
Epoch 1551
Epoch 1551 :: Batch 0/1
Batch Loss = 0.0019551390205798523
1551, epoch_train_loss=0.0019551390205798523
Epoch 1552
Epoch 1552 :: Batch 0/1
Batch Loss = 0.001954865772536858
1552, epoch_train_loss=0.001954865772536858
Epoch 1553
Epoch 1553 :: Batch 0/1
Batch Loss = 0.0019545932010634464
1553, epoch_train_loss=0.0019545932010634464
Epoch 1554
Epoch 1554 :: Batch 0/1
Batch Loss = 0.001954321297588446
1554, epoch_train_loss=0.001954321297588446
Epoch 1555
Epoch 1555 :: Batch 0/1
Batch Loss = 0.001954050053613019
1555, epoch_train_loss=0.001954050053613019
Epoch 1556
Epoch 1556 :: Batch 0/1
Batch Loss = 0.001953779461196136
1556, epoch_train_loss=0.001953779461196136
Epoch 1557
Epoch 1557 :: Batch 0/1
Batch Loss = 0.0019535095122276457
1557, epoch_train_loss=0.0019535095122276457
Epoch 1558
Epoch 1558 :: Batch 0/1
Batch Loss = 0.001953240200497435
1558, epoch_train_loss=0.001953240200497435
Epoch 1559
Epoch 1559 :: Batch 0/1
Batch Loss = 0.0019529715183213543
1559, epoch_train_loss=0.0019529715183213543
Epoch 1560
Epoch 1560 :: Batch 0/1
Batch Loss = 0.0019527034603054624
1560, epoch_train_loss=0.0019527034603054624
Epoch 1561
Epoch 1561 :: Batch 0/1
Batch Loss = 0.0019524360203973913
1561, epoch_train_loss=0.0019524360203973913
Epoch 1562
Epoch 1562 :: Batch 0/1
Batch Loss = 0.0019521691953887695
1562, epoch_train_loss=0.0019521691953887695
Epoch 1563
Epoch 1563 :: Batch 0/1
Batch Loss = 0.0019519029810988336
1563, epoch_train_loss=0.0019519029810988336
Epoch 1564
Epoch 1564 :: Batch 0/1
Batch Loss = 0.0019516373778915143
1564, epoch_train_loss=0.0019516373778915143
Epoch 1565
Epoch 1565 :: Batch 0/1
Batch Loss = 0.0019513723859512484
1565, epoch_train_loss=0.0019513723859512484
Epoch 1566
Epoch 1566 :: Batch 0/1
Batch Loss = 0.0019511080131822814
1566, epoch_train_loss=0.0019511080131822814
Epoch 1567
Epoch 1567 :: Batch 0/1
Batch Loss = 0.0019508442700210495
1567, epoch_train_loss=0.0019508442700210495
Epoch 1568
Epoch 1568 :: Batch 0/1
Batch Loss = 0.001950581181344277
1568, epoch_train_loss=0.001950581181344277
Epoch 1569
Epoch 1569 :: Batch 0/1
Batch Loss = 0.0019503187818396296
1569, epoch_train_loss=0.0019503187818396296
Epoch 1570
Epoch 1570 :: Batch 0/1
Batch Loss = 0.0019500571377049458
1570, epoch_train_loss=0.0019500571377049458
Epoch 1571
Epoch 1571 :: Batch 0/1
Batch Loss = 0.0019497963452591175
1571, epoch_train_loss=0.0019497963452591175
Epoch 1572
Epoch 1572 :: Batch 0/1
Batch Loss = 0.0019495365740092115
1572, epoch_train_loss=0.0019495365740092115
Epoch 1573
Epoch 1573 :: Batch 0/1
Batch Loss = 0.0019492780812882657
1573, epoch_train_loss=0.0019492780812882657
Epoch 1574
Epoch 1574 :: Batch 0/1
Batch Loss = 0.001949021307450979
1574, epoch_train_loss=0.001949021307450979
Epoch 1575
Epoch 1575 :: Batch 0/1
Batch Loss = 0.0019487669420231463
1575, epoch_train_loss=0.0019487669420231463
Epoch 1576
Epoch 1576 :: Batch 0/1
Batch Loss = 0.0019485161613165948
1576, epoch_train_loss=0.0019485161613165948
Epoch 1577
Epoch 1577 :: Batch 0/1
Batch Loss = 0.0019482708500032841
1577, epoch_train_loss=0.0019482708500032841
Epoch 1578
Epoch 1578 :: Batch 0/1
Batch Loss = 0.0019480342511731875
1578, epoch_train_loss=0.0019480342511731875
Epoch 1579
Epoch 1579 :: Batch 0/1
Batch Loss = 0.001947811648482521
1579, epoch_train_loss=0.001947811648482521
Epoch 1580
Epoch 1580 :: Batch 0/1
Batch Loss = 0.0019476122828511692
1580, epoch_train_loss=0.0019476122828511692
Epoch 1581
Epoch 1581 :: Batch 0/1
Batch Loss = 0.0019474513678726828
1581, epoch_train_loss=0.0019474513678726828
Epoch 1582
Epoch 1582 :: Batch 0/1
Batch Loss = 0.0019473561322841506
1582, epoch_train_loss=0.0019473561322841506
Epoch 1583
Epoch 1583 :: Batch 0/1
Batch Loss = 0.0019473715441527205
1583, epoch_train_loss=0.0019473715441527205
Epoch 1584
Epoch 1584 :: Batch 0/1
Batch Loss = 0.001947580622968901
1584, epoch_train_loss=0.001947580622968901
Epoch 1585
Epoch 1585 :: Batch 0/1
Batch Loss = 0.0019481195336794417
1585, epoch_train_loss=0.0019481195336794417
Epoch 1586
Epoch 1586 :: Batch 0/1
Batch Loss = 0.0019492505960775905
1586, epoch_train_loss=0.0019492505960775905
Epoch 1587
Epoch 1587 :: Batch 0/1
Batch Loss = 0.0019513945083105488
1587, epoch_train_loss=0.0019513945083105488
Epoch 1588
Epoch 1588 :: Batch 0/1
Batch Loss = 0.0019554127258044306
1588, epoch_train_loss=0.0019554127258044306
Epoch 1589
Epoch 1589 :: Batch 0/1
Batch Loss = 0.001962621463543491
1589, epoch_train_loss=0.001962621463543491
Epoch 1590
Epoch 1590 :: Batch 0/1
Batch Loss = 0.001975975253503541
1590, epoch_train_loss=0.001975975253503541
Epoch 1591
Epoch 1591 :: Batch 0/1
Batch Loss = 0.001999580608007552
1591, epoch_train_loss=0.001999580608007552
Epoch 1592
Epoch 1592 :: Batch 0/1
Batch Loss = 0.0020440968265342517
1592, epoch_train_loss=0.0020440968265342517
Epoch 1593
Epoch 1593 :: Batch 0/1
Batch Loss = 0.002121854018421634
1593, epoch_train_loss=0.002121854018421634
Epoch 1594
Epoch 1594 :: Batch 0/1
Batch Loss = 0.00227364849730281
1594, epoch_train_loss=0.00227364849730281
Epoch 1595
Epoch 1595 :: Batch 0/1
Batch Loss = 0.002531510840924978
1595, epoch_train_loss=0.002531510840924978
Epoch 1596
Epoch 1596 :: Batch 0/1
Batch Loss = 0.003061756172857006
1596, epoch_train_loss=0.003061756172857006
Epoch 1597
Epoch 1597 :: Batch 0/1
Batch Loss = 0.0039016136333574598
1597, epoch_train_loss=0.0039016136333574598
Epoch 1598
Epoch 1598 :: Batch 0/1
Batch Loss = 0.005760841089527918
1598, epoch_train_loss=0.005760841089527918
Epoch 1599
Epoch 1599 :: Batch 0/1
Batch Loss = 0.00822340882106714
1599, epoch_train_loss=0.00822340882106714
Epoch 1600
Epoch 1600 :: Batch 0/1
Batch Loss = 0.014218773681852323
1600, epoch_train_loss=0.014218773681852323
Epoch 1601
Epoch 1601 :: Batch 0/1
Batch Loss = 0.018852270414475972
1601, epoch_train_loss=0.018852270414475972
Epoch 1602
Epoch 1602 :: Batch 0/1
Batch Loss = 0.031753136625788096
1602, epoch_train_loss=0.031753136625788096
Epoch 1603
Epoch 1603 :: Batch 0/1
Batch Loss = 0.028663227376398308
1603, epoch_train_loss=0.028663227376398308
Epoch 1604
Epoch 1604 :: Batch 0/1
Batch Loss = 0.031499116592621515
1604, epoch_train_loss=0.031499116592621515
Epoch 1605
Epoch 1605 :: Batch 0/1
Batch Loss = 0.014099537902285362
1605, epoch_train_loss=0.014099537902285362
Epoch 1606
Epoch 1606 :: Batch 0/1
Batch Loss = 0.00404548496714823
1606, epoch_train_loss=0.00404548496714823
Epoch 1607
Epoch 1607 :: Batch 0/1
Batch Loss = 0.0028729767408873005
1607, epoch_train_loss=0.0028729767408873005
Epoch 1608
Epoch 1608 :: Batch 0/1
Batch Loss = 0.009228139585014119
1608, epoch_train_loss=0.009228139585014119
Epoch 1609
Epoch 1609 :: Batch 0/1
Batch Loss = 0.017038512956359858
1609, epoch_train_loss=0.017038512956359858
Epoch 1610
Epoch 1610 :: Batch 0/1
Batch Loss = 0.012233975917518994
1610, epoch_train_loss=0.012233975917518994
Epoch 1611
Epoch 1611 :: Batch 0/1
Batch Loss = 0.006040794953839919
1611, epoch_train_loss=0.006040794953839919
Epoch 1612
Epoch 1612 :: Batch 0/1
Batch Loss = 0.0021876019403279726
1612, epoch_train_loss=0.0021876019403279726
Epoch 1613
Epoch 1613 :: Batch 0/1
Batch Loss = 0.005185886262452933
1613, epoch_train_loss=0.005185886262452933
Epoch 1614
Epoch 1614 :: Batch 0/1
Batch Loss = 0.009884822494753831
1614, epoch_train_loss=0.009884822494753831
Epoch 1615
Epoch 1615 :: Batch 0/1
Batch Loss = 0.0074793413004776614
1615, epoch_train_loss=0.0074793413004776614
Epoch 1616
Epoch 1616 :: Batch 0/1
Batch Loss = 0.0035996202734004173
1616, epoch_train_loss=0.0035996202734004173
Epoch 1617
Epoch 1617 :: Batch 0/1
Batch Loss = 0.002237874375593751
1617, epoch_train_loss=0.002237874375593751
Epoch 1618
Epoch 1618 :: Batch 0/1
Batch Loss = 0.004644535389695514
1618, epoch_train_loss=0.004644535389695514
Epoch 1619
Epoch 1619 :: Batch 0/1
Batch Loss = 0.006788252615282211
1619, epoch_train_loss=0.006788252615282211
Epoch 1620
Epoch 1620 :: Batch 0/1
Batch Loss = 0.004437591464949528
1620, epoch_train_loss=0.004437591464949528
Epoch 1621
Epoch 1621 :: Batch 0/1
Batch Loss = 0.0022206320846421836
1621, epoch_train_loss=0.0022206320846421836
Epoch 1622
Epoch 1622 :: Batch 0/1
Batch Loss = 0.0028224553561660847
1622, epoch_train_loss=0.0028224553561660847
Epoch 1623
Epoch 1623 :: Batch 0/1
Batch Loss = 0.004369015592279394
1623, epoch_train_loss=0.004369015592279394
Epoch 1624
Epoch 1624 :: Batch 0/1
Batch Loss = 0.004233983671266505
1624, epoch_train_loss=0.004233983671266505
Epoch 1625
Epoch 1625 :: Batch 0/1
Batch Loss = 0.0024809562760084343
1625, epoch_train_loss=0.0024809562760084343
Epoch 1626
Epoch 1626 :: Batch 0/1
Batch Loss = 0.0022598646164718153
1626, epoch_train_loss=0.0022598646164718153
Epoch 1627
Epoch 1627 :: Batch 0/1
Batch Loss = 0.0034093603432926826
1627, epoch_train_loss=0.0034093603432926826
Epoch 1628
Epoch 1628 :: Batch 0/1
Batch Loss = 0.0035354743814214992
1628, epoch_train_loss=0.0035354743814214992
Epoch 1629
Epoch 1629 :: Batch 0/1
Batch Loss = 0.0026183782409900744
1629, epoch_train_loss=0.0026183782409900744
Epoch 1630
Epoch 1630 :: Batch 0/1
Batch Loss = 0.0020857570523884223
1630, epoch_train_loss=0.0020857570523884223
Epoch 1631
Epoch 1631 :: Batch 0/1
Batch Loss = 0.0026486235440637417
1631, epoch_train_loss=0.0026486235440637417
Epoch 1632
Epoch 1632 :: Batch 0/1
Batch Loss = 0.0031377921506886924
1632, epoch_train_loss=0.0031377921506886924
Epoch 1633
Epoch 1633 :: Batch 0/1
Batch Loss = 0.0026033150479741396
1633, epoch_train_loss=0.0026033150479741396
Epoch 1634
Epoch 1634 :: Batch 0/1
Batch Loss = 0.002069804324086922
1634, epoch_train_loss=0.002069804324086922
Epoch 1635
Epoch 1635 :: Batch 0/1
Batch Loss = 0.002255952499670361
1635, epoch_train_loss=0.002255952499670361
Epoch 1636
Epoch 1636 :: Batch 0/1
Batch Loss = 0.002646891438065895
1636, epoch_train_loss=0.002646891438065895
Epoch 1637
Epoch 1637 :: Batch 0/1
Batch Loss = 0.0025663636110219296
1637, epoch_train_loss=0.0025663636110219296
Epoch 1638
Epoch 1638 :: Batch 0/1
Batch Loss = 0.002133787020848942
1638, epoch_train_loss=0.002133787020848942
Epoch 1639
Epoch 1639 :: Batch 0/1
Batch Loss = 0.0020548649355292756
1639, epoch_train_loss=0.0020548649355292756
Epoch 1640
Epoch 1640 :: Batch 0/1
Batch Loss = 0.002328968985276247
1640, epoch_train_loss=0.002328968985276247
Epoch 1641
Epoch 1641 :: Batch 0/1
Batch Loss = 0.0024148877528075395
1641, epoch_train_loss=0.0024148877528075395
Epoch 1642
Epoch 1642 :: Batch 0/1
Batch Loss = 0.002198718361272672
1642, epoch_train_loss=0.002198718361272672
Epoch 1643
Epoch 1643 :: Batch 0/1
Batch Loss = 0.0020030440219248823
1643, epoch_train_loss=0.0020030440219248823
Epoch 1644
Epoch 1644 :: Batch 0/1
Batch Loss = 0.0020900804008666864
1644, epoch_train_loss=0.0020900804008666864
Epoch 1645
Epoch 1645 :: Batch 0/1
Batch Loss = 0.002257660576699576
1645, epoch_train_loss=0.002257660576699576
Epoch 1646
Epoch 1646 :: Batch 0/1
Batch Loss = 0.002210310645067702
1646, epoch_train_loss=0.002210310645067702
Epoch 1647
Epoch 1647 :: Batch 0/1
Batch Loss = 0.002042360435229295
1647, epoch_train_loss=0.002042360435229295
Epoch 1648
Epoch 1648 :: Batch 0/1
Batch Loss = 0.001984423180136806
1648, epoch_train_loss=0.001984423180136806
Epoch 1649
Epoch 1649 :: Batch 0/1
Batch Loss = 0.0020780194148305735
1649, epoch_train_loss=0.0020780194148305735
Epoch 1650
Epoch 1650 :: Batch 0/1
Batch Loss = 0.00215001264061826
1650, epoch_train_loss=0.00215001264061826
Epoch 1651
Epoch 1651 :: Batch 0/1
Batch Loss = 0.0020851737360635695
1651, epoch_train_loss=0.0020851737360635695
Epoch 1652
Epoch 1652 :: Batch 0/1
Batch Loss = 0.0019869012623928077
1652, epoch_train_loss=0.0019869012623928077
Epoch 1653
Epoch 1653 :: Batch 0/1
Batch Loss = 0.0019772662379402063
1653, epoch_train_loss=0.0019772662379402063
Epoch 1654
Epoch 1654 :: Batch 0/1
Batch Loss = 0.00204124042608631
1654, epoch_train_loss=0.00204124042608631
Epoch 1655
Epoch 1655 :: Batch 0/1
Batch Loss = 0.0020721025735245487
1655, epoch_train_loss=0.0020721025735245487
Epoch 1656
Epoch 1656 :: Batch 0/1
Batch Loss = 0.002023259308349751
1656, epoch_train_loss=0.002023259308349751
Epoch 1657
Epoch 1657 :: Batch 0/1
Batch Loss = 0.0019675394286324467
1657, epoch_train_loss=0.0019675394286324467
Epoch 1658
Epoch 1658 :: Batch 0/1
Batch Loss = 0.00196679719072333
1658, epoch_train_loss=0.00196679719072333
Epoch 1659
Epoch 1659 :: Batch 0/1
Batch Loss = 0.002004246857203664
1659, epoch_train_loss=0.002004246857203664
Epoch 1660
Epoch 1660 :: Batch 0/1
Batch Loss = 0.0020214756345827895
1660, epoch_train_loss=0.0020214756345827895
Epoch 1661
Epoch 1661 :: Batch 0/1
Batch Loss = 0.001994262885817312
1661, epoch_train_loss=0.001994262885817312
Epoch 1662
Epoch 1662 :: Batch 0/1
Batch Loss = 0.0019595489895512398
1662, epoch_train_loss=0.0019595489895512398
Epoch 1663
Epoch 1663 :: Batch 0/1
Batch Loss = 0.001955008190523935
1663, epoch_train_loss=0.001955008190523935
Epoch 1664
Epoch 1664 :: Batch 0/1
Batch Loss = 0.001975852179879542
1664, epoch_train_loss=0.001975852179879542
Epoch 1665
Epoch 1665 :: Batch 0/1
Batch Loss = 0.0019896424511396784
1665, epoch_train_loss=0.0019896424511396784
Epoch 1666
Epoch 1666 :: Batch 0/1
Batch Loss = 0.001977820601008862
1666, epoch_train_loss=0.001977820601008862
Epoch 1667
Epoch 1667 :: Batch 0/1
Batch Loss = 0.0019560235032948974
1667, epoch_train_loss=0.0019560235032948974
Epoch 1668
Epoch 1668 :: Batch 0/1
Batch Loss = 0.0019471253530352656
1668, epoch_train_loss=0.0019471253530352656
Epoch 1669
Epoch 1669 :: Batch 0/1
Batch Loss = 0.0019563767587261176
1669, epoch_train_loss=0.0019563767587261176
Epoch 1670
Epoch 1670 :: Batch 0/1
Batch Loss = 0.0019677103684501767
1670, epoch_train_loss=0.0019677103684501767
Epoch 1671
Epoch 1671 :: Batch 0/1
Batch Loss = 0.0019657387660913044
1671, epoch_train_loss=0.0019657387660913044
Epoch 1672
Epoch 1672 :: Batch 0/1
Batch Loss = 0.001953543381651769
1672, epoch_train_loss=0.001953543381651769
Epoch 1673
Epoch 1673 :: Batch 0/1
Batch Loss = 0.0019438857590995371
1673, epoch_train_loss=0.0019438857590995371
Epoch 1674
Epoch 1674 :: Batch 0/1
Batch Loss = 0.0019447094478258228
1674, epoch_train_loss=0.0019447094478258228
Epoch 1675
Epoch 1675 :: Batch 0/1
Batch Loss = 0.0019517188709350275
1675, epoch_train_loss=0.0019517188709350275
Epoch 1676
Epoch 1676 :: Batch 0/1
Batch Loss = 0.00195487143577356
1676, epoch_train_loss=0.00195487143577356
Epoch 1677
Epoch 1677 :: Batch 0/1
Batch Loss = 0.0019504759874994062
1677, epoch_train_loss=0.0019504759874994062
Epoch 1678
Epoch 1678 :: Batch 0/1
Batch Loss = 0.0019430941366080244
1678, epoch_train_loss=0.0019430941366080244
Epoch 1679
Epoch 1679 :: Batch 0/1
Batch Loss = 0.0019394941064681329
1679, epoch_train_loss=0.0019394941064681329
Epoch 1680
Epoch 1680 :: Batch 0/1
Batch Loss = 0.0019413029935665227
1680, epoch_train_loss=0.0019413029935665227
Epoch 1681
Epoch 1681 :: Batch 0/1
Batch Loss = 0.00194478247240264
1681, epoch_train_loss=0.00194478247240264
Epoch 1682
Epoch 1682 :: Batch 0/1
Batch Loss = 0.0019453400231102635
1682, epoch_train_loss=0.0019453400231102635
Epoch 1683
Epoch 1683 :: Batch 0/1
Batch Loss = 0.001942031780076188
1683, epoch_train_loss=0.001942031780076188
Epoch 1684
Epoch 1684 :: Batch 0/1
Batch Loss = 0.0019380167420149337
1684, epoch_train_loss=0.0019380167420149337
Epoch 1685
Epoch 1685 :: Batch 0/1
Batch Loss = 0.0019363452511544376
1685, epoch_train_loss=0.0019363452511544376
Epoch 1686
Epoch 1686 :: Batch 0/1
Batch Loss = 0.001937375189764446
1686, epoch_train_loss=0.001937375189764446
Epoch 1687
Epoch 1687 :: Batch 0/1
Batch Loss = 0.0019390428791281466
1687, epoch_train_loss=0.0019390428791281466
Epoch 1688
Epoch 1688 :: Batch 0/1
Batch Loss = 0.0019390613436179342
1688, epoch_train_loss=0.0019390613436179342
Epoch 1689
Epoch 1689 :: Batch 0/1
Batch Loss = 0.0019371546082509305
1689, epoch_train_loss=0.0019371546082509305
Epoch 1690
Epoch 1690 :: Batch 0/1
Batch Loss = 0.0019348087652621688
1690, epoch_train_loss=0.0019348087652621688
Epoch 1691
Epoch 1691 :: Batch 0/1
Batch Loss = 0.0019336015327643748
1691, epoch_train_loss=0.0019336015327643748
Epoch 1692
Epoch 1692 :: Batch 0/1
Batch Loss = 0.0019338559674368936
1692, epoch_train_loss=0.0019338559674368936
Epoch 1693
Epoch 1693 :: Batch 0/1
Batch Loss = 0.0019346332742276374
1693, epoch_train_loss=0.0019346332742276374
Epoch 1694
Epoch 1694 :: Batch 0/1
Batch Loss = 0.0019347183951062667
1694, epoch_train_loss=0.0019347183951062667
Epoch 1695
Epoch 1695 :: Batch 0/1
Batch Loss = 0.0019337229924029538
1695, epoch_train_loss=0.0019337229924029538
Epoch 1696
Epoch 1696 :: Batch 0/1
Batch Loss = 0.0019323019668949276
1696, epoch_train_loss=0.0019323019668949276
Epoch 1697
Epoch 1697 :: Batch 0/1
Batch Loss = 0.0019312952803975018
1697, epoch_train_loss=0.0019312952803975018
Epoch 1698
Epoch 1698 :: Batch 0/1
Batch Loss = 0.0019310711883887752
1698, epoch_train_loss=0.0019310711883887752
Epoch 1699
Epoch 1699 :: Batch 0/1
Batch Loss = 0.0019313106483268227
1699, epoch_train_loss=0.0019313106483268227
Epoch 1700
Epoch 1700 :: Batch 0/1
Batch Loss = 0.0019313878404243101
1700, epoch_train_loss=0.0019313878404243101
Epoch 1701
Epoch 1701 :: Batch 0/1
Batch Loss = 0.0019309664859856373
1701, epoch_train_loss=0.0019309664859856373
Epoch 1702
Epoch 1702 :: Batch 0/1
Batch Loss = 0.0019301691340283067
1702, epoch_train_loss=0.0019301691340283067
Epoch 1703
Epoch 1703 :: Batch 0/1
Batch Loss = 0.0019293659396237097
1703, epoch_train_loss=0.0019293659396237097
Epoch 1704
Epoch 1704 :: Batch 0/1
Batch Loss = 0.001928873073858378
1704, epoch_train_loss=0.001928873073858378
Epoch 1705
Epoch 1705 :: Batch 0/1
Batch Loss = 0.0019287292383119796
1705, epoch_train_loss=0.0019287292383119796
Epoch 1706
Epoch 1706 :: Batch 0/1
Batch Loss = 0.0019287101784109108
1706, epoch_train_loss=0.0019287101784109108
Epoch 1707
Epoch 1707 :: Batch 0/1
Batch Loss = 0.0019285566624601483
1707, epoch_train_loss=0.0019285566624601483
Epoch 1708
Epoch 1708 :: Batch 0/1
Batch Loss = 0.0019281666208910073
1708, epoch_train_loss=0.0019281666208910073
Epoch 1709
Epoch 1709 :: Batch 0/1
Batch Loss = 0.0019276165387343016
1709, epoch_train_loss=0.0019276165387343016
Epoch 1710
Epoch 1710 :: Batch 0/1
Batch Loss = 0.0019270978947354056
1710, epoch_train_loss=0.0019270978947354056
Epoch 1711
Epoch 1711 :: Batch 0/1
Batch Loss = 0.0019267399501411001
1711, epoch_train_loss=0.0019267399501411001
Epoch 1712
Epoch 1712 :: Batch 0/1
Batch Loss = 0.001926538413763968
1712, epoch_train_loss=0.001926538413763968
Epoch 1713
Epoch 1713 :: Batch 0/1
Batch Loss = 0.0019263961305182801
1713, epoch_train_loss=0.0019263961305182801
Epoch 1714
Epoch 1714 :: Batch 0/1
Batch Loss = 0.0019261992893949997
1714, epoch_train_loss=0.0019261992893949997
Epoch 1715
Epoch 1715 :: Batch 0/1
Batch Loss = 0.0019258909592480115
1715, epoch_train_loss=0.0019258909592480115
Epoch 1716
Epoch 1716 :: Batch 0/1
Batch Loss = 0.001925504997611336
1716, epoch_train_loss=0.001925504997611336
Epoch 1717
Epoch 1717 :: Batch 0/1
Batch Loss = 0.0019251189937639554
1717, epoch_train_loss=0.0019251189937639554
Epoch 1718
Epoch 1718 :: Batch 0/1
Batch Loss = 0.0019247955413473814
1718, epoch_train_loss=0.0019247955413473814
Epoch 1719
Epoch 1719 :: Batch 0/1
Batch Loss = 0.0019245504864595102
1719, epoch_train_loss=0.0019245504864595102
Epoch 1720
Epoch 1720 :: Batch 0/1
Batch Loss = 0.0019243500489478599
1720, epoch_train_loss=0.0019243500489478599
Epoch 1721
Epoch 1721 :: Batch 0/1
Batch Loss = 0.0019241429891223847
1721, epoch_train_loss=0.0019241429891223847
Epoch 1722
Epoch 1722 :: Batch 0/1
Batch Loss = 0.0019238992083578709
1722, epoch_train_loss=0.0019238992083578709
Epoch 1723
Epoch 1723 :: Batch 0/1
Batch Loss = 0.001923613490366804
1723, epoch_train_loss=0.001923613490366804
Epoch 1724
Epoch 1724 :: Batch 0/1
Batch Loss = 0.0019233073122936104
1724, epoch_train_loss=0.0019233073122936104
Epoch 1725
Epoch 1725 :: Batch 0/1
Batch Loss = 0.001923011449012084
1725, epoch_train_loss=0.001923011449012084
Epoch 1726
Epoch 1726 :: Batch 0/1
Batch Loss = 0.0019227455897410592
1726, epoch_train_loss=0.0019227455897410592
Epoch 1727
Epoch 1727 :: Batch 0/1
Batch Loss = 0.0019225106458658314
1727, epoch_train_loss=0.0019225106458658314
Epoch 1728
Epoch 1728 :: Batch 0/1
Batch Loss = 0.0019222940331581294
1728, epoch_train_loss=0.0019222940331581294
Epoch 1729
Epoch 1729 :: Batch 0/1
Batch Loss = 0.001922076379647303
1729, epoch_train_loss=0.001922076379647303
Epoch 1730
Epoch 1730 :: Batch 0/1
Batch Loss = 0.0019218450562438448
1730, epoch_train_loss=0.0019218450562438448
Epoch 1731
Epoch 1731 :: Batch 0/1
Batch Loss = 0.001921597913860585
1731, epoch_train_loss=0.001921597913860585
Epoch 1732
Epoch 1732 :: Batch 0/1
Batch Loss = 0.0019213407723008068
1732, epoch_train_loss=0.0019213407723008068
Epoch 1733
Epoch 1733 :: Batch 0/1
Batch Loss = 0.0019210848027942729
1733, epoch_train_loss=0.0019210848027942729
Epoch 1734
Epoch 1734 :: Batch 0/1
Batch Loss = 0.0019208398491508886
1734, epoch_train_loss=0.0019208398491508886
Epoch 1735
Epoch 1735 :: Batch 0/1
Batch Loss = 0.0019206085824174835
1735, epoch_train_loss=0.0019206085824174835
Epoch 1736
Epoch 1736 :: Batch 0/1
Batch Loss = 0.0019203885169209887
1736, epoch_train_loss=0.0019203885169209887
Epoch 1737
Epoch 1737 :: Batch 0/1
Batch Loss = 0.001920173509184059
1737, epoch_train_loss=0.001920173509184059
Epoch 1738
Epoch 1738 :: Batch 0/1
Batch Loss = 0.0019199572375272183
1738, epoch_train_loss=0.0019199572375272183
Epoch 1739
Epoch 1739 :: Batch 0/1
Batch Loss = 0.00191973625319413
1739, epoch_train_loss=0.00191973625319413
Epoch 1740
Epoch 1740 :: Batch 0/1
Batch Loss = 0.0019195104056728667
1740, epoch_train_loss=0.0019195104056728667
Epoch 1741
Epoch 1741 :: Batch 0/1
Batch Loss = 0.0019192818506800925
1741, epoch_train_loss=0.0019192818506800925
Epoch 1742
Epoch 1742 :: Batch 0/1
Batch Loss = 0.0019190546182804205
1742, epoch_train_loss=0.0019190546182804205
Epoch 1743
Epoch 1743 :: Batch 0/1
Batch Loss = 0.0019188315378914121
1743, epoch_train_loss=0.0019188315378914121
Epoch 1744
Epoch 1744 :: Batch 0/1
Batch Loss = 0.0019186138129835492
1744, epoch_train_loss=0.0019186138129835492
Epoch 1745
Epoch 1745 :: Batch 0/1
Batch Loss = 0.0019184012493275984
1745, epoch_train_loss=0.0019184012493275984
Epoch 1746
Epoch 1746 :: Batch 0/1
Batch Loss = 0.0019181923735173238
1746, epoch_train_loss=0.0019181923735173238
Epoch 1747
Epoch 1747 :: Batch 0/1
Batch Loss = 0.0019179852136113422
1747, epoch_train_loss=0.0019179852136113422
Epoch 1748
Epoch 1748 :: Batch 0/1
Batch Loss = 0.0019177783110940752
1748, epoch_train_loss=0.0019177783110940752
Epoch 1749
Epoch 1749 :: Batch 0/1
Batch Loss = 0.0019175707109974828
1749, epoch_train_loss=0.0019175707109974828
Epoch 1750
Epoch 1750 :: Batch 0/1
Batch Loss = 0.0019173624486051282
1750, epoch_train_loss=0.0019173624486051282
Epoch 1751
Epoch 1751 :: Batch 0/1
Batch Loss = 0.0019171542062086614
1751, epoch_train_loss=0.0019171542062086614
Epoch 1752
Epoch 1752 :: Batch 0/1
Batch Loss = 0.0019169467224898764
1752, epoch_train_loss=0.0019169467224898764
Epoch 1753
Epoch 1753 :: Batch 0/1
Batch Loss = 0.0019167408213279463
1753, epoch_train_loss=0.0019167408213279463
Epoch 1754
Epoch 1754 :: Batch 0/1
Batch Loss = 0.0019165371386088551
1754, epoch_train_loss=0.0019165371386088551
Epoch 1755
Epoch 1755 :: Batch 0/1
Batch Loss = 0.0019163357863656956
1755, epoch_train_loss=0.0019163357863656956
Epoch 1756
Epoch 1756 :: Batch 0/1
Batch Loss = 0.0019161366533168328
1756, epoch_train_loss=0.0019161366533168328
Epoch 1757
Epoch 1757 :: Batch 0/1
Batch Loss = 0.0019159393910306887
1757, epoch_train_loss=0.0019159393910306887
Epoch 1758
Epoch 1758 :: Batch 0/1
Batch Loss = 0.0019157435710726127
1758, epoch_train_loss=0.0019157435710726127
Epoch 1759
Epoch 1759 :: Batch 0/1
Batch Loss = 0.0019155488496885806
1759, epoch_train_loss=0.0019155488496885806
Epoch 1760
Epoch 1760 :: Batch 0/1
Batch Loss = 0.0019153549648323151
1760, epoch_train_loss=0.0019153549648323151
Epoch 1761
Epoch 1761 :: Batch 0/1
Batch Loss = 0.0019151617479995907
1761, epoch_train_loss=0.0019151617479995907
Epoch 1762
Epoch 1762 :: Batch 0/1
Batch Loss = 0.0019149692611978657
1762, epoch_train_loss=0.0019149692611978657
Epoch 1763
Epoch 1763 :: Batch 0/1
Batch Loss = 0.0019147775656114783
1763, epoch_train_loss=0.0019147775656114783
Epoch 1764
Epoch 1764 :: Batch 0/1
Batch Loss = 0.0019145867667488673
1764, epoch_train_loss=0.0019145867667488673
Epoch 1765
Epoch 1765 :: Batch 0/1
Batch Loss = 0.0019143969987998612
1765, epoch_train_loss=0.0019143969987998612
Epoch 1766
Epoch 1766 :: Batch 0/1
Batch Loss = 0.0019142083549258313
1766, epoch_train_loss=0.0019142083549258313
Epoch 1767
Epoch 1767 :: Batch 0/1
Batch Loss = 0.0019140208921517136
1767, epoch_train_loss=0.0019140208921517136
Epoch 1768
Epoch 1768 :: Batch 0/1
Batch Loss = 0.0019138346429157013
1768, epoch_train_loss=0.0019138346429157013
Epoch 1769
Epoch 1769 :: Batch 0/1
Batch Loss = 0.0019136495848972711
1769, epoch_train_loss=0.0019136495848972711
Epoch 1770
Epoch 1770 :: Batch 0/1
Batch Loss = 0.0019134657035830888
1770, epoch_train_loss=0.0019134657035830888
Epoch 1771
Epoch 1771 :: Batch 0/1
Batch Loss = 0.0019132829761271124
1771, epoch_train_loss=0.0019132829761271124
Epoch 1772
Epoch 1772 :: Batch 0/1
Batch Loss = 0.0019131013476048943
1772, epoch_train_loss=0.0019131013476048943
Epoch 1773
Epoch 1773 :: Batch 0/1
Batch Loss = 0.001912920792329476
1773, epoch_train_loss=0.001912920792329476
Epoch 1774
Epoch 1774 :: Batch 0/1
Batch Loss = 0.0019127412871421917
1774, epoch_train_loss=0.0019127412871421917
Epoch 1775
Epoch 1775 :: Batch 0/1
Batch Loss = 0.0019125628039320304
1775, epoch_train_loss=0.0019125628039320304
Epoch 1776
Epoch 1776 :: Batch 0/1
Batch Loss = 0.0019123853309777632
1776, epoch_train_loss=0.0019123853309777632
Epoch 1777
Epoch 1777 :: Batch 0/1
Batch Loss = 0.0019122088537128904
1777, epoch_train_loss=0.0019122088537128904
Epoch 1778
Epoch 1778 :: Batch 0/1
Batch Loss = 0.0019120333666431488
1778, epoch_train_loss=0.0019120333666431488
Epoch 1779
Epoch 1779 :: Batch 0/1
Batch Loss = 0.0019118588703039945
1779, epoch_train_loss=0.0019118588703039945
Epoch 1780
Epoch 1780 :: Batch 0/1
Batch Loss = 0.0019116853649812932
1780, epoch_train_loss=0.0019116853649812932
Epoch 1781
Epoch 1781 :: Batch 0/1
Batch Loss = 0.0019115128496290104
1781, epoch_train_loss=0.0019115128496290104
Epoch 1782
Epoch 1782 :: Batch 0/1
Batch Loss = 0.0019113413480205386
1782, epoch_train_loss=0.0019113413480205386
Epoch 1783
Epoch 1783 :: Batch 0/1
Batch Loss = 0.0019111708685643823
1783, epoch_train_loss=0.0019111708685643823
Epoch 1784
Epoch 1784 :: Batch 0/1
Batch Loss = 0.0019110014494811303
1784, epoch_train_loss=0.0019110014494811303
Epoch 1785
Epoch 1785 :: Batch 0/1
Batch Loss = 0.0019108331279731714
1785, epoch_train_loss=0.0019108331279731714
Epoch 1786
Epoch 1786 :: Batch 0/1
Batch Loss = 0.0019106659843432935
1786, epoch_train_loss=0.0019106659843432935
Epoch 1787
Epoch 1787 :: Batch 0/1
Batch Loss = 0.001910500112207855
1787, epoch_train_loss=0.001910500112207855
Epoch 1788
Epoch 1788 :: Batch 0/1
Batch Loss = 0.0019103356886142685
1788, epoch_train_loss=0.0019103356886142685
Epoch 1789
Epoch 1789 :: Batch 0/1
Batch Loss = 0.001910172939998648
1789, epoch_train_loss=0.001910172939998648
Epoch 1790
Epoch 1790 :: Batch 0/1
Batch Loss = 0.0019100122751463484
1790, epoch_train_loss=0.0019100122751463484
Epoch 1791
Epoch 1791 :: Batch 0/1
Batch Loss = 0.0019098542653820884
1791, epoch_train_loss=0.0019098542653820884
Epoch 1792
Epoch 1792 :: Batch 0/1
Batch Loss = 0.0019096998902163743
1792, epoch_train_loss=0.0019096998902163743
Epoch 1793
Epoch 1793 :: Batch 0/1
Batch Loss = 0.0019095506175686083
1793, epoch_train_loss=0.0019095506175686083
Epoch 1794
Epoch 1794 :: Batch 0/1
Batch Loss = 0.0019094089473586122
1794, epoch_train_loss=0.0019094089473586122
Epoch 1795
Epoch 1795 :: Batch 0/1
Batch Loss = 0.0019092787831124676
1795, epoch_train_loss=0.0019092787831124676
Epoch 1796
Epoch 1796 :: Batch 0/1
Batch Loss = 0.00190916683887636
1796, epoch_train_loss=0.00190916683887636
Epoch 1797
Epoch 1797 :: Batch 0/1
Batch Loss = 0.001909083931239855
1797, epoch_train_loss=0.001909083931239855
Epoch 1798
Epoch 1798 :: Batch 0/1
Batch Loss = 0.0019090490407549312
1798, epoch_train_loss=0.0019090490407549312
Epoch 1799
Epoch 1799 :: Batch 0/1
Batch Loss = 0.0019090934178774802
1799, epoch_train_loss=0.0019090934178774802
Epoch 1800
Epoch 1800 :: Batch 0/1
Batch Loss = 0.0019092734891669957
1800, epoch_train_loss=0.0019092734891669957
Epoch 1801
Epoch 1801 :: Batch 0/1
Batch Loss = 0.0019096832220239339
1801, epoch_train_loss=0.0019096832220239339
Epoch 1802
Epoch 1802 :: Batch 0/1
Batch Loss = 0.0019104990120940258
1802, epoch_train_loss=0.0019104990120940258
Epoch 1803
Epoch 1803 :: Batch 0/1
Batch Loss = 0.0019120135914146672
1803, epoch_train_loss=0.0019120135914146672
Epoch 1804
Epoch 1804 :: Batch 0/1
Batch Loss = 0.0019148054473885658
1804, epoch_train_loss=0.0019148054473885658
Epoch 1805
Epoch 1805 :: Batch 0/1
Batch Loss = 0.0019198069026375585
1805, epoch_train_loss=0.0019198069026375585
Epoch 1806
Epoch 1806 :: Batch 0/1
Batch Loss = 0.0019289957970747974
1806, epoch_train_loss=0.0019289957970747974
Epoch 1807
Epoch 1807 :: Batch 0/1
Batch Loss = 0.001945303726240871
1807, epoch_train_loss=0.001945303726240871
Epoch 1808
Epoch 1808 :: Batch 0/1
Batch Loss = 0.001975599957882795
1808, epoch_train_loss=0.001975599957882795
Epoch 1809
Epoch 1809 :: Batch 0/1
Batch Loss = 0.0020279761047283617
1809, epoch_train_loss=0.0020279761047283617
Epoch 1810
Epoch 1810 :: Batch 0/1
Batch Loss = 0.0021244440094155
1810, epoch_train_loss=0.0021244440094155
Epoch 1811
Epoch 1811 :: Batch 0/1
Batch Loss = 0.0022751479101232315
1811, epoch_train_loss=0.0022751479101232315
Epoch 1812
Epoch 1812 :: Batch 0/1
Batch Loss = 0.0025244831126542
1812, epoch_train_loss=0.0025244831126542
Epoch 1813
Epoch 1813 :: Batch 0/1
Batch Loss = 0.0028095058185612734
1813, epoch_train_loss=0.0028095058185612734
Epoch 1814
Epoch 1814 :: Batch 0/1
Batch Loss = 0.003115190214609123
1814, epoch_train_loss=0.003115190214609123
Epoch 1815
Epoch 1815 :: Batch 0/1
Batch Loss = 0.003358326596439367
1815, epoch_train_loss=0.003358326596439367
Epoch 1816
Epoch 1816 :: Batch 0/1
Batch Loss = 0.00365994374094289
1816, epoch_train_loss=0.00365994374094289
Epoch 1817
Epoch 1817 :: Batch 0/1
Batch Loss = 0.004740170531016581
1817, epoch_train_loss=0.004740170531016581
Epoch 1818
Epoch 1818 :: Batch 0/1
Batch Loss = 0.006215799125164456
1818, epoch_train_loss=0.006215799125164456
Epoch 1819
Epoch 1819 :: Batch 0/1
Batch Loss = 0.009338688914926828
1819, epoch_train_loss=0.009338688914926828
Epoch 1820
Epoch 1820 :: Batch 0/1
Batch Loss = 0.011322842370731466
1820, epoch_train_loss=0.011322842370731466
Epoch 1821
Epoch 1821 :: Batch 0/1
Batch Loss = 0.016717862366982028
1821, epoch_train_loss=0.016717862366982028
Epoch 1822
Epoch 1822 :: Batch 0/1
Batch Loss = 0.016972220763100305
1822, epoch_train_loss=0.016972220763100305
Epoch 1823
Epoch 1823 :: Batch 0/1
Batch Loss = 0.02092856219296423
1823, epoch_train_loss=0.02092856219296423
Epoch 1824
Epoch 1824 :: Batch 0/1
Batch Loss = 0.014674017354029262
1824, epoch_train_loss=0.014674017354029262
Epoch 1825
Epoch 1825 :: Batch 0/1
Batch Loss = 0.010481310416995319
1825, epoch_train_loss=0.010481310416995319
Epoch 1826
Epoch 1826 :: Batch 0/1
Batch Loss = 0.004421111653502861
1826, epoch_train_loss=0.004421111653502861
Epoch 1827
Epoch 1827 :: Batch 0/1
Batch Loss = 0.002156943238837478
1827, epoch_train_loss=0.002156943238837478
Epoch 1828
Epoch 1828 :: Batch 0/1
Batch Loss = 0.003403582517136995
1828, epoch_train_loss=0.003403582517136995
Epoch 1829
Epoch 1829 :: Batch 0/1
Batch Loss = 0.006136539614000003
1829, epoch_train_loss=0.006136539614000003
Epoch 1830
Epoch 1830 :: Batch 0/1
Batch Loss = 0.008993923120525708
1830, epoch_train_loss=0.008993923120525708
Epoch 1831
Epoch 1831 :: Batch 0/1
Batch Loss = 0.007414799383883241
1831, epoch_train_loss=0.007414799383883241
Epoch 1832
Epoch 1832 :: Batch 0/1
Batch Loss = 0.005015472098062624
1832, epoch_train_loss=0.005015472098062624
Epoch 1833
Epoch 1833 :: Batch 0/1
Batch Loss = 0.0024189421037100482
1833, epoch_train_loss=0.0024189421037100482
Epoch 1834
Epoch 1834 :: Batch 0/1
Batch Loss = 0.002262835422864594
1834, epoch_train_loss=0.002262835422864594
Epoch 1835
Epoch 1835 :: Batch 0/1
Batch Loss = 0.003895168223251995
1835, epoch_train_loss=0.003895168223251995
Epoch 1836
Epoch 1836 :: Batch 0/1
Batch Loss = 0.00505812086174719
1836, epoch_train_loss=0.00505812086174719
Epoch 1837
Epoch 1837 :: Batch 0/1
Batch Loss = 0.005204297268971824
1837, epoch_train_loss=0.005204297268971824
Epoch 1838
Epoch 1838 :: Batch 0/1
Batch Loss = 0.003515944453650269
1838, epoch_train_loss=0.003515944453650269
Epoch 1839
Epoch 1839 :: Batch 0/1
Batch Loss = 0.002193700295020581
1839, epoch_train_loss=0.002193700295020581
Epoch 1840
Epoch 1840 :: Batch 0/1
Batch Loss = 0.002156028384578064
1840, epoch_train_loss=0.002156028384578064
Epoch 1841
Epoch 1841 :: Batch 0/1
Batch Loss = 0.0030570947336000963
1841, epoch_train_loss=0.0030570947336000963
Epoch 1842
Epoch 1842 :: Batch 0/1
Batch Loss = 0.0038035952301520453
1842, epoch_train_loss=0.0038035952301520453
Epoch 1843
Epoch 1843 :: Batch 0/1
Batch Loss = 0.0032812257984700466
1843, epoch_train_loss=0.0032812257984700466
Epoch 1844
Epoch 1844 :: Batch 0/1
Batch Loss = 0.0024119970607644424
1844, epoch_train_loss=0.0024119970607644424
Epoch 1845
Epoch 1845 :: Batch 0/1
Batch Loss = 0.0019769239507082784
1845, epoch_train_loss=0.0019769239507082784
Epoch 1846
Epoch 1846 :: Batch 0/1
Batch Loss = 0.0023208461035964605
1846, epoch_train_loss=0.0023208461035964605
Epoch 1847
Epoch 1847 :: Batch 0/1
Batch Loss = 0.002851864154570415
1847, epoch_train_loss=0.002851864154570415
Epoch 1848
Epoch 1848 :: Batch 0/1
Batch Loss = 0.0028671945169598738
1848, epoch_train_loss=0.0028671945169598738
Epoch 1849
Epoch 1849 :: Batch 0/1
Batch Loss = 0.0024914803905425682
1849, epoch_train_loss=0.0024914803905425682
Epoch 1850
Epoch 1850 :: Batch 0/1
Batch Loss = 0.0020525487921356717
1850, epoch_train_loss=0.0020525487921356717
Epoch 1851
Epoch 1851 :: Batch 0/1
Batch Loss = 0.0020015424361260726
1851, epoch_train_loss=0.0020015424361260726
Epoch 1852
Epoch 1852 :: Batch 0/1
Batch Loss = 0.0023076325129671015
1852, epoch_train_loss=0.0023076325129671015
Epoch 1853
Epoch 1853 :: Batch 0/1
Batch Loss = 0.0025179261667557847
1853, epoch_train_loss=0.0025179261667557847
Epoch 1854
Epoch 1854 :: Batch 0/1
Batch Loss = 0.0024396887153015565
1854, epoch_train_loss=0.0024396887153015565
Epoch 1855
Epoch 1855 :: Batch 0/1
Batch Loss = 0.0021299234447023107
1855, epoch_train_loss=0.0021299234447023107
Epoch 1856
Epoch 1856 :: Batch 0/1
Batch Loss = 0.001955274427360679
1856, epoch_train_loss=0.001955274427360679
Epoch 1857
Epoch 1857 :: Batch 0/1
Batch Loss = 0.002018847245845316
1857, epoch_train_loss=0.002018847245845316
Epoch 1858
Epoch 1858 :: Batch 0/1
Batch Loss = 0.0021871586313285986
1858, epoch_train_loss=0.0021871586313285986
Epoch 1859
Epoch 1859 :: Batch 0/1
Batch Loss = 0.0022640129582000776
1859, epoch_train_loss=0.0022640129582000776
Epoch 1860
Epoch 1860 :: Batch 0/1
Batch Loss = 0.0021663687810443257
1860, epoch_train_loss=0.0021663687810443257
Epoch 1861
Epoch 1861 :: Batch 0/1
Batch Loss = 0.002016832100295294
1861, epoch_train_loss=0.002016832100295294
Epoch 1862
Epoch 1862 :: Batch 0/1
Batch Loss = 0.0019381413421156375
1862, epoch_train_loss=0.0019381413421156375
Epoch 1863
Epoch 1863 :: Batch 0/1
Batch Loss = 0.001980406794663193
1863, epoch_train_loss=0.001980406794663193
Epoch 1864
Epoch 1864 :: Batch 0/1
Batch Loss = 0.0020787640155613186
1864, epoch_train_loss=0.0020787640155613186
Epoch 1865
Epoch 1865 :: Batch 0/1
Batch Loss = 0.0021106186576554426
1865, epoch_train_loss=0.0021106186576554426
Epoch 1866
Epoch 1866 :: Batch 0/1
Batch Loss = 0.002059300345565004
1866, epoch_train_loss=0.002059300345565004
Epoch 1867
Epoch 1867 :: Batch 0/1
Batch Loss = 0.0019713866214361923
1867, epoch_train_loss=0.0019713866214361923
Epoch 1868
Epoch 1868 :: Batch 0/1
Batch Loss = 0.0019286165362378013
1868, epoch_train_loss=0.0019286165362378013
Epoch 1869
Epoch 1869 :: Batch 0/1
Batch Loss = 0.0019492584704257506
1869, epoch_train_loss=0.0019492584704257506
Epoch 1870
Epoch 1870 :: Batch 0/1
Batch Loss = 0.0019979352404135985
1870, epoch_train_loss=0.0019979352404135985
Epoch 1871
Epoch 1871 :: Batch 0/1
Batch Loss = 0.00202437153787045
1871, epoch_train_loss=0.00202437153787045
Epoch 1872
Epoch 1872 :: Batch 0/1
Batch Loss = 0.002005783800165293
1872, epoch_train_loss=0.002005783800165293
Epoch 1873
Epoch 1873 :: Batch 0/1
Batch Loss = 0.0019629168201569726
1873, epoch_train_loss=0.0019629168201569726
Epoch 1874
Epoch 1874 :: Batch 0/1
Batch Loss = 0.0019269563053309414
1874, epoch_train_loss=0.0019269563053309414
Epoch 1875
Epoch 1875 :: Batch 0/1
Batch Loss = 0.0019227870437367113
1875, epoch_train_loss=0.0019227870437367113
Epoch 1876
Epoch 1876 :: Batch 0/1
Batch Loss = 0.0019453550772437944
1876, epoch_train_loss=0.0019453550772437944
Epoch 1877
Epoch 1877 :: Batch 0/1
Batch Loss = 0.0019671649206975464
1877, epoch_train_loss=0.0019671649206975464
Epoch 1878
Epoch 1878 :: Batch 0/1
Batch Loss = 0.0019714566154195386
1878, epoch_train_loss=0.0019714566154195386
Epoch 1879
Epoch 1879 :: Batch 0/1
Batch Loss = 0.0019543491490604304
1879, epoch_train_loss=0.0019543491490604304
Epoch 1880
Epoch 1880 :: Batch 0/1
Batch Loss = 0.0019316759854435032
1880, epoch_train_loss=0.0019316759854435032
Epoch 1881
Epoch 1881 :: Batch 0/1
Batch Loss = 0.001917668074703898
1881, epoch_train_loss=0.001917668074703898
Epoch 1882
Epoch 1882 :: Batch 0/1
Batch Loss = 0.0019183818707022907
1882, epoch_train_loss=0.0019183818707022907
Epoch 1883
Epoch 1883 :: Batch 0/1
Batch Loss = 0.0019284020102500732
1883, epoch_train_loss=0.0019284020102500732
Epoch 1884
Epoch 1884 :: Batch 0/1
Batch Loss = 0.0019387003316750444
1884, epoch_train_loss=0.0019387003316750444
Epoch 1885
Epoch 1885 :: Batch 0/1
Batch Loss = 0.0019409768430190147
1885, epoch_train_loss=0.0019409768430190147
Epoch 1886
Epoch 1886 :: Batch 0/1
Batch Loss = 0.0019332343595340302
1886, epoch_train_loss=0.0019332343595340302
Epoch 1887
Epoch 1887 :: Batch 0/1
Batch Loss = 0.0019218728631336492
1887, epoch_train_loss=0.0019218728631336492
Epoch 1888
Epoch 1888 :: Batch 0/1
Batch Loss = 0.001913582983102056
1888, epoch_train_loss=0.001913582983102056
Epoch 1889
Epoch 1889 :: Batch 0/1
Batch Loss = 0.00191184341846345
1889, epoch_train_loss=0.00191184341846345
Epoch 1890
Epoch 1890 :: Batch 0/1
Batch Loss = 0.0019156679655798258
1890, epoch_train_loss=0.0019156679655798258
Epoch 1891
Epoch 1891 :: Batch 0/1
Batch Loss = 0.0019205030422806918
1891, epoch_train_loss=0.0019205030422806918
Epoch 1892
Epoch 1892 :: Batch 0/1
Batch Loss = 0.0019230827180094507
1892, epoch_train_loss=0.0019230827180094507
Epoch 1893
Epoch 1893 :: Batch 0/1
Batch Loss = 0.001921772289831289
1893, epoch_train_loss=0.001921772289831289
Epoch 1894
Epoch 1894 :: Batch 0/1
Batch Loss = 0.001917456874589002
1894, epoch_train_loss=0.001917456874589002
Epoch 1895
Epoch 1895 :: Batch 0/1
Batch Loss = 0.001912247017890967
1895, epoch_train_loss=0.001912247017890967
Epoch 1896
Epoch 1896 :: Batch 0/1
Batch Loss = 0.0019088829768902755
1896, epoch_train_loss=0.0019088829768902755
Epoch 1897
Epoch 1897 :: Batch 0/1
Batch Loss = 0.0019082488170900761
1897, epoch_train_loss=0.0019082488170900761
Epoch 1898
Epoch 1898 :: Batch 0/1
Batch Loss = 0.0019098177790862314
1898, epoch_train_loss=0.0019098177790862314
Epoch 1899
Epoch 1899 :: Batch 0/1
Batch Loss = 0.0019118819934601011
1899, epoch_train_loss=0.0019118819934601011
Epoch 1900
Epoch 1900 :: Batch 0/1
Batch Loss = 0.001912906578059294
1900, epoch_train_loss=0.001912906578059294
Epoch 1901
Epoch 1901 :: Batch 0/1
Batch Loss = 0.0019125059070385958
1901, epoch_train_loss=0.0019125059070385958
Epoch 1902
Epoch 1902 :: Batch 0/1
Batch Loss = 0.0019108296785663094
1902, epoch_train_loss=0.0019108296785663094
Epoch 1903
Epoch 1903 :: Batch 0/1
Batch Loss = 0.0019085481413849714
1903, epoch_train_loss=0.0019085481413849714
Epoch 1904
Epoch 1904 :: Batch 0/1
Batch Loss = 0.0019065608394955662
1904, epoch_train_loss=0.0019065608394955662
Epoch 1905
Epoch 1905 :: Batch 0/1
Batch Loss = 0.001905457813565285
1905, epoch_train_loss=0.001905457813565285
Epoch 1906
Epoch 1906 :: Batch 0/1
Batch Loss = 0.0019053551788314703
1906, epoch_train_loss=0.0019053551788314703
Epoch 1907
Epoch 1907 :: Batch 0/1
Batch Loss = 0.0019059314122237489
1907, epoch_train_loss=0.0019059314122237489
Epoch 1908
Epoch 1908 :: Batch 0/1
Batch Loss = 0.001906582944814266
1908, epoch_train_loss=0.001906582944814266
Epoch 1909
Epoch 1909 :: Batch 0/1
Batch Loss = 0.001906859175392098
1909, epoch_train_loss=0.001906859175392098
Epoch 1910
Epoch 1910 :: Batch 0/1
Batch Loss = 0.001906652771959952
1910, epoch_train_loss=0.001906652771959952
Epoch 1911
Epoch 1911 :: Batch 0/1
Batch Loss = 0.0019059556443097815
1911, epoch_train_loss=0.0019059556443097815
Epoch 1912
Epoch 1912 :: Batch 0/1
Batch Loss = 0.001905008714008394
1912, epoch_train_loss=0.001905008714008394
Epoch 1913
Epoch 1913 :: Batch 0/1
Batch Loss = 0.001904057946800483
1913, epoch_train_loss=0.001904057946800483
Epoch 1914
Epoch 1914 :: Batch 0/1
Batch Loss = 0.0019032944041125167
1914, epoch_train_loss=0.0019032944041125167
Epoch 1915
Epoch 1915 :: Batch 0/1
Batch Loss = 0.0019028543857515377
1915, epoch_train_loss=0.0019028543857515377
Epoch 1916
Epoch 1916 :: Batch 0/1
Batch Loss = 0.0019027283905385477
1916, epoch_train_loss=0.0019027283905385477
Epoch 1917
Epoch 1917 :: Batch 0/1
Batch Loss = 0.0019027851284293708
1917, epoch_train_loss=0.0019027851284293708
Epoch 1918
Epoch 1918 :: Batch 0/1
Batch Loss = 0.0019028873623654163
1918, epoch_train_loss=0.0019028873623654163
Epoch 1919
Epoch 1919 :: Batch 0/1
Batch Loss = 0.0019029147618626061
1919, epoch_train_loss=0.0019029147618626061
Epoch 1920
Epoch 1920 :: Batch 0/1
Batch Loss = 0.0019028000790645189
1920, epoch_train_loss=0.0019028000790645189
Epoch 1921
Epoch 1921 :: Batch 0/1
Batch Loss = 0.001902563791085945
1921, epoch_train_loss=0.001902563791085945
Epoch 1922
Epoch 1922 :: Batch 0/1
Batch Loss = 0.0019022069560996118
1922, epoch_train_loss=0.0019022069560996118
Epoch 1923
Epoch 1923 :: Batch 0/1
Batch Loss = 0.0019017798405714839
1923, epoch_train_loss=0.0019017798405714839
Epoch 1924
Epoch 1924 :: Batch 0/1
Batch Loss = 0.00190134567264058
1924, epoch_train_loss=0.00190134567264058
Epoch 1925
Epoch 1925 :: Batch 0/1
Batch Loss = 0.0019009579924548622
1925, epoch_train_loss=0.0019009579924548622
Epoch 1926
Epoch 1926 :: Batch 0/1
Batch Loss = 0.0019006467998015067
1926, epoch_train_loss=0.0019006467998015067
Epoch 1927
Epoch 1927 :: Batch 0/1
Batch Loss = 0.001900418749719897
1927, epoch_train_loss=0.001900418749719897
Epoch 1928
Epoch 1928 :: Batch 0/1
Batch Loss = 0.001900248674263263
1928, epoch_train_loss=0.001900248674263263
Epoch 1929
Epoch 1929 :: Batch 0/1
Batch Loss = 0.001900125515674206
1929, epoch_train_loss=0.001900125515674206
Epoch 1930
Epoch 1930 :: Batch 0/1
Batch Loss = 0.0019000270762570726
1930, epoch_train_loss=0.0019000270762570726
Epoch 1931
Epoch 1931 :: Batch 0/1
Batch Loss = 0.0018999324875451909
1931, epoch_train_loss=0.0018999324875451909
Epoch 1932
Epoch 1932 :: Batch 0/1
Batch Loss = 0.0018998295112246815
1932, epoch_train_loss=0.0018998295112246815
Epoch 1933
Epoch 1933 :: Batch 0/1
Batch Loss = 0.0018997060118671233
1933, epoch_train_loss=0.0018997060118671233
Epoch 1934
Epoch 1934 :: Batch 0/1
Batch Loss = 0.00189955563430199
1934, epoch_train_loss=0.00189955563430199
Epoch 1935
Epoch 1935 :: Batch 0/1
Batch Loss = 0.0018993847805808199
1935, epoch_train_loss=0.0018993847805808199
Epoch 1936
Epoch 1936 :: Batch 0/1
Batch Loss = 0.0018991980695567791
1936, epoch_train_loss=0.0018991980695567791
Epoch 1937
Epoch 1937 :: Batch 0/1
Batch Loss = 0.0018990032049426525
1937, epoch_train_loss=0.0018990032049426525
Epoch 1938
Epoch 1938 :: Batch 0/1
Batch Loss = 0.0018988055022354218
1938, epoch_train_loss=0.0018988055022354218
Epoch 1939
Epoch 1939 :: Batch 0/1
Batch Loss = 0.0018986057747853546
1939, epoch_train_loss=0.0018986057747853546
Epoch 1940
Epoch 1940 :: Batch 0/1
Batch Loss = 0.0018984090651011328
1940, epoch_train_loss=0.0018984090651011328
Epoch 1941
Epoch 1941 :: Batch 0/1
Batch Loss = 0.0018982200332459513
1941, epoch_train_loss=0.0018982200332459513
Epoch 1942
Epoch 1942 :: Batch 0/1
Batch Loss = 0.001898038565530367
1942, epoch_train_loss=0.001898038565530367
Epoch 1943
Epoch 1943 :: Batch 0/1
Batch Loss = 0.0018978665324420707
1943, epoch_train_loss=0.0018978665324420707
Epoch 1944
Epoch 1944 :: Batch 0/1
Batch Loss = 0.0018977019231950743
1944, epoch_train_loss=0.0018977019231950743
Epoch 1945
Epoch 1945 :: Batch 0/1
Batch Loss = 0.0018975435273815948
1945, epoch_train_loss=0.0018975435273815948
Epoch 1946
Epoch 1946 :: Batch 0/1
Batch Loss = 0.0018973909380512169
1946, epoch_train_loss=0.0018973909380512169
Epoch 1947
Epoch 1947 :: Batch 0/1
Batch Loss = 0.001897243263418489
1947, epoch_train_loss=0.001897243263418489
Epoch 1948
Epoch 1948 :: Batch 0/1
Batch Loss = 0.0018971000574197758
1948, epoch_train_loss=0.0018971000574197758
Epoch 1949
Epoch 1949 :: Batch 0/1
Batch Loss = 0.0018969615387010338
1949, epoch_train_loss=0.0018969615387010338
Epoch 1950
Epoch 1950 :: Batch 0/1
Batch Loss = 0.0018968254719924124
1950, epoch_train_loss=0.0018968254719924124
Epoch 1951
Epoch 1951 :: Batch 0/1
Batch Loss = 0.0018966917029625773
1951, epoch_train_loss=0.0018966917029625773
Epoch 1952
Epoch 1952 :: Batch 0/1
Batch Loss = 0.00189655996447344
1952, epoch_train_loss=0.00189655996447344
Epoch 1953
Epoch 1953 :: Batch 0/1
Batch Loss = 0.0018964301085831329
1953, epoch_train_loss=0.0018964301085831329
Epoch 1954
Epoch 1954 :: Batch 0/1
Batch Loss = 0.0018963024466852319
1954, epoch_train_loss=0.0018963024466852319
Epoch 1955
Epoch 1955 :: Batch 0/1
Batch Loss = 0.001896176824172958
1955, epoch_train_loss=0.001896176824172958
Epoch 1956
Epoch 1956 :: Batch 0/1
Batch Loss = 0.0018960529400301627
1956, epoch_train_loss=0.0018960529400301627
Epoch 1957
Epoch 1957 :: Batch 0/1
Batch Loss = 0.001895931147355267
1957, epoch_train_loss=0.001895931147355267
Epoch 1958
Epoch 1958 :: Batch 0/1
Batch Loss = 0.0018958111693004253
1958, epoch_train_loss=0.0018958111693004253
Epoch 1959
Epoch 1959 :: Batch 0/1
Batch Loss = 0.0018956935058621674
1959, epoch_train_loss=0.0018956935058621674
Epoch 1960
Epoch 1960 :: Batch 0/1
Batch Loss = 0.0018955789760123959
1960, epoch_train_loss=0.0018955789760123959
Epoch 1961
Epoch 1961 :: Batch 0/1
Batch Loss = 0.001895468170960896
1961, epoch_train_loss=0.001895468170960896
Epoch 1962
Epoch 1962 :: Batch 0/1
Batch Loss = 0.0018953621869381295
1962, epoch_train_loss=0.0018953621869381295
Epoch 1963
Epoch 1963 :: Batch 0/1
Batch Loss = 0.0018952626056296668
1963, epoch_train_loss=0.0018952626056296668
Epoch 1964
Epoch 1964 :: Batch 0/1
Batch Loss = 0.001895171629807734
1964, epoch_train_loss=0.001895171629807734
Epoch 1965
Epoch 1965 :: Batch 0/1
Batch Loss = 0.001895093286270628
1965, epoch_train_loss=0.001895093286270628
Epoch 1966
Epoch 1966 :: Batch 0/1
Batch Loss = 0.001895032980929934
1966, epoch_train_loss=0.001895032980929934
Epoch 1967
Epoch 1967 :: Batch 0/1
Batch Loss = 0.0018949999675566616
1967, epoch_train_loss=0.0018949999675566616
Epoch 1968
Epoch 1968 :: Batch 0/1
Batch Loss = 0.001895007483001541
1968, epoch_train_loss=0.001895007483001541
Epoch 1969
Epoch 1969 :: Batch 0/1
Batch Loss = 0.0018950780724891732
1969, epoch_train_loss=0.0018950780724891732
Epoch 1970
Epoch 1970 :: Batch 0/1
Batch Loss = 0.0018952438196639533
1970, epoch_train_loss=0.0018952438196639533
Epoch 1971
Epoch 1971 :: Batch 0/1
Batch Loss = 0.0018955627438266686
1971, epoch_train_loss=0.0018955627438266686
Epoch 1972
Epoch 1972 :: Batch 0/1
Batch Loss = 0.001896115799094274
1972, epoch_train_loss=0.001896115799094274
Epoch 1973
Epoch 1973 :: Batch 0/1
Batch Loss = 0.0018970572656210952
1973, epoch_train_loss=0.0018970572656210952
Epoch 1974
Epoch 1974 :: Batch 0/1
Batch Loss = 0.0018985960705219802
1974, epoch_train_loss=0.0018985960705219802
Epoch 1975
Epoch 1975 :: Batch 0/1
Batch Loss = 0.0019011593807029858
1975, epoch_train_loss=0.0019011593807029858
Epoch 1976
Epoch 1976 :: Batch 0/1
Batch Loss = 0.0019052946821075305
1976, epoch_train_loss=0.0019052946821075305
Epoch 1977
Epoch 1977 :: Batch 0/1
Batch Loss = 0.0019122392276811546
1977, epoch_train_loss=0.0019122392276811546
Epoch 1978
Epoch 1978 :: Batch 0/1
Batch Loss = 0.0019234313121069717
1978, epoch_train_loss=0.0019234313121069717
Epoch 1979
Epoch 1979 :: Batch 0/1
Batch Loss = 0.0019426343091214703
1979, epoch_train_loss=0.0019426343091214703
Epoch 1980
Epoch 1980 :: Batch 0/1
Batch Loss = 0.00197353786680976
1980, epoch_train_loss=0.00197353786680976
Epoch 1981
Epoch 1981 :: Batch 0/1
Batch Loss = 0.0020282333409147825
1981, epoch_train_loss=0.0020282333409147825
Epoch 1982
Epoch 1982 :: Batch 0/1
Batch Loss = 0.0021153534977574047
1982, epoch_train_loss=0.0021153534977574047
Epoch 1983
Epoch 1983 :: Batch 0/1
Batch Loss = 0.002276114702854841
1983, epoch_train_loss=0.002276114702854841
Epoch 1984
Epoch 1984 :: Batch 0/1
Batch Loss = 0.0025245073574043145
1984, epoch_train_loss=0.0025245073574043145
Epoch 1985
Epoch 1985 :: Batch 0/1
Batch Loss = 0.0030093467683935246
1985, epoch_train_loss=0.0030093467683935246
Epoch 1986
Epoch 1986 :: Batch 0/1
Batch Loss = 0.003705938212528734
1986, epoch_train_loss=0.003705938212528734
Epoch 1987
Epoch 1987 :: Batch 0/1
Batch Loss = 0.005172073306570774
1987, epoch_train_loss=0.005172073306570774
Epoch 1988
Epoch 1988 :: Batch 0/1
Batch Loss = 0.006946921025994317
1988, epoch_train_loss=0.006946921025994317
Epoch 1989
Epoch 1989 :: Batch 0/1
Batch Loss = 0.011063815225573096
1989, epoch_train_loss=0.011063815225573096
Epoch 1990
Epoch 1990 :: Batch 0/1
Batch Loss = 0.014204478257998242
1990, epoch_train_loss=0.014204478257998242
Epoch 1991
Epoch 1991 :: Batch 0/1
Batch Loss = 0.02262121581484874
1991, epoch_train_loss=0.02262121581484874
Epoch 1992
Epoch 1992 :: Batch 0/1
Batch Loss = 0.022237975998130773
1992, epoch_train_loss=0.022237975998130773
Epoch 1993
Epoch 1993 :: Batch 0/1
Batch Loss = 0.02695666822908549
1993, epoch_train_loss=0.02695666822908549
Epoch 1994
Epoch 1994 :: Batch 0/1
Batch Loss = 0.015933733592032408
1994, epoch_train_loss=0.015933733592032408
Epoch 1995
Epoch 1995 :: Batch 0/1
Batch Loss = 0.008387311357912833
1995, epoch_train_loss=0.008387311357912833
Epoch 1996
Epoch 1996 :: Batch 0/1
Batch Loss = 0.0025300969000391776
1996, epoch_train_loss=0.0025300969000391776
Epoch 1997
Epoch 1997 :: Batch 0/1
Batch Loss = 0.003279943581410062
1997, epoch_train_loss=0.003279943581410062
Epoch 1998
Epoch 1998 :: Batch 0/1
Batch Loss = 0.008366939123423376
1998, epoch_train_loss=0.008366939123423376
Epoch 1999
Epoch 1999 :: Batch 0/1
Batch Loss = 0.01063766232220364
1999, epoch_train_loss=0.01063766232220364
Epoch 2000
Epoch 2000 :: Batch 0/1
Batch Loss = 0.0106735946139826
2000, epoch_train_loss=0.0106735946139826
Epoch 2001
Epoch 2001 :: Batch 0/1
Batch Loss = 0.0051179560104827534
2001, epoch_train_loss=0.0051179560104827534
Epoch 2002
Epoch 2002 :: Batch 0/1
Batch Loss = 0.002143219656450679
2002, epoch_train_loss=0.002143219656450679
Epoch 2003
Epoch 2003 :: Batch 0/1
Batch Loss = 0.0032425168671900853
2003, epoch_train_loss=0.0032425168671900853
Epoch 2004
Epoch 2004 :: Batch 0/1
Batch Loss = 0.005920036642710635
2004, epoch_train_loss=0.005920036642710635
Epoch 2005
Epoch 2005 :: Batch 0/1
Batch Loss = 0.007385487968992633
2005, epoch_train_loss=0.007385487968992633
Epoch 2006
Epoch 2006 :: Batch 0/1
Batch Loss = 0.004796080888979863
2006, epoch_train_loss=0.004796080888979863
Epoch 2007
Epoch 2007 :: Batch 0/1
Batch Loss = 0.002433036574773767
2007, epoch_train_loss=0.002433036574773767
Epoch 2008
Epoch 2008 :: Batch 0/1
Batch Loss = 0.0022555890115961534
2008, epoch_train_loss=0.0022555890115961534
Epoch 2009
Epoch 2009 :: Batch 0/1
Batch Loss = 0.0038435375117134626
2009, epoch_train_loss=0.0038435375117134626
Epoch 2010
Epoch 2010 :: Batch 0/1
Batch Loss = 0.004921085980760483
2010, epoch_train_loss=0.004921085980760483
Epoch 2011
Epoch 2011 :: Batch 0/1
Batch Loss = 0.0036514444959365325
2011, epoch_train_loss=0.0036514444959365325
Epoch 2012
Epoch 2012 :: Batch 0/1
Batch Loss = 0.002231654181300633
2012, epoch_train_loss=0.002231654181300633
Epoch 2013
Epoch 2013 :: Batch 0/1
Batch Loss = 0.0021624945188436105
2013, epoch_train_loss=0.0021624945188436105
Epoch 2014
Epoch 2014 :: Batch 0/1
Batch Loss = 0.0031532771953324845
2014, epoch_train_loss=0.0031532771953324845
Epoch 2015
Epoch 2015 :: Batch 0/1
Batch Loss = 0.0037008587363970137
2015, epoch_train_loss=0.0037008587363970137
Epoch 2016
Epoch 2016 :: Batch 0/1
Batch Loss = 0.002887745727818321
2016, epoch_train_loss=0.002887745727818321
Epoch 2017
Epoch 2017 :: Batch 0/1
Batch Loss = 0.002095201924235095
2017, epoch_train_loss=0.002095201924235095
Epoch 2018
Epoch 2018 :: Batch 0/1
Batch Loss = 0.0021651016179846305
2018, epoch_train_loss=0.0021651016179846305
Epoch 2019
Epoch 2019 :: Batch 0/1
Batch Loss = 0.0027291116862631225
2019, epoch_train_loss=0.0027291116862631225
Epoch 2020
Epoch 2020 :: Batch 0/1
Batch Loss = 0.002912963671640411
2020, epoch_train_loss=0.002912963671640411
Epoch 2021
Epoch 2021 :: Batch 0/1
Batch Loss = 0.0024350938709710506
2021, epoch_train_loss=0.0024350938709710506
Epoch 2022
Epoch 2022 :: Batch 0/1
Batch Loss = 0.0020457544713898164
2022, epoch_train_loss=0.0020457544713898164
Epoch 2023
Epoch 2023 :: Batch 0/1
Batch Loss = 0.002123303137392002
2023, epoch_train_loss=0.002123303137392002
Epoch 2024
Epoch 2024 :: Batch 0/1
Batch Loss = 0.0024085089081554607
2024, epoch_train_loss=0.0024085089081554607
Epoch 2025
Epoch 2025 :: Batch 0/1
Batch Loss = 0.0024781744154994093
2025, epoch_train_loss=0.0024781744154994093
Epoch 2026
Epoch 2026 :: Batch 0/1
Batch Loss = 0.002234171386401829
2026, epoch_train_loss=0.002234171386401829
Epoch 2027
Epoch 2027 :: Batch 0/1
Batch Loss = 0.0020365315189364128
2027, epoch_train_loss=0.0020365315189364128
Epoch 2028
Epoch 2028 :: Batch 0/1
Batch Loss = 0.0020521612267871037
2028, epoch_train_loss=0.0020521612267871037
Epoch 2029
Epoch 2029 :: Batch 0/1
Batch Loss = 0.002189925324746876
2029, epoch_train_loss=0.002189925324746876
Epoch 2030
Epoch 2030 :: Batch 0/1
Batch Loss = 0.0022353751737367944
2030, epoch_train_loss=0.0022353751737367944
Epoch 2031
Epoch 2031 :: Batch 0/1
Batch Loss = 0.0021292385899970814
2031, epoch_train_loss=0.0021292385899970814
Epoch 2032
Epoch 2032 :: Batch 0/1
Batch Loss = 0.002021231710600498
2032, epoch_train_loss=0.002021231710600498
Epoch 2033
Epoch 2033 :: Batch 0/1
Batch Loss = 0.001989023592044375
2033, epoch_train_loss=0.001989023592044375
Epoch 2034
Epoch 2034 :: Batch 0/1
Batch Loss = 0.0020344056505030355
2034, epoch_train_loss=0.0020344056505030355
Epoch 2035
Epoch 2035 :: Batch 0/1
Batch Loss = 0.0020824938476293846
2035, epoch_train_loss=0.0020824938476293846
Epoch 2036
Epoch 2036 :: Batch 0/1
Batch Loss = 0.0020715771639714265
2036, epoch_train_loss=0.0020715771639714265
Epoch 2037
Epoch 2037 :: Batch 0/1
Batch Loss = 0.002015948595047191
2037, epoch_train_loss=0.002015948595047191
Epoch 2038
Epoch 2038 :: Batch 0/1
Batch Loss = 0.0019531825439776406
2038, epoch_train_loss=0.0019531825439776406
Epoch 2039
Epoch 2039 :: Batch 0/1
Batch Loss = 0.0019456355039828546
2039, epoch_train_loss=0.0019456355039828546
Epoch 2040
Epoch 2040 :: Batch 0/1
Batch Loss = 0.001991643329555272
2040, epoch_train_loss=0.001991643329555272
Epoch 2041
Epoch 2041 :: Batch 0/1
Batch Loss = 0.0020251661425478547
2041, epoch_train_loss=0.0020251661425478547
Epoch 2042
Epoch 2042 :: Batch 0/1
Batch Loss = 0.0020029176807088924
2042, epoch_train_loss=0.0020029176807088924
Epoch 2043
Epoch 2043 :: Batch 0/1
Batch Loss = 0.0019414314111442735
2043, epoch_train_loss=0.0019414314111442735
Epoch 2044
Epoch 2044 :: Batch 0/1
Batch Loss = 0.0019138724658458075
2044, epoch_train_loss=0.0019138724658458075
Epoch 2045
Epoch 2045 :: Batch 0/1
Batch Loss = 0.0019377444571182269
2045, epoch_train_loss=0.0019377444571182269
Epoch 2046
Epoch 2046 :: Batch 0/1
Batch Loss = 0.001971439429173567
2046, epoch_train_loss=0.001971439429173567
Epoch 2047
Epoch 2047 :: Batch 0/1
Batch Loss = 0.001973240945129421
2047, epoch_train_loss=0.001973240945129421
Epoch 2048
Epoch 2048 :: Batch 0/1
Batch Loss = 0.0019426760206086285
2048, epoch_train_loss=0.0019426760206086285
Epoch 2049
Epoch 2049 :: Batch 0/1
Batch Loss = 0.0019175590410673586
2049, epoch_train_loss=0.0019175590410673586
Epoch 2050
Epoch 2050 :: Batch 0/1
Batch Loss = 0.0019158720520436108
2050, epoch_train_loss=0.0019158720520436108
Epoch 2051
Epoch 2051 :: Batch 0/1
Batch Loss = 0.0019264500194994265
2051, epoch_train_loss=0.0019264500194994265
Epoch 2052
Epoch 2052 :: Batch 0/1
Batch Loss = 0.0019342579058589562
2052, epoch_train_loss=0.0019342579058589562
Epoch 2053
Epoch 2053 :: Batch 0/1
Batch Loss = 0.0019329740725206647
2053, epoch_train_loss=0.0019329740725206647
Epoch 2054
Epoch 2054 :: Batch 0/1
Batch Loss = 0.001926480432050051
2054, epoch_train_loss=0.001926480432050051
Epoch 2055
Epoch 2055 :: Batch 0/1
Batch Loss = 0.0019171352218415331
2055, epoch_train_loss=0.0019171352218415331
Epoch 2056
Epoch 2056 :: Batch 0/1
Batch Loss = 0.0019086383926382028
2056, epoch_train_loss=0.0019086383926382028
Epoch 2057
Epoch 2057 :: Batch 0/1
Batch Loss = 0.0019068027760527463
2057, epoch_train_loss=0.0019068027760527463
Epoch 2058
Epoch 2058 :: Batch 0/1
Batch Loss = 0.0019127948729170143
2058, epoch_train_loss=0.0019127948729170143
Epoch 2059
Epoch 2059 :: Batch 0/1
Batch Loss = 0.0019194103209094425
2059, epoch_train_loss=0.0019194103209094425
Epoch 2060
Epoch 2060 :: Batch 0/1
Batch Loss = 0.0019180273679693975
2060, epoch_train_loss=0.0019180273679693975
Epoch 2061
Epoch 2061 :: Batch 0/1
Batch Loss = 0.0019095124892568266
2061, epoch_train_loss=0.0019095124892568266
Epoch 2062
Epoch 2062 :: Batch 0/1
Batch Loss = 0.0019016230332972413
2062, epoch_train_loss=0.0019016230332972413
Epoch 2063
Epoch 2063 :: Batch 0/1
Batch Loss = 0.0019005203036465868
2063, epoch_train_loss=0.0019005203036465868
Epoch 2064
Epoch 2064 :: Batch 0/1
Batch Loss = 0.001904389238368711
2064, epoch_train_loss=0.001904389238368711
Epoch 2065
Epoch 2065 :: Batch 0/1
Batch Loss = 0.0019075054166358134
2065, epoch_train_loss=0.0019075054166358134
Epoch 2066
Epoch 2066 :: Batch 0/1
Batch Loss = 0.0019070354144374684
2066, epoch_train_loss=0.0019070354144374684
Epoch 2067
Epoch 2067 :: Batch 0/1
Batch Loss = 0.0019042019337740278
2067, epoch_train_loss=0.0019042019337740278
Epoch 2068
Epoch 2068 :: Batch 0/1
Batch Loss = 0.001901325031818844
2068, epoch_train_loss=0.001901325031818844
Epoch 2069
Epoch 2069 :: Batch 0/1
Batch Loss = 0.0018993151793005026
2069, epoch_train_loss=0.0018993151793005026
Epoch 2070
Epoch 2070 :: Batch 0/1
Batch Loss = 0.0018982471272494395
2070, epoch_train_loss=0.0018982471272494395
Epoch 2071
Epoch 2071 :: Batch 0/1
Batch Loss = 0.0018982541843837355
2071, epoch_train_loss=0.0018982541843837355
Epoch 2072
Epoch 2072 :: Batch 0/1
Batch Loss = 0.0018993303467581886
2072, epoch_train_loss=0.0018993303467581886
Epoch 2073
Epoch 2073 :: Batch 0/1
Batch Loss = 0.0019003715997015255
2073, epoch_train_loss=0.0019003715997015255
Epoch 2074
Epoch 2074 :: Batch 0/1
Batch Loss = 0.0018999467294431227
2074, epoch_train_loss=0.0018999467294431227
Epoch 2075
Epoch 2075 :: Batch 0/1
Batch Loss = 0.0018979946269828604
2075, epoch_train_loss=0.0018979946269828604
Epoch 2076
Epoch 2076 :: Batch 0/1
Batch Loss = 0.001895783244727947
2076, epoch_train_loss=0.001895783244727947
Epoch 2077
Epoch 2077 :: Batch 0/1
Batch Loss = 0.0018947892489758027
2077, epoch_train_loss=0.0018947892489758027
Epoch 2078
Epoch 2078 :: Batch 0/1
Batch Loss = 0.0018951642335693097
2078, epoch_train_loss=0.0018951642335693097
Epoch 2079
Epoch 2079 :: Batch 0/1
Batch Loss = 0.0018958817224023425
2079, epoch_train_loss=0.0018958817224023425
Epoch 2080
Epoch 2080 :: Batch 0/1
Batch Loss = 0.0018960967411362597
2080, epoch_train_loss=0.0018960967411362597
Epoch 2081
Epoch 2081 :: Batch 0/1
Batch Loss = 0.0018956743318797296
2081, epoch_train_loss=0.0018956743318797296
Epoch 2082
Epoch 2082 :: Batch 0/1
Batch Loss = 0.0018949941707752378
2082, epoch_train_loss=0.0018949941707752378
Epoch 2083
Epoch 2083 :: Batch 0/1
Batch Loss = 0.001894302455641144
2083, epoch_train_loss=0.001894302455641144
Epoch 2084
Epoch 2084 :: Batch 0/1
Batch Loss = 0.0018936412766903286
2084, epoch_train_loss=0.0018936412766903286
Epoch 2085
Epoch 2085 :: Batch 0/1
Batch Loss = 0.001893076456107087
2085, epoch_train_loss=0.001893076456107087
Epoch 2086
Epoch 2086 :: Batch 0/1
Batch Loss = 0.00189278940117362
2086, epoch_train_loss=0.00189278940117362
Epoch 2087
Epoch 2087 :: Batch 0/1
Batch Loss = 0.0018928365917717917
2087, epoch_train_loss=0.0018928365917717917
Epoch 2088
Epoch 2088 :: Batch 0/1
Batch Loss = 0.0018930306464719326
2088, epoch_train_loss=0.0018930306464719326
Epoch 2089
Epoch 2089 :: Batch 0/1
Batch Loss = 0.0018930601415677133
2089, epoch_train_loss=0.0018930601415677133
Epoch 2090
Epoch 2090 :: Batch 0/1
Batch Loss = 0.0018927399457777554
2090, epoch_train_loss=0.0018927399457777554
Epoch 2091
Epoch 2091 :: Batch 0/1
Batch Loss = 0.0018921921987972356
2091, epoch_train_loss=0.0018921921987972356
Epoch 2092
Epoch 2092 :: Batch 0/1
Batch Loss = 0.0018916691689532035
2092, epoch_train_loss=0.0018916691689532035
Epoch 2093
Epoch 2093 :: Batch 0/1
Batch Loss = 0.0018913375406546375
2093, epoch_train_loss=0.0018913375406546375
Epoch 2094
Epoch 2094 :: Batch 0/1
Batch Loss = 0.0018911802592448095
2094, epoch_train_loss=0.0018911802592448095
Epoch 2095
Epoch 2095 :: Batch 0/1
Batch Loss = 0.0018910863954271799
2095, epoch_train_loss=0.0018910863954271799
Epoch 2096
Epoch 2096 :: Batch 0/1
Batch Loss = 0.0018909859395847262
2096, epoch_train_loss=0.0018909859395847262
Epoch 2097
Epoch 2097 :: Batch 0/1
Batch Loss = 0.0018908827527921996
2097, epoch_train_loss=0.0018908827527921996
Epoch 2098
Epoch 2098 :: Batch 0/1
Batch Loss = 0.0018907746742293779
2098, epoch_train_loss=0.0018907746742293779
Epoch 2099
Epoch 2099 :: Batch 0/1
Batch Loss = 0.00189063205773856
2099, epoch_train_loss=0.00189063205773856
Epoch 2100
Epoch 2100 :: Batch 0/1
Batch Loss = 0.0018904233052264392
2100, epoch_train_loss=0.0018904233052264392
Epoch 2101
Epoch 2101 :: Batch 0/1
Batch Loss = 0.0018901519278682348
2101, epoch_train_loss=0.0018901519278682348
Epoch 2102
Epoch 2102 :: Batch 0/1
Batch Loss = 0.0018898753756041544
2102, epoch_train_loss=0.0018898753756041544
Epoch 2103
Epoch 2103 :: Batch 0/1
Batch Loss = 0.0018896528501798641
2103, epoch_train_loss=0.0018896528501798641
Epoch 2104
Epoch 2104 :: Batch 0/1
Batch Loss = 0.0018895069218309685
2104, epoch_train_loss=0.0018895069218309685
Epoch 2105
Epoch 2105 :: Batch 0/1
Batch Loss = 0.0018894138207703063
2105, epoch_train_loss=0.0018894138207703063
Epoch 2106
Epoch 2106 :: Batch 0/1
Batch Loss = 0.0018893268312474988
2106, epoch_train_loss=0.0018893268312474988
Epoch 2107
Epoch 2107 :: Batch 0/1
Batch Loss = 0.00188921877113986
2107, epoch_train_loss=0.00188921877113986
Epoch 2108
Epoch 2108 :: Batch 0/1
Batch Loss = 0.0018890915879660748
2108, epoch_train_loss=0.0018890915879660748
Epoch 2109
Epoch 2109 :: Batch 0/1
Batch Loss = 0.0018889567538413563
2109, epoch_train_loss=0.0018889567538413563
Epoch 2110
Epoch 2110 :: Batch 0/1
Batch Loss = 0.0018888194027466558
2110, epoch_train_loss=0.0018888194027466558
Epoch 2111
Epoch 2111 :: Batch 0/1
Batch Loss = 0.0018886747658745898
2111, epoch_train_loss=0.0018886747658745898
Epoch 2112
Epoch 2112 :: Batch 0/1
Batch Loss = 0.0018885185120999014
2112, epoch_train_loss=0.0018885185120999014
Epoch 2113
Epoch 2113 :: Batch 0/1
Batch Loss = 0.0018883569208009991
2113, epoch_train_loss=0.0018883569208009991
Epoch 2114
Epoch 2114 :: Batch 0/1
Batch Loss = 0.0018882022787506418
2114, epoch_train_loss=0.0018882022787506418
Epoch 2115
Epoch 2115 :: Batch 0/1
Batch Loss = 0.001888065705251907
2115, epoch_train_loss=0.001888065705251907
Epoch 2116
Epoch 2116 :: Batch 0/1
Batch Loss = 0.0018879498107572331
2116, epoch_train_loss=0.0018879498107572331
Epoch 2117
Epoch 2117 :: Batch 0/1
Batch Loss = 0.0018878462242470815
2117, epoch_train_loss=0.0018878462242470815
Epoch 2118
Epoch 2118 :: Batch 0/1
Batch Loss = 0.0018877446287302454
2118, epoch_train_loss=0.0018877446287302454
Epoch 2119
Epoch 2119 :: Batch 0/1
Batch Loss = 0.0018876395609586654
2119, epoch_train_loss=0.0018876395609586654
Epoch 2120
Epoch 2120 :: Batch 0/1
Batch Loss = 0.0018875313729239702
2120, epoch_train_loss=0.0018875313729239702
Epoch 2121
Epoch 2121 :: Batch 0/1
Batch Loss = 0.0018874234150337268
2121, epoch_train_loss=0.0018874234150337268
Epoch 2122
Epoch 2122 :: Batch 0/1
Batch Loss = 0.0018873168243070562
2122, epoch_train_loss=0.0018873168243070562
Epoch 2123
Epoch 2123 :: Batch 0/1
Batch Loss = 0.001887210109277734
2123, epoch_train_loss=0.001887210109277734
Epoch 2124
Epoch 2124 :: Batch 0/1
Batch Loss = 0.001887101296421404
2124, epoch_train_loss=0.001887101296421404
Epoch 2125
Epoch 2125 :: Batch 0/1
Batch Loss = 0.001886989010530401
2125, epoch_train_loss=0.001886989010530401
Epoch 2126
Epoch 2126 :: Batch 0/1
Batch Loss = 0.0018868748964581726
2126, epoch_train_loss=0.0018868748964581726
Epoch 2127
Epoch 2127 :: Batch 0/1
Batch Loss = 0.001886762064218622
2127, epoch_train_loss=0.001886762064218622
Epoch 2128
Epoch 2128 :: Batch 0/1
Batch Loss = 0.0018866529141587758
2128, epoch_train_loss=0.0018866529141587758
Epoch 2129
Epoch 2129 :: Batch 0/1
Batch Loss = 0.001886547962587041
2129, epoch_train_loss=0.001886547962587041
Epoch 2130
Epoch 2130 :: Batch 0/1
Batch Loss = 0.0018864459849206095
2130, epoch_train_loss=0.0018864459849206095
Epoch 2131
Epoch 2131 :: Batch 0/1
Batch Loss = 0.0018863455102333163
2131, epoch_train_loss=0.0018863455102333163
Epoch 2132
Epoch 2132 :: Batch 0/1
Batch Loss = 0.0018862460098640946
2132, epoch_train_loss=0.0018862460098640946
Epoch 2133
Epoch 2133 :: Batch 0/1
Batch Loss = 0.0018861476296465126
2133, epoch_train_loss=0.0018861476296465126
Epoch 2134
Epoch 2134 :: Batch 0/1
Batch Loss = 0.0018860511559669962
2134, epoch_train_loss=0.0018860511559669962
Epoch 2135
Epoch 2135 :: Batch 0/1
Batch Loss = 0.001885957147739149
2135, epoch_train_loss=0.001885957147739149
Epoch 2136
Epoch 2136 :: Batch 0/1
Batch Loss = 0.001885865346536862
2136, epoch_train_loss=0.001885865346536862
Epoch 2137
Epoch 2137 :: Batch 0/1
Batch Loss = 0.0018857750210884592
2137, epoch_train_loss=0.0018857750210884592
Epoch 2138
Epoch 2138 :: Batch 0/1
Batch Loss = 0.001885685429939378
2138, epoch_train_loss=0.001885685429939378
Epoch 2139
Epoch 2139 :: Batch 0/1
Batch Loss = 0.001885596258981081
2139, epoch_train_loss=0.001885596258981081
Epoch 2140
Epoch 2140 :: Batch 0/1
Batch Loss = 0.0018855077282597568
2140, epoch_train_loss=0.0018855077282597568
Epoch 2141
Epoch 2141 :: Batch 0/1
Batch Loss = 0.0018854201906016394
2141, epoch_train_loss=0.0018854201906016394
Epoch 2142
Epoch 2142 :: Batch 0/1
Batch Loss = 0.0018853340210631037
2142, epoch_train_loss=0.0018853340210631037
Epoch 2143
Epoch 2143 :: Batch 0/1
Batch Loss = 0.0018852494172670864
2143, epoch_train_loss=0.0018852494172670864
Epoch 2144
Epoch 2144 :: Batch 0/1
Batch Loss = 0.0018851662060814641
2144, epoch_train_loss=0.0018851662060814641
Epoch 2145
Epoch 2145 :: Batch 0/1
Batch Loss = 0.0018850843045816925
2145, epoch_train_loss=0.0018850843045816925
Epoch 2146
Epoch 2146 :: Batch 0/1
Batch Loss = 0.001885003666381756
2146, epoch_train_loss=0.001885003666381756
Epoch 2147
Epoch 2147 :: Batch 0/1
Batch Loss = 0.0018849246406228064
2147, epoch_train_loss=0.0018849246406228064
Epoch 2148
Epoch 2148 :: Batch 0/1
Batch Loss = 0.0018848476846695117
2148, epoch_train_loss=0.0018848476846695117
Epoch 2149
Epoch 2149 :: Batch 0/1
Batch Loss = 0.0018847735803727262
2149, epoch_train_loss=0.0018847735803727262
Epoch 2150
Epoch 2150 :: Batch 0/1
Batch Loss = 0.0018847031360270023
2150, epoch_train_loss=0.0018847031360270023
Epoch 2151
Epoch 2151 :: Batch 0/1
Batch Loss = 0.001884637611566414
2151, epoch_train_loss=0.001884637611566414
Epoch 2152
Epoch 2152 :: Batch 0/1
Batch Loss = 0.0018845783868312795
2152, epoch_train_loss=0.0018845783868312795
Epoch 2153
Epoch 2153 :: Batch 0/1
Batch Loss = 0.0018845279578719126
2153, epoch_train_loss=0.0018845279578719126
Epoch 2154
Epoch 2154 :: Batch 0/1
Batch Loss = 0.001884489514960156
2154, epoch_train_loss=0.001884489514960156
Epoch 2155
Epoch 2155 :: Batch 0/1
Batch Loss = 0.0018844686802833752
2155, epoch_train_loss=0.0018844686802833752
Epoch 2156
Epoch 2156 :: Batch 0/1
Batch Loss = 0.0018844727567106656
2156, epoch_train_loss=0.0018844727567106656
Epoch 2157
Epoch 2157 :: Batch 0/1
Batch Loss = 0.001884514504988978
2157, epoch_train_loss=0.001884514504988978
Epoch 2158
Epoch 2158 :: Batch 0/1
Batch Loss = 0.00188461055014232
2158, epoch_train_loss=0.00188461055014232
Epoch 2159
Epoch 2159 :: Batch 0/1
Batch Loss = 0.00188479077333422
2159, epoch_train_loss=0.00188479077333422
Epoch 2160
Epoch 2160 :: Batch 0/1
Batch Loss = 0.001885093945807371
2160, epoch_train_loss=0.001885093945807371
Epoch 2161
Epoch 2161 :: Batch 0/1
Batch Loss = 0.0018855930517213476
2161, epoch_train_loss=0.0018855930517213476
Epoch 2162
Epoch 2162 :: Batch 0/1
Batch Loss = 0.0018863810490539867
2162, epoch_train_loss=0.0018863810490539867
Epoch 2163
Epoch 2163 :: Batch 0/1
Batch Loss = 0.0018876437943862662
2163, epoch_train_loss=0.0018876437943862662
Epoch 2164
Epoch 2164 :: Batch 0/1
Batch Loss = 0.0018896083720186133
2164, epoch_train_loss=0.0018896083720186133
Epoch 2165
Epoch 2165 :: Batch 0/1
Batch Loss = 0.0018927684045305058
2165, epoch_train_loss=0.0018927684045305058
Epoch 2166
Epoch 2166 :: Batch 0/1
Batch Loss = 0.0018976840448020687
2166, epoch_train_loss=0.0018976840448020687
Epoch 2167
Epoch 2167 :: Batch 0/1
Batch Loss = 0.001905726862682255
2167, epoch_train_loss=0.001905726862682255
Epoch 2168
Epoch 2168 :: Batch 0/1
Batch Loss = 0.0019182702959677617
2168, epoch_train_loss=0.0019182702959677617
Epoch 2169
Epoch 2169 :: Batch 0/1
Batch Loss = 0.0019393198129943907
2169, epoch_train_loss=0.0019393198129943907
Epoch 2170
Epoch 2170 :: Batch 0/1
Batch Loss = 0.0019721062756023187
2170, epoch_train_loss=0.0019721062756023187
Epoch 2171
Epoch 2171 :: Batch 0/1
Batch Loss = 0.0020289776509792995
2171, epoch_train_loss=0.0020289776509792995
Epoch 2172
Epoch 2172 :: Batch 0/1
Batch Loss = 0.0021165235400312107
2172, epoch_train_loss=0.0021165235400312107
Epoch 2173
Epoch 2173 :: Batch 0/1
Batch Loss = 0.002275048031638437
2173, epoch_train_loss=0.002275048031638437
Epoch 2174
Epoch 2174 :: Batch 0/1
Batch Loss = 0.002511241353294247
2174, epoch_train_loss=0.002511241353294247
Epoch 2175
Epoch 2175 :: Batch 0/1
Batch Loss = 0.002963896609665061
2175, epoch_train_loss=0.002963896609665061
Epoch 2176
Epoch 2176 :: Batch 0/1
Batch Loss = 0.0035896190872293935
2176, epoch_train_loss=0.0035896190872293935
Epoch 2177
Epoch 2177 :: Batch 0/1
Batch Loss = 0.004881927868685866
2177, epoch_train_loss=0.004881927868685866
Epoch 2178
Epoch 2178 :: Batch 0/1
Batch Loss = 0.006387677073594115
2178, epoch_train_loss=0.006387677073594115
Epoch 2179
Epoch 2179 :: Batch 0/1
Batch Loss = 0.009811542220067955
2179, epoch_train_loss=0.009811542220067955
Epoch 2180
Epoch 2180 :: Batch 0/1
Batch Loss = 0.012371503705133664
2180, epoch_train_loss=0.012371503705133664
Epoch 2181
Epoch 2181 :: Batch 0/1
Batch Loss = 0.019105767321212816
2181, epoch_train_loss=0.019105767321212816
Epoch 2182
Epoch 2182 :: Batch 0/1
Batch Loss = 0.01911548766941648
2182, epoch_train_loss=0.01911548766941648
Epoch 2183
Epoch 2183 :: Batch 0/1
Batch Loss = 0.023416290557618864
2183, epoch_train_loss=0.023416290557618864
Epoch 2184
Epoch 2184 :: Batch 0/1
Batch Loss = 0.015010234314296383
2184, epoch_train_loss=0.015010234314296383
Epoch 2185
Epoch 2185 :: Batch 0/1
Batch Loss = 0.009272078905512397
2185, epoch_train_loss=0.009272078905512397
Epoch 2186
Epoch 2186 :: Batch 0/1
Batch Loss = 0.003222988519784696
2186, epoch_train_loss=0.003222988519784696
Epoch 2187
Epoch 2187 :: Batch 0/1
Batch Loss = 0.00230730306687336
2187, epoch_train_loss=0.00230730306687336
Epoch 2188
Epoch 2188 :: Batch 0/1
Batch Loss = 0.005492831186993704
2188, epoch_train_loss=0.005492831186993704
Epoch 2189
Epoch 2189 :: Batch 0/1
Batch Loss = 0.008483476565026953
2189, epoch_train_loss=0.008483476565026953
Epoch 2190
Epoch 2190 :: Batch 0/1
Batch Loss = 0.010337094122638175
2190, epoch_train_loss=0.010337094122638175
Epoch 2191
Epoch 2191 :: Batch 0/1
Batch Loss = 0.006542947775223238
2191, epoch_train_loss=0.006542947775223238
Epoch 2192
Epoch 2192 :: Batch 0/1
Batch Loss = 0.003214994365753739
2192, epoch_train_loss=0.003214994365753739
Epoch 2193
Epoch 2193 :: Batch 0/1
Batch Loss = 0.0020236104994008487
2193, epoch_train_loss=0.0020236104994008487
Epoch 2194
Epoch 2194 :: Batch 0/1
Batch Loss = 0.0036132604597672287
2194, epoch_train_loss=0.0036132604597672287
Epoch 2195
Epoch 2195 :: Batch 0/1
Batch Loss = 0.0060175491927090975
2195, epoch_train_loss=0.0060175491927090975
Epoch 2196
Epoch 2196 :: Batch 0/1
Batch Loss = 0.00585320723771457
2196, epoch_train_loss=0.00585320723771457
Epoch 2197
Epoch 2197 :: Batch 0/1
Batch Loss = 0.0044031789486099265
2197, epoch_train_loss=0.0044031789486099265
Epoch 2198
Epoch 2198 :: Batch 0/1
Batch Loss = 0.0025316347604626152
2198, epoch_train_loss=0.0025316347604626152
Epoch 2199
Epoch 2199 :: Batch 0/1
Batch Loss = 0.0023740604286118746
2199, epoch_train_loss=0.0023740604286118746
Epoch 2200
Epoch 2200 :: Batch 0/1
Batch Loss = 0.0036473525984594105
2200, epoch_train_loss=0.0036473525984594105
Epoch 2201
Epoch 2201 :: Batch 0/1
Batch Loss = 0.004241339803619333
2201, epoch_train_loss=0.004241339803619333
Epoch 2202
Epoch 2202 :: Batch 0/1
Batch Loss = 0.003809348842007846
2202, epoch_train_loss=0.003809348842007846
Epoch 2203
Epoch 2203 :: Batch 0/1
Batch Loss = 0.0024722052513528807
2203, epoch_train_loss=0.0024722052513528807
Epoch 2204
Epoch 2204 :: Batch 0/1
Batch Loss = 0.0019608351915404157
2204, epoch_train_loss=0.0019608351915404157
Epoch 2205
Epoch 2205 :: Batch 0/1
Batch Loss = 0.00252244819220637
2205, epoch_train_loss=0.00252244819220637
Epoch 2206
Epoch 2206 :: Batch 0/1
Batch Loss = 0.0031748220606948574
2206, epoch_train_loss=0.0031748220606948574
Epoch 2207
Epoch 2207 :: Batch 0/1
Batch Loss = 0.0032200217688394452
2207, epoch_train_loss=0.0032200217688394452
Epoch 2208
Epoch 2208 :: Batch 0/1
Batch Loss = 0.0024742887655364726
2208, epoch_train_loss=0.0024742887655364726
Epoch 2209
Epoch 2209 :: Batch 0/1
Batch Loss = 0.0019620460698524926
2209, epoch_train_loss=0.0019620460698524926
Epoch 2210
Epoch 2210 :: Batch 0/1
Batch Loss = 0.0021090772161478084
2210, epoch_train_loss=0.0021090772161478084
Epoch 2211
Epoch 2211 :: Batch 0/1
Batch Loss = 0.0025643371661562167
2211, epoch_train_loss=0.0025643371661562167
Epoch 2212
Epoch 2212 :: Batch 0/1
Batch Loss = 0.0027677955255422993
2212, epoch_train_loss=0.0027677955255422993
Epoch 2213
Epoch 2213 :: Batch 0/1
Batch Loss = 0.0024296732735433913
2213, epoch_train_loss=0.0024296732735433913
Epoch 2214
Epoch 2214 :: Batch 0/1
Batch Loss = 0.0020386700143522752
2214, epoch_train_loss=0.0020386700143522752
Epoch 2215
Epoch 2215 :: Batch 0/1
Batch Loss = 0.0019461086138897153
2215, epoch_train_loss=0.0019461086138897153
Epoch 2216
Epoch 2216 :: Batch 0/1
Batch Loss = 0.0021668208273393753
2216, epoch_train_loss=0.0021668208273393753
Epoch 2217
Epoch 2217 :: Batch 0/1
Batch Loss = 0.0023858149238349087
2217, epoch_train_loss=0.0023858149238349087
Epoch 2218
Epoch 2218 :: Batch 0/1
Batch Loss = 0.0023124057742897686
2218, epoch_train_loss=0.0023124057742897686
Epoch 2219
Epoch 2219 :: Batch 0/1
Batch Loss = 0.002089292558207757
2219, epoch_train_loss=0.002089292558207757
Epoch 2220
Epoch 2220 :: Batch 0/1
Batch Loss = 0.0019280527786858233
2220, epoch_train_loss=0.0019280527786858233
Epoch 2221
Epoch 2221 :: Batch 0/1
Batch Loss = 0.0019710991332558014
2221, epoch_train_loss=0.0019710991332558014
Epoch 2222
Epoch 2222 :: Batch 0/1
Batch Loss = 0.0021223267591842527
2222, epoch_train_loss=0.0021223267591842527
Epoch 2223
Epoch 2223 :: Batch 0/1
Batch Loss = 0.0021772567151094563
2223, epoch_train_loss=0.0021772567151094563
Epoch 2224
Epoch 2224 :: Batch 0/1
Batch Loss = 0.002096221640618597
2224, epoch_train_loss=0.002096221640618597
Epoch 2225
Epoch 2225 :: Batch 0/1
Batch Loss = 0.0019615788315387687
2225, epoch_train_loss=0.0019615788315387687
Epoch 2226
Epoch 2226 :: Batch 0/1
Batch Loss = 0.001911333748167616
2226, epoch_train_loss=0.001911333748167616
Epoch 2227
Epoch 2227 :: Batch 0/1
Batch Loss = 0.0019663891364023536
2227, epoch_train_loss=0.0019663891364023536
Epoch 2228
Epoch 2228 :: Batch 0/1
Batch Loss = 0.0020405537980009124
2228, epoch_train_loss=0.0020405537980009124
Epoch 2229
Epoch 2229 :: Batch 0/1
Batch Loss = 0.002054005975593572
2229, epoch_train_loss=0.002054005975593572
Epoch 2230
Epoch 2230 :: Batch 0/1
Batch Loss = 0.0019941133800627326
2230, epoch_train_loss=0.0019941133800627326
Epoch 2231
Epoch 2231 :: Batch 0/1
Batch Loss = 0.00192950090531106
2231, epoch_train_loss=0.00192950090531106
Epoch 2232
Epoch 2232 :: Batch 0/1
Batch Loss = 0.0019085496513736495
2232, epoch_train_loss=0.0019085496513736495
Epoch 2233
Epoch 2233 :: Batch 0/1
Batch Loss = 0.0019355991233165053
2233, epoch_train_loss=0.0019355991233165053
Epoch 2234
Epoch 2234 :: Batch 0/1
Batch Loss = 0.001972518309750125
2234, epoch_train_loss=0.001972518309750125
Epoch 2235
Epoch 2235 :: Batch 0/1
Batch Loss = 0.00197897127464854
2235, epoch_train_loss=0.00197897127464854
Epoch 2236
Epoch 2236 :: Batch 0/1
Batch Loss = 0.001955585122080402
2236, epoch_train_loss=0.001955585122080402
Epoch 2237
Epoch 2237 :: Batch 0/1
Batch Loss = 0.0019208881535291264
2237, epoch_train_loss=0.0019208881535291264
Epoch 2238
Epoch 2238 :: Batch 0/1
Batch Loss = 0.0019023303083864091
2238, epoch_train_loss=0.0019023303083864091
Epoch 2239
Epoch 2239 :: Batch 0/1
Batch Loss = 0.0019085308118448065
2239, epoch_train_loss=0.0019085308118448065
Epoch 2240
Epoch 2240 :: Batch 0/1
Batch Loss = 0.0019261686795851132
2240, epoch_train_loss=0.0019261686795851132
Epoch 2241
Epoch 2241 :: Batch 0/1
Batch Loss = 0.0019377722081427514
2241, epoch_train_loss=0.0019377722081427514
Epoch 2242
Epoch 2242 :: Batch 0/1
Batch Loss = 0.0019328758236000746
2242, epoch_train_loss=0.0019328758236000746
Epoch 2243
Epoch 2243 :: Batch 0/1
Batch Loss = 0.0019162986132380151
2243, epoch_train_loss=0.0019162986132380151
Epoch 2244
Epoch 2244 :: Batch 0/1
Batch Loss = 0.001899924326876266
2244, epoch_train_loss=0.001899924326876266
Epoch 2245
Epoch 2245 :: Batch 0/1
Batch Loss = 0.0018947219850557109
2245, epoch_train_loss=0.0018947219850557109
Epoch 2246
Epoch 2246 :: Batch 0/1
Batch Loss = 0.0019009917169151367
2246, epoch_train_loss=0.0019009917169151367
Epoch 2247
Epoch 2247 :: Batch 0/1
Batch Loss = 0.0019109987093950227
2247, epoch_train_loss=0.0019109987093950227
Epoch 2248
Epoch 2248 :: Batch 0/1
Batch Loss = 0.001915441757805652
2248, epoch_train_loss=0.001915441757805652
Epoch 2249
Epoch 2249 :: Batch 0/1
Batch Loss = 0.00191009711246666
2249, epoch_train_loss=0.00191009711246666
Epoch 2250
Epoch 2250 :: Batch 0/1
Batch Loss = 0.0019001791338354558
2250, epoch_train_loss=0.0019001791338354558
Epoch 2251
Epoch 2251 :: Batch 0/1
Batch Loss = 0.0018922997275315327
2251, epoch_train_loss=0.0018922997275315327
Epoch 2252
Epoch 2252 :: Batch 0/1
Batch Loss = 0.0018906423269881047
2252, epoch_train_loss=0.0018906423269881047
Epoch 2253
Epoch 2253 :: Batch 0/1
Batch Loss = 0.0018944201297829643
2253, epoch_train_loss=0.0018944201297829643
Epoch 2254
Epoch 2254 :: Batch 0/1
Batch Loss = 0.0018990346816085504
2254, epoch_train_loss=0.0018990346816085504
Epoch 2255
Epoch 2255 :: Batch 0/1
Batch Loss = 0.0019006971990389028
2255, epoch_train_loss=0.0019006971990389028
Epoch 2256
Epoch 2256 :: Batch 0/1
Batch Loss = 0.0018983462153845689
2256, epoch_train_loss=0.0018983462153845689
Epoch 2257
Epoch 2257 :: Batch 0/1
Batch Loss = 0.0018938624754793634
2257, epoch_train_loss=0.0018938624754793634
Epoch 2258
Epoch 2258 :: Batch 0/1
Batch Loss = 0.0018899571133266656
2258, epoch_train_loss=0.0018899571133266656
Epoch 2259
Epoch 2259 :: Batch 0/1
Batch Loss = 0.0018883425501652967
2259, epoch_train_loss=0.0018883425501652967
Epoch 2260
Epoch 2260 :: Batch 0/1
Batch Loss = 0.0018888563132839814
2260, epoch_train_loss=0.0018888563132839814
Epoch 2261
Epoch 2261 :: Batch 0/1
Batch Loss = 0.0018904948710336273
2261, epoch_train_loss=0.0018904948710336273
Epoch 2262
Epoch 2262 :: Batch 0/1
Batch Loss = 0.00189186963258493
2262, epoch_train_loss=0.00189186963258493
Epoch 2263
Epoch 2263 :: Batch 0/1
Batch Loss = 0.0018919535980352392
2263, epoch_train_loss=0.0018919535980352392
Epoch 2264
Epoch 2264 :: Batch 0/1
Batch Loss = 0.0018907386377009887
2264, epoch_train_loss=0.0018907386377009887
Epoch 2265
Epoch 2265 :: Batch 0/1
Batch Loss = 0.001888684338350637
2265, epoch_train_loss=0.001888684338350637
Epoch 2266
Epoch 2266 :: Batch 0/1
Batch Loss = 0.0018867940895357983
2266, epoch_train_loss=0.0018867940895357983
Epoch 2267
Epoch 2267 :: Batch 0/1
Batch Loss = 0.0018858168891465864
2267, epoch_train_loss=0.0018858168891465864
Epoch 2268
Epoch 2268 :: Batch 0/1
Batch Loss = 0.0018858654120332797
2268, epoch_train_loss=0.0018858654120332797
Epoch 2269
Epoch 2269 :: Batch 0/1
Batch Loss = 0.001886586156080286
2269, epoch_train_loss=0.001886586156080286
Epoch 2270
Epoch 2270 :: Batch 0/1
Batch Loss = 0.0018872976493063255
2270, epoch_train_loss=0.0018872976493063255
Epoch 2271
Epoch 2271 :: Batch 0/1
Batch Loss = 0.0018874334767576492
2271, epoch_train_loss=0.0018874334767576492
Epoch 2272
Epoch 2272 :: Batch 0/1
Batch Loss = 0.001886877780688241
2272, epoch_train_loss=0.001886877780688241
Epoch 2273
Epoch 2273 :: Batch 0/1
Batch Loss = 0.0018859070579444576
2273, epoch_train_loss=0.0018859070579444576
Epoch 2274
Epoch 2274 :: Batch 0/1
Batch Loss = 0.0018849185293104112
2274, epoch_train_loss=0.0018849185293104112
Epoch 2275
Epoch 2275 :: Batch 0/1
Batch Loss = 0.0018842546619636448
2275, epoch_train_loss=0.0018842546619636448
Epoch 2276
Epoch 2276 :: Batch 0/1
Batch Loss = 0.001883980004881481
2276, epoch_train_loss=0.001883980004881481
Epoch 2277
Epoch 2277 :: Batch 0/1
Batch Loss = 0.0018840136885075662
2277, epoch_train_loss=0.0018840136885075662
Epoch 2278
Epoch 2278 :: Batch 0/1
Batch Loss = 0.001884182730543219
2278, epoch_train_loss=0.001884182730543219
Epoch 2279
Epoch 2279 :: Batch 0/1
Batch Loss = 0.001884291127596673
2279, epoch_train_loss=0.001884291127596673
Epoch 2280
Epoch 2280 :: Batch 0/1
Batch Loss = 0.0018842554083171273
2280, epoch_train_loss=0.0018842554083171273
Epoch 2281
Epoch 2281 :: Batch 0/1
Batch Loss = 0.00188403939021247
2281, epoch_train_loss=0.00188403939021247
Epoch 2282
Epoch 2282 :: Batch 0/1
Batch Loss = 0.001883675799566287
2282, epoch_train_loss=0.001883675799566287
Epoch 2283
Epoch 2283 :: Batch 0/1
Batch Loss = 0.0018832436593024702
2283, epoch_train_loss=0.0018832436593024702
Epoch 2284
Epoch 2284 :: Batch 0/1
Batch Loss = 0.0018828292916440678
2284, epoch_train_loss=0.0018828292916440678
Epoch 2285
Epoch 2285 :: Batch 0/1
Batch Loss = 0.001882513277387541
2285, epoch_train_loss=0.001882513277387541
Epoch 2286
Epoch 2286 :: Batch 0/1
Batch Loss = 0.001882334859641002
2286, epoch_train_loss=0.001882334859641002
Epoch 2287
Epoch 2287 :: Batch 0/1
Batch Loss = 0.0018822665849023154
2287, epoch_train_loss=0.0018822665849023154
Epoch 2288
Epoch 2288 :: Batch 0/1
Batch Loss = 0.0018822606575292498
2288, epoch_train_loss=0.0018822606575292498
Epoch 2289
Epoch 2289 :: Batch 0/1
Batch Loss = 0.0018822541669760776
2289, epoch_train_loss=0.0018822541669760776
Epoch 2290
Epoch 2290 :: Batch 0/1
Batch Loss = 0.001882194294503155
2290, epoch_train_loss=0.001882194294503155
Epoch 2291
Epoch 2291 :: Batch 0/1
Batch Loss = 0.0018820708604205649
2291, epoch_train_loss=0.0018820708604205649
Epoch 2292
Epoch 2292 :: Batch 0/1
Batch Loss = 0.0018818906347086155
2292, epoch_train_loss=0.0018818906347086155
Epoch 2293
Epoch 2293 :: Batch 0/1
Batch Loss = 0.0018816815630297475
2293, epoch_train_loss=0.0018816815630297475
Epoch 2294
Epoch 2294 :: Batch 0/1
Batch Loss = 0.001881469339703248
2294, epoch_train_loss=0.001881469339703248
Epoch 2295
Epoch 2295 :: Batch 0/1
Batch Loss = 0.001881268809168632
2295, epoch_train_loss=0.001881268809168632
Epoch 2296
Epoch 2296 :: Batch 0/1
Batch Loss = 0.0018810926057111353
2296, epoch_train_loss=0.0018810926057111353
Epoch 2297
Epoch 2297 :: Batch 0/1
Batch Loss = 0.0018809450279431044
2297, epoch_train_loss=0.0018809450279431044
Epoch 2298
Epoch 2298 :: Batch 0/1
Batch Loss = 0.0018808230318703768
2298, epoch_train_loss=0.0018808230318703768
Epoch 2299
Epoch 2299 :: Batch 0/1
Batch Loss = 0.0018807253074992617
2299, epoch_train_loss=0.0018807253074992617
Epoch 2300
Epoch 2300 :: Batch 0/1
Batch Loss = 0.0018806438582292487
2300, epoch_train_loss=0.0018806438582292487
Epoch 2301
Epoch 2301 :: Batch 0/1
Batch Loss = 0.0018805692062219424
2301, epoch_train_loss=0.0018805692062219424
Epoch 2302
Epoch 2302 :: Batch 0/1
Batch Loss = 0.001880492777280987
2302, epoch_train_loss=0.001880492777280987
Epoch 2303
Epoch 2303 :: Batch 0/1
Batch Loss = 0.0018804057414845625
2303, epoch_train_loss=0.0018804057414845625
Epoch 2304
Epoch 2304 :: Batch 0/1
Batch Loss = 0.0018803073571171827
2304, epoch_train_loss=0.0018803073571171827
Epoch 2305
Epoch 2305 :: Batch 0/1
Batch Loss = 0.0018801981858812629
2305, epoch_train_loss=0.0018801981858812629
Epoch 2306
Epoch 2306 :: Batch 0/1
Batch Loss = 0.0018800814395381264
2306, epoch_train_loss=0.0018800814395381264
Epoch 2307
Epoch 2307 :: Batch 0/1
Batch Loss = 0.0018799621530787305
2307, epoch_train_loss=0.0018799621530787305
Epoch 2308
Epoch 2308 :: Batch 0/1
Batch Loss = 0.0018798434305456787
2308, epoch_train_loss=0.0018798434305456787
Epoch 2309
Epoch 2309 :: Batch 0/1
Batch Loss = 0.0018797267258585113
2309, epoch_train_loss=0.0018797267258585113
Epoch 2310
Epoch 2310 :: Batch 0/1
Batch Loss = 0.0018796133551238678
2310, epoch_train_loss=0.0018796133551238678
Epoch 2311
Epoch 2311 :: Batch 0/1
Batch Loss = 0.0018795029704668361
2311, epoch_train_loss=0.0018795029704668361
Epoch 2312
Epoch 2312 :: Batch 0/1
Batch Loss = 0.0018793966604485114
2312, epoch_train_loss=0.0018793966604485114
Epoch 2313
Epoch 2313 :: Batch 0/1
Batch Loss = 0.0018792945223995885
2313, epoch_train_loss=0.0018792945223995885
Epoch 2314
Epoch 2314 :: Batch 0/1
Batch Loss = 0.001879196529329784
2314, epoch_train_loss=0.001879196529329784
Epoch 2315
Epoch 2315 :: Batch 0/1
Batch Loss = 0.0018791031130691422
2315, epoch_train_loss=0.0018791031130691422
Epoch 2316
Epoch 2316 :: Batch 0/1
Batch Loss = 0.0018790133488830972
2316, epoch_train_loss=0.0018790133488830972
Epoch 2317
Epoch 2317 :: Batch 0/1
Batch Loss = 0.0018789263520417108
2317, epoch_train_loss=0.0018789263520417108
Epoch 2318
Epoch 2318 :: Batch 0/1
Batch Loss = 0.0018788414153431605
2318, epoch_train_loss=0.0018788414153431605
Epoch 2319
Epoch 2319 :: Batch 0/1
Batch Loss = 0.0018787577300065064
2319, epoch_train_loss=0.0018787577300065064
Epoch 2320
Epoch 2320 :: Batch 0/1
Batch Loss = 0.0018786752495676827
2320, epoch_train_loss=0.0018786752495676827
Epoch 2321
Epoch 2321 :: Batch 0/1
Batch Loss = 0.0018785938547412845
2321, epoch_train_loss=0.0018785938547412845
Epoch 2322
Epoch 2322 :: Batch 0/1
Batch Loss = 0.0018785136451545737
2322, epoch_train_loss=0.0018785136451545737
Epoch 2323
Epoch 2323 :: Batch 0/1
Batch Loss = 0.0018784350906561852
2323, epoch_train_loss=0.0018784350906561852
Epoch 2324
Epoch 2324 :: Batch 0/1
Batch Loss = 0.0018783580497529464
2324, epoch_train_loss=0.0018783580497529464
Epoch 2325
Epoch 2325 :: Batch 0/1
Batch Loss = 0.0018782827602805883
2325, epoch_train_loss=0.0018782827602805883
Epoch 2326
Epoch 2326 :: Batch 0/1
Batch Loss = 0.0018782093288960144
2326, epoch_train_loss=0.0018782093288960144
Epoch 2327
Epoch 2327 :: Batch 0/1
Batch Loss = 0.0018781380729055326
2327, epoch_train_loss=0.0018781380729055326
Epoch 2328
Epoch 2328 :: Batch 0/1
Batch Loss = 0.0018780694538875643
2328, epoch_train_loss=0.0018780694538875643
Epoch 2329
Epoch 2329 :: Batch 0/1
Batch Loss = 0.0018780043414255818
2329, epoch_train_loss=0.0018780043414255818
Epoch 2330
Epoch 2330 :: Batch 0/1
Batch Loss = 0.0018779437104860252
2330, epoch_train_loss=0.0018779437104860252
Epoch 2331
Epoch 2331 :: Batch 0/1
Batch Loss = 0.001877889439070553
2331, epoch_train_loss=0.001877889439070553
Epoch 2332
Epoch 2332 :: Batch 0/1
Batch Loss = 0.0018778432952758065
2332, epoch_train_loss=0.0018778432952758065
Epoch 2333
Epoch 2333 :: Batch 0/1
Batch Loss = 0.0018778087745172209
2333, epoch_train_loss=0.0018778087745172209
Epoch 2334
Epoch 2334 :: Batch 0/1
Batch Loss = 0.0018777897237595134
2334, epoch_train_loss=0.0018777897237595134
Epoch 2335
Epoch 2335 :: Batch 0/1
Batch Loss = 0.0018777931812706404
2335, epoch_train_loss=0.0018777931812706404
Epoch 2336
Epoch 2336 :: Batch 0/1
Batch Loss = 0.0018778275046945766
2336, epoch_train_loss=0.0018778275046945766
Epoch 2337
Epoch 2337 :: Batch 0/1
Batch Loss = 0.001877908085398995
2337, epoch_train_loss=0.001877908085398995
Epoch 2338
Epoch 2338 :: Batch 0/1
Batch Loss = 0.001878053410529449
2338, epoch_train_loss=0.001878053410529449
Epoch 2339
Epoch 2339 :: Batch 0/1
Batch Loss = 0.0018782984386243532
2339, epoch_train_loss=0.0018782984386243532
Epoch 2340
Epoch 2340 :: Batch 0/1
Batch Loss = 0.0018786848269636786
2340, epoch_train_loss=0.0018786848269636786
Epoch 2341
Epoch 2341 :: Batch 0/1
Batch Loss = 0.0018792951161035072
2341, epoch_train_loss=0.0018792951161035072
Epoch 2342
Epoch 2342 :: Batch 0/1
Batch Loss = 0.001880225426543723
2342, epoch_train_loss=0.001880225426543723
Epoch 2343
Epoch 2343 :: Batch 0/1
Batch Loss = 0.0018816786584954944
2343, epoch_train_loss=0.0018816786584954944
Epoch 2344
Epoch 2344 :: Batch 0/1
Batch Loss = 0.0018838799845889629
2344, epoch_train_loss=0.0018838799845889629
Epoch 2345
Epoch 2345 :: Batch 0/1
Batch Loss = 0.0018873505434354085
2345, epoch_train_loss=0.0018873505434354085
Epoch 2346
Epoch 2346 :: Batch 0/1
Batch Loss = 0.0018926213919107187
2346, epoch_train_loss=0.0018926213919107187
Epoch 2347
Epoch 2347 :: Batch 0/1
Batch Loss = 0.0019010949254142932
2347, epoch_train_loss=0.0019010949254142932
Epoch 2348
Epoch 2348 :: Batch 0/1
Batch Loss = 0.0019140088999883363
2348, epoch_train_loss=0.0019140088999883363
Epoch 2349
Epoch 2349 :: Batch 0/1
Batch Loss = 0.0019353293207369866
2349, epoch_train_loss=0.0019353293207369866
Epoch 2350
Epoch 2350 :: Batch 0/1
Batch Loss = 0.0019677862515301574
2350, epoch_train_loss=0.0019677862515301574
Epoch 2351
Epoch 2351 :: Batch 0/1
Batch Loss = 0.0020232146312376747
2351, epoch_train_loss=0.0020232146312376747
Epoch 2352
Epoch 2352 :: Batch 0/1
Batch Loss = 0.0021066187023654945
2352, epoch_train_loss=0.0021066187023654945
Epoch 2353
Epoch 2353 :: Batch 0/1
Batch Loss = 0.002255339918108323
2353, epoch_train_loss=0.002255339918108323
Epoch 2354
Epoch 2354 :: Batch 0/1
Batch Loss = 0.002472085532060657
2354, epoch_train_loss=0.002472085532060657
Epoch 2355
Epoch 2355 :: Batch 0/1
Batch Loss = 0.002880909329281688
2355, epoch_train_loss=0.002880909329281688
Epoch 2356
Epoch 2356 :: Batch 0/1
Batch Loss = 0.0034352774650744065
2356, epoch_train_loss=0.0034352774650744065
Epoch 2357
Epoch 2357 :: Batch 0/1
Batch Loss = 0.004560493508297083
2357, epoch_train_loss=0.004560493508297083
Epoch 2358
Epoch 2358 :: Batch 0/1
Batch Loss = 0.005860030231469195
2358, epoch_train_loss=0.005860030231469195
Epoch 2359
Epoch 2359 :: Batch 0/1
Batch Loss = 0.008758032827618424
2359, epoch_train_loss=0.008758032827618424
Epoch 2360
Epoch 2360 :: Batch 0/1
Batch Loss = 0.011000508765645943
2360, epoch_train_loss=0.011000508765645943
Epoch 2361
Epoch 2361 :: Batch 0/1
Batch Loss = 0.01674661953966741
2361, epoch_train_loss=0.01674661953966741
Epoch 2362
Epoch 2362 :: Batch 0/1
Batch Loss = 0.01729310830848741
2362, epoch_train_loss=0.01729310830848741
Epoch 2363
Epoch 2363 :: Batch 0/1
Batch Loss = 0.0218227939264093
2363, epoch_train_loss=0.0218227939264093
Epoch 2364
Epoch 2364 :: Batch 0/1
Batch Loss = 0.015197987279400063
2364, epoch_train_loss=0.015197987279400063
Epoch 2365
Epoch 2365 :: Batch 0/1
Batch Loss = 0.010830347196890336
2365, epoch_train_loss=0.010830347196890336
Epoch 2366
Epoch 2366 :: Batch 0/1
Batch Loss = 0.004312564246191081
2366, epoch_train_loss=0.004312564246191081
Epoch 2367
Epoch 2367 :: Batch 0/1
Batch Loss = 0.0020547983623477265
2367, epoch_train_loss=0.0020547983623477265
Epoch 2368
Epoch 2368 :: Batch 0/1
Batch Loss = 0.0038457554933508165
2368, epoch_train_loss=0.0038457554933508165
Epoch 2369
Epoch 2369 :: Batch 0/1
Batch Loss = 0.006967572043427888
2369, epoch_train_loss=0.006967572043427888
Epoch 2370
Epoch 2370 :: Batch 0/1
Batch Loss = 0.009811889769923808
2370, epoch_train_loss=0.009811889769923808
Epoch 2371
Epoch 2371 :: Batch 0/1
Batch Loss = 0.0075231721854109555
2371, epoch_train_loss=0.0075231721854109555
Epoch 2372
Epoch 2372 :: Batch 0/1
Batch Loss = 0.004770627902597027
2372, epoch_train_loss=0.004770627902597027
Epoch 2373
Epoch 2373 :: Batch 0/1
Batch Loss = 0.0023668150314425777
2373, epoch_train_loss=0.0023668150314425777
Epoch 2374
Epoch 2374 :: Batch 0/1
Batch Loss = 0.0026515466182766567
2374, epoch_train_loss=0.0026515466182766567
Epoch 2375
Epoch 2375 :: Batch 0/1
Batch Loss = 0.004634950994088471
2375, epoch_train_loss=0.004634950994088471
Epoch 2376
Epoch 2376 :: Batch 0/1
Batch Loss = 0.005715099578045012
2376, epoch_train_loss=0.005715099578045012
Epoch 2377
Epoch 2377 :: Batch 0/1
Batch Loss = 0.005384772214261545
2377, epoch_train_loss=0.005384772214261545
Epoch 2378
Epoch 2378 :: Batch 0/1
Batch Loss = 0.003283205793301084
2378, epoch_train_loss=0.003283205793301084
Epoch 2379
Epoch 2379 :: Batch 0/1
Batch Loss = 0.002009893378023529
2379, epoch_train_loss=0.002009893378023529
Epoch 2380
Epoch 2380 :: Batch 0/1
Batch Loss = 0.002338974669259327
2380, epoch_train_loss=0.002338974669259327
Epoch 2381
Epoch 2381 :: Batch 0/1
Batch Loss = 0.003422416781685204
2381, epoch_train_loss=0.003422416781685204
Epoch 2382
Epoch 2382 :: Batch 0/1
Batch Loss = 0.003985579379153998
2382, epoch_train_loss=0.003985579379153998
Epoch 2383
Epoch 2383 :: Batch 0/1
Batch Loss = 0.003156640014058631
2383, epoch_train_loss=0.003156640014058631
Epoch 2384
Epoch 2384 :: Batch 0/1
Batch Loss = 0.002216188244372576
2384, epoch_train_loss=0.002216188244372576
Epoch 2385
Epoch 2385 :: Batch 0/1
Batch Loss = 0.001997703668790037
2385, epoch_train_loss=0.001997703668790037
Epoch 2386
Epoch 2386 :: Batch 0/1
Batch Loss = 0.0025158195374889803
2386, epoch_train_loss=0.0025158195374889803
Epoch 2387
Epoch 2387 :: Batch 0/1
Batch Loss = 0.0029962258684572253
2387, epoch_train_loss=0.0029962258684572253
Epoch 2388
Epoch 2388 :: Batch 0/1
Batch Loss = 0.002786836140742965
2388, epoch_train_loss=0.002786836140742965
Epoch 2389
Epoch 2389 :: Batch 0/1
Batch Loss = 0.0023068625410522623
2389, epoch_train_loss=0.0023068625410522623
Epoch 2390
Epoch 2390 :: Batch 0/1
Batch Loss = 0.002008437640078258
2390, epoch_train_loss=0.002008437640078258
Epoch 2391
Epoch 2391 :: Batch 0/1
Batch Loss = 0.002133554162472901
2391, epoch_train_loss=0.002133554162472901
Epoch 2392
Epoch 2392 :: Batch 0/1
Batch Loss = 0.002388696867077585
2392, epoch_train_loss=0.002388696867077585
Epoch 2393
Epoch 2393 :: Batch 0/1
Batch Loss = 0.002453468368255207
2393, epoch_train_loss=0.002453468368255207
Epoch 2394
Epoch 2394 :: Batch 0/1
Batch Loss = 0.002329948148888683
2394, epoch_train_loss=0.002329948148888683
Epoch 2395
Epoch 2395 :: Batch 0/1
Batch Loss = 0.0021027889541770355
2395, epoch_train_loss=0.0021027889541770355
Epoch 2396
Epoch 2396 :: Batch 0/1
Batch Loss = 0.0019925046040572053
2396, epoch_train_loss=0.0019925046040572053
Epoch 2397
Epoch 2397 :: Batch 0/1
Batch Loss = 0.0020291941581450736
2397, epoch_train_loss=0.0020291941581450736
Epoch 2398
Epoch 2398 :: Batch 0/1
Batch Loss = 0.002149506448163756
2398, epoch_train_loss=0.002149506448163756
Epoch 2399
Epoch 2399 :: Batch 0/1
Batch Loss = 0.0022280288530322438
2399, epoch_train_loss=0.0022280288530322438
Epoch 2400
Epoch 2400 :: Batch 0/1
Batch Loss = 0.002139740309574015
2400, epoch_train_loss=0.002139740309574015
Epoch 2401
Epoch 2401 :: Batch 0/1
Batch Loss = 0.0019860128506431294
2401, epoch_train_loss=0.0019860128506431294
Epoch 2402
Epoch 2402 :: Batch 0/1
Batch Loss = 0.0019070937229013897
2402, epoch_train_loss=0.0019070937229013897
Epoch 2403
Epoch 2403 :: Batch 0/1
Batch Loss = 0.001970120043073962
2403, epoch_train_loss=0.001970120043073962
Epoch 2404
Epoch 2404 :: Batch 0/1
Batch Loss = 0.0020731052509566092
2404, epoch_train_loss=0.0020731052509566092
Epoch 2405
Epoch 2405 :: Batch 0/1
Batch Loss = 0.002079412749456319
2405, epoch_train_loss=0.002079412749456319
Epoch 2406
Epoch 2406 :: Batch 0/1
Batch Loss = 0.0020060272775270335
2406, epoch_train_loss=0.0020060272775270335
Epoch 2407
Epoch 2407 :: Batch 0/1
Batch Loss = 0.0019313336709639735
2407, epoch_train_loss=0.0019313336709639735
Epoch 2408
Epoch 2408 :: Batch 0/1
Batch Loss = 0.0019156780674054155
2408, epoch_train_loss=0.0019156780674054155
Epoch 2409
Epoch 2409 :: Batch 0/1
Batch Loss = 0.001941017151100868
2409, epoch_train_loss=0.001941017151100868
Epoch 2410
Epoch 2410 :: Batch 0/1
Batch Loss = 0.001968478524178245
2410, epoch_train_loss=0.001968478524178245
Epoch 2411
Epoch 2411 :: Batch 0/1
Batch Loss = 0.0019826650353479284
2411, epoch_train_loss=0.0019826650353479284
Epoch 2412
Epoch 2412 :: Batch 0/1
Batch Loss = 0.001969164787792003
2412, epoch_train_loss=0.001969164787792003
Epoch 2413
Epoch 2413 :: Batch 0/1
Batch Loss = 0.0019355070719614972
2413, epoch_train_loss=0.0019355070719614972
Epoch 2414
Epoch 2414 :: Batch 0/1
Batch Loss = 0.0019017700810737947
2414, epoch_train_loss=0.0019017700810737947
Epoch 2415
Epoch 2415 :: Batch 0/1
Batch Loss = 0.00189612886112209
2415, epoch_train_loss=0.00189612886112209
Epoch 2416
Epoch 2416 :: Batch 0/1
Batch Loss = 0.00191946106317253
2416, epoch_train_loss=0.00191946106317253
Epoch 2417
Epoch 2417 :: Batch 0/1
Batch Loss = 0.0019414232712090744
2417, epoch_train_loss=0.0019414232712090744
Epoch 2418
Epoch 2418 :: Batch 0/1
Batch Loss = 0.0019392560020861625
2418, epoch_train_loss=0.0019392560020861625
Epoch 2419
Epoch 2419 :: Batch 0/1
Batch Loss = 0.0019177089182328814
2419, epoch_train_loss=0.0019177089182328814
Epoch 2420
Epoch 2420 :: Batch 0/1
Batch Loss = 0.0018990583588187875
2420, epoch_train_loss=0.0018990583588187875
Epoch 2421
Epoch 2421 :: Batch 0/1
Batch Loss = 0.001893007245680525
2421, epoch_train_loss=0.001893007245680525
Epoch 2422
Epoch 2422 :: Batch 0/1
Batch Loss = 0.0018960965473606107
2422, epoch_train_loss=0.0018960965473606107
Epoch 2423
Epoch 2423 :: Batch 0/1
Batch Loss = 0.0019025715065613166
2423, epoch_train_loss=0.0019025715065613166
Epoch 2424
Epoch 2424 :: Batch 0/1
Batch Loss = 0.0019090494375205048
2424, epoch_train_loss=0.0019090494375205048
Epoch 2425
Epoch 2425 :: Batch 0/1
Batch Loss = 0.0019108400394281335
2425, epoch_train_loss=0.0019108400394281335
Epoch 2426
Epoch 2426 :: Batch 0/1
Batch Loss = 0.0019034368401208705
2426, epoch_train_loss=0.0019034368401208705
Epoch 2427
Epoch 2427 :: Batch 0/1
Batch Loss = 0.001891889961597147
2427, epoch_train_loss=0.001891889961597147
Epoch 2428
Epoch 2428 :: Batch 0/1
Batch Loss = 0.0018847912571454215
2428, epoch_train_loss=0.0018847912571454215
Epoch 2429
Epoch 2429 :: Batch 0/1
Batch Loss = 0.001886267912592312
2429, epoch_train_loss=0.001886267912592312
Epoch 2430
Epoch 2430 :: Batch 0/1
Batch Loss = 0.0018917647734195712
2430, epoch_train_loss=0.0018917647734195712
Epoch 2431
Epoch 2431 :: Batch 0/1
Batch Loss = 0.0018949605419683968
2431, epoch_train_loss=0.0018949605419683968
Epoch 2432
Epoch 2432 :: Batch 0/1
Batch Loss = 0.0018947101338864525
2432, epoch_train_loss=0.0018947101338864525
Epoch 2433
Epoch 2433 :: Batch 0/1
Batch Loss = 0.0018922456013511202
2433, epoch_train_loss=0.0018922456013511202
Epoch 2434
Epoch 2434 :: Batch 0/1
Batch Loss = 0.0018887283872231788
2434, epoch_train_loss=0.0018887283872231788
Epoch 2435
Epoch 2435 :: Batch 0/1
Batch Loss = 0.001884705436145893
2435, epoch_train_loss=0.001884705436145893
Epoch 2436
Epoch 2436 :: Batch 0/1
Batch Loss = 0.0018819845579353647
2436, epoch_train_loss=0.0018819845579353647
Epoch 2437
Epoch 2437 :: Batch 0/1
Batch Loss = 0.001882282190726985
2437, epoch_train_loss=0.001882282190726985
Epoch 2438
Epoch 2438 :: Batch 0/1
Batch Loss = 0.0018847286257721151
2438, epoch_train_loss=0.0018847286257721151
Epoch 2439
Epoch 2439 :: Batch 0/1
Batch Loss = 0.0018866406250121477
2439, epoch_train_loss=0.0018866406250121477
Epoch 2440
Epoch 2440 :: Batch 0/1
Batch Loss = 0.0018864525407048748
2440, epoch_train_loss=0.0018864525407048748
Epoch 2441
Epoch 2441 :: Batch 0/1
Batch Loss = 0.0018847990140700081
2441, epoch_train_loss=0.0018847990140700081
Epoch 2442
Epoch 2442 :: Batch 0/1
Batch Loss = 0.0018828713037661034
2442, epoch_train_loss=0.0018828713037661034
Epoch 2443
Epoch 2443 :: Batch 0/1
Batch Loss = 0.0018812955780990702
2443, epoch_train_loss=0.0018812955780990702
Epoch 2444
Epoch 2444 :: Batch 0/1
Batch Loss = 0.0018800910281108023
2444, epoch_train_loss=0.0018800910281108023
Epoch 2445
Epoch 2445 :: Batch 0/1
Batch Loss = 0.0018795539899636648
2445, epoch_train_loss=0.0018795539899636648
Epoch 2446
Epoch 2446 :: Batch 0/1
Batch Loss = 0.0018799113909379844
2446, epoch_train_loss=0.0018799113909379844
Epoch 2447
Epoch 2447 :: Batch 0/1
Batch Loss = 0.001880774137450236
2447, epoch_train_loss=0.001880774137450236
Epoch 2448
Epoch 2448 :: Batch 0/1
Batch Loss = 0.0018813359823611506
2448, epoch_train_loss=0.0018813359823611506
Epoch 2449
Epoch 2449 :: Batch 0/1
Batch Loss = 0.0018810471745825384
2449, epoch_train_loss=0.0018810471745825384
Epoch 2450
Epoch 2450 :: Batch 0/1
Batch Loss = 0.001880177248584103
2450, epoch_train_loss=0.001880177248584103
Epoch 2451
Epoch 2451 :: Batch 0/1
Batch Loss = 0.0018792556466133467
2451, epoch_train_loss=0.0018792556466133467
Epoch 2452
Epoch 2452 :: Batch 0/1
Batch Loss = 0.0018785389507133916
2452, epoch_train_loss=0.0018785389507133916
Epoch 2453
Epoch 2453 :: Batch 0/1
Batch Loss = 0.0018779871070053232
2453, epoch_train_loss=0.0018779871070053232
Epoch 2454
Epoch 2454 :: Batch 0/1
Batch Loss = 0.0018776022521330416
2454, epoch_train_loss=0.0018776022521330416
Epoch 2455
Epoch 2455 :: Batch 0/1
Batch Loss = 0.0018775075008388993
2455, epoch_train_loss=0.0018775075008388993
Epoch 2456
Epoch 2456 :: Batch 0/1
Batch Loss = 0.001877698078413665
2456, epoch_train_loss=0.001877698078413665
Epoch 2457
Epoch 2457 :: Batch 0/1
Batch Loss = 0.0018779205288763623
2457, epoch_train_loss=0.0018779205288763623
Epoch 2458
Epoch 2458 :: Batch 0/1
Batch Loss = 0.0018779226228113515
2458, epoch_train_loss=0.0018779226228113515
Epoch 2459
Epoch 2459 :: Batch 0/1
Batch Loss = 0.0018776789295971528
2459, epoch_train_loss=0.0018776789295971528
Epoch 2460
Epoch 2460 :: Batch 0/1
Batch Loss = 0.0018773135332682672
2460, epoch_train_loss=0.0018773135332682672
Epoch 2461
Epoch 2461 :: Batch 0/1
Batch Loss = 0.0018769520965477055
2461, epoch_train_loss=0.0018769520965477055
Epoch 2462
Epoch 2462 :: Batch 0/1
Batch Loss = 0.0018766043313637164
2462, epoch_train_loss=0.0018766043313637164
Epoch 2463
Epoch 2463 :: Batch 0/1
Batch Loss = 0.0018762654784182365
2463, epoch_train_loss=0.0018762654784182365
Epoch 2464
Epoch 2464 :: Batch 0/1
Batch Loss = 0.0018759772962265175
2464, epoch_train_loss=0.0018759772962265175
Epoch 2465
Epoch 2465 :: Batch 0/1
Batch Loss = 0.0018758008597895362
2465, epoch_train_loss=0.0018758008597895362
Epoch 2466
Epoch 2466 :: Batch 0/1
Batch Loss = 0.0018757452237712486
2466, epoch_train_loss=0.0018757452237712486
Epoch 2467
Epoch 2467 :: Batch 0/1
Batch Loss = 0.0018757387804219329
2467, epoch_train_loss=0.0018757387804219329
Epoch 2468
Epoch 2468 :: Batch 0/1
Batch Loss = 0.0018757017678186572
2468, epoch_train_loss=0.0018757017678186572
Epoch 2469
Epoch 2469 :: Batch 0/1
Batch Loss = 0.0018756150035226392
2469, epoch_train_loss=0.0018756150035226392
Epoch 2470
Epoch 2470 :: Batch 0/1
Batch Loss = 0.001875503894743855
2470, epoch_train_loss=0.001875503894743855
Epoch 2471
Epoch 2471 :: Batch 0/1
Batch Loss = 0.001875382267492624
2471, epoch_train_loss=0.001875382267492624
Epoch 2472
Epoch 2472 :: Batch 0/1
Batch Loss = 0.0018752372202490255
2472, epoch_train_loss=0.0018752372202490255
Epoch 2473
Epoch 2473 :: Batch 0/1
Batch Loss = 0.001875053928056109
2473, epoch_train_loss=0.001875053928056109
Epoch 2474
Epoch 2474 :: Batch 0/1
Batch Loss = 0.001874851240774438
2474, epoch_train_loss=0.001874851240774438
Epoch 2475
Epoch 2475 :: Batch 0/1
Batch Loss = 0.0018746588865064088
2475, epoch_train_loss=0.0018746588865064088
Epoch 2476
Epoch 2476 :: Batch 0/1
Batch Loss = 0.0018744966783218677
2476, epoch_train_loss=0.0018744966783218677
Epoch 2477
Epoch 2477 :: Batch 0/1
Batch Loss = 0.0018743607702641284
2477, epoch_train_loss=0.0018743607702641284
Epoch 2478
Epoch 2478 :: Batch 0/1
Batch Loss = 0.0018742362950980589
2478, epoch_train_loss=0.0018742362950980589
Epoch 2479
Epoch 2479 :: Batch 0/1
Batch Loss = 0.001874117538501533
2479, epoch_train_loss=0.001874117538501533
Epoch 2480
Epoch 2480 :: Batch 0/1
Batch Loss = 0.0018740102398110347
2480, epoch_train_loss=0.0018740102398110347
Epoch 2481
Epoch 2481 :: Batch 0/1
Batch Loss = 0.0018739200809413901
2481, epoch_train_loss=0.0018739200809413901
Epoch 2482
Epoch 2482 :: Batch 0/1
Batch Loss = 0.0018738419528530697
2482, epoch_train_loss=0.0018738419528530697
Epoch 2483
Epoch 2483 :: Batch 0/1
Batch Loss = 0.0018737631260164523
2483, epoch_train_loss=0.0018737631260164523
Epoch 2484
Epoch 2484 :: Batch 0/1
Batch Loss = 0.0018736769724446798
2484, epoch_train_loss=0.0018736769724446798
Epoch 2485
Epoch 2485 :: Batch 0/1
Batch Loss = 0.0018735866034183622
2485, epoch_train_loss=0.0018735866034183622
Epoch 2486
Epoch 2486 :: Batch 0/1
Batch Loss = 0.0018734969244778039
2486, epoch_train_loss=0.0018734969244778039
Epoch 2487
Epoch 2487 :: Batch 0/1
Batch Loss = 0.0018734100323014931
2487, epoch_train_loss=0.0018734100323014931
Epoch 2488
Epoch 2488 :: Batch 0/1
Batch Loss = 0.0018733221087472859
2488, epoch_train_loss=0.0018733221087472859
Epoch 2489
Epoch 2489 :: Batch 0/1
Batch Loss = 0.001873230538558377
2489, epoch_train_loss=0.001873230538558377
Epoch 2490
Epoch 2490 :: Batch 0/1
Batch Loss = 0.0018731352808038356
2490, epoch_train_loss=0.0018731352808038356
Epoch 2491
Epoch 2491 :: Batch 0/1
Batch Loss = 0.0018730403346260969
2491, epoch_train_loss=0.0018730403346260969
Epoch 2492
Epoch 2492 :: Batch 0/1
Batch Loss = 0.0018729489340710604
2492, epoch_train_loss=0.0018729489340710604
Epoch 2493
Epoch 2493 :: Batch 0/1
Batch Loss = 0.0018728614303706024
2493, epoch_train_loss=0.0018728614303706024
Epoch 2494
Epoch 2494 :: Batch 0/1
Batch Loss = 0.0018727753071369733
2494, epoch_train_loss=0.0018727753071369733
Epoch 2495
Epoch 2495 :: Batch 0/1
Batch Loss = 0.0018726901099666276
2495, epoch_train_loss=0.0018726901099666276
Epoch 2496
Epoch 2496 :: Batch 0/1
Batch Loss = 0.0018726066530556271
2496, epoch_train_loss=0.0018726066530556271
Epoch 2497
Epoch 2497 :: Batch 0/1
Batch Loss = 0.0018725279435559373
2497, epoch_train_loss=0.0018725279435559373
Epoch 2498
Epoch 2498 :: Batch 0/1
Batch Loss = 0.001872454938888012
2498, epoch_train_loss=0.001872454938888012
Epoch 2499
Epoch 2499 :: Batch 0/1
Batch Loss = 0.0018723886456018845
2499, epoch_train_loss=0.0018723886456018845
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting "B3LYP_WITH_VWN5 = True" in pyscf_conf.py
  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8043520> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8043520> in UKS object of <class 'pyscf.dft.uks.UKS'>
<pyscf.gto.mole.Mole object at 0x7ffeb8043520> [['P', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb8042710> [['N', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb8040b50> [['H', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb8042ef0> [['Li', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb8043e50> [['O', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb80426b0> [['Cl', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb80427a0> [['Al', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb8042ce0> [['S', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb8041d80> [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb80422f0> [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb80408b0> [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb8041a80> [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb8041450> [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb8040dc0> [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb8043df0> [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb8041db0> [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb8040c40> [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb8041810> [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb8040310> [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb8042800> [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb8043880> [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb80420e0> [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb8043fa0> [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb8041960> [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb8043a30> [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]] 5
<pyscf.gto.mole.Mole object at 0x7ffeb80437f0> [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb8041690> [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]] 5
mol:  [['P', array([0., 0., 0.])]]
converged SCF energy = -341.104145992717  <S^2> = 3.7502984  2S+1 = 4.0001492
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8042710> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8042710> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-3.47389956e-03 -8.82676818e-04 -2.08411238e-03 ... -1.11301603e+01
 -1.11301603e+01 -1.11301603e+01] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 3)
mol:  [['N', array([0., 0., 0.])]]
converged SCF energy = -54.5289742046675  <S^2> = 3.7524945  2S+1 = 4.0012471
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8040b50> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8040b50> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5016, 22)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.07670570e-03 -5.92340671e-04 -6.66573372e-05 ... -5.03679786e+00
 -5.03679786e+00 -5.03679786e+00] = SCAN,
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5016, 22)
rho shape (4, 4, 2, 5016)
rho_filt shape: (5016,)
get descriptors tdrho.shape=(5016, 3)
mol:  [['H', array([0., 0., 0.])]]
converged SCF energy = -0.499812984008539  <S^2> = 0.75  2S+1 = 2
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8042ef0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8042ef0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 7, 7)
ao.shape (10, 2440, 7)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.59173730e-03 -7.67300285e-04 -6.09330303e-05 ... -7.27736608e-01
 -7.27736608e-01 -7.27736608e-01] = SCAN,
get_data, dm shape = (2, 7, 7)
ao_eval.shape=(4, 2440, 7)
rho shape (4, 4, 2, 2440)
rho_filt shape: (2440,)
get descriptors tdrho.shape=(2440, 3)
mol:  [['Li', array([0., 0., 0.])]]
converged SCF energy = -7.46006188627842  <S^2> = 0.75000049  2S+1 = 2.0000005
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8043e50> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8043e50> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 4592, 22)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-6.71503005e-03 -1.44519923e-03 -1.44519923e-03 ... -1.46899070e-02
 -2.03947707e+00 -2.03947707e+00] = SCAN,
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 4592, 22)
rho shape (4, 4, 2, 4592)
rho_filt shape: (4592,)
get descriptors tdrho.shape=(4592, 3)
mol:  [['O', array([0., 0., 0.])]]
converged SCF energy = -75.0033834885785  <S^2> = 2.0027437  2S+1 = 3.0018286
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb80426b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb80426b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5040, 22)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.03909418e-03 -1.69662637e-04 -8.02681466e-06 ... -5.78449312e+00
 -5.78449312e+00 -5.78449312e+00] = SCAN,
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5040, 22)
rho shape (4, 4, 2, 5040)
rho_filt shape: (5040,)
get descriptors tdrho.shape=(5040, 3)
mol:  [['Cl', array([0., 0., 0.])]]
converged SCF energy = -459.957577120879  <S^2> = 0.7516194  2S+1 = 2.0016187
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb80427a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb80427a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6152, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-7.06820868e-04 -9.17979579e-04 -3.13830971e-04 ... -1.26648275e+01
 -1.26648275e+01 -1.26648275e+01] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6152, 30)
rho shape (4, 4, 2, 6152)
rho_filt shape: (6152,)
get descriptors tdrho.shape=(6152, 3)
mol:  [['Al', array([0., 0., 0.])]]
converged SCF energy = -242.226560993883  <S^2> = 0.75226414  2S+1 = 2.0022629
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8042ce0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8042ce0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6088, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-0.02356543 -0.0151998  -0.00773866 ... -0.00013664 -0.00169117
 -0.00012232] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6088, 30)
rho shape (4, 4, 2, 6088)
rho_filt shape: (6088,)
get descriptors tdrho.shape=(6088, 3)
mol:  [['S', array([0., 0., 0.])]]
converged SCF energy = -397.938786807112  <S^2> = 2.0022329  2S+1 = 3.0014882
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8041d80> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8041d80> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.59239226e-03 -7.51631110e-04 -9.06634250e-04 ... -1.18986567e+01
 -1.18986567e+01 -1.18986567e+01] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 3)
mol:  [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]]
converged SCF energy = -1.16580491182912  <S^2> = 2.220446e-16  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb80422f0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb80422f0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 14, 14)
ao.shape (10, 4776, 14)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.31557088e-04 -9.73828620e-06 -3.66768667e-04 ... -5.54165574e-01
 -5.54165574e-01 -5.54165574e-01] = SCAN,
get_data, dm shape = (2, 14, 14)
ao_eval.shape=(4, 4776, 14)
rho shape (4, 4, 2, 4776)
rho_filt shape: (4776,)
get descriptors tdrho.shape=(4776, 3)
mol:  [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]]
converged SCF energy = -109.439263799566  <S^2> = 1.7763568e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb80408b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb80408b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9848, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-9.68474977e-05 -9.84742592e-04 -2.59676393e-04 ... -2.39645778e-05
 -2.39645778e-05 -9.68474977e-05] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9848, 44)
rho shape (4, 4, 2, 9848)
rho_filt shape: (9848,)
get descriptors tdrho.shape=(9848, 3)
mol:  [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]]
converged SCF energy = -107.339357395734  <S^2> = 8.8817842e-15  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8041a80> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8041a80> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9752, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.04987770e-03 -6.68954111e-04 -8.57556562e-04 ... -1.07485605e-03
 -8.01425702e-01 -8.01425702e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9752, 44)
rho shape (4, 4, 2, 9752)
rho_filt shape: (9752,)
get descriptors tdrho.shape=(9752, 3)
mol:  [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]]
converged SCF energy = -93.3377924465129  <S^2> = 4.0072834e-10  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8041450> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8041450> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 12256, 51)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.97917639e-04 -2.54437615e-05 -3.15202008e-05 ... -6.37386388e-01
 -6.37386388e-01 -6.37386388e-01] = SCAN,
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 12256, 51)
rho shape (4, 4, 2, 12256)
rho_filt shape: (12256,)
get descriptors tdrho.shape=(12256, 3)
mol:  [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]]
converged SCF energy = -188.456965322844  <S^2> = 1.7763568e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8040dc0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8040dc0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 66, 66)
ao.shape (10, 14920, 66)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.50217343e-04 -2.07520331e-04 -9.23619961e-04 ... -2.76182455e-06
 -4.27559894e+00 -4.27559894e+00] = SCAN,
get_data, dm shape = (2, 66, 66)
ao_eval.shape=(4, 14920, 66)
rho shape (4, 4, 2, 14920)
rho_filt shape: (14920,)
get descriptors tdrho.shape=(14920, 3)
mol:  [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]]
converged SCF energy = -920.00560888896  <S^2> = 5.0093263e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8043df0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8043df0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12208, 60)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-0.00155834 -0.00091688 -0.00215831 ... -0.00091688 -0.41618506
 -0.41618506] = SCAN,
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12208, 60)
rho shape (4, 4, 2, 12208)
rho_filt shape: (12208,)
get descriptors tdrho.shape=(12208, 3)
mol:  [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]]
converged SCF energy = -199.394370591172  <S^2> = 1.2079227e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8041db0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8041db0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9824, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.92948752e-04 -1.95215890e-05 -1.16699780e-03 ... -4.89378326e-01
 -4.89378326e-01 -4.89378326e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9824, 44)
rho shape (4, 4, 2, 9824)
rho_filt shape: (9824,)
get descriptors tdrho.shape=(9824, 3)
mol:  [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]]
converged SCF energy = -150.214894485937  <S^2> = 1.0018599  2S+1 = 2.2377309
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8040c40> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8040c40> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9912, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-7.55964964e-04 -1.35225323e-04 -6.99690653e-06 ... -6.59150642e-01
 -6.59150642e-01 -6.59150642e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9912, 44)
rho shape (4, 4, 2, 9912)
rho_filt shape: (9912,)
get descriptors tdrho.shape=(9912, 3)
mol:  [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]]
converged SCF energy = -77.2435048346375  <S^2> = 1.2434498e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8041810> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8041810> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 58, 58)
ao.shape (10, 15208, 58)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-8.83278187e-05 -8.83278187e-05 -9.75839793e-04 ... -3.46740731e-05
 -3.31729009e-05 -3.31729009e-05] = SCAN,
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15208, 58)
rho shape (4, 4, 2, 15208)
rho_filt shape: (15208,)
get descriptors tdrho.shape=(15208, 3)
mol:  [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]]
converged SCF energy = -113.221335689652  <S^2> = 6.5725203e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8040310> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8040310> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 10040, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-5.37000596e-04 -8.55494373e-04 -2.46853248e-03 ... -7.34251993e-01
 -7.34251993e-01 -7.34251993e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 10040, 44)
rho shape (4, 4, 2, 10040)
rho_filt shape: (10040,)
get descriptors tdrho.shape=(10040, 3)
mol:  [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]]
converged SCF energy = -460.624592374077  <S^2> = 6.9277917e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8042800> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8042800> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 37, 37)
ao.shape (10, 8552, 37)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.38161478e-04 -1.81223966e-05 -2.37327566e-05 ... -2.83738108e+00
 -2.83738108e+00 -2.83738108e+00] = SCAN,
get_data, dm shape = (2, 37, 37)
ao_eval.shape=(4, 8552, 37)
rho shape (4, 4, 2, 8552)
rho_filt shape: (8552,)
get descriptors tdrho.shape=(8552, 3)
mol:  [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]]
converged SCF energy = -8.04458854018922  <S^2> = 7.5495166e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8043880> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8043880> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 29, 29)
ao.shape (10, 6936, 29)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-0.00113445 -0.00118624 -0.00118624 ... -0.48434639 -0.48434639
 -0.48434639] = SCAN,
get_data, dm shape = (2, 29, 29)
ao_eval.shape=(4, 6936, 29)
rho shape (4, 4, 2, 6936)
rho_filt shape: (6936,)
get descriptors tdrho.shape=(6936, 3)
mol:  [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]]
converged SCF energy = -324.340512506578  <S^2> = 1.5862867e-11  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb80420e0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb80420e0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 11536, 60)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-0.00297936 -0.00297936 -0.00407091 ... -0.00297936 -0.00297936
 -0.00407091] = SCAN,
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 11536, 60)
rho shape (4, 4, 2, 11536)
rho_filt shape: (11536,)
get descriptors tdrho.shape=(11536, 3)
mol:  [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]]
converged SCF energy = -1622.57507845814  <S^2> = 8.2778229e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8043fa0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8043fa0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 120, 120)
ao.shape (10, 24512, 120)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.61401455e-04 -4.90485117e-04 -2.56451688e-03 ... -9.59296114e+00
 -9.59296114e+00 -9.59296114e+00] = SCAN,
get_data, dm shape = (2, 120, 120)
ao_eval.shape=(4, 24512, 120)
rho shape (4, 4, 2, 24512)
rho_filt shape: (24512,)
get descriptors tdrho.shape=(24512, 3)
mol:  [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]]
converged SCF energy = -342.979728469575  <S^2> = 2.5389468e-11  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8041960> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8041960> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 13096, 51)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.28637187e-03 -4.32380890e-04 -3.74072638e-05 ... -1.91722763e+00
 -1.91722763e+00 -1.91722763e+00] = SCAN,
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 13096, 51)
rho shape (4, 4, 2, 13096)
rho_filt shape: (13096,)
get descriptors tdrho.shape=(13096, 3)
mol:  [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]]
converged SCF energy = -578.565336196082  <S^2> = 1.0034705  2S+1 = 2.2391699
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8043a30> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8043a30> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12384, 60)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.60122985e-04 -2.60140242e-04 -2.60157890e-04 ... -3.86943961e-01
 -3.86943961e-01 -3.86943961e-01] = SCAN,
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12384, 60)
rho shape (4, 4, 2, 12384)
rho_filt shape: (12384,)
get descriptors tdrho.shape=(12384, 3)
mol:  [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]]
converged SCF energy = -40.4598214864076  <S^2> = 3.2152059e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb80437f0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb80437f0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 50, 50)
ao.shape (10, 13936, 50)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-8.68439856e-04 -2.42462783e-04 -1.69965237e-05 ... -2.55256081e-05
 -2.55256081e-05 -2.55256081e-05] = SCAN,
get_data, dm shape = (2, 50, 50)
ao_eval.shape=(4, 13936, 50)
rho shape (4, 4, 2, 13936)
rho_filt shape: (13936,)
get descriptors tdrho.shape=(13936, 3)
mol:  [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]]
converged SCF energy = -39.0756147483503  <S^2> = 6.2079231e-12  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8041690> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8041690> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 36, 36)
ao.shape (10, 9656, 36)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-7.67691257e-04 -4.57409182e-05 -2.02835243e-04 ... -1.14928928e+00
 -1.14928928e+00 -1.14928928e+00] = SCAN,
get_data, dm shape = (2, 36, 36)
ao_eval.shape=(4, 9656, 36)
rho shape (4, 4, 2, 9656)
rho_filt shape: (9656,)
get descriptors tdrho.shape=(9656, 3)
mol:  [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]]
converged SCF energy = -291.719272437819  <S^2> = 1.3152146e-11  2S+1 = 1
New DM shape: (2, 58, 58)
ao.shape (10, 15256, 58)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-8.33847724e-04 -2.34902391e-04 -1.75660753e-05 ... -1.92925750e-05
 -1.92925750e-05 -1.92925750e-05] = SCAN,
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15256, 58)
rho shape (4, 4, 2, 15256)
rho_filt shape: (15256,)
get descriptors tdrho.shape=(15256, 3)
PRE NAN FILT: tFxc.shape=(237007,), tdrho.shape=(237007, 3)
nan_filt_rho.shape=(237007,)
nan_filt_fxc.shape=(237007,)
tFxc.shape=(237007,), tdrho.shape=(237007, 3)
inp[0].shape = (237007, 3)
Epoch 0
Epoch 0 :: Batch 0/1
Batch Loss = 4.706672885115587
0, epoch_train_loss=4.706672885115587
Epoch 1
Epoch 1 :: Batch 0/1
Batch Loss = 4.37526545012383
1, epoch_train_loss=4.37526545012383
Epoch 2
Epoch 2 :: Batch 0/1
Batch Loss = 4.142400353165158
2, epoch_train_loss=4.142400353165158
Epoch 3
Epoch 3 :: Batch 0/1
Batch Loss = 3.9709986751182185
3, epoch_train_loss=3.9709986751182185
Epoch 4
Epoch 4 :: Batch 0/1
Batch Loss = 3.675724393394951
4, epoch_train_loss=3.675724393394951
Epoch 5
Epoch 5 :: Batch 0/1
Batch Loss = 3.331158510145481
5, epoch_train_loss=3.331158510145481
Epoch 6
Epoch 6 :: Batch 0/1
Batch Loss = 2.8801972160643627
6, epoch_train_loss=2.8801972160643627
Epoch 7
Epoch 7 :: Batch 0/1
Batch Loss = 2.2928337961793295
7, epoch_train_loss=2.2928337961793295
Epoch 8
Epoch 8 :: Batch 0/1
Batch Loss = 1.651488739853929
8, epoch_train_loss=1.651488739853929
Epoch 9
Epoch 9 :: Batch 0/1
Batch Loss = 1.0348127251183505
9, epoch_train_loss=1.0348127251183505
Epoch 10
Epoch 10 :: Batch 0/1
Batch Loss = 0.7465447943417074
10, epoch_train_loss=0.7465447943417074
Epoch 11
Epoch 11 :: Batch 0/1
Batch Loss = 0.5359701901231063
11, epoch_train_loss=0.5359701901231063
Epoch 12
Epoch 12 :: Batch 0/1
Batch Loss = 2.310116940867905
12, epoch_train_loss=2.310116940867905
Epoch 13
Epoch 13 :: Batch 0/1
Batch Loss = 0.8591543063196277
13, epoch_train_loss=0.8591543063196277
Epoch 14
Epoch 14 :: Batch 0/1
Batch Loss = 2.0282523522812013
14, epoch_train_loss=2.0282523522812013
Epoch 15
Epoch 15 :: Batch 0/1
Batch Loss = 0.9494735916580025
15, epoch_train_loss=0.9494735916580025
Epoch 16
Epoch 16 :: Batch 0/1
Batch Loss = 0.2686231792008657
16, epoch_train_loss=0.2686231792008657
Epoch 17
Epoch 17 :: Batch 0/1
Batch Loss = 0.7365044330197834
17, epoch_train_loss=0.7365044330197834
Epoch 18
Epoch 18 :: Batch 0/1
Batch Loss = 1.0015294505381875
18, epoch_train_loss=1.0015294505381875
Epoch 19
Epoch 19 :: Batch 0/1
Batch Loss = 0.682112568077853
19, epoch_train_loss=0.682112568077853
Epoch 20
Epoch 20 :: Batch 0/1
Batch Loss = 0.3340192629180512
20, epoch_train_loss=0.3340192629180512
Epoch 21
Epoch 21 :: Batch 0/1
Batch Loss = 0.2237435919696783
21, epoch_train_loss=0.2237435919696783
Epoch 22
Epoch 22 :: Batch 0/1
Batch Loss = 0.2519311282991045
22, epoch_train_loss=0.2519311282991045
Epoch 23
Epoch 23 :: Batch 0/1
Batch Loss = 0.2950587841810439
23, epoch_train_loss=0.2950587841810439
Epoch 24
Epoch 24 :: Batch 0/1
Batch Loss = 0.3091723881479731
24, epoch_train_loss=0.3091723881479731
Epoch 25
Epoch 25 :: Batch 0/1
Batch Loss = 0.2922317809680241
25, epoch_train_loss=0.2922317809680241
Epoch 26
Epoch 26 :: Batch 0/1
Batch Loss = 0.259664098028499
26, epoch_train_loss=0.259664098028499
Epoch 27
Epoch 27 :: Batch 0/1
Batch Loss = 0.22592406880877394
27, epoch_train_loss=0.22592406880877394
Epoch 28
Epoch 28 :: Batch 0/1
Batch Loss = 0.19499452989001428
28, epoch_train_loss=0.19499452989001428
Epoch 29
Epoch 29 :: Batch 0/1
Batch Loss = 0.1655885391896601
29, epoch_train_loss=0.1655885391896601
Epoch 30
Epoch 30 :: Batch 0/1
Batch Loss = 0.13437716091132407
30, epoch_train_loss=0.13437716091132407
Epoch 31
Epoch 31 :: Batch 0/1
Batch Loss = 0.10389102903630715
31, epoch_train_loss=0.10389102903630715
Epoch 32
Epoch 32 :: Batch 0/1
Batch Loss = 0.07951684925464585
32, epoch_train_loss=0.07951684925464585
Epoch 33
Epoch 33 :: Batch 0/1
Batch Loss = 0.06440666908046404
33, epoch_train_loss=0.06440666908046404
Epoch 34
Epoch 34 :: Batch 0/1
Batch Loss = 0.05611804325735374
34, epoch_train_loss=0.05611804325735374
Epoch 35
Epoch 35 :: Batch 0/1
Batch Loss = 0.049076792336434596
35, epoch_train_loss=0.049076792336434596
Epoch 36
Epoch 36 :: Batch 0/1
Batch Loss = 0.04334747344604983
36, epoch_train_loss=0.04334747344604983
Epoch 37
Epoch 37 :: Batch 0/1
Batch Loss = 0.04121777024075691
37, epoch_train_loss=0.04121777024075691
Epoch 38
Epoch 38 :: Batch 0/1
Batch Loss = 0.04276055151634837
38, epoch_train_loss=0.04276055151634837
Epoch 39
Epoch 39 :: Batch 0/1
Batch Loss = 0.04455190728888158
39, epoch_train_loss=0.04455190728888158
Epoch 40
Epoch 40 :: Batch 0/1
Batch Loss = 0.04689802870486714
40, epoch_train_loss=0.04689802870486714
Epoch 41
Epoch 41 :: Batch 0/1
Batch Loss = 0.04977566717804471
41, epoch_train_loss=0.04977566717804471
Epoch 42
Epoch 42 :: Batch 0/1
Batch Loss = 0.0502792173125385
42, epoch_train_loss=0.0502792173125385
Epoch 43
Epoch 43 :: Batch 0/1
Batch Loss = 0.04765506569841325
43, epoch_train_loss=0.04765506569841325
Epoch 44
Epoch 44 :: Batch 0/1
Batch Loss = 0.045848529750028566
44, epoch_train_loss=0.045848529750028566
Epoch 45
Epoch 45 :: Batch 0/1
Batch Loss = 0.044451476979637736
45, epoch_train_loss=0.044451476979637736
Epoch 46
Epoch 46 :: Batch 0/1
Batch Loss = 0.040730691982278515
46, epoch_train_loss=0.040730691982278515
Epoch 47
Epoch 47 :: Batch 0/1
Batch Loss = 0.03561260169575463
47, epoch_train_loss=0.03561260169575463
Epoch 48
Epoch 48 :: Batch 0/1
Batch Loss = 0.03151391348733185
48, epoch_train_loss=0.03151391348733185
Epoch 49
Epoch 49 :: Batch 0/1
Batch Loss = 0.027824646556459606
49, epoch_train_loss=0.027824646556459606
Epoch 50
Epoch 50 :: Batch 0/1
Batch Loss = 0.02424455594132879
50, epoch_train_loss=0.02424455594132879
Epoch 51
Epoch 51 :: Batch 0/1
Batch Loss = 0.021494640026010047
51, epoch_train_loss=0.021494640026010047
Epoch 52
Epoch 52 :: Batch 0/1
Batch Loss = 0.019317411453162462
52, epoch_train_loss=0.019317411453162462
Epoch 53
Epoch 53 :: Batch 0/1
Batch Loss = 0.016943235581507225
53, epoch_train_loss=0.016943235581507225
Epoch 54
Epoch 54 :: Batch 0/1
Batch Loss = 0.015262748222477944
54, epoch_train_loss=0.015262748222477944
Epoch 55
Epoch 55 :: Batch 0/1
Batch Loss = 0.014775821878192603
55, epoch_train_loss=0.014775821878192603
Epoch 56
Epoch 56 :: Batch 0/1
Batch Loss = 0.014598898394070144
56, epoch_train_loss=0.014598898394070144
Epoch 57
Epoch 57 :: Batch 0/1
Batch Loss = 0.014044760168592916
57, epoch_train_loss=0.014044760168592916
Epoch 58
Epoch 58 :: Batch 0/1
Batch Loss = 0.013740033269389591
58, epoch_train_loss=0.013740033269389591
Epoch 59
Epoch 59 :: Batch 0/1
Batch Loss = 0.013990727872110427
59, epoch_train_loss=0.013990727872110427
Epoch 60
Epoch 60 :: Batch 0/1
Batch Loss = 0.014275814880163365
60, epoch_train_loss=0.014275814880163365
Epoch 61
Epoch 61 :: Batch 0/1
Batch Loss = 0.014364562987060441
61, epoch_train_loss=0.014364562987060441
Epoch 62
Epoch 62 :: Batch 0/1
Batch Loss = 0.014371811515629634
62, epoch_train_loss=0.014371811515629634
Epoch 63
Epoch 63 :: Batch 0/1
Batch Loss = 0.014281110070441003
63, epoch_train_loss=0.014281110070441003
Epoch 64
Epoch 64 :: Batch 0/1
Batch Loss = 0.01397077357230004
64, epoch_train_loss=0.01397077357230004
Epoch 65
Epoch 65 :: Batch 0/1
Batch Loss = 0.013632879236406038
65, epoch_train_loss=0.013632879236406038
Epoch 66
Epoch 66 :: Batch 0/1
Batch Loss = 0.013286886928282934
66, epoch_train_loss=0.013286886928282934
Epoch 67
Epoch 67 :: Batch 0/1
Batch Loss = 0.012679715403746582
67, epoch_train_loss=0.012679715403746582
Epoch 68
Epoch 68 :: Batch 0/1
Batch Loss = 0.011913742587590561
68, epoch_train_loss=0.011913742587590561
Epoch 69
Epoch 69 :: Batch 0/1
Batch Loss = 0.01131243631891669
69, epoch_train_loss=0.01131243631891669
Epoch 70
Epoch 70 :: Batch 0/1
Batch Loss = 0.010872272345979336
70, epoch_train_loss=0.010872272345979336
Epoch 71
Epoch 71 :: Batch 0/1
Batch Loss = 0.010359763880580544
71, epoch_train_loss=0.010359763880580544
Epoch 72
Epoch 72 :: Batch 0/1
Batch Loss = 0.009861371461910243
72, epoch_train_loss=0.009861371461910243
Epoch 73
Epoch 73 :: Batch 0/1
Batch Loss = 0.009561359580977353
73, epoch_train_loss=0.009561359580977353
Epoch 74
Epoch 74 :: Batch 0/1
Batch Loss = 0.00936087755194238
74, epoch_train_loss=0.00936087755194238
Epoch 75
Epoch 75 :: Batch 0/1
Batch Loss = 0.00918555970497664
75, epoch_train_loss=0.00918555970497664
Epoch 76
Epoch 76 :: Batch 0/1
Batch Loss = 0.009071803975590206
76, epoch_train_loss=0.009071803975590206
Epoch 77
Epoch 77 :: Batch 0/1
Batch Loss = 0.008964375907292441
77, epoch_train_loss=0.008964375907292441
Epoch 78
Epoch 78 :: Batch 0/1
Batch Loss = 0.008843278208640547
78, epoch_train_loss=0.008843278208640547
Epoch 79
Epoch 79 :: Batch 0/1
Batch Loss = 0.008776957636473078
79, epoch_train_loss=0.008776957636473078
Epoch 80
Epoch 80 :: Batch 0/1
Batch Loss = 0.008714385108710565
80, epoch_train_loss=0.008714385108710565
Epoch 81
Epoch 81 :: Batch 0/1
Batch Loss = 0.008564942427302743
81, epoch_train_loss=0.008564942427302743
Epoch 82
Epoch 82 :: Batch 0/1
Batch Loss = 0.008399479925229061
82, epoch_train_loss=0.008399479925229061
Epoch 83
Epoch 83 :: Batch 0/1
Batch Loss = 0.008271326482271324
83, epoch_train_loss=0.008271326482271324
Epoch 84
Epoch 84 :: Batch 0/1
Batch Loss = 0.008107129903830673
84, epoch_train_loss=0.008107129903830673
Epoch 85
Epoch 85 :: Batch 0/1
Batch Loss = 0.007905246789826454
85, epoch_train_loss=0.007905246789826454
Epoch 86
Epoch 86 :: Batch 0/1
Batch Loss = 0.007713131507757237
86, epoch_train_loss=0.007713131507757237
Epoch 87
Epoch 87 :: Batch 0/1
Batch Loss = 0.007518356896496365
87, epoch_train_loss=0.007518356896496365
Epoch 88
Epoch 88 :: Batch 0/1
Batch Loss = 0.00733286090816102
88, epoch_train_loss=0.00733286090816102
Epoch 89
Epoch 89 :: Batch 0/1
Batch Loss = 0.007175274378296427
89, epoch_train_loss=0.007175274378296427
Epoch 90
Epoch 90 :: Batch 0/1
Batch Loss = 0.007017156918502014
90, epoch_train_loss=0.007017156918502014
Epoch 91
Epoch 91 :: Batch 0/1
Batch Loss = 0.006870652093975891
91, epoch_train_loss=0.006870652093975891
Epoch 92
Epoch 92 :: Batch 0/1
Batch Loss = 0.006766839501953259
92, epoch_train_loss=0.006766839501953259
Epoch 93
Epoch 93 :: Batch 0/1
Batch Loss = 0.006674137109693149
93, epoch_train_loss=0.006674137109693149
Epoch 94
Epoch 94 :: Batch 0/1
Batch Loss = 0.006572038715285856
94, epoch_train_loss=0.006572038715285856
Epoch 95
Epoch 95 :: Batch 0/1
Batch Loss = 0.006482802864259815
95, epoch_train_loss=0.006482802864259815
Epoch 96
Epoch 96 :: Batch 0/1
Batch Loss = 0.006401116480910606
96, epoch_train_loss=0.006401116480910606
Epoch 97
Epoch 97 :: Batch 0/1
Batch Loss = 0.0063115207394339986
97, epoch_train_loss=0.0063115207394339986
Epoch 98
Epoch 98 :: Batch 0/1
Batch Loss = 0.006220007379625134
98, epoch_train_loss=0.006220007379625134
Epoch 99
Epoch 99 :: Batch 0/1
Batch Loss = 0.006123288950249584
99, epoch_train_loss=0.006123288950249584
Epoch 100
Epoch 100 :: Batch 0/1
Batch Loss = 0.006020386543612835
100, epoch_train_loss=0.006020386543612835
Epoch 101
Epoch 101 :: Batch 0/1
Batch Loss = 0.005923270464818507
101, epoch_train_loss=0.005923270464818507
Epoch 102
Epoch 102 :: Batch 0/1
Batch Loss = 0.00582599448165437
102, epoch_train_loss=0.00582599448165437
Epoch 103
Epoch 103 :: Batch 0/1
Batch Loss = 0.005722552296933557
103, epoch_train_loss=0.005722552296933557
Epoch 104
Epoch 104 :: Batch 0/1
Batch Loss = 0.00562610665265841
104, epoch_train_loss=0.00562610665265841
Epoch 105
Epoch 105 :: Batch 0/1
Batch Loss = 0.005537798988035902
105, epoch_train_loss=0.005537798988035902
Epoch 106
Epoch 106 :: Batch 0/1
Batch Loss = 0.005450470447987149
106, epoch_train_loss=0.005450470447987149
Epoch 107
Epoch 107 :: Batch 0/1
Batch Loss = 0.005368525339959493
107, epoch_train_loss=0.005368525339959493
Epoch 108
Epoch 108 :: Batch 0/1
Batch Loss = 0.005292652981791746
108, epoch_train_loss=0.005292652981791746
Epoch 109
Epoch 109 :: Batch 0/1
Batch Loss = 0.005220786500730331
109, epoch_train_loss=0.005220786500730331
Epoch 110
Epoch 110 :: Batch 0/1
Batch Loss = 0.005154724320866941
110, epoch_train_loss=0.005154724320866941
Epoch 111
Epoch 111 :: Batch 0/1
Batch Loss = 0.005089675771539307
111, epoch_train_loss=0.005089675771539307
Epoch 112
Epoch 112 :: Batch 0/1
Batch Loss = 0.005023849838493707
112, epoch_train_loss=0.005023849838493707
Epoch 113
Epoch 113 :: Batch 0/1
Batch Loss = 0.004961614403083206
113, epoch_train_loss=0.004961614403083206
Epoch 114
Epoch 114 :: Batch 0/1
Batch Loss = 0.0048997633433819935
114, epoch_train_loss=0.0048997633433819935
Epoch 115
Epoch 115 :: Batch 0/1
Batch Loss = 0.00483624046331843
115, epoch_train_loss=0.00483624046331843
Epoch 116
Epoch 116 :: Batch 0/1
Batch Loss = 0.004774706548202516
116, epoch_train_loss=0.004774706548202516
Epoch 117
Epoch 117 :: Batch 0/1
Batch Loss = 0.0047147930777043335
117, epoch_train_loss=0.0047147930777043335
Epoch 118
Epoch 118 :: Batch 0/1
Batch Loss = 0.004656405094958222
118, epoch_train_loss=0.004656405094958222
Epoch 119
Epoch 119 :: Batch 0/1
Batch Loss = 0.004600565655054397
119, epoch_train_loss=0.004600565655054397
Epoch 120
Epoch 120 :: Batch 0/1
Batch Loss = 0.004546320980213042
120, epoch_train_loss=0.004546320980213042
Epoch 121
Epoch 121 :: Batch 0/1
Batch Loss = 0.004495122911281517
121, epoch_train_loss=0.004495122911281517
Epoch 122
Epoch 122 :: Batch 0/1
Batch Loss = 0.004446934868303264
122, epoch_train_loss=0.004446934868303264
Epoch 123
Epoch 123 :: Batch 0/1
Batch Loss = 0.0043995579129386155
123, epoch_train_loss=0.0043995579129386155
Epoch 124
Epoch 124 :: Batch 0/1
Batch Loss = 0.004354179016666128
124, epoch_train_loss=0.004354179016666128
Epoch 125
Epoch 125 :: Batch 0/1
Batch Loss = 0.004310997189672507
125, epoch_train_loss=0.004310997189672507
Epoch 126
Epoch 126 :: Batch 0/1
Batch Loss = 0.004268561884481188
126, epoch_train_loss=0.004268561884481188
Epoch 127
Epoch 127 :: Batch 0/1
Batch Loss = 0.004227164611844878
127, epoch_train_loss=0.004227164611844878
Epoch 128
Epoch 128 :: Batch 0/1
Batch Loss = 0.004186520023342047
128, epoch_train_loss=0.004186520023342047
Epoch 129
Epoch 129 :: Batch 0/1
Batch Loss = 0.004146586667772544
129, epoch_train_loss=0.004146586667772544
Epoch 130
Epoch 130 :: Batch 0/1
Batch Loss = 0.004107676762717358
130, epoch_train_loss=0.004107676762717358
Epoch 131
Epoch 131 :: Batch 0/1
Batch Loss = 0.00406913828334035
131, epoch_train_loss=0.00406913828334035
Epoch 132
Epoch 132 :: Batch 0/1
Batch Loss = 0.0040316469529024625
132, epoch_train_loss=0.0040316469529024625
Epoch 133
Epoch 133 :: Batch 0/1
Batch Loss = 0.00399566705774069
133, epoch_train_loss=0.00399566705774069
Epoch 134
Epoch 134 :: Batch 0/1
Batch Loss = 0.003960569565787518
134, epoch_train_loss=0.003960569565787518
Epoch 135
Epoch 135 :: Batch 0/1
Batch Loss = 0.0039267410074969806
135, epoch_train_loss=0.0039267410074969806
Epoch 136
Epoch 136 :: Batch 0/1
Batch Loss = 0.0038942597143355752
136, epoch_train_loss=0.0038942597143355752
Epoch 137
Epoch 137 :: Batch 0/1
Batch Loss = 0.0038629201529696245
137, epoch_train_loss=0.0038629201529696245
Epoch 138
Epoch 138 :: Batch 0/1
Batch Loss = 0.003832849249837209
138, epoch_train_loss=0.003832849249837209
Epoch 139
Epoch 139 :: Batch 0/1
Batch Loss = 0.0038040793033702574
139, epoch_train_loss=0.0038040793033702574
Epoch 140
Epoch 140 :: Batch 0/1
Batch Loss = 0.003777695936567504
140, epoch_train_loss=0.003777695936567504
Epoch 141
Epoch 141 :: Batch 0/1
Batch Loss = 0.0037561701113379745
141, epoch_train_loss=0.0037561701113379745
Epoch 142
Epoch 142 :: Batch 0/1
Batch Loss = 0.0037456407218430456
142, epoch_train_loss=0.0037456407218430456
Epoch 143
Epoch 143 :: Batch 0/1
Batch Loss = 0.0037659198289833923
143, epoch_train_loss=0.0037659198289833923
Epoch 144
Epoch 144 :: Batch 0/1
Batch Loss = 0.003866128626946981
144, epoch_train_loss=0.003866128626946981
Epoch 145
Epoch 145 :: Batch 0/1
Batch Loss = 0.004206041719729668
145, epoch_train_loss=0.004206041719729668
Epoch 146
Epoch 146 :: Batch 0/1
Batch Loss = 0.005055542970264785
146, epoch_train_loss=0.005055542970264785
Epoch 147
Epoch 147 :: Batch 0/1
Batch Loss = 0.007173002312356127
147, epoch_train_loss=0.007173002312356127
Epoch 148
Epoch 148 :: Batch 0/1
Batch Loss = 0.00887627175728729
148, epoch_train_loss=0.00887627175728729
Epoch 149
Epoch 149 :: Batch 0/1
Batch Loss = 0.00962217248440791
149, epoch_train_loss=0.00962217248440791
Epoch 150
Epoch 150 :: Batch 0/1
Batch Loss = 0.005054565167760891
150, epoch_train_loss=0.005054565167760891
Epoch 151
Epoch 151 :: Batch 0/1
Batch Loss = 0.0037166003089605945
151, epoch_train_loss=0.0037166003089605945
Epoch 152
Epoch 152 :: Batch 0/1
Batch Loss = 0.006312664174401687
152, epoch_train_loss=0.006312664174401687
Epoch 153
Epoch 153 :: Batch 0/1
Batch Loss = 0.005919343183084972
153, epoch_train_loss=0.005919343183084972
Epoch 154
Epoch 154 :: Batch 0/1
Batch Loss = 0.0037355903834039236
154, epoch_train_loss=0.0037355903834039236
Epoch 155
Epoch 155 :: Batch 0/1
Batch Loss = 0.004045932900348945
155, epoch_train_loss=0.004045932900348945
Epoch 156
Epoch 156 :: Batch 0/1
Batch Loss = 0.005200088290257076
156, epoch_train_loss=0.005200088290257076
Epoch 157
Epoch 157 :: Batch 0/1
Batch Loss = 0.004167300844703697
157, epoch_train_loss=0.004167300844703697
Epoch 158
Epoch 158 :: Batch 0/1
Batch Loss = 0.0034800697741439283
158, epoch_train_loss=0.0034800697741439283
Epoch 159
Epoch 159 :: Batch 0/1
Batch Loss = 0.00458588780184108
159, epoch_train_loss=0.00458588780184108
Epoch 160
Epoch 160 :: Batch 0/1
Batch Loss = 0.004231016476095254
160, epoch_train_loss=0.004231016476095254
Epoch 161
Epoch 161 :: Batch 0/1
Batch Loss = 0.0033543428079798065
161, epoch_train_loss=0.0033543428079798065
Epoch 162
Epoch 162 :: Batch 0/1
Batch Loss = 0.004214588820680752
162, epoch_train_loss=0.004214588820680752
Epoch 163
Epoch 163 :: Batch 0/1
Batch Loss = 0.004102466271122931
163, epoch_train_loss=0.004102466271122931
Epoch 164
Epoch 164 :: Batch 0/1
Batch Loss = 0.0033032846848768914
164, epoch_train_loss=0.0033032846848768914
Epoch 165
Epoch 165 :: Batch 0/1
Batch Loss = 0.0039865052050977266
165, epoch_train_loss=0.0039865052050977266
Epoch 166
Epoch 166 :: Batch 0/1
Batch Loss = 0.003912206469001362
166, epoch_train_loss=0.003912206469001362
Epoch 167
Epoch 167 :: Batch 0/1
Batch Loss = 0.0032616316206143126
167, epoch_train_loss=0.0032616316206143126
Epoch 168
Epoch 168 :: Batch 0/1
Batch Loss = 0.00382833380289201
168, epoch_train_loss=0.00382833380289201
Epoch 169
Epoch 169 :: Batch 0/1
Batch Loss = 0.0037279735615482488
169, epoch_train_loss=0.0037279735615482488
Epoch 170
Epoch 170 :: Batch 0/1
Batch Loss = 0.003223730602781152
170, epoch_train_loss=0.003223730602781152
Epoch 171
Epoch 171 :: Batch 0/1
Batch Loss = 0.0036946122872275147
171, epoch_train_loss=0.0036946122872275147
Epoch 172
Epoch 172 :: Batch 0/1
Batch Loss = 0.003566476310847463
172, epoch_train_loss=0.003566476310847463
Epoch 173
Epoch 173 :: Batch 0/1
Batch Loss = 0.0031894487502386482
173, epoch_train_loss=0.0031894487502386482
Epoch 174
Epoch 174 :: Batch 0/1
Batch Loss = 0.003574370898366277
174, epoch_train_loss=0.003574370898366277
Epoch 175
Epoch 175 :: Batch 0/1
Batch Loss = 0.003434431729185232
175, epoch_train_loss=0.003434431729185232
Epoch 176
Epoch 176 :: Batch 0/1
Batch Loss = 0.003155596995058139
176, epoch_train_loss=0.003155596995058139
Epoch 177
Epoch 177 :: Batch 0/1
Batch Loss = 0.0034626643157126785
177, epoch_train_loss=0.0034626643157126785
Epoch 178
Epoch 178 :: Batch 0/1
Batch Loss = 0.003331713633272643
178, epoch_train_loss=0.003331713633272643
Epoch 179
Epoch 179 :: Batch 0/1
Batch Loss = 0.0031211833373098008
179, epoch_train_loss=0.0031211833373098008
Epoch 180
Epoch 180 :: Batch 0/1
Batch Loss = 0.0033606774986805316
180, epoch_train_loss=0.0033606774986805316
Epoch 181
Epoch 181 :: Batch 0/1
Batch Loss = 0.0032517298291996305
181, epoch_train_loss=0.0032517298291996305
Epoch 182
Epoch 182 :: Batch 0/1
Batch Loss = 0.0030867478238555874
182, epoch_train_loss=0.0030867478238555874
Epoch 183
Epoch 183 :: Batch 0/1
Batch Loss = 0.003270657305629986
183, epoch_train_loss=0.003270657305629986
Epoch 184
Epoch 184 :: Batch 0/1
Batch Loss = 0.003190331054305483
184, epoch_train_loss=0.003190331054305483
Epoch 185
Epoch 185 :: Batch 0/1
Batch Loss = 0.003052964120236713
185, epoch_train_loss=0.003052964120236713
Epoch 186
Epoch 186 :: Batch 0/1
Batch Loss = 0.0031911580557018324
186, epoch_train_loss=0.0031911580557018324
Epoch 187
Epoch 187 :: Batch 0/1
Batch Loss = 0.0031417446808497365
187, epoch_train_loss=0.0031417446808497365
Epoch 188
Epoch 188 :: Batch 0/1
Batch Loss = 0.0030219069713360064
188, epoch_train_loss=0.0030219069713360064
Epoch 189
Epoch 189 :: Batch 0/1
Batch Loss = 0.0031213751125579382
189, epoch_train_loss=0.0031213751125579382
Epoch 190
Epoch 190 :: Batch 0/1
Batch Loss = 0.003101181581103402
190, epoch_train_loss=0.003101181581103402
Epoch 191
Epoch 191 :: Batch 0/1
Batch Loss = 0.00299501944509147
191, epoch_train_loss=0.00299501944509147
Epoch 192
Epoch 192 :: Batch 0/1
Batch Loss = 0.0030595585719264988
192, epoch_train_loss=0.0030595585719264988
Epoch 193
Epoch 193 :: Batch 0/1
Batch Loss = 0.003064624526760412
193, epoch_train_loss=0.003064624526760412
Epoch 194
Epoch 194 :: Batch 0/1
Batch Loss = 0.0029733306896211676
194, epoch_train_loss=0.0029733306896211676
Epoch 195
Epoch 195 :: Batch 0/1
Batch Loss = 0.0030051284162075378
195, epoch_train_loss=0.0030051284162075378
Epoch 196
Epoch 196 :: Batch 0/1
Batch Loss = 0.003028305765247362
196, epoch_train_loss=0.003028305765247362
Epoch 197
Epoch 197 :: Batch 0/1
Batch Loss = 0.0029565381216553896
197, epoch_train_loss=0.0029565381216553896
Epoch 198
Epoch 198 :: Batch 0/1
Batch Loss = 0.0029589745771236966
198, epoch_train_loss=0.0029589745771236966
Epoch 199
Epoch 199 :: Batch 0/1
Batch Loss = 0.0029902023246775053
199, epoch_train_loss=0.0029902023246775053
Epoch 200
Epoch 200 :: Batch 0/1
Batch Loss = 0.0029429013285952347
200, epoch_train_loss=0.0029429013285952347
Epoch 201
Epoch 201 :: Batch 0/1
Batch Loss = 0.0029226308957219483
201, epoch_train_loss=0.0029226308957219483
Epoch 202
Epoch 202 :: Batch 0/1
Batch Loss = 0.002950049682705898
202, epoch_train_loss=0.002950049682705898
Epoch 203
Epoch 203 :: Batch 0/1
Batch Loss = 0.0029289441371807405
203, epoch_train_loss=0.0029289441371807405
Epoch 204
Epoch 204 :: Batch 0/1
Batch Loss = 0.0028975060735225593
204, epoch_train_loss=0.0028975060735225593
Epoch 205
Epoch 205 :: Batch 0/1
Batch Loss = 0.0029107720221946175
205, epoch_train_loss=0.0029107720221946175
Epoch 206
Epoch 206 :: Batch 0/1
Batch Loss = 0.0029107614866681402
206, epoch_train_loss=0.0029107614866681402
Epoch 207
Epoch 207 :: Batch 0/1
Batch Loss = 0.0028821508875816154
207, epoch_train_loss=0.0028821508875816154
Epoch 208
Epoch 208 :: Batch 0/1
Batch Loss = 0.0028773716762997233
208, epoch_train_loss=0.0028773716762997233
Epoch 209
Epoch 209 :: Batch 0/1
Batch Loss = 0.002886282402732331
209, epoch_train_loss=0.002886282402732331
Epoch 210
Epoch 210 :: Batch 0/1
Batch Loss = 0.002871052431686547
210, epoch_train_loss=0.002871052431686547
Epoch 211
Epoch 211 :: Batch 0/1
Batch Loss = 0.00285416801871046
211, epoch_train_loss=0.00285416801871046
Epoch 212
Epoch 212 :: Batch 0/1
Batch Loss = 0.002858146589830645
212, epoch_train_loss=0.002858146589830645
Epoch 213
Epoch 213 :: Batch 0/1
Batch Loss = 0.0028567627162242506
213, epoch_train_loss=0.0028567627162242506
Epoch 214
Epoch 214 :: Batch 0/1
Batch Loss = 0.0028403308472794184
214, epoch_train_loss=0.0028403308472794184
Epoch 215
Epoch 215 :: Batch 0/1
Batch Loss = 0.002833170959404111
215, epoch_train_loss=0.002833170959404111
Epoch 216
Epoch 216 :: Batch 0/1
Batch Loss = 0.002835879821341297
216, epoch_train_loss=0.002835879821341297
Epoch 217
Epoch 217 :: Batch 0/1
Batch Loss = 0.002828977586462532
217, epoch_train_loss=0.002828977586462532
Epoch 218
Epoch 218 :: Batch 0/1
Batch Loss = 0.002816630900498993
218, epoch_train_loss=0.002816630900498993
Epoch 219
Epoch 219 :: Batch 0/1
Batch Loss = 0.002813202507302782
219, epoch_train_loss=0.002813202507302782
Epoch 220
Epoch 220 :: Batch 0/1
Batch Loss = 0.0028130235988132993
220, epoch_train_loss=0.0028130235988132993
Epoch 221
Epoch 221 :: Batch 0/1
Batch Loss = 0.002805534810518622
221, epoch_train_loss=0.002805534810518622
Epoch 222
Epoch 222 :: Batch 0/1
Batch Loss = 0.002796370153332564
222, epoch_train_loss=0.002796370153332564
Epoch 223
Epoch 223 :: Batch 0/1
Batch Loss = 0.0027935204670611782
223, epoch_train_loss=0.0027935204670611782
Epoch 224
Epoch 224 :: Batch 0/1
Batch Loss = 0.0027918416340758914
224, epoch_train_loss=0.0027918416340758914
Epoch 225
Epoch 225 :: Batch 0/1
Batch Loss = 0.0027851986088478485
225, epoch_train_loss=0.0027851986088478485
Epoch 226
Epoch 226 :: Batch 0/1
Batch Loss = 0.0027778698017280816
226, epoch_train_loss=0.0027778698017280816
Epoch 227
Epoch 227 :: Batch 0/1
Batch Loss = 0.0027746328900409034
227, epoch_train_loss=0.0027746328900409034
Epoch 228
Epoch 228 :: Batch 0/1
Batch Loss = 0.002772339105124067
228, epoch_train_loss=0.002772339105124067
Epoch 229
Epoch 229 :: Batch 0/1
Batch Loss = 0.0027670261572550784
229, epoch_train_loss=0.0027670261572550784
Epoch 230
Epoch 230 :: Batch 0/1
Batch Loss = 0.00276074080450935
230, epoch_train_loss=0.00276074080450935
Epoch 231
Epoch 231 :: Batch 0/1
Batch Loss = 0.00275690679335904
231, epoch_train_loss=0.00275690679335904
Epoch 232
Epoch 232 :: Batch 0/1
Batch Loss = 0.0027543798110366406
232, epoch_train_loss=0.0027543798110366406
Epoch 233
Epoch 233 :: Batch 0/1
Batch Loss = 0.002750262953742235
233, epoch_train_loss=0.002750262953742235
Epoch 234
Epoch 234 :: Batch 0/1
Batch Loss = 0.002744887723323504
234, epoch_train_loss=0.002744887723323504
Epoch 235
Epoch 235 :: Batch 0/1
Batch Loss = 0.00274055636818976
235, epoch_train_loss=0.00274055636818976
Epoch 236
Epoch 236 :: Batch 0/1
Batch Loss = 0.0027376430646574683
236, epoch_train_loss=0.0027376430646574683
Epoch 237
Epoch 237 :: Batch 0/1
Batch Loss = 0.0027344469029240495
237, epoch_train_loss=0.0027344469029240495
Epoch 238
Epoch 238 :: Batch 0/1
Batch Loss = 0.0027301122074831767
238, epoch_train_loss=0.0027301122074831767
Epoch 239
Epoch 239 :: Batch 0/1
Batch Loss = 0.0027256843871628226
239, epoch_train_loss=0.0027256843871628226
Epoch 240
Epoch 240 :: Batch 0/1
Batch Loss = 0.0027222026003537233
240, epoch_train_loss=0.0027222026003537233
Epoch 241
Epoch 241 :: Batch 0/1
Batch Loss = 0.002719290268052577
241, epoch_train_loss=0.002719290268052577
Epoch 242
Epoch 242 :: Batch 0/1
Batch Loss = 0.002715974739906322
242, epoch_train_loss=0.002715974739906322
Epoch 243
Epoch 243 :: Batch 0/1
Batch Loss = 0.0027120784016663175
243, epoch_train_loss=0.0027120784016663175
Epoch 244
Epoch 244 :: Batch 0/1
Batch Loss = 0.0027082758064047993
244, epoch_train_loss=0.0027082758064047993
Epoch 245
Epoch 245 :: Batch 0/1
Batch Loss = 0.0027050468914124105
245, epoch_train_loss=0.0027050468914124105
Epoch 246
Epoch 246 :: Batch 0/1
Batch Loss = 0.002702145764527466
246, epoch_train_loss=0.002702145764527466
Epoch 247
Epoch 247 :: Batch 0/1
Batch Loss = 0.002699058083144931
247, epoch_train_loss=0.002699058083144931
Epoch 248
Epoch 248 :: Batch 0/1
Batch Loss = 0.0026956457801011243
248, epoch_train_loss=0.0026956457801011243
Epoch 249
Epoch 249 :: Batch 0/1
Batch Loss = 0.0026922188985299424
249, epoch_train_loss=0.0026922188985299424
Epoch 250
Epoch 250 :: Batch 0/1
Batch Loss = 0.002689085138338243
250, epoch_train_loss=0.002689085138338243
Epoch 251
Epoch 251 :: Batch 0/1
Batch Loss = 0.0026862303052643363
251, epoch_train_loss=0.0026862303052643363
Epoch 252
Epoch 252 :: Batch 0/1
Batch Loss = 0.0026834087938726898
252, epoch_train_loss=0.0026834087938726898
Epoch 253
Epoch 253 :: Batch 0/1
Batch Loss = 0.002680438688659814
253, epoch_train_loss=0.002680438688659814
Epoch 254
Epoch 254 :: Batch 0/1
Batch Loss = 0.0026773623822206184
254, epoch_train_loss=0.0026773623822206184
Epoch 255
Epoch 255 :: Batch 0/1
Batch Loss = 0.0026743457423988536
255, epoch_train_loss=0.0026743457423988536
Epoch 256
Epoch 256 :: Batch 0/1
Batch Loss = 0.0026715030828285457
256, epoch_train_loss=0.0026715030828285457
Epoch 257
Epoch 257 :: Batch 0/1
Batch Loss = 0.0026688101023015897
257, epoch_train_loss=0.0026688101023015897
Epoch 258
Epoch 258 :: Batch 0/1
Batch Loss = 0.002666158939132579
258, epoch_train_loss=0.002666158939132579
Epoch 259
Epoch 259 :: Batch 0/1
Batch Loss = 0.0026634654837012924
259, epoch_train_loss=0.0026634654837012924
Epoch 260
Epoch 260 :: Batch 0/1
Batch Loss = 0.0026607235557353113
260, epoch_train_loss=0.0026607235557353113
Epoch 261
Epoch 261 :: Batch 0/1
Batch Loss = 0.0026579905764220237
261, epoch_train_loss=0.0026579905764220237
Epoch 262
Epoch 262 :: Batch 0/1
Batch Loss = 0.002655327480419372
262, epoch_train_loss=0.002655327480419372
Epoch 263
Epoch 263 :: Batch 0/1
Batch Loss = 0.0026527591044860944
263, epoch_train_loss=0.0026527591044860944
Epoch 264
Epoch 264 :: Batch 0/1
Batch Loss = 0.002650267768852842
264, epoch_train_loss=0.002650267768852842
Epoch 265
Epoch 265 :: Batch 0/1
Batch Loss = 0.002647815544454635
265, epoch_train_loss=0.002647815544454635
Epoch 266
Epoch 266 :: Batch 0/1
Batch Loss = 0.0026453716738950965
266, epoch_train_loss=0.0026453716738950965
Epoch 267
Epoch 267 :: Batch 0/1
Batch Loss = 0.0026429249513234797
267, epoch_train_loss=0.0026429249513234797
Epoch 268
Epoch 268 :: Batch 0/1
Batch Loss = 0.0026404844333208595
268, epoch_train_loss=0.0026404844333208595
Epoch 269
Epoch 269 :: Batch 0/1
Batch Loss = 0.0026380667749187573
269, epoch_train_loss=0.0026380667749187573
Epoch 270
Epoch 270 :: Batch 0/1
Batch Loss = 0.0026356877523952345
270, epoch_train_loss=0.0026356877523952345
Epoch 271
Epoch 271 :: Batch 0/1
Batch Loss = 0.0026333549422994586
271, epoch_train_loss=0.0026333549422994586
Epoch 272
Epoch 272 :: Batch 0/1
Batch Loss = 0.0026310681579454795
272, epoch_train_loss=0.0026310681579454795
Epoch 273
Epoch 273 :: Batch 0/1
Batch Loss = 0.002628821991439991
273, epoch_train_loss=0.002628821991439991
Epoch 274
Epoch 274 :: Batch 0/1
Batch Loss = 0.0026266093343248714
274, epoch_train_loss=0.0026266093343248714
Epoch 275
Epoch 275 :: Batch 0/1
Batch Loss = 0.0026244243793186596
275, epoch_train_loss=0.0026244243793186596
Epoch 276
Epoch 276 :: Batch 0/1
Batch Loss = 0.0026222631409935117
276, epoch_train_loss=0.0026222631409935117
Epoch 277
Epoch 277 :: Batch 0/1
Batch Loss = 0.0026201244423112173
277, epoch_train_loss=0.0026201244423112173
Epoch 278
Epoch 278 :: Batch 0/1
Batch Loss = 0.0026180080091278038
278, epoch_train_loss=0.0026180080091278038
Epoch 279
Epoch 279 :: Batch 0/1
Batch Loss = 0.0026159153834537253
279, epoch_train_loss=0.0026159153834537253
Epoch 280
Epoch 280 :: Batch 0/1
Batch Loss = 0.0026138476207527577
280, epoch_train_loss=0.0026138476207527577
Epoch 281
Epoch 281 :: Batch 0/1
Batch Loss = 0.0026118070709709703
281, epoch_train_loss=0.0026118070709709703
Epoch 282
Epoch 282 :: Batch 0/1
Batch Loss = 0.002609795398472633
282, epoch_train_loss=0.002609795398472633
Epoch 283
Epoch 283 :: Batch 0/1
Batch Loss = 0.002607816373501825
283, epoch_train_loss=0.002607816373501825
Epoch 284
Epoch 284 :: Batch 0/1
Batch Loss = 0.0026058741362303454
284, epoch_train_loss=0.0026058741362303454
Epoch 285
Epoch 285 :: Batch 0/1
Batch Loss = 0.0026039778512153607
285, epoch_train_loss=0.0026039778512153607
Epoch 286
Epoch 286 :: Batch 0/1
Batch Loss = 0.0026021406559336544
286, epoch_train_loss=0.0026021406559336544
Epoch 287
Epoch 287 :: Batch 0/1
Batch Loss = 0.0026003900228970226
287, epoch_train_loss=0.0026003900228970226
Epoch 288
Epoch 288 :: Batch 0/1
Batch Loss = 0.002598769946833383
288, epoch_train_loss=0.002598769946833383
Epoch 289
Epoch 289 :: Batch 0/1
Batch Loss = 0.002597372351162934
289, epoch_train_loss=0.002597372351162934
Epoch 290
Epoch 290 :: Batch 0/1
Batch Loss = 0.0025963532759942097
290, epoch_train_loss=0.0025963532759942097
Epoch 291
Epoch 291 :: Batch 0/1
Batch Loss = 0.0025960515386201123
291, epoch_train_loss=0.0025960515386201123
Epoch 292
Epoch 292 :: Batch 0/1
Batch Loss = 0.0025970600018439417
292, epoch_train_loss=0.0025970600018439417
Epoch 293
Epoch 293 :: Batch 0/1
Batch Loss = 0.00260075370345792
293, epoch_train_loss=0.00260075370345792
Epoch 294
Epoch 294 :: Batch 0/1
Batch Loss = 0.002609538252759885
294, epoch_train_loss=0.002609538252759885
Epoch 295
Epoch 295 :: Batch 0/1
Batch Loss = 0.002629556349302064
295, epoch_train_loss=0.002629556349302064
Epoch 296
Epoch 296 :: Batch 0/1
Batch Loss = 0.002671009341782374
296, epoch_train_loss=0.002671009341782374
Epoch 297
Epoch 297 :: Batch 0/1
Batch Loss = 0.002764003507039318
297, epoch_train_loss=0.002764003507039318
Epoch 298
Epoch 298 :: Batch 0/1
Batch Loss = 0.0029505529319348587
298, epoch_train_loss=0.0029505529319348587
Epoch 299
Epoch 299 :: Batch 0/1
Batch Loss = 0.0033883410971713717
299, epoch_train_loss=0.0033883410971713717
Epoch 300
Epoch 300 :: Batch 0/1
Batch Loss = 0.00419213805801978
300, epoch_train_loss=0.00419213805801978
Epoch 301
Epoch 301 :: Batch 0/1
Batch Loss = 0.006130469836578118
301, epoch_train_loss=0.006130469836578118
Epoch 302
Epoch 302 :: Batch 0/1
Batch Loss = 0.008479062663643084
302, epoch_train_loss=0.008479062663643084
Epoch 303
Epoch 303 :: Batch 0/1
Batch Loss = 0.013261801380459752
303, epoch_train_loss=0.013261801380459752
Epoch 304
Epoch 304 :: Batch 0/1
Batch Loss = 0.011483951821998835
304, epoch_train_loss=0.011483951821998835
Epoch 305
Epoch 305 :: Batch 0/1
Batch Loss = 0.008277968779056396
305, epoch_train_loss=0.008277968779056396
Epoch 306
Epoch 306 :: Batch 0/1
Batch Loss = 0.0029353747892851135
306, epoch_train_loss=0.0029353747892851135
Epoch 307
Epoch 307 :: Batch 0/1
Batch Loss = 0.004371265370802119
307, epoch_train_loss=0.004371265370802119
Epoch 308
Epoch 308 :: Batch 0/1
Batch Loss = 0.008190846142591461
308, epoch_train_loss=0.008190846142591461
Epoch 309
Epoch 309 :: Batch 0/1
Batch Loss = 0.004840094821825097
309, epoch_train_loss=0.004840094821825097
Epoch 310
Epoch 310 :: Batch 0/1
Batch Loss = 0.002664388568386046
310, epoch_train_loss=0.002664388568386046
Epoch 311
Epoch 311 :: Batch 0/1
Batch Loss = 0.0049984495955926805
311, epoch_train_loss=0.0049984495955926805
Epoch 312
Epoch 312 :: Batch 0/1
Batch Loss = 0.004272333496141179
312, epoch_train_loss=0.004272333496141179
Epoch 313
Epoch 313 :: Batch 0/1
Batch Loss = 0.0026139412402043805
313, epoch_train_loss=0.0026139412402043805
Epoch 314
Epoch 314 :: Batch 0/1
Batch Loss = 0.0039037230755729663
314, epoch_train_loss=0.0039037230755729663
Epoch 315
Epoch 315 :: Batch 0/1
Batch Loss = 0.0036100799646332556
315, epoch_train_loss=0.0036100799646332556
Epoch 316
Epoch 316 :: Batch 0/1
Batch Loss = 0.0026241985491665034
316, epoch_train_loss=0.0026241985491665034
Epoch 317
Epoch 317 :: Batch 0/1
Batch Loss = 0.003485705665868762
317, epoch_train_loss=0.003485705665868762
Epoch 318
Epoch 318 :: Batch 0/1
Batch Loss = 0.0030794014502903244
318, epoch_train_loss=0.0030794014502903244
Epoch 319
Epoch 319 :: Batch 0/1
Batch Loss = 0.0026921005396842784
319, epoch_train_loss=0.0026921005396842784
Epoch 320
Epoch 320 :: Batch 0/1
Batch Loss = 0.003275017544425201
320, epoch_train_loss=0.003275017544425201
Epoch 321
Epoch 321 :: Batch 0/1
Batch Loss = 0.0027526785541254132
321, epoch_train_loss=0.0027526785541254132
Epoch 322
Epoch 322 :: Batch 0/1
Batch Loss = 0.0028031143466835805
322, epoch_train_loss=0.0028031143466835805
Epoch 323
Epoch 323 :: Batch 0/1
Batch Loss = 0.0030625437376372235
323, epoch_train_loss=0.0030625437376372235
Epoch 324
Epoch 324 :: Batch 0/1
Batch Loss = 0.0026146979828863184
324, epoch_train_loss=0.0026146979828863184
Epoch 325
Epoch 325 :: Batch 0/1
Batch Loss = 0.0028878235212816974
325, epoch_train_loss=0.0028878235212816974
Epoch 326
Epoch 326 :: Batch 0/1
Batch Loss = 0.002850810769010934
326, epoch_train_loss=0.002850810769010934
Epoch 327
Epoch 327 :: Batch 0/1
Batch Loss = 0.002602838065601405
327, epoch_train_loss=0.002602838065601405
Epoch 328
Epoch 328 :: Batch 0/1
Batch Loss = 0.0028839443995766147
328, epoch_train_loss=0.0028839443995766147
Epoch 329
Epoch 329 :: Batch 0/1
Batch Loss = 0.0026899982757287912
329, epoch_train_loss=0.0026899982757287912
Epoch 330
Epoch 330 :: Batch 0/1
Batch Loss = 0.0026392675186681478
330, epoch_train_loss=0.0026392675186681478
Epoch 331
Epoch 331 :: Batch 0/1
Batch Loss = 0.0028048747984425826
331, epoch_train_loss=0.0028048747984425826
Epoch 332
Epoch 332 :: Batch 0/1
Batch Loss = 0.002597555486392704
332, epoch_train_loss=0.002597555486392704
Epoch 333
Epoch 333 :: Batch 0/1
Batch Loss = 0.002667882471369032
333, epoch_train_loss=0.002667882471369032
Epoch 334
Epoch 334 :: Batch 0/1
Batch Loss = 0.002706305829504037
334, epoch_train_loss=0.002706305829504037
Epoch 335
Epoch 335 :: Batch 0/1
Batch Loss = 0.0025636630705495717
335, epoch_train_loss=0.0025636630705495717
Epoch 336
Epoch 336 :: Batch 0/1
Batch Loss = 0.0026664385575397953
336, epoch_train_loss=0.0026664385575397953
Epoch 337
Epoch 337 :: Batch 0/1
Batch Loss = 0.002627868721261105
337, epoch_train_loss=0.002627868721261105
Epoch 338
Epoch 338 :: Batch 0/1
Batch Loss = 0.002560724671675947
338, epoch_train_loss=0.002560724671675947
Epoch 339
Epoch 339 :: Batch 0/1
Batch Loss = 0.0026439490506633394
339, epoch_train_loss=0.0026439490506633394
Epoch 340
Epoch 340 :: Batch 0/1
Batch Loss = 0.002577902263821248
340, epoch_train_loss=0.002577902263821248
Epoch 341
Epoch 341 :: Batch 0/1
Batch Loss = 0.002565708188148924
341, epoch_train_loss=0.002565708188148924
Epoch 342
Epoch 342 :: Batch 0/1
Batch Loss = 0.002614932536582832
342, epoch_train_loss=0.002614932536582832
Epoch 343
Epoch 343 :: Batch 0/1
Batch Loss = 0.0025521963593441157
343, epoch_train_loss=0.0025521963593441157
Epoch 344
Epoch 344 :: Batch 0/1
Batch Loss = 0.002567163122436121
344, epoch_train_loss=0.002567163122436121
Epoch 345
Epoch 345 :: Batch 0/1
Batch Loss = 0.002587629189016236
345, epoch_train_loss=0.002587629189016236
Epoch 346
Epoch 346 :: Batch 0/1
Batch Loss = 0.0025397746073766085
346, epoch_train_loss=0.0025397746073766085
Epoch 347
Epoch 347 :: Batch 0/1
Batch Loss = 0.002563868865292172
347, epoch_train_loss=0.002563868865292172
Epoch 348
Epoch 348 :: Batch 0/1
Batch Loss = 0.0025663639430992336
348, epoch_train_loss=0.0025663639430992336
Epoch 349
Epoch 349 :: Batch 0/1
Batch Loss = 0.002533487802130205
349, epoch_train_loss=0.002533487802130205
Epoch 350
Epoch 350 :: Batch 0/1
Batch Loss = 0.0025569163453857888
350, epoch_train_loss=0.0025569163453857888
Epoch 351
Epoch 351 :: Batch 0/1
Batch Loss = 0.0025510303461346823
351, epoch_train_loss=0.0025510303461346823
Epoch 352
Epoch 352 :: Batch 0/1
Batch Loss = 0.002529301286273609
352, epoch_train_loss=0.002529301286273609
Epoch 353
Epoch 353 :: Batch 0/1
Batch Loss = 0.002548471420069849
353, epoch_train_loss=0.002548471420069849
Epoch 354
Epoch 354 :: Batch 0/1
Batch Loss = 0.0025399415439424753
354, epoch_train_loss=0.0025399415439424753
Epoch 355
Epoch 355 :: Batch 0/1
Batch Loss = 0.002525552964610503
355, epoch_train_loss=0.002525552964610503
Epoch 356
Epoch 356 :: Batch 0/1
Batch Loss = 0.002540043761118838
356, epoch_train_loss=0.002540043761118838
Epoch 357
Epoch 357 :: Batch 0/1
Batch Loss = 0.002531793047869276
357, epoch_train_loss=0.002531793047869276
Epoch 358
Epoch 358 :: Batch 0/1
Batch Loss = 0.0025216822084604376
358, epoch_train_loss=0.0025216822084604376
Epoch 359
Epoch 359 :: Batch 0/1
Batch Loss = 0.0025321643394848924
359, epoch_train_loss=0.0025321643394848924
Epoch 360
Epoch 360 :: Batch 0/1
Batch Loss = 0.002525366852970749
360, epoch_train_loss=0.002525366852970749
Epoch 361
Epoch 361 :: Batch 0/1
Batch Loss = 0.0025177472442095925
361, epoch_train_loss=0.0025177472442095925
Epoch 362
Epoch 362 :: Batch 0/1
Batch Loss = 0.002525143570838097
362, epoch_train_loss=0.002525143570838097
Epoch 363
Epoch 363 :: Batch 0/1
Batch Loss = 0.0025200108527400986
363, epoch_train_loss=0.0025200108527400986
Epoch 364
Epoch 364 :: Batch 0/1
Batch Loss = 0.0025137886998518873
364, epoch_train_loss=0.0025137886998518873
Epoch 365
Epoch 365 :: Batch 0/1
Batch Loss = 0.0025188871807077778
365, epoch_train_loss=0.0025188871807077778
Epoch 366
Epoch 366 :: Batch 0/1
Batch Loss = 0.0025153421751311633
366, epoch_train_loss=0.0025153421751311633
Epoch 367
Epoch 367 :: Batch 0/1
Batch Loss = 0.002509987716530749
367, epoch_train_loss=0.002509987716530749
Epoch 368
Epoch 368 :: Batch 0/1
Batch Loss = 0.002513315621179955
368, epoch_train_loss=0.002513315621179955
Epoch 369
Epoch 369 :: Batch 0/1
Batch Loss = 0.002511086324599453
369, epoch_train_loss=0.002511086324599453
Epoch 370
Epoch 370 :: Batch 0/1
Batch Loss = 0.0025063746180452465
370, epoch_train_loss=0.0025063746180452465
Epoch 371
Epoch 371 :: Batch 0/1
Batch Loss = 0.0025082919974272503
371, epoch_train_loss=0.0025082919974272503
Epoch 372
Epoch 372 :: Batch 0/1
Batch Loss = 0.002507110655114162
372, epoch_train_loss=0.002507110655114162
Epoch 373
Epoch 373 :: Batch 0/1
Batch Loss = 0.0025029991710628736
373, epoch_train_loss=0.0025029991710628736
Epoch 374
Epoch 374 :: Batch 0/1
Batch Loss = 0.0025037414255420455
374, epoch_train_loss=0.0025037414255420455
Epoch 375
Epoch 375 :: Batch 0/1
Batch Loss = 0.002503288935666927
375, epoch_train_loss=0.002503288935666927
Epoch 376
Epoch 376 :: Batch 0/1
Batch Loss = 0.002499831408438172
376, epoch_train_loss=0.002499831408438172
Epoch 377
Epoch 377 :: Batch 0/1
Batch Loss = 0.00249961700564871
377, epoch_train_loss=0.00249961700564871
Epoch 378
Epoch 378 :: Batch 0/1
Batch Loss = 0.002499581429574548
378, epoch_train_loss=0.002499581429574548
Epoch 379
Epoch 379 :: Batch 0/1
Batch Loss = 0.0024968334100352444
379, epoch_train_loss=0.0024968334100352444
Epoch 380
Epoch 380 :: Batch 0/1
Batch Loss = 0.0024958842858399566
380, epoch_train_loss=0.0024958842858399566
Epoch 381
Epoch 381 :: Batch 0/1
Batch Loss = 0.0024959615462234928
381, epoch_train_loss=0.0024959615462234928
Epoch 382
Epoch 382 :: Batch 0/1
Batch Loss = 0.0024939371133599884
382, epoch_train_loss=0.0024939371133599884
Epoch 383
Epoch 383 :: Batch 0/1
Batch Loss = 0.0024925186934208307
383, epoch_train_loss=0.0024925186934208307
Epoch 384
Epoch 384 :: Batch 0/1
Batch Loss = 0.00249245564380159
384, epoch_train_loss=0.00249245564380159
Epoch 385
Epoch 385 :: Batch 0/1
Batch Loss = 0.0024910807186283475
385, epoch_train_loss=0.0024910807186283475
Epoch 386
Epoch 386 :: Batch 0/1
Batch Loss = 0.00248946542701146
386, epoch_train_loss=0.00248946542701146
Epoch 387
Epoch 387 :: Batch 0/1
Batch Loss = 0.00248909495336685
387, epoch_train_loss=0.00248909495336685
Epoch 388
Epoch 388 :: Batch 0/1
Batch Loss = 0.0024882094599190667
388, epoch_train_loss=0.0024882094599190667
Epoch 389
Epoch 389 :: Batch 0/1
Batch Loss = 0.002486652790491209
389, epoch_train_loss=0.002486652790491209
Epoch 390
Epoch 390 :: Batch 0/1
Batch Loss = 0.0024859276066220557
390, epoch_train_loss=0.0024859276066220557
Epoch 391
Epoch 391 :: Batch 0/1
Batch Loss = 0.0024853088007600437
391, epoch_train_loss=0.0024853088007600437
Epoch 392
Epoch 392 :: Batch 0/1
Batch Loss = 0.0024839862869667387
392, epoch_train_loss=0.0024839862869667387
Epoch 393
Epoch 393 :: Batch 0/1
Batch Loss = 0.002482979584858429
393, epoch_train_loss=0.002482979584858429
Epoch 394
Epoch 394 :: Batch 0/1
Batch Loss = 0.0024824033392596046
394, epoch_train_loss=0.0024824033392596046
Epoch 395
Epoch 395 :: Batch 0/1
Batch Loss = 0.0024813751728728554
395, epoch_train_loss=0.0024813751728728554
Epoch 396
Epoch 396 :: Batch 0/1
Batch Loss = 0.0024802452525763976
396, epoch_train_loss=0.0024802452525763976
Epoch 397
Epoch 397 :: Batch 0/1
Batch Loss = 0.0024795533248375707
397, epoch_train_loss=0.0024795533248375707
Epoch 398
Epoch 398 :: Batch 0/1
Batch Loss = 0.002478757674421751
398, epoch_train_loss=0.002478757674421751
Epoch 399
Epoch 399 :: Batch 0/1
Batch Loss = 0.0024776749736607133
399, epoch_train_loss=0.0024776749736607133
Epoch 400
Epoch 400 :: Batch 0/1
Batch Loss = 0.002476824867683045
400, epoch_train_loss=0.002476824867683045
Epoch 401
Epoch 401 :: Batch 0/1
Batch Loss = 0.0024761248510009166
401, epoch_train_loss=0.0024761248510009166
Epoch 402
Epoch 402 :: Batch 0/1
Batch Loss = 0.0024751919960035416
402, epoch_train_loss=0.0024751919960035416
Epoch 403
Epoch 403 :: Batch 0/1
Batch Loss = 0.002474250952834302
403, epoch_train_loss=0.002474250952834302
Epoch 404
Epoch 404 :: Batch 0/1
Batch Loss = 0.0024735183306698483
404, epoch_train_loss=0.0024735183306698483
Epoch 405
Epoch 405 :: Batch 0/1
Batch Loss = 0.0024727308892225288
405, epoch_train_loss=0.0024727308892225288
Epoch 406
Epoch 406 :: Batch 0/1
Batch Loss = 0.0024718122042357
406, epoch_train_loss=0.0024718122042357
Epoch 407
Epoch 407 :: Batch 0/1
Batch Loss = 0.0024709994173857993
407, epoch_train_loss=0.0024709994173857993
Epoch 408
Epoch 408 :: Batch 0/1
Batch Loss = 0.0024702731647380763
408, epoch_train_loss=0.0024702731647380763
Epoch 409
Epoch 409 :: Batch 0/1
Batch Loss = 0.002469449202730205
409, epoch_train_loss=0.002469449202730205
Epoch 410
Epoch 410 :: Batch 0/1
Batch Loss = 0.0024686000367747404
410, epoch_train_loss=0.0024686000367747404
Epoch 411
Epoch 411 :: Batch 0/1
Batch Loss = 0.0024678524184947558
411, epoch_train_loss=0.0024678524184947558
Epoch 412
Epoch 412 :: Batch 0/1
Batch Loss = 0.00246711129995003
412, epoch_train_loss=0.00246711129995003
Epoch 413
Epoch 413 :: Batch 0/1
Batch Loss = 0.00246630010717539
413, epoch_train_loss=0.00246630010717539
Epoch 414
Epoch 414 :: Batch 0/1
Batch Loss = 0.0024655154432999767
414, epoch_train_loss=0.0024655154432999767
Epoch 415
Epoch 415 :: Batch 0/1
Batch Loss = 0.0024647946899042407
415, epoch_train_loss=0.0024647946899042407
Epoch 416
Epoch 416 :: Batch 0/1
Batch Loss = 0.00246405128338866
416, epoch_train_loss=0.00246405128338866
Epoch 417
Epoch 417 :: Batch 0/1
Batch Loss = 0.0024632740711813413
417, epoch_train_loss=0.0024632740711813413
Epoch 418
Epoch 418 :: Batch 0/1
Batch Loss = 0.002462534459903937
418, epoch_train_loss=0.002462534459903937
Epoch 419
Epoch 419 :: Batch 0/1
Batch Loss = 0.0024618278795880823
419, epoch_train_loss=0.0024618278795880823
Epoch 420
Epoch 420 :: Batch 0/1
Batch Loss = 0.0024610974763062264
420, epoch_train_loss=0.0024610974763062264
Epoch 421
Epoch 421 :: Batch 0/1
Batch Loss = 0.002460355877517894
421, epoch_train_loss=0.002460355877517894
Epoch 422
Epoch 422 :: Batch 0/1
Batch Loss = 0.002459647048390512
422, epoch_train_loss=0.002459647048390512
Epoch 423
Epoch 423 :: Batch 0/1
Batch Loss = 0.0024589543836909533
423, epoch_train_loss=0.0024589543836909533
Epoch 424
Epoch 424 :: Batch 0/1
Batch Loss = 0.002458244536516126
424, epoch_train_loss=0.002458244536516126
Epoch 425
Epoch 425 :: Batch 0/1
Batch Loss = 0.002457534528933536
425, epoch_train_loss=0.002457534528933536
Epoch 426
Epoch 426 :: Batch 0/1
Batch Loss = 0.0024568488842883
426, epoch_train_loss=0.0024568488842883
Epoch 427
Epoch 427 :: Batch 0/1
Batch Loss = 0.0024561721777225385
427, epoch_train_loss=0.0024561721777225385
Epoch 428
Epoch 428 :: Batch 0/1
Batch Loss = 0.002455485797854111
428, epoch_train_loss=0.002455485797854111
Epoch 429
Epoch 429 :: Batch 0/1
Batch Loss = 0.002454802417490205
429, epoch_train_loss=0.002454802417490205
Epoch 430
Epoch 430 :: Batch 0/1
Batch Loss = 0.0024541366332203438
430, epoch_train_loss=0.0024541366332203438
Epoch 431
Epoch 431 :: Batch 0/1
Batch Loss = 0.002453477951006527
431, epoch_train_loss=0.002453477951006527
Epoch 432
Epoch 432 :: Batch 0/1
Batch Loss = 0.0024528144366506095
432, epoch_train_loss=0.0024528144366506095
Epoch 433
Epoch 433 :: Batch 0/1
Batch Loss = 0.0024521543715735243
433, epoch_train_loss=0.0024521543715735243
Epoch 434
Epoch 434 :: Batch 0/1
Batch Loss = 0.002451507285737422
434, epoch_train_loss=0.002451507285737422
Epoch 435
Epoch 435 :: Batch 0/1
Batch Loss = 0.0024508669125336787
435, epoch_train_loss=0.0024508669125336787
Epoch 436
Epoch 436 :: Batch 0/1
Batch Loss = 0.0024502251677020206
436, epoch_train_loss=0.0024502251677020206
Epoch 437
Epoch 437 :: Batch 0/1
Batch Loss = 0.0024495862723459038
437, epoch_train_loss=0.0024495862723459038
Epoch 438
Epoch 438 :: Batch 0/1
Batch Loss = 0.00244895712369504
438, epoch_train_loss=0.00244895712369504
Epoch 439
Epoch 439 :: Batch 0/1
Batch Loss = 0.002448334775036047
439, epoch_train_loss=0.002448334775036047
Epoch 440
Epoch 440 :: Batch 0/1
Batch Loss = 0.0024477132951420846
440, epoch_train_loss=0.0024477132951420846
Epoch 441
Epoch 441 :: Batch 0/1
Batch Loss = 0.002447094312285426
441, epoch_train_loss=0.002447094312285426
Epoch 442
Epoch 442 :: Batch 0/1
Batch Loss = 0.0024464824717370377
442, epoch_train_loss=0.0024464824717370377
Epoch 443
Epoch 443 :: Batch 0/1
Batch Loss = 0.0024458772692383165
443, epoch_train_loss=0.0024458772692383165
Epoch 444
Epoch 444 :: Batch 0/1
Batch Loss = 0.0024452748460024177
444, epoch_train_loss=0.0024452748460024177
Epoch 445
Epoch 445 :: Batch 0/1
Batch Loss = 0.0024446746232426597
445, epoch_train_loss=0.0024446746232426597
Epoch 446
Epoch 446 :: Batch 0/1
Batch Loss = 0.002444079622205596
446, epoch_train_loss=0.002444079622205596
Epoch 447
Epoch 447 :: Batch 0/1
Batch Loss = 0.0024434908013758936
447, epoch_train_loss=0.0024434908013758936
Epoch 448
Epoch 448 :: Batch 0/1
Batch Loss = 0.0024429058925085575
448, epoch_train_loss=0.0024429058925085575
Epoch 449
Epoch 449 :: Batch 0/1
Batch Loss = 0.0024423235409013317
449, epoch_train_loss=0.0024423235409013317
Epoch 450
Epoch 450 :: Batch 0/1
Batch Loss = 0.002441745016537825
450, epoch_train_loss=0.002441745016537825
Epoch 451
Epoch 451 :: Batch 0/1
Batch Loss = 0.0024411717611198115
451, epoch_train_loss=0.0024411717611198115
Epoch 452
Epoch 452 :: Batch 0/1
Batch Loss = 0.0024406030568003274
452, epoch_train_loss=0.0024406030568003274
Epoch 453
Epoch 453 :: Batch 0/1
Batch Loss = 0.0024400374594511745
453, epoch_train_loss=0.0024400374594511745
Epoch 454
Epoch 454 :: Batch 0/1
Batch Loss = 0.002439475049914281
454, epoch_train_loss=0.002439475049914281
Epoch 455
Epoch 455 :: Batch 0/1
Batch Loss = 0.0024389168231384595
455, epoch_train_loss=0.0024389168231384595
Epoch 456
Epoch 456 :: Batch 0/1
Batch Loss = 0.002438363094071589
456, epoch_train_loss=0.002438363094071589
Epoch 457
Epoch 457 :: Batch 0/1
Batch Loss = 0.0024378130894337196
457, epoch_train_loss=0.0024378130894337196
Epoch 458
Epoch 458 :: Batch 0/1
Batch Loss = 0.002437266142017741
458, epoch_train_loss=0.002437266142017741
Epoch 459
Epoch 459 :: Batch 0/1
Batch Loss = 0.0024367226137586755
459, epoch_train_loss=0.0024367226137586755
Epoch 460
Epoch 460 :: Batch 0/1
Batch Loss = 0.0024361830354161372
460, epoch_train_loss=0.0024361830354161372
Epoch 461
Epoch 461 :: Batch 0/1
Batch Loss = 0.002435647329207884
461, epoch_train_loss=0.002435647329207884
Epoch 462
Epoch 462 :: Batch 0/1
Batch Loss = 0.002435114983428557
462, epoch_train_loss=0.002435114983428557
Epoch 463
Epoch 463 :: Batch 0/1
Batch Loss = 0.0024345857133342775
463, epoch_train_loss=0.0024345857133342775
Epoch 464
Epoch 464 :: Batch 0/1
Batch Loss = 0.0024340597808762353
464, epoch_train_loss=0.0024340597808762353
Epoch 465
Epoch 465 :: Batch 0/1
Batch Loss = 0.0024335374298976066
465, epoch_train_loss=0.0024335374298976066
Epoch 466
Epoch 466 :: Batch 0/1
Batch Loss = 0.002433018554069197
466, epoch_train_loss=0.002433018554069197
Epoch 467
Epoch 467 :: Batch 0/1
Batch Loss = 0.002432502843361574
467, epoch_train_loss=0.002432502843361574
Epoch 468
Epoch 468 :: Batch 0/1
Batch Loss = 0.002431990126745696
468, epoch_train_loss=0.002431990126745696
Epoch 469
Epoch 469 :: Batch 0/1
Batch Loss = 0.002431480548390157
469, epoch_train_loss=0.002431480548390157
Epoch 470
Epoch 470 :: Batch 0/1
Batch Loss = 0.0024309742207171098
470, epoch_train_loss=0.0024309742207171098
Epoch 471
Epoch 471 :: Batch 0/1
Batch Loss = 0.002430471077456006
471, epoch_train_loss=0.002430471077456006
Epoch 472
Epoch 472 :: Batch 0/1
Batch Loss = 0.002429970938315689
472, epoch_train_loss=0.002429970938315689
Epoch 473
Epoch 473 :: Batch 0/1
Batch Loss = 0.0024294736638770967
473, epoch_train_loss=0.0024294736638770967
Epoch 474
Epoch 474 :: Batch 0/1
Batch Loss = 0.0024289793002657037
474, epoch_train_loss=0.0024289793002657037
Epoch 475
Epoch 475 :: Batch 0/1
Batch Loss = 0.0024284879010385884
475, epoch_train_loss=0.0024284879010385884
Epoch 476
Epoch 476 :: Batch 0/1
Batch Loss = 0.0024279994439931553
476, epoch_train_loss=0.0024279994439931553
Epoch 477
Epoch 477 :: Batch 0/1
Batch Loss = 0.0024275138226338274
477, epoch_train_loss=0.0024275138226338274
Epoch 478
Epoch 478 :: Batch 0/1
Batch Loss = 0.0024270309215448675
478, epoch_train_loss=0.0024270309215448675
Epoch 479
Epoch 479 :: Batch 0/1
Batch Loss = 0.0024265507238002094
479, epoch_train_loss=0.0024265507238002094
Epoch 480
Epoch 480 :: Batch 0/1
Batch Loss = 0.0024260732350403417
480, epoch_train_loss=0.0024260732350403417
Epoch 481
Epoch 481 :: Batch 0/1
Batch Loss = 0.0024255984593866283
481, epoch_train_loss=0.0024255984593866283
Epoch 482
Epoch 482 :: Batch 0/1
Batch Loss = 0.0024251263412356323
482, epoch_train_loss=0.0024251263412356323
Epoch 483
Epoch 483 :: Batch 0/1
Batch Loss = 0.0024246567940249095
483, epoch_train_loss=0.0024246567940249095
Epoch 484
Epoch 484 :: Batch 0/1
Batch Loss = 0.002424189770771565
484, epoch_train_loss=0.002424189770771565
Epoch 485
Epoch 485 :: Batch 0/1
Batch Loss = 0.002423725239022711
485, epoch_train_loss=0.002423725239022711
Epoch 486
Epoch 486 :: Batch 0/1
Batch Loss = 0.002423263198663012
486, epoch_train_loss=0.002423263198663012
Epoch 487
Epoch 487 :: Batch 0/1
Batch Loss = 0.0024228036227528383
487, epoch_train_loss=0.0024228036227528383
Epoch 488
Epoch 488 :: Batch 0/1
Batch Loss = 0.0024223464619060487
488, epoch_train_loss=0.0024223464619060487
Epoch 489
Epoch 489 :: Batch 0/1
Batch Loss = 0.0024218916655136306
489, epoch_train_loss=0.0024218916655136306
Epoch 490
Epoch 490 :: Batch 0/1
Batch Loss = 0.002421439182352431
490, epoch_train_loss=0.002421439182352431
Epoch 491
Epoch 491 :: Batch 0/1
Batch Loss = 0.0024209889921443235
491, epoch_train_loss=0.0024209889921443235
Epoch 492
Epoch 492 :: Batch 0/1
Batch Loss = 0.0024205410683235994
492, epoch_train_loss=0.0024205410683235994
Epoch 493
Epoch 493 :: Batch 0/1
Batch Loss = 0.0024200953878289327
493, epoch_train_loss=0.0024200953878289327
Epoch 494
Epoch 494 :: Batch 0/1
Batch Loss = 0.002419651913614131
494, epoch_train_loss=0.002419651913614131
Epoch 495
Epoch 495 :: Batch 0/1
Batch Loss = 0.0024192106003756175
495, epoch_train_loss=0.0024192106003756175
Epoch 496
Epoch 496 :: Batch 0/1
Batch Loss = 0.0024187714138761223
496, epoch_train_loss=0.0024187714138761223
Epoch 497
Epoch 497 :: Batch 0/1
Batch Loss = 0.0024183343164949664
497, epoch_train_loss=0.0024183343164949664
Epoch 498
Epoch 498 :: Batch 0/1
Batch Loss = 0.0024178992866765946
498, epoch_train_loss=0.0024178992866765946
Epoch 499
Epoch 499 :: Batch 0/1
Batch Loss = 0.002417466294931064
499, epoch_train_loss=0.002417466294931064
Epoch 500
Epoch 500 :: Batch 0/1
Batch Loss = 0.002417035312968949
500, epoch_train_loss=0.002417035312968949
Epoch 501
Epoch 501 :: Batch 0/1
Batch Loss = 0.002416606308170728
501, epoch_train_loss=0.002416606308170728
Epoch 502
Epoch 502 :: Batch 0/1
Batch Loss = 0.0024161792441318034
502, epoch_train_loss=0.0024161792441318034
Epoch 503
Epoch 503 :: Batch 0/1
Batch Loss = 0.0024157540917435208
503, epoch_train_loss=0.0024157540917435208
Epoch 504
Epoch 504 :: Batch 0/1
Batch Loss = 0.0024153308177654468
504, epoch_train_loss=0.0024153308177654468
Epoch 505
Epoch 505 :: Batch 0/1
Batch Loss = 0.0024149093986387616
505, epoch_train_loss=0.0024149093986387616
Epoch 506
Epoch 506 :: Batch 0/1
Batch Loss = 0.0024144898049953357
506, epoch_train_loss=0.0024144898049953357
Epoch 507
Epoch 507 :: Batch 0/1
Batch Loss = 0.002414072010976981
507, epoch_train_loss=0.002414072010976981
Epoch 508
Epoch 508 :: Batch 0/1
Batch Loss = 0.0024136559878796724
508, epoch_train_loss=0.0024136559878796724
Epoch 509
Epoch 509 :: Batch 0/1
Batch Loss = 0.0024132417056870136
509, epoch_train_loss=0.0024132417056870136
Epoch 510
Epoch 510 :: Batch 0/1
Batch Loss = 0.0024128291375577504
510, epoch_train_loss=0.0024128291375577504
Epoch 511
Epoch 511 :: Batch 0/1
Batch Loss = 0.002412418253568494
511, epoch_train_loss=0.002412418253568494
Epoch 512
Epoch 512 :: Batch 0/1
Batch Loss = 0.002412009029761175
512, epoch_train_loss=0.002412009029761175
Epoch 513
Epoch 513 :: Batch 0/1
Batch Loss = 0.0024116014383023915
513, epoch_train_loss=0.0024116014383023915
Epoch 514
Epoch 514 :: Batch 0/1
Batch Loss = 0.0024111954559099327
514, epoch_train_loss=0.0024111954559099327
Epoch 515
Epoch 515 :: Batch 0/1
Batch Loss = 0.002410791056255843
515, epoch_train_loss=0.002410791056255843
Epoch 516
Epoch 516 :: Batch 0/1
Batch Loss = 0.0024103882146993792
516, epoch_train_loss=0.0024103882146993792
Epoch 517
Epoch 517 :: Batch 0/1
Batch Loss = 0.0024099869062577848
517, epoch_train_loss=0.0024099869062577848
Epoch 518
Epoch 518 :: Batch 0/1
Batch Loss = 0.0024095871052636202
518, epoch_train_loss=0.0024095871052636202
Epoch 519
Epoch 519 :: Batch 0/1
Batch Loss = 0.0024091887884186963
519, epoch_train_loss=0.0024091887884186963
Epoch 520
Epoch 520 :: Batch 0/1
Batch Loss = 0.0024087919302797065
520, epoch_train_loss=0.0024087919302797065
Epoch 521
Epoch 521 :: Batch 0/1
Batch Loss = 0.002408396508972879
521, epoch_train_loss=0.002408396508972879
Epoch 522
Epoch 522 :: Batch 0/1
Batch Loss = 0.002408002499936529
522, epoch_train_loss=0.002408002499936529
Epoch 523
Epoch 523 :: Batch 0/1
Batch Loss = 0.002407609881950817
523, epoch_train_loss=0.002407609881950817
Epoch 524
Epoch 524 :: Batch 0/1
Batch Loss = 0.0024072186314630806
524, epoch_train_loss=0.0024072186314630806
Epoch 525
Epoch 525 :: Batch 0/1
Batch Loss = 0.0024068287273442523
525, epoch_train_loss=0.0024068287273442523
Epoch 526
Epoch 526 :: Batch 0/1
Batch Loss = 0.002406440147256806
526, epoch_train_loss=0.002406440147256806
Epoch 527
Epoch 527 :: Batch 0/1
Batch Loss = 0.002406052869830982
527, epoch_train_loss=0.002406052869830982
Epoch 528
Epoch 528 :: Batch 0/1
Batch Loss = 0.0024056668739423073
528, epoch_train_loss=0.0024056668739423073
Epoch 529
Epoch 529 :: Batch 0/1
Batch Loss = 0.0024052821382561083
529, epoch_train_loss=0.0024052821382561083
Epoch 530
Epoch 530 :: Batch 0/1
Batch Loss = 0.002404898642734296
530, epoch_train_loss=0.002404898642734296
Epoch 531
Epoch 531 :: Batch 0/1
Batch Loss = 0.002404516366349743
531, epoch_train_loss=0.002404516366349743
Epoch 532
Epoch 532 :: Batch 0/1
Batch Loss = 0.0024041352899077236
532, epoch_train_loss=0.0024041352899077236
Epoch 533
Epoch 533 :: Batch 0/1
Batch Loss = 0.0024037553928696023
533, epoch_train_loss=0.0024037553928696023
Epoch 534
Epoch 534 :: Batch 0/1
Batch Loss = 0.00240337665686079
534, epoch_train_loss=0.00240337665686079
Epoch 535
Epoch 535 :: Batch 0/1
Batch Loss = 0.0024029990617601485
535, epoch_train_loss=0.0024029990617601485
Epoch 536
Epoch 536 :: Batch 0/1
Batch Loss = 0.0024026225898656456
536, epoch_train_loss=0.0024026225898656456
Epoch 537
Epoch 537 :: Batch 0/1
Batch Loss = 0.002402247221678453
537, epoch_train_loss=0.002402247221678453
Epoch 538
Epoch 538 :: Batch 0/1
Batch Loss = 0.0024018729399777994
538, epoch_train_loss=0.0024018729399777994
Epoch 539
Epoch 539 :: Batch 0/1
Batch Loss = 0.002401499725920015
539, epoch_train_loss=0.002401499725920015
Epoch 540
Epoch 540 :: Batch 0/1
Batch Loss = 0.002401127562786065
540, epoch_train_loss=0.002401127562786065
Epoch 541
Epoch 541 :: Batch 0/1
Batch Loss = 0.0024007564322750443
541, epoch_train_loss=0.0024007564322750443
Epoch 542
Epoch 542 :: Batch 0/1
Batch Loss = 0.002400386318247421
542, epoch_train_loss=0.002400386318247421
Epoch 543
Epoch 543 :: Batch 0/1
Batch Loss = 0.002400017202923862
543, epoch_train_loss=0.002400017202923862
Epoch 544
Epoch 544 :: Batch 0/1
Batch Loss = 0.0023996490707978903
544, epoch_train_loss=0.0023996490707978903
Epoch 545
Epoch 545 :: Batch 0/1
Batch Loss = 0.0023992819044786886
545, epoch_train_loss=0.0023992819044786886
Epoch 546
Epoch 546 :: Batch 0/1
Batch Loss = 0.0023989156891665504
546, epoch_train_loss=0.0023989156891665504
Epoch 547
Epoch 547 :: Batch 0/1
Batch Loss = 0.002398550407901863
547, epoch_train_loss=0.002398550407901863
Epoch 548
Epoch 548 :: Batch 0/1
Batch Loss = 0.002398186046697004
548, epoch_train_loss=0.002398186046697004
Epoch 549
Epoch 549 :: Batch 0/1
Batch Loss = 0.0023978225887898645
549, epoch_train_loss=0.0023978225887898645
Epoch 550
Epoch 550 :: Batch 0/1
Batch Loss = 0.0023974600214202117
550, epoch_train_loss=0.0023974600214202117
Epoch 551
Epoch 551 :: Batch 0/1
Batch Loss = 0.0023970983280626897
551, epoch_train_loss=0.0023970983280626897
Epoch 552
Epoch 552 :: Batch 0/1
Batch Loss = 0.002396737497784791
552, epoch_train_loss=0.002396737497784791
Epoch 553
Epoch 553 :: Batch 0/1
Batch Loss = 0.002396377514418806
553, epoch_train_loss=0.002396377514418806
Epoch 554
Epoch 554 :: Batch 0/1
Batch Loss = 0.002396018370448117
554, epoch_train_loss=0.002396018370448117
Epoch 555
Epoch 555 :: Batch 0/1
Batch Loss = 0.002395660051403748
555, epoch_train_loss=0.002395660051403748
Epoch 556
Epoch 556 :: Batch 0/1
Batch Loss = 0.0023953025574847044
556, epoch_train_loss=0.0023953025574847044
Epoch 557
Epoch 557 :: Batch 0/1
Batch Loss = 0.0023949458809331887
557, epoch_train_loss=0.0023949458809331887
Epoch 558
Epoch 558 :: Batch 0/1
Batch Loss = 0.0023945900427321818
558, epoch_train_loss=0.0023945900427321818
Epoch 559
Epoch 559 :: Batch 0/1
Batch Loss = 0.0023942350610874174
559, epoch_train_loss=0.0023942350610874174
Epoch 560
Epoch 560 :: Batch 0/1
Batch Loss = 0.002393881020875267
560, epoch_train_loss=0.002393881020875267
Epoch 561
Epoch 561 :: Batch 0/1
Batch Loss = 0.002393528038565907
561, epoch_train_loss=0.002393528038565907
Epoch 562
Epoch 562 :: Batch 0/1
Batch Loss = 0.0023931764153167392
562, epoch_train_loss=0.0023931764153167392
Epoch 563
Epoch 563 :: Batch 0/1
Batch Loss = 0.002392826643582968
563, epoch_train_loss=0.002392826643582968
Epoch 564
Epoch 564 :: Batch 0/1
Batch Loss = 0.0023924798136241163
564, epoch_train_loss=0.0023924798136241163
Epoch 565
Epoch 565 :: Batch 0/1
Batch Loss = 0.0023921378919135013
565, epoch_train_loss=0.0023921378919135013
Epoch 566
Epoch 566 :: Batch 0/1
Batch Loss = 0.0023918050291765547
566, epoch_train_loss=0.0023918050291765547
Epoch 567
Epoch 567 :: Batch 0/1
Batch Loss = 0.002391489166507561
567, epoch_train_loss=0.002391489166507561
Epoch 568
Epoch 568 :: Batch 0/1
Batch Loss = 0.0023912069705638375
568, epoch_train_loss=0.0023912069705638375
Epoch 569
Epoch 569 :: Batch 0/1
Batch Loss = 0.002390991505425971
569, epoch_train_loss=0.002390991505425971
Epoch 570
Epoch 570 :: Batch 0/1
Batch Loss = 0.002390913208423768
570, epoch_train_loss=0.002390913208423768
Epoch 571
Epoch 571 :: Batch 0/1
Batch Loss = 0.0023911147478220683
571, epoch_train_loss=0.0023911147478220683
Epoch 572
Epoch 572 :: Batch 0/1
Batch Loss = 0.0023919090031286373
572, epoch_train_loss=0.0023919090031286373
Epoch 573
Epoch 573 :: Batch 0/1
Batch Loss = 0.002393933191057896
573, epoch_train_loss=0.002393933191057896
Epoch 574
Epoch 574 :: Batch 0/1
Batch Loss = 0.002398649220702141
574, epoch_train_loss=0.002398649220702141
Epoch 575
Epoch 575 :: Batch 0/1
Batch Loss = 0.0024089820769093214
575, epoch_train_loss=0.0024089820769093214
Epoch 576
Epoch 576 :: Batch 0/1
Batch Loss = 0.0024321409760133354
576, epoch_train_loss=0.0024321409760133354
Epoch 577
Epoch 577 :: Batch 0/1
Batch Loss = 0.002481644723358046
577, epoch_train_loss=0.002481644723358046
Epoch 578
Epoch 578 :: Batch 0/1
Batch Loss = 0.0025951333672910907
578, epoch_train_loss=0.0025951333672910907
Epoch 579
Epoch 579 :: Batch 0/1
Batch Loss = 0.002831357631471235
579, epoch_train_loss=0.002831357631471235
Epoch 580
Epoch 580 :: Batch 0/1
Batch Loss = 0.003394965666862638
580, epoch_train_loss=0.003394965666862638
Epoch 581
Epoch 581 :: Batch 0/1
Batch Loss = 0.004452588396889764
581, epoch_train_loss=0.004452588396889764
Epoch 582
Epoch 582 :: Batch 0/1
Batch Loss = 0.007022771664028597
582, epoch_train_loss=0.007022771664028597
Epoch 583
Epoch 583 :: Batch 0/1
Batch Loss = 0.010087875232275351
583, epoch_train_loss=0.010087875232275351
Epoch 584
Epoch 584 :: Batch 0/1
Batch Loss = 0.01632585141542773
584, epoch_train_loss=0.01632585141542773
Epoch 585
Epoch 585 :: Batch 0/1
Batch Loss = 0.013130883611907167
585, epoch_train_loss=0.013130883611907167
Epoch 586
Epoch 586 :: Batch 0/1
Batch Loss = 0.00817218940687325
586, epoch_train_loss=0.00817218940687325
Epoch 587
Epoch 587 :: Batch 0/1
Batch Loss = 0.002481971513365299
587, epoch_train_loss=0.002481971513365299
Epoch 588
Epoch 588 :: Batch 0/1
Batch Loss = 0.0059693123103805685
588, epoch_train_loss=0.0059693123103805685
Epoch 589
Epoch 589 :: Batch 0/1
Batch Loss = 0.009376797414238247
589, epoch_train_loss=0.009376797414238247
Epoch 590
Epoch 590 :: Batch 0/1
Batch Loss = 0.0033644081170686223
590, epoch_train_loss=0.0033644081170686223
Epoch 591
Epoch 591 :: Batch 0/1
Batch Loss = 0.004482856741859753
591, epoch_train_loss=0.004482856741859753
Epoch 592
Epoch 592 :: Batch 0/1
Batch Loss = 0.006969052925056166
592, epoch_train_loss=0.006969052925056166
Epoch 593
Epoch 593 :: Batch 0/1
Batch Loss = 0.0026372434008387607
593, epoch_train_loss=0.0026372434008387607
Epoch 594
Epoch 594 :: Batch 0/1
Batch Loss = 0.005345806924044885
594, epoch_train_loss=0.005345806924044885
Epoch 595
Epoch 595 :: Batch 0/1
Batch Loss = 0.005225742015003028
595, epoch_train_loss=0.005225742015003028
Epoch 596
Epoch 596 :: Batch 0/1
Batch Loss = 0.0027569580339809586
596, epoch_train_loss=0.0027569580339809586
Epoch 597
Epoch 597 :: Batch 0/1
Batch Loss = 0.005599906210482866
597, epoch_train_loss=0.005599906210482866
Epoch 598
Epoch 598 :: Batch 0/1
Batch Loss = 0.003121543540892133
598, epoch_train_loss=0.003121543540892133
Epoch 599
Epoch 599 :: Batch 0/1
Batch Loss = 0.003911070102379367
599, epoch_train_loss=0.003911070102379367
Epoch 600
Epoch 600 :: Batch 0/1
Batch Loss = 0.003945733369649518
600, epoch_train_loss=0.003945733369649518
Epoch 601
Epoch 601 :: Batch 0/1
Batch Loss = 0.002729022831349283
601, epoch_train_loss=0.002729022831349283
Epoch 602
Epoch 602 :: Batch 0/1
Batch Loss = 0.004061124237001431
602, epoch_train_loss=0.004061124237001431
Epoch 603
Epoch 603 :: Batch 0/1
Batch Loss = 0.00255926817211114
603, epoch_train_loss=0.00255926817211114
Epoch 604
Epoch 604 :: Batch 0/1
Batch Loss = 0.0035981302987843303
604, epoch_train_loss=0.0035981302987843303
Epoch 605
Epoch 605 :: Batch 0/1
Batch Loss = 0.002940389350500169
605, epoch_train_loss=0.002940389350500169
Epoch 606
Epoch 606 :: Batch 0/1
Batch Loss = 0.002926079836203499
606, epoch_train_loss=0.002926079836203499
Epoch 607
Epoch 607 :: Batch 0/1
Batch Loss = 0.003225713687808959
607, epoch_train_loss=0.003225713687808959
Epoch 608
Epoch 608 :: Batch 0/1
Batch Loss = 0.0025270189236834876
608, epoch_train_loss=0.0025270189236834876
Epoch 609
Epoch 609 :: Batch 0/1
Batch Loss = 0.003208627121885823
609, epoch_train_loss=0.003208627121885823
Epoch 610
Epoch 610 :: Batch 0/1
Batch Loss = 0.0025070475835204543
610, epoch_train_loss=0.0025070475835204543
Epoch 611
Epoch 611 :: Batch 0/1
Batch Loss = 0.002916341600334866
611, epoch_train_loss=0.002916341600334866
Epoch 612
Epoch 612 :: Batch 0/1
Batch Loss = 0.0027309692617973266
612, epoch_train_loss=0.0027309692617973266
Epoch 613
Epoch 613 :: Batch 0/1
Batch Loss = 0.002585207584790193
613, epoch_train_loss=0.002585207584790193
Epoch 614
Epoch 614 :: Batch 0/1
Batch Loss = 0.0028477649996376387
614, epoch_train_loss=0.0028477649996376387
Epoch 615
Epoch 615 :: Batch 0/1
Batch Loss = 0.002446663613598175
615, epoch_train_loss=0.002446663613598175
Epoch 616
Epoch 616 :: Batch 0/1
Batch Loss = 0.002782521808762872
616, epoch_train_loss=0.002782521808762872
Epoch 617
Epoch 617 :: Batch 0/1
Batch Loss = 0.002505523566393655
617, epoch_train_loss=0.002505523566393655
Epoch 618
Epoch 618 :: Batch 0/1
Batch Loss = 0.002579335749189286
618, epoch_train_loss=0.002579335749189286
Epoch 619
Epoch 619 :: Batch 0/1
Batch Loss = 0.002629589915599612
619, epoch_train_loss=0.002629589915599612
Epoch 620
Epoch 620 :: Batch 0/1
Batch Loss = 0.002447280643548692
620, epoch_train_loss=0.002447280643548692
Epoch 621
Epoch 621 :: Batch 0/1
Batch Loss = 0.002636672554397832
621, epoch_train_loss=0.002636672554397832
Epoch 622
Epoch 622 :: Batch 0/1
Batch Loss = 0.0024380269198797846
622, epoch_train_loss=0.0024380269198797846
Epoch 623
Epoch 623 :: Batch 0/1
Batch Loss = 0.002555865665021292
623, epoch_train_loss=0.002555865665021292
Epoch 624
Epoch 624 :: Batch 0/1
Batch Loss = 0.002501233771323496
624, epoch_train_loss=0.002501233771323496
Epoch 625
Epoch 625 :: Batch 0/1
Batch Loss = 0.002449607560618624
625, epoch_train_loss=0.002449607560618624
Epoch 626
Epoch 626 :: Batch 0/1
Batch Loss = 0.002539542769347536
626, epoch_train_loss=0.002539542769347536
Epoch 627
Epoch 627 :: Batch 0/1
Batch Loss = 0.002419721963754193
627, epoch_train_loss=0.002419721963754193
Epoch 628
Epoch 628 :: Batch 0/1
Batch Loss = 0.002509028857585239
628, epoch_train_loss=0.002509028857585239
Epoch 629
Epoch 629 :: Batch 0/1
Batch Loss = 0.00244045037342566
629, epoch_train_loss=0.00244045037342566
Epoch 630
Epoch 630 :: Batch 0/1
Batch Loss = 0.00245378168855338
630, epoch_train_loss=0.00245378168855338
Epoch 631
Epoch 631 :: Batch 0/1
Batch Loss = 0.0024740063870144534
631, epoch_train_loss=0.0024740063870144534
Epoch 632
Epoch 632 :: Batch 0/1
Batch Loss = 0.002414153670349372
632, epoch_train_loss=0.002414153670349372
Epoch 633
Epoch 633 :: Batch 0/1
Batch Loss = 0.0024739573352045964
633, epoch_train_loss=0.0024739573352045964
Epoch 634
Epoch 634 :: Batch 0/1
Batch Loss = 0.002422834503819228
634, epoch_train_loss=0.002422834503819228
Epoch 635
Epoch 635 :: Batch 0/1
Batch Loss = 0.0024512182057995716
635, epoch_train_loss=0.0024512182057995716
Epoch 636
Epoch 636 :: Batch 0/1
Batch Loss = 0.002451156960429638
636, epoch_train_loss=0.002451156960429638
Epoch 637
Epoch 637 :: Batch 0/1
Batch Loss = 0.0024416931900477527
637, epoch_train_loss=0.0024416931900477527
Epoch 638
Epoch 638 :: Batch 0/1
Batch Loss = 0.0024885680095311346
638, epoch_train_loss=0.0024885680095311346
Epoch 639
Epoch 639 :: Batch 0/1
Batch Loss = 0.0024807021854962885
639, epoch_train_loss=0.0024807021854962885
Epoch 640
Epoch 640 :: Batch 0/1
Batch Loss = 0.0025476117962779346
640, epoch_train_loss=0.0025476117962779346
Epoch 641
Epoch 641 :: Batch 0/1
Batch Loss = 0.002611092199063204
641, epoch_train_loss=0.002611092199063204
Epoch 642
Epoch 642 :: Batch 0/1
Batch Loss = 0.002718045814331871
642, epoch_train_loss=0.002718045814331871
Epoch 643
Epoch 643 :: Batch 0/1
Batch Loss = 0.0029246905190335646
643, epoch_train_loss=0.0029246905190335646
Epoch 644
Epoch 644 :: Batch 0/1
Batch Loss = 0.003171684968797507
644, epoch_train_loss=0.003171684968797507
Epoch 645
Epoch 645 :: Batch 0/1
Batch Loss = 0.003639213362082098
645, epoch_train_loss=0.003639213362082098
Epoch 646
Epoch 646 :: Batch 0/1
Batch Loss = 0.004113732488814044
646, epoch_train_loss=0.004113732488814044
Epoch 647
Epoch 647 :: Batch 0/1
Batch Loss = 0.004854260386677534
647, epoch_train_loss=0.004854260386677534
Epoch 648
Epoch 648 :: Batch 0/1
Batch Loss = 0.0051678181131703
648, epoch_train_loss=0.0051678181131703
Epoch 649
Epoch 649 :: Batch 0/1
Batch Loss = 0.005432239927133103
649, epoch_train_loss=0.005432239927133103
Epoch 650
Epoch 650 :: Batch 0/1
Batch Loss = 0.004633575825703879
650, epoch_train_loss=0.004633575825703879
Epoch 651
Epoch 651 :: Batch 0/1
Batch Loss = 0.003674656862682582
651, epoch_train_loss=0.003674656862682582
Epoch 652
Epoch 652 :: Batch 0/1
Batch Loss = 0.0026937374650302186
652, epoch_train_loss=0.0026937374650302186
Epoch 653
Epoch 653 :: Batch 0/1
Batch Loss = 0.002402883995413765
653, epoch_train_loss=0.002402883995413765
Epoch 654
Epoch 654 :: Batch 0/1
Batch Loss = 0.0027777449998893852
654, epoch_train_loss=0.0027777449998893852
Epoch 655
Epoch 655 :: Batch 0/1
Batch Loss = 0.0033168120308697266
655, epoch_train_loss=0.0033168120308697266
Epoch 656
Epoch 656 :: Batch 0/1
Batch Loss = 0.0035635677122280015
656, epoch_train_loss=0.0035635677122280015
Epoch 657
Epoch 657 :: Batch 0/1
Batch Loss = 0.003168520679283817
657, epoch_train_loss=0.003168520679283817
Epoch 658
Epoch 658 :: Batch 0/1
Batch Loss = 0.002649629979825751
658, epoch_train_loss=0.002649629979825751
Epoch 659
Epoch 659 :: Batch 0/1
Batch Loss = 0.0023883280940587
659, epoch_train_loss=0.0023883280940587
Epoch 660
Epoch 660 :: Batch 0/1
Batch Loss = 0.0025601181046557907
660, epoch_train_loss=0.0025601181046557907
Epoch 661
Epoch 661 :: Batch 0/1
Batch Loss = 0.002876175454611348
661, epoch_train_loss=0.002876175454611348
Epoch 662
Epoch 662 :: Batch 0/1
Batch Loss = 0.0029342514568066664
662, epoch_train_loss=0.0029342514568066664
Epoch 663
Epoch 663 :: Batch 0/1
Batch Loss = 0.002731272662173258
663, epoch_train_loss=0.002731272662173258
Epoch 664
Epoch 664 :: Batch 0/1
Batch Loss = 0.0024542309320092766
664, epoch_train_loss=0.0024542309320092766
Epoch 665
Epoch 665 :: Batch 0/1
Batch Loss = 0.0023936624225106688
665, epoch_train_loss=0.0023936624225106688
Epoch 666
Epoch 666 :: Batch 0/1
Batch Loss = 0.0025388224895645595
666, epoch_train_loss=0.0025388224895645595
Epoch 667
Epoch 667 :: Batch 0/1
Batch Loss = 0.002680620066416615
667, epoch_train_loss=0.002680620066416615
Epoch 668
Epoch 668 :: Batch 0/1
Batch Loss = 0.002671078039855646
668, epoch_train_loss=0.002671078039855646
Epoch 669
Epoch 669 :: Batch 0/1
Batch Loss = 0.0025123378558542333
669, epoch_train_loss=0.0025123378558542333
Epoch 670
Epoch 670 :: Batch 0/1
Batch Loss = 0.0023919039415731687
670, epoch_train_loss=0.0023919039415731687
Epoch 671
Epoch 671 :: Batch 0/1
Batch Loss = 0.002401996114837087
671, epoch_train_loss=0.002401996114837087
Epoch 672
Epoch 672 :: Batch 0/1
Batch Loss = 0.0024970745144769877
672, epoch_train_loss=0.0024970745144769877
Epoch 673
Epoch 673 :: Batch 0/1
Batch Loss = 0.0025573272874669724
673, epoch_train_loss=0.0025573272874669724
Epoch 674
Epoch 674 :: Batch 0/1
Batch Loss = 0.002511782008697373
674, epoch_train_loss=0.002511782008697373
Epoch 675
Epoch 675 :: Batch 0/1
Batch Loss = 0.0024252697672631396
675, epoch_train_loss=0.0024252697672631396
Epoch 676
Epoch 676 :: Batch 0/1
Batch Loss = 0.0023761128015446237
676, epoch_train_loss=0.0023761128015446237
Epoch 677
Epoch 677 :: Batch 0/1
Batch Loss = 0.002400064674049289
677, epoch_train_loss=0.002400064674049289
Epoch 678
Epoch 678 :: Batch 0/1
Batch Loss = 0.002453724986432624
678, epoch_train_loss=0.002453724986432624
Epoch 679
Epoch 679 :: Batch 0/1
Batch Loss = 0.0024722491627132397
679, epoch_train_loss=0.0024722491627132397
Epoch 680
Epoch 680 :: Batch 0/1
Batch Loss = 0.002443676472418615
680, epoch_train_loss=0.002443676472418615
Epoch 681
Epoch 681 :: Batch 0/1
Batch Loss = 0.002394655213053472
681, epoch_train_loss=0.002394655213053472
Epoch 682
Epoch 682 :: Batch 0/1
Batch Loss = 0.0023724504108838945
682, epoch_train_loss=0.0023724504108838945
Epoch 683
Epoch 683 :: Batch 0/1
Batch Loss = 0.002387795995613118
683, epoch_train_loss=0.002387795995613118
Epoch 684
Epoch 684 :: Batch 0/1
Batch Loss = 0.002415273883254867
684, epoch_train_loss=0.002415273883254867
Epoch 685
Epoch 685 :: Batch 0/1
Batch Loss = 0.0024262797930614424
685, epoch_train_loss=0.0024262797930614424
Epoch 686
Epoch 686 :: Batch 0/1
Batch Loss = 0.00240937057077901
686, epoch_train_loss=0.00240937057077901
Epoch 687
Epoch 687 :: Batch 0/1
Batch Loss = 0.0023835137850562486
687, epoch_train_loss=0.0023835137850562486
Epoch 688
Epoch 688 :: Batch 0/1
Batch Loss = 0.002369602730100847
688, epoch_train_loss=0.002369602730100847
Epoch 689
Epoch 689 :: Batch 0/1
Batch Loss = 0.002374738689802522
689, epoch_train_loss=0.002374738689802522
Epoch 690
Epoch 690 :: Batch 0/1
Batch Loss = 0.0023894598731360292
690, epoch_train_loss=0.0023894598731360292
Epoch 691
Epoch 691 :: Batch 0/1
Batch Loss = 0.00239739043369343
691, epoch_train_loss=0.00239739043369343
Epoch 692
Epoch 692 :: Batch 0/1
Batch Loss = 0.0023922448589008684
692, epoch_train_loss=0.0023922448589008684
Epoch 693
Epoch 693 :: Batch 0/1
Batch Loss = 0.0023787505664304813
693, epoch_train_loss=0.0023787505664304813
Epoch 694
Epoch 694 :: Batch 0/1
Batch Loss = 0.002367885968455059
694, epoch_train_loss=0.002367885968455059
Epoch 695
Epoch 695 :: Batch 0/1
Batch Loss = 0.0023663980335186293
695, epoch_train_loss=0.0023663980335186293
Epoch 696
Epoch 696 :: Batch 0/1
Batch Loss = 0.002372427979195067
696, epoch_train_loss=0.002372427979195067
Epoch 697
Epoch 697 :: Batch 0/1
Batch Loss = 0.0023788811223690105
697, epoch_train_loss=0.0023788811223690105
Epoch 698
Epoch 698 :: Batch 0/1
Batch Loss = 0.00237992243351699
698, epoch_train_loss=0.00237992243351699
Epoch 699
Epoch 699 :: Batch 0/1
Batch Loss = 0.0023750359763056233
699, epoch_train_loss=0.0023750359763056233
Epoch 700
Epoch 700 :: Batch 0/1
Batch Loss = 0.0023678477827495884
700, epoch_train_loss=0.0023678477827495884
Epoch 701
Epoch 701 :: Batch 0/1
Batch Loss = 0.002363231789176532
701, epoch_train_loss=0.002363231789176532
Epoch 702
Epoch 702 :: Batch 0/1
Batch Loss = 0.0023629938957764668
702, epoch_train_loss=0.0023629938957764668
Epoch 703
Epoch 703 :: Batch 0/1
Batch Loss = 0.0023658819823048653
703, epoch_train_loss=0.0023658819823048653
Epoch 704
Epoch 704 :: Batch 0/1
Batch Loss = 0.002368749361673167
704, epoch_train_loss=0.002368749361673167
Epoch 705
Epoch 705 :: Batch 0/1
Batch Loss = 0.0023690720025125243
705, epoch_train_loss=0.0023690720025125243
Epoch 706
Epoch 706 :: Batch 0/1
Batch Loss = 0.002366789693739393
706, epoch_train_loss=0.002366789693739393
Epoch 707
Epoch 707 :: Batch 0/1
Batch Loss = 0.002363181771269442
707, epoch_train_loss=0.002363181771269442
Epoch 708
Epoch 708 :: Batch 0/1
Batch Loss = 0.0023603080800469775
708, epoch_train_loss=0.0023603080800469775
Epoch 709
Epoch 709 :: Batch 0/1
Batch Loss = 0.0023592966939831343
709, epoch_train_loss=0.0023592966939831343
Epoch 710
Epoch 710 :: Batch 0/1
Batch Loss = 0.0023599182748821924
710, epoch_train_loss=0.0023599182748821924
Epoch 711
Epoch 711 :: Batch 0/1
Batch Loss = 0.0023611784521402625
711, epoch_train_loss=0.0023611784521402625
Epoch 712
Epoch 712 :: Batch 0/1
Batch Loss = 0.002361873211469065
712, epoch_train_loss=0.002361873211469065
Epoch 713
Epoch 713 :: Batch 0/1
Batch Loss = 0.0023614230793952196
713, epoch_train_loss=0.0023614230793952196
Epoch 714
Epoch 714 :: Batch 0/1
Batch Loss = 0.0023599740671596155
714, epoch_train_loss=0.0023599740671596155
Epoch 715
Epoch 715 :: Batch 0/1
Batch Loss = 0.002358163986616583
715, epoch_train_loss=0.002358163986616583
Epoch 716
Epoch 716 :: Batch 0/1
Batch Loss = 0.0023566764483767756
716, epoch_train_loss=0.0023566764483767756
Epoch 717
Epoch 717 :: Batch 0/1
Batch Loss = 0.002355921909165237
717, epoch_train_loss=0.002355921909165237
Epoch 718
Epoch 718 :: Batch 0/1
Batch Loss = 0.0023558344358213116
718, epoch_train_loss=0.0023558344358213116
Epoch 719
Epoch 719 :: Batch 0/1
Batch Loss = 0.0023561064620929665
719, epoch_train_loss=0.0023561064620929665
Epoch 720
Epoch 720 :: Batch 0/1
Batch Loss = 0.002356332597642857
720, epoch_train_loss=0.002356332597642857
Epoch 721
Epoch 721 :: Batch 0/1
Batch Loss = 0.002356203428100451
721, epoch_train_loss=0.002356203428100451
Epoch 722
Epoch 722 :: Batch 0/1
Batch Loss = 0.0023556783580470355
722, epoch_train_loss=0.0023556783580470355
Epoch 723
Epoch 723 :: Batch 0/1
Batch Loss = 0.0023548387566715095
723, epoch_train_loss=0.0023548387566715095
Epoch 724
Epoch 724 :: Batch 0/1
Batch Loss = 0.00235390023364593
724, epoch_train_loss=0.00235390023364593
Epoch 725
Epoch 725 :: Batch 0/1
Batch Loss = 0.0023530647200387124
725, epoch_train_loss=0.0023530647200387124
Epoch 726
Epoch 726 :: Batch 0/1
Batch Loss = 0.00235244203357783
726, epoch_train_loss=0.00235244203357783
Epoch 727
Epoch 727 :: Batch 0/1
Batch Loss = 0.0023520580091259953
727, epoch_train_loss=0.0023520580091259953
Epoch 728
Epoch 728 :: Batch 0/1
Batch Loss = 0.002351854744044017
728, epoch_train_loss=0.002351854744044017
Epoch 729
Epoch 729 :: Batch 0/1
Batch Loss = 0.002351722917216072
729, epoch_train_loss=0.002351722917216072
Epoch 730
Epoch 730 :: Batch 0/1
Batch Loss = 0.00235157274082107
730, epoch_train_loss=0.00235157274082107
Epoch 731
Epoch 731 :: Batch 0/1
Batch Loss = 0.002351340300856791
731, epoch_train_loss=0.002351340300856791
Epoch 732
Epoch 732 :: Batch 0/1
Batch Loss = 0.0023509929887146706
732, epoch_train_loss=0.0023509929887146706
Epoch 733
Epoch 733 :: Batch 0/1
Batch Loss = 0.002350553746713512
733, epoch_train_loss=0.002350553746713512
Epoch 734
Epoch 734 :: Batch 0/1
Batch Loss = 0.0023500474272293245
734, epoch_train_loss=0.0023500474272293245
Epoch 735
Epoch 735 :: Batch 0/1
Batch Loss = 0.0023495167458779565
735, epoch_train_loss=0.0023495167458779565
Epoch 736
Epoch 736 :: Batch 0/1
Batch Loss = 0.002349000011108356
736, epoch_train_loss=0.002349000011108356
Epoch 737
Epoch 737 :: Batch 0/1
Batch Loss = 0.0023485197266939505
737, epoch_train_loss=0.0023485197266939505
Epoch 738
Epoch 738 :: Batch 0/1
Batch Loss = 0.0023480873299234063
738, epoch_train_loss=0.0023480873299234063
Epoch 739
Epoch 739 :: Batch 0/1
Batch Loss = 0.0023477050505026917
739, epoch_train_loss=0.0023477050505026917
Epoch 740
Epoch 740 :: Batch 0/1
Batch Loss = 0.002347361919392195
740, epoch_train_loss=0.002347361919392195
Epoch 741
Epoch 741 :: Batch 0/1
Batch Loss = 0.0023470484420148453
741, epoch_train_loss=0.0023470484420148453
Epoch 742
Epoch 742 :: Batch 0/1
Batch Loss = 0.0023467541171485052
742, epoch_train_loss=0.0023467541171485052
Epoch 743
Epoch 743 :: Batch 0/1
Batch Loss = 0.002346467093120139
743, epoch_train_loss=0.002346467093120139
Epoch 744
Epoch 744 :: Batch 0/1
Batch Loss = 0.0023461823597011613
744, epoch_train_loss=0.0023461823597011613
Epoch 745
Epoch 745 :: Batch 0/1
Batch Loss = 0.0023458948052957175
745, epoch_train_loss=0.0023458948052957175
Epoch 746
Epoch 746 :: Batch 0/1
Batch Loss = 0.002345602655102263
746, epoch_train_loss=0.002345602655102263
Epoch 747
Epoch 747 :: Batch 0/1
Batch Loss = 0.002345306833769487
747, epoch_train_loss=0.002345306833769487
Epoch 748
Epoch 748 :: Batch 0/1
Batch Loss = 0.002345009907758944
748, epoch_train_loss=0.002345009907758944
Epoch 749
Epoch 749 :: Batch 0/1
Batch Loss = 0.0023447124787834865
749, epoch_train_loss=0.0023447124787834865
Epoch 750
Epoch 750 :: Batch 0/1
Batch Loss = 0.0023444206698092955
750, epoch_train_loss=0.0023444206698092955
Epoch 751
Epoch 751 :: Batch 0/1
Batch Loss = 0.0023441357435119866
751, epoch_train_loss=0.0023441357435119866
Epoch 752
Epoch 752 :: Batch 0/1
Batch Loss = 0.002343864629948602
752, epoch_train_loss=0.002343864629948602
Epoch 753
Epoch 753 :: Batch 0/1
Batch Loss = 0.0023436114359380966
753, epoch_train_loss=0.0023436114359380966
Epoch 754
Epoch 754 :: Batch 0/1
Batch Loss = 0.002343386345746328
754, epoch_train_loss=0.002343386345746328
Epoch 755
Epoch 755 :: Batch 0/1
Batch Loss = 0.0023431964196126615
755, epoch_train_loss=0.0023431964196126615
Epoch 756
Epoch 756 :: Batch 0/1
Batch Loss = 0.002343062214242754
756, epoch_train_loss=0.002343062214242754
Epoch 757
Epoch 757 :: Batch 0/1
Batch Loss = 0.0023429997097400086
757, epoch_train_loss=0.0023429997097400086
Epoch 758
Epoch 758 :: Batch 0/1
Batch Loss = 0.002343050901193853
758, epoch_train_loss=0.002343050901193853
Epoch 759
Epoch 759 :: Batch 0/1
Batch Loss = 0.0023432551142189843
759, epoch_train_loss=0.0023432551142189843
Epoch 760
Epoch 760 :: Batch 0/1
Batch Loss = 0.002343706753167461
760, epoch_train_loss=0.002343706753167461
Epoch 761
Epoch 761 :: Batch 0/1
Batch Loss = 0.0023445000610844407
761, epoch_train_loss=0.0023445000610844407
Epoch 762
Epoch 762 :: Batch 0/1
Batch Loss = 0.0023458654545986377
762, epoch_train_loss=0.0023458654545986377
Epoch 763
Epoch 763 :: Batch 0/1
Batch Loss = 0.0023480332509323216
763, epoch_train_loss=0.0023480332509323216
Epoch 764
Epoch 764 :: Batch 0/1
Batch Loss = 0.002351599010610571
764, epoch_train_loss=0.002351599010610571
Epoch 765
Epoch 765 :: Batch 0/1
Batch Loss = 0.0023571362061081966
765, epoch_train_loss=0.0023571362061081966
Epoch 766
Epoch 766 :: Batch 0/1
Batch Loss = 0.0023662824286495936
766, epoch_train_loss=0.0023662824286495936
Epoch 767
Epoch 767 :: Batch 0/1
Batch Loss = 0.002380460411673979
767, epoch_train_loss=0.002380460411673979
Epoch 768
Epoch 768 :: Batch 0/1
Batch Loss = 0.0024044548499648735
768, epoch_train_loss=0.0024044548499648735
Epoch 769
Epoch 769 :: Batch 0/1
Batch Loss = 0.0024416186568499954
769, epoch_train_loss=0.0024416186568499954
Epoch 770
Epoch 770 :: Batch 0/1
Batch Loss = 0.0025067400530521086
770, epoch_train_loss=0.0025067400530521086
Epoch 771
Epoch 771 :: Batch 0/1
Batch Loss = 0.002606164937221209
771, epoch_train_loss=0.002606164937221209
Epoch 772
Epoch 772 :: Batch 0/1
Batch Loss = 0.002787633666602363
772, epoch_train_loss=0.002787633666602363
Epoch 773
Epoch 773 :: Batch 0/1
Batch Loss = 0.003051151677269925
773, epoch_train_loss=0.003051151677269925
Epoch 774
Epoch 774 :: Batch 0/1
Batch Loss = 0.003551678248070548
774, epoch_train_loss=0.003551678248070548
Epoch 775
Epoch 775 :: Batch 0/1
Batch Loss = 0.004183791557126584
775, epoch_train_loss=0.004183791557126584
Epoch 776
Epoch 776 :: Batch 0/1
Batch Loss = 0.005407112085531877
776, epoch_train_loss=0.005407112085531877
Epoch 777
Epoch 777 :: Batch 0/1
Batch Loss = 0.006430458151568585
777, epoch_train_loss=0.006430458151568585
Epoch 778
Epoch 778 :: Batch 0/1
Batch Loss = 0.008318542114186327
778, epoch_train_loss=0.008318542114186327
Epoch 779
Epoch 779 :: Batch 0/1
Batch Loss = 0.008177941678919777
779, epoch_train_loss=0.008177941678919777
Epoch 780
Epoch 780 :: Batch 0/1
Batch Loss = 0.008106651910320027
780, epoch_train_loss=0.008106651910320027
Epoch 781
Epoch 781 :: Batch 0/1
Batch Loss = 0.005356002806602693
781, epoch_train_loss=0.005356002806602693
Epoch 782
Epoch 782 :: Batch 0/1
Batch Loss = 0.003218386874124358
782, epoch_train_loss=0.003218386874124358
Epoch 783
Epoch 783 :: Batch 0/1
Batch Loss = 0.0023491350445349426
783, epoch_train_loss=0.0023491350445349426
Epoch 784
Epoch 784 :: Batch 0/1
Batch Loss = 0.0031304986251191086
784, epoch_train_loss=0.0031304986251191086
Epoch 785
Epoch 785 :: Batch 0/1
Batch Loss = 0.004590984961303052
785, epoch_train_loss=0.004590984961303052
Epoch 786
Epoch 786 :: Batch 0/1
Batch Loss = 0.004903513661602084
786, epoch_train_loss=0.004903513661602084
Epoch 787
Epoch 787 :: Batch 0/1
Batch Loss = 0.004332797616762776
787, epoch_train_loss=0.004332797616762776
Epoch 788
Epoch 788 :: Batch 0/1
Batch Loss = 0.002935044272554036
788, epoch_train_loss=0.002935044272554036
Epoch 789
Epoch 789 :: Batch 0/1
Batch Loss = 0.0023525219416546082
789, epoch_train_loss=0.0023525219416546082
Epoch 790
Epoch 790 :: Batch 0/1
Batch Loss = 0.002842687417063309
790, epoch_train_loss=0.002842687417063309
Epoch 791
Epoch 791 :: Batch 0/1
Batch Loss = 0.003510794128542925
791, epoch_train_loss=0.003510794128542925
Epoch 792
Epoch 792 :: Batch 0/1
Batch Loss = 0.0036080309574970447
792, epoch_train_loss=0.0036080309574970447
Epoch 793
Epoch 793 :: Batch 0/1
Batch Loss = 0.0028891205539657173
793, epoch_train_loss=0.0028891205539657173
Epoch 794
Epoch 794 :: Batch 0/1
Batch Loss = 0.0023744121421424816
794, epoch_train_loss=0.0023744121421424816
Epoch 795
Epoch 795 :: Batch 0/1
Batch Loss = 0.0024992644685276617
795, epoch_train_loss=0.0024992644685276617
Epoch 796
Epoch 796 :: Batch 0/1
Batch Loss = 0.0029349323549432943
796, epoch_train_loss=0.0029349323549432943
Epoch 797
Epoch 797 :: Batch 0/1
Batch Loss = 0.0031247428022821906
797, epoch_train_loss=0.0031247428022821906
Epoch 798
Epoch 798 :: Batch 0/1
Batch Loss = 0.0027784694379835626
798, epoch_train_loss=0.0027784694379835626
Epoch 799
Epoch 799 :: Batch 0/1
Batch Loss = 0.002407939027265111
799, epoch_train_loss=0.002407939027265111
Epoch 800
Epoch 800 :: Batch 0/1
Batch Loss = 0.002376666025528301
800, epoch_train_loss=0.002376666025528301
Epoch 801
Epoch 801 :: Batch 0/1
Batch Loss = 0.0026216345118526157
801, epoch_train_loss=0.0026216345118526157
Epoch 802
Epoch 802 :: Batch 0/1
Batch Loss = 0.002787813171427864
802, epoch_train_loss=0.002787813171427864
Epoch 803
Epoch 803 :: Batch 0/1
Batch Loss = 0.002638837017956499
803, epoch_train_loss=0.002638837017956499
Epoch 804
Epoch 804 :: Batch 0/1
Batch Loss = 0.0024093643452505055
804, epoch_train_loss=0.0024093643452505055
Epoch 805
Epoch 805 :: Batch 0/1
Batch Loss = 0.0023404968329598444
805, epoch_train_loss=0.0023404968329598444
Epoch 806
Epoch 806 :: Batch 0/1
Batch Loss = 0.00246288652353161
806, epoch_train_loss=0.00246288652353161
Epoch 807
Epoch 807 :: Batch 0/1
Batch Loss = 0.0025904854445994636
807, epoch_train_loss=0.0025904854445994636
Epoch 808
Epoch 808 :: Batch 0/1
Batch Loss = 0.0025506959579835553
808, epoch_train_loss=0.0025506959579835553
Epoch 809
Epoch 809 :: Batch 0/1
Batch Loss = 0.0024176736750892336
809, epoch_train_loss=0.0024176736750892336
Epoch 810
Epoch 810 :: Batch 0/1
Batch Loss = 0.002334923858553344
810, epoch_train_loss=0.002334923858553344
Epoch 811
Epoch 811 :: Batch 0/1
Batch Loss = 0.0023760282391072037
811, epoch_train_loss=0.0023760282391072037
Epoch 812
Epoch 812 :: Batch 0/1
Batch Loss = 0.0024620052159851423
812, epoch_train_loss=0.0024620052159851423
Epoch 813
Epoch 813 :: Batch 0/1
Batch Loss = 0.0024750258861860456
813, epoch_train_loss=0.0024750258861860456
Epoch 814
Epoch 814 :: Batch 0/1
Batch Loss = 0.0024121779958707562
814, epoch_train_loss=0.0024121779958707562
Epoch 815
Epoch 815 :: Batch 0/1
Batch Loss = 0.00234324733166881
815, epoch_train_loss=0.00234324733166881
Epoch 816
Epoch 816 :: Batch 0/1
Batch Loss = 0.002338294650255999
816, epoch_train_loss=0.002338294650255999
Epoch 817
Epoch 817 :: Batch 0/1
Batch Loss = 0.0023838540168142222
817, epoch_train_loss=0.0023838540168142222
Epoch 818
Epoch 818 :: Batch 0/1
Batch Loss = 0.002415387183548278
818, epoch_train_loss=0.002415387183548278
Epoch 819
Epoch 819 :: Batch 0/1
Batch Loss = 0.0023998872702983886
819, epoch_train_loss=0.0023998872702983886
Epoch 820
Epoch 820 :: Batch 0/1
Batch Loss = 0.0023548556065688874
820, epoch_train_loss=0.0023548556065688874
Epoch 821
Epoch 821 :: Batch 0/1
Batch Loss = 0.0023296740559810517
821, epoch_train_loss=0.0023296740559810517
Epoch 822
Epoch 822 :: Batch 0/1
Batch Loss = 0.002341894448383033
822, epoch_train_loss=0.002341894448383033
Epoch 823
Epoch 823 :: Batch 0/1
Batch Loss = 0.0023681167895060707
823, epoch_train_loss=0.0023681167895060707
Epoch 824
Epoch 824 :: Batch 0/1
Batch Loss = 0.002376862007733674
824, epoch_train_loss=0.002376862007733674
Epoch 825
Epoch 825 :: Batch 0/1
Batch Loss = 0.002358667395151435
825, epoch_train_loss=0.002358667395151435
Epoch 826
Epoch 826 :: Batch 0/1
Batch Loss = 0.0023350260294692123
826, epoch_train_loss=0.0023350260294692123
Epoch 827
Epoch 827 :: Batch 0/1
Batch Loss = 0.0023270393912373606
827, epoch_train_loss=0.0023270393912373606
Epoch 828
Epoch 828 :: Batch 0/1
Batch Loss = 0.0023373355001255216
828, epoch_train_loss=0.0023373355001255216
Epoch 829
Epoch 829 :: Batch 0/1
Batch Loss = 0.002350809320535559
829, epoch_train_loss=0.002350809320535559
Epoch 830
Epoch 830 :: Batch 0/1
Batch Loss = 0.0023519246378651756
830, epoch_train_loss=0.0023519246378651756
Epoch 831
Epoch 831 :: Batch 0/1
Batch Loss = 0.0023408590520465154
831, epoch_train_loss=0.0023408590520465154
Epoch 832
Epoch 832 :: Batch 0/1
Batch Loss = 0.002328302850861738
832, epoch_train_loss=0.002328302850861738
Epoch 833
Epoch 833 :: Batch 0/1
Batch Loss = 0.002324949984724407
833, epoch_train_loss=0.002324949984724407
Epoch 834
Epoch 834 :: Batch 0/1
Batch Loss = 0.0023308461252518026
834, epoch_train_loss=0.0023308461252518026
Epoch 835
Epoch 835 :: Batch 0/1
Batch Loss = 0.0023377478687392
835, epoch_train_loss=0.0023377478687392
Epoch 836
Epoch 836 :: Batch 0/1
Batch Loss = 0.002338501585532342
836, epoch_train_loss=0.002338501585532342
Epoch 837
Epoch 837 :: Batch 0/1
Batch Loss = 0.002332446895174805
837, epoch_train_loss=0.002332446895174805
Epoch 838
Epoch 838 :: Batch 0/1
Batch Loss = 0.002325413007822359
838, epoch_train_loss=0.002325413007822359
Epoch 839
Epoch 839 :: Batch 0/1
Batch Loss = 0.0023226828328987113
839, epoch_train_loss=0.0023226828328987113
Epoch 840
Epoch 840 :: Batch 0/1
Batch Loss = 0.0023250022692791294
840, epoch_train_loss=0.0023250022692791294
Epoch 841
Epoch 841 :: Batch 0/1
Batch Loss = 0.0023288039877479615
841, epoch_train_loss=0.0023288039877479615
Epoch 842
Epoch 842 :: Batch 0/1
Batch Loss = 0.002329984786480223
842, epoch_train_loss=0.002329984786480223
Epoch 843
Epoch 843 :: Batch 0/1
Batch Loss = 0.002327595965522205
843, epoch_train_loss=0.002327595965522205
Epoch 844
Epoch 844 :: Batch 0/1
Batch Loss = 0.0023236000280220472
844, epoch_train_loss=0.0023236000280220472
Epoch 845
Epoch 845 :: Batch 0/1
Batch Loss = 0.0023209804791662495
845, epoch_train_loss=0.0023209804791662495
Epoch 846
Epoch 846 :: Batch 0/1
Batch Loss = 0.0023209948626987335
846, epoch_train_loss=0.0023209948626987335
Epoch 847
Epoch 847 :: Batch 0/1
Batch Loss = 0.0023226720231795273
847, epoch_train_loss=0.0023226720231795273
Epoch 848
Epoch 848 :: Batch 0/1
Batch Loss = 0.0023240411757721596
848, epoch_train_loss=0.0023240411757721596
Epoch 849
Epoch 849 :: Batch 0/1
Batch Loss = 0.0023237434697895346
849, epoch_train_loss=0.0023237434697895346
Epoch 850
Epoch 850 :: Batch 0/1
Batch Loss = 0.002321990837237447
850, epoch_train_loss=0.002321990837237447
Epoch 851
Epoch 851 :: Batch 0/1
Batch Loss = 0.0023199296767530708
851, epoch_train_loss=0.0023199296767530708
Epoch 852
Epoch 852 :: Batch 0/1
Batch Loss = 0.0023187416789108082
852, epoch_train_loss=0.0023187416789108082
Epoch 853
Epoch 853 :: Batch 0/1
Batch Loss = 0.002318760122262348
853, epoch_train_loss=0.002318760122262348
Epoch 854
Epoch 854 :: Batch 0/1
Batch Loss = 0.002319450541415721
854, epoch_train_loss=0.002319450541415721
Epoch 855
Epoch 855 :: Batch 0/1
Batch Loss = 0.002319959851698584
855, epoch_train_loss=0.002319959851698584
Epoch 856
Epoch 856 :: Batch 0/1
Batch Loss = 0.002319719892865299
856, epoch_train_loss=0.002319719892865299
Epoch 857
Epoch 857 :: Batch 0/1
Batch Loss = 0.002318793091441579
857, epoch_train_loss=0.002318793091441579
Epoch 858
Epoch 858 :: Batch 0/1
Batch Loss = 0.0023176408547630816
858, epoch_train_loss=0.0023176408547630816
Epoch 859
Epoch 859 :: Batch 0/1
Batch Loss = 0.0023167938772964245
859, epoch_train_loss=0.0023167938772964245
Epoch 860
Epoch 860 :: Batch 0/1
Batch Loss = 0.0023164846906129147
860, epoch_train_loss=0.0023164846906129147
Epoch 861
Epoch 861 :: Batch 0/1
Batch Loss = 0.002316582959046454
861, epoch_train_loss=0.002316582959046454
Epoch 862
Epoch 862 :: Batch 0/1
Batch Loss = 0.002316753962198294
862, epoch_train_loss=0.002316753962198294
Epoch 863
Epoch 863 :: Batch 0/1
Batch Loss = 0.002316694153062429
863, epoch_train_loss=0.002316694153062429
Epoch 864
Epoch 864 :: Batch 0/1
Batch Loss = 0.0023163066593531885
864, epoch_train_loss=0.0023163066593531885
Epoch 865
Epoch 865 :: Batch 0/1
Batch Loss = 0.0023156865515387742
865, epoch_train_loss=0.0023156865515387742
Epoch 866
Epoch 866 :: Batch 0/1
Batch Loss = 0.002315042525318792
866, epoch_train_loss=0.002315042525318792
Epoch 867
Epoch 867 :: Batch 0/1
Batch Loss = 0.002314546949895938
867, epoch_train_loss=0.002314546949895938
Epoch 868
Epoch 868 :: Batch 0/1
Batch Loss = 0.002314265399401525
868, epoch_train_loss=0.002314265399401525
Epoch 869
Epoch 869 :: Batch 0/1
Batch Loss = 0.0023141460942880596
869, epoch_train_loss=0.0023141460942880596
Epoch 870
Epoch 870 :: Batch 0/1
Batch Loss = 0.002314071256339997
870, epoch_train_loss=0.002314071256339997
Epoch 871
Epoch 871 :: Batch 0/1
Batch Loss = 0.0023139294401874552
871, epoch_train_loss=0.0023139294401874552
Epoch 872
Epoch 872 :: Batch 0/1
Batch Loss = 0.00231366286038404
872, epoch_train_loss=0.00231366286038404
Epoch 873
Epoch 873 :: Batch 0/1
Batch Loss = 0.002313287961775816
873, epoch_train_loss=0.002313287961775816
Epoch 874
Epoch 874 :: Batch 0/1
Batch Loss = 0.0023128638182791632
874, epoch_train_loss=0.0023128638182791632
Epoch 875
Epoch 875 :: Batch 0/1
Batch Loss = 0.0023124610868033306
875, epoch_train_loss=0.0023124610868033306
Epoch 876
Epoch 876 :: Batch 0/1
Batch Loss = 0.0023121246973643057
876, epoch_train_loss=0.0023121246973643057
Epoch 877
Epoch 877 :: Batch 0/1
Batch Loss = 0.002311864099227514
877, epoch_train_loss=0.002311864099227514
Epoch 878
Epoch 878 :: Batch 0/1
Batch Loss = 0.002311658202281217
878, epoch_train_loss=0.002311658202281217
Epoch 879
Epoch 879 :: Batch 0/1
Batch Loss = 0.0023114709554923722
879, epoch_train_loss=0.0023114709554923722
Epoch 880
Epoch 880 :: Batch 0/1
Batch Loss = 0.002311268522638563
880, epoch_train_loss=0.002311268522638563
Epoch 881
Epoch 881 :: Batch 0/1
Batch Loss = 0.0023110295780742595
881, epoch_train_loss=0.0023110295780742595
Epoch 882
Epoch 882 :: Batch 0/1
Batch Loss = 0.002310751633572654
882, epoch_train_loss=0.002310751633572654
Epoch 883
Epoch 883 :: Batch 0/1
Batch Loss = 0.0023104454104683194
883, epoch_train_loss=0.0023104454104683194
Epoch 884
Epoch 884 :: Batch 0/1
Batch Loss = 0.002310130055094726
884, epoch_train_loss=0.002310130055094726
Epoch 885
Epoch 885 :: Batch 0/1
Batch Loss = 0.0023098225978437285
885, epoch_train_loss=0.0023098225978437285
Epoch 886
Epoch 886 :: Batch 0/1
Batch Loss = 0.0023095342148679236
886, epoch_train_loss=0.0023095342148679236
Epoch 887
Epoch 887 :: Batch 0/1
Batch Loss = 0.002309267992337003
887, epoch_train_loss=0.002309267992337003
Epoch 888
Epoch 888 :: Batch 0/1
Batch Loss = 0.002309020748013513
888, epoch_train_loss=0.002309020748013513
Epoch 889
Epoch 889 :: Batch 0/1
Batch Loss = 0.0023087855208602753
889, epoch_train_loss=0.0023087855208602753
Epoch 890
Epoch 890 :: Batch 0/1
Batch Loss = 0.002308554099556632
890, epoch_train_loss=0.002308554099556632
Epoch 891
Epoch 891 :: Batch 0/1
Batch Loss = 0.0023083195279474744
891, epoch_train_loss=0.0023083195279474744
Epoch 892
Epoch 892 :: Batch 0/1
Batch Loss = 0.0023080772581729084
892, epoch_train_loss=0.0023080772581729084
Epoch 893
Epoch 893 :: Batch 0/1
Batch Loss = 0.0023078262033534567
893, epoch_train_loss=0.0023078262033534567
Epoch 894
Epoch 894 :: Batch 0/1
Batch Loss = 0.0023075672704391034
894, epoch_train_loss=0.0023075672704391034
Epoch 895
Epoch 895 :: Batch 0/1
Batch Loss = 0.0023073031448600854
895, epoch_train_loss=0.0023073031448600854
Epoch 896
Epoch 896 :: Batch 0/1
Batch Loss = 0.00230703643847608
896, epoch_train_loss=0.00230703643847608
Epoch 897
Epoch 897 :: Batch 0/1
Batch Loss = 0.0023067698906582886
897, epoch_train_loss=0.0023067698906582886
Epoch 898
Epoch 898 :: Batch 0/1
Batch Loss = 0.0023065054323393916
898, epoch_train_loss=0.0023065054323393916
Epoch 899
Epoch 899 :: Batch 0/1
Batch Loss = 0.0023062444238739683
899, epoch_train_loss=0.0023062444238739683
Epoch 900
Epoch 900 :: Batch 0/1
Batch Loss = 0.0023059872604913345
900, epoch_train_loss=0.0023059872604913345
Epoch 901
Epoch 901 :: Batch 0/1
Batch Loss = 0.0023057337659921906
901, epoch_train_loss=0.0023057337659921906
Epoch 902
Epoch 902 :: Batch 0/1
Batch Loss = 0.002305483378756237
902, epoch_train_loss=0.002305483378756237
Epoch 903
Epoch 903 :: Batch 0/1
Batch Loss = 0.0023052354726260035
903, epoch_train_loss=0.0023052354726260035
Epoch 904
Epoch 904 :: Batch 0/1
Batch Loss = 0.0023049894377666865
904, epoch_train_loss=0.0023049894377666865
Epoch 905
Epoch 905 :: Batch 0/1
Batch Loss = 0.0023047446793956136
905, epoch_train_loss=0.0023047446793956136
Epoch 906
Epoch 906 :: Batch 0/1
Batch Loss = 0.0023045007423271196
906, epoch_train_loss=0.0023045007423271196
Epoch 907
Epoch 907 :: Batch 0/1
Batch Loss = 0.0023042572974189567
907, epoch_train_loss=0.0023042572974189567
Epoch 908
Epoch 908 :: Batch 0/1
Batch Loss = 0.0023040142790319925
908, epoch_train_loss=0.0023040142790319925
Epoch 909
Epoch 909 :: Batch 0/1
Batch Loss = 0.0023037716490508665
909, epoch_train_loss=0.0023037716490508665
Epoch 910
Epoch 910 :: Batch 0/1
Batch Loss = 0.0023035295545892395
910, epoch_train_loss=0.0023035295545892395
Epoch 911
Epoch 911 :: Batch 0/1
Batch Loss = 0.002303288074361849
911, epoch_train_loss=0.002303288074361849
Epoch 912
Epoch 912 :: Batch 0/1
Batch Loss = 0.00230304755363644
912, epoch_train_loss=0.00230304755363644
Epoch 913
Epoch 913 :: Batch 0/1
Batch Loss = 0.0023028082737581957
913, epoch_train_loss=0.0023028082737581957
Epoch 914
Epoch 914 :: Batch 0/1
Batch Loss = 0.0023025708467425405
914, epoch_train_loss=0.0023025708467425405
Epoch 915
Epoch 915 :: Batch 0/1
Batch Loss = 0.002302335780304382
915, epoch_train_loss=0.002302335780304382
Epoch 916
Epoch 916 :: Batch 0/1
Batch Loss = 0.0023021041144753443
916, epoch_train_loss=0.0023021041144753443
Epoch 917
Epoch 917 :: Batch 0/1
Batch Loss = 0.0023018769018928855
917, epoch_train_loss=0.0023018769018928855
Epoch 918
Epoch 918 :: Batch 0/1
Batch Loss = 0.0023016561869536926
918, epoch_train_loss=0.0023016561869536926
Epoch 919
Epoch 919 :: Batch 0/1
Batch Loss = 0.002301444247291891
919, epoch_train_loss=0.002301444247291891
Epoch 920
Epoch 920 :: Batch 0/1
Batch Loss = 0.002301245305677542
920, epoch_train_loss=0.002301245305677542
Epoch 921
Epoch 921 :: Batch 0/1
Batch Loss = 0.0023010644302491074
921, epoch_train_loss=0.0023010644302491074
Epoch 922
Epoch 922 :: Batch 0/1
Batch Loss = 0.0023009109886646243
922, epoch_train_loss=0.0023009109886646243
Epoch 923
Epoch 923 :: Batch 0/1
Batch Loss = 0.0023007967192761073
923, epoch_train_loss=0.0023007967192761073
Epoch 924
Epoch 924 :: Batch 0/1
Batch Loss = 0.0023007435094826827
924, epoch_train_loss=0.0023007435094826827
Epoch 925
Epoch 925 :: Batch 0/1
Batch Loss = 0.002300779323143328
925, epoch_train_loss=0.002300779323143328
Epoch 926
Epoch 926 :: Batch 0/1
Batch Loss = 0.0023009579034972763
926, epoch_train_loss=0.0023009579034972763
Epoch 927
Epoch 927 :: Batch 0/1
Batch Loss = 0.002301347978451767
927, epoch_train_loss=0.002301347978451767
Epoch 928
Epoch 928 :: Batch 0/1
Batch Loss = 0.0023020881951517237
928, epoch_train_loss=0.0023020881951517237
Epoch 929
Epoch 929 :: Batch 0/1
Batch Loss = 0.0023033521357623443
929, epoch_train_loss=0.0023033521357623443
Epoch 930
Epoch 930 :: Batch 0/1
Batch Loss = 0.0023055157614603953
930, epoch_train_loss=0.0023055157614603953
Epoch 931
Epoch 931 :: Batch 0/1
Batch Loss = 0.002309026805763278
931, epoch_train_loss=0.002309026805763278
Epoch 932
Epoch 932 :: Batch 0/1
Batch Loss = 0.002314959066994596
932, epoch_train_loss=0.002314959066994596
Epoch 933
Epoch 933 :: Batch 0/1
Batch Loss = 0.002324475432279194
933, epoch_train_loss=0.002324475432279194
Epoch 934
Epoch 934 :: Batch 0/1
Batch Loss = 0.00234081350088257
934, epoch_train_loss=0.00234081350088257
Epoch 935
Epoch 935 :: Batch 0/1
Batch Loss = 0.0023669181954957133
935, epoch_train_loss=0.0023669181954957133
Epoch 936
Epoch 936 :: Batch 0/1
Batch Loss = 0.0024131115461877573
936, epoch_train_loss=0.0024131115461877573
Epoch 937
Epoch 937 :: Batch 0/1
Batch Loss = 0.002486041540672556
937, epoch_train_loss=0.002486041540672556
Epoch 938
Epoch 938 :: Batch 0/1
Batch Loss = 0.002620402440684554
938, epoch_train_loss=0.002620402440684554
Epoch 939
Epoch 939 :: Batch 0/1
Batch Loss = 0.002824759000127785
939, epoch_train_loss=0.002824759000127785
Epoch 940
Epoch 940 :: Batch 0/1
Batch Loss = 0.0032198082828921453
940, epoch_train_loss=0.0032198082828921453
Epoch 941
Epoch 941 :: Batch 0/1
Batch Loss = 0.00376390306584904
941, epoch_train_loss=0.00376390306584904
Epoch 942
Epoch 942 :: Batch 0/1
Batch Loss = 0.0048668857190747
942, epoch_train_loss=0.0048668857190747
Epoch 943
Epoch 943 :: Batch 0/1
Batch Loss = 0.006027824151277751
943, epoch_train_loss=0.006027824151277751
Epoch 944
Epoch 944 :: Batch 0/1
Batch Loss = 0.008433440465055375
944, epoch_train_loss=0.008433440465055375
Epoch 945
Epoch 945 :: Batch 0/1
Batch Loss = 0.009339858427344909
945, epoch_train_loss=0.009339858427344909
Epoch 946
Epoch 946 :: Batch 0/1
Batch Loss = 0.011333110181904556
946, epoch_train_loss=0.011333110181904556
Epoch 947
Epoch 947 :: Batch 0/1
Batch Loss = 0.008645788698111747
947, epoch_train_loss=0.008645788698111747
Epoch 948
Epoch 948 :: Batch 0/1
Batch Loss = 0.0061629297489358215
948, epoch_train_loss=0.0061629297489358215
Epoch 949
Epoch 949 :: Batch 0/1
Batch Loss = 0.0030876045066946384
949, epoch_train_loss=0.0030876045066946384
Epoch 950
Epoch 950 :: Batch 0/1
Batch Loss = 0.002398913581683584
950, epoch_train_loss=0.002398913581683584
Epoch 951
Epoch 951 :: Batch 0/1
Batch Loss = 0.003865583306167074
951, epoch_train_loss=0.003865583306167074
Epoch 952
Epoch 952 :: Batch 0/1
Batch Loss = 0.005432536081756448
952, epoch_train_loss=0.005432536081756448
Epoch 953
Epoch 953 :: Batch 0/1
Batch Loss = 0.006168415237770794
953, epoch_train_loss=0.006168415237770794
Epoch 954
Epoch 954 :: Batch 0/1
Batch Loss = 0.0043872025411950556
954, epoch_train_loss=0.0043872025411950556
Epoch 955
Epoch 955 :: Batch 0/1
Batch Loss = 0.002749182064580619
955, epoch_train_loss=0.002749182064580619
Epoch 956
Epoch 956 :: Batch 0/1
Batch Loss = 0.0024213926734863713
956, epoch_train_loss=0.0024213926734863713
Epoch 957
Epoch 957 :: Batch 0/1
Batch Loss = 0.0033764821089378306
957, epoch_train_loss=0.0033764821089378306
Epoch 958
Epoch 958 :: Batch 0/1
Batch Loss = 0.004229634578036199
958, epoch_train_loss=0.004229634578036199
Epoch 959
Epoch 959 :: Batch 0/1
Batch Loss = 0.0035935000636118813
959, epoch_train_loss=0.0035935000636118813
Epoch 960
Epoch 960 :: Batch 0/1
Batch Loss = 0.0026435948421848748
960, epoch_train_loss=0.0026435948421848748
Epoch 961
Epoch 961 :: Batch 0/1
Batch Loss = 0.0023486875088272923
961, epoch_train_loss=0.0023486875088272923
Epoch 962
Epoch 962 :: Batch 0/1
Batch Loss = 0.002877018630770295
962, epoch_train_loss=0.002877018630770295
Epoch 963
Epoch 963 :: Batch 0/1
Batch Loss = 0.0034156043251523187
963, epoch_train_loss=0.0034156043251523187
Epoch 964
Epoch 964 :: Batch 0/1
Batch Loss = 0.0031256267250386904
964, epoch_train_loss=0.0031256267250386904
Epoch 965
Epoch 965 :: Batch 0/1
Batch Loss = 0.0025507138612336153
965, epoch_train_loss=0.0025507138612336153
Epoch 966
Epoch 966 :: Batch 0/1
Batch Loss = 0.0023223341893485253
966, epoch_train_loss=0.0023223341893485253
Epoch 967
Epoch 967 :: Batch 0/1
Batch Loss = 0.002619580498350685
967, epoch_train_loss=0.002619580498350685
Epoch 968
Epoch 968 :: Batch 0/1
Batch Loss = 0.0029414028084329474
968, epoch_train_loss=0.0029414028084329474
Epoch 969
Epoch 969 :: Batch 0/1
Batch Loss = 0.0027902711480580085
969, epoch_train_loss=0.0027902711480580085
Epoch 970
Epoch 970 :: Batch 0/1
Batch Loss = 0.0024510807710692003
970, epoch_train_loss=0.0024510807710692003
Epoch 971
Epoch 971 :: Batch 0/1
Batch Loss = 0.002306055946531434
971, epoch_train_loss=0.002306055946531434
Epoch 972
Epoch 972 :: Batch 0/1
Batch Loss = 0.0024690948770180726
972, epoch_train_loss=0.0024690948770180726
Epoch 973
Epoch 973 :: Batch 0/1
Batch Loss = 0.002670074855721177
973, epoch_train_loss=0.002670074855721177
Epoch 974
Epoch 974 :: Batch 0/1
Batch Loss = 0.0026164053255353066
974, epoch_train_loss=0.0026164053255353066
Epoch 975
Epoch 975 :: Batch 0/1
Batch Loss = 0.0024192803253588646
975, epoch_train_loss=0.0024192803253588646
Epoch 976
Epoch 976 :: Batch 0/1
Batch Loss = 0.0023013926120874265
976, epoch_train_loss=0.0023013926120874265
Epoch 977
Epoch 977 :: Batch 0/1
Batch Loss = 0.002373569398098176
977, epoch_train_loss=0.002373569398098176
Epoch 978
Epoch 978 :: Batch 0/1
Batch Loss = 0.0025016972435923652
978, epoch_train_loss=0.0025016972435923652
Epoch 979
Epoch 979 :: Batch 0/1
Batch Loss = 0.0025030844394306018
979, epoch_train_loss=0.0025030844394306018
Epoch 980
Epoch 980 :: Batch 0/1
Batch Loss = 0.0023982797854339996
980, epoch_train_loss=0.0023982797854339996
Epoch 981
Epoch 981 :: Batch 0/1
Batch Loss = 0.0023052760656577737
981, epoch_train_loss=0.0023052760656577737
Epoch 982
Epoch 982 :: Batch 0/1
Batch Loss = 0.002318229163883178
982, epoch_train_loss=0.002318229163883178
Epoch 983
Epoch 983 :: Batch 0/1
Batch Loss = 0.0023931726361554957
983, epoch_train_loss=0.0023931726361554957
Epoch 984
Epoch 984 :: Batch 0/1
Batch Loss = 0.00242337500314226
984, epoch_train_loss=0.00242337500314226
Epoch 985
Epoch 985 :: Batch 0/1
Batch Loss = 0.0023806137909527856
985, epoch_train_loss=0.0023806137909527856
Epoch 986
Epoch 986 :: Batch 0/1
Batch Loss = 0.002313578825308719
986, epoch_train_loss=0.002313578825308719
Epoch 987
Epoch 987 :: Batch 0/1
Batch Loss = 0.0022959158512316094
987, epoch_train_loss=0.0022959158512316094
Epoch 988
Epoch 988 :: Batch 0/1
Batch Loss = 0.002330943676184302
988, epoch_train_loss=0.002330943676184302
Epoch 989
Epoch 989 :: Batch 0/1
Batch Loss = 0.002364555000818802
989, epoch_train_loss=0.002364555000818802
Epoch 990
Epoch 990 :: Batch 0/1
Batch Loss = 0.0023583627702263694
990, epoch_train_loss=0.0023583627702263694
Epoch 991
Epoch 991 :: Batch 0/1
Batch Loss = 0.0023196668313192016
991, epoch_train_loss=0.0023196668313192016
Epoch 992
Epoch 992 :: Batch 0/1
Batch Loss = 0.0022924779826074305
992, epoch_train_loss=0.0022924779826074305
Epoch 993
Epoch 993 :: Batch 0/1
Batch Loss = 0.0022984514427653374
993, epoch_train_loss=0.0022984514427653374
Epoch 994
Epoch 994 :: Batch 0/1
Batch Loss = 0.0023216462813701
994, epoch_train_loss=0.0023216462813701
Epoch 995
Epoch 995 :: Batch 0/1
Batch Loss = 0.0023325203301681583
995, epoch_train_loss=0.0023325203301681583
Epoch 996
Epoch 996 :: Batch 0/1
Batch Loss = 0.0023185622215193013
996, epoch_train_loss=0.0023185622215193013
Epoch 997
Epoch 997 :: Batch 0/1
Batch Loss = 0.0022971628312363573
997, epoch_train_loss=0.0022971628312363573
Epoch 998
Epoch 998 :: Batch 0/1
Batch Loss = 0.002288657742772717
998, epoch_train_loss=0.002288657742772717
Epoch 999
Epoch 999 :: Batch 0/1
Batch Loss = 0.002297222225359541
999, epoch_train_loss=0.002297222225359541
Epoch 1000
Epoch 1000 :: Batch 0/1
Batch Loss = 0.0023094992474815036
1000, epoch_train_loss=0.0023094992474815036
Epoch 1001
Epoch 1001 :: Batch 0/1
Batch Loss = 0.002310887913721421
1001, epoch_train_loss=0.002310887913721421
Epoch 1002
Epoch 1002 :: Batch 0/1
Batch Loss = 0.0023011311047112
1002, epoch_train_loss=0.0023011311047112
Epoch 1003
Epoch 1003 :: Batch 0/1
Batch Loss = 0.0022899667098144265
1003, epoch_train_loss=0.0022899667098144265
Epoch 1004
Epoch 1004 :: Batch 0/1
Batch Loss = 0.0022871245053163332
1004, epoch_train_loss=0.0022871245053163332
Epoch 1005
Epoch 1005 :: Batch 0/1
Batch Loss = 0.002292463690118596
1005, epoch_train_loss=0.002292463690118596
Epoch 1006
Epoch 1006 :: Batch 0/1
Batch Loss = 0.002298327531678201
1006, epoch_train_loss=0.002298327531678201
Epoch 1007
Epoch 1007 :: Batch 0/1
Batch Loss = 0.002298447454400801
1007, epoch_train_loss=0.002298447454400801
Epoch 1008
Epoch 1008 :: Batch 0/1
Batch Loss = 0.0022928079654311413
1008, epoch_train_loss=0.0022928079654311413
Epoch 1009
Epoch 1009 :: Batch 0/1
Batch Loss = 0.0022869267084206176
1009, epoch_train_loss=0.0022869267084206176
Epoch 1010
Epoch 1010 :: Batch 0/1
Batch Loss = 0.002285198566473169
1010, epoch_train_loss=0.002285198566473169
Epoch 1011
Epoch 1011 :: Batch 0/1
Batch Loss = 0.002287621103067383
1011, epoch_train_loss=0.002287621103067383
Epoch 1012
Epoch 1012 :: Batch 0/1
Batch Loss = 0.0022907186762076547
1012, epoch_train_loss=0.0022907186762076547
Epoch 1013
Epoch 1013 :: Batch 0/1
Batch Loss = 0.0022911406312825107
1013, epoch_train_loss=0.0022911406312825107
Epoch 1014
Epoch 1014 :: Batch 0/1
Batch Loss = 0.0022886158882096546
1014, epoch_train_loss=0.0022886158882096546
Epoch 1015
Epoch 1015 :: Batch 0/1
Batch Loss = 0.00228522137680981
1015, epoch_train_loss=0.00228522137680981
Epoch 1016
Epoch 1016 :: Batch 0/1
Batch Loss = 0.0022834403205138757
1016, epoch_train_loss=0.0022834403205138757
Epoch 1017
Epoch 1017 :: Batch 0/1
Batch Loss = 0.0022839433408064565
1017, epoch_train_loss=0.0022839433408064565
Epoch 1018
Epoch 1018 :: Batch 0/1
Batch Loss = 0.002285496451648398
1018, epoch_train_loss=0.002285496451648398
Epoch 1019
Epoch 1019 :: Batch 0/1
Batch Loss = 0.0022863462774465396
1019, epoch_train_loss=0.0022863462774465396
Epoch 1020
Epoch 1020 :: Batch 0/1
Batch Loss = 0.002285612960056523
1020, epoch_train_loss=0.002285612960056523
Epoch 1021
Epoch 1021 :: Batch 0/1
Batch Loss = 0.002283869645102682
1021, epoch_train_loss=0.002283869645102682
Epoch 1022
Epoch 1022 :: Batch 0/1
Batch Loss = 0.0022822855198258215
1022, epoch_train_loss=0.0022822855198258215
Epoch 1023
Epoch 1023 :: Batch 0/1
Batch Loss = 0.0022817079406638316
1023, epoch_train_loss=0.0022817079406638316
Epoch 1024
Epoch 1024 :: Batch 0/1
Batch Loss = 0.002282076277045183
1024, epoch_train_loss=0.002282076277045183
Epoch 1025
Epoch 1025 :: Batch 0/1
Batch Loss = 0.0022826791555562603
1025, epoch_train_loss=0.0022826791555562603
Epoch 1026
Epoch 1026 :: Batch 0/1
Batch Loss = 0.0022828184163725413
1026, epoch_train_loss=0.0022828184163725413
Epoch 1027
Epoch 1027 :: Batch 0/1
Batch Loss = 0.0022822618227919377
1027, epoch_train_loss=0.0022822618227919377
Epoch 1028
Epoch 1028 :: Batch 0/1
Batch Loss = 0.0022813261531933046
1028, epoch_train_loss=0.0022813261531933046
Epoch 1029
Epoch 1029 :: Batch 0/1
Batch Loss = 0.0022804910326731935
1029, epoch_train_loss=0.0022804910326731935
Epoch 1030
Epoch 1030 :: Batch 0/1
Batch Loss = 0.0022800772313844777
1030, epoch_train_loss=0.0022800772313844777
Epoch 1031
Epoch 1031 :: Batch 0/1
Batch Loss = 0.0022800698780178087
1031, epoch_train_loss=0.0022800698780178087
Epoch 1032
Epoch 1032 :: Batch 0/1
Batch Loss = 0.002280211201640291
1032, epoch_train_loss=0.002280211201640291
Epoch 1033
Epoch 1033 :: Batch 0/1
Batch Loss = 0.0022802157602190166
1033, epoch_train_loss=0.0022802157602190166
Epoch 1034
Epoch 1034 :: Batch 0/1
Batch Loss = 0.002279939730808903
1034, epoch_train_loss=0.002279939730808903
Epoch 1035
Epoch 1035 :: Batch 0/1
Batch Loss = 0.00227944997622363
1035, epoch_train_loss=0.00227944997622363
Epoch 1036
Epoch 1036 :: Batch 0/1
Batch Loss = 0.0022789218485909867
1036, epoch_train_loss=0.0022789218485909867
Epoch 1037
Epoch 1037 :: Batch 0/1
Batch Loss = 0.0022785216857633315
1037, epoch_train_loss=0.0022785216857633315
Epoch 1038
Epoch 1038 :: Batch 0/1
Batch Loss = 0.0022783038241122543
1038, epoch_train_loss=0.0022783038241122543
Epoch 1039
Epoch 1039 :: Batch 0/1
Batch Loss = 0.0022782096378337084
1039, epoch_train_loss=0.0022782096378337084
Epoch 1040
Epoch 1040 :: Batch 0/1
Batch Loss = 0.0022781296526374795
1040, epoch_train_loss=0.0022781296526374795
Epoch 1041
Epoch 1041 :: Batch 0/1
Batch Loss = 0.0022779757414903177
1041, epoch_train_loss=0.0022779757414903177
Epoch 1042
Epoch 1042 :: Batch 0/1
Batch Loss = 0.002277724487418641
1042, epoch_train_loss=0.002277724487418641
Epoch 1043
Epoch 1043 :: Batch 0/1
Batch Loss = 0.0022774056748406605
1043, epoch_train_loss=0.0022774056748406605
Epoch 1044
Epoch 1044 :: Batch 0/1
Batch Loss = 0.0022770780430628143
1044, epoch_train_loss=0.0022770780430628143
Epoch 1045
Epoch 1045 :: Batch 0/1
Batch Loss = 0.0022767898615713702
1045, epoch_train_loss=0.0022767898615713702
Epoch 1046
Epoch 1046 :: Batch 0/1
Batch Loss = 0.0022765611883971176
1046, epoch_train_loss=0.0022765611883971176
Epoch 1047
Epoch 1047 :: Batch 0/1
Batch Loss = 0.002276379493976958
1047, epoch_train_loss=0.002276379493976958
Epoch 1048
Epoch 1048 :: Batch 0/1
Batch Loss = 0.0022762138950299937
1048, epoch_train_loss=0.0022762138950299937
Epoch 1049
Epoch 1049 :: Batch 0/1
Batch Loss = 0.002276034918203049
1049, epoch_train_loss=0.002276034918203049
Epoch 1050
Epoch 1050 :: Batch 0/1
Batch Loss = 0.002275827688775657
1050, epoch_train_loss=0.002275827688775657
Epoch 1051
Epoch 1051 :: Batch 0/1
Batch Loss = 0.002275595902362767
1051, epoch_train_loss=0.002275595902362767
Epoch 1052
Epoch 1052 :: Batch 0/1
Batch Loss = 0.002275352312547485
1052, epoch_train_loss=0.002275352312547485
Epoch 1053
Epoch 1053 :: Batch 0/1
Batch Loss = 0.00227511150971175
1053, epoch_train_loss=0.00227511150971175
Epoch 1054
Epoch 1054 :: Batch 0/1
Batch Loss = 0.0022748824027727667
1054, epoch_train_loss=0.0022748824027727667
Epoch 1055
Epoch 1055 :: Batch 0/1
Batch Loss = 0.002274667662934935
1055, epoch_train_loss=0.002274667662934935
Epoch 1056
Epoch 1056 :: Batch 0/1
Batch Loss = 0.0022744639089939114
1056, epoch_train_loss=0.0022744639089939114
Epoch 1057
Epoch 1057 :: Batch 0/1
Batch Loss = 0.002274264789022468
1057, epoch_train_loss=0.002274264789022468
Epoch 1058
Epoch 1058 :: Batch 0/1
Batch Loss = 0.002274064109474455
1058, epoch_train_loss=0.002274064109474455
Epoch 1059
Epoch 1059 :: Batch 0/1
Batch Loss = 0.002273858564048509
1059, epoch_train_loss=0.002273858564048509
Epoch 1060
Epoch 1060 :: Batch 0/1
Batch Loss = 0.002273648717245032
1060, epoch_train_loss=0.002273648717245032
Epoch 1061
Epoch 1061 :: Batch 0/1
Batch Loss = 0.002273436921748828
1061, epoch_train_loss=0.002273436921748828
Epoch 1062
Epoch 1062 :: Batch 0/1
Batch Loss = 0.0022732259833932907
1062, epoch_train_loss=0.0022732259833932907
Epoch 1063
Epoch 1063 :: Batch 0/1
Batch Loss = 0.002273017391681368
1063, epoch_train_loss=0.002273017391681368
Epoch 1064
Epoch 1064 :: Batch 0/1
Batch Loss = 0.0022728117883705373
1064, epoch_train_loss=0.0022728117883705373
Epoch 1065
Epoch 1065 :: Batch 0/1
Batch Loss = 0.0022726086590251276
1065, epoch_train_loss=0.0022726086590251276
Epoch 1066
Epoch 1066 :: Batch 0/1
Batch Loss = 0.002272406924394795
1066, epoch_train_loss=0.002272406924394795
Epoch 1067
Epoch 1067 :: Batch 0/1
Batch Loss = 0.0022722050253816527
1067, epoch_train_loss=0.0022722050253816527
Epoch 1068
Epoch 1068 :: Batch 0/1
Batch Loss = 0.0022720018862160836
1068, epoch_train_loss=0.0022720018862160836
Epoch 1069
Epoch 1069 :: Batch 0/1
Batch Loss = 0.0022717972055184515
1069, epoch_train_loss=0.0022717972055184515
Epoch 1070
Epoch 1070 :: Batch 0/1
Batch Loss = 0.002271591456899383
1070, epoch_train_loss=0.002271591456899383
Epoch 1071
Epoch 1071 :: Batch 0/1
Batch Loss = 0.0022713854535591715
1071, epoch_train_loss=0.0022713854535591715
Epoch 1072
Epoch 1072 :: Batch 0/1
Batch Loss = 0.002271179935075746
1072, epoch_train_loss=0.002271179935075746
Epoch 1073
Epoch 1073 :: Batch 0/1
Batch Loss = 0.0022709756303657107
1073, epoch_train_loss=0.0022709756303657107
Epoch 1074
Epoch 1074 :: Batch 0/1
Batch Loss = 0.002270773042757263
1074, epoch_train_loss=0.002270773042757263
Epoch 1075
Epoch 1075 :: Batch 0/1
Batch Loss = 0.002270572483058502
1075, epoch_train_loss=0.002270572483058502
Epoch 1076
Epoch 1076 :: Batch 0/1
Batch Loss = 0.0022703738158478073
1076, epoch_train_loss=0.0022703738158478073
Epoch 1077
Epoch 1077 :: Batch 0/1
Batch Loss = 0.002270176719299034
1077, epoch_train_loss=0.002270176719299034
Epoch 1078
Epoch 1078 :: Batch 0/1
Batch Loss = 0.0022699807676397703
1078, epoch_train_loss=0.0022699807676397703
Epoch 1079
Epoch 1079 :: Batch 0/1
Batch Loss = 0.002269785668167975
1079, epoch_train_loss=0.002269785668167975
Epoch 1080
Epoch 1080 :: Batch 0/1
Batch Loss = 0.0022695911381278022
1080, epoch_train_loss=0.0022695911381278022
Epoch 1081
Epoch 1081 :: Batch 0/1
Batch Loss = 0.002269396995802544
1081, epoch_train_loss=0.002269396995802544
Epoch 1082
Epoch 1082 :: Batch 0/1
Batch Loss = 0.0022692031134197523
1082, epoch_train_loss=0.0022692031134197523
Epoch 1083
Epoch 1083 :: Batch 0/1
Batch Loss = 0.0022690095884713808
1083, epoch_train_loss=0.0022690095884713808
Epoch 1084
Epoch 1084 :: Batch 0/1
Batch Loss = 0.002268816601245039
1084, epoch_train_loss=0.002268816601245039
Epoch 1085
Epoch 1085 :: Batch 0/1
Batch Loss = 0.0022686244950021056
1085, epoch_train_loss=0.0022686244950021056
Epoch 1086
Epoch 1086 :: Batch 0/1
Batch Loss = 0.0022684336108169333
1086, epoch_train_loss=0.0022684336108169333
Epoch 1087
Epoch 1087 :: Batch 0/1
Batch Loss = 0.002268244550975477
1087, epoch_train_loss=0.002268244550975477
Epoch 1088
Epoch 1088 :: Batch 0/1
Batch Loss = 0.002268058025817609
1088, epoch_train_loss=0.002268058025817609
Epoch 1089
Epoch 1089 :: Batch 0/1
Batch Loss = 0.0022678752286638074
1089, epoch_train_loss=0.0022678752286638074
Epoch 1090
Epoch 1090 :: Batch 0/1
Batch Loss = 0.0022676975897245755
1090, epoch_train_loss=0.0022676975897245755
Epoch 1091
Epoch 1091 :: Batch 0/1
Batch Loss = 0.002267527543152848
1091, epoch_train_loss=0.002267527543152848
Epoch 1092
Epoch 1092 :: Batch 0/1
Batch Loss = 0.0022673682902808164
1092, epoch_train_loss=0.0022673682902808164
Epoch 1093
Epoch 1093 :: Batch 0/1
Batch Loss = 0.0022672253944296455
1093, epoch_train_loss=0.0022672253944296455
Epoch 1094
Epoch 1094 :: Batch 0/1
Batch Loss = 0.0022671064729629787
1094, epoch_train_loss=0.0022671064729629787
Epoch 1095
Epoch 1095 :: Batch 0/1
Batch Loss = 0.0022670248741884684
1095, epoch_train_loss=0.0022670248741884684
Epoch 1096
Epoch 1096 :: Batch 0/1
Batch Loss = 0.002266999431313962
1096, epoch_train_loss=0.002266999431313962
Epoch 1097
Epoch 1097 :: Batch 0/1
Batch Loss = 0.0022670639712615635
1097, epoch_train_loss=0.0022670639712615635
Epoch 1098
Epoch 1098 :: Batch 0/1
Batch Loss = 0.0022672669416062352
1098, epoch_train_loss=0.0022672669416062352
Epoch 1099
Epoch 1099 :: Batch 0/1
Batch Loss = 0.002267698335094638
1099, epoch_train_loss=0.002267698335094638
Epoch 1100
Epoch 1100 :: Batch 0/1
Batch Loss = 0.002268487064341192
1100, epoch_train_loss=0.002268487064341192
Epoch 1101
Epoch 1101 :: Batch 0/1
Batch Loss = 0.0022698844693077877
1101, epoch_train_loss=0.0022698844693077877
Epoch 1102
Epoch 1102 :: Batch 0/1
Batch Loss = 0.0022722441596173685
1102, epoch_train_loss=0.0022722441596173685
Epoch 1103
Epoch 1103 :: Batch 0/1
Batch Loss = 0.002276303149704488
1103, epoch_train_loss=0.002276303149704488
Epoch 1104
Epoch 1104 :: Batch 0/1
Batch Loss = 0.0022830518099032446
1104, epoch_train_loss=0.0022830518099032446
Epoch 1105
Epoch 1105 :: Batch 0/1
Batch Loss = 0.0022947584741124347
1105, epoch_train_loss=0.0022947584741124347
Epoch 1106
Epoch 1106 :: Batch 0/1
Batch Loss = 0.0023141966603172207
1106, epoch_train_loss=0.0023141966603172207
Epoch 1107
Epoch 1107 :: Batch 0/1
Batch Loss = 0.002348669705292758
1107, epoch_train_loss=0.002348669705292758
Epoch 1108
Epoch 1108 :: Batch 0/1
Batch Loss = 0.0024055275575091367
1108, epoch_train_loss=0.0024055275575091367
Epoch 1109
Epoch 1109 :: Batch 0/1
Batch Loss = 0.00250902172306753
1109, epoch_train_loss=0.00250902172306753
Epoch 1110
Epoch 1110 :: Batch 0/1
Batch Loss = 0.0026742034184652624
1110, epoch_train_loss=0.0026742034184652624
Epoch 1111
Epoch 1111 :: Batch 0/1
Batch Loss = 0.0029802418095446442
1111, epoch_train_loss=0.0029802418095446442
Epoch 1112
Epoch 1112 :: Batch 0/1
Batch Loss = 0.0034191722121754337
1112, epoch_train_loss=0.0034191722121754337
Epoch 1113
Epoch 1113 :: Batch 0/1
Batch Loss = 0.004211348520344843
1113, epoch_train_loss=0.004211348520344843
Epoch 1114
Epoch 1114 :: Batch 0/1
Batch Loss = 0.005014264881615676
1114, epoch_train_loss=0.005014264881615676
Epoch 1115
Epoch 1115 :: Batch 0/1
Batch Loss = 0.006203542727619867
1115, epoch_train_loss=0.006203542727619867
Epoch 1116
Epoch 1116 :: Batch 0/1
Batch Loss = 0.0061704248499430585
1116, epoch_train_loss=0.0061704248499430585
Epoch 1117
Epoch 1117 :: Batch 0/1
Batch Loss = 0.005685521435115774
1117, epoch_train_loss=0.005685521435115774
Epoch 1118
Epoch 1118 :: Batch 0/1
Batch Loss = 0.004057545424412312
1118, epoch_train_loss=0.004057545424412312
Epoch 1119
Epoch 1119 :: Batch 0/1
Batch Loss = 0.003726502980380048
1119, epoch_train_loss=0.003726502980380048
Epoch 1120
Epoch 1120 :: Batch 0/1
Batch Loss = 0.005002414538685064
1120, epoch_train_loss=0.005002414538685064
Epoch 1121
Epoch 1121 :: Batch 0/1
Batch Loss = 0.006815628261044516
1121, epoch_train_loss=0.006815628261044516
Epoch 1122
Epoch 1122 :: Batch 0/1
Batch Loss = 0.0072377368360966214
1122, epoch_train_loss=0.0072377368360966214
Epoch 1123
Epoch 1123 :: Batch 0/1
Batch Loss = 0.008126161949872571
1123, epoch_train_loss=0.008126161949872571
Epoch 1124
Epoch 1124 :: Batch 0/1
Batch Loss = 0.008335906704677964
1124, epoch_train_loss=0.008335906704677964
Epoch 1125
Epoch 1125 :: Batch 0/1
Batch Loss = 0.009305129251839045
1125, epoch_train_loss=0.009305129251839045
Epoch 1126
Epoch 1126 :: Batch 0/1
Batch Loss = 0.006451884790679345
1126, epoch_train_loss=0.006451884790679345
Epoch 1127
Epoch 1127 :: Batch 0/1
Batch Loss = 0.004279580929343492
1127, epoch_train_loss=0.004279580929343492
Epoch 1128
Epoch 1128 :: Batch 0/1
Batch Loss = 0.0031122371641523743
1128, epoch_train_loss=0.0031122371641523743
Epoch 1129
Epoch 1129 :: Batch 0/1
Batch Loss = 0.0028494543273745168
1129, epoch_train_loss=0.0028494543273745168
Epoch 1130
Epoch 1130 :: Batch 0/1
Batch Loss = 0.0032959937474442594
1130, epoch_train_loss=0.0032959937474442594
Epoch 1131
Epoch 1131 :: Batch 0/1
Batch Loss = 0.004358839423029898
1131, epoch_train_loss=0.004358839423029898
Epoch 1132
Epoch 1132 :: Batch 0/1
Batch Loss = 0.005238926376639679
1132, epoch_train_loss=0.005238926376639679
Epoch 1133
Epoch 1133 :: Batch 0/1
Batch Loss = 0.004337570479645734
1133, epoch_train_loss=0.004337570479645734
Epoch 1134
Epoch 1134 :: Batch 0/1
Batch Loss = 0.0031812184853793795
1134, epoch_train_loss=0.0031812184853793795
Epoch 1135
Epoch 1135 :: Batch 0/1
Batch Loss = 0.0025967077819513
1135, epoch_train_loss=0.0025967077819513
Epoch 1136
Epoch 1136 :: Batch 0/1
Batch Loss = 0.0027091892078748426
1136, epoch_train_loss=0.0027091892078748426
Epoch 1137
Epoch 1137 :: Batch 0/1
Batch Loss = 0.0031260345332179544
1137, epoch_train_loss=0.0031260345332179544
Epoch 1138
Epoch 1138 :: Batch 0/1
Batch Loss = 0.0035078435142439587
1138, epoch_train_loss=0.0035078435142439587
Epoch 1139
Epoch 1139 :: Batch 0/1
Batch Loss = 0.0035670312275052686
1139, epoch_train_loss=0.0035670312275052686
Epoch 1140
Epoch 1140 :: Batch 0/1
Batch Loss = 0.002887126642497822
1140, epoch_train_loss=0.002887126642497822
Epoch 1141
Epoch 1141 :: Batch 0/1
Batch Loss = 0.00236049602550995
1141, epoch_train_loss=0.00236049602550995
Epoch 1142
Epoch 1142 :: Batch 0/1
Batch Loss = 0.0024264407455139945
1142, epoch_train_loss=0.0024264407455139945
Epoch 1143
Epoch 1143 :: Batch 0/1
Batch Loss = 0.0027489085550464034
1143, epoch_train_loss=0.0027489085550464034
Epoch 1144
Epoch 1144 :: Batch 0/1
Batch Loss = 0.002925756606381281
1144, epoch_train_loss=0.002925756606381281
Epoch 1145
Epoch 1145 :: Batch 0/1
Batch Loss = 0.0028301298105761897
1145, epoch_train_loss=0.0028301298105761897
Epoch 1146
Epoch 1146 :: Batch 0/1
Batch Loss = 0.002628661085402254
1146, epoch_train_loss=0.002628661085402254
Epoch 1147
Epoch 1147 :: Batch 0/1
Batch Loss = 0.0023731579769917426
1147, epoch_train_loss=0.0023731579769917426
Epoch 1148
Epoch 1148 :: Batch 0/1
Batch Loss = 0.0023106151716322256
1148, epoch_train_loss=0.0023106151716322256
Epoch 1149
Epoch 1149 :: Batch 0/1
Batch Loss = 0.0025016693392311633
1149, epoch_train_loss=0.0025016693392311633
Epoch 1150
Epoch 1150 :: Batch 0/1
Batch Loss = 0.002665103527583532
1150, epoch_train_loss=0.002665103527583532
Epoch 1151
Epoch 1151 :: Batch 0/1
Batch Loss = 0.002629031783381464
1151, epoch_train_loss=0.002629031783381464
Epoch 1152
Epoch 1152 :: Batch 0/1
Batch Loss = 0.0024703771741288946
1152, epoch_train_loss=0.0024703771741288946
Epoch 1153
Epoch 1153 :: Batch 0/1
Batch Loss = 0.002362519338926022
1153, epoch_train_loss=0.002362519338926022
Epoch 1154
Epoch 1154 :: Batch 0/1
Batch Loss = 0.0023143578121984395
1154, epoch_train_loss=0.0023143578121984395
Epoch 1155
Epoch 1155 :: Batch 0/1
Batch Loss = 0.002342183305796666
1155, epoch_train_loss=0.002342183305796666
Epoch 1156
Epoch 1156 :: Batch 0/1
Batch Loss = 0.0024365672664260632
1156, epoch_train_loss=0.0024365672664260632
Epoch 1157
Epoch 1157 :: Batch 0/1
Batch Loss = 0.002485525549800487
1157, epoch_train_loss=0.002485525549800487
Epoch 1158
Epoch 1158 :: Batch 0/1
Batch Loss = 0.0024355420559668706
1158, epoch_train_loss=0.0024355420559668706
Epoch 1159
Epoch 1159 :: Batch 0/1
Batch Loss = 0.0023388542718958726
1159, epoch_train_loss=0.0023388542718958726
Epoch 1160
Epoch 1160 :: Batch 0/1
Batch Loss = 0.002293262676575455
1160, epoch_train_loss=0.002293262676575455
Epoch 1161
Epoch 1161 :: Batch 0/1
Batch Loss = 0.002297548847632549
1161, epoch_train_loss=0.002297548847632549
Epoch 1162
Epoch 1162 :: Batch 0/1
Batch Loss = 0.0023219589360373187
1162, epoch_train_loss=0.0023219589360373187
Epoch 1163
Epoch 1163 :: Batch 0/1
Batch Loss = 0.0023568022445147425
1163, epoch_train_loss=0.0023568022445147425
Epoch 1164
Epoch 1164 :: Batch 0/1
Batch Loss = 0.0023722311677130347
1164, epoch_train_loss=0.0023722311677130347
Epoch 1165
Epoch 1165 :: Batch 0/1
Batch Loss = 0.0023459284529086088
1165, epoch_train_loss=0.0023459284529086088
Epoch 1166
Epoch 1166 :: Batch 0/1
Batch Loss = 0.0022967127258085845
1166, epoch_train_loss=0.0022967127258085845
Epoch 1167
Epoch 1167 :: Batch 0/1
Batch Loss = 0.002273034382982519
1167, epoch_train_loss=0.002273034382982519
Epoch 1168
Epoch 1168 :: Batch 0/1
Batch Loss = 0.002281018581343181
1168, epoch_train_loss=0.002281018581343181
Epoch 1169
Epoch 1169 :: Batch 0/1
Batch Loss = 0.002296857737729652
1169, epoch_train_loss=0.002296857737729652
Epoch 1170
Epoch 1170 :: Batch 0/1
Batch Loss = 0.002309175728982602
1170, epoch_train_loss=0.002309175728982602
Epoch 1171
Epoch 1171 :: Batch 0/1
Batch Loss = 0.002314732377635793
1171, epoch_train_loss=0.002314732377635793
Epoch 1172
Epoch 1172 :: Batch 0/1
Batch Loss = 0.00230717758995666
1172, epoch_train_loss=0.00230717758995666
Epoch 1173
Epoch 1173 :: Batch 0/1
Batch Loss = 0.002285128812747775
1173, epoch_train_loss=0.002285128812747775
Epoch 1174
Epoch 1174 :: Batch 0/1
Batch Loss = 0.00226784057651169
1174, epoch_train_loss=0.00226784057651169
Epoch 1175
Epoch 1175 :: Batch 0/1
Batch Loss = 0.0022671488935053423
1175, epoch_train_loss=0.0022671488935053423
Epoch 1176
Epoch 1176 :: Batch 0/1
Batch Loss = 0.002275582838032452
1176, epoch_train_loss=0.002275582838032452
Epoch 1177
Epoch 1177 :: Batch 0/1
Batch Loss = 0.0022826311240543265
1177, epoch_train_loss=0.0022826311240543265
Epoch 1178
Epoch 1178 :: Batch 0/1
Batch Loss = 0.0022861106477298546
1178, epoch_train_loss=0.0022861106477298546
Epoch 1179
Epoch 1179 :: Batch 0/1
Batch Loss = 0.0022859560356982673
1179, epoch_train_loss=0.0022859560356982673
Epoch 1180
Epoch 1180 :: Batch 0/1
Batch Loss = 0.0022788427528102925
1180, epoch_train_loss=0.0022788427528102925
Epoch 1181
Epoch 1181 :: Batch 0/1
Batch Loss = 0.002268487101928125
1181, epoch_train_loss=0.002268487101928125
Epoch 1182
Epoch 1182 :: Batch 0/1
Batch Loss = 0.0022626154355856235
1182, epoch_train_loss=0.0022626154355856235
Epoch 1183
Epoch 1183 :: Batch 0/1
Batch Loss = 0.0022635570005762294
1183, epoch_train_loss=0.0022635570005762294
Epoch 1184
Epoch 1184 :: Batch 0/1
Batch Loss = 0.002266894488025567
1184, epoch_train_loss=0.002266894488025567
Epoch 1185
Epoch 1185 :: Batch 0/1
Batch Loss = 0.0022694588319170077
1185, epoch_train_loss=0.0022694588319170077
Epoch 1186
Epoch 1186 :: Batch 0/1
Batch Loss = 0.0022713872767115783
1186, epoch_train_loss=0.0022713872767115783
Epoch 1187
Epoch 1187 :: Batch 0/1
Batch Loss = 0.0022715830926670327
1187, epoch_train_loss=0.0022715830926670327
Epoch 1188
Epoch 1188 :: Batch 0/1
Batch Loss = 0.0022687105712579972
1188, epoch_train_loss=0.0022687105712579972
Epoch 1189
Epoch 1189 :: Batch 0/1
Batch Loss = 0.0022639662748457005
1189, epoch_train_loss=0.0022639662748457005
Epoch 1190
Epoch 1190 :: Batch 0/1
Batch Loss = 0.002260731328645432
1190, epoch_train_loss=0.002260731328645432
Epoch 1191
Epoch 1191 :: Batch 0/1
Batch Loss = 0.0022599723153879227
1191, epoch_train_loss=0.0022599723153879227
Epoch 1192
Epoch 1192 :: Batch 0/1
Batch Loss = 0.002260353683109968
1192, epoch_train_loss=0.002260353683109968
Epoch 1193
Epoch 1193 :: Batch 0/1
Batch Loss = 0.00226108678673971
1193, epoch_train_loss=0.00226108678673971
Epoch 1194
Epoch 1194 :: Batch 0/1
Batch Loss = 0.002262282324259464
1194, epoch_train_loss=0.002262282324259464
Epoch 1195
Epoch 1195 :: Batch 0/1
Batch Loss = 0.0022633729103511196
1195, epoch_train_loss=0.0022633729103511196
Epoch 1196
Epoch 1196 :: Batch 0/1
Batch Loss = 0.0022631703842073596
1196, epoch_train_loss=0.0022631703842073596
Epoch 1197
Epoch 1197 :: Batch 0/1
Batch Loss = 0.0022617227656227808
1197, epoch_train_loss=0.0022617227656227808
Epoch 1198
Epoch 1198 :: Batch 0/1
Batch Loss = 0.002260049931246157
1198, epoch_train_loss=0.002260049931246157
Epoch 1199
Epoch 1199 :: Batch 0/1
Batch Loss = 0.002258831379931919
1199, epoch_train_loss=0.002258831379931919
Epoch 1200
Epoch 1200 :: Batch 0/1
Batch Loss = 0.0022578870347992656
1200, epoch_train_loss=0.0022578870347992656
Epoch 1201
Epoch 1201 :: Batch 0/1
Batch Loss = 0.002257182538249888
1201, epoch_train_loss=0.002257182538249888
Epoch 1202
Epoch 1202 :: Batch 0/1
Batch Loss = 0.0022570402414760484
1202, epoch_train_loss=0.0022570402414760484
Epoch 1203
Epoch 1203 :: Batch 0/1
Batch Loss = 0.0022574588277384858
1203, epoch_train_loss=0.0022574588277384858
Epoch 1204
Epoch 1204 :: Batch 0/1
Batch Loss = 0.0022579260086566318
1204, epoch_train_loss=0.0022579260086566318
Epoch 1205
Epoch 1205 :: Batch 0/1
Batch Loss = 0.002258047902107535
1205, epoch_train_loss=0.002258047902107535
Epoch 1206
Epoch 1206 :: Batch 0/1
Batch Loss = 0.002257928913386152
1206, epoch_train_loss=0.002257928913386152
Epoch 1207
Epoch 1207 :: Batch 0/1
Batch Loss = 0.0022577168603886448
1207, epoch_train_loss=0.0022577168603886448
Epoch 1208
Epoch 1208 :: Batch 0/1
Batch Loss = 0.002257347722312382
1208, epoch_train_loss=0.002257347722312382
Epoch 1209
Epoch 1209 :: Batch 0/1
Batch Loss = 0.0022567275552309064
1209, epoch_train_loss=0.0022567275552309064
Epoch 1210
Epoch 1210 :: Batch 0/1
Batch Loss = 0.0022560399505184756
1210, epoch_train_loss=0.0022560399505184756
Epoch 1211
Epoch 1211 :: Batch 0/1
Batch Loss = 0.0022555075770695817
1211, epoch_train_loss=0.0022555075770695817
Epoch 1212
Epoch 1212 :: Batch 0/1
Batch Loss = 0.002255149867735147
1212, epoch_train_loss=0.002255149867735147
Epoch 1213
Epoch 1213 :: Batch 0/1
Batch Loss = 0.0022548519329400837
1213, epoch_train_loss=0.0022548519329400837
Epoch 1214
Epoch 1214 :: Batch 0/1
Batch Loss = 0.0022546002394626184
1214, epoch_train_loss=0.0022546002394626184
Epoch 1215
Epoch 1215 :: Batch 0/1
Batch Loss = 0.0022544692120704166
1215, epoch_train_loss=0.0022544692120704166
Epoch 1216
Epoch 1216 :: Batch 0/1
Batch Loss = 0.0022544494130036148
1216, epoch_train_loss=0.0022544494130036148
Epoch 1217
Epoch 1217 :: Batch 0/1
Batch Loss = 0.0022544315086933915
1217, epoch_train_loss=0.0022544315086933915
Epoch 1218
Epoch 1218 :: Batch 0/1
Batch Loss = 0.0022543479725633
1218, epoch_train_loss=0.0022543479725633
Epoch 1219
Epoch 1219 :: Batch 0/1
Batch Loss = 0.00225423543158893
1219, epoch_train_loss=0.00225423543158893
Epoch 1220
Epoch 1220 :: Batch 0/1
Batch Loss = 0.002254129277085261
1220, epoch_train_loss=0.002254129277085261
Epoch 1221
Epoch 1221 :: Batch 0/1
Batch Loss = 0.0022540046692850414
1221, epoch_train_loss=0.0022540046692850414
Epoch 1222
Epoch 1222 :: Batch 0/1
Batch Loss = 0.002253816782167422
1222, epoch_train_loss=0.002253816782167422
Epoch 1223
Epoch 1223 :: Batch 0/1
Batch Loss = 0.002253585616479857
1223, epoch_train_loss=0.002253585616479857
Epoch 1224
Epoch 1224 :: Batch 0/1
Batch Loss = 0.0022533551105149187
1224, epoch_train_loss=0.0022533551105149187
Epoch 1225
Epoch 1225 :: Batch 0/1
Batch Loss = 0.002253141670996649
1225, epoch_train_loss=0.002253141670996649
Epoch 1226
Epoch 1226 :: Batch 0/1
Batch Loss = 0.002252918509074657
1226, epoch_train_loss=0.002252918509074657
Epoch 1227
Epoch 1227 :: Batch 0/1
Batch Loss = 0.002252678414250086
1227, epoch_train_loss=0.002252678414250086
Epoch 1228
Epoch 1228 :: Batch 0/1
Batch Loss = 0.0022524407130862954
1228, epoch_train_loss=0.0022524407130862954
Epoch 1229
Epoch 1229 :: Batch 0/1
Batch Loss = 0.002252227348298742
1229, epoch_train_loss=0.002252227348298742
Epoch 1230
Epoch 1230 :: Batch 0/1
Batch Loss = 0.0022520290063563616
1230, epoch_train_loss=0.0022520290063563616
Epoch 1231
Epoch 1231 :: Batch 0/1
Batch Loss = 0.0022518290985253067
1231, epoch_train_loss=0.0022518290985253067
Epoch 1232
Epoch 1232 :: Batch 0/1
Batch Loss = 0.0022516263224951097
1232, epoch_train_loss=0.0022516263224951097
Epoch 1233
Epoch 1233 :: Batch 0/1
Batch Loss = 0.0022514353928185476
1233, epoch_train_loss=0.0022514353928185476
Epoch 1234
Epoch 1234 :: Batch 0/1
Batch Loss = 0.0022512608674018927
1234, epoch_train_loss=0.0022512608674018927
Epoch 1235
Epoch 1235 :: Batch 0/1
Batch Loss = 0.0022510943413072584
1235, epoch_train_loss=0.0022510943413072584
Epoch 1236
Epoch 1236 :: Batch 0/1
Batch Loss = 0.0022509267674563414
1236, epoch_train_loss=0.0022509267674563414
Epoch 1237
Epoch 1237 :: Batch 0/1
Batch Loss = 0.0022507631693149104
1237, epoch_train_loss=0.0022507631693149104
Epoch 1238
Epoch 1238 :: Batch 0/1
Batch Loss = 0.0022506121769179537
1238, epoch_train_loss=0.0022506121769179537
Epoch 1239
Epoch 1239 :: Batch 0/1
Batch Loss = 0.002250477250278497
1239, epoch_train_loss=0.002250477250278497
Epoch 1240
Epoch 1240 :: Batch 0/1
Batch Loss = 0.0022503541713599726
1240, epoch_train_loss=0.0022503541713599726
Epoch 1241
Epoch 1241 :: Batch 0/1
Batch Loss = 0.0022502451488711312
1241, epoch_train_loss=0.0022502451488711312
Epoch 1242
Epoch 1242 :: Batch 0/1
Batch Loss = 0.002250158887434498
1242, epoch_train_loss=0.002250158887434498
Epoch 1243
Epoch 1243 :: Batch 0/1
Batch Loss = 0.002250111773351926
1243, epoch_train_loss=0.002250111773351926
Epoch 1244
Epoch 1244 :: Batch 0/1
Batch Loss = 0.002250115615022638
1244, epoch_train_loss=0.002250115615022638
Epoch 1245
Epoch 1245 :: Batch 0/1
Batch Loss = 0.002250193092253632
1245, epoch_train_loss=0.002250193092253632
Epoch 1246
Epoch 1246 :: Batch 0/1
Batch Loss = 0.002250371085885765
1246, epoch_train_loss=0.002250371085885765
Epoch 1247
Epoch 1247 :: Batch 0/1
Batch Loss = 0.0022507126555500363
1247, epoch_train_loss=0.0022507126555500363
Epoch 1248
Epoch 1248 :: Batch 0/1
Batch Loss = 0.002251285801084229
1248, epoch_train_loss=0.002251285801084229
Epoch 1249
Epoch 1249 :: Batch 0/1
Batch Loss = 0.002252237069836035
1249, epoch_train_loss=0.002252237069836035
Epoch 1250
Epoch 1250 :: Batch 0/1
Batch Loss = 0.002253719406803965
1250, epoch_train_loss=0.002253719406803965
Epoch 1251
Epoch 1251 :: Batch 0/1
Batch Loss = 0.0022561064718842434
1251, epoch_train_loss=0.0022561064718842434
Epoch 1252
Epoch 1252 :: Batch 0/1
Batch Loss = 0.002259768792892261
1252, epoch_train_loss=0.002259768792892261
Epoch 1253
Epoch 1253 :: Batch 0/1
Batch Loss = 0.0022657110988412725
1253, epoch_train_loss=0.0022657110988412725
Epoch 1254
Epoch 1254 :: Batch 0/1
Batch Loss = 0.0022748037353056407
1254, epoch_train_loss=0.0022748037353056407
Epoch 1255
Epoch 1255 :: Batch 0/1
Batch Loss = 0.0022898792654260916
1255, epoch_train_loss=0.0022898792654260916
Epoch 1256
Epoch 1256 :: Batch 0/1
Batch Loss = 0.002312888630070256
1256, epoch_train_loss=0.002312888630070256
Epoch 1257
Epoch 1257 :: Batch 0/1
Batch Loss = 0.0023522841800448736
1257, epoch_train_loss=0.0023522841800448736
Epoch 1258
Epoch 1258 :: Batch 0/1
Batch Loss = 0.0024117349101873916
1258, epoch_train_loss=0.0024117349101873916
Epoch 1259
Epoch 1259 :: Batch 0/1
Batch Loss = 0.0025178532953016764
1259, epoch_train_loss=0.0025178532953016764
Epoch 1260
Epoch 1260 :: Batch 0/1
Batch Loss = 0.002672805048697934
1260, epoch_train_loss=0.002672805048697934
Epoch 1261
Epoch 1261 :: Batch 0/1
Batch Loss = 0.0029641300570256504
1261, epoch_train_loss=0.0029641300570256504
Epoch 1262
Epoch 1262 :: Batch 0/1
Batch Loss = 0.0033567823977555078
1262, epoch_train_loss=0.0033567823977555078
Epoch 1263
Epoch 1263 :: Batch 0/1
Batch Loss = 0.004141377134444541
1263, epoch_train_loss=0.004141377134444541
Epoch 1264
Epoch 1264 :: Batch 0/1
Batch Loss = 0.005012709101202413
1264, epoch_train_loss=0.005012709101202413
Epoch 1265
Epoch 1265 :: Batch 0/1
Batch Loss = 0.006867115339279683
1265, epoch_train_loss=0.006867115339279683
Epoch 1266
Epoch 1266 :: Batch 0/1
Batch Loss = 0.008051322698065622
1266, epoch_train_loss=0.008051322698065622
Epoch 1267
Epoch 1267 :: Batch 0/1
Batch Loss = 0.010806364104706465
1267, epoch_train_loss=0.010806364104706465
Epoch 1268
Epoch 1268 :: Batch 0/1
Batch Loss = 0.010042758953977821
1268, epoch_train_loss=0.010042758953977821
Epoch 1269
Epoch 1269 :: Batch 0/1
Batch Loss = 0.010056358433014554
1269, epoch_train_loss=0.010056358433014554
Epoch 1270
Epoch 1270 :: Batch 0/1
Batch Loss = 0.006205917109997467
1270, epoch_train_loss=0.006205917109997467
Epoch 1271
Epoch 1271 :: Batch 0/1
Batch Loss = 0.003490611610478366
1271, epoch_train_loss=0.003490611610478366
Epoch 1272
Epoch 1272 :: Batch 0/1
Batch Loss = 0.0022969263882334236
1272, epoch_train_loss=0.0022969263882334236
Epoch 1273
Epoch 1273 :: Batch 0/1
Batch Loss = 0.003156154452708251
1273, epoch_train_loss=0.003156154452708251
Epoch 1274
Epoch 1274 :: Batch 0/1
Batch Loss = 0.00504144184287083
1274, epoch_train_loss=0.00504144184287083
Epoch 1275
Epoch 1275 :: Batch 0/1
Batch Loss = 0.005674763921503007
1275, epoch_train_loss=0.005674763921503007
Epoch 1276
Epoch 1276 :: Batch 0/1
Batch Loss = 0.005384647871471863
1276, epoch_train_loss=0.005384647871471863
Epoch 1277
Epoch 1277 :: Batch 0/1
Batch Loss = 0.003504670319356104
1277, epoch_train_loss=0.003504670319356104
Epoch 1278
Epoch 1278 :: Batch 0/1
Batch Loss = 0.0024090405012894872
1278, epoch_train_loss=0.0024090405012894872
Epoch 1279
Epoch 1279 :: Batch 0/1
Batch Loss = 0.0027095061849813796
1279, epoch_train_loss=0.0027095061849813796
Epoch 1280
Epoch 1280 :: Batch 0/1
Batch Loss = 0.003626817134489108
1280, epoch_train_loss=0.003626817134489108
Epoch 1281
Epoch 1281 :: Batch 0/1
Batch Loss = 0.004139150802680834
1281, epoch_train_loss=0.004139150802680834
Epoch 1282
Epoch 1282 :: Batch 0/1
Batch Loss = 0.0034263477781690506
1282, epoch_train_loss=0.0034263477781690506
Epoch 1283
Epoch 1283 :: Batch 0/1
Batch Loss = 0.0026088013160002503
1283, epoch_train_loss=0.0026088013160002503
Epoch 1284
Epoch 1284 :: Batch 0/1
Batch Loss = 0.0023437782728173624
1284, epoch_train_loss=0.0023437782728173624
Epoch 1285
Epoch 1285 :: Batch 0/1
Batch Loss = 0.0027302722943737968
1285, epoch_train_loss=0.0027302722943737968
Epoch 1286
Epoch 1286 :: Batch 0/1
Batch Loss = 0.003192483149400377
1286, epoch_train_loss=0.003192483149400377
Epoch 1287
Epoch 1287 :: Batch 0/1
Batch Loss = 0.0030543513313847632
1287, epoch_train_loss=0.0030543513313847632
Epoch 1288
Epoch 1288 :: Batch 0/1
Batch Loss = 0.002628463789013822
1288, epoch_train_loss=0.002628463789013822
Epoch 1289
Epoch 1289 :: Batch 0/1
Batch Loss = 0.002342016765708125
1289, epoch_train_loss=0.002342016765708125
Epoch 1290
Epoch 1290 :: Batch 0/1
Batch Loss = 0.002451267652689115
1290, epoch_train_loss=0.002451267652689115
Epoch 1291
Epoch 1291 :: Batch 0/1
Batch Loss = 0.002725561239352317
1291, epoch_train_loss=0.002725561239352317
Epoch 1292
Epoch 1292 :: Batch 0/1
Batch Loss = 0.0027771559199530594
1292, epoch_train_loss=0.0027771559199530594
Epoch 1293
Epoch 1293 :: Batch 0/1
Batch Loss = 0.002614731870521784
1293, epoch_train_loss=0.002614731870521784
Epoch 1294
Epoch 1294 :: Batch 0/1
Batch Loss = 0.002386240243058275
1294, epoch_train_loss=0.002386240243058275
Epoch 1295
Epoch 1295 :: Batch 0/1
Batch Loss = 0.002323255451985513
1295, epoch_train_loss=0.002323255451985513
Epoch 1296
Epoch 1296 :: Batch 0/1
Batch Loss = 0.002422451232997051
1296, epoch_train_loss=0.002422451232997051
Epoch 1297
Epoch 1297 :: Batch 0/1
Batch Loss = 0.002515268056383336
1297, epoch_train_loss=0.002515268056383336
Epoch 1298
Epoch 1298 :: Batch 0/1
Batch Loss = 0.0025089293385834157
1298, epoch_train_loss=0.0025089293385834157
Epoch 1299
Epoch 1299 :: Batch 0/1
Batch Loss = 0.002404232756345822
1299, epoch_train_loss=0.002404232756345822
Epoch 1300
Epoch 1300 :: Batch 0/1
Batch Loss = 0.002318895305843684
1300, epoch_train_loss=0.002318895305843684
Epoch 1301
Epoch 1301 :: Batch 0/1
Batch Loss = 0.002308638484116183
1301, epoch_train_loss=0.002308638484116183
Epoch 1302
Epoch 1302 :: Batch 0/1
Batch Loss = 0.002351340217560054
1302, epoch_train_loss=0.002351340217560054
Epoch 1303
Epoch 1303 :: Batch 0/1
Batch Loss = 0.002393383782353455
1303, epoch_train_loss=0.002393383782353455
Epoch 1304
Epoch 1304 :: Batch 0/1
Batch Loss = 0.0023856666499501972
1304, epoch_train_loss=0.0023856666499501972
Epoch 1305
Epoch 1305 :: Batch 0/1
Batch Loss = 0.0023440911846707908
1305, epoch_train_loss=0.0023440911846707908
Epoch 1306
Epoch 1306 :: Batch 0/1
Batch Loss = 0.0022986496682670787
1306, epoch_train_loss=0.0022986496682670787
Epoch 1307
Epoch 1307 :: Batch 0/1
Batch Loss = 0.0022804310378494804
1307, epoch_train_loss=0.0022804310378494804
Epoch 1308
Epoch 1308 :: Batch 0/1
Batch Loss = 0.002293641497853681
1308, epoch_train_loss=0.002293641497853681
Epoch 1309
Epoch 1309 :: Batch 0/1
Batch Loss = 0.0023163558046511556
1309, epoch_train_loss=0.0023163558046511556
Epoch 1310
Epoch 1310 :: Batch 0/1
Batch Loss = 0.0023269793795526105
1310, epoch_train_loss=0.0023269793795526105
Epoch 1311
Epoch 1311 :: Batch 0/1
Batch Loss = 0.0023121900240821135
1311, epoch_train_loss=0.0023121900240821135
Epoch 1312
Epoch 1312 :: Batch 0/1
Batch Loss = 0.0022841220970780895
1312, epoch_train_loss=0.0022841220970780895
Epoch 1313
Epoch 1313 :: Batch 0/1
Batch Loss = 0.0022624412315515835
1313, epoch_train_loss=0.0022624412315515835
Epoch 1314
Epoch 1314 :: Batch 0/1
Batch Loss = 0.0022598424290527484
1314, epoch_train_loss=0.0022598424290527484
Epoch 1315
Epoch 1315 :: Batch 0/1
Batch Loss = 0.0022730836403531763
1315, epoch_train_loss=0.0022730836403531763
Epoch 1316
Epoch 1316 :: Batch 0/1
Batch Loss = 0.002286267905864793
1316, epoch_train_loss=0.002286267905864793
Epoch 1317
Epoch 1317 :: Batch 0/1
Batch Loss = 0.00228724987740541
1317, epoch_train_loss=0.00228724987740541
Epoch 1318
Epoch 1318 :: Batch 0/1
Batch Loss = 0.002274156486031822
1318, epoch_train_loss=0.002274156486031822
Epoch 1319
Epoch 1319 :: Batch 0/1
Batch Loss = 0.0022571276090897823
1319, epoch_train_loss=0.0022571276090897823
Epoch 1320
Epoch 1320 :: Batch 0/1
Batch Loss = 0.0022480652273773544
1320, epoch_train_loss=0.0022480652273773544
Epoch 1321
Epoch 1321 :: Batch 0/1
Batch Loss = 0.002251116611420225
1321, epoch_train_loss=0.002251116611420225
Epoch 1322
Epoch 1322 :: Batch 0/1
Batch Loss = 0.002260940322672213
1322, epoch_train_loss=0.002260940322672213
Epoch 1323
Epoch 1323 :: Batch 0/1
Batch Loss = 0.002267648422494784
1323, epoch_train_loss=0.002267648422494784
Epoch 1324
Epoch 1324 :: Batch 0/1
Batch Loss = 0.0022655076888158402
1324, epoch_train_loss=0.0022655076888158402
Epoch 1325
Epoch 1325 :: Batch 0/1
Batch Loss = 0.0022562398475271246
1325, epoch_train_loss=0.0022562398475271246
Epoch 1326
Epoch 1326 :: Batch 0/1
Batch Loss = 0.0022468453801529013
1326, epoch_train_loss=0.0022468453801529013
Epoch 1327
Epoch 1327 :: Batch 0/1
Batch Loss = 0.002243374571424747
1327, epoch_train_loss=0.002243374571424747
Epoch 1328
Epoch 1328 :: Batch 0/1
Batch Loss = 0.00224652578578557
1328, epoch_train_loss=0.00224652578578557
Epoch 1329
Epoch 1329 :: Batch 0/1
Batch Loss = 0.0022521860673755225
1329, epoch_train_loss=0.0022521860673755225
Epoch 1330
Epoch 1330 :: Batch 0/1
Batch Loss = 0.002255275449668354
1330, epoch_train_loss=0.002255275449668354
Epoch 1331
Epoch 1331 :: Batch 0/1
Batch Loss = 0.0022536131393623812
1331, epoch_train_loss=0.0022536131393623812
Epoch 1332
Epoch 1332 :: Batch 0/1
Batch Loss = 0.0022486550904067468
1332, epoch_train_loss=0.0022486550904067468
Epoch 1333
Epoch 1333 :: Batch 0/1
Batch Loss = 0.0022437803993997306
1333, epoch_train_loss=0.0022437803993997306
Epoch 1334
Epoch 1334 :: Batch 0/1
Batch Loss = 0.0022416369726912066
1334, epoch_train_loss=0.0022416369726912066
Epoch 1335
Epoch 1335 :: Batch 0/1
Batch Loss = 0.002242644027882164
1335, epoch_train_loss=0.002242644027882164
Epoch 1336
Epoch 1336 :: Batch 0/1
Batch Loss = 0.0022452335166410968
1336, epoch_train_loss=0.0022452335166410968
Epoch 1337
Epoch 1337 :: Batch 0/1
Batch Loss = 0.002247210523211456
1337, epoch_train_loss=0.002247210523211456
Epoch 1338
Epoch 1338 :: Batch 0/1
Batch Loss = 0.0022472049147144024
1338, epoch_train_loss=0.0022472049147144024
Epoch 1339
Epoch 1339 :: Batch 0/1
Batch Loss = 0.002245305912465936
1339, epoch_train_loss=0.002245305912465936
Epoch 1340
Epoch 1340 :: Batch 0/1
Batch Loss = 0.002242723013449429
1340, epoch_train_loss=0.002242723013449429
Epoch 1341
Epoch 1341 :: Batch 0/1
Batch Loss = 0.0022408185595287054
1341, epoch_train_loss=0.0022408185595287054
Epoch 1342
Epoch 1342 :: Batch 0/1
Batch Loss = 0.002240291121649915
1342, epoch_train_loss=0.002240291121649915
Epoch 1343
Epoch 1343 :: Batch 0/1
Batch Loss = 0.0022409346130947863
1343, epoch_train_loss=0.0022409346130947863
Epoch 1344
Epoch 1344 :: Batch 0/1
Batch Loss = 0.0022419893598140833
1344, epoch_train_loss=0.0022419893598140833
Epoch 1345
Epoch 1345 :: Batch 0/1
Batch Loss = 0.00224267539203151
1345, epoch_train_loss=0.00224267539203151
Epoch 1346
Epoch 1346 :: Batch 0/1
Batch Loss = 0.0022425855200878825
1346, epoch_train_loss=0.0022425855200878825
Epoch 1347
Epoch 1347 :: Batch 0/1
Batch Loss = 0.0022417797175378587
1347, epoch_train_loss=0.0022417797175378587
Epoch 1348
Epoch 1348 :: Batch 0/1
Batch Loss = 0.0022406396857168847
1348, epoch_train_loss=0.0022406396857168847
Epoch 1349
Epoch 1349 :: Batch 0/1
Batch Loss = 0.0022396398412509394
1349, epoch_train_loss=0.0022396398412509394
Epoch 1350
Epoch 1350 :: Batch 0/1
Batch Loss = 0.0022390948149835273
1350, epoch_train_loss=0.0022390948149835273
Epoch 1351
Epoch 1351 :: Batch 0/1
Batch Loss = 0.00223904987002981
1351, epoch_train_loss=0.00223904987002981
Epoch 1352
Epoch 1352 :: Batch 0/1
Batch Loss = 0.0022393154905801928
1352, epoch_train_loss=0.0022393154905801928
Epoch 1353
Epoch 1353 :: Batch 0/1
Batch Loss = 0.0022396140058165335
1353, epoch_train_loss=0.0022396140058165335
Epoch 1354
Epoch 1354 :: Batch 0/1
Batch Loss = 0.0022397267256659237
1354, epoch_train_loss=0.0022397267256659237
Epoch 1355
Epoch 1355 :: Batch 0/1
Batch Loss = 0.002239566759755314
1355, epoch_train_loss=0.002239566759755314
Epoch 1356
Epoch 1356 :: Batch 0/1
Batch Loss = 0.002239173589704638
1356, epoch_train_loss=0.002239173589704638
Epoch 1357
Epoch 1357 :: Batch 0/1
Batch Loss = 0.002238662929737611
1357, epoch_train_loss=0.002238662929737611
Epoch 1358
Epoch 1358 :: Batch 0/1
Batch Loss = 0.0022381756353945345
1358, epoch_train_loss=0.0022381756353945345
Epoch 1359
Epoch 1359 :: Batch 0/1
Batch Loss = 0.00223781584222428
1359, epoch_train_loss=0.00223781584222428
Epoch 1360
Epoch 1360 :: Batch 0/1
Batch Loss = 0.0022376223213297357
1360, epoch_train_loss=0.0022376223213297357
Epoch 1361
Epoch 1361 :: Batch 0/1
Batch Loss = 0.0022375635350979596
1361, epoch_train_loss=0.0022375635350979596
Epoch 1362
Epoch 1362 :: Batch 0/1
Batch Loss = 0.0022375700533521755
1362, epoch_train_loss=0.0022375700533521755
Epoch 1363
Epoch 1363 :: Batch 0/1
Batch Loss = 0.002237570950642083
1363, epoch_train_loss=0.002237570950642083
Epoch 1364
Epoch 1364 :: Batch 0/1
Batch Loss = 0.0022375167173367947
1364, epoch_train_loss=0.0022375167173367947
Epoch 1365
Epoch 1365 :: Batch 0/1
Batch Loss = 0.0022373862982134813
1365, epoch_train_loss=0.0022373862982134813
Epoch 1366
Epoch 1366 :: Batch 0/1
Batch Loss = 0.002237183548633503
1366, epoch_train_loss=0.002237183548633503
Epoch 1367
Epoch 1367 :: Batch 0/1
Batch Loss = 0.002236934634948042
1367, epoch_train_loss=0.002236934634948042
Epoch 1368
Epoch 1368 :: Batch 0/1
Batch Loss = 0.0022366737100198014
1368, epoch_train_loss=0.0022366737100198014
Epoch 1369
Epoch 1369 :: Batch 0/1
Batch Loss = 0.0022364323402784287
1369, epoch_train_loss=0.0022364323402784287
Epoch 1370
Epoch 1370 :: Batch 0/1
Batch Loss = 0.002236227449182111
1370, epoch_train_loss=0.002236227449182111
Epoch 1371
Epoch 1371 :: Batch 0/1
Batch Loss = 0.002236063042461596
1371, epoch_train_loss=0.002236063042461596
Epoch 1372
Epoch 1372 :: Batch 0/1
Batch Loss = 0.0022359334846005874
1372, epoch_train_loss=0.0022359334846005874
Epoch 1373
Epoch 1373 :: Batch 0/1
Batch Loss = 0.002235828266596721
1373, epoch_train_loss=0.002235828266596721
Epoch 1374
Epoch 1374 :: Batch 0/1
Batch Loss = 0.0022357337541506045
1374, epoch_train_loss=0.0022357337541506045
Epoch 1375
Epoch 1375 :: Batch 0/1
Batch Loss = 0.002235636507553922
1375, epoch_train_loss=0.002235636507553922
Epoch 1376
Epoch 1376 :: Batch 0/1
Batch Loss = 0.0022355272858961615
1376, epoch_train_loss=0.0022355272858961615
Epoch 1377
Epoch 1377 :: Batch 0/1
Batch Loss = 0.0022354021550366563
1377, epoch_train_loss=0.0022354021550366563
Epoch 1378
Epoch 1378 :: Batch 0/1
Batch Loss = 0.002235262063780521
1378, epoch_train_loss=0.002235262063780521
Epoch 1379
Epoch 1379 :: Batch 0/1
Batch Loss = 0.0022351092199931867
1379, epoch_train_loss=0.0022351092199931867
Epoch 1380
Epoch 1380 :: Batch 0/1
Batch Loss = 0.002234947671898435
1380, epoch_train_loss=0.002234947671898435
Epoch 1381
Epoch 1381 :: Batch 0/1
Batch Loss = 0.0022347816750307036
1381, epoch_train_loss=0.0022347816750307036
Epoch 1382
Epoch 1382 :: Batch 0/1
Batch Loss = 0.002234616269398354
1382, epoch_train_loss=0.002234616269398354
Epoch 1383
Epoch 1383 :: Batch 0/1
Batch Loss = 0.0022344545951479664
1383, epoch_train_loss=0.0022344545951479664
Epoch 1384
Epoch 1384 :: Batch 0/1
Batch Loss = 0.0022342984866186976
1384, epoch_train_loss=0.0022342984866186976
Epoch 1385
Epoch 1385 :: Batch 0/1
Batch Loss = 0.002234148266763514
1385, epoch_train_loss=0.002234148266763514
Epoch 1386
Epoch 1386 :: Batch 0/1
Batch Loss = 0.002234004143525826
1386, epoch_train_loss=0.002234004143525826
Epoch 1387
Epoch 1387 :: Batch 0/1
Batch Loss = 0.0022338656773145836
1387, epoch_train_loss=0.0022338656773145836
Epoch 1388
Epoch 1388 :: Batch 0/1
Batch Loss = 0.0022337318708154806
1388, epoch_train_loss=0.0022337318708154806
Epoch 1389
Epoch 1389 :: Batch 0/1
Batch Loss = 0.002233601382936214
1389, epoch_train_loss=0.002233601382936214
Epoch 1390
Epoch 1390 :: Batch 0/1
Batch Loss = 0.0022334731355372808
1390, epoch_train_loss=0.0022334731355372808
Epoch 1391
Epoch 1391 :: Batch 0/1
Batch Loss = 0.00223334654143935
1391, epoch_train_loss=0.00223334654143935
Epoch 1392
Epoch 1392 :: Batch 0/1
Batch Loss = 0.0022332210648534634
1392, epoch_train_loss=0.0022332210648534634
Epoch 1393
Epoch 1393 :: Batch 0/1
Batch Loss = 0.0022330962859056663
1393, epoch_train_loss=0.0022330962859056663
Epoch 1394
Epoch 1394 :: Batch 0/1
Batch Loss = 0.0022329718531422806
1394, epoch_train_loss=0.0022329718531422806
Epoch 1395
Epoch 1395 :: Batch 0/1
Batch Loss = 0.0022328479899889507
1395, epoch_train_loss=0.0022328479899889507
Epoch 1396
Epoch 1396 :: Batch 0/1
Batch Loss = 0.002232724915261324
1396, epoch_train_loss=0.002232724915261324
Epoch 1397
Epoch 1397 :: Batch 0/1
Batch Loss = 0.002232603160279409
1397, epoch_train_loss=0.002232603160279409
Epoch 1398
Epoch 1398 :: Batch 0/1
Batch Loss = 0.0022324830079614435
1398, epoch_train_loss=0.0022324830079614435
Epoch 1399
Epoch 1399 :: Batch 0/1
Batch Loss = 0.0022323653937589712
1399, epoch_train_loss=0.0022323653937589712
Epoch 1400
Epoch 1400 :: Batch 0/1
Batch Loss = 0.002232251220057982
1400, epoch_train_loss=0.002232251220057982
Epoch 1401
Epoch 1401 :: Batch 0/1
Batch Loss = 0.0022321421620194733
1401, epoch_train_loss=0.0022321421620194733
Epoch 1402
Epoch 1402 :: Batch 0/1
Batch Loss = 0.0022320398540553146
1402, epoch_train_loss=0.0022320398540553146
Epoch 1403
Epoch 1403 :: Batch 0/1
Batch Loss = 0.0022319472502411205
1403, epoch_train_loss=0.0022319472502411205
Epoch 1404
Epoch 1404 :: Batch 0/1
Batch Loss = 0.002231867925831036
1404, epoch_train_loss=0.002231867925831036
Epoch 1405
Epoch 1405 :: Batch 0/1
Batch Loss = 0.0022318079100361395
1405, epoch_train_loss=0.0022318079100361395
Epoch 1406
Epoch 1406 :: Batch 0/1
Batch Loss = 0.002231774953250408
1406, epoch_train_loss=0.002231774953250408
Epoch 1407
Epoch 1407 :: Batch 0/1
Batch Loss = 0.002231781678221746
1407, epoch_train_loss=0.002231781678221746
Epoch 1408
Epoch 1408 :: Batch 0/1
Batch Loss = 0.0022318456207086695
1408, epoch_train_loss=0.0022318456207086695
Epoch 1409
Epoch 1409 :: Batch 0/1
Batch Loss = 0.0022319950405396724
1409, epoch_train_loss=0.0022319950405396724
Epoch 1410
Epoch 1410 :: Batch 0/1
Batch Loss = 0.0022322709851537942
1410, epoch_train_loss=0.0022322709851537942
Epoch 1411
Epoch 1411 :: Batch 0/1
Batch Loss = 0.002232739206652328
1411, epoch_train_loss=0.002232739206652328
Epoch 1412
Epoch 1412 :: Batch 0/1
Batch Loss = 0.0022334990237409423
1412, epoch_train_loss=0.0022334990237409423
Epoch 1413
Epoch 1413 :: Batch 0/1
Batch Loss = 0.002234709951060591
1413, epoch_train_loss=0.002234709951060591
Epoch 1414
Epoch 1414 :: Batch 0/1
Batch Loss = 0.002236621359808131
1414, epoch_train_loss=0.002236621359808131
Epoch 1415
Epoch 1415 :: Batch 0/1
Batch Loss = 0.0022396346388024306
1415, epoch_train_loss=0.0022396346388024306
Epoch 1416
Epoch 1416 :: Batch 0/1
Batch Loss = 0.0022443991752001554
1416, epoch_train_loss=0.0022443991752001554
Epoch 1417
Epoch 1417 :: Batch 0/1
Batch Loss = 0.0022519585947279506
1417, epoch_train_loss=0.0022519585947279506
Epoch 1418
Epoch 1418 :: Batch 0/1
Batch Loss = 0.002264065491360011
1418, epoch_train_loss=0.002264065491360011
Epoch 1419
Epoch 1419 :: Batch 0/1
Batch Loss = 0.0022835024883841627
1419, epoch_train_loss=0.0022835024883841627
Epoch 1420
Epoch 1420 :: Batch 0/1
Batch Loss = 0.00231515040789402
1420, epoch_train_loss=0.00231515040789402
Epoch 1421
Epoch 1421 :: Batch 0/1
Batch Loss = 0.002366498262762405
1421, epoch_train_loss=0.002366498262762405
Epoch 1422
Epoch 1422 :: Batch 0/1
Batch Loss = 0.0024513926581602157
1422, epoch_train_loss=0.0024513926581602157
Epoch 1423
Epoch 1423 :: Batch 0/1
Batch Loss = 0.0025894108969588938
1423, epoch_train_loss=0.0025894108969588938
Epoch 1424
Epoch 1424 :: Batch 0/1
Batch Loss = 0.0028190928202074753
1424, epoch_train_loss=0.0028190928202074753
Epoch 1425
Epoch 1425 :: Batch 0/1
Batch Loss = 0.0031850014933312537
1425, epoch_train_loss=0.0031850014933312537
Epoch 1426
Epoch 1426 :: Batch 0/1
Batch Loss = 0.003780768001097919
1426, epoch_train_loss=0.003780768001097919
Epoch 1427
Epoch 1427 :: Batch 0/1
Batch Loss = 0.004663005394027628
1427, epoch_train_loss=0.004663005394027628
Epoch 1428
Epoch 1428 :: Batch 0/1
Batch Loss = 0.005953483706560116
1428, epoch_train_loss=0.005953483706560116
Epoch 1429
Epoch 1429 :: Batch 0/1
Batch Loss = 0.007564151091457207
1429, epoch_train_loss=0.007564151091457207
Epoch 1430
Epoch 1430 :: Batch 0/1
Batch Loss = 0.009177238053988603
1430, epoch_train_loss=0.009177238053988603
Epoch 1431
Epoch 1431 :: Batch 0/1
Batch Loss = 0.011015852999576557
1431, epoch_train_loss=0.011015852999576557
Epoch 1432
Epoch 1432 :: Batch 0/1
Batch Loss = 0.010989143846396071
1432, epoch_train_loss=0.010989143846396071
Epoch 1433
Epoch 1433 :: Batch 0/1
Batch Loss = 0.01261443596439892
1433, epoch_train_loss=0.01261443596439892
Epoch 1434
Epoch 1434 :: Batch 0/1
Batch Loss = 0.009813202375547234
1434, epoch_train_loss=0.009813202375547234
Epoch 1435
Epoch 1435 :: Batch 0/1
Batch Loss = 0.008106951010215272
1435, epoch_train_loss=0.008106951010215272
Epoch 1436
Epoch 1436 :: Batch 0/1
Batch Loss = 0.004339303032756505
1436, epoch_train_loss=0.004339303032756505
Epoch 1437
Epoch 1437 :: Batch 0/1
Batch Loss = 0.0025471140549057562
1437, epoch_train_loss=0.0025471140549057562
Epoch 1438
Epoch 1438 :: Batch 0/1
Batch Loss = 0.0034499702557296416
1438, epoch_train_loss=0.0034499702557296416
Epoch 1439
Epoch 1439 :: Batch 0/1
Batch Loss = 0.0056142003633163655
1439, epoch_train_loss=0.0056142003633163655
Epoch 1440
Epoch 1440 :: Batch 0/1
Batch Loss = 0.007371122560916687
1440, epoch_train_loss=0.007371122560916687
Epoch 1441
Epoch 1441 :: Batch 0/1
Batch Loss = 0.005625337300532054
1441, epoch_train_loss=0.005625337300532054
Epoch 1442
Epoch 1442 :: Batch 0/1
Batch Loss = 0.00352944383566176
1442, epoch_train_loss=0.00352944383566176
Epoch 1443
Epoch 1443 :: Batch 0/1
Batch Loss = 0.002483154449730982
1443, epoch_train_loss=0.002483154449730982
Epoch 1444
Epoch 1444 :: Batch 0/1
Batch Loss = 0.003297794782616672
1444, epoch_train_loss=0.003297794782616672
Epoch 1445
Epoch 1445 :: Batch 0/1
Batch Loss = 0.004495625362586987
1445, epoch_train_loss=0.004495625362586987
Epoch 1446
Epoch 1446 :: Batch 0/1
Batch Loss = 0.004208465409698875
1446, epoch_train_loss=0.004208465409698875
Epoch 1447
Epoch 1447 :: Batch 0/1
Batch Loss = 0.0033601427350774017
1447, epoch_train_loss=0.0033601427350774017
Epoch 1448
Epoch 1448 :: Batch 0/1
Batch Loss = 0.0026107249288431543
1448, epoch_train_loss=0.0026107249288431543
Epoch 1449
Epoch 1449 :: Batch 0/1
Batch Loss = 0.0026645868790653687
1449, epoch_train_loss=0.0026645868790653687
Epoch 1450
Epoch 1450 :: Batch 0/1
Batch Loss = 0.0031688594021990115
1450, epoch_train_loss=0.0031688594021990115
Epoch 1451
Epoch 1451 :: Batch 0/1
Batch Loss = 0.0033059049603945344
1451, epoch_train_loss=0.0033059049603945344
Epoch 1452
Epoch 1452 :: Batch 0/1
Batch Loss = 0.0030555962128897113
1452, epoch_train_loss=0.0030555962128897113
Epoch 1453
Epoch 1453 :: Batch 0/1
Batch Loss = 0.0026035244251474505
1453, epoch_train_loss=0.0026035244251474505
Epoch 1454
Epoch 1454 :: Batch 0/1
Batch Loss = 0.0024618038856093712
1454, epoch_train_loss=0.0024618038856093712
Epoch 1455
Epoch 1455 :: Batch 0/1
Batch Loss = 0.002673821880088273
1455, epoch_train_loss=0.002673821880088273
Epoch 1456
Epoch 1456 :: Batch 0/1
Batch Loss = 0.002888257984551883
1456, epoch_train_loss=0.002888257984551883
Epoch 1457
Epoch 1457 :: Batch 0/1
Batch Loss = 0.002877885025274568
1457, epoch_train_loss=0.002877885025274568
Epoch 1458
Epoch 1458 :: Batch 0/1
Batch Loss = 0.0025702030251500163
1458, epoch_train_loss=0.0025702030251500163
Epoch 1459
Epoch 1459 :: Batch 0/1
Batch Loss = 0.002320200880907924
1459, epoch_train_loss=0.002320200880907924
Epoch 1460
Epoch 1460 :: Batch 0/1
Batch Loss = 0.0023736912818142913
1460, epoch_train_loss=0.0023736912818142913
Epoch 1461
Epoch 1461 :: Batch 0/1
Batch Loss = 0.0025946139225985153
1461, epoch_train_loss=0.0025946139225985153
Epoch 1462
Epoch 1462 :: Batch 0/1
Batch Loss = 0.0026775148446229313
1462, epoch_train_loss=0.0026775148446229313
Epoch 1463
Epoch 1463 :: Batch 0/1
Batch Loss = 0.0024948820286798965
1463, epoch_train_loss=0.0024948820286798965
Epoch 1464
Epoch 1464 :: Batch 0/1
Batch Loss = 0.002287770195470413
1464, epoch_train_loss=0.002287770195470413
Epoch 1465
Epoch 1465 :: Batch 0/1
Batch Loss = 0.0022750624987194295
1465, epoch_train_loss=0.0022750624987194295
Epoch 1466
Epoch 1466 :: Batch 0/1
Batch Loss = 0.0024140195468460414
1466, epoch_train_loss=0.0024140195468460414
Epoch 1467
Epoch 1467 :: Batch 0/1
Batch Loss = 0.0025018578140627323
1467, epoch_train_loss=0.0025018578140627323
Epoch 1468
Epoch 1468 :: Batch 0/1
Batch Loss = 0.0024351290563468385
1468, epoch_train_loss=0.0024351290563468385
Epoch 1469
Epoch 1469 :: Batch 0/1
Batch Loss = 0.002319656082866658
1469, epoch_train_loss=0.002319656082866658
Epoch 1470
Epoch 1470 :: Batch 0/1
Batch Loss = 0.0022665513657582547
1470, epoch_train_loss=0.0022665513657582547
Epoch 1471
Epoch 1471 :: Batch 0/1
Batch Loss = 0.0022945891284853522
1471, epoch_train_loss=0.0022945891284853522
Epoch 1472
Epoch 1472 :: Batch 0/1
Batch Loss = 0.0023410603289068296
1472, epoch_train_loss=0.0023410603289068296
Epoch 1473
Epoch 1473 :: Batch 0/1
Batch Loss = 0.0023564631139760778
1473, epoch_train_loss=0.0023564631139760778
Epoch 1474
Epoch 1474 :: Batch 0/1
Batch Loss = 0.0023409919385905744
1474, epoch_train_loss=0.0023409919385905744
Epoch 1475
Epoch 1475 :: Batch 0/1
Batch Loss = 0.0023004228864550013
1475, epoch_train_loss=0.0023004228864550013
Epoch 1476
Epoch 1476 :: Batch 0/1
Batch Loss = 0.002261496784873332
1476, epoch_train_loss=0.002261496784873332
Epoch 1477
Epoch 1477 :: Batch 0/1
Batch Loss = 0.002249175120409301
1477, epoch_train_loss=0.002249175120409301
Epoch 1478
Epoch 1478 :: Batch 0/1
Batch Loss = 0.002272423418688985
1478, epoch_train_loss=0.002272423418688985
Epoch 1479
Epoch 1479 :: Batch 0/1
Batch Loss = 0.002304443882933887
1479, epoch_train_loss=0.002304443882933887
Epoch 1480
Epoch 1480 :: Batch 0/1
Batch Loss = 0.0023049586121009122
1480, epoch_train_loss=0.0023049586121009122
Epoch 1481
Epoch 1481 :: Batch 0/1
Batch Loss = 0.0022739030204288594
1481, epoch_train_loss=0.0022739030204288594
Epoch 1482
Epoch 1482 :: Batch 0/1
Batch Loss = 0.002241183881999593
1482, epoch_train_loss=0.002241183881999593
Epoch 1483
Epoch 1483 :: Batch 0/1
Batch Loss = 0.002236141637637674
1483, epoch_train_loss=0.002236141637637674
Epoch 1484
Epoch 1484 :: Batch 0/1
Batch Loss = 0.002253591246371708
1484, epoch_train_loss=0.002253591246371708
Epoch 1485
Epoch 1485 :: Batch 0/1
Batch Loss = 0.0022688448593402694
1485, epoch_train_loss=0.0022688448593402694
Epoch 1486
Epoch 1486 :: Batch 0/1
Batch Loss = 0.002268746541674494
1486, epoch_train_loss=0.002268746541674494
Epoch 1487
Epoch 1487 :: Batch 0/1
Batch Loss = 0.002256683925369063
1487, epoch_train_loss=0.002256683925369063
Epoch 1488
Epoch 1488 :: Batch 0/1
Batch Loss = 0.0022446698521354745
1488, epoch_train_loss=0.0022446698521354745
Epoch 1489
Epoch 1489 :: Batch 0/1
Batch Loss = 0.002237720207066552
1489, epoch_train_loss=0.002237720207066552
Epoch 1490
Epoch 1490 :: Batch 0/1
Batch Loss = 0.0022368010793441633
1490, epoch_train_loss=0.0022368010793441633
Epoch 1491
Epoch 1491 :: Batch 0/1
Batch Loss = 0.0022412441063865717
1491, epoch_train_loss=0.0022412441063865717
Epoch 1492
Epoch 1492 :: Batch 0/1
Batch Loss = 0.002247485790057083
1492, epoch_train_loss=0.002247485790057083
Epoch 1493
Epoch 1493 :: Batch 0/1
Batch Loss = 0.0022499339967735335
1493, epoch_train_loss=0.0022499339967735335
Epoch 1494
Epoch 1494 :: Batch 0/1
Batch Loss = 0.002244806272984596
1494, epoch_train_loss=0.002244806272984596
Epoch 1495
Epoch 1495 :: Batch 0/1
Batch Loss = 0.002235900722188538
1495, epoch_train_loss=0.002235900722188538
Epoch 1496
Epoch 1496 :: Batch 0/1
Batch Loss = 0.0022303255067595337
1496, epoch_train_loss=0.0022303255067595337
Epoch 1497
Epoch 1497 :: Batch 0/1
Batch Loss = 0.002231557843451964
1497, epoch_train_loss=0.002231557843451964
Epoch 1498
Epoch 1498 :: Batch 0/1
Batch Loss = 0.002236429601714551
1498, epoch_train_loss=0.002236429601714551
Epoch 1499
Epoch 1499 :: Batch 0/1
Batch Loss = 0.0022394750919625846
1499, epoch_train_loss=0.0022394750919625846
Epoch 1500
Epoch 1500 :: Batch 0/1
Batch Loss = 0.00223849863821858
1500, epoch_train_loss=0.00223849863821858
Epoch 1501
Epoch 1501 :: Batch 0/1
Batch Loss = 0.0022350763218204955
1501, epoch_train_loss=0.0022350763218204955
Epoch 1502
Epoch 1502 :: Batch 0/1
Batch Loss = 0.0022320006852830556
1502, epoch_train_loss=0.0022320006852830556
Epoch 1503
Epoch 1503 :: Batch 0/1
Batch Loss = 0.002230375255882552
1503, epoch_train_loss=0.002230375255882552
Epoch 1504
Epoch 1504 :: Batch 0/1
Batch Loss = 0.0022300949450978435
1504, epoch_train_loss=0.0022300949450978435
Epoch 1505
Epoch 1505 :: Batch 0/1
Batch Loss = 0.0022307660140850247
1505, epoch_train_loss=0.0022307660140850247
Epoch 1506
Epoch 1506 :: Batch 0/1
Batch Loss = 0.002231925883848027
1506, epoch_train_loss=0.002231925883848027
Epoch 1507
Epoch 1507 :: Batch 0/1
Batch Loss = 0.002232784356213804
1507, epoch_train_loss=0.002232784356213804
Epoch 1508
Epoch 1508 :: Batch 0/1
Batch Loss = 0.0022324134041503928
1508, epoch_train_loss=0.0022324134041503928
Epoch 1509
Epoch 1509 :: Batch 0/1
Batch Loss = 0.00223079764459964
1509, epoch_train_loss=0.00223079764459964
Epoch 1510
Epoch 1510 :: Batch 0/1
Batch Loss = 0.0022288587670622048
1510, epoch_train_loss=0.0022288587670622048
Epoch 1511
Epoch 1511 :: Batch 0/1
Batch Loss = 0.002227795597178743
1511, epoch_train_loss=0.002227795597178743
Epoch 1512
Epoch 1512 :: Batch 0/1
Batch Loss = 0.002227990752438706
1512, epoch_train_loss=0.002227990752438706
Epoch 1513
Epoch 1513 :: Batch 0/1
Batch Loss = 0.0022288455019687558
1513, epoch_train_loss=0.0022288455019687558
Epoch 1514
Epoch 1514 :: Batch 0/1
Batch Loss = 0.002229463860321067
1514, epoch_train_loss=0.002229463860321067
Epoch 1515
Epoch 1515 :: Batch 0/1
Batch Loss = 0.0022293862713814987
1515, epoch_train_loss=0.0022293862713814987
Epoch 1516
Epoch 1516 :: Batch 0/1
Batch Loss = 0.0022287904582445773
1516, epoch_train_loss=0.0022287904582445773
Epoch 1517
Epoch 1517 :: Batch 0/1
Batch Loss = 0.0022280454137482038
1517, epoch_train_loss=0.0022280454137482038
Epoch 1518
Epoch 1518 :: Batch 0/1
Batch Loss = 0.0022274223841537563
1518, epoch_train_loss=0.0022274223841537563
Epoch 1519
Epoch 1519 :: Batch 0/1
Batch Loss = 0.0022269840652807656
1519, epoch_train_loss=0.0022269840652807656
Epoch 1520
Epoch 1520 :: Batch 0/1
Batch Loss = 0.002226744688253486
1520, epoch_train_loss=0.002226744688253486
Epoch 1521
Epoch 1521 :: Batch 0/1
Batch Loss = 0.002226724872422204
1521, epoch_train_loss=0.002226724872422204
Epoch 1522
Epoch 1522 :: Batch 0/1
Batch Loss = 0.0022268749326323964
1522, epoch_train_loss=0.0022268749326323964
Epoch 1523
Epoch 1523 :: Batch 0/1
Batch Loss = 0.002227032474051411
1523, epoch_train_loss=0.002227032474051411
Epoch 1524
Epoch 1524 :: Batch 0/1
Batch Loss = 0.002226994914682679
1524, epoch_train_loss=0.002226994914682679
Epoch 1525
Epoch 1525 :: Batch 0/1
Batch Loss = 0.0022266897805864453
1525, epoch_train_loss=0.0022266897805864453
Epoch 1526
Epoch 1526 :: Batch 0/1
Batch Loss = 0.002226220395506345
1526, epoch_train_loss=0.002226220395506345
Epoch 1527
Epoch 1527 :: Batch 0/1
Batch Loss = 0.0022257865562568912
1527, epoch_train_loss=0.0022257865562568912
Epoch 1528
Epoch 1528 :: Batch 0/1
Batch Loss = 0.0022255186247250127
1528, epoch_train_loss=0.0022255186247250127
Epoch 1529
Epoch 1529 :: Batch 0/1
Batch Loss = 0.0022254133524697477
1529, epoch_train_loss=0.0022254133524697477
Epoch 1530
Epoch 1530 :: Batch 0/1
Batch Loss = 0.002225380871106784
1530, epoch_train_loss=0.002225380871106784
Epoch 1531
Epoch 1531 :: Batch 0/1
Batch Loss = 0.002225342900208887
1531, epoch_train_loss=0.002225342900208887
Epoch 1532
Epoch 1532 :: Batch 0/1
Batch Loss = 0.0022252759035601096
1532, epoch_train_loss=0.0022252759035601096
Epoch 1533
Epoch 1533 :: Batch 0/1
Batch Loss = 0.002225181891301062
1533, epoch_train_loss=0.002225181891301062
Epoch 1534
Epoch 1534 :: Batch 0/1
Batch Loss = 0.0022250564244529927
1534, epoch_train_loss=0.0022250564244529927
Epoch 1535
Epoch 1535 :: Batch 0/1
Batch Loss = 0.002224881024033698
1535, epoch_train_loss=0.002224881024033698
Epoch 1536
Epoch 1536 :: Batch 0/1
Batch Loss = 0.0022246588172648075
1536, epoch_train_loss=0.0022246588172648075
Epoch 1537
Epoch 1537 :: Batch 0/1
Batch Loss = 0.0022244219592712318
1537, epoch_train_loss=0.0022244219592712318
Epoch 1538
Epoch 1538 :: Batch 0/1
Batch Loss = 0.002224216509459172
1538, epoch_train_loss=0.002224216509459172
Epoch 1539
Epoch 1539 :: Batch 0/1
Batch Loss = 0.0022240676971762153
1539, epoch_train_loss=0.0022240676971762153
Epoch 1540
Epoch 1540 :: Batch 0/1
Batch Loss = 0.0022239676311500427
1540, epoch_train_loss=0.0022239676311500427
Epoch 1541
Epoch 1541 :: Batch 0/1
Batch Loss = 0.0022238873054626546
1541, epoch_train_loss=0.0022238873054626546
Epoch 1542
Epoch 1542 :: Batch 0/1
Batch Loss = 0.0022237998871326727
1542, epoch_train_loss=0.0022237998871326727
Epoch 1543
Epoch 1543 :: Batch 0/1
Batch Loss = 0.002223695040018841
1543, epoch_train_loss=0.002223695040018841
Epoch 1544
Epoch 1544 :: Batch 0/1
Batch Loss = 0.0022235751407549487
1544, epoch_train_loss=0.0022235751407549487
Epoch 1545
Epoch 1545 :: Batch 0/1
Batch Loss = 0.0022234472835317644
1545, epoch_train_loss=0.0022234472835317644
Epoch 1546
Epoch 1546 :: Batch 0/1
Batch Loss = 0.002223312664245646
1546, epoch_train_loss=0.002223312664245646
Epoch 1547
Epoch 1547 :: Batch 0/1
Batch Loss = 0.0022231697893936196
1547, epoch_train_loss=0.0022231697893936196
Epoch 1548
Epoch 1548 :: Batch 0/1
Batch Loss = 0.002223018167495711
1548, epoch_train_loss=0.002223018167495711
Epoch 1549
Epoch 1549 :: Batch 0/1
Batch Loss = 0.0022228639327576687
1549, epoch_train_loss=0.0022228639327576687
Epoch 1550
Epoch 1550 :: Batch 0/1
Batch Loss = 0.002222716420145964
1550, epoch_train_loss=0.002222716420145964
Epoch 1551
Epoch 1551 :: Batch 0/1
Batch Loss = 0.002222582420173259
1551, epoch_train_loss=0.002222582420173259
Epoch 1552
Epoch 1552 :: Batch 0/1
Batch Loss = 0.0022224614207260636
1552, epoch_train_loss=0.0022224614207260636
Epoch 1553
Epoch 1553 :: Batch 0/1
Batch Loss = 0.002222347674803591
1553, epoch_train_loss=0.002222347674803591
Epoch 1554
Epoch 1554 :: Batch 0/1
Batch Loss = 0.0022222351085056822
1554, epoch_train_loss=0.0022222351085056822
Epoch 1555
Epoch 1555 :: Batch 0/1
Batch Loss = 0.002222121023332488
1555, epoch_train_loss=0.002222121023332488
Epoch 1556
Epoch 1556 :: Batch 0/1
Batch Loss = 0.0022220060862987124
1556, epoch_train_loss=0.0022220060862987124
Epoch 1557
Epoch 1557 :: Batch 0/1
Batch Loss = 0.002221891355459311
1557, epoch_train_loss=0.002221891355459311
Epoch 1558
Epoch 1558 :: Batch 0/1
Batch Loss = 0.002221777039664916
1558, epoch_train_loss=0.002221777039664916
Epoch 1559
Epoch 1559 :: Batch 0/1
Batch Loss = 0.002221661555527714
1559, epoch_train_loss=0.002221661555527714
Epoch 1560
Epoch 1560 :: Batch 0/1
Batch Loss = 0.002221543561478367
1560, epoch_train_loss=0.002221543561478367
Epoch 1561
Epoch 1561 :: Batch 0/1
Batch Loss = 0.0022214221929413446
1561, epoch_train_loss=0.0022214221929413446
Epoch 1562
Epoch 1562 :: Batch 0/1
Batch Loss = 0.002221298545263264
1562, epoch_train_loss=0.002221298545263264
Epoch 1563
Epoch 1563 :: Batch 0/1
Batch Loss = 0.0022211741650490944
1563, epoch_train_loss=0.0022211741650490944
Epoch 1564
Epoch 1564 :: Batch 0/1
Batch Loss = 0.0022210508896811686
1564, epoch_train_loss=0.0022210508896811686
Epoch 1565
Epoch 1565 :: Batch 0/1
Batch Loss = 0.0022209290103178026
1565, epoch_train_loss=0.0022209290103178026
Epoch 1566
Epoch 1566 :: Batch 0/1
Batch Loss = 0.0022208081639864146
1566, epoch_train_loss=0.0022208081639864146
Epoch 1567
Epoch 1567 :: Batch 0/1
Batch Loss = 0.0022206874024545626
1567, epoch_train_loss=0.0022206874024545626
Epoch 1568
Epoch 1568 :: Batch 0/1
Batch Loss = 0.0022205665853720497
1568, epoch_train_loss=0.0022205665853720497
Epoch 1569
Epoch 1569 :: Batch 0/1
Batch Loss = 0.0022204459523810557
1569, epoch_train_loss=0.0022204459523810557
Epoch 1570
Epoch 1570 :: Batch 0/1
Batch Loss = 0.002220326166070455
1570, epoch_train_loss=0.002220326166070455
Epoch 1571
Epoch 1571 :: Batch 0/1
Batch Loss = 0.002220207467299454
1571, epoch_train_loss=0.002220207467299454
Epoch 1572
Epoch 1572 :: Batch 0/1
Batch Loss = 0.0022200899251583494
1572, epoch_train_loss=0.0022200899251583494
Epoch 1573
Epoch 1573 :: Batch 0/1
Batch Loss = 0.0022199731339379004
1573, epoch_train_loss=0.0022199731339379004
Epoch 1574
Epoch 1574 :: Batch 0/1
Batch Loss = 0.002219856793258941
1574, epoch_train_loss=0.002219856793258941
Epoch 1575
Epoch 1575 :: Batch 0/1
Batch Loss = 0.0022197405593546056
1575, epoch_train_loss=0.0022197405593546056
Epoch 1576
Epoch 1576 :: Batch 0/1
Batch Loss = 0.002219624535365343
1576, epoch_train_loss=0.002219624535365343
Epoch 1577
Epoch 1577 :: Batch 0/1
Batch Loss = 0.002219508889349385
1577, epoch_train_loss=0.002219508889349385
Epoch 1578
Epoch 1578 :: Batch 0/1
Batch Loss = 0.0022193939796785337
1578, epoch_train_loss=0.0022193939796785337
Epoch 1579
Epoch 1579 :: Batch 0/1
Batch Loss = 0.0022192798685448127
1579, epoch_train_loss=0.0022192798685448127
Epoch 1580
Epoch 1580 :: Batch 0/1
Batch Loss = 0.002219166648622027
1580, epoch_train_loss=0.002219166648622027
Epoch 1581
Epoch 1581 :: Batch 0/1
Batch Loss = 0.002219054225095045
1581, epoch_train_loss=0.002219054225095045
Epoch 1582
Epoch 1582 :: Batch 0/1
Batch Loss = 0.00221894280932904
1582, epoch_train_loss=0.00221894280932904
Epoch 1583
Epoch 1583 :: Batch 0/1
Batch Loss = 0.0022188325888032687
1583, epoch_train_loss=0.0022188325888032687
Epoch 1584
Epoch 1584 :: Batch 0/1
Batch Loss = 0.0022187242228788404
1584, epoch_train_loss=0.0022187242228788404
Epoch 1585
Epoch 1585 :: Batch 0/1
Batch Loss = 0.002218618387464668
1585, epoch_train_loss=0.002218618387464668
Epoch 1586
Epoch 1586 :: Batch 0/1
Batch Loss = 0.0022185164651329897
1586, epoch_train_loss=0.0022185164651329897
Epoch 1587
Epoch 1587 :: Batch 0/1
Batch Loss = 0.0022184199444513363
1587, epoch_train_loss=0.0022184199444513363
Epoch 1588
Epoch 1588 :: Batch 0/1
Batch Loss = 0.002218331650030609
1588, epoch_train_loss=0.002218331650030609
Epoch 1589
Epoch 1589 :: Batch 0/1
Batch Loss = 0.0022182550534531934
1589, epoch_train_loss=0.0022182550534531934
Epoch 1590
Epoch 1590 :: Batch 0/1
Batch Loss = 0.002218196734419013
1590, epoch_train_loss=0.002218196734419013
Epoch 1591
Epoch 1591 :: Batch 0/1
Batch Loss = 0.0022181654213408626
1591, epoch_train_loss=0.0022181654213408626
Epoch 1592
Epoch 1592 :: Batch 0/1
Batch Loss = 0.0022181773041295805
1592, epoch_train_loss=0.0022181773041295805
Epoch 1593
Epoch 1593 :: Batch 0/1
Batch Loss = 0.0022182544633179914
1593, epoch_train_loss=0.0022182544633179914
Epoch 1594
Epoch 1594 :: Batch 0/1
Batch Loss = 0.0022184382669913484
1594, epoch_train_loss=0.0022184382669913484
Epoch 1595
Epoch 1595 :: Batch 0/1
Batch Loss = 0.0022187855680212945
1595, epoch_train_loss=0.0022187855680212945
Epoch 1596
Epoch 1596 :: Batch 0/1
Batch Loss = 0.0022194068009168794
1596, epoch_train_loss=0.0022194068009168794
Epoch 1597
Epoch 1597 :: Batch 0/1
Batch Loss = 0.002220451375362022
1597, epoch_train_loss=0.002220451375362022
Epoch 1598
Epoch 1598 :: Batch 0/1
Batch Loss = 0.0022222279227844715
1598, epoch_train_loss=0.0022222279227844715
Epoch 1599
Epoch 1599 :: Batch 0/1
Batch Loss = 0.002225135396236867
1599, epoch_train_loss=0.002225135396236867
Epoch 1600
Epoch 1600 :: Batch 0/1
Batch Loss = 0.002230078408933856
1600, epoch_train_loss=0.002230078408933856
Epoch 1601
Epoch 1601 :: Batch 0/1
Batch Loss = 0.0022381256827586726
1601, epoch_train_loss=0.0022381256827586726
Epoch 1602
Epoch 1602 :: Batch 0/1
Batch Loss = 0.0022520669411187485
1602, epoch_train_loss=0.0022520669411187485
Epoch 1603
Epoch 1603 :: Batch 0/1
Batch Loss = 0.0022746972291577804
1603, epoch_train_loss=0.0022746972291577804
Epoch 1604
Epoch 1604 :: Batch 0/1
Batch Loss = 0.0023151077344569014
1604, epoch_train_loss=0.0023151077344569014
Epoch 1605
Epoch 1605 :: Batch 0/1
Batch Loss = 0.002379940743340927
1605, epoch_train_loss=0.002379940743340927
Epoch 1606
Epoch 1606 :: Batch 0/1
Batch Loss = 0.002500582307402369
1606, epoch_train_loss=0.002500582307402369
Epoch 1607
Epoch 1607 :: Batch 0/1
Batch Loss = 0.00268764602755629
1607, epoch_train_loss=0.00268764602755629
Epoch 1608
Epoch 1608 :: Batch 0/1
Batch Loss = 0.0030551175238811984
1608, epoch_train_loss=0.0030551175238811984
Epoch 1609
Epoch 1609 :: Batch 0/1
Batch Loss = 0.0035786770151872076
1609, epoch_train_loss=0.0035786770151872076
Epoch 1610
Epoch 1610 :: Batch 0/1
Batch Loss = 0.004679014070628262
1610, epoch_train_loss=0.004679014070628262
Epoch 1611
Epoch 1611 :: Batch 0/1
Batch Loss = 0.005946923316744656
1611, epoch_train_loss=0.005946923316744656
Epoch 1612
Epoch 1612 :: Batch 0/1
Batch Loss = 0.00881588860990417
1612, epoch_train_loss=0.00881588860990417
Epoch 1613
Epoch 1613 :: Batch 0/1
Batch Loss = 0.01053950730869568
1613, epoch_train_loss=0.01053950730869568
Epoch 1614
Epoch 1614 :: Batch 0/1
Batch Loss = 0.01492757057733744
1614, epoch_train_loss=0.01492757057733744
Epoch 1615
Epoch 1615 :: Batch 0/1
Batch Loss = 0.013031782052579514
1615, epoch_train_loss=0.013031782052579514
Epoch 1616
Epoch 1616 :: Batch 0/1
Batch Loss = 0.012370847013192824
1616, epoch_train_loss=0.012370847013192824
Epoch 1617
Epoch 1617 :: Batch 0/1
Batch Loss = 0.006856811071428571
1617, epoch_train_loss=0.006856811071428571
Epoch 1618
Epoch 1618 :: Batch 0/1
Batch Loss = 0.0036865906611294828
1618, epoch_train_loss=0.0036865906611294828
Epoch 1619
Epoch 1619 :: Batch 0/1
Batch Loss = 0.003510836377624978
1619, epoch_train_loss=0.003510836377624978
Epoch 1620
Epoch 1620 :: Batch 0/1
Batch Loss = 0.005278661602468898
1620, epoch_train_loss=0.005278661602468898
Epoch 1621
Epoch 1621 :: Batch 0/1
Batch Loss = 0.007463352768295785
1621, epoch_train_loss=0.007463352768295785
Epoch 1622
Epoch 1622 :: Batch 0/1
Batch Loss = 0.0065247465414713
1622, epoch_train_loss=0.0065247465414713
Epoch 1623
Epoch 1623 :: Batch 0/1
Batch Loss = 0.004891582110226128
1623, epoch_train_loss=0.004891582110226128
Epoch 1624
Epoch 1624 :: Batch 0/1
Batch Loss = 0.0032259074158347083
1624, epoch_train_loss=0.0032259074158347083
Epoch 1625
Epoch 1625 :: Batch 0/1
Batch Loss = 0.003476984597783881
1625, epoch_train_loss=0.003476984597783881
Epoch 1626
Epoch 1626 :: Batch 0/1
Batch Loss = 0.004702037550721557
1626, epoch_train_loss=0.004702037550721557
Epoch 1627
Epoch 1627 :: Batch 0/1
Batch Loss = 0.004581736680719568
1627, epoch_train_loss=0.004581736680719568
Epoch 1628
Epoch 1628 :: Batch 0/1
Batch Loss = 0.0037345050033165536
1628, epoch_train_loss=0.0037345050033165536
Epoch 1629
Epoch 1629 :: Batch 0/1
Batch Loss = 0.0028160625948621676
1629, epoch_train_loss=0.0028160625948621676
Epoch 1630
Epoch 1630 :: Batch 0/1
Batch Loss = 0.0029110042007615836
1630, epoch_train_loss=0.0029110042007615836
Epoch 1631
Epoch 1631 :: Batch 0/1
Batch Loss = 0.0035679757581104457
1631, epoch_train_loss=0.0035679757581104457
Epoch 1632
Epoch 1632 :: Batch 0/1
Batch Loss = 0.003545849315193825
1632, epoch_train_loss=0.003545849315193825
Epoch 1633
Epoch 1633 :: Batch 0/1
Batch Loss = 0.002954257025910148
1633, epoch_train_loss=0.002954257025910148
Epoch 1634
Epoch 1634 :: Batch 0/1
Batch Loss = 0.002465656856917017
1634, epoch_train_loss=0.002465656856917017
Epoch 1635
Epoch 1635 :: Batch 0/1
Batch Loss = 0.0027176682477332336
1635, epoch_train_loss=0.0027176682477332336
Epoch 1636
Epoch 1636 :: Batch 0/1
Batch Loss = 0.003208462386359345
1636, epoch_train_loss=0.003208462386359345
Epoch 1637
Epoch 1637 :: Batch 0/1
Batch Loss = 0.003125350262314336
1637, epoch_train_loss=0.003125350262314336
Epoch 1638
Epoch 1638 :: Batch 0/1
Batch Loss = 0.0026589120360844035
1638, epoch_train_loss=0.0026589120360844035
Epoch 1639
Epoch 1639 :: Batch 0/1
Batch Loss = 0.0023365423833098388
1639, epoch_train_loss=0.0023365423833098388
Epoch 1640
Epoch 1640 :: Batch 0/1
Batch Loss = 0.0025068191524460232
1640, epoch_train_loss=0.0025068191524460232
Epoch 1641
Epoch 1641 :: Batch 0/1
Batch Loss = 0.0028178420241244495
1641, epoch_train_loss=0.0028178420241244495
Epoch 1642
Epoch 1642 :: Batch 0/1
Batch Loss = 0.002761377191990281
1642, epoch_train_loss=0.002761377191990281
Epoch 1643
Epoch 1643 :: Batch 0/1
Batch Loss = 0.0024535399745719494
1643, epoch_train_loss=0.0024535399745719494
Epoch 1644
Epoch 1644 :: Batch 0/1
Batch Loss = 0.0022735360509736928
1644, epoch_train_loss=0.0022735360509736928
Epoch 1645
Epoch 1645 :: Batch 0/1
Batch Loss = 0.0023990030421482305
1645, epoch_train_loss=0.0023990030421482305
Epoch 1646
Epoch 1646 :: Batch 0/1
Batch Loss = 0.0025925126475162966
1646, epoch_train_loss=0.0025925126475162966
Epoch 1647
Epoch 1647 :: Batch 0/1
Batch Loss = 0.0025666822421884037
1647, epoch_train_loss=0.0025666822421884037
Epoch 1648
Epoch 1648 :: Batch 0/1
Batch Loss = 0.0023823119728012252
1648, epoch_train_loss=0.0023823119728012252
Epoch 1649
Epoch 1649 :: Batch 0/1
Batch Loss = 0.0022568386450403473
1649, epoch_train_loss=0.0022568386450403473
Epoch 1650
Epoch 1650 :: Batch 0/1
Batch Loss = 0.0023132837183824483
1650, epoch_train_loss=0.0023132837183824483
Epoch 1651
Epoch 1651 :: Batch 0/1
Batch Loss = 0.002430445339672423
1651, epoch_train_loss=0.002430445339672423
Epoch 1652
Epoch 1652 :: Batch 0/1
Batch Loss = 0.0024377275980605015
1652, epoch_train_loss=0.0024377275980605015
Epoch 1653
Epoch 1653 :: Batch 0/1
Batch Loss = 0.0023389211292627107
1653, epoch_train_loss=0.0023389211292627107
Epoch 1654
Epoch 1654 :: Batch 0/1
Batch Loss = 0.0022498188220520412
1654, epoch_train_loss=0.0022498188220520412
Epoch 1655
Epoch 1655 :: Batch 0/1
Batch Loss = 0.0022601887404197388
1655, epoch_train_loss=0.0022601887404197388
Epoch 1656
Epoch 1656 :: Batch 0/1
Batch Loss = 0.002325625733539039
1656, epoch_train_loss=0.002325625733539039
Epoch 1657
Epoch 1657 :: Batch 0/1
Batch Loss = 0.0023514582462489005
1657, epoch_train_loss=0.0023514582462489005
Epoch 1658
Epoch 1658 :: Batch 0/1
Batch Loss = 0.0023119296506127138
1658, epoch_train_loss=0.0023119296506127138
Epoch 1659
Epoch 1659 :: Batch 0/1
Batch Loss = 0.002253336108335929
1659, epoch_train_loss=0.002253336108335929
Epoch 1660
Epoch 1660 :: Batch 0/1
Batch Loss = 0.002239283708037304
1660, epoch_train_loss=0.002239283708037304
Epoch 1661
Epoch 1661 :: Batch 0/1
Batch Loss = 0.002267874734618205
1661, epoch_train_loss=0.002267874734618205
Epoch 1662
Epoch 1662 :: Batch 0/1
Batch Loss = 0.0022926001027076624
1662, epoch_train_loss=0.0022926001027076624
Epoch 1663
Epoch 1663 :: Batch 0/1
Batch Loss = 0.002284270291076511
1663, epoch_train_loss=0.002284270291076511
Epoch 1664
Epoch 1664 :: Batch 0/1
Batch Loss = 0.002253409336811337
1664, epoch_train_loss=0.002253409336811337
Epoch 1665
Epoch 1665 :: Batch 0/1
Batch Loss = 0.002235659051780878
1665, epoch_train_loss=0.002235659051780878
Epoch 1666
Epoch 1666 :: Batch 0/1
Batch Loss = 0.002242460866771257
1666, epoch_train_loss=0.002242460866771257
Epoch 1667
Epoch 1667 :: Batch 0/1
Batch Loss = 0.002256543118182041
1667, epoch_train_loss=0.002256543118182041
Epoch 1668
Epoch 1668 :: Batch 0/1
Batch Loss = 0.0022583114036028033
1668, epoch_train_loss=0.0022583114036028033
Epoch 1669
Epoch 1669 :: Batch 0/1
Batch Loss = 0.0022445946169075055
1669, epoch_train_loss=0.0022445946169075055
Epoch 1670
Epoch 1670 :: Batch 0/1
Batch Loss = 0.002231404129446525
1670, epoch_train_loss=0.002231404129446525
Epoch 1671
Epoch 1671 :: Batch 0/1
Batch Loss = 0.002230664655072183
1671, epoch_train_loss=0.002230664655072183
Epoch 1672
Epoch 1672 :: Batch 0/1
Batch Loss = 0.0022382400375761608
1672, epoch_train_loss=0.0022382400375761608
Epoch 1673
Epoch 1673 :: Batch 0/1
Batch Loss = 0.0022425804414300966
1673, epoch_train_loss=0.0022425804414300966
Epoch 1674
Epoch 1674 :: Batch 0/1
Batch Loss = 0.002237186802103472
1674, epoch_train_loss=0.002237186802103472
Epoch 1675
Epoch 1675 :: Batch 0/1
Batch Loss = 0.0022276911134621017
1675, epoch_train_loss=0.0022276911134621017
Epoch 1676
Epoch 1676 :: Batch 0/1
Batch Loss = 0.002223424402560025
1676, epoch_train_loss=0.002223424402560025
Epoch 1677
Epoch 1677 :: Batch 0/1
Batch Loss = 0.002226607122442163
1677, epoch_train_loss=0.002226607122442163
Epoch 1678
Epoch 1678 :: Batch 0/1
Batch Loss = 0.0022318661145668494
1678, epoch_train_loss=0.0022318661145668494
Epoch 1679
Epoch 1679 :: Batch 0/1
Batch Loss = 0.0022326398545870763
1679, epoch_train_loss=0.0022326398545870763
Epoch 1680
Epoch 1680 :: Batch 0/1
Batch Loss = 0.0022277526235553384
1680, epoch_train_loss=0.0022277526235553384
Epoch 1681
Epoch 1681 :: Batch 0/1
Batch Loss = 0.002221863787397998
1681, epoch_train_loss=0.002221863787397998
Epoch 1682
Epoch 1682 :: Batch 0/1
Batch Loss = 0.0022198751397142437
1682, epoch_train_loss=0.0022198751397142437
Epoch 1683
Epoch 1683 :: Batch 0/1
Batch Loss = 0.0022222892929801056
1683, epoch_train_loss=0.0022222892929801056
Epoch 1684
Epoch 1684 :: Batch 0/1
Batch Loss = 0.0022258371948356333
1684, epoch_train_loss=0.0022258371948356333
Epoch 1685
Epoch 1685 :: Batch 0/1
Batch Loss = 0.0022266457561980414
1685, epoch_train_loss=0.0022266457561980414
Epoch 1686
Epoch 1686 :: Batch 0/1
Batch Loss = 0.002223929829579695
1686, epoch_train_loss=0.002223929829579695
Epoch 1687
Epoch 1687 :: Batch 0/1
Batch Loss = 0.0022201800313069193
1687, epoch_train_loss=0.0022201800313069193
Epoch 1688
Epoch 1688 :: Batch 0/1
Batch Loss = 0.0022181479984967707
1688, epoch_train_loss=0.0022181479984967707
Epoch 1689
Epoch 1689 :: Batch 0/1
Batch Loss = 0.002218916668678959
1689, epoch_train_loss=0.002218916668678959
Epoch 1690
Epoch 1690 :: Batch 0/1
Batch Loss = 0.0022210401513978718
1690, epoch_train_loss=0.0022210401513978718
Epoch 1691
Epoch 1691 :: Batch 0/1
Batch Loss = 0.0022222599701490022
1691, epoch_train_loss=0.0022222599701490022
Epoch 1692
Epoch 1692 :: Batch 0/1
Batch Loss = 0.002221581799959939
1692, epoch_train_loss=0.002221581799959939
Epoch 1693
Epoch 1693 :: Batch 0/1
Batch Loss = 0.0022195596568425125
1693, epoch_train_loss=0.0022195596568425125
Epoch 1694
Epoch 1694 :: Batch 0/1
Batch Loss = 0.0022177675508319567
1694, epoch_train_loss=0.0022177675508319567
Epoch 1695
Epoch 1695 :: Batch 0/1
Batch Loss = 0.0022172445392684903
1695, epoch_train_loss=0.0022172445392684903
Epoch 1696
Epoch 1696 :: Batch 0/1
Batch Loss = 0.0022178557521950262
1696, epoch_train_loss=0.0022178557521950262
Epoch 1697
Epoch 1697 :: Batch 0/1
Batch Loss = 0.002218760312422366
1697, epoch_train_loss=0.002218760312422366
Epoch 1698
Epoch 1698 :: Batch 0/1
Batch Loss = 0.002219067471710674
1698, epoch_train_loss=0.002219067471710674
Epoch 1699
Epoch 1699 :: Batch 0/1
Batch Loss = 0.002218546876673052
1699, epoch_train_loss=0.002218546876673052
Epoch 1700
Epoch 1700 :: Batch 0/1
Batch Loss = 0.0022175868213593465
1700, epoch_train_loss=0.0022175868213593465
Epoch 1701
Epoch 1701 :: Batch 0/1
Batch Loss = 0.0022167637017822595
1701, epoch_train_loss=0.0022167637017822595
Epoch 1702
Epoch 1702 :: Batch 0/1
Batch Loss = 0.0022164659566728754
1702, epoch_train_loss=0.0022164659566728754
Epoch 1703
Epoch 1703 :: Batch 0/1
Batch Loss = 0.0022166261123244967
1703, epoch_train_loss=0.0022166261123244967
Epoch 1704
Epoch 1704 :: Batch 0/1
Batch Loss = 0.0022168866490493494
1704, epoch_train_loss=0.0022168866490493494
Epoch 1705
Epoch 1705 :: Batch 0/1
Batch Loss = 0.0022169328079372722
1705, epoch_train_loss=0.0022169328079372722
Epoch 1706
Epoch 1706 :: Batch 0/1
Batch Loss = 0.0022166602973723655
1706, epoch_train_loss=0.0022166602973723655
Epoch 1707
Epoch 1707 :: Batch 0/1
Batch Loss = 0.0022162252886113935
1707, epoch_train_loss=0.0022162252886113935
Epoch 1708
Epoch 1708 :: Batch 0/1
Batch Loss = 0.0022158564245731566
1708, epoch_train_loss=0.0022158564245731566
Epoch 1709
Epoch 1709 :: Batch 0/1
Batch Loss = 0.002215661540325881
1709, epoch_train_loss=0.002215661540325881
Epoch 1710
Epoch 1710 :: Batch 0/1
Batch Loss = 0.0022156185795807674
1710, epoch_train_loss=0.0022156185795807674
Epoch 1711
Epoch 1711 :: Batch 0/1
Batch Loss = 0.0022156037634058763
1711, epoch_train_loss=0.0022156037634058763
Epoch 1712
Epoch 1712 :: Batch 0/1
Batch Loss = 0.0022155145308829457
1712, epoch_train_loss=0.0022155145308829457
Epoch 1713
Epoch 1713 :: Batch 0/1
Batch Loss = 0.002215334257048413
1713, epoch_train_loss=0.002215334257048413
Epoch 1714
Epoch 1714 :: Batch 0/1
Batch Loss = 0.0022151022961754826
1714, epoch_train_loss=0.0022151022961754826
Epoch 1715
Epoch 1715 :: Batch 0/1
Batch Loss = 0.002214898569465901
1715, epoch_train_loss=0.002214898569465901
Epoch 1716
Epoch 1716 :: Batch 0/1
Batch Loss = 0.002214766396452622
1716, epoch_train_loss=0.002214766396452622
Epoch 1717
Epoch 1717 :: Batch 0/1
Batch Loss = 0.0022146925623665985
1717, epoch_train_loss=0.0022146925623665985
Epoch 1718
Epoch 1718 :: Batch 0/1
Batch Loss = 0.0022146320920748506
1718, epoch_train_loss=0.0022146320920748506
Epoch 1719
Epoch 1719 :: Batch 0/1
Batch Loss = 0.0022145298863519443
1719, epoch_train_loss=0.0022145298863519443
Epoch 1720
Epoch 1720 :: Batch 0/1
Batch Loss = 0.002214373993173184
1720, epoch_train_loss=0.002214373993173184
Epoch 1721
Epoch 1721 :: Batch 0/1
Batch Loss = 0.002214187760521836
1721, epoch_train_loss=0.002214187760521836
Epoch 1722
Epoch 1722 :: Batch 0/1
Batch Loss = 0.002214008428728042
1722, epoch_train_loss=0.002214008428728042
Epoch 1723
Epoch 1723 :: Batch 0/1
Batch Loss = 0.002213868259843851
1723, epoch_train_loss=0.002213868259843851
Epoch 1724
Epoch 1724 :: Batch 0/1
Batch Loss = 0.0022137708343727357
1724, epoch_train_loss=0.0022137708343727357
Epoch 1725
Epoch 1725 :: Batch 0/1
Batch Loss = 0.0022136998335867447
1725, epoch_train_loss=0.0022136998335867447
Epoch 1726
Epoch 1726 :: Batch 0/1
Batch Loss = 0.002213628808477509
1726, epoch_train_loss=0.002213628808477509
Epoch 1727
Epoch 1727 :: Batch 0/1
Batch Loss = 0.0022135328938781723
1727, epoch_train_loss=0.0022135328938781723
Epoch 1728
Epoch 1728 :: Batch 0/1
Batch Loss = 0.002213405392741373
1728, epoch_train_loss=0.002213405392741373
Epoch 1729
Epoch 1729 :: Batch 0/1
Batch Loss = 0.002213255871941755
1729, epoch_train_loss=0.002213255871941755
Epoch 1730
Epoch 1730 :: Batch 0/1
Batch Loss = 0.0022131026909767058
1730, epoch_train_loss=0.0022131026909767058
Epoch 1731
Epoch 1731 :: Batch 0/1
Batch Loss = 0.002212964329576771
1731, epoch_train_loss=0.002212964329576771
Epoch 1732
Epoch 1732 :: Batch 0/1
Batch Loss = 0.002212846944783214
1732, epoch_train_loss=0.002212846944783214
Epoch 1733
Epoch 1733 :: Batch 0/1
Batch Loss = 0.0022127486286968264
1733, epoch_train_loss=0.0022127486286968264
Epoch 1734
Epoch 1734 :: Batch 0/1
Batch Loss = 0.0022126602530224446
1734, epoch_train_loss=0.0022126602530224446
Epoch 1735
Epoch 1735 :: Batch 0/1
Batch Loss = 0.00221257039434016
1735, epoch_train_loss=0.00221257039434016
Epoch 1736
Epoch 1736 :: Batch 0/1
Batch Loss = 0.002212471917853492
1736, epoch_train_loss=0.002212471917853492
Epoch 1737
Epoch 1737 :: Batch 0/1
Batch Loss = 0.002212361298615418
1737, epoch_train_loss=0.002212361298615418
Epoch 1738
Epoch 1738 :: Batch 0/1
Batch Loss = 0.0022122419742329
1738, epoch_train_loss=0.0022122419742329
Epoch 1739
Epoch 1739 :: Batch 0/1
Batch Loss = 0.002212119817262538
1739, epoch_train_loss=0.002212119817262538
Epoch 1740
Epoch 1740 :: Batch 0/1
Batch Loss = 0.002212000091562233
1740, epoch_train_loss=0.002212000091562233
Epoch 1741
Epoch 1741 :: Batch 0/1
Batch Loss = 0.0022118861991271927
1741, epoch_train_loss=0.0022118861991271927
Epoch 1742
Epoch 1742 :: Batch 0/1
Batch Loss = 0.002211778323135938
1742, epoch_train_loss=0.002211778323135938
Epoch 1743
Epoch 1743 :: Batch 0/1
Batch Loss = 0.00221167489930268
1743, epoch_train_loss=0.00221167489930268
Epoch 1744
Epoch 1744 :: Batch 0/1
Batch Loss = 0.0022115735084520926
1744, epoch_train_loss=0.0022115735084520926
Epoch 1745
Epoch 1745 :: Batch 0/1
Batch Loss = 0.002211471231754035
1745, epoch_train_loss=0.002211471231754035
Epoch 1746
Epoch 1746 :: Batch 0/1
Batch Loss = 0.002211366650995314
1746, epoch_train_loss=0.002211366650995314
Epoch 1747
Epoch 1747 :: Batch 0/1
Batch Loss = 0.0022112595680171417
1747, epoch_train_loss=0.0022112595680171417
Epoch 1748
Epoch 1748 :: Batch 0/1
Batch Loss = 0.0022111508257855396
1748, epoch_train_loss=0.0022111508257855396
Epoch 1749
Epoch 1749 :: Batch 0/1
Batch Loss = 0.0022110419705578302
1749, epoch_train_loss=0.0022110419705578302
Epoch 1750
Epoch 1750 :: Batch 0/1
Batch Loss = 0.0022109338658301138
1750, epoch_train_loss=0.0022109338658301138
Epoch 1751
Epoch 1751 :: Batch 0/1
Batch Loss = 0.0022108274588907495
1751, epoch_train_loss=0.0022108274588907495
Epoch 1752
Epoch 1752 :: Batch 0/1
Batch Loss = 0.0022107229378944297
1752, epoch_train_loss=0.0022107229378944297
Epoch 1753
Epoch 1753 :: Batch 0/1
Batch Loss = 0.0022106200188807285
1753, epoch_train_loss=0.0022106200188807285
Epoch 1754
Epoch 1754 :: Batch 0/1
Batch Loss = 0.002210518119003785
1754, epoch_train_loss=0.002210518119003785
Epoch 1755
Epoch 1755 :: Batch 0/1
Batch Loss = 0.002210416418082504
1755, epoch_train_loss=0.002210416418082504
Epoch 1756
Epoch 1756 :: Batch 0/1
Batch Loss = 0.0022103144468729402
1756, epoch_train_loss=0.0022103144468729402
Epoch 1757
Epoch 1757 :: Batch 0/1
Batch Loss = 0.0022102119405280805
1757, epoch_train_loss=0.0022102119405280805
Epoch 1758
Epoch 1758 :: Batch 0/1
Batch Loss = 0.0022101087741598618
1758, epoch_train_loss=0.0022101087741598618
Epoch 1759
Epoch 1759 :: Batch 0/1
Batch Loss = 0.0022100051095581643
1759, epoch_train_loss=0.0022100051095581643
Epoch 1760
Epoch 1760 :: Batch 0/1
Batch Loss = 0.002209901144623281
1760, epoch_train_loss=0.002209901144623281
Epoch 1761
Epoch 1761 :: Batch 0/1
Batch Loss = 0.0022097972369756547
1761, epoch_train_loss=0.0022097972369756547
Epoch 1762
Epoch 1762 :: Batch 0/1
Batch Loss = 0.0022096936678632864
1762, epoch_train_loss=0.0022096936678632864
Epoch 1763
Epoch 1763 :: Batch 0/1
Batch Loss = 0.002209590586319659
1763, epoch_train_loss=0.002209590586319659
Epoch 1764
Epoch 1764 :: Batch 0/1
Batch Loss = 0.002209488113378654
1764, epoch_train_loss=0.002209488113378654
Epoch 1765
Epoch 1765 :: Batch 0/1
Batch Loss = 0.002209386310073171
1765, epoch_train_loss=0.002209386310073171
Epoch 1766
Epoch 1766 :: Batch 0/1
Batch Loss = 0.0022092851619849034
1766, epoch_train_loss=0.0022092851619849034
Epoch 1767
Epoch 1767 :: Batch 0/1
Batch Loss = 0.0022091847044505143
1767, epoch_train_loss=0.0022091847044505143
Epoch 1768
Epoch 1768 :: Batch 0/1
Batch Loss = 0.002209084830996262
1768, epoch_train_loss=0.002209084830996262
Epoch 1769
Epoch 1769 :: Batch 0/1
Batch Loss = 0.0022089856724237214
1769, epoch_train_loss=0.0022089856724237214
Epoch 1770
Epoch 1770 :: Batch 0/1
Batch Loss = 0.002208887243732271
1770, epoch_train_loss=0.002208887243732271
Epoch 1771
Epoch 1771 :: Batch 0/1
Batch Loss = 0.002208789827286735
1771, epoch_train_loss=0.002208789827286735
Epoch 1772
Epoch 1772 :: Batch 0/1
Batch Loss = 0.002208693624929928
1772, epoch_train_loss=0.002208693624929928
Epoch 1773
Epoch 1773 :: Batch 0/1
Batch Loss = 0.002208599212163528
1773, epoch_train_loss=0.002208599212163528
Epoch 1774
Epoch 1774 :: Batch 0/1
Batch Loss = 0.0022085071805196727
1774, epoch_train_loss=0.0022085071805196727
Epoch 1775
Epoch 1775 :: Batch 0/1
Batch Loss = 0.002208418738245091
1775, epoch_train_loss=0.002208418738245091
Epoch 1776
Epoch 1776 :: Batch 0/1
Batch Loss = 0.002208335240384595
1776, epoch_train_loss=0.002208335240384595
Epoch 1777
Epoch 1777 :: Batch 0/1
Batch Loss = 0.0022082592472630917
1777, epoch_train_loss=0.0022082592472630917
Epoch 1778
Epoch 1778 :: Batch 0/1
Batch Loss = 0.0022081939603988848
1778, epoch_train_loss=0.0022081939603988848
Epoch 1779
Epoch 1779 :: Batch 0/1
Batch Loss = 0.0022081451025793685
1779, epoch_train_loss=0.0022081451025793685
Epoch 1780
Epoch 1780 :: Batch 0/1
Batch Loss = 0.0022081203581823376
1780, epoch_train_loss=0.0022081203581823376
Epoch 1781
Epoch 1781 :: Batch 0/1
Batch Loss = 0.0022081331268712376
1781, epoch_train_loss=0.0022081331268712376
Epoch 1782
Epoch 1782 :: Batch 0/1
Batch Loss = 0.0022082022717249358
1782, epoch_train_loss=0.0022082022717249358
Epoch 1783
Epoch 1783 :: Batch 0/1
Batch Loss = 0.002208360695315728
1783, epoch_train_loss=0.002208360695315728
Epoch 1784
Epoch 1784 :: Batch 0/1
Batch Loss = 0.0022086558831131767
1784, epoch_train_loss=0.0022086558831131767
Epoch 1785
Epoch 1785 :: Batch 0/1
Batch Loss = 0.002209172008215675
1785, epoch_train_loss=0.002209172008215675
Epoch 1786
Epoch 1786 :: Batch 0/1
Batch Loss = 0.002210031839503121
1786, epoch_train_loss=0.002210031839503121
Epoch 1787
Epoch 1787 :: Batch 0/1
Batch Loss = 0.0022114593966912083
1787, epoch_train_loss=0.0022114593966912083
Epoch 1788
Epoch 1788 :: Batch 0/1
Batch Loss = 0.002213780303575207
1788, epoch_train_loss=0.002213780303575207
Epoch 1789
Epoch 1789 :: Batch 0/1
Batch Loss = 0.0022176145607840513
1789, epoch_train_loss=0.0022176145607840513
Epoch 1790
Epoch 1790 :: Batch 0/1
Batch Loss = 0.00222384503515827
1790, epoch_train_loss=0.00222384503515827
Epoch 1791
Epoch 1791 :: Batch 0/1
Batch Loss = 0.002234256618569519
1791, epoch_train_loss=0.002234256618569519
Epoch 1792
Epoch 1792 :: Batch 0/1
Batch Loss = 0.002251277005589098
1792, epoch_train_loss=0.002251277005589098
Epoch 1793
Epoch 1793 :: Batch 0/1
Batch Loss = 0.002280252430142389
1793, epoch_train_loss=0.002280252430142389
Epoch 1794
Epoch 1794 :: Batch 0/1
Batch Loss = 0.002327842164988428
1794, epoch_train_loss=0.002327842164988428
Epoch 1795
Epoch 1795 :: Batch 0/1
Batch Loss = 0.002410680913078749
1795, epoch_train_loss=0.002410680913078749
Epoch 1796
Epoch 1796 :: Batch 0/1
Batch Loss = 0.002546228466334358
1796, epoch_train_loss=0.002546228466334358
Epoch 1797
Epoch 1797 :: Batch 0/1
Batch Loss = 0.0027877366134156454
1797, epoch_train_loss=0.0027877366134156454
Epoch 1798
Epoch 1798 :: Batch 0/1
Batch Loss = 0.0031731210592678676
1798, epoch_train_loss=0.0031731210592678676
Epoch 1799
Epoch 1799 :: Batch 0/1
Batch Loss = 0.0038724104891815735
1799, epoch_train_loss=0.0038724104891815735
Epoch 1800
Epoch 1800 :: Batch 0/1
Batch Loss = 0.004902592716586772
1800, epoch_train_loss=0.004902592716586772
Epoch 1801
Epoch 1801 :: Batch 0/1
Batch Loss = 0.006764816437578153
1801, epoch_train_loss=0.006764816437578153
Epoch 1802
Epoch 1802 :: Batch 0/1
Batch Loss = 0.008921187738232944
1802, epoch_train_loss=0.008921187738232944
Epoch 1803
Epoch 1803 :: Batch 0/1
Batch Loss = 0.012619936557487627
1803, epoch_train_loss=0.012619936557487627
Epoch 1804
Epoch 1804 :: Batch 0/1
Batch Loss = 0.014200595618164325
1804, epoch_train_loss=0.014200595618164325
Epoch 1805
Epoch 1805 :: Batch 0/1
Batch Loss = 0.01703242285528862
1805, epoch_train_loss=0.01703242285528862
Epoch 1806
Epoch 1806 :: Batch 0/1
Batch Loss = 0.012492623923948402
1806, epoch_train_loss=0.012492623923948402
Epoch 1807
Epoch 1807 :: Batch 0/1
Batch Loss = 0.009171215613372224
1807, epoch_train_loss=0.009171215613372224
Epoch 1808
Epoch 1808 :: Batch 0/1
Batch Loss = 0.004203398600771007
1808, epoch_train_loss=0.004203398600771007
Epoch 1809
Epoch 1809 :: Batch 0/1
Batch Loss = 0.0028900237847863087
1809, epoch_train_loss=0.0028900237847863087
Epoch 1810
Epoch 1810 :: Batch 0/1
Batch Loss = 0.004593700023999304
1810, epoch_train_loss=0.004593700023999304
Epoch 1811
Epoch 1811 :: Batch 0/1
Batch Loss = 0.006787063371116983
1811, epoch_train_loss=0.006787063371116983
Epoch 1812
Epoch 1812 :: Batch 0/1
Batch Loss = 0.008424560946181277
1812, epoch_train_loss=0.008424560946181277
Epoch 1813
Epoch 1813 :: Batch 0/1
Batch Loss = 0.006050996553571519
1813, epoch_train_loss=0.006050996553571519
Epoch 1814
Epoch 1814 :: Batch 0/1
Batch Loss = 0.003673112366801833
1814, epoch_train_loss=0.003673112366801833
Epoch 1815
Epoch 1815 :: Batch 0/1
Batch Loss = 0.0027112189059331983
1815, epoch_train_loss=0.0027112189059331983
Epoch 1816
Epoch 1816 :: Batch 0/1
Batch Loss = 0.003939477596289783
1816, epoch_train_loss=0.003939477596289783
Epoch 1817
Epoch 1817 :: Batch 0/1
Batch Loss = 0.00549543552725259
1817, epoch_train_loss=0.00549543552725259
Epoch 1818
Epoch 1818 :: Batch 0/1
Batch Loss = 0.004699673012467585
1818, epoch_train_loss=0.004699673012467585
Epoch 1819
Epoch 1819 :: Batch 0/1
Batch Loss = 0.003263761621968267
1819, epoch_train_loss=0.003263761621968267
Epoch 1820
Epoch 1820 :: Batch 0/1
Batch Loss = 0.002432061710983355
1820, epoch_train_loss=0.002432061710983355
Epoch 1821
Epoch 1821 :: Batch 0/1
Batch Loss = 0.003001218511764917
1821, epoch_train_loss=0.003001218511764917
Epoch 1822
Epoch 1822 :: Batch 0/1
Batch Loss = 0.003933471976768382
1822, epoch_train_loss=0.003933471976768382
Epoch 1823
Epoch 1823 :: Batch 0/1
Batch Loss = 0.003619556273800071
1823, epoch_train_loss=0.003619556273800071
Epoch 1824
Epoch 1824 :: Batch 0/1
Batch Loss = 0.0027486840630670186
1824, epoch_train_loss=0.0027486840630670186
Epoch 1825
Epoch 1825 :: Batch 0/1
Batch Loss = 0.0023300527499652743
1825, epoch_train_loss=0.0023300527499652743
Epoch 1826
Epoch 1826 :: Batch 0/1
Batch Loss = 0.0027974374650706684
1826, epoch_train_loss=0.0027974374650706684
Epoch 1827
Epoch 1827 :: Batch 0/1
Batch Loss = 0.0033796689685752254
1827, epoch_train_loss=0.0033796689685752254
Epoch 1828
Epoch 1828 :: Batch 0/1
Batch Loss = 0.0031633831429132317
1828, epoch_train_loss=0.0031633831429132317
Epoch 1829
Epoch 1829 :: Batch 0/1
Batch Loss = 0.002590356441352463
1829, epoch_train_loss=0.002590356441352463
Epoch 1830
Epoch 1830 :: Batch 0/1
Batch Loss = 0.002277558381562805
1830, epoch_train_loss=0.002277558381562805
Epoch 1831
Epoch 1831 :: Batch 0/1
Batch Loss = 0.0025292793962491578
1831, epoch_train_loss=0.0025292793962491578
Epoch 1832
Epoch 1832 :: Batch 0/1
Batch Loss = 0.00288082507552831
1832, epoch_train_loss=0.00288082507552831
Epoch 1833
Epoch 1833 :: Batch 0/1
Batch Loss = 0.0027674086088606865
1833, epoch_train_loss=0.0027674086088606865
Epoch 1834
Epoch 1834 :: Batch 0/1
Batch Loss = 0.0024257664563172867
1834, epoch_train_loss=0.0024257664563172867
Epoch 1835
Epoch 1835 :: Batch 0/1
Batch Loss = 0.002253969561435206
1835, epoch_train_loss=0.002253969561435206
Epoch 1836
Epoch 1836 :: Batch 0/1
Batch Loss = 0.00240853783655864
1836, epoch_train_loss=0.00240853783655864
Epoch 1837
Epoch 1837 :: Batch 0/1
Batch Loss = 0.002620618288681115
1837, epoch_train_loss=0.002620618288681115
Epoch 1838
Epoch 1838 :: Batch 0/1
Batch Loss = 0.002573316983500391
1838, epoch_train_loss=0.002573316983500391
Epoch 1839
Epoch 1839 :: Batch 0/1
Batch Loss = 0.0023732941784320796
1839, epoch_train_loss=0.0023732941784320796
Epoch 1840
Epoch 1840 :: Batch 0/1
Batch Loss = 0.0022429195536660643
1840, epoch_train_loss=0.0022429195536660643
Epoch 1841
Epoch 1841 :: Batch 0/1
Batch Loss = 0.002308134338470625
1841, epoch_train_loss=0.002308134338470625
Epoch 1842
Epoch 1842 :: Batch 0/1
Batch Loss = 0.0024364153471021248
1842, epoch_train_loss=0.0024364153471021248
Epoch 1843
Epoch 1843 :: Batch 0/1
Batch Loss = 0.0024398433437072984
1843, epoch_train_loss=0.0024398433437072984
Epoch 1844
Epoch 1844 :: Batch 0/1
Batch Loss = 0.0023389778682080965
1844, epoch_train_loss=0.0023389778682080965
Epoch 1845
Epoch 1845 :: Batch 0/1
Batch Loss = 0.002242624007965227
1845, epoch_train_loss=0.002242624007965227
Epoch 1846
Epoch 1846 :: Batch 0/1
Batch Loss = 0.0022450580060101224
1846, epoch_train_loss=0.0022450580060101224
Epoch 1847
Epoch 1847 :: Batch 0/1
Batch Loss = 0.0023120631454863747
1847, epoch_train_loss=0.0023120631454863747
Epoch 1848
Epoch 1848 :: Batch 0/1
Batch Loss = 0.0023469514803647284
1848, epoch_train_loss=0.0023469514803647284
Epoch 1849
Epoch 1849 :: Batch 0/1
Batch Loss = 0.002318702358888177
1849, epoch_train_loss=0.002318702358888177
Epoch 1850
Epoch 1850 :: Batch 0/1
Batch Loss = 0.0022563985922987666
1850, epoch_train_loss=0.0022563985922987666
Epoch 1851
Epoch 1851 :: Batch 0/1
Batch Loss = 0.002224937839557428
1851, epoch_train_loss=0.002224937839557428
Epoch 1852
Epoch 1852 :: Batch 0/1
Batch Loss = 0.002241233682341446
1852, epoch_train_loss=0.002241233682341446
Epoch 1853
Epoch 1853 :: Batch 0/1
Batch Loss = 0.002274066851306878
1853, epoch_train_loss=0.002274066851306878
Epoch 1854
Epoch 1854 :: Batch 0/1
Batch Loss = 0.0022881888395170896
1854, epoch_train_loss=0.0022881888395170896
Epoch 1855
Epoch 1855 :: Batch 0/1
Batch Loss = 0.0022689412034015873
1855, epoch_train_loss=0.0022689412034015873
Epoch 1856
Epoch 1856 :: Batch 0/1
Batch Loss = 0.0022383100380820157
1856, epoch_train_loss=0.0022383100380820157
Epoch 1857
Epoch 1857 :: Batch 0/1
Batch Loss = 0.002220554558772152
1857, epoch_train_loss=0.002220554558772152
Epoch 1858
Epoch 1858 :: Batch 0/1
Batch Loss = 0.002225228742186801
1858, epoch_train_loss=0.002225228742186801
Epoch 1859
Epoch 1859 :: Batch 0/1
Batch Loss = 0.0022418947483561803
1859, epoch_train_loss=0.0022418947483561803
Epoch 1860
Epoch 1860 :: Batch 0/1
Batch Loss = 0.0022519036084696817
1860, epoch_train_loss=0.0022519036084696817
Epoch 1861
Epoch 1861 :: Batch 0/1
Batch Loss = 0.0022470256387117633
1861, epoch_train_loss=0.0022470256387117633
Epoch 1862
Epoch 1862 :: Batch 0/1
Batch Loss = 0.0022311843196123355
1862, epoch_train_loss=0.0022311843196123355
Epoch 1863
Epoch 1863 :: Batch 0/1
Batch Loss = 0.002217441355949174
1863, epoch_train_loss=0.002217441355949174
Epoch 1864
Epoch 1864 :: Batch 0/1
Batch Loss = 0.0022147895484707593
1864, epoch_train_loss=0.0022147895484707593
Epoch 1865
Epoch 1865 :: Batch 0/1
Batch Loss = 0.002222317678497469
1865, epoch_train_loss=0.002222317678497469
Epoch 1866
Epoch 1866 :: Batch 0/1
Batch Loss = 0.002231119249264396
1866, epoch_train_loss=0.002231119249264396
Epoch 1867
Epoch 1867 :: Batch 0/1
Batch Loss = 0.002232416391765524
1867, epoch_train_loss=0.002232416391765524
Epoch 1868
Epoch 1868 :: Batch 0/1
Batch Loss = 0.002225406811090379
1868, epoch_train_loss=0.002225406811090379
Epoch 1869
Epoch 1869 :: Batch 0/1
Batch Loss = 0.0022158277146090435
1869, epoch_train_loss=0.0022158277146090435
Epoch 1870
Epoch 1870 :: Batch 0/1
Batch Loss = 0.0022110823532788515
1870, epoch_train_loss=0.0022110823532788515
Epoch 1871
Epoch 1871 :: Batch 0/1
Batch Loss = 0.0022133076398729535
1871, epoch_train_loss=0.0022133076398729535
Epoch 1872
Epoch 1872 :: Batch 0/1
Batch Loss = 0.002218777775606739
1872, epoch_train_loss=0.002218777775606739
Epoch 1873
Epoch 1873 :: Batch 0/1
Batch Loss = 0.002222014583586485
1873, epoch_train_loss=0.002222014583586485
Epoch 1874
Epoch 1874 :: Batch 0/1
Batch Loss = 0.0022202050210523476
1874, epoch_train_loss=0.0022202050210523476
Epoch 1875
Epoch 1875 :: Batch 0/1
Batch Loss = 0.0022152982767888
1875, epoch_train_loss=0.0022152982767888
Epoch 1876
Epoch 1876 :: Batch 0/1
Batch Loss = 0.0022110758823445493
1876, epoch_train_loss=0.0022110758823445493
Epoch 1877
Epoch 1877 :: Batch 0/1
Batch Loss = 0.002210135821422317
1877, epoch_train_loss=0.002210135821422317
Epoch 1878
Epoch 1878 :: Batch 0/1
Batch Loss = 0.0022121421453610117
1878, epoch_train_loss=0.0022121421453610117
Epoch 1879
Epoch 1879 :: Batch 0/1
Batch Loss = 0.0022146586057630695
1879, epoch_train_loss=0.0022146586057630695
Epoch 1880
Epoch 1880 :: Batch 0/1
Batch Loss = 0.002215481115767852
1880, epoch_train_loss=0.002215481115767852
Epoch 1881
Epoch 1881 :: Batch 0/1
Batch Loss = 0.0022140402282627594
1881, epoch_train_loss=0.0022140402282627594
Epoch 1882
Epoch 1882 :: Batch 0/1
Batch Loss = 0.0022115549773377647
1882, epoch_train_loss=0.0022115549773377647
Epoch 1883
Epoch 1883 :: Batch 0/1
Batch Loss = 0.0022096954758762926
1883, epoch_train_loss=0.0022096954758762926
Epoch 1884
Epoch 1884 :: Batch 0/1
Batch Loss = 0.0022093513608226474
1884, epoch_train_loss=0.0022093513608226474
Epoch 1885
Epoch 1885 :: Batch 0/1
Batch Loss = 0.0022101947881197517
1885, epoch_train_loss=0.0022101947881197517
Epoch 1886
Epoch 1886 :: Batch 0/1
Batch Loss = 0.0022111976919740027
1886, epoch_train_loss=0.0022111976919740027
Epoch 1887
Epoch 1887 :: Batch 0/1
Batch Loss = 0.0022114852757134937
1887, epoch_train_loss=0.0022114852757134937
Epoch 1888
Epoch 1888 :: Batch 0/1
Batch Loss = 0.002210864023000643
1888, epoch_train_loss=0.002210864023000643
Epoch 1889
Epoch 1889 :: Batch 0/1
Batch Loss = 0.0022097683274949864
1889, epoch_train_loss=0.0022097683274949864
Epoch 1890
Epoch 1890 :: Batch 0/1
Batch Loss = 0.002208789782490896
1890, epoch_train_loss=0.002208789782490896
Epoch 1891
Epoch 1891 :: Batch 0/1
Batch Loss = 0.002208338681300433
1891, epoch_train_loss=0.002208338681300433
Epoch 1892
Epoch 1892 :: Batch 0/1
Batch Loss = 0.002208425166165105
1892, epoch_train_loss=0.002208425166165105
Epoch 1893
Epoch 1893 :: Batch 0/1
Batch Loss = 0.002208778213450147
1893, epoch_train_loss=0.002208778213450147
Epoch 1894
Epoch 1894 :: Batch 0/1
Batch Loss = 0.002209032014769994
1894, epoch_train_loss=0.002209032014769994
Epoch 1895
Epoch 1895 :: Batch 0/1
Batch Loss = 0.0022089430671553803
1895, epoch_train_loss=0.0022089430671553803
Epoch 1896
Epoch 1896 :: Batch 0/1
Batch Loss = 0.0022085263838071256
1896, epoch_train_loss=0.0022085263838071256
Epoch 1897
Epoch 1897 :: Batch 0/1
Batch Loss = 0.002207959343072994
1897, epoch_train_loss=0.002207959343072994
Epoch 1898
Epoch 1898 :: Batch 0/1
Batch Loss = 0.002207486263492187
1898, epoch_train_loss=0.002207486263492187
Epoch 1899
Epoch 1899 :: Batch 0/1
Batch Loss = 0.002207246545998464
1899, epoch_train_loss=0.002207246545998464
Epoch 1900
Epoch 1900 :: Batch 0/1
Batch Loss = 0.0022072347487867208
1900, epoch_train_loss=0.0022072347487867208
Epoch 1901
Epoch 1901 :: Batch 0/1
Batch Loss = 0.002207338473378436
1901, epoch_train_loss=0.002207338473378436
Epoch 1902
Epoch 1902 :: Batch 0/1
Batch Loss = 0.0022074032612895114
1902, epoch_train_loss=0.0022074032612895114
Epoch 1903
Epoch 1903 :: Batch 0/1
Batch Loss = 0.002207326657940224
1903, epoch_train_loss=0.002207326657940224
Epoch 1904
Epoch 1904 :: Batch 0/1
Batch Loss = 0.002207089549962408
1904, epoch_train_loss=0.002207089549962408
Epoch 1905
Epoch 1905 :: Batch 0/1
Batch Loss = 0.002206765321757952
1905, epoch_train_loss=0.002206765321757952
Epoch 1906
Epoch 1906 :: Batch 0/1
Batch Loss = 0.0022064614452234443
1906, epoch_train_loss=0.0022064614452234443
Epoch 1907
Epoch 1907 :: Batch 0/1
Batch Loss = 0.0022062580169875962
1907, epoch_train_loss=0.0022062580169875962
Epoch 1908
Epoch 1908 :: Batch 0/1
Batch Loss = 0.0022061698253929424
1908, epoch_train_loss=0.0022061698253929424
Epoch 1909
Epoch 1909 :: Batch 0/1
Batch Loss = 0.0022061536054802764
1909, epoch_train_loss=0.0022061536054802764
Epoch 1910
Epoch 1910 :: Batch 0/1
Batch Loss = 0.0022061412889618675
1910, epoch_train_loss=0.0022061412889618675
Epoch 1911
Epoch 1911 :: Batch 0/1
Batch Loss = 0.0022060788858891327
1911, epoch_train_loss=0.0022060788858891327
Epoch 1912
Epoch 1912 :: Batch 0/1
Batch Loss = 0.0022059474242941664
1912, epoch_train_loss=0.0022059474242941664
Epoch 1913
Epoch 1913 :: Batch 0/1
Batch Loss = 0.002205761305022591
1913, epoch_train_loss=0.002205761305022591
Epoch 1914
Epoch 1914 :: Batch 0/1
Batch Loss = 0.0022055605293392236
1914, epoch_train_loss=0.0022055605293392236
Epoch 1915
Epoch 1915 :: Batch 0/1
Batch Loss = 0.002205381784586621
1915, epoch_train_loss=0.002205381784586621
Epoch 1916
Epoch 1916 :: Batch 0/1
Batch Loss = 0.0022052462997049103
1916, epoch_train_loss=0.0022052462997049103
Epoch 1917
Epoch 1917 :: Batch 0/1
Batch Loss = 0.002205150627700343
1917, epoch_train_loss=0.002205150627700343
Epoch 1918
Epoch 1918 :: Batch 0/1
Batch Loss = 0.0022050760390540766
1918, epoch_train_loss=0.0022050760390540766
Epoch 1919
Epoch 1919 :: Batch 0/1
Batch Loss = 0.0022050020140014968
1919, epoch_train_loss=0.0022050020140014968
Epoch 1920
Epoch 1920 :: Batch 0/1
Batch Loss = 0.002204912714409805
1920, epoch_train_loss=0.002204912714409805
Epoch 1921
Epoch 1921 :: Batch 0/1
Batch Loss = 0.002204803451501796
1921, epoch_train_loss=0.002204803451501796
Epoch 1922
Epoch 1922 :: Batch 0/1
Batch Loss = 0.0022046765775054536
1922, epoch_train_loss=0.0022046765775054536
Epoch 1923
Epoch 1923 :: Batch 0/1
Batch Loss = 0.0022045410472524
1923, epoch_train_loss=0.0022045410472524
Epoch 1924
Epoch 1924 :: Batch 0/1
Batch Loss = 0.0022044065499848063
1924, epoch_train_loss=0.0022044065499848063
Epoch 1925
Epoch 1925 :: Batch 0/1
Batch Loss = 0.002204280000290226
1925, epoch_train_loss=0.002204280000290226
Epoch 1926
Epoch 1926 :: Batch 0/1
Batch Loss = 0.002204163949956339
1926, epoch_train_loss=0.002204163949956339
Epoch 1927
Epoch 1927 :: Batch 0/1
Batch Loss = 0.0022040574344543607
1927, epoch_train_loss=0.0022040574344543607
Epoch 1928
Epoch 1928 :: Batch 0/1
Batch Loss = 0.0022039576392802428
1928, epoch_train_loss=0.0022039576392802428
Epoch 1929
Epoch 1929 :: Batch 0/1
Batch Loss = 0.00220386113995378
1929, epoch_train_loss=0.00220386113995378
Epoch 1930
Epoch 1930 :: Batch 0/1
Batch Loss = 0.002203764304824951
1930, epoch_train_loss=0.002203764304824951
Epoch 1931
Epoch 1931 :: Batch 0/1
Batch Loss = 0.0022036640934581953
1931, epoch_train_loss=0.0022036640934581953
Epoch 1932
Epoch 1932 :: Batch 0/1
Batch Loss = 0.0022035593431564424
1932, epoch_train_loss=0.0022035593431564424
Epoch 1933
Epoch 1933 :: Batch 0/1
Batch Loss = 0.0022034500071915035
1933, epoch_train_loss=0.0022034500071915035
Epoch 1934
Epoch 1934 :: Batch 0/1
Batch Loss = 0.0022033376473457977
1934, epoch_train_loss=0.0022033376473457977
Epoch 1935
Epoch 1935 :: Batch 0/1
Batch Loss = 0.002203223754950007
1935, epoch_train_loss=0.002203223754950007
Epoch 1936
Epoch 1936 :: Batch 0/1
Batch Loss = 0.0022031103697766037
1936, epoch_train_loss=0.0022031103697766037
Epoch 1937
Epoch 1937 :: Batch 0/1
Batch Loss = 0.0022029991365204965
1937, epoch_train_loss=0.0022029991365204965
Epoch 1938
Epoch 1938 :: Batch 0/1
Batch Loss = 0.002202891080891243
1938, epoch_train_loss=0.002202891080891243
Epoch 1939
Epoch 1939 :: Batch 0/1
Batch Loss = 0.0022027863156891714
1939, epoch_train_loss=0.0022027863156891714
Epoch 1940
Epoch 1940 :: Batch 0/1
Batch Loss = 0.002202684100617439
1940, epoch_train_loss=0.002202684100617439
Epoch 1941
Epoch 1941 :: Batch 0/1
Batch Loss = 0.0022025834861269353
1941, epoch_train_loss=0.0022025834861269353
Epoch 1942
Epoch 1942 :: Batch 0/1
Batch Loss = 0.0022024834517491584
1942, epoch_train_loss=0.0022024834517491584
Epoch 1943
Epoch 1943 :: Batch 0/1
Batch Loss = 0.0022023831666061738
1943, epoch_train_loss=0.0022023831666061738
Epoch 1944
Epoch 1944 :: Batch 0/1
Batch Loss = 0.002202281998385543
1944, epoch_train_loss=0.002202281998385543
Epoch 1945
Epoch 1945 :: Batch 0/1
Batch Loss = 0.0022021797973722336
1945, epoch_train_loss=0.0022021797973722336
Epoch 1946
Epoch 1946 :: Batch 0/1
Batch Loss = 0.0022020766425195053
1946, epoch_train_loss=0.0022020766425195053
Epoch 1947
Epoch 1947 :: Batch 0/1
Batch Loss = 0.002201972964367618
1947, epoch_train_loss=0.002201972964367618
Epoch 1948
Epoch 1948 :: Batch 0/1
Batch Loss = 0.0022018689600245703
1948, epoch_train_loss=0.0022018689600245703
Epoch 1949
Epoch 1949 :: Batch 0/1
Batch Loss = 0.0022017650067439856
1949, epoch_train_loss=0.0022017650067439856
Epoch 1950
Epoch 1950 :: Batch 0/1
Batch Loss = 0.0022016612180931774
1950, epoch_train_loss=0.0022016612180931774
Epoch 1951
Epoch 1951 :: Batch 0/1
Batch Loss = 0.0022015578389898544
1951, epoch_train_loss=0.0022015578389898544
Epoch 1952
Epoch 1952 :: Batch 0/1
Batch Loss = 0.002201454847835346
1952, epoch_train_loss=0.002201454847835346
Epoch 1953
Epoch 1953 :: Batch 0/1
Batch Loss = 0.002201352281710465
1953, epoch_train_loss=0.002201352281710465
Epoch 1954
Epoch 1954 :: Batch 0/1
Batch Loss = 0.002201250071683786
1954, epoch_train_loss=0.002201250071683786
Epoch 1955
Epoch 1955 :: Batch 0/1
Batch Loss = 0.002201148262504192
1955, epoch_train_loss=0.002201148262504192
Epoch 1956
Epoch 1956 :: Batch 0/1
Batch Loss = 0.0022010468115485115
1956, epoch_train_loss=0.0022010468115485115
Epoch 1957
Epoch 1957 :: Batch 0/1
Batch Loss = 0.002200945726981218
1957, epoch_train_loss=0.002200945726981218
Epoch 1958
Epoch 1958 :: Batch 0/1
Batch Loss = 0.00220084495003225
1958, epoch_train_loss=0.00220084495003225
Epoch 1959
Epoch 1959 :: Batch 0/1
Batch Loss = 0.002200744485886677
1959, epoch_train_loss=0.002200744485886677
Epoch 1960
Epoch 1960 :: Batch 0/1
Batch Loss = 0.0022006443097728327
1960, epoch_train_loss=0.0022006443097728327
Epoch 1961
Epoch 1961 :: Batch 0/1
Batch Loss = 0.0022005443853052474
1961, epoch_train_loss=0.0022005443853052474
Epoch 1962
Epoch 1962 :: Batch 0/1
Batch Loss = 0.002200444688002637
1962, epoch_train_loss=0.002200444688002637
Epoch 1963
Epoch 1963 :: Batch 0/1
Batch Loss = 0.0022003451699770123
1963, epoch_train_loss=0.0022003451699770123
Epoch 1964
Epoch 1964 :: Batch 0/1
Batch Loss = 0.0022002458550249753
1964, epoch_train_loss=0.0022002458550249753
Epoch 1965
Epoch 1965 :: Batch 0/1
Batch Loss = 0.0022001467044232012
1965, epoch_train_loss=0.0022001467044232012
Epoch 1966
Epoch 1966 :: Batch 0/1
Batch Loss = 0.0022000477563812897
1966, epoch_train_loss=0.0022000477563812897
Epoch 1967
Epoch 1967 :: Batch 0/1
Batch Loss = 0.0021999489836359008
1967, epoch_train_loss=0.0021999489836359008
Epoch 1968
Epoch 1968 :: Batch 0/1
Batch Loss = 0.0021998504643718424
1968, epoch_train_loss=0.0021998504643718424
Epoch 1969
Epoch 1969 :: Batch 0/1
Batch Loss = 0.002199752194271011
1969, epoch_train_loss=0.002199752194271011
Epoch 1970
Epoch 1970 :: Batch 0/1
Batch Loss = 0.0021996542844360486
1970, epoch_train_loss=0.0021996542844360486
Epoch 1971
Epoch 1971 :: Batch 0/1
Batch Loss = 0.0021995567483135
1971, epoch_train_loss=0.0021995567483135
Epoch 1972
Epoch 1972 :: Batch 0/1
Batch Loss = 0.0021994597714816893
1972, epoch_train_loss=0.0021994597714816893
Epoch 1973
Epoch 1973 :: Batch 0/1
Batch Loss = 0.002199363435565255
1973, epoch_train_loss=0.002199363435565255
Epoch 1974
Epoch 1974 :: Batch 0/1
Batch Loss = 0.00219926806287874
1974, epoch_train_loss=0.00219926806287874
Epoch 1975
Epoch 1975 :: Batch 0/1
Batch Loss = 0.0021991738822241596
1975, epoch_train_loss=0.0021991738822241596
Epoch 1976
Epoch 1976 :: Batch 0/1
Batch Loss = 0.0021990815124731306
1976, epoch_train_loss=0.0021990815124731306
Epoch 1977
Epoch 1977 :: Batch 0/1
Batch Loss = 0.0021989915571035807
1977, epoch_train_loss=0.0021989915571035807
Epoch 1978
Epoch 1978 :: Batch 0/1
Batch Loss = 0.0021989053073985703
1978, epoch_train_loss=0.0021989053073985703
Epoch 1979
Epoch 1979 :: Batch 0/1
Batch Loss = 0.002198824254914781
1979, epoch_train_loss=0.002198824254914781
Epoch 1980
Epoch 1980 :: Batch 0/1
Batch Loss = 0.0021987512693839497
1980, epoch_train_loss=0.0021987512693839497
Epoch 1981
Epoch 1981 :: Batch 0/1
Batch Loss = 0.002198690040157637
1981, epoch_train_loss=0.002198690040157637
Epoch 1982
Epoch 1982 :: Batch 0/1
Batch Loss = 0.0021986473031749437
1982, epoch_train_loss=0.0021986473031749437
Epoch 1983
Epoch 1983 :: Batch 0/1
Batch Loss = 0.002198632280600317
1983, epoch_train_loss=0.002198632280600317
Epoch 1984
Epoch 1984 :: Batch 0/1
Batch Loss = 0.0021986615353103826
1984, epoch_train_loss=0.0021986615353103826
Epoch 1985
Epoch 1985 :: Batch 0/1
Batch Loss = 0.0021987585913551874
1985, epoch_train_loss=0.0021987585913551874
Epoch 1986
Epoch 1986 :: Batch 0/1
Batch Loss = 0.0021989660110711042
1986, epoch_train_loss=0.0021989660110711042
Epoch 1987
Epoch 1987 :: Batch 0/1
Batch Loss = 0.0021993451623280847
1987, epoch_train_loss=0.0021993451623280847
Epoch 1988
Epoch 1988 :: Batch 0/1
Batch Loss = 0.0022000100943892105
1988, epoch_train_loss=0.0022000100943892105
Epoch 1989
Epoch 1989 :: Batch 0/1
Batch Loss = 0.0022011244698069876
1989, epoch_train_loss=0.0022011244698069876
Epoch 1990
Epoch 1990 :: Batch 0/1
Batch Loss = 0.0022030068904247387
1990, epoch_train_loss=0.0022030068904247387
Epoch 1991
Epoch 1991 :: Batch 0/1
Batch Loss = 0.0022061021133002396
1991, epoch_train_loss=0.0022061021133002396
Epoch 1992
Epoch 1992 :: Batch 0/1
Batch Loss = 0.0022113392711401864
1992, epoch_train_loss=0.0022113392711401864
Epoch 1993
Epoch 1993 :: Batch 0/1
Batch Loss = 0.002219940353410122
1993, epoch_train_loss=0.002219940353410122
Epoch 1994
Epoch 1994 :: Batch 0/1
Batch Loss = 0.002234741898940895
1994, epoch_train_loss=0.002234741898940895
Epoch 1995
Epoch 1995 :: Batch 0/1
Batch Loss = 0.0022590811036517933
1995, epoch_train_loss=0.0022590811036517933
Epoch 1996
Epoch 1996 :: Batch 0/1
Batch Loss = 0.0023020557404151852
1996, epoch_train_loss=0.0023020557404151852
Epoch 1997
Epoch 1997 :: Batch 0/1
Batch Loss = 0.0023723599910397274
1997, epoch_train_loss=0.0023723599910397274
Epoch 1998
Epoch 1998 :: Batch 0/1
Batch Loss = 0.002500713706237622
1998, epoch_train_loss=0.002500713706237622
Epoch 1999
Epoch 1999 :: Batch 0/1
Batch Loss = 0.002706106767157819
1999, epoch_train_loss=0.002706106767157819
Epoch 2000
Epoch 2000 :: Batch 0/1
Batch Loss = 0.0030970687050743038
2000, epoch_train_loss=0.0030970687050743038
Epoch 2001
Epoch 2001 :: Batch 0/1
Batch Loss = 0.0036858842539704896
2001, epoch_train_loss=0.0036858842539704896
Epoch 2002
Epoch 2002 :: Batch 0/1
Batch Loss = 0.004861113667561487
2002, epoch_train_loss=0.004861113667561487
Epoch 2003
Epoch 2003 :: Batch 0/1
Batch Loss = 0.006373653891679656
2003, epoch_train_loss=0.006373653891679656
Epoch 2004
Epoch 2004 :: Batch 0/1
Batch Loss = 0.009505953698059724
2004, epoch_train_loss=0.009505953698059724
Epoch 2005
Epoch 2005 :: Batch 0/1
Batch Loss = 0.012038744602716103
2005, epoch_train_loss=0.012038744602716103
Epoch 2006
Epoch 2006 :: Batch 0/1
Batch Loss = 0.017228044712560724
2006, epoch_train_loss=0.017228044712560724
Epoch 2007
Epoch 2007 :: Batch 0/1
Batch Loss = 0.016285915678833095
2007, epoch_train_loss=0.016285915678833095
Epoch 2008
Epoch 2008 :: Batch 0/1
Batch Loss = 0.01584233415839243
2008, epoch_train_loss=0.01584233415839243
Epoch 2009
Epoch 2009 :: Batch 0/1
Batch Loss = 0.008305858387147862
2009, epoch_train_loss=0.008305858387147862
Epoch 2010
Epoch 2010 :: Batch 0/1
Batch Loss = 0.0033780800087754013
2010, epoch_train_loss=0.0033780800087754013
Epoch 2011
Epoch 2011 :: Batch 0/1
Batch Loss = 0.0026756293176592844
2011, epoch_train_loss=0.0026756293176592844
Epoch 2012
Epoch 2012 :: Batch 0/1
Batch Loss = 0.0056489632596812426
2012, epoch_train_loss=0.0056489632596812426
Epoch 2013
Epoch 2013 :: Batch 0/1
Batch Loss = 0.008959720010507394
2013, epoch_train_loss=0.008959720010507394
Epoch 2014
Epoch 2014 :: Batch 0/1
Batch Loss = 0.0077819777035632864
2014, epoch_train_loss=0.0077819777035632864
Epoch 2015
Epoch 2015 :: Batch 0/1
Batch Loss = 0.0053534051762837796
2015, epoch_train_loss=0.0053534051762837796
Epoch 2016
Epoch 2016 :: Batch 0/1
Batch Loss = 0.002913614049664491
2016, epoch_train_loss=0.002913614049664491
Epoch 2017
Epoch 2017 :: Batch 0/1
Batch Loss = 0.003427851593309788
2017, epoch_train_loss=0.003427851593309788
Epoch 2018
Epoch 2018 :: Batch 0/1
Batch Loss = 0.005392489072531621
2018, epoch_train_loss=0.005392489072531621
Epoch 2019
Epoch 2019 :: Batch 0/1
Batch Loss = 0.005501127476068375
2019, epoch_train_loss=0.005501127476068375
Epoch 2020
Epoch 2020 :: Batch 0/1
Batch Loss = 0.004318926230510517
2020, epoch_train_loss=0.004318926230510517
Epoch 2021
Epoch 2021 :: Batch 0/1
Batch Loss = 0.0028001982091187838
2021, epoch_train_loss=0.0028001982091187838
Epoch 2022
Epoch 2022 :: Batch 0/1
Batch Loss = 0.002749979855215848
2022, epoch_train_loss=0.002749979855215848
Epoch 2023
Epoch 2023 :: Batch 0/1
Batch Loss = 0.003694571191997145
2023, epoch_train_loss=0.003694571191997145
Epoch 2024
Epoch 2024 :: Batch 0/1
Batch Loss = 0.004017750671258449
2024, epoch_train_loss=0.004017750671258449
Epoch 2025
Epoch 2025 :: Batch 0/1
Batch Loss = 0.003413622951511707
2025, epoch_train_loss=0.003413622951511707
Epoch 2026
Epoch 2026 :: Batch 0/1
Batch Loss = 0.002537694086927489
2026, epoch_train_loss=0.002537694086927489
Epoch 2027
Epoch 2027 :: Batch 0/1
Batch Loss = 0.0025384030742372095
2027, epoch_train_loss=0.0025384030742372095
Epoch 2028
Epoch 2028 :: Batch 0/1
Batch Loss = 0.003136791480636266
2028, epoch_train_loss=0.003136791480636266
Epoch 2029
Epoch 2029 :: Batch 0/1
Batch Loss = 0.0033926752952357572
2029, epoch_train_loss=0.0033926752952357572
Epoch 2030
Epoch 2030 :: Batch 0/1
Batch Loss = 0.003121503012432965
2030, epoch_train_loss=0.003121503012432965
Epoch 2031
Epoch 2031 :: Batch 0/1
Batch Loss = 0.002600814961859006
2031, epoch_train_loss=0.002600814961859006
Epoch 2032
Epoch 2032 :: Batch 0/1
Batch Loss = 0.0024355100312286166
2032, epoch_train_loss=0.0024355100312286166
Epoch 2033
Epoch 2033 :: Batch 0/1
Batch Loss = 0.002635538443243685
2033, epoch_train_loss=0.002635538443243685
Epoch 2034
Epoch 2034 :: Batch 0/1
Batch Loss = 0.0028230539394334423
2034, epoch_train_loss=0.0028230539394334423
Epoch 2035
Epoch 2035 :: Batch 0/1
Batch Loss = 0.0027717939404585633
2035, epoch_train_loss=0.0027717939404585633
Epoch 2036
Epoch 2036 :: Batch 0/1
Batch Loss = 0.0025192852771598064
2036, epoch_train_loss=0.0025192852771598064
Epoch 2037
Epoch 2037 :: Batch 0/1
Batch Loss = 0.0023658788270931653
2037, epoch_train_loss=0.0023658788270931653
Epoch 2038
Epoch 2038 :: Batch 0/1
Batch Loss = 0.0024093209209292347
2038, epoch_train_loss=0.0024093209209292347
Epoch 2039
Epoch 2039 :: Batch 0/1
Batch Loss = 0.002536564341595237
2039, epoch_train_loss=0.002536564341595237
Epoch 2040
Epoch 2040 :: Batch 0/1
Batch Loss = 0.002574526773054761
2040, epoch_train_loss=0.002574526773054761
Epoch 2041
Epoch 2041 :: Batch 0/1
Batch Loss = 0.002446444935254206
2041, epoch_train_loss=0.002446444935254206
Epoch 2042
Epoch 2042 :: Batch 0/1
Batch Loss = 0.002315758817351708
2042, epoch_train_loss=0.002315758817351708
Epoch 2043
Epoch 2043 :: Batch 0/1
Batch Loss = 0.002309669330647378
2043, epoch_train_loss=0.002309669330647378
Epoch 2044
Epoch 2044 :: Batch 0/1
Batch Loss = 0.002401261776292122
2044, epoch_train_loss=0.002401261776292122
Epoch 2045
Epoch 2045 :: Batch 0/1
Batch Loss = 0.0024541484130808115
2045, epoch_train_loss=0.0024541484130808115
Epoch 2046
Epoch 2046 :: Batch 0/1
Batch Loss = 0.002384855586207059
2046, epoch_train_loss=0.002384855586207059
Epoch 2047
Epoch 2047 :: Batch 0/1
Batch Loss = 0.0022814529793658005
2047, epoch_train_loss=0.0022814529793658005
Epoch 2048
Epoch 2048 :: Batch 0/1
Batch Loss = 0.002251189703371996
2048, epoch_train_loss=0.002251189703371996
Epoch 2049
Epoch 2049 :: Batch 0/1
Batch Loss = 0.0023097021746925294
2049, epoch_train_loss=0.0023097021746925294
Epoch 2050
Epoch 2050 :: Batch 0/1
Batch Loss = 0.0023625999829335483
2050, epoch_train_loss=0.0023625999829335483
Epoch 2051
Epoch 2051 :: Batch 0/1
Batch Loss = 0.0023325602450195108
2051, epoch_train_loss=0.0023325602450195108
Epoch 2052
Epoch 2052 :: Batch 0/1
Batch Loss = 0.002261323367659496
2052, epoch_train_loss=0.002261323367659496
Epoch 2053
Epoch 2053 :: Batch 0/1
Batch Loss = 0.0022263283453982452
2053, epoch_train_loss=0.0022263283453982452
Epoch 2054
Epoch 2054 :: Batch 0/1
Batch Loss = 0.0022557329362234584
2054, epoch_train_loss=0.0022557329362234584
Epoch 2055
Epoch 2055 :: Batch 0/1
Batch Loss = 0.002297794903326409
2055, epoch_train_loss=0.002297794903326409
Epoch 2056
Epoch 2056 :: Batch 0/1
Batch Loss = 0.0022957665888732885
2056, epoch_train_loss=0.0022957665888732885
Epoch 2057
Epoch 2057 :: Batch 0/1
Batch Loss = 0.0022563327915446204
2057, epoch_train_loss=0.0022563327915446204
Epoch 2058
Epoch 2058 :: Batch 0/1
Batch Loss = 0.002220775723203395
2058, epoch_train_loss=0.002220775723203395
Epoch 2059
Epoch 2059 :: Batch 0/1
Batch Loss = 0.002222171254304485
2059, epoch_train_loss=0.002222171254304485
Epoch 2060
Epoch 2060 :: Batch 0/1
Batch Loss = 0.00224594414244663
2060, epoch_train_loss=0.00224594414244663
Epoch 2061
Epoch 2061 :: Batch 0/1
Batch Loss = 0.002259263256652812
2061, epoch_train_loss=0.002259263256652812
Epoch 2062
Epoch 2062 :: Batch 0/1
Batch Loss = 0.0022493150517743474
2062, epoch_train_loss=0.0022493150517743474
Epoch 2063
Epoch 2063 :: Batch 0/1
Batch Loss = 0.002226544091562693
2063, epoch_train_loss=0.002226544091562693
Epoch 2064
Epoch 2064 :: Batch 0/1
Batch Loss = 0.0022134284132607754
2064, epoch_train_loss=0.0022134284132607754
Epoch 2065
Epoch 2065 :: Batch 0/1
Batch Loss = 0.002215863817843639
2065, epoch_train_loss=0.002215863817843639
Epoch 2066
Epoch 2066 :: Batch 0/1
Batch Loss = 0.0022254739839513565
2066, epoch_train_loss=0.0022254739839513565
Epoch 2067
Epoch 2067 :: Batch 0/1
Batch Loss = 0.002230870297796448
2067, epoch_train_loss=0.002230870297796448
Epoch 2068
Epoch 2068 :: Batch 0/1
Batch Loss = 0.002226816460053401
2068, epoch_train_loss=0.002226816460053401
Epoch 2069
Epoch 2069 :: Batch 0/1
Batch Loss = 0.0022187256183962955
2069, epoch_train_loss=0.0022187256183962955
Epoch 2070
Epoch 2070 :: Batch 0/1
Batch Loss = 0.0022118227736376366
2070, epoch_train_loss=0.0022118227736376366
Epoch 2071
Epoch 2071 :: Batch 0/1
Batch Loss = 0.002209436500537012
2071, epoch_train_loss=0.002209436500537012
Epoch 2072
Epoch 2072 :: Batch 0/1
Batch Loss = 0.002211217174546243
2072, epoch_train_loss=0.002211217174546243
Epoch 2073
Epoch 2073 :: Batch 0/1
Batch Loss = 0.002214458950877151
2073, epoch_train_loss=0.002214458950877151
Epoch 2074
Epoch 2074 :: Batch 0/1
Batch Loss = 0.002216781050661767
2074, epoch_train_loss=0.002216781050661767
Epoch 2075
Epoch 2075 :: Batch 0/1
Batch Loss = 0.002215690535587813
2075, epoch_train_loss=0.002215690535587813
Epoch 2076
Epoch 2076 :: Batch 0/1
Batch Loss = 0.002211558637792644
2076, epoch_train_loss=0.002211558637792644
Epoch 2077
Epoch 2077 :: Batch 0/1
Batch Loss = 0.002206915349867272
2077, epoch_train_loss=0.002206915349867272
Epoch 2078
Epoch 2078 :: Batch 0/1
Batch Loss = 0.0022047356328639195
2078, epoch_train_loss=0.0022047356328639195
Epoch 2079
Epoch 2079 :: Batch 0/1
Batch Loss = 0.0022061855389337915
2079, epoch_train_loss=0.0022061855389337915
Epoch 2080
Epoch 2080 :: Batch 0/1
Batch Loss = 0.0022092067129124053
2080, epoch_train_loss=0.0022092067129124053
Epoch 2081
Epoch 2081 :: Batch 0/1
Batch Loss = 0.0022108214854769494
2081, epoch_train_loss=0.0022108214854769494
Epoch 2082
Epoch 2082 :: Batch 0/1
Batch Loss = 0.00220960667155268
2082, epoch_train_loss=0.00220960667155268
Epoch 2083
Epoch 2083 :: Batch 0/1
Batch Loss = 0.0022066226101940326
2083, epoch_train_loss=0.0022066226101940326
Epoch 2084
Epoch 2084 :: Batch 0/1
Batch Loss = 0.0022041388188320677
2084, epoch_train_loss=0.0022041388188320677
Epoch 2085
Epoch 2085 :: Batch 0/1
Batch Loss = 0.0022034692344467663
2085, epoch_train_loss=0.0022034692344467663
Epoch 2086
Epoch 2086 :: Batch 0/1
Batch Loss = 0.0022042957248386307
2086, epoch_train_loss=0.0022042957248386307
Epoch 2087
Epoch 2087 :: Batch 0/1
Batch Loss = 0.0022054329167626495
2087, epoch_train_loss=0.0022054329167626495
Epoch 2088
Epoch 2088 :: Batch 0/1
Batch Loss = 0.002205846509318298
2088, epoch_train_loss=0.002205846509318298
Epoch 2089
Epoch 2089 :: Batch 0/1
Batch Loss = 0.002205390542154155
2089, epoch_train_loss=0.002205390542154155
Epoch 2090
Epoch 2090 :: Batch 0/1
Batch Loss = 0.0022044583966142964
2090, epoch_train_loss=0.0022044583966142964
Epoch 2091
Epoch 2091 :: Batch 0/1
Batch Loss = 0.002203501208341178
2091, epoch_train_loss=0.002203501208341178
Epoch 2092
Epoch 2092 :: Batch 0/1
Batch Loss = 0.002202855718868901
2092, epoch_train_loss=0.002202855718868901
Epoch 2093
Epoch 2093 :: Batch 0/1
Batch Loss = 0.0022025760774245684
2093, epoch_train_loss=0.0022025760774245684
Epoch 2094
Epoch 2094 :: Batch 0/1
Batch Loss = 0.0022026579146978025
2094, epoch_train_loss=0.0022026579146978025
Epoch 2095
Epoch 2095 :: Batch 0/1
Batch Loss = 0.0022029353300632246
2095, epoch_train_loss=0.0022029353300632246
Epoch 2096
Epoch 2096 :: Batch 0/1
Batch Loss = 0.0022031505676221763
2096, epoch_train_loss=0.0022031505676221763
Epoch 2097
Epoch 2097 :: Batch 0/1
Batch Loss = 0.002203090502647799
2097, epoch_train_loss=0.002203090502647799
Epoch 2098
Epoch 2098 :: Batch 0/1
Batch Loss = 0.0022026485686616323
2098, epoch_train_loss=0.0022026485686616323
Epoch 2099
Epoch 2099 :: Batch 0/1
Batch Loss = 0.002202019527220305
2099, epoch_train_loss=0.002202019527220305
Epoch 2100
Epoch 2100 :: Batch 0/1
Batch Loss = 0.0022014908978629497
2100, epoch_train_loss=0.0022014908978629497
Epoch 2101
Epoch 2101 :: Batch 0/1
Batch Loss = 0.002201274813624425
2101, epoch_train_loss=0.002201274813624425
Epoch 2102
Epoch 2102 :: Batch 0/1
Batch Loss = 0.002201363996176111
2102, epoch_train_loss=0.002201363996176111
Epoch 2103
Epoch 2103 :: Batch 0/1
Batch Loss = 0.0022015381787927357
2103, epoch_train_loss=0.0022015381787927357
Epoch 2104
Epoch 2104 :: Batch 0/1
Batch Loss = 0.002201591707317602
2104, epoch_train_loss=0.002201591707317602
Epoch 2105
Epoch 2105 :: Batch 0/1
Batch Loss = 0.0022014336834092056
2105, epoch_train_loss=0.0022014336834092056
Epoch 2106
Epoch 2106 :: Batch 0/1
Batch Loss = 0.00220113121701067
2106, epoch_train_loss=0.00220113121701067
Epoch 2107
Epoch 2107 :: Batch 0/1
Batch Loss = 0.0022008136458253344
2107, epoch_train_loss=0.0022008136458253344
Epoch 2108
Epoch 2108 :: Batch 0/1
Batch Loss = 0.002200565708460562
2108, epoch_train_loss=0.002200565708460562
Epoch 2109
Epoch 2109 :: Batch 0/1
Batch Loss = 0.0022004137341260386
2109, epoch_train_loss=0.0022004137341260386
Epoch 2110
Epoch 2110 :: Batch 0/1
Batch Loss = 0.002200327684584673
2110, epoch_train_loss=0.002200327684584673
Epoch 2111
Epoch 2111 :: Batch 0/1
Batch Loss = 0.0022002677428593054
2111, epoch_train_loss=0.0022002677428593054
Epoch 2112
Epoch 2112 :: Batch 0/1
Batch Loss = 0.00220021012274841
2112, epoch_train_loss=0.00220021012274841
Epoch 2113
Epoch 2113 :: Batch 0/1
Batch Loss = 0.0022001352707062202
2113, epoch_train_loss=0.0022001352707062202
Epoch 2114
Epoch 2114 :: Batch 0/1
Batch Loss = 0.002200034259160119
2114, epoch_train_loss=0.002200034259160119
Epoch 2115
Epoch 2115 :: Batch 0/1
Batch Loss = 0.002199898134847226
2115, epoch_train_loss=0.002199898134847226
Epoch 2116
Epoch 2116 :: Batch 0/1
Batch Loss = 0.0021997283592993786
2116, epoch_train_loss=0.0021997283592993786
Epoch 2117
Epoch 2117 :: Batch 0/1
Batch Loss = 0.0021995487315046103
2117, epoch_train_loss=0.0021995487315046103
Epoch 2118
Epoch 2118 :: Batch 0/1
Batch Loss = 0.0021993872327416442
2118, epoch_train_loss=0.0021993872327416442
Epoch 2119
Epoch 2119 :: Batch 0/1
Batch Loss = 0.002199268145971775
2119, epoch_train_loss=0.002199268145971775
Epoch 2120
Epoch 2120 :: Batch 0/1
Batch Loss = 0.0021991891634044786
2120, epoch_train_loss=0.0021991891634044786
Epoch 2121
Epoch 2121 :: Batch 0/1
Batch Loss = 0.002199128003155352
2121, epoch_train_loss=0.002199128003155352
Epoch 2122
Epoch 2122 :: Batch 0/1
Batch Loss = 0.002199059717134317
2122, epoch_train_loss=0.002199059717134317
Epoch 2123
Epoch 2123 :: Batch 0/1
Batch Loss = 0.002198967012340979
2123, epoch_train_loss=0.002198967012340979
Epoch 2124
Epoch 2124 :: Batch 0/1
Batch Loss = 0.0021988519679068024
2124, epoch_train_loss=0.0021988519679068024
Epoch 2125
Epoch 2125 :: Batch 0/1
Batch Loss = 0.0021987243865560736
2125, epoch_train_loss=0.0021987243865560736
Epoch 2126
Epoch 2126 :: Batch 0/1
Batch Loss = 0.0021985969310566074
2126, epoch_train_loss=0.0021985969310566074
Epoch 2127
Epoch 2127 :: Batch 0/1
Batch Loss = 0.0021984776973627594
2127, epoch_train_loss=0.0021984776973627594
Epoch 2128
Epoch 2128 :: Batch 0/1
Batch Loss = 0.0021983673404190485
2128, epoch_train_loss=0.0021983673404190485
Epoch 2129
Epoch 2129 :: Batch 0/1
Batch Loss = 0.002198264416177569
2129, epoch_train_loss=0.002198264416177569
Epoch 2130
Epoch 2130 :: Batch 0/1
Batch Loss = 0.002198166211990099
2130, epoch_train_loss=0.002198166211990099
Epoch 2131
Epoch 2131 :: Batch 0/1
Batch Loss = 0.0021980718861006163
2131, epoch_train_loss=0.0021980718861006163
Epoch 2132
Epoch 2132 :: Batch 0/1
Batch Loss = 0.0021979810909224635
2132, epoch_train_loss=0.0021979810909224635
Epoch 2133
Epoch 2133 :: Batch 0/1
Batch Loss = 0.0021978912519348423
2133, epoch_train_loss=0.0021978912519348423
Epoch 2134
Epoch 2134 :: Batch 0/1
Batch Loss = 0.0021977992960812522
2134, epoch_train_loss=0.0021977992960812522
Epoch 2135
Epoch 2135 :: Batch 0/1
Batch Loss = 0.0021977021911653745
2135, epoch_train_loss=0.0021977021911653745
Epoch 2136
Epoch 2136 :: Batch 0/1
Batch Loss = 0.002197599328500306
2136, epoch_train_loss=0.002197599328500306
Epoch 2137
Epoch 2137 :: Batch 0/1
Batch Loss = 0.002197493030424936
2137, epoch_train_loss=0.002197493030424936
Epoch 2138
Epoch 2138 :: Batch 0/1
Batch Loss = 0.0021973860737506514
2138, epoch_train_loss=0.0021973860737506514
Epoch 2139
Epoch 2139 :: Batch 0/1
Batch Loss = 0.002197281746543024
2139, epoch_train_loss=0.002197281746543024
Epoch 2140
Epoch 2140 :: Batch 0/1
Batch Loss = 0.0021971811684721033
2140, epoch_train_loss=0.0021971811684721033
Epoch 2141
Epoch 2141 :: Batch 0/1
Batch Loss = 0.002197084199060835
2141, epoch_train_loss=0.002197084199060835
Epoch 2142
Epoch 2142 :: Batch 0/1
Batch Loss = 0.0021969894983082418
2142, epoch_train_loss=0.0021969894983082418
Epoch 2143
Epoch 2143 :: Batch 0/1
Batch Loss = 0.0021968954229087873
2143, epoch_train_loss=0.0021968954229087873
Epoch 2144
Epoch 2144 :: Batch 0/1
Batch Loss = 0.002196801398682927
2144, epoch_train_loss=0.002196801398682927
Epoch 2145
Epoch 2145 :: Batch 0/1
Batch Loss = 0.0021967071583370193
2145, epoch_train_loss=0.0021967071583370193
Epoch 2146
Epoch 2146 :: Batch 0/1
Batch Loss = 0.0021966130121878454
2146, epoch_train_loss=0.0021966130121878454
Epoch 2147
Epoch 2147 :: Batch 0/1
Batch Loss = 0.0021965188701228247
2147, epoch_train_loss=0.0021965188701228247
Epoch 2148
Epoch 2148 :: Batch 0/1
Batch Loss = 0.002196424454327754
2148, epoch_train_loss=0.002196424454327754
Epoch 2149
Epoch 2149 :: Batch 0/1
Batch Loss = 0.002196329476840674
2149, epoch_train_loss=0.002196329476840674
Epoch 2150
Epoch 2150 :: Batch 0/1
Batch Loss = 0.0021962337478682184
2150, epoch_train_loss=0.0021962337478682184
Epoch 2151
Epoch 2151 :: Batch 0/1
Batch Loss = 0.002196137363512178
2151, epoch_train_loss=0.002196137363512178
Epoch 2152
Epoch 2152 :: Batch 0/1
Batch Loss = 0.0021960407192570977
2152, epoch_train_loss=0.0021960407192570977
Epoch 2153
Epoch 2153 :: Batch 0/1
Batch Loss = 0.002195944209837121
2153, epoch_train_loss=0.002195944209837121
Epoch 2154
Epoch 2154 :: Batch 0/1
Batch Loss = 0.0021958483221475814
2154, epoch_train_loss=0.0021958483221475814
Epoch 2155
Epoch 2155 :: Batch 0/1
Batch Loss = 0.002195753090406084
2155, epoch_train_loss=0.002195753090406084
Epoch 2156
Epoch 2156 :: Batch 0/1
Batch Loss = 0.0021956584885537535
2156, epoch_train_loss=0.0021956584885537535
Epoch 2157
Epoch 2157 :: Batch 0/1
Batch Loss = 0.002195564273824214
2157, epoch_train_loss=0.002195564273824214
Epoch 2158
Epoch 2158 :: Batch 0/1
Batch Loss = 0.0021954703034755066
2158, epoch_train_loss=0.0021954703034755066
Epoch 2159
Epoch 2159 :: Batch 0/1
Batch Loss = 0.0021953765216719536
2159, epoch_train_loss=0.0021953765216719536
Epoch 2160
Epoch 2160 :: Batch 0/1
Batch Loss = 0.0021952829066307377
2160, epoch_train_loss=0.0021952829066307377
Epoch 2161
Epoch 2161 :: Batch 0/1
Batch Loss = 0.0021951895374377173
2161, epoch_train_loss=0.0021951895374377173
Epoch 2162
Epoch 2162 :: Batch 0/1
Batch Loss = 0.002195096440907315
2162, epoch_train_loss=0.002195096440907315
Epoch 2163
Epoch 2163 :: Batch 0/1
Batch Loss = 0.0021950036340588246
2163, epoch_train_loss=0.0021950036340588246
Epoch 2164
Epoch 2164 :: Batch 0/1
Batch Loss = 0.0021949110503166227
2164, epoch_train_loss=0.0021949110503166227
Epoch 2165
Epoch 2165 :: Batch 0/1
Batch Loss = 0.002194818605809532
2165, epoch_train_loss=0.002194818605809532
Epoch 2166
Epoch 2166 :: Batch 0/1
Batch Loss = 0.002194726228916317
2166, epoch_train_loss=0.002194726228916317
Epoch 2167
Epoch 2167 :: Batch 0/1
Batch Loss = 0.0021946339171783503
2167, epoch_train_loss=0.0021946339171783503
Epoch 2168
Epoch 2168 :: Batch 0/1
Batch Loss = 0.002194541662226504
2168, epoch_train_loss=0.002194541662226504
Epoch 2169
Epoch 2169 :: Batch 0/1
Batch Loss = 0.0021944495423123493
2169, epoch_train_loss=0.0021944495423123493
Epoch 2170
Epoch 2170 :: Batch 0/1
Batch Loss = 0.002194357561421009
2170, epoch_train_loss=0.002194357561421009
Epoch 2171
Epoch 2171 :: Batch 0/1
Batch Loss = 0.0021942658200443895
2171, epoch_train_loss=0.0021942658200443895
Epoch 2172
Epoch 2172 :: Batch 0/1
Batch Loss = 0.0021941742998957178
2172, epoch_train_loss=0.0021941742998957178
Epoch 2173
Epoch 2173 :: Batch 0/1
Batch Loss = 0.0021940830757645542
2173, epoch_train_loss=0.0021940830757645542
Epoch 2174
Epoch 2174 :: Batch 0/1
Batch Loss = 0.002193992123910844
2174, epoch_train_loss=0.002193992123910844
Epoch 2175
Epoch 2175 :: Batch 0/1
Batch Loss = 0.0021939015492523323
2175, epoch_train_loss=0.0021939015492523323
Epoch 2176
Epoch 2176 :: Batch 0/1
Batch Loss = 0.0021938113877706163
2176, epoch_train_loss=0.0021938113877706163
Epoch 2177
Epoch 2177 :: Batch 0/1
Batch Loss = 0.002193721837097862
2177, epoch_train_loss=0.002193721837097862
Epoch 2178
Epoch 2178 :: Batch 0/1
Batch Loss = 0.002193633021971957
2178, epoch_train_loss=0.002193633021971957
Epoch 2179
Epoch 2179 :: Batch 0/1
Batch Loss = 0.0021935453080888885
2179, epoch_train_loss=0.0021935453080888885
Epoch 2180
Epoch 2180 :: Batch 0/1
Batch Loss = 0.0021934589850423104
2180, epoch_train_loss=0.0021934589850423104
Epoch 2181
Epoch 2181 :: Batch 0/1
Batch Loss = 0.0021933747301168833
2181, epoch_train_loss=0.0021933747301168833
Epoch 2182
Epoch 2182 :: Batch 0/1
Batch Loss = 0.002193293182764167
2182, epoch_train_loss=0.002193293182764167
Epoch 2183
Epoch 2183 :: Batch 0/1
Batch Loss = 0.00219321567936573
2183, epoch_train_loss=0.00219321567936573
Epoch 2184
Epoch 2184 :: Batch 0/1
Batch Loss = 0.0021931436839319435
2184, epoch_train_loss=0.0021931436839319435
Epoch 2185
Epoch 2185 :: Batch 0/1
Batch Loss = 0.002193080011818201
2185, epoch_train_loss=0.002193080011818201
Epoch 2186
Epoch 2186 :: Batch 0/1
Batch Loss = 0.00219302802867765
2186, epoch_train_loss=0.00219302802867765
Epoch 2187
Epoch 2187 :: Batch 0/1
Batch Loss = 0.0021929939576411594
2187, epoch_train_loss=0.0021929939576411594
Epoch 2188
Epoch 2188 :: Batch 0/1
Batch Loss = 0.002192985610815614
2188, epoch_train_loss=0.002192985610815614
Epoch 2189
Epoch 2189 :: Batch 0/1
Batch Loss = 0.0021930173507695803
2189, epoch_train_loss=0.0021930173507695803
Epoch 2190
Epoch 2190 :: Batch 0/1
Batch Loss = 0.0021931075955515206
2190, epoch_train_loss=0.0021931075955515206
Epoch 2191
Epoch 2191 :: Batch 0/1
Batch Loss = 0.0021932908744070516
2191, epoch_train_loss=0.0021932908744070516
Epoch 2192
Epoch 2192 :: Batch 0/1
Batch Loss = 0.0021936114218505157
2192, epoch_train_loss=0.0021936114218505157
Epoch 2193
Epoch 2193 :: Batch 0/1
Batch Loss = 0.0021941557169101074
2193, epoch_train_loss=0.0021941557169101074
Epoch 2194
Epoch 2194 :: Batch 0/1
Batch Loss = 0.002195031775164604
2194, epoch_train_loss=0.002195031775164604
Epoch 2195
Epoch 2195 :: Batch 0/1
Batch Loss = 0.0021964653264011087
2195, epoch_train_loss=0.0021964653264011087
Epoch 2196
Epoch 2196 :: Batch 0/1
Batch Loss = 0.0021987229656529285
2196, epoch_train_loss=0.0021987229656529285
Epoch 2197
Epoch 2197 :: Batch 0/1
Batch Loss = 0.002202420011364557
2197, epoch_train_loss=0.002202420011364557
Epoch 2198
Epoch 2198 :: Batch 0/1
Batch Loss = 0.002208213591018973
2198, epoch_train_loss=0.002208213591018973
Epoch 2199
Epoch 2199 :: Batch 0/1
Batch Loss = 0.0022178612754308446
2199, epoch_train_loss=0.0022178612754308446
Epoch 2200
Epoch 2200 :: Batch 0/1
Batch Loss = 0.00223293967403213
2200, epoch_train_loss=0.00223293967403213
Epoch 2201
Epoch 2201 :: Batch 0/1
Batch Loss = 0.0022587314581660927
2201, epoch_train_loss=0.0022587314581660927
Epoch 2202
Epoch 2202 :: Batch 0/1
Batch Loss = 0.002298683862830616
2202, epoch_train_loss=0.002298683862830616
Epoch 2203
Epoch 2203 :: Batch 0/1
Batch Loss = 0.002369560242856893
2203, epoch_train_loss=0.002369560242856893
Epoch 2204
Epoch 2204 :: Batch 0/1
Batch Loss = 0.002476681140468739
2204, epoch_train_loss=0.002476681140468739
Epoch 2205
Epoch 2205 :: Batch 0/1
Batch Loss = 0.0026761121684201502
2205, epoch_train_loss=0.0026761121684201502
Epoch 2206
Epoch 2206 :: Batch 0/1
Batch Loss = 0.0029607757087138525
2206, epoch_train_loss=0.0029607757087138525
Epoch 2207
Epoch 2207 :: Batch 0/1
Batch Loss = 0.0035251899608768193
2207, epoch_train_loss=0.0035251899608768193
Epoch 2208
Epoch 2208 :: Batch 0/1
Batch Loss = 0.004233551654105118
2208, epoch_train_loss=0.004233551654105118
Epoch 2209
Epoch 2209 :: Batch 0/1
Batch Loss = 0.005752662636556825
2209, epoch_train_loss=0.005752662636556825
Epoch 2210
Epoch 2210 :: Batch 0/1
Batch Loss = 0.007145912326612722
2210, epoch_train_loss=0.007145912326612722
Epoch 2211
Epoch 2211 :: Batch 0/1
Batch Loss = 0.010438832236353928
2211, epoch_train_loss=0.010438832236353928
Epoch 2212
Epoch 2212 :: Batch 0/1
Batch Loss = 0.011420713910218424
2212, epoch_train_loss=0.011420713910218424
Epoch 2213
Epoch 2213 :: Batch 0/1
Batch Loss = 0.014784345710979217
2213, epoch_train_loss=0.014784345710979217
Epoch 2214
Epoch 2214 :: Batch 0/1
Batch Loss = 0.011716025755514548
2214, epoch_train_loss=0.011716025755514548
Epoch 2215
Epoch 2215 :: Batch 0/1
Batch Loss = 0.009706182615804877
2215, epoch_train_loss=0.009706182615804877
Epoch 2216
Epoch 2216 :: Batch 0/1
Batch Loss = 0.0055023669818458935
2216, epoch_train_loss=0.0055023669818458935
Epoch 2217
Epoch 2217 :: Batch 0/1
Batch Loss = 0.003307556565166974
2217, epoch_train_loss=0.003307556565166974
Epoch 2218
Epoch 2218 :: Batch 0/1
Batch Loss = 0.0035143159450240617
2218, epoch_train_loss=0.0035143159450240617
Epoch 2219
Epoch 2219 :: Batch 0/1
Batch Loss = 0.005026301575851021
2219, epoch_train_loss=0.005026301575851021
Epoch 2220
Epoch 2220 :: Batch 0/1
Batch Loss = 0.006983052719353317
2220, epoch_train_loss=0.006983052719353317
Epoch 2221
Epoch 2221 :: Batch 0/1
Batch Loss = 0.006391636449777403
2221, epoch_train_loss=0.006391636449777403
Epoch 2222
Epoch 2222 :: Batch 0/1
Batch Loss = 0.004977527755441166
2222, epoch_train_loss=0.004977527755441166
Epoch 2223
Epoch 2223 :: Batch 0/1
Batch Loss = 0.0031913307173652085
2223, epoch_train_loss=0.0031913307173652085
Epoch 2224
Epoch 2224 :: Batch 0/1
Batch Loss = 0.0029064675304309117
2224, epoch_train_loss=0.0029064675304309117
Epoch 2225
Epoch 2225 :: Batch 0/1
Batch Loss = 0.0039741417517218285
2225, epoch_train_loss=0.0039741417517218285
Epoch 2226
Epoch 2226 :: Batch 0/1
Batch Loss = 0.004682853496195892
2226, epoch_train_loss=0.004682853496195892
Epoch 2227
Epoch 2227 :: Batch 0/1
Batch Loss = 0.004718598626005357
2227, epoch_train_loss=0.004718598626005357
Epoch 2228
Epoch 2228 :: Batch 0/1
Batch Loss = 0.0035940606274897573
2228, epoch_train_loss=0.0035940606274897573
Epoch 2229
Epoch 2229 :: Batch 0/1
Batch Loss = 0.0026910774645680364
2229, epoch_train_loss=0.0026910774645680364
Epoch 2230
Epoch 2230 :: Batch 0/1
Batch Loss = 0.0026209074977429307
2230, epoch_train_loss=0.0026209074977429307
Epoch 2231
Epoch 2231 :: Batch 0/1
Batch Loss = 0.0030866973938353773
2231, epoch_train_loss=0.0030866973938353773
Epoch 2232
Epoch 2232 :: Batch 0/1
Batch Loss = 0.0034280442676350486
2232, epoch_train_loss=0.0034280442676350486
Epoch 2233
Epoch 2233 :: Batch 0/1
Batch Loss = 0.0031250582852864147
2233, epoch_train_loss=0.0031250582852864147
Epoch 2234
Epoch 2234 :: Batch 0/1
Batch Loss = 0.00269314658245864
2234, epoch_train_loss=0.00269314658245864
Epoch 2235
Epoch 2235 :: Batch 0/1
Batch Loss = 0.0024750608951190035
2235, epoch_train_loss=0.0024750608951190035
Epoch 2236
Epoch 2236 :: Batch 0/1
Batch Loss = 0.002573399319129164
2236, epoch_train_loss=0.002573399319129164
Epoch 2237
Epoch 2237 :: Batch 0/1
Batch Loss = 0.0027983650847270528
2237, epoch_train_loss=0.0027983650847270528
Epoch 2238
Epoch 2238 :: Batch 0/1
Batch Loss = 0.0028151682150484593
2238, epoch_train_loss=0.0028151682150484593
Epoch 2239
Epoch 2239 :: Batch 0/1
Batch Loss = 0.0026597981341174985
2239, epoch_train_loss=0.0026597981341174985
Epoch 2240
Epoch 2240 :: Batch 0/1
Batch Loss = 0.002456156125936517
2240, epoch_train_loss=0.002456156125936517
Epoch 2241
Epoch 2241 :: Batch 0/1
Batch Loss = 0.0023973742850174184
2241, epoch_train_loss=0.0023973742850174184
Epoch 2242
Epoch 2242 :: Batch 0/1
Batch Loss = 0.0024792074100111086
2242, epoch_train_loss=0.0024792074100111086
Epoch 2243
Epoch 2243 :: Batch 0/1
Batch Loss = 0.0025523675691824497
2243, epoch_train_loss=0.0025523675691824497
Epoch 2244
Epoch 2244 :: Batch 0/1
Batch Loss = 0.0025505874889173684
2244, epoch_train_loss=0.0025505874889173684
Epoch 2245
Epoch 2245 :: Batch 0/1
Batch Loss = 0.002451719443514234
2245, epoch_train_loss=0.002451719443514234
Epoch 2246
Epoch 2246 :: Batch 0/1
Batch Loss = 0.002358666138552432
2246, epoch_train_loss=0.002358666138552432
Epoch 2247
Epoch 2247 :: Batch 0/1
Batch Loss = 0.0023289698591246037
2247, epoch_train_loss=0.0023289698591246037
Epoch 2248
Epoch 2248 :: Batch 0/1
Batch Loss = 0.00234786008501174
2248, epoch_train_loss=0.00234786008501174
Epoch 2249
Epoch 2249 :: Batch 0/1
Batch Loss = 0.0023756517625574862
2249, epoch_train_loss=0.0023756517625574862
Epoch 2250
Epoch 2250 :: Batch 0/1
Batch Loss = 0.0023705207408766214
2250, epoch_train_loss=0.0023705207408766214
Epoch 2251
Epoch 2251 :: Batch 0/1
Batch Loss = 0.0023422963078148034
2251, epoch_train_loss=0.0023422963078148034
Epoch 2252
Epoch 2252 :: Batch 0/1
Batch Loss = 0.0023034352854437307
2252, epoch_train_loss=0.0023034352854437307
Epoch 2253
Epoch 2253 :: Batch 0/1
Batch Loss = 0.0022738611992247767
2253, epoch_train_loss=0.0022738611992247767
Epoch 2254
Epoch 2254 :: Batch 0/1
Batch Loss = 0.002270813510642783
2254, epoch_train_loss=0.002270813510642783
Epoch 2255
Epoch 2255 :: Batch 0/1
Batch Loss = 0.002285450660407528
2255, epoch_train_loss=0.002285450660407528
Epoch 2256
Epoch 2256 :: Batch 0/1
Batch Loss = 0.00229945426426632
2256, epoch_train_loss=0.00229945426426632
Epoch 2257
Epoch 2257 :: Batch 0/1
Batch Loss = 0.002291301431391656
2257, epoch_train_loss=0.002291301431391656
Epoch 2258
Epoch 2258 :: Batch 0/1
Batch Loss = 0.002263087786906759
2258, epoch_train_loss=0.002263087786906759
Epoch 2259
Epoch 2259 :: Batch 0/1
Batch Loss = 0.002235581113056196
2259, epoch_train_loss=0.002235581113056196
Epoch 2260
Epoch 2260 :: Batch 0/1
Batch Loss = 0.002226799987925973
2260, epoch_train_loss=0.002226799987925973
Epoch 2261
Epoch 2261 :: Batch 0/1
Batch Loss = 0.0022393240128042078
2261, epoch_train_loss=0.0022393240128042078
Epoch 2262
Epoch 2262 :: Batch 0/1
Batch Loss = 0.002255937007211264
2262, epoch_train_loss=0.002255937007211264
Epoch 2263
Epoch 2263 :: Batch 0/1
Batch Loss = 0.0022583725002570617
2263, epoch_train_loss=0.0022583725002570617
Epoch 2264
Epoch 2264 :: Batch 0/1
Batch Loss = 0.002241750454295055
2264, epoch_train_loss=0.002241750454295055
Epoch 2265
Epoch 2265 :: Batch 0/1
Batch Loss = 0.0022176081412014718
2265, epoch_train_loss=0.0022176081412014718
Epoch 2266
Epoch 2266 :: Batch 0/1
Batch Loss = 0.002203723064742624
2266, epoch_train_loss=0.002203723064742624
Epoch 2267
Epoch 2267 :: Batch 0/1
Batch Loss = 0.0022075149984051474
2267, epoch_train_loss=0.0022075149984051474
Epoch 2268
Epoch 2268 :: Batch 0/1
Batch Loss = 0.0022217226331311332
2268, epoch_train_loss=0.0022217226331311332
Epoch 2269
Epoch 2269 :: Batch 0/1
Batch Loss = 0.002231705191756288
2269, epoch_train_loss=0.002231705191756288
Epoch 2270
Epoch 2270 :: Batch 0/1
Batch Loss = 0.0022283783992104564
2270, epoch_train_loss=0.0022283783992104564
Epoch 2271
Epoch 2271 :: Batch 0/1
Batch Loss = 0.002214799542485862
2271, epoch_train_loss=0.002214799542485862
Epoch 2272
Epoch 2272 :: Batch 0/1
Batch Loss = 0.002201154115170006
2272, epoch_train_loss=0.002201154115170006
Epoch 2273
Epoch 2273 :: Batch 0/1
Batch Loss = 0.0021960039072011833
2273, epoch_train_loss=0.0021960039072011833
Epoch 2274
Epoch 2274 :: Batch 0/1
Batch Loss = 0.0022000626232355296
2274, epoch_train_loss=0.0022000626232355296
Epoch 2275
Epoch 2275 :: Batch 0/1
Batch Loss = 0.0022075665657715566
2275, epoch_train_loss=0.0022075665657715566
Epoch 2276
Epoch 2276 :: Batch 0/1
Batch Loss = 0.0022118894938006977
2276, epoch_train_loss=0.0022118894938006977
Epoch 2277
Epoch 2277 :: Batch 0/1
Batch Loss = 0.0022100989932451734
2277, epoch_train_loss=0.0022100989932451734
Epoch 2278
Epoch 2278 :: Batch 0/1
Batch Loss = 0.0022043032286677183
2278, epoch_train_loss=0.0022043032286677183
Epoch 2279
Epoch 2279 :: Batch 0/1
Batch Loss = 0.002198787929035721
2279, epoch_train_loss=0.002198787929035721
Epoch 2280
Epoch 2280 :: Batch 0/1
Batch Loss = 0.002196471015759867
2280, epoch_train_loss=0.002196471015759867
Epoch 2281
Epoch 2281 :: Batch 0/1
Batch Loss = 0.0021972730706960064
2281, epoch_train_loss=0.0021972730706960064
Epoch 2282
Epoch 2282 :: Batch 0/1
Batch Loss = 0.002198966751344162
2282, epoch_train_loss=0.002198966751344162
Epoch 2283
Epoch 2283 :: Batch 0/1
Batch Loss = 0.002199627029281628
2283, epoch_train_loss=0.002199627029281628
Epoch 2284
Epoch 2284 :: Batch 0/1
Batch Loss = 0.0021989139930408863
2284, epoch_train_loss=0.0021989139930408863
Epoch 2285
Epoch 2285 :: Batch 0/1
Batch Loss = 0.0021978023908936412
2285, epoch_train_loss=0.0021978023908936412
Epoch 2286
Epoch 2286 :: Batch 0/1
Batch Loss = 0.0021971945962335902
2286, epoch_train_loss=0.0021971945962335902
Epoch 2287
Epoch 2287 :: Batch 0/1
Batch Loss = 0.0021970734032497876
2287, epoch_train_loss=0.0021970734032497876
Epoch 2288
Epoch 2288 :: Batch 0/1
Batch Loss = 0.0021968872044204626
2288, epoch_train_loss=0.0021968872044204626
Epoch 2289
Epoch 2289 :: Batch 0/1
Batch Loss = 0.0021961127618964965
2289, epoch_train_loss=0.0021961127618964965
Epoch 2290
Epoch 2290 :: Batch 0/1
Batch Loss = 0.002194885601258761
2290, epoch_train_loss=0.002194885601258761
Epoch 2291
Epoch 2291 :: Batch 0/1
Batch Loss = 0.0021937710368651617
2291, epoch_train_loss=0.0021937710368651617
Epoch 2292
Epoch 2292 :: Batch 0/1
Batch Loss = 0.002193352758205899
2292, epoch_train_loss=0.002193352758205899
Epoch 2293
Epoch 2293 :: Batch 0/1
Batch Loss = 0.0021937583251394515
2293, epoch_train_loss=0.0021937583251394515
Epoch 2294
Epoch 2294 :: Batch 0/1
Batch Loss = 0.00219455638766458
2294, epoch_train_loss=0.00219455638766458
Epoch 2295
Epoch 2295 :: Batch 0/1
Batch Loss = 0.002195088917272061
2295, epoch_train_loss=0.002195088917272061
Epoch 2296
Epoch 2296 :: Batch 0/1
Batch Loss = 0.0021948902131839953
2296, epoch_train_loss=0.0021948902131839953
Epoch 2297
Epoch 2297 :: Batch 0/1
Batch Loss = 0.002194047926286668
2297, epoch_train_loss=0.002194047926286668
Epoch 2298
Epoch 2298 :: Batch 0/1
Batch Loss = 0.0021929959487951268
2298, epoch_train_loss=0.0021929959487951268
Epoch 2299
Epoch 2299 :: Batch 0/1
Batch Loss = 0.0021922253994104334
2299, epoch_train_loss=0.0021922253994104334
Epoch 2300
Epoch 2300 :: Batch 0/1
Batch Loss = 0.002191948032830521
2300, epoch_train_loss=0.002191948032830521
Epoch 2301
Epoch 2301 :: Batch 0/1
Batch Loss = 0.002192067714437992
2301, epoch_train_loss=0.002192067714437992
Epoch 2302
Epoch 2302 :: Batch 0/1
Batch Loss = 0.002192319756662847
2302, epoch_train_loss=0.002192319756662847
Epoch 2303
Epoch 2303 :: Batch 0/1
Batch Loss = 0.0021924541442382268
2303, epoch_train_loss=0.0021924541442382268
Epoch 2304
Epoch 2304 :: Batch 0/1
Batch Loss = 0.002192365767061524
2304, epoch_train_loss=0.002192365767061524
Epoch 2305
Epoch 2305 :: Batch 0/1
Batch Loss = 0.002192100132612793
2305, epoch_train_loss=0.002192100132612793
Epoch 2306
Epoch 2306 :: Batch 0/1
Batch Loss = 0.0021917984923503124
2306, epoch_train_loss=0.0021917984923503124
Epoch 2307
Epoch 2307 :: Batch 0/1
Batch Loss = 0.0021915663362146094
2307, epoch_train_loss=0.0021915663362146094
Epoch 2308
Epoch 2308 :: Batch 0/1
Batch Loss = 0.002191424333539684
2308, epoch_train_loss=0.002191424333539684
Epoch 2309
Epoch 2309 :: Batch 0/1
Batch Loss = 0.0021913119245755546
2309, epoch_train_loss=0.0021913119245755546
Epoch 2310
Epoch 2310 :: Batch 0/1
Batch Loss = 0.0021911635456547565
2310, epoch_train_loss=0.0021911635456547565
Epoch 2311
Epoch 2311 :: Batch 0/1
Batch Loss = 0.002190958088770582
2311, epoch_train_loss=0.002190958088770582
Epoch 2312
Epoch 2312 :: Batch 0/1
Batch Loss = 0.0021907235489423494
2312, epoch_train_loss=0.0021907235489423494
Epoch 2313
Epoch 2313 :: Batch 0/1
Batch Loss = 0.0021905119024171094
2313, epoch_train_loss=0.0021905119024171094
Epoch 2314
Epoch 2314 :: Batch 0/1
Batch Loss = 0.0021903658421261914
2314, epoch_train_loss=0.0021903658421261914
Epoch 2315
Epoch 2315 :: Batch 0/1
Batch Loss = 0.002190297653968022
2315, epoch_train_loss=0.002190297653968022
Epoch 2316
Epoch 2316 :: Batch 0/1
Batch Loss = 0.0021902810542459993
2316, epoch_train_loss=0.0021902810542459993
Epoch 2317
Epoch 2317 :: Batch 0/1
Batch Loss = 0.0021902667287011643
2317, epoch_train_loss=0.0021902667287011643
Epoch 2318
Epoch 2318 :: Batch 0/1
Batch Loss = 0.002190209937674605
2318, epoch_train_loss=0.002190209937674605
Epoch 2319
Epoch 2319 :: Batch 0/1
Batch Loss = 0.002190094864581936
2319, epoch_train_loss=0.002190094864581936
Epoch 2320
Epoch 2320 :: Batch 0/1
Batch Loss = 0.0021899354798918915
2320, epoch_train_loss=0.0021899354798918915
Epoch 2321
Epoch 2321 :: Batch 0/1
Batch Loss = 0.0021897608130776063
2321, epoch_train_loss=0.0021897608130776063
Epoch 2322
Epoch 2322 :: Batch 0/1
Batch Loss = 0.002189596653587073
2322, epoch_train_loss=0.002189596653587073
Epoch 2323
Epoch 2323 :: Batch 0/1
Batch Loss = 0.0021894576437121987
2323, epoch_train_loss=0.0021894576437121987
Epoch 2324
Epoch 2324 :: Batch 0/1
Batch Loss = 0.002189343616464671
2324, epoch_train_loss=0.002189343616464671
Epoch 2325
Epoch 2325 :: Batch 0/1
Batch Loss = 0.002189244287743698
2325, epoch_train_loss=0.002189244287743698
Epoch 2326
Epoch 2326 :: Batch 0/1
Batch Loss = 0.0021891450504565825
2326, epoch_train_loss=0.0021891450504565825
Epoch 2327
Epoch 2327 :: Batch 0/1
Batch Loss = 0.0021890371867043274
2327, epoch_train_loss=0.0021890371867043274
Epoch 2328
Epoch 2328 :: Batch 0/1
Batch Loss = 0.0021889208606400154
2328, epoch_train_loss=0.0021889208606400154
Epoch 2329
Epoch 2329 :: Batch 0/1
Batch Loss = 0.0021888031827920922
2329, epoch_train_loss=0.0021888031827920922
Epoch 2330
Epoch 2330 :: Batch 0/1
Batch Loss = 0.0021886915934798907
2330, epoch_train_loss=0.0021886915934798907
Epoch 2331
Epoch 2331 :: Batch 0/1
Batch Loss = 0.002188590367031519
2331, epoch_train_loss=0.002188590367031519
Epoch 2332
Epoch 2332 :: Batch 0/1
Batch Loss = 0.0021884997650470513
2332, epoch_train_loss=0.0021884997650470513
Epoch 2333
Epoch 2333 :: Batch 0/1
Batch Loss = 0.002188416385643001
2333, epoch_train_loss=0.002188416385643001
Epoch 2334
Epoch 2334 :: Batch 0/1
Batch Loss = 0.0021883349148737047
2334, epoch_train_loss=0.0021883349148737047
Epoch 2335
Epoch 2335 :: Batch 0/1
Batch Loss = 0.002188249677946889
2335, epoch_train_loss=0.002188249677946889
Epoch 2336
Epoch 2336 :: Batch 0/1
Batch Loss = 0.0021881582470312778
2336, epoch_train_loss=0.0021881582470312778
Epoch 2337
Epoch 2337 :: Batch 0/1
Batch Loss = 0.0021880609314059787
2337, epoch_train_loss=0.0021880609314059787
Epoch 2338
Epoch 2338 :: Batch 0/1
Batch Loss = 0.0021879608715557647
2338, epoch_train_loss=0.0021879608715557647
Epoch 2339
Epoch 2339 :: Batch 0/1
Batch Loss = 0.00218786069456436
2339, epoch_train_loss=0.00218786069456436
Epoch 2340
Epoch 2340 :: Batch 0/1
Batch Loss = 0.0021877631323296842
2340, epoch_train_loss=0.0021877631323296842
Epoch 2341
Epoch 2341 :: Batch 0/1
Batch Loss = 0.002187669180475039
2341, epoch_train_loss=0.002187669180475039
Epoch 2342
Epoch 2342 :: Batch 0/1
Batch Loss = 0.0021875793976851125
2342, epoch_train_loss=0.0021875793976851125
Epoch 2343
Epoch 2343 :: Batch 0/1
Batch Loss = 0.0021874925030011465
2343, epoch_train_loss=0.0021874925030011465
Epoch 2344
Epoch 2344 :: Batch 0/1
Batch Loss = 0.0021874075908261386
2344, epoch_train_loss=0.0021874075908261386
Epoch 2345
Epoch 2345 :: Batch 0/1
Batch Loss = 0.0021873236330553028
2345, epoch_train_loss=0.0021873236330553028
Epoch 2346
Epoch 2346 :: Batch 0/1
Batch Loss = 0.002187241371908866
2346, epoch_train_loss=0.002187241371908866
Epoch 2347
Epoch 2347 :: Batch 0/1
Batch Loss = 0.002187161576376485
2347, epoch_train_loss=0.002187161576376485
Epoch 2348
Epoch 2348 :: Batch 0/1
Batch Loss = 0.0021870866382150754
2348, epoch_train_loss=0.0021870866382150754
Epoch 2349
Epoch 2349 :: Batch 0/1
Batch Loss = 0.0021870185770220167
2349, epoch_train_loss=0.0021870185770220167
Epoch 2350
Epoch 2350 :: Batch 0/1
Batch Loss = 0.0021869615711717027
2350, epoch_train_loss=0.0021869615711717027
Epoch 2351
Epoch 2351 :: Batch 0/1
Batch Loss = 0.0021869188716406346
2351, epoch_train_loss=0.0021869188716406346
Epoch 2352
Epoch 2352 :: Batch 0/1
Batch Loss = 0.0021868970909298567
2352, epoch_train_loss=0.0021868970909298567
Epoch 2353
Epoch 2353 :: Batch 0/1
Batch Loss = 0.0021869018362378366
2353, epoch_train_loss=0.0021869018362378366
Epoch 2354
Epoch 2354 :: Batch 0/1
Batch Loss = 0.0021869460601699744
2354, epoch_train_loss=0.0021869460601699744
Epoch 2355
Epoch 2355 :: Batch 0/1
Batch Loss = 0.0021870420512977227
2355, epoch_train_loss=0.0021870420512977227
Epoch 2356
Epoch 2356 :: Batch 0/1
Batch Loss = 0.0021872176323456723
2356, epoch_train_loss=0.0021872176323456723
Epoch 2357
Epoch 2357 :: Batch 0/1
Batch Loss = 0.002187500061915885
2357, epoch_train_loss=0.002187500061915885
Epoch 2358
Epoch 2358 :: Batch 0/1
Batch Loss = 0.0021879515311370975
2358, epoch_train_loss=0.0021879515311370975
Epoch 2359
Epoch 2359 :: Batch 0/1
Batch Loss = 0.002188632953938065
2359, epoch_train_loss=0.002188632953938065
Epoch 2360
Epoch 2360 :: Batch 0/1
Batch Loss = 0.002189688430358836
2360, epoch_train_loss=0.002189688430358836
Epoch 2361
Epoch 2361 :: Batch 0/1
Batch Loss = 0.0021912543872473285
2361, epoch_train_loss=0.0021912543872473285
Epoch 2362
Epoch 2362 :: Batch 0/1
Batch Loss = 0.0021936797043879156
2362, epoch_train_loss=0.0021936797043879156
Epoch 2363
Epoch 2363 :: Batch 0/1
Batch Loss = 0.002197271538333683
2363, epoch_train_loss=0.002197271538333683
Epoch 2364
Epoch 2364 :: Batch 0/1
Batch Loss = 0.0022029165214088645
2364, epoch_train_loss=0.0022029165214088645
Epoch 2365
Epoch 2365 :: Batch 0/1
Batch Loss = 0.0022112962297159105
2365, epoch_train_loss=0.0022112962297159105
Epoch 2366
Epoch 2366 :: Batch 0/1
Batch Loss = 0.002224779117349481
2366, epoch_train_loss=0.002224779117349481
Epoch 2367
Epoch 2367 :: Batch 0/1
Batch Loss = 0.0022447917603537247
2367, epoch_train_loss=0.0022447917603537247
Epoch 2368
Epoch 2368 :: Batch 0/1
Batch Loss = 0.002277998516775213
2368, epoch_train_loss=0.002277998516775213
Epoch 2369
Epoch 2369 :: Batch 0/1
Batch Loss = 0.0023268686421581775
2369, epoch_train_loss=0.0023268686421581775
Epoch 2370
Epoch 2370 :: Batch 0/1
Batch Loss = 0.0024111669731246028
2370, epoch_train_loss=0.0024111669731246028
Epoch 2371
Epoch 2371 :: Batch 0/1
Batch Loss = 0.002532114790362064
2371, epoch_train_loss=0.002532114790362064
Epoch 2372
Epoch 2372 :: Batch 0/1
Batch Loss = 0.0027511583954941674
2372, epoch_train_loss=0.0027511583954941674
Epoch 2373
Epoch 2373 :: Batch 0/1
Batch Loss = 0.0030475280907597017
2373, epoch_train_loss=0.0030475280907597017
Epoch 2374
Epoch 2374 :: Batch 0/1
Batch Loss = 0.003617492409647974
2374, epoch_train_loss=0.003617492409647974
Epoch 2375
Epoch 2375 :: Batch 0/1
Batch Loss = 0.0042949367603751926
2375, epoch_train_loss=0.0042949367603751926
Epoch 2376
Epoch 2376 :: Batch 0/1
Batch Loss = 0.005692262458137181
2376, epoch_train_loss=0.005692262458137181
Epoch 2377
Epoch 2377 :: Batch 0/1
Batch Loss = 0.006914074759207488
2377, epoch_train_loss=0.006914074759207488
Epoch 2378
Epoch 2378 :: Batch 0/1
Batch Loss = 0.009645030519086911
2378, epoch_train_loss=0.009645030519086911
Epoch 2379
Epoch 2379 :: Batch 0/1
Batch Loss = 0.010484186629805775
2379, epoch_train_loss=0.010484186629805775
Epoch 2380
Epoch 2380 :: Batch 0/1
Batch Loss = 0.012993941746313476
2380, epoch_train_loss=0.012993941746313476
Epoch 2381
Epoch 2381 :: Batch 0/1
Batch Loss = 0.010729243462181912
2381, epoch_train_loss=0.010729243462181912
Epoch 2382
Epoch 2382 :: Batch 0/1
Batch Loss = 0.00892619723389296
2382, epoch_train_loss=0.00892619723389296
Epoch 2383
Epoch 2383 :: Batch 0/1
Batch Loss = 0.0051260750447503865
2383, epoch_train_loss=0.0051260750447503865
Epoch 2384
Epoch 2384 :: Batch 0/1
Batch Loss = 0.002793473919288769
2384, epoch_train_loss=0.002793473919288769
Epoch 2385
Epoch 2385 :: Batch 0/1
Batch Loss = 0.002614838407671342
2385, epoch_train_loss=0.002614838407671342
Epoch 2386
Epoch 2386 :: Batch 0/1
Batch Loss = 0.00407788875007098
2386, epoch_train_loss=0.00407788875007098
Epoch 2387
Epoch 2387 :: Batch 0/1
Batch Loss = 0.005985553525858055
2387, epoch_train_loss=0.005985553525858055
Epoch 2388
Epoch 2388 :: Batch 0/1
Batch Loss = 0.005945049842225218
2388, epoch_train_loss=0.005945049842225218
Epoch 2389
Epoch 2389 :: Batch 0/1
Batch Loss = 0.004839488713407012
2389, epoch_train_loss=0.004839488713407012
Epoch 2390
Epoch 2390 :: Batch 0/1
Batch Loss = 0.003008618566874659
2390, epoch_train_loss=0.003008618566874659
Epoch 2391
Epoch 2391 :: Batch 0/1
Batch Loss = 0.0023123556891637173
2391, epoch_train_loss=0.0023123556891637173
Epoch 2392
Epoch 2392 :: Batch 0/1
Batch Loss = 0.0029863014416010793
2392, epoch_train_loss=0.0029863014416010793
Epoch 2393
Epoch 2393 :: Batch 0/1
Batch Loss = 0.003992195658799285
2393, epoch_train_loss=0.003992195658799285
Epoch 2394
Epoch 2394 :: Batch 0/1
Batch Loss = 0.0045398474482653516
2394, epoch_train_loss=0.0045398474482653516
Epoch 2395
Epoch 2395 :: Batch 0/1
Batch Loss = 0.0038708306336221166
2395, epoch_train_loss=0.0038708306336221166
Epoch 2396
Epoch 2396 :: Batch 0/1
Batch Loss = 0.002917269976077948
2396, epoch_train_loss=0.002917269976077948
Epoch 2397
Epoch 2397 :: Batch 0/1
Batch Loss = 0.002299811033359539
2397, epoch_train_loss=0.002299811033359539
Epoch 2398
Epoch 2398 :: Batch 0/1
Batch Loss = 0.002425641867985307
2398, epoch_train_loss=0.002425641867985307
Epoch 2399
Epoch 2399 :: Batch 0/1
Batch Loss = 0.002972514763597339
2399, epoch_train_loss=0.002972514763597339
Epoch 2400
Epoch 2400 :: Batch 0/1
Batch Loss = 0.003268482165007875
2400, epoch_train_loss=0.003268482165007875
Epoch 2401
Epoch 2401 :: Batch 0/1
Batch Loss = 0.0031510183030140724
2401, epoch_train_loss=0.0031510183030140724
Epoch 2402
Epoch 2402 :: Batch 0/1
Batch Loss = 0.0026645897428944727
2402, epoch_train_loss=0.0026645897428944727
Epoch 2403
Epoch 2403 :: Batch 0/1
Batch Loss = 0.002291559952395561
2403, epoch_train_loss=0.002291559952395561
Epoch 2404
Epoch 2404 :: Batch 0/1
Batch Loss = 0.0022695545948493525
2404, epoch_train_loss=0.0022695545948493525
Epoch 2405
Epoch 2405 :: Batch 0/1
Batch Loss = 0.0025180252025112284
2405, epoch_train_loss=0.0025180252025112284
Epoch 2406
Epoch 2406 :: Batch 0/1
Batch Loss = 0.0027524995700082913
2406, epoch_train_loss=0.0027524995700082913
Epoch 2407
Epoch 2407 :: Batch 0/1
Batch Loss = 0.002713219848062095
2407, epoch_train_loss=0.002713219848062095
Epoch 2408
Epoch 2408 :: Batch 0/1
Batch Loss = 0.00249164806301354
2408, epoch_train_loss=0.00249164806301354
Epoch 2409
Epoch 2409 :: Batch 0/1
Batch Loss = 0.002271480999799016
2409, epoch_train_loss=0.002271480999799016
Epoch 2410
Epoch 2410 :: Batch 0/1
Batch Loss = 0.0022292579868399658
2410, epoch_train_loss=0.0022292579868399658
Epoch 2411
Epoch 2411 :: Batch 0/1
Batch Loss = 0.0023465944879727313
2411, epoch_train_loss=0.0023465944879727313
Epoch 2412
Epoch 2412 :: Batch 0/1
Batch Loss = 0.002476300744767311
2412, epoch_train_loss=0.002476300744767311
Epoch 2413
Epoch 2413 :: Batch 0/1
Batch Loss = 0.0025056374116862084
2413, epoch_train_loss=0.0025056374116862084
Epoch 2414
Epoch 2414 :: Batch 0/1
Batch Loss = 0.002402526415949448
2414, epoch_train_loss=0.002402526415949448
Epoch 2415
Epoch 2415 :: Batch 0/1
Batch Loss = 0.002275676614025621
2415, epoch_train_loss=0.002275676614025621
Epoch 2416
Epoch 2416 :: Batch 0/1
Batch Loss = 0.0022153538000291126
2416, epoch_train_loss=0.0022153538000291126
Epoch 2417
Epoch 2417 :: Batch 0/1
Batch Loss = 0.0022499202498010447
2417, epoch_train_loss=0.0022499202498010447
Epoch 2418
Epoch 2418 :: Batch 0/1
Batch Loss = 0.0023251086971556325
2418, epoch_train_loss=0.0023251086971556325
Epoch 2419
Epoch 2419 :: Batch 0/1
Batch Loss = 0.0023630174185902456
2419, epoch_train_loss=0.0023630174185902456
Epoch 2420
Epoch 2420 :: Batch 0/1
Batch Loss = 0.002340090316630095
2420, epoch_train_loss=0.002340090316630095
Epoch 2421
Epoch 2421 :: Batch 0/1
Batch Loss = 0.0022740017358415126
2421, epoch_train_loss=0.0022740017358415126
Epoch 2422
Epoch 2422 :: Batch 0/1
Batch Loss = 0.0022216255023741046
2422, epoch_train_loss=0.0022216255023741046
Epoch 2423
Epoch 2423 :: Batch 0/1
Batch Loss = 0.0022105428254812227
2423, epoch_train_loss=0.0022105428254812227
Epoch 2424
Epoch 2424 :: Batch 0/1
Batch Loss = 0.0022350633634600383
2424, epoch_train_loss=0.0022350633634600383
Epoch 2425
Epoch 2425 :: Batch 0/1
Batch Loss = 0.0022665944377491362
2425, epoch_train_loss=0.0022665944377491362
Epoch 2426
Epoch 2426 :: Batch 0/1
Batch Loss = 0.002277636043207269
2426, epoch_train_loss=0.002277636043207269
Epoch 2427
Epoch 2427 :: Batch 0/1
Batch Loss = 0.0022644878921637083
2427, epoch_train_loss=0.0022644878921637083
Epoch 2428
Epoch 2428 :: Batch 0/1
Batch Loss = 0.002234733343301559
2428, epoch_train_loss=0.002234733343301559
Epoch 2429
Epoch 2429 :: Batch 0/1
Batch Loss = 0.0022093203671426274
2429, epoch_train_loss=0.0022093203671426274
Epoch 2430
Epoch 2430 :: Batch 0/1
Batch Loss = 0.002199436005707309
2430, epoch_train_loss=0.002199436005707309
Epoch 2431
Epoch 2431 :: Batch 0/1
Batch Loss = 0.002205944420949225
2431, epoch_train_loss=0.002205944420949225
Epoch 2432
Epoch 2432 :: Batch 0/1
Batch Loss = 0.0022199436169244363
2432, epoch_train_loss=0.0022199436169244363
Epoch 2433
Epoch 2433 :: Batch 0/1
Batch Loss = 0.0022299789729272965
2433, epoch_train_loss=0.0022299789729272965
Epoch 2434
Epoch 2434 :: Batch 0/1
Batch Loss = 0.0022308810240568764
2434, epoch_train_loss=0.0022308810240568764
Epoch 2435
Epoch 2435 :: Batch 0/1
Batch Loss = 0.0022214476942349435
2435, epoch_train_loss=0.0022214476942349435
Epoch 2436
Epoch 2436 :: Batch 0/1
Batch Loss = 0.0022082724867474863
2436, epoch_train_loss=0.0022082724867474863
Epoch 2437
Epoch 2437 :: Batch 0/1
Batch Loss = 0.002196639981130038
2437, epoch_train_loss=0.002196639981130038
Epoch 2438
Epoch 2438 :: Batch 0/1
Batch Loss = 0.0021912467252677903
2438, epoch_train_loss=0.0021912467252677903
Epoch 2439
Epoch 2439 :: Batch 0/1
Batch Loss = 0.0021927089881815806
2439, epoch_train_loss=0.0021927089881815806
Epoch 2440
Epoch 2440 :: Batch 0/1
Batch Loss = 0.0021984059072552705
2440, epoch_train_loss=0.0021984059072552705
Epoch 2441
Epoch 2441 :: Batch 0/1
Batch Loss = 0.0022044499209379785
2441, epoch_train_loss=0.0022044499209379785
Epoch 2442
Epoch 2442 :: Batch 0/1
Batch Loss = 0.002207272216163819
2442, epoch_train_loss=0.002207272216163819
Epoch 2443
Epoch 2443 :: Batch 0/1
Batch Loss = 0.0022062443097655357
2443, epoch_train_loss=0.0022062443097655357
Epoch 2444
Epoch 2444 :: Batch 0/1
Batch Loss = 0.002201630036928651
2444, epoch_train_loss=0.002201630036928651
Epoch 2445
Epoch 2445 :: Batch 0/1
Batch Loss = 0.002195910927032533
2445, epoch_train_loss=0.002195910927032533
Epoch 2446
Epoch 2446 :: Batch 0/1
Batch Loss = 0.002190931575966225
2446, epoch_train_loss=0.002190931575966225
Epoch 2447
Epoch 2447 :: Batch 0/1
Batch Loss = 0.0021882266344382625
2447, epoch_train_loss=0.0021882266344382625
Epoch 2448
Epoch 2448 :: Batch 0/1
Batch Loss = 0.002188039484991145
2448, epoch_train_loss=0.002188039484991145
Epoch 2449
Epoch 2449 :: Batch 0/1
Batch Loss = 0.0021896476465717574
2449, epoch_train_loss=0.0021896476465717574
Epoch 2450
Epoch 2450 :: Batch 0/1
Batch Loss = 0.002191900436033977
2450, epoch_train_loss=0.002191900436033977
Epoch 2451
Epoch 2451 :: Batch 0/1
Batch Loss = 0.0021937141866330673
2451, epoch_train_loss=0.0021937141866330673
Epoch 2452
Epoch 2452 :: Batch 0/1
Batch Loss = 0.0021945819367740066
2452, epoch_train_loss=0.0021945819367740066
Epoch 2453
Epoch 2453 :: Batch 0/1
Batch Loss = 0.0021942200931743417
2453, epoch_train_loss=0.0021942200931743417
Epoch 2454
Epoch 2454 :: Batch 0/1
Batch Loss = 0.0021929812333351083
2454, epoch_train_loss=0.0021929812333351083
Epoch 2455
Epoch 2455 :: Batch 0/1
Batch Loss = 0.0021911671036817515
2455, epoch_train_loss=0.0021911671036817515
Epoch 2456
Epoch 2456 :: Batch 0/1
Batch Loss = 0.00218933338853974
2456, epoch_train_loss=0.00218933338853974
Epoch 2457
Epoch 2457 :: Batch 0/1
Batch Loss = 0.002187808523686855
2457, epoch_train_loss=0.002187808523686855
Epoch 2458
Epoch 2458 :: Batch 0/1
Batch Loss = 0.002186821148953244
2458, epoch_train_loss=0.002186821148953244
Epoch 2459
Epoch 2459 :: Batch 0/1
Batch Loss = 0.002186424023253646
2459, epoch_train_loss=0.002186424023253646
Epoch 2460
Epoch 2460 :: Batch 0/1
Batch Loss = 0.002186545870607351
2460, epoch_train_loss=0.002186545870607351
Epoch 2461
Epoch 2461 :: Batch 0/1
Batch Loss = 0.0021870110902661513
2461, epoch_train_loss=0.0021870110902661513
Epoch 2462
Epoch 2462 :: Batch 0/1
Batch Loss = 0.0021875836334125285
2462, epoch_train_loss=0.0021875836334125285
Epoch 2463
Epoch 2463 :: Batch 0/1
Batch Loss = 0.0021880607120678642
2463, epoch_train_loss=0.0021880607120678642
Epoch 2464
Epoch 2464 :: Batch 0/1
Batch Loss = 0.0021882916561393392
2464, epoch_train_loss=0.0021882916561393392
Epoch 2465
Epoch 2465 :: Batch 0/1
Batch Loss = 0.0021882531580045147
2465, epoch_train_loss=0.0021882531580045147
Epoch 2466
Epoch 2466 :: Batch 0/1
Batch Loss = 0.002187943910635144
2466, epoch_train_loss=0.002187943910635144
Epoch 2467
Epoch 2467 :: Batch 0/1
Batch Loss = 0.002187460264986688
2467, epoch_train_loss=0.002187460264986688
Epoch 2468
Epoch 2468 :: Batch 0/1
Batch Loss = 0.002186877358186437
2468, epoch_train_loss=0.002186877358186437
Epoch 2469
Epoch 2469 :: Batch 0/1
Batch Loss = 0.0021863043185826898
2469, epoch_train_loss=0.0021863043185826898
Epoch 2470
Epoch 2470 :: Batch 0/1
Batch Loss = 0.0021857935277135275
2470, epoch_train_loss=0.0021857935277135275
Epoch 2471
Epoch 2471 :: Batch 0/1
Batch Loss = 0.002185381633651323
2471, epoch_train_loss=0.002185381633651323
Epoch 2472
Epoch 2472 :: Batch 0/1
Batch Loss = 0.002185074338728618
2472, epoch_train_loss=0.002185074338728618
Epoch 2473
Epoch 2473 :: Batch 0/1
Batch Loss = 0.0021848704096453417
2473, epoch_train_loss=0.0021848704096453417
Epoch 2474
Epoch 2474 :: Batch 0/1
Batch Loss = 0.0021847564418988564
2474, epoch_train_loss=0.0021847564418988564
Epoch 2475
Epoch 2475 :: Batch 0/1
Batch Loss = 0.002184710304705701
2475, epoch_train_loss=0.002184710304705701
Epoch 2476
Epoch 2476 :: Batch 0/1
Batch Loss = 0.002184707293292144
2476, epoch_train_loss=0.002184707293292144
Epoch 2477
Epoch 2477 :: Batch 0/1
Batch Loss = 0.002184724740979214
2477, epoch_train_loss=0.002184724740979214
Epoch 2478
Epoch 2478 :: Batch 0/1
Batch Loss = 0.0021847449355649855
2478, epoch_train_loss=0.0021847449355649855
Epoch 2479
Epoch 2479 :: Batch 0/1
Batch Loss = 0.0021847489697592714
2479, epoch_train_loss=0.0021847489697592714
Epoch 2480
Epoch 2480 :: Batch 0/1
Batch Loss = 0.0021847283950775336
2480, epoch_train_loss=0.0021847283950775336
Epoch 2481
Epoch 2481 :: Batch 0/1
Batch Loss = 0.0021846757294838324
2481, epoch_train_loss=0.0021846757294838324
Epoch 2482
Epoch 2482 :: Batch 0/1
Batch Loss = 0.0021845986539170024
2482, epoch_train_loss=0.0021845986539170024
Epoch 2483
Epoch 2483 :: Batch 0/1
Batch Loss = 0.0021844974342704717
2483, epoch_train_loss=0.0021844974342704717
Epoch 2484
Epoch 2484 :: Batch 0/1
Batch Loss = 0.002184382539891264
2484, epoch_train_loss=0.002184382539891264
Epoch 2485
Epoch 2485 :: Batch 0/1
Batch Loss = 0.0021842546292252475
2485, epoch_train_loss=0.0021842546292252475
Epoch 2486
Epoch 2486 :: Batch 0/1
Batch Loss = 0.002184123965593654
2486, epoch_train_loss=0.002184123965593654
Epoch 2487
Epoch 2487 :: Batch 0/1
Batch Loss = 0.0021839899818300655
2487, epoch_train_loss=0.0021839899818300655
Epoch 2488
Epoch 2488 :: Batch 0/1
Batch Loss = 0.0021838584647513658
2488, epoch_train_loss=0.0021838584647513658
Epoch 2489
Epoch 2489 :: Batch 0/1
Batch Loss = 0.002183727052009957
2489, epoch_train_loss=0.002183727052009957
Epoch 2490
Epoch 2490 :: Batch 0/1
Batch Loss = 0.0021836010774378936
2490, epoch_train_loss=0.0021836010774378936
Epoch 2491
Epoch 2491 :: Batch 0/1
Batch Loss = 0.00218347953253365
2491, epoch_train_loss=0.00218347953253365
Epoch 2492
Epoch 2492 :: Batch 0/1
Batch Loss = 0.0021833668346548257
2492, epoch_train_loss=0.0021833668346548257
Epoch 2493
Epoch 2493 :: Batch 0/1
Batch Loss = 0.0021832612601474484
2493, epoch_train_loss=0.0021832612601474484
Epoch 2494
Epoch 2494 :: Batch 0/1
Batch Loss = 0.0021831670348321877
2494, epoch_train_loss=0.0021831670348321877
Epoch 2495
Epoch 2495 :: Batch 0/1
Batch Loss = 0.0021830829377655664
2495, epoch_train_loss=0.0021830829377655664
Epoch 2496
Epoch 2496 :: Batch 0/1
Batch Loss = 0.002183013372100551
2496, epoch_train_loss=0.002183013372100551
Epoch 2497
Epoch 2497 :: Batch 0/1
Batch Loss = 0.0021829568773824404
2497, epoch_train_loss=0.0021829568773824404
Epoch 2498
Epoch 2498 :: Batch 0/1
Batch Loss = 0.0021829197328107988
2498, epoch_train_loss=0.0021829197328107988
Epoch 2499
Epoch 2499 :: Batch 0/1
Batch Loss = 0.0021829025096306498
2499, epoch_train_loss=0.0021829025096306498
