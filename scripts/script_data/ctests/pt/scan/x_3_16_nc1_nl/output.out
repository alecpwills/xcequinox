/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting "B3LYP_WITH_VWN5 = True" in pyscf_conf.py
  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '
Warning: <pyscf.gto.mole.Mole object at 0x7ffec40a4a30> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec40a4a30> in UKS object of <class 'pyscf.dft.uks.UKS'>
WARNING: External module "mldftdat" required for non-local descriptor use.
<pyscf.gto.mole.Mole object at 0x7ffec40a4a30> [['P', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec42f9c90> [['N', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec42f9ea0> [['H', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec42fb7f0> [['Li', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec42fb580> [['O', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec42f8610> [['Cl', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec42f8e50> [['Al', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec42fb3a0> [['S', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec42fa380> [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec42fb6d0> [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec42f9870> [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec42f8730> [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]] 3
<pyscf.gto.mole.Mole object at 0x7ffec42f9480> [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]] 3
<pyscf.gto.mole.Mole object at 0x7ffec42fb250> [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec42d3490> [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec42d0c70> [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec42d2620> [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]] 4
<pyscf.gto.mole.Mole object at 0x7ffec42d3400> [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec42d31c0> [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec42d1240> [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec42d0700> [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec42d0ac0> [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]] 4
<pyscf.gto.mole.Mole object at 0x7ffec42d09d0> [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]] 4
<pyscf.gto.mole.Mole object at 0x7ffec42d13c0> [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec42d24d0> [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]] 5
<pyscf.gto.mole.Mole object at 0x7ffec42d13f0> [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]] 3
<pyscf.gto.mole.Mole object at 0x7ffec42d14e0> [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]] 5
mol:  [['P', array([0., 0., 0.])]]
converged SCF energy = -341.104145992717  <S^2> = 3.7502984  2S+1 = 4.0001492
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42f9c90> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42f9c90> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-3.47389956e-03 -8.82676818e-04 -2.08411238e-03 ... -1.11301603e+01
 -1.11301603e+01 -1.11301603e+01] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 16)
mol:  [['N', array([0., 0., 0.])]]
converged SCF energy = -54.5289742046675  <S^2> = 3.7524945  2S+1 = 4.0012471
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42f9ea0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42f9ea0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5016, 22)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.07670570e-03 -5.92340671e-04 -6.66573372e-05 ... -5.03679786e+00
 -5.03679786e+00 -5.03679786e+00] = SCAN,
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5016, 22)
rho shape (4, 4, 2, 5016)
rho_filt shape: (5016,)
get descriptors tdrho.shape=(5016, 16)
mol:  [['H', array([0., 0., 0.])]]
converged SCF energy = -0.499812984008539  <S^2> = 0.75  2S+1 = 2
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42fb7f0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42fb7f0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 7, 7)
ao.shape (10, 2440, 7)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.59173730e-03 -7.67300285e-04 -6.09330303e-05 ... -7.27736608e-01
 -7.27736608e-01 -7.27736608e-01] = SCAN,
get_data, dm shape = (2, 7, 7)
ao_eval.shape=(4, 2440, 7)
rho shape (4, 4, 2, 2440)
rho_filt shape: (2440,)
get descriptors tdrho.shape=(2440, 16)
mol:  [['Li', array([0., 0., 0.])]]
converged SCF energy = -7.46006188627841  <S^2> = 0.75000049  2S+1 = 2.0000005
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42fb580> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42fb580> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 4592, 22)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-6.71503005e-03 -1.44519923e-03 -1.44519923e-03 ... -1.46899070e-02
 -2.03947707e+00 -2.03947707e+00] = SCAN,
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 4592, 22)
rho shape (4, 4, 2, 4592)
rho_filt shape: (4592,)
get descriptors tdrho.shape=(4592, 16)
mol:  [['O', array([0., 0., 0.])]]
converged SCF energy = -75.0033802925613  <S^2> = 2.0027445  2S+1 = 3.0018291
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42f8610> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42f8610> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5040, 22)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-6.33488417e-04 -1.08917461e-04 -5.45248598e-06 ... -5.78449347e+00
 -5.78449347e+00 -5.78449347e+00] = SCAN,
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5040, 22)
rho shape (4, 4, 2, 5040)
rho_filt shape: (5040,)
get descriptors tdrho.shape=(5040, 16)
mol:  [['Cl', array([0., 0., 0.])]]
converged SCF energy = -459.957577121955  <S^2> = 0.75161941  2S+1 = 2.0016188
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42f8e50> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42f8e50> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6152, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-9.59197305e-04 -9.57081192e-04 -3.30728589e-04 ... -1.26648275e+01
 -1.26648275e+01 -1.26648275e+01] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6152, 30)
rho shape (4, 4, 2, 6152)
rho_filt shape: (6152,)
get descriptors tdrho.shape=(6152, 16)
mol:  [['Al', array([0., 0., 0.])]]
converged SCF energy = -242.226560989243  <S^2> = 0.75226414  2S+1 = 2.0022629
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42fb3a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42fb3a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6088, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.33118550e-02 -8.49068703e-03 -4.25301188e-03 ... -1.37659916e-04
 -1.02991814e-03 -7.41975204e-05] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6088, 30)
rho shape (4, 4, 2, 6088)
rho_filt shape: (6088,)
get descriptors tdrho.shape=(6088, 16)
mol:  [['S', array([0., 0., 0.])]]
converged SCF energy = -397.938786830939  <S^2> = 2.0022329  2S+1 = 3.0014882
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42fa380> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42fa380> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.68269746e-03 -7.17875172e-04 -9.02884636e-04 ... -1.18986567e+01
 -1.18986567e+01 -1.18986567e+01] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 16)
mol:  [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]]
converged SCF energy = -1.16580491182912  <S^2> = 4.4408921e-16  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42fb6d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42fb6d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 14, 14)
ao.shape (10, 4776, 14)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.31557088e-04 -9.73828620e-06 -3.66768667e-04 ... -5.54165574e-01
 -5.54165574e-01 -5.54165574e-01] = SCAN,
get_data, dm shape = (2, 14, 14)
ao_eval.shape=(4, 4776, 14)
rho shape (4, 4, 2, 4776)
rho_filt shape: (4776,)
get descriptors tdrho.shape=(4776, 16)
mol:  [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]]
converged SCF energy = -109.439263799566  <S^2> = 1.4210855e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42f9870> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42f9870> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9848, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-9.68474977e-05 -9.84742592e-04 -2.59676393e-04 ... -2.39645778e-05
 -2.39645778e-05 -9.68474977e-05] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9848, 44)
rho shape (4, 4, 2, 9848)
rho_filt shape: (9848,)
get descriptors tdrho.shape=(9848, 16)
mol:  [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]]
converged SCF energy = -107.339357395734  <S^2> = 1.7763568e-15  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42f8730> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42f8730> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9752, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.04987770e-03 -6.68954111e-04 -8.57556562e-04 ... -1.07485605e-03
 -8.01425702e-01 -8.01425702e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9752, 44)
rho shape (4, 4, 2, 9752)
rho_filt shape: (9752,)
get descriptors tdrho.shape=(9752, 16)
mol:  [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]]
converged SCF energy = -93.3377924465131  <S^2> = 4.0073189e-10  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42f9480> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42f9480> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 12256, 51)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.97917639e-04 -2.54437615e-05 -3.15202008e-05 ... -6.37386388e-01
 -6.37386388e-01 -6.37386388e-01] = SCAN,
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 12256, 51)
rho shape (4, 4, 2, 12256)
rho_filt shape: (12256,)
get descriptors tdrho.shape=(12256, 16)
mol:  [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]]
converged SCF energy = -188.456965322843  <S^2> = 1.7763568e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42fb250> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42fb250> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 66, 66)
ao.shape (10, 14920, 66)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.50217343e-04 -2.07520331e-04 -9.23619961e-04 ... -2.76182455e-06
 -4.27559894e+00 -4.27559894e+00] = SCAN,
get_data, dm shape = (2, 66, 66)
ao_eval.shape=(4, 14920, 66)
rho shape (4, 4, 2, 14920)
rho_filt shape: (14920,)
get descriptors tdrho.shape=(14920, 16)
mol:  [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]]
converged SCF energy = -920.00560888896  <S^2> = 5.0093263e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42d3490> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42d3490> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12208, 60)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-0.00155834 -0.00091688 -0.00215831 ... -0.00091688 -0.41618506
 -0.41618506] = SCAN,
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12208, 60)
rho shape (4, 4, 2, 12208)
rho_filt shape: (12208,)
get descriptors tdrho.shape=(12208, 16)
mol:  [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]]
converged SCF energy = -199.394370591172  <S^2> = 1.1723955e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42d0c70> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42d0c70> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9824, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.92948752e-04 -1.95215890e-05 -1.16699780e-03 ... -4.89378326e-01
 -4.89378326e-01 -4.89378326e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9824, 44)
rho shape (4, 4, 2, 9824)
rho_filt shape: (9824,)
get descriptors tdrho.shape=(9824, 16)
mol:  [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]]
converged SCF energy = -150.214894435364  <S^2> = 1.0018598  2S+1 = 2.2377308
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42d2620> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42d2620> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9912, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-7.75399118e-04 -1.38913432e-04 -7.19465430e-06 ... -6.59150673e-01
 -6.59150673e-01 -6.59150673e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9912, 44)
rho shape (4, 4, 2, 9912)
rho_filt shape: (9912,)
get descriptors tdrho.shape=(9912, 16)
mol:  [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]]
converged SCF energy = -77.2435048346374  <S^2> = 7.1054274e-15  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42d3400> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42d3400> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 58, 58)
ao.shape (10, 15208, 58)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-8.83278187e-05 -8.83278187e-05 -9.75839793e-04 ... -3.46740731e-05
 -3.31729009e-05 -3.31729009e-05] = SCAN,
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15208, 58)
rho shape (4, 4, 2, 15208)
rho_filt shape: (15208,)
get descriptors tdrho.shape=(15208, 16)
mol:  [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]]
converged SCF energy = -113.221335689652  <S^2> = 6.5902839e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42d31c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42d31c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 10040, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-5.37000596e-04 -8.55494373e-04 -2.46853248e-03 ... -7.34251993e-01
 -7.34251993e-01 -7.34251993e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 10040, 44)
rho shape (4, 4, 2, 10040)
rho_filt shape: (10040,)
get descriptors tdrho.shape=(10040, 16)
mol:  [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]]
converged SCF energy = -460.624592374077  <S^2> = 7.1054274e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42d1240> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42d1240> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 37, 37)
ao.shape (10, 8552, 37)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.38161478e-04 -1.81223966e-05 -2.37327566e-05 ... -2.83738108e+00
 -2.83738108e+00 -2.83738108e+00] = SCAN,
get_data, dm shape = (2, 37, 37)
ao_eval.shape=(4, 8552, 37)
rho shape (4, 4, 2, 8552)
rho_filt shape: (8552,)
get descriptors tdrho.shape=(8552, 16)
mol:  [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]]
converged SCF energy = -8.04458854018922  <S^2> = 7.9047879e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42d0700> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42d0700> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 29, 29)
ao.shape (10, 6936, 29)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-0.00113445 -0.00118624 -0.00118624 ... -0.48434639 -0.48434639
 -0.48434639] = SCAN,
get_data, dm shape = (2, 29, 29)
ao_eval.shape=(4, 6936, 29)
rho shape (4, 4, 2, 6936)
rho_filt shape: (6936,)
get descriptors tdrho.shape=(6936, 16)
mol:  [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]]
converged SCF energy = -324.340512506578  <S^2> = 1.5855761e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42d0ac0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42d0ac0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 11536, 60)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-0.00297936 -0.00297936 -0.00407091 ... -0.00297936 -0.00297936
 -0.00407091] = SCAN,
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 11536, 60)
rho shape (4, 4, 2, 11536)
rho_filt shape: (11536,)
get descriptors tdrho.shape=(11536, 16)
mol:  [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]]
converged SCF energy = -1622.57507845814  <S^2> = 8.2422957e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42d09d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42d09d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 120, 120)
ao.shape (10, 24512, 120)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.61401455e-04 -4.90485117e-04 -2.56451688e-03 ... -9.59296114e+00
 -9.59296114e+00 -9.59296114e+00] = SCAN,
get_data, dm shape = (2, 120, 120)
ao_eval.shape=(4, 24512, 120)
rho shape (4, 4, 2, 24512)
rho_filt shape: (24512,)
get descriptors tdrho.shape=(24512, 16)
mol:  [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]]
converged SCF energy = -342.979728469574  <S^2> = 2.5394797e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42d13c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42d13c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 13096, 51)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.28637187e-03 -4.32380890e-04 -3.74072638e-05 ... -1.91722763e+00
 -1.91722763e+00 -1.91722763e+00] = SCAN,
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 13096, 51)
rho shape (4, 4, 2, 13096)
rho_filt shape: (13096,)
get descriptors tdrho.shape=(13096, 16)
mol:  [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]]
converged SCF energy = -578.565335809629  <S^2> = 1.0034705  2S+1 = 2.2391699
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42d24d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42d24d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12384, 60)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.60128931e-04 -2.60147640e-04 -2.60153280e-04 ... -3.86944099e-01
 -3.86944099e-01 -3.86944099e-01] = SCAN,
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12384, 60)
rho shape (4, 4, 2, 12384)
rho_filt shape: (12384,)
get descriptors tdrho.shape=(12384, 16)
mol:  [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]]
converged SCF energy = -40.4598214864075  <S^2> = 3.2063241e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42d13f0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42d13f0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 50, 50)
ao.shape (10, 13936, 50)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-8.68439856e-04 -2.42462783e-04 -1.69965237e-05 ... -2.55256081e-05
 -2.55256081e-05 -2.55256081e-05] = SCAN,
get_data, dm shape = (2, 50, 50)
ao_eval.shape=(4, 13936, 50)
rho shape (4, 4, 2, 13936)
rho_filt shape: (13936,)
get descriptors tdrho.shape=(13936, 16)
mol:  [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]]
converged SCF energy = -39.0756147483504  <S^2> = 6.1990413e-12  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42d14e0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42d14e0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 36, 36)
ao.shape (10, 9656, 36)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-7.67691257e-04 -4.57409182e-05 -2.02835243e-04 ... -1.14928928e+00
 -1.14928928e+00 -1.14928928e+00] = SCAN,
get_data, dm shape = (2, 36, 36)
ao_eval.shape=(4, 9656, 36)
rho shape (4, 4, 2, 9656)
rho_filt shape: (9656,)
get descriptors tdrho.shape=(9656, 16)
mol:  [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]]
converged SCF energy = -291.719272437819  <S^2> = 1.3155699e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
New DM shape: (2, 58, 58)
ao.shape (10, 15256, 58)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-8.33847724e-04 -2.34902391e-04 -1.75660753e-05 ... -1.92925750e-05
 -1.92925750e-05 -1.92925750e-05] = SCAN,
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15256, 58)
rho shape (4, 4, 2, 15256)
rho_filt shape: (15256,)
get descriptors tdrho.shape=(15256, 16)
PRE NAN FILT: tFxc.shape=(237013,), tdrho.shape=(237013, 16)
nan_filt_rho.shape=(237013,)
nan_filt_fxc.shape=(237013,)
tFxc.shape=(237013,), tdrho.shape=(237013, 16)
inp[0].shape = (237013, 16)
Epoch 0
Epoch 0 :: Batch 0/1
Batch Loss = 8138533.367451707
0, epoch_train_loss=8138533.367451707
Epoch 1
Epoch 1 :: Batch 0/1
Batch Loss = 224640.31196927596
1, epoch_train_loss=224640.31196927596
Epoch 2
Epoch 2 :: Batch 0/1
Batch Loss = 13.87681175902535
2, epoch_train_loss=13.87681175902535
Epoch 3
Epoch 3 :: Batch 0/1
Batch Loss = 41403.43961970564
3, epoch_train_loss=41403.43961970564
Epoch 4
Epoch 4 :: Batch 0/1
Batch Loss = 1581881.4125262697
4, epoch_train_loss=1581881.4125262697
Epoch 5
Epoch 5 :: Batch 0/1
Batch Loss = 3.5972837558675677
5, epoch_train_loss=3.5972837558675677
Epoch 6
Epoch 6 :: Batch 0/1
Batch Loss = 4.6128574933811795
6, epoch_train_loss=4.6128574933811795
Epoch 7
Epoch 7 :: Batch 0/1
Batch Loss = 4.595274100140608
7, epoch_train_loss=4.595274100140608
Epoch 8
Epoch 8 :: Batch 0/1
Batch Loss = 4.570941706783204
8, epoch_train_loss=4.570941706783204
Epoch 9
Epoch 9 :: Batch 0/1
Batch Loss = 23.055359111362627
9, epoch_train_loss=23.055359111362627
Epoch 10
Epoch 10 :: Batch 0/1
Batch Loss = 4.57041983435946
10, epoch_train_loss=4.57041983435946
Epoch 11
Epoch 11 :: Batch 0/1
Batch Loss = 4.568188167607608
11, epoch_train_loss=4.568188167607608
Epoch 12
Epoch 12 :: Batch 0/1
Batch Loss = 4.564368591164169
12, epoch_train_loss=4.564368591164169
Epoch 13
Epoch 13 :: Batch 0/1
Batch Loss = 4.560433871884191
13, epoch_train_loss=4.560433871884191
Epoch 14
Epoch 14 :: Batch 0/1
Batch Loss = 4.556496522628868
14, epoch_train_loss=4.556496522628868
Epoch 15
Epoch 15 :: Batch 0/1
Batch Loss = 4.5527079637264025
15, epoch_train_loss=4.5527079637264025
Epoch 16
Epoch 16 :: Batch 0/1
Batch Loss = 4.549097368922558
16, epoch_train_loss=4.549097368922558
Epoch 17
Epoch 17 :: Batch 0/1
Batch Loss = 4.54567279333928
17, epoch_train_loss=4.54567279333928
Epoch 18
Epoch 18 :: Batch 0/1
Batch Loss = 4.5424494236288995
18, epoch_train_loss=4.5424494236288995
Epoch 19
Epoch 19 :: Batch 0/1
Batch Loss = 4.5393970312645635
19, epoch_train_loss=4.5393970312645635
Epoch 20
Epoch 20 :: Batch 0/1
Batch Loss = 4.536462998593283
20, epoch_train_loss=4.536462998593283
Epoch 21
Epoch 21 :: Batch 0/1
Batch Loss = 4.533621099880256
21, epoch_train_loss=4.533621099880256
Epoch 22
Epoch 22 :: Batch 0/1
Batch Loss = 4.531134198057755
22, epoch_train_loss=4.531134198057755
Epoch 23
Epoch 23 :: Batch 0/1
Batch Loss = 4.528311194268081
23, epoch_train_loss=4.528311194268081
Epoch 24
Epoch 24 :: Batch 0/1
Batch Loss = 4.525843472638485
24, epoch_train_loss=4.525843472638485
Epoch 25
Epoch 25 :: Batch 0/1
Batch Loss = 4.523411620077204
25, epoch_train_loss=4.523411620077204
Epoch 26
Epoch 26 :: Batch 0/1
Batch Loss = 4.520996699668694
26, epoch_train_loss=4.520996699668694
Epoch 27
Epoch 27 :: Batch 0/1
Batch Loss = 4.518603331087248
27, epoch_train_loss=4.518603331087248
Epoch 28
Epoch 28 :: Batch 0/1
Batch Loss = 4.516228681944123
28, epoch_train_loss=4.516228681944123
Epoch 29
Epoch 29 :: Batch 0/1
Batch Loss = 4.513861087727075
29, epoch_train_loss=4.513861087727075
Epoch 30
Epoch 30 :: Batch 0/1
Batch Loss = 4.511472561270985
30, epoch_train_loss=4.511472561270985
Epoch 31
Epoch 31 :: Batch 0/1
Batch Loss = 4.509017791738431
31, epoch_train_loss=4.509017791738431
Epoch 32
Epoch 32 :: Batch 0/1
Batch Loss = 4.5067914320060325
32, epoch_train_loss=4.5067914320060325
Epoch 33
Epoch 33 :: Batch 0/1
Batch Loss = 4.504348414978327
33, epoch_train_loss=4.504348414978327
Epoch 34
Epoch 34 :: Batch 0/1
Batch Loss = 4.502092429322698
34, epoch_train_loss=4.502092429322698
Epoch 35
Epoch 35 :: Batch 0/1
Batch Loss = 4.499842148618271
35, epoch_train_loss=4.499842148618271
Epoch 36
Epoch 36 :: Batch 0/1
Batch Loss = 4.497511612544938
36, epoch_train_loss=4.497511612544938
Epoch 37
Epoch 37 :: Batch 0/1
Batch Loss = 4.495234158256584
37, epoch_train_loss=4.495234158256584
Epoch 38
Epoch 38 :: Batch 0/1
Batch Loss = 4.492971899021547
38, epoch_train_loss=4.492971899021547
Epoch 39
Epoch 39 :: Batch 0/1
Batch Loss = 4.490705784684569
39, epoch_train_loss=4.490705784684569
Epoch 40
Epoch 40 :: Batch 0/1
Batch Loss = 4.488481682018958
40, epoch_train_loss=4.488481682018958
Epoch 41
Epoch 41 :: Batch 0/1
Batch Loss = 4.486172691975136
41, epoch_train_loss=4.486172691975136
Epoch 42
Epoch 42 :: Batch 0/1
Batch Loss = 4.4839949698978305
42, epoch_train_loss=4.4839949698978305
Epoch 43
Epoch 43 :: Batch 0/1
Batch Loss = 4.481674881560087
43, epoch_train_loss=4.481674881560087
Epoch 44
Epoch 44 :: Batch 0/1
Batch Loss = 4.479472299794413
44, epoch_train_loss=4.479472299794413
Epoch 45
Epoch 45 :: Batch 0/1
Batch Loss = 4.477178729138626
45, epoch_train_loss=4.477178729138626
Epoch 46
Epoch 46 :: Batch 0/1
Batch Loss = 4.474987218915251
46, epoch_train_loss=4.474987218915251
Epoch 47
Epoch 47 :: Batch 0/1
Batch Loss = 4.472681892777465
47, epoch_train_loss=4.472681892777465
Epoch 48
Epoch 48 :: Batch 0/1
Batch Loss = 4.47046988572222
48, epoch_train_loss=4.47046988572222
Epoch 49
Epoch 49 :: Batch 0/1
Batch Loss = 4.468185463443041
49, epoch_train_loss=4.468185463443041
Epoch 50
Epoch 50 :: Batch 0/1
Batch Loss = 4.4659924219787905
50, epoch_train_loss=4.4659924219787905
Epoch 51
Epoch 51 :: Batch 0/1
Batch Loss = 4.463706715628831
51, epoch_train_loss=4.463706715628831
Epoch 52
Epoch 52 :: Batch 0/1
Batch Loss = 4.46148614608527
52, epoch_train_loss=4.46148614608527
Epoch 53
Epoch 53 :: Batch 0/1
Batch Loss = 4.459198866432476
53, epoch_train_loss=4.459198866432476
Epoch 54
Epoch 54 :: Batch 0/1
Batch Loss = 4.457006627904572
54, epoch_train_loss=4.457006627904572
Epoch 55
Epoch 55 :: Batch 0/1
Batch Loss = 4.4547403152905565
55, epoch_train_loss=4.4547403152905565
Epoch 56
Epoch 56 :: Batch 0/1
Batch Loss = 4.45251675680082
56, epoch_train_loss=4.45251675680082
Epoch 57
Epoch 57 :: Batch 0/1
Batch Loss = 4.450220815632685
57, epoch_train_loss=4.450220815632685
Epoch 58
Epoch 58 :: Batch 0/1
Batch Loss = 4.448040387465877
58, epoch_train_loss=4.448040387465877
Epoch 59
Epoch 59 :: Batch 0/1
Batch Loss = 4.445784412888096
59, epoch_train_loss=4.445784412888096
Epoch 60
Epoch 60 :: Batch 0/1
Batch Loss = 4.443573214671024
60, epoch_train_loss=4.443573214671024
Epoch 61
Epoch 61 :: Batch 0/1
Batch Loss = 4.441255338456312
61, epoch_train_loss=4.441255338456312
Epoch 62
Epoch 62 :: Batch 0/1
Batch Loss = 4.4391362969707036
62, epoch_train_loss=4.4391362969707036
Epoch 63
Epoch 63 :: Batch 0/1
Batch Loss = 4.43685994759759
63, epoch_train_loss=4.43685994759759
Epoch 64
Epoch 64 :: Batch 0/1
Batch Loss = 4.434702949127458
64, epoch_train_loss=4.434702949127458
Epoch 65
Epoch 65 :: Batch 0/1
Batch Loss = 4.432339628858255
65, epoch_train_loss=4.432339628858255
Epoch 66
Epoch 66 :: Batch 0/1
Batch Loss = 4.430307825621228
66, epoch_train_loss=4.430307825621228
Epoch 67
Epoch 67 :: Batch 0/1
Batch Loss = 4.42792605179554
67, epoch_train_loss=4.42792605179554
Epoch 68
Epoch 68 :: Batch 0/1
Batch Loss = 4.425832507698532
68, epoch_train_loss=4.425832507698532
Epoch 69
Epoch 69 :: Batch 0/1
Batch Loss = 4.423510335044642
69, epoch_train_loss=4.423510335044642
Epoch 70
Epoch 70 :: Batch 0/1
Batch Loss = 4.42130840382861
70, epoch_train_loss=4.42130840382861
Epoch 71
Epoch 71 :: Batch 0/1
Batch Loss = 4.419056786040238
71, epoch_train_loss=4.419056786040238
Epoch 72
Epoch 72 :: Batch 0/1
Batch Loss = 4.416873652654918
72, epoch_train_loss=4.416873652654918
Epoch 73
Epoch 73 :: Batch 0/1
Batch Loss = 4.41470432443617
73, epoch_train_loss=4.41470432443617
Epoch 74
Epoch 74 :: Batch 0/1
Batch Loss = 4.412441175870114
74, epoch_train_loss=4.412441175870114
Epoch 75
Epoch 75 :: Batch 0/1
Batch Loss = 4.410350126280424
75, epoch_train_loss=4.410350126280424
Epoch 76
Epoch 76 :: Batch 0/1
Batch Loss = 4.408087422930508
76, epoch_train_loss=4.408087422930508
Epoch 77
Epoch 77 :: Batch 0/1
Batch Loss = 4.40597541943493
77, epoch_train_loss=4.40597541943493
Epoch 78
Epoch 78 :: Batch 0/1
Batch Loss = 4.403751845250099
78, epoch_train_loss=4.403751845250099
Epoch 79
Epoch 79 :: Batch 0/1
Batch Loss = 4.401633439807304
79, epoch_train_loss=4.401633439807304
Epoch 80
Epoch 80 :: Batch 0/1
Batch Loss = 4.399429505698238
80, epoch_train_loss=4.399429505698238
Epoch 81
Epoch 81 :: Batch 0/1
Batch Loss = 4.39731205286385
81, epoch_train_loss=4.39731205286385
Epoch 82
Epoch 82 :: Batch 0/1
Batch Loss = 4.395162877018285
82, epoch_train_loss=4.395162877018285
Epoch 83
Epoch 83 :: Batch 0/1
Batch Loss = 4.3930253453760715
83, epoch_train_loss=4.3930253453760715
Epoch 84
Epoch 84 :: Batch 0/1
Batch Loss = 4.390909635554538
84, epoch_train_loss=4.390909635554538
Epoch 85
Epoch 85 :: Batch 0/1
Batch Loss = 4.388789305412558
85, epoch_train_loss=4.388789305412558
Epoch 86
Epoch 86 :: Batch 0/1
Batch Loss = 4.386693788867471
86, epoch_train_loss=4.386693788867471
Epoch 87
Epoch 87 :: Batch 0/1
Batch Loss = 4.3845774820346675
87, epoch_train_loss=4.3845774820346675
Epoch 88
Epoch 88 :: Batch 0/1
Batch Loss = 4.382510345424029
88, epoch_train_loss=4.382510345424029
Epoch 89
Epoch 89 :: Batch 0/1
Batch Loss = 4.38041582238007
89, epoch_train_loss=4.38041582238007
Epoch 90
Epoch 90 :: Batch 0/1
Batch Loss = 4.378359187424582
90, epoch_train_loss=4.378359187424582
Epoch 91
Epoch 91 :: Batch 0/1
Batch Loss = 4.376281969344607
91, epoch_train_loss=4.376281969344607
Epoch 92
Epoch 92 :: Batch 0/1
Batch Loss = 4.374249323532403
92, epoch_train_loss=4.374249323532403
Epoch 93
Epoch 93 :: Batch 0/1
Batch Loss = 4.372198074512023
93, epoch_train_loss=4.372198074512023
Epoch 94
Epoch 94 :: Batch 0/1
Batch Loss = 4.370175806280111
94, epoch_train_loss=4.370175806280111
Epoch 95
Epoch 95 :: Batch 0/1
Batch Loss = 4.368146129120967
95, epoch_train_loss=4.368146129120967
Epoch 96
Epoch 96 :: Batch 0/1
Batch Loss = 4.366143178652927
96, epoch_train_loss=4.366143178652927
Epoch 97
Epoch 97 :: Batch 0/1
Batch Loss = 4.364144378392223
97, epoch_train_loss=4.364144378392223
Epoch 98
Epoch 98 :: Batch 0/1
Batch Loss = 4.362155177797004
98, epoch_train_loss=4.362155177797004
Epoch 99
Epoch 99 :: Batch 0/1
Batch Loss = 4.360181216229633
99, epoch_train_loss=4.360181216229633
Epoch 100
Epoch 100 :: Batch 0/1
Batch Loss = 4.358207327385364
100, epoch_train_loss=4.358207327385364
Epoch 101
Epoch 101 :: Batch 0/1
Batch Loss = 4.356261241702484
101, epoch_train_loss=4.356261241702484
Epoch 102
Epoch 102 :: Batch 0/1
Batch Loss = 4.354310370870647
102, epoch_train_loss=4.354310370870647
Epoch 103
Epoch 103 :: Batch 0/1
Batch Loss = 4.352388564811874
103, epoch_train_loss=4.352388564811874
Epoch 104
Epoch 104 :: Batch 0/1
Batch Loss = 4.350462005486428
104, epoch_train_loss=4.350462005486428
Epoch 105
Epoch 105 :: Batch 0/1
Batch Loss = 4.3485573353765625
105, epoch_train_loss=4.3485573353765625
Epoch 106
Epoch 106 :: Batch 0/1
Batch Loss = 4.346660776533225
106, epoch_train_loss=4.346660776533225
Epoch 107
Epoch 107 :: Batch 0/1
Batch Loss = 4.344774315610535
107, epoch_train_loss=4.344774315610535
Epoch 108
Epoch 108 :: Batch 0/1
Batch Loss = 4.342906908366668
108, epoch_train_loss=4.342906908366668
Epoch 109
Epoch 109 :: Batch 0/1
Batch Loss = 4.3410433409367615
109, epoch_train_loss=4.3410433409367615
Epoch 110
Epoch 110 :: Batch 0/1
Batch Loss = 4.339201154264798
110, epoch_train_loss=4.339201154264798
Epoch 111
Epoch 111 :: Batch 0/1
Batch Loss = 4.337364685496242
111, epoch_train_loss=4.337364685496242
Epoch 112
Epoch 112 :: Batch 0/1
Batch Loss = 4.335542557052738
112, epoch_train_loss=4.335542557052738
Epoch 113
Epoch 113 :: Batch 0/1
Batch Loss = 4.333736109533345
113, epoch_train_loss=4.333736109533345
Epoch 114
Epoch 114 :: Batch 0/1
Batch Loss = 4.331936366339099
114, epoch_train_loss=4.331936366339099
Epoch 115
Epoch 115 :: Batch 0/1
Batch Loss = 4.330155018800478
115, epoch_train_loss=4.330155018800478
Epoch 116
Epoch 116 :: Batch 0/1
Batch Loss = 4.328383979915126
116, epoch_train_loss=4.328383979915126
Epoch 117
Epoch 117 :: Batch 0/1
Batch Loss = 4.326625320271481
117, epoch_train_loss=4.326625320271481
Epoch 118
Epoch 118 :: Batch 0/1
Batch Loss = 4.324882621704504
118, epoch_train_loss=4.324882621704504
Epoch 119
Epoch 119 :: Batch 0/1
Batch Loss = 4.323149790360985
119, epoch_train_loss=4.323149790360985
Epoch 120
Epoch 120 :: Batch 0/1
Batch Loss = 4.3214322190067564
120, epoch_train_loss=4.3214322190067564
Epoch 121
Epoch 121 :: Batch 0/1
Batch Loss = 4.319727951416821
121, epoch_train_loss=4.319727951416821
Epoch 122
Epoch 122 :: Batch 0/1
Batch Loss = 4.318034983850769
122, epoch_train_loss=4.318034983850769
Epoch 123
Epoch 123 :: Batch 0/1
Batch Loss = 4.316357759385502
123, epoch_train_loss=4.316357759385502
Epoch 124
Epoch 124 :: Batch 0/1
Batch Loss = 4.314692798169162
124, epoch_train_loss=4.314692798169162
Epoch 125
Epoch 125 :: Batch 0/1
Batch Loss = 4.313040182475796
125, epoch_train_loss=4.313040182475796
Epoch 126
Epoch 126 :: Batch 0/1
Batch Loss = 4.311403145993612
126, epoch_train_loss=4.311403145993612
Epoch 127
Epoch 127 :: Batch 0/1
Batch Loss = 4.309778340295448
127, epoch_train_loss=4.309778340295448
Epoch 128
Epoch 128 :: Batch 0/1
Batch Loss = 4.30816616212826
128, epoch_train_loss=4.30816616212826
Epoch 129
Epoch 129 :: Batch 0/1
Batch Loss = 4.306569426232821
129, epoch_train_loss=4.306569426232821
Epoch 130
Epoch 130 :: Batch 0/1
Batch Loss = 4.304985329511471
130, epoch_train_loss=4.304985329511471
Epoch 131
Epoch 131 :: Batch 0/1
Batch Loss = 4.303413655072794
131, epoch_train_loss=4.303413655072794
Epoch 132
Epoch 132 :: Batch 0/1
Batch Loss = 4.301857247239711
132, epoch_train_loss=4.301857247239711
Epoch 133
Epoch 133 :: Batch 0/1
Batch Loss = 4.300314144232325
133, epoch_train_loss=4.300314144232325
Epoch 134
Epoch 134 :: Batch 0/1
Batch Loss = 4.298783175817882
134, epoch_train_loss=4.298783175817882
Epoch 135
Epoch 135 :: Batch 0/1
Batch Loss = 4.297266965869167
135, epoch_train_loss=4.297266965869167
Epoch 136
Epoch 136 :: Batch 0/1
Batch Loss = 4.295764806449673
136, epoch_train_loss=4.295764806449673
Epoch 137
Epoch 137 :: Batch 0/1
Batch Loss = 4.294274920721195
137, epoch_train_loss=4.294274920721195
Epoch 138
Epoch 138 :: Batch 0/1
Batch Loss = 4.292798863289751
138, epoch_train_loss=4.292798863289751
Epoch 139
Epoch 139 :: Batch 0/1
Batch Loss = 4.291337199892787
139, epoch_train_loss=4.291337199892787
Epoch 140
Epoch 140 :: Batch 0/1
Batch Loss = 4.290071361828146
140, epoch_train_loss=4.290071361828146
Epoch 141
Epoch 141 :: Batch 0/1
Batch Loss = 4.290407887404106
141, epoch_train_loss=4.290407887404106
Epoch 142
Epoch 142 :: Batch 0/1
Batch Loss = 4.289446941627961
142, epoch_train_loss=4.289446941627961
Epoch 143
Epoch 143 :: Batch 0/1
Batch Loss = 4.288208395476245
143, epoch_train_loss=4.288208395476245
Epoch 144
Epoch 144 :: Batch 0/1
Batch Loss = 4.286917462946826
144, epoch_train_loss=4.286917462946826
Epoch 145
Epoch 145 :: Batch 0/1
Batch Loss = 4.285610603101555
145, epoch_train_loss=4.285610603101555
Epoch 146
Epoch 146 :: Batch 0/1
Batch Loss = 4.2843014634327865
146, epoch_train_loss=4.2843014634327865
Epoch 147
Epoch 147 :: Batch 0/1
Batch Loss = 4.282996822505276
147, epoch_train_loss=4.282996822505276
Epoch 148
Epoch 148 :: Batch 0/1
Batch Loss = 4.28170037273406
148, epoch_train_loss=4.28170037273406
Epoch 149
Epoch 149 :: Batch 0/1
Batch Loss = 4.280414201504855
149, epoch_train_loss=4.280414201504855
Epoch 150
Epoch 150 :: Batch 0/1
Batch Loss = 4.279139511627666
150, epoch_train_loss=4.279139511627666
Epoch 151
Epoch 151 :: Batch 0/1
Batch Loss = 4.277877002085338
151, epoch_train_loss=4.277877002085338
Epoch 152
Epoch 152 :: Batch 0/1
Batch Loss = 4.276627077377623
152, epoch_train_loss=4.276627077377623
Epoch 153
Epoch 153 :: Batch 0/1
Batch Loss = 4.275389965119245
153, epoch_train_loss=4.275389965119245
Epoch 154
Epoch 154 :: Batch 0/1
Batch Loss = 4.27416578419903
154, epoch_train_loss=4.27416578419903
Epoch 155
Epoch 155 :: Batch 0/1
Batch Loss = 4.272954585444738
155, epoch_train_loss=4.272954585444738
Epoch 156
Epoch 156 :: Batch 0/1
Batch Loss = 4.271756375865249
156, epoch_train_loss=4.271756375865249
Epoch 157
Epoch 157 :: Batch 0/1
Batch Loss = 4.27057113388406
157, epoch_train_loss=4.27057113388406
Epoch 158
Epoch 158 :: Batch 0/1
Batch Loss = 4.269398819264885
158, epoch_train_loss=4.269398819264885
Epoch 159
Epoch 159 :: Batch 0/1
Batch Loss = 4.268239379083944
159, epoch_train_loss=4.268239379083944
Epoch 160
Epoch 160 :: Batch 0/1
Batch Loss = 4.267092751831499
160, epoch_train_loss=4.267092751831499
Epoch 161
Epoch 161 :: Batch 0/1
Batch Loss = 4.265958870395362
161, epoch_train_loss=4.265958870395362
Epoch 162
Epoch 162 :: Batch 0/1
Batch Loss = 4.264837663549033
162, epoch_train_loss=4.264837663549033
Epoch 163
Epoch 163 :: Batch 0/1
Batch Loss = 4.263729057448379
163, epoch_train_loss=4.263729057448379
Epoch 164
Epoch 164 :: Batch 0/1
Batch Loss = 4.262632976469792
164, epoch_train_loss=4.262632976469792
Epoch 165
Epoch 165 :: Batch 0/1
Batch Loss = 4.261549343536105
165, epoch_train_loss=4.261549343536105
Epoch 166
Epoch 166 :: Batch 0/1
Batch Loss = 4.260478080832813
166, epoch_train_loss=4.260478080832813
Epoch 167
Epoch 167 :: Batch 0/1
Batch Loss = 4.259419109743782
167, epoch_train_loss=4.259419109743782
Epoch 168
Epoch 168 :: Batch 0/1
Batch Loss = 4.25837235126595
168, epoch_train_loss=4.25837235126595
Epoch 169
Epoch 169 :: Batch 0/1
Batch Loss = 4.2573377260384575
169, epoch_train_loss=4.2573377260384575
Epoch 170
Epoch 170 :: Batch 0/1
Batch Loss = 4.256315154410727
170, epoch_train_loss=4.256315154410727
Epoch 171
Epoch 171 :: Batch 0/1
Batch Loss = 4.25530455655678
171, epoch_train_loss=4.25530455655678
Epoch 172
Epoch 172 :: Batch 0/1
Batch Loss = 4.254305852266649
172, epoch_train_loss=4.254305852266649
Epoch 173
Epoch 173 :: Batch 0/1
Batch Loss = 4.2533189612551725
173, epoch_train_loss=4.2533189612551725
Epoch 174
Epoch 174 :: Batch 0/1
Batch Loss = 4.25234380278756
174, epoch_train_loss=4.25234380278756
Epoch 175
Epoch 175 :: Batch 0/1
Batch Loss = 4.251380296187431
175, epoch_train_loss=4.251380296187431
Epoch 176
Epoch 176 :: Batch 0/1
Batch Loss = 4.250428360313273
176, epoch_train_loss=4.250428360313273
Epoch 177
Epoch 177 :: Batch 0/1
Batch Loss = 4.249487913840112
177, epoch_train_loss=4.249487913840112
Epoch 178
Epoch 178 :: Batch 0/1
Batch Loss = 4.248558875175271
178, epoch_train_loss=4.248558875175271
Epoch 179
Epoch 179 :: Batch 0/1
Batch Loss = 4.247641162448377
179, epoch_train_loss=4.247641162448377
Epoch 180
Epoch 180 :: Batch 0/1
Batch Loss = 4.24673469350287
180, epoch_train_loss=4.24673469350287
Epoch 181
Epoch 181 :: Batch 0/1
Batch Loss = 4.245839385889264
181, epoch_train_loss=4.245839385889264
Epoch 182
Epoch 182 :: Batch 0/1
Batch Loss = 4.244955156860382
182, epoch_train_loss=4.244955156860382
Epoch 183
Epoch 183 :: Batch 0/1
Batch Loss = 4.244081923368627
183, epoch_train_loss=4.244081923368627
Epoch 184
Epoch 184 :: Batch 0/1
Batch Loss = 4.243219602065272
184, epoch_train_loss=4.243219602065272
Epoch 185
Epoch 185 :: Batch 0/1
Batch Loss = 4.242368109301731
185, epoch_train_loss=4.242368109301731
Epoch 186
Epoch 186 :: Batch 0/1
Batch Loss = 4.241527361069973
186, epoch_train_loss=4.241527361069973
Epoch 187
Epoch 187 :: Batch 0/1
Batch Loss = 4.24069727325903
187, epoch_train_loss=4.24069727325903
Epoch 188
Epoch 188 :: Batch 0/1
Batch Loss = 4.239877761222306
188, epoch_train_loss=4.239877761222306
Epoch 189
Epoch 189 :: Batch 0/1
Batch Loss = 4.239068740223122
189, epoch_train_loss=4.239068740223122
Epoch 190
Epoch 190 :: Batch 0/1
Batch Loss = 4.238270125135236
190, epoch_train_loss=4.238270125135236
Epoch 191
Epoch 191 :: Batch 0/1
Batch Loss = 4.237481830696233
191, epoch_train_loss=4.237481830696233
Epoch 192
Epoch 192 :: Batch 0/1
Batch Loss = 4.236703771338028
192, epoch_train_loss=4.236703771338028
Epoch 193
Epoch 193 :: Batch 0/1
Batch Loss = 4.235935861260652
193, epoch_train_loss=4.235935861260652
Epoch 194
Epoch 194 :: Batch 0/1
Batch Loss = 4.235178014446049
194, epoch_train_loss=4.235178014446049
Epoch 195
Epoch 195 :: Batch 0/1
Batch Loss = 4.234430144616788
195, epoch_train_loss=4.234430144616788
Epoch 196
Epoch 196 :: Batch 0/1
Batch Loss = 4.233692165420017
196, epoch_train_loss=4.233692165420017
Epoch 197
Epoch 197 :: Batch 0/1
Batch Loss = 4.232963990218901
197, epoch_train_loss=4.232963990218901
Epoch 198
Epoch 198 :: Batch 0/1
Batch Loss = 4.232245532330199
198, epoch_train_loss=4.232245532330199
Epoch 199
Epoch 199 :: Batch 0/1
Batch Loss = 4.231536704875337
199, epoch_train_loss=4.231536704875337
Epoch 200
Epoch 200 :: Batch 0/1
Batch Loss = 4.2308374208006105
200, epoch_train_loss=4.2308374208006105
Epoch 201
Epoch 201 :: Batch 0/1
Batch Loss = 4.230147593052088
201, epoch_train_loss=4.230147593052088
Epoch 202
Epoch 202 :: Batch 0/1
Batch Loss = 4.229467134383019
202, epoch_train_loss=4.229467134383019
Epoch 203
Epoch 203 :: Batch 0/1
Batch Loss = 4.228795957527953
203, epoch_train_loss=4.228795957527953
Epoch 204
Epoch 204 :: Batch 0/1
Batch Loss = 4.2281339752167915
204, epoch_train_loss=4.2281339752167915
Epoch 205
Epoch 205 :: Batch 0/1
Batch Loss = 4.227481100042572
205, epoch_train_loss=4.227481100042572
Epoch 206
Epoch 206 :: Batch 0/1
Batch Loss = 4.226837244629133
206, epoch_train_loss=4.226837244629133
Epoch 207
Epoch 207 :: Batch 0/1
Batch Loss = 4.22620232164495
207, epoch_train_loss=4.22620232164495
Epoch 208
Epoch 208 :: Batch 0/1
Batch Loss = 4.22557624367652
208, epoch_train_loss=4.22557624367652
Epoch 209
Epoch 209 :: Batch 0/1
Batch Loss = 4.224958923389299
209, epoch_train_loss=4.224958923389299
Epoch 210
Epoch 210 :: Batch 0/1
Batch Loss = 4.224350273540874
210, epoch_train_loss=4.224350273540874
Epoch 211
Epoch 211 :: Batch 0/1
Batch Loss = 4.223750206859371
211, epoch_train_loss=4.223750206859371
Epoch 212
Epoch 212 :: Batch 0/1
Batch Loss = 4.223158636197445
212, epoch_train_loss=4.223158636197445
Epoch 213
Epoch 213 :: Batch 0/1
Batch Loss = 4.222575474544411
213, epoch_train_loss=4.222575474544411
Epoch 214
Epoch 214 :: Batch 0/1
Batch Loss = 4.222000634909131
214, epoch_train_loss=4.222000634909131
Epoch 215
Epoch 215 :: Batch 0/1
Batch Loss = 4.2214340304245415
215, epoch_train_loss=4.2214340304245415
Epoch 216
Epoch 216 :: Batch 0/1
Batch Loss = 4.220875574486379
216, epoch_train_loss=4.220875574486379
Epoch 217
Epoch 217 :: Batch 0/1
Batch Loss = 4.220325180511033
217, epoch_train_loss=4.220325180511033
Epoch 218
Epoch 218 :: Batch 0/1
Batch Loss = 4.219782762118381
218, epoch_train_loss=4.219782762118381
Epoch 219
Epoch 219 :: Batch 0/1
Batch Loss = 4.2192482331008145
219, epoch_train_loss=4.2192482331008145
Epoch 220
Epoch 220 :: Batch 0/1
Batch Loss = 4.2187215074342275
220, epoch_train_loss=4.2187215074342275
Epoch 221
Epoch 221 :: Batch 0/1
Batch Loss = 4.218202499288241
221, epoch_train_loss=4.218202499288241
Epoch 222
Epoch 222 :: Batch 0/1
Batch Loss = 4.21769112303561
222, epoch_train_loss=4.21769112303561
Epoch 223
Epoch 223 :: Batch 0/1
Batch Loss = 4.217187293260715
223, epoch_train_loss=4.217187293260715
Epoch 224
Epoch 224 :: Batch 0/1
Batch Loss = 4.21669092476712
224, epoch_train_loss=4.21669092476712
Epoch 225
Epoch 225 :: Batch 0/1
Batch Loss = 4.2162019325840445
225, epoch_train_loss=4.2162019325840445
Epoch 226
Epoch 226 :: Batch 0/1
Batch Loss = 4.215720231971755
226, epoch_train_loss=4.215720231971755
Epoch 227
Epoch 227 :: Batch 0/1
Batch Loss = 4.2152457383902
227, epoch_train_loss=4.2152457383902
Epoch 228
Epoch 228 :: Batch 0/1
Batch Loss = 4.214778367609064
228, epoch_train_loss=4.214778367609064
Epoch 229
Epoch 229 :: Batch 0/1
Batch Loss = 4.2143180356009236
229, epoch_train_loss=4.2143180356009236
Epoch 230
Epoch 230 :: Batch 0/1
Batch Loss = 4.213864658577213
230, epoch_train_loss=4.213864658577213
Epoch 231
Epoch 231 :: Batch 0/1
Batch Loss = 4.213418152986065
231, epoch_train_loss=4.213418152986065
Epoch 232
Epoch 232 :: Batch 0/1
Batch Loss = 4.212978435475255
232, epoch_train_loss=4.212978435475255
Epoch 233
Epoch 233 :: Batch 0/1
Batch Loss = 4.212545422985192
233, epoch_train_loss=4.212545422985192
Epoch 234
Epoch 234 :: Batch 0/1
Batch Loss = 4.212119032639716
234, epoch_train_loss=4.212119032639716
Epoch 235
Epoch 235 :: Batch 0/1
Batch Loss = 4.211699181767953
235, epoch_train_loss=4.211699181767953
Epoch 236
Epoch 236 :: Batch 0/1
Batch Loss = 4.211285787889278
236, epoch_train_loss=4.211285787889278
Epoch 237
Epoch 237 :: Batch 0/1
Batch Loss = 4.210878768664165
237, epoch_train_loss=4.210878768664165
Epoch 238
Epoch 238 :: Batch 0/1
Batch Loss = 4.210478041933386
238, epoch_train_loss=4.210478041933386
Epoch 239
Epoch 239 :: Batch 0/1
Batch Loss = 4.2100835256881215
239, epoch_train_loss=4.2100835256881215
Epoch 240
Epoch 240 :: Batch 0/1
Batch Loss = 4.20969513794745
240, epoch_train_loss=4.20969513794745
Epoch 241
Epoch 241 :: Batch 0/1
Batch Loss = 4.209312796837237
241, epoch_train_loss=4.209312796837237
Epoch 242
Epoch 242 :: Batch 0/1
Batch Loss = 4.208936420454388
242, epoch_train_loss=4.208936420454388
Epoch 243
Epoch 243 :: Batch 0/1
Batch Loss = 4.208565926813019
243, epoch_train_loss=4.208565926813019
Epoch 244
Epoch 244 :: Batch 0/1
Batch Loss = 4.208201233862122
244, epoch_train_loss=4.208201233862122
Epoch 245
Epoch 245 :: Batch 0/1
Batch Loss = 4.207842259295915
245, epoch_train_loss=4.207842259295915
Epoch 246
Epoch 246 :: Batch 0/1
Batch Loss = 4.207488920544379
246, epoch_train_loss=4.207488920544379
Epoch 247
Epoch 247 :: Batch 0/1
Batch Loss = 4.20714113463549
247, epoch_train_loss=4.20714113463549
Epoch 248
Epoch 248 :: Batch 0/1
Batch Loss = 4.206798818090408
248, epoch_train_loss=4.206798818090408
Epoch 249
Epoch 249 :: Batch 0/1
Batch Loss = 4.206461886690807
249, epoch_train_loss=4.206461886690807
Epoch 250
Epoch 250 :: Batch 0/1
Batch Loss = 4.206130255371667
250, epoch_train_loss=4.206130255371667
Epoch 251
Epoch 251 :: Batch 0/1
Batch Loss = 4.205803837997395
251, epoch_train_loss=4.205803837997395
Epoch 252
Epoch 252 :: Batch 0/1
Batch Loss = 4.205482547020789
252, epoch_train_loss=4.205482547020789
Epoch 253
Epoch 253 :: Batch 0/1
Batch Loss = 4.205166293234632
253, epoch_train_loss=4.205166293234632
Epoch 254
Epoch 254 :: Batch 0/1
Batch Loss = 4.204854985377714
254, epoch_train_loss=4.204854985377714
Epoch 255
Epoch 255 :: Batch 0/1
Batch Loss = 4.20454852958524
255, epoch_train_loss=4.20454852958524
Epoch 256
Epoch 256 :: Batch 0/1
Batch Loss = 4.204246828871154
256, epoch_train_loss=4.204246828871154
Epoch 257
Epoch 257 :: Batch 0/1
Batch Loss = 4.203949782376183
257, epoch_train_loss=4.203949782376183
Epoch 258
Epoch 258 :: Batch 0/1
Batch Loss = 4.203657284461235
258, epoch_train_loss=4.203657284461235
Epoch 259
Epoch 259 :: Batch 0/1
Batch Loss = 4.203369223570249
259, epoch_train_loss=4.203369223570249
Epoch 260
Epoch 260 :: Batch 0/1
Batch Loss = 4.203085480792767
260, epoch_train_loss=4.203085480792767
Epoch 261
Epoch 261 :: Batch 0/1
Batch Loss = 4.202805928032003
261, epoch_train_loss=4.202805928032003
Epoch 262
Epoch 262 :: Batch 0/1
Batch Loss = 4.202530425649743
262, epoch_train_loss=4.202530425649743
Epoch 263
Epoch 263 :: Batch 0/1
Batch Loss = 4.202258819410694
263, epoch_train_loss=4.202258819410694
Epoch 264
Epoch 264 :: Batch 0/1
Batch Loss = 4.201990936478886
264, epoch_train_loss=4.201990936478886
Epoch 265
Epoch 265 :: Batch 0/1
Batch Loss = 4.20172658011688
265, epoch_train_loss=4.20172658011688
Epoch 266
Epoch 266 :: Batch 0/1
Batch Loss = 4.201465522588075
266, epoch_train_loss=4.201465522588075
Epoch 267
Epoch 267 :: Batch 0/1
Batch Loss = 4.201207495536258
267, epoch_train_loss=4.201207495536258
Epoch 268
Epoch 268 :: Batch 0/1
Batch Loss = 4.200952176770014
268, epoch_train_loss=4.200952176770014
Epoch 269
Epoch 269 :: Batch 0/1
Batch Loss = 4.200699171817977
269, epoch_train_loss=4.200699171817977
Epoch 270
Epoch 270 :: Batch 0/1
Batch Loss = 4.200447987863512
270, epoch_train_loss=4.200447987863512
Epoch 271
Epoch 271 :: Batch 0/1
Batch Loss = 4.200197995941169
271, epoch_train_loss=4.200197995941169
Epoch 272
Epoch 272 :: Batch 0/1
Batch Loss = 4.199948375197025
272, epoch_train_loss=4.199948375197025
Epoch 273
Epoch 273 :: Batch 0/1
Batch Loss = 4.199698028329682
273, epoch_train_loss=4.199698028329682
Epoch 274
Epoch 274 :: Batch 0/1
Batch Loss = 4.199445449389112
274, epoch_train_loss=4.199445449389112
Epoch 275
Epoch 275 :: Batch 0/1
Batch Loss = 4.1991885086497485
275, epoch_train_loss=4.1991885086497485
Epoch 276
Epoch 276 :: Batch 0/1
Batch Loss = 4.1989240855028775
276, epoch_train_loss=4.1989240855028775
Epoch 277
Epoch 277 :: Batch 0/1
Batch Loss = 4.198647400198089
277, epoch_train_loss=4.198647400198089
Epoch 278
Epoch 278 :: Batch 0/1
Batch Loss = 4.1983506890974684
278, epoch_train_loss=4.1983506890974684
Epoch 279
Epoch 279 :: Batch 0/1
Batch Loss = 4.198020249365224
279, epoch_train_loss=4.198020249365224
Epoch 280
Epoch 280 :: Batch 0/1
Batch Loss = 4.197628639046248
280, epoch_train_loss=4.197628639046248
Epoch 281
Epoch 281 :: Batch 0/1
Batch Loss = 4.197107135241952
281, epoch_train_loss=4.197107135241952
Epoch 282
Epoch 282 :: Batch 0/1
Batch Loss = 4.197033448661041
282, epoch_train_loss=4.197033448661041
Epoch 283
Epoch 283 :: Batch 0/1
Batch Loss = 4.196436309228829
283, epoch_train_loss=4.196436309228829
Epoch 284
Epoch 284 :: Batch 0/1
Batch Loss = 4.196230479617025
284, epoch_train_loss=4.196230479617025
Epoch 285
Epoch 285 :: Batch 0/1
Batch Loss = 4.331940655315312
285, epoch_train_loss=4.331940655315312
Epoch 286
Epoch 286 :: Batch 0/1
Batch Loss = 4.19614030005377
286, epoch_train_loss=4.19614030005377
Epoch 287
Epoch 287 :: Batch 0/1
Batch Loss = 4.196273125119545
287, epoch_train_loss=4.196273125119545
Epoch 288
Epoch 288 :: Batch 0/1
Batch Loss = 4.196276351591138
288, epoch_train_loss=4.196276351591138
Epoch 289
Epoch 289 :: Batch 0/1
Batch Loss = 4.19620872935805
289, epoch_train_loss=4.19620872935805
Epoch 290
Epoch 290 :: Batch 0/1
Batch Loss = 4.196099609898375
290, epoch_train_loss=4.196099609898375
Epoch 291
Epoch 291 :: Batch 0/1
Batch Loss = 4.1959657709931495
291, epoch_train_loss=4.1959657709931495
Epoch 292
Epoch 292 :: Batch 0/1
Batch Loss = 4.195817253477464
292, epoch_train_loss=4.195817253477464
Epoch 293
Epoch 293 :: Batch 0/1
Batch Loss = 4.195660256153713
293, epoch_train_loss=4.195660256153713
Epoch 294
Epoch 294 :: Batch 0/1
Batch Loss = 4.195498700530657
294, epoch_train_loss=4.195498700530657
Epoch 295
Epoch 295 :: Batch 0/1
Batch Loss = 4.19533511682343
295, epoch_train_loss=4.19533511682343
Epoch 296
Epoch 296 :: Batch 0/1
Batch Loss = 4.195171162483347
296, epoch_train_loss=4.195171162483347
Epoch 297
Epoch 297 :: Batch 0/1
Batch Loss = 4.195007933346603
297, epoch_train_loss=4.195007933346603
Epoch 298
Epoch 298 :: Batch 0/1
Batch Loss = 4.194846154391515
298, epoch_train_loss=4.194846154391515
Epoch 299
Epoch 299 :: Batch 0/1
Batch Loss = 4.194686299486442
299, epoch_train_loss=4.194686299486442
Epoch 300
Epoch 300 :: Batch 0/1
Batch Loss = 4.194528668627193
300, epoch_train_loss=4.194528668627193
Epoch 301
Epoch 301 :: Batch 0/1
Batch Loss = 4.1943734393386425
301, epoch_train_loss=4.1943734393386425
Epoch 302
Epoch 302 :: Batch 0/1
Batch Loss = 4.194220702089874
302, epoch_train_loss=4.194220702089874
Epoch 303
Epoch 303 :: Batch 0/1
Batch Loss = 4.194070485508281
303, epoch_train_loss=4.194070485508281
Epoch 304
Epoch 304 :: Batch 0/1
Batch Loss = 4.193922774737891
304, epoch_train_loss=4.193922774737891
Epoch 305
Epoch 305 :: Batch 0/1
Batch Loss = 4.1937775249659675
305, epoch_train_loss=4.1937775249659675
Epoch 306
Epoch 306 :: Batch 0/1
Batch Loss = 4.19363467147944
306, epoch_train_loss=4.19363467147944
Epoch 307
Epoch 307 :: Batch 0/1
Batch Loss = 4.1934941372525065
307, epoch_train_loss=4.1934941372525065
Epoch 308
Epoch 308 :: Batch 0/1
Batch Loss = 4.193355838478348
308, epoch_train_loss=4.193355838478348
Epoch 309
Epoch 309 :: Batch 0/1
Batch Loss = 4.19321968862721
309, epoch_train_loss=4.19321968862721
Epoch 310
Epoch 310 :: Batch 0/1
Batch Loss = 4.193085601216037
310, epoch_train_loss=4.193085601216037
Epoch 311
Epoch 311 :: Batch 0/1
Batch Loss = 4.192953491402782
311, epoch_train_loss=4.192953491402782
Epoch 312
Epoch 312 :: Batch 0/1
Batch Loss = 4.192823276474736
312, epoch_train_loss=4.192823276474736
Epoch 313
Epoch 313 :: Batch 0/1
Batch Loss = 4.192694875035023
313, epoch_train_loss=4.192694875035023
Epoch 314
Epoch 314 :: Batch 0/1
Batch Loss = 4.192568205117728
314, epoch_train_loss=4.192568205117728
Epoch 315
Epoch 315 :: Batch 0/1
Batch Loss = 4.1924431811545455
315, epoch_train_loss=4.1924431811545455
Epoch 316
Epoch 316 :: Batch 0/1
Batch Loss = 4.192319710115384
316, epoch_train_loss=4.192319710115384
Epoch 317
Epoch 317 :: Batch 0/1
Batch Loss = 4.192197687562455
317, epoch_train_loss=4.192197687562455
Epoch 318
Epoch 318 :: Batch 0/1
Batch Loss = 4.192076994241169
318, epoch_train_loss=4.192076994241169
Epoch 319
Epoch 319 :: Batch 0/1
Batch Loss = 4.191957494094258
319, epoch_train_loss=4.191957494094258
Epoch 320
Epoch 320 :: Batch 0/1
Batch Loss = 4.191839034574803
320, epoch_train_loss=4.191839034574803
Epoch 321
Epoch 321 :: Batch 0/1
Batch Loss = 4.191721449296248
321, epoch_train_loss=4.191721449296248
Epoch 322
Epoch 322 :: Batch 0/1
Batch Loss = 4.191604562504425
322, epoch_train_loss=4.191604562504425
Epoch 323
Epoch 323 :: Batch 0/1
Batch Loss = 4.191488193980976
323, epoch_train_loss=4.191488193980976
Epoch 324
Epoch 324 :: Batch 0/1
Batch Loss = 4.191372162429909
324, epoch_train_loss=4.191372162429909
Epoch 325
Epoch 325 :: Batch 0/1
Batch Loss = 4.191256285425129
325, epoch_train_loss=4.191256285425129
Epoch 326
Epoch 326 :: Batch 0/1
Batch Loss = 4.191140375104236
326, epoch_train_loss=4.191140375104236
Epoch 327
Epoch 327 :: Batch 0/1
Batch Loss = 4.191024229890621
327, epoch_train_loss=4.191024229890621
Epoch 328
Epoch 328 :: Batch 0/1
Batch Loss = 4.190907624058151
328, epoch_train_loss=4.190907624058151
Epoch 329
Epoch 329 :: Batch 0/1
Batch Loss = 4.190790297310933
329, epoch_train_loss=4.190790297310933
Epoch 330
Epoch 330 :: Batch 0/1
Batch Loss = 4.190671946284312
330, epoch_train_loss=4.190671946284312
Epoch 331
Epoch 331 :: Batch 0/1
Batch Loss = 4.190552218869091
331, epoch_train_loss=4.190552218869091
Epoch 332
Epoch 332 :: Batch 0/1
Batch Loss = 4.190430710764398
332, epoch_train_loss=4.190430710764398
Epoch 333
Epoch 333 :: Batch 0/1
Batch Loss = 4.190306962721403
333, epoch_train_loss=4.190306962721403
Epoch 334
Epoch 334 :: Batch 0/1
Batch Loss = 4.190180456270763
334, epoch_train_loss=4.190180456270763
Epoch 335
Epoch 335 :: Batch 0/1
Batch Loss = 4.190050605871266
335, epoch_train_loss=4.190050605871266
Epoch 336
Epoch 336 :: Batch 0/1
Batch Loss = 4.189916745678927
336, epoch_train_loss=4.189916745678927
Epoch 337
Epoch 337 :: Batch 0/1
Batch Loss = 4.189778110043375
337, epoch_train_loss=4.189778110043375
Epoch 338
Epoch 338 :: Batch 0/1
Batch Loss = 4.189633807792907
338, epoch_train_loss=4.189633807792907
Epoch 339
Epoch 339 :: Batch 0/1
Batch Loss = 4.189482791446623
339, epoch_train_loss=4.189482791446623
Epoch 340
Epoch 340 :: Batch 0/1
Batch Loss = 4.1893238239801445
340, epoch_train_loss=4.1893238239801445
Epoch 341
Epoch 341 :: Batch 0/1
Batch Loss = 4.1891554469207595
341, epoch_train_loss=4.1891554469207595
Epoch 342
Epoch 342 :: Batch 0/1
Batch Loss = 4.188975957311274
342, epoch_train_loss=4.188975957311274
Epoch 343
Epoch 343 :: Batch 0/1
Batch Loss = 4.188783409367942
343, epoch_train_loss=4.188783409367942
Epoch 344
Epoch 344 :: Batch 0/1
Batch Loss = 4.1885756824619005
344, epoch_train_loss=4.1885756824619005
Epoch 345
Epoch 345 :: Batch 0/1
Batch Loss = 4.188350709924277
345, epoch_train_loss=4.188350709924277
Epoch 346
Epoch 346 :: Batch 0/1
Batch Loss = 4.188107029543422
346, epoch_train_loss=4.188107029543422
Epoch 347
Epoch 347 :: Batch 0/1
Batch Loss = 4.187843824592427
347, epoch_train_loss=4.187843824592427
Epoch 348
Epoch 348 :: Batch 0/1
Batch Loss = 2351.734836810379
348, epoch_train_loss=2351.734836810379
Epoch 349
Epoch 349 :: Batch 0/1
Batch Loss = 4.187838491998322
349, epoch_train_loss=4.187838491998322
Epoch 350
Epoch 350 :: Batch 0/1
Batch Loss = 4.188293971445205
350, epoch_train_loss=4.188293971445205
Epoch 351
Epoch 351 :: Batch 0/1
Batch Loss = 4.188752862870654
351, epoch_train_loss=4.188752862870654
Epoch 352
Epoch 352 :: Batch 0/1
Batch Loss = 4.189126666711651
352, epoch_train_loss=4.189126666711651
Epoch 353
Epoch 353 :: Batch 0/1
Batch Loss = 4.18939727303927
353, epoch_train_loss=4.18939727303927
Epoch 354
Epoch 354 :: Batch 0/1
Batch Loss = 4.189576514222547
354, epoch_train_loss=4.189576514222547
Epoch 355
Epoch 355 :: Batch 0/1
Batch Loss = 4.189687176192858
355, epoch_train_loss=4.189687176192858
Epoch 356
Epoch 356 :: Batch 0/1
Batch Loss = 4.189751049826378
356, epoch_train_loss=4.189751049826378
Epoch 357
Epoch 357 :: Batch 0/1
Batch Loss = 4.189784035626077
357, epoch_train_loss=4.189784035626077
Epoch 358
Epoch 358 :: Batch 0/1
Batch Loss = 4.189796093520116
358, epoch_train_loss=4.189796093520116
Epoch 359
Epoch 359 :: Batch 0/1
Batch Loss = 4.1897928751307045
359, epoch_train_loss=4.1897928751307045
Epoch 360
Epoch 360 :: Batch 0/1
Batch Loss = 4.189777300626521
360, epoch_train_loss=4.189777300626521
Epoch 361
Epoch 361 :: Batch 0/1
Batch Loss = 4.189750599825971
361, epoch_train_loss=4.189750599825971
Epoch 362
Epoch 362 :: Batch 0/1
Batch Loss = 4.189712867617233
362, epoch_train_loss=4.189712867617233
Epoch 363
Epoch 363 :: Batch 0/1
Batch Loss = 4.189663290264026
363, epoch_train_loss=4.189663290264026
Epoch 364
Epoch 364 :: Batch 0/1
Batch Loss = 4.189600165973996
364, epoch_train_loss=4.189600165973996
Epoch 365
Epoch 365 :: Batch 0/1
Batch Loss = 4.189520798286495
365, epoch_train_loss=4.189520798286495
Epoch 366
Epoch 366 :: Batch 0/1
Batch Loss = 4.1894213275190335
366, epoch_train_loss=4.1894213275190335
Epoch 367
Epoch 367 :: Batch 0/1
Batch Loss = 4.189296608783538
367, epoch_train_loss=4.189296608783538
Epoch 368
Epoch 368 :: Batch 0/1
Batch Loss = 4.18914039182122
368, epoch_train_loss=4.18914039182122
Epoch 369
Epoch 369 :: Batch 0/1
Batch Loss = 4.188946410448122
369, epoch_train_loss=4.188946410448122
Epoch 370
Epoch 370 :: Batch 0/1
Batch Loss = 4.188711718630422
370, epoch_train_loss=4.188711718630422
Epoch 371
Epoch 371 :: Batch 0/1
Batch Loss = 4.1884448112672334
371, epoch_train_loss=4.1884448112672334
Epoch 372
Epoch 372 :: Batch 0/1
Batch Loss = 4.188181867591066
372, epoch_train_loss=4.188181867591066
Epoch 373
Epoch 373 :: Batch 0/1
Batch Loss = 4.188009031505865
373, epoch_train_loss=4.188009031505865
Epoch 374
Epoch 374 :: Batch 0/1
Batch Loss = 4.188048452983214
374, epoch_train_loss=4.188048452983214
Epoch 375
Epoch 375 :: Batch 0/1
Batch Loss = 4.1882487228677086
375, epoch_train_loss=4.1882487228677086
Epoch 376
Epoch 376 :: Batch 0/1
Batch Loss = 4.188246603294709
376, epoch_train_loss=4.188246603294709
Epoch 377
Epoch 377 :: Batch 0/1
Batch Loss = 4.18798324224391
377, epoch_train_loss=4.18798324224391
Epoch 378
Epoch 378 :: Batch 0/1
Batch Loss = 4.187689954391522
378, epoch_train_loss=4.187689954391522
Epoch 379
Epoch 379 :: Batch 0/1
Batch Loss = 4.187524485353387
379, epoch_train_loss=4.187524485353387
Epoch 380
Epoch 380 :: Batch 0/1
Batch Loss = 4.187481179208751
380, epoch_train_loss=4.187481179208751
Epoch 381
Epoch 381 :: Batch 0/1
Batch Loss = 4.187481110617023
381, epoch_train_loss=4.187481110617023
Epoch 382
Epoch 382 :: Batch 0/1
Batch Loss = 4.187461586570247
382, epoch_train_loss=4.187461586570247
Epoch 383
Epoch 383 :: Batch 0/1
Batch Loss = 4.187393886593443
383, epoch_train_loss=4.187393886593443
Epoch 384
Epoch 384 :: Batch 0/1
Batch Loss = 4.187270861614773
384, epoch_train_loss=4.187270861614773
Epoch 385
Epoch 385 :: Batch 0/1
Batch Loss = 4.187098442770268
385, epoch_train_loss=4.187098442770268
Epoch 386
Epoch 386 :: Batch 0/1
Batch Loss = 4.186896515617861
386, epoch_train_loss=4.186896515617861
Epoch 387
Epoch 387 :: Batch 0/1
Batch Loss = 4.1867026930836655
387, epoch_train_loss=4.1867026930836655
Epoch 388
Epoch 388 :: Batch 0/1
Batch Loss = 4.186560961104865
388, epoch_train_loss=4.186560961104865
Epoch 389
Epoch 389 :: Batch 0/1
Batch Loss = 4.1864720786319
389, epoch_train_loss=4.1864720786319
Epoch 390
Epoch 390 :: Batch 0/1
Batch Loss = 4.18634949492356
390, epoch_train_loss=4.18634949492356
Epoch 391
Epoch 391 :: Batch 0/1
Batch Loss = 4.186107493246167
391, epoch_train_loss=4.186107493246167
Epoch 392
Epoch 392 :: Batch 0/1
Batch Loss = 4.185768565493076
392, epoch_train_loss=4.185768565493076
Epoch 393
Epoch 393 :: Batch 0/1
Batch Loss = 4.185393076333671
393, epoch_train_loss=4.185393076333671
Epoch 394
Epoch 394 :: Batch 0/1
Batch Loss = 4.184980606601499
394, epoch_train_loss=4.184980606601499
Epoch 395
Epoch 395 :: Batch 0/1
Batch Loss = 4.184477327431636
395, epoch_train_loss=4.184477327431636
Epoch 396
Epoch 396 :: Batch 0/1
Batch Loss = 4.183883513112376
396, epoch_train_loss=4.183883513112376
Epoch 397
Epoch 397 :: Batch 0/1
Batch Loss = 4.1835165408311035
397, epoch_train_loss=4.1835165408311035
Epoch 398
Epoch 398 :: Batch 0/1
Batch Loss = 4.184069833434669
398, epoch_train_loss=4.184069833434669
Epoch 399
Epoch 399 :: Batch 0/1
Batch Loss = 4.184026994678068
399, epoch_train_loss=4.184026994678068
Epoch 400
Epoch 400 :: Batch 0/1
Batch Loss = 4.183379319597971
400, epoch_train_loss=4.183379319597971
Epoch 401
Epoch 401 :: Batch 0/1
Batch Loss = 4.182960346738614
401, epoch_train_loss=4.182960346738614
Epoch 402
Epoch 402 :: Batch 0/1
Batch Loss = 4.182881111052564
402, epoch_train_loss=4.182881111052564
Epoch 403
Epoch 403 :: Batch 0/1
Batch Loss = 4.1829042287630545
403, epoch_train_loss=4.1829042287630545
Epoch 404
Epoch 404 :: Batch 0/1
Batch Loss = 4.182888825665948
404, epoch_train_loss=4.182888825665948
Epoch 405
Epoch 405 :: Batch 0/1
Batch Loss = 4.182791373200891
405, epoch_train_loss=4.182791373200891
Epoch 406
Epoch 406 :: Batch 0/1
Batch Loss = 4.182594947672332
406, epoch_train_loss=4.182594947672332
Epoch 407
Epoch 407 :: Batch 0/1
Batch Loss = 4.182306337587413
407, epoch_train_loss=4.182306337587413
Epoch 408
Epoch 408 :: Batch 0/1
Batch Loss = 4.181971952178624
408, epoch_train_loss=4.181971952178624
Epoch 409
Epoch 409 :: Batch 0/1
Batch Loss = 4.181681413736792
409, epoch_train_loss=4.181681413736792
Epoch 410
Epoch 410 :: Batch 0/1
Batch Loss = 4.181521636373855
410, epoch_train_loss=4.181521636373855
Epoch 411
Epoch 411 :: Batch 0/1
Batch Loss = 4.181434724768755
411, epoch_train_loss=4.181434724768755
Epoch 412
Epoch 412 :: Batch 0/1
Batch Loss = 4.181188158056961
412, epoch_train_loss=4.181188158056961
Epoch 413
Epoch 413 :: Batch 0/1
Batch Loss = 4.180680454654307
413, epoch_train_loss=4.180680454654307
Epoch 414
Epoch 414 :: Batch 0/1
Batch Loss = 4.180002266740203
414, epoch_train_loss=4.180002266740203
Epoch 415
Epoch 415 :: Batch 0/1
Batch Loss = 4.179226763174594
415, epoch_train_loss=4.179226763174594
Epoch 416
Epoch 416 :: Batch 0/1
Batch Loss = 4.178562932240649
416, epoch_train_loss=4.178562932240649
Epoch 417
Epoch 417 :: Batch 0/1
Batch Loss = 4.1787725483657105
417, epoch_train_loss=4.1787725483657105
Epoch 418
Epoch 418 :: Batch 0/1
Batch Loss = 4.179464056464287
418, epoch_train_loss=4.179464056464287
Epoch 419
Epoch 419 :: Batch 0/1
Batch Loss = 4.178976912326755
419, epoch_train_loss=4.178976912326755
Epoch 420
Epoch 420 :: Batch 0/1
Batch Loss = 4.178244094759942
420, epoch_train_loss=4.178244094759942
Epoch 421
Epoch 421 :: Batch 0/1
Batch Loss = 4.177951139610786
421, epoch_train_loss=4.177951139610786
Epoch 422
Epoch 422 :: Batch 0/1
Batch Loss = 4.177958929679214
422, epoch_train_loss=4.177958929679214
Epoch 423
Epoch 423 :: Batch 0/1
Batch Loss = 4.177974052446838
423, epoch_train_loss=4.177974052446838
Epoch 424
Epoch 424 :: Batch 0/1
Batch Loss = 4.177815772198148
424, epoch_train_loss=4.177815772198148
Epoch 425
Epoch 425 :: Batch 0/1
Batch Loss = 4.177586639226732
425, epoch_train_loss=4.177586639226732
Epoch 426
Epoch 426 :: Batch 0/1
Batch Loss = 4.177514868091972
426, epoch_train_loss=4.177514868091972
Epoch 427
Epoch 427 :: Batch 0/1
Batch Loss = 4.177007638823784
427, epoch_train_loss=4.177007638823784
Epoch 428
Epoch 428 :: Batch 0/1
Batch Loss = 4.176868228955302
428, epoch_train_loss=4.176868228955302
Epoch 429
Epoch 429 :: Batch 0/1
Batch Loss = 4.176810543122819
429, epoch_train_loss=4.176810543122819
Epoch 430
Epoch 430 :: Batch 0/1
Batch Loss = 4.176835359297357
430, epoch_train_loss=4.176835359297357
Epoch 431
Epoch 431 :: Batch 0/1
Batch Loss = 4.176785215459272
431, epoch_train_loss=4.176785215459272
Epoch 432
Epoch 432 :: Batch 0/1
Batch Loss = 4.176530639013104
432, epoch_train_loss=4.176530639013104
Epoch 433
Epoch 433 :: Batch 0/1
Batch Loss = 4.176230149501798
433, epoch_train_loss=4.176230149501798
Epoch 434
Epoch 434 :: Batch 0/1
Batch Loss = 4.176182824624458
434, epoch_train_loss=4.176182824624458
Epoch 435
Epoch 435 :: Batch 0/1
Batch Loss = 4.176158009958285
435, epoch_train_loss=4.176158009958285
Epoch 436
Epoch 436 :: Batch 0/1
Batch Loss = 4.175950384291662
436, epoch_train_loss=4.175950384291662
Epoch 437
Epoch 437 :: Batch 0/1
Batch Loss = 4.175864157238011
437, epoch_train_loss=4.175864157238011
Epoch 438
Epoch 438 :: Batch 0/1
Batch Loss = 4.175824041011892
438, epoch_train_loss=4.175824041011892
Epoch 439
Epoch 439 :: Batch 0/1
Batch Loss = 4.175702391544947
439, epoch_train_loss=4.175702391544947
Epoch 440
Epoch 440 :: Batch 0/1
Batch Loss = 4.175473552294584
440, epoch_train_loss=4.175473552294584
Epoch 441
Epoch 441 :: Batch 0/1
Batch Loss = 4.175154452297608
441, epoch_train_loss=4.175154452297608
Epoch 442
Epoch 442 :: Batch 0/1
Batch Loss = 4.1748529570749175
442, epoch_train_loss=4.1748529570749175
Epoch 443
Epoch 443 :: Batch 0/1
Batch Loss = 4.174857860763198
443, epoch_train_loss=4.174857860763198
Epoch 444
Epoch 444 :: Batch 0/1
Batch Loss = 4.174226892705315
444, epoch_train_loss=4.174226892705315
Epoch 445
Epoch 445 :: Batch 0/1
Batch Loss = 4.174008350699424
445, epoch_train_loss=4.174008350699424
Epoch 446
Epoch 446 :: Batch 0/1
Batch Loss = 4.173340664935324
446, epoch_train_loss=4.173340664935324
Epoch 447
Epoch 447 :: Batch 0/1
Batch Loss = 4.173558252403504
447, epoch_train_loss=4.173558252403504
Epoch 448
Epoch 448 :: Batch 0/1
Batch Loss = 4.173337500414297
448, epoch_train_loss=4.173337500414297
Epoch 449
Epoch 449 :: Batch 0/1
Batch Loss = 4.173511298702781
449, epoch_train_loss=4.173511298702781
Epoch 450
Epoch 450 :: Batch 0/1
Batch Loss = 4.172342799390687
450, epoch_train_loss=4.172342799390687
Epoch 451
Epoch 451 :: Batch 0/1
Batch Loss = 4.174326502119151
451, epoch_train_loss=4.174326502119151
Epoch 452
Epoch 452 :: Batch 0/1
Batch Loss = 4.172392112093673
452, epoch_train_loss=4.172392112093673
Epoch 453
Epoch 453 :: Batch 0/1
Batch Loss = 4.173745566254925
453, epoch_train_loss=4.173745566254925
Epoch 454
Epoch 454 :: Batch 0/1
Batch Loss = 4.173995141404906
454, epoch_train_loss=4.173995141404906
Epoch 455
Epoch 455 :: Batch 0/1
Batch Loss = 4.17334915822438
455, epoch_train_loss=4.17334915822438
Epoch 456
Epoch 456 :: Batch 0/1
Batch Loss = 4.1717022058697895
456, epoch_train_loss=4.1717022058697895
Epoch 457
Epoch 457 :: Batch 0/1
Batch Loss = 4.1737094191702235
457, epoch_train_loss=4.1737094191702235
Epoch 458
Epoch 458 :: Batch 0/1
Batch Loss = 4.171219513937446
458, epoch_train_loss=4.171219513937446
Epoch 459
Epoch 459 :: Batch 0/1
Batch Loss = 4.171944166949004
459, epoch_train_loss=4.171944166949004
Epoch 460
Epoch 460 :: Batch 0/1
Batch Loss = 4.172277989479603
460, epoch_train_loss=4.172277989479603
Epoch 461
Epoch 461 :: Batch 0/1
Batch Loss = 4.1715611455295045
461, epoch_train_loss=4.1715611455295045
Epoch 462
Epoch 462 :: Batch 0/1
Batch Loss = 4.170703280977581
462, epoch_train_loss=4.170703280977581
Epoch 463
Epoch 463 :: Batch 0/1
Batch Loss = 4.171884422560694
463, epoch_train_loss=4.171884422560694
Epoch 464
Epoch 464 :: Batch 0/1
Batch Loss = 4.170357522729504
464, epoch_train_loss=4.170357522729504
Epoch 465
Epoch 465 :: Batch 0/1
Batch Loss = 4.170814642727249
465, epoch_train_loss=4.170814642727249
Epoch 466
Epoch 466 :: Batch 0/1
Batch Loss = 4.170834673833788
466, epoch_train_loss=4.170834673833788
Epoch 467
Epoch 467 :: Batch 0/1
Batch Loss = 4.170020464885307
467, epoch_train_loss=4.170020464885307
Epoch 468
Epoch 468 :: Batch 0/1
Batch Loss = 4.16988171486251
468, epoch_train_loss=4.16988171486251
Epoch 469
Epoch 469 :: Batch 0/1
Batch Loss = 4.1698303534011405
469, epoch_train_loss=4.1698303534011405
Epoch 470
Epoch 470 :: Batch 0/1
Batch Loss = 4.169131882803229
470, epoch_train_loss=4.169131882803229
Epoch 471
Epoch 471 :: Batch 0/1
Batch Loss = 4.169294518158311
471, epoch_train_loss=4.169294518158311
Epoch 472
Epoch 472 :: Batch 0/1
Batch Loss = 4.168575717307149
472, epoch_train_loss=4.168575717307149
Epoch 473
Epoch 473 :: Batch 0/1
Batch Loss = 4.1679425436759105
473, epoch_train_loss=4.1679425436759105
Epoch 474
Epoch 474 :: Batch 0/1
Batch Loss = 4.167221894960547
474, epoch_train_loss=4.167221894960547
Epoch 475
Epoch 475 :: Batch 0/1
Batch Loss = 4.166938202206899
475, epoch_train_loss=4.166938202206899
Epoch 476
Epoch 476 :: Batch 0/1
Batch Loss = 4.164801689077553
476, epoch_train_loss=4.164801689077553
Epoch 477
Epoch 477 :: Batch 0/1
Batch Loss = 4.193858588189648
477, epoch_train_loss=4.193858588189648
Epoch 478
Epoch 478 :: Batch 0/1
Batch Loss = 4.173123051210194
478, epoch_train_loss=4.173123051210194
Epoch 479
Epoch 479 :: Batch 0/1
Batch Loss = 4.174388285548832
479, epoch_train_loss=4.174388285548832
Epoch 480
Epoch 480 :: Batch 0/1
Batch Loss = 4.174544440294532
480, epoch_train_loss=4.174544440294532
Epoch 481
Epoch 481 :: Batch 0/1
Batch Loss = 4.174530257985727
481, epoch_train_loss=4.174530257985727
Epoch 482
Epoch 482 :: Batch 0/1
Batch Loss = 4.174516056646213
482, epoch_train_loss=4.174516056646213
Epoch 483
Epoch 483 :: Batch 0/1
Batch Loss = 4.174528120221756
483, epoch_train_loss=4.174528120221756
Epoch 484
Epoch 484 :: Batch 0/1
Batch Loss = 4.174516978597597
484, epoch_train_loss=4.174516978597597
Epoch 485
Epoch 485 :: Batch 0/1
Batch Loss = 4.174470778890935
485, epoch_train_loss=4.174470778890935
Epoch 486
Epoch 486 :: Batch 0/1
Batch Loss = 4.174429618072921
486, epoch_train_loss=4.174429618072921
Epoch 487
Epoch 487 :: Batch 0/1
Batch Loss = 4.174408948033107
487, epoch_train_loss=4.174408948033107
Epoch 488
Epoch 488 :: Batch 0/1
Batch Loss = 4.174385683580232
488, epoch_train_loss=4.174385683580232
Epoch 489
Epoch 489 :: Batch 0/1
Batch Loss = 4.1743432817576425
489, epoch_train_loss=4.1743432817576425
Epoch 490
Epoch 490 :: Batch 0/1
Batch Loss = 4.17429426266694
490, epoch_train_loss=4.17429426266694
Epoch 491
Epoch 491 :: Batch 0/1
Batch Loss = 4.174257756027135
491, epoch_train_loss=4.174257756027135
Epoch 492
Epoch 492 :: Batch 0/1
Batch Loss = 4.174228569715193
492, epoch_train_loss=4.174228569715193
Epoch 493
Epoch 493 :: Batch 0/1
Batch Loss = 4.174187261420206
493, epoch_train_loss=4.174187261420206
Epoch 494
Epoch 494 :: Batch 0/1
Batch Loss = 4.174134616058712
494, epoch_train_loss=4.174134616058712
Epoch 495
Epoch 495 :: Batch 0/1
Batch Loss = 4.174086514895347
495, epoch_train_loss=4.174086514895347
Epoch 496
Epoch 496 :: Batch 0/1
Batch Loss = 4.174044589263935
496, epoch_train_loss=4.174044589263935
Epoch 497
Epoch 497 :: Batch 0/1
Batch Loss = 4.1739969135716155
497, epoch_train_loss=4.1739969135716155
Epoch 498
Epoch 498 :: Batch 0/1
Batch Loss = 4.173939047775988
498, epoch_train_loss=4.173939047775988
Epoch 499
Epoch 499 :: Batch 0/1
Batch Loss = 4.173879396225091
499, epoch_train_loss=4.173879396225091
Epoch 500
Epoch 500 :: Batch 0/1
Batch Loss = 4.1738245746237475
500, epoch_train_loss=4.1738245746237475
Epoch 501
Epoch 501 :: Batch 0/1
Batch Loss = 4.173769187117117
501, epoch_train_loss=4.173769187117117
Epoch 502
Epoch 502 :: Batch 0/1
Batch Loss = 4.173706191415208
502, epoch_train_loss=4.173706191415208
Epoch 503
Epoch 503 :: Batch 0/1
Batch Loss = 4.17363851213153
503, epoch_train_loss=4.17363851213153
Epoch 504
Epoch 504 :: Batch 0/1
Batch Loss = 4.173571662417394
504, epoch_train_loss=4.173571662417394
Epoch 505
Epoch 505 :: Batch 0/1
Batch Loss = 4.17350379686118
505, epoch_train_loss=4.17350379686118
Epoch 506
Epoch 506 :: Batch 0/1
Batch Loss = 4.1734301888623015
506, epoch_train_loss=4.1734301888623015
Epoch 507
Epoch 507 :: Batch 0/1
Batch Loss = 4.17335104182997
507, epoch_train_loss=4.17335104182997
Epoch 508
Epoch 508 :: Batch 0/1
Batch Loss = 4.173269235714161
508, epoch_train_loss=4.173269235714161
Epoch 509
Epoch 509 :: Batch 0/1
Batch Loss = 4.173183585456694
509, epoch_train_loss=4.173183585456694
Epoch 510
Epoch 510 :: Batch 0/1
Batch Loss = 4.173089723872269
510, epoch_train_loss=4.173089723872269
Epoch 511
Epoch 511 :: Batch 0/1
Batch Loss = 4.172985487472796
511, epoch_train_loss=4.172985487472796
Epoch 512
Epoch 512 :: Batch 0/1
Batch Loss = 4.172869327848376
512, epoch_train_loss=4.172869327848376
Epoch 513
Epoch 513 :: Batch 0/1
Batch Loss = 4.172734109764102
513, epoch_train_loss=4.172734109764102
Epoch 514
Epoch 514 :: Batch 0/1
Batch Loss = 4.172563741369238
514, epoch_train_loss=4.172563741369238
Epoch 515
Epoch 515 :: Batch 0/1
Batch Loss = 4.172324726232088
515, epoch_train_loss=4.172324726232088
Epoch 516
Epoch 516 :: Batch 0/1
Batch Loss = 4.171932537503275
516, epoch_train_loss=4.171932537503275
Epoch 517
Epoch 517 :: Batch 0/1
Batch Loss = 4.171207203299258
517, epoch_train_loss=4.171207203299258
Epoch 518
Epoch 518 :: Batch 0/1
Batch Loss = 4.170095509232778
518, epoch_train_loss=4.170095509232778
Epoch 519
Epoch 519 :: Batch 0/1
Batch Loss = 4.169600898790523
519, epoch_train_loss=4.169600898790523
Epoch 520
Epoch 520 :: Batch 0/1
Batch Loss = 4.170795579281952
520, epoch_train_loss=4.170795579281952
Epoch 521
Epoch 521 :: Batch 0/1
Batch Loss = 4.17009225648247
521, epoch_train_loss=4.17009225648247
Epoch 522
Epoch 522 :: Batch 0/1
Batch Loss = 4.169405356590657
522, epoch_train_loss=4.169405356590657
Epoch 523
Epoch 523 :: Batch 0/1
Batch Loss = 4.1693908625689575
523, epoch_train_loss=4.1693908625689575
Epoch 524
Epoch 524 :: Batch 0/1
Batch Loss = 4.1695569654836255
524, epoch_train_loss=4.1695569654836255
Epoch 525
Epoch 525 :: Batch 0/1
Batch Loss = 4.169644731760155
525, epoch_train_loss=4.169644731760155
Epoch 526
Epoch 526 :: Batch 0/1
Batch Loss = 4.169619509322021
526, epoch_train_loss=4.169619509322021
Epoch 527
Epoch 527 :: Batch 0/1
Batch Loss = 4.16940295738053
527, epoch_train_loss=4.16940295738053
Epoch 528
Epoch 528 :: Batch 0/1
Batch Loss = 4.1690342432345195
528, epoch_train_loss=4.1690342432345195
Epoch 529
Epoch 529 :: Batch 0/1
Batch Loss = 4.1687366145918165
529, epoch_train_loss=4.1687366145918165
Epoch 530
Epoch 530 :: Batch 0/1
Batch Loss = 4.168704378190781
530, epoch_train_loss=4.168704378190781
Epoch 531
Epoch 531 :: Batch 0/1
Batch Loss = 4.168849482622813
531, epoch_train_loss=4.168849482622813
Epoch 532
Epoch 532 :: Batch 0/1
Batch Loss = 4.168802735382583
532, epoch_train_loss=4.168802735382583
Epoch 533
Epoch 533 :: Batch 0/1
Batch Loss = 4.168521245252641
533, epoch_train_loss=4.168521245252641
Epoch 534
Epoch 534 :: Batch 0/1
Batch Loss = 4.168287687822384
534, epoch_train_loss=4.168287687822384
Epoch 535
Epoch 535 :: Batch 0/1
Batch Loss = 4.168218874638565
535, epoch_train_loss=4.168218874638565
Epoch 536
Epoch 536 :: Batch 0/1
Batch Loss = 4.1682293451975445
536, epoch_train_loss=4.1682293451975445
Epoch 537
Epoch 537 :: Batch 0/1
Batch Loss = 4.168202787913247
537, epoch_train_loss=4.168202787913247
Epoch 538
Epoch 538 :: Batch 0/1
Batch Loss = 4.168084369201547
538, epoch_train_loss=4.168084369201547
Epoch 539
Epoch 539 :: Batch 0/1
Batch Loss = 4.167899473323863
539, epoch_train_loss=4.167899473323863
Epoch 540
Epoch 540 :: Batch 0/1
Batch Loss = 4.16772718346796
540, epoch_train_loss=4.16772718346796
Epoch 541
Epoch 541 :: Batch 0/1
Batch Loss = 4.167638511048384
541, epoch_train_loss=4.167638511048384
Epoch 542
Epoch 542 :: Batch 0/1
Batch Loss = 4.16761279423234
542, epoch_train_loss=4.16761279423234
Epoch 543
Epoch 543 :: Batch 0/1
Batch Loss = 4.167543273952995
543, epoch_train_loss=4.167543273952995
Epoch 544
Epoch 544 :: Batch 0/1
Batch Loss = 4.167394678270874
544, epoch_train_loss=4.167394678270874
Epoch 545
Epoch 545 :: Batch 0/1
Batch Loss = 4.167245891751686
545, epoch_train_loss=4.167245891751686
Epoch 546
Epoch 546 :: Batch 0/1
Batch Loss = 4.167151721666825
546, epoch_train_loss=4.167151721666825
Epoch 547
Epoch 547 :: Batch 0/1
Batch Loss = 4.167088999300555
547, epoch_train_loss=4.167088999300555
Epoch 548
Epoch 548 :: Batch 0/1
Batch Loss = 4.167012155559053
548, epoch_train_loss=4.167012155559053
Epoch 549
Epoch 549 :: Batch 0/1
Batch Loss = 4.166898170779705
549, epoch_train_loss=4.166898170779705
Epoch 550
Epoch 550 :: Batch 0/1
Batch Loss = 4.16676266633955
550, epoch_train_loss=4.16676266633955
Epoch 551
Epoch 551 :: Batch 0/1
Batch Loss = 4.166644608766224
551, epoch_train_loss=4.166644608766224
Epoch 552
Epoch 552 :: Batch 0/1
Batch Loss = 4.166559724120836
552, epoch_train_loss=4.166559724120836
Epoch 553
Epoch 553 :: Batch 0/1
Batch Loss = 4.166479485486448
553, epoch_train_loss=4.166479485486448
Epoch 554
Epoch 554 :: Batch 0/1
Batch Loss = 4.166371344801688
554, epoch_train_loss=4.166371344801688
Epoch 555
Epoch 555 :: Batch 0/1
Batch Loss = 4.166244511708839
555, epoch_train_loss=4.166244511708839
Epoch 556
Epoch 556 :: Batch 0/1
Batch Loss = 4.166128441961394
556, epoch_train_loss=4.166128441961394
Epoch 557
Epoch 557 :: Batch 0/1
Batch Loss = 4.16603024121954
557, epoch_train_loss=4.16603024121954
Epoch 558
Epoch 558 :: Batch 0/1
Batch Loss = 4.165934294256284
558, epoch_train_loss=4.165934294256284
Epoch 559
Epoch 559 :: Batch 0/1
Batch Loss = 4.165823724087755
559, epoch_train_loss=4.165823724087755
Epoch 560
Epoch 560 :: Batch 0/1
Batch Loss = 4.165698918255993
560, epoch_train_loss=4.165698918255993
Epoch 561
Epoch 561 :: Batch 0/1
Batch Loss = 4.165578176037607
561, epoch_train_loss=4.165578176037607
Epoch 562
Epoch 562 :: Batch 0/1
Batch Loss = 4.165471745709208
562, epoch_train_loss=4.165471745709208
Epoch 563
Epoch 563 :: Batch 0/1
Batch Loss = 4.1653647019474604
563, epoch_train_loss=4.1653647019474604
Epoch 564
Epoch 564 :: Batch 0/1
Batch Loss = 4.16524285101262
564, epoch_train_loss=4.16524285101262
Epoch 565
Epoch 565 :: Batch 0/1
Batch Loss = 4.165115414132089
565, epoch_train_loss=4.165115414132089
Epoch 566
Epoch 566 :: Batch 0/1
Batch Loss = 4.16499495709578
566, epoch_train_loss=4.16499495709578
Epoch 567
Epoch 567 :: Batch 0/1
Batch Loss = 4.164879489105769
567, epoch_train_loss=4.164879489105769
Epoch 568
Epoch 568 :: Batch 0/1
Batch Loss = 4.164760097540474
568, epoch_train_loss=4.164760097540474
Epoch 569
Epoch 569 :: Batch 0/1
Batch Loss = 4.164632916623608
569, epoch_train_loss=4.164632916623608
Epoch 570
Epoch 570 :: Batch 0/1
Batch Loss = 4.164503588382432
570, epoch_train_loss=4.164503588382432
Epoch 571
Epoch 571 :: Batch 0/1
Batch Loss = 4.16437919051561
571, epoch_train_loss=4.16437919051561
Epoch 572
Epoch 572 :: Batch 0/1
Batch Loss = 4.164256930810768
572, epoch_train_loss=4.164256930810768
Epoch 573
Epoch 573 :: Batch 0/1
Batch Loss = 4.164129726509524
573, epoch_train_loss=4.164129726509524
Epoch 574
Epoch 574 :: Batch 0/1
Batch Loss = 4.163998704541153
574, epoch_train_loss=4.163998704541153
Epoch 575
Epoch 575 :: Batch 0/1
Batch Loss = 4.163870063627119
575, epoch_train_loss=4.163870063627119
Epoch 576
Epoch 576 :: Batch 0/1
Batch Loss = 4.163744540696988
576, epoch_train_loss=4.163744540696988
Epoch 577
Epoch 577 :: Batch 0/1
Batch Loss = 4.163617336032626
577, epoch_train_loss=4.163617336032626
Epoch 578
Epoch 578 :: Batch 0/1
Batch Loss = 4.16348635602534
578, epoch_train_loss=4.16348635602534
Epoch 579
Epoch 579 :: Batch 0/1
Batch Loss = 4.163355664968859
579, epoch_train_loss=4.163355664968859
Epoch 580
Epoch 580 :: Batch 0/1
Batch Loss = 4.163227978630766
580, epoch_train_loss=4.163227978630766
Epoch 581
Epoch 581 :: Batch 0/1
Batch Loss = 4.163099924120438
581, epoch_train_loss=4.163099924120438
Epoch 582
Epoch 582 :: Batch 0/1
Batch Loss = 4.162969022311522
582, epoch_train_loss=4.162969022311522
Epoch 583
Epoch 583 :: Batch 0/1
Batch Loss = 4.162837643761623
583, epoch_train_loss=4.162837643761623
Epoch 584
Epoch 584 :: Batch 0/1
Batch Loss = 4.162707837669299
584, epoch_train_loss=4.162707837669299
Epoch 585
Epoch 585 :: Batch 0/1
Batch Loss = 4.162577819074984
585, epoch_train_loss=4.162577819074984
Epoch 586
Epoch 586 :: Batch 0/1
Batch Loss = 4.162445375348525
586, epoch_train_loss=4.162445375348525
Epoch 587
Epoch 587 :: Batch 0/1
Batch Loss = 4.162311873950557
587, epoch_train_loss=4.162311873950557
Epoch 588
Epoch 588 :: Batch 0/1
Batch Loss = 4.162179009866453
588, epoch_train_loss=4.162179009866453
Epoch 589
Epoch 589 :: Batch 0/1
Batch Loss = 4.162045357750645
589, epoch_train_loss=4.162045357750645
Epoch 590
Epoch 590 :: Batch 0/1
Batch Loss = 4.161909429149195
590, epoch_train_loss=4.161909429149195
Epoch 591
Epoch 591 :: Batch 0/1
Batch Loss = 4.16177203172605
591, epoch_train_loss=4.16177203172605
Epoch 592
Epoch 592 :: Batch 0/1
Batch Loss = 4.161634228684068
592, epoch_train_loss=4.161634228684068
Epoch 593
Epoch 593 :: Batch 0/1
Batch Loss = 4.161494980616358
593, epoch_train_loss=4.161494980616358
Epoch 594
Epoch 594 :: Batch 0/1
Batch Loss = 4.16135300826488
594, epoch_train_loss=4.16135300826488
Epoch 595
Epoch 595 :: Batch 0/1
Batch Loss = 4.1612093321469645
595, epoch_train_loss=4.1612093321469645
Epoch 596
Epoch 596 :: Batch 0/1
Batch Loss = 4.1610644742322
596, epoch_train_loss=4.1610644742322
Epoch 597
Epoch 597 :: Batch 0/1
Batch Loss = 4.160917146443206
597, epoch_train_loss=4.160917146443206
Epoch 598
Epoch 598 :: Batch 0/1
Batch Loss = 4.1607670367842475
598, epoch_train_loss=4.1607670367842475
Epoch 599
Epoch 599 :: Batch 0/1
Batch Loss = 4.160614858705479
599, epoch_train_loss=4.160614858705479
Epoch 600
Epoch 600 :: Batch 0/1
Batch Loss = 4.160460532117936
600, epoch_train_loss=4.160460532117936
Epoch 601
Epoch 601 :: Batch 0/1
Batch Loss = 4.1603032049555395
601, epoch_train_loss=4.1603032049555395
Epoch 602
Epoch 602 :: Batch 0/1
Batch Loss = 4.160142998365825
602, epoch_train_loss=4.160142998365825
Epoch 603
Epoch 603 :: Batch 0/1
Batch Loss = 4.159980495434835
603, epoch_train_loss=4.159980495434835
Epoch 604
Epoch 604 :: Batch 0/1
Batch Loss = 4.159815263827583
604, epoch_train_loss=4.159815263827583
Epoch 605
Epoch 605 :: Batch 0/1
Batch Loss = 4.159646973831083
605, epoch_train_loss=4.159646973831083
Epoch 606
Epoch 606 :: Batch 0/1
Batch Loss = 4.159476159630476
606, epoch_train_loss=4.159476159630476
Epoch 607
Epoch 607 :: Batch 0/1
Batch Loss = 4.159303045142912
607, epoch_train_loss=4.159303045142912
Epoch 608
Epoch 608 :: Batch 0/1
Batch Loss = 4.159127296922446
608, epoch_train_loss=4.159127296922446
Epoch 609
Epoch 609 :: Batch 0/1
Batch Loss = 4.1589492975903575
609, epoch_train_loss=4.1589492975903575
Epoch 610
Epoch 610 :: Batch 0/1
Batch Loss = 4.158769591619861
610, epoch_train_loss=4.158769591619861
Epoch 611
Epoch 611 :: Batch 0/1
Batch Loss = 4.158588122243656
611, epoch_train_loss=4.158588122243656
Epoch 612
Epoch 612 :: Batch 0/1
Batch Loss = 4.158405110529129
612, epoch_train_loss=4.158405110529129
Epoch 613
Epoch 613 :: Batch 0/1
Batch Loss = 4.158221196769491
613, epoch_train_loss=4.158221196769491
Epoch 614
Epoch 614 :: Batch 0/1
Batch Loss = 4.15803653945444
614, epoch_train_loss=4.15803653945444
Epoch 615
Epoch 615 :: Batch 0/1
Batch Loss = 4.157851300364968
615, epoch_train_loss=4.157851300364968
Epoch 616
Epoch 616 :: Batch 0/1
Batch Loss = 4.157665985272639
616, epoch_train_loss=4.157665985272639
Epoch 617
Epoch 617 :: Batch 0/1
Batch Loss = 4.157480823604942
617, epoch_train_loss=4.157480823604942
Epoch 618
Epoch 618 :: Batch 0/1
Batch Loss = 4.1572958329476295
618, epoch_train_loss=4.1572958329476295
Epoch 619
Epoch 619 :: Batch 0/1
Batch Loss = 4.157111396821799
619, epoch_train_loss=4.157111396821799
Epoch 620
Epoch 620 :: Batch 0/1
Batch Loss = 4.156927654310088
620, epoch_train_loss=4.156927654310088
Epoch 621
Epoch 621 :: Batch 0/1
Batch Loss = 4.156744572921404
621, epoch_train_loss=4.156744572921404
Epoch 622
Epoch 622 :: Batch 0/1
Batch Loss = 4.156562376613212
622, epoch_train_loss=4.156562376613212
Epoch 623
Epoch 623 :: Batch 0/1
Batch Loss = 4.156381098483032
623, epoch_train_loss=4.156381098483032
Epoch 624
Epoch 624 :: Batch 0/1
Batch Loss = 4.156200639523299
624, epoch_train_loss=4.156200639523299
Epoch 625
Epoch 625 :: Batch 0/1
Batch Loss = 4.156021108805241
625, epoch_train_loss=4.156021108805241
Epoch 626
Epoch 626 :: Batch 0/1
Batch Loss = 4.155842413701792
626, epoch_train_loss=4.155842413701792
Epoch 627
Epoch 627 :: Batch 0/1
Batch Loss = 4.155664397561516
627, epoch_train_loss=4.155664397561516
Epoch 628
Epoch 628 :: Batch 0/1
Batch Loss = 4.155487021595308
628, epoch_train_loss=4.155487021595308
Epoch 629
Epoch 629 :: Batch 0/1
Batch Loss = 4.155310058282066
629, epoch_train_loss=4.155310058282066
Epoch 630
Epoch 630 :: Batch 0/1
Batch Loss = 4.155133275583004
630, epoch_train_loss=4.155133275583004
Epoch 631
Epoch 631 :: Batch 0/1
Batch Loss = 4.154956503104115
631, epoch_train_loss=4.154956503104115
Epoch 632
Epoch 632 :: Batch 0/1
Batch Loss = 4.154779398380912
632, epoch_train_loss=4.154779398380912
Epoch 633
Epoch 633 :: Batch 0/1
Batch Loss = 4.154601704284271
633, epoch_train_loss=4.154601704284271
Epoch 634
Epoch 634 :: Batch 0/1
Batch Loss = 4.154423147110941
634, epoch_train_loss=4.154423147110941
Epoch 635
Epoch 635 :: Batch 0/1
Batch Loss = 4.154243385661884
635, epoch_train_loss=4.154243385661884
Epoch 636
Epoch 636 :: Batch 0/1
Batch Loss = 4.154062186559725
636, epoch_train_loss=4.154062186559725
Epoch 637
Epoch 637 :: Batch 0/1
Batch Loss = 4.153879276100528
637, epoch_train_loss=4.153879276100528
Epoch 638
Epoch 638 :: Batch 0/1
Batch Loss = 4.153694415629235
638, epoch_train_loss=4.153694415629235
Epoch 639
Epoch 639 :: Batch 0/1
Batch Loss = 4.153507459623557
639, epoch_train_loss=4.153507459623557
Epoch 640
Epoch 640 :: Batch 0/1
Batch Loss = 4.153318228827562
640, epoch_train_loss=4.153318228827562
Epoch 641
Epoch 641 :: Batch 0/1
Batch Loss = 4.153126654151174
641, epoch_train_loss=4.153126654151174
Epoch 642
Epoch 642 :: Batch 0/1
Batch Loss = 4.152932675643288
642, epoch_train_loss=4.152932675643288
Epoch 643
Epoch 643 :: Batch 0/1
Batch Loss = 4.152736261508275
643, epoch_train_loss=4.152736261508275
Epoch 644
Epoch 644 :: Batch 0/1
Batch Loss = 4.1525374357974085
644, epoch_train_loss=4.1525374357974085
Epoch 645
Epoch 645 :: Batch 0/1
Batch Loss = 4.152336187767837
645, epoch_train_loss=4.152336187767837
Epoch 646
Epoch 646 :: Batch 0/1
Batch Loss = 4.152132546261404
646, epoch_train_loss=4.152132546261404
Epoch 647
Epoch 647 :: Batch 0/1
Batch Loss = 4.151926494318022
647, epoch_train_loss=4.151926494318022
Epoch 648
Epoch 648 :: Batch 0/1
Batch Loss = 4.1517180005578735
648, epoch_train_loss=4.1517180005578735
Epoch 649
Epoch 649 :: Batch 0/1
Batch Loss = 4.151507006936736
649, epoch_train_loss=4.151507006936736
Epoch 650
Epoch 650 :: Batch 0/1
Batch Loss = 4.151293405643219
650, epoch_train_loss=4.151293405643219
Epoch 651
Epoch 651 :: Batch 0/1
Batch Loss = 4.1510770898842635
651, epoch_train_loss=4.1510770898842635
Epoch 652
Epoch 652 :: Batch 0/1
Batch Loss = 4.150857912494454
652, epoch_train_loss=4.150857912494454
Epoch 653
Epoch 653 :: Batch 0/1
Batch Loss = 4.150635753861377
653, epoch_train_loss=4.150635753861377
Epoch 654
Epoch 654 :: Batch 0/1
Batch Loss = 4.15041049238294
654, epoch_train_loss=4.15041049238294
Epoch 655
Epoch 655 :: Batch 0/1
Batch Loss = 4.150182056419041
655, epoch_train_loss=4.150182056419041
Epoch 656
Epoch 656 :: Batch 0/1
Batch Loss = 4.149950411810379
656, epoch_train_loss=4.149950411810379
Epoch 657
Epoch 657 :: Batch 0/1
Batch Loss = 4.149715578135651
657, epoch_train_loss=4.149715578135651
Epoch 658
Epoch 658 :: Batch 0/1
Batch Loss = 4.149477621984539
658, epoch_train_loss=4.149477621984539
Epoch 659
Epoch 659 :: Batch 0/1
Batch Loss = 4.149236646609365
659, epoch_train_loss=4.149236646609365
Epoch 660
Epoch 660 :: Batch 0/1
Batch Loss = 4.1489927905415005
660, epoch_train_loss=4.1489927905415005
Epoch 661
Epoch 661 :: Batch 0/1
Batch Loss = 4.1487461978095945
661, epoch_train_loss=4.1487461978095945
Epoch 662
Epoch 662 :: Batch 0/1
Batch Loss = 4.148497024320402
662, epoch_train_loss=4.148497024320402
Epoch 663
Epoch 663 :: Batch 0/1
Batch Loss = 4.1482454121885795
663, epoch_train_loss=4.1482454121885795
Epoch 664
Epoch 664 :: Batch 0/1
Batch Loss = 4.147991505705462
664, epoch_train_loss=4.147991505705462
Epoch 665
Epoch 665 :: Batch 0/1
Batch Loss = 4.147735432968048
665, epoch_train_loss=4.147735432968048
Epoch 666
Epoch 666 :: Batch 0/1
Batch Loss = 4.147477328337459
666, epoch_train_loss=4.147477328337459
Epoch 667
Epoch 667 :: Batch 0/1
Batch Loss = 4.147217326460741
667, epoch_train_loss=4.147217326460741
Epoch 668
Epoch 668 :: Batch 0/1
Batch Loss = 4.146955621574512
668, epoch_train_loss=4.146955621574512
Epoch 669
Epoch 669 :: Batch 0/1
Batch Loss = 4.146692578558733
669, epoch_train_loss=4.146692578558733
Epoch 670
Epoch 670 :: Batch 0/1
Batch Loss = 4.14642952334687
670, epoch_train_loss=4.14642952334687
Epoch 671
Epoch 671 :: Batch 0/1
Batch Loss = 4.146173195818591
671, epoch_train_loss=4.146173195818591
Epoch 672
Epoch 672 :: Batch 0/1
Batch Loss = 4.145956440849903
672, epoch_train_loss=4.145956440849903
Epoch 673
Epoch 673 :: Batch 0/1
Batch Loss = 4.145930936890162
673, epoch_train_loss=4.145930936890162
Epoch 674
Epoch 674 :: Batch 0/1
Batch Loss = 4.145724197483173
674, epoch_train_loss=4.145724197483173
Epoch 675
Epoch 675 :: Batch 0/1
Batch Loss = 4.145588916000661
675, epoch_train_loss=4.145588916000661
Epoch 676
Epoch 676 :: Batch 0/1
Batch Loss = 4.145009222520201
676, epoch_train_loss=4.145009222520201
Epoch 677
Epoch 677 :: Batch 0/1
Batch Loss = 4.1448675060509
677, epoch_train_loss=4.1448675060509
Epoch 678
Epoch 678 :: Batch 0/1
Batch Loss = 4.144540164967123
678, epoch_train_loss=4.144540164967123
Epoch 679
Epoch 679 :: Batch 0/1
Batch Loss = 4.144308050123068
679, epoch_train_loss=4.144308050123068
Epoch 680
Epoch 680 :: Batch 0/1
Batch Loss = 4.1441936154331165
680, epoch_train_loss=4.1441936154331165
Epoch 681
Epoch 681 :: Batch 0/1
Batch Loss = 4.143606079351964
681, epoch_train_loss=4.143606079351964
Epoch 682
Epoch 682 :: Batch 0/1
Batch Loss = 4.143754822886166
682, epoch_train_loss=4.143754822886166
Epoch 683
Epoch 683 :: Batch 0/1
Batch Loss = 4.14308244684521
683, epoch_train_loss=4.14308244684521
Epoch 684
Epoch 684 :: Batch 0/1
Batch Loss = 4.142914467870834
684, epoch_train_loss=4.142914467870834
Epoch 685
Epoch 685 :: Batch 0/1
Batch Loss = 4.142492826643922
685, epoch_train_loss=4.142492826643922
Epoch 686
Epoch 686 :: Batch 0/1
Batch Loss = 4.142219238038437
686, epoch_train_loss=4.142219238038437
Epoch 687
Epoch 687 :: Batch 0/1
Batch Loss = 4.141898992190747
687, epoch_train_loss=4.141898992190747
Epoch 688
Epoch 688 :: Batch 0/1
Batch Loss = 4.14157779115066
688, epoch_train_loss=4.14157779115066
Epoch 689
Epoch 689 :: Batch 0/1
Batch Loss = 4.141114730050663
689, epoch_train_loss=4.141114730050663
Epoch 690
Epoch 690 :: Batch 0/1
Batch Loss = 4.140873637121513
690, epoch_train_loss=4.140873637121513
Epoch 691
Epoch 691 :: Batch 0/1
Batch Loss = 4.140420067193889
691, epoch_train_loss=4.140420067193889
Epoch 692
Epoch 692 :: Batch 0/1
Batch Loss = 4.1401453304368525
692, epoch_train_loss=4.1401453304368525
Epoch 693
Epoch 693 :: Batch 0/1
Batch Loss = 4.139645608610872
693, epoch_train_loss=4.139645608610872
Epoch 694
Epoch 694 :: Batch 0/1
Batch Loss = 4.13931615587973
694, epoch_train_loss=4.13931615587973
Epoch 695
Epoch 695 :: Batch 0/1
Batch Loss = 4.138820354168171
695, epoch_train_loss=4.138820354168171
Epoch 696
Epoch 696 :: Batch 0/1
Batch Loss = 4.138444547690843
696, epoch_train_loss=4.138444547690843
Epoch 697
Epoch 697 :: Batch 0/1
Batch Loss = 4.138011023420727
697, epoch_train_loss=4.138011023420727
Epoch 698
Epoch 698 :: Batch 0/1
Batch Loss = 4.137542851430762
698, epoch_train_loss=4.137542851430762
Epoch 699
Epoch 699 :: Batch 0/1
Batch Loss = 4.137130007662874
699, epoch_train_loss=4.137130007662874
Epoch 700
Epoch 700 :: Batch 0/1
Batch Loss = 4.136595373607464
700, epoch_train_loss=4.136595373607464
Epoch 701
Epoch 701 :: Batch 0/1
Batch Loss = 4.136215288738488
701, epoch_train_loss=4.136215288738488
Epoch 702
Epoch 702 :: Batch 0/1
Batch Loss = 4.135656085672247
702, epoch_train_loss=4.135656085672247
Epoch 703
Epoch 703 :: Batch 0/1
Batch Loss = 4.1352721298031785
703, epoch_train_loss=4.1352721298031785
Epoch 704
Epoch 704 :: Batch 0/1
Batch Loss = 4.134736666614001
704, epoch_train_loss=4.134736666614001
Epoch 705
Epoch 705 :: Batch 0/1
Batch Loss = 4.134319518908011
705, epoch_train_loss=4.134319518908011
Epoch 706
Epoch 706 :: Batch 0/1
Batch Loss = 4.133888189237224
706, epoch_train_loss=4.133888189237224
Epoch 707
Epoch 707 :: Batch 0/1
Batch Loss = 4.133492048148981
707, epoch_train_loss=4.133492048148981
Epoch 708
Epoch 708 :: Batch 0/1
Batch Loss = 4.13328468891949
708, epoch_train_loss=4.13328468891949
Epoch 709
Epoch 709 :: Batch 0/1
Batch Loss = 4.13349881790681
709, epoch_train_loss=4.13349881790681
Epoch 710
Epoch 710 :: Batch 0/1
Batch Loss = 4.132690064949145
710, epoch_train_loss=4.132690064949145
Epoch 711
Epoch 711 :: Batch 0/1
Batch Loss = 4.133002007307138
711, epoch_train_loss=4.133002007307138
Epoch 712
Epoch 712 :: Batch 0/1
Batch Loss = 4.131615765589226
712, epoch_train_loss=4.131615765589226
Epoch 713
Epoch 713 :: Batch 0/1
Batch Loss = 4.132679113994051
713, epoch_train_loss=4.132679113994051
Epoch 714
Epoch 714 :: Batch 0/1
Batch Loss = 4.132543038208447
714, epoch_train_loss=4.132543038208447
Epoch 715
Epoch 715 :: Batch 0/1
Batch Loss = 4.132178590311158
715, epoch_train_loss=4.132178590311158
Epoch 716
Epoch 716 :: Batch 0/1
Batch Loss = 4.130651262634777
716, epoch_train_loss=4.130651262634777
Epoch 717
Epoch 717 :: Batch 0/1
Batch Loss = 4.130353866847206
717, epoch_train_loss=4.130353866847206
Epoch 718
Epoch 718 :: Batch 0/1
Batch Loss = 4.129682970176517
718, epoch_train_loss=4.129682970176517
Epoch 719
Epoch 719 :: Batch 0/1
Batch Loss = 4.129883585994538
719, epoch_train_loss=4.129883585994538
Epoch 720
Epoch 720 :: Batch 0/1
Batch Loss = 4.1287535371727015
720, epoch_train_loss=4.1287535371727015
Epoch 721
Epoch 721 :: Batch 0/1
Batch Loss = 4.128419515011773
721, epoch_train_loss=4.128419515011773
Epoch 722
Epoch 722 :: Batch 0/1
Batch Loss = 4.1288248224128195
722, epoch_train_loss=4.1288248224128195
Epoch 723
Epoch 723 :: Batch 0/1
Batch Loss = 4.127666752630846
723, epoch_train_loss=4.127666752630846
Epoch 724
Epoch 724 :: Batch 0/1
Batch Loss = 4.128263316294956
724, epoch_train_loss=4.128263316294956
Epoch 725
Epoch 725 :: Batch 0/1
Batch Loss = 4.128501017924678
725, epoch_train_loss=4.128501017924678
Epoch 726
Epoch 726 :: Batch 0/1
Batch Loss = 4.127703690419674
726, epoch_train_loss=4.127703690419674
Epoch 727
Epoch 727 :: Batch 0/1
Batch Loss = 4.127147769079229
727, epoch_train_loss=4.127147769079229
Epoch 728
Epoch 728 :: Batch 0/1
Batch Loss = 4.125468451699774
728, epoch_train_loss=4.125468451699774
Epoch 729
Epoch 729 :: Batch 0/1
Batch Loss = 4.126461323563024
729, epoch_train_loss=4.126461323563024
Epoch 730
Epoch 730 :: Batch 0/1
Batch Loss = 4.125107963727363
730, epoch_train_loss=4.125107963727363
Epoch 731
Epoch 731 :: Batch 0/1
Batch Loss = 4.12555379749266
731, epoch_train_loss=4.12555379749266
Epoch 732
Epoch 732 :: Batch 0/1
Batch Loss = 4.124513266192742
732, epoch_train_loss=4.124513266192742
Epoch 733
Epoch 733 :: Batch 0/1
Batch Loss = 4.12453815319488
733, epoch_train_loss=4.12453815319488
Epoch 734
Epoch 734 :: Batch 0/1
Batch Loss = 4.123523936298022
734, epoch_train_loss=4.123523936298022
Epoch 735
Epoch 735 :: Batch 0/1
Batch Loss = 4.123192996014576
735, epoch_train_loss=4.123192996014576
Epoch 736
Epoch 736 :: Batch 0/1
Batch Loss = 4.123304133036348
736, epoch_train_loss=4.123304133036348
Epoch 737
Epoch 737 :: Batch 0/1
Batch Loss = 4.122484318286314
737, epoch_train_loss=4.122484318286314
Epoch 738
Epoch 738 :: Batch 0/1
Batch Loss = 4.1224745177098745
738, epoch_train_loss=4.1224745177098745
Epoch 739
Epoch 739 :: Batch 0/1
Batch Loss = 4.121741210644788
739, epoch_train_loss=4.121741210644788
Epoch 740
Epoch 740 :: Batch 0/1
Batch Loss = 4.12187788060551
740, epoch_train_loss=4.12187788060551
Epoch 741
Epoch 741 :: Batch 0/1
Batch Loss = 4.121371872678244
741, epoch_train_loss=4.121371872678244
Epoch 742
Epoch 742 :: Batch 0/1
Batch Loss = 4.120857568812794
742, epoch_train_loss=4.120857568812794
Epoch 743
Epoch 743 :: Batch 0/1
Batch Loss = 4.121083641197957
743, epoch_train_loss=4.121083641197957
Epoch 744
Epoch 744 :: Batch 0/1
Batch Loss = 4.120828903997135
744, epoch_train_loss=4.120828903997135
Epoch 745
Epoch 745 :: Batch 0/1
Batch Loss = 4.120619542527817
745, epoch_train_loss=4.120619542527817
Epoch 746
Epoch 746 :: Batch 0/1
Batch Loss = 4.119561733001208
746, epoch_train_loss=4.119561733001208
Epoch 747
Epoch 747 :: Batch 0/1
Batch Loss = 4.119376683673059
747, epoch_train_loss=4.119376683673059
Epoch 748
Epoch 748 :: Batch 0/1
Batch Loss = 4.119293090830141
748, epoch_train_loss=4.119293090830141
Epoch 749
Epoch 749 :: Batch 0/1
Batch Loss = 4.118548335660854
749, epoch_train_loss=4.118548335660854
Epoch 750
Epoch 750 :: Batch 0/1
Batch Loss = 4.118145635886882
750, epoch_train_loss=4.118145635886882
Epoch 751
Epoch 751 :: Batch 0/1
Batch Loss = 4.117664855549521
751, epoch_train_loss=4.117664855549521
Epoch 752
Epoch 752 :: Batch 0/1
Batch Loss = 4.117391382378613
752, epoch_train_loss=4.117391382378613
Epoch 753
Epoch 753 :: Batch 0/1
Batch Loss = 4.117198068989407
753, epoch_train_loss=4.117198068989407
Epoch 754
Epoch 754 :: Batch 0/1
Batch Loss = 4.116731628174013
754, epoch_train_loss=4.116731628174013
Epoch 755
Epoch 755 :: Batch 0/1
Batch Loss = 4.116498748521784
755, epoch_train_loss=4.116498748521784
Epoch 756
Epoch 756 :: Batch 0/1
Batch Loss = 4.116139091409769
756, epoch_train_loss=4.116139091409769
Epoch 757
Epoch 757 :: Batch 0/1
Batch Loss = 4.115821182573949
757, epoch_train_loss=4.115821182573949
Epoch 758
Epoch 758 :: Batch 0/1
Batch Loss = 4.115721705594417
758, epoch_train_loss=4.115721705594417
Epoch 759
Epoch 759 :: Batch 0/1
Batch Loss = 4.115381284036291
759, epoch_train_loss=4.115381284036291
Epoch 760
Epoch 760 :: Batch 0/1
Batch Loss = 4.115157439802103
760, epoch_train_loss=4.115157439802103
Epoch 761
Epoch 761 :: Batch 0/1
Batch Loss = 4.114954866421842
761, epoch_train_loss=4.114954866421842
Epoch 762
Epoch 762 :: Batch 0/1
Batch Loss = 4.114477978981651
762, epoch_train_loss=4.114477978981651
Epoch 763
Epoch 763 :: Batch 0/1
Batch Loss = 4.11429599172129
763, epoch_train_loss=4.11429599172129
Epoch 764
Epoch 764 :: Batch 0/1
Batch Loss = 4.113915719508362
764, epoch_train_loss=4.113915719508362
Epoch 765
Epoch 765 :: Batch 0/1
Batch Loss = 4.1137387539950145
765, epoch_train_loss=4.1137387539950145
Epoch 766
Epoch 766 :: Batch 0/1
Batch Loss = 4.1135858216722685
766, epoch_train_loss=4.1135858216722685
Epoch 767
Epoch 767 :: Batch 0/1
Batch Loss = 4.113330397302695
767, epoch_train_loss=4.113330397302695
Epoch 768
Epoch 768 :: Batch 0/1
Batch Loss = 4.113383036085348
768, epoch_train_loss=4.113383036085348
Epoch 769
Epoch 769 :: Batch 0/1
Batch Loss = 4.112991131285252
769, epoch_train_loss=4.112991131285252
Epoch 770
Epoch 770 :: Batch 0/1
Batch Loss = 4.113033280146411
770, epoch_train_loss=4.113033280146411
Epoch 771
Epoch 771 :: Batch 0/1
Batch Loss = 4.112611819202242
771, epoch_train_loss=4.112611819202242
Epoch 772
Epoch 772 :: Batch 0/1
Batch Loss = 4.112675301371479
772, epoch_train_loss=4.112675301371479
Epoch 773
Epoch 773 :: Batch 0/1
Batch Loss = 4.112073448310002
773, epoch_train_loss=4.112073448310002
Epoch 774
Epoch 774 :: Batch 0/1
Batch Loss = 4.112325665984697
774, epoch_train_loss=4.112325665984697
Epoch 775
Epoch 775 :: Batch 0/1
Batch Loss = 4.111228555671341
775, epoch_train_loss=4.111228555671341
Epoch 776
Epoch 776 :: Batch 0/1
Batch Loss = 4.111261417330567
776, epoch_train_loss=4.111261417330567
Epoch 777
Epoch 777 :: Batch 0/1
Batch Loss = 4.110617746855588
777, epoch_train_loss=4.110617746855588
Epoch 778
Epoch 778 :: Batch 0/1
Batch Loss = 4.111082376876273
778, epoch_train_loss=4.111082376876273
Epoch 779
Epoch 779 :: Batch 0/1
Batch Loss = 4.112253707789734
779, epoch_train_loss=4.112253707789734
Epoch 780
Epoch 780 :: Batch 0/1
Batch Loss = 4.112278931004388
780, epoch_train_loss=4.112278931004388
Epoch 781
Epoch 781 :: Batch 0/1
Batch Loss = 4.114438242768539
781, epoch_train_loss=4.114438242768539
Epoch 782
Epoch 782 :: Batch 0/1
Batch Loss = 4.1135258926931035
782, epoch_train_loss=4.1135258926931035
Epoch 783
Epoch 783 :: Batch 0/1
Batch Loss = 4.111322154078255
783, epoch_train_loss=4.111322154078255
Epoch 784
Epoch 784 :: Batch 0/1
Batch Loss = 4.11238677624191
784, epoch_train_loss=4.11238677624191
Epoch 785
Epoch 785 :: Batch 0/1
Batch Loss = 4.108848383098361
785, epoch_train_loss=4.108848383098361
Epoch 786
Epoch 786 :: Batch 0/1
Batch Loss = 4.112732443819755
786, epoch_train_loss=4.112732443819755
Epoch 787
Epoch 787 :: Batch 0/1
Batch Loss = 4.115593717607623
787, epoch_train_loss=4.115593717607623
Epoch 788
Epoch 788 :: Batch 0/1
Batch Loss = 4.116926798514766
788, epoch_train_loss=4.116926798514766
Epoch 789
Epoch 789 :: Batch 0/1
Batch Loss = 4.115923321233934
789, epoch_train_loss=4.115923321233934
Epoch 790
Epoch 790 :: Batch 0/1
Batch Loss = 4.1126219585284165
790, epoch_train_loss=4.1126219585284165
Epoch 791
Epoch 791 :: Batch 0/1
Batch Loss = 4.115277254885507
791, epoch_train_loss=4.115277254885507
Epoch 792
Epoch 792 :: Batch 0/1
Batch Loss = 4.1155653526881535
792, epoch_train_loss=4.1155653526881535
Epoch 793
Epoch 793 :: Batch 0/1
Batch Loss = 4.1148273596469895
793, epoch_train_loss=4.1148273596469895
Epoch 794
Epoch 794 :: Batch 0/1
Batch Loss = 4.123061742491873
794, epoch_train_loss=4.123061742491873
Epoch 795
Epoch 795 :: Batch 0/1
Batch Loss = 4.125010995560562
795, epoch_train_loss=4.125010995560562
Epoch 796
Epoch 796 :: Batch 0/1
Batch Loss = 4.122401897797232
796, epoch_train_loss=4.122401897797232
Epoch 797
Epoch 797 :: Batch 0/1
Batch Loss = 4.10534735845709
797, epoch_train_loss=4.10534735845709
Epoch 798
Epoch 798 :: Batch 0/1
Batch Loss = 4.135134609773321
798, epoch_train_loss=4.135134609773321
Epoch 799
Epoch 799 :: Batch 0/1
Batch Loss = 4.145924960984783
799, epoch_train_loss=4.145924960984783
Epoch 800
Epoch 800 :: Batch 0/1
Batch Loss = 4.159666205242927
800, epoch_train_loss=4.159666205242927
Epoch 801
Epoch 801 :: Batch 0/1
Batch Loss = 4.157242829462223
801, epoch_train_loss=4.157242829462223
Epoch 802
Epoch 802 :: Batch 0/1
Batch Loss = 4.161750086223624
802, epoch_train_loss=4.161750086223624
Epoch 803
Epoch 803 :: Batch 0/1
Batch Loss = 4.154527997164108
803, epoch_train_loss=4.154527997164108
Epoch 804
Epoch 804 :: Batch 0/1
Batch Loss = 4.154527802964212
804, epoch_train_loss=4.154527802964212
Epoch 805
Epoch 805 :: Batch 0/1
Batch Loss = 4.150962599292613
805, epoch_train_loss=4.150962599292613
Epoch 806
Epoch 806 :: Batch 0/1
Batch Loss = 4.161287293162374
806, epoch_train_loss=4.161287293162374
Epoch 807
Epoch 807 :: Batch 0/1
Batch Loss = 4.150829016355072
807, epoch_train_loss=4.150829016355072
Epoch 808
Epoch 808 :: Batch 0/1
Batch Loss = 4.155167154473075
808, epoch_train_loss=4.155167154473075
Epoch 809
Epoch 809 :: Batch 0/1
Batch Loss = 4.156569255318852
809, epoch_train_loss=4.156569255318852
Epoch 810
Epoch 810 :: Batch 0/1
Batch Loss = 4.156876867630512
810, epoch_train_loss=4.156876867630512
Epoch 811
Epoch 811 :: Batch 0/1
Batch Loss = 4.156220522323595
811, epoch_train_loss=4.156220522323595
Epoch 812
Epoch 812 :: Batch 0/1
Batch Loss = 4.155383646734709
812, epoch_train_loss=4.155383646734709
Epoch 813
Epoch 813 :: Batch 0/1
Batch Loss = 4.153852139196016
813, epoch_train_loss=4.153852139196016
Epoch 814
Epoch 814 :: Batch 0/1
Batch Loss = 4.151659487471575
814, epoch_train_loss=4.151659487471575
Epoch 815
Epoch 815 :: Batch 0/1
Batch Loss = 4.147575668167449
815, epoch_train_loss=4.147575668167449
Epoch 816
Epoch 816 :: Batch 0/1
Batch Loss = 4.14270183443607
816, epoch_train_loss=4.14270183443607
Epoch 817
Epoch 817 :: Batch 0/1
Batch Loss = 4.15538712166561
817, epoch_train_loss=4.15538712166561
Epoch 818
Epoch 818 :: Batch 0/1
Batch Loss = 4.149046852186281
818, epoch_train_loss=4.149046852186281
Epoch 819
Epoch 819 :: Batch 0/1
Batch Loss = 4.151645573763286
819, epoch_train_loss=4.151645573763286
Epoch 820
Epoch 820 :: Batch 0/1
Batch Loss = 4.1554971126885505
820, epoch_train_loss=4.1554971126885505
Epoch 821
Epoch 821 :: Batch 0/1
Batch Loss = 4.1525389671069135
821, epoch_train_loss=4.1525389671069135
Epoch 822
Epoch 822 :: Batch 0/1
Batch Loss = 4.15387347483452
822, epoch_train_loss=4.15387347483452
Epoch 823
Epoch 823 :: Batch 0/1
Batch Loss = 4.15072189098356
823, epoch_train_loss=4.15072189098356
Epoch 824
Epoch 824 :: Batch 0/1
Batch Loss = 4.152031819521649
824, epoch_train_loss=4.152031819521649
Epoch 825
Epoch 825 :: Batch 0/1
Batch Loss = 4.147927748483303
825, epoch_train_loss=4.147927748483303
Epoch 826
Epoch 826 :: Batch 0/1
Batch Loss = 4.144056714389133
826, epoch_train_loss=4.144056714389133
Epoch 827
Epoch 827 :: Batch 0/1
Batch Loss = 4.156425560208578
827, epoch_train_loss=4.156425560208578
Epoch 828
Epoch 828 :: Batch 0/1
Batch Loss = 4.14267598100002
828, epoch_train_loss=4.14267598100002
Epoch 829
Epoch 829 :: Batch 0/1
Batch Loss = 4.146512325257489
829, epoch_train_loss=4.146512325257489
Epoch 830
Epoch 830 :: Batch 0/1
Batch Loss = 4.149833037553724
830, epoch_train_loss=4.149833037553724
Epoch 831
Epoch 831 :: Batch 0/1
Batch Loss = 4.149444503189733
831, epoch_train_loss=4.149444503189733
Epoch 832
Epoch 832 :: Batch 0/1
Batch Loss = 4.1496470664654215
832, epoch_train_loss=4.1496470664654215
Epoch 833
Epoch 833 :: Batch 0/1
Batch Loss = 4.1481452117348505
833, epoch_train_loss=4.1481452117348505
Epoch 834
Epoch 834 :: Batch 0/1
Batch Loss = 4.146884680321872
834, epoch_train_loss=4.146884680321872
Epoch 835
Epoch 835 :: Batch 0/1
Batch Loss = 4.143551349929421
835, epoch_train_loss=4.143551349929421
Epoch 836
Epoch 836 :: Batch 0/1
Batch Loss = 4.140428172958441
836, epoch_train_loss=4.140428172958441
Epoch 837
Epoch 837 :: Batch 0/1
Batch Loss = 4.144748796561737
837, epoch_train_loss=4.144748796561737
Epoch 838
Epoch 838 :: Batch 0/1
Batch Loss = 4.140094948160219
838, epoch_train_loss=4.140094948160219
Epoch 839
Epoch 839 :: Batch 0/1
Batch Loss = 4.141025061052255
839, epoch_train_loss=4.141025061052255
Epoch 840
Epoch 840 :: Batch 0/1
Batch Loss = 4.142618173855116
840, epoch_train_loss=4.142618173855116
Epoch 841
Epoch 841 :: Batch 0/1
Batch Loss = 4.140651679021926
841, epoch_train_loss=4.140651679021926
Epoch 842
Epoch 842 :: Batch 0/1
Batch Loss = 4.139401131195388
842, epoch_train_loss=4.139401131195388
Epoch 843
Epoch 843 :: Batch 0/1
Batch Loss = 4.140881110943094
843, epoch_train_loss=4.140881110943094
Epoch 844
Epoch 844 :: Batch 0/1
Batch Loss = 4.1383922375913285
844, epoch_train_loss=4.1383922375913285
Epoch 845
Epoch 845 :: Batch 0/1
Batch Loss = 4.138374082798065
845, epoch_train_loss=4.138374082798065
Epoch 846
Epoch 846 :: Batch 0/1
Batch Loss = 4.138583386447442
846, epoch_train_loss=4.138583386447442
Epoch 847
Epoch 847 :: Batch 0/1
Batch Loss = 4.137300186178329
847, epoch_train_loss=4.137300186178329
Epoch 848
Epoch 848 :: Batch 0/1
Batch Loss = 4.136428974978332
848, epoch_train_loss=4.136428974978332
Epoch 849
Epoch 849 :: Batch 0/1
Batch Loss = 4.1368247955929265
849, epoch_train_loss=4.1368247955929265
Epoch 850
Epoch 850 :: Batch 0/1
Batch Loss = 4.134950801163028
850, epoch_train_loss=4.134950801163028
Epoch 851
Epoch 851 :: Batch 0/1
Batch Loss = 4.134191516106541
851, epoch_train_loss=4.134191516106541
Epoch 852
Epoch 852 :: Batch 0/1
Batch Loss = 4.132886596623438
852, epoch_train_loss=4.132886596623438
Epoch 853
Epoch 853 :: Batch 0/1
Batch Loss = 4.129688754630523
853, epoch_train_loss=4.129688754630523
Epoch 854
Epoch 854 :: Batch 0/1
Batch Loss = 4.12571396937529
854, epoch_train_loss=4.12571396937529
Epoch 855
Epoch 855 :: Batch 0/1
Batch Loss = 4.126468513833639
855, epoch_train_loss=4.126468513833639
Epoch 856
Epoch 856 :: Batch 0/1
Batch Loss = 4.126323661929421
856, epoch_train_loss=4.126323661929421
Epoch 857
Epoch 857 :: Batch 0/1
Batch Loss = 4.12514361084696
857, epoch_train_loss=4.12514361084696
Epoch 858
Epoch 858 :: Batch 0/1
Batch Loss = 4.126387807950543
858, epoch_train_loss=4.126387807950543
Epoch 859
Epoch 859 :: Batch 0/1
Batch Loss = 4.121624960109454
859, epoch_train_loss=4.121624960109454
Epoch 860
Epoch 860 :: Batch 0/1
Batch Loss = 4.123884692891588
860, epoch_train_loss=4.123884692891588
Epoch 861
Epoch 861 :: Batch 0/1
Batch Loss = 4.120614442195918
861, epoch_train_loss=4.120614442195918
Epoch 862
Epoch 862 :: Batch 0/1
Batch Loss = 4.119509287275976
862, epoch_train_loss=4.119509287275976
Epoch 863
Epoch 863 :: Batch 0/1
Batch Loss = 4.120656047184827
863, epoch_train_loss=4.120656047184827
Epoch 864
Epoch 864 :: Batch 0/1
Batch Loss = 4.1180086768733135
864, epoch_train_loss=4.1180086768733135
Epoch 865
Epoch 865 :: Batch 0/1
Batch Loss = 4.115797195669166
865, epoch_train_loss=4.115797195669166
Epoch 866
Epoch 866 :: Batch 0/1
Batch Loss = 4.114034846049528
866, epoch_train_loss=4.114034846049528
Epoch 867
Epoch 867 :: Batch 0/1
Batch Loss = 4.108966262992782
867, epoch_train_loss=4.108966262992782
Epoch 868
Epoch 868 :: Batch 0/1
Batch Loss = 4.101290492426899
868, epoch_train_loss=4.101290492426899
Epoch 869
Epoch 869 :: Batch 0/1
Batch Loss = 4.127437386250134
869, epoch_train_loss=4.127437386250134
Epoch 870
Epoch 870 :: Batch 0/1
Batch Loss = 4.114737393150851
870, epoch_train_loss=4.114737393150851
Epoch 871
Epoch 871 :: Batch 0/1
Batch Loss = 4.115377281918346
871, epoch_train_loss=4.115377281918346
Epoch 872
Epoch 872 :: Batch 0/1
Batch Loss = 4.129764888770079
872, epoch_train_loss=4.129764888770079
Epoch 873
Epoch 873 :: Batch 0/1
Batch Loss = 4.134454333728921
873, epoch_train_loss=4.134454333728921
Epoch 874
Epoch 874 :: Batch 0/1
Batch Loss = 4.145394061391944
874, epoch_train_loss=4.145394061391944
Epoch 875
Epoch 875 :: Batch 0/1
Batch Loss = 4.141744256921381
875, epoch_train_loss=4.141744256921381
Epoch 876
Epoch 876 :: Batch 0/1
Batch Loss = 4.131617293963329
876, epoch_train_loss=4.131617293963329
Epoch 877
Epoch 877 :: Batch 0/1
Batch Loss = 4.1508337034108855
877, epoch_train_loss=4.1508337034108855
Epoch 878
Epoch 878 :: Batch 0/1
Batch Loss = 4.141233252274655
878, epoch_train_loss=4.141233252274655
Epoch 879
Epoch 879 :: Batch 0/1
Batch Loss = 4.149602644137937
879, epoch_train_loss=4.149602644137937
Epoch 880
Epoch 880 :: Batch 0/1
Batch Loss = 4.147793631059753
880, epoch_train_loss=4.147793631059753
Epoch 881
Epoch 881 :: Batch 0/1
Batch Loss = 4.151160886610656
881, epoch_train_loss=4.151160886610656
Epoch 882
Epoch 882 :: Batch 0/1
Batch Loss = 4.147525873109792
882, epoch_train_loss=4.147525873109792
Epoch 883
Epoch 883 :: Batch 0/1
Batch Loss = 4.143691935459314
883, epoch_train_loss=4.143691935459314
Epoch 884
Epoch 884 :: Batch 0/1
Batch Loss = 4.148884834577309
884, epoch_train_loss=4.148884834577309
Epoch 885
Epoch 885 :: Batch 0/1
Batch Loss = 4.1485355618835404
885, epoch_train_loss=4.1485355618835404
Epoch 886
Epoch 886 :: Batch 0/1
Batch Loss = 4.145526601644682
886, epoch_train_loss=4.145526601644682
Epoch 887
Epoch 887 :: Batch 0/1
Batch Loss = 4.14044559742716
887, epoch_train_loss=4.14044559742716
Epoch 888
Epoch 888 :: Batch 0/1
Batch Loss = 4.147661202957366
888, epoch_train_loss=4.147661202957366
Epoch 889
Epoch 889 :: Batch 0/1
Batch Loss = 4.138744179102653
889, epoch_train_loss=4.138744179102653
Epoch 890
Epoch 890 :: Batch 0/1
Batch Loss = 4.141115480718576
890, epoch_train_loss=4.141115480718576
Epoch 891
Epoch 891 :: Batch 0/1
Batch Loss = 4.140192042378384
891, epoch_train_loss=4.140192042378384
Epoch 892
Epoch 892 :: Batch 0/1
Batch Loss = 4.137850861937077
892, epoch_train_loss=4.137850861937077
Epoch 893
Epoch 893 :: Batch 0/1
Batch Loss = 4.137372536075348
893, epoch_train_loss=4.137372536075348
Epoch 894
Epoch 894 :: Batch 0/1
Batch Loss = 4.1391406148342575
894, epoch_train_loss=4.1391406148342575
Epoch 895
Epoch 895 :: Batch 0/1
Batch Loss = 4.1386819622520905
895, epoch_train_loss=4.1386819622520905
Epoch 896
Epoch 896 :: Batch 0/1
Batch Loss = 4.135514301237109
896, epoch_train_loss=4.135514301237109
Epoch 897
Epoch 897 :: Batch 0/1
Batch Loss = 4.136055482662208
897, epoch_train_loss=4.136055482662208
Epoch 898
Epoch 898 :: Batch 0/1
Batch Loss = 4.135432145828089
898, epoch_train_loss=4.135432145828089
Epoch 899
Epoch 899 :: Batch 0/1
Batch Loss = 4.134660560286048
899, epoch_train_loss=4.134660560286048
Epoch 900
Epoch 900 :: Batch 0/1
Batch Loss = 4.1313078883245895
900, epoch_train_loss=4.1313078883245895
Epoch 901
Epoch 901 :: Batch 0/1
Batch Loss = 4.132509133226632
901, epoch_train_loss=4.132509133226632
Epoch 902
Epoch 902 :: Batch 0/1
Batch Loss = 4.127541953736153
902, epoch_train_loss=4.127541953736153
Epoch 903
Epoch 903 :: Batch 0/1
Batch Loss = 4.122443466346534
903, epoch_train_loss=4.122443466346534
Epoch 904
Epoch 904 :: Batch 0/1
Batch Loss = 4.124152336096085
904, epoch_train_loss=4.124152336096085
Epoch 905
Epoch 905 :: Batch 0/1
Batch Loss = 4.1194978725855025
905, epoch_train_loss=4.1194978725855025
Epoch 906
Epoch 906 :: Batch 0/1
Batch Loss = 4.121079203561982
906, epoch_train_loss=4.121079203561982
Epoch 907
Epoch 907 :: Batch 0/1
Batch Loss = 4.120811151714366
907, epoch_train_loss=4.120811151714366
Epoch 908
Epoch 908 :: Batch 0/1
Batch Loss = 4.119800391659483
908, epoch_train_loss=4.119800391659483
Epoch 909
Epoch 909 :: Batch 0/1
Batch Loss = 4.117419403357059
909, epoch_train_loss=4.117419403357059
Epoch 910
Epoch 910 :: Batch 0/1
Batch Loss = 4.120113635538624
910, epoch_train_loss=4.120113635538624
Epoch 911
Epoch 911 :: Batch 0/1
Batch Loss = 4.118726101316944
911, epoch_train_loss=4.118726101316944
Epoch 912
Epoch 912 :: Batch 0/1
Batch Loss = 4.115536480132419
912, epoch_train_loss=4.115536480132419
Epoch 913
Epoch 913 :: Batch 0/1
Batch Loss = 4.118916416865986
913, epoch_train_loss=4.118916416865986
Epoch 914
Epoch 914 :: Batch 0/1
Batch Loss = 4.115888450210699
914, epoch_train_loss=4.115888450210699
Epoch 915
Epoch 915 :: Batch 0/1
Batch Loss = 4.11516079452442
915, epoch_train_loss=4.11516079452442
Epoch 916
Epoch 916 :: Batch 0/1
Batch Loss = 4.116213227060083
916, epoch_train_loss=4.116213227060083
Epoch 917
Epoch 917 :: Batch 0/1
Batch Loss = 4.113291317030757
917, epoch_train_loss=4.113291317030757
Epoch 918
Epoch 918 :: Batch 0/1
Batch Loss = 4.113961901543363
918, epoch_train_loss=4.113961901543363
Epoch 919
Epoch 919 :: Batch 0/1
Batch Loss = 4.114104367168898
919, epoch_train_loss=4.114104367168898
Epoch 920
Epoch 920 :: Batch 0/1
Batch Loss = 4.112503021123279
920, epoch_train_loss=4.112503021123279
Epoch 921
Epoch 921 :: Batch 0/1
Batch Loss = 4.112611938822231
921, epoch_train_loss=4.112611938822231
Epoch 922
Epoch 922 :: Batch 0/1
Batch Loss = 4.11300265518572
922, epoch_train_loss=4.11300265518572
Epoch 923
Epoch 923 :: Batch 0/1
Batch Loss = 4.111708461101279
923, epoch_train_loss=4.111708461101279
Epoch 924
Epoch 924 :: Batch 0/1
Batch Loss = 4.112101359091567
924, epoch_train_loss=4.112101359091567
Epoch 925
Epoch 925 :: Batch 0/1
Batch Loss = 4.111791208634712
925, epoch_train_loss=4.111791208634712
Epoch 926
Epoch 926 :: Batch 0/1
Batch Loss = 4.111343119565132
926, epoch_train_loss=4.111343119565132
Epoch 927
Epoch 927 :: Batch 0/1
Batch Loss = 4.111321169153278
927, epoch_train_loss=4.111321169153278
Epoch 928
Epoch 928 :: Batch 0/1
Batch Loss = 4.11095154002591
928, epoch_train_loss=4.11095154002591
Epoch 929
Epoch 929 :: Batch 0/1
Batch Loss = 4.11089038615375
929, epoch_train_loss=4.11089038615375
Epoch 930
Epoch 930 :: Batch 0/1
Batch Loss = 4.1105445806127685
930, epoch_train_loss=4.1105445806127685
Epoch 931
Epoch 931 :: Batch 0/1
Batch Loss = 4.110397687547173
931, epoch_train_loss=4.110397687547173
Epoch 932
Epoch 932 :: Batch 0/1
Batch Loss = 4.110263484609051
932, epoch_train_loss=4.110263484609051
Epoch 933
Epoch 933 :: Batch 0/1
Batch Loss = 4.1098770538933085
933, epoch_train_loss=4.1098770538933085
Epoch 934
Epoch 934 :: Batch 0/1
Batch Loss = 4.109900205062561
934, epoch_train_loss=4.109900205062561
Epoch 935
Epoch 935 :: Batch 0/1
Batch Loss = 4.109527908342904
935, epoch_train_loss=4.109527908342904
Epoch 936
Epoch 936 :: Batch 0/1
Batch Loss = 4.1093337593371215
936, epoch_train_loss=4.1093337593371215
Epoch 937
Epoch 937 :: Batch 0/1
Batch Loss = 4.10927361413144
937, epoch_train_loss=4.10927361413144
Epoch 938
Epoch 938 :: Batch 0/1
Batch Loss = 4.108900221952337
938, epoch_train_loss=4.108900221952337
Epoch 939
Epoch 939 :: Batch 0/1
Batch Loss = 4.108799240692496
939, epoch_train_loss=4.108799240692496
Epoch 940
Epoch 940 :: Batch 0/1
Batch Loss = 4.108668127086057
940, epoch_train_loss=4.108668127086057
Epoch 941
Epoch 941 :: Batch 0/1
Batch Loss = 4.10835936790824
941, epoch_train_loss=4.10835936790824
Epoch 942
Epoch 942 :: Batch 0/1
Batch Loss = 4.108263239284111
942, epoch_train_loss=4.108263239284111
Epoch 943
Epoch 943 :: Batch 0/1
Batch Loss = 4.108082512833032
943, epoch_train_loss=4.108082512833032
Epoch 944
Epoch 944 :: Batch 0/1
Batch Loss = 4.107831130562437
944, epoch_train_loss=4.107831130562437
Epoch 945
Epoch 945 :: Batch 0/1
Batch Loss = 4.107692915162352
945, epoch_train_loss=4.107692915162352
Epoch 946
Epoch 946 :: Batch 0/1
Batch Loss = 4.107489873791321
946, epoch_train_loss=4.107489873791321
Epoch 947
Epoch 947 :: Batch 0/1
Batch Loss = 4.107258604737969
947, epoch_train_loss=4.107258604737969
Epoch 948
Epoch 948 :: Batch 0/1
Batch Loss = 4.1070836880069805
948, epoch_train_loss=4.1070836880069805
Epoch 949
Epoch 949 :: Batch 0/1
Batch Loss = 4.106874530181416
949, epoch_train_loss=4.106874530181416
Epoch 950
Epoch 950 :: Batch 0/1
Batch Loss = 4.106646323036917
950, epoch_train_loss=4.106646323036917
Epoch 951
Epoch 951 :: Batch 0/1
Batch Loss = 4.106440524972084
951, epoch_train_loss=4.106440524972084
Epoch 952
Epoch 952 :: Batch 0/1
Batch Loss = 4.10622754223539
952, epoch_train_loss=4.10622754223539
Epoch 953
Epoch 953 :: Batch 0/1
Batch Loss = 4.10598584530882
953, epoch_train_loss=4.10598584530882
Epoch 954
Epoch 954 :: Batch 0/1
Batch Loss = 4.105769177012496
954, epoch_train_loss=4.105769177012496
Epoch 955
Epoch 955 :: Batch 0/1
Batch Loss = 4.105532825744178
955, epoch_train_loss=4.105532825744178
Epoch 956
Epoch 956 :: Batch 0/1
Batch Loss = 4.1052704313906325
956, epoch_train_loss=4.1052704313906325
Epoch 957
Epoch 957 :: Batch 0/1
Batch Loss = 4.105035630767235
957, epoch_train_loss=4.105035630767235
Epoch 958
Epoch 958 :: Batch 0/1
Batch Loss = 4.104758662520629
958, epoch_train_loss=4.104758662520629
Epoch 959
Epoch 959 :: Batch 0/1
Batch Loss = 4.104475908604915
959, epoch_train_loss=4.104475908604915
Epoch 960
Epoch 960 :: Batch 0/1
Batch Loss = 4.104197417759261
960, epoch_train_loss=4.104197417759261
Epoch 961
Epoch 961 :: Batch 0/1
Batch Loss = 4.103870946010491
961, epoch_train_loss=4.103870946010491
Epoch 962
Epoch 962 :: Batch 0/1
Batch Loss = 4.103546847917377
962, epoch_train_loss=4.103546847917377
Epoch 963
Epoch 963 :: Batch 0/1
Batch Loss = 4.103191372052027
963, epoch_train_loss=4.103191372052027
Epoch 964
Epoch 964 :: Batch 0/1
Batch Loss = 4.102788858076712
964, epoch_train_loss=4.102788858076712
Epoch 965
Epoch 965 :: Batch 0/1
Batch Loss = 4.102353515661783
965, epoch_train_loss=4.102353515661783
Epoch 966
Epoch 966 :: Batch 0/1
Batch Loss = 4.101835444590098
966, epoch_train_loss=4.101835444590098
Epoch 967
Epoch 967 :: Batch 0/1
Batch Loss = 4.101214862234231
967, epoch_train_loss=4.101214862234231
Epoch 968
Epoch 968 :: Batch 0/1
Batch Loss = 4.1004202320724135
968, epoch_train_loss=4.1004202320724135
Epoch 969
Epoch 969 :: Batch 0/1
Batch Loss = 4.099312916853823
969, epoch_train_loss=4.099312916853823
Epoch 970
Epoch 970 :: Batch 0/1
Batch Loss = 4.097594195544109
970, epoch_train_loss=4.097594195544109
Epoch 971
Epoch 971 :: Batch 0/1
Batch Loss = 4.094618391303133
971, epoch_train_loss=4.094618391303133
Epoch 972
Epoch 972 :: Batch 0/1
Batch Loss = 4.090249715335722
972, epoch_train_loss=4.090249715335722
Epoch 973
Epoch 973 :: Batch 0/1
Batch Loss = 4.095155308095197
973, epoch_train_loss=4.095155308095197
Epoch 974
Epoch 974 :: Batch 0/1
Batch Loss = 4.12595649605079
974, epoch_train_loss=4.12595649605079
Epoch 975
Epoch 975 :: Batch 0/1
Batch Loss = 4.088663699262208
975, epoch_train_loss=4.088663699262208
Epoch 976
Epoch 976 :: Batch 0/1
Batch Loss = 4.08720980531158
976, epoch_train_loss=4.08720980531158
Epoch 977
Epoch 977 :: Batch 0/1
Batch Loss = 4.087561639137038
977, epoch_train_loss=4.087561639137038
Epoch 978
Epoch 978 :: Batch 0/1
Batch Loss = 4.086808227696
978, epoch_train_loss=4.086808227696
Epoch 979
Epoch 979 :: Batch 0/1
Batch Loss = 4.08624320283567
979, epoch_train_loss=4.08624320283567
Epoch 980
Epoch 980 :: Batch 0/1
Batch Loss = 4.086019293665212
980, epoch_train_loss=4.086019293665212
Epoch 981
Epoch 981 :: Batch 0/1
Batch Loss = 4.085430775754123
981, epoch_train_loss=4.085430775754123
Epoch 982
Epoch 982 :: Batch 0/1
Batch Loss = 4.085169701970231
982, epoch_train_loss=4.085169701970231
Epoch 983
Epoch 983 :: Batch 0/1
Batch Loss = 4.084543400795348
983, epoch_train_loss=4.084543400795348
Epoch 984
Epoch 984 :: Batch 0/1
Batch Loss = 4.084459527151865
984, epoch_train_loss=4.084459527151865
Epoch 985
Epoch 985 :: Batch 0/1
Batch Loss = 4.08371788686089
985, epoch_train_loss=4.08371788686089
Epoch 986
Epoch 986 :: Batch 0/1
Batch Loss = 4.083549475307228
986, epoch_train_loss=4.083549475307228
Epoch 987
Epoch 987 :: Batch 0/1
Batch Loss = 4.083181985943436
987, epoch_train_loss=4.083181985943436
Epoch 988
Epoch 988 :: Batch 0/1
Batch Loss = 4.082844819570076
988, epoch_train_loss=4.082844819570076
Epoch 989
Epoch 989 :: Batch 0/1
Batch Loss = 4.082567376140218
989, epoch_train_loss=4.082567376140218
Epoch 990
Epoch 990 :: Batch 0/1
Batch Loss = 4.08251446276903
990, epoch_train_loss=4.08251446276903
Epoch 991
Epoch 991 :: Batch 0/1
Batch Loss = 4.082314057820886
991, epoch_train_loss=4.082314057820886
Epoch 992
Epoch 992 :: Batch 0/1
Batch Loss = 4.082161341805731
992, epoch_train_loss=4.082161341805731
Epoch 993
Epoch 993 :: Batch 0/1
Batch Loss = 4.082049834733845
993, epoch_train_loss=4.082049834733845
Epoch 994
Epoch 994 :: Batch 0/1
Batch Loss = 4.081785876439246
994, epoch_train_loss=4.081785876439246
Epoch 995
Epoch 995 :: Batch 0/1
Batch Loss = 4.08177326127761
995, epoch_train_loss=4.08177326127761
Epoch 996
Epoch 996 :: Batch 0/1
Batch Loss = 4.081274538988585
996, epoch_train_loss=4.081274538988585
Epoch 997
Epoch 997 :: Batch 0/1
Batch Loss = 4.081318932196435
997, epoch_train_loss=4.081318932196435
Epoch 998
Epoch 998 :: Batch 0/1
Batch Loss = 4.080952068399741
998, epoch_train_loss=4.080952068399741
Epoch 999
Epoch 999 :: Batch 0/1
Batch Loss = 4.080878660571253
999, epoch_train_loss=4.080878660571253
Epoch 1000
Epoch 1000 :: Batch 0/1
Batch Loss = 4.080739114885338
1000, epoch_train_loss=4.080739114885338
Epoch 1001
Epoch 1001 :: Batch 0/1
Batch Loss = 4.080483333736028
1001, epoch_train_loss=4.080483333736028
Epoch 1002
Epoch 1002 :: Batch 0/1
Batch Loss = 4.080472409831881
1002, epoch_train_loss=4.080472409831881
Epoch 1003
Epoch 1003 :: Batch 0/1
Batch Loss = 4.080220989723355
1003, epoch_train_loss=4.080220989723355
Epoch 1004
Epoch 1004 :: Batch 0/1
Batch Loss = 4.080105784671127
1004, epoch_train_loss=4.080105784671127
Epoch 1005
Epoch 1005 :: Batch 0/1
Batch Loss = 4.079916922661253
1005, epoch_train_loss=4.079916922661253
Epoch 1006
Epoch 1006 :: Batch 0/1
Batch Loss = 4.079769180166748
1006, epoch_train_loss=4.079769180166748
Epoch 1007
Epoch 1007 :: Batch 0/1
Batch Loss = 4.079578415463485
1007, epoch_train_loss=4.079578415463485
Epoch 1008
Epoch 1008 :: Batch 0/1
Batch Loss = 4.079501394617883
1008, epoch_train_loss=4.079501394617883
Epoch 1009
Epoch 1009 :: Batch 0/1
Batch Loss = 4.0792577910635
1009, epoch_train_loss=4.0792577910635
Epoch 1010
Epoch 1010 :: Batch 0/1
Batch Loss = 4.079191377705493
1010, epoch_train_loss=4.079191377705493
Epoch 1011
Epoch 1011 :: Batch 0/1
Batch Loss = 4.079001599487612
1011, epoch_train_loss=4.079001599487612
Epoch 1012
Epoch 1012 :: Batch 0/1
Batch Loss = 4.078855900272155
1012, epoch_train_loss=4.078855900272155
Epoch 1013
Epoch 1013 :: Batch 0/1
Batch Loss = 4.078715845701481
1013, epoch_train_loss=4.078715845701481
Epoch 1014
Epoch 1014 :: Batch 0/1
Batch Loss = 4.078537300904008
1014, epoch_train_loss=4.078537300904008
Epoch 1015
Epoch 1015 :: Batch 0/1
Batch Loss = 4.078379783360747
1015, epoch_train_loss=4.078379783360747
Epoch 1016
Epoch 1016 :: Batch 0/1
Batch Loss = 4.078237849943671
1016, epoch_train_loss=4.078237849943671
Epoch 1017
Epoch 1017 :: Batch 0/1
Batch Loss = 4.078073255054051
1017, epoch_train_loss=4.078073255054051
Epoch 1018
Epoch 1018 :: Batch 0/1
Batch Loss = 4.077926816576318
1018, epoch_train_loss=4.077926816576318
Epoch 1019
Epoch 1019 :: Batch 0/1
Batch Loss = 4.0777991046987845
1019, epoch_train_loss=4.0777991046987845
Epoch 1020
Epoch 1020 :: Batch 0/1
Batch Loss = 4.077630410617049
1020, epoch_train_loss=4.077630410617049
Epoch 1021
Epoch 1021 :: Batch 0/1
Batch Loss = 4.077514082243207
1021, epoch_train_loss=4.077514082243207
Epoch 1022
Epoch 1022 :: Batch 0/1
Batch Loss = 4.077356595909238
1022, epoch_train_loss=4.077356595909238
Epoch 1023
Epoch 1023 :: Batch 0/1
Batch Loss = 4.077218820704001
1023, epoch_train_loss=4.077218820704001
Epoch 1024
Epoch 1024 :: Batch 0/1
Batch Loss = 4.077075421079286
1024, epoch_train_loss=4.077075421079286
Epoch 1025
Epoch 1025 :: Batch 0/1
Batch Loss = 4.076942978358928
1025, epoch_train_loss=4.076942978358928
Epoch 1026
Epoch 1026 :: Batch 0/1
Batch Loss = 4.07679097505033
1026, epoch_train_loss=4.07679097505033
Epoch 1027
Epoch 1027 :: Batch 0/1
Batch Loss = 4.076669215738474
1027, epoch_train_loss=4.076669215738474
Epoch 1028
Epoch 1028 :: Batch 0/1
Batch Loss = 4.076522925239632
1028, epoch_train_loss=4.076522925239632
Epoch 1029
Epoch 1029 :: Batch 0/1
Batch Loss = 4.07638688443732
1029, epoch_train_loss=4.07638688443732
Epoch 1030
Epoch 1030 :: Batch 0/1
Batch Loss = 4.076252511598104
1030, epoch_train_loss=4.076252511598104
Epoch 1031
Epoch 1031 :: Batch 0/1
Batch Loss = 4.076115300316526
1031, epoch_train_loss=4.076115300316526
Epoch 1032
Epoch 1032 :: Batch 0/1
Batch Loss = 4.075973627385629
1032, epoch_train_loss=4.075973627385629
Epoch 1033
Epoch 1033 :: Batch 0/1
Batch Loss = 4.075844166546484
1033, epoch_train_loss=4.075844166546484
Epoch 1034
Epoch 1034 :: Batch 0/1
Batch Loss = 4.075706154895245
1034, epoch_train_loss=4.075706154895245
Epoch 1035
Epoch 1035 :: Batch 0/1
Batch Loss = 4.075570605509192
1035, epoch_train_loss=4.075570605509192
Epoch 1036
Epoch 1036 :: Batch 0/1
Batch Loss = 4.075441243602555
1036, epoch_train_loss=4.075441243602555
Epoch 1037
Epoch 1037 :: Batch 0/1
Batch Loss = 4.075307170190287
1037, epoch_train_loss=4.075307170190287
Epoch 1038
Epoch 1038 :: Batch 0/1
Batch Loss = 4.0751737970472455
1038, epoch_train_loss=4.0751737970472455
Epoch 1039
Epoch 1039 :: Batch 0/1
Batch Loss = 4.075042560864589
1039, epoch_train_loss=4.075042560864589
Epoch 1040
Epoch 1040 :: Batch 0/1
Batch Loss = 4.074911059760542
1040, epoch_train_loss=4.074911059760542
Epoch 1041
Epoch 1041 :: Batch 0/1
Batch Loss = 4.074775457789397
1041, epoch_train_loss=4.074775457789397
Epoch 1042
Epoch 1042 :: Batch 0/1
Batch Loss = 4.07464620498783
1042, epoch_train_loss=4.07464620498783
Epoch 1043
Epoch 1043 :: Batch 0/1
Batch Loss = 4.074513142171622
1043, epoch_train_loss=4.074513142171622
Epoch 1044
Epoch 1044 :: Batch 0/1
Batch Loss = 4.0743806666347355
1044, epoch_train_loss=4.0743806666347355
Epoch 1045
Epoch 1045 :: Batch 0/1
Batch Loss = 4.074248815263611
1045, epoch_train_loss=4.074248815263611
Epoch 1046
Epoch 1046 :: Batch 0/1
Batch Loss = 4.074118453052215
1046, epoch_train_loss=4.074118453052215
Epoch 1047
Epoch 1047 :: Batch 0/1
Batch Loss = 4.073983935550468
1047, epoch_train_loss=4.073983935550468
Epoch 1048
Epoch 1048 :: Batch 0/1
Batch Loss = 4.073852601790932
1048, epoch_train_loss=4.073852601790932
Epoch 1049
Epoch 1049 :: Batch 0/1
Batch Loss = 4.073719756525626
1049, epoch_train_loss=4.073719756525626
Epoch 1050
Epoch 1050 :: Batch 0/1
Batch Loss = 4.0735863805489405
1050, epoch_train_loss=4.0735863805489405
Epoch 1051
Epoch 1051 :: Batch 0/1
Batch Loss = 4.073451922383786
1051, epoch_train_loss=4.073451922383786
Epoch 1052
Epoch 1052 :: Batch 0/1
Batch Loss = 4.073319340019398
1052, epoch_train_loss=4.073319340019398
Epoch 1053
Epoch 1053 :: Batch 0/1
Batch Loss = 4.073183660218202
1053, epoch_train_loss=4.073183660218202
Epoch 1054
Epoch 1054 :: Batch 0/1
Batch Loss = 4.073048464863982
1054, epoch_train_loss=4.073048464863982
Epoch 1055
Epoch 1055 :: Batch 0/1
Batch Loss = 4.0729123442660935
1055, epoch_train_loss=4.0729123442660935
Epoch 1056
Epoch 1056 :: Batch 0/1
Batch Loss = 4.072775805823648
1056, epoch_train_loss=4.072775805823648
Epoch 1057
Epoch 1057 :: Batch 0/1
Batch Loss = 4.0726370198162725
1057, epoch_train_loss=4.0726370198162725
Epoch 1058
Epoch 1058 :: Batch 0/1
Batch Loss = 4.07249842530185
1058, epoch_train_loss=4.07249842530185
Epoch 1059
Epoch 1059 :: Batch 0/1
Batch Loss = 4.072358307316339
1059, epoch_train_loss=4.072358307316339
Epoch 1060
Epoch 1060 :: Batch 0/1
Batch Loss = 4.0722168638345675
1060, epoch_train_loss=4.0722168638345675
Epoch 1061
Epoch 1061 :: Batch 0/1
Batch Loss = 4.072073580003265
1061, epoch_train_loss=4.072073580003265
Epoch 1062
Epoch 1062 :: Batch 0/1
Batch Loss = 4.071929566290736
1062, epoch_train_loss=4.071929566290736
Epoch 1063
Epoch 1063 :: Batch 0/1
Batch Loss = 4.07178311226414
1063, epoch_train_loss=4.07178311226414
Epoch 1064
Epoch 1064 :: Batch 0/1
Batch Loss = 4.071634828294456
1064, epoch_train_loss=4.071634828294456
Epoch 1065
Epoch 1065 :: Batch 0/1
Batch Loss = 4.07148442341893
1065, epoch_train_loss=4.07148442341893
Epoch 1066
Epoch 1066 :: Batch 0/1
Batch Loss = 4.071332392664558
1066, epoch_train_loss=4.071332392664558
Epoch 1067
Epoch 1067 :: Batch 0/1
Batch Loss = 4.071177772232805
1067, epoch_train_loss=4.071177772232805
Epoch 1068
Epoch 1068 :: Batch 0/1
Batch Loss = 4.071021502569084
1068, epoch_train_loss=4.071021502569084
Epoch 1069
Epoch 1069 :: Batch 0/1
Batch Loss = 4.070863671278404
1069, epoch_train_loss=4.070863671278404
Epoch 1070
Epoch 1070 :: Batch 0/1
Batch Loss = 4.070705143434798
1070, epoch_train_loss=4.070705143434798
Epoch 1071
Epoch 1071 :: Batch 0/1
Batch Loss = 4.070546228894465
1071, epoch_train_loss=4.070546228894465
Epoch 1072
Epoch 1072 :: Batch 0/1
Batch Loss = 4.070388784966967
1072, epoch_train_loss=4.070388784966967
Epoch 1073
Epoch 1073 :: Batch 0/1
Batch Loss = 4.070234326647382
1073, epoch_train_loss=4.070234326647382
Epoch 1074
Epoch 1074 :: Batch 0/1
Batch Loss = 4.070084948756895
1074, epoch_train_loss=4.070084948756895
Epoch 1075
Epoch 1075 :: Batch 0/1
Batch Loss = 4.069942213938164
1075, epoch_train_loss=4.069942213938164
Epoch 1076
Epoch 1076 :: Batch 0/1
Batch Loss = 4.069807110727599
1076, epoch_train_loss=4.069807110727599
Epoch 1077
Epoch 1077 :: Batch 0/1
Batch Loss = 4.069677388552876
1077, epoch_train_loss=4.069677388552876
Epoch 1078
Epoch 1078 :: Batch 0/1
Batch Loss = 4.069547981281612
1078, epoch_train_loss=4.069547981281612
Epoch 1079
Epoch 1079 :: Batch 0/1
Batch Loss = 4.069413626733804
1079, epoch_train_loss=4.069413626733804
Epoch 1080
Epoch 1080 :: Batch 0/1
Batch Loss = 4.069273932862351
1080, epoch_train_loss=4.069273932862351
Epoch 1081
Epoch 1081 :: Batch 0/1
Batch Loss = 4.069132317517631
1081, epoch_train_loss=4.069132317517631
Epoch 1082
Epoch 1082 :: Batch 0/1
Batch Loss = 4.0689919701385255
1082, epoch_train_loss=4.0689919701385255
Epoch 1083
Epoch 1083 :: Batch 0/1
Batch Loss = 4.068852560349902
1083, epoch_train_loss=4.068852560349902
Epoch 1084
Epoch 1084 :: Batch 0/1
Batch Loss = 4.068711699375647
1084, epoch_train_loss=4.068711699375647
Epoch 1085
Epoch 1085 :: Batch 0/1
Batch Loss = 4.0685670570321415
1085, epoch_train_loss=4.0685670570321415
Epoch 1086
Epoch 1086 :: Batch 0/1
Batch Loss = 4.068418357085593
1086, epoch_train_loss=4.068418357085593
Epoch 1087
Epoch 1087 :: Batch 0/1
Batch Loss = 4.068267319388012
1087, epoch_train_loss=4.068267319388012
Epoch 1088
Epoch 1088 :: Batch 0/1
Batch Loss = 4.068116933901008
1088, epoch_train_loss=4.068116933901008
Epoch 1089
Epoch 1089 :: Batch 0/1
Batch Loss = 4.067969788987206
1089, epoch_train_loss=4.067969788987206
Epoch 1090
Epoch 1090 :: Batch 0/1
Batch Loss = 4.067827105054889
1090, epoch_train_loss=4.067827105054889
Epoch 1091
Epoch 1091 :: Batch 0/1
Batch Loss = 4.067688452156419
1091, epoch_train_loss=4.067688452156419
Epoch 1092
Epoch 1092 :: Batch 0/1
Batch Loss = 4.067552477100447
1092, epoch_train_loss=4.067552477100447
Epoch 1093
Epoch 1093 :: Batch 0/1
Batch Loss = 4.06741792596182
1093, epoch_train_loss=4.06741792596182
Epoch 1094
Epoch 1094 :: Batch 0/1
Batch Loss = 4.067284257786342
1094, epoch_train_loss=4.067284257786342
Epoch 1095
Epoch 1095 :: Batch 0/1
Batch Loss = 4.067151655744573
1095, epoch_train_loss=4.067151655744573
Epoch 1096
Epoch 1096 :: Batch 0/1
Batch Loss = 4.06702053750884
1096, epoch_train_loss=4.06702053750884
Epoch 1097
Epoch 1097 :: Batch 0/1
Batch Loss = 4.066891147424277
1097, epoch_train_loss=4.066891147424277
Epoch 1098
Epoch 1098 :: Batch 0/1
Batch Loss = 4.06676331719863
1098, epoch_train_loss=4.06676331719863
Epoch 1099
Epoch 1099 :: Batch 0/1
Batch Loss = 4.066636576573189
1099, epoch_train_loss=4.066636576573189
Epoch 1100
Epoch 1100 :: Batch 0/1
Batch Loss = 4.066510314674417
1100, epoch_train_loss=4.066510314674417
Epoch 1101
Epoch 1101 :: Batch 0/1
Batch Loss = 4.066384074827382
1101, epoch_train_loss=4.066384074827382
Epoch 1102
Epoch 1102 :: Batch 0/1
Batch Loss = 4.0662576676808895
1102, epoch_train_loss=4.0662576676808895
Epoch 1103
Epoch 1103 :: Batch 0/1
Batch Loss = 4.066131250821739
1103, epoch_train_loss=4.066131250821739
Epoch 1104
Epoch 1104 :: Batch 0/1
Batch Loss = 4.066005164015304
1104, epoch_train_loss=4.066005164015304
Epoch 1105
Epoch 1105 :: Batch 0/1
Batch Loss = 4.065879762206005
1105, epoch_train_loss=4.065879762206005
Epoch 1106
Epoch 1106 :: Batch 0/1
Batch Loss = 4.0657551674996055
1106, epoch_train_loss=4.0657551674996055
Epoch 1107
Epoch 1107 :: Batch 0/1
Batch Loss = 4.0656312130010175
1107, epoch_train_loss=4.0656312130010175
Epoch 1108
Epoch 1108 :: Batch 0/1
Batch Loss = 4.065507493350032
1108, epoch_train_loss=4.065507493350032
Epoch 1109
Epoch 1109 :: Batch 0/1
Batch Loss = 4.065383615993949
1109, epoch_train_loss=4.065383615993949
Epoch 1110
Epoch 1110 :: Batch 0/1
Batch Loss = 4.065259372952187
1110, epoch_train_loss=4.065259372952187
Epoch 1111
Epoch 1111 :: Batch 0/1
Batch Loss = 4.065134797086872
1111, epoch_train_loss=4.065134797086872
Epoch 1112
Epoch 1112 :: Batch 0/1
Batch Loss = 4.065010005018453
1112, epoch_train_loss=4.065010005018453
Epoch 1113
Epoch 1113 :: Batch 0/1
Batch Loss = 4.06488504419138
1113, epoch_train_loss=4.06488504419138
Epoch 1114
Epoch 1114 :: Batch 0/1
Batch Loss = 4.06475982318025
1114, epoch_train_loss=4.06475982318025
Epoch 1115
Epoch 1115 :: Batch 0/1
Batch Loss = 4.064634172967154
1115, epoch_train_loss=4.064634172967154
Epoch 1116
Epoch 1116 :: Batch 0/1
Batch Loss = 4.064507943099834
1116, epoch_train_loss=4.064507943099834
Epoch 1117
Epoch 1117 :: Batch 0/1
Batch Loss = 4.064381047929522
1117, epoch_train_loss=4.064381047929522
Epoch 1118
Epoch 1118 :: Batch 0/1
Batch Loss = 4.0642534603461105
1118, epoch_train_loss=4.0642534603461105
Epoch 1119
Epoch 1119 :: Batch 0/1
Batch Loss = 4.064125167167792
1119, epoch_train_loss=4.064125167167792
Epoch 1120
Epoch 1120 :: Batch 0/1
Batch Loss = 4.063996155076622
1120, epoch_train_loss=4.063996155076622
Epoch 1121
Epoch 1121 :: Batch 0/1
Batch Loss = 4.063866403906206
1121, epoch_train_loss=4.063866403906206
Epoch 1122
Epoch 1122 :: Batch 0/1
Batch Loss = 4.063735888755257
1122, epoch_train_loss=4.063735888755257
Epoch 1123
Epoch 1123 :: Batch 0/1
Batch Loss = 4.0636045561127
1123, epoch_train_loss=4.0636045561127
Epoch 1124
Epoch 1124 :: Batch 0/1
Batch Loss = 4.063472319646809
1124, epoch_train_loss=4.063472319646809
Epoch 1125
Epoch 1125 :: Batch 0/1
Batch Loss = 4.063339081557765
1125, epoch_train_loss=4.063339081557765
Epoch 1126
Epoch 1126 :: Batch 0/1
Batch Loss = 4.0632047881932785
1126, epoch_train_loss=4.0632047881932785
Epoch 1127
Epoch 1127 :: Batch 0/1
Batch Loss = 4.063069454330665
1127, epoch_train_loss=4.063069454330665
Epoch 1128
Epoch 1128 :: Batch 0/1
Batch Loss = 4.062933143516141
1128, epoch_train_loss=4.062933143516141
Epoch 1129
Epoch 1129 :: Batch 0/1
Batch Loss = 4.062795908510171
1129, epoch_train_loss=4.062795908510171
Epoch 1130
Epoch 1130 :: Batch 0/1
Batch Loss = 4.062657759090405
1130, epoch_train_loss=4.062657759090405
Epoch 1131
Epoch 1131 :: Batch 0/1
Batch Loss = 4.062518674607608
1131, epoch_train_loss=4.062518674607608
Epoch 1132
Epoch 1132 :: Batch 0/1
Batch Loss = 4.062378648754811
1132, epoch_train_loss=4.062378648754811
Epoch 1133
Epoch 1133 :: Batch 0/1
Batch Loss = 4.062237716183299
1133, epoch_train_loss=4.062237716183299
Epoch 1134
Epoch 1134 :: Batch 0/1
Batch Loss = 4.0620959404295105
1134, epoch_train_loss=4.0620959404295105
Epoch 1135
Epoch 1135 :: Batch 0/1
Batch Loss = 4.061953381650183
1135, epoch_train_loss=4.061953381650183
Epoch 1136
Epoch 1136 :: Batch 0/1
Batch Loss = 4.061810074035201
1136, epoch_train_loss=4.061810074035201
Epoch 1137
Epoch 1137 :: Batch 0/1
Batch Loss = 4.061666031091342
1137, epoch_train_loss=4.061666031091342
Epoch 1138
Epoch 1138 :: Batch 0/1
Batch Loss = 4.061521260842729
1138, epoch_train_loss=4.061521260842729
Epoch 1139
Epoch 1139 :: Batch 0/1
Batch Loss = 4.061375777402265
1139, epoch_train_loss=4.061375777402265
Epoch 1140
Epoch 1140 :: Batch 0/1
Batch Loss = 4.061229597040255
1140, epoch_train_loss=4.061229597040255
Epoch 1141
Epoch 1141 :: Batch 0/1
Batch Loss = 4.061082733131836
1141, epoch_train_loss=4.061082733131836
Epoch 1142
Epoch 1142 :: Batch 0/1
Batch Loss = 4.060935193782675
1142, epoch_train_loss=4.060935193782675
Epoch 1143
Epoch 1143 :: Batch 0/1
Batch Loss = 4.060786985869835
1143, epoch_train_loss=4.060786985869835
Epoch 1144
Epoch 1144 :: Batch 0/1
Batch Loss = 4.060638115462969
1144, epoch_train_loss=4.060638115462969
Epoch 1145
Epoch 1145 :: Batch 0/1
Batch Loss = 4.060488584577516
1145, epoch_train_loss=4.060488584577516
Epoch 1146
Epoch 1146 :: Batch 0/1
Batch Loss = 4.060338386969741
1146, epoch_train_loss=4.060338386969741
Epoch 1147
Epoch 1147 :: Batch 0/1
Batch Loss = 4.060187508454785
1147, epoch_train_loss=4.060187508454785
Epoch 1148
Epoch 1148 :: Batch 0/1
Batch Loss = 4.060035932369246
1148, epoch_train_loss=4.060035932369246
Epoch 1149
Epoch 1149 :: Batch 0/1
Batch Loss = 4.059883643541494
1149, epoch_train_loss=4.059883643541494
Epoch 1150
Epoch 1150 :: Batch 0/1
Batch Loss = 4.059730628204674
1150, epoch_train_loss=4.059730628204674
Epoch 1151
Epoch 1151 :: Batch 0/1
Batch Loss = 4.059576868572904
1151, epoch_train_loss=4.059576868572904
Epoch 1152
Epoch 1152 :: Batch 0/1
Batch Loss = 4.059422339452617
1152, epoch_train_loss=4.059422339452617
Epoch 1153
Epoch 1153 :: Batch 0/1
Batch Loss = 4.05926700829169
1153, epoch_train_loss=4.05926700829169
Epoch 1154
Epoch 1154 :: Batch 0/1
Batch Loss = 4.059110841126484
1154, epoch_train_loss=4.059110841126484
Epoch 1155
Epoch 1155 :: Batch 0/1
Batch Loss = 4.058953807092244
1155, epoch_train_loss=4.058953807092244
Epoch 1156
Epoch 1156 :: Batch 0/1
Batch Loss = 4.058795883125132
1156, epoch_train_loss=4.058795883125132
Epoch 1157
Epoch 1157 :: Batch 0/1
Batch Loss = 4.058637053667611
1157, epoch_train_loss=4.058637053667611
Epoch 1158
Epoch 1158 :: Batch 0/1
Batch Loss = 4.05847731817412
1158, epoch_train_loss=4.05847731817412
Epoch 1159
Epoch 1159 :: Batch 0/1
Batch Loss = 4.05831670003877
1159, epoch_train_loss=4.05831670003877
Epoch 1160
Epoch 1160 :: Batch 0/1
Batch Loss = 4.058155277381123
1160, epoch_train_loss=4.058155277381123
Epoch 1161
Epoch 1161 :: Batch 0/1
Batch Loss = 4.05799324752187
1161, epoch_train_loss=4.05799324752187
Epoch 1162
Epoch 1162 :: Batch 0/1
Batch Loss = 4.057831007583132
1162, epoch_train_loss=4.057831007583132
Epoch 1163
Epoch 1163 :: Batch 0/1
Batch Loss = 4.057669634077186
1163, epoch_train_loss=4.057669634077186
Epoch 1164
Epoch 1164 :: Batch 0/1
Batch Loss = 4.057510909417268
1164, epoch_train_loss=4.057510909417268
Epoch 1165
Epoch 1165 :: Batch 0/1
Batch Loss = 4.0573614082162575
1165, epoch_train_loss=4.0573614082162575
Epoch 1166
Epoch 1166 :: Batch 0/1
Batch Loss = 4.057228573336929
1166, epoch_train_loss=4.057228573336929
Epoch 1167
Epoch 1167 :: Batch 0/1
Batch Loss = 4.057160022500978
1167, epoch_train_loss=4.057160022500978
Epoch 1168
Epoch 1168 :: Batch 0/1
Batch Loss = 4.057162456026078
1168, epoch_train_loss=4.057162456026078
Epoch 1169
Epoch 1169 :: Batch 0/1
Batch Loss = 4.057620688843088
1169, epoch_train_loss=4.057620688843088
Epoch 1170
Epoch 1170 :: Batch 0/1
Batch Loss = 4.057912669028649
1170, epoch_train_loss=4.057912669028649
Epoch 1171
Epoch 1171 :: Batch 0/1
Batch Loss = 4.060536439238305
1171, epoch_train_loss=4.060536439238305
Epoch 1172
Epoch 1172 :: Batch 0/1
Batch Loss = 4.057809186484474
1172, epoch_train_loss=4.057809186484474
Epoch 1173
Epoch 1173 :: Batch 0/1
Batch Loss = 4.057546263380526
1173, epoch_train_loss=4.057546263380526
Epoch 1174
Epoch 1174 :: Batch 0/1
Batch Loss = 4.0566218790304145
1174, epoch_train_loss=4.0566218790304145
Epoch 1175
Epoch 1175 :: Batch 0/1
Batch Loss = 4.056696426823624
1175, epoch_train_loss=4.056696426823624
Epoch 1176
Epoch 1176 :: Batch 0/1
Batch Loss = 4.056253210519767
1176, epoch_train_loss=4.056253210519767
Epoch 1177
Epoch 1177 :: Batch 0/1
Batch Loss = 4.056539299428762
1177, epoch_train_loss=4.056539299428762
Epoch 1178
Epoch 1178 :: Batch 0/1
Batch Loss = 4.056426591734651
1178, epoch_train_loss=4.056426591734651
Epoch 1179
Epoch 1179 :: Batch 0/1
Batch Loss = 4.058152275716061
1179, epoch_train_loss=4.058152275716061
Epoch 1180
Epoch 1180 :: Batch 0/1
Batch Loss = 4.056500522632538
1180, epoch_train_loss=4.056500522632538
Epoch 1181
Epoch 1181 :: Batch 0/1
Batch Loss = 4.056295871373468
1181, epoch_train_loss=4.056295871373468
Epoch 1182
Epoch 1182 :: Batch 0/1
Batch Loss = 4.056052543214226
1182, epoch_train_loss=4.056052543214226
Epoch 1183
Epoch 1183 :: Batch 0/1
Batch Loss = 4.057110394944314
1183, epoch_train_loss=4.057110394944314
Epoch 1184
Epoch 1184 :: Batch 0/1
Batch Loss = 4.056703627367309
1184, epoch_train_loss=4.056703627367309
Epoch 1185
Epoch 1185 :: Batch 0/1
Batch Loss = 4.0562288085509675
1185, epoch_train_loss=4.0562288085509675
Epoch 1186
Epoch 1186 :: Batch 0/1
Batch Loss = 4.057209136762443
1186, epoch_train_loss=4.057209136762443
Epoch 1187
Epoch 1187 :: Batch 0/1
Batch Loss = 4.058382631078426
1187, epoch_train_loss=4.058382631078426
Epoch 1188
Epoch 1188 :: Batch 0/1
Batch Loss = 4.06116582928926
1188, epoch_train_loss=4.06116582928926
Epoch 1189
Epoch 1189 :: Batch 0/1
Batch Loss = 4.059105437504075
1189, epoch_train_loss=4.059105437504075
Epoch 1190
Epoch 1190 :: Batch 0/1
Batch Loss = 4.0721927699659926
1190, epoch_train_loss=4.0721927699659926
Epoch 1191
Epoch 1191 :: Batch 0/1
Batch Loss = 4.080822421162153
1191, epoch_train_loss=4.080822421162153
Epoch 1192
Epoch 1192 :: Batch 0/1
Batch Loss = 4.082166674244192
1192, epoch_train_loss=4.082166674244192
Epoch 1193
Epoch 1193 :: Batch 0/1
Batch Loss = 4.101736665003634
1193, epoch_train_loss=4.101736665003634
Epoch 1194
Epoch 1194 :: Batch 0/1
Batch Loss = 4.101374313498787
1194, epoch_train_loss=4.101374313498787
Epoch 1195
Epoch 1195 :: Batch 0/1
Batch Loss = 4.110026691059836
1195, epoch_train_loss=4.110026691059836
Epoch 1196
Epoch 1196 :: Batch 0/1
Batch Loss = 4.117971321767548
1196, epoch_train_loss=4.117971321767548
Epoch 1197
Epoch 1197 :: Batch 0/1
Batch Loss = 4.112993765323327
1197, epoch_train_loss=4.112993765323327
Epoch 1198
Epoch 1198 :: Batch 0/1
Batch Loss = 4.110087640982428
1198, epoch_train_loss=4.110087640982428
Epoch 1199
Epoch 1199 :: Batch 0/1
Batch Loss = 4.102334766100312
1199, epoch_train_loss=4.102334766100312
Epoch 1200
Epoch 1200 :: Batch 0/1
Batch Loss = 4.100160349920237
1200, epoch_train_loss=4.100160349920237
Epoch 1201
Epoch 1201 :: Batch 0/1
Batch Loss = 4.095839361981853
1201, epoch_train_loss=4.095839361981853
Epoch 1202
Epoch 1202 :: Batch 0/1
Batch Loss = 4.106235276286815
1202, epoch_train_loss=4.106235276286815
Epoch 1203
Epoch 1203 :: Batch 0/1
Batch Loss = 4.100904299631893
1203, epoch_train_loss=4.100904299631893
Epoch 1204
Epoch 1204 :: Batch 0/1
Batch Loss = 4.097923459906157
1204, epoch_train_loss=4.097923459906157
Epoch 1205
Epoch 1205 :: Batch 0/1
Batch Loss = 4.089776082510519
1205, epoch_train_loss=4.089776082510519
Epoch 1206
Epoch 1206 :: Batch 0/1
Batch Loss = 4.099333751757333
1206, epoch_train_loss=4.099333751757333
Epoch 1207
Epoch 1207 :: Batch 0/1
Batch Loss = 4.095270436833147
1207, epoch_train_loss=4.095270436833147
Epoch 1208
Epoch 1208 :: Batch 0/1
Batch Loss = 4.097170982600664
1208, epoch_train_loss=4.097170982600664
Epoch 1209
Epoch 1209 :: Batch 0/1
Batch Loss = 4.085592273300855
1209, epoch_train_loss=4.085592273300855
Epoch 1210
Epoch 1210 :: Batch 0/1
Batch Loss = 4.100080996747243
1210, epoch_train_loss=4.100080996747243
Epoch 1211
Epoch 1211 :: Batch 0/1
Batch Loss = 4.090554308921475
1211, epoch_train_loss=4.090554308921475
Epoch 1212
Epoch 1212 :: Batch 0/1
Batch Loss = 4.101632642287935
1212, epoch_train_loss=4.101632642287935
Epoch 1213
Epoch 1213 :: Batch 0/1
Batch Loss = 4.103766967695391
1213, epoch_train_loss=4.103766967695391
Epoch 1214
Epoch 1214 :: Batch 0/1
Batch Loss = 4.096988234692217
1214, epoch_train_loss=4.096988234692217
Epoch 1215
Epoch 1215 :: Batch 0/1
Batch Loss = 4.083259392831994
1215, epoch_train_loss=4.083259392831994
Epoch 1216
Epoch 1216 :: Batch 0/1
Batch Loss = 4.084347540677775
1216, epoch_train_loss=4.084347540677775
Epoch 1217
Epoch 1217 :: Batch 0/1
Batch Loss = 4.0707914227294415
1217, epoch_train_loss=4.0707914227294415
Epoch 1218
Epoch 1218 :: Batch 0/1
Batch Loss = 4.083936333551042
1218, epoch_train_loss=4.083936333551042
Epoch 1219
Epoch 1219 :: Batch 0/1
Batch Loss = 4.078370211860224
1219, epoch_train_loss=4.078370211860224
Epoch 1220
Epoch 1220 :: Batch 0/1
Batch Loss = 4.078301057624068
1220, epoch_train_loss=4.078301057624068
Epoch 1221
Epoch 1221 :: Batch 0/1
Batch Loss = 4.083104744796982
1221, epoch_train_loss=4.083104744796982
Epoch 1222
Epoch 1222 :: Batch 0/1
Batch Loss = 4.062471583988977
1222, epoch_train_loss=4.062471583988977
Epoch 1223
Epoch 1223 :: Batch 0/1
Batch Loss = 4.065231691578827
1223, epoch_train_loss=4.065231691578827
Epoch 1224
Epoch 1224 :: Batch 0/1
Batch Loss = 4.063435676258804
1224, epoch_train_loss=4.063435676258804
Epoch 1225
Epoch 1225 :: Batch 0/1
Batch Loss = 4.066251266915814
1225, epoch_train_loss=4.066251266915814
Epoch 1226
Epoch 1226 :: Batch 0/1
Batch Loss = 4.061061852743673
1226, epoch_train_loss=4.061061852743673
Epoch 1227
Epoch 1227 :: Batch 0/1
Batch Loss = 4.064031854040452
1227, epoch_train_loss=4.064031854040452
Epoch 1228
Epoch 1228 :: Batch 0/1
Batch Loss = 4.0636161464350105
1228, epoch_train_loss=4.0636161464350105
Epoch 1229
Epoch 1229 :: Batch 0/1
Batch Loss = 4.072192780125245
1229, epoch_train_loss=4.072192780125245
Epoch 1230
Epoch 1230 :: Batch 0/1
Batch Loss = 4.065681590667283
1230, epoch_train_loss=4.065681590667283
Epoch 1231
Epoch 1231 :: Batch 0/1
Batch Loss = 4.062746979291496
1231, epoch_train_loss=4.062746979291496
Epoch 1232
Epoch 1232 :: Batch 0/1
Batch Loss = 4.053791690652766
1232, epoch_train_loss=4.053791690652766
Epoch 1233
Epoch 1233 :: Batch 0/1
Batch Loss = 4.062160048803954
1233, epoch_train_loss=4.062160048803954
Epoch 1234
Epoch 1234 :: Batch 0/1
Batch Loss = 4.063266534110748
1234, epoch_train_loss=4.063266534110748
Epoch 1235
Epoch 1235 :: Batch 0/1
Batch Loss = 4.063566525514261
1235, epoch_train_loss=4.063566525514261
Epoch 1236
Epoch 1236 :: Batch 0/1
Batch Loss = 4.052680412428316
1236, epoch_train_loss=4.052680412428316
Epoch 1237
Epoch 1237 :: Batch 0/1
Batch Loss = 4.0663166502409736
1237, epoch_train_loss=4.0663166502409736
Epoch 1238
Epoch 1238 :: Batch 0/1
Batch Loss = 4.071131636954798
1238, epoch_train_loss=4.071131636954798
Epoch 1239
Epoch 1239 :: Batch 0/1
Batch Loss = 4.082800883435272
1239, epoch_train_loss=4.082800883435272
Epoch 1240
Epoch 1240 :: Batch 0/1
Batch Loss = 4.07422081211251
1240, epoch_train_loss=4.07422081211251
Epoch 1241
Epoch 1241 :: Batch 0/1
Batch Loss = 4.0673842358365295
1241, epoch_train_loss=4.0673842358365295
Epoch 1242
Epoch 1242 :: Batch 0/1
Batch Loss = 4.08402777469363
1242, epoch_train_loss=4.08402777469363
Epoch 1243
Epoch 1243 :: Batch 0/1
Batch Loss = 4.077116204550269
1243, epoch_train_loss=4.077116204550269
Epoch 1244
Epoch 1244 :: Batch 0/1
Batch Loss = 4.092826037106396
1244, epoch_train_loss=4.092826037106396
Epoch 1245
Epoch 1245 :: Batch 0/1
Batch Loss = 4.0959660329769765
1245, epoch_train_loss=4.0959660329769765
Epoch 1246
Epoch 1246 :: Batch 0/1
Batch Loss = 4.097855719590716
1246, epoch_train_loss=4.097855719590716
Epoch 1247
Epoch 1247 :: Batch 0/1
Batch Loss = 4.085415145325983
1247, epoch_train_loss=4.085415145325983
Epoch 1248
Epoch 1248 :: Batch 0/1
Batch Loss = 4.091302051826744
1248, epoch_train_loss=4.091302051826744
Epoch 1249
Epoch 1249 :: Batch 0/1
Batch Loss = 4.0847309417200846
1249, epoch_train_loss=4.0847309417200846
Epoch 1250
Epoch 1250 :: Batch 0/1
Batch Loss = 4.086488422715927
1250, epoch_train_loss=4.086488422715927
Epoch 1251
Epoch 1251 :: Batch 0/1
Batch Loss = 4.085891448995302
1251, epoch_train_loss=4.085891448995302
Epoch 1252
Epoch 1252 :: Batch 0/1
Batch Loss = 4.0774881186453955
1252, epoch_train_loss=4.0774881186453955
Epoch 1253
Epoch 1253 :: Batch 0/1
Batch Loss = 4.075906153088955
1253, epoch_train_loss=4.075906153088955
Epoch 1254
Epoch 1254 :: Batch 0/1
Batch Loss = 4.07612667006424
1254, epoch_train_loss=4.07612667006424
Epoch 1255
Epoch 1255 :: Batch 0/1
Batch Loss = 4.070516295255909
1255, epoch_train_loss=4.070516295255909
Epoch 1256
Epoch 1256 :: Batch 0/1
Batch Loss = 4.069419275820107
1256, epoch_train_loss=4.069419275820107
Epoch 1257
Epoch 1257 :: Batch 0/1
Batch Loss = 4.056991574570543
1257, epoch_train_loss=4.056991574570543
Epoch 1258
Epoch 1258 :: Batch 0/1
Batch Loss = 4.0834743280990375
1258, epoch_train_loss=4.0834743280990375
Epoch 1259
Epoch 1259 :: Batch 0/1
Batch Loss = 4.06755564123797
1259, epoch_train_loss=4.06755564123797
Epoch 1260
Epoch 1260 :: Batch 0/1
Batch Loss = 4.0806017193324795
1260, epoch_train_loss=4.0806017193324795
Epoch 1261
Epoch 1261 :: Batch 0/1
Batch Loss = 4.082991399688981
1261, epoch_train_loss=4.082991399688981
Epoch 1262
Epoch 1262 :: Batch 0/1
Batch Loss = 4.088244606421659
1262, epoch_train_loss=4.088244606421659
Epoch 1263
Epoch 1263 :: Batch 0/1
Batch Loss = 4.09090344417302
1263, epoch_train_loss=4.09090344417302
Epoch 1264
Epoch 1264 :: Batch 0/1
Batch Loss = 4.086409120554056
1264, epoch_train_loss=4.086409120554056
Epoch 1265
Epoch 1265 :: Batch 0/1
Batch Loss = 4.0835304554867555
1265, epoch_train_loss=4.0835304554867555
Epoch 1266
Epoch 1266 :: Batch 0/1
Batch Loss = 4.0786781542345665
1266, epoch_train_loss=4.0786781542345665
Epoch 1267
Epoch 1267 :: Batch 0/1
Batch Loss = 4.071034005101487
1267, epoch_train_loss=4.071034005101487
Epoch 1268
Epoch 1268 :: Batch 0/1
Batch Loss = 4.067635652823367
1268, epoch_train_loss=4.067635652823367
Epoch 1269
Epoch 1269 :: Batch 0/1
Batch Loss = 4.068904731252062
1269, epoch_train_loss=4.068904731252062
Epoch 1270
Epoch 1270 :: Batch 0/1
Batch Loss = 4.056919484585916
1270, epoch_train_loss=4.056919484585916
Epoch 1271
Epoch 1271 :: Batch 0/1
Batch Loss = 4.061350325203523
1271, epoch_train_loss=4.061350325203523
Epoch 1272
Epoch 1272 :: Batch 0/1
Batch Loss = 4.060486264483143
1272, epoch_train_loss=4.060486264483143
Epoch 1273
Epoch 1273 :: Batch 0/1
Batch Loss = 4.058095818228204
1273, epoch_train_loss=4.058095818228204
Epoch 1274
Epoch 1274 :: Batch 0/1
Batch Loss = 4.058040870458393
1274, epoch_train_loss=4.058040870458393
Epoch 1275
Epoch 1275 :: Batch 0/1
Batch Loss = 4.052817014083633
1275, epoch_train_loss=4.052817014083633
Epoch 1276
Epoch 1276 :: Batch 0/1
Batch Loss = 4.051407836266261
1276, epoch_train_loss=4.051407836266261
Epoch 1277
Epoch 1277 :: Batch 0/1
Batch Loss = 4.056112718123475
1277, epoch_train_loss=4.056112718123475
Epoch 1278
Epoch 1278 :: Batch 0/1
Batch Loss = 4.052883539961821
1278, epoch_train_loss=4.052883539961821
Epoch 1279
Epoch 1279 :: Batch 0/1
Batch Loss = 4.051769753944481
1279, epoch_train_loss=4.051769753944481
Epoch 1280
Epoch 1280 :: Batch 0/1
Batch Loss = 4.050318785945734
1280, epoch_train_loss=4.050318785945734
Epoch 1281
Epoch 1281 :: Batch 0/1
Batch Loss = 4.046013080250595
1281, epoch_train_loss=4.046013080250595
Epoch 1282
Epoch 1282 :: Batch 0/1
Batch Loss = 4.044990046250402
1282, epoch_train_loss=4.044990046250402
Epoch 1283
Epoch 1283 :: Batch 0/1
Batch Loss = 4.046317519814881
1283, epoch_train_loss=4.046317519814881
Epoch 1284
Epoch 1284 :: Batch 0/1
Batch Loss = 4.0448306634380105
1284, epoch_train_loss=4.0448306634380105
Epoch 1285
Epoch 1285 :: Batch 0/1
Batch Loss = 4.043377885379924
1285, epoch_train_loss=4.043377885379924
Epoch 1286
Epoch 1286 :: Batch 0/1
Batch Loss = 4.046248356604461
1286, epoch_train_loss=4.046248356604461
Epoch 1287
Epoch 1287 :: Batch 0/1
Batch Loss = 4.042926182374452
1287, epoch_train_loss=4.042926182374452
Epoch 1288
Epoch 1288 :: Batch 0/1
Batch Loss = 4.042746045311599
1288, epoch_train_loss=4.042746045311599
Epoch 1289
Epoch 1289 :: Batch 0/1
Batch Loss = 4.042015824277293
1289, epoch_train_loss=4.042015824277293
Epoch 1290
Epoch 1290 :: Batch 0/1
Batch Loss = 4.039927111751127
1290, epoch_train_loss=4.039927111751127
Epoch 1291
Epoch 1291 :: Batch 0/1
Batch Loss = 4.040266938962408
1291, epoch_train_loss=4.040266938962408
Epoch 1292
Epoch 1292 :: Batch 0/1
Batch Loss = 4.040211472858915
1292, epoch_train_loss=4.040211472858915
Epoch 1293
Epoch 1293 :: Batch 0/1
Batch Loss = 4.040443876283514
1293, epoch_train_loss=4.040443876283514
Epoch 1294
Epoch 1294 :: Batch 0/1
Batch Loss = 4.038936029594549
1294, epoch_train_loss=4.038936029594549
Epoch 1295
Epoch 1295 :: Batch 0/1
Batch Loss = 4.039713052483404
1295, epoch_train_loss=4.039713052483404
Epoch 1296
Epoch 1296 :: Batch 0/1
Batch Loss = 4.038075331009618
1296, epoch_train_loss=4.038075331009618
Epoch 1297
Epoch 1297 :: Batch 0/1
Batch Loss = 4.037824491192474
1297, epoch_train_loss=4.037824491192474
Epoch 1298
Epoch 1298 :: Batch 0/1
Batch Loss = 4.03707816126988
1298, epoch_train_loss=4.03707816126988
Epoch 1299
Epoch 1299 :: Batch 0/1
Batch Loss = 4.037532985277004
1299, epoch_train_loss=4.037532985277004
Epoch 1300
Epoch 1300 :: Batch 0/1
Batch Loss = 4.036622241078421
1300, epoch_train_loss=4.036622241078421
Epoch 1301
Epoch 1301 :: Batch 0/1
Batch Loss = 4.037068398632383
1301, epoch_train_loss=4.037068398632383
Epoch 1302
Epoch 1302 :: Batch 0/1
Batch Loss = 4.036316281522512
1302, epoch_train_loss=4.036316281522512
Epoch 1303
Epoch 1303 :: Batch 0/1
Batch Loss = 4.03590194386484
1303, epoch_train_loss=4.03590194386484
Epoch 1304
Epoch 1304 :: Batch 0/1
Batch Loss = 4.0353381588540715
1304, epoch_train_loss=4.0353381588540715
Epoch 1305
Epoch 1305 :: Batch 0/1
Batch Loss = 4.03524659007045
1305, epoch_train_loss=4.03524659007045
Epoch 1306
Epoch 1306 :: Batch 0/1
Batch Loss = 4.034865182920258
1306, epoch_train_loss=4.034865182920258
Epoch 1307
Epoch 1307 :: Batch 0/1
Batch Loss = 4.034660729809039
1307, epoch_train_loss=4.034660729809039
Epoch 1308
Epoch 1308 :: Batch 0/1
Batch Loss = 4.034454036935936
1308, epoch_train_loss=4.034454036935936
Epoch 1309
Epoch 1309 :: Batch 0/1
Batch Loss = 4.034153194241891
1309, epoch_train_loss=4.034153194241891
Epoch 1310
Epoch 1310 :: Batch 0/1
Batch Loss = 4.033700966079137
1310, epoch_train_loss=4.033700966079137
Epoch 1311
Epoch 1311 :: Batch 0/1
Batch Loss = 4.0334477946519804
1311, epoch_train_loss=4.0334477946519804
Epoch 1312
Epoch 1312 :: Batch 0/1
Batch Loss = 4.033113197671912
1312, epoch_train_loss=4.033113197671912
Epoch 1313
Epoch 1313 :: Batch 0/1
Batch Loss = 4.032850005432742
1313, epoch_train_loss=4.032850005432742
Epoch 1314
Epoch 1314 :: Batch 0/1
Batch Loss = 4.032698349079433
1314, epoch_train_loss=4.032698349079433
Epoch 1315
Epoch 1315 :: Batch 0/1
Batch Loss = 4.03240941682418
1315, epoch_train_loss=4.03240941682418
Epoch 1316
Epoch 1316 :: Batch 0/1
Batch Loss = 4.032102333098722
1316, epoch_train_loss=4.032102333098722
Epoch 1317
Epoch 1317 :: Batch 0/1
Batch Loss = 4.031787243133026
1317, epoch_train_loss=4.031787243133026
Epoch 1318
Epoch 1318 :: Batch 0/1
Batch Loss = 4.0314990202227285
1318, epoch_train_loss=4.0314990202227285
Epoch 1319
Epoch 1319 :: Batch 0/1
Batch Loss = 4.031103470960397
1319, epoch_train_loss=4.031103470960397
Epoch 1320
Epoch 1320 :: Batch 0/1
Batch Loss = 4.0309199612509135
1320, epoch_train_loss=4.0309199612509135
Epoch 1321
Epoch 1321 :: Batch 0/1
Batch Loss = 4.0305710179692325
1321, epoch_train_loss=4.0305710179692325
Epoch 1322
Epoch 1322 :: Batch 0/1
Batch Loss = 4.030312189936984
1322, epoch_train_loss=4.030312189936984
Epoch 1323
Epoch 1323 :: Batch 0/1
Batch Loss = 4.029987209087852
1323, epoch_train_loss=4.029987209087852
Epoch 1324
Epoch 1324 :: Batch 0/1
Batch Loss = 4.029697743336949
1324, epoch_train_loss=4.029697743336949
Epoch 1325
Epoch 1325 :: Batch 0/1
Batch Loss = 4.029380639398421
1325, epoch_train_loss=4.029380639398421
Epoch 1326
Epoch 1326 :: Batch 0/1
Batch Loss = 4.029140718251304
1326, epoch_train_loss=4.029140718251304
Epoch 1327
Epoch 1327 :: Batch 0/1
Batch Loss = 4.02883813475051
1327, epoch_train_loss=4.02883813475051
Epoch 1328
Epoch 1328 :: Batch 0/1
Batch Loss = 4.028614649379576
1328, epoch_train_loss=4.028614649379576
Epoch 1329
Epoch 1329 :: Batch 0/1
Batch Loss = 4.028338385607892
1329, epoch_train_loss=4.028338385607892
Epoch 1330
Epoch 1330 :: Batch 0/1
Batch Loss = 4.028103657011049
1330, epoch_train_loss=4.028103657011049
Epoch 1331
Epoch 1331 :: Batch 0/1
Batch Loss = 4.027871295965742
1331, epoch_train_loss=4.027871295965742
Epoch 1332
Epoch 1332 :: Batch 0/1
Batch Loss = 4.027668209256484
1332, epoch_train_loss=4.027668209256484
Epoch 1333
Epoch 1333 :: Batch 0/1
Batch Loss = 4.027453254601836
1333, epoch_train_loss=4.027453254601836
Epoch 1334
Epoch 1334 :: Batch 0/1
Batch Loss = 4.027265441062476
1334, epoch_train_loss=4.027265441062476
Epoch 1335
Epoch 1335 :: Batch 0/1
Batch Loss = 4.027029495437
1335, epoch_train_loss=4.027029495437
Epoch 1336
Epoch 1336 :: Batch 0/1
Batch Loss = 4.026799498627122
1336, epoch_train_loss=4.026799498627122
Epoch 1337
Epoch 1337 :: Batch 0/1
Batch Loss = 4.026551605530468
1337, epoch_train_loss=4.026551605530468
Epoch 1338
Epoch 1338 :: Batch 0/1
Batch Loss = 4.026289528902921
1338, epoch_train_loss=4.026289528902921
Epoch 1339
Epoch 1339 :: Batch 0/1
Batch Loss = 4.026041788927865
1339, epoch_train_loss=4.026041788927865
Epoch 1340
Epoch 1340 :: Batch 0/1
Batch Loss = 4.025794682802592
1340, epoch_train_loss=4.025794682802592
Epoch 1341
Epoch 1341 :: Batch 0/1
Batch Loss = 4.025552918161089
1341, epoch_train_loss=4.025552918161089
Epoch 1342
Epoch 1342 :: Batch 0/1
Batch Loss = 4.025317439122548
1342, epoch_train_loss=4.025317439122548
Epoch 1343
Epoch 1343 :: Batch 0/1
Batch Loss = 4.0250879232403065
1343, epoch_train_loss=4.0250879232403065
Epoch 1344
Epoch 1344 :: Batch 0/1
Batch Loss = 4.024853456418072
1344, epoch_train_loss=4.024853456418072
Epoch 1345
Epoch 1345 :: Batch 0/1
Batch Loss = 4.024635655864172
1345, epoch_train_loss=4.024635655864172
Epoch 1346
Epoch 1346 :: Batch 0/1
Batch Loss = 4.024410719994062
1346, epoch_train_loss=4.024410719994062
Epoch 1347
Epoch 1347 :: Batch 0/1
Batch Loss = 4.024195185957625
1347, epoch_train_loss=4.024195185957625
Epoch 1348
Epoch 1348 :: Batch 0/1
Batch Loss = 4.023972222009807
1348, epoch_train_loss=4.023972222009807
Epoch 1349
Epoch 1349 :: Batch 0/1
Batch Loss = 4.023754355648973
1349, epoch_train_loss=4.023754355648973
Epoch 1350
Epoch 1350 :: Batch 0/1
Batch Loss = 4.023527216932263
1350, epoch_train_loss=4.023527216932263
Epoch 1351
Epoch 1351 :: Batch 0/1
Batch Loss = 4.023309974409129
1351, epoch_train_loss=4.023309974409129
Epoch 1352
Epoch 1352 :: Batch 0/1
Batch Loss = 4.023089088170515
1352, epoch_train_loss=4.023089088170515
Epoch 1353
Epoch 1353 :: Batch 0/1
Batch Loss = 4.0228774945271235
1353, epoch_train_loss=4.0228774945271235
Epoch 1354
Epoch 1354 :: Batch 0/1
Batch Loss = 4.022665263029991
1354, epoch_train_loss=4.022665263029991
Epoch 1355
Epoch 1355 :: Batch 0/1
Batch Loss = 4.022458750568758
1355, epoch_train_loss=4.022458750568758
Epoch 1356
Epoch 1356 :: Batch 0/1
Batch Loss = 4.0222493299885524
1356, epoch_train_loss=4.0222493299885524
Epoch 1357
Epoch 1357 :: Batch 0/1
Batch Loss = 4.0220438320640985
1357, epoch_train_loss=4.0220438320640985
Epoch 1358
Epoch 1358 :: Batch 0/1
Batch Loss = 4.021837063735719
1358, epoch_train_loss=4.021837063735719
Epoch 1359
Epoch 1359 :: Batch 0/1
Batch Loss = 4.021630905099457
1359, epoch_train_loss=4.021630905099457
Epoch 1360
Epoch 1360 :: Batch 0/1
Batch Loss = 4.0214253576918555
1360, epoch_train_loss=4.0214253576918555
Epoch 1361
Epoch 1361 :: Batch 0/1
Batch Loss = 4.021217192639874
1361, epoch_train_loss=4.021217192639874
Epoch 1362
Epoch 1362 :: Batch 0/1
Batch Loss = 4.0210092330871845
1362, epoch_train_loss=4.0210092330871845
Epoch 1363
Epoch 1363 :: Batch 0/1
Batch Loss = 4.020798745099064
1363, epoch_train_loss=4.020798745099064
Epoch 1364
Epoch 1364 :: Batch 0/1
Batch Loss = 4.020588874212495
1364, epoch_train_loss=4.020588874212495
Epoch 1365
Epoch 1365 :: Batch 0/1
Batch Loss = 4.02037721672949
1365, epoch_train_loss=4.02037721672949
Epoch 1366
Epoch 1366 :: Batch 0/1
Batch Loss = 4.020166254212833
1366, epoch_train_loss=4.020166254212833
Epoch 1367
Epoch 1367 :: Batch 0/1
Batch Loss = 4.019953005625183
1367, epoch_train_loss=4.019953005625183
Epoch 1368
Epoch 1368 :: Batch 0/1
Batch Loss = 4.019739893893436
1368, epoch_train_loss=4.019739893893436
Epoch 1369
Epoch 1369 :: Batch 0/1
Batch Loss = 4.019524504692323
1369, epoch_train_loss=4.019524504692323
Epoch 1370
Epoch 1370 :: Batch 0/1
Batch Loss = 4.019309142300657
1370, epoch_train_loss=4.019309142300657
Epoch 1371
Epoch 1371 :: Batch 0/1
Batch Loss = 4.019092561811885
1371, epoch_train_loss=4.019092561811885
Epoch 1372
Epoch 1372 :: Batch 0/1
Batch Loss = 4.018875005062062
1372, epoch_train_loss=4.018875005062062
Epoch 1373
Epoch 1373 :: Batch 0/1
Batch Loss = 4.018656859089404
1373, epoch_train_loss=4.018656859089404
Epoch 1374
Epoch 1374 :: Batch 0/1
Batch Loss = 4.018436543076511
1374, epoch_train_loss=4.018436543076511
Epoch 1375
Epoch 1375 :: Batch 0/1
Batch Loss = 4.018215350812549
1375, epoch_train_loss=4.018215350812549
Epoch 1376
Epoch 1376 :: Batch 0/1
Batch Loss = 4.017992039129742
1376, epoch_train_loss=4.017992039129742
Epoch 1377
Epoch 1377 :: Batch 0/1
Batch Loss = 4.017767142319255
1377, epoch_train_loss=4.017767142319255
Epoch 1378
Epoch 1378 :: Batch 0/1
Batch Loss = 4.0175399258820494
1378, epoch_train_loss=4.0175399258820494
Epoch 1379
Epoch 1379 :: Batch 0/1
Batch Loss = 4.017310357243495
1379, epoch_train_loss=4.017310357243495
Epoch 1380
Epoch 1380 :: Batch 0/1
Batch Loss = 4.017077513002718
1380, epoch_train_loss=4.017077513002718
Epoch 1381
Epoch 1381 :: Batch 0/1
Batch Loss = 4.016841950730814
1381, epoch_train_loss=4.016841950730814
Epoch 1382
Epoch 1382 :: Batch 0/1
Batch Loss = 4.016603575762131
1382, epoch_train_loss=4.016603575762131
Epoch 1383
Epoch 1383 :: Batch 0/1
Batch Loss = 4.0163634513156765
1383, epoch_train_loss=4.0163634513156765
Epoch 1384
Epoch 1384 :: Batch 0/1
Batch Loss = 4.016123012447916
1384, epoch_train_loss=4.016123012447916
Epoch 1385
Epoch 1385 :: Batch 0/1
Batch Loss = 4.015882902983131
1385, epoch_train_loss=4.015882902983131
Epoch 1386
Epoch 1386 :: Batch 0/1
Batch Loss = 4.015644421896143
1386, epoch_train_loss=4.015644421896143
Epoch 1387
Epoch 1387 :: Batch 0/1
Batch Loss = 4.01540729384065
1387, epoch_train_loss=4.01540729384065
Epoch 1388
Epoch 1388 :: Batch 0/1
Batch Loss = 4.015171571217045
1388, epoch_train_loss=4.015171571217045
Epoch 1389
Epoch 1389 :: Batch 0/1
Batch Loss = 4.0149366312672266
1389, epoch_train_loss=4.0149366312672266
Epoch 1390
Epoch 1390 :: Batch 0/1
Batch Loss = 4.01470206163907
1390, epoch_train_loss=4.01470206163907
Epoch 1391
Epoch 1391 :: Batch 0/1
Batch Loss = 4.0144676542333535
1391, epoch_train_loss=4.0144676542333535
Epoch 1392
Epoch 1392 :: Batch 0/1
Batch Loss = 4.014233214693695
1392, epoch_train_loss=4.014233214693695
Epoch 1393
Epoch 1393 :: Batch 0/1
Batch Loss = 4.013999115628041
1393, epoch_train_loss=4.013999115628041
Epoch 1394
Epoch 1394 :: Batch 0/1
Batch Loss = 4.0137654482833725
1394, epoch_train_loss=4.0137654482833725
Epoch 1395
Epoch 1395 :: Batch 0/1
Batch Loss = 4.013532554470729
1395, epoch_train_loss=4.013532554470729
Epoch 1396
Epoch 1396 :: Batch 0/1
Batch Loss = 4.013300331927302
1396, epoch_train_loss=4.013300331927302
Epoch 1397
Epoch 1397 :: Batch 0/1
Batch Loss = 4.013068461897343
1397, epoch_train_loss=4.013068461897343
Epoch 1398
Epoch 1398 :: Batch 0/1
Batch Loss = 4.0128364910360235
1398, epoch_train_loss=4.0128364910360235
Epoch 1399
Epoch 1399 :: Batch 0/1
Batch Loss = 4.01260380337322
1399, epoch_train_loss=4.01260380337322
Epoch 1400
Epoch 1400 :: Batch 0/1
Batch Loss = 4.012370058290847
1400, epoch_train_loss=4.012370058290847
Epoch 1401
Epoch 1401 :: Batch 0/1
Batch Loss = 4.012134914602701
1401, epoch_train_loss=4.012134914602701
Epoch 1402
Epoch 1402 :: Batch 0/1
Batch Loss = 4.011898303327571
1402, epoch_train_loss=4.011898303327571
Epoch 1403
Epoch 1403 :: Batch 0/1
Batch Loss = 4.011660121134817
1403, epoch_train_loss=4.011660121134817
Epoch 1404
Epoch 1404 :: Batch 0/1
Batch Loss = 4.011420205049392
1404, epoch_train_loss=4.011420205049392
Epoch 1405
Epoch 1405 :: Batch 0/1
Batch Loss = 4.01117842940122
1405, epoch_train_loss=4.01117842940122
Epoch 1406
Epoch 1406 :: Batch 0/1
Batch Loss = 4.010934560680169
1406, epoch_train_loss=4.010934560680169
Epoch 1407
Epoch 1407 :: Batch 0/1
Batch Loss = 4.010688647769804
1407, epoch_train_loss=4.010688647769804
Epoch 1408
Epoch 1408 :: Batch 0/1
Batch Loss = 4.010441011136263
1408, epoch_train_loss=4.010441011136263
Epoch 1409
Epoch 1409 :: Batch 0/1
Batch Loss = 4.010192414128817
1409, epoch_train_loss=4.010192414128817
Epoch 1410
Epoch 1410 :: Batch 0/1
Batch Loss = 4.009944066546098
1410, epoch_train_loss=4.009944066546098
Epoch 1411
Epoch 1411 :: Batch 0/1
Batch Loss = 4.009697239389881
1411, epoch_train_loss=4.009697239389881
Epoch 1412
Epoch 1412 :: Batch 0/1
Batch Loss = 4.009453094367337
1412, epoch_train_loss=4.009453094367337
Epoch 1413
Epoch 1413 :: Batch 0/1
Batch Loss = 4.00921231989684
1413, epoch_train_loss=4.00921231989684
Epoch 1414
Epoch 1414 :: Batch 0/1
Batch Loss = 4.008975068118794
1414, epoch_train_loss=4.008975068118794
Epoch 1415
Epoch 1415 :: Batch 0/1
Batch Loss = 4.008741059104039
1415, epoch_train_loss=4.008741059104039
Epoch 1416
Epoch 1416 :: Batch 0/1
Batch Loss = 4.008509679344916
1416, epoch_train_loss=4.008509679344916
Epoch 1417
Epoch 1417 :: Batch 0/1
Batch Loss = 4.008280278607728
1417, epoch_train_loss=4.008280278607728
Epoch 1418
Epoch 1418 :: Batch 0/1
Batch Loss = 4.008052244598758
1418, epoch_train_loss=4.008052244598758
Epoch 1419
Epoch 1419 :: Batch 0/1
Batch Loss = 4.00782504417457
1419, epoch_train_loss=4.00782504417457
Epoch 1420
Epoch 1420 :: Batch 0/1
Batch Loss = 4.007598263803916
1420, epoch_train_loss=4.007598263803916
Epoch 1421
Epoch 1421 :: Batch 0/1
Batch Loss = 4.007371562766569
1421, epoch_train_loss=4.007371562766569
Epoch 1422
Epoch 1422 :: Batch 0/1
Batch Loss = 4.007144719411015
1422, epoch_train_loss=4.007144719411015
Epoch 1423
Epoch 1423 :: Batch 0/1
Batch Loss = 4.0069175814427425
1423, epoch_train_loss=4.0069175814427425
Epoch 1424
Epoch 1424 :: Batch 0/1
Batch Loss = 4.006690050834761
1424, epoch_train_loss=4.006690050834761
Epoch 1425
Epoch 1425 :: Batch 0/1
Batch Loss = 4.006462115693089
1425, epoch_train_loss=4.006462115693089
Epoch 1426
Epoch 1426 :: Batch 0/1
Batch Loss = 4.006233816258762
1426, epoch_train_loss=4.006233816258762
Epoch 1427
Epoch 1427 :: Batch 0/1
Batch Loss = 4.006005260802808
1427, epoch_train_loss=4.006005260802808
Epoch 1428
Epoch 1428 :: Batch 0/1
Batch Loss = 4.005776583155506
1428, epoch_train_loss=4.005776583155506
Epoch 1429
Epoch 1429 :: Batch 0/1
Batch Loss = 4.005547882262217
1429, epoch_train_loss=4.005547882262217
Epoch 1430
Epoch 1430 :: Batch 0/1
Batch Loss = 4.005319203464518
1430, epoch_train_loss=4.005319203464518
Epoch 1431
Epoch 1431 :: Batch 0/1
Batch Loss = 4.0050905442068325
1431, epoch_train_loss=4.0050905442068325
Epoch 1432
Epoch 1432 :: Batch 0/1
Batch Loss = 4.004861968310825
1432, epoch_train_loss=4.004861968310825
Epoch 1433
Epoch 1433 :: Batch 0/1
Batch Loss = 4.004633756470255
1433, epoch_train_loss=4.004633756470255
Epoch 1434
Epoch 1434 :: Batch 0/1
Batch Loss = 4.004406477248176
1434, epoch_train_loss=4.004406477248176
Epoch 1435
Epoch 1435 :: Batch 0/1
Batch Loss = 4.004181013258883
1435, epoch_train_loss=4.004181013258883
Epoch 1436
Epoch 1436 :: Batch 0/1
Batch Loss = 4.0039584436711335
1436, epoch_train_loss=4.0039584436711335
Epoch 1437
Epoch 1437 :: Batch 0/1
Batch Loss = 4.003739238018616
1437, epoch_train_loss=4.003739238018616
Epoch 1438
Epoch 1438 :: Batch 0/1
Batch Loss = 4.003522726960677
1438, epoch_train_loss=4.003522726960677
Epoch 1439
Epoch 1439 :: Batch 0/1
Batch Loss = 4.0033078885204585
1439, epoch_train_loss=4.0033078885204585
Epoch 1440
Epoch 1440 :: Batch 0/1
Batch Loss = 4.00309405960497
1440, epoch_train_loss=4.00309405960497
Epoch 1441
Epoch 1441 :: Batch 0/1
Batch Loss = 4.002880945978225
1441, epoch_train_loss=4.002880945978225
Epoch 1442
Epoch 1442 :: Batch 0/1
Batch Loss = 4.002668414704442
1442, epoch_train_loss=4.002668414704442
Epoch 1443
Epoch 1443 :: Batch 0/1
Batch Loss = 4.0024563863123355
1443, epoch_train_loss=4.0024563863123355
Epoch 1444
Epoch 1444 :: Batch 0/1
Batch Loss = 4.002244800679322
1444, epoch_train_loss=4.002244800679322
Epoch 1445
Epoch 1445 :: Batch 0/1
Batch Loss = 4.002033604287488
1445, epoch_train_loss=4.002033604287488
Epoch 1446
Epoch 1446 :: Batch 0/1
Batch Loss = 4.001822749595641
1446, epoch_train_loss=4.001822749595641
Epoch 1447
Epoch 1447 :: Batch 0/1
Batch Loss = 4.00161219342994
1447, epoch_train_loss=4.00161219342994
Epoch 1448
Epoch 1448 :: Batch 0/1
Batch Loss = 4.0014018957851345
1448, epoch_train_loss=4.0014018957851345
Epoch 1449
Epoch 1449 :: Batch 0/1
Batch Loss = 4.001191820279181
1449, epoch_train_loss=4.001191820279181
Epoch 1450
Epoch 1450 :: Batch 0/1
Batch Loss = 4.000981937746628
1450, epoch_train_loss=4.000981937746628
Epoch 1451
Epoch 1451 :: Batch 0/1
Batch Loss = 4.000772226976528
1451, epoch_train_loss=4.000772226976528
Epoch 1452
Epoch 1452 :: Batch 0/1
Batch Loss = 4.000562675993372
1452, epoch_train_loss=4.000562675993372
Epoch 1453
Epoch 1453 :: Batch 0/1
Batch Loss = 4.000353283379783
1453, epoch_train_loss=4.000353283379783
Epoch 1454
Epoch 1454 :: Batch 0/1
Batch Loss = 4.000144056075318
1454, epoch_train_loss=4.000144056075318
Epoch 1455
Epoch 1455 :: Batch 0/1
Batch Loss = 3.99993500791183
1455, epoch_train_loss=3.99993500791183
Epoch 1456
Epoch 1456 :: Batch 0/1
Batch Loss = 3.999726156114123
1456, epoch_train_loss=3.999726156114123
Epoch 1457
Epoch 1457 :: Batch 0/1
Batch Loss = 3.99951751556654
1457, epoch_train_loss=3.99951751556654
Epoch 1458
Epoch 1458 :: Batch 0/1
Batch Loss = 3.999309092452641
1458, epoch_train_loss=3.999309092452641
Epoch 1459
Epoch 1459 :: Batch 0/1
Batch Loss = 3.999100879582465
1459, epoch_train_loss=3.999100879582465
Epoch 1460
Epoch 1460 :: Batch 0/1
Batch Loss = 3.9988928528625576
1460, epoch_train_loss=3.9988928528625576
Epoch 1461
Epoch 1461 :: Batch 0/1
Batch Loss = 3.9986849703319196
1461, epoch_train_loss=3.9986849703319196
Epoch 1462
Epoch 1462 :: Batch 0/1
Batch Loss = 3.9984771743872844
1462, epoch_train_loss=3.9984771743872844
Epoch 1463
Epoch 1463 :: Batch 0/1
Batch Loss = 3.998269393577938
1463, epoch_train_loss=3.998269393577938
Epoch 1464
Epoch 1464 :: Batch 0/1
Batch Loss = 3.998061545452534
1464, epoch_train_loss=3.998061545452534
Epoch 1465
Epoch 1465 :: Batch 0/1
Batch Loss = 3.997853538987666
1465, epoch_train_loss=3.997853538987666
Epoch 1466
Epoch 1466 :: Batch 0/1
Batch Loss = 3.9976452750284195
1466, epoch_train_loss=3.9976452750284195
Epoch 1467
Epoch 1467 :: Batch 0/1
Batch Loss = 3.9974366447518457
1467, epoch_train_loss=3.9974366447518457
Epoch 1468
Epoch 1468 :: Batch 0/1
Batch Loss = 3.9972275249272604
1468, epoch_train_loss=3.9972275249272604
Epoch 1469
Epoch 1469 :: Batch 0/1
Batch Loss = 3.9970177691944215
1469, epoch_train_loss=3.9970177691944215
Epoch 1470
Epoch 1470 :: Batch 0/1
Batch Loss = 3.996807194839272
1470, epoch_train_loss=3.996807194839272
Epoch 1471
Epoch 1471 :: Batch 0/1
Batch Loss = 3.99659556588122
1471, epoch_train_loss=3.99659556588122
Epoch 1472
Epoch 1472 :: Batch 0/1
Batch Loss = 3.996382571940569
1472, epoch_train_loss=3.996382571940569
Epoch 1473
Epoch 1473 :: Batch 0/1
Batch Loss = 3.996167804594772
1473, epoch_train_loss=3.996167804594772
Epoch 1474
Epoch 1474 :: Batch 0/1
Batch Loss = 3.9959507328977493
1474, epoch_train_loss=3.9959507328977493
Epoch 1475
Epoch 1475 :: Batch 0/1
Batch Loss = 3.9957306832594823
1475, epoch_train_loss=3.9957306832594823
Epoch 1476
Epoch 1476 :: Batch 0/1
Batch Loss = 3.995506833400185
1476, epoch_train_loss=3.995506833400185
Epoch 1477
Epoch 1477 :: Batch 0/1
Batch Loss = 3.995278238515784
1477, epoch_train_loss=3.995278238515784
Epoch 1478
Epoch 1478 :: Batch 0/1
Batch Loss = 3.9950439164972793
1478, epoch_train_loss=3.9950439164972793
Epoch 1479
Epoch 1479 :: Batch 0/1
Batch Loss = 3.994803016895772
1479, epoch_train_loss=3.994803016895772
Epoch 1480
Epoch 1480 :: Batch 0/1
Batch Loss = 3.9945550671282937
1480, epoch_train_loss=3.9945550671282937
Epoch 1481
Epoch 1481 :: Batch 0/1
Batch Loss = 3.9943002282831914
1481, epoch_train_loss=3.9943002282831914
Epoch 1482
Epoch 1482 :: Batch 0/1
Batch Loss = 3.9940394455459214
1482, epoch_train_loss=3.9940394455459214
Epoch 1483
Epoch 1483 :: Batch 0/1
Batch Loss = 3.9937744142438243
1483, epoch_train_loss=3.9937744142438243
Epoch 1484
Epoch 1484 :: Batch 0/1
Batch Loss = 3.993507345973696
1484, epoch_train_loss=3.993507345973696
Epoch 1485
Epoch 1485 :: Batch 0/1
Batch Loss = 3.993240489552101
1485, epoch_train_loss=3.993240489552101
Epoch 1486
Epoch 1486 :: Batch 0/1
Batch Loss = 3.992975390045517
1486, epoch_train_loss=3.992975390045517
Epoch 1487
Epoch 1487 :: Batch 0/1
Batch Loss = 3.992712221661133
1487, epoch_train_loss=3.992712221661133
Epoch 1488
Epoch 1488 :: Batch 0/1
Batch Loss = 3.9924498146111698
1488, epoch_train_loss=3.9924498146111698
Epoch 1489
Epoch 1489 :: Batch 0/1
Batch Loss = 3.992186545844156
1489, epoch_train_loss=3.992186545844156
Epoch 1490
Epoch 1490 :: Batch 0/1
Batch Loss = 3.99192147666765
1490, epoch_train_loss=3.99192147666765
Epoch 1491
Epoch 1491 :: Batch 0/1
Batch Loss = 3.9916548905311338
1491, epoch_train_loss=3.9916548905311338
Epoch 1492
Epoch 1492 :: Batch 0/1
Batch Loss = 3.9913878197450323
1492, epoch_train_loss=3.9913878197450323
Epoch 1493
Epoch 1493 :: Batch 0/1
Batch Loss = 3.9911209489967727
1493, epoch_train_loss=3.9911209489967727
Epoch 1494
Epoch 1494 :: Batch 0/1
Batch Loss = 3.9908539049889864
1494, epoch_train_loss=3.9908539049889864
Epoch 1495
Epoch 1495 :: Batch 0/1
Batch Loss = 3.9905854828684104
1495, epoch_train_loss=3.9905854828684104
Epoch 1496
Epoch 1496 :: Batch 0/1
Batch Loss = 3.9903142467410024
1496, epoch_train_loss=3.9903142467410024
Epoch 1497
Epoch 1497 :: Batch 0/1
Batch Loss = 3.9900387309487795
1497, epoch_train_loss=3.9900387309487795
Epoch 1498
Epoch 1498 :: Batch 0/1
Batch Loss = 3.989757223187047
1498, epoch_train_loss=3.989757223187047
Epoch 1499
Epoch 1499 :: Batch 0/1
Batch Loss = 3.9894677461511296
1499, epoch_train_loss=3.9894677461511296
Epoch 1500
Epoch 1500 :: Batch 0/1
Batch Loss = 3.9891688323007966
1500, epoch_train_loss=3.9891688323007966
Epoch 1501
Epoch 1501 :: Batch 0/1
Batch Loss = 3.988861038182836
1501, epoch_train_loss=3.988861038182836
Epoch 1502
Epoch 1502 :: Batch 0/1
Batch Loss = 3.988548204345956
1502, epoch_train_loss=3.988548204345956
Epoch 1503
Epoch 1503 :: Batch 0/1
Batch Loss = 3.9882370296419998
1503, epoch_train_loss=3.9882370296419998
Epoch 1504
Epoch 1504 :: Batch 0/1
Batch Loss = 3.9879345867350677
1504, epoch_train_loss=3.9879345867350677
Epoch 1505
Epoch 1505 :: Batch 0/1
Batch Loss = 3.987645244173104
1505, epoch_train_loss=3.987645244173104
Epoch 1506
Epoch 1506 :: Batch 0/1
Batch Loss = 3.9873690101561046
1506, epoch_train_loss=3.9873690101561046
Epoch 1507
Epoch 1507 :: Batch 0/1
Batch Loss = 3.987102335744485
1507, epoch_train_loss=3.987102335744485
Epoch 1508
Epoch 1508 :: Batch 0/1
Batch Loss = 3.986840513945712
1508, epoch_train_loss=3.986840513945712
Epoch 1509
Epoch 1509 :: Batch 0/1
Batch Loss = 3.9865796229524304
1509, epoch_train_loss=3.9865796229524304
Epoch 1510
Epoch 1510 :: Batch 0/1
Batch Loss = 3.9863171749403516
1510, epoch_train_loss=3.9863171749403516
Epoch 1511
Epoch 1511 :: Batch 0/1
Batch Loss = 3.986052069777916
1511, epoch_train_loss=3.986052069777916
Epoch 1512
Epoch 1512 :: Batch 0/1
Batch Loss = 3.9857843600113863
1512, epoch_train_loss=3.9857843600113863
Epoch 1513
Epoch 1513 :: Batch 0/1
Batch Loss = 3.9855148645245473
1513, epoch_train_loss=3.9855148645245473
Epoch 1514
Epoch 1514 :: Batch 0/1
Batch Loss = 3.985244678473294
1514, epoch_train_loss=3.985244678473294
Epoch 1515
Epoch 1515 :: Batch 0/1
Batch Loss = 3.9849747575790375
1515, epoch_train_loss=3.9849747575790375
Epoch 1516
Epoch 1516 :: Batch 0/1
Batch Loss = 3.984705653130712
1516, epoch_train_loss=3.984705653130712
Epoch 1517
Epoch 1517 :: Batch 0/1
Batch Loss = 3.984437350452654
1517, epoch_train_loss=3.984437350452654
Epoch 1518
Epoch 1518 :: Batch 0/1
Batch Loss = 3.9841692141136935
1518, epoch_train_loss=3.9841692141136935
Epoch 1519
Epoch 1519 :: Batch 0/1
Batch Loss = 3.983900114327451
1519, epoch_train_loss=3.983900114327451
Epoch 1520
Epoch 1520 :: Batch 0/1
Batch Loss = 3.9836287399787182
1520, epoch_train_loss=3.9836287399787182
Epoch 1521
Epoch 1521 :: Batch 0/1
Batch Loss = 3.9833539680868872
1521, epoch_train_loss=3.9833539680868872
Epoch 1522
Epoch 1522 :: Batch 0/1
Batch Loss = 3.9830751182381836
1522, epoch_train_loss=3.9830751182381836
Epoch 1523
Epoch 1523 :: Batch 0/1
Batch Loss = 3.9827920319533963
1523, epoch_train_loss=3.9827920319533963
Epoch 1524
Epoch 1524 :: Batch 0/1
Batch Loss = 3.982505041982563
1524, epoch_train_loss=3.982505041982563
Epoch 1525
Epoch 1525 :: Batch 0/1
Batch Loss = 3.9822148741443666
1525, epoch_train_loss=3.9822148741443666
Epoch 1526
Epoch 1526 :: Batch 0/1
Batch Loss = 3.9819224610787765
1526, epoch_train_loss=3.9819224610787765
Epoch 1527
Epoch 1527 :: Batch 0/1
Batch Loss = 3.981628732872626
1527, epoch_train_loss=3.981628732872626
Epoch 1528
Epoch 1528 :: Batch 0/1
Batch Loss = 3.9813345241867064
1528, epoch_train_loss=3.9813345241867064
Epoch 1529
Epoch 1529 :: Batch 0/1
Batch Loss = 3.9810405980615817
1529, epoch_train_loss=3.9810405980615817
Epoch 1530
Epoch 1530 :: Batch 0/1
Batch Loss = 3.9807476293454758
1530, epoch_train_loss=3.9807476293454758
Epoch 1531
Epoch 1531 :: Batch 0/1
Batch Loss = 3.9804560824445985
1531, epoch_train_loss=3.9804560824445985
Epoch 1532
Epoch 1532 :: Batch 0/1
Batch Loss = 3.98016612688982
1532, epoch_train_loss=3.98016612688982
Epoch 1533
Epoch 1533 :: Batch 0/1
Batch Loss = 3.979877692884215
1533, epoch_train_loss=3.979877692884215
Epoch 1534
Epoch 1534 :: Batch 0/1
Batch Loss = 3.979590543279946
1534, epoch_train_loss=3.979590543279946
Epoch 1535
Epoch 1535 :: Batch 0/1
Batch Loss = 3.9793042398101806
1535, epoch_train_loss=3.9793042398101806
Epoch 1536
Epoch 1536 :: Batch 0/1
Batch Loss = 3.979018107994659
1536, epoch_train_loss=3.979018107994659
Epoch 1537
Epoch 1537 :: Batch 0/1
Batch Loss = 3.978731362667059
1537, epoch_train_loss=3.978731362667059
Epoch 1538
Epoch 1538 :: Batch 0/1
Batch Loss = 3.9784433588597623
1538, epoch_train_loss=3.9784433588597623
Epoch 1539
Epoch 1539 :: Batch 0/1
Batch Loss = 3.9781537812968235
1539, epoch_train_loss=3.9781537812968235
Epoch 1540
Epoch 1540 :: Batch 0/1
Batch Loss = 3.9778626333809757
1540, epoch_train_loss=3.9778626333809757
Epoch 1541
Epoch 1541 :: Batch 0/1
Batch Loss = 3.977570044380856
1541, epoch_train_loss=3.977570044380856
Epoch 1542
Epoch 1542 :: Batch 0/1
Batch Loss = 3.9772760526545414
1542, epoch_train_loss=3.9772760526545414
Epoch 1543
Epoch 1543 :: Batch 0/1
Batch Loss = 3.976980547150551
1543, epoch_train_loss=3.976980547150551
Epoch 1544
Epoch 1544 :: Batch 0/1
Batch Loss = 3.976683428894457
1544, epoch_train_loss=3.976683428894457
Epoch 1545
Epoch 1545 :: Batch 0/1
Batch Loss = 3.9763848959455297
1545, epoch_train_loss=3.9763848959455297
Epoch 1546
Epoch 1546 :: Batch 0/1
Batch Loss = 3.9760855509184494
1546, epoch_train_loss=3.9760855509184494
Epoch 1547
Epoch 1547 :: Batch 0/1
Batch Loss = 3.9757860100001863
1547, epoch_train_loss=3.9757860100001863
Epoch 1548
Epoch 1548 :: Batch 0/1
Batch Loss = 3.9754862998082943
1548, epoch_train_loss=3.9754862998082943
Epoch 1549
Epoch 1549 :: Batch 0/1
Batch Loss = 3.9751854062193153
1549, epoch_train_loss=3.9751854062193153
Epoch 1550
Epoch 1550 :: Batch 0/1
Batch Loss = 3.97488085499937
1550, epoch_train_loss=3.97488085499937
Epoch 1551
Epoch 1551 :: Batch 0/1
Batch Loss = 3.9745690361742994
1551, epoch_train_loss=3.9745690361742994
Epoch 1552
Epoch 1552 :: Batch 0/1
Batch Loss = 3.9742507699657392
1552, epoch_train_loss=3.9742507699657392
Epoch 1553
Epoch 1553 :: Batch 0/1
Batch Loss = 3.9739393185577327
1553, epoch_train_loss=3.9739393185577327
Epoch 1554
Epoch 1554 :: Batch 0/1
Batch Loss = 3.9736415079104637
1554, epoch_train_loss=3.9736415079104637
Epoch 1555
Epoch 1555 :: Batch 0/1
Batch Loss = 3.973347601968172
1555, epoch_train_loss=3.973347601968172
Epoch 1556
Epoch 1556 :: Batch 0/1
Batch Loss = 3.9730402833849605
1556, epoch_train_loss=3.9730402833849605
Epoch 1557
Epoch 1557 :: Batch 0/1
Batch Loss = 3.972767114513198
1557, epoch_train_loss=3.972767114513198
Epoch 1558
Epoch 1558 :: Batch 0/1
Batch Loss = 3.9725514630316794
1558, epoch_train_loss=3.9725514630316794
Epoch 1559
Epoch 1559 :: Batch 0/1
Batch Loss = 3.9726051805293148
1559, epoch_train_loss=3.9726051805293148
Epoch 1560
Epoch 1560 :: Batch 0/1
Batch Loss = 3.9727945853304223
1560, epoch_train_loss=3.9727945853304223
Epoch 1561
Epoch 1561 :: Batch 0/1
Batch Loss = 3.9748668245999546
1561, epoch_train_loss=3.9748668245999546
Epoch 1562
Epoch 1562 :: Batch 0/1
Batch Loss = 3.973031286314816
1562, epoch_train_loss=3.973031286314816
Epoch 1563
Epoch 1563 :: Batch 0/1
Batch Loss = 3.9735924385046926
1563, epoch_train_loss=3.9735924385046926
Epoch 1564
Epoch 1564 :: Batch 0/1
Batch Loss = 3.972015181611057
1564, epoch_train_loss=3.972015181611057
Epoch 1565
Epoch 1565 :: Batch 0/1
Batch Loss = 3.971824352337279
1565, epoch_train_loss=3.971824352337279
Epoch 1566
Epoch 1566 :: Batch 0/1
Batch Loss = 3.97108280527928
1566, epoch_train_loss=3.97108280527928
Epoch 1567
Epoch 1567 :: Batch 0/1
Batch Loss = 3.970762663647622
1567, epoch_train_loss=3.970762663647622
Epoch 1568
Epoch 1568 :: Batch 0/1
Batch Loss = 3.970388063511566
1568, epoch_train_loss=3.970388063511566
Epoch 1569
Epoch 1569 :: Batch 0/1
Batch Loss = 3.9700537271061296
1569, epoch_train_loss=3.9700537271061296
Epoch 1570
Epoch 1570 :: Batch 0/1
Batch Loss = 3.970067939821559
1570, epoch_train_loss=3.970067939821559
Epoch 1571
Epoch 1571 :: Batch 0/1
Batch Loss = 3.9704604304209123
1571, epoch_train_loss=3.9704604304209123
Epoch 1572
Epoch 1572 :: Batch 0/1
Batch Loss = 3.9708486179916815
1572, epoch_train_loss=3.9708486179916815
Epoch 1573
Epoch 1573 :: Batch 0/1
Batch Loss = 3.9732691560470754
1573, epoch_train_loss=3.9732691560470754
Epoch 1574
Epoch 1574 :: Batch 0/1
Batch Loss = 3.971761006403472
1574, epoch_train_loss=3.971761006403472
Epoch 1575
Epoch 1575 :: Batch 0/1
Batch Loss = 3.9717221158695857
1575, epoch_train_loss=3.9717221158695857
Epoch 1576
Epoch 1576 :: Batch 0/1
Batch Loss = 3.9757342233444595
1576, epoch_train_loss=3.9757342233444595
Epoch 1577
Epoch 1577 :: Batch 0/1
Batch Loss = 3.9689737547687183
1577, epoch_train_loss=3.9689737547687183
Epoch 1578
Epoch 1578 :: Batch 0/1
Batch Loss = 3.969511150098579
1578, epoch_train_loss=3.969511150098579
Epoch 1579
Epoch 1579 :: Batch 0/1
Batch Loss = 3.96763159981172
1579, epoch_train_loss=3.96763159981172
Epoch 1580
Epoch 1580 :: Batch 0/1
Batch Loss = 3.9742796606097777
1580, epoch_train_loss=3.9742796606097777
Epoch 1581
Epoch 1581 :: Batch 0/1
Batch Loss = 3.989918079708191
1581, epoch_train_loss=3.989918079708191
Epoch 1582
Epoch 1582 :: Batch 0/1
Batch Loss = 3.9922643649106604
1582, epoch_train_loss=3.9922643649106604
Epoch 1583
Epoch 1583 :: Batch 0/1
Batch Loss = 3.98705985572247
1583, epoch_train_loss=3.98705985572247
Epoch 1584
Epoch 1584 :: Batch 0/1
Batch Loss = 3.9764700710869354
1584, epoch_train_loss=3.9764700710869354
Epoch 1585
Epoch 1585 :: Batch 0/1
Batch Loss = 3.981393097267974
1585, epoch_train_loss=3.981393097267974
Epoch 1586
Epoch 1586 :: Batch 0/1
Batch Loss = 3.9759806461819784
1586, epoch_train_loss=3.9759806461819784
Epoch 1587
Epoch 1587 :: Batch 0/1
Batch Loss = 3.977905531728394
1587, epoch_train_loss=3.977905531728394
Epoch 1588
Epoch 1588 :: Batch 0/1
Batch Loss = 3.9884927006165363
1588, epoch_train_loss=3.9884927006165363
Epoch 1589
Epoch 1589 :: Batch 0/1
Batch Loss = 3.9938689878984484
1589, epoch_train_loss=3.9938689878984484
Epoch 1590
Epoch 1590 :: Batch 0/1
Batch Loss = 3.9707538175952326
1590, epoch_train_loss=3.9707538175952326
Epoch 1591
Epoch 1591 :: Batch 0/1
Batch Loss = 4.024419928512787
1591, epoch_train_loss=4.024419928512787
Epoch 1592
Epoch 1592 :: Batch 0/1
Batch Loss = 4.033124464298081
1592, epoch_train_loss=4.033124464298081
Epoch 1593
Epoch 1593 :: Batch 0/1
Batch Loss = 4.059052805055043
1593, epoch_train_loss=4.059052805055043
Epoch 1594
Epoch 1594 :: Batch 0/1
Batch Loss = 4.065158505460129
1594, epoch_train_loss=4.065158505460129
Epoch 1595
Epoch 1595 :: Batch 0/1
Batch Loss = 4.070277305993264
1595, epoch_train_loss=4.070277305993264
Epoch 1596
Epoch 1596 :: Batch 0/1
Batch Loss = 4.074897153334139
1596, epoch_train_loss=4.074897153334139
Epoch 1597
Epoch 1597 :: Batch 0/1
Batch Loss = 4.07128077225068
1597, epoch_train_loss=4.07128077225068
Epoch 1598
Epoch 1598 :: Batch 0/1
Batch Loss = 4.069999996378359
1598, epoch_train_loss=4.069999996378359
Epoch 1599
Epoch 1599 :: Batch 0/1
Batch Loss = 4.0678775311474125
1599, epoch_train_loss=4.0678775311474125
Epoch 1600
Epoch 1600 :: Batch 0/1
Batch Loss = 4.064612014908486
1600, epoch_train_loss=4.064612014908486
Epoch 1601
Epoch 1601 :: Batch 0/1
Batch Loss = 4.06788816820559
1601, epoch_train_loss=4.06788816820559
Epoch 1602
Epoch 1602 :: Batch 0/1
Batch Loss = 4.065126251085152
1602, epoch_train_loss=4.065126251085152
Epoch 1603
Epoch 1603 :: Batch 0/1
Batch Loss = 4.060565803330497
1603, epoch_train_loss=4.060565803330497
Epoch 1604
Epoch 1604 :: Batch 0/1
Batch Loss = 4.061150583622331
1604, epoch_train_loss=4.061150583622331
Epoch 1605
Epoch 1605 :: Batch 0/1
Batch Loss = 4.061112925583226
1605, epoch_train_loss=4.061112925583226
Epoch 1606
Epoch 1606 :: Batch 0/1
Batch Loss = 4.060566806781588
1606, epoch_train_loss=4.060566806781588
Epoch 1607
Epoch 1607 :: Batch 0/1
Batch Loss = 4.058842745159605
1607, epoch_train_loss=4.058842745159605
Epoch 1608
Epoch 1608 :: Batch 0/1
Batch Loss = 4.056130178505436
1608, epoch_train_loss=4.056130178505436
Epoch 1609
Epoch 1609 :: Batch 0/1
Batch Loss = 4.053090434056793
1609, epoch_train_loss=4.053090434056793
Epoch 1610
Epoch 1610 :: Batch 0/1
Batch Loss = 4.0529158757297825
1610, epoch_train_loss=4.0529158757297825
Epoch 1611
Epoch 1611 :: Batch 0/1
Batch Loss = 4.053155746171835
1611, epoch_train_loss=4.053155746171835
Epoch 1612
Epoch 1612 :: Batch 0/1
Batch Loss = 4.049822293024751
1612, epoch_train_loss=4.049822293024751
Epoch 1613
Epoch 1613 :: Batch 0/1
Batch Loss = 4.048033747890198
1613, epoch_train_loss=4.048033747890198
Epoch 1614
Epoch 1614 :: Batch 0/1
Batch Loss = 4.047709734925739
1614, epoch_train_loss=4.047709734925739
Epoch 1615
Epoch 1615 :: Batch 0/1
Batch Loss = 4.046934122191656
1615, epoch_train_loss=4.046934122191656
Epoch 1616
Epoch 1616 :: Batch 0/1
Batch Loss = 4.044950140970814
1616, epoch_train_loss=4.044950140970814
Epoch 1617
Epoch 1617 :: Batch 0/1
Batch Loss = 4.043002048331613
1617, epoch_train_loss=4.043002048331613
Epoch 1618
Epoch 1618 :: Batch 0/1
Batch Loss = 4.04182218984208
1618, epoch_train_loss=4.04182218984208
Epoch 1619
Epoch 1619 :: Batch 0/1
Batch Loss = 4.041163743713702
1619, epoch_train_loss=4.041163743713702
Epoch 1620
Epoch 1620 :: Batch 0/1
Batch Loss = 4.039642556518016
1620, epoch_train_loss=4.039642556518016
Epoch 1621
Epoch 1621 :: Batch 0/1
Batch Loss = 4.0380925676557515
1621, epoch_train_loss=4.0380925676557515
Epoch 1622
Epoch 1622 :: Batch 0/1
Batch Loss = 4.037150979669
1622, epoch_train_loss=4.037150979669
Epoch 1623
Epoch 1623 :: Batch 0/1
Batch Loss = 4.036244359316273
1623, epoch_train_loss=4.036244359316273
Epoch 1624
Epoch 1624 :: Batch 0/1
Batch Loss = 4.035146826903436
1624, epoch_train_loss=4.035146826903436
Epoch 1625
Epoch 1625 :: Batch 0/1
Batch Loss = 4.0335777976774585
1625, epoch_train_loss=4.0335777976774585
Epoch 1626
Epoch 1626 :: Batch 0/1
Batch Loss = 4.0326281691645125
1626, epoch_train_loss=4.0326281691645125
Epoch 1627
Epoch 1627 :: Batch 0/1
Batch Loss = 4.040508373359836
1627, epoch_train_loss=4.040508373359836
Epoch 1628
Epoch 1628 :: Batch 0/1
Batch Loss = 4.056344188393374
1628, epoch_train_loss=4.056344188393374
Epoch 1629
Epoch 1629 :: Batch 0/1
Batch Loss = 4.060028125113264
1629, epoch_train_loss=4.060028125113264
Epoch 1630
Epoch 1630 :: Batch 0/1
Batch Loss = 4.061275243177783
1630, epoch_train_loss=4.061275243177783
Epoch 1631
Epoch 1631 :: Batch 0/1
Batch Loss = 4.061482462188759
1631, epoch_train_loss=4.061482462188759
Epoch 1632
Epoch 1632 :: Batch 0/1
Batch Loss = 4.061310912001233
1632, epoch_train_loss=4.061310912001233
Epoch 1633
Epoch 1633 :: Batch 0/1
Batch Loss = 4.06197053692213
1633, epoch_train_loss=4.06197053692213
Epoch 1634
Epoch 1634 :: Batch 0/1
Batch Loss = 4.06224422012446
1634, epoch_train_loss=4.06224422012446
Epoch 1635
Epoch 1635 :: Batch 0/1
Batch Loss = 4.061616223106656
1635, epoch_train_loss=4.061616223106656
Epoch 1636
Epoch 1636 :: Batch 0/1
Batch Loss = 4.05932868502616
1636, epoch_train_loss=4.05932868502616
Epoch 1637
Epoch 1637 :: Batch 0/1
Batch Loss = 4.060477840082213
1637, epoch_train_loss=4.060477840082213
Epoch 1638
Epoch 1638 :: Batch 0/1
Batch Loss = 4.058576475123708
1638, epoch_train_loss=4.058576475123708
Epoch 1639
Epoch 1639 :: Batch 0/1
Batch Loss = 4.057908168320412
1639, epoch_train_loss=4.057908168320412
Epoch 1640
Epoch 1640 :: Batch 0/1
Batch Loss = 4.056658929721132
1640, epoch_train_loss=4.056658929721132
Epoch 1641
Epoch 1641 :: Batch 0/1
Batch Loss = 4.055742949596697
1641, epoch_train_loss=4.055742949596697
Epoch 1642
Epoch 1642 :: Batch 0/1
Batch Loss = 4.054337490337708
1642, epoch_train_loss=4.054337490337708
Epoch 1643
Epoch 1643 :: Batch 0/1
Batch Loss = 4.053444082935643
1643, epoch_train_loss=4.053444082935643
Epoch 1644
Epoch 1644 :: Batch 0/1
Batch Loss = 4.051249725671444
1644, epoch_train_loss=4.051249725671444
Epoch 1645
Epoch 1645 :: Batch 0/1
Batch Loss = 4.050287221714616
1645, epoch_train_loss=4.050287221714616
Epoch 1646
Epoch 1646 :: Batch 0/1
Batch Loss = 4.0471847796498235
1646, epoch_train_loss=4.0471847796498235
Epoch 1647
Epoch 1647 :: Batch 0/1
Batch Loss = 4.044142149910406
1647, epoch_train_loss=4.044142149910406
Epoch 1648
Epoch 1648 :: Batch 0/1
Batch Loss = 4.034356660244181
1648, epoch_train_loss=4.034356660244181
Epoch 1649
Epoch 1649 :: Batch 0/1
Batch Loss = 4.942828259466111
1649, epoch_train_loss=4.942828259466111
Epoch 1650
Epoch 1650 :: Batch 0/1
Batch Loss = 4.069548620030387
1650, epoch_train_loss=4.069548620030387
Epoch 1651
Epoch 1651 :: Batch 0/1
Batch Loss = 4.077871371247147
1651, epoch_train_loss=4.077871371247147
Epoch 1652
Epoch 1652 :: Batch 0/1
Batch Loss = 4.07770135348355
1652, epoch_train_loss=4.07770135348355
Epoch 1653
Epoch 1653 :: Batch 0/1
Batch Loss = 4.077658379124758
1653, epoch_train_loss=4.077658379124758
Epoch 1654
Epoch 1654 :: Batch 0/1
Batch Loss = 4.0778704739889795
1654, epoch_train_loss=4.0778704739889795
Epoch 1655
Epoch 1655 :: Batch 0/1
Batch Loss = 4.078610979746398
1655, epoch_train_loss=4.078610979746398
Epoch 1656
Epoch 1656 :: Batch 0/1
Batch Loss = 4.079022510879784
1656, epoch_train_loss=4.079022510879784
Epoch 1657
Epoch 1657 :: Batch 0/1
Batch Loss = 4.077590350241811
1657, epoch_train_loss=4.077590350241811
Epoch 1658
Epoch 1658 :: Batch 0/1
Batch Loss = 4.074758327100858
1658, epoch_train_loss=4.074758327100858
Epoch 1659
Epoch 1659 :: Batch 0/1
Batch Loss = 4.071938824746846
1659, epoch_train_loss=4.071938824746846
Epoch 1660
Epoch 1660 :: Batch 0/1
Batch Loss = 4.070577914547869
1660, epoch_train_loss=4.070577914547869
Epoch 1661
Epoch 1661 :: Batch 0/1
Batch Loss = 4.071256894970154
1661, epoch_train_loss=4.071256894970154
Epoch 1662
Epoch 1662 :: Batch 0/1
Batch Loss = 4.071140626983014
1662, epoch_train_loss=4.071140626983014
Epoch 1663
Epoch 1663 :: Batch 0/1
Batch Loss = 4.069308892237394
1663, epoch_train_loss=4.069308892237394
Epoch 1664
Epoch 1664 :: Batch 0/1
Batch Loss = 4.0689203255252995
1664, epoch_train_loss=4.0689203255252995
Epoch 1665
Epoch 1665 :: Batch 0/1
Batch Loss = 4.069114622291744
1665, epoch_train_loss=4.069114622291744
Epoch 1666
Epoch 1666 :: Batch 0/1
Batch Loss = 4.068757168593439
1666, epoch_train_loss=4.068757168593439
Epoch 1667
Epoch 1667 :: Batch 0/1
Batch Loss = 4.067912899400542
1667, epoch_train_loss=4.067912899400542
Epoch 1668
Epoch 1668 :: Batch 0/1
Batch Loss = 4.067646276436891
1668, epoch_train_loss=4.067646276436891
Epoch 1669
Epoch 1669 :: Batch 0/1
Batch Loss = 4.0678029336961545
1669, epoch_train_loss=4.0678029336961545
Epoch 1670
Epoch 1670 :: Batch 0/1
Batch Loss = 4.068609123687147
1670, epoch_train_loss=4.068609123687147
Epoch 1671
Epoch 1671 :: Batch 0/1
Batch Loss = 4.06910557038898
1671, epoch_train_loss=4.06910557038898
Epoch 1672
Epoch 1672 :: Batch 0/1
Batch Loss = 4.069306803603964
1672, epoch_train_loss=4.069306803603964
Epoch 1673
Epoch 1673 :: Batch 0/1
Batch Loss = 4.069902320028067
1673, epoch_train_loss=4.069902320028067
Epoch 1674
Epoch 1674 :: Batch 0/1
Batch Loss = 4.070250553707998
1674, epoch_train_loss=4.070250553707998
Epoch 1675
Epoch 1675 :: Batch 0/1
Batch Loss = 4.0699478600919345
1675, epoch_train_loss=4.0699478600919345
Epoch 1676
Epoch 1676 :: Batch 0/1
Batch Loss = 4.069656706277467
1676, epoch_train_loss=4.069656706277467
Epoch 1677
Epoch 1677 :: Batch 0/1
Batch Loss = 4.069594351872423
1677, epoch_train_loss=4.069594351872423
Epoch 1678
Epoch 1678 :: Batch 0/1
Batch Loss = 4.069149379883778
1678, epoch_train_loss=4.069149379883778
Epoch 1679
Epoch 1679 :: Batch 0/1
Batch Loss = 4.068486374388156
1679, epoch_train_loss=4.068486374388156
Epoch 1680
Epoch 1680 :: Batch 0/1
Batch Loss = 4.067947733432507
1680, epoch_train_loss=4.067947733432507
Epoch 1681
Epoch 1681 :: Batch 0/1
Batch Loss = 4.067358706479841
1681, epoch_train_loss=4.067358706479841
Epoch 1682
Epoch 1682 :: Batch 0/1
Batch Loss = 4.066580852785891
1682, epoch_train_loss=4.066580852785891
Epoch 1683
Epoch 1683 :: Batch 0/1
Batch Loss = 4.065710159668781
1683, epoch_train_loss=4.065710159668781
Epoch 1684
Epoch 1684 :: Batch 0/1
Batch Loss = 4.064908546423003
1684, epoch_train_loss=4.064908546423003
Epoch 1685
Epoch 1685 :: Batch 0/1
Batch Loss = 4.064162518085532
1685, epoch_train_loss=4.064162518085532
Epoch 1686
Epoch 1686 :: Batch 0/1
Batch Loss = 4.063326850791029
1686, epoch_train_loss=4.063326850791029
Epoch 1687
Epoch 1687 :: Batch 0/1
Batch Loss = 4.06241640322496
1687, epoch_train_loss=4.06241640322496
Epoch 1688
Epoch 1688 :: Batch 0/1
Batch Loss = 4.061542873222464
1688, epoch_train_loss=4.061542873222464
Epoch 1689
Epoch 1689 :: Batch 0/1
Batch Loss = 4.060701137180293
1689, epoch_train_loss=4.060701137180293
Epoch 1690
Epoch 1690 :: Batch 0/1
Batch Loss = 4.059819136892075
1690, epoch_train_loss=4.059819136892075
Epoch 1691
Epoch 1691 :: Batch 0/1
Batch Loss = 4.058934640841768
1691, epoch_train_loss=4.058934640841768
Epoch 1692
Epoch 1692 :: Batch 0/1
Batch Loss = 4.0582312837579
1692, epoch_train_loss=4.0582312837579
Epoch 1693
Epoch 1693 :: Batch 0/1
Batch Loss = 4.057676978545333
1693, epoch_train_loss=4.057676978545333
Epoch 1694
Epoch 1694 :: Batch 0/1
Batch Loss = 4.056961739081586
1694, epoch_train_loss=4.056961739081586
Epoch 1695
Epoch 1695 :: Batch 0/1
Batch Loss = 4.056404836958662
1695, epoch_train_loss=4.056404836958662
Epoch 1696
Epoch 1696 :: Batch 0/1
Batch Loss = 4.0558187491019
1696, epoch_train_loss=4.0558187491019
Epoch 1697
Epoch 1697 :: Batch 0/1
Batch Loss = 4.055019049617932
1697, epoch_train_loss=4.055019049617932
Epoch 1698
Epoch 1698 :: Batch 0/1
Batch Loss = 4.054409439221114
1698, epoch_train_loss=4.054409439221114
Epoch 1699
Epoch 1699 :: Batch 0/1
Batch Loss = 4.053636221923787
1699, epoch_train_loss=4.053636221923787
Epoch 1700
Epoch 1700 :: Batch 0/1
Batch Loss = 4.052930995885493
1700, epoch_train_loss=4.052930995885493
Epoch 1701
Epoch 1701 :: Batch 0/1
Batch Loss = 4.052240879053764
1701, epoch_train_loss=4.052240879053764
Epoch 1702
Epoch 1702 :: Batch 0/1
Batch Loss = 4.0514705605631685
1702, epoch_train_loss=4.0514705605631685
Epoch 1703
Epoch 1703 :: Batch 0/1
Batch Loss = 4.050863621391025
1703, epoch_train_loss=4.050863621391025
Epoch 1704
Epoch 1704 :: Batch 0/1
Batch Loss = 4.0501381306949
1704, epoch_train_loss=4.0501381306949
Epoch 1705
Epoch 1705 :: Batch 0/1
Batch Loss = 4.049542589270866
1705, epoch_train_loss=4.049542589270866
Epoch 1706
Epoch 1706 :: Batch 0/1
Batch Loss = 4.048846142506133
1706, epoch_train_loss=4.048846142506133
Epoch 1707
Epoch 1707 :: Batch 0/1
Batch Loss = 4.048288123012981
1707, epoch_train_loss=4.048288123012981
Epoch 1708
Epoch 1708 :: Batch 0/1
Batch Loss = 4.0476296897227195
1708, epoch_train_loss=4.0476296897227195
Epoch 1709
Epoch 1709 :: Batch 0/1
Batch Loss = 4.047071683727113
1709, epoch_train_loss=4.047071683727113
Epoch 1710
Epoch 1710 :: Batch 0/1
Batch Loss = 4.046563278171734
1710, epoch_train_loss=4.046563278171734
Epoch 1711
Epoch 1711 :: Batch 0/1
Batch Loss = 4.0460067118087615
1711, epoch_train_loss=4.0460067118087615
Epoch 1712
Epoch 1712 :: Batch 0/1
Batch Loss = 4.0452953932225375
1712, epoch_train_loss=4.0452953932225375
Epoch 1713
Epoch 1713 :: Batch 0/1
Batch Loss = 4.044647859772291
1713, epoch_train_loss=4.044647859772291
Epoch 1714
Epoch 1714 :: Batch 0/1
Batch Loss = 4.044047759122751
1714, epoch_train_loss=4.044047759122751
Epoch 1715
Epoch 1715 :: Batch 0/1
Batch Loss = 4.043455329410117
1715, epoch_train_loss=4.043455329410117
Epoch 1716
Epoch 1716 :: Batch 0/1
Batch Loss = 4.042910386531369
1716, epoch_train_loss=4.042910386531369
Epoch 1717
Epoch 1717 :: Batch 0/1
Batch Loss = 4.042319644502752
1717, epoch_train_loss=4.042319644502752
Epoch 1718
Epoch 1718 :: Batch 0/1
Batch Loss = 4.041751563909982
1718, epoch_train_loss=4.041751563909982
Epoch 1719
Epoch 1719 :: Batch 0/1
Batch Loss = 4.041240446176552
1719, epoch_train_loss=4.041240446176552
Epoch 1720
Epoch 1720 :: Batch 0/1
Batch Loss = 4.040752072290162
1720, epoch_train_loss=4.040752072290162
Epoch 1721
Epoch 1721 :: Batch 0/1
Batch Loss = 4.040396153720907
1721, epoch_train_loss=4.040396153720907
Epoch 1722
Epoch 1722 :: Batch 0/1
Batch Loss = 4.0397829784289145
1722, epoch_train_loss=4.0397829784289145
Epoch 1723
Epoch 1723 :: Batch 0/1
Batch Loss = 4.039181580442549
1723, epoch_train_loss=4.039181580442549
Epoch 1724
Epoch 1724 :: Batch 0/1
Batch Loss = 4.038315422743416
1724, epoch_train_loss=4.038315422743416
Epoch 1725
Epoch 1725 :: Batch 0/1
Batch Loss = 4.037996743305465
1725, epoch_train_loss=4.037996743305465
Epoch 1726
Epoch 1726 :: Batch 0/1
Batch Loss = 4.037486224505161
1726, epoch_train_loss=4.037486224505161
Epoch 1727
Epoch 1727 :: Batch 0/1
Batch Loss = 4.036568306458237
1727, epoch_train_loss=4.036568306458237
Epoch 1728
Epoch 1728 :: Batch 0/1
Batch Loss = 4.036379951978533
1728, epoch_train_loss=4.036379951978533
Epoch 1729
Epoch 1729 :: Batch 0/1
Batch Loss = 4.035458388295229
1729, epoch_train_loss=4.035458388295229
Epoch 1730
Epoch 1730 :: Batch 0/1
Batch Loss = 4.034934580016406
1730, epoch_train_loss=4.034934580016406
Epoch 1731
Epoch 1731 :: Batch 0/1
Batch Loss = 4.0342193920094696
1731, epoch_train_loss=4.0342193920094696
Epoch 1732
Epoch 1732 :: Batch 0/1
Batch Loss = 4.033485511343721
1732, epoch_train_loss=4.033485511343721
Epoch 1733
Epoch 1733 :: Batch 0/1
Batch Loss = 4.032915237350259
1733, epoch_train_loss=4.032915237350259
Epoch 1734
Epoch 1734 :: Batch 0/1
Batch Loss = 4.032041707449188
1734, epoch_train_loss=4.032041707449188
Epoch 1735
Epoch 1735 :: Batch 0/1
Batch Loss = 4.031506722796415
1735, epoch_train_loss=4.031506722796415
Epoch 1736
Epoch 1736 :: Batch 0/1
Batch Loss = 4.030518126073487
1736, epoch_train_loss=4.030518126073487
Epoch 1737
Epoch 1737 :: Batch 0/1
Batch Loss = 4.029849462166509
1737, epoch_train_loss=4.029849462166509
Epoch 1738
Epoch 1738 :: Batch 0/1
Batch Loss = 4.028891409138566
1738, epoch_train_loss=4.028891409138566
Epoch 1739
Epoch 1739 :: Batch 0/1
Batch Loss = 4.028046342105545
1739, epoch_train_loss=4.028046342105545
Epoch 1740
Epoch 1740 :: Batch 0/1
Batch Loss = 4.0270520350893335
1740, epoch_train_loss=4.0270520350893335
Epoch 1741
Epoch 1741 :: Batch 0/1
Batch Loss = 4.02600249532746
1741, epoch_train_loss=4.02600249532746
Epoch 1742
Epoch 1742 :: Batch 0/1
Batch Loss = 4.024897939611654
1742, epoch_train_loss=4.024897939611654
Epoch 1743
Epoch 1743 :: Batch 0/1
Batch Loss = 4.023632407407757
1743, epoch_train_loss=4.023632407407757
Epoch 1744
Epoch 1744 :: Batch 0/1
Batch Loss = 4.022338350713491
1744, epoch_train_loss=4.022338350713491
Epoch 1745
Epoch 1745 :: Batch 0/1
Batch Loss = 4.020824775095763
1745, epoch_train_loss=4.020824775095763
Epoch 1746
Epoch 1746 :: Batch 0/1
Batch Loss = 4.019196167441518
1746, epoch_train_loss=4.019196167441518
Epoch 1747
Epoch 1747 :: Batch 0/1
Batch Loss = 4.017201055399739
1747, epoch_train_loss=4.017201055399739
Epoch 1748
Epoch 1748 :: Batch 0/1
Batch Loss = 4.015023598674905
1748, epoch_train_loss=4.015023598674905
Epoch 1749
Epoch 1749 :: Batch 0/1
Batch Loss = 4.0125051810695185
1749, epoch_train_loss=4.0125051810695185
Epoch 1750
Epoch 1750 :: Batch 0/1
Batch Loss = 4.009976577834519
1750, epoch_train_loss=4.009976577834519
Epoch 1751
Epoch 1751 :: Batch 0/1
Batch Loss = 4.007550666016282
1751, epoch_train_loss=4.007550666016282
Epoch 1752
Epoch 1752 :: Batch 0/1
Batch Loss = 4.00535285362651
1752, epoch_train_loss=4.00535285362651
Epoch 1753
Epoch 1753 :: Batch 0/1
Batch Loss = 4.002738618568055
1753, epoch_train_loss=4.002738618568055
Epoch 1754
Epoch 1754 :: Batch 0/1
Batch Loss = 3.999602612083794
1754, epoch_train_loss=3.999602612083794
Epoch 1755
Epoch 1755 :: Batch 0/1
Batch Loss = 3.9969540279477975
1755, epoch_train_loss=3.9969540279477975
Epoch 1756
Epoch 1756 :: Batch 0/1
Batch Loss = 3.9950797984676334
1756, epoch_train_loss=3.9950797984676334
Epoch 1757
Epoch 1757 :: Batch 0/1
Batch Loss = 3.993421797935861
1757, epoch_train_loss=3.993421797935861
Epoch 1758
Epoch 1758 :: Batch 0/1
Batch Loss = 3.991632757098339
1758, epoch_train_loss=3.991632757098339
Epoch 1759
Epoch 1759 :: Batch 0/1
Batch Loss = 3.990402577283706
1759, epoch_train_loss=3.990402577283706
Epoch 1760
Epoch 1760 :: Batch 0/1
Batch Loss = 3.989732509272422
1760, epoch_train_loss=3.989732509272422
Epoch 1761
Epoch 1761 :: Batch 0/1
Batch Loss = 3.9885618808231964
1761, epoch_train_loss=3.9885618808231964
Epoch 1762
Epoch 1762 :: Batch 0/1
Batch Loss = 3.987682559278811
1762, epoch_train_loss=3.987682559278811
Epoch 1763
Epoch 1763 :: Batch 0/1
Batch Loss = 3.987198852665689
1763, epoch_train_loss=3.987198852665689
Epoch 1764
Epoch 1764 :: Batch 0/1
Batch Loss = 3.986329493526599
1764, epoch_train_loss=3.986329493526599
Epoch 1765
Epoch 1765 :: Batch 0/1
Batch Loss = 3.9855774119955765
1765, epoch_train_loss=3.9855774119955765
Epoch 1766
Epoch 1766 :: Batch 0/1
Batch Loss = 3.9851057499836773
1766, epoch_train_loss=3.9851057499836773
Epoch 1767
Epoch 1767 :: Batch 0/1
Batch Loss = 3.9842550921362574
1767, epoch_train_loss=3.9842550921362574
Epoch 1768
Epoch 1768 :: Batch 0/1
Batch Loss = 3.9835902892069086
1768, epoch_train_loss=3.9835902892069086
Epoch 1769
Epoch 1769 :: Batch 0/1
Batch Loss = 3.9830032792014665
1769, epoch_train_loss=3.9830032792014665
Epoch 1770
Epoch 1770 :: Batch 0/1
Batch Loss = 3.9822076604202743
1770, epoch_train_loss=3.9822076604202743
Epoch 1771
Epoch 1771 :: Batch 0/1
Batch Loss = 3.981556938173121
1771, epoch_train_loss=3.981556938173121
Epoch 1772
Epoch 1772 :: Batch 0/1
Batch Loss = 3.9809172694742636
1772, epoch_train_loss=3.9809172694742636
Epoch 1773
Epoch 1773 :: Batch 0/1
Batch Loss = 3.9801519048859255
1773, epoch_train_loss=3.9801519048859255
Epoch 1774
Epoch 1774 :: Batch 0/1
Batch Loss = 3.9795207616745167
1774, epoch_train_loss=3.9795207616745167
Epoch 1775
Epoch 1775 :: Batch 0/1
Batch Loss = 3.978885648541095
1775, epoch_train_loss=3.978885648541095
Epoch 1776
Epoch 1776 :: Batch 0/1
Batch Loss = 3.9781694664013294
1776, epoch_train_loss=3.9781694664013294
Epoch 1777
Epoch 1777 :: Batch 0/1
Batch Loss = 3.9775397620363884
1777, epoch_train_loss=3.9775397620363884
Epoch 1778
Epoch 1778 :: Batch 0/1
Batch Loss = 3.976880904103394
1778, epoch_train_loss=3.976880904103394
Epoch 1779
Epoch 1779 :: Batch 0/1
Batch Loss = 3.9761731669566984
1779, epoch_train_loss=3.9761731669566984
Epoch 1780
Epoch 1780 :: Batch 0/1
Batch Loss = 3.975545467393046
1780, epoch_train_loss=3.975545467393046
Epoch 1781
Epoch 1781 :: Batch 0/1
Batch Loss = 3.9748378438187157
1781, epoch_train_loss=3.9748378438187157
Epoch 1782
Epoch 1782 :: Batch 0/1
Batch Loss = 3.974148538960982
1782, epoch_train_loss=3.974148538960982
Epoch 1783
Epoch 1783 :: Batch 0/1
Batch Loss = 3.9735190619862477
1783, epoch_train_loss=3.9735190619862477
Epoch 1784
Epoch 1784 :: Batch 0/1
Batch Loss = 3.9728843328490795
1784, epoch_train_loss=3.9728843328490795
Epoch 1785
Epoch 1785 :: Batch 0/1
Batch Loss = 3.9722591211575895
1785, epoch_train_loss=3.9722591211575895
Epoch 1786
Epoch 1786 :: Batch 0/1
Batch Loss = 3.971686840324896
1786, epoch_train_loss=3.971686840324896
Epoch 1787
Epoch 1787 :: Batch 0/1
Batch Loss = 3.971062680266134
1787, epoch_train_loss=3.971062680266134
Epoch 1788
Epoch 1788 :: Batch 0/1
Batch Loss = 3.970478259500287
1788, epoch_train_loss=3.970478259500287
Epoch 1789
Epoch 1789 :: Batch 0/1
Batch Loss = 3.9699253445669362
1789, epoch_train_loss=3.9699253445669362
Epoch 1790
Epoch 1790 :: Batch 0/1
Batch Loss = 3.9693722596015393
1790, epoch_train_loss=3.9693722596015393
Epoch 1791
Epoch 1791 :: Batch 0/1
Batch Loss = 3.9688560201971774
1791, epoch_train_loss=3.9688560201971774
Epoch 1792
Epoch 1792 :: Batch 0/1
Batch Loss = 3.9683654121941054
1792, epoch_train_loss=3.9683654121941054
Epoch 1793
Epoch 1793 :: Batch 0/1
Batch Loss = 3.9678576535197236
1793, epoch_train_loss=3.9678576535197236
Epoch 1794
Epoch 1794 :: Batch 0/1
Batch Loss = 3.9673743490200173
1794, epoch_train_loss=3.9673743490200173
Epoch 1795
Epoch 1795 :: Batch 0/1
Batch Loss = 3.9668778553662447
1795, epoch_train_loss=3.9668778553662447
Epoch 1796
Epoch 1796 :: Batch 0/1
Batch Loss = 3.9663672516957127
1796, epoch_train_loss=3.9663672516957127
Epoch 1797
Epoch 1797 :: Batch 0/1
Batch Loss = 3.9658773016752584
1797, epoch_train_loss=3.9658773016752584
Epoch 1798
Epoch 1798 :: Batch 0/1
Batch Loss = 3.9653874800009015
1798, epoch_train_loss=3.9653874800009015
Epoch 1799
Epoch 1799 :: Batch 0/1
Batch Loss = 3.964905712520933
1799, epoch_train_loss=3.964905712520933
Epoch 1800
Epoch 1800 :: Batch 0/1
Batch Loss = 3.9644357217493074
1800, epoch_train_loss=3.9644357217493074
Epoch 1801
Epoch 1801 :: Batch 0/1
Batch Loss = 3.963958659331732
1801, epoch_train_loss=3.963958659331732
Epoch 1802
Epoch 1802 :: Batch 0/1
Batch Loss = 3.9634782829802884
1802, epoch_train_loss=3.9634782829802884
Epoch 1803
Epoch 1803 :: Batch 0/1
Batch Loss = 3.963004604177521
1803, epoch_train_loss=3.963004604177521
Epoch 1804
Epoch 1804 :: Batch 0/1
Batch Loss = 3.9625249368265223
1804, epoch_train_loss=3.9625249368265223
Epoch 1805
Epoch 1805 :: Batch 0/1
Batch Loss = 3.9620484885515137
1805, epoch_train_loss=3.9620484885515137
Epoch 1806
Epoch 1806 :: Batch 0/1
Batch Loss = 3.961576616139332
1806, epoch_train_loss=3.961576616139332
Epoch 1807
Epoch 1807 :: Batch 0/1
Batch Loss = 3.961097383304526
1807, epoch_train_loss=3.961097383304526
Epoch 1808
Epoch 1808 :: Batch 0/1
Batch Loss = 3.9606182601844147
1808, epoch_train_loss=3.9606182601844147
Epoch 1809
Epoch 1809 :: Batch 0/1
Batch Loss = 3.9601314534631853
1809, epoch_train_loss=3.9601314534631853
Epoch 1810
Epoch 1810 :: Batch 0/1
Batch Loss = 3.9596362210562672
1810, epoch_train_loss=3.9596362210562672
Epoch 1811
Epoch 1811 :: Batch 0/1
Batch Loss = 3.9591429431770586
1811, epoch_train_loss=3.9591429431770586
Epoch 1812
Epoch 1812 :: Batch 0/1
Batch Loss = 3.9586464910839974
1812, epoch_train_loss=3.9586464910839974
Epoch 1813
Epoch 1813 :: Batch 0/1
Batch Loss = 3.9581515840763926
1813, epoch_train_loss=3.9581515840763926
Epoch 1814
Epoch 1814 :: Batch 0/1
Batch Loss = 3.957660656545118
1814, epoch_train_loss=3.957660656545118
Epoch 1815
Epoch 1815 :: Batch 0/1
Batch Loss = 3.957160837564801
1815, epoch_train_loss=3.957160837564801
Epoch 1816
Epoch 1816 :: Batch 0/1
Batch Loss = 3.9566555688007674
1816, epoch_train_loss=3.9566555688007674
Epoch 1817
Epoch 1817 :: Batch 0/1
Batch Loss = 3.9561458495673163
1817, epoch_train_loss=3.9561458495673163
Epoch 1818
Epoch 1818 :: Batch 0/1
Batch Loss = 3.95563352627668
1818, epoch_train_loss=3.95563352627668
Epoch 1819
Epoch 1819 :: Batch 0/1
Batch Loss = 3.955123754120879
1819, epoch_train_loss=3.955123754120879
Epoch 1820
Epoch 1820 :: Batch 0/1
Batch Loss = 3.9546150984637176
1820, epoch_train_loss=3.9546150984637176
Epoch 1821
Epoch 1821 :: Batch 0/1
Batch Loss = 3.954110671155833
1821, epoch_train_loss=3.954110671155833
Epoch 1822
Epoch 1822 :: Batch 0/1
Batch Loss = 3.9536134719123677
1822, epoch_train_loss=3.9536134719123677
Epoch 1823
Epoch 1823 :: Batch 0/1
Batch Loss = 3.9531225305618705
1823, epoch_train_loss=3.9531225305618705
Epoch 1824
Epoch 1824 :: Batch 0/1
Batch Loss = 3.952640111997177
1824, epoch_train_loss=3.952640111997177
Epoch 1825
Epoch 1825 :: Batch 0/1
Batch Loss = 3.95216399729876
1825, epoch_train_loss=3.95216399729876
Epoch 1826
Epoch 1826 :: Batch 0/1
Batch Loss = 3.9516912925420846
1826, epoch_train_loss=3.9516912925420846
Epoch 1827
Epoch 1827 :: Batch 0/1
Batch Loss = 3.9512199692076173
1827, epoch_train_loss=3.9512199692076173
Epoch 1828
Epoch 1828 :: Batch 0/1
Batch Loss = 3.950746272653529
1828, epoch_train_loss=3.950746272653529
Epoch 1829
Epoch 1829 :: Batch 0/1
Batch Loss = 3.950271396552008
1829, epoch_train_loss=3.950271396552008
Epoch 1830
Epoch 1830 :: Batch 0/1
Batch Loss = 3.949796002017868
1830, epoch_train_loss=3.949796002017868
Epoch 1831
Epoch 1831 :: Batch 0/1
Batch Loss = 3.9493207428984776
1831, epoch_train_loss=3.9493207428984776
Epoch 1832
Epoch 1832 :: Batch 0/1
Batch Loss = 3.948847050925807
1832, epoch_train_loss=3.948847050925807
Epoch 1833
Epoch 1833 :: Batch 0/1
Batch Loss = 3.9483811281374734
1833, epoch_train_loss=3.9483811281374734
Epoch 1834
Epoch 1834 :: Batch 0/1
Batch Loss = 3.9479330017319314
1834, epoch_train_loss=3.9479330017319314
Epoch 1835
Epoch 1835 :: Batch 0/1
Batch Loss = 3.947562341684461
1835, epoch_train_loss=3.947562341684461
Epoch 1836
Epoch 1836 :: Batch 0/1
Batch Loss = 3.947306245244115
1836, epoch_train_loss=3.947306245244115
Epoch 1837
Epoch 1837 :: Batch 0/1
Batch Loss = 3.947385207173449
1837, epoch_train_loss=3.947385207173449
Epoch 1838
Epoch 1838 :: Batch 0/1
Batch Loss = 3.946384983610193
1838, epoch_train_loss=3.946384983610193
Epoch 1839
Epoch 1839 :: Batch 0/1
Batch Loss = 3.945494354517406
1839, epoch_train_loss=3.945494354517406
Epoch 1840
Epoch 1840 :: Batch 0/1
Batch Loss = 3.9451991262967914
1840, epoch_train_loss=3.9451991262967914
Epoch 1841
Epoch 1841 :: Batch 0/1
Batch Loss = 3.9447719750127743
1841, epoch_train_loss=3.9447719750127743
Epoch 1842
Epoch 1842 :: Batch 0/1
Batch Loss = 3.9440567815590013
1842, epoch_train_loss=3.9440567815590013
Epoch 1843
Epoch 1843 :: Batch 0/1
Batch Loss = 3.9435029823563923
1843, epoch_train_loss=3.9435029823563923
Epoch 1844
Epoch 1844 :: Batch 0/1
Batch Loss = 3.943162972493261
1844, epoch_train_loss=3.943162972493261
Epoch 1845
Epoch 1845 :: Batch 0/1
Batch Loss = 3.942579498162346
1845, epoch_train_loss=3.942579498162346
Epoch 1846
Epoch 1846 :: Batch 0/1
Batch Loss = 3.9419554607258185
1846, epoch_train_loss=3.9419554607258185
Epoch 1847
Epoch 1847 :: Batch 0/1
Batch Loss = 3.941618153818415
1847, epoch_train_loss=3.941618153818415
Epoch 1848
Epoch 1848 :: Batch 0/1
Batch Loss = 3.9411061361830324
1848, epoch_train_loss=3.9411061361830324
Epoch 1849
Epoch 1849 :: Batch 0/1
Batch Loss = 3.9404305588111135
1849, epoch_train_loss=3.9404305588111135
Epoch 1850
Epoch 1850 :: Batch 0/1
Batch Loss = 3.9400924641867126
1850, epoch_train_loss=3.9400924641867126
Epoch 1851
Epoch 1851 :: Batch 0/1
Batch Loss = 3.9395892337153082
1851, epoch_train_loss=3.9395892337153082
Epoch 1852
Epoch 1852 :: Batch 0/1
Batch Loss = 3.9389280256840338
1852, epoch_train_loss=3.9389280256840338
Epoch 1853
Epoch 1853 :: Batch 0/1
Batch Loss = 3.9386009136862294
1853, epoch_train_loss=3.9386009136862294
Epoch 1854
Epoch 1854 :: Batch 0/1
Batch Loss = 3.9380672686376723
1854, epoch_train_loss=3.9380672686376723
Epoch 1855
Epoch 1855 :: Batch 0/1
Batch Loss = 3.937410486377569
1855, epoch_train_loss=3.937410486377569
Epoch 1856
Epoch 1856 :: Batch 0/1
Batch Loss = 3.937100091041111
1856, epoch_train_loss=3.937100091041111
Epoch 1857
Epoch 1857 :: Batch 0/1
Batch Loss = 3.936600025137618
1857, epoch_train_loss=3.936600025137618
Epoch 1858
Epoch 1858 :: Batch 0/1
Batch Loss = 3.935918925095993
1858, epoch_train_loss=3.935918925095993
Epoch 1859
Epoch 1859 :: Batch 0/1
Batch Loss = 3.9356212976719616
1859, epoch_train_loss=3.9356212976719616
Epoch 1860
Epoch 1860 :: Batch 0/1
Batch Loss = 3.9351676119650794
1860, epoch_train_loss=3.9351676119650794
Epoch 1861
Epoch 1861 :: Batch 0/1
Batch Loss = 3.93445973519825
1861, epoch_train_loss=3.93445973519825
Epoch 1862
Epoch 1862 :: Batch 0/1
Batch Loss = 3.9341758954786865
1862, epoch_train_loss=3.9341758954786865
Epoch 1863
Epoch 1863 :: Batch 0/1
Batch Loss = 3.933783074270747
1863, epoch_train_loss=3.933783074270747
Epoch 1864
Epoch 1864 :: Batch 0/1
Batch Loss = 3.9330307434558835
1864, epoch_train_loss=3.9330307434558835
Epoch 1865
Epoch 1865 :: Batch 0/1
Batch Loss = 3.9327458379288434
1865, epoch_train_loss=3.9327458379288434
Epoch 1866
Epoch 1866 :: Batch 0/1
Batch Loss = 3.9324136533513943
1866, epoch_train_loss=3.9324136533513943
Epoch 1867
Epoch 1867 :: Batch 0/1
Batch Loss = 3.9316072580547092
1867, epoch_train_loss=3.9316072580547092
Epoch 1868
Epoch 1868 :: Batch 0/1
Batch Loss = 3.9312833038879793
1868, epoch_train_loss=3.9312833038879793
Epoch 1869
Epoch 1869 :: Batch 0/1
Batch Loss = 3.931026279922511
1869, epoch_train_loss=3.931026279922511
Epoch 1870
Epoch 1870 :: Batch 0/1
Batch Loss = 3.9301991574747364
1870, epoch_train_loss=3.9301991574747364
Epoch 1871
Epoch 1871 :: Batch 0/1
Batch Loss = 3.9298007686721856
1871, epoch_train_loss=3.9298007686721856
Epoch 1872
Epoch 1872 :: Batch 0/1
Batch Loss = 3.9295669202931434
1872, epoch_train_loss=3.9295669202931434
Epoch 1873
Epoch 1873 :: Batch 0/1
Batch Loss = 3.9287889912772807
1873, epoch_train_loss=3.9287889912772807
Epoch 1874
Epoch 1874 :: Batch 0/1
Batch Loss = 3.9283008138315356
1874, epoch_train_loss=3.9283008138315356
Epoch 1875
Epoch 1875 :: Batch 0/1
Batch Loss = 3.9280412757053647
1875, epoch_train_loss=3.9280412757053647
Epoch 1876
Epoch 1876 :: Batch 0/1
Batch Loss = 3.9273910056468946
1876, epoch_train_loss=3.9273910056468946
Epoch 1877
Epoch 1877 :: Batch 0/1
Batch Loss = 3.9268309649924467
1877, epoch_train_loss=3.9268309649924467
Epoch 1878
Epoch 1878 :: Batch 0/1
Batch Loss = 3.926496410839558
1878, epoch_train_loss=3.926496410839558
Epoch 1879
Epoch 1879 :: Batch 0/1
Batch Loss = 3.925993784262531
1879, epoch_train_loss=3.925993784262531
Epoch 1880
Epoch 1880 :: Batch 0/1
Batch Loss = 3.925414439747082
1880, epoch_train_loss=3.925414439747082
Epoch 1881
Epoch 1881 :: Batch 0/1
Batch Loss = 3.9249587628829286
1881, epoch_train_loss=3.9249587628829286
Epoch 1882
Epoch 1882 :: Batch 0/1
Batch Loss = 3.9245522622895987
1882, epoch_train_loss=3.9245522622895987
Epoch 1883
Epoch 1883 :: Batch 0/1
Batch Loss = 3.9240536213637083
1883, epoch_train_loss=3.9240536213637083
Epoch 1884
Epoch 1884 :: Batch 0/1
Batch Loss = 3.9235088129601734
1884, epoch_train_loss=3.9235088129601734
Epoch 1885
Epoch 1885 :: Batch 0/1
Batch Loss = 3.923060732536787
1885, epoch_train_loss=3.923060732536787
Epoch 1886
Epoch 1886 :: Batch 0/1
Batch Loss = 3.922636546285956
1886, epoch_train_loss=3.922636546285956
Epoch 1887
Epoch 1887 :: Batch 0/1
Batch Loss = 3.9221193168574833
1887, epoch_train_loss=3.9221193168574833
Epoch 1888
Epoch 1888 :: Batch 0/1
Batch Loss = 3.9215887271913488
1888, epoch_train_loss=3.9215887271913488
Epoch 1889
Epoch 1889 :: Batch 0/1
Batch Loss = 3.9211094708827807
1889, epoch_train_loss=3.9211094708827807
Epoch 1890
Epoch 1890 :: Batch 0/1
Batch Loss = 3.9206681676365913
1890, epoch_train_loss=3.9206681676365913
Epoch 1891
Epoch 1891 :: Batch 0/1
Batch Loss = 3.9202245924545163
1891, epoch_train_loss=3.9202245924545163
Epoch 1892
Epoch 1892 :: Batch 0/1
Batch Loss = 3.9197376582522003
1892, epoch_train_loss=3.9197376582522003
Epoch 1893
Epoch 1893 :: Batch 0/1
Batch Loss = 3.9192411857548
1893, epoch_train_loss=3.9192411857548
Epoch 1894
Epoch 1894 :: Batch 0/1
Batch Loss = 3.9187284411136027
1894, epoch_train_loss=3.9187284411136027
Epoch 1895
Epoch 1895 :: Batch 0/1
Batch Loss = 3.9182275717526185
1895, epoch_train_loss=3.9182275717526185
Epoch 1896
Epoch 1896 :: Batch 0/1
Batch Loss = 3.9177320719101614
1896, epoch_train_loss=3.9177320719101614
Epoch 1897
Epoch 1897 :: Batch 0/1
Batch Loss = 3.9172429498227235
1897, epoch_train_loss=3.9172429498227235
Epoch 1898
Epoch 1898 :: Batch 0/1
Batch Loss = 3.9167581131443505
1898, epoch_train_loss=3.9167581131443505
Epoch 1899
Epoch 1899 :: Batch 0/1
Batch Loss = 3.9162812526710225
1899, epoch_train_loss=3.9162812526710225
Epoch 1900
Epoch 1900 :: Batch 0/1
Batch Loss = 3.9158271010687793
1900, epoch_train_loss=3.9158271010687793
Epoch 1901
Epoch 1901 :: Batch 0/1
Batch Loss = 3.915470455869152
1901, epoch_train_loss=3.915470455869152
Epoch 1902
Epoch 1902 :: Batch 0/1
Batch Loss = 3.915380318640598
1902, epoch_train_loss=3.915380318640598
Epoch 1903
Epoch 1903 :: Batch 0/1
Batch Loss = 3.9164128715306568
1903, epoch_train_loss=3.9164128715306568
Epoch 1904
Epoch 1904 :: Batch 0/1
Batch Loss = 3.9164447540520424
1904, epoch_train_loss=3.9164447540520424
Epoch 1905
Epoch 1905 :: Batch 0/1
Batch Loss = 3.9160067190969636
1905, epoch_train_loss=3.9160067190969636
Epoch 1906
Epoch 1906 :: Batch 0/1
Batch Loss = 3.9130774524732894
1906, epoch_train_loss=3.9130774524732894
Epoch 1907
Epoch 1907 :: Batch 0/1
Batch Loss = 3.9154214139001793
1907, epoch_train_loss=3.9154214139001793
Epoch 1908
Epoch 1908 :: Batch 0/1
Batch Loss = 3.9140878853668117
1908, epoch_train_loss=3.9140878853668117
Epoch 1909
Epoch 1909 :: Batch 0/1
Batch Loss = 3.9127044203527857
1909, epoch_train_loss=3.9127044203527857
Epoch 1910
Epoch 1910 :: Batch 0/1
Batch Loss = 3.9135903278288153
1910, epoch_train_loss=3.9135903278288153
Epoch 1911
Epoch 1911 :: Batch 0/1
Batch Loss = 3.910567050840641
1911, epoch_train_loss=3.910567050840641
Epoch 1912
Epoch 1912 :: Batch 0/1
Batch Loss = 3.9116139654105475
1912, epoch_train_loss=3.9116139654105475
Epoch 1913
Epoch 1913 :: Batch 0/1
Batch Loss = 3.909517669694892
1913, epoch_train_loss=3.909517669694892
Epoch 1914
Epoch 1914 :: Batch 0/1
Batch Loss = 3.9106510048550547
1914, epoch_train_loss=3.9106510048550547
Epoch 1915
Epoch 1915 :: Batch 0/1
Batch Loss = 3.908721690906842
1915, epoch_train_loss=3.908721690906842
Epoch 1916
Epoch 1916 :: Batch 0/1
Batch Loss = 3.909088557600172
1916, epoch_train_loss=3.909088557600172
Epoch 1917
Epoch 1917 :: Batch 0/1
Batch Loss = 3.907565062917144
1917, epoch_train_loss=3.907565062917144
Epoch 1918
Epoch 1918 :: Batch 0/1
Batch Loss = 3.907823471714534
1918, epoch_train_loss=3.907823471714534
Epoch 1919
Epoch 1919 :: Batch 0/1
Batch Loss = 3.9071838881779617
1919, epoch_train_loss=3.9071838881779617
Epoch 1920
Epoch 1920 :: Batch 0/1
Batch Loss = 3.906464193348828
1920, epoch_train_loss=3.906464193348828
Epoch 1921
Epoch 1921 :: Batch 0/1
Batch Loss = 3.9062039806802313
1921, epoch_train_loss=3.9062039806802313
Epoch 1922
Epoch 1922 :: Batch 0/1
Batch Loss = 3.905059851304026
1922, epoch_train_loss=3.905059851304026
Epoch 1923
Epoch 1923 :: Batch 0/1
Batch Loss = 3.9050325139620434
1923, epoch_train_loss=3.9050325139620434
Epoch 1924
Epoch 1924 :: Batch 0/1
Batch Loss = 3.9040190499693805
1924, epoch_train_loss=3.9040190499693805
Epoch 1925
Epoch 1925 :: Batch 0/1
Batch Loss = 3.9040168804671636
1925, epoch_train_loss=3.9040168804671636
Epoch 1926
Epoch 1926 :: Batch 0/1
Batch Loss = 3.9028948968168504
1926, epoch_train_loss=3.9028948968168504
Epoch 1927
Epoch 1927 :: Batch 0/1
Batch Loss = 3.902733306118041
1927, epoch_train_loss=3.902733306118041
Epoch 1928
Epoch 1928 :: Batch 0/1
Batch Loss = 3.90183866018485
1928, epoch_train_loss=3.90183866018485
Epoch 1929
Epoch 1929 :: Batch 0/1
Batch Loss = 3.9014718764585927
1929, epoch_train_loss=3.9014718764585927
Epoch 1930
Epoch 1930 :: Batch 0/1
Batch Loss = 3.90089589449359
1930, epoch_train_loss=3.90089589449359
Epoch 1931
Epoch 1931 :: Batch 0/1
Batch Loss = 3.9003000748780545
1931, epoch_train_loss=3.9003000748780545
Epoch 1932
Epoch 1932 :: Batch 0/1
Batch Loss = 3.8999202773482016
1932, epoch_train_loss=3.8999202773482016
Epoch 1933
Epoch 1933 :: Batch 0/1
Batch Loss = 3.899126661258701
1933, epoch_train_loss=3.899126661258701
Epoch 1934
Epoch 1934 :: Batch 0/1
Batch Loss = 3.8988275226680456
1934, epoch_train_loss=3.8988275226680456
Epoch 1935
Epoch 1935 :: Batch 0/1
Batch Loss = 3.8981814598263216
1935, epoch_train_loss=3.8981814598263216
Epoch 1936
Epoch 1936 :: Batch 0/1
Batch Loss = 3.8976310410341313
1936, epoch_train_loss=3.8976310410341313
Epoch 1937
Epoch 1937 :: Batch 0/1
Batch Loss = 3.897221031821863
1937, epoch_train_loss=3.897221031821863
Epoch 1938
Epoch 1938 :: Batch 0/1
Batch Loss = 3.8965664702547014
1938, epoch_train_loss=3.8965664702547014
Epoch 1939
Epoch 1939 :: Batch 0/1
Batch Loss = 3.896153000146754
1939, epoch_train_loss=3.896153000146754
Epoch 1940
Epoch 1940 :: Batch 0/1
Batch Loss = 3.8955921167753873
1940, epoch_train_loss=3.8955921167753873
Epoch 1941
Epoch 1941 :: Batch 0/1
Batch Loss = 3.89499558057992
1941, epoch_train_loss=3.89499558057992
Epoch 1942
Epoch 1942 :: Batch 0/1
Batch Loss = 3.894574102238892
1942, epoch_train_loss=3.894574102238892
Epoch 1943
Epoch 1943 :: Batch 0/1
Batch Loss = 3.89396334692213
1943, epoch_train_loss=3.89396334692213
Epoch 1944
Epoch 1944 :: Batch 0/1
Batch Loss = 3.8933910681343615
1944, epoch_train_loss=3.8933910681343615
Epoch 1945
Epoch 1945 :: Batch 0/1
Batch Loss = 3.892934908606668
1945, epoch_train_loss=3.892934908606668
Epoch 1946
Epoch 1946 :: Batch 0/1
Batch Loss = 3.8923224312579943
1946, epoch_train_loss=3.8923224312579943
Epoch 1947
Epoch 1947 :: Batch 0/1
Batch Loss = 3.8917415264650432
1947, epoch_train_loss=3.8917415264650432
Epoch 1948
Epoch 1948 :: Batch 0/1
Batch Loss = 3.8912716242472833
1948, epoch_train_loss=3.8912716242472833
Epoch 1949
Epoch 1949 :: Batch 0/1
Batch Loss = 3.890684492362128
1949, epoch_train_loss=3.890684492362128
Epoch 1950
Epoch 1950 :: Batch 0/1
Batch Loss = 3.8900749406618473
1950, epoch_train_loss=3.8900749406618473
Epoch 1951
Epoch 1951 :: Batch 0/1
Batch Loss = 3.889597075607054
1951, epoch_train_loss=3.889597075607054
Epoch 1952
Epoch 1952 :: Batch 0/1
Batch Loss = 3.8890632146152027
1952, epoch_train_loss=3.8890632146152027
Epoch 1953
Epoch 1953 :: Batch 0/1
Batch Loss = 3.8884223219916425
1953, epoch_train_loss=3.8884223219916425
Epoch 1954
Epoch 1954 :: Batch 0/1
Batch Loss = 3.887891811599492
1954, epoch_train_loss=3.887891811599492
Epoch 1955
Epoch 1955 :: Batch 0/1
Batch Loss = 3.8873954073643135
1955, epoch_train_loss=3.8873954073643135
Epoch 1956
Epoch 1956 :: Batch 0/1
Batch Loss = 3.886783677539608
1956, epoch_train_loss=3.886783677539608
Epoch 1957
Epoch 1957 :: Batch 0/1
Batch Loss = 3.8861803192422957
1957, epoch_train_loss=3.8861803192422957
Epoch 1958
Epoch 1958 :: Batch 0/1
Batch Loss = 3.8856438812803313
1958, epoch_train_loss=3.8856438812803313
Epoch 1959
Epoch 1959 :: Batch 0/1
Batch Loss = 3.88509711318658
1959, epoch_train_loss=3.88509711318658
Epoch 1960
Epoch 1960 :: Batch 0/1
Batch Loss = 3.884500625412559
1960, epoch_train_loss=3.884500625412559
Epoch 1961
Epoch 1961 :: Batch 0/1
Batch Loss = 3.88387181539911
1961, epoch_train_loss=3.88387181539911
Epoch 1962
Epoch 1962 :: Batch 0/1
Batch Loss = 3.883266676518802
1962, epoch_train_loss=3.883266676518802
Epoch 1963
Epoch 1963 :: Batch 0/1
Batch Loss = 3.8826836637053828
1963, epoch_train_loss=3.8826836637053828
Epoch 1964
Epoch 1964 :: Batch 0/1
Batch Loss = 3.8820846290253166
1964, epoch_train_loss=3.8820846290253166
Epoch 1965
Epoch 1965 :: Batch 0/1
Batch Loss = 3.8814590036190726
1965, epoch_train_loss=3.8814590036190726
Epoch 1966
Epoch 1966 :: Batch 0/1
Batch Loss = 3.8807988075031417
1966, epoch_train_loss=3.8807988075031417
Epoch 1967
Epoch 1967 :: Batch 0/1
Batch Loss = 3.880132926166461
1967, epoch_train_loss=3.880132926166461
Epoch 1968
Epoch 1968 :: Batch 0/1
Batch Loss = 3.879465054871531
1968, epoch_train_loss=3.879465054871531
Epoch 1969
Epoch 1969 :: Batch 0/1
Batch Loss = 3.8788007181144475
1969, epoch_train_loss=3.8788007181144475
Epoch 1970
Epoch 1970 :: Batch 0/1
Batch Loss = 3.8781386690562325
1970, epoch_train_loss=3.8781386690562325
Epoch 1971
Epoch 1971 :: Batch 0/1
Batch Loss = 3.8774737132406636
1971, epoch_train_loss=3.8774737132406636
Epoch 1972
Epoch 1972 :: Batch 0/1
Batch Loss = 3.876811118719944
1972, epoch_train_loss=3.876811118719944
Epoch 1973
Epoch 1973 :: Batch 0/1
Batch Loss = 3.8761447429022424
1973, epoch_train_loss=3.8761447429022424
Epoch 1974
Epoch 1974 :: Batch 0/1
Batch Loss = 3.875502908763856
1974, epoch_train_loss=3.875502908763856
Epoch 1975
Epoch 1975 :: Batch 0/1
Batch Loss = 3.8748704926494897
1975, epoch_train_loss=3.8748704926494897
Epoch 1976
Epoch 1976 :: Batch 0/1
Batch Loss = 3.874351325000583
1976, epoch_train_loss=3.874351325000583
Epoch 1977
Epoch 1977 :: Batch 0/1
Batch Loss = 3.873831017054233
1977, epoch_train_loss=3.873831017054233
Epoch 1978
Epoch 1978 :: Batch 0/1
Batch Loss = 3.8736868276076852
1978, epoch_train_loss=3.8736868276076852
Epoch 1979
Epoch 1979 :: Batch 0/1
Batch Loss = 3.873089936467682
1979, epoch_train_loss=3.873089936467682
Epoch 1980
Epoch 1980 :: Batch 0/1
Batch Loss = 3.873022336937917
1980, epoch_train_loss=3.873022336937917
Epoch 1981
Epoch 1981 :: Batch 0/1
Batch Loss = 3.871490650552366
1981, epoch_train_loss=3.871490650552366
Epoch 1982
Epoch 1982 :: Batch 0/1
Batch Loss = 3.8703489316635364
1982, epoch_train_loss=3.8703489316635364
Epoch 1983
Epoch 1983 :: Batch 0/1
Batch Loss = 3.8695483086028766
1983, epoch_train_loss=3.8695483086028766
Epoch 1984
Epoch 1984 :: Batch 0/1
Batch Loss = 3.8691618065976168
1984, epoch_train_loss=3.8691618065976168
Epoch 1985
Epoch 1985 :: Batch 0/1
Batch Loss = 3.8691208665792503
1985, epoch_train_loss=3.8691208665792503
Epoch 1986
Epoch 1986 :: Batch 0/1
Batch Loss = 3.868373370623998
1986, epoch_train_loss=3.868373370623998
Epoch 1987
Epoch 1987 :: Batch 0/1
Batch Loss = 3.867767348498874
1987, epoch_train_loss=3.867767348498874
Epoch 1988
Epoch 1988 :: Batch 0/1
Batch Loss = 3.8664666129385172
1988, epoch_train_loss=3.8664666129385172
Epoch 1989
Epoch 1989 :: Batch 0/1
Batch Loss = 3.865520588087244
1989, epoch_train_loss=3.865520588087244
Epoch 1990
Epoch 1990 :: Batch 0/1
Batch Loss = 3.864756277600612
1990, epoch_train_loss=3.864756277600612
Epoch 1991
Epoch 1991 :: Batch 0/1
Batch Loss = 3.8642809839303687
1991, epoch_train_loss=3.8642809839303687
Epoch 1992
Epoch 1992 :: Batch 0/1
Batch Loss = 3.863953114760087
1992, epoch_train_loss=3.863953114760087
Epoch 1993
Epoch 1993 :: Batch 0/1
Batch Loss = 3.863210556785977
1993, epoch_train_loss=3.863210556785977
Epoch 1994
Epoch 1994 :: Batch 0/1
Batch Loss = 3.8624636107385264
1994, epoch_train_loss=3.8624636107385264
Epoch 1995
Epoch 1995 :: Batch 0/1
Batch Loss = 3.861348899943931
1995, epoch_train_loss=3.861348899943931
Epoch 1996
Epoch 1996 :: Batch 0/1
Batch Loss = 3.8603730492453407
1996, epoch_train_loss=3.8603730492453407
Epoch 1997
Epoch 1997 :: Batch 0/1
Batch Loss = 3.859758650820241
1997, epoch_train_loss=3.859758650820241
Epoch 1998
Epoch 1998 :: Batch 0/1
Batch Loss = 3.858833174586577
1998, epoch_train_loss=3.858833174586577
Epoch 1999
Epoch 1999 :: Batch 0/1
Batch Loss = 3.857938592181686
1999, epoch_train_loss=3.857938592181686
Epoch 2000
Epoch 2000 :: Batch 0/1
Batch Loss = 3.856864462383823
2000, epoch_train_loss=3.856864462383823
Epoch 2001
Epoch 2001 :: Batch 0/1
Batch Loss = 3.855844013535382
2001, epoch_train_loss=3.855844013535382
Epoch 2002
Epoch 2002 :: Batch 0/1
Batch Loss = 3.8548336722541863
2002, epoch_train_loss=3.8548336722541863
Epoch 2003
Epoch 2003 :: Batch 0/1
Batch Loss = 3.8535664291707317
2003, epoch_train_loss=3.8535664291707317
Epoch 2004
Epoch 2004 :: Batch 0/1
Batch Loss = 3.8523358451383927
2004, epoch_train_loss=3.8523358451383927
Epoch 2005
Epoch 2005 :: Batch 0/1
Batch Loss = 3.8513988843880806
2005, epoch_train_loss=3.8513988843880806
Epoch 2006
Epoch 2006 :: Batch 0/1
Batch Loss = 3.8504598406028143
2006, epoch_train_loss=3.8504598406028143
Epoch 2007
Epoch 2007 :: Batch 0/1
Batch Loss = 3.8496625851325157
2007, epoch_train_loss=3.8496625851325157
Epoch 2008
Epoch 2008 :: Batch 0/1
Batch Loss = 3.84883981403269
2008, epoch_train_loss=3.84883981403269
Epoch 2009
Epoch 2009 :: Batch 0/1
Batch Loss = 3.848004021317187
2009, epoch_train_loss=3.848004021317187
Epoch 2010
Epoch 2010 :: Batch 0/1
Batch Loss = 3.847211458752714
2010, epoch_train_loss=3.847211458752714
Epoch 2011
Epoch 2011 :: Batch 0/1
Batch Loss = 3.8464955429605
2011, epoch_train_loss=3.8464955429605
Epoch 2012
Epoch 2012 :: Batch 0/1
Batch Loss = 3.846141094142
2012, epoch_train_loss=3.846141094142
Epoch 2013
Epoch 2013 :: Batch 0/1
Batch Loss = 3.846528998925306
2013, epoch_train_loss=3.846528998925306
Epoch 2014
Epoch 2014 :: Batch 0/1
Batch Loss = 3.8455653970738157
2014, epoch_train_loss=3.8455653970738157
Epoch 2015
Epoch 2015 :: Batch 0/1
Batch Loss = 3.843903039723498
2015, epoch_train_loss=3.843903039723498
Epoch 2016
Epoch 2016 :: Batch 0/1
Batch Loss = 3.842314044421541
2016, epoch_train_loss=3.842314044421541
Epoch 2017
Epoch 2017 :: Batch 0/1
Batch Loss = 3.8425564389429394
2017, epoch_train_loss=3.8425564389429394
Epoch 2018
Epoch 2018 :: Batch 0/1
Batch Loss = 3.8413971615565727
2018, epoch_train_loss=3.8413971615565727
Epoch 2019
Epoch 2019 :: Batch 0/1
Batch Loss = 3.839764596110808
2019, epoch_train_loss=3.839764596110808
Epoch 2020
Epoch 2020 :: Batch 0/1
Batch Loss = 3.8395194577231995
2020, epoch_train_loss=3.8395194577231995
Epoch 2021
Epoch 2021 :: Batch 0/1
Batch Loss = 3.8388852415753196
2021, epoch_train_loss=3.8388852415753196
Epoch 2022
Epoch 2022 :: Batch 0/1
Batch Loss = 3.837803700190238
2022, epoch_train_loss=3.837803700190238
Epoch 2023
Epoch 2023 :: Batch 0/1
Batch Loss = 3.83640519797456
2023, epoch_train_loss=3.83640519797456
Epoch 2024
Epoch 2024 :: Batch 0/1
Batch Loss = 3.8355287972711563
2024, epoch_train_loss=3.8355287972711563
Epoch 2025
Epoch 2025 :: Batch 0/1
Batch Loss = 3.8347131044213962
2025, epoch_train_loss=3.8347131044213962
Epoch 2026
Epoch 2026 :: Batch 0/1
Batch Loss = 3.8337189014933903
2026, epoch_train_loss=3.8337189014933903
Epoch 2027
Epoch 2027 :: Batch 0/1
Batch Loss = 3.832945551094478
2027, epoch_train_loss=3.832945551094478
Epoch 2028
Epoch 2028 :: Batch 0/1
Batch Loss = 3.831747022882901
2028, epoch_train_loss=3.831747022882901
Epoch 2029
Epoch 2029 :: Batch 0/1
Batch Loss = 3.830870602316235
2029, epoch_train_loss=3.830870602316235
Epoch 2030
Epoch 2030 :: Batch 0/1
Batch Loss = 3.829762107720336
2030, epoch_train_loss=3.829762107720336
Epoch 2031
Epoch 2031 :: Batch 0/1
Batch Loss = 3.8288681216093865
2031, epoch_train_loss=3.8288681216093865
Epoch 2032
Epoch 2032 :: Batch 0/1
Batch Loss = 3.828138716949573
2032, epoch_train_loss=3.828138716949573
Epoch 2033
Epoch 2033 :: Batch 0/1
Batch Loss = 3.8288732026839303
2033, epoch_train_loss=3.8288732026839303
Epoch 2034
Epoch 2034 :: Batch 0/1
Batch Loss = 3.83321229913149
2034, epoch_train_loss=3.83321229913149
Epoch 2035
Epoch 2035 :: Batch 0/1
Batch Loss = 3.830107254170583
2035, epoch_train_loss=3.830107254170583
Epoch 2036
Epoch 2036 :: Batch 0/1
Batch Loss = 3.8261715521858752
2036, epoch_train_loss=3.8261715521858752
Epoch 2037
Epoch 2037 :: Batch 0/1
Batch Loss = 3.8297609981836342
2037, epoch_train_loss=3.8297609981836342
Epoch 2038
Epoch 2038 :: Batch 0/1
Batch Loss = 3.8269879723954316
2038, epoch_train_loss=3.8269879723954316
Epoch 2039
Epoch 2039 :: Batch 0/1
Batch Loss = 3.8294823216963163
2039, epoch_train_loss=3.8294823216963163
Epoch 2040
Epoch 2040 :: Batch 0/1
Batch Loss = 3.8267927130528196
2040, epoch_train_loss=3.8267927130528196
Epoch 2041
Epoch 2041 :: Batch 0/1
Batch Loss = 3.8268023758110705
2041, epoch_train_loss=3.8268023758110705
Epoch 2042
Epoch 2042 :: Batch 0/1
Batch Loss = 3.824282572531667
2042, epoch_train_loss=3.824282572531667
Epoch 2043
Epoch 2043 :: Batch 0/1
Batch Loss = 3.8210935508031354
2043, epoch_train_loss=3.8210935508031354
Epoch 2044
Epoch 2044 :: Batch 0/1
Batch Loss = 3.821566835454696
2044, epoch_train_loss=3.821566835454696
Epoch 2045
Epoch 2045 :: Batch 0/1
Batch Loss = 3.822266229806216
2045, epoch_train_loss=3.822266229806216
Epoch 2046
Epoch 2046 :: Batch 0/1
Batch Loss = 3.8230103408389517
2046, epoch_train_loss=3.8230103408389517
Epoch 2047
Epoch 2047 :: Batch 0/1
Batch Loss = 3.8199868580890386
2047, epoch_train_loss=3.8199868580890386
Epoch 2048
Epoch 2048 :: Batch 0/1
Batch Loss = 3.8222416637039944
2048, epoch_train_loss=3.8222416637039944
Epoch 2049
Epoch 2049 :: Batch 0/1
Batch Loss = 3.819172516913987
2049, epoch_train_loss=3.819172516913987
Epoch 2050
Epoch 2050 :: Batch 0/1
Batch Loss = 3.8182137097755953
2050, epoch_train_loss=3.8182137097755953
Epoch 2051
Epoch 2051 :: Batch 0/1
Batch Loss = 3.8158344977780834
2051, epoch_train_loss=3.8158344977780834
Epoch 2052
Epoch 2052 :: Batch 0/1
Batch Loss = 3.815202883895107
2052, epoch_train_loss=3.815202883895107
Epoch 2053
Epoch 2053 :: Batch 0/1
Batch Loss = 3.8147451611958636
2053, epoch_train_loss=3.8147451611958636
Epoch 2054
Epoch 2054 :: Batch 0/1
Batch Loss = 3.8129117180333223
2054, epoch_train_loss=3.8129117180333223
Epoch 2055
Epoch 2055 :: Batch 0/1
Batch Loss = 3.809606914689911
2055, epoch_train_loss=3.809606914689911
Epoch 2056
Epoch 2056 :: Batch 0/1
Batch Loss = 3.8064629922676843
2056, epoch_train_loss=3.8064629922676843
Epoch 2057
Epoch 2057 :: Batch 0/1
Batch Loss = 3.805553395472096
2057, epoch_train_loss=3.805553395472096
Epoch 2058
Epoch 2058 :: Batch 0/1
Batch Loss = 3.8053463416197197
2058, epoch_train_loss=3.8053463416197197
Epoch 2059
Epoch 2059 :: Batch 0/1
Batch Loss = 3.8049255199564307
2059, epoch_train_loss=3.8049255199564307
Epoch 2060
Epoch 2060 :: Batch 0/1
Batch Loss = 3.801657680571567
2060, epoch_train_loss=3.801657680571567
Epoch 2061
Epoch 2061 :: Batch 0/1
Batch Loss = 3.7988258443346967
2061, epoch_train_loss=3.7988258443346967
Epoch 2062
Epoch 2062 :: Batch 0/1
Batch Loss = 3.798003844265205
2062, epoch_train_loss=3.798003844265205
Epoch 2063
Epoch 2063 :: Batch 0/1
Batch Loss = 3.7969465752489415
2063, epoch_train_loss=3.7969465752489415
Epoch 2064
Epoch 2064 :: Batch 0/1
Batch Loss = 3.794469766045274
2064, epoch_train_loss=3.794469766045274
Epoch 2065
Epoch 2065 :: Batch 0/1
Batch Loss = 3.791579636239898
2065, epoch_train_loss=3.791579636239898
Epoch 2066
Epoch 2066 :: Batch 0/1
Batch Loss = 3.788678897846349
2066, epoch_train_loss=3.788678897846349
Epoch 2067
Epoch 2067 :: Batch 0/1
Batch Loss = 3.784572087401378
2067, epoch_train_loss=3.784572087401378
Epoch 2068
Epoch 2068 :: Batch 0/1
Batch Loss = 3.7617826774008254
2068, epoch_train_loss=3.7617826774008254
Epoch 2069
Epoch 2069 :: Batch 0/1
Batch Loss = 4.772493420497297
2069, epoch_train_loss=4.772493420497297
Epoch 2070
Epoch 2070 :: Batch 0/1
Batch Loss = 4.049082577136959
2070, epoch_train_loss=4.049082577136959
Epoch 2071
Epoch 2071 :: Batch 0/1
Batch Loss = 3.8726115222519866
2071, epoch_train_loss=3.8726115222519866
Epoch 2072
Epoch 2072 :: Batch 0/1
Batch Loss = 3.8493419834580656
2072, epoch_train_loss=3.8493419834580656
Epoch 2073
Epoch 2073 :: Batch 0/1
Batch Loss = 3.8595521908194588
2073, epoch_train_loss=3.8595521908194588
Epoch 2074
Epoch 2074 :: Batch 0/1
Batch Loss = 3.839344279753797
2074, epoch_train_loss=3.839344279753797
Epoch 2075
Epoch 2075 :: Batch 0/1
Batch Loss = 3.8240593349725613
2075, epoch_train_loss=3.8240593349725613
Epoch 2076
Epoch 2076 :: Batch 0/1
Batch Loss = 3.8166165991461036
2076, epoch_train_loss=3.8166165991461036
Epoch 2077
Epoch 2077 :: Batch 0/1
Batch Loss = 3.8134751035882757
2077, epoch_train_loss=3.8134751035882757
Epoch 2078
Epoch 2078 :: Batch 0/1
Batch Loss = 3.812933890618076
2078, epoch_train_loss=3.812933890618076
Epoch 2079
Epoch 2079 :: Batch 0/1
Batch Loss = 3.8142700202745026
2079, epoch_train_loss=3.8142700202745026
Epoch 2080
Epoch 2080 :: Batch 0/1
Batch Loss = 3.816568382846278
2080, epoch_train_loss=3.816568382846278
Epoch 2081
Epoch 2081 :: Batch 0/1
Batch Loss = 3.8178662531860326
2081, epoch_train_loss=3.8178662531860326
Epoch 2082
Epoch 2082 :: Batch 0/1
Batch Loss = 3.817604727644191
2082, epoch_train_loss=3.817604727644191
Epoch 2083
Epoch 2083 :: Batch 0/1
Batch Loss = 3.8155508239930924
2083, epoch_train_loss=3.8155508239930924
Epoch 2084
Epoch 2084 :: Batch 0/1
Batch Loss = 3.812370421756772
2084, epoch_train_loss=3.812370421756772
Epoch 2085
Epoch 2085 :: Batch 0/1
Batch Loss = 3.8093643220959246
2085, epoch_train_loss=3.8093643220959246
Epoch 2086
Epoch 2086 :: Batch 0/1
Batch Loss = 3.806467005205465
2086, epoch_train_loss=3.806467005205465
Epoch 2087
Epoch 2087 :: Batch 0/1
Batch Loss = 3.803914060574243
2087, epoch_train_loss=3.803914060574243
Epoch 2088
Epoch 2088 :: Batch 0/1
Batch Loss = 3.801636489734368
2088, epoch_train_loss=3.801636489734368
Epoch 2089
Epoch 2089 :: Batch 0/1
Batch Loss = 3.7996168797262633
2089, epoch_train_loss=3.7996168797262633
Epoch 2090
Epoch 2090 :: Batch 0/1
Batch Loss = 3.7982038415938617
2090, epoch_train_loss=3.7982038415938617
Epoch 2091
Epoch 2091 :: Batch 0/1
Batch Loss = 3.7970619078127483
2091, epoch_train_loss=3.7970619078127483
Epoch 2092
Epoch 2092 :: Batch 0/1
Batch Loss = 3.7961955788253796
2092, epoch_train_loss=3.7961955788253796
Epoch 2093
Epoch 2093 :: Batch 0/1
Batch Loss = 3.795130432525851
2093, epoch_train_loss=3.795130432525851
Epoch 2094
Epoch 2094 :: Batch 0/1
Batch Loss = 3.793648067431557
2094, epoch_train_loss=3.793648067431557
Epoch 2095
Epoch 2095 :: Batch 0/1
Batch Loss = 3.7917854894948677
2095, epoch_train_loss=3.7917854894948677
Epoch 2096
Epoch 2096 :: Batch 0/1
Batch Loss = 3.789419233826866
2096, epoch_train_loss=3.789419233826866
Epoch 2097
Epoch 2097 :: Batch 0/1
Batch Loss = 3.78666257346797
2097, epoch_train_loss=3.78666257346797
Epoch 2098
Epoch 2098 :: Batch 0/1
Batch Loss = 3.7836845288449616
2098, epoch_train_loss=3.7836845288449616
Epoch 2099
Epoch 2099 :: Batch 0/1
Batch Loss = 3.781230103456217
2099, epoch_train_loss=3.781230103456217
Epoch 2100
Epoch 2100 :: Batch 0/1
Batch Loss = 3.7802600339657024
2100, epoch_train_loss=3.7802600339657024
Epoch 2101
Epoch 2101 :: Batch 0/1
Batch Loss = 3.7787984390097993
2101, epoch_train_loss=3.7787984390097993
Epoch 2102
Epoch 2102 :: Batch 0/1
Batch Loss = 3.776410037609128
2102, epoch_train_loss=3.776410037609128
Epoch 2103
Epoch 2103 :: Batch 0/1
Batch Loss = 3.774975219414844
2103, epoch_train_loss=3.774975219414844
Epoch 2104
Epoch 2104 :: Batch 0/1
Batch Loss = 3.774092571625974
2104, epoch_train_loss=3.774092571625974
Epoch 2105
Epoch 2105 :: Batch 0/1
Batch Loss = 3.7730372655801983
2105, epoch_train_loss=3.7730372655801983
Epoch 2106
Epoch 2106 :: Batch 0/1
Batch Loss = 3.7715162228298835
2106, epoch_train_loss=3.7715162228298835
Epoch 2107
Epoch 2107 :: Batch 0/1
Batch Loss = 3.7696172512048984
2107, epoch_train_loss=3.7696172512048984
Epoch 2108
Epoch 2108 :: Batch 0/1
Batch Loss = 3.767646102514759
2108, epoch_train_loss=3.767646102514759
Epoch 2109
Epoch 2109 :: Batch 0/1
Batch Loss = 3.7659577796343444
2109, epoch_train_loss=3.7659577796343444
Epoch 2110
Epoch 2110 :: Batch 0/1
Batch Loss = 3.764630537136954
2110, epoch_train_loss=3.764630537136954
Epoch 2111
Epoch 2111 :: Batch 0/1
Batch Loss = 3.7630776075793317
2111, epoch_train_loss=3.7630776075793317
Epoch 2112
Epoch 2112 :: Batch 0/1
Batch Loss = 3.761172337707705
2112, epoch_train_loss=3.761172337707705
Epoch 2113
Epoch 2113 :: Batch 0/1
Batch Loss = 3.7593814736933786
2113, epoch_train_loss=3.7593814736933786
Epoch 2114
Epoch 2114 :: Batch 0/1
Batch Loss = 3.757798206399715
2114, epoch_train_loss=3.757798206399715
Epoch 2115
Epoch 2115 :: Batch 0/1
Batch Loss = 3.7563257176662503
2115, epoch_train_loss=3.7563257176662503
Epoch 2116
Epoch 2116 :: Batch 0/1
Batch Loss = 3.7548182391150395
2116, epoch_train_loss=3.7548182391150395
Epoch 2117
Epoch 2117 :: Batch 0/1
Batch Loss = 3.753193083448151
2117, epoch_train_loss=3.753193083448151
Epoch 2118
Epoch 2118 :: Batch 0/1
Batch Loss = 3.751539929606101
2118, epoch_train_loss=3.751539929606101
Epoch 2119
Epoch 2119 :: Batch 0/1
Batch Loss = 3.7499853643713164
2119, epoch_train_loss=3.7499853643713164
Epoch 2120
Epoch 2120 :: Batch 0/1
Batch Loss = 3.748570347324167
2120, epoch_train_loss=3.748570347324167
Epoch 2121
Epoch 2121 :: Batch 0/1
Batch Loss = 3.747144679850708
2121, epoch_train_loss=3.747144679850708
Epoch 2122
Epoch 2122 :: Batch 0/1
Batch Loss = 3.745655032546384
2122, epoch_train_loss=3.745655032546384
Epoch 2123
Epoch 2123 :: Batch 0/1
Batch Loss = 3.744211564932315
2123, epoch_train_loss=3.744211564932315
Epoch 2124
Epoch 2124 :: Batch 0/1
Batch Loss = 3.7427183841925236
2124, epoch_train_loss=3.7427183841925236
Epoch 2125
Epoch 2125 :: Batch 0/1
Batch Loss = 3.741034058371317
2125, epoch_train_loss=3.741034058371317
Epoch 2126
Epoch 2126 :: Batch 0/1
Batch Loss = 3.7391975350063817
2126, epoch_train_loss=3.7391975350063817
Epoch 2127
Epoch 2127 :: Batch 0/1
Batch Loss = 3.7372877665928104
2127, epoch_train_loss=3.7372877665928104
Epoch 2128
Epoch 2128 :: Batch 0/1
Batch Loss = 3.7353863340742306
2128, epoch_train_loss=3.7353863340742306
Epoch 2129
Epoch 2129 :: Batch 0/1
Batch Loss = 3.733527393832851
2129, epoch_train_loss=3.733527393832851
Epoch 2130
Epoch 2130 :: Batch 0/1
Batch Loss = 3.7316949589379482
2130, epoch_train_loss=3.7316949589379482
Epoch 2131
Epoch 2131 :: Batch 0/1
Batch Loss = 3.7299056687874894
2131, epoch_train_loss=3.7299056687874894
Epoch 2132
Epoch 2132 :: Batch 0/1
Batch Loss = 3.728208730098006
2132, epoch_train_loss=3.728208730098006
Epoch 2133
Epoch 2133 :: Batch 0/1
Batch Loss = 3.726502905336557
2133, epoch_train_loss=3.726502905336557
Epoch 2134
Epoch 2134 :: Batch 0/1
Batch Loss = 3.7245373796393
2134, epoch_train_loss=3.7245373796393
Epoch 2135
Epoch 2135 :: Batch 0/1
Batch Loss = 3.7221564398948854
2135, epoch_train_loss=3.7221564398948854
Epoch 2136
Epoch 2136 :: Batch 0/1
Batch Loss = 3.7191921055290726
2136, epoch_train_loss=3.7191921055290726
Epoch 2137
Epoch 2137 :: Batch 0/1
Batch Loss = 3.7156428180092154
2137, epoch_train_loss=3.7156428180092154
Epoch 2138
Epoch 2138 :: Batch 0/1
Batch Loss = 3.7115199197112325
2138, epoch_train_loss=3.7115199197112325
Epoch 2139
Epoch 2139 :: Batch 0/1
Batch Loss = 3.7072879619484302
2139, epoch_train_loss=3.7072879619484302
Epoch 2140
Epoch 2140 :: Batch 0/1
Batch Loss = 3.7039121583605983
2140, epoch_train_loss=3.7039121583605983
Epoch 2141
Epoch 2141 :: Batch 0/1
Batch Loss = 3.7023708984173362
2141, epoch_train_loss=3.7023708984173362
Epoch 2142
Epoch 2142 :: Batch 0/1
Batch Loss = 3.7018749332480367
2142, epoch_train_loss=3.7018749332480367
Epoch 2143
Epoch 2143 :: Batch 0/1
Batch Loss = 3.6999376288399843
2143, epoch_train_loss=3.6999376288399843
Epoch 2144
Epoch 2144 :: Batch 0/1
Batch Loss = 3.696261824243849
2144, epoch_train_loss=3.696261824243849
Epoch 2145
Epoch 2145 :: Batch 0/1
Batch Loss = 3.6921079972230073
2145, epoch_train_loss=3.6921079972230073
Epoch 2146
Epoch 2146 :: Batch 0/1
Batch Loss = 3.6886833735353184
2146, epoch_train_loss=3.6886833735353184
Epoch 2147
Epoch 2147 :: Batch 0/1
Batch Loss = 3.686680065683649
2147, epoch_train_loss=3.686680065683649
Epoch 2148
Epoch 2148 :: Batch 0/1
Batch Loss = 3.685334815595501
2148, epoch_train_loss=3.685334815595501
Epoch 2149
Epoch 2149 :: Batch 0/1
Batch Loss = 3.6835331497917685
2149, epoch_train_loss=3.6835331497917685
Epoch 2150
Epoch 2150 :: Batch 0/1
Batch Loss = 3.681271127671356
2150, epoch_train_loss=3.681271127671356
Epoch 2151
Epoch 2151 :: Batch 0/1
Batch Loss = 3.678687629824675
2151, epoch_train_loss=3.678687629824675
Epoch 2152
Epoch 2152 :: Batch 0/1
Batch Loss = 3.675752518230655
2152, epoch_train_loss=3.675752518230655
Epoch 2153
Epoch 2153 :: Batch 0/1
Batch Loss = 3.6723349939663135
2153, epoch_train_loss=3.6723349939663135
Epoch 2154
Epoch 2154 :: Batch 0/1
Batch Loss = 3.6690637397160417
2154, epoch_train_loss=3.6690637397160417
Epoch 2155
Epoch 2155 :: Batch 0/1
Batch Loss = 3.666725398673572
2155, epoch_train_loss=3.666725398673572
Epoch 2156
Epoch 2156 :: Batch 0/1
Batch Loss = 3.664755374026903
2156, epoch_train_loss=3.664755374026903
Epoch 2157
Epoch 2157 :: Batch 0/1
Batch Loss = 3.6623032201425296
2157, epoch_train_loss=3.6623032201425296
Epoch 2158
Epoch 2158 :: Batch 0/1
Batch Loss = 3.6591646407896152
2158, epoch_train_loss=3.6591646407896152
Epoch 2159
Epoch 2159 :: Batch 0/1
Batch Loss = 3.655842004187358
2159, epoch_train_loss=3.655842004187358
Epoch 2160
Epoch 2160 :: Batch 0/1
Batch Loss = 3.6528327912754244
2160, epoch_train_loss=3.6528327912754244
Epoch 2161
Epoch 2161 :: Batch 0/1
Batch Loss = 3.6501095680158993
2161, epoch_train_loss=3.6501095680158993
Epoch 2162
Epoch 2162 :: Batch 0/1
Batch Loss = 3.647112076205732
2162, epoch_train_loss=3.647112076205732
Epoch 2163
Epoch 2163 :: Batch 0/1
Batch Loss = 3.6434645895561593
2163, epoch_train_loss=3.6434645895561593
Epoch 2164
Epoch 2164 :: Batch 0/1
Batch Loss = 3.639650477620124
2164, epoch_train_loss=3.639650477620124
Epoch 2165
Epoch 2165 :: Batch 0/1
Batch Loss = 3.6361290872559753
2165, epoch_train_loss=3.6361290872559753
Epoch 2166
Epoch 2166 :: Batch 0/1
Batch Loss = 3.6329619829912825
2166, epoch_train_loss=3.6329619829912825
Epoch 2167
Epoch 2167 :: Batch 0/1
Batch Loss = 3.629642998936276
2167, epoch_train_loss=3.629642998936276
Epoch 2168
Epoch 2168 :: Batch 0/1
Batch Loss = 3.6258351227135357
2168, epoch_train_loss=3.6258351227135357
Epoch 2169
Epoch 2169 :: Batch 0/1
Batch Loss = 3.6219075988481184
2169, epoch_train_loss=3.6219075988481184
Epoch 2170
Epoch 2170 :: Batch 0/1
Batch Loss = 3.618000088853748
2170, epoch_train_loss=3.618000088853748
Epoch 2171
Epoch 2171 :: Batch 0/1
Batch Loss = 3.613808028728284
2171, epoch_train_loss=3.613808028728284
Epoch 2172
Epoch 2172 :: Batch 0/1
Batch Loss = 3.6091150964128795
2172, epoch_train_loss=3.6091150964128795
Epoch 2173
Epoch 2173 :: Batch 0/1
Batch Loss = 3.6036259335854224
2173, epoch_train_loss=3.6036259335854224
Epoch 2174
Epoch 2174 :: Batch 0/1
Batch Loss = 3.597987996044949
2174, epoch_train_loss=3.597987996044949
Epoch 2175
Epoch 2175 :: Batch 0/1
Batch Loss = 3.592495940584369
2175, epoch_train_loss=3.592495940584369
Epoch 2176
Epoch 2176 :: Batch 0/1
Batch Loss = 3.586487695037677
2176, epoch_train_loss=3.586487695037677
Epoch 2177
Epoch 2177 :: Batch 0/1
Batch Loss = 3.579299222989536
2177, epoch_train_loss=3.579299222989536
Epoch 2178
Epoch 2178 :: Batch 0/1
Batch Loss = 3.5712810840275075
2178, epoch_train_loss=3.5712810840275075
Epoch 2179
Epoch 2179 :: Batch 0/1
Batch Loss = 3.5626746180014557
2179, epoch_train_loss=3.5626746180014557
Epoch 2180
Epoch 2180 :: Batch 0/1
Batch Loss = 3.554155192662044
2180, epoch_train_loss=3.554155192662044
Epoch 2181
Epoch 2181 :: Batch 0/1
Batch Loss = 3.545301614966771
2181, epoch_train_loss=3.545301614966771
Epoch 2182
Epoch 2182 :: Batch 0/1
Batch Loss = 3.5359459530008173
2182, epoch_train_loss=3.5359459530008173
Epoch 2183
Epoch 2183 :: Batch 0/1
Batch Loss = 3.5263486007060383
2183, epoch_train_loss=3.5263486007060383
Epoch 2184
Epoch 2184 :: Batch 0/1
Batch Loss = 3.516215507941386
2184, epoch_train_loss=3.516215507941386
Epoch 2185
Epoch 2185 :: Batch 0/1
Batch Loss = 3.5055350268607866
2185, epoch_train_loss=3.5055350268607866
Epoch 2186
Epoch 2186 :: Batch 0/1
Batch Loss = 3.494339849379206
2186, epoch_train_loss=3.494339849379206
Epoch 2187
Epoch 2187 :: Batch 0/1
Batch Loss = 3.4820015773361344
2187, epoch_train_loss=3.4820015773361344
Epoch 2188
Epoch 2188 :: Batch 0/1
Batch Loss = 3.4693275893392594
2188, epoch_train_loss=3.4693275893392594
Epoch 2189
Epoch 2189 :: Batch 0/1
Batch Loss = 3.4565630638818905
2189, epoch_train_loss=3.4565630638818905
Epoch 2190
Epoch 2190 :: Batch 0/1
Batch Loss = 3.441640784526815
2190, epoch_train_loss=3.441640784526815
Epoch 2191
Epoch 2191 :: Batch 0/1
Batch Loss = 3.427590349362025
2191, epoch_train_loss=3.427590349362025
Epoch 2192
Epoch 2192 :: Batch 0/1
Batch Loss = 3.4146800645810402
2192, epoch_train_loss=3.4146800645810402
Epoch 2193
Epoch 2193 :: Batch 0/1
Batch Loss = 3.4093454089489494
2193, epoch_train_loss=3.4093454089489494
Epoch 2194
Epoch 2194 :: Batch 0/1
Batch Loss = 3.399536432158777
2194, epoch_train_loss=3.399536432158777
Epoch 2195
Epoch 2195 :: Batch 0/1
Batch Loss = 3.501752524167045
2195, epoch_train_loss=3.501752524167045
Epoch 2196
Epoch 2196 :: Batch 0/1
Batch Loss = 3.425151673056363
2196, epoch_train_loss=3.425151673056363
Epoch 2197
Epoch 2197 :: Batch 0/1
Batch Loss = 3.4427812094371113
2197, epoch_train_loss=3.4427812094371113
Epoch 2198
Epoch 2198 :: Batch 0/1
Batch Loss = 3.4552388544222175
2198, epoch_train_loss=3.4552388544222175
Epoch 2199
Epoch 2199 :: Batch 0/1
Batch Loss = 3.440470919958381
2199, epoch_train_loss=3.440470919958381
Epoch 2200
Epoch 2200 :: Batch 0/1
Batch Loss = 3.44289765254762
2200, epoch_train_loss=3.44289765254762
Epoch 2201
Epoch 2201 :: Batch 0/1
Batch Loss = 3.438698943924319
2201, epoch_train_loss=3.438698943924319
Epoch 2202
Epoch 2202 :: Batch 0/1
Batch Loss = 3.44100911047369
2202, epoch_train_loss=3.44100911047369
Epoch 2203
Epoch 2203 :: Batch 0/1
Batch Loss = 3.4436727940636085
2203, epoch_train_loss=3.4436727940636085
Epoch 2204
Epoch 2204 :: Batch 0/1
Batch Loss = 3.444993731303922
2204, epoch_train_loss=3.444993731303922
Epoch 2205
Epoch 2205 :: Batch 0/1
Batch Loss = 3.4435685819902
2205, epoch_train_loss=3.4435685819902
Epoch 2206
Epoch 2206 :: Batch 0/1
Batch Loss = 3.4408147877109605
2206, epoch_train_loss=3.4408147877109605
Epoch 2207
Epoch 2207 :: Batch 0/1
Batch Loss = 3.4386708897661076
2207, epoch_train_loss=3.4386708897661076
Epoch 2208
Epoch 2208 :: Batch 0/1
Batch Loss = 3.4356744884611596
2208, epoch_train_loss=3.4356744884611596
Epoch 2209
Epoch 2209 :: Batch 0/1
Batch Loss = 3.43125230904173
2209, epoch_train_loss=3.43125230904173
Epoch 2210
Epoch 2210 :: Batch 0/1
Batch Loss = 3.429485863838873
2210, epoch_train_loss=3.429485863838873
Epoch 2211
Epoch 2211 :: Batch 0/1
Batch Loss = 3.4272283211814893
2211, epoch_train_loss=3.4272283211814893
Epoch 2212
Epoch 2212 :: Batch 0/1
Batch Loss = 3.4234293500713497
2212, epoch_train_loss=3.4234293500713497
Epoch 2213
Epoch 2213 :: Batch 0/1
Batch Loss = 3.4200496468273376
2213, epoch_train_loss=3.4200496468273376
Epoch 2214
Epoch 2214 :: Batch 0/1
Batch Loss = 3.416014720171927
2214, epoch_train_loss=3.416014720171927
Epoch 2215
Epoch 2215 :: Batch 0/1
Batch Loss = 3.41192323262758
2215, epoch_train_loss=3.41192323262758
Epoch 2216
Epoch 2216 :: Batch 0/1
Batch Loss = 3.4079872211674425
2216, epoch_train_loss=3.4079872211674425
Epoch 2217
Epoch 2217 :: Batch 0/1
Batch Loss = 3.4044428868603682
2217, epoch_train_loss=3.4044428868603682
Epoch 2218
Epoch 2218 :: Batch 0/1
Batch Loss = 3.4007294084852173
2218, epoch_train_loss=3.4007294084852173
Epoch 2219
Epoch 2219 :: Batch 0/1
Batch Loss = 3.3962555461436774
2219, epoch_train_loss=3.3962555461436774
Epoch 2220
Epoch 2220 :: Batch 0/1
Batch Loss = 3.3921707787457076
2220, epoch_train_loss=3.3921707787457076
Epoch 2221
Epoch 2221 :: Batch 0/1
Batch Loss = 3.388837264227229
2221, epoch_train_loss=3.388837264227229
Epoch 2222
Epoch 2222 :: Batch 0/1
Batch Loss = 3.3847237607544893
2222, epoch_train_loss=3.3847237607544893
Epoch 2223
Epoch 2223 :: Batch 0/1
Batch Loss = 3.3809526183228753
2223, epoch_train_loss=3.3809526183228753
Epoch 2224
Epoch 2224 :: Batch 0/1
Batch Loss = 3.377171925193078
2224, epoch_train_loss=3.377171925193078
Epoch 2225
Epoch 2225 :: Batch 0/1
Batch Loss = 3.3731906631736637
2225, epoch_train_loss=3.3731906631736637
Epoch 2226
Epoch 2226 :: Batch 0/1
Batch Loss = 3.3694233420010433
2226, epoch_train_loss=3.3694233420010433
Epoch 2227
Epoch 2227 :: Batch 0/1
Batch Loss = 3.3659805189498453
2227, epoch_train_loss=3.3659805189498453
Epoch 2228
Epoch 2228 :: Batch 0/1
Batch Loss = 3.3620780149996548
2228, epoch_train_loss=3.3620780149996548
Epoch 2229
Epoch 2229 :: Batch 0/1
Batch Loss = 3.358165966546179
2229, epoch_train_loss=3.358165966546179
Epoch 2230
Epoch 2230 :: Batch 0/1
Batch Loss = 3.354360622028554
2230, epoch_train_loss=3.354360622028554
Epoch 2231
Epoch 2231 :: Batch 0/1
Batch Loss = 3.35069742574459
2231, epoch_train_loss=3.35069742574459
Epoch 2232
Epoch 2232 :: Batch 0/1
Batch Loss = 3.3469412986627742
2232, epoch_train_loss=3.3469412986627742
Epoch 2233
Epoch 2233 :: Batch 0/1
Batch Loss = 3.34334912894639
2233, epoch_train_loss=3.34334912894639
Epoch 2234
Epoch 2234 :: Batch 0/1
Batch Loss = 3.3404353794249513
2234, epoch_train_loss=3.3404353794249513
Epoch 2235
Epoch 2235 :: Batch 0/1
Batch Loss = 3.337597042074797
2235, epoch_train_loss=3.337597042074797
Epoch 2236
Epoch 2236 :: Batch 0/1
Batch Loss = 3.333232222407075
2236, epoch_train_loss=3.333232222407075
Epoch 2237
Epoch 2237 :: Batch 0/1
Batch Loss = 3.327455639928619
2237, epoch_train_loss=3.327455639928619
Epoch 2238
Epoch 2238 :: Batch 0/1
Batch Loss = 3.323555614713496
2238, epoch_train_loss=3.323555614713496
Epoch 2239
Epoch 2239 :: Batch 0/1
Batch Loss = 3.3213540808254796
2239, epoch_train_loss=3.3213540808254796
Epoch 2240
Epoch 2240 :: Batch 0/1
Batch Loss = 3.3170102502394183
2240, epoch_train_loss=3.3170102502394183
Epoch 2241
Epoch 2241 :: Batch 0/1
Batch Loss = 3.311968569691765
2241, epoch_train_loss=3.311968569691765
Epoch 2242
Epoch 2242 :: Batch 0/1
Batch Loss = 3.3095919430905756
2242, epoch_train_loss=3.3095919430905756
Epoch 2243
Epoch 2243 :: Batch 0/1
Batch Loss = 3.3075270135544654
2243, epoch_train_loss=3.3075270135544654
Epoch 2244
Epoch 2244 :: Batch 0/1
Batch Loss = 3.303679156471392
2244, epoch_train_loss=3.303679156471392
Epoch 2245
Epoch 2245 :: Batch 0/1
Batch Loss = 3.300271481222146
2245, epoch_train_loss=3.300271481222146
Epoch 2246
Epoch 2246 :: Batch 0/1
Batch Loss = 3.2978678598724365
2246, epoch_train_loss=3.2978678598724365
Epoch 2247
Epoch 2247 :: Batch 0/1
Batch Loss = 3.2946486413512286
2247, epoch_train_loss=3.2946486413512286
Epoch 2248
Epoch 2248 :: Batch 0/1
Batch Loss = 3.290599648532467
2248, epoch_train_loss=3.290599648532467
Epoch 2249
Epoch 2249 :: Batch 0/1
Batch Loss = 3.287572987624124
2249, epoch_train_loss=3.287572987624124
Epoch 2250
Epoch 2250 :: Batch 0/1
Batch Loss = 3.2852609755332773
2250, epoch_train_loss=3.2852609755332773
Epoch 2251
Epoch 2251 :: Batch 0/1
Batch Loss = 3.282402389526552
2251, epoch_train_loss=3.282402389526552
Epoch 2252
Epoch 2252 :: Batch 0/1
Batch Loss = 3.2785564295856933
2252, epoch_train_loss=3.2785564295856933
Epoch 2253
Epoch 2253 :: Batch 0/1
Batch Loss = 3.2751740136010046
2253, epoch_train_loss=3.2751740136010046
Epoch 2254
Epoch 2254 :: Batch 0/1
Batch Loss = 3.272310206657847
2254, epoch_train_loss=3.272310206657847
Epoch 2255
Epoch 2255 :: Batch 0/1
Batch Loss = 3.269817497152964
2255, epoch_train_loss=3.269817497152964
Epoch 2256
Epoch 2256 :: Batch 0/1
Batch Loss = 3.2668161554559774
2256, epoch_train_loss=3.2668161554559774
Epoch 2257
Epoch 2257 :: Batch 0/1
Batch Loss = 3.263028063521715
2257, epoch_train_loss=3.263028063521715
Epoch 2258
Epoch 2258 :: Batch 0/1
Batch Loss = 3.2591341184877174
2258, epoch_train_loss=3.2591341184877174
Epoch 2259
Epoch 2259 :: Batch 0/1
Batch Loss = 3.2559441769844417
2259, epoch_train_loss=3.2559441769844417
Epoch 2260
Epoch 2260 :: Batch 0/1
Batch Loss = 3.2533561307688497
2260, epoch_train_loss=3.2533561307688497
Epoch 2261
Epoch 2261 :: Batch 0/1
Batch Loss = 3.2513593303424932
2261, epoch_train_loss=3.2513593303424932
Epoch 2262
Epoch 2262 :: Batch 0/1
Batch Loss = 3.2497470989552144
2262, epoch_train_loss=3.2497470989552144
Epoch 2263
Epoch 2263 :: Batch 0/1
Batch Loss = 3.247889854807321
2263, epoch_train_loss=3.247889854807321
Epoch 2264
Epoch 2264 :: Batch 0/1
Batch Loss = 3.2445461640703632
2264, epoch_train_loss=3.2445461640703632
Epoch 2265
Epoch 2265 :: Batch 0/1
Batch Loss = 3.240358161678886
2265, epoch_train_loss=3.240358161678886
Epoch 2266
Epoch 2266 :: Batch 0/1
Batch Loss = 3.23635550873305
2266, epoch_train_loss=3.23635550873305
Epoch 2267
Epoch 2267 :: Batch 0/1
Batch Loss = 3.2323720142512036
2267, epoch_train_loss=3.2323720142512036
Epoch 2268
Epoch 2268 :: Batch 0/1
Batch Loss = 3.229091158742499
2268, epoch_train_loss=3.229091158742499
Epoch 2269
Epoch 2269 :: Batch 0/1
Batch Loss = 3.225963555789819
2269, epoch_train_loss=3.225963555789819
Epoch 2270
Epoch 2270 :: Batch 0/1
Batch Loss = 3.222338547495327
2270, epoch_train_loss=3.222338547495327
Epoch 2271
Epoch 2271 :: Batch 0/1
Batch Loss = 3.2181530019860523
2271, epoch_train_loss=3.2181530019860523
Epoch 2272
Epoch 2272 :: Batch 0/1
Batch Loss = 3.2148694404221483
2272, epoch_train_loss=3.2148694404221483
Epoch 2273
Epoch 2273 :: Batch 0/1
Batch Loss = 3.212972359415418
2273, epoch_train_loss=3.212972359415418
Epoch 2274
Epoch 2274 :: Batch 0/1
Batch Loss = 3.2114760858536755
2274, epoch_train_loss=3.2114760858536755
Epoch 2275
Epoch 2275 :: Batch 0/1
Batch Loss = 3.2092799498798774
2275, epoch_train_loss=3.2092799498798774
Epoch 2276
Epoch 2276 :: Batch 0/1
Batch Loss = 3.205838545583806
2276, epoch_train_loss=3.205838545583806
Epoch 2277
Epoch 2277 :: Batch 0/1
Batch Loss = 3.202336831158399
2277, epoch_train_loss=3.202336831158399
Epoch 2278
Epoch 2278 :: Batch 0/1
Batch Loss = 3.1998273985935533
2278, epoch_train_loss=3.1998273985935533
Epoch 2279
Epoch 2279 :: Batch 0/1
Batch Loss = 3.198494226687985
2279, epoch_train_loss=3.198494226687985
Epoch 2280
Epoch 2280 :: Batch 0/1
Batch Loss = 3.19710656466519
2280, epoch_train_loss=3.19710656466519
Epoch 2281
Epoch 2281 :: Batch 0/1
Batch Loss = 3.194401932596231
2281, epoch_train_loss=3.194401932596231
Epoch 2282
Epoch 2282 :: Batch 0/1
Batch Loss = 3.1906837676865165
2282, epoch_train_loss=3.1906837676865165
Epoch 2283
Epoch 2283 :: Batch 0/1
Batch Loss = 3.187742958901062
2283, epoch_train_loss=3.187742958901062
Epoch 2284
Epoch 2284 :: Batch 0/1
Batch Loss = 3.1859647346468822
2284, epoch_train_loss=3.1859647346468822
Epoch 2285
Epoch 2285 :: Batch 0/1
Batch Loss = 3.1844281650799906
2285, epoch_train_loss=3.1844281650799906
Epoch 2286
Epoch 2286 :: Batch 0/1
Batch Loss = 3.182201652741117
2286, epoch_train_loss=3.182201652741117
Epoch 2287
Epoch 2287 :: Batch 0/1
Batch Loss = 3.1789150729464413
2287, epoch_train_loss=3.1789150729464413
Epoch 2288
Epoch 2288 :: Batch 0/1
Batch Loss = 3.1757493082063735
2288, epoch_train_loss=3.1757493082063735
Epoch 2289
Epoch 2289 :: Batch 0/1
Batch Loss = 3.172862529481158
2289, epoch_train_loss=3.172862529481158
Epoch 2290
Epoch 2290 :: Batch 0/1
Batch Loss = 3.1702320656138405
2290, epoch_train_loss=3.1702320656138405
Epoch 2291
Epoch 2291 :: Batch 0/1
Batch Loss = 3.1678049778525863
2291, epoch_train_loss=3.1678049778525863
Epoch 2292
Epoch 2292 :: Batch 0/1
Batch Loss = 3.165689555089856
2292, epoch_train_loss=3.165689555089856
Epoch 2293
Epoch 2293 :: Batch 0/1
Batch Loss = 3.1647660113681177
2293, epoch_train_loss=3.1647660113681177
Epoch 2294
Epoch 2294 :: Batch 0/1
Batch Loss = 3.170156320806688
2294, epoch_train_loss=3.170156320806688
Epoch 2295
Epoch 2295 :: Batch 0/1
Batch Loss = 3.200380866984022
2295, epoch_train_loss=3.200380866984022
Epoch 2296
Epoch 2296 :: Batch 0/1
Batch Loss = 3.309397658760724
2296, epoch_train_loss=3.309397658760724
Epoch 2297
Epoch 2297 :: Batch 0/1
Batch Loss = 3.3013684313400575
2297, epoch_train_loss=3.3013684313400575
Epoch 2298
Epoch 2298 :: Batch 0/1
Batch Loss = 3.160831929303048
2298, epoch_train_loss=3.160831929303048
Epoch 2299
Epoch 2299 :: Batch 0/1
Batch Loss = 3.212696662996147
2299, epoch_train_loss=3.212696662996147
Epoch 2300
Epoch 2300 :: Batch 0/1
Batch Loss = 3.192686808085002
2300, epoch_train_loss=3.192686808085002
Epoch 2301
Epoch 2301 :: Batch 0/1
Batch Loss = 3.1570518875104767
2301, epoch_train_loss=3.1570518875104767
Epoch 2302
Epoch 2302 :: Batch 0/1
Batch Loss = 3.184368569121186
2302, epoch_train_loss=3.184368569121186
Epoch 2303
Epoch 2303 :: Batch 0/1
Batch Loss = 3.164192566054413
2303, epoch_train_loss=3.164192566054413
Epoch 2304
Epoch 2304 :: Batch 0/1
Batch Loss = 3.1607917774856262
2304, epoch_train_loss=3.1607917774856262
Epoch 2305
Epoch 2305 :: Batch 0/1
Batch Loss = 3.168788313241255
2305, epoch_train_loss=3.168788313241255
Epoch 2306
Epoch 2306 :: Batch 0/1
Batch Loss = 3.161711422474054
2306, epoch_train_loss=3.161711422474054
Epoch 2307
Epoch 2307 :: Batch 0/1
Batch Loss = 3.151328731251044
2307, epoch_train_loss=3.151328731251044
Epoch 2308
Epoch 2308 :: Batch 0/1
Batch Loss = 3.1463086428805433
2308, epoch_train_loss=3.1463086428805433
Epoch 2309
Epoch 2309 :: Batch 0/1
Batch Loss = 3.1546860113863464
2309, epoch_train_loss=3.1546860113863464
Epoch 2310
Epoch 2310 :: Batch 0/1
Batch Loss = 3.1377111576887717
2310, epoch_train_loss=3.1377111576887717
Epoch 2311
Epoch 2311 :: Batch 0/1
Batch Loss = 3.135508915993404
2311, epoch_train_loss=3.135508915993404
Epoch 2312
Epoch 2312 :: Batch 0/1
Batch Loss = 3.129814384087146
2312, epoch_train_loss=3.129814384087146
Epoch 2313
Epoch 2313 :: Batch 0/1
Batch Loss = 3.1279310116080237
2313, epoch_train_loss=3.1279310116080237
Epoch 2314
Epoch 2314 :: Batch 0/1
Batch Loss = 3.1253840423354604
2314, epoch_train_loss=3.1253840423354604
Epoch 2315
Epoch 2315 :: Batch 0/1
Batch Loss = 3.116612460434652
2315, epoch_train_loss=3.116612460434652
Epoch 2316
Epoch 2316 :: Batch 0/1
Batch Loss = 3.1145895449771333
2316, epoch_train_loss=3.1145895449771333
Epoch 2317
Epoch 2317 :: Batch 0/1
Batch Loss = 3.108882400745361
2317, epoch_train_loss=3.108882400745361
Epoch 2318
Epoch 2318 :: Batch 0/1
Batch Loss = 3.1094028135029754
2318, epoch_train_loss=3.1094028135029754
Epoch 2319
Epoch 2319 :: Batch 0/1
Batch Loss = 3.104435601760996
2319, epoch_train_loss=3.104435601760996
Epoch 2320
Epoch 2320 :: Batch 0/1
Batch Loss = 3.101252165300571
2320, epoch_train_loss=3.101252165300571
Epoch 2321
Epoch 2321 :: Batch 0/1
Batch Loss = 3.099888355000004
2321, epoch_train_loss=3.099888355000004
Epoch 2322
Epoch 2322 :: Batch 0/1
Batch Loss = 3.0955980473936733
2322, epoch_train_loss=3.0955980473936733
Epoch 2323
Epoch 2323 :: Batch 0/1
Batch Loss = 3.094330795560245
2323, epoch_train_loss=3.094330795560245
Epoch 2324
Epoch 2324 :: Batch 0/1
Batch Loss = 3.0909549742250997
2324, epoch_train_loss=3.0909549742250997
Epoch 2325
Epoch 2325 :: Batch 0/1
Batch Loss = 3.088023004645939
2325, epoch_train_loss=3.088023004645939
Epoch 2326
Epoch 2326 :: Batch 0/1
Batch Loss = 3.0859433108231986
2326, epoch_train_loss=3.0859433108231986
Epoch 2327
Epoch 2327 :: Batch 0/1
Batch Loss = 3.082010876119692
2327, epoch_train_loss=3.082010876119692
Epoch 2328
Epoch 2328 :: Batch 0/1
Batch Loss = 3.07957227148405
2328, epoch_train_loss=3.07957227148405
Epoch 2329
Epoch 2329 :: Batch 0/1
Batch Loss = 3.0749011690785513
2329, epoch_train_loss=3.0749011690785513
Epoch 2330
Epoch 2330 :: Batch 0/1
Batch Loss = 3.0708536268581663
2330, epoch_train_loss=3.0708536268581663
Epoch 2331
Epoch 2331 :: Batch 0/1
Batch Loss = 3.0685272509670485
2331, epoch_train_loss=3.0685272509670485
Epoch 2332
Epoch 2332 :: Batch 0/1
Batch Loss = 3.0642733697474136
2332, epoch_train_loss=3.0642733697474136
Epoch 2333
Epoch 2333 :: Batch 0/1
Batch Loss = 3.0601505655657424
2333, epoch_train_loss=3.0601505655657424
Epoch 2334
Epoch 2334 :: Batch 0/1
Batch Loss = 3.0573464693797523
2334, epoch_train_loss=3.0573464693797523
Epoch 2335
Epoch 2335 :: Batch 0/1
Batch Loss = 3.053742202175878
2335, epoch_train_loss=3.053742202175878
Epoch 2336
Epoch 2336 :: Batch 0/1
Batch Loss = 3.0512001147153422
2336, epoch_train_loss=3.0512001147153422
Epoch 2337
Epoch 2337 :: Batch 0/1
Batch Loss = 3.049095253245187
2337, epoch_train_loss=3.049095253245187
Epoch 2338
Epoch 2338 :: Batch 0/1
Batch Loss = 3.0459268320523956
2338, epoch_train_loss=3.0459268320523956
Epoch 2339
Epoch 2339 :: Batch 0/1
Batch Loss = 3.042813181876823
2339, epoch_train_loss=3.042813181876823
Epoch 2340
Epoch 2340 :: Batch 0/1
Batch Loss = 3.040938509744483
2340, epoch_train_loss=3.040938509744483
Epoch 2341
Epoch 2341 :: Batch 0/1
Batch Loss = 3.0387942317511656
2341, epoch_train_loss=3.0387942317511656
Epoch 2342
Epoch 2342 :: Batch 0/1
Batch Loss = 3.036321121199583
2342, epoch_train_loss=3.036321121199583
Epoch 2343
Epoch 2343 :: Batch 0/1
Batch Loss = 3.0345520269306663
2343, epoch_train_loss=3.0345520269306663
Epoch 2344
Epoch 2344 :: Batch 0/1
Batch Loss = 3.032496176349156
2344, epoch_train_loss=3.032496176349156
Epoch 2345
Epoch 2345 :: Batch 0/1
Batch Loss = 3.0296614238275428
2345, epoch_train_loss=3.0296614238275428
Epoch 2346
Epoch 2346 :: Batch 0/1
Batch Loss = 3.027082168190506
2346, epoch_train_loss=3.027082168190506
Epoch 2347
Epoch 2347 :: Batch 0/1
Batch Loss = 3.092617994325547
2347, epoch_train_loss=3.092617994325547
Epoch 2348
Epoch 2348 :: Batch 0/1
Batch Loss = 3.0639595472164802
2348, epoch_train_loss=3.0639595472164802
Epoch 2349
Epoch 2349 :: Batch 0/1
Batch Loss = 3.075924418937182
2349, epoch_train_loss=3.075924418937182
Epoch 2350
Epoch 2350 :: Batch 0/1
Batch Loss = 3.0781055208773513
2350, epoch_train_loss=3.0781055208773513
Epoch 2351
Epoch 2351 :: Batch 0/1
Batch Loss = 3.0780193093748807
2351, epoch_train_loss=3.0780193093748807
Epoch 2352
Epoch 2352 :: Batch 0/1
Batch Loss = 3.0747067716265497
2352, epoch_train_loss=3.0747067716265497
Epoch 2353
Epoch 2353 :: Batch 0/1
Batch Loss = 3.070422347465826
2353, epoch_train_loss=3.070422347465826
Epoch 2354
Epoch 2354 :: Batch 0/1
Batch Loss = 3.068243176272896
2354, epoch_train_loss=3.068243176272896
Epoch 2355
Epoch 2355 :: Batch 0/1
Batch Loss = 3.064850321834581
2355, epoch_train_loss=3.064850321834581
Epoch 2356
Epoch 2356 :: Batch 0/1
Batch Loss = 3.058897505560064
2356, epoch_train_loss=3.058897505560064
Epoch 2357
Epoch 2357 :: Batch 0/1
Batch Loss = 3.771354814498769
2357, epoch_train_loss=3.771354814498769
Epoch 2358
Epoch 2358 :: Batch 0/1
Batch Loss = 3.6964321466346717
2358, epoch_train_loss=3.6964321466346717
Epoch 2359
Epoch 2359 :: Batch 0/1
Batch Loss = 3.785342166450021
2359, epoch_train_loss=3.785342166450021
Epoch 2360
Epoch 2360 :: Batch 0/1
Batch Loss = 3.756460274240352
2360, epoch_train_loss=3.756460274240352
Epoch 2361
Epoch 2361 :: Batch 0/1
Batch Loss = 3.7423387388270615
2361, epoch_train_loss=3.7423387388270615
Epoch 2362
Epoch 2362 :: Batch 0/1
Batch Loss = 3.739474797982982
2362, epoch_train_loss=3.739474797982982
Epoch 2363
Epoch 2363 :: Batch 0/1
Batch Loss = 3.7382787635987285
2363, epoch_train_loss=3.7382787635987285
Epoch 2364
Epoch 2364 :: Batch 0/1
Batch Loss = 3.737043271774957
2364, epoch_train_loss=3.737043271774957
Epoch 2365
Epoch 2365 :: Batch 0/1
Batch Loss = 3.731403704002526
2365, epoch_train_loss=3.731403704002526
Epoch 2366
Epoch 2366 :: Batch 0/1
Batch Loss = 3.721274622763186
2366, epoch_train_loss=3.721274622763186
Epoch 2367
Epoch 2367 :: Batch 0/1
Batch Loss = 3.70407507235771
2367, epoch_train_loss=3.70407507235771
Epoch 2368
Epoch 2368 :: Batch 0/1
Batch Loss = 3.6792445604525708
2368, epoch_train_loss=3.6792445604525708
Epoch 2369
Epoch 2369 :: Batch 0/1
Batch Loss = 3.651083196884065
2369, epoch_train_loss=3.651083196884065
Epoch 2370
Epoch 2370 :: Batch 0/1
Batch Loss = 3.6254840135782254
2370, epoch_train_loss=3.6254840135782254
Epoch 2371
Epoch 2371 :: Batch 0/1
Batch Loss = 3.6042206697569714
2371, epoch_train_loss=3.6042206697569714
Epoch 2372
Epoch 2372 :: Batch 0/1
Batch Loss = 3.5877051438121295
2372, epoch_train_loss=3.5877051438121295
Epoch 2373
Epoch 2373 :: Batch 0/1
Batch Loss = 3.5755017077997895
2373, epoch_train_loss=3.5755017077997895
Epoch 2374
Epoch 2374 :: Batch 0/1
Batch Loss = 3.5637052485007734
2374, epoch_train_loss=3.5637052485007734
Epoch 2375
Epoch 2375 :: Batch 0/1
Batch Loss = 3.5530887058235296
2375, epoch_train_loss=3.5530887058235296
Epoch 2376
Epoch 2376 :: Batch 0/1
Batch Loss = 3.540499940110433
2376, epoch_train_loss=3.540499940110433
Epoch 2377
Epoch 2377 :: Batch 0/1
Batch Loss = 3.528136663465841
2377, epoch_train_loss=3.528136663465841
Epoch 2378
Epoch 2378 :: Batch 0/1
Batch Loss = 3.5162908801299557
2378, epoch_train_loss=3.5162908801299557
Epoch 2379
Epoch 2379 :: Batch 0/1
Batch Loss = 3.492654559648313
2379, epoch_train_loss=3.492654559648313
Epoch 2380
Epoch 2380 :: Batch 0/1
Batch Loss = 6.645222774306775
2380, epoch_train_loss=6.645222774306775
Epoch 2381
Epoch 2381 :: Batch 0/1
Batch Loss = 3.5096864743270975
2381, epoch_train_loss=3.5096864743270975
Epoch 2382
Epoch 2382 :: Batch 0/1
Batch Loss = 3.5028281488056328
2382, epoch_train_loss=3.5028281488056328
Epoch 2383
Epoch 2383 :: Batch 0/1
Batch Loss = 3.4940185752818347
2383, epoch_train_loss=3.4940185752818347
Epoch 2384
Epoch 2384 :: Batch 0/1
Batch Loss = 3.4838513797366573
2384, epoch_train_loss=3.4838513797366573
Epoch 2385
Epoch 2385 :: Batch 0/1
Batch Loss = 3.4928309455087025
2385, epoch_train_loss=3.4928309455087025
Epoch 2386
Epoch 2386 :: Batch 0/1
Batch Loss = 3.484427192303308
2386, epoch_train_loss=3.484427192303308
Epoch 2387
Epoch 2387 :: Batch 0/1
Batch Loss = 3.4850302997298432
2387, epoch_train_loss=3.4850302997298432
Epoch 2388
Epoch 2388 :: Batch 0/1
Batch Loss = 3.4812785641006836
2388, epoch_train_loss=3.4812785641006836
Epoch 2389
Epoch 2389 :: Batch 0/1
Batch Loss = 3.4765577588185366
2389, epoch_train_loss=3.4765577588185366
Epoch 2390
Epoch 2390 :: Batch 0/1
Batch Loss = 3.471983148843435
2390, epoch_train_loss=3.471983148843435
Epoch 2391
Epoch 2391 :: Batch 0/1
Batch Loss = 3.4681765051074196
2391, epoch_train_loss=3.4681765051074196
Epoch 2392
Epoch 2392 :: Batch 0/1
Batch Loss = 3.4641515488669636
2392, epoch_train_loss=3.4641515488669636
Epoch 2393
Epoch 2393 :: Batch 0/1
Batch Loss = 3.4591071029830363
2393, epoch_train_loss=3.4591071029830363
Epoch 2394
Epoch 2394 :: Batch 0/1
Batch Loss = 3.456494939625504
2394, epoch_train_loss=3.456494939625504
Epoch 2395
Epoch 2395 :: Batch 0/1
Batch Loss = 3.457831294748196
2395, epoch_train_loss=3.457831294748196
Epoch 2396
Epoch 2396 :: Batch 0/1
Batch Loss = 3.489798505887384
2396, epoch_train_loss=3.489798505887384
Epoch 2397
Epoch 2397 :: Batch 0/1
Batch Loss = 3.7700935446329424
2397, epoch_train_loss=3.7700935446329424
Epoch 2398
Epoch 2398 :: Batch 0/1
Batch Loss = 3.8917683458190906
2398, epoch_train_loss=3.8917683458190906
Epoch 2399
Epoch 2399 :: Batch 0/1
Batch Loss = 3.4670173725819575
2399, epoch_train_loss=3.4670173725819575
Epoch 2400
Epoch 2400 :: Batch 0/1
Batch Loss = 3.483287851294216
2400, epoch_train_loss=3.483287851294216
Epoch 2401
Epoch 2401 :: Batch 0/1
Batch Loss = 3.5254313199618985
2401, epoch_train_loss=3.5254313199618985
Epoch 2402
Epoch 2402 :: Batch 0/1
Batch Loss = 3.5417672079348677
2402, epoch_train_loss=3.5417672079348677
Epoch 2403
Epoch 2403 :: Batch 0/1
Batch Loss = 3.5392612077537824
2403, epoch_train_loss=3.5392612077537824
Epoch 2404
Epoch 2404 :: Batch 0/1
Batch Loss = 3.5363984815271987
2404, epoch_train_loss=3.5363984815271987
Epoch 2405
Epoch 2405 :: Batch 0/1
Batch Loss = 3.521739454877358
2405, epoch_train_loss=3.521739454877358
Epoch 2406
Epoch 2406 :: Batch 0/1
Batch Loss = 3.504612381175788
2406, epoch_train_loss=3.504612381175788
Epoch 2407
Epoch 2407 :: Batch 0/1
Batch Loss = 3.5035927284188197
2407, epoch_train_loss=3.5035927284188197
Epoch 2408
Epoch 2408 :: Batch 0/1
Batch Loss = 3.5134105675479037
2408, epoch_train_loss=3.5134105675479037
Epoch 2409
Epoch 2409 :: Batch 0/1
Batch Loss = 3.5139980318820765
2409, epoch_train_loss=3.5139980318820765
Epoch 2410
Epoch 2410 :: Batch 0/1
Batch Loss = 3.5147153173274313
2410, epoch_train_loss=3.5147153173274313
Epoch 2411
Epoch 2411 :: Batch 0/1
Batch Loss = 3.5157430579736566
2411, epoch_train_loss=3.5157430579736566
Epoch 2412
Epoch 2412 :: Batch 0/1
Batch Loss = 3.5149030950594278
2412, epoch_train_loss=3.5149030950594278
Epoch 2413
Epoch 2413 :: Batch 0/1
Batch Loss = 3.5107654083215247
2413, epoch_train_loss=3.5107654083215247
Epoch 2414
Epoch 2414 :: Batch 0/1
Batch Loss = 3.5038248891828476
2414, epoch_train_loss=3.5038248891828476
Epoch 2415
Epoch 2415 :: Batch 0/1
Batch Loss = 3.494140198779866
2415, epoch_train_loss=3.494140198779866
Epoch 2416
Epoch 2416 :: Batch 0/1
Batch Loss = 3.485102917950933
2416, epoch_train_loss=3.485102917950933
Epoch 2417
Epoch 2417 :: Batch 0/1
Batch Loss = 3.48176947609074
2417, epoch_train_loss=3.48176947609074
Epoch 2418
Epoch 2418 :: Batch 0/1
Batch Loss = 3.4827238218327503
2418, epoch_train_loss=3.4827238218327503
Epoch 2419
Epoch 2419 :: Batch 0/1
Batch Loss = 3.48472938549628
2419, epoch_train_loss=3.48472938549628
Epoch 2420
Epoch 2420 :: Batch 0/1
Batch Loss = 3.4852673893173143
2420, epoch_train_loss=3.4852673893173143
Epoch 2421
Epoch 2421 :: Batch 0/1
Batch Loss = 3.4833719644365946
2421, epoch_train_loss=3.4833719644365946
Epoch 2422
Epoch 2422 :: Batch 0/1
Batch Loss = 3.4791819870320695
2422, epoch_train_loss=3.4791819870320695
Epoch 2423
Epoch 2423 :: Batch 0/1
Batch Loss = 3.473881523942639
2423, epoch_train_loss=3.473881523942639
Epoch 2424
Epoch 2424 :: Batch 0/1
Batch Loss = 3.4691428902644277
2424, epoch_train_loss=3.4691428902644277
Epoch 2425
Epoch 2425 :: Batch 0/1
Batch Loss = 3.466071733285688
2425, epoch_train_loss=3.466071733285688
Epoch 2426
Epoch 2426 :: Batch 0/1
Batch Loss = 3.464465248925455
2426, epoch_train_loss=3.464465248925455
Epoch 2427
Epoch 2427 :: Batch 0/1
Batch Loss = 3.4635744926111265
2427, epoch_train_loss=3.4635744926111265
Epoch 2428
Epoch 2428 :: Batch 0/1
Batch Loss = 3.4626186344390906
2428, epoch_train_loss=3.4626186344390906
Epoch 2429
Epoch 2429 :: Batch 0/1
Batch Loss = 3.461068949214421
2429, epoch_train_loss=3.461068949214421
Epoch 2430
Epoch 2430 :: Batch 0/1
Batch Loss = 3.458825847737396
2430, epoch_train_loss=3.458825847737396
Epoch 2431
Epoch 2431 :: Batch 0/1
Batch Loss = 3.456141701495477
2431, epoch_train_loss=3.456141701495477
Epoch 2432
Epoch 2432 :: Batch 0/1
Batch Loss = 3.453444952901016
2432, epoch_train_loss=3.453444952901016
Epoch 2433
Epoch 2433 :: Batch 0/1
Batch Loss = 3.4509550943634575
2433, epoch_train_loss=3.4509550943634575
Epoch 2434
Epoch 2434 :: Batch 0/1
Batch Loss = 3.448896147302656
2434, epoch_train_loss=3.448896147302656
Epoch 2435
Epoch 2435 :: Batch 0/1
Batch Loss = 3.4473375377473063
2435, epoch_train_loss=3.4473375377473063
Epoch 2436
Epoch 2436 :: Batch 0/1
Batch Loss = 3.4459548001361067
2436, epoch_train_loss=3.4459548001361067
Epoch 2437
Epoch 2437 :: Batch 0/1
Batch Loss = 3.444347331661417
2437, epoch_train_loss=3.444347331661417
Epoch 2438
Epoch 2438 :: Batch 0/1
Batch Loss = 3.442471924912624
2438, epoch_train_loss=3.442471924912624
Epoch 2439
Epoch 2439 :: Batch 0/1
Batch Loss = 3.440390689891881
2439, epoch_train_loss=3.440390689891881
Epoch 2440
Epoch 2440 :: Batch 0/1
Batch Loss = 3.438267162033691
2440, epoch_train_loss=3.438267162033691
Epoch 2441
Epoch 2441 :: Batch 0/1
Batch Loss = 3.4362371144680486
2441, epoch_train_loss=3.4362371144680486
Epoch 2442
Epoch 2442 :: Batch 0/1
Batch Loss = 3.434447651337986
2442, epoch_train_loss=3.434447651337986
Epoch 2443
Epoch 2443 :: Batch 0/1
Batch Loss = 3.4328808730000113
2443, epoch_train_loss=3.4328808730000113
Epoch 2444
Epoch 2444 :: Batch 0/1
Batch Loss = 3.4314846816651086
2444, epoch_train_loss=3.4314846816651086
Epoch 2445
Epoch 2445 :: Batch 0/1
Batch Loss = 3.4301238551888766
2445, epoch_train_loss=3.4301238551888766
Epoch 2446
Epoch 2446 :: Batch 0/1
Batch Loss = 3.428664943354429
2446, epoch_train_loss=3.428664943354429
Epoch 2447
Epoch 2447 :: Batch 0/1
Batch Loss = 3.4269949608464936
2447, epoch_train_loss=3.4269949608464936
Epoch 2448
Epoch 2448 :: Batch 0/1
Batch Loss = 3.4252322360399057
2448, epoch_train_loss=3.4252322360399057
Epoch 2449
Epoch 2449 :: Batch 0/1
Batch Loss = 3.423464496334617
2449, epoch_train_loss=3.423464496334617
Epoch 2450
Epoch 2450 :: Batch 0/1
Batch Loss = 3.4217563308586145
2450, epoch_train_loss=3.4217563308586145
Epoch 2451
Epoch 2451 :: Batch 0/1
Batch Loss = 3.420108678632131
2451, epoch_train_loss=3.420108678632131
Epoch 2452
Epoch 2452 :: Batch 0/1
Batch Loss = 3.41859129894267
2452, epoch_train_loss=3.41859129894267
Epoch 2453
Epoch 2453 :: Batch 0/1
Batch Loss = 3.41713921613436
2453, epoch_train_loss=3.41713921613436
Epoch 2454
Epoch 2454 :: Batch 0/1
Batch Loss = 3.4156948350338605
2454, epoch_train_loss=3.4156948350338605
Epoch 2455
Epoch 2455 :: Batch 0/1
Batch Loss = 3.41422981499139
2455, epoch_train_loss=3.41422981499139
Epoch 2456
Epoch 2456 :: Batch 0/1
Batch Loss = 3.412748566010179
2456, epoch_train_loss=3.412748566010179
Epoch 2457
Epoch 2457 :: Batch 0/1
Batch Loss = 3.4112512380750917
2457, epoch_train_loss=3.4112512380750917
Epoch 2458
Epoch 2458 :: Batch 0/1
Batch Loss = 3.4097742706665177
2458, epoch_train_loss=3.4097742706665177
Epoch 2459
Epoch 2459 :: Batch 0/1
Batch Loss = 3.408407684603883
2459, epoch_train_loss=3.408407684603883
Epoch 2460
Epoch 2460 :: Batch 0/1
Batch Loss = 3.4070663171294875
2460, epoch_train_loss=3.4070663171294875
Epoch 2461
Epoch 2461 :: Batch 0/1
Batch Loss = 3.4057446664097375
2461, epoch_train_loss=3.4057446664097375
Epoch 2462
Epoch 2462 :: Batch 0/1
Batch Loss = 3.4044247637236555
2462, epoch_train_loss=3.4044247637236555
Epoch 2463
Epoch 2463 :: Batch 0/1
Batch Loss = 3.4031138683682287
2463, epoch_train_loss=3.4031138683682287
Epoch 2464
Epoch 2464 :: Batch 0/1
Batch Loss = 3.401791233753592
2464, epoch_train_loss=3.401791233753592
Epoch 2465
Epoch 2465 :: Batch 0/1
Batch Loss = 3.4004828637204656
2465, epoch_train_loss=3.4004828637204656
Epoch 2466
Epoch 2466 :: Batch 0/1
Batch Loss = 3.399151841727073
2466, epoch_train_loss=3.399151841727073
Epoch 2467
Epoch 2467 :: Batch 0/1
Batch Loss = 3.3978495045184647
2467, epoch_train_loss=3.3978495045184647
Epoch 2468
Epoch 2468 :: Batch 0/1
Batch Loss = 3.3966124384066148
2468, epoch_train_loss=3.3966124384066148
Epoch 2469
Epoch 2469 :: Batch 0/1
Batch Loss = 3.3953960301307737
2469, epoch_train_loss=3.3953960301307737
Epoch 2470
Epoch 2470 :: Batch 0/1
Batch Loss = 3.394198805911677
2470, epoch_train_loss=3.394198805911677
Epoch 2471
Epoch 2471 :: Batch 0/1
Batch Loss = 3.3929960488746107
2471, epoch_train_loss=3.3929960488746107
Epoch 2472
Epoch 2472 :: Batch 0/1
Batch Loss = 3.391776250593915
2472, epoch_train_loss=3.391776250593915
Epoch 2473
Epoch 2473 :: Batch 0/1
Batch Loss = 3.390566533332521
2473, epoch_train_loss=3.390566533332521
Epoch 2474
Epoch 2474 :: Batch 0/1
Batch Loss = 3.389393183743293
2474, epoch_train_loss=3.389393183743293
Epoch 2475
Epoch 2475 :: Batch 0/1
Batch Loss = 3.388268360997624
2475, epoch_train_loss=3.388268360997624
Epoch 2476
Epoch 2476 :: Batch 0/1
Batch Loss = 3.387172627853687
2476, epoch_train_loss=3.387172627853687
Epoch 2477
Epoch 2477 :: Batch 0/1
Batch Loss = 3.386101730740721
2477, epoch_train_loss=3.386101730740721
Epoch 2478
Epoch 2478 :: Batch 0/1
Batch Loss = 3.3850405086298805
2478, epoch_train_loss=3.3850405086298805
Epoch 2479
Epoch 2479 :: Batch 0/1
Batch Loss = 3.38397445226062
2479, epoch_train_loss=3.38397445226062
Epoch 2480
Epoch 2480 :: Batch 0/1
Batch Loss = 3.3829160025385776
2480, epoch_train_loss=3.3829160025385776
Epoch 2481
Epoch 2481 :: Batch 0/1
Batch Loss = 3.381832788507461
2481, epoch_train_loss=3.381832788507461
Epoch 2482
Epoch 2482 :: Batch 0/1
Batch Loss = 3.3807255059152284
2482, epoch_train_loss=3.3807255059152284
Epoch 2483
Epoch 2483 :: Batch 0/1
Batch Loss = 3.3795777369994258
2483, epoch_train_loss=3.3795777369994258
Epoch 2484
Epoch 2484 :: Batch 0/1
Batch Loss = 3.378421057106975
2484, epoch_train_loss=3.378421057106975
Epoch 2485
Epoch 2485 :: Batch 0/1
Batch Loss = 3.3772510413349353
2485, epoch_train_loss=3.3772510413349353
Epoch 2486
Epoch 2486 :: Batch 0/1
Batch Loss = 3.3761008936360413
2486, epoch_train_loss=3.3761008936360413
Epoch 2487
Epoch 2487 :: Batch 0/1
Batch Loss = 3.374966277544708
2487, epoch_train_loss=3.374966277544708
Epoch 2488
Epoch 2488 :: Batch 0/1
Batch Loss = 3.3738546527145274
2488, epoch_train_loss=3.3738546527145274
Epoch 2489
Epoch 2489 :: Batch 0/1
Batch Loss = 3.3727780739515216
2489, epoch_train_loss=3.3727780739515216
Epoch 2490
Epoch 2490 :: Batch 0/1
Batch Loss = 3.371700577258213
2490, epoch_train_loss=3.371700577258213
Epoch 2491
Epoch 2491 :: Batch 0/1
Batch Loss = 3.37061672635764
2491, epoch_train_loss=3.37061672635764
Epoch 2492
Epoch 2492 :: Batch 0/1
Batch Loss = 3.369517604802585
2492, epoch_train_loss=3.369517604802585
Epoch 2493
Epoch 2493 :: Batch 0/1
Batch Loss = 3.3684425774773823
2493, epoch_train_loss=3.3684425774773823
Epoch 2494
Epoch 2494 :: Batch 0/1
Batch Loss = 3.367386340771009
2494, epoch_train_loss=3.367386340771009
Epoch 2495
Epoch 2495 :: Batch 0/1
Batch Loss = 3.3663243278475363
2495, epoch_train_loss=3.3663243278475363
Epoch 2496
Epoch 2496 :: Batch 0/1
Batch Loss = 3.365247318280369
2496, epoch_train_loss=3.365247318280369
Epoch 2497
Epoch 2497 :: Batch 0/1
Batch Loss = 3.364148169310152
2497, epoch_train_loss=3.364148169310152
Epoch 2498
Epoch 2498 :: Batch 0/1
Batch Loss = 3.363078332806685
2498, epoch_train_loss=3.363078332806685
Epoch 2499
Epoch 2499 :: Batch 0/1
Batch Loss = 3.362035518933991
2499, epoch_train_loss=3.362035518933991
