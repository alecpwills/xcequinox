/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting "B3LYP_WITH_VWN5 = True" in pyscf_conf.py
  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e9750> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e9750> in UKS object of <class 'pyscf.dft.uks.UKS'>
<pyscf.gto.mole.Mole object at 0x7ffeac1e9750> [['P', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac1e8e20> [['N', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac1ea260> [['H', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac1e95a0> [['Li', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac1e80d0> [['O', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac1ea200> [['Cl', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac1ebbb0> [['Al', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac1e9210> [['S', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac1ea410> [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac1eb8b0> [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeac1eb4c0> [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeac1ebac0> [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac1eb4f0> [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac1e9d80> [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac1e8af0> [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeac1e9360> [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac1e88e0> [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac1ebe50> [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac1ebe20> [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac1e8100> [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeac1e8910> [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeac1ea380> [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac07ded0> [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]] 5
<pyscf.gto.mole.Mole object at 0x7ffeac07dcc0> [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeac07e350> [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]] 5
mol:  [['P', array([0., 0., 0.])]]
converged SCF energy = -341.104145992718  <S^2> = 3.7502984  2S+1 = 4.0001492
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e8e20> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e8e20> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-3.48053232e-03 -9.22981806e-04 -2.09507924e-03 ... -1.11294850e+01
 -1.11294850e+01 -1.11294850e+01] = SCAN,
rho_a.shape=(6, 6320), rho_b.shape=(6, 6320)
fxc_a.shape=(6320,), fxc_b.shape=(6320,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12640), fxc.shape=(12640,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(2, 6320, 3)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12640, 3)
rho_filt.shape=(12640,)
mol:  [['N', array([0., 0., 0.])]]
converged SCF energy = -54.5289742046675  <S^2> = 3.7524945  2S+1 = 4.0012471
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1ea260> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1ea260> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5016, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.10256797e-03 -5.98013179e-04 -6.71209617e-05 ... -5.03581543e+00
 -5.03581543e+00 -5.03581543e+00] = SCAN,
rho_a.shape=(6, 5016), rho_b.shape=(6, 5016)
fxc_a.shape=(5016,), fxc_b.shape=(5016,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 10032), fxc.shape=(10032,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5016, 22)
rho shape (4, 4, 2, 5016)
rho_filt shape: (5016,)
get descriptors tdrho.shape=(2, 5016, 3)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(10032, 3)
rho_filt.shape=(10032,)
mol:  [['H', array([0., 0., 0.])]]
converged SCF energy = -0.499812984008539  <S^2> = 0.75  2S+1 = 2
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e95a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e95a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 7, 7)
ao.shape (10, 2440, 7)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.59173730e-03 -7.67300285e-04 -6.09330303e-05 ... -7.27736608e-01
 -7.27736608e-01 -7.27736608e-01] = SCAN,
rho_a.shape=(6, 2440), rho_b.shape=(6, 2440)
fxc_a.shape=(2440,), fxc_b.shape=(2440,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 2440), fxc.shape=(2440,)
get_data, dm shape = (2, 7, 7)
ao_eval.shape=(4, 2440, 7)
rho shape (4, 4, 2, 2440)
rho_filt shape: (2440,)
get descriptors tdrho.shape=(2, 2440, 3)
mol:  [['Li', array([0., 0., 0.])]]
converged SCF energy = -7.46006188627841  <S^2> = 0.75000049  2S+1 = 2.0000005
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e80d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e80d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 4592, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-6.71507910e-03 -1.45299376e-03 -1.45299376e-03 ... -1.46930969e-02
 -2.05021258e+00 -2.05021258e+00] = SCAN,
rho_a.shape=(6, 4592), rho_b.shape=(6, 4592)
fxc_a.shape=(4592,), fxc_b.shape=(4592,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 9184), fxc.shape=(9184,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 4592, 22)
rho shape (4, 4, 2, 4592)
rho_filt shape: (4592,)
get descriptors tdrho.shape=(2, 4592, 3)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(9184, 3)
rho_filt.shape=(9184,)
mol:  [['O', array([0., 0., 0.])]]
converged SCF energy = -75.0033774427828  <S^2> = 2.0027452  2S+1 = 3.0018296
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1ea200> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1ea200> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5040, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-7.09691652e-04 -1.23398274e-04 -6.19401081e-06 ... -5.78388655e+00
 -5.78388655e+00 -5.78388655e+00] = SCAN,
rho_a.shape=(6, 5040), rho_b.shape=(6, 5040)
fxc_a.shape=(5040,), fxc_b.shape=(5040,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 10080), fxc.shape=(10080,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5040, 22)
rho shape (4, 4, 2, 5040)
rho_filt shape: (5040,)
get descriptors tdrho.shape=(2, 5040, 3)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(10080, 3)
rho_filt.shape=(10080,)
mol:  [['Cl', array([0., 0., 0.])]]
converged SCF energy = -459.957577121336  <S^2> = 0.75161941  2S+1 = 2.0016188
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1ebbb0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1ebbb0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6152, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-9.44782437e-04 -1.01991647e-03 -3.62319459e-04 ... -1.26646370e+01
 -1.26646370e+01 -1.26646370e+01] = SCAN,
rho_a.shape=(6, 6152), rho_b.shape=(6, 6152)
fxc_a.shape=(6152,), fxc_b.shape=(6152,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12304), fxc.shape=(12304,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6152, 30)
rho shape (4, 4, 2, 6152)
rho_filt shape: (6152,)
get descriptors tdrho.shape=(2, 6152, 3)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12304, 3)
rho_filt.shape=(12304,)
mol:  [['Al', array([0., 0., 0.])]]
converged SCF energy = -242.22656098928  <S^2> = 0.75226414  2S+1 = 2.0022629
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e9210> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e9210> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6088, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.39566757e-02 -8.69603390e-03 -4.30180232e-03 ... -1.39784131e-04
 -1.04894578e-03 -7.75317699e-05] = SCAN,
rho_a.shape=(6, 6088), rho_b.shape=(6, 6088)
fxc_a.shape=(6088,), fxc_b.shape=(6088,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12176), fxc.shape=(12176,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6088, 30)
rho shape (4, 4, 2, 6088)
rho_filt shape: (6088,)
get descriptors tdrho.shape=(2, 6088, 3)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12176, 3)
rho_filt.shape=(12176,)
mol:  [['S', array([0., 0., 0.])]]
converged SCF energy = -397.938786807091  <S^2> = 2.0022329  2S+1 = 3.0014882
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1ea410> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1ea410> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.43010404e-03 -7.97631460e-04 -9.75922385e-04 ... -1.18982463e+01
 -1.18982463e+01 -1.18982463e+01] = SCAN,
rho_a.shape=(6, 6320), rho_b.shape=(6, 6320)
fxc_a.shape=(6320,), fxc_b.shape=(6320,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12640), fxc.shape=(12640,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(2, 6320, 3)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12640, 3)
rho_filt.shape=(12640,)
mol:  [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]]
converged SCF energy = -107.339357395734  <S^2> = 7.1054274e-15  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1eb8b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1eb8b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9752, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.04987750e-03 -6.68953858e-04 -8.57556270e-04 ... -1.07485583e-03
 -8.01425698e-01 -8.01425698e-01] = SCAN,
rho_a.shape=(6, 9752), rho_b.shape=(6, 9752)
fxc_a.shape=(9752,), fxc_b.shape=(9752,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9752), fxc.shape=(9752,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9752, 44)
rho shape (4, 4, 2, 9752)
rho_filt shape: (9752,)
get descriptors tdrho.shape=(2, 9752, 3)
mol:  [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]]
converged SCF energy = -93.3377924465131  <S^2> = 4.0073012e-10  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1eb4c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1eb4c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 12256, 51)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.97917285e-04 -2.54412366e-05 -3.15182243e-05 ... -6.37386500e-01
 -6.37386500e-01 -6.37386500e-01] = SCAN,
rho_a.shape=(6, 12256), rho_b.shape=(6, 12256)
fxc_a.shape=(12256,), fxc_b.shape=(12256,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12256), fxc.shape=(12256,)
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 12256, 51)
rho shape (4, 4, 2, 12256)
rho_filt shape: (12256,)
get descriptors tdrho.shape=(2, 12256, 3)
mol:  [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]]
converged SCF energy = -188.456965322844  <S^2> = 1.7763568e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1ebac0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1ebac0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 66, 66)
ao.shape (10, 14920, 66)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.50217115e-04 -2.07520066e-04 -9.23619896e-04 ... -2.74295208e-06
 -4.27559894e+00 -4.27559894e+00] = SCAN,
rho_a.shape=(6, 14920), rho_b.shape=(6, 14920)
fxc_a.shape=(14920,), fxc_b.shape=(14920,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 14920), fxc.shape=(14920,)
get_data, dm shape = (2, 66, 66)
ao_eval.shape=(4, 14920, 66)
rho shape (4, 4, 2, 14920)
rho_filt shape: (14920,)
get descriptors tdrho.shape=(2, 14920, 3)
mol:  [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]]
converged SCF energy = -920.005608888959  <S^2> = 5.0803806e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1eb4f0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1eb4f0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12208, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00155834 -0.00091688 -0.00215831 ... -0.00091688 -0.41618507
 -0.41618507] = SCAN,
rho_a.shape=(6, 12208), rho_b.shape=(6, 12208)
fxc_a.shape=(12208,), fxc_b.shape=(12208,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12208), fxc.shape=(12208,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12208, 60)
rho shape (4, 4, 2, 12208)
rho_filt shape: (12208,)
get descriptors tdrho.shape=(2, 12208, 3)
mol:  [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]]
converged SCF energy = -199.394370591172  <S^2> = 1.2079227e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e9d80> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e9d80> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9824, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.92948614e-04 -1.95198688e-05 -1.16699802e-03 ... -4.89378340e-01
 -4.89378340e-01 -4.89378340e-01] = SCAN,
rho_a.shape=(6, 9824), rho_b.shape=(6, 9824)
fxc_a.shape=(9824,), fxc_b.shape=(9824,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9824), fxc.shape=(9824,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9824, 44)
rho shape (4, 4, 2, 9824)
rho_filt shape: (9824,)
get descriptors tdrho.shape=(2, 9824, 3)
mol:  [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]]
converged SCF energy = -150.214894551844  <S^2> = 1.0018599  2S+1 = 2.2377309
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e8af0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e8af0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9912, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-5.64370031e-04 -9.92738314e-05 -5.07343544e-06 ... -6.59150586e-01
 -6.59150586e-01 -6.59150586e-01] = SCAN,
rho_a.shape=(6, 9912), rho_b.shape=(6, 9912)
fxc_a.shape=(9912,), fxc_b.shape=(9912,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9912), fxc.shape=(9912,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9912, 44)
rho shape (4, 4, 2, 9912)
rho_filt shape: (9912,)
get descriptors tdrho.shape=(2, 9912, 3)
mol:  [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]]
converged SCF energy = -77.2435048346371  <S^2> = 1.4210855e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e9360> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e9360> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 58, 58)
ao.shape (10, 15208, 58)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.83270456e-05 -8.83270456e-05 -9.75839850e-04 ... -3.46719667e-05
 -3.31708644e-05 -3.31708644e-05] = SCAN,
rho_a.shape=(6, 15208), rho_b.shape=(6, 15208)
fxc_a.shape=(15208,), fxc_b.shape=(15208,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 15208), fxc.shape=(15208,)
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15208, 58)
rho shape (4, 4, 2, 15208)
rho_filt shape: (15208,)
get descriptors tdrho.shape=(2, 15208, 3)
mol:  [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]]
converged SCF energy = -113.221335689652  <S^2> = 6.5636385e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e88e0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e88e0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 10040, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-5.37000578e-04 -8.55494549e-04 -2.46853288e-03 ... -7.34251999e-01
 -7.34251999e-01 -7.34251999e-01] = SCAN,
rho_a.shape=(6, 10040), rho_b.shape=(6, 10040)
fxc_a.shape=(10040,), fxc_b.shape=(10040,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 10040), fxc.shape=(10040,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 10040, 44)
rho shape (4, 4, 2, 10040)
rho_filt shape: (10040,)
get descriptors tdrho.shape=(2, 10040, 3)
mol:  [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]]
converged SCF energy = -460.624592374078  <S^2> = 7.4606987e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1ebe50> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1ebe50> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 37, 37)
ao.shape (10, 8552, 37)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.38161177e-04 -1.81188367e-05 -2.37300299e-05 ... -2.83738108e+00
 -2.83738108e+00 -2.83738108e+00] = SCAN,
rho_a.shape=(6, 8552), rho_b.shape=(6, 8552)
fxc_a.shape=(8552,), fxc_b.shape=(8552,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 8552), fxc.shape=(8552,)
get_data, dm shape = (2, 37, 37)
ao_eval.shape=(4, 8552, 37)
rho shape (4, 4, 2, 8552)
rho_filt shape: (8552,)
get descriptors tdrho.shape=(2, 8552, 3)
mol:  [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]]
converged SCF energy = -8.04458854018922  <S^2> = 7.4606987e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1ebe20> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1ebe20> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 29, 29)
ao.shape (10, 6936, 29)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00113445 -0.00118624 -0.00118624 ... -0.48434639 -0.48434639
 -0.48434639] = SCAN,
rho_a.shape=(6, 6936), rho_b.shape=(6, 6936)
fxc_a.shape=(6936,), fxc_b.shape=(6936,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 6936), fxc.shape=(6936,)
get_data, dm shape = (2, 29, 29)
ao_eval.shape=(4, 6936, 29)
rho shape (4, 4, 2, 6936)
rho_filt shape: (6936,)
get descriptors tdrho.shape=(2, 6936, 3)
mol:  [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]]
converged SCF energy = -324.340512506579  <S^2> = 1.5859314e-11  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e8100> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e8100> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 11536, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00297935 -0.00297935 -0.00407089 ... -0.00297935 -0.00297935
 -0.00407089] = SCAN,
rho_a.shape=(6, 11536), rho_b.shape=(6, 11536)
fxc_a.shape=(11536,), fxc_b.shape=(11536,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 11536), fxc.shape=(11536,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 11536, 60)
rho shape (4, 4, 2, 11536)
rho_filt shape: (11536,)
get descriptors tdrho.shape=(2, 11536, 3)
mol:  [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]]
converged SCF energy = -1622.57507845814  <S^2> = 8.3844043e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e8910> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e8910> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 120, 120)
ao.shape (10, 24512, 120)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.61400996e-04 -4.90484900e-04 -2.56451718e-03 ... -9.59296113e+00
 -9.59296113e+00 -9.59296113e+00] = SCAN,
rho_a.shape=(6, 24512), rho_b.shape=(6, 24512)
fxc_a.shape=(24512,), fxc_b.shape=(24512,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 24512), fxc.shape=(24512,)
get_data, dm shape = (2, 120, 120)
ao_eval.shape=(4, 24512, 120)
rho shape (4, 4, 2, 24512)
rho_filt shape: (24512,)
get descriptors tdrho.shape=(2, 24512, 3)
mol:  [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]]
converged SCF energy = -342.979728469576  <S^2> = 2.5391245e-11  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1ea380> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1ea380> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 13096, 51)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.28637920e-03 -4.32383300e-04 -3.74057272e-05 ... -1.91722770e+00
 -1.91722770e+00 -1.91722770e+00] = SCAN,
rho_a.shape=(6, 13096), rho_b.shape=(6, 13096)
fxc_a.shape=(13096,), fxc_b.shape=(13096,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 13096), fxc.shape=(13096,)
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 13096, 51)
rho shape (4, 4, 2, 13096)
rho_filt shape: (13096,)
get descriptors tdrho.shape=(2, 13096, 3)
mol:  [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]]
converged SCF energy = -578.565336125344  <S^2> = 1.0034705  2S+1 = 2.2391699
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac07ded0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac07ded0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12384, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.59505974e-04 -2.59076798e-04 -2.59978780e-04 ... -3.86944047e-01
 -3.86944047e-01 -3.86944047e-01] = SCAN,
rho_a.shape=(6, 12384), rho_b.shape=(6, 12384)
fxc_a.shape=(12384,), fxc_b.shape=(12384,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12384), fxc.shape=(12384,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12384, 60)
rho shape (4, 4, 2, 12384)
rho_filt shape: (12384,)
get descriptors tdrho.shape=(2, 12384, 3)
mol:  [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]]
converged SCF energy = -40.4598214864076  <S^2> = 3.1530334e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac07dcc0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac07dcc0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 50, 50)
ao.shape (10, 13936, 50)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.68439986e-04 -2.42462569e-04 -1.69927031e-05 ... -2.55230307e-05
 -2.55230307e-05 -2.55230307e-05] = SCAN,
rho_a.shape=(6, 13936), rho_b.shape=(6, 13936)
fxc_a.shape=(13936,), fxc_b.shape=(13936,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 13936), fxc.shape=(13936,)
get_data, dm shape = (2, 50, 50)
ao_eval.shape=(4, 13936, 50)
rho shape (4, 4, 2, 13936)
rho_filt shape: (13936,)
get descriptors tdrho.shape=(2, 13936, 3)
mol:  [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]]
converged SCF energy = -39.0756147483505  <S^2> = 6.1932681e-12  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac07e350> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac07e350> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 36, 36)
ao.shape (10, 9656, 36)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-7.67688751e-04 -4.57393214e-05 -2.02834191e-04 ... -1.14928924e+00
 -1.14928924e+00 -1.14928924e+00] = SCAN,
rho_a.shape=(6, 9656), rho_b.shape=(6, 9656)
fxc_a.shape=(9656,), fxc_b.shape=(9656,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9656), fxc.shape=(9656,)
get_data, dm shape = (2, 36, 36)
ao_eval.shape=(4, 9656, 36)
rho shape (4, 4, 2, 9656)
rho_filt shape: (9656,)
get descriptors tdrho.shape=(2, 9656, 3)
mol:  [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]]
converged SCF energy = -291.719272437819  <S^2> = 1.3152146e-11  2S+1 = 1
New DM shape: (2, 58, 58)
ao.shape (10, 15256, 58)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.33850535e-04 -2.34903029e-04 -1.75623665e-05 ... -1.92891112e-05
 -1.92891112e-05 -1.92891112e-05] = SCAN,
rho_a.shape=(6, 15256), rho_b.shape=(6, 15256)
fxc_a.shape=(15256,), fxc_b.shape=(15256,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 15256), fxc.shape=(15256,)
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15256, 58)
rho shape (4, 4, 2, 15256)
rho_filt shape: (15256,)
get descriptors tdrho.shape=(2, 15256, 3)
localnet.spin_scaling: concatenating the data
first data shape = (10940, 3)
concatenated: tdrho.shape=(258865, 3)
PRE NAN FILT: tFxc.shape=(258865,), tdrho.shape=(258865, 3)
nan_filt_rho.shape=(258865,)
nan_filt_fxc.shape=(258865,)
tFxc.shape=(258865,), tdrho.shape=(258865, 3)
inp[0].shape = (258865, 2)
Epoch 0
Epoch 0 :: Batch 0/1
Batch Loss = 5.17777097652608
0, epoch_train_loss=5.17777097652608
Epoch 1
Epoch 1 :: Batch 0/1
Batch Loss = 4.884949382849235
1, epoch_train_loss=4.884949382849235
Epoch 2
Epoch 2 :: Batch 0/1
Batch Loss = 4.663667232123323
2, epoch_train_loss=4.663667232123323
Epoch 3
Epoch 3 :: Batch 0/1
Batch Loss = 4.562867561769796
3, epoch_train_loss=4.562867561769796
Epoch 4
Epoch 4 :: Batch 0/1
Batch Loss = 4.467923416098074
4, epoch_train_loss=4.467923416098074
Epoch 5
Epoch 5 :: Batch 0/1
Batch Loss = 4.282469831031646
5, epoch_train_loss=4.282469831031646
Epoch 6
Epoch 6 :: Batch 0/1
Batch Loss = 4.068319499588049
6, epoch_train_loss=4.068319499588049
Epoch 7
Epoch 7 :: Batch 0/1
Batch Loss = 3.7917351208997947
7, epoch_train_loss=3.7917351208997947
Epoch 8
Epoch 8 :: Batch 0/1
Batch Loss = 3.550574543728887
8, epoch_train_loss=3.550574543728887
Epoch 9
Epoch 9 :: Batch 0/1
Batch Loss = 3.2743281630714756
9, epoch_train_loss=3.2743281630714756
Epoch 10
Epoch 10 :: Batch 0/1
Batch Loss = 3.2690288857434204
10, epoch_train_loss=3.2690288857434204
Epoch 11
Epoch 11 :: Batch 0/1
Batch Loss = 3.146873298839932
11, epoch_train_loss=3.146873298839932
Epoch 12
Epoch 12 :: Batch 0/1
Batch Loss = 3.1910217217099794
12, epoch_train_loss=3.1910217217099794
Epoch 13
Epoch 13 :: Batch 0/1
Batch Loss = 3.0118236326019545
13, epoch_train_loss=3.0118236326019545
Epoch 14
Epoch 14 :: Batch 0/1
Batch Loss = 2.9693703165394525
14, epoch_train_loss=2.9693703165394525
Epoch 15
Epoch 15 :: Batch 0/1
Batch Loss = 2.7839158110064526
15, epoch_train_loss=2.7839158110064526
Epoch 16
Epoch 16 :: Batch 0/1
Batch Loss = 2.914067057312435
16, epoch_train_loss=2.914067057312435
Epoch 17
Epoch 17 :: Batch 0/1
Batch Loss = 2.7335481680509104
17, epoch_train_loss=2.7335481680509104
Epoch 18
Epoch 18 :: Batch 0/1
Batch Loss = 2.758311842486933
18, epoch_train_loss=2.758311842486933
Epoch 19
Epoch 19 :: Batch 0/1
Batch Loss = 2.7373621878281504
19, epoch_train_loss=2.7373621878281504
Epoch 20
Epoch 20 :: Batch 0/1
Batch Loss = 2.7538780286840727
20, epoch_train_loss=2.7538780286840727
Epoch 21
Epoch 21 :: Batch 0/1
Batch Loss = 2.682183050948182
21, epoch_train_loss=2.682183050948182
Epoch 22
Epoch 22 :: Batch 0/1
Batch Loss = 2.7345580305418946
22, epoch_train_loss=2.7345580305418946
Epoch 23
Epoch 23 :: Batch 0/1
Batch Loss = 2.705066685082934
23, epoch_train_loss=2.705066685082934
Epoch 24
Epoch 24 :: Batch 0/1
Batch Loss = 2.705712305390559
24, epoch_train_loss=2.705712305390559
Epoch 25
Epoch 25 :: Batch 0/1
Batch Loss = 2.668391589052495
25, epoch_train_loss=2.668391589052495
Epoch 26
Epoch 26 :: Batch 0/1
Batch Loss = 2.702055279652386
26, epoch_train_loss=2.702055279652386
Epoch 27
Epoch 27 :: Batch 0/1
Batch Loss = 2.676459984966291
27, epoch_train_loss=2.676459984966291
Epoch 28
Epoch 28 :: Batch 0/1
Batch Loss = 2.6864547969242123
28, epoch_train_loss=2.6864547969242123
Epoch 29
Epoch 29 :: Batch 0/1
Batch Loss = 2.6543057785914037
29, epoch_train_loss=2.6543057785914037
Epoch 30
Epoch 30 :: Batch 0/1
Batch Loss = 2.67384091110228
30, epoch_train_loss=2.67384091110228
Epoch 31
Epoch 31 :: Batch 0/1
Batch Loss = 2.661748473811049
31, epoch_train_loss=2.661748473811049
Epoch 32
Epoch 32 :: Batch 0/1
Batch Loss = 2.668986453585336
32, epoch_train_loss=2.668986453585336
Epoch 33
Epoch 33 :: Batch 0/1
Batch Loss = 2.6549102396036273
33, epoch_train_loss=2.6549102396036273
Epoch 34
Epoch 34 :: Batch 0/1
Batch Loss = 2.6565900855195874
34, epoch_train_loss=2.6565900855195874
Epoch 35
Epoch 35 :: Batch 0/1
Batch Loss = 2.6623276104204696
35, epoch_train_loss=2.6623276104204696
Epoch 36
Epoch 36 :: Batch 0/1
Batch Loss = 2.657644960725614
36, epoch_train_loss=2.657644960725614
Epoch 37
Epoch 37 :: Batch 0/1
Batch Loss = 2.657896109087415
37, epoch_train_loss=2.657896109087415
Epoch 38
Epoch 38 :: Batch 0/1
Batch Loss = 2.6471297633422686
38, epoch_train_loss=2.6471297633422686
Epoch 39
Epoch 39 :: Batch 0/1
Batch Loss = 2.654657912820529
39, epoch_train_loss=2.654657912820529
Epoch 40
Epoch 40 :: Batch 0/1
Batch Loss = 2.6487276233905943
40, epoch_train_loss=2.6487276233905943
Epoch 41
Epoch 41 :: Batch 0/1
Batch Loss = 2.651078336830818
41, epoch_train_loss=2.651078336830818
Epoch 42
Epoch 42 :: Batch 0/1
Batch Loss = 2.6428117353712772
42, epoch_train_loss=2.6428117353712772
Epoch 43
Epoch 43 :: Batch 0/1
Batch Loss = 2.6460318507218914
43, epoch_train_loss=2.6460318507218914
Epoch 44
Epoch 44 :: Batch 0/1
Batch Loss = 2.643462718923679
44, epoch_train_loss=2.643462718923679
Epoch 45
Epoch 45 :: Batch 0/1
Batch Loss = 2.6466566568757948
45, epoch_train_loss=2.6466566568757948
Epoch 46
Epoch 46 :: Batch 0/1
Batch Loss = 2.6421606427048956
46, epoch_train_loss=2.6421606427048956
Epoch 47
Epoch 47 :: Batch 0/1
Batch Loss = 2.643349976172039
47, epoch_train_loss=2.643349976172039
Epoch 48
Epoch 48 :: Batch 0/1
Batch Loss = 2.6401524933731593
48, epoch_train_loss=2.6401524933731593
Epoch 49
Epoch 49 :: Batch 0/1
Batch Loss = 2.6436711985515995
49, epoch_train_loss=2.6436711985515995
Epoch 50
Epoch 50 :: Batch 0/1
Batch Loss = 2.6408194085168253
50, epoch_train_loss=2.6408194085168253
Epoch 51
Epoch 51 :: Batch 0/1
Batch Loss = 2.6421771148106297
51, epoch_train_loss=2.6421771148106297
Epoch 52
Epoch 52 :: Batch 0/1
Batch Loss = 2.639140673432803
52, epoch_train_loss=2.639140673432803
Epoch 53
Epoch 53 :: Batch 0/1
Batch Loss = 2.6405560715560226
53, epoch_train_loss=2.6405560715560226
Epoch 54
Epoch 54 :: Batch 0/1
Batch Loss = 2.640036152405527
54, epoch_train_loss=2.640036152405527
Epoch 55
Epoch 55 :: Batch 0/1
Batch Loss = 2.639262542778888
55, epoch_train_loss=2.639262542778888
Epoch 56
Epoch 56 :: Batch 0/1
Batch Loss = 2.6392380735357714
56, epoch_train_loss=2.6392380735357714
Epoch 57
Epoch 57 :: Batch 0/1
Batch Loss = 2.637356231879728
57, epoch_train_loss=2.637356231879728
Epoch 58
Epoch 58 :: Batch 0/1
Batch Loss = 2.6387877673241666
58, epoch_train_loss=2.6387877673241666
Epoch 59
Epoch 59 :: Batch 0/1
Batch Loss = 2.637740314272963
59, epoch_train_loss=2.637740314272963
Epoch 60
Epoch 60 :: Batch 0/1
Batch Loss = 2.637645837836585
60, epoch_train_loss=2.637645837836585
Epoch 61
Epoch 61 :: Batch 0/1
Batch Loss = 2.6374119654710433
61, epoch_train_loss=2.6374119654710433
Epoch 62
Epoch 62 :: Batch 0/1
Batch Loss = 2.636462731189404
62, epoch_train_loss=2.636462731189404
Epoch 63
Epoch 63 :: Batch 0/1
Batch Loss = 2.637343706997577
63, epoch_train_loss=2.637343706997577
Epoch 64
Epoch 64 :: Batch 0/1
Batch Loss = 2.6366259298351147
64, epoch_train_loss=2.6366259298351147
Epoch 65
Epoch 65 :: Batch 0/1
Batch Loss = 2.6360448968401453
65, epoch_train_loss=2.6360448968401453
Epoch 66
Epoch 66 :: Batch 0/1
Batch Loss = 2.6363028118480205
66, epoch_train_loss=2.6363028118480205
Epoch 67
Epoch 67 :: Batch 0/1
Batch Loss = 2.6356765640620288
67, epoch_train_loss=2.6356765640620288
Epoch 68
Epoch 68 :: Batch 0/1
Batch Loss = 2.6356760542968534
68, epoch_train_loss=2.6356760542968534
Epoch 69
Epoch 69 :: Batch 0/1
Batch Loss = 2.6357683017455757
69, epoch_train_loss=2.6357683017455757
Epoch 70
Epoch 70 :: Batch 0/1
Batch Loss = 2.6350266740385058
70, epoch_train_loss=2.6350266740385058
Epoch 71
Epoch 71 :: Batch 0/1
Batch Loss = 2.6349193368207637
71, epoch_train_loss=2.6349193368207637
Epoch 72
Epoch 72 :: Batch 0/1
Batch Loss = 2.635193292889205
72, epoch_train_loss=2.635193292889205
Epoch 73
Epoch 73 :: Batch 0/1
Batch Loss = 2.6347129197966175
73, epoch_train_loss=2.6347129197966175
Epoch 74
Epoch 74 :: Batch 0/1
Batch Loss = 2.6342525717900975
74, epoch_train_loss=2.6342525717900975
Epoch 75
Epoch 75 :: Batch 0/1
Batch Loss = 2.634332488799823
75, epoch_train_loss=2.634332488799823
Epoch 76
Epoch 76 :: Batch 0/1
Batch Loss = 2.634179156354882
76, epoch_train_loss=2.634179156354882
Epoch 77
Epoch 77 :: Batch 0/1
Batch Loss = 2.633732177841181
77, epoch_train_loss=2.633732177841181
Epoch 78
Epoch 78 :: Batch 0/1
Batch Loss = 2.6335508185913614
78, epoch_train_loss=2.6335508185913614
Epoch 79
Epoch 79 :: Batch 0/1
Batch Loss = 2.6334845212955083
79, epoch_train_loss=2.6334845212955083
Epoch 80
Epoch 80 :: Batch 0/1
Batch Loss = 2.633229487393167
80, epoch_train_loss=2.633229487393167
Epoch 81
Epoch 81 :: Batch 0/1
Batch Loss = 2.632983479577325
81, epoch_train_loss=2.632983479577325
Epoch 82
Epoch 82 :: Batch 0/1
Batch Loss = 2.6328957393823345
82, epoch_train_loss=2.6328957393823345
Epoch 83
Epoch 83 :: Batch 0/1
Batch Loss = 2.6327328322134176
83, epoch_train_loss=2.6327328322134176
Epoch 84
Epoch 84 :: Batch 0/1
Batch Loss = 2.632462105544965
84, epoch_train_loss=2.632462105544965
Epoch 85
Epoch 85 :: Batch 0/1
Batch Loss = 2.632275084181813
85, epoch_train_loss=2.632275084181813
Epoch 86
Epoch 86 :: Batch 0/1
Batch Loss = 2.6321354667425636
86, epoch_train_loss=2.6321354667425636
Epoch 87
Epoch 87 :: Batch 0/1
Batch Loss = 2.6319188583151143
87, epoch_train_loss=2.6319188583151143
Epoch 88
Epoch 88 :: Batch 0/1
Batch Loss = 2.6316813495086815
88, epoch_train_loss=2.6316813495086815
Epoch 89
Epoch 89 :: Batch 0/1
Batch Loss = 2.631508066501884
89, epoch_train_loss=2.631508066501884
Epoch 90
Epoch 90 :: Batch 0/1
Batch Loss = 2.631368194714153
90, epoch_train_loss=2.631368194714153
Epoch 91
Epoch 91 :: Batch 0/1
Batch Loss = 2.6311907320884935
91, epoch_train_loss=2.6311907320884935
Epoch 92
Epoch 92 :: Batch 0/1
Batch Loss = 2.630956664219568
92, epoch_train_loss=2.630956664219568
Epoch 93
Epoch 93 :: Batch 0/1
Batch Loss = 2.6307169565365953
93, epoch_train_loss=2.6307169565365953
Epoch 94
Epoch 94 :: Batch 0/1
Batch Loss = 2.63052120618804
94, epoch_train_loss=2.63052120618804
Epoch 95
Epoch 95 :: Batch 0/1
Batch Loss = 2.630353821808706
95, epoch_train_loss=2.630353821808706
Epoch 96
Epoch 96 :: Batch 0/1
Batch Loss = 2.6301551751610073
96, epoch_train_loss=2.6301551751610073
Epoch 97
Epoch 97 :: Batch 0/1
Batch Loss = 2.629895035377484
97, epoch_train_loss=2.629895035377484
Epoch 98
Epoch 98 :: Batch 0/1
Batch Loss = 2.629622242085255
98, epoch_train_loss=2.629622242085255
Epoch 99
Epoch 99 :: Batch 0/1
Batch Loss = 2.6293729431648343
99, epoch_train_loss=2.6293729431648343
Epoch 100
Epoch 100 :: Batch 0/1
Batch Loss = 2.62913003783891
100, epoch_train_loss=2.62913003783891
Epoch 101
Epoch 101 :: Batch 0/1
Batch Loss = 2.62887114250817
101, epoch_train_loss=2.62887114250817
Epoch 102
Epoch 102 :: Batch 0/1
Batch Loss = 2.6285967141800533
102, epoch_train_loss=2.6285967141800533
Epoch 103
Epoch 103 :: Batch 0/1
Batch Loss = 2.6283018798473425
103, epoch_train_loss=2.6283018798473425
Epoch 104
Epoch 104 :: Batch 0/1
Batch Loss = 2.627971642407208
104, epoch_train_loss=2.627971642407208
Epoch 105
Epoch 105 :: Batch 0/1
Batch Loss = 2.627617153274639
105, epoch_train_loss=2.627617153274639
Epoch 106
Epoch 106 :: Batch 0/1
Batch Loss = 2.6272546960071064
106, epoch_train_loss=2.6272546960071064
Epoch 107
Epoch 107 :: Batch 0/1
Batch Loss = 2.6268802713430666
107, epoch_train_loss=2.6268802713430666
Epoch 108
Epoch 108 :: Batch 0/1
Batch Loss = 2.6264812433910474
108, epoch_train_loss=2.6264812433910474
Epoch 109
Epoch 109 :: Batch 0/1
Batch Loss = 2.626061540714157
109, epoch_train_loss=2.626061540714157
Epoch 110
Epoch 110 :: Batch 0/1
Batch Loss = 2.625623160448925
110, epoch_train_loss=2.625623160448925
Epoch 111
Epoch 111 :: Batch 0/1
Batch Loss = 2.6251602635231706
111, epoch_train_loss=2.6251602635231706
Epoch 112
Epoch 112 :: Batch 0/1
Batch Loss = 2.6246714915794587
112, epoch_train_loss=2.6246714915794587
Epoch 113
Epoch 113 :: Batch 0/1
Batch Loss = 2.6241553275855387
113, epoch_train_loss=2.6241553275855387
Epoch 114
Epoch 114 :: Batch 0/1
Batch Loss = 2.623603956184019
114, epoch_train_loss=2.623603956184019
Epoch 115
Epoch 115 :: Batch 0/1
Batch Loss = 2.623018016636098
115, epoch_train_loss=2.623018016636098
Epoch 116
Epoch 116 :: Batch 0/1
Batch Loss = 2.622401934334281
116, epoch_train_loss=2.622401934334281
Epoch 117
Epoch 117 :: Batch 0/1
Batch Loss = 2.6217513634642917
117, epoch_train_loss=2.6217513634642917
Epoch 118
Epoch 118 :: Batch 0/1
Batch Loss = 2.6210685545349928
118, epoch_train_loss=2.6210685545349928
Epoch 119
Epoch 119 :: Batch 0/1
Batch Loss = 2.6203807185514876
119, epoch_train_loss=2.6203807185514876
Epoch 120
Epoch 120 :: Batch 0/1
Batch Loss = 2.619737116433271
120, epoch_train_loss=2.619737116433271
Epoch 121
Epoch 121 :: Batch 0/1
Batch Loss = 2.619260689766926
121, epoch_train_loss=2.619260689766926
Epoch 122
Epoch 122 :: Batch 0/1
Batch Loss = 2.6192637526485014
122, epoch_train_loss=2.6192637526485014
Epoch 123
Epoch 123 :: Batch 0/1
Batch Loss = 2.6203426785044153
123, epoch_train_loss=2.6203426785044153
Epoch 124
Epoch 124 :: Batch 0/1
Batch Loss = 2.623452274514617
124, epoch_train_loss=2.623452274514617
Epoch 125
Epoch 125 :: Batch 0/1
Batch Loss = 2.6282323931604354
125, epoch_train_loss=2.6282323931604354
Epoch 126
Epoch 126 :: Batch 0/1
Batch Loss = 2.6308612421146766
126, epoch_train_loss=2.6308612421146766
Epoch 127
Epoch 127 :: Batch 0/1
Batch Loss = 2.623151589897236
127, epoch_train_loss=2.623151589897236
Epoch 128
Epoch 128 :: Batch 0/1
Batch Loss = 2.612554157362747
128, epoch_train_loss=2.612554157362747
Epoch 129
Epoch 129 :: Batch 0/1
Batch Loss = 2.612955941916541
129, epoch_train_loss=2.612955941916541
Epoch 130
Epoch 130 :: Batch 0/1
Batch Loss = 2.6179471718640057
130, epoch_train_loss=2.6179471718640057
Epoch 131
Epoch 131 :: Batch 0/1
Batch Loss = 2.61347811619808
131, epoch_train_loss=2.61347811619808
Epoch 132
Epoch 132 :: Batch 0/1
Batch Loss = 2.6074141753309776
132, epoch_train_loss=2.6074141753309776
Epoch 133
Epoch 133 :: Batch 0/1
Batch Loss = 2.6107384813872865
133, epoch_train_loss=2.6107384813872865
Epoch 134
Epoch 134 :: Batch 0/1
Batch Loss = 2.6100112227826724
134, epoch_train_loss=2.6100112227826724
Epoch 135
Epoch 135 :: Batch 0/1
Batch Loss = 2.6036147200665942
135, epoch_train_loss=2.6036147200665942
Epoch 136
Epoch 136 :: Batch 0/1
Batch Loss = 2.605616698868087
136, epoch_train_loss=2.605616698868087
Epoch 137
Epoch 137 :: Batch 0/1
Batch Loss = 2.605090835024332
137, epoch_train_loss=2.605090835024332
Epoch 138
Epoch 138 :: Batch 0/1
Batch Loss = 2.600689039114328
138, epoch_train_loss=2.600689039114328
Epoch 139
Epoch 139 :: Batch 0/1
Batch Loss = 2.600911381167539
139, epoch_train_loss=2.600911381167539
Epoch 140
Epoch 140 :: Batch 0/1
Batch Loss = 2.601779628643124
140, epoch_train_loss=2.601779628643124
Epoch 141
Epoch 141 :: Batch 0/1
Batch Loss = 2.6000739899600833
141, epoch_train_loss=2.6000739899600833
Epoch 142
Epoch 142 :: Batch 0/1
Batch Loss = 2.5964963066350237
142, epoch_train_loss=2.5964963066350237
Epoch 143
Epoch 143 :: Batch 0/1
Batch Loss = 2.596183763720304
143, epoch_train_loss=2.596183763720304
Epoch 144
Epoch 144 :: Batch 0/1
Batch Loss = 2.5990610234922213
144, epoch_train_loss=2.5990610234922213
Epoch 145
Epoch 145 :: Batch 0/1
Batch Loss = 2.601201272470409
145, epoch_train_loss=2.601201272470409
Epoch 146
Epoch 146 :: Batch 0/1
Batch Loss = 2.6052641331806536
146, epoch_train_loss=2.6052641331806536
Epoch 147
Epoch 147 :: Batch 0/1
Batch Loss = 2.6093536031538336
147, epoch_train_loss=2.6093536031538336
Epoch 148
Epoch 148 :: Batch 0/1
Batch Loss = 2.6090215531499776
148, epoch_train_loss=2.6090215531499776
Epoch 149
Epoch 149 :: Batch 0/1
Batch Loss = 2.604506932280671
149, epoch_train_loss=2.604506932280671
Epoch 150
Epoch 150 :: Batch 0/1
Batch Loss = 2.5959141164193644
150, epoch_train_loss=2.5959141164193644
Epoch 151
Epoch 151 :: Batch 0/1
Batch Loss = 2.5897364148232356
151, epoch_train_loss=2.5897364148232356
Epoch 152
Epoch 152 :: Batch 0/1
Batch Loss = 2.5879310928785615
152, epoch_train_loss=2.5879310928785615
Epoch 153
Epoch 153 :: Batch 0/1
Batch Loss = 2.58935686144021
153, epoch_train_loss=2.58935686144021
Epoch 154
Epoch 154 :: Batch 0/1
Batch Loss = 2.591250376466263
154, epoch_train_loss=2.591250376466263
Epoch 155
Epoch 155 :: Batch 0/1
Batch Loss = 2.590862595308327
155, epoch_train_loss=2.590862595308327
Epoch 156
Epoch 156 :: Batch 0/1
Batch Loss = 2.591597063676287
156, epoch_train_loss=2.591597063676287
Epoch 157
Epoch 157 :: Batch 0/1
Batch Loss = 2.591748429352255
157, epoch_train_loss=2.591748429352255
Epoch 158
Epoch 158 :: Batch 0/1
Batch Loss = 2.5903523631479217
158, epoch_train_loss=2.5903523631479217
Epoch 159
Epoch 159 :: Batch 0/1
Batch Loss = 2.58884897813038
159, epoch_train_loss=2.58884897813038
Epoch 160
Epoch 160 :: Batch 0/1
Batch Loss = 2.5885786464770844
160, epoch_train_loss=2.5885786464770844
Epoch 161
Epoch 161 :: Batch 0/1
Batch Loss = 2.5864185714298493
161, epoch_train_loss=2.5864185714298493
Epoch 162
Epoch 162 :: Batch 0/1
Batch Loss = 2.585870949726866
162, epoch_train_loss=2.585870949726866
Epoch 163
Epoch 163 :: Batch 0/1
Batch Loss = 2.5853413813885786
163, epoch_train_loss=2.5853413813885786
Epoch 164
Epoch 164 :: Batch 0/1
Batch Loss = 2.586719857460667
164, epoch_train_loss=2.586719857460667
Epoch 165
Epoch 165 :: Batch 0/1
Batch Loss = 2.5897732788679435
165, epoch_train_loss=2.5897732788679435
Epoch 166
Epoch 166 :: Batch 0/1
Batch Loss = 2.5939671034942484
166, epoch_train_loss=2.5939671034942484
Epoch 167
Epoch 167 :: Batch 0/1
Batch Loss = 2.5940256780210906
167, epoch_train_loss=2.5940256780210906
Epoch 168
Epoch 168 :: Batch 0/1
Batch Loss = 2.5937462182588638
168, epoch_train_loss=2.5937462182588638
Epoch 169
Epoch 169 :: Batch 0/1
Batch Loss = 2.579557443854102
169, epoch_train_loss=2.579557443854102
Epoch 170
Epoch 170 :: Batch 0/1
Batch Loss = 2.573077415382073
170, epoch_train_loss=2.573077415382073
Epoch 171
Epoch 171 :: Batch 0/1
Batch Loss = 2.5713232664576804
171, epoch_train_loss=2.5713232664576804
Epoch 172
Epoch 172 :: Batch 0/1
Batch Loss = 2.5727761099797033
172, epoch_train_loss=2.5727761099797033
Epoch 173
Epoch 173 :: Batch 0/1
Batch Loss = 2.5775434179778087
173, epoch_train_loss=2.5775434179778087
Epoch 174
Epoch 174 :: Batch 0/1
Batch Loss = 2.5794642856037955
174, epoch_train_loss=2.5794642856037955
Epoch 175
Epoch 175 :: Batch 0/1
Batch Loss = 2.585076510970425
175, epoch_train_loss=2.585076510970425
Epoch 176
Epoch 176 :: Batch 0/1
Batch Loss = 2.574309075341952
176, epoch_train_loss=2.574309075341952
Epoch 177
Epoch 177 :: Batch 0/1
Batch Loss = 2.5722600513910385
177, epoch_train_loss=2.5722600513910385
Epoch 178
Epoch 178 :: Batch 0/1
Batch Loss = 2.577232438670017
178, epoch_train_loss=2.577232438670017
Epoch 179
Epoch 179 :: Batch 0/1
Batch Loss = 2.588746065829637
179, epoch_train_loss=2.588746065829637
Epoch 180
Epoch 180 :: Batch 0/1
Batch Loss = 2.594587491184599
180, epoch_train_loss=2.594587491184599
Epoch 181
Epoch 181 :: Batch 0/1
Batch Loss = 2.597887395129344
181, epoch_train_loss=2.597887395129344
Epoch 182
Epoch 182 :: Batch 0/1
Batch Loss = 2.581370871354067
182, epoch_train_loss=2.581370871354067
Epoch 183
Epoch 183 :: Batch 0/1
Batch Loss = 2.5694015533572636
183, epoch_train_loss=2.5694015533572636
Epoch 184
Epoch 184 :: Batch 0/1
Batch Loss = 2.5686401068359537
184, epoch_train_loss=2.5686401068359537
Epoch 185
Epoch 185 :: Batch 0/1
Batch Loss = 2.5725284050441655
185, epoch_train_loss=2.5725284050441655
Epoch 186
Epoch 186 :: Batch 0/1
Batch Loss = 2.5816959500963264
186, epoch_train_loss=2.5816959500963264
Epoch 187
Epoch 187 :: Batch 0/1
Batch Loss = 2.5664076015540482
187, epoch_train_loss=2.5664076015540482
Epoch 188
Epoch 188 :: Batch 0/1
Batch Loss = 2.5649763976761912
188, epoch_train_loss=2.5649763976761912
Epoch 189
Epoch 189 :: Batch 0/1
Batch Loss = 2.5772772078677133
189, epoch_train_loss=2.5772772078677133
Epoch 190
Epoch 190 :: Batch 0/1
Batch Loss = 2.5595140139823713
190, epoch_train_loss=2.5595140139823713
Epoch 191
Epoch 191 :: Batch 0/1
Batch Loss = 2.5609706168040858
191, epoch_train_loss=2.5609706168040858
Epoch 192
Epoch 192 :: Batch 0/1
Batch Loss = 2.5750444202653875
192, epoch_train_loss=2.5750444202653875
Epoch 193
Epoch 193 :: Batch 0/1
Batch Loss = 2.5581407810530084
193, epoch_train_loss=2.5581407810530084
Epoch 194
Epoch 194 :: Batch 0/1
Batch Loss = 2.5633348221966368
194, epoch_train_loss=2.5633348221966368
Epoch 195
Epoch 195 :: Batch 0/1
Batch Loss = 2.566291742560396
195, epoch_train_loss=2.566291742560396
Epoch 196
Epoch 196 :: Batch 0/1
Batch Loss = 2.5532773686252113
196, epoch_train_loss=2.5532773686252113
Epoch 197
Epoch 197 :: Batch 0/1
Batch Loss = 2.5726158956129725
197, epoch_train_loss=2.5726158956129725
Epoch 198
Epoch 198 :: Batch 0/1
Batch Loss = 2.5772276830213925
198, epoch_train_loss=2.5772276830213925
Epoch 199
Epoch 199 :: Batch 0/1
Batch Loss = 2.5898060951438118
199, epoch_train_loss=2.5898060951438118
Epoch 200
Epoch 200 :: Batch 0/1
Batch Loss = 2.7104601311783183
200, epoch_train_loss=2.7104601311783183
Epoch 201
Epoch 201 :: Batch 0/1
Batch Loss = 2.729116350388031
201, epoch_train_loss=2.729116350388031
Epoch 202
Epoch 202 :: Batch 0/1
Batch Loss = 2.714501416715384
202, epoch_train_loss=2.714501416715384
Epoch 203
Epoch 203 :: Batch 0/1
Batch Loss = 2.6276246636656424
203, epoch_train_loss=2.6276246636656424
Epoch 204
Epoch 204 :: Batch 0/1
Batch Loss = 2.6212199495167
204, epoch_train_loss=2.6212199495167
Epoch 205
Epoch 205 :: Batch 0/1
Batch Loss = 2.5797238481955542
205, epoch_train_loss=2.5797238481955542
Epoch 206
Epoch 206 :: Batch 0/1
Batch Loss = 2.6094219676949155
206, epoch_train_loss=2.6094219676949155
Epoch 207
Epoch 207 :: Batch 0/1
Batch Loss = 2.575661587148137
207, epoch_train_loss=2.575661587148137
Epoch 208
Epoch 208 :: Batch 0/1
Batch Loss = 2.606583823967505
208, epoch_train_loss=2.606583823967505
Epoch 209
Epoch 209 :: Batch 0/1
Batch Loss = 2.567665703040193
209, epoch_train_loss=2.567665703040193
Epoch 210
Epoch 210 :: Batch 0/1
Batch Loss = 2.5854034381269897
210, epoch_train_loss=2.5854034381269897
Epoch 211
Epoch 211 :: Batch 0/1
Batch Loss = 2.5793577873113223
211, epoch_train_loss=2.5793577873113223
Epoch 212
Epoch 212 :: Batch 0/1
Batch Loss = 2.560405420435855
212, epoch_train_loss=2.560405420435855
Epoch 213
Epoch 213 :: Batch 0/1
Batch Loss = 2.58404777524465
213, epoch_train_loss=2.58404777524465
Epoch 214
Epoch 214 :: Batch 0/1
Batch Loss = 2.5589210499361377
214, epoch_train_loss=2.5589210499361377
Epoch 215
Epoch 215 :: Batch 0/1
Batch Loss = 2.5673158709039425
215, epoch_train_loss=2.5673158709039425
Epoch 216
Epoch 216 :: Batch 0/1
Batch Loss = 2.568428352009778
216, epoch_train_loss=2.568428352009778
Epoch 217
Epoch 217 :: Batch 0/1
Batch Loss = 2.5544026125144916
217, epoch_train_loss=2.5544026125144916
Epoch 218
Epoch 218 :: Batch 0/1
Batch Loss = 2.565293913406328
218, epoch_train_loss=2.565293913406328
Epoch 219
Epoch 219 :: Batch 0/1
Batch Loss = 2.55453681454885
219, epoch_train_loss=2.55453681454885
Epoch 220
Epoch 220 :: Batch 0/1
Batch Loss = 2.5525358244803424
220, epoch_train_loss=2.5525358244803424
Epoch 221
Epoch 221 :: Batch 0/1
Batch Loss = 2.5572554347237406
221, epoch_train_loss=2.5572554347237406
Epoch 222
Epoch 222 :: Batch 0/1
Batch Loss = 2.5448477192680103
222, epoch_train_loss=2.5448477192680103
Epoch 223
Epoch 223 :: Batch 0/1
Batch Loss = 2.554117543422678
223, epoch_train_loss=2.554117543422678
Epoch 224
Epoch 224 :: Batch 0/1
Batch Loss = 2.5433984276555206
224, epoch_train_loss=2.5433984276555206
Epoch 225
Epoch 225 :: Batch 0/1
Batch Loss = 2.546998355507284
225, epoch_train_loss=2.546998355507284
Epoch 226
Epoch 226 :: Batch 0/1
Batch Loss = 2.54261300902362
226, epoch_train_loss=2.54261300902362
Epoch 227
Epoch 227 :: Batch 0/1
Batch Loss = 2.5399917757733768
227, epoch_train_loss=2.5399917757733768
Epoch 228
Epoch 228 :: Batch 0/1
Batch Loss = 2.540174105718613
228, epoch_train_loss=2.540174105718613
Epoch 229
Epoch 229 :: Batch 0/1
Batch Loss = 2.5339018730197598
229, epoch_train_loss=2.5339018730197598
Epoch 230
Epoch 230 :: Batch 0/1
Batch Loss = 2.5361622974638545
230, epoch_train_loss=2.5361622974638545
Epoch 231
Epoch 231 :: Batch 0/1
Batch Loss = 2.52916235289828
231, epoch_train_loss=2.52916235289828
Epoch 232
Epoch 232 :: Batch 0/1
Batch Loss = 2.530821057685509
232, epoch_train_loss=2.530821057685509
Epoch 233
Epoch 233 :: Batch 0/1
Batch Loss = 2.523948529201328
233, epoch_train_loss=2.523948529201328
Epoch 234
Epoch 234 :: Batch 0/1
Batch Loss = 2.524785382510613
234, epoch_train_loss=2.524785382510613
Epoch 235
Epoch 235 :: Batch 0/1
Batch Loss = 2.519065954790681
235, epoch_train_loss=2.519065954790681
Epoch 236
Epoch 236 :: Batch 0/1
Batch Loss = 2.5168179098508228
236, epoch_train_loss=2.5168179098508228
Epoch 237
Epoch 237 :: Batch 0/1
Batch Loss = 2.5143180408339356
237, epoch_train_loss=2.5143180408339356
Epoch 238
Epoch 238 :: Batch 0/1
Batch Loss = 2.50767033446386
238, epoch_train_loss=2.50767033446386
Epoch 239
Epoch 239 :: Batch 0/1
Batch Loss = 2.5073462393163073
239, epoch_train_loss=2.5073462393163073
Epoch 240
Epoch 240 :: Batch 0/1
Batch Loss = 2.502155265386952
240, epoch_train_loss=2.502155265386952
Epoch 241
Epoch 241 :: Batch 0/1
Batch Loss = 2.4964193809569832
241, epoch_train_loss=2.4964193809569832
Epoch 242
Epoch 242 :: Batch 0/1
Batch Loss = 2.4930573638901654
242, epoch_train_loss=2.4930573638901654
Epoch 243
Epoch 243 :: Batch 0/1
Batch Loss = 2.491790062831669
243, epoch_train_loss=2.491790062831669
Epoch 244
Epoch 244 :: Batch 0/1
Batch Loss = 2.489788486577565
244, epoch_train_loss=2.489788486577565
Epoch 245
Epoch 245 :: Batch 0/1
Batch Loss = 2.4904214455089186
245, epoch_train_loss=2.4904214455089186
Epoch 246
Epoch 246 :: Batch 0/1
Batch Loss = 2.5097742700560866
246, epoch_train_loss=2.5097742700560866
Epoch 247
Epoch 247 :: Batch 0/1
Batch Loss = 2.5654741873618225
247, epoch_train_loss=2.5654741873618225
Epoch 248
Epoch 248 :: Batch 0/1
Batch Loss = 2.745905799216372
248, epoch_train_loss=2.745905799216372
Epoch 249
Epoch 249 :: Batch 0/1
Batch Loss = 2.6297041838705324
249, epoch_train_loss=2.6297041838705324
Epoch 250
Epoch 250 :: Batch 0/1
Batch Loss = 2.707805904824573
250, epoch_train_loss=2.707805904824573
Epoch 251
Epoch 251 :: Batch 0/1
Batch Loss = 2.692264005189058
251, epoch_train_loss=2.692264005189058
Epoch 252
Epoch 252 :: Batch 0/1
Batch Loss = 2.7068058248061284
252, epoch_train_loss=2.7068058248061284
Epoch 253
Epoch 253 :: Batch 0/1
Batch Loss = 2.733223429926299
253, epoch_train_loss=2.733223429926299
Epoch 254
Epoch 254 :: Batch 0/1
Batch Loss = 2.7293290875139276
254, epoch_train_loss=2.7293290875139276
Epoch 255
Epoch 255 :: Batch 0/1
Batch Loss = 2.6223642085097203
255, epoch_train_loss=2.6223642085097203
Epoch 256
Epoch 256 :: Batch 0/1
Batch Loss = 2.705799733005363
256, epoch_train_loss=2.705799733005363
Epoch 257
Epoch 257 :: Batch 0/1
Batch Loss = 2.612846364107174
257, epoch_train_loss=2.612846364107174
Epoch 258
Epoch 258 :: Batch 0/1
Batch Loss = 2.637171891169268
258, epoch_train_loss=2.637171891169268
Epoch 259
Epoch 259 :: Batch 0/1
Batch Loss = 2.5984181077667974
259, epoch_train_loss=2.5984181077667974
Epoch 260
Epoch 260 :: Batch 0/1
Batch Loss = 2.634312332116326
260, epoch_train_loss=2.634312332116326
Epoch 261
Epoch 261 :: Batch 0/1
Batch Loss = 2.593262308587535
261, epoch_train_loss=2.593262308587535
Epoch 262
Epoch 262 :: Batch 0/1
Batch Loss = 2.6010316640717406
262, epoch_train_loss=2.6010316640717406
Epoch 263
Epoch 263 :: Batch 0/1
Batch Loss = 2.6019268081109637
263, epoch_train_loss=2.6019268081109637
Epoch 264
Epoch 264 :: Batch 0/1
Batch Loss = 2.579231325853655
264, epoch_train_loss=2.579231325853655
Epoch 265
Epoch 265 :: Batch 0/1
Batch Loss = 2.5868497354351923
265, epoch_train_loss=2.5868497354351923
Epoch 266
Epoch 266 :: Batch 0/1
Batch Loss = 2.573815325217422
266, epoch_train_loss=2.573815325217422
Epoch 267
Epoch 267 :: Batch 0/1
Batch Loss = 2.577484033969524
267, epoch_train_loss=2.577484033969524
Epoch 268
Epoch 268 :: Batch 0/1
Batch Loss = 2.5742845823977847
268, epoch_train_loss=2.5742845823977847
Epoch 269
Epoch 269 :: Batch 0/1
Batch Loss = 2.566935467100133
269, epoch_train_loss=2.566935467100133
Epoch 270
Epoch 270 :: Batch 0/1
Batch Loss = 2.574606475746745
270, epoch_train_loss=2.574606475746745
Epoch 271
Epoch 271 :: Batch 0/1
Batch Loss = 2.5683164734909383
271, epoch_train_loss=2.5683164734909383
Epoch 272
Epoch 272 :: Batch 0/1
Batch Loss = 2.5671607413150825
272, epoch_train_loss=2.5671607413150825
Epoch 273
Epoch 273 :: Batch 0/1
Batch Loss = 2.568639419110074
273, epoch_train_loss=2.568639419110074
Epoch 274
Epoch 274 :: Batch 0/1
Batch Loss = 2.5645106325162565
274, epoch_train_loss=2.5645106325162565
Epoch 275
Epoch 275 :: Batch 0/1
Batch Loss = 2.5637291560843036
275, epoch_train_loss=2.5637291560843036
Epoch 276
Epoch 276 :: Batch 0/1
Batch Loss = 2.562793529699287
276, epoch_train_loss=2.562793529699287
Epoch 277
Epoch 277 :: Batch 0/1
Batch Loss = 2.5597833393707514
277, epoch_train_loss=2.5597833393707514
Epoch 278
Epoch 278 :: Batch 0/1
Batch Loss = 2.560887396645981
278, epoch_train_loss=2.560887396645981
Epoch 279
Epoch 279 :: Batch 0/1
Batch Loss = 2.5567363128994582
279, epoch_train_loss=2.5567363128994582
Epoch 280
Epoch 280 :: Batch 0/1
Batch Loss = 2.5540744108870554
280, epoch_train_loss=2.5540744108870554
Epoch 281
Epoch 281 :: Batch 0/1
Batch Loss = 2.554776776146466
281, epoch_train_loss=2.554776776146466
Epoch 282
Epoch 282 :: Batch 0/1
Batch Loss = 2.5494835642137206
282, epoch_train_loss=2.5494835642137206
Epoch 283
Epoch 283 :: Batch 0/1
Batch Loss = 2.54897150616699
283, epoch_train_loss=2.54897150616699
Epoch 284
Epoch 284 :: Batch 0/1
Batch Loss = 2.5466947344729816
284, epoch_train_loss=2.5466947344729816
Epoch 285
Epoch 285 :: Batch 0/1
Batch Loss = 2.5441471412681524
285, epoch_train_loss=2.5441471412681524
Epoch 286
Epoch 286 :: Batch 0/1
Batch Loss = 2.5427558889028505
286, epoch_train_loss=2.5427558889028505
Epoch 287
Epoch 287 :: Batch 0/1
Batch Loss = 2.5388991129688594
287, epoch_train_loss=2.5388991129688594
Epoch 288
Epoch 288 :: Batch 0/1
Batch Loss = 2.5382973406379654
288, epoch_train_loss=2.5382973406379654
Epoch 289
Epoch 289 :: Batch 0/1
Batch Loss = 2.5344819576821482
289, epoch_train_loss=2.5344819576821482
Epoch 290
Epoch 290 :: Batch 0/1
Batch Loss = 2.532628003147149
290, epoch_train_loss=2.532628003147149
Epoch 291
Epoch 291 :: Batch 0/1
Batch Loss = 2.5299177234170944
291, epoch_train_loss=2.5299177234170944
Epoch 292
Epoch 292 :: Batch 0/1
Batch Loss = 2.527617557999882
292, epoch_train_loss=2.527617557999882
Epoch 293
Epoch 293 :: Batch 0/1
Batch Loss = 2.524926722348861
293, epoch_train_loss=2.524926722348861
Epoch 294
Epoch 294 :: Batch 0/1
Batch Loss = 2.521684168413229
294, epoch_train_loss=2.521684168413229
Epoch 295
Epoch 295 :: Batch 0/1
Batch Loss = 2.5194261866942775
295, epoch_train_loss=2.5194261866942775
Epoch 296
Epoch 296 :: Batch 0/1
Batch Loss = 2.5157121168985612
296, epoch_train_loss=2.5157121168985612
Epoch 297
Epoch 297 :: Batch 0/1
Batch Loss = 2.513020059266047
297, epoch_train_loss=2.513020059266047
Epoch 298
Epoch 298 :: Batch 0/1
Batch Loss = 2.509341755549121
298, epoch_train_loss=2.509341755549121
Epoch 299
Epoch 299 :: Batch 0/1
Batch Loss = 2.5062669975134404
299, epoch_train_loss=2.5062669975134404
Epoch 300
Epoch 300 :: Batch 0/1
Batch Loss = 2.502349270409108
300, epoch_train_loss=2.502349270409108
Epoch 301
Epoch 301 :: Batch 0/1
Batch Loss = 2.4989949288257143
301, epoch_train_loss=2.4989949288257143
Epoch 302
Epoch 302 :: Batch 0/1
Batch Loss = 2.4952150869210588
302, epoch_train_loss=2.4952150869210588
Epoch 303
Epoch 303 :: Batch 0/1
Batch Loss = 2.4912804488939972
303, epoch_train_loss=2.4912804488939972
Epoch 304
Epoch 304 :: Batch 0/1
Batch Loss = 2.4876868695192074
304, epoch_train_loss=2.4876868695192074
Epoch 305
Epoch 305 :: Batch 0/1
Batch Loss = 2.483509350994514
305, epoch_train_loss=2.483509350994514
Epoch 306
Epoch 306 :: Batch 0/1
Batch Loss = 2.479764311152787
306, epoch_train_loss=2.479764311152787
Epoch 307
Epoch 307 :: Batch 0/1
Batch Loss = 2.475520976709742
307, epoch_train_loss=2.475520976709742
Epoch 308
Epoch 308 :: Batch 0/1
Batch Loss = 2.4717117573201564
308, epoch_train_loss=2.4717117573201564
Epoch 309
Epoch 309 :: Batch 0/1
Batch Loss = 2.4677985296426375
309, epoch_train_loss=2.4677985296426375
Epoch 310
Epoch 310 :: Batch 0/1
Batch Loss = 2.4640211675517683
310, epoch_train_loss=2.4640211675517683
Epoch 311
Epoch 311 :: Batch 0/1
Batch Loss = 2.4600293041460315
311, epoch_train_loss=2.4600293041460315
Epoch 312
Epoch 312 :: Batch 0/1
Batch Loss = 2.456747386259075
312, epoch_train_loss=2.456747386259075
Epoch 313
Epoch 313 :: Batch 0/1
Batch Loss = 2.4534458375386214
313, epoch_train_loss=2.4534458375386214
Epoch 314
Epoch 314 :: Batch 0/1
Batch Loss = 2.450653972857565
314, epoch_train_loss=2.450653972857565
Epoch 315
Epoch 315 :: Batch 0/1
Batch Loss = 2.4484295404667864
315, epoch_train_loss=2.4484295404667864
Epoch 316
Epoch 316 :: Batch 0/1
Batch Loss = 2.4496239156234734
316, epoch_train_loss=2.4496239156234734
Epoch 317
Epoch 317 :: Batch 0/1
Batch Loss = 2.4682886327202134
317, epoch_train_loss=2.4682886327202134
Epoch 318
Epoch 318 :: Batch 0/1
Batch Loss = 2.5940351722784274
318, epoch_train_loss=2.5940351722784274
Epoch 319
Epoch 319 :: Batch 0/1
Batch Loss = 3.073465825955404
319, epoch_train_loss=3.073465825955404
Epoch 320
Epoch 320 :: Batch 0/1
Batch Loss = 3.02753991703696
320, epoch_train_loss=3.02753991703696
Epoch 321
Epoch 321 :: Batch 0/1
Batch Loss = 2.682604602184474
321, epoch_train_loss=2.682604602184474
Epoch 322
Epoch 322 :: Batch 0/1
Batch Loss = 2.741546376511708
322, epoch_train_loss=2.741546376511708
Epoch 323
Epoch 323 :: Batch 0/1
Batch Loss = 2.862086865545287
323, epoch_train_loss=2.862086865545287
Epoch 324
Epoch 324 :: Batch 0/1
Batch Loss = 2.5521909758706887
324, epoch_train_loss=2.5521909758706887
Epoch 325
Epoch 325 :: Batch 0/1
Batch Loss = 2.705210416324837
325, epoch_train_loss=2.705210416324837
Epoch 326
Epoch 326 :: Batch 0/1
Batch Loss = 2.7218521813383356
326, epoch_train_loss=2.7218521813383356
Epoch 327
Epoch 327 :: Batch 0/1
Batch Loss = 2.53320772831034
327, epoch_train_loss=2.53320772831034
Epoch 328
Epoch 328 :: Batch 0/1
Batch Loss = 2.5223058902108764
328, epoch_train_loss=2.5223058902108764
Epoch 329
Epoch 329 :: Batch 0/1
Batch Loss = 2.6610613434129653
329, epoch_train_loss=2.6610613434129653
Epoch 330
Epoch 330 :: Batch 0/1
Batch Loss = 2.5397617407956874
330, epoch_train_loss=2.5397617407956874
Epoch 331
Epoch 331 :: Batch 0/1
Batch Loss = 2.504770960567821
331, epoch_train_loss=2.504770960567821
Epoch 332
Epoch 332 :: Batch 0/1
Batch Loss = 2.572378142865655
332, epoch_train_loss=2.572378142865655
Epoch 333
Epoch 333 :: Batch 0/1
Batch Loss = 2.5750665906710006
333, epoch_train_loss=2.5750665906710006
Epoch 334
Epoch 334 :: Batch 0/1
Batch Loss = 2.5490413817811683
334, epoch_train_loss=2.5490413817811683
Epoch 335
Epoch 335 :: Batch 0/1
Batch Loss = 2.5067288139943247
335, epoch_train_loss=2.5067288139943247
Epoch 336
Epoch 336 :: Batch 0/1
Batch Loss = 2.5145936633133066
336, epoch_train_loss=2.5145936633133066
Epoch 337
Epoch 337 :: Batch 0/1
Batch Loss = 2.5398007095198736
337, epoch_train_loss=2.5398007095198736
Epoch 338
Epoch 338 :: Batch 0/1
Batch Loss = 2.5337372167888415
338, epoch_train_loss=2.5337372167888415
Epoch 339
Epoch 339 :: Batch 0/1
Batch Loss = 2.518604201314563
339, epoch_train_loss=2.518604201314563
Epoch 340
Epoch 340 :: Batch 0/1
Batch Loss = 2.493313528792189
340, epoch_train_loss=2.493313528792189
Epoch 341
Epoch 341 :: Batch 0/1
Batch Loss = 2.498477206552773
341, epoch_train_loss=2.498477206552773
Epoch 342
Epoch 342 :: Batch 0/1
Batch Loss = 2.5184307287829517
342, epoch_train_loss=2.5184307287829517
Epoch 343
Epoch 343 :: Batch 0/1
Batch Loss = 2.4931155326873813
343, epoch_train_loss=2.4931155326873813
Epoch 344
Epoch 344 :: Batch 0/1
Batch Loss = 2.4757540440436467
344, epoch_train_loss=2.4757540440436467
Epoch 345
Epoch 345 :: Batch 0/1
Batch Loss = 2.4860660068484286
345, epoch_train_loss=2.4860660068484286
Epoch 346
Epoch 346 :: Batch 0/1
Batch Loss = 2.489082715438861
346, epoch_train_loss=2.489082715438861
Epoch 347
Epoch 347 :: Batch 0/1
Batch Loss = 2.4754437455591054
347, epoch_train_loss=2.4754437455591054
Epoch 348
Epoch 348 :: Batch 0/1
Batch Loss = 2.4602832897070526
348, epoch_train_loss=2.4602832897070526
Epoch 349
Epoch 349 :: Batch 0/1
Batch Loss = 2.4617301096608384
349, epoch_train_loss=2.4617301096608384
Epoch 350
Epoch 350 :: Batch 0/1
Batch Loss = 2.4641485096117464
350, epoch_train_loss=2.4641485096117464
Epoch 351
Epoch 351 :: Batch 0/1
Batch Loss = 2.455695902804157
351, epoch_train_loss=2.455695902804157
Epoch 352
Epoch 352 :: Batch 0/1
Batch Loss = 2.443396586041225
352, epoch_train_loss=2.443396586041225
Epoch 353
Epoch 353 :: Batch 0/1
Batch Loss = 2.4469522985618877
353, epoch_train_loss=2.4469522985618877
Epoch 354
Epoch 354 :: Batch 0/1
Batch Loss = 2.4474851344778763
354, epoch_train_loss=2.4474851344778763
Epoch 355
Epoch 355 :: Batch 0/1
Batch Loss = 2.433727116390074
355, epoch_train_loss=2.433727116390074
Epoch 356
Epoch 356 :: Batch 0/1
Batch Loss = 2.4345109555103797
356, epoch_train_loss=2.4345109555103797
Epoch 357
Epoch 357 :: Batch 0/1
Batch Loss = 2.4330324770558183
357, epoch_train_loss=2.4330324770558183
Epoch 358
Epoch 358 :: Batch 0/1
Batch Loss = 2.42893174826782
358, epoch_train_loss=2.42893174826782
Epoch 359
Epoch 359 :: Batch 0/1
Batch Loss = 2.420728241506792
359, epoch_train_loss=2.420728241506792
Epoch 360
Epoch 360 :: Batch 0/1
Batch Loss = 2.423415358934194
360, epoch_train_loss=2.423415358934194
Epoch 361
Epoch 361 :: Batch 0/1
Batch Loss = 2.420091821575147
361, epoch_train_loss=2.420091821575147
Epoch 362
Epoch 362 :: Batch 0/1
Batch Loss = 2.4136662839491305
362, epoch_train_loss=2.4136662839491305
Epoch 363
Epoch 363 :: Batch 0/1
Batch Loss = 2.412480544066619
363, epoch_train_loss=2.412480544066619
Epoch 364
Epoch 364 :: Batch 0/1
Batch Loss = 2.412227616082141
364, epoch_train_loss=2.412227616082141
Epoch 365
Epoch 365 :: Batch 0/1
Batch Loss = 2.4057126818370103
365, epoch_train_loss=2.4057126818370103
Epoch 366
Epoch 366 :: Batch 0/1
Batch Loss = 2.406022633349976
366, epoch_train_loss=2.406022633349976
Epoch 367
Epoch 367 :: Batch 0/1
Batch Loss = 2.402483153379266
367, epoch_train_loss=2.402483153379266
Epoch 368
Epoch 368 :: Batch 0/1
Batch Loss = 2.399645392563203
368, epoch_train_loss=2.399645392563203
Epoch 369
Epoch 369 :: Batch 0/1
Batch Loss = 2.3978919048939793
369, epoch_train_loss=2.3978919048939793
Epoch 370
Epoch 370 :: Batch 0/1
Batch Loss = 2.3949095407829506
370, epoch_train_loss=2.3949095407829506
Epoch 371
Epoch 371 :: Batch 0/1
Batch Loss = 2.3930089972985797
371, epoch_train_loss=2.3930089972985797
Epoch 372
Epoch 372 :: Batch 0/1
Batch Loss = 2.390594290323302
372, epoch_train_loss=2.390594290323302
Epoch 373
Epoch 373 :: Batch 0/1
Batch Loss = 2.3873238309232065
373, epoch_train_loss=2.3873238309232065
Epoch 374
Epoch 374 :: Batch 0/1
Batch Loss = 2.38583215276459
374, epoch_train_loss=2.38583215276459
Epoch 375
Epoch 375 :: Batch 0/1
Batch Loss = 2.3834107095917143
375, epoch_train_loss=2.3834107095917143
Epoch 376
Epoch 376 :: Batch 0/1
Batch Loss = 2.379678333313476
376, epoch_train_loss=2.379678333313476
Epoch 377
Epoch 377 :: Batch 0/1
Batch Loss = 2.3784207905261376
377, epoch_train_loss=2.3784207905261376
Epoch 378
Epoch 378 :: Batch 0/1
Batch Loss = 2.375957799012362
378, epoch_train_loss=2.375957799012362
Epoch 379
Epoch 379 :: Batch 0/1
Batch Loss = 2.3723032539084077
379, epoch_train_loss=2.3723032539084077
Epoch 380
Epoch 380 :: Batch 0/1
Batch Loss = 2.370350362536618
380, epoch_train_loss=2.370350362536618
Epoch 381
Epoch 381 :: Batch 0/1
Batch Loss = 2.3674632916926397
381, epoch_train_loss=2.3674632916926397
Epoch 382
Epoch 382 :: Batch 0/1
Batch Loss = 2.3647033914331703
382, epoch_train_loss=2.3647033914331703
Epoch 383
Epoch 383 :: Batch 0/1
Batch Loss = 2.3616468672200135
383, epoch_train_loss=2.3616468672200135
Epoch 384
Epoch 384 :: Batch 0/1
Batch Loss = 2.3582554661487123
384, epoch_train_loss=2.3582554661487123
Epoch 385
Epoch 385 :: Batch 0/1
Batch Loss = 2.3556915273202628
385, epoch_train_loss=2.3556915273202628
Epoch 386
Epoch 386 :: Batch 0/1
Batch Loss = 2.351944230325223
386, epoch_train_loss=2.351944230325223
Epoch 387
Epoch 387 :: Batch 0/1
Batch Loss = 2.3481863185879637
387, epoch_train_loss=2.3481863185879637
Epoch 388
Epoch 388 :: Batch 0/1
Batch Loss = 2.344694735999591
388, epoch_train_loss=2.344694735999591
Epoch 389
Epoch 389 :: Batch 0/1
Batch Loss = 2.340835460018133
389, epoch_train_loss=2.340835460018133
Epoch 390
Epoch 390 :: Batch 0/1
Batch Loss = 2.3368399969710234
390, epoch_train_loss=2.3368399969710234
Epoch 391
Epoch 391 :: Batch 0/1
Batch Loss = 2.331981007551201
391, epoch_train_loss=2.331981007551201
Epoch 392
Epoch 392 :: Batch 0/1
Batch Loss = 2.3274098466850393
392, epoch_train_loss=2.3274098466850393
Epoch 393
Epoch 393 :: Batch 0/1
Batch Loss = 2.3222819435485085
393, epoch_train_loss=2.3222819435485085
Epoch 394
Epoch 394 :: Batch 0/1
Batch Loss = 2.317440250040153
394, epoch_train_loss=2.317440250040153
Epoch 395
Epoch 395 :: Batch 0/1
Batch Loss = 2.3118967549676075
395, epoch_train_loss=2.3118967549676075
Epoch 396
Epoch 396 :: Batch 0/1
Batch Loss = 2.306430021194404
396, epoch_train_loss=2.306430021194404
Epoch 397
Epoch 397 :: Batch 0/1
Batch Loss = 2.300309172915993
397, epoch_train_loss=2.300309172915993
Epoch 398
Epoch 398 :: Batch 0/1
Batch Loss = 2.2940665782607383
398, epoch_train_loss=2.2940665782607383
Epoch 399
Epoch 399 :: Batch 0/1
Batch Loss = 2.287663531362879
399, epoch_train_loss=2.287663531362879
Epoch 400
Epoch 400 :: Batch 0/1
Batch Loss = 2.2813792380833005
400, epoch_train_loss=2.2813792380833005
Epoch 401
Epoch 401 :: Batch 0/1
Batch Loss = 2.2796636638941394
401, epoch_train_loss=2.2796636638941394
Epoch 402
Epoch 402 :: Batch 0/1
Batch Loss = 2.2975493315520508
402, epoch_train_loss=2.2975493315520508
Epoch 403
Epoch 403 :: Batch 0/1
Batch Loss = 2.3620226322980677
403, epoch_train_loss=2.3620226322980677
Epoch 404
Epoch 404 :: Batch 0/1
Batch Loss = 2.3234779713747535
404, epoch_train_loss=2.3234779713747535
Epoch 405
Epoch 405 :: Batch 0/1
Batch Loss = 2.281469084707472
405, epoch_train_loss=2.281469084707472
Epoch 406
Epoch 406 :: Batch 0/1
Batch Loss = 2.305698252609297
406, epoch_train_loss=2.305698252609297
Epoch 407
Epoch 407 :: Batch 0/1
Batch Loss = 2.28538437668318
407, epoch_train_loss=2.28538437668318
Epoch 408
Epoch 408 :: Batch 0/1
Batch Loss = 2.322311370618318
408, epoch_train_loss=2.322311370618318
Epoch 409
Epoch 409 :: Batch 0/1
Batch Loss = 2.3431062199802377
409, epoch_train_loss=2.3431062199802377
Epoch 410
Epoch 410 :: Batch 0/1
Batch Loss = 2.277661847462827
410, epoch_train_loss=2.277661847462827
Epoch 411
Epoch 411 :: Batch 0/1
Batch Loss = 2.4134129100688346
411, epoch_train_loss=2.4134129100688346
Epoch 412
Epoch 412 :: Batch 0/1
Batch Loss = 2.274279666885814
412, epoch_train_loss=2.274279666885814
Epoch 413
Epoch 413 :: Batch 0/1
Batch Loss = 2.344375703065344
413, epoch_train_loss=2.344375703065344
Epoch 414
Epoch 414 :: Batch 0/1
Batch Loss = 2.2991964949487715
414, epoch_train_loss=2.2991964949487715
Epoch 415
Epoch 415 :: Batch 0/1
Batch Loss = 2.2817692573569084
415, epoch_train_loss=2.2817692573569084
Epoch 416
Epoch 416 :: Batch 0/1
Batch Loss = 2.3656899397187767
416, epoch_train_loss=2.3656899397187767
Epoch 417
Epoch 417 :: Batch 0/1
Batch Loss = 2.3737155023936003
417, epoch_train_loss=2.3737155023936003
Epoch 418
Epoch 418 :: Batch 0/1
Batch Loss = 2.39059279554333
418, epoch_train_loss=2.39059279554333
Epoch 419
Epoch 419 :: Batch 0/1
Batch Loss = 2.269412456494413
419, epoch_train_loss=2.269412456494413
Epoch 420
Epoch 420 :: Batch 0/1
Batch Loss = 2.3775932023889568
420, epoch_train_loss=2.3775932023889568
Epoch 421
Epoch 421 :: Batch 0/1
Batch Loss = 2.3057646758391064
421, epoch_train_loss=2.3057646758391064
Epoch 422
Epoch 422 :: Batch 0/1
Batch Loss = 2.3268417577199467
422, epoch_train_loss=2.3268417577199467
Epoch 423
Epoch 423 :: Batch 0/1
Batch Loss = 2.3045518503086937
423, epoch_train_loss=2.3045518503086937
Epoch 424
Epoch 424 :: Batch 0/1
Batch Loss = 2.24939189617619
424, epoch_train_loss=2.24939189617619
Epoch 425
Epoch 425 :: Batch 0/1
Batch Loss = 2.3208228939123203
425, epoch_train_loss=2.3208228939123203
Epoch 426
Epoch 426 :: Batch 0/1
Batch Loss = 2.2479985201821013
426, epoch_train_loss=2.2479985201821013
Epoch 427
Epoch 427 :: Batch 0/1
Batch Loss = 2.2727764748300574
427, epoch_train_loss=2.2727764748300574
Epoch 428
Epoch 428 :: Batch 0/1
Batch Loss = 2.2633917674903805
428, epoch_train_loss=2.2633917674903805
Epoch 429
Epoch 429 :: Batch 0/1
Batch Loss = 2.2335560998474615
429, epoch_train_loss=2.2335560998474615
Epoch 430
Epoch 430 :: Batch 0/1
Batch Loss = 2.251663816225023
430, epoch_train_loss=2.251663816225023
Epoch 431
Epoch 431 :: Batch 0/1
Batch Loss = 2.2287121670045504
431, epoch_train_loss=2.2287121670045504
Epoch 432
Epoch 432 :: Batch 0/1
Batch Loss = 2.2211585538375784
432, epoch_train_loss=2.2211585538375784
Epoch 433
Epoch 433 :: Batch 0/1
Batch Loss = 2.2350862817461117
433, epoch_train_loss=2.2350862817461117
Epoch 434
Epoch 434 :: Batch 0/1
Batch Loss = 2.1974722256049666
434, epoch_train_loss=2.1974722256049666
Epoch 435
Epoch 435 :: Batch 0/1
Batch Loss = 2.2276581501730153
435, epoch_train_loss=2.2276581501730153
Epoch 436
Epoch 436 :: Batch 0/1
Batch Loss = 2.195663916045378
436, epoch_train_loss=2.195663916045378
Epoch 437
Epoch 437 :: Batch 0/1
Batch Loss = 2.2008100515866937
437, epoch_train_loss=2.2008100515866937
Epoch 438
Epoch 438 :: Batch 0/1
Batch Loss = 2.1929006755336595
438, epoch_train_loss=2.1929006755336595
Epoch 439
Epoch 439 :: Batch 0/1
Batch Loss = 2.192084629137827
439, epoch_train_loss=2.192084629137827
Epoch 440
Epoch 440 :: Batch 0/1
Batch Loss = 2.1943473470536743
440, epoch_train_loss=2.1943473470536743
Epoch 441
Epoch 441 :: Batch 0/1
Batch Loss = 2.170488117668471
441, epoch_train_loss=2.170488117668471
Epoch 442
Epoch 442 :: Batch 0/1
Batch Loss = 2.1851129144980206
442, epoch_train_loss=2.1851129144980206
Epoch 443
Epoch 443 :: Batch 0/1
Batch Loss = 2.1691608644879246
443, epoch_train_loss=2.1691608644879246
Epoch 444
Epoch 444 :: Batch 0/1
Batch Loss = 2.1625792724055475
444, epoch_train_loss=2.1625792724055475
Epoch 445
Epoch 445 :: Batch 0/1
Batch Loss = 2.1732600416640064
445, epoch_train_loss=2.1732600416640064
Epoch 446
Epoch 446 :: Batch 0/1
Batch Loss = 2.1738474552484024
446, epoch_train_loss=2.1738474552484024
Epoch 447
Epoch 447 :: Batch 0/1
Batch Loss = 2.146199544412827
447, epoch_train_loss=2.146199544412827
Epoch 448
Epoch 448 :: Batch 0/1
Batch Loss = 2.1690177932369137
448, epoch_train_loss=2.1690177932369137
Epoch 449
Epoch 449 :: Batch 0/1
Batch Loss = 2.1809657095432735
449, epoch_train_loss=2.1809657095432735
Epoch 450
Epoch 450 :: Batch 0/1
Batch Loss = 2.183538559107758
450, epoch_train_loss=2.183538559107758
Epoch 451
Epoch 451 :: Batch 0/1
Batch Loss = 2.129294405953376
451, epoch_train_loss=2.129294405953376
Epoch 452
Epoch 452 :: Batch 0/1
Batch Loss = 2.179380954590225
452, epoch_train_loss=2.179380954590225
Epoch 453
Epoch 453 :: Batch 0/1
Batch Loss = 2.249734059882735
453, epoch_train_loss=2.249734059882735
Epoch 454
Epoch 454 :: Batch 0/1
Batch Loss = 2.267716614861541
454, epoch_train_loss=2.267716614861541
Epoch 455
Epoch 455 :: Batch 0/1
Batch Loss = 2.1786540464556103
455, epoch_train_loss=2.1786540464556103
Epoch 456
Epoch 456 :: Batch 0/1
Batch Loss = 2.4201456888796473
456, epoch_train_loss=2.4201456888796473
Epoch 457
Epoch 457 :: Batch 0/1
Batch Loss = 2.5367340698410503
457, epoch_train_loss=2.5367340698410503
Epoch 458
Epoch 458 :: Batch 0/1
Batch Loss = 2.579056788197407
458, epoch_train_loss=2.579056788197407
Epoch 459
Epoch 459 :: Batch 0/1
Batch Loss = 2.395995281367191
459, epoch_train_loss=2.395995281367191
Epoch 460
Epoch 460 :: Batch 0/1
Batch Loss = 2.2409517967516988
460, epoch_train_loss=2.2409517967516988
Epoch 461
Epoch 461 :: Batch 0/1
Batch Loss = 2.4663221512896483
461, epoch_train_loss=2.4663221512896483
Epoch 462
Epoch 462 :: Batch 0/1
Batch Loss = 2.269098097185689
462, epoch_train_loss=2.269098097185689
Epoch 463
Epoch 463 :: Batch 0/1
Batch Loss = 2.318687932927974
463, epoch_train_loss=2.318687932927974
Epoch 464
Epoch 464 :: Batch 0/1
Batch Loss = 2.243653091459785
464, epoch_train_loss=2.243653091459785
Epoch 465
Epoch 465 :: Batch 0/1
Batch Loss = 2.254902127919973
465, epoch_train_loss=2.254902127919973
Epoch 466
Epoch 466 :: Batch 0/1
Batch Loss = 2.3116530745561064
466, epoch_train_loss=2.3116530745561064
Epoch 467
Epoch 467 :: Batch 0/1
Batch Loss = 2.2639529524122897
467, epoch_train_loss=2.2639529524122897
Epoch 468
Epoch 468 :: Batch 0/1
Batch Loss = 2.197856577446302
468, epoch_train_loss=2.197856577446302
Epoch 469
Epoch 469 :: Batch 0/1
Batch Loss = 2.2630426579026297
469, epoch_train_loss=2.2630426579026297
Epoch 470
Epoch 470 :: Batch 0/1
Batch Loss = 2.2230470597871954
470, epoch_train_loss=2.2230470597871954
Epoch 471
Epoch 471 :: Batch 0/1
Batch Loss = 2.206171821849775
471, epoch_train_loss=2.206171821849775
Epoch 472
Epoch 472 :: Batch 0/1
Batch Loss = 2.219823733506928
472, epoch_train_loss=2.219823733506928
Epoch 473
Epoch 473 :: Batch 0/1
Batch Loss = 2.2010258883263685
473, epoch_train_loss=2.2010258883263685
Epoch 474
Epoch 474 :: Batch 0/1
Batch Loss = 2.2088074029645246
474, epoch_train_loss=2.2088074029645246
Epoch 475
Epoch 475 :: Batch 0/1
Batch Loss = 2.1740568391783484
475, epoch_train_loss=2.1740568391783484
Epoch 476
Epoch 476 :: Batch 0/1
Batch Loss = 2.1864321469608194
476, epoch_train_loss=2.1864321469608194
Epoch 477
Epoch 477 :: Batch 0/1
Batch Loss = 2.1749861424911967
477, epoch_train_loss=2.1749861424911967
Epoch 478
Epoch 478 :: Batch 0/1
Batch Loss = 2.161270746536338
478, epoch_train_loss=2.161270746536338
Epoch 479
Epoch 479 :: Batch 0/1
Batch Loss = 2.160708454700526
479, epoch_train_loss=2.160708454700526
Epoch 480
Epoch 480 :: Batch 0/1
Batch Loss = 2.164080184078821
480, epoch_train_loss=2.164080184078821
Epoch 481
Epoch 481 :: Batch 0/1
Batch Loss = 2.143000588605379
481, epoch_train_loss=2.143000588605379
Epoch 482
Epoch 482 :: Batch 0/1
Batch Loss = 2.1515177662701936
482, epoch_train_loss=2.1515177662701936
Epoch 483
Epoch 483 :: Batch 0/1
Batch Loss = 2.136070473983454
483, epoch_train_loss=2.136070473983454
Epoch 484
Epoch 484 :: Batch 0/1
Batch Loss = 2.140430582679939
484, epoch_train_loss=2.140430582679939
Epoch 485
Epoch 485 :: Batch 0/1
Batch Loss = 2.137670692848602
485, epoch_train_loss=2.137670692848602
Epoch 486
Epoch 486 :: Batch 0/1
Batch Loss = 2.129112304564968
486, epoch_train_loss=2.129112304564968
Epoch 487
Epoch 487 :: Batch 0/1
Batch Loss = 2.1279241495313888
487, epoch_train_loss=2.1279241495313888
Epoch 488
Epoch 488 :: Batch 0/1
Batch Loss = 2.1254025286574363
488, epoch_train_loss=2.1254025286574363
Epoch 489
Epoch 489 :: Batch 0/1
Batch Loss = 2.1147036216947526
489, epoch_train_loss=2.1147036216947526
Epoch 490
Epoch 490 :: Batch 0/1
Batch Loss = 2.1177214949999046
490, epoch_train_loss=2.1177214949999046
Epoch 491
Epoch 491 :: Batch 0/1
Batch Loss = 2.1128164978000217
491, epoch_train_loss=2.1128164978000217
Epoch 492
Epoch 492 :: Batch 0/1
Batch Loss = 2.108837850624922
492, epoch_train_loss=2.108837850624922
Epoch 493
Epoch 493 :: Batch 0/1
Batch Loss = 2.1053381237491573
493, epoch_train_loss=2.1053381237491573
Epoch 494
Epoch 494 :: Batch 0/1
Batch Loss = 2.1007064013090275
494, epoch_train_loss=2.1007064013090275
Epoch 495
Epoch 495 :: Batch 0/1
Batch Loss = 2.0988325063381827
495, epoch_train_loss=2.0988325063381827
Epoch 496
Epoch 496 :: Batch 0/1
Batch Loss = 2.09317816404855
496, epoch_train_loss=2.09317816404855
Epoch 497
Epoch 497 :: Batch 0/1
Batch Loss = 2.093497769732784
497, epoch_train_loss=2.093497769732784
Epoch 498
Epoch 498 :: Batch 0/1
Batch Loss = 2.088631128586948
498, epoch_train_loss=2.088631128586948
Epoch 499
Epoch 499 :: Batch 0/1
Batch Loss = 2.084288687870447
499, epoch_train_loss=2.084288687870447
Epoch 500
Epoch 500 :: Batch 0/1
Batch Loss = 2.082364256291398
500, epoch_train_loss=2.082364256291398
Epoch 501
Epoch 501 :: Batch 0/1
Batch Loss = 2.078439569050854
501, epoch_train_loss=2.078439569050854
Epoch 502
Epoch 502 :: Batch 0/1
Batch Loss = 2.0766470427531085
502, epoch_train_loss=2.0766470427531085
Epoch 503
Epoch 503 :: Batch 0/1
Batch Loss = 2.072025129723041
503, epoch_train_loss=2.072025129723041
Epoch 504
Epoch 504 :: Batch 0/1
Batch Loss = 2.070277847544629
504, epoch_train_loss=2.070277847544629
Epoch 505
Epoch 505 :: Batch 0/1
Batch Loss = 2.0674278054815187
505, epoch_train_loss=2.0674278054815187
Epoch 506
Epoch 506 :: Batch 0/1
Batch Loss = 2.0634019007067277
506, epoch_train_loss=2.0634019007067277
Epoch 507
Epoch 507 :: Batch 0/1
Batch Loss = 2.0620133417907143
507, epoch_train_loss=2.0620133417907143
Epoch 508
Epoch 508 :: Batch 0/1
Batch Loss = 2.0581145401775687
508, epoch_train_loss=2.0581145401775687
Epoch 509
Epoch 509 :: Batch 0/1
Batch Loss = 2.0557571959974847
509, epoch_train_loss=2.0557571959974847
Epoch 510
Epoch 510 :: Batch 0/1
Batch Loss = 2.05239704861746
510, epoch_train_loss=2.05239704861746
Epoch 511
Epoch 511 :: Batch 0/1
Batch Loss = 2.050007130454239
511, epoch_train_loss=2.050007130454239
Epoch 512
Epoch 512 :: Batch 0/1
Batch Loss = 2.0473120838733645
512, epoch_train_loss=2.0473120838733645
Epoch 513
Epoch 513 :: Batch 0/1
Batch Loss = 2.0441560312543148
513, epoch_train_loss=2.0441560312543148
Epoch 514
Epoch 514 :: Batch 0/1
Batch Loss = 2.041855070605249
514, epoch_train_loss=2.041855070605249
Epoch 515
Epoch 515 :: Batch 0/1
Batch Loss = 2.0387471592603448
515, epoch_train_loss=2.0387471592603448
Epoch 516
Epoch 516 :: Batch 0/1
Batch Loss = 2.0359362145190394
516, epoch_train_loss=2.0359362145190394
Epoch 517
Epoch 517 :: Batch 0/1
Batch Loss = 2.0332020853209722
517, epoch_train_loss=2.0332020853209722
Epoch 518
Epoch 518 :: Batch 0/1
Batch Loss = 2.030887146167514
518, epoch_train_loss=2.030887146167514
Epoch 519
Epoch 519 :: Batch 0/1
Batch Loss = 2.028005095744118
519, epoch_train_loss=2.028005095744118
Epoch 520
Epoch 520 :: Batch 0/1
Batch Loss = 2.0261061726687077
520, epoch_train_loss=2.0261061726687077
Epoch 521
Epoch 521 :: Batch 0/1
Batch Loss = 2.0248090362465287
521, epoch_train_loss=2.0248090362465287
Epoch 522
Epoch 522 :: Batch 0/1
Batch Loss = 2.029259152637518
522, epoch_train_loss=2.029259152637518
Epoch 523
Epoch 523 :: Batch 0/1
Batch Loss = 2.0573819629228876
523, epoch_train_loss=2.0573819629228876
Epoch 524
Epoch 524 :: Batch 0/1
Batch Loss = 2.2027157590110447
524, epoch_train_loss=2.2027157590110447
Epoch 525
Epoch 525 :: Batch 0/1
Batch Loss = 2.9487076295721275
525, epoch_train_loss=2.9487076295721275
Epoch 526
Epoch 526 :: Batch 0/1
Batch Loss = 2.714085666200149
526, epoch_train_loss=2.714085666200149
Epoch 527
Epoch 527 :: Batch 0/1
Batch Loss = 2.6254763541739745
527, epoch_train_loss=2.6254763541739745
Epoch 528
Epoch 528 :: Batch 0/1
Batch Loss = 2.7483099861859532
528, epoch_train_loss=2.7483099861859532
Epoch 529
Epoch 529 :: Batch 0/1
Batch Loss = 2.9361574744310035
529, epoch_train_loss=2.9361574744310035
Epoch 530
Epoch 530 :: Batch 0/1
Batch Loss = 2.317414169757928
530, epoch_train_loss=2.317414169757928
Epoch 531
Epoch 531 :: Batch 0/1
Batch Loss = 2.5746340682459032
531, epoch_train_loss=2.5746340682459032
Epoch 532
Epoch 532 :: Batch 0/1
Batch Loss = 2.289562427423691
532, epoch_train_loss=2.289562427423691
Epoch 533
Epoch 533 :: Batch 0/1
Batch Loss = 2.492794027839878
533, epoch_train_loss=2.492794027839878
Epoch 534
Epoch 534 :: Batch 0/1
Batch Loss = 2.2741823548422175
534, epoch_train_loss=2.2741823548422175
Epoch 535
Epoch 535 :: Batch 0/1
Batch Loss = 2.2097839546214932
535, epoch_train_loss=2.2097839546214932
Epoch 536
Epoch 536 :: Batch 0/1
Batch Loss = 2.3671468721745703
536, epoch_train_loss=2.3671468721745703
Epoch 537
Epoch 537 :: Batch 0/1
Batch Loss = 2.2817083183492937
537, epoch_train_loss=2.2817083183492937
Epoch 538
Epoch 538 :: Batch 0/1
Batch Loss = 2.3009324326085547
538, epoch_train_loss=2.3009324326085547
Epoch 539
Epoch 539 :: Batch 0/1
Batch Loss = 2.3152380813942837
539, epoch_train_loss=2.3152380813942837
Epoch 540
Epoch 540 :: Batch 0/1
Batch Loss = 2.23950131275723
540, epoch_train_loss=2.23950131275723
Epoch 541
Epoch 541 :: Batch 0/1
Batch Loss = 2.1994061050210294
541, epoch_train_loss=2.1994061050210294
Epoch 542
Epoch 542 :: Batch 0/1
Batch Loss = 2.1845323977023154
542, epoch_train_loss=2.1845323977023154
Epoch 543
Epoch 543 :: Batch 0/1
Batch Loss = 2.1852239412675134
543, epoch_train_loss=2.1852239412675134
Epoch 544
Epoch 544 :: Batch 0/1
Batch Loss = 2.137500433368066
544, epoch_train_loss=2.137500433368066
Epoch 545
Epoch 545 :: Batch 0/1
Batch Loss = 2.176685492629611
545, epoch_train_loss=2.176685492629611
Epoch 546
Epoch 546 :: Batch 0/1
Batch Loss = 2.1828323286224194
546, epoch_train_loss=2.1828323286224194
Epoch 547
Epoch 547 :: Batch 0/1
Batch Loss = 2.1526506095103723
547, epoch_train_loss=2.1526506095103723
Epoch 548
Epoch 548 :: Batch 0/1
Batch Loss = 2.178938677075408
548, epoch_train_loss=2.178938677075408
Epoch 549
Epoch 549 :: Batch 0/1
Batch Loss = 2.139900739223997
549, epoch_train_loss=2.139900739223997
Epoch 550
Epoch 550 :: Batch 0/1
Batch Loss = 2.132012068467264
550, epoch_train_loss=2.132012068467264
Epoch 551
Epoch 551 :: Batch 0/1
Batch Loss = 2.134864441980536
551, epoch_train_loss=2.134864441980536
Epoch 552
Epoch 552 :: Batch 0/1
Batch Loss = 2.1118908687980986
552, epoch_train_loss=2.1118908687980986
Epoch 553
Epoch 553 :: Batch 0/1
Batch Loss = 2.1246407005364083
553, epoch_train_loss=2.1246407005364083
Epoch 554
Epoch 554 :: Batch 0/1
Batch Loss = 2.124111157318106
554, epoch_train_loss=2.124111157318106
Epoch 555
Epoch 555 :: Batch 0/1
Batch Loss = 2.111440188410293
555, epoch_train_loss=2.111440188410293
Epoch 556
Epoch 556 :: Batch 0/1
Batch Loss = 2.1162465278442903
556, epoch_train_loss=2.1162465278442903
Epoch 557
Epoch 557 :: Batch 0/1
Batch Loss = 2.1046280501229813
557, epoch_train_loss=2.1046280501229813
Epoch 558
Epoch 558 :: Batch 0/1
Batch Loss = 2.086958044338082
558, epoch_train_loss=2.086958044338082
Epoch 559
Epoch 559 :: Batch 0/1
Batch Loss = 2.092719636482852
559, epoch_train_loss=2.092719636482852
Epoch 560
Epoch 560 :: Batch 0/1
Batch Loss = 2.0800162238875397
560, epoch_train_loss=2.0800162238875397
Epoch 561
Epoch 561 :: Batch 0/1
Batch Loss = 2.0878012530634584
561, epoch_train_loss=2.0878012530634584
Epoch 562
Epoch 562 :: Batch 0/1
Batch Loss = 2.0841795283000626
562, epoch_train_loss=2.0841795283000626
Epoch 563
Epoch 563 :: Batch 0/1
Batch Loss = 2.079085571352889
563, epoch_train_loss=2.079085571352889
Epoch 564
Epoch 564 :: Batch 0/1
Batch Loss = 2.075384789009973
564, epoch_train_loss=2.075384789009973
Epoch 565
Epoch 565 :: Batch 0/1
Batch Loss = 2.0678584163533564
565, epoch_train_loss=2.0678584163533564
Epoch 566
Epoch 566 :: Batch 0/1
Batch Loss = 2.0659554281643837
566, epoch_train_loss=2.0659554281643837
Epoch 567
Epoch 567 :: Batch 0/1
Batch Loss = 2.0627600492848566
567, epoch_train_loss=2.0627600492848566
Epoch 568
Epoch 568 :: Batch 0/1
Batch Loss = 2.0642428430978024
568, epoch_train_loss=2.0642428430978024
Epoch 569
Epoch 569 :: Batch 0/1
Batch Loss = 2.0591022136990262
569, epoch_train_loss=2.0591022136990262
Epoch 570
Epoch 570 :: Batch 0/1
Batch Loss = 2.058944758668256
570, epoch_train_loss=2.058944758668256
Epoch 571
Epoch 571 :: Batch 0/1
Batch Loss = 2.0537448260427196
571, epoch_train_loss=2.0537448260427196
Epoch 572
Epoch 572 :: Batch 0/1
Batch Loss = 2.0515024156616666
572, epoch_train_loss=2.0515024156616666
Epoch 573
Epoch 573 :: Batch 0/1
Batch Loss = 2.0471392882070107
573, epoch_train_loss=2.0471392882070107
Epoch 574
Epoch 574 :: Batch 0/1
Batch Loss = 2.0482070322765367
574, epoch_train_loss=2.0482070322765367
Epoch 575
Epoch 575 :: Batch 0/1
Batch Loss = 2.043737498783984
575, epoch_train_loss=2.043737498783984
Epoch 576
Epoch 576 :: Batch 0/1
Batch Loss = 2.0438014411428096
576, epoch_train_loss=2.0438014411428096
Epoch 577
Epoch 577 :: Batch 0/1
Batch Loss = 2.0397883216325345
577, epoch_train_loss=2.0397883216325345
Epoch 578
Epoch 578 :: Batch 0/1
Batch Loss = 2.0375123207150785
578, epoch_train_loss=2.0375123207150785
Epoch 579
Epoch 579 :: Batch 0/1
Batch Loss = 2.0347999164429655
579, epoch_train_loss=2.0347999164429655
Epoch 580
Epoch 580 :: Batch 0/1
Batch Loss = 2.0336648419615564
580, epoch_train_loss=2.0336648419615564
Epoch 581
Epoch 581 :: Batch 0/1
Batch Loss = 2.0307665277474296
581, epoch_train_loss=2.0307665277474296
Epoch 582
Epoch 582 :: Batch 0/1
Batch Loss = 2.0297266764953434
582, epoch_train_loss=2.0297266764953434
Epoch 583
Epoch 583 :: Batch 0/1
Batch Loss = 2.027101797955757
583, epoch_train_loss=2.027101797955757
Epoch 584
Epoch 584 :: Batch 0/1
Batch Loss = 2.0252438222962144
584, epoch_train_loss=2.0252438222962144
Epoch 585
Epoch 585 :: Batch 0/1
Batch Loss = 2.0236234596368714
585, epoch_train_loss=2.0236234596368714
Epoch 586
Epoch 586 :: Batch 0/1
Batch Loss = 2.021432098868228
586, epoch_train_loss=2.021432098868228
Epoch 587
Epoch 587 :: Batch 0/1
Batch Loss = 2.020586912891282
587, epoch_train_loss=2.020586912891282
Epoch 588
Epoch 588 :: Batch 0/1
Batch Loss = 2.018217290248284
588, epoch_train_loss=2.018217290248284
Epoch 589
Epoch 589 :: Batch 0/1
Batch Loss = 2.016732325289151
589, epoch_train_loss=2.016732325289151
Epoch 590
Epoch 590 :: Batch 0/1
Batch Loss = 2.015093313155566
590, epoch_train_loss=2.015093313155566
Epoch 591
Epoch 591 :: Batch 0/1
Batch Loss = 2.0130470963798386
591, epoch_train_loss=2.0130470963798386
Epoch 592
Epoch 592 :: Batch 0/1
Batch Loss = 2.0118222659343044
592, epoch_train_loss=2.0118222659343044
Epoch 593
Epoch 593 :: Batch 0/1
Batch Loss = 2.0099515114500783
593, epoch_train_loss=2.0099515114500783
Epoch 594
Epoch 594 :: Batch 0/1
Batch Loss = 2.0082143410284785
594, epoch_train_loss=2.0082143410284785
Epoch 595
Epoch 595 :: Batch 0/1
Batch Loss = 2.006606643379299
595, epoch_train_loss=2.006606643379299
Epoch 596
Epoch 596 :: Batch 0/1
Batch Loss = 2.004996452505732
596, epoch_train_loss=2.004996452505732
Epoch 597
Epoch 597 :: Batch 0/1
Batch Loss = 2.0030978909640913
597, epoch_train_loss=2.0030978909640913
Epoch 598
Epoch 598 :: Batch 0/1
Batch Loss = 2.0017640813019826
598, epoch_train_loss=2.0017640813019826
Epoch 599
Epoch 599 :: Batch 0/1
Batch Loss = 2.000010307304176
599, epoch_train_loss=2.000010307304176
Epoch 600
Epoch 600 :: Batch 0/1
Batch Loss = 1.9982602780943257
600, epoch_train_loss=1.9982602780943257
Epoch 601
Epoch 601 :: Batch 0/1
Batch Loss = 1.9966157234200308
601, epoch_train_loss=1.9966157234200308
Epoch 602
Epoch 602 :: Batch 0/1
Batch Loss = 1.9950681744349377
602, epoch_train_loss=1.9950681744349377
Epoch 603
Epoch 603 :: Batch 0/1
Batch Loss = 1.9932780510476773
603, epoch_train_loss=1.9932780510476773
Epoch 604
Epoch 604 :: Batch 0/1
Batch Loss = 1.991565899357191
604, epoch_train_loss=1.991565899357191
Epoch 605
Epoch 605 :: Batch 0/1
Batch Loss = 1.9900730107513094
605, epoch_train_loss=1.9900730107513094
Epoch 606
Epoch 606 :: Batch 0/1
Batch Loss = 1.9883625721042018
606, epoch_train_loss=1.9883625721042018
Epoch 607
Epoch 607 :: Batch 0/1
Batch Loss = 1.9866022494677937
607, epoch_train_loss=1.9866022494677937
Epoch 608
Epoch 608 :: Batch 0/1
Batch Loss = 1.9849436344977336
608, epoch_train_loss=1.9849436344977336
Epoch 609
Epoch 609 :: Batch 0/1
Batch Loss = 1.9833974041105726
609, epoch_train_loss=1.9833974041105726
Epoch 610
Epoch 610 :: Batch 0/1
Batch Loss = 1.9817517886218683
610, epoch_train_loss=1.9817517886218683
Epoch 611
Epoch 611 :: Batch 0/1
Batch Loss = 1.9800569738302545
611, epoch_train_loss=1.9800569738302545
Epoch 612
Epoch 612 :: Batch 0/1
Batch Loss = 1.9783550885877756
612, epoch_train_loss=1.9783550885877756
Epoch 613
Epoch 613 :: Batch 0/1
Batch Loss = 1.9767611436661165
613, epoch_train_loss=1.9767611436661165
Epoch 614
Epoch 614 :: Batch 0/1
Batch Loss = 1.9751455411362009
614, epoch_train_loss=1.9751455411362009
Epoch 615
Epoch 615 :: Batch 0/1
Batch Loss = 1.973499016195147
615, epoch_train_loss=1.973499016195147
Epoch 616
Epoch 616 :: Batch 0/1
Batch Loss = 1.97182507594032
616, epoch_train_loss=1.97182507594032
Epoch 617
Epoch 617 :: Batch 0/1
Batch Loss = 1.970127373964744
617, epoch_train_loss=1.970127373964744
Epoch 618
Epoch 618 :: Batch 0/1
Batch Loss = 1.9684782783613701
618, epoch_train_loss=1.9684782783613701
Epoch 619
Epoch 619 :: Batch 0/1
Batch Loss = 1.9668311948845454
619, epoch_train_loss=1.9668311948845454
Epoch 620
Epoch 620 :: Batch 0/1
Batch Loss = 1.965212206909244
620, epoch_train_loss=1.965212206909244
Epoch 621
Epoch 621 :: Batch 0/1
Batch Loss = 1.9635583622091195
621, epoch_train_loss=1.9635583622091195
Epoch 622
Epoch 622 :: Batch 0/1
Batch Loss = 1.9619031397181965
622, epoch_train_loss=1.9619031397181965
Epoch 623
Epoch 623 :: Batch 0/1
Batch Loss = 1.9602155624277167
623, epoch_train_loss=1.9602155624277167
Epoch 624
Epoch 624 :: Batch 0/1
Batch Loss = 1.9585783162759796
624, epoch_train_loss=1.9585783162759796
Epoch 625
Epoch 625 :: Batch 0/1
Batch Loss = 1.9569631234970726
625, epoch_train_loss=1.9569631234970726
Epoch 626
Epoch 626 :: Batch 0/1
Batch Loss = 1.9554754278222435
626, epoch_train_loss=1.9554754278222435
Epoch 627
Epoch 627 :: Batch 0/1
Batch Loss = 1.9542554998694817
627, epoch_train_loss=1.9542554998694817
Epoch 628
Epoch 628 :: Batch 0/1
Batch Loss = 1.953761335752563
628, epoch_train_loss=1.953761335752563
Epoch 629
Epoch 629 :: Batch 0/1
Batch Loss = 1.9554848042316788
629, epoch_train_loss=1.9554848042316788
Epoch 630
Epoch 630 :: Batch 0/1
Batch Loss = 1.9632455413506509
630, epoch_train_loss=1.9632455413506509
Epoch 631
Epoch 631 :: Batch 0/1
Batch Loss = 1.9927949214231857
631, epoch_train_loss=1.9927949214231857
Epoch 632
Epoch 632 :: Batch 0/1
Batch Loss = 2.073644525039189
632, epoch_train_loss=2.073644525039189
Epoch 633
Epoch 633 :: Batch 0/1
Batch Loss = 2.3521924403312746
633, epoch_train_loss=2.3521924403312746
Epoch 634
Epoch 634 :: Batch 0/1
Batch Loss = 2.374828976515759
634, epoch_train_loss=2.374828976515759
Epoch 635
Epoch 635 :: Batch 0/1
Batch Loss = 2.263969559978471
635, epoch_train_loss=2.263969559978471
Epoch 636
Epoch 636 :: Batch 0/1
Batch Loss = 2.053033666643433
636, epoch_train_loss=2.053033666643433
Epoch 637
Epoch 637 :: Batch 0/1
Batch Loss = 2.2667432810097643
637, epoch_train_loss=2.2667432810097643
Epoch 638
Epoch 638 :: Batch 0/1
Batch Loss = 2.0433962599934565
638, epoch_train_loss=2.0433962599934565
Epoch 639
Epoch 639 :: Batch 0/1
Batch Loss = 2.0357855191903713
639, epoch_train_loss=2.0357855191903713
Epoch 640
Epoch 640 :: Batch 0/1
Batch Loss = 2.147199434566951
640, epoch_train_loss=2.147199434566951
Epoch 641
Epoch 641 :: Batch 0/1
Batch Loss = 1.9961299323542794
641, epoch_train_loss=1.9961299323542794
Epoch 642
Epoch 642 :: Batch 0/1
Batch Loss = 2.077494459804952
642, epoch_train_loss=2.077494459804952
Epoch 643
Epoch 643 :: Batch 0/1
Batch Loss = 2.0944592388206247
643, epoch_train_loss=2.0944592388206247
Epoch 644
Epoch 644 :: Batch 0/1
Batch Loss = 1.9733943752770828
644, epoch_train_loss=1.9733943752770828
Epoch 645
Epoch 645 :: Batch 0/1
Batch Loss = 2.0372934497512722
645, epoch_train_loss=2.0372934497512722
Epoch 646
Epoch 646 :: Batch 0/1
Batch Loss = 2.0122036577164377
646, epoch_train_loss=2.0122036577164377
Epoch 647
Epoch 647 :: Batch 0/1
Batch Loss = 2.004618854819549
647, epoch_train_loss=2.004618854819549
Epoch 648
Epoch 648 :: Batch 0/1
Batch Loss = 1.9994932226704207
648, epoch_train_loss=1.9994932226704207
Epoch 649
Epoch 649 :: Batch 0/1
Batch Loss = 1.975432947189107
649, epoch_train_loss=1.975432947189107
Epoch 650
Epoch 650 :: Batch 0/1
Batch Loss = 1.9925846242108125
650, epoch_train_loss=1.9925846242108125
Epoch 651
Epoch 651 :: Batch 0/1
Batch Loss = 1.993782048544387
651, epoch_train_loss=1.993782048544387
Epoch 652
Epoch 652 :: Batch 0/1
Batch Loss = 1.9764258315203393
652, epoch_train_loss=1.9764258315203393
Epoch 653
Epoch 653 :: Batch 0/1
Batch Loss = 1.9810645717966227
653, epoch_train_loss=1.9810645717966227
Epoch 654
Epoch 654 :: Batch 0/1
Batch Loss = 1.9651491438206237
654, epoch_train_loss=1.9651491438206237
Epoch 655
Epoch 655 :: Batch 0/1
Batch Loss = 1.9614184008972961
655, epoch_train_loss=1.9614184008972961
Epoch 656
Epoch 656 :: Batch 0/1
Batch Loss = 1.9718660199421474
656, epoch_train_loss=1.9718660199421474
Epoch 657
Epoch 657 :: Batch 0/1
Batch Loss = 1.9595215712452139
657, epoch_train_loss=1.9595215712452139
Epoch 658
Epoch 658 :: Batch 0/1
Batch Loss = 1.9516640630275455
658, epoch_train_loss=1.9516640630275455
Epoch 659
Epoch 659 :: Batch 0/1
Batch Loss = 1.9509419433952573
659, epoch_train_loss=1.9509419433952573
Epoch 660
Epoch 660 :: Batch 0/1
Batch Loss = 1.947229025469597
660, epoch_train_loss=1.947229025469597
Epoch 661
Epoch 661 :: Batch 0/1
Batch Loss = 1.9501670669269167
661, epoch_train_loss=1.9501670669269167
Epoch 662
Epoch 662 :: Batch 0/1
Batch Loss = 1.9448543696524179
662, epoch_train_loss=1.9448543696524179
Epoch 663
Epoch 663 :: Batch 0/1
Batch Loss = 1.937476059281931
663, epoch_train_loss=1.937476059281931
Epoch 664
Epoch 664 :: Batch 0/1
Batch Loss = 1.9416086739348335
664, epoch_train_loss=1.9416086739348335
Epoch 665
Epoch 665 :: Batch 0/1
Batch Loss = 1.9341176027299436
665, epoch_train_loss=1.9341176027299436
Epoch 666
Epoch 666 :: Batch 0/1
Batch Loss = 1.9316530245368755
666, epoch_train_loss=1.9316530245368755
Epoch 667
Epoch 667 :: Batch 0/1
Batch Loss = 1.934301368485779
667, epoch_train_loss=1.934301368485779
Epoch 668
Epoch 668 :: Batch 0/1
Batch Loss = 1.9250503285240341
668, epoch_train_loss=1.9250503285240341
Epoch 669
Epoch 669 :: Batch 0/1
Batch Loss = 1.9245515083863753
669, epoch_train_loss=1.9245515083863753
Epoch 670
Epoch 670 :: Batch 0/1
Batch Loss = 1.929035854078768
670, epoch_train_loss=1.929035854078768
Epoch 671
Epoch 671 :: Batch 0/1
Batch Loss = 1.9265289869243354
671, epoch_train_loss=1.9265289869243354
Epoch 672
Epoch 672 :: Batch 0/1
Batch Loss = 1.9227426142686395
672, epoch_train_loss=1.9227426142686395
Epoch 673
Epoch 673 :: Batch 0/1
Batch Loss = 1.9161852656834146
673, epoch_train_loss=1.9161852656834146
Epoch 674
Epoch 674 :: Batch 0/1
Batch Loss = 1.915648217646009
674, epoch_train_loss=1.915648217646009
Epoch 675
Epoch 675 :: Batch 0/1
Batch Loss = 1.9196381889011644
675, epoch_train_loss=1.9196381889011644
Epoch 676
Epoch 676 :: Batch 0/1
Batch Loss = 1.9186775971182608
676, epoch_train_loss=1.9186775971182608
Epoch 677
Epoch 677 :: Batch 0/1
Batch Loss = 1.9188627963791054
677, epoch_train_loss=1.9188627963791054
Epoch 678
Epoch 678 :: Batch 0/1
Batch Loss = 1.9140758806688827
678, epoch_train_loss=1.9140758806688827
Epoch 679
Epoch 679 :: Batch 0/1
Batch Loss = 1.907393180230243
679, epoch_train_loss=1.907393180230243
Epoch 680
Epoch 680 :: Batch 0/1
Batch Loss = 1.9049337297846396
680, epoch_train_loss=1.9049337297846396
Epoch 681
Epoch 681 :: Batch 0/1
Batch Loss = 1.9016065595469527
681, epoch_train_loss=1.9016065595469527
Epoch 682
Epoch 682 :: Batch 0/1
Batch Loss = 1.8994262636307806
682, epoch_train_loss=1.8994262636307806
Epoch 683
Epoch 683 :: Batch 0/1
Batch Loss = 1.897857688767095
683, epoch_train_loss=1.897857688767095
Epoch 684
Epoch 684 :: Batch 0/1
Batch Loss = 1.8952165155661438
684, epoch_train_loss=1.8952165155661438
Epoch 685
Epoch 685 :: Batch 0/1
Batch Loss = 1.8947514244416894
685, epoch_train_loss=1.8947514244416894
Epoch 686
Epoch 686 :: Batch 0/1
Batch Loss = 1.8930842730944004
686, epoch_train_loss=1.8930842730944004
Epoch 687
Epoch 687 :: Batch 0/1
Batch Loss = 1.8927688477472115
687, epoch_train_loss=1.8927688477472115
Epoch 688
Epoch 688 :: Batch 0/1
Batch Loss = 1.8966732282725878
688, epoch_train_loss=1.8966732282725878
Epoch 689
Epoch 689 :: Batch 0/1
Batch Loss = 1.9094421230370486
689, epoch_train_loss=1.9094421230370486
Epoch 690
Epoch 690 :: Batch 0/1
Batch Loss = 1.9661440287812826
690, epoch_train_loss=1.9661440287812826
Epoch 691
Epoch 691 :: Batch 0/1
Batch Loss = 2.176281264992146
691, epoch_train_loss=2.176281264992146
Epoch 692
Epoch 692 :: Batch 0/1
Batch Loss = 3.1887456115253214
692, epoch_train_loss=3.1887456115253214
Epoch 693
Epoch 693 :: Batch 0/1
Batch Loss = 2.2441977357009937
693, epoch_train_loss=2.2441977357009937
Epoch 694
Epoch 694 :: Batch 0/1
Batch Loss = 2.3379611360541372
694, epoch_train_loss=2.3379611360541372
Epoch 695
Epoch 695 :: Batch 0/1
Batch Loss = 2.2685157510819183
695, epoch_train_loss=2.2685157510819183
Epoch 696
Epoch 696 :: Batch 0/1
Batch Loss = 2.0197244991442758
696, epoch_train_loss=2.0197244991442758
Epoch 697
Epoch 697 :: Batch 0/1
Batch Loss = 2.399312780072298
697, epoch_train_loss=2.399312780072298
Epoch 698
Epoch 698 :: Batch 0/1
Batch Loss = 3.0109662693408894
698, epoch_train_loss=3.0109662693408894
Epoch 699
Epoch 699 :: Batch 0/1
Batch Loss = 3.0467726941810396
699, epoch_train_loss=3.0467726941810396
Epoch 700
Epoch 700 :: Batch 0/1
Batch Loss = 2.4756228802999245
700, epoch_train_loss=2.4756228802999245
Epoch 701
Epoch 701 :: Batch 0/1
Batch Loss = 2.575734639458667
701, epoch_train_loss=2.575734639458667
Epoch 702
Epoch 702 :: Batch 0/1
Batch Loss = 2.5684046946362784
702, epoch_train_loss=2.5684046946362784
Epoch 703
Epoch 703 :: Batch 0/1
Batch Loss = 2.1608649978664487
703, epoch_train_loss=2.1608649978664487
Epoch 704
Epoch 704 :: Batch 0/1
Batch Loss = 2.375912193789124
704, epoch_train_loss=2.375912193789124
Epoch 705
Epoch 705 :: Batch 0/1
Batch Loss = 2.3145166571753504
705, epoch_train_loss=2.3145166571753504
Epoch 706
Epoch 706 :: Batch 0/1
Batch Loss = 2.1452064063044056
706, epoch_train_loss=2.1452064063044056
Epoch 707
Epoch 707 :: Batch 0/1
Batch Loss = 2.2954243817089677
707, epoch_train_loss=2.2954243817089677
Epoch 708
Epoch 708 :: Batch 0/1
Batch Loss = 2.183003537508616
708, epoch_train_loss=2.183003537508616
Epoch 709
Epoch 709 :: Batch 0/1
Batch Loss = 2.125103481804061
709, epoch_train_loss=2.125103481804061
Epoch 710
Epoch 710 :: Batch 0/1
Batch Loss = 2.1740743735359978
710, epoch_train_loss=2.1740743735359978
Epoch 711
Epoch 711 :: Batch 0/1
Batch Loss = 2.0692373517447122
711, epoch_train_loss=2.0692373517447122
Epoch 712
Epoch 712 :: Batch 0/1
Batch Loss = 2.1223448930395317
712, epoch_train_loss=2.1223448930395317
Epoch 713
Epoch 713 :: Batch 0/1
Batch Loss = 2.092214407432151
713, epoch_train_loss=2.092214407432151
Epoch 714
Epoch 714 :: Batch 0/1
Batch Loss = 2.0996032239297007
714, epoch_train_loss=2.0996032239297007
Epoch 715
Epoch 715 :: Batch 0/1
Batch Loss = 2.0854493763417508
715, epoch_train_loss=2.0854493763417508
Epoch 716
Epoch 716 :: Batch 0/1
Batch Loss = 2.039740601164203
716, epoch_train_loss=2.039740601164203
Epoch 717
Epoch 717 :: Batch 0/1
Batch Loss = 2.081276966026929
717, epoch_train_loss=2.081276966026929
Epoch 718
Epoch 718 :: Batch 0/1
Batch Loss = 2.0394824262515483
718, epoch_train_loss=2.0394824262515483
Epoch 719
Epoch 719 :: Batch 0/1
Batch Loss = 2.0496788931729397
719, epoch_train_loss=2.0496788931729397
Epoch 720
Epoch 720 :: Batch 0/1
Batch Loss = 2.0078867620626055
720, epoch_train_loss=2.0078867620626055
Epoch 721
Epoch 721 :: Batch 0/1
Batch Loss = 2.045681375371196
721, epoch_train_loss=2.045681375371196
Epoch 722
Epoch 722 :: Batch 0/1
Batch Loss = 1.9857614750762822
722, epoch_train_loss=1.9857614750762822
Epoch 723
Epoch 723 :: Batch 0/1
Batch Loss = 2.0229305822784087
723, epoch_train_loss=2.0229305822784087
Epoch 724
Epoch 724 :: Batch 0/1
Batch Loss = 1.988023563609751
724, epoch_train_loss=1.988023563609751
Epoch 725
Epoch 725 :: Batch 0/1
Batch Loss = 2.007865711576034
725, epoch_train_loss=2.007865711576034
Epoch 726
Epoch 726 :: Batch 0/1
Batch Loss = 1.977004210918974
726, epoch_train_loss=1.977004210918974
Epoch 727
Epoch 727 :: Batch 0/1
Batch Loss = 1.9935459252532006
727, epoch_train_loss=1.9935459252532006
Epoch 728
Epoch 728 :: Batch 0/1
Batch Loss = 1.96566589018025
728, epoch_train_loss=1.96566589018025
Epoch 729
Epoch 729 :: Batch 0/1
Batch Loss = 1.9822709820599642
729, epoch_train_loss=1.9822709820599642
Epoch 730
Epoch 730 :: Batch 0/1
Batch Loss = 1.964426779492249
730, epoch_train_loss=1.964426779492249
Epoch 731
Epoch 731 :: Batch 0/1
Batch Loss = 1.9693260871325993
731, epoch_train_loss=1.9693260871325993
Epoch 732
Epoch 732 :: Batch 0/1
Batch Loss = 1.949765830788106
732, epoch_train_loss=1.949765830788106
Epoch 733
Epoch 733 :: Batch 0/1
Batch Loss = 1.961252568374486
733, epoch_train_loss=1.961252568374486
Epoch 734
Epoch 734 :: Batch 0/1
Batch Loss = 1.947897857788182
734, epoch_train_loss=1.947897857788182
Epoch 735
Epoch 735 :: Batch 0/1
Batch Loss = 1.953758230116515
735, epoch_train_loss=1.953758230116515
Epoch 736
Epoch 736 :: Batch 0/1
Batch Loss = 1.9424121007652766
736, epoch_train_loss=1.9424121007652766
Epoch 737
Epoch 737 :: Batch 0/1
Batch Loss = 1.949763983494547
737, epoch_train_loss=1.949763983494547
Epoch 738
Epoch 738 :: Batch 0/1
Batch Loss = 1.937336739694431
738, epoch_train_loss=1.937336739694431
Epoch 739
Epoch 739 :: Batch 0/1
Batch Loss = 1.943589546378901
739, epoch_train_loss=1.943589546378901
Epoch 740
Epoch 740 :: Batch 0/1
Batch Loss = 1.9352122857623029
740, epoch_train_loss=1.9352122857623029
Epoch 741
Epoch 741 :: Batch 0/1
Batch Loss = 1.9363950693279328
741, epoch_train_loss=1.9363950693279328
Epoch 742
Epoch 742 :: Batch 0/1
Batch Loss = 1.9319878682780978
742, epoch_train_loss=1.9319878682780978
Epoch 743
Epoch 743 :: Batch 0/1
Batch Loss = 1.9299380332195468
743, epoch_train_loss=1.9299380332195468
Epoch 744
Epoch 744 :: Batch 0/1
Batch Loss = 1.9290857482121466
744, epoch_train_loss=1.9290857482121466
Epoch 745
Epoch 745 :: Batch 0/1
Batch Loss = 1.924190966314797
745, epoch_train_loss=1.924190966314797
Epoch 746
Epoch 746 :: Batch 0/1
Batch Loss = 1.9262863074974175
746, epoch_train_loss=1.9262863074974175
Epoch 747
Epoch 747 :: Batch 0/1
Batch Loss = 1.92017159563685
747, epoch_train_loss=1.92017159563685
Epoch 748
Epoch 748 :: Batch 0/1
Batch Loss = 1.9215259115348964
748, epoch_train_loss=1.9215259115348964
Epoch 749
Epoch 749 :: Batch 0/1
Batch Loss = 1.919207743948881
749, epoch_train_loss=1.919207743948881
Epoch 750
Epoch 750 :: Batch 0/1
Batch Loss = 1.9170899112891626
750, epoch_train_loss=1.9170899112891626
Epoch 751
Epoch 751 :: Batch 0/1
Batch Loss = 1.9175726690890005
751, epoch_train_loss=1.9175726690890005
Epoch 752
Epoch 752 :: Batch 0/1
Batch Loss = 1.9132934243850164
752, epoch_train_loss=1.9132934243850164
Epoch 753
Epoch 753 :: Batch 0/1
Batch Loss = 1.9132182276667782
753, epoch_train_loss=1.9132182276667782
Epoch 754
Epoch 754 :: Batch 0/1
Batch Loss = 1.9119885756885668
754, epoch_train_loss=1.9119885756885668
Epoch 755
Epoch 755 :: Batch 0/1
Batch Loss = 1.909380211513203
755, epoch_train_loss=1.909380211513203
Epoch 756
Epoch 756 :: Batch 0/1
Batch Loss = 1.909348981110362
756, epoch_train_loss=1.909348981110362
Epoch 757
Epoch 757 :: Batch 0/1
Batch Loss = 1.9065764724679912
757, epoch_train_loss=1.9065764724679912
Epoch 758
Epoch 758 :: Batch 0/1
Batch Loss = 1.9049619011319512
758, epoch_train_loss=1.9049619011319512
Epoch 759
Epoch 759 :: Batch 0/1
Batch Loss = 1.9046396355226816
759, epoch_train_loss=1.9046396355226816
Epoch 760
Epoch 760 :: Batch 0/1
Batch Loss = 1.902175857138079
760, epoch_train_loss=1.902175857138079
Epoch 761
Epoch 761 :: Batch 0/1
Batch Loss = 1.900809843691851
761, epoch_train_loss=1.900809843691851
Epoch 762
Epoch 762 :: Batch 0/1
Batch Loss = 1.9000883772948425
762, epoch_train_loss=1.9000883772948425
Epoch 763
Epoch 763 :: Batch 0/1
Batch Loss = 1.897836035253093
763, epoch_train_loss=1.897836035253093
Epoch 764
Epoch 764 :: Batch 0/1
Batch Loss = 1.8964309431733186
764, epoch_train_loss=1.8964309431733186
Epoch 765
Epoch 765 :: Batch 0/1
Batch Loss = 1.8957770996852856
765, epoch_train_loss=1.8957770996852856
Epoch 766
Epoch 766 :: Batch 0/1
Batch Loss = 1.8939081011280718
766, epoch_train_loss=1.8939081011280718
Epoch 767
Epoch 767 :: Batch 0/1
Batch Loss = 1.892230879858934
767, epoch_train_loss=1.892230879858934
Epoch 768
Epoch 768 :: Batch 0/1
Batch Loss = 1.891355851890885
768, epoch_train_loss=1.891355851890885
Epoch 769
Epoch 769 :: Batch 0/1
Batch Loss = 1.8899685663613695
769, epoch_train_loss=1.8899685663613695
Epoch 770
Epoch 770 :: Batch 0/1
Batch Loss = 1.8882764079501024
770, epoch_train_loss=1.8882764079501024
Epoch 771
Epoch 771 :: Batch 0/1
Batch Loss = 1.8869850981619665
771, epoch_train_loss=1.8869850981619665
Epoch 772
Epoch 772 :: Batch 0/1
Batch Loss = 1.8858846566229421
772, epoch_train_loss=1.8858846566229421
Epoch 773
Epoch 773 :: Batch 0/1
Batch Loss = 1.8844463548450716
773, epoch_train_loss=1.8844463548450716
Epoch 774
Epoch 774 :: Batch 0/1
Batch Loss = 1.8828454497314064
774, epoch_train_loss=1.8828454497314064
Epoch 775
Epoch 775 :: Batch 0/1
Batch Loss = 1.8816643134337596
775, epoch_train_loss=1.8816643134337596
Epoch 776
Epoch 776 :: Batch 0/1
Batch Loss = 1.8805133091150834
776, epoch_train_loss=1.8805133091150834
Epoch 777
Epoch 777 :: Batch 0/1
Batch Loss = 1.8790178441511403
777, epoch_train_loss=1.8790178441511403
Epoch 778
Epoch 778 :: Batch 0/1
Batch Loss = 1.8775531835152703
778, epoch_train_loss=1.8775531835152703
Epoch 779
Epoch 779 :: Batch 0/1
Batch Loss = 1.8762695715375177
779, epoch_train_loss=1.8762695715375177
Epoch 780
Epoch 780 :: Batch 0/1
Batch Loss = 1.8750822762068695
780, epoch_train_loss=1.8750822762068695
Epoch 781
Epoch 781 :: Batch 0/1
Batch Loss = 1.8737946574457351
781, epoch_train_loss=1.8737946574457351
Epoch 782
Epoch 782 :: Batch 0/1
Batch Loss = 1.8723616506048075
782, epoch_train_loss=1.8723616506048075
Epoch 783
Epoch 783 :: Batch 0/1
Batch Loss = 1.8709506134249478
783, epoch_train_loss=1.8709506134249478
Epoch 784
Epoch 784 :: Batch 0/1
Batch Loss = 1.8696315366200544
784, epoch_train_loss=1.8696315366200544
Epoch 785
Epoch 785 :: Batch 0/1
Batch Loss = 1.868376490245846
785, epoch_train_loss=1.868376490245846
Epoch 786
Epoch 786 :: Batch 0/1
Batch Loss = 1.8671474880798153
786, epoch_train_loss=1.8671474880798153
Epoch 787
Epoch 787 :: Batch 0/1
Batch Loss = 1.8658841341917065
787, epoch_train_loss=1.8658841341917065
Epoch 788
Epoch 788 :: Batch 0/1
Batch Loss = 1.864552135636997
788, epoch_train_loss=1.864552135636997
Epoch 789
Epoch 789 :: Batch 0/1
Batch Loss = 1.8632049993475963
789, epoch_train_loss=1.8632049993475963
Epoch 790
Epoch 790 :: Batch 0/1
Batch Loss = 1.861833563998214
790, epoch_train_loss=1.861833563998214
Epoch 791
Epoch 791 :: Batch 0/1
Batch Loss = 1.860485478718949
791, epoch_train_loss=1.860485478718949
Epoch 792
Epoch 792 :: Batch 0/1
Batch Loss = 1.8591492437329757
792, epoch_train_loss=1.8591492437329757
Epoch 793
Epoch 793 :: Batch 0/1
Batch Loss = 1.8578167370667489
793, epoch_train_loss=1.8578167370667489
Epoch 794
Epoch 794 :: Batch 0/1
Batch Loss = 1.8564977163562062
794, epoch_train_loss=1.8564977163562062
Epoch 795
Epoch 795 :: Batch 0/1
Batch Loss = 1.855179840985702
795, epoch_train_loss=1.855179840985702
Epoch 796
Epoch 796 :: Batch 0/1
Batch Loss = 1.8538669475560496
796, epoch_train_loss=1.8538669475560496
Epoch 797
Epoch 797 :: Batch 0/1
Batch Loss = 1.8525634540834437
797, epoch_train_loss=1.8525634540834437
Epoch 798
Epoch 798 :: Batch 0/1
Batch Loss = 1.8512781450285525
798, epoch_train_loss=1.8512781450285525
Epoch 799
Epoch 799 :: Batch 0/1
Batch Loss = 1.8500820832338463
799, epoch_train_loss=1.8500820832338463
Epoch 800
Epoch 800 :: Batch 0/1
Batch Loss = 1.8490987370466012
800, epoch_train_loss=1.8490987370466012
Epoch 801
Epoch 801 :: Batch 0/1
Batch Loss = 1.8488100329455248
801, epoch_train_loss=1.8488100329455248
Epoch 802
Epoch 802 :: Batch 0/1
Batch Loss = 1.8506142662742795
802, epoch_train_loss=1.8506142662742795
Epoch 803
Epoch 803 :: Batch 0/1
Batch Loss = 1.8600861802737192
803, epoch_train_loss=1.8600861802737192
Epoch 804
Epoch 804 :: Batch 0/1
Batch Loss = 1.892794954779393
804, epoch_train_loss=1.892794954779393
Epoch 805
Epoch 805 :: Batch 0/1
Batch Loss = 2.0285143049587244
805, epoch_train_loss=2.0285143049587244
Epoch 806
Epoch 806 :: Batch 0/1
Batch Loss = 2.3390609678967595
806, epoch_train_loss=2.3390609678967595
Epoch 807
Epoch 807 :: Batch 0/1
Batch Loss = 3.4260726005535402
807, epoch_train_loss=3.4260726005535402
Epoch 808
Epoch 808 :: Batch 0/1
Batch Loss = 3.3510782319760843
808, epoch_train_loss=3.3510782319760843
Epoch 809
Epoch 809 :: Batch 0/1
Batch Loss = 2.6016882076441203
809, epoch_train_loss=2.6016882076441203
Epoch 810
Epoch 810 :: Batch 0/1
Batch Loss = 2.895199296960582
810, epoch_train_loss=2.895199296960582
Epoch 811
Epoch 811 :: Batch 0/1
Batch Loss = 3.115232465342947
811, epoch_train_loss=3.115232465342947
Epoch 812
Epoch 812 :: Batch 0/1
Batch Loss = 2.721947091294218
812, epoch_train_loss=2.721947091294218
Epoch 813
Epoch 813 :: Batch 0/1
Batch Loss = 2.7091082735564282
813, epoch_train_loss=2.7091082735564282
Epoch 814
Epoch 814 :: Batch 0/1
Batch Loss = 2.5886575021003413
814, epoch_train_loss=2.5886575021003413
Epoch 815
Epoch 815 :: Batch 0/1
Batch Loss = 3.161212673735729
815, epoch_train_loss=3.161212673735729
Epoch 816
Epoch 816 :: Batch 0/1
Batch Loss = 3.1827961431769034
816, epoch_train_loss=3.1827961431769034
Epoch 817
Epoch 817 :: Batch 0/1
Batch Loss = 3.107155910318165
817, epoch_train_loss=3.107155910318165
Epoch 818
Epoch 818 :: Batch 0/1
Batch Loss = 3.0299428672891433
818, epoch_train_loss=3.0299428672891433
Epoch 819
Epoch 819 :: Batch 0/1
Batch Loss = 2.9896875050945195
819, epoch_train_loss=2.9896875050945195
Epoch 820
Epoch 820 :: Batch 0/1
Batch Loss = 2.8545003082740092
820, epoch_train_loss=2.8545003082740092
Epoch 821
Epoch 821 :: Batch 0/1
Batch Loss = 2.7508643795634806
821, epoch_train_loss=2.7508643795634806
Epoch 822
Epoch 822 :: Batch 0/1
Batch Loss = 2.7552595821350523
822, epoch_train_loss=2.7552595821350523
Epoch 823
Epoch 823 :: Batch 0/1
Batch Loss = 2.747987135770702
823, epoch_train_loss=2.747987135770702
Epoch 824
Epoch 824 :: Batch 0/1
Batch Loss = 2.722506866202357
824, epoch_train_loss=2.722506866202357
Epoch 825
Epoch 825 :: Batch 0/1
Batch Loss = 2.7294144886892364
825, epoch_train_loss=2.7294144886892364
Epoch 826
Epoch 826 :: Batch 0/1
Batch Loss = 2.719869474369582
826, epoch_train_loss=2.719869474369582
Epoch 827
Epoch 827 :: Batch 0/1
Batch Loss = 2.6851814839550268
827, epoch_train_loss=2.6851814839550268
Epoch 828
Epoch 828 :: Batch 0/1
Batch Loss = 2.640735097190422
828, epoch_train_loss=2.640735097190422
Epoch 829
Epoch 829 :: Batch 0/1
Batch Loss = 2.5825296175528822
829, epoch_train_loss=2.5825296175528822
Epoch 830
Epoch 830 :: Batch 0/1
Batch Loss = 2.573414129573833
830, epoch_train_loss=2.573414129573833
Epoch 831
Epoch 831 :: Batch 0/1
Batch Loss = 2.582816430365909
831, epoch_train_loss=2.582816430365909
Epoch 832
Epoch 832 :: Batch 0/1
Batch Loss = 2.5435548161445083
832, epoch_train_loss=2.5435548161445083
Epoch 833
Epoch 833 :: Batch 0/1
Batch Loss = 2.4921392174282726
833, epoch_train_loss=2.4921392174282726
Epoch 834
Epoch 834 :: Batch 0/1
Batch Loss = 2.4637305687068567
834, epoch_train_loss=2.4637305687068567
Epoch 835
Epoch 835 :: Batch 0/1
Batch Loss = 2.4585388058823576
835, epoch_train_loss=2.4585388058823576
Epoch 836
Epoch 836 :: Batch 0/1
Batch Loss = 2.43625838169159
836, epoch_train_loss=2.43625838169159
Epoch 837
Epoch 837 :: Batch 0/1
Batch Loss = 2.39161188381697
837, epoch_train_loss=2.39161188381697
Epoch 838
Epoch 838 :: Batch 0/1
Batch Loss = 2.3727495681841706
838, epoch_train_loss=2.3727495681841706
Epoch 839
Epoch 839 :: Batch 0/1
Batch Loss = 2.373685109171816
839, epoch_train_loss=2.373685109171816
Epoch 840
Epoch 840 :: Batch 0/1
Batch Loss = 2.351475497232173
840, epoch_train_loss=2.351475497232173
Epoch 841
Epoch 841 :: Batch 0/1
Batch Loss = 2.334042607374084
841, epoch_train_loss=2.334042607374084
Epoch 842
Epoch 842 :: Batch 0/1
Batch Loss = 2.3151123720078717
842, epoch_train_loss=2.3151123720078717
Epoch 843
Epoch 843 :: Batch 0/1
Batch Loss = 2.300047665160727
843, epoch_train_loss=2.300047665160727
Epoch 844
Epoch 844 :: Batch 0/1
Batch Loss = 2.277280767699857
844, epoch_train_loss=2.277280767699857
Epoch 845
Epoch 845 :: Batch 0/1
Batch Loss = 2.2604571963068056
845, epoch_train_loss=2.2604571963068056
Epoch 846
Epoch 846 :: Batch 0/1
Batch Loss = 2.2501934371890497
846, epoch_train_loss=2.2501934371890497
Epoch 847
Epoch 847 :: Batch 0/1
Batch Loss = 2.2292065707766313
847, epoch_train_loss=2.2292065707766313
Epoch 848
Epoch 848 :: Batch 0/1
Batch Loss = 2.208370839998774
848, epoch_train_loss=2.208370839998774
Epoch 849
Epoch 849 :: Batch 0/1
Batch Loss = 2.2035793056095665
849, epoch_train_loss=2.2035793056095665
Epoch 850
Epoch 850 :: Batch 0/1
Batch Loss = 2.1907709608266024
850, epoch_train_loss=2.1907709608266024
Epoch 851
Epoch 851 :: Batch 0/1
Batch Loss = 2.1686005987253845
851, epoch_train_loss=2.1686005987253845
Epoch 852
Epoch 852 :: Batch 0/1
Batch Loss = 2.1563234729443916
852, epoch_train_loss=2.1563234729443916
Epoch 853
Epoch 853 :: Batch 0/1
Batch Loss = 2.1481954817034854
853, epoch_train_loss=2.1481954817034854
Epoch 854
Epoch 854 :: Batch 0/1
Batch Loss = 2.138828112032438
854, epoch_train_loss=2.138828112032438
Epoch 855
Epoch 855 :: Batch 0/1
Batch Loss = 2.1338982000116653
855, epoch_train_loss=2.1338982000116653
Epoch 856
Epoch 856 :: Batch 0/1
Batch Loss = 2.1436539172055085
856, epoch_train_loss=2.1436539172055085
Epoch 857
Epoch 857 :: Batch 0/1
Batch Loss = 2.172160073281953
857, epoch_train_loss=2.172160073281953
Epoch 858
Epoch 858 :: Batch 0/1
Batch Loss = 2.1180326128761418
858, epoch_train_loss=2.1180326128761418
Epoch 859
Epoch 859 :: Batch 0/1
Batch Loss = 2.068762281624149
859, epoch_train_loss=2.068762281624149
Epoch 860
Epoch 860 :: Batch 0/1
Batch Loss = 2.096282828727441
860, epoch_train_loss=2.096282828727441
Epoch 861
Epoch 861 :: Batch 0/1
Batch Loss = 2.1139438115493814
861, epoch_train_loss=2.1139438115493814
Epoch 862
Epoch 862 :: Batch 0/1
Batch Loss = 2.1305841983568614
862, epoch_train_loss=2.1305841983568614
Epoch 863
Epoch 863 :: Batch 0/1
Batch Loss = 2.096567302677136
863, epoch_train_loss=2.096567302677136
Epoch 864
Epoch 864 :: Batch 0/1
Batch Loss = 2.0274187207283663
864, epoch_train_loss=2.0274187207283663
Epoch 865
Epoch 865 :: Batch 0/1
Batch Loss = 1.9960716461150332
865, epoch_train_loss=1.9960716461150332
Epoch 866
Epoch 866 :: Batch 0/1
Batch Loss = 2.014543020028013
866, epoch_train_loss=2.014543020028013
Epoch 867
Epoch 867 :: Batch 0/1
Batch Loss = 2.0449823389943034
867, epoch_train_loss=2.0449823389943034
Epoch 868
Epoch 868 :: Batch 0/1
Batch Loss = 2.073298493227937
868, epoch_train_loss=2.073298493227937
Epoch 869
Epoch 869 :: Batch 0/1
Batch Loss = 2.1117293536742188
869, epoch_train_loss=2.1117293536742188
Epoch 870
Epoch 870 :: Batch 0/1
Batch Loss = 2.109719706822939
870, epoch_train_loss=2.109719706822939
Epoch 871
Epoch 871 :: Batch 0/1
Batch Loss = 2.038645212654617
871, epoch_train_loss=2.038645212654617
Epoch 872
Epoch 872 :: Batch 0/1
Batch Loss = 1.9598210774584797
872, epoch_train_loss=1.9598210774584797
Epoch 873
Epoch 873 :: Batch 0/1
Batch Loss = 1.9915536668067089
873, epoch_train_loss=1.9915536668067089
Epoch 874
Epoch 874 :: Batch 0/1
Batch Loss = 2.0677394387916066
874, epoch_train_loss=2.0677394387916066
Epoch 875
Epoch 875 :: Batch 0/1
Batch Loss = 2.064996320774356
875, epoch_train_loss=2.064996320774356
Epoch 876
Epoch 876 :: Batch 0/1
Batch Loss = 1.9948977697782162
876, epoch_train_loss=1.9948977697782162
Epoch 877
Epoch 877 :: Batch 0/1
Batch Loss = 1.9343948470985273
877, epoch_train_loss=1.9343948470985273
Epoch 878
Epoch 878 :: Batch 0/1
Batch Loss = 1.936295211661612
878, epoch_train_loss=1.936295211661612
Epoch 879
Epoch 879 :: Batch 0/1
Batch Loss = 1.9880448990754283
879, epoch_train_loss=1.9880448990754283
Epoch 880
Epoch 880 :: Batch 0/1
Batch Loss = 2.0750552373846594
880, epoch_train_loss=2.0750552373846594
Epoch 881
Epoch 881 :: Batch 0/1
Batch Loss = 2.1416875647414577
881, epoch_train_loss=2.1416875647414577
Epoch 882
Epoch 882 :: Batch 0/1
Batch Loss = 2.1305346478498235
882, epoch_train_loss=2.1305346478498235
Epoch 883
Epoch 883 :: Batch 0/1
Batch Loss = 1.940966700867146
883, epoch_train_loss=1.940966700867146
Epoch 884
Epoch 884 :: Batch 0/1
Batch Loss = 1.9955506880018685
884, epoch_train_loss=1.9955506880018685
Epoch 885
Epoch 885 :: Batch 0/1
Batch Loss = 2.062744638467753
885, epoch_train_loss=2.062744638467753
Epoch 886
Epoch 886 :: Batch 0/1
Batch Loss = 1.922060360433498
886, epoch_train_loss=1.922060360433498
Epoch 887
Epoch 887 :: Batch 0/1
Batch Loss = 2.0055973906729703
887, epoch_train_loss=2.0055973906729703
Epoch 888
Epoch 888 :: Batch 0/1
Batch Loss = 2.0872455148377376
888, epoch_train_loss=2.0872455148377376
Epoch 889
Epoch 889 :: Batch 0/1
Batch Loss = 1.9043792951106966
889, epoch_train_loss=1.9043792951106966
Epoch 890
Epoch 890 :: Batch 0/1
Batch Loss = 2.025531074508035
890, epoch_train_loss=2.025531074508035
Epoch 891
Epoch 891 :: Batch 0/1
Batch Loss = 2.140809417976004
891, epoch_train_loss=2.140809417976004
Epoch 892
Epoch 892 :: Batch 0/1
Batch Loss = 1.9045569128434838
892, epoch_train_loss=1.9045569128434838
Epoch 893
Epoch 893 :: Batch 0/1
Batch Loss = 2.2876518506861174
893, epoch_train_loss=2.2876518506861174
Epoch 894
Epoch 894 :: Batch 0/1
Batch Loss = 2.3110106227288627
894, epoch_train_loss=2.3110106227288627
Epoch 895
Epoch 895 :: Batch 0/1
Batch Loss = 2.3669567154220004
895, epoch_train_loss=2.3669567154220004
Epoch 896
Epoch 896 :: Batch 0/1
Batch Loss = 2.0372523526394164
896, epoch_train_loss=2.0372523526394164
Epoch 897
Epoch 897 :: Batch 0/1
Batch Loss = 2.4172022412317498
897, epoch_train_loss=2.4172022412317498
Epoch 898
Epoch 898 :: Batch 0/1
Batch Loss = 2.035616146525765
898, epoch_train_loss=2.035616146525765
Epoch 899
Epoch 899 :: Batch 0/1
Batch Loss = 2.269983872468426
899, epoch_train_loss=2.269983872468426
Epoch 900
Epoch 900 :: Batch 0/1
Batch Loss = 2.2476560476652687
900, epoch_train_loss=2.2476560476652687
Epoch 901
Epoch 901 :: Batch 0/1
Batch Loss = 1.9547773829722117
901, epoch_train_loss=1.9547773829722117
Epoch 902
Epoch 902 :: Batch 0/1
Batch Loss = 2.258803414378134
902, epoch_train_loss=2.258803414378134
Epoch 903
Epoch 903 :: Batch 0/1
Batch Loss = 1.9652604748269038
903, epoch_train_loss=1.9652604748269038
Epoch 904
Epoch 904 :: Batch 0/1
Batch Loss = 2.1409554510396274
904, epoch_train_loss=2.1409554510396274
Epoch 905
Epoch 905 :: Batch 0/1
Batch Loss = 1.980154849722667
905, epoch_train_loss=1.980154849722667
Epoch 906
Epoch 906 :: Batch 0/1
Batch Loss = 2.242299229582302
906, epoch_train_loss=2.242299229582302
Epoch 907
Epoch 907 :: Batch 0/1
Batch Loss = 1.9909317876994794
907, epoch_train_loss=1.9909317876994794
Epoch 908
Epoch 908 :: Batch 0/1
Batch Loss = 2.1234041007501965
908, epoch_train_loss=2.1234041007501965
Epoch 909
Epoch 909 :: Batch 0/1
Batch Loss = 2.036597466753525
909, epoch_train_loss=2.036597466753525
Epoch 910
Epoch 910 :: Batch 0/1
Batch Loss = 2.0606427713661555
910, epoch_train_loss=2.0606427713661555
Epoch 911
Epoch 911 :: Batch 0/1
Batch Loss = 2.047638917292214
911, epoch_train_loss=2.047638917292214
Epoch 912
Epoch 912 :: Batch 0/1
Batch Loss = 2.0509853492715053
912, epoch_train_loss=2.0509853492715053
Epoch 913
Epoch 913 :: Batch 0/1
Batch Loss = 2.0235427223349642
913, epoch_train_loss=2.0235427223349642
Epoch 914
Epoch 914 :: Batch 0/1
Batch Loss = 1.966357287449612
914, epoch_train_loss=1.966357287449612
Epoch 915
Epoch 915 :: Batch 0/1
Batch Loss = 2.012856879044286
915, epoch_train_loss=2.012856879044286
Epoch 916
Epoch 916 :: Batch 0/1
Batch Loss = 2.0078557083086572
916, epoch_train_loss=2.0078557083086572
Epoch 917
Epoch 917 :: Batch 0/1
Batch Loss = 1.9659250105643622
917, epoch_train_loss=1.9659250105643622
Epoch 918
Epoch 918 :: Batch 0/1
Batch Loss = 2.003453846701411
918, epoch_train_loss=2.003453846701411
Epoch 919
Epoch 919 :: Batch 0/1
Batch Loss = 1.9314698067461171
919, epoch_train_loss=1.9314698067461171
Epoch 920
Epoch 920 :: Batch 0/1
Batch Loss = 1.9566660464009062
920, epoch_train_loss=1.9566660464009062
Epoch 921
Epoch 921 :: Batch 0/1
Batch Loss = 1.9353291354160194
921, epoch_train_loss=1.9353291354160194
Epoch 922
Epoch 922 :: Batch 0/1
Batch Loss = 1.9033768225917012
922, epoch_train_loss=1.9033768225917012
Epoch 923
Epoch 923 :: Batch 0/1
Batch Loss = 1.928304991820095
923, epoch_train_loss=1.928304991820095
Epoch 924
Epoch 924 :: Batch 0/1
Batch Loss = 1.8982821613891145
924, epoch_train_loss=1.8982821613891145
Epoch 925
Epoch 925 :: Batch 0/1
Batch Loss = 1.908519305311101
925, epoch_train_loss=1.908519305311101
Epoch 926
Epoch 926 :: Batch 0/1
Batch Loss = 1.9022919889975918
926, epoch_train_loss=1.9022919889975918
Epoch 927
Epoch 927 :: Batch 0/1
Batch Loss = 1.8712480112268615
927, epoch_train_loss=1.8712480112268615
Epoch 928
Epoch 928 :: Batch 0/1
Batch Loss = 1.877495311571111
928, epoch_train_loss=1.877495311571111
Epoch 929
Epoch 929 :: Batch 0/1
Batch Loss = 1.907127702486427
929, epoch_train_loss=1.907127702486427
Epoch 930
Epoch 930 :: Batch 0/1
Batch Loss = 1.8749203398980632
930, epoch_train_loss=1.8749203398980632
Epoch 931
Epoch 931 :: Batch 0/1
Batch Loss = 1.855793879846768
931, epoch_train_loss=1.855793879846768
Epoch 932
Epoch 932 :: Batch 0/1
Batch Loss = 1.864722058762454
932, epoch_train_loss=1.864722058762454
Epoch 933
Epoch 933 :: Batch 0/1
Batch Loss = 1.8975864445256891
933, epoch_train_loss=1.8975864445256891
Epoch 934
Epoch 934 :: Batch 0/1
Batch Loss = 2.0447831574994266
934, epoch_train_loss=2.0447831574994266
Epoch 935
Epoch 935 :: Batch 0/1
Batch Loss = 1.8556695305868165
935, epoch_train_loss=1.8556695305868165
Epoch 936
Epoch 936 :: Batch 0/1
Batch Loss = 1.8345086920254998
936, epoch_train_loss=1.8345086920254998
Epoch 937
Epoch 937 :: Batch 0/1
Batch Loss = 1.8869594961218485
937, epoch_train_loss=1.8869594961218485
Epoch 938
Epoch 938 :: Batch 0/1
Batch Loss = 1.9262116668160827
938, epoch_train_loss=1.9262116668160827
Epoch 939
Epoch 939 :: Batch 0/1
Batch Loss = 2.1173798285640744
939, epoch_train_loss=2.1173798285640744
Epoch 940
Epoch 940 :: Batch 0/1
Batch Loss = 1.8295602455133344
940, epoch_train_loss=1.8295602455133344
Epoch 941
Epoch 941 :: Batch 0/1
Batch Loss = 2.4738624426506615
941, epoch_train_loss=2.4738624426506615
Epoch 942
Epoch 942 :: Batch 0/1
Batch Loss = 2.744716690378025
942, epoch_train_loss=2.744716690378025
Epoch 943
Epoch 943 :: Batch 0/1
Batch Loss = 3.09392583673859
943, epoch_train_loss=3.09392583673859
Epoch 944
Epoch 944 :: Batch 0/1
Batch Loss = 2.9578180977376514
944, epoch_train_loss=2.9578180977376514
Epoch 945
Epoch 945 :: Batch 0/1
Batch Loss = 2.695920159792325
945, epoch_train_loss=2.695920159792325
Epoch 946
Epoch 946 :: Batch 0/1
Batch Loss = 2.5441588992531856
946, epoch_train_loss=2.5441588992531856
Epoch 947
Epoch 947 :: Batch 0/1
Batch Loss = 2.5178088745867906
947, epoch_train_loss=2.5178088745867906
Epoch 948
Epoch 948 :: Batch 0/1
Batch Loss = 2.5632268285869255
948, epoch_train_loss=2.5632268285869255
Epoch 949
Epoch 949 :: Batch 0/1
Batch Loss = 2.4828262703254347
949, epoch_train_loss=2.4828262703254347
Epoch 950
Epoch 950 :: Batch 0/1
Batch Loss = 2.5736244418764422
950, epoch_train_loss=2.5736244418764422
Epoch 951
Epoch 951 :: Batch 0/1
Batch Loss = 2.3735446870241694
951, epoch_train_loss=2.3735446870241694
Epoch 952
Epoch 952 :: Batch 0/1
Batch Loss = 2.415172842358673
952, epoch_train_loss=2.415172842358673
Epoch 953
Epoch 953 :: Batch 0/1
Batch Loss = 2.2778998327633
953, epoch_train_loss=2.2778998327633
Epoch 954
Epoch 954 :: Batch 0/1
Batch Loss = 2.3483516534294377
954, epoch_train_loss=2.3483516534294377
Epoch 955
Epoch 955 :: Batch 0/1
Batch Loss = 2.288096671021615
955, epoch_train_loss=2.288096671021615
Epoch 956
Epoch 956 :: Batch 0/1
Batch Loss = 2.3069891606053132
956, epoch_train_loss=2.3069891606053132
Epoch 957
Epoch 957 :: Batch 0/1
Batch Loss = 2.290765579892195
957, epoch_train_loss=2.290765579892195
Epoch 958
Epoch 958 :: Batch 0/1
Batch Loss = 2.223383852386002
958, epoch_train_loss=2.223383852386002
Epoch 959
Epoch 959 :: Batch 0/1
Batch Loss = 2.2250043768771777
959, epoch_train_loss=2.2250043768771777
Epoch 960
Epoch 960 :: Batch 0/1
Batch Loss = 2.2171982641641574
960, epoch_train_loss=2.2171982641641574
Epoch 961
Epoch 961 :: Batch 0/1
Batch Loss = 2.180272067450607
961, epoch_train_loss=2.180272067450607
Epoch 962
Epoch 962 :: Batch 0/1
Batch Loss = 2.1503555135227455
962, epoch_train_loss=2.1503555135227455
Epoch 963
Epoch 963 :: Batch 0/1
Batch Loss = 2.13648488185005
963, epoch_train_loss=2.13648488185005
Epoch 964
Epoch 964 :: Batch 0/1
Batch Loss = 2.1332073614425218
964, epoch_train_loss=2.1332073614425218
Epoch 965
Epoch 965 :: Batch 0/1
Batch Loss = 2.125683175150103
965, epoch_train_loss=2.125683175150103
Epoch 966
Epoch 966 :: Batch 0/1
Batch Loss = 2.120356861161797
966, epoch_train_loss=2.120356861161797
Epoch 967
Epoch 967 :: Batch 0/1
Batch Loss = 2.1218181641077645
967, epoch_train_loss=2.1218181641077645
Epoch 968
Epoch 968 :: Batch 0/1
Batch Loss = 2.113092547918077
968, epoch_train_loss=2.113092547918077
Epoch 969
Epoch 969 :: Batch 0/1
Batch Loss = 2.089964632279546
969, epoch_train_loss=2.089964632279546
Epoch 970
Epoch 970 :: Batch 0/1
Batch Loss = 2.068810532837022
970, epoch_train_loss=2.068810532837022
Epoch 971
Epoch 971 :: Batch 0/1
Batch Loss = 2.053963616622162
971, epoch_train_loss=2.053963616622162
Epoch 972
Epoch 972 :: Batch 0/1
Batch Loss = 2.04294801705645
972, epoch_train_loss=2.04294801705645
Epoch 973
Epoch 973 :: Batch 0/1
Batch Loss = 2.0352227314047013
973, epoch_train_loss=2.0352227314047013
Epoch 974
Epoch 974 :: Batch 0/1
Batch Loss = 2.0234004546644906
974, epoch_train_loss=2.0234004546644906
Epoch 975
Epoch 975 :: Batch 0/1
Batch Loss = 2.0123464972802934
975, epoch_train_loss=2.0123464972802934
Epoch 976
Epoch 976 :: Batch 0/1
Batch Loss = 2.002695353318593
976, epoch_train_loss=2.002695353318593
Epoch 977
Epoch 977 :: Batch 0/1
Batch Loss = 1.9898006341536918
977, epoch_train_loss=1.9898006341536918
Epoch 978
Epoch 978 :: Batch 0/1
Batch Loss = 1.9836967055332821
978, epoch_train_loss=1.9836967055332821
Epoch 979
Epoch 979 :: Batch 0/1
Batch Loss = 1.9820675112758661
979, epoch_train_loss=1.9820675112758661
Epoch 980
Epoch 980 :: Batch 0/1
Batch Loss = 1.9773898345037126
980, epoch_train_loss=1.9773898345037126
Epoch 981
Epoch 981 :: Batch 0/1
Batch Loss = 1.9701556510413631
981, epoch_train_loss=1.9701556510413631
Epoch 982
Epoch 982 :: Batch 0/1
Batch Loss = 1.9640060609508092
982, epoch_train_loss=1.9640060609508092
Epoch 983
Epoch 983 :: Batch 0/1
Batch Loss = 1.9607175910327583
983, epoch_train_loss=1.9607175910327583
Epoch 984
Epoch 984 :: Batch 0/1
Batch Loss = 1.9569330014076207
984, epoch_train_loss=1.9569330014076207
Epoch 985
Epoch 985 :: Batch 0/1
Batch Loss = 1.9549196189946305
985, epoch_train_loss=1.9549196189946305
Epoch 986
Epoch 986 :: Batch 0/1
Batch Loss = 1.9516665037504495
986, epoch_train_loss=1.9516665037504495
Epoch 987
Epoch 987 :: Batch 0/1
Batch Loss = 1.946156593284104
987, epoch_train_loss=1.946156593284104
Epoch 988
Epoch 988 :: Batch 0/1
Batch Loss = 1.9382182544427011
988, epoch_train_loss=1.9382182544427011
Epoch 989
Epoch 989 :: Batch 0/1
Batch Loss = 1.9335339773325178
989, epoch_train_loss=1.9335339773325178
Epoch 990
Epoch 990 :: Batch 0/1
Batch Loss = 1.931113740661097
990, epoch_train_loss=1.931113740661097
Epoch 991
Epoch 991 :: Batch 0/1
Batch Loss = 1.928619453658504
991, epoch_train_loss=1.928619453658504
Epoch 992
Epoch 992 :: Batch 0/1
Batch Loss = 1.9238553576274882
992, epoch_train_loss=1.9238553576274882
Epoch 993
Epoch 993 :: Batch 0/1
Batch Loss = 1.9195279519835913
993, epoch_train_loss=1.9195279519835913
Epoch 994
Epoch 994 :: Batch 0/1
Batch Loss = 1.9160631904071022
994, epoch_train_loss=1.9160631904071022
Epoch 995
Epoch 995 :: Batch 0/1
Batch Loss = 1.9135088029440581
995, epoch_train_loss=1.9135088029440581
Epoch 996
Epoch 996 :: Batch 0/1
Batch Loss = 1.9110177988848027
996, epoch_train_loss=1.9110177988848027
Epoch 997
Epoch 997 :: Batch 0/1
Batch Loss = 1.90819358590891
997, epoch_train_loss=1.90819358590891
Epoch 998
Epoch 998 :: Batch 0/1
Batch Loss = 1.9050116160091621
998, epoch_train_loss=1.9050116160091621
Epoch 999
Epoch 999 :: Batch 0/1
Batch Loss = 1.901819552813095
999, epoch_train_loss=1.901819552813095
Epoch 1000
Epoch 1000 :: Batch 0/1
Batch Loss = 1.8999869591263805
1000, epoch_train_loss=1.8999869591263805
Epoch 1001
Epoch 1001 :: Batch 0/1
Batch Loss = 1.8989187130601846
1001, epoch_train_loss=1.8989187130601846
Epoch 1002
Epoch 1002 :: Batch 0/1
Batch Loss = 1.8975662216130134
1002, epoch_train_loss=1.8975662216130134
Epoch 1003
Epoch 1003 :: Batch 0/1
Batch Loss = 1.8954997169499466
1003, epoch_train_loss=1.8954997169499466
Epoch 1004
Epoch 1004 :: Batch 0/1
Batch Loss = 1.893186118467906
1004, epoch_train_loss=1.893186118467906
Epoch 1005
Epoch 1005 :: Batch 0/1
Batch Loss = 1.8915560943162766
1005, epoch_train_loss=1.8915560943162766
Epoch 1006
Epoch 1006 :: Batch 0/1
Batch Loss = 1.8900460206407688
1006, epoch_train_loss=1.8900460206407688
Epoch 1007
Epoch 1007 :: Batch 0/1
Batch Loss = 1.888183876322363
1007, epoch_train_loss=1.888183876322363
Epoch 1008
Epoch 1008 :: Batch 0/1
Batch Loss = 1.8863524169163093
1008, epoch_train_loss=1.8863524169163093
Epoch 1009
Epoch 1009 :: Batch 0/1
Batch Loss = 1.8848012502011284
1009, epoch_train_loss=1.8848012502011284
Epoch 1010
Epoch 1010 :: Batch 0/1
Batch Loss = 1.8835411761730638
1010, epoch_train_loss=1.8835411761730638
Epoch 1011
Epoch 1011 :: Batch 0/1
Batch Loss = 1.8822058458923556
1011, epoch_train_loss=1.8822058458923556
Epoch 1012
Epoch 1012 :: Batch 0/1
Batch Loss = 1.880474824759924
1012, epoch_train_loss=1.880474824759924
Epoch 1013
Epoch 1013 :: Batch 0/1
Batch Loss = 1.8787798566802203
1013, epoch_train_loss=1.8787798566802203
Epoch 1014
Epoch 1014 :: Batch 0/1
Batch Loss = 1.877554397172684
1014, epoch_train_loss=1.877554397172684
Epoch 1015
Epoch 1015 :: Batch 0/1
Batch Loss = 1.8765050697921157
1015, epoch_train_loss=1.8765050697921157
Epoch 1016
Epoch 1016 :: Batch 0/1
Batch Loss = 1.8751823497402733
1016, epoch_train_loss=1.8751823497402733
Epoch 1017
Epoch 1017 :: Batch 0/1
Batch Loss = 1.8736561856360996
1017, epoch_train_loss=1.8736561856360996
Epoch 1018
Epoch 1018 :: Batch 0/1
Batch Loss = 1.87227036500562
1018, epoch_train_loss=1.87227036500562
Epoch 1019
Epoch 1019 :: Batch 0/1
Batch Loss = 1.8710639502167654
1019, epoch_train_loss=1.8710639502167654
Epoch 1020
Epoch 1020 :: Batch 0/1
Batch Loss = 1.8698547347276158
1020, epoch_train_loss=1.8698547347276158
Epoch 1021
Epoch 1021 :: Batch 0/1
Batch Loss = 1.8685483172282795
1021, epoch_train_loss=1.8685483172282795
Epoch 1022
Epoch 1022 :: Batch 0/1
Batch Loss = 1.8672040029690073
1022, epoch_train_loss=1.8672040029690073
Epoch 1023
Epoch 1023 :: Batch 0/1
Batch Loss = 1.865911465827323
1023, epoch_train_loss=1.865911465827323
Epoch 1024
Epoch 1024 :: Batch 0/1
Batch Loss = 1.864645356803869
1024, epoch_train_loss=1.864645356803869
Epoch 1025
Epoch 1025 :: Batch 0/1
Batch Loss = 1.8633395244868498
1025, epoch_train_loss=1.8633395244868498
Epoch 1026
Epoch 1026 :: Batch 0/1
Batch Loss = 1.8620255841481057
1026, epoch_train_loss=1.8620255841481057
Epoch 1027
Epoch 1027 :: Batch 0/1
Batch Loss = 1.860819153506217
1027, epoch_train_loss=1.860819153506217
Epoch 1028
Epoch 1028 :: Batch 0/1
Batch Loss = 1.8596920491922015
1028, epoch_train_loss=1.8596920491922015
Epoch 1029
Epoch 1029 :: Batch 0/1
Batch Loss = 1.858485285860873
1029, epoch_train_loss=1.858485285860873
Epoch 1030
Epoch 1030 :: Batch 0/1
Batch Loss = 1.8572192074108191
1030, epoch_train_loss=1.8572192074108191
Epoch 1031
Epoch 1031 :: Batch 0/1
Batch Loss = 1.8560040190218912
1031, epoch_train_loss=1.8560040190218912
Epoch 1032
Epoch 1032 :: Batch 0/1
Batch Loss = 1.85484927780617
1032, epoch_train_loss=1.85484927780617
Epoch 1033
Epoch 1033 :: Batch 0/1
Batch Loss = 1.8537019325749475
1033, epoch_train_loss=1.8537019325749475
Epoch 1034
Epoch 1034 :: Batch 0/1
Batch Loss = 1.8525297109738632
1034, epoch_train_loss=1.8525297109738632
Epoch 1035
Epoch 1035 :: Batch 0/1
Batch Loss = 1.8513655555545907
1035, epoch_train_loss=1.8513655555545907
Epoch 1036
Epoch 1036 :: Batch 0/1
Batch Loss = 1.8502300141263714
1036, epoch_train_loss=1.8502300141263714
Epoch 1037
Epoch 1037 :: Batch 0/1
Batch Loss = 1.849085593027381
1037, epoch_train_loss=1.849085593027381
Epoch 1038
Epoch 1038 :: Batch 0/1
Batch Loss = 1.8479262274543662
1038, epoch_train_loss=1.8479262274543662
Epoch 1039
Epoch 1039 :: Batch 0/1
Batch Loss = 1.8467701213697274
1039, epoch_train_loss=1.8467701213697274
Epoch 1040
Epoch 1040 :: Batch 0/1
Batch Loss = 1.8456316104792536
1040, epoch_train_loss=1.8456316104792536
Epoch 1041
Epoch 1041 :: Batch 0/1
Batch Loss = 1.844498751122265
1041, epoch_train_loss=1.844498751122265
Epoch 1042
Epoch 1042 :: Batch 0/1
Batch Loss = 1.8433449436790903
1042, epoch_train_loss=1.8433449436790903
Epoch 1043
Epoch 1043 :: Batch 0/1
Batch Loss = 1.8422120368181398
1043, epoch_train_loss=1.8422120368181398
Epoch 1044
Epoch 1044 :: Batch 0/1
Batch Loss = 1.841217811176923
1044, epoch_train_loss=1.841217811176923
Epoch 1045
Epoch 1045 :: Batch 0/1
Batch Loss = 1.8406609929023516
1045, epoch_train_loss=1.8406609929023516
Epoch 1046
Epoch 1046 :: Batch 0/1
Batch Loss = 1.8420342415547617
1046, epoch_train_loss=1.8420342415547617
Epoch 1047
Epoch 1047 :: Batch 0/1
Batch Loss = 1.8506108886020234
1047, epoch_train_loss=1.8506108886020234
Epoch 1048
Epoch 1048 :: Batch 0/1
Batch Loss = 1.8987571487404775
1048, epoch_train_loss=1.8987571487404775
Epoch 1049
Epoch 1049 :: Batch 0/1
Batch Loss = 2.013050054840389
1049, epoch_train_loss=2.013050054840389
Epoch 1050
Epoch 1050 :: Batch 0/1
Batch Loss = 2.5805430708642305
1050, epoch_train_loss=2.5805430708642305
Epoch 1051
Epoch 1051 :: Batch 0/1
Batch Loss = 2.101345049176006
1051, epoch_train_loss=2.101345049176006
Epoch 1052
Epoch 1052 :: Batch 0/1
Batch Loss = 2.2438435871666123
1052, epoch_train_loss=2.2438435871666123
Epoch 1053
Epoch 1053 :: Batch 0/1
Batch Loss = 2.0601318018777706
1053, epoch_train_loss=2.0601318018777706
Epoch 1054
Epoch 1054 :: Batch 0/1
Batch Loss = 2.1683945922874353
1054, epoch_train_loss=2.1683945922874353
Epoch 1055
Epoch 1055 :: Batch 0/1
Batch Loss = 1.935483267159682
1055, epoch_train_loss=1.935483267159682
Epoch 1056
Epoch 1056 :: Batch 0/1
Batch Loss = 2.0643322249222846
1056, epoch_train_loss=2.0643322249222846
Epoch 1057
Epoch 1057 :: Batch 0/1
Batch Loss = 1.933341638581512
1057, epoch_train_loss=1.933341638581512
Epoch 1058
Epoch 1058 :: Batch 0/1
Batch Loss = 2.107103755141144
1058, epoch_train_loss=2.107103755141144
Epoch 1059
Epoch 1059 :: Batch 0/1
Batch Loss = 1.9473734443193071
1059, epoch_train_loss=1.9473734443193071
Epoch 1060
Epoch 1060 :: Batch 0/1
Batch Loss = 2.009996211627031
1060, epoch_train_loss=2.009996211627031
Epoch 1061
Epoch 1061 :: Batch 0/1
Batch Loss = 1.9454531843082221
1061, epoch_train_loss=1.9454531843082221
Epoch 1062
Epoch 1062 :: Batch 0/1
Batch Loss = 1.99483414804624
1062, epoch_train_loss=1.99483414804624
Epoch 1063
Epoch 1063 :: Batch 0/1
Batch Loss = 1.9121594160906104
1063, epoch_train_loss=1.9121594160906104
Epoch 1064
Epoch 1064 :: Batch 0/1
Batch Loss = 1.9350560925737983
1064, epoch_train_loss=1.9350560925737983
Epoch 1065
Epoch 1065 :: Batch 0/1
Batch Loss = 1.9380440650809647
1065, epoch_train_loss=1.9380440650809647
Epoch 1066
Epoch 1066 :: Batch 0/1
Batch Loss = 1.947446996854015
1066, epoch_train_loss=1.947446996854015
Epoch 1067
Epoch 1067 :: Batch 0/1
Batch Loss = 1.9379443821965885
1067, epoch_train_loss=1.9379443821965885
Epoch 1068
Epoch 1068 :: Batch 0/1
Batch Loss = 1.9175318217049553
1068, epoch_train_loss=1.9175318217049553
Epoch 1069
Epoch 1069 :: Batch 0/1
Batch Loss = 1.9227250085934509
1069, epoch_train_loss=1.9227250085934509
Epoch 1070
Epoch 1070 :: Batch 0/1
Batch Loss = 1.8964120645450728
1070, epoch_train_loss=1.8964120645450728
Epoch 1071
Epoch 1071 :: Batch 0/1
Batch Loss = 1.9129301642196348
1071, epoch_train_loss=1.9129301642196348
Epoch 1072
Epoch 1072 :: Batch 0/1
Batch Loss = 1.8950425261935866
1072, epoch_train_loss=1.8950425261935866
Epoch 1073
Epoch 1073 :: Batch 0/1
Batch Loss = 1.907447030328844
1073, epoch_train_loss=1.907447030328844
Epoch 1074
Epoch 1074 :: Batch 0/1
Batch Loss = 1.8836356380486758
1074, epoch_train_loss=1.8836356380486758
Epoch 1075
Epoch 1075 :: Batch 0/1
Batch Loss = 1.900774523649442
1075, epoch_train_loss=1.900774523649442
Epoch 1076
Epoch 1076 :: Batch 0/1
Batch Loss = 1.8829839074482715
1076, epoch_train_loss=1.8829839074482715
Epoch 1077
Epoch 1077 :: Batch 0/1
Batch Loss = 1.890167011018896
1077, epoch_train_loss=1.890167011018896
Epoch 1078
Epoch 1078 :: Batch 0/1
Batch Loss = 1.8694616153841268
1078, epoch_train_loss=1.8694616153841268
Epoch 1079
Epoch 1079 :: Batch 0/1
Batch Loss = 1.8782846089762955
1079, epoch_train_loss=1.8782846089762955
Epoch 1080
Epoch 1080 :: Batch 0/1
Batch Loss = 1.865103681351313
1080, epoch_train_loss=1.865103681351313
Epoch 1081
Epoch 1081 :: Batch 0/1
Batch Loss = 1.8712783606293697
1081, epoch_train_loss=1.8712783606293697
Epoch 1082
Epoch 1082 :: Batch 0/1
Batch Loss = 1.861562552083986
1082, epoch_train_loss=1.861562552083986
Epoch 1083
Epoch 1083 :: Batch 0/1
Batch Loss = 1.8669897727026974
1083, epoch_train_loss=1.8669897727026974
Epoch 1084
Epoch 1084 :: Batch 0/1
Batch Loss = 1.8516599370406301
1084, epoch_train_loss=1.8516599370406301
Epoch 1085
Epoch 1085 :: Batch 0/1
Batch Loss = 1.854876564287845
1085, epoch_train_loss=1.854876564287845
Epoch 1086
Epoch 1086 :: Batch 0/1
Batch Loss = 1.8460905551187226
1086, epoch_train_loss=1.8460905551187226
Epoch 1087
Epoch 1087 :: Batch 0/1
Batch Loss = 1.849453377103061
1087, epoch_train_loss=1.849453377103061
Epoch 1088
Epoch 1088 :: Batch 0/1
Batch Loss = 1.8437307204798354
1088, epoch_train_loss=1.8437307204798354
Epoch 1089
Epoch 1089 :: Batch 0/1
Batch Loss = 1.841600975468358
1089, epoch_train_loss=1.841600975468358
Epoch 1090
Epoch 1090 :: Batch 0/1
Batch Loss = 1.8406689224640964
1090, epoch_train_loss=1.8406689224640964
Epoch 1091
Epoch 1091 :: Batch 0/1
Batch Loss = 1.833413242839016
1091, epoch_train_loss=1.833413242839016
Epoch 1092
Epoch 1092 :: Batch 0/1
Batch Loss = 1.8356911704040382
1092, epoch_train_loss=1.8356911704040382
Epoch 1093
Epoch 1093 :: Batch 0/1
Batch Loss = 1.8311221278182563
1093, epoch_train_loss=1.8311221278182563
Epoch 1094
Epoch 1094 :: Batch 0/1
Batch Loss = 1.8303892351041415
1094, epoch_train_loss=1.8303892351041415
Epoch 1095
Epoch 1095 :: Batch 0/1
Batch Loss = 1.8296894555460874
1095, epoch_train_loss=1.8296894555460874
Epoch 1096
Epoch 1096 :: Batch 0/1
Batch Loss = 1.8247029737556741
1096, epoch_train_loss=1.8247029737556741
Epoch 1097
Epoch 1097 :: Batch 0/1
Batch Loss = 1.8249464197287062
1097, epoch_train_loss=1.8249464197287062
Epoch 1098
Epoch 1098 :: Batch 0/1
Batch Loss = 1.8222571803561654
1098, epoch_train_loss=1.8222571803561654
Epoch 1099
Epoch 1099 :: Batch 0/1
Batch Loss = 1.8192790575925086
1099, epoch_train_loss=1.8192790575925086
Epoch 1100
Epoch 1100 :: Batch 0/1
Batch Loss = 1.8202683754018392
1100, epoch_train_loss=1.8202683754018392
Epoch 1101
Epoch 1101 :: Batch 0/1
Batch Loss = 1.817468491843781
1101, epoch_train_loss=1.817468491843781
Epoch 1102
Epoch 1102 :: Batch 0/1
Batch Loss = 1.8143541797413756
1102, epoch_train_loss=1.8143541797413756
Epoch 1103
Epoch 1103 :: Batch 0/1
Batch Loss = 1.8145385613584033
1103, epoch_train_loss=1.8145385613584033
Epoch 1104
Epoch 1104 :: Batch 0/1
Batch Loss = 1.8126695487241746
1104, epoch_train_loss=1.8126695487241746
Epoch 1105
Epoch 1105 :: Batch 0/1
Batch Loss = 1.8100990406880157
1105, epoch_train_loss=1.8100990406880157
Epoch 1106
Epoch 1106 :: Batch 0/1
Batch Loss = 1.8099569897349708
1106, epoch_train_loss=1.8099569897349708
Epoch 1107
Epoch 1107 :: Batch 0/1
Batch Loss = 1.8091717074966656
1107, epoch_train_loss=1.8091717074966656
Epoch 1108
Epoch 1108 :: Batch 0/1
Batch Loss = 1.8064449046051845
1108, epoch_train_loss=1.8064449046051845
Epoch 1109
Epoch 1109 :: Batch 0/1
Batch Loss = 1.805009392102831
1109, epoch_train_loss=1.805009392102831
Epoch 1110
Epoch 1110 :: Batch 0/1
Batch Loss = 1.8048874240097401
1110, epoch_train_loss=1.8048874240097401
Epoch 1111
Epoch 1111 :: Batch 0/1
Batch Loss = 1.803648005626882
1111, epoch_train_loss=1.803648005626882
Epoch 1112
Epoch 1112 :: Batch 0/1
Batch Loss = 1.8015519914050246
1112, epoch_train_loss=1.8015519914050246
Epoch 1113
Epoch 1113 :: Batch 0/1
Batch Loss = 1.8002446225479583
1113, epoch_train_loss=1.8002446225479583
Epoch 1114
Epoch 1114 :: Batch 0/1
Batch Loss = 1.799621989226354
1114, epoch_train_loss=1.799621989226354
Epoch 1115
Epoch 1115 :: Batch 0/1
Batch Loss = 1.7984693076281564
1115, epoch_train_loss=1.7984693076281564
Epoch 1116
Epoch 1116 :: Batch 0/1
Batch Loss = 1.7967123964731015
1116, epoch_train_loss=1.7967123964731015
Epoch 1117
Epoch 1117 :: Batch 0/1
Batch Loss = 1.7952236445109857
1117, epoch_train_loss=1.7952236445109857
Epoch 1118
Epoch 1118 :: Batch 0/1
Batch Loss = 1.7943129935984616
1118, epoch_train_loss=1.7943129935984616
Epoch 1119
Epoch 1119 :: Batch 0/1
Batch Loss = 1.7934485308198498
1119, epoch_train_loss=1.7934485308198498
Epoch 1120
Epoch 1120 :: Batch 0/1
Batch Loss = 1.7921082068846068
1120, epoch_train_loss=1.7921082068846068
Epoch 1121
Epoch 1121 :: Batch 0/1
Batch Loss = 1.7905270625092562
1121, epoch_train_loss=1.7905270625092562
Epoch 1122
Epoch 1122 :: Batch 0/1
Batch Loss = 1.7892572013087327
1122, epoch_train_loss=1.7892572013087327
Epoch 1123
Epoch 1123 :: Batch 0/1
Batch Loss = 1.7883504688179666
1123, epoch_train_loss=1.7883504688179666
Epoch 1124
Epoch 1124 :: Batch 0/1
Batch Loss = 1.7874288522879271
1124, epoch_train_loss=1.7874288522879271
Epoch 1125
Epoch 1125 :: Batch 0/1
Batch Loss = 1.7862359815405737
1125, epoch_train_loss=1.7862359815405737
Epoch 1126
Epoch 1126 :: Batch 0/1
Batch Loss = 1.7848717059972388
1126, epoch_train_loss=1.7848717059972388
Epoch 1127
Epoch 1127 :: Batch 0/1
Batch Loss = 1.783540232286226
1127, epoch_train_loss=1.783540232286226
Epoch 1128
Epoch 1128 :: Batch 0/1
Batch Loss = 1.7823410879645256
1128, epoch_train_loss=1.7823410879645256
Epoch 1129
Epoch 1129 :: Batch 0/1
Batch Loss = 1.7812854139970333
1129, epoch_train_loss=1.7812854139970333
Epoch 1130
Epoch 1130 :: Batch 0/1
Batch Loss = 1.7802723010337298
1130, epoch_train_loss=1.7802723010337298
Epoch 1131
Epoch 1131 :: Batch 0/1
Batch Loss = 1.7792242156172033
1131, epoch_train_loss=1.7792242156172033
Epoch 1132
Epoch 1132 :: Batch 0/1
Batch Loss = 1.7781456885132865
1132, epoch_train_loss=1.7781456885132865
Epoch 1133
Epoch 1133 :: Batch 0/1
Batch Loss = 1.7770368836857784
1133, epoch_train_loss=1.7770368836857784
Epoch 1134
Epoch 1134 :: Batch 0/1
Batch Loss = 1.7759215658661343
1134, epoch_train_loss=1.7759215658661343
Epoch 1135
Epoch 1135 :: Batch 0/1
Batch Loss = 1.7747937229143798
1135, epoch_train_loss=1.7747937229143798
Epoch 1136
Epoch 1136 :: Batch 0/1
Batch Loss = 1.773717766608237
1136, epoch_train_loss=1.773717766608237
Epoch 1137
Epoch 1137 :: Batch 0/1
Batch Loss = 1.7727364485622648
1137, epoch_train_loss=1.7727364485622648
Epoch 1138
Epoch 1138 :: Batch 0/1
Batch Loss = 1.771974934704696
1138, epoch_train_loss=1.771974934704696
Epoch 1139
Epoch 1139 :: Batch 0/1
Batch Loss = 1.7716204790554597
1139, epoch_train_loss=1.7716204790554597
Epoch 1140
Epoch 1140 :: Batch 0/1
Batch Loss = 1.77229220284374
1140, epoch_train_loss=1.77229220284374
Epoch 1141
Epoch 1141 :: Batch 0/1
Batch Loss = 1.7750083796585956
1141, epoch_train_loss=1.7750083796585956
Epoch 1142
Epoch 1142 :: Batch 0/1
Batch Loss = 1.7839905573441097
1142, epoch_train_loss=1.7839905573441097
Epoch 1143
Epoch 1143 :: Batch 0/1
Batch Loss = 1.805799614494436
1143, epoch_train_loss=1.805799614494436
Epoch 1144
Epoch 1144 :: Batch 0/1
Batch Loss = 1.8765230368850518
1144, epoch_train_loss=1.8765230368850518
Epoch 1145
Epoch 1145 :: Batch 0/1
Batch Loss = 2.0053115961846526
1145, epoch_train_loss=2.0053115961846526
Epoch 1146
Epoch 1146 :: Batch 0/1
Batch Loss = 2.4306832350066236
1146, epoch_train_loss=2.4306832350066236
Epoch 1147
Epoch 1147 :: Batch 0/1
Batch Loss = 1.8944756384705628
1147, epoch_train_loss=1.8944756384705628
Epoch 1148
Epoch 1148 :: Batch 0/1
Batch Loss = 1.8717806219631223
1148, epoch_train_loss=1.8717806219631223
Epoch 1149
Epoch 1149 :: Batch 0/1
Batch Loss = 2.1561181188078336
1149, epoch_train_loss=2.1561181188078336
Epoch 1150
Epoch 1150 :: Batch 0/1
Batch Loss = 1.8550261889606279
1150, epoch_train_loss=1.8550261889606279
Epoch 1151
Epoch 1151 :: Batch 0/1
Batch Loss = 1.8378242225923316
1151, epoch_train_loss=1.8378242225923316
Epoch 1152
Epoch 1152 :: Batch 0/1
Batch Loss = 1.9400630034399386
1152, epoch_train_loss=1.9400630034399386
Epoch 1153
Epoch 1153 :: Batch 0/1
Batch Loss = 1.8294669406488393
1153, epoch_train_loss=1.8294669406488393
Epoch 1154
Epoch 1154 :: Batch 0/1
Batch Loss = 1.9019600566691586
1154, epoch_train_loss=1.9019600566691586
Epoch 1155
Epoch 1155 :: Batch 0/1
Batch Loss = 1.853820907391158
1155, epoch_train_loss=1.853820907391158
Epoch 1156
Epoch 1156 :: Batch 0/1
Batch Loss = 1.8205043448670637
1156, epoch_train_loss=1.8205043448670637
Epoch 1157
Epoch 1157 :: Batch 0/1
Batch Loss = 1.9115121629482128
1157, epoch_train_loss=1.9115121629482128
Epoch 1158
Epoch 1158 :: Batch 0/1
Batch Loss = 1.925509914977216
1158, epoch_train_loss=1.925509914977216
Epoch 1159
Epoch 1159 :: Batch 0/1
Batch Loss = 1.8291437027775332
1159, epoch_train_loss=1.8291437027775332
Epoch 1160
Epoch 1160 :: Batch 0/1
Batch Loss = 1.9110408254818414
1160, epoch_train_loss=1.9110408254818414
Epoch 1161
Epoch 1161 :: Batch 0/1
Batch Loss = 1.8103508892629274
1161, epoch_train_loss=1.8103508892629274
Epoch 1162
Epoch 1162 :: Batch 0/1
Batch Loss = 1.86536052425414
1162, epoch_train_loss=1.86536052425414
Epoch 1163
Epoch 1163 :: Batch 0/1
Batch Loss = 1.8916654165105766
1163, epoch_train_loss=1.8916654165105766
Epoch 1164
Epoch 1164 :: Batch 0/1
Batch Loss = 1.8064098339497965
1164, epoch_train_loss=1.8064098339497965
Epoch 1165
Epoch 1165 :: Batch 0/1
Batch Loss = 1.8302200408153628
1165, epoch_train_loss=1.8302200408153628
Epoch 1166
Epoch 1166 :: Batch 0/1
Batch Loss = 1.834765400664894
1166, epoch_train_loss=1.834765400664894
Epoch 1167
Epoch 1167 :: Batch 0/1
Batch Loss = 1.7943117153299473
1167, epoch_train_loss=1.7943117153299473
Epoch 1168
Epoch 1168 :: Batch 0/1
Batch Loss = 1.8330560121836728
1168, epoch_train_loss=1.8330560121836728
Epoch 1169
Epoch 1169 :: Batch 0/1
Batch Loss = 1.812942091846424
1169, epoch_train_loss=1.812942091846424
Epoch 1170
Epoch 1170 :: Batch 0/1
Batch Loss = 1.7847927630176252
1170, epoch_train_loss=1.7847927630176252
Epoch 1171
Epoch 1171 :: Batch 0/1
Batch Loss = 1.8010199760951895
1171, epoch_train_loss=1.8010199760951895
Epoch 1172
Epoch 1172 :: Batch 0/1
Batch Loss = 1.7954795746462213
1172, epoch_train_loss=1.7954795746462213
Epoch 1173
Epoch 1173 :: Batch 0/1
Batch Loss = 1.7904442559422127
1173, epoch_train_loss=1.7904442559422127
Epoch 1174
Epoch 1174 :: Batch 0/1
Batch Loss = 1.792704516914022
1174, epoch_train_loss=1.792704516914022
Epoch 1175
Epoch 1175 :: Batch 0/1
Batch Loss = 1.78597546081395
1175, epoch_train_loss=1.78597546081395
Epoch 1176
Epoch 1176 :: Batch 0/1
Batch Loss = 1.7860675511650437
1176, epoch_train_loss=1.7860675511650437
Epoch 1177
Epoch 1177 :: Batch 0/1
Batch Loss = 1.7879595487710669
1177, epoch_train_loss=1.7879595487710669
Epoch 1178
Epoch 1178 :: Batch 0/1
Batch Loss = 1.7809563490185176
1178, epoch_train_loss=1.7809563490185176
Epoch 1179
Epoch 1179 :: Batch 0/1
Batch Loss = 1.7784922126591807
1179, epoch_train_loss=1.7784922126591807
Epoch 1180
Epoch 1180 :: Batch 0/1
Batch Loss = 1.7798661064614718
1180, epoch_train_loss=1.7798661064614718
Epoch 1181
Epoch 1181 :: Batch 0/1
Batch Loss = 1.7733518366546628
1181, epoch_train_loss=1.7733518366546628
Epoch 1182
Epoch 1182 :: Batch 0/1
Batch Loss = 1.771017147415846
1182, epoch_train_loss=1.771017147415846
Epoch 1183
Epoch 1183 :: Batch 0/1
Batch Loss = 1.7725520970811186
1183, epoch_train_loss=1.7725520970811186
Epoch 1184
Epoch 1184 :: Batch 0/1
Batch Loss = 1.7716700537154655
1184, epoch_train_loss=1.7716700537154655
Epoch 1185
Epoch 1185 :: Batch 0/1
Batch Loss = 1.7670245999354506
1185, epoch_train_loss=1.7670245999354506
Epoch 1186
Epoch 1186 :: Batch 0/1
Batch Loss = 1.7636761220997765
1186, epoch_train_loss=1.7636761220997765
Epoch 1187
Epoch 1187 :: Batch 0/1
Batch Loss = 1.7660942679238443
1187, epoch_train_loss=1.7660942679238443
Epoch 1188
Epoch 1188 :: Batch 0/1
Batch Loss = 1.7677341599969072
1188, epoch_train_loss=1.7677341599969072
Epoch 1189
Epoch 1189 :: Batch 0/1
Batch Loss = 1.7619647103377332
1189, epoch_train_loss=1.7619647103377332
Epoch 1190
Epoch 1190 :: Batch 0/1
Batch Loss = 1.7566995202302382
1190, epoch_train_loss=1.7566995202302382
Epoch 1191
Epoch 1191 :: Batch 0/1
Batch Loss = 1.7569523186794627
1191, epoch_train_loss=1.7569523186794627
Epoch 1192
Epoch 1192 :: Batch 0/1
Batch Loss = 1.757407269896104
1192, epoch_train_loss=1.757407269896104
Epoch 1193
Epoch 1193 :: Batch 0/1
Batch Loss = 1.7555652856174646
1193, epoch_train_loss=1.7555652856174646
Epoch 1194
Epoch 1194 :: Batch 0/1
Batch Loss = 1.7533041536604814
1194, epoch_train_loss=1.7533041536604814
Epoch 1195
Epoch 1195 :: Batch 0/1
Batch Loss = 1.7507669286558765
1195, epoch_train_loss=1.7507669286558765
Epoch 1196
Epoch 1196 :: Batch 0/1
Batch Loss = 1.7487647802895092
1196, epoch_train_loss=1.7487647802895092
Epoch 1197
Epoch 1197 :: Batch 0/1
Batch Loss = 1.7482175045353265
1197, epoch_train_loss=1.7482175045353265
Epoch 1198
Epoch 1198 :: Batch 0/1
Batch Loss = 1.7471477883448545
1198, epoch_train_loss=1.7471477883448545
Epoch 1199
Epoch 1199 :: Batch 0/1
Batch Loss = 1.7456098561058573
1199, epoch_train_loss=1.7456098561058573
Epoch 1200
Epoch 1200 :: Batch 0/1
Batch Loss = 1.744492626340647
1200, epoch_train_loss=1.744492626340647
Epoch 1201
Epoch 1201 :: Batch 0/1
Batch Loss = 1.7428024718561166
1201, epoch_train_loss=1.7428024718561166
Epoch 1202
Epoch 1202 :: Batch 0/1
Batch Loss = 1.7409964461590386
1202, epoch_train_loss=1.7409964461590386
Epoch 1203
Epoch 1203 :: Batch 0/1
Batch Loss = 1.739568727651137
1203, epoch_train_loss=1.739568727651137
Epoch 1204
Epoch 1204 :: Batch 0/1
Batch Loss = 1.737911859154509
1204, epoch_train_loss=1.737911859154509
Epoch 1205
Epoch 1205 :: Batch 0/1
Batch Loss = 1.736441814993535
1205, epoch_train_loss=1.736441814993535
Epoch 1206
Epoch 1206 :: Batch 0/1
Batch Loss = 1.73517420674663
1206, epoch_train_loss=1.73517420674663
Epoch 1207
Epoch 1207 :: Batch 0/1
Batch Loss = 1.7336013927724778
1207, epoch_train_loss=1.7336013927724778
Epoch 1208
Epoch 1208 :: Batch 0/1
Batch Loss = 1.732315952907233
1208, epoch_train_loss=1.732315952907233
Epoch 1209
Epoch 1209 :: Batch 0/1
Batch Loss = 1.7311969822990452
1209, epoch_train_loss=1.7311969822990452
Epoch 1210
Epoch 1210 :: Batch 0/1
Batch Loss = 1.7300886819384895
1210, epoch_train_loss=1.7300886819384895
Epoch 1211
Epoch 1211 :: Batch 0/1
Batch Loss = 1.7309228325861812
1211, epoch_train_loss=1.7309228325861812
Epoch 1212
Epoch 1212 :: Batch 0/1
Batch Loss = 1.7354296885657374
1212, epoch_train_loss=1.7354296885657374
Epoch 1213
Epoch 1213 :: Batch 0/1
Batch Loss = 1.7558758135183168
1213, epoch_train_loss=1.7558758135183168
Epoch 1214
Epoch 1214 :: Batch 0/1
Batch Loss = 1.7983513279361774
1214, epoch_train_loss=1.7983513279361774
Epoch 1215
Epoch 1215 :: Batch 0/1
Batch Loss = 1.991894817149136
1215, epoch_train_loss=1.991894817149136
Epoch 1216
Epoch 1216 :: Batch 0/1
Batch Loss = 2.3016015092253066
1216, epoch_train_loss=2.3016015092253066
Epoch 1217
Epoch 1217 :: Batch 0/1
Batch Loss = 3.7218304600044725
1217, epoch_train_loss=3.7218304600044725
Epoch 1218
Epoch 1218 :: Batch 0/1
Batch Loss = 3.46487444807418
1218, epoch_train_loss=3.46487444807418
Epoch 1219
Epoch 1219 :: Batch 0/1
Batch Loss = 3.6246300508191114
1219, epoch_train_loss=3.6246300508191114
Epoch 1220
Epoch 1220 :: Batch 0/1
Batch Loss = 3.0846215776595587
1220, epoch_train_loss=3.0846215776595587
Epoch 1221
Epoch 1221 :: Batch 0/1
Batch Loss = 2.833078763677544
1221, epoch_train_loss=2.833078763677544
Epoch 1222
Epoch 1222 :: Batch 0/1
Batch Loss = 2.703412974572488
1222, epoch_train_loss=2.703412974572488
Epoch 1223
Epoch 1223 :: Batch 0/1
Batch Loss = 2.555578629663764
1223, epoch_train_loss=2.555578629663764
Epoch 1224
Epoch 1224 :: Batch 0/1
Batch Loss = 2.6642415049908106
1224, epoch_train_loss=2.6642415049908106
Epoch 1225
Epoch 1225 :: Batch 0/1
Batch Loss = 2.3192949300970835
1225, epoch_train_loss=2.3192949300970835
Epoch 1226
Epoch 1226 :: Batch 0/1
Batch Loss = 2.725493911902035
1226, epoch_train_loss=2.725493911902035
Epoch 1227
Epoch 1227 :: Batch 0/1
Batch Loss = 2.3491182465175404
1227, epoch_train_loss=2.3491182465175404
Epoch 1228
Epoch 1228 :: Batch 0/1
Batch Loss = 2.5140024661116573
1228, epoch_train_loss=2.5140024661116573
Epoch 1229
Epoch 1229 :: Batch 0/1
Batch Loss = 2.4481177512385566
1229, epoch_train_loss=2.4481177512385566
Epoch 1230
Epoch 1230 :: Batch 0/1
Batch Loss = 2.5246125550480563
1230, epoch_train_loss=2.5246125550480563
Epoch 1231
Epoch 1231 :: Batch 0/1
Batch Loss = 2.47044314467145
1231, epoch_train_loss=2.47044314467145
Epoch 1232
Epoch 1232 :: Batch 0/1
Batch Loss = 2.427505165465209
1232, epoch_train_loss=2.427505165465209
Epoch 1233
Epoch 1233 :: Batch 0/1
Batch Loss = 2.314209412129893
1233, epoch_train_loss=2.314209412129893
Epoch 1234
Epoch 1234 :: Batch 0/1
Batch Loss = 2.2874102960420593
1234, epoch_train_loss=2.2874102960420593
Epoch 1235
Epoch 1235 :: Batch 0/1
Batch Loss = 2.199907967101413
1235, epoch_train_loss=2.199907967101413
Epoch 1236
Epoch 1236 :: Batch 0/1
Batch Loss = 2.144785356965126
1236, epoch_train_loss=2.144785356965126
Epoch 1237
Epoch 1237 :: Batch 0/1
Batch Loss = 2.1077337413338615
1237, epoch_train_loss=2.1077337413338615
Epoch 1238
Epoch 1238 :: Batch 0/1
Batch Loss = 2.051596528447532
1238, epoch_train_loss=2.051596528447532
Epoch 1239
Epoch 1239 :: Batch 0/1
Batch Loss = 3.9340896185615275
1239, epoch_train_loss=3.9340896185615275
Epoch 1240
Epoch 1240 :: Batch 0/1
Batch Loss = 2.14834522499729
1240, epoch_train_loss=2.14834522499729
Epoch 1241
Epoch 1241 :: Batch 0/1
Batch Loss = 2.264222861553971
1241, epoch_train_loss=2.264222861553971
Epoch 1242
Epoch 1242 :: Batch 0/1
Batch Loss = 2.3631863089263883
1242, epoch_train_loss=2.3631863089263883
Epoch 1243
Epoch 1243 :: Batch 0/1
Batch Loss = 2.415082859447623
1243, epoch_train_loss=2.415082859447623
Epoch 1244
Epoch 1244 :: Batch 0/1
Batch Loss = 2.445786179812129
1244, epoch_train_loss=2.445786179812129
Epoch 1245
Epoch 1245 :: Batch 0/1
Batch Loss = 2.412071338532293
1245, epoch_train_loss=2.412071338532293
Epoch 1246
Epoch 1246 :: Batch 0/1
Batch Loss = 2.379553704328893
1246, epoch_train_loss=2.379553704328893
Epoch 1247
Epoch 1247 :: Batch 0/1
Batch Loss = 2.3821141494952314
1247, epoch_train_loss=2.3821141494952314
Epoch 1248
Epoch 1248 :: Batch 0/1
Batch Loss = 2.342771299575065
1248, epoch_train_loss=2.342771299575065
Epoch 1249
Epoch 1249 :: Batch 0/1
Batch Loss = 2.3677159993108563
1249, epoch_train_loss=2.3677159993108563
Epoch 1250
Epoch 1250 :: Batch 0/1
Batch Loss = 2.3077801415320476
1250, epoch_train_loss=2.3077801415320476
Epoch 1251
Epoch 1251 :: Batch 0/1
Batch Loss = 2.2947532463661173
1251, epoch_train_loss=2.2947532463661173
Epoch 1252
Epoch 1252 :: Batch 0/1
Batch Loss = 2.294890633326874
1252, epoch_train_loss=2.294890633326874
Epoch 1253
Epoch 1253 :: Batch 0/1
Batch Loss = 2.2625165709062705
1253, epoch_train_loss=2.2625165709062705
Epoch 1254
Epoch 1254 :: Batch 0/1
Batch Loss = 2.2650246852868294
1254, epoch_train_loss=2.2650246852868294
Epoch 1255
Epoch 1255 :: Batch 0/1
Batch Loss = 2.2318722568427134
1255, epoch_train_loss=2.2318722568427134
Epoch 1256
Epoch 1256 :: Batch 0/1
Batch Loss = 2.222600439563249
1256, epoch_train_loss=2.222600439563249
Epoch 1257
Epoch 1257 :: Batch 0/1
Batch Loss = 2.1806936507172296
1257, epoch_train_loss=2.1806936507172296
Epoch 1258
Epoch 1258 :: Batch 0/1
Batch Loss = 2.1711805465218528
1258, epoch_train_loss=2.1711805465218528
Epoch 1259
Epoch 1259 :: Batch 0/1
Batch Loss = 2.132614637198811
1259, epoch_train_loss=2.132614637198811
Epoch 1260
Epoch 1260 :: Batch 0/1
Batch Loss = 2.1104025347151962
1260, epoch_train_loss=2.1104025347151962
Epoch 1261
Epoch 1261 :: Batch 0/1
Batch Loss = 2.093642693058766
1261, epoch_train_loss=2.093642693058766
Epoch 1262
Epoch 1262 :: Batch 0/1
Batch Loss = 2.0578282760313145
1262, epoch_train_loss=2.0578282760313145
Epoch 1263
Epoch 1263 :: Batch 0/1
Batch Loss = 2.0389735079218383
1263, epoch_train_loss=2.0389735079218383
Epoch 1264
Epoch 1264 :: Batch 0/1
Batch Loss = 2.020389691238899
1264, epoch_train_loss=2.020389691238899
Epoch 1265
Epoch 1265 :: Batch 0/1
Batch Loss = 1.9867662609145946
1265, epoch_train_loss=1.9867662609145946
Epoch 1266
Epoch 1266 :: Batch 0/1
Batch Loss = 1.978556049490662
1266, epoch_train_loss=1.978556049490662
Epoch 1267
Epoch 1267 :: Batch 0/1
Batch Loss = 1.9711726838472616
1267, epoch_train_loss=1.9711726838472616
Epoch 1268
Epoch 1268 :: Batch 0/1
Batch Loss = 1.9608334210869731
1268, epoch_train_loss=1.9608334210869731
Epoch 1269
Epoch 1269 :: Batch 0/1
Batch Loss = 1.94299711249461
1269, epoch_train_loss=1.94299711249461
Epoch 1270
Epoch 1270 :: Batch 0/1
Batch Loss = 1.9230965914310942
1270, epoch_train_loss=1.9230965914310942
Epoch 1271
Epoch 1271 :: Batch 0/1
Batch Loss = 1.9229270986332723
1271, epoch_train_loss=1.9229270986332723
Epoch 1272
Epoch 1272 :: Batch 0/1
Batch Loss = 1.9106407528631917
1272, epoch_train_loss=1.9106407528631917
Epoch 1273
Epoch 1273 :: Batch 0/1
Batch Loss = 1.895532813691562
1273, epoch_train_loss=1.895532813691562
Epoch 1274
Epoch 1274 :: Batch 0/1
Batch Loss = 1.8936906936582503
1274, epoch_train_loss=1.8936906936582503
Epoch 1275
Epoch 1275 :: Batch 0/1
Batch Loss = 1.879762535628171
1275, epoch_train_loss=1.879762535628171
Epoch 1276
Epoch 1276 :: Batch 0/1
Batch Loss = 1.8790102419820276
1276, epoch_train_loss=1.8790102419820276
Epoch 1277
Epoch 1277 :: Batch 0/1
Batch Loss = 1.8672980474482708
1277, epoch_train_loss=1.8672980474482708
Epoch 1278
Epoch 1278 :: Batch 0/1
Batch Loss = 1.8681490063546815
1278, epoch_train_loss=1.8681490063546815
Epoch 1279
Epoch 1279 :: Batch 0/1
Batch Loss = 1.867460630056079
1279, epoch_train_loss=1.867460630056079
Epoch 1280
Epoch 1280 :: Batch 0/1
Batch Loss = 1.8849902099799107
1280, epoch_train_loss=1.8849902099799107
Epoch 1281
Epoch 1281 :: Batch 0/1
Batch Loss = 1.901711726723518
1281, epoch_train_loss=1.901711726723518
Epoch 1282
Epoch 1282 :: Batch 0/1
Batch Loss = 1.9185106860903727
1282, epoch_train_loss=1.9185106860903727
Epoch 1283
Epoch 1283 :: Batch 0/1
Batch Loss = 1.8636158303312729
1283, epoch_train_loss=1.8636158303312729
Epoch 1284
Epoch 1284 :: Batch 0/1
Batch Loss = 1.8331119529893336
1284, epoch_train_loss=1.8331119529893336
Epoch 1285
Epoch 1285 :: Batch 0/1
Batch Loss = 1.8294101548164168
1285, epoch_train_loss=1.8294101548164168
Epoch 1286
Epoch 1286 :: Batch 0/1
Batch Loss = 1.8434803122848922
1286, epoch_train_loss=1.8434803122848922
Epoch 1287
Epoch 1287 :: Batch 0/1
Batch Loss = 1.848858097303516
1287, epoch_train_loss=1.848858097303516
Epoch 1288
Epoch 1288 :: Batch 0/1
Batch Loss = 1.8208452815864065
1288, epoch_train_loss=1.8208452815864065
Epoch 1289
Epoch 1289 :: Batch 0/1
Batch Loss = 1.8072446235805755
1289, epoch_train_loss=1.8072446235805755
Epoch 1290
Epoch 1290 :: Batch 0/1
Batch Loss = 1.8060027808996582
1290, epoch_train_loss=1.8060027808996582
Epoch 1291
Epoch 1291 :: Batch 0/1
Batch Loss = 1.8180087417406245
1291, epoch_train_loss=1.8180087417406245
Epoch 1292
Epoch 1292 :: Batch 0/1
Batch Loss = 1.8327242722504642
1292, epoch_train_loss=1.8327242722504642
Epoch 1293
Epoch 1293 :: Batch 0/1
Batch Loss = 1.8297341829129852
1293, epoch_train_loss=1.8297341829129852
Epoch 1294
Epoch 1294 :: Batch 0/1
Batch Loss = 1.8500060670688003
1294, epoch_train_loss=1.8500060670688003
Epoch 1295
Epoch 1295 :: Batch 0/1
Batch Loss = 1.9038679304128538
1295, epoch_train_loss=1.9038679304128538
Epoch 1296
Epoch 1296 :: Batch 0/1
Batch Loss = 1.984950154959495
1296, epoch_train_loss=1.984950154959495
Epoch 1297
Epoch 1297 :: Batch 0/1
Batch Loss = 1.8406531229298122
1297, epoch_train_loss=1.8406531229298122
Epoch 1298
Epoch 1298 :: Batch 0/1
Batch Loss = 1.7974349312180564
1298, epoch_train_loss=1.7974349312180564
Epoch 1299
Epoch 1299 :: Batch 0/1
Batch Loss = 1.9515037069380137
1299, epoch_train_loss=1.9515037069380137
Epoch 1300
Epoch 1300 :: Batch 0/1
Batch Loss = 2.306978178888774
1300, epoch_train_loss=2.306978178888774
Epoch 1301
Epoch 1301 :: Batch 0/1
Batch Loss = 2.287326369252699
1301, epoch_train_loss=2.287326369252699
Epoch 1302
Epoch 1302 :: Batch 0/1
Batch Loss = 2.445187806543273
1302, epoch_train_loss=2.445187806543273
Epoch 1303
Epoch 1303 :: Batch 0/1
Batch Loss = 3.31770535394059
1303, epoch_train_loss=3.31770535394059
Epoch 1304
Epoch 1304 :: Batch 0/1
Batch Loss = 3.3020313763793965
1304, epoch_train_loss=3.3020313763793965
Epoch 1305
Epoch 1305 :: Batch 0/1
Batch Loss = 2.7569951798270798
1305, epoch_train_loss=2.7569951798270798
Epoch 1306
Epoch 1306 :: Batch 0/1
Batch Loss = 2.7047707839866892
1306, epoch_train_loss=2.7047707839866892
Epoch 1307
Epoch 1307 :: Batch 0/1
Batch Loss = 2.5106093992745953
1307, epoch_train_loss=2.5106093992745953
Epoch 1308
Epoch 1308 :: Batch 0/1
Batch Loss = 2.2499737950969747
1308, epoch_train_loss=2.2499737950969747
Epoch 1309
Epoch 1309 :: Batch 0/1
Batch Loss = 2.377929585085084
1309, epoch_train_loss=2.377929585085084
Epoch 1310
Epoch 1310 :: Batch 0/1
Batch Loss = 2.5588180300224757
1310, epoch_train_loss=2.5588180300224757
Epoch 1311
Epoch 1311 :: Batch 0/1
Batch Loss = 2.306583579619122
1311, epoch_train_loss=2.306583579619122
Epoch 1312
Epoch 1312 :: Batch 0/1
Batch Loss = 2.3452404619924994
1312, epoch_train_loss=2.3452404619924994
Epoch 1313
Epoch 1313 :: Batch 0/1
Batch Loss = 2.0853890216407023
1313, epoch_train_loss=2.0853890216407023
Epoch 1314
Epoch 1314 :: Batch 0/1
Batch Loss = 2.21559129160831
1314, epoch_train_loss=2.21559129160831
Epoch 1315
Epoch 1315 :: Batch 0/1
Batch Loss = 2.3510081281528072
1315, epoch_train_loss=2.3510081281528072
Epoch 1316
Epoch 1316 :: Batch 0/1
Batch Loss = 2.3086043030709797
1316, epoch_train_loss=2.3086043030709797
Epoch 1317
Epoch 1317 :: Batch 0/1
Batch Loss = 2.187614928723189
1317, epoch_train_loss=2.187614928723189
Epoch 1318
Epoch 1318 :: Batch 0/1
Batch Loss = 2.108179783416595
1318, epoch_train_loss=2.108179783416595
Epoch 1319
Epoch 1319 :: Batch 0/1
Batch Loss = 2.0785376978636703
1319, epoch_train_loss=2.0785376978636703
Epoch 1320
Epoch 1320 :: Batch 0/1
Batch Loss = 2.188600144450648
1320, epoch_train_loss=2.188600144450648
Epoch 1321
Epoch 1321 :: Batch 0/1
Batch Loss = 2.126524483068076
1321, epoch_train_loss=2.126524483068076
Epoch 1322
Epoch 1322 :: Batch 0/1
Batch Loss = 2.0876678722600115
1322, epoch_train_loss=2.0876678722600115
Epoch 1323
Epoch 1323 :: Batch 0/1
Batch Loss = 2.0495744826415336
1323, epoch_train_loss=2.0495744826415336
Epoch 1324
Epoch 1324 :: Batch 0/1
Batch Loss = 1.980590642708702
1324, epoch_train_loss=1.980590642708702
Epoch 1325
Epoch 1325 :: Batch 0/1
Batch Loss = 2.017642459654156
1325, epoch_train_loss=2.017642459654156
Epoch 1326
Epoch 1326 :: Batch 0/1
Batch Loss = 2.0201003971680396
1326, epoch_train_loss=2.0201003971680396
Epoch 1327
Epoch 1327 :: Batch 0/1
Batch Loss = 1.9502581845434803
1327, epoch_train_loss=1.9502581845434803
Epoch 1328
Epoch 1328 :: Batch 0/1
Batch Loss = 1.9557084711202315
1328, epoch_train_loss=1.9557084711202315
Epoch 1329
Epoch 1329 :: Batch 0/1
Batch Loss = 1.946088787183823
1329, epoch_train_loss=1.946088787183823
Epoch 1330
Epoch 1330 :: Batch 0/1
Batch Loss = 1.976686912699942
1330, epoch_train_loss=1.976686912699942
Epoch 1331
Epoch 1331 :: Batch 0/1
Batch Loss = 1.909126915981483
1331, epoch_train_loss=1.909126915981483
Epoch 1332
Epoch 1332 :: Batch 0/1
Batch Loss = 1.9062752316263192
1332, epoch_train_loss=1.9062752316263192
Epoch 1333
Epoch 1333 :: Batch 0/1
Batch Loss = 1.8975473754289904
1333, epoch_train_loss=1.8975473754289904
Epoch 1334
Epoch 1334 :: Batch 0/1
Batch Loss = 1.9186619279323187
1334, epoch_train_loss=1.9186619279323187
Epoch 1335
Epoch 1335 :: Batch 0/1
Batch Loss = 1.9043804832567814
1335, epoch_train_loss=1.9043804832567814
Epoch 1336
Epoch 1336 :: Batch 0/1
Batch Loss = 1.8801205633136457
1336, epoch_train_loss=1.8801205633136457
Epoch 1337
Epoch 1337 :: Batch 0/1
Batch Loss = 1.8804206623034
1337, epoch_train_loss=1.8804206623034
Epoch 1338
Epoch 1338 :: Batch 0/1
Batch Loss = 1.8772794513770557
1338, epoch_train_loss=1.8772794513770557
Epoch 1339
Epoch 1339 :: Batch 0/1
Batch Loss = 1.8708137440285135
1339, epoch_train_loss=1.8708137440285135
Epoch 1340
Epoch 1340 :: Batch 0/1
Batch Loss = 1.852962016547001
1340, epoch_train_loss=1.852962016547001
Epoch 1341
Epoch 1341 :: Batch 0/1
Batch Loss = 1.8431624262667627
1341, epoch_train_loss=1.8431624262667627
Epoch 1342
Epoch 1342 :: Batch 0/1
Batch Loss = 1.8494009441738093
1342, epoch_train_loss=1.8494009441738093
Epoch 1343
Epoch 1343 :: Batch 0/1
Batch Loss = 1.8493089877706046
1343, epoch_train_loss=1.8493089877706046
Epoch 1344
Epoch 1344 :: Batch 0/1
Batch Loss = 1.8467191496396018
1344, epoch_train_loss=1.8467191496396018
Epoch 1345
Epoch 1345 :: Batch 0/1
Batch Loss = 1.8350103011743135
1345, epoch_train_loss=1.8350103011743135
Epoch 1346
Epoch 1346 :: Batch 0/1
Batch Loss = 1.8318919450239555
1346, epoch_train_loss=1.8318919450239555
Epoch 1347
Epoch 1347 :: Batch 0/1
Batch Loss = 1.8242892813595857
1347, epoch_train_loss=1.8242892813595857
Epoch 1348
Epoch 1348 :: Batch 0/1
Batch Loss = 1.8254762821798847
1348, epoch_train_loss=1.8254762821798847
Epoch 1349
Epoch 1349 :: Batch 0/1
Batch Loss = 1.8155386376959117
1349, epoch_train_loss=1.8155386376959117
Epoch 1350
Epoch 1350 :: Batch 0/1
Batch Loss = 1.8131350316641777
1350, epoch_train_loss=1.8131350316641777
Epoch 1351
Epoch 1351 :: Batch 0/1
Batch Loss = 1.8076258509989769
1351, epoch_train_loss=1.8076258509989769
Epoch 1352
Epoch 1352 :: Batch 0/1
Batch Loss = 1.8036737036085773
1352, epoch_train_loss=1.8036737036085773
Epoch 1353
Epoch 1353 :: Batch 0/1
Batch Loss = 1.7985732617275456
1353, epoch_train_loss=1.7985732617275456
Epoch 1354
Epoch 1354 :: Batch 0/1
Batch Loss = 1.7913899000696278
1354, epoch_train_loss=1.7913899000696278
Epoch 1355
Epoch 1355 :: Batch 0/1
Batch Loss = 1.791609700901732
1355, epoch_train_loss=1.791609700901732
Epoch 1356
Epoch 1356 :: Batch 0/1
Batch Loss = 1.7899328685130047
1356, epoch_train_loss=1.7899328685130047
Epoch 1357
Epoch 1357 :: Batch 0/1
Batch Loss = 1.7847697293807443
1357, epoch_train_loss=1.7847697293807443
Epoch 1358
Epoch 1358 :: Batch 0/1
Batch Loss = 1.7826279745019253
1358, epoch_train_loss=1.7826279745019253
Epoch 1359
Epoch 1359 :: Batch 0/1
Batch Loss = 1.782977957764544
1359, epoch_train_loss=1.782977957764544
Epoch 1360
Epoch 1360 :: Batch 0/1
Batch Loss = 1.7810602793507175
1360, epoch_train_loss=1.7810602793507175
Epoch 1361
Epoch 1361 :: Batch 0/1
Batch Loss = 1.7772992556317646
1361, epoch_train_loss=1.7772992556317646
Epoch 1362
Epoch 1362 :: Batch 0/1
Batch Loss = 1.7748656947454369
1362, epoch_train_loss=1.7748656947454369
Epoch 1363
Epoch 1363 :: Batch 0/1
Batch Loss = 1.7734027984778853
1363, epoch_train_loss=1.7734027984778853
Epoch 1364
Epoch 1364 :: Batch 0/1
Batch Loss = 1.770920264148736
1364, epoch_train_loss=1.770920264148736
Epoch 1365
Epoch 1365 :: Batch 0/1
Batch Loss = 1.7673763828944107
1365, epoch_train_loss=1.7673763828944107
Epoch 1366
Epoch 1366 :: Batch 0/1
Batch Loss = 1.763671870530637
1366, epoch_train_loss=1.763671870530637
Epoch 1367
Epoch 1367 :: Batch 0/1
Batch Loss = 1.7610822023964898
1367, epoch_train_loss=1.7610822023964898
Epoch 1368
Epoch 1368 :: Batch 0/1
Batch Loss = 1.759730556796281
1368, epoch_train_loss=1.759730556796281
Epoch 1369
Epoch 1369 :: Batch 0/1
Batch Loss = 1.7584484170826138
1369, epoch_train_loss=1.7584484170826138
Epoch 1370
Epoch 1370 :: Batch 0/1
Batch Loss = 1.7564293756690619
1370, epoch_train_loss=1.7564293756690619
Epoch 1371
Epoch 1371 :: Batch 0/1
Batch Loss = 1.7540303529349506
1371, epoch_train_loss=1.7540303529349506
Epoch 1372
Epoch 1372 :: Batch 0/1
Batch Loss = 1.7516256784272741
1372, epoch_train_loss=1.7516256784272741
Epoch 1373
Epoch 1373 :: Batch 0/1
Batch Loss = 1.749000485395227
1373, epoch_train_loss=1.749000485395227
Epoch 1374
Epoch 1374 :: Batch 0/1
Batch Loss = 1.7468418541701298
1374, epoch_train_loss=1.7468418541701298
Epoch 1375
Epoch 1375 :: Batch 0/1
Batch Loss = 1.7452345635015516
1375, epoch_train_loss=1.7452345635015516
Epoch 1376
Epoch 1376 :: Batch 0/1
Batch Loss = 1.7437957044555032
1376, epoch_train_loss=1.7437957044555032
Epoch 1377
Epoch 1377 :: Batch 0/1
Batch Loss = 1.7431541882314237
1377, epoch_train_loss=1.7431541882314237
Epoch 1378
Epoch 1378 :: Batch 0/1
Batch Loss = 1.743905293439914
1378, epoch_train_loss=1.743905293439914
Epoch 1379
Epoch 1379 :: Batch 0/1
Batch Loss = 1.7495152113732335
1379, epoch_train_loss=1.7495152113732335
Epoch 1380
Epoch 1380 :: Batch 0/1
Batch Loss = 1.765372484849812
1380, epoch_train_loss=1.765372484849812
Epoch 1381
Epoch 1381 :: Batch 0/1
Batch Loss = 1.8337810611243006
1381, epoch_train_loss=1.8337810611243006
Epoch 1382
Epoch 1382 :: Batch 0/1
Batch Loss = 1.9795183850296032
1382, epoch_train_loss=1.9795183850296032
Epoch 1383
Epoch 1383 :: Batch 0/1
Batch Loss = 2.633830675220041
1383, epoch_train_loss=2.633830675220041
Epoch 1384
Epoch 1384 :: Batch 0/1
Batch Loss = 2.058820017846266
1384, epoch_train_loss=2.058820017846266
Epoch 1385
Epoch 1385 :: Batch 0/1
Batch Loss = 1.8058610985715404
1385, epoch_train_loss=1.8058610985715404
Epoch 1386
Epoch 1386 :: Batch 0/1
Batch Loss = 2.0860524181299
1386, epoch_train_loss=2.0860524181299
Epoch 1387
Epoch 1387 :: Batch 0/1
Batch Loss = 1.9919304784712006
1387, epoch_train_loss=1.9919304784712006
Epoch 1388
Epoch 1388 :: Batch 0/1
Batch Loss = 1.928983044960595
1388, epoch_train_loss=1.928983044960595
Epoch 1389
Epoch 1389 :: Batch 0/1
Batch Loss = 1.8293084820640508
1389, epoch_train_loss=1.8293084820640508
Epoch 1390
Epoch 1390 :: Batch 0/1
Batch Loss = 1.9293065901891933
1390, epoch_train_loss=1.9293065901891933
Epoch 1391
Epoch 1391 :: Batch 0/1
Batch Loss = 1.8011177284228763
1391, epoch_train_loss=1.8011177284228763
Epoch 1392
Epoch 1392 :: Batch 0/1
Batch Loss = 1.8213761205851469
1392, epoch_train_loss=1.8213761205851469
Epoch 1393
Epoch 1393 :: Batch 0/1
Batch Loss = 1.9785732735857104
1393, epoch_train_loss=1.9785732735857104
Epoch 1394
Epoch 1394 :: Batch 0/1
Batch Loss = 2.179666803452319
1394, epoch_train_loss=2.179666803452319
Epoch 1395
Epoch 1395 :: Batch 0/1
Batch Loss = 1.783533018695943
1395, epoch_train_loss=1.783533018695943
Epoch 1396
Epoch 1396 :: Batch 0/1
Batch Loss = 2.033149594213102
1396, epoch_train_loss=2.033149594213102
Epoch 1397
Epoch 1397 :: Batch 0/1
Batch Loss = 1.9827726050076138
1397, epoch_train_loss=1.9827726050076138
Epoch 1398
Epoch 1398 :: Batch 0/1
Batch Loss = 2.0462655347234184
1398, epoch_train_loss=2.0462655347234184
Epoch 1399
Epoch 1399 :: Batch 0/1
Batch Loss = 1.9255563857889682
1399, epoch_train_loss=1.9255563857889682
Epoch 1400
Epoch 1400 :: Batch 0/1
Batch Loss = 2.11607437311086
1400, epoch_train_loss=2.11607437311086
Epoch 1401
Epoch 1401 :: Batch 0/1
Batch Loss = 2.2798778685992827
1401, epoch_train_loss=2.2798778685992827
Epoch 1402
Epoch 1402 :: Batch 0/1
Batch Loss = 2.042728681276095
1402, epoch_train_loss=2.042728681276095
Epoch 1403
Epoch 1403 :: Batch 0/1
Batch Loss = 2.098918269771272
1403, epoch_train_loss=2.098918269771272
Epoch 1404
Epoch 1404 :: Batch 0/1
Batch Loss = 1.8860353481732408
1404, epoch_train_loss=1.8860353481732408
Epoch 1405
Epoch 1405 :: Batch 0/1
Batch Loss = 1.9762860873629085
1405, epoch_train_loss=1.9762860873629085
Epoch 1406
Epoch 1406 :: Batch 0/1
Batch Loss = 1.8712723686285793
1406, epoch_train_loss=1.8712723686285793
Epoch 1407
Epoch 1407 :: Batch 0/1
Batch Loss = 1.9894833819728435
1407, epoch_train_loss=1.9894833819728435
Epoch 1408
Epoch 1408 :: Batch 0/1
Batch Loss = 1.8144102320920035
1408, epoch_train_loss=1.8144102320920035
Epoch 1409
Epoch 1409 :: Batch 0/1
Batch Loss = 1.9009752014413968
1409, epoch_train_loss=1.9009752014413968
Epoch 1410
Epoch 1410 :: Batch 0/1
Batch Loss = 1.8036603521698877
1410, epoch_train_loss=1.8036603521698877
Epoch 1411
Epoch 1411 :: Batch 0/1
Batch Loss = 1.8978575848950678
1411, epoch_train_loss=1.8978575848950678
Epoch 1412
Epoch 1412 :: Batch 0/1
Batch Loss = 1.8060159165737515
1412, epoch_train_loss=1.8060159165737515
Epoch 1413
Epoch 1413 :: Batch 0/1
Batch Loss = 1.8777471211714585
1413, epoch_train_loss=1.8777471211714585
Epoch 1414
Epoch 1414 :: Batch 0/1
Batch Loss = 1.7874445059740305
1414, epoch_train_loss=1.7874445059740305
Epoch 1415
Epoch 1415 :: Batch 0/1
Batch Loss = 1.884858917813907
1415, epoch_train_loss=1.884858917813907
Epoch 1416
Epoch 1416 :: Batch 0/1
Batch Loss = 1.8240915820101766
1416, epoch_train_loss=1.8240915820101766
Epoch 1417
Epoch 1417 :: Batch 0/1
Batch Loss = 1.8447153817999913
1417, epoch_train_loss=1.8447153817999913
Epoch 1418
Epoch 1418 :: Batch 0/1
Batch Loss = 1.8082760521550263
1418, epoch_train_loss=1.8082760521550263
Epoch 1419
Epoch 1419 :: Batch 0/1
Batch Loss = 1.8259338121600768
1419, epoch_train_loss=1.8259338121600768
Epoch 1420
Epoch 1420 :: Batch 0/1
Batch Loss = 1.818805413497982
1420, epoch_train_loss=1.818805413497982
Epoch 1421
Epoch 1421 :: Batch 0/1
Batch Loss = 1.786772289489582
1421, epoch_train_loss=1.786772289489582
Epoch 1422
Epoch 1422 :: Batch 0/1
Batch Loss = 1.8548260723490206
1422, epoch_train_loss=1.8548260723490206
Epoch 1423
Epoch 1423 :: Batch 0/1
Batch Loss = 1.7840632049230982
1423, epoch_train_loss=1.7840632049230982
Epoch 1424
Epoch 1424 :: Batch 0/1
Batch Loss = 1.776224975768069
1424, epoch_train_loss=1.776224975768069
Epoch 1425
Epoch 1425 :: Batch 0/1
Batch Loss = 1.8085203081826333
1425, epoch_train_loss=1.8085203081826333
Epoch 1426
Epoch 1426 :: Batch 0/1
Batch Loss = 1.737534485137103
1426, epoch_train_loss=1.737534485137103
Epoch 1427
Epoch 1427 :: Batch 0/1
Batch Loss = 1.783008367576621
1427, epoch_train_loss=1.783008367576621
Epoch 1428
Epoch 1428 :: Batch 0/1
Batch Loss = 1.7682806736492316
1428, epoch_train_loss=1.7682806736492316
Epoch 1429
Epoch 1429 :: Batch 0/1
Batch Loss = 1.7318598995122902
1429, epoch_train_loss=1.7318598995122902
Epoch 1430
Epoch 1430 :: Batch 0/1
Batch Loss = 1.7418605037831616
1430, epoch_train_loss=1.7418605037831616
Epoch 1431
Epoch 1431 :: Batch 0/1
Batch Loss = 1.7446020326696021
1431, epoch_train_loss=1.7446020326696021
Epoch 1432
Epoch 1432 :: Batch 0/1
Batch Loss = 1.7264822893427825
1432, epoch_train_loss=1.7264822893427825
Epoch 1433
Epoch 1433 :: Batch 0/1
Batch Loss = 1.7360299223112472
1433, epoch_train_loss=1.7360299223112472
Epoch 1434
Epoch 1434 :: Batch 0/1
Batch Loss = 1.7300053068506063
1434, epoch_train_loss=1.7300053068506063
Epoch 1435
Epoch 1435 :: Batch 0/1
Batch Loss = 1.7175866950453613
1435, epoch_train_loss=1.7175866950453613
Epoch 1436
Epoch 1436 :: Batch 0/1
Batch Loss = 1.7251217727345105
1436, epoch_train_loss=1.7251217727345105
Epoch 1437
Epoch 1437 :: Batch 0/1
Batch Loss = 1.7238435907697436
1437, epoch_train_loss=1.7238435907697436
Epoch 1438
Epoch 1438 :: Batch 0/1
Batch Loss = 1.712757580392945
1438, epoch_train_loss=1.712757580392945
Epoch 1439
Epoch 1439 :: Batch 0/1
Batch Loss = 1.7115819080299233
1439, epoch_train_loss=1.7115819080299233
Epoch 1440
Epoch 1440 :: Batch 0/1
Batch Loss = 1.7146286972483735
1440, epoch_train_loss=1.7146286972483735
Epoch 1441
Epoch 1441 :: Batch 0/1
Batch Loss = 1.707772687977743
1441, epoch_train_loss=1.707772687977743
Epoch 1442
Epoch 1442 :: Batch 0/1
Batch Loss = 1.7040032532682439
1442, epoch_train_loss=1.7040032532682439
Epoch 1443
Epoch 1443 :: Batch 0/1
Batch Loss = 1.7075145338381876
1443, epoch_train_loss=1.7075145338381876
Epoch 1444
Epoch 1444 :: Batch 0/1
Batch Loss = 1.7063398311517164
1444, epoch_train_loss=1.7063398311517164
Epoch 1445
Epoch 1445 :: Batch 0/1
Batch Loss = 1.698699254548268
1445, epoch_train_loss=1.698699254548268
Epoch 1446
Epoch 1446 :: Batch 0/1
Batch Loss = 1.6969680212211868
1446, epoch_train_loss=1.6969680212211868
Epoch 1447
Epoch 1447 :: Batch 0/1
Batch Loss = 1.6990111728682096
1447, epoch_train_loss=1.6990111728682096
Epoch 1448
Epoch 1448 :: Batch 0/1
Batch Loss = 1.6958958363131735
1448, epoch_train_loss=1.6958958363131735
Epoch 1449
Epoch 1449 :: Batch 0/1
Batch Loss = 1.6911257121842278
1449, epoch_train_loss=1.6911257121842278
Epoch 1450
Epoch 1450 :: Batch 0/1
Batch Loss = 1.6889470988799427
1450, epoch_train_loss=1.6889470988799427
Epoch 1451
Epoch 1451 :: Batch 0/1
Batch Loss = 1.6890218539834942
1451, epoch_train_loss=1.6890218539834942
Epoch 1452
Epoch 1452 :: Batch 0/1
Batch Loss = 1.6889767148881378
1452, epoch_train_loss=1.6889767148881378
Epoch 1453
Epoch 1453 :: Batch 0/1
Batch Loss = 1.6861383375056405
1453, epoch_train_loss=1.6861383375056405
Epoch 1454
Epoch 1454 :: Batch 0/1
Batch Loss = 1.6819915750012548
1454, epoch_train_loss=1.6819915750012548
Epoch 1455
Epoch 1455 :: Batch 0/1
Batch Loss = 1.6791613901607656
1455, epoch_train_loss=1.6791613901607656
Epoch 1456
Epoch 1456 :: Batch 0/1
Batch Loss = 1.6787861493265355
1456, epoch_train_loss=1.6787861493265355
Epoch 1457
Epoch 1457 :: Batch 0/1
Batch Loss = 1.6793157952186653
1457, epoch_train_loss=1.6793157952186653
Epoch 1458
Epoch 1458 :: Batch 0/1
Batch Loss = 1.6791822381059451
1458, epoch_train_loss=1.6791822381059451
Epoch 1459
Epoch 1459 :: Batch 0/1
Batch Loss = 1.6795893291995907
1459, epoch_train_loss=1.6795893291995907
Epoch 1460
Epoch 1460 :: Batch 0/1
Batch Loss = 1.678126765218285
1460, epoch_train_loss=1.678126765218285
Epoch 1461
Epoch 1461 :: Batch 0/1
Batch Loss = 1.6770129114895869
1461, epoch_train_loss=1.6770129114895869
Epoch 1462
Epoch 1462 :: Batch 0/1
Batch Loss = 1.6756751538302126
1462, epoch_train_loss=1.6756751538302126
Epoch 1463
Epoch 1463 :: Batch 0/1
Batch Loss = 1.6792145922531676
1463, epoch_train_loss=1.6792145922531676
Epoch 1464
Epoch 1464 :: Batch 0/1
Batch Loss = 1.6865543257967526
1464, epoch_train_loss=1.6865543257967526
Epoch 1465
Epoch 1465 :: Batch 0/1
Batch Loss = 1.7181744964274759
1465, epoch_train_loss=1.7181744964274759
Epoch 1466
Epoch 1466 :: Batch 0/1
Batch Loss = 1.7624526969276701
1466, epoch_train_loss=1.7624526969276701
Epoch 1467
Epoch 1467 :: Batch 0/1
Batch Loss = 1.9434437118314851
1467, epoch_train_loss=1.9434437118314851
Epoch 1468
Epoch 1468 :: Batch 0/1
Batch Loss = 2.0141749902641974
1468, epoch_train_loss=2.0141749902641974
Epoch 1469
Epoch 1469 :: Batch 0/1
Batch Loss = 2.453375002239015
1469, epoch_train_loss=2.453375002239015
Epoch 1470
Epoch 1470 :: Batch 0/1
Batch Loss = 1.7837050064528082
1470, epoch_train_loss=1.7837050064528082
Epoch 1471
Epoch 1471 :: Batch 0/1
Batch Loss = 2.641618218032034
1471, epoch_train_loss=2.641618218032034
Epoch 1472
Epoch 1472 :: Batch 0/1
Batch Loss = 3.8949241795345895
1472, epoch_train_loss=3.8949241795345895
Epoch 1473
Epoch 1473 :: Batch 0/1
Batch Loss = 3.8232592136393495
1473, epoch_train_loss=3.8232592136393495
Epoch 1474
Epoch 1474 :: Batch 0/1
Batch Loss = 3.8652260094814115
1474, epoch_train_loss=3.8652260094814115
Epoch 1475
Epoch 1475 :: Batch 0/1
Batch Loss = 4.100424759583484
1475, epoch_train_loss=4.100424759583484
Epoch 1476
Epoch 1476 :: Batch 0/1
Batch Loss = 3.742620507778511
1476, epoch_train_loss=3.742620507778511
Epoch 1477
Epoch 1477 :: Batch 0/1
Batch Loss = 3.370273804971205
1477, epoch_train_loss=3.370273804971205
Epoch 1478
Epoch 1478 :: Batch 0/1
Batch Loss = 3.0751741395748073
1478, epoch_train_loss=3.0751741395748073
Epoch 1479
Epoch 1479 :: Batch 0/1
Batch Loss = 3.039559190586851
1479, epoch_train_loss=3.039559190586851
Epoch 1480
Epoch 1480 :: Batch 0/1
Batch Loss = 3.087490171966584
1480, epoch_train_loss=3.087490171966584
Epoch 1481
Epoch 1481 :: Batch 0/1
Batch Loss = 3.049537474031659
1481, epoch_train_loss=3.049537474031659
Epoch 1482
Epoch 1482 :: Batch 0/1
Batch Loss = 2.9649367411863077
1482, epoch_train_loss=2.9649367411863077
Epoch 1483
Epoch 1483 :: Batch 0/1
Batch Loss = 2.893256967204218
1483, epoch_train_loss=2.893256967204218
Epoch 1484
Epoch 1484 :: Batch 0/1
Batch Loss = 2.790418653645794
1484, epoch_train_loss=2.790418653645794
Epoch 1485
Epoch 1485 :: Batch 0/1
Batch Loss = 2.697633251368299
1485, epoch_train_loss=2.697633251368299
Epoch 1486
Epoch 1486 :: Batch 0/1
Batch Loss = 2.615314140042348
1486, epoch_train_loss=2.615314140042348
Epoch 1487
Epoch 1487 :: Batch 0/1
Batch Loss = 2.5412494208805274
1487, epoch_train_loss=2.5412494208805274
Epoch 1488
Epoch 1488 :: Batch 0/1
Batch Loss = 2.5119186787379877
1488, epoch_train_loss=2.5119186787379877
Epoch 1489
Epoch 1489 :: Batch 0/1
Batch Loss = 2.4822562422839654
1489, epoch_train_loss=2.4822562422839654
Epoch 1490
Epoch 1490 :: Batch 0/1
Batch Loss = 2.464680064211637
1490, epoch_train_loss=2.464680064211637
Epoch 1491
Epoch 1491 :: Batch 0/1
Batch Loss = 2.44411623077959
1491, epoch_train_loss=2.44411623077959
Epoch 1492
Epoch 1492 :: Batch 0/1
Batch Loss = 2.4090256527488205
1492, epoch_train_loss=2.4090256527488205
Epoch 1493
Epoch 1493 :: Batch 0/1
Batch Loss = 2.37763910247443
1493, epoch_train_loss=2.37763910247443
Epoch 1494
Epoch 1494 :: Batch 0/1
Batch Loss = 2.365320625395567
1494, epoch_train_loss=2.365320625395567
Epoch 1495
Epoch 1495 :: Batch 0/1
Batch Loss = 2.362826505385908
1495, epoch_train_loss=2.362826505385908
Epoch 1496
Epoch 1496 :: Batch 0/1
Batch Loss = 2.394372523758522
1496, epoch_train_loss=2.394372523758522
Epoch 1497
Epoch 1497 :: Batch 0/1
Batch Loss = 2.4664965412199007
1497, epoch_train_loss=2.4664965412199007
Epoch 1498
Epoch 1498 :: Batch 0/1
Batch Loss = 2.3295135824438757
1498, epoch_train_loss=2.3295135824438757
Epoch 1499
Epoch 1499 :: Batch 0/1
Batch Loss = 2.367951450110128
1499, epoch_train_loss=2.367951450110128
Epoch 1500
Epoch 1500 :: Batch 0/1
Batch Loss = 2.3159098626827226
1500, epoch_train_loss=2.3159098626827226
Epoch 1501
Epoch 1501 :: Batch 0/1
Batch Loss = 2.306409070460963
1501, epoch_train_loss=2.306409070460963
Epoch 1502
Epoch 1502 :: Batch 0/1
Batch Loss = 2.2490835613424474
1502, epoch_train_loss=2.2490835613424474
Epoch 1503
Epoch 1503 :: Batch 0/1
Batch Loss = 2.26323091422164
1503, epoch_train_loss=2.26323091422164
Epoch 1504
Epoch 1504 :: Batch 0/1
Batch Loss = 2.229351787873016
1504, epoch_train_loss=2.229351787873016
Epoch 1505
Epoch 1505 :: Batch 0/1
Batch Loss = 2.229883064619735
1505, epoch_train_loss=2.229883064619735
Epoch 1506
Epoch 1506 :: Batch 0/1
Batch Loss = 2.1903091682162135
1506, epoch_train_loss=2.1903091682162135
Epoch 1507
Epoch 1507 :: Batch 0/1
Batch Loss = 2.1891648898293288
1507, epoch_train_loss=2.1891648898293288
Epoch 1508
Epoch 1508 :: Batch 0/1
Batch Loss = 2.156003344992711
1508, epoch_train_loss=2.156003344992711
Epoch 1509
Epoch 1509 :: Batch 0/1
Batch Loss = 2.1598039454590436
1509, epoch_train_loss=2.1598039454590436
Epoch 1510
Epoch 1510 :: Batch 0/1
Batch Loss = 2.146453568409791
1510, epoch_train_loss=2.146453568409791
Epoch 1511
Epoch 1511 :: Batch 0/1
Batch Loss = 2.1476017522883555
1511, epoch_train_loss=2.1476017522883555
Epoch 1512
Epoch 1512 :: Batch 0/1
Batch Loss = 2.1297515345806235
1512, epoch_train_loss=2.1297515345806235
Epoch 1513
Epoch 1513 :: Batch 0/1
Batch Loss = 2.128318370503247
1513, epoch_train_loss=2.128318370503247
Epoch 1514
Epoch 1514 :: Batch 0/1
Batch Loss = 2.118631329218195
1514, epoch_train_loss=2.118631329218195
Epoch 1515
Epoch 1515 :: Batch 0/1
Batch Loss = 2.11693149103679
1515, epoch_train_loss=2.11693149103679
Epoch 1516
Epoch 1516 :: Batch 0/1
Batch Loss = 2.109704905858255
1516, epoch_train_loss=2.109704905858255
Epoch 1517
Epoch 1517 :: Batch 0/1
Batch Loss = 2.1016833497578005
1517, epoch_train_loss=2.1016833497578005
Epoch 1518
Epoch 1518 :: Batch 0/1
Batch Loss = 2.0944223085050813
1518, epoch_train_loss=2.0944223085050813
Epoch 1519
Epoch 1519 :: Batch 0/1
Batch Loss = 2.085109052809017
1519, epoch_train_loss=2.085109052809017
Epoch 1520
Epoch 1520 :: Batch 0/1
Batch Loss = 2.0779435777709496
1520, epoch_train_loss=2.0779435777709496
Epoch 1521
Epoch 1521 :: Batch 0/1
Batch Loss = 2.0640313623316398
1521, epoch_train_loss=2.0640313623316398
Epoch 1522
Epoch 1522 :: Batch 0/1
Batch Loss = 2.0584114038698877
1522, epoch_train_loss=2.0584114038698877
Epoch 1523
Epoch 1523 :: Batch 0/1
Batch Loss = 2.053149747815972
1523, epoch_train_loss=2.053149747815972
Epoch 1524
Epoch 1524 :: Batch 0/1
Batch Loss = 2.0499974796439018
1524, epoch_train_loss=2.0499974796439018
Epoch 1525
Epoch 1525 :: Batch 0/1
Batch Loss = 2.0449224044312704
1525, epoch_train_loss=2.0449224044312704
Epoch 1526
Epoch 1526 :: Batch 0/1
Batch Loss = 2.0461109587857997
1526, epoch_train_loss=2.0461109587857997
Epoch 1527
Epoch 1527 :: Batch 0/1
Batch Loss = 2.0412804750429823
1527, epoch_train_loss=2.0412804750429823
Epoch 1528
Epoch 1528 :: Batch 0/1
Batch Loss = 2.0433369643698676
1528, epoch_train_loss=2.0433369643698676
Epoch 1529
Epoch 1529 :: Batch 0/1
Batch Loss = 2.0339886963163614
1529, epoch_train_loss=2.0339886963163614
Epoch 1530
Epoch 1530 :: Batch 0/1
Batch Loss = 2.0222381716858564
1530, epoch_train_loss=2.0222381716858564
Epoch 1531
Epoch 1531 :: Batch 0/1
Batch Loss = 1.9996025504041717
1531, epoch_train_loss=1.9996025504041717
Epoch 1532
Epoch 1532 :: Batch 0/1
Batch Loss = 1.992490228873629
1532, epoch_train_loss=1.992490228873629
Epoch 1533
Epoch 1533 :: Batch 0/1
Batch Loss = 2.0022085793195448
1533, epoch_train_loss=2.0022085793195448
Epoch 1534
Epoch 1534 :: Batch 0/1
Batch Loss = 2.0046740337448847
1534, epoch_train_loss=2.0046740337448847
Epoch 1535
Epoch 1535 :: Batch 0/1
Batch Loss = 2.0119000259221953
1535, epoch_train_loss=2.0119000259221953
Epoch 1536
Epoch 1536 :: Batch 0/1
Batch Loss = 1.9805847818240412
1536, epoch_train_loss=1.9805847818240412
Epoch 1537
Epoch 1537 :: Batch 0/1
Batch Loss = 1.9579375906403418
1537, epoch_train_loss=1.9579375906403418
Epoch 1538
Epoch 1538 :: Batch 0/1
Batch Loss = 1.9538870520697438
1538, epoch_train_loss=1.9538870520697438
Epoch 1539
Epoch 1539 :: Batch 0/1
Batch Loss = 1.963034409316288
1539, epoch_train_loss=1.963034409316288
Epoch 1540
Epoch 1540 :: Batch 0/1
Batch Loss = 2.0136004939684193
1540, epoch_train_loss=2.0136004939684193
Epoch 1541
Epoch 1541 :: Batch 0/1
Batch Loss = 2.013638765656989
1541, epoch_train_loss=2.013638765656989
Epoch 1542
Epoch 1542 :: Batch 0/1
Batch Loss = 1.9980802090127059
1542, epoch_train_loss=1.9980802090127059
Epoch 1543
Epoch 1543 :: Batch 0/1
Batch Loss = 1.9266916464079842
1543, epoch_train_loss=1.9266916464079842
Epoch 1544
Epoch 1544 :: Batch 0/1
Batch Loss = 1.9795351063627264
1544, epoch_train_loss=1.9795351063627264
Epoch 1545
Epoch 1545 :: Batch 0/1
Batch Loss = 2.293204158720708
1545, epoch_train_loss=2.293204158720708
Epoch 1546
Epoch 1546 :: Batch 0/1
Batch Loss = 1.9899781676551358
1546, epoch_train_loss=1.9899781676551358
Epoch 1547
Epoch 1547 :: Batch 0/1
Batch Loss = 2.289503433167545
1547, epoch_train_loss=2.289503433167545
Epoch 1548
Epoch 1548 :: Batch 0/1
Batch Loss = 2.630011382399473
1548, epoch_train_loss=2.630011382399473
Epoch 1549
Epoch 1549 :: Batch 0/1
Batch Loss = 2.7193681667440255
1549, epoch_train_loss=2.7193681667440255
Epoch 1550
Epoch 1550 :: Batch 0/1
Batch Loss = 2.347369166488899
1550, epoch_train_loss=2.347369166488899
Epoch 1551
Epoch 1551 :: Batch 0/1
Batch Loss = 2.4498596337877387
1551, epoch_train_loss=2.4498596337877387
Epoch 1552
Epoch 1552 :: Batch 0/1
Batch Loss = 2.353272262554447
1552, epoch_train_loss=2.353272262554447
Epoch 1553
Epoch 1553 :: Batch 0/1
Batch Loss = 2.520753901536679
1553, epoch_train_loss=2.520753901536679
Epoch 1554
Epoch 1554 :: Batch 0/1
Batch Loss = 2.225106668162388
1554, epoch_train_loss=2.225106668162388
Epoch 1555
Epoch 1555 :: Batch 0/1
Batch Loss = 2.353401219670085
1555, epoch_train_loss=2.353401219670085
Epoch 1556
Epoch 1556 :: Batch 0/1
Batch Loss = 2.3536884523355375
1556, epoch_train_loss=2.3536884523355375
Epoch 1557
Epoch 1557 :: Batch 0/1
Batch Loss = 2.21699988446803
1557, epoch_train_loss=2.21699988446803
Epoch 1558
Epoch 1558 :: Batch 0/1
Batch Loss = 2.2816298283828296
1558, epoch_train_loss=2.2816298283828296
Epoch 1559
Epoch 1559 :: Batch 0/1
Batch Loss = 2.2507067077369096
1559, epoch_train_loss=2.2507067077369096
Epoch 1560
Epoch 1560 :: Batch 0/1
Batch Loss = 2.207159675292853
1560, epoch_train_loss=2.207159675292853
Epoch 1561
Epoch 1561 :: Batch 0/1
Batch Loss = 2.1620479766699594
1561, epoch_train_loss=2.1620479766699594
Epoch 1562
Epoch 1562 :: Batch 0/1
Batch Loss = 2.151898097568284
1562, epoch_train_loss=2.151898097568284
Epoch 1563
Epoch 1563 :: Batch 0/1
Batch Loss = 2.1893320206705216
1563, epoch_train_loss=2.1893320206705216
Epoch 1564
Epoch 1564 :: Batch 0/1
Batch Loss = 2.08795592909394
1564, epoch_train_loss=2.08795592909394
Epoch 1565
Epoch 1565 :: Batch 0/1
Batch Loss = 2.1315185195681505
1565, epoch_train_loss=2.1315185195681505
Epoch 1566
Epoch 1566 :: Batch 0/1
Batch Loss = 2.0802935400173426
1566, epoch_train_loss=2.0802935400173426
Epoch 1567
Epoch 1567 :: Batch 0/1
Batch Loss = 2.1172473113389048
1567, epoch_train_loss=2.1172473113389048
Epoch 1568
Epoch 1568 :: Batch 0/1
Batch Loss = 2.04887441162934
1568, epoch_train_loss=2.04887441162934
Epoch 1569
Epoch 1569 :: Batch 0/1
Batch Loss = 2.102032185746676
1569, epoch_train_loss=2.102032185746676
Epoch 1570
Epoch 1570 :: Batch 0/1
Batch Loss = 2.0641471881654927
1570, epoch_train_loss=2.0641471881654927
Epoch 1571
Epoch 1571 :: Batch 0/1
Batch Loss = 2.0945835021183177
1571, epoch_train_loss=2.0945835021183177
Epoch 1572
Epoch 1572 :: Batch 0/1
Batch Loss = 2.046811382210559
1572, epoch_train_loss=2.046811382210559
Epoch 1573
Epoch 1573 :: Batch 0/1
Batch Loss = 2.067882524428605
1573, epoch_train_loss=2.067882524428605
Epoch 1574
Epoch 1574 :: Batch 0/1
Batch Loss = 2.0224427559936333
1574, epoch_train_loss=2.0224427559936333
Epoch 1575
Epoch 1575 :: Batch 0/1
Batch Loss = 2.043488926132795
1575, epoch_train_loss=2.043488926132795
Epoch 1576
Epoch 1576 :: Batch 0/1
Batch Loss = 2.016936155972664
1576, epoch_train_loss=2.016936155972664
Epoch 1577
Epoch 1577 :: Batch 0/1
Batch Loss = 2.0349262668475494
1577, epoch_train_loss=2.0349262668475494
Epoch 1578
Epoch 1578 :: Batch 0/1
Batch Loss = 2.013011886016958
1578, epoch_train_loss=2.013011886016958
Epoch 1579
Epoch 1579 :: Batch 0/1
Batch Loss = 2.0268461574014007
1579, epoch_train_loss=2.0268461574014007
Epoch 1580
Epoch 1580 :: Batch 0/1
Batch Loss = 2.009560215773717
1580, epoch_train_loss=2.009560215773717
Epoch 1581
Epoch 1581 :: Batch 0/1
Batch Loss = 2.013424548288266
1581, epoch_train_loss=2.013424548288266
Epoch 1582
Epoch 1582 :: Batch 0/1
Batch Loss = 1.9968144422063372
1582, epoch_train_loss=1.9968144422063372
Epoch 1583
Epoch 1583 :: Batch 0/1
Batch Loss = 2.0002249538397514
1583, epoch_train_loss=2.0002249538397514
Epoch 1584
Epoch 1584 :: Batch 0/1
Batch Loss = 1.990264146845771
1584, epoch_train_loss=1.990264146845771
Epoch 1585
Epoch 1585 :: Batch 0/1
Batch Loss = 1.9913446579766523
1585, epoch_train_loss=1.9913446579766523
Epoch 1586
Epoch 1586 :: Batch 0/1
Batch Loss = 1.9837589740485477
1586, epoch_train_loss=1.9837589740485477
Epoch 1587
Epoch 1587 :: Batch 0/1
Batch Loss = 1.9817459560318615
1587, epoch_train_loss=1.9817459560318615
Epoch 1588
Epoch 1588 :: Batch 0/1
Batch Loss = 1.9778598273813048
1588, epoch_train_loss=1.9778598273813048
Epoch 1589
Epoch 1589 :: Batch 0/1
Batch Loss = 1.971998748389963
1589, epoch_train_loss=1.971998748389963
Epoch 1590
Epoch 1590 :: Batch 0/1
Batch Loss = 1.9685252624029188
1590, epoch_train_loss=1.9685252624029188
Epoch 1591
Epoch 1591 :: Batch 0/1
Batch Loss = 1.9570792357809368
1591, epoch_train_loss=1.9570792357809368
Epoch 1592
Epoch 1592 :: Batch 0/1
Batch Loss = 1.9499370822203375
1592, epoch_train_loss=1.9499370822203375
Epoch 1593
Epoch 1593 :: Batch 0/1
Batch Loss = 1.9354537957685505
1593, epoch_train_loss=1.9354537957685505
Epoch 1594
Epoch 1594 :: Batch 0/1
Batch Loss = 1.9255160912012177
1594, epoch_train_loss=1.9255160912012177
Epoch 1595
Epoch 1595 :: Batch 0/1
Batch Loss = 1.9206121370807867
1595, epoch_train_loss=1.9206121370807867
Epoch 1596
Epoch 1596 :: Batch 0/1
Batch Loss = 1.923775025589273
1596, epoch_train_loss=1.923775025589273
Epoch 1597
Epoch 1597 :: Batch 0/1
Batch Loss = 1.9219544837594422
1597, epoch_train_loss=1.9219544837594422
Epoch 1598
Epoch 1598 :: Batch 0/1
Batch Loss = 1.9091797456564632
1598, epoch_train_loss=1.9091797456564632
Epoch 1599
Epoch 1599 :: Batch 0/1
Batch Loss = 1.9014357273983575
1599, epoch_train_loss=1.9014357273983575
Epoch 1600
Epoch 1600 :: Batch 0/1
Batch Loss = 1.9015347065007395
1600, epoch_train_loss=1.9015347065007395
Epoch 1601
Epoch 1601 :: Batch 0/1
Batch Loss = 1.897445428133719
1601, epoch_train_loss=1.897445428133719
Epoch 1602
Epoch 1602 :: Batch 0/1
Batch Loss = 1.8891699870212046
1602, epoch_train_loss=1.8891699870212046
Epoch 1603
Epoch 1603 :: Batch 0/1
Batch Loss = 1.8819520776607441
1603, epoch_train_loss=1.8819520776607441
Epoch 1604
Epoch 1604 :: Batch 0/1
Batch Loss = 1.8777749085023285
1604, epoch_train_loss=1.8777749085023285
Epoch 1605
Epoch 1605 :: Batch 0/1
Batch Loss = 1.8750828153955867
1605, epoch_train_loss=1.8750828153955867
Epoch 1606
Epoch 1606 :: Batch 0/1
Batch Loss = 1.871253624675624
1606, epoch_train_loss=1.871253624675624
Epoch 1607
Epoch 1607 :: Batch 0/1
Batch Loss = 1.8652894056806009
1607, epoch_train_loss=1.8652894056806009
Epoch 1608
Epoch 1608 :: Batch 0/1
Batch Loss = 1.8603057805360392
1608, epoch_train_loss=1.8603057805360392
Epoch 1609
Epoch 1609 :: Batch 0/1
Batch Loss = 1.8583049025559768
1609, epoch_train_loss=1.8583049025559768
Epoch 1610
Epoch 1610 :: Batch 0/1
Batch Loss = 1.8571152530147759
1610, epoch_train_loss=1.8571152530147759
Epoch 1611
Epoch 1611 :: Batch 0/1
Batch Loss = 1.85369582822144
1611, epoch_train_loss=1.85369582822144
Epoch 1612
Epoch 1612 :: Batch 0/1
Batch Loss = 1.8508120999483553
1612, epoch_train_loss=1.8508120999483553
Epoch 1613
Epoch 1613 :: Batch 0/1
Batch Loss = 1.85074883028889
1613, epoch_train_loss=1.85074883028889
Epoch 1614
Epoch 1614 :: Batch 0/1
Batch Loss = 1.8722209676871
1614, epoch_train_loss=1.8722209676871
Epoch 1615
Epoch 1615 :: Batch 0/1
Batch Loss = 1.9268289673755756
1615, epoch_train_loss=1.9268289673755756
Epoch 1616
Epoch 1616 :: Batch 0/1
Batch Loss = 2.132564453992892
1616, epoch_train_loss=2.132564453992892
Epoch 1617
Epoch 1617 :: Batch 0/1
Batch Loss = 1.8976255190466347
1617, epoch_train_loss=1.8976255190466347
Epoch 1618
Epoch 1618 :: Batch 0/1
Batch Loss = 1.9046579579215397
1618, epoch_train_loss=1.9046579579215397
Epoch 1619
Epoch 1619 :: Batch 0/1
Batch Loss = 1.987037578897947
1619, epoch_train_loss=1.987037578897947
Epoch 1620
Epoch 1620 :: Batch 0/1
Batch Loss = 1.8415358060347253
1620, epoch_train_loss=1.8415358060347253
Epoch 1621
Epoch 1621 :: Batch 0/1
Batch Loss = 2.0243447248932656
1621, epoch_train_loss=2.0243447248932656
Epoch 1622
Epoch 1622 :: Batch 0/1
Batch Loss = 2.4836951606077693
1622, epoch_train_loss=2.4836951606077693
Epoch 1623
Epoch 1623 :: Batch 0/1
Batch Loss = 1.8897569256933766
1623, epoch_train_loss=1.8897569256933766
Epoch 1624
Epoch 1624 :: Batch 0/1
Batch Loss = 2.590811876149275
1624, epoch_train_loss=2.590811876149275
Epoch 1625
Epoch 1625 :: Batch 0/1
Batch Loss = 2.1311455148321623
1625, epoch_train_loss=2.1311455148321623
Epoch 1626
Epoch 1626 :: Batch 0/1
Batch Loss = 2.4978451875989265
1626, epoch_train_loss=2.4978451875989265
Epoch 1627
Epoch 1627 :: Batch 0/1
Batch Loss = 1.9992336752249267
1627, epoch_train_loss=1.9992336752249267
Epoch 1628
Epoch 1628 :: Batch 0/1
Batch Loss = 2.4175050488528265
1628, epoch_train_loss=2.4175050488528265
Epoch 1629
Epoch 1629 :: Batch 0/1
Batch Loss = 2.2662712677441292
1629, epoch_train_loss=2.2662712677441292
Epoch 1630
Epoch 1630 :: Batch 0/1
Batch Loss = 2.317576321555478
1630, epoch_train_loss=2.317576321555478
Epoch 1631
Epoch 1631 :: Batch 0/1
Batch Loss = 2.1111761878852695
1631, epoch_train_loss=2.1111761878852695
Epoch 1632
Epoch 1632 :: Batch 0/1
Batch Loss = 2.2687866280318207
1632, epoch_train_loss=2.2687866280318207
Epoch 1633
Epoch 1633 :: Batch 0/1
Batch Loss = 2.2299798874242884
1633, epoch_train_loss=2.2299798874242884
Epoch 1634
Epoch 1634 :: Batch 0/1
Batch Loss = 1.9877867926336361
1634, epoch_train_loss=1.9877867926336361
Epoch 1635
Epoch 1635 :: Batch 0/1
Batch Loss = 2.206761157919461
1635, epoch_train_loss=2.206761157919461
Epoch 1636
Epoch 1636 :: Batch 0/1
Batch Loss = 2.012359300221776
1636, epoch_train_loss=2.012359300221776
Epoch 1637
Epoch 1637 :: Batch 0/1
Batch Loss = 2.0229190617409105
1637, epoch_train_loss=2.0229190617409105
Epoch 1638
Epoch 1638 :: Batch 0/1
Batch Loss = 1.939812155857
1638, epoch_train_loss=1.939812155857
Epoch 1639
Epoch 1639 :: Batch 0/1
Batch Loss = 2.0397294154279937
1639, epoch_train_loss=2.0397294154279937
Epoch 1640
Epoch 1640 :: Batch 0/1
Batch Loss = 1.9336430769164312
1640, epoch_train_loss=1.9336430769164312
Epoch 1641
Epoch 1641 :: Batch 0/1
Batch Loss = 1.9670503736182758
1641, epoch_train_loss=1.9670503736182758
Epoch 1642
Epoch 1642 :: Batch 0/1
Batch Loss = 1.9843440311484462
1642, epoch_train_loss=1.9843440311484462
Epoch 1643
Epoch 1643 :: Batch 0/1
Batch Loss = 1.9186827689448538
1643, epoch_train_loss=1.9186827689448538
Epoch 1644
Epoch 1644 :: Batch 0/1
Batch Loss = 1.9718708138217644
1644, epoch_train_loss=1.9718708138217644
Epoch 1645
Epoch 1645 :: Batch 0/1
Batch Loss = 1.9886711478054324
1645, epoch_train_loss=1.9886711478054324
Epoch 1646
Epoch 1646 :: Batch 0/1
Batch Loss = 1.9009969487496912
1646, epoch_train_loss=1.9009969487496912
Epoch 1647
Epoch 1647 :: Batch 0/1
Batch Loss = 1.9476085292410175
1647, epoch_train_loss=1.9476085292410175
Epoch 1648
Epoch 1648 :: Batch 0/1
Batch Loss = 1.8949285455005909
1648, epoch_train_loss=1.8949285455005909
Epoch 1649
Epoch 1649 :: Batch 0/1
Batch Loss = 1.912905166200114
1649, epoch_train_loss=1.912905166200114
Epoch 1650
Epoch 1650 :: Batch 0/1
Batch Loss = 1.8592006645666042
1650, epoch_train_loss=1.8592006645666042
Epoch 1651
Epoch 1651 :: Batch 0/1
Batch Loss = 1.8825766637520247
1651, epoch_train_loss=1.8825766637520247
Epoch 1652
Epoch 1652 :: Batch 0/1
Batch Loss = 1.985342887638318
1652, epoch_train_loss=1.985342887638318
Epoch 1653
Epoch 1653 :: Batch 0/1
Batch Loss = 1.8815031530520903
1653, epoch_train_loss=1.8815031530520903
Epoch 1654
Epoch 1654 :: Batch 0/1
Batch Loss = 1.8762475478731941
1654, epoch_train_loss=1.8762475478731941
Epoch 1655
Epoch 1655 :: Batch 0/1
Batch Loss = 1.9063884987437956
1655, epoch_train_loss=1.9063884987437956
Epoch 1656
Epoch 1656 :: Batch 0/1
Batch Loss = 1.8665802450532896
1656, epoch_train_loss=1.8665802450532896
Epoch 1657
Epoch 1657 :: Batch 0/1
Batch Loss = 1.902906550141457
1657, epoch_train_loss=1.902906550141457
Epoch 1658
Epoch 1658 :: Batch 0/1
Batch Loss = 1.8918892106086174
1658, epoch_train_loss=1.8918892106086174
Epoch 1659
Epoch 1659 :: Batch 0/1
Batch Loss = 1.8529036994800452
1659, epoch_train_loss=1.8529036994800452
Epoch 1660
Epoch 1660 :: Batch 0/1
Batch Loss = 1.934029197002334
1660, epoch_train_loss=1.934029197002334
Epoch 1661
Epoch 1661 :: Batch 0/1
Batch Loss = 2.031273133900861
1661, epoch_train_loss=2.031273133900861
Epoch 1662
Epoch 1662 :: Batch 0/1
Batch Loss = 1.8402737037004098
1662, epoch_train_loss=1.8402737037004098
Epoch 1663
Epoch 1663 :: Batch 0/1
Batch Loss = 1.9085536671204055
1663, epoch_train_loss=1.9085536671204055
Epoch 1664
Epoch 1664 :: Batch 0/1
Batch Loss = 2.036818176092597
1664, epoch_train_loss=2.036818176092597
Epoch 1665
Epoch 1665 :: Batch 0/1
Batch Loss = 1.8682989631719145
1665, epoch_train_loss=1.8682989631719145
Epoch 1666
Epoch 1666 :: Batch 0/1
Batch Loss = 2.104498698543376
1666, epoch_train_loss=2.104498698543376
Epoch 1667
Epoch 1667 :: Batch 0/1
Batch Loss = 1.9571727076109633
1667, epoch_train_loss=1.9571727076109633
Epoch 1668
Epoch 1668 :: Batch 0/1
Batch Loss = 2.028719673634648
1668, epoch_train_loss=2.028719673634648
Epoch 1669
Epoch 1669 :: Batch 0/1
Batch Loss = 1.9431653892813858
1669, epoch_train_loss=1.9431653892813858
Epoch 1670
Epoch 1670 :: Batch 0/1
Batch Loss = 2.0347488049582547
1670, epoch_train_loss=2.0347488049582547
Epoch 1671
Epoch 1671 :: Batch 0/1
Batch Loss = 1.9553313936392933
1671, epoch_train_loss=1.9553313936392933
Epoch 1672
Epoch 1672 :: Batch 0/1
Batch Loss = 1.9904901074912764
1672, epoch_train_loss=1.9904901074912764
Epoch 1673
Epoch 1673 :: Batch 0/1
Batch Loss = 1.9753789404285214
1673, epoch_train_loss=1.9753789404285214
Epoch 1674
Epoch 1674 :: Batch 0/1
Batch Loss = 1.8376267050205632
1674, epoch_train_loss=1.8376267050205632
Epoch 1675
Epoch 1675 :: Batch 0/1
Batch Loss = 1.9119082282727464
1675, epoch_train_loss=1.9119082282727464
Epoch 1676
Epoch 1676 :: Batch 0/1
Batch Loss = 1.8584496988457186
1676, epoch_train_loss=1.8584496988457186
Epoch 1677
Epoch 1677 :: Batch 0/1
Batch Loss = 1.812598302533472
1677, epoch_train_loss=1.812598302533472
Epoch 1678
Epoch 1678 :: Batch 0/1
Batch Loss = 1.8242765604101825
1678, epoch_train_loss=1.8242765604101825
Epoch 1679
Epoch 1679 :: Batch 0/1
Batch Loss = 1.8811153498122257
1679, epoch_train_loss=1.8811153498122257
Epoch 1680
Epoch 1680 :: Batch 0/1
Batch Loss = 2.0082902161523526
1680, epoch_train_loss=2.0082902161523526
Epoch 1681
Epoch 1681 :: Batch 0/1
Batch Loss = 1.8973592075879453
1681, epoch_train_loss=1.8973592075879453
Epoch 1682
Epoch 1682 :: Batch 0/1
Batch Loss = 1.8316472185203405
1682, epoch_train_loss=1.8316472185203405
Epoch 1683
Epoch 1683 :: Batch 0/1
Batch Loss = 1.842763724807191
1683, epoch_train_loss=1.842763724807191
Epoch 1684
Epoch 1684 :: Batch 0/1
Batch Loss = 1.853509688620857
1684, epoch_train_loss=1.853509688620857
Epoch 1685
Epoch 1685 :: Batch 0/1
Batch Loss = 1.8248061743958266
1685, epoch_train_loss=1.8248061743958266
Epoch 1686
Epoch 1686 :: Batch 0/1
Batch Loss = 1.7941324020066074
1686, epoch_train_loss=1.7941324020066074
Epoch 1687
Epoch 1687 :: Batch 0/1
Batch Loss = 1.794305804729766
1687, epoch_train_loss=1.794305804729766
Epoch 1688
Epoch 1688 :: Batch 0/1
Batch Loss = 1.8075473315226565
1688, epoch_train_loss=1.8075473315226565
Epoch 1689
Epoch 1689 :: Batch 0/1
Batch Loss = 1.8782828151890965
1689, epoch_train_loss=1.8782828151890965
Epoch 1690
Epoch 1690 :: Batch 0/1
Batch Loss = 2.1882899108646585
1690, epoch_train_loss=2.1882899108646585
Epoch 1691
Epoch 1691 :: Batch 0/1
Batch Loss = 1.897187516246703
1691, epoch_train_loss=1.897187516246703
Epoch 1692
Epoch 1692 :: Batch 0/1
Batch Loss = 1.908997113838458
1692, epoch_train_loss=1.908997113838458
Epoch 1693
Epoch 1693 :: Batch 0/1
Batch Loss = 1.9540987463266377
1693, epoch_train_loss=1.9540987463266377
Epoch 1694
Epoch 1694 :: Batch 0/1
Batch Loss = 1.9170618761607012
1694, epoch_train_loss=1.9170618761607012
Epoch 1695
Epoch 1695 :: Batch 0/1
Batch Loss = 1.9104455707776935
1695, epoch_train_loss=1.9104455707776935
Epoch 1696
Epoch 1696 :: Batch 0/1
Batch Loss = 1.816963627315463
1696, epoch_train_loss=1.816963627315463
Epoch 1697
Epoch 1697 :: Batch 0/1
Batch Loss = 1.7837294216674218
1697, epoch_train_loss=1.7837294216674218
Epoch 1698
Epoch 1698 :: Batch 0/1
Batch Loss = 1.9409444085128638
1698, epoch_train_loss=1.9409444085128638
Epoch 1699
Epoch 1699 :: Batch 0/1
Batch Loss = 2.709979482394542
1699, epoch_train_loss=2.709979482394542
Epoch 1700
Epoch 1700 :: Batch 0/1
Batch Loss = 1.8320207099205448
1700, epoch_train_loss=1.8320207099205448
Epoch 1701
Epoch 1701 :: Batch 0/1
Batch Loss = 2.1292370026613288
1701, epoch_train_loss=2.1292370026613288
Epoch 1702
Epoch 1702 :: Batch 0/1
Batch Loss = 2.0933027502277373
1702, epoch_train_loss=2.0933027502277373
Epoch 1703
Epoch 1703 :: Batch 0/1
Batch Loss = 2.3139360307624357
1703, epoch_train_loss=2.3139360307624357
Epoch 1704
Epoch 1704 :: Batch 0/1
Batch Loss = 2.1142403337958546
1704, epoch_train_loss=2.1142403337958546
Epoch 1705
Epoch 1705 :: Batch 0/1
Batch Loss = 2.2387685420744776
1705, epoch_train_loss=2.2387685420744776
Epoch 1706
Epoch 1706 :: Batch 0/1
Batch Loss = 2.040943337375453
1706, epoch_train_loss=2.040943337375453
Epoch 1707
Epoch 1707 :: Batch 0/1
Batch Loss = 2.118103472868713
1707, epoch_train_loss=2.118103472868713
Epoch 1708
Epoch 1708 :: Batch 0/1
Batch Loss = 2.0752914974160013
1708, epoch_train_loss=2.0752914974160013
Epoch 1709
Epoch 1709 :: Batch 0/1
Batch Loss = 2.0634101957458864
1709, epoch_train_loss=2.0634101957458864
Epoch 1710
Epoch 1710 :: Batch 0/1
Batch Loss = 2.0743809335711885
1710, epoch_train_loss=2.0743809335711885
Epoch 1711
Epoch 1711 :: Batch 0/1
Batch Loss = 1.9881159049805794
1711, epoch_train_loss=1.9881159049805794
Epoch 1712
Epoch 1712 :: Batch 0/1
Batch Loss = 2.0689545943670162
1712, epoch_train_loss=2.0689545943670162
Epoch 1713
Epoch 1713 :: Batch 0/1
Batch Loss = 1.9661420409109944
1713, epoch_train_loss=1.9661420409109944
Epoch 1714
Epoch 1714 :: Batch 0/1
Batch Loss = 2.0214682810430498
1714, epoch_train_loss=2.0214682810430498
Epoch 1715
Epoch 1715 :: Batch 0/1
Batch Loss = 1.9543226345694846
1715, epoch_train_loss=1.9543226345694846
Epoch 1716
Epoch 1716 :: Batch 0/1
Batch Loss = 1.9585611820321829
1716, epoch_train_loss=1.9585611820321829
Epoch 1717
Epoch 1717 :: Batch 0/1
Batch Loss = 1.9617849395926914
1717, epoch_train_loss=1.9617849395926914
Epoch 1718
Epoch 1718 :: Batch 0/1
Batch Loss = 1.8926254807163663
1718, epoch_train_loss=1.8926254807163663
Epoch 1719
Epoch 1719 :: Batch 0/1
Batch Loss = 1.9298489757938362
1719, epoch_train_loss=1.9298489757938362
Epoch 1720
Epoch 1720 :: Batch 0/1
Batch Loss = 1.8899769501700379
1720, epoch_train_loss=1.8899769501700379
Epoch 1721
Epoch 1721 :: Batch 0/1
Batch Loss = 1.8637214806366251
1721, epoch_train_loss=1.8637214806366251
Epoch 1722
Epoch 1722 :: Batch 0/1
Batch Loss = 1.8619439155477955
1722, epoch_train_loss=1.8619439155477955
Epoch 1723
Epoch 1723 :: Batch 0/1
Batch Loss = 1.87235296331459
1723, epoch_train_loss=1.87235296331459
Epoch 1724
Epoch 1724 :: Batch 0/1
Batch Loss = 1.9211767852928843
1724, epoch_train_loss=1.9211767852928843
Epoch 1725
Epoch 1725 :: Batch 0/1
Batch Loss = 1.853804642152057
1725, epoch_train_loss=1.853804642152057
Epoch 1726
Epoch 1726 :: Batch 0/1
Batch Loss = 1.8514380304063958
1726, epoch_train_loss=1.8514380304063958
Epoch 1727
Epoch 1727 :: Batch 0/1
Batch Loss = 1.868909104816341
1727, epoch_train_loss=1.868909104816341
Epoch 1728
Epoch 1728 :: Batch 0/1
Batch Loss = 1.817343599997785
1728, epoch_train_loss=1.817343599997785
Epoch 1729
Epoch 1729 :: Batch 0/1
Batch Loss = 1.8532880453279297
1729, epoch_train_loss=1.8532880453279297
Epoch 1730
Epoch 1730 :: Batch 0/1
Batch Loss = 1.9529571577275693
1730, epoch_train_loss=1.9529571577275693
Epoch 1731
Epoch 1731 :: Batch 0/1
Batch Loss = 1.8085771366489587
1731, epoch_train_loss=1.8085771366489587
Epoch 1732
Epoch 1732 :: Batch 0/1
Batch Loss = 1.8072121269410502
1732, epoch_train_loss=1.8072121269410502
Epoch 1733
Epoch 1733 :: Batch 0/1
Batch Loss = 1.9105374373800126
1733, epoch_train_loss=1.9105374373800126
Epoch 1734
Epoch 1734 :: Batch 0/1
Batch Loss = 1.841174596469678
1734, epoch_train_loss=1.841174596469678
Epoch 1735
Epoch 1735 :: Batch 0/1
Batch Loss = 1.8060903605556107
1735, epoch_train_loss=1.8060903605556107
Epoch 1736
Epoch 1736 :: Batch 0/1
Batch Loss = 1.7794004983015979
1736, epoch_train_loss=1.7794004983015979
Epoch 1737
Epoch 1737 :: Batch 0/1
Batch Loss = 1.7975990068014676
1737, epoch_train_loss=1.7975990068014676
Epoch 1738
Epoch 1738 :: Batch 0/1
Batch Loss = 1.90318406026861
1738, epoch_train_loss=1.90318406026861
Epoch 1739
Epoch 1739 :: Batch 0/1
Batch Loss = 1.9600386931503084
1739, epoch_train_loss=1.9600386931503084
Epoch 1740
Epoch 1740 :: Batch 0/1
Batch Loss = 2.2456637351674984
1740, epoch_train_loss=2.2456637351674984
Epoch 1741
Epoch 1741 :: Batch 0/1
Batch Loss = 1.8185488767373632
1741, epoch_train_loss=1.8185488767373632
Epoch 1742
Epoch 1742 :: Batch 0/1
Batch Loss = 2.340497548056678
1742, epoch_train_loss=2.340497548056678
Epoch 1743
Epoch 1743 :: Batch 0/1
Batch Loss = 2.1259964214050253
1743, epoch_train_loss=2.1259964214050253
Epoch 1744
Epoch 1744 :: Batch 0/1
Batch Loss = 2.304627611513848
1744, epoch_train_loss=2.304627611513848
Epoch 1745
Epoch 1745 :: Batch 0/1
Batch Loss = 2.1832459171615004
1745, epoch_train_loss=2.1832459171615004
Epoch 1746
Epoch 1746 :: Batch 0/1
Batch Loss = 2.353501896399192
1746, epoch_train_loss=2.353501896399192
Epoch 1747
Epoch 1747 :: Batch 0/1
Batch Loss = 2.1889586402792798
1747, epoch_train_loss=2.1889586402792798
Epoch 1748
Epoch 1748 :: Batch 0/1
Batch Loss = 1.951634177078088
1748, epoch_train_loss=1.951634177078088
Epoch 1749
Epoch 1749 :: Batch 0/1
Batch Loss = 2.1383564307432774
1749, epoch_train_loss=2.1383564307432774
Epoch 1750
Epoch 1750 :: Batch 0/1
Batch Loss = 1.8925511202407246
1750, epoch_train_loss=1.8925511202407246
Epoch 1751
Epoch 1751 :: Batch 0/1
Batch Loss = 2.323717613206787
1751, epoch_train_loss=2.323717613206787
Epoch 1752
Epoch 1752 :: Batch 0/1
Batch Loss = 2.83320752241188
1752, epoch_train_loss=2.83320752241188
Epoch 1753
Epoch 1753 :: Batch 0/1
Batch Loss = 2.271301773641829
1753, epoch_train_loss=2.271301773641829
Epoch 1754
Epoch 1754 :: Batch 0/1
Batch Loss = 2.1341145050773487
1754, epoch_train_loss=2.1341145050773487
Epoch 1755
Epoch 1755 :: Batch 0/1
Batch Loss = 2.8095942040875936
1755, epoch_train_loss=2.8095942040875936
Epoch 1756
Epoch 1756 :: Batch 0/1
Batch Loss = 2.380885779857706
1756, epoch_train_loss=2.380885779857706
Epoch 1757
Epoch 1757 :: Batch 0/1
Batch Loss = 2.141979872806905
1757, epoch_train_loss=2.141979872806905
Epoch 1758
Epoch 1758 :: Batch 0/1
Batch Loss = 2.649663444024456
1758, epoch_train_loss=2.649663444024456
Epoch 1759
Epoch 1759 :: Batch 0/1
Batch Loss = 2.187277325904347
1759, epoch_train_loss=2.187277325904347
Epoch 1760
Epoch 1760 :: Batch 0/1
Batch Loss = 2.362368769897619
1760, epoch_train_loss=2.362368769897619
Epoch 1761
Epoch 1761 :: Batch 0/1
Batch Loss = 2.4404762084675213
1761, epoch_train_loss=2.4404762084675213
Epoch 1762
Epoch 1762 :: Batch 0/1
Batch Loss = 2.16298735728456
1762, epoch_train_loss=2.16298735728456
Epoch 1763
Epoch 1763 :: Batch 0/1
Batch Loss = 2.2402107286942092
1763, epoch_train_loss=2.2402107286942092
Epoch 1764
Epoch 1764 :: Batch 0/1
Batch Loss = 2.268559191647896
1764, epoch_train_loss=2.268559191647896
Epoch 1765
Epoch 1765 :: Batch 0/1
Batch Loss = 2.0630337240079903
1765, epoch_train_loss=2.0630337240079903
Epoch 1766
Epoch 1766 :: Batch 0/1
Batch Loss = 2.2381370826578797
1766, epoch_train_loss=2.2381370826578797
Epoch 1767
Epoch 1767 :: Batch 0/1
Batch Loss = 2.153954099477835
1767, epoch_train_loss=2.153954099477835
Epoch 1768
Epoch 1768 :: Batch 0/1
Batch Loss = 2.0665525853210647
1768, epoch_train_loss=2.0665525853210647
Epoch 1769
Epoch 1769 :: Batch 0/1
Batch Loss = 2.174193787150999
1769, epoch_train_loss=2.174193787150999
Epoch 1770
Epoch 1770 :: Batch 0/1
Batch Loss = 2.1375159072491883
1770, epoch_train_loss=2.1375159072491883
Epoch 1771
Epoch 1771 :: Batch 0/1
Batch Loss = 2.0453675147254637
1771, epoch_train_loss=2.0453675147254637
Epoch 1772
Epoch 1772 :: Batch 0/1
Batch Loss = 2.1026705677764803
1772, epoch_train_loss=2.1026705677764803
Epoch 1773
Epoch 1773 :: Batch 0/1
Batch Loss = 2.1014478285045404
1773, epoch_train_loss=2.1014478285045404
Epoch 1774
Epoch 1774 :: Batch 0/1
Batch Loss = 2.0199217972619827
1774, epoch_train_loss=2.0199217972619827
Epoch 1775
Epoch 1775 :: Batch 0/1
Batch Loss = 2.0449761364842556
1775, epoch_train_loss=2.0449761364842556
Epoch 1776
Epoch 1776 :: Batch 0/1
Batch Loss = 2.0449556700420537
1776, epoch_train_loss=2.0449556700420537
Epoch 1777
Epoch 1777 :: Batch 0/1
Batch Loss = 2.000796367105709
1777, epoch_train_loss=2.000796367105709
Epoch 1778
Epoch 1778 :: Batch 0/1
Batch Loss = 2.003023844728646
1778, epoch_train_loss=2.003023844728646
Epoch 1779
Epoch 1779 :: Batch 0/1
Batch Loss = 1.9886536865288722
1779, epoch_train_loss=1.9886536865288722
Epoch 1780
Epoch 1780 :: Batch 0/1
Batch Loss = 1.9865651049442588
1780, epoch_train_loss=1.9865651049442588
Epoch 1781
Epoch 1781 :: Batch 0/1
Batch Loss = 1.9659761233881898
1781, epoch_train_loss=1.9659761233881898
Epoch 1782
Epoch 1782 :: Batch 0/1
Batch Loss = 1.9489742882140133
1782, epoch_train_loss=1.9489742882140133
Epoch 1783
Epoch 1783 :: Batch 0/1
Batch Loss = 1.9681112040684743
1783, epoch_train_loss=1.9681112040684743
Epoch 1784
Epoch 1784 :: Batch 0/1
Batch Loss = 1.9373232706744086
1784, epoch_train_loss=1.9373232706744086
Epoch 1785
Epoch 1785 :: Batch 0/1
Batch Loss = 1.9405669430362487
1785, epoch_train_loss=1.9405669430362487
Epoch 1786
Epoch 1786 :: Batch 0/1
Batch Loss = 1.9357802582212316
1786, epoch_train_loss=1.9357802582212316
Epoch 1787
Epoch 1787 :: Batch 0/1
Batch Loss = 1.9313651919779735
1787, epoch_train_loss=1.9313651919779735
Epoch 1788
Epoch 1788 :: Batch 0/1
Batch Loss = 1.9269291380374622
1788, epoch_train_loss=1.9269291380374622
Epoch 1789
Epoch 1789 :: Batch 0/1
Batch Loss = 1.9080655678613458
1789, epoch_train_loss=1.9080655678613458
Epoch 1790
Epoch 1790 :: Batch 0/1
Batch Loss = 1.914175849508748
1790, epoch_train_loss=1.914175849508748
Epoch 1791
Epoch 1791 :: Batch 0/1
Batch Loss = 1.8926956278569622
1791, epoch_train_loss=1.8926956278569622
Epoch 1792
Epoch 1792 :: Batch 0/1
Batch Loss = 1.8831247179267974
1792, epoch_train_loss=1.8831247179267974
Epoch 1793
Epoch 1793 :: Batch 0/1
Batch Loss = 1.8633390035938768
1793, epoch_train_loss=1.8633390035938768
Epoch 1794
Epoch 1794 :: Batch 0/1
Batch Loss = 1.861406940662658
1794, epoch_train_loss=1.861406940662658
Epoch 1795
Epoch 1795 :: Batch 0/1
Batch Loss = 1.8521664321426083
1795, epoch_train_loss=1.8521664321426083
Epoch 1796
Epoch 1796 :: Batch 0/1
Batch Loss = 1.8513208806198225
1796, epoch_train_loss=1.8513208806198225
Epoch 1797
Epoch 1797 :: Batch 0/1
Batch Loss = 1.8490755318890912
1797, epoch_train_loss=1.8490755318890912
Epoch 1798
Epoch 1798 :: Batch 0/1
Batch Loss = 1.8303184916036785
1798, epoch_train_loss=1.8303184916036785
Epoch 1799
Epoch 1799 :: Batch 0/1
Batch Loss = 1.8216139886747804
1799, epoch_train_loss=1.8216139886747804
Epoch 1800
Epoch 1800 :: Batch 0/1
Batch Loss = 1.8213683883273024
1800, epoch_train_loss=1.8213683883273024
Epoch 1801
Epoch 1801 :: Batch 0/1
Batch Loss = 1.8138054391838445
1801, epoch_train_loss=1.8138054391838445
Epoch 1802
Epoch 1802 :: Batch 0/1
Batch Loss = 1.8093296897807847
1802, epoch_train_loss=1.8093296897807847
Epoch 1803
Epoch 1803 :: Batch 0/1
Batch Loss = 1.8087452417667411
1803, epoch_train_loss=1.8087452417667411
Epoch 1804
Epoch 1804 :: Batch 0/1
Batch Loss = 1.803178086521074
1804, epoch_train_loss=1.803178086521074
Epoch 1805
Epoch 1805 :: Batch 0/1
Batch Loss = 1.7957111261452234
1805, epoch_train_loss=1.7957111261452234
Epoch 1806
Epoch 1806 :: Batch 0/1
Batch Loss = 1.7883602564783365
1806, epoch_train_loss=1.7883602564783365
Epoch 1807
Epoch 1807 :: Batch 0/1
Batch Loss = 1.7832394422431872
1807, epoch_train_loss=1.7832394422431872
Epoch 1808
Epoch 1808 :: Batch 0/1
Batch Loss = 1.7766883184172502
1808, epoch_train_loss=1.7766883184172502
Epoch 1809
Epoch 1809 :: Batch 0/1
Batch Loss = 1.7704823550986957
1809, epoch_train_loss=1.7704823550986957
Epoch 1810
Epoch 1810 :: Batch 0/1
Batch Loss = 1.7681115909992247
1810, epoch_train_loss=1.7681115909992247
Epoch 1811
Epoch 1811 :: Batch 0/1
Batch Loss = 1.7765549998642078
1811, epoch_train_loss=1.7765549998642078
Epoch 1812
Epoch 1812 :: Batch 0/1
Batch Loss = 1.794884561200731
1812, epoch_train_loss=1.794884561200731
Epoch 1813
Epoch 1813 :: Batch 0/1
Batch Loss = 1.8594284494533906
1813, epoch_train_loss=1.8594284494533906
Epoch 1814
Epoch 1814 :: Batch 0/1
Batch Loss = 1.8018195444817493
1814, epoch_train_loss=1.8018195444817493
Epoch 1815
Epoch 1815 :: Batch 0/1
Batch Loss = 1.7789918845295707
1815, epoch_train_loss=1.7789918845295707
Epoch 1816
Epoch 1816 :: Batch 0/1
Batch Loss = 1.7457005859277117
1816, epoch_train_loss=1.7457005859277117
Epoch 1817
Epoch 1817 :: Batch 0/1
Batch Loss = 1.742192953109186
1817, epoch_train_loss=1.742192953109186
Epoch 1818
Epoch 1818 :: Batch 0/1
Batch Loss = 1.792909178897372
1818, epoch_train_loss=1.792909178897372
Epoch 1819
Epoch 1819 :: Batch 0/1
Batch Loss = 1.9776131653745075
1819, epoch_train_loss=1.9776131653745075
Epoch 1820
Epoch 1820 :: Batch 0/1
Batch Loss = 2.593567063720655
1820, epoch_train_loss=2.593567063720655
Epoch 1821
Epoch 1821 :: Batch 0/1
Batch Loss = 1.7869084194331146
1821, epoch_train_loss=1.7869084194331146
Epoch 1822
Epoch 1822 :: Batch 0/1
Batch Loss = 2.835666812958454
1822, epoch_train_loss=2.835666812958454
Epoch 1823
Epoch 1823 :: Batch 0/1
Batch Loss = 2.156840256601475
1823, epoch_train_loss=2.156840256601475
Epoch 1824
Epoch 1824 :: Batch 0/1
Batch Loss = 2.851571621314735
1824, epoch_train_loss=2.851571621314735
Epoch 1825
Epoch 1825 :: Batch 0/1
Batch Loss = 2.1209369124800177
1825, epoch_train_loss=2.1209369124800177
Epoch 1826
Epoch 1826 :: Batch 0/1
Batch Loss = 2.5810505435618105
1826, epoch_train_loss=2.5810505435618105
Epoch 1827
Epoch 1827 :: Batch 0/1
Batch Loss = 2.1919002221316206
1827, epoch_train_loss=2.1919002221316206
Epoch 1828
Epoch 1828 :: Batch 0/1
Batch Loss = 2.466192665744114
1828, epoch_train_loss=2.466192665744114
Epoch 1829
Epoch 1829 :: Batch 0/1
Batch Loss = 2.3150740097893046
1829, epoch_train_loss=2.3150740097893046
Epoch 1830
Epoch 1830 :: Batch 0/1
Batch Loss = 2.1771546800582615
1830, epoch_train_loss=2.1771546800582615
Epoch 1831
Epoch 1831 :: Batch 0/1
Batch Loss = 2.3267029465570537
1831, epoch_train_loss=2.3267029465570537
Epoch 1832
Epoch 1832 :: Batch 0/1
Batch Loss = 2.1960450576218125
1832, epoch_train_loss=2.1960450576218125
Epoch 1833
Epoch 1833 :: Batch 0/1
Batch Loss = 2.179914052883093
1833, epoch_train_loss=2.179914052883093
Epoch 1834
Epoch 1834 :: Batch 0/1
Batch Loss = 2.2658661522487495
1834, epoch_train_loss=2.2658661522487495
Epoch 1835
Epoch 1835 :: Batch 0/1
Batch Loss = 2.1854400319585454
1835, epoch_train_loss=2.1854400319585454
Epoch 1836
Epoch 1836 :: Batch 0/1
Batch Loss = 2.1696913180496846
1836, epoch_train_loss=2.1696913180496846
Epoch 1837
Epoch 1837 :: Batch 0/1
Batch Loss = 2.1992191359805573
1837, epoch_train_loss=2.1992191359805573
Epoch 1838
Epoch 1838 :: Batch 0/1
Batch Loss = 2.154221270331696
1838, epoch_train_loss=2.154221270331696
Epoch 1839
Epoch 1839 :: Batch 0/1
Batch Loss = 2.1255366500514574
1839, epoch_train_loss=2.1255366500514574
Epoch 1840
Epoch 1840 :: Batch 0/1
Batch Loss = 2.1603384602464675
1840, epoch_train_loss=2.1603384602464675
Epoch 1841
Epoch 1841 :: Batch 0/1
Batch Loss = 2.115214245139821
1841, epoch_train_loss=2.115214245139821
Epoch 1842
Epoch 1842 :: Batch 0/1
Batch Loss = 2.1000609527007934
1842, epoch_train_loss=2.1000609527007934
Epoch 1843
Epoch 1843 :: Batch 0/1
Batch Loss = 2.134719733675144
1843, epoch_train_loss=2.134719733675144
Epoch 1844
Epoch 1844 :: Batch 0/1
Batch Loss = 2.0997892163572334
1844, epoch_train_loss=2.0997892163572334
Epoch 1845
Epoch 1845 :: Batch 0/1
Batch Loss = 2.0697445284350757
1845, epoch_train_loss=2.0697445284350757
Epoch 1846
Epoch 1846 :: Batch 0/1
Batch Loss = 2.0779996832709084
1846, epoch_train_loss=2.0779996832709084
Epoch 1847
Epoch 1847 :: Batch 0/1
Batch Loss = 2.067416462674745
1847, epoch_train_loss=2.067416462674745
Epoch 1848
Epoch 1848 :: Batch 0/1
Batch Loss = 2.0481760304985386
1848, epoch_train_loss=2.0481760304985386
Epoch 1849
Epoch 1849 :: Batch 0/1
Batch Loss = 2.0458598737772866
1849, epoch_train_loss=2.0458598737772866
Epoch 1850
Epoch 1850 :: Batch 0/1
Batch Loss = 2.028605551804732
1850, epoch_train_loss=2.028605551804732
Epoch 1851
Epoch 1851 :: Batch 0/1
Batch Loss = 2.0191562410686474
1851, epoch_train_loss=2.0191562410686474
Epoch 1852
Epoch 1852 :: Batch 0/1
Batch Loss = 2.0226846947753443
1852, epoch_train_loss=2.0226846947753443
Epoch 1853
Epoch 1853 :: Batch 0/1
Batch Loss = 1.9982670728713547
1853, epoch_train_loss=1.9982670728713547
Epoch 1854
Epoch 1854 :: Batch 0/1
Batch Loss = 1.9994568610096908
1854, epoch_train_loss=1.9994568610096908
Epoch 1855
Epoch 1855 :: Batch 0/1
Batch Loss = 1.9853068545766286
1855, epoch_train_loss=1.9853068545766286
Epoch 1856
Epoch 1856 :: Batch 0/1
Batch Loss = 1.980127282945107
1856, epoch_train_loss=1.980127282945107
Epoch 1857
Epoch 1857 :: Batch 0/1
Batch Loss = 1.977532268403692
1857, epoch_train_loss=1.977532268403692
Epoch 1858
Epoch 1858 :: Batch 0/1
Batch Loss = 1.9630673148874949
1858, epoch_train_loss=1.9630673148874949
Epoch 1859
Epoch 1859 :: Batch 0/1
Batch Loss = 1.9630531764130563
1859, epoch_train_loss=1.9630531764130563
Epoch 1860
Epoch 1860 :: Batch 0/1
Batch Loss = 1.9466348291506528
1860, epoch_train_loss=1.9466348291506528
Epoch 1861
Epoch 1861 :: Batch 0/1
Batch Loss = 1.948806122961735
1861, epoch_train_loss=1.948806122961735
Epoch 1862
Epoch 1862 :: Batch 0/1
Batch Loss = 1.9368873281509016
1862, epoch_train_loss=1.9368873281509016
Epoch 1863
Epoch 1863 :: Batch 0/1
Batch Loss = 1.9345861898948116
1863, epoch_train_loss=1.9345861898948116
Epoch 1864
Epoch 1864 :: Batch 0/1
Batch Loss = 1.9296671010776958
1864, epoch_train_loss=1.9296671010776958
Epoch 1865
Epoch 1865 :: Batch 0/1
Batch Loss = 1.9239559241727013
1865, epoch_train_loss=1.9239559241727013
Epoch 1866
Epoch 1866 :: Batch 0/1
Batch Loss = 1.9268543753053562
1866, epoch_train_loss=1.9268543753053562
Epoch 1867
Epoch 1867 :: Batch 0/1
Batch Loss = 1.918042394294665
1867, epoch_train_loss=1.918042394294665
Epoch 1868
Epoch 1868 :: Batch 0/1
Batch Loss = 1.9209432673663218
1868, epoch_train_loss=1.9209432673663218
Epoch 1869
Epoch 1869 :: Batch 0/1
Batch Loss = 1.9157807096113098
1869, epoch_train_loss=1.9157807096113098
Epoch 1870
Epoch 1870 :: Batch 0/1
Batch Loss = 1.913661279130383
1870, epoch_train_loss=1.913661279130383
Epoch 1871
Epoch 1871 :: Batch 0/1
Batch Loss = 1.9139823498457005
1871, epoch_train_loss=1.9139823498457005
Epoch 1872
Epoch 1872 :: Batch 0/1
Batch Loss = 1.9072331693382876
1872, epoch_train_loss=1.9072331693382876
Epoch 1873
Epoch 1873 :: Batch 0/1
Batch Loss = 1.9076613379257499
1873, epoch_train_loss=1.9076613379257499
Epoch 1874
Epoch 1874 :: Batch 0/1
Batch Loss = 1.903842977512991
1874, epoch_train_loss=1.903842977512991
Epoch 1875
Epoch 1875 :: Batch 0/1
Batch Loss = 1.9001411064133602
1875, epoch_train_loss=1.9001411064133602
Epoch 1876
Epoch 1876 :: Batch 0/1
Batch Loss = 1.900116913132662
1876, epoch_train_loss=1.900116913132662
Epoch 1877
Epoch 1877 :: Batch 0/1
Batch Loss = 1.8960568000686686
1877, epoch_train_loss=1.8960568000686686
Epoch 1878
Epoch 1878 :: Batch 0/1
Batch Loss = 1.8955709559043945
1878, epoch_train_loss=1.8955709559043945
Epoch 1879
Epoch 1879 :: Batch 0/1
Batch Loss = 1.8931767097693784
1879, epoch_train_loss=1.8931767097693784
Epoch 1880
Epoch 1880 :: Batch 0/1
Batch Loss = 1.8898698737326611
1880, epoch_train_loss=1.8898698737326611
Epoch 1881
Epoch 1881 :: Batch 0/1
Batch Loss = 1.889427567445672
1881, epoch_train_loss=1.889427567445672
Epoch 1882
Epoch 1882 :: Batch 0/1
Batch Loss = 1.8862431788217724
1882, epoch_train_loss=1.8862431788217724
Epoch 1883
Epoch 1883 :: Batch 0/1
Batch Loss = 1.884473024669262
1883, epoch_train_loss=1.884473024669262
Epoch 1884
Epoch 1884 :: Batch 0/1
Batch Loss = 1.8832683014331537
1884, epoch_train_loss=1.8832683014331537
Epoch 1885
Epoch 1885 :: Batch 0/1
Batch Loss = 1.8806064147692358
1885, epoch_train_loss=1.8806064147692358
Epoch 1886
Epoch 1886 :: Batch 0/1
Batch Loss = 1.8797983850867657
1886, epoch_train_loss=1.8797983850867657
Epoch 1887
Epoch 1887 :: Batch 0/1
Batch Loss = 1.8782115417039684
1887, epoch_train_loss=1.8782115417039684
Epoch 1888
Epoch 1888 :: Batch 0/1
Batch Loss = 1.8763453134918675
1888, epoch_train_loss=1.8763453134918675
Epoch 1889
Epoch 1889 :: Batch 0/1
Batch Loss = 1.8756622914851724
1889, epoch_train_loss=1.8756622914851724
Epoch 1890
Epoch 1890 :: Batch 0/1
Batch Loss = 1.8738126878486394
1890, epoch_train_loss=1.8738126878486394
Epoch 1891
Epoch 1891 :: Batch 0/1
Batch Loss = 1.8722408443168508
1891, epoch_train_loss=1.8722408443168508
Epoch 1892
Epoch 1892 :: Batch 0/1
Batch Loss = 1.8712010171358369
1892, epoch_train_loss=1.8712010171358369
Epoch 1893
Epoch 1893 :: Batch 0/1
Batch Loss = 1.8693917968096527
1893, epoch_train_loss=1.8693917968096527
Epoch 1894
Epoch 1894 :: Batch 0/1
Batch Loss = 1.8681888724050644
1894, epoch_train_loss=1.8681888724050644
Epoch 1895
Epoch 1895 :: Batch 0/1
Batch Loss = 1.8669404065322766
1895, epoch_train_loss=1.8669404065322766
Epoch 1896
Epoch 1896 :: Batch 0/1
Batch Loss = 1.8652432152306178
1896, epoch_train_loss=1.8652432152306178
Epoch 1897
Epoch 1897 :: Batch 0/1
Batch Loss = 1.8642649827175142
1897, epoch_train_loss=1.8642649827175142
Epoch 1898
Epoch 1898 :: Batch 0/1
Batch Loss = 1.8630832365830288
1898, epoch_train_loss=1.8630832365830288
Epoch 1899
Epoch 1899 :: Batch 0/1
Batch Loss = 1.8617111532174444
1899, epoch_train_loss=1.8617111532174444
Epoch 1900
Epoch 1900 :: Batch 0/1
Batch Loss = 1.86079502920199
1900, epoch_train_loss=1.86079502920199
Epoch 1901
Epoch 1901 :: Batch 0/1
Batch Loss = 1.8595531780542955
1901, epoch_train_loss=1.8595531780542955
Epoch 1902
Epoch 1902 :: Batch 0/1
Batch Loss = 1.8583290286729002
1902, epoch_train_loss=1.8583290286729002
Epoch 1903
Epoch 1903 :: Batch 0/1
Batch Loss = 1.8573336592212748
1903, epoch_train_loss=1.8573336592212748
Epoch 1904
Epoch 1904 :: Batch 0/1
Batch Loss = 1.8560638145976822
1904, epoch_train_loss=1.8560638145976822
Epoch 1905
Epoch 1905 :: Batch 0/1
Batch Loss = 1.85489402230086
1905, epoch_train_loss=1.85489402230086
Epoch 1906
Epoch 1906 :: Batch 0/1
Batch Loss = 1.8538474765908513
1906, epoch_train_loss=1.8538474765908513
Epoch 1907
Epoch 1907 :: Batch 0/1
Batch Loss = 1.852585010149655
1907, epoch_train_loss=1.852585010149655
Epoch 1908
Epoch 1908 :: Batch 0/1
Batch Loss = 1.8514055244229675
1908, epoch_train_loss=1.8514055244229675
Epoch 1909
Epoch 1909 :: Batch 0/1
Batch Loss = 1.8503447699448543
1909, epoch_train_loss=1.8503447699448543
Epoch 1910
Epoch 1910 :: Batch 0/1
Batch Loss = 1.8491190629941816
1910, epoch_train_loss=1.8491190629941816
Epoch 1911
Epoch 1911 :: Batch 0/1
Batch Loss = 1.8479475199250195
1911, epoch_train_loss=1.8479475199250195
Epoch 1912
Epoch 1912 :: Batch 0/1
Batch Loss = 1.846865688727686
1912, epoch_train_loss=1.846865688727686
Epoch 1913
Epoch 1913 :: Batch 0/1
Batch Loss = 1.8456750396155432
1913, epoch_train_loss=1.8456750396155432
Epoch 1914
Epoch 1914 :: Batch 0/1
Batch Loss = 1.8445090786804788
1914, epoch_train_loss=1.8445090786804788
Epoch 1915
Epoch 1915 :: Batch 0/1
Batch Loss = 1.8434135622799595
1915, epoch_train_loss=1.8434135622799595
Epoch 1916
Epoch 1916 :: Batch 0/1
Batch Loss = 1.8422483304794588
1916, epoch_train_loss=1.8422483304794588
Epoch 1917
Epoch 1917 :: Batch 0/1
Batch Loss = 1.8410557183078842
1917, epoch_train_loss=1.8410557183078842
Epoch 1918
Epoch 1918 :: Batch 0/1
Batch Loss = 1.8399214775401596
1918, epoch_train_loss=1.8399214775401596
Epoch 1919
Epoch 1919 :: Batch 0/1
Batch Loss = 1.8387513157946334
1919, epoch_train_loss=1.8387513157946334
Epoch 1920
Epoch 1920 :: Batch 0/1
Batch Loss = 1.837549385053143
1920, epoch_train_loss=1.837549385053143
Epoch 1921
Epoch 1921 :: Batch 0/1
Batch Loss = 1.8363960292619224
1921, epoch_train_loss=1.8363960292619224
Epoch 1922
Epoch 1922 :: Batch 0/1
Batch Loss = 1.8352424064731148
1922, epoch_train_loss=1.8352424064731148
Epoch 1923
Epoch 1923 :: Batch 0/1
Batch Loss = 1.8340495198329163
1923, epoch_train_loss=1.8340495198329163
Epoch 1924
Epoch 1924 :: Batch 0/1
Batch Loss = 1.832872580576566
1924, epoch_train_loss=1.832872580576566
Epoch 1925
Epoch 1925 :: Batch 0/1
Batch Loss = 1.8317240890110835
1925, epoch_train_loss=1.8317240890110835
Epoch 1926
Epoch 1926 :: Batch 0/1
Batch Loss = 1.8305470620529403
1926, epoch_train_loss=1.8305470620529403
Epoch 1927
Epoch 1927 :: Batch 0/1
Batch Loss = 1.8293500248338448
1927, epoch_train_loss=1.8293500248338448
Epoch 1928
Epoch 1928 :: Batch 0/1
Batch Loss = 1.8281676420245052
1928, epoch_train_loss=1.8281676420245052
Epoch 1929
Epoch 1929 :: Batch 0/1
Batch Loss = 1.8269872067727948
1929, epoch_train_loss=1.8269872067727948
Epoch 1930
Epoch 1930 :: Batch 0/1
Batch Loss = 1.82578251126892
1930, epoch_train_loss=1.82578251126892
Epoch 1931
Epoch 1931 :: Batch 0/1
Batch Loss = 1.82456927368069
1931, epoch_train_loss=1.82456927368069
Epoch 1932
Epoch 1932 :: Batch 0/1
Batch Loss = 1.8233658008114748
1932, epoch_train_loss=1.8233658008114748
Epoch 1933
Epoch 1933 :: Batch 0/1
Batch Loss = 1.822158378861917
1933, epoch_train_loss=1.822158378861917
Epoch 1934
Epoch 1934 :: Batch 0/1
Batch Loss = 1.8209339811980616
1934, epoch_train_loss=1.8209339811980616
Epoch 1935
Epoch 1935 :: Batch 0/1
Batch Loss = 1.8197012364326437
1935, epoch_train_loss=1.8197012364326437
Epoch 1936
Epoch 1936 :: Batch 0/1
Batch Loss = 1.818471461698557
1936, epoch_train_loss=1.818471461698557
Epoch 1937
Epoch 1937 :: Batch 0/1
Batch Loss = 1.81723716268789
1937, epoch_train_loss=1.81723716268789
Epoch 1938
Epoch 1938 :: Batch 0/1
Batch Loss = 1.8159897963188016
1938, epoch_train_loss=1.8159897963188016
Epoch 1939
Epoch 1939 :: Batch 0/1
Batch Loss = 1.8147294741808262
1939, epoch_train_loss=1.8147294741808262
Epoch 1940
Epoch 1940 :: Batch 0/1
Batch Loss = 1.8134635842875013
1940, epoch_train_loss=1.8134635842875013
Epoch 1941
Epoch 1941 :: Batch 0/1
Batch Loss = 1.812192861648883
1941, epoch_train_loss=1.812192861648883
Epoch 1942
Epoch 1942 :: Batch 0/1
Batch Loss = 1.8109133279626362
1942, epoch_train_loss=1.8109133279626362
Epoch 1943
Epoch 1943 :: Batch 0/1
Batch Loss = 1.8096203933096167
1943, epoch_train_loss=1.8096203933096167
Epoch 1944
Epoch 1944 :: Batch 0/1
Batch Loss = 1.8083144048768867
1944, epoch_train_loss=1.8083144048768867
Epoch 1945
Epoch 1945 :: Batch 0/1
Batch Loss = 1.8069981895667033
1945, epoch_train_loss=1.8069981895667033
Epoch 1946
Epoch 1946 :: Batch 0/1
Batch Loss = 1.8056739540390188
1946, epoch_train_loss=1.8056739540390188
Epoch 1947
Epoch 1947 :: Batch 0/1
Batch Loss = 1.8043406627077236
1947, epoch_train_loss=1.8043406627077236
Epoch 1948
Epoch 1948 :: Batch 0/1
Batch Loss = 1.8029961134831571
1948, epoch_train_loss=1.8029961134831571
Epoch 1949
Epoch 1949 :: Batch 0/1
Batch Loss = 1.8016391155632414
1949, epoch_train_loss=1.8016391155632414
Epoch 1950
Epoch 1950 :: Batch 0/1
Batch Loss = 1.8002697371459282
1950, epoch_train_loss=1.8002697371459282
Epoch 1951
Epoch 1951 :: Batch 0/1
Batch Loss = 1.7988885663529075
1951, epoch_train_loss=1.7988885663529075
Epoch 1952
Epoch 1952 :: Batch 0/1
Batch Loss = 1.7974962689123517
1952, epoch_train_loss=1.7974962689123517
Epoch 1953
Epoch 1953 :: Batch 0/1
Batch Loss = 1.796093680081508
1953, epoch_train_loss=1.796093680081508
Epoch 1954
Epoch 1954 :: Batch 0/1
Batch Loss = 1.7946809947970073
1954, epoch_train_loss=1.7946809947970073
Epoch 1955
Epoch 1955 :: Batch 0/1
Batch Loss = 1.7932582835381299
1955, epoch_train_loss=1.7932582835381299
Epoch 1956
Epoch 1956 :: Batch 0/1
Batch Loss = 1.7918257219063316
1956, epoch_train_loss=1.7918257219063316
Epoch 1957
Epoch 1957 :: Batch 0/1
Batch Loss = 1.7903840163973905
1957, epoch_train_loss=1.7903840163973905
Epoch 1958
Epoch 1958 :: Batch 0/1
Batch Loss = 1.78893370607377
1958, epoch_train_loss=1.78893370607377
Epoch 1959
Epoch 1959 :: Batch 0/1
Batch Loss = 1.7874758707012208
1959, epoch_train_loss=1.7874758707012208
Epoch 1960
Epoch 1960 :: Batch 0/1
Batch Loss = 1.786012424670078
1960, epoch_train_loss=1.786012424670078
Epoch 1961
Epoch 1961 :: Batch 0/1
Batch Loss = 1.784548472288758
1961, epoch_train_loss=1.784548472288758
Epoch 1962
Epoch 1962 :: Batch 0/1
Batch Loss = 1.7830951337882668
1962, epoch_train_loss=1.7830951337882668
Epoch 1963
Epoch 1963 :: Batch 0/1
Batch Loss = 1.7816853402818684
1963, epoch_train_loss=1.7816853402818684
Epoch 1964
Epoch 1964 :: Batch 0/1
Batch Loss = 1.780401950650863
1964, epoch_train_loss=1.780401950650863
Epoch 1965
Epoch 1965 :: Batch 0/1
Batch Loss = 1.779518106115471
1965, epoch_train_loss=1.779518106115471
Epoch 1966
Epoch 1966 :: Batch 0/1
Batch Loss = 1.779744539392022
1966, epoch_train_loss=1.779744539392022
Epoch 1967
Epoch 1967 :: Batch 0/1
Batch Loss = 1.783828310200856
1967, epoch_train_loss=1.783828310200856
Epoch 1968
Epoch 1968 :: Batch 0/1
Batch Loss = 1.797591139987532
1968, epoch_train_loss=1.797591139987532
Epoch 1969
Epoch 1969 :: Batch 0/1
Batch Loss = 1.8474687710839663
1969, epoch_train_loss=1.8474687710839663
Epoch 1970
Epoch 1970 :: Batch 0/1
Batch Loss = 1.9095520512022268
1970, epoch_train_loss=1.9095520512022268
Epoch 1971
Epoch 1971 :: Batch 0/1
Batch Loss = 2.037228968623111
1971, epoch_train_loss=2.037228968623111
Epoch 1972
Epoch 1972 :: Batch 0/1
Batch Loss = 1.8125357426701443
1972, epoch_train_loss=1.8125357426701443
Epoch 1973
Epoch 1973 :: Batch 0/1
Batch Loss = 1.8330017972423627
1973, epoch_train_loss=1.8330017972423627
Epoch 1974
Epoch 1974 :: Batch 0/1
Batch Loss = 2.079187866406178
1974, epoch_train_loss=2.079187866406178
Epoch 1975
Epoch 1975 :: Batch 0/1
Batch Loss = 1.8435134403001148
1975, epoch_train_loss=1.8435134403001148
Epoch 1976
Epoch 1976 :: Batch 0/1
Batch Loss = 1.8121403387742074
1976, epoch_train_loss=1.8121403387742074
Epoch 1977
Epoch 1977 :: Batch 0/1
Batch Loss = 2.053467534492296
1977, epoch_train_loss=2.053467534492296
Epoch 1978
Epoch 1978 :: Batch 0/1
Batch Loss = 1.8437344444223065
1978, epoch_train_loss=1.8437344444223065
Epoch 1979
Epoch 1979 :: Batch 0/1
Batch Loss = 1.8101425698914488
1979, epoch_train_loss=1.8101425698914488
Epoch 1980
Epoch 1980 :: Batch 0/1
Batch Loss = 1.7960096308590405
1980, epoch_train_loss=1.7960096308590405
Epoch 1981
Epoch 1981 :: Batch 0/1
Batch Loss = 1.88387907080444
1981, epoch_train_loss=1.88387907080444
Epoch 1982
Epoch 1982 :: Batch 0/1
Batch Loss = 1.9971488207404324
1982, epoch_train_loss=1.9971488207404324
Epoch 1983
Epoch 1983 :: Batch 0/1
Batch Loss = 1.853500785349711
1983, epoch_train_loss=1.853500785349711
Epoch 1984
Epoch 1984 :: Batch 0/1
Batch Loss = 2.0346946469542506
1984, epoch_train_loss=2.0346946469542506
Epoch 1985
Epoch 1985 :: Batch 0/1
Batch Loss = 2.5457014654787877
1985, epoch_train_loss=2.5457014654787877
Epoch 1986
Epoch 1986 :: Batch 0/1
Batch Loss = 1.9572381411714392
1986, epoch_train_loss=1.9572381411714392
Epoch 1987
Epoch 1987 :: Batch 0/1
Batch Loss = 2.446989816437967
1987, epoch_train_loss=2.446989816437967
Epoch 1988
Epoch 1988 :: Batch 0/1
Batch Loss = 2.0846256197050175
1988, epoch_train_loss=2.0846256197050175
Epoch 1989
Epoch 1989 :: Batch 0/1
Batch Loss = 2.2889503708136556
1989, epoch_train_loss=2.2889503708136556
Epoch 1990
Epoch 1990 :: Batch 0/1
Batch Loss = 1.912898090856616
1990, epoch_train_loss=1.912898090856616
Epoch 1991
Epoch 1991 :: Batch 0/1
Batch Loss = 2.2286065339061234
1991, epoch_train_loss=2.2286065339061234
Epoch 1992
Epoch 1992 :: Batch 0/1
Batch Loss = 2.434120534883174
1992, epoch_train_loss=2.434120534883174
Epoch 1993
Epoch 1993 :: Batch 0/1
Batch Loss = 2.0815601777818693
1993, epoch_train_loss=2.0815601777818693
Epoch 1994
Epoch 1994 :: Batch 0/1
Batch Loss = 2.3014297491935767
1994, epoch_train_loss=2.3014297491935767
Epoch 1995
Epoch 1995 :: Batch 0/1
Batch Loss = 1.9868337104170302
1995, epoch_train_loss=1.9868337104170302
Epoch 1996
Epoch 1996 :: Batch 0/1
Batch Loss = 1.9853200130789157
1996, epoch_train_loss=1.9853200130789157
Epoch 1997
Epoch 1997 :: Batch 0/1
Batch Loss = 2.07996777980709
1997, epoch_train_loss=2.07996777980709
Epoch 1998
Epoch 1998 :: Batch 0/1
Batch Loss = 1.8235784899882286
1998, epoch_train_loss=1.8235784899882286
Epoch 1999
Epoch 1999 :: Batch 0/1
Batch Loss = 2.0062268282926894
1999, epoch_train_loss=2.0062268282926894
Epoch 2000
Epoch 2000 :: Batch 0/1
Batch Loss = 2.071838138858833
2000, epoch_train_loss=2.071838138858833
Epoch 2001
Epoch 2001 :: Batch 0/1
Batch Loss = 2.0053916944542785
2001, epoch_train_loss=2.0053916944542785
Epoch 2002
Epoch 2002 :: Batch 0/1
Batch Loss = 2.0947917295124547
2002, epoch_train_loss=2.0947917295124547
Epoch 2003
Epoch 2003 :: Batch 0/1
Batch Loss = 2.1776739733738957
2003, epoch_train_loss=2.1776739733738957
Epoch 2004
Epoch 2004 :: Batch 0/1
Batch Loss = 2.045433982108898
2004, epoch_train_loss=2.045433982108898
Epoch 2005
Epoch 2005 :: Batch 0/1
Batch Loss = 1.8498587449570618
2005, epoch_train_loss=1.8498587449570618
Epoch 2006
Epoch 2006 :: Batch 0/1
Batch Loss = 2.029339040658247
2006, epoch_train_loss=2.029339040658247
Epoch 2007
Epoch 2007 :: Batch 0/1
Batch Loss = 2.0689495234890956
2007, epoch_train_loss=2.0689495234890956
Epoch 2008
Epoch 2008 :: Batch 0/1
Batch Loss = 1.8827918747550831
2008, epoch_train_loss=1.8827918747550831
Epoch 2009
Epoch 2009 :: Batch 0/1
Batch Loss = 1.9044472483868484
2009, epoch_train_loss=1.9044472483868484
Epoch 2010
Epoch 2010 :: Batch 0/1
Batch Loss = 1.9930570394084106
2010, epoch_train_loss=1.9930570394084106
Epoch 2011
Epoch 2011 :: Batch 0/1
Batch Loss = 1.870173096756329
2011, epoch_train_loss=1.870173096756329
Epoch 2012
Epoch 2012 :: Batch 0/1
Batch Loss = 1.8425012286230145
2012, epoch_train_loss=1.8425012286230145
Epoch 2013
Epoch 2013 :: Batch 0/1
Batch Loss = 1.908682735415086
2013, epoch_train_loss=1.908682735415086
Epoch 2014
Epoch 2014 :: Batch 0/1
Batch Loss = 1.8824442776752734
2014, epoch_train_loss=1.8824442776752734
Epoch 2015
Epoch 2015 :: Batch 0/1
Batch Loss = 1.84412369590332
2015, epoch_train_loss=1.84412369590332
Epoch 2016
Epoch 2016 :: Batch 0/1
Batch Loss = 1.8608337467839864
2016, epoch_train_loss=1.8608337467839864
Epoch 2017
Epoch 2017 :: Batch 0/1
Batch Loss = 1.8689637336569063
2017, epoch_train_loss=1.8689637336569063
Epoch 2018
Epoch 2018 :: Batch 0/1
Batch Loss = 1.8170228100368733
2018, epoch_train_loss=1.8170228100368733
Epoch 2019
Epoch 2019 :: Batch 0/1
Batch Loss = 1.8088423240523104
2019, epoch_train_loss=1.8088423240523104
Epoch 2020
Epoch 2020 :: Batch 0/1
Batch Loss = 1.830451042529826
2020, epoch_train_loss=1.830451042529826
Epoch 2021
Epoch 2021 :: Batch 0/1
Batch Loss = 1.7993983497767998
2021, epoch_train_loss=1.7993983497767998
Epoch 2022
Epoch 2022 :: Batch 0/1
Batch Loss = 1.7702526620721017
2022, epoch_train_loss=1.7702526620721017
Epoch 2023
Epoch 2023 :: Batch 0/1
Batch Loss = 1.789647043611657
2023, epoch_train_loss=1.789647043611657
Epoch 2024
Epoch 2024 :: Batch 0/1
Batch Loss = 1.7932021257030248
2024, epoch_train_loss=1.7932021257030248
Epoch 2025
Epoch 2025 :: Batch 0/1
Batch Loss = 1.7675447073560782
2025, epoch_train_loss=1.7675447073560782
Epoch 2026
Epoch 2026 :: Batch 0/1
Batch Loss = 1.7704910346689096
2026, epoch_train_loss=1.7704910346689096
Epoch 2027
Epoch 2027 :: Batch 0/1
Batch Loss = 1.7850866710691848
2027, epoch_train_loss=1.7850866710691848
Epoch 2028
Epoch 2028 :: Batch 0/1
Batch Loss = 1.7689866398005483
2028, epoch_train_loss=1.7689866398005483
Epoch 2029
Epoch 2029 :: Batch 0/1
Batch Loss = 1.7549168927208696
2029, epoch_train_loss=1.7549168927208696
Epoch 2030
Epoch 2030 :: Batch 0/1
Batch Loss = 1.765570834992101
2030, epoch_train_loss=1.765570834992101
Epoch 2031
Epoch 2031 :: Batch 0/1
Batch Loss = 1.7648947200954517
2031, epoch_train_loss=1.7648947200954517
Epoch 2032
Epoch 2032 :: Batch 0/1
Batch Loss = 1.7506392632624572
2032, epoch_train_loss=1.7506392632624572
Epoch 2033
Epoch 2033 :: Batch 0/1
Batch Loss = 1.7520199086477226
2033, epoch_train_loss=1.7520199086477226
Epoch 2034
Epoch 2034 :: Batch 0/1
Batch Loss = 1.7574577490368142
2034, epoch_train_loss=1.7574577490368142
Epoch 2035
Epoch 2035 :: Batch 0/1
Batch Loss = 1.749782976118346
2035, epoch_train_loss=1.749782976118346
Epoch 2036
Epoch 2036 :: Batch 0/1
Batch Loss = 1.7487015410580655
2036, epoch_train_loss=1.7487015410580655
Epoch 2037
Epoch 2037 :: Batch 0/1
Batch Loss = 1.7646178195963136
2037, epoch_train_loss=1.7646178195963136
Epoch 2038
Epoch 2038 :: Batch 0/1
Batch Loss = 1.7855451399502895
2038, epoch_train_loss=1.7855451399502895
Epoch 2039
Epoch 2039 :: Batch 0/1
Batch Loss = 1.7876736402591595
2039, epoch_train_loss=1.7876736402591595
Epoch 2040
Epoch 2040 :: Batch 0/1
Batch Loss = 1.8241626835855413
2040, epoch_train_loss=1.8241626835855413
Epoch 2041
Epoch 2041 :: Batch 0/1
Batch Loss = 1.8381695678565497
2041, epoch_train_loss=1.8381695678565497
Epoch 2042
Epoch 2042 :: Batch 0/1
Batch Loss = 1.9101882732700455
2042, epoch_train_loss=1.9101882732700455
Epoch 2043
Epoch 2043 :: Batch 0/1
Batch Loss = 1.8045483178471098
2043, epoch_train_loss=1.8045483178471098
Epoch 2044
Epoch 2044 :: Batch 0/1
Batch Loss = 1.7494350633260776
2044, epoch_train_loss=1.7494350633260776
Epoch 2045
Epoch 2045 :: Batch 0/1
Batch Loss = 1.7393468970076704
2045, epoch_train_loss=1.7393468970076704
Epoch 2046
Epoch 2046 :: Batch 0/1
Batch Loss = 1.7989110512250996
2046, epoch_train_loss=1.7989110512250996
Epoch 2047
Epoch 2047 :: Batch 0/1
Batch Loss = 1.9888096427086022
2047, epoch_train_loss=1.9888096427086022
Epoch 2048
Epoch 2048 :: Batch 0/1
Batch Loss = 1.9173944207507838
2048, epoch_train_loss=1.9173944207507838
Epoch 2049
Epoch 2049 :: Batch 0/1
Batch Loss = 1.9022876803792914
2049, epoch_train_loss=1.9022876803792914
Epoch 2050
Epoch 2050 :: Batch 0/1
Batch Loss = 1.759259093601196
2050, epoch_train_loss=1.759259093601196
Epoch 2051
Epoch 2051 :: Batch 0/1
Batch Loss = 1.8481026190962841
2051, epoch_train_loss=1.8481026190962841
Epoch 2052
Epoch 2052 :: Batch 0/1
Batch Loss = 2.0601154778409363
2052, epoch_train_loss=2.0601154778409363
Epoch 2053
Epoch 2053 :: Batch 0/1
Batch Loss = 1.749996424470083
2053, epoch_train_loss=1.749996424470083
Epoch 2054
Epoch 2054 :: Batch 0/1
Batch Loss = 1.8217752241123384
2054, epoch_train_loss=1.8217752241123384
Epoch 2055
Epoch 2055 :: Batch 0/1
Batch Loss = 2.071174301585573
2055, epoch_train_loss=2.071174301585573
Epoch 2056
Epoch 2056 :: Batch 0/1
Batch Loss = 1.7831873545174795
2056, epoch_train_loss=1.7831873545174795
Epoch 2057
Epoch 2057 :: Batch 0/1
Batch Loss = 2.2796154536089808
2057, epoch_train_loss=2.2796154536089808
Epoch 2058
Epoch 2058 :: Batch 0/1
Batch Loss = 2.137003258962436
2058, epoch_train_loss=2.137003258962436
Epoch 2059
Epoch 2059 :: Batch 0/1
Batch Loss = 2.080555272730731
2059, epoch_train_loss=2.080555272730731
Epoch 2060
Epoch 2060 :: Batch 0/1
Batch Loss = 1.977917599368673
2060, epoch_train_loss=1.977917599368673
Epoch 2061
Epoch 2061 :: Batch 0/1
Batch Loss = 1.9013505846472591
2061, epoch_train_loss=1.9013505846472591
Epoch 2062
Epoch 2062 :: Batch 0/1
Batch Loss = 2.0301062851928524
2062, epoch_train_loss=2.0301062851928524
Epoch 2063
Epoch 2063 :: Batch 0/1
Batch Loss = 2.0063048606474907
2063, epoch_train_loss=2.0063048606474907
Epoch 2064
Epoch 2064 :: Batch 0/1
Batch Loss = 1.8961256818168646
2064, epoch_train_loss=1.8961256818168646
Epoch 2065
Epoch 2065 :: Batch 0/1
Batch Loss = 1.8222988967558855
2065, epoch_train_loss=1.8222988967558855
Epoch 2066
Epoch 2066 :: Batch 0/1
Batch Loss = 1.8648385939306373
2066, epoch_train_loss=1.8648385939306373
Epoch 2067
Epoch 2067 :: Batch 0/1
Batch Loss = 1.82404608363546
2067, epoch_train_loss=1.82404608363546
Epoch 2068
Epoch 2068 :: Batch 0/1
Batch Loss = 1.835656102995928
2068, epoch_train_loss=1.835656102995928
Epoch 2069
Epoch 2069 :: Batch 0/1
Batch Loss = 1.776493687072282
2069, epoch_train_loss=1.776493687072282
Epoch 2070
Epoch 2070 :: Batch 0/1
Batch Loss = 1.7755640450321937
2070, epoch_train_loss=1.7755640450321937
Epoch 2071
Epoch 2071 :: Batch 0/1
Batch Loss = 1.8128950558474133
2071, epoch_train_loss=1.8128950558474133
Epoch 2072
Epoch 2072 :: Batch 0/1
Batch Loss = 1.799076383119384
2072, epoch_train_loss=1.799076383119384
Epoch 2073
Epoch 2073 :: Batch 0/1
Batch Loss = 1.7458755846115181
2073, epoch_train_loss=1.7458755846115181
Epoch 2074
Epoch 2074 :: Batch 0/1
Batch Loss = 1.816598382240391
2074, epoch_train_loss=1.816598382240391
Epoch 2075
Epoch 2075 :: Batch 0/1
Batch Loss = 1.9528277404409253
2075, epoch_train_loss=1.9528277404409253
Epoch 2076
Epoch 2076 :: Batch 0/1
Batch Loss = 1.7757287633405554
2076, epoch_train_loss=1.7757287633405554
Epoch 2077
Epoch 2077 :: Batch 0/1
Batch Loss = 1.7359682082894035
2077, epoch_train_loss=1.7359682082894035
Epoch 2078
Epoch 2078 :: Batch 0/1
Batch Loss = 1.7896824070951987
2078, epoch_train_loss=1.7896824070951987
Epoch 2079
Epoch 2079 :: Batch 0/1
Batch Loss = 1.7497636409120865
2079, epoch_train_loss=1.7497636409120865
Epoch 2080
Epoch 2080 :: Batch 0/1
Batch Loss = 1.7176668610608272
2080, epoch_train_loss=1.7176668610608272
Epoch 2081
Epoch 2081 :: Batch 0/1
Batch Loss = 1.7302577782304438
2081, epoch_train_loss=1.7302577782304438
Epoch 2082
Epoch 2082 :: Batch 0/1
Batch Loss = 1.768814893453412
2082, epoch_train_loss=1.768814893453412
Epoch 2083
Epoch 2083 :: Batch 0/1
Batch Loss = 1.8743721628802048
2083, epoch_train_loss=1.8743721628802048
Epoch 2084
Epoch 2084 :: Batch 0/1
Batch Loss = 1.8536018204836073
2084, epoch_train_loss=1.8536018204836073
Epoch 2085
Epoch 2085 :: Batch 0/1
Batch Loss = 1.8082533315037876
2085, epoch_train_loss=1.8082533315037876
Epoch 2086
Epoch 2086 :: Batch 0/1
Batch Loss = 1.7084383276577326
2086, epoch_train_loss=1.7084383276577326
Epoch 2087
Epoch 2087 :: Batch 0/1
Batch Loss = 1.785527887841483
2087, epoch_train_loss=1.785527887841483
Epoch 2088
Epoch 2088 :: Batch 0/1
Batch Loss = 2.018744680793751
2088, epoch_train_loss=2.018744680793751
Epoch 2089
Epoch 2089 :: Batch 0/1
Batch Loss = 1.9327885546051529
2089, epoch_train_loss=1.9327885546051529
Epoch 2090
Epoch 2090 :: Batch 0/1
Batch Loss = 1.814414451366231
2090, epoch_train_loss=1.814414451366231
Epoch 2091
Epoch 2091 :: Batch 0/1
Batch Loss = 1.8103732564577968
2091, epoch_train_loss=1.8103732564577968
Epoch 2092
Epoch 2092 :: Batch 0/1
Batch Loss = 1.9159995747875007
2092, epoch_train_loss=1.9159995747875007
Epoch 2093
Epoch 2093 :: Batch 0/1
Batch Loss = 1.9702362637667212
2093, epoch_train_loss=1.9702362637667212
Epoch 2094
Epoch 2094 :: Batch 0/1
Batch Loss = 1.7045405989198368
2094, epoch_train_loss=1.7045405989198368
Epoch 2095
Epoch 2095 :: Batch 0/1
Batch Loss = 1.9171787567474234
2095, epoch_train_loss=1.9171787567474234
Epoch 2096
Epoch 2096 :: Batch 0/1
Batch Loss = 2.2509571445817893
2096, epoch_train_loss=2.2509571445817893
Epoch 2097
Epoch 2097 :: Batch 0/1
Batch Loss = 2.5226496995785834
2097, epoch_train_loss=2.5226496995785834
Epoch 2098
Epoch 2098 :: Batch 0/1
Batch Loss = 2.000851387932833
2098, epoch_train_loss=2.000851387932833
Epoch 2099
Epoch 2099 :: Batch 0/1
Batch Loss = 2.171518575942267
2099, epoch_train_loss=2.171518575942267
Epoch 2100
Epoch 2100 :: Batch 0/1
Batch Loss = 2.1925663151813883
2100, epoch_train_loss=2.1925663151813883
Epoch 2101
Epoch 2101 :: Batch 0/1
Batch Loss = 2.3036134077577235
2101, epoch_train_loss=2.3036134077577235
Epoch 2102
Epoch 2102 :: Batch 0/1
Batch Loss = 1.8953185883465185
2102, epoch_train_loss=1.8953185883465185
Epoch 2103
Epoch 2103 :: Batch 0/1
Batch Loss = 2.006162660469885
2103, epoch_train_loss=2.006162660469885
Epoch 2104
Epoch 2104 :: Batch 0/1
Batch Loss = 1.9184240346729324
2104, epoch_train_loss=1.9184240346729324
Epoch 2105
Epoch 2105 :: Batch 0/1
Batch Loss = 1.7454924789842758
2105, epoch_train_loss=1.7454924789842758
Epoch 2106
Epoch 2106 :: Batch 0/1
Batch Loss = 1.9148573716828008
2106, epoch_train_loss=1.9148573716828008
Epoch 2107
Epoch 2107 :: Batch 0/1
Batch Loss = 2.2381333231742775
2107, epoch_train_loss=2.2381333231742775
Epoch 2108
Epoch 2108 :: Batch 0/1
Batch Loss = 3.0677104373292408
2108, epoch_train_loss=3.0677104373292408
Epoch 2109
Epoch 2109 :: Batch 0/1
Batch Loss = 3.766116491500019
2109, epoch_train_loss=3.766116491500019
Epoch 2110
Epoch 2110 :: Batch 0/1
Batch Loss = 3.3109444328751465
2110, epoch_train_loss=3.3109444328751465
Epoch 2111
Epoch 2111 :: Batch 0/1
Batch Loss = 2.6696686656824675
2111, epoch_train_loss=2.6696686656824675
Epoch 2112
Epoch 2112 :: Batch 0/1
Batch Loss = 3.183344108957676
2112, epoch_train_loss=3.183344108957676
Epoch 2113
Epoch 2113 :: Batch 0/1
Batch Loss = 3.5939570373158167
2113, epoch_train_loss=3.5939570373158167
Epoch 2114
Epoch 2114 :: Batch 0/1
Batch Loss = 3.1469790543271605
2114, epoch_train_loss=3.1469790543271605
Epoch 2115
Epoch 2115 :: Batch 0/1
Batch Loss = 2.476752262056997
2115, epoch_train_loss=2.476752262056997
Epoch 2116
Epoch 2116 :: Batch 0/1
Batch Loss = 2.5501022064667866
2116, epoch_train_loss=2.5501022064667866
Epoch 2117
Epoch 2117 :: Batch 0/1
Batch Loss = 2.7303053271779225
2117, epoch_train_loss=2.7303053271779225
Epoch 2118
Epoch 2118 :: Batch 0/1
Batch Loss = 2.8578308401487362
2118, epoch_train_loss=2.8578308401487362
Epoch 2119
Epoch 2119 :: Batch 0/1
Batch Loss = 2.883142759229238
2119, epoch_train_loss=2.883142759229238
Epoch 2120
Epoch 2120 :: Batch 0/1
Batch Loss = 2.8717868093662102
2120, epoch_train_loss=2.8717868093662102
Epoch 2121
Epoch 2121 :: Batch 0/1
Batch Loss = 2.818619628562163
2121, epoch_train_loss=2.818619628562163
Epoch 2122
Epoch 2122 :: Batch 0/1
Batch Loss = 2.7348580623867886
2122, epoch_train_loss=2.7348580623867886
Epoch 2123
Epoch 2123 :: Batch 0/1
Batch Loss = 2.671946047503314
2123, epoch_train_loss=2.671946047503314
Epoch 2124
Epoch 2124 :: Batch 0/1
Batch Loss = 2.591917662980845
2124, epoch_train_loss=2.591917662980845
Epoch 2125
Epoch 2125 :: Batch 0/1
Batch Loss = 2.499265182815002
2125, epoch_train_loss=2.499265182815002
Epoch 2126
Epoch 2126 :: Batch 0/1
Batch Loss = 2.483671817695659
2126, epoch_train_loss=2.483671817695659
Epoch 2127
Epoch 2127 :: Batch 0/1
Batch Loss = 2.5585620629025683
2127, epoch_train_loss=2.5585620629025683
Epoch 2128
Epoch 2128 :: Batch 0/1
Batch Loss = 2.6366888428138555
2128, epoch_train_loss=2.6366888428138555
Epoch 2129
Epoch 2129 :: Batch 0/1
Batch Loss = 2.61067109996246
2129, epoch_train_loss=2.61067109996246
Epoch 2130
Epoch 2130 :: Batch 0/1
Batch Loss = 2.447556060717605
2130, epoch_train_loss=2.447556060717605
Epoch 2131
Epoch 2131 :: Batch 0/1
Batch Loss = 2.444572918598133
2131, epoch_train_loss=2.444572918598133
Epoch 2132
Epoch 2132 :: Batch 0/1
Batch Loss = 2.594247847219706
2132, epoch_train_loss=2.594247847219706
Epoch 2133
Epoch 2133 :: Batch 0/1
Batch Loss = 2.4875017541806774
2133, epoch_train_loss=2.4875017541806774
Epoch 2134
Epoch 2134 :: Batch 0/1
Batch Loss = 2.8430101302247692
2134, epoch_train_loss=2.8430101302247692
Epoch 2135
Epoch 2135 :: Batch 0/1
Batch Loss = 2.691804524062139
2135, epoch_train_loss=2.691804524062139
Epoch 2136
Epoch 2136 :: Batch 0/1
Batch Loss = 2.5858117244357723
2136, epoch_train_loss=2.5858117244357723
Epoch 2137
Epoch 2137 :: Batch 0/1
Batch Loss = 2.561357608220018
2137, epoch_train_loss=2.561357608220018
Epoch 2138
Epoch 2138 :: Batch 0/1
Batch Loss = 2.5289805066220463
2138, epoch_train_loss=2.5289805066220463
Epoch 2139
Epoch 2139 :: Batch 0/1
Batch Loss = 2.5143185200803924
2139, epoch_train_loss=2.5143185200803924
Epoch 2140
Epoch 2140 :: Batch 0/1
Batch Loss = 2.510690934152991
2140, epoch_train_loss=2.510690934152991
Epoch 2141
Epoch 2141 :: Batch 0/1
Batch Loss = 2.5053921824694965
2141, epoch_train_loss=2.5053921824694965
Epoch 2142
Epoch 2142 :: Batch 0/1
Batch Loss = 2.5134316502237115
2142, epoch_train_loss=2.5134316502237115
Epoch 2143
Epoch 2143 :: Batch 0/1
Batch Loss = 2.5120774096523832
2143, epoch_train_loss=2.5120774096523832
Epoch 2144
Epoch 2144 :: Batch 0/1
Batch Loss = 2.5097067160842585
2144, epoch_train_loss=2.5097067160842585
Epoch 2145
Epoch 2145 :: Batch 0/1
Batch Loss = 2.510958021254543
2145, epoch_train_loss=2.510958021254543
Epoch 2146
Epoch 2146 :: Batch 0/1
Batch Loss = 2.508933896683239
2146, epoch_train_loss=2.508933896683239
Epoch 2147
Epoch 2147 :: Batch 0/1
Batch Loss = 2.506276210250341
2147, epoch_train_loss=2.506276210250341
Epoch 2148
Epoch 2148 :: Batch 0/1
Batch Loss = 2.4962167297143787
2148, epoch_train_loss=2.4962167297143787
Epoch 2149
Epoch 2149 :: Batch 0/1
Batch Loss = 2.490965226597976
2149, epoch_train_loss=2.490965226597976
Epoch 2150
Epoch 2150 :: Batch 0/1
Batch Loss = 2.4911326009824393
2150, epoch_train_loss=2.4911326009824393
Epoch 2151
Epoch 2151 :: Batch 0/1
Batch Loss = 2.494131854114153
2151, epoch_train_loss=2.494131854114153
Epoch 2152
Epoch 2152 :: Batch 0/1
Batch Loss = 2.4951700779120403
2152, epoch_train_loss=2.4951700779120403
Epoch 2153
Epoch 2153 :: Batch 0/1
Batch Loss = 2.4951312884556054
2153, epoch_train_loss=2.4951312884556054
Epoch 2154
Epoch 2154 :: Batch 0/1
Batch Loss = 2.491389121424704
2154, epoch_train_loss=2.491389121424704
Epoch 2155
Epoch 2155 :: Batch 0/1
Batch Loss = 2.4880883305705206
2155, epoch_train_loss=2.4880883305705206
Epoch 2156
Epoch 2156 :: Batch 0/1
Batch Loss = 2.4803804690439217
2156, epoch_train_loss=2.4803804690439217
Epoch 2157
Epoch 2157 :: Batch 0/1
Batch Loss = 2.474741045741555
2157, epoch_train_loss=2.474741045741555
Epoch 2158
Epoch 2158 :: Batch 0/1
Batch Loss = 2.468269412917471
2158, epoch_train_loss=2.468269412917471
Epoch 2159
Epoch 2159 :: Batch 0/1
Batch Loss = 2.4646298829744016
2159, epoch_train_loss=2.4646298829744016
Epoch 2160
Epoch 2160 :: Batch 0/1
Batch Loss = 2.463075161941195
2160, epoch_train_loss=2.463075161941195
Epoch 2161
Epoch 2161 :: Batch 0/1
Batch Loss = 2.4598636798195694
2161, epoch_train_loss=2.4598636798195694
Epoch 2162
Epoch 2162 :: Batch 0/1
Batch Loss = 2.4579223397874097
2162, epoch_train_loss=2.4579223397874097
Epoch 2163
Epoch 2163 :: Batch 0/1
Batch Loss = 2.4518757990997804
2163, epoch_train_loss=2.4518757990997804
Epoch 2164
Epoch 2164 :: Batch 0/1
Batch Loss = 2.4457255288088033
2164, epoch_train_loss=2.4457255288088033
Epoch 2165
Epoch 2165 :: Batch 0/1
Batch Loss = 2.439391591361679
2165, epoch_train_loss=2.439391591361679
Epoch 2166
Epoch 2166 :: Batch 0/1
Batch Loss = 2.432755432261973
2166, epoch_train_loss=2.432755432261973
Epoch 2167
Epoch 2167 :: Batch 0/1
Batch Loss = 2.428019892120911
2167, epoch_train_loss=2.428019892120911
Epoch 2168
Epoch 2168 :: Batch 0/1
Batch Loss = 2.4216406550405902
2168, epoch_train_loss=2.4216406550405902
Epoch 2169
Epoch 2169 :: Batch 0/1
Batch Loss = 2.41485642823503
2169, epoch_train_loss=2.41485642823503
Epoch 2170
Epoch 2170 :: Batch 0/1
Batch Loss = 2.406283765281732
2170, epoch_train_loss=2.406283765281732
Epoch 2171
Epoch 2171 :: Batch 0/1
Batch Loss = 2.396299574052714
2171, epoch_train_loss=2.396299574052714
Epoch 2172
Epoch 2172 :: Batch 0/1
Batch Loss = 2.387461343706955
2172, epoch_train_loss=2.387461343706955
Epoch 2173
Epoch 2173 :: Batch 0/1
Batch Loss = 2.378828230687515
2173, epoch_train_loss=2.378828230687515
Epoch 2174
Epoch 2174 :: Batch 0/1
Batch Loss = 2.3708132790617573
2174, epoch_train_loss=2.3708132790617573
Epoch 2175
Epoch 2175 :: Batch 0/1
Batch Loss = 2.363332909070949
2175, epoch_train_loss=2.363332909070949
Epoch 2176
Epoch 2176 :: Batch 0/1
Batch Loss = 2.355973704580885
2176, epoch_train_loss=2.355973704580885
Epoch 2177
Epoch 2177 :: Batch 0/1
Batch Loss = 2.349136779207935
2177, epoch_train_loss=2.349136779207935
Epoch 2178
Epoch 2178 :: Batch 0/1
Batch Loss = 2.3434470011006874
2178, epoch_train_loss=2.3434470011006874
Epoch 2179
Epoch 2179 :: Batch 0/1
Batch Loss = 2.3391690192282386
2179, epoch_train_loss=2.3391690192282386
Epoch 2180
Epoch 2180 :: Batch 0/1
Batch Loss = 2.335406555461769
2180, epoch_train_loss=2.335406555461769
Epoch 2181
Epoch 2181 :: Batch 0/1
Batch Loss = 2.330759570131096
2181, epoch_train_loss=2.330759570131096
Epoch 2182
Epoch 2182 :: Batch 0/1
Batch Loss = 2.3248997706014083
2182, epoch_train_loss=2.3248997706014083
Epoch 2183
Epoch 2183 :: Batch 0/1
Batch Loss = 2.319360928621818
2183, epoch_train_loss=2.319360928621818
Epoch 2184
Epoch 2184 :: Batch 0/1
Batch Loss = 2.3142455676593108
2184, epoch_train_loss=2.3142455676593108
Epoch 2185
Epoch 2185 :: Batch 0/1
Batch Loss = 2.3090600385503945
2185, epoch_train_loss=2.3090600385503945
Epoch 2186
Epoch 2186 :: Batch 0/1
Batch Loss = 2.3036888257250316
2186, epoch_train_loss=2.3036888257250316
Epoch 2187
Epoch 2187 :: Batch 0/1
Batch Loss = 2.298149218443111
2187, epoch_train_loss=2.298149218443111
Epoch 2188
Epoch 2188 :: Batch 0/1
Batch Loss = 2.293046387046199
2188, epoch_train_loss=2.293046387046199
Epoch 2189
Epoch 2189 :: Batch 0/1
Batch Loss = 2.2892220492825612
2189, epoch_train_loss=2.2892220492825612
Epoch 2190
Epoch 2190 :: Batch 0/1
Batch Loss = 2.286749552712228
2190, epoch_train_loss=2.286749552712228
Epoch 2191
Epoch 2191 :: Batch 0/1
Batch Loss = 2.2858816061067597
2191, epoch_train_loss=2.2858816061067597
Epoch 2192
Epoch 2192 :: Batch 0/1
Batch Loss = 2.29093229821917
2192, epoch_train_loss=2.29093229821917
Epoch 2193
Epoch 2193 :: Batch 0/1
Batch Loss = 2.3030575423845696
2193, epoch_train_loss=2.3030575423845696
Epoch 2194
Epoch 2194 :: Batch 0/1
Batch Loss = 2.3288675336204077
2194, epoch_train_loss=2.3288675336204077
Epoch 2195
Epoch 2195 :: Batch 0/1
Batch Loss = 2.3050085511016998
2195, epoch_train_loss=2.3050085511016998
Epoch 2196
Epoch 2196 :: Batch 0/1
Batch Loss = 2.265386803417236
2196, epoch_train_loss=2.265386803417236
Epoch 2197
Epoch 2197 :: Batch 0/1
Batch Loss = 2.255177137044834
2197, epoch_train_loss=2.255177137044834
Epoch 2198
Epoch 2198 :: Batch 0/1
Batch Loss = 2.276983214911073
2198, epoch_train_loss=2.276983214911073
Epoch 2199
Epoch 2199 :: Batch 0/1
Batch Loss = 2.2786370446458952
2199, epoch_train_loss=2.2786370446458952
Epoch 2200
Epoch 2200 :: Batch 0/1
Batch Loss = 2.2419003362985817
2200, epoch_train_loss=2.2419003362985817
Epoch 2201
Epoch 2201 :: Batch 0/1
Batch Loss = 2.237203822962105
2201, epoch_train_loss=2.237203822962105
Epoch 2202
Epoch 2202 :: Batch 0/1
Batch Loss = 2.258329057614918
2202, epoch_train_loss=2.258329057614918
Epoch 2203
Epoch 2203 :: Batch 0/1
Batch Loss = 2.249597011710594
2203, epoch_train_loss=2.249597011710594
Epoch 2204
Epoch 2204 :: Batch 0/1
Batch Loss = 2.22514747391657
2204, epoch_train_loss=2.22514747391657
Epoch 2205
Epoch 2205 :: Batch 0/1
Batch Loss = 2.216888288871652
2205, epoch_train_loss=2.216888288871652
Epoch 2206
Epoch 2206 :: Batch 0/1
Batch Loss = 2.2286803868123837
2206, epoch_train_loss=2.2286803868123837
Epoch 2207
Epoch 2207 :: Batch 0/1
Batch Loss = 2.2328159599324446
2207, epoch_train_loss=2.2328159599324446
Epoch 2208
Epoch 2208 :: Batch 0/1
Batch Loss = 2.2120269925094176
2208, epoch_train_loss=2.2120269925094176
Epoch 2209
Epoch 2209 :: Batch 0/1
Batch Loss = 2.199366074675937
2209, epoch_train_loss=2.199366074675937
Epoch 2210
Epoch 2210 :: Batch 0/1
Batch Loss = 2.205507751935354
2210, epoch_train_loss=2.205507751935354
Epoch 2211
Epoch 2211 :: Batch 0/1
Batch Loss = 2.2087925207480428
2211, epoch_train_loss=2.2087925207480428
Epoch 2212
Epoch 2212 :: Batch 0/1
Batch Loss = 2.2002367553910105
2212, epoch_train_loss=2.2002367553910105
Epoch 2213
Epoch 2213 :: Batch 0/1
Batch Loss = 2.186420520466296
2213, epoch_train_loss=2.186420520466296
Epoch 2214
Epoch 2214 :: Batch 0/1
Batch Loss = 2.183917535182622
2214, epoch_train_loss=2.183917535182622
Epoch 2215
Epoch 2215 :: Batch 0/1
Batch Loss = 2.189189674685623
2215, epoch_train_loss=2.189189674685623
Epoch 2216
Epoch 2216 :: Batch 0/1
Batch Loss = 2.188774061271482
2216, epoch_train_loss=2.188774061271482
Epoch 2217
Epoch 2217 :: Batch 0/1
Batch Loss = 2.1822524366654985
2217, epoch_train_loss=2.1822524366654985
Epoch 2218
Epoch 2218 :: Batch 0/1
Batch Loss = 2.171521436167304
2218, epoch_train_loss=2.171521436167304
Epoch 2219
Epoch 2219 :: Batch 0/1
Batch Loss = 2.1657176290687015
2219, epoch_train_loss=2.1657176290687015
Epoch 2220
Epoch 2220 :: Batch 0/1
Batch Loss = 2.165914924359721
2220, epoch_train_loss=2.165914924359721
Epoch 2221
Epoch 2221 :: Batch 0/1
Batch Loss = 2.168424132153366
2221, epoch_train_loss=2.168424132153366
Epoch 2222
Epoch 2222 :: Batch 0/1
Batch Loss = 2.171704648267875
2222, epoch_train_loss=2.171704648267875
Epoch 2223
Epoch 2223 :: Batch 0/1
Batch Loss = 2.170076283992172
2223, epoch_train_loss=2.170076283992172
Epoch 2224
Epoch 2224 :: Batch 0/1
Batch Loss = 2.167527475475015
2224, epoch_train_loss=2.167527475475015
Epoch 2225
Epoch 2225 :: Batch 0/1
Batch Loss = 2.1587021738386296
2225, epoch_train_loss=2.1587021738386296
Epoch 2226
Epoch 2226 :: Batch 0/1
Batch Loss = 2.150819418793578
2226, epoch_train_loss=2.150819418793578
Epoch 2227
Epoch 2227 :: Batch 0/1
Batch Loss = 2.143692333679821
2227, epoch_train_loss=2.143692333679821
Epoch 2228
Epoch 2228 :: Batch 0/1
Batch Loss = 2.139106521926074
2228, epoch_train_loss=2.139106521926074
Epoch 2229
Epoch 2229 :: Batch 0/1
Batch Loss = 2.1367921462099635
2229, epoch_train_loss=2.1367921462099635
Epoch 2230
Epoch 2230 :: Batch 0/1
Batch Loss = 2.1362028199548435
2230, epoch_train_loss=2.1362028199548435
Epoch 2231
Epoch 2231 :: Batch 0/1
Batch Loss = 2.137675239353356
2231, epoch_train_loss=2.137675239353356
Epoch 2232
Epoch 2232 :: Batch 0/1
Batch Loss = 2.1414402712903953
2232, epoch_train_loss=2.1414402712903953
Epoch 2233
Epoch 2233 :: Batch 0/1
Batch Loss = 2.153005784176071
2233, epoch_train_loss=2.153005784176071
Epoch 2234
Epoch 2234 :: Batch 0/1
Batch Loss = 2.163641330623399
2234, epoch_train_loss=2.163641330623399
Epoch 2235
Epoch 2235 :: Batch 0/1
Batch Loss = 2.1794478267616855
2235, epoch_train_loss=2.1794478267616855
Epoch 2236
Epoch 2236 :: Batch 0/1
Batch Loss = 2.153053033603005
2236, epoch_train_loss=2.153053033603005
Epoch 2237
Epoch 2237 :: Batch 0/1
Batch Loss = 2.1232450771617426
2237, epoch_train_loss=2.1232450771617426
Epoch 2238
Epoch 2238 :: Batch 0/1
Batch Loss = 2.117734084460379
2238, epoch_train_loss=2.117734084460379
Epoch 2239
Epoch 2239 :: Batch 0/1
Batch Loss = 2.135823792271087
2239, epoch_train_loss=2.135823792271087
Epoch 2240
Epoch 2240 :: Batch 0/1
Batch Loss = 2.1544850338811057
2240, epoch_train_loss=2.1544850338811057
Epoch 2241
Epoch 2241 :: Batch 0/1
Batch Loss = 2.1329570701839025
2241, epoch_train_loss=2.1329570701839025
Epoch 2242
Epoch 2242 :: Batch 0/1
Batch Loss = 2.1068711545720102
2242, epoch_train_loss=2.1068711545720102
Epoch 2243
Epoch 2243 :: Batch 0/1
Batch Loss = 2.108152518224727
2243, epoch_train_loss=2.108152518224727
Epoch 2244
Epoch 2244 :: Batch 0/1
Batch Loss = 2.126880715235047
2244, epoch_train_loss=2.126880715235047
Epoch 2245
Epoch 2245 :: Batch 0/1
Batch Loss = 2.138108255245011
2245, epoch_train_loss=2.138108255245011
Epoch 2246
Epoch 2246 :: Batch 0/1
Batch Loss = 2.1127478684503243
2246, epoch_train_loss=2.1127478684503243
Epoch 2247
Epoch 2247 :: Batch 0/1
Batch Loss = 2.096021308419204
2247, epoch_train_loss=2.096021308419204
Epoch 2248
Epoch 2248 :: Batch 0/1
Batch Loss = 2.115036242072987
2248, epoch_train_loss=2.115036242072987
Epoch 2249
Epoch 2249 :: Batch 0/1
Batch Loss = 2.1195933224894374
2249, epoch_train_loss=2.1195933224894374
Epoch 2250
Epoch 2250 :: Batch 0/1
Batch Loss = 2.1045739915893007
2250, epoch_train_loss=2.1045739915893007
Epoch 2251
Epoch 2251 :: Batch 0/1
Batch Loss = 2.0904482597821987
2251, epoch_train_loss=2.0904482597821987
Epoch 2252
Epoch 2252 :: Batch 0/1
Batch Loss = 2.0934989111161126
2252, epoch_train_loss=2.0934989111161126
Epoch 2253
Epoch 2253 :: Batch 0/1
Batch Loss = 2.1048733491362195
2253, epoch_train_loss=2.1048733491362195
Epoch 2254
Epoch 2254 :: Batch 0/1
Batch Loss = 2.0924875402384355
2254, epoch_train_loss=2.0924875402384355
Epoch 2255
Epoch 2255 :: Batch 0/1
Batch Loss = 2.0773066539203158
2255, epoch_train_loss=2.0773066539203158
Epoch 2256
Epoch 2256 :: Batch 0/1
Batch Loss = 2.0807754431582617
2256, epoch_train_loss=2.0807754431582617
Epoch 2257
Epoch 2257 :: Batch 0/1
Batch Loss = 2.079010182224265
2257, epoch_train_loss=2.079010182224265
Epoch 2258
Epoch 2258 :: Batch 0/1
Batch Loss = 2.0746722964258155
2258, epoch_train_loss=2.0746722964258155
Epoch 2259
Epoch 2259 :: Batch 0/1
Batch Loss = 2.070007938214354
2259, epoch_train_loss=2.070007938214354
Epoch 2260
Epoch 2260 :: Batch 0/1
Batch Loss = 2.067070858528697
2260, epoch_train_loss=2.067070858528697
Epoch 2261
Epoch 2261 :: Batch 0/1
Batch Loss = 2.069613556250225
2261, epoch_train_loss=2.069613556250225
Epoch 2262
Epoch 2262 :: Batch 0/1
Batch Loss = 2.063298541860733
2262, epoch_train_loss=2.063298541860733
Epoch 2263
Epoch 2263 :: Batch 0/1
Batch Loss = 2.0598893276830834
2263, epoch_train_loss=2.0598893276830834
Epoch 2264
Epoch 2264 :: Batch 0/1
Batch Loss = 2.060443706458808
2264, epoch_train_loss=2.060443706458808
Epoch 2265
Epoch 2265 :: Batch 0/1
Batch Loss = 2.058213999662004
2265, epoch_train_loss=2.058213999662004
Epoch 2266
Epoch 2266 :: Batch 0/1
Batch Loss = 2.056851116462096
2266, epoch_train_loss=2.056851116462096
Epoch 2267
Epoch 2267 :: Batch 0/1
Batch Loss = 2.0513853478480617
2267, epoch_train_loss=2.0513853478480617
Epoch 2268
Epoch 2268 :: Batch 0/1
Batch Loss = 2.0502926362263842
2268, epoch_train_loss=2.0502926362263842
Epoch 2269
Epoch 2269 :: Batch 0/1
Batch Loss = 2.0510967506670523
2269, epoch_train_loss=2.0510967506670523
Epoch 2270
Epoch 2270 :: Batch 0/1
Batch Loss = 2.0486837075365107
2270, epoch_train_loss=2.0486837075365107
Epoch 2271
Epoch 2271 :: Batch 0/1
Batch Loss = 2.046181401685982
2271, epoch_train_loss=2.046181401685982
Epoch 2272
Epoch 2272 :: Batch 0/1
Batch Loss = 2.0409341559745826
2272, epoch_train_loss=2.0409341559745826
Epoch 2273
Epoch 2273 :: Batch 0/1
Batch Loss = 2.040011639543878
2273, epoch_train_loss=2.040011639543878
Epoch 2274
Epoch 2274 :: Batch 0/1
Batch Loss = 2.0401964102907093
2274, epoch_train_loss=2.0401964102907093
Epoch 2275
Epoch 2275 :: Batch 0/1
Batch Loss = 2.039481143206917
2275, epoch_train_loss=2.039481143206917
Epoch 2276
Epoch 2276 :: Batch 0/1
Batch Loss = 2.037168659109727
2276, epoch_train_loss=2.037168659109727
Epoch 2277
Epoch 2277 :: Batch 0/1
Batch Loss = 2.031822025325722
2277, epoch_train_loss=2.031822025325722
Epoch 2278
Epoch 2278 :: Batch 0/1
Batch Loss = 2.0294068939281624
2278, epoch_train_loss=2.0294068939281624
Epoch 2279
Epoch 2279 :: Batch 0/1
Batch Loss = 2.0280775222735086
2279, epoch_train_loss=2.0280775222735086
Epoch 2280
Epoch 2280 :: Batch 0/1
Batch Loss = 2.028329326373564
2280, epoch_train_loss=2.028329326373564
Epoch 2281
Epoch 2281 :: Batch 0/1
Batch Loss = 2.027571004757937
2281, epoch_train_loss=2.027571004757937
Epoch 2282
Epoch 2282 :: Batch 0/1
Batch Loss = 2.0247181098030467
2282, epoch_train_loss=2.0247181098030467
Epoch 2283
Epoch 2283 :: Batch 0/1
Batch Loss = 2.0217923079553075
2283, epoch_train_loss=2.0217923079553075
Epoch 2284
Epoch 2284 :: Batch 0/1
Batch Loss = 2.0184130947737486
2284, epoch_train_loss=2.0184130947737486
Epoch 2285
Epoch 2285 :: Batch 0/1
Batch Loss = 2.015853588736549
2285, epoch_train_loss=2.015853588736549
Epoch 2286
Epoch 2286 :: Batch 0/1
Batch Loss = 2.0137028624772526
2286, epoch_train_loss=2.0137028624772526
Epoch 2287
Epoch 2287 :: Batch 0/1
Batch Loss = 2.0125348931493305
2287, epoch_train_loss=2.0125348931493305
Epoch 2288
Epoch 2288 :: Batch 0/1
Batch Loss = 2.01155365631176
2288, epoch_train_loss=2.01155365631176
Epoch 2289
Epoch 2289 :: Batch 0/1
Batch Loss = 2.010874206984714
2289, epoch_train_loss=2.010874206984714
Epoch 2290
Epoch 2290 :: Batch 0/1
Batch Loss = 2.0100477770975713
2290, epoch_train_loss=2.0100477770975713
Epoch 2291
Epoch 2291 :: Batch 0/1
Batch Loss = 2.0088371688146704
2291, epoch_train_loss=2.0088371688146704
Epoch 2292
Epoch 2292 :: Batch 0/1
Batch Loss = 2.0081441627859076
2292, epoch_train_loss=2.0081441627859076
Epoch 2293
Epoch 2293 :: Batch 0/1
Batch Loss = 2.007001044006045
2293, epoch_train_loss=2.007001044006045
Epoch 2294
Epoch 2294 :: Batch 0/1
Batch Loss = 2.0061981706497214
2294, epoch_train_loss=2.0061981706497214
Epoch 2295
Epoch 2295 :: Batch 0/1
Batch Loss = 2.004505361689881
2295, epoch_train_loss=2.004505361689881
Epoch 2296
Epoch 2296 :: Batch 0/1
Batch Loss = 2.0041698299869046
2296, epoch_train_loss=2.0041698299869046
Epoch 2297
Epoch 2297 :: Batch 0/1
Batch Loss = 2.003135220873576
2297, epoch_train_loss=2.003135220873576
Epoch 2298
Epoch 2298 :: Batch 0/1
Batch Loss = 2.003662410281058
2298, epoch_train_loss=2.003662410281058
Epoch 2299
Epoch 2299 :: Batch 0/1
Batch Loss = 2.0024531969419246
2299, epoch_train_loss=2.0024531969419246
Epoch 2300
Epoch 2300 :: Batch 0/1
Batch Loss = 2.003408293921507
2300, epoch_train_loss=2.003408293921507
Epoch 2301
Epoch 2301 :: Batch 0/1
Batch Loss = 2.0011487049808117
2301, epoch_train_loss=2.0011487049808117
Epoch 2302
Epoch 2302 :: Batch 0/1
Batch Loss = 2.0000090834381807
2302, epoch_train_loss=2.0000090834381807
Epoch 2303
Epoch 2303 :: Batch 0/1
Batch Loss = 1.9946799126606238
2303, epoch_train_loss=1.9946799126606238
Epoch 2304
Epoch 2304 :: Batch 0/1
Batch Loss = 1.9904746471275974
2304, epoch_train_loss=1.9904746471275974
Epoch 2305
Epoch 2305 :: Batch 0/1
Batch Loss = 1.9847493991131773
2305, epoch_train_loss=1.9847493991131773
Epoch 2306
Epoch 2306 :: Batch 0/1
Batch Loss = 1.9803965241302859
2306, epoch_train_loss=1.9803965241302859
Epoch 2307
Epoch 2307 :: Batch 0/1
Batch Loss = 1.9768453132644241
2307, epoch_train_loss=1.9768453132644241
Epoch 2308
Epoch 2308 :: Batch 0/1
Batch Loss = 1.9743912642271397
2308, epoch_train_loss=1.9743912642271397
Epoch 2309
Epoch 2309 :: Batch 0/1
Batch Loss = 1.9727402688480828
2309, epoch_train_loss=1.9727402688480828
Epoch 2310
Epoch 2310 :: Batch 0/1
Batch Loss = 1.9716929066349107
2310, epoch_train_loss=1.9716929066349107
Epoch 2311
Epoch 2311 :: Batch 0/1
Batch Loss = 1.9712759981825392
2311, epoch_train_loss=1.9712759981825392
Epoch 2312
Epoch 2312 :: Batch 0/1
Batch Loss = 1.9714006102836306
2312, epoch_train_loss=1.9714006102836306
Epoch 2313
Epoch 2313 :: Batch 0/1
Batch Loss = 1.973282314030433
2313, epoch_train_loss=1.973282314030433
Epoch 2314
Epoch 2314 :: Batch 0/1
Batch Loss = 1.9758562273030604
2314, epoch_train_loss=1.9758562273030604
Epoch 2315
Epoch 2315 :: Batch 0/1
Batch Loss = 1.984109214455388
2315, epoch_train_loss=1.984109214455388
Epoch 2316
Epoch 2316 :: Batch 0/1
Batch Loss = 1.9898967236176832
2316, epoch_train_loss=1.9898967236176832
Epoch 2317
Epoch 2317 :: Batch 0/1
Batch Loss = 2.0045844591796906
2317, epoch_train_loss=2.0045844591796906
Epoch 2318
Epoch 2318 :: Batch 0/1
Batch Loss = 1.9972089899962482
2318, epoch_train_loss=1.9972089899962482
Epoch 2319
Epoch 2319 :: Batch 0/1
Batch Loss = 1.9870152083349806
2319, epoch_train_loss=1.9870152083349806
Epoch 2320
Epoch 2320 :: Batch 0/1
Batch Loss = 1.9639766000936596
2320, epoch_train_loss=1.9639766000936596
Epoch 2321
Epoch 2321 :: Batch 0/1
Batch Loss = 1.952867576001298
2321, epoch_train_loss=1.952867576001298
Epoch 2322
Epoch 2322 :: Batch 0/1
Batch Loss = 1.9550705399700943
2322, epoch_train_loss=1.9550705399700943
Epoch 2323
Epoch 2323 :: Batch 0/1
Batch Loss = 1.9647901413127193
2323, epoch_train_loss=1.9647901413127193
Epoch 2324
Epoch 2324 :: Batch 0/1
Batch Loss = 1.9801872624532317
2324, epoch_train_loss=1.9801872624532317
Epoch 2325
Epoch 2325 :: Batch 0/1
Batch Loss = 1.9690260310930086
2325, epoch_train_loss=1.9690260310930086
Epoch 2326
Epoch 2326 :: Batch 0/1
Batch Loss = 1.9544816970988401
2326, epoch_train_loss=1.9544816970988401
Epoch 2327
Epoch 2327 :: Batch 0/1
Batch Loss = 1.9441526201311963
2327, epoch_train_loss=1.9441526201311963
Epoch 2328
Epoch 2328 :: Batch 0/1
Batch Loss = 1.9452205700416383
2328, epoch_train_loss=1.9452205700416383
Epoch 2329
Epoch 2329 :: Batch 0/1
Batch Loss = 1.948338344499075
2329, epoch_train_loss=1.948338344499075
Epoch 2330
Epoch 2330 :: Batch 0/1
Batch Loss = 1.9520790228936566
2330, epoch_train_loss=1.9520790228936566
Epoch 2331
Epoch 2331 :: Batch 0/1
Batch Loss = 1.9590709435208313
2331, epoch_train_loss=1.9590709435208313
Epoch 2332
Epoch 2332 :: Batch 0/1
Batch Loss = 1.9464110668521644
2332, epoch_train_loss=1.9464110668521644
Epoch 2333
Epoch 2333 :: Batch 0/1
Batch Loss = 1.939534742656601
2333, epoch_train_loss=1.939534742656601
Epoch 2334
Epoch 2334 :: Batch 0/1
Batch Loss = 1.9340587976313772
2334, epoch_train_loss=1.9340587976313772
Epoch 2335
Epoch 2335 :: Batch 0/1
Batch Loss = 1.9466796357053966
2335, epoch_train_loss=1.9466796357053966
Epoch 2336
Epoch 2336 :: Batch 0/1
Batch Loss = 1.9467708522167773
2336, epoch_train_loss=1.9467708522167773
Epoch 2337
Epoch 2337 :: Batch 0/1
Batch Loss = 1.9469932756062056
2337, epoch_train_loss=1.9469932756062056
Epoch 2338
Epoch 2338 :: Batch 0/1
Batch Loss = 1.9656953948552673
2338, epoch_train_loss=1.9656953948552673
Epoch 2339
Epoch 2339 :: Batch 0/1
Batch Loss = 1.9283255869213307
2339, epoch_train_loss=1.9283255869213307
Epoch 2340
Epoch 2340 :: Batch 0/1
Batch Loss = 1.9655319839836793
2340, epoch_train_loss=1.9655319839836793
Epoch 2341
Epoch 2341 :: Batch 0/1
Batch Loss = 2.0341251102106574
2341, epoch_train_loss=2.0341251102106574
Epoch 2342
Epoch 2342 :: Batch 0/1
Batch Loss = 2.0627528165922127
2342, epoch_train_loss=2.0627528165922127
Epoch 2343
Epoch 2343 :: Batch 0/1
Batch Loss = 2.326899601642896
2343, epoch_train_loss=2.326899601642896
Epoch 2344
Epoch 2344 :: Batch 0/1
Batch Loss = 2.776789849226003
2344, epoch_train_loss=2.776789849226003
Epoch 2345
Epoch 2345 :: Batch 0/1
Batch Loss = 4.552988391790029
2345, epoch_train_loss=4.552988391790029
Epoch 2346
Epoch 2346 :: Batch 0/1
Batch Loss = 3.595183056580457
2346, epoch_train_loss=3.595183056580457
Epoch 2347
Epoch 2347 :: Batch 0/1
Batch Loss = 3.0656541817522887
2347, epoch_train_loss=3.0656541817522887
Epoch 2348
Epoch 2348 :: Batch 0/1
Batch Loss = 2.62028365796525
2348, epoch_train_loss=2.62028365796525
Epoch 2349
Epoch 2349 :: Batch 0/1
Batch Loss = 2.3481015118163646
2349, epoch_train_loss=2.3481015118163646
Epoch 2350
Epoch 2350 :: Batch 0/1
Batch Loss = 2.468138289235842
2350, epoch_train_loss=2.468138289235842
Epoch 2351
Epoch 2351 :: Batch 0/1
Batch Loss = 2.2555880262677905
2351, epoch_train_loss=2.2555880262677905
Epoch 2352
Epoch 2352 :: Batch 0/1
Batch Loss = 2.1436124640703746
2352, epoch_train_loss=2.1436124640703746
Epoch 2353
Epoch 2353 :: Batch 0/1
Batch Loss = 2.3852632692735667
2353, epoch_train_loss=2.3852632692735667
Epoch 2354
Epoch 2354 :: Batch 0/1
Batch Loss = 2.1819887858248284
2354, epoch_train_loss=2.1819887858248284
Epoch 2355
Epoch 2355 :: Batch 0/1
Batch Loss = 2.659655067882834
2355, epoch_train_loss=2.659655067882834
Epoch 2356
Epoch 2356 :: Batch 0/1
Batch Loss = 2.838175867709452
2356, epoch_train_loss=2.838175867709452
Epoch 2357
Epoch 2357 :: Batch 0/1
Batch Loss = 2.929927808849037
2357, epoch_train_loss=2.929927808849037
Epoch 2358
Epoch 2358 :: Batch 0/1
Batch Loss = 2.8956134781939906
2358, epoch_train_loss=2.8956134781939906
Epoch 2359
Epoch 2359 :: Batch 0/1
Batch Loss = 3.009155809193612
2359, epoch_train_loss=3.009155809193612
Epoch 2360
Epoch 2360 :: Batch 0/1
Batch Loss = 2.8211699756898003
2360, epoch_train_loss=2.8211699756898003
Epoch 2361
Epoch 2361 :: Batch 0/1
Batch Loss = 2.805219308598133
2361, epoch_train_loss=2.805219308598133
Epoch 2362
Epoch 2362 :: Batch 0/1
Batch Loss = 2.63964959894479
2362, epoch_train_loss=2.63964959894479
Epoch 2363
Epoch 2363 :: Batch 0/1
Batch Loss = 2.3696817147798472
2363, epoch_train_loss=2.3696817147798472
Epoch 2364
Epoch 2364 :: Batch 0/1
Batch Loss = 2.3803687172572534
2364, epoch_train_loss=2.3803687172572534
Epoch 2365
Epoch 2365 :: Batch 0/1
Batch Loss = 2.3693319103189086
2365, epoch_train_loss=2.3693319103189086
Epoch 2366
Epoch 2366 :: Batch 0/1
Batch Loss = 2.275484017431695
2366, epoch_train_loss=2.275484017431695
Epoch 2367
Epoch 2367 :: Batch 0/1
Batch Loss = 2.356460523974042
2367, epoch_train_loss=2.356460523974042
Epoch 2368
Epoch 2368 :: Batch 0/1
Batch Loss = 2.4014533390192527
2368, epoch_train_loss=2.4014533390192527
Epoch 2369
Epoch 2369 :: Batch 0/1
Batch Loss = 2.3033169405482985
2369, epoch_train_loss=2.3033169405482985
Epoch 2370
Epoch 2370 :: Batch 0/1
Batch Loss = 2.2927429545611453
2370, epoch_train_loss=2.2927429545611453
Epoch 2371
Epoch 2371 :: Batch 0/1
Batch Loss = 2.279423165131345
2371, epoch_train_loss=2.279423165131345
Epoch 2372
Epoch 2372 :: Batch 0/1
Batch Loss = 2.2800609857558336
2372, epoch_train_loss=2.2800609857558336
Epoch 2373
Epoch 2373 :: Batch 0/1
Batch Loss = 2.300763411690112
2373, epoch_train_loss=2.300763411690112
Epoch 2374
Epoch 2374 :: Batch 0/1
Batch Loss = 2.2604971369398643
2374, epoch_train_loss=2.2604971369398643
Epoch 2375
Epoch 2375 :: Batch 0/1
Batch Loss = 2.274590975589888
2375, epoch_train_loss=2.274590975589888
Epoch 2376
Epoch 2376 :: Batch 0/1
Batch Loss = 2.2145888721597955
2376, epoch_train_loss=2.2145888721597955
Epoch 2377
Epoch 2377 :: Batch 0/1
Batch Loss = 2.206609336861222
2377, epoch_train_loss=2.206609336861222
Epoch 2378
Epoch 2378 :: Batch 0/1
Batch Loss = 2.179940272890741
2378, epoch_train_loss=2.179940272890741
Epoch 2379
Epoch 2379 :: Batch 0/1
Batch Loss = 2.1615264958698286
2379, epoch_train_loss=2.1615264958698286
Epoch 2380
Epoch 2380 :: Batch 0/1
Batch Loss = 2.170724078561001
2380, epoch_train_loss=2.170724078561001
Epoch 2381
Epoch 2381 :: Batch 0/1
Batch Loss = 2.170337968178275
2381, epoch_train_loss=2.170337968178275
Epoch 2382
Epoch 2382 :: Batch 0/1
Batch Loss = 2.1661938183754006
2382, epoch_train_loss=2.1661938183754006
Epoch 2383
Epoch 2383 :: Batch 0/1
Batch Loss = 2.1654672731760534
2383, epoch_train_loss=2.1654672731760534
Epoch 2384
Epoch 2384 :: Batch 0/1
Batch Loss = 2.157779005095053
2384, epoch_train_loss=2.157779005095053
Epoch 2385
Epoch 2385 :: Batch 0/1
Batch Loss = 2.150619067580521
2385, epoch_train_loss=2.150619067580521
Epoch 2386
Epoch 2386 :: Batch 0/1
Batch Loss = 2.151785024605075
2386, epoch_train_loss=2.151785024605075
Epoch 2387
Epoch 2387 :: Batch 0/1
Batch Loss = 2.149852915565092
2387, epoch_train_loss=2.149852915565092
Epoch 2388
Epoch 2388 :: Batch 0/1
Batch Loss = 2.140214816009748
2388, epoch_train_loss=2.140214816009748
Epoch 2389
Epoch 2389 :: Batch 0/1
Batch Loss = 2.1314697839757253
2389, epoch_train_loss=2.1314697839757253
Epoch 2390
Epoch 2390 :: Batch 0/1
Batch Loss = 2.1248429470954022
2390, epoch_train_loss=2.1248429470954022
Epoch 2391
Epoch 2391 :: Batch 0/1
Batch Loss = 2.1187657235626665
2391, epoch_train_loss=2.1187657235626665
Epoch 2392
Epoch 2392 :: Batch 0/1
Batch Loss = 2.1161204327848564
2392, epoch_train_loss=2.1161204327848564
Epoch 2393
Epoch 2393 :: Batch 0/1
Batch Loss = 2.1168745023254045
2393, epoch_train_loss=2.1168745023254045
Epoch 2394
Epoch 2394 :: Batch 0/1
Batch Loss = 2.113617181019764
2394, epoch_train_loss=2.113617181019764
Epoch 2395
Epoch 2395 :: Batch 0/1
Batch Loss = 2.105979432393026
2395, epoch_train_loss=2.105979432393026
Epoch 2396
Epoch 2396 :: Batch 0/1
Batch Loss = 2.09840860261174
2396, epoch_train_loss=2.09840860261174
Epoch 2397
Epoch 2397 :: Batch 0/1
Batch Loss = 2.090263299816097
2397, epoch_train_loss=2.090263299816097
Epoch 2398
Epoch 2398 :: Batch 0/1
Batch Loss = 2.0830748283551483
2398, epoch_train_loss=2.0830748283551483
Epoch 2399
Epoch 2399 :: Batch 0/1
Batch Loss = 2.080655278788903
2399, epoch_train_loss=2.080655278788903
Epoch 2400
Epoch 2400 :: Batch 0/1
Batch Loss = 2.0805430243880414
2400, epoch_train_loss=2.0805430243880414
Epoch 2401
Epoch 2401 :: Batch 0/1
Batch Loss = 2.0769362783152303
2401, epoch_train_loss=2.0769362783152303
Epoch 2402
Epoch 2402 :: Batch 0/1
Batch Loss = 2.0716030659188656
2402, epoch_train_loss=2.0716030659188656
Epoch 2403
Epoch 2403 :: Batch 0/1
Batch Loss = 2.0671387704598105
2403, epoch_train_loss=2.0671387704598105
Epoch 2404
Epoch 2404 :: Batch 0/1
Batch Loss = 2.0633181445978424
2404, epoch_train_loss=2.0633181445978424
Epoch 2405
Epoch 2405 :: Batch 0/1
Batch Loss = 2.0615215293606837
2405, epoch_train_loss=2.0615215293606837
Epoch 2406
Epoch 2406 :: Batch 0/1
Batch Loss = 2.060879335055574
2406, epoch_train_loss=2.060879335055574
Epoch 2407
Epoch 2407 :: Batch 0/1
Batch Loss = 2.0577069102669068
2407, epoch_train_loss=2.0577069102669068
Epoch 2408
Epoch 2408 :: Batch 0/1
Batch Loss = 2.0530808425511635
2408, epoch_train_loss=2.0530808425511635
Epoch 2409
Epoch 2409 :: Batch 0/1
Batch Loss = 2.0483843164739617
2409, epoch_train_loss=2.0483843164739617
Epoch 2410
Epoch 2410 :: Batch 0/1
Batch Loss = 2.043072098280318
2410, epoch_train_loss=2.043072098280318
Epoch 2411
Epoch 2411 :: Batch 0/1
Batch Loss = 2.038751837546713
2411, epoch_train_loss=2.038751837546713
Epoch 2412
Epoch 2412 :: Batch 0/1
Batch Loss = 2.0359450923504987
2412, epoch_train_loss=2.0359450923504987
Epoch 2413
Epoch 2413 :: Batch 0/1
Batch Loss = 2.0321056348817805
2413, epoch_train_loss=2.0321056348817805
Epoch 2414
Epoch 2414 :: Batch 0/1
Batch Loss = 2.0272879984113072
2414, epoch_train_loss=2.0272879984113072
Epoch 2415
Epoch 2415 :: Batch 0/1
Batch Loss = 2.0228451520546766
2415, epoch_train_loss=2.0228451520546766
Epoch 2416
Epoch 2416 :: Batch 0/1
Batch Loss = 2.0185867608464543
2416, epoch_train_loss=2.0185867608464543
Epoch 2417
Epoch 2417 :: Batch 0/1
Batch Loss = 2.01517719599861
2417, epoch_train_loss=2.01517719599861
Epoch 2418
Epoch 2418 :: Batch 0/1
Batch Loss = 2.0120301311869224
2418, epoch_train_loss=2.0120301311869224
Epoch 2419
Epoch 2419 :: Batch 0/1
Batch Loss = 2.007808191444354
2419, epoch_train_loss=2.007808191444354
Epoch 2420
Epoch 2420 :: Batch 0/1
Batch Loss = 2.0035204213171927
2420, epoch_train_loss=2.0035204213171927
Epoch 2421
Epoch 2421 :: Batch 0/1
Batch Loss = 1.9989911472613267
2421, epoch_train_loss=1.9989911472613267
Epoch 2422
Epoch 2422 :: Batch 0/1
Batch Loss = 1.9941722651809946
2422, epoch_train_loss=1.9941722651809946
Epoch 2423
Epoch 2423 :: Batch 0/1
Batch Loss = 1.9897817284879948
2423, epoch_train_loss=1.9897817284879948
Epoch 2424
Epoch 2424 :: Batch 0/1
Batch Loss = 1.9852339451281507
2424, epoch_train_loss=1.9852339451281507
Epoch 2425
Epoch 2425 :: Batch 0/1
Batch Loss = 1.9808349861218686
2425, epoch_train_loss=1.9808349861218686
Epoch 2426
Epoch 2426 :: Batch 0/1
Batch Loss = 1.9764664166849208
2426, epoch_train_loss=1.9764664166849208
Epoch 2427
Epoch 2427 :: Batch 0/1
Batch Loss = 1.9715754938965602
2427, epoch_train_loss=1.9715754938965602
Epoch 2428
Epoch 2428 :: Batch 0/1
Batch Loss = 1.9668566974904123
2428, epoch_train_loss=1.9668566974904123
Epoch 2429
Epoch 2429 :: Batch 0/1
Batch Loss = 1.9621419277011578
2429, epoch_train_loss=1.9621419277011578
Epoch 2430
Epoch 2430 :: Batch 0/1
Batch Loss = 1.9576849590909757
2430, epoch_train_loss=1.9576849590909757
Epoch 2431
Epoch 2431 :: Batch 0/1
Batch Loss = 1.953391146506828
2431, epoch_train_loss=1.953391146506828
Epoch 2432
Epoch 2432 :: Batch 0/1
Batch Loss = 1.9489260733478377
2432, epoch_train_loss=1.9489260733478377
Epoch 2433
Epoch 2433 :: Batch 0/1
Batch Loss = 1.9446569015784554
2433, epoch_train_loss=1.9446569015784554
Epoch 2434
Epoch 2434 :: Batch 0/1
Batch Loss = 1.940350669514863
2434, epoch_train_loss=1.940350669514863
Epoch 2435
Epoch 2435 :: Batch 0/1
Batch Loss = 1.9362894859733428
2435, epoch_train_loss=1.9362894859733428
Epoch 2436
Epoch 2436 :: Batch 0/1
Batch Loss = 1.9323297005237756
2436, epoch_train_loss=1.9323297005237756
Epoch 2437
Epoch 2437 :: Batch 0/1
Batch Loss = 1.928566142844118
2437, epoch_train_loss=1.928566142844118
Epoch 2438
Epoch 2438 :: Batch 0/1
Batch Loss = 1.9249906463971116
2438, epoch_train_loss=1.9249906463971116
Epoch 2439
Epoch 2439 :: Batch 0/1
Batch Loss = 1.9214488489014647
2439, epoch_train_loss=1.9214488489014647
Epoch 2440
Epoch 2440 :: Batch 0/1
Batch Loss = 1.9180844834921296
2440, epoch_train_loss=1.9180844834921296
Epoch 2441
Epoch 2441 :: Batch 0/1
Batch Loss = 1.914828403948324
2441, epoch_train_loss=1.914828403948324
Epoch 2442
Epoch 2442 :: Batch 0/1
Batch Loss = 1.9118411971283578
2442, epoch_train_loss=1.9118411971283578
Epoch 2443
Epoch 2443 :: Batch 0/1
Batch Loss = 1.9088916288539846
2443, epoch_train_loss=1.9088916288539846
Epoch 2444
Epoch 2444 :: Batch 0/1
Batch Loss = 1.9060617721193678
2444, epoch_train_loss=1.9060617721193678
Epoch 2445
Epoch 2445 :: Batch 0/1
Batch Loss = 1.9032365423660382
2445, epoch_train_loss=1.9032365423660382
Epoch 2446
Epoch 2446 :: Batch 0/1
Batch Loss = 1.9004725466563892
2446, epoch_train_loss=1.9004725466563892
Epoch 2447
Epoch 2447 :: Batch 0/1
Batch Loss = 1.897707440043188
2447, epoch_train_loss=1.897707440043188
Epoch 2448
Epoch 2448 :: Batch 0/1
Batch Loss = 1.895045480992742
2448, epoch_train_loss=1.895045480992742
Epoch 2449
Epoch 2449 :: Batch 0/1
Batch Loss = 1.8924358609417555
2449, epoch_train_loss=1.8924358609417555
Epoch 2450
Epoch 2450 :: Batch 0/1
Batch Loss = 1.889869768092693
2450, epoch_train_loss=1.889869768092693
Epoch 2451
Epoch 2451 :: Batch 0/1
Batch Loss = 1.8873185270806057
2451, epoch_train_loss=1.8873185270806057
Epoch 2452
Epoch 2452 :: Batch 0/1
Batch Loss = 1.8848233801596577
2452, epoch_train_loss=1.8848233801596577
Epoch 2453
Epoch 2453 :: Batch 0/1
Batch Loss = 1.8823481835070055
2453, epoch_train_loss=1.8823481835070055
Epoch 2454
Epoch 2454 :: Batch 0/1
Batch Loss = 1.8799538658806891
2454, epoch_train_loss=1.8799538658806891
Epoch 2455
Epoch 2455 :: Batch 0/1
Batch Loss = 1.8776233525564958
2455, epoch_train_loss=1.8776233525564958
Epoch 2456
Epoch 2456 :: Batch 0/1
Batch Loss = 1.8753638066071667
2456, epoch_train_loss=1.8753638066071667
Epoch 2457
Epoch 2457 :: Batch 0/1
Batch Loss = 1.8731379852707797
2457, epoch_train_loss=1.8731379852707797
Epoch 2458
Epoch 2458 :: Batch 0/1
Batch Loss = 1.8709892228842122
2458, epoch_train_loss=1.8709892228842122
Epoch 2459
Epoch 2459 :: Batch 0/1
Batch Loss = 1.8688772338302417
2459, epoch_train_loss=1.8688772338302417
Epoch 2460
Epoch 2460 :: Batch 0/1
Batch Loss = 1.8668398936360846
2460, epoch_train_loss=1.8668398936360846
Epoch 2461
Epoch 2461 :: Batch 0/1
Batch Loss = 1.864863804809375
2461, epoch_train_loss=1.864863804809375
Epoch 2462
Epoch 2462 :: Batch 0/1
Batch Loss = 1.8629451913420962
2462, epoch_train_loss=1.8629451913420962
Epoch 2463
Epoch 2463 :: Batch 0/1
Batch Loss = 1.8610692352652307
2463, epoch_train_loss=1.8610692352652307
Epoch 2464
Epoch 2464 :: Batch 0/1
Batch Loss = 1.8592344899462587
2464, epoch_train_loss=1.8592344899462587
Epoch 2465
Epoch 2465 :: Batch 0/1
Batch Loss = 1.8574377187835174
2465, epoch_train_loss=1.8574377187835174
Epoch 2466
Epoch 2466 :: Batch 0/1
Batch Loss = 1.8556699004784571
2466, epoch_train_loss=1.8556699004784571
Epoch 2467
Epoch 2467 :: Batch 0/1
Batch Loss = 1.853945398111628
2467, epoch_train_loss=1.853945398111628
Epoch 2468
Epoch 2468 :: Batch 0/1
Batch Loss = 1.852239189268741
2468, epoch_train_loss=1.852239189268741
Epoch 2469
Epoch 2469 :: Batch 0/1
Batch Loss = 1.850557403288082
2469, epoch_train_loss=1.850557403288082
Epoch 2470
Epoch 2470 :: Batch 0/1
Batch Loss = 1.8488963197529331
2470, epoch_train_loss=1.8488963197529331
Epoch 2471
Epoch 2471 :: Batch 0/1
Batch Loss = 1.847244823781543
2471, epoch_train_loss=1.847244823781543
Epoch 2472
Epoch 2472 :: Batch 0/1
Batch Loss = 1.845616397919031
2472, epoch_train_loss=1.845616397919031
Epoch 2473
Epoch 2473 :: Batch 0/1
Batch Loss = 1.844001570247251
2473, epoch_train_loss=1.844001570247251
Epoch 2474
Epoch 2474 :: Batch 0/1
Batch Loss = 1.84239989617397
2474, epoch_train_loss=1.84239989617397
Epoch 2475
Epoch 2475 :: Batch 0/1
Batch Loss = 1.8408153902123536
2475, epoch_train_loss=1.8408153902123536
Epoch 2476
Epoch 2476 :: Batch 0/1
Batch Loss = 1.839238503952728
2476, epoch_train_loss=1.839238503952728
Epoch 2477
Epoch 2477 :: Batch 0/1
Batch Loss = 1.8376748832194971
2477, epoch_train_loss=1.8376748832194971
Epoch 2478
Epoch 2478 :: Batch 0/1
Batch Loss = 1.8361266372796814
2478, epoch_train_loss=1.8361266372796814
Epoch 2479
Epoch 2479 :: Batch 0/1
Batch Loss = 1.834588990143143
2479, epoch_train_loss=1.834588990143143
Epoch 2480
Epoch 2480 :: Batch 0/1
Batch Loss = 1.833067174054438
2480, epoch_train_loss=1.833067174054438
Epoch 2481
Epoch 2481 :: Batch 0/1
Batch Loss = 1.8315582509961248
2481, epoch_train_loss=1.8315582509961248
Epoch 2482
Epoch 2482 :: Batch 0/1
Batch Loss = 1.8300579259550906
2482, epoch_train_loss=1.8300579259550906
Epoch 2483
Epoch 2483 :: Batch 0/1
Batch Loss = 1.8285693465691182
2483, epoch_train_loss=1.8285693465691182
Epoch 2484
Epoch 2484 :: Batch 0/1
Batch Loss = 1.8270907428955385
2484, epoch_train_loss=1.8270907428955385
Epoch 2485
Epoch 2485 :: Batch 0/1
Batch Loss = 1.8256212016015227
2485, epoch_train_loss=1.8256212016015227
Epoch 2486
Epoch 2486 :: Batch 0/1
Batch Loss = 1.8241609983286535
2486, epoch_train_loss=1.8241609983286535
Epoch 2487
Epoch 2487 :: Batch 0/1
Batch Loss = 1.8227091571257321
2487, epoch_train_loss=1.8227091571257321
Epoch 2488
Epoch 2488 :: Batch 0/1
Batch Loss = 1.821264640309372
2488, epoch_train_loss=1.821264640309372
Epoch 2489
Epoch 2489 :: Batch 0/1
Batch Loss = 1.8198252601405414
2489, epoch_train_loss=1.8198252601405414
Epoch 2490
Epoch 2490 :: Batch 0/1
Batch Loss = 1.8183922950258091
2490, epoch_train_loss=1.8183922950258091
Epoch 2491
Epoch 2491 :: Batch 0/1
Batch Loss = 1.8169657279682134
2491, epoch_train_loss=1.8169657279682134
Epoch 2492
Epoch 2492 :: Batch 0/1
Batch Loss = 1.8155437754633177
2492, epoch_train_loss=1.8155437754633177
Epoch 2493
Epoch 2493 :: Batch 0/1
Batch Loss = 1.8141262882942772
2493, epoch_train_loss=1.8141262882942772
Epoch 2494
Epoch 2494 :: Batch 0/1
Batch Loss = 1.812712528294153
2494, epoch_train_loss=1.812712528294153
Epoch 2495
Epoch 2495 :: Batch 0/1
Batch Loss = 1.811303132091139
2495, epoch_train_loss=1.811303132091139
Epoch 2496
Epoch 2496 :: Batch 0/1
Batch Loss = 1.809897705603057
2496, epoch_train_loss=1.809897705603057
Epoch 2497
Epoch 2497 :: Batch 0/1
Batch Loss = 1.808495554793435
2497, epoch_train_loss=1.808495554793435
Epoch 2498
Epoch 2498 :: Batch 0/1
Batch Loss = 1.8070966206322616
2498, epoch_train_loss=1.8070966206322616
Epoch 2499
Epoch 2499 :: Batch 0/1
Batch Loss = 1.8057002003836007
2499, epoch_train_loss=1.8057002003836007
