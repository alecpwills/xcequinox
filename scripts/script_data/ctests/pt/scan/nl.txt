no change     /gpfs/projects/FernandezGroup/Alec/miniconda3/condabin/conda
no change     /gpfs/projects/FernandezGroup/Alec/miniconda3/bin/conda
no change     /gpfs/projects/FernandezGroup/Alec/miniconda3/bin/conda-env
no change     /gpfs/projects/FernandezGroup/Alec/miniconda3/bin/activate
no change     /gpfs/projects/FernandezGroup/Alec/miniconda3/bin/deactivate
no change     /gpfs/projects/FernandezGroup/Alec/miniconda3/etc/profile.d/conda.sh
no change     /gpfs/projects/FernandezGroup/Alec/miniconda3/etc/fish/conf.d/conda.fish
no change     /gpfs/projects/FernandezGroup/Alec/miniconda3/shell/condabin/Conda.psm1
no change     /gpfs/projects/FernandezGroup/Alec/miniconda3/shell/condabin/conda-hook.ps1
no change     /gpfs/projects/FernandezGroup/Alec/miniconda3/lib/python3.9/site-packages/xontrib/conda.xsh
no change     /gpfs/projects/FernandezGroup/Alec/miniconda3/etc/profile.d/conda.csh
no change     /gpfs/home/jofranklin/.bashrc
No action taken.
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting "B3LYP_WITH_VWN5 = True" in pyscf_conf.py
  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4136890> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4136890> in UKS object of <class 'pyscf.dft.uks.UKS'>
WARNING: External module "mldftdat" required for non-local descriptor use.
<pyscf.gto.mole.Mole object at 0x7ffec4136890> [['P', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec4137250> [['N', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec4134130> [['H', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec41354b0> [['Li', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec4135120> [['O', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec4137220> [['Cl', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec4136290> [['Al', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec4137550> [['S', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec41354e0> [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec4137b80> [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec4137be0> [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec4300250> [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]] 3
<pyscf.gto.mole.Mole object at 0x7ffec43020b0> [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]] 3
<pyscf.gto.mole.Mole object at 0x7ffec43024a0> [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec4301450> [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec4303bb0> [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec43006a0> [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]] 4
<pyscf.gto.mole.Mole object at 0x7ffec4303d30> [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec4303220> [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec4300fd0> [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec4302620> [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec43038e0> [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]] 4
<pyscf.gto.mole.Mole object at 0x7ffec4303010> [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]] 4
<pyscf.gto.mole.Mole object at 0x7ffec4301570> [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec4302590> [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]] 5
<pyscf.gto.mole.Mole object at 0x7ffec4257f40> [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]] 3
<pyscf.gto.mole.Mole object at 0x7ffec4255510> [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]] 5
mol:  [['P', array([0., 0., 0.])]]
converged SCF energy = -341.104145992717  <S^2> = 3.7502984  2S+1 = 4.0001492
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4137250> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4137250> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00051022 -0.00019156 -0.00051334 ... -0.02830887 -0.02830887
 -0.02830887] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 16)
mol:  [['N', array([0., 0., 0.])]]
converged SCF energy = -54.5289742046674  <S^2> = 3.7524945  2S+1 = 4.0012471
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4134130> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4134130> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5016, 22)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-3.60081838e-04 -1.08775305e-04 -1.31917160e-05 ... -2.74817476e-02
 -2.74817476e-02 -2.74817476e-02] = ,SCAN
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5016, 22)
rho shape (4, 4, 2, 5016)
rho_filt shape: (5016,)
get descriptors tdrho.shape=(5016, 16)
mol:  [['H', array([0., 0., 0.])]]
converged SCF energy = -0.49981298400854  <S^2> = 0.75  2S+1 = 2
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec41354b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec41354b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 7, 7)
ao.shape (10, 2440, 7)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.92637305e-09 -1.31700808e-07 -9.61527370e-06 ... -7.35522754e-16
 -7.35522754e-16 -7.35522754e-16] = ,SCAN
get_data, dm shape = (2, 7, 7)
ao_eval.shape=(4, 2440, 7)
rho shape (4, 4, 2, 2440)
rho_filt shape: (2440,)
get descriptors tdrho.shape=(2440, 16)
mol:  [['Li', array([0., 0., 0.])]]
converged SCF energy = -7.46006188627842  <S^2> = 0.75000049  2S+1 = 2.0000005
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4135120> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4135120> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 4592, 22)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-5.31884219e-04 -2.81911891e-04 -2.81911891e-04 ... -1.27154711e-05
 -2.64861768e-02 -2.64861768e-02] = ,SCAN
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 4592, 22)
rho shape (4, 4, 2, 4592)
rho_filt shape: (4592,)
get descriptors tdrho.shape=(4592, 16)
mol:  [['O', array([0., 0., 0.])]]
converged SCF energy = -75.0033802924676  <S^2> = 2.0027445  2S+1 = 3.0018291
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4137220> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4137220> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5040, 22)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.16018019e-04 -4.01576154e-05 -2.14295568e-06 ... -2.76158581e-02
 -2.76158581e-02 -2.76158581e-02] = ,SCAN
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5040, 22)
rho shape (4, 4, 2, 5040)
rho_filt shape: (5040,)
get descriptors tdrho.shape=(5040, 16)
mol:  [['Cl', array([0., 0., 0.])]]
converged SCF energy = -459.957577121377  <S^2> = 0.7516194  2S+1 = 2.0016187
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4136290> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4136290> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6152, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.99670324e-04 -2.40562882e-04 -8.22177430e-05 ... -2.84484386e-02
 -2.84484386e-02 -2.84484386e-02] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6152, 30)
rho shape (4, 4, 2, 6152)
rho_filt shape: (6152,)
get descriptors tdrho.shape=(6152, 16)
mol:  [['Al', array([0., 0., 0.])]]
converged SCF energy = -242.226560989243  <S^2> = 0.75226414  2S+1 = 2.0022629
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4137550> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4137550> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6088, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.12369990e-03 -1.34687832e-03 -6.90063325e-04 ... -2.71546352e-05
 -1.90328177e-04 -1.52141565e-05] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6088, 30)
rho shape (4, 4, 2, 6088)
rho_filt shape: (6088,)
get descriptors tdrho.shape=(6088, 16)
mol:  [['S', array([0., 0., 0.])]]
converged SCF energy = -397.938786806852  <S^2> = 2.0022329  2S+1 = 3.0014882
Warning: <pyscf.gto.mole.Mole object at 0x7ffec41354e0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec41354e0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00038776 -0.00017482 -0.00023385 ... -0.02838402 -0.02838402
 -0.02838402] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 16)
mol:  [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]]
converged SCF energy = -1.16580491182912  <S^2> = -4.4408921e-16  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4137b80> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4137b80> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 14, 14)
ao.shape (10, 4776, 14)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.43725660e-05 -1.02204687e-06 -4.05575842e-05 ... -2.36278434e-02
 -2.36278434e-02 -2.36278434e-02] = ,SCAN
get_data, dm shape = (2, 14, 14)
ao_eval.shape=(4, 4776, 14)
rho shape (4, 4, 2, 4776)
rho_filt shape: (4776,)
get descriptors tdrho.shape=(4776, 16)
mol:  [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]]
converged SCF energy = -109.439263799566  <S^2> = 1.7763568e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4137be0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4137be0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9848, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.89629699e-05 -2.76172354e-04 -7.59017288e-05 ... -7.34654212e-06
 -7.34654212e-06 -2.89629699e-05] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9848, 44)
rho shape (4, 4, 2, 9848)
rho_filt shape: (9848,)
get descriptors tdrho.shape=(9848, 16)
mol:  [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]]
converged SCF energy = -107.339357395734  <S^2> = 2.6645353e-15  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4300250> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4300250> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9752, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00043469 -0.00024024 -0.00035532 ... -0.00047537 -0.03728133
 -0.03728133] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9752, 44)
rho shape (4, 4, 2, 9752)
rho_filt shape: (9752,)
get descriptors tdrho.shape=(9752, 16)
mol:  [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]]
converged SCF energy = -93.3377924465131  <S^2> = 4.0073633e-10  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec43020b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec43020b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 12256, 51)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-9.02468888e-05 -7.92694658e-06 -9.80568469e-06 ... -4.33714150e-02
 -4.33714150e-02 -4.33714150e-02] = ,SCAN
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 12256, 51)
rho shape (4, 4, 2, 12256)
rho_filt shape: (12256,)
get descriptors tdrho.shape=(12256, 16)
mol:  [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]]
converged SCF energy = -188.456965322844  <S^2> = 1.4210855e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec43024a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec43024a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 66, 66)
ao.shape (10, 14920, 66)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-7.48187338e-05 -6.19475249e-05 -2.61742784e-04 ... -8.70042314e-07
 -2.73391097e-02 -2.73391097e-02] = ,SCAN
get_data, dm shape = (2, 66, 66)
ao_eval.shape=(4, 14920, 66)
rho shape (4, 4, 2, 14920)
rho_filt shape: (14920,)
get descriptors tdrho.shape=(14920, 16)
mol:  [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]]
converged SCF energy = -920.005608888958  <S^2> = 5.0448534e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4301450> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4301450> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12208, 60)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00051559 -0.00027432 -0.00088583 ... -0.00027432 -0.04174728
 -0.04174728] = ,SCAN
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12208, 60)
rho shape (4, 4, 2, 12208)
rho_filt shape: (12208,)
get descriptors tdrho.shape=(12208, 16)
mol:  [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]]
converged SCF energy = -199.394370591172  <S^2> = 1.2434498e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4303bb0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4303bb0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9824, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-5.53951178e-05 -5.93507199e-06 -3.10072916e-04 ... -5.94325581e-02
 -5.94325581e-02 -5.94325581e-02] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9824, 44)
rho shape (4, 4, 2, 9824)
rho_filt shape: (9824,)
get descriptors tdrho.shape=(9824, 16)
mol:  [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]]
converged SCF energy = -150.214894498221  <S^2> = 1.0018599  2S+1 = 2.2377309
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec43006a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec43006a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9912, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.58909181e-04 -2.98552412e-05 -1.62454545e-06 ... -4.22396734e-02
 -4.22396734e-02 -4.22396734e-02] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9912, 44)
rho shape (4, 4, 2, 9912)
rho_filt shape: (9912,)
get descriptors tdrho.shape=(9912, 16)
mol:  [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]]
converged SCF energy = -77.2435048346374  <S^2> = 1.4210855e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4303d30> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4303d30> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 58, 58)
ao.shape (10, 15208, 58)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.72190712e-05 -2.72190712e-05 -2.84904833e-04 ... -1.08108260e-05
 -1.03072478e-05 -1.03072478e-05] = ,SCAN
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15208, 58)
rho shape (4, 4, 2, 15208)
rho_filt shape: (15208,)
get descriptors tdrho.shape=(15208, 16)
mol:  [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]]
converged SCF energy = -113.221335689652  <S^2> = 6.6080474e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4303220> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4303220> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 10040, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00015688 -0.00024669 -0.00068269 ... -0.03791166 -0.03791166
 -0.03791166] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 10040, 44)
rho shape (4, 4, 2, 10040)
rho_filt shape: (10040,)
get descriptors tdrho.shape=(10040, 16)
mol:  [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]]
converged SCF energy = -460.624592374077  <S^2> = 6.750156e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4300fd0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4300fd0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 37, 37)
ao.shape (10, 8552, 37)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-7.28500681e-05 -5.65091132e-06 -7.37932132e-06 ... -4.76689214e-02
 -4.76689214e-02 -4.76689214e-02] = ,SCAN
get_data, dm shape = (2, 37, 37)
ao_eval.shape=(4, 8552, 37)
rho shape (4, 4, 2, 8552)
rho_filt shape: (8552,)
get descriptors tdrho.shape=(8552, 16)
mol:  [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]]
converged SCF energy = -8.04458854018922  <S^2> = 7.6605389e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4302620> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4302620> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 29, 29)
ao.shape (10, 6936, 29)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.0003863  -0.00040095 -0.00040095 ... -0.0213199  -0.0213199
 -0.0213199 ] = ,SCAN
get_data, dm shape = (2, 29, 29)
ao_eval.shape=(4, 6936, 29)
rho shape (4, 4, 2, 6936)
rho_filt shape: (6936,)
get descriptors tdrho.shape=(6936, 16)
mol:  [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]]
converged SCF energy = -324.340512506578  <S^2> = 1.586109e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec43038e0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec43038e0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 11536, 60)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00088473 -0.00088473 -0.00116894 ... -0.00088473 -0.00088473
 -0.00116894] = ,SCAN
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 11536, 60)
rho shape (4, 4, 2, 11536)
rho_filt shape: (11536,)
get descriptors tdrho.shape=(11536, 16)
mol:  [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]]
converged SCF energy = -1622.57507845814  <S^2> = 8.2422957e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4303010> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4303010> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 120, 120)
ao.shape (10, 24512, 120)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.91408540e-05 -1.46971271e-04 -1.08734417e-03 ... -2.81566369e-02
 -2.81566369e-02 -2.81566369e-02] = ,SCAN
get_data, dm shape = (2, 120, 120)
ao_eval.shape=(4, 24512, 120)
rho shape (4, 4, 2, 24512)
rho_filt shape: (24512,)
get descriptors tdrho.shape=(24512, 16)
mol:  [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]]
converged SCF energy = -342.979728469575  <S^2> = 2.5389468e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4301570> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4301570> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 13096, 51)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.39373335e-04 -1.31641332e-04 -1.15950750e-05 ... -7.32416564e-02
 -7.32416564e-02 -7.32416564e-02] = ,SCAN
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 13096, 51)
rho shape (4, 4, 2, 13096)
rho_filt shape: (13096,)
get descriptors tdrho.shape=(13096, 16)
mol:  [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]]
converged SCF energy = -578.565336131232  <S^2> = 1.0034705  2S+1 = 2.2391699
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4302590> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4302590> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12384, 60)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.84680960e-05 -7.80560097e-05 -7.80525182e-05 ... -2.92531356e-02
 -2.92531356e-02 -2.92531356e-02] = ,SCAN
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12384, 60)
rho shape (4, 4, 2, 12384)
rho_filt shape: (12384,)
get descriptors tdrho.shape=(12384, 16)
mol:  [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]]
converged SCF energy = -40.4598214864077  <S^2> = 3.2418512e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4257f40> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4257f40> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 50, 50)
ao.shape (10, 13936, 50)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.56165538e-04 -7.34744214e-05 -5.30574304e-06 ... -7.93995702e-06
 -7.93995702e-06 -7.93995702e-06] = ,SCAN
get_data, dm shape = (2, 50, 50)
ao_eval.shape=(4, 13936, 50)
rho shape (4, 4, 2, 13936)
rho_filt shape: (13936,)
get descriptors tdrho.shape=(13936, 16)
mol:  [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]]
converged SCF energy = -39.0756147483504  <S^2> = 6.1937122e-12  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4255510> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4255510> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 36, 36)
ao.shape (10, 9656, 36)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.28290753e-04 -1.41305449e-05 -6.13700492e-05 ... -2.47993463e-02
 -2.47993463e-02 -2.47993463e-02] = ,SCAN
get_data, dm shape = (2, 36, 36)
ao_eval.shape=(4, 9656, 36)
rho shape (4, 4, 2, 9656)
rho_filt shape: (9656,)
get descriptors tdrho.shape=(9656, 16)
mol:  [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]]
converged SCF energy = -291.719272437819  <S^2> = 1.3157475e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
New DM shape: (2, 58, 58)
ao.shape (10, 15256, 58)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.45512011e-04 -7.12775692e-05 -5.48666345e-06 ... -6.02613084e-06
 -6.02613084e-06 -6.02613084e-06] = ,SCAN
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15256, 58)
rho shape (4, 4, 2, 15256)
rho_filt shape: (15256,)
get descriptors tdrho.shape=(15256, 16)
PRE NAN FILT: tFxc.shape=(237019,), tdrho.shape=(237019, 16)
nan_filt_rho.shape=(237019,)
nan_filt_fxc.shape=(237019,)
tFxc.shape=(237019,), tdrho.shape=(237019, 16)
inp[0].shape = (237019, 16)
Epoch 0
Epoch 0 :: Batch 0/1
Batch Loss = 948332.5415955218
0, epoch_train_loss=948332.5415955218
Epoch 1
Epoch 1 :: Batch 0/1
Batch Loss = 0.2395479914264525
1, epoch_train_loss=0.2395479914264525
Epoch 2
Epoch 2 :: Batch 0/1
Batch Loss = 0.0713065740540384
2, epoch_train_loss=0.0713065740540384
Epoch 3
Epoch 3 :: Batch 0/1
Batch Loss = 0.021490812461111324
3, epoch_train_loss=0.021490812461111324
Epoch 4
Epoch 4 :: Batch 0/1
Batch Loss = 0.006642192076408138
4, epoch_train_loss=0.006642192076408138
Epoch 5
Epoch 5 :: Batch 0/1
Batch Loss = 0.0019726160289277746
5, epoch_train_loss=0.0019726160289277746
Epoch 6
Epoch 6 :: Batch 0/1
Batch Loss = 0.0009519868141982416
6, epoch_train_loss=0.0009519868141982416
Epoch 7
Epoch 7 :: Batch 0/1
Batch Loss = 0.0007709114693973935
7, epoch_train_loss=0.0007709114693973935
Epoch 8
Epoch 8 :: Batch 0/1
Batch Loss = 0.0007384060753113702
8, epoch_train_loss=0.0007384060753113702
Epoch 9
Epoch 9 :: Batch 0/1
Batch Loss = 0.0007323737152335648
9, epoch_train_loss=0.0007323737152335648
Epoch 10
Epoch 10 :: Batch 0/1
Batch Loss = 0.0007312402155327645
10, epoch_train_loss=0.0007312402155327645
Epoch 11
Epoch 11 :: Batch 0/1
Batch Loss = 0.000731026101355363
11, epoch_train_loss=0.000731026101355363
Epoch 12
Epoch 12 :: Batch 0/1
Batch Loss = 0.0007309852973867499
12, epoch_train_loss=0.0007309852973867499
Epoch 13
Epoch 13 :: Batch 0/1
Batch Loss = 0.0007309773788451988
13, epoch_train_loss=0.0007309773788451988
Epoch 14
Epoch 14 :: Batch 0/1
Batch Loss = 0.0007309757975514138
14, epoch_train_loss=0.0007309757975514138
Epoch 15
Epoch 15 :: Batch 0/1
Batch Loss = 0.0007309754695928844
15, epoch_train_loss=0.0007309754695928844
Epoch 16
Epoch 16 :: Batch 0/1
Batch Loss = 0.0007309753984120301
16, epoch_train_loss=0.0007309753984120301
Epoch 17
Epoch 17 :: Batch 0/1
Batch Loss = 0.0007309753821451818
17, epoch_train_loss=0.0007309753821451818
Epoch 18
Epoch 18 :: Batch 0/1
Batch Loss = 0.0007309753782119984
18, epoch_train_loss=0.0007309753782119984
Epoch 19
Epoch 19 :: Batch 0/1
Batch Loss = 0.0007309753772020818
19, epoch_train_loss=0.0007309753772020818
Epoch 20
Epoch 20 :: Batch 0/1
Batch Loss = 0.0007309753769259925
20, epoch_train_loss=0.0007309753769259925
Epoch 21
Epoch 21 :: Batch 0/1
Batch Loss = 0.000730975376845509
21, epoch_train_loss=0.000730975376845509
Epoch 22
Epoch 22 :: Batch 0/1
Batch Loss = 0.0007309753768204762
22, epoch_train_loss=0.0007309753768204762
Epoch 23
Epoch 23 :: Batch 0/1
Batch Loss = 0.0007309753768121708
23, epoch_train_loss=0.0007309753768121708
Epoch 24
Epoch 24 :: Batch 0/1
Batch Loss = 0.0007309753768092343
24, epoch_train_loss=0.0007309753768092343
Epoch 25
Epoch 25 :: Batch 0/1
Batch Loss = 0.0007309753768081296
25, epoch_train_loss=0.0007309753768081296
Epoch 26
Epoch 26 :: Batch 0/1
Batch Loss = 0.0007309753768076885
26, epoch_train_loss=0.0007309753768076885
Epoch 27
Epoch 27 :: Batch 0/1
Batch Loss = 0.0007309753768075017
27, epoch_train_loss=0.0007309753768075017
Epoch 28
Epoch 28 :: Batch 0/1
Batch Loss = 0.0007309753768074182
28, epoch_train_loss=0.0007309753768074182
Epoch 29
Epoch 29 :: Batch 0/1
Batch Loss = 0.0007309753768073788
29, epoch_train_loss=0.0007309753768073788
Epoch 30
Epoch 30 :: Batch 0/1
Batch Loss = 0.0007309753768073594
30, epoch_train_loss=0.0007309753768073594
Epoch 31
Epoch 31 :: Batch 0/1
Batch Loss = 0.0007309753768073492
31, epoch_train_loss=0.0007309753768073492
Epoch 32
Epoch 32 :: Batch 0/1
Batch Loss = 0.0007309753768073438
32, epoch_train_loss=0.0007309753768073438
Epoch 33
Epoch 33 :: Batch 0/1
Batch Loss = 0.0007309753768073407
33, epoch_train_loss=0.0007309753768073407
Epoch 34
Epoch 34 :: Batch 0/1
Batch Loss = 0.0007309753768073388
34, epoch_train_loss=0.0007309753768073388
Epoch 35
Epoch 35 :: Batch 0/1
Batch Loss = 0.0007309753768073379
35, epoch_train_loss=0.0007309753768073379
Epoch 36
Epoch 36 :: Batch 0/1
Batch Loss = 0.0007309753768073371
36, epoch_train_loss=0.0007309753768073371
Epoch 37
Epoch 37 :: Batch 0/1
Batch Loss = 0.0007309753768073367
37, epoch_train_loss=0.0007309753768073367
Epoch 38
Epoch 38 :: Batch 0/1
Batch Loss = 0.0007309753768073363
38, epoch_train_loss=0.0007309753768073363
Epoch 39
Epoch 39 :: Batch 0/1
Batch Loss = 0.0007309753768073361
39, epoch_train_loss=0.0007309753768073361
Epoch 40
Epoch 40 :: Batch 0/1
Batch Loss = 0.0007309753768073359
40, epoch_train_loss=0.0007309753768073359
Epoch 41
Epoch 41 :: Batch 0/1
Batch Loss = 0.0007309753768073359
41, epoch_train_loss=0.0007309753768073359
Epoch 42
Epoch 42 :: Batch 0/1
Batch Loss = 0.0007309753768073359
42, epoch_train_loss=0.0007309753768073359
Epoch 43
Epoch 43 :: Batch 0/1
Batch Loss = 0.0007309753768073358
43, epoch_train_loss=0.0007309753768073358
Epoch 44
Epoch 44 :: Batch 0/1
Batch Loss = 0.0007309753768073357
44, epoch_train_loss=0.0007309753768073357
Epoch 45
Epoch 45 :: Batch 0/1
Batch Loss = 0.0007309753768073357
45, epoch_train_loss=0.0007309753768073357
Epoch 46
Epoch 46 :: Batch 0/1
Batch Loss = 0.0007309753768073357
46, epoch_train_loss=0.0007309753768073357
Epoch 47
Epoch 47 :: Batch 0/1
Batch Loss = 0.0007309753768073357
47, epoch_train_loss=0.0007309753768073357
Epoch 48
Epoch 48 :: Batch 0/1
Batch Loss = 0.0007309753768073357
48, epoch_train_loss=0.0007309753768073357
Epoch 49
Epoch 49 :: Batch 0/1
Batch Loss = 0.0007309753768073357
49, epoch_train_loss=0.0007309753768073357
Epoch 50
Epoch 50 :: Batch 0/1
Batch Loss = 0.0007309753768073357
50, epoch_train_loss=0.0007309753768073357
Epoch 51
Epoch 51 :: Batch 0/1
Batch Loss = 0.0007309753768073357
51, epoch_train_loss=0.0007309753768073357
Epoch 52
Epoch 52 :: Batch 0/1
Batch Loss = 0.0007309753768073357
52, epoch_train_loss=0.0007309753768073357
Epoch 53
Epoch 53 :: Batch 0/1
Batch Loss = 0.0007309753768073357
53, epoch_train_loss=0.0007309753768073357
Epoch 54
Epoch 54 :: Batch 0/1
Batch Loss = 0.0007309753768073357
54, epoch_train_loss=0.0007309753768073357
Epoch 55
Epoch 55 :: Batch 0/1
Batch Loss = 0.0007309753768073357
55, epoch_train_loss=0.0007309753768073357
Epoch 56
Epoch 56 :: Batch 0/1
Batch Loss = 0.0007309753768073357
56, epoch_train_loss=0.0007309753768073357
Epoch 57
Epoch 57 :: Batch 0/1
Batch Loss = 0.0007309753768073357
57, epoch_train_loss=0.0007309753768073357
Epoch 58
Epoch 58 :: Batch 0/1
Batch Loss = 0.0007309753768073357
58, epoch_train_loss=0.0007309753768073357
Epoch 59
Epoch 59 :: Batch 0/1
Batch Loss = 0.0007309753768073357
59, epoch_train_loss=0.0007309753768073357
Epoch 60
Epoch 60 :: Batch 0/1
Batch Loss = 0.0007309753768073356
60, epoch_train_loss=0.0007309753768073356
Epoch 61
Epoch 61 :: Batch 0/1
Batch Loss = 0.0007309753768073356
61, epoch_train_loss=0.0007309753768073356
Epoch 62
Epoch 62 :: Batch 0/1
Batch Loss = 0.0007309753768073356
62, epoch_train_loss=0.0007309753768073356
Epoch 63
Epoch 63 :: Batch 0/1
Batch Loss = 0.0007309753768073356
63, epoch_train_loss=0.0007309753768073356
Epoch 64
Epoch 64 :: Batch 0/1
Batch Loss = 0.0007309753768073356
64, epoch_train_loss=0.0007309753768073356
Epoch 65
Epoch 65 :: Batch 0/1
Batch Loss = 0.0007309753768073356
65, epoch_train_loss=0.0007309753768073356
Epoch 66
Epoch 66 :: Batch 0/1
Batch Loss = 0.0007309753768073356
66, epoch_train_loss=0.0007309753768073356
Epoch 67
Epoch 67 :: Batch 0/1
Batch Loss = 0.0007309753768073356
67, epoch_train_loss=0.0007309753768073356
Epoch 68
Epoch 68 :: Batch 0/1
Batch Loss = 0.0007309753768073356
68, epoch_train_loss=0.0007309753768073356
Epoch 69
Epoch 69 :: Batch 0/1
Batch Loss = 0.0007309753768073356
69, epoch_train_loss=0.0007309753768073356
Epoch 70
Epoch 70 :: Batch 0/1
Batch Loss = 0.0007309753768073356
70, epoch_train_loss=0.0007309753768073356
Epoch 71
Epoch 71 :: Batch 0/1
Batch Loss = 0.0007309753768073356
71, epoch_train_loss=0.0007309753768073356
Epoch 72
Epoch 72 :: Batch 0/1
Batch Loss = 0.0007309753768073356
72, epoch_train_loss=0.0007309753768073356
Epoch 73
Epoch 73 :: Batch 0/1
Batch Loss = 0.0007309753768073356
73, epoch_train_loss=0.0007309753768073356
Epoch 74
Epoch 74 :: Batch 0/1
Batch Loss = 0.0007309753768073356
74, epoch_train_loss=0.0007309753768073356
Epoch 75
Epoch 75 :: Batch 0/1
Batch Loss = 0.0007309753768073356
75, epoch_train_loss=0.0007309753768073356
Epoch 76
Epoch 76 :: Batch 0/1
Batch Loss = 0.0007309753768073356
76, epoch_train_loss=0.0007309753768073356
Epoch 77
Epoch 77 :: Batch 0/1
Batch Loss = 0.0007309753768073356
77, epoch_train_loss=0.0007309753768073356
Epoch 78
Epoch 78 :: Batch 0/1
Batch Loss = 0.0007309753768073356
78, epoch_train_loss=0.0007309753768073356
Epoch 79
Epoch 79 :: Batch 0/1
Batch Loss = 0.0007309753768073356
79, epoch_train_loss=0.0007309753768073356
Epoch 80
Epoch 80 :: Batch 0/1
Batch Loss = 0.0007309753768073356
80, epoch_train_loss=0.0007309753768073356
Epoch 81
Epoch 81 :: Batch 0/1
Batch Loss = 0.0007309753768073356
81, epoch_train_loss=0.0007309753768073356
Epoch 82
Epoch 82 :: Batch 0/1
Batch Loss = 0.0007309753768073356
82, epoch_train_loss=0.0007309753768073356
Epoch 83
Epoch 83 :: Batch 0/1
Batch Loss = 0.0007309753768073356
83, epoch_train_loss=0.0007309753768073356
Epoch 84
Epoch 84 :: Batch 0/1
Batch Loss = 0.0007309753768073356
84, epoch_train_loss=0.0007309753768073356
Epoch 85
Epoch 85 :: Batch 0/1
Batch Loss = 0.0007309753768073356
85, epoch_train_loss=0.0007309753768073356
Epoch 86
Epoch 86 :: Batch 0/1
Batch Loss = 0.0007309753768073356
86, epoch_train_loss=0.0007309753768073356
Epoch 87
Epoch 87 :: Batch 0/1
Batch Loss = 0.0007309753768073356
87, epoch_train_loss=0.0007309753768073356
Epoch 88
Epoch 88 :: Batch 0/1
Batch Loss = 0.0007309753768073356
88, epoch_train_loss=0.0007309753768073356
Epoch 89
Epoch 89 :: Batch 0/1
Batch Loss = 0.0007309753768073356
89, epoch_train_loss=0.0007309753768073356
Epoch 90
Epoch 90 :: Batch 0/1
Batch Loss = 0.0007309753768073356
90, epoch_train_loss=0.0007309753768073356
Epoch 91
Epoch 91 :: Batch 0/1
Batch Loss = 0.0007309753768073356
91, epoch_train_loss=0.0007309753768073356
Epoch 92
Epoch 92 :: Batch 0/1
Batch Loss = 0.0007309753768073356
92, epoch_train_loss=0.0007309753768073356
Epoch 93
Epoch 93 :: Batch 0/1
Batch Loss = 0.0007309753768073356
93, epoch_train_loss=0.0007309753768073356
Epoch 94
Epoch 94 :: Batch 0/1
Batch Loss = 0.0007309753768073356
94, epoch_train_loss=0.0007309753768073356
Epoch 95
Epoch 95 :: Batch 0/1
Batch Loss = 0.0007309753768073356
95, epoch_train_loss=0.0007309753768073356
Epoch 96
Epoch 96 :: Batch 0/1
Batch Loss = 0.0007309753768073356
96, epoch_train_loss=0.0007309753768073356
Epoch 97
Epoch 97 :: Batch 0/1
Batch Loss = 0.0007309753768073356
97, epoch_train_loss=0.0007309753768073356
Epoch 98
Epoch 98 :: Batch 0/1
Batch Loss = 0.0007309753768073356
98, epoch_train_loss=0.0007309753768073356
Epoch 99
Epoch 99 :: Batch 0/1
Batch Loss = 0.0007309753768073356
99, epoch_train_loss=0.0007309753768073356
Epoch 100
Epoch 100 :: Batch 0/1
Batch Loss = 0.0007309753768073356
100, epoch_train_loss=0.0007309753768073356
Epoch 101
Epoch 101 :: Batch 0/1
Batch Loss = 0.0007309753768073356
101, epoch_train_loss=0.0007309753768073356
Epoch 102
Epoch 102 :: Batch 0/1
Batch Loss = 0.0007309753768073356
102, epoch_train_loss=0.0007309753768073356
Epoch 103
Epoch 103 :: Batch 0/1
Batch Loss = 0.0007309753768073356
103, epoch_train_loss=0.0007309753768073356
Epoch 104
Epoch 104 :: Batch 0/1
Batch Loss = 0.0007309753768073356
104, epoch_train_loss=0.0007309753768073356
Epoch 105
Epoch 105 :: Batch 0/1
Batch Loss = 0.0007309753768073356
105, epoch_train_loss=0.0007309753768073356
Epoch 106
Epoch 106 :: Batch 0/1
Batch Loss = 0.0007309753768073356
106, epoch_train_loss=0.0007309753768073356
Epoch 107
Epoch 107 :: Batch 0/1
Batch Loss = 0.0007309753768073356
107, epoch_train_loss=0.0007309753768073356
Epoch 108
Epoch 108 :: Batch 0/1
Batch Loss = 0.0007309753768073356
108, epoch_train_loss=0.0007309753768073356
Epoch 109
Epoch 109 :: Batch 0/1
Batch Loss = 0.0007309753768073356
109, epoch_train_loss=0.0007309753768073356
Epoch 110
Epoch 110 :: Batch 0/1
Batch Loss = 0.0007309753768073356
110, epoch_train_loss=0.0007309753768073356
Epoch 111
Epoch 111 :: Batch 0/1
Batch Loss = 0.0007309753768073356
111, epoch_train_loss=0.0007309753768073356
Epoch 112
Epoch 112 :: Batch 0/1
Batch Loss = 0.0007309753768073356
112, epoch_train_loss=0.0007309753768073356
Epoch 113
Epoch 113 :: Batch 0/1
Batch Loss = 0.0007309753768073356
113, epoch_train_loss=0.0007309753768073356
Epoch 114
Epoch 114 :: Batch 0/1
Batch Loss = 0.0007309753768073356
114, epoch_train_loss=0.0007309753768073356
Epoch 115
Epoch 115 :: Batch 0/1
Batch Loss = 0.0007309753768073356
115, epoch_train_loss=0.0007309753768073356
Epoch 116
Epoch 116 :: Batch 0/1
Batch Loss = 0.0007309753768073356
116, epoch_train_loss=0.0007309753768073356
Epoch 117
Epoch 117 :: Batch 0/1
Batch Loss = 0.0007309753768073356
117, epoch_train_loss=0.0007309753768073356
Epoch 118
Epoch 118 :: Batch 0/1
Batch Loss = 0.0007309753768073356
118, epoch_train_loss=0.0007309753768073356
Epoch 119
Epoch 119 :: Batch 0/1
Batch Loss = 0.0007309753768073356
119, epoch_train_loss=0.0007309753768073356
Epoch 120
Epoch 120 :: Batch 0/1
Batch Loss = 0.0007309753768073356
120, epoch_train_loss=0.0007309753768073356
Epoch 121
Epoch 121 :: Batch 0/1
Batch Loss = 0.0007309753768073356
121, epoch_train_loss=0.0007309753768073356
Epoch 122
Epoch 122 :: Batch 0/1
Batch Loss = 0.0007309753768073356
122, epoch_train_loss=0.0007309753768073356
Epoch 123
Epoch 123 :: Batch 0/1
Batch Loss = 0.0007309753768073356
123, epoch_train_loss=0.0007309753768073356
Epoch 124
Epoch 124 :: Batch 0/1
Batch Loss = 0.0007309753768073356
124, epoch_train_loss=0.0007309753768073356
Epoch 125
Epoch 125 :: Batch 0/1
Batch Loss = 0.0007309753768073356
125, epoch_train_loss=0.0007309753768073356
Epoch 126
Epoch 126 :: Batch 0/1
Batch Loss = 0.0007309753768073356
126, epoch_train_loss=0.0007309753768073356
Epoch 127
Epoch 127 :: Batch 0/1
Batch Loss = 0.0007309753768073356
127, epoch_train_loss=0.0007309753768073356
Epoch 128
Epoch 128 :: Batch 0/1
Batch Loss = 0.0007309753768073356
128, epoch_train_loss=0.0007309753768073356
Epoch 129
Epoch 129 :: Batch 0/1
Batch Loss = 0.0007309753768073356
129, epoch_train_loss=0.0007309753768073356
Epoch 130
Epoch 130 :: Batch 0/1
Batch Loss = 0.0007309753768073356
130, epoch_train_loss=0.0007309753768073356
Epoch 131
Epoch 131 :: Batch 0/1
Batch Loss = 0.0007309753768073356
131, epoch_train_loss=0.0007309753768073356
Epoch 132
Epoch 132 :: Batch 0/1
Batch Loss = 0.0007309753768073356
132, epoch_train_loss=0.0007309753768073356
Epoch 133
Epoch 133 :: Batch 0/1
Batch Loss = 0.0007309753768073356
133, epoch_train_loss=0.0007309753768073356
Epoch 134
Epoch 134 :: Batch 0/1
Batch Loss = 0.0007309753768073356
134, epoch_train_loss=0.0007309753768073356
Epoch 135
Epoch 135 :: Batch 0/1
Batch Loss = 0.0007309753768073356
135, epoch_train_loss=0.0007309753768073356
Epoch 136
Epoch 136 :: Batch 0/1
Batch Loss = 0.0007309753768073356
136, epoch_train_loss=0.0007309753768073356
Epoch 137
Epoch 137 :: Batch 0/1
Batch Loss = 0.0007309753768073356
137, epoch_train_loss=0.0007309753768073356
Epoch 138
Epoch 138 :: Batch 0/1
Batch Loss = 0.0007309753768073356
138, epoch_train_loss=0.0007309753768073356
Epoch 139
Epoch 139 :: Batch 0/1
Batch Loss = 0.0007309753768073356
139, epoch_train_loss=0.0007309753768073356
Epoch 140
Epoch 140 :: Batch 0/1
Batch Loss = 0.0007309753768073356
140, epoch_train_loss=0.0007309753768073356
Epoch 141
Epoch 141 :: Batch 0/1
Batch Loss = 0.0007309753768073356
141, epoch_train_loss=0.0007309753768073356
Epoch 142
Epoch 142 :: Batch 0/1
Batch Loss = 0.0007309753768073356
142, epoch_train_loss=0.0007309753768073356
Epoch 143
Epoch 143 :: Batch 0/1
Batch Loss = 0.0007309753768073356
143, epoch_train_loss=0.0007309753768073356
Epoch 144
Epoch 144 :: Batch 0/1
Batch Loss = 0.0007309753768073356
144, epoch_train_loss=0.0007309753768073356
Epoch 145
Epoch 145 :: Batch 0/1
Batch Loss = 0.0007309753768073356
145, epoch_train_loss=0.0007309753768073356
Epoch 146
Epoch 146 :: Batch 0/1
Batch Loss = 0.0007309753768073356
146, epoch_train_loss=0.0007309753768073356
Epoch 147
Epoch 147 :: Batch 0/1
Batch Loss = 0.0007309753768073356
147, epoch_train_loss=0.0007309753768073356
Epoch 148
Epoch 148 :: Batch 0/1
Batch Loss = 0.0007309753768073356
148, epoch_train_loss=0.0007309753768073356
Epoch 149
Epoch 149 :: Batch 0/1
Batch Loss = 0.0007309753768073356
149, epoch_train_loss=0.0007309753768073356
Epoch 150
Epoch 150 :: Batch 0/1
Batch Loss = 0.0007309753768073356
150, epoch_train_loss=0.0007309753768073356
Epoch 151
Epoch 151 :: Batch 0/1
Batch Loss = 0.0007309753768073356
151, epoch_train_loss=0.0007309753768073356
Epoch 152
Epoch 152 :: Batch 0/1
Batch Loss = 0.0007309753768073356
152, epoch_train_loss=0.0007309753768073356
Epoch 153
Epoch 153 :: Batch 0/1
Batch Loss = 0.0007309753768073356
153, epoch_train_loss=0.0007309753768073356
Epoch 154
Epoch 154 :: Batch 0/1
Batch Loss = 0.0007309753768073356
154, epoch_train_loss=0.0007309753768073356
Epoch 155
Epoch 155 :: Batch 0/1
Batch Loss = 0.0007309753768073356
155, epoch_train_loss=0.0007309753768073356
Epoch 156
Epoch 156 :: Batch 0/1
Batch Loss = 0.0007309753768073356
156, epoch_train_loss=0.0007309753768073356
Epoch 157
Epoch 157 :: Batch 0/1
Batch Loss = 0.0007309753768073356
157, epoch_train_loss=0.0007309753768073356
Epoch 158
Epoch 158 :: Batch 0/1
Batch Loss = 0.0007309753768073356
158, epoch_train_loss=0.0007309753768073356
Epoch 159
Epoch 159 :: Batch 0/1
Batch Loss = 0.0007309753768073356
159, epoch_train_loss=0.0007309753768073356
Epoch 160
Epoch 160 :: Batch 0/1
Batch Loss = 0.0007309753768073356
160, epoch_train_loss=0.0007309753768073356
Epoch 161
Epoch 161 :: Batch 0/1
Batch Loss = 0.0007309753768073356
161, epoch_train_loss=0.0007309753768073356
Epoch 162
Epoch 162 :: Batch 0/1
Batch Loss = 0.0007309753768073356
162, epoch_train_loss=0.0007309753768073356
Epoch 163
Epoch 163 :: Batch 0/1
Batch Loss = 0.0007309753768073356
163, epoch_train_loss=0.0007309753768073356
Epoch 164
Epoch 164 :: Batch 0/1
Batch Loss = 0.0007309753768073356
164, epoch_train_loss=0.0007309753768073356
Epoch 165
Epoch 165 :: Batch 0/1
Batch Loss = 0.0007309753768073356
165, epoch_train_loss=0.0007309753768073356
Epoch 166
Epoch 166 :: Batch 0/1
Batch Loss = 0.0007309753768073356
166, epoch_train_loss=0.0007309753768073356
Epoch 167
Epoch 167 :: Batch 0/1
Batch Loss = 0.0007309753768073356
167, epoch_train_loss=0.0007309753768073356
Epoch 168
Epoch 168 :: Batch 0/1
Batch Loss = 0.0007309753768073356
168, epoch_train_loss=0.0007309753768073356
Epoch 169
Epoch 169 :: Batch 0/1
Batch Loss = 0.0007309753768073356
169, epoch_train_loss=0.0007309753768073356
Epoch 170
Epoch 170 :: Batch 0/1
Batch Loss = 0.0007309753768073356
170, epoch_train_loss=0.0007309753768073356
Epoch 171
Epoch 171 :: Batch 0/1
Batch Loss = 0.0007309753768073356
171, epoch_train_loss=0.0007309753768073356
Epoch 172
Epoch 172 :: Batch 0/1
Batch Loss = 0.0007309753768073356
172, epoch_train_loss=0.0007309753768073356
Epoch 173
Epoch 173 :: Batch 0/1
Batch Loss = 0.0007309753768073356
173, epoch_train_loss=0.0007309753768073356
Epoch 174
Epoch 174 :: Batch 0/1
Batch Loss = 0.0007309753768073356
174, epoch_train_loss=0.0007309753768073356
Epoch 175
Epoch 175 :: Batch 0/1
Batch Loss = 0.0007309753768073356
175, epoch_train_loss=0.0007309753768073356
Epoch 176
Epoch 176 :: Batch 0/1
Batch Loss = 0.0007309753768073356
176, epoch_train_loss=0.0007309753768073356
Epoch 177
Epoch 177 :: Batch 0/1
Batch Loss = 0.0007309753768073356
177, epoch_train_loss=0.0007309753768073356
Epoch 178
Epoch 178 :: Batch 0/1
Batch Loss = 0.0007309753768073356
178, epoch_train_loss=0.0007309753768073356
Epoch 179
Epoch 179 :: Batch 0/1
Batch Loss = 0.0007309753768073356
179, epoch_train_loss=0.0007309753768073356
Epoch 180
Epoch 180 :: Batch 0/1
Batch Loss = 0.0007309753768073356
180, epoch_train_loss=0.0007309753768073356
Epoch 181
Epoch 181 :: Batch 0/1
Batch Loss = 0.0007309753768073356
181, epoch_train_loss=0.0007309753768073356
Epoch 182
Epoch 182 :: Batch 0/1
Batch Loss = 0.0007309753768073356
182, epoch_train_loss=0.0007309753768073356
Epoch 183
Epoch 183 :: Batch 0/1
Batch Loss = 0.0007309753768073356
183, epoch_train_loss=0.0007309753768073356
Epoch 184
Epoch 184 :: Batch 0/1
Batch Loss = 0.0007309753768073356
184, epoch_train_loss=0.0007309753768073356
Epoch 185
Epoch 185 :: Batch 0/1
Batch Loss = 0.0007309753768073356
185, epoch_train_loss=0.0007309753768073356
Epoch 186
Epoch 186 :: Batch 0/1
Batch Loss = 0.0007309753768073356
186, epoch_train_loss=0.0007309753768073356
Epoch 187
Epoch 187 :: Batch 0/1
Batch Loss = 0.0007309753768073356
187, epoch_train_loss=0.0007309753768073356
Epoch 188
Epoch 188 :: Batch 0/1
Batch Loss = 0.0007309753768073356
188, epoch_train_loss=0.0007309753768073356
Epoch 189
Epoch 189 :: Batch 0/1
Batch Loss = 0.0007309753768073356
189, epoch_train_loss=0.0007309753768073356
Epoch 190
Epoch 190 :: Batch 0/1
Batch Loss = 0.0007309753768073356
190, epoch_train_loss=0.0007309753768073356
Epoch 191
Epoch 191 :: Batch 0/1
Batch Loss = 0.0007309753768073356
191, epoch_train_loss=0.0007309753768073356
Epoch 192
Epoch 192 :: Batch 0/1
Batch Loss = 0.0007309753768073356
192, epoch_train_loss=0.0007309753768073356
Epoch 193
Epoch 193 :: Batch 0/1
Batch Loss = 0.0007309753768073356
193, epoch_train_loss=0.0007309753768073356
Epoch 194
Epoch 194 :: Batch 0/1
Batch Loss = 0.0007309753768073356
194, epoch_train_loss=0.0007309753768073356
Epoch 195
Epoch 195 :: Batch 0/1
Batch Loss = 0.0007309753768073356
195, epoch_train_loss=0.0007309753768073356
Epoch 196
Epoch 196 :: Batch 0/1
Batch Loss = 0.0007309753768073356
196, epoch_train_loss=0.0007309753768073356
Epoch 197
Epoch 197 :: Batch 0/1
Batch Loss = 0.0007309753768073356
197, epoch_train_loss=0.0007309753768073356
Epoch 198
Epoch 198 :: Batch 0/1
Batch Loss = 0.0007309753768073356
198, epoch_train_loss=0.0007309753768073356
Epoch 199
Epoch 199 :: Batch 0/1
Batch Loss = 0.0007309753768073356
199, epoch_train_loss=0.0007309753768073356
Epoch 200
Epoch 200 :: Batch 0/1
Batch Loss = 0.0007309753768073356
200, epoch_train_loss=0.0007309753768073356
Epoch 201
Epoch 201 :: Batch 0/1
Batch Loss = 0.0007309753768073356
201, epoch_train_loss=0.0007309753768073356
Epoch 202
Epoch 202 :: Batch 0/1
Batch Loss = 0.0007309753768073356
202, epoch_train_loss=0.0007309753768073356
Epoch 203
Epoch 203 :: Batch 0/1
Batch Loss = 0.0007309753768073356
203, epoch_train_loss=0.0007309753768073356
Epoch 204
Epoch 204 :: Batch 0/1
Batch Loss = 0.0007309753768073356
204, epoch_train_loss=0.0007309753768073356
Epoch 205
Epoch 205 :: Batch 0/1
Batch Loss = 0.0007309753768073356
205, epoch_train_loss=0.0007309753768073356
Epoch 206
Epoch 206 :: Batch 0/1
Batch Loss = 0.0007309753768073356
206, epoch_train_loss=0.0007309753768073356
Epoch 207
Epoch 207 :: Batch 0/1
Batch Loss = 0.0007309753768073356
207, epoch_train_loss=0.0007309753768073356
Epoch 208
Epoch 208 :: Batch 0/1
Batch Loss = 0.0007309753768073356
208, epoch_train_loss=0.0007309753768073356
Epoch 209
Epoch 209 :: Batch 0/1
Batch Loss = 0.0007309753768073356
209, epoch_train_loss=0.0007309753768073356
Epoch 210
Epoch 210 :: Batch 0/1
Batch Loss = 0.0007309753768073356
210, epoch_train_loss=0.0007309753768073356
Epoch 211
Epoch 211 :: Batch 0/1
Batch Loss = 0.0007309753768073356
211, epoch_train_loss=0.0007309753768073356
Epoch 212
Epoch 212 :: Batch 0/1
Batch Loss = 0.0007309753768073356
212, epoch_train_loss=0.0007309753768073356
Epoch 213
Epoch 213 :: Batch 0/1
Batch Loss = 0.0007309753768073356
213, epoch_train_loss=0.0007309753768073356
Epoch 214
Epoch 214 :: Batch 0/1
Batch Loss = 0.0007309753768073356
214, epoch_train_loss=0.0007309753768073356
Epoch 215
Epoch 215 :: Batch 0/1
Batch Loss = 0.0007309753768073356
215, epoch_train_loss=0.0007309753768073356
Epoch 216
Epoch 216 :: Batch 0/1
Batch Loss = 0.0007309753768073356
216, epoch_train_loss=0.0007309753768073356
Epoch 217
Epoch 217 :: Batch 0/1
Batch Loss = 0.0007309753768073356
217, epoch_train_loss=0.0007309753768073356
Epoch 218
Epoch 218 :: Batch 0/1
Batch Loss = 0.0007309753768073356
218, epoch_train_loss=0.0007309753768073356
Epoch 219
Epoch 219 :: Batch 0/1
Batch Loss = 0.0007309753768073356
219, epoch_train_loss=0.0007309753768073356
Epoch 220
Epoch 220 :: Batch 0/1
Batch Loss = 0.0007309753768073356
220, epoch_train_loss=0.0007309753768073356
Epoch 221
Epoch 221 :: Batch 0/1
Batch Loss = 0.0007309753768073356
221, epoch_train_loss=0.0007309753768073356
Epoch 222
Epoch 222 :: Batch 0/1
Batch Loss = 0.0007309753768073356
222, epoch_train_loss=0.0007309753768073356
Epoch 223
Epoch 223 :: Batch 0/1
Batch Loss = 0.0007309753768073356
223, epoch_train_loss=0.0007309753768073356
Epoch 224
Epoch 224 :: Batch 0/1
Batch Loss = 0.0007309753768073356
224, epoch_train_loss=0.0007309753768073356
Epoch 225
Epoch 225 :: Batch 0/1
Batch Loss = 0.0007309753768073356
225, epoch_train_loss=0.0007309753768073356
Epoch 226
Epoch 226 :: Batch 0/1
Batch Loss = 0.0007309753768073356
226, epoch_train_loss=0.0007309753768073356
Epoch 227
Epoch 227 :: Batch 0/1
Batch Loss = 0.0007309753768073356
227, epoch_train_loss=0.0007309753768073356
Epoch 228
Epoch 228 :: Batch 0/1
Batch Loss = 0.0007309753768073356
228, epoch_train_loss=0.0007309753768073356
Epoch 229
Epoch 229 :: Batch 0/1
Batch Loss = 0.0007309753768073356
229, epoch_train_loss=0.0007309753768073356
Epoch 230
Epoch 230 :: Batch 0/1
Batch Loss = 0.0007309753768073356
230, epoch_train_loss=0.0007309753768073356
Epoch 231
Epoch 231 :: Batch 0/1
Batch Loss = 0.0007309753768073356
231, epoch_train_loss=0.0007309753768073356
Epoch 232
Epoch 232 :: Batch 0/1
Batch Loss = 0.0007309753768073356
232, epoch_train_loss=0.0007309753768073356
Epoch 233
Epoch 233 :: Batch 0/1
Batch Loss = 0.0007309753768073356
233, epoch_train_loss=0.0007309753768073356
Epoch 234
Epoch 234 :: Batch 0/1
Batch Loss = 0.0007309753768073356
234, epoch_train_loss=0.0007309753768073356
Epoch 235
Epoch 235 :: Batch 0/1
Batch Loss = 0.0007309753768073356
235, epoch_train_loss=0.0007309753768073356
Epoch 236
Epoch 236 :: Batch 0/1
Batch Loss = 0.0007309753768073356
236, epoch_train_loss=0.0007309753768073356
Epoch 237
Epoch 237 :: Batch 0/1
Batch Loss = 0.0007309753768073356
237, epoch_train_loss=0.0007309753768073356
Epoch 238
Epoch 238 :: Batch 0/1
Batch Loss = 0.0007309753768073356
238, epoch_train_loss=0.0007309753768073356
Epoch 239
Epoch 239 :: Batch 0/1
Batch Loss = 0.0007309753768073356
239, epoch_train_loss=0.0007309753768073356
Epoch 240
Epoch 240 :: Batch 0/1
Batch Loss = 0.0007309753768073356
240, epoch_train_loss=0.0007309753768073356
Epoch 241
Epoch 241 :: Batch 0/1
Batch Loss = 0.0007309753768073356
241, epoch_train_loss=0.0007309753768073356
Epoch 242
Epoch 242 :: Batch 0/1
Batch Loss = 0.0007309753768073356
242, epoch_train_loss=0.0007309753768073356
Epoch 243
Epoch 243 :: Batch 0/1
Batch Loss = 0.0007309753768073356
243, epoch_train_loss=0.0007309753768073356
Epoch 244
Epoch 244 :: Batch 0/1
Batch Loss = 0.0007309753768073356
244, epoch_train_loss=0.0007309753768073356
Epoch 245
Epoch 245 :: Batch 0/1
Batch Loss = 0.0007309753768073356
245, epoch_train_loss=0.0007309753768073356
Epoch 246
Epoch 246 :: Batch 0/1
Batch Loss = 0.0007309753768073356
246, epoch_train_loss=0.0007309753768073356
Epoch 247
Epoch 247 :: Batch 0/1
Batch Loss = 0.0007309753768073356
247, epoch_train_loss=0.0007309753768073356
Epoch 248
Epoch 248 :: Batch 0/1
Batch Loss = 0.0007309753768073356
248, epoch_train_loss=0.0007309753768073356
Epoch 249
Epoch 249 :: Batch 0/1
Batch Loss = 0.0007309753768073356
249, epoch_train_loss=0.0007309753768073356
Epoch 250
Epoch 250 :: Batch 0/1
Batch Loss = 0.0007309753768073356
250, epoch_train_loss=0.0007309753768073356
Epoch 251
Epoch 251 :: Batch 0/1
Batch Loss = 0.0007309753768073356
251, epoch_train_loss=0.0007309753768073356
Epoch 252
Epoch 252 :: Batch 0/1
Batch Loss = 0.0007309753768073356
252, epoch_train_loss=0.0007309753768073356
Epoch 253
Epoch 253 :: Batch 0/1
Batch Loss = 0.0007309753768073356
253, epoch_train_loss=0.0007309753768073356
Epoch 254
Epoch 254 :: Batch 0/1
Batch Loss = 0.0007309753768073356
254, epoch_train_loss=0.0007309753768073356
Epoch 255
Epoch 255 :: Batch 0/1
Batch Loss = 0.0007309753768073356
255, epoch_train_loss=0.0007309753768073356
Epoch 256
Epoch 256 :: Batch 0/1
Batch Loss = 0.0007309753768073356
256, epoch_train_loss=0.0007309753768073356
Epoch 257
Epoch 257 :: Batch 0/1
Batch Loss = 0.0007309753768073356
257, epoch_train_loss=0.0007309753768073356
Epoch 258
Epoch 258 :: Batch 0/1
Batch Loss = 0.0007309753768073356
258, epoch_train_loss=0.0007309753768073356
Epoch 259
Epoch 259 :: Batch 0/1
Batch Loss = 0.0007309753768073356
259, epoch_train_loss=0.0007309753768073356
Epoch 260
Epoch 260 :: Batch 0/1
Batch Loss = 0.0007309753768073356
260, epoch_train_loss=0.0007309753768073356
Epoch 261
Epoch 261 :: Batch 0/1
Batch Loss = 0.0007309753768073356
261, epoch_train_loss=0.0007309753768073356
Epoch 262
Epoch 262 :: Batch 0/1
Batch Loss = 0.0007309753768073356
262, epoch_train_loss=0.0007309753768073356
Epoch 263
Epoch 263 :: Batch 0/1
Batch Loss = 0.0007309753768073356
263, epoch_train_loss=0.0007309753768073356
Epoch 264
Epoch 264 :: Batch 0/1
Batch Loss = 0.0007309753768073356
264, epoch_train_loss=0.0007309753768073356
Epoch 265
Epoch 265 :: Batch 0/1
Batch Loss = 0.0007309753768073356
265, epoch_train_loss=0.0007309753768073356
Epoch 266
Epoch 266 :: Batch 0/1
Batch Loss = 0.0007309753768073356
266, epoch_train_loss=0.0007309753768073356
Epoch 267
Epoch 267 :: Batch 0/1
Batch Loss = 0.0007309753768073356
267, epoch_train_loss=0.0007309753768073356
Epoch 268
Epoch 268 :: Batch 0/1
Batch Loss = 0.0007309753768073356
268, epoch_train_loss=0.0007309753768073356
Epoch 269
Epoch 269 :: Batch 0/1
Batch Loss = 0.0007309753768073356
269, epoch_train_loss=0.0007309753768073356
Epoch 270
Epoch 270 :: Batch 0/1
Batch Loss = 0.0007309753768073356
270, epoch_train_loss=0.0007309753768073356
Epoch 271
Epoch 271 :: Batch 0/1
Batch Loss = 0.0007309753768073356
271, epoch_train_loss=0.0007309753768073356
Epoch 272
Epoch 272 :: Batch 0/1
Batch Loss = 0.0007309753768073356
272, epoch_train_loss=0.0007309753768073356
Epoch 273
Epoch 273 :: Batch 0/1
Batch Loss = 0.0007309753768073356
273, epoch_train_loss=0.0007309753768073356
Epoch 274
Epoch 274 :: Batch 0/1
Batch Loss = 0.0007309753768073356
274, epoch_train_loss=0.0007309753768073356
Epoch 275
Epoch 275 :: Batch 0/1
Batch Loss = 0.0007309753768073356
275, epoch_train_loss=0.0007309753768073356
Epoch 276
Epoch 276 :: Batch 0/1
Batch Loss = 0.0007309753768073356
276, epoch_train_loss=0.0007309753768073356
Epoch 277
Epoch 277 :: Batch 0/1
Batch Loss = 0.0007309753768073356
277, epoch_train_loss=0.0007309753768073356
Epoch 278
Epoch 278 :: Batch 0/1
Batch Loss = 0.0007309753768073356
278, epoch_train_loss=0.0007309753768073356
Epoch 279
Epoch 279 :: Batch 0/1
Batch Loss = 0.0007309753768073356
279, epoch_train_loss=0.0007309753768073356
Epoch 280
Epoch 280 :: Batch 0/1
Batch Loss = 0.0007309753768073356
280, epoch_train_loss=0.0007309753768073356
Epoch 281
Epoch 281 :: Batch 0/1
Batch Loss = 0.0007309753768073356
281, epoch_train_loss=0.0007309753768073356
Epoch 282
Epoch 282 :: Batch 0/1
Batch Loss = 0.0007309753768073356
282, epoch_train_loss=0.0007309753768073356
Epoch 283
Epoch 283 :: Batch 0/1
Batch Loss = 0.0007309753768073356
283, epoch_train_loss=0.0007309753768073356
Epoch 284
Epoch 284 :: Batch 0/1
Batch Loss = 0.0007309753768073356
284, epoch_train_loss=0.0007309753768073356
Epoch 285
Epoch 285 :: Batch 0/1
Batch Loss = 0.0007309753768073356
285, epoch_train_loss=0.0007309753768073356
Epoch 286
Epoch 286 :: Batch 0/1
Batch Loss = 0.0007309753768073356
286, epoch_train_loss=0.0007309753768073356
Epoch 287
Epoch 287 :: Batch 0/1
Batch Loss = 0.0007309753768073356
287, epoch_train_loss=0.0007309753768073356
Epoch 288
Epoch 288 :: Batch 0/1
Batch Loss = 0.0007309753768073356
288, epoch_train_loss=0.0007309753768073356
Epoch 289
Epoch 289 :: Batch 0/1
Batch Loss = 0.0007309753768073356
289, epoch_train_loss=0.0007309753768073356
Epoch 290
Epoch 290 :: Batch 0/1
Batch Loss = 0.0007309753768073356
290, epoch_train_loss=0.0007309753768073356
Epoch 291
Epoch 291 :: Batch 0/1
Batch Loss = 0.0007309753768073356
291, epoch_train_loss=0.0007309753768073356
Epoch 292
Epoch 292 :: Batch 0/1
Batch Loss = 0.0007309753768073356
292, epoch_train_loss=0.0007309753768073356
Epoch 293
Epoch 293 :: Batch 0/1
Batch Loss = 0.0007309753768073356
293, epoch_train_loss=0.0007309753768073356
Epoch 294
Epoch 294 :: Batch 0/1
Batch Loss = 0.0007309753768073356
294, epoch_train_loss=0.0007309753768073356
Epoch 295
Epoch 295 :: Batch 0/1
Batch Loss = 0.0007309753768073356
295, epoch_train_loss=0.0007309753768073356
Epoch 296
Epoch 296 :: Batch 0/1
Batch Loss = 0.0007309753768073356
296, epoch_train_loss=0.0007309753768073356
Epoch 297
Epoch 297 :: Batch 0/1
Batch Loss = 0.0007309753768073356
297, epoch_train_loss=0.0007309753768073356
Epoch 298
Epoch 298 :: Batch 0/1
Batch Loss = 0.0007309753768073356
298, epoch_train_loss=0.0007309753768073356
Epoch 299
Epoch 299 :: Batch 0/1
Batch Loss = 0.0007309753768073356
299, epoch_train_loss=0.0007309753768073356
Epoch 300
Epoch 300 :: Batch 0/1
Batch Loss = 0.0007309753768073356
300, epoch_train_loss=0.0007309753768073356
Epoch 301
Epoch 301 :: Batch 0/1
Batch Loss = 0.0007309753768073356
301, epoch_train_loss=0.0007309753768073356
Epoch 302
Epoch 302 :: Batch 0/1
Batch Loss = 0.0007309753768073356
302, epoch_train_loss=0.0007309753768073356
Epoch 303
Epoch 303 :: Batch 0/1
Batch Loss = 0.0007309753768073356
303, epoch_train_loss=0.0007309753768073356
Epoch 304
Epoch 304 :: Batch 0/1
Batch Loss = 0.0007309753768073356
304, epoch_train_loss=0.0007309753768073356
Epoch 305
Epoch 305 :: Batch 0/1
Batch Loss = 0.0007309753768073356
305, epoch_train_loss=0.0007309753768073356
Epoch 306
Epoch 306 :: Batch 0/1
Batch Loss = 0.0007309753768073356
306, epoch_train_loss=0.0007309753768073356
Epoch 307
Epoch 307 :: Batch 0/1
Batch Loss = 0.0007309753768073356
307, epoch_train_loss=0.0007309753768073356
Epoch 308
Epoch 308 :: Batch 0/1
Batch Loss = 0.0007309753768073356
308, epoch_train_loss=0.0007309753768073356
Epoch 309
Epoch 309 :: Batch 0/1
Batch Loss = 0.0007309753768073356
309, epoch_train_loss=0.0007309753768073356
Epoch 310
Epoch 310 :: Batch 0/1
Batch Loss = 0.0007309753768073356
310, epoch_train_loss=0.0007309753768073356
Epoch 311
Epoch 311 :: Batch 0/1
Batch Loss = 0.0007309753768073356
311, epoch_train_loss=0.0007309753768073356
Epoch 312
Epoch 312 :: Batch 0/1
Batch Loss = 0.0007309753768073356
312, epoch_train_loss=0.0007309753768073356
Epoch 313
Epoch 313 :: Batch 0/1
Batch Loss = 0.0007309753768073356
313, epoch_train_loss=0.0007309753768073356
Epoch 314
Epoch 314 :: Batch 0/1
Batch Loss = 0.0007309753768073356
314, epoch_train_loss=0.0007309753768073356
Epoch 315
Epoch 315 :: Batch 0/1
Batch Loss = 0.0007309753768073356
315, epoch_train_loss=0.0007309753768073356
Epoch 316
Epoch 316 :: Batch 0/1
Batch Loss = 0.0007309753768073356
316, epoch_train_loss=0.0007309753768073356
Epoch 317
Epoch 317 :: Batch 0/1
Batch Loss = 0.0007309753768073356
317, epoch_train_loss=0.0007309753768073356
Epoch 318
Epoch 318 :: Batch 0/1
Batch Loss = 0.0007309753768073356
318, epoch_train_loss=0.0007309753768073356
Epoch 319
Epoch 319 :: Batch 0/1
Batch Loss = 0.0007309753768073356
319, epoch_train_loss=0.0007309753768073356
Epoch 320
Epoch 320 :: Batch 0/1
Batch Loss = 0.0007309753768073356
320, epoch_train_loss=0.0007309753768073356
Epoch 321
Epoch 321 :: Batch 0/1
Batch Loss = 0.0007309753768073356
321, epoch_train_loss=0.0007309753768073356
Epoch 322
Epoch 322 :: Batch 0/1
Batch Loss = 0.0007309753768073356
322, epoch_train_loss=0.0007309753768073356
Epoch 323
Epoch 323 :: Batch 0/1
Batch Loss = 0.0007309753768073356
323, epoch_train_loss=0.0007309753768073356
Epoch 324
Epoch 324 :: Batch 0/1
Batch Loss = 0.0007309753768073356
324, epoch_train_loss=0.0007309753768073356
Epoch 325
Epoch 325 :: Batch 0/1
Batch Loss = 0.0007309753768073356
325, epoch_train_loss=0.0007309753768073356
Epoch 326
Epoch 326 :: Batch 0/1
Batch Loss = 0.0007309753768073356
326, epoch_train_loss=0.0007309753768073356
Epoch 327
Epoch 327 :: Batch 0/1
Batch Loss = 0.0007309753768073356
327, epoch_train_loss=0.0007309753768073356
Epoch 328
Epoch 328 :: Batch 0/1
Batch Loss = 0.0007309753768073356
328, epoch_train_loss=0.0007309753768073356
Epoch 329
Epoch 329 :: Batch 0/1
Batch Loss = 0.0007309753768073356
329, epoch_train_loss=0.0007309753768073356
Epoch 330
Epoch 330 :: Batch 0/1
Batch Loss = 0.0007309753768073356
330, epoch_train_loss=0.0007309753768073356
Epoch 331
Epoch 331 :: Batch 0/1
Batch Loss = 0.0007309753768073356
331, epoch_train_loss=0.0007309753768073356
Epoch 332
Epoch 332 :: Batch 0/1
Batch Loss = 0.0007309753768073356
332, epoch_train_loss=0.0007309753768073356
Epoch 333
Epoch 333 :: Batch 0/1
Batch Loss = 0.0007309753768073356
333, epoch_train_loss=0.0007309753768073356
Epoch 334
Epoch 334 :: Batch 0/1
Batch Loss = 0.0007309753768073356
334, epoch_train_loss=0.0007309753768073356
Epoch 335
Epoch 335 :: Batch 0/1
Batch Loss = 0.0007309753768073356
335, epoch_train_loss=0.0007309753768073356
Epoch 336
Epoch 336 :: Batch 0/1
Batch Loss = 0.0007309753768073356
336, epoch_train_loss=0.0007309753768073356
Epoch 337
Epoch 337 :: Batch 0/1
Batch Loss = 0.0007309753768073356
337, epoch_train_loss=0.0007309753768073356
Epoch 338
Epoch 338 :: Batch 0/1
Batch Loss = 0.0007309753768073356
338, epoch_train_loss=0.0007309753768073356
Epoch 339
Epoch 339 :: Batch 0/1
Batch Loss = 0.0007309753768073356
339, epoch_train_loss=0.0007309753768073356
Epoch 340
Epoch 340 :: Batch 0/1
Batch Loss = 0.0007309753768073356
340, epoch_train_loss=0.0007309753768073356
Epoch 341
Epoch 341 :: Batch 0/1
Batch Loss = 0.0007309753768073356
341, epoch_train_loss=0.0007309753768073356
Epoch 342
Epoch 342 :: Batch 0/1
Batch Loss = 0.0007309753768073356
342, epoch_train_loss=0.0007309753768073356
Epoch 343
Epoch 343 :: Batch 0/1
Batch Loss = 0.0007309753768073356
343, epoch_train_loss=0.0007309753768073356
Epoch 344
Epoch 344 :: Batch 0/1
Batch Loss = 0.0007309753768073356
344, epoch_train_loss=0.0007309753768073356
Epoch 345
Epoch 345 :: Batch 0/1
Batch Loss = 0.0007309753768073356
345, epoch_train_loss=0.0007309753768073356
Epoch 346
Epoch 346 :: Batch 0/1
Batch Loss = 0.0007309753768073356
346, epoch_train_loss=0.0007309753768073356
Epoch 347
Epoch 347 :: Batch 0/1
Batch Loss = 0.0007309753768073356
347, epoch_train_loss=0.0007309753768073356
Epoch 348
Epoch 348 :: Batch 0/1
Batch Loss = 0.0007309753768073356
348, epoch_train_loss=0.0007309753768073356
Epoch 349
Epoch 349 :: Batch 0/1
Batch Loss = 0.0007309753768073356
349, epoch_train_loss=0.0007309753768073356
Epoch 350
Epoch 350 :: Batch 0/1
Batch Loss = 0.0007309753768073356
350, epoch_train_loss=0.0007309753768073356
Epoch 351
Epoch 351 :: Batch 0/1
Batch Loss = 0.0007309753768073356
351, epoch_train_loss=0.0007309753768073356
Epoch 352
Epoch 352 :: Batch 0/1
Batch Loss = 0.0007309753768073356
352, epoch_train_loss=0.0007309753768073356
Epoch 353
Epoch 353 :: Batch 0/1
Batch Loss = 0.0007309753768073356
353, epoch_train_loss=0.0007309753768073356
Epoch 354
Epoch 354 :: Batch 0/1
Batch Loss = 0.0007309753768073356
354, epoch_train_loss=0.0007309753768073356
Epoch 355
Epoch 355 :: Batch 0/1
Batch Loss = 0.0007309753768073356
355, epoch_train_loss=0.0007309753768073356
Epoch 356
Epoch 356 :: Batch 0/1
Batch Loss = 0.0007309753768073356
356, epoch_train_loss=0.0007309753768073356
Epoch 357
Epoch 357 :: Batch 0/1
Batch Loss = 0.0007309753768073356
357, epoch_train_loss=0.0007309753768073356
Epoch 358
Epoch 358 :: Batch 0/1
Batch Loss = 0.0007309753768073356
358, epoch_train_loss=0.0007309753768073356
Epoch 359
Epoch 359 :: Batch 0/1
Batch Loss = 0.0007309753768073356
359, epoch_train_loss=0.0007309753768073356
Epoch 360
Epoch 360 :: Batch 0/1
Batch Loss = 0.0007309753768073356
360, epoch_train_loss=0.0007309753768073356
Epoch 361
Epoch 361 :: Batch 0/1
Batch Loss = 0.0007309753768073356
361, epoch_train_loss=0.0007309753768073356
Epoch 362
Epoch 362 :: Batch 0/1
Batch Loss = 0.0007309753768073356
362, epoch_train_loss=0.0007309753768073356
Epoch 363
Epoch 363 :: Batch 0/1
Batch Loss = 0.0007309753768073356
363, epoch_train_loss=0.0007309753768073356
Epoch 364
Epoch 364 :: Batch 0/1
Batch Loss = 0.0007309753768073356
364, epoch_train_loss=0.0007309753768073356
Epoch 365
Epoch 365 :: Batch 0/1
Batch Loss = 0.0007309753768073356
365, epoch_train_loss=0.0007309753768073356
Epoch 366
Epoch 366 :: Batch 0/1
Batch Loss = 0.0007309753768073356
366, epoch_train_loss=0.0007309753768073356
Epoch 367
Epoch 367 :: Batch 0/1
Batch Loss = 0.0007309753768073356
367, epoch_train_loss=0.0007309753768073356
Epoch 368
Epoch 368 :: Batch 0/1
Batch Loss = 0.0007309753768073356
368, epoch_train_loss=0.0007309753768073356
Epoch 369
Epoch 369 :: Batch 0/1
Batch Loss = 0.0007309753768073356
369, epoch_train_loss=0.0007309753768073356
Epoch 370
Epoch 370 :: Batch 0/1
Batch Loss = 0.0007309753768073356
370, epoch_train_loss=0.0007309753768073356
Epoch 371
Epoch 371 :: Batch 0/1
Batch Loss = 0.0007309753768073356
371, epoch_train_loss=0.0007309753768073356
Epoch 372
Epoch 372 :: Batch 0/1
Batch Loss = 0.0007309753768073356
372, epoch_train_loss=0.0007309753768073356
Epoch 373
Epoch 373 :: Batch 0/1
Batch Loss = 0.0007309753768073356
373, epoch_train_loss=0.0007309753768073356
Epoch 374
Epoch 374 :: Batch 0/1
Batch Loss = 0.0007309753768073356
374, epoch_train_loss=0.0007309753768073356
Epoch 375
Epoch 375 :: Batch 0/1
Batch Loss = 0.0007309753768073356
375, epoch_train_loss=0.0007309753768073356
Epoch 376
Epoch 376 :: Batch 0/1
Batch Loss = 0.0007309753768073356
376, epoch_train_loss=0.0007309753768073356
Epoch 377
Epoch 377 :: Batch 0/1
Batch Loss = 0.0007309753768073356
377, epoch_train_loss=0.0007309753768073356
Epoch 378
Epoch 378 :: Batch 0/1
Batch Loss = 0.0007309753768073356
378, epoch_train_loss=0.0007309753768073356
Epoch 379
Epoch 379 :: Batch 0/1
Batch Loss = 0.0007309753768073356
379, epoch_train_loss=0.0007309753768073356
Epoch 380
Epoch 380 :: Batch 0/1
Batch Loss = 0.0007309753768073356
380, epoch_train_loss=0.0007309753768073356
Epoch 381
Epoch 381 :: Batch 0/1
Batch Loss = 0.0007309753768073356
381, epoch_train_loss=0.0007309753768073356
Epoch 382
Epoch 382 :: Batch 0/1
Batch Loss = 0.0007309753768073356
382, epoch_train_loss=0.0007309753768073356
Epoch 383
Epoch 383 :: Batch 0/1
Batch Loss = 0.0007309753768073356
383, epoch_train_loss=0.0007309753768073356
Epoch 384
Epoch 384 :: Batch 0/1
Batch Loss = 0.0007309753768073356
384, epoch_train_loss=0.0007309753768073356
Epoch 385
Epoch 385 :: Batch 0/1
Batch Loss = 0.0007309753768073356
385, epoch_train_loss=0.0007309753768073356
Epoch 386
Epoch 386 :: Batch 0/1
Batch Loss = 0.0007309753768073356
386, epoch_train_loss=0.0007309753768073356
Epoch 387
Epoch 387 :: Batch 0/1
Batch Loss = 0.0007309753768073356
387, epoch_train_loss=0.0007309753768073356
Epoch 388
Epoch 388 :: Batch 0/1
Batch Loss = 0.0007309753768073356
388, epoch_train_loss=0.0007309753768073356
Epoch 389
Epoch 389 :: Batch 0/1
Batch Loss = 0.0007309753768073356
389, epoch_train_loss=0.0007309753768073356
Epoch 390
Epoch 390 :: Batch 0/1
Batch Loss = 0.0007309753768073356
390, epoch_train_loss=0.0007309753768073356
Epoch 391
Epoch 391 :: Batch 0/1
Batch Loss = 0.0007309753768073356
391, epoch_train_loss=0.0007309753768073356
Epoch 392
Epoch 392 :: Batch 0/1
Batch Loss = 0.0007309753768073356
392, epoch_train_loss=0.0007309753768073356
Epoch 393
Epoch 393 :: Batch 0/1
Batch Loss = 0.0007309753768073356
393, epoch_train_loss=0.0007309753768073356
Epoch 394
Epoch 394 :: Batch 0/1
Batch Loss = 0.0007309753768073356
394, epoch_train_loss=0.0007309753768073356
Epoch 395
Epoch 395 :: Batch 0/1
Batch Loss = 0.0007309753768073356
395, epoch_train_loss=0.0007309753768073356
Epoch 396
Epoch 396 :: Batch 0/1
Batch Loss = 0.0007309753768073356
396, epoch_train_loss=0.0007309753768073356
Epoch 397
Epoch 397 :: Batch 0/1
Batch Loss = 0.0007309753768073356
397, epoch_train_loss=0.0007309753768073356
Epoch 398
Epoch 398 :: Batch 0/1
Batch Loss = 0.0007309753768073356
398, epoch_train_loss=0.0007309753768073356
Epoch 399
Epoch 399 :: Batch 0/1
Batch Loss = 0.0007309753768073356
399, epoch_train_loss=0.0007309753768073356
Epoch 400
Epoch 400 :: Batch 0/1
Batch Loss = 0.0007309753768073356
400, epoch_train_loss=0.0007309753768073356
Epoch 401
Epoch 401 :: Batch 0/1
Batch Loss = 0.0007309753768073356
401, epoch_train_loss=0.0007309753768073356
Epoch 402
Epoch 402 :: Batch 0/1
Batch Loss = 0.0007309753768073356
402, epoch_train_loss=0.0007309753768073356
Epoch 403
Epoch 403 :: Batch 0/1
Batch Loss = 0.0007309753768073356
403, epoch_train_loss=0.0007309753768073356
Epoch 404
Epoch 404 :: Batch 0/1
Batch Loss = 0.0007309753768073356
404, epoch_train_loss=0.0007309753768073356
Epoch 405
Epoch 405 :: Batch 0/1
Batch Loss = 0.0007309753768073356
405, epoch_train_loss=0.0007309753768073356
Epoch 406
Epoch 406 :: Batch 0/1
Batch Loss = 0.0007309753768073356
406, epoch_train_loss=0.0007309753768073356
Epoch 407
Epoch 407 :: Batch 0/1
Batch Loss = 0.0007309753768073356
407, epoch_train_loss=0.0007309753768073356
Epoch 408
Epoch 408 :: Batch 0/1
Batch Loss = 0.0007309753768073356
408, epoch_train_loss=0.0007309753768073356
Epoch 409
Epoch 409 :: Batch 0/1
Batch Loss = 0.0007309753768073356
409, epoch_train_loss=0.0007309753768073356
Epoch 410
Epoch 410 :: Batch 0/1
Batch Loss = 0.0007309753768073356
410, epoch_train_loss=0.0007309753768073356
Epoch 411
Epoch 411 :: Batch 0/1
Batch Loss = 0.0007309753768073356
411, epoch_train_loss=0.0007309753768073356
Epoch 412
Epoch 412 :: Batch 0/1
Batch Loss = 0.0007309753768073356
412, epoch_train_loss=0.0007309753768073356
Epoch 413
Epoch 413 :: Batch 0/1
Batch Loss = 0.0007309753768073356
413, epoch_train_loss=0.0007309753768073356
Epoch 414
Epoch 414 :: Batch 0/1
Batch Loss = 0.0007309753768073356
414, epoch_train_loss=0.0007309753768073356
Epoch 415
Epoch 415 :: Batch 0/1
Batch Loss = 0.0007309753768073356
415, epoch_train_loss=0.0007309753768073356
Epoch 416
Epoch 416 :: Batch 0/1
Batch Loss = 0.0007309753768073356
416, epoch_train_loss=0.0007309753768073356
Epoch 417
Epoch 417 :: Batch 0/1
Batch Loss = 0.0007309753768073356
417, epoch_train_loss=0.0007309753768073356
Epoch 418
Epoch 418 :: Batch 0/1
Batch Loss = 0.0007309753768073356
418, epoch_train_loss=0.0007309753768073356
Epoch 419
Epoch 419 :: Batch 0/1
Batch Loss = 0.0007309753768073356
419, epoch_train_loss=0.0007309753768073356
Epoch 420
Epoch 420 :: Batch 0/1
Batch Loss = 0.0007309753768073356
420, epoch_train_loss=0.0007309753768073356
Epoch 421
Epoch 421 :: Batch 0/1
Batch Loss = 0.0007309753768073356
421, epoch_train_loss=0.0007309753768073356
Epoch 422
Epoch 422 :: Batch 0/1
Batch Loss = 0.0007309753768073356
422, epoch_train_loss=0.0007309753768073356
Epoch 423
Epoch 423 :: Batch 0/1
Batch Loss = 0.0007309753768073356
423, epoch_train_loss=0.0007309753768073356
Epoch 424
Epoch 424 :: Batch 0/1
Batch Loss = 0.0007309753768073356
424, epoch_train_loss=0.0007309753768073356
Epoch 425
Epoch 425 :: Batch 0/1
Batch Loss = 0.0007309753768073356
425, epoch_train_loss=0.0007309753768073356
Epoch 426
Epoch 426 :: Batch 0/1
Batch Loss = 0.0007309753768073356
426, epoch_train_loss=0.0007309753768073356
Epoch 427
Epoch 427 :: Batch 0/1
Batch Loss = 0.0007309753768073356
427, epoch_train_loss=0.0007309753768073356
Epoch 428
Epoch 428 :: Batch 0/1
Batch Loss = 0.0007309753768073356
428, epoch_train_loss=0.0007309753768073356
Epoch 429
Epoch 429 :: Batch 0/1
Batch Loss = 0.0007309753768073356
429, epoch_train_loss=0.0007309753768073356
Epoch 430
Epoch 430 :: Batch 0/1
Batch Loss = 0.0007309753768073356
430, epoch_train_loss=0.0007309753768073356
Epoch 431
Epoch 431 :: Batch 0/1
Batch Loss = 0.0007309753768073356
431, epoch_train_loss=0.0007309753768073356
Epoch 432
Epoch 432 :: Batch 0/1
Batch Loss = 0.0007309753768073356
432, epoch_train_loss=0.0007309753768073356
Epoch 433
Epoch 433 :: Batch 0/1
Batch Loss = 0.0007309753768073356
433, epoch_train_loss=0.0007309753768073356
Epoch 434
Epoch 434 :: Batch 0/1
Batch Loss = 0.0007309753768073356
434, epoch_train_loss=0.0007309753768073356
Epoch 435
Epoch 435 :: Batch 0/1
Batch Loss = 0.0007309753768073356
435, epoch_train_loss=0.0007309753768073356
Epoch 436
Epoch 436 :: Batch 0/1
Batch Loss = 0.0007309753768073356
436, epoch_train_loss=0.0007309753768073356
Epoch 437
Epoch 437 :: Batch 0/1
Batch Loss = 0.0007309753768073356
437, epoch_train_loss=0.0007309753768073356
Epoch 438
Epoch 438 :: Batch 0/1
Batch Loss = 0.0007309753768073356
438, epoch_train_loss=0.0007309753768073356
Epoch 439
Epoch 439 :: Batch 0/1
Batch Loss = 0.0007309753768073356
439, epoch_train_loss=0.0007309753768073356
Epoch 440
Epoch 440 :: Batch 0/1
Batch Loss = 0.0007309753768073356
440, epoch_train_loss=0.0007309753768073356
Epoch 441
Epoch 441 :: Batch 0/1
Batch Loss = 0.0007309753768073356
441, epoch_train_loss=0.0007309753768073356
Epoch 442
Epoch 442 :: Batch 0/1
Batch Loss = 0.0007309753768073356
442, epoch_train_loss=0.0007309753768073356
Epoch 443
Epoch 443 :: Batch 0/1
Batch Loss = 0.0007309753768073356
443, epoch_train_loss=0.0007309753768073356
Epoch 444
Epoch 444 :: Batch 0/1
Batch Loss = 0.0007309753768073356
444, epoch_train_loss=0.0007309753768073356
Epoch 445
Epoch 445 :: Batch 0/1
Batch Loss = 0.0007309753768073356
445, epoch_train_loss=0.0007309753768073356
Epoch 446
Epoch 446 :: Batch 0/1
Batch Loss = 0.0007309753768073356
446, epoch_train_loss=0.0007309753768073356
Epoch 447
Epoch 447 :: Batch 0/1
Batch Loss = 0.0007309753768073356
447, epoch_train_loss=0.0007309753768073356
Epoch 448
Epoch 448 :: Batch 0/1
Batch Loss = 0.0007309753768073356
448, epoch_train_loss=0.0007309753768073356
Epoch 449
Epoch 449 :: Batch 0/1
Batch Loss = 0.0007309753768073356
449, epoch_train_loss=0.0007309753768073356
Epoch 450
Epoch 450 :: Batch 0/1
Batch Loss = 0.0007309753768073356
450, epoch_train_loss=0.0007309753768073356
Epoch 451
Epoch 451 :: Batch 0/1
Batch Loss = 0.0007309753768073356
451, epoch_train_loss=0.0007309753768073356
Epoch 452
Epoch 452 :: Batch 0/1
Batch Loss = 0.0007309753768073356
452, epoch_train_loss=0.0007309753768073356
Epoch 453
Epoch 453 :: Batch 0/1
Batch Loss = 0.0007309753768073356
453, epoch_train_loss=0.0007309753768073356
Epoch 454
Epoch 454 :: Batch 0/1
Batch Loss = 0.0007309753768073356
454, epoch_train_loss=0.0007309753768073356
Epoch 455
Epoch 455 :: Batch 0/1
Batch Loss = 0.0007309753768073356
455, epoch_train_loss=0.0007309753768073356
Epoch 456
Epoch 456 :: Batch 0/1
Batch Loss = 0.0007309753768073356
456, epoch_train_loss=0.0007309753768073356
Epoch 457
Epoch 457 :: Batch 0/1
Batch Loss = 0.0007309753768073356
457, epoch_train_loss=0.0007309753768073356
Epoch 458
Epoch 458 :: Batch 0/1
Batch Loss = 0.0007309753768073356
458, epoch_train_loss=0.0007309753768073356
Epoch 459
Epoch 459 :: Batch 0/1
Batch Loss = 0.0007309753768073356
459, epoch_train_loss=0.0007309753768073356
Epoch 460
Epoch 460 :: Batch 0/1
Batch Loss = 0.0007309753768073356
460, epoch_train_loss=0.0007309753768073356
Epoch 461
Epoch 461 :: Batch 0/1
Batch Loss = 0.0007309753768073356
461, epoch_train_loss=0.0007309753768073356
Epoch 462
Epoch 462 :: Batch 0/1
Batch Loss = 0.0007309753768073356
462, epoch_train_loss=0.0007309753768073356
Epoch 463
Epoch 463 :: Batch 0/1
Batch Loss = 0.0007309753768073356
463, epoch_train_loss=0.0007309753768073356
Epoch 464
Epoch 464 :: Batch 0/1
Batch Loss = 0.0007309753768073356
464, epoch_train_loss=0.0007309753768073356
Epoch 465
Epoch 465 :: Batch 0/1
Batch Loss = 0.0007309753768073356
465, epoch_train_loss=0.0007309753768073356
Epoch 466
Epoch 466 :: Batch 0/1
Batch Loss = 0.0007309753768073356
466, epoch_train_loss=0.0007309753768073356
Epoch 467
Epoch 467 :: Batch 0/1
Batch Loss = 0.0007309753768073356
467, epoch_train_loss=0.0007309753768073356
Epoch 468
Epoch 468 :: Batch 0/1
Batch Loss = 0.0007309753768073356
468, epoch_train_loss=0.0007309753768073356
Epoch 469
Epoch 469 :: Batch 0/1
Batch Loss = 0.0007309753768073356
469, epoch_train_loss=0.0007309753768073356
Epoch 470
Epoch 470 :: Batch 0/1
Batch Loss = 0.0007309753768073356
470, epoch_train_loss=0.0007309753768073356
Epoch 471
Epoch 471 :: Batch 0/1
Batch Loss = 0.0007309753768073356
471, epoch_train_loss=0.0007309753768073356
Epoch 472
Epoch 472 :: Batch 0/1
Batch Loss = 0.0007309753768073356
472, epoch_train_loss=0.0007309753768073356
Epoch 473
Epoch 473 :: Batch 0/1
Batch Loss = 0.0007309753768073356
473, epoch_train_loss=0.0007309753768073356
Epoch 474
Epoch 474 :: Batch 0/1
Batch Loss = 0.0007309753768073356
474, epoch_train_loss=0.0007309753768073356
Epoch 475
Epoch 475 :: Batch 0/1
Batch Loss = 0.0007309753768073356
475, epoch_train_loss=0.0007309753768073356
Epoch 476
Epoch 476 :: Batch 0/1
Batch Loss = 0.0007309753768073356
476, epoch_train_loss=0.0007309753768073356
Epoch 477
Epoch 477 :: Batch 0/1
Batch Loss = 0.0007309753768073356
477, epoch_train_loss=0.0007309753768073356
Epoch 478
Epoch 478 :: Batch 0/1
Batch Loss = 0.0007309753768073356
478, epoch_train_loss=0.0007309753768073356
Epoch 479
Epoch 479 :: Batch 0/1
Batch Loss = 0.0007309753768073356
479, epoch_train_loss=0.0007309753768073356
Epoch 480
Epoch 480 :: Batch 0/1
Batch Loss = 0.0007309753768073356
480, epoch_train_loss=0.0007309753768073356
Epoch 481
Epoch 481 :: Batch 0/1
Batch Loss = 0.0007309753768073356
481, epoch_train_loss=0.0007309753768073356
Epoch 482
Epoch 482 :: Batch 0/1
Batch Loss = 0.0007309753768073356
482, epoch_train_loss=0.0007309753768073356
Epoch 483
Epoch 483 :: Batch 0/1
Batch Loss = 0.0007309753768073356
483, epoch_train_loss=0.0007309753768073356
Epoch 484
Epoch 484 :: Batch 0/1
Batch Loss = 0.0007309753768073356
484, epoch_train_loss=0.0007309753768073356
Epoch 485
Epoch 485 :: Batch 0/1
Batch Loss = 0.0007309753768073356
485, epoch_train_loss=0.0007309753768073356
Epoch 486
Epoch 486 :: Batch 0/1
Batch Loss = 0.0007309753768073356
486, epoch_train_loss=0.0007309753768073356
Epoch 487
Epoch 487 :: Batch 0/1
Batch Loss = 0.0007309753768073356
487, epoch_train_loss=0.0007309753768073356
Epoch 488
Epoch 488 :: Batch 0/1
Batch Loss = 0.0007309753768073356
488, epoch_train_loss=0.0007309753768073356
Epoch 489
Epoch 489 :: Batch 0/1
Batch Loss = 0.0007309753768073356
489, epoch_train_loss=0.0007309753768073356
Epoch 490
Epoch 490 :: Batch 0/1
Batch Loss = 0.0007309753768073356
490, epoch_train_loss=0.0007309753768073356
Epoch 491
Epoch 491 :: Batch 0/1
Batch Loss = 0.0007309753768073356
491, epoch_train_loss=0.0007309753768073356
Epoch 492
Epoch 492 :: Batch 0/1
Batch Loss = 0.0007309753768073356
492, epoch_train_loss=0.0007309753768073356
Epoch 493
Epoch 493 :: Batch 0/1
Batch Loss = 0.0007309753768073356
493, epoch_train_loss=0.0007309753768073356
Epoch 494
Epoch 494 :: Batch 0/1
Batch Loss = 0.0007309753768073356
494, epoch_train_loss=0.0007309753768073356
Epoch 495
Epoch 495 :: Batch 0/1
Batch Loss = 0.0007309753768073356
495, epoch_train_loss=0.0007309753768073356
Epoch 496
Epoch 496 :: Batch 0/1
Batch Loss = 0.0007309753768073356
496, epoch_train_loss=0.0007309753768073356
Epoch 497
Epoch 497 :: Batch 0/1
Batch Loss = 0.0007309753768073356
497, epoch_train_loss=0.0007309753768073356
Epoch 498
Epoch 498 :: Batch 0/1
Batch Loss = 0.0007309753768073356
498, epoch_train_loss=0.0007309753768073356
Epoch 499
Epoch 499 :: Batch 0/1
Batch Loss = 0.0007309753768073356
499, epoch_train_loss=0.0007309753768073356
Epoch 500
Epoch 500 :: Batch 0/1
Batch Loss = 0.0007309753768073356
500, epoch_train_loss=0.0007309753768073356
Epoch 501
Epoch 501 :: Batch 0/1
Batch Loss = 0.0007309753768073356
501, epoch_train_loss=0.0007309753768073356
Epoch 502
Epoch 502 :: Batch 0/1
Batch Loss = 0.0007309753768073356
502, epoch_train_loss=0.0007309753768073356
Epoch 503
Epoch 503 :: Batch 0/1
Batch Loss = 0.0007309753768073356
503, epoch_train_loss=0.0007309753768073356
Epoch 504
Epoch 504 :: Batch 0/1
Batch Loss = 0.0007309753768073356
504, epoch_train_loss=0.0007309753768073356
Epoch 505
Epoch 505 :: Batch 0/1
Batch Loss = 0.0007309753768073356
505, epoch_train_loss=0.0007309753768073356
Epoch 506
Epoch 506 :: Batch 0/1
Batch Loss = 0.0007309753768073356
506, epoch_train_loss=0.0007309753768073356
Epoch 507
Epoch 507 :: Batch 0/1
Batch Loss = 0.0007309753768073356
507, epoch_train_loss=0.0007309753768073356
Epoch 508
Epoch 508 :: Batch 0/1
Batch Loss = 0.0007309753768073356
508, epoch_train_loss=0.0007309753768073356
Epoch 509
Epoch 509 :: Batch 0/1
Batch Loss = 0.0007309753768073356
509, epoch_train_loss=0.0007309753768073356
Epoch 510
Epoch 510 :: Batch 0/1
Batch Loss = 0.0007309753768073356
510, epoch_train_loss=0.0007309753768073356
Epoch 511
Epoch 511 :: Batch 0/1
Batch Loss = 0.0007309753768073356
511, epoch_train_loss=0.0007309753768073356
Epoch 512
Epoch 512 :: Batch 0/1
Batch Loss = 0.0007309753768073356
512, epoch_train_loss=0.0007309753768073356
Epoch 513
Epoch 513 :: Batch 0/1
Batch Loss = 0.0007309753768073356
513, epoch_train_loss=0.0007309753768073356
Epoch 514
Epoch 514 :: Batch 0/1
Batch Loss = 0.0007309753768073356
514, epoch_train_loss=0.0007309753768073356
Epoch 515
Epoch 515 :: Batch 0/1
Batch Loss = 0.0007309753768073356
515, epoch_train_loss=0.0007309753768073356
Epoch 516
Epoch 516 :: Batch 0/1
Batch Loss = 0.0007309753768073356
516, epoch_train_loss=0.0007309753768073356
Epoch 517
Epoch 517 :: Batch 0/1
Batch Loss = 0.0007309753768073356
517, epoch_train_loss=0.0007309753768073356
Epoch 518
Epoch 518 :: Batch 0/1
Batch Loss = 0.0007309753768073356
518, epoch_train_loss=0.0007309753768073356
Epoch 519
Epoch 519 :: Batch 0/1
Batch Loss = 0.0007309753768073356
519, epoch_train_loss=0.0007309753768073356
Epoch 520
Epoch 520 :: Batch 0/1
Batch Loss = 0.0007309753768073356
520, epoch_train_loss=0.0007309753768073356
Epoch 521
Epoch 521 :: Batch 0/1
Batch Loss = 0.0007309753768073356
521, epoch_train_loss=0.0007309753768073356
Epoch 522
Epoch 522 :: Batch 0/1
Batch Loss = 0.0007309753768073356
522, epoch_train_loss=0.0007309753768073356
Epoch 523
Epoch 523 :: Batch 0/1
Batch Loss = 0.0007309753768073356
523, epoch_train_loss=0.0007309753768073356
Epoch 524
Epoch 524 :: Batch 0/1
Batch Loss = 0.0007309753768073356
524, epoch_train_loss=0.0007309753768073356
Epoch 525
Epoch 525 :: Batch 0/1
Batch Loss = 0.0007309753768073356
525, epoch_train_loss=0.0007309753768073356
Epoch 526
Epoch 526 :: Batch 0/1
Batch Loss = 0.0007309753768073356
526, epoch_train_loss=0.0007309753768073356
Epoch 527
Epoch 527 :: Batch 0/1
Batch Loss = 0.0007309753768073356
527, epoch_train_loss=0.0007309753768073356
Epoch 528
Epoch 528 :: Batch 0/1
Batch Loss = 0.0007309753768073356
528, epoch_train_loss=0.0007309753768073356
Epoch 529
Epoch 529 :: Batch 0/1
Batch Loss = 0.0007309753768073356
529, epoch_train_loss=0.0007309753768073356
Epoch 530
Epoch 530 :: Batch 0/1
Batch Loss = 0.0007309753768073356
530, epoch_train_loss=0.0007309753768073356
Epoch 531
Epoch 531 :: Batch 0/1
Batch Loss = 0.0007309753768073356
531, epoch_train_loss=0.0007309753768073356
Epoch 532
Epoch 532 :: Batch 0/1
Batch Loss = 0.0007309753768073356
532, epoch_train_loss=0.0007309753768073356
Epoch 533
Epoch 533 :: Batch 0/1
Batch Loss = 0.0007309753768073356
533, epoch_train_loss=0.0007309753768073356
Epoch 534
Epoch 534 :: Batch 0/1
Batch Loss = 0.0007309753768073356
534, epoch_train_loss=0.0007309753768073356
Epoch 535
Epoch 535 :: Batch 0/1
Batch Loss = 0.0007309753768073356
535, epoch_train_loss=0.0007309753768073356
Epoch 536
Epoch 536 :: Batch 0/1
Batch Loss = 0.0007309753768073356
536, epoch_train_loss=0.0007309753768073356
Epoch 537
Epoch 537 :: Batch 0/1
Batch Loss = 0.0007309753768073356
537, epoch_train_loss=0.0007309753768073356
Epoch 538
Epoch 538 :: Batch 0/1
Batch Loss = 0.0007309753768073356
538, epoch_train_loss=0.0007309753768073356
Epoch 539
Epoch 539 :: Batch 0/1
Batch Loss = 0.0007309753768073356
539, epoch_train_loss=0.0007309753768073356
Epoch 540
Epoch 540 :: Batch 0/1
Batch Loss = 0.0007309753768073356
540, epoch_train_loss=0.0007309753768073356
Epoch 541
Epoch 541 :: Batch 0/1
Batch Loss = 0.0007309753768073356
541, epoch_train_loss=0.0007309753768073356
Epoch 542
Epoch 542 :: Batch 0/1
Batch Loss = 0.0007309753768073356
542, epoch_train_loss=0.0007309753768073356
Epoch 543
Epoch 543 :: Batch 0/1
Batch Loss = 0.0007309753768073356
543, epoch_train_loss=0.0007309753768073356
Epoch 544
Epoch 544 :: Batch 0/1
Batch Loss = 0.0007309753768073356
544, epoch_train_loss=0.0007309753768073356
Epoch 545
Epoch 545 :: Batch 0/1
Batch Loss = 0.0007309753768073356
545, epoch_train_loss=0.0007309753768073356
Epoch 546
Epoch 546 :: Batch 0/1
Batch Loss = 0.0007309753768073356
546, epoch_train_loss=0.0007309753768073356
Epoch 547
Epoch 547 :: Batch 0/1
Batch Loss = 0.0007309753768073356
547, epoch_train_loss=0.0007309753768073356
Epoch 548
Epoch 548 :: Batch 0/1
Batch Loss = 0.0007309753768073356
548, epoch_train_loss=0.0007309753768073356
Epoch 549
Epoch 549 :: Batch 0/1
Batch Loss = 0.0007309753768073356
549, epoch_train_loss=0.0007309753768073356
Epoch 550
Epoch 550 :: Batch 0/1
Batch Loss = 0.0007309753768073356
550, epoch_train_loss=0.0007309753768073356
Epoch 551
Epoch 551 :: Batch 0/1
Batch Loss = 0.0007309753768073356
551, epoch_train_loss=0.0007309753768073356
Epoch 552
Epoch 552 :: Batch 0/1
Batch Loss = 0.0007309753768073356
552, epoch_train_loss=0.0007309753768073356
Epoch 553
Epoch 553 :: Batch 0/1
Batch Loss = 0.0007309753768073356
553, epoch_train_loss=0.0007309753768073356
Epoch 554
Epoch 554 :: Batch 0/1
Batch Loss = 0.0007309753768073356
554, epoch_train_loss=0.0007309753768073356
Epoch 555
Epoch 555 :: Batch 0/1
Batch Loss = 0.0007309753768073356
555, epoch_train_loss=0.0007309753768073356
Epoch 556
Epoch 556 :: Batch 0/1
Batch Loss = 0.0007309753768073356
556, epoch_train_loss=0.0007309753768073356
Epoch 557
Epoch 557 :: Batch 0/1
Batch Loss = 0.0007309753768073356
557, epoch_train_loss=0.0007309753768073356
Epoch 558
Epoch 558 :: Batch 0/1
Batch Loss = 0.0007309753768073356
558, epoch_train_loss=0.0007309753768073356
Epoch 559
Epoch 559 :: Batch 0/1
Batch Loss = 0.0007309753768073356
559, epoch_train_loss=0.0007309753768073356
Epoch 560
Epoch 560 :: Batch 0/1
Batch Loss = 0.0007309753768073356
560, epoch_train_loss=0.0007309753768073356
Epoch 561
Epoch 561 :: Batch 0/1
Batch Loss = 0.0007309753768073356
561, epoch_train_loss=0.0007309753768073356
Epoch 562
Epoch 562 :: Batch 0/1
Batch Loss = 0.0007309753768073356
562, epoch_train_loss=0.0007309753768073356
Epoch 563
Epoch 563 :: Batch 0/1
Batch Loss = 0.0007309753768073356
563, epoch_train_loss=0.0007309753768073356
Epoch 564
Epoch 564 :: Batch 0/1
Batch Loss = 0.0007309753768073356
564, epoch_train_loss=0.0007309753768073356
Epoch 565
Epoch 565 :: Batch 0/1
Batch Loss = 0.0007309753768073356
565, epoch_train_loss=0.0007309753768073356
Epoch 566
Epoch 566 :: Batch 0/1
Batch Loss = 0.0007309753768073356
566, epoch_train_loss=0.0007309753768073356
Epoch 567
Epoch 567 :: Batch 0/1
Batch Loss = 0.0007309753768073356
567, epoch_train_loss=0.0007309753768073356
Epoch 568
Epoch 568 :: Batch 0/1
Batch Loss = 0.0007309753768073356
568, epoch_train_loss=0.0007309753768073356
Epoch 569
Epoch 569 :: Batch 0/1
Batch Loss = 0.0007309753768073356
569, epoch_train_loss=0.0007309753768073356
Epoch 570
Epoch 570 :: Batch 0/1
Batch Loss = 0.0007309753768073356
570, epoch_train_loss=0.0007309753768073356
Epoch 571
Epoch 571 :: Batch 0/1
Batch Loss = 0.0007309753768073356
571, epoch_train_loss=0.0007309753768073356
Epoch 572
Epoch 572 :: Batch 0/1
Batch Loss = 0.0007309753768073356
572, epoch_train_loss=0.0007309753768073356
Epoch 573
Epoch 573 :: Batch 0/1
Batch Loss = 0.0007309753768073356
573, epoch_train_loss=0.0007309753768073356
Epoch 574
Epoch 574 :: Batch 0/1
Batch Loss = 0.0007309753768073356
574, epoch_train_loss=0.0007309753768073356
Epoch 575
Epoch 575 :: Batch 0/1
Batch Loss = 0.0007309753768073356
575, epoch_train_loss=0.0007309753768073356
Epoch 576
Epoch 576 :: Batch 0/1
Batch Loss = 0.0007309753768073356
576, epoch_train_loss=0.0007309753768073356
Epoch 577
Epoch 577 :: Batch 0/1
Batch Loss = 0.0007309753768073356
577, epoch_train_loss=0.0007309753768073356
Epoch 578
Epoch 578 :: Batch 0/1
Batch Loss = 0.0007309753768073356
578, epoch_train_loss=0.0007309753768073356
Epoch 579
Epoch 579 :: Batch 0/1
Batch Loss = 0.0007309753768073356
579, epoch_train_loss=0.0007309753768073356
Epoch 580
Epoch 580 :: Batch 0/1
Batch Loss = 0.0007309753768073356
580, epoch_train_loss=0.0007309753768073356
Epoch 581
Epoch 581 :: Batch 0/1
Batch Loss = 0.0007309753768073356
581, epoch_train_loss=0.0007309753768073356
Epoch 582
Epoch 582 :: Batch 0/1
Batch Loss = 0.0007309753768073356
582, epoch_train_loss=0.0007309753768073356
Epoch 583
Epoch 583 :: Batch 0/1
Batch Loss = 0.0007309753768073356
583, epoch_train_loss=0.0007309753768073356
Epoch 584
Epoch 584 :: Batch 0/1
Batch Loss = 0.0007309753768073356
584, epoch_train_loss=0.0007309753768073356
Epoch 585
Epoch 585 :: Batch 0/1
Batch Loss = 0.0007309753768073356
585, epoch_train_loss=0.0007309753768073356
Epoch 586
Epoch 586 :: Batch 0/1
Batch Loss = 0.0007309753768073356
586, epoch_train_loss=0.0007309753768073356
Epoch 587
Epoch 587 :: Batch 0/1
Batch Loss = 0.0007309753768073356
587, epoch_train_loss=0.0007309753768073356
Epoch 588
Epoch 588 :: Batch 0/1
Batch Loss = 0.0007309753768073356
588, epoch_train_loss=0.0007309753768073356
Epoch 589
Epoch 589 :: Batch 0/1
Batch Loss = 0.0007309753768073356
589, epoch_train_loss=0.0007309753768073356
Epoch 590
Epoch 590 :: Batch 0/1
Batch Loss = 0.0007309753768073356
590, epoch_train_loss=0.0007309753768073356
Epoch 591
Epoch 591 :: Batch 0/1
Batch Loss = 0.0007309753768073356
591, epoch_train_loss=0.0007309753768073356
Epoch 592
Epoch 592 :: Batch 0/1
Batch Loss = 0.0007309753768073356
592, epoch_train_loss=0.0007309753768073356
Epoch 593
Epoch 593 :: Batch 0/1
Batch Loss = 0.0007309753768073356
593, epoch_train_loss=0.0007309753768073356
Epoch 594
Epoch 594 :: Batch 0/1
Batch Loss = 0.0007309753768073356
594, epoch_train_loss=0.0007309753768073356
Epoch 595
Epoch 595 :: Batch 0/1
Batch Loss = 0.0007309753768073356
595, epoch_train_loss=0.0007309753768073356
Epoch 596
Epoch 596 :: Batch 0/1
Batch Loss = 0.0007309753768073356
596, epoch_train_loss=0.0007309753768073356
Epoch 597
Epoch 597 :: Batch 0/1
Batch Loss = 0.0007309753768073356
597, epoch_train_loss=0.0007309753768073356
Epoch 598
Epoch 598 :: Batch 0/1
Batch Loss = 0.0007309753768073356
598, epoch_train_loss=0.0007309753768073356
Epoch 599
Epoch 599 :: Batch 0/1
Batch Loss = 0.0007309753768073356
599, epoch_train_loss=0.0007309753768073356
Epoch 600
Epoch 600 :: Batch 0/1
Batch Loss = 0.0007309753768073356
600, epoch_train_loss=0.0007309753768073356
Epoch 601
Epoch 601 :: Batch 0/1
Batch Loss = 0.0007309753768073356
601, epoch_train_loss=0.0007309753768073356
Epoch 602
Epoch 602 :: Batch 0/1
Batch Loss = 0.0007309753768073356
602, epoch_train_loss=0.0007309753768073356
Epoch 603
Epoch 603 :: Batch 0/1
Batch Loss = 0.0007309753768073356
603, epoch_train_loss=0.0007309753768073356
Epoch 604
Epoch 604 :: Batch 0/1
Batch Loss = 0.0007309753768073356
604, epoch_train_loss=0.0007309753768073356
Epoch 605
Epoch 605 :: Batch 0/1
Batch Loss = 0.0007309753768073356
605, epoch_train_loss=0.0007309753768073356
Epoch 606
Epoch 606 :: Batch 0/1
Batch Loss = 0.0007309753768073356
606, epoch_train_loss=0.0007309753768073356
Epoch 607
Epoch 607 :: Batch 0/1
Batch Loss = 0.0007309753768073356
607, epoch_train_loss=0.0007309753768073356
Epoch 608
Epoch 608 :: Batch 0/1
Batch Loss = 0.0007309753768073356
608, epoch_train_loss=0.0007309753768073356
Epoch 609
Epoch 609 :: Batch 0/1
Batch Loss = 0.0007309753768073356
609, epoch_train_loss=0.0007309753768073356
Epoch 610
Epoch 610 :: Batch 0/1
Batch Loss = 0.0007309753768073356
610, epoch_train_loss=0.0007309753768073356
Epoch 611
Epoch 611 :: Batch 0/1
Batch Loss = 0.0007309753768073356
611, epoch_train_loss=0.0007309753768073356
Epoch 612
Epoch 612 :: Batch 0/1
Batch Loss = 0.0007309753768073356
612, epoch_train_loss=0.0007309753768073356
Epoch 613
Epoch 613 :: Batch 0/1
Batch Loss = 0.0007309753768073356
613, epoch_train_loss=0.0007309753768073356
Epoch 614
Epoch 614 :: Batch 0/1
Batch Loss = 0.0007309753768073356
614, epoch_train_loss=0.0007309753768073356
Epoch 615
Epoch 615 :: Batch 0/1
Batch Loss = 0.0007309753768073356
615, epoch_train_loss=0.0007309753768073356
Epoch 616
Epoch 616 :: Batch 0/1
Batch Loss = 0.0007309753768073356
616, epoch_train_loss=0.0007309753768073356
Epoch 617
Epoch 617 :: Batch 0/1
Batch Loss = 0.0007309753768073356
617, epoch_train_loss=0.0007309753768073356
Epoch 618
Epoch 618 :: Batch 0/1
Batch Loss = 0.0007309753768073356
618, epoch_train_loss=0.0007309753768073356
Epoch 619
Epoch 619 :: Batch 0/1
Batch Loss = 0.0007309753768073356
619, epoch_train_loss=0.0007309753768073356
Epoch 620
Epoch 620 :: Batch 0/1
Batch Loss = 0.0007309753768073356
620, epoch_train_loss=0.0007309753768073356
Epoch 621
Epoch 621 :: Batch 0/1
Batch Loss = 0.0007309753768073356
621, epoch_train_loss=0.0007309753768073356
Epoch 622
Epoch 622 :: Batch 0/1
Batch Loss = 0.0007309753768073356
622, epoch_train_loss=0.0007309753768073356
Epoch 623
Epoch 623 :: Batch 0/1
Batch Loss = 0.0007309753768073356
623, epoch_train_loss=0.0007309753768073356
Epoch 624
Epoch 624 :: Batch 0/1
Batch Loss = 0.0007309753768073356
624, epoch_train_loss=0.0007309753768073356
Epoch 625
Epoch 625 :: Batch 0/1
Batch Loss = 0.0007309753768073356
625, epoch_train_loss=0.0007309753768073356
Epoch 626
Epoch 626 :: Batch 0/1
Batch Loss = 0.0007309753768073356
626, epoch_train_loss=0.0007309753768073356
Epoch 627
Epoch 627 :: Batch 0/1
Batch Loss = 0.0007309753768073356
627, epoch_train_loss=0.0007309753768073356
Epoch 628
Epoch 628 :: Batch 0/1
Batch Loss = 0.0007309753768073356
628, epoch_train_loss=0.0007309753768073356
Epoch 629
Epoch 629 :: Batch 0/1
Batch Loss = 0.0007309753768073356
629, epoch_train_loss=0.0007309753768073356
Epoch 630
Epoch 630 :: Batch 0/1
Batch Loss = 0.0007309753768073356
630, epoch_train_loss=0.0007309753768073356
Epoch 631
Epoch 631 :: Batch 0/1
Batch Loss = 0.0007309753768073356
631, epoch_train_loss=0.0007309753768073356
Epoch 632
Epoch 632 :: Batch 0/1
Batch Loss = 0.0007309753768073356
632, epoch_train_loss=0.0007309753768073356
Epoch 633
Epoch 633 :: Batch 0/1
Batch Loss = 0.0007309753768073356
633, epoch_train_loss=0.0007309753768073356
Epoch 634
Epoch 634 :: Batch 0/1
Batch Loss = 0.0007309753768073356
634, epoch_train_loss=0.0007309753768073356
Epoch 635
Epoch 635 :: Batch 0/1
Batch Loss = 0.0007309753768073356
635, epoch_train_loss=0.0007309753768073356
Epoch 636
Epoch 636 :: Batch 0/1
Batch Loss = 0.0007309753768073356
636, epoch_train_loss=0.0007309753768073356
Epoch 637
Epoch 637 :: Batch 0/1
Batch Loss = 0.0007309753768073356
637, epoch_train_loss=0.0007309753768073356
Epoch 638
Epoch 638 :: Batch 0/1
Batch Loss = 0.0007309753768073356
638, epoch_train_loss=0.0007309753768073356
Epoch 639
Epoch 639 :: Batch 0/1
Batch Loss = 0.0007309753768073356
639, epoch_train_loss=0.0007309753768073356
Epoch 640
Epoch 640 :: Batch 0/1
Batch Loss = 0.0007309753768073356
640, epoch_train_loss=0.0007309753768073356
Epoch 641
Epoch 641 :: Batch 0/1
Batch Loss = 0.0007309753768073356
641, epoch_train_loss=0.0007309753768073356
Epoch 642
Epoch 642 :: Batch 0/1
Batch Loss = 0.0007309753768073356
642, epoch_train_loss=0.0007309753768073356
Epoch 643
Epoch 643 :: Batch 0/1
Batch Loss = 0.0007309753768073356
643, epoch_train_loss=0.0007309753768073356
Epoch 644
Epoch 644 :: Batch 0/1
Batch Loss = 0.0007309753768073356
644, epoch_train_loss=0.0007309753768073356
Epoch 645
Epoch 645 :: Batch 0/1
Batch Loss = 0.0007309753768073356
645, epoch_train_loss=0.0007309753768073356
Epoch 646
Epoch 646 :: Batch 0/1
Batch Loss = 0.0007309753768073356
646, epoch_train_loss=0.0007309753768073356
Epoch 647
Epoch 647 :: Batch 0/1
Batch Loss = 0.0007309753768073356
647, epoch_train_loss=0.0007309753768073356
Epoch 648
Epoch 648 :: Batch 0/1
Batch Loss = 0.0007309753768073356
648, epoch_train_loss=0.0007309753768073356
Epoch 649
Epoch 649 :: Batch 0/1
Batch Loss = 0.0007309753768073356
649, epoch_train_loss=0.0007309753768073356
Epoch 650
Epoch 650 :: Batch 0/1
Batch Loss = 0.0007309753768073356
650, epoch_train_loss=0.0007309753768073356
Epoch 651
Epoch 651 :: Batch 0/1
Batch Loss = 0.0007309753768073356
651, epoch_train_loss=0.0007309753768073356
Epoch 652
Epoch 652 :: Batch 0/1
Batch Loss = 0.0007309753768073356
652, epoch_train_loss=0.0007309753768073356
Epoch 653
Epoch 653 :: Batch 0/1
Batch Loss = 0.0007309753768073356
653, epoch_train_loss=0.0007309753768073356
Epoch 654
Epoch 654 :: Batch 0/1
Batch Loss = 0.0007309753768073356
654, epoch_train_loss=0.0007309753768073356
Epoch 655
Epoch 655 :: Batch 0/1
Batch Loss = 0.0007309753768073356
655, epoch_train_loss=0.0007309753768073356
Epoch 656
Epoch 656 :: Batch 0/1
Batch Loss = 0.0007309753768073356
656, epoch_train_loss=0.0007309753768073356
Epoch 657
Epoch 657 :: Batch 0/1
Batch Loss = 0.0007309753768073356
657, epoch_train_loss=0.0007309753768073356
Epoch 658
Epoch 658 :: Batch 0/1
Batch Loss = 0.0007309753768073356
658, epoch_train_loss=0.0007309753768073356
Epoch 659
Epoch 659 :: Batch 0/1
Batch Loss = 0.0007309753768073356
659, epoch_train_loss=0.0007309753768073356
Epoch 660
Epoch 660 :: Batch 0/1
Batch Loss = 0.0007309753768073356
660, epoch_train_loss=0.0007309753768073356
Epoch 661
Epoch 661 :: Batch 0/1
Batch Loss = 0.0007309753768073356
661, epoch_train_loss=0.0007309753768073356
Epoch 662
Epoch 662 :: Batch 0/1
Batch Loss = 0.0007309753768073356
662, epoch_train_loss=0.0007309753768073356
Epoch 663
Epoch 663 :: Batch 0/1
Batch Loss = 0.0007309753768073356
663, epoch_train_loss=0.0007309753768073356
Epoch 664
Epoch 664 :: Batch 0/1
Batch Loss = 0.0007309753768073356
664, epoch_train_loss=0.0007309753768073356
Epoch 665
Epoch 665 :: Batch 0/1
Batch Loss = 0.0007309753768073356
665, epoch_train_loss=0.0007309753768073356
Epoch 666
Epoch 666 :: Batch 0/1
Batch Loss = 0.0007309753768073356
666, epoch_train_loss=0.0007309753768073356
Epoch 667
Epoch 667 :: Batch 0/1
Batch Loss = 0.0007309753768073356
667, epoch_train_loss=0.0007309753768073356
Epoch 668
Epoch 668 :: Batch 0/1
Batch Loss = 0.0007309753768073356
668, epoch_train_loss=0.0007309753768073356
Epoch 669
Epoch 669 :: Batch 0/1
Batch Loss = 0.0007309753768073356
669, epoch_train_loss=0.0007309753768073356
Epoch 670
Epoch 670 :: Batch 0/1
Batch Loss = 0.0007309753768073356
670, epoch_train_loss=0.0007309753768073356
Epoch 671
Epoch 671 :: Batch 0/1
Batch Loss = 0.0007309753768073356
671, epoch_train_loss=0.0007309753768073356
Epoch 672
Epoch 672 :: Batch 0/1
Batch Loss = 0.0007309753768073356
672, epoch_train_loss=0.0007309753768073356
Epoch 673
Epoch 673 :: Batch 0/1
Batch Loss = 0.0007309753768073356
673, epoch_train_loss=0.0007309753768073356
Epoch 674
Epoch 674 :: Batch 0/1
Batch Loss = 0.0007309753768073356
674, epoch_train_loss=0.0007309753768073356
Epoch 675
Epoch 675 :: Batch 0/1
Batch Loss = 0.0007309753768073356
675, epoch_train_loss=0.0007309753768073356
Epoch 676
Epoch 676 :: Batch 0/1
Batch Loss = 0.0007309753768073356
676, epoch_train_loss=0.0007309753768073356
Epoch 677
Epoch 677 :: Batch 0/1
Batch Loss = 0.0007309753768073356
677, epoch_train_loss=0.0007309753768073356
Epoch 678
Epoch 678 :: Batch 0/1
Batch Loss = 0.0007309753768073356
678, epoch_train_loss=0.0007309753768073356
Epoch 679
Epoch 679 :: Batch 0/1
Batch Loss = 0.0007309753768073356
679, epoch_train_loss=0.0007309753768073356
Epoch 680
Epoch 680 :: Batch 0/1
Batch Loss = 0.0007309753768073356
680, epoch_train_loss=0.0007309753768073356
Epoch 681
Epoch 681 :: Batch 0/1
Batch Loss = 0.0007309753768073356
681, epoch_train_loss=0.0007309753768073356
Epoch 682
Epoch 682 :: Batch 0/1
Batch Loss = 0.0007309753768073356
682, epoch_train_loss=0.0007309753768073356
Epoch 683
Epoch 683 :: Batch 0/1
Batch Loss = 0.0007309753768073356
683, epoch_train_loss=0.0007309753768073356
Epoch 684
Epoch 684 :: Batch 0/1
Batch Loss = 0.0007309753768073356
684, epoch_train_loss=0.0007309753768073356
Epoch 685
Epoch 685 :: Batch 0/1
Batch Loss = 0.0007309753768073356
685, epoch_train_loss=0.0007309753768073356
Epoch 686
Epoch 686 :: Batch 0/1
Batch Loss = 0.0007309753768073356
686, epoch_train_loss=0.0007309753768073356
Epoch 687
Epoch 687 :: Batch 0/1
Batch Loss = 0.0007309753768073356
687, epoch_train_loss=0.0007309753768073356
Epoch 688
Epoch 688 :: Batch 0/1
Batch Loss = 0.0007309753768073356
688, epoch_train_loss=0.0007309753768073356
Epoch 689
Epoch 689 :: Batch 0/1
Batch Loss = 0.0007309753768073356
689, epoch_train_loss=0.0007309753768073356
Epoch 690
Epoch 690 :: Batch 0/1
Batch Loss = 0.0007309753768073356
690, epoch_train_loss=0.0007309753768073356
Epoch 691
Epoch 691 :: Batch 0/1
Batch Loss = 0.0007309753768073356
691, epoch_train_loss=0.0007309753768073356
Epoch 692
Epoch 692 :: Batch 0/1
Batch Loss = 0.0007309753768073356
692, epoch_train_loss=0.0007309753768073356
Epoch 693
Epoch 693 :: Batch 0/1
Batch Loss = 0.0007309753768073356
693, epoch_train_loss=0.0007309753768073356
Epoch 694
Epoch 694 :: Batch 0/1
Batch Loss = 0.0007309753768073356
694, epoch_train_loss=0.0007309753768073356
Epoch 695
Epoch 695 :: Batch 0/1
Batch Loss = 0.0007309753768073356
695, epoch_train_loss=0.0007309753768073356
Epoch 696
Epoch 696 :: Batch 0/1
Batch Loss = 0.0007309753768073356
696, epoch_train_loss=0.0007309753768073356
Epoch 697
Epoch 697 :: Batch 0/1
Batch Loss = 0.0007309753768073356
697, epoch_train_loss=0.0007309753768073356
Epoch 698
Epoch 698 :: Batch 0/1
Batch Loss = 0.0007309753768073356
698, epoch_train_loss=0.0007309753768073356
Epoch 699
Epoch 699 :: Batch 0/1
Batch Loss = 0.0007309753768073356
699, epoch_train_loss=0.0007309753768073356
Epoch 700
Epoch 700 :: Batch 0/1
Batch Loss = 0.0007309753768073356
700, epoch_train_loss=0.0007309753768073356
Epoch 701
Epoch 701 :: Batch 0/1
Batch Loss = 0.0007309753768073356
701, epoch_train_loss=0.0007309753768073356
Epoch 702
Epoch 702 :: Batch 0/1
Batch Loss = 0.0007309753768073356
702, epoch_train_loss=0.0007309753768073356
Epoch 703
Epoch 703 :: Batch 0/1
Batch Loss = 0.0007309753768073356
703, epoch_train_loss=0.0007309753768073356
Epoch 704
Epoch 704 :: Batch 0/1
Batch Loss = 0.0007309753768073356
704, epoch_train_loss=0.0007309753768073356
Epoch 705
Epoch 705 :: Batch 0/1
Batch Loss = 0.0007309753768073356
705, epoch_train_loss=0.0007309753768073356
Epoch 706
Epoch 706 :: Batch 0/1
Batch Loss = 0.0007309753768073356
706, epoch_train_loss=0.0007309753768073356
Epoch 707
Epoch 707 :: Batch 0/1
Batch Loss = 0.0007309753768073356
707, epoch_train_loss=0.0007309753768073356
Epoch 708
Epoch 708 :: Batch 0/1
Batch Loss = 0.0007309753768073356
708, epoch_train_loss=0.0007309753768073356
Epoch 709
Epoch 709 :: Batch 0/1
Batch Loss = 0.0007309753768073356
709, epoch_train_loss=0.0007309753768073356
Epoch 710
Epoch 710 :: Batch 0/1
Batch Loss = 0.0007309753768073356
710, epoch_train_loss=0.0007309753768073356
Epoch 711
Epoch 711 :: Batch 0/1
Batch Loss = 0.0007309753768073356
711, epoch_train_loss=0.0007309753768073356
Epoch 712
Epoch 712 :: Batch 0/1
Batch Loss = 0.0007309753768073356
712, epoch_train_loss=0.0007309753768073356
Epoch 713
Epoch 713 :: Batch 0/1
Batch Loss = 0.0007309753768073356
713, epoch_train_loss=0.0007309753768073356
Epoch 714
Epoch 714 :: Batch 0/1
Batch Loss = 0.0007309753768073356
714, epoch_train_loss=0.0007309753768073356
Epoch 715
Epoch 715 :: Batch 0/1
Batch Loss = 0.0007309753768073356
715, epoch_train_loss=0.0007309753768073356
Epoch 716
Epoch 716 :: Batch 0/1
Batch Loss = 0.0007309753768073356
716, epoch_train_loss=0.0007309753768073356
Epoch 717
Epoch 717 :: Batch 0/1
Batch Loss = 0.0007309753768073356
717, epoch_train_loss=0.0007309753768073356
Epoch 718
Epoch 718 :: Batch 0/1
Batch Loss = 0.0007309753768073356
718, epoch_train_loss=0.0007309753768073356
Epoch 719
Epoch 719 :: Batch 0/1
Batch Loss = 0.0007309753768073356
719, epoch_train_loss=0.0007309753768073356
Epoch 720
Epoch 720 :: Batch 0/1
Batch Loss = 0.0007309753768073356
720, epoch_train_loss=0.0007309753768073356
Epoch 721
Epoch 721 :: Batch 0/1
Batch Loss = 0.0007309753768073356
721, epoch_train_loss=0.0007309753768073356
Epoch 722
Epoch 722 :: Batch 0/1
Batch Loss = 0.0007309753768073356
722, epoch_train_loss=0.0007309753768073356
Epoch 723
Epoch 723 :: Batch 0/1
Batch Loss = 0.0007309753768073356
723, epoch_train_loss=0.0007309753768073356
Epoch 724
Epoch 724 :: Batch 0/1
Batch Loss = 0.0007309753768073356
724, epoch_train_loss=0.0007309753768073356
Epoch 725
Epoch 725 :: Batch 0/1
Batch Loss = 0.0007309753768073356
725, epoch_train_loss=0.0007309753768073356
Epoch 726
Epoch 726 :: Batch 0/1
Batch Loss = 0.0007309753768073356
726, epoch_train_loss=0.0007309753768073356
Epoch 727
Epoch 727 :: Batch 0/1
Batch Loss = 0.0007309753768073356
727, epoch_train_loss=0.0007309753768073356
Epoch 728
Epoch 728 :: Batch 0/1
Batch Loss = 0.0007309753768073356
728, epoch_train_loss=0.0007309753768073356
Epoch 729
Epoch 729 :: Batch 0/1
Batch Loss = 0.0007309753768073356
729, epoch_train_loss=0.0007309753768073356
Epoch 730
Epoch 730 :: Batch 0/1
Batch Loss = 0.0007309753768073356
730, epoch_train_loss=0.0007309753768073356
Epoch 731
Epoch 731 :: Batch 0/1
Batch Loss = 0.0007309753768073356
731, epoch_train_loss=0.0007309753768073356
Epoch 732
Epoch 732 :: Batch 0/1
Batch Loss = 0.0007309753768073356
732, epoch_train_loss=0.0007309753768073356
Epoch 733
Epoch 733 :: Batch 0/1
Batch Loss = 0.0007309753768073356
733, epoch_train_loss=0.0007309753768073356
Epoch 734
Epoch 734 :: Batch 0/1
Batch Loss = 0.0007309753768073356
734, epoch_train_loss=0.0007309753768073356
Epoch 735
Epoch 735 :: Batch 0/1
Batch Loss = 0.0007309753768073356
735, epoch_train_loss=0.0007309753768073356
Epoch 736
Epoch 736 :: Batch 0/1
Batch Loss = 0.0007309753768073356
736, epoch_train_loss=0.0007309753768073356
Epoch 737
Epoch 737 :: Batch 0/1
Batch Loss = 0.0007309753768073356
737, epoch_train_loss=0.0007309753768073356
Epoch 738
Epoch 738 :: Batch 0/1
Batch Loss = 0.0007309753768073356
738, epoch_train_loss=0.0007309753768073356
Epoch 739
Epoch 739 :: Batch 0/1
Batch Loss = 0.0007309753768073356
739, epoch_train_loss=0.0007309753768073356
Epoch 740
Epoch 740 :: Batch 0/1
Batch Loss = 0.0007309753768073356
740, epoch_train_loss=0.0007309753768073356
Epoch 741
Epoch 741 :: Batch 0/1
Batch Loss = 0.0007309753768073356
741, epoch_train_loss=0.0007309753768073356
Epoch 742
Epoch 742 :: Batch 0/1
Batch Loss = 0.0007309753768073356
742, epoch_train_loss=0.0007309753768073356
Epoch 743
Epoch 743 :: Batch 0/1
Batch Loss = 0.0007309753768073356
743, epoch_train_loss=0.0007309753768073356
Epoch 744
Epoch 744 :: Batch 0/1
Batch Loss = 0.0007309753768073356
744, epoch_train_loss=0.0007309753768073356
Epoch 745
Epoch 745 :: Batch 0/1
Batch Loss = 0.0007309753768073356
745, epoch_train_loss=0.0007309753768073356
Epoch 746
Epoch 746 :: Batch 0/1
Batch Loss = 0.0007309753768073356
746, epoch_train_loss=0.0007309753768073356
Epoch 747
Epoch 747 :: Batch 0/1
Batch Loss = 0.0007309753768073356
747, epoch_train_loss=0.0007309753768073356
Epoch 748
Epoch 748 :: Batch 0/1
Batch Loss = 0.0007309753768073356
748, epoch_train_loss=0.0007309753768073356
Epoch 749
Epoch 749 :: Batch 0/1
Batch Loss = 0.0007309753768073356
749, epoch_train_loss=0.0007309753768073356
Epoch 750
Epoch 750 :: Batch 0/1
Batch Loss = 0.0007309753768073356
750, epoch_train_loss=0.0007309753768073356
Epoch 751
Epoch 751 :: Batch 0/1
Batch Loss = 0.0007309753768073356
751, epoch_train_loss=0.0007309753768073356
Epoch 752
Epoch 752 :: Batch 0/1
Batch Loss = 0.0007309753768073356
752, epoch_train_loss=0.0007309753768073356
Epoch 753
Epoch 753 :: Batch 0/1
Batch Loss = 0.0007309753768073356
753, epoch_train_loss=0.0007309753768073356
Epoch 754
Epoch 754 :: Batch 0/1
Batch Loss = 0.0007309753768073356
754, epoch_train_loss=0.0007309753768073356
Epoch 755
Epoch 755 :: Batch 0/1
Batch Loss = 0.0007309753768073356
755, epoch_train_loss=0.0007309753768073356
Epoch 756
Epoch 756 :: Batch 0/1
Batch Loss = 0.0007309753768073356
756, epoch_train_loss=0.0007309753768073356
Epoch 757
Epoch 757 :: Batch 0/1
Batch Loss = 0.0007309753768073356
757, epoch_train_loss=0.0007309753768073356
Epoch 758
Epoch 758 :: Batch 0/1
Batch Loss = 0.0007309753768073356
758, epoch_train_loss=0.0007309753768073356
Epoch 759
Epoch 759 :: Batch 0/1
Batch Loss = 0.0007309753768073356
759, epoch_train_loss=0.0007309753768073356
Epoch 760
Epoch 760 :: Batch 0/1
Batch Loss = 0.0007309753768073356
760, epoch_train_loss=0.0007309753768073356
Epoch 761
Epoch 761 :: Batch 0/1
Batch Loss = 0.0007309753768073356
761, epoch_train_loss=0.0007309753768073356
Epoch 762
Epoch 762 :: Batch 0/1
Batch Loss = 0.0007309753768073356
762, epoch_train_loss=0.0007309753768073356
Epoch 763
Epoch 763 :: Batch 0/1
Batch Loss = 0.0007309753768073356
763, epoch_train_loss=0.0007309753768073356
Epoch 764
Epoch 764 :: Batch 0/1
Batch Loss = 0.0007309753768073356
764, epoch_train_loss=0.0007309753768073356
Epoch 765
Epoch 765 :: Batch 0/1
Batch Loss = 0.0007309753768073356
765, epoch_train_loss=0.0007309753768073356
Epoch 766
Epoch 766 :: Batch 0/1
Batch Loss = 0.0007309753768073356
766, epoch_train_loss=0.0007309753768073356
Epoch 767
Epoch 767 :: Batch 0/1
Batch Loss = 0.0007309753768073356
767, epoch_train_loss=0.0007309753768073356
Epoch 768
Epoch 768 :: Batch 0/1
Batch Loss = 0.0007309753768073356
768, epoch_train_loss=0.0007309753768073356
Epoch 769
Epoch 769 :: Batch 0/1
Batch Loss = 0.0007309753768073356
769, epoch_train_loss=0.0007309753768073356
Epoch 770
Epoch 770 :: Batch 0/1
Batch Loss = 0.0007309753768073356
770, epoch_train_loss=0.0007309753768073356
Epoch 771
Epoch 771 :: Batch 0/1
Batch Loss = 0.0007309753768073356
771, epoch_train_loss=0.0007309753768073356
Epoch 772
Epoch 772 :: Batch 0/1
Batch Loss = 0.0007309753768073356
772, epoch_train_loss=0.0007309753768073356
Epoch 773
Epoch 773 :: Batch 0/1
Batch Loss = 0.0007309753768073356
773, epoch_train_loss=0.0007309753768073356
Epoch 774
Epoch 774 :: Batch 0/1
Batch Loss = 0.0007309753768073356
774, epoch_train_loss=0.0007309753768073356
Epoch 775
Epoch 775 :: Batch 0/1
Batch Loss = 0.0007309753768073356
775, epoch_train_loss=0.0007309753768073356
Epoch 776
Epoch 776 :: Batch 0/1
Batch Loss = 0.0007309753768073356
776, epoch_train_loss=0.0007309753768073356
Epoch 777
Epoch 777 :: Batch 0/1
Batch Loss = 0.0007309753768073356
777, epoch_train_loss=0.0007309753768073356
Epoch 778
Epoch 778 :: Batch 0/1
Batch Loss = 0.0007309753768073356
778, epoch_train_loss=0.0007309753768073356
Epoch 779
Epoch 779 :: Batch 0/1
Batch Loss = 0.0007309753768073356
779, epoch_train_loss=0.0007309753768073356
Epoch 780
Epoch 780 :: Batch 0/1
Batch Loss = 0.0007309753768073356
780, epoch_train_loss=0.0007309753768073356
Epoch 781
Epoch 781 :: Batch 0/1
Batch Loss = 0.0007309753768073356
781, epoch_train_loss=0.0007309753768073356
Epoch 782
Epoch 782 :: Batch 0/1
Batch Loss = 0.0007309753768073356
782, epoch_train_loss=0.0007309753768073356
Epoch 783
Epoch 783 :: Batch 0/1
Batch Loss = 0.0007309753768073356
783, epoch_train_loss=0.0007309753768073356
Epoch 784
Epoch 784 :: Batch 0/1
Batch Loss = 0.0007309753768073356
784, epoch_train_loss=0.0007309753768073356
Epoch 785
Epoch 785 :: Batch 0/1
Batch Loss = 0.0007309753768073356
785, epoch_train_loss=0.0007309753768073356
Epoch 786
Epoch 786 :: Batch 0/1
Batch Loss = 0.0007309753768073356
786, epoch_train_loss=0.0007309753768073356
Epoch 787
Epoch 787 :: Batch 0/1
Batch Loss = 0.0007309753768073356
787, epoch_train_loss=0.0007309753768073356
Epoch 788
Epoch 788 :: Batch 0/1
Batch Loss = 0.0007309753768073356
788, epoch_train_loss=0.0007309753768073356
Epoch 789
Epoch 789 :: Batch 0/1
Batch Loss = 0.0007309753768073356
789, epoch_train_loss=0.0007309753768073356
Epoch 790
Epoch 790 :: Batch 0/1
Batch Loss = 0.0007309753768073356
790, epoch_train_loss=0.0007309753768073356
Epoch 791
Epoch 791 :: Batch 0/1
Batch Loss = 0.0007309753768073356
791, epoch_train_loss=0.0007309753768073356
Epoch 792
Epoch 792 :: Batch 0/1
Batch Loss = 0.0007309753768073356
792, epoch_train_loss=0.0007309753768073356
Epoch 793
Epoch 793 :: Batch 0/1
Batch Loss = 0.0007309753768073356
793, epoch_train_loss=0.0007309753768073356
Epoch 794
Epoch 794 :: Batch 0/1
Batch Loss = 0.0007309753768073356
794, epoch_train_loss=0.0007309753768073356
Epoch 795
Epoch 795 :: Batch 0/1
Batch Loss = 0.0007309753768073356
795, epoch_train_loss=0.0007309753768073356
Epoch 796
Epoch 796 :: Batch 0/1
Batch Loss = 0.0007309753768073356
796, epoch_train_loss=0.0007309753768073356
Epoch 797
Epoch 797 :: Batch 0/1
Batch Loss = 0.0007309753768073356
797, epoch_train_loss=0.0007309753768073356
Epoch 798
Epoch 798 :: Batch 0/1
Batch Loss = 0.0007309753768073356
798, epoch_train_loss=0.0007309753768073356
Epoch 799
Epoch 799 :: Batch 0/1
Batch Loss = 0.0007309753768073356
799, epoch_train_loss=0.0007309753768073356
Epoch 800
Epoch 800 :: Batch 0/1
Batch Loss = 0.0007309753768073356
800, epoch_train_loss=0.0007309753768073356
Epoch 801
Epoch 801 :: Batch 0/1
Batch Loss = 0.0007309753768073356
801, epoch_train_loss=0.0007309753768073356
Epoch 802
Epoch 802 :: Batch 0/1
Batch Loss = 0.0007309753768073356
802, epoch_train_loss=0.0007309753768073356
Epoch 803
Epoch 803 :: Batch 0/1
Batch Loss = 0.0007309753768073356
803, epoch_train_loss=0.0007309753768073356
Epoch 804
Epoch 804 :: Batch 0/1
Batch Loss = 0.0007309753768073356
804, epoch_train_loss=0.0007309753768073356
Epoch 805
Epoch 805 :: Batch 0/1
Batch Loss = 0.0007309753768073356
805, epoch_train_loss=0.0007309753768073356
Epoch 806
Epoch 806 :: Batch 0/1
Batch Loss = 0.0007309753768073356
806, epoch_train_loss=0.0007309753768073356
Epoch 807
Epoch 807 :: Batch 0/1
Batch Loss = 0.0007309753768073356
807, epoch_train_loss=0.0007309753768073356
Epoch 808
Epoch 808 :: Batch 0/1
Batch Loss = 0.0007309753768073356
808, epoch_train_loss=0.0007309753768073356
Epoch 809
Epoch 809 :: Batch 0/1
Batch Loss = 0.0007309753768073356
809, epoch_train_loss=0.0007309753768073356
Epoch 810
Epoch 810 :: Batch 0/1
Batch Loss = 0.0007309753768073356
810, epoch_train_loss=0.0007309753768073356
Epoch 811
Epoch 811 :: Batch 0/1
Batch Loss = 0.0007309753768073356
811, epoch_train_loss=0.0007309753768073356
Epoch 812
Epoch 812 :: Batch 0/1
Batch Loss = 0.0007309753768073356
812, epoch_train_loss=0.0007309753768073356
Epoch 813
Epoch 813 :: Batch 0/1
Batch Loss = 0.0007309753768073356
813, epoch_train_loss=0.0007309753768073356
Epoch 814
Epoch 814 :: Batch 0/1
Batch Loss = 0.0007309753768073356
814, epoch_train_loss=0.0007309753768073356
Epoch 815
Epoch 815 :: Batch 0/1
Batch Loss = 0.0007309753768073356
815, epoch_train_loss=0.0007309753768073356
Epoch 816
Epoch 816 :: Batch 0/1
Batch Loss = 0.0007309753768073356
816, epoch_train_loss=0.0007309753768073356
Epoch 817
Epoch 817 :: Batch 0/1
Batch Loss = 0.0007309753768073356
817, epoch_train_loss=0.0007309753768073356
Epoch 818
Epoch 818 :: Batch 0/1
Batch Loss = 0.0007309753768073356
818, epoch_train_loss=0.0007309753768073356
Epoch 819
Epoch 819 :: Batch 0/1
Batch Loss = 0.0007309753768073356
819, epoch_train_loss=0.0007309753768073356
Epoch 820
Epoch 820 :: Batch 0/1
Batch Loss = 0.0007309753768073356
820, epoch_train_loss=0.0007309753768073356
Epoch 821
Epoch 821 :: Batch 0/1
Batch Loss = 0.0007309753768073356
821, epoch_train_loss=0.0007309753768073356
Epoch 822
Epoch 822 :: Batch 0/1
Batch Loss = 0.0007309753768073356
822, epoch_train_loss=0.0007309753768073356
Epoch 823
Epoch 823 :: Batch 0/1
Batch Loss = 0.0007309753768073356
823, epoch_train_loss=0.0007309753768073356
Epoch 824
Epoch 824 :: Batch 0/1
Batch Loss = 0.0007309753768073356
824, epoch_train_loss=0.0007309753768073356
Epoch 825
Epoch 825 :: Batch 0/1
Batch Loss = 0.0007309753768073356
825, epoch_train_loss=0.0007309753768073356
Epoch 826
Epoch 826 :: Batch 0/1
Batch Loss = 0.0007309753768073356
826, epoch_train_loss=0.0007309753768073356
Epoch 827
Epoch 827 :: Batch 0/1
Batch Loss = 0.0007309753768073356
827, epoch_train_loss=0.0007309753768073356
Epoch 828
Epoch 828 :: Batch 0/1
Batch Loss = 0.0007309753768073356
828, epoch_train_loss=0.0007309753768073356
Epoch 829
Epoch 829 :: Batch 0/1
Batch Loss = 0.0007309753768073356
829, epoch_train_loss=0.0007309753768073356
Epoch 830
Epoch 830 :: Batch 0/1
Batch Loss = 0.0007309753768073356
830, epoch_train_loss=0.0007309753768073356
Epoch 831
Epoch 831 :: Batch 0/1
Batch Loss = 0.0007309753768073356
831, epoch_train_loss=0.0007309753768073356
Epoch 832
Epoch 832 :: Batch 0/1
Batch Loss = 0.0007309753768073356
832, epoch_train_loss=0.0007309753768073356
Epoch 833
Epoch 833 :: Batch 0/1
Batch Loss = 0.0007309753768073356
833, epoch_train_loss=0.0007309753768073356
Epoch 834
Epoch 834 :: Batch 0/1
Batch Loss = 0.0007309753768073356
834, epoch_train_loss=0.0007309753768073356
Epoch 835
Epoch 835 :: Batch 0/1
Batch Loss = 0.0007309753768073356
835, epoch_train_loss=0.0007309753768073356
Epoch 836
Epoch 836 :: Batch 0/1
Batch Loss = 0.0007309753768073356
836, epoch_train_loss=0.0007309753768073356
Epoch 837
Epoch 837 :: Batch 0/1
Batch Loss = 0.0007309753768073356
837, epoch_train_loss=0.0007309753768073356
Epoch 838
Epoch 838 :: Batch 0/1
Batch Loss = 0.0007309753768073356
838, epoch_train_loss=0.0007309753768073356
Epoch 839
Epoch 839 :: Batch 0/1
Batch Loss = 0.0007309753768073356
839, epoch_train_loss=0.0007309753768073356
Epoch 840
Epoch 840 :: Batch 0/1
Batch Loss = 0.0007309753768073356
840, epoch_train_loss=0.0007309753768073356
Epoch 841
Epoch 841 :: Batch 0/1
Batch Loss = 0.0007309753768073356
841, epoch_train_loss=0.0007309753768073356
Epoch 842
Epoch 842 :: Batch 0/1
Batch Loss = 0.0007309753768073356
842, epoch_train_loss=0.0007309753768073356
Epoch 843
Epoch 843 :: Batch 0/1
Batch Loss = 0.0007309753768073356
843, epoch_train_loss=0.0007309753768073356
Epoch 844
Epoch 844 :: Batch 0/1
Batch Loss = 0.0007309753768073356
844, epoch_train_loss=0.0007309753768073356
Epoch 845
Epoch 845 :: Batch 0/1
Batch Loss = 0.0007309753768073356
845, epoch_train_loss=0.0007309753768073356
Epoch 846
Epoch 846 :: Batch 0/1
Batch Loss = 0.0007309753768073356
846, epoch_train_loss=0.0007309753768073356
Epoch 847
Epoch 847 :: Batch 0/1
Batch Loss = 0.0007309753768073356
847, epoch_train_loss=0.0007309753768073356
Epoch 848
Epoch 848 :: Batch 0/1
Batch Loss = 0.0007309753768073356
848, epoch_train_loss=0.0007309753768073356
Epoch 849
Epoch 849 :: Batch 0/1
Batch Loss = 0.0007309753768073356
849, epoch_train_loss=0.0007309753768073356
Epoch 850
Epoch 850 :: Batch 0/1
Batch Loss = 0.0007309753768073356
850, epoch_train_loss=0.0007309753768073356
Epoch 851
Epoch 851 :: Batch 0/1
Batch Loss = 0.0007309753768073356
851, epoch_train_loss=0.0007309753768073356
Epoch 852
Epoch 852 :: Batch 0/1
Batch Loss = 0.0007309753768073356
852, epoch_train_loss=0.0007309753768073356
Epoch 853
Epoch 853 :: Batch 0/1
Batch Loss = 0.0007309753768073356
853, epoch_train_loss=0.0007309753768073356
Epoch 854
Epoch 854 :: Batch 0/1
Batch Loss = 0.0007309753768073356
854, epoch_train_loss=0.0007309753768073356
Epoch 855
Epoch 855 :: Batch 0/1
Batch Loss = 0.0007309753768073356
855, epoch_train_loss=0.0007309753768073356
Epoch 856
Epoch 856 :: Batch 0/1
Batch Loss = 0.0007309753768073356
856, epoch_train_loss=0.0007309753768073356
Epoch 857
Epoch 857 :: Batch 0/1
Batch Loss = 0.0007309753768073356
857, epoch_train_loss=0.0007309753768073356
Epoch 858
Epoch 858 :: Batch 0/1
Batch Loss = 0.0007309753768073356
858, epoch_train_loss=0.0007309753768073356
Epoch 859
Epoch 859 :: Batch 0/1
Batch Loss = 0.0007309753768073356
859, epoch_train_loss=0.0007309753768073356
Epoch 860
Epoch 860 :: Batch 0/1
Batch Loss = 0.0007309753768073356
860, epoch_train_loss=0.0007309753768073356
Epoch 861
Epoch 861 :: Batch 0/1
Batch Loss = 0.0007309753768073356
861, epoch_train_loss=0.0007309753768073356
Epoch 862
Epoch 862 :: Batch 0/1
Batch Loss = 0.0007309753768073356
862, epoch_train_loss=0.0007309753768073356
Epoch 863
Epoch 863 :: Batch 0/1
Batch Loss = 0.0007309753768073356
863, epoch_train_loss=0.0007309753768073356
Epoch 864
Epoch 864 :: Batch 0/1
Batch Loss = 0.0007309753768073356
864, epoch_train_loss=0.0007309753768073356
Epoch 865
Epoch 865 :: Batch 0/1
Batch Loss = 0.0007309753768073356
865, epoch_train_loss=0.0007309753768073356
Epoch 866
Epoch 866 :: Batch 0/1
Batch Loss = 0.0007309753768073356
866, epoch_train_loss=0.0007309753768073356
Epoch 867
Epoch 867 :: Batch 0/1
Batch Loss = 0.0007309753768073356
867, epoch_train_loss=0.0007309753768073356
Epoch 868
Epoch 868 :: Batch 0/1
Batch Loss = 0.0007309753768073356
868, epoch_train_loss=0.0007309753768073356
Epoch 869
Epoch 869 :: Batch 0/1
Batch Loss = 0.0007309753768073356
869, epoch_train_loss=0.0007309753768073356
Epoch 870
Epoch 870 :: Batch 0/1
Batch Loss = 0.0007309753768073356
870, epoch_train_loss=0.0007309753768073356
Epoch 871
Epoch 871 :: Batch 0/1
Batch Loss = 0.0007309753768073356
871, epoch_train_loss=0.0007309753768073356
Epoch 872
Epoch 872 :: Batch 0/1
Batch Loss = 0.0007309753768073356
872, epoch_train_loss=0.0007309753768073356
Epoch 873
Epoch 873 :: Batch 0/1
Batch Loss = 0.0007309753768073356
873, epoch_train_loss=0.0007309753768073356
Epoch 874
Epoch 874 :: Batch 0/1
Batch Loss = 0.0007309753768073356
874, epoch_train_loss=0.0007309753768073356
Epoch 875
Epoch 875 :: Batch 0/1
Batch Loss = 0.0007309753768073356
875, epoch_train_loss=0.0007309753768073356
Epoch 876
Epoch 876 :: Batch 0/1
Batch Loss = 0.0007309753768073356
876, epoch_train_loss=0.0007309753768073356
Epoch 877
Epoch 877 :: Batch 0/1
Batch Loss = 0.0007309753768073356
877, epoch_train_loss=0.0007309753768073356
Epoch 878
Epoch 878 :: Batch 0/1
Batch Loss = 0.0007309753768073356
878, epoch_train_loss=0.0007309753768073356
Epoch 879
Epoch 879 :: Batch 0/1
Batch Loss = 0.0007309753768073356
879, epoch_train_loss=0.0007309753768073356
Epoch 880
Epoch 880 :: Batch 0/1
Batch Loss = 0.0007309753768073356
880, epoch_train_loss=0.0007309753768073356
Epoch 881
Epoch 881 :: Batch 0/1
Batch Loss = 0.0007309753768073356
881, epoch_train_loss=0.0007309753768073356
Epoch 882
Epoch 882 :: Batch 0/1
Batch Loss = 0.0007309753768073356
882, epoch_train_loss=0.0007309753768073356
Epoch 883
Epoch 883 :: Batch 0/1
Batch Loss = 0.0007309753768073356
883, epoch_train_loss=0.0007309753768073356
Epoch 884
Epoch 884 :: Batch 0/1
Batch Loss = 0.0007309753768073356
884, epoch_train_loss=0.0007309753768073356
Epoch 885
Epoch 885 :: Batch 0/1
Batch Loss = 0.0007309753768073356
885, epoch_train_loss=0.0007309753768073356
Epoch 886
Epoch 886 :: Batch 0/1
Batch Loss = 0.0007309753768073356
886, epoch_train_loss=0.0007309753768073356
Epoch 887
Epoch 887 :: Batch 0/1
Batch Loss = 0.0007309753768073356
887, epoch_train_loss=0.0007309753768073356
Epoch 888
Epoch 888 :: Batch 0/1
Batch Loss = 0.0007309753768073356
888, epoch_train_loss=0.0007309753768073356
Epoch 889
Epoch 889 :: Batch 0/1
Batch Loss = 0.0007309753768073356
889, epoch_train_loss=0.0007309753768073356
Epoch 890
Epoch 890 :: Batch 0/1
Batch Loss = 0.0007309753768073356
890, epoch_train_loss=0.0007309753768073356
Epoch 891
Epoch 891 :: Batch 0/1
Batch Loss = 0.0007309753768073356
891, epoch_train_loss=0.0007309753768073356
Epoch 892
Epoch 892 :: Batch 0/1
Batch Loss = 0.0007309753768073356
892, epoch_train_loss=0.0007309753768073356
Epoch 893
Epoch 893 :: Batch 0/1
Batch Loss = 0.0007309753768073356
893, epoch_train_loss=0.0007309753768073356
Epoch 894
Epoch 894 :: Batch 0/1
Batch Loss = 0.0007309753768073356
894, epoch_train_loss=0.0007309753768073356
Epoch 895
Epoch 895 :: Batch 0/1
Batch Loss = 0.0007309753768073356
895, epoch_train_loss=0.0007309753768073356
Epoch 896
Epoch 896 :: Batch 0/1
Batch Loss = 0.0007309753768073356
896, epoch_train_loss=0.0007309753768073356
Epoch 897
Epoch 897 :: Batch 0/1
Batch Loss = 0.0007309753768073356
897, epoch_train_loss=0.0007309753768073356
Epoch 898
Epoch 898 :: Batch 0/1
Batch Loss = 0.0007309753768073356
898, epoch_train_loss=0.0007309753768073356
Epoch 899
Epoch 899 :: Batch 0/1
Batch Loss = 0.0007309753768073356
899, epoch_train_loss=0.0007309753768073356
Epoch 900
Epoch 900 :: Batch 0/1
Batch Loss = 0.0007309753768073356
900, epoch_train_loss=0.0007309753768073356
Epoch 901
Epoch 901 :: Batch 0/1
Batch Loss = 0.0007309753768073356
901, epoch_train_loss=0.0007309753768073356
Epoch 902
Epoch 902 :: Batch 0/1
Batch Loss = 0.0007309753768073356
902, epoch_train_loss=0.0007309753768073356
Epoch 903
Epoch 903 :: Batch 0/1
Batch Loss = 0.0007309753768073356
903, epoch_train_loss=0.0007309753768073356
Epoch 904
Epoch 904 :: Batch 0/1
Batch Loss = 0.0007309753768073356
904, epoch_train_loss=0.0007309753768073356
Epoch 905
Epoch 905 :: Batch 0/1
Batch Loss = 0.0007309753768073356
905, epoch_train_loss=0.0007309753768073356
Epoch 906
Epoch 906 :: Batch 0/1
Batch Loss = 0.0007309753768073356
906, epoch_train_loss=0.0007309753768073356
Epoch 907
Epoch 907 :: Batch 0/1
Batch Loss = 0.0007309753768073356
907, epoch_train_loss=0.0007309753768073356
Epoch 908
Epoch 908 :: Batch 0/1
Batch Loss = 0.0007309753768073356
908, epoch_train_loss=0.0007309753768073356
Epoch 909
Epoch 909 :: Batch 0/1
Batch Loss = 0.0007309753768073356
909, epoch_train_loss=0.0007309753768073356
Epoch 910
Epoch 910 :: Batch 0/1
Batch Loss = 0.0007309753768073356
910, epoch_train_loss=0.0007309753768073356
Epoch 911
Epoch 911 :: Batch 0/1
Batch Loss = 0.0007309753768073356
911, epoch_train_loss=0.0007309753768073356
Epoch 912
Epoch 912 :: Batch 0/1
Batch Loss = 0.0007309753768073356
912, epoch_train_loss=0.0007309753768073356
Epoch 913
Epoch 913 :: Batch 0/1
Batch Loss = 0.0007309753768073356
913, epoch_train_loss=0.0007309753768073356
Epoch 914
Epoch 914 :: Batch 0/1
Batch Loss = 0.0007309753768073356
914, epoch_train_loss=0.0007309753768073356
Epoch 915
Epoch 915 :: Batch 0/1
Batch Loss = 0.0007309753768073356
915, epoch_train_loss=0.0007309753768073356
Epoch 916
Epoch 916 :: Batch 0/1
Batch Loss = 0.0007309753768073356
916, epoch_train_loss=0.0007309753768073356
Epoch 917
Epoch 917 :: Batch 0/1
Batch Loss = 0.0007309753768073356
917, epoch_train_loss=0.0007309753768073356
Epoch 918
Epoch 918 :: Batch 0/1
Batch Loss = 0.0007309753768073356
918, epoch_train_loss=0.0007309753768073356
Epoch 919
Epoch 919 :: Batch 0/1
Batch Loss = 0.0007309753768073356
919, epoch_train_loss=0.0007309753768073356
Epoch 920
Epoch 920 :: Batch 0/1
Batch Loss = 0.0007309753768073356
920, epoch_train_loss=0.0007309753768073356
Epoch 921
Epoch 921 :: Batch 0/1
Batch Loss = 0.0007309753768073356
921, epoch_train_loss=0.0007309753768073356
Epoch 922
Epoch 922 :: Batch 0/1
Batch Loss = 0.0007309753768073356
922, epoch_train_loss=0.0007309753768073356
Epoch 923
Epoch 923 :: Batch 0/1
Batch Loss = 0.0007309753768073356
923, epoch_train_loss=0.0007309753768073356
Epoch 924
Epoch 924 :: Batch 0/1
Batch Loss = 0.0007309753768073356
924, epoch_train_loss=0.0007309753768073356
Epoch 925
Epoch 925 :: Batch 0/1
Batch Loss = 0.0007309753768073356
925, epoch_train_loss=0.0007309753768073356
Epoch 926
Epoch 926 :: Batch 0/1
Batch Loss = 0.0007309753768073356
926, epoch_train_loss=0.0007309753768073356
Epoch 927
Epoch 927 :: Batch 0/1
Batch Loss = 0.0007309753768073356
927, epoch_train_loss=0.0007309753768073356
Epoch 928
Epoch 928 :: Batch 0/1
Batch Loss = 0.0007309753768073356
928, epoch_train_loss=0.0007309753768073356
Epoch 929
Epoch 929 :: Batch 0/1
Batch Loss = 0.0007309753768073356
929, epoch_train_loss=0.0007309753768073356
Epoch 930
Epoch 930 :: Batch 0/1
Batch Loss = 0.0007309753768073356
930, epoch_train_loss=0.0007309753768073356
Epoch 931
Epoch 931 :: Batch 0/1
Batch Loss = 0.0007309753768073356
931, epoch_train_loss=0.0007309753768073356
Epoch 932
Epoch 932 :: Batch 0/1
Batch Loss = 0.0007309753768073356
932, epoch_train_loss=0.0007309753768073356
Epoch 933
Epoch 933 :: Batch 0/1
Batch Loss = 0.0007309753768073356
933, epoch_train_loss=0.0007309753768073356
Epoch 934
Epoch 934 :: Batch 0/1
Batch Loss = 0.0007309753768073356
934, epoch_train_loss=0.0007309753768073356
Epoch 935
Epoch 935 :: Batch 0/1
Batch Loss = 0.0007309753768073356
935, epoch_train_loss=0.0007309753768073356
Epoch 936
Epoch 936 :: Batch 0/1
Batch Loss = 0.0007309753768073356
936, epoch_train_loss=0.0007309753768073356
Epoch 937
Epoch 937 :: Batch 0/1
Batch Loss = 0.0007309753768073356
937, epoch_train_loss=0.0007309753768073356
Epoch 938
Epoch 938 :: Batch 0/1
Batch Loss = 0.0007309753768073356
938, epoch_train_loss=0.0007309753768073356
Epoch 939
Epoch 939 :: Batch 0/1
Batch Loss = 0.0007309753768073356
939, epoch_train_loss=0.0007309753768073356
Epoch 940
Epoch 940 :: Batch 0/1
Batch Loss = 0.0007309753768073356
940, epoch_train_loss=0.0007309753768073356
Epoch 941
Epoch 941 :: Batch 0/1
Batch Loss = 0.0007309753768073356
941, epoch_train_loss=0.0007309753768073356
Epoch 942
Epoch 942 :: Batch 0/1
Batch Loss = 0.0007309753768073356
942, epoch_train_loss=0.0007309753768073356
Epoch 943
Epoch 943 :: Batch 0/1
Batch Loss = 0.0007309753768073356
943, epoch_train_loss=0.0007309753768073356
Epoch 944
Epoch 944 :: Batch 0/1
Batch Loss = 0.0007309753768073356
944, epoch_train_loss=0.0007309753768073356
Epoch 945
Epoch 945 :: Batch 0/1
Batch Loss = 0.0007309753768073356
945, epoch_train_loss=0.0007309753768073356
Epoch 946
Epoch 946 :: Batch 0/1
Batch Loss = 0.0007309753768073356
946, epoch_train_loss=0.0007309753768073356
Epoch 947
Epoch 947 :: Batch 0/1
Batch Loss = 0.0007309753768073356
947, epoch_train_loss=0.0007309753768073356
Epoch 948
Epoch 948 :: Batch 0/1
Batch Loss = 0.0007309753768073356
948, epoch_train_loss=0.0007309753768073356
Epoch 949
Epoch 949 :: Batch 0/1
Batch Loss = 0.0007309753768073356
949, epoch_train_loss=0.0007309753768073356
Epoch 950
Epoch 950 :: Batch 0/1
Batch Loss = 0.0007309753768073356
950, epoch_train_loss=0.0007309753768073356
Epoch 951
Epoch 951 :: Batch 0/1
Batch Loss = 0.0007309753768073356
951, epoch_train_loss=0.0007309753768073356
Epoch 952
Epoch 952 :: Batch 0/1
Batch Loss = 0.0007309753768073356
952, epoch_train_loss=0.0007309753768073356
Epoch 953
Epoch 953 :: Batch 0/1
Batch Loss = 0.0007309753768073356
953, epoch_train_loss=0.0007309753768073356
Epoch 954
Epoch 954 :: Batch 0/1
Batch Loss = 0.0007309753768073356
954, epoch_train_loss=0.0007309753768073356
Epoch 955
Epoch 955 :: Batch 0/1
Batch Loss = 0.0007309753768073356
955, epoch_train_loss=0.0007309753768073356
Epoch 956
Epoch 956 :: Batch 0/1
Batch Loss = 0.0007309753768073356
956, epoch_train_loss=0.0007309753768073356
Epoch 957
Epoch 957 :: Batch 0/1
Batch Loss = 0.0007309753768073356
957, epoch_train_loss=0.0007309753768073356
Epoch 958
Epoch 958 :: Batch 0/1
Batch Loss = 0.0007309753768073356
958, epoch_train_loss=0.0007309753768073356
Epoch 959
Epoch 959 :: Batch 0/1
Batch Loss = 0.0007309753768073356
959, epoch_train_loss=0.0007309753768073356
Epoch 960
Epoch 960 :: Batch 0/1
Batch Loss = 0.0007309753768073356
960, epoch_train_loss=0.0007309753768073356
Epoch 961
Epoch 961 :: Batch 0/1
Batch Loss = 0.0007309753768073356
961, epoch_train_loss=0.0007309753768073356
Epoch 962
Epoch 962 :: Batch 0/1
Batch Loss = 0.0007309753768073356
962, epoch_train_loss=0.0007309753768073356
Epoch 963
Epoch 963 :: Batch 0/1
Batch Loss = 0.0007309753768073356
963, epoch_train_loss=0.0007309753768073356
Epoch 964
Epoch 964 :: Batch 0/1
Batch Loss = 0.0007309753768073356
964, epoch_train_loss=0.0007309753768073356
Epoch 965
Epoch 965 :: Batch 0/1
Batch Loss = 0.0007309753768073356
965, epoch_train_loss=0.0007309753768073356
Epoch 966
Epoch 966 :: Batch 0/1
Batch Loss = 0.0007309753768073356
966, epoch_train_loss=0.0007309753768073356
Epoch 967
Epoch 967 :: Batch 0/1
Batch Loss = 0.0007309753768073356
967, epoch_train_loss=0.0007309753768073356
Epoch 968
Epoch 968 :: Batch 0/1
Batch Loss = 0.0007309753768073356
968, epoch_train_loss=0.0007309753768073356
Epoch 969
Epoch 969 :: Batch 0/1
Batch Loss = 0.0007309753768073356
969, epoch_train_loss=0.0007309753768073356
Epoch 970
Epoch 970 :: Batch 0/1
Batch Loss = 0.0007309753768073356
970, epoch_train_loss=0.0007309753768073356
Epoch 971
Epoch 971 :: Batch 0/1
Batch Loss = 0.0007309753768073356
971, epoch_train_loss=0.0007309753768073356
Epoch 972
Epoch 972 :: Batch 0/1
Batch Loss = 0.0007309753768073356
972, epoch_train_loss=0.0007309753768073356
Epoch 973
Epoch 973 :: Batch 0/1
Batch Loss = 0.0007309753768073356
973, epoch_train_loss=0.0007309753768073356
Epoch 974
Epoch 974 :: Batch 0/1
Batch Loss = 0.0007309753768073356
974, epoch_train_loss=0.0007309753768073356
Epoch 975
Epoch 975 :: Batch 0/1
Batch Loss = 0.0007309753768073356
975, epoch_train_loss=0.0007309753768073356
Epoch 976
Epoch 976 :: Batch 0/1
Batch Loss = 0.0007309753768073356
976, epoch_train_loss=0.0007309753768073356
Epoch 977
Epoch 977 :: Batch 0/1
Batch Loss = 0.0007309753768073356
977, epoch_train_loss=0.0007309753768073356
Epoch 978
Epoch 978 :: Batch 0/1
Batch Loss = 0.0007309753768073356
978, epoch_train_loss=0.0007309753768073356
Epoch 979
Epoch 979 :: Batch 0/1
Batch Loss = 0.0007309753768073356
979, epoch_train_loss=0.0007309753768073356
Epoch 980
Epoch 980 :: Batch 0/1
Batch Loss = 0.0007309753768073356
980, epoch_train_loss=0.0007309753768073356
Epoch 981
Epoch 981 :: Batch 0/1
Batch Loss = 0.0007309753768073356
981, epoch_train_loss=0.0007309753768073356
Epoch 982
Epoch 982 :: Batch 0/1
Batch Loss = 0.0007309753768073356
982, epoch_train_loss=0.0007309753768073356
Epoch 983
Epoch 983 :: Batch 0/1
Batch Loss = 0.0007309753768073356
983, epoch_train_loss=0.0007309753768073356
Epoch 984
Epoch 984 :: Batch 0/1
Batch Loss = 0.0007309753768073356
984, epoch_train_loss=0.0007309753768073356
Epoch 985
Epoch 985 :: Batch 0/1
Batch Loss = 0.0007309753768073356
985, epoch_train_loss=0.0007309753768073356
Epoch 986
Epoch 986 :: Batch 0/1
Batch Loss = 0.0007309753768073356
986, epoch_train_loss=0.0007309753768073356
Epoch 987
Epoch 987 :: Batch 0/1
Batch Loss = 0.0007309753768073356
987, epoch_train_loss=0.0007309753768073356
Epoch 988
Epoch 988 :: Batch 0/1
Batch Loss = 0.0007309753768073356
988, epoch_train_loss=0.0007309753768073356
Epoch 989
Epoch 989 :: Batch 0/1
Batch Loss = 0.0007309753768073356
989, epoch_train_loss=0.0007309753768073356
Epoch 990
Epoch 990 :: Batch 0/1
Batch Loss = 0.0007309753768073356
990, epoch_train_loss=0.0007309753768073356
Epoch 991
Epoch 991 :: Batch 0/1
Batch Loss = 0.0007309753768073356
991, epoch_train_loss=0.0007309753768073356
Epoch 992
Epoch 992 :: Batch 0/1
Batch Loss = 0.0007309753768073356
992, epoch_train_loss=0.0007309753768073356
Epoch 993
Epoch 993 :: Batch 0/1
Batch Loss = 0.0007309753768073356
993, epoch_train_loss=0.0007309753768073356
Epoch 994
Epoch 994 :: Batch 0/1
Batch Loss = 0.0007309753768073356
994, epoch_train_loss=0.0007309753768073356
Epoch 995
Epoch 995 :: Batch 0/1
Batch Loss = 0.0007309753768073356
995, epoch_train_loss=0.0007309753768073356
Epoch 996
Epoch 996 :: Batch 0/1
Batch Loss = 0.0007309753768073356
996, epoch_train_loss=0.0007309753768073356
Epoch 997
Epoch 997 :: Batch 0/1
Batch Loss = 0.0007309753768073356
997, epoch_train_loss=0.0007309753768073356
Epoch 998
Epoch 998 :: Batch 0/1
Batch Loss = 0.0007309753768073356
998, epoch_train_loss=0.0007309753768073356
Epoch 999
Epoch 999 :: Batch 0/1
Batch Loss = 0.0007309753768073356
999, epoch_train_loss=0.0007309753768073356
Epoch 1000
Epoch 1000 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1000, epoch_train_loss=0.0007309753768073356
Epoch 1001
Epoch 1001 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1001, epoch_train_loss=0.0007309753768073356
Epoch 1002
Epoch 1002 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1002, epoch_train_loss=0.0007309753768073356
Epoch 1003
Epoch 1003 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1003, epoch_train_loss=0.0007309753768073356
Epoch 1004
Epoch 1004 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1004, epoch_train_loss=0.0007309753768073356
Epoch 1005
Epoch 1005 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1005, epoch_train_loss=0.0007309753768073356
Epoch 1006
Epoch 1006 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1006, epoch_train_loss=0.0007309753768073356
Epoch 1007
Epoch 1007 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1007, epoch_train_loss=0.0007309753768073356
Epoch 1008
Epoch 1008 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1008, epoch_train_loss=0.0007309753768073356
Epoch 1009
Epoch 1009 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1009, epoch_train_loss=0.0007309753768073356
Epoch 1010
Epoch 1010 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1010, epoch_train_loss=0.0007309753768073356
Epoch 1011
Epoch 1011 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1011, epoch_train_loss=0.0007309753768073356
Epoch 1012
Epoch 1012 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1012, epoch_train_loss=0.0007309753768073356
Epoch 1013
Epoch 1013 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1013, epoch_train_loss=0.0007309753768073356
Epoch 1014
Epoch 1014 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1014, epoch_train_loss=0.0007309753768073356
Epoch 1015
Epoch 1015 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1015, epoch_train_loss=0.0007309753768073356
Epoch 1016
Epoch 1016 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1016, epoch_train_loss=0.0007309753768073356
Epoch 1017
Epoch 1017 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1017, epoch_train_loss=0.0007309753768073356
Epoch 1018
Epoch 1018 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1018, epoch_train_loss=0.0007309753768073356
Epoch 1019
Epoch 1019 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1019, epoch_train_loss=0.0007309753768073356
Epoch 1020
Epoch 1020 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1020, epoch_train_loss=0.0007309753768073356
Epoch 1021
Epoch 1021 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1021, epoch_train_loss=0.0007309753768073356
Epoch 1022
Epoch 1022 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1022, epoch_train_loss=0.0007309753768073356
Epoch 1023
Epoch 1023 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1023, epoch_train_loss=0.0007309753768073356
Epoch 1024
Epoch 1024 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1024, epoch_train_loss=0.0007309753768073356
Epoch 1025
Epoch 1025 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1025, epoch_train_loss=0.0007309753768073356
Epoch 1026
Epoch 1026 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1026, epoch_train_loss=0.0007309753768073356
Epoch 1027
Epoch 1027 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1027, epoch_train_loss=0.0007309753768073356
Epoch 1028
Epoch 1028 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1028, epoch_train_loss=0.0007309753768073356
Epoch 1029
Epoch 1029 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1029, epoch_train_loss=0.0007309753768073356
Epoch 1030
Epoch 1030 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1030, epoch_train_loss=0.0007309753768073356
Epoch 1031
Epoch 1031 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1031, epoch_train_loss=0.0007309753768073356
Epoch 1032
Epoch 1032 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1032, epoch_train_loss=0.0007309753768073356
Epoch 1033
Epoch 1033 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1033, epoch_train_loss=0.0007309753768073356
Epoch 1034
Epoch 1034 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1034, epoch_train_loss=0.0007309753768073356
Epoch 1035
Epoch 1035 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1035, epoch_train_loss=0.0007309753768073356
Epoch 1036
Epoch 1036 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1036, epoch_train_loss=0.0007309753768073356
Epoch 1037
Epoch 1037 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1037, epoch_train_loss=0.0007309753768073356
Epoch 1038
Epoch 1038 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1038, epoch_train_loss=0.0007309753768073356
Epoch 1039
Epoch 1039 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1039, epoch_train_loss=0.0007309753768073356
Epoch 1040
Epoch 1040 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1040, epoch_train_loss=0.0007309753768073356
Epoch 1041
Epoch 1041 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1041, epoch_train_loss=0.0007309753768073356
Epoch 1042
Epoch 1042 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1042, epoch_train_loss=0.0007309753768073356
Epoch 1043
Epoch 1043 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1043, epoch_train_loss=0.0007309753768073356
Epoch 1044
Epoch 1044 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1044, epoch_train_loss=0.0007309753768073356
Epoch 1045
Epoch 1045 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1045, epoch_train_loss=0.0007309753768073356
Epoch 1046
Epoch 1046 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1046, epoch_train_loss=0.0007309753768073356
Epoch 1047
Epoch 1047 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1047, epoch_train_loss=0.0007309753768073356
Epoch 1048
Epoch 1048 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1048, epoch_train_loss=0.0007309753768073356
Epoch 1049
Epoch 1049 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1049, epoch_train_loss=0.0007309753768073356
Epoch 1050
Epoch 1050 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1050, epoch_train_loss=0.0007309753768073356
Epoch 1051
Epoch 1051 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1051, epoch_train_loss=0.0007309753768073356
Epoch 1052
Epoch 1052 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1052, epoch_train_loss=0.0007309753768073356
Epoch 1053
Epoch 1053 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1053, epoch_train_loss=0.0007309753768073356
Epoch 1054
Epoch 1054 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1054, epoch_train_loss=0.0007309753768073356
Epoch 1055
Epoch 1055 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1055, epoch_train_loss=0.0007309753768073356
Epoch 1056
Epoch 1056 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1056, epoch_train_loss=0.0007309753768073356
Epoch 1057
Epoch 1057 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1057, epoch_train_loss=0.0007309753768073356
Epoch 1058
Epoch 1058 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1058, epoch_train_loss=0.0007309753768073356
Epoch 1059
Epoch 1059 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1059, epoch_train_loss=0.0007309753768073356
Epoch 1060
Epoch 1060 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1060, epoch_train_loss=0.0007309753768073356
Epoch 1061
Epoch 1061 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1061, epoch_train_loss=0.0007309753768073356
Epoch 1062
Epoch 1062 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1062, epoch_train_loss=0.0007309753768073356
Epoch 1063
Epoch 1063 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1063, epoch_train_loss=0.0007309753768073356
Epoch 1064
Epoch 1064 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1064, epoch_train_loss=0.0007309753768073356
Epoch 1065
Epoch 1065 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1065, epoch_train_loss=0.0007309753768073356
Epoch 1066
Epoch 1066 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1066, epoch_train_loss=0.0007309753768073356
Epoch 1067
Epoch 1067 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1067, epoch_train_loss=0.0007309753768073356
Epoch 1068
Epoch 1068 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1068, epoch_train_loss=0.0007309753768073356
Epoch 1069
Epoch 1069 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1069, epoch_train_loss=0.0007309753768073356
Epoch 1070
Epoch 1070 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1070, epoch_train_loss=0.0007309753768073356
Epoch 1071
Epoch 1071 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1071, epoch_train_loss=0.0007309753768073356
Epoch 1072
Epoch 1072 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1072, epoch_train_loss=0.0007309753768073356
Epoch 1073
Epoch 1073 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1073, epoch_train_loss=0.0007309753768073356
Epoch 1074
Epoch 1074 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1074, epoch_train_loss=0.0007309753768073356
Epoch 1075
Epoch 1075 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1075, epoch_train_loss=0.0007309753768073356
Epoch 1076
Epoch 1076 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1076, epoch_train_loss=0.0007309753768073356
Epoch 1077
Epoch 1077 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1077, epoch_train_loss=0.0007309753768073356
Epoch 1078
Epoch 1078 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1078, epoch_train_loss=0.0007309753768073356
Epoch 1079
Epoch 1079 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1079, epoch_train_loss=0.0007309753768073356
Epoch 1080
Epoch 1080 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1080, epoch_train_loss=0.0007309753768073356
Epoch 1081
Epoch 1081 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1081, epoch_train_loss=0.0007309753768073356
Epoch 1082
Epoch 1082 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1082, epoch_train_loss=0.0007309753768073356
Epoch 1083
Epoch 1083 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1083, epoch_train_loss=0.0007309753768073356
Epoch 1084
Epoch 1084 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1084, epoch_train_loss=0.0007309753768073356
Epoch 1085
Epoch 1085 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1085, epoch_train_loss=0.0007309753768073356
Epoch 1086
Epoch 1086 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1086, epoch_train_loss=0.0007309753768073356
Epoch 1087
Epoch 1087 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1087, epoch_train_loss=0.0007309753768073356
Epoch 1088
Epoch 1088 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1088, epoch_train_loss=0.0007309753768073356
Epoch 1089
Epoch 1089 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1089, epoch_train_loss=0.0007309753768073356
Epoch 1090
Epoch 1090 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1090, epoch_train_loss=0.0007309753768073356
Epoch 1091
Epoch 1091 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1091, epoch_train_loss=0.0007309753768073356
Epoch 1092
Epoch 1092 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1092, epoch_train_loss=0.0007309753768073356
Epoch 1093
Epoch 1093 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1093, epoch_train_loss=0.0007309753768073356
Epoch 1094
Epoch 1094 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1094, epoch_train_loss=0.0007309753768073356
Epoch 1095
Epoch 1095 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1095, epoch_train_loss=0.0007309753768073356
Epoch 1096
Epoch 1096 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1096, epoch_train_loss=0.0007309753768073356
Epoch 1097
Epoch 1097 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1097, epoch_train_loss=0.0007309753768073356
Epoch 1098
Epoch 1098 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1098, epoch_train_loss=0.0007309753768073356
Epoch 1099
Epoch 1099 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1099, epoch_train_loss=0.0007309753768073356
Epoch 1100
Epoch 1100 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1100, epoch_train_loss=0.0007309753768073356
Epoch 1101
Epoch 1101 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1101, epoch_train_loss=0.0007309753768073356
Epoch 1102
Epoch 1102 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1102, epoch_train_loss=0.0007309753768073356
Epoch 1103
Epoch 1103 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1103, epoch_train_loss=0.0007309753768073356
Epoch 1104
Epoch 1104 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1104, epoch_train_loss=0.0007309753768073356
Epoch 1105
Epoch 1105 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1105, epoch_train_loss=0.0007309753768073356
Epoch 1106
Epoch 1106 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1106, epoch_train_loss=0.0007309753768073356
Epoch 1107
Epoch 1107 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1107, epoch_train_loss=0.0007309753768073356
Epoch 1108
Epoch 1108 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1108, epoch_train_loss=0.0007309753768073356
Epoch 1109
Epoch 1109 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1109, epoch_train_loss=0.0007309753768073356
Epoch 1110
Epoch 1110 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1110, epoch_train_loss=0.0007309753768073356
Epoch 1111
Epoch 1111 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1111, epoch_train_loss=0.0007309753768073356
Epoch 1112
Epoch 1112 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1112, epoch_train_loss=0.0007309753768073356
Epoch 1113
Epoch 1113 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1113, epoch_train_loss=0.0007309753768073356
Epoch 1114
Epoch 1114 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1114, epoch_train_loss=0.0007309753768073356
Epoch 1115
Epoch 1115 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1115, epoch_train_loss=0.0007309753768073356
Epoch 1116
Epoch 1116 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1116, epoch_train_loss=0.0007309753768073356
Epoch 1117
Epoch 1117 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1117, epoch_train_loss=0.0007309753768073356
Epoch 1118
Epoch 1118 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1118, epoch_train_loss=0.0007309753768073356
Epoch 1119
Epoch 1119 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1119, epoch_train_loss=0.0007309753768073356
Epoch 1120
Epoch 1120 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1120, epoch_train_loss=0.0007309753768073356
Epoch 1121
Epoch 1121 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1121, epoch_train_loss=0.0007309753768073356
Epoch 1122
Epoch 1122 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1122, epoch_train_loss=0.0007309753768073356
Epoch 1123
Epoch 1123 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1123, epoch_train_loss=0.0007309753768073356
Epoch 1124
Epoch 1124 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1124, epoch_train_loss=0.0007309753768073356
Epoch 1125
Epoch 1125 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1125, epoch_train_loss=0.0007309753768073356
Epoch 1126
Epoch 1126 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1126, epoch_train_loss=0.0007309753768073356
Epoch 1127
Epoch 1127 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1127, epoch_train_loss=0.0007309753768073356
Epoch 1128
Epoch 1128 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1128, epoch_train_loss=0.0007309753768073356
Epoch 1129
Epoch 1129 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1129, epoch_train_loss=0.0007309753768073356
Epoch 1130
Epoch 1130 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1130, epoch_train_loss=0.0007309753768073356
Epoch 1131
Epoch 1131 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1131, epoch_train_loss=0.0007309753768073356
Epoch 1132
Epoch 1132 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1132, epoch_train_loss=0.0007309753768073356
Epoch 1133
Epoch 1133 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1133, epoch_train_loss=0.0007309753768073356
Epoch 1134
Epoch 1134 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1134, epoch_train_loss=0.0007309753768073356
Epoch 1135
Epoch 1135 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1135, epoch_train_loss=0.0007309753768073356
Epoch 1136
Epoch 1136 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1136, epoch_train_loss=0.0007309753768073356
Epoch 1137
Epoch 1137 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1137, epoch_train_loss=0.0007309753768073356
Epoch 1138
Epoch 1138 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1138, epoch_train_loss=0.0007309753768073356
Epoch 1139
Epoch 1139 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1139, epoch_train_loss=0.0007309753768073356
Epoch 1140
Epoch 1140 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1140, epoch_train_loss=0.0007309753768073356
Epoch 1141
Epoch 1141 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1141, epoch_train_loss=0.0007309753768073356
Epoch 1142
Epoch 1142 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1142, epoch_train_loss=0.0007309753768073356
Epoch 1143
Epoch 1143 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1143, epoch_train_loss=0.0007309753768073356
Epoch 1144
Epoch 1144 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1144, epoch_train_loss=0.0007309753768073356
Epoch 1145
Epoch 1145 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1145, epoch_train_loss=0.0007309753768073356
Epoch 1146
Epoch 1146 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1146, epoch_train_loss=0.0007309753768073356
Epoch 1147
Epoch 1147 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1147, epoch_train_loss=0.0007309753768073356
Epoch 1148
Epoch 1148 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1148, epoch_train_loss=0.0007309753768073356
Epoch 1149
Epoch 1149 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1149, epoch_train_loss=0.0007309753768073356
Epoch 1150
Epoch 1150 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1150, epoch_train_loss=0.0007309753768073356
Epoch 1151
Epoch 1151 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1151, epoch_train_loss=0.0007309753768073356
Epoch 1152
Epoch 1152 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1152, epoch_train_loss=0.0007309753768073356
Epoch 1153
Epoch 1153 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1153, epoch_train_loss=0.0007309753768073356
Epoch 1154
Epoch 1154 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1154, epoch_train_loss=0.0007309753768073356
Epoch 1155
Epoch 1155 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1155, epoch_train_loss=0.0007309753768073356
Epoch 1156
Epoch 1156 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1156, epoch_train_loss=0.0007309753768073356
Epoch 1157
Epoch 1157 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1157, epoch_train_loss=0.0007309753768073356
Epoch 1158
Epoch 1158 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1158, epoch_train_loss=0.0007309753768073356
Epoch 1159
Epoch 1159 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1159, epoch_train_loss=0.0007309753768073356
Epoch 1160
Epoch 1160 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1160, epoch_train_loss=0.0007309753768073356
Epoch 1161
Epoch 1161 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1161, epoch_train_loss=0.0007309753768073356
Epoch 1162
Epoch 1162 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1162, epoch_train_loss=0.0007309753768073356
Epoch 1163
Epoch 1163 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1163, epoch_train_loss=0.0007309753768073356
Epoch 1164
Epoch 1164 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1164, epoch_train_loss=0.0007309753768073356
Epoch 1165
Epoch 1165 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1165, epoch_train_loss=0.0007309753768073356
Epoch 1166
Epoch 1166 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1166, epoch_train_loss=0.0007309753768073356
Epoch 1167
Epoch 1167 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1167, epoch_train_loss=0.0007309753768073356
Epoch 1168
Epoch 1168 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1168, epoch_train_loss=0.0007309753768073356
Epoch 1169
Epoch 1169 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1169, epoch_train_loss=0.0007309753768073356
Epoch 1170
Epoch 1170 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1170, epoch_train_loss=0.0007309753768073356
Epoch 1171
Epoch 1171 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1171, epoch_train_loss=0.0007309753768073356
Epoch 1172
Epoch 1172 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1172, epoch_train_loss=0.0007309753768073356
Epoch 1173
Epoch 1173 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1173, epoch_train_loss=0.0007309753768073356
Epoch 1174
Epoch 1174 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1174, epoch_train_loss=0.0007309753768073356
Epoch 1175
Epoch 1175 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1175, epoch_train_loss=0.0007309753768073356
Epoch 1176
Epoch 1176 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1176, epoch_train_loss=0.0007309753768073356
Epoch 1177
Epoch 1177 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1177, epoch_train_loss=0.0007309753768073356
Epoch 1178
Epoch 1178 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1178, epoch_train_loss=0.0007309753768073356
Epoch 1179
Epoch 1179 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1179, epoch_train_loss=0.0007309753768073356
Epoch 1180
Epoch 1180 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1180, epoch_train_loss=0.0007309753768073356
Epoch 1181
Epoch 1181 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1181, epoch_train_loss=0.0007309753768073356
Epoch 1182
Epoch 1182 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1182, epoch_train_loss=0.0007309753768073356
Epoch 1183
Epoch 1183 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1183, epoch_train_loss=0.0007309753768073356
Epoch 1184
Epoch 1184 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1184, epoch_train_loss=0.0007309753768073356
Epoch 1185
Epoch 1185 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1185, epoch_train_loss=0.0007309753768073356
Epoch 1186
Epoch 1186 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1186, epoch_train_loss=0.0007309753768073356
Epoch 1187
Epoch 1187 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1187, epoch_train_loss=0.0007309753768073356
Epoch 1188
Epoch 1188 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1188, epoch_train_loss=0.0007309753768073356
Epoch 1189
Epoch 1189 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1189, epoch_train_loss=0.0007309753768073356
Epoch 1190
Epoch 1190 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1190, epoch_train_loss=0.0007309753768073356
Epoch 1191
Epoch 1191 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1191, epoch_train_loss=0.0007309753768073356
Epoch 1192
Epoch 1192 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1192, epoch_train_loss=0.0007309753768073356
Epoch 1193
Epoch 1193 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1193, epoch_train_loss=0.0007309753768073356
Epoch 1194
Epoch 1194 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1194, epoch_train_loss=0.0007309753768073356
Epoch 1195
Epoch 1195 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1195, epoch_train_loss=0.0007309753768073356
Epoch 1196
Epoch 1196 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1196, epoch_train_loss=0.0007309753768073356
Epoch 1197
Epoch 1197 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1197, epoch_train_loss=0.0007309753768073356
Epoch 1198
Epoch 1198 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1198, epoch_train_loss=0.0007309753768073356
Epoch 1199
Epoch 1199 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1199, epoch_train_loss=0.0007309753768073356
Epoch 1200
Epoch 1200 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1200, epoch_train_loss=0.0007309753768073356
Epoch 1201
Epoch 1201 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1201, epoch_train_loss=0.0007309753768073356
Epoch 1202
Epoch 1202 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1202, epoch_train_loss=0.0007309753768073356
Epoch 1203
Epoch 1203 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1203, epoch_train_loss=0.0007309753768073356
Epoch 1204
Epoch 1204 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1204, epoch_train_loss=0.0007309753768073356
Epoch 1205
Epoch 1205 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1205, epoch_train_loss=0.0007309753768073356
Epoch 1206
Epoch 1206 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1206, epoch_train_loss=0.0007309753768073356
Epoch 1207
Epoch 1207 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1207, epoch_train_loss=0.0007309753768073356
Epoch 1208
Epoch 1208 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1208, epoch_train_loss=0.0007309753768073356
Epoch 1209
Epoch 1209 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1209, epoch_train_loss=0.0007309753768073356
Epoch 1210
Epoch 1210 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1210, epoch_train_loss=0.0007309753768073356
Epoch 1211
Epoch 1211 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1211, epoch_train_loss=0.0007309753768073356
Epoch 1212
Epoch 1212 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1212, epoch_train_loss=0.0007309753768073356
Epoch 1213
Epoch 1213 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1213, epoch_train_loss=0.0007309753768073356
Epoch 1214
Epoch 1214 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1214, epoch_train_loss=0.0007309753768073356
Epoch 1215
Epoch 1215 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1215, epoch_train_loss=0.0007309753768073356
Epoch 1216
Epoch 1216 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1216, epoch_train_loss=0.0007309753768073356
Epoch 1217
Epoch 1217 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1217, epoch_train_loss=0.0007309753768073356
Epoch 1218
Epoch 1218 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1218, epoch_train_loss=0.0007309753768073356
Epoch 1219
Epoch 1219 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1219, epoch_train_loss=0.0007309753768073356
Epoch 1220
Epoch 1220 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1220, epoch_train_loss=0.0007309753768073356
Epoch 1221
Epoch 1221 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1221, epoch_train_loss=0.0007309753768073356
Epoch 1222
Epoch 1222 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1222, epoch_train_loss=0.0007309753768073356
Epoch 1223
Epoch 1223 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1223, epoch_train_loss=0.0007309753768073356
Epoch 1224
Epoch 1224 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1224, epoch_train_loss=0.0007309753768073356
Epoch 1225
Epoch 1225 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1225, epoch_train_loss=0.0007309753768073356
Epoch 1226
Epoch 1226 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1226, epoch_train_loss=0.0007309753768073356
Epoch 1227
Epoch 1227 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1227, epoch_train_loss=0.0007309753768073356
Epoch 1228
Epoch 1228 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1228, epoch_train_loss=0.0007309753768073356
Epoch 1229
Epoch 1229 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1229, epoch_train_loss=0.0007309753768073356
Epoch 1230
Epoch 1230 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1230, epoch_train_loss=0.0007309753768073356
Epoch 1231
Epoch 1231 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1231, epoch_train_loss=0.0007309753768073356
Epoch 1232
Epoch 1232 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1232, epoch_train_loss=0.0007309753768073356
Epoch 1233
Epoch 1233 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1233, epoch_train_loss=0.0007309753768073356
Epoch 1234
Epoch 1234 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1234, epoch_train_loss=0.0007309753768073356
Epoch 1235
Epoch 1235 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1235, epoch_train_loss=0.0007309753768073356
Epoch 1236
Epoch 1236 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1236, epoch_train_loss=0.0007309753768073356
Epoch 1237
Epoch 1237 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1237, epoch_train_loss=0.0007309753768073356
Epoch 1238
Epoch 1238 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1238, epoch_train_loss=0.0007309753768073356
Epoch 1239
Epoch 1239 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1239, epoch_train_loss=0.0007309753768073356
Epoch 1240
Epoch 1240 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1240, epoch_train_loss=0.0007309753768073356
Epoch 1241
Epoch 1241 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1241, epoch_train_loss=0.0007309753768073356
Epoch 1242
Epoch 1242 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1242, epoch_train_loss=0.0007309753768073356
Epoch 1243
Epoch 1243 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1243, epoch_train_loss=0.0007309753768073356
Epoch 1244
Epoch 1244 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1244, epoch_train_loss=0.0007309753768073356
Epoch 1245
Epoch 1245 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1245, epoch_train_loss=0.0007309753768073356
Epoch 1246
Epoch 1246 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1246, epoch_train_loss=0.0007309753768073356
Epoch 1247
Epoch 1247 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1247, epoch_train_loss=0.0007309753768073356
Epoch 1248
Epoch 1248 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1248, epoch_train_loss=0.0007309753768073356
Epoch 1249
Epoch 1249 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1249, epoch_train_loss=0.0007309753768073356
Epoch 1250
Epoch 1250 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1250, epoch_train_loss=0.0007309753768073356
Epoch 1251
Epoch 1251 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1251, epoch_train_loss=0.0007309753768073356
Epoch 1252
Epoch 1252 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1252, epoch_train_loss=0.0007309753768073356
Epoch 1253
Epoch 1253 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1253, epoch_train_loss=0.0007309753768073356
Epoch 1254
Epoch 1254 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1254, epoch_train_loss=0.0007309753768073356
Epoch 1255
Epoch 1255 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1255, epoch_train_loss=0.0007309753768073356
Epoch 1256
Epoch 1256 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1256, epoch_train_loss=0.0007309753768073356
Epoch 1257
Epoch 1257 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1257, epoch_train_loss=0.0007309753768073356
Epoch 1258
Epoch 1258 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1258, epoch_train_loss=0.0007309753768073356
Epoch 1259
Epoch 1259 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1259, epoch_train_loss=0.0007309753768073356
Epoch 1260
Epoch 1260 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1260, epoch_train_loss=0.0007309753768073356
Epoch 1261
Epoch 1261 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1261, epoch_train_loss=0.0007309753768073356
Epoch 1262
Epoch 1262 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1262, epoch_train_loss=0.0007309753768073356
Epoch 1263
Epoch 1263 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1263, epoch_train_loss=0.0007309753768073356
Epoch 1264
Epoch 1264 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1264, epoch_train_loss=0.0007309753768073356
Epoch 1265
Epoch 1265 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1265, epoch_train_loss=0.0007309753768073356
Epoch 1266
Epoch 1266 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1266, epoch_train_loss=0.0007309753768073356
Epoch 1267
Epoch 1267 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1267, epoch_train_loss=0.0007309753768073356
Epoch 1268
Epoch 1268 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1268, epoch_train_loss=0.0007309753768073356
Epoch 1269
Epoch 1269 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1269, epoch_train_loss=0.0007309753768073356
Epoch 1270
Epoch 1270 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1270, epoch_train_loss=0.0007309753768073356
Epoch 1271
Epoch 1271 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1271, epoch_train_loss=0.0007309753768073356
Epoch 1272
Epoch 1272 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1272, epoch_train_loss=0.0007309753768073356
Epoch 1273
Epoch 1273 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1273, epoch_train_loss=0.0007309753768073356
Epoch 1274
Epoch 1274 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1274, epoch_train_loss=0.0007309753768073356
Epoch 1275
Epoch 1275 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1275, epoch_train_loss=0.0007309753768073356
Epoch 1276
Epoch 1276 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1276, epoch_train_loss=0.0007309753768073356
Epoch 1277
Epoch 1277 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1277, epoch_train_loss=0.0007309753768073356
Epoch 1278
Epoch 1278 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1278, epoch_train_loss=0.0007309753768073356
Epoch 1279
Epoch 1279 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1279, epoch_train_loss=0.0007309753768073356
Epoch 1280
Epoch 1280 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1280, epoch_train_loss=0.0007309753768073356
Epoch 1281
Epoch 1281 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1281, epoch_train_loss=0.0007309753768073356
Epoch 1282
Epoch 1282 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1282, epoch_train_loss=0.0007309753768073356
Epoch 1283
Epoch 1283 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1283, epoch_train_loss=0.0007309753768073356
Epoch 1284
Epoch 1284 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1284, epoch_train_loss=0.0007309753768073356
Epoch 1285
Epoch 1285 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1285, epoch_train_loss=0.0007309753768073356
Epoch 1286
Epoch 1286 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1286, epoch_train_loss=0.0007309753768073356
Epoch 1287
Epoch 1287 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1287, epoch_train_loss=0.0007309753768073356
Epoch 1288
Epoch 1288 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1288, epoch_train_loss=0.0007309753768073356
Epoch 1289
Epoch 1289 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1289, epoch_train_loss=0.0007309753768073356
Epoch 1290
Epoch 1290 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1290, epoch_train_loss=0.0007309753768073356
Epoch 1291
Epoch 1291 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1291, epoch_train_loss=0.0007309753768073356
Epoch 1292
Epoch 1292 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1292, epoch_train_loss=0.0007309753768073356
Epoch 1293
Epoch 1293 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1293, epoch_train_loss=0.0007309753768073356
Epoch 1294
Epoch 1294 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1294, epoch_train_loss=0.0007309753768073356
Epoch 1295
Epoch 1295 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1295, epoch_train_loss=0.0007309753768073356
Epoch 1296
Epoch 1296 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1296, epoch_train_loss=0.0007309753768073356
Epoch 1297
Epoch 1297 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1297, epoch_train_loss=0.0007309753768073356
Epoch 1298
Epoch 1298 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1298, epoch_train_loss=0.0007309753768073356
Epoch 1299
Epoch 1299 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1299, epoch_train_loss=0.0007309753768073356
Epoch 1300
Epoch 1300 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1300, epoch_train_loss=0.0007309753768073356
Epoch 1301
Epoch 1301 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1301, epoch_train_loss=0.0007309753768073356
Epoch 1302
Epoch 1302 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1302, epoch_train_loss=0.0007309753768073356
Epoch 1303
Epoch 1303 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1303, epoch_train_loss=0.0007309753768073356
Epoch 1304
Epoch 1304 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1304, epoch_train_loss=0.0007309753768073356
Epoch 1305
Epoch 1305 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1305, epoch_train_loss=0.0007309753768073356
Epoch 1306
Epoch 1306 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1306, epoch_train_loss=0.0007309753768073356
Epoch 1307
Epoch 1307 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1307, epoch_train_loss=0.0007309753768073356
Epoch 1308
Epoch 1308 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1308, epoch_train_loss=0.0007309753768073356
Epoch 1309
Epoch 1309 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1309, epoch_train_loss=0.0007309753768073356
Epoch 1310
Epoch 1310 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1310, epoch_train_loss=0.0007309753768073356
Epoch 1311
Epoch 1311 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1311, epoch_train_loss=0.0007309753768073356
Epoch 1312
Epoch 1312 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1312, epoch_train_loss=0.0007309753768073356
Epoch 1313
Epoch 1313 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1313, epoch_train_loss=0.0007309753768073356
Epoch 1314
Epoch 1314 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1314, epoch_train_loss=0.0007309753768073356
Epoch 1315
Epoch 1315 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1315, epoch_train_loss=0.0007309753768073356
Epoch 1316
Epoch 1316 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1316, epoch_train_loss=0.0007309753768073356
Epoch 1317
Epoch 1317 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1317, epoch_train_loss=0.0007309753768073356
Epoch 1318
Epoch 1318 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1318, epoch_train_loss=0.0007309753768073356
Epoch 1319
Epoch 1319 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1319, epoch_train_loss=0.0007309753768073356
Epoch 1320
Epoch 1320 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1320, epoch_train_loss=0.0007309753768073356
Epoch 1321
Epoch 1321 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1321, epoch_train_loss=0.0007309753768073356
Epoch 1322
Epoch 1322 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1322, epoch_train_loss=0.0007309753768073356
Epoch 1323
Epoch 1323 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1323, epoch_train_loss=0.0007309753768073356
Epoch 1324
Epoch 1324 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1324, epoch_train_loss=0.0007309753768073356
Epoch 1325
Epoch 1325 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1325, epoch_train_loss=0.0007309753768073356
Epoch 1326
Epoch 1326 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1326, epoch_train_loss=0.0007309753768073356
Epoch 1327
Epoch 1327 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1327, epoch_train_loss=0.0007309753768073356
Epoch 1328
Epoch 1328 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1328, epoch_train_loss=0.0007309753768073356
Epoch 1329
Epoch 1329 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1329, epoch_train_loss=0.0007309753768073356
Epoch 1330
Epoch 1330 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1330, epoch_train_loss=0.0007309753768073356
Epoch 1331
Epoch 1331 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1331, epoch_train_loss=0.0007309753768073356
Epoch 1332
Epoch 1332 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1332, epoch_train_loss=0.0007309753768073356
Epoch 1333
Epoch 1333 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1333, epoch_train_loss=0.0007309753768073356
Epoch 1334
Epoch 1334 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1334, epoch_train_loss=0.0007309753768073356
Epoch 1335
Epoch 1335 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1335, epoch_train_loss=0.0007309753768073356
Epoch 1336
Epoch 1336 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1336, epoch_train_loss=0.0007309753768073356
Epoch 1337
Epoch 1337 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1337, epoch_train_loss=0.0007309753768073356
Epoch 1338
Epoch 1338 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1338, epoch_train_loss=0.0007309753768073356
Epoch 1339
Epoch 1339 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1339, epoch_train_loss=0.0007309753768073356
Epoch 1340
Epoch 1340 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1340, epoch_train_loss=0.0007309753768073356
Epoch 1341
Epoch 1341 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1341, epoch_train_loss=0.0007309753768073356
Epoch 1342
Epoch 1342 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1342, epoch_train_loss=0.0007309753768073356
Epoch 1343
Epoch 1343 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1343, epoch_train_loss=0.0007309753768073356
Epoch 1344
Epoch 1344 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1344, epoch_train_loss=0.0007309753768073356
Epoch 1345
Epoch 1345 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1345, epoch_train_loss=0.0007309753768073356
Epoch 1346
Epoch 1346 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1346, epoch_train_loss=0.0007309753768073356
Epoch 1347
Epoch 1347 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1347, epoch_train_loss=0.0007309753768073356
Epoch 1348
Epoch 1348 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1348, epoch_train_loss=0.0007309753768073356
Epoch 1349
Epoch 1349 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1349, epoch_train_loss=0.0007309753768073356
Epoch 1350
Epoch 1350 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1350, epoch_train_loss=0.0007309753768073356
Epoch 1351
Epoch 1351 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1351, epoch_train_loss=0.0007309753768073356
Epoch 1352
Epoch 1352 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1352, epoch_train_loss=0.0007309753768073356
Epoch 1353
Epoch 1353 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1353, epoch_train_loss=0.0007309753768073356
Epoch 1354
Epoch 1354 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1354, epoch_train_loss=0.0007309753768073356
Epoch 1355
Epoch 1355 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1355, epoch_train_loss=0.0007309753768073356
Epoch 1356
Epoch 1356 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1356, epoch_train_loss=0.0007309753768073356
Epoch 1357
Epoch 1357 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1357, epoch_train_loss=0.0007309753768073356
Epoch 1358
Epoch 1358 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1358, epoch_train_loss=0.0007309753768073356
Epoch 1359
Epoch 1359 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1359, epoch_train_loss=0.0007309753768073356
Epoch 1360
Epoch 1360 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1360, epoch_train_loss=0.0007309753768073356
Epoch 1361
Epoch 1361 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1361, epoch_train_loss=0.0007309753768073356
Epoch 1362
Epoch 1362 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1362, epoch_train_loss=0.0007309753768073356
Epoch 1363
Epoch 1363 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1363, epoch_train_loss=0.0007309753768073356
Epoch 1364
Epoch 1364 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1364, epoch_train_loss=0.0007309753768073356
Epoch 1365
Epoch 1365 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1365, epoch_train_loss=0.0007309753768073356
Epoch 1366
Epoch 1366 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1366, epoch_train_loss=0.0007309753768073356
Epoch 1367
Epoch 1367 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1367, epoch_train_loss=0.0007309753768073356
Epoch 1368
Epoch 1368 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1368, epoch_train_loss=0.0007309753768073356
Epoch 1369
Epoch 1369 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1369, epoch_train_loss=0.0007309753768073356
Epoch 1370
Epoch 1370 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1370, epoch_train_loss=0.0007309753768073356
Epoch 1371
Epoch 1371 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1371, epoch_train_loss=0.0007309753768073356
Epoch 1372
Epoch 1372 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1372, epoch_train_loss=0.0007309753768073356
Epoch 1373
Epoch 1373 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1373, epoch_train_loss=0.0007309753768073356
Epoch 1374
Epoch 1374 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1374, epoch_train_loss=0.0007309753768073356
Epoch 1375
Epoch 1375 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1375, epoch_train_loss=0.0007309753768073356
Epoch 1376
Epoch 1376 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1376, epoch_train_loss=0.0007309753768073356
Epoch 1377
Epoch 1377 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1377, epoch_train_loss=0.0007309753768073356
Epoch 1378
Epoch 1378 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1378, epoch_train_loss=0.0007309753768073356
Epoch 1379
Epoch 1379 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1379, epoch_train_loss=0.0007309753768073356
Epoch 1380
Epoch 1380 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1380, epoch_train_loss=0.0007309753768073356
Epoch 1381
Epoch 1381 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1381, epoch_train_loss=0.0007309753768073356
Epoch 1382
Epoch 1382 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1382, epoch_train_loss=0.0007309753768073356
Epoch 1383
Epoch 1383 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1383, epoch_train_loss=0.0007309753768073356
Epoch 1384
Epoch 1384 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1384, epoch_train_loss=0.0007309753768073356
Epoch 1385
Epoch 1385 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1385, epoch_train_loss=0.0007309753768073356
Epoch 1386
Epoch 1386 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1386, epoch_train_loss=0.0007309753768073356
Epoch 1387
Epoch 1387 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1387, epoch_train_loss=0.0007309753768073356
Epoch 1388
Epoch 1388 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1388, epoch_train_loss=0.0007309753768073356
Epoch 1389
Epoch 1389 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1389, epoch_train_loss=0.0007309753768073356
Epoch 1390
Epoch 1390 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1390, epoch_train_loss=0.0007309753768073356
Epoch 1391
Epoch 1391 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1391, epoch_train_loss=0.0007309753768073356
Epoch 1392
Epoch 1392 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1392, epoch_train_loss=0.0007309753768073356
Epoch 1393
Epoch 1393 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1393, epoch_train_loss=0.0007309753768073356
Epoch 1394
Epoch 1394 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1394, epoch_train_loss=0.0007309753768073356
Epoch 1395
Epoch 1395 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1395, epoch_train_loss=0.0007309753768073356
Epoch 1396
Epoch 1396 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1396, epoch_train_loss=0.0007309753768073356
Epoch 1397
Epoch 1397 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1397, epoch_train_loss=0.0007309753768073356
Epoch 1398
Epoch 1398 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1398, epoch_train_loss=0.0007309753768073356
Epoch 1399
Epoch 1399 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1399, epoch_train_loss=0.0007309753768073356
Epoch 1400
Epoch 1400 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1400, epoch_train_loss=0.0007309753768073356
Epoch 1401
Epoch 1401 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1401, epoch_train_loss=0.0007309753768073356
Epoch 1402
Epoch 1402 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1402, epoch_train_loss=0.0007309753768073356
Epoch 1403
Epoch 1403 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1403, epoch_train_loss=0.0007309753768073356
Epoch 1404
Epoch 1404 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1404, epoch_train_loss=0.0007309753768073356
Epoch 1405
Epoch 1405 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1405, epoch_train_loss=0.0007309753768073356
Epoch 1406
Epoch 1406 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1406, epoch_train_loss=0.0007309753768073356
Epoch 1407
Epoch 1407 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1407, epoch_train_loss=0.0007309753768073356
Epoch 1408
Epoch 1408 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1408, epoch_train_loss=0.0007309753768073356
Epoch 1409
Epoch 1409 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1409, epoch_train_loss=0.0007309753768073356
Epoch 1410
Epoch 1410 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1410, epoch_train_loss=0.0007309753768073356
Epoch 1411
Epoch 1411 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1411, epoch_train_loss=0.0007309753768073356
Epoch 1412
Epoch 1412 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1412, epoch_train_loss=0.0007309753768073356
Epoch 1413
Epoch 1413 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1413, epoch_train_loss=0.0007309753768073356
Epoch 1414
Epoch 1414 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1414, epoch_train_loss=0.0007309753768073356
Epoch 1415
Epoch 1415 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1415, epoch_train_loss=0.0007309753768073356
Epoch 1416
Epoch 1416 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1416, epoch_train_loss=0.0007309753768073356
Epoch 1417
Epoch 1417 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1417, epoch_train_loss=0.0007309753768073356
Epoch 1418
Epoch 1418 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1418, epoch_train_loss=0.0007309753768073356
Epoch 1419
Epoch 1419 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1419, epoch_train_loss=0.0007309753768073356
Epoch 1420
Epoch 1420 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1420, epoch_train_loss=0.0007309753768073356
Epoch 1421
Epoch 1421 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1421, epoch_train_loss=0.0007309753768073356
Epoch 1422
Epoch 1422 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1422, epoch_train_loss=0.0007309753768073356
Epoch 1423
Epoch 1423 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1423, epoch_train_loss=0.0007309753768073356
Epoch 1424
Epoch 1424 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1424, epoch_train_loss=0.0007309753768073356
Epoch 1425
Epoch 1425 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1425, epoch_train_loss=0.0007309753768073356
Epoch 1426
Epoch 1426 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1426, epoch_train_loss=0.0007309753768073356
Epoch 1427
Epoch 1427 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1427, epoch_train_loss=0.0007309753768073356
Epoch 1428
Epoch 1428 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1428, epoch_train_loss=0.0007309753768073356
Epoch 1429
Epoch 1429 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1429, epoch_train_loss=0.0007309753768073356
Epoch 1430
Epoch 1430 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1430, epoch_train_loss=0.0007309753768073356
Epoch 1431
Epoch 1431 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1431, epoch_train_loss=0.0007309753768073356
Epoch 1432
Epoch 1432 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1432, epoch_train_loss=0.0007309753768073356
Epoch 1433
Epoch 1433 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1433, epoch_train_loss=0.0007309753768073356
Epoch 1434
Epoch 1434 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1434, epoch_train_loss=0.0007309753768073356
Epoch 1435
Epoch 1435 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1435, epoch_train_loss=0.0007309753768073356
Epoch 1436
Epoch 1436 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1436, epoch_train_loss=0.0007309753768073356
Epoch 1437
Epoch 1437 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1437, epoch_train_loss=0.0007309753768073356
Epoch 1438
Epoch 1438 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1438, epoch_train_loss=0.0007309753768073356
Epoch 1439
Epoch 1439 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1439, epoch_train_loss=0.0007309753768073356
Epoch 1440
Epoch 1440 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1440, epoch_train_loss=0.0007309753768073356
Epoch 1441
Epoch 1441 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1441, epoch_train_loss=0.0007309753768073356
Epoch 1442
Epoch 1442 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1442, epoch_train_loss=0.0007309753768073356
Epoch 1443
Epoch 1443 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1443, epoch_train_loss=0.0007309753768073356
Epoch 1444
Epoch 1444 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1444, epoch_train_loss=0.0007309753768073356
Epoch 1445
Epoch 1445 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1445, epoch_train_loss=0.0007309753768073356
Epoch 1446
Epoch 1446 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1446, epoch_train_loss=0.0007309753768073356
Epoch 1447
Epoch 1447 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1447, epoch_train_loss=0.0007309753768073356
Epoch 1448
Epoch 1448 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1448, epoch_train_loss=0.0007309753768073356
Epoch 1449
Epoch 1449 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1449, epoch_train_loss=0.0007309753768073356
Epoch 1450
Epoch 1450 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1450, epoch_train_loss=0.0007309753768073356
Epoch 1451
Epoch 1451 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1451, epoch_train_loss=0.0007309753768073356
Epoch 1452
Epoch 1452 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1452, epoch_train_loss=0.0007309753768073356
Epoch 1453
Epoch 1453 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1453, epoch_train_loss=0.0007309753768073356
Epoch 1454
Epoch 1454 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1454, epoch_train_loss=0.0007309753768073356
Epoch 1455
Epoch 1455 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1455, epoch_train_loss=0.0007309753768073356
Epoch 1456
Epoch 1456 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1456, epoch_train_loss=0.0007309753768073356
Epoch 1457
Epoch 1457 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1457, epoch_train_loss=0.0007309753768073356
Epoch 1458
Epoch 1458 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1458, epoch_train_loss=0.0007309753768073356
Epoch 1459
Epoch 1459 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1459, epoch_train_loss=0.0007309753768073356
Epoch 1460
Epoch 1460 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1460, epoch_train_loss=0.0007309753768073356
Epoch 1461
Epoch 1461 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1461, epoch_train_loss=0.0007309753768073356
Epoch 1462
Epoch 1462 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1462, epoch_train_loss=0.0007309753768073356
Epoch 1463
Epoch 1463 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1463, epoch_train_loss=0.0007309753768073356
Epoch 1464
Epoch 1464 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1464, epoch_train_loss=0.0007309753768073356
Epoch 1465
Epoch 1465 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1465, epoch_train_loss=0.0007309753768073356
Epoch 1466
Epoch 1466 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1466, epoch_train_loss=0.0007309753768073356
Epoch 1467
Epoch 1467 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1467, epoch_train_loss=0.0007309753768073356
Epoch 1468
Epoch 1468 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1468, epoch_train_loss=0.0007309753768073356
Epoch 1469
Epoch 1469 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1469, epoch_train_loss=0.0007309753768073356
Epoch 1470
Epoch 1470 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1470, epoch_train_loss=0.0007309753768073356
Epoch 1471
Epoch 1471 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1471, epoch_train_loss=0.0007309753768073356
Epoch 1472
Epoch 1472 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1472, epoch_train_loss=0.0007309753768073356
Epoch 1473
Epoch 1473 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1473, epoch_train_loss=0.0007309753768073356
Epoch 1474
Epoch 1474 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1474, epoch_train_loss=0.0007309753768073356
Epoch 1475
Epoch 1475 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1475, epoch_train_loss=0.0007309753768073356
Epoch 1476
Epoch 1476 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1476, epoch_train_loss=0.0007309753768073356
Epoch 1477
Epoch 1477 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1477, epoch_train_loss=0.0007309753768073356
Epoch 1478
Epoch 1478 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1478, epoch_train_loss=0.0007309753768073356
Epoch 1479
Epoch 1479 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1479, epoch_train_loss=0.0007309753768073356
Epoch 1480
Epoch 1480 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1480, epoch_train_loss=0.0007309753768073356
Epoch 1481
Epoch 1481 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1481, epoch_train_loss=0.0007309753768073356
Epoch 1482
Epoch 1482 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1482, epoch_train_loss=0.0007309753768073356
Epoch 1483
Epoch 1483 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1483, epoch_train_loss=0.0007309753768073356
Epoch 1484
Epoch 1484 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1484, epoch_train_loss=0.0007309753768073356
Epoch 1485
Epoch 1485 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1485, epoch_train_loss=0.0007309753768073356
Epoch 1486
Epoch 1486 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1486, epoch_train_loss=0.0007309753768073356
Epoch 1487
Epoch 1487 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1487, epoch_train_loss=0.0007309753768073356
Epoch 1488
Epoch 1488 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1488, epoch_train_loss=0.0007309753768073356
Epoch 1489
Epoch 1489 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1489, epoch_train_loss=0.0007309753768073356
Epoch 1490
Epoch 1490 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1490, epoch_train_loss=0.0007309753768073356
Epoch 1491
Epoch 1491 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1491, epoch_train_loss=0.0007309753768073356
Epoch 1492
Epoch 1492 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1492, epoch_train_loss=0.0007309753768073356
Epoch 1493
Epoch 1493 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1493, epoch_train_loss=0.0007309753768073356
Epoch 1494
Epoch 1494 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1494, epoch_train_loss=0.0007309753768073356
Epoch 1495
Epoch 1495 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1495, epoch_train_loss=0.0007309753768073356
Epoch 1496
Epoch 1496 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1496, epoch_train_loss=0.0007309753768073356
Epoch 1497
Epoch 1497 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1497, epoch_train_loss=0.0007309753768073356
Epoch 1498
Epoch 1498 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1498, epoch_train_loss=0.0007309753768073356
Epoch 1499
Epoch 1499 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1499, epoch_train_loss=0.0007309753768073356
Epoch 1500
Epoch 1500 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1500, epoch_train_loss=0.0007309753768073356
Epoch 1501
Epoch 1501 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1501, epoch_train_loss=0.0007309753768073356
Epoch 1502
Epoch 1502 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1502, epoch_train_loss=0.0007309753768073356
Epoch 1503
Epoch 1503 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1503, epoch_train_loss=0.0007309753768073356
Epoch 1504
Epoch 1504 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1504, epoch_train_loss=0.0007309753768073356
Epoch 1505
Epoch 1505 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1505, epoch_train_loss=0.0007309753768073356
Epoch 1506
Epoch 1506 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1506, epoch_train_loss=0.0007309753768073356
Epoch 1507
Epoch 1507 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1507, epoch_train_loss=0.0007309753768073356
Epoch 1508
Epoch 1508 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1508, epoch_train_loss=0.0007309753768073356
Epoch 1509
Epoch 1509 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1509, epoch_train_loss=0.0007309753768073356
Epoch 1510
Epoch 1510 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1510, epoch_train_loss=0.0007309753768073356
Epoch 1511
Epoch 1511 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1511, epoch_train_loss=0.0007309753768073356
Epoch 1512
Epoch 1512 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1512, epoch_train_loss=0.0007309753768073356
Epoch 1513
Epoch 1513 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1513, epoch_train_loss=0.0007309753768073356
Epoch 1514
Epoch 1514 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1514, epoch_train_loss=0.0007309753768073356
Epoch 1515
Epoch 1515 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1515, epoch_train_loss=0.0007309753768073356
Epoch 1516
Epoch 1516 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1516, epoch_train_loss=0.0007309753768073356
Epoch 1517
Epoch 1517 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1517, epoch_train_loss=0.0007309753768073356
Epoch 1518
Epoch 1518 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1518, epoch_train_loss=0.0007309753768073356
Epoch 1519
Epoch 1519 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1519, epoch_train_loss=0.0007309753768073356
Epoch 1520
Epoch 1520 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1520, epoch_train_loss=0.0007309753768073356
Epoch 1521
Epoch 1521 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1521, epoch_train_loss=0.0007309753768073356
Epoch 1522
Epoch 1522 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1522, epoch_train_loss=0.0007309753768073356
Epoch 1523
Epoch 1523 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1523, epoch_train_loss=0.0007309753768073356
Epoch 1524
Epoch 1524 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1524, epoch_train_loss=0.0007309753768073356
Epoch 1525
Epoch 1525 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1525, epoch_train_loss=0.0007309753768073356
Epoch 1526
Epoch 1526 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1526, epoch_train_loss=0.0007309753768073356
Epoch 1527
Epoch 1527 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1527, epoch_train_loss=0.0007309753768073356
Epoch 1528
Epoch 1528 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1528, epoch_train_loss=0.0007309753768073356
Epoch 1529
Epoch 1529 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1529, epoch_train_loss=0.0007309753768073356
Epoch 1530
Epoch 1530 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1530, epoch_train_loss=0.0007309753768073356
Epoch 1531
Epoch 1531 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1531, epoch_train_loss=0.0007309753768073356
Epoch 1532
Epoch 1532 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1532, epoch_train_loss=0.0007309753768073356
Epoch 1533
Epoch 1533 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1533, epoch_train_loss=0.0007309753768073356
Epoch 1534
Epoch 1534 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1534, epoch_train_loss=0.0007309753768073356
Epoch 1535
Epoch 1535 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1535, epoch_train_loss=0.0007309753768073356
Epoch 1536
Epoch 1536 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1536, epoch_train_loss=0.0007309753768073356
Epoch 1537
Epoch 1537 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1537, epoch_train_loss=0.0007309753768073356
Epoch 1538
Epoch 1538 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1538, epoch_train_loss=0.0007309753768073356
Epoch 1539
Epoch 1539 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1539, epoch_train_loss=0.0007309753768073356
Epoch 1540
Epoch 1540 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1540, epoch_train_loss=0.0007309753768073356
Epoch 1541
Epoch 1541 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1541, epoch_train_loss=0.0007309753768073356
Epoch 1542
Epoch 1542 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1542, epoch_train_loss=0.0007309753768073356
Epoch 1543
Epoch 1543 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1543, epoch_train_loss=0.0007309753768073356
Epoch 1544
Epoch 1544 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1544, epoch_train_loss=0.0007309753768073356
Epoch 1545
Epoch 1545 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1545, epoch_train_loss=0.0007309753768073356
Epoch 1546
Epoch 1546 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1546, epoch_train_loss=0.0007309753768073356
Epoch 1547
Epoch 1547 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1547, epoch_train_loss=0.0007309753768073356
Epoch 1548
Epoch 1548 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1548, epoch_train_loss=0.0007309753768073356
Epoch 1549
Epoch 1549 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1549, epoch_train_loss=0.0007309753768073356
Epoch 1550
Epoch 1550 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1550, epoch_train_loss=0.0007309753768073356
Epoch 1551
Epoch 1551 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1551, epoch_train_loss=0.0007309753768073356
Epoch 1552
Epoch 1552 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1552, epoch_train_loss=0.0007309753768073356
Epoch 1553
Epoch 1553 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1553, epoch_train_loss=0.0007309753768073356
Epoch 1554
Epoch 1554 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1554, epoch_train_loss=0.0007309753768073356
Epoch 1555
Epoch 1555 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1555, epoch_train_loss=0.0007309753768073356
Epoch 1556
Epoch 1556 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1556, epoch_train_loss=0.0007309753768073356
Epoch 1557
Epoch 1557 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1557, epoch_train_loss=0.0007309753768073356
Epoch 1558
Epoch 1558 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1558, epoch_train_loss=0.0007309753768073356
Epoch 1559
Epoch 1559 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1559, epoch_train_loss=0.0007309753768073356
Epoch 1560
Epoch 1560 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1560, epoch_train_loss=0.0007309753768073356
Epoch 1561
Epoch 1561 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1561, epoch_train_loss=0.0007309753768073356
Epoch 1562
Epoch 1562 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1562, epoch_train_loss=0.0007309753768073356
Epoch 1563
Epoch 1563 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1563, epoch_train_loss=0.0007309753768073356
Epoch 1564
Epoch 1564 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1564, epoch_train_loss=0.0007309753768073356
Epoch 1565
Epoch 1565 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1565, epoch_train_loss=0.0007309753768073356
Epoch 1566
Epoch 1566 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1566, epoch_train_loss=0.0007309753768073356
Epoch 1567
Epoch 1567 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1567, epoch_train_loss=0.0007309753768073356
Epoch 1568
Epoch 1568 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1568, epoch_train_loss=0.0007309753768073356
Epoch 1569
Epoch 1569 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1569, epoch_train_loss=0.0007309753768073356
Epoch 1570
Epoch 1570 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1570, epoch_train_loss=0.0007309753768073356
Epoch 1571
Epoch 1571 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1571, epoch_train_loss=0.0007309753768073356
Epoch 1572
Epoch 1572 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1572, epoch_train_loss=0.0007309753768073356
Epoch 1573
Epoch 1573 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1573, epoch_train_loss=0.0007309753768073356
Epoch 1574
Epoch 1574 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1574, epoch_train_loss=0.0007309753768073356
Epoch 1575
Epoch 1575 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1575, epoch_train_loss=0.0007309753768073356
Epoch 1576
Epoch 1576 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1576, epoch_train_loss=0.0007309753768073356
Epoch 1577
Epoch 1577 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1577, epoch_train_loss=0.0007309753768073356
Epoch 1578
Epoch 1578 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1578, epoch_train_loss=0.0007309753768073356
Epoch 1579
Epoch 1579 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1579, epoch_train_loss=0.0007309753768073356
Epoch 1580
Epoch 1580 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1580, epoch_train_loss=0.0007309753768073356
Epoch 1581
Epoch 1581 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1581, epoch_train_loss=0.0007309753768073356
Epoch 1582
Epoch 1582 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1582, epoch_train_loss=0.0007309753768073356
Epoch 1583
Epoch 1583 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1583, epoch_train_loss=0.0007309753768073356
Epoch 1584
Epoch 1584 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1584, epoch_train_loss=0.0007309753768073356
Epoch 1585
Epoch 1585 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1585, epoch_train_loss=0.0007309753768073356
Epoch 1586
Epoch 1586 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1586, epoch_train_loss=0.0007309753768073356
Epoch 1587
Epoch 1587 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1587, epoch_train_loss=0.0007309753768073356
Epoch 1588
Epoch 1588 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1588, epoch_train_loss=0.0007309753768073356
Epoch 1589
Epoch 1589 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1589, epoch_train_loss=0.0007309753768073356
Epoch 1590
Epoch 1590 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1590, epoch_train_loss=0.0007309753768073356
Epoch 1591
Epoch 1591 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1591, epoch_train_loss=0.0007309753768073356
Epoch 1592
Epoch 1592 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1592, epoch_train_loss=0.0007309753768073356
Epoch 1593
Epoch 1593 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1593, epoch_train_loss=0.0007309753768073356
Epoch 1594
Epoch 1594 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1594, epoch_train_loss=0.0007309753768073356
Epoch 1595
Epoch 1595 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1595, epoch_train_loss=0.0007309753768073356
Epoch 1596
Epoch 1596 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1596, epoch_train_loss=0.0007309753768073356
Epoch 1597
Epoch 1597 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1597, epoch_train_loss=0.0007309753768073356
Epoch 1598
Epoch 1598 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1598, epoch_train_loss=0.0007309753768073356
Epoch 1599
Epoch 1599 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1599, epoch_train_loss=0.0007309753768073356
Epoch 1600
Epoch 1600 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1600, epoch_train_loss=0.0007309753768073356
Epoch 1601
Epoch 1601 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1601, epoch_train_loss=0.0007309753768073356
Epoch 1602
Epoch 1602 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1602, epoch_train_loss=0.0007309753768073356
Epoch 1603
Epoch 1603 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1603, epoch_train_loss=0.0007309753768073356
Epoch 1604
Epoch 1604 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1604, epoch_train_loss=0.0007309753768073356
Epoch 1605
Epoch 1605 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1605, epoch_train_loss=0.0007309753768073356
Epoch 1606
Epoch 1606 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1606, epoch_train_loss=0.0007309753768073356
Epoch 1607
Epoch 1607 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1607, epoch_train_loss=0.0007309753768073356
Epoch 1608
Epoch 1608 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1608, epoch_train_loss=0.0007309753768073356
Epoch 1609
Epoch 1609 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1609, epoch_train_loss=0.0007309753768073356
Epoch 1610
Epoch 1610 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1610, epoch_train_loss=0.0007309753768073356
Epoch 1611
Epoch 1611 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1611, epoch_train_loss=0.0007309753768073356
Epoch 1612
Epoch 1612 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1612, epoch_train_loss=0.0007309753768073356
Epoch 1613
Epoch 1613 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1613, epoch_train_loss=0.0007309753768073356
Epoch 1614
Epoch 1614 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1614, epoch_train_loss=0.0007309753768073356
Epoch 1615
Epoch 1615 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1615, epoch_train_loss=0.0007309753768073356
Epoch 1616
Epoch 1616 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1616, epoch_train_loss=0.0007309753768073356
Epoch 1617
Epoch 1617 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1617, epoch_train_loss=0.0007309753768073356
Epoch 1618
Epoch 1618 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1618, epoch_train_loss=0.0007309753768073356
Epoch 1619
Epoch 1619 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1619, epoch_train_loss=0.0007309753768073356
Epoch 1620
Epoch 1620 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1620, epoch_train_loss=0.0007309753768073356
Epoch 1621
Epoch 1621 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1621, epoch_train_loss=0.0007309753768073356
Epoch 1622
Epoch 1622 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1622, epoch_train_loss=0.0007309753768073356
Epoch 1623
Epoch 1623 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1623, epoch_train_loss=0.0007309753768073356
Epoch 1624
Epoch 1624 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1624, epoch_train_loss=0.0007309753768073356
Epoch 1625
Epoch 1625 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1625, epoch_train_loss=0.0007309753768073356
Epoch 1626
Epoch 1626 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1626, epoch_train_loss=0.0007309753768073356
Epoch 1627
Epoch 1627 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1627, epoch_train_loss=0.0007309753768073356
Epoch 1628
Epoch 1628 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1628, epoch_train_loss=0.0007309753768073356
Epoch 1629
Epoch 1629 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1629, epoch_train_loss=0.0007309753768073356
Epoch 1630
Epoch 1630 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1630, epoch_train_loss=0.0007309753768073356
Epoch 1631
Epoch 1631 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1631, epoch_train_loss=0.0007309753768073356
Epoch 1632
Epoch 1632 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1632, epoch_train_loss=0.0007309753768073356
Epoch 1633
Epoch 1633 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1633, epoch_train_loss=0.0007309753768073356
Epoch 1634
Epoch 1634 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1634, epoch_train_loss=0.0007309753768073356
Epoch 1635
Epoch 1635 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1635, epoch_train_loss=0.0007309753768073356
Epoch 1636
Epoch 1636 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1636, epoch_train_loss=0.0007309753768073356
Epoch 1637
Epoch 1637 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1637, epoch_train_loss=0.0007309753768073356
Epoch 1638
Epoch 1638 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1638, epoch_train_loss=0.0007309753768073356
Epoch 1639
Epoch 1639 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1639, epoch_train_loss=0.0007309753768073356
Epoch 1640
Epoch 1640 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1640, epoch_train_loss=0.0007309753768073356
Epoch 1641
Epoch 1641 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1641, epoch_train_loss=0.0007309753768073356
Epoch 1642
Epoch 1642 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1642, epoch_train_loss=0.0007309753768073356
Epoch 1643
Epoch 1643 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1643, epoch_train_loss=0.0007309753768073356
Epoch 1644
Epoch 1644 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1644, epoch_train_loss=0.0007309753768073356
Epoch 1645
Epoch 1645 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1645, epoch_train_loss=0.0007309753768073356
Epoch 1646
Epoch 1646 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1646, epoch_train_loss=0.0007309753768073356
Epoch 1647
Epoch 1647 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1647, epoch_train_loss=0.0007309753768073356
Epoch 1648
Epoch 1648 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1648, epoch_train_loss=0.0007309753768073356
Epoch 1649
Epoch 1649 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1649, epoch_train_loss=0.0007309753768073356
Epoch 1650
Epoch 1650 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1650, epoch_train_loss=0.0007309753768073356
Epoch 1651
Epoch 1651 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1651, epoch_train_loss=0.0007309753768073356
Epoch 1652
Epoch 1652 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1652, epoch_train_loss=0.0007309753768073356
Epoch 1653
Epoch 1653 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1653, epoch_train_loss=0.0007309753768073356
Epoch 1654
Epoch 1654 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1654, epoch_train_loss=0.0007309753768073356
Epoch 1655
Epoch 1655 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1655, epoch_train_loss=0.0007309753768073356
Epoch 1656
Epoch 1656 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1656, epoch_train_loss=0.0007309753768073356
Epoch 1657
Epoch 1657 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1657, epoch_train_loss=0.0007309753768073356
Epoch 1658
Epoch 1658 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1658, epoch_train_loss=0.0007309753768073356
Epoch 1659
Epoch 1659 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1659, epoch_train_loss=0.0007309753768073356
Epoch 1660
Epoch 1660 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1660, epoch_train_loss=0.0007309753768073356
Epoch 1661
Epoch 1661 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1661, epoch_train_loss=0.0007309753768073356
Epoch 1662
Epoch 1662 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1662, epoch_train_loss=0.0007309753768073356
Epoch 1663
Epoch 1663 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1663, epoch_train_loss=0.0007309753768073356
Epoch 1664
Epoch 1664 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1664, epoch_train_loss=0.0007309753768073356
Epoch 1665
Epoch 1665 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1665, epoch_train_loss=0.0007309753768073356
Epoch 1666
Epoch 1666 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1666, epoch_train_loss=0.0007309753768073356
Epoch 1667
Epoch 1667 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1667, epoch_train_loss=0.0007309753768073356
Epoch 1668
Epoch 1668 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1668, epoch_train_loss=0.0007309753768073356
Epoch 1669
Epoch 1669 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1669, epoch_train_loss=0.0007309753768073356
Epoch 1670
Epoch 1670 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1670, epoch_train_loss=0.0007309753768073356
Epoch 1671
Epoch 1671 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1671, epoch_train_loss=0.0007309753768073356
Epoch 1672
Epoch 1672 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1672, epoch_train_loss=0.0007309753768073356
Epoch 1673
Epoch 1673 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1673, epoch_train_loss=0.0007309753768073356
Epoch 1674
Epoch 1674 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1674, epoch_train_loss=0.0007309753768073356
Epoch 1675
Epoch 1675 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1675, epoch_train_loss=0.0007309753768073356
Epoch 1676
Epoch 1676 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1676, epoch_train_loss=0.0007309753768073356
Epoch 1677
Epoch 1677 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1677, epoch_train_loss=0.0007309753768073356
Epoch 1678
Epoch 1678 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1678, epoch_train_loss=0.0007309753768073356
Epoch 1679
Epoch 1679 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1679, epoch_train_loss=0.0007309753768073356
Epoch 1680
Epoch 1680 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1680, epoch_train_loss=0.0007309753768073356
Epoch 1681
Epoch 1681 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1681, epoch_train_loss=0.0007309753768073356
Epoch 1682
Epoch 1682 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1682, epoch_train_loss=0.0007309753768073356
Epoch 1683
Epoch 1683 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1683, epoch_train_loss=0.0007309753768073356
Epoch 1684
Epoch 1684 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1684, epoch_train_loss=0.0007309753768073356
Epoch 1685
Epoch 1685 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1685, epoch_train_loss=0.0007309753768073356
Epoch 1686
Epoch 1686 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1686, epoch_train_loss=0.0007309753768073356
Epoch 1687
Epoch 1687 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1687, epoch_train_loss=0.0007309753768073356
Epoch 1688
Epoch 1688 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1688, epoch_train_loss=0.0007309753768073356
Epoch 1689
Epoch 1689 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1689, epoch_train_loss=0.0007309753768073356
Epoch 1690
Epoch 1690 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1690, epoch_train_loss=0.0007309753768073356
Epoch 1691
Epoch 1691 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1691, epoch_train_loss=0.0007309753768073356
Epoch 1692
Epoch 1692 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1692, epoch_train_loss=0.0007309753768073356
Epoch 1693
Epoch 1693 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1693, epoch_train_loss=0.0007309753768073356
Epoch 1694
Epoch 1694 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1694, epoch_train_loss=0.0007309753768073356
Epoch 1695
Epoch 1695 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1695, epoch_train_loss=0.0007309753768073356
Epoch 1696
Epoch 1696 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1696, epoch_train_loss=0.0007309753768073356
Epoch 1697
Epoch 1697 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1697, epoch_train_loss=0.0007309753768073356
Epoch 1698
Epoch 1698 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1698, epoch_train_loss=0.0007309753768073356
Epoch 1699
Epoch 1699 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1699, epoch_train_loss=0.0007309753768073356
Epoch 1700
Epoch 1700 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1700, epoch_train_loss=0.0007309753768073356
Epoch 1701
Epoch 1701 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1701, epoch_train_loss=0.0007309753768073356
Epoch 1702
Epoch 1702 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1702, epoch_train_loss=0.0007309753768073356
Epoch 1703
Epoch 1703 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1703, epoch_train_loss=0.0007309753768073356
Epoch 1704
Epoch 1704 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1704, epoch_train_loss=0.0007309753768073356
Epoch 1705
Epoch 1705 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1705, epoch_train_loss=0.0007309753768073356
Epoch 1706
Epoch 1706 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1706, epoch_train_loss=0.0007309753768073356
Epoch 1707
Epoch 1707 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1707, epoch_train_loss=0.0007309753768073356
Epoch 1708
Epoch 1708 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1708, epoch_train_loss=0.0007309753768073356
Epoch 1709
Epoch 1709 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1709, epoch_train_loss=0.0007309753768073356
Epoch 1710
Epoch 1710 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1710, epoch_train_loss=0.0007309753768073356
Epoch 1711
Epoch 1711 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1711, epoch_train_loss=0.0007309753768073356
Epoch 1712
Epoch 1712 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1712, epoch_train_loss=0.0007309753768073356
Epoch 1713
Epoch 1713 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1713, epoch_train_loss=0.0007309753768073356
Epoch 1714
Epoch 1714 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1714, epoch_train_loss=0.0007309753768073356
Epoch 1715
Epoch 1715 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1715, epoch_train_loss=0.0007309753768073356
Epoch 1716
Epoch 1716 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1716, epoch_train_loss=0.0007309753768073356
Epoch 1717
Epoch 1717 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1717, epoch_train_loss=0.0007309753768073356
Epoch 1718
Epoch 1718 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1718, epoch_train_loss=0.0007309753768073356
Epoch 1719
Epoch 1719 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1719, epoch_train_loss=0.0007309753768073356
Epoch 1720
Epoch 1720 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1720, epoch_train_loss=0.0007309753768073356
Epoch 1721
Epoch 1721 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1721, epoch_train_loss=0.0007309753768073356
Epoch 1722
Epoch 1722 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1722, epoch_train_loss=0.0007309753768073356
Epoch 1723
Epoch 1723 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1723, epoch_train_loss=0.0007309753768073356
Epoch 1724
Epoch 1724 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1724, epoch_train_loss=0.0007309753768073356
Epoch 1725
Epoch 1725 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1725, epoch_train_loss=0.0007309753768073356
Epoch 1726
Epoch 1726 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1726, epoch_train_loss=0.0007309753768073356
Epoch 1727
Epoch 1727 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1727, epoch_train_loss=0.0007309753768073356
Epoch 1728
Epoch 1728 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1728, epoch_train_loss=0.0007309753768073356
Epoch 1729
Epoch 1729 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1729, epoch_train_loss=0.0007309753768073356
Epoch 1730
Epoch 1730 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1730, epoch_train_loss=0.0007309753768073356
Epoch 1731
Epoch 1731 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1731, epoch_train_loss=0.0007309753768073356
Epoch 1732
Epoch 1732 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1732, epoch_train_loss=0.0007309753768073356
Epoch 1733
Epoch 1733 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1733, epoch_train_loss=0.0007309753768073356
Epoch 1734
Epoch 1734 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1734, epoch_train_loss=0.0007309753768073356
Epoch 1735
Epoch 1735 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1735, epoch_train_loss=0.0007309753768073356
Epoch 1736
Epoch 1736 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1736, epoch_train_loss=0.0007309753768073356
Epoch 1737
Epoch 1737 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1737, epoch_train_loss=0.0007309753768073356
Epoch 1738
Epoch 1738 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1738, epoch_train_loss=0.0007309753768073356
Epoch 1739
Epoch 1739 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1739, epoch_train_loss=0.0007309753768073356
Epoch 1740
Epoch 1740 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1740, epoch_train_loss=0.0007309753768073356
Epoch 1741
Epoch 1741 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1741, epoch_train_loss=0.0007309753768073356
Epoch 1742
Epoch 1742 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1742, epoch_train_loss=0.0007309753768073356
Epoch 1743
Epoch 1743 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1743, epoch_train_loss=0.0007309753768073356
Epoch 1744
Epoch 1744 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1744, epoch_train_loss=0.0007309753768073356
Epoch 1745
Epoch 1745 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1745, epoch_train_loss=0.0007309753768073356
Epoch 1746
Epoch 1746 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1746, epoch_train_loss=0.0007309753768073356
Epoch 1747
Epoch 1747 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1747, epoch_train_loss=0.0007309753768073356
Epoch 1748
Epoch 1748 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1748, epoch_train_loss=0.0007309753768073356
Epoch 1749
Epoch 1749 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1749, epoch_train_loss=0.0007309753768073356
Epoch 1750
Epoch 1750 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1750, epoch_train_loss=0.0007309753768073356
Epoch 1751
Epoch 1751 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1751, epoch_train_loss=0.0007309753768073356
Epoch 1752
Epoch 1752 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1752, epoch_train_loss=0.0007309753768073356
Epoch 1753
Epoch 1753 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1753, epoch_train_loss=0.0007309753768073356
Epoch 1754
Epoch 1754 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1754, epoch_train_loss=0.0007309753768073356
Epoch 1755
Epoch 1755 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1755, epoch_train_loss=0.0007309753768073356
Epoch 1756
Epoch 1756 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1756, epoch_train_loss=0.0007309753768073356
Epoch 1757
Epoch 1757 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1757, epoch_train_loss=0.0007309753768073356
Epoch 1758
Epoch 1758 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1758, epoch_train_loss=0.0007309753768073356
Epoch 1759
Epoch 1759 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1759, epoch_train_loss=0.0007309753768073356
Epoch 1760
Epoch 1760 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1760, epoch_train_loss=0.0007309753768073356
Epoch 1761
Epoch 1761 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1761, epoch_train_loss=0.0007309753768073356
Epoch 1762
Epoch 1762 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1762, epoch_train_loss=0.0007309753768073356
Epoch 1763
Epoch 1763 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1763, epoch_train_loss=0.0007309753768073356
Epoch 1764
Epoch 1764 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1764, epoch_train_loss=0.0007309753768073356
Epoch 1765
Epoch 1765 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1765, epoch_train_loss=0.0007309753768073356
Epoch 1766
Epoch 1766 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1766, epoch_train_loss=0.0007309753768073356
Epoch 1767
Epoch 1767 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1767, epoch_train_loss=0.0007309753768073356
Epoch 1768
Epoch 1768 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1768, epoch_train_loss=0.0007309753768073356
Epoch 1769
Epoch 1769 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1769, epoch_train_loss=0.0007309753768073356
Epoch 1770
Epoch 1770 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1770, epoch_train_loss=0.0007309753768073356
Epoch 1771
Epoch 1771 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1771, epoch_train_loss=0.0007309753768073356
Epoch 1772
Epoch 1772 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1772, epoch_train_loss=0.0007309753768073356
Epoch 1773
Epoch 1773 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1773, epoch_train_loss=0.0007309753768073356
Epoch 1774
Epoch 1774 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1774, epoch_train_loss=0.0007309753768073356
Epoch 1775
Epoch 1775 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1775, epoch_train_loss=0.0007309753768073356
Epoch 1776
Epoch 1776 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1776, epoch_train_loss=0.0007309753768073356
Epoch 1777
Epoch 1777 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1777, epoch_train_loss=0.0007309753768073356
Epoch 1778
Epoch 1778 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1778, epoch_train_loss=0.0007309753768073356
Epoch 1779
Epoch 1779 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1779, epoch_train_loss=0.0007309753768073356
Epoch 1780
Epoch 1780 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1780, epoch_train_loss=0.0007309753768073356
Epoch 1781
Epoch 1781 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1781, epoch_train_loss=0.0007309753768073356
Epoch 1782
Epoch 1782 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1782, epoch_train_loss=0.0007309753768073356
Epoch 1783
Epoch 1783 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1783, epoch_train_loss=0.0007309753768073356
Epoch 1784
Epoch 1784 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1784, epoch_train_loss=0.0007309753768073356
Epoch 1785
Epoch 1785 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1785, epoch_train_loss=0.0007309753768073356
Epoch 1786
Epoch 1786 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1786, epoch_train_loss=0.0007309753768073356
Epoch 1787
Epoch 1787 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1787, epoch_train_loss=0.0007309753768073356
Epoch 1788
Epoch 1788 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1788, epoch_train_loss=0.0007309753768073356
Epoch 1789
Epoch 1789 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1789, epoch_train_loss=0.0007309753768073356
Epoch 1790
Epoch 1790 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1790, epoch_train_loss=0.0007309753768073356
Epoch 1791
Epoch 1791 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1791, epoch_train_loss=0.0007309753768073356
Epoch 1792
Epoch 1792 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1792, epoch_train_loss=0.0007309753768073356
Epoch 1793
Epoch 1793 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1793, epoch_train_loss=0.0007309753768073356
Epoch 1794
Epoch 1794 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1794, epoch_train_loss=0.0007309753768073356
Epoch 1795
Epoch 1795 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1795, epoch_train_loss=0.0007309753768073356
Epoch 1796
Epoch 1796 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1796, epoch_train_loss=0.0007309753768073356
Epoch 1797
Epoch 1797 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1797, epoch_train_loss=0.0007309753768073356
Epoch 1798
Epoch 1798 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1798, epoch_train_loss=0.0007309753768073356
Epoch 1799
Epoch 1799 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1799, epoch_train_loss=0.0007309753768073356
Epoch 1800
Epoch 1800 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1800, epoch_train_loss=0.0007309753768073356
Epoch 1801
Epoch 1801 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1801, epoch_train_loss=0.0007309753768073356
Epoch 1802
Epoch 1802 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1802, epoch_train_loss=0.0007309753768073356
Epoch 1803
Epoch 1803 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1803, epoch_train_loss=0.0007309753768073356
Epoch 1804
Epoch 1804 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1804, epoch_train_loss=0.0007309753768073356
Epoch 1805
Epoch 1805 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1805, epoch_train_loss=0.0007309753768073356
Epoch 1806
Epoch 1806 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1806, epoch_train_loss=0.0007309753768073356
Epoch 1807
Epoch 1807 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1807, epoch_train_loss=0.0007309753768073356
Epoch 1808
Epoch 1808 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1808, epoch_train_loss=0.0007309753768073356
Epoch 1809
Epoch 1809 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1809, epoch_train_loss=0.0007309753768073356
Epoch 1810
Epoch 1810 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1810, epoch_train_loss=0.0007309753768073356
Epoch 1811
Epoch 1811 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1811, epoch_train_loss=0.0007309753768073356
Epoch 1812
Epoch 1812 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1812, epoch_train_loss=0.0007309753768073356
Epoch 1813
Epoch 1813 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1813, epoch_train_loss=0.0007309753768073356
Epoch 1814
Epoch 1814 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1814, epoch_train_loss=0.0007309753768073356
Epoch 1815
Epoch 1815 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1815, epoch_train_loss=0.0007309753768073356
Epoch 1816
Epoch 1816 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1816, epoch_train_loss=0.0007309753768073356
Epoch 1817
Epoch 1817 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1817, epoch_train_loss=0.0007309753768073356
Epoch 1818
Epoch 1818 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1818, epoch_train_loss=0.0007309753768073356
Epoch 1819
Epoch 1819 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1819, epoch_train_loss=0.0007309753768073356
Epoch 1820
Epoch 1820 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1820, epoch_train_loss=0.0007309753768073356
Epoch 1821
Epoch 1821 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1821, epoch_train_loss=0.0007309753768073356
Epoch 1822
Epoch 1822 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1822, epoch_train_loss=0.0007309753768073356
Epoch 1823
Epoch 1823 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1823, epoch_train_loss=0.0007309753768073356
Epoch 1824
Epoch 1824 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1824, epoch_train_loss=0.0007309753768073356
Epoch 1825
Epoch 1825 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1825, epoch_train_loss=0.0007309753768073356
Epoch 1826
Epoch 1826 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1826, epoch_train_loss=0.0007309753768073356
Epoch 1827
Epoch 1827 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1827, epoch_train_loss=0.0007309753768073356
Epoch 1828
Epoch 1828 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1828, epoch_train_loss=0.0007309753768073356
Epoch 1829
Epoch 1829 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1829, epoch_train_loss=0.0007309753768073356
Epoch 1830
Epoch 1830 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1830, epoch_train_loss=0.0007309753768073356
Epoch 1831
Epoch 1831 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1831, epoch_train_loss=0.0007309753768073356
Epoch 1832
Epoch 1832 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1832, epoch_train_loss=0.0007309753768073356
Epoch 1833
Epoch 1833 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1833, epoch_train_loss=0.0007309753768073356
Epoch 1834
Epoch 1834 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1834, epoch_train_loss=0.0007309753768073356
Epoch 1835
Epoch 1835 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1835, epoch_train_loss=0.0007309753768073356
Epoch 1836
Epoch 1836 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1836, epoch_train_loss=0.0007309753768073356
Epoch 1837
Epoch 1837 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1837, epoch_train_loss=0.0007309753768073356
Epoch 1838
Epoch 1838 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1838, epoch_train_loss=0.0007309753768073356
Epoch 1839
Epoch 1839 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1839, epoch_train_loss=0.0007309753768073356
Epoch 1840
Epoch 1840 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1840, epoch_train_loss=0.0007309753768073356
Epoch 1841
Epoch 1841 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1841, epoch_train_loss=0.0007309753768073356
Epoch 1842
Epoch 1842 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1842, epoch_train_loss=0.0007309753768073356
Epoch 1843
Epoch 1843 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1843, epoch_train_loss=0.0007309753768073356
Epoch 1844
Epoch 1844 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1844, epoch_train_loss=0.0007309753768073356
Epoch 1845
Epoch 1845 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1845, epoch_train_loss=0.0007309753768073356
Epoch 1846
Epoch 1846 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1846, epoch_train_loss=0.0007309753768073356
Epoch 1847
Epoch 1847 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1847, epoch_train_loss=0.0007309753768073356
Epoch 1848
Epoch 1848 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1848, epoch_train_loss=0.0007309753768073356
Epoch 1849
Epoch 1849 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1849, epoch_train_loss=0.0007309753768073356
Epoch 1850
Epoch 1850 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1850, epoch_train_loss=0.0007309753768073356
Epoch 1851
Epoch 1851 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1851, epoch_train_loss=0.0007309753768073356
Epoch 1852
Epoch 1852 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1852, epoch_train_loss=0.0007309753768073356
Epoch 1853
Epoch 1853 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1853, epoch_train_loss=0.0007309753768073356
Epoch 1854
Epoch 1854 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1854, epoch_train_loss=0.0007309753768073356
Epoch 1855
Epoch 1855 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1855, epoch_train_loss=0.0007309753768073356
Epoch 1856
Epoch 1856 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1856, epoch_train_loss=0.0007309753768073356
Epoch 1857
Epoch 1857 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1857, epoch_train_loss=0.0007309753768073356
Epoch 1858
Epoch 1858 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1858, epoch_train_loss=0.0007309753768073356
Epoch 1859
Epoch 1859 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1859, epoch_train_loss=0.0007309753768073356
Epoch 1860
Epoch 1860 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1860, epoch_train_loss=0.0007309753768073356
Epoch 1861
Epoch 1861 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1861, epoch_train_loss=0.0007309753768073356
Epoch 1862
Epoch 1862 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1862, epoch_train_loss=0.0007309753768073356
Epoch 1863
Epoch 1863 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1863, epoch_train_loss=0.0007309753768073356
Epoch 1864
Epoch 1864 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1864, epoch_train_loss=0.0007309753768073356
Epoch 1865
Epoch 1865 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1865, epoch_train_loss=0.0007309753768073356
Epoch 1866
Epoch 1866 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1866, epoch_train_loss=0.0007309753768073356
Epoch 1867
Epoch 1867 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1867, epoch_train_loss=0.0007309753768073356
Epoch 1868
Epoch 1868 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1868, epoch_train_loss=0.0007309753768073356
Epoch 1869
Epoch 1869 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1869, epoch_train_loss=0.0007309753768073356
Epoch 1870
Epoch 1870 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1870, epoch_train_loss=0.0007309753768073356
Epoch 1871
Epoch 1871 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1871, epoch_train_loss=0.0007309753768073356
Epoch 1872
Epoch 1872 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1872, epoch_train_loss=0.0007309753768073356
Epoch 1873
Epoch 1873 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1873, epoch_train_loss=0.0007309753768073356
Epoch 1874
Epoch 1874 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1874, epoch_train_loss=0.0007309753768073356
Epoch 1875
Epoch 1875 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1875, epoch_train_loss=0.0007309753768073356
Epoch 1876
Epoch 1876 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1876, epoch_train_loss=0.0007309753768073356
Epoch 1877
Epoch 1877 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1877, epoch_train_loss=0.0007309753768073356
Epoch 1878
Epoch 1878 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1878, epoch_train_loss=0.0007309753768073356
Epoch 1879
Epoch 1879 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1879, epoch_train_loss=0.0007309753768073356
Epoch 1880
Epoch 1880 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1880, epoch_train_loss=0.0007309753768073356
Epoch 1881
Epoch 1881 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1881, epoch_train_loss=0.0007309753768073356
Epoch 1882
Epoch 1882 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1882, epoch_train_loss=0.0007309753768073356
Epoch 1883
Epoch 1883 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1883, epoch_train_loss=0.0007309753768073356
Epoch 1884
Epoch 1884 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1884, epoch_train_loss=0.0007309753768073356
Epoch 1885
Epoch 1885 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1885, epoch_train_loss=0.0007309753768073356
Epoch 1886
Epoch 1886 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1886, epoch_train_loss=0.0007309753768073356
Epoch 1887
Epoch 1887 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1887, epoch_train_loss=0.0007309753768073356
Epoch 1888
Epoch 1888 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1888, epoch_train_loss=0.0007309753768073356
Epoch 1889
Epoch 1889 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1889, epoch_train_loss=0.0007309753768073356
Epoch 1890
Epoch 1890 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1890, epoch_train_loss=0.0007309753768073356
Epoch 1891
Epoch 1891 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1891, epoch_train_loss=0.0007309753768073356
Epoch 1892
Epoch 1892 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1892, epoch_train_loss=0.0007309753768073356
Epoch 1893
Epoch 1893 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1893, epoch_train_loss=0.0007309753768073356
Epoch 1894
Epoch 1894 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1894, epoch_train_loss=0.0007309753768073356
Epoch 1895
Epoch 1895 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1895, epoch_train_loss=0.0007309753768073356
Epoch 1896
Epoch 1896 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1896, epoch_train_loss=0.0007309753768073356
Epoch 1897
Epoch 1897 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1897, epoch_train_loss=0.0007309753768073356
Epoch 1898
Epoch 1898 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1898, epoch_train_loss=0.0007309753768073356
Epoch 1899
Epoch 1899 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1899, epoch_train_loss=0.0007309753768073356
Epoch 1900
Epoch 1900 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1900, epoch_train_loss=0.0007309753768073356
Epoch 1901
Epoch 1901 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1901, epoch_train_loss=0.0007309753768073356
Epoch 1902
Epoch 1902 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1902, epoch_train_loss=0.0007309753768073356
Epoch 1903
Epoch 1903 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1903, epoch_train_loss=0.0007309753768073356
Epoch 1904
Epoch 1904 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1904, epoch_train_loss=0.0007309753768073356
Epoch 1905
Epoch 1905 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1905, epoch_train_loss=0.0007309753768073356
Epoch 1906
Epoch 1906 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1906, epoch_train_loss=0.0007309753768073356
Epoch 1907
Epoch 1907 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1907, epoch_train_loss=0.0007309753768073356
Epoch 1908
Epoch 1908 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1908, epoch_train_loss=0.0007309753768073356
Epoch 1909
Epoch 1909 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1909, epoch_train_loss=0.0007309753768073356
Epoch 1910
Epoch 1910 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1910, epoch_train_loss=0.0007309753768073356
Epoch 1911
Epoch 1911 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1911, epoch_train_loss=0.0007309753768073356
Epoch 1912
Epoch 1912 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1912, epoch_train_loss=0.0007309753768073356
Epoch 1913
Epoch 1913 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1913, epoch_train_loss=0.0007309753768073356
Epoch 1914
Epoch 1914 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1914, epoch_train_loss=0.0007309753768073356
Epoch 1915
Epoch 1915 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1915, epoch_train_loss=0.0007309753768073356
Epoch 1916
Epoch 1916 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1916, epoch_train_loss=0.0007309753768073356
Epoch 1917
Epoch 1917 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1917, epoch_train_loss=0.0007309753768073356
Epoch 1918
Epoch 1918 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1918, epoch_train_loss=0.0007309753768073356
Epoch 1919
Epoch 1919 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1919, epoch_train_loss=0.0007309753768073356
Epoch 1920
Epoch 1920 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1920, epoch_train_loss=0.0007309753768073356
Epoch 1921
Epoch 1921 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1921, epoch_train_loss=0.0007309753768073356
Epoch 1922
Epoch 1922 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1922, epoch_train_loss=0.0007309753768073356
Epoch 1923
Epoch 1923 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1923, epoch_train_loss=0.0007309753768073356
Epoch 1924
Epoch 1924 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1924, epoch_train_loss=0.0007309753768073356
Epoch 1925
Epoch 1925 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1925, epoch_train_loss=0.0007309753768073356
Epoch 1926
Epoch 1926 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1926, epoch_train_loss=0.0007309753768073356
Epoch 1927
Epoch 1927 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1927, epoch_train_loss=0.0007309753768073356
Epoch 1928
Epoch 1928 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1928, epoch_train_loss=0.0007309753768073356
Epoch 1929
Epoch 1929 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1929, epoch_train_loss=0.0007309753768073356
Epoch 1930
Epoch 1930 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1930, epoch_train_loss=0.0007309753768073356
Epoch 1931
Epoch 1931 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1931, epoch_train_loss=0.0007309753768073356
Epoch 1932
Epoch 1932 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1932, epoch_train_loss=0.0007309753768073356
Epoch 1933
Epoch 1933 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1933, epoch_train_loss=0.0007309753768073356
Epoch 1934
Epoch 1934 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1934, epoch_train_loss=0.0007309753768073356
Epoch 1935
Epoch 1935 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1935, epoch_train_loss=0.0007309753768073356
Epoch 1936
Epoch 1936 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1936, epoch_train_loss=0.0007309753768073356
Epoch 1937
Epoch 1937 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1937, epoch_train_loss=0.0007309753768073356
Epoch 1938
Epoch 1938 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1938, epoch_train_loss=0.0007309753768073356
Epoch 1939
Epoch 1939 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1939, epoch_train_loss=0.0007309753768073356
Epoch 1940
Epoch 1940 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1940, epoch_train_loss=0.0007309753768073356
Epoch 1941
Epoch 1941 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1941, epoch_train_loss=0.0007309753768073356
Epoch 1942
Epoch 1942 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1942, epoch_train_loss=0.0007309753768073356
Epoch 1943
Epoch 1943 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1943, epoch_train_loss=0.0007309753768073356
Epoch 1944
Epoch 1944 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1944, epoch_train_loss=0.0007309753768073356
Epoch 1945
Epoch 1945 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1945, epoch_train_loss=0.0007309753768073356
Epoch 1946
Epoch 1946 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1946, epoch_train_loss=0.0007309753768073356
Epoch 1947
Epoch 1947 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1947, epoch_train_loss=0.0007309753768073356
Epoch 1948
Epoch 1948 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1948, epoch_train_loss=0.0007309753768073356
Epoch 1949
Epoch 1949 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1949, epoch_train_loss=0.0007309753768073356
Epoch 1950
Epoch 1950 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1950, epoch_train_loss=0.0007309753768073356
Epoch 1951
Epoch 1951 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1951, epoch_train_loss=0.0007309753768073356
Epoch 1952
Epoch 1952 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1952, epoch_train_loss=0.0007309753768073356
Epoch 1953
Epoch 1953 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1953, epoch_train_loss=0.0007309753768073356
Epoch 1954
Epoch 1954 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1954, epoch_train_loss=0.0007309753768073356
Epoch 1955
Epoch 1955 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1955, epoch_train_loss=0.0007309753768073356
Epoch 1956
Epoch 1956 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1956, epoch_train_loss=0.0007309753768073356
Epoch 1957
Epoch 1957 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1957, epoch_train_loss=0.0007309753768073356
Epoch 1958
Epoch 1958 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1958, epoch_train_loss=0.0007309753768073356
Epoch 1959
Epoch 1959 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1959, epoch_train_loss=0.0007309753768073356
Epoch 1960
Epoch 1960 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1960, epoch_train_loss=0.0007309753768073356
Epoch 1961
Epoch 1961 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1961, epoch_train_loss=0.0007309753768073356
Epoch 1962
Epoch 1962 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1962, epoch_train_loss=0.0007309753768073356
Epoch 1963
Epoch 1963 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1963, epoch_train_loss=0.0007309753768073356
Epoch 1964
Epoch 1964 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1964, epoch_train_loss=0.0007309753768073356
Epoch 1965
Epoch 1965 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1965, epoch_train_loss=0.0007309753768073356
Epoch 1966
Epoch 1966 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1966, epoch_train_loss=0.0007309753768073356
Epoch 1967
Epoch 1967 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1967, epoch_train_loss=0.0007309753768073356
Epoch 1968
Epoch 1968 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1968, epoch_train_loss=0.0007309753768073356
Epoch 1969
Epoch 1969 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1969, epoch_train_loss=0.0007309753768073356
Epoch 1970
Epoch 1970 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1970, epoch_train_loss=0.0007309753768073356
Epoch 1971
Epoch 1971 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1971, epoch_train_loss=0.0007309753768073356
Epoch 1972
Epoch 1972 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1972, epoch_train_loss=0.0007309753768073356
Epoch 1973
Epoch 1973 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1973, epoch_train_loss=0.0007309753768073356
Epoch 1974
Epoch 1974 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1974, epoch_train_loss=0.0007309753768073356
Epoch 1975
Epoch 1975 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1975, epoch_train_loss=0.0007309753768073356
Epoch 1976
Epoch 1976 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1976, epoch_train_loss=0.0007309753768073356
Epoch 1977
Epoch 1977 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1977, epoch_train_loss=0.0007309753768073356
Epoch 1978
Epoch 1978 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1978, epoch_train_loss=0.0007309753768073356
Epoch 1979
Epoch 1979 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1979, epoch_train_loss=0.0007309753768073356
Epoch 1980
Epoch 1980 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1980, epoch_train_loss=0.0007309753768073356
Epoch 1981
Epoch 1981 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1981, epoch_train_loss=0.0007309753768073356
Epoch 1982
Epoch 1982 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1982, epoch_train_loss=0.0007309753768073356
Epoch 1983
Epoch 1983 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1983, epoch_train_loss=0.0007309753768073356
Epoch 1984
Epoch 1984 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1984, epoch_train_loss=0.0007309753768073356
Epoch 1985
Epoch 1985 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1985, epoch_train_loss=0.0007309753768073356
Epoch 1986
Epoch 1986 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1986, epoch_train_loss=0.0007309753768073356
Epoch 1987
Epoch 1987 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1987, epoch_train_loss=0.0007309753768073356
Epoch 1988
Epoch 1988 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1988, epoch_train_loss=0.0007309753768073356
Epoch 1989
Epoch 1989 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1989, epoch_train_loss=0.0007309753768073356
Epoch 1990
Epoch 1990 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1990, epoch_train_loss=0.0007309753768073356
Epoch 1991
Epoch 1991 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1991, epoch_train_loss=0.0007309753768073356
Epoch 1992
Epoch 1992 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1992, epoch_train_loss=0.0007309753768073356
Epoch 1993
Epoch 1993 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1993, epoch_train_loss=0.0007309753768073356
Epoch 1994
Epoch 1994 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1994, epoch_train_loss=0.0007309753768073356
Epoch 1995
Epoch 1995 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1995, epoch_train_loss=0.0007309753768073356
Epoch 1996
Epoch 1996 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1996, epoch_train_loss=0.0007309753768073356
Epoch 1997
Epoch 1997 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1997, epoch_train_loss=0.0007309753768073356
Epoch 1998
Epoch 1998 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1998, epoch_train_loss=0.0007309753768073356
Epoch 1999
Epoch 1999 :: Batch 0/1
Batch Loss = 0.0007309753768073356
1999, epoch_train_loss=0.0007309753768073356
Epoch 2000
Epoch 2000 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2000, epoch_train_loss=0.0007309753768073356
Epoch 2001
Epoch 2001 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2001, epoch_train_loss=0.0007309753768073356
Epoch 2002
Epoch 2002 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2002, epoch_train_loss=0.0007309753768073356
Epoch 2003
Epoch 2003 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2003, epoch_train_loss=0.0007309753768073356
Epoch 2004
Epoch 2004 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2004, epoch_train_loss=0.0007309753768073356
Epoch 2005
Epoch 2005 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2005, epoch_train_loss=0.0007309753768073356
Epoch 2006
Epoch 2006 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2006, epoch_train_loss=0.0007309753768073356
Epoch 2007
Epoch 2007 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2007, epoch_train_loss=0.0007309753768073356
Epoch 2008
Epoch 2008 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2008, epoch_train_loss=0.0007309753768073356
Epoch 2009
Epoch 2009 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2009, epoch_train_loss=0.0007309753768073356
Epoch 2010
Epoch 2010 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2010, epoch_train_loss=0.0007309753768073356
Epoch 2011
Epoch 2011 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2011, epoch_train_loss=0.0007309753768073356
Epoch 2012
Epoch 2012 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2012, epoch_train_loss=0.0007309753768073356
Epoch 2013
Epoch 2013 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2013, epoch_train_loss=0.0007309753768073356
Epoch 2014
Epoch 2014 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2014, epoch_train_loss=0.0007309753768073356
Epoch 2015
Epoch 2015 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2015, epoch_train_loss=0.0007309753768073356
Epoch 2016
Epoch 2016 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2016, epoch_train_loss=0.0007309753768073356
Epoch 2017
Epoch 2017 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2017, epoch_train_loss=0.0007309753768073356
Epoch 2018
Epoch 2018 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2018, epoch_train_loss=0.0007309753768073356
Epoch 2019
Epoch 2019 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2019, epoch_train_loss=0.0007309753768073356
Epoch 2020
Epoch 2020 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2020, epoch_train_loss=0.0007309753768073356
Epoch 2021
Epoch 2021 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2021, epoch_train_loss=0.0007309753768073356
Epoch 2022
Epoch 2022 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2022, epoch_train_loss=0.0007309753768073356
Epoch 2023
Epoch 2023 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2023, epoch_train_loss=0.0007309753768073356
Epoch 2024
Epoch 2024 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2024, epoch_train_loss=0.0007309753768073356
Epoch 2025
Epoch 2025 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2025, epoch_train_loss=0.0007309753768073356
Epoch 2026
Epoch 2026 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2026, epoch_train_loss=0.0007309753768073356
Epoch 2027
Epoch 2027 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2027, epoch_train_loss=0.0007309753768073356
Epoch 2028
Epoch 2028 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2028, epoch_train_loss=0.0007309753768073356
Epoch 2029
Epoch 2029 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2029, epoch_train_loss=0.0007309753768073356
Epoch 2030
Epoch 2030 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2030, epoch_train_loss=0.0007309753768073356
Epoch 2031
Epoch 2031 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2031, epoch_train_loss=0.0007309753768073356
Epoch 2032
Epoch 2032 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2032, epoch_train_loss=0.0007309753768073356
Epoch 2033
Epoch 2033 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2033, epoch_train_loss=0.0007309753768073356
Epoch 2034
Epoch 2034 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2034, epoch_train_loss=0.0007309753768073356
Epoch 2035
Epoch 2035 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2035, epoch_train_loss=0.0007309753768073356
Epoch 2036
Epoch 2036 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2036, epoch_train_loss=0.0007309753768073356
Epoch 2037
Epoch 2037 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2037, epoch_train_loss=0.0007309753768073356
Epoch 2038
Epoch 2038 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2038, epoch_train_loss=0.0007309753768073356
Epoch 2039
Epoch 2039 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2039, epoch_train_loss=0.0007309753768073356
Epoch 2040
Epoch 2040 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2040, epoch_train_loss=0.0007309753768073356
Epoch 2041
Epoch 2041 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2041, epoch_train_loss=0.0007309753768073356
Epoch 2042
Epoch 2042 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2042, epoch_train_loss=0.0007309753768073356
Epoch 2043
Epoch 2043 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2043, epoch_train_loss=0.0007309753768073356
Epoch 2044
Epoch 2044 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2044, epoch_train_loss=0.0007309753768073356
Epoch 2045
Epoch 2045 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2045, epoch_train_loss=0.0007309753768073356
Epoch 2046
Epoch 2046 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2046, epoch_train_loss=0.0007309753768073356
Epoch 2047
Epoch 2047 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2047, epoch_train_loss=0.0007309753768073356
Epoch 2048
Epoch 2048 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2048, epoch_train_loss=0.0007309753768073356
Epoch 2049
Epoch 2049 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2049, epoch_train_loss=0.0007309753768073356
Epoch 2050
Epoch 2050 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2050, epoch_train_loss=0.0007309753768073356
Epoch 2051
Epoch 2051 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2051, epoch_train_loss=0.0007309753768073356
Epoch 2052
Epoch 2052 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2052, epoch_train_loss=0.0007309753768073356
Epoch 2053
Epoch 2053 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2053, epoch_train_loss=0.0007309753768073356
Epoch 2054
Epoch 2054 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2054, epoch_train_loss=0.0007309753768073356
Epoch 2055
Epoch 2055 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2055, epoch_train_loss=0.0007309753768073356
Epoch 2056
Epoch 2056 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2056, epoch_train_loss=0.0007309753768073356
Epoch 2057
Epoch 2057 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2057, epoch_train_loss=0.0007309753768073356
Epoch 2058
Epoch 2058 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2058, epoch_train_loss=0.0007309753768073356
Epoch 2059
Epoch 2059 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2059, epoch_train_loss=0.0007309753768073356
Epoch 2060
Epoch 2060 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2060, epoch_train_loss=0.0007309753768073356
Epoch 2061
Epoch 2061 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2061, epoch_train_loss=0.0007309753768073356
Epoch 2062
Epoch 2062 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2062, epoch_train_loss=0.0007309753768073356
Epoch 2063
Epoch 2063 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2063, epoch_train_loss=0.0007309753768073356
Epoch 2064
Epoch 2064 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2064, epoch_train_loss=0.0007309753768073356
Epoch 2065
Epoch 2065 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2065, epoch_train_loss=0.0007309753768073356
Epoch 2066
Epoch 2066 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2066, epoch_train_loss=0.0007309753768073356
Epoch 2067
Epoch 2067 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2067, epoch_train_loss=0.0007309753768073356
Epoch 2068
Epoch 2068 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2068, epoch_train_loss=0.0007309753768073356
Epoch 2069
Epoch 2069 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2069, epoch_train_loss=0.0007309753768073356
Epoch 2070
Epoch 2070 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2070, epoch_train_loss=0.0007309753768073356
Epoch 2071
Epoch 2071 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2071, epoch_train_loss=0.0007309753768073356
Epoch 2072
Epoch 2072 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2072, epoch_train_loss=0.0007309753768073356
Epoch 2073
Epoch 2073 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2073, epoch_train_loss=0.0007309753768073356
Epoch 2074
Epoch 2074 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2074, epoch_train_loss=0.0007309753768073356
Epoch 2075
Epoch 2075 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2075, epoch_train_loss=0.0007309753768073356
Epoch 2076
Epoch 2076 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2076, epoch_train_loss=0.0007309753768073356
Epoch 2077
Epoch 2077 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2077, epoch_train_loss=0.0007309753768073356
Epoch 2078
Epoch 2078 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2078, epoch_train_loss=0.0007309753768073356
Epoch 2079
Epoch 2079 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2079, epoch_train_loss=0.0007309753768073356
Epoch 2080
Epoch 2080 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2080, epoch_train_loss=0.0007309753768073356
Epoch 2081
Epoch 2081 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2081, epoch_train_loss=0.0007309753768073356
Epoch 2082
Epoch 2082 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2082, epoch_train_loss=0.0007309753768073356
Epoch 2083
Epoch 2083 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2083, epoch_train_loss=0.0007309753768073356
Epoch 2084
Epoch 2084 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2084, epoch_train_loss=0.0007309753768073356
Epoch 2085
Epoch 2085 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2085, epoch_train_loss=0.0007309753768073356
Epoch 2086
Epoch 2086 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2086, epoch_train_loss=0.0007309753768073356
Epoch 2087
Epoch 2087 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2087, epoch_train_loss=0.0007309753768073356
Epoch 2088
Epoch 2088 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2088, epoch_train_loss=0.0007309753768073356
Epoch 2089
Epoch 2089 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2089, epoch_train_loss=0.0007309753768073356
Epoch 2090
Epoch 2090 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2090, epoch_train_loss=0.0007309753768073356
Epoch 2091
Epoch 2091 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2091, epoch_train_loss=0.0007309753768073356
Epoch 2092
Epoch 2092 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2092, epoch_train_loss=0.0007309753768073356
Epoch 2093
Epoch 2093 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2093, epoch_train_loss=0.0007309753768073356
Epoch 2094
Epoch 2094 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2094, epoch_train_loss=0.0007309753768073356
Epoch 2095
Epoch 2095 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2095, epoch_train_loss=0.0007309753768073356
Epoch 2096
Epoch 2096 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2096, epoch_train_loss=0.0007309753768073356
Epoch 2097
Epoch 2097 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2097, epoch_train_loss=0.0007309753768073356
Epoch 2098
Epoch 2098 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2098, epoch_train_loss=0.0007309753768073356
Epoch 2099
Epoch 2099 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2099, epoch_train_loss=0.0007309753768073356
Epoch 2100
Epoch 2100 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2100, epoch_train_loss=0.0007309753768073356
Epoch 2101
Epoch 2101 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2101, epoch_train_loss=0.0007309753768073356
Epoch 2102
Epoch 2102 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2102, epoch_train_loss=0.0007309753768073356
Epoch 2103
Epoch 2103 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2103, epoch_train_loss=0.0007309753768073356
Epoch 2104
Epoch 2104 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2104, epoch_train_loss=0.0007309753768073356
Epoch 2105
Epoch 2105 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2105, epoch_train_loss=0.0007309753768073356
Epoch 2106
Epoch 2106 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2106, epoch_train_loss=0.0007309753768073356
Epoch 2107
Epoch 2107 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2107, epoch_train_loss=0.0007309753768073356
Epoch 2108
Epoch 2108 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2108, epoch_train_loss=0.0007309753768073356
Epoch 2109
Epoch 2109 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2109, epoch_train_loss=0.0007309753768073356
Epoch 2110
Epoch 2110 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2110, epoch_train_loss=0.0007309753768073356
Epoch 2111
Epoch 2111 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2111, epoch_train_loss=0.0007309753768073356
Epoch 2112
Epoch 2112 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2112, epoch_train_loss=0.0007309753768073356
Epoch 2113
Epoch 2113 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2113, epoch_train_loss=0.0007309753768073356
Epoch 2114
Epoch 2114 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2114, epoch_train_loss=0.0007309753768073356
Epoch 2115
Epoch 2115 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2115, epoch_train_loss=0.0007309753768073356
Epoch 2116
Epoch 2116 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2116, epoch_train_loss=0.0007309753768073356
Epoch 2117
Epoch 2117 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2117, epoch_train_loss=0.0007309753768073356
Epoch 2118
Epoch 2118 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2118, epoch_train_loss=0.0007309753768073356
Epoch 2119
Epoch 2119 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2119, epoch_train_loss=0.0007309753768073356
Epoch 2120
Epoch 2120 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2120, epoch_train_loss=0.0007309753768073356
Epoch 2121
Epoch 2121 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2121, epoch_train_loss=0.0007309753768073356
Epoch 2122
Epoch 2122 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2122, epoch_train_loss=0.0007309753768073356
Epoch 2123
Epoch 2123 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2123, epoch_train_loss=0.0007309753768073356
Epoch 2124
Epoch 2124 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2124, epoch_train_loss=0.0007309753768073356
Epoch 2125
Epoch 2125 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2125, epoch_train_loss=0.0007309753768073356
Epoch 2126
Epoch 2126 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2126, epoch_train_loss=0.0007309753768073356
Epoch 2127
Epoch 2127 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2127, epoch_train_loss=0.0007309753768073356
Epoch 2128
Epoch 2128 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2128, epoch_train_loss=0.0007309753768073356
Epoch 2129
Epoch 2129 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2129, epoch_train_loss=0.0007309753768073356
Epoch 2130
Epoch 2130 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2130, epoch_train_loss=0.0007309753768073356
Epoch 2131
Epoch 2131 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2131, epoch_train_loss=0.0007309753768073356
Epoch 2132
Epoch 2132 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2132, epoch_train_loss=0.0007309753768073356
Epoch 2133
Epoch 2133 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2133, epoch_train_loss=0.0007309753768073356
Epoch 2134
Epoch 2134 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2134, epoch_train_loss=0.0007309753768073356
Epoch 2135
Epoch 2135 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2135, epoch_train_loss=0.0007309753768073356
Epoch 2136
Epoch 2136 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2136, epoch_train_loss=0.0007309753768073356
Epoch 2137
Epoch 2137 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2137, epoch_train_loss=0.0007309753768073356
Epoch 2138
Epoch 2138 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2138, epoch_train_loss=0.0007309753768073356
Epoch 2139
Epoch 2139 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2139, epoch_train_loss=0.0007309753768073356
Epoch 2140
Epoch 2140 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2140, epoch_train_loss=0.0007309753768073356
Epoch 2141
Epoch 2141 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2141, epoch_train_loss=0.0007309753768073356
Epoch 2142
Epoch 2142 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2142, epoch_train_loss=0.0007309753768073356
Epoch 2143
Epoch 2143 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2143, epoch_train_loss=0.0007309753768073356
Epoch 2144
Epoch 2144 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2144, epoch_train_loss=0.0007309753768073356
Epoch 2145
Epoch 2145 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2145, epoch_train_loss=0.0007309753768073356
Epoch 2146
Epoch 2146 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2146, epoch_train_loss=0.0007309753768073356
Epoch 2147
Epoch 2147 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2147, epoch_train_loss=0.0007309753768073356
Epoch 2148
Epoch 2148 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2148, epoch_train_loss=0.0007309753768073356
Epoch 2149
Epoch 2149 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2149, epoch_train_loss=0.0007309753768073356
Epoch 2150
Epoch 2150 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2150, epoch_train_loss=0.0007309753768073356
Epoch 2151
Epoch 2151 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2151, epoch_train_loss=0.0007309753768073356
Epoch 2152
Epoch 2152 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2152, epoch_train_loss=0.0007309753768073356
Epoch 2153
Epoch 2153 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2153, epoch_train_loss=0.0007309753768073356
Epoch 2154
Epoch 2154 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2154, epoch_train_loss=0.0007309753768073356
Epoch 2155
Epoch 2155 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2155, epoch_train_loss=0.0007309753768073356
Epoch 2156
Epoch 2156 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2156, epoch_train_loss=0.0007309753768073356
Epoch 2157
Epoch 2157 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2157, epoch_train_loss=0.0007309753768073356
Epoch 2158
Epoch 2158 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2158, epoch_train_loss=0.0007309753768073356
Epoch 2159
Epoch 2159 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2159, epoch_train_loss=0.0007309753768073356
Epoch 2160
Epoch 2160 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2160, epoch_train_loss=0.0007309753768073356
Epoch 2161
Epoch 2161 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2161, epoch_train_loss=0.0007309753768073356
Epoch 2162
Epoch 2162 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2162, epoch_train_loss=0.0007309753768073356
Epoch 2163
Epoch 2163 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2163, epoch_train_loss=0.0007309753768073356
Epoch 2164
Epoch 2164 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2164, epoch_train_loss=0.0007309753768073356
Epoch 2165
Epoch 2165 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2165, epoch_train_loss=0.0007309753768073356
Epoch 2166
Epoch 2166 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2166, epoch_train_loss=0.0007309753768073356
Epoch 2167
Epoch 2167 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2167, epoch_train_loss=0.0007309753768073356
Epoch 2168
Epoch 2168 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2168, epoch_train_loss=0.0007309753768073356
Epoch 2169
Epoch 2169 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2169, epoch_train_loss=0.0007309753768073356
Epoch 2170
Epoch 2170 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2170, epoch_train_loss=0.0007309753768073356
Epoch 2171
Epoch 2171 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2171, epoch_train_loss=0.0007309753768073356
Epoch 2172
Epoch 2172 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2172, epoch_train_loss=0.0007309753768073356
Epoch 2173
Epoch 2173 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2173, epoch_train_loss=0.0007309753768073356
Epoch 2174
Epoch 2174 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2174, epoch_train_loss=0.0007309753768073356
Epoch 2175
Epoch 2175 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2175, epoch_train_loss=0.0007309753768073356
Epoch 2176
Epoch 2176 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2176, epoch_train_loss=0.0007309753768073356
Epoch 2177
Epoch 2177 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2177, epoch_train_loss=0.0007309753768073356
Epoch 2178
Epoch 2178 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2178, epoch_train_loss=0.0007309753768073356
Epoch 2179
Epoch 2179 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2179, epoch_train_loss=0.0007309753768073356
Epoch 2180
Epoch 2180 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2180, epoch_train_loss=0.0007309753768073356
Epoch 2181
Epoch 2181 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2181, epoch_train_loss=0.0007309753768073356
Epoch 2182
Epoch 2182 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2182, epoch_train_loss=0.0007309753768073356
Epoch 2183
Epoch 2183 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2183, epoch_train_loss=0.0007309753768073356
Epoch 2184
Epoch 2184 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2184, epoch_train_loss=0.0007309753768073356
Epoch 2185
Epoch 2185 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2185, epoch_train_loss=0.0007309753768073356
Epoch 2186
Epoch 2186 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2186, epoch_train_loss=0.0007309753768073356
Epoch 2187
Epoch 2187 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2187, epoch_train_loss=0.0007309753768073356
Epoch 2188
Epoch 2188 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2188, epoch_train_loss=0.0007309753768073356
Epoch 2189
Epoch 2189 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2189, epoch_train_loss=0.0007309753768073356
Epoch 2190
Epoch 2190 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2190, epoch_train_loss=0.0007309753768073356
Epoch 2191
Epoch 2191 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2191, epoch_train_loss=0.0007309753768073356
Epoch 2192
Epoch 2192 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2192, epoch_train_loss=0.0007309753768073356
Epoch 2193
Epoch 2193 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2193, epoch_train_loss=0.0007309753768073356
Epoch 2194
Epoch 2194 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2194, epoch_train_loss=0.0007309753768073356
Epoch 2195
Epoch 2195 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2195, epoch_train_loss=0.0007309753768073356
Epoch 2196
Epoch 2196 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2196, epoch_train_loss=0.0007309753768073356
Epoch 2197
Epoch 2197 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2197, epoch_train_loss=0.0007309753768073356
Epoch 2198
Epoch 2198 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2198, epoch_train_loss=0.0007309753768073356
Epoch 2199
Epoch 2199 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2199, epoch_train_loss=0.0007309753768073356
Epoch 2200
Epoch 2200 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2200, epoch_train_loss=0.0007309753768073356
Epoch 2201
Epoch 2201 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2201, epoch_train_loss=0.0007309753768073356
Epoch 2202
Epoch 2202 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2202, epoch_train_loss=0.0007309753768073356
Epoch 2203
Epoch 2203 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2203, epoch_train_loss=0.0007309753768073356
Epoch 2204
Epoch 2204 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2204, epoch_train_loss=0.0007309753768073356
Epoch 2205
Epoch 2205 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2205, epoch_train_loss=0.0007309753768073356
Epoch 2206
Epoch 2206 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2206, epoch_train_loss=0.0007309753768073356
Epoch 2207
Epoch 2207 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2207, epoch_train_loss=0.0007309753768073356
Epoch 2208
Epoch 2208 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2208, epoch_train_loss=0.0007309753768073356
Epoch 2209
Epoch 2209 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2209, epoch_train_loss=0.0007309753768073356
Epoch 2210
Epoch 2210 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2210, epoch_train_loss=0.0007309753768073356
Epoch 2211
Epoch 2211 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2211, epoch_train_loss=0.0007309753768073356
Epoch 2212
Epoch 2212 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2212, epoch_train_loss=0.0007309753768073356
Epoch 2213
Epoch 2213 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2213, epoch_train_loss=0.0007309753768073356
Epoch 2214
Epoch 2214 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2214, epoch_train_loss=0.0007309753768073356
Epoch 2215
Epoch 2215 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2215, epoch_train_loss=0.0007309753768073356
Epoch 2216
Epoch 2216 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2216, epoch_train_loss=0.0007309753768073356
Epoch 2217
Epoch 2217 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2217, epoch_train_loss=0.0007309753768073356
Epoch 2218
Epoch 2218 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2218, epoch_train_loss=0.0007309753768073356
Epoch 2219
Epoch 2219 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2219, epoch_train_loss=0.0007309753768073356
Epoch 2220
Epoch 2220 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2220, epoch_train_loss=0.0007309753768073356
Epoch 2221
Epoch 2221 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2221, epoch_train_loss=0.0007309753768073356
Epoch 2222
Epoch 2222 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2222, epoch_train_loss=0.0007309753768073356
Epoch 2223
Epoch 2223 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2223, epoch_train_loss=0.0007309753768073356
Epoch 2224
Epoch 2224 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2224, epoch_train_loss=0.0007309753768073356
Epoch 2225
Epoch 2225 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2225, epoch_train_loss=0.0007309753768073356
Epoch 2226
Epoch 2226 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2226, epoch_train_loss=0.0007309753768073356
Epoch 2227
Epoch 2227 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2227, epoch_train_loss=0.0007309753768073356
Epoch 2228
Epoch 2228 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2228, epoch_train_loss=0.0007309753768073356
Epoch 2229
Epoch 2229 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2229, epoch_train_loss=0.0007309753768073356
Epoch 2230
Epoch 2230 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2230, epoch_train_loss=0.0007309753768073356
Epoch 2231
Epoch 2231 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2231, epoch_train_loss=0.0007309753768073356
Epoch 2232
Epoch 2232 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2232, epoch_train_loss=0.0007309753768073356
Epoch 2233
Epoch 2233 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2233, epoch_train_loss=0.0007309753768073356
Epoch 2234
Epoch 2234 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2234, epoch_train_loss=0.0007309753768073356
Epoch 2235
Epoch 2235 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2235, epoch_train_loss=0.0007309753768073356
Epoch 2236
Epoch 2236 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2236, epoch_train_loss=0.0007309753768073356
Epoch 2237
Epoch 2237 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2237, epoch_train_loss=0.0007309753768073356
Epoch 2238
Epoch 2238 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2238, epoch_train_loss=0.0007309753768073356
Epoch 2239
Epoch 2239 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2239, epoch_train_loss=0.0007309753768073356
Epoch 2240
Epoch 2240 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2240, epoch_train_loss=0.0007309753768073356
Epoch 2241
Epoch 2241 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2241, epoch_train_loss=0.0007309753768073356
Epoch 2242
Epoch 2242 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2242, epoch_train_loss=0.0007309753768073356
Epoch 2243
Epoch 2243 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2243, epoch_train_loss=0.0007309753768073356
Epoch 2244
Epoch 2244 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2244, epoch_train_loss=0.0007309753768073356
Epoch 2245
Epoch 2245 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2245, epoch_train_loss=0.0007309753768073356
Epoch 2246
Epoch 2246 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2246, epoch_train_loss=0.0007309753768073356
Epoch 2247
Epoch 2247 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2247, epoch_train_loss=0.0007309753768073356
Epoch 2248
Epoch 2248 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2248, epoch_train_loss=0.0007309753768073356
Epoch 2249
Epoch 2249 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2249, epoch_train_loss=0.0007309753768073356
Epoch 2250
Epoch 2250 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2250, epoch_train_loss=0.0007309753768073356
Epoch 2251
Epoch 2251 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2251, epoch_train_loss=0.0007309753768073356
Epoch 2252
Epoch 2252 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2252, epoch_train_loss=0.0007309753768073356
Epoch 2253
Epoch 2253 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2253, epoch_train_loss=0.0007309753768073356
Epoch 2254
Epoch 2254 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2254, epoch_train_loss=0.0007309753768073356
Epoch 2255
Epoch 2255 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2255, epoch_train_loss=0.0007309753768073356
Epoch 2256
Epoch 2256 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2256, epoch_train_loss=0.0007309753768073356
Epoch 2257
Epoch 2257 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2257, epoch_train_loss=0.0007309753768073356
Epoch 2258
Epoch 2258 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2258, epoch_train_loss=0.0007309753768073356
Epoch 2259
Epoch 2259 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2259, epoch_train_loss=0.0007309753768073356
Epoch 2260
Epoch 2260 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2260, epoch_train_loss=0.0007309753768073356
Epoch 2261
Epoch 2261 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2261, epoch_train_loss=0.0007309753768073356
Epoch 2262
Epoch 2262 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2262, epoch_train_loss=0.0007309753768073356
Epoch 2263
Epoch 2263 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2263, epoch_train_loss=0.0007309753768073356
Epoch 2264
Epoch 2264 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2264, epoch_train_loss=0.0007309753768073356
Epoch 2265
Epoch 2265 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2265, epoch_train_loss=0.0007309753768073356
Epoch 2266
Epoch 2266 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2266, epoch_train_loss=0.0007309753768073356
Epoch 2267
Epoch 2267 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2267, epoch_train_loss=0.0007309753768073356
Epoch 2268
Epoch 2268 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2268, epoch_train_loss=0.0007309753768073356
Epoch 2269
Epoch 2269 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2269, epoch_train_loss=0.0007309753768073356
Epoch 2270
Epoch 2270 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2270, epoch_train_loss=0.0007309753768073356
Epoch 2271
Epoch 2271 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2271, epoch_train_loss=0.0007309753768073356
Epoch 2272
Epoch 2272 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2272, epoch_train_loss=0.0007309753768073356
Epoch 2273
Epoch 2273 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2273, epoch_train_loss=0.0007309753768073356
Epoch 2274
Epoch 2274 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2274, epoch_train_loss=0.0007309753768073356
Epoch 2275
Epoch 2275 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2275, epoch_train_loss=0.0007309753768073356
Epoch 2276
Epoch 2276 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2276, epoch_train_loss=0.0007309753768073356
Epoch 2277
Epoch 2277 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2277, epoch_train_loss=0.0007309753768073356
Epoch 2278
Epoch 2278 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2278, epoch_train_loss=0.0007309753768073356
Epoch 2279
Epoch 2279 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2279, epoch_train_loss=0.0007309753768073356
Epoch 2280
Epoch 2280 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2280, epoch_train_loss=0.0007309753768073356
Epoch 2281
Epoch 2281 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2281, epoch_train_loss=0.0007309753768073356
Epoch 2282
Epoch 2282 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2282, epoch_train_loss=0.0007309753768073356
Epoch 2283
Epoch 2283 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2283, epoch_train_loss=0.0007309753768073356
Epoch 2284
Epoch 2284 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2284, epoch_train_loss=0.0007309753768073356
Epoch 2285
Epoch 2285 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2285, epoch_train_loss=0.0007309753768073356
Epoch 2286
Epoch 2286 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2286, epoch_train_loss=0.0007309753768073356
Epoch 2287
Epoch 2287 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2287, epoch_train_loss=0.0007309753768073356
Epoch 2288
Epoch 2288 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2288, epoch_train_loss=0.0007309753768073356
Epoch 2289
Epoch 2289 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2289, epoch_train_loss=0.0007309753768073356
Epoch 2290
Epoch 2290 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2290, epoch_train_loss=0.0007309753768073356
Epoch 2291
Epoch 2291 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2291, epoch_train_loss=0.0007309753768073356
Epoch 2292
Epoch 2292 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2292, epoch_train_loss=0.0007309753768073356
Epoch 2293
Epoch 2293 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2293, epoch_train_loss=0.0007309753768073356
Epoch 2294
Epoch 2294 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2294, epoch_train_loss=0.0007309753768073356
Epoch 2295
Epoch 2295 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2295, epoch_train_loss=0.0007309753768073356
Epoch 2296
Epoch 2296 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2296, epoch_train_loss=0.0007309753768073356
Epoch 2297
Epoch 2297 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2297, epoch_train_loss=0.0007309753768073356
Epoch 2298
Epoch 2298 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2298, epoch_train_loss=0.0007309753768073356
Epoch 2299
Epoch 2299 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2299, epoch_train_loss=0.0007309753768073356
Epoch 2300
Epoch 2300 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2300, epoch_train_loss=0.0007309753768073356
Epoch 2301
Epoch 2301 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2301, epoch_train_loss=0.0007309753768073356
Epoch 2302
Epoch 2302 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2302, epoch_train_loss=0.0007309753768073356
Epoch 2303
Epoch 2303 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2303, epoch_train_loss=0.0007309753768073356
Epoch 2304
Epoch 2304 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2304, epoch_train_loss=0.0007309753768073356
Epoch 2305
Epoch 2305 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2305, epoch_train_loss=0.0007309753768073356
Epoch 2306
Epoch 2306 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2306, epoch_train_loss=0.0007309753768073356
Epoch 2307
Epoch 2307 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2307, epoch_train_loss=0.0007309753768073356
Epoch 2308
Epoch 2308 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2308, epoch_train_loss=0.0007309753768073356
Epoch 2309
Epoch 2309 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2309, epoch_train_loss=0.0007309753768073356
Epoch 2310
Epoch 2310 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2310, epoch_train_loss=0.0007309753768073356
Epoch 2311
Epoch 2311 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2311, epoch_train_loss=0.0007309753768073356
Epoch 2312
Epoch 2312 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2312, epoch_train_loss=0.0007309753768073356
Epoch 2313
Epoch 2313 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2313, epoch_train_loss=0.0007309753768073356
Epoch 2314
Epoch 2314 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2314, epoch_train_loss=0.0007309753768073356
Epoch 2315
Epoch 2315 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2315, epoch_train_loss=0.0007309753768073356
Epoch 2316
Epoch 2316 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2316, epoch_train_loss=0.0007309753768073356
Epoch 2317
Epoch 2317 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2317, epoch_train_loss=0.0007309753768073356
Epoch 2318
Epoch 2318 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2318, epoch_train_loss=0.0007309753768073356
Epoch 2319
Epoch 2319 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2319, epoch_train_loss=0.0007309753768073356
Epoch 2320
Epoch 2320 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2320, epoch_train_loss=0.0007309753768073356
Epoch 2321
Epoch 2321 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2321, epoch_train_loss=0.0007309753768073356
Epoch 2322
Epoch 2322 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2322, epoch_train_loss=0.0007309753768073356
Epoch 2323
Epoch 2323 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2323, epoch_train_loss=0.0007309753768073356
Epoch 2324
Epoch 2324 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2324, epoch_train_loss=0.0007309753768073356
Epoch 2325
Epoch 2325 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2325, epoch_train_loss=0.0007309753768073356
Epoch 2326
Epoch 2326 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2326, epoch_train_loss=0.0007309753768073356
Epoch 2327
Epoch 2327 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2327, epoch_train_loss=0.0007309753768073356
Epoch 2328
Epoch 2328 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2328, epoch_train_loss=0.0007309753768073356
Epoch 2329
Epoch 2329 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2329, epoch_train_loss=0.0007309753768073356
Epoch 2330
Epoch 2330 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2330, epoch_train_loss=0.0007309753768073356
Epoch 2331
Epoch 2331 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2331, epoch_train_loss=0.0007309753768073356
Epoch 2332
Epoch 2332 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2332, epoch_train_loss=0.0007309753768073356
Epoch 2333
Epoch 2333 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2333, epoch_train_loss=0.0007309753768073356
Epoch 2334
Epoch 2334 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2334, epoch_train_loss=0.0007309753768073356
Epoch 2335
Epoch 2335 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2335, epoch_train_loss=0.0007309753768073356
Epoch 2336
Epoch 2336 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2336, epoch_train_loss=0.0007309753768073356
Epoch 2337
Epoch 2337 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2337, epoch_train_loss=0.0007309753768073356
Epoch 2338
Epoch 2338 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2338, epoch_train_loss=0.0007309753768073356
Epoch 2339
Epoch 2339 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2339, epoch_train_loss=0.0007309753768073356
Epoch 2340
Epoch 2340 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2340, epoch_train_loss=0.0007309753768073356
Epoch 2341
Epoch 2341 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2341, epoch_train_loss=0.0007309753768073356
Epoch 2342
Epoch 2342 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2342, epoch_train_loss=0.0007309753768073356
Epoch 2343
Epoch 2343 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2343, epoch_train_loss=0.0007309753768073356
Epoch 2344
Epoch 2344 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2344, epoch_train_loss=0.0007309753768073356
Epoch 2345
Epoch 2345 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2345, epoch_train_loss=0.0007309753768073356
Epoch 2346
Epoch 2346 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2346, epoch_train_loss=0.0007309753768073356
Epoch 2347
Epoch 2347 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2347, epoch_train_loss=0.0007309753768073356
Epoch 2348
Epoch 2348 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2348, epoch_train_loss=0.0007309753768073356
Epoch 2349
Epoch 2349 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2349, epoch_train_loss=0.0007309753768073356
Epoch 2350
Epoch 2350 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2350, epoch_train_loss=0.0007309753768073356
Epoch 2351
Epoch 2351 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2351, epoch_train_loss=0.0007309753768073356
Epoch 2352
Epoch 2352 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2352, epoch_train_loss=0.0007309753768073356
Epoch 2353
Epoch 2353 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2353, epoch_train_loss=0.0007309753768073356
Epoch 2354
Epoch 2354 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2354, epoch_train_loss=0.0007309753768073356
Epoch 2355
Epoch 2355 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2355, epoch_train_loss=0.0007309753768073356
Epoch 2356
Epoch 2356 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2356, epoch_train_loss=0.0007309753768073356
Epoch 2357
Epoch 2357 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2357, epoch_train_loss=0.0007309753768073356
Epoch 2358
Epoch 2358 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2358, epoch_train_loss=0.0007309753768073356
Epoch 2359
Epoch 2359 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2359, epoch_train_loss=0.0007309753768073356
Epoch 2360
Epoch 2360 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2360, epoch_train_loss=0.0007309753768073356
Epoch 2361
Epoch 2361 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2361, epoch_train_loss=0.0007309753768073356
Epoch 2362
Epoch 2362 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2362, epoch_train_loss=0.0007309753768073356
Epoch 2363
Epoch 2363 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2363, epoch_train_loss=0.0007309753768073356
Epoch 2364
Epoch 2364 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2364, epoch_train_loss=0.0007309753768073356
Epoch 2365
Epoch 2365 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2365, epoch_train_loss=0.0007309753768073356
Epoch 2366
Epoch 2366 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2366, epoch_train_loss=0.0007309753768073356
Epoch 2367
Epoch 2367 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2367, epoch_train_loss=0.0007309753768073356
Epoch 2368
Epoch 2368 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2368, epoch_train_loss=0.0007309753768073356
Epoch 2369
Epoch 2369 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2369, epoch_train_loss=0.0007309753768073356
Epoch 2370
Epoch 2370 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2370, epoch_train_loss=0.0007309753768073356
Epoch 2371
Epoch 2371 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2371, epoch_train_loss=0.0007309753768073356
Epoch 2372
Epoch 2372 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2372, epoch_train_loss=0.0007309753768073356
Epoch 2373
Epoch 2373 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2373, epoch_train_loss=0.0007309753768073356
Epoch 2374
Epoch 2374 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2374, epoch_train_loss=0.0007309753768073356
Epoch 2375
Epoch 2375 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2375, epoch_train_loss=0.0007309753768073356
Epoch 2376
Epoch 2376 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2376, epoch_train_loss=0.0007309753768073356
Epoch 2377
Epoch 2377 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2377, epoch_train_loss=0.0007309753768073356
Epoch 2378
Epoch 2378 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2378, epoch_train_loss=0.0007309753768073356
Epoch 2379
Epoch 2379 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2379, epoch_train_loss=0.0007309753768073356
Epoch 2380
Epoch 2380 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2380, epoch_train_loss=0.0007309753768073356
Epoch 2381
Epoch 2381 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2381, epoch_train_loss=0.0007309753768073356
Epoch 2382
Epoch 2382 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2382, epoch_train_loss=0.0007309753768073356
Epoch 2383
Epoch 2383 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2383, epoch_train_loss=0.0007309753768073356
Epoch 2384
Epoch 2384 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2384, epoch_train_loss=0.0007309753768073356
Epoch 2385
Epoch 2385 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2385, epoch_train_loss=0.0007309753768073356
Epoch 2386
Epoch 2386 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2386, epoch_train_loss=0.0007309753768073356
Epoch 2387
Epoch 2387 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2387, epoch_train_loss=0.0007309753768073356
Epoch 2388
Epoch 2388 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2388, epoch_train_loss=0.0007309753768073356
Epoch 2389
Epoch 2389 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2389, epoch_train_loss=0.0007309753768073356
Epoch 2390
Epoch 2390 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2390, epoch_train_loss=0.0007309753768073356
Epoch 2391
Epoch 2391 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2391, epoch_train_loss=0.0007309753768073356
Epoch 2392
Epoch 2392 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2392, epoch_train_loss=0.0007309753768073356
Epoch 2393
Epoch 2393 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2393, epoch_train_loss=0.0007309753768073356
Epoch 2394
Epoch 2394 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2394, epoch_train_loss=0.0007309753768073356
Epoch 2395
Epoch 2395 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2395, epoch_train_loss=0.0007309753768073356
Epoch 2396
Epoch 2396 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2396, epoch_train_loss=0.0007309753768073356
Epoch 2397
Epoch 2397 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2397, epoch_train_loss=0.0007309753768073356
Epoch 2398
Epoch 2398 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2398, epoch_train_loss=0.0007309753768073356
Epoch 2399
Epoch 2399 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2399, epoch_train_loss=0.0007309753768073356
Epoch 2400
Epoch 2400 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2400, epoch_train_loss=0.0007309753768073356
Epoch 2401
Epoch 2401 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2401, epoch_train_loss=0.0007309753768073356
Epoch 2402
Epoch 2402 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2402, epoch_train_loss=0.0007309753768073356
Epoch 2403
Epoch 2403 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2403, epoch_train_loss=0.0007309753768073356
Epoch 2404
Epoch 2404 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2404, epoch_train_loss=0.0007309753768073356
Epoch 2405
Epoch 2405 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2405, epoch_train_loss=0.0007309753768073356
Epoch 2406
Epoch 2406 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2406, epoch_train_loss=0.0007309753768073356
Epoch 2407
Epoch 2407 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2407, epoch_train_loss=0.0007309753768073356
Epoch 2408
Epoch 2408 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2408, epoch_train_loss=0.0007309753768073356
Epoch 2409
Epoch 2409 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2409, epoch_train_loss=0.0007309753768073356
Epoch 2410
Epoch 2410 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2410, epoch_train_loss=0.0007309753768073356
Epoch 2411
Epoch 2411 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2411, epoch_train_loss=0.0007309753768073356
Epoch 2412
Epoch 2412 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2412, epoch_train_loss=0.0007309753768073356
Epoch 2413
Epoch 2413 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2413, epoch_train_loss=0.0007309753768073356
Epoch 2414
Epoch 2414 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2414, epoch_train_loss=0.0007309753768073356
Epoch 2415
Epoch 2415 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2415, epoch_train_loss=0.0007309753768073356
Epoch 2416
Epoch 2416 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2416, epoch_train_loss=0.0007309753768073356
Epoch 2417
Epoch 2417 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2417, epoch_train_loss=0.0007309753768073356
Epoch 2418
Epoch 2418 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2418, epoch_train_loss=0.0007309753768073356
Epoch 2419
Epoch 2419 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2419, epoch_train_loss=0.0007309753768073356
Epoch 2420
Epoch 2420 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2420, epoch_train_loss=0.0007309753768073356
Epoch 2421
Epoch 2421 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2421, epoch_train_loss=0.0007309753768073356
Epoch 2422
Epoch 2422 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2422, epoch_train_loss=0.0007309753768073356
Epoch 2423
Epoch 2423 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2423, epoch_train_loss=0.0007309753768073356
Epoch 2424
Epoch 2424 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2424, epoch_train_loss=0.0007309753768073356
Epoch 2425
Epoch 2425 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2425, epoch_train_loss=0.0007309753768073356
Epoch 2426
Epoch 2426 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2426, epoch_train_loss=0.0007309753768073356
Epoch 2427
Epoch 2427 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2427, epoch_train_loss=0.0007309753768073356
Epoch 2428
Epoch 2428 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2428, epoch_train_loss=0.0007309753768073356
Epoch 2429
Epoch 2429 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2429, epoch_train_loss=0.0007309753768073356
Epoch 2430
Epoch 2430 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2430, epoch_train_loss=0.0007309753768073356
Epoch 2431
Epoch 2431 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2431, epoch_train_loss=0.0007309753768073356
Epoch 2432
Epoch 2432 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2432, epoch_train_loss=0.0007309753768073356
Epoch 2433
Epoch 2433 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2433, epoch_train_loss=0.0007309753768073356
Epoch 2434
Epoch 2434 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2434, epoch_train_loss=0.0007309753768073356
Epoch 2435
Epoch 2435 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2435, epoch_train_loss=0.0007309753768073356
Epoch 2436
Epoch 2436 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2436, epoch_train_loss=0.0007309753768073356
Epoch 2437
Epoch 2437 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2437, epoch_train_loss=0.0007309753768073356
Epoch 2438
Epoch 2438 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2438, epoch_train_loss=0.0007309753768073356
Epoch 2439
Epoch 2439 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2439, epoch_train_loss=0.0007309753768073356
Epoch 2440
Epoch 2440 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2440, epoch_train_loss=0.0007309753768073356
Epoch 2441
Epoch 2441 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2441, epoch_train_loss=0.0007309753768073356
Epoch 2442
Epoch 2442 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2442, epoch_train_loss=0.0007309753768073356
Epoch 2443
Epoch 2443 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2443, epoch_train_loss=0.0007309753768073356
Epoch 2444
Epoch 2444 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2444, epoch_train_loss=0.0007309753768073356
Epoch 2445
Epoch 2445 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2445, epoch_train_loss=0.0007309753768073356
Epoch 2446
Epoch 2446 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2446, epoch_train_loss=0.0007309753768073356
Epoch 2447
Epoch 2447 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2447, epoch_train_loss=0.0007309753768073356
Epoch 2448
Epoch 2448 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2448, epoch_train_loss=0.0007309753768073356
Epoch 2449
Epoch 2449 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2449, epoch_train_loss=0.0007309753768073356
Epoch 2450
Epoch 2450 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2450, epoch_train_loss=0.0007309753768073356
Epoch 2451
Epoch 2451 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2451, epoch_train_loss=0.0007309753768073356
Epoch 2452
Epoch 2452 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2452, epoch_train_loss=0.0007309753768073356
Epoch 2453
Epoch 2453 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2453, epoch_train_loss=0.0007309753768073356
Epoch 2454
Epoch 2454 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2454, epoch_train_loss=0.0007309753768073356
Epoch 2455
Epoch 2455 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2455, epoch_train_loss=0.0007309753768073356
Epoch 2456
Epoch 2456 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2456, epoch_train_loss=0.0007309753768073356
Epoch 2457
Epoch 2457 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2457, epoch_train_loss=0.0007309753768073356
Epoch 2458
Epoch 2458 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2458, epoch_train_loss=0.0007309753768073356
Epoch 2459
Epoch 2459 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2459, epoch_train_loss=0.0007309753768073356
Epoch 2460
Epoch 2460 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2460, epoch_train_loss=0.0007309753768073356
Epoch 2461
Epoch 2461 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2461, epoch_train_loss=0.0007309753768073356
Epoch 2462
Epoch 2462 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2462, epoch_train_loss=0.0007309753768073356
Epoch 2463
Epoch 2463 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2463, epoch_train_loss=0.0007309753768073356
Epoch 2464
Epoch 2464 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2464, epoch_train_loss=0.0007309753768073356
Epoch 2465
Epoch 2465 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2465, epoch_train_loss=0.0007309753768073356
Epoch 2466
Epoch 2466 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2466, epoch_train_loss=0.0007309753768073356
Epoch 2467
Epoch 2467 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2467, epoch_train_loss=0.0007309753768073356
Epoch 2468
Epoch 2468 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2468, epoch_train_loss=0.0007309753768073356
Epoch 2469
Epoch 2469 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2469, epoch_train_loss=0.0007309753768073356
Epoch 2470
Epoch 2470 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2470, epoch_train_loss=0.0007309753768073356
Epoch 2471
Epoch 2471 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2471, epoch_train_loss=0.0007309753768073356
Epoch 2472
Epoch 2472 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2472, epoch_train_loss=0.0007309753768073356
Epoch 2473
Epoch 2473 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2473, epoch_train_loss=0.0007309753768073356
Epoch 2474
Epoch 2474 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2474, epoch_train_loss=0.0007309753768073356
Epoch 2475
Epoch 2475 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2475, epoch_train_loss=0.0007309753768073356
Epoch 2476
Epoch 2476 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2476, epoch_train_loss=0.0007309753768073356
Epoch 2477
Epoch 2477 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2477, epoch_train_loss=0.0007309753768073356
Epoch 2478
Epoch 2478 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2478, epoch_train_loss=0.0007309753768073356
Epoch 2479
Epoch 2479 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2479, epoch_train_loss=0.0007309753768073356
Epoch 2480
Epoch 2480 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2480, epoch_train_loss=0.0007309753768073356
Epoch 2481
Epoch 2481 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2481, epoch_train_loss=0.0007309753768073356
Epoch 2482
Epoch 2482 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2482, epoch_train_loss=0.0007309753768073356
Epoch 2483
Epoch 2483 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2483, epoch_train_loss=0.0007309753768073356
Epoch 2484
Epoch 2484 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2484, epoch_train_loss=0.0007309753768073356
Epoch 2485
Epoch 2485 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2485, epoch_train_loss=0.0007309753768073356
Epoch 2486
Epoch 2486 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2486, epoch_train_loss=0.0007309753768073356
Epoch 2487
Epoch 2487 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2487, epoch_train_loss=0.0007309753768073356
Epoch 2488
Epoch 2488 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2488, epoch_train_loss=0.0007309753768073356
Epoch 2489
Epoch 2489 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2489, epoch_train_loss=0.0007309753768073356
Epoch 2490
Epoch 2490 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2490, epoch_train_loss=0.0007309753768073356
Epoch 2491
Epoch 2491 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2491, epoch_train_loss=0.0007309753768073356
Epoch 2492
Epoch 2492 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2492, epoch_train_loss=0.0007309753768073356
Epoch 2493
Epoch 2493 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2493, epoch_train_loss=0.0007309753768073356
Epoch 2494
Epoch 2494 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2494, epoch_train_loss=0.0007309753768073356
Epoch 2495
Epoch 2495 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2495, epoch_train_loss=0.0007309753768073356
Epoch 2496
Epoch 2496 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2496, epoch_train_loss=0.0007309753768073356
Epoch 2497
Epoch 2497 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2497, epoch_train_loss=0.0007309753768073356
Epoch 2498
Epoch 2498 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2498, epoch_train_loss=0.0007309753768073356
Epoch 2499
Epoch 2499 :: Batch 0/1
Batch Loss = 0.0007309753768073356
2499, epoch_train_loss=0.0007309753768073356
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting "B3LYP_WITH_VWN5 = True" in pyscf_conf.py
  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '
Warning: <pyscf.gto.mole.Mole object at 0x7ffec430b940> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec430b940> in UKS object of <class 'pyscf.dft.uks.UKS'>
WARNING: External module "mldftdat" required for non-local descriptor use.
<pyscf.gto.mole.Mole object at 0x7ffec430b940> [['P', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec430b160> [['N', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec430b400> [['H', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec430ac80> [['Li', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec430a0e0> [['O', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec430b670> [['Cl', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec4309a20> [['Al', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec430b340> [['S', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec43088e0> [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec4308b20> [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec4308910> [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec4309f30> [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]] 3
<pyscf.gto.mole.Mole object at 0x7ffec430ba00> [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]] 3
<pyscf.gto.mole.Mole object at 0x7ffec43089d0> [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec4308790> [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec4308670> [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec425a560> [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]] 4
<pyscf.gto.mole.Mole object at 0x7ffec4259de0> [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec425a5c0> [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec4259b70> [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec41b0f70> [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec41b3d30> [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]] 4
<pyscf.gto.mole.Mole object at 0x7ffec41b3340> [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]] 4
<pyscf.gto.mole.Mole object at 0x7ffec41b1d20> [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec41b09a0> [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]] 5
<pyscf.gto.mole.Mole object at 0x7ffec41b0160> [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]] 3
<pyscf.gto.mole.Mole object at 0x7ffec41b3550> [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]] 5
mol:  [['P', array([0., 0., 0.])]]
converged SCF energy = -341.104145992718  <S^2> = 3.7502984  2S+1 = 4.0001492
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec430b160> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec430b160> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00051022 -0.00019156 -0.00051334 ... -0.02830887 -0.02830887
 -0.02830887] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 16)
mol:  [['N', array([0., 0., 0.])]]
converged SCF energy = -54.5289742046674  <S^2> = 3.7524945  2S+1 = 4.0012471
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec430b400> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec430b400> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5016, 22)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-3.60081838e-04 -1.08775305e-04 -1.31917160e-05 ... -2.74817476e-02
 -2.74817476e-02 -2.74817476e-02] = ,SCAN
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5016, 22)
rho shape (4, 4, 2, 5016)
rho_filt shape: (5016,)
get descriptors tdrho.shape=(5016, 16)
mol:  [['H', array([0., 0., 0.])]]
converged SCF energy = -0.499812984008539  <S^2> = 0.75  2S+1 = 2
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec430ac80> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec430ac80> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 7, 7)
ao.shape (10, 2440, 7)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.92637355e-09 -1.31700807e-07 -9.61527370e-06 ... -7.42461648e-16
 -7.42461648e-16 -7.42461648e-16] = ,SCAN
get_data, dm shape = (2, 7, 7)
ao_eval.shape=(4, 2440, 7)
rho shape (4, 4, 2, 2440)
rho_filt shape: (2440,)
get descriptors tdrho.shape=(2440, 16)
mol:  [['Li', array([0., 0., 0.])]]
converged SCF energy = -7.46006188627842  <S^2> = 0.75000049  2S+1 = 2.0000005
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec430a0e0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec430a0e0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 4592, 22)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-5.31884219e-04 -2.81911891e-04 -2.81911891e-04 ... -1.27154711e-05
 -2.64861768e-02 -2.64861768e-02] = ,SCAN
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 4592, 22)
rho shape (4, 4, 2, 4592)
rho_filt shape: (4592,)
get descriptors tdrho.shape=(4592, 16)
mol:  [['O', array([0., 0., 0.])]]
converged SCF energy = -75.0033802931119  <S^2> = 2.0027444  2S+1 = 3.0018291
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec430b670> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec430b670> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5040, 22)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.15858504e-04 -4.01258773e-05 -2.14117748e-06 ... -2.76158580e-02
 -2.76158580e-02 -2.76158580e-02] = ,SCAN
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5040, 22)
rho shape (4, 4, 2, 5040)
rho_filt shape: (5040,)
get descriptors tdrho.shape=(5040, 16)
mol:  [['Cl', array([0., 0., 0.])]]
converged SCF energy = -459.957577121377  <S^2> = 0.7516194  2S+1 = 2.0016187
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4309a20> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4309a20> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6152, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.99669571e-04 -2.40563119e-04 -8.22178014e-05 ... -2.84484386e-02
 -2.84484386e-02 -2.84484386e-02] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6152, 30)
rho shape (4, 4, 2, 6152)
rho_filt shape: (6152,)
get descriptors tdrho.shape=(6152, 16)
mol:  [['Al', array([0., 0., 0.])]]
converged SCF energy = -242.226560974222  <S^2> = 0.75226419  2S+1 = 2.0022629
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec430b340> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec430b340> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6088, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.43603673e-03 -1.51421506e-03 -7.66133644e-04 ... -2.92106789e-05
 -3.16140742e-04 -3.60148424e-05] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6088, 30)
rho shape (4, 4, 2, 6088)
rho_filt shape: (6088,)
get descriptors tdrho.shape=(6088, 16)
mol:  [['S', array([0., 0., 0.])]]
converged SCF energy = -397.938786807113  <S^2> = 2.0022329  2S+1 = 3.0014882
Warning: <pyscf.gto.mole.Mole object at 0x7ffec43088e0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec43088e0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00038745 -0.00017488 -0.00023373 ... -0.02838402 -0.02838402
 -0.02838402] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 16)
mol:  [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]]
converged SCF energy = -1.16580491182912  <S^2> = 8.8817842e-16  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4308b20> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4308b20> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 14, 14)
ao.shape (10, 4776, 14)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.43725660e-05 -1.02204687e-06 -4.05575842e-05 ... -2.36278434e-02
 -2.36278434e-02 -2.36278434e-02] = ,SCAN
get_data, dm shape = (2, 14, 14)
ao_eval.shape=(4, 4776, 14)
rho shape (4, 4, 2, 4776)
rho_filt shape: (4776,)
get descriptors tdrho.shape=(4776, 16)
mol:  [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]]
converged SCF energy = -109.439263799566  <S^2> = 1.4210855e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4308910> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4308910> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9848, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.89629699e-05 -2.76172354e-04 -7.59017288e-05 ... -7.34654212e-06
 -7.34654212e-06 -2.89629699e-05] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9848, 44)
rho shape (4, 4, 2, 9848)
rho_filt shape: (9848,)
get descriptors tdrho.shape=(9848, 16)
mol:  [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]]
converged SCF energy = -107.339357395734  <S^2> = 1.7763568e-15  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4309f30> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4309f30> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9752, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00043469 -0.00024024 -0.00035532 ... -0.00047537 -0.03728133
 -0.03728133] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9752, 44)
rho shape (4, 4, 2, 9752)
rho_filt shape: (9752,)
get descriptors tdrho.shape=(9752, 16)
mol:  [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]]
converged SCF energy = -93.3377924465131  <S^2> = 4.0072123e-10  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec430ba00> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec430ba00> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 12256, 51)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-9.02468888e-05 -7.92694658e-06 -9.80568469e-06 ... -4.33714150e-02
 -4.33714150e-02 -4.33714150e-02] = ,SCAN
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 12256, 51)
rho shape (4, 4, 2, 12256)
rho_filt shape: (12256,)
get descriptors tdrho.shape=(12256, 16)
mol:  [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]]
converged SCF energy = -188.456965322843  <S^2> = 1.5987212e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec43089d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec43089d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 66, 66)
ao.shape (10, 14920, 66)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-7.48187338e-05 -6.19475249e-05 -2.61742784e-04 ... -8.70042314e-07
 -2.73391097e-02 -2.73391097e-02] = ,SCAN
get_data, dm shape = (2, 66, 66)
ao_eval.shape=(4, 14920, 66)
rho shape (4, 4, 2, 14920)
rho_filt shape: (14920,)
get descriptors tdrho.shape=(14920, 16)
mol:  [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]]
converged SCF energy = -920.005608888959  <S^2> = 4.938272e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4308790> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4308790> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12208, 60)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00051559 -0.00027432 -0.00088583 ... -0.00027432 -0.04174728
 -0.04174728] = ,SCAN
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12208, 60)
rho shape (4, 4, 2, 12208)
rho_filt shape: (12208,)
get descriptors tdrho.shape=(12208, 16)
mol:  [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]]
converged SCF energy = -199.394370591172  <S^2> = 1.2256862e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4308670> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4308670> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9824, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-5.53951178e-05 -5.93507199e-06 -3.10072916e-04 ... -5.94325581e-02
 -5.94325581e-02 -5.94325581e-02] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9824, 44)
rho shape (4, 4, 2, 9824)
rho_filt shape: (9824,)
get descriptors tdrho.shape=(9824, 16)
mol:  [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]]
converged SCF energy = -150.21489456173  <S^2> = 1.0018599  2S+1 = 2.2377309
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec425a560> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec425a560> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9912, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.74564232e-04 -3.26401427e-05 -1.76686070e-06 ... -4.22396849e-02
 -4.22396849e-02 -4.22396849e-02] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9912, 44)
rho shape (4, 4, 2, 9912)
rho_filt shape: (9912,)
get descriptors tdrho.shape=(9912, 16)
mol:  [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]]
converged SCF energy = -77.2435048346374  <S^2> = 1.0658141e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4259de0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4259de0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 58, 58)
ao.shape (10, 15208, 58)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.72190712e-05 -2.72190712e-05 -2.84904833e-04 ... -1.08108260e-05
 -1.03072478e-05 -1.03072478e-05] = ,SCAN
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15208, 58)
rho shape (4, 4, 2, 15208)
rho_filt shape: (15208,)
get descriptors tdrho.shape=(15208, 16)
mol:  [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]]
converged SCF energy = -113.221335689652  <S^2> = 6.5547567e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec425a5c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec425a5c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 10040, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00015688 -0.00024669 -0.00068269 ... -0.03791166 -0.03791166
 -0.03791166] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 10040, 44)
rho shape (4, 4, 2, 10040)
rho_filt shape: (10040,)
get descriptors tdrho.shape=(10040, 16)
mol:  [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]]
converged SCF energy = -460.624592374078  <S^2> = 7.8159701e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec4259b70> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec4259b70> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 37, 37)
ao.shape (10, 8552, 37)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-7.28500681e-05 -5.65091132e-06 -7.37932132e-06 ... -4.76689214e-02
 -4.76689214e-02 -4.76689214e-02] = ,SCAN
get_data, dm shape = (2, 37, 37)
ao_eval.shape=(4, 8552, 37)
rho shape (4, 4, 2, 8552)
rho_filt shape: (8552,)
get descriptors tdrho.shape=(8552, 16)
mol:  [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]]
converged SCF energy = -8.04458854018922  <S^2> = 7.8381746e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec41b0f70> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec41b0f70> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 29, 29)
ao.shape (10, 6936, 29)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.0003863  -0.00040095 -0.00040095 ... -0.0213199  -0.0213199
 -0.0213199 ] = ,SCAN
get_data, dm shape = (2, 29, 29)
ao_eval.shape=(4, 6936, 29)
rho shape (4, 4, 2, 6936)
rho_filt shape: (6936,)
get descriptors tdrho.shape=(6936, 16)
mol:  [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]]
converged SCF energy = -324.340512506578  <S^2> = 1.5869972e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec41b3d30> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec41b3d30> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 11536, 60)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00088473 -0.00088473 -0.00116894 ... -0.00088473 -0.00088473
 -0.00116894] = ,SCAN
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 11536, 60)
rho shape (4, 4, 2, 11536)
rho_filt shape: (11536,)
get descriptors tdrho.shape=(11536, 16)
mol:  [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]]
converged SCF energy = -1622.57507845814  <S^2> = 8.0646601e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec41b3340> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec41b3340> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 120, 120)
ao.shape (10, 24512, 120)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.91408540e-05 -1.46971271e-04 -1.08734417e-03 ... -2.81566369e-02
 -2.81566369e-02 -2.81566369e-02] = ,SCAN
get_data, dm shape = (2, 120, 120)
ao_eval.shape=(4, 24512, 120)
rho shape (4, 4, 2, 24512)
rho_filt shape: (24512,)
get descriptors tdrho.shape=(24512, 16)
mol:  [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]]
converged SCF energy = -342.979728469575  <S^2> = 2.5389468e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec41b1d20> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec41b1d20> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 13096, 51)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.39373335e-04 -1.31641332e-04 -1.15950750e-05 ... -7.32416564e-02
 -7.32416564e-02 -7.32416564e-02] = ,SCAN
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 13096, 51)
rho shape (4, 4, 2, 13096)
rho_filt shape: (13096,)
get descriptors tdrho.shape=(13096, 16)
mol:  [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]]
converged SCF energy = -578.565336179611  <S^2> = 1.0034705  2S+1 = 2.2391699
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec41b09a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec41b09a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12384, 60)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.84683816e-05 -7.80525601e-05 -7.80561012e-05 ... -2.92531391e-02
 -2.92531391e-02 -2.92531391e-02] = ,SCAN
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12384, 60)
rho shape (4, 4, 2, 12384)
rho_filt shape: (12384,)
get descriptors tdrho.shape=(12384, 16)
mol:  [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]]
converged SCF energy = -40.4598214864075  <S^2> = 3.250733e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec41b0160> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec41b0160> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 50, 50)
ao.shape (10, 13936, 50)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.56165538e-04 -7.34744214e-05 -5.30574304e-06 ... -7.93995702e-06
 -7.93995702e-06 -7.93995702e-06] = ,SCAN
get_data, dm shape = (2, 50, 50)
ao_eval.shape=(4, 13936, 50)
rho shape (4, 4, 2, 13936)
rho_filt shape: (13936,)
get descriptors tdrho.shape=(13936, 16)
mol:  [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]]
converged SCF energy = -39.0756147483504  <S^2> = 6.2043704e-12  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec41b3550> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec41b3550> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 36, 36)
ao.shape (10, 9656, 36)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.28290753e-04 -1.41305449e-05 -6.13700492e-05 ... -2.47993463e-02
 -2.47993463e-02 -2.47993463e-02] = ,SCAN
get_data, dm shape = (2, 36, 36)
ao_eval.shape=(4, 9656, 36)
rho shape (4, 4, 2, 9656)
rho_filt shape: (9656,)
get descriptors tdrho.shape=(9656, 16)
mol:  [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]]
converged SCF energy = -291.719272437819  <S^2> = 1.3155699e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
New DM shape: (2, 58, 58)
ao.shape (10, 15256, 58)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.45512011e-04 -7.12775692e-05 -5.48666345e-06 ... -6.02613084e-06
 -6.02613084e-06 -6.02613084e-06] = ,SCAN
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15256, 58)
rho shape (4, 4, 2, 15256)
rho_filt shape: (15256,)
get descriptors tdrho.shape=(15256, 16)
PRE NAN FILT: tFxc.shape=(237019,), tdrho.shape=(237019, 16)
nan_filt_rho.shape=(237019,)
nan_filt_fxc.shape=(237019,)
tFxc.shape=(237019,), tdrho.shape=(237019, 16)
inp[0].shape = (237019, 16)
Epoch 0
Epoch 0 :: Batch 0/1
Batch Loss = 8137902.219844131
0, epoch_train_loss=8137902.219844131
Epoch 1
Epoch 1 :: Batch 0/1
Batch Loss = 0.2588562610498327
1, epoch_train_loss=0.2588562610498327
Epoch 2
Epoch 2 :: Batch 0/1
Batch Loss = 0.20424770785823015
2, epoch_train_loss=0.20424770785823015
Epoch 3
Epoch 3 :: Batch 0/1
Batch Loss = 0.12121796401483195
3, epoch_train_loss=0.12121796401483195
Epoch 4
Epoch 4 :: Batch 0/1
Batch Loss = 0.05901488940597297
4, epoch_train_loss=0.05901488940597297
Epoch 5
Epoch 5 :: Batch 0/1
Batch Loss = 0.028979680349016004
5, epoch_train_loss=0.028979680349016004
Epoch 6
Epoch 6 :: Batch 0/1
Batch Loss = 0.014443061141216362
6, epoch_train_loss=0.014443061141216362
Epoch 7
Epoch 7 :: Batch 0/1
Batch Loss = 0.0069289161156723495
7, epoch_train_loss=0.0069289161156723495
Epoch 8
Epoch 8 :: Batch 0/1
Batch Loss = 0.0032800507312211145
8, epoch_train_loss=0.0032800507312211145
Epoch 9
Epoch 9 :: Batch 0/1
Batch Loss = 0.0017105469229856288
9, epoch_train_loss=0.0017105469229856288
Epoch 10
Epoch 10 :: Batch 0/1
Batch Loss = 0.0010986450148674578
10, epoch_train_loss=0.0010986450148674578
Epoch 11
Epoch 11 :: Batch 0/1
Batch Loss = 0.0008708330472866597
11, epoch_train_loss=0.0008708330472866597
Epoch 12
Epoch 12 :: Batch 0/1
Batch Loss = 0.0007859738099594423
12, epoch_train_loss=0.0007859738099594423
Epoch 13
Epoch 13 :: Batch 0/1
Batch Loss = 0.000753440668563172
13, epoch_train_loss=0.000753440668563172
Epoch 14
Epoch 14 :: Batch 0/1
Batch Loss = 0.0007404697705117318
14, epoch_train_loss=0.0007404697705117318
Epoch 15
Epoch 15 :: Batch 0/1
Batch Loss = 0.000735101702904073
15, epoch_train_loss=0.000735101702904073
Epoch 16
Epoch 16 :: Batch 0/1
Batch Loss = 0.0007328099649781783
16, epoch_train_loss=0.0007328099649781783
Epoch 17
Epoch 17 :: Batch 0/1
Batch Loss = 0.0007318054960108508
17, epoch_train_loss=0.0007318054960108508
Epoch 18
Epoch 18 :: Batch 0/1
Batch Loss = 0.0007313542794690047
18, epoch_train_loss=0.0007313542794690047
Epoch 19
Epoch 19 :: Batch 0/1
Batch Loss = 0.0007311464413161955
19, epoch_train_loss=0.0007311464413161955
Epoch 20
Epoch 20 :: Batch 0/1
Batch Loss = 0.0007310481317869393
20, epoch_train_loss=0.0007310481317869393
Epoch 21
Epoch 21 :: Batch 0/1
Batch Loss = 0.0007310003027044655
21, epoch_train_loss=0.0007310003027044655
Epoch 22
Epoch 22 :: Batch 0/1
Batch Loss = 0.0007309763370545175
22, epoch_train_loss=0.0007309763370545175
Epoch 23
Epoch 23 :: Batch 0/1
Batch Loss = 0.0007309639583166886
23, epoch_train_loss=0.0007309639583166886
Epoch 24
Epoch 24 :: Batch 0/1
Batch Loss = 0.0007309573643032436
24, epoch_train_loss=0.0007309573643032436
Epoch 25
Epoch 25 :: Batch 0/1
Batch Loss = 0.0007309537416290633
25, epoch_train_loss=0.0007309537416290633
Epoch 26
Epoch 26 :: Batch 0/1
Batch Loss = 0.0007309516896050161
26, epoch_train_loss=0.0007309516896050161
Epoch 27
Epoch 27 :: Batch 0/1
Batch Loss = 0.0007309504918957007
27, epoch_train_loss=0.0007309504918957007
Epoch 28
Epoch 28 :: Batch 0/1
Batch Loss = 0.0007309497721523718
28, epoch_train_loss=0.0007309497721523718
Epoch 29
Epoch 29 :: Batch 0/1
Batch Loss = 0.0007309493272912562
29, epoch_train_loss=0.0007309493272912562
Epoch 30
Epoch 30 :: Batch 0/1
Batch Loss = 0.0007309490448052835
30, epoch_train_loss=0.0007309490448052835
Epoch 31
Epoch 31 :: Batch 0/1
Batch Loss = 0.0007309488607455358
31, epoch_train_loss=0.0007309488607455358
Epoch 32
Epoch 32 :: Batch 0/1
Batch Loss = 0.0007309487378468129
32, epoch_train_loss=0.0007309487378468129
Epoch 33
Epoch 33 :: Batch 0/1
Batch Loss = 0.0007309486538650457
33, epoch_train_loss=0.0007309486538650457
Epoch 34
Epoch 34 :: Batch 0/1
Batch Loss = 0.0007309485952121347
34, epoch_train_loss=0.0007309485952121347
Epoch 35
Epoch 35 :: Batch 0/1
Batch Loss = 0.0007309485534019265
35, epoch_train_loss=0.0007309485534019265
Epoch 36
Epoch 36 :: Batch 0/1
Batch Loss = 0.0007309485230214562
36, epoch_train_loss=0.0007309485230214562
Epoch 37
Epoch 37 :: Batch 0/1
Batch Loss = 0.0007309485005478785
37, epoch_train_loss=0.0007309485005478785
Epoch 38
Epoch 38 :: Batch 0/1
Batch Loss = 0.0007309484836442281
38, epoch_train_loss=0.0007309484836442281
Epoch 39
Epoch 39 :: Batch 0/1
Batch Loss = 0.0007309484707318582
39, epoch_train_loss=0.0007309484707318582
Epoch 40
Epoch 40 :: Batch 0/1
Batch Loss = 0.0007309484607258968
40, epoch_train_loss=0.0007309484607258968
Epoch 41
Epoch 41 :: Batch 0/1
Batch Loss = 0.000730948452868582
41, epoch_train_loss=0.000730948452868582
Epoch 42
Epoch 42 :: Batch 0/1
Batch Loss = 0.0007309484466224295
42, epoch_train_loss=0.0007309484466224295
Epoch 43
Epoch 43 :: Batch 0/1
Batch Loss = 0.0007309484416006179
43, epoch_train_loss=0.0007309484416006179
Epoch 44
Epoch 44 :: Batch 0/1
Batch Loss = 0.0007309484375209053
44, epoch_train_loss=0.0007309484375209053
Epoch 45
Epoch 45 :: Batch 0/1
Batch Loss = 0.000730948434174666
45, epoch_train_loss=0.000730948434174666
Epoch 46
Epoch 46 :: Batch 0/1
Batch Loss = 0.0007309484314057868
46, epoch_train_loss=0.0007309484314057868
Epoch 47
Epoch 47 :: Batch 0/1
Batch Loss = 0.0007309484290960868
47, epoch_train_loss=0.0007309484290960868
Epoch 48
Epoch 48 :: Batch 0/1
Batch Loss = 0.0007309484271551164
48, epoch_train_loss=0.0007309484271551164
Epoch 49
Epoch 49 :: Batch 0/1
Batch Loss = 0.0007309484255129281
49, epoch_train_loss=0.0007309484255129281
Epoch 50
Epoch 50 :: Batch 0/1
Batch Loss = 0.0007309484241148962
50, epoch_train_loss=0.0007309484241148962
Epoch 51
Epoch 51 :: Batch 0/1
Batch Loss = 0.0007309484229179647
51, epoch_train_loss=0.0007309484229179647
Epoch 52
Epoch 52 :: Batch 0/1
Batch Loss = 0.0007309484218879387
52, epoch_train_loss=0.0007309484218879387
Epoch 53
Epoch 53 :: Batch 0/1
Batch Loss = 0.0007309484209973526
53, epoch_train_loss=0.0007309484209973526
Epoch 54
Epoch 54 :: Batch 0/1
Batch Loss = 0.0007309484202240108
54, epoch_train_loss=0.0007309484202240108
Epoch 55
Epoch 55 :: Batch 0/1
Batch Loss = 0.0007309484195498416
55, epoch_train_loss=0.0007309484195498416
Epoch 56
Epoch 56 :: Batch 0/1
Batch Loss = 0.0007309484189600277
56, epoch_train_loss=0.0007309484189600277
Epoch 57
Epoch 57 :: Batch 0/1
Batch Loss = 0.0007309484184423367
57, epoch_train_loss=0.0007309484184423367
Epoch 58
Epoch 58 :: Batch 0/1
Batch Loss = 0.0007309484179866075
58, epoch_train_loss=0.0007309484179866075
Epoch 59
Epoch 59 :: Batch 0/1
Batch Loss = 0.0007309484175843471
59, epoch_train_loss=0.0007309484175843471
Epoch 60
Epoch 60 :: Batch 0/1
Batch Loss = 0.0007309484172284176
60, epoch_train_loss=0.0007309484172284176
Epoch 61
Epoch 61 :: Batch 0/1
Batch Loss = 0.0007309484169127863
61, epoch_train_loss=0.0007309484169127863
Epoch 62
Epoch 62 :: Batch 0/1
Batch Loss = 0.0007309484166323307
62, epoch_train_loss=0.0007309484166323307
Epoch 63
Epoch 63 :: Batch 0/1
Batch Loss = 0.0007309484163826775
63, epoch_train_loss=0.0007309484163826775
Epoch 64
Epoch 64 :: Batch 0/1
Batch Loss = 0.0007309484161600789
64, epoch_train_loss=0.0007309484161600789
Epoch 65
Epoch 65 :: Batch 0/1
Batch Loss = 0.0007309484159613074
65, epoch_train_loss=0.0007309484159613074
Epoch 66
Epoch 66 :: Batch 0/1
Batch Loss = 0.0007309484157835742
66, epoch_train_loss=0.0007309484157835742
Epoch 67
Epoch 67 :: Batch 0/1
Batch Loss = 0.0007309484156244594
67, epoch_train_loss=0.0007309484156244594
Epoch 68
Epoch 68 :: Batch 0/1
Batch Loss = 0.000730948415481856
68, epoch_train_loss=0.000730948415481856
Epoch 69
Epoch 69 :: Batch 0/1
Batch Loss = 0.0007309484153539245
69, epoch_train_loss=0.0007309484153539245
Epoch 70
Epoch 70 :: Batch 0/1
Batch Loss = 0.0007309484152390528
70, epoch_train_loss=0.0007309484152390528
Epoch 71
Epoch 71 :: Batch 0/1
Batch Loss = 0.0007309484151358248
71, epoch_train_loss=0.0007309484151358248
Epoch 72
Epoch 72 :: Batch 0/1
Batch Loss = 0.0007309484150429931
72, epoch_train_loss=0.0007309484150429931
Epoch 73
Epoch 73 :: Batch 0/1
Batch Loss = 0.0007309484149594565
73, epoch_train_loss=0.0007309484149594565
Epoch 74
Epoch 74 :: Batch 0/1
Batch Loss = 0.0007309484148842403
74, epoch_train_loss=0.0007309484148842403
Epoch 75
Epoch 75 :: Batch 0/1
Batch Loss = 0.0007309484148164804
75, epoch_train_loss=0.0007309484148164804
Epoch 76
Epoch 76 :: Batch 0/1
Batch Loss = 0.0007309484147554089
76, epoch_train_loss=0.0007309484147554089
Epoch 77
Epoch 77 :: Batch 0/1
Batch Loss = 0.0007309484147003426
77, epoch_train_loss=0.0007309484147003426
Epoch 78
Epoch 78 :: Batch 0/1
Batch Loss = 0.0007309484146506723
78, epoch_train_loss=0.0007309484146506723
Epoch 79
Epoch 79 :: Batch 0/1
Batch Loss = 0.0007309484146058541
79, epoch_train_loss=0.0007309484146058541
Epoch 80
Epoch 80 :: Batch 0/1
Batch Loss = 0.0007309484145654022
80, epoch_train_loss=0.0007309484145654022
Epoch 81
Epoch 81 :: Batch 0/1
Batch Loss = 0.0007309484145288817
81, epoch_train_loss=0.0007309484145288817
Epoch 82
Epoch 82 :: Batch 0/1
Batch Loss = 0.0007309484144959025
82, epoch_train_loss=0.0007309484144959025
Epoch 83
Epoch 83 :: Batch 0/1
Batch Loss = 0.0007309484144661151
83, epoch_train_loss=0.0007309484144661151
Epoch 84
Epoch 84 :: Batch 0/1
Batch Loss = 0.0007309484144392056
84, epoch_train_loss=0.0007309484144392056
Epoch 85
Epoch 85 :: Batch 0/1
Batch Loss = 0.0007309484144148924
85, epoch_train_loss=0.0007309484144148924
Epoch 86
Epoch 86 :: Batch 0/1
Batch Loss = 0.0007309484143929214
86, epoch_train_loss=0.0007309484143929214
Epoch 87
Epoch 87 :: Batch 0/1
Batch Loss = 0.0007309484143730649
87, epoch_train_loss=0.0007309484143730649
Epoch 88
Epoch 88 :: Batch 0/1
Batch Loss = 0.0007309484143551174
88, epoch_train_loss=0.0007309484143551174
Epoch 89
Epoch 89 :: Batch 0/1
Batch Loss = 0.0007309484143388936
89, epoch_train_loss=0.0007309484143388936
Epoch 90
Epoch 90 :: Batch 0/1
Batch Loss = 0.0007309484143242274
90, epoch_train_loss=0.0007309484143242274
Epoch 91
Epoch 91 :: Batch 0/1
Batch Loss = 0.0007309484143109677
91, epoch_train_loss=0.0007309484143109677
Epoch 92
Epoch 92 :: Batch 0/1
Batch Loss = 0.0007309484142989796
92, epoch_train_loss=0.0007309484142989796
Epoch 93
Epoch 93 :: Batch 0/1
Batch Loss = 0.0007309484142881402
93, epoch_train_loss=0.0007309484142881402
Epoch 94
Epoch 94 :: Batch 0/1
Batch Loss = 0.0007309484142783392
94, epoch_train_loss=0.0007309484142783392
Epoch 95
Epoch 95 :: Batch 0/1
Batch Loss = 0.0007309484142694766
95, epoch_train_loss=0.0007309484142694766
Epoch 96
Epoch 96 :: Batch 0/1
Batch Loss = 0.0007309484142614628
96, epoch_train_loss=0.0007309484142614628
Epoch 97
Epoch 97 :: Batch 0/1
Batch Loss = 0.0007309484142542163
97, epoch_train_loss=0.0007309484142542163
Epoch 98
Epoch 98 :: Batch 0/1
Batch Loss = 0.0007309484142476632
98, epoch_train_loss=0.0007309484142476632
Epoch 99
Epoch 99 :: Batch 0/1
Batch Loss = 0.0007309484142417375
99, epoch_train_loss=0.0007309484142417375
Epoch 100
Epoch 100 :: Batch 0/1
Batch Loss = 0.0007309484142363788
100, epoch_train_loss=0.0007309484142363788
Epoch 101
Epoch 101 :: Batch 0/1
Batch Loss = 0.0007309484142315332
101, epoch_train_loss=0.0007309484142315332
Epoch 102
Epoch 102 :: Batch 0/1
Batch Loss = 0.0007309484142271513
102, epoch_train_loss=0.0007309484142271513
Epoch 103
Epoch 103 :: Batch 0/1
Batch Loss = 0.0007309484142231889
103, epoch_train_loss=0.0007309484142231889
Epoch 104
Epoch 104 :: Batch 0/1
Batch Loss = 0.0007309484142196059
104, epoch_train_loss=0.0007309484142196059
Epoch 105
Epoch 105 :: Batch 0/1
Batch Loss = 0.0007309484142163659
105, epoch_train_loss=0.0007309484142163659
Epoch 106
Epoch 106 :: Batch 0/1
Batch Loss = 0.0007309484142134362
106, epoch_train_loss=0.0007309484142134362
Epoch 107
Epoch 107 :: Batch 0/1
Batch Loss = 0.000730948414210787
107, epoch_train_loss=0.000730948414210787
Epoch 108
Epoch 108 :: Batch 0/1
Batch Loss = 0.0007309484142083916
108, epoch_train_loss=0.0007309484142083916
Epoch 109
Epoch 109 :: Batch 0/1
Batch Loss = 0.0007309484142062257
109, epoch_train_loss=0.0007309484142062257
Epoch 110
Epoch 110 :: Batch 0/1
Batch Loss = 0.0007309484142042673
110, epoch_train_loss=0.0007309484142042673
Epoch 111
Epoch 111 :: Batch 0/1
Batch Loss = 0.0007309484142024967
111, epoch_train_loss=0.0007309484142024967
Epoch 112
Epoch 112 :: Batch 0/1
Batch Loss = 0.0007309484142008957
112, epoch_train_loss=0.0007309484142008957
Epoch 113
Epoch 113 :: Batch 0/1
Batch Loss = 0.0007309484141994482
113, epoch_train_loss=0.0007309484141994482
Epoch 114
Epoch 114 :: Batch 0/1
Batch Loss = 0.0007309484141981395
114, epoch_train_loss=0.0007309484141981395
Epoch 115
Epoch 115 :: Batch 0/1
Batch Loss = 0.0007309484141969565
115, epoch_train_loss=0.0007309484141969565
Epoch 116
Epoch 116 :: Batch 0/1
Batch Loss = 0.0007309484141958869
116, epoch_train_loss=0.0007309484141958869
Epoch 117
Epoch 117 :: Batch 0/1
Batch Loss = 0.0007309484141949198
117, epoch_train_loss=0.0007309484141949198
Epoch 118
Epoch 118 :: Batch 0/1
Batch Loss = 0.0007309484141940456
118, epoch_train_loss=0.0007309484141940456
Epoch 119
Epoch 119 :: Batch 0/1
Batch Loss = 0.0007309484141932555
119, epoch_train_loss=0.0007309484141932555
Epoch 120
Epoch 120 :: Batch 0/1
Batch Loss = 0.000730948414192541
120, epoch_train_loss=0.000730948414192541
Epoch 121
Epoch 121 :: Batch 0/1
Batch Loss = 0.0007309484141918952
121, epoch_train_loss=0.0007309484141918952
Epoch 122
Epoch 122 :: Batch 0/1
Batch Loss = 0.0007309484141913114
122, epoch_train_loss=0.0007309484141913114
Epoch 123
Epoch 123 :: Batch 0/1
Batch Loss = 0.0007309484141907838
123, epoch_train_loss=0.0007309484141907838
Epoch 124
Epoch 124 :: Batch 0/1
Batch Loss = 0.0007309484141903068
124, epoch_train_loss=0.0007309484141903068
Epoch 125
Epoch 125 :: Batch 0/1
Batch Loss = 0.0007309484141898756
125, epoch_train_loss=0.0007309484141898756
Epoch 126
Epoch 126 :: Batch 0/1
Batch Loss = 0.0007309484141894857
126, epoch_train_loss=0.0007309484141894857
Epoch 127
Epoch 127 :: Batch 0/1
Batch Loss = 0.0007309484141891334
127, epoch_train_loss=0.0007309484141891334
Epoch 128
Epoch 128 :: Batch 0/1
Batch Loss = 0.0007309484141888149
128, epoch_train_loss=0.0007309484141888149
Epoch 129
Epoch 129 :: Batch 0/1
Batch Loss = 0.000730948414188527
129, epoch_train_loss=0.000730948414188527
Epoch 130
Epoch 130 :: Batch 0/1
Batch Loss = 0.0007309484141882668
130, epoch_train_loss=0.0007309484141882668
Epoch 131
Epoch 131 :: Batch 0/1
Batch Loss = 0.0007309484141880317
131, epoch_train_loss=0.0007309484141880317
Epoch 132
Epoch 132 :: Batch 0/1
Batch Loss = 0.0007309484141878191
132, epoch_train_loss=0.0007309484141878191
Epoch 133
Epoch 133 :: Batch 0/1
Batch Loss = 0.0007309484141876269
133, epoch_train_loss=0.0007309484141876269
Epoch 134
Epoch 134 :: Batch 0/1
Batch Loss = 0.0007309484141874531
134, epoch_train_loss=0.0007309484141874531
Epoch 135
Epoch 135 :: Batch 0/1
Batch Loss = 0.0007309484141872962
135, epoch_train_loss=0.0007309484141872962
Epoch 136
Epoch 136 :: Batch 0/1
Batch Loss = 0.0007309484141871542
136, epoch_train_loss=0.0007309484141871542
Epoch 137
Epoch 137 :: Batch 0/1
Batch Loss = 0.0007309484141870259
137, epoch_train_loss=0.0007309484141870259
Epoch 138
Epoch 138 :: Batch 0/1
Batch Loss = 0.0007309484141869097
138, epoch_train_loss=0.0007309484141869097
Epoch 139
Epoch 139 :: Batch 0/1
Batch Loss = 0.0007309484141868049
139, epoch_train_loss=0.0007309484141868049
Epoch 140
Epoch 140 :: Batch 0/1
Batch Loss = 0.0007309484141867101
140, epoch_train_loss=0.0007309484141867101
Epoch 141
Epoch 141 :: Batch 0/1
Batch Loss = 0.0007309484141866243
141, epoch_train_loss=0.0007309484141866243
Epoch 142
Epoch 142 :: Batch 0/1
Batch Loss = 0.0007309484141865467
142, epoch_train_loss=0.0007309484141865467
Epoch 143
Epoch 143 :: Batch 0/1
Batch Loss = 0.0007309484141864765
143, epoch_train_loss=0.0007309484141864765
Epoch 144
Epoch 144 :: Batch 0/1
Batch Loss = 0.0007309484141864132
144, epoch_train_loss=0.0007309484141864132
Epoch 145
Epoch 145 :: Batch 0/1
Batch Loss = 0.0007309484141863559
145, epoch_train_loss=0.0007309484141863559
Epoch 146
Epoch 146 :: Batch 0/1
Batch Loss = 0.0007309484141863039
146, epoch_train_loss=0.0007309484141863039
Epoch 147
Epoch 147 :: Batch 0/1
Batch Loss = 0.0007309484141862568
147, epoch_train_loss=0.0007309484141862568
Epoch 148
Epoch 148 :: Batch 0/1
Batch Loss = 0.0007309484141862144
148, epoch_train_loss=0.0007309484141862144
Epoch 149
Epoch 149 :: Batch 0/1
Batch Loss = 0.0007309484141861761
149, epoch_train_loss=0.0007309484141861761
Epoch 150
Epoch 150 :: Batch 0/1
Batch Loss = 0.0007309484141861411
150, epoch_train_loss=0.0007309484141861411
Epoch 151
Epoch 151 :: Batch 0/1
Batch Loss = 0.0007309484141861096
151, epoch_train_loss=0.0007309484141861096
Epoch 152
Epoch 152 :: Batch 0/1
Batch Loss = 0.0007309484141860811
152, epoch_train_loss=0.0007309484141860811
Epoch 153
Epoch 153 :: Batch 0/1
Batch Loss = 0.0007309484141860552
153, epoch_train_loss=0.0007309484141860552
Epoch 154
Epoch 154 :: Batch 0/1
Batch Loss = 0.0007309484141860316
154, epoch_train_loss=0.0007309484141860316
Epoch 155
Epoch 155 :: Batch 0/1
Batch Loss = 0.0007309484141860103
155, epoch_train_loss=0.0007309484141860103
Epoch 156
Epoch 156 :: Batch 0/1
Batch Loss = 0.0007309484141859911
156, epoch_train_loss=0.0007309484141859911
Epoch 157
Epoch 157 :: Batch 0/1
Batch Loss = 0.0007309484141859736
157, epoch_train_loss=0.0007309484141859736
Epoch 158
Epoch 158 :: Batch 0/1
Batch Loss = 0.0007309484141859577
158, epoch_train_loss=0.0007309484141859577
Epoch 159
Epoch 159 :: Batch 0/1
Batch Loss = 0.0007309484141859434
159, epoch_train_loss=0.0007309484141859434
Epoch 160
Epoch 160 :: Batch 0/1
Batch Loss = 0.0007309484141859302
160, epoch_train_loss=0.0007309484141859302
Epoch 161
Epoch 161 :: Batch 0/1
Batch Loss = 0.0007309484141859182
161, epoch_train_loss=0.0007309484141859182
Epoch 162
Epoch 162 :: Batch 0/1
Batch Loss = 0.0007309484141859074
162, epoch_train_loss=0.0007309484141859074
Epoch 163
Epoch 163 :: Batch 0/1
Batch Loss = 0.0007309484141858975
163, epoch_train_loss=0.0007309484141858975
Epoch 164
Epoch 164 :: Batch 0/1
Batch Loss = 0.0007309484141858884
164, epoch_train_loss=0.0007309484141858884
Epoch 165
Epoch 165 :: Batch 0/1
Batch Loss = 0.0007309484141858803
165, epoch_train_loss=0.0007309484141858803
Epoch 166
Epoch 166 :: Batch 0/1
Batch Loss = 0.0007309484141858728
166, epoch_train_loss=0.0007309484141858728
Epoch 167
Epoch 167 :: Batch 0/1
Batch Loss = 0.0007309484141858659
167, epoch_train_loss=0.0007309484141858659
Epoch 168
Epoch 168 :: Batch 0/1
Batch Loss = 0.0007309484141858596
168, epoch_train_loss=0.0007309484141858596
Epoch 169
Epoch 169 :: Batch 0/1
Batch Loss = 0.0007309484141858538
169, epoch_train_loss=0.0007309484141858538
Epoch 170
Epoch 170 :: Batch 0/1
Batch Loss = 0.0007309484141858486
170, epoch_train_loss=0.0007309484141858486
Epoch 171
Epoch 171 :: Batch 0/1
Batch Loss = 0.0007309484141858436
171, epoch_train_loss=0.0007309484141858436
Epoch 172
Epoch 172 :: Batch 0/1
Batch Loss = 0.0007309484141858392
172, epoch_train_loss=0.0007309484141858392
Epoch 173
Epoch 173 :: Batch 0/1
Batch Loss = 0.0007309484141858351
173, epoch_train_loss=0.0007309484141858351
Epoch 174
Epoch 174 :: Batch 0/1
Batch Loss = 0.0007309484141858313
174, epoch_train_loss=0.0007309484141858313
Epoch 175
Epoch 175 :: Batch 0/1
Batch Loss = 0.0007309484141858278
175, epoch_train_loss=0.0007309484141858278
Epoch 176
Epoch 176 :: Batch 0/1
Batch Loss = 0.0007309484141858245
176, epoch_train_loss=0.0007309484141858245
Epoch 177
Epoch 177 :: Batch 0/1
Batch Loss = 0.0007309484141858214
177, epoch_train_loss=0.0007309484141858214
Epoch 178
Epoch 178 :: Batch 0/1
Batch Loss = 0.0007309484141858187
178, epoch_train_loss=0.0007309484141858187
Epoch 179
Epoch 179 :: Batch 0/1
Batch Loss = 0.0007309484141858159
179, epoch_train_loss=0.0007309484141858159
Epoch 180
Epoch 180 :: Batch 0/1
Batch Loss = 0.0007309484141858134
180, epoch_train_loss=0.0007309484141858134
Epoch 181
Epoch 181 :: Batch 0/1
Batch Loss = 0.0007309484141858111
181, epoch_train_loss=0.0007309484141858111
Epoch 182
Epoch 182 :: Batch 0/1
Batch Loss = 0.0007309484141858088
182, epoch_train_loss=0.0007309484141858088
Epoch 183
Epoch 183 :: Batch 0/1
Batch Loss = 0.0007309484141858069
183, epoch_train_loss=0.0007309484141858069
Epoch 184
Epoch 184 :: Batch 0/1
Batch Loss = 0.0007309484141858049
184, epoch_train_loss=0.0007309484141858049
Epoch 185
Epoch 185 :: Batch 0/1
Batch Loss = 0.0007309484141858031
185, epoch_train_loss=0.0007309484141858031
Epoch 186
Epoch 186 :: Batch 0/1
Batch Loss = 0.0007309484141858011
186, epoch_train_loss=0.0007309484141858011
Epoch 187
Epoch 187 :: Batch 0/1
Batch Loss = 0.0007309484141857996
187, epoch_train_loss=0.0007309484141857996
Epoch 188
Epoch 188 :: Batch 0/1
Batch Loss = 0.0007309484141857979
188, epoch_train_loss=0.0007309484141857979
Epoch 189
Epoch 189 :: Batch 0/1
Batch Loss = 0.0007309484141857964
189, epoch_train_loss=0.0007309484141857964
Epoch 190
Epoch 190 :: Batch 0/1
Batch Loss = 0.000730948414185795
190, epoch_train_loss=0.000730948414185795
Epoch 191
Epoch 191 :: Batch 0/1
Batch Loss = 0.0007309484141857934
191, epoch_train_loss=0.0007309484141857934
Epoch 192
Epoch 192 :: Batch 0/1
Batch Loss = 0.000730948414185792
192, epoch_train_loss=0.000730948414185792
Epoch 193
Epoch 193 :: Batch 0/1
Batch Loss = 0.0007309484141857908
193, epoch_train_loss=0.0007309484141857908
Epoch 194
Epoch 194 :: Batch 0/1
Batch Loss = 0.0007309484141857895
194, epoch_train_loss=0.0007309484141857895
Epoch 195
Epoch 195 :: Batch 0/1
Batch Loss = 0.0007309484141857882
195, epoch_train_loss=0.0007309484141857882
Epoch 196
Epoch 196 :: Batch 0/1
Batch Loss = 0.0007309484141857869
196, epoch_train_loss=0.0007309484141857869
Epoch 197
Epoch 197 :: Batch 0/1
Batch Loss = 0.0007309484141857859
197, epoch_train_loss=0.0007309484141857859
Epoch 198
Epoch 198 :: Batch 0/1
Batch Loss = 0.0007309484141857846
198, epoch_train_loss=0.0007309484141857846
Epoch 199
Epoch 199 :: Batch 0/1
Batch Loss = 0.0007309484141857835
199, epoch_train_loss=0.0007309484141857835
Epoch 200
Epoch 200 :: Batch 0/1
Batch Loss = 0.0007309484141857822
200, epoch_train_loss=0.0007309484141857822
Epoch 201
Epoch 201 :: Batch 0/1
Batch Loss = 0.000730948414185781
201, epoch_train_loss=0.000730948414185781
Epoch 202
Epoch 202 :: Batch 0/1
Batch Loss = 0.0007309484141857799
202, epoch_train_loss=0.0007309484141857799
Epoch 203
Epoch 203 :: Batch 0/1
Batch Loss = 0.0007309484141857789
203, epoch_train_loss=0.0007309484141857789
Epoch 204
Epoch 204 :: Batch 0/1
Batch Loss = 0.0007309484141857778
204, epoch_train_loss=0.0007309484141857778
Epoch 205
Epoch 205 :: Batch 0/1
Batch Loss = 0.0007309484141857767
205, epoch_train_loss=0.0007309484141857767
Epoch 206
Epoch 206 :: Batch 0/1
Batch Loss = 0.0007309484141857758
206, epoch_train_loss=0.0007309484141857758
Epoch 207
Epoch 207 :: Batch 0/1
Batch Loss = 0.0007309484141857746
207, epoch_train_loss=0.0007309484141857746
Epoch 208
Epoch 208 :: Batch 0/1
Batch Loss = 0.0007309484141857736
208, epoch_train_loss=0.0007309484141857736
Epoch 209
Epoch 209 :: Batch 0/1
Batch Loss = 0.0007309484141857724
209, epoch_train_loss=0.0007309484141857724
Epoch 210
Epoch 210 :: Batch 0/1
Batch Loss = 0.0007309484141857714
210, epoch_train_loss=0.0007309484141857714
Epoch 211
Epoch 211 :: Batch 0/1
Batch Loss = 0.0007309484141857704
211, epoch_train_loss=0.0007309484141857704
Epoch 212
Epoch 212 :: Batch 0/1
Batch Loss = 0.0007309484141857693
212, epoch_train_loss=0.0007309484141857693
Epoch 213
Epoch 213 :: Batch 0/1
Batch Loss = 0.0007309484141857683
213, epoch_train_loss=0.0007309484141857683
Epoch 214
Epoch 214 :: Batch 0/1
Batch Loss = 0.0007309484141857673
214, epoch_train_loss=0.0007309484141857673
Epoch 215
Epoch 215 :: Batch 0/1
Batch Loss = 0.0007309484141857662
215, epoch_train_loss=0.0007309484141857662
Epoch 216
Epoch 216 :: Batch 0/1
Batch Loss = 0.0007309484141857651
216, epoch_train_loss=0.0007309484141857651
Epoch 217
Epoch 217 :: Batch 0/1
Batch Loss = 0.0007309484141857643
217, epoch_train_loss=0.0007309484141857643
Epoch 218
Epoch 218 :: Batch 0/1
Batch Loss = 0.0007309484141857631
218, epoch_train_loss=0.0007309484141857631
Epoch 219
Epoch 219 :: Batch 0/1
Batch Loss = 0.0007309484141857621
219, epoch_train_loss=0.0007309484141857621
Epoch 220
Epoch 220 :: Batch 0/1
Batch Loss = 0.0007309484141857611
220, epoch_train_loss=0.0007309484141857611
Epoch 221
Epoch 221 :: Batch 0/1
Batch Loss = 0.0007309484141857602
221, epoch_train_loss=0.0007309484141857602
Epoch 222
Epoch 222 :: Batch 0/1
Batch Loss = 0.0007309484141857591
222, epoch_train_loss=0.0007309484141857591
Epoch 223
Epoch 223 :: Batch 0/1
Batch Loss = 0.000730948414185758
223, epoch_train_loss=0.000730948414185758
Epoch 224
Epoch 224 :: Batch 0/1
Batch Loss = 0.000730948414185757
224, epoch_train_loss=0.000730948414185757
Epoch 225
Epoch 225 :: Batch 0/1
Batch Loss = 0.000730948414185756
225, epoch_train_loss=0.000730948414185756
Epoch 226
Epoch 226 :: Batch 0/1
Batch Loss = 0.0007309484141857548
226, epoch_train_loss=0.0007309484141857548
Epoch 227
Epoch 227 :: Batch 0/1
Batch Loss = 0.0007309484141857539
227, epoch_train_loss=0.0007309484141857539
Epoch 228
Epoch 228 :: Batch 0/1
Batch Loss = 0.000730948414185753
228, epoch_train_loss=0.000730948414185753
Epoch 229
Epoch 229 :: Batch 0/1
Batch Loss = 0.0007309484141857519
229, epoch_train_loss=0.0007309484141857519
Epoch 230
Epoch 230 :: Batch 0/1
Batch Loss = 0.0007309484141857508
230, epoch_train_loss=0.0007309484141857508
Epoch 231
Epoch 231 :: Batch 0/1
Batch Loss = 0.0007309484141857499
231, epoch_train_loss=0.0007309484141857499
Epoch 232
Epoch 232 :: Batch 0/1
Batch Loss = 0.0007309484141857489
232, epoch_train_loss=0.0007309484141857489
Epoch 233
Epoch 233 :: Batch 0/1
Batch Loss = 0.0007309484141857478
233, epoch_train_loss=0.0007309484141857478
Epoch 234
Epoch 234 :: Batch 0/1
Batch Loss = 0.0007309484141857468
234, epoch_train_loss=0.0007309484141857468
Epoch 235
Epoch 235 :: Batch 0/1
Batch Loss = 0.0007309484141857457
235, epoch_train_loss=0.0007309484141857457
Epoch 236
Epoch 236 :: Batch 0/1
Batch Loss = 0.0007309484141857447
236, epoch_train_loss=0.0007309484141857447
Epoch 237
Epoch 237 :: Batch 0/1
Batch Loss = 0.0007309484141857436
237, epoch_train_loss=0.0007309484141857436
Epoch 238
Epoch 238 :: Batch 0/1
Batch Loss = 0.0007309484141857427
238, epoch_train_loss=0.0007309484141857427
Epoch 239
Epoch 239 :: Batch 0/1
Batch Loss = 0.0007309484141857416
239, epoch_train_loss=0.0007309484141857416
Epoch 240
Epoch 240 :: Batch 0/1
Batch Loss = 0.0007309484141857405
240, epoch_train_loss=0.0007309484141857405
Epoch 241
Epoch 241 :: Batch 0/1
Batch Loss = 0.0007309484141857395
241, epoch_train_loss=0.0007309484141857395
Epoch 242
Epoch 242 :: Batch 0/1
Batch Loss = 0.0007309484141857385
242, epoch_train_loss=0.0007309484141857385
Epoch 243
Epoch 243 :: Batch 0/1
Batch Loss = 0.0007309484141857374
243, epoch_train_loss=0.0007309484141857374
Epoch 244
Epoch 244 :: Batch 0/1
Batch Loss = 0.0007309484141857364
244, epoch_train_loss=0.0007309484141857364
Epoch 245
Epoch 245 :: Batch 0/1
Batch Loss = 0.0007309484141857352
245, epoch_train_loss=0.0007309484141857352
Epoch 246
Epoch 246 :: Batch 0/1
Batch Loss = 0.0007309484141857342
246, epoch_train_loss=0.0007309484141857342
Epoch 247
Epoch 247 :: Batch 0/1
Batch Loss = 0.0007309484141857332
247, epoch_train_loss=0.0007309484141857332
Epoch 248
Epoch 248 :: Batch 0/1
Batch Loss = 0.0007309484141857321
248, epoch_train_loss=0.0007309484141857321
Epoch 249
Epoch 249 :: Batch 0/1
Batch Loss = 0.000730948414185731
249, epoch_train_loss=0.000730948414185731
Epoch 250
Epoch 250 :: Batch 0/1
Batch Loss = 0.0007309484141857299
250, epoch_train_loss=0.0007309484141857299
Epoch 251
Epoch 251 :: Batch 0/1
Batch Loss = 0.0007309484141857288
251, epoch_train_loss=0.0007309484141857288
Epoch 252
Epoch 252 :: Batch 0/1
Batch Loss = 0.0007309484141857277
252, epoch_train_loss=0.0007309484141857277
Epoch 253
Epoch 253 :: Batch 0/1
Batch Loss = 0.0007309484141857267
253, epoch_train_loss=0.0007309484141857267
Epoch 254
Epoch 254 :: Batch 0/1
Batch Loss = 0.0007309484141857256
254, epoch_train_loss=0.0007309484141857256
Epoch 255
Epoch 255 :: Batch 0/1
Batch Loss = 0.0007309484141857246
255, epoch_train_loss=0.0007309484141857246
Epoch 256
Epoch 256 :: Batch 0/1
Batch Loss = 0.0007309484141857235
256, epoch_train_loss=0.0007309484141857235
Epoch 257
Epoch 257 :: Batch 0/1
Batch Loss = 0.0007309484141857224
257, epoch_train_loss=0.0007309484141857224
Epoch 258
Epoch 258 :: Batch 0/1
Batch Loss = 0.0007309484141857213
258, epoch_train_loss=0.0007309484141857213
Epoch 259
Epoch 259 :: Batch 0/1
Batch Loss = 0.0007309484141857203
259, epoch_train_loss=0.0007309484141857203
Epoch 260
Epoch 260 :: Batch 0/1
Batch Loss = 0.0007309484141857192
260, epoch_train_loss=0.0007309484141857192
Epoch 261
Epoch 261 :: Batch 0/1
Batch Loss = 0.000730948414185718
261, epoch_train_loss=0.000730948414185718
Epoch 262
Epoch 262 :: Batch 0/1
Batch Loss = 0.000730948414185717
262, epoch_train_loss=0.000730948414185717
Epoch 263
Epoch 263 :: Batch 0/1
Batch Loss = 0.0007309484141857158
263, epoch_train_loss=0.0007309484141857158
Epoch 264
Epoch 264 :: Batch 0/1
Batch Loss = 0.0007309484141857147
264, epoch_train_loss=0.0007309484141857147
Epoch 265
Epoch 265 :: Batch 0/1
Batch Loss = 0.0007309484141857136
265, epoch_train_loss=0.0007309484141857136
Epoch 266
Epoch 266 :: Batch 0/1
Batch Loss = 0.0007309484141857125
266, epoch_train_loss=0.0007309484141857125
Epoch 267
Epoch 267 :: Batch 0/1
Batch Loss = 0.0007309484141857114
267, epoch_train_loss=0.0007309484141857114
Epoch 268
Epoch 268 :: Batch 0/1
Batch Loss = 0.0007309484141857103
268, epoch_train_loss=0.0007309484141857103
Epoch 269
Epoch 269 :: Batch 0/1
Batch Loss = 0.0007309484141857092
269, epoch_train_loss=0.0007309484141857092
Epoch 270
Epoch 270 :: Batch 0/1
Batch Loss = 0.0007309484141857081
270, epoch_train_loss=0.0007309484141857081
Epoch 271
Epoch 271 :: Batch 0/1
Batch Loss = 0.0007309484141857069
271, epoch_train_loss=0.0007309484141857069
Epoch 272
Epoch 272 :: Batch 0/1
Batch Loss = 0.0007309484141857058
272, epoch_train_loss=0.0007309484141857058
Epoch 273
Epoch 273 :: Batch 0/1
Batch Loss = 0.0007309484141857048
273, epoch_train_loss=0.0007309484141857048
Epoch 274
Epoch 274 :: Batch 0/1
Batch Loss = 0.0007309484141857036
274, epoch_train_loss=0.0007309484141857036
Epoch 275
Epoch 275 :: Batch 0/1
Batch Loss = 0.0007309484141857025
275, epoch_train_loss=0.0007309484141857025
Epoch 276
Epoch 276 :: Batch 0/1
Batch Loss = 0.0007309484141857014
276, epoch_train_loss=0.0007309484141857014
Epoch 277
Epoch 277 :: Batch 0/1
Batch Loss = 0.0007309484141857002
277, epoch_train_loss=0.0007309484141857002
Epoch 278
Epoch 278 :: Batch 0/1
Batch Loss = 0.0007309484141856991
278, epoch_train_loss=0.0007309484141856991
Epoch 279
Epoch 279 :: Batch 0/1
Batch Loss = 0.000730948414185698
279, epoch_train_loss=0.000730948414185698
Epoch 280
Epoch 280 :: Batch 0/1
Batch Loss = 0.0007309484141856968
280, epoch_train_loss=0.0007309484141856968
Epoch 281
Epoch 281 :: Batch 0/1
Batch Loss = 0.0007309484141856957
281, epoch_train_loss=0.0007309484141856957
Epoch 282
Epoch 282 :: Batch 0/1
Batch Loss = 0.0007309484141856945
282, epoch_train_loss=0.0007309484141856945
Epoch 283
Epoch 283 :: Batch 0/1
Batch Loss = 0.0007309484141856934
283, epoch_train_loss=0.0007309484141856934
Epoch 284
Epoch 284 :: Batch 0/1
Batch Loss = 0.0007309484141856923
284, epoch_train_loss=0.0007309484141856923
Epoch 285
Epoch 285 :: Batch 0/1
Batch Loss = 0.0007309484141856911
285, epoch_train_loss=0.0007309484141856911
Epoch 286
Epoch 286 :: Batch 0/1
Batch Loss = 0.0007309484141856899
286, epoch_train_loss=0.0007309484141856899
Epoch 287
Epoch 287 :: Batch 0/1
Batch Loss = 0.0007309484141856888
287, epoch_train_loss=0.0007309484141856888
Epoch 288
Epoch 288 :: Batch 0/1
Batch Loss = 0.0007309484141856876
288, epoch_train_loss=0.0007309484141856876
Epoch 289
Epoch 289 :: Batch 0/1
Batch Loss = 0.0007309484141856865
289, epoch_train_loss=0.0007309484141856865
Epoch 290
Epoch 290 :: Batch 0/1
Batch Loss = 0.0007309484141856854
290, epoch_train_loss=0.0007309484141856854
Epoch 291
Epoch 291 :: Batch 0/1
Batch Loss = 0.0007309484141856842
291, epoch_train_loss=0.0007309484141856842
Epoch 292
Epoch 292 :: Batch 0/1
Batch Loss = 0.000730948414185683
292, epoch_train_loss=0.000730948414185683
Epoch 293
Epoch 293 :: Batch 0/1
Batch Loss = 0.0007309484141856819
293, epoch_train_loss=0.0007309484141856819
Epoch 294
Epoch 294 :: Batch 0/1
Batch Loss = 0.0007309484141856806
294, epoch_train_loss=0.0007309484141856806
Epoch 295
Epoch 295 :: Batch 0/1
Batch Loss = 0.0007309484141856795
295, epoch_train_loss=0.0007309484141856795
Epoch 296
Epoch 296 :: Batch 0/1
Batch Loss = 0.0007309484141856784
296, epoch_train_loss=0.0007309484141856784
Epoch 297
Epoch 297 :: Batch 0/1
Batch Loss = 0.0007309484141856772
297, epoch_train_loss=0.0007309484141856772
Epoch 298
Epoch 298 :: Batch 0/1
Batch Loss = 0.000730948414185676
298, epoch_train_loss=0.000730948414185676
Epoch 299
Epoch 299 :: Batch 0/1
Batch Loss = 0.0007309484141856748
299, epoch_train_loss=0.0007309484141856748
Epoch 300
Epoch 300 :: Batch 0/1
Batch Loss = 0.0007309484141856735
300, epoch_train_loss=0.0007309484141856735
Epoch 301
Epoch 301 :: Batch 0/1
Batch Loss = 0.0007309484141856723
301, epoch_train_loss=0.0007309484141856723
Epoch 302
Epoch 302 :: Batch 0/1
Batch Loss = 0.0007309484141856711
302, epoch_train_loss=0.0007309484141856711
Epoch 303
Epoch 303 :: Batch 0/1
Batch Loss = 0.00073094841418567
303, epoch_train_loss=0.00073094841418567
Epoch 304
Epoch 304 :: Batch 0/1
Batch Loss = 0.0007309484141856688
304, epoch_train_loss=0.0007309484141856688
Epoch 305
Epoch 305 :: Batch 0/1
Batch Loss = 0.0007309484141856676
305, epoch_train_loss=0.0007309484141856676
Epoch 306
Epoch 306 :: Batch 0/1
Batch Loss = 0.0007309484141856664
306, epoch_train_loss=0.0007309484141856664
Epoch 307
Epoch 307 :: Batch 0/1
Batch Loss = 0.0007309484141856653
307, epoch_train_loss=0.0007309484141856653
Epoch 308
Epoch 308 :: Batch 0/1
Batch Loss = 0.0007309484141856641
308, epoch_train_loss=0.0007309484141856641
Epoch 309
Epoch 309 :: Batch 0/1
Batch Loss = 0.0007309484141856628
309, epoch_train_loss=0.0007309484141856628
Epoch 310
Epoch 310 :: Batch 0/1
Batch Loss = 0.0007309484141856617
310, epoch_train_loss=0.0007309484141856617
Epoch 311
Epoch 311 :: Batch 0/1
Batch Loss = 0.0007309484141856604
311, epoch_train_loss=0.0007309484141856604
Epoch 312
Epoch 312 :: Batch 0/1
Batch Loss = 0.0007309484141856592
312, epoch_train_loss=0.0007309484141856592
Epoch 313
Epoch 313 :: Batch 0/1
Batch Loss = 0.000730948414185658
313, epoch_train_loss=0.000730948414185658
Epoch 314
Epoch 314 :: Batch 0/1
Batch Loss = 0.0007309484141856568
314, epoch_train_loss=0.0007309484141856568
Epoch 315
Epoch 315 :: Batch 0/1
Batch Loss = 0.0007309484141856556
315, epoch_train_loss=0.0007309484141856556
Epoch 316
Epoch 316 :: Batch 0/1
Batch Loss = 0.0007309484141856543
316, epoch_train_loss=0.0007309484141856543
Epoch 317
Epoch 317 :: Batch 0/1
Batch Loss = 0.0007309484141856532
317, epoch_train_loss=0.0007309484141856532
Epoch 318
Epoch 318 :: Batch 0/1
Batch Loss = 0.0007309484141856518
318, epoch_train_loss=0.0007309484141856518
Epoch 319
Epoch 319 :: Batch 0/1
Batch Loss = 0.0007309484141856508
319, epoch_train_loss=0.0007309484141856508
Epoch 320
Epoch 320 :: Batch 0/1
Batch Loss = 0.0007309484141856495
320, epoch_train_loss=0.0007309484141856495
Epoch 321
Epoch 321 :: Batch 0/1
Batch Loss = 0.0007309484141856482
321, epoch_train_loss=0.0007309484141856482
Epoch 322
Epoch 322 :: Batch 0/1
Batch Loss = 0.000730948414185647
322, epoch_train_loss=0.000730948414185647
Epoch 323
Epoch 323 :: Batch 0/1
Batch Loss = 0.0007309484141856458
323, epoch_train_loss=0.0007309484141856458
Epoch 324
Epoch 324 :: Batch 0/1
Batch Loss = 0.0007309484141856446
324, epoch_train_loss=0.0007309484141856446
Epoch 325
Epoch 325 :: Batch 0/1
Batch Loss = 0.0007309484141856434
325, epoch_train_loss=0.0007309484141856434
Epoch 326
Epoch 326 :: Batch 0/1
Batch Loss = 0.0007309484141856422
326, epoch_train_loss=0.0007309484141856422
Epoch 327
Epoch 327 :: Batch 0/1
Batch Loss = 0.0007309484141856407
327, epoch_train_loss=0.0007309484141856407
Epoch 328
Epoch 328 :: Batch 0/1
Batch Loss = 0.0007309484141856396
328, epoch_train_loss=0.0007309484141856396
Epoch 329
Epoch 329 :: Batch 0/1
Batch Loss = 0.0007309484141856383
329, epoch_train_loss=0.0007309484141856383
Epoch 330
Epoch 330 :: Batch 0/1
Batch Loss = 0.0007309484141856371
330, epoch_train_loss=0.0007309484141856371
Epoch 331
Epoch 331 :: Batch 0/1
Batch Loss = 0.0007309484141856359
331, epoch_train_loss=0.0007309484141856359
Epoch 332
Epoch 332 :: Batch 0/1
Batch Loss = 0.0007309484141856345
332, epoch_train_loss=0.0007309484141856345
Epoch 333
Epoch 333 :: Batch 0/1
Batch Loss = 0.0007309484141856334
333, epoch_train_loss=0.0007309484141856334
Epoch 334
Epoch 334 :: Batch 0/1
Batch Loss = 0.0007309484141856321
334, epoch_train_loss=0.0007309484141856321
Epoch 335
Epoch 335 :: Batch 0/1
Batch Loss = 0.0007309484141856309
335, epoch_train_loss=0.0007309484141856309
Epoch 336
Epoch 336 :: Batch 0/1
Batch Loss = 0.0007309484141856296
336, epoch_train_loss=0.0007309484141856296
Epoch 337
Epoch 337 :: Batch 0/1
Batch Loss = 0.0007309484141856283
337, epoch_train_loss=0.0007309484141856283
Epoch 338
Epoch 338 :: Batch 0/1
Batch Loss = 0.000730948414185627
338, epoch_train_loss=0.000730948414185627
Epoch 339
Epoch 339 :: Batch 0/1
Batch Loss = 0.0007309484141856258
339, epoch_train_loss=0.0007309484141856258
Epoch 340
Epoch 340 :: Batch 0/1
Batch Loss = 0.0007309484141856245
340, epoch_train_loss=0.0007309484141856245
Epoch 341
Epoch 341 :: Batch 0/1
Batch Loss = 0.0007309484141856232
341, epoch_train_loss=0.0007309484141856232
Epoch 342
Epoch 342 :: Batch 0/1
Batch Loss = 0.000730948414185622
342, epoch_train_loss=0.000730948414185622
Epoch 343
Epoch 343 :: Batch 0/1
Batch Loss = 0.0007309484141856206
343, epoch_train_loss=0.0007309484141856206
Epoch 344
Epoch 344 :: Batch 0/1
Batch Loss = 0.0007309484141856195
344, epoch_train_loss=0.0007309484141856195
Epoch 345
Epoch 345 :: Batch 0/1
Batch Loss = 0.0007309484141856181
345, epoch_train_loss=0.0007309484141856181
Epoch 346
Epoch 346 :: Batch 0/1
Batch Loss = 0.0007309484141856167
346, epoch_train_loss=0.0007309484141856167
Epoch 347
Epoch 347 :: Batch 0/1
Batch Loss = 0.0007309484141856155
347, epoch_train_loss=0.0007309484141856155
Epoch 348
Epoch 348 :: Batch 0/1
Batch Loss = 0.0007309484141856143
348, epoch_train_loss=0.0007309484141856143
Epoch 349
Epoch 349 :: Batch 0/1
Batch Loss = 0.0007309484141856129
349, epoch_train_loss=0.0007309484141856129
Epoch 350
Epoch 350 :: Batch 0/1
Batch Loss = 0.0007309484141856117
350, epoch_train_loss=0.0007309484141856117
Epoch 351
Epoch 351 :: Batch 0/1
Batch Loss = 0.0007309484141856105
351, epoch_train_loss=0.0007309484141856105
Epoch 352
Epoch 352 :: Batch 0/1
Batch Loss = 0.000730948414185609
352, epoch_train_loss=0.000730948414185609
Epoch 353
Epoch 353 :: Batch 0/1
Batch Loss = 0.0007309484141856078
353, epoch_train_loss=0.0007309484141856078
Epoch 354
Epoch 354 :: Batch 0/1
Batch Loss = 0.0007309484141856065
354, epoch_train_loss=0.0007309484141856065
Epoch 355
Epoch 355 :: Batch 0/1
Batch Loss = 0.0007309484141856052
355, epoch_train_loss=0.0007309484141856052
Epoch 356
Epoch 356 :: Batch 0/1
Batch Loss = 0.0007309484141856039
356, epoch_train_loss=0.0007309484141856039
Epoch 357
Epoch 357 :: Batch 0/1
Batch Loss = 0.0007309484141856026
357, epoch_train_loss=0.0007309484141856026
Epoch 358
Epoch 358 :: Batch 0/1
Batch Loss = 0.0007309484141856013
358, epoch_train_loss=0.0007309484141856013
Epoch 359
Epoch 359 :: Batch 0/1
Batch Loss = 0.0007309484141855999
359, epoch_train_loss=0.0007309484141855999
Epoch 360
Epoch 360 :: Batch 0/1
Batch Loss = 0.0007309484141855987
360, epoch_train_loss=0.0007309484141855987
Epoch 361
Epoch 361 :: Batch 0/1
Batch Loss = 0.0007309484141855973
361, epoch_train_loss=0.0007309484141855973
Epoch 362
Epoch 362 :: Batch 0/1
Batch Loss = 0.000730948414185596
362, epoch_train_loss=0.000730948414185596
Epoch 363
Epoch 363 :: Batch 0/1
Batch Loss = 0.0007309484141855947
363, epoch_train_loss=0.0007309484141855947
Epoch 364
Epoch 364 :: Batch 0/1
Batch Loss = 0.0007309484141855935
364, epoch_train_loss=0.0007309484141855935
Epoch 365
Epoch 365 :: Batch 0/1
Batch Loss = 0.000730948414185592
365, epoch_train_loss=0.000730948414185592
Epoch 366
Epoch 366 :: Batch 0/1
Batch Loss = 0.0007309484141855908
366, epoch_train_loss=0.0007309484141855908
Epoch 367
Epoch 367 :: Batch 0/1
Batch Loss = 0.0007309484141855894
367, epoch_train_loss=0.0007309484141855894
Epoch 368
Epoch 368 :: Batch 0/1
Batch Loss = 0.0007309484141855881
368, epoch_train_loss=0.0007309484141855881
Epoch 369
Epoch 369 :: Batch 0/1
Batch Loss = 0.0007309484141855868
369, epoch_train_loss=0.0007309484141855868
Epoch 370
Epoch 370 :: Batch 0/1
Batch Loss = 0.0007309484141855853
370, epoch_train_loss=0.0007309484141855853
Epoch 371
Epoch 371 :: Batch 0/1
Batch Loss = 0.0007309484141855841
371, epoch_train_loss=0.0007309484141855841
Epoch 372
Epoch 372 :: Batch 0/1
Batch Loss = 0.0007309484141855827
372, epoch_train_loss=0.0007309484141855827
Epoch 373
Epoch 373 :: Batch 0/1
Batch Loss = 0.0007309484141855815
373, epoch_train_loss=0.0007309484141855815
Epoch 374
Epoch 374 :: Batch 0/1
Batch Loss = 0.0007309484141855801
374, epoch_train_loss=0.0007309484141855801
Epoch 375
Epoch 375 :: Batch 0/1
Batch Loss = 0.0007309484141855788
375, epoch_train_loss=0.0007309484141855788
Epoch 376
Epoch 376 :: Batch 0/1
Batch Loss = 0.0007309484141855774
376, epoch_train_loss=0.0007309484141855774
Epoch 377
Epoch 377 :: Batch 0/1
Batch Loss = 0.000730948414185576
377, epoch_train_loss=0.000730948414185576
Epoch 378
Epoch 378 :: Batch 0/1
Batch Loss = 0.0007309484141855747
378, epoch_train_loss=0.0007309484141855747
Epoch 379
Epoch 379 :: Batch 0/1
Batch Loss = 0.0007309484141855734
379, epoch_train_loss=0.0007309484141855734
Epoch 380
Epoch 380 :: Batch 0/1
Batch Loss = 0.000730948414185572
380, epoch_train_loss=0.000730948414185572
Epoch 381
Epoch 381 :: Batch 0/1
Batch Loss = 0.0007309484141855708
381, epoch_train_loss=0.0007309484141855708
Epoch 382
Epoch 382 :: Batch 0/1
Batch Loss = 0.0007309484141855692
382, epoch_train_loss=0.0007309484141855692
Epoch 383
Epoch 383 :: Batch 0/1
Batch Loss = 0.0007309484141855679
383, epoch_train_loss=0.0007309484141855679
Epoch 384
Epoch 384 :: Batch 0/1
Batch Loss = 0.0007309484141855666
384, epoch_train_loss=0.0007309484141855666
Epoch 385
Epoch 385 :: Batch 0/1
Batch Loss = 0.0007309484141855652
385, epoch_train_loss=0.0007309484141855652
Epoch 386
Epoch 386 :: Batch 0/1
Batch Loss = 0.0007309484141855637
386, epoch_train_loss=0.0007309484141855637
Epoch 387
Epoch 387 :: Batch 0/1
Batch Loss = 0.0007309484141855625
387, epoch_train_loss=0.0007309484141855625
Epoch 388
Epoch 388 :: Batch 0/1
Batch Loss = 0.000730948414185561
388, epoch_train_loss=0.000730948414185561
Epoch 389
Epoch 389 :: Batch 0/1
Batch Loss = 0.0007309484141855597
389, epoch_train_loss=0.0007309484141855597
Epoch 390
Epoch 390 :: Batch 0/1
Batch Loss = 0.0007309484141855582
390, epoch_train_loss=0.0007309484141855582
Epoch 391
Epoch 391 :: Batch 0/1
Batch Loss = 0.0007309484141855569
391, epoch_train_loss=0.0007309484141855569
Epoch 392
Epoch 392 :: Batch 0/1
Batch Loss = 0.0007309484141855556
392, epoch_train_loss=0.0007309484141855556
Epoch 393
Epoch 393 :: Batch 0/1
Batch Loss = 0.0007309484141855542
393, epoch_train_loss=0.0007309484141855542
Epoch 394
Epoch 394 :: Batch 0/1
Batch Loss = 0.0007309484141855529
394, epoch_train_loss=0.0007309484141855529
Epoch 395
Epoch 395 :: Batch 0/1
Batch Loss = 0.0007309484141855513
395, epoch_train_loss=0.0007309484141855513
Epoch 396
Epoch 396 :: Batch 0/1
Batch Loss = 0.00073094841418555
396, epoch_train_loss=0.00073094841418555
Epoch 397
Epoch 397 :: Batch 0/1
Batch Loss = 0.0007309484141855486
397, epoch_train_loss=0.0007309484141855486
Epoch 398
Epoch 398 :: Batch 0/1
Batch Loss = 0.0007309484141855473
398, epoch_train_loss=0.0007309484141855473
Epoch 399
Epoch 399 :: Batch 0/1
Batch Loss = 0.0007309484141855459
399, epoch_train_loss=0.0007309484141855459
Epoch 400
Epoch 400 :: Batch 0/1
Batch Loss = 0.0007309484141855444
400, epoch_train_loss=0.0007309484141855444
Epoch 401
Epoch 401 :: Batch 0/1
Batch Loss = 0.0007309484141855431
401, epoch_train_loss=0.0007309484141855431
Epoch 402
Epoch 402 :: Batch 0/1
Batch Loss = 0.0007309484141855417
402, epoch_train_loss=0.0007309484141855417
Epoch 403
Epoch 403 :: Batch 0/1
Batch Loss = 0.0007309484141855403
403, epoch_train_loss=0.0007309484141855403
Epoch 404
Epoch 404 :: Batch 0/1
Batch Loss = 0.0007309484141855389
404, epoch_train_loss=0.0007309484141855389
Epoch 405
Epoch 405 :: Batch 0/1
Batch Loss = 0.0007309484141855375
405, epoch_train_loss=0.0007309484141855375
Epoch 406
Epoch 406 :: Batch 0/1
Batch Loss = 0.0007309484141855361
406, epoch_train_loss=0.0007309484141855361
Epoch 407
Epoch 407 :: Batch 0/1
Batch Loss = 0.0007309484141855348
407, epoch_train_loss=0.0007309484141855348
Epoch 408
Epoch 408 :: Batch 0/1
Batch Loss = 0.0007309484141855331
408, epoch_train_loss=0.0007309484141855331
Epoch 409
Epoch 409 :: Batch 0/1
Batch Loss = 0.0007309484141855318
409, epoch_train_loss=0.0007309484141855318
Epoch 410
Epoch 410 :: Batch 0/1
Batch Loss = 0.0007309484141855304
410, epoch_train_loss=0.0007309484141855304
Epoch 411
Epoch 411 :: Batch 0/1
Batch Loss = 0.000730948414185529
411, epoch_train_loss=0.000730948414185529
Epoch 412
Epoch 412 :: Batch 0/1
Batch Loss = 0.0007309484141855275
412, epoch_train_loss=0.0007309484141855275
Epoch 413
Epoch 413 :: Batch 0/1
Batch Loss = 0.0007309484141855261
413, epoch_train_loss=0.0007309484141855261
Epoch 414
Epoch 414 :: Batch 0/1
Batch Loss = 0.0007309484141855247
414, epoch_train_loss=0.0007309484141855247
Epoch 415
Epoch 415 :: Batch 0/1
Batch Loss = 0.0007309484141855233
415, epoch_train_loss=0.0007309484141855233
Epoch 416
Epoch 416 :: Batch 0/1
Batch Loss = 0.0007309484141855217
416, epoch_train_loss=0.0007309484141855217
Epoch 417
Epoch 417 :: Batch 0/1
Batch Loss = 0.0007309484141855204
417, epoch_train_loss=0.0007309484141855204
Epoch 418
Epoch 418 :: Batch 0/1
Batch Loss = 0.000730948414185519
418, epoch_train_loss=0.000730948414185519
Epoch 419
Epoch 419 :: Batch 0/1
Batch Loss = 0.0007309484141855176
419, epoch_train_loss=0.0007309484141855176
Epoch 420
Epoch 420 :: Batch 0/1
Batch Loss = 0.0007309484141855161
420, epoch_train_loss=0.0007309484141855161
Epoch 421
Epoch 421 :: Batch 0/1
Batch Loss = 0.0007309484141855147
421, epoch_train_loss=0.0007309484141855147
Epoch 422
Epoch 422 :: Batch 0/1
Batch Loss = 0.0007309484141855132
422, epoch_train_loss=0.0007309484141855132
Epoch 423
Epoch 423 :: Batch 0/1
Batch Loss = 0.0007309484141855118
423, epoch_train_loss=0.0007309484141855118
Epoch 424
Epoch 424 :: Batch 0/1
Batch Loss = 0.0007309484141855104
424, epoch_train_loss=0.0007309484141855104
Epoch 425
Epoch 425 :: Batch 0/1
Batch Loss = 0.0007309484141855088
425, epoch_train_loss=0.0007309484141855088
Epoch 426
Epoch 426 :: Batch 0/1
Batch Loss = 0.0007309484141855074
426, epoch_train_loss=0.0007309484141855074
Epoch 427
Epoch 427 :: Batch 0/1
Batch Loss = 0.000730948414185506
427, epoch_train_loss=0.000730948414185506
Epoch 428
Epoch 428 :: Batch 0/1
Batch Loss = 0.0007309484141855045
428, epoch_train_loss=0.0007309484141855045
Epoch 429
Epoch 429 :: Batch 0/1
Batch Loss = 0.0007309484141855031
429, epoch_train_loss=0.0007309484141855031
Epoch 430
Epoch 430 :: Batch 0/1
Batch Loss = 0.0007309484141855016
430, epoch_train_loss=0.0007309484141855016
Epoch 431
Epoch 431 :: Batch 0/1
Batch Loss = 0.0007309484141855002
431, epoch_train_loss=0.0007309484141855002
Epoch 432
Epoch 432 :: Batch 0/1
Batch Loss = 0.0007309484141854988
432, epoch_train_loss=0.0007309484141854988
Epoch 433
Epoch 433 :: Batch 0/1
Batch Loss = 0.0007309484141854974
433, epoch_train_loss=0.0007309484141854974
Epoch 434
Epoch 434 :: Batch 0/1
Batch Loss = 0.0007309484141854958
434, epoch_train_loss=0.0007309484141854958
Epoch 435
Epoch 435 :: Batch 0/1
Batch Loss = 0.0007309484141854943
435, epoch_train_loss=0.0007309484141854943
Epoch 436
Epoch 436 :: Batch 0/1
Batch Loss = 0.0007309484141854929
436, epoch_train_loss=0.0007309484141854929
Epoch 437
Epoch 437 :: Batch 0/1
Batch Loss = 0.0007309484141854913
437, epoch_train_loss=0.0007309484141854913
Epoch 438
Epoch 438 :: Batch 0/1
Batch Loss = 0.0007309484141854899
438, epoch_train_loss=0.0007309484141854899
Epoch 439
Epoch 439 :: Batch 0/1
Batch Loss = 0.0007309484141854885
439, epoch_train_loss=0.0007309484141854885
Epoch 440
Epoch 440 :: Batch 0/1
Batch Loss = 0.000730948414185487
440, epoch_train_loss=0.000730948414185487
Epoch 441
Epoch 441 :: Batch 0/1
Batch Loss = 0.0007309484141854855
441, epoch_train_loss=0.0007309484141854855
Epoch 442
Epoch 442 :: Batch 0/1
Batch Loss = 0.0007309484141854841
442, epoch_train_loss=0.0007309484141854841
Epoch 443
Epoch 443 :: Batch 0/1
Batch Loss = 0.0007309484141854824
443, epoch_train_loss=0.0007309484141854824
Epoch 444
Epoch 444 :: Batch 0/1
Batch Loss = 0.000730948414185481
444, epoch_train_loss=0.000730948414185481
Epoch 445
Epoch 445 :: Batch 0/1
Batch Loss = 0.0007309484141854796
445, epoch_train_loss=0.0007309484141854796
Epoch 446
Epoch 446 :: Batch 0/1
Batch Loss = 0.0007309484141854782
446, epoch_train_loss=0.0007309484141854782
Epoch 447
Epoch 447 :: Batch 0/1
Batch Loss = 0.0007309484141854765
447, epoch_train_loss=0.0007309484141854765
Epoch 448
Epoch 448 :: Batch 0/1
Batch Loss = 0.000730948414185475
448, epoch_train_loss=0.000730948414185475
Epoch 449
Epoch 449 :: Batch 0/1
Batch Loss = 0.0007309484141854736
449, epoch_train_loss=0.0007309484141854736
Epoch 450
Epoch 450 :: Batch 0/1
Batch Loss = 0.0007309484141854721
450, epoch_train_loss=0.0007309484141854721
Epoch 451
Epoch 451 :: Batch 0/1
Batch Loss = 0.0007309484141854706
451, epoch_train_loss=0.0007309484141854706
Epoch 452
Epoch 452 :: Batch 0/1
Batch Loss = 0.000730948414185469
452, epoch_train_loss=0.000730948414185469
Epoch 453
Epoch 453 :: Batch 0/1
Batch Loss = 0.0007309484141854675
453, epoch_train_loss=0.0007309484141854675
Epoch 454
Epoch 454 :: Batch 0/1
Batch Loss = 0.0007309484141854661
454, epoch_train_loss=0.0007309484141854661
Epoch 455
Epoch 455 :: Batch 0/1
Batch Loss = 0.0007309484141854646
455, epoch_train_loss=0.0007309484141854646
Epoch 456
Epoch 456 :: Batch 0/1
Batch Loss = 0.000730948414185463
456, epoch_train_loss=0.000730948414185463
Epoch 457
Epoch 457 :: Batch 0/1
Batch Loss = 0.0007309484141854616
457, epoch_train_loss=0.0007309484141854616
Epoch 458
Epoch 458 :: Batch 0/1
Batch Loss = 0.00073094841418546
458, epoch_train_loss=0.00073094841418546
Epoch 459
Epoch 459 :: Batch 0/1
Batch Loss = 0.0007309484141854584
459, epoch_train_loss=0.0007309484141854584
Epoch 460
Epoch 460 :: Batch 0/1
Batch Loss = 0.000730948414185457
460, epoch_train_loss=0.000730948414185457
Epoch 461
Epoch 461 :: Batch 0/1
Batch Loss = 0.0007309484141854555
461, epoch_train_loss=0.0007309484141854555
Epoch 462
Epoch 462 :: Batch 0/1
Batch Loss = 0.0007309484141854539
462, epoch_train_loss=0.0007309484141854539
Epoch 463
Epoch 463 :: Batch 0/1
Batch Loss = 0.0007309484141854525
463, epoch_train_loss=0.0007309484141854525
Epoch 464
Epoch 464 :: Batch 0/1
Batch Loss = 0.000730948414185451
464, epoch_train_loss=0.000730948414185451
Epoch 465
Epoch 465 :: Batch 0/1
Batch Loss = 0.0007309484141854493
465, epoch_train_loss=0.0007309484141854493
Epoch 466
Epoch 466 :: Batch 0/1
Batch Loss = 0.0007309484141854479
466, epoch_train_loss=0.0007309484141854479
Epoch 467
Epoch 467 :: Batch 0/1
Batch Loss = 0.0007309484141854465
467, epoch_train_loss=0.0007309484141854465
Epoch 468
Epoch 468 :: Batch 0/1
Batch Loss = 0.0007309484141854448
468, epoch_train_loss=0.0007309484141854448
Epoch 469
Epoch 469 :: Batch 0/1
Batch Loss = 0.0007309484141854434
469, epoch_train_loss=0.0007309484141854434
Epoch 470
Epoch 470 :: Batch 0/1
Batch Loss = 0.0007309484141854418
470, epoch_train_loss=0.0007309484141854418
Epoch 471
Epoch 471 :: Batch 0/1
Batch Loss = 0.0007309484141854402
471, epoch_train_loss=0.0007309484141854402
Epoch 472
Epoch 472 :: Batch 0/1
Batch Loss = 0.0007309484141854386
472, epoch_train_loss=0.0007309484141854386
Epoch 473
Epoch 473 :: Batch 0/1
Batch Loss = 0.0007309484141854371
473, epoch_train_loss=0.0007309484141854371
Epoch 474
Epoch 474 :: Batch 0/1
Batch Loss = 0.0007309484141854356
474, epoch_train_loss=0.0007309484141854356
Epoch 475
Epoch 475 :: Batch 0/1
Batch Loss = 0.000730948414185434
475, epoch_train_loss=0.000730948414185434
Epoch 476
Epoch 476 :: Batch 0/1
Batch Loss = 0.0007309484141854325
476, epoch_train_loss=0.0007309484141854325
Epoch 477
Epoch 477 :: Batch 0/1
Batch Loss = 0.0007309484141854309
477, epoch_train_loss=0.0007309484141854309
Epoch 478
Epoch 478 :: Batch 0/1
Batch Loss = 0.0007309484141854294
478, epoch_train_loss=0.0007309484141854294
Epoch 479
Epoch 479 :: Batch 0/1
Batch Loss = 0.0007309484141854277
479, epoch_train_loss=0.0007309484141854277
Epoch 480
Epoch 480 :: Batch 0/1
Batch Loss = 0.0007309484141854263
480, epoch_train_loss=0.0007309484141854263
Epoch 481
Epoch 481 :: Batch 0/1
Batch Loss = 0.0007309484141854247
481, epoch_train_loss=0.0007309484141854247
Epoch 482
Epoch 482 :: Batch 0/1
Batch Loss = 0.0007309484141854232
482, epoch_train_loss=0.0007309484141854232
Epoch 483
Epoch 483 :: Batch 0/1
Batch Loss = 0.0007309484141854216
483, epoch_train_loss=0.0007309484141854216
Epoch 484
Epoch 484 :: Batch 0/1
Batch Loss = 0.0007309484141854199
484, epoch_train_loss=0.0007309484141854199
Epoch 485
Epoch 485 :: Batch 0/1
Batch Loss = 0.0007309484141854184
485, epoch_train_loss=0.0007309484141854184
Epoch 486
Epoch 486 :: Batch 0/1
Batch Loss = 0.000730948414185417
486, epoch_train_loss=0.000730948414185417
Epoch 487
Epoch 487 :: Batch 0/1
Batch Loss = 0.0007309484141854153
487, epoch_train_loss=0.0007309484141854153
Epoch 488
Epoch 488 :: Batch 0/1
Batch Loss = 0.0007309484141854137
488, epoch_train_loss=0.0007309484141854137
Epoch 489
Epoch 489 :: Batch 0/1
Batch Loss = 0.0007309484141854121
489, epoch_train_loss=0.0007309484141854121
Epoch 490
Epoch 490 :: Batch 0/1
Batch Loss = 0.0007309484141854106
490, epoch_train_loss=0.0007309484141854106
Epoch 491
Epoch 491 :: Batch 0/1
Batch Loss = 0.0007309484141854091
491, epoch_train_loss=0.0007309484141854091
Epoch 492
Epoch 492 :: Batch 0/1
Batch Loss = 0.0007309484141854074
492, epoch_train_loss=0.0007309484141854074
Epoch 493
Epoch 493 :: Batch 0/1
Batch Loss = 0.0007309484141854058
493, epoch_train_loss=0.0007309484141854058
Epoch 494
Epoch 494 :: Batch 0/1
Batch Loss = 0.0007309484141854042
494, epoch_train_loss=0.0007309484141854042
Epoch 495
Epoch 495 :: Batch 0/1
Batch Loss = 0.0007309484141854026
495, epoch_train_loss=0.0007309484141854026
Epoch 496
Epoch 496 :: Batch 0/1
Batch Loss = 0.0007309484141854011
496, epoch_train_loss=0.0007309484141854011
Epoch 497
Epoch 497 :: Batch 0/1
Batch Loss = 0.0007309484141853994
497, epoch_train_loss=0.0007309484141853994
Epoch 498
Epoch 498 :: Batch 0/1
Batch Loss = 0.0007309484141853979
498, epoch_train_loss=0.0007309484141853979
Epoch 499
Epoch 499 :: Batch 0/1
Batch Loss = 0.0007309484141853962
499, epoch_train_loss=0.0007309484141853962
Epoch 500
Epoch 500 :: Batch 0/1
Batch Loss = 0.0007309484141853947
500, epoch_train_loss=0.0007309484141853947
Epoch 501
Epoch 501 :: Batch 0/1
Batch Loss = 0.0007309484141853932
501, epoch_train_loss=0.0007309484141853932
Epoch 502
Epoch 502 :: Batch 0/1
Batch Loss = 0.0007309484141853915
502, epoch_train_loss=0.0007309484141853915
Epoch 503
Epoch 503 :: Batch 0/1
Batch Loss = 0.0007309484141853899
503, epoch_train_loss=0.0007309484141853899
Epoch 504
Epoch 504 :: Batch 0/1
Batch Loss = 0.0007309484141853883
504, epoch_train_loss=0.0007309484141853883
Epoch 505
Epoch 505 :: Batch 0/1
Batch Loss = 0.0007309484141853868
505, epoch_train_loss=0.0007309484141853868
Epoch 506
Epoch 506 :: Batch 0/1
Batch Loss = 0.000730948414185385
506, epoch_train_loss=0.000730948414185385
Epoch 507
Epoch 507 :: Batch 0/1
Batch Loss = 0.0007309484141853835
507, epoch_train_loss=0.0007309484141853835
Epoch 508
Epoch 508 :: Batch 0/1
Batch Loss = 0.000730948414185382
508, epoch_train_loss=0.000730948414185382
Epoch 509
Epoch 509 :: Batch 0/1
Batch Loss = 0.0007309484141853803
509, epoch_train_loss=0.0007309484141853803
Epoch 510
Epoch 510 :: Batch 0/1
Batch Loss = 0.0007309484141853787
510, epoch_train_loss=0.0007309484141853787
Epoch 511
Epoch 511 :: Batch 0/1
Batch Loss = 0.000730948414185377
511, epoch_train_loss=0.000730948414185377
Epoch 512
Epoch 512 :: Batch 0/1
Batch Loss = 0.0007309484141853755
512, epoch_train_loss=0.0007309484141853755
Epoch 513
Epoch 513 :: Batch 0/1
Batch Loss = 0.0007309484141853738
513, epoch_train_loss=0.0007309484141853738
Epoch 514
Epoch 514 :: Batch 0/1
Batch Loss = 0.0007309484141853721
514, epoch_train_loss=0.0007309484141853721
Epoch 515
Epoch 515 :: Batch 0/1
Batch Loss = 0.0007309484141853704
515, epoch_train_loss=0.0007309484141853704
Epoch 516
Epoch 516 :: Batch 0/1
Batch Loss = 0.000730948414185369
516, epoch_train_loss=0.000730948414185369
Epoch 517
Epoch 517 :: Batch 0/1
Batch Loss = 0.0007309484141853674
517, epoch_train_loss=0.0007309484141853674
Epoch 518
Epoch 518 :: Batch 0/1
Batch Loss = 0.0007309484141853656
518, epoch_train_loss=0.0007309484141853656
Epoch 519
Epoch 519 :: Batch 0/1
Batch Loss = 0.000730948414185364
519, epoch_train_loss=0.000730948414185364
Epoch 520
Epoch 520 :: Batch 0/1
Batch Loss = 0.0007309484141853623
520, epoch_train_loss=0.0007309484141853623
Epoch 521
Epoch 521 :: Batch 0/1
Batch Loss = 0.0007309484141853607
521, epoch_train_loss=0.0007309484141853607
Epoch 522
Epoch 522 :: Batch 0/1
Batch Loss = 0.0007309484141853592
522, epoch_train_loss=0.0007309484141853592
Epoch 523
Epoch 523 :: Batch 0/1
Batch Loss = 0.0007309484141853575
523, epoch_train_loss=0.0007309484141853575
Epoch 524
Epoch 524 :: Batch 0/1
Batch Loss = 0.0007309484141853559
524, epoch_train_loss=0.0007309484141853559
Epoch 525
Epoch 525 :: Batch 0/1
Batch Loss = 0.0007309484141853541
525, epoch_train_loss=0.0007309484141853541
Epoch 526
Epoch 526 :: Batch 0/1
Batch Loss = 0.0007309484141853525
526, epoch_train_loss=0.0007309484141853525
Epoch 527
Epoch 527 :: Batch 0/1
Batch Loss = 0.0007309484141853509
527, epoch_train_loss=0.0007309484141853509
Epoch 528
Epoch 528 :: Batch 0/1
Batch Loss = 0.0007309484141853494
528, epoch_train_loss=0.0007309484141853494
Epoch 529
Epoch 529 :: Batch 0/1
Batch Loss = 0.0007309484141853476
529, epoch_train_loss=0.0007309484141853476
Epoch 530
Epoch 530 :: Batch 0/1
Batch Loss = 0.000730948414185346
530, epoch_train_loss=0.000730948414185346
Epoch 531
Epoch 531 :: Batch 0/1
Batch Loss = 0.0007309484141853443
531, epoch_train_loss=0.0007309484141853443
Epoch 532
Epoch 532 :: Batch 0/1
Batch Loss = 0.0007309484141853426
532, epoch_train_loss=0.0007309484141853426
Epoch 533
Epoch 533 :: Batch 0/1
Batch Loss = 0.0007309484141853409
533, epoch_train_loss=0.0007309484141853409
Epoch 534
Epoch 534 :: Batch 0/1
Batch Loss = 0.0007309484141853393
534, epoch_train_loss=0.0007309484141853393
Epoch 535
Epoch 535 :: Batch 0/1
Batch Loss = 0.0007309484141853375
535, epoch_train_loss=0.0007309484141853375
Epoch 536
Epoch 536 :: Batch 0/1
Batch Loss = 0.0007309484141853359
536, epoch_train_loss=0.0007309484141853359
Epoch 537
Epoch 537 :: Batch 0/1
Batch Loss = 0.0007309484141853342
537, epoch_train_loss=0.0007309484141853342
Epoch 538
Epoch 538 :: Batch 0/1
Batch Loss = 0.0007309484141853327
538, epoch_train_loss=0.0007309484141853327
Epoch 539
Epoch 539 :: Batch 0/1
Batch Loss = 0.000730948414185331
539, epoch_train_loss=0.000730948414185331
Epoch 540
Epoch 540 :: Batch 0/1
Batch Loss = 0.0007309484141853292
540, epoch_train_loss=0.0007309484141853292
Epoch 541
Epoch 541 :: Batch 0/1
Batch Loss = 0.0007309484141853276
541, epoch_train_loss=0.0007309484141853276
Epoch 542
Epoch 542 :: Batch 0/1
Batch Loss = 0.0007309484141853259
542, epoch_train_loss=0.0007309484141853259
Epoch 543
Epoch 543 :: Batch 0/1
Batch Loss = 0.0007309484141853243
543, epoch_train_loss=0.0007309484141853243
Epoch 544
Epoch 544 :: Batch 0/1
Batch Loss = 0.0007309484141853225
544, epoch_train_loss=0.0007309484141853225
Epoch 545
Epoch 545 :: Batch 0/1
Batch Loss = 0.000730948414185321
545, epoch_train_loss=0.000730948414185321
Epoch 546
Epoch 546 :: Batch 0/1
Batch Loss = 0.0007309484141853192
546, epoch_train_loss=0.0007309484141853192
Epoch 547
Epoch 547 :: Batch 0/1
Batch Loss = 0.0007309484141853175
547, epoch_train_loss=0.0007309484141853175
Epoch 548
Epoch 548 :: Batch 0/1
Batch Loss = 0.0007309484141853159
548, epoch_train_loss=0.0007309484141853159
Epoch 549
Epoch 549 :: Batch 0/1
Batch Loss = 0.0007309484141853141
549, epoch_train_loss=0.0007309484141853141
Epoch 550
Epoch 550 :: Batch 0/1
Batch Loss = 0.0007309484141853124
550, epoch_train_loss=0.0007309484141853124
Epoch 551
Epoch 551 :: Batch 0/1
Batch Loss = 0.0007309484141853108
551, epoch_train_loss=0.0007309484141853108
Epoch 552
Epoch 552 :: Batch 0/1
Batch Loss = 0.000730948414185309
552, epoch_train_loss=0.000730948414185309
Epoch 553
Epoch 553 :: Batch 0/1
Batch Loss = 0.0007309484141853074
553, epoch_train_loss=0.0007309484141853074
Epoch 554
Epoch 554 :: Batch 0/1
Batch Loss = 0.0007309484141853057
554, epoch_train_loss=0.0007309484141853057
Epoch 555
Epoch 555 :: Batch 0/1
Batch Loss = 0.000730948414185304
555, epoch_train_loss=0.000730948414185304
Epoch 556
Epoch 556 :: Batch 0/1
Batch Loss = 0.0007309484141853022
556, epoch_train_loss=0.0007309484141853022
Epoch 557
Epoch 557 :: Batch 0/1
Batch Loss = 0.0007309484141853007
557, epoch_train_loss=0.0007309484141853007
Epoch 558
Epoch 558 :: Batch 0/1
Batch Loss = 0.0007309484141852987
558, epoch_train_loss=0.0007309484141852987
Epoch 559
Epoch 559 :: Batch 0/1
Batch Loss = 0.0007309484141852971
559, epoch_train_loss=0.0007309484141852971
Epoch 560
Epoch 560 :: Batch 0/1
Batch Loss = 0.0007309484141852954
560, epoch_train_loss=0.0007309484141852954
Epoch 561
Epoch 561 :: Batch 0/1
Batch Loss = 0.0007309484141852937
561, epoch_train_loss=0.0007309484141852937
Epoch 562
Epoch 562 :: Batch 0/1
Batch Loss = 0.0007309484141852919
562, epoch_train_loss=0.0007309484141852919
Epoch 563
Epoch 563 :: Batch 0/1
Batch Loss = 0.0007309484141852903
563, epoch_train_loss=0.0007309484141852903
Epoch 564
Epoch 564 :: Batch 0/1
Batch Loss = 0.0007309484141852884
564, epoch_train_loss=0.0007309484141852884
Epoch 565
Epoch 565 :: Batch 0/1
Batch Loss = 0.0007309484141852867
565, epoch_train_loss=0.0007309484141852867
Epoch 566
Epoch 566 :: Batch 0/1
Batch Loss = 0.0007309484141852851
566, epoch_train_loss=0.0007309484141852851
Epoch 567
Epoch 567 :: Batch 0/1
Batch Loss = 0.0007309484141852834
567, epoch_train_loss=0.0007309484141852834
Epoch 568
Epoch 568 :: Batch 0/1
Batch Loss = 0.0007309484141852817
568, epoch_train_loss=0.0007309484141852817
Epoch 569
Epoch 569 :: Batch 0/1
Batch Loss = 0.0007309484141852799
569, epoch_train_loss=0.0007309484141852799
Epoch 570
Epoch 570 :: Batch 0/1
Batch Loss = 0.0007309484141852782
570, epoch_train_loss=0.0007309484141852782
Epoch 571
Epoch 571 :: Batch 0/1
Batch Loss = 0.0007309484141852764
571, epoch_train_loss=0.0007309484141852764
Epoch 572
Epoch 572 :: Batch 0/1
Batch Loss = 0.0007309484141852748
572, epoch_train_loss=0.0007309484141852748
Epoch 573
Epoch 573 :: Batch 0/1
Batch Loss = 0.000730948414185273
573, epoch_train_loss=0.000730948414185273
Epoch 574
Epoch 574 :: Batch 0/1
Batch Loss = 0.0007309484141852712
574, epoch_train_loss=0.0007309484141852712
Epoch 575
Epoch 575 :: Batch 0/1
Batch Loss = 0.0007309484141852695
575, epoch_train_loss=0.0007309484141852695
Epoch 576
Epoch 576 :: Batch 0/1
Batch Loss = 0.0007309484141852678
576, epoch_train_loss=0.0007309484141852678
Epoch 577
Epoch 577 :: Batch 0/1
Batch Loss = 0.000730948414185266
577, epoch_train_loss=0.000730948414185266
Epoch 578
Epoch 578 :: Batch 0/1
Batch Loss = 0.0007309484141852644
578, epoch_train_loss=0.0007309484141852644
Epoch 579
Epoch 579 :: Batch 0/1
Batch Loss = 0.0007309484141852625
579, epoch_train_loss=0.0007309484141852625
Epoch 580
Epoch 580 :: Batch 0/1
Batch Loss = 0.0007309484141852607
580, epoch_train_loss=0.0007309484141852607
Epoch 581
Epoch 581 :: Batch 0/1
Batch Loss = 0.000730948414185259
581, epoch_train_loss=0.000730948414185259
Epoch 582
Epoch 582 :: Batch 0/1
Batch Loss = 0.0007309484141852572
582, epoch_train_loss=0.0007309484141852572
Epoch 583
Epoch 583 :: Batch 0/1
Batch Loss = 0.0007309484141852555
583, epoch_train_loss=0.0007309484141852555
Epoch 584
Epoch 584 :: Batch 0/1
Batch Loss = 0.0007309484141852536
584, epoch_train_loss=0.0007309484141852536
Epoch 585
Epoch 585 :: Batch 0/1
Batch Loss = 0.000730948414185252
585, epoch_train_loss=0.000730948414185252
Epoch 586
Epoch 586 :: Batch 0/1
Batch Loss = 0.0007309484141852503
586, epoch_train_loss=0.0007309484141852503
Epoch 587
Epoch 587 :: Batch 0/1
Batch Loss = 0.0007309484141852484
587, epoch_train_loss=0.0007309484141852484
Epoch 588
Epoch 588 :: Batch 0/1
Batch Loss = 0.0007309484141852467
588, epoch_train_loss=0.0007309484141852467
Epoch 589
Epoch 589 :: Batch 0/1
Batch Loss = 0.0007309484141852451
589, epoch_train_loss=0.0007309484141852451
Epoch 590
Epoch 590 :: Batch 0/1
Batch Loss = 0.0007309484141852431
590, epoch_train_loss=0.0007309484141852431
Epoch 591
Epoch 591 :: Batch 0/1
Batch Loss = 0.0007309484141852414
591, epoch_train_loss=0.0007309484141852414
Epoch 592
Epoch 592 :: Batch 0/1
Batch Loss = 0.0007309484141852396
592, epoch_train_loss=0.0007309484141852396
Epoch 593
Epoch 593 :: Batch 0/1
Batch Loss = 0.0007309484141852378
593, epoch_train_loss=0.0007309484141852378
Epoch 594
Epoch 594 :: Batch 0/1
Batch Loss = 0.0007309484141852361
594, epoch_train_loss=0.0007309484141852361
Epoch 595
Epoch 595 :: Batch 0/1
Batch Loss = 0.0007309484141852342
595, epoch_train_loss=0.0007309484141852342
Epoch 596
Epoch 596 :: Batch 0/1
Batch Loss = 0.0007309484141852324
596, epoch_train_loss=0.0007309484141852324
Epoch 597
Epoch 597 :: Batch 0/1
Batch Loss = 0.0007309484141852306
597, epoch_train_loss=0.0007309484141852306
Epoch 598
Epoch 598 :: Batch 0/1
Batch Loss = 0.0007309484141852289
598, epoch_train_loss=0.0007309484141852289
Epoch 599
Epoch 599 :: Batch 0/1
Batch Loss = 0.0007309484141852272
599, epoch_train_loss=0.0007309484141852272
Epoch 600
Epoch 600 :: Batch 0/1
Batch Loss = 0.0007309484141852253
600, epoch_train_loss=0.0007309484141852253
Epoch 601
Epoch 601 :: Batch 0/1
Batch Loss = 0.0007309484141852236
601, epoch_train_loss=0.0007309484141852236
Epoch 602
Epoch 602 :: Batch 0/1
Batch Loss = 0.0007309484141852217
602, epoch_train_loss=0.0007309484141852217
Epoch 603
Epoch 603 :: Batch 0/1
Batch Loss = 0.0007309484141852199
603, epoch_train_loss=0.0007309484141852199
Epoch 604
Epoch 604 :: Batch 0/1
Batch Loss = 0.0007309484141852182
604, epoch_train_loss=0.0007309484141852182
Epoch 605
Epoch 605 :: Batch 0/1
Batch Loss = 0.0007309484141852164
605, epoch_train_loss=0.0007309484141852164
Epoch 606
Epoch 606 :: Batch 0/1
Batch Loss = 0.0007309484141852146
606, epoch_train_loss=0.0007309484141852146
Epoch 607
Epoch 607 :: Batch 0/1
Batch Loss = 0.0007309484141852127
607, epoch_train_loss=0.0007309484141852127
Epoch 608
Epoch 608 :: Batch 0/1
Batch Loss = 0.0007309484141852109
608, epoch_train_loss=0.0007309484141852109
Epoch 609
Epoch 609 :: Batch 0/1
Batch Loss = 0.0007309484141852093
609, epoch_train_loss=0.0007309484141852093
Epoch 610
Epoch 610 :: Batch 0/1
Batch Loss = 0.0007309484141852073
610, epoch_train_loss=0.0007309484141852073
Epoch 611
Epoch 611 :: Batch 0/1
Batch Loss = 0.0007309484141852056
611, epoch_train_loss=0.0007309484141852056
Epoch 612
Epoch 612 :: Batch 0/1
Batch Loss = 0.0007309484141852037
612, epoch_train_loss=0.0007309484141852037
Epoch 613
Epoch 613 :: Batch 0/1
Batch Loss = 0.0007309484141852018
613, epoch_train_loss=0.0007309484141852018
Epoch 614
Epoch 614 :: Batch 0/1
Batch Loss = 0.0007309484141852002
614, epoch_train_loss=0.0007309484141852002
Epoch 615
Epoch 615 :: Batch 0/1
Batch Loss = 0.0007309484141851982
615, epoch_train_loss=0.0007309484141851982
Epoch 616
Epoch 616 :: Batch 0/1
Batch Loss = 0.0007309484141851965
616, epoch_train_loss=0.0007309484141851965
Epoch 617
Epoch 617 :: Batch 0/1
Batch Loss = 0.0007309484141851946
617, epoch_train_loss=0.0007309484141851946
Epoch 618
Epoch 618 :: Batch 0/1
Batch Loss = 0.0007309484141851927
618, epoch_train_loss=0.0007309484141851927
Epoch 619
Epoch 619 :: Batch 0/1
Batch Loss = 0.0007309484141851911
619, epoch_train_loss=0.0007309484141851911
Epoch 620
Epoch 620 :: Batch 0/1
Batch Loss = 0.0007309484141851891
620, epoch_train_loss=0.0007309484141851891
Epoch 621
Epoch 621 :: Batch 0/1
Batch Loss = 0.0007309484141851874
621, epoch_train_loss=0.0007309484141851874
Epoch 622
Epoch 622 :: Batch 0/1
Batch Loss = 0.0007309484141851855
622, epoch_train_loss=0.0007309484141851855
Epoch 623
Epoch 623 :: Batch 0/1
Batch Loss = 0.0007309484141851837
623, epoch_train_loss=0.0007309484141851837
Epoch 624
Epoch 624 :: Batch 0/1
Batch Loss = 0.0007309484141851818
624, epoch_train_loss=0.0007309484141851818
Epoch 625
Epoch 625 :: Batch 0/1
Batch Loss = 0.00073094841418518
625, epoch_train_loss=0.00073094841418518
Epoch 626
Epoch 626 :: Batch 0/1
Batch Loss = 0.0007309484141851783
626, epoch_train_loss=0.0007309484141851783
Epoch 627
Epoch 627 :: Batch 0/1
Batch Loss = 0.0007309484141851764
627, epoch_train_loss=0.0007309484141851764
Epoch 628
Epoch 628 :: Batch 0/1
Batch Loss = 0.0007309484141851745
628, epoch_train_loss=0.0007309484141851745
Epoch 629
Epoch 629 :: Batch 0/1
Batch Loss = 0.0007309484141851726
629, epoch_train_loss=0.0007309484141851726
Epoch 630
Epoch 630 :: Batch 0/1
Batch Loss = 0.0007309484141851709
630, epoch_train_loss=0.0007309484141851709
Epoch 631
Epoch 631 :: Batch 0/1
Batch Loss = 0.0007309484141851689
631, epoch_train_loss=0.0007309484141851689
Epoch 632
Epoch 632 :: Batch 0/1
Batch Loss = 0.0007309484141851671
632, epoch_train_loss=0.0007309484141851671
Epoch 633
Epoch 633 :: Batch 0/1
Batch Loss = 0.0007309484141851653
633, epoch_train_loss=0.0007309484141851653
Epoch 634
Epoch 634 :: Batch 0/1
Batch Loss = 0.0007309484141851635
634, epoch_train_loss=0.0007309484141851635
Epoch 635
Epoch 635 :: Batch 0/1
Batch Loss = 0.0007309484141851616
635, epoch_train_loss=0.0007309484141851616
Epoch 636
Epoch 636 :: Batch 0/1
Batch Loss = 0.0007309484141851596
636, epoch_train_loss=0.0007309484141851596
Epoch 637
Epoch 637 :: Batch 0/1
Batch Loss = 0.0007309484141851579
637, epoch_train_loss=0.0007309484141851579
Epoch 638
Epoch 638 :: Batch 0/1
Batch Loss = 0.0007309484141851559
638, epoch_train_loss=0.0007309484141851559
Epoch 639
Epoch 639 :: Batch 0/1
Batch Loss = 0.0007309484141851541
639, epoch_train_loss=0.0007309484141851541
Epoch 640
Epoch 640 :: Batch 0/1
Batch Loss = 0.0007309484141851522
640, epoch_train_loss=0.0007309484141851522
Epoch 641
Epoch 641 :: Batch 0/1
Batch Loss = 0.0007309484141851504
641, epoch_train_loss=0.0007309484141851504
Epoch 642
Epoch 642 :: Batch 0/1
Batch Loss = 0.0007309484141851485
642, epoch_train_loss=0.0007309484141851485
Epoch 643
Epoch 643 :: Batch 0/1
Batch Loss = 0.0007309484141851466
643, epoch_train_loss=0.0007309484141851466
Epoch 644
Epoch 644 :: Batch 0/1
Batch Loss = 0.0007309484141851448
644, epoch_train_loss=0.0007309484141851448
Epoch 645
Epoch 645 :: Batch 0/1
Batch Loss = 0.0007309484141851428
645, epoch_train_loss=0.0007309484141851428
Epoch 646
Epoch 646 :: Batch 0/1
Batch Loss = 0.0007309484141851411
646, epoch_train_loss=0.0007309484141851411
Epoch 647
Epoch 647 :: Batch 0/1
Batch Loss = 0.000730948414185139
647, epoch_train_loss=0.000730948414185139
Epoch 648
Epoch 648 :: Batch 0/1
Batch Loss = 0.0007309484141851372
648, epoch_train_loss=0.0007309484141851372
Epoch 649
Epoch 649 :: Batch 0/1
Batch Loss = 0.0007309484141851354
649, epoch_train_loss=0.0007309484141851354
Epoch 650
Epoch 650 :: Batch 0/1
Batch Loss = 0.0007309484141851335
650, epoch_train_loss=0.0007309484141851335
Epoch 651
Epoch 651 :: Batch 0/1
Batch Loss = 0.0007309484141851315
651, epoch_train_loss=0.0007309484141851315
Epoch 652
Epoch 652 :: Batch 0/1
Batch Loss = 0.0007309484141851297
652, epoch_train_loss=0.0007309484141851297
Epoch 653
Epoch 653 :: Batch 0/1
Batch Loss = 0.0007309484141851279
653, epoch_train_loss=0.0007309484141851279
Epoch 654
Epoch 654 :: Batch 0/1
Batch Loss = 0.0007309484141851258
654, epoch_train_loss=0.0007309484141851258
Epoch 655
Epoch 655 :: Batch 0/1
Batch Loss = 0.0007309484141851241
655, epoch_train_loss=0.0007309484141851241
Epoch 656
Epoch 656 :: Batch 0/1
Batch Loss = 0.0007309484141851222
656, epoch_train_loss=0.0007309484141851222
Epoch 657
Epoch 657 :: Batch 0/1
Batch Loss = 0.0007309484141851203
657, epoch_train_loss=0.0007309484141851203
Epoch 658
Epoch 658 :: Batch 0/1
Batch Loss = 0.0007309484141851184
658, epoch_train_loss=0.0007309484141851184
Epoch 659
Epoch 659 :: Batch 0/1
Batch Loss = 0.0007309484141851165
659, epoch_train_loss=0.0007309484141851165
Epoch 660
Epoch 660 :: Batch 0/1
Batch Loss = 0.0007309484141851145
660, epoch_train_loss=0.0007309484141851145
Epoch 661
Epoch 661 :: Batch 0/1
Batch Loss = 0.0007309484141851127
661, epoch_train_loss=0.0007309484141851127
Epoch 662
Epoch 662 :: Batch 0/1
Batch Loss = 0.0007309484141851107
662, epoch_train_loss=0.0007309484141851107
Epoch 663
Epoch 663 :: Batch 0/1
Batch Loss = 0.0007309484141851088
663, epoch_train_loss=0.0007309484141851088
Epoch 664
Epoch 664 :: Batch 0/1
Batch Loss = 0.0007309484141851068
664, epoch_train_loss=0.0007309484141851068
Epoch 665
Epoch 665 :: Batch 0/1
Batch Loss = 0.000730948414185105
665, epoch_train_loss=0.000730948414185105
Epoch 666
Epoch 666 :: Batch 0/1
Batch Loss = 0.000730948414185103
666, epoch_train_loss=0.000730948414185103
Epoch 667
Epoch 667 :: Batch 0/1
Batch Loss = 0.0007309484141851011
667, epoch_train_loss=0.0007309484141851011
Epoch 668
Epoch 668 :: Batch 0/1
Batch Loss = 0.0007309484141850992
668, epoch_train_loss=0.0007309484141850992
Epoch 669
Epoch 669 :: Batch 0/1
Batch Loss = 0.0007309484141850973
669, epoch_train_loss=0.0007309484141850973
Epoch 670
Epoch 670 :: Batch 0/1
Batch Loss = 0.0007309484141850953
670, epoch_train_loss=0.0007309484141850953
Epoch 671
Epoch 671 :: Batch 0/1
Batch Loss = 0.0007309484141850935
671, epoch_train_loss=0.0007309484141850935
Epoch 672
Epoch 672 :: Batch 0/1
Batch Loss = 0.0007309484141850915
672, epoch_train_loss=0.0007309484141850915
Epoch 673
Epoch 673 :: Batch 0/1
Batch Loss = 0.0007309484141850896
673, epoch_train_loss=0.0007309484141850896
Epoch 674
Epoch 674 :: Batch 0/1
Batch Loss = 0.0007309484141850877
674, epoch_train_loss=0.0007309484141850877
Epoch 675
Epoch 675 :: Batch 0/1
Batch Loss = 0.0007309484141850858
675, epoch_train_loss=0.0007309484141850858
Epoch 676
Epoch 676 :: Batch 0/1
Batch Loss = 0.0007309484141850838
676, epoch_train_loss=0.0007309484141850838
Epoch 677
Epoch 677 :: Batch 0/1
Batch Loss = 0.0007309484141850818
677, epoch_train_loss=0.0007309484141850818
Epoch 678
Epoch 678 :: Batch 0/1
Batch Loss = 0.00073094841418508
678, epoch_train_loss=0.00073094841418508
Epoch 679
Epoch 679 :: Batch 0/1
Batch Loss = 0.000730948414185078
679, epoch_train_loss=0.000730948414185078
Epoch 680
Epoch 680 :: Batch 0/1
Batch Loss = 0.0007309484141850761
680, epoch_train_loss=0.0007309484141850761
Epoch 681
Epoch 681 :: Batch 0/1
Batch Loss = 0.0007309484141850743
681, epoch_train_loss=0.0007309484141850743
Epoch 682
Epoch 682 :: Batch 0/1
Batch Loss = 0.0007309484141850722
682, epoch_train_loss=0.0007309484141850722
Epoch 683
Epoch 683 :: Batch 0/1
Batch Loss = 0.0007309484141850703
683, epoch_train_loss=0.0007309484141850703
Epoch 684
Epoch 684 :: Batch 0/1
Batch Loss = 0.0007309484141850682
684, epoch_train_loss=0.0007309484141850682
Epoch 685
Epoch 685 :: Batch 0/1
Batch Loss = 0.0007309484141850664
685, epoch_train_loss=0.0007309484141850664
Epoch 686
Epoch 686 :: Batch 0/1
Batch Loss = 0.0007309484141850644
686, epoch_train_loss=0.0007309484141850644
Epoch 687
Epoch 687 :: Batch 0/1
Batch Loss = 0.0007309484141850625
687, epoch_train_loss=0.0007309484141850625
Epoch 688
Epoch 688 :: Batch 0/1
Batch Loss = 0.0007309484141850604
688, epoch_train_loss=0.0007309484141850604
Epoch 689
Epoch 689 :: Batch 0/1
Batch Loss = 0.0007309484141850585
689, epoch_train_loss=0.0007309484141850585
Epoch 690
Epoch 690 :: Batch 0/1
Batch Loss = 0.0007309484141850565
690, epoch_train_loss=0.0007309484141850565
Epoch 691
Epoch 691 :: Batch 0/1
Batch Loss = 0.0007309484141850546
691, epoch_train_loss=0.0007309484141850546
Epoch 692
Epoch 692 :: Batch 0/1
Batch Loss = 0.0007309484141850527
692, epoch_train_loss=0.0007309484141850527
Epoch 693
Epoch 693 :: Batch 0/1
Batch Loss = 0.0007309484141850507
693, epoch_train_loss=0.0007309484141850507
Epoch 694
Epoch 694 :: Batch 0/1
Batch Loss = 0.0007309484141850488
694, epoch_train_loss=0.0007309484141850488
Epoch 695
Epoch 695 :: Batch 0/1
Batch Loss = 0.0007309484141850468
695, epoch_train_loss=0.0007309484141850468
Epoch 696
Epoch 696 :: Batch 0/1
Batch Loss = 0.0007309484141850448
696, epoch_train_loss=0.0007309484141850448
Epoch 697
Epoch 697 :: Batch 0/1
Batch Loss = 0.0007309484141850429
697, epoch_train_loss=0.0007309484141850429
Epoch 698
Epoch 698 :: Batch 0/1
Batch Loss = 0.0007309484141850408
698, epoch_train_loss=0.0007309484141850408
Epoch 699
Epoch 699 :: Batch 0/1
Batch Loss = 0.0007309484141850387
699, epoch_train_loss=0.0007309484141850387
Epoch 700
Epoch 700 :: Batch 0/1
Batch Loss = 0.000730948414185037
700, epoch_train_loss=0.000730948414185037
Epoch 701
Epoch 701 :: Batch 0/1
Batch Loss = 0.0007309484141850349
701, epoch_train_loss=0.0007309484141850349
Epoch 702
Epoch 702 :: Batch 0/1
Batch Loss = 0.000730948414185033
702, epoch_train_loss=0.000730948414185033
Epoch 703
Epoch 703 :: Batch 0/1
Batch Loss = 0.0007309484141850309
703, epoch_train_loss=0.0007309484141850309
Epoch 704
Epoch 704 :: Batch 0/1
Batch Loss = 0.000730948414185029
704, epoch_train_loss=0.000730948414185029
Epoch 705
Epoch 705 :: Batch 0/1
Batch Loss = 0.000730948414185027
705, epoch_train_loss=0.000730948414185027
Epoch 706
Epoch 706 :: Batch 0/1
Batch Loss = 0.0007309484141850249
706, epoch_train_loss=0.0007309484141850249
Epoch 707
Epoch 707 :: Batch 0/1
Batch Loss = 0.0007309484141850229
707, epoch_train_loss=0.0007309484141850229
Epoch 708
Epoch 708 :: Batch 0/1
Batch Loss = 0.0007309484141850211
708, epoch_train_loss=0.0007309484141850211
Epoch 709
Epoch 709 :: Batch 0/1
Batch Loss = 0.000730948414185019
709, epoch_train_loss=0.000730948414185019
Epoch 710
Epoch 710 :: Batch 0/1
Batch Loss = 0.000730948414185017
710, epoch_train_loss=0.000730948414185017
Epoch 711
Epoch 711 :: Batch 0/1
Batch Loss = 0.000730948414185015
711, epoch_train_loss=0.000730948414185015
Epoch 712
Epoch 712 :: Batch 0/1
Batch Loss = 0.0007309484141850131
712, epoch_train_loss=0.0007309484141850131
Epoch 713
Epoch 713 :: Batch 0/1
Batch Loss = 0.000730948414185011
713, epoch_train_loss=0.000730948414185011
Epoch 714
Epoch 714 :: Batch 0/1
Batch Loss = 0.000730948414185009
714, epoch_train_loss=0.000730948414185009
Epoch 715
Epoch 715 :: Batch 0/1
Batch Loss = 0.0007309484141850071
715, epoch_train_loss=0.0007309484141850071
Epoch 716
Epoch 716 :: Batch 0/1
Batch Loss = 0.000730948414185005
716, epoch_train_loss=0.000730948414185005
Epoch 717
Epoch 717 :: Batch 0/1
Batch Loss = 0.0007309484141850031
717, epoch_train_loss=0.0007309484141850031
Epoch 718
Epoch 718 :: Batch 0/1
Batch Loss = 0.000730948414185001
718, epoch_train_loss=0.000730948414185001
Epoch 719
Epoch 719 :: Batch 0/1
Batch Loss = 0.0007309484141849989
719, epoch_train_loss=0.0007309484141849989
Epoch 720
Epoch 720 :: Batch 0/1
Batch Loss = 0.0007309484141849969
720, epoch_train_loss=0.0007309484141849969
Epoch 721
Epoch 721 :: Batch 0/1
Batch Loss = 0.0007309484141849948
721, epoch_train_loss=0.0007309484141849948
Epoch 722
Epoch 722 :: Batch 0/1
Batch Loss = 0.000730948414184993
722, epoch_train_loss=0.000730948414184993
Epoch 723
Epoch 723 :: Batch 0/1
Batch Loss = 0.0007309484141849909
723, epoch_train_loss=0.0007309484141849909
Epoch 724
Epoch 724 :: Batch 0/1
Batch Loss = 0.0007309484141849889
724, epoch_train_loss=0.0007309484141849889
Epoch 725
Epoch 725 :: Batch 0/1
Batch Loss = 0.0007309484141849868
725, epoch_train_loss=0.0007309484141849868
Epoch 726
Epoch 726 :: Batch 0/1
Batch Loss = 0.0007309484141849848
726, epoch_train_loss=0.0007309484141849848
Epoch 727
Epoch 727 :: Batch 0/1
Batch Loss = 0.0007309484141849828
727, epoch_train_loss=0.0007309484141849828
Epoch 728
Epoch 728 :: Batch 0/1
Batch Loss = 0.0007309484141849807
728, epoch_train_loss=0.0007309484141849807
Epoch 729
Epoch 729 :: Batch 0/1
Batch Loss = 0.0007309484141849788
729, epoch_train_loss=0.0007309484141849788
Epoch 730
Epoch 730 :: Batch 0/1
Batch Loss = 0.0007309484141849766
730, epoch_train_loss=0.0007309484141849766
Epoch 731
Epoch 731 :: Batch 0/1
Batch Loss = 0.0007309484141849748
731, epoch_train_loss=0.0007309484141849748
Epoch 732
Epoch 732 :: Batch 0/1
Batch Loss = 0.0007309484141849727
732, epoch_train_loss=0.0007309484141849727
Epoch 733
Epoch 733 :: Batch 0/1
Batch Loss = 0.0007309484141849706
733, epoch_train_loss=0.0007309484141849706
Epoch 734
Epoch 734 :: Batch 0/1
Batch Loss = 0.0007309484141849686
734, epoch_train_loss=0.0007309484141849686
Epoch 735
Epoch 735 :: Batch 0/1
Batch Loss = 0.0007309484141849665
735, epoch_train_loss=0.0007309484141849665
Epoch 736
Epoch 736 :: Batch 0/1
Batch Loss = 0.0007309484141849645
736, epoch_train_loss=0.0007309484141849645
Epoch 737
Epoch 737 :: Batch 0/1
Batch Loss = 0.0007309484141849625
737, epoch_train_loss=0.0007309484141849625
Epoch 738
Epoch 738 :: Batch 0/1
Batch Loss = 0.0007309484141849603
738, epoch_train_loss=0.0007309484141849603
Epoch 739
Epoch 739 :: Batch 0/1
Batch Loss = 0.0007309484141849583
739, epoch_train_loss=0.0007309484141849583
Epoch 740
Epoch 740 :: Batch 0/1
Batch Loss = 0.0007309484141849562
740, epoch_train_loss=0.0007309484141849562
Epoch 741
Epoch 741 :: Batch 0/1
Batch Loss = 0.0007309484141849541
741, epoch_train_loss=0.0007309484141849541
Epoch 742
Epoch 742 :: Batch 0/1
Batch Loss = 0.0007309484141849522
742, epoch_train_loss=0.0007309484141849522
Epoch 743
Epoch 743 :: Batch 0/1
Batch Loss = 0.00073094841418495
743, epoch_train_loss=0.00073094841418495
Epoch 744
Epoch 744 :: Batch 0/1
Batch Loss = 0.000730948414184948
744, epoch_train_loss=0.000730948414184948
Epoch 745
Epoch 745 :: Batch 0/1
Batch Loss = 0.0007309484141849459
745, epoch_train_loss=0.0007309484141849459
Epoch 746
Epoch 746 :: Batch 0/1
Batch Loss = 0.0007309484141849438
746, epoch_train_loss=0.0007309484141849438
Epoch 747
Epoch 747 :: Batch 0/1
Batch Loss = 0.0007309484141849418
747, epoch_train_loss=0.0007309484141849418
Epoch 748
Epoch 748 :: Batch 0/1
Batch Loss = 0.0007309484141849397
748, epoch_train_loss=0.0007309484141849397
Epoch 749
Epoch 749 :: Batch 0/1
Batch Loss = 0.0007309484141849378
749, epoch_train_loss=0.0007309484141849378
Epoch 750
Epoch 750 :: Batch 0/1
Batch Loss = 0.0007309484141849356
750, epoch_train_loss=0.0007309484141849356
Epoch 751
Epoch 751 :: Batch 0/1
Batch Loss = 0.0007309484141849335
751, epoch_train_loss=0.0007309484141849335
Epoch 752
Epoch 752 :: Batch 0/1
Batch Loss = 0.0007309484141849316
752, epoch_train_loss=0.0007309484141849316
Epoch 753
Epoch 753 :: Batch 0/1
Batch Loss = 0.0007309484141849294
753, epoch_train_loss=0.0007309484141849294
Epoch 754
Epoch 754 :: Batch 0/1
Batch Loss = 0.0007309484141849273
754, epoch_train_loss=0.0007309484141849273
Epoch 755
Epoch 755 :: Batch 0/1
Batch Loss = 0.0007309484141849253
755, epoch_train_loss=0.0007309484141849253
Epoch 756
Epoch 756 :: Batch 0/1
Batch Loss = 0.0007309484141849232
756, epoch_train_loss=0.0007309484141849232
Epoch 757
Epoch 757 :: Batch 0/1
Batch Loss = 0.000730948414184921
757, epoch_train_loss=0.000730948414184921
Epoch 758
Epoch 758 :: Batch 0/1
Batch Loss = 0.0007309484141849189
758, epoch_train_loss=0.0007309484141849189
Epoch 759
Epoch 759 :: Batch 0/1
Batch Loss = 0.000730948414184917
759, epoch_train_loss=0.000730948414184917
Epoch 760
Epoch 760 :: Batch 0/1
Batch Loss = 0.0007309484141849148
760, epoch_train_loss=0.0007309484141849148
Epoch 761
Epoch 761 :: Batch 0/1
Batch Loss = 0.0007309484141849126
761, epoch_train_loss=0.0007309484141849126
Epoch 762
Epoch 762 :: Batch 0/1
Batch Loss = 0.0007309484141849106
762, epoch_train_loss=0.0007309484141849106
Epoch 763
Epoch 763 :: Batch 0/1
Batch Loss = 0.0007309484141849085
763, epoch_train_loss=0.0007309484141849085
Epoch 764
Epoch 764 :: Batch 0/1
Batch Loss = 0.0007309484141849064
764, epoch_train_loss=0.0007309484141849064
Epoch 765
Epoch 765 :: Batch 0/1
Batch Loss = 0.0007309484141849043
765, epoch_train_loss=0.0007309484141849043
Epoch 766
Epoch 766 :: Batch 0/1
Batch Loss = 0.0007309484141849023
766, epoch_train_loss=0.0007309484141849023
Epoch 767
Epoch 767 :: Batch 0/1
Batch Loss = 0.0007309484141849001
767, epoch_train_loss=0.0007309484141849001
Epoch 768
Epoch 768 :: Batch 0/1
Batch Loss = 0.000730948414184898
768, epoch_train_loss=0.000730948414184898
Epoch 769
Epoch 769 :: Batch 0/1
Batch Loss = 0.0007309484141848959
769, epoch_train_loss=0.0007309484141848959
Epoch 770
Epoch 770 :: Batch 0/1
Batch Loss = 0.0007309484141848939
770, epoch_train_loss=0.0007309484141848939
Epoch 771
Epoch 771 :: Batch 0/1
Batch Loss = 0.0007309484141848917
771, epoch_train_loss=0.0007309484141848917
Epoch 772
Epoch 772 :: Batch 0/1
Batch Loss = 0.0007309484141848895
772, epoch_train_loss=0.0007309484141848895
Epoch 773
Epoch 773 :: Batch 0/1
Batch Loss = 0.0007309484141848874
773, epoch_train_loss=0.0007309484141848874
Epoch 774
Epoch 774 :: Batch 0/1
Batch Loss = 0.0007309484141848853
774, epoch_train_loss=0.0007309484141848853
Epoch 775
Epoch 775 :: Batch 0/1
Batch Loss = 0.0007309484141848833
775, epoch_train_loss=0.0007309484141848833
Epoch 776
Epoch 776 :: Batch 0/1
Batch Loss = 0.0007309484141848812
776, epoch_train_loss=0.0007309484141848812
Epoch 777
Epoch 777 :: Batch 0/1
Batch Loss = 0.000730948414184879
777, epoch_train_loss=0.000730948414184879
Epoch 778
Epoch 778 :: Batch 0/1
Batch Loss = 0.0007309484141848769
778, epoch_train_loss=0.0007309484141848769
Epoch 779
Epoch 779 :: Batch 0/1
Batch Loss = 0.0007309484141848747
779, epoch_train_loss=0.0007309484141848747
Epoch 780
Epoch 780 :: Batch 0/1
Batch Loss = 0.0007309484141848725
780, epoch_train_loss=0.0007309484141848725
Epoch 781
Epoch 781 :: Batch 0/1
Batch Loss = 0.0007309484141848704
781, epoch_train_loss=0.0007309484141848704
Epoch 782
Epoch 782 :: Batch 0/1
Batch Loss = 0.0007309484141848684
782, epoch_train_loss=0.0007309484141848684
Epoch 783
Epoch 783 :: Batch 0/1
Batch Loss = 0.0007309484141848662
783, epoch_train_loss=0.0007309484141848662
Epoch 784
Epoch 784 :: Batch 0/1
Batch Loss = 0.0007309484141848641
784, epoch_train_loss=0.0007309484141848641
Epoch 785
Epoch 785 :: Batch 0/1
Batch Loss = 0.000730948414184862
785, epoch_train_loss=0.000730948414184862
Epoch 786
Epoch 786 :: Batch 0/1
Batch Loss = 0.0007309484141848598
786, epoch_train_loss=0.0007309484141848598
Epoch 787
Epoch 787 :: Batch 0/1
Batch Loss = 0.0007309484141848577
787, epoch_train_loss=0.0007309484141848577
Epoch 788
Epoch 788 :: Batch 0/1
Batch Loss = 0.0007309484141848555
788, epoch_train_loss=0.0007309484141848555
Epoch 789
Epoch 789 :: Batch 0/1
Batch Loss = 0.0007309484141848533
789, epoch_train_loss=0.0007309484141848533
Epoch 790
Epoch 790 :: Batch 0/1
Batch Loss = 0.0007309484141848512
790, epoch_train_loss=0.0007309484141848512
Epoch 791
Epoch 791 :: Batch 0/1
Batch Loss = 0.0007309484141848491
791, epoch_train_loss=0.0007309484141848491
Epoch 792
Epoch 792 :: Batch 0/1
Batch Loss = 0.0007309484141848469
792, epoch_train_loss=0.0007309484141848469
Epoch 793
Epoch 793 :: Batch 0/1
Batch Loss = 0.0007309484141848448
793, epoch_train_loss=0.0007309484141848448
Epoch 794
Epoch 794 :: Batch 0/1
Batch Loss = 0.0007309484141848426
794, epoch_train_loss=0.0007309484141848426
Epoch 795
Epoch 795 :: Batch 0/1
Batch Loss = 0.0007309484141848404
795, epoch_train_loss=0.0007309484141848404
Epoch 796
Epoch 796 :: Batch 0/1
Batch Loss = 0.0007309484141848383
796, epoch_train_loss=0.0007309484141848383
Epoch 797
Epoch 797 :: Batch 0/1
Batch Loss = 0.0007309484141848362
797, epoch_train_loss=0.0007309484141848362
Epoch 798
Epoch 798 :: Batch 0/1
Batch Loss = 0.0007309484141848339
798, epoch_train_loss=0.0007309484141848339
Epoch 799
Epoch 799 :: Batch 0/1
Batch Loss = 0.0007309484141848318
799, epoch_train_loss=0.0007309484141848318
Epoch 800
Epoch 800 :: Batch 0/1
Batch Loss = 0.0007309484141848296
800, epoch_train_loss=0.0007309484141848296
Epoch 801
Epoch 801 :: Batch 0/1
Batch Loss = 0.0007309484141848274
801, epoch_train_loss=0.0007309484141848274
Epoch 802
Epoch 802 :: Batch 0/1
Batch Loss = 0.0007309484141848254
802, epoch_train_loss=0.0007309484141848254
Epoch 803
Epoch 803 :: Batch 0/1
Batch Loss = 0.0007309484141848232
803, epoch_train_loss=0.0007309484141848232
Epoch 804
Epoch 804 :: Batch 0/1
Batch Loss = 0.000730948414184821
804, epoch_train_loss=0.000730948414184821
Epoch 805
Epoch 805 :: Batch 0/1
Batch Loss = 0.0007309484141848189
805, epoch_train_loss=0.0007309484141848189
Epoch 806
Epoch 806 :: Batch 0/1
Batch Loss = 0.0007309484141848166
806, epoch_train_loss=0.0007309484141848166
Epoch 807
Epoch 807 :: Batch 0/1
Batch Loss = 0.0007309484141848144
807, epoch_train_loss=0.0007309484141848144
Epoch 808
Epoch 808 :: Batch 0/1
Batch Loss = 0.0007309484141848122
808, epoch_train_loss=0.0007309484141848122
Epoch 809
Epoch 809 :: Batch 0/1
Batch Loss = 0.0007309484141848102
809, epoch_train_loss=0.0007309484141848102
Epoch 810
Epoch 810 :: Batch 0/1
Batch Loss = 0.0007309484141848078
810, epoch_train_loss=0.0007309484141848078
Epoch 811
Epoch 811 :: Batch 0/1
Batch Loss = 0.0007309484141848057
811, epoch_train_loss=0.0007309484141848057
Epoch 812
Epoch 812 :: Batch 0/1
Batch Loss = 0.0007309484141848035
812, epoch_train_loss=0.0007309484141848035
Epoch 813
Epoch 813 :: Batch 0/1
Batch Loss = 0.0007309484141848013
813, epoch_train_loss=0.0007309484141848013
Epoch 814
Epoch 814 :: Batch 0/1
Batch Loss = 0.0007309484141847991
814, epoch_train_loss=0.0007309484141847991
Epoch 815
Epoch 815 :: Batch 0/1
Batch Loss = 0.0007309484141847971
815, epoch_train_loss=0.0007309484141847971
Epoch 816
Epoch 816 :: Batch 0/1
Batch Loss = 0.0007309484141847948
816, epoch_train_loss=0.0007309484141847948
Epoch 817
Epoch 817 :: Batch 0/1
Batch Loss = 0.0007309484141847926
817, epoch_train_loss=0.0007309484141847926
Epoch 818
Epoch 818 :: Batch 0/1
Batch Loss = 0.0007309484141847905
818, epoch_train_loss=0.0007309484141847905
Epoch 819
Epoch 819 :: Batch 0/1
Batch Loss = 0.0007309484141847882
819, epoch_train_loss=0.0007309484141847882
Epoch 820
Epoch 820 :: Batch 0/1
Batch Loss = 0.000730948414184786
820, epoch_train_loss=0.000730948414184786
Epoch 821
Epoch 821 :: Batch 0/1
Batch Loss = 0.0007309484141847837
821, epoch_train_loss=0.0007309484141847837
Epoch 822
Epoch 822 :: Batch 0/1
Batch Loss = 0.0007309484141847816
822, epoch_train_loss=0.0007309484141847816
Epoch 823
Epoch 823 :: Batch 0/1
Batch Loss = 0.0007309484141847793
823, epoch_train_loss=0.0007309484141847793
Epoch 824
Epoch 824 :: Batch 0/1
Batch Loss = 0.0007309484141847771
824, epoch_train_loss=0.0007309484141847771
Epoch 825
Epoch 825 :: Batch 0/1
Batch Loss = 0.000730948414184775
825, epoch_train_loss=0.000730948414184775
Epoch 826
Epoch 826 :: Batch 0/1
Batch Loss = 0.0007309484141847727
826, epoch_train_loss=0.0007309484141847727
Epoch 827
Epoch 827 :: Batch 0/1
Batch Loss = 0.0007309484141847704
827, epoch_train_loss=0.0007309484141847704
Epoch 828
Epoch 828 :: Batch 0/1
Batch Loss = 0.0007309484141847682
828, epoch_train_loss=0.0007309484141847682
Epoch 829
Epoch 829 :: Batch 0/1
Batch Loss = 0.0007309484141847661
829, epoch_train_loss=0.0007309484141847661
Epoch 830
Epoch 830 :: Batch 0/1
Batch Loss = 0.0007309484141847638
830, epoch_train_loss=0.0007309484141847638
Epoch 831
Epoch 831 :: Batch 0/1
Batch Loss = 0.0007309484141847616
831, epoch_train_loss=0.0007309484141847616
Epoch 832
Epoch 832 :: Batch 0/1
Batch Loss = 0.0007309484141847593
832, epoch_train_loss=0.0007309484141847593
Epoch 833
Epoch 833 :: Batch 0/1
Batch Loss = 0.0007309484141847572
833, epoch_train_loss=0.0007309484141847572
Epoch 834
Epoch 834 :: Batch 0/1
Batch Loss = 0.0007309484141847549
834, epoch_train_loss=0.0007309484141847549
Epoch 835
Epoch 835 :: Batch 0/1
Batch Loss = 0.0007309484141847526
835, epoch_train_loss=0.0007309484141847526
Epoch 836
Epoch 836 :: Batch 0/1
Batch Loss = 0.0007309484141847504
836, epoch_train_loss=0.0007309484141847504
Epoch 837
Epoch 837 :: Batch 0/1
Batch Loss = 0.0007309484141847484
837, epoch_train_loss=0.0007309484141847484
Epoch 838
Epoch 838 :: Batch 0/1
Batch Loss = 0.0007309484141847459
838, epoch_train_loss=0.0007309484141847459
Epoch 839
Epoch 839 :: Batch 0/1
Batch Loss = 0.0007309484141847438
839, epoch_train_loss=0.0007309484141847438
Epoch 840
Epoch 840 :: Batch 0/1
Batch Loss = 0.0007309484141847414
840, epoch_train_loss=0.0007309484141847414
Epoch 841
Epoch 841 :: Batch 0/1
Batch Loss = 0.0007309484141847392
841, epoch_train_loss=0.0007309484141847392
Epoch 842
Epoch 842 :: Batch 0/1
Batch Loss = 0.0007309484141847369
842, epoch_train_loss=0.0007309484141847369
Epoch 843
Epoch 843 :: Batch 0/1
Batch Loss = 0.0007309484141847346
843, epoch_train_loss=0.0007309484141847346
Epoch 844
Epoch 844 :: Batch 0/1
Batch Loss = 0.0007309484141847326
844, epoch_train_loss=0.0007309484141847326
Epoch 845
Epoch 845 :: Batch 0/1
Batch Loss = 0.0007309484141847304
845, epoch_train_loss=0.0007309484141847304
Epoch 846
Epoch 846 :: Batch 0/1
Batch Loss = 0.000730948414184728
846, epoch_train_loss=0.000730948414184728
Epoch 847
Epoch 847 :: Batch 0/1
Batch Loss = 0.0007309484141847258
847, epoch_train_loss=0.0007309484141847258
Epoch 848
Epoch 848 :: Batch 0/1
Batch Loss = 0.0007309484141847237
848, epoch_train_loss=0.0007309484141847237
Epoch 849
Epoch 849 :: Batch 0/1
Batch Loss = 0.0007309484141847213
849, epoch_train_loss=0.0007309484141847213
Epoch 850
Epoch 850 :: Batch 0/1
Batch Loss = 0.0007309484141847191
850, epoch_train_loss=0.0007309484141847191
Epoch 851
Epoch 851 :: Batch 0/1
Batch Loss = 0.0007309484141847167
851, epoch_train_loss=0.0007309484141847167
Epoch 852
Epoch 852 :: Batch 0/1
Batch Loss = 0.0007309484141847144
852, epoch_train_loss=0.0007309484141847144
Epoch 853
Epoch 853 :: Batch 0/1
Batch Loss = 0.0007309484141847122
853, epoch_train_loss=0.0007309484141847122
Epoch 854
Epoch 854 :: Batch 0/1
Batch Loss = 0.00073094841418471
854, epoch_train_loss=0.00073094841418471
Epoch 855
Epoch 855 :: Batch 0/1
Batch Loss = 0.0007309484141847078
855, epoch_train_loss=0.0007309484141847078
Epoch 856
Epoch 856 :: Batch 0/1
Batch Loss = 0.0007309484141847054
856, epoch_train_loss=0.0007309484141847054
Epoch 857
Epoch 857 :: Batch 0/1
Batch Loss = 0.0007309484141847033
857, epoch_train_loss=0.0007309484141847033
Epoch 858
Epoch 858 :: Batch 0/1
Batch Loss = 0.0007309484141847009
858, epoch_train_loss=0.0007309484141847009
Epoch 859
Epoch 859 :: Batch 0/1
Batch Loss = 0.0007309484141846987
859, epoch_train_loss=0.0007309484141846987
Epoch 860
Epoch 860 :: Batch 0/1
Batch Loss = 0.0007309484141846963
860, epoch_train_loss=0.0007309484141846963
Epoch 861
Epoch 861 :: Batch 0/1
Batch Loss = 0.0007309484141846942
861, epoch_train_loss=0.0007309484141846942
Epoch 862
Epoch 862 :: Batch 0/1
Batch Loss = 0.0007309484141846917
862, epoch_train_loss=0.0007309484141846917
Epoch 863
Epoch 863 :: Batch 0/1
Batch Loss = 0.0007309484141846896
863, epoch_train_loss=0.0007309484141846896
Epoch 864
Epoch 864 :: Batch 0/1
Batch Loss = 0.0007309484141846871
864, epoch_train_loss=0.0007309484141846871
Epoch 865
Epoch 865 :: Batch 0/1
Batch Loss = 0.0007309484141846848
865, epoch_train_loss=0.0007309484141846848
Epoch 866
Epoch 866 :: Batch 0/1
Batch Loss = 0.0007309484141846826
866, epoch_train_loss=0.0007309484141846826
Epoch 867
Epoch 867 :: Batch 0/1
Batch Loss = 0.0007309484141846802
867, epoch_train_loss=0.0007309484141846802
Epoch 868
Epoch 868 :: Batch 0/1
Batch Loss = 0.000730948414184678
868, epoch_train_loss=0.000730948414184678
Epoch 869
Epoch 869 :: Batch 0/1
Batch Loss = 0.0007309484141846757
869, epoch_train_loss=0.0007309484141846757
Epoch 870
Epoch 870 :: Batch 0/1
Batch Loss = 0.0007309484141846734
870, epoch_train_loss=0.0007309484141846734
Epoch 871
Epoch 871 :: Batch 0/1
Batch Loss = 0.0007309484141846711
871, epoch_train_loss=0.0007309484141846711
Epoch 872
Epoch 872 :: Batch 0/1
Batch Loss = 0.0007309484141846688
872, epoch_train_loss=0.0007309484141846688
Epoch 873
Epoch 873 :: Batch 0/1
Batch Loss = 0.0007309484141846665
873, epoch_train_loss=0.0007309484141846665
Epoch 874
Epoch 874 :: Batch 0/1
Batch Loss = 0.0007309484141846642
874, epoch_train_loss=0.0007309484141846642
Epoch 875
Epoch 875 :: Batch 0/1
Batch Loss = 0.000730948414184662
875, epoch_train_loss=0.000730948414184662
Epoch 876
Epoch 876 :: Batch 0/1
Batch Loss = 0.0007309484141846596
876, epoch_train_loss=0.0007309484141846596
Epoch 877
Epoch 877 :: Batch 0/1
Batch Loss = 0.0007309484141846574
877, epoch_train_loss=0.0007309484141846574
Epoch 878
Epoch 878 :: Batch 0/1
Batch Loss = 0.000730948414184655
878, epoch_train_loss=0.000730948414184655
Epoch 879
Epoch 879 :: Batch 0/1
Batch Loss = 0.0007309484141846526
879, epoch_train_loss=0.0007309484141846526
Epoch 880
Epoch 880 :: Batch 0/1
Batch Loss = 0.0007309484141846504
880, epoch_train_loss=0.0007309484141846504
Epoch 881
Epoch 881 :: Batch 0/1
Batch Loss = 0.0007309484141846481
881, epoch_train_loss=0.0007309484141846481
Epoch 882
Epoch 882 :: Batch 0/1
Batch Loss = 0.0007309484141846457
882, epoch_train_loss=0.0007309484141846457
Epoch 883
Epoch 883 :: Batch 0/1
Batch Loss = 0.0007309484141846434
883, epoch_train_loss=0.0007309484141846434
Epoch 884
Epoch 884 :: Batch 0/1
Batch Loss = 0.0007309484141846412
884, epoch_train_loss=0.0007309484141846412
Epoch 885
Epoch 885 :: Batch 0/1
Batch Loss = 0.0007309484141846388
885, epoch_train_loss=0.0007309484141846388
Epoch 886
Epoch 886 :: Batch 0/1
Batch Loss = 0.0007309484141846365
886, epoch_train_loss=0.0007309484141846365
Epoch 887
Epoch 887 :: Batch 0/1
Batch Loss = 0.0007309484141846342
887, epoch_train_loss=0.0007309484141846342
Epoch 888
Epoch 888 :: Batch 0/1
Batch Loss = 0.0007309484141846317
888, epoch_train_loss=0.0007309484141846317
Epoch 889
Epoch 889 :: Batch 0/1
Batch Loss = 0.0007309484141846293
889, epoch_train_loss=0.0007309484141846293
Epoch 890
Epoch 890 :: Batch 0/1
Batch Loss = 0.0007309484141846271
890, epoch_train_loss=0.0007309484141846271
Epoch 891
Epoch 891 :: Batch 0/1
Batch Loss = 0.0007309484141846248
891, epoch_train_loss=0.0007309484141846248
Epoch 892
Epoch 892 :: Batch 0/1
Batch Loss = 0.0007309484141846224
892, epoch_train_loss=0.0007309484141846224
Epoch 893
Epoch 893 :: Batch 0/1
Batch Loss = 0.0007309484141846201
893, epoch_train_loss=0.0007309484141846201
Epoch 894
Epoch 894 :: Batch 0/1
Batch Loss = 0.0007309484141846177
894, epoch_train_loss=0.0007309484141846177
Epoch 895
Epoch 895 :: Batch 0/1
Batch Loss = 0.0007309484141846155
895, epoch_train_loss=0.0007309484141846155
Epoch 896
Epoch 896 :: Batch 0/1
Batch Loss = 0.0007309484141846131
896, epoch_train_loss=0.0007309484141846131
Epoch 897
Epoch 897 :: Batch 0/1
Batch Loss = 0.0007309484141846107
897, epoch_train_loss=0.0007309484141846107
Epoch 898
Epoch 898 :: Batch 0/1
Batch Loss = 0.0007309484141846084
898, epoch_train_loss=0.0007309484141846084
Epoch 899
Epoch 899 :: Batch 0/1
Batch Loss = 0.0007309484141846061
899, epoch_train_loss=0.0007309484141846061
Epoch 900
Epoch 900 :: Batch 0/1
Batch Loss = 0.0007309484141846038
900, epoch_train_loss=0.0007309484141846038
Epoch 901
Epoch 901 :: Batch 0/1
Batch Loss = 0.0007309484141846014
901, epoch_train_loss=0.0007309484141846014
Epoch 902
Epoch 902 :: Batch 0/1
Batch Loss = 0.0007309484141845989
902, epoch_train_loss=0.0007309484141845989
Epoch 903
Epoch 903 :: Batch 0/1
Batch Loss = 0.0007309484141845965
903, epoch_train_loss=0.0007309484141845965
Epoch 904
Epoch 904 :: Batch 0/1
Batch Loss = 0.0007309484141845942
904, epoch_train_loss=0.0007309484141845942
Epoch 905
Epoch 905 :: Batch 0/1
Batch Loss = 0.0007309484141845919
905, epoch_train_loss=0.0007309484141845919
Epoch 906
Epoch 906 :: Batch 0/1
Batch Loss = 0.0007309484141845895
906, epoch_train_loss=0.0007309484141845895
Epoch 907
Epoch 907 :: Batch 0/1
Batch Loss = 0.0007309484141845872
907, epoch_train_loss=0.0007309484141845872
Epoch 908
Epoch 908 :: Batch 0/1
Batch Loss = 0.0007309484141845848
908, epoch_train_loss=0.0007309484141845848
Epoch 909
Epoch 909 :: Batch 0/1
Batch Loss = 0.0007309484141845824
909, epoch_train_loss=0.0007309484141845824
Epoch 910
Epoch 910 :: Batch 0/1
Batch Loss = 0.00073094841418458
910, epoch_train_loss=0.00073094841418458
Epoch 911
Epoch 911 :: Batch 0/1
Batch Loss = 0.0007309484141845777
911, epoch_train_loss=0.0007309484141845777
Epoch 912
Epoch 912 :: Batch 0/1
Batch Loss = 0.0007309484141845753
912, epoch_train_loss=0.0007309484141845753
Epoch 913
Epoch 913 :: Batch 0/1
Batch Loss = 0.0007309484141845729
913, epoch_train_loss=0.0007309484141845729
Epoch 914
Epoch 914 :: Batch 0/1
Batch Loss = 0.0007309484141845705
914, epoch_train_loss=0.0007309484141845705
Epoch 915
Epoch 915 :: Batch 0/1
Batch Loss = 0.0007309484141845681
915, epoch_train_loss=0.0007309484141845681
Epoch 916
Epoch 916 :: Batch 0/1
Batch Loss = 0.0007309484141845658
916, epoch_train_loss=0.0007309484141845658
Epoch 917
Epoch 917 :: Batch 0/1
Batch Loss = 0.0007309484141845634
917, epoch_train_loss=0.0007309484141845634
Epoch 918
Epoch 918 :: Batch 0/1
Batch Loss = 0.000730948414184561
918, epoch_train_loss=0.000730948414184561
Epoch 919
Epoch 919 :: Batch 0/1
Batch Loss = 0.0007309484141845586
919, epoch_train_loss=0.0007309484141845586
Epoch 920
Epoch 920 :: Batch 0/1
Batch Loss = 0.0007309484141845563
920, epoch_train_loss=0.0007309484141845563
Epoch 921
Epoch 921 :: Batch 0/1
Batch Loss = 0.0007309484141845539
921, epoch_train_loss=0.0007309484141845539
Epoch 922
Epoch 922 :: Batch 0/1
Batch Loss = 0.0007309484141845514
922, epoch_train_loss=0.0007309484141845514
Epoch 923
Epoch 923 :: Batch 0/1
Batch Loss = 0.000730948414184549
923, epoch_train_loss=0.000730948414184549
Epoch 924
Epoch 924 :: Batch 0/1
Batch Loss = 0.0007309484141845466
924, epoch_train_loss=0.0007309484141845466
Epoch 925
Epoch 925 :: Batch 0/1
Batch Loss = 0.0007309484141845442
925, epoch_train_loss=0.0007309484141845442
Epoch 926
Epoch 926 :: Batch 0/1
Batch Loss = 0.0007309484141845418
926, epoch_train_loss=0.0007309484141845418
Epoch 927
Epoch 927 :: Batch 0/1
Batch Loss = 0.0007309484141845395
927, epoch_train_loss=0.0007309484141845395
Epoch 928
Epoch 928 :: Batch 0/1
Batch Loss = 0.0007309484141845371
928, epoch_train_loss=0.0007309484141845371
Epoch 929
Epoch 929 :: Batch 0/1
Batch Loss = 0.0007309484141845347
929, epoch_train_loss=0.0007309484141845347
Epoch 930
Epoch 930 :: Batch 0/1
Batch Loss = 0.0007309484141845323
930, epoch_train_loss=0.0007309484141845323
Epoch 931
Epoch 931 :: Batch 0/1
Batch Loss = 0.0007309484141845298
931, epoch_train_loss=0.0007309484141845298
Epoch 932
Epoch 932 :: Batch 0/1
Batch Loss = 0.0007309484141845273
932, epoch_train_loss=0.0007309484141845273
Epoch 933
Epoch 933 :: Batch 0/1
Batch Loss = 0.000730948414184525
933, epoch_train_loss=0.000730948414184525
Epoch 934
Epoch 934 :: Batch 0/1
Batch Loss = 0.0007309484141845225
934, epoch_train_loss=0.0007309484141845225
Epoch 935
Epoch 935 :: Batch 0/1
Batch Loss = 0.0007309484141845203
935, epoch_train_loss=0.0007309484141845203
Epoch 936
Epoch 936 :: Batch 0/1
Batch Loss = 0.0007309484141845178
936, epoch_train_loss=0.0007309484141845178
Epoch 937
Epoch 937 :: Batch 0/1
Batch Loss = 0.0007309484141845154
937, epoch_train_loss=0.0007309484141845154
Epoch 938
Epoch 938 :: Batch 0/1
Batch Loss = 0.0007309484141845128
938, epoch_train_loss=0.0007309484141845128
Epoch 939
Epoch 939 :: Batch 0/1
Batch Loss = 0.0007309484141845104
939, epoch_train_loss=0.0007309484141845104
Epoch 940
Epoch 940 :: Batch 0/1
Batch Loss = 0.000730948414184508
940, epoch_train_loss=0.000730948414184508
Epoch 941
Epoch 941 :: Batch 0/1
Batch Loss = 0.0007309484141845056
941, epoch_train_loss=0.0007309484141845056
Epoch 942
Epoch 942 :: Batch 0/1
Batch Loss = 0.0007309484141845032
942, epoch_train_loss=0.0007309484141845032
Epoch 943
Epoch 943 :: Batch 0/1
Batch Loss = 0.0007309484141845009
943, epoch_train_loss=0.0007309484141845009
Epoch 944
Epoch 944 :: Batch 0/1
Batch Loss = 0.0007309484141844981
944, epoch_train_loss=0.0007309484141844981
Epoch 945
Epoch 945 :: Batch 0/1
Batch Loss = 0.0007309484141844958
945, epoch_train_loss=0.0007309484141844958
Epoch 946
Epoch 946 :: Batch 0/1
Batch Loss = 0.0007309484141844934
946, epoch_train_loss=0.0007309484141844934
Epoch 947
Epoch 947 :: Batch 0/1
Batch Loss = 0.000730948414184491
947, epoch_train_loss=0.000730948414184491
Epoch 948
Epoch 948 :: Batch 0/1
Batch Loss = 0.0007309484141844886
948, epoch_train_loss=0.0007309484141844886
Epoch 949
Epoch 949 :: Batch 0/1
Batch Loss = 0.0007309484141844861
949, epoch_train_loss=0.0007309484141844861
Epoch 950
Epoch 950 :: Batch 0/1
Batch Loss = 0.0007309484141844835
950, epoch_train_loss=0.0007309484141844835
Epoch 951
Epoch 951 :: Batch 0/1
Batch Loss = 0.0007309484141844811
951, epoch_train_loss=0.0007309484141844811
Epoch 952
Epoch 952 :: Batch 0/1
Batch Loss = 0.0007309484141844787
952, epoch_train_loss=0.0007309484141844787
Epoch 953
Epoch 953 :: Batch 0/1
Batch Loss = 0.0007309484141844764
953, epoch_train_loss=0.0007309484141844764
Epoch 954
Epoch 954 :: Batch 0/1
Batch Loss = 0.0007309484141844738
954, epoch_train_loss=0.0007309484141844738
Epoch 955
Epoch 955 :: Batch 0/1
Batch Loss = 0.0007309484141844714
955, epoch_train_loss=0.0007309484141844714
Epoch 956
Epoch 956 :: Batch 0/1
Batch Loss = 0.000730948414184469
956, epoch_train_loss=0.000730948414184469
Epoch 957
Epoch 957 :: Batch 0/1
Batch Loss = 0.0007309484141844665
957, epoch_train_loss=0.0007309484141844665
Epoch 958
Epoch 958 :: Batch 0/1
Batch Loss = 0.0007309484141844639
958, epoch_train_loss=0.0007309484141844639
Epoch 959
Epoch 959 :: Batch 0/1
Batch Loss = 0.0007309484141844615
959, epoch_train_loss=0.0007309484141844615
Epoch 960
Epoch 960 :: Batch 0/1
Batch Loss = 0.0007309484141844591
960, epoch_train_loss=0.0007309484141844591
Epoch 961
Epoch 961 :: Batch 0/1
Batch Loss = 0.0007309484141844567
961, epoch_train_loss=0.0007309484141844567
Epoch 962
Epoch 962 :: Batch 0/1
Batch Loss = 0.0007309484141844542
962, epoch_train_loss=0.0007309484141844542
Epoch 963
Epoch 963 :: Batch 0/1
Batch Loss = 0.0007309484141844516
963, epoch_train_loss=0.0007309484141844516
Epoch 964
Epoch 964 :: Batch 0/1
Batch Loss = 0.0007309484141844491
964, epoch_train_loss=0.0007309484141844491
Epoch 965
Epoch 965 :: Batch 0/1
Batch Loss = 0.0007309484141844467
965, epoch_train_loss=0.0007309484141844467
Epoch 966
Epoch 966 :: Batch 0/1
Batch Loss = 0.0007309484141844443
966, epoch_train_loss=0.0007309484141844443
Epoch 967
Epoch 967 :: Batch 0/1
Batch Loss = 0.0007309484141844419
967, epoch_train_loss=0.0007309484141844419
Epoch 968
Epoch 968 :: Batch 0/1
Batch Loss = 0.0007309484141844393
968, epoch_train_loss=0.0007309484141844393
Epoch 969
Epoch 969 :: Batch 0/1
Batch Loss = 0.0007309484141844368
969, epoch_train_loss=0.0007309484141844368
Epoch 970
Epoch 970 :: Batch 0/1
Batch Loss = 0.0007309484141844344
970, epoch_train_loss=0.0007309484141844344
Epoch 971
Epoch 971 :: Batch 0/1
Batch Loss = 0.0007309484141844319
971, epoch_train_loss=0.0007309484141844319
Epoch 972
Epoch 972 :: Batch 0/1
Batch Loss = 0.0007309484141844294
972, epoch_train_loss=0.0007309484141844294
Epoch 973
Epoch 973 :: Batch 0/1
Batch Loss = 0.0007309484141844268
973, epoch_train_loss=0.0007309484141844268
Epoch 974
Epoch 974 :: Batch 0/1
Batch Loss = 0.0007309484141844243
974, epoch_train_loss=0.0007309484141844243
Epoch 975
Epoch 975 :: Batch 0/1
Batch Loss = 0.0007309484141844219
975, epoch_train_loss=0.0007309484141844219
Epoch 976
Epoch 976 :: Batch 0/1
Batch Loss = 0.0007309484141844195
976, epoch_train_loss=0.0007309484141844195
Epoch 977
Epoch 977 :: Batch 0/1
Batch Loss = 0.0007309484141844169
977, epoch_train_loss=0.0007309484141844169
Epoch 978
Epoch 978 :: Batch 0/1
Batch Loss = 0.0007309484141844142
978, epoch_train_loss=0.0007309484141844142
Epoch 979
Epoch 979 :: Batch 0/1
Batch Loss = 0.0007309484141844118
979, epoch_train_loss=0.0007309484141844118
Epoch 980
Epoch 980 :: Batch 0/1
Batch Loss = 0.0007309484141844095
980, epoch_train_loss=0.0007309484141844095
Epoch 981
Epoch 981 :: Batch 0/1
Batch Loss = 0.0007309484141844069
981, epoch_train_loss=0.0007309484141844069
Epoch 982
Epoch 982 :: Batch 0/1
Batch Loss = 0.0007309484141844043
982, epoch_train_loss=0.0007309484141844043
Epoch 983
Epoch 983 :: Batch 0/1
Batch Loss = 0.0007309484141844018
983, epoch_train_loss=0.0007309484141844018
Epoch 984
Epoch 984 :: Batch 0/1
Batch Loss = 0.0007309484141843993
984, epoch_train_loss=0.0007309484141843993
Epoch 985
Epoch 985 :: Batch 0/1
Batch Loss = 0.0007309484141843968
985, epoch_train_loss=0.0007309484141843968
Epoch 986
Epoch 986 :: Batch 0/1
Batch Loss = 0.0007309484141843944
986, epoch_train_loss=0.0007309484141843944
Epoch 987
Epoch 987 :: Batch 0/1
Batch Loss = 0.0007309484141843917
987, epoch_train_loss=0.0007309484141843917
Epoch 988
Epoch 988 :: Batch 0/1
Batch Loss = 0.0007309484141843893
988, epoch_train_loss=0.0007309484141843893
Epoch 989
Epoch 989 :: Batch 0/1
Batch Loss = 0.0007309484141843867
989, epoch_train_loss=0.0007309484141843867
Epoch 990
Epoch 990 :: Batch 0/1
Batch Loss = 0.0007309484141843843
990, epoch_train_loss=0.0007309484141843843
Epoch 991
Epoch 991 :: Batch 0/1
Batch Loss = 0.0007309484141843816
991, epoch_train_loss=0.0007309484141843816
Epoch 992
Epoch 992 :: Batch 0/1
Batch Loss = 0.0007309484141843792
992, epoch_train_loss=0.0007309484141843792
Epoch 993
Epoch 993 :: Batch 0/1
Batch Loss = 0.0007309484141843767
993, epoch_train_loss=0.0007309484141843767
Epoch 994
Epoch 994 :: Batch 0/1
Batch Loss = 0.0007309484141843742
994, epoch_train_loss=0.0007309484141843742
Epoch 995
Epoch 995 :: Batch 0/1
Batch Loss = 0.0007309484141843715
995, epoch_train_loss=0.0007309484141843715
Epoch 996
Epoch 996 :: Batch 0/1
Batch Loss = 0.0007309484141843689
996, epoch_train_loss=0.0007309484141843689
Epoch 997
Epoch 997 :: Batch 0/1
Batch Loss = 0.0007309484141843665
997, epoch_train_loss=0.0007309484141843665
Epoch 998
Epoch 998 :: Batch 0/1
Batch Loss = 0.0007309484141843639
998, epoch_train_loss=0.0007309484141843639
Epoch 999
Epoch 999 :: Batch 0/1
Batch Loss = 0.0007309484141843615
999, epoch_train_loss=0.0007309484141843615
Epoch 1000
Epoch 1000 :: Batch 0/1
Batch Loss = 0.0007309484141843588
1000, epoch_train_loss=0.0007309484141843588
Epoch 1001
Epoch 1001 :: Batch 0/1
Batch Loss = 0.0007309484141843563
1001, epoch_train_loss=0.0007309484141843563
Epoch 1002
Epoch 1002 :: Batch 0/1
Batch Loss = 0.0007309484141843538
1002, epoch_train_loss=0.0007309484141843538
Epoch 1003
Epoch 1003 :: Batch 0/1
Batch Loss = 0.0007309484141843511
1003, epoch_train_loss=0.0007309484141843511
Epoch 1004
Epoch 1004 :: Batch 0/1
Batch Loss = 0.0007309484141843486
1004, epoch_train_loss=0.0007309484141843486
Epoch 1005
Epoch 1005 :: Batch 0/1
Batch Loss = 0.0007309484141843461
1005, epoch_train_loss=0.0007309484141843461
Epoch 1006
Epoch 1006 :: Batch 0/1
Batch Loss = 0.0007309484141843437
1006, epoch_train_loss=0.0007309484141843437
Epoch 1007
Epoch 1007 :: Batch 0/1
Batch Loss = 0.0007309484141843412
1007, epoch_train_loss=0.0007309484141843412
Epoch 1008
Epoch 1008 :: Batch 0/1
Batch Loss = 0.0007309484141843384
1008, epoch_train_loss=0.0007309484141843384
Epoch 1009
Epoch 1009 :: Batch 0/1
Batch Loss = 0.000730948414184336
1009, epoch_train_loss=0.000730948414184336
Epoch 1010
Epoch 1010 :: Batch 0/1
Batch Loss = 0.0007309484141843335
1010, epoch_train_loss=0.0007309484141843335
Epoch 1011
Epoch 1011 :: Batch 0/1
Batch Loss = 0.0007309484141843307
1011, epoch_train_loss=0.0007309484141843307
Epoch 1012
Epoch 1012 :: Batch 0/1
Batch Loss = 0.0007309484141843281
1012, epoch_train_loss=0.0007309484141843281
Epoch 1013
Epoch 1013 :: Batch 0/1
Batch Loss = 0.0007309484141843258
1013, epoch_train_loss=0.0007309484141843258
Epoch 1014
Epoch 1014 :: Batch 0/1
Batch Loss = 0.0007309484141843232
1014, epoch_train_loss=0.0007309484141843232
Epoch 1015
Epoch 1015 :: Batch 0/1
Batch Loss = 0.0007309484141843204
1015, epoch_train_loss=0.0007309484141843204
Epoch 1016
Epoch 1016 :: Batch 0/1
Batch Loss = 0.000730948414184318
1016, epoch_train_loss=0.000730948414184318
Epoch 1017
Epoch 1017 :: Batch 0/1
Batch Loss = 0.0007309484141843155
1017, epoch_train_loss=0.0007309484141843155
Epoch 1018
Epoch 1018 :: Batch 0/1
Batch Loss = 0.0007309484141843129
1018, epoch_train_loss=0.0007309484141843129
Epoch 1019
Epoch 1019 :: Batch 0/1
Batch Loss = 0.0007309484141843101
1019, epoch_train_loss=0.0007309484141843101
Epoch 1020
Epoch 1020 :: Batch 0/1
Batch Loss = 0.0007309484141843077
1020, epoch_train_loss=0.0007309484141843077
Epoch 1021
Epoch 1021 :: Batch 0/1
Batch Loss = 0.0007309484141843052
1021, epoch_train_loss=0.0007309484141843052
Epoch 1022
Epoch 1022 :: Batch 0/1
Batch Loss = 0.0007309484141843025
1022, epoch_train_loss=0.0007309484141843025
Epoch 1023
Epoch 1023 :: Batch 0/1
Batch Loss = 0.0007309484141842998
1023, epoch_train_loss=0.0007309484141842998
Epoch 1024
Epoch 1024 :: Batch 0/1
Batch Loss = 0.0007309484141842974
1024, epoch_train_loss=0.0007309484141842974
Epoch 1025
Epoch 1025 :: Batch 0/1
Batch Loss = 0.0007309484141842948
1025, epoch_train_loss=0.0007309484141842948
Epoch 1026
Epoch 1026 :: Batch 0/1
Batch Loss = 0.0007309484141842922
1026, epoch_train_loss=0.0007309484141842922
Epoch 1027
Epoch 1027 :: Batch 0/1
Batch Loss = 0.0007309484141842895
1027, epoch_train_loss=0.0007309484141842895
Epoch 1028
Epoch 1028 :: Batch 0/1
Batch Loss = 0.000730948414184287
1028, epoch_train_loss=0.000730948414184287
Epoch 1029
Epoch 1029 :: Batch 0/1
Batch Loss = 0.0007309484141842842
1029, epoch_train_loss=0.0007309484141842842
Epoch 1030
Epoch 1030 :: Batch 0/1
Batch Loss = 0.0007309484141842819
1030, epoch_train_loss=0.0007309484141842819
Epoch 1031
Epoch 1031 :: Batch 0/1
Batch Loss = 0.0007309484141842792
1031, epoch_train_loss=0.0007309484141842792
Epoch 1032
Epoch 1032 :: Batch 0/1
Batch Loss = 0.0007309484141842766
1032, epoch_train_loss=0.0007309484141842766
Epoch 1033
Epoch 1033 :: Batch 0/1
Batch Loss = 0.0007309484141842739
1033, epoch_train_loss=0.0007309484141842739
Epoch 1034
Epoch 1034 :: Batch 0/1
Batch Loss = 0.0007309484141842713
1034, epoch_train_loss=0.0007309484141842713
Epoch 1035
Epoch 1035 :: Batch 0/1
Batch Loss = 0.0007309484141842687
1035, epoch_train_loss=0.0007309484141842687
Epoch 1036
Epoch 1036 :: Batch 0/1
Batch Loss = 0.0007309484141842661
1036, epoch_train_loss=0.0007309484141842661
Epoch 1037
Epoch 1037 :: Batch 0/1
Batch Loss = 0.0007309484141842635
1037, epoch_train_loss=0.0007309484141842635
Epoch 1038
Epoch 1038 :: Batch 0/1
Batch Loss = 0.0007309484141842609
1038, epoch_train_loss=0.0007309484141842609
Epoch 1039
Epoch 1039 :: Batch 0/1
Batch Loss = 0.0007309484141842582
1039, epoch_train_loss=0.0007309484141842582
Epoch 1040
Epoch 1040 :: Batch 0/1
Batch Loss = 0.0007309484141842557
1040, epoch_train_loss=0.0007309484141842557
Epoch 1041
Epoch 1041 :: Batch 0/1
Batch Loss = 0.000730948414184253
1041, epoch_train_loss=0.000730948414184253
Epoch 1042
Epoch 1042 :: Batch 0/1
Batch Loss = 0.0007309484141842504
1042, epoch_train_loss=0.0007309484141842504
Epoch 1043
Epoch 1043 :: Batch 0/1
Batch Loss = 0.0007309484141842478
1043, epoch_train_loss=0.0007309484141842478
Epoch 1044
Epoch 1044 :: Batch 0/1
Batch Loss = 0.0007309484141842452
1044, epoch_train_loss=0.0007309484141842452
Epoch 1045
Epoch 1045 :: Batch 0/1
Batch Loss = 0.0007309484141842425
1045, epoch_train_loss=0.0007309484141842425
Epoch 1046
Epoch 1046 :: Batch 0/1
Batch Loss = 0.0007309484141842399
1046, epoch_train_loss=0.0007309484141842399
Epoch 1047
Epoch 1047 :: Batch 0/1
Batch Loss = 0.0007309484141842373
1047, epoch_train_loss=0.0007309484141842373
Epoch 1048
Epoch 1048 :: Batch 0/1
Batch Loss = 0.0007309484141842346
1048, epoch_train_loss=0.0007309484141842346
Epoch 1049
Epoch 1049 :: Batch 0/1
Batch Loss = 0.000730948414184232
1049, epoch_train_loss=0.000730948414184232
Epoch 1050
Epoch 1050 :: Batch 0/1
Batch Loss = 0.0007309484141842294
1050, epoch_train_loss=0.0007309484141842294
Epoch 1051
Epoch 1051 :: Batch 0/1
Batch Loss = 0.0007309484141842267
1051, epoch_train_loss=0.0007309484141842267
Epoch 1052
Epoch 1052 :: Batch 0/1
Batch Loss = 0.0007309484141842241
1052, epoch_train_loss=0.0007309484141842241
Epoch 1053
Epoch 1053 :: Batch 0/1
Batch Loss = 0.0007309484141842216
1053, epoch_train_loss=0.0007309484141842216
Epoch 1054
Epoch 1054 :: Batch 0/1
Batch Loss = 0.0007309484141842188
1054, epoch_train_loss=0.0007309484141842188
Epoch 1055
Epoch 1055 :: Batch 0/1
Batch Loss = 0.0007309484141842161
1055, epoch_train_loss=0.0007309484141842161
Epoch 1056
Epoch 1056 :: Batch 0/1
Batch Loss = 0.0007309484141842135
1056, epoch_train_loss=0.0007309484141842135
Epoch 1057
Epoch 1057 :: Batch 0/1
Batch Loss = 0.0007309484141842108
1057, epoch_train_loss=0.0007309484141842108
Epoch 1058
Epoch 1058 :: Batch 0/1
Batch Loss = 0.0007309484141842082
1058, epoch_train_loss=0.0007309484141842082
Epoch 1059
Epoch 1059 :: Batch 0/1
Batch Loss = 0.0007309484141842056
1059, epoch_train_loss=0.0007309484141842056
Epoch 1060
Epoch 1060 :: Batch 0/1
Batch Loss = 0.0007309484141842029
1060, epoch_train_loss=0.0007309484141842029
Epoch 1061
Epoch 1061 :: Batch 0/1
Batch Loss = 0.0007309484141842003
1061, epoch_train_loss=0.0007309484141842003
Epoch 1062
Epoch 1062 :: Batch 0/1
Batch Loss = 0.0007309484141841977
1062, epoch_train_loss=0.0007309484141841977
Epoch 1063
Epoch 1063 :: Batch 0/1
Batch Loss = 0.0007309484141841949
1063, epoch_train_loss=0.0007309484141841949
Epoch 1064
Epoch 1064 :: Batch 0/1
Batch Loss = 0.0007309484141841922
1064, epoch_train_loss=0.0007309484141841922
Epoch 1065
Epoch 1065 :: Batch 0/1
Batch Loss = 0.0007309484141841897
1065, epoch_train_loss=0.0007309484141841897
Epoch 1066
Epoch 1066 :: Batch 0/1
Batch Loss = 0.0007309484141841869
1066, epoch_train_loss=0.0007309484141841869
Epoch 1067
Epoch 1067 :: Batch 0/1
Batch Loss = 0.0007309484141841843
1067, epoch_train_loss=0.0007309484141841843
Epoch 1068
Epoch 1068 :: Batch 0/1
Batch Loss = 0.0007309484141841817
1068, epoch_train_loss=0.0007309484141841817
Epoch 1069
Epoch 1069 :: Batch 0/1
Batch Loss = 0.000730948414184179
1069, epoch_train_loss=0.000730948414184179
Epoch 1070
Epoch 1070 :: Batch 0/1
Batch Loss = 0.0007309484141841763
1070, epoch_train_loss=0.0007309484141841763
Epoch 1071
Epoch 1071 :: Batch 0/1
Batch Loss = 0.0007309484141841735
1071, epoch_train_loss=0.0007309484141841735
Epoch 1072
Epoch 1072 :: Batch 0/1
Batch Loss = 0.0007309484141841708
1072, epoch_train_loss=0.0007309484141841708
Epoch 1073
Epoch 1073 :: Batch 0/1
Batch Loss = 0.0007309484141841682
1073, epoch_train_loss=0.0007309484141841682
Epoch 1074
Epoch 1074 :: Batch 0/1
Batch Loss = 0.0007309484141841655
1074, epoch_train_loss=0.0007309484141841655
Epoch 1075
Epoch 1075 :: Batch 0/1
Batch Loss = 0.0007309484141841628
1075, epoch_train_loss=0.0007309484141841628
Epoch 1076
Epoch 1076 :: Batch 0/1
Batch Loss = 0.0007309484141841603
1076, epoch_train_loss=0.0007309484141841603
Epoch 1077
Epoch 1077 :: Batch 0/1
Batch Loss = 0.0007309484141841574
1077, epoch_train_loss=0.0007309484141841574
Epoch 1078
Epoch 1078 :: Batch 0/1
Batch Loss = 0.0007309484141841548
1078, epoch_train_loss=0.0007309484141841548
Epoch 1079
Epoch 1079 :: Batch 0/1
Batch Loss = 0.0007309484141841521
1079, epoch_train_loss=0.0007309484141841521
Epoch 1080
Epoch 1080 :: Batch 0/1
Batch Loss = 0.0007309484141841495
1080, epoch_train_loss=0.0007309484141841495
Epoch 1081
Epoch 1081 :: Batch 0/1
Batch Loss = 0.0007309484141841467
1081, epoch_train_loss=0.0007309484141841467
Epoch 1082
Epoch 1082 :: Batch 0/1
Batch Loss = 0.0007309484141841439
1082, epoch_train_loss=0.0007309484141841439
Epoch 1083
Epoch 1083 :: Batch 0/1
Batch Loss = 0.0007309484141841413
1083, epoch_train_loss=0.0007309484141841413
Epoch 1084
Epoch 1084 :: Batch 0/1
Batch Loss = 0.0007309484141841385
1084, epoch_train_loss=0.0007309484141841385
Epoch 1085
Epoch 1085 :: Batch 0/1
Batch Loss = 0.0007309484141841359
1085, epoch_train_loss=0.0007309484141841359
Epoch 1086
Epoch 1086 :: Batch 0/1
Batch Loss = 0.0007309484141841332
1086, epoch_train_loss=0.0007309484141841332
Epoch 1087
Epoch 1087 :: Batch 0/1
Batch Loss = 0.0007309484141841304
1087, epoch_train_loss=0.0007309484141841304
Epoch 1088
Epoch 1088 :: Batch 0/1
Batch Loss = 0.0007309484141841279
1088, epoch_train_loss=0.0007309484141841279
Epoch 1089
Epoch 1089 :: Batch 0/1
Batch Loss = 0.0007309484141841251
1089, epoch_train_loss=0.0007309484141841251
Epoch 1090
Epoch 1090 :: Batch 0/1
Batch Loss = 0.0007309484141841224
1090, epoch_train_loss=0.0007309484141841224
Epoch 1091
Epoch 1091 :: Batch 0/1
Batch Loss = 0.0007309484141841195
1091, epoch_train_loss=0.0007309484141841195
Epoch 1092
Epoch 1092 :: Batch 0/1
Batch Loss = 0.0007309484141841168
1092, epoch_train_loss=0.0007309484141841168
Epoch 1093
Epoch 1093 :: Batch 0/1
Batch Loss = 0.0007309484141841142
1093, epoch_train_loss=0.0007309484141841142
Epoch 1094
Epoch 1094 :: Batch 0/1
Batch Loss = 0.0007309484141841115
1094, epoch_train_loss=0.0007309484141841115
Epoch 1095
Epoch 1095 :: Batch 0/1
Batch Loss = 0.0007309484141841087
1095, epoch_train_loss=0.0007309484141841087
Epoch 1096
Epoch 1096 :: Batch 0/1
Batch Loss = 0.0007309484141841061
1096, epoch_train_loss=0.0007309484141841061
Epoch 1097
Epoch 1097 :: Batch 0/1
Batch Loss = 0.0007309484141841033
1097, epoch_train_loss=0.0007309484141841033
Epoch 1098
Epoch 1098 :: Batch 0/1
Batch Loss = 0.0007309484141841006
1098, epoch_train_loss=0.0007309484141841006
Epoch 1099
Epoch 1099 :: Batch 0/1
Batch Loss = 0.0007309484141840978
1099, epoch_train_loss=0.0007309484141840978
Epoch 1100
Epoch 1100 :: Batch 0/1
Batch Loss = 0.000730948414184095
1100, epoch_train_loss=0.000730948414184095
Epoch 1101
Epoch 1101 :: Batch 0/1
Batch Loss = 0.0007309484141840924
1101, epoch_train_loss=0.0007309484141840924
Epoch 1102
Epoch 1102 :: Batch 0/1
Batch Loss = 0.0007309484141840896
1102, epoch_train_loss=0.0007309484141840896
Epoch 1103
Epoch 1103 :: Batch 0/1
Batch Loss = 0.0007309484141840869
1103, epoch_train_loss=0.0007309484141840869
Epoch 1104
Epoch 1104 :: Batch 0/1
Batch Loss = 0.0007309484141840841
1104, epoch_train_loss=0.0007309484141840841
Epoch 1105
Epoch 1105 :: Batch 0/1
Batch Loss = 0.0007309484141840814
1105, epoch_train_loss=0.0007309484141840814
Epoch 1106
Epoch 1106 :: Batch 0/1
Batch Loss = 0.0007309484141840788
1106, epoch_train_loss=0.0007309484141840788
Epoch 1107
Epoch 1107 :: Batch 0/1
Batch Loss = 0.000730948414184076
1107, epoch_train_loss=0.000730948414184076
Epoch 1108
Epoch 1108 :: Batch 0/1
Batch Loss = 0.0007309484141840733
1108, epoch_train_loss=0.0007309484141840733
Epoch 1109
Epoch 1109 :: Batch 0/1
Batch Loss = 0.0007309484141840703
1109, epoch_train_loss=0.0007309484141840703
Epoch 1110
Epoch 1110 :: Batch 0/1
Batch Loss = 0.0007309484141840677
1110, epoch_train_loss=0.0007309484141840677
Epoch 1111
Epoch 1111 :: Batch 0/1
Batch Loss = 0.0007309484141840648
1111, epoch_train_loss=0.0007309484141840648
Epoch 1112
Epoch 1112 :: Batch 0/1
Batch Loss = 0.0007309484141840622
1112, epoch_train_loss=0.0007309484141840622
Epoch 1113
Epoch 1113 :: Batch 0/1
Batch Loss = 0.0007309484141840593
1113, epoch_train_loss=0.0007309484141840593
Epoch 1114
Epoch 1114 :: Batch 0/1
Batch Loss = 0.0007309484141840567
1114, epoch_train_loss=0.0007309484141840567
Epoch 1115
Epoch 1115 :: Batch 0/1
Batch Loss = 0.0007309484141840538
1115, epoch_train_loss=0.0007309484141840538
Epoch 1116
Epoch 1116 :: Batch 0/1
Batch Loss = 0.0007309484141840511
1116, epoch_train_loss=0.0007309484141840511
Epoch 1117
Epoch 1117 :: Batch 0/1
Batch Loss = 0.0007309484141840484
1117, epoch_train_loss=0.0007309484141840484
Epoch 1118
Epoch 1118 :: Batch 0/1
Batch Loss = 0.0007309484141840456
1118, epoch_train_loss=0.0007309484141840456
Epoch 1119
Epoch 1119 :: Batch 0/1
Batch Loss = 0.0007309484141840428
1119, epoch_train_loss=0.0007309484141840428
Epoch 1120
Epoch 1120 :: Batch 0/1
Batch Loss = 0.00073094841418404
1120, epoch_train_loss=0.00073094841418404
Epoch 1121
Epoch 1121 :: Batch 0/1
Batch Loss = 0.0007309484141840373
1121, epoch_train_loss=0.0007309484141840373
Epoch 1122
Epoch 1122 :: Batch 0/1
Batch Loss = 0.0007309484141840344
1122, epoch_train_loss=0.0007309484141840344
Epoch 1123
Epoch 1123 :: Batch 0/1
Batch Loss = 0.0007309484141840317
1123, epoch_train_loss=0.0007309484141840317
Epoch 1124
Epoch 1124 :: Batch 0/1
Batch Loss = 0.0007309484141840289
1124, epoch_train_loss=0.0007309484141840289
Epoch 1125
Epoch 1125 :: Batch 0/1
Batch Loss = 0.0007309484141840261
1125, epoch_train_loss=0.0007309484141840261
Epoch 1126
Epoch 1126 :: Batch 0/1
Batch Loss = 0.0007309484141840234
1126, epoch_train_loss=0.0007309484141840234
Epoch 1127
Epoch 1127 :: Batch 0/1
Batch Loss = 0.0007309484141840206
1127, epoch_train_loss=0.0007309484141840206
Epoch 1128
Epoch 1128 :: Batch 0/1
Batch Loss = 0.0007309484141840178
1128, epoch_train_loss=0.0007309484141840178
Epoch 1129
Epoch 1129 :: Batch 0/1
Batch Loss = 0.0007309484141840149
1129, epoch_train_loss=0.0007309484141840149
Epoch 1130
Epoch 1130 :: Batch 0/1
Batch Loss = 0.0007309484141840122
1130, epoch_train_loss=0.0007309484141840122
Epoch 1131
Epoch 1131 :: Batch 0/1
Batch Loss = 0.0007309484141840094
1131, epoch_train_loss=0.0007309484141840094
Epoch 1132
Epoch 1132 :: Batch 0/1
Batch Loss = 0.0007309484141840067
1132, epoch_train_loss=0.0007309484141840067
Epoch 1133
Epoch 1133 :: Batch 0/1
Batch Loss = 0.0007309484141840038
1133, epoch_train_loss=0.0007309484141840038
Epoch 1134
Epoch 1134 :: Batch 0/1
Batch Loss = 0.000730948414184001
1134, epoch_train_loss=0.000730948414184001
Epoch 1135
Epoch 1135 :: Batch 0/1
Batch Loss = 0.0007309484141839982
1135, epoch_train_loss=0.0007309484141839982
Epoch 1136
Epoch 1136 :: Batch 0/1
Batch Loss = 0.0007309484141839955
1136, epoch_train_loss=0.0007309484141839955
Epoch 1137
Epoch 1137 :: Batch 0/1
Batch Loss = 0.0007309484141839926
1137, epoch_train_loss=0.0007309484141839926
Epoch 1138
Epoch 1138 :: Batch 0/1
Batch Loss = 0.0007309484141839899
1138, epoch_train_loss=0.0007309484141839899
Epoch 1139
Epoch 1139 :: Batch 0/1
Batch Loss = 0.0007309484141839871
1139, epoch_train_loss=0.0007309484141839871
Epoch 1140
Epoch 1140 :: Batch 0/1
Batch Loss = 0.0007309484141839841
1140, epoch_train_loss=0.0007309484141839841
Epoch 1141
Epoch 1141 :: Batch 0/1
Batch Loss = 0.0007309484141839814
1141, epoch_train_loss=0.0007309484141839814
Epoch 1142
Epoch 1142 :: Batch 0/1
Batch Loss = 0.0007309484141839785
1142, epoch_train_loss=0.0007309484141839785
Epoch 1143
Epoch 1143 :: Batch 0/1
Batch Loss = 0.0007309484141839758
1143, epoch_train_loss=0.0007309484141839758
Epoch 1144
Epoch 1144 :: Batch 0/1
Batch Loss = 0.000730948414183973
1144, epoch_train_loss=0.000730948414183973
Epoch 1145
Epoch 1145 :: Batch 0/1
Batch Loss = 0.0007309484141839701
1145, epoch_train_loss=0.0007309484141839701
Epoch 1146
Epoch 1146 :: Batch 0/1
Batch Loss = 0.0007309484141839672
1146, epoch_train_loss=0.0007309484141839672
Epoch 1147
Epoch 1147 :: Batch 0/1
Batch Loss = 0.0007309484141839645
1147, epoch_train_loss=0.0007309484141839645
Epoch 1148
Epoch 1148 :: Batch 0/1
Batch Loss = 0.0007309484141839616
1148, epoch_train_loss=0.0007309484141839616
Epoch 1149
Epoch 1149 :: Batch 0/1
Batch Loss = 0.0007309484141839589
1149, epoch_train_loss=0.0007309484141839589
Epoch 1150
Epoch 1150 :: Batch 0/1
Batch Loss = 0.0007309484141839559
1150, epoch_train_loss=0.0007309484141839559
Epoch 1151
Epoch 1151 :: Batch 0/1
Batch Loss = 0.0007309484141839532
1151, epoch_train_loss=0.0007309484141839532
Epoch 1152
Epoch 1152 :: Batch 0/1
Batch Loss = 0.0007309484141839502
1152, epoch_train_loss=0.0007309484141839502
Epoch 1153
Epoch 1153 :: Batch 0/1
Batch Loss = 0.0007309484141839475
1153, epoch_train_loss=0.0007309484141839475
Epoch 1154
Epoch 1154 :: Batch 0/1
Batch Loss = 0.0007309484141839447
1154, epoch_train_loss=0.0007309484141839447
Epoch 1155
Epoch 1155 :: Batch 0/1
Batch Loss = 0.0007309484141839418
1155, epoch_train_loss=0.0007309484141839418
Epoch 1156
Epoch 1156 :: Batch 0/1
Batch Loss = 0.0007309484141839389
1156, epoch_train_loss=0.0007309484141839389
Epoch 1157
Epoch 1157 :: Batch 0/1
Batch Loss = 0.0007309484141839362
1157, epoch_train_loss=0.0007309484141839362
Epoch 1158
Epoch 1158 :: Batch 0/1
Batch Loss = 0.0007309484141839333
1158, epoch_train_loss=0.0007309484141839333
Epoch 1159
Epoch 1159 :: Batch 0/1
Batch Loss = 0.0007309484141839304
1159, epoch_train_loss=0.0007309484141839304
Epoch 1160
Epoch 1160 :: Batch 0/1
Batch Loss = 0.0007309484141839276
1160, epoch_train_loss=0.0007309484141839276
Epoch 1161
Epoch 1161 :: Batch 0/1
Batch Loss = 0.0007309484141839247
1161, epoch_train_loss=0.0007309484141839247
Epoch 1162
Epoch 1162 :: Batch 0/1
Batch Loss = 0.0007309484141839219
1162, epoch_train_loss=0.0007309484141839219
Epoch 1163
Epoch 1163 :: Batch 0/1
Batch Loss = 0.000730948414183919
1163, epoch_train_loss=0.000730948414183919
Epoch 1164
Epoch 1164 :: Batch 0/1
Batch Loss = 0.0007309484141839161
1164, epoch_train_loss=0.0007309484141839161
Epoch 1165
Epoch 1165 :: Batch 0/1
Batch Loss = 0.0007309484141839132
1165, epoch_train_loss=0.0007309484141839132
Epoch 1166
Epoch 1166 :: Batch 0/1
Batch Loss = 0.0007309484141839104
1166, epoch_train_loss=0.0007309484141839104
Epoch 1167
Epoch 1167 :: Batch 0/1
Batch Loss = 0.0007309484141839075
1167, epoch_train_loss=0.0007309484141839075
Epoch 1168
Epoch 1168 :: Batch 0/1
Batch Loss = 0.0007309484141839047
1168, epoch_train_loss=0.0007309484141839047
Epoch 1169
Epoch 1169 :: Batch 0/1
Batch Loss = 0.0007309484141839018
1169, epoch_train_loss=0.0007309484141839018
Epoch 1170
Epoch 1170 :: Batch 0/1
Batch Loss = 0.0007309484141838989
1170, epoch_train_loss=0.0007309484141838989
Epoch 1171
Epoch 1171 :: Batch 0/1
Batch Loss = 0.0007309484141838962
1171, epoch_train_loss=0.0007309484141838962
Epoch 1172
Epoch 1172 :: Batch 0/1
Batch Loss = 0.0007309484141838933
1172, epoch_train_loss=0.0007309484141838933
Epoch 1173
Epoch 1173 :: Batch 0/1
Batch Loss = 0.0007309484141838903
1173, epoch_train_loss=0.0007309484141838903
Epoch 1174
Epoch 1174 :: Batch 0/1
Batch Loss = 0.0007309484141838876
1174, epoch_train_loss=0.0007309484141838876
Epoch 1175
Epoch 1175 :: Batch 0/1
Batch Loss = 0.0007309484141838846
1175, epoch_train_loss=0.0007309484141838846
Epoch 1176
Epoch 1176 :: Batch 0/1
Batch Loss = 0.0007309484141838817
1176, epoch_train_loss=0.0007309484141838817
Epoch 1177
Epoch 1177 :: Batch 0/1
Batch Loss = 0.0007309484141838789
1177, epoch_train_loss=0.0007309484141838789
Epoch 1178
Epoch 1178 :: Batch 0/1
Batch Loss = 0.0007309484141838759
1178, epoch_train_loss=0.0007309484141838759
Epoch 1179
Epoch 1179 :: Batch 0/1
Batch Loss = 0.000730948414183873
1179, epoch_train_loss=0.000730948414183873
Epoch 1180
Epoch 1180 :: Batch 0/1
Batch Loss = 0.0007309484141838701
1180, epoch_train_loss=0.0007309484141838701
Epoch 1181
Epoch 1181 :: Batch 0/1
Batch Loss = 0.0007309484141838673
1181, epoch_train_loss=0.0007309484141838673
Epoch 1182
Epoch 1182 :: Batch 0/1
Batch Loss = 0.0007309484141838643
1182, epoch_train_loss=0.0007309484141838643
Epoch 1183
Epoch 1183 :: Batch 0/1
Batch Loss = 0.0007309484141838615
1183, epoch_train_loss=0.0007309484141838615
Epoch 1184
Epoch 1184 :: Batch 0/1
Batch Loss = 0.0007309484141838586
1184, epoch_train_loss=0.0007309484141838586
Epoch 1185
Epoch 1185 :: Batch 0/1
Batch Loss = 0.0007309484141838557
1185, epoch_train_loss=0.0007309484141838557
Epoch 1186
Epoch 1186 :: Batch 0/1
Batch Loss = 0.0007309484141838528
1186, epoch_train_loss=0.0007309484141838528
Epoch 1187
Epoch 1187 :: Batch 0/1
Batch Loss = 0.0007309484141838499
1187, epoch_train_loss=0.0007309484141838499
Epoch 1188
Epoch 1188 :: Batch 0/1
Batch Loss = 0.0007309484141838471
1188, epoch_train_loss=0.0007309484141838471
Epoch 1189
Epoch 1189 :: Batch 0/1
Batch Loss = 0.000730948414183844
1189, epoch_train_loss=0.000730948414183844
Epoch 1190
Epoch 1190 :: Batch 0/1
Batch Loss = 0.0007309484141838412
1190, epoch_train_loss=0.0007309484141838412
Epoch 1191
Epoch 1191 :: Batch 0/1
Batch Loss = 0.0007309484141838383
1191, epoch_train_loss=0.0007309484141838383
Epoch 1192
Epoch 1192 :: Batch 0/1
Batch Loss = 0.0007309484141838353
1192, epoch_train_loss=0.0007309484141838353
Epoch 1193
Epoch 1193 :: Batch 0/1
Batch Loss = 0.0007309484141838324
1193, epoch_train_loss=0.0007309484141838324
Epoch 1194
Epoch 1194 :: Batch 0/1
Batch Loss = 0.0007309484141838295
1194, epoch_train_loss=0.0007309484141838295
Epoch 1195
Epoch 1195 :: Batch 0/1
Batch Loss = 0.0007309484141838267
1195, epoch_train_loss=0.0007309484141838267
Epoch 1196
Epoch 1196 :: Batch 0/1
Batch Loss = 0.0007309484141838237
1196, epoch_train_loss=0.0007309484141838237
Epoch 1197
Epoch 1197 :: Batch 0/1
Batch Loss = 0.0007309484141838208
1197, epoch_train_loss=0.0007309484141838208
Epoch 1198
Epoch 1198 :: Batch 0/1
Batch Loss = 0.0007309484141838178
1198, epoch_train_loss=0.0007309484141838178
Epoch 1199
Epoch 1199 :: Batch 0/1
Batch Loss = 0.0007309484141838149
1199, epoch_train_loss=0.0007309484141838149
Epoch 1200
Epoch 1200 :: Batch 0/1
Batch Loss = 0.0007309484141838121
1200, epoch_train_loss=0.0007309484141838121
Epoch 1201
Epoch 1201 :: Batch 0/1
Batch Loss = 0.000730948414183809
1201, epoch_train_loss=0.000730948414183809
Epoch 1202
Epoch 1202 :: Batch 0/1
Batch Loss = 0.0007309484141838062
1202, epoch_train_loss=0.0007309484141838062
Epoch 1203
Epoch 1203 :: Batch 0/1
Batch Loss = 0.0007309484141838032
1203, epoch_train_loss=0.0007309484141838032
Epoch 1204
Epoch 1204 :: Batch 0/1
Batch Loss = 0.0007309484141838002
1204, epoch_train_loss=0.0007309484141838002
Epoch 1205
Epoch 1205 :: Batch 0/1
Batch Loss = 0.0007309484141837973
1205, epoch_train_loss=0.0007309484141837973
Epoch 1206
Epoch 1206 :: Batch 0/1
Batch Loss = 0.0007309484141837944
1206, epoch_train_loss=0.0007309484141837944
Epoch 1207
Epoch 1207 :: Batch 0/1
Batch Loss = 0.0007309484141837915
1207, epoch_train_loss=0.0007309484141837915
Epoch 1208
Epoch 1208 :: Batch 0/1
Batch Loss = 0.0007309484141837884
1208, epoch_train_loss=0.0007309484141837884
Epoch 1209
Epoch 1209 :: Batch 0/1
Batch Loss = 0.0007309484141837855
1209, epoch_train_loss=0.0007309484141837855
Epoch 1210
Epoch 1210 :: Batch 0/1
Batch Loss = 0.0007309484141837826
1210, epoch_train_loss=0.0007309484141837826
Epoch 1211
Epoch 1211 :: Batch 0/1
Batch Loss = 0.0007309484141837796
1211, epoch_train_loss=0.0007309484141837796
Epoch 1212
Epoch 1212 :: Batch 0/1
Batch Loss = 0.0007309484141837768
1212, epoch_train_loss=0.0007309484141837768
Epoch 1213
Epoch 1213 :: Batch 0/1
Batch Loss = 0.0007309484141837737
1213, epoch_train_loss=0.0007309484141837737
Epoch 1214
Epoch 1214 :: Batch 0/1
Batch Loss = 0.0007309484141837708
1214, epoch_train_loss=0.0007309484141837708
Epoch 1215
Epoch 1215 :: Batch 0/1
Batch Loss = 0.0007309484141837679
1215, epoch_train_loss=0.0007309484141837679
Epoch 1216
Epoch 1216 :: Batch 0/1
Batch Loss = 0.0007309484141837648
1216, epoch_train_loss=0.0007309484141837648
Epoch 1217
Epoch 1217 :: Batch 0/1
Batch Loss = 0.000730948414183762
1217, epoch_train_loss=0.000730948414183762
Epoch 1218
Epoch 1218 :: Batch 0/1
Batch Loss = 0.000730948414183759
1218, epoch_train_loss=0.000730948414183759
Epoch 1219
Epoch 1219 :: Batch 0/1
Batch Loss = 0.0007309484141837559
1219, epoch_train_loss=0.0007309484141837559
Epoch 1220
Epoch 1220 :: Batch 0/1
Batch Loss = 0.0007309484141837531
1220, epoch_train_loss=0.0007309484141837531
Epoch 1221
Epoch 1221 :: Batch 0/1
Batch Loss = 0.00073094841418375
1221, epoch_train_loss=0.00073094841418375
Epoch 1222
Epoch 1222 :: Batch 0/1
Batch Loss = 0.000730948414183747
1222, epoch_train_loss=0.000730948414183747
Epoch 1223
Epoch 1223 :: Batch 0/1
Batch Loss = 0.0007309484141837442
1223, epoch_train_loss=0.0007309484141837442
Epoch 1224
Epoch 1224 :: Batch 0/1
Batch Loss = 0.000730948414183741
1224, epoch_train_loss=0.000730948414183741
Epoch 1225
Epoch 1225 :: Batch 0/1
Batch Loss = 0.0007309484141837382
1225, epoch_train_loss=0.0007309484141837382
Epoch 1226
Epoch 1226 :: Batch 0/1
Batch Loss = 0.0007309484141837352
1226, epoch_train_loss=0.0007309484141837352
Epoch 1227
Epoch 1227 :: Batch 0/1
Batch Loss = 0.0007309484141837322
1227, epoch_train_loss=0.0007309484141837322
Epoch 1228
Epoch 1228 :: Batch 0/1
Batch Loss = 0.0007309484141837292
1228, epoch_train_loss=0.0007309484141837292
Epoch 1229
Epoch 1229 :: Batch 0/1
Batch Loss = 0.0007309484141837263
1229, epoch_train_loss=0.0007309484141837263
Epoch 1230
Epoch 1230 :: Batch 0/1
Batch Loss = 0.0007309484141837233
1230, epoch_train_loss=0.0007309484141837233
Epoch 1231
Epoch 1231 :: Batch 0/1
Batch Loss = 0.0007309484141837202
1231, epoch_train_loss=0.0007309484141837202
Epoch 1232
Epoch 1232 :: Batch 0/1
Batch Loss = 0.0007309484141837173
1232, epoch_train_loss=0.0007309484141837173
Epoch 1233
Epoch 1233 :: Batch 0/1
Batch Loss = 0.0007309484141837142
1233, epoch_train_loss=0.0007309484141837142
Epoch 1234
Epoch 1234 :: Batch 0/1
Batch Loss = 0.0007309484141837113
1234, epoch_train_loss=0.0007309484141837113
Epoch 1235
Epoch 1235 :: Batch 0/1
Batch Loss = 0.0007309484141837082
1235, epoch_train_loss=0.0007309484141837082
Epoch 1236
Epoch 1236 :: Batch 0/1
Batch Loss = 0.0007309484141837054
1236, epoch_train_loss=0.0007309484141837054
Epoch 1237
Epoch 1237 :: Batch 0/1
Batch Loss = 0.0007309484141837023
1237, epoch_train_loss=0.0007309484141837023
Epoch 1238
Epoch 1238 :: Batch 0/1
Batch Loss = 0.0007309484141836993
1238, epoch_train_loss=0.0007309484141836993
Epoch 1239
Epoch 1239 :: Batch 0/1
Batch Loss = 0.0007309484141836962
1239, epoch_train_loss=0.0007309484141836962
Epoch 1240
Epoch 1240 :: Batch 0/1
Batch Loss = 0.0007309484141836933
1240, epoch_train_loss=0.0007309484141836933
Epoch 1241
Epoch 1241 :: Batch 0/1
Batch Loss = 0.0007309484141836902
1241, epoch_train_loss=0.0007309484141836902
Epoch 1242
Epoch 1242 :: Batch 0/1
Batch Loss = 0.0007309484141836874
1242, epoch_train_loss=0.0007309484141836874
Epoch 1243
Epoch 1243 :: Batch 0/1
Batch Loss = 0.0007309484141836843
1243, epoch_train_loss=0.0007309484141836843
Epoch 1244
Epoch 1244 :: Batch 0/1
Batch Loss = 0.0007309484141836811
1244, epoch_train_loss=0.0007309484141836811
Epoch 1245
Epoch 1245 :: Batch 0/1
Batch Loss = 0.0007309484141836783
1245, epoch_train_loss=0.0007309484141836783
Epoch 1246
Epoch 1246 :: Batch 0/1
Batch Loss = 0.0007309484141836752
1246, epoch_train_loss=0.0007309484141836752
Epoch 1247
Epoch 1247 :: Batch 0/1
Batch Loss = 0.0007309484141836722
1247, epoch_train_loss=0.0007309484141836722
Epoch 1248
Epoch 1248 :: Batch 0/1
Batch Loss = 0.0007309484141836691
1248, epoch_train_loss=0.0007309484141836691
Epoch 1249
Epoch 1249 :: Batch 0/1
Batch Loss = 0.0007309484141836661
1249, epoch_train_loss=0.0007309484141836661
Epoch 1250
Epoch 1250 :: Batch 0/1
Batch Loss = 0.0007309484141836631
1250, epoch_train_loss=0.0007309484141836631
Epoch 1251
Epoch 1251 :: Batch 0/1
Batch Loss = 0.0007309484141836602
1251, epoch_train_loss=0.0007309484141836602
Epoch 1252
Epoch 1252 :: Batch 0/1
Batch Loss = 0.000730948414183657
1252, epoch_train_loss=0.000730948414183657
Epoch 1253
Epoch 1253 :: Batch 0/1
Batch Loss = 0.000730948414183654
1253, epoch_train_loss=0.000730948414183654
Epoch 1254
Epoch 1254 :: Batch 0/1
Batch Loss = 0.000730948414183651
1254, epoch_train_loss=0.000730948414183651
Epoch 1255
Epoch 1255 :: Batch 0/1
Batch Loss = 0.000730948414183648
1255, epoch_train_loss=0.000730948414183648
Epoch 1256
Epoch 1256 :: Batch 0/1
Batch Loss = 0.0007309484141836449
1256, epoch_train_loss=0.0007309484141836449
Epoch 1257
Epoch 1257 :: Batch 0/1
Batch Loss = 0.0007309484141836418
1257, epoch_train_loss=0.0007309484141836418
Epoch 1258
Epoch 1258 :: Batch 0/1
Batch Loss = 0.0007309484141836389
1258, epoch_train_loss=0.0007309484141836389
Epoch 1259
Epoch 1259 :: Batch 0/1
Batch Loss = 0.0007309484141836358
1259, epoch_train_loss=0.0007309484141836358
Epoch 1260
Epoch 1260 :: Batch 0/1
Batch Loss = 0.0007309484141836326
1260, epoch_train_loss=0.0007309484141836326
Epoch 1261
Epoch 1261 :: Batch 0/1
Batch Loss = 0.0007309484141836297
1261, epoch_train_loss=0.0007309484141836297
Epoch 1262
Epoch 1262 :: Batch 0/1
Batch Loss = 0.0007309484141836267
1262, epoch_train_loss=0.0007309484141836267
Epoch 1263
Epoch 1263 :: Batch 0/1
Batch Loss = 0.0007309484141836235
1263, epoch_train_loss=0.0007309484141836235
Epoch 1264
Epoch 1264 :: Batch 0/1
Batch Loss = 0.0007309484141836204
1264, epoch_train_loss=0.0007309484141836204
Epoch 1265
Epoch 1265 :: Batch 0/1
Batch Loss = 0.0007309484141836176
1265, epoch_train_loss=0.0007309484141836176
Epoch 1266
Epoch 1266 :: Batch 0/1
Batch Loss = 0.0007309484141836144
1266, epoch_train_loss=0.0007309484141836144
Epoch 1267
Epoch 1267 :: Batch 0/1
Batch Loss = 0.0007309484141836113
1267, epoch_train_loss=0.0007309484141836113
Epoch 1268
Epoch 1268 :: Batch 0/1
Batch Loss = 0.0007309484141836082
1268, epoch_train_loss=0.0007309484141836082
Epoch 1269
Epoch 1269 :: Batch 0/1
Batch Loss = 0.0007309484141836052
1269, epoch_train_loss=0.0007309484141836052
Epoch 1270
Epoch 1270 :: Batch 0/1
Batch Loss = 0.0007309484141836021
1270, epoch_train_loss=0.0007309484141836021
Epoch 1271
Epoch 1271 :: Batch 0/1
Batch Loss = 0.0007309484141835991
1271, epoch_train_loss=0.0007309484141835991
Epoch 1272
Epoch 1272 :: Batch 0/1
Batch Loss = 0.000730948414183596
1272, epoch_train_loss=0.000730948414183596
Epoch 1273
Epoch 1273 :: Batch 0/1
Batch Loss = 0.000730948414183593
1273, epoch_train_loss=0.000730948414183593
Epoch 1274
Epoch 1274 :: Batch 0/1
Batch Loss = 0.0007309484141835898
1274, epoch_train_loss=0.0007309484141835898
Epoch 1275
Epoch 1275 :: Batch 0/1
Batch Loss = 0.0007309484141835869
1275, epoch_train_loss=0.0007309484141835869
Epoch 1276
Epoch 1276 :: Batch 0/1
Batch Loss = 0.0007309484141835836
1276, epoch_train_loss=0.0007309484141835836
Epoch 1277
Epoch 1277 :: Batch 0/1
Batch Loss = 0.0007309484141835806
1277, epoch_train_loss=0.0007309484141835806
Epoch 1278
Epoch 1278 :: Batch 0/1
Batch Loss = 0.0007309484141835776
1278, epoch_train_loss=0.0007309484141835776
Epoch 1279
Epoch 1279 :: Batch 0/1
Batch Loss = 0.0007309484141835745
1279, epoch_train_loss=0.0007309484141835745
Epoch 1280
Epoch 1280 :: Batch 0/1
Batch Loss = 0.0007309484141835715
1280, epoch_train_loss=0.0007309484141835715
Epoch 1281
Epoch 1281 :: Batch 0/1
Batch Loss = 0.0007309484141835683
1281, epoch_train_loss=0.0007309484141835683
Epoch 1282
Epoch 1282 :: Batch 0/1
Batch Loss = 0.0007309484141835653
1282, epoch_train_loss=0.0007309484141835653
Epoch 1283
Epoch 1283 :: Batch 0/1
Batch Loss = 0.0007309484141835622
1283, epoch_train_loss=0.0007309484141835622
Epoch 1284
Epoch 1284 :: Batch 0/1
Batch Loss = 0.000730948414183559
1284, epoch_train_loss=0.000730948414183559
Epoch 1285
Epoch 1285 :: Batch 0/1
Batch Loss = 0.0007309484141835559
1285, epoch_train_loss=0.0007309484141835559
Epoch 1286
Epoch 1286 :: Batch 0/1
Batch Loss = 0.0007309484141835528
1286, epoch_train_loss=0.0007309484141835528
Epoch 1287
Epoch 1287 :: Batch 0/1
Batch Loss = 0.0007309484141835497
1287, epoch_train_loss=0.0007309484141835497
Epoch 1288
Epoch 1288 :: Batch 0/1
Batch Loss = 0.0007309484141835465
1288, epoch_train_loss=0.0007309484141835465
Epoch 1289
Epoch 1289 :: Batch 0/1
Batch Loss = 0.0007309484141835434
1289, epoch_train_loss=0.0007309484141835434
Epoch 1290
Epoch 1290 :: Batch 0/1
Batch Loss = 0.0007309484141835404
1290, epoch_train_loss=0.0007309484141835404
Epoch 1291
Epoch 1291 :: Batch 0/1
Batch Loss = 0.0007309484141835373
1291, epoch_train_loss=0.0007309484141835373
Epoch 1292
Epoch 1292 :: Batch 0/1
Batch Loss = 0.0007309484141835342
1292, epoch_train_loss=0.0007309484141835342
Epoch 1293
Epoch 1293 :: Batch 0/1
Batch Loss = 0.000730948414183531
1293, epoch_train_loss=0.000730948414183531
Epoch 1294
Epoch 1294 :: Batch 0/1
Batch Loss = 0.0007309484141835279
1294, epoch_train_loss=0.0007309484141835279
Epoch 1295
Epoch 1295 :: Batch 0/1
Batch Loss = 0.0007309484141835249
1295, epoch_train_loss=0.0007309484141835249
Epoch 1296
Epoch 1296 :: Batch 0/1
Batch Loss = 0.0007309484141835217
1296, epoch_train_loss=0.0007309484141835217
Epoch 1297
Epoch 1297 :: Batch 0/1
Batch Loss = 0.0007309484141835186
1297, epoch_train_loss=0.0007309484141835186
Epoch 1298
Epoch 1298 :: Batch 0/1
Batch Loss = 0.0007309484141835155
1298, epoch_train_loss=0.0007309484141835155
Epoch 1299
Epoch 1299 :: Batch 0/1
Batch Loss = 0.0007309484141835123
1299, epoch_train_loss=0.0007309484141835123
Epoch 1300
Epoch 1300 :: Batch 0/1
Batch Loss = 0.0007309484141835091
1300, epoch_train_loss=0.0007309484141835091
Epoch 1301
Epoch 1301 :: Batch 0/1
Batch Loss = 0.000730948414183506
1301, epoch_train_loss=0.000730948414183506
Epoch 1302
Epoch 1302 :: Batch 0/1
Batch Loss = 0.000730948414183503
1302, epoch_train_loss=0.000730948414183503
Epoch 1303
Epoch 1303 :: Batch 0/1
Batch Loss = 0.0007309484141834998
1303, epoch_train_loss=0.0007309484141834998
Epoch 1304
Epoch 1304 :: Batch 0/1
Batch Loss = 0.0007309484141834968
1304, epoch_train_loss=0.0007309484141834968
Epoch 1305
Epoch 1305 :: Batch 0/1
Batch Loss = 0.0007309484141834935
1305, epoch_train_loss=0.0007309484141834935
Epoch 1306
Epoch 1306 :: Batch 0/1
Batch Loss = 0.0007309484141834905
1306, epoch_train_loss=0.0007309484141834905
Epoch 1307
Epoch 1307 :: Batch 0/1
Batch Loss = 0.0007309484141834873
1307, epoch_train_loss=0.0007309484141834873
Epoch 1308
Epoch 1308 :: Batch 0/1
Batch Loss = 0.0007309484141834841
1308, epoch_train_loss=0.0007309484141834841
Epoch 1309
Epoch 1309 :: Batch 0/1
Batch Loss = 0.0007309484141834811
1309, epoch_train_loss=0.0007309484141834811
Epoch 1310
Epoch 1310 :: Batch 0/1
Batch Loss = 0.0007309484141834778
1310, epoch_train_loss=0.0007309484141834778
Epoch 1311
Epoch 1311 :: Batch 0/1
Batch Loss = 0.0007309484141834748
1311, epoch_train_loss=0.0007309484141834748
Epoch 1312
Epoch 1312 :: Batch 0/1
Batch Loss = 0.0007309484141834715
1312, epoch_train_loss=0.0007309484141834715
Epoch 1313
Epoch 1313 :: Batch 0/1
Batch Loss = 0.0007309484141834684
1313, epoch_train_loss=0.0007309484141834684
Epoch 1314
Epoch 1314 :: Batch 0/1
Batch Loss = 0.0007309484141834652
1314, epoch_train_loss=0.0007309484141834652
Epoch 1315
Epoch 1315 :: Batch 0/1
Batch Loss = 0.0007309484141834622
1315, epoch_train_loss=0.0007309484141834622
Epoch 1316
Epoch 1316 :: Batch 0/1
Batch Loss = 0.0007309484141834589
1316, epoch_train_loss=0.0007309484141834589
Epoch 1317
Epoch 1317 :: Batch 0/1
Batch Loss = 0.0007309484141834558
1317, epoch_train_loss=0.0007309484141834558
Epoch 1318
Epoch 1318 :: Batch 0/1
Batch Loss = 0.0007309484141834527
1318, epoch_train_loss=0.0007309484141834527
Epoch 1319
Epoch 1319 :: Batch 0/1
Batch Loss = 0.0007309484141834494
1319, epoch_train_loss=0.0007309484141834494
Epoch 1320
Epoch 1320 :: Batch 0/1
Batch Loss = 0.0007309484141834464
1320, epoch_train_loss=0.0007309484141834464
Epoch 1321
Epoch 1321 :: Batch 0/1
Batch Loss = 0.0007309484141834431
1321, epoch_train_loss=0.0007309484141834431
Epoch 1322
Epoch 1322 :: Batch 0/1
Batch Loss = 0.0007309484141834399
1322, epoch_train_loss=0.0007309484141834399
Epoch 1323
Epoch 1323 :: Batch 0/1
Batch Loss = 0.0007309484141834367
1323, epoch_train_loss=0.0007309484141834367
Epoch 1324
Epoch 1324 :: Batch 0/1
Batch Loss = 0.0007309484141834336
1324, epoch_train_loss=0.0007309484141834336
Epoch 1325
Epoch 1325 :: Batch 0/1
Batch Loss = 0.0007309484141834305
1325, epoch_train_loss=0.0007309484141834305
Epoch 1326
Epoch 1326 :: Batch 0/1
Batch Loss = 0.0007309484141834272
1326, epoch_train_loss=0.0007309484141834272
Epoch 1327
Epoch 1327 :: Batch 0/1
Batch Loss = 0.0007309484141834241
1327, epoch_train_loss=0.0007309484141834241
Epoch 1328
Epoch 1328 :: Batch 0/1
Batch Loss = 0.0007309484141834209
1328, epoch_train_loss=0.0007309484141834209
Epoch 1329
Epoch 1329 :: Batch 0/1
Batch Loss = 0.0007309484141834176
1329, epoch_train_loss=0.0007309484141834176
Epoch 1330
Epoch 1330 :: Batch 0/1
Batch Loss = 0.0007309484141834144
1330, epoch_train_loss=0.0007309484141834144
Epoch 1331
Epoch 1331 :: Batch 0/1
Batch Loss = 0.0007309484141834113
1331, epoch_train_loss=0.0007309484141834113
Epoch 1332
Epoch 1332 :: Batch 0/1
Batch Loss = 0.0007309484141834081
1332, epoch_train_loss=0.0007309484141834081
Epoch 1333
Epoch 1333 :: Batch 0/1
Batch Loss = 0.000730948414183405
1333, epoch_train_loss=0.000730948414183405
Epoch 1334
Epoch 1334 :: Batch 0/1
Batch Loss = 0.0007309484141834017
1334, epoch_train_loss=0.0007309484141834017
Epoch 1335
Epoch 1335 :: Batch 0/1
Batch Loss = 0.0007309484141833986
1335, epoch_train_loss=0.0007309484141833986
Epoch 1336
Epoch 1336 :: Batch 0/1
Batch Loss = 0.0007309484141833952
1336, epoch_train_loss=0.0007309484141833952
Epoch 1337
Epoch 1337 :: Batch 0/1
Batch Loss = 0.0007309484141833922
1337, epoch_train_loss=0.0007309484141833922
Epoch 1338
Epoch 1338 :: Batch 0/1
Batch Loss = 0.0007309484141833889
1338, epoch_train_loss=0.0007309484141833889
Epoch 1339
Epoch 1339 :: Batch 0/1
Batch Loss = 0.0007309484141833856
1339, epoch_train_loss=0.0007309484141833856
Epoch 1340
Epoch 1340 :: Batch 0/1
Batch Loss = 0.0007309484141833825
1340, epoch_train_loss=0.0007309484141833825
Epoch 1341
Epoch 1341 :: Batch 0/1
Batch Loss = 0.0007309484141833793
1341, epoch_train_loss=0.0007309484141833793
Epoch 1342
Epoch 1342 :: Batch 0/1
Batch Loss = 0.000730948414183376
1342, epoch_train_loss=0.000730948414183376
Epoch 1343
Epoch 1343 :: Batch 0/1
Batch Loss = 0.0007309484141833729
1343, epoch_train_loss=0.0007309484141833729
Epoch 1344
Epoch 1344 :: Batch 0/1
Batch Loss = 0.0007309484141833696
1344, epoch_train_loss=0.0007309484141833696
Epoch 1345
Epoch 1345 :: Batch 0/1
Batch Loss = 0.0007309484141833665
1345, epoch_train_loss=0.0007309484141833665
Epoch 1346
Epoch 1346 :: Batch 0/1
Batch Loss = 0.0007309484141833631
1346, epoch_train_loss=0.0007309484141833631
Epoch 1347
Epoch 1347 :: Batch 0/1
Batch Loss = 0.00073094841418336
1347, epoch_train_loss=0.00073094841418336
Epoch 1348
Epoch 1348 :: Batch 0/1
Batch Loss = 0.0007309484141833567
1348, epoch_train_loss=0.0007309484141833567
Epoch 1349
Epoch 1349 :: Batch 0/1
Batch Loss = 0.0007309484141833534
1349, epoch_train_loss=0.0007309484141833534
Epoch 1350
Epoch 1350 :: Batch 0/1
Batch Loss = 0.0007309484141833503
1350, epoch_train_loss=0.0007309484141833503
Epoch 1351
Epoch 1351 :: Batch 0/1
Batch Loss = 0.000730948414183347
1351, epoch_train_loss=0.000730948414183347
Epoch 1352
Epoch 1352 :: Batch 0/1
Batch Loss = 0.0007309484141833439
1352, epoch_train_loss=0.0007309484141833439
Epoch 1353
Epoch 1353 :: Batch 0/1
Batch Loss = 0.0007309484141833405
1353, epoch_train_loss=0.0007309484141833405
Epoch 1354
Epoch 1354 :: Batch 0/1
Batch Loss = 0.0007309484141833374
1354, epoch_train_loss=0.0007309484141833374
Epoch 1355
Epoch 1355 :: Batch 0/1
Batch Loss = 0.000730948414183334
1355, epoch_train_loss=0.000730948414183334
Epoch 1356
Epoch 1356 :: Batch 0/1
Batch Loss = 0.0007309484141833308
1356, epoch_train_loss=0.0007309484141833308
Epoch 1357
Epoch 1357 :: Batch 0/1
Batch Loss = 0.0007309484141833276
1357, epoch_train_loss=0.0007309484141833276
Epoch 1358
Epoch 1358 :: Batch 0/1
Batch Loss = 0.0007309484141833244
1358, epoch_train_loss=0.0007309484141833244
Epoch 1359
Epoch 1359 :: Batch 0/1
Batch Loss = 0.0007309484141833211
1359, epoch_train_loss=0.0007309484141833211
Epoch 1360
Epoch 1360 :: Batch 0/1
Batch Loss = 0.0007309484141833179
1360, epoch_train_loss=0.0007309484141833179
Epoch 1361
Epoch 1361 :: Batch 0/1
Batch Loss = 0.0007309484141833146
1361, epoch_train_loss=0.0007309484141833146
Epoch 1362
Epoch 1362 :: Batch 0/1
Batch Loss = 0.0007309484141833114
1362, epoch_train_loss=0.0007309484141833114
Epoch 1363
Epoch 1363 :: Batch 0/1
Batch Loss = 0.0007309484141833081
1363, epoch_train_loss=0.0007309484141833081
Epoch 1364
Epoch 1364 :: Batch 0/1
Batch Loss = 0.0007309484141833048
1364, epoch_train_loss=0.0007309484141833048
Epoch 1365
Epoch 1365 :: Batch 0/1
Batch Loss = 0.0007309484141833016
1365, epoch_train_loss=0.0007309484141833016
Epoch 1366
Epoch 1366 :: Batch 0/1
Batch Loss = 0.0007309484141832984
1366, epoch_train_loss=0.0007309484141832984
Epoch 1367
Epoch 1367 :: Batch 0/1
Batch Loss = 0.0007309484141832951
1367, epoch_train_loss=0.0007309484141832951
Epoch 1368
Epoch 1368 :: Batch 0/1
Batch Loss = 0.0007309484141832919
1368, epoch_train_loss=0.0007309484141832919
Epoch 1369
Epoch 1369 :: Batch 0/1
Batch Loss = 0.0007309484141832886
1369, epoch_train_loss=0.0007309484141832886
Epoch 1370
Epoch 1370 :: Batch 0/1
Batch Loss = 0.0007309484141832854
1370, epoch_train_loss=0.0007309484141832854
Epoch 1371
Epoch 1371 :: Batch 0/1
Batch Loss = 0.000730948414183282
1371, epoch_train_loss=0.000730948414183282
Epoch 1372
Epoch 1372 :: Batch 0/1
Batch Loss = 0.0007309484141832787
1372, epoch_train_loss=0.0007309484141832787
Epoch 1373
Epoch 1373 :: Batch 0/1
Batch Loss = 0.0007309484141832756
1373, epoch_train_loss=0.0007309484141832756
Epoch 1374
Epoch 1374 :: Batch 0/1
Batch Loss = 0.0007309484141832722
1374, epoch_train_loss=0.0007309484141832722
Epoch 1375
Epoch 1375 :: Batch 0/1
Batch Loss = 0.000730948414183269
1375, epoch_train_loss=0.000730948414183269
Epoch 1376
Epoch 1376 :: Batch 0/1
Batch Loss = 0.0007309484141832657
1376, epoch_train_loss=0.0007309484141832657
Epoch 1377
Epoch 1377 :: Batch 0/1
Batch Loss = 0.0007309484141832624
1377, epoch_train_loss=0.0007309484141832624
Epoch 1378
Epoch 1378 :: Batch 0/1
Batch Loss = 0.000730948414183259
1378, epoch_train_loss=0.000730948414183259
Epoch 1379
Epoch 1379 :: Batch 0/1
Batch Loss = 0.0007309484141832558
1379, epoch_train_loss=0.0007309484141832558
Epoch 1380
Epoch 1380 :: Batch 0/1
Batch Loss = 0.0007309484141832525
1380, epoch_train_loss=0.0007309484141832525
Epoch 1381
Epoch 1381 :: Batch 0/1
Batch Loss = 0.0007309484141832491
1381, epoch_train_loss=0.0007309484141832491
Epoch 1382
Epoch 1382 :: Batch 0/1
Batch Loss = 0.0007309484141832458
1382, epoch_train_loss=0.0007309484141832458
Epoch 1383
Epoch 1383 :: Batch 0/1
Batch Loss = 0.0007309484141832428
1383, epoch_train_loss=0.0007309484141832428
Epoch 1384
Epoch 1384 :: Batch 0/1
Batch Loss = 0.0007309484141832394
1384, epoch_train_loss=0.0007309484141832394
Epoch 1385
Epoch 1385 :: Batch 0/1
Batch Loss = 0.000730948414183236
1385, epoch_train_loss=0.000730948414183236
Epoch 1386
Epoch 1386 :: Batch 0/1
Batch Loss = 0.0007309484141832328
1386, epoch_train_loss=0.0007309484141832328
Epoch 1387
Epoch 1387 :: Batch 0/1
Batch Loss = 0.0007309484141832294
1387, epoch_train_loss=0.0007309484141832294
Epoch 1388
Epoch 1388 :: Batch 0/1
Batch Loss = 0.0007309484141832262
1388, epoch_train_loss=0.0007309484141832262
Epoch 1389
Epoch 1389 :: Batch 0/1
Batch Loss = 0.0007309484141832228
1389, epoch_train_loss=0.0007309484141832228
Epoch 1390
Epoch 1390 :: Batch 0/1
Batch Loss = 0.0007309484141832196
1390, epoch_train_loss=0.0007309484141832196
Epoch 1391
Epoch 1391 :: Batch 0/1
Batch Loss = 0.0007309484141832161
1391, epoch_train_loss=0.0007309484141832161
Epoch 1392
Epoch 1392 :: Batch 0/1
Batch Loss = 0.0007309484141832129
1392, epoch_train_loss=0.0007309484141832129
Epoch 1393
Epoch 1393 :: Batch 0/1
Batch Loss = 0.0007309484141832096
1393, epoch_train_loss=0.0007309484141832096
Epoch 1394
Epoch 1394 :: Batch 0/1
Batch Loss = 0.0007309484141832062
1394, epoch_train_loss=0.0007309484141832062
Epoch 1395
Epoch 1395 :: Batch 0/1
Batch Loss = 0.0007309484141832029
1395, epoch_train_loss=0.0007309484141832029
Epoch 1396
Epoch 1396 :: Batch 0/1
Batch Loss = 0.0007309484141831995
1396, epoch_train_loss=0.0007309484141831995
Epoch 1397
Epoch 1397 :: Batch 0/1
Batch Loss = 0.0007309484141831963
1397, epoch_train_loss=0.0007309484141831963
Epoch 1398
Epoch 1398 :: Batch 0/1
Batch Loss = 0.000730948414183193
1398, epoch_train_loss=0.000730948414183193
Epoch 1399
Epoch 1399 :: Batch 0/1
Batch Loss = 0.0007309484141831897
1399, epoch_train_loss=0.0007309484141831897
Epoch 1400
Epoch 1400 :: Batch 0/1
Batch Loss = 0.0007309484141831864
1400, epoch_train_loss=0.0007309484141831864
Epoch 1401
Epoch 1401 :: Batch 0/1
Batch Loss = 0.000730948414183183
1401, epoch_train_loss=0.000730948414183183
Epoch 1402
Epoch 1402 :: Batch 0/1
Batch Loss = 0.0007309484141831797
1402, epoch_train_loss=0.0007309484141831797
Epoch 1403
Epoch 1403 :: Batch 0/1
Batch Loss = 0.0007309484141831763
1403, epoch_train_loss=0.0007309484141831763
Epoch 1404
Epoch 1404 :: Batch 0/1
Batch Loss = 0.0007309484141831729
1404, epoch_train_loss=0.0007309484141831729
Epoch 1405
Epoch 1405 :: Batch 0/1
Batch Loss = 0.0007309484141831696
1405, epoch_train_loss=0.0007309484141831696
Epoch 1406
Epoch 1406 :: Batch 0/1
Batch Loss = 0.0007309484141831662
1406, epoch_train_loss=0.0007309484141831662
Epoch 1407
Epoch 1407 :: Batch 0/1
Batch Loss = 0.0007309484141831628
1407, epoch_train_loss=0.0007309484141831628
Epoch 1408
Epoch 1408 :: Batch 0/1
Batch Loss = 0.0007309484141831595
1408, epoch_train_loss=0.0007309484141831595
Epoch 1409
Epoch 1409 :: Batch 0/1
Batch Loss = 0.0007309484141831562
1409, epoch_train_loss=0.0007309484141831562
Epoch 1410
Epoch 1410 :: Batch 0/1
Batch Loss = 0.0007309484141831529
1410, epoch_train_loss=0.0007309484141831529
Epoch 1411
Epoch 1411 :: Batch 0/1
Batch Loss = 0.0007309484141831495
1411, epoch_train_loss=0.0007309484141831495
Epoch 1412
Epoch 1412 :: Batch 0/1
Batch Loss = 0.0007309484141831461
1412, epoch_train_loss=0.0007309484141831461
Epoch 1413
Epoch 1413 :: Batch 0/1
Batch Loss = 0.0007309484141831428
1413, epoch_train_loss=0.0007309484141831428
Epoch 1414
Epoch 1414 :: Batch 0/1
Batch Loss = 0.0007309484141831394
1414, epoch_train_loss=0.0007309484141831394
Epoch 1415
Epoch 1415 :: Batch 0/1
Batch Loss = 0.000730948414183136
1415, epoch_train_loss=0.000730948414183136
Epoch 1416
Epoch 1416 :: Batch 0/1
Batch Loss = 0.0007309484141831327
1416, epoch_train_loss=0.0007309484141831327
Epoch 1417
Epoch 1417 :: Batch 0/1
Batch Loss = 0.0007309484141831293
1417, epoch_train_loss=0.0007309484141831293
Epoch 1418
Epoch 1418 :: Batch 0/1
Batch Loss = 0.0007309484141831259
1418, epoch_train_loss=0.0007309484141831259
Epoch 1419
Epoch 1419 :: Batch 0/1
Batch Loss = 0.0007309484141831225
1419, epoch_train_loss=0.0007309484141831225
Epoch 1420
Epoch 1420 :: Batch 0/1
Batch Loss = 0.0007309484141831192
1420, epoch_train_loss=0.0007309484141831192
Epoch 1421
Epoch 1421 :: Batch 0/1
Batch Loss = 0.0007309484141831158
1421, epoch_train_loss=0.0007309484141831158
Epoch 1422
Epoch 1422 :: Batch 0/1
Batch Loss = 0.0007309484141831124
1422, epoch_train_loss=0.0007309484141831124
Epoch 1423
Epoch 1423 :: Batch 0/1
Batch Loss = 0.0007309484141831091
1423, epoch_train_loss=0.0007309484141831091
Epoch 1424
Epoch 1424 :: Batch 0/1
Batch Loss = 0.0007309484141831057
1424, epoch_train_loss=0.0007309484141831057
Epoch 1425
Epoch 1425 :: Batch 0/1
Batch Loss = 0.0007309484141831025
1425, epoch_train_loss=0.0007309484141831025
Epoch 1426
Epoch 1426 :: Batch 0/1
Batch Loss = 0.0007309484141830991
1426, epoch_train_loss=0.0007309484141830991
Epoch 1427
Epoch 1427 :: Batch 0/1
Batch Loss = 0.0007309484141830955
1427, epoch_train_loss=0.0007309484141830955
Epoch 1428
Epoch 1428 :: Batch 0/1
Batch Loss = 0.0007309484141830923
1428, epoch_train_loss=0.0007309484141830923
Epoch 1429
Epoch 1429 :: Batch 0/1
Batch Loss = 0.0007309484141830887
1429, epoch_train_loss=0.0007309484141830887
Epoch 1430
Epoch 1430 :: Batch 0/1
Batch Loss = 0.0007309484141830853
1430, epoch_train_loss=0.0007309484141830853
Epoch 1431
Epoch 1431 :: Batch 0/1
Batch Loss = 0.000730948414183082
1431, epoch_train_loss=0.000730948414183082
Epoch 1432
Epoch 1432 :: Batch 0/1
Batch Loss = 0.0007309484141830787
1432, epoch_train_loss=0.0007309484141830787
Epoch 1433
Epoch 1433 :: Batch 0/1
Batch Loss = 0.0007309484141830754
1433, epoch_train_loss=0.0007309484141830754
Epoch 1434
Epoch 1434 :: Batch 0/1
Batch Loss = 0.000730948414183072
1434, epoch_train_loss=0.000730948414183072
Epoch 1435
Epoch 1435 :: Batch 0/1
Batch Loss = 0.0007309484141830683
1435, epoch_train_loss=0.0007309484141830683
Epoch 1436
Epoch 1436 :: Batch 0/1
Batch Loss = 0.0007309484141830649
1436, epoch_train_loss=0.0007309484141830649
Epoch 1437
Epoch 1437 :: Batch 0/1
Batch Loss = 0.0007309484141830616
1437, epoch_train_loss=0.0007309484141830616
Epoch 1438
Epoch 1438 :: Batch 0/1
Batch Loss = 0.0007309484141830583
1438, epoch_train_loss=0.0007309484141830583
Epoch 1439
Epoch 1439 :: Batch 0/1
Batch Loss = 0.0007309484141830548
1439, epoch_train_loss=0.0007309484141830548
Epoch 1440
Epoch 1440 :: Batch 0/1
Batch Loss = 0.0007309484141830514
1440, epoch_train_loss=0.0007309484141830514
Epoch 1441
Epoch 1441 :: Batch 0/1
Batch Loss = 0.0007309484141830479
1441, epoch_train_loss=0.0007309484141830479
Epoch 1442
Epoch 1442 :: Batch 0/1
Batch Loss = 0.0007309484141830446
1442, epoch_train_loss=0.0007309484141830446
Epoch 1443
Epoch 1443 :: Batch 0/1
Batch Loss = 0.0007309484141830411
1443, epoch_train_loss=0.0007309484141830411
Epoch 1444
Epoch 1444 :: Batch 0/1
Batch Loss = 0.0007309484141830376
1444, epoch_train_loss=0.0007309484141830376
Epoch 1445
Epoch 1445 :: Batch 0/1
Batch Loss = 0.0007309484141830343
1445, epoch_train_loss=0.0007309484141830343
Epoch 1446
Epoch 1446 :: Batch 0/1
Batch Loss = 0.0007309484141830308
1446, epoch_train_loss=0.0007309484141830308
Epoch 1447
Epoch 1447 :: Batch 0/1
Batch Loss = 0.0007309484141830273
1447, epoch_train_loss=0.0007309484141830273
Epoch 1448
Epoch 1448 :: Batch 0/1
Batch Loss = 0.000730948414183024
1448, epoch_train_loss=0.000730948414183024
Epoch 1449
Epoch 1449 :: Batch 0/1
Batch Loss = 0.0007309484141830206
1449, epoch_train_loss=0.0007309484141830206
Epoch 1450
Epoch 1450 :: Batch 0/1
Batch Loss = 0.0007309484141830171
1450, epoch_train_loss=0.0007309484141830171
Epoch 1451
Epoch 1451 :: Batch 0/1
Batch Loss = 0.0007309484141830137
1451, epoch_train_loss=0.0007309484141830137
Epoch 1452
Epoch 1452 :: Batch 0/1
Batch Loss = 0.0007309484141830102
1452, epoch_train_loss=0.0007309484141830102
Epoch 1453
Epoch 1453 :: Batch 0/1
Batch Loss = 0.0007309484141830067
1453, epoch_train_loss=0.0007309484141830067
Epoch 1454
Epoch 1454 :: Batch 0/1
Batch Loss = 0.0007309484141830034
1454, epoch_train_loss=0.0007309484141830034
Epoch 1455
Epoch 1455 :: Batch 0/1
Batch Loss = 0.000730948414183
1455, epoch_train_loss=0.000730948414183
Epoch 1456
Epoch 1456 :: Batch 0/1
Batch Loss = 0.0007309484141829965
1456, epoch_train_loss=0.0007309484141829965
Epoch 1457
Epoch 1457 :: Batch 0/1
Batch Loss = 0.0007309484141829931
1457, epoch_train_loss=0.0007309484141829931
Epoch 1458
Epoch 1458 :: Batch 0/1
Batch Loss = 0.0007309484141829896
1458, epoch_train_loss=0.0007309484141829896
Epoch 1459
Epoch 1459 :: Batch 0/1
Batch Loss = 0.0007309484141829861
1459, epoch_train_loss=0.0007309484141829861
Epoch 1460
Epoch 1460 :: Batch 0/1
Batch Loss = 0.0007309484141829827
1460, epoch_train_loss=0.0007309484141829827
Epoch 1461
Epoch 1461 :: Batch 0/1
Batch Loss = 0.0007309484141829792
1461, epoch_train_loss=0.0007309484141829792
Epoch 1462
Epoch 1462 :: Batch 0/1
Batch Loss = 0.0007309484141829758
1462, epoch_train_loss=0.0007309484141829758
Epoch 1463
Epoch 1463 :: Batch 0/1
Batch Loss = 0.0007309484141829724
1463, epoch_train_loss=0.0007309484141829724
Epoch 1464
Epoch 1464 :: Batch 0/1
Batch Loss = 0.0007309484141829688
1464, epoch_train_loss=0.0007309484141829688
Epoch 1465
Epoch 1465 :: Batch 0/1
Batch Loss = 0.0007309484141829654
1465, epoch_train_loss=0.0007309484141829654
Epoch 1466
Epoch 1466 :: Batch 0/1
Batch Loss = 0.0007309484141829618
1466, epoch_train_loss=0.0007309484141829618
Epoch 1467
Epoch 1467 :: Batch 0/1
Batch Loss = 0.0007309484141829584
1467, epoch_train_loss=0.0007309484141829584
Epoch 1468
Epoch 1468 :: Batch 0/1
Batch Loss = 0.0007309484141829549
1468, epoch_train_loss=0.0007309484141829549
Epoch 1469
Epoch 1469 :: Batch 0/1
Batch Loss = 0.0007309484141829515
1469, epoch_train_loss=0.0007309484141829515
Epoch 1470
Epoch 1470 :: Batch 0/1
Batch Loss = 0.000730948414182948
1470, epoch_train_loss=0.000730948414182948
Epoch 1471
Epoch 1471 :: Batch 0/1
Batch Loss = 0.0007309484141829445
1471, epoch_train_loss=0.0007309484141829445
Epoch 1472
Epoch 1472 :: Batch 0/1
Batch Loss = 0.000730948414182941
1472, epoch_train_loss=0.000730948414182941
Epoch 1473
Epoch 1473 :: Batch 0/1
Batch Loss = 0.0007309484141829375
1473, epoch_train_loss=0.0007309484141829375
Epoch 1474
Epoch 1474 :: Batch 0/1
Batch Loss = 0.0007309484141829341
1474, epoch_train_loss=0.0007309484141829341
Epoch 1475
Epoch 1475 :: Batch 0/1
Batch Loss = 0.0007309484141829307
1475, epoch_train_loss=0.0007309484141829307
Epoch 1476
Epoch 1476 :: Batch 0/1
Batch Loss = 0.0007309484141829271
1476, epoch_train_loss=0.0007309484141829271
Epoch 1477
Epoch 1477 :: Batch 0/1
Batch Loss = 0.0007309484141829236
1477, epoch_train_loss=0.0007309484141829236
Epoch 1478
Epoch 1478 :: Batch 0/1
Batch Loss = 0.0007309484141829201
1478, epoch_train_loss=0.0007309484141829201
Epoch 1479
Epoch 1479 :: Batch 0/1
Batch Loss = 0.0007309484141829165
1479, epoch_train_loss=0.0007309484141829165
Epoch 1480
Epoch 1480 :: Batch 0/1
Batch Loss = 0.0007309484141829132
1480, epoch_train_loss=0.0007309484141829132
Epoch 1481
Epoch 1481 :: Batch 0/1
Batch Loss = 0.0007309484141829097
1481, epoch_train_loss=0.0007309484141829097
Epoch 1482
Epoch 1482 :: Batch 0/1
Batch Loss = 0.000730948414182906
1482, epoch_train_loss=0.000730948414182906
Epoch 1483
Epoch 1483 :: Batch 0/1
Batch Loss = 0.0007309484141829026
1483, epoch_train_loss=0.0007309484141829026
Epoch 1484
Epoch 1484 :: Batch 0/1
Batch Loss = 0.0007309484141828991
1484, epoch_train_loss=0.0007309484141828991
Epoch 1485
Epoch 1485 :: Batch 0/1
Batch Loss = 0.0007309484141828956
1485, epoch_train_loss=0.0007309484141828956
Epoch 1486
Epoch 1486 :: Batch 0/1
Batch Loss = 0.0007309484141828921
1486, epoch_train_loss=0.0007309484141828921
Epoch 1487
Epoch 1487 :: Batch 0/1
Batch Loss = 0.0007309484141828887
1487, epoch_train_loss=0.0007309484141828887
Epoch 1488
Epoch 1488 :: Batch 0/1
Batch Loss = 0.0007309484141828851
1488, epoch_train_loss=0.0007309484141828851
Epoch 1489
Epoch 1489 :: Batch 0/1
Batch Loss = 0.0007309484141828815
1489, epoch_train_loss=0.0007309484141828815
Epoch 1490
Epoch 1490 :: Batch 0/1
Batch Loss = 0.000730948414182878
1490, epoch_train_loss=0.000730948414182878
Epoch 1491
Epoch 1491 :: Batch 0/1
Batch Loss = 0.0007309484141828746
1491, epoch_train_loss=0.0007309484141828746
Epoch 1492
Epoch 1492 :: Batch 0/1
Batch Loss = 0.000730948414182871
1492, epoch_train_loss=0.000730948414182871
Epoch 1493
Epoch 1493 :: Batch 0/1
Batch Loss = 0.0007309484141828675
1493, epoch_train_loss=0.0007309484141828675
Epoch 1494
Epoch 1494 :: Batch 0/1
Batch Loss = 0.0007309484141828639
1494, epoch_train_loss=0.0007309484141828639
Epoch 1495
Epoch 1495 :: Batch 0/1
Batch Loss = 0.0007309484141828605
1495, epoch_train_loss=0.0007309484141828605
Epoch 1496
Epoch 1496 :: Batch 0/1
Batch Loss = 0.0007309484141828568
1496, epoch_train_loss=0.0007309484141828568
Epoch 1497
Epoch 1497 :: Batch 0/1
Batch Loss = 0.0007309484141828532
1497, epoch_train_loss=0.0007309484141828532
Epoch 1498
Epoch 1498 :: Batch 0/1
Batch Loss = 0.0007309484141828497
1498, epoch_train_loss=0.0007309484141828497
Epoch 1499
Epoch 1499 :: Batch 0/1
Batch Loss = 0.0007309484141828463
1499, epoch_train_loss=0.0007309484141828463
Epoch 1500
Epoch 1500 :: Batch 0/1
Batch Loss = 0.0007309484141828427
1500, epoch_train_loss=0.0007309484141828427
Epoch 1501
Epoch 1501 :: Batch 0/1
Batch Loss = 0.0007309484141828392
1501, epoch_train_loss=0.0007309484141828392
Epoch 1502
Epoch 1502 :: Batch 0/1
Batch Loss = 0.0007309484141828356
1502, epoch_train_loss=0.0007309484141828356
Epoch 1503
Epoch 1503 :: Batch 0/1
Batch Loss = 0.0007309484141828322
1503, epoch_train_loss=0.0007309484141828322
Epoch 1504
Epoch 1504 :: Batch 0/1
Batch Loss = 0.0007309484141828285
1504, epoch_train_loss=0.0007309484141828285
Epoch 1505
Epoch 1505 :: Batch 0/1
Batch Loss = 0.0007309484141828249
1505, epoch_train_loss=0.0007309484141828249
Epoch 1506
Epoch 1506 :: Batch 0/1
Batch Loss = 0.0007309484141828213
1506, epoch_train_loss=0.0007309484141828213
Epoch 1507
Epoch 1507 :: Batch 0/1
Batch Loss = 0.000730948414182818
1507, epoch_train_loss=0.000730948414182818
Epoch 1508
Epoch 1508 :: Batch 0/1
Batch Loss = 0.0007309484141828143
1508, epoch_train_loss=0.0007309484141828143
Epoch 1509
Epoch 1509 :: Batch 0/1
Batch Loss = 0.0007309484141828108
1509, epoch_train_loss=0.0007309484141828108
Epoch 1510
Epoch 1510 :: Batch 0/1
Batch Loss = 0.0007309484141828072
1510, epoch_train_loss=0.0007309484141828072
Epoch 1511
Epoch 1511 :: Batch 0/1
Batch Loss = 0.0007309484141828035
1511, epoch_train_loss=0.0007309484141828035
Epoch 1512
Epoch 1512 :: Batch 0/1
Batch Loss = 0.0007309484141828
1512, epoch_train_loss=0.0007309484141828
Epoch 1513
Epoch 1513 :: Batch 0/1
Batch Loss = 0.0007309484141827965
1513, epoch_train_loss=0.0007309484141827965
Epoch 1514
Epoch 1514 :: Batch 0/1
Batch Loss = 0.0007309484141827928
1514, epoch_train_loss=0.0007309484141827928
Epoch 1515
Epoch 1515 :: Batch 0/1
Batch Loss = 0.0007309484141827894
1515, epoch_train_loss=0.0007309484141827894
Epoch 1516
Epoch 1516 :: Batch 0/1
Batch Loss = 0.0007309484141827859
1516, epoch_train_loss=0.0007309484141827859
Epoch 1517
Epoch 1517 :: Batch 0/1
Batch Loss = 0.0007309484141827822
1517, epoch_train_loss=0.0007309484141827822
Epoch 1518
Epoch 1518 :: Batch 0/1
Batch Loss = 0.0007309484141827786
1518, epoch_train_loss=0.0007309484141827786
Epoch 1519
Epoch 1519 :: Batch 0/1
Batch Loss = 0.000730948414182775
1519, epoch_train_loss=0.000730948414182775
Epoch 1520
Epoch 1520 :: Batch 0/1
Batch Loss = 0.0007309484141827714
1520, epoch_train_loss=0.0007309484141827714
Epoch 1521
Epoch 1521 :: Batch 0/1
Batch Loss = 0.0007309484141827679
1521, epoch_train_loss=0.0007309484141827679
Epoch 1522
Epoch 1522 :: Batch 0/1
Batch Loss = 0.0007309484141827643
1522, epoch_train_loss=0.0007309484141827643
Epoch 1523
Epoch 1523 :: Batch 0/1
Batch Loss = 0.0007309484141827606
1523, epoch_train_loss=0.0007309484141827606
Epoch 1524
Epoch 1524 :: Batch 0/1
Batch Loss = 0.000730948414182757
1524, epoch_train_loss=0.000730948414182757
Epoch 1525
Epoch 1525 :: Batch 0/1
Batch Loss = 0.0007309484141827535
1525, epoch_train_loss=0.0007309484141827535
Epoch 1526
Epoch 1526 :: Batch 0/1
Batch Loss = 0.0007309484141827499
1526, epoch_train_loss=0.0007309484141827499
Epoch 1527
Epoch 1527 :: Batch 0/1
Batch Loss = 0.0007309484141827463
1527, epoch_train_loss=0.0007309484141827463
Epoch 1528
Epoch 1528 :: Batch 0/1
Batch Loss = 0.0007309484141827427
1528, epoch_train_loss=0.0007309484141827427
Epoch 1529
Epoch 1529 :: Batch 0/1
Batch Loss = 0.000730948414182739
1529, epoch_train_loss=0.000730948414182739
Epoch 1530
Epoch 1530 :: Batch 0/1
Batch Loss = 0.0007309484141827355
1530, epoch_train_loss=0.0007309484141827355
Epoch 1531
Epoch 1531 :: Batch 0/1
Batch Loss = 0.0007309484141827319
1531, epoch_train_loss=0.0007309484141827319
Epoch 1532
Epoch 1532 :: Batch 0/1
Batch Loss = 0.0007309484141827283
1532, epoch_train_loss=0.0007309484141827283
Epoch 1533
Epoch 1533 :: Batch 0/1
Batch Loss = 0.0007309484141827247
1533, epoch_train_loss=0.0007309484141827247
Epoch 1534
Epoch 1534 :: Batch 0/1
Batch Loss = 0.000730948414182721
1534, epoch_train_loss=0.000730948414182721
Epoch 1535
Epoch 1535 :: Batch 0/1
Batch Loss = 0.0007309484141827175
1535, epoch_train_loss=0.0007309484141827175
Epoch 1536
Epoch 1536 :: Batch 0/1
Batch Loss = 0.0007309484141827138
1536, epoch_train_loss=0.0007309484141827138
Epoch 1537
Epoch 1537 :: Batch 0/1
Batch Loss = 0.0007309484141827103
1537, epoch_train_loss=0.0007309484141827103
Epoch 1538
Epoch 1538 :: Batch 0/1
Batch Loss = 0.0007309484141827067
1538, epoch_train_loss=0.0007309484141827067
Epoch 1539
Epoch 1539 :: Batch 0/1
Batch Loss = 0.0007309484141827031
1539, epoch_train_loss=0.0007309484141827031
Epoch 1540
Epoch 1540 :: Batch 0/1
Batch Loss = 0.0007309484141826993
1540, epoch_train_loss=0.0007309484141826993
Epoch 1541
Epoch 1541 :: Batch 0/1
Batch Loss = 0.0007309484141826958
1541, epoch_train_loss=0.0007309484141826958
Epoch 1542
Epoch 1542 :: Batch 0/1
Batch Loss = 0.0007309484141826921
1542, epoch_train_loss=0.0007309484141826921
Epoch 1543
Epoch 1543 :: Batch 0/1
Batch Loss = 0.0007309484141826885
1543, epoch_train_loss=0.0007309484141826885
Epoch 1544
Epoch 1544 :: Batch 0/1
Batch Loss = 0.0007309484141826848
1544, epoch_train_loss=0.0007309484141826848
Epoch 1545
Epoch 1545 :: Batch 0/1
Batch Loss = 0.0007309484141826811
1545, epoch_train_loss=0.0007309484141826811
Epoch 1546
Epoch 1546 :: Batch 0/1
Batch Loss = 0.0007309484141826776
1546, epoch_train_loss=0.0007309484141826776
Epoch 1547
Epoch 1547 :: Batch 0/1
Batch Loss = 0.000730948414182674
1547, epoch_train_loss=0.000730948414182674
Epoch 1548
Epoch 1548 :: Batch 0/1
Batch Loss = 0.0007309484141826702
1548, epoch_train_loss=0.0007309484141826702
Epoch 1549
Epoch 1549 :: Batch 0/1
Batch Loss = 0.0007309484141826666
1549, epoch_train_loss=0.0007309484141826666
Epoch 1550
Epoch 1550 :: Batch 0/1
Batch Loss = 0.000730948414182663
1550, epoch_train_loss=0.000730948414182663
Epoch 1551
Epoch 1551 :: Batch 0/1
Batch Loss = 0.0007309484141826593
1551, epoch_train_loss=0.0007309484141826593
Epoch 1552
Epoch 1552 :: Batch 0/1
Batch Loss = 0.0007309484141826557
1552, epoch_train_loss=0.0007309484141826557
Epoch 1553
Epoch 1553 :: Batch 0/1
Batch Loss = 0.000730948414182652
1553, epoch_train_loss=0.000730948414182652
Epoch 1554
Epoch 1554 :: Batch 0/1
Batch Loss = 0.0007309484141826484
1554, epoch_train_loss=0.0007309484141826484
Epoch 1555
Epoch 1555 :: Batch 0/1
Batch Loss = 0.0007309484141826448
1555, epoch_train_loss=0.0007309484141826448
Epoch 1556
Epoch 1556 :: Batch 0/1
Batch Loss = 0.0007309484141826411
1556, epoch_train_loss=0.0007309484141826411
Epoch 1557
Epoch 1557 :: Batch 0/1
Batch Loss = 0.0007309484141826373
1557, epoch_train_loss=0.0007309484141826373
Epoch 1558
Epoch 1558 :: Batch 0/1
Batch Loss = 0.0007309484141826338
1558, epoch_train_loss=0.0007309484141826338
Epoch 1559
Epoch 1559 :: Batch 0/1
Batch Loss = 0.0007309484141826301
1559, epoch_train_loss=0.0007309484141826301
Epoch 1560
Epoch 1560 :: Batch 0/1
Batch Loss = 0.0007309484141826264
1560, epoch_train_loss=0.0007309484141826264
Epoch 1561
Epoch 1561 :: Batch 0/1
Batch Loss = 0.0007309484141826227
1561, epoch_train_loss=0.0007309484141826227
Epoch 1562
Epoch 1562 :: Batch 0/1
Batch Loss = 0.000730948414182619
1562, epoch_train_loss=0.000730948414182619
Epoch 1563
Epoch 1563 :: Batch 0/1
Batch Loss = 0.0007309484141826153
1563, epoch_train_loss=0.0007309484141826153
Epoch 1564
Epoch 1564 :: Batch 0/1
Batch Loss = 0.0007309484141826117
1564, epoch_train_loss=0.0007309484141826117
Epoch 1565
Epoch 1565 :: Batch 0/1
Batch Loss = 0.000730948414182608
1565, epoch_train_loss=0.000730948414182608
Epoch 1566
Epoch 1566 :: Batch 0/1
Batch Loss = 0.0007309484141826043
1566, epoch_train_loss=0.0007309484141826043
Epoch 1567
Epoch 1567 :: Batch 0/1
Batch Loss = 0.0007309484141826007
1567, epoch_train_loss=0.0007309484141826007
Epoch 1568
Epoch 1568 :: Batch 0/1
Batch Loss = 0.0007309484141825969
1568, epoch_train_loss=0.0007309484141825969
Epoch 1569
Epoch 1569 :: Batch 0/1
Batch Loss = 0.0007309484141825932
1569, epoch_train_loss=0.0007309484141825932
Epoch 1570
Epoch 1570 :: Batch 0/1
Batch Loss = 0.0007309484141825896
1570, epoch_train_loss=0.0007309484141825896
Epoch 1571
Epoch 1571 :: Batch 0/1
Batch Loss = 0.0007309484141825859
1571, epoch_train_loss=0.0007309484141825859
Epoch 1572
Epoch 1572 :: Batch 0/1
Batch Loss = 0.0007309484141825823
1572, epoch_train_loss=0.0007309484141825823
Epoch 1573
Epoch 1573 :: Batch 0/1
Batch Loss = 0.0007309484141825784
1573, epoch_train_loss=0.0007309484141825784
Epoch 1574
Epoch 1574 :: Batch 0/1
Batch Loss = 0.0007309484141825748
1574, epoch_train_loss=0.0007309484141825748
Epoch 1575
Epoch 1575 :: Batch 0/1
Batch Loss = 0.0007309484141825711
1575, epoch_train_loss=0.0007309484141825711
Epoch 1576
Epoch 1576 :: Batch 0/1
Batch Loss = 0.0007309484141825674
1576, epoch_train_loss=0.0007309484141825674
Epoch 1577
Epoch 1577 :: Batch 0/1
Batch Loss = 0.0007309484141825637
1577, epoch_train_loss=0.0007309484141825637
Epoch 1578
Epoch 1578 :: Batch 0/1
Batch Loss = 0.00073094841418256
1578, epoch_train_loss=0.00073094841418256
Epoch 1579
Epoch 1579 :: Batch 0/1
Batch Loss = 0.0007309484141825563
1579, epoch_train_loss=0.0007309484141825563
Epoch 1580
Epoch 1580 :: Batch 0/1
Batch Loss = 0.0007309484141825525
1580, epoch_train_loss=0.0007309484141825525
Epoch 1581
Epoch 1581 :: Batch 0/1
Batch Loss = 0.0007309484141825489
1581, epoch_train_loss=0.0007309484141825489
Epoch 1582
Epoch 1582 :: Batch 0/1
Batch Loss = 0.0007309484141825452
1582, epoch_train_loss=0.0007309484141825452
Epoch 1583
Epoch 1583 :: Batch 0/1
Batch Loss = 0.0007309484141825415
1583, epoch_train_loss=0.0007309484141825415
Epoch 1584
Epoch 1584 :: Batch 0/1
Batch Loss = 0.0007309484141825376
1584, epoch_train_loss=0.0007309484141825376
Epoch 1585
Epoch 1585 :: Batch 0/1
Batch Loss = 0.000730948414182534
1585, epoch_train_loss=0.000730948414182534
Epoch 1586
Epoch 1586 :: Batch 0/1
Batch Loss = 0.0007309484141825302
1586, epoch_train_loss=0.0007309484141825302
Epoch 1587
Epoch 1587 :: Batch 0/1
Batch Loss = 0.0007309484141825265
1587, epoch_train_loss=0.0007309484141825265
Epoch 1588
Epoch 1588 :: Batch 0/1
Batch Loss = 0.0007309484141825227
1588, epoch_train_loss=0.0007309484141825227
Epoch 1589
Epoch 1589 :: Batch 0/1
Batch Loss = 0.0007309484141825192
1589, epoch_train_loss=0.0007309484141825192
Epoch 1590
Epoch 1590 :: Batch 0/1
Batch Loss = 0.0007309484141825154
1590, epoch_train_loss=0.0007309484141825154
Epoch 1591
Epoch 1591 :: Batch 0/1
Batch Loss = 0.0007309484141825115
1591, epoch_train_loss=0.0007309484141825115
Epoch 1592
Epoch 1592 :: Batch 0/1
Batch Loss = 0.0007309484141825079
1592, epoch_train_loss=0.0007309484141825079
Epoch 1593
Epoch 1593 :: Batch 0/1
Batch Loss = 0.0007309484141825042
1593, epoch_train_loss=0.0007309484141825042
Epoch 1594
Epoch 1594 :: Batch 0/1
Batch Loss = 0.0007309484141825004
1594, epoch_train_loss=0.0007309484141825004
Epoch 1595
Epoch 1595 :: Batch 0/1
Batch Loss = 0.0007309484141824966
1595, epoch_train_loss=0.0007309484141824966
Epoch 1596
Epoch 1596 :: Batch 0/1
Batch Loss = 0.000730948414182493
1596, epoch_train_loss=0.000730948414182493
Epoch 1597
Epoch 1597 :: Batch 0/1
Batch Loss = 0.0007309484141824891
1597, epoch_train_loss=0.0007309484141824891
Epoch 1598
Epoch 1598 :: Batch 0/1
Batch Loss = 0.0007309484141824854
1598, epoch_train_loss=0.0007309484141824854
Epoch 1599
Epoch 1599 :: Batch 0/1
Batch Loss = 0.0007309484141824816
1599, epoch_train_loss=0.0007309484141824816
Epoch 1600
Epoch 1600 :: Batch 0/1
Batch Loss = 0.0007309484141824778
1600, epoch_train_loss=0.0007309484141824778
Epoch 1601
Epoch 1601 :: Batch 0/1
Batch Loss = 0.000730948414182474
1601, epoch_train_loss=0.000730948414182474
Epoch 1602
Epoch 1602 :: Batch 0/1
Batch Loss = 0.0007309484141824705
1602, epoch_train_loss=0.0007309484141824705
Epoch 1603
Epoch 1603 :: Batch 0/1
Batch Loss = 0.0007309484141824666
1603, epoch_train_loss=0.0007309484141824666
Epoch 1604
Epoch 1604 :: Batch 0/1
Batch Loss = 0.0007309484141824628
1604, epoch_train_loss=0.0007309484141824628
Epoch 1605
Epoch 1605 :: Batch 0/1
Batch Loss = 0.0007309484141824591
1605, epoch_train_loss=0.0007309484141824591
Epoch 1606
Epoch 1606 :: Batch 0/1
Batch Loss = 0.0007309484141824554
1606, epoch_train_loss=0.0007309484141824554
Epoch 1607
Epoch 1607 :: Batch 0/1
Batch Loss = 0.0007309484141824515
1607, epoch_train_loss=0.0007309484141824515
Epoch 1608
Epoch 1608 :: Batch 0/1
Batch Loss = 0.0007309484141824477
1608, epoch_train_loss=0.0007309484141824477
Epoch 1609
Epoch 1609 :: Batch 0/1
Batch Loss = 0.0007309484141824439
1609, epoch_train_loss=0.0007309484141824439
Epoch 1610
Epoch 1610 :: Batch 0/1
Batch Loss = 0.0007309484141824402
1610, epoch_train_loss=0.0007309484141824402
Epoch 1611
Epoch 1611 :: Batch 0/1
Batch Loss = 0.0007309484141824364
1611, epoch_train_loss=0.0007309484141824364
Epoch 1612
Epoch 1612 :: Batch 0/1
Batch Loss = 0.0007309484141824325
1612, epoch_train_loss=0.0007309484141824325
Epoch 1613
Epoch 1613 :: Batch 0/1
Batch Loss = 0.0007309484141824288
1613, epoch_train_loss=0.0007309484141824288
Epoch 1614
Epoch 1614 :: Batch 0/1
Batch Loss = 0.0007309484141824252
1614, epoch_train_loss=0.0007309484141824252
Epoch 1615
Epoch 1615 :: Batch 0/1
Batch Loss = 0.0007309484141824213
1615, epoch_train_loss=0.0007309484141824213
Epoch 1616
Epoch 1616 :: Batch 0/1
Batch Loss = 0.0007309484141824175
1616, epoch_train_loss=0.0007309484141824175
Epoch 1617
Epoch 1617 :: Batch 0/1
Batch Loss = 0.0007309484141824137
1617, epoch_train_loss=0.0007309484141824137
Epoch 1618
Epoch 1618 :: Batch 0/1
Batch Loss = 0.00073094841418241
1618, epoch_train_loss=0.00073094841418241
Epoch 1619
Epoch 1619 :: Batch 0/1
Batch Loss = 0.0007309484141824062
1619, epoch_train_loss=0.0007309484141824062
Epoch 1620
Epoch 1620 :: Batch 0/1
Batch Loss = 0.0007309484141824024
1620, epoch_train_loss=0.0007309484141824024
Epoch 1621
Epoch 1621 :: Batch 0/1
Batch Loss = 0.0007309484141823985
1621, epoch_train_loss=0.0007309484141823985
Epoch 1622
Epoch 1622 :: Batch 0/1
Batch Loss = 0.0007309484141823947
1622, epoch_train_loss=0.0007309484141823947
Epoch 1623
Epoch 1623 :: Batch 0/1
Batch Loss = 0.0007309484141823908
1623, epoch_train_loss=0.0007309484141823908
Epoch 1624
Epoch 1624 :: Batch 0/1
Batch Loss = 0.000730948414182387
1624, epoch_train_loss=0.000730948414182387
Epoch 1625
Epoch 1625 :: Batch 0/1
Batch Loss = 0.0007309484141823833
1625, epoch_train_loss=0.0007309484141823833
Epoch 1626
Epoch 1626 :: Batch 0/1
Batch Loss = 0.0007309484141823793
1626, epoch_train_loss=0.0007309484141823793
Epoch 1627
Epoch 1627 :: Batch 0/1
Batch Loss = 0.0007309484141823757
1627, epoch_train_loss=0.0007309484141823757
Epoch 1628
Epoch 1628 :: Batch 0/1
Batch Loss = 0.0007309484141823719
1628, epoch_train_loss=0.0007309484141823719
Epoch 1629
Epoch 1629 :: Batch 0/1
Batch Loss = 0.000730948414182368
1629, epoch_train_loss=0.000730948414182368
Epoch 1630
Epoch 1630 :: Batch 0/1
Batch Loss = 0.0007309484141823642
1630, epoch_train_loss=0.0007309484141823642
Epoch 1631
Epoch 1631 :: Batch 0/1
Batch Loss = 0.0007309484141823604
1631, epoch_train_loss=0.0007309484141823604
Epoch 1632
Epoch 1632 :: Batch 0/1
Batch Loss = 0.0007309484141823565
1632, epoch_train_loss=0.0007309484141823565
Epoch 1633
Epoch 1633 :: Batch 0/1
Batch Loss = 0.0007309484141823527
1633, epoch_train_loss=0.0007309484141823527
Epoch 1634
Epoch 1634 :: Batch 0/1
Batch Loss = 0.0007309484141823488
1634, epoch_train_loss=0.0007309484141823488
Epoch 1635
Epoch 1635 :: Batch 0/1
Batch Loss = 0.000730948414182345
1635, epoch_train_loss=0.000730948414182345
Epoch 1636
Epoch 1636 :: Batch 0/1
Batch Loss = 0.0007309484141823412
1636, epoch_train_loss=0.0007309484141823412
Epoch 1637
Epoch 1637 :: Batch 0/1
Batch Loss = 0.0007309484141823373
1637, epoch_train_loss=0.0007309484141823373
Epoch 1638
Epoch 1638 :: Batch 0/1
Batch Loss = 0.0007309484141823335
1638, epoch_train_loss=0.0007309484141823335
Epoch 1639
Epoch 1639 :: Batch 0/1
Batch Loss = 0.0007309484141823296
1639, epoch_train_loss=0.0007309484141823296
Epoch 1640
Epoch 1640 :: Batch 0/1
Batch Loss = 0.0007309484141823258
1640, epoch_train_loss=0.0007309484141823258
Epoch 1641
Epoch 1641 :: Batch 0/1
Batch Loss = 0.000730948414182322
1641, epoch_train_loss=0.000730948414182322
Epoch 1642
Epoch 1642 :: Batch 0/1
Batch Loss = 0.0007309484141823181
1642, epoch_train_loss=0.0007309484141823181
Epoch 1643
Epoch 1643 :: Batch 0/1
Batch Loss = 0.0007309484141823143
1643, epoch_train_loss=0.0007309484141823143
Epoch 1644
Epoch 1644 :: Batch 0/1
Batch Loss = 0.0007309484141823104
1644, epoch_train_loss=0.0007309484141823104
Epoch 1645
Epoch 1645 :: Batch 0/1
Batch Loss = 0.0007309484141823067
1645, epoch_train_loss=0.0007309484141823067
Epoch 1646
Epoch 1646 :: Batch 0/1
Batch Loss = 0.0007309484141823029
1646, epoch_train_loss=0.0007309484141823029
Epoch 1647
Epoch 1647 :: Batch 0/1
Batch Loss = 0.0007309484141822988
1647, epoch_train_loss=0.0007309484141822988
Epoch 1648
Epoch 1648 :: Batch 0/1
Batch Loss = 0.000730948414182295
1648, epoch_train_loss=0.000730948414182295
Epoch 1649
Epoch 1649 :: Batch 0/1
Batch Loss = 0.000730948414182291
1649, epoch_train_loss=0.000730948414182291
Epoch 1650
Epoch 1650 :: Batch 0/1
Batch Loss = 0.0007309484141822872
1650, epoch_train_loss=0.0007309484141822872
Epoch 1651
Epoch 1651 :: Batch 0/1
Batch Loss = 0.0007309484141822834
1651, epoch_train_loss=0.0007309484141822834
Epoch 1652
Epoch 1652 :: Batch 0/1
Batch Loss = 0.0007309484141822795
1652, epoch_train_loss=0.0007309484141822795
Epoch 1653
Epoch 1653 :: Batch 0/1
Batch Loss = 0.0007309484141822758
1653, epoch_train_loss=0.0007309484141822758
Epoch 1654
Epoch 1654 :: Batch 0/1
Batch Loss = 0.0007309484141822717
1654, epoch_train_loss=0.0007309484141822717
Epoch 1655
Epoch 1655 :: Batch 0/1
Batch Loss = 0.000730948414182268
1655, epoch_train_loss=0.000730948414182268
Epoch 1656
Epoch 1656 :: Batch 0/1
Batch Loss = 0.000730948414182264
1656, epoch_train_loss=0.000730948414182264
Epoch 1657
Epoch 1657 :: Batch 0/1
Batch Loss = 0.0007309484141822601
1657, epoch_train_loss=0.0007309484141822601
Epoch 1658
Epoch 1658 :: Batch 0/1
Batch Loss = 0.0007309484141822563
1658, epoch_train_loss=0.0007309484141822563
Epoch 1659
Epoch 1659 :: Batch 0/1
Batch Loss = 0.0007309484141822524
1659, epoch_train_loss=0.0007309484141822524
Epoch 1660
Epoch 1660 :: Batch 0/1
Batch Loss = 0.0007309484141822485
1660, epoch_train_loss=0.0007309484141822485
Epoch 1661
Epoch 1661 :: Batch 0/1
Batch Loss = 0.0007309484141822445
1661, epoch_train_loss=0.0007309484141822445
Epoch 1662
Epoch 1662 :: Batch 0/1
Batch Loss = 0.0007309484141822407
1662, epoch_train_loss=0.0007309484141822407
Epoch 1663
Epoch 1663 :: Batch 0/1
Batch Loss = 0.0007309484141822368
1663, epoch_train_loss=0.0007309484141822368
Epoch 1664
Epoch 1664 :: Batch 0/1
Batch Loss = 0.0007309484141822329
1664, epoch_train_loss=0.0007309484141822329
Epoch 1665
Epoch 1665 :: Batch 0/1
Batch Loss = 0.0007309484141822289
1665, epoch_train_loss=0.0007309484141822289
Epoch 1666
Epoch 1666 :: Batch 0/1
Batch Loss = 0.0007309484141822251
1666, epoch_train_loss=0.0007309484141822251
Epoch 1667
Epoch 1667 :: Batch 0/1
Batch Loss = 0.0007309484141822213
1667, epoch_train_loss=0.0007309484141822213
Epoch 1668
Epoch 1668 :: Batch 0/1
Batch Loss = 0.0007309484141822173
1668, epoch_train_loss=0.0007309484141822173
Epoch 1669
Epoch 1669 :: Batch 0/1
Batch Loss = 0.0007309484141822134
1669, epoch_train_loss=0.0007309484141822134
Epoch 1670
Epoch 1670 :: Batch 0/1
Batch Loss = 0.0007309484141822095
1670, epoch_train_loss=0.0007309484141822095
Epoch 1671
Epoch 1671 :: Batch 0/1
Batch Loss = 0.0007309484141822057
1671, epoch_train_loss=0.0007309484141822057
Epoch 1672
Epoch 1672 :: Batch 0/1
Batch Loss = 0.0007309484141822016
1672, epoch_train_loss=0.0007309484141822016
Epoch 1673
Epoch 1673 :: Batch 0/1
Batch Loss = 0.0007309484141821978
1673, epoch_train_loss=0.0007309484141821978
Epoch 1674
Epoch 1674 :: Batch 0/1
Batch Loss = 0.0007309484141821938
1674, epoch_train_loss=0.0007309484141821938
Epoch 1675
Epoch 1675 :: Batch 0/1
Batch Loss = 0.00073094841418219
1675, epoch_train_loss=0.00073094841418219
Epoch 1676
Epoch 1676 :: Batch 0/1
Batch Loss = 0.000730948414182186
1676, epoch_train_loss=0.000730948414182186
Epoch 1677
Epoch 1677 :: Batch 0/1
Batch Loss = 0.0007309484141821822
1677, epoch_train_loss=0.0007309484141821822
Epoch 1678
Epoch 1678 :: Batch 0/1
Batch Loss = 0.0007309484141821781
1678, epoch_train_loss=0.0007309484141821781
Epoch 1679
Epoch 1679 :: Batch 0/1
Batch Loss = 0.0007309484141821743
1679, epoch_train_loss=0.0007309484141821743
Epoch 1680
Epoch 1680 :: Batch 0/1
Batch Loss = 0.0007309484141821704
1680, epoch_train_loss=0.0007309484141821704
Epoch 1681
Epoch 1681 :: Batch 0/1
Batch Loss = 0.0007309484141821664
1681, epoch_train_loss=0.0007309484141821664
Epoch 1682
Epoch 1682 :: Batch 0/1
Batch Loss = 0.0007309484141821625
1682, epoch_train_loss=0.0007309484141821625
Epoch 1683
Epoch 1683 :: Batch 0/1
Batch Loss = 0.0007309484141821584
1683, epoch_train_loss=0.0007309484141821584
Epoch 1684
Epoch 1684 :: Batch 0/1
Batch Loss = 0.0007309484141821545
1684, epoch_train_loss=0.0007309484141821545
Epoch 1685
Epoch 1685 :: Batch 0/1
Batch Loss = 0.0007309484141821505
1685, epoch_train_loss=0.0007309484141821505
Epoch 1686
Epoch 1686 :: Batch 0/1
Batch Loss = 0.0007309484141821467
1686, epoch_train_loss=0.0007309484141821467
Epoch 1687
Epoch 1687 :: Batch 0/1
Batch Loss = 0.0007309484141821426
1687, epoch_train_loss=0.0007309484141821426
Epoch 1688
Epoch 1688 :: Batch 0/1
Batch Loss = 0.0007309484141821388
1688, epoch_train_loss=0.0007309484141821388
Epoch 1689
Epoch 1689 :: Batch 0/1
Batch Loss = 0.0007309484141821348
1689, epoch_train_loss=0.0007309484141821348
Epoch 1690
Epoch 1690 :: Batch 0/1
Batch Loss = 0.0007309484141821309
1690, epoch_train_loss=0.0007309484141821309
Epoch 1691
Epoch 1691 :: Batch 0/1
Batch Loss = 0.0007309484141821268
1691, epoch_train_loss=0.0007309484141821268
Epoch 1692
Epoch 1692 :: Batch 0/1
Batch Loss = 0.000730948414182123
1692, epoch_train_loss=0.000730948414182123
Epoch 1693
Epoch 1693 :: Batch 0/1
Batch Loss = 0.0007309484141821189
1693, epoch_train_loss=0.0007309484141821189
Epoch 1694
Epoch 1694 :: Batch 0/1
Batch Loss = 0.0007309484141821151
1694, epoch_train_loss=0.0007309484141821151
Epoch 1695
Epoch 1695 :: Batch 0/1
Batch Loss = 0.000730948414182111
1695, epoch_train_loss=0.000730948414182111
Epoch 1696
Epoch 1696 :: Batch 0/1
Batch Loss = 0.0007309484141821072
1696, epoch_train_loss=0.0007309484141821072
Epoch 1697
Epoch 1697 :: Batch 0/1
Batch Loss = 0.000730948414182103
1697, epoch_train_loss=0.000730948414182103
Epoch 1698
Epoch 1698 :: Batch 0/1
Batch Loss = 0.0007309484141820992
1698, epoch_train_loss=0.0007309484141820992
Epoch 1699
Epoch 1699 :: Batch 0/1
Batch Loss = 0.0007309484141820951
1699, epoch_train_loss=0.0007309484141820951
Epoch 1700
Epoch 1700 :: Batch 0/1
Batch Loss = 0.000730948414182091
1700, epoch_train_loss=0.000730948414182091
Epoch 1701
Epoch 1701 :: Batch 0/1
Batch Loss = 0.0007309484141820872
1701, epoch_train_loss=0.0007309484141820872
Epoch 1702
Epoch 1702 :: Batch 0/1
Batch Loss = 0.0007309484141820831
1702, epoch_train_loss=0.0007309484141820831
Epoch 1703
Epoch 1703 :: Batch 0/1
Batch Loss = 0.0007309484141820793
1703, epoch_train_loss=0.0007309484141820793
Epoch 1704
Epoch 1704 :: Batch 0/1
Batch Loss = 0.0007309484141820752
1704, epoch_train_loss=0.0007309484141820752
Epoch 1705
Epoch 1705 :: Batch 0/1
Batch Loss = 0.0007309484141820712
1705, epoch_train_loss=0.0007309484141820712
Epoch 1706
Epoch 1706 :: Batch 0/1
Batch Loss = 0.0007309484141820672
1706, epoch_train_loss=0.0007309484141820672
Epoch 1707
Epoch 1707 :: Batch 0/1
Batch Loss = 0.0007309484141820632
1707, epoch_train_loss=0.0007309484141820632
Epoch 1708
Epoch 1708 :: Batch 0/1
Batch Loss = 0.0007309484141820591
1708, epoch_train_loss=0.0007309484141820591
Epoch 1709
Epoch 1709 :: Batch 0/1
Batch Loss = 0.0007309484141820553
1709, epoch_train_loss=0.0007309484141820553
Epoch 1710
Epoch 1710 :: Batch 0/1
Batch Loss = 0.0007309484141820512
1710, epoch_train_loss=0.0007309484141820512
Epoch 1711
Epoch 1711 :: Batch 0/1
Batch Loss = 0.0007309484141820472
1711, epoch_train_loss=0.0007309484141820472
Epoch 1712
Epoch 1712 :: Batch 0/1
Batch Loss = 0.0007309484141820432
1712, epoch_train_loss=0.0007309484141820432
Epoch 1713
Epoch 1713 :: Batch 0/1
Batch Loss = 0.0007309484141820393
1713, epoch_train_loss=0.0007309484141820393
Epoch 1714
Epoch 1714 :: Batch 0/1
Batch Loss = 0.0007309484141820352
1714, epoch_train_loss=0.0007309484141820352
Epoch 1715
Epoch 1715 :: Batch 0/1
Batch Loss = 0.000730948414182031
1715, epoch_train_loss=0.000730948414182031
Epoch 1716
Epoch 1716 :: Batch 0/1
Batch Loss = 0.0007309484141820273
1716, epoch_train_loss=0.0007309484141820273
Epoch 1717
Epoch 1717 :: Batch 0/1
Batch Loss = 0.0007309484141820231
1717, epoch_train_loss=0.0007309484141820231
Epoch 1718
Epoch 1718 :: Batch 0/1
Batch Loss = 0.0007309484141820191
1718, epoch_train_loss=0.0007309484141820191
Epoch 1719
Epoch 1719 :: Batch 0/1
Batch Loss = 0.000730948414182015
1719, epoch_train_loss=0.000730948414182015
Epoch 1720
Epoch 1720 :: Batch 0/1
Batch Loss = 0.0007309484141820112
1720, epoch_train_loss=0.0007309484141820112
Epoch 1721
Epoch 1721 :: Batch 0/1
Batch Loss = 0.0007309484141820071
1721, epoch_train_loss=0.0007309484141820071
Epoch 1722
Epoch 1722 :: Batch 0/1
Batch Loss = 0.0007309484141820031
1722, epoch_train_loss=0.0007309484141820031
Epoch 1723
Epoch 1723 :: Batch 0/1
Batch Loss = 0.000730948414181999
1723, epoch_train_loss=0.000730948414181999
Epoch 1724
Epoch 1724 :: Batch 0/1
Batch Loss = 0.0007309484141819948
1724, epoch_train_loss=0.0007309484141819948
Epoch 1725
Epoch 1725 :: Batch 0/1
Batch Loss = 0.000730948414181991
1725, epoch_train_loss=0.000730948414181991
Epoch 1726
Epoch 1726 :: Batch 0/1
Batch Loss = 0.0007309484141819869
1726, epoch_train_loss=0.0007309484141819869
Epoch 1727
Epoch 1727 :: Batch 0/1
Batch Loss = 0.0007309484141819829
1727, epoch_train_loss=0.0007309484141819829
Epoch 1728
Epoch 1728 :: Batch 0/1
Batch Loss = 0.0007309484141819788
1728, epoch_train_loss=0.0007309484141819788
Epoch 1729
Epoch 1729 :: Batch 0/1
Batch Loss = 0.0007309484141819748
1729, epoch_train_loss=0.0007309484141819748
Epoch 1730
Epoch 1730 :: Batch 0/1
Batch Loss = 0.0007309484141819708
1730, epoch_train_loss=0.0007309484141819708
Epoch 1731
Epoch 1731 :: Batch 0/1
Batch Loss = 0.0007309484141819668
1731, epoch_train_loss=0.0007309484141819668
Epoch 1732
Epoch 1732 :: Batch 0/1
Batch Loss = 0.0007309484141819627
1732, epoch_train_loss=0.0007309484141819627
Epoch 1733
Epoch 1733 :: Batch 0/1
Batch Loss = 0.0007309484141819586
1733, epoch_train_loss=0.0007309484141819586
Epoch 1734
Epoch 1734 :: Batch 0/1
Batch Loss = 0.0007309484141819546
1734, epoch_train_loss=0.0007309484141819546
Epoch 1735
Epoch 1735 :: Batch 0/1
Batch Loss = 0.0007309484141819505
1735, epoch_train_loss=0.0007309484141819505
Epoch 1736
Epoch 1736 :: Batch 0/1
Batch Loss = 0.0007309484141819465
1736, epoch_train_loss=0.0007309484141819465
Epoch 1737
Epoch 1737 :: Batch 0/1
Batch Loss = 0.0007309484141819424
1737, epoch_train_loss=0.0007309484141819424
Epoch 1738
Epoch 1738 :: Batch 0/1
Batch Loss = 0.0007309484141819382
1738, epoch_train_loss=0.0007309484141819382
Epoch 1739
Epoch 1739 :: Batch 0/1
Batch Loss = 0.0007309484141819342
1739, epoch_train_loss=0.0007309484141819342
Epoch 1740
Epoch 1740 :: Batch 0/1
Batch Loss = 0.0007309484141819301
1740, epoch_train_loss=0.0007309484141819301
Epoch 1741
Epoch 1741 :: Batch 0/1
Batch Loss = 0.0007309484141819261
1741, epoch_train_loss=0.0007309484141819261
Epoch 1742
Epoch 1742 :: Batch 0/1
Batch Loss = 0.000730948414181922
1742, epoch_train_loss=0.000730948414181922
Epoch 1743
Epoch 1743 :: Batch 0/1
Batch Loss = 0.0007309484141819179
1743, epoch_train_loss=0.0007309484141819179
Epoch 1744
Epoch 1744 :: Batch 0/1
Batch Loss = 0.0007309484141819138
1744, epoch_train_loss=0.0007309484141819138
Epoch 1745
Epoch 1745 :: Batch 0/1
Batch Loss = 0.0007309484141819097
1745, epoch_train_loss=0.0007309484141819097
Epoch 1746
Epoch 1746 :: Batch 0/1
Batch Loss = 0.0007309484141819057
1746, epoch_train_loss=0.0007309484141819057
Epoch 1747
Epoch 1747 :: Batch 0/1
Batch Loss = 0.0007309484141819016
1747, epoch_train_loss=0.0007309484141819016
Epoch 1748
Epoch 1748 :: Batch 0/1
Batch Loss = 0.0007309484141818975
1748, epoch_train_loss=0.0007309484141818975
Epoch 1749
Epoch 1749 :: Batch 0/1
Batch Loss = 0.0007309484141818935
1749, epoch_train_loss=0.0007309484141818935
Epoch 1750
Epoch 1750 :: Batch 0/1
Batch Loss = 0.0007309484141818893
1750, epoch_train_loss=0.0007309484141818893
Epoch 1751
Epoch 1751 :: Batch 0/1
Batch Loss = 0.0007309484141818853
1751, epoch_train_loss=0.0007309484141818853
Epoch 1752
Epoch 1752 :: Batch 0/1
Batch Loss = 0.0007309484141818812
1752, epoch_train_loss=0.0007309484141818812
Epoch 1753
Epoch 1753 :: Batch 0/1
Batch Loss = 0.0007309484141818771
1753, epoch_train_loss=0.0007309484141818771
Epoch 1754
Epoch 1754 :: Batch 0/1
Batch Loss = 0.0007309484141818731
1754, epoch_train_loss=0.0007309484141818731
Epoch 1755
Epoch 1755 :: Batch 0/1
Batch Loss = 0.0007309484141818689
1755, epoch_train_loss=0.0007309484141818689
Epoch 1756
Epoch 1756 :: Batch 0/1
Batch Loss = 0.0007309484141818646
1756, epoch_train_loss=0.0007309484141818646
Epoch 1757
Epoch 1757 :: Batch 0/1
Batch Loss = 0.0007309484141818607
1757, epoch_train_loss=0.0007309484141818607
Epoch 1758
Epoch 1758 :: Batch 0/1
Batch Loss = 0.0007309484141818565
1758, epoch_train_loss=0.0007309484141818565
Epoch 1759
Epoch 1759 :: Batch 0/1
Batch Loss = 0.0007309484141818525
1759, epoch_train_loss=0.0007309484141818525
Epoch 1760
Epoch 1760 :: Batch 0/1
Batch Loss = 0.0007309484141818484
1760, epoch_train_loss=0.0007309484141818484
Epoch 1761
Epoch 1761 :: Batch 0/1
Batch Loss = 0.0007309484141818441
1761, epoch_train_loss=0.0007309484141818441
Epoch 1762
Epoch 1762 :: Batch 0/1
Batch Loss = 0.0007309484141818401
1762, epoch_train_loss=0.0007309484141818401
Epoch 1763
Epoch 1763 :: Batch 0/1
Batch Loss = 0.0007309484141818359
1763, epoch_train_loss=0.0007309484141818359
Epoch 1764
Epoch 1764 :: Batch 0/1
Batch Loss = 0.0007309484141818318
1764, epoch_train_loss=0.0007309484141818318
Epoch 1765
Epoch 1765 :: Batch 0/1
Batch Loss = 0.0007309484141818278
1765, epoch_train_loss=0.0007309484141818278
Epoch 1766
Epoch 1766 :: Batch 0/1
Batch Loss = 0.0007309484141818236
1766, epoch_train_loss=0.0007309484141818236
Epoch 1767
Epoch 1767 :: Batch 0/1
Batch Loss = 0.0007309484141818194
1767, epoch_train_loss=0.0007309484141818194
Epoch 1768
Epoch 1768 :: Batch 0/1
Batch Loss = 0.0007309484141818153
1768, epoch_train_loss=0.0007309484141818153
Epoch 1769
Epoch 1769 :: Batch 0/1
Batch Loss = 0.0007309484141818112
1769, epoch_train_loss=0.0007309484141818112
Epoch 1770
Epoch 1770 :: Batch 0/1
Batch Loss = 0.0007309484141818069
1770, epoch_train_loss=0.0007309484141818069
Epoch 1771
Epoch 1771 :: Batch 0/1
Batch Loss = 0.0007309484141818029
1771, epoch_train_loss=0.0007309484141818029
Epoch 1772
Epoch 1772 :: Batch 0/1
Batch Loss = 0.0007309484141817988
1772, epoch_train_loss=0.0007309484141817988
Epoch 1773
Epoch 1773 :: Batch 0/1
Batch Loss = 0.0007309484141817946
1773, epoch_train_loss=0.0007309484141817946
Epoch 1774
Epoch 1774 :: Batch 0/1
Batch Loss = 0.0007309484141817906
1774, epoch_train_loss=0.0007309484141817906
Epoch 1775
Epoch 1775 :: Batch 0/1
Batch Loss = 0.0007309484141817862
1775, epoch_train_loss=0.0007309484141817862
Epoch 1776
Epoch 1776 :: Batch 0/1
Batch Loss = 0.0007309484141817821
1776, epoch_train_loss=0.0007309484141817821
Epoch 1777
Epoch 1777 :: Batch 0/1
Batch Loss = 0.0007309484141817781
1777, epoch_train_loss=0.0007309484141817781
Epoch 1778
Epoch 1778 :: Batch 0/1
Batch Loss = 0.0007309484141817739
1778, epoch_train_loss=0.0007309484141817739
Epoch 1779
Epoch 1779 :: Batch 0/1
Batch Loss = 0.0007309484141817696
1779, epoch_train_loss=0.0007309484141817696
Epoch 1780
Epoch 1780 :: Batch 0/1
Batch Loss = 0.0007309484141817656
1780, epoch_train_loss=0.0007309484141817656
Epoch 1781
Epoch 1781 :: Batch 0/1
Batch Loss = 0.0007309484141817614
1781, epoch_train_loss=0.0007309484141817614
Epoch 1782
Epoch 1782 :: Batch 0/1
Batch Loss = 0.0007309484141817572
1782, epoch_train_loss=0.0007309484141817572
Epoch 1783
Epoch 1783 :: Batch 0/1
Batch Loss = 0.0007309484141817531
1783, epoch_train_loss=0.0007309484141817531
Epoch 1784
Epoch 1784 :: Batch 0/1
Batch Loss = 0.0007309484141817488
1784, epoch_train_loss=0.0007309484141817488
Epoch 1785
Epoch 1785 :: Batch 0/1
Batch Loss = 0.0007309484141817447
1785, epoch_train_loss=0.0007309484141817447
Epoch 1786
Epoch 1786 :: Batch 0/1
Batch Loss = 0.0007309484141817405
1786, epoch_train_loss=0.0007309484141817405
Epoch 1787
Epoch 1787 :: Batch 0/1
Batch Loss = 0.0007309484141817364
1787, epoch_train_loss=0.0007309484141817364
Epoch 1788
Epoch 1788 :: Batch 0/1
Batch Loss = 0.0007309484141817321
1788, epoch_train_loss=0.0007309484141817321
Epoch 1789
Epoch 1789 :: Batch 0/1
Batch Loss = 0.0007309484141817279
1789, epoch_train_loss=0.0007309484141817279
Epoch 1790
Epoch 1790 :: Batch 0/1
Batch Loss = 0.0007309484141817239
1790, epoch_train_loss=0.0007309484141817239
Epoch 1791
Epoch 1791 :: Batch 0/1
Batch Loss = 0.0007309484141817196
1791, epoch_train_loss=0.0007309484141817196
Epoch 1792
Epoch 1792 :: Batch 0/1
Batch Loss = 0.0007309484141817153
1792, epoch_train_loss=0.0007309484141817153
Epoch 1793
Epoch 1793 :: Batch 0/1
Batch Loss = 0.0007309484141817113
1793, epoch_train_loss=0.0007309484141817113
Epoch 1794
Epoch 1794 :: Batch 0/1
Batch Loss = 0.0007309484141817071
1794, epoch_train_loss=0.0007309484141817071
Epoch 1795
Epoch 1795 :: Batch 0/1
Batch Loss = 0.0007309484141817028
1795, epoch_train_loss=0.0007309484141817028
Epoch 1796
Epoch 1796 :: Batch 0/1
Batch Loss = 0.0007309484141816986
1796, epoch_train_loss=0.0007309484141816986
Epoch 1797
Epoch 1797 :: Batch 0/1
Batch Loss = 0.0007309484141816944
1797, epoch_train_loss=0.0007309484141816944
Epoch 1798
Epoch 1798 :: Batch 0/1
Batch Loss = 0.0007309484141816903
1798, epoch_train_loss=0.0007309484141816903
Epoch 1799
Epoch 1799 :: Batch 0/1
Batch Loss = 0.000730948414181686
1799, epoch_train_loss=0.000730948414181686
Epoch 1800
Epoch 1800 :: Batch 0/1
Batch Loss = 0.0007309484141816818
1800, epoch_train_loss=0.0007309484141816818
Epoch 1801
Epoch 1801 :: Batch 0/1
Batch Loss = 0.0007309484141816776
1801, epoch_train_loss=0.0007309484141816776
Epoch 1802
Epoch 1802 :: Batch 0/1
Batch Loss = 0.0007309484141816734
1802, epoch_train_loss=0.0007309484141816734
Epoch 1803
Epoch 1803 :: Batch 0/1
Batch Loss = 0.0007309484141816692
1803, epoch_train_loss=0.0007309484141816692
Epoch 1804
Epoch 1804 :: Batch 0/1
Batch Loss = 0.0007309484141816649
1804, epoch_train_loss=0.0007309484141816649
Epoch 1805
Epoch 1805 :: Batch 0/1
Batch Loss = 0.0007309484141816607
1805, epoch_train_loss=0.0007309484141816607
Epoch 1806
Epoch 1806 :: Batch 0/1
Batch Loss = 0.0007309484141816565
1806, epoch_train_loss=0.0007309484141816565
Epoch 1807
Epoch 1807 :: Batch 0/1
Batch Loss = 0.0007309484141816523
1807, epoch_train_loss=0.0007309484141816523
Epoch 1808
Epoch 1808 :: Batch 0/1
Batch Loss = 0.0007309484141816481
1808, epoch_train_loss=0.0007309484141816481
Epoch 1809
Epoch 1809 :: Batch 0/1
Batch Loss = 0.0007309484141816438
1809, epoch_train_loss=0.0007309484141816438
Epoch 1810
Epoch 1810 :: Batch 0/1
Batch Loss = 0.0007309484141816397
1810, epoch_train_loss=0.0007309484141816397
Epoch 1811
Epoch 1811 :: Batch 0/1
Batch Loss = 0.0007309484141816354
1811, epoch_train_loss=0.0007309484141816354
Epoch 1812
Epoch 1812 :: Batch 0/1
Batch Loss = 0.0007309484141816311
1812, epoch_train_loss=0.0007309484141816311
Epoch 1813
Epoch 1813 :: Batch 0/1
Batch Loss = 0.000730948414181627
1813, epoch_train_loss=0.000730948414181627
Epoch 1814
Epoch 1814 :: Batch 0/1
Batch Loss = 0.0007309484141816226
1814, epoch_train_loss=0.0007309484141816226
Epoch 1815
Epoch 1815 :: Batch 0/1
Batch Loss = 0.0007309484141816184
1815, epoch_train_loss=0.0007309484141816184
Epoch 1816
Epoch 1816 :: Batch 0/1
Batch Loss = 0.0007309484141816142
1816, epoch_train_loss=0.0007309484141816142
Epoch 1817
Epoch 1817 :: Batch 0/1
Batch Loss = 0.0007309484141816099
1817, epoch_train_loss=0.0007309484141816099
Epoch 1818
Epoch 1818 :: Batch 0/1
Batch Loss = 0.0007309484141816056
1818, epoch_train_loss=0.0007309484141816056
Epoch 1819
Epoch 1819 :: Batch 0/1
Batch Loss = 0.0007309484141816015
1819, epoch_train_loss=0.0007309484141816015
Epoch 1820
Epoch 1820 :: Batch 0/1
Batch Loss = 0.0007309484141815971
1820, epoch_train_loss=0.0007309484141815971
Epoch 1821
Epoch 1821 :: Batch 0/1
Batch Loss = 0.0007309484141815928
1821, epoch_train_loss=0.0007309484141815928
Epoch 1822
Epoch 1822 :: Batch 0/1
Batch Loss = 0.0007309484141815886
1822, epoch_train_loss=0.0007309484141815886
Epoch 1823
Epoch 1823 :: Batch 0/1
Batch Loss = 0.0007309484141815844
1823, epoch_train_loss=0.0007309484141815844
Epoch 1824
Epoch 1824 :: Batch 0/1
Batch Loss = 0.00073094841418158
1824, epoch_train_loss=0.00073094841418158
Epoch 1825
Epoch 1825 :: Batch 0/1
Batch Loss = 0.0007309484141815759
1825, epoch_train_loss=0.0007309484141815759
Epoch 1826
Epoch 1826 :: Batch 0/1
Batch Loss = 0.0007309484141815716
1826, epoch_train_loss=0.0007309484141815716
Epoch 1827
Epoch 1827 :: Batch 0/1
Batch Loss = 0.0007309484141815672
1827, epoch_train_loss=0.0007309484141815672
Epoch 1828
Epoch 1828 :: Batch 0/1
Batch Loss = 0.0007309484141815631
1828, epoch_train_loss=0.0007309484141815631
Epoch 1829
Epoch 1829 :: Batch 0/1
Batch Loss = 0.0007309484141815588
1829, epoch_train_loss=0.0007309484141815588
Epoch 1830
Epoch 1830 :: Batch 0/1
Batch Loss = 0.0007309484141815545
1830, epoch_train_loss=0.0007309484141815545
Epoch 1831
Epoch 1831 :: Batch 0/1
Batch Loss = 0.0007309484141815502
1831, epoch_train_loss=0.0007309484141815502
Epoch 1832
Epoch 1832 :: Batch 0/1
Batch Loss = 0.000730948414181546
1832, epoch_train_loss=0.000730948414181546
Epoch 1833
Epoch 1833 :: Batch 0/1
Batch Loss = 0.0007309484141815416
1833, epoch_train_loss=0.0007309484141815416
Epoch 1834
Epoch 1834 :: Batch 0/1
Batch Loss = 0.0007309484141815373
1834, epoch_train_loss=0.0007309484141815373
Epoch 1835
Epoch 1835 :: Batch 0/1
Batch Loss = 0.0007309484141815331
1835, epoch_train_loss=0.0007309484141815331
Epoch 1836
Epoch 1836 :: Batch 0/1
Batch Loss = 0.0007309484141815287
1836, epoch_train_loss=0.0007309484141815287
Epoch 1837
Epoch 1837 :: Batch 0/1
Batch Loss = 0.0007309484141815245
1837, epoch_train_loss=0.0007309484141815245
Epoch 1838
Epoch 1838 :: Batch 0/1
Batch Loss = 0.0007309484141815202
1838, epoch_train_loss=0.0007309484141815202
Epoch 1839
Epoch 1839 :: Batch 0/1
Batch Loss = 0.0007309484141815158
1839, epoch_train_loss=0.0007309484141815158
Epoch 1840
Epoch 1840 :: Batch 0/1
Batch Loss = 0.0007309484141815115
1840, epoch_train_loss=0.0007309484141815115
Epoch 1841
Epoch 1841 :: Batch 0/1
Batch Loss = 0.0007309484141815073
1841, epoch_train_loss=0.0007309484141815073
Epoch 1842
Epoch 1842 :: Batch 0/1
Batch Loss = 0.000730948414181503
1842, epoch_train_loss=0.000730948414181503
Epoch 1843
Epoch 1843 :: Batch 0/1
Batch Loss = 0.0007309484141814987
1843, epoch_train_loss=0.0007309484141814987
Epoch 1844
Epoch 1844 :: Batch 0/1
Batch Loss = 0.0007309484141814944
1844, epoch_train_loss=0.0007309484141814944
Epoch 1845
Epoch 1845 :: Batch 0/1
Batch Loss = 0.00073094841418149
1845, epoch_train_loss=0.00073094841418149
Epoch 1846
Epoch 1846 :: Batch 0/1
Batch Loss = 0.0007309484141814857
1846, epoch_train_loss=0.0007309484141814857
Epoch 1847
Epoch 1847 :: Batch 0/1
Batch Loss = 0.0007309484141814815
1847, epoch_train_loss=0.0007309484141814815
Epoch 1848
Epoch 1848 :: Batch 0/1
Batch Loss = 0.0007309484141814771
1848, epoch_train_loss=0.0007309484141814771
Epoch 1849
Epoch 1849 :: Batch 0/1
Batch Loss = 0.0007309484141814728
1849, epoch_train_loss=0.0007309484141814728
Epoch 1850
Epoch 1850 :: Batch 0/1
Batch Loss = 0.0007309484141814685
1850, epoch_train_loss=0.0007309484141814685
Epoch 1851
Epoch 1851 :: Batch 0/1
Batch Loss = 0.0007309484141814641
1851, epoch_train_loss=0.0007309484141814641
Epoch 1852
Epoch 1852 :: Batch 0/1
Batch Loss = 0.0007309484141814598
1852, epoch_train_loss=0.0007309484141814598
Epoch 1853
Epoch 1853 :: Batch 0/1
Batch Loss = 0.0007309484141814556
1853, epoch_train_loss=0.0007309484141814556
Epoch 1854
Epoch 1854 :: Batch 0/1
Batch Loss = 0.0007309484141814511
1854, epoch_train_loss=0.0007309484141814511
Epoch 1855
Epoch 1855 :: Batch 0/1
Batch Loss = 0.0007309484141814468
1855, epoch_train_loss=0.0007309484141814468
Epoch 1856
Epoch 1856 :: Batch 0/1
Batch Loss = 0.0007309484141814424
1856, epoch_train_loss=0.0007309484141814424
Epoch 1857
Epoch 1857 :: Batch 0/1
Batch Loss = 0.000730948414181438
1857, epoch_train_loss=0.000730948414181438
Epoch 1858
Epoch 1858 :: Batch 0/1
Batch Loss = 0.0007309484141814337
1858, epoch_train_loss=0.0007309484141814337
Epoch 1859
Epoch 1859 :: Batch 0/1
Batch Loss = 0.0007309484141814294
1859, epoch_train_loss=0.0007309484141814294
Epoch 1860
Epoch 1860 :: Batch 0/1
Batch Loss = 0.0007309484141814251
1860, epoch_train_loss=0.0007309484141814251
Epoch 1861
Epoch 1861 :: Batch 0/1
Batch Loss = 0.0007309484141814208
1861, epoch_train_loss=0.0007309484141814208
Epoch 1862
Epoch 1862 :: Batch 0/1
Batch Loss = 0.0007309484141814164
1862, epoch_train_loss=0.0007309484141814164
Epoch 1863
Epoch 1863 :: Batch 0/1
Batch Loss = 0.0007309484141814119
1863, epoch_train_loss=0.0007309484141814119
Epoch 1864
Epoch 1864 :: Batch 0/1
Batch Loss = 0.0007309484141814076
1864, epoch_train_loss=0.0007309484141814076
Epoch 1865
Epoch 1865 :: Batch 0/1
Batch Loss = 0.0007309484141814032
1865, epoch_train_loss=0.0007309484141814032
Epoch 1866
Epoch 1866 :: Batch 0/1
Batch Loss = 0.000730948414181399
1866, epoch_train_loss=0.000730948414181399
Epoch 1867
Epoch 1867 :: Batch 0/1
Batch Loss = 0.0007309484141813946
1867, epoch_train_loss=0.0007309484141813946
Epoch 1868
Epoch 1868 :: Batch 0/1
Batch Loss = 0.0007309484141813902
1868, epoch_train_loss=0.0007309484141813902
Epoch 1869
Epoch 1869 :: Batch 0/1
Batch Loss = 0.0007309484141813857
1869, epoch_train_loss=0.0007309484141813857
Epoch 1870
Epoch 1870 :: Batch 0/1
Batch Loss = 0.0007309484141813814
1870, epoch_train_loss=0.0007309484141813814
Epoch 1871
Epoch 1871 :: Batch 0/1
Batch Loss = 0.0007309484141813771
1871, epoch_train_loss=0.0007309484141813771
Epoch 1872
Epoch 1872 :: Batch 0/1
Batch Loss = 0.0007309484141813728
1872, epoch_train_loss=0.0007309484141813728
Epoch 1873
Epoch 1873 :: Batch 0/1
Batch Loss = 0.0007309484141813682
1873, epoch_train_loss=0.0007309484141813682
Epoch 1874
Epoch 1874 :: Batch 0/1
Batch Loss = 0.0007309484141813639
1874, epoch_train_loss=0.0007309484141813639
Epoch 1875
Epoch 1875 :: Batch 0/1
Batch Loss = 0.0007309484141813596
1875, epoch_train_loss=0.0007309484141813596
Epoch 1876
Epoch 1876 :: Batch 0/1
Batch Loss = 0.0007309484141813552
1876, epoch_train_loss=0.0007309484141813552
Epoch 1877
Epoch 1877 :: Batch 0/1
Batch Loss = 0.0007309484141813507
1877, epoch_train_loss=0.0007309484141813507
Epoch 1878
Epoch 1878 :: Batch 0/1
Batch Loss = 0.0007309484141813464
1878, epoch_train_loss=0.0007309484141813464
Epoch 1879
Epoch 1879 :: Batch 0/1
Batch Loss = 0.0007309484141813419
1879, epoch_train_loss=0.0007309484141813419
Epoch 1880
Epoch 1880 :: Batch 0/1
Batch Loss = 0.0007309484141813375
1880, epoch_train_loss=0.0007309484141813375
Epoch 1881
Epoch 1881 :: Batch 0/1
Batch Loss = 0.0007309484141813333
1881, epoch_train_loss=0.0007309484141813333
Epoch 1882
Epoch 1882 :: Batch 0/1
Batch Loss = 0.0007309484141813288
1882, epoch_train_loss=0.0007309484141813288
Epoch 1883
Epoch 1883 :: Batch 0/1
Batch Loss = 0.0007309484141813244
1883, epoch_train_loss=0.0007309484141813244
Epoch 1884
Epoch 1884 :: Batch 0/1
Batch Loss = 0.0007309484141813199
1884, epoch_train_loss=0.0007309484141813199
Epoch 1885
Epoch 1885 :: Batch 0/1
Batch Loss = 0.0007309484141813156
1885, epoch_train_loss=0.0007309484141813156
Epoch 1886
Epoch 1886 :: Batch 0/1
Batch Loss = 0.0007309484141813111
1886, epoch_train_loss=0.0007309484141813111
Epoch 1887
Epoch 1887 :: Batch 0/1
Batch Loss = 0.0007309484141813067
1887, epoch_train_loss=0.0007309484141813067
Epoch 1888
Epoch 1888 :: Batch 0/1
Batch Loss = 0.0007309484141813022
1888, epoch_train_loss=0.0007309484141813022
Epoch 1889
Epoch 1889 :: Batch 0/1
Batch Loss = 0.0007309484141812979
1889, epoch_train_loss=0.0007309484141812979
Epoch 1890
Epoch 1890 :: Batch 0/1
Batch Loss = 0.0007309484141812934
1890, epoch_train_loss=0.0007309484141812934
Epoch 1891
Epoch 1891 :: Batch 0/1
Batch Loss = 0.0007309484141812891
1891, epoch_train_loss=0.0007309484141812891
Epoch 1892
Epoch 1892 :: Batch 0/1
Batch Loss = 0.0007309484141812845
1892, epoch_train_loss=0.0007309484141812845
Epoch 1893
Epoch 1893 :: Batch 0/1
Batch Loss = 0.0007309484141812802
1893, epoch_train_loss=0.0007309484141812802
Epoch 1894
Epoch 1894 :: Batch 0/1
Batch Loss = 0.0007309484141812757
1894, epoch_train_loss=0.0007309484141812757
Epoch 1895
Epoch 1895 :: Batch 0/1
Batch Loss = 0.0007309484141812713
1895, epoch_train_loss=0.0007309484141812713
Epoch 1896
Epoch 1896 :: Batch 0/1
Batch Loss = 0.0007309484141812669
1896, epoch_train_loss=0.0007309484141812669
Epoch 1897
Epoch 1897 :: Batch 0/1
Batch Loss = 0.0007309484141812625
1897, epoch_train_loss=0.0007309484141812625
Epoch 1898
Epoch 1898 :: Batch 0/1
Batch Loss = 0.0007309484141812579
1898, epoch_train_loss=0.0007309484141812579
Epoch 1899
Epoch 1899 :: Batch 0/1
Batch Loss = 0.0007309484141812536
1899, epoch_train_loss=0.0007309484141812536
Epoch 1900
Epoch 1900 :: Batch 0/1
Batch Loss = 0.000730948414181249
1900, epoch_train_loss=0.000730948414181249
Epoch 1901
Epoch 1901 :: Batch 0/1
Batch Loss = 0.0007309484141812447
1901, epoch_train_loss=0.0007309484141812447
Epoch 1902
Epoch 1902 :: Batch 0/1
Batch Loss = 0.0007309484141812401
1902, epoch_train_loss=0.0007309484141812401
Epoch 1903
Epoch 1903 :: Batch 0/1
Batch Loss = 0.0007309484141812358
1903, epoch_train_loss=0.0007309484141812358
Epoch 1904
Epoch 1904 :: Batch 0/1
Batch Loss = 0.0007309484141812312
1904, epoch_train_loss=0.0007309484141812312
Epoch 1905
Epoch 1905 :: Batch 0/1
Batch Loss = 0.0007309484141812267
1905, epoch_train_loss=0.0007309484141812267
Epoch 1906
Epoch 1906 :: Batch 0/1
Batch Loss = 0.0007309484141812224
1906, epoch_train_loss=0.0007309484141812224
Epoch 1907
Epoch 1907 :: Batch 0/1
Batch Loss = 0.0007309484141812179
1907, epoch_train_loss=0.0007309484141812179
Epoch 1908
Epoch 1908 :: Batch 0/1
Batch Loss = 0.0007309484141812136
1908, epoch_train_loss=0.0007309484141812136
Epoch 1909
Epoch 1909 :: Batch 0/1
Batch Loss = 0.000730948414181209
1909, epoch_train_loss=0.000730948414181209
Epoch 1910
Epoch 1910 :: Batch 0/1
Batch Loss = 0.0007309484141812045
1910, epoch_train_loss=0.0007309484141812045
Epoch 1911
Epoch 1911 :: Batch 0/1
Batch Loss = 0.0007309484141812001
1911, epoch_train_loss=0.0007309484141812001
Epoch 1912
Epoch 1912 :: Batch 0/1
Batch Loss = 0.0007309484141811956
1912, epoch_train_loss=0.0007309484141811956
Epoch 1913
Epoch 1913 :: Batch 0/1
Batch Loss = 0.0007309484141811911
1913, epoch_train_loss=0.0007309484141811911
Epoch 1914
Epoch 1914 :: Batch 0/1
Batch Loss = 0.0007309484141811867
1914, epoch_train_loss=0.0007309484141811867
Epoch 1915
Epoch 1915 :: Batch 0/1
Batch Loss = 0.0007309484141811821
1915, epoch_train_loss=0.0007309484141811821
Epoch 1916
Epoch 1916 :: Batch 0/1
Batch Loss = 0.0007309484141811777
1916, epoch_train_loss=0.0007309484141811777
Epoch 1917
Epoch 1917 :: Batch 0/1
Batch Loss = 0.0007309484141811731
1917, epoch_train_loss=0.0007309484141811731
Epoch 1918
Epoch 1918 :: Batch 0/1
Batch Loss = 0.0007309484141811687
1918, epoch_train_loss=0.0007309484141811687
Epoch 1919
Epoch 1919 :: Batch 0/1
Batch Loss = 0.0007309484141811641
1919, epoch_train_loss=0.0007309484141811641
Epoch 1920
Epoch 1920 :: Batch 0/1
Batch Loss = 0.0007309484141811597
1920, epoch_train_loss=0.0007309484141811597
Epoch 1921
Epoch 1921 :: Batch 0/1
Batch Loss = 0.0007309484141811551
1921, epoch_train_loss=0.0007309484141811551
Epoch 1922
Epoch 1922 :: Batch 0/1
Batch Loss = 0.0007309484141811507
1922, epoch_train_loss=0.0007309484141811507
Epoch 1923
Epoch 1923 :: Batch 0/1
Batch Loss = 0.0007309484141811461
1923, epoch_train_loss=0.0007309484141811461
Epoch 1924
Epoch 1924 :: Batch 0/1
Batch Loss = 0.0007309484141811417
1924, epoch_train_loss=0.0007309484141811417
Epoch 1925
Epoch 1925 :: Batch 0/1
Batch Loss = 0.0007309484141811371
1925, epoch_train_loss=0.0007309484141811371
Epoch 1926
Epoch 1926 :: Batch 0/1
Batch Loss = 0.0007309484141811327
1926, epoch_train_loss=0.0007309484141811327
Epoch 1927
Epoch 1927 :: Batch 0/1
Batch Loss = 0.0007309484141811281
1927, epoch_train_loss=0.0007309484141811281
Epoch 1928
Epoch 1928 :: Batch 0/1
Batch Loss = 0.0007309484141811236
1928, epoch_train_loss=0.0007309484141811236
Epoch 1929
Epoch 1929 :: Batch 0/1
Batch Loss = 0.0007309484141811191
1929, epoch_train_loss=0.0007309484141811191
Epoch 1930
Epoch 1930 :: Batch 0/1
Batch Loss = 0.0007309484141811146
1930, epoch_train_loss=0.0007309484141811146
Epoch 1931
Epoch 1931 :: Batch 0/1
Batch Loss = 0.00073094841418111
1931, epoch_train_loss=0.00073094841418111
Epoch 1932
Epoch 1932 :: Batch 0/1
Batch Loss = 0.0007309484141811055
1932, epoch_train_loss=0.0007309484141811055
Epoch 1933
Epoch 1933 :: Batch 0/1
Batch Loss = 0.000730948414181101
1933, epoch_train_loss=0.000730948414181101
Epoch 1934
Epoch 1934 :: Batch 0/1
Batch Loss = 0.0007309484141810965
1934, epoch_train_loss=0.0007309484141810965
Epoch 1935
Epoch 1935 :: Batch 0/1
Batch Loss = 0.0007309484141810919
1935, epoch_train_loss=0.0007309484141810919
Epoch 1936
Epoch 1936 :: Batch 0/1
Batch Loss = 0.0007309484141810875
1936, epoch_train_loss=0.0007309484141810875
Epoch 1937
Epoch 1937 :: Batch 0/1
Batch Loss = 0.0007309484141810829
1937, epoch_train_loss=0.0007309484141810829
Epoch 1938
Epoch 1938 :: Batch 0/1
Batch Loss = 0.0007309484141810784
1938, epoch_train_loss=0.0007309484141810784
Epoch 1939
Epoch 1939 :: Batch 0/1
Batch Loss = 0.0007309484141810738
1939, epoch_train_loss=0.0007309484141810738
Epoch 1940
Epoch 1940 :: Batch 0/1
Batch Loss = 0.0007309484141810693
1940, epoch_train_loss=0.0007309484141810693
Epoch 1941
Epoch 1941 :: Batch 0/1
Batch Loss = 0.0007309484141810647
1941, epoch_train_loss=0.0007309484141810647
Epoch 1942
Epoch 1942 :: Batch 0/1
Batch Loss = 0.0007309484141810601
1942, epoch_train_loss=0.0007309484141810601
Epoch 1943
Epoch 1943 :: Batch 0/1
Batch Loss = 0.0007309484141810556
1943, epoch_train_loss=0.0007309484141810556
Epoch 1944
Epoch 1944 :: Batch 0/1
Batch Loss = 0.0007309484141810511
1944, epoch_train_loss=0.0007309484141810511
Epoch 1945
Epoch 1945 :: Batch 0/1
Batch Loss = 0.0007309484141810465
1945, epoch_train_loss=0.0007309484141810465
Epoch 1946
Epoch 1946 :: Batch 0/1
Batch Loss = 0.0007309484141810419
1946, epoch_train_loss=0.0007309484141810419
Epoch 1947
Epoch 1947 :: Batch 0/1
Batch Loss = 0.0007309484141810374
1947, epoch_train_loss=0.0007309484141810374
Epoch 1948
Epoch 1948 :: Batch 0/1
Batch Loss = 0.0007309484141810328
1948, epoch_train_loss=0.0007309484141810328
Epoch 1949
Epoch 1949 :: Batch 0/1
Batch Loss = 0.0007309484141810283
1949, epoch_train_loss=0.0007309484141810283
Epoch 1950
Epoch 1950 :: Batch 0/1
Batch Loss = 0.0007309484141810237
1950, epoch_train_loss=0.0007309484141810237
Epoch 1951
Epoch 1951 :: Batch 0/1
Batch Loss = 0.0007309484141810192
1951, epoch_train_loss=0.0007309484141810192
Epoch 1952
Epoch 1952 :: Batch 0/1
Batch Loss = 0.0007309484141810146
1952, epoch_train_loss=0.0007309484141810146
Epoch 1953
Epoch 1953 :: Batch 0/1
Batch Loss = 0.0007309484141810101
1953, epoch_train_loss=0.0007309484141810101
Epoch 1954
Epoch 1954 :: Batch 0/1
Batch Loss = 0.0007309484141810055
1954, epoch_train_loss=0.0007309484141810055
Epoch 1955
Epoch 1955 :: Batch 0/1
Batch Loss = 0.0007309484141810008
1955, epoch_train_loss=0.0007309484141810008
Epoch 1956
Epoch 1956 :: Batch 0/1
Batch Loss = 0.0007309484141809963
1956, epoch_train_loss=0.0007309484141809963
Epoch 1957
Epoch 1957 :: Batch 0/1
Batch Loss = 0.0007309484141809917
1957, epoch_train_loss=0.0007309484141809917
Epoch 1958
Epoch 1958 :: Batch 0/1
Batch Loss = 0.000730948414180987
1958, epoch_train_loss=0.000730948414180987
Epoch 1959
Epoch 1959 :: Batch 0/1
Batch Loss = 0.0007309484141809826
1959, epoch_train_loss=0.0007309484141809826
Epoch 1960
Epoch 1960 :: Batch 0/1
Batch Loss = 0.0007309484141809779
1960, epoch_train_loss=0.0007309484141809779
Epoch 1961
Epoch 1961 :: Batch 0/1
Batch Loss = 0.0007309484141809733
1961, epoch_train_loss=0.0007309484141809733
Epoch 1962
Epoch 1962 :: Batch 0/1
Batch Loss = 0.0007309484141809686
1962, epoch_train_loss=0.0007309484141809686
Epoch 1963
Epoch 1963 :: Batch 0/1
Batch Loss = 0.0007309484141809641
1963, epoch_train_loss=0.0007309484141809641
Epoch 1964
Epoch 1964 :: Batch 0/1
Batch Loss = 0.0007309484141809595
1964, epoch_train_loss=0.0007309484141809595
Epoch 1965
Epoch 1965 :: Batch 0/1
Batch Loss = 0.0007309484141809549
1965, epoch_train_loss=0.0007309484141809549
Epoch 1966
Epoch 1966 :: Batch 0/1
Batch Loss = 0.0007309484141809503
1966, epoch_train_loss=0.0007309484141809503
Epoch 1967
Epoch 1967 :: Batch 0/1
Batch Loss = 0.0007309484141809457
1967, epoch_train_loss=0.0007309484141809457
Epoch 1968
Epoch 1968 :: Batch 0/1
Batch Loss = 0.0007309484141809411
1968, epoch_train_loss=0.0007309484141809411
Epoch 1969
Epoch 1969 :: Batch 0/1
Batch Loss = 0.0007309484141809365
1969, epoch_train_loss=0.0007309484141809365
Epoch 1970
Epoch 1970 :: Batch 0/1
Batch Loss = 0.0007309484141809319
1970, epoch_train_loss=0.0007309484141809319
Epoch 1971
Epoch 1971 :: Batch 0/1
Batch Loss = 0.0007309484141809273
1971, epoch_train_loss=0.0007309484141809273
Epoch 1972
Epoch 1972 :: Batch 0/1
Batch Loss = 0.0007309484141809227
1972, epoch_train_loss=0.0007309484141809227
Epoch 1973
Epoch 1973 :: Batch 0/1
Batch Loss = 0.0007309484141809181
1973, epoch_train_loss=0.0007309484141809181
Epoch 1974
Epoch 1974 :: Batch 0/1
Batch Loss = 0.0007309484141809133
1974, epoch_train_loss=0.0007309484141809133
Epoch 1975
Epoch 1975 :: Batch 0/1
Batch Loss = 0.0007309484141809087
1975, epoch_train_loss=0.0007309484141809087
Epoch 1976
Epoch 1976 :: Batch 0/1
Batch Loss = 0.0007309484141809041
1976, epoch_train_loss=0.0007309484141809041
Epoch 1977
Epoch 1977 :: Batch 0/1
Batch Loss = 0.0007309484141808994
1977, epoch_train_loss=0.0007309484141808994
Epoch 1978
Epoch 1978 :: Batch 0/1
Batch Loss = 0.0007309484141808948
1978, epoch_train_loss=0.0007309484141808948
Epoch 1979
Epoch 1979 :: Batch 0/1
Batch Loss = 0.0007309484141808903
1979, epoch_train_loss=0.0007309484141808903
Epoch 1980
Epoch 1980 :: Batch 0/1
Batch Loss = 0.0007309484141808855
1980, epoch_train_loss=0.0007309484141808855
Epoch 1981
Epoch 1981 :: Batch 0/1
Batch Loss = 0.0007309484141808809
1981, epoch_train_loss=0.0007309484141808809
Epoch 1982
Epoch 1982 :: Batch 0/1
Batch Loss = 0.0007309484141808764
1982, epoch_train_loss=0.0007309484141808764
Epoch 1983
Epoch 1983 :: Batch 0/1
Batch Loss = 0.0007309484141808715
1983, epoch_train_loss=0.0007309484141808715
Epoch 1984
Epoch 1984 :: Batch 0/1
Batch Loss = 0.0007309484141808669
1984, epoch_train_loss=0.0007309484141808669
Epoch 1985
Epoch 1985 :: Batch 0/1
Batch Loss = 0.0007309484141808622
1985, epoch_train_loss=0.0007309484141808622
Epoch 1986
Epoch 1986 :: Batch 0/1
Batch Loss = 0.0007309484141808576
1986, epoch_train_loss=0.0007309484141808576
Epoch 1987
Epoch 1987 :: Batch 0/1
Batch Loss = 0.0007309484141808531
1987, epoch_train_loss=0.0007309484141808531
Epoch 1988
Epoch 1988 :: Batch 0/1
Batch Loss = 0.0007309484141808483
1988, epoch_train_loss=0.0007309484141808483
Epoch 1989
Epoch 1989 :: Batch 0/1
Batch Loss = 0.0007309484141808437
1989, epoch_train_loss=0.0007309484141808437
Epoch 1990
Epoch 1990 :: Batch 0/1
Batch Loss = 0.000730948414180839
1990, epoch_train_loss=0.000730948414180839
Epoch 1991
Epoch 1991 :: Batch 0/1
Batch Loss = 0.0007309484141808344
1991, epoch_train_loss=0.0007309484141808344
Epoch 1992
Epoch 1992 :: Batch 0/1
Batch Loss = 0.0007309484141808296
1992, epoch_train_loss=0.0007309484141808296
Epoch 1993
Epoch 1993 :: Batch 0/1
Batch Loss = 0.000730948414180825
1993, epoch_train_loss=0.000730948414180825
Epoch 1994
Epoch 1994 :: Batch 0/1
Batch Loss = 0.0007309484141808202
1994, epoch_train_loss=0.0007309484141808202
Epoch 1995
Epoch 1995 :: Batch 0/1
Batch Loss = 0.0007309484141808157
1995, epoch_train_loss=0.0007309484141808157
Epoch 1996
Epoch 1996 :: Batch 0/1
Batch Loss = 0.0007309484141808109
1996, epoch_train_loss=0.0007309484141808109
Epoch 1997
Epoch 1997 :: Batch 0/1
Batch Loss = 0.0007309484141808063
1997, epoch_train_loss=0.0007309484141808063
Epoch 1998
Epoch 1998 :: Batch 0/1
Batch Loss = 0.0007309484141808016
1998, epoch_train_loss=0.0007309484141808016
Epoch 1999
Epoch 1999 :: Batch 0/1
Batch Loss = 0.0007309484141807969
1999, epoch_train_loss=0.0007309484141807969
Epoch 2000
Epoch 2000 :: Batch 0/1
Batch Loss = 0.0007309484141807921
2000, epoch_train_loss=0.0007309484141807921
Epoch 2001
Epoch 2001 :: Batch 0/1
Batch Loss = 0.0007309484141807875
2001, epoch_train_loss=0.0007309484141807875
Epoch 2002
Epoch 2002 :: Batch 0/1
Batch Loss = 0.0007309484141807828
2002, epoch_train_loss=0.0007309484141807828
Epoch 2003
Epoch 2003 :: Batch 0/1
Batch Loss = 0.0007309484141807781
2003, epoch_train_loss=0.0007309484141807781
Epoch 2004
Epoch 2004 :: Batch 0/1
Batch Loss = 0.0007309484141807735
2004, epoch_train_loss=0.0007309484141807735
Epoch 2005
Epoch 2005 :: Batch 0/1
Batch Loss = 0.0007309484141807686
2005, epoch_train_loss=0.0007309484141807686
Epoch 2006
Epoch 2006 :: Batch 0/1
Batch Loss = 0.0007309484141807639
2006, epoch_train_loss=0.0007309484141807639
Epoch 2007
Epoch 2007 :: Batch 0/1
Batch Loss = 0.0007309484141807593
2007, epoch_train_loss=0.0007309484141807593
Epoch 2008
Epoch 2008 :: Batch 0/1
Batch Loss = 0.0007309484141807545
2008, epoch_train_loss=0.0007309484141807545
Epoch 2009
Epoch 2009 :: Batch 0/1
Batch Loss = 0.0007309484141807498
2009, epoch_train_loss=0.0007309484141807498
Epoch 2010
Epoch 2010 :: Batch 0/1
Batch Loss = 0.0007309484141807452
2010, epoch_train_loss=0.0007309484141807452
Epoch 2011
Epoch 2011 :: Batch 0/1
Batch Loss = 0.0007309484141807405
2011, epoch_train_loss=0.0007309484141807405
Epoch 2012
Epoch 2012 :: Batch 0/1
Batch Loss = 0.0007309484141807356
2012, epoch_train_loss=0.0007309484141807356
Epoch 2013
Epoch 2013 :: Batch 0/1
Batch Loss = 0.000730948414180731
2013, epoch_train_loss=0.000730948414180731
Epoch 2014
Epoch 2014 :: Batch 0/1
Batch Loss = 0.0007309484141807262
2014, epoch_train_loss=0.0007309484141807262
Epoch 2015
Epoch 2015 :: Batch 0/1
Batch Loss = 0.0007309484141807215
2015, epoch_train_loss=0.0007309484141807215
Epoch 2016
Epoch 2016 :: Batch 0/1
Batch Loss = 0.0007309484141807169
2016, epoch_train_loss=0.0007309484141807169
Epoch 2017
Epoch 2017 :: Batch 0/1
Batch Loss = 0.000730948414180712
2017, epoch_train_loss=0.000730948414180712
Epoch 2018
Epoch 2018 :: Batch 0/1
Batch Loss = 0.0007309484141807072
2018, epoch_train_loss=0.0007309484141807072
Epoch 2019
Epoch 2019 :: Batch 0/1
Batch Loss = 0.0007309484141807026
2019, epoch_train_loss=0.0007309484141807026
Epoch 2020
Epoch 2020 :: Batch 0/1
Batch Loss = 0.0007309484141806979
2020, epoch_train_loss=0.0007309484141806979
Epoch 2021
Epoch 2021 :: Batch 0/1
Batch Loss = 0.0007309484141806931
2021, epoch_train_loss=0.0007309484141806931
Epoch 2022
Epoch 2022 :: Batch 0/1
Batch Loss = 0.0007309484141806883
2022, epoch_train_loss=0.0007309484141806883
Epoch 2023
Epoch 2023 :: Batch 0/1
Batch Loss = 0.0007309484141806835
2023, epoch_train_loss=0.0007309484141806835
Epoch 2024
Epoch 2024 :: Batch 0/1
Batch Loss = 0.0007309484141806787
2024, epoch_train_loss=0.0007309484141806787
Epoch 2025
Epoch 2025 :: Batch 0/1
Batch Loss = 0.0007309484141806742
2025, epoch_train_loss=0.0007309484141806742
Epoch 2026
Epoch 2026 :: Batch 0/1
Batch Loss = 0.0007309484141806694
2026, epoch_train_loss=0.0007309484141806694
Epoch 2027
Epoch 2027 :: Batch 0/1
Batch Loss = 0.0007309484141806645
2027, epoch_train_loss=0.0007309484141806645
Epoch 2028
Epoch 2028 :: Batch 0/1
Batch Loss = 0.0007309484141806597
2028, epoch_train_loss=0.0007309484141806597
Epoch 2029
Epoch 2029 :: Batch 0/1
Batch Loss = 0.000730948414180655
2029, epoch_train_loss=0.000730948414180655
Epoch 2030
Epoch 2030 :: Batch 0/1
Batch Loss = 0.0007309484141806503
2030, epoch_train_loss=0.0007309484141806503
Epoch 2031
Epoch 2031 :: Batch 0/1
Batch Loss = 0.0007309484141806455
2031, epoch_train_loss=0.0007309484141806455
Epoch 2032
Epoch 2032 :: Batch 0/1
Batch Loss = 0.0007309484141806407
2032, epoch_train_loss=0.0007309484141806407
Epoch 2033
Epoch 2033 :: Batch 0/1
Batch Loss = 0.0007309484141806359
2033, epoch_train_loss=0.0007309484141806359
Epoch 2034
Epoch 2034 :: Batch 0/1
Batch Loss = 0.0007309484141806312
2034, epoch_train_loss=0.0007309484141806312
Epoch 2035
Epoch 2035 :: Batch 0/1
Batch Loss = 0.0007309484141806265
2035, epoch_train_loss=0.0007309484141806265
Epoch 2036
Epoch 2036 :: Batch 0/1
Batch Loss = 0.0007309484141806217
2036, epoch_train_loss=0.0007309484141806217
Epoch 2037
Epoch 2037 :: Batch 0/1
Batch Loss = 0.0007309484141806168
2037, epoch_train_loss=0.0007309484141806168
Epoch 2038
Epoch 2038 :: Batch 0/1
Batch Loss = 0.000730948414180612
2038, epoch_train_loss=0.000730948414180612
Epoch 2039
Epoch 2039 :: Batch 0/1
Batch Loss = 0.0007309484141806073
2039, epoch_train_loss=0.0007309484141806073
Epoch 2040
Epoch 2040 :: Batch 0/1
Batch Loss = 0.0007309484141806025
2040, epoch_train_loss=0.0007309484141806025
Epoch 2041
Epoch 2041 :: Batch 0/1
Batch Loss = 0.0007309484141805976
2041, epoch_train_loss=0.0007309484141805976
Epoch 2042
Epoch 2042 :: Batch 0/1
Batch Loss = 0.0007309484141805929
2042, epoch_train_loss=0.0007309484141805929
Epoch 2043
Epoch 2043 :: Batch 0/1
Batch Loss = 0.0007309484141805881
2043, epoch_train_loss=0.0007309484141805881
Epoch 2044
Epoch 2044 :: Batch 0/1
Batch Loss = 0.0007309484141805833
2044, epoch_train_loss=0.0007309484141805833
Epoch 2045
Epoch 2045 :: Batch 0/1
Batch Loss = 0.0007309484141805784
2045, epoch_train_loss=0.0007309484141805784
Epoch 2046
Epoch 2046 :: Batch 0/1
Batch Loss = 0.0007309484141805737
2046, epoch_train_loss=0.0007309484141805737
Epoch 2047
Epoch 2047 :: Batch 0/1
Batch Loss = 0.0007309484141805689
2047, epoch_train_loss=0.0007309484141805689
Epoch 2048
Epoch 2048 :: Batch 0/1
Batch Loss = 0.0007309484141805641
2048, epoch_train_loss=0.0007309484141805641
Epoch 2049
Epoch 2049 :: Batch 0/1
Batch Loss = 0.0007309484141805591
2049, epoch_train_loss=0.0007309484141805591
Epoch 2050
Epoch 2050 :: Batch 0/1
Batch Loss = 0.0007309484141805544
2050, epoch_train_loss=0.0007309484141805544
Epoch 2051
Epoch 2051 :: Batch 0/1
Batch Loss = 0.0007309484141805496
2051, epoch_train_loss=0.0007309484141805496
Epoch 2052
Epoch 2052 :: Batch 0/1
Batch Loss = 0.0007309484141805448
2052, epoch_train_loss=0.0007309484141805448
Epoch 2053
Epoch 2053 :: Batch 0/1
Batch Loss = 0.0007309484141805399
2053, epoch_train_loss=0.0007309484141805399
Epoch 2054
Epoch 2054 :: Batch 0/1
Batch Loss = 0.0007309484141805351
2054, epoch_train_loss=0.0007309484141805351
Epoch 2055
Epoch 2055 :: Batch 0/1
Batch Loss = 0.0007309484141805303
2055, epoch_train_loss=0.0007309484141805303
Epoch 2056
Epoch 2056 :: Batch 0/1
Batch Loss = 0.0007309484141805254
2056, epoch_train_loss=0.0007309484141805254
Epoch 2057
Epoch 2057 :: Batch 0/1
Batch Loss = 0.0007309484141805206
2057, epoch_train_loss=0.0007309484141805206
Epoch 2058
Epoch 2058 :: Batch 0/1
Batch Loss = 0.0007309484141805159
2058, epoch_train_loss=0.0007309484141805159
Epoch 2059
Epoch 2059 :: Batch 0/1
Batch Loss = 0.0007309484141805109
2059, epoch_train_loss=0.0007309484141805109
Epoch 2060
Epoch 2060 :: Batch 0/1
Batch Loss = 0.000730948414180506
2060, epoch_train_loss=0.000730948414180506
Epoch 2061
Epoch 2061 :: Batch 0/1
Batch Loss = 0.0007309484141805012
2061, epoch_train_loss=0.0007309484141805012
Epoch 2062
Epoch 2062 :: Batch 0/1
Batch Loss = 0.0007309484141804964
2062, epoch_train_loss=0.0007309484141804964
Epoch 2063
Epoch 2063 :: Batch 0/1
Batch Loss = 0.0007309484141804915
2063, epoch_train_loss=0.0007309484141804915
Epoch 2064
Epoch 2064 :: Batch 0/1
Batch Loss = 0.0007309484141804866
2064, epoch_train_loss=0.0007309484141804866
Epoch 2065
Epoch 2065 :: Batch 0/1
Batch Loss = 0.0007309484141804818
2065, epoch_train_loss=0.0007309484141804818
Epoch 2066
Epoch 2066 :: Batch 0/1
Batch Loss = 0.0007309484141804771
2066, epoch_train_loss=0.0007309484141804771
Epoch 2067
Epoch 2067 :: Batch 0/1
Batch Loss = 0.0007309484141804721
2067, epoch_train_loss=0.0007309484141804721
Epoch 2068
Epoch 2068 :: Batch 0/1
Batch Loss = 0.0007309484141804673
2068, epoch_train_loss=0.0007309484141804673
Epoch 2069
Epoch 2069 :: Batch 0/1
Batch Loss = 0.0007309484141804624
2069, epoch_train_loss=0.0007309484141804624
Epoch 2070
Epoch 2070 :: Batch 0/1
Batch Loss = 0.0007309484141804574
2070, epoch_train_loss=0.0007309484141804574
Epoch 2071
Epoch 2071 :: Batch 0/1
Batch Loss = 0.0007309484141804526
2071, epoch_train_loss=0.0007309484141804526
Epoch 2072
Epoch 2072 :: Batch 0/1
Batch Loss = 0.0007309484141804478
2072, epoch_train_loss=0.0007309484141804478
Epoch 2073
Epoch 2073 :: Batch 0/1
Batch Loss = 0.000730948414180443
2073, epoch_train_loss=0.000730948414180443
Epoch 2074
Epoch 2074 :: Batch 0/1
Batch Loss = 0.0007309484141804379
2074, epoch_train_loss=0.0007309484141804379
Epoch 2075
Epoch 2075 :: Batch 0/1
Batch Loss = 0.0007309484141804332
2075, epoch_train_loss=0.0007309484141804332
Epoch 2076
Epoch 2076 :: Batch 0/1
Batch Loss = 0.0007309484141804281
2076, epoch_train_loss=0.0007309484141804281
Epoch 2077
Epoch 2077 :: Batch 0/1
Batch Loss = 0.0007309484141804233
2077, epoch_train_loss=0.0007309484141804233
Epoch 2078
Epoch 2078 :: Batch 0/1
Batch Loss = 0.0007309484141804185
2078, epoch_train_loss=0.0007309484141804185
Epoch 2079
Epoch 2079 :: Batch 0/1
Batch Loss = 0.0007309484141804134
2079, epoch_train_loss=0.0007309484141804134
Epoch 2080
Epoch 2080 :: Batch 0/1
Batch Loss = 0.0007309484141804086
2080, epoch_train_loss=0.0007309484141804086
Epoch 2081
Epoch 2081 :: Batch 0/1
Batch Loss = 0.0007309484141804038
2081, epoch_train_loss=0.0007309484141804038
Epoch 2082
Epoch 2082 :: Batch 0/1
Batch Loss = 0.0007309484141803989
2082, epoch_train_loss=0.0007309484141803989
Epoch 2083
Epoch 2083 :: Batch 0/1
Batch Loss = 0.0007309484141803939
2083, epoch_train_loss=0.0007309484141803939
Epoch 2084
Epoch 2084 :: Batch 0/1
Batch Loss = 0.000730948414180389
2084, epoch_train_loss=0.000730948414180389
Epoch 2085
Epoch 2085 :: Batch 0/1
Batch Loss = 0.0007309484141803843
2085, epoch_train_loss=0.0007309484141803843
Epoch 2086
Epoch 2086 :: Batch 0/1
Batch Loss = 0.0007309484141803792
2086, epoch_train_loss=0.0007309484141803792
Epoch 2087
Epoch 2087 :: Batch 0/1
Batch Loss = 0.0007309484141803744
2087, epoch_train_loss=0.0007309484141803744
Epoch 2088
Epoch 2088 :: Batch 0/1
Batch Loss = 0.0007309484141803693
2088, epoch_train_loss=0.0007309484141803693
Epoch 2089
Epoch 2089 :: Batch 0/1
Batch Loss = 0.0007309484141803645
2089, epoch_train_loss=0.0007309484141803645
Epoch 2090
Epoch 2090 :: Batch 0/1
Batch Loss = 0.0007309484141803595
2090, epoch_train_loss=0.0007309484141803595
Epoch 2091
Epoch 2091 :: Batch 0/1
Batch Loss = 0.0007309484141803548
2091, epoch_train_loss=0.0007309484141803548
Epoch 2092
Epoch 2092 :: Batch 0/1
Batch Loss = 0.0007309484141803497
2092, epoch_train_loss=0.0007309484141803497
Epoch 2093
Epoch 2093 :: Batch 0/1
Batch Loss = 0.0007309484141803448
2093, epoch_train_loss=0.0007309484141803448
Epoch 2094
Epoch 2094 :: Batch 0/1
Batch Loss = 0.0007309484141803398
2094, epoch_train_loss=0.0007309484141803398
Epoch 2095
Epoch 2095 :: Batch 0/1
Batch Loss = 0.0007309484141803349
2095, epoch_train_loss=0.0007309484141803349
Epoch 2096
Epoch 2096 :: Batch 0/1
Batch Loss = 0.00073094841418033
2096, epoch_train_loss=0.00073094841418033
Epoch 2097
Epoch 2097 :: Batch 0/1
Batch Loss = 0.000730948414180325
2097, epoch_train_loss=0.000730948414180325
Epoch 2098
Epoch 2098 :: Batch 0/1
Batch Loss = 0.00073094841418032
2098, epoch_train_loss=0.00073094841418032
Epoch 2099
Epoch 2099 :: Batch 0/1
Batch Loss = 0.0007309484141803152
2099, epoch_train_loss=0.0007309484141803152
Epoch 2100
Epoch 2100 :: Batch 0/1
Batch Loss = 0.0007309484141803101
2100, epoch_train_loss=0.0007309484141803101
Epoch 2101
Epoch 2101 :: Batch 0/1
Batch Loss = 0.0007309484141803053
2101, epoch_train_loss=0.0007309484141803053
Epoch 2102
Epoch 2102 :: Batch 0/1
Batch Loss = 0.0007309484141803002
2102, epoch_train_loss=0.0007309484141803002
Epoch 2103
Epoch 2103 :: Batch 0/1
Batch Loss = 0.0007309484141802952
2103, epoch_train_loss=0.0007309484141802952
Epoch 2104
Epoch 2104 :: Batch 0/1
Batch Loss = 0.0007309484141802904
2104, epoch_train_loss=0.0007309484141802904
Epoch 2105
Epoch 2105 :: Batch 0/1
Batch Loss = 0.0007309484141802854
2105, epoch_train_loss=0.0007309484141802854
Epoch 2106
Epoch 2106 :: Batch 0/1
Batch Loss = 0.0007309484141802805
2106, epoch_train_loss=0.0007309484141802805
Epoch 2107
Epoch 2107 :: Batch 0/1
Batch Loss = 0.0007309484141802756
2107, epoch_train_loss=0.0007309484141802756
Epoch 2108
Epoch 2108 :: Batch 0/1
Batch Loss = 0.0007309484141802705
2108, epoch_train_loss=0.0007309484141802705
Epoch 2109
Epoch 2109 :: Batch 0/1
Batch Loss = 0.0007309484141802655
2109, epoch_train_loss=0.0007309484141802655
Epoch 2110
Epoch 2110 :: Batch 0/1
Batch Loss = 0.0007309484141802604
2110, epoch_train_loss=0.0007309484141802604
Epoch 2111
Epoch 2111 :: Batch 0/1
Batch Loss = 0.0007309484141802557
2111, epoch_train_loss=0.0007309484141802557
Epoch 2112
Epoch 2112 :: Batch 0/1
Batch Loss = 0.0007309484141802507
2112, epoch_train_loss=0.0007309484141802507
Epoch 2113
Epoch 2113 :: Batch 0/1
Batch Loss = 0.0007309484141802456
2113, epoch_train_loss=0.0007309484141802456
Epoch 2114
Epoch 2114 :: Batch 0/1
Batch Loss = 0.0007309484141802407
2114, epoch_train_loss=0.0007309484141802407
Epoch 2115
Epoch 2115 :: Batch 0/1
Batch Loss = 0.0007309484141802356
2115, epoch_train_loss=0.0007309484141802356
Epoch 2116
Epoch 2116 :: Batch 0/1
Batch Loss = 0.0007309484141802307
2116, epoch_train_loss=0.0007309484141802307
Epoch 2117
Epoch 2117 :: Batch 0/1
Batch Loss = 0.0007309484141802256
2117, epoch_train_loss=0.0007309484141802256
Epoch 2118
Epoch 2118 :: Batch 0/1
Batch Loss = 0.0007309484141802206
2118, epoch_train_loss=0.0007309484141802206
Epoch 2119
Epoch 2119 :: Batch 0/1
Batch Loss = 0.0007309484141802157
2119, epoch_train_loss=0.0007309484141802157
Epoch 2120
Epoch 2120 :: Batch 0/1
Batch Loss = 0.0007309484141802107
2120, epoch_train_loss=0.0007309484141802107
Epoch 2121
Epoch 2121 :: Batch 0/1
Batch Loss = 0.0007309484141802058
2121, epoch_train_loss=0.0007309484141802058
Epoch 2122
Epoch 2122 :: Batch 0/1
Batch Loss = 0.0007309484141802007
2122, epoch_train_loss=0.0007309484141802007
Epoch 2123
Epoch 2123 :: Batch 0/1
Batch Loss = 0.0007309484141801957
2123, epoch_train_loss=0.0007309484141801957
Epoch 2124
Epoch 2124 :: Batch 0/1
Batch Loss = 0.0007309484141801907
2124, epoch_train_loss=0.0007309484141801907
Epoch 2125
Epoch 2125 :: Batch 0/1
Batch Loss = 0.0007309484141801856
2125, epoch_train_loss=0.0007309484141801856
Epoch 2126
Epoch 2126 :: Batch 0/1
Batch Loss = 0.0007309484141801807
2126, epoch_train_loss=0.0007309484141801807
Epoch 2127
Epoch 2127 :: Batch 0/1
Batch Loss = 0.0007309484141801757
2127, epoch_train_loss=0.0007309484141801757
Epoch 2128
Epoch 2128 :: Batch 0/1
Batch Loss = 0.0007309484141801707
2128, epoch_train_loss=0.0007309484141801707
Epoch 2129
Epoch 2129 :: Batch 0/1
Batch Loss = 0.0007309484141801655
2129, epoch_train_loss=0.0007309484141801655
Epoch 2130
Epoch 2130 :: Batch 0/1
Batch Loss = 0.0007309484141801606
2130, epoch_train_loss=0.0007309484141801606
Epoch 2131
Epoch 2131 :: Batch 0/1
Batch Loss = 0.0007309484141801555
2131, epoch_train_loss=0.0007309484141801555
Epoch 2132
Epoch 2132 :: Batch 0/1
Batch Loss = 0.0007309484141801506
2132, epoch_train_loss=0.0007309484141801506
Epoch 2133
Epoch 2133 :: Batch 0/1
Batch Loss = 0.0007309484141801455
2133, epoch_train_loss=0.0007309484141801455
Epoch 2134
Epoch 2134 :: Batch 0/1
Batch Loss = 0.0007309484141801404
2134, epoch_train_loss=0.0007309484141801404
Epoch 2135
Epoch 2135 :: Batch 0/1
Batch Loss = 0.0007309484141801354
2135, epoch_train_loss=0.0007309484141801354
Epoch 2136
Epoch 2136 :: Batch 0/1
Batch Loss = 0.0007309484141801303
2136, epoch_train_loss=0.0007309484141801303
Epoch 2137
Epoch 2137 :: Batch 0/1
Batch Loss = 0.0007309484141801253
2137, epoch_train_loss=0.0007309484141801253
Epoch 2138
Epoch 2138 :: Batch 0/1
Batch Loss = 0.0007309484141801201
2138, epoch_train_loss=0.0007309484141801201
Epoch 2139
Epoch 2139 :: Batch 0/1
Batch Loss = 0.0007309484141801152
2139, epoch_train_loss=0.0007309484141801152
Epoch 2140
Epoch 2140 :: Batch 0/1
Batch Loss = 0.0007309484141801101
2140, epoch_train_loss=0.0007309484141801101
Epoch 2141
Epoch 2141 :: Batch 0/1
Batch Loss = 0.0007309484141801051
2141, epoch_train_loss=0.0007309484141801051
Epoch 2142
Epoch 2142 :: Batch 0/1
Batch Loss = 0.0007309484141801
2142, epoch_train_loss=0.0007309484141801
Epoch 2143
Epoch 2143 :: Batch 0/1
Batch Loss = 0.000730948414180095
2143, epoch_train_loss=0.000730948414180095
Epoch 2144
Epoch 2144 :: Batch 0/1
Batch Loss = 0.0007309484141800899
2144, epoch_train_loss=0.0007309484141800899
Epoch 2145
Epoch 2145 :: Batch 0/1
Batch Loss = 0.0007309484141800849
2145, epoch_train_loss=0.0007309484141800849
Epoch 2146
Epoch 2146 :: Batch 0/1
Batch Loss = 0.0007309484141800797
2146, epoch_train_loss=0.0007309484141800797
Epoch 2147
Epoch 2147 :: Batch 0/1
Batch Loss = 0.0007309484141800747
2147, epoch_train_loss=0.0007309484141800747
Epoch 2148
Epoch 2148 :: Batch 0/1
Batch Loss = 0.0007309484141800695
2148, epoch_train_loss=0.0007309484141800695
Epoch 2149
Epoch 2149 :: Batch 0/1
Batch Loss = 0.0007309484141800645
2149, epoch_train_loss=0.0007309484141800645
Epoch 2150
Epoch 2150 :: Batch 0/1
Batch Loss = 0.0007309484141800595
2150, epoch_train_loss=0.0007309484141800595
Epoch 2151
Epoch 2151 :: Batch 0/1
Batch Loss = 0.0007309484141800544
2151, epoch_train_loss=0.0007309484141800544
Epoch 2152
Epoch 2152 :: Batch 0/1
Batch Loss = 0.0007309484141800495
2152, epoch_train_loss=0.0007309484141800495
Epoch 2153
Epoch 2153 :: Batch 0/1
Batch Loss = 0.0007309484141800442
2153, epoch_train_loss=0.0007309484141800442
Epoch 2154
Epoch 2154 :: Batch 0/1
Batch Loss = 0.0007309484141800393
2154, epoch_train_loss=0.0007309484141800393
Epoch 2155
Epoch 2155 :: Batch 0/1
Batch Loss = 0.0007309484141800341
2155, epoch_train_loss=0.0007309484141800341
Epoch 2156
Epoch 2156 :: Batch 0/1
Batch Loss = 0.0007309484141800291
2156, epoch_train_loss=0.0007309484141800291
Epoch 2157
Epoch 2157 :: Batch 0/1
Batch Loss = 0.000730948414180024
2157, epoch_train_loss=0.000730948414180024
Epoch 2158
Epoch 2158 :: Batch 0/1
Batch Loss = 0.0007309484141800188
2158, epoch_train_loss=0.0007309484141800188
Epoch 2159
Epoch 2159 :: Batch 0/1
Batch Loss = 0.0007309484141800137
2159, epoch_train_loss=0.0007309484141800137
Epoch 2160
Epoch 2160 :: Batch 0/1
Batch Loss = 0.0007309484141800087
2160, epoch_train_loss=0.0007309484141800087
Epoch 2161
Epoch 2161 :: Batch 0/1
Batch Loss = 0.0007309484141800035
2161, epoch_train_loss=0.0007309484141800035
Epoch 2162
Epoch 2162 :: Batch 0/1
Batch Loss = 0.0007309484141799983
2162, epoch_train_loss=0.0007309484141799983
Epoch 2163
Epoch 2163 :: Batch 0/1
Batch Loss = 0.0007309484141799933
2163, epoch_train_loss=0.0007309484141799933
Epoch 2164
Epoch 2164 :: Batch 0/1
Batch Loss = 0.000730948414179988
2164, epoch_train_loss=0.000730948414179988
Epoch 2165
Epoch 2165 :: Batch 0/1
Batch Loss = 0.000730948414179983
2165, epoch_train_loss=0.000730948414179983
Epoch 2166
Epoch 2166 :: Batch 0/1
Batch Loss = 0.0007309484141799779
2166, epoch_train_loss=0.0007309484141799779
Epoch 2167
Epoch 2167 :: Batch 0/1
Batch Loss = 0.0007309484141799727
2167, epoch_train_loss=0.0007309484141799727
Epoch 2168
Epoch 2168 :: Batch 0/1
Batch Loss = 0.0007309484141799676
2168, epoch_train_loss=0.0007309484141799676
Epoch 2169
Epoch 2169 :: Batch 0/1
Batch Loss = 0.0007309484141799625
2169, epoch_train_loss=0.0007309484141799625
Epoch 2170
Epoch 2170 :: Batch 0/1
Batch Loss = 0.0007309484141799574
2170, epoch_train_loss=0.0007309484141799574
Epoch 2171
Epoch 2171 :: Batch 0/1
Batch Loss = 0.0007309484141799523
2171, epoch_train_loss=0.0007309484141799523
Epoch 2172
Epoch 2172 :: Batch 0/1
Batch Loss = 0.000730948414179947
2172, epoch_train_loss=0.000730948414179947
Epoch 2173
Epoch 2173 :: Batch 0/1
Batch Loss = 0.000730948414179942
2173, epoch_train_loss=0.000730948414179942
Epoch 2174
Epoch 2174 :: Batch 0/1
Batch Loss = 0.0007309484141799367
2174, epoch_train_loss=0.0007309484141799367
Epoch 2175
Epoch 2175 :: Batch 0/1
Batch Loss = 0.0007309484141799317
2175, epoch_train_loss=0.0007309484141799317
Epoch 2176
Epoch 2176 :: Batch 0/1
Batch Loss = 0.0007309484141799266
2176, epoch_train_loss=0.0007309484141799266
Epoch 2177
Epoch 2177 :: Batch 0/1
Batch Loss = 0.0007309484141799215
2177, epoch_train_loss=0.0007309484141799215
Epoch 2178
Epoch 2178 :: Batch 0/1
Batch Loss = 0.0007309484141799163
2178, epoch_train_loss=0.0007309484141799163
Epoch 2179
Epoch 2179 :: Batch 0/1
Batch Loss = 0.000730948414179911
2179, epoch_train_loss=0.000730948414179911
Epoch 2180
Epoch 2180 :: Batch 0/1
Batch Loss = 0.000730948414179906
2180, epoch_train_loss=0.000730948414179906
Epoch 2181
Epoch 2181 :: Batch 0/1
Batch Loss = 0.0007309484141799007
2181, epoch_train_loss=0.0007309484141799007
Epoch 2182
Epoch 2182 :: Batch 0/1
Batch Loss = 0.0007309484141798957
2182, epoch_train_loss=0.0007309484141798957
Epoch 2183
Epoch 2183 :: Batch 0/1
Batch Loss = 0.0007309484141798904
2183, epoch_train_loss=0.0007309484141798904
Epoch 2184
Epoch 2184 :: Batch 0/1
Batch Loss = 0.0007309484141798853
2184, epoch_train_loss=0.0007309484141798853
Epoch 2185
Epoch 2185 :: Batch 0/1
Batch Loss = 0.0007309484141798801
2185, epoch_train_loss=0.0007309484141798801
Epoch 2186
Epoch 2186 :: Batch 0/1
Batch Loss = 0.0007309484141798749
2186, epoch_train_loss=0.0007309484141798749
Epoch 2187
Epoch 2187 :: Batch 0/1
Batch Loss = 0.0007309484141798698
2187, epoch_train_loss=0.0007309484141798698
Epoch 2188
Epoch 2188 :: Batch 0/1
Batch Loss = 0.0007309484141798645
2188, epoch_train_loss=0.0007309484141798645
Epoch 2189
Epoch 2189 :: Batch 0/1
Batch Loss = 0.0007309484141798594
2189, epoch_train_loss=0.0007309484141798594
Epoch 2190
Epoch 2190 :: Batch 0/1
Batch Loss = 0.0007309484141798542
2190, epoch_train_loss=0.0007309484141798542
Epoch 2191
Epoch 2191 :: Batch 0/1
Batch Loss = 0.0007309484141798489
2191, epoch_train_loss=0.0007309484141798489
Epoch 2192
Epoch 2192 :: Batch 0/1
Batch Loss = 0.0007309484141798438
2192, epoch_train_loss=0.0007309484141798438
Epoch 2193
Epoch 2193 :: Batch 0/1
Batch Loss = 0.0007309484141798386
2193, epoch_train_loss=0.0007309484141798386
Epoch 2194
Epoch 2194 :: Batch 0/1
Batch Loss = 0.0007309484141798334
2194, epoch_train_loss=0.0007309484141798334
Epoch 2195
Epoch 2195 :: Batch 0/1
Batch Loss = 0.0007309484141798283
2195, epoch_train_loss=0.0007309484141798283
Epoch 2196
Epoch 2196 :: Batch 0/1
Batch Loss = 0.0007309484141798231
2196, epoch_train_loss=0.0007309484141798231
Epoch 2197
Epoch 2197 :: Batch 0/1
Batch Loss = 0.0007309484141798178
2197, epoch_train_loss=0.0007309484141798178
Epoch 2198
Epoch 2198 :: Batch 0/1
Batch Loss = 0.0007309484141798126
2198, epoch_train_loss=0.0007309484141798126
Epoch 2199
Epoch 2199 :: Batch 0/1
Batch Loss = 0.0007309484141798073
2199, epoch_train_loss=0.0007309484141798073
Epoch 2200
Epoch 2200 :: Batch 0/1
Batch Loss = 0.0007309484141798021
2200, epoch_train_loss=0.0007309484141798021
Epoch 2201
Epoch 2201 :: Batch 0/1
Batch Loss = 0.0007309484141797968
2201, epoch_train_loss=0.0007309484141797968
Epoch 2202
Epoch 2202 :: Batch 0/1
Batch Loss = 0.0007309484141797918
2202, epoch_train_loss=0.0007309484141797918
Epoch 2203
Epoch 2203 :: Batch 0/1
Batch Loss = 0.0007309484141797865
2203, epoch_train_loss=0.0007309484141797865
Epoch 2204
Epoch 2204 :: Batch 0/1
Batch Loss = 0.0007309484141797813
2204, epoch_train_loss=0.0007309484141797813
Epoch 2205
Epoch 2205 :: Batch 0/1
Batch Loss = 0.000730948414179776
2205, epoch_train_loss=0.000730948414179776
Epoch 2206
Epoch 2206 :: Batch 0/1
Batch Loss = 0.0007309484141797708
2206, epoch_train_loss=0.0007309484141797708
Epoch 2207
Epoch 2207 :: Batch 0/1
Batch Loss = 0.0007309484141797656
2207, epoch_train_loss=0.0007309484141797656
Epoch 2208
Epoch 2208 :: Batch 0/1
Batch Loss = 0.0007309484141797604
2208, epoch_train_loss=0.0007309484141797604
Epoch 2209
Epoch 2209 :: Batch 0/1
Batch Loss = 0.0007309484141797552
2209, epoch_train_loss=0.0007309484141797552
Epoch 2210
Epoch 2210 :: Batch 0/1
Batch Loss = 0.0007309484141797499
2210, epoch_train_loss=0.0007309484141797499
Epoch 2211
Epoch 2211 :: Batch 0/1
Batch Loss = 0.0007309484141797447
2211, epoch_train_loss=0.0007309484141797447
Epoch 2212
Epoch 2212 :: Batch 0/1
Batch Loss = 0.0007309484141797395
2212, epoch_train_loss=0.0007309484141797395
Epoch 2213
Epoch 2213 :: Batch 0/1
Batch Loss = 0.0007309484141797342
2213, epoch_train_loss=0.0007309484141797342
Epoch 2214
Epoch 2214 :: Batch 0/1
Batch Loss = 0.0007309484141797289
2214, epoch_train_loss=0.0007309484141797289
Epoch 2215
Epoch 2215 :: Batch 0/1
Batch Loss = 0.0007309484141797238
2215, epoch_train_loss=0.0007309484141797238
Epoch 2216
Epoch 2216 :: Batch 0/1
Batch Loss = 0.0007309484141797184
2216, epoch_train_loss=0.0007309484141797184
Epoch 2217
Epoch 2217 :: Batch 0/1
Batch Loss = 0.0007309484141797131
2217, epoch_train_loss=0.0007309484141797131
Epoch 2218
Epoch 2218 :: Batch 0/1
Batch Loss = 0.0007309484141797079
2218, epoch_train_loss=0.0007309484141797079
Epoch 2219
Epoch 2219 :: Batch 0/1
Batch Loss = 0.0007309484141797026
2219, epoch_train_loss=0.0007309484141797026
Epoch 2220
Epoch 2220 :: Batch 0/1
Batch Loss = 0.0007309484141796973
2220, epoch_train_loss=0.0007309484141796973
Epoch 2221
Epoch 2221 :: Batch 0/1
Batch Loss = 0.0007309484141796921
2221, epoch_train_loss=0.0007309484141796921
Epoch 2222
Epoch 2222 :: Batch 0/1
Batch Loss = 0.0007309484141796868
2222, epoch_train_loss=0.0007309484141796868
Epoch 2223
Epoch 2223 :: Batch 0/1
Batch Loss = 0.0007309484141796815
2223, epoch_train_loss=0.0007309484141796815
Epoch 2224
Epoch 2224 :: Batch 0/1
Batch Loss = 0.0007309484141796763
2224, epoch_train_loss=0.0007309484141796763
Epoch 2225
Epoch 2225 :: Batch 0/1
Batch Loss = 0.000730948414179671
2225, epoch_train_loss=0.000730948414179671
Epoch 2226
Epoch 2226 :: Batch 0/1
Batch Loss = 0.0007309484141796656
2226, epoch_train_loss=0.0007309484141796656
Epoch 2227
Epoch 2227 :: Batch 0/1
Batch Loss = 0.0007309484141796604
2227, epoch_train_loss=0.0007309484141796604
Epoch 2228
Epoch 2228 :: Batch 0/1
Batch Loss = 0.0007309484141796551
2228, epoch_train_loss=0.0007309484141796551
Epoch 2229
Epoch 2229 :: Batch 0/1
Batch Loss = 0.0007309484141796498
2229, epoch_train_loss=0.0007309484141796498
Epoch 2230
Epoch 2230 :: Batch 0/1
Batch Loss = 0.0007309484141796446
2230, epoch_train_loss=0.0007309484141796446
Epoch 2231
Epoch 2231 :: Batch 0/1
Batch Loss = 0.0007309484141796392
2231, epoch_train_loss=0.0007309484141796392
Epoch 2232
Epoch 2232 :: Batch 0/1
Batch Loss = 0.000730948414179634
2232, epoch_train_loss=0.000730948414179634
Epoch 2233
Epoch 2233 :: Batch 0/1
Batch Loss = 0.0007309484141796287
2233, epoch_train_loss=0.0007309484141796287
Epoch 2234
Epoch 2234 :: Batch 0/1
Batch Loss = 0.0007309484141796235
2234, epoch_train_loss=0.0007309484141796235
Epoch 2235
Epoch 2235 :: Batch 0/1
Batch Loss = 0.0007309484141796179
2235, epoch_train_loss=0.0007309484141796179
Epoch 2236
Epoch 2236 :: Batch 0/1
Batch Loss = 0.0007309484141796127
2236, epoch_train_loss=0.0007309484141796127
Epoch 2237
Epoch 2237 :: Batch 0/1
Batch Loss = 0.0007309484141796074
2237, epoch_train_loss=0.0007309484141796074
Epoch 2238
Epoch 2238 :: Batch 0/1
Batch Loss = 0.0007309484141796021
2238, epoch_train_loss=0.0007309484141796021
Epoch 2239
Epoch 2239 :: Batch 0/1
Batch Loss = 0.0007309484141795969
2239, epoch_train_loss=0.0007309484141795969
Epoch 2240
Epoch 2240 :: Batch 0/1
Batch Loss = 0.0007309484141795915
2240, epoch_train_loss=0.0007309484141795915
Epoch 2241
Epoch 2241 :: Batch 0/1
Batch Loss = 0.0007309484141795862
2241, epoch_train_loss=0.0007309484141795862
Epoch 2242
Epoch 2242 :: Batch 0/1
Batch Loss = 0.0007309484141795808
2242, epoch_train_loss=0.0007309484141795808
Epoch 2243
Epoch 2243 :: Batch 0/1
Batch Loss = 0.0007309484141795755
2243, epoch_train_loss=0.0007309484141795755
Epoch 2244
Epoch 2244 :: Batch 0/1
Batch Loss = 0.0007309484141795701
2244, epoch_train_loss=0.0007309484141795701
Epoch 2245
Epoch 2245 :: Batch 0/1
Batch Loss = 0.0007309484141795648
2245, epoch_train_loss=0.0007309484141795648
Epoch 2246
Epoch 2246 :: Batch 0/1
Batch Loss = 0.0007309484141795595
2246, epoch_train_loss=0.0007309484141795595
Epoch 2247
Epoch 2247 :: Batch 0/1
Batch Loss = 0.0007309484141795542
2247, epoch_train_loss=0.0007309484141795542
Epoch 2248
Epoch 2248 :: Batch 0/1
Batch Loss = 0.0007309484141795488
2248, epoch_train_loss=0.0007309484141795488
Epoch 2249
Epoch 2249 :: Batch 0/1
Batch Loss = 0.0007309484141795433
2249, epoch_train_loss=0.0007309484141795433
Epoch 2250
Epoch 2250 :: Batch 0/1
Batch Loss = 0.0007309484141795381
2250, epoch_train_loss=0.0007309484141795381
Epoch 2251
Epoch 2251 :: Batch 0/1
Batch Loss = 0.0007309484141795327
2251, epoch_train_loss=0.0007309484141795327
Epoch 2252
Epoch 2252 :: Batch 0/1
Batch Loss = 0.0007309484141795274
2252, epoch_train_loss=0.0007309484141795274
Epoch 2253
Epoch 2253 :: Batch 0/1
Batch Loss = 0.000730948414179522
2253, epoch_train_loss=0.000730948414179522
Epoch 2254
Epoch 2254 :: Batch 0/1
Batch Loss = 0.0007309484141795168
2254, epoch_train_loss=0.0007309484141795168
Epoch 2255
Epoch 2255 :: Batch 0/1
Batch Loss = 0.0007309484141795114
2255, epoch_train_loss=0.0007309484141795114
Epoch 2256
Epoch 2256 :: Batch 0/1
Batch Loss = 0.0007309484141795059
2256, epoch_train_loss=0.0007309484141795059
Epoch 2257
Epoch 2257 :: Batch 0/1
Batch Loss = 0.0007309484141795007
2257, epoch_train_loss=0.0007309484141795007
Epoch 2258
Epoch 2258 :: Batch 0/1
Batch Loss = 0.0007309484141794952
2258, epoch_train_loss=0.0007309484141794952
Epoch 2259
Epoch 2259 :: Batch 0/1
Batch Loss = 0.0007309484141794899
2259, epoch_train_loss=0.0007309484141794899
Epoch 2260
Epoch 2260 :: Batch 0/1
Batch Loss = 0.0007309484141794844
2260, epoch_train_loss=0.0007309484141794844
Epoch 2261
Epoch 2261 :: Batch 0/1
Batch Loss = 0.000730948414179479
2261, epoch_train_loss=0.000730948414179479
Epoch 2262
Epoch 2262 :: Batch 0/1
Batch Loss = 0.0007309484141794737
2262, epoch_train_loss=0.0007309484141794737
Epoch 2263
Epoch 2263 :: Batch 0/1
Batch Loss = 0.0007309484141794683
2263, epoch_train_loss=0.0007309484141794683
Epoch 2264
Epoch 2264 :: Batch 0/1
Batch Loss = 0.0007309484141794629
2264, epoch_train_loss=0.0007309484141794629
Epoch 2265
Epoch 2265 :: Batch 0/1
Batch Loss = 0.0007309484141794575
2265, epoch_train_loss=0.0007309484141794575
Epoch 2266
Epoch 2266 :: Batch 0/1
Batch Loss = 0.0007309484141794522
2266, epoch_train_loss=0.0007309484141794522
Epoch 2267
Epoch 2267 :: Batch 0/1
Batch Loss = 0.0007309484141794467
2267, epoch_train_loss=0.0007309484141794467
Epoch 2268
Epoch 2268 :: Batch 0/1
Batch Loss = 0.0007309484141794413
2268, epoch_train_loss=0.0007309484141794413
Epoch 2269
Epoch 2269 :: Batch 0/1
Batch Loss = 0.0007309484141794359
2269, epoch_train_loss=0.0007309484141794359
Epoch 2270
Epoch 2270 :: Batch 0/1
Batch Loss = 0.0007309484141794304
2270, epoch_train_loss=0.0007309484141794304
Epoch 2271
Epoch 2271 :: Batch 0/1
Batch Loss = 0.0007309484141794252
2271, epoch_train_loss=0.0007309484141794252
Epoch 2272
Epoch 2272 :: Batch 0/1
Batch Loss = 0.0007309484141794196
2272, epoch_train_loss=0.0007309484141794196
Epoch 2273
Epoch 2273 :: Batch 0/1
Batch Loss = 0.0007309484141794143
2273, epoch_train_loss=0.0007309484141794143
Epoch 2274
Epoch 2274 :: Batch 0/1
Batch Loss = 0.0007309484141794088
2274, epoch_train_loss=0.0007309484141794088
Epoch 2275
Epoch 2275 :: Batch 0/1
Batch Loss = 0.0007309484141794036
2275, epoch_train_loss=0.0007309484141794036
Epoch 2276
Epoch 2276 :: Batch 0/1
Batch Loss = 0.0007309484141793981
2276, epoch_train_loss=0.0007309484141793981
Epoch 2277
Epoch 2277 :: Batch 0/1
Batch Loss = 0.0007309484141793926
2277, epoch_train_loss=0.0007309484141793926
Epoch 2278
Epoch 2278 :: Batch 0/1
Batch Loss = 0.0007309484141793872
2278, epoch_train_loss=0.0007309484141793872
Epoch 2279
Epoch 2279 :: Batch 0/1
Batch Loss = 0.0007309484141793817
2279, epoch_train_loss=0.0007309484141793817
Epoch 2280
Epoch 2280 :: Batch 0/1
Batch Loss = 0.0007309484141793764
2280, epoch_train_loss=0.0007309484141793764
Epoch 2281
Epoch 2281 :: Batch 0/1
Batch Loss = 0.000730948414179371
2281, epoch_train_loss=0.000730948414179371
Epoch 2282
Epoch 2282 :: Batch 0/1
Batch Loss = 0.0007309484141793654
2282, epoch_train_loss=0.0007309484141793654
Epoch 2283
Epoch 2283 :: Batch 0/1
Batch Loss = 0.00073094841417936
2283, epoch_train_loss=0.00073094841417936
Epoch 2284
Epoch 2284 :: Batch 0/1
Batch Loss = 0.0007309484141793546
2284, epoch_train_loss=0.0007309484141793546
Epoch 2285
Epoch 2285 :: Batch 0/1
Batch Loss = 0.0007309484141793492
2285, epoch_train_loss=0.0007309484141793492
Epoch 2286
Epoch 2286 :: Batch 0/1
Batch Loss = 0.0007309484141793437
2286, epoch_train_loss=0.0007309484141793437
Epoch 2287
Epoch 2287 :: Batch 0/1
Batch Loss = 0.0007309484141793382
2287, epoch_train_loss=0.0007309484141793382
Epoch 2288
Epoch 2288 :: Batch 0/1
Batch Loss = 0.0007309484141793328
2288, epoch_train_loss=0.0007309484141793328
Epoch 2289
Epoch 2289 :: Batch 0/1
Batch Loss = 0.0007309484141793273
2289, epoch_train_loss=0.0007309484141793273
Epoch 2290
Epoch 2290 :: Batch 0/1
Batch Loss = 0.0007309484141793217
2290, epoch_train_loss=0.0007309484141793217
Epoch 2291
Epoch 2291 :: Batch 0/1
Batch Loss = 0.0007309484141793164
2291, epoch_train_loss=0.0007309484141793164
Epoch 2292
Epoch 2292 :: Batch 0/1
Batch Loss = 0.0007309484141793109
2292, epoch_train_loss=0.0007309484141793109
Epoch 2293
Epoch 2293 :: Batch 0/1
Batch Loss = 0.0007309484141793055
2293, epoch_train_loss=0.0007309484141793055
Epoch 2294
Epoch 2294 :: Batch 0/1
Batch Loss = 0.0007309484141792999
2294, epoch_train_loss=0.0007309484141792999
Epoch 2295
Epoch 2295 :: Batch 0/1
Batch Loss = 0.0007309484141792944
2295, epoch_train_loss=0.0007309484141792944
Epoch 2296
Epoch 2296 :: Batch 0/1
Batch Loss = 0.0007309484141792889
2296, epoch_train_loss=0.0007309484141792889
Epoch 2297
Epoch 2297 :: Batch 0/1
Batch Loss = 0.0007309484141792836
2297, epoch_train_loss=0.0007309484141792836
Epoch 2298
Epoch 2298 :: Batch 0/1
Batch Loss = 0.0007309484141792781
2298, epoch_train_loss=0.0007309484141792781
Epoch 2299
Epoch 2299 :: Batch 0/1
Batch Loss = 0.0007309484141792726
2299, epoch_train_loss=0.0007309484141792726
Epoch 2300
Epoch 2300 :: Batch 0/1
Batch Loss = 0.0007309484141792671
2300, epoch_train_loss=0.0007309484141792671
Epoch 2301
Epoch 2301 :: Batch 0/1
Batch Loss = 0.0007309484141792616
2301, epoch_train_loss=0.0007309484141792616
Epoch 2302
Epoch 2302 :: Batch 0/1
Batch Loss = 0.0007309484141792561
2302, epoch_train_loss=0.0007309484141792561
Epoch 2303
Epoch 2303 :: Batch 0/1
Batch Loss = 0.0007309484141792507
2303, epoch_train_loss=0.0007309484141792507
Epoch 2304
Epoch 2304 :: Batch 0/1
Batch Loss = 0.0007309484141792451
2304, epoch_train_loss=0.0007309484141792451
Epoch 2305
Epoch 2305 :: Batch 0/1
Batch Loss = 0.0007309484141792398
2305, epoch_train_loss=0.0007309484141792398
Epoch 2306
Epoch 2306 :: Batch 0/1
Batch Loss = 0.0007309484141792341
2306, epoch_train_loss=0.0007309484141792341
Epoch 2307
Epoch 2307 :: Batch 0/1
Batch Loss = 0.0007309484141792286
2307, epoch_train_loss=0.0007309484141792286
Epoch 2308
Epoch 2308 :: Batch 0/1
Batch Loss = 0.0007309484141792232
2308, epoch_train_loss=0.0007309484141792232
Epoch 2309
Epoch 2309 :: Batch 0/1
Batch Loss = 0.0007309484141792177
2309, epoch_train_loss=0.0007309484141792177
Epoch 2310
Epoch 2310 :: Batch 0/1
Batch Loss = 0.0007309484141792122
2310, epoch_train_loss=0.0007309484141792122
Epoch 2311
Epoch 2311 :: Batch 0/1
Batch Loss = 0.0007309484141792066
2311, epoch_train_loss=0.0007309484141792066
Epoch 2312
Epoch 2312 :: Batch 0/1
Batch Loss = 0.0007309484141792011
2312, epoch_train_loss=0.0007309484141792011
Epoch 2313
Epoch 2313 :: Batch 0/1
Batch Loss = 0.0007309484141791955
2313, epoch_train_loss=0.0007309484141791955
Epoch 2314
Epoch 2314 :: Batch 0/1
Batch Loss = 0.0007309484141791901
2314, epoch_train_loss=0.0007309484141791901
Epoch 2315
Epoch 2315 :: Batch 0/1
Batch Loss = 0.0007309484141791846
2315, epoch_train_loss=0.0007309484141791846
Epoch 2316
Epoch 2316 :: Batch 0/1
Batch Loss = 0.0007309484141791789
2316, epoch_train_loss=0.0007309484141791789
Epoch 2317
Epoch 2317 :: Batch 0/1
Batch Loss = 0.0007309484141791734
2317, epoch_train_loss=0.0007309484141791734
Epoch 2318
Epoch 2318 :: Batch 0/1
Batch Loss = 0.0007309484141791678
2318, epoch_train_loss=0.0007309484141791678
Epoch 2319
Epoch 2319 :: Batch 0/1
Batch Loss = 0.0007309484141791622
2319, epoch_train_loss=0.0007309484141791622
Epoch 2320
Epoch 2320 :: Batch 0/1
Batch Loss = 0.0007309484141791568
2320, epoch_train_loss=0.0007309484141791568
Epoch 2321
Epoch 2321 :: Batch 0/1
Batch Loss = 0.0007309484141791513
2321, epoch_train_loss=0.0007309484141791513
Epoch 2322
Epoch 2322 :: Batch 0/1
Batch Loss = 0.0007309484141791457
2322, epoch_train_loss=0.0007309484141791457
Epoch 2323
Epoch 2323 :: Batch 0/1
Batch Loss = 0.0007309484141791402
2323, epoch_train_loss=0.0007309484141791402
Epoch 2324
Epoch 2324 :: Batch 0/1
Batch Loss = 0.0007309484141791346
2324, epoch_train_loss=0.0007309484141791346
Epoch 2325
Epoch 2325 :: Batch 0/1
Batch Loss = 0.000730948414179129
2325, epoch_train_loss=0.000730948414179129
Epoch 2326
Epoch 2326 :: Batch 0/1
Batch Loss = 0.0007309484141791235
2326, epoch_train_loss=0.0007309484141791235
Epoch 2327
Epoch 2327 :: Batch 0/1
Batch Loss = 0.0007309484141791179
2327, epoch_train_loss=0.0007309484141791179
Epoch 2328
Epoch 2328 :: Batch 0/1
Batch Loss = 0.0007309484141791123
2328, epoch_train_loss=0.0007309484141791123
Epoch 2329
Epoch 2329 :: Batch 0/1
Batch Loss = 0.0007309484141791066
2329, epoch_train_loss=0.0007309484141791066
Epoch 2330
Epoch 2330 :: Batch 0/1
Batch Loss = 0.0007309484141791011
2330, epoch_train_loss=0.0007309484141791011
Epoch 2331
Epoch 2331 :: Batch 0/1
Batch Loss = 0.0007309484141790956
2331, epoch_train_loss=0.0007309484141790956
Epoch 2332
Epoch 2332 :: Batch 0/1
Batch Loss = 0.0007309484141790901
2332, epoch_train_loss=0.0007309484141790901
Epoch 2333
Epoch 2333 :: Batch 0/1
Batch Loss = 0.0007309484141790844
2333, epoch_train_loss=0.0007309484141790844
Epoch 2334
Epoch 2334 :: Batch 0/1
Batch Loss = 0.0007309484141790789
2334, epoch_train_loss=0.0007309484141790789
Epoch 2335
Epoch 2335 :: Batch 0/1
Batch Loss = 0.0007309484141790732
2335, epoch_train_loss=0.0007309484141790732
Epoch 2336
Epoch 2336 :: Batch 0/1
Batch Loss = 0.0007309484141790677
2336, epoch_train_loss=0.0007309484141790677
Epoch 2337
Epoch 2337 :: Batch 0/1
Batch Loss = 0.0007309484141790621
2337, epoch_train_loss=0.0007309484141790621
Epoch 2338
Epoch 2338 :: Batch 0/1
Batch Loss = 0.0007309484141790565
2338, epoch_train_loss=0.0007309484141790565
Epoch 2339
Epoch 2339 :: Batch 0/1
Batch Loss = 0.0007309484141790509
2339, epoch_train_loss=0.0007309484141790509
Epoch 2340
Epoch 2340 :: Batch 0/1
Batch Loss = 0.0007309484141790454
2340, epoch_train_loss=0.0007309484141790454
Epoch 2341
Epoch 2341 :: Batch 0/1
Batch Loss = 0.0007309484141790397
2341, epoch_train_loss=0.0007309484141790397
Epoch 2342
Epoch 2342 :: Batch 0/1
Batch Loss = 0.0007309484141790341
2342, epoch_train_loss=0.0007309484141790341
Epoch 2343
Epoch 2343 :: Batch 0/1
Batch Loss = 0.0007309484141790285
2343, epoch_train_loss=0.0007309484141790285
Epoch 2344
Epoch 2344 :: Batch 0/1
Batch Loss = 0.0007309484141790228
2344, epoch_train_loss=0.0007309484141790228
Epoch 2345
Epoch 2345 :: Batch 0/1
Batch Loss = 0.0007309484141790173
2345, epoch_train_loss=0.0007309484141790173
Epoch 2346
Epoch 2346 :: Batch 0/1
Batch Loss = 0.0007309484141790117
2346, epoch_train_loss=0.0007309484141790117
Epoch 2347
Epoch 2347 :: Batch 0/1
Batch Loss = 0.000730948414179006
2347, epoch_train_loss=0.000730948414179006
Epoch 2348
Epoch 2348 :: Batch 0/1
Batch Loss = 0.0007309484141790004
2348, epoch_train_loss=0.0007309484141790004
Epoch 2349
Epoch 2349 :: Batch 0/1
Batch Loss = 0.0007309484141789947
2349, epoch_train_loss=0.0007309484141789947
Epoch 2350
Epoch 2350 :: Batch 0/1
Batch Loss = 0.0007309484141789891
2350, epoch_train_loss=0.0007309484141789891
Epoch 2351
Epoch 2351 :: Batch 0/1
Batch Loss = 0.0007309484141789835
2351, epoch_train_loss=0.0007309484141789835
Epoch 2352
Epoch 2352 :: Batch 0/1
Batch Loss = 0.0007309484141789778
2352, epoch_train_loss=0.0007309484141789778
Epoch 2353
Epoch 2353 :: Batch 0/1
Batch Loss = 0.0007309484141789724
2353, epoch_train_loss=0.0007309484141789724
Epoch 2354
Epoch 2354 :: Batch 0/1
Batch Loss = 0.0007309484141789666
2354, epoch_train_loss=0.0007309484141789666
Epoch 2355
Epoch 2355 :: Batch 0/1
Batch Loss = 0.0007309484141789608
2355, epoch_train_loss=0.0007309484141789608
Epoch 2356
Epoch 2356 :: Batch 0/1
Batch Loss = 0.0007309484141789553
2356, epoch_train_loss=0.0007309484141789553
Epoch 2357
Epoch 2357 :: Batch 0/1
Batch Loss = 0.0007309484141789496
2357, epoch_train_loss=0.0007309484141789496
Epoch 2358
Epoch 2358 :: Batch 0/1
Batch Loss = 0.0007309484141789441
2358, epoch_train_loss=0.0007309484141789441
Epoch 2359
Epoch 2359 :: Batch 0/1
Batch Loss = 0.0007309484141789385
2359, epoch_train_loss=0.0007309484141789385
Epoch 2360
Epoch 2360 :: Batch 0/1
Batch Loss = 0.0007309484141789326
2360, epoch_train_loss=0.0007309484141789326
Epoch 2361
Epoch 2361 :: Batch 0/1
Batch Loss = 0.000730948414178927
2361, epoch_train_loss=0.000730948414178927
Epoch 2362
Epoch 2362 :: Batch 0/1
Batch Loss = 0.0007309484141789212
2362, epoch_train_loss=0.0007309484141789212
Epoch 2363
Epoch 2363 :: Batch 0/1
Batch Loss = 0.0007309484141789157
2363, epoch_train_loss=0.0007309484141789157
Epoch 2364
Epoch 2364 :: Batch 0/1
Batch Loss = 0.00073094841417891
2364, epoch_train_loss=0.00073094841417891
Epoch 2365
Epoch 2365 :: Batch 0/1
Batch Loss = 0.0007309484141789043
2365, epoch_train_loss=0.0007309484141789043
Epoch 2366
Epoch 2366 :: Batch 0/1
Batch Loss = 0.0007309484141788987
2366, epoch_train_loss=0.0007309484141788987
Epoch 2367
Epoch 2367 :: Batch 0/1
Batch Loss = 0.0007309484141788929
2367, epoch_train_loss=0.0007309484141788929
Epoch 2368
Epoch 2368 :: Batch 0/1
Batch Loss = 0.0007309484141788872
2368, epoch_train_loss=0.0007309484141788872
Epoch 2369
Epoch 2369 :: Batch 0/1
Batch Loss = 0.0007309484141788817
2369, epoch_train_loss=0.0007309484141788817
Epoch 2370
Epoch 2370 :: Batch 0/1
Batch Loss = 0.0007309484141788759
2370, epoch_train_loss=0.0007309484141788759
Epoch 2371
Epoch 2371 :: Batch 0/1
Batch Loss = 0.0007309484141788703
2371, epoch_train_loss=0.0007309484141788703
Epoch 2372
Epoch 2372 :: Batch 0/1
Batch Loss = 0.0007309484141788645
2372, epoch_train_loss=0.0007309484141788645
Epoch 2373
Epoch 2373 :: Batch 0/1
Batch Loss = 0.0007309484141788589
2373, epoch_train_loss=0.0007309484141788589
Epoch 2374
Epoch 2374 :: Batch 0/1
Batch Loss = 0.0007309484141788531
2374, epoch_train_loss=0.0007309484141788531
Epoch 2375
Epoch 2375 :: Batch 0/1
Batch Loss = 0.0007309484141788474
2375, epoch_train_loss=0.0007309484141788474
Epoch 2376
Epoch 2376 :: Batch 0/1
Batch Loss = 0.0007309484141788418
2376, epoch_train_loss=0.0007309484141788418
Epoch 2377
Epoch 2377 :: Batch 0/1
Batch Loss = 0.0007309484141788361
2377, epoch_train_loss=0.0007309484141788361
Epoch 2378
Epoch 2378 :: Batch 0/1
Batch Loss = 0.0007309484141788304
2378, epoch_train_loss=0.0007309484141788304
Epoch 2379
Epoch 2379 :: Batch 0/1
Batch Loss = 0.0007309484141788246
2379, epoch_train_loss=0.0007309484141788246
Epoch 2380
Epoch 2380 :: Batch 0/1
Batch Loss = 0.0007309484141788189
2380, epoch_train_loss=0.0007309484141788189
Epoch 2381
Epoch 2381 :: Batch 0/1
Batch Loss = 0.0007309484141788132
2381, epoch_train_loss=0.0007309484141788132
Epoch 2382
Epoch 2382 :: Batch 0/1
Batch Loss = 0.0007309484141788075
2382, epoch_train_loss=0.0007309484141788075
Epoch 2383
Epoch 2383 :: Batch 0/1
Batch Loss = 0.0007309484141788019
2383, epoch_train_loss=0.0007309484141788019
Epoch 2384
Epoch 2384 :: Batch 0/1
Batch Loss = 0.0007309484141787961
2384, epoch_train_loss=0.0007309484141787961
Epoch 2385
Epoch 2385 :: Batch 0/1
Batch Loss = 0.0007309484141787904
2385, epoch_train_loss=0.0007309484141787904
Epoch 2386
Epoch 2386 :: Batch 0/1
Batch Loss = 0.0007309484141787846
2386, epoch_train_loss=0.0007309484141787846
Epoch 2387
Epoch 2387 :: Batch 0/1
Batch Loss = 0.0007309484141787788
2387, epoch_train_loss=0.0007309484141787788
Epoch 2388
Epoch 2388 :: Batch 0/1
Batch Loss = 0.0007309484141787731
2388, epoch_train_loss=0.0007309484141787731
Epoch 2389
Epoch 2389 :: Batch 0/1
Batch Loss = 0.0007309484141787674
2389, epoch_train_loss=0.0007309484141787674
Epoch 2390
Epoch 2390 :: Batch 0/1
Batch Loss = 0.0007309484141787615
2390, epoch_train_loss=0.0007309484141787615
Epoch 2391
Epoch 2391 :: Batch 0/1
Batch Loss = 0.0007309484141787558
2391, epoch_train_loss=0.0007309484141787558
Epoch 2392
Epoch 2392 :: Batch 0/1
Batch Loss = 0.00073094841417875
2392, epoch_train_loss=0.00073094841417875
Epoch 2393
Epoch 2393 :: Batch 0/1
Batch Loss = 0.0007309484141787444
2393, epoch_train_loss=0.0007309484141787444
Epoch 2394
Epoch 2394 :: Batch 0/1
Batch Loss = 0.0007309484141787387
2394, epoch_train_loss=0.0007309484141787387
Epoch 2395
Epoch 2395 :: Batch 0/1
Batch Loss = 0.0007309484141787329
2395, epoch_train_loss=0.0007309484141787329
Epoch 2396
Epoch 2396 :: Batch 0/1
Batch Loss = 0.0007309484141787272
2396, epoch_train_loss=0.0007309484141787272
Epoch 2397
Epoch 2397 :: Batch 0/1
Batch Loss = 0.0007309484141787213
2397, epoch_train_loss=0.0007309484141787213
Epoch 2398
Epoch 2398 :: Batch 0/1
Batch Loss = 0.0007309484141787154
2398, epoch_train_loss=0.0007309484141787154
Epoch 2399
Epoch 2399 :: Batch 0/1
Batch Loss = 0.0007309484141787097
2399, epoch_train_loss=0.0007309484141787097
Epoch 2400
Epoch 2400 :: Batch 0/1
Batch Loss = 0.000730948414178704
2400, epoch_train_loss=0.000730948414178704
Epoch 2401
Epoch 2401 :: Batch 0/1
Batch Loss = 0.0007309484141786982
2401, epoch_train_loss=0.0007309484141786982
Epoch 2402
Epoch 2402 :: Batch 0/1
Batch Loss = 0.0007309484141786925
2402, epoch_train_loss=0.0007309484141786925
Epoch 2403
Epoch 2403 :: Batch 0/1
Batch Loss = 0.0007309484141786867
2403, epoch_train_loss=0.0007309484141786867
Epoch 2404
Epoch 2404 :: Batch 0/1
Batch Loss = 0.000730948414178681
2404, epoch_train_loss=0.000730948414178681
Epoch 2405
Epoch 2405 :: Batch 0/1
Batch Loss = 0.0007309484141786751
2405, epoch_train_loss=0.0007309484141786751
Epoch 2406
Epoch 2406 :: Batch 0/1
Batch Loss = 0.0007309484141786694
2406, epoch_train_loss=0.0007309484141786694
Epoch 2407
Epoch 2407 :: Batch 0/1
Batch Loss = 0.0007309484141786634
2407, epoch_train_loss=0.0007309484141786634
Epoch 2408
Epoch 2408 :: Batch 0/1
Batch Loss = 0.0007309484141786577
2408, epoch_train_loss=0.0007309484141786577
Epoch 2409
Epoch 2409 :: Batch 0/1
Batch Loss = 0.0007309484141786519
2409, epoch_train_loss=0.0007309484141786519
Epoch 2410
Epoch 2410 :: Batch 0/1
Batch Loss = 0.0007309484141786461
2410, epoch_train_loss=0.0007309484141786461
Epoch 2411
Epoch 2411 :: Batch 0/1
Batch Loss = 0.0007309484141786403
2411, epoch_train_loss=0.0007309484141786403
Epoch 2412
Epoch 2412 :: Batch 0/1
Batch Loss = 0.0007309484141786345
2412, epoch_train_loss=0.0007309484141786345
Epoch 2413
Epoch 2413 :: Batch 0/1
Batch Loss = 0.0007309484141786287
2413, epoch_train_loss=0.0007309484141786287
Epoch 2414
Epoch 2414 :: Batch 0/1
Batch Loss = 0.0007309484141786229
2414, epoch_train_loss=0.0007309484141786229
Epoch 2415
Epoch 2415 :: Batch 0/1
Batch Loss = 0.000730948414178617
2415, epoch_train_loss=0.000730948414178617
Epoch 2416
Epoch 2416 :: Batch 0/1
Batch Loss = 0.0007309484141786113
2416, epoch_train_loss=0.0007309484141786113
Epoch 2417
Epoch 2417 :: Batch 0/1
Batch Loss = 0.0007309484141786054
2417, epoch_train_loss=0.0007309484141786054
Epoch 2418
Epoch 2418 :: Batch 0/1
Batch Loss = 0.0007309484141785995
2418, epoch_train_loss=0.0007309484141785995
Epoch 2419
Epoch 2419 :: Batch 0/1
Batch Loss = 0.0007309484141785937
2419, epoch_train_loss=0.0007309484141785937
Epoch 2420
Epoch 2420 :: Batch 0/1
Batch Loss = 0.000730948414178588
2420, epoch_train_loss=0.000730948414178588
Epoch 2421
Epoch 2421 :: Batch 0/1
Batch Loss = 0.0007309484141785822
2421, epoch_train_loss=0.0007309484141785822
Epoch 2422
Epoch 2422 :: Batch 0/1
Batch Loss = 0.0007309484141785761
2422, epoch_train_loss=0.0007309484141785761
Epoch 2423
Epoch 2423 :: Batch 0/1
Batch Loss = 0.0007309484141785704
2423, epoch_train_loss=0.0007309484141785704
Epoch 2424
Epoch 2424 :: Batch 0/1
Batch Loss = 0.0007309484141785645
2424, epoch_train_loss=0.0007309484141785645
Epoch 2425
Epoch 2425 :: Batch 0/1
Batch Loss = 0.0007309484141785587
2425, epoch_train_loss=0.0007309484141785587
Epoch 2426
Epoch 2426 :: Batch 0/1
Batch Loss = 0.0007309484141785529
2426, epoch_train_loss=0.0007309484141785529
Epoch 2427
Epoch 2427 :: Batch 0/1
Batch Loss = 0.0007309484141785469
2427, epoch_train_loss=0.0007309484141785469
Epoch 2428
Epoch 2428 :: Batch 0/1
Batch Loss = 0.0007309484141785411
2428, epoch_train_loss=0.0007309484141785411
Epoch 2429
Epoch 2429 :: Batch 0/1
Batch Loss = 0.0007309484141785353
2429, epoch_train_loss=0.0007309484141785353
Epoch 2430
Epoch 2430 :: Batch 0/1
Batch Loss = 0.0007309484141785294
2430, epoch_train_loss=0.0007309484141785294
Epoch 2431
Epoch 2431 :: Batch 0/1
Batch Loss = 0.0007309484141785237
2431, epoch_train_loss=0.0007309484141785237
Epoch 2432
Epoch 2432 :: Batch 0/1
Batch Loss = 0.0007309484141785177
2432, epoch_train_loss=0.0007309484141785177
Epoch 2433
Epoch 2433 :: Batch 0/1
Batch Loss = 0.0007309484141785118
2433, epoch_train_loss=0.0007309484141785118
Epoch 2434
Epoch 2434 :: Batch 0/1
Batch Loss = 0.0007309484141785059
2434, epoch_train_loss=0.0007309484141785059
Epoch 2435
Epoch 2435 :: Batch 0/1
Batch Loss = 0.0007309484141785
2435, epoch_train_loss=0.0007309484141785
Epoch 2436
Epoch 2436 :: Batch 0/1
Batch Loss = 0.0007309484141784942
2436, epoch_train_loss=0.0007309484141784942
Epoch 2437
Epoch 2437 :: Batch 0/1
Batch Loss = 0.0007309484141784884
2437, epoch_train_loss=0.0007309484141784884
Epoch 2438
Epoch 2438 :: Batch 0/1
Batch Loss = 0.0007309484141784823
2438, epoch_train_loss=0.0007309484141784823
Epoch 2439
Epoch 2439 :: Batch 0/1
Batch Loss = 0.0007309484141784765
2439, epoch_train_loss=0.0007309484141784765
Epoch 2440
Epoch 2440 :: Batch 0/1
Batch Loss = 0.0007309484141784706
2440, epoch_train_loss=0.0007309484141784706
Epoch 2441
Epoch 2441 :: Batch 0/1
Batch Loss = 0.0007309484141784648
2441, epoch_train_loss=0.0007309484141784648
Epoch 2442
Epoch 2442 :: Batch 0/1
Batch Loss = 0.0007309484141784588
2442, epoch_train_loss=0.0007309484141784588
Epoch 2443
Epoch 2443 :: Batch 0/1
Batch Loss = 0.0007309484141784529
2443, epoch_train_loss=0.0007309484141784529
Epoch 2444
Epoch 2444 :: Batch 0/1
Batch Loss = 0.000730948414178447
2444, epoch_train_loss=0.000730948414178447
Epoch 2445
Epoch 2445 :: Batch 0/1
Batch Loss = 0.0007309484141784411
2445, epoch_train_loss=0.0007309484141784411
Epoch 2446
Epoch 2446 :: Batch 0/1
Batch Loss = 0.0007309484141784352
2446, epoch_train_loss=0.0007309484141784352
Epoch 2447
Epoch 2447 :: Batch 0/1
Batch Loss = 0.0007309484141784294
2447, epoch_train_loss=0.0007309484141784294
Epoch 2448
Epoch 2448 :: Batch 0/1
Batch Loss = 0.0007309484141784234
2448, epoch_train_loss=0.0007309484141784234
Epoch 2449
Epoch 2449 :: Batch 0/1
Batch Loss = 0.0007309484141784175
2449, epoch_train_loss=0.0007309484141784175
Epoch 2450
Epoch 2450 :: Batch 0/1
Batch Loss = 0.0007309484141784117
2450, epoch_train_loss=0.0007309484141784117
Epoch 2451
Epoch 2451 :: Batch 0/1
Batch Loss = 0.0007309484141784057
2451, epoch_train_loss=0.0007309484141784057
Epoch 2452
Epoch 2452 :: Batch 0/1
Batch Loss = 0.0007309484141783996
2452, epoch_train_loss=0.0007309484141783996
Epoch 2453
Epoch 2453 :: Batch 0/1
Batch Loss = 0.0007309484141783938
2453, epoch_train_loss=0.0007309484141783938
Epoch 2454
Epoch 2454 :: Batch 0/1
Batch Loss = 0.0007309484141783879
2454, epoch_train_loss=0.0007309484141783879
Epoch 2455
Epoch 2455 :: Batch 0/1
Batch Loss = 0.000730948414178382
2455, epoch_train_loss=0.000730948414178382
Epoch 2456
Epoch 2456 :: Batch 0/1
Batch Loss = 0.0007309484141783759
2456, epoch_train_loss=0.0007309484141783759
Epoch 2457
Epoch 2457 :: Batch 0/1
Batch Loss = 0.0007309484141783699
2457, epoch_train_loss=0.0007309484141783699
Epoch 2458
Epoch 2458 :: Batch 0/1
Batch Loss = 0.0007309484141783642
2458, epoch_train_loss=0.0007309484141783642
Epoch 2459
Epoch 2459 :: Batch 0/1
Batch Loss = 0.0007309484141783582
2459, epoch_train_loss=0.0007309484141783582
Epoch 2460
Epoch 2460 :: Batch 0/1
Batch Loss = 0.0007309484141783521
2460, epoch_train_loss=0.0007309484141783521
Epoch 2461
Epoch 2461 :: Batch 0/1
Batch Loss = 0.0007309484141783462
2461, epoch_train_loss=0.0007309484141783462
Epoch 2462
Epoch 2462 :: Batch 0/1
Batch Loss = 0.0007309484141783403
2462, epoch_train_loss=0.0007309484141783403
Epoch 2463
Epoch 2463 :: Batch 0/1
Batch Loss = 0.0007309484141783342
2463, epoch_train_loss=0.0007309484141783342
Epoch 2464
Epoch 2464 :: Batch 0/1
Batch Loss = 0.0007309484141783284
2464, epoch_train_loss=0.0007309484141783284
Epoch 2465
Epoch 2465 :: Batch 0/1
Batch Loss = 0.0007309484141783223
2465, epoch_train_loss=0.0007309484141783223
Epoch 2466
Epoch 2466 :: Batch 0/1
Batch Loss = 0.0007309484141783165
2466, epoch_train_loss=0.0007309484141783165
Epoch 2467
Epoch 2467 :: Batch 0/1
Batch Loss = 0.0007309484141783104
2467, epoch_train_loss=0.0007309484141783104
Epoch 2468
Epoch 2468 :: Batch 0/1
Batch Loss = 0.0007309484141783044
2468, epoch_train_loss=0.0007309484141783044
Epoch 2469
Epoch 2469 :: Batch 0/1
Batch Loss = 0.0007309484141782985
2469, epoch_train_loss=0.0007309484141782985
Epoch 2470
Epoch 2470 :: Batch 0/1
Batch Loss = 0.0007309484141782924
2470, epoch_train_loss=0.0007309484141782924
Epoch 2471
Epoch 2471 :: Batch 0/1
Batch Loss = 0.0007309484141782864
2471, epoch_train_loss=0.0007309484141782864
Epoch 2472
Epoch 2472 :: Batch 0/1
Batch Loss = 0.0007309484141782805
2472, epoch_train_loss=0.0007309484141782805
Epoch 2473
Epoch 2473 :: Batch 0/1
Batch Loss = 0.0007309484141782745
2473, epoch_train_loss=0.0007309484141782745
Epoch 2474
Epoch 2474 :: Batch 0/1
Batch Loss = 0.0007309484141782684
2474, epoch_train_loss=0.0007309484141782684
Epoch 2475
Epoch 2475 :: Batch 0/1
Batch Loss = 0.0007309484141782626
2475, epoch_train_loss=0.0007309484141782626
Epoch 2476
Epoch 2476 :: Batch 0/1
Batch Loss = 0.0007309484141782565
2476, epoch_train_loss=0.0007309484141782565
Epoch 2477
Epoch 2477 :: Batch 0/1
Batch Loss = 0.0007309484141782504
2477, epoch_train_loss=0.0007309484141782504
Epoch 2478
Epoch 2478 :: Batch 0/1
Batch Loss = 0.0007309484141782445
2478, epoch_train_loss=0.0007309484141782445
Epoch 2479
Epoch 2479 :: Batch 0/1
Batch Loss = 0.0007309484141782385
2479, epoch_train_loss=0.0007309484141782385
Epoch 2480
Epoch 2480 :: Batch 0/1
Batch Loss = 0.0007309484141782324
2480, epoch_train_loss=0.0007309484141782324
Epoch 2481
Epoch 2481 :: Batch 0/1
Batch Loss = 0.0007309484141782265
2481, epoch_train_loss=0.0007309484141782265
Epoch 2482
Epoch 2482 :: Batch 0/1
Batch Loss = 0.0007309484141782205
2482, epoch_train_loss=0.0007309484141782205
Epoch 2483
Epoch 2483 :: Batch 0/1
Batch Loss = 0.0007309484141782145
2483, epoch_train_loss=0.0007309484141782145
Epoch 2484
Epoch 2484 :: Batch 0/1
Batch Loss = 0.0007309484141782084
2484, epoch_train_loss=0.0007309484141782084
Epoch 2485
Epoch 2485 :: Batch 0/1
Batch Loss = 0.0007309484141782024
2485, epoch_train_loss=0.0007309484141782024
Epoch 2486
Epoch 2486 :: Batch 0/1
Batch Loss = 0.0007309484141781962
2486, epoch_train_loss=0.0007309484141781962
Epoch 2487
Epoch 2487 :: Batch 0/1
Batch Loss = 0.0007309484141781903
2487, epoch_train_loss=0.0007309484141781903
Epoch 2488
Epoch 2488 :: Batch 0/1
Batch Loss = 0.0007309484141781843
2488, epoch_train_loss=0.0007309484141781843
Epoch 2489
Epoch 2489 :: Batch 0/1
Batch Loss = 0.0007309484141781783
2489, epoch_train_loss=0.0007309484141781783
Epoch 2490
Epoch 2490 :: Batch 0/1
Batch Loss = 0.0007309484141781722
2490, epoch_train_loss=0.0007309484141781722
Epoch 2491
Epoch 2491 :: Batch 0/1
Batch Loss = 0.0007309484141781662
2491, epoch_train_loss=0.0007309484141781662
Epoch 2492
Epoch 2492 :: Batch 0/1
Batch Loss = 0.00073094841417816
2492, epoch_train_loss=0.00073094841417816
Epoch 2493
Epoch 2493 :: Batch 0/1
Batch Loss = 0.000730948414178154
2493, epoch_train_loss=0.000730948414178154
Epoch 2494
Epoch 2494 :: Batch 0/1
Batch Loss = 0.0007309484141781481
2494, epoch_train_loss=0.0007309484141781481
Epoch 2495
Epoch 2495 :: Batch 0/1
Batch Loss = 0.0007309484141781418
2495, epoch_train_loss=0.0007309484141781418
Epoch 2496
Epoch 2496 :: Batch 0/1
Batch Loss = 0.0007309484141781358
2496, epoch_train_loss=0.0007309484141781358
Epoch 2497
Epoch 2497 :: Batch 0/1
Batch Loss = 0.0007309484141781299
2497, epoch_train_loss=0.0007309484141781299
Epoch 2498
Epoch 2498 :: Batch 0/1
Batch Loss = 0.0007309484141781238
2498, epoch_train_loss=0.0007309484141781238
Epoch 2499
Epoch 2499 :: Batch 0/1
Batch Loss = 0.0007309484141781177
2499, epoch_train_loss=0.0007309484141781177
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting "B3LYP_WITH_VWN5 = True" in pyscf_conf.py
  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '
Warning: <pyscf.gto.mole.Mole object at 0x7ffef8259e40> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef8259e40> in UKS object of <class 'pyscf.dft.uks.UKS'>
WARNING: External module "mldftdat" required for non-local descriptor use.
<pyscf.gto.mole.Mole object at 0x7ffef8259e40> [['P', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffef825bfa0> [['N', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffef825a770> [['H', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffef84275b0> [['Li', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffef84261d0> [['O', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffef83120e0> [['Cl', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffef8311990> [['Al', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffef8311fc0> [['S', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffef83111e0> [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef8311240> [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef8313820> [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef8313010> [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]] 3
<pyscf.gto.mole.Mole object at 0x7ffef83102e0> [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]] 3
<pyscf.gto.mole.Mole object at 0x7ffef8310430> [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef8311090> [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef8311de0> [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef8310b80> [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]] 4
<pyscf.gto.mole.Mole object at 0x7ffef8310940> [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef8312e30> [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef83105b0> [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef824c1f0> [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef824ee30> [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]] 4
<pyscf.gto.mole.Mole object at 0x7ffef824eec0> [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]] 4
<pyscf.gto.mole.Mole object at 0x7ffef824ec50> [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef81b32e0> [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]] 5
<pyscf.gto.mole.Mole object at 0x7ffef81b09d0> [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]] 3
<pyscf.gto.mole.Mole object at 0x7ffef81b3460> [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]] 5
mol:  [['P', array([0., 0., 0.])]]
converged SCF energy = -341.104145992717  <S^2> = 3.7502984  2S+1 = 4.0001492
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffef825bfa0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef825bfa0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00051022 -0.00019156 -0.00051334 ... -0.02830887 -0.02830887
 -0.02830887] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 16)
mol:  [['N', array([0., 0., 0.])]]
converged SCF energy = -54.5289742046674  <S^2> = 3.7524945  2S+1 = 4.0012471
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffef825a770> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef825a770> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5016, 22)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-3.60081838e-04 -1.08775305e-04 -1.31917160e-05 ... -2.74817476e-02
 -2.74817476e-02 -2.74817476e-02] = ,SCAN
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5016, 22)
rho shape (4, 4, 2, 5016)
rho_filt shape: (5016,)
get descriptors tdrho.shape=(5016, 16)
mol:  [['H', array([0., 0., 0.])]]
converged SCF energy = -0.49981298400854  <S^2> = 0.75  2S+1 = 2
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffef84275b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef84275b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 7, 7)
ao.shape (10, 2440, 7)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.92637305e-09 -1.31700808e-07 -9.61527370e-06 ... -7.35522754e-16
 -7.35522754e-16 -7.35522754e-16] = ,SCAN
get_data, dm shape = (2, 7, 7)
ao_eval.shape=(4, 2440, 7)
rho shape (4, 4, 2, 2440)
rho_filt shape: (2440,)
get descriptors tdrho.shape=(2440, 16)
mol:  [['Li', array([0., 0., 0.])]]
converged SCF energy = -7.46006188627842  <S^2> = 0.75000049  2S+1 = 2.0000005
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffef84261d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef84261d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 4592, 22)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-5.31884219e-04 -2.81911891e-04 -2.81911891e-04 ... -1.27154711e-05
 -2.64861768e-02 -2.64861768e-02] = ,SCAN
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 4592, 22)
rho shape (4, 4, 2, 4592)
rho_filt shape: (4592,)
get descriptors tdrho.shape=(4592, 16)
mol:  [['O', array([0., 0., 0.])]]
converged SCF energy = -75.0033831257731  <S^2> = 2.0027438  2S+1 = 3.0018286
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffef83120e0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef83120e0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5040, 22)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.51292208e-04 -2.76349338e-05 -1.45401287e-06 ... -2.76158583e-02
 -2.76158583e-02 -2.76158583e-02] = ,SCAN
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5040, 22)
rho shape (4, 4, 2, 5040)
rho_filt shape: (5040,)
get descriptors tdrho.shape=(5040, 16)
mol:  [['Cl', array([0., 0., 0.])]]
converged SCF energy = -459.957577121203  <S^2> = 0.75161941  2S+1 = 2.0016188
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffef8311990> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef8311990> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6152, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.81979827e-04 -2.46352783e-04 -8.35577351e-05 ... -2.84484389e-02
 -2.84484389e-02 -2.84484389e-02] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6152, 30)
rho shape (4, 4, 2, 6152)
rho_filt shape: (6152,)
get descriptors tdrho.shape=(6152, 16)
mol:  [['Al', array([0., 0., 0.])]]
converged SCF energy = -242.226561009122  <S^2> = 0.75226414  2S+1 = 2.0022629
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffef8311fc0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef8311fc0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6088, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.31761367e-03 -1.43605283e-03 -7.25113459e-04 ... -2.64647712e-05
 -2.75294543e-04 -2.31673064e-05] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6088, 30)
rho shape (4, 4, 2, 6088)
rho_filt shape: (6088,)
get descriptors tdrho.shape=(6088, 16)
mol:  [['S', array([0., 0., 0.])]]
converged SCF energy = -397.938786807103  <S^2> = 2.0022329  2S+1 = 3.0014882
Warning: <pyscf.gto.mole.Mole object at 0x7ffef83111e0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef83111e0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00038746 -0.00017487 -0.00023373 ... -0.02838402 -0.02838402
 -0.02838402] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 16)
mol:  [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]]
converged SCF energy = -1.16580491182913  <S^2> = -4.4408921e-16  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffef8311240> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef8311240> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 14, 14)
ao.shape (10, 4776, 14)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.43725660e-05 -1.02204686e-06 -4.05575842e-05 ... -2.36278434e-02
 -2.36278434e-02 -2.36278434e-02] = ,SCAN
get_data, dm shape = (2, 14, 14)
ao_eval.shape=(4, 4776, 14)
rho shape (4, 4, 2, 4776)
rho_filt shape: (4776,)
get descriptors tdrho.shape=(4776, 16)
mol:  [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]]
converged SCF energy = -109.439263799566  <S^2> = 9.7699626e-15  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffef8313820> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef8313820> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9848, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.89629699e-05 -2.76172354e-04 -7.59017288e-05 ... -7.34654212e-06
 -7.34654212e-06 -2.89629699e-05] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9848, 44)
rho shape (4, 4, 2, 9848)
rho_filt shape: (9848,)
get descriptors tdrho.shape=(9848, 16)
mol:  [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]]
converged SCF energy = -107.339357395734  <S^2> = 1.7763568e-15  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffef8313010> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef8313010> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9752, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00043469 -0.00024024 -0.00035532 ... -0.00047537 -0.03728133
 -0.03728133] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9752, 44)
rho shape (4, 4, 2, 9752)
rho_filt shape: (9752,)
get descriptors tdrho.shape=(9752, 16)
mol:  [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]]
converged SCF energy = -93.337792446513  <S^2> = 4.0072834e-10  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffef83102e0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef83102e0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 12256, 51)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-9.02468888e-05 -7.92694658e-06 -9.80568469e-06 ... -4.33714150e-02
 -4.33714150e-02 -4.33714150e-02] = ,SCAN
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 12256, 51)
rho shape (4, 4, 2, 12256)
rho_filt shape: (12256,)
get descriptors tdrho.shape=(12256, 16)
mol:  [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]]
converged SCF energy = -188.456965322844  <S^2> = 2.1316282e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffef8310430> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef8310430> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 66, 66)
ao.shape (10, 14920, 66)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-7.48187338e-05 -6.19475249e-05 -2.61742784e-04 ... -8.70042314e-07
 -2.73391097e-02 -2.73391097e-02] = ,SCAN
get_data, dm shape = (2, 66, 66)
ao_eval.shape=(4, 14920, 66)
rho shape (4, 4, 2, 14920)
rho_filt shape: (14920,)
get descriptors tdrho.shape=(14920, 16)
mol:  [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]]
converged SCF energy = -920.005608888959  <S^2> = 5.0448534e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffef8311090> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef8311090> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12208, 60)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00051559 -0.00027432 -0.00088583 ... -0.00027432 -0.04174728
 -0.04174728] = ,SCAN
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12208, 60)
rho shape (4, 4, 2, 12208)
rho_filt shape: (12208,)
get descriptors tdrho.shape=(12208, 16)
mol:  [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]]
converged SCF energy = -199.394370591172  <S^2> = 1.2256862e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffef8311de0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef8311de0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9824, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-5.53951178e-05 -5.93507199e-06 -3.10072916e-04 ... -5.94325581e-02
 -5.94325581e-02 -5.94325581e-02] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9824, 44)
rho shape (4, 4, 2, 9824)
rho_filt shape: (9824,)
get descriptors tdrho.shape=(9824, 16)
mol:  [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]]
converged SCF energy = -150.214894457621  <S^2> = 1.0018598  2S+1 = 2.2377309
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffef8310b80> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef8310b80> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9912, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.53859026e-04 -2.90415469e-05 -1.58754300e-06 ... -4.22396696e-02
 -4.22396696e-02 -4.22396696e-02] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9912, 44)
rho shape (4, 4, 2, 9912)
rho_filt shape: (9912,)
get descriptors tdrho.shape=(9912, 16)
mol:  [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]]
converged SCF energy = -77.2435048346373  <S^2> = 1.0658141e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffef8310940> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef8310940> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 58, 58)
ao.shape (10, 15208, 58)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.72190712e-05 -2.72190712e-05 -2.84904833e-04 ... -1.08108260e-05
 -1.03072478e-05 -1.03072478e-05] = ,SCAN
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15208, 58)
rho shape (4, 4, 2, 15208)
rho_filt shape: (15208,)
get descriptors tdrho.shape=(15208, 16)
mol:  [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]]
converged SCF energy = -113.221335689652  <S^2> = 6.5814021e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffef8312e30> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef8312e30> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 10040, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00015688 -0.00024669 -0.00068269 ... -0.03791166 -0.03791166
 -0.03791166] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 10040, 44)
rho shape (4, 4, 2, 10040)
rho_filt shape: (10040,)
get descriptors tdrho.shape=(10040, 16)
mol:  [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]]
converged SCF energy = -460.624592374077  <S^2> = 7.9936058e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffef83105b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef83105b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 37, 37)
ao.shape (10, 8552, 37)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-7.28500681e-05 -5.65091132e-06 -7.37932132e-06 ... -4.76689214e-02
 -4.76689214e-02 -4.76689214e-02] = ,SCAN
get_data, dm shape = (2, 37, 37)
ao_eval.shape=(4, 8552, 37)
rho shape (4, 4, 2, 8552)
rho_filt shape: (8552,)
get descriptors tdrho.shape=(8552, 16)
mol:  [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]]
converged SCF energy = -8.04458854018922  <S^2> = 7.4384943e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffef824c1f0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef824c1f0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 29, 29)
ao.shape (10, 6936, 29)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.0003863  -0.00040095 -0.00040095 ... -0.0213199  -0.0213199
 -0.0213199 ] = ,SCAN
get_data, dm shape = (2, 29, 29)
ao_eval.shape=(4, 6936, 29)
rho shape (4, 4, 2, 6936)
rho_filt shape: (6936,)
get descriptors tdrho.shape=(6936, 16)
mol:  [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]]
converged SCF energy = -324.340512506578  <S^2> = 1.5859314e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffef824ee30> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef824ee30> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 11536, 60)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00088473 -0.00088473 -0.00116894 ... -0.00088473 -0.00088473
 -0.00116894] = ,SCAN
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 11536, 60)
rho shape (4, 4, 2, 11536)
rho_filt shape: (11536,)
get descriptors tdrho.shape=(11536, 16)
mol:  [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]]
converged SCF energy = -1622.57507845814  <S^2> = 8.31335e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffef824eec0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef824eec0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 120, 120)
ao.shape (10, 24512, 120)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.91408540e-05 -1.46971271e-04 -1.08734417e-03 ... -2.81566369e-02
 -2.81566369e-02 -2.81566369e-02] = ,SCAN
get_data, dm shape = (2, 120, 120)
ao_eval.shape=(4, 24512, 120)
rho shape (4, 4, 2, 24512)
rho_filt shape: (24512,)
get descriptors tdrho.shape=(24512, 16)
mol:  [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]]
converged SCF energy = -342.979728469576  <S^2> = 2.5385916e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffef824ec50> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef824ec50> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 13096, 51)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.39373335e-04 -1.31641332e-04 -1.15950750e-05 ... -7.32416564e-02
 -7.32416564e-02 -7.32416564e-02] = ,SCAN
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 13096, 51)
rho shape (4, 4, 2, 13096)
rho_filt shape: (13096,)
get descriptors tdrho.shape=(13096, 16)
mol:  [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]]
converged SCF energy = -578.565336069392  <S^2> = 1.0034705  2S+1 = 2.2391699
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffef81b32e0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef81b32e0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12384, 60)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.84677944e-05 -7.80558648e-05 -7.80526069e-05 ... -2.92531323e-02
 -2.92531323e-02 -2.92531323e-02] = ,SCAN
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12384, 60)
rho shape (4, 4, 2, 12384)
rho_filt shape: (12384,)
get descriptors tdrho.shape=(12384, 16)
mol:  [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]]
converged SCF energy = -40.4598214864075  <S^2> = 3.170797e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffef81b09d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef81b09d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 50, 50)
ao.shape (10, 13936, 50)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.56165538e-04 -7.34744214e-05 -5.30574304e-06 ... -7.93995702e-06
 -7.93995702e-06 -7.93995702e-06] = ,SCAN
get_data, dm shape = (2, 50, 50)
ao_eval.shape=(4, 13936, 50)
rho shape (4, 4, 2, 13936)
rho_filt shape: (13936,)
get descriptors tdrho.shape=(13936, 16)
mol:  [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]]
converged SCF energy = -39.0756147483504  <S^2> = 6.1946004e-12  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffef81b3460> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef81b3460> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 36, 36)
ao.shape (10, 9656, 36)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.28290753e-04 -1.41305449e-05 -6.13700492e-05 ... -2.47993463e-02
 -2.47993463e-02 -2.47993463e-02] = ,SCAN
get_data, dm shape = (2, 36, 36)
ao_eval.shape=(4, 9656, 36)
rho shape (4, 4, 2, 9656)
rho_filt shape: (9656,)
get descriptors tdrho.shape=(9656, 16)
mol:  [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]]
converged SCF energy = -291.71927243782  <S^2> = 1.3152146e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
New DM shape: (2, 58, 58)
ao.shape (10, 15256, 58)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.45512011e-04 -7.12775692e-05 -5.48666345e-06 ... -6.02613084e-06
 -6.02613084e-06 -6.02613084e-06] = ,SCAN
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15256, 58)
rho shape (4, 4, 2, 15256)
rho_filt shape: (15256,)
get descriptors tdrho.shape=(15256, 16)
PRE NAN FILT: tFxc.shape=(236999,), tdrho.shape=(236999, 16)
nan_filt_rho.shape=(236999,)
nan_filt_fxc.shape=(236999,)
tFxc.shape=(236999,), tdrho.shape=(236999, 16)
inp[0].shape = (236999, 16)
Epoch 0
Epoch 0 :: Batch 0/1
Batch Loss = 948412.6105632321
0, epoch_train_loss=948412.6105632321
Epoch 1
Epoch 1 :: Batch 0/1
Batch Loss = 0.23800455765516615
1, epoch_train_loss=0.23800455765516615
Epoch 2
Epoch 2 :: Batch 0/1
Batch Loss = 0.07035390423481776
2, epoch_train_loss=0.07035390423481776
Epoch 3
Epoch 3 :: Batch 0/1
Batch Loss = 0.02104946224034254
3, epoch_train_loss=0.02104946224034254
Epoch 4
Epoch 4 :: Batch 0/1
Batch Loss = 0.006440715410832197
4, epoch_train_loss=0.006440715410832197
Epoch 5
Epoch 5 :: Batch 0/1
Batch Loss = 0.0019255633174086692
5, epoch_train_loss=0.0019255633174086692
Epoch 6
Epoch 6 :: Batch 0/1
Batch Loss = 0.0009451124942227813
6, epoch_train_loss=0.0009451124942227813
Epoch 7
Epoch 7 :: Batch 0/1
Batch Loss = 0.0007701240096567991
7, epoch_train_loss=0.0007701240096567991
Epoch 8
Epoch 8 :: Batch 0/1
Batch Loss = 0.0007384163327242666
8, epoch_train_loss=0.0007384163327242666
Epoch 9
Epoch 9 :: Batch 0/1
Batch Loss = 0.0007324486316081383
9, epoch_train_loss=0.0007324486316081383
Epoch 10
Epoch 10 :: Batch 0/1
Batch Loss = 0.0007313084454963183
10, epoch_train_loss=0.0007313084454963183
Epoch 11
Epoch 11 :: Batch 0/1
Batch Loss = 0.0007310895860123894
11, epoch_train_loss=0.0007310895860123894
Epoch 12
Epoch 12 :: Batch 0/1
Batch Loss = 0.0007310472681046405
12, epoch_train_loss=0.0007310472681046405
Epoch 13
Epoch 13 :: Batch 0/1
Batch Loss = 0.0007310389462271596
13, epoch_train_loss=0.0007310389462271596
Epoch 14
Epoch 14 :: Batch 0/1
Batch Loss = 0.0007310372636321824
14, epoch_train_loss=0.0007310372636321824
Epoch 15
Epoch 15 :: Batch 0/1
Batch Loss = 0.0007310369105003731
15, epoch_train_loss=0.0007310369105003731
Epoch 16
Epoch 16 :: Batch 0/1
Batch Loss = 0.0007310368329730548
16, epoch_train_loss=0.0007310368329730548
Epoch 17
Epoch 17 :: Batch 0/1
Batch Loss = 0.0007310368150581528
17, epoch_train_loss=0.0007310368150581528
Epoch 18
Epoch 18 :: Batch 0/1
Batch Loss = 0.0007310368106797203
18, epoch_train_loss=0.0007310368106797203
Epoch 19
Epoch 19 :: Batch 0/1
Batch Loss = 0.0007310368095437906
19, epoch_train_loss=0.0007310368095437906
Epoch 20
Epoch 20 :: Batch 0/1
Batch Loss = 0.0007310368092301638
20, epoch_train_loss=0.0007310368092301638
Epoch 21
Epoch 21 :: Batch 0/1
Batch Loss = 0.000731036809137874
21, epoch_train_loss=0.000731036809137874
Epoch 22
Epoch 22 :: Batch 0/1
Batch Loss = 0.000731036809108913
22, epoch_train_loss=0.000731036809108913
Epoch 23
Epoch 23 :: Batch 0/1
Batch Loss = 0.0007310368090992242
23, epoch_train_loss=0.0007310368090992242
Epoch 24
Epoch 24 :: Batch 0/1
Batch Loss = 0.0007310368090957718
24, epoch_train_loss=0.0007310368090957718
Epoch 25
Epoch 25 :: Batch 0/1
Batch Loss = 0.0007310368090944637
25, epoch_train_loss=0.0007310368090944637
Epoch 26
Epoch 26 :: Batch 0/1
Batch Loss = 0.0007310368090939377
26, epoch_train_loss=0.0007310368090939377
Epoch 27
Epoch 27 :: Batch 0/1
Batch Loss = 0.0007310368090937137
27, epoch_train_loss=0.0007310368090937137
Epoch 28
Epoch 28 :: Batch 0/1
Batch Loss = 0.000731036809093613
28, epoch_train_loss=0.000731036809093613
Epoch 29
Epoch 29 :: Batch 0/1
Batch Loss = 0.0007310368090935654
29, epoch_train_loss=0.0007310368090935654
Epoch 30
Epoch 30 :: Batch 0/1
Batch Loss = 0.0007310368090935415
30, epoch_train_loss=0.0007310368090935415
Epoch 31
Epoch 31 :: Batch 0/1
Batch Loss = 0.0007310368090935293
31, epoch_train_loss=0.0007310368090935293
Epoch 32
Epoch 32 :: Batch 0/1
Batch Loss = 0.0007310368090935226
32, epoch_train_loss=0.0007310368090935226
Epoch 33
Epoch 33 :: Batch 0/1
Batch Loss = 0.0007310368090935188
33, epoch_train_loss=0.0007310368090935188
Epoch 34
Epoch 34 :: Batch 0/1
Batch Loss = 0.0007310368090935165
34, epoch_train_loss=0.0007310368090935165
Epoch 35
Epoch 35 :: Batch 0/1
Batch Loss = 0.000731036809093515
35, epoch_train_loss=0.000731036809093515
Epoch 36
Epoch 36 :: Batch 0/1
Batch Loss = 0.0007310368090935142
36, epoch_train_loss=0.0007310368090935142
Epoch 37
Epoch 37 :: Batch 0/1
Batch Loss = 0.0007310368090935137
37, epoch_train_loss=0.0007310368090935137
Epoch 38
Epoch 38 :: Batch 0/1
Batch Loss = 0.0007310368090935132
38, epoch_train_loss=0.0007310368090935132
Epoch 39
Epoch 39 :: Batch 0/1
Batch Loss = 0.000731036809093513
39, epoch_train_loss=0.000731036809093513
Epoch 40
Epoch 40 :: Batch 0/1
Batch Loss = 0.000731036809093513
40, epoch_train_loss=0.000731036809093513
Epoch 41
Epoch 41 :: Batch 0/1
Batch Loss = 0.0007310368090935127
41, epoch_train_loss=0.0007310368090935127
Epoch 42
Epoch 42 :: Batch 0/1
Batch Loss = 0.0007310368090935127
42, epoch_train_loss=0.0007310368090935127
Epoch 43
Epoch 43 :: Batch 0/1
Batch Loss = 0.0007310368090935125
43, epoch_train_loss=0.0007310368090935125
Epoch 44
Epoch 44 :: Batch 0/1
Batch Loss = 0.0007310368090935125
44, epoch_train_loss=0.0007310368090935125
Epoch 45
Epoch 45 :: Batch 0/1
Batch Loss = 0.0007310368090935125
45, epoch_train_loss=0.0007310368090935125
Epoch 46
Epoch 46 :: Batch 0/1
Batch Loss = 0.0007310368090935125
46, epoch_train_loss=0.0007310368090935125
Epoch 47
Epoch 47 :: Batch 0/1
Batch Loss = 0.0007310368090935125
47, epoch_train_loss=0.0007310368090935125
Epoch 48
Epoch 48 :: Batch 0/1
Batch Loss = 0.0007310368090935125
48, epoch_train_loss=0.0007310368090935125
Epoch 49
Epoch 49 :: Batch 0/1
Batch Loss = 0.0007310368090935123
49, epoch_train_loss=0.0007310368090935123
Epoch 50
Epoch 50 :: Batch 0/1
Batch Loss = 0.0007310368090935123
50, epoch_train_loss=0.0007310368090935123
Epoch 51
Epoch 51 :: Batch 0/1
Batch Loss = 0.0007310368090935123
51, epoch_train_loss=0.0007310368090935123
Epoch 52
Epoch 52 :: Batch 0/1
Batch Loss = 0.0007310368090935123
52, epoch_train_loss=0.0007310368090935123
Epoch 53
Epoch 53 :: Batch 0/1
Batch Loss = 0.0007310368090935123
53, epoch_train_loss=0.0007310368090935123
Epoch 54
Epoch 54 :: Batch 0/1
Batch Loss = 0.0007310368090935123
54, epoch_train_loss=0.0007310368090935123
Epoch 55
Epoch 55 :: Batch 0/1
Batch Loss = 0.0007310368090935123
55, epoch_train_loss=0.0007310368090935123
Epoch 56
Epoch 56 :: Batch 0/1
Batch Loss = 0.0007310368090935123
56, epoch_train_loss=0.0007310368090935123
Epoch 57
Epoch 57 :: Batch 0/1
Batch Loss = 0.0007310368090935123
57, epoch_train_loss=0.0007310368090935123
Epoch 58
Epoch 58 :: Batch 0/1
Batch Loss = 0.0007310368090935123
58, epoch_train_loss=0.0007310368090935123
Epoch 59
Epoch 59 :: Batch 0/1
Batch Loss = 0.0007310368090935123
59, epoch_train_loss=0.0007310368090935123
Epoch 60
Epoch 60 :: Batch 0/1
Batch Loss = 0.0007310368090935123
60, epoch_train_loss=0.0007310368090935123
Epoch 61
Epoch 61 :: Batch 0/1
Batch Loss = 0.0007310368090935123
61, epoch_train_loss=0.0007310368090935123
Epoch 62
Epoch 62 :: Batch 0/1
Batch Loss = 0.0007310368090935123
62, epoch_train_loss=0.0007310368090935123
Epoch 63
Epoch 63 :: Batch 0/1
Batch Loss = 0.0007310368090935123
63, epoch_train_loss=0.0007310368090935123
Epoch 64
Epoch 64 :: Batch 0/1
Batch Loss = 0.0007310368090935123
64, epoch_train_loss=0.0007310368090935123
Epoch 65
Epoch 65 :: Batch 0/1
Batch Loss = 0.0007310368090935123
65, epoch_train_loss=0.0007310368090935123
Epoch 66
Epoch 66 :: Batch 0/1
Batch Loss = 0.0007310368090935123
66, epoch_train_loss=0.0007310368090935123
Epoch 67
Epoch 67 :: Batch 0/1
Batch Loss = 0.0007310368090935123
67, epoch_train_loss=0.0007310368090935123
Epoch 68
Epoch 68 :: Batch 0/1
Batch Loss = 0.0007310368090935123
68, epoch_train_loss=0.0007310368090935123
Epoch 69
Epoch 69 :: Batch 0/1
Batch Loss = 0.0007310368090935123
69, epoch_train_loss=0.0007310368090935123
Epoch 70
Epoch 70 :: Batch 0/1
Batch Loss = 0.0007310368090935123
70, epoch_train_loss=0.0007310368090935123
Epoch 71
Epoch 71 :: Batch 0/1
Batch Loss = 0.0007310368090935123
71, epoch_train_loss=0.0007310368090935123
Epoch 72
Epoch 72 :: Batch 0/1
Batch Loss = 0.0007310368090935123
72, epoch_train_loss=0.0007310368090935123
Epoch 73
Epoch 73 :: Batch 0/1
Batch Loss = 0.0007310368090935123
73, epoch_train_loss=0.0007310368090935123
Epoch 74
Epoch 74 :: Batch 0/1
Batch Loss = 0.0007310368090935123
74, epoch_train_loss=0.0007310368090935123
Epoch 75
Epoch 75 :: Batch 0/1
Batch Loss = 0.0007310368090935123
75, epoch_train_loss=0.0007310368090935123
Epoch 76
Epoch 76 :: Batch 0/1
Batch Loss = 0.0007310368090935123
76, epoch_train_loss=0.0007310368090935123
Epoch 77
Epoch 77 :: Batch 0/1
Batch Loss = 0.0007310368090935123
77, epoch_train_loss=0.0007310368090935123
Epoch 78
Epoch 78 :: Batch 0/1
Batch Loss = 0.0007310368090935123
78, epoch_train_loss=0.0007310368090935123
Epoch 79
Epoch 79 :: Batch 0/1
Batch Loss = 0.0007310368090935123
79, epoch_train_loss=0.0007310368090935123
Epoch 80
Epoch 80 :: Batch 0/1
Batch Loss = 0.0007310368090935123
80, epoch_train_loss=0.0007310368090935123
Epoch 81
Epoch 81 :: Batch 0/1
Batch Loss = 0.0007310368090935123
81, epoch_train_loss=0.0007310368090935123
Epoch 82
Epoch 82 :: Batch 0/1
Batch Loss = 0.0007310368090935123
82, epoch_train_loss=0.0007310368090935123
Epoch 83
Epoch 83 :: Batch 0/1
Batch Loss = 0.0007310368090935123
83, epoch_train_loss=0.0007310368090935123
Epoch 84
Epoch 84 :: Batch 0/1
Batch Loss = 0.0007310368090935123
84, epoch_train_loss=0.0007310368090935123
Epoch 85
Epoch 85 :: Batch 0/1
Batch Loss = 0.0007310368090935123
85, epoch_train_loss=0.0007310368090935123
Epoch 86
Epoch 86 :: Batch 0/1
Batch Loss = 0.0007310368090935123
86, epoch_train_loss=0.0007310368090935123
Epoch 87
Epoch 87 :: Batch 0/1
Batch Loss = 0.0007310368090935123
87, epoch_train_loss=0.0007310368090935123
Epoch 88
Epoch 88 :: Batch 0/1
Batch Loss = 0.0007310368090935123
88, epoch_train_loss=0.0007310368090935123
Epoch 89
Epoch 89 :: Batch 0/1
Batch Loss = 0.0007310368090935123
89, epoch_train_loss=0.0007310368090935123
Epoch 90
Epoch 90 :: Batch 0/1
Batch Loss = 0.0007310368090935123
90, epoch_train_loss=0.0007310368090935123
Epoch 91
Epoch 91 :: Batch 0/1
Batch Loss = 0.0007310368090935123
91, epoch_train_loss=0.0007310368090935123
Epoch 92
Epoch 92 :: Batch 0/1
Batch Loss = 0.0007310368090935123
92, epoch_train_loss=0.0007310368090935123
Epoch 93
Epoch 93 :: Batch 0/1
Batch Loss = 0.0007310368090935123
93, epoch_train_loss=0.0007310368090935123
Epoch 94
Epoch 94 :: Batch 0/1
Batch Loss = 0.0007310368090935123
94, epoch_train_loss=0.0007310368090935123
Epoch 95
Epoch 95 :: Batch 0/1
Batch Loss = 0.0007310368090935123
95, epoch_train_loss=0.0007310368090935123
Epoch 96
Epoch 96 :: Batch 0/1
Batch Loss = 0.0007310368090935123
96, epoch_train_loss=0.0007310368090935123
Epoch 97
Epoch 97 :: Batch 0/1
Batch Loss = 0.0007310368090935123
97, epoch_train_loss=0.0007310368090935123
Epoch 98
Epoch 98 :: Batch 0/1
Batch Loss = 0.0007310368090935123
98, epoch_train_loss=0.0007310368090935123
Epoch 99
Epoch 99 :: Batch 0/1
Batch Loss = 0.0007310368090935123
99, epoch_train_loss=0.0007310368090935123
Epoch 100
Epoch 100 :: Batch 0/1
Batch Loss = 0.0007310368090935123
100, epoch_train_loss=0.0007310368090935123
Epoch 101
Epoch 101 :: Batch 0/1
Batch Loss = 0.0007310368090935123
101, epoch_train_loss=0.0007310368090935123
Epoch 102
Epoch 102 :: Batch 0/1
Batch Loss = 0.0007310368090935123
102, epoch_train_loss=0.0007310368090935123
Epoch 103
Epoch 103 :: Batch 0/1
Batch Loss = 0.0007310368090935123
103, epoch_train_loss=0.0007310368090935123
Epoch 104
Epoch 104 :: Batch 0/1
Batch Loss = 0.0007310368090935123
104, epoch_train_loss=0.0007310368090935123
Epoch 105
Epoch 105 :: Batch 0/1
Batch Loss = 0.0007310368090935123
105, epoch_train_loss=0.0007310368090935123
Epoch 106
Epoch 106 :: Batch 0/1
Batch Loss = 0.0007310368090935123
106, epoch_train_loss=0.0007310368090935123
Epoch 107
Epoch 107 :: Batch 0/1
Batch Loss = 0.0007310368090935123
107, epoch_train_loss=0.0007310368090935123
Epoch 108
Epoch 108 :: Batch 0/1
Batch Loss = 0.0007310368090935123
108, epoch_train_loss=0.0007310368090935123
Epoch 109
Epoch 109 :: Batch 0/1
Batch Loss = 0.0007310368090935123
109, epoch_train_loss=0.0007310368090935123
Epoch 110
Epoch 110 :: Batch 0/1
Batch Loss = 0.0007310368090935123
110, epoch_train_loss=0.0007310368090935123
Epoch 111
Epoch 111 :: Batch 0/1
Batch Loss = 0.0007310368090935123
111, epoch_train_loss=0.0007310368090935123
Epoch 112
Epoch 112 :: Batch 0/1
Batch Loss = 0.0007310368090935123
112, epoch_train_loss=0.0007310368090935123
Epoch 113
Epoch 113 :: Batch 0/1
Batch Loss = 0.0007310368090935123
113, epoch_train_loss=0.0007310368090935123
Epoch 114
Epoch 114 :: Batch 0/1
Batch Loss = 0.0007310368090935123
114, epoch_train_loss=0.0007310368090935123
Epoch 115
Epoch 115 :: Batch 0/1
Batch Loss = 0.0007310368090935123
115, epoch_train_loss=0.0007310368090935123
Epoch 116
Epoch 116 :: Batch 0/1
Batch Loss = 0.0007310368090935123
116, epoch_train_loss=0.0007310368090935123
Epoch 117
Epoch 117 :: Batch 0/1
Batch Loss = 0.0007310368090935123
117, epoch_train_loss=0.0007310368090935123
Epoch 118
Epoch 118 :: Batch 0/1
Batch Loss = 0.0007310368090935123
118, epoch_train_loss=0.0007310368090935123
Epoch 119
Epoch 119 :: Batch 0/1
Batch Loss = 0.0007310368090935123
119, epoch_train_loss=0.0007310368090935123
Epoch 120
Epoch 120 :: Batch 0/1
Batch Loss = 0.0007310368090935123
120, epoch_train_loss=0.0007310368090935123
Epoch 121
Epoch 121 :: Batch 0/1
Batch Loss = 0.0007310368090935123
121, epoch_train_loss=0.0007310368090935123
Epoch 122
Epoch 122 :: Batch 0/1
Batch Loss = 0.0007310368090935123
122, epoch_train_loss=0.0007310368090935123
Epoch 123
Epoch 123 :: Batch 0/1
Batch Loss = 0.0007310368090935123
123, epoch_train_loss=0.0007310368090935123
Epoch 124
Epoch 124 :: Batch 0/1
Batch Loss = 0.0007310368090935123
124, epoch_train_loss=0.0007310368090935123
Epoch 125
Epoch 125 :: Batch 0/1
Batch Loss = 0.0007310368090935123
125, epoch_train_loss=0.0007310368090935123
Epoch 126
Epoch 126 :: Batch 0/1
Batch Loss = 0.0007310368090935123
126, epoch_train_loss=0.0007310368090935123
Epoch 127
Epoch 127 :: Batch 0/1
Batch Loss = 0.0007310368090935123
127, epoch_train_loss=0.0007310368090935123
Epoch 128
Epoch 128 :: Batch 0/1
Batch Loss = 0.0007310368090935123
128, epoch_train_loss=0.0007310368090935123
Epoch 129
Epoch 129 :: Batch 0/1
Batch Loss = 0.0007310368090935123
129, epoch_train_loss=0.0007310368090935123
Epoch 130
Epoch 130 :: Batch 0/1
Batch Loss = 0.0007310368090935123
130, epoch_train_loss=0.0007310368090935123
Epoch 131
Epoch 131 :: Batch 0/1
Batch Loss = 0.0007310368090935123
131, epoch_train_loss=0.0007310368090935123
Epoch 132
Epoch 132 :: Batch 0/1
Batch Loss = 0.0007310368090935123
132, epoch_train_loss=0.0007310368090935123
Epoch 133
Epoch 133 :: Batch 0/1
Batch Loss = 0.0007310368090935123
133, epoch_train_loss=0.0007310368090935123
Epoch 134
Epoch 134 :: Batch 0/1
Batch Loss = 0.0007310368090935123
134, epoch_train_loss=0.0007310368090935123
Epoch 135
Epoch 135 :: Batch 0/1
Batch Loss = 0.0007310368090935123
135, epoch_train_loss=0.0007310368090935123
Epoch 136
Epoch 136 :: Batch 0/1
Batch Loss = 0.0007310368090935123
136, epoch_train_loss=0.0007310368090935123
Epoch 137
Epoch 137 :: Batch 0/1
Batch Loss = 0.0007310368090935123
137, epoch_train_loss=0.0007310368090935123
Epoch 138
Epoch 138 :: Batch 0/1
Batch Loss = 0.0007310368090935123
138, epoch_train_loss=0.0007310368090935123
Epoch 139
Epoch 139 :: Batch 0/1
Batch Loss = 0.0007310368090935123
139, epoch_train_loss=0.0007310368090935123
Epoch 140
Epoch 140 :: Batch 0/1
Batch Loss = 0.0007310368090935123
140, epoch_train_loss=0.0007310368090935123
Epoch 141
Epoch 141 :: Batch 0/1
Batch Loss = 0.0007310368090935123
141, epoch_train_loss=0.0007310368090935123
Epoch 142
Epoch 142 :: Batch 0/1
Batch Loss = 0.0007310368090935123
142, epoch_train_loss=0.0007310368090935123
Epoch 143
Epoch 143 :: Batch 0/1
Batch Loss = 0.0007310368090935123
143, epoch_train_loss=0.0007310368090935123
Epoch 144
Epoch 144 :: Batch 0/1
Batch Loss = 0.0007310368090935123
144, epoch_train_loss=0.0007310368090935123
Epoch 145
Epoch 145 :: Batch 0/1
Batch Loss = 0.0007310368090935123
145, epoch_train_loss=0.0007310368090935123
Epoch 146
Epoch 146 :: Batch 0/1
Batch Loss = 0.0007310368090935123
146, epoch_train_loss=0.0007310368090935123
Epoch 147
Epoch 147 :: Batch 0/1
Batch Loss = 0.0007310368090935123
147, epoch_train_loss=0.0007310368090935123
Epoch 148
Epoch 148 :: Batch 0/1
Batch Loss = 0.0007310368090935123
148, epoch_train_loss=0.0007310368090935123
Epoch 149
Epoch 149 :: Batch 0/1
Batch Loss = 0.0007310368090935123
149, epoch_train_loss=0.0007310368090935123
Epoch 150
Epoch 150 :: Batch 0/1
Batch Loss = 0.0007310368090935123
150, epoch_train_loss=0.0007310368090935123
Epoch 151
Epoch 151 :: Batch 0/1
Batch Loss = 0.0007310368090935123
151, epoch_train_loss=0.0007310368090935123
Epoch 152
Epoch 152 :: Batch 0/1
Batch Loss = 0.0007310368090935123
152, epoch_train_loss=0.0007310368090935123
Epoch 153
Epoch 153 :: Batch 0/1
Batch Loss = 0.0007310368090935123
153, epoch_train_loss=0.0007310368090935123
Epoch 154
Epoch 154 :: Batch 0/1
Batch Loss = 0.0007310368090935123
154, epoch_train_loss=0.0007310368090935123
Epoch 155
Epoch 155 :: Batch 0/1
Batch Loss = 0.0007310368090935123
155, epoch_train_loss=0.0007310368090935123
Epoch 156
Epoch 156 :: Batch 0/1
Batch Loss = 0.0007310368090935123
156, epoch_train_loss=0.0007310368090935123
Epoch 157
Epoch 157 :: Batch 0/1
Batch Loss = 0.0007310368090935123
157, epoch_train_loss=0.0007310368090935123
Epoch 158
Epoch 158 :: Batch 0/1
Batch Loss = 0.0007310368090935123
158, epoch_train_loss=0.0007310368090935123
Epoch 159
Epoch 159 :: Batch 0/1
Batch Loss = 0.0007310368090935123
159, epoch_train_loss=0.0007310368090935123
Epoch 160
Epoch 160 :: Batch 0/1
Batch Loss = 0.0007310368090935123
160, epoch_train_loss=0.0007310368090935123
Epoch 161
Epoch 161 :: Batch 0/1
Batch Loss = 0.0007310368090935123
161, epoch_train_loss=0.0007310368090935123
Epoch 162
Epoch 162 :: Batch 0/1
Batch Loss = 0.0007310368090935123
162, epoch_train_loss=0.0007310368090935123
Epoch 163
Epoch 163 :: Batch 0/1
Batch Loss = 0.0007310368090935123
163, epoch_train_loss=0.0007310368090935123
Epoch 164
Epoch 164 :: Batch 0/1
Batch Loss = 0.0007310368090935123
164, epoch_train_loss=0.0007310368090935123
Epoch 165
Epoch 165 :: Batch 0/1
Batch Loss = 0.0007310368090935123
165, epoch_train_loss=0.0007310368090935123
Epoch 166
Epoch 166 :: Batch 0/1
Batch Loss = 0.0007310368090935123
166, epoch_train_loss=0.0007310368090935123
Epoch 167
Epoch 167 :: Batch 0/1
Batch Loss = 0.0007310368090935123
167, epoch_train_loss=0.0007310368090935123
Epoch 168
Epoch 168 :: Batch 0/1
Batch Loss = 0.0007310368090935123
168, epoch_train_loss=0.0007310368090935123
Epoch 169
Epoch 169 :: Batch 0/1
Batch Loss = 0.0007310368090935123
169, epoch_train_loss=0.0007310368090935123
Epoch 170
Epoch 170 :: Batch 0/1
Batch Loss = 0.0007310368090935123
170, epoch_train_loss=0.0007310368090935123
Epoch 171
Epoch 171 :: Batch 0/1
Batch Loss = 0.0007310368090935123
171, epoch_train_loss=0.0007310368090935123
Epoch 172
Epoch 172 :: Batch 0/1
Batch Loss = 0.0007310368090935123
172, epoch_train_loss=0.0007310368090935123
Epoch 173
Epoch 173 :: Batch 0/1
Batch Loss = 0.0007310368090935123
173, epoch_train_loss=0.0007310368090935123
Epoch 174
Epoch 174 :: Batch 0/1
Batch Loss = 0.0007310368090935123
174, epoch_train_loss=0.0007310368090935123
Epoch 175
Epoch 175 :: Batch 0/1
Batch Loss = 0.0007310368090935123
175, epoch_train_loss=0.0007310368090935123
Epoch 176
Epoch 176 :: Batch 0/1
Batch Loss = 0.0007310368090935123
176, epoch_train_loss=0.0007310368090935123
Epoch 177
Epoch 177 :: Batch 0/1
Batch Loss = 0.0007310368090935123
177, epoch_train_loss=0.0007310368090935123
Epoch 178
Epoch 178 :: Batch 0/1
Batch Loss = 0.0007310368090935123
178, epoch_train_loss=0.0007310368090935123
Epoch 179
Epoch 179 :: Batch 0/1
Batch Loss = 0.0007310368090935123
179, epoch_train_loss=0.0007310368090935123
Epoch 180
Epoch 180 :: Batch 0/1
Batch Loss = 0.0007310368090935123
180, epoch_train_loss=0.0007310368090935123
Epoch 181
Epoch 181 :: Batch 0/1
Batch Loss = 0.0007310368090935123
181, epoch_train_loss=0.0007310368090935123
Epoch 182
Epoch 182 :: Batch 0/1
Batch Loss = 0.0007310368090935123
182, epoch_train_loss=0.0007310368090935123
Epoch 183
Epoch 183 :: Batch 0/1
Batch Loss = 0.0007310368090935123
183, epoch_train_loss=0.0007310368090935123
Epoch 184
Epoch 184 :: Batch 0/1
Batch Loss = 0.0007310368090935123
184, epoch_train_loss=0.0007310368090935123
Epoch 185
Epoch 185 :: Batch 0/1
Batch Loss = 0.0007310368090935123
185, epoch_train_loss=0.0007310368090935123
Epoch 186
Epoch 186 :: Batch 0/1
Batch Loss = 0.0007310368090935123
186, epoch_train_loss=0.0007310368090935123
Epoch 187
Epoch 187 :: Batch 0/1
Batch Loss = 0.0007310368090935123
187, epoch_train_loss=0.0007310368090935123
Epoch 188
Epoch 188 :: Batch 0/1
Batch Loss = 0.0007310368090935123
188, epoch_train_loss=0.0007310368090935123
Epoch 189
Epoch 189 :: Batch 0/1
Batch Loss = 0.0007310368090935123
189, epoch_train_loss=0.0007310368090935123
Epoch 190
Epoch 190 :: Batch 0/1
Batch Loss = 0.0007310368090935123
190, epoch_train_loss=0.0007310368090935123
Epoch 191
Epoch 191 :: Batch 0/1
Batch Loss = 0.0007310368090935123
191, epoch_train_loss=0.0007310368090935123
Epoch 192
Epoch 192 :: Batch 0/1
Batch Loss = 0.0007310368090935123
192, epoch_train_loss=0.0007310368090935123
Epoch 193
Epoch 193 :: Batch 0/1
Batch Loss = 0.0007310368090935123
193, epoch_train_loss=0.0007310368090935123
Epoch 194
Epoch 194 :: Batch 0/1
Batch Loss = 0.0007310368090935123
194, epoch_train_loss=0.0007310368090935123
Epoch 195
Epoch 195 :: Batch 0/1
Batch Loss = 0.0007310368090935123
195, epoch_train_loss=0.0007310368090935123
Epoch 196
Epoch 196 :: Batch 0/1
Batch Loss = 0.0007310368090935123
196, epoch_train_loss=0.0007310368090935123
Epoch 197
Epoch 197 :: Batch 0/1
Batch Loss = 0.0007310368090935123
197, epoch_train_loss=0.0007310368090935123
Epoch 198
Epoch 198 :: Batch 0/1
Batch Loss = 0.0007310368090935123
198, epoch_train_loss=0.0007310368090935123
Epoch 199
Epoch 199 :: Batch 0/1
Batch Loss = 0.0007310368090935123
199, epoch_train_loss=0.0007310368090935123
Epoch 200
Epoch 200 :: Batch 0/1
Batch Loss = 0.0007310368090935123
200, epoch_train_loss=0.0007310368090935123
Epoch 201
Epoch 201 :: Batch 0/1
Batch Loss = 0.0007310368090935123
201, epoch_train_loss=0.0007310368090935123
Epoch 202
Epoch 202 :: Batch 0/1
Batch Loss = 0.0007310368090935123
202, epoch_train_loss=0.0007310368090935123
Epoch 203
Epoch 203 :: Batch 0/1
Batch Loss = 0.0007310368090935123
203, epoch_train_loss=0.0007310368090935123
Epoch 204
Epoch 204 :: Batch 0/1
Batch Loss = 0.0007310368090935123
204, epoch_train_loss=0.0007310368090935123
Epoch 205
Epoch 205 :: Batch 0/1
Batch Loss = 0.0007310368090935123
205, epoch_train_loss=0.0007310368090935123
Epoch 206
Epoch 206 :: Batch 0/1
Batch Loss = 0.0007310368090935123
206, epoch_train_loss=0.0007310368090935123
Epoch 207
Epoch 207 :: Batch 0/1
Batch Loss = 0.0007310368090935123
207, epoch_train_loss=0.0007310368090935123
Epoch 208
Epoch 208 :: Batch 0/1
Batch Loss = 0.0007310368090935123
208, epoch_train_loss=0.0007310368090935123
Epoch 209
Epoch 209 :: Batch 0/1
Batch Loss = 0.0007310368090935123
209, epoch_train_loss=0.0007310368090935123
Epoch 210
Epoch 210 :: Batch 0/1
Batch Loss = 0.0007310368090935123
210, epoch_train_loss=0.0007310368090935123
Epoch 211
Epoch 211 :: Batch 0/1
Batch Loss = 0.0007310368090935123
211, epoch_train_loss=0.0007310368090935123
Epoch 212
Epoch 212 :: Batch 0/1
Batch Loss = 0.0007310368090935123
212, epoch_train_loss=0.0007310368090935123
Epoch 213
Epoch 213 :: Batch 0/1
Batch Loss = 0.0007310368090935123
213, epoch_train_loss=0.0007310368090935123
Epoch 214
Epoch 214 :: Batch 0/1
Batch Loss = 0.0007310368090935123
214, epoch_train_loss=0.0007310368090935123
Epoch 215
Epoch 215 :: Batch 0/1
Batch Loss = 0.0007310368090935123
215, epoch_train_loss=0.0007310368090935123
Epoch 216
Epoch 216 :: Batch 0/1
Batch Loss = 0.0007310368090935123
216, epoch_train_loss=0.0007310368090935123
Epoch 217
Epoch 217 :: Batch 0/1
Batch Loss = 0.0007310368090935123
217, epoch_train_loss=0.0007310368090935123
Epoch 218
Epoch 218 :: Batch 0/1
Batch Loss = 0.0007310368090935123
218, epoch_train_loss=0.0007310368090935123
Epoch 219
Epoch 219 :: Batch 0/1
Batch Loss = 0.0007310368090935123
219, epoch_train_loss=0.0007310368090935123
Epoch 220
Epoch 220 :: Batch 0/1
Batch Loss = 0.0007310368090935123
220, epoch_train_loss=0.0007310368090935123
Epoch 221
Epoch 221 :: Batch 0/1
Batch Loss = 0.0007310368090935123
221, epoch_train_loss=0.0007310368090935123
Epoch 222
Epoch 222 :: Batch 0/1
Batch Loss = 0.0007310368090935123
222, epoch_train_loss=0.0007310368090935123
Epoch 223
Epoch 223 :: Batch 0/1
Batch Loss = 0.0007310368090935123
223, epoch_train_loss=0.0007310368090935123
Epoch 224
Epoch 224 :: Batch 0/1
Batch Loss = 0.0007310368090935123
224, epoch_train_loss=0.0007310368090935123
Epoch 225
Epoch 225 :: Batch 0/1
Batch Loss = 0.0007310368090935123
225, epoch_train_loss=0.0007310368090935123
Epoch 226
Epoch 226 :: Batch 0/1
Batch Loss = 0.0007310368090935123
226, epoch_train_loss=0.0007310368090935123
Epoch 227
Epoch 227 :: Batch 0/1
Batch Loss = 0.0007310368090935123
227, epoch_train_loss=0.0007310368090935123
Epoch 228
Epoch 228 :: Batch 0/1
Batch Loss = 0.0007310368090935123
228, epoch_train_loss=0.0007310368090935123
Epoch 229
Epoch 229 :: Batch 0/1
Batch Loss = 0.0007310368090935123
229, epoch_train_loss=0.0007310368090935123
Epoch 230
Epoch 230 :: Batch 0/1
Batch Loss = 0.0007310368090935123
230, epoch_train_loss=0.0007310368090935123
Epoch 231
Epoch 231 :: Batch 0/1
Batch Loss = 0.0007310368090935123
231, epoch_train_loss=0.0007310368090935123
Epoch 232
Epoch 232 :: Batch 0/1
Batch Loss = 0.0007310368090935123
232, epoch_train_loss=0.0007310368090935123
Epoch 233
Epoch 233 :: Batch 0/1
Batch Loss = 0.0007310368090935123
233, epoch_train_loss=0.0007310368090935123
Epoch 234
Epoch 234 :: Batch 0/1
Batch Loss = 0.0007310368090935123
234, epoch_train_loss=0.0007310368090935123
Epoch 235
Epoch 235 :: Batch 0/1
Batch Loss = 0.0007310368090935123
235, epoch_train_loss=0.0007310368090935123
Epoch 236
Epoch 236 :: Batch 0/1
Batch Loss = 0.0007310368090935123
236, epoch_train_loss=0.0007310368090935123
Epoch 237
Epoch 237 :: Batch 0/1
Batch Loss = 0.0007310368090935123
237, epoch_train_loss=0.0007310368090935123
Epoch 238
Epoch 238 :: Batch 0/1
Batch Loss = 0.0007310368090935123
238, epoch_train_loss=0.0007310368090935123
Epoch 239
Epoch 239 :: Batch 0/1
Batch Loss = 0.0007310368090935123
239, epoch_train_loss=0.0007310368090935123
Epoch 240
Epoch 240 :: Batch 0/1
Batch Loss = 0.0007310368090935123
240, epoch_train_loss=0.0007310368090935123
Epoch 241
Epoch 241 :: Batch 0/1
Batch Loss = 0.0007310368090935123
241, epoch_train_loss=0.0007310368090935123
Epoch 242
Epoch 242 :: Batch 0/1
Batch Loss = 0.0007310368090935123
242, epoch_train_loss=0.0007310368090935123
Epoch 243
Epoch 243 :: Batch 0/1
Batch Loss = 0.0007310368090935123
243, epoch_train_loss=0.0007310368090935123
Epoch 244
Epoch 244 :: Batch 0/1
Batch Loss = 0.0007310368090935123
244, epoch_train_loss=0.0007310368090935123
Epoch 245
Epoch 245 :: Batch 0/1
Batch Loss = 0.0007310368090935123
245, epoch_train_loss=0.0007310368090935123
Epoch 246
Epoch 246 :: Batch 0/1
Batch Loss = 0.0007310368090935123
246, epoch_train_loss=0.0007310368090935123
Epoch 247
Epoch 247 :: Batch 0/1
Batch Loss = 0.0007310368090935123
247, epoch_train_loss=0.0007310368090935123
Epoch 248
Epoch 248 :: Batch 0/1
Batch Loss = 0.0007310368090935123
248, epoch_train_loss=0.0007310368090935123
Epoch 249
Epoch 249 :: Batch 0/1
Batch Loss = 0.0007310368090935123
249, epoch_train_loss=0.0007310368090935123
Epoch 250
Epoch 250 :: Batch 0/1
Batch Loss = 0.0007310368090935123
250, epoch_train_loss=0.0007310368090935123
Epoch 251
Epoch 251 :: Batch 0/1
Batch Loss = 0.0007310368090935123
251, epoch_train_loss=0.0007310368090935123
Epoch 252
Epoch 252 :: Batch 0/1
Batch Loss = 0.0007310368090935123
252, epoch_train_loss=0.0007310368090935123
Epoch 253
Epoch 253 :: Batch 0/1
Batch Loss = 0.0007310368090935123
253, epoch_train_loss=0.0007310368090935123
Epoch 254
Epoch 254 :: Batch 0/1
Batch Loss = 0.0007310368090935123
254, epoch_train_loss=0.0007310368090935123
Epoch 255
Epoch 255 :: Batch 0/1
Batch Loss = 0.0007310368090935123
255, epoch_train_loss=0.0007310368090935123
Epoch 256
Epoch 256 :: Batch 0/1
Batch Loss = 0.0007310368090935123
256, epoch_train_loss=0.0007310368090935123
Epoch 257
Epoch 257 :: Batch 0/1
Batch Loss = 0.0007310368090935123
257, epoch_train_loss=0.0007310368090935123
Epoch 258
Epoch 258 :: Batch 0/1
Batch Loss = 0.0007310368090935123
258, epoch_train_loss=0.0007310368090935123
Epoch 259
Epoch 259 :: Batch 0/1
Batch Loss = 0.0007310368090935123
259, epoch_train_loss=0.0007310368090935123
Epoch 260
Epoch 260 :: Batch 0/1
Batch Loss = 0.0007310368090935123
260, epoch_train_loss=0.0007310368090935123
Epoch 261
Epoch 261 :: Batch 0/1
Batch Loss = 0.0007310368090935123
261, epoch_train_loss=0.0007310368090935123
Epoch 262
Epoch 262 :: Batch 0/1
Batch Loss = 0.0007310368090935123
262, epoch_train_loss=0.0007310368090935123
Epoch 263
Epoch 263 :: Batch 0/1
Batch Loss = 0.0007310368090935123
263, epoch_train_loss=0.0007310368090935123
Epoch 264
Epoch 264 :: Batch 0/1
Batch Loss = 0.0007310368090935123
264, epoch_train_loss=0.0007310368090935123
Epoch 265
Epoch 265 :: Batch 0/1
Batch Loss = 0.0007310368090935123
265, epoch_train_loss=0.0007310368090935123
Epoch 266
Epoch 266 :: Batch 0/1
Batch Loss = 0.0007310368090935123
266, epoch_train_loss=0.0007310368090935123
Epoch 267
Epoch 267 :: Batch 0/1
Batch Loss = 0.0007310368090935123
267, epoch_train_loss=0.0007310368090935123
Epoch 268
Epoch 268 :: Batch 0/1
Batch Loss = 0.0007310368090935123
268, epoch_train_loss=0.0007310368090935123
Epoch 269
Epoch 269 :: Batch 0/1
Batch Loss = 0.0007310368090935123
269, epoch_train_loss=0.0007310368090935123
Epoch 270
Epoch 270 :: Batch 0/1
Batch Loss = 0.0007310368090935123
270, epoch_train_loss=0.0007310368090935123
Epoch 271
Epoch 271 :: Batch 0/1
Batch Loss = 0.0007310368090935123
271, epoch_train_loss=0.0007310368090935123
Epoch 272
Epoch 272 :: Batch 0/1
Batch Loss = 0.0007310368090935123
272, epoch_train_loss=0.0007310368090935123
Epoch 273
Epoch 273 :: Batch 0/1
Batch Loss = 0.0007310368090935123
273, epoch_train_loss=0.0007310368090935123
Epoch 274
Epoch 274 :: Batch 0/1
Batch Loss = 0.0007310368090935123
274, epoch_train_loss=0.0007310368090935123
Epoch 275
Epoch 275 :: Batch 0/1
Batch Loss = 0.0007310368090935123
275, epoch_train_loss=0.0007310368090935123
Epoch 276
Epoch 276 :: Batch 0/1
Batch Loss = 0.0007310368090935123
276, epoch_train_loss=0.0007310368090935123
Epoch 277
Epoch 277 :: Batch 0/1
Batch Loss = 0.0007310368090935123
277, epoch_train_loss=0.0007310368090935123
Epoch 278
Epoch 278 :: Batch 0/1
Batch Loss = 0.0007310368090935123
278, epoch_train_loss=0.0007310368090935123
Epoch 279
Epoch 279 :: Batch 0/1
Batch Loss = 0.0007310368090935123
279, epoch_train_loss=0.0007310368090935123
Epoch 280
Epoch 280 :: Batch 0/1
Batch Loss = 0.0007310368090935123
280, epoch_train_loss=0.0007310368090935123
Epoch 281
Epoch 281 :: Batch 0/1
Batch Loss = 0.0007310368090935123
281, epoch_train_loss=0.0007310368090935123
Epoch 282
Epoch 282 :: Batch 0/1
Batch Loss = 0.0007310368090935123
282, epoch_train_loss=0.0007310368090935123
Epoch 283
Epoch 283 :: Batch 0/1
Batch Loss = 0.0007310368090935123
283, epoch_train_loss=0.0007310368090935123
Epoch 284
Epoch 284 :: Batch 0/1
Batch Loss = 0.0007310368090935123
284, epoch_train_loss=0.0007310368090935123
Epoch 285
Epoch 285 :: Batch 0/1
Batch Loss = 0.0007310368090935123
285, epoch_train_loss=0.0007310368090935123
Epoch 286
Epoch 286 :: Batch 0/1
Batch Loss = 0.0007310368090935123
286, epoch_train_loss=0.0007310368090935123
Epoch 287
Epoch 287 :: Batch 0/1
Batch Loss = 0.0007310368090935123
287, epoch_train_loss=0.0007310368090935123
Epoch 288
Epoch 288 :: Batch 0/1
Batch Loss = 0.0007310368090935123
288, epoch_train_loss=0.0007310368090935123
Epoch 289
Epoch 289 :: Batch 0/1
Batch Loss = 0.0007310368090935123
289, epoch_train_loss=0.0007310368090935123
Epoch 290
Epoch 290 :: Batch 0/1
Batch Loss = 0.0007310368090935123
290, epoch_train_loss=0.0007310368090935123
Epoch 291
Epoch 291 :: Batch 0/1
Batch Loss = 0.0007310368090935123
291, epoch_train_loss=0.0007310368090935123
Epoch 292
Epoch 292 :: Batch 0/1
Batch Loss = 0.0007310368090935123
292, epoch_train_loss=0.0007310368090935123
Epoch 293
Epoch 293 :: Batch 0/1
Batch Loss = 0.0007310368090935123
293, epoch_train_loss=0.0007310368090935123
Epoch 294
Epoch 294 :: Batch 0/1
Batch Loss = 0.0007310368090935123
294, epoch_train_loss=0.0007310368090935123
Epoch 295
Epoch 295 :: Batch 0/1
Batch Loss = 0.0007310368090935123
295, epoch_train_loss=0.0007310368090935123
Epoch 296
Epoch 296 :: Batch 0/1
Batch Loss = 0.0007310368090935123
296, epoch_train_loss=0.0007310368090935123
Epoch 297
Epoch 297 :: Batch 0/1
Batch Loss = 0.0007310368090935123
297, epoch_train_loss=0.0007310368090935123
Epoch 298
Epoch 298 :: Batch 0/1
Batch Loss = 0.0007310368090935123
298, epoch_train_loss=0.0007310368090935123
Epoch 299
Epoch 299 :: Batch 0/1
Batch Loss = 0.0007310368090935123
299, epoch_train_loss=0.0007310368090935123
Epoch 300
Epoch 300 :: Batch 0/1
Batch Loss = 0.0007310368090935123
300, epoch_train_loss=0.0007310368090935123
Epoch 301
Epoch 301 :: Batch 0/1
Batch Loss = 0.0007310368090935123
301, epoch_train_loss=0.0007310368090935123
Epoch 302
Epoch 302 :: Batch 0/1
Batch Loss = 0.0007310368090935123
302, epoch_train_loss=0.0007310368090935123
Epoch 303
Epoch 303 :: Batch 0/1
Batch Loss = 0.0007310368090935123
303, epoch_train_loss=0.0007310368090935123
Epoch 304
Epoch 304 :: Batch 0/1
Batch Loss = 0.0007310368090935123
304, epoch_train_loss=0.0007310368090935123
Epoch 305
Epoch 305 :: Batch 0/1
Batch Loss = 0.0007310368090935123
305, epoch_train_loss=0.0007310368090935123
Epoch 306
Epoch 306 :: Batch 0/1
Batch Loss = 0.0007310368090935123
306, epoch_train_loss=0.0007310368090935123
Epoch 307
Epoch 307 :: Batch 0/1
Batch Loss = 0.0007310368090935123
307, epoch_train_loss=0.0007310368090935123
Epoch 308
Epoch 308 :: Batch 0/1
Batch Loss = 0.0007310368090935123
308, epoch_train_loss=0.0007310368090935123
Epoch 309
Epoch 309 :: Batch 0/1
Batch Loss = 0.0007310368090935123
309, epoch_train_loss=0.0007310368090935123
Epoch 310
Epoch 310 :: Batch 0/1
Batch Loss = 0.0007310368090935123
310, epoch_train_loss=0.0007310368090935123
Epoch 311
Epoch 311 :: Batch 0/1
Batch Loss = 0.0007310368090935123
311, epoch_train_loss=0.0007310368090935123
Epoch 312
Epoch 312 :: Batch 0/1
Batch Loss = 0.0007310368090935123
312, epoch_train_loss=0.0007310368090935123
Epoch 313
Epoch 313 :: Batch 0/1
Batch Loss = 0.0007310368090935123
313, epoch_train_loss=0.0007310368090935123
Epoch 314
Epoch 314 :: Batch 0/1
Batch Loss = 0.0007310368090935123
314, epoch_train_loss=0.0007310368090935123
Epoch 315
Epoch 315 :: Batch 0/1
Batch Loss = 0.0007310368090935123
315, epoch_train_loss=0.0007310368090935123
Epoch 316
Epoch 316 :: Batch 0/1
Batch Loss = 0.0007310368090935123
316, epoch_train_loss=0.0007310368090935123
Epoch 317
Epoch 317 :: Batch 0/1
Batch Loss = 0.0007310368090935123
317, epoch_train_loss=0.0007310368090935123
Epoch 318
Epoch 318 :: Batch 0/1
Batch Loss = 0.0007310368090935123
318, epoch_train_loss=0.0007310368090935123
Epoch 319
Epoch 319 :: Batch 0/1
Batch Loss = 0.0007310368090935123
319, epoch_train_loss=0.0007310368090935123
Epoch 320
Epoch 320 :: Batch 0/1
Batch Loss = 0.0007310368090935123
320, epoch_train_loss=0.0007310368090935123
Epoch 321
Epoch 321 :: Batch 0/1
Batch Loss = 0.0007310368090935123
321, epoch_train_loss=0.0007310368090935123
Epoch 322
Epoch 322 :: Batch 0/1
Batch Loss = 0.0007310368090935123
322, epoch_train_loss=0.0007310368090935123
Epoch 323
Epoch 323 :: Batch 0/1
Batch Loss = 0.0007310368090935123
323, epoch_train_loss=0.0007310368090935123
Epoch 324
Epoch 324 :: Batch 0/1
Batch Loss = 0.0007310368090935123
324, epoch_train_loss=0.0007310368090935123
Epoch 325
Epoch 325 :: Batch 0/1
Batch Loss = 0.0007310368090935123
325, epoch_train_loss=0.0007310368090935123
Epoch 326
Epoch 326 :: Batch 0/1
Batch Loss = 0.0007310368090935123
326, epoch_train_loss=0.0007310368090935123
Epoch 327
Epoch 327 :: Batch 0/1
Batch Loss = 0.0007310368090935123
327, epoch_train_loss=0.0007310368090935123
Epoch 328
Epoch 328 :: Batch 0/1
Batch Loss = 0.0007310368090935123
328, epoch_train_loss=0.0007310368090935123
Epoch 329
Epoch 329 :: Batch 0/1
Batch Loss = 0.0007310368090935123
329, epoch_train_loss=0.0007310368090935123
Epoch 330
Epoch 330 :: Batch 0/1
Batch Loss = 0.0007310368090935123
330, epoch_train_loss=0.0007310368090935123
Epoch 331
Epoch 331 :: Batch 0/1
Batch Loss = 0.0007310368090935123
331, epoch_train_loss=0.0007310368090935123
Epoch 332
Epoch 332 :: Batch 0/1
Batch Loss = 0.0007310368090935123
332, epoch_train_loss=0.0007310368090935123
Epoch 333
Epoch 333 :: Batch 0/1
Batch Loss = 0.0007310368090935123
333, epoch_train_loss=0.0007310368090935123
Epoch 334
Epoch 334 :: Batch 0/1
Batch Loss = 0.0007310368090935123
334, epoch_train_loss=0.0007310368090935123
Epoch 335
Epoch 335 :: Batch 0/1
Batch Loss = 0.0007310368090935123
335, epoch_train_loss=0.0007310368090935123
Epoch 336
Epoch 336 :: Batch 0/1
Batch Loss = 0.0007310368090935123
336, epoch_train_loss=0.0007310368090935123
Epoch 337
Epoch 337 :: Batch 0/1
Batch Loss = 0.0007310368090935123
337, epoch_train_loss=0.0007310368090935123
Epoch 338
Epoch 338 :: Batch 0/1
Batch Loss = 0.0007310368090935123
338, epoch_train_loss=0.0007310368090935123
Epoch 339
Epoch 339 :: Batch 0/1
Batch Loss = 0.0007310368090935123
339, epoch_train_loss=0.0007310368090935123
Epoch 340
Epoch 340 :: Batch 0/1
Batch Loss = 0.0007310368090935123
340, epoch_train_loss=0.0007310368090935123
Epoch 341
Epoch 341 :: Batch 0/1
Batch Loss = 0.0007310368090935123
341, epoch_train_loss=0.0007310368090935123
Epoch 342
Epoch 342 :: Batch 0/1
Batch Loss = 0.0007310368090935123
342, epoch_train_loss=0.0007310368090935123
Epoch 343
Epoch 343 :: Batch 0/1
Batch Loss = 0.0007310368090935123
343, epoch_train_loss=0.0007310368090935123
Epoch 344
Epoch 344 :: Batch 0/1
Batch Loss = 0.0007310368090935123
344, epoch_train_loss=0.0007310368090935123
Epoch 345
Epoch 345 :: Batch 0/1
Batch Loss = 0.0007310368090935123
345, epoch_train_loss=0.0007310368090935123
Epoch 346
Epoch 346 :: Batch 0/1
Batch Loss = 0.0007310368090935123
346, epoch_train_loss=0.0007310368090935123
Epoch 347
Epoch 347 :: Batch 0/1
Batch Loss = 0.0007310368090935123
347, epoch_train_loss=0.0007310368090935123
Epoch 348
Epoch 348 :: Batch 0/1
Batch Loss = 0.0007310368090935123
348, epoch_train_loss=0.0007310368090935123
Epoch 349
Epoch 349 :: Batch 0/1
Batch Loss = 0.0007310368090935123
349, epoch_train_loss=0.0007310368090935123
Epoch 350
Epoch 350 :: Batch 0/1
Batch Loss = 0.0007310368090935123
350, epoch_train_loss=0.0007310368090935123
Epoch 351
Epoch 351 :: Batch 0/1
Batch Loss = 0.0007310368090935123
351, epoch_train_loss=0.0007310368090935123
Epoch 352
Epoch 352 :: Batch 0/1
Batch Loss = 0.0007310368090935123
352, epoch_train_loss=0.0007310368090935123
Epoch 353
Epoch 353 :: Batch 0/1
Batch Loss = 0.0007310368090935123
353, epoch_train_loss=0.0007310368090935123
Epoch 354
Epoch 354 :: Batch 0/1
Batch Loss = 0.0007310368090935123
354, epoch_train_loss=0.0007310368090935123
Epoch 355
Epoch 355 :: Batch 0/1
Batch Loss = 0.0007310368090935123
355, epoch_train_loss=0.0007310368090935123
Epoch 356
Epoch 356 :: Batch 0/1
Batch Loss = 0.0007310368090935123
356, epoch_train_loss=0.0007310368090935123
Epoch 357
Epoch 357 :: Batch 0/1
Batch Loss = 0.0007310368090935123
357, epoch_train_loss=0.0007310368090935123
Epoch 358
Epoch 358 :: Batch 0/1
Batch Loss = 0.0007310368090935123
358, epoch_train_loss=0.0007310368090935123
Epoch 359
Epoch 359 :: Batch 0/1
Batch Loss = 0.0007310368090935123
359, epoch_train_loss=0.0007310368090935123
Epoch 360
Epoch 360 :: Batch 0/1
Batch Loss = 0.0007310368090935123
360, epoch_train_loss=0.0007310368090935123
Epoch 361
Epoch 361 :: Batch 0/1
Batch Loss = 0.0007310368090935123
361, epoch_train_loss=0.0007310368090935123
Epoch 362
Epoch 362 :: Batch 0/1
Batch Loss = 0.0007310368090935123
362, epoch_train_loss=0.0007310368090935123
Epoch 363
Epoch 363 :: Batch 0/1
Batch Loss = 0.0007310368090935123
363, epoch_train_loss=0.0007310368090935123
Epoch 364
Epoch 364 :: Batch 0/1
Batch Loss = 0.0007310368090935123
364, epoch_train_loss=0.0007310368090935123
Epoch 365
Epoch 365 :: Batch 0/1
Batch Loss = 0.0007310368090935123
365, epoch_train_loss=0.0007310368090935123
Epoch 366
Epoch 366 :: Batch 0/1
Batch Loss = 0.0007310368090935123
366, epoch_train_loss=0.0007310368090935123
Epoch 367
Epoch 367 :: Batch 0/1
Batch Loss = 0.0007310368090935123
367, epoch_train_loss=0.0007310368090935123
Epoch 368
Epoch 368 :: Batch 0/1
Batch Loss = 0.0007310368090935123
368, epoch_train_loss=0.0007310368090935123
Epoch 369
Epoch 369 :: Batch 0/1
Batch Loss = 0.0007310368090935123
369, epoch_train_loss=0.0007310368090935123
Epoch 370
Epoch 370 :: Batch 0/1
Batch Loss = 0.0007310368090935123
370, epoch_train_loss=0.0007310368090935123
Epoch 371
Epoch 371 :: Batch 0/1
Batch Loss = 0.0007310368090935123
371, epoch_train_loss=0.0007310368090935123
Epoch 372
Epoch 372 :: Batch 0/1
Batch Loss = 0.0007310368090935123
372, epoch_train_loss=0.0007310368090935123
Epoch 373
Epoch 373 :: Batch 0/1
Batch Loss = 0.0007310368090935123
373, epoch_train_loss=0.0007310368090935123
Epoch 374
Epoch 374 :: Batch 0/1
Batch Loss = 0.0007310368090935123
374, epoch_train_loss=0.0007310368090935123
Epoch 375
Epoch 375 :: Batch 0/1
Batch Loss = 0.0007310368090935123
375, epoch_train_loss=0.0007310368090935123
Epoch 376
Epoch 376 :: Batch 0/1
Batch Loss = 0.0007310368090935123
376, epoch_train_loss=0.0007310368090935123
Epoch 377
Epoch 377 :: Batch 0/1
Batch Loss = 0.0007310368090935123
377, epoch_train_loss=0.0007310368090935123
Epoch 378
Epoch 378 :: Batch 0/1
Batch Loss = 0.0007310368090935123
378, epoch_train_loss=0.0007310368090935123
Epoch 379
Epoch 379 :: Batch 0/1
Batch Loss = 0.0007310368090935123
379, epoch_train_loss=0.0007310368090935123
Epoch 380
Epoch 380 :: Batch 0/1
Batch Loss = 0.0007310368090935123
380, epoch_train_loss=0.0007310368090935123
Epoch 381
Epoch 381 :: Batch 0/1
Batch Loss = 0.0007310368090935123
381, epoch_train_loss=0.0007310368090935123
Epoch 382
Epoch 382 :: Batch 0/1
Batch Loss = 0.0007310368090935123
382, epoch_train_loss=0.0007310368090935123
Epoch 383
Epoch 383 :: Batch 0/1
Batch Loss = 0.0007310368090935123
383, epoch_train_loss=0.0007310368090935123
Epoch 384
Epoch 384 :: Batch 0/1
Batch Loss = 0.0007310368090935123
384, epoch_train_loss=0.0007310368090935123
Epoch 385
Epoch 385 :: Batch 0/1
Batch Loss = 0.0007310368090935123
385, epoch_train_loss=0.0007310368090935123
Epoch 386
Epoch 386 :: Batch 0/1
Batch Loss = 0.0007310368090935123
386, epoch_train_loss=0.0007310368090935123
Epoch 387
Epoch 387 :: Batch 0/1
Batch Loss = 0.0007310368090935123
387, epoch_train_loss=0.0007310368090935123
Epoch 388
Epoch 388 :: Batch 0/1
Batch Loss = 0.0007310368090935123
388, epoch_train_loss=0.0007310368090935123
Epoch 389
Epoch 389 :: Batch 0/1
Batch Loss = 0.0007310368090935123
389, epoch_train_loss=0.0007310368090935123
Epoch 390
Epoch 390 :: Batch 0/1
Batch Loss = 0.0007310368090935123
390, epoch_train_loss=0.0007310368090935123
Epoch 391
Epoch 391 :: Batch 0/1
Batch Loss = 0.0007310368090935123
391, epoch_train_loss=0.0007310368090935123
Epoch 392
Epoch 392 :: Batch 0/1
Batch Loss = 0.0007310368090935123
392, epoch_train_loss=0.0007310368090935123
Epoch 393
Epoch 393 :: Batch 0/1
Batch Loss = 0.0007310368090935123
393, epoch_train_loss=0.0007310368090935123
Epoch 394
Epoch 394 :: Batch 0/1
Batch Loss = 0.0007310368090935123
394, epoch_train_loss=0.0007310368090935123
Epoch 395
Epoch 395 :: Batch 0/1
Batch Loss = 0.0007310368090935123
395, epoch_train_loss=0.0007310368090935123
Epoch 396
Epoch 396 :: Batch 0/1
Batch Loss = 0.0007310368090935123
396, epoch_train_loss=0.0007310368090935123
Epoch 397
Epoch 397 :: Batch 0/1
Batch Loss = 0.0007310368090935123
397, epoch_train_loss=0.0007310368090935123
Epoch 398
Epoch 398 :: Batch 0/1
Batch Loss = 0.0007310368090935123
398, epoch_train_loss=0.0007310368090935123
Epoch 399
Epoch 399 :: Batch 0/1
Batch Loss = 0.0007310368090935123
399, epoch_train_loss=0.0007310368090935123
Epoch 400
Epoch 400 :: Batch 0/1
Batch Loss = 0.0007310368090935123
400, epoch_train_loss=0.0007310368090935123
Epoch 401
Epoch 401 :: Batch 0/1
Batch Loss = 0.0007310368090935123
401, epoch_train_loss=0.0007310368090935123
Epoch 402
Epoch 402 :: Batch 0/1
Batch Loss = 0.0007310368090935123
402, epoch_train_loss=0.0007310368090935123
Epoch 403
Epoch 403 :: Batch 0/1
Batch Loss = 0.0007310368090935123
403, epoch_train_loss=0.0007310368090935123
Epoch 404
Epoch 404 :: Batch 0/1
Batch Loss = 0.0007310368090935123
404, epoch_train_loss=0.0007310368090935123
Epoch 405
Epoch 405 :: Batch 0/1
Batch Loss = 0.0007310368090935123
405, epoch_train_loss=0.0007310368090935123
Epoch 406
Epoch 406 :: Batch 0/1
Batch Loss = 0.0007310368090935123
406, epoch_train_loss=0.0007310368090935123
Epoch 407
Epoch 407 :: Batch 0/1
Batch Loss = 0.0007310368090935123
407, epoch_train_loss=0.0007310368090935123
Epoch 408
Epoch 408 :: Batch 0/1
Batch Loss = 0.0007310368090935123
408, epoch_train_loss=0.0007310368090935123
Epoch 409
Epoch 409 :: Batch 0/1
Batch Loss = 0.0007310368090935123
409, epoch_train_loss=0.0007310368090935123
Epoch 410
Epoch 410 :: Batch 0/1
Batch Loss = 0.0007310368090935123
410, epoch_train_loss=0.0007310368090935123
Epoch 411
Epoch 411 :: Batch 0/1
Batch Loss = 0.0007310368090935123
411, epoch_train_loss=0.0007310368090935123
Epoch 412
Epoch 412 :: Batch 0/1
Batch Loss = 0.0007310368090935123
412, epoch_train_loss=0.0007310368090935123
Epoch 413
Epoch 413 :: Batch 0/1
Batch Loss = 0.0007310368090935123
413, epoch_train_loss=0.0007310368090935123
Epoch 414
Epoch 414 :: Batch 0/1
Batch Loss = 0.0007310368090935123
414, epoch_train_loss=0.0007310368090935123
Epoch 415
Epoch 415 :: Batch 0/1
Batch Loss = 0.0007310368090935123
415, epoch_train_loss=0.0007310368090935123
Epoch 416
Epoch 416 :: Batch 0/1
Batch Loss = 0.0007310368090935123
416, epoch_train_loss=0.0007310368090935123
Epoch 417
Epoch 417 :: Batch 0/1
Batch Loss = 0.0007310368090935123
417, epoch_train_loss=0.0007310368090935123
Epoch 418
Epoch 418 :: Batch 0/1
Batch Loss = 0.0007310368090935123
418, epoch_train_loss=0.0007310368090935123
Epoch 419
Epoch 419 :: Batch 0/1
Batch Loss = 0.0007310368090935123
419, epoch_train_loss=0.0007310368090935123
Epoch 420
Epoch 420 :: Batch 0/1
Batch Loss = 0.0007310368090935123
420, epoch_train_loss=0.0007310368090935123
Epoch 421
Epoch 421 :: Batch 0/1
Batch Loss = 0.0007310368090935123
421, epoch_train_loss=0.0007310368090935123
Epoch 422
Epoch 422 :: Batch 0/1
Batch Loss = 0.0007310368090935123
422, epoch_train_loss=0.0007310368090935123
Epoch 423
Epoch 423 :: Batch 0/1
Batch Loss = 0.0007310368090935123
423, epoch_train_loss=0.0007310368090935123
Epoch 424
Epoch 424 :: Batch 0/1
Batch Loss = 0.0007310368090935123
424, epoch_train_loss=0.0007310368090935123
Epoch 425
Epoch 425 :: Batch 0/1
Batch Loss = 0.0007310368090935123
425, epoch_train_loss=0.0007310368090935123
Epoch 426
Epoch 426 :: Batch 0/1
Batch Loss = 0.0007310368090935123
426, epoch_train_loss=0.0007310368090935123
Epoch 427
Epoch 427 :: Batch 0/1
Batch Loss = 0.0007310368090935123
427, epoch_train_loss=0.0007310368090935123
Epoch 428
Epoch 428 :: Batch 0/1
Batch Loss = 0.0007310368090935123
428, epoch_train_loss=0.0007310368090935123
Epoch 429
Epoch 429 :: Batch 0/1
Batch Loss = 0.0007310368090935123
429, epoch_train_loss=0.0007310368090935123
Epoch 430
Epoch 430 :: Batch 0/1
Batch Loss = 0.0007310368090935123
430, epoch_train_loss=0.0007310368090935123
Epoch 431
Epoch 431 :: Batch 0/1
Batch Loss = 0.0007310368090935123
431, epoch_train_loss=0.0007310368090935123
Epoch 432
Epoch 432 :: Batch 0/1
Batch Loss = 0.0007310368090935123
432, epoch_train_loss=0.0007310368090935123
Epoch 433
Epoch 433 :: Batch 0/1
Batch Loss = 0.0007310368090935123
433, epoch_train_loss=0.0007310368090935123
Epoch 434
Epoch 434 :: Batch 0/1
Batch Loss = 0.0007310368090935123
434, epoch_train_loss=0.0007310368090935123
Epoch 435
Epoch 435 :: Batch 0/1
Batch Loss = 0.0007310368090935123
435, epoch_train_loss=0.0007310368090935123
Epoch 436
Epoch 436 :: Batch 0/1
Batch Loss = 0.0007310368090935123
436, epoch_train_loss=0.0007310368090935123
Epoch 437
Epoch 437 :: Batch 0/1
Batch Loss = 0.0007310368090935123
437, epoch_train_loss=0.0007310368090935123
Epoch 438
Epoch 438 :: Batch 0/1
Batch Loss = 0.0007310368090935123
438, epoch_train_loss=0.0007310368090935123
Epoch 439
Epoch 439 :: Batch 0/1
Batch Loss = 0.0007310368090935123
439, epoch_train_loss=0.0007310368090935123
Epoch 440
Epoch 440 :: Batch 0/1
Batch Loss = 0.0007310368090935123
440, epoch_train_loss=0.0007310368090935123
Epoch 441
Epoch 441 :: Batch 0/1
Batch Loss = 0.0007310368090935123
441, epoch_train_loss=0.0007310368090935123
Epoch 442
Epoch 442 :: Batch 0/1
Batch Loss = 0.0007310368090935123
442, epoch_train_loss=0.0007310368090935123
Epoch 443
Epoch 443 :: Batch 0/1
Batch Loss = 0.0007310368090935123
443, epoch_train_loss=0.0007310368090935123
Epoch 444
Epoch 444 :: Batch 0/1
Batch Loss = 0.0007310368090935123
444, epoch_train_loss=0.0007310368090935123
Epoch 445
Epoch 445 :: Batch 0/1
Batch Loss = 0.0007310368090935123
445, epoch_train_loss=0.0007310368090935123
Epoch 446
Epoch 446 :: Batch 0/1
Batch Loss = 0.0007310368090935123
446, epoch_train_loss=0.0007310368090935123
Epoch 447
Epoch 447 :: Batch 0/1
Batch Loss = 0.0007310368090935123
447, epoch_train_loss=0.0007310368090935123
Epoch 448
Epoch 448 :: Batch 0/1
Batch Loss = 0.0007310368090935123
448, epoch_train_loss=0.0007310368090935123
Epoch 449
Epoch 449 :: Batch 0/1
Batch Loss = 0.0007310368090935123
449, epoch_train_loss=0.0007310368090935123
Epoch 450
Epoch 450 :: Batch 0/1
Batch Loss = 0.0007310368090935123
450, epoch_train_loss=0.0007310368090935123
Epoch 451
Epoch 451 :: Batch 0/1
Batch Loss = 0.0007310368090935123
451, epoch_train_loss=0.0007310368090935123
Epoch 452
Epoch 452 :: Batch 0/1
Batch Loss = 0.0007310368090935123
452, epoch_train_loss=0.0007310368090935123
Epoch 453
Epoch 453 :: Batch 0/1
Batch Loss = 0.0007310368090935123
453, epoch_train_loss=0.0007310368090935123
Epoch 454
Epoch 454 :: Batch 0/1
Batch Loss = 0.0007310368090935123
454, epoch_train_loss=0.0007310368090935123
Epoch 455
Epoch 455 :: Batch 0/1
Batch Loss = 0.0007310368090935123
455, epoch_train_loss=0.0007310368090935123
Epoch 456
Epoch 456 :: Batch 0/1
Batch Loss = 0.0007310368090935123
456, epoch_train_loss=0.0007310368090935123
Epoch 457
Epoch 457 :: Batch 0/1
Batch Loss = 0.0007310368090935123
457, epoch_train_loss=0.0007310368090935123
Epoch 458
Epoch 458 :: Batch 0/1
Batch Loss = 0.0007310368090935123
458, epoch_train_loss=0.0007310368090935123
Epoch 459
Epoch 459 :: Batch 0/1
Batch Loss = 0.0007310368090935123
459, epoch_train_loss=0.0007310368090935123
Epoch 460
Epoch 460 :: Batch 0/1
Batch Loss = 0.0007310368090935123
460, epoch_train_loss=0.0007310368090935123
Epoch 461
Epoch 461 :: Batch 0/1
Batch Loss = 0.0007310368090935123
461, epoch_train_loss=0.0007310368090935123
Epoch 462
Epoch 462 :: Batch 0/1
Batch Loss = 0.0007310368090935123
462, epoch_train_loss=0.0007310368090935123
Epoch 463
Epoch 463 :: Batch 0/1
Batch Loss = 0.0007310368090935123
463, epoch_train_loss=0.0007310368090935123
Epoch 464
Epoch 464 :: Batch 0/1
Batch Loss = 0.0007310368090935123
464, epoch_train_loss=0.0007310368090935123
Epoch 465
Epoch 465 :: Batch 0/1
Batch Loss = 0.0007310368090935123
465, epoch_train_loss=0.0007310368090935123
Epoch 466
Epoch 466 :: Batch 0/1
Batch Loss = 0.0007310368090935123
466, epoch_train_loss=0.0007310368090935123
Epoch 467
Epoch 467 :: Batch 0/1
Batch Loss = 0.0007310368090935123
467, epoch_train_loss=0.0007310368090935123
Epoch 468
Epoch 468 :: Batch 0/1
Batch Loss = 0.0007310368090935123
468, epoch_train_loss=0.0007310368090935123
Epoch 469
Epoch 469 :: Batch 0/1
Batch Loss = 0.0007310368090935123
469, epoch_train_loss=0.0007310368090935123
Epoch 470
Epoch 470 :: Batch 0/1
Batch Loss = 0.0007310368090935123
470, epoch_train_loss=0.0007310368090935123
Epoch 471
Epoch 471 :: Batch 0/1
Batch Loss = 0.0007310368090935123
471, epoch_train_loss=0.0007310368090935123
Epoch 472
Epoch 472 :: Batch 0/1
Batch Loss = 0.0007310368090935123
472, epoch_train_loss=0.0007310368090935123
Epoch 473
Epoch 473 :: Batch 0/1
Batch Loss = 0.0007310368090935123
473, epoch_train_loss=0.0007310368090935123
Epoch 474
Epoch 474 :: Batch 0/1
Batch Loss = 0.0007310368090935123
474, epoch_train_loss=0.0007310368090935123
Epoch 475
Epoch 475 :: Batch 0/1
Batch Loss = 0.0007310368090935123
475, epoch_train_loss=0.0007310368090935123
Epoch 476
Epoch 476 :: Batch 0/1
Batch Loss = 0.0007310368090935123
476, epoch_train_loss=0.0007310368090935123
Epoch 477
Epoch 477 :: Batch 0/1
Batch Loss = 0.0007310368090935123
477, epoch_train_loss=0.0007310368090935123
Epoch 478
Epoch 478 :: Batch 0/1
Batch Loss = 0.0007310368090935123
478, epoch_train_loss=0.0007310368090935123
Epoch 479
Epoch 479 :: Batch 0/1
Batch Loss = 0.0007310368090935123
479, epoch_train_loss=0.0007310368090935123
Epoch 480
Epoch 480 :: Batch 0/1
Batch Loss = 0.0007310368090935123
480, epoch_train_loss=0.0007310368090935123
Epoch 481
Epoch 481 :: Batch 0/1
Batch Loss = 0.0007310368090935123
481, epoch_train_loss=0.0007310368090935123
Epoch 482
Epoch 482 :: Batch 0/1
Batch Loss = 0.0007310368090935123
482, epoch_train_loss=0.0007310368090935123
Epoch 483
Epoch 483 :: Batch 0/1
Batch Loss = 0.0007310368090935123
483, epoch_train_loss=0.0007310368090935123
Epoch 484
Epoch 484 :: Batch 0/1
Batch Loss = 0.0007310368090935123
484, epoch_train_loss=0.0007310368090935123
Epoch 485
Epoch 485 :: Batch 0/1
Batch Loss = 0.0007310368090935123
485, epoch_train_loss=0.0007310368090935123
Epoch 486
Epoch 486 :: Batch 0/1
Batch Loss = 0.0007310368090935123
486, epoch_train_loss=0.0007310368090935123
Epoch 487
Epoch 487 :: Batch 0/1
Batch Loss = 0.0007310368090935123
487, epoch_train_loss=0.0007310368090935123
Epoch 488
Epoch 488 :: Batch 0/1
Batch Loss = 0.0007310368090935123
488, epoch_train_loss=0.0007310368090935123
Epoch 489
Epoch 489 :: Batch 0/1
Batch Loss = 0.0007310368090935123
489, epoch_train_loss=0.0007310368090935123
Epoch 490
Epoch 490 :: Batch 0/1
Batch Loss = 0.0007310368090935123
490, epoch_train_loss=0.0007310368090935123
Epoch 491
Epoch 491 :: Batch 0/1
Batch Loss = 0.0007310368090935123
491, epoch_train_loss=0.0007310368090935123
Epoch 492
Epoch 492 :: Batch 0/1
Batch Loss = 0.0007310368090935123
492, epoch_train_loss=0.0007310368090935123
Epoch 493
Epoch 493 :: Batch 0/1
Batch Loss = 0.0007310368090935123
493, epoch_train_loss=0.0007310368090935123
Epoch 494
Epoch 494 :: Batch 0/1
Batch Loss = 0.0007310368090935123
494, epoch_train_loss=0.0007310368090935123
Epoch 495
Epoch 495 :: Batch 0/1
Batch Loss = 0.0007310368090935123
495, epoch_train_loss=0.0007310368090935123
Epoch 496
Epoch 496 :: Batch 0/1
Batch Loss = 0.0007310368090935123
496, epoch_train_loss=0.0007310368090935123
Epoch 497
Epoch 497 :: Batch 0/1
Batch Loss = 0.0007310368090935123
497, epoch_train_loss=0.0007310368090935123
Epoch 498
Epoch 498 :: Batch 0/1
Batch Loss = 0.0007310368090935123
498, epoch_train_loss=0.0007310368090935123
Epoch 499
Epoch 499 :: Batch 0/1
Batch Loss = 0.0007310368090935123
499, epoch_train_loss=0.0007310368090935123
Epoch 500
Epoch 500 :: Batch 0/1
Batch Loss = 0.0007310368090935123
500, epoch_train_loss=0.0007310368090935123
Epoch 501
Epoch 501 :: Batch 0/1
Batch Loss = 0.0007310368090935123
501, epoch_train_loss=0.0007310368090935123
Epoch 502
Epoch 502 :: Batch 0/1
Batch Loss = 0.0007310368090935123
502, epoch_train_loss=0.0007310368090935123
Epoch 503
Epoch 503 :: Batch 0/1
Batch Loss = 0.0007310368090935123
503, epoch_train_loss=0.0007310368090935123
Epoch 504
Epoch 504 :: Batch 0/1
Batch Loss = 0.0007310368090935123
504, epoch_train_loss=0.0007310368090935123
Epoch 505
Epoch 505 :: Batch 0/1
Batch Loss = 0.0007310368090935123
505, epoch_train_loss=0.0007310368090935123
Epoch 506
Epoch 506 :: Batch 0/1
Batch Loss = 0.0007310368090935123
506, epoch_train_loss=0.0007310368090935123
Epoch 507
Epoch 507 :: Batch 0/1
Batch Loss = 0.0007310368090935123
507, epoch_train_loss=0.0007310368090935123
Epoch 508
Epoch 508 :: Batch 0/1
Batch Loss = 0.0007310368090935123
508, epoch_train_loss=0.0007310368090935123
Epoch 509
Epoch 509 :: Batch 0/1
Batch Loss = 0.0007310368090935123
509, epoch_train_loss=0.0007310368090935123
Epoch 510
Epoch 510 :: Batch 0/1
Batch Loss = 0.0007310368090935123
510, epoch_train_loss=0.0007310368090935123
Epoch 511
Epoch 511 :: Batch 0/1
Batch Loss = 0.0007310368090935123
511, epoch_train_loss=0.0007310368090935123
Epoch 512
Epoch 512 :: Batch 0/1
Batch Loss = 0.0007310368090935123
512, epoch_train_loss=0.0007310368090935123
Epoch 513
Epoch 513 :: Batch 0/1
Batch Loss = 0.0007310368090935123
513, epoch_train_loss=0.0007310368090935123
Epoch 514
Epoch 514 :: Batch 0/1
Batch Loss = 0.0007310368090935123
514, epoch_train_loss=0.0007310368090935123
Epoch 515
Epoch 515 :: Batch 0/1
Batch Loss = 0.0007310368090935123
515, epoch_train_loss=0.0007310368090935123
Epoch 516
Epoch 516 :: Batch 0/1
Batch Loss = 0.0007310368090935123
516, epoch_train_loss=0.0007310368090935123
Epoch 517
Epoch 517 :: Batch 0/1
Batch Loss = 0.0007310368090935123
517, epoch_train_loss=0.0007310368090935123
Epoch 518
Epoch 518 :: Batch 0/1
Batch Loss = 0.0007310368090935123
518, epoch_train_loss=0.0007310368090935123
Epoch 519
Epoch 519 :: Batch 0/1
Batch Loss = 0.0007310368090935123
519, epoch_train_loss=0.0007310368090935123
Epoch 520
Epoch 520 :: Batch 0/1
Batch Loss = 0.0007310368090935123
520, epoch_train_loss=0.0007310368090935123
Epoch 521
Epoch 521 :: Batch 0/1
Batch Loss = 0.0007310368090935123
521, epoch_train_loss=0.0007310368090935123
Epoch 522
Epoch 522 :: Batch 0/1
Batch Loss = 0.0007310368090935123
522, epoch_train_loss=0.0007310368090935123
Epoch 523
Epoch 523 :: Batch 0/1
Batch Loss = 0.0007310368090935123
523, epoch_train_loss=0.0007310368090935123
Epoch 524
Epoch 524 :: Batch 0/1
Batch Loss = 0.0007310368090935123
524, epoch_train_loss=0.0007310368090935123
Epoch 525
Epoch 525 :: Batch 0/1
Batch Loss = 0.0007310368090935123
525, epoch_train_loss=0.0007310368090935123
Epoch 526
Epoch 526 :: Batch 0/1
Batch Loss = 0.0007310368090935123
526, epoch_train_loss=0.0007310368090935123
Epoch 527
Epoch 527 :: Batch 0/1
Batch Loss = 0.0007310368090935123
527, epoch_train_loss=0.0007310368090935123
Epoch 528
Epoch 528 :: Batch 0/1
Batch Loss = 0.0007310368090935123
528, epoch_train_loss=0.0007310368090935123
Epoch 529
Epoch 529 :: Batch 0/1
Batch Loss = 0.0007310368090935123
529, epoch_train_loss=0.0007310368090935123
Epoch 530
Epoch 530 :: Batch 0/1
Batch Loss = 0.0007310368090935123
530, epoch_train_loss=0.0007310368090935123
Epoch 531
Epoch 531 :: Batch 0/1
Batch Loss = 0.0007310368090935123
531, epoch_train_loss=0.0007310368090935123
Epoch 532
Epoch 532 :: Batch 0/1
Batch Loss = 0.0007310368090935123
532, epoch_train_loss=0.0007310368090935123
Epoch 533
Epoch 533 :: Batch 0/1
Batch Loss = 0.0007310368090935123
533, epoch_train_loss=0.0007310368090935123
Epoch 534
Epoch 534 :: Batch 0/1
Batch Loss = 0.0007310368090935123
534, epoch_train_loss=0.0007310368090935123
Epoch 535
Epoch 535 :: Batch 0/1
Batch Loss = 0.0007310368090935123
535, epoch_train_loss=0.0007310368090935123
Epoch 536
Epoch 536 :: Batch 0/1
Batch Loss = 0.0007310368090935123
536, epoch_train_loss=0.0007310368090935123
Epoch 537
Epoch 537 :: Batch 0/1
Batch Loss = 0.0007310368090935123
537, epoch_train_loss=0.0007310368090935123
Epoch 538
Epoch 538 :: Batch 0/1
Batch Loss = 0.0007310368090935123
538, epoch_train_loss=0.0007310368090935123
Epoch 539
Epoch 539 :: Batch 0/1
Batch Loss = 0.0007310368090935123
539, epoch_train_loss=0.0007310368090935123
Epoch 540
Epoch 540 :: Batch 0/1
Batch Loss = 0.0007310368090935123
540, epoch_train_loss=0.0007310368090935123
Epoch 541
Epoch 541 :: Batch 0/1
Batch Loss = 0.0007310368090935123
541, epoch_train_loss=0.0007310368090935123
Epoch 542
Epoch 542 :: Batch 0/1
Batch Loss = 0.0007310368090935123
542, epoch_train_loss=0.0007310368090935123
Epoch 543
Epoch 543 :: Batch 0/1
Batch Loss = 0.0007310368090935123
543, epoch_train_loss=0.0007310368090935123
Epoch 544
Epoch 544 :: Batch 0/1
Batch Loss = 0.0007310368090935123
544, epoch_train_loss=0.0007310368090935123
Epoch 545
Epoch 545 :: Batch 0/1
Batch Loss = 0.0007310368090935123
545, epoch_train_loss=0.0007310368090935123
Epoch 546
Epoch 546 :: Batch 0/1
Batch Loss = 0.0007310368090935123
546, epoch_train_loss=0.0007310368090935123
Epoch 547
Epoch 547 :: Batch 0/1
Batch Loss = 0.0007310368090935123
547, epoch_train_loss=0.0007310368090935123
Epoch 548
Epoch 548 :: Batch 0/1
Batch Loss = 0.0007310368090935123
548, epoch_train_loss=0.0007310368090935123
Epoch 549
Epoch 549 :: Batch 0/1
Batch Loss = 0.0007310368090935123
549, epoch_train_loss=0.0007310368090935123
Epoch 550
Epoch 550 :: Batch 0/1
Batch Loss = 0.0007310368090935123
550, epoch_train_loss=0.0007310368090935123
Epoch 551
Epoch 551 :: Batch 0/1
Batch Loss = 0.0007310368090935123
551, epoch_train_loss=0.0007310368090935123
Epoch 552
Epoch 552 :: Batch 0/1
Batch Loss = 0.0007310368090935123
552, epoch_train_loss=0.0007310368090935123
Epoch 553
Epoch 553 :: Batch 0/1
Batch Loss = 0.0007310368090935123
553, epoch_train_loss=0.0007310368090935123
Epoch 554
Epoch 554 :: Batch 0/1
Batch Loss = 0.0007310368090935123
554, epoch_train_loss=0.0007310368090935123
Epoch 555
Epoch 555 :: Batch 0/1
Batch Loss = 0.0007310368090935123
555, epoch_train_loss=0.0007310368090935123
Epoch 556
Epoch 556 :: Batch 0/1
Batch Loss = 0.0007310368090935123
556, epoch_train_loss=0.0007310368090935123
Epoch 557
Epoch 557 :: Batch 0/1
Batch Loss = 0.0007310368090935123
557, epoch_train_loss=0.0007310368090935123
Epoch 558
Epoch 558 :: Batch 0/1
Batch Loss = 0.0007310368090935123
558, epoch_train_loss=0.0007310368090935123
Epoch 559
Epoch 559 :: Batch 0/1
Batch Loss = 0.0007310368090935123
559, epoch_train_loss=0.0007310368090935123
Epoch 560
Epoch 560 :: Batch 0/1
Batch Loss = 0.0007310368090935123
560, epoch_train_loss=0.0007310368090935123
Epoch 561
Epoch 561 :: Batch 0/1
Batch Loss = 0.0007310368090935123
561, epoch_train_loss=0.0007310368090935123
Epoch 562
Epoch 562 :: Batch 0/1
Batch Loss = 0.0007310368090935123
562, epoch_train_loss=0.0007310368090935123
Epoch 563
Epoch 563 :: Batch 0/1
Batch Loss = 0.0007310368090935123
563, epoch_train_loss=0.0007310368090935123
Epoch 564
Epoch 564 :: Batch 0/1
Batch Loss = 0.0007310368090935123
564, epoch_train_loss=0.0007310368090935123
Epoch 565
Epoch 565 :: Batch 0/1
Batch Loss = 0.0007310368090935123
565, epoch_train_loss=0.0007310368090935123
Epoch 566
Epoch 566 :: Batch 0/1
Batch Loss = 0.0007310368090935123
566, epoch_train_loss=0.0007310368090935123
Epoch 567
Epoch 567 :: Batch 0/1
Batch Loss = 0.0007310368090935123
567, epoch_train_loss=0.0007310368090935123
Epoch 568
Epoch 568 :: Batch 0/1
Batch Loss = 0.0007310368090935123
568, epoch_train_loss=0.0007310368090935123
Epoch 569
Epoch 569 :: Batch 0/1
Batch Loss = 0.0007310368090935123
569, epoch_train_loss=0.0007310368090935123
Epoch 570
Epoch 570 :: Batch 0/1
Batch Loss = 0.0007310368090935123
570, epoch_train_loss=0.0007310368090935123
Epoch 571
Epoch 571 :: Batch 0/1
Batch Loss = 0.0007310368090935123
571, epoch_train_loss=0.0007310368090935123
Epoch 572
Epoch 572 :: Batch 0/1
Batch Loss = 0.0007310368090935123
572, epoch_train_loss=0.0007310368090935123
Epoch 573
Epoch 573 :: Batch 0/1
Batch Loss = 0.0007310368090935123
573, epoch_train_loss=0.0007310368090935123
Epoch 574
Epoch 574 :: Batch 0/1
Batch Loss = 0.0007310368090935123
574, epoch_train_loss=0.0007310368090935123
Epoch 575
Epoch 575 :: Batch 0/1
Batch Loss = 0.0007310368090935123
575, epoch_train_loss=0.0007310368090935123
Epoch 576
Epoch 576 :: Batch 0/1
Batch Loss = 0.0007310368090935123
576, epoch_train_loss=0.0007310368090935123
Epoch 577
Epoch 577 :: Batch 0/1
Batch Loss = 0.0007310368090935123
577, epoch_train_loss=0.0007310368090935123
Epoch 578
Epoch 578 :: Batch 0/1
Batch Loss = 0.0007310368090935123
578, epoch_train_loss=0.0007310368090935123
Epoch 579
Epoch 579 :: Batch 0/1
Batch Loss = 0.0007310368090935123
579, epoch_train_loss=0.0007310368090935123
Epoch 580
Epoch 580 :: Batch 0/1
Batch Loss = 0.0007310368090935123
580, epoch_train_loss=0.0007310368090935123
Epoch 581
Epoch 581 :: Batch 0/1
Batch Loss = 0.0007310368090935123
581, epoch_train_loss=0.0007310368090935123
Epoch 582
Epoch 582 :: Batch 0/1
Batch Loss = 0.0007310368090935123
582, epoch_train_loss=0.0007310368090935123
Epoch 583
Epoch 583 :: Batch 0/1
Batch Loss = 0.0007310368090935123
583, epoch_train_loss=0.0007310368090935123
Epoch 584
Epoch 584 :: Batch 0/1
Batch Loss = 0.0007310368090935123
584, epoch_train_loss=0.0007310368090935123
Epoch 585
Epoch 585 :: Batch 0/1
Batch Loss = 0.0007310368090935123
585, epoch_train_loss=0.0007310368090935123
Epoch 586
Epoch 586 :: Batch 0/1
Batch Loss = 0.0007310368090935123
586, epoch_train_loss=0.0007310368090935123
Epoch 587
Epoch 587 :: Batch 0/1
Batch Loss = 0.0007310368090935123
587, epoch_train_loss=0.0007310368090935123
Epoch 588
Epoch 588 :: Batch 0/1
Batch Loss = 0.0007310368090935123
588, epoch_train_loss=0.0007310368090935123
Epoch 589
Epoch 589 :: Batch 0/1
Batch Loss = 0.0007310368090935123
589, epoch_train_loss=0.0007310368090935123
Epoch 590
Epoch 590 :: Batch 0/1
Batch Loss = 0.0007310368090935123
590, epoch_train_loss=0.0007310368090935123
Epoch 591
Epoch 591 :: Batch 0/1
Batch Loss = 0.0007310368090935123
591, epoch_train_loss=0.0007310368090935123
Epoch 592
Epoch 592 :: Batch 0/1
Batch Loss = 0.0007310368090935123
592, epoch_train_loss=0.0007310368090935123
Epoch 593
Epoch 593 :: Batch 0/1
Batch Loss = 0.0007310368090935123
593, epoch_train_loss=0.0007310368090935123
Epoch 594
Epoch 594 :: Batch 0/1
Batch Loss = 0.0007310368090935123
594, epoch_train_loss=0.0007310368090935123
Epoch 595
Epoch 595 :: Batch 0/1
Batch Loss = 0.0007310368090935123
595, epoch_train_loss=0.0007310368090935123
Epoch 596
Epoch 596 :: Batch 0/1
Batch Loss = 0.0007310368090935123
596, epoch_train_loss=0.0007310368090935123
Epoch 597
Epoch 597 :: Batch 0/1
Batch Loss = 0.0007310368090935123
597, epoch_train_loss=0.0007310368090935123
Epoch 598
Epoch 598 :: Batch 0/1
Batch Loss = 0.0007310368090935123
598, epoch_train_loss=0.0007310368090935123
Epoch 599
Epoch 599 :: Batch 0/1
Batch Loss = 0.0007310368090935123
599, epoch_train_loss=0.0007310368090935123
Epoch 600
Epoch 600 :: Batch 0/1
Batch Loss = 0.0007310368090935123
600, epoch_train_loss=0.0007310368090935123
Epoch 601
Epoch 601 :: Batch 0/1
Batch Loss = 0.0007310368090935123
601, epoch_train_loss=0.0007310368090935123
Epoch 602
Epoch 602 :: Batch 0/1
Batch Loss = 0.0007310368090935123
602, epoch_train_loss=0.0007310368090935123
Epoch 603
Epoch 603 :: Batch 0/1
Batch Loss = 0.0007310368090935123
603, epoch_train_loss=0.0007310368090935123
Epoch 604
Epoch 604 :: Batch 0/1
Batch Loss = 0.0007310368090935123
604, epoch_train_loss=0.0007310368090935123
Epoch 605
Epoch 605 :: Batch 0/1
Batch Loss = 0.0007310368090935123
605, epoch_train_loss=0.0007310368090935123
Epoch 606
Epoch 606 :: Batch 0/1
Batch Loss = 0.0007310368090935123
606, epoch_train_loss=0.0007310368090935123
Epoch 607
Epoch 607 :: Batch 0/1
Batch Loss = 0.0007310368090935123
607, epoch_train_loss=0.0007310368090935123
Epoch 608
Epoch 608 :: Batch 0/1
Batch Loss = 0.0007310368090935123
608, epoch_train_loss=0.0007310368090935123
Epoch 609
Epoch 609 :: Batch 0/1
Batch Loss = 0.0007310368090935123
609, epoch_train_loss=0.0007310368090935123
Epoch 610
Epoch 610 :: Batch 0/1
Batch Loss = 0.0007310368090935123
610, epoch_train_loss=0.0007310368090935123
Epoch 611
Epoch 611 :: Batch 0/1
Batch Loss = 0.0007310368090935123
611, epoch_train_loss=0.0007310368090935123
Epoch 612
Epoch 612 :: Batch 0/1
Batch Loss = 0.0007310368090935123
612, epoch_train_loss=0.0007310368090935123
Epoch 613
Epoch 613 :: Batch 0/1
Batch Loss = 0.0007310368090935123
613, epoch_train_loss=0.0007310368090935123
Epoch 614
Epoch 614 :: Batch 0/1
Batch Loss = 0.0007310368090935123
614, epoch_train_loss=0.0007310368090935123
Epoch 615
Epoch 615 :: Batch 0/1
Batch Loss = 0.0007310368090935123
615, epoch_train_loss=0.0007310368090935123
Epoch 616
Epoch 616 :: Batch 0/1
Batch Loss = 0.0007310368090935123
616, epoch_train_loss=0.0007310368090935123
Epoch 617
Epoch 617 :: Batch 0/1
Batch Loss = 0.0007310368090935123
617, epoch_train_loss=0.0007310368090935123
Epoch 618
Epoch 618 :: Batch 0/1
Batch Loss = 0.0007310368090935123
618, epoch_train_loss=0.0007310368090935123
Epoch 619
Epoch 619 :: Batch 0/1
Batch Loss = 0.0007310368090935123
619, epoch_train_loss=0.0007310368090935123
Epoch 620
Epoch 620 :: Batch 0/1
Batch Loss = 0.0007310368090935123
620, epoch_train_loss=0.0007310368090935123
Epoch 621
Epoch 621 :: Batch 0/1
Batch Loss = 0.0007310368090935123
621, epoch_train_loss=0.0007310368090935123
Epoch 622
Epoch 622 :: Batch 0/1
Batch Loss = 0.0007310368090935123
622, epoch_train_loss=0.0007310368090935123
Epoch 623
Epoch 623 :: Batch 0/1
Batch Loss = 0.0007310368090935123
623, epoch_train_loss=0.0007310368090935123
Epoch 624
Epoch 624 :: Batch 0/1
Batch Loss = 0.0007310368090935123
624, epoch_train_loss=0.0007310368090935123
Epoch 625
Epoch 625 :: Batch 0/1
Batch Loss = 0.0007310368090935123
625, epoch_train_loss=0.0007310368090935123
Epoch 626
Epoch 626 :: Batch 0/1
Batch Loss = 0.0007310368090935123
626, epoch_train_loss=0.0007310368090935123
Epoch 627
Epoch 627 :: Batch 0/1
Batch Loss = 0.0007310368090935123
627, epoch_train_loss=0.0007310368090935123
Epoch 628
Epoch 628 :: Batch 0/1
Batch Loss = 0.0007310368090935123
628, epoch_train_loss=0.0007310368090935123
Epoch 629
Epoch 629 :: Batch 0/1
Batch Loss = 0.0007310368090935123
629, epoch_train_loss=0.0007310368090935123
Epoch 630
Epoch 630 :: Batch 0/1
Batch Loss = 0.0007310368090935123
630, epoch_train_loss=0.0007310368090935123
Epoch 631
Epoch 631 :: Batch 0/1
Batch Loss = 0.0007310368090935123
631, epoch_train_loss=0.0007310368090935123
Epoch 632
Epoch 632 :: Batch 0/1
Batch Loss = 0.0007310368090935123
632, epoch_train_loss=0.0007310368090935123
Epoch 633
Epoch 633 :: Batch 0/1
Batch Loss = 0.0007310368090935123
633, epoch_train_loss=0.0007310368090935123
Epoch 634
Epoch 634 :: Batch 0/1
Batch Loss = 0.0007310368090935123
634, epoch_train_loss=0.0007310368090935123
Epoch 635
Epoch 635 :: Batch 0/1
Batch Loss = 0.0007310368090935123
635, epoch_train_loss=0.0007310368090935123
Epoch 636
Epoch 636 :: Batch 0/1
Batch Loss = 0.0007310368090935123
636, epoch_train_loss=0.0007310368090935123
Epoch 637
Epoch 637 :: Batch 0/1
Batch Loss = 0.0007310368090935123
637, epoch_train_loss=0.0007310368090935123
Epoch 638
Epoch 638 :: Batch 0/1
Batch Loss = 0.0007310368090935123
638, epoch_train_loss=0.0007310368090935123
Epoch 639
Epoch 639 :: Batch 0/1
Batch Loss = 0.0007310368090935123
639, epoch_train_loss=0.0007310368090935123
Epoch 640
Epoch 640 :: Batch 0/1
Batch Loss = 0.0007310368090935123
640, epoch_train_loss=0.0007310368090935123
Epoch 641
Epoch 641 :: Batch 0/1
Batch Loss = 0.0007310368090935123
641, epoch_train_loss=0.0007310368090935123
Epoch 642
Epoch 642 :: Batch 0/1
Batch Loss = 0.0007310368090935123
642, epoch_train_loss=0.0007310368090935123
Epoch 643
Epoch 643 :: Batch 0/1
Batch Loss = 0.0007310368090935123
643, epoch_train_loss=0.0007310368090935123
Epoch 644
Epoch 644 :: Batch 0/1
Batch Loss = 0.0007310368090935123
644, epoch_train_loss=0.0007310368090935123
Epoch 645
Epoch 645 :: Batch 0/1
Batch Loss = 0.0007310368090935123
645, epoch_train_loss=0.0007310368090935123
Epoch 646
Epoch 646 :: Batch 0/1
Batch Loss = 0.0007310368090935123
646, epoch_train_loss=0.0007310368090935123
Epoch 647
Epoch 647 :: Batch 0/1
Batch Loss = 0.0007310368090935123
647, epoch_train_loss=0.0007310368090935123
Epoch 648
Epoch 648 :: Batch 0/1
Batch Loss = 0.0007310368090935123
648, epoch_train_loss=0.0007310368090935123
Epoch 649
Epoch 649 :: Batch 0/1
Batch Loss = 0.0007310368090935123
649, epoch_train_loss=0.0007310368090935123
Epoch 650
Epoch 650 :: Batch 0/1
Batch Loss = 0.0007310368090935123
650, epoch_train_loss=0.0007310368090935123
Epoch 651
Epoch 651 :: Batch 0/1
Batch Loss = 0.0007310368090935123
651, epoch_train_loss=0.0007310368090935123
Epoch 652
Epoch 652 :: Batch 0/1
Batch Loss = 0.0007310368090935123
652, epoch_train_loss=0.0007310368090935123
Epoch 653
Epoch 653 :: Batch 0/1
Batch Loss = 0.0007310368090935123
653, epoch_train_loss=0.0007310368090935123
Epoch 654
Epoch 654 :: Batch 0/1
Batch Loss = 0.0007310368090935123
654, epoch_train_loss=0.0007310368090935123
Epoch 655
Epoch 655 :: Batch 0/1
Batch Loss = 0.0007310368090935123
655, epoch_train_loss=0.0007310368090935123
Epoch 656
Epoch 656 :: Batch 0/1
Batch Loss = 0.0007310368090935123
656, epoch_train_loss=0.0007310368090935123
Epoch 657
Epoch 657 :: Batch 0/1
Batch Loss = 0.0007310368090935123
657, epoch_train_loss=0.0007310368090935123
Epoch 658
Epoch 658 :: Batch 0/1
Batch Loss = 0.0007310368090935123
658, epoch_train_loss=0.0007310368090935123
Epoch 659
Epoch 659 :: Batch 0/1
Batch Loss = 0.0007310368090935123
659, epoch_train_loss=0.0007310368090935123
Epoch 660
Epoch 660 :: Batch 0/1
Batch Loss = 0.0007310368090935123
660, epoch_train_loss=0.0007310368090935123
Epoch 661
Epoch 661 :: Batch 0/1
Batch Loss = 0.0007310368090935123
661, epoch_train_loss=0.0007310368090935123
Epoch 662
Epoch 662 :: Batch 0/1
Batch Loss = 0.0007310368090935123
662, epoch_train_loss=0.0007310368090935123
Epoch 663
Epoch 663 :: Batch 0/1
Batch Loss = 0.0007310368090935123
663, epoch_train_loss=0.0007310368090935123
Epoch 664
Epoch 664 :: Batch 0/1
Batch Loss = 0.0007310368090935123
664, epoch_train_loss=0.0007310368090935123
Epoch 665
Epoch 665 :: Batch 0/1
Batch Loss = 0.0007310368090935123
665, epoch_train_loss=0.0007310368090935123
Epoch 666
Epoch 666 :: Batch 0/1
Batch Loss = 0.0007310368090935123
666, epoch_train_loss=0.0007310368090935123
Epoch 667
Epoch 667 :: Batch 0/1
Batch Loss = 0.0007310368090935123
667, epoch_train_loss=0.0007310368090935123
Epoch 668
Epoch 668 :: Batch 0/1
Batch Loss = 0.0007310368090935123
668, epoch_train_loss=0.0007310368090935123
Epoch 669
Epoch 669 :: Batch 0/1
Batch Loss = 0.0007310368090935123
669, epoch_train_loss=0.0007310368090935123
Epoch 670
Epoch 670 :: Batch 0/1
Batch Loss = 0.0007310368090935123
670, epoch_train_loss=0.0007310368090935123
Epoch 671
Epoch 671 :: Batch 0/1
Batch Loss = 0.0007310368090935123
671, epoch_train_loss=0.0007310368090935123
Epoch 672
Epoch 672 :: Batch 0/1
Batch Loss = 0.0007310368090935123
672, epoch_train_loss=0.0007310368090935123
Epoch 673
Epoch 673 :: Batch 0/1
Batch Loss = 0.0007310368090935123
673, epoch_train_loss=0.0007310368090935123
Epoch 674
Epoch 674 :: Batch 0/1
Batch Loss = 0.0007310368090935123
674, epoch_train_loss=0.0007310368090935123
Epoch 675
Epoch 675 :: Batch 0/1
Batch Loss = 0.0007310368090935123
675, epoch_train_loss=0.0007310368090935123
Epoch 676
Epoch 676 :: Batch 0/1
Batch Loss = 0.0007310368090935123
676, epoch_train_loss=0.0007310368090935123
Epoch 677
Epoch 677 :: Batch 0/1
Batch Loss = 0.0007310368090935123
677, epoch_train_loss=0.0007310368090935123
Epoch 678
Epoch 678 :: Batch 0/1
Batch Loss = 0.0007310368090935123
678, epoch_train_loss=0.0007310368090935123
Epoch 679
Epoch 679 :: Batch 0/1
Batch Loss = 0.0007310368090935123
679, epoch_train_loss=0.0007310368090935123
Epoch 680
Epoch 680 :: Batch 0/1
Batch Loss = 0.0007310368090935123
680, epoch_train_loss=0.0007310368090935123
Epoch 681
Epoch 681 :: Batch 0/1
Batch Loss = 0.0007310368090935123
681, epoch_train_loss=0.0007310368090935123
Epoch 682
Epoch 682 :: Batch 0/1
Batch Loss = 0.0007310368090935123
682, epoch_train_loss=0.0007310368090935123
Epoch 683
Epoch 683 :: Batch 0/1
Batch Loss = 0.0007310368090935123
683, epoch_train_loss=0.0007310368090935123
Epoch 684
Epoch 684 :: Batch 0/1
Batch Loss = 0.0007310368090935123
684, epoch_train_loss=0.0007310368090935123
Epoch 685
Epoch 685 :: Batch 0/1
Batch Loss = 0.0007310368090935123
685, epoch_train_loss=0.0007310368090935123
Epoch 686
Epoch 686 :: Batch 0/1
Batch Loss = 0.0007310368090935123
686, epoch_train_loss=0.0007310368090935123
Epoch 687
Epoch 687 :: Batch 0/1
Batch Loss = 0.0007310368090935123
687, epoch_train_loss=0.0007310368090935123
Epoch 688
Epoch 688 :: Batch 0/1
Batch Loss = 0.0007310368090935123
688, epoch_train_loss=0.0007310368090935123
Epoch 689
Epoch 689 :: Batch 0/1
Batch Loss = 0.0007310368090935123
689, epoch_train_loss=0.0007310368090935123
Epoch 690
Epoch 690 :: Batch 0/1
Batch Loss = 0.0007310368090935123
690, epoch_train_loss=0.0007310368090935123
Epoch 691
Epoch 691 :: Batch 0/1
Batch Loss = 0.0007310368090935123
691, epoch_train_loss=0.0007310368090935123
Epoch 692
Epoch 692 :: Batch 0/1
Batch Loss = 0.0007310368090935123
692, epoch_train_loss=0.0007310368090935123
Epoch 693
Epoch 693 :: Batch 0/1
Batch Loss = 0.0007310368090935123
693, epoch_train_loss=0.0007310368090935123
Epoch 694
Epoch 694 :: Batch 0/1
Batch Loss = 0.0007310368090935123
694, epoch_train_loss=0.0007310368090935123
Epoch 695
Epoch 695 :: Batch 0/1
Batch Loss = 0.0007310368090935123
695, epoch_train_loss=0.0007310368090935123
Epoch 696
Epoch 696 :: Batch 0/1
Batch Loss = 0.0007310368090935123
696, epoch_train_loss=0.0007310368090935123
Epoch 697
Epoch 697 :: Batch 0/1
Batch Loss = 0.0007310368090935123
697, epoch_train_loss=0.0007310368090935123
Epoch 698
Epoch 698 :: Batch 0/1
Batch Loss = 0.0007310368090935123
698, epoch_train_loss=0.0007310368090935123
Epoch 699
Epoch 699 :: Batch 0/1
Batch Loss = 0.0007310368090935123
699, epoch_train_loss=0.0007310368090935123
Epoch 700
Epoch 700 :: Batch 0/1
Batch Loss = 0.0007310368090935123
700, epoch_train_loss=0.0007310368090935123
Epoch 701
Epoch 701 :: Batch 0/1
Batch Loss = 0.0007310368090935123
701, epoch_train_loss=0.0007310368090935123
Epoch 702
Epoch 702 :: Batch 0/1
Batch Loss = 0.0007310368090935123
702, epoch_train_loss=0.0007310368090935123
Epoch 703
Epoch 703 :: Batch 0/1
Batch Loss = 0.0007310368090935123
703, epoch_train_loss=0.0007310368090935123
Epoch 704
Epoch 704 :: Batch 0/1
Batch Loss = 0.0007310368090935123
704, epoch_train_loss=0.0007310368090935123
Epoch 705
Epoch 705 :: Batch 0/1
Batch Loss = 0.0007310368090935123
705, epoch_train_loss=0.0007310368090935123
Epoch 706
Epoch 706 :: Batch 0/1
Batch Loss = 0.0007310368090935123
706, epoch_train_loss=0.0007310368090935123
Epoch 707
Epoch 707 :: Batch 0/1
Batch Loss = 0.0007310368090935123
707, epoch_train_loss=0.0007310368090935123
Epoch 708
Epoch 708 :: Batch 0/1
Batch Loss = 0.0007310368090935123
708, epoch_train_loss=0.0007310368090935123
Epoch 709
Epoch 709 :: Batch 0/1
Batch Loss = 0.0007310368090935123
709, epoch_train_loss=0.0007310368090935123
Epoch 710
Epoch 710 :: Batch 0/1
Batch Loss = 0.0007310368090935123
710, epoch_train_loss=0.0007310368090935123
Epoch 711
Epoch 711 :: Batch 0/1
Batch Loss = 0.0007310368090935123
711, epoch_train_loss=0.0007310368090935123
Epoch 712
Epoch 712 :: Batch 0/1
Batch Loss = 0.0007310368090935123
712, epoch_train_loss=0.0007310368090935123
Epoch 713
Epoch 713 :: Batch 0/1
Batch Loss = 0.0007310368090935123
713, epoch_train_loss=0.0007310368090935123
Epoch 714
Epoch 714 :: Batch 0/1
Batch Loss = 0.0007310368090935123
714, epoch_train_loss=0.0007310368090935123
Epoch 715
Epoch 715 :: Batch 0/1
Batch Loss = 0.0007310368090935123
715, epoch_train_loss=0.0007310368090935123
Epoch 716
Epoch 716 :: Batch 0/1
Batch Loss = 0.0007310368090935123
716, epoch_train_loss=0.0007310368090935123
Epoch 717
Epoch 717 :: Batch 0/1
Batch Loss = 0.0007310368090935123
717, epoch_train_loss=0.0007310368090935123
Epoch 718
Epoch 718 :: Batch 0/1
Batch Loss = 0.0007310368090935123
718, epoch_train_loss=0.0007310368090935123
Epoch 719
Epoch 719 :: Batch 0/1
Batch Loss = 0.0007310368090935123
719, epoch_train_loss=0.0007310368090935123
Epoch 720
Epoch 720 :: Batch 0/1
Batch Loss = 0.0007310368090935123
720, epoch_train_loss=0.0007310368090935123
Epoch 721
Epoch 721 :: Batch 0/1
Batch Loss = 0.0007310368090935123
721, epoch_train_loss=0.0007310368090935123
Epoch 722
Epoch 722 :: Batch 0/1
Batch Loss = 0.0007310368090935123
722, epoch_train_loss=0.0007310368090935123
Epoch 723
Epoch 723 :: Batch 0/1
Batch Loss = 0.0007310368090935123
723, epoch_train_loss=0.0007310368090935123
Epoch 724
Epoch 724 :: Batch 0/1
Batch Loss = 0.0007310368090935123
724, epoch_train_loss=0.0007310368090935123
Epoch 725
Epoch 725 :: Batch 0/1
Batch Loss = 0.0007310368090935123
725, epoch_train_loss=0.0007310368090935123
Epoch 726
Epoch 726 :: Batch 0/1
Batch Loss = 0.0007310368090935123
726, epoch_train_loss=0.0007310368090935123
Epoch 727
Epoch 727 :: Batch 0/1
Batch Loss = 0.0007310368090935123
727, epoch_train_loss=0.0007310368090935123
Epoch 728
Epoch 728 :: Batch 0/1
Batch Loss = 0.0007310368090935123
728, epoch_train_loss=0.0007310368090935123
Epoch 729
Epoch 729 :: Batch 0/1
Batch Loss = 0.0007310368090935123
729, epoch_train_loss=0.0007310368090935123
Epoch 730
Epoch 730 :: Batch 0/1
Batch Loss = 0.0007310368090935123
730, epoch_train_loss=0.0007310368090935123
Epoch 731
Epoch 731 :: Batch 0/1
Batch Loss = 0.0007310368090935123
731, epoch_train_loss=0.0007310368090935123
Epoch 732
Epoch 732 :: Batch 0/1
Batch Loss = 0.0007310368090935123
732, epoch_train_loss=0.0007310368090935123
Epoch 733
Epoch 733 :: Batch 0/1
Batch Loss = 0.0007310368090935123
733, epoch_train_loss=0.0007310368090935123
Epoch 734
Epoch 734 :: Batch 0/1
Batch Loss = 0.0007310368090935123
734, epoch_train_loss=0.0007310368090935123
Epoch 735
Epoch 735 :: Batch 0/1
Batch Loss = 0.0007310368090935123
735, epoch_train_loss=0.0007310368090935123
Epoch 736
Epoch 736 :: Batch 0/1
Batch Loss = 0.0007310368090935123
736, epoch_train_loss=0.0007310368090935123
Epoch 737
Epoch 737 :: Batch 0/1
Batch Loss = 0.0007310368090935123
737, epoch_train_loss=0.0007310368090935123
Epoch 738
Epoch 738 :: Batch 0/1
Batch Loss = 0.0007310368090935123
738, epoch_train_loss=0.0007310368090935123
Epoch 739
Epoch 739 :: Batch 0/1
Batch Loss = 0.0007310368090935123
739, epoch_train_loss=0.0007310368090935123
Epoch 740
Epoch 740 :: Batch 0/1
Batch Loss = 0.0007310368090935123
740, epoch_train_loss=0.0007310368090935123
Epoch 741
Epoch 741 :: Batch 0/1
Batch Loss = 0.0007310368090935123
741, epoch_train_loss=0.0007310368090935123
Epoch 742
Epoch 742 :: Batch 0/1
Batch Loss = 0.0007310368090935123
742, epoch_train_loss=0.0007310368090935123
Epoch 743
Epoch 743 :: Batch 0/1
Batch Loss = 0.0007310368090935123
743, epoch_train_loss=0.0007310368090935123
Epoch 744
Epoch 744 :: Batch 0/1
Batch Loss = 0.0007310368090935123
744, epoch_train_loss=0.0007310368090935123
Epoch 745
Epoch 745 :: Batch 0/1
Batch Loss = 0.0007310368090935123
745, epoch_train_loss=0.0007310368090935123
Epoch 746
Epoch 746 :: Batch 0/1
Batch Loss = 0.0007310368090935123
746, epoch_train_loss=0.0007310368090935123
Epoch 747
Epoch 747 :: Batch 0/1
Batch Loss = 0.0007310368090935123
747, epoch_train_loss=0.0007310368090935123
Epoch 748
Epoch 748 :: Batch 0/1
Batch Loss = 0.0007310368090935123
748, epoch_train_loss=0.0007310368090935123
Epoch 749
Epoch 749 :: Batch 0/1
Batch Loss = 0.0007310368090935123
749, epoch_train_loss=0.0007310368090935123
Epoch 750
Epoch 750 :: Batch 0/1
Batch Loss = 0.0007310368090935123
750, epoch_train_loss=0.0007310368090935123
Epoch 751
Epoch 751 :: Batch 0/1
Batch Loss = 0.0007310368090935123
751, epoch_train_loss=0.0007310368090935123
Epoch 752
Epoch 752 :: Batch 0/1
Batch Loss = 0.0007310368090935123
752, epoch_train_loss=0.0007310368090935123
Epoch 753
Epoch 753 :: Batch 0/1
Batch Loss = 0.0007310368090935123
753, epoch_train_loss=0.0007310368090935123
Epoch 754
Epoch 754 :: Batch 0/1
Batch Loss = 0.0007310368090935123
754, epoch_train_loss=0.0007310368090935123
Epoch 755
Epoch 755 :: Batch 0/1
Batch Loss = 0.0007310368090935123
755, epoch_train_loss=0.0007310368090935123
Epoch 756
Epoch 756 :: Batch 0/1
Batch Loss = 0.0007310368090935123
756, epoch_train_loss=0.0007310368090935123
Epoch 757
Epoch 757 :: Batch 0/1
Batch Loss = 0.0007310368090935123
757, epoch_train_loss=0.0007310368090935123
Epoch 758
Epoch 758 :: Batch 0/1
Batch Loss = 0.0007310368090935123
758, epoch_train_loss=0.0007310368090935123
Epoch 759
Epoch 759 :: Batch 0/1
Batch Loss = 0.0007310368090935123
759, epoch_train_loss=0.0007310368090935123
Epoch 760
Epoch 760 :: Batch 0/1
Batch Loss = 0.0007310368090935123
760, epoch_train_loss=0.0007310368090935123
Epoch 761
Epoch 761 :: Batch 0/1
Batch Loss = 0.0007310368090935123
761, epoch_train_loss=0.0007310368090935123
Epoch 762
Epoch 762 :: Batch 0/1
Batch Loss = 0.0007310368090935123
762, epoch_train_loss=0.0007310368090935123
Epoch 763
Epoch 763 :: Batch 0/1
Batch Loss = 0.0007310368090935123
763, epoch_train_loss=0.0007310368090935123
Epoch 764
Epoch 764 :: Batch 0/1
Batch Loss = 0.0007310368090935123
764, epoch_train_loss=0.0007310368090935123
Epoch 765
Epoch 765 :: Batch 0/1
Batch Loss = 0.0007310368090935123
765, epoch_train_loss=0.0007310368090935123
Epoch 766
Epoch 766 :: Batch 0/1
Batch Loss = 0.0007310368090935123
766, epoch_train_loss=0.0007310368090935123
Epoch 767
Epoch 767 :: Batch 0/1
Batch Loss = 0.0007310368090935123
767, epoch_train_loss=0.0007310368090935123
Epoch 768
Epoch 768 :: Batch 0/1
Batch Loss = 0.0007310368090935123
768, epoch_train_loss=0.0007310368090935123
Epoch 769
Epoch 769 :: Batch 0/1
Batch Loss = 0.0007310368090935123
769, epoch_train_loss=0.0007310368090935123
Epoch 770
Epoch 770 :: Batch 0/1
Batch Loss = 0.0007310368090935123
770, epoch_train_loss=0.0007310368090935123
Epoch 771
Epoch 771 :: Batch 0/1
Batch Loss = 0.0007310368090935123
771, epoch_train_loss=0.0007310368090935123
Epoch 772
Epoch 772 :: Batch 0/1
Batch Loss = 0.0007310368090935123
772, epoch_train_loss=0.0007310368090935123
Epoch 773
Epoch 773 :: Batch 0/1
Batch Loss = 0.0007310368090935123
773, epoch_train_loss=0.0007310368090935123
Epoch 774
Epoch 774 :: Batch 0/1
Batch Loss = 0.0007310368090935123
774, epoch_train_loss=0.0007310368090935123
Epoch 775
Epoch 775 :: Batch 0/1
Batch Loss = 0.0007310368090935123
775, epoch_train_loss=0.0007310368090935123
Epoch 776
Epoch 776 :: Batch 0/1
Batch Loss = 0.0007310368090935123
776, epoch_train_loss=0.0007310368090935123
Epoch 777
Epoch 777 :: Batch 0/1
Batch Loss = 0.0007310368090935123
777, epoch_train_loss=0.0007310368090935123
Epoch 778
Epoch 778 :: Batch 0/1
Batch Loss = 0.0007310368090935123
778, epoch_train_loss=0.0007310368090935123
Epoch 779
Epoch 779 :: Batch 0/1
Batch Loss = 0.0007310368090935123
779, epoch_train_loss=0.0007310368090935123
Epoch 780
Epoch 780 :: Batch 0/1
Batch Loss = 0.0007310368090935123
780, epoch_train_loss=0.0007310368090935123
Epoch 781
Epoch 781 :: Batch 0/1
Batch Loss = 0.0007310368090935123
781, epoch_train_loss=0.0007310368090935123
Epoch 782
Epoch 782 :: Batch 0/1
Batch Loss = 0.0007310368090935123
782, epoch_train_loss=0.0007310368090935123
Epoch 783
Epoch 783 :: Batch 0/1
Batch Loss = 0.0007310368090935123
783, epoch_train_loss=0.0007310368090935123
Epoch 784
Epoch 784 :: Batch 0/1
Batch Loss = 0.0007310368090935123
784, epoch_train_loss=0.0007310368090935123
Epoch 785
Epoch 785 :: Batch 0/1
Batch Loss = 0.0007310368090935123
785, epoch_train_loss=0.0007310368090935123
Epoch 786
Epoch 786 :: Batch 0/1
Batch Loss = 0.0007310368090935123
786, epoch_train_loss=0.0007310368090935123
Epoch 787
Epoch 787 :: Batch 0/1
Batch Loss = 0.0007310368090935123
787, epoch_train_loss=0.0007310368090935123
Epoch 788
Epoch 788 :: Batch 0/1
Batch Loss = 0.0007310368090935123
788, epoch_train_loss=0.0007310368090935123
Epoch 789
Epoch 789 :: Batch 0/1
Batch Loss = 0.0007310368090935123
789, epoch_train_loss=0.0007310368090935123
Epoch 790
Epoch 790 :: Batch 0/1
Batch Loss = 0.0007310368090935123
790, epoch_train_loss=0.0007310368090935123
Epoch 791
Epoch 791 :: Batch 0/1
Batch Loss = 0.0007310368090935123
791, epoch_train_loss=0.0007310368090935123
Epoch 792
Epoch 792 :: Batch 0/1
Batch Loss = 0.0007310368090935123
792, epoch_train_loss=0.0007310368090935123
Epoch 793
Epoch 793 :: Batch 0/1
Batch Loss = 0.0007310368090935123
793, epoch_train_loss=0.0007310368090935123
Epoch 794
Epoch 794 :: Batch 0/1
Batch Loss = 0.0007310368090935123
794, epoch_train_loss=0.0007310368090935123
Epoch 795
Epoch 795 :: Batch 0/1
Batch Loss = 0.0007310368090935123
795, epoch_train_loss=0.0007310368090935123
Epoch 796
Epoch 796 :: Batch 0/1
Batch Loss = 0.0007310368090935123
796, epoch_train_loss=0.0007310368090935123
Epoch 797
Epoch 797 :: Batch 0/1
Batch Loss = 0.0007310368090935123
797, epoch_train_loss=0.0007310368090935123
Epoch 798
Epoch 798 :: Batch 0/1
Batch Loss = 0.0007310368090935123
798, epoch_train_loss=0.0007310368090935123
Epoch 799
Epoch 799 :: Batch 0/1
Batch Loss = 0.0007310368090935123
799, epoch_train_loss=0.0007310368090935123
Epoch 800
Epoch 800 :: Batch 0/1
Batch Loss = 0.0007310368090935123
800, epoch_train_loss=0.0007310368090935123
Epoch 801
Epoch 801 :: Batch 0/1
Batch Loss = 0.0007310368090935123
801, epoch_train_loss=0.0007310368090935123
Epoch 802
Epoch 802 :: Batch 0/1
Batch Loss = 0.0007310368090935123
802, epoch_train_loss=0.0007310368090935123
Epoch 803
Epoch 803 :: Batch 0/1
Batch Loss = 0.0007310368090935123
803, epoch_train_loss=0.0007310368090935123
Epoch 804
Epoch 804 :: Batch 0/1
Batch Loss = 0.0007310368090935123
804, epoch_train_loss=0.0007310368090935123
Epoch 805
Epoch 805 :: Batch 0/1
Batch Loss = 0.0007310368090935123
805, epoch_train_loss=0.0007310368090935123
Epoch 806
Epoch 806 :: Batch 0/1
Batch Loss = 0.0007310368090935123
806, epoch_train_loss=0.0007310368090935123
Epoch 807
Epoch 807 :: Batch 0/1
Batch Loss = 0.0007310368090935123
807, epoch_train_loss=0.0007310368090935123
Epoch 808
Epoch 808 :: Batch 0/1
Batch Loss = 0.0007310368090935123
808, epoch_train_loss=0.0007310368090935123
Epoch 809
Epoch 809 :: Batch 0/1
Batch Loss = 0.0007310368090935123
809, epoch_train_loss=0.0007310368090935123
Epoch 810
Epoch 810 :: Batch 0/1
Batch Loss = 0.0007310368090935123
810, epoch_train_loss=0.0007310368090935123
Epoch 811
Epoch 811 :: Batch 0/1
Batch Loss = 0.0007310368090935123
811, epoch_train_loss=0.0007310368090935123
Epoch 812
Epoch 812 :: Batch 0/1
Batch Loss = 0.0007310368090935123
812, epoch_train_loss=0.0007310368090935123
Epoch 813
Epoch 813 :: Batch 0/1
Batch Loss = 0.0007310368090935123
813, epoch_train_loss=0.0007310368090935123
Epoch 814
Epoch 814 :: Batch 0/1
Batch Loss = 0.0007310368090935123
814, epoch_train_loss=0.0007310368090935123
Epoch 815
Epoch 815 :: Batch 0/1
Batch Loss = 0.0007310368090935123
815, epoch_train_loss=0.0007310368090935123
Epoch 816
Epoch 816 :: Batch 0/1
Batch Loss = 0.0007310368090935123
816, epoch_train_loss=0.0007310368090935123
Epoch 817
Epoch 817 :: Batch 0/1
Batch Loss = 0.0007310368090935123
817, epoch_train_loss=0.0007310368090935123
Epoch 818
Epoch 818 :: Batch 0/1
Batch Loss = 0.0007310368090935123
818, epoch_train_loss=0.0007310368090935123
Epoch 819
Epoch 819 :: Batch 0/1
Batch Loss = 0.0007310368090935123
819, epoch_train_loss=0.0007310368090935123
Epoch 820
Epoch 820 :: Batch 0/1
Batch Loss = 0.0007310368090935123
820, epoch_train_loss=0.0007310368090935123
Epoch 821
Epoch 821 :: Batch 0/1
Batch Loss = 0.0007310368090935123
821, epoch_train_loss=0.0007310368090935123
Epoch 822
Epoch 822 :: Batch 0/1
Batch Loss = 0.0007310368090935123
822, epoch_train_loss=0.0007310368090935123
Epoch 823
Epoch 823 :: Batch 0/1
Batch Loss = 0.0007310368090935123
823, epoch_train_loss=0.0007310368090935123
Epoch 824
Epoch 824 :: Batch 0/1
Batch Loss = 0.0007310368090935123
824, epoch_train_loss=0.0007310368090935123
Epoch 825
Epoch 825 :: Batch 0/1
Batch Loss = 0.0007310368090935123
825, epoch_train_loss=0.0007310368090935123
Epoch 826
Epoch 826 :: Batch 0/1
Batch Loss = 0.0007310368090935123
826, epoch_train_loss=0.0007310368090935123
Epoch 827
Epoch 827 :: Batch 0/1
Batch Loss = 0.0007310368090935123
827, epoch_train_loss=0.0007310368090935123
Epoch 828
Epoch 828 :: Batch 0/1
Batch Loss = 0.0007310368090935123
828, epoch_train_loss=0.0007310368090935123
Epoch 829
Epoch 829 :: Batch 0/1
Batch Loss = 0.0007310368090935123
829, epoch_train_loss=0.0007310368090935123
Epoch 830
Epoch 830 :: Batch 0/1
Batch Loss = 0.0007310368090935123
830, epoch_train_loss=0.0007310368090935123
Epoch 831
Epoch 831 :: Batch 0/1
Batch Loss = 0.0007310368090935123
831, epoch_train_loss=0.0007310368090935123
Epoch 832
Epoch 832 :: Batch 0/1
Batch Loss = 0.0007310368090935123
832, epoch_train_loss=0.0007310368090935123
Epoch 833
Epoch 833 :: Batch 0/1
Batch Loss = 0.0007310368090935123
833, epoch_train_loss=0.0007310368090935123
Epoch 834
Epoch 834 :: Batch 0/1
Batch Loss = 0.0007310368090935123
834, epoch_train_loss=0.0007310368090935123
Epoch 835
Epoch 835 :: Batch 0/1
Batch Loss = 0.0007310368090935123
835, epoch_train_loss=0.0007310368090935123
Epoch 836
Epoch 836 :: Batch 0/1
Batch Loss = 0.0007310368090935123
836, epoch_train_loss=0.0007310368090935123
Epoch 837
Epoch 837 :: Batch 0/1
Batch Loss = 0.0007310368090935123
837, epoch_train_loss=0.0007310368090935123
Epoch 838
Epoch 838 :: Batch 0/1
Batch Loss = 0.0007310368090935123
838, epoch_train_loss=0.0007310368090935123
Epoch 839
Epoch 839 :: Batch 0/1
Batch Loss = 0.0007310368090935123
839, epoch_train_loss=0.0007310368090935123
Epoch 840
Epoch 840 :: Batch 0/1
Batch Loss = 0.0007310368090935123
840, epoch_train_loss=0.0007310368090935123
Epoch 841
Epoch 841 :: Batch 0/1
Batch Loss = 0.0007310368090935123
841, epoch_train_loss=0.0007310368090935123
Epoch 842
Epoch 842 :: Batch 0/1
Batch Loss = 0.0007310368090935123
842, epoch_train_loss=0.0007310368090935123
Epoch 843
Epoch 843 :: Batch 0/1
Batch Loss = 0.0007310368090935123
843, epoch_train_loss=0.0007310368090935123
Epoch 844
Epoch 844 :: Batch 0/1
Batch Loss = 0.0007310368090935123
844, epoch_train_loss=0.0007310368090935123
Epoch 845
Epoch 845 :: Batch 0/1
Batch Loss = 0.0007310368090935123
845, epoch_train_loss=0.0007310368090935123
Epoch 846
Epoch 846 :: Batch 0/1
Batch Loss = 0.0007310368090935123
846, epoch_train_loss=0.0007310368090935123
Epoch 847
Epoch 847 :: Batch 0/1
Batch Loss = 0.0007310368090935123
847, epoch_train_loss=0.0007310368090935123
Epoch 848
Epoch 848 :: Batch 0/1
Batch Loss = 0.0007310368090935123
848, epoch_train_loss=0.0007310368090935123
Epoch 849
Epoch 849 :: Batch 0/1
Batch Loss = 0.0007310368090935123
849, epoch_train_loss=0.0007310368090935123
Epoch 850
Epoch 850 :: Batch 0/1
Batch Loss = 0.0007310368090935123
850, epoch_train_loss=0.0007310368090935123
Epoch 851
Epoch 851 :: Batch 0/1
Batch Loss = 0.0007310368090935123
851, epoch_train_loss=0.0007310368090935123
Epoch 852
Epoch 852 :: Batch 0/1
Batch Loss = 0.0007310368090935123
852, epoch_train_loss=0.0007310368090935123
Epoch 853
Epoch 853 :: Batch 0/1
Batch Loss = 0.0007310368090935123
853, epoch_train_loss=0.0007310368090935123
Epoch 854
Epoch 854 :: Batch 0/1
Batch Loss = 0.0007310368090935123
854, epoch_train_loss=0.0007310368090935123
Epoch 855
Epoch 855 :: Batch 0/1
Batch Loss = 0.0007310368090935123
855, epoch_train_loss=0.0007310368090935123
Epoch 856
Epoch 856 :: Batch 0/1
Batch Loss = 0.0007310368090935123
856, epoch_train_loss=0.0007310368090935123
Epoch 857
Epoch 857 :: Batch 0/1
Batch Loss = 0.0007310368090935123
857, epoch_train_loss=0.0007310368090935123
Epoch 858
Epoch 858 :: Batch 0/1
Batch Loss = 0.0007310368090935123
858, epoch_train_loss=0.0007310368090935123
Epoch 859
Epoch 859 :: Batch 0/1
Batch Loss = 0.0007310368090935123
859, epoch_train_loss=0.0007310368090935123
Epoch 860
Epoch 860 :: Batch 0/1
Batch Loss = 0.0007310368090935123
860, epoch_train_loss=0.0007310368090935123
Epoch 861
Epoch 861 :: Batch 0/1
Batch Loss = 0.0007310368090935123
861, epoch_train_loss=0.0007310368090935123
Epoch 862
Epoch 862 :: Batch 0/1
Batch Loss = 0.0007310368090935123
862, epoch_train_loss=0.0007310368090935123
Epoch 863
Epoch 863 :: Batch 0/1
Batch Loss = 0.0007310368090935123
863, epoch_train_loss=0.0007310368090935123
Epoch 864
Epoch 864 :: Batch 0/1
Batch Loss = 0.0007310368090935123
864, epoch_train_loss=0.0007310368090935123
Epoch 865
Epoch 865 :: Batch 0/1
Batch Loss = 0.0007310368090935123
865, epoch_train_loss=0.0007310368090935123
Epoch 866
Epoch 866 :: Batch 0/1
Batch Loss = 0.0007310368090935123
866, epoch_train_loss=0.0007310368090935123
Epoch 867
Epoch 867 :: Batch 0/1
Batch Loss = 0.0007310368090935123
867, epoch_train_loss=0.0007310368090935123
Epoch 868
Epoch 868 :: Batch 0/1
Batch Loss = 0.0007310368090935123
868, epoch_train_loss=0.0007310368090935123
Epoch 869
Epoch 869 :: Batch 0/1
Batch Loss = 0.0007310368090935123
869, epoch_train_loss=0.0007310368090935123
Epoch 870
Epoch 870 :: Batch 0/1
Batch Loss = 0.0007310368090935123
870, epoch_train_loss=0.0007310368090935123
Epoch 871
Epoch 871 :: Batch 0/1
Batch Loss = 0.0007310368090935123
871, epoch_train_loss=0.0007310368090935123
Epoch 872
Epoch 872 :: Batch 0/1
Batch Loss = 0.0007310368090935123
872, epoch_train_loss=0.0007310368090935123
Epoch 873
Epoch 873 :: Batch 0/1
Batch Loss = 0.0007310368090935123
873, epoch_train_loss=0.0007310368090935123
Epoch 874
Epoch 874 :: Batch 0/1
Batch Loss = 0.0007310368090935123
874, epoch_train_loss=0.0007310368090935123
Epoch 875
Epoch 875 :: Batch 0/1
Batch Loss = 0.0007310368090935123
875, epoch_train_loss=0.0007310368090935123
Epoch 876
Epoch 876 :: Batch 0/1
Batch Loss = 0.0007310368090935123
876, epoch_train_loss=0.0007310368090935123
Epoch 877
Epoch 877 :: Batch 0/1
Batch Loss = 0.0007310368090935123
877, epoch_train_loss=0.0007310368090935123
Epoch 878
Epoch 878 :: Batch 0/1
Batch Loss = 0.0007310368090935123
878, epoch_train_loss=0.0007310368090935123
Epoch 879
Epoch 879 :: Batch 0/1
Batch Loss = 0.0007310368090935123
879, epoch_train_loss=0.0007310368090935123
Epoch 880
Epoch 880 :: Batch 0/1
Batch Loss = 0.0007310368090935123
880, epoch_train_loss=0.0007310368090935123
Epoch 881
Epoch 881 :: Batch 0/1
Batch Loss = 0.0007310368090935123
881, epoch_train_loss=0.0007310368090935123
Epoch 882
Epoch 882 :: Batch 0/1
Batch Loss = 0.0007310368090935123
882, epoch_train_loss=0.0007310368090935123
Epoch 883
Epoch 883 :: Batch 0/1
Batch Loss = 0.0007310368090935123
883, epoch_train_loss=0.0007310368090935123
Epoch 884
Epoch 884 :: Batch 0/1
Batch Loss = 0.0007310368090935123
884, epoch_train_loss=0.0007310368090935123
Epoch 885
Epoch 885 :: Batch 0/1
Batch Loss = 0.0007310368090935123
885, epoch_train_loss=0.0007310368090935123
Epoch 886
Epoch 886 :: Batch 0/1
Batch Loss = 0.0007310368090935123
886, epoch_train_loss=0.0007310368090935123
Epoch 887
Epoch 887 :: Batch 0/1
Batch Loss = 0.0007310368090935123
887, epoch_train_loss=0.0007310368090935123
Epoch 888
Epoch 888 :: Batch 0/1
Batch Loss = 0.0007310368090935123
888, epoch_train_loss=0.0007310368090935123
Epoch 889
Epoch 889 :: Batch 0/1
Batch Loss = 0.0007310368090935123
889, epoch_train_loss=0.0007310368090935123
Epoch 890
Epoch 890 :: Batch 0/1
Batch Loss = 0.0007310368090935123
890, epoch_train_loss=0.0007310368090935123
Epoch 891
Epoch 891 :: Batch 0/1
Batch Loss = 0.0007310368090935123
891, epoch_train_loss=0.0007310368090935123
Epoch 892
Epoch 892 :: Batch 0/1
Batch Loss = 0.0007310368090935123
892, epoch_train_loss=0.0007310368090935123
Epoch 893
Epoch 893 :: Batch 0/1
Batch Loss = 0.0007310368090935123
893, epoch_train_loss=0.0007310368090935123
Epoch 894
Epoch 894 :: Batch 0/1
Batch Loss = 0.0007310368090935123
894, epoch_train_loss=0.0007310368090935123
Epoch 895
Epoch 895 :: Batch 0/1
Batch Loss = 0.0007310368090935123
895, epoch_train_loss=0.0007310368090935123
Epoch 896
Epoch 896 :: Batch 0/1
Batch Loss = 0.0007310368090935123
896, epoch_train_loss=0.0007310368090935123
Epoch 897
Epoch 897 :: Batch 0/1
Batch Loss = 0.0007310368090935123
897, epoch_train_loss=0.0007310368090935123
Epoch 898
Epoch 898 :: Batch 0/1
Batch Loss = 0.0007310368090935123
898, epoch_train_loss=0.0007310368090935123
Epoch 899
Epoch 899 :: Batch 0/1
Batch Loss = 0.0007310368090935123
899, epoch_train_loss=0.0007310368090935123
Epoch 900
Epoch 900 :: Batch 0/1
Batch Loss = 0.0007310368090935123
900, epoch_train_loss=0.0007310368090935123
Epoch 901
Epoch 901 :: Batch 0/1
Batch Loss = 0.0007310368090935123
901, epoch_train_loss=0.0007310368090935123
Epoch 902
Epoch 902 :: Batch 0/1
Batch Loss = 0.0007310368090935123
902, epoch_train_loss=0.0007310368090935123
Epoch 903
Epoch 903 :: Batch 0/1
Batch Loss = 0.0007310368090935123
903, epoch_train_loss=0.0007310368090935123
Epoch 904
Epoch 904 :: Batch 0/1
Batch Loss = 0.0007310368090935123
904, epoch_train_loss=0.0007310368090935123
Epoch 905
Epoch 905 :: Batch 0/1
Batch Loss = 0.0007310368090935123
905, epoch_train_loss=0.0007310368090935123
Epoch 906
Epoch 906 :: Batch 0/1
Batch Loss = 0.0007310368090935123
906, epoch_train_loss=0.0007310368090935123
Epoch 907
Epoch 907 :: Batch 0/1
Batch Loss = 0.0007310368090935123
907, epoch_train_loss=0.0007310368090935123
Epoch 908
Epoch 908 :: Batch 0/1
Batch Loss = 0.0007310368090935123
908, epoch_train_loss=0.0007310368090935123
Epoch 909
Epoch 909 :: Batch 0/1
Batch Loss = 0.0007310368090935123
909, epoch_train_loss=0.0007310368090935123
Epoch 910
Epoch 910 :: Batch 0/1
Batch Loss = 0.0007310368090935123
910, epoch_train_loss=0.0007310368090935123
Epoch 911
Epoch 911 :: Batch 0/1
Batch Loss = 0.0007310368090935123
911, epoch_train_loss=0.0007310368090935123
Epoch 912
Epoch 912 :: Batch 0/1
Batch Loss = 0.0007310368090935123
912, epoch_train_loss=0.0007310368090935123
Epoch 913
Epoch 913 :: Batch 0/1
Batch Loss = 0.0007310368090935123
913, epoch_train_loss=0.0007310368090935123
Epoch 914
Epoch 914 :: Batch 0/1
Batch Loss = 0.0007310368090935123
914, epoch_train_loss=0.0007310368090935123
Epoch 915
Epoch 915 :: Batch 0/1
Batch Loss = 0.0007310368090935123
915, epoch_train_loss=0.0007310368090935123
Epoch 916
Epoch 916 :: Batch 0/1
Batch Loss = 0.0007310368090935123
916, epoch_train_loss=0.0007310368090935123
Epoch 917
Epoch 917 :: Batch 0/1
Batch Loss = 0.0007310368090935123
917, epoch_train_loss=0.0007310368090935123
Epoch 918
Epoch 918 :: Batch 0/1
Batch Loss = 0.0007310368090935123
918, epoch_train_loss=0.0007310368090935123
Epoch 919
Epoch 919 :: Batch 0/1
Batch Loss = 0.0007310368090935123
919, epoch_train_loss=0.0007310368090935123
Epoch 920
Epoch 920 :: Batch 0/1
Batch Loss = 0.0007310368090935123
920, epoch_train_loss=0.0007310368090935123
Epoch 921
Epoch 921 :: Batch 0/1
Batch Loss = 0.0007310368090935123
921, epoch_train_loss=0.0007310368090935123
Epoch 922
Epoch 922 :: Batch 0/1
Batch Loss = 0.0007310368090935123
922, epoch_train_loss=0.0007310368090935123
Epoch 923
Epoch 923 :: Batch 0/1
Batch Loss = 0.0007310368090935123
923, epoch_train_loss=0.0007310368090935123
Epoch 924
Epoch 924 :: Batch 0/1
Batch Loss = 0.0007310368090935123
924, epoch_train_loss=0.0007310368090935123
Epoch 925
Epoch 925 :: Batch 0/1
Batch Loss = 0.0007310368090935123
925, epoch_train_loss=0.0007310368090935123
Epoch 926
Epoch 926 :: Batch 0/1
Batch Loss = 0.0007310368090935123
926, epoch_train_loss=0.0007310368090935123
Epoch 927
Epoch 927 :: Batch 0/1
Batch Loss = 0.0007310368090935123
927, epoch_train_loss=0.0007310368090935123
Epoch 928
Epoch 928 :: Batch 0/1
Batch Loss = 0.0007310368090935123
928, epoch_train_loss=0.0007310368090935123
Epoch 929
Epoch 929 :: Batch 0/1
Batch Loss = 0.0007310368090935123
929, epoch_train_loss=0.0007310368090935123
Epoch 930
Epoch 930 :: Batch 0/1
Batch Loss = 0.0007310368090935123
930, epoch_train_loss=0.0007310368090935123
Epoch 931
Epoch 931 :: Batch 0/1
Batch Loss = 0.0007310368090935123
931, epoch_train_loss=0.0007310368090935123
Epoch 932
Epoch 932 :: Batch 0/1
Batch Loss = 0.0007310368090935123
932, epoch_train_loss=0.0007310368090935123
Epoch 933
Epoch 933 :: Batch 0/1
Batch Loss = 0.0007310368090935123
933, epoch_train_loss=0.0007310368090935123
Epoch 934
Epoch 934 :: Batch 0/1
Batch Loss = 0.0007310368090935123
934, epoch_train_loss=0.0007310368090935123
Epoch 935
Epoch 935 :: Batch 0/1
Batch Loss = 0.0007310368090935123
935, epoch_train_loss=0.0007310368090935123
Epoch 936
Epoch 936 :: Batch 0/1
Batch Loss = 0.0007310368090935123
936, epoch_train_loss=0.0007310368090935123
Epoch 937
Epoch 937 :: Batch 0/1
Batch Loss = 0.0007310368090935123
937, epoch_train_loss=0.0007310368090935123
Epoch 938
Epoch 938 :: Batch 0/1
Batch Loss = 0.0007310368090935123
938, epoch_train_loss=0.0007310368090935123
Epoch 939
Epoch 939 :: Batch 0/1
Batch Loss = 0.0007310368090935123
939, epoch_train_loss=0.0007310368090935123
Epoch 940
Epoch 940 :: Batch 0/1
Batch Loss = 0.0007310368090935123
940, epoch_train_loss=0.0007310368090935123
Epoch 941
Epoch 941 :: Batch 0/1
Batch Loss = 0.0007310368090935123
941, epoch_train_loss=0.0007310368090935123
Epoch 942
Epoch 942 :: Batch 0/1
Batch Loss = 0.0007310368090935123
942, epoch_train_loss=0.0007310368090935123
Epoch 943
Epoch 943 :: Batch 0/1
Batch Loss = 0.0007310368090935123
943, epoch_train_loss=0.0007310368090935123
Epoch 944
Epoch 944 :: Batch 0/1
Batch Loss = 0.0007310368090935123
944, epoch_train_loss=0.0007310368090935123
Epoch 945
Epoch 945 :: Batch 0/1
Batch Loss = 0.0007310368090935123
945, epoch_train_loss=0.0007310368090935123
Epoch 946
Epoch 946 :: Batch 0/1
Batch Loss = 0.0007310368090935123
946, epoch_train_loss=0.0007310368090935123
Epoch 947
Epoch 947 :: Batch 0/1
Batch Loss = 0.0007310368090935123
947, epoch_train_loss=0.0007310368090935123
Epoch 948
Epoch 948 :: Batch 0/1
Batch Loss = 0.0007310368090935123
948, epoch_train_loss=0.0007310368090935123
Epoch 949
Epoch 949 :: Batch 0/1
Batch Loss = 0.0007310368090935123
949, epoch_train_loss=0.0007310368090935123
Epoch 950
Epoch 950 :: Batch 0/1
Batch Loss = 0.0007310368090935123
950, epoch_train_loss=0.0007310368090935123
Epoch 951
Epoch 951 :: Batch 0/1
Batch Loss = 0.0007310368090935123
951, epoch_train_loss=0.0007310368090935123
Epoch 952
Epoch 952 :: Batch 0/1
Batch Loss = 0.0007310368090935123
952, epoch_train_loss=0.0007310368090935123
Epoch 953
Epoch 953 :: Batch 0/1
Batch Loss = 0.0007310368090935123
953, epoch_train_loss=0.0007310368090935123
Epoch 954
Epoch 954 :: Batch 0/1
Batch Loss = 0.0007310368090935123
954, epoch_train_loss=0.0007310368090935123
Epoch 955
Epoch 955 :: Batch 0/1
Batch Loss = 0.0007310368090935123
955, epoch_train_loss=0.0007310368090935123
Epoch 956
Epoch 956 :: Batch 0/1
Batch Loss = 0.0007310368090935123
956, epoch_train_loss=0.0007310368090935123
Epoch 957
Epoch 957 :: Batch 0/1
Batch Loss = 0.0007310368090935123
957, epoch_train_loss=0.0007310368090935123
Epoch 958
Epoch 958 :: Batch 0/1
Batch Loss = 0.0007310368090935123
958, epoch_train_loss=0.0007310368090935123
Epoch 959
Epoch 959 :: Batch 0/1
Batch Loss = 0.0007310368090935123
959, epoch_train_loss=0.0007310368090935123
Epoch 960
Epoch 960 :: Batch 0/1
Batch Loss = 0.0007310368090935123
960, epoch_train_loss=0.0007310368090935123
Epoch 961
Epoch 961 :: Batch 0/1
Batch Loss = 0.0007310368090935123
961, epoch_train_loss=0.0007310368090935123
Epoch 962
Epoch 962 :: Batch 0/1
Batch Loss = 0.0007310368090935123
962, epoch_train_loss=0.0007310368090935123
Epoch 963
Epoch 963 :: Batch 0/1
Batch Loss = 0.0007310368090935123
963, epoch_train_loss=0.0007310368090935123
Epoch 964
Epoch 964 :: Batch 0/1
Batch Loss = 0.0007310368090935123
964, epoch_train_loss=0.0007310368090935123
Epoch 965
Epoch 965 :: Batch 0/1
Batch Loss = 0.0007310368090935123
965, epoch_train_loss=0.0007310368090935123
Epoch 966
Epoch 966 :: Batch 0/1
Batch Loss = 0.0007310368090935123
966, epoch_train_loss=0.0007310368090935123
Epoch 967
Epoch 967 :: Batch 0/1
Batch Loss = 0.0007310368090935123
967, epoch_train_loss=0.0007310368090935123
Epoch 968
Epoch 968 :: Batch 0/1
Batch Loss = 0.0007310368090935123
968, epoch_train_loss=0.0007310368090935123
Epoch 969
Epoch 969 :: Batch 0/1
Batch Loss = 0.0007310368090935123
969, epoch_train_loss=0.0007310368090935123
Epoch 970
Epoch 970 :: Batch 0/1
Batch Loss = 0.0007310368090935123
970, epoch_train_loss=0.0007310368090935123
Epoch 971
Epoch 971 :: Batch 0/1
Batch Loss = 0.0007310368090935123
971, epoch_train_loss=0.0007310368090935123
Epoch 972
Epoch 972 :: Batch 0/1
Batch Loss = 0.0007310368090935123
972, epoch_train_loss=0.0007310368090935123
Epoch 973
Epoch 973 :: Batch 0/1
Batch Loss = 0.0007310368090935123
973, epoch_train_loss=0.0007310368090935123
Epoch 974
Epoch 974 :: Batch 0/1
Batch Loss = 0.0007310368090935123
974, epoch_train_loss=0.0007310368090935123
Epoch 975
Epoch 975 :: Batch 0/1
Batch Loss = 0.0007310368090935123
975, epoch_train_loss=0.0007310368090935123
Epoch 976
Epoch 976 :: Batch 0/1
Batch Loss = 0.0007310368090935123
976, epoch_train_loss=0.0007310368090935123
Epoch 977
Epoch 977 :: Batch 0/1
Batch Loss = 0.0007310368090935123
977, epoch_train_loss=0.0007310368090935123
Epoch 978
Epoch 978 :: Batch 0/1
Batch Loss = 0.0007310368090935123
978, epoch_train_loss=0.0007310368090935123
Epoch 979
Epoch 979 :: Batch 0/1
Batch Loss = 0.0007310368090935123
979, epoch_train_loss=0.0007310368090935123
Epoch 980
Epoch 980 :: Batch 0/1
Batch Loss = 0.0007310368090935123
980, epoch_train_loss=0.0007310368090935123
Epoch 981
Epoch 981 :: Batch 0/1
Batch Loss = 0.0007310368090935123
981, epoch_train_loss=0.0007310368090935123
Epoch 982
Epoch 982 :: Batch 0/1
Batch Loss = 0.0007310368090935123
982, epoch_train_loss=0.0007310368090935123
Epoch 983
Epoch 983 :: Batch 0/1
Batch Loss = 0.0007310368090935123
983, epoch_train_loss=0.0007310368090935123
Epoch 984
Epoch 984 :: Batch 0/1
Batch Loss = 0.0007310368090935123
984, epoch_train_loss=0.0007310368090935123
Epoch 985
Epoch 985 :: Batch 0/1
Batch Loss = 0.0007310368090935123
985, epoch_train_loss=0.0007310368090935123
Epoch 986
Epoch 986 :: Batch 0/1
Batch Loss = 0.0007310368090935123
986, epoch_train_loss=0.0007310368090935123
Epoch 987
Epoch 987 :: Batch 0/1
Batch Loss = 0.0007310368090935123
987, epoch_train_loss=0.0007310368090935123
Epoch 988
Epoch 988 :: Batch 0/1
Batch Loss = 0.0007310368090935123
988, epoch_train_loss=0.0007310368090935123
Epoch 989
Epoch 989 :: Batch 0/1
Batch Loss = 0.0007310368090935123
989, epoch_train_loss=0.0007310368090935123
Epoch 990
Epoch 990 :: Batch 0/1
Batch Loss = 0.0007310368090935123
990, epoch_train_loss=0.0007310368090935123
Epoch 991
Epoch 991 :: Batch 0/1
Batch Loss = 0.0007310368090935123
991, epoch_train_loss=0.0007310368090935123
Epoch 992
Epoch 992 :: Batch 0/1
Batch Loss = 0.0007310368090935123
992, epoch_train_loss=0.0007310368090935123
Epoch 993
Epoch 993 :: Batch 0/1
Batch Loss = 0.0007310368090935123
993, epoch_train_loss=0.0007310368090935123
Epoch 994
Epoch 994 :: Batch 0/1
Batch Loss = 0.0007310368090935123
994, epoch_train_loss=0.0007310368090935123
Epoch 995
Epoch 995 :: Batch 0/1
Batch Loss = 0.0007310368090935123
995, epoch_train_loss=0.0007310368090935123
Epoch 996
Epoch 996 :: Batch 0/1
Batch Loss = 0.0007310368090935123
996, epoch_train_loss=0.0007310368090935123
Epoch 997
Epoch 997 :: Batch 0/1
Batch Loss = 0.0007310368090935123
997, epoch_train_loss=0.0007310368090935123
Epoch 998
Epoch 998 :: Batch 0/1
Batch Loss = 0.0007310368090935123
998, epoch_train_loss=0.0007310368090935123
Epoch 999
Epoch 999 :: Batch 0/1
Batch Loss = 0.0007310368090935123
999, epoch_train_loss=0.0007310368090935123
Epoch 1000
Epoch 1000 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1000, epoch_train_loss=0.0007310368090935123
Epoch 1001
Epoch 1001 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1001, epoch_train_loss=0.0007310368090935123
Epoch 1002
Epoch 1002 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1002, epoch_train_loss=0.0007310368090935123
Epoch 1003
Epoch 1003 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1003, epoch_train_loss=0.0007310368090935123
Epoch 1004
Epoch 1004 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1004, epoch_train_loss=0.0007310368090935123
Epoch 1005
Epoch 1005 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1005, epoch_train_loss=0.0007310368090935123
Epoch 1006
Epoch 1006 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1006, epoch_train_loss=0.0007310368090935123
Epoch 1007
Epoch 1007 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1007, epoch_train_loss=0.0007310368090935123
Epoch 1008
Epoch 1008 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1008, epoch_train_loss=0.0007310368090935123
Epoch 1009
Epoch 1009 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1009, epoch_train_loss=0.0007310368090935123
Epoch 1010
Epoch 1010 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1010, epoch_train_loss=0.0007310368090935123
Epoch 1011
Epoch 1011 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1011, epoch_train_loss=0.0007310368090935123
Epoch 1012
Epoch 1012 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1012, epoch_train_loss=0.0007310368090935123
Epoch 1013
Epoch 1013 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1013, epoch_train_loss=0.0007310368090935123
Epoch 1014
Epoch 1014 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1014, epoch_train_loss=0.0007310368090935123
Epoch 1015
Epoch 1015 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1015, epoch_train_loss=0.0007310368090935123
Epoch 1016
Epoch 1016 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1016, epoch_train_loss=0.0007310368090935123
Epoch 1017
Epoch 1017 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1017, epoch_train_loss=0.0007310368090935123
Epoch 1018
Epoch 1018 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1018, epoch_train_loss=0.0007310368090935123
Epoch 1019
Epoch 1019 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1019, epoch_train_loss=0.0007310368090935123
Epoch 1020
Epoch 1020 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1020, epoch_train_loss=0.0007310368090935123
Epoch 1021
Epoch 1021 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1021, epoch_train_loss=0.0007310368090935123
Epoch 1022
Epoch 1022 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1022, epoch_train_loss=0.0007310368090935123
Epoch 1023
Epoch 1023 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1023, epoch_train_loss=0.0007310368090935123
Epoch 1024
Epoch 1024 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1024, epoch_train_loss=0.0007310368090935123
Epoch 1025
Epoch 1025 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1025, epoch_train_loss=0.0007310368090935123
Epoch 1026
Epoch 1026 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1026, epoch_train_loss=0.0007310368090935123
Epoch 1027
Epoch 1027 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1027, epoch_train_loss=0.0007310368090935123
Epoch 1028
Epoch 1028 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1028, epoch_train_loss=0.0007310368090935123
Epoch 1029
Epoch 1029 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1029, epoch_train_loss=0.0007310368090935123
Epoch 1030
Epoch 1030 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1030, epoch_train_loss=0.0007310368090935123
Epoch 1031
Epoch 1031 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1031, epoch_train_loss=0.0007310368090935123
Epoch 1032
Epoch 1032 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1032, epoch_train_loss=0.0007310368090935123
Epoch 1033
Epoch 1033 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1033, epoch_train_loss=0.0007310368090935123
Epoch 1034
Epoch 1034 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1034, epoch_train_loss=0.0007310368090935123
Epoch 1035
Epoch 1035 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1035, epoch_train_loss=0.0007310368090935123
Epoch 1036
Epoch 1036 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1036, epoch_train_loss=0.0007310368090935123
Epoch 1037
Epoch 1037 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1037, epoch_train_loss=0.0007310368090935123
Epoch 1038
Epoch 1038 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1038, epoch_train_loss=0.0007310368090935123
Epoch 1039
Epoch 1039 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1039, epoch_train_loss=0.0007310368090935123
Epoch 1040
Epoch 1040 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1040, epoch_train_loss=0.0007310368090935123
Epoch 1041
Epoch 1041 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1041, epoch_train_loss=0.0007310368090935123
Epoch 1042
Epoch 1042 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1042, epoch_train_loss=0.0007310368090935123
Epoch 1043
Epoch 1043 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1043, epoch_train_loss=0.0007310368090935123
Epoch 1044
Epoch 1044 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1044, epoch_train_loss=0.0007310368090935123
Epoch 1045
Epoch 1045 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1045, epoch_train_loss=0.0007310368090935123
Epoch 1046
Epoch 1046 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1046, epoch_train_loss=0.0007310368090935123
Epoch 1047
Epoch 1047 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1047, epoch_train_loss=0.0007310368090935123
Epoch 1048
Epoch 1048 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1048, epoch_train_loss=0.0007310368090935123
Epoch 1049
Epoch 1049 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1049, epoch_train_loss=0.0007310368090935123
Epoch 1050
Epoch 1050 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1050, epoch_train_loss=0.0007310368090935123
Epoch 1051
Epoch 1051 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1051, epoch_train_loss=0.0007310368090935123
Epoch 1052
Epoch 1052 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1052, epoch_train_loss=0.0007310368090935123
Epoch 1053
Epoch 1053 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1053, epoch_train_loss=0.0007310368090935123
Epoch 1054
Epoch 1054 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1054, epoch_train_loss=0.0007310368090935123
Epoch 1055
Epoch 1055 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1055, epoch_train_loss=0.0007310368090935123
Epoch 1056
Epoch 1056 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1056, epoch_train_loss=0.0007310368090935123
Epoch 1057
Epoch 1057 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1057, epoch_train_loss=0.0007310368090935123
Epoch 1058
Epoch 1058 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1058, epoch_train_loss=0.0007310368090935123
Epoch 1059
Epoch 1059 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1059, epoch_train_loss=0.0007310368090935123
Epoch 1060
Epoch 1060 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1060, epoch_train_loss=0.0007310368090935123
Epoch 1061
Epoch 1061 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1061, epoch_train_loss=0.0007310368090935123
Epoch 1062
Epoch 1062 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1062, epoch_train_loss=0.0007310368090935123
Epoch 1063
Epoch 1063 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1063, epoch_train_loss=0.0007310368090935123
Epoch 1064
Epoch 1064 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1064, epoch_train_loss=0.0007310368090935123
Epoch 1065
Epoch 1065 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1065, epoch_train_loss=0.0007310368090935123
Epoch 1066
Epoch 1066 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1066, epoch_train_loss=0.0007310368090935123
Epoch 1067
Epoch 1067 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1067, epoch_train_loss=0.0007310368090935123
Epoch 1068
Epoch 1068 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1068, epoch_train_loss=0.0007310368090935123
Epoch 1069
Epoch 1069 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1069, epoch_train_loss=0.0007310368090935123
Epoch 1070
Epoch 1070 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1070, epoch_train_loss=0.0007310368090935123
Epoch 1071
Epoch 1071 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1071, epoch_train_loss=0.0007310368090935123
Epoch 1072
Epoch 1072 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1072, epoch_train_loss=0.0007310368090935123
Epoch 1073
Epoch 1073 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1073, epoch_train_loss=0.0007310368090935123
Epoch 1074
Epoch 1074 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1074, epoch_train_loss=0.0007310368090935123
Epoch 1075
Epoch 1075 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1075, epoch_train_loss=0.0007310368090935123
Epoch 1076
Epoch 1076 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1076, epoch_train_loss=0.0007310368090935123
Epoch 1077
Epoch 1077 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1077, epoch_train_loss=0.0007310368090935123
Epoch 1078
Epoch 1078 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1078, epoch_train_loss=0.0007310368090935123
Epoch 1079
Epoch 1079 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1079, epoch_train_loss=0.0007310368090935123
Epoch 1080
Epoch 1080 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1080, epoch_train_loss=0.0007310368090935123
Epoch 1081
Epoch 1081 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1081, epoch_train_loss=0.0007310368090935123
Epoch 1082
Epoch 1082 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1082, epoch_train_loss=0.0007310368090935123
Epoch 1083
Epoch 1083 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1083, epoch_train_loss=0.0007310368090935123
Epoch 1084
Epoch 1084 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1084, epoch_train_loss=0.0007310368090935123
Epoch 1085
Epoch 1085 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1085, epoch_train_loss=0.0007310368090935123
Epoch 1086
Epoch 1086 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1086, epoch_train_loss=0.0007310368090935123
Epoch 1087
Epoch 1087 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1087, epoch_train_loss=0.0007310368090935123
Epoch 1088
Epoch 1088 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1088, epoch_train_loss=0.0007310368090935123
Epoch 1089
Epoch 1089 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1089, epoch_train_loss=0.0007310368090935123
Epoch 1090
Epoch 1090 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1090, epoch_train_loss=0.0007310368090935123
Epoch 1091
Epoch 1091 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1091, epoch_train_loss=0.0007310368090935123
Epoch 1092
Epoch 1092 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1092, epoch_train_loss=0.0007310368090935123
Epoch 1093
Epoch 1093 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1093, epoch_train_loss=0.0007310368090935123
Epoch 1094
Epoch 1094 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1094, epoch_train_loss=0.0007310368090935123
Epoch 1095
Epoch 1095 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1095, epoch_train_loss=0.0007310368090935123
Epoch 1096
Epoch 1096 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1096, epoch_train_loss=0.0007310368090935123
Epoch 1097
Epoch 1097 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1097, epoch_train_loss=0.0007310368090935123
Epoch 1098
Epoch 1098 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1098, epoch_train_loss=0.0007310368090935123
Epoch 1099
Epoch 1099 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1099, epoch_train_loss=0.0007310368090935123
Epoch 1100
Epoch 1100 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1100, epoch_train_loss=0.0007310368090935123
Epoch 1101
Epoch 1101 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1101, epoch_train_loss=0.0007310368090935123
Epoch 1102
Epoch 1102 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1102, epoch_train_loss=0.0007310368090935123
Epoch 1103
Epoch 1103 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1103, epoch_train_loss=0.0007310368090935123
Epoch 1104
Epoch 1104 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1104, epoch_train_loss=0.0007310368090935123
Epoch 1105
Epoch 1105 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1105, epoch_train_loss=0.0007310368090935123
Epoch 1106
Epoch 1106 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1106, epoch_train_loss=0.0007310368090935123
Epoch 1107
Epoch 1107 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1107, epoch_train_loss=0.0007310368090935123
Epoch 1108
Epoch 1108 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1108, epoch_train_loss=0.0007310368090935123
Epoch 1109
Epoch 1109 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1109, epoch_train_loss=0.0007310368090935123
Epoch 1110
Epoch 1110 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1110, epoch_train_loss=0.0007310368090935123
Epoch 1111
Epoch 1111 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1111, epoch_train_loss=0.0007310368090935123
Epoch 1112
Epoch 1112 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1112, epoch_train_loss=0.0007310368090935123
Epoch 1113
Epoch 1113 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1113, epoch_train_loss=0.0007310368090935123
Epoch 1114
Epoch 1114 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1114, epoch_train_loss=0.0007310368090935123
Epoch 1115
Epoch 1115 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1115, epoch_train_loss=0.0007310368090935123
Epoch 1116
Epoch 1116 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1116, epoch_train_loss=0.0007310368090935123
Epoch 1117
Epoch 1117 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1117, epoch_train_loss=0.0007310368090935123
Epoch 1118
Epoch 1118 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1118, epoch_train_loss=0.0007310368090935123
Epoch 1119
Epoch 1119 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1119, epoch_train_loss=0.0007310368090935123
Epoch 1120
Epoch 1120 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1120, epoch_train_loss=0.0007310368090935123
Epoch 1121
Epoch 1121 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1121, epoch_train_loss=0.0007310368090935123
Epoch 1122
Epoch 1122 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1122, epoch_train_loss=0.0007310368090935123
Epoch 1123
Epoch 1123 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1123, epoch_train_loss=0.0007310368090935123
Epoch 1124
Epoch 1124 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1124, epoch_train_loss=0.0007310368090935123
Epoch 1125
Epoch 1125 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1125, epoch_train_loss=0.0007310368090935123
Epoch 1126
Epoch 1126 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1126, epoch_train_loss=0.0007310368090935123
Epoch 1127
Epoch 1127 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1127, epoch_train_loss=0.0007310368090935123
Epoch 1128
Epoch 1128 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1128, epoch_train_loss=0.0007310368090935123
Epoch 1129
Epoch 1129 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1129, epoch_train_loss=0.0007310368090935123
Epoch 1130
Epoch 1130 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1130, epoch_train_loss=0.0007310368090935123
Epoch 1131
Epoch 1131 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1131, epoch_train_loss=0.0007310368090935123
Epoch 1132
Epoch 1132 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1132, epoch_train_loss=0.0007310368090935123
Epoch 1133
Epoch 1133 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1133, epoch_train_loss=0.0007310368090935123
Epoch 1134
Epoch 1134 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1134, epoch_train_loss=0.0007310368090935123
Epoch 1135
Epoch 1135 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1135, epoch_train_loss=0.0007310368090935123
Epoch 1136
Epoch 1136 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1136, epoch_train_loss=0.0007310368090935123
Epoch 1137
Epoch 1137 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1137, epoch_train_loss=0.0007310368090935123
Epoch 1138
Epoch 1138 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1138, epoch_train_loss=0.0007310368090935123
Epoch 1139
Epoch 1139 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1139, epoch_train_loss=0.0007310368090935123
Epoch 1140
Epoch 1140 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1140, epoch_train_loss=0.0007310368090935123
Epoch 1141
Epoch 1141 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1141, epoch_train_loss=0.0007310368090935123
Epoch 1142
Epoch 1142 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1142, epoch_train_loss=0.0007310368090935123
Epoch 1143
Epoch 1143 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1143, epoch_train_loss=0.0007310368090935123
Epoch 1144
Epoch 1144 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1144, epoch_train_loss=0.0007310368090935123
Epoch 1145
Epoch 1145 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1145, epoch_train_loss=0.0007310368090935123
Epoch 1146
Epoch 1146 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1146, epoch_train_loss=0.0007310368090935123
Epoch 1147
Epoch 1147 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1147, epoch_train_loss=0.0007310368090935123
Epoch 1148
Epoch 1148 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1148, epoch_train_loss=0.0007310368090935123
Epoch 1149
Epoch 1149 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1149, epoch_train_loss=0.0007310368090935123
Epoch 1150
Epoch 1150 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1150, epoch_train_loss=0.0007310368090935123
Epoch 1151
Epoch 1151 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1151, epoch_train_loss=0.0007310368090935123
Epoch 1152
Epoch 1152 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1152, epoch_train_loss=0.0007310368090935123
Epoch 1153
Epoch 1153 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1153, epoch_train_loss=0.0007310368090935123
Epoch 1154
Epoch 1154 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1154, epoch_train_loss=0.0007310368090935123
Epoch 1155
Epoch 1155 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1155, epoch_train_loss=0.0007310368090935123
Epoch 1156
Epoch 1156 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1156, epoch_train_loss=0.0007310368090935123
Epoch 1157
Epoch 1157 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1157, epoch_train_loss=0.0007310368090935123
Epoch 1158
Epoch 1158 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1158, epoch_train_loss=0.0007310368090935123
Epoch 1159
Epoch 1159 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1159, epoch_train_loss=0.0007310368090935123
Epoch 1160
Epoch 1160 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1160, epoch_train_loss=0.0007310368090935123
Epoch 1161
Epoch 1161 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1161, epoch_train_loss=0.0007310368090935123
Epoch 1162
Epoch 1162 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1162, epoch_train_loss=0.0007310368090935123
Epoch 1163
Epoch 1163 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1163, epoch_train_loss=0.0007310368090935123
Epoch 1164
Epoch 1164 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1164, epoch_train_loss=0.0007310368090935123
Epoch 1165
Epoch 1165 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1165, epoch_train_loss=0.0007310368090935123
Epoch 1166
Epoch 1166 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1166, epoch_train_loss=0.0007310368090935123
Epoch 1167
Epoch 1167 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1167, epoch_train_loss=0.0007310368090935123
Epoch 1168
Epoch 1168 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1168, epoch_train_loss=0.0007310368090935123
Epoch 1169
Epoch 1169 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1169, epoch_train_loss=0.0007310368090935123
Epoch 1170
Epoch 1170 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1170, epoch_train_loss=0.0007310368090935123
Epoch 1171
Epoch 1171 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1171, epoch_train_loss=0.0007310368090935123
Epoch 1172
Epoch 1172 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1172, epoch_train_loss=0.0007310368090935123
Epoch 1173
Epoch 1173 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1173, epoch_train_loss=0.0007310368090935123
Epoch 1174
Epoch 1174 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1174, epoch_train_loss=0.0007310368090935123
Epoch 1175
Epoch 1175 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1175, epoch_train_loss=0.0007310368090935123
Epoch 1176
Epoch 1176 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1176, epoch_train_loss=0.0007310368090935123
Epoch 1177
Epoch 1177 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1177, epoch_train_loss=0.0007310368090935123
Epoch 1178
Epoch 1178 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1178, epoch_train_loss=0.0007310368090935123
Epoch 1179
Epoch 1179 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1179, epoch_train_loss=0.0007310368090935123
Epoch 1180
Epoch 1180 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1180, epoch_train_loss=0.0007310368090935123
Epoch 1181
Epoch 1181 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1181, epoch_train_loss=0.0007310368090935123
Epoch 1182
Epoch 1182 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1182, epoch_train_loss=0.0007310368090935123
Epoch 1183
Epoch 1183 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1183, epoch_train_loss=0.0007310368090935123
Epoch 1184
Epoch 1184 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1184, epoch_train_loss=0.0007310368090935123
Epoch 1185
Epoch 1185 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1185, epoch_train_loss=0.0007310368090935123
Epoch 1186
Epoch 1186 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1186, epoch_train_loss=0.0007310368090935123
Epoch 1187
Epoch 1187 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1187, epoch_train_loss=0.0007310368090935123
Epoch 1188
Epoch 1188 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1188, epoch_train_loss=0.0007310368090935123
Epoch 1189
Epoch 1189 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1189, epoch_train_loss=0.0007310368090935123
Epoch 1190
Epoch 1190 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1190, epoch_train_loss=0.0007310368090935123
Epoch 1191
Epoch 1191 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1191, epoch_train_loss=0.0007310368090935123
Epoch 1192
Epoch 1192 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1192, epoch_train_loss=0.0007310368090935123
Epoch 1193
Epoch 1193 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1193, epoch_train_loss=0.0007310368090935123
Epoch 1194
Epoch 1194 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1194, epoch_train_loss=0.0007310368090935123
Epoch 1195
Epoch 1195 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1195, epoch_train_loss=0.0007310368090935123
Epoch 1196
Epoch 1196 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1196, epoch_train_loss=0.0007310368090935123
Epoch 1197
Epoch 1197 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1197, epoch_train_loss=0.0007310368090935123
Epoch 1198
Epoch 1198 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1198, epoch_train_loss=0.0007310368090935123
Epoch 1199
Epoch 1199 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1199, epoch_train_loss=0.0007310368090935123
Epoch 1200
Epoch 1200 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1200, epoch_train_loss=0.0007310368090935123
Epoch 1201
Epoch 1201 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1201, epoch_train_loss=0.0007310368090935123
Epoch 1202
Epoch 1202 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1202, epoch_train_loss=0.0007310368090935123
Epoch 1203
Epoch 1203 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1203, epoch_train_loss=0.0007310368090935123
Epoch 1204
Epoch 1204 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1204, epoch_train_loss=0.0007310368090935123
Epoch 1205
Epoch 1205 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1205, epoch_train_loss=0.0007310368090935123
Epoch 1206
Epoch 1206 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1206, epoch_train_loss=0.0007310368090935123
Epoch 1207
Epoch 1207 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1207, epoch_train_loss=0.0007310368090935123
Epoch 1208
Epoch 1208 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1208, epoch_train_loss=0.0007310368090935123
Epoch 1209
Epoch 1209 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1209, epoch_train_loss=0.0007310368090935123
Epoch 1210
Epoch 1210 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1210, epoch_train_loss=0.0007310368090935123
Epoch 1211
Epoch 1211 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1211, epoch_train_loss=0.0007310368090935123
Epoch 1212
Epoch 1212 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1212, epoch_train_loss=0.0007310368090935123
Epoch 1213
Epoch 1213 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1213, epoch_train_loss=0.0007310368090935123
Epoch 1214
Epoch 1214 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1214, epoch_train_loss=0.0007310368090935123
Epoch 1215
Epoch 1215 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1215, epoch_train_loss=0.0007310368090935123
Epoch 1216
Epoch 1216 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1216, epoch_train_loss=0.0007310368090935123
Epoch 1217
Epoch 1217 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1217, epoch_train_loss=0.0007310368090935123
Epoch 1218
Epoch 1218 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1218, epoch_train_loss=0.0007310368090935123
Epoch 1219
Epoch 1219 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1219, epoch_train_loss=0.0007310368090935123
Epoch 1220
Epoch 1220 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1220, epoch_train_loss=0.0007310368090935123
Epoch 1221
Epoch 1221 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1221, epoch_train_loss=0.0007310368090935123
Epoch 1222
Epoch 1222 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1222, epoch_train_loss=0.0007310368090935123
Epoch 1223
Epoch 1223 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1223, epoch_train_loss=0.0007310368090935123
Epoch 1224
Epoch 1224 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1224, epoch_train_loss=0.0007310368090935123
Epoch 1225
Epoch 1225 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1225, epoch_train_loss=0.0007310368090935123
Epoch 1226
Epoch 1226 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1226, epoch_train_loss=0.0007310368090935123
Epoch 1227
Epoch 1227 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1227, epoch_train_loss=0.0007310368090935123
Epoch 1228
Epoch 1228 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1228, epoch_train_loss=0.0007310368090935123
Epoch 1229
Epoch 1229 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1229, epoch_train_loss=0.0007310368090935123
Epoch 1230
Epoch 1230 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1230, epoch_train_loss=0.0007310368090935123
Epoch 1231
Epoch 1231 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1231, epoch_train_loss=0.0007310368090935123
Epoch 1232
Epoch 1232 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1232, epoch_train_loss=0.0007310368090935123
Epoch 1233
Epoch 1233 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1233, epoch_train_loss=0.0007310368090935123
Epoch 1234
Epoch 1234 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1234, epoch_train_loss=0.0007310368090935123
Epoch 1235
Epoch 1235 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1235, epoch_train_loss=0.0007310368090935123
Epoch 1236
Epoch 1236 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1236, epoch_train_loss=0.0007310368090935123
Epoch 1237
Epoch 1237 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1237, epoch_train_loss=0.0007310368090935123
Epoch 1238
Epoch 1238 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1238, epoch_train_loss=0.0007310368090935123
Epoch 1239
Epoch 1239 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1239, epoch_train_loss=0.0007310368090935123
Epoch 1240
Epoch 1240 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1240, epoch_train_loss=0.0007310368090935123
Epoch 1241
Epoch 1241 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1241, epoch_train_loss=0.0007310368090935123
Epoch 1242
Epoch 1242 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1242, epoch_train_loss=0.0007310368090935123
Epoch 1243
Epoch 1243 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1243, epoch_train_loss=0.0007310368090935123
Epoch 1244
Epoch 1244 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1244, epoch_train_loss=0.0007310368090935123
Epoch 1245
Epoch 1245 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1245, epoch_train_loss=0.0007310368090935123
Epoch 1246
Epoch 1246 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1246, epoch_train_loss=0.0007310368090935123
Epoch 1247
Epoch 1247 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1247, epoch_train_loss=0.0007310368090935123
Epoch 1248
Epoch 1248 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1248, epoch_train_loss=0.0007310368090935123
Epoch 1249
Epoch 1249 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1249, epoch_train_loss=0.0007310368090935123
Epoch 1250
Epoch 1250 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1250, epoch_train_loss=0.0007310368090935123
Epoch 1251
Epoch 1251 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1251, epoch_train_loss=0.0007310368090935123
Epoch 1252
Epoch 1252 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1252, epoch_train_loss=0.0007310368090935123
Epoch 1253
Epoch 1253 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1253, epoch_train_loss=0.0007310368090935123
Epoch 1254
Epoch 1254 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1254, epoch_train_loss=0.0007310368090935123
Epoch 1255
Epoch 1255 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1255, epoch_train_loss=0.0007310368090935123
Epoch 1256
Epoch 1256 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1256, epoch_train_loss=0.0007310368090935123
Epoch 1257
Epoch 1257 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1257, epoch_train_loss=0.0007310368090935123
Epoch 1258
Epoch 1258 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1258, epoch_train_loss=0.0007310368090935123
Epoch 1259
Epoch 1259 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1259, epoch_train_loss=0.0007310368090935123
Epoch 1260
Epoch 1260 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1260, epoch_train_loss=0.0007310368090935123
Epoch 1261
Epoch 1261 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1261, epoch_train_loss=0.0007310368090935123
Epoch 1262
Epoch 1262 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1262, epoch_train_loss=0.0007310368090935123
Epoch 1263
Epoch 1263 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1263, epoch_train_loss=0.0007310368090935123
Epoch 1264
Epoch 1264 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1264, epoch_train_loss=0.0007310368090935123
Epoch 1265
Epoch 1265 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1265, epoch_train_loss=0.0007310368090935123
Epoch 1266
Epoch 1266 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1266, epoch_train_loss=0.0007310368090935123
Epoch 1267
Epoch 1267 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1267, epoch_train_loss=0.0007310368090935123
Epoch 1268
Epoch 1268 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1268, epoch_train_loss=0.0007310368090935123
Epoch 1269
Epoch 1269 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1269, epoch_train_loss=0.0007310368090935123
Epoch 1270
Epoch 1270 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1270, epoch_train_loss=0.0007310368090935123
Epoch 1271
Epoch 1271 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1271, epoch_train_loss=0.0007310368090935123
Epoch 1272
Epoch 1272 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1272, epoch_train_loss=0.0007310368090935123
Epoch 1273
Epoch 1273 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1273, epoch_train_loss=0.0007310368090935123
Epoch 1274
Epoch 1274 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1274, epoch_train_loss=0.0007310368090935123
Epoch 1275
Epoch 1275 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1275, epoch_train_loss=0.0007310368090935123
Epoch 1276
Epoch 1276 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1276, epoch_train_loss=0.0007310368090935123
Epoch 1277
Epoch 1277 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1277, epoch_train_loss=0.0007310368090935123
Epoch 1278
Epoch 1278 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1278, epoch_train_loss=0.0007310368090935123
Epoch 1279
Epoch 1279 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1279, epoch_train_loss=0.0007310368090935123
Epoch 1280
Epoch 1280 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1280, epoch_train_loss=0.0007310368090935123
Epoch 1281
Epoch 1281 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1281, epoch_train_loss=0.0007310368090935123
Epoch 1282
Epoch 1282 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1282, epoch_train_loss=0.0007310368090935123
Epoch 1283
Epoch 1283 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1283, epoch_train_loss=0.0007310368090935123
Epoch 1284
Epoch 1284 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1284, epoch_train_loss=0.0007310368090935123
Epoch 1285
Epoch 1285 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1285, epoch_train_loss=0.0007310368090935123
Epoch 1286
Epoch 1286 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1286, epoch_train_loss=0.0007310368090935123
Epoch 1287
Epoch 1287 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1287, epoch_train_loss=0.0007310368090935123
Epoch 1288
Epoch 1288 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1288, epoch_train_loss=0.0007310368090935123
Epoch 1289
Epoch 1289 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1289, epoch_train_loss=0.0007310368090935123
Epoch 1290
Epoch 1290 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1290, epoch_train_loss=0.0007310368090935123
Epoch 1291
Epoch 1291 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1291, epoch_train_loss=0.0007310368090935123
Epoch 1292
Epoch 1292 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1292, epoch_train_loss=0.0007310368090935123
Epoch 1293
Epoch 1293 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1293, epoch_train_loss=0.0007310368090935123
Epoch 1294
Epoch 1294 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1294, epoch_train_loss=0.0007310368090935123
Epoch 1295
Epoch 1295 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1295, epoch_train_loss=0.0007310368090935123
Epoch 1296
Epoch 1296 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1296, epoch_train_loss=0.0007310368090935123
Epoch 1297
Epoch 1297 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1297, epoch_train_loss=0.0007310368090935123
Epoch 1298
Epoch 1298 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1298, epoch_train_loss=0.0007310368090935123
Epoch 1299
Epoch 1299 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1299, epoch_train_loss=0.0007310368090935123
Epoch 1300
Epoch 1300 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1300, epoch_train_loss=0.0007310368090935123
Epoch 1301
Epoch 1301 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1301, epoch_train_loss=0.0007310368090935123
Epoch 1302
Epoch 1302 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1302, epoch_train_loss=0.0007310368090935123
Epoch 1303
Epoch 1303 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1303, epoch_train_loss=0.0007310368090935123
Epoch 1304
Epoch 1304 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1304, epoch_train_loss=0.0007310368090935123
Epoch 1305
Epoch 1305 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1305, epoch_train_loss=0.0007310368090935123
Epoch 1306
Epoch 1306 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1306, epoch_train_loss=0.0007310368090935123
Epoch 1307
Epoch 1307 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1307, epoch_train_loss=0.0007310368090935123
Epoch 1308
Epoch 1308 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1308, epoch_train_loss=0.0007310368090935123
Epoch 1309
Epoch 1309 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1309, epoch_train_loss=0.0007310368090935123
Epoch 1310
Epoch 1310 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1310, epoch_train_loss=0.0007310368090935123
Epoch 1311
Epoch 1311 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1311, epoch_train_loss=0.0007310368090935123
Epoch 1312
Epoch 1312 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1312, epoch_train_loss=0.0007310368090935123
Epoch 1313
Epoch 1313 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1313, epoch_train_loss=0.0007310368090935123
Epoch 1314
Epoch 1314 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1314, epoch_train_loss=0.0007310368090935123
Epoch 1315
Epoch 1315 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1315, epoch_train_loss=0.0007310368090935123
Epoch 1316
Epoch 1316 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1316, epoch_train_loss=0.0007310368090935123
Epoch 1317
Epoch 1317 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1317, epoch_train_loss=0.0007310368090935123
Epoch 1318
Epoch 1318 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1318, epoch_train_loss=0.0007310368090935123
Epoch 1319
Epoch 1319 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1319, epoch_train_loss=0.0007310368090935123
Epoch 1320
Epoch 1320 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1320, epoch_train_loss=0.0007310368090935123
Epoch 1321
Epoch 1321 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1321, epoch_train_loss=0.0007310368090935123
Epoch 1322
Epoch 1322 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1322, epoch_train_loss=0.0007310368090935123
Epoch 1323
Epoch 1323 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1323, epoch_train_loss=0.0007310368090935123
Epoch 1324
Epoch 1324 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1324, epoch_train_loss=0.0007310368090935123
Epoch 1325
Epoch 1325 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1325, epoch_train_loss=0.0007310368090935123
Epoch 1326
Epoch 1326 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1326, epoch_train_loss=0.0007310368090935123
Epoch 1327
Epoch 1327 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1327, epoch_train_loss=0.0007310368090935123
Epoch 1328
Epoch 1328 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1328, epoch_train_loss=0.0007310368090935123
Epoch 1329
Epoch 1329 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1329, epoch_train_loss=0.0007310368090935123
Epoch 1330
Epoch 1330 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1330, epoch_train_loss=0.0007310368090935123
Epoch 1331
Epoch 1331 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1331, epoch_train_loss=0.0007310368090935123
Epoch 1332
Epoch 1332 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1332, epoch_train_loss=0.0007310368090935123
Epoch 1333
Epoch 1333 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1333, epoch_train_loss=0.0007310368090935123
Epoch 1334
Epoch 1334 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1334, epoch_train_loss=0.0007310368090935123
Epoch 1335
Epoch 1335 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1335, epoch_train_loss=0.0007310368090935123
Epoch 1336
Epoch 1336 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1336, epoch_train_loss=0.0007310368090935123
Epoch 1337
Epoch 1337 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1337, epoch_train_loss=0.0007310368090935123
Epoch 1338
Epoch 1338 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1338, epoch_train_loss=0.0007310368090935123
Epoch 1339
Epoch 1339 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1339, epoch_train_loss=0.0007310368090935123
Epoch 1340
Epoch 1340 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1340, epoch_train_loss=0.0007310368090935123
Epoch 1341
Epoch 1341 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1341, epoch_train_loss=0.0007310368090935123
Epoch 1342
Epoch 1342 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1342, epoch_train_loss=0.0007310368090935123
Epoch 1343
Epoch 1343 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1343, epoch_train_loss=0.0007310368090935123
Epoch 1344
Epoch 1344 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1344, epoch_train_loss=0.0007310368090935123
Epoch 1345
Epoch 1345 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1345, epoch_train_loss=0.0007310368090935123
Epoch 1346
Epoch 1346 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1346, epoch_train_loss=0.0007310368090935123
Epoch 1347
Epoch 1347 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1347, epoch_train_loss=0.0007310368090935123
Epoch 1348
Epoch 1348 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1348, epoch_train_loss=0.0007310368090935123
Epoch 1349
Epoch 1349 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1349, epoch_train_loss=0.0007310368090935123
Epoch 1350
Epoch 1350 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1350, epoch_train_loss=0.0007310368090935123
Epoch 1351
Epoch 1351 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1351, epoch_train_loss=0.0007310368090935123
Epoch 1352
Epoch 1352 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1352, epoch_train_loss=0.0007310368090935123
Epoch 1353
Epoch 1353 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1353, epoch_train_loss=0.0007310368090935123
Epoch 1354
Epoch 1354 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1354, epoch_train_loss=0.0007310368090935123
Epoch 1355
Epoch 1355 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1355, epoch_train_loss=0.0007310368090935123
Epoch 1356
Epoch 1356 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1356, epoch_train_loss=0.0007310368090935123
Epoch 1357
Epoch 1357 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1357, epoch_train_loss=0.0007310368090935123
Epoch 1358
Epoch 1358 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1358, epoch_train_loss=0.0007310368090935123
Epoch 1359
Epoch 1359 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1359, epoch_train_loss=0.0007310368090935123
Epoch 1360
Epoch 1360 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1360, epoch_train_loss=0.0007310368090935123
Epoch 1361
Epoch 1361 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1361, epoch_train_loss=0.0007310368090935123
Epoch 1362
Epoch 1362 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1362, epoch_train_loss=0.0007310368090935123
Epoch 1363
Epoch 1363 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1363, epoch_train_loss=0.0007310368090935123
Epoch 1364
Epoch 1364 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1364, epoch_train_loss=0.0007310368090935123
Epoch 1365
Epoch 1365 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1365, epoch_train_loss=0.0007310368090935123
Epoch 1366
Epoch 1366 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1366, epoch_train_loss=0.0007310368090935123
Epoch 1367
Epoch 1367 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1367, epoch_train_loss=0.0007310368090935123
Epoch 1368
Epoch 1368 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1368, epoch_train_loss=0.0007310368090935123
Epoch 1369
Epoch 1369 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1369, epoch_train_loss=0.0007310368090935123
Epoch 1370
Epoch 1370 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1370, epoch_train_loss=0.0007310368090935123
Epoch 1371
Epoch 1371 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1371, epoch_train_loss=0.0007310368090935123
Epoch 1372
Epoch 1372 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1372, epoch_train_loss=0.0007310368090935123
Epoch 1373
Epoch 1373 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1373, epoch_train_loss=0.0007310368090935123
Epoch 1374
Epoch 1374 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1374, epoch_train_loss=0.0007310368090935123
Epoch 1375
Epoch 1375 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1375, epoch_train_loss=0.0007310368090935123
Epoch 1376
Epoch 1376 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1376, epoch_train_loss=0.0007310368090935123
Epoch 1377
Epoch 1377 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1377, epoch_train_loss=0.0007310368090935123
Epoch 1378
Epoch 1378 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1378, epoch_train_loss=0.0007310368090935123
Epoch 1379
Epoch 1379 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1379, epoch_train_loss=0.0007310368090935123
Epoch 1380
Epoch 1380 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1380, epoch_train_loss=0.0007310368090935123
Epoch 1381
Epoch 1381 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1381, epoch_train_loss=0.0007310368090935123
Epoch 1382
Epoch 1382 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1382, epoch_train_loss=0.0007310368090935123
Epoch 1383
Epoch 1383 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1383, epoch_train_loss=0.0007310368090935123
Epoch 1384
Epoch 1384 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1384, epoch_train_loss=0.0007310368090935123
Epoch 1385
Epoch 1385 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1385, epoch_train_loss=0.0007310368090935123
Epoch 1386
Epoch 1386 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1386, epoch_train_loss=0.0007310368090935123
Epoch 1387
Epoch 1387 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1387, epoch_train_loss=0.0007310368090935123
Epoch 1388
Epoch 1388 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1388, epoch_train_loss=0.0007310368090935123
Epoch 1389
Epoch 1389 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1389, epoch_train_loss=0.0007310368090935123
Epoch 1390
Epoch 1390 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1390, epoch_train_loss=0.0007310368090935123
Epoch 1391
Epoch 1391 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1391, epoch_train_loss=0.0007310368090935123
Epoch 1392
Epoch 1392 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1392, epoch_train_loss=0.0007310368090935123
Epoch 1393
Epoch 1393 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1393, epoch_train_loss=0.0007310368090935123
Epoch 1394
Epoch 1394 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1394, epoch_train_loss=0.0007310368090935123
Epoch 1395
Epoch 1395 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1395, epoch_train_loss=0.0007310368090935123
Epoch 1396
Epoch 1396 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1396, epoch_train_loss=0.0007310368090935123
Epoch 1397
Epoch 1397 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1397, epoch_train_loss=0.0007310368090935123
Epoch 1398
Epoch 1398 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1398, epoch_train_loss=0.0007310368090935123
Epoch 1399
Epoch 1399 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1399, epoch_train_loss=0.0007310368090935123
Epoch 1400
Epoch 1400 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1400, epoch_train_loss=0.0007310368090935123
Epoch 1401
Epoch 1401 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1401, epoch_train_loss=0.0007310368090935123
Epoch 1402
Epoch 1402 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1402, epoch_train_loss=0.0007310368090935123
Epoch 1403
Epoch 1403 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1403, epoch_train_loss=0.0007310368090935123
Epoch 1404
Epoch 1404 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1404, epoch_train_loss=0.0007310368090935123
Epoch 1405
Epoch 1405 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1405, epoch_train_loss=0.0007310368090935123
Epoch 1406
Epoch 1406 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1406, epoch_train_loss=0.0007310368090935123
Epoch 1407
Epoch 1407 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1407, epoch_train_loss=0.0007310368090935123
Epoch 1408
Epoch 1408 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1408, epoch_train_loss=0.0007310368090935123
Epoch 1409
Epoch 1409 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1409, epoch_train_loss=0.0007310368090935123
Epoch 1410
Epoch 1410 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1410, epoch_train_loss=0.0007310368090935123
Epoch 1411
Epoch 1411 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1411, epoch_train_loss=0.0007310368090935123
Epoch 1412
Epoch 1412 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1412, epoch_train_loss=0.0007310368090935123
Epoch 1413
Epoch 1413 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1413, epoch_train_loss=0.0007310368090935123
Epoch 1414
Epoch 1414 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1414, epoch_train_loss=0.0007310368090935123
Epoch 1415
Epoch 1415 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1415, epoch_train_loss=0.0007310368090935123
Epoch 1416
Epoch 1416 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1416, epoch_train_loss=0.0007310368090935123
Epoch 1417
Epoch 1417 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1417, epoch_train_loss=0.0007310368090935123
Epoch 1418
Epoch 1418 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1418, epoch_train_loss=0.0007310368090935123
Epoch 1419
Epoch 1419 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1419, epoch_train_loss=0.0007310368090935123
Epoch 1420
Epoch 1420 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1420, epoch_train_loss=0.0007310368090935123
Epoch 1421
Epoch 1421 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1421, epoch_train_loss=0.0007310368090935123
Epoch 1422
Epoch 1422 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1422, epoch_train_loss=0.0007310368090935123
Epoch 1423
Epoch 1423 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1423, epoch_train_loss=0.0007310368090935123
Epoch 1424
Epoch 1424 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1424, epoch_train_loss=0.0007310368090935123
Epoch 1425
Epoch 1425 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1425, epoch_train_loss=0.0007310368090935123
Epoch 1426
Epoch 1426 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1426, epoch_train_loss=0.0007310368090935123
Epoch 1427
Epoch 1427 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1427, epoch_train_loss=0.0007310368090935123
Epoch 1428
Epoch 1428 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1428, epoch_train_loss=0.0007310368090935123
Epoch 1429
Epoch 1429 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1429, epoch_train_loss=0.0007310368090935123
Epoch 1430
Epoch 1430 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1430, epoch_train_loss=0.0007310368090935123
Epoch 1431
Epoch 1431 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1431, epoch_train_loss=0.0007310368090935123
Epoch 1432
Epoch 1432 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1432, epoch_train_loss=0.0007310368090935123
Epoch 1433
Epoch 1433 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1433, epoch_train_loss=0.0007310368090935123
Epoch 1434
Epoch 1434 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1434, epoch_train_loss=0.0007310368090935123
Epoch 1435
Epoch 1435 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1435, epoch_train_loss=0.0007310368090935123
Epoch 1436
Epoch 1436 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1436, epoch_train_loss=0.0007310368090935123
Epoch 1437
Epoch 1437 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1437, epoch_train_loss=0.0007310368090935123
Epoch 1438
Epoch 1438 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1438, epoch_train_loss=0.0007310368090935123
Epoch 1439
Epoch 1439 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1439, epoch_train_loss=0.0007310368090935123
Epoch 1440
Epoch 1440 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1440, epoch_train_loss=0.0007310368090935123
Epoch 1441
Epoch 1441 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1441, epoch_train_loss=0.0007310368090935123
Epoch 1442
Epoch 1442 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1442, epoch_train_loss=0.0007310368090935123
Epoch 1443
Epoch 1443 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1443, epoch_train_loss=0.0007310368090935123
Epoch 1444
Epoch 1444 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1444, epoch_train_loss=0.0007310368090935123
Epoch 1445
Epoch 1445 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1445, epoch_train_loss=0.0007310368090935123
Epoch 1446
Epoch 1446 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1446, epoch_train_loss=0.0007310368090935123
Epoch 1447
Epoch 1447 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1447, epoch_train_loss=0.0007310368090935123
Epoch 1448
Epoch 1448 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1448, epoch_train_loss=0.0007310368090935123
Epoch 1449
Epoch 1449 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1449, epoch_train_loss=0.0007310368090935123
Epoch 1450
Epoch 1450 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1450, epoch_train_loss=0.0007310368090935123
Epoch 1451
Epoch 1451 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1451, epoch_train_loss=0.0007310368090935123
Epoch 1452
Epoch 1452 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1452, epoch_train_loss=0.0007310368090935123
Epoch 1453
Epoch 1453 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1453, epoch_train_loss=0.0007310368090935123
Epoch 1454
Epoch 1454 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1454, epoch_train_loss=0.0007310368090935123
Epoch 1455
Epoch 1455 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1455, epoch_train_loss=0.0007310368090935123
Epoch 1456
Epoch 1456 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1456, epoch_train_loss=0.0007310368090935123
Epoch 1457
Epoch 1457 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1457, epoch_train_loss=0.0007310368090935123
Epoch 1458
Epoch 1458 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1458, epoch_train_loss=0.0007310368090935123
Epoch 1459
Epoch 1459 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1459, epoch_train_loss=0.0007310368090935123
Epoch 1460
Epoch 1460 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1460, epoch_train_loss=0.0007310368090935123
Epoch 1461
Epoch 1461 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1461, epoch_train_loss=0.0007310368090935123
Epoch 1462
Epoch 1462 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1462, epoch_train_loss=0.0007310368090935123
Epoch 1463
Epoch 1463 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1463, epoch_train_loss=0.0007310368090935123
Epoch 1464
Epoch 1464 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1464, epoch_train_loss=0.0007310368090935123
Epoch 1465
Epoch 1465 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1465, epoch_train_loss=0.0007310368090935123
Epoch 1466
Epoch 1466 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1466, epoch_train_loss=0.0007310368090935123
Epoch 1467
Epoch 1467 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1467, epoch_train_loss=0.0007310368090935123
Epoch 1468
Epoch 1468 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1468, epoch_train_loss=0.0007310368090935123
Epoch 1469
Epoch 1469 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1469, epoch_train_loss=0.0007310368090935123
Epoch 1470
Epoch 1470 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1470, epoch_train_loss=0.0007310368090935123
Epoch 1471
Epoch 1471 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1471, epoch_train_loss=0.0007310368090935123
Epoch 1472
Epoch 1472 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1472, epoch_train_loss=0.0007310368090935123
Epoch 1473
Epoch 1473 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1473, epoch_train_loss=0.0007310368090935123
Epoch 1474
Epoch 1474 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1474, epoch_train_loss=0.0007310368090935123
Epoch 1475
Epoch 1475 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1475, epoch_train_loss=0.0007310368090935123
Epoch 1476
Epoch 1476 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1476, epoch_train_loss=0.0007310368090935123
Epoch 1477
Epoch 1477 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1477, epoch_train_loss=0.0007310368090935123
Epoch 1478
Epoch 1478 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1478, epoch_train_loss=0.0007310368090935123
Epoch 1479
Epoch 1479 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1479, epoch_train_loss=0.0007310368090935123
Epoch 1480
Epoch 1480 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1480, epoch_train_loss=0.0007310368090935123
Epoch 1481
Epoch 1481 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1481, epoch_train_loss=0.0007310368090935123
Epoch 1482
Epoch 1482 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1482, epoch_train_loss=0.0007310368090935123
Epoch 1483
Epoch 1483 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1483, epoch_train_loss=0.0007310368090935123
Epoch 1484
Epoch 1484 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1484, epoch_train_loss=0.0007310368090935123
Epoch 1485
Epoch 1485 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1485, epoch_train_loss=0.0007310368090935123
Epoch 1486
Epoch 1486 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1486, epoch_train_loss=0.0007310368090935123
Epoch 1487
Epoch 1487 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1487, epoch_train_loss=0.0007310368090935123
Epoch 1488
Epoch 1488 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1488, epoch_train_loss=0.0007310368090935123
Epoch 1489
Epoch 1489 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1489, epoch_train_loss=0.0007310368090935123
Epoch 1490
Epoch 1490 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1490, epoch_train_loss=0.0007310368090935123
Epoch 1491
Epoch 1491 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1491, epoch_train_loss=0.0007310368090935123
Epoch 1492
Epoch 1492 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1492, epoch_train_loss=0.0007310368090935123
Epoch 1493
Epoch 1493 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1493, epoch_train_loss=0.0007310368090935123
Epoch 1494
Epoch 1494 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1494, epoch_train_loss=0.0007310368090935123
Epoch 1495
Epoch 1495 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1495, epoch_train_loss=0.0007310368090935123
Epoch 1496
Epoch 1496 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1496, epoch_train_loss=0.0007310368090935123
Epoch 1497
Epoch 1497 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1497, epoch_train_loss=0.0007310368090935123
Epoch 1498
Epoch 1498 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1498, epoch_train_loss=0.0007310368090935123
Epoch 1499
Epoch 1499 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1499, epoch_train_loss=0.0007310368090935123
Epoch 1500
Epoch 1500 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1500, epoch_train_loss=0.0007310368090935123
Epoch 1501
Epoch 1501 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1501, epoch_train_loss=0.0007310368090935123
Epoch 1502
Epoch 1502 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1502, epoch_train_loss=0.0007310368090935123
Epoch 1503
Epoch 1503 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1503, epoch_train_loss=0.0007310368090935123
Epoch 1504
Epoch 1504 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1504, epoch_train_loss=0.0007310368090935123
Epoch 1505
Epoch 1505 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1505, epoch_train_loss=0.0007310368090935123
Epoch 1506
Epoch 1506 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1506, epoch_train_loss=0.0007310368090935123
Epoch 1507
Epoch 1507 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1507, epoch_train_loss=0.0007310368090935123
Epoch 1508
Epoch 1508 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1508, epoch_train_loss=0.0007310368090935123
Epoch 1509
Epoch 1509 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1509, epoch_train_loss=0.0007310368090935123
Epoch 1510
Epoch 1510 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1510, epoch_train_loss=0.0007310368090935123
Epoch 1511
Epoch 1511 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1511, epoch_train_loss=0.0007310368090935123
Epoch 1512
Epoch 1512 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1512, epoch_train_loss=0.0007310368090935123
Epoch 1513
Epoch 1513 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1513, epoch_train_loss=0.0007310368090935123
Epoch 1514
Epoch 1514 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1514, epoch_train_loss=0.0007310368090935123
Epoch 1515
Epoch 1515 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1515, epoch_train_loss=0.0007310368090935123
Epoch 1516
Epoch 1516 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1516, epoch_train_loss=0.0007310368090935123
Epoch 1517
Epoch 1517 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1517, epoch_train_loss=0.0007310368090935123
Epoch 1518
Epoch 1518 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1518, epoch_train_loss=0.0007310368090935123
Epoch 1519
Epoch 1519 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1519, epoch_train_loss=0.0007310368090935123
Epoch 1520
Epoch 1520 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1520, epoch_train_loss=0.0007310368090935123
Epoch 1521
Epoch 1521 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1521, epoch_train_loss=0.0007310368090935123
Epoch 1522
Epoch 1522 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1522, epoch_train_loss=0.0007310368090935123
Epoch 1523
Epoch 1523 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1523, epoch_train_loss=0.0007310368090935123
Epoch 1524
Epoch 1524 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1524, epoch_train_loss=0.0007310368090935123
Epoch 1525
Epoch 1525 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1525, epoch_train_loss=0.0007310368090935123
Epoch 1526
Epoch 1526 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1526, epoch_train_loss=0.0007310368090935123
Epoch 1527
Epoch 1527 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1527, epoch_train_loss=0.0007310368090935123
Epoch 1528
Epoch 1528 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1528, epoch_train_loss=0.0007310368090935123
Epoch 1529
Epoch 1529 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1529, epoch_train_loss=0.0007310368090935123
Epoch 1530
Epoch 1530 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1530, epoch_train_loss=0.0007310368090935123
Epoch 1531
Epoch 1531 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1531, epoch_train_loss=0.0007310368090935123
Epoch 1532
Epoch 1532 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1532, epoch_train_loss=0.0007310368090935123
Epoch 1533
Epoch 1533 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1533, epoch_train_loss=0.0007310368090935123
Epoch 1534
Epoch 1534 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1534, epoch_train_loss=0.0007310368090935123
Epoch 1535
Epoch 1535 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1535, epoch_train_loss=0.0007310368090935123
Epoch 1536
Epoch 1536 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1536, epoch_train_loss=0.0007310368090935123
Epoch 1537
Epoch 1537 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1537, epoch_train_loss=0.0007310368090935123
Epoch 1538
Epoch 1538 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1538, epoch_train_loss=0.0007310368090935123
Epoch 1539
Epoch 1539 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1539, epoch_train_loss=0.0007310368090935123
Epoch 1540
Epoch 1540 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1540, epoch_train_loss=0.0007310368090935123
Epoch 1541
Epoch 1541 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1541, epoch_train_loss=0.0007310368090935123
Epoch 1542
Epoch 1542 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1542, epoch_train_loss=0.0007310368090935123
Epoch 1543
Epoch 1543 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1543, epoch_train_loss=0.0007310368090935123
Epoch 1544
Epoch 1544 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1544, epoch_train_loss=0.0007310368090935123
Epoch 1545
Epoch 1545 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1545, epoch_train_loss=0.0007310368090935123
Epoch 1546
Epoch 1546 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1546, epoch_train_loss=0.0007310368090935123
Epoch 1547
Epoch 1547 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1547, epoch_train_loss=0.0007310368090935123
Epoch 1548
Epoch 1548 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1548, epoch_train_loss=0.0007310368090935123
Epoch 1549
Epoch 1549 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1549, epoch_train_loss=0.0007310368090935123
Epoch 1550
Epoch 1550 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1550, epoch_train_loss=0.0007310368090935123
Epoch 1551
Epoch 1551 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1551, epoch_train_loss=0.0007310368090935123
Epoch 1552
Epoch 1552 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1552, epoch_train_loss=0.0007310368090935123
Epoch 1553
Epoch 1553 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1553, epoch_train_loss=0.0007310368090935123
Epoch 1554
Epoch 1554 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1554, epoch_train_loss=0.0007310368090935123
Epoch 1555
Epoch 1555 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1555, epoch_train_loss=0.0007310368090935123
Epoch 1556
Epoch 1556 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1556, epoch_train_loss=0.0007310368090935123
Epoch 1557
Epoch 1557 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1557, epoch_train_loss=0.0007310368090935123
Epoch 1558
Epoch 1558 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1558, epoch_train_loss=0.0007310368090935123
Epoch 1559
Epoch 1559 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1559, epoch_train_loss=0.0007310368090935123
Epoch 1560
Epoch 1560 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1560, epoch_train_loss=0.0007310368090935123
Epoch 1561
Epoch 1561 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1561, epoch_train_loss=0.0007310368090935123
Epoch 1562
Epoch 1562 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1562, epoch_train_loss=0.0007310368090935123
Epoch 1563
Epoch 1563 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1563, epoch_train_loss=0.0007310368090935123
Epoch 1564
Epoch 1564 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1564, epoch_train_loss=0.0007310368090935123
Epoch 1565
Epoch 1565 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1565, epoch_train_loss=0.0007310368090935123
Epoch 1566
Epoch 1566 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1566, epoch_train_loss=0.0007310368090935123
Epoch 1567
Epoch 1567 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1567, epoch_train_loss=0.0007310368090935123
Epoch 1568
Epoch 1568 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1568, epoch_train_loss=0.0007310368090935123
Epoch 1569
Epoch 1569 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1569, epoch_train_loss=0.0007310368090935123
Epoch 1570
Epoch 1570 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1570, epoch_train_loss=0.0007310368090935123
Epoch 1571
Epoch 1571 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1571, epoch_train_loss=0.0007310368090935123
Epoch 1572
Epoch 1572 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1572, epoch_train_loss=0.0007310368090935123
Epoch 1573
Epoch 1573 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1573, epoch_train_loss=0.0007310368090935123
Epoch 1574
Epoch 1574 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1574, epoch_train_loss=0.0007310368090935123
Epoch 1575
Epoch 1575 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1575, epoch_train_loss=0.0007310368090935123
Epoch 1576
Epoch 1576 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1576, epoch_train_loss=0.0007310368090935123
Epoch 1577
Epoch 1577 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1577, epoch_train_loss=0.0007310368090935123
Epoch 1578
Epoch 1578 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1578, epoch_train_loss=0.0007310368090935123
Epoch 1579
Epoch 1579 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1579, epoch_train_loss=0.0007310368090935123
Epoch 1580
Epoch 1580 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1580, epoch_train_loss=0.0007310368090935123
Epoch 1581
Epoch 1581 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1581, epoch_train_loss=0.0007310368090935123
Epoch 1582
Epoch 1582 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1582, epoch_train_loss=0.0007310368090935123
Epoch 1583
Epoch 1583 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1583, epoch_train_loss=0.0007310368090935123
Epoch 1584
Epoch 1584 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1584, epoch_train_loss=0.0007310368090935123
Epoch 1585
Epoch 1585 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1585, epoch_train_loss=0.0007310368090935123
Epoch 1586
Epoch 1586 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1586, epoch_train_loss=0.0007310368090935123
Epoch 1587
Epoch 1587 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1587, epoch_train_loss=0.0007310368090935123
Epoch 1588
Epoch 1588 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1588, epoch_train_loss=0.0007310368090935123
Epoch 1589
Epoch 1589 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1589, epoch_train_loss=0.0007310368090935123
Epoch 1590
Epoch 1590 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1590, epoch_train_loss=0.0007310368090935123
Epoch 1591
Epoch 1591 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1591, epoch_train_loss=0.0007310368090935123
Epoch 1592
Epoch 1592 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1592, epoch_train_loss=0.0007310368090935123
Epoch 1593
Epoch 1593 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1593, epoch_train_loss=0.0007310368090935123
Epoch 1594
Epoch 1594 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1594, epoch_train_loss=0.0007310368090935123
Epoch 1595
Epoch 1595 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1595, epoch_train_loss=0.0007310368090935123
Epoch 1596
Epoch 1596 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1596, epoch_train_loss=0.0007310368090935123
Epoch 1597
Epoch 1597 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1597, epoch_train_loss=0.0007310368090935123
Epoch 1598
Epoch 1598 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1598, epoch_train_loss=0.0007310368090935123
Epoch 1599
Epoch 1599 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1599, epoch_train_loss=0.0007310368090935123
Epoch 1600
Epoch 1600 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1600, epoch_train_loss=0.0007310368090935123
Epoch 1601
Epoch 1601 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1601, epoch_train_loss=0.0007310368090935123
Epoch 1602
Epoch 1602 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1602, epoch_train_loss=0.0007310368090935123
Epoch 1603
Epoch 1603 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1603, epoch_train_loss=0.0007310368090935123
Epoch 1604
Epoch 1604 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1604, epoch_train_loss=0.0007310368090935123
Epoch 1605
Epoch 1605 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1605, epoch_train_loss=0.0007310368090935123
Epoch 1606
Epoch 1606 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1606, epoch_train_loss=0.0007310368090935123
Epoch 1607
Epoch 1607 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1607, epoch_train_loss=0.0007310368090935123
Epoch 1608
Epoch 1608 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1608, epoch_train_loss=0.0007310368090935123
Epoch 1609
Epoch 1609 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1609, epoch_train_loss=0.0007310368090935123
Epoch 1610
Epoch 1610 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1610, epoch_train_loss=0.0007310368090935123
Epoch 1611
Epoch 1611 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1611, epoch_train_loss=0.0007310368090935123
Epoch 1612
Epoch 1612 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1612, epoch_train_loss=0.0007310368090935123
Epoch 1613
Epoch 1613 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1613, epoch_train_loss=0.0007310368090935123
Epoch 1614
Epoch 1614 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1614, epoch_train_loss=0.0007310368090935123
Epoch 1615
Epoch 1615 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1615, epoch_train_loss=0.0007310368090935123
Epoch 1616
Epoch 1616 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1616, epoch_train_loss=0.0007310368090935123
Epoch 1617
Epoch 1617 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1617, epoch_train_loss=0.0007310368090935123
Epoch 1618
Epoch 1618 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1618, epoch_train_loss=0.0007310368090935123
Epoch 1619
Epoch 1619 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1619, epoch_train_loss=0.0007310368090935123
Epoch 1620
Epoch 1620 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1620, epoch_train_loss=0.0007310368090935123
Epoch 1621
Epoch 1621 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1621, epoch_train_loss=0.0007310368090935123
Epoch 1622
Epoch 1622 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1622, epoch_train_loss=0.0007310368090935123
Epoch 1623
Epoch 1623 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1623, epoch_train_loss=0.0007310368090935123
Epoch 1624
Epoch 1624 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1624, epoch_train_loss=0.0007310368090935123
Epoch 1625
Epoch 1625 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1625, epoch_train_loss=0.0007310368090935123
Epoch 1626
Epoch 1626 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1626, epoch_train_loss=0.0007310368090935123
Epoch 1627
Epoch 1627 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1627, epoch_train_loss=0.0007310368090935123
Epoch 1628
Epoch 1628 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1628, epoch_train_loss=0.0007310368090935123
Epoch 1629
Epoch 1629 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1629, epoch_train_loss=0.0007310368090935123
Epoch 1630
Epoch 1630 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1630, epoch_train_loss=0.0007310368090935123
Epoch 1631
Epoch 1631 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1631, epoch_train_loss=0.0007310368090935123
Epoch 1632
Epoch 1632 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1632, epoch_train_loss=0.0007310368090935123
Epoch 1633
Epoch 1633 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1633, epoch_train_loss=0.0007310368090935123
Epoch 1634
Epoch 1634 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1634, epoch_train_loss=0.0007310368090935123
Epoch 1635
Epoch 1635 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1635, epoch_train_loss=0.0007310368090935123
Epoch 1636
Epoch 1636 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1636, epoch_train_loss=0.0007310368090935123
Epoch 1637
Epoch 1637 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1637, epoch_train_loss=0.0007310368090935123
Epoch 1638
Epoch 1638 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1638, epoch_train_loss=0.0007310368090935123
Epoch 1639
Epoch 1639 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1639, epoch_train_loss=0.0007310368090935123
Epoch 1640
Epoch 1640 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1640, epoch_train_loss=0.0007310368090935123
Epoch 1641
Epoch 1641 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1641, epoch_train_loss=0.0007310368090935123
Epoch 1642
Epoch 1642 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1642, epoch_train_loss=0.0007310368090935123
Epoch 1643
Epoch 1643 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1643, epoch_train_loss=0.0007310368090935123
Epoch 1644
Epoch 1644 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1644, epoch_train_loss=0.0007310368090935123
Epoch 1645
Epoch 1645 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1645, epoch_train_loss=0.0007310368090935123
Epoch 1646
Epoch 1646 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1646, epoch_train_loss=0.0007310368090935123
Epoch 1647
Epoch 1647 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1647, epoch_train_loss=0.0007310368090935123
Epoch 1648
Epoch 1648 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1648, epoch_train_loss=0.0007310368090935123
Epoch 1649
Epoch 1649 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1649, epoch_train_loss=0.0007310368090935123
Epoch 1650
Epoch 1650 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1650, epoch_train_loss=0.0007310368090935123
Epoch 1651
Epoch 1651 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1651, epoch_train_loss=0.0007310368090935123
Epoch 1652
Epoch 1652 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1652, epoch_train_loss=0.0007310368090935123
Epoch 1653
Epoch 1653 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1653, epoch_train_loss=0.0007310368090935123
Epoch 1654
Epoch 1654 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1654, epoch_train_loss=0.0007310368090935123
Epoch 1655
Epoch 1655 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1655, epoch_train_loss=0.0007310368090935123
Epoch 1656
Epoch 1656 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1656, epoch_train_loss=0.0007310368090935123
Epoch 1657
Epoch 1657 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1657, epoch_train_loss=0.0007310368090935123
Epoch 1658
Epoch 1658 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1658, epoch_train_loss=0.0007310368090935123
Epoch 1659
Epoch 1659 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1659, epoch_train_loss=0.0007310368090935123
Epoch 1660
Epoch 1660 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1660, epoch_train_loss=0.0007310368090935123
Epoch 1661
Epoch 1661 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1661, epoch_train_loss=0.0007310368090935123
Epoch 1662
Epoch 1662 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1662, epoch_train_loss=0.0007310368090935123
Epoch 1663
Epoch 1663 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1663, epoch_train_loss=0.0007310368090935123
Epoch 1664
Epoch 1664 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1664, epoch_train_loss=0.0007310368090935123
Epoch 1665
Epoch 1665 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1665, epoch_train_loss=0.0007310368090935123
Epoch 1666
Epoch 1666 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1666, epoch_train_loss=0.0007310368090935123
Epoch 1667
Epoch 1667 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1667, epoch_train_loss=0.0007310368090935123
Epoch 1668
Epoch 1668 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1668, epoch_train_loss=0.0007310368090935123
Epoch 1669
Epoch 1669 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1669, epoch_train_loss=0.0007310368090935123
Epoch 1670
Epoch 1670 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1670, epoch_train_loss=0.0007310368090935123
Epoch 1671
Epoch 1671 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1671, epoch_train_loss=0.0007310368090935123
Epoch 1672
Epoch 1672 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1672, epoch_train_loss=0.0007310368090935123
Epoch 1673
Epoch 1673 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1673, epoch_train_loss=0.0007310368090935123
Epoch 1674
Epoch 1674 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1674, epoch_train_loss=0.0007310368090935123
Epoch 1675
Epoch 1675 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1675, epoch_train_loss=0.0007310368090935123
Epoch 1676
Epoch 1676 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1676, epoch_train_loss=0.0007310368090935123
Epoch 1677
Epoch 1677 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1677, epoch_train_loss=0.0007310368090935123
Epoch 1678
Epoch 1678 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1678, epoch_train_loss=0.0007310368090935123
Epoch 1679
Epoch 1679 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1679, epoch_train_loss=0.0007310368090935123
Epoch 1680
Epoch 1680 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1680, epoch_train_loss=0.0007310368090935123
Epoch 1681
Epoch 1681 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1681, epoch_train_loss=0.0007310368090935123
Epoch 1682
Epoch 1682 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1682, epoch_train_loss=0.0007310368090935123
Epoch 1683
Epoch 1683 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1683, epoch_train_loss=0.0007310368090935123
Epoch 1684
Epoch 1684 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1684, epoch_train_loss=0.0007310368090935123
Epoch 1685
Epoch 1685 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1685, epoch_train_loss=0.0007310368090935123
Epoch 1686
Epoch 1686 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1686, epoch_train_loss=0.0007310368090935123
Epoch 1687
Epoch 1687 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1687, epoch_train_loss=0.0007310368090935123
Epoch 1688
Epoch 1688 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1688, epoch_train_loss=0.0007310368090935123
Epoch 1689
Epoch 1689 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1689, epoch_train_loss=0.0007310368090935123
Epoch 1690
Epoch 1690 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1690, epoch_train_loss=0.0007310368090935123
Epoch 1691
Epoch 1691 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1691, epoch_train_loss=0.0007310368090935123
Epoch 1692
Epoch 1692 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1692, epoch_train_loss=0.0007310368090935123
Epoch 1693
Epoch 1693 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1693, epoch_train_loss=0.0007310368090935123
Epoch 1694
Epoch 1694 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1694, epoch_train_loss=0.0007310368090935123
Epoch 1695
Epoch 1695 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1695, epoch_train_loss=0.0007310368090935123
Epoch 1696
Epoch 1696 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1696, epoch_train_loss=0.0007310368090935123
Epoch 1697
Epoch 1697 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1697, epoch_train_loss=0.0007310368090935123
Epoch 1698
Epoch 1698 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1698, epoch_train_loss=0.0007310368090935123
Epoch 1699
Epoch 1699 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1699, epoch_train_loss=0.0007310368090935123
Epoch 1700
Epoch 1700 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1700, epoch_train_loss=0.0007310368090935123
Epoch 1701
Epoch 1701 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1701, epoch_train_loss=0.0007310368090935123
Epoch 1702
Epoch 1702 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1702, epoch_train_loss=0.0007310368090935123
Epoch 1703
Epoch 1703 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1703, epoch_train_loss=0.0007310368090935123
Epoch 1704
Epoch 1704 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1704, epoch_train_loss=0.0007310368090935123
Epoch 1705
Epoch 1705 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1705, epoch_train_loss=0.0007310368090935123
Epoch 1706
Epoch 1706 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1706, epoch_train_loss=0.0007310368090935123
Epoch 1707
Epoch 1707 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1707, epoch_train_loss=0.0007310368090935123
Epoch 1708
Epoch 1708 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1708, epoch_train_loss=0.0007310368090935123
Epoch 1709
Epoch 1709 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1709, epoch_train_loss=0.0007310368090935123
Epoch 1710
Epoch 1710 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1710, epoch_train_loss=0.0007310368090935123
Epoch 1711
Epoch 1711 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1711, epoch_train_loss=0.0007310368090935123
Epoch 1712
Epoch 1712 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1712, epoch_train_loss=0.0007310368090935123
Epoch 1713
Epoch 1713 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1713, epoch_train_loss=0.0007310368090935123
Epoch 1714
Epoch 1714 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1714, epoch_train_loss=0.0007310368090935123
Epoch 1715
Epoch 1715 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1715, epoch_train_loss=0.0007310368090935123
Epoch 1716
Epoch 1716 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1716, epoch_train_loss=0.0007310368090935123
Epoch 1717
Epoch 1717 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1717, epoch_train_loss=0.0007310368090935123
Epoch 1718
Epoch 1718 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1718, epoch_train_loss=0.0007310368090935123
Epoch 1719
Epoch 1719 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1719, epoch_train_loss=0.0007310368090935123
Epoch 1720
Epoch 1720 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1720, epoch_train_loss=0.0007310368090935123
Epoch 1721
Epoch 1721 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1721, epoch_train_loss=0.0007310368090935123
Epoch 1722
Epoch 1722 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1722, epoch_train_loss=0.0007310368090935123
Epoch 1723
Epoch 1723 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1723, epoch_train_loss=0.0007310368090935123
Epoch 1724
Epoch 1724 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1724, epoch_train_loss=0.0007310368090935123
Epoch 1725
Epoch 1725 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1725, epoch_train_loss=0.0007310368090935123
Epoch 1726
Epoch 1726 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1726, epoch_train_loss=0.0007310368090935123
Epoch 1727
Epoch 1727 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1727, epoch_train_loss=0.0007310368090935123
Epoch 1728
Epoch 1728 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1728, epoch_train_loss=0.0007310368090935123
Epoch 1729
Epoch 1729 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1729, epoch_train_loss=0.0007310368090935123
Epoch 1730
Epoch 1730 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1730, epoch_train_loss=0.0007310368090935123
Epoch 1731
Epoch 1731 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1731, epoch_train_loss=0.0007310368090935123
Epoch 1732
Epoch 1732 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1732, epoch_train_loss=0.0007310368090935123
Epoch 1733
Epoch 1733 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1733, epoch_train_loss=0.0007310368090935123
Epoch 1734
Epoch 1734 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1734, epoch_train_loss=0.0007310368090935123
Epoch 1735
Epoch 1735 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1735, epoch_train_loss=0.0007310368090935123
Epoch 1736
Epoch 1736 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1736, epoch_train_loss=0.0007310368090935123
Epoch 1737
Epoch 1737 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1737, epoch_train_loss=0.0007310368090935123
Epoch 1738
Epoch 1738 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1738, epoch_train_loss=0.0007310368090935123
Epoch 1739
Epoch 1739 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1739, epoch_train_loss=0.0007310368090935123
Epoch 1740
Epoch 1740 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1740, epoch_train_loss=0.0007310368090935123
Epoch 1741
Epoch 1741 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1741, epoch_train_loss=0.0007310368090935123
Epoch 1742
Epoch 1742 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1742, epoch_train_loss=0.0007310368090935123
Epoch 1743
Epoch 1743 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1743, epoch_train_loss=0.0007310368090935123
Epoch 1744
Epoch 1744 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1744, epoch_train_loss=0.0007310368090935123
Epoch 1745
Epoch 1745 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1745, epoch_train_loss=0.0007310368090935123
Epoch 1746
Epoch 1746 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1746, epoch_train_loss=0.0007310368090935123
Epoch 1747
Epoch 1747 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1747, epoch_train_loss=0.0007310368090935123
Epoch 1748
Epoch 1748 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1748, epoch_train_loss=0.0007310368090935123
Epoch 1749
Epoch 1749 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1749, epoch_train_loss=0.0007310368090935123
Epoch 1750
Epoch 1750 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1750, epoch_train_loss=0.0007310368090935123
Epoch 1751
Epoch 1751 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1751, epoch_train_loss=0.0007310368090935123
Epoch 1752
Epoch 1752 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1752, epoch_train_loss=0.0007310368090935123
Epoch 1753
Epoch 1753 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1753, epoch_train_loss=0.0007310368090935123
Epoch 1754
Epoch 1754 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1754, epoch_train_loss=0.0007310368090935123
Epoch 1755
Epoch 1755 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1755, epoch_train_loss=0.0007310368090935123
Epoch 1756
Epoch 1756 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1756, epoch_train_loss=0.0007310368090935123
Epoch 1757
Epoch 1757 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1757, epoch_train_loss=0.0007310368090935123
Epoch 1758
Epoch 1758 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1758, epoch_train_loss=0.0007310368090935123
Epoch 1759
Epoch 1759 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1759, epoch_train_loss=0.0007310368090935123
Epoch 1760
Epoch 1760 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1760, epoch_train_loss=0.0007310368090935123
Epoch 1761
Epoch 1761 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1761, epoch_train_loss=0.0007310368090935123
Epoch 1762
Epoch 1762 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1762, epoch_train_loss=0.0007310368090935123
Epoch 1763
Epoch 1763 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1763, epoch_train_loss=0.0007310368090935123
Epoch 1764
Epoch 1764 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1764, epoch_train_loss=0.0007310368090935123
Epoch 1765
Epoch 1765 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1765, epoch_train_loss=0.0007310368090935123
Epoch 1766
Epoch 1766 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1766, epoch_train_loss=0.0007310368090935123
Epoch 1767
Epoch 1767 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1767, epoch_train_loss=0.0007310368090935123
Epoch 1768
Epoch 1768 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1768, epoch_train_loss=0.0007310368090935123
Epoch 1769
Epoch 1769 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1769, epoch_train_loss=0.0007310368090935123
Epoch 1770
Epoch 1770 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1770, epoch_train_loss=0.0007310368090935123
Epoch 1771
Epoch 1771 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1771, epoch_train_loss=0.0007310368090935123
Epoch 1772
Epoch 1772 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1772, epoch_train_loss=0.0007310368090935123
Epoch 1773
Epoch 1773 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1773, epoch_train_loss=0.0007310368090935123
Epoch 1774
Epoch 1774 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1774, epoch_train_loss=0.0007310368090935123
Epoch 1775
Epoch 1775 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1775, epoch_train_loss=0.0007310368090935123
Epoch 1776
Epoch 1776 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1776, epoch_train_loss=0.0007310368090935123
Epoch 1777
Epoch 1777 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1777, epoch_train_loss=0.0007310368090935123
Epoch 1778
Epoch 1778 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1778, epoch_train_loss=0.0007310368090935123
Epoch 1779
Epoch 1779 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1779, epoch_train_loss=0.0007310368090935123
Epoch 1780
Epoch 1780 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1780, epoch_train_loss=0.0007310368090935123
Epoch 1781
Epoch 1781 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1781, epoch_train_loss=0.0007310368090935123
Epoch 1782
Epoch 1782 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1782, epoch_train_loss=0.0007310368090935123
Epoch 1783
Epoch 1783 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1783, epoch_train_loss=0.0007310368090935123
Epoch 1784
Epoch 1784 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1784, epoch_train_loss=0.0007310368090935123
Epoch 1785
Epoch 1785 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1785, epoch_train_loss=0.0007310368090935123
Epoch 1786
Epoch 1786 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1786, epoch_train_loss=0.0007310368090935123
Epoch 1787
Epoch 1787 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1787, epoch_train_loss=0.0007310368090935123
Epoch 1788
Epoch 1788 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1788, epoch_train_loss=0.0007310368090935123
Epoch 1789
Epoch 1789 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1789, epoch_train_loss=0.0007310368090935123
Epoch 1790
Epoch 1790 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1790, epoch_train_loss=0.0007310368090935123
Epoch 1791
Epoch 1791 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1791, epoch_train_loss=0.0007310368090935123
Epoch 1792
Epoch 1792 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1792, epoch_train_loss=0.0007310368090935123
Epoch 1793
Epoch 1793 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1793, epoch_train_loss=0.0007310368090935123
Epoch 1794
Epoch 1794 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1794, epoch_train_loss=0.0007310368090935123
Epoch 1795
Epoch 1795 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1795, epoch_train_loss=0.0007310368090935123
Epoch 1796
Epoch 1796 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1796, epoch_train_loss=0.0007310368090935123
Epoch 1797
Epoch 1797 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1797, epoch_train_loss=0.0007310368090935123
Epoch 1798
Epoch 1798 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1798, epoch_train_loss=0.0007310368090935123
Epoch 1799
Epoch 1799 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1799, epoch_train_loss=0.0007310368090935123
Epoch 1800
Epoch 1800 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1800, epoch_train_loss=0.0007310368090935123
Epoch 1801
Epoch 1801 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1801, epoch_train_loss=0.0007310368090935123
Epoch 1802
Epoch 1802 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1802, epoch_train_loss=0.0007310368090935123
Epoch 1803
Epoch 1803 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1803, epoch_train_loss=0.0007310368090935123
Epoch 1804
Epoch 1804 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1804, epoch_train_loss=0.0007310368090935123
Epoch 1805
Epoch 1805 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1805, epoch_train_loss=0.0007310368090935123
Epoch 1806
Epoch 1806 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1806, epoch_train_loss=0.0007310368090935123
Epoch 1807
Epoch 1807 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1807, epoch_train_loss=0.0007310368090935123
Epoch 1808
Epoch 1808 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1808, epoch_train_loss=0.0007310368090935123
Epoch 1809
Epoch 1809 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1809, epoch_train_loss=0.0007310368090935123
Epoch 1810
Epoch 1810 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1810, epoch_train_loss=0.0007310368090935123
Epoch 1811
Epoch 1811 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1811, epoch_train_loss=0.0007310368090935123
Epoch 1812
Epoch 1812 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1812, epoch_train_loss=0.0007310368090935123
Epoch 1813
Epoch 1813 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1813, epoch_train_loss=0.0007310368090935123
Epoch 1814
Epoch 1814 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1814, epoch_train_loss=0.0007310368090935123
Epoch 1815
Epoch 1815 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1815, epoch_train_loss=0.0007310368090935123
Epoch 1816
Epoch 1816 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1816, epoch_train_loss=0.0007310368090935123
Epoch 1817
Epoch 1817 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1817, epoch_train_loss=0.0007310368090935123
Epoch 1818
Epoch 1818 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1818, epoch_train_loss=0.0007310368090935123
Epoch 1819
Epoch 1819 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1819, epoch_train_loss=0.0007310368090935123
Epoch 1820
Epoch 1820 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1820, epoch_train_loss=0.0007310368090935123
Epoch 1821
Epoch 1821 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1821, epoch_train_loss=0.0007310368090935123
Epoch 1822
Epoch 1822 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1822, epoch_train_loss=0.0007310368090935123
Epoch 1823
Epoch 1823 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1823, epoch_train_loss=0.0007310368090935123
Epoch 1824
Epoch 1824 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1824, epoch_train_loss=0.0007310368090935123
Epoch 1825
Epoch 1825 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1825, epoch_train_loss=0.0007310368090935123
Epoch 1826
Epoch 1826 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1826, epoch_train_loss=0.0007310368090935123
Epoch 1827
Epoch 1827 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1827, epoch_train_loss=0.0007310368090935123
Epoch 1828
Epoch 1828 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1828, epoch_train_loss=0.0007310368090935123
Epoch 1829
Epoch 1829 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1829, epoch_train_loss=0.0007310368090935123
Epoch 1830
Epoch 1830 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1830, epoch_train_loss=0.0007310368090935123
Epoch 1831
Epoch 1831 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1831, epoch_train_loss=0.0007310368090935123
Epoch 1832
Epoch 1832 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1832, epoch_train_loss=0.0007310368090935123
Epoch 1833
Epoch 1833 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1833, epoch_train_loss=0.0007310368090935123
Epoch 1834
Epoch 1834 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1834, epoch_train_loss=0.0007310368090935123
Epoch 1835
Epoch 1835 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1835, epoch_train_loss=0.0007310368090935123
Epoch 1836
Epoch 1836 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1836, epoch_train_loss=0.0007310368090935123
Epoch 1837
Epoch 1837 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1837, epoch_train_loss=0.0007310368090935123
Epoch 1838
Epoch 1838 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1838, epoch_train_loss=0.0007310368090935123
Epoch 1839
Epoch 1839 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1839, epoch_train_loss=0.0007310368090935123
Epoch 1840
Epoch 1840 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1840, epoch_train_loss=0.0007310368090935123
Epoch 1841
Epoch 1841 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1841, epoch_train_loss=0.0007310368090935123
Epoch 1842
Epoch 1842 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1842, epoch_train_loss=0.0007310368090935123
Epoch 1843
Epoch 1843 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1843, epoch_train_loss=0.0007310368090935123
Epoch 1844
Epoch 1844 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1844, epoch_train_loss=0.0007310368090935123
Epoch 1845
Epoch 1845 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1845, epoch_train_loss=0.0007310368090935123
Epoch 1846
Epoch 1846 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1846, epoch_train_loss=0.0007310368090935123
Epoch 1847
Epoch 1847 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1847, epoch_train_loss=0.0007310368090935123
Epoch 1848
Epoch 1848 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1848, epoch_train_loss=0.0007310368090935123
Epoch 1849
Epoch 1849 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1849, epoch_train_loss=0.0007310368090935123
Epoch 1850
Epoch 1850 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1850, epoch_train_loss=0.0007310368090935123
Epoch 1851
Epoch 1851 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1851, epoch_train_loss=0.0007310368090935123
Epoch 1852
Epoch 1852 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1852, epoch_train_loss=0.0007310368090935123
Epoch 1853
Epoch 1853 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1853, epoch_train_loss=0.0007310368090935123
Epoch 1854
Epoch 1854 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1854, epoch_train_loss=0.0007310368090935123
Epoch 1855
Epoch 1855 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1855, epoch_train_loss=0.0007310368090935123
Epoch 1856
Epoch 1856 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1856, epoch_train_loss=0.0007310368090935123
Epoch 1857
Epoch 1857 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1857, epoch_train_loss=0.0007310368090935123
Epoch 1858
Epoch 1858 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1858, epoch_train_loss=0.0007310368090935123
Epoch 1859
Epoch 1859 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1859, epoch_train_loss=0.0007310368090935123
Epoch 1860
Epoch 1860 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1860, epoch_train_loss=0.0007310368090935123
Epoch 1861
Epoch 1861 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1861, epoch_train_loss=0.0007310368090935123
Epoch 1862
Epoch 1862 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1862, epoch_train_loss=0.0007310368090935123
Epoch 1863
Epoch 1863 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1863, epoch_train_loss=0.0007310368090935123
Epoch 1864
Epoch 1864 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1864, epoch_train_loss=0.0007310368090935123
Epoch 1865
Epoch 1865 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1865, epoch_train_loss=0.0007310368090935123
Epoch 1866
Epoch 1866 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1866, epoch_train_loss=0.0007310368090935123
Epoch 1867
Epoch 1867 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1867, epoch_train_loss=0.0007310368090935123
Epoch 1868
Epoch 1868 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1868, epoch_train_loss=0.0007310368090935123
Epoch 1869
Epoch 1869 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1869, epoch_train_loss=0.0007310368090935123
Epoch 1870
Epoch 1870 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1870, epoch_train_loss=0.0007310368090935123
Epoch 1871
Epoch 1871 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1871, epoch_train_loss=0.0007310368090935123
Epoch 1872
Epoch 1872 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1872, epoch_train_loss=0.0007310368090935123
Epoch 1873
Epoch 1873 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1873, epoch_train_loss=0.0007310368090935123
Epoch 1874
Epoch 1874 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1874, epoch_train_loss=0.0007310368090935123
Epoch 1875
Epoch 1875 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1875, epoch_train_loss=0.0007310368090935123
Epoch 1876
Epoch 1876 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1876, epoch_train_loss=0.0007310368090935123
Epoch 1877
Epoch 1877 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1877, epoch_train_loss=0.0007310368090935123
Epoch 1878
Epoch 1878 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1878, epoch_train_loss=0.0007310368090935123
Epoch 1879
Epoch 1879 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1879, epoch_train_loss=0.0007310368090935123
Epoch 1880
Epoch 1880 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1880, epoch_train_loss=0.0007310368090935123
Epoch 1881
Epoch 1881 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1881, epoch_train_loss=0.0007310368090935123
Epoch 1882
Epoch 1882 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1882, epoch_train_loss=0.0007310368090935123
Epoch 1883
Epoch 1883 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1883, epoch_train_loss=0.0007310368090935123
Epoch 1884
Epoch 1884 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1884, epoch_train_loss=0.0007310368090935123
Epoch 1885
Epoch 1885 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1885, epoch_train_loss=0.0007310368090935123
Epoch 1886
Epoch 1886 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1886, epoch_train_loss=0.0007310368090935123
Epoch 1887
Epoch 1887 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1887, epoch_train_loss=0.0007310368090935123
Epoch 1888
Epoch 1888 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1888, epoch_train_loss=0.0007310368090935123
Epoch 1889
Epoch 1889 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1889, epoch_train_loss=0.0007310368090935123
Epoch 1890
Epoch 1890 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1890, epoch_train_loss=0.0007310368090935123
Epoch 1891
Epoch 1891 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1891, epoch_train_loss=0.0007310368090935123
Epoch 1892
Epoch 1892 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1892, epoch_train_loss=0.0007310368090935123
Epoch 1893
Epoch 1893 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1893, epoch_train_loss=0.0007310368090935123
Epoch 1894
Epoch 1894 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1894, epoch_train_loss=0.0007310368090935123
Epoch 1895
Epoch 1895 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1895, epoch_train_loss=0.0007310368090935123
Epoch 1896
Epoch 1896 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1896, epoch_train_loss=0.0007310368090935123
Epoch 1897
Epoch 1897 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1897, epoch_train_loss=0.0007310368090935123
Epoch 1898
Epoch 1898 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1898, epoch_train_loss=0.0007310368090935123
Epoch 1899
Epoch 1899 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1899, epoch_train_loss=0.0007310368090935123
Epoch 1900
Epoch 1900 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1900, epoch_train_loss=0.0007310368090935123
Epoch 1901
Epoch 1901 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1901, epoch_train_loss=0.0007310368090935123
Epoch 1902
Epoch 1902 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1902, epoch_train_loss=0.0007310368090935123
Epoch 1903
Epoch 1903 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1903, epoch_train_loss=0.0007310368090935123
Epoch 1904
Epoch 1904 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1904, epoch_train_loss=0.0007310368090935123
Epoch 1905
Epoch 1905 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1905, epoch_train_loss=0.0007310368090935123
Epoch 1906
Epoch 1906 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1906, epoch_train_loss=0.0007310368090935123
Epoch 1907
Epoch 1907 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1907, epoch_train_loss=0.0007310368090935123
Epoch 1908
Epoch 1908 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1908, epoch_train_loss=0.0007310368090935123
Epoch 1909
Epoch 1909 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1909, epoch_train_loss=0.0007310368090935123
Epoch 1910
Epoch 1910 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1910, epoch_train_loss=0.0007310368090935123
Epoch 1911
Epoch 1911 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1911, epoch_train_loss=0.0007310368090935123
Epoch 1912
Epoch 1912 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1912, epoch_train_loss=0.0007310368090935123
Epoch 1913
Epoch 1913 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1913, epoch_train_loss=0.0007310368090935123
Epoch 1914
Epoch 1914 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1914, epoch_train_loss=0.0007310368090935123
Epoch 1915
Epoch 1915 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1915, epoch_train_loss=0.0007310368090935123
Epoch 1916
Epoch 1916 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1916, epoch_train_loss=0.0007310368090935123
Epoch 1917
Epoch 1917 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1917, epoch_train_loss=0.0007310368090935123
Epoch 1918
Epoch 1918 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1918, epoch_train_loss=0.0007310368090935123
Epoch 1919
Epoch 1919 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1919, epoch_train_loss=0.0007310368090935123
Epoch 1920
Epoch 1920 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1920, epoch_train_loss=0.0007310368090935123
Epoch 1921
Epoch 1921 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1921, epoch_train_loss=0.0007310368090935123
Epoch 1922
Epoch 1922 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1922, epoch_train_loss=0.0007310368090935123
Epoch 1923
Epoch 1923 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1923, epoch_train_loss=0.0007310368090935123
Epoch 1924
Epoch 1924 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1924, epoch_train_loss=0.0007310368090935123
Epoch 1925
Epoch 1925 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1925, epoch_train_loss=0.0007310368090935123
Epoch 1926
Epoch 1926 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1926, epoch_train_loss=0.0007310368090935123
Epoch 1927
Epoch 1927 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1927, epoch_train_loss=0.0007310368090935123
Epoch 1928
Epoch 1928 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1928, epoch_train_loss=0.0007310368090935123
Epoch 1929
Epoch 1929 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1929, epoch_train_loss=0.0007310368090935123
Epoch 1930
Epoch 1930 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1930, epoch_train_loss=0.0007310368090935123
Epoch 1931
Epoch 1931 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1931, epoch_train_loss=0.0007310368090935123
Epoch 1932
Epoch 1932 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1932, epoch_train_loss=0.0007310368090935123
Epoch 1933
Epoch 1933 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1933, epoch_train_loss=0.0007310368090935123
Epoch 1934
Epoch 1934 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1934, epoch_train_loss=0.0007310368090935123
Epoch 1935
Epoch 1935 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1935, epoch_train_loss=0.0007310368090935123
Epoch 1936
Epoch 1936 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1936, epoch_train_loss=0.0007310368090935123
Epoch 1937
Epoch 1937 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1937, epoch_train_loss=0.0007310368090935123
Epoch 1938
Epoch 1938 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1938, epoch_train_loss=0.0007310368090935123
Epoch 1939
Epoch 1939 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1939, epoch_train_loss=0.0007310368090935123
Epoch 1940
Epoch 1940 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1940, epoch_train_loss=0.0007310368090935123
Epoch 1941
Epoch 1941 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1941, epoch_train_loss=0.0007310368090935123
Epoch 1942
Epoch 1942 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1942, epoch_train_loss=0.0007310368090935123
Epoch 1943
Epoch 1943 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1943, epoch_train_loss=0.0007310368090935123
Epoch 1944
Epoch 1944 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1944, epoch_train_loss=0.0007310368090935123
Epoch 1945
Epoch 1945 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1945, epoch_train_loss=0.0007310368090935123
Epoch 1946
Epoch 1946 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1946, epoch_train_loss=0.0007310368090935123
Epoch 1947
Epoch 1947 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1947, epoch_train_loss=0.0007310368090935123
Epoch 1948
Epoch 1948 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1948, epoch_train_loss=0.0007310368090935123
Epoch 1949
Epoch 1949 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1949, epoch_train_loss=0.0007310368090935123
Epoch 1950
Epoch 1950 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1950, epoch_train_loss=0.0007310368090935123
Epoch 1951
Epoch 1951 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1951, epoch_train_loss=0.0007310368090935123
Epoch 1952
Epoch 1952 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1952, epoch_train_loss=0.0007310368090935123
Epoch 1953
Epoch 1953 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1953, epoch_train_loss=0.0007310368090935123
Epoch 1954
Epoch 1954 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1954, epoch_train_loss=0.0007310368090935123
Epoch 1955
Epoch 1955 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1955, epoch_train_loss=0.0007310368090935123
Epoch 1956
Epoch 1956 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1956, epoch_train_loss=0.0007310368090935123
Epoch 1957
Epoch 1957 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1957, epoch_train_loss=0.0007310368090935123
Epoch 1958
Epoch 1958 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1958, epoch_train_loss=0.0007310368090935123
Epoch 1959
Epoch 1959 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1959, epoch_train_loss=0.0007310368090935123
Epoch 1960
Epoch 1960 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1960, epoch_train_loss=0.0007310368090935123
Epoch 1961
Epoch 1961 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1961, epoch_train_loss=0.0007310368090935123
Epoch 1962
Epoch 1962 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1962, epoch_train_loss=0.0007310368090935123
Epoch 1963
Epoch 1963 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1963, epoch_train_loss=0.0007310368090935123
Epoch 1964
Epoch 1964 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1964, epoch_train_loss=0.0007310368090935123
Epoch 1965
Epoch 1965 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1965, epoch_train_loss=0.0007310368090935123
Epoch 1966
Epoch 1966 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1966, epoch_train_loss=0.0007310368090935123
Epoch 1967
Epoch 1967 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1967, epoch_train_loss=0.0007310368090935123
Epoch 1968
Epoch 1968 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1968, epoch_train_loss=0.0007310368090935123
Epoch 1969
Epoch 1969 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1969, epoch_train_loss=0.0007310368090935123
Epoch 1970
Epoch 1970 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1970, epoch_train_loss=0.0007310368090935123
Epoch 1971
Epoch 1971 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1971, epoch_train_loss=0.0007310368090935123
Epoch 1972
Epoch 1972 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1972, epoch_train_loss=0.0007310368090935123
Epoch 1973
Epoch 1973 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1973, epoch_train_loss=0.0007310368090935123
Epoch 1974
Epoch 1974 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1974, epoch_train_loss=0.0007310368090935123
Epoch 1975
Epoch 1975 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1975, epoch_train_loss=0.0007310368090935123
Epoch 1976
Epoch 1976 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1976, epoch_train_loss=0.0007310368090935123
Epoch 1977
Epoch 1977 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1977, epoch_train_loss=0.0007310368090935123
Epoch 1978
Epoch 1978 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1978, epoch_train_loss=0.0007310368090935123
Epoch 1979
Epoch 1979 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1979, epoch_train_loss=0.0007310368090935123
Epoch 1980
Epoch 1980 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1980, epoch_train_loss=0.0007310368090935123
Epoch 1981
Epoch 1981 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1981, epoch_train_loss=0.0007310368090935123
Epoch 1982
Epoch 1982 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1982, epoch_train_loss=0.0007310368090935123
Epoch 1983
Epoch 1983 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1983, epoch_train_loss=0.0007310368090935123
Epoch 1984
Epoch 1984 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1984, epoch_train_loss=0.0007310368090935123
Epoch 1985
Epoch 1985 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1985, epoch_train_loss=0.0007310368090935123
Epoch 1986
Epoch 1986 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1986, epoch_train_loss=0.0007310368090935123
Epoch 1987
Epoch 1987 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1987, epoch_train_loss=0.0007310368090935123
Epoch 1988
Epoch 1988 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1988, epoch_train_loss=0.0007310368090935123
Epoch 1989
Epoch 1989 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1989, epoch_train_loss=0.0007310368090935123
Epoch 1990
Epoch 1990 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1990, epoch_train_loss=0.0007310368090935123
Epoch 1991
Epoch 1991 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1991, epoch_train_loss=0.0007310368090935123
Epoch 1992
Epoch 1992 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1992, epoch_train_loss=0.0007310368090935123
Epoch 1993
Epoch 1993 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1993, epoch_train_loss=0.0007310368090935123
Epoch 1994
Epoch 1994 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1994, epoch_train_loss=0.0007310368090935123
Epoch 1995
Epoch 1995 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1995, epoch_train_loss=0.0007310368090935123
Epoch 1996
Epoch 1996 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1996, epoch_train_loss=0.0007310368090935123
Epoch 1997
Epoch 1997 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1997, epoch_train_loss=0.0007310368090935123
Epoch 1998
Epoch 1998 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1998, epoch_train_loss=0.0007310368090935123
Epoch 1999
Epoch 1999 :: Batch 0/1
Batch Loss = 0.0007310368090935123
1999, epoch_train_loss=0.0007310368090935123
Epoch 2000
Epoch 2000 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2000, epoch_train_loss=0.0007310368090935123
Epoch 2001
Epoch 2001 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2001, epoch_train_loss=0.0007310368090935123
Epoch 2002
Epoch 2002 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2002, epoch_train_loss=0.0007310368090935123
Epoch 2003
Epoch 2003 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2003, epoch_train_loss=0.0007310368090935123
Epoch 2004
Epoch 2004 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2004, epoch_train_loss=0.0007310368090935123
Epoch 2005
Epoch 2005 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2005, epoch_train_loss=0.0007310368090935123
Epoch 2006
Epoch 2006 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2006, epoch_train_loss=0.0007310368090935123
Epoch 2007
Epoch 2007 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2007, epoch_train_loss=0.0007310368090935123
Epoch 2008
Epoch 2008 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2008, epoch_train_loss=0.0007310368090935123
Epoch 2009
Epoch 2009 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2009, epoch_train_loss=0.0007310368090935123
Epoch 2010
Epoch 2010 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2010, epoch_train_loss=0.0007310368090935123
Epoch 2011
Epoch 2011 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2011, epoch_train_loss=0.0007310368090935123
Epoch 2012
Epoch 2012 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2012, epoch_train_loss=0.0007310368090935123
Epoch 2013
Epoch 2013 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2013, epoch_train_loss=0.0007310368090935123
Epoch 2014
Epoch 2014 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2014, epoch_train_loss=0.0007310368090935123
Epoch 2015
Epoch 2015 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2015, epoch_train_loss=0.0007310368090935123
Epoch 2016
Epoch 2016 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2016, epoch_train_loss=0.0007310368090935123
Epoch 2017
Epoch 2017 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2017, epoch_train_loss=0.0007310368090935123
Epoch 2018
Epoch 2018 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2018, epoch_train_loss=0.0007310368090935123
Epoch 2019
Epoch 2019 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2019, epoch_train_loss=0.0007310368090935123
Epoch 2020
Epoch 2020 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2020, epoch_train_loss=0.0007310368090935123
Epoch 2021
Epoch 2021 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2021, epoch_train_loss=0.0007310368090935123
Epoch 2022
Epoch 2022 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2022, epoch_train_loss=0.0007310368090935123
Epoch 2023
Epoch 2023 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2023, epoch_train_loss=0.0007310368090935123
Epoch 2024
Epoch 2024 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2024, epoch_train_loss=0.0007310368090935123
Epoch 2025
Epoch 2025 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2025, epoch_train_loss=0.0007310368090935123
Epoch 2026
Epoch 2026 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2026, epoch_train_loss=0.0007310368090935123
Epoch 2027
Epoch 2027 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2027, epoch_train_loss=0.0007310368090935123
Epoch 2028
Epoch 2028 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2028, epoch_train_loss=0.0007310368090935123
Epoch 2029
Epoch 2029 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2029, epoch_train_loss=0.0007310368090935123
Epoch 2030
Epoch 2030 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2030, epoch_train_loss=0.0007310368090935123
Epoch 2031
Epoch 2031 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2031, epoch_train_loss=0.0007310368090935123
Epoch 2032
Epoch 2032 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2032, epoch_train_loss=0.0007310368090935123
Epoch 2033
Epoch 2033 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2033, epoch_train_loss=0.0007310368090935123
Epoch 2034
Epoch 2034 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2034, epoch_train_loss=0.0007310368090935123
Epoch 2035
Epoch 2035 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2035, epoch_train_loss=0.0007310368090935123
Epoch 2036
Epoch 2036 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2036, epoch_train_loss=0.0007310368090935123
Epoch 2037
Epoch 2037 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2037, epoch_train_loss=0.0007310368090935123
Epoch 2038
Epoch 2038 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2038, epoch_train_loss=0.0007310368090935123
Epoch 2039
Epoch 2039 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2039, epoch_train_loss=0.0007310368090935123
Epoch 2040
Epoch 2040 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2040, epoch_train_loss=0.0007310368090935123
Epoch 2041
Epoch 2041 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2041, epoch_train_loss=0.0007310368090935123
Epoch 2042
Epoch 2042 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2042, epoch_train_loss=0.0007310368090935123
Epoch 2043
Epoch 2043 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2043, epoch_train_loss=0.0007310368090935123
Epoch 2044
Epoch 2044 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2044, epoch_train_loss=0.0007310368090935123
Epoch 2045
Epoch 2045 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2045, epoch_train_loss=0.0007310368090935123
Epoch 2046
Epoch 2046 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2046, epoch_train_loss=0.0007310368090935123
Epoch 2047
Epoch 2047 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2047, epoch_train_loss=0.0007310368090935123
Epoch 2048
Epoch 2048 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2048, epoch_train_loss=0.0007310368090935123
Epoch 2049
Epoch 2049 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2049, epoch_train_loss=0.0007310368090935123
Epoch 2050
Epoch 2050 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2050, epoch_train_loss=0.0007310368090935123
Epoch 2051
Epoch 2051 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2051, epoch_train_loss=0.0007310368090935123
Epoch 2052
Epoch 2052 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2052, epoch_train_loss=0.0007310368090935123
Epoch 2053
Epoch 2053 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2053, epoch_train_loss=0.0007310368090935123
Epoch 2054
Epoch 2054 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2054, epoch_train_loss=0.0007310368090935123
Epoch 2055
Epoch 2055 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2055, epoch_train_loss=0.0007310368090935123
Epoch 2056
Epoch 2056 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2056, epoch_train_loss=0.0007310368090935123
Epoch 2057
Epoch 2057 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2057, epoch_train_loss=0.0007310368090935123
Epoch 2058
Epoch 2058 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2058, epoch_train_loss=0.0007310368090935123
Epoch 2059
Epoch 2059 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2059, epoch_train_loss=0.0007310368090935123
Epoch 2060
Epoch 2060 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2060, epoch_train_loss=0.0007310368090935123
Epoch 2061
Epoch 2061 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2061, epoch_train_loss=0.0007310368090935123
Epoch 2062
Epoch 2062 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2062, epoch_train_loss=0.0007310368090935123
Epoch 2063
Epoch 2063 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2063, epoch_train_loss=0.0007310368090935123
Epoch 2064
Epoch 2064 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2064, epoch_train_loss=0.0007310368090935123
Epoch 2065
Epoch 2065 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2065, epoch_train_loss=0.0007310368090935123
Epoch 2066
Epoch 2066 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2066, epoch_train_loss=0.0007310368090935123
Epoch 2067
Epoch 2067 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2067, epoch_train_loss=0.0007310368090935123
Epoch 2068
Epoch 2068 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2068, epoch_train_loss=0.0007310368090935123
Epoch 2069
Epoch 2069 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2069, epoch_train_loss=0.0007310368090935123
Epoch 2070
Epoch 2070 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2070, epoch_train_loss=0.0007310368090935123
Epoch 2071
Epoch 2071 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2071, epoch_train_loss=0.0007310368090935123
Epoch 2072
Epoch 2072 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2072, epoch_train_loss=0.0007310368090935123
Epoch 2073
Epoch 2073 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2073, epoch_train_loss=0.0007310368090935123
Epoch 2074
Epoch 2074 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2074, epoch_train_loss=0.0007310368090935123
Epoch 2075
Epoch 2075 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2075, epoch_train_loss=0.0007310368090935123
Epoch 2076
Epoch 2076 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2076, epoch_train_loss=0.0007310368090935123
Epoch 2077
Epoch 2077 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2077, epoch_train_loss=0.0007310368090935123
Epoch 2078
Epoch 2078 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2078, epoch_train_loss=0.0007310368090935123
Epoch 2079
Epoch 2079 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2079, epoch_train_loss=0.0007310368090935123
Epoch 2080
Epoch 2080 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2080, epoch_train_loss=0.0007310368090935123
Epoch 2081
Epoch 2081 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2081, epoch_train_loss=0.0007310368090935123
Epoch 2082
Epoch 2082 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2082, epoch_train_loss=0.0007310368090935123
Epoch 2083
Epoch 2083 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2083, epoch_train_loss=0.0007310368090935123
Epoch 2084
Epoch 2084 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2084, epoch_train_loss=0.0007310368090935123
Epoch 2085
Epoch 2085 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2085, epoch_train_loss=0.0007310368090935123
Epoch 2086
Epoch 2086 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2086, epoch_train_loss=0.0007310368090935123
Epoch 2087
Epoch 2087 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2087, epoch_train_loss=0.0007310368090935123
Epoch 2088
Epoch 2088 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2088, epoch_train_loss=0.0007310368090935123
Epoch 2089
Epoch 2089 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2089, epoch_train_loss=0.0007310368090935123
Epoch 2090
Epoch 2090 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2090, epoch_train_loss=0.0007310368090935123
Epoch 2091
Epoch 2091 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2091, epoch_train_loss=0.0007310368090935123
Epoch 2092
Epoch 2092 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2092, epoch_train_loss=0.0007310368090935123
Epoch 2093
Epoch 2093 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2093, epoch_train_loss=0.0007310368090935123
Epoch 2094
Epoch 2094 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2094, epoch_train_loss=0.0007310368090935123
Epoch 2095
Epoch 2095 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2095, epoch_train_loss=0.0007310368090935123
Epoch 2096
Epoch 2096 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2096, epoch_train_loss=0.0007310368090935123
Epoch 2097
Epoch 2097 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2097, epoch_train_loss=0.0007310368090935123
Epoch 2098
Epoch 2098 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2098, epoch_train_loss=0.0007310368090935123
Epoch 2099
Epoch 2099 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2099, epoch_train_loss=0.0007310368090935123
Epoch 2100
Epoch 2100 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2100, epoch_train_loss=0.0007310368090935123
Epoch 2101
Epoch 2101 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2101, epoch_train_loss=0.0007310368090935123
Epoch 2102
Epoch 2102 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2102, epoch_train_loss=0.0007310368090935123
Epoch 2103
Epoch 2103 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2103, epoch_train_loss=0.0007310368090935123
Epoch 2104
Epoch 2104 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2104, epoch_train_loss=0.0007310368090935123
Epoch 2105
Epoch 2105 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2105, epoch_train_loss=0.0007310368090935123
Epoch 2106
Epoch 2106 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2106, epoch_train_loss=0.0007310368090935123
Epoch 2107
Epoch 2107 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2107, epoch_train_loss=0.0007310368090935123
Epoch 2108
Epoch 2108 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2108, epoch_train_loss=0.0007310368090935123
Epoch 2109
Epoch 2109 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2109, epoch_train_loss=0.0007310368090935123
Epoch 2110
Epoch 2110 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2110, epoch_train_loss=0.0007310368090935123
Epoch 2111
Epoch 2111 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2111, epoch_train_loss=0.0007310368090935123
Epoch 2112
Epoch 2112 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2112, epoch_train_loss=0.0007310368090935123
Epoch 2113
Epoch 2113 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2113, epoch_train_loss=0.0007310368090935123
Epoch 2114
Epoch 2114 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2114, epoch_train_loss=0.0007310368090935123
Epoch 2115
Epoch 2115 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2115, epoch_train_loss=0.0007310368090935123
Epoch 2116
Epoch 2116 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2116, epoch_train_loss=0.0007310368090935123
Epoch 2117
Epoch 2117 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2117, epoch_train_loss=0.0007310368090935123
Epoch 2118
Epoch 2118 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2118, epoch_train_loss=0.0007310368090935123
Epoch 2119
Epoch 2119 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2119, epoch_train_loss=0.0007310368090935123
Epoch 2120
Epoch 2120 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2120, epoch_train_loss=0.0007310368090935123
Epoch 2121
Epoch 2121 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2121, epoch_train_loss=0.0007310368090935123
Epoch 2122
Epoch 2122 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2122, epoch_train_loss=0.0007310368090935123
Epoch 2123
Epoch 2123 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2123, epoch_train_loss=0.0007310368090935123
Epoch 2124
Epoch 2124 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2124, epoch_train_loss=0.0007310368090935123
Epoch 2125
Epoch 2125 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2125, epoch_train_loss=0.0007310368090935123
Epoch 2126
Epoch 2126 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2126, epoch_train_loss=0.0007310368090935123
Epoch 2127
Epoch 2127 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2127, epoch_train_loss=0.0007310368090935123
Epoch 2128
Epoch 2128 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2128, epoch_train_loss=0.0007310368090935123
Epoch 2129
Epoch 2129 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2129, epoch_train_loss=0.0007310368090935123
Epoch 2130
Epoch 2130 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2130, epoch_train_loss=0.0007310368090935123
Epoch 2131
Epoch 2131 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2131, epoch_train_loss=0.0007310368090935123
Epoch 2132
Epoch 2132 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2132, epoch_train_loss=0.0007310368090935123
Epoch 2133
Epoch 2133 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2133, epoch_train_loss=0.0007310368090935123
Epoch 2134
Epoch 2134 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2134, epoch_train_loss=0.0007310368090935123
Epoch 2135
Epoch 2135 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2135, epoch_train_loss=0.0007310368090935123
Epoch 2136
Epoch 2136 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2136, epoch_train_loss=0.0007310368090935123
Epoch 2137
Epoch 2137 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2137, epoch_train_loss=0.0007310368090935123
Epoch 2138
Epoch 2138 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2138, epoch_train_loss=0.0007310368090935123
Epoch 2139
Epoch 2139 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2139, epoch_train_loss=0.0007310368090935123
Epoch 2140
Epoch 2140 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2140, epoch_train_loss=0.0007310368090935123
Epoch 2141
Epoch 2141 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2141, epoch_train_loss=0.0007310368090935123
Epoch 2142
Epoch 2142 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2142, epoch_train_loss=0.0007310368090935123
Epoch 2143
Epoch 2143 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2143, epoch_train_loss=0.0007310368090935123
Epoch 2144
Epoch 2144 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2144, epoch_train_loss=0.0007310368090935123
Epoch 2145
Epoch 2145 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2145, epoch_train_loss=0.0007310368090935123
Epoch 2146
Epoch 2146 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2146, epoch_train_loss=0.0007310368090935123
Epoch 2147
Epoch 2147 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2147, epoch_train_loss=0.0007310368090935123
Epoch 2148
Epoch 2148 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2148, epoch_train_loss=0.0007310368090935123
Epoch 2149
Epoch 2149 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2149, epoch_train_loss=0.0007310368090935123
Epoch 2150
Epoch 2150 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2150, epoch_train_loss=0.0007310368090935123
Epoch 2151
Epoch 2151 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2151, epoch_train_loss=0.0007310368090935123
Epoch 2152
Epoch 2152 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2152, epoch_train_loss=0.0007310368090935123
Epoch 2153
Epoch 2153 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2153, epoch_train_loss=0.0007310368090935123
Epoch 2154
Epoch 2154 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2154, epoch_train_loss=0.0007310368090935123
Epoch 2155
Epoch 2155 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2155, epoch_train_loss=0.0007310368090935123
Epoch 2156
Epoch 2156 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2156, epoch_train_loss=0.0007310368090935123
Epoch 2157
Epoch 2157 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2157, epoch_train_loss=0.0007310368090935123
Epoch 2158
Epoch 2158 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2158, epoch_train_loss=0.0007310368090935123
Epoch 2159
Epoch 2159 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2159, epoch_train_loss=0.0007310368090935123
Epoch 2160
Epoch 2160 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2160, epoch_train_loss=0.0007310368090935123
Epoch 2161
Epoch 2161 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2161, epoch_train_loss=0.0007310368090935123
Epoch 2162
Epoch 2162 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2162, epoch_train_loss=0.0007310368090935123
Epoch 2163
Epoch 2163 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2163, epoch_train_loss=0.0007310368090935123
Epoch 2164
Epoch 2164 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2164, epoch_train_loss=0.0007310368090935123
Epoch 2165
Epoch 2165 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2165, epoch_train_loss=0.0007310368090935123
Epoch 2166
Epoch 2166 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2166, epoch_train_loss=0.0007310368090935123
Epoch 2167
Epoch 2167 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2167, epoch_train_loss=0.0007310368090935123
Epoch 2168
Epoch 2168 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2168, epoch_train_loss=0.0007310368090935123
Epoch 2169
Epoch 2169 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2169, epoch_train_loss=0.0007310368090935123
Epoch 2170
Epoch 2170 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2170, epoch_train_loss=0.0007310368090935123
Epoch 2171
Epoch 2171 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2171, epoch_train_loss=0.0007310368090935123
Epoch 2172
Epoch 2172 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2172, epoch_train_loss=0.0007310368090935123
Epoch 2173
Epoch 2173 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2173, epoch_train_loss=0.0007310368090935123
Epoch 2174
Epoch 2174 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2174, epoch_train_loss=0.0007310368090935123
Epoch 2175
Epoch 2175 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2175, epoch_train_loss=0.0007310368090935123
Epoch 2176
Epoch 2176 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2176, epoch_train_loss=0.0007310368090935123
Epoch 2177
Epoch 2177 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2177, epoch_train_loss=0.0007310368090935123
Epoch 2178
Epoch 2178 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2178, epoch_train_loss=0.0007310368090935123
Epoch 2179
Epoch 2179 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2179, epoch_train_loss=0.0007310368090935123
Epoch 2180
Epoch 2180 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2180, epoch_train_loss=0.0007310368090935123
Epoch 2181
Epoch 2181 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2181, epoch_train_loss=0.0007310368090935123
Epoch 2182
Epoch 2182 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2182, epoch_train_loss=0.0007310368090935123
Epoch 2183
Epoch 2183 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2183, epoch_train_loss=0.0007310368090935123
Epoch 2184
Epoch 2184 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2184, epoch_train_loss=0.0007310368090935123
Epoch 2185
Epoch 2185 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2185, epoch_train_loss=0.0007310368090935123
Epoch 2186
Epoch 2186 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2186, epoch_train_loss=0.0007310368090935123
Epoch 2187
Epoch 2187 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2187, epoch_train_loss=0.0007310368090935123
Epoch 2188
Epoch 2188 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2188, epoch_train_loss=0.0007310368090935123
Epoch 2189
Epoch 2189 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2189, epoch_train_loss=0.0007310368090935123
Epoch 2190
Epoch 2190 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2190, epoch_train_loss=0.0007310368090935123
Epoch 2191
Epoch 2191 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2191, epoch_train_loss=0.0007310368090935123
Epoch 2192
Epoch 2192 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2192, epoch_train_loss=0.0007310368090935123
Epoch 2193
Epoch 2193 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2193, epoch_train_loss=0.0007310368090935123
Epoch 2194
Epoch 2194 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2194, epoch_train_loss=0.0007310368090935123
Epoch 2195
Epoch 2195 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2195, epoch_train_loss=0.0007310368090935123
Epoch 2196
Epoch 2196 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2196, epoch_train_loss=0.0007310368090935123
Epoch 2197
Epoch 2197 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2197, epoch_train_loss=0.0007310368090935123
Epoch 2198
Epoch 2198 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2198, epoch_train_loss=0.0007310368090935123
Epoch 2199
Epoch 2199 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2199, epoch_train_loss=0.0007310368090935123
Epoch 2200
Epoch 2200 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2200, epoch_train_loss=0.0007310368090935123
Epoch 2201
Epoch 2201 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2201, epoch_train_loss=0.0007310368090935123
Epoch 2202
Epoch 2202 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2202, epoch_train_loss=0.0007310368090935123
Epoch 2203
Epoch 2203 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2203, epoch_train_loss=0.0007310368090935123
Epoch 2204
Epoch 2204 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2204, epoch_train_loss=0.0007310368090935123
Epoch 2205
Epoch 2205 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2205, epoch_train_loss=0.0007310368090935123
Epoch 2206
Epoch 2206 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2206, epoch_train_loss=0.0007310368090935123
Epoch 2207
Epoch 2207 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2207, epoch_train_loss=0.0007310368090935123
Epoch 2208
Epoch 2208 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2208, epoch_train_loss=0.0007310368090935123
Epoch 2209
Epoch 2209 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2209, epoch_train_loss=0.0007310368090935123
Epoch 2210
Epoch 2210 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2210, epoch_train_loss=0.0007310368090935123
Epoch 2211
Epoch 2211 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2211, epoch_train_loss=0.0007310368090935123
Epoch 2212
Epoch 2212 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2212, epoch_train_loss=0.0007310368090935123
Epoch 2213
Epoch 2213 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2213, epoch_train_loss=0.0007310368090935123
Epoch 2214
Epoch 2214 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2214, epoch_train_loss=0.0007310368090935123
Epoch 2215
Epoch 2215 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2215, epoch_train_loss=0.0007310368090935123
Epoch 2216
Epoch 2216 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2216, epoch_train_loss=0.0007310368090935123
Epoch 2217
Epoch 2217 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2217, epoch_train_loss=0.0007310368090935123
Epoch 2218
Epoch 2218 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2218, epoch_train_loss=0.0007310368090935123
Epoch 2219
Epoch 2219 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2219, epoch_train_loss=0.0007310368090935123
Epoch 2220
Epoch 2220 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2220, epoch_train_loss=0.0007310368090935123
Epoch 2221
Epoch 2221 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2221, epoch_train_loss=0.0007310368090935123
Epoch 2222
Epoch 2222 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2222, epoch_train_loss=0.0007310368090935123
Epoch 2223
Epoch 2223 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2223, epoch_train_loss=0.0007310368090935123
Epoch 2224
Epoch 2224 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2224, epoch_train_loss=0.0007310368090935123
Epoch 2225
Epoch 2225 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2225, epoch_train_loss=0.0007310368090935123
Epoch 2226
Epoch 2226 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2226, epoch_train_loss=0.0007310368090935123
Epoch 2227
Epoch 2227 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2227, epoch_train_loss=0.0007310368090935123
Epoch 2228
Epoch 2228 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2228, epoch_train_loss=0.0007310368090935123
Epoch 2229
Epoch 2229 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2229, epoch_train_loss=0.0007310368090935123
Epoch 2230
Epoch 2230 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2230, epoch_train_loss=0.0007310368090935123
Epoch 2231
Epoch 2231 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2231, epoch_train_loss=0.0007310368090935123
Epoch 2232
Epoch 2232 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2232, epoch_train_loss=0.0007310368090935123
Epoch 2233
Epoch 2233 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2233, epoch_train_loss=0.0007310368090935123
Epoch 2234
Epoch 2234 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2234, epoch_train_loss=0.0007310368090935123
Epoch 2235
Epoch 2235 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2235, epoch_train_loss=0.0007310368090935123
Epoch 2236
Epoch 2236 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2236, epoch_train_loss=0.0007310368090935123
Epoch 2237
Epoch 2237 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2237, epoch_train_loss=0.0007310368090935123
Epoch 2238
Epoch 2238 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2238, epoch_train_loss=0.0007310368090935123
Epoch 2239
Epoch 2239 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2239, epoch_train_loss=0.0007310368090935123
Epoch 2240
Epoch 2240 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2240, epoch_train_loss=0.0007310368090935123
Epoch 2241
Epoch 2241 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2241, epoch_train_loss=0.0007310368090935123
Epoch 2242
Epoch 2242 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2242, epoch_train_loss=0.0007310368090935123
Epoch 2243
Epoch 2243 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2243, epoch_train_loss=0.0007310368090935123
Epoch 2244
Epoch 2244 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2244, epoch_train_loss=0.0007310368090935123
Epoch 2245
Epoch 2245 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2245, epoch_train_loss=0.0007310368090935123
Epoch 2246
Epoch 2246 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2246, epoch_train_loss=0.0007310368090935123
Epoch 2247
Epoch 2247 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2247, epoch_train_loss=0.0007310368090935123
Epoch 2248
Epoch 2248 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2248, epoch_train_loss=0.0007310368090935123
Epoch 2249
Epoch 2249 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2249, epoch_train_loss=0.0007310368090935123
Epoch 2250
Epoch 2250 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2250, epoch_train_loss=0.0007310368090935123
Epoch 2251
Epoch 2251 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2251, epoch_train_loss=0.0007310368090935123
Epoch 2252
Epoch 2252 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2252, epoch_train_loss=0.0007310368090935123
Epoch 2253
Epoch 2253 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2253, epoch_train_loss=0.0007310368090935123
Epoch 2254
Epoch 2254 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2254, epoch_train_loss=0.0007310368090935123
Epoch 2255
Epoch 2255 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2255, epoch_train_loss=0.0007310368090935123
Epoch 2256
Epoch 2256 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2256, epoch_train_loss=0.0007310368090935123
Epoch 2257
Epoch 2257 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2257, epoch_train_loss=0.0007310368090935123
Epoch 2258
Epoch 2258 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2258, epoch_train_loss=0.0007310368090935123
Epoch 2259
Epoch 2259 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2259, epoch_train_loss=0.0007310368090935123
Epoch 2260
Epoch 2260 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2260, epoch_train_loss=0.0007310368090935123
Epoch 2261
Epoch 2261 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2261, epoch_train_loss=0.0007310368090935123
Epoch 2262
Epoch 2262 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2262, epoch_train_loss=0.0007310368090935123
Epoch 2263
Epoch 2263 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2263, epoch_train_loss=0.0007310368090935123
Epoch 2264
Epoch 2264 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2264, epoch_train_loss=0.0007310368090935123
Epoch 2265
Epoch 2265 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2265, epoch_train_loss=0.0007310368090935123
Epoch 2266
Epoch 2266 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2266, epoch_train_loss=0.0007310368090935123
Epoch 2267
Epoch 2267 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2267, epoch_train_loss=0.0007310368090935123
Epoch 2268
Epoch 2268 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2268, epoch_train_loss=0.0007310368090935123
Epoch 2269
Epoch 2269 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2269, epoch_train_loss=0.0007310368090935123
Epoch 2270
Epoch 2270 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2270, epoch_train_loss=0.0007310368090935123
Epoch 2271
Epoch 2271 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2271, epoch_train_loss=0.0007310368090935123
Epoch 2272
Epoch 2272 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2272, epoch_train_loss=0.0007310368090935123
Epoch 2273
Epoch 2273 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2273, epoch_train_loss=0.0007310368090935123
Epoch 2274
Epoch 2274 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2274, epoch_train_loss=0.0007310368090935123
Epoch 2275
Epoch 2275 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2275, epoch_train_loss=0.0007310368090935123
Epoch 2276
Epoch 2276 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2276, epoch_train_loss=0.0007310368090935123
Epoch 2277
Epoch 2277 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2277, epoch_train_loss=0.0007310368090935123
Epoch 2278
Epoch 2278 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2278, epoch_train_loss=0.0007310368090935123
Epoch 2279
Epoch 2279 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2279, epoch_train_loss=0.0007310368090935123
Epoch 2280
Epoch 2280 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2280, epoch_train_loss=0.0007310368090935123
Epoch 2281
Epoch 2281 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2281, epoch_train_loss=0.0007310368090935123
Epoch 2282
Epoch 2282 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2282, epoch_train_loss=0.0007310368090935123
Epoch 2283
Epoch 2283 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2283, epoch_train_loss=0.0007310368090935123
Epoch 2284
Epoch 2284 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2284, epoch_train_loss=0.0007310368090935123
Epoch 2285
Epoch 2285 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2285, epoch_train_loss=0.0007310368090935123
Epoch 2286
Epoch 2286 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2286, epoch_train_loss=0.0007310368090935123
Epoch 2287
Epoch 2287 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2287, epoch_train_loss=0.0007310368090935123
Epoch 2288
Epoch 2288 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2288, epoch_train_loss=0.0007310368090935123
Epoch 2289
Epoch 2289 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2289, epoch_train_loss=0.0007310368090935123
Epoch 2290
Epoch 2290 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2290, epoch_train_loss=0.0007310368090935123
Epoch 2291
Epoch 2291 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2291, epoch_train_loss=0.0007310368090935123
Epoch 2292
Epoch 2292 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2292, epoch_train_loss=0.0007310368090935123
Epoch 2293
Epoch 2293 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2293, epoch_train_loss=0.0007310368090935123
Epoch 2294
Epoch 2294 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2294, epoch_train_loss=0.0007310368090935123
Epoch 2295
Epoch 2295 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2295, epoch_train_loss=0.0007310368090935123
Epoch 2296
Epoch 2296 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2296, epoch_train_loss=0.0007310368090935123
Epoch 2297
Epoch 2297 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2297, epoch_train_loss=0.0007310368090935123
Epoch 2298
Epoch 2298 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2298, epoch_train_loss=0.0007310368090935123
Epoch 2299
Epoch 2299 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2299, epoch_train_loss=0.0007310368090935123
Epoch 2300
Epoch 2300 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2300, epoch_train_loss=0.0007310368090935123
Epoch 2301
Epoch 2301 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2301, epoch_train_loss=0.0007310368090935123
Epoch 2302
Epoch 2302 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2302, epoch_train_loss=0.0007310368090935123
Epoch 2303
Epoch 2303 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2303, epoch_train_loss=0.0007310368090935123
Epoch 2304
Epoch 2304 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2304, epoch_train_loss=0.0007310368090935123
Epoch 2305
Epoch 2305 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2305, epoch_train_loss=0.0007310368090935123
Epoch 2306
Epoch 2306 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2306, epoch_train_loss=0.0007310368090935123
Epoch 2307
Epoch 2307 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2307, epoch_train_loss=0.0007310368090935123
Epoch 2308
Epoch 2308 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2308, epoch_train_loss=0.0007310368090935123
Epoch 2309
Epoch 2309 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2309, epoch_train_loss=0.0007310368090935123
Epoch 2310
Epoch 2310 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2310, epoch_train_loss=0.0007310368090935123
Epoch 2311
Epoch 2311 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2311, epoch_train_loss=0.0007310368090935123
Epoch 2312
Epoch 2312 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2312, epoch_train_loss=0.0007310368090935123
Epoch 2313
Epoch 2313 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2313, epoch_train_loss=0.0007310368090935123
Epoch 2314
Epoch 2314 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2314, epoch_train_loss=0.0007310368090935123
Epoch 2315
Epoch 2315 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2315, epoch_train_loss=0.0007310368090935123
Epoch 2316
Epoch 2316 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2316, epoch_train_loss=0.0007310368090935123
Epoch 2317
Epoch 2317 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2317, epoch_train_loss=0.0007310368090935123
Epoch 2318
Epoch 2318 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2318, epoch_train_loss=0.0007310368090935123
Epoch 2319
Epoch 2319 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2319, epoch_train_loss=0.0007310368090935123
Epoch 2320
Epoch 2320 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2320, epoch_train_loss=0.0007310368090935123
Epoch 2321
Epoch 2321 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2321, epoch_train_loss=0.0007310368090935123
Epoch 2322
Epoch 2322 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2322, epoch_train_loss=0.0007310368090935123
Epoch 2323
Epoch 2323 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2323, epoch_train_loss=0.0007310368090935123
Epoch 2324
Epoch 2324 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2324, epoch_train_loss=0.0007310368090935123
Epoch 2325
Epoch 2325 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2325, epoch_train_loss=0.0007310368090935123
Epoch 2326
Epoch 2326 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2326, epoch_train_loss=0.0007310368090935123
Epoch 2327
Epoch 2327 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2327, epoch_train_loss=0.0007310368090935123
Epoch 2328
Epoch 2328 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2328, epoch_train_loss=0.0007310368090935123
Epoch 2329
Epoch 2329 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2329, epoch_train_loss=0.0007310368090935123
Epoch 2330
Epoch 2330 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2330, epoch_train_loss=0.0007310368090935123
Epoch 2331
Epoch 2331 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2331, epoch_train_loss=0.0007310368090935123
Epoch 2332
Epoch 2332 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2332, epoch_train_loss=0.0007310368090935123
Epoch 2333
Epoch 2333 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2333, epoch_train_loss=0.0007310368090935123
Epoch 2334
Epoch 2334 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2334, epoch_train_loss=0.0007310368090935123
Epoch 2335
Epoch 2335 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2335, epoch_train_loss=0.0007310368090935123
Epoch 2336
Epoch 2336 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2336, epoch_train_loss=0.0007310368090935123
Epoch 2337
Epoch 2337 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2337, epoch_train_loss=0.0007310368090935123
Epoch 2338
Epoch 2338 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2338, epoch_train_loss=0.0007310368090935123
Epoch 2339
Epoch 2339 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2339, epoch_train_loss=0.0007310368090935123
Epoch 2340
Epoch 2340 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2340, epoch_train_loss=0.0007310368090935123
Epoch 2341
Epoch 2341 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2341, epoch_train_loss=0.0007310368090935123
Epoch 2342
Epoch 2342 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2342, epoch_train_loss=0.0007310368090935123
Epoch 2343
Epoch 2343 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2343, epoch_train_loss=0.0007310368090935123
Epoch 2344
Epoch 2344 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2344, epoch_train_loss=0.0007310368090935123
Epoch 2345
Epoch 2345 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2345, epoch_train_loss=0.0007310368090935123
Epoch 2346
Epoch 2346 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2346, epoch_train_loss=0.0007310368090935123
Epoch 2347
Epoch 2347 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2347, epoch_train_loss=0.0007310368090935123
Epoch 2348
Epoch 2348 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2348, epoch_train_loss=0.0007310368090935123
Epoch 2349
Epoch 2349 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2349, epoch_train_loss=0.0007310368090935123
Epoch 2350
Epoch 2350 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2350, epoch_train_loss=0.0007310368090935123
Epoch 2351
Epoch 2351 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2351, epoch_train_loss=0.0007310368090935123
Epoch 2352
Epoch 2352 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2352, epoch_train_loss=0.0007310368090935123
Epoch 2353
Epoch 2353 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2353, epoch_train_loss=0.0007310368090935123
Epoch 2354
Epoch 2354 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2354, epoch_train_loss=0.0007310368090935123
Epoch 2355
Epoch 2355 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2355, epoch_train_loss=0.0007310368090935123
Epoch 2356
Epoch 2356 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2356, epoch_train_loss=0.0007310368090935123
Epoch 2357
Epoch 2357 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2357, epoch_train_loss=0.0007310368090935123
Epoch 2358
Epoch 2358 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2358, epoch_train_loss=0.0007310368090935123
Epoch 2359
Epoch 2359 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2359, epoch_train_loss=0.0007310368090935123
Epoch 2360
Epoch 2360 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2360, epoch_train_loss=0.0007310368090935123
Epoch 2361
Epoch 2361 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2361, epoch_train_loss=0.0007310368090935123
Epoch 2362
Epoch 2362 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2362, epoch_train_loss=0.0007310368090935123
Epoch 2363
Epoch 2363 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2363, epoch_train_loss=0.0007310368090935123
Epoch 2364
Epoch 2364 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2364, epoch_train_loss=0.0007310368090935123
Epoch 2365
Epoch 2365 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2365, epoch_train_loss=0.0007310368090935123
Epoch 2366
Epoch 2366 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2366, epoch_train_loss=0.0007310368090935123
Epoch 2367
Epoch 2367 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2367, epoch_train_loss=0.0007310368090935123
Epoch 2368
Epoch 2368 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2368, epoch_train_loss=0.0007310368090935123
Epoch 2369
Epoch 2369 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2369, epoch_train_loss=0.0007310368090935123
Epoch 2370
Epoch 2370 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2370, epoch_train_loss=0.0007310368090935123
Epoch 2371
Epoch 2371 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2371, epoch_train_loss=0.0007310368090935123
Epoch 2372
Epoch 2372 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2372, epoch_train_loss=0.0007310368090935123
Epoch 2373
Epoch 2373 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2373, epoch_train_loss=0.0007310368090935123
Epoch 2374
Epoch 2374 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2374, epoch_train_loss=0.0007310368090935123
Epoch 2375
Epoch 2375 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2375, epoch_train_loss=0.0007310368090935123
Epoch 2376
Epoch 2376 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2376, epoch_train_loss=0.0007310368090935123
Epoch 2377
Epoch 2377 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2377, epoch_train_loss=0.0007310368090935123
Epoch 2378
Epoch 2378 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2378, epoch_train_loss=0.0007310368090935123
Epoch 2379
Epoch 2379 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2379, epoch_train_loss=0.0007310368090935123
Epoch 2380
Epoch 2380 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2380, epoch_train_loss=0.0007310368090935123
Epoch 2381
Epoch 2381 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2381, epoch_train_loss=0.0007310368090935123
Epoch 2382
Epoch 2382 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2382, epoch_train_loss=0.0007310368090935123
Epoch 2383
Epoch 2383 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2383, epoch_train_loss=0.0007310368090935123
Epoch 2384
Epoch 2384 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2384, epoch_train_loss=0.0007310368090935123
Epoch 2385
Epoch 2385 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2385, epoch_train_loss=0.0007310368090935123
Epoch 2386
Epoch 2386 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2386, epoch_train_loss=0.0007310368090935123
Epoch 2387
Epoch 2387 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2387, epoch_train_loss=0.0007310368090935123
Epoch 2388
Epoch 2388 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2388, epoch_train_loss=0.0007310368090935123
Epoch 2389
Epoch 2389 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2389, epoch_train_loss=0.0007310368090935123
Epoch 2390
Epoch 2390 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2390, epoch_train_loss=0.0007310368090935123
Epoch 2391
Epoch 2391 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2391, epoch_train_loss=0.0007310368090935123
Epoch 2392
Epoch 2392 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2392, epoch_train_loss=0.0007310368090935123
Epoch 2393
Epoch 2393 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2393, epoch_train_loss=0.0007310368090935123
Epoch 2394
Epoch 2394 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2394, epoch_train_loss=0.0007310368090935123
Epoch 2395
Epoch 2395 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2395, epoch_train_loss=0.0007310368090935123
Epoch 2396
Epoch 2396 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2396, epoch_train_loss=0.0007310368090935123
Epoch 2397
Epoch 2397 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2397, epoch_train_loss=0.0007310368090935123
Epoch 2398
Epoch 2398 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2398, epoch_train_loss=0.0007310368090935123
Epoch 2399
Epoch 2399 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2399, epoch_train_loss=0.0007310368090935123
Epoch 2400
Epoch 2400 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2400, epoch_train_loss=0.0007310368090935123
Epoch 2401
Epoch 2401 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2401, epoch_train_loss=0.0007310368090935123
Epoch 2402
Epoch 2402 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2402, epoch_train_loss=0.0007310368090935123
Epoch 2403
Epoch 2403 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2403, epoch_train_loss=0.0007310368090935123
Epoch 2404
Epoch 2404 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2404, epoch_train_loss=0.0007310368090935123
Epoch 2405
Epoch 2405 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2405, epoch_train_loss=0.0007310368090935123
Epoch 2406
Epoch 2406 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2406, epoch_train_loss=0.0007310368090935123
Epoch 2407
Epoch 2407 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2407, epoch_train_loss=0.0007310368090935123
Epoch 2408
Epoch 2408 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2408, epoch_train_loss=0.0007310368090935123
Epoch 2409
Epoch 2409 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2409, epoch_train_loss=0.0007310368090935123
Epoch 2410
Epoch 2410 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2410, epoch_train_loss=0.0007310368090935123
Epoch 2411
Epoch 2411 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2411, epoch_train_loss=0.0007310368090935123
Epoch 2412
Epoch 2412 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2412, epoch_train_loss=0.0007310368090935123
Epoch 2413
Epoch 2413 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2413, epoch_train_loss=0.0007310368090935123
Epoch 2414
Epoch 2414 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2414, epoch_train_loss=0.0007310368090935123
Epoch 2415
Epoch 2415 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2415, epoch_train_loss=0.0007310368090935123
Epoch 2416
Epoch 2416 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2416, epoch_train_loss=0.0007310368090935123
Epoch 2417
Epoch 2417 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2417, epoch_train_loss=0.0007310368090935123
Epoch 2418
Epoch 2418 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2418, epoch_train_loss=0.0007310368090935123
Epoch 2419
Epoch 2419 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2419, epoch_train_loss=0.0007310368090935123
Epoch 2420
Epoch 2420 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2420, epoch_train_loss=0.0007310368090935123
Epoch 2421
Epoch 2421 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2421, epoch_train_loss=0.0007310368090935123
Epoch 2422
Epoch 2422 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2422, epoch_train_loss=0.0007310368090935123
Epoch 2423
Epoch 2423 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2423, epoch_train_loss=0.0007310368090935123
Epoch 2424
Epoch 2424 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2424, epoch_train_loss=0.0007310368090935123
Epoch 2425
Epoch 2425 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2425, epoch_train_loss=0.0007310368090935123
Epoch 2426
Epoch 2426 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2426, epoch_train_loss=0.0007310368090935123
Epoch 2427
Epoch 2427 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2427, epoch_train_loss=0.0007310368090935123
Epoch 2428
Epoch 2428 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2428, epoch_train_loss=0.0007310368090935123
Epoch 2429
Epoch 2429 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2429, epoch_train_loss=0.0007310368090935123
Epoch 2430
Epoch 2430 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2430, epoch_train_loss=0.0007310368090935123
Epoch 2431
Epoch 2431 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2431, epoch_train_loss=0.0007310368090935123
Epoch 2432
Epoch 2432 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2432, epoch_train_loss=0.0007310368090935123
Epoch 2433
Epoch 2433 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2433, epoch_train_loss=0.0007310368090935123
Epoch 2434
Epoch 2434 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2434, epoch_train_loss=0.0007310368090935123
Epoch 2435
Epoch 2435 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2435, epoch_train_loss=0.0007310368090935123
Epoch 2436
Epoch 2436 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2436, epoch_train_loss=0.0007310368090935123
Epoch 2437
Epoch 2437 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2437, epoch_train_loss=0.0007310368090935123
Epoch 2438
Epoch 2438 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2438, epoch_train_loss=0.0007310368090935123
Epoch 2439
Epoch 2439 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2439, epoch_train_loss=0.0007310368090935123
Epoch 2440
Epoch 2440 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2440, epoch_train_loss=0.0007310368090935123
Epoch 2441
Epoch 2441 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2441, epoch_train_loss=0.0007310368090935123
Epoch 2442
Epoch 2442 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2442, epoch_train_loss=0.0007310368090935123
Epoch 2443
Epoch 2443 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2443, epoch_train_loss=0.0007310368090935123
Epoch 2444
Epoch 2444 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2444, epoch_train_loss=0.0007310368090935123
Epoch 2445
Epoch 2445 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2445, epoch_train_loss=0.0007310368090935123
Epoch 2446
Epoch 2446 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2446, epoch_train_loss=0.0007310368090935123
Epoch 2447
Epoch 2447 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2447, epoch_train_loss=0.0007310368090935123
Epoch 2448
Epoch 2448 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2448, epoch_train_loss=0.0007310368090935123
Epoch 2449
Epoch 2449 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2449, epoch_train_loss=0.0007310368090935123
Epoch 2450
Epoch 2450 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2450, epoch_train_loss=0.0007310368090935123
Epoch 2451
Epoch 2451 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2451, epoch_train_loss=0.0007310368090935123
Epoch 2452
Epoch 2452 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2452, epoch_train_loss=0.0007310368090935123
Epoch 2453
Epoch 2453 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2453, epoch_train_loss=0.0007310368090935123
Epoch 2454
Epoch 2454 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2454, epoch_train_loss=0.0007310368090935123
Epoch 2455
Epoch 2455 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2455, epoch_train_loss=0.0007310368090935123
Epoch 2456
Epoch 2456 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2456, epoch_train_loss=0.0007310368090935123
Epoch 2457
Epoch 2457 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2457, epoch_train_loss=0.0007310368090935123
Epoch 2458
Epoch 2458 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2458, epoch_train_loss=0.0007310368090935123
Epoch 2459
Epoch 2459 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2459, epoch_train_loss=0.0007310368090935123
Epoch 2460
Epoch 2460 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2460, epoch_train_loss=0.0007310368090935123
Epoch 2461
Epoch 2461 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2461, epoch_train_loss=0.0007310368090935123
Epoch 2462
Epoch 2462 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2462, epoch_train_loss=0.0007310368090935123
Epoch 2463
Epoch 2463 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2463, epoch_train_loss=0.0007310368090935123
Epoch 2464
Epoch 2464 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2464, epoch_train_loss=0.0007310368090935123
Epoch 2465
Epoch 2465 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2465, epoch_train_loss=0.0007310368090935123
Epoch 2466
Epoch 2466 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2466, epoch_train_loss=0.0007310368090935123
Epoch 2467
Epoch 2467 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2467, epoch_train_loss=0.0007310368090935123
Epoch 2468
Epoch 2468 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2468, epoch_train_loss=0.0007310368090935123
Epoch 2469
Epoch 2469 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2469, epoch_train_loss=0.0007310368090935123
Epoch 2470
Epoch 2470 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2470, epoch_train_loss=0.0007310368090935123
Epoch 2471
Epoch 2471 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2471, epoch_train_loss=0.0007310368090935123
Epoch 2472
Epoch 2472 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2472, epoch_train_loss=0.0007310368090935123
Epoch 2473
Epoch 2473 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2473, epoch_train_loss=0.0007310368090935123
Epoch 2474
Epoch 2474 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2474, epoch_train_loss=0.0007310368090935123
Epoch 2475
Epoch 2475 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2475, epoch_train_loss=0.0007310368090935123
Epoch 2476
Epoch 2476 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2476, epoch_train_loss=0.0007310368090935123
Epoch 2477
Epoch 2477 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2477, epoch_train_loss=0.0007310368090935123
Epoch 2478
Epoch 2478 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2478, epoch_train_loss=0.0007310368090935123
Epoch 2479
Epoch 2479 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2479, epoch_train_loss=0.0007310368090935123
Epoch 2480
Epoch 2480 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2480, epoch_train_loss=0.0007310368090935123
Epoch 2481
Epoch 2481 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2481, epoch_train_loss=0.0007310368090935123
Epoch 2482
Epoch 2482 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2482, epoch_train_loss=0.0007310368090935123
Epoch 2483
Epoch 2483 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2483, epoch_train_loss=0.0007310368090935123
Epoch 2484
Epoch 2484 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2484, epoch_train_loss=0.0007310368090935123
Epoch 2485
Epoch 2485 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2485, epoch_train_loss=0.0007310368090935123
Epoch 2486
Epoch 2486 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2486, epoch_train_loss=0.0007310368090935123
Epoch 2487
Epoch 2487 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2487, epoch_train_loss=0.0007310368090935123
Epoch 2488
Epoch 2488 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2488, epoch_train_loss=0.0007310368090935123
Epoch 2489
Epoch 2489 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2489, epoch_train_loss=0.0007310368090935123
Epoch 2490
Epoch 2490 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2490, epoch_train_loss=0.0007310368090935123
Epoch 2491
Epoch 2491 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2491, epoch_train_loss=0.0007310368090935123
Epoch 2492
Epoch 2492 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2492, epoch_train_loss=0.0007310368090935123
Epoch 2493
Epoch 2493 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2493, epoch_train_loss=0.0007310368090935123
Epoch 2494
Epoch 2494 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2494, epoch_train_loss=0.0007310368090935123
Epoch 2495
Epoch 2495 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2495, epoch_train_loss=0.0007310368090935123
Epoch 2496
Epoch 2496 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2496, epoch_train_loss=0.0007310368090935123
Epoch 2497
Epoch 2497 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2497, epoch_train_loss=0.0007310368090935123
Epoch 2498
Epoch 2498 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2498, epoch_train_loss=0.0007310368090935123
Epoch 2499
Epoch 2499 :: Batch 0/1
Batch Loss = 0.0007310368090935123
2499, epoch_train_loss=0.0007310368090935123
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting "B3LYP_WITH_VWN5 = True" in pyscf_conf.py
  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac136890> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac136890> in UKS object of <class 'pyscf.dft.uks.UKS'>
WARNING: External module "mldftdat" required for non-local descriptor use.
<pyscf.gto.mole.Mole object at 0x7ffeac136890> [['P', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac137250> [['N', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac134130> [['H', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac1354b0> [['Li', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac135120> [['O', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac137220> [['Cl', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac136290> [['Al', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac137550> [['S', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac1354e0> [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac137b80> [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac137be0> [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac308250> [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeac30a0b0> [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeac30a4a0> [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac309450> [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac30bbb0> [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac3086a0> [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeac30bd30> [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac30b220> [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac308fd0> [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac30a620> [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac30b8e0> [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeac30b010> [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeac309570> [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac30a590> [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]] 5
<pyscf.gto.mole.Mole object at 0x7ffeac25bf40> [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeac259510> [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]] 5
mol:  [['P', array([0., 0., 0.])]]
converged SCF energy = -341.104145992717  <S^2> = 3.7502984  2S+1 = 4.0001492
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac137250> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac137250> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00051022 -0.00019156 -0.00051334 ... -0.02830887 -0.02830887
 -0.02830887] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 16)
mol:  [['N', array([0., 0., 0.])]]
converged SCF energy = -54.5289742046675  <S^2> = 3.7524945  2S+1 = 4.0012471
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac134130> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac134130> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5016, 22)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-3.60081838e-04 -1.08775305e-04 -1.31917160e-05 ... -2.74817476e-02
 -2.74817476e-02 -2.74817476e-02] = ,SCAN
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5016, 22)
rho shape (4, 4, 2, 5016)
rho_filt shape: (5016,)
get descriptors tdrho.shape=(5016, 16)
mol:  [['H', array([0., 0., 0.])]]
converged SCF energy = -0.499812984008539  <S^2> = 0.75  2S+1 = 2
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1354b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1354b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 7, 7)
ao.shape (10, 2440, 7)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.92637327e-09 -1.31700807e-07 -9.61527370e-06 ... -7.49400542e-16
 -7.49400542e-16 -7.49400542e-16] = ,SCAN
get_data, dm shape = (2, 7, 7)
ao_eval.shape=(4, 2440, 7)
rho shape (4, 4, 2, 2440)
rho_filt shape: (2440,)
get descriptors tdrho.shape=(2440, 16)
mol:  [['Li', array([0., 0., 0.])]]
converged SCF energy = -7.46006188627842  <S^2> = 0.75000049  2S+1 = 2.0000005
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac135120> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac135120> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 4592, 22)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-5.31884219e-04 -2.81911891e-04 -2.81911891e-04 ... -1.27154711e-05
 -2.64861768e-02 -2.64861768e-02] = ,SCAN
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 4592, 22)
rho shape (4, 4, 2, 4592)
rho_filt shape: (4592,)
get descriptors tdrho.shape=(4592, 16)
mol:  [['O', array([0., 0., 0.])]]
converged SCF energy = -75.003380291085  <S^2> = 2.0027445  2S+1 = 3.0018291
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac137220> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac137220> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5040, 22)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.73037378e-04 -3.17612430e-05 -1.67795455e-06 ... -2.76158576e-02
 -2.76158576e-02 -2.76158576e-02] = ,SCAN
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5040, 22)
rho shape (4, 4, 2, 5040)
rho_filt shape: (5040,)
get descriptors tdrho.shape=(5040, 16)
mol:  [['Cl', array([0., 0., 0.])]]
converged SCF energy = -459.957577120848  <S^2> = 0.75161941  2S+1 = 2.0016188
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac136290> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac136290> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6152, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.84101467e-04 -2.45637489e-04 -8.34007605e-05 ... -2.84484389e-02
 -2.84484389e-02 -2.84484389e-02] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6152, 30)
rho shape (4, 4, 2, 6152)
rho_filt shape: (6152,)
get descriptors tdrho.shape=(6152, 16)
mol:  [['Al', array([0., 0., 0.])]]
converged SCF energy = -242.226560968318  <S^2> = 0.75226418  2S+1 = 2.0022629
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac137550> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac137550> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6088, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.89024082e-03 -1.17097558e-03 -5.99252442e-04 ... -1.57580551e-05
 -7.06201323e-05 -7.69537216e-06] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6088, 30)
rho shape (4, 4, 2, 6088)
rho_filt shape: (6088,)
get descriptors tdrho.shape=(6088, 16)
mol:  [['S', array([0., 0., 0.])]]
converged SCF energy = -397.93878682149  <S^2> = 2.0022329  2S+1 = 3.0014882
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1354e0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1354e0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00025812 -0.00019313 -0.00020203 ... -0.02838402 -0.02838402
 -0.02838402] = ,SCAN
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 16)
mol:  [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]]
converged SCF energy = -1.16580491182912  <S^2> = -4.4408921e-16  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac137b80> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac137b80> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 14, 14)
ao.shape (10, 4776, 14)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.43725660e-05 -1.02204687e-06 -4.05575842e-05 ... -2.36278434e-02
 -2.36278434e-02 -2.36278434e-02] = ,SCAN
get_data, dm shape = (2, 14, 14)
ao_eval.shape=(4, 4776, 14)
rho shape (4, 4, 2, 4776)
rho_filt shape: (4776,)
get descriptors tdrho.shape=(4776, 16)
mol:  [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]]
converged SCF energy = -109.439263799566  <S^2> = 7.1054274e-15  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac137be0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac137be0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9848, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.89629699e-05 -2.76172354e-04 -7.59017288e-05 ... -7.34654212e-06
 -7.34654212e-06 -2.89629699e-05] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9848, 44)
rho shape (4, 4, 2, 9848)
rho_filt shape: (9848,)
get descriptors tdrho.shape=(9848, 16)
mol:  [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]]
converged SCF energy = -107.339357395734  <S^2> = -1.7763568e-15  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac308250> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac308250> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9752, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00043469 -0.00024024 -0.00035532 ... -0.00047537 -0.03728133
 -0.03728133] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9752, 44)
rho shape (4, 4, 2, 9752)
rho_filt shape: (9752,)
get descriptors tdrho.shape=(9752, 16)
mol:  [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]]
converged SCF energy = -93.3377924465129  <S^2> = 4.0073189e-10  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac30a0b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac30a0b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 12256, 51)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-9.02468888e-05 -7.92694658e-06 -9.80568469e-06 ... -4.33714150e-02
 -4.33714150e-02 -4.33714150e-02] = ,SCAN
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 12256, 51)
rho shape (4, 4, 2, 12256)
rho_filt shape: (12256,)
get descriptors tdrho.shape=(12256, 16)
mol:  [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]]
converged SCF energy = -188.456965322843  <S^2> = 1.0658141e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac30a4a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac30a4a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 66, 66)
ao.shape (10, 14920, 66)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-7.48187338e-05 -6.19475249e-05 -2.61742784e-04 ... -8.70042314e-07
 -2.73391097e-02 -2.73391097e-02] = ,SCAN
get_data, dm shape = (2, 66, 66)
ao_eval.shape=(4, 14920, 66)
rho shape (4, 4, 2, 14920)
rho_filt shape: (14920,)
get descriptors tdrho.shape=(14920, 16)
mol:  [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]]
converged SCF energy = -920.005608888959  <S^2> = 5.0448534e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac309450> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac309450> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12208, 60)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00051559 -0.00027432 -0.00088583 ... -0.00027432 -0.04174728
 -0.04174728] = ,SCAN
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12208, 60)
rho shape (4, 4, 2, 12208)
rho_filt shape: (12208,)
get descriptors tdrho.shape=(12208, 16)
mol:  [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]]
converged SCF energy = -199.394370591172  <S^2> = 1.2256862e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac30bbb0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac30bbb0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9824, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-5.53951178e-05 -5.93507199e-06 -3.10072916e-04 ... -5.94325581e-02
 -5.94325581e-02 -5.94325581e-02] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9824, 44)
rho shape (4, 4, 2, 9824)
rho_filt shape: (9824,)
get descriptors tdrho.shape=(9824, 16)
mol:  [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]]
converged SCF energy = -150.214894382029  <S^2> = 1.0018598  2S+1 = 2.2377308
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac3086a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac3086a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9912, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-1.51888662e-04 -2.87671708e-05 -1.57743123e-06 ... -4.22396684e-02
 -4.22396684e-02 -4.22396684e-02] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9912, 44)
rho shape (4, 4, 2, 9912)
rho_filt shape: (9912,)
get descriptors tdrho.shape=(9912, 16)
mol:  [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]]
converged SCF energy = -77.2435048346375  <S^2> = 5.3290705e-15  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac30bd30> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac30bd30> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 58, 58)
ao.shape (10, 15208, 58)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.72190712e-05 -2.72190712e-05 -2.84904833e-04 ... -1.08108260e-05
 -1.03072478e-05 -1.03072478e-05] = ,SCAN
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15208, 58)
rho shape (4, 4, 2, 15208)
rho_filt shape: (15208,)
get descriptors tdrho.shape=(15208, 16)
mol:  [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]]
converged SCF energy = -113.221335689652  <S^2> = 6.6435746e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac30b220> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac30b220> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 10040, 44)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00015688 -0.00024669 -0.00068269 ... -0.03791166 -0.03791166
 -0.03791166] = ,SCAN
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 10040, 44)
rho shape (4, 4, 2, 10040)
rho_filt shape: (10040,)
get descriptors tdrho.shape=(10040, 16)
mol:  [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]]
converged SCF energy = -460.624592374078  <S^2> = 6.5725203e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac308fd0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac308fd0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 37, 37)
ao.shape (10, 8552, 37)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-7.28500681e-05 -5.65091132e-06 -7.37932132e-06 ... -4.76689214e-02
 -4.76689214e-02 -4.76689214e-02] = ,SCAN
get_data, dm shape = (2, 37, 37)
ao_eval.shape=(4, 8552, 37)
rho shape (4, 4, 2, 8552)
rho_filt shape: (8552,)
get descriptors tdrho.shape=(8552, 16)
mol:  [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]]
converged SCF energy = -8.04458854018922  <S^2> = 7.4384943e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac30a620> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac30a620> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 29, 29)
ao.shape (10, 6936, 29)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.0003863  -0.00040095 -0.00040095 ... -0.0213199  -0.0213199
 -0.0213199 ] = ,SCAN
get_data, dm shape = (2, 29, 29)
ao_eval.shape=(4, 6936, 29)
rho shape (4, 4, 2, 6936)
rho_filt shape: (6936,)
get descriptors tdrho.shape=(6936, 16)
mol:  [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]]
converged SCF energy = -324.340512506578  <S^2> = 1.5864643e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac30b8e0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac30b8e0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 11536, 60)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-0.00088473 -0.00088473 -0.00116894 ... -0.00088473 -0.00088473
 -0.00116894] = ,SCAN
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 11536, 60)
rho shape (4, 4, 2, 11536)
rho_filt shape: (11536,)
get descriptors tdrho.shape=(11536, 16)
mol:  [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]]
converged SCF energy = -1622.57507845814  <S^2> = 8.31335e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac30b010> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac30b010> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 120, 120)
ao.shape (10, 24512, 120)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.91408540e-05 -1.46971271e-04 -1.08734417e-03 ... -2.81566369e-02
 -2.81566369e-02 -2.81566369e-02] = ,SCAN
get_data, dm shape = (2, 120, 120)
ao_eval.shape=(4, 24512, 120)
rho shape (4, 4, 2, 24512)
rho_filt shape: (24512,)
get descriptors tdrho.shape=(24512, 16)
mol:  [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]]
converged SCF energy = -342.979728469576  <S^2> = 2.5393021e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac309570> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac309570> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 13096, 51)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.39373335e-04 -1.31641332e-04 -1.15950750e-05 ... -7.32416564e-02
 -7.32416564e-02 -7.32416564e-02] = ,SCAN
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 13096, 51)
rho shape (4, 4, 2, 13096)
rho_filt shape: (13096,)
get descriptors tdrho.shape=(13096, 16)
mol:  [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]]
converged SCF energy = -578.565336663474  <S^2> = 1.0034707  2S+1 = 2.2391701
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac30a590> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac30a590> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12384, 60)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-4.84695804e-05 -7.80542058e-05 -7.80552512e-05 ... -2.92531580e-02
 -2.92531580e-02 -2.92531580e-02] = ,SCAN
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12384, 60)
rho shape (4, 4, 2, 12384)
rho_filt shape: (12384,)
get descriptors tdrho.shape=(12384, 16)
mol:  [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]]
converged SCF energy = -40.4598214864077  <S^2> = 3.1530334e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac25bf40> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac25bf40> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 50, 50)
ao.shape (10, 13936, 50)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.56165538e-04 -7.34744214e-05 -5.30574304e-06 ... -7.93995702e-06
 -7.93995702e-06 -7.93995702e-06] = ,SCAN
get_data, dm shape = (2, 50, 50)
ao_eval.shape=(4, 13936, 50)
rho shape (4, 4, 2, 13936)
rho_filt shape: (13936,)
get descriptors tdrho.shape=(13936, 16)
mol:  [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]]
converged SCF energy = -39.0756147483504  <S^2> = 6.2021499e-12  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac259510> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac259510> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 36, 36)
ao.shape (10, 9656, 36)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.28290753e-04 -1.41305449e-05 -6.13700492e-05 ... -2.47993463e-02
 -2.47993463e-02 -2.47993463e-02] = ,SCAN
get_data, dm shape = (2, 36, 36)
ao_eval.shape=(4, 9656, 36)
rho shape (4, 4, 2, 9656)
rho_filt shape: (9656,)
get descriptors tdrho.shape=(9656, 16)
mol:  [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]]
converged SCF energy = -291.719272437819  <S^2> = 1.3148593e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
New DM shape: (2, 58, 58)
ao.shape (10, 15256, 58)
Correlation contribution only
,SCAN
no spin scaling
exc with xc_func = [-2.45512011e-04 -7.12775692e-05 -5.48666345e-06 ... -6.02613084e-06
 -6.02613084e-06 -6.02613084e-06] = ,SCAN
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15256, 58)
rho shape (4, 4, 2, 15256)
rho_filt shape: (15256,)
get descriptors tdrho.shape=(15256, 16)
PRE NAN FILT: tFxc.shape=(237021,), tdrho.shape=(237021, 16)
nan_filt_rho.shape=(237021,)
nan_filt_fxc.shape=(237021,)
tFxc.shape=(237021,), tdrho.shape=(237021, 16)
inp[0].shape = (237021, 16)
Epoch 0
Epoch 0 :: Batch 0/1
Batch Loss = 8137804.412064842
0, epoch_train_loss=8137804.412064842
Epoch 1
Epoch 1 :: Batch 0/1
Batch Loss = 0.2580283954445288
1, epoch_train_loss=0.2580283954445288
Epoch 2
Epoch 2 :: Batch 0/1
Batch Loss = 0.20316855983313084
2, epoch_train_loss=0.20316855983313084
Epoch 3
Epoch 3 :: Batch 0/1
Batch Loss = 0.1168131062490871
3, epoch_train_loss=0.1168131062490871
Epoch 4
Epoch 4 :: Batch 0/1
Batch Loss = 0.05618124029315146
4, epoch_train_loss=0.05618124029315146
Epoch 5
Epoch 5 :: Batch 0/1
Batch Loss = 0.0277108422595207
5, epoch_train_loss=0.0277108422595207
Epoch 6
Epoch 6 :: Batch 0/1
Batch Loss = 0.013758165592447696
6, epoch_train_loss=0.013758165592447696
Epoch 7
Epoch 7 :: Batch 0/1
Batch Loss = 0.006500298671379788
7, epoch_train_loss=0.006500298671379788
Epoch 8
Epoch 8 :: Batch 0/1
Batch Loss = 0.0030248321589408304
8, epoch_train_loss=0.0030248321589408304
Epoch 9
Epoch 9 :: Batch 0/1
Batch Loss = 0.001569177947882609
9, epoch_train_loss=0.001569177947882609
Epoch 10
Epoch 10 :: Batch 0/1
Batch Loss = 0.0010257514561957883
10, epoch_train_loss=0.0010257514561957883
Epoch 11
Epoch 11 :: Batch 0/1
Batch Loss = 0.0008352921981583402
11, epoch_train_loss=0.0008352921981583402
Epoch 12
Epoch 12 :: Batch 0/1
Batch Loss = 0.0007689626089731433
12, epoch_train_loss=0.0007689626089731433
Epoch 13
Epoch 13 :: Batch 0/1
Batch Loss = 0.000745236775812853
13, epoch_train_loss=0.000745236775812853
Epoch 14
Epoch 14 :: Batch 0/1
Batch Loss = 0.0007364507776535054
14, epoch_train_loss=0.0007364507776535054
Epoch 15
Epoch 15 :: Batch 0/1
Batch Loss = 0.000733099735352235
15, epoch_train_loss=0.000733099735352235
Epoch 16
Epoch 16 :: Batch 0/1
Batch Loss = 0.0007317921708400761
16, epoch_train_loss=0.0007317921708400761
Epoch 17
Epoch 17 :: Batch 0/1
Batch Loss = 0.0007312716671799884
17, epoch_train_loss=0.0007312716671799884
Epoch 18
Epoch 18 :: Batch 0/1
Batch Loss = 0.0007310600684375825
18, epoch_train_loss=0.0007310600684375825
Epoch 19
Epoch 19 :: Batch 0/1
Batch Loss = 0.0007309719650171163
19, epoch_train_loss=0.0007309719650171163
Epoch 20
Epoch 20 :: Batch 0/1
Batch Loss = 0.0007309342712509028
20, epoch_train_loss=0.0007309342712509028
Epoch 21
Epoch 21 :: Batch 0/1
Batch Loss = 0.0007309176533698015
21, epoch_train_loss=0.0007309176533698015
Epoch 22
Epoch 22 :: Batch 0/1
Batch Loss = 0.0007309100874453658
22, epoch_train_loss=0.0007309100874453658
Epoch 23
Epoch 23 :: Batch 0/1
Batch Loss = 0.0007309065246934477
23, epoch_train_loss=0.0007309065246934477
Epoch 24
Epoch 24 :: Batch 0/1
Batch Loss = 0.000730904787961573
24, epoch_train_loss=0.000730904787961573
Epoch 25
Epoch 25 :: Batch 0/1
Batch Loss = 0.0007309039112553222
25, epoch_train_loss=0.0007309039112553222
Epoch 26
Epoch 26 :: Batch 0/1
Batch Loss = 0.0007309034530069283
26, epoch_train_loss=0.0007309034530069283
Epoch 27
Epoch 27 :: Batch 0/1
Batch Loss = 0.0007309032051160516
27, epoch_train_loss=0.0007309032051160516
Epoch 28
Epoch 28 :: Batch 0/1
Batch Loss = 0.0007309030664443528
28, epoch_train_loss=0.0007309030664443528
Epoch 29
Epoch 29 :: Batch 0/1
Batch Loss = 0.0007309029863071995
29, epoch_train_loss=0.0007309029863071995
Epoch 30
Epoch 30 :: Batch 0/1
Batch Loss = 0.0007309029385244839
30, epoch_train_loss=0.0007309029385244839
Epoch 31
Epoch 31 :: Batch 0/1
Batch Loss = 0.0007309029091673885
31, epoch_train_loss=0.0007309029091673885
Epoch 32
Epoch 32 :: Batch 0/1
Batch Loss = 0.0007309028906091865
32, epoch_train_loss=0.0007309028906091865
Epoch 33
Epoch 33 :: Batch 0/1
Batch Loss = 0.0007309028785563132
33, epoch_train_loss=0.0007309028785563132
Epoch 34
Epoch 34 :: Batch 0/1
Batch Loss = 0.0007309028705262795
34, epoch_train_loss=0.0007309028705262795
Epoch 35
Epoch 35 :: Batch 0/1
Batch Loss = 0.000730902865046593
35, epoch_train_loss=0.000730902865046593
Epoch 36
Epoch 36 :: Batch 0/1
Batch Loss = 0.0007309028612222854
36, epoch_train_loss=0.0007309028612222854
Epoch 37
Epoch 37 :: Batch 0/1
Batch Loss = 0.0007309028584966345
37, epoch_train_loss=0.0007309028584966345
Epoch 38
Epoch 38 :: Batch 0/1
Batch Loss = 0.0007309028565156148
38, epoch_train_loss=0.0007309028565156148
Epoch 39
Epoch 39 :: Batch 0/1
Batch Loss = 0.0007309028550493489
39, epoch_train_loss=0.0007309028550493489
Epoch 40
Epoch 40 :: Batch 0/1
Batch Loss = 0.0007309028539455957
40, epoch_train_loss=0.0007309028539455957
Epoch 41
Epoch 41 :: Batch 0/1
Batch Loss = 0.0007309028531016306
41, epoch_train_loss=0.0007309028531016306
Epoch 42
Epoch 42 :: Batch 0/1
Batch Loss = 0.0007309028524469091
42, epoch_train_loss=0.0007309028524469091
Epoch 43
Epoch 43 :: Batch 0/1
Batch Loss = 0.0007309028519321728
43, epoch_train_loss=0.0007309028519321728
Epoch 44
Epoch 44 :: Batch 0/1
Batch Loss = 0.0007309028515224843
44, epoch_train_loss=0.0007309028515224843
Epoch 45
Epoch 45 :: Batch 0/1
Batch Loss = 0.0007309028511926944
45, epoch_train_loss=0.0007309028511926944
Epoch 46
Epoch 46 :: Batch 0/1
Batch Loss = 0.000730902850924445
46, epoch_train_loss=0.000730902850924445
Epoch 47
Epoch 47 :: Batch 0/1
Batch Loss = 0.0007309028507041591
47, epoch_train_loss=0.0007309028507041591
Epoch 48
Epoch 48 :: Batch 0/1
Batch Loss = 0.0007309028505216701
48, epoch_train_loss=0.0007309028505216701
Epoch 49
Epoch 49 :: Batch 0/1
Batch Loss = 0.0007309028503692752
49, epoch_train_loss=0.0007309028503692752
Epoch 50
Epoch 50 :: Batch 0/1
Batch Loss = 0.000730902850241074
50, epoch_train_loss=0.000730902850241074
Epoch 51
Epoch 51 :: Batch 0/1
Batch Loss = 0.0007309028501324995
51, epoch_train_loss=0.0007309028501324995
Epoch 52
Epoch 52 :: Batch 0/1
Batch Loss = 0.0007309028500399854
52, epoch_train_loss=0.0007309028500399854
Epoch 53
Epoch 53 :: Batch 0/1
Batch Loss = 0.0007309028499607132
53, epoch_train_loss=0.0007309028499607132
Epoch 54
Epoch 54 :: Batch 0/1
Batch Loss = 0.0007309028498924399
54, epoch_train_loss=0.0007309028498924399
Epoch 55
Epoch 55 :: Batch 0/1
Batch Loss = 0.000730902849833365
55, epoch_train_loss=0.000730902849833365
Epoch 56
Epoch 56 :: Batch 0/1
Batch Loss = 0.0007309028497820321
56, epoch_train_loss=0.0007309028497820321
Epoch 57
Epoch 57 :: Batch 0/1
Batch Loss = 0.000730902849737254
57, epoch_train_loss=0.000730902849737254
Epoch 58
Epoch 58 :: Batch 0/1
Batch Loss = 0.0007309028496980565
58, epoch_train_loss=0.0007309028496980565
Epoch 59
Epoch 59 :: Batch 0/1
Batch Loss = 0.0007309028496636343
59, epoch_train_loss=0.0007309028496636343
Epoch 60
Epoch 60 :: Batch 0/1
Batch Loss = 0.0007309028496333182
60, epoch_train_loss=0.0007309028496333182
Epoch 61
Epoch 61 :: Batch 0/1
Batch Loss = 0.0007309028496065476
61, epoch_train_loss=0.0007309028496065476
Epoch 62
Epoch 62 :: Batch 0/1
Batch Loss = 0.0007309028495828515
62, epoch_train_loss=0.0007309028495828515
Epoch 63
Epoch 63 :: Batch 0/1
Batch Loss = 0.0007309028495618314
63, epoch_train_loss=0.0007309028495618314
Epoch 64
Epoch 64 :: Batch 0/1
Batch Loss = 0.0007309028495431486
64, epoch_train_loss=0.0007309028495431486
Epoch 65
Epoch 65 :: Batch 0/1
Batch Loss = 0.0007309028495265132
65, epoch_train_loss=0.0007309028495265132
Epoch 66
Epoch 66 :: Batch 0/1
Batch Loss = 0.0007309028495116774
66, epoch_train_loss=0.0007309028495116774
Epoch 67
Epoch 67 :: Batch 0/1
Batch Loss = 0.0007309028494984271
67, epoch_train_loss=0.0007309028494984271
Epoch 68
Epoch 68 :: Batch 0/1
Batch Loss = 0.0007309028494865774
68, epoch_train_loss=0.0007309028494865774
Epoch 69
Epoch 69 :: Batch 0/1
Batch Loss = 0.0007309028494759674
69, epoch_train_loss=0.0007309028494759674
Epoch 70
Epoch 70 :: Batch 0/1
Batch Loss = 0.0007309028494664575
70, epoch_train_loss=0.0007309028494664575
Epoch 71
Epoch 71 :: Batch 0/1
Batch Loss = 0.0007309028494579251
71, epoch_train_loss=0.0007309028494579251
Epoch 72
Epoch 72 :: Batch 0/1
Batch Loss = 0.0007309028494502631
72, epoch_train_loss=0.0007309028494502631
Epoch 73
Epoch 73 :: Batch 0/1
Batch Loss = 0.0007309028494433776
73, epoch_train_loss=0.0007309028494433776
Epoch 74
Epoch 74 :: Batch 0/1
Batch Loss = 0.0007309028494371852
74, epoch_train_loss=0.0007309028494371852
Epoch 75
Epoch 75 :: Batch 0/1
Batch Loss = 0.0007309028494316128
75, epoch_train_loss=0.0007309028494316128
Epoch 76
Epoch 76 :: Batch 0/1
Batch Loss = 0.0007309028494265953
76, epoch_train_loss=0.0007309028494265953
Epoch 77
Epoch 77 :: Batch 0/1
Batch Loss = 0.0007309028494220752
77, epoch_train_loss=0.0007309028494220752
Epoch 78
Epoch 78 :: Batch 0/1
Batch Loss = 0.0007309028494180013
78, epoch_train_loss=0.0007309028494180013
Epoch 79
Epoch 79 :: Batch 0/1
Batch Loss = 0.000730902849414328
79, epoch_train_loss=0.000730902849414328
Epoch 80
Epoch 80 :: Batch 0/1
Batch Loss = 0.0007309028494110148
80, epoch_train_loss=0.0007309028494110148
Epoch 81
Epoch 81 :: Batch 0/1
Batch Loss = 0.0007309028494080255
81, epoch_train_loss=0.0007309028494080255
Epoch 82
Epoch 82 :: Batch 0/1
Batch Loss = 0.0007309028494053275
82, epoch_train_loss=0.0007309028494053275
Epoch 83
Epoch 83 :: Batch 0/1
Batch Loss = 0.0007309028494028917
83, epoch_train_loss=0.0007309028494028917
Epoch 84
Epoch 84 :: Batch 0/1
Batch Loss = 0.0007309028494006924
84, epoch_train_loss=0.0007309028494006924
Epoch 85
Epoch 85 :: Batch 0/1
Batch Loss = 0.000730902849398706
85, epoch_train_loss=0.000730902849398706
Epoch 86
Epoch 86 :: Batch 0/1
Batch Loss = 0.0007309028493969116
86, epoch_train_loss=0.0007309028493969116
Epoch 87
Epoch 87 :: Batch 0/1
Batch Loss = 0.0007309028493952905
87, epoch_train_loss=0.0007309028493952905
Epoch 88
Epoch 88 :: Batch 0/1
Batch Loss = 0.0007309028493938257
88, epoch_train_loss=0.0007309028493938257
Epoch 89
Epoch 89 :: Batch 0/1
Batch Loss = 0.0007309028493925019
89, epoch_train_loss=0.0007309028493925019
Epoch 90
Epoch 90 :: Batch 0/1
Batch Loss = 0.0007309028493913055
90, epoch_train_loss=0.0007309028493913055
Epoch 91
Epoch 91 :: Batch 0/1
Batch Loss = 0.0007309028493902241
91, epoch_train_loss=0.0007309028493902241
Epoch 92
Epoch 92 :: Batch 0/1
Batch Loss = 0.0007309028493892466
92, epoch_train_loss=0.0007309028493892466
Epoch 93
Epoch 93 :: Batch 0/1
Batch Loss = 0.0007309028493883628
93, epoch_train_loss=0.0007309028493883628
Epoch 94
Epoch 94 :: Batch 0/1
Batch Loss = 0.0007309028493875639
94, epoch_train_loss=0.0007309028493875639
Epoch 95
Epoch 95 :: Batch 0/1
Batch Loss = 0.0007309028493868417
95, epoch_train_loss=0.0007309028493868417
Epoch 96
Epoch 96 :: Batch 0/1
Batch Loss = 0.0007309028493861888
96, epoch_train_loss=0.0007309028493861888
Epoch 97
Epoch 97 :: Batch 0/1
Batch Loss = 0.0007309028493855983
97, epoch_train_loss=0.0007309028493855983
Epoch 98
Epoch 98 :: Batch 0/1
Batch Loss = 0.0007309028493850644
98, epoch_train_loss=0.0007309028493850644
Epoch 99
Epoch 99 :: Batch 0/1
Batch Loss = 0.0007309028493845817
99, epoch_train_loss=0.0007309028493845817
Epoch 100
Epoch 100 :: Batch 0/1
Batch Loss = 0.0007309028493841453
100, epoch_train_loss=0.0007309028493841453
Epoch 101
Epoch 101 :: Batch 0/1
Batch Loss = 0.0007309028493837508
101, epoch_train_loss=0.0007309028493837508
Epoch 102
Epoch 102 :: Batch 0/1
Batch Loss = 0.0007309028493833939
102, epoch_train_loss=0.0007309028493833939
Epoch 103
Epoch 103 :: Batch 0/1
Batch Loss = 0.0007309028493830712
103, epoch_train_loss=0.0007309028493830712
Epoch 104
Epoch 104 :: Batch 0/1
Batch Loss = 0.0007309028493827795
104, epoch_train_loss=0.0007309028493827795
Epoch 105
Epoch 105 :: Batch 0/1
Batch Loss = 0.0007309028493825157
105, epoch_train_loss=0.0007309028493825157
Epoch 106
Epoch 106 :: Batch 0/1
Batch Loss = 0.0007309028493822773
106, epoch_train_loss=0.0007309028493822773
Epoch 107
Epoch 107 :: Batch 0/1
Batch Loss = 0.0007309028493820616
107, epoch_train_loss=0.0007309028493820616
Epoch 108
Epoch 108 :: Batch 0/1
Batch Loss = 0.0007309028493818666
108, epoch_train_loss=0.0007309028493818666
Epoch 109
Epoch 109 :: Batch 0/1
Batch Loss = 0.0007309028493816902
109, epoch_train_loss=0.0007309028493816902
Epoch 110
Epoch 110 :: Batch 0/1
Batch Loss = 0.0007309028493815308
110, epoch_train_loss=0.0007309028493815308
Epoch 111
Epoch 111 :: Batch 0/1
Batch Loss = 0.0007309028493813868
111, epoch_train_loss=0.0007309028493813868
Epoch 112
Epoch 112 :: Batch 0/1
Batch Loss = 0.0007309028493812565
112, epoch_train_loss=0.0007309028493812565
Epoch 113
Epoch 113 :: Batch 0/1
Batch Loss = 0.0007309028493811387
113, epoch_train_loss=0.0007309028493811387
Epoch 114
Epoch 114 :: Batch 0/1
Batch Loss = 0.0007309028493810323
114, epoch_train_loss=0.0007309028493810323
Epoch 115
Epoch 115 :: Batch 0/1
Batch Loss = 0.000730902849380936
115, epoch_train_loss=0.000730902849380936
Epoch 116
Epoch 116 :: Batch 0/1
Batch Loss = 0.0007309028493808489
116, epoch_train_loss=0.0007309028493808489
Epoch 117
Epoch 117 :: Batch 0/1
Batch Loss = 0.0007309028493807702
117, epoch_train_loss=0.0007309028493807702
Epoch 118
Epoch 118 :: Batch 0/1
Batch Loss = 0.0007309028493806992
118, epoch_train_loss=0.0007309028493806992
Epoch 119
Epoch 119 :: Batch 0/1
Batch Loss = 0.0007309028493806348
119, epoch_train_loss=0.0007309028493806348
Epoch 120
Epoch 120 :: Batch 0/1
Batch Loss = 0.0007309028493805768
120, epoch_train_loss=0.0007309028493805768
Epoch 121
Epoch 121 :: Batch 0/1
Batch Loss = 0.0007309028493805244
121, epoch_train_loss=0.0007309028493805244
Epoch 122
Epoch 122 :: Batch 0/1
Batch Loss = 0.0007309028493804768
122, epoch_train_loss=0.0007309028493804768
Epoch 123
Epoch 123 :: Batch 0/1
Batch Loss = 0.000730902849380434
123, epoch_train_loss=0.000730902849380434
Epoch 124
Epoch 124 :: Batch 0/1
Batch Loss = 0.0007309028493803952
124, epoch_train_loss=0.0007309028493803952
Epoch 125
Epoch 125 :: Batch 0/1
Batch Loss = 0.0007309028493803602
125, epoch_train_loss=0.0007309028493803602
Epoch 126
Epoch 126 :: Batch 0/1
Batch Loss = 0.0007309028493803285
126, epoch_train_loss=0.0007309028493803285
Epoch 127
Epoch 127 :: Batch 0/1
Batch Loss = 0.0007309028493802998
127, epoch_train_loss=0.0007309028493802998
Epoch 128
Epoch 128 :: Batch 0/1
Batch Loss = 0.000730902849380274
128, epoch_train_loss=0.000730902849380274
Epoch 129
Epoch 129 :: Batch 0/1
Batch Loss = 0.0007309028493802506
129, epoch_train_loss=0.0007309028493802506
Epoch 130
Epoch 130 :: Batch 0/1
Batch Loss = 0.0007309028493802294
130, epoch_train_loss=0.0007309028493802294
Epoch 131
Epoch 131 :: Batch 0/1
Batch Loss = 0.0007309028493802104
131, epoch_train_loss=0.0007309028493802104
Epoch 132
Epoch 132 :: Batch 0/1
Batch Loss = 0.0007309028493801932
132, epoch_train_loss=0.0007309028493801932
Epoch 133
Epoch 133 :: Batch 0/1
Batch Loss = 0.0007309028493801775
133, epoch_train_loss=0.0007309028493801775
Epoch 134
Epoch 134 :: Batch 0/1
Batch Loss = 0.0007309028493801635
134, epoch_train_loss=0.0007309028493801635
Epoch 135
Epoch 135 :: Batch 0/1
Batch Loss = 0.0007309028493801507
135, epoch_train_loss=0.0007309028493801507
Epoch 136
Epoch 136 :: Batch 0/1
Batch Loss = 0.0007309028493801392
136, epoch_train_loss=0.0007309028493801392
Epoch 137
Epoch 137 :: Batch 0/1
Batch Loss = 0.0007309028493801288
137, epoch_train_loss=0.0007309028493801288
Epoch 138
Epoch 138 :: Batch 0/1
Batch Loss = 0.0007309028493801195
138, epoch_train_loss=0.0007309028493801195
Epoch 139
Epoch 139 :: Batch 0/1
Batch Loss = 0.0007309028493801109
139, epoch_train_loss=0.0007309028493801109
Epoch 140
Epoch 140 :: Batch 0/1
Batch Loss = 0.0007309028493801033
140, epoch_train_loss=0.0007309028493801033
Epoch 141
Epoch 141 :: Batch 0/1
Batch Loss = 0.0007309028493800963
141, epoch_train_loss=0.0007309028493800963
Epoch 142
Epoch 142 :: Batch 0/1
Batch Loss = 0.0007309028493800901
142, epoch_train_loss=0.0007309028493800901
Epoch 143
Epoch 143 :: Batch 0/1
Batch Loss = 0.0007309028493800845
143, epoch_train_loss=0.0007309028493800845
Epoch 144
Epoch 144 :: Batch 0/1
Batch Loss = 0.0007309028493800794
144, epoch_train_loss=0.0007309028493800794
Epoch 145
Epoch 145 :: Batch 0/1
Batch Loss = 0.0007309028493800747
145, epoch_train_loss=0.0007309028493800747
Epoch 146
Epoch 146 :: Batch 0/1
Batch Loss = 0.0007309028493800707
146, epoch_train_loss=0.0007309028493800707
Epoch 147
Epoch 147 :: Batch 0/1
Batch Loss = 0.0007309028493800668
147, epoch_train_loss=0.0007309028493800668
Epoch 148
Epoch 148 :: Batch 0/1
Batch Loss = 0.0007309028493800634
148, epoch_train_loss=0.0007309028493800634
Epoch 149
Epoch 149 :: Batch 0/1
Batch Loss = 0.0007309028493800604
149, epoch_train_loss=0.0007309028493800604
Epoch 150
Epoch 150 :: Batch 0/1
Batch Loss = 0.0007309028493800576
150, epoch_train_loss=0.0007309028493800576
Epoch 151
Epoch 151 :: Batch 0/1
Batch Loss = 0.0007309028493800551
151, epoch_train_loss=0.0007309028493800551
Epoch 152
Epoch 152 :: Batch 0/1
Batch Loss = 0.0007309028493800527
152, epoch_train_loss=0.0007309028493800527
Epoch 153
Epoch 153 :: Batch 0/1
Batch Loss = 0.0007309028493800507
153, epoch_train_loss=0.0007309028493800507
Epoch 154
Epoch 154 :: Batch 0/1
Batch Loss = 0.0007309028493800488
154, epoch_train_loss=0.0007309028493800488
Epoch 155
Epoch 155 :: Batch 0/1
Batch Loss = 0.0007309028493800472
155, epoch_train_loss=0.0007309028493800472
Epoch 156
Epoch 156 :: Batch 0/1
Batch Loss = 0.0007309028493800458
156, epoch_train_loss=0.0007309028493800458
Epoch 157
Epoch 157 :: Batch 0/1
Batch Loss = 0.0007309028493800442
157, epoch_train_loss=0.0007309028493800442
Epoch 158
Epoch 158 :: Batch 0/1
Batch Loss = 0.000730902849380043
158, epoch_train_loss=0.000730902849380043
Epoch 159
Epoch 159 :: Batch 0/1
Batch Loss = 0.0007309028493800418
159, epoch_train_loss=0.0007309028493800418
Epoch 160
Epoch 160 :: Batch 0/1
Batch Loss = 0.0007309028493800409
160, epoch_train_loss=0.0007309028493800409
Epoch 161
Epoch 161 :: Batch 0/1
Batch Loss = 0.0007309028493800401
161, epoch_train_loss=0.0007309028493800401
Epoch 162
Epoch 162 :: Batch 0/1
Batch Loss = 0.0007309028493800392
162, epoch_train_loss=0.0007309028493800392
Epoch 163
Epoch 163 :: Batch 0/1
Batch Loss = 0.0007309028493800385
163, epoch_train_loss=0.0007309028493800385
Epoch 164
Epoch 164 :: Batch 0/1
Batch Loss = 0.0007309028493800378
164, epoch_train_loss=0.0007309028493800378
Epoch 165
Epoch 165 :: Batch 0/1
Batch Loss = 0.0007309028493800372
165, epoch_train_loss=0.0007309028493800372
Epoch 166
Epoch 166 :: Batch 0/1
Batch Loss = 0.0007309028493800366
166, epoch_train_loss=0.0007309028493800366
Epoch 167
Epoch 167 :: Batch 0/1
Batch Loss = 0.0007309028493800361
167, epoch_train_loss=0.0007309028493800361
Epoch 168
Epoch 168 :: Batch 0/1
Batch Loss = 0.0007309028493800357
168, epoch_train_loss=0.0007309028493800357
Epoch 169
Epoch 169 :: Batch 0/1
Batch Loss = 0.0007309028493800352
169, epoch_train_loss=0.0007309028493800352
Epoch 170
Epoch 170 :: Batch 0/1
Batch Loss = 0.0007309028493800349
170, epoch_train_loss=0.0007309028493800349
Epoch 171
Epoch 171 :: Batch 0/1
Batch Loss = 0.0007309028493800346
171, epoch_train_loss=0.0007309028493800346
Epoch 172
Epoch 172 :: Batch 0/1
Batch Loss = 0.0007309028493800344
172, epoch_train_loss=0.0007309028493800344
Epoch 173
Epoch 173 :: Batch 0/1
Batch Loss = 0.000730902849380034
173, epoch_train_loss=0.000730902849380034
Epoch 174
Epoch 174 :: Batch 0/1
Batch Loss = 0.0007309028493800337
174, epoch_train_loss=0.0007309028493800337
Epoch 175
Epoch 175 :: Batch 0/1
Batch Loss = 0.0007309028493800335
175, epoch_train_loss=0.0007309028493800335
Epoch 176
Epoch 176 :: Batch 0/1
Batch Loss = 0.0007309028493800333
176, epoch_train_loss=0.0007309028493800333
Epoch 177
Epoch 177 :: Batch 0/1
Batch Loss = 0.0007309028493800333
177, epoch_train_loss=0.0007309028493800333
Epoch 178
Epoch 178 :: Batch 0/1
Batch Loss = 0.000730902849380033
178, epoch_train_loss=0.000730902849380033
Epoch 179
Epoch 179 :: Batch 0/1
Batch Loss = 0.0007309028493800327
179, epoch_train_loss=0.0007309028493800327
Epoch 180
Epoch 180 :: Batch 0/1
Batch Loss = 0.0007309028493800327
180, epoch_train_loss=0.0007309028493800327
Epoch 181
Epoch 181 :: Batch 0/1
Batch Loss = 0.0007309028493800325
181, epoch_train_loss=0.0007309028493800325
Epoch 182
Epoch 182 :: Batch 0/1
Batch Loss = 0.0007309028493800325
182, epoch_train_loss=0.0007309028493800325
Epoch 183
Epoch 183 :: Batch 0/1
Batch Loss = 0.0007309028493800323
183, epoch_train_loss=0.0007309028493800323
Epoch 184
Epoch 184 :: Batch 0/1
Batch Loss = 0.0007309028493800323
184, epoch_train_loss=0.0007309028493800323
Epoch 185
Epoch 185 :: Batch 0/1
Batch Loss = 0.0007309028493800322
185, epoch_train_loss=0.0007309028493800322
Epoch 186
Epoch 186 :: Batch 0/1
Batch Loss = 0.0007309028493800321
186, epoch_train_loss=0.0007309028493800321
Epoch 187
Epoch 187 :: Batch 0/1
Batch Loss = 0.0007309028493800321
187, epoch_train_loss=0.0007309028493800321
Epoch 188
Epoch 188 :: Batch 0/1
Batch Loss = 0.0007309028493800321
188, epoch_train_loss=0.0007309028493800321
Epoch 189
Epoch 189 :: Batch 0/1
Batch Loss = 0.0007309028493800318
189, epoch_train_loss=0.0007309028493800318
Epoch 190
Epoch 190 :: Batch 0/1
Batch Loss = 0.0007309028493800318
190, epoch_train_loss=0.0007309028493800318
Epoch 191
Epoch 191 :: Batch 0/1
Batch Loss = 0.0007309028493800318
191, epoch_train_loss=0.0007309028493800318
Epoch 192
Epoch 192 :: Batch 0/1
Batch Loss = 0.0007309028493800318
192, epoch_train_loss=0.0007309028493800318
Epoch 193
Epoch 193 :: Batch 0/1
Batch Loss = 0.0007309028493800318
193, epoch_train_loss=0.0007309028493800318
Epoch 194
Epoch 194 :: Batch 0/1
Batch Loss = 0.0007309028493800317
194, epoch_train_loss=0.0007309028493800317
Epoch 195
Epoch 195 :: Batch 0/1
Batch Loss = 0.0007309028493800317
195, epoch_train_loss=0.0007309028493800317
Epoch 196
Epoch 196 :: Batch 0/1
Batch Loss = 0.0007309028493800315
196, epoch_train_loss=0.0007309028493800315
Epoch 197
Epoch 197 :: Batch 0/1
Batch Loss = 0.0007309028493800315
197, epoch_train_loss=0.0007309028493800315
Epoch 198
Epoch 198 :: Batch 0/1
Batch Loss = 0.0007309028493800314
198, epoch_train_loss=0.0007309028493800314
Epoch 199
Epoch 199 :: Batch 0/1
Batch Loss = 0.0007309028493800314
199, epoch_train_loss=0.0007309028493800314
Epoch 200
Epoch 200 :: Batch 0/1
Batch Loss = 0.0007309028493800314
200, epoch_train_loss=0.0007309028493800314
Epoch 201
Epoch 201 :: Batch 0/1
Batch Loss = 0.0007309028493800314
201, epoch_train_loss=0.0007309028493800314
Epoch 202
Epoch 202 :: Batch 0/1
Batch Loss = 0.0007309028493800313
202, epoch_train_loss=0.0007309028493800313
Epoch 203
Epoch 203 :: Batch 0/1
Batch Loss = 0.0007309028493800313
203, epoch_train_loss=0.0007309028493800313
Epoch 204
Epoch 204 :: Batch 0/1
Batch Loss = 0.0007309028493800313
204, epoch_train_loss=0.0007309028493800313
Epoch 205
Epoch 205 :: Batch 0/1
Batch Loss = 0.0007309028493800313
205, epoch_train_loss=0.0007309028493800313
Epoch 206
Epoch 206 :: Batch 0/1
Batch Loss = 0.0007309028493800313
206, epoch_train_loss=0.0007309028493800313
Epoch 207
Epoch 207 :: Batch 0/1
Batch Loss = 0.0007309028493800313
207, epoch_train_loss=0.0007309028493800313
Epoch 208
Epoch 208 :: Batch 0/1
Batch Loss = 0.0007309028493800313
208, epoch_train_loss=0.0007309028493800313
Epoch 209
Epoch 209 :: Batch 0/1
Batch Loss = 0.0007309028493800313
209, epoch_train_loss=0.0007309028493800313
Epoch 210
Epoch 210 :: Batch 0/1
Batch Loss = 0.0007309028493800313
210, epoch_train_loss=0.0007309028493800313
Epoch 211
Epoch 211 :: Batch 0/1
Batch Loss = 0.0007309028493800312
211, epoch_train_loss=0.0007309028493800312
Epoch 212
Epoch 212 :: Batch 0/1
Batch Loss = 0.0007309028493800312
212, epoch_train_loss=0.0007309028493800312
Epoch 213
Epoch 213 :: Batch 0/1
Batch Loss = 0.0007309028493800312
213, epoch_train_loss=0.0007309028493800312
Epoch 214
Epoch 214 :: Batch 0/1
Batch Loss = 0.0007309028493800312
214, epoch_train_loss=0.0007309028493800312
Epoch 215
Epoch 215 :: Batch 0/1
Batch Loss = 0.0007309028493800312
215, epoch_train_loss=0.0007309028493800312
Epoch 216
Epoch 216 :: Batch 0/1
Batch Loss = 0.0007309028493800311
216, epoch_train_loss=0.0007309028493800311
Epoch 217
Epoch 217 :: Batch 0/1
Batch Loss = 0.0007309028493800311
217, epoch_train_loss=0.0007309028493800311
Epoch 218
Epoch 218 :: Batch 0/1
Batch Loss = 0.0007309028493800311
218, epoch_train_loss=0.0007309028493800311
Epoch 219
Epoch 219 :: Batch 0/1
Batch Loss = 0.0007309028493800311
219, epoch_train_loss=0.0007309028493800311
Epoch 220
Epoch 220 :: Batch 0/1
Batch Loss = 0.0007309028493800311
220, epoch_train_loss=0.0007309028493800311
Epoch 221
Epoch 221 :: Batch 0/1
Batch Loss = 0.0007309028493800311
221, epoch_train_loss=0.0007309028493800311
Epoch 222
Epoch 222 :: Batch 0/1
Batch Loss = 0.0007309028493800311
222, epoch_train_loss=0.0007309028493800311
Epoch 223
Epoch 223 :: Batch 0/1
Batch Loss = 0.0007309028493800311
223, epoch_train_loss=0.0007309028493800311
Epoch 224
Epoch 224 :: Batch 0/1
Batch Loss = 0.0007309028493800311
224, epoch_train_loss=0.0007309028493800311
Epoch 225
Epoch 225 :: Batch 0/1
Batch Loss = 0.0007309028493800311
225, epoch_train_loss=0.0007309028493800311
Epoch 226
Epoch 226 :: Batch 0/1
Batch Loss = 0.0007309028493800311
226, epoch_train_loss=0.0007309028493800311
Epoch 227
Epoch 227 :: Batch 0/1
Batch Loss = 0.0007309028493800311
227, epoch_train_loss=0.0007309028493800311
Epoch 228
Epoch 228 :: Batch 0/1
Batch Loss = 0.0007309028493800311
228, epoch_train_loss=0.0007309028493800311
Epoch 229
Epoch 229 :: Batch 0/1
Batch Loss = 0.0007309028493800311
229, epoch_train_loss=0.0007309028493800311
Epoch 230
Epoch 230 :: Batch 0/1
Batch Loss = 0.0007309028493800311
230, epoch_train_loss=0.0007309028493800311
Epoch 231
Epoch 231 :: Batch 0/1
Batch Loss = 0.0007309028493800311
231, epoch_train_loss=0.0007309028493800311
Epoch 232
Epoch 232 :: Batch 0/1
Batch Loss = 0.0007309028493800311
232, epoch_train_loss=0.0007309028493800311
Epoch 233
Epoch 233 :: Batch 0/1
Batch Loss = 0.0007309028493800311
233, epoch_train_loss=0.0007309028493800311
Epoch 234
Epoch 234 :: Batch 0/1
Batch Loss = 0.0007309028493800311
234, epoch_train_loss=0.0007309028493800311
Epoch 235
Epoch 235 :: Batch 0/1
Batch Loss = 0.000730902849380031
235, epoch_train_loss=0.000730902849380031
Epoch 236
Epoch 236 :: Batch 0/1
Batch Loss = 0.000730902849380031
236, epoch_train_loss=0.000730902849380031
Epoch 237
Epoch 237 :: Batch 0/1
Batch Loss = 0.0007309028493800308
237, epoch_train_loss=0.0007309028493800308
Epoch 238
Epoch 238 :: Batch 0/1
Batch Loss = 0.0007309028493800308
238, epoch_train_loss=0.0007309028493800308
Epoch 239
Epoch 239 :: Batch 0/1
Batch Loss = 0.0007309028493800308
239, epoch_train_loss=0.0007309028493800308
Epoch 240
Epoch 240 :: Batch 0/1
Batch Loss = 0.0007309028493800308
240, epoch_train_loss=0.0007309028493800308
Epoch 241
Epoch 241 :: Batch 0/1
Batch Loss = 0.0007309028493800308
241, epoch_train_loss=0.0007309028493800308
Epoch 242
Epoch 242 :: Batch 0/1
Batch Loss = 0.0007309028493800308
242, epoch_train_loss=0.0007309028493800308
Epoch 243
Epoch 243 :: Batch 0/1
Batch Loss = 0.0007309028493800308
243, epoch_train_loss=0.0007309028493800308
Epoch 244
Epoch 244 :: Batch 0/1
Batch Loss = 0.0007309028493800308
244, epoch_train_loss=0.0007309028493800308
Epoch 245
Epoch 245 :: Batch 0/1
Batch Loss = 0.0007309028493800308
245, epoch_train_loss=0.0007309028493800308
Epoch 246
Epoch 246 :: Batch 0/1
Batch Loss = 0.0007309028493800308
246, epoch_train_loss=0.0007309028493800308
Epoch 247
Epoch 247 :: Batch 0/1
Batch Loss = 0.0007309028493800308
247, epoch_train_loss=0.0007309028493800308
Epoch 248
Epoch 248 :: Batch 0/1
Batch Loss = 0.0007309028493800308
248, epoch_train_loss=0.0007309028493800308
Epoch 249
Epoch 249 :: Batch 0/1
Batch Loss = 0.0007309028493800308
249, epoch_train_loss=0.0007309028493800308
Epoch 250
Epoch 250 :: Batch 0/1
Batch Loss = 0.0007309028493800308
250, epoch_train_loss=0.0007309028493800308
Epoch 251
Epoch 251 :: Batch 0/1
Batch Loss = 0.0007309028493800308
251, epoch_train_loss=0.0007309028493800308
Epoch 252
Epoch 252 :: Batch 0/1
Batch Loss = 0.0007309028493800308
252, epoch_train_loss=0.0007309028493800308
Epoch 253
Epoch 253 :: Batch 0/1
Batch Loss = 0.0007309028493800308
253, epoch_train_loss=0.0007309028493800308
Epoch 254
Epoch 254 :: Batch 0/1
Batch Loss = 0.0007309028493800308
254, epoch_train_loss=0.0007309028493800308
Epoch 255
Epoch 255 :: Batch 0/1
Batch Loss = 0.0007309028493800308
255, epoch_train_loss=0.0007309028493800308
Epoch 256
Epoch 256 :: Batch 0/1
Batch Loss = 0.0007309028493800308
256, epoch_train_loss=0.0007309028493800308
Epoch 257
Epoch 257 :: Batch 0/1
Batch Loss = 0.0007309028493800308
257, epoch_train_loss=0.0007309028493800308
Epoch 258
Epoch 258 :: Batch 0/1
Batch Loss = 0.0007309028493800307
258, epoch_train_loss=0.0007309028493800307
Epoch 259
Epoch 259 :: Batch 0/1
Batch Loss = 0.0007309028493800307
259, epoch_train_loss=0.0007309028493800307
Epoch 260
Epoch 260 :: Batch 0/1
Batch Loss = 0.0007309028493800306
260, epoch_train_loss=0.0007309028493800306
Epoch 261
Epoch 261 :: Batch 0/1
Batch Loss = 0.0007309028493800306
261, epoch_train_loss=0.0007309028493800306
Epoch 262
Epoch 262 :: Batch 0/1
Batch Loss = 0.0007309028493800306
262, epoch_train_loss=0.0007309028493800306
Epoch 263
Epoch 263 :: Batch 0/1
Batch Loss = 0.0007309028493800306
263, epoch_train_loss=0.0007309028493800306
Epoch 264
Epoch 264 :: Batch 0/1
Batch Loss = 0.0007309028493800306
264, epoch_train_loss=0.0007309028493800306
Epoch 265
Epoch 265 :: Batch 0/1
Batch Loss = 0.0007309028493800306
265, epoch_train_loss=0.0007309028493800306
Epoch 266
Epoch 266 :: Batch 0/1
Batch Loss = 0.0007309028493800306
266, epoch_train_loss=0.0007309028493800306
Epoch 267
Epoch 267 :: Batch 0/1
Batch Loss = 0.0007309028493800306
267, epoch_train_loss=0.0007309028493800306
Epoch 268
Epoch 268 :: Batch 0/1
Batch Loss = 0.0007309028493800306
268, epoch_train_loss=0.0007309028493800306
Epoch 269
Epoch 269 :: Batch 0/1
Batch Loss = 0.0007309028493800306
269, epoch_train_loss=0.0007309028493800306
Epoch 270
Epoch 270 :: Batch 0/1
Batch Loss = 0.0007309028493800306
270, epoch_train_loss=0.0007309028493800306
Epoch 271
Epoch 271 :: Batch 0/1
Batch Loss = 0.0007309028493800306
271, epoch_train_loss=0.0007309028493800306
Epoch 272
Epoch 272 :: Batch 0/1
Batch Loss = 0.0007309028493800306
272, epoch_train_loss=0.0007309028493800306
Epoch 273
Epoch 273 :: Batch 0/1
Batch Loss = 0.0007309028493800306
273, epoch_train_loss=0.0007309028493800306
Epoch 274
Epoch 274 :: Batch 0/1
Batch Loss = 0.0007309028493800305
274, epoch_train_loss=0.0007309028493800305
Epoch 275
Epoch 275 :: Batch 0/1
Batch Loss = 0.0007309028493800305
275, epoch_train_loss=0.0007309028493800305
Epoch 276
Epoch 276 :: Batch 0/1
Batch Loss = 0.0007309028493800305
276, epoch_train_loss=0.0007309028493800305
Epoch 277
Epoch 277 :: Batch 0/1
Batch Loss = 0.0007309028493800305
277, epoch_train_loss=0.0007309028493800305
Epoch 278
Epoch 278 :: Batch 0/1
Batch Loss = 0.0007309028493800305
278, epoch_train_loss=0.0007309028493800305
Epoch 279
Epoch 279 :: Batch 0/1
Batch Loss = 0.0007309028493800305
279, epoch_train_loss=0.0007309028493800305
Epoch 280
Epoch 280 :: Batch 0/1
Batch Loss = 0.0007309028493800305
280, epoch_train_loss=0.0007309028493800305
Epoch 281
Epoch 281 :: Batch 0/1
Batch Loss = 0.0007309028493800305
281, epoch_train_loss=0.0007309028493800305
Epoch 282
Epoch 282 :: Batch 0/1
Batch Loss = 0.0007309028493800305
282, epoch_train_loss=0.0007309028493800305
Epoch 283
Epoch 283 :: Batch 0/1
Batch Loss = 0.0007309028493800305
283, epoch_train_loss=0.0007309028493800305
Epoch 284
Epoch 284 :: Batch 0/1
Batch Loss = 0.0007309028493800305
284, epoch_train_loss=0.0007309028493800305
Epoch 285
Epoch 285 :: Batch 0/1
Batch Loss = 0.0007309028493800304
285, epoch_train_loss=0.0007309028493800304
Epoch 286
Epoch 286 :: Batch 0/1
Batch Loss = 0.0007309028493800304
286, epoch_train_loss=0.0007309028493800304
Epoch 287
Epoch 287 :: Batch 0/1
Batch Loss = 0.0007309028493800304
287, epoch_train_loss=0.0007309028493800304
Epoch 288
Epoch 288 :: Batch 0/1
Batch Loss = 0.0007309028493800304
288, epoch_train_loss=0.0007309028493800304
Epoch 289
Epoch 289 :: Batch 0/1
Batch Loss = 0.0007309028493800304
289, epoch_train_loss=0.0007309028493800304
Epoch 290
Epoch 290 :: Batch 0/1
Batch Loss = 0.0007309028493800304
290, epoch_train_loss=0.0007309028493800304
Epoch 291
Epoch 291 :: Batch 0/1
Batch Loss = 0.0007309028493800304
291, epoch_train_loss=0.0007309028493800304
Epoch 292
Epoch 292 :: Batch 0/1
Batch Loss = 0.0007309028493800304
292, epoch_train_loss=0.0007309028493800304
Epoch 293
Epoch 293 :: Batch 0/1
Batch Loss = 0.0007309028493800304
293, epoch_train_loss=0.0007309028493800304
Epoch 294
Epoch 294 :: Batch 0/1
Batch Loss = 0.0007309028493800304
294, epoch_train_loss=0.0007309028493800304
Epoch 295
Epoch 295 :: Batch 0/1
Batch Loss = 0.0007309028493800304
295, epoch_train_loss=0.0007309028493800304
Epoch 296
Epoch 296 :: Batch 0/1
Batch Loss = 0.0007309028493800304
296, epoch_train_loss=0.0007309028493800304
Epoch 297
Epoch 297 :: Batch 0/1
Batch Loss = 0.0007309028493800304
297, epoch_train_loss=0.0007309028493800304
Epoch 298
Epoch 298 :: Batch 0/1
Batch Loss = 0.0007309028493800304
298, epoch_train_loss=0.0007309028493800304
Epoch 299
Epoch 299 :: Batch 0/1
Batch Loss = 0.0007309028493800304
299, epoch_train_loss=0.0007309028493800304
Epoch 300
Epoch 300 :: Batch 0/1
Batch Loss = 0.0007309028493800304
300, epoch_train_loss=0.0007309028493800304
Epoch 301
Epoch 301 :: Batch 0/1
Batch Loss = 0.0007309028493800304
301, epoch_train_loss=0.0007309028493800304
Epoch 302
Epoch 302 :: Batch 0/1
Batch Loss = 0.0007309028493800304
302, epoch_train_loss=0.0007309028493800304
Epoch 303
Epoch 303 :: Batch 0/1
Batch Loss = 0.0007309028493800304
303, epoch_train_loss=0.0007309028493800304
Epoch 304
Epoch 304 :: Batch 0/1
Batch Loss = 0.0007309028493800304
304, epoch_train_loss=0.0007309028493800304
Epoch 305
Epoch 305 :: Batch 0/1
Batch Loss = 0.0007309028493800304
305, epoch_train_loss=0.0007309028493800304
Epoch 306
Epoch 306 :: Batch 0/1
Batch Loss = 0.0007309028493800302
306, epoch_train_loss=0.0007309028493800302
Epoch 307
Epoch 307 :: Batch 0/1
Batch Loss = 0.0007309028493800302
307, epoch_train_loss=0.0007309028493800302
Epoch 308
Epoch 308 :: Batch 0/1
Batch Loss = 0.0007309028493800302
308, epoch_train_loss=0.0007309028493800302
Epoch 309
Epoch 309 :: Batch 0/1
Batch Loss = 0.0007309028493800301
309, epoch_train_loss=0.0007309028493800301
Epoch 310
Epoch 310 :: Batch 0/1
Batch Loss = 0.0007309028493800301
310, epoch_train_loss=0.0007309028493800301
Epoch 311
Epoch 311 :: Batch 0/1
Batch Loss = 0.0007309028493800301
311, epoch_train_loss=0.0007309028493800301
Epoch 312
Epoch 312 :: Batch 0/1
Batch Loss = 0.0007309028493800301
312, epoch_train_loss=0.0007309028493800301
Epoch 313
Epoch 313 :: Batch 0/1
Batch Loss = 0.0007309028493800301
313, epoch_train_loss=0.0007309028493800301
Epoch 314
Epoch 314 :: Batch 0/1
Batch Loss = 0.0007309028493800301
314, epoch_train_loss=0.0007309028493800301
Epoch 315
Epoch 315 :: Batch 0/1
Batch Loss = 0.0007309028493800301
315, epoch_train_loss=0.0007309028493800301
Epoch 316
Epoch 316 :: Batch 0/1
Batch Loss = 0.0007309028493800301
316, epoch_train_loss=0.0007309028493800301
Epoch 317
Epoch 317 :: Batch 0/1
Batch Loss = 0.0007309028493800301
317, epoch_train_loss=0.0007309028493800301
Epoch 318
Epoch 318 :: Batch 0/1
Batch Loss = 0.0007309028493800301
318, epoch_train_loss=0.0007309028493800301
Epoch 319
Epoch 319 :: Batch 0/1
Batch Loss = 0.0007309028493800301
319, epoch_train_loss=0.0007309028493800301
Epoch 320
Epoch 320 :: Batch 0/1
Batch Loss = 0.0007309028493800301
320, epoch_train_loss=0.0007309028493800301
Epoch 321
Epoch 321 :: Batch 0/1
Batch Loss = 0.00073090284938003
321, epoch_train_loss=0.00073090284938003
Epoch 322
Epoch 322 :: Batch 0/1
Batch Loss = 0.00073090284938003
322, epoch_train_loss=0.00073090284938003
Epoch 323
Epoch 323 :: Batch 0/1
Batch Loss = 0.00073090284938003
323, epoch_train_loss=0.00073090284938003
Epoch 324
Epoch 324 :: Batch 0/1
Batch Loss = 0.00073090284938003
324, epoch_train_loss=0.00073090284938003
Epoch 325
Epoch 325 :: Batch 0/1
Batch Loss = 0.00073090284938003
325, epoch_train_loss=0.00073090284938003
Epoch 326
Epoch 326 :: Batch 0/1
Batch Loss = 0.0007309028493800299
326, epoch_train_loss=0.0007309028493800299
Epoch 327
Epoch 327 :: Batch 0/1
Batch Loss = 0.0007309028493800299
327, epoch_train_loss=0.0007309028493800299
Epoch 328
Epoch 328 :: Batch 0/1
Batch Loss = 0.0007309028493800299
328, epoch_train_loss=0.0007309028493800299
Epoch 329
Epoch 329 :: Batch 0/1
Batch Loss = 0.0007309028493800299
329, epoch_train_loss=0.0007309028493800299
Epoch 330
Epoch 330 :: Batch 0/1
Batch Loss = 0.0007309028493800299
330, epoch_train_loss=0.0007309028493800299
Epoch 331
Epoch 331 :: Batch 0/1
Batch Loss = 0.0007309028493800299
331, epoch_train_loss=0.0007309028493800299
Epoch 332
Epoch 332 :: Batch 0/1
Batch Loss = 0.0007309028493800299
332, epoch_train_loss=0.0007309028493800299
Epoch 333
Epoch 333 :: Batch 0/1
Batch Loss = 0.0007309028493800299
333, epoch_train_loss=0.0007309028493800299
Epoch 334
Epoch 334 :: Batch 0/1
Batch Loss = 0.0007309028493800299
334, epoch_train_loss=0.0007309028493800299
Epoch 335
Epoch 335 :: Batch 0/1
Batch Loss = 0.0007309028493800299
335, epoch_train_loss=0.0007309028493800299
Epoch 336
Epoch 336 :: Batch 0/1
Batch Loss = 0.0007309028493800298
336, epoch_train_loss=0.0007309028493800298
Epoch 337
Epoch 337 :: Batch 0/1
Batch Loss = 0.0007309028493800298
337, epoch_train_loss=0.0007309028493800298
Epoch 338
Epoch 338 :: Batch 0/1
Batch Loss = 0.0007309028493800298
338, epoch_train_loss=0.0007309028493800298
Epoch 339
Epoch 339 :: Batch 0/1
Batch Loss = 0.0007309028493800298
339, epoch_train_loss=0.0007309028493800298
Epoch 340
Epoch 340 :: Batch 0/1
Batch Loss = 0.0007309028493800298
340, epoch_train_loss=0.0007309028493800298
Epoch 341
Epoch 341 :: Batch 0/1
Batch Loss = 0.0007309028493800298
341, epoch_train_loss=0.0007309028493800298
Epoch 342
Epoch 342 :: Batch 0/1
Batch Loss = 0.0007309028493800298
342, epoch_train_loss=0.0007309028493800298
Epoch 343
Epoch 343 :: Batch 0/1
Batch Loss = 0.0007309028493800298
343, epoch_train_loss=0.0007309028493800298
Epoch 344
Epoch 344 :: Batch 0/1
Batch Loss = 0.0007309028493800298
344, epoch_train_loss=0.0007309028493800298
Epoch 345
Epoch 345 :: Batch 0/1
Batch Loss = 0.0007309028493800298
345, epoch_train_loss=0.0007309028493800298
Epoch 346
Epoch 346 :: Batch 0/1
Batch Loss = 0.0007309028493800298
346, epoch_train_loss=0.0007309028493800298
Epoch 347
Epoch 347 :: Batch 0/1
Batch Loss = 0.0007309028493800298
347, epoch_train_loss=0.0007309028493800298
Epoch 348
Epoch 348 :: Batch 0/1
Batch Loss = 0.0007309028493800298
348, epoch_train_loss=0.0007309028493800298
Epoch 349
Epoch 349 :: Batch 0/1
Batch Loss = 0.0007309028493800298
349, epoch_train_loss=0.0007309028493800298
Epoch 350
Epoch 350 :: Batch 0/1
Batch Loss = 0.0007309028493800296
350, epoch_train_loss=0.0007309028493800296
Epoch 351
Epoch 351 :: Batch 0/1
Batch Loss = 0.0007309028493800296
351, epoch_train_loss=0.0007309028493800296
Epoch 352
Epoch 352 :: Batch 0/1
Batch Loss = 0.0007309028493800296
352, epoch_train_loss=0.0007309028493800296
Epoch 353
Epoch 353 :: Batch 0/1
Batch Loss = 0.0007309028493800296
353, epoch_train_loss=0.0007309028493800296
Epoch 354
Epoch 354 :: Batch 0/1
Batch Loss = 0.0007309028493800296
354, epoch_train_loss=0.0007309028493800296
Epoch 355
Epoch 355 :: Batch 0/1
Batch Loss = 0.0007309028493800296
355, epoch_train_loss=0.0007309028493800296
Epoch 356
Epoch 356 :: Batch 0/1
Batch Loss = 0.0007309028493800295
356, epoch_train_loss=0.0007309028493800295
Epoch 357
Epoch 357 :: Batch 0/1
Batch Loss = 0.0007309028493800295
357, epoch_train_loss=0.0007309028493800295
Epoch 358
Epoch 358 :: Batch 0/1
Batch Loss = 0.0007309028493800295
358, epoch_train_loss=0.0007309028493800295
Epoch 359
Epoch 359 :: Batch 0/1
Batch Loss = 0.0007309028493800295
359, epoch_train_loss=0.0007309028493800295
Epoch 360
Epoch 360 :: Batch 0/1
Batch Loss = 0.0007309028493800295
360, epoch_train_loss=0.0007309028493800295
Epoch 361
Epoch 361 :: Batch 0/1
Batch Loss = 0.0007309028493800295
361, epoch_train_loss=0.0007309028493800295
Epoch 362
Epoch 362 :: Batch 0/1
Batch Loss = 0.0007309028493800295
362, epoch_train_loss=0.0007309028493800295
Epoch 363
Epoch 363 :: Batch 0/1
Batch Loss = 0.0007309028493800295
363, epoch_train_loss=0.0007309028493800295
Epoch 364
Epoch 364 :: Batch 0/1
Batch Loss = 0.0007309028493800295
364, epoch_train_loss=0.0007309028493800295
Epoch 365
Epoch 365 :: Batch 0/1
Batch Loss = 0.0007309028493800295
365, epoch_train_loss=0.0007309028493800295
Epoch 366
Epoch 366 :: Batch 0/1
Batch Loss = 0.0007309028493800295
366, epoch_train_loss=0.0007309028493800295
Epoch 367
Epoch 367 :: Batch 0/1
Batch Loss = 0.0007309028493800295
367, epoch_train_loss=0.0007309028493800295
Epoch 368
Epoch 368 :: Batch 0/1
Batch Loss = 0.0007309028493800295
368, epoch_train_loss=0.0007309028493800295
Epoch 369
Epoch 369 :: Batch 0/1
Batch Loss = 0.0007309028493800295
369, epoch_train_loss=0.0007309028493800295
Epoch 370
Epoch 370 :: Batch 0/1
Batch Loss = 0.0007309028493800295
370, epoch_train_loss=0.0007309028493800295
Epoch 371
Epoch 371 :: Batch 0/1
Batch Loss = 0.0007309028493800294
371, epoch_train_loss=0.0007309028493800294
Epoch 372
Epoch 372 :: Batch 0/1
Batch Loss = 0.0007309028493800294
372, epoch_train_loss=0.0007309028493800294
Epoch 373
Epoch 373 :: Batch 0/1
Batch Loss = 0.0007309028493800294
373, epoch_train_loss=0.0007309028493800294
Epoch 374
Epoch 374 :: Batch 0/1
Batch Loss = 0.0007309028493800294
374, epoch_train_loss=0.0007309028493800294
Epoch 375
Epoch 375 :: Batch 0/1
Batch Loss = 0.0007309028493800294
375, epoch_train_loss=0.0007309028493800294
Epoch 376
Epoch 376 :: Batch 0/1
Batch Loss = 0.0007309028493800294
376, epoch_train_loss=0.0007309028493800294
Epoch 377
Epoch 377 :: Batch 0/1
Batch Loss = 0.0007309028493800293
377, epoch_train_loss=0.0007309028493800293
Epoch 378
Epoch 378 :: Batch 0/1
Batch Loss = 0.0007309028493800293
378, epoch_train_loss=0.0007309028493800293
Epoch 379
Epoch 379 :: Batch 0/1
Batch Loss = 0.0007309028493800293
379, epoch_train_loss=0.0007309028493800293
Epoch 380
Epoch 380 :: Batch 0/1
Batch Loss = 0.0007309028493800293
380, epoch_train_loss=0.0007309028493800293
Epoch 381
Epoch 381 :: Batch 0/1
Batch Loss = 0.0007309028493800293
381, epoch_train_loss=0.0007309028493800293
Epoch 382
Epoch 382 :: Batch 0/1
Batch Loss = 0.0007309028493800293
382, epoch_train_loss=0.0007309028493800293
Epoch 383
Epoch 383 :: Batch 0/1
Batch Loss = 0.0007309028493800293
383, epoch_train_loss=0.0007309028493800293
Epoch 384
Epoch 384 :: Batch 0/1
Batch Loss = 0.0007309028493800293
384, epoch_train_loss=0.0007309028493800293
Epoch 385
Epoch 385 :: Batch 0/1
Batch Loss = 0.0007309028493800293
385, epoch_train_loss=0.0007309028493800293
Epoch 386
Epoch 386 :: Batch 0/1
Batch Loss = 0.0007309028493800293
386, epoch_train_loss=0.0007309028493800293
Epoch 387
Epoch 387 :: Batch 0/1
Batch Loss = 0.0007309028493800292
387, epoch_train_loss=0.0007309028493800292
Epoch 388
Epoch 388 :: Batch 0/1
Batch Loss = 0.0007309028493800292
388, epoch_train_loss=0.0007309028493800292
Epoch 389
Epoch 389 :: Batch 0/1
Batch Loss = 0.0007309028493800292
389, epoch_train_loss=0.0007309028493800292
Epoch 390
Epoch 390 :: Batch 0/1
Batch Loss = 0.0007309028493800292
390, epoch_train_loss=0.0007309028493800292
Epoch 391
Epoch 391 :: Batch 0/1
Batch Loss = 0.0007309028493800292
391, epoch_train_loss=0.0007309028493800292
Epoch 392
Epoch 392 :: Batch 0/1
Batch Loss = 0.0007309028493800291
392, epoch_train_loss=0.0007309028493800291
Epoch 393
Epoch 393 :: Batch 0/1
Batch Loss = 0.0007309028493800291
393, epoch_train_loss=0.0007309028493800291
Epoch 394
Epoch 394 :: Batch 0/1
Batch Loss = 0.0007309028493800291
394, epoch_train_loss=0.0007309028493800291
Epoch 395
Epoch 395 :: Batch 0/1
Batch Loss = 0.0007309028493800291
395, epoch_train_loss=0.0007309028493800291
Epoch 396
Epoch 396 :: Batch 0/1
Batch Loss = 0.0007309028493800291
396, epoch_train_loss=0.0007309028493800291
Epoch 397
Epoch 397 :: Batch 0/1
Batch Loss = 0.0007309028493800291
397, epoch_train_loss=0.0007309028493800291
Epoch 398
Epoch 398 :: Batch 0/1
Batch Loss = 0.0007309028493800291
398, epoch_train_loss=0.0007309028493800291
Epoch 399
Epoch 399 :: Batch 0/1
Batch Loss = 0.0007309028493800291
399, epoch_train_loss=0.0007309028493800291
Epoch 400
Epoch 400 :: Batch 0/1
Batch Loss = 0.0007309028493800291
400, epoch_train_loss=0.0007309028493800291
Epoch 401
Epoch 401 :: Batch 0/1
Batch Loss = 0.0007309028493800291
401, epoch_train_loss=0.0007309028493800291
Epoch 402
Epoch 402 :: Batch 0/1
Batch Loss = 0.0007309028493800291
402, epoch_train_loss=0.0007309028493800291
Epoch 403
Epoch 403 :: Batch 0/1
Batch Loss = 0.000730902849380029
403, epoch_train_loss=0.000730902849380029
Epoch 404
Epoch 404 :: Batch 0/1
Batch Loss = 0.000730902849380029
404, epoch_train_loss=0.000730902849380029
Epoch 405
Epoch 405 :: Batch 0/1
Batch Loss = 0.000730902849380029
405, epoch_train_loss=0.000730902849380029
Epoch 406
Epoch 406 :: Batch 0/1
Batch Loss = 0.000730902849380029
406, epoch_train_loss=0.000730902849380029
Epoch 407
Epoch 407 :: Batch 0/1
Batch Loss = 0.000730902849380029
407, epoch_train_loss=0.000730902849380029
Epoch 408
Epoch 408 :: Batch 0/1
Batch Loss = 0.000730902849380029
408, epoch_train_loss=0.000730902849380029
Epoch 409
Epoch 409 :: Batch 0/1
Batch Loss = 0.000730902849380029
409, epoch_train_loss=0.000730902849380029
Epoch 410
Epoch 410 :: Batch 0/1
Batch Loss = 0.0007309028493800288
410, epoch_train_loss=0.0007309028493800288
Epoch 411
Epoch 411 :: Batch 0/1
Batch Loss = 0.0007309028493800288
411, epoch_train_loss=0.0007309028493800288
Epoch 412
Epoch 412 :: Batch 0/1
Batch Loss = 0.0007309028493800288
412, epoch_train_loss=0.0007309028493800288
Epoch 413
Epoch 413 :: Batch 0/1
Batch Loss = 0.0007309028493800288
413, epoch_train_loss=0.0007309028493800288
Epoch 414
Epoch 414 :: Batch 0/1
Batch Loss = 0.0007309028493800288
414, epoch_train_loss=0.0007309028493800288
Epoch 415
Epoch 415 :: Batch 0/1
Batch Loss = 0.0007309028493800288
415, epoch_train_loss=0.0007309028493800288
Epoch 416
Epoch 416 :: Batch 0/1
Batch Loss = 0.0007309028493800288
416, epoch_train_loss=0.0007309028493800288
Epoch 417
Epoch 417 :: Batch 0/1
Batch Loss = 0.0007309028493800288
417, epoch_train_loss=0.0007309028493800288
Epoch 418
Epoch 418 :: Batch 0/1
Batch Loss = 0.0007309028493800288
418, epoch_train_loss=0.0007309028493800288
Epoch 419
Epoch 419 :: Batch 0/1
Batch Loss = 0.0007309028493800288
419, epoch_train_loss=0.0007309028493800288
Epoch 420
Epoch 420 :: Batch 0/1
Batch Loss = 0.0007309028493800288
420, epoch_train_loss=0.0007309028493800288
Epoch 421
Epoch 421 :: Batch 0/1
Batch Loss = 0.0007309028493800287
421, epoch_train_loss=0.0007309028493800287
Epoch 422
Epoch 422 :: Batch 0/1
Batch Loss = 0.0007309028493800287
422, epoch_train_loss=0.0007309028493800287
Epoch 423
Epoch 423 :: Batch 0/1
Batch Loss = 0.0007309028493800287
423, epoch_train_loss=0.0007309028493800287
Epoch 424
Epoch 424 :: Batch 0/1
Batch Loss = 0.0007309028493800287
424, epoch_train_loss=0.0007309028493800287
Epoch 425
Epoch 425 :: Batch 0/1
Batch Loss = 0.0007309028493800287
425, epoch_train_loss=0.0007309028493800287
Epoch 426
Epoch 426 :: Batch 0/1
Batch Loss = 0.0007309028493800287
426, epoch_train_loss=0.0007309028493800287
Epoch 427
Epoch 427 :: Batch 0/1
Batch Loss = 0.0007309028493800287
427, epoch_train_loss=0.0007309028493800287
Epoch 428
Epoch 428 :: Batch 0/1
Batch Loss = 0.0007309028493800287
428, epoch_train_loss=0.0007309028493800287
Epoch 429
Epoch 429 :: Batch 0/1
Batch Loss = 0.0007309028493800285
429, epoch_train_loss=0.0007309028493800285
Epoch 430
Epoch 430 :: Batch 0/1
Batch Loss = 0.0007309028493800285
430, epoch_train_loss=0.0007309028493800285
Epoch 431
Epoch 431 :: Batch 0/1
Batch Loss = 0.0007309028493800285
431, epoch_train_loss=0.0007309028493800285
Epoch 432
Epoch 432 :: Batch 0/1
Batch Loss = 0.0007309028493800285
432, epoch_train_loss=0.0007309028493800285
Epoch 433
Epoch 433 :: Batch 0/1
Batch Loss = 0.0007309028493800285
433, epoch_train_loss=0.0007309028493800285
Epoch 434
Epoch 434 :: Batch 0/1
Batch Loss = 0.0007309028493800285
434, epoch_train_loss=0.0007309028493800285
Epoch 435
Epoch 435 :: Batch 0/1
Batch Loss = 0.0007309028493800284
435, epoch_train_loss=0.0007309028493800284
Epoch 436
Epoch 436 :: Batch 0/1
Batch Loss = 0.0007309028493800284
436, epoch_train_loss=0.0007309028493800284
Epoch 437
Epoch 437 :: Batch 0/1
Batch Loss = 0.0007309028493800284
437, epoch_train_loss=0.0007309028493800284
Epoch 438
Epoch 438 :: Batch 0/1
Batch Loss = 0.0007309028493800284
438, epoch_train_loss=0.0007309028493800284
Epoch 439
Epoch 439 :: Batch 0/1
Batch Loss = 0.0007309028493800284
439, epoch_train_loss=0.0007309028493800284
Epoch 440
Epoch 440 :: Batch 0/1
Batch Loss = 0.0007309028493800284
440, epoch_train_loss=0.0007309028493800284
Epoch 441
Epoch 441 :: Batch 0/1
Batch Loss = 0.0007309028493800284
441, epoch_train_loss=0.0007309028493800284
Epoch 442
Epoch 442 :: Batch 0/1
Batch Loss = 0.0007309028493800284
442, epoch_train_loss=0.0007309028493800284
Epoch 443
Epoch 443 :: Batch 0/1
Batch Loss = 0.0007309028493800283
443, epoch_train_loss=0.0007309028493800283
Epoch 444
Epoch 444 :: Batch 0/1
Batch Loss = 0.0007309028493800283
444, epoch_train_loss=0.0007309028493800283
Epoch 445
Epoch 445 :: Batch 0/1
Batch Loss = 0.0007309028493800283
445, epoch_train_loss=0.0007309028493800283
Epoch 446
Epoch 446 :: Batch 0/1
Batch Loss = 0.0007309028493800283
446, epoch_train_loss=0.0007309028493800283
Epoch 447
Epoch 447 :: Batch 0/1
Batch Loss = 0.0007309028493800283
447, epoch_train_loss=0.0007309028493800283
Epoch 448
Epoch 448 :: Batch 0/1
Batch Loss = 0.0007309028493800283
448, epoch_train_loss=0.0007309028493800283
Epoch 449
Epoch 449 :: Batch 0/1
Batch Loss = 0.0007309028493800283
449, epoch_train_loss=0.0007309028493800283
Epoch 450
Epoch 450 :: Batch 0/1
Batch Loss = 0.0007309028493800283
450, epoch_train_loss=0.0007309028493800283
Epoch 451
Epoch 451 :: Batch 0/1
Batch Loss = 0.0007309028493800282
451, epoch_train_loss=0.0007309028493800282
Epoch 452
Epoch 452 :: Batch 0/1
Batch Loss = 0.0007309028493800282
452, epoch_train_loss=0.0007309028493800282
Epoch 453
Epoch 453 :: Batch 0/1
Batch Loss = 0.0007309028493800282
453, epoch_train_loss=0.0007309028493800282
Epoch 454
Epoch 454 :: Batch 0/1
Batch Loss = 0.0007309028493800282
454, epoch_train_loss=0.0007309028493800282
Epoch 455
Epoch 455 :: Batch 0/1
Batch Loss = 0.0007309028493800282
455, epoch_train_loss=0.0007309028493800282
Epoch 456
Epoch 456 :: Batch 0/1
Batch Loss = 0.0007309028493800282
456, epoch_train_loss=0.0007309028493800282
Epoch 457
Epoch 457 :: Batch 0/1
Batch Loss = 0.0007309028493800282
457, epoch_train_loss=0.0007309028493800282
Epoch 458
Epoch 458 :: Batch 0/1
Batch Loss = 0.0007309028493800282
458, epoch_train_loss=0.0007309028493800282
Epoch 459
Epoch 459 :: Batch 0/1
Batch Loss = 0.0007309028493800282
459, epoch_train_loss=0.0007309028493800282
Epoch 460
Epoch 460 :: Batch 0/1
Batch Loss = 0.0007309028493800282
460, epoch_train_loss=0.0007309028493800282
Epoch 461
Epoch 461 :: Batch 0/1
Batch Loss = 0.0007309028493800282
461, epoch_train_loss=0.0007309028493800282
Epoch 462
Epoch 462 :: Batch 0/1
Batch Loss = 0.0007309028493800282
462, epoch_train_loss=0.0007309028493800282
Epoch 463
Epoch 463 :: Batch 0/1
Batch Loss = 0.0007309028493800281
463, epoch_train_loss=0.0007309028493800281
Epoch 464
Epoch 464 :: Batch 0/1
Batch Loss = 0.0007309028493800281
464, epoch_train_loss=0.0007309028493800281
Epoch 465
Epoch 465 :: Batch 0/1
Batch Loss = 0.0007309028493800281
465, epoch_train_loss=0.0007309028493800281
Epoch 466
Epoch 466 :: Batch 0/1
Batch Loss = 0.0007309028493800281
466, epoch_train_loss=0.0007309028493800281
Epoch 467
Epoch 467 :: Batch 0/1
Batch Loss = 0.0007309028493800281
467, epoch_train_loss=0.0007309028493800281
Epoch 468
Epoch 468 :: Batch 0/1
Batch Loss = 0.0007309028493800281
468, epoch_train_loss=0.0007309028493800281
Epoch 469
Epoch 469 :: Batch 0/1
Batch Loss = 0.0007309028493800281
469, epoch_train_loss=0.0007309028493800281
Epoch 470
Epoch 470 :: Batch 0/1
Batch Loss = 0.000730902849380028
470, epoch_train_loss=0.000730902849380028
Epoch 471
Epoch 471 :: Batch 0/1
Batch Loss = 0.000730902849380028
471, epoch_train_loss=0.000730902849380028
Epoch 472
Epoch 472 :: Batch 0/1
Batch Loss = 0.000730902849380028
472, epoch_train_loss=0.000730902849380028
Epoch 473
Epoch 473 :: Batch 0/1
Batch Loss = 0.000730902849380028
473, epoch_train_loss=0.000730902849380028
Epoch 474
Epoch 474 :: Batch 0/1
Batch Loss = 0.000730902849380028
474, epoch_train_loss=0.000730902849380028
Epoch 475
Epoch 475 :: Batch 0/1
Batch Loss = 0.000730902849380028
475, epoch_train_loss=0.000730902849380028
Epoch 476
Epoch 476 :: Batch 0/1
Batch Loss = 0.000730902849380028
476, epoch_train_loss=0.000730902849380028
Epoch 477
Epoch 477 :: Batch 0/1
Batch Loss = 0.000730902849380028
477, epoch_train_loss=0.000730902849380028
Epoch 478
Epoch 478 :: Batch 0/1
Batch Loss = 0.000730902849380028
478, epoch_train_loss=0.000730902849380028
Epoch 479
Epoch 479 :: Batch 0/1
Batch Loss = 0.000730902849380028
479, epoch_train_loss=0.000730902849380028
Epoch 480
Epoch 480 :: Batch 0/1
Batch Loss = 0.000730902849380028
480, epoch_train_loss=0.000730902849380028
Epoch 481
Epoch 481 :: Batch 0/1
Batch Loss = 0.000730902849380028
481, epoch_train_loss=0.000730902849380028
Epoch 482
Epoch 482 :: Batch 0/1
Batch Loss = 0.0007309028493800279
482, epoch_train_loss=0.0007309028493800279
Epoch 483
Epoch 483 :: Batch 0/1
Batch Loss = 0.0007309028493800279
483, epoch_train_loss=0.0007309028493800279
Epoch 484
Epoch 484 :: Batch 0/1
Batch Loss = 0.0007309028493800278
484, epoch_train_loss=0.0007309028493800278
Epoch 485
Epoch 485 :: Batch 0/1
Batch Loss = 0.0007309028493800278
485, epoch_train_loss=0.0007309028493800278
Epoch 486
Epoch 486 :: Batch 0/1
Batch Loss = 0.0007309028493800278
486, epoch_train_loss=0.0007309028493800278
Epoch 487
Epoch 487 :: Batch 0/1
Batch Loss = 0.0007309028493800278
487, epoch_train_loss=0.0007309028493800278
Epoch 488
Epoch 488 :: Batch 0/1
Batch Loss = 0.0007309028493800278
488, epoch_train_loss=0.0007309028493800278
Epoch 489
Epoch 489 :: Batch 0/1
Batch Loss = 0.0007309028493800278
489, epoch_train_loss=0.0007309028493800278
Epoch 490
Epoch 490 :: Batch 0/1
Batch Loss = 0.0007309028493800278
490, epoch_train_loss=0.0007309028493800278
Epoch 491
Epoch 491 :: Batch 0/1
Batch Loss = 0.0007309028493800278
491, epoch_train_loss=0.0007309028493800278
Epoch 492
Epoch 492 :: Batch 0/1
Batch Loss = 0.0007309028493800278
492, epoch_train_loss=0.0007309028493800278
Epoch 493
Epoch 493 :: Batch 0/1
Batch Loss = 0.0007309028493800278
493, epoch_train_loss=0.0007309028493800278
Epoch 494
Epoch 494 :: Batch 0/1
Batch Loss = 0.0007309028493800278
494, epoch_train_loss=0.0007309028493800278
Epoch 495
Epoch 495 :: Batch 0/1
Batch Loss = 0.0007309028493800278
495, epoch_train_loss=0.0007309028493800278
Epoch 496
Epoch 496 :: Batch 0/1
Batch Loss = 0.0007309028493800276
496, epoch_train_loss=0.0007309028493800276
Epoch 497
Epoch 497 :: Batch 0/1
Batch Loss = 0.0007309028493800275
497, epoch_train_loss=0.0007309028493800275
Epoch 498
Epoch 498 :: Batch 0/1
Batch Loss = 0.0007309028493800275
498, epoch_train_loss=0.0007309028493800275
Epoch 499
Epoch 499 :: Batch 0/1
Batch Loss = 0.0007309028493800275
499, epoch_train_loss=0.0007309028493800275
Epoch 500
Epoch 500 :: Batch 0/1
Batch Loss = 0.0007309028493800275
500, epoch_train_loss=0.0007309028493800275
Epoch 501
Epoch 501 :: Batch 0/1
Batch Loss = 0.0007309028493800275
501, epoch_train_loss=0.0007309028493800275
Epoch 502
Epoch 502 :: Batch 0/1
Batch Loss = 0.0007309028493800275
502, epoch_train_loss=0.0007309028493800275
Epoch 503
Epoch 503 :: Batch 0/1
Batch Loss = 0.0007309028493800275
503, epoch_train_loss=0.0007309028493800275
Epoch 504
Epoch 504 :: Batch 0/1
Batch Loss = 0.0007309028493800275
504, epoch_train_loss=0.0007309028493800275
Epoch 505
Epoch 505 :: Batch 0/1
Batch Loss = 0.0007309028493800275
505, epoch_train_loss=0.0007309028493800275
Epoch 506
Epoch 506 :: Batch 0/1
Batch Loss = 0.0007309028493800275
506, epoch_train_loss=0.0007309028493800275
Epoch 507
Epoch 507 :: Batch 0/1
Batch Loss = 0.0007309028493800275
507, epoch_train_loss=0.0007309028493800275
Epoch 508
Epoch 508 :: Batch 0/1
Batch Loss = 0.0007309028493800275
508, epoch_train_loss=0.0007309028493800275
Epoch 509
Epoch 509 :: Batch 0/1
Batch Loss = 0.0007309028493800275
509, epoch_train_loss=0.0007309028493800275
Epoch 510
Epoch 510 :: Batch 0/1
Batch Loss = 0.0007309028493800275
510, epoch_train_loss=0.0007309028493800275
Epoch 511
Epoch 511 :: Batch 0/1
Batch Loss = 0.0007309028493800275
511, epoch_train_loss=0.0007309028493800275
Epoch 512
Epoch 512 :: Batch 0/1
Batch Loss = 0.0007309028493800273
512, epoch_train_loss=0.0007309028493800273
Epoch 513
Epoch 513 :: Batch 0/1
Batch Loss = 0.0007309028493800273
513, epoch_train_loss=0.0007309028493800273
Epoch 514
Epoch 514 :: Batch 0/1
Batch Loss = 0.0007309028493800272
514, epoch_train_loss=0.0007309028493800272
Epoch 515
Epoch 515 :: Batch 0/1
Batch Loss = 0.0007309028493800272
515, epoch_train_loss=0.0007309028493800272
Epoch 516
Epoch 516 :: Batch 0/1
Batch Loss = 0.0007309028493800272
516, epoch_train_loss=0.0007309028493800272
Epoch 517
Epoch 517 :: Batch 0/1
Batch Loss = 0.0007309028493800272
517, epoch_train_loss=0.0007309028493800272
Epoch 518
Epoch 518 :: Batch 0/1
Batch Loss = 0.0007309028493800272
518, epoch_train_loss=0.0007309028493800272
Epoch 519
Epoch 519 :: Batch 0/1
Batch Loss = 0.0007309028493800272
519, epoch_train_loss=0.0007309028493800272
Epoch 520
Epoch 520 :: Batch 0/1
Batch Loss = 0.0007309028493800272
520, epoch_train_loss=0.0007309028493800272
Epoch 521
Epoch 521 :: Batch 0/1
Batch Loss = 0.0007309028493800272
521, epoch_train_loss=0.0007309028493800272
Epoch 522
Epoch 522 :: Batch 0/1
Batch Loss = 0.0007309028493800272
522, epoch_train_loss=0.0007309028493800272
Epoch 523
Epoch 523 :: Batch 0/1
Batch Loss = 0.0007309028493800272
523, epoch_train_loss=0.0007309028493800272
Epoch 524
Epoch 524 :: Batch 0/1
Batch Loss = 0.0007309028493800272
524, epoch_train_loss=0.0007309028493800272
Epoch 525
Epoch 525 :: Batch 0/1
Batch Loss = 0.0007309028493800271
525, epoch_train_loss=0.0007309028493800271
Epoch 526
Epoch 526 :: Batch 0/1
Batch Loss = 0.0007309028493800271
526, epoch_train_loss=0.0007309028493800271
Epoch 527
Epoch 527 :: Batch 0/1
Batch Loss = 0.0007309028493800271
527, epoch_train_loss=0.0007309028493800271
Epoch 528
Epoch 528 :: Batch 0/1
Batch Loss = 0.0007309028493800271
528, epoch_train_loss=0.0007309028493800271
Epoch 529
Epoch 529 :: Batch 0/1
Batch Loss = 0.0007309028493800271
529, epoch_train_loss=0.0007309028493800271
Epoch 530
Epoch 530 :: Batch 0/1
Batch Loss = 0.000730902849380027
530, epoch_train_loss=0.000730902849380027
Epoch 531
Epoch 531 :: Batch 0/1
Batch Loss = 0.000730902849380027
531, epoch_train_loss=0.000730902849380027
Epoch 532
Epoch 532 :: Batch 0/1
Batch Loss = 0.000730902849380027
532, epoch_train_loss=0.000730902849380027
Epoch 533
Epoch 533 :: Batch 0/1
Batch Loss = 0.000730902849380027
533, epoch_train_loss=0.000730902849380027
Epoch 534
Epoch 534 :: Batch 0/1
Batch Loss = 0.000730902849380027
534, epoch_train_loss=0.000730902849380027
Epoch 535
Epoch 535 :: Batch 0/1
Batch Loss = 0.000730902849380027
535, epoch_train_loss=0.000730902849380027
Epoch 536
Epoch 536 :: Batch 0/1
Batch Loss = 0.000730902849380027
536, epoch_train_loss=0.000730902849380027
Epoch 537
Epoch 537 :: Batch 0/1
Batch Loss = 0.000730902849380027
537, epoch_train_loss=0.000730902849380027
Epoch 538
Epoch 538 :: Batch 0/1
Batch Loss = 0.000730902849380027
538, epoch_train_loss=0.000730902849380027
Epoch 539
Epoch 539 :: Batch 0/1
Batch Loss = 0.000730902849380027
539, epoch_train_loss=0.000730902849380027
Epoch 540
Epoch 540 :: Batch 0/1
Batch Loss = 0.000730902849380027
540, epoch_train_loss=0.000730902849380027
Epoch 541
Epoch 541 :: Batch 0/1
Batch Loss = 0.0007309028493800269
541, epoch_train_loss=0.0007309028493800269
Epoch 542
Epoch 542 :: Batch 0/1
Batch Loss = 0.0007309028493800269
542, epoch_train_loss=0.0007309028493800269
Epoch 543
Epoch 543 :: Batch 0/1
Batch Loss = 0.0007309028493800269
543, epoch_train_loss=0.0007309028493800269
Epoch 544
Epoch 544 :: Batch 0/1
Batch Loss = 0.0007309028493800269
544, epoch_train_loss=0.0007309028493800269
Epoch 545
Epoch 545 :: Batch 0/1
Batch Loss = 0.0007309028493800268
545, epoch_train_loss=0.0007309028493800268
Epoch 546
Epoch 546 :: Batch 0/1
Batch Loss = 0.0007309028493800268
546, epoch_train_loss=0.0007309028493800268
Epoch 547
Epoch 547 :: Batch 0/1
Batch Loss = 0.0007309028493800268
547, epoch_train_loss=0.0007309028493800268
Epoch 548
Epoch 548 :: Batch 0/1
Batch Loss = 0.0007309028493800268
548, epoch_train_loss=0.0007309028493800268
Epoch 549
Epoch 549 :: Batch 0/1
Batch Loss = 0.0007309028493800268
549, epoch_train_loss=0.0007309028493800268
Epoch 550
Epoch 550 :: Batch 0/1
Batch Loss = 0.0007309028493800268
550, epoch_train_loss=0.0007309028493800268
Epoch 551
Epoch 551 :: Batch 0/1
Batch Loss = 0.0007309028493800268
551, epoch_train_loss=0.0007309028493800268
Epoch 552
Epoch 552 :: Batch 0/1
Batch Loss = 0.0007309028493800268
552, epoch_train_loss=0.0007309028493800268
Epoch 553
Epoch 553 :: Batch 0/1
Batch Loss = 0.0007309028493800268
553, epoch_train_loss=0.0007309028493800268
Epoch 554
Epoch 554 :: Batch 0/1
Batch Loss = 0.0007309028493800267
554, epoch_train_loss=0.0007309028493800267
Epoch 555
Epoch 555 :: Batch 0/1
Batch Loss = 0.0007309028493800267
555, epoch_train_loss=0.0007309028493800267
Epoch 556
Epoch 556 :: Batch 0/1
Batch Loss = 0.0007309028493800267
556, epoch_train_loss=0.0007309028493800267
Epoch 557
Epoch 557 :: Batch 0/1
Batch Loss = 0.0007309028493800266
557, epoch_train_loss=0.0007309028493800266
Epoch 558
Epoch 558 :: Batch 0/1
Batch Loss = 0.0007309028493800266
558, epoch_train_loss=0.0007309028493800266
Epoch 559
Epoch 559 :: Batch 0/1
Batch Loss = 0.0007309028493800266
559, epoch_train_loss=0.0007309028493800266
Epoch 560
Epoch 560 :: Batch 0/1
Batch Loss = 0.0007309028493800266
560, epoch_train_loss=0.0007309028493800266
Epoch 561
Epoch 561 :: Batch 0/1
Batch Loss = 0.0007309028493800266
561, epoch_train_loss=0.0007309028493800266
Epoch 562
Epoch 562 :: Batch 0/1
Batch Loss = 0.0007309028493800266
562, epoch_train_loss=0.0007309028493800266
Epoch 563
Epoch 563 :: Batch 0/1
Batch Loss = 0.0007309028493800266
563, epoch_train_loss=0.0007309028493800266
Epoch 564
Epoch 564 :: Batch 0/1
Batch Loss = 0.0007309028493800266
564, epoch_train_loss=0.0007309028493800266
Epoch 565
Epoch 565 :: Batch 0/1
Batch Loss = 0.0007309028493800266
565, epoch_train_loss=0.0007309028493800266
Epoch 566
Epoch 566 :: Batch 0/1
Batch Loss = 0.0007309028493800266
566, epoch_train_loss=0.0007309028493800266
Epoch 567
Epoch 567 :: Batch 0/1
Batch Loss = 0.0007309028493800266
567, epoch_train_loss=0.0007309028493800266
Epoch 568
Epoch 568 :: Batch 0/1
Batch Loss = 0.0007309028493800266
568, epoch_train_loss=0.0007309028493800266
Epoch 569
Epoch 569 :: Batch 0/1
Batch Loss = 0.0007309028493800265
569, epoch_train_loss=0.0007309028493800265
Epoch 570
Epoch 570 :: Batch 0/1
Batch Loss = 0.0007309028493800262
570, epoch_train_loss=0.0007309028493800262
Epoch 571
Epoch 571 :: Batch 0/1
Batch Loss = 0.0007309028493800262
571, epoch_train_loss=0.0007309028493800262
Epoch 572
Epoch 572 :: Batch 0/1
Batch Loss = 0.0007309028493800262
572, epoch_train_loss=0.0007309028493800262
Epoch 573
Epoch 573 :: Batch 0/1
Batch Loss = 0.0007309028493800262
573, epoch_train_loss=0.0007309028493800262
Epoch 574
Epoch 574 :: Batch 0/1
Batch Loss = 0.0007309028493800262
574, epoch_train_loss=0.0007309028493800262
Epoch 575
Epoch 575 :: Batch 0/1
Batch Loss = 0.0007309028493800262
575, epoch_train_loss=0.0007309028493800262
Epoch 576
Epoch 576 :: Batch 0/1
Batch Loss = 0.0007309028493800262
576, epoch_train_loss=0.0007309028493800262
Epoch 577
Epoch 577 :: Batch 0/1
Batch Loss = 0.0007309028493800262
577, epoch_train_loss=0.0007309028493800262
Epoch 578
Epoch 578 :: Batch 0/1
Batch Loss = 0.0007309028493800262
578, epoch_train_loss=0.0007309028493800262
Epoch 579
Epoch 579 :: Batch 0/1
Batch Loss = 0.0007309028493800262
579, epoch_train_loss=0.0007309028493800262
Epoch 580
Epoch 580 :: Batch 0/1
Batch Loss = 0.0007309028493800262
580, epoch_train_loss=0.0007309028493800262
Epoch 581
Epoch 581 :: Batch 0/1
Batch Loss = 0.0007309028493800262
581, epoch_train_loss=0.0007309028493800262
Epoch 582
Epoch 582 :: Batch 0/1
Batch Loss = 0.0007309028493800262
582, epoch_train_loss=0.0007309028493800262
Epoch 583
Epoch 583 :: Batch 0/1
Batch Loss = 0.0007309028493800262
583, epoch_train_loss=0.0007309028493800262
Epoch 584
Epoch 584 :: Batch 0/1
Batch Loss = 0.0007309028493800261
584, epoch_train_loss=0.0007309028493800261
Epoch 585
Epoch 585 :: Batch 0/1
Batch Loss = 0.000730902849380026
585, epoch_train_loss=0.000730902849380026
Epoch 586
Epoch 586 :: Batch 0/1
Batch Loss = 0.000730902849380026
586, epoch_train_loss=0.000730902849380026
Epoch 587
Epoch 587 :: Batch 0/1
Batch Loss = 0.000730902849380026
587, epoch_train_loss=0.000730902849380026
Epoch 588
Epoch 588 :: Batch 0/1
Batch Loss = 0.000730902849380026
588, epoch_train_loss=0.000730902849380026
Epoch 589
Epoch 589 :: Batch 0/1
Batch Loss = 0.000730902849380026
589, epoch_train_loss=0.000730902849380026
Epoch 590
Epoch 590 :: Batch 0/1
Batch Loss = 0.000730902849380026
590, epoch_train_loss=0.000730902849380026
Epoch 591
Epoch 591 :: Batch 0/1
Batch Loss = 0.000730902849380026
591, epoch_train_loss=0.000730902849380026
Epoch 592
Epoch 592 :: Batch 0/1
Batch Loss = 0.000730902849380026
592, epoch_train_loss=0.000730902849380026
Epoch 593
Epoch 593 :: Batch 0/1
Batch Loss = 0.000730902849380026
593, epoch_train_loss=0.000730902849380026
Epoch 594
Epoch 594 :: Batch 0/1
Batch Loss = 0.000730902849380026
594, epoch_train_loss=0.000730902849380026
Epoch 595
Epoch 595 :: Batch 0/1
Batch Loss = 0.000730902849380026
595, epoch_train_loss=0.000730902849380026
Epoch 596
Epoch 596 :: Batch 0/1
Batch Loss = 0.000730902849380026
596, epoch_train_loss=0.000730902849380026
Epoch 597
Epoch 597 :: Batch 0/1
Batch Loss = 0.000730902849380026
597, epoch_train_loss=0.000730902849380026
Epoch 598
Epoch 598 :: Batch 0/1
Batch Loss = 0.000730902849380026
598, epoch_train_loss=0.000730902849380026
Epoch 599
Epoch 599 :: Batch 0/1
Batch Loss = 0.000730902849380026
599, epoch_train_loss=0.000730902849380026
Epoch 600
Epoch 600 :: Batch 0/1
Batch Loss = 0.0007309028493800258
600, epoch_train_loss=0.0007309028493800258
Epoch 601
Epoch 601 :: Batch 0/1
Batch Loss = 0.0007309028493800258
601, epoch_train_loss=0.0007309028493800258
Epoch 602
Epoch 602 :: Batch 0/1
Batch Loss = 0.0007309028493800258
602, epoch_train_loss=0.0007309028493800258
Epoch 603
Epoch 603 :: Batch 0/1
Batch Loss = 0.0007309028493800258
603, epoch_train_loss=0.0007309028493800258
Epoch 604
Epoch 604 :: Batch 0/1
Batch Loss = 0.0007309028493800258
604, epoch_train_loss=0.0007309028493800258
Epoch 605
Epoch 605 :: Batch 0/1
Batch Loss = 0.0007309028493800258
605, epoch_train_loss=0.0007309028493800258
Epoch 606
Epoch 606 :: Batch 0/1
Batch Loss = 0.0007309028493800258
606, epoch_train_loss=0.0007309028493800258
Epoch 607
Epoch 607 :: Batch 0/1
Batch Loss = 0.0007309028493800258
607, epoch_train_loss=0.0007309028493800258
Epoch 608
Epoch 608 :: Batch 0/1
Batch Loss = 0.0007309028493800258
608, epoch_train_loss=0.0007309028493800258
Epoch 609
Epoch 609 :: Batch 0/1
Batch Loss = 0.0007309028493800258
609, epoch_train_loss=0.0007309028493800258
Epoch 610
Epoch 610 :: Batch 0/1
Batch Loss = 0.0007309028493800258
610, epoch_train_loss=0.0007309028493800258
Epoch 611
Epoch 611 :: Batch 0/1
Batch Loss = 0.0007309028493800258
611, epoch_train_loss=0.0007309028493800258
Epoch 612
Epoch 612 :: Batch 0/1
Batch Loss = 0.0007309028493800258
612, epoch_train_loss=0.0007309028493800258
Epoch 613
Epoch 613 :: Batch 0/1
Batch Loss = 0.0007309028493800258
613, epoch_train_loss=0.0007309028493800258
Epoch 614
Epoch 614 :: Batch 0/1
Batch Loss = 0.0007309028493800257
614, epoch_train_loss=0.0007309028493800257
Epoch 615
Epoch 615 :: Batch 0/1
Batch Loss = 0.0007309028493800256
615, epoch_train_loss=0.0007309028493800256
Epoch 616
Epoch 616 :: Batch 0/1
Batch Loss = 0.0007309028493800256
616, epoch_train_loss=0.0007309028493800256
Epoch 617
Epoch 617 :: Batch 0/1
Batch Loss = 0.0007309028493800256
617, epoch_train_loss=0.0007309028493800256
Epoch 618
Epoch 618 :: Batch 0/1
Batch Loss = 0.0007309028493800256
618, epoch_train_loss=0.0007309028493800256
Epoch 619
Epoch 619 :: Batch 0/1
Batch Loss = 0.0007309028493800256
619, epoch_train_loss=0.0007309028493800256
Epoch 620
Epoch 620 :: Batch 0/1
Batch Loss = 0.0007309028493800256
620, epoch_train_loss=0.0007309028493800256
Epoch 621
Epoch 621 :: Batch 0/1
Batch Loss = 0.0007309028493800256
621, epoch_train_loss=0.0007309028493800256
Epoch 622
Epoch 622 :: Batch 0/1
Batch Loss = 0.0007309028493800256
622, epoch_train_loss=0.0007309028493800256
Epoch 623
Epoch 623 :: Batch 0/1
Batch Loss = 0.0007309028493800256
623, epoch_train_loss=0.0007309028493800256
Epoch 624
Epoch 624 :: Batch 0/1
Batch Loss = 0.0007309028493800256
624, epoch_train_loss=0.0007309028493800256
Epoch 625
Epoch 625 :: Batch 0/1
Batch Loss = 0.0007309028493800256
625, epoch_train_loss=0.0007309028493800256
Epoch 626
Epoch 626 :: Batch 0/1
Batch Loss = 0.0007309028493800254
626, epoch_train_loss=0.0007309028493800254
Epoch 627
Epoch 627 :: Batch 0/1
Batch Loss = 0.0007309028493800254
627, epoch_train_loss=0.0007309028493800254
Epoch 628
Epoch 628 :: Batch 0/1
Batch Loss = 0.0007309028493800254
628, epoch_train_loss=0.0007309028493800254
Epoch 629
Epoch 629 :: Batch 0/1
Batch Loss = 0.0007309028493800254
629, epoch_train_loss=0.0007309028493800254
Epoch 630
Epoch 630 :: Batch 0/1
Batch Loss = 0.0007309028493800254
630, epoch_train_loss=0.0007309028493800254
Epoch 631
Epoch 631 :: Batch 0/1
Batch Loss = 0.0007309028493800254
631, epoch_train_loss=0.0007309028493800254
Epoch 632
Epoch 632 :: Batch 0/1
Batch Loss = 0.0007309028493800254
632, epoch_train_loss=0.0007309028493800254
Epoch 633
Epoch 633 :: Batch 0/1
Batch Loss = 0.0007309028493800254
633, epoch_train_loss=0.0007309028493800254
Epoch 634
Epoch 634 :: Batch 0/1
Batch Loss = 0.0007309028493800254
634, epoch_train_loss=0.0007309028493800254
Epoch 635
Epoch 635 :: Batch 0/1
Batch Loss = 0.0007309028493800254
635, epoch_train_loss=0.0007309028493800254
Epoch 636
Epoch 636 :: Batch 0/1
Batch Loss = 0.0007309028493800254
636, epoch_train_loss=0.0007309028493800254
Epoch 637
Epoch 637 :: Batch 0/1
Batch Loss = 0.0007309028493800253
637, epoch_train_loss=0.0007309028493800253
Epoch 638
Epoch 638 :: Batch 0/1
Batch Loss = 0.0007309028493800253
638, epoch_train_loss=0.0007309028493800253
Epoch 639
Epoch 639 :: Batch 0/1
Batch Loss = 0.0007309028493800253
639, epoch_train_loss=0.0007309028493800253
Epoch 640
Epoch 640 :: Batch 0/1
Batch Loss = 0.0007309028493800253
640, epoch_train_loss=0.0007309028493800253
Epoch 641
Epoch 641 :: Batch 0/1
Batch Loss = 0.0007309028493800253
641, epoch_train_loss=0.0007309028493800253
Epoch 642
Epoch 642 :: Batch 0/1
Batch Loss = 0.000730902849380025
642, epoch_train_loss=0.000730902849380025
Epoch 643
Epoch 643 :: Batch 0/1
Batch Loss = 0.000730902849380025
643, epoch_train_loss=0.000730902849380025
Epoch 644
Epoch 644 :: Batch 0/1
Batch Loss = 0.000730902849380025
644, epoch_train_loss=0.000730902849380025
Epoch 645
Epoch 645 :: Batch 0/1
Batch Loss = 0.000730902849380025
645, epoch_train_loss=0.000730902849380025
Epoch 646
Epoch 646 :: Batch 0/1
Batch Loss = 0.000730902849380025
646, epoch_train_loss=0.000730902849380025
Epoch 647
Epoch 647 :: Batch 0/1
Batch Loss = 0.0007309028493800249
647, epoch_train_loss=0.0007309028493800249
Epoch 648
Epoch 648 :: Batch 0/1
Batch Loss = 0.0007309028493800249
648, epoch_train_loss=0.0007309028493800249
Epoch 649
Epoch 649 :: Batch 0/1
Batch Loss = 0.0007309028493800249
649, epoch_train_loss=0.0007309028493800249
Epoch 650
Epoch 650 :: Batch 0/1
Batch Loss = 0.0007309028493800249
650, epoch_train_loss=0.0007309028493800249
Epoch 651
Epoch 651 :: Batch 0/1
Batch Loss = 0.0007309028493800249
651, epoch_train_loss=0.0007309028493800249
Epoch 652
Epoch 652 :: Batch 0/1
Batch Loss = 0.0007309028493800249
652, epoch_train_loss=0.0007309028493800249
Epoch 653
Epoch 653 :: Batch 0/1
Batch Loss = 0.0007309028493800248
653, epoch_train_loss=0.0007309028493800248
Epoch 654
Epoch 654 :: Batch 0/1
Batch Loss = 0.0007309028493800248
654, epoch_train_loss=0.0007309028493800248
Epoch 655
Epoch 655 :: Batch 0/1
Batch Loss = 0.0007309028493800248
655, epoch_train_loss=0.0007309028493800248
Epoch 656
Epoch 656 :: Batch 0/1
Batch Loss = 0.0007309028493800248
656, epoch_train_loss=0.0007309028493800248
Epoch 657
Epoch 657 :: Batch 0/1
Batch Loss = 0.0007309028493800248
657, epoch_train_loss=0.0007309028493800248
Epoch 658
Epoch 658 :: Batch 0/1
Batch Loss = 0.0007309028493800248
658, epoch_train_loss=0.0007309028493800248
Epoch 659
Epoch 659 :: Batch 0/1
Batch Loss = 0.0007309028493800248
659, epoch_train_loss=0.0007309028493800248
Epoch 660
Epoch 660 :: Batch 0/1
Batch Loss = 0.0007309028493800248
660, epoch_train_loss=0.0007309028493800248
Epoch 661
Epoch 661 :: Batch 0/1
Batch Loss = 0.0007309028493800248
661, epoch_train_loss=0.0007309028493800248
Epoch 662
Epoch 662 :: Batch 0/1
Batch Loss = 0.0007309028493800247
662, epoch_train_loss=0.0007309028493800247
Epoch 663
Epoch 663 :: Batch 0/1
Batch Loss = 0.0007309028493800247
663, epoch_train_loss=0.0007309028493800247
Epoch 664
Epoch 664 :: Batch 0/1
Batch Loss = 0.0007309028493800247
664, epoch_train_loss=0.0007309028493800247
Epoch 665
Epoch 665 :: Batch 0/1
Batch Loss = 0.0007309028493800247
665, epoch_train_loss=0.0007309028493800247
Epoch 666
Epoch 666 :: Batch 0/1
Batch Loss = 0.0007309028493800247
666, epoch_train_loss=0.0007309028493800247
Epoch 667
Epoch 667 :: Batch 0/1
Batch Loss = 0.0007309028493800246
667, epoch_train_loss=0.0007309028493800246
Epoch 668
Epoch 668 :: Batch 0/1
Batch Loss = 0.0007309028493800246
668, epoch_train_loss=0.0007309028493800246
Epoch 669
Epoch 669 :: Batch 0/1
Batch Loss = 0.0007309028493800246
669, epoch_train_loss=0.0007309028493800246
Epoch 670
Epoch 670 :: Batch 0/1
Batch Loss = 0.0007309028493800246
670, epoch_train_loss=0.0007309028493800246
Epoch 671
Epoch 671 :: Batch 0/1
Batch Loss = 0.0007309028493800246
671, epoch_train_loss=0.0007309028493800246
Epoch 672
Epoch 672 :: Batch 0/1
Batch Loss = 0.0007309028493800246
672, epoch_train_loss=0.0007309028493800246
Epoch 673
Epoch 673 :: Batch 0/1
Batch Loss = 0.0007309028493800246
673, epoch_train_loss=0.0007309028493800246
Epoch 674
Epoch 674 :: Batch 0/1
Batch Loss = 0.0007309028493800246
674, epoch_train_loss=0.0007309028493800246
Epoch 675
Epoch 675 :: Batch 0/1
Batch Loss = 0.0007309028493800246
675, epoch_train_loss=0.0007309028493800246
Epoch 676
Epoch 676 :: Batch 0/1
Batch Loss = 0.0007309028493800245
676, epoch_train_loss=0.0007309028493800245
Epoch 677
Epoch 677 :: Batch 0/1
Batch Loss = 0.0007309028493800245
677, epoch_train_loss=0.0007309028493800245
Epoch 678
Epoch 678 :: Batch 0/1
Batch Loss = 0.0007309028493800245
678, epoch_train_loss=0.0007309028493800245
Epoch 679
Epoch 679 :: Batch 0/1
Batch Loss = 0.0007309028493800245
679, epoch_train_loss=0.0007309028493800245
Epoch 680
Epoch 680 :: Batch 0/1
Batch Loss = 0.0007309028493800245
680, epoch_train_loss=0.0007309028493800245
Epoch 681
Epoch 681 :: Batch 0/1
Batch Loss = 0.0007309028493800244
681, epoch_train_loss=0.0007309028493800244
Epoch 682
Epoch 682 :: Batch 0/1
Batch Loss = 0.0007309028493800244
682, epoch_train_loss=0.0007309028493800244
Epoch 683
Epoch 683 :: Batch 0/1
Batch Loss = 0.0007309028493800244
683, epoch_train_loss=0.0007309028493800244
Epoch 684
Epoch 684 :: Batch 0/1
Batch Loss = 0.0007309028493800244
684, epoch_train_loss=0.0007309028493800244
Epoch 685
Epoch 685 :: Batch 0/1
Batch Loss = 0.0007309028493800244
685, epoch_train_loss=0.0007309028493800244
Epoch 686
Epoch 686 :: Batch 0/1
Batch Loss = 0.0007309028493800244
686, epoch_train_loss=0.0007309028493800244
Epoch 687
Epoch 687 :: Batch 0/1
Batch Loss = 0.0007309028493800244
687, epoch_train_loss=0.0007309028493800244
Epoch 688
Epoch 688 :: Batch 0/1
Batch Loss = 0.0007309028493800243
688, epoch_train_loss=0.0007309028493800243
Epoch 689
Epoch 689 :: Batch 0/1
Batch Loss = 0.0007309028493800243
689, epoch_train_loss=0.0007309028493800243
Epoch 690
Epoch 690 :: Batch 0/1
Batch Loss = 0.0007309028493800243
690, epoch_train_loss=0.0007309028493800243
Epoch 691
Epoch 691 :: Batch 0/1
Batch Loss = 0.0007309028493800242
691, epoch_train_loss=0.0007309028493800242
Epoch 692
Epoch 692 :: Batch 0/1
Batch Loss = 0.0007309028493800242
692, epoch_train_loss=0.0007309028493800242
Epoch 693
Epoch 693 :: Batch 0/1
Batch Loss = 0.0007309028493800242
693, epoch_train_loss=0.0007309028493800242
Epoch 694
Epoch 694 :: Batch 0/1
Batch Loss = 0.0007309028493800242
694, epoch_train_loss=0.0007309028493800242
Epoch 695
Epoch 695 :: Batch 0/1
Batch Loss = 0.0007309028493800242
695, epoch_train_loss=0.0007309028493800242
Epoch 696
Epoch 696 :: Batch 0/1
Batch Loss = 0.0007309028493800242
696, epoch_train_loss=0.0007309028493800242
Epoch 697
Epoch 697 :: Batch 0/1
Batch Loss = 0.0007309028493800242
697, epoch_train_loss=0.0007309028493800242
Epoch 698
Epoch 698 :: Batch 0/1
Batch Loss = 0.0007309028493800242
698, epoch_train_loss=0.0007309028493800242
Epoch 699
Epoch 699 :: Batch 0/1
Batch Loss = 0.0007309028493800242
699, epoch_train_loss=0.0007309028493800242
Epoch 700
Epoch 700 :: Batch 0/1
Batch Loss = 0.000730902849380024
700, epoch_train_loss=0.000730902849380024
Epoch 701
Epoch 701 :: Batch 0/1
Batch Loss = 0.000730902849380024
701, epoch_train_loss=0.000730902849380024
Epoch 702
Epoch 702 :: Batch 0/1
Batch Loss = 0.000730902849380024
702, epoch_train_loss=0.000730902849380024
Epoch 703
Epoch 703 :: Batch 0/1
Batch Loss = 0.000730902849380024
703, epoch_train_loss=0.000730902849380024
Epoch 704
Epoch 704 :: Batch 0/1
Batch Loss = 0.000730902849380024
704, epoch_train_loss=0.000730902849380024
Epoch 705
Epoch 705 :: Batch 0/1
Batch Loss = 0.0007309028493800239
705, epoch_train_loss=0.0007309028493800239
Epoch 706
Epoch 706 :: Batch 0/1
Batch Loss = 0.0007309028493800239
706, epoch_train_loss=0.0007309028493800239
Epoch 707
Epoch 707 :: Batch 0/1
Batch Loss = 0.0007309028493800239
707, epoch_train_loss=0.0007309028493800239
Epoch 708
Epoch 708 :: Batch 0/1
Batch Loss = 0.0007309028493800239
708, epoch_train_loss=0.0007309028493800239
Epoch 709
Epoch 709 :: Batch 0/1
Batch Loss = 0.0007309028493800239
709, epoch_train_loss=0.0007309028493800239
Epoch 710
Epoch 710 :: Batch 0/1
Batch Loss = 0.0007309028493800239
710, epoch_train_loss=0.0007309028493800239
Epoch 711
Epoch 711 :: Batch 0/1
Batch Loss = 0.0007309028493800239
711, epoch_train_loss=0.0007309028493800239
Epoch 712
Epoch 712 :: Batch 0/1
Batch Loss = 0.0007309028493800239
712, epoch_train_loss=0.0007309028493800239
Epoch 713
Epoch 713 :: Batch 0/1
Batch Loss = 0.0007309028493800237
713, epoch_train_loss=0.0007309028493800237
Epoch 714
Epoch 714 :: Batch 0/1
Batch Loss = 0.0007309028493800237
714, epoch_train_loss=0.0007309028493800237
Epoch 715
Epoch 715 :: Batch 0/1
Batch Loss = 0.0007309028493800237
715, epoch_train_loss=0.0007309028493800237
Epoch 716
Epoch 716 :: Batch 0/1
Batch Loss = 0.0007309028493800237
716, epoch_train_loss=0.0007309028493800237
Epoch 717
Epoch 717 :: Batch 0/1
Batch Loss = 0.0007309028493800237
717, epoch_train_loss=0.0007309028493800237
Epoch 718
Epoch 718 :: Batch 0/1
Batch Loss = 0.0007309028493800236
718, epoch_train_loss=0.0007309028493800236
Epoch 719
Epoch 719 :: Batch 0/1
Batch Loss = 0.0007309028493800236
719, epoch_train_loss=0.0007309028493800236
Epoch 720
Epoch 720 :: Batch 0/1
Batch Loss = 0.0007309028493800236
720, epoch_train_loss=0.0007309028493800236
Epoch 721
Epoch 721 :: Batch 0/1
Batch Loss = 0.0007309028493800236
721, epoch_train_loss=0.0007309028493800236
Epoch 722
Epoch 722 :: Batch 0/1
Batch Loss = 0.0007309028493800236
722, epoch_train_loss=0.0007309028493800236
Epoch 723
Epoch 723 :: Batch 0/1
Batch Loss = 0.0007309028493800236
723, epoch_train_loss=0.0007309028493800236
Epoch 724
Epoch 724 :: Batch 0/1
Batch Loss = 0.0007309028493800236
724, epoch_train_loss=0.0007309028493800236
Epoch 725
Epoch 725 :: Batch 0/1
Batch Loss = 0.0007309028493800235
725, epoch_train_loss=0.0007309028493800235
Epoch 726
Epoch 726 :: Batch 0/1
Batch Loss = 0.0007309028493800235
726, epoch_train_loss=0.0007309028493800235
Epoch 727
Epoch 727 :: Batch 0/1
Batch Loss = 0.0007309028493800235
727, epoch_train_loss=0.0007309028493800235
Epoch 728
Epoch 728 :: Batch 0/1
Batch Loss = 0.0007309028493800235
728, epoch_train_loss=0.0007309028493800235
Epoch 729
Epoch 729 :: Batch 0/1
Batch Loss = 0.0007309028493800234
729, epoch_train_loss=0.0007309028493800234
Epoch 730
Epoch 730 :: Batch 0/1
Batch Loss = 0.0007309028493800234
730, epoch_train_loss=0.0007309028493800234
Epoch 731
Epoch 731 :: Batch 0/1
Batch Loss = 0.0007309028493800234
731, epoch_train_loss=0.0007309028493800234
Epoch 732
Epoch 732 :: Batch 0/1
Batch Loss = 0.0007309028493800234
732, epoch_train_loss=0.0007309028493800234
Epoch 733
Epoch 733 :: Batch 0/1
Batch Loss = 0.0007309028493800234
733, epoch_train_loss=0.0007309028493800234
Epoch 734
Epoch 734 :: Batch 0/1
Batch Loss = 0.0007309028493800234
734, epoch_train_loss=0.0007309028493800234
Epoch 735
Epoch 735 :: Batch 0/1
Batch Loss = 0.0007309028493800234
735, epoch_train_loss=0.0007309028493800234
Epoch 736
Epoch 736 :: Batch 0/1
Batch Loss = 0.0007309028493800234
736, epoch_train_loss=0.0007309028493800234
Epoch 737
Epoch 737 :: Batch 0/1
Batch Loss = 0.0007309028493800234
737, epoch_train_loss=0.0007309028493800234
Epoch 738
Epoch 738 :: Batch 0/1
Batch Loss = 0.0007309028493800234
738, epoch_train_loss=0.0007309028493800234
Epoch 739
Epoch 739 :: Batch 0/1
Batch Loss = 0.0007309028493800234
739, epoch_train_loss=0.0007309028493800234
Epoch 740
Epoch 740 :: Batch 0/1
Batch Loss = 0.0007309028493800233
740, epoch_train_loss=0.0007309028493800233
Epoch 741
Epoch 741 :: Batch 0/1
Batch Loss = 0.0007309028493800232
741, epoch_train_loss=0.0007309028493800232
Epoch 742
Epoch 742 :: Batch 0/1
Batch Loss = 0.0007309028493800232
742, epoch_train_loss=0.0007309028493800232
Epoch 743
Epoch 743 :: Batch 0/1
Batch Loss = 0.0007309028493800232
743, epoch_train_loss=0.0007309028493800232
Epoch 744
Epoch 744 :: Batch 0/1
Batch Loss = 0.0007309028493800232
744, epoch_train_loss=0.0007309028493800232
Epoch 745
Epoch 745 :: Batch 0/1
Batch Loss = 0.0007309028493800232
745, epoch_train_loss=0.0007309028493800232
Epoch 746
Epoch 746 :: Batch 0/1
Batch Loss = 0.0007309028493800232
746, epoch_train_loss=0.0007309028493800232
Epoch 747
Epoch 747 :: Batch 0/1
Batch Loss = 0.0007309028493800232
747, epoch_train_loss=0.0007309028493800232
Epoch 748
Epoch 748 :: Batch 0/1
Batch Loss = 0.0007309028493800232
748, epoch_train_loss=0.0007309028493800232
Epoch 749
Epoch 749 :: Batch 0/1
Batch Loss = 0.0007309028493800231
749, epoch_train_loss=0.0007309028493800231
Epoch 750
Epoch 750 :: Batch 0/1
Batch Loss = 0.0007309028493800231
750, epoch_train_loss=0.0007309028493800231
Epoch 751
Epoch 751 :: Batch 0/1
Batch Loss = 0.0007309028493800231
751, epoch_train_loss=0.0007309028493800231
Epoch 752
Epoch 752 :: Batch 0/1
Batch Loss = 0.0007309028493800231
752, epoch_train_loss=0.0007309028493800231
Epoch 753
Epoch 753 :: Batch 0/1
Batch Loss = 0.0007309028493800229
753, epoch_train_loss=0.0007309028493800229
Epoch 754
Epoch 754 :: Batch 0/1
Batch Loss = 0.0007309028493800229
754, epoch_train_loss=0.0007309028493800229
Epoch 755
Epoch 755 :: Batch 0/1
Batch Loss = 0.0007309028493800229
755, epoch_train_loss=0.0007309028493800229
Epoch 756
Epoch 756 :: Batch 0/1
Batch Loss = 0.0007309028493800229
756, epoch_train_loss=0.0007309028493800229
Epoch 757
Epoch 757 :: Batch 0/1
Batch Loss = 0.0007309028493800229
757, epoch_train_loss=0.0007309028493800229
Epoch 758
Epoch 758 :: Batch 0/1
Batch Loss = 0.0007309028493800229
758, epoch_train_loss=0.0007309028493800229
Epoch 759
Epoch 759 :: Batch 0/1
Batch Loss = 0.0007309028493800229
759, epoch_train_loss=0.0007309028493800229
Epoch 760
Epoch 760 :: Batch 0/1
Batch Loss = 0.0007309028493800229
760, epoch_train_loss=0.0007309028493800229
Epoch 761
Epoch 761 :: Batch 0/1
Batch Loss = 0.0007309028493800229
761, epoch_train_loss=0.0007309028493800229
Epoch 762
Epoch 762 :: Batch 0/1
Batch Loss = 0.0007309028493800228
762, epoch_train_loss=0.0007309028493800228
Epoch 763
Epoch 763 :: Batch 0/1
Batch Loss = 0.0007309028493800228
763, epoch_train_loss=0.0007309028493800228
Epoch 764
Epoch 764 :: Batch 0/1
Batch Loss = 0.0007309028493800227
764, epoch_train_loss=0.0007309028493800227
Epoch 765
Epoch 765 :: Batch 0/1
Batch Loss = 0.0007309028493800227
765, epoch_train_loss=0.0007309028493800227
Epoch 766
Epoch 766 :: Batch 0/1
Batch Loss = 0.0007309028493800227
766, epoch_train_loss=0.0007309028493800227
Epoch 767
Epoch 767 :: Batch 0/1
Batch Loss = 0.0007309028493800227
767, epoch_train_loss=0.0007309028493800227
Epoch 768
Epoch 768 :: Batch 0/1
Batch Loss = 0.0007309028493800227
768, epoch_train_loss=0.0007309028493800227
Epoch 769
Epoch 769 :: Batch 0/1
Batch Loss = 0.0007309028493800227
769, epoch_train_loss=0.0007309028493800227
Epoch 770
Epoch 770 :: Batch 0/1
Batch Loss = 0.0007309028493800227
770, epoch_train_loss=0.0007309028493800227
Epoch 771
Epoch 771 :: Batch 0/1
Batch Loss = 0.0007309028493800227
771, epoch_train_loss=0.0007309028493800227
Epoch 772
Epoch 772 :: Batch 0/1
Batch Loss = 0.0007309028493800226
772, epoch_train_loss=0.0007309028493800226
Epoch 773
Epoch 773 :: Batch 0/1
Batch Loss = 0.0007309028493800226
773, epoch_train_loss=0.0007309028493800226
Epoch 774
Epoch 774 :: Batch 0/1
Batch Loss = 0.0007309028493800226
774, epoch_train_loss=0.0007309028493800226
Epoch 775
Epoch 775 :: Batch 0/1
Batch Loss = 0.0007309028493800224
775, epoch_train_loss=0.0007309028493800224
Epoch 776
Epoch 776 :: Batch 0/1
Batch Loss = 0.0007309028493800224
776, epoch_train_loss=0.0007309028493800224
Epoch 777
Epoch 777 :: Batch 0/1
Batch Loss = 0.0007309028493800224
777, epoch_train_loss=0.0007309028493800224
Epoch 778
Epoch 778 :: Batch 0/1
Batch Loss = 0.0007309028493800224
778, epoch_train_loss=0.0007309028493800224
Epoch 779
Epoch 779 :: Batch 0/1
Batch Loss = 0.0007309028493800224
779, epoch_train_loss=0.0007309028493800224
Epoch 780
Epoch 780 :: Batch 0/1
Batch Loss = 0.0007309028493800224
780, epoch_train_loss=0.0007309028493800224
Epoch 781
Epoch 781 :: Batch 0/1
Batch Loss = 0.0007309028493800224
781, epoch_train_loss=0.0007309028493800224
Epoch 782
Epoch 782 :: Batch 0/1
Batch Loss = 0.0007309028493800224
782, epoch_train_loss=0.0007309028493800224
Epoch 783
Epoch 783 :: Batch 0/1
Batch Loss = 0.0007309028493800224
783, epoch_train_loss=0.0007309028493800224
Epoch 784
Epoch 784 :: Batch 0/1
Batch Loss = 0.0007309028493800224
784, epoch_train_loss=0.0007309028493800224
Epoch 785
Epoch 785 :: Batch 0/1
Batch Loss = 0.0007309028493800224
785, epoch_train_loss=0.0007309028493800224
Epoch 786
Epoch 786 :: Batch 0/1
Batch Loss = 0.0007309028493800222
786, epoch_train_loss=0.0007309028493800222
Epoch 787
Epoch 787 :: Batch 0/1
Batch Loss = 0.0007309028493800222
787, epoch_train_loss=0.0007309028493800222
Epoch 788
Epoch 788 :: Batch 0/1
Batch Loss = 0.0007309028493800222
788, epoch_train_loss=0.0007309028493800222
Epoch 789
Epoch 789 :: Batch 0/1
Batch Loss = 0.0007309028493800222
789, epoch_train_loss=0.0007309028493800222
Epoch 790
Epoch 790 :: Batch 0/1
Batch Loss = 0.0007309028493800222
790, epoch_train_loss=0.0007309028493800222
Epoch 791
Epoch 791 :: Batch 0/1
Batch Loss = 0.0007309028493800222
791, epoch_train_loss=0.0007309028493800222
Epoch 792
Epoch 792 :: Batch 0/1
Batch Loss = 0.0007309028493800222
792, epoch_train_loss=0.0007309028493800222
Epoch 793
Epoch 793 :: Batch 0/1
Batch Loss = 0.0007309028493800222
793, epoch_train_loss=0.0007309028493800222
Epoch 794
Epoch 794 :: Batch 0/1
Batch Loss = 0.0007309028493800222
794, epoch_train_loss=0.0007309028493800222
Epoch 795
Epoch 795 :: Batch 0/1
Batch Loss = 0.0007309028493800222
795, epoch_train_loss=0.0007309028493800222
Epoch 796
Epoch 796 :: Batch 0/1
Batch Loss = 0.0007309028493800222
796, epoch_train_loss=0.0007309028493800222
Epoch 797
Epoch 797 :: Batch 0/1
Batch Loss = 0.0007309028493800221
797, epoch_train_loss=0.0007309028493800221
Epoch 798
Epoch 798 :: Batch 0/1
Batch Loss = 0.0007309028493800221
798, epoch_train_loss=0.0007309028493800221
Epoch 799
Epoch 799 :: Batch 0/1
Batch Loss = 0.000730902849380022
799, epoch_train_loss=0.000730902849380022
Epoch 800
Epoch 800 :: Batch 0/1
Batch Loss = 0.000730902849380022
800, epoch_train_loss=0.000730902849380022
Epoch 801
Epoch 801 :: Batch 0/1
Batch Loss = 0.000730902849380022
801, epoch_train_loss=0.000730902849380022
Epoch 802
Epoch 802 :: Batch 0/1
Batch Loss = 0.000730902849380022
802, epoch_train_loss=0.000730902849380022
Epoch 803
Epoch 803 :: Batch 0/1
Batch Loss = 0.000730902849380022
803, epoch_train_loss=0.000730902849380022
Epoch 804
Epoch 804 :: Batch 0/1
Batch Loss = 0.000730902849380022
804, epoch_train_loss=0.000730902849380022
Epoch 805
Epoch 805 :: Batch 0/1
Batch Loss = 0.000730902849380022
805, epoch_train_loss=0.000730902849380022
Epoch 806
Epoch 806 :: Batch 0/1
Batch Loss = 0.000730902849380022
806, epoch_train_loss=0.000730902849380022
Epoch 807
Epoch 807 :: Batch 0/1
Batch Loss = 0.000730902849380022
807, epoch_train_loss=0.000730902849380022
Epoch 808
Epoch 808 :: Batch 0/1
Batch Loss = 0.0007309028493800219
808, epoch_train_loss=0.0007309028493800219
Epoch 809
Epoch 809 :: Batch 0/1
Batch Loss = 0.0007309028493800217
809, epoch_train_loss=0.0007309028493800217
Epoch 810
Epoch 810 :: Batch 0/1
Batch Loss = 0.0007309028493800217
810, epoch_train_loss=0.0007309028493800217
Epoch 811
Epoch 811 :: Batch 0/1
Batch Loss = 0.0007309028493800217
811, epoch_train_loss=0.0007309028493800217
Epoch 812
Epoch 812 :: Batch 0/1
Batch Loss = 0.0007309028493800217
812, epoch_train_loss=0.0007309028493800217
Epoch 813
Epoch 813 :: Batch 0/1
Batch Loss = 0.0007309028493800217
813, epoch_train_loss=0.0007309028493800217
Epoch 814
Epoch 814 :: Batch 0/1
Batch Loss = 0.0007309028493800217
814, epoch_train_loss=0.0007309028493800217
Epoch 815
Epoch 815 :: Batch 0/1
Batch Loss = 0.0007309028493800217
815, epoch_train_loss=0.0007309028493800217
Epoch 816
Epoch 816 :: Batch 0/1
Batch Loss = 0.0007309028493800217
816, epoch_train_loss=0.0007309028493800217
Epoch 817
Epoch 817 :: Batch 0/1
Batch Loss = 0.0007309028493800217
817, epoch_train_loss=0.0007309028493800217
Epoch 818
Epoch 818 :: Batch 0/1
Batch Loss = 0.0007309028493800216
818, epoch_train_loss=0.0007309028493800216
Epoch 819
Epoch 819 :: Batch 0/1
Batch Loss = 0.0007309028493800216
819, epoch_train_loss=0.0007309028493800216
Epoch 820
Epoch 820 :: Batch 0/1
Batch Loss = 0.0007309028493800216
820, epoch_train_loss=0.0007309028493800216
Epoch 821
Epoch 821 :: Batch 0/1
Batch Loss = 0.0007309028493800216
821, epoch_train_loss=0.0007309028493800216
Epoch 822
Epoch 822 :: Batch 0/1
Batch Loss = 0.0007309028493800215
822, epoch_train_loss=0.0007309028493800215
Epoch 823
Epoch 823 :: Batch 0/1
Batch Loss = 0.0007309028493800215
823, epoch_train_loss=0.0007309028493800215
Epoch 824
Epoch 824 :: Batch 0/1
Batch Loss = 0.0007309028493800215
824, epoch_train_loss=0.0007309028493800215
Epoch 825
Epoch 825 :: Batch 0/1
Batch Loss = 0.0007309028493800215
825, epoch_train_loss=0.0007309028493800215
Epoch 826
Epoch 826 :: Batch 0/1
Batch Loss = 0.0007309028493800215
826, epoch_train_loss=0.0007309028493800215
Epoch 827
Epoch 827 :: Batch 0/1
Batch Loss = 0.0007309028493800215
827, epoch_train_loss=0.0007309028493800215
Epoch 828
Epoch 828 :: Batch 0/1
Batch Loss = 0.0007309028493800215
828, epoch_train_loss=0.0007309028493800215
Epoch 829
Epoch 829 :: Batch 0/1
Batch Loss = 0.0007309028493800214
829, epoch_train_loss=0.0007309028493800214
Epoch 830
Epoch 830 :: Batch 0/1
Batch Loss = 0.0007309028493800214
830, epoch_train_loss=0.0007309028493800214
Epoch 831
Epoch 831 :: Batch 0/1
Batch Loss = 0.0007309028493800212
831, epoch_train_loss=0.0007309028493800212
Epoch 832
Epoch 832 :: Batch 0/1
Batch Loss = 0.0007309028493800212
832, epoch_train_loss=0.0007309028493800212
Epoch 833
Epoch 833 :: Batch 0/1
Batch Loss = 0.0007309028493800212
833, epoch_train_loss=0.0007309028493800212
Epoch 834
Epoch 834 :: Batch 0/1
Batch Loss = 0.0007309028493800212
834, epoch_train_loss=0.0007309028493800212
Epoch 835
Epoch 835 :: Batch 0/1
Batch Loss = 0.0007309028493800212
835, epoch_train_loss=0.0007309028493800212
Epoch 836
Epoch 836 :: Batch 0/1
Batch Loss = 0.0007309028493800212
836, epoch_train_loss=0.0007309028493800212
Epoch 837
Epoch 837 :: Batch 0/1
Batch Loss = 0.0007309028493800212
837, epoch_train_loss=0.0007309028493800212
Epoch 838
Epoch 838 :: Batch 0/1
Batch Loss = 0.0007309028493800212
838, epoch_train_loss=0.0007309028493800212
Epoch 839
Epoch 839 :: Batch 0/1
Batch Loss = 0.0007309028493800211
839, epoch_train_loss=0.0007309028493800211
Epoch 840
Epoch 840 :: Batch 0/1
Batch Loss = 0.0007309028493800211
840, epoch_train_loss=0.0007309028493800211
Epoch 841
Epoch 841 :: Batch 0/1
Batch Loss = 0.0007309028493800211
841, epoch_train_loss=0.0007309028493800211
Epoch 842
Epoch 842 :: Batch 0/1
Batch Loss = 0.0007309028493800211
842, epoch_train_loss=0.0007309028493800211
Epoch 843
Epoch 843 :: Batch 0/1
Batch Loss = 0.000730902849380021
843, epoch_train_loss=0.000730902849380021
Epoch 844
Epoch 844 :: Batch 0/1
Batch Loss = 0.000730902849380021
844, epoch_train_loss=0.000730902849380021
Epoch 845
Epoch 845 :: Batch 0/1
Batch Loss = 0.000730902849380021
845, epoch_train_loss=0.000730902849380021
Epoch 846
Epoch 846 :: Batch 0/1
Batch Loss = 0.000730902849380021
846, epoch_train_loss=0.000730902849380021
Epoch 847
Epoch 847 :: Batch 0/1
Batch Loss = 0.000730902849380021
847, epoch_train_loss=0.000730902849380021
Epoch 848
Epoch 848 :: Batch 0/1
Batch Loss = 0.000730902849380021
848, epoch_train_loss=0.000730902849380021
Epoch 849
Epoch 849 :: Batch 0/1
Batch Loss = 0.000730902849380021
849, epoch_train_loss=0.000730902849380021
Epoch 850
Epoch 850 :: Batch 0/1
Batch Loss = 0.000730902849380021
850, epoch_train_loss=0.000730902849380021
Epoch 851
Epoch 851 :: Batch 0/1
Batch Loss = 0.000730902849380021
851, epoch_train_loss=0.000730902849380021
Epoch 852
Epoch 852 :: Batch 0/1
Batch Loss = 0.0007309028493800209
852, epoch_train_loss=0.0007309028493800209
Epoch 853
Epoch 853 :: Batch 0/1
Batch Loss = 0.0007309028493800208
853, epoch_train_loss=0.0007309028493800208
Epoch 854
Epoch 854 :: Batch 0/1
Batch Loss = 0.0007309028493800208
854, epoch_train_loss=0.0007309028493800208
Epoch 855
Epoch 855 :: Batch 0/1
Batch Loss = 0.0007309028493800208
855, epoch_train_loss=0.0007309028493800208
Epoch 856
Epoch 856 :: Batch 0/1
Batch Loss = 0.0007309028493800208
856, epoch_train_loss=0.0007309028493800208
Epoch 857
Epoch 857 :: Batch 0/1
Batch Loss = 0.0007309028493800208
857, epoch_train_loss=0.0007309028493800208
Epoch 858
Epoch 858 :: Batch 0/1
Batch Loss = 0.0007309028493800208
858, epoch_train_loss=0.0007309028493800208
Epoch 859
Epoch 859 :: Batch 0/1
Batch Loss = 0.0007309028493800208
859, epoch_train_loss=0.0007309028493800208
Epoch 860
Epoch 860 :: Batch 0/1
Batch Loss = 0.0007309028493800208
860, epoch_train_loss=0.0007309028493800208
Epoch 861
Epoch 861 :: Batch 0/1
Batch Loss = 0.0007309028493800206
861, epoch_train_loss=0.0007309028493800206
Epoch 862
Epoch 862 :: Batch 0/1
Batch Loss = 0.0007309028493800205
862, epoch_train_loss=0.0007309028493800205
Epoch 863
Epoch 863 :: Batch 0/1
Batch Loss = 0.0007309028493800205
863, epoch_train_loss=0.0007309028493800205
Epoch 864
Epoch 864 :: Batch 0/1
Batch Loss = 0.0007309028493800205
864, epoch_train_loss=0.0007309028493800205
Epoch 865
Epoch 865 :: Batch 0/1
Batch Loss = 0.0007309028493800205
865, epoch_train_loss=0.0007309028493800205
Epoch 866
Epoch 866 :: Batch 0/1
Batch Loss = 0.0007309028493800205
866, epoch_train_loss=0.0007309028493800205
Epoch 867
Epoch 867 :: Batch 0/1
Batch Loss = 0.0007309028493800205
867, epoch_train_loss=0.0007309028493800205
Epoch 868
Epoch 868 :: Batch 0/1
Batch Loss = 0.0007309028493800205
868, epoch_train_loss=0.0007309028493800205
Epoch 869
Epoch 869 :: Batch 0/1
Batch Loss = 0.0007309028493800205
869, epoch_train_loss=0.0007309028493800205
Epoch 870
Epoch 870 :: Batch 0/1
Batch Loss = 0.0007309028493800205
870, epoch_train_loss=0.0007309028493800205
Epoch 871
Epoch 871 :: Batch 0/1
Batch Loss = 0.0007309028493800205
871, epoch_train_loss=0.0007309028493800205
Epoch 872
Epoch 872 :: Batch 0/1
Batch Loss = 0.0007309028493800204
872, epoch_train_loss=0.0007309028493800204
Epoch 873
Epoch 873 :: Batch 0/1
Batch Loss = 0.0007309028493800203
873, epoch_train_loss=0.0007309028493800203
Epoch 874
Epoch 874 :: Batch 0/1
Batch Loss = 0.0007309028493800203
874, epoch_train_loss=0.0007309028493800203
Epoch 875
Epoch 875 :: Batch 0/1
Batch Loss = 0.0007309028493800203
875, epoch_train_loss=0.0007309028493800203
Epoch 876
Epoch 876 :: Batch 0/1
Batch Loss = 0.0007309028493800203
876, epoch_train_loss=0.0007309028493800203
Epoch 877
Epoch 877 :: Batch 0/1
Batch Loss = 0.0007309028493800203
877, epoch_train_loss=0.0007309028493800203
Epoch 878
Epoch 878 :: Batch 0/1
Batch Loss = 0.0007309028493800203
878, epoch_train_loss=0.0007309028493800203
Epoch 879
Epoch 879 :: Batch 0/1
Batch Loss = 0.0007309028493800203
879, epoch_train_loss=0.0007309028493800203
Epoch 880
Epoch 880 :: Batch 0/1
Batch Loss = 0.0007309028493800203
880, epoch_train_loss=0.0007309028493800203
Epoch 881
Epoch 881 :: Batch 0/1
Batch Loss = 0.0007309028493800203
881, epoch_train_loss=0.0007309028493800203
Epoch 882
Epoch 882 :: Batch 0/1
Batch Loss = 0.0007309028493800202
882, epoch_train_loss=0.0007309028493800202
Epoch 883
Epoch 883 :: Batch 0/1
Batch Loss = 0.0007309028493800201
883, epoch_train_loss=0.0007309028493800201
Epoch 884
Epoch 884 :: Batch 0/1
Batch Loss = 0.0007309028493800201
884, epoch_train_loss=0.0007309028493800201
Epoch 885
Epoch 885 :: Batch 0/1
Batch Loss = 0.0007309028493800201
885, epoch_train_loss=0.0007309028493800201
Epoch 886
Epoch 886 :: Batch 0/1
Batch Loss = 0.0007309028493800201
886, epoch_train_loss=0.0007309028493800201
Epoch 887
Epoch 887 :: Batch 0/1
Batch Loss = 0.0007309028493800201
887, epoch_train_loss=0.0007309028493800201
Epoch 888
Epoch 888 :: Batch 0/1
Batch Loss = 0.0007309028493800201
888, epoch_train_loss=0.0007309028493800201
Epoch 889
Epoch 889 :: Batch 0/1
Batch Loss = 0.0007309028493800201
889, epoch_train_loss=0.0007309028493800201
Epoch 890
Epoch 890 :: Batch 0/1
Batch Loss = 0.0007309028493800201
890, epoch_train_loss=0.0007309028493800201
Epoch 891
Epoch 891 :: Batch 0/1
Batch Loss = 0.0007309028493800201
891, epoch_train_loss=0.0007309028493800201
Epoch 892
Epoch 892 :: Batch 0/1
Batch Loss = 0.0007309028493800201
892, epoch_train_loss=0.0007309028493800201
Epoch 893
Epoch 893 :: Batch 0/1
Batch Loss = 0.0007309028493800201
893, epoch_train_loss=0.0007309028493800201
Epoch 894
Epoch 894 :: Batch 0/1
Batch Loss = 0.0007309028493800198
894, epoch_train_loss=0.0007309028493800198
Epoch 895
Epoch 895 :: Batch 0/1
Batch Loss = 0.0007309028493800198
895, epoch_train_loss=0.0007309028493800198
Epoch 896
Epoch 896 :: Batch 0/1
Batch Loss = 0.0007309028493800198
896, epoch_train_loss=0.0007309028493800198
Epoch 897
Epoch 897 :: Batch 0/1
Batch Loss = 0.0007309028493800198
897, epoch_train_loss=0.0007309028493800198
Epoch 898
Epoch 898 :: Batch 0/1
Batch Loss = 0.0007309028493800198
898, epoch_train_loss=0.0007309028493800198
Epoch 899
Epoch 899 :: Batch 0/1
Batch Loss = 0.0007309028493800198
899, epoch_train_loss=0.0007309028493800198
Epoch 900
Epoch 900 :: Batch 0/1
Batch Loss = 0.0007309028493800198
900, epoch_train_loss=0.0007309028493800198
Epoch 901
Epoch 901 :: Batch 0/1
Batch Loss = 0.0007309028493800198
901, epoch_train_loss=0.0007309028493800198
Epoch 902
Epoch 902 :: Batch 0/1
Batch Loss = 0.0007309028493800198
902, epoch_train_loss=0.0007309028493800198
Epoch 903
Epoch 903 :: Batch 0/1
Batch Loss = 0.0007309028493800198
903, epoch_train_loss=0.0007309028493800198
Epoch 904
Epoch 904 :: Batch 0/1
Batch Loss = 0.0007309028493800198
904, epoch_train_loss=0.0007309028493800198
Epoch 905
Epoch 905 :: Batch 0/1
Batch Loss = 0.0007309028493800196
905, epoch_train_loss=0.0007309028493800196
Epoch 906
Epoch 906 :: Batch 0/1
Batch Loss = 0.0007309028493800196
906, epoch_train_loss=0.0007309028493800196
Epoch 907
Epoch 907 :: Batch 0/1
Batch Loss = 0.0007309028493800196
907, epoch_train_loss=0.0007309028493800196
Epoch 908
Epoch 908 :: Batch 0/1
Batch Loss = 0.0007309028493800196
908, epoch_train_loss=0.0007309028493800196
Epoch 909
Epoch 909 :: Batch 0/1
Batch Loss = 0.0007309028493800196
909, epoch_train_loss=0.0007309028493800196
Epoch 910
Epoch 910 :: Batch 0/1
Batch Loss = 0.0007309028493800196
910, epoch_train_loss=0.0007309028493800196
Epoch 911
Epoch 911 :: Batch 0/1
Batch Loss = 0.0007309028493800196
911, epoch_train_loss=0.0007309028493800196
Epoch 912
Epoch 912 :: Batch 0/1
Batch Loss = 0.0007309028493800196
912, epoch_train_loss=0.0007309028493800196
Epoch 913
Epoch 913 :: Batch 0/1
Batch Loss = 0.0007309028493800196
913, epoch_train_loss=0.0007309028493800196
Epoch 914
Epoch 914 :: Batch 0/1
Batch Loss = 0.0007309028493800194
914, epoch_train_loss=0.0007309028493800194
Epoch 915
Epoch 915 :: Batch 0/1
Batch Loss = 0.0007309028493800193
915, epoch_train_loss=0.0007309028493800193
Epoch 916
Epoch 916 :: Batch 0/1
Batch Loss = 0.0007309028493800193
916, epoch_train_loss=0.0007309028493800193
Epoch 917
Epoch 917 :: Batch 0/1
Batch Loss = 0.0007309028493800193
917, epoch_train_loss=0.0007309028493800193
Epoch 918
Epoch 918 :: Batch 0/1
Batch Loss = 0.0007309028493800193
918, epoch_train_loss=0.0007309028493800193
Epoch 919
Epoch 919 :: Batch 0/1
Batch Loss = 0.0007309028493800193
919, epoch_train_loss=0.0007309028493800193
Epoch 920
Epoch 920 :: Batch 0/1
Batch Loss = 0.0007309028493800193
920, epoch_train_loss=0.0007309028493800193
Epoch 921
Epoch 921 :: Batch 0/1
Batch Loss = 0.0007309028493800193
921, epoch_train_loss=0.0007309028493800193
Epoch 922
Epoch 922 :: Batch 0/1
Batch Loss = 0.0007309028493800193
922, epoch_train_loss=0.0007309028493800193
Epoch 923
Epoch 923 :: Batch 0/1
Batch Loss = 0.0007309028493800192
923, epoch_train_loss=0.0007309028493800192
Epoch 924
Epoch 924 :: Batch 0/1
Batch Loss = 0.0007309028493800192
924, epoch_train_loss=0.0007309028493800192
Epoch 925
Epoch 925 :: Batch 0/1
Batch Loss = 0.0007309028493800192
925, epoch_train_loss=0.0007309028493800192
Epoch 926
Epoch 926 :: Batch 0/1
Batch Loss = 0.0007309028493800192
926, epoch_train_loss=0.0007309028493800192
Epoch 927
Epoch 927 :: Batch 0/1
Batch Loss = 0.0007309028493800191
927, epoch_train_loss=0.0007309028493800191
Epoch 928
Epoch 928 :: Batch 0/1
Batch Loss = 0.0007309028493800191
928, epoch_train_loss=0.0007309028493800191
Epoch 929
Epoch 929 :: Batch 0/1
Batch Loss = 0.0007309028493800191
929, epoch_train_loss=0.0007309028493800191
Epoch 930
Epoch 930 :: Batch 0/1
Batch Loss = 0.0007309028493800191
930, epoch_train_loss=0.0007309028493800191
Epoch 931
Epoch 931 :: Batch 0/1
Batch Loss = 0.0007309028493800191
931, epoch_train_loss=0.0007309028493800191
Epoch 932
Epoch 932 :: Batch 0/1
Batch Loss = 0.0007309028493800191
932, epoch_train_loss=0.0007309028493800191
Epoch 933
Epoch 933 :: Batch 0/1
Batch Loss = 0.0007309028493800191
933, epoch_train_loss=0.0007309028493800191
Epoch 934
Epoch 934 :: Batch 0/1
Batch Loss = 0.000730902849380019
934, epoch_train_loss=0.000730902849380019
Epoch 935
Epoch 935 :: Batch 0/1
Batch Loss = 0.000730902849380019
935, epoch_train_loss=0.000730902849380019
Epoch 936
Epoch 936 :: Batch 0/1
Batch Loss = 0.000730902849380019
936, epoch_train_loss=0.000730902849380019
Epoch 937
Epoch 937 :: Batch 0/1
Batch Loss = 0.0007309028493800189
937, epoch_train_loss=0.0007309028493800189
Epoch 938
Epoch 938 :: Batch 0/1
Batch Loss = 0.0007309028493800189
938, epoch_train_loss=0.0007309028493800189
Epoch 939
Epoch 939 :: Batch 0/1
Batch Loss = 0.0007309028493800189
939, epoch_train_loss=0.0007309028493800189
Epoch 940
Epoch 940 :: Batch 0/1
Batch Loss = 0.0007309028493800189
940, epoch_train_loss=0.0007309028493800189
Epoch 941
Epoch 941 :: Batch 0/1
Batch Loss = 0.0007309028493800188
941, epoch_train_loss=0.0007309028493800188
Epoch 942
Epoch 942 :: Batch 0/1
Batch Loss = 0.0007309028493800188
942, epoch_train_loss=0.0007309028493800188
Epoch 943
Epoch 943 :: Batch 0/1
Batch Loss = 0.0007309028493800188
943, epoch_train_loss=0.0007309028493800188
Epoch 944
Epoch 944 :: Batch 0/1
Batch Loss = 0.0007309028493800188
944, epoch_train_loss=0.0007309028493800188
Epoch 945
Epoch 945 :: Batch 0/1
Batch Loss = 0.0007309028493800188
945, epoch_train_loss=0.0007309028493800188
Epoch 946
Epoch 946 :: Batch 0/1
Batch Loss = 0.0007309028493800188
946, epoch_train_loss=0.0007309028493800188
Epoch 947
Epoch 947 :: Batch 0/1
Batch Loss = 0.0007309028493800188
947, epoch_train_loss=0.0007309028493800188
Epoch 948
Epoch 948 :: Batch 0/1
Batch Loss = 0.0007309028493800188
948, epoch_train_loss=0.0007309028493800188
Epoch 949
Epoch 949 :: Batch 0/1
Batch Loss = 0.0007309028493800188
949, epoch_train_loss=0.0007309028493800188
Epoch 950
Epoch 950 :: Batch 0/1
Batch Loss = 0.0007309028493800186
950, epoch_train_loss=0.0007309028493800186
Epoch 951
Epoch 951 :: Batch 0/1
Batch Loss = 0.0007309028493800185
951, epoch_train_loss=0.0007309028493800185
Epoch 952
Epoch 952 :: Batch 0/1
Batch Loss = 0.0007309028493800185
952, epoch_train_loss=0.0007309028493800185
Epoch 953
Epoch 953 :: Batch 0/1
Batch Loss = 0.0007309028493800185
953, epoch_train_loss=0.0007309028493800185
Epoch 954
Epoch 954 :: Batch 0/1
Batch Loss = 0.0007309028493800185
954, epoch_train_loss=0.0007309028493800185
Epoch 955
Epoch 955 :: Batch 0/1
Batch Loss = 0.0007309028493800185
955, epoch_train_loss=0.0007309028493800185
Epoch 956
Epoch 956 :: Batch 0/1
Batch Loss = 0.0007309028493800185
956, epoch_train_loss=0.0007309028493800185
Epoch 957
Epoch 957 :: Batch 0/1
Batch Loss = 0.0007309028493800185
957, epoch_train_loss=0.0007309028493800185
Epoch 958
Epoch 958 :: Batch 0/1
Batch Loss = 0.0007309028493800185
958, epoch_train_loss=0.0007309028493800185
Epoch 959
Epoch 959 :: Batch 0/1
Batch Loss = 0.0007309028493800185
959, epoch_train_loss=0.0007309028493800185
Epoch 960
Epoch 960 :: Batch 0/1
Batch Loss = 0.0007309028493800185
960, epoch_train_loss=0.0007309028493800185
Epoch 961
Epoch 961 :: Batch 0/1
Batch Loss = 0.0007309028493800183
961, epoch_train_loss=0.0007309028493800183
Epoch 962
Epoch 962 :: Batch 0/1
Batch Loss = 0.0007309028493800183
962, epoch_train_loss=0.0007309028493800183
Epoch 963
Epoch 963 :: Batch 0/1
Batch Loss = 0.0007309028493800183
963, epoch_train_loss=0.0007309028493800183
Epoch 964
Epoch 964 :: Batch 0/1
Batch Loss = 0.0007309028493800182
964, epoch_train_loss=0.0007309028493800182
Epoch 965
Epoch 965 :: Batch 0/1
Batch Loss = 0.0007309028493800182
965, epoch_train_loss=0.0007309028493800182
Epoch 966
Epoch 966 :: Batch 0/1
Batch Loss = 0.0007309028493800182
966, epoch_train_loss=0.0007309028493800182
Epoch 967
Epoch 967 :: Batch 0/1
Batch Loss = 0.0007309028493800182
967, epoch_train_loss=0.0007309028493800182
Epoch 968
Epoch 968 :: Batch 0/1
Batch Loss = 0.0007309028493800182
968, epoch_train_loss=0.0007309028493800182
Epoch 969
Epoch 969 :: Batch 0/1
Batch Loss = 0.0007309028493800181
969, epoch_train_loss=0.0007309028493800181
Epoch 970
Epoch 970 :: Batch 0/1
Batch Loss = 0.0007309028493800181
970, epoch_train_loss=0.0007309028493800181
Epoch 971
Epoch 971 :: Batch 0/1
Batch Loss = 0.0007309028493800181
971, epoch_train_loss=0.0007309028493800181
Epoch 972
Epoch 972 :: Batch 0/1
Batch Loss = 0.0007309028493800181
972, epoch_train_loss=0.0007309028493800181
Epoch 973
Epoch 973 :: Batch 0/1
Batch Loss = 0.0007309028493800181
973, epoch_train_loss=0.0007309028493800181
Epoch 974
Epoch 974 :: Batch 0/1
Batch Loss = 0.0007309028493800181
974, epoch_train_loss=0.0007309028493800181
Epoch 975
Epoch 975 :: Batch 0/1
Batch Loss = 0.000730902849380018
975, epoch_train_loss=0.000730902849380018
Epoch 976
Epoch 976 :: Batch 0/1
Batch Loss = 0.0007309028493800179
976, epoch_train_loss=0.0007309028493800179
Epoch 977
Epoch 977 :: Batch 0/1
Batch Loss = 0.0007309028493800179
977, epoch_train_loss=0.0007309028493800179
Epoch 978
Epoch 978 :: Batch 0/1
Batch Loss = 0.0007309028493800179
978, epoch_train_loss=0.0007309028493800179
Epoch 979
Epoch 979 :: Batch 0/1
Batch Loss = 0.0007309028493800179
979, epoch_train_loss=0.0007309028493800179
Epoch 980
Epoch 980 :: Batch 0/1
Batch Loss = 0.0007309028493800179
980, epoch_train_loss=0.0007309028493800179
Epoch 981
Epoch 981 :: Batch 0/1
Batch Loss = 0.0007309028493800179
981, epoch_train_loss=0.0007309028493800179
Epoch 982
Epoch 982 :: Batch 0/1
Batch Loss = 0.0007309028493800178
982, epoch_train_loss=0.0007309028493800178
Epoch 983
Epoch 983 :: Batch 0/1
Batch Loss = 0.0007309028493800178
983, epoch_train_loss=0.0007309028493800178
Epoch 984
Epoch 984 :: Batch 0/1
Batch Loss = 0.0007309028493800178
984, epoch_train_loss=0.0007309028493800178
Epoch 985
Epoch 985 :: Batch 0/1
Batch Loss = 0.0007309028493800178
985, epoch_train_loss=0.0007309028493800178
Epoch 986
Epoch 986 :: Batch 0/1
Batch Loss = 0.0007309028493800178
986, epoch_train_loss=0.0007309028493800178
Epoch 987
Epoch 987 :: Batch 0/1
Batch Loss = 0.0007309028493800178
987, epoch_train_loss=0.0007309028493800178
Epoch 988
Epoch 988 :: Batch 0/1
Batch Loss = 0.0007309028493800177
988, epoch_train_loss=0.0007309028493800177
Epoch 989
Epoch 989 :: Batch 0/1
Batch Loss = 0.0007309028493800177
989, epoch_train_loss=0.0007309028493800177
Epoch 990
Epoch 990 :: Batch 0/1
Batch Loss = 0.0007309028493800177
990, epoch_train_loss=0.0007309028493800177
Epoch 991
Epoch 991 :: Batch 0/1
Batch Loss = 0.0007309028493800177
991, epoch_train_loss=0.0007309028493800177
Epoch 992
Epoch 992 :: Batch 0/1
Batch Loss = 0.0007309028493800177
992, epoch_train_loss=0.0007309028493800177
Epoch 993
Epoch 993 :: Batch 0/1
Batch Loss = 0.0007309028493800176
993, epoch_train_loss=0.0007309028493800176
Epoch 994
Epoch 994 :: Batch 0/1
Batch Loss = 0.0007309028493800176
994, epoch_train_loss=0.0007309028493800176
Epoch 995
Epoch 995 :: Batch 0/1
Batch Loss = 0.0007309028493800176
995, epoch_train_loss=0.0007309028493800176
Epoch 996
Epoch 996 :: Batch 0/1
Batch Loss = 0.0007309028493800176
996, epoch_train_loss=0.0007309028493800176
Epoch 997
Epoch 997 :: Batch 0/1
Batch Loss = 0.0007309028493800175
997, epoch_train_loss=0.0007309028493800175
Epoch 998
Epoch 998 :: Batch 0/1
Batch Loss = 0.0007309028493800175
998, epoch_train_loss=0.0007309028493800175
Epoch 999
Epoch 999 :: Batch 0/1
Batch Loss = 0.0007309028493800175
999, epoch_train_loss=0.0007309028493800175
Epoch 1000
Epoch 1000 :: Batch 0/1
Batch Loss = 0.0007309028493800175
1000, epoch_train_loss=0.0007309028493800175
Epoch 1001
Epoch 1001 :: Batch 0/1
Batch Loss = 0.0007309028493800175
1001, epoch_train_loss=0.0007309028493800175
Epoch 1002
Epoch 1002 :: Batch 0/1
Batch Loss = 0.0007309028493800175
1002, epoch_train_loss=0.0007309028493800175
Epoch 1003
Epoch 1003 :: Batch 0/1
Batch Loss = 0.0007309028493800175
1003, epoch_train_loss=0.0007309028493800175
Epoch 1004
Epoch 1004 :: Batch 0/1
Batch Loss = 0.0007309028493800173
1004, epoch_train_loss=0.0007309028493800173
Epoch 1005
Epoch 1005 :: Batch 0/1
Batch Loss = 0.0007309028493800171
1005, epoch_train_loss=0.0007309028493800171
Epoch 1006
Epoch 1006 :: Batch 0/1
Batch Loss = 0.0007309028493800171
1006, epoch_train_loss=0.0007309028493800171
Epoch 1007
Epoch 1007 :: Batch 0/1
Batch Loss = 0.0007309028493800171
1007, epoch_train_loss=0.0007309028493800171
Epoch 1008
Epoch 1008 :: Batch 0/1
Batch Loss = 0.0007309028493800171
1008, epoch_train_loss=0.0007309028493800171
Epoch 1009
Epoch 1009 :: Batch 0/1
Batch Loss = 0.0007309028493800171
1009, epoch_train_loss=0.0007309028493800171
Epoch 1010
Epoch 1010 :: Batch 0/1
Batch Loss = 0.0007309028493800171
1010, epoch_train_loss=0.0007309028493800171
Epoch 1011
Epoch 1011 :: Batch 0/1
Batch Loss = 0.0007309028493800171
1011, epoch_train_loss=0.0007309028493800171
Epoch 1012
Epoch 1012 :: Batch 0/1
Batch Loss = 0.000730902849380017
1012, epoch_train_loss=0.000730902849380017
Epoch 1013
Epoch 1013 :: Batch 0/1
Batch Loss = 0.000730902849380017
1013, epoch_train_loss=0.000730902849380017
Epoch 1014
Epoch 1014 :: Batch 0/1
Batch Loss = 0.000730902849380017
1014, epoch_train_loss=0.000730902849380017
Epoch 1015
Epoch 1015 :: Batch 0/1
Batch Loss = 0.000730902849380017
1015, epoch_train_loss=0.000730902849380017
Epoch 1016
Epoch 1016 :: Batch 0/1
Batch Loss = 0.0007309028493800169
1016, epoch_train_loss=0.0007309028493800169
Epoch 1017
Epoch 1017 :: Batch 0/1
Batch Loss = 0.0007309028493800169
1017, epoch_train_loss=0.0007309028493800169
Epoch 1018
Epoch 1018 :: Batch 0/1
Batch Loss = 0.0007309028493800169
1018, epoch_train_loss=0.0007309028493800169
Epoch 1019
Epoch 1019 :: Batch 0/1
Batch Loss = 0.0007309028493800169
1019, epoch_train_loss=0.0007309028493800169
Epoch 1020
Epoch 1020 :: Batch 0/1
Batch Loss = 0.0007309028493800169
1020, epoch_train_loss=0.0007309028493800169
Epoch 1021
Epoch 1021 :: Batch 0/1
Batch Loss = 0.0007309028493800169
1021, epoch_train_loss=0.0007309028493800169
Epoch 1022
Epoch 1022 :: Batch 0/1
Batch Loss = 0.0007309028493800168
1022, epoch_train_loss=0.0007309028493800168
Epoch 1023
Epoch 1023 :: Batch 0/1
Batch Loss = 0.0007309028493800168
1023, epoch_train_loss=0.0007309028493800168
Epoch 1024
Epoch 1024 :: Batch 0/1
Batch Loss = 0.0007309028493800168
1024, epoch_train_loss=0.0007309028493800168
Epoch 1025
Epoch 1025 :: Batch 0/1
Batch Loss = 0.0007309028493800168
1025, epoch_train_loss=0.0007309028493800168
Epoch 1026
Epoch 1026 :: Batch 0/1
Batch Loss = 0.0007309028493800167
1026, epoch_train_loss=0.0007309028493800167
Epoch 1027
Epoch 1027 :: Batch 0/1
Batch Loss = 0.0007309028493800167
1027, epoch_train_loss=0.0007309028493800167
Epoch 1028
Epoch 1028 :: Batch 0/1
Batch Loss = 0.0007309028493800167
1028, epoch_train_loss=0.0007309028493800167
Epoch 1029
Epoch 1029 :: Batch 0/1
Batch Loss = 0.0007309028493800167
1029, epoch_train_loss=0.0007309028493800167
Epoch 1030
Epoch 1030 :: Batch 0/1
Batch Loss = 0.0007309028493800167
1030, epoch_train_loss=0.0007309028493800167
Epoch 1031
Epoch 1031 :: Batch 0/1
Batch Loss = 0.0007309028493800167
1031, epoch_train_loss=0.0007309028493800167
Epoch 1032
Epoch 1032 :: Batch 0/1
Batch Loss = 0.0007309028493800166
1032, epoch_train_loss=0.0007309028493800166
Epoch 1033
Epoch 1033 :: Batch 0/1
Batch Loss = 0.0007309028493800166
1033, epoch_train_loss=0.0007309028493800166
Epoch 1034
Epoch 1034 :: Batch 0/1
Batch Loss = 0.0007309028493800165
1034, epoch_train_loss=0.0007309028493800165
Epoch 1035
Epoch 1035 :: Batch 0/1
Batch Loss = 0.0007309028493800165
1035, epoch_train_loss=0.0007309028493800165
Epoch 1036
Epoch 1036 :: Batch 0/1
Batch Loss = 0.0007309028493800165
1036, epoch_train_loss=0.0007309028493800165
Epoch 1037
Epoch 1037 :: Batch 0/1
Batch Loss = 0.0007309028493800165
1037, epoch_train_loss=0.0007309028493800165
Epoch 1038
Epoch 1038 :: Batch 0/1
Batch Loss = 0.0007309028493800165
1038, epoch_train_loss=0.0007309028493800165
Epoch 1039
Epoch 1039 :: Batch 0/1
Batch Loss = 0.0007309028493800165
1039, epoch_train_loss=0.0007309028493800165
Epoch 1040
Epoch 1040 :: Batch 0/1
Batch Loss = 0.0007309028493800165
1040, epoch_train_loss=0.0007309028493800165
Epoch 1041
Epoch 1041 :: Batch 0/1
Batch Loss = 0.0007309028493800165
1041, epoch_train_loss=0.0007309028493800165
Epoch 1042
Epoch 1042 :: Batch 0/1
Batch Loss = 0.0007309028493800165
1042, epoch_train_loss=0.0007309028493800165
Epoch 1043
Epoch 1043 :: Batch 0/1
Batch Loss = 0.0007309028493800163
1043, epoch_train_loss=0.0007309028493800163
Epoch 1044
Epoch 1044 :: Batch 0/1
Batch Loss = 0.0007309028493800163
1044, epoch_train_loss=0.0007309028493800163
Epoch 1045
Epoch 1045 :: Batch 0/1
Batch Loss = 0.0007309028493800163
1045, epoch_train_loss=0.0007309028493800163
Epoch 1046
Epoch 1046 :: Batch 0/1
Batch Loss = 0.0007309028493800163
1046, epoch_train_loss=0.0007309028493800163
Epoch 1047
Epoch 1047 :: Batch 0/1
Batch Loss = 0.0007309028493800163
1047, epoch_train_loss=0.0007309028493800163
Epoch 1048
Epoch 1048 :: Batch 0/1
Batch Loss = 0.0007309028493800163
1048, epoch_train_loss=0.0007309028493800163
Epoch 1049
Epoch 1049 :: Batch 0/1
Batch Loss = 0.0007309028493800163
1049, epoch_train_loss=0.0007309028493800163
Epoch 1050
Epoch 1050 :: Batch 0/1
Batch Loss = 0.0007309028493800163
1050, epoch_train_loss=0.0007309028493800163
Epoch 1051
Epoch 1051 :: Batch 0/1
Batch Loss = 0.0007309028493800163
1051, epoch_train_loss=0.0007309028493800163
Epoch 1052
Epoch 1052 :: Batch 0/1
Batch Loss = 0.000730902849380016
1052, epoch_train_loss=0.000730902849380016
Epoch 1053
Epoch 1053 :: Batch 0/1
Batch Loss = 0.0007309028493800159
1053, epoch_train_loss=0.0007309028493800159
Epoch 1054
Epoch 1054 :: Batch 0/1
Batch Loss = 0.0007309028493800159
1054, epoch_train_loss=0.0007309028493800159
Epoch 1055
Epoch 1055 :: Batch 0/1
Batch Loss = 0.0007309028493800159
1055, epoch_train_loss=0.0007309028493800159
Epoch 1056
Epoch 1056 :: Batch 0/1
Batch Loss = 0.0007309028493800159
1056, epoch_train_loss=0.0007309028493800159
Epoch 1057
Epoch 1057 :: Batch 0/1
Batch Loss = 0.0007309028493800159
1057, epoch_train_loss=0.0007309028493800159
Epoch 1058
Epoch 1058 :: Batch 0/1
Batch Loss = 0.0007309028493800159
1058, epoch_train_loss=0.0007309028493800159
Epoch 1059
Epoch 1059 :: Batch 0/1
Batch Loss = 0.0007309028493800159
1059, epoch_train_loss=0.0007309028493800159
Epoch 1060
Epoch 1060 :: Batch 0/1
Batch Loss = 0.0007309028493800159
1060, epoch_train_loss=0.0007309028493800159
Epoch 1061
Epoch 1061 :: Batch 0/1
Batch Loss = 0.0007309028493800157
1061, epoch_train_loss=0.0007309028493800157
Epoch 1062
Epoch 1062 :: Batch 0/1
Batch Loss = 0.0007309028493800157
1062, epoch_train_loss=0.0007309028493800157
Epoch 1063
Epoch 1063 :: Batch 0/1
Batch Loss = 0.0007309028493800157
1063, epoch_train_loss=0.0007309028493800157
Epoch 1064
Epoch 1064 :: Batch 0/1
Batch Loss = 0.0007309028493800157
1064, epoch_train_loss=0.0007309028493800157
Epoch 1065
Epoch 1065 :: Batch 0/1
Batch Loss = 0.0007309028493800157
1065, epoch_train_loss=0.0007309028493800157
Epoch 1066
Epoch 1066 :: Batch 0/1
Batch Loss = 0.0007309028493800157
1066, epoch_train_loss=0.0007309028493800157
Epoch 1067
Epoch 1067 :: Batch 0/1
Batch Loss = 0.0007309028493800157
1067, epoch_train_loss=0.0007309028493800157
Epoch 1068
Epoch 1068 :: Batch 0/1
Batch Loss = 0.0007309028493800157
1068, epoch_train_loss=0.0007309028493800157
Epoch 1069
Epoch 1069 :: Batch 0/1
Batch Loss = 0.0007309028493800155
1069, epoch_train_loss=0.0007309028493800155
Epoch 1070
Epoch 1070 :: Batch 0/1
Batch Loss = 0.0007309028493800155
1070, epoch_train_loss=0.0007309028493800155
Epoch 1071
Epoch 1071 :: Batch 0/1
Batch Loss = 0.0007309028493800155
1071, epoch_train_loss=0.0007309028493800155
Epoch 1072
Epoch 1072 :: Batch 0/1
Batch Loss = 0.0007309028493800155
1072, epoch_train_loss=0.0007309028493800155
Epoch 1073
Epoch 1073 :: Batch 0/1
Batch Loss = 0.0007309028493800155
1073, epoch_train_loss=0.0007309028493800155
Epoch 1074
Epoch 1074 :: Batch 0/1
Batch Loss = 0.0007309028493800155
1074, epoch_train_loss=0.0007309028493800155
Epoch 1075
Epoch 1075 :: Batch 0/1
Batch Loss = 0.0007309028493800155
1075, epoch_train_loss=0.0007309028493800155
Epoch 1076
Epoch 1076 :: Batch 0/1
Batch Loss = 0.0007309028493800155
1076, epoch_train_loss=0.0007309028493800155
Epoch 1077
Epoch 1077 :: Batch 0/1
Batch Loss = 0.0007309028493800155
1077, epoch_train_loss=0.0007309028493800155
Epoch 1078
Epoch 1078 :: Batch 0/1
Batch Loss = 0.0007309028493800155
1078, epoch_train_loss=0.0007309028493800155
Epoch 1079
Epoch 1079 :: Batch 0/1
Batch Loss = 0.0007309028493800154
1079, epoch_train_loss=0.0007309028493800154
Epoch 1080
Epoch 1080 :: Batch 0/1
Batch Loss = 0.0007309028493800153
1080, epoch_train_loss=0.0007309028493800153
Epoch 1081
Epoch 1081 :: Batch 0/1
Batch Loss = 0.0007309028493800153
1081, epoch_train_loss=0.0007309028493800153
Epoch 1082
Epoch 1082 :: Batch 0/1
Batch Loss = 0.0007309028493800153
1082, epoch_train_loss=0.0007309028493800153
Epoch 1083
Epoch 1083 :: Batch 0/1
Batch Loss = 0.0007309028493800153
1083, epoch_train_loss=0.0007309028493800153
Epoch 1084
Epoch 1084 :: Batch 0/1
Batch Loss = 0.0007309028493800153
1084, epoch_train_loss=0.0007309028493800153
Epoch 1085
Epoch 1085 :: Batch 0/1
Batch Loss = 0.0007309028493800153
1085, epoch_train_loss=0.0007309028493800153
Epoch 1086
Epoch 1086 :: Batch 0/1
Batch Loss = 0.0007309028493800153
1086, epoch_train_loss=0.0007309028493800153
Epoch 1087
Epoch 1087 :: Batch 0/1
Batch Loss = 0.0007309028493800153
1087, epoch_train_loss=0.0007309028493800153
Epoch 1088
Epoch 1088 :: Batch 0/1
Batch Loss = 0.0007309028493800153
1088, epoch_train_loss=0.0007309028493800153
Epoch 1089
Epoch 1089 :: Batch 0/1
Batch Loss = 0.0007309028493800152
1089, epoch_train_loss=0.0007309028493800152
Epoch 1090
Epoch 1090 :: Batch 0/1
Batch Loss = 0.0007309028493800151
1090, epoch_train_loss=0.0007309028493800151
Epoch 1091
Epoch 1091 :: Batch 0/1
Batch Loss = 0.0007309028493800151
1091, epoch_train_loss=0.0007309028493800151
Epoch 1092
Epoch 1092 :: Batch 0/1
Batch Loss = 0.0007309028493800151
1092, epoch_train_loss=0.0007309028493800151
Epoch 1093
Epoch 1093 :: Batch 0/1
Batch Loss = 0.0007309028493800151
1093, epoch_train_loss=0.0007309028493800151
Epoch 1094
Epoch 1094 :: Batch 0/1
Batch Loss = 0.0007309028493800151
1094, epoch_train_loss=0.0007309028493800151
Epoch 1095
Epoch 1095 :: Batch 0/1
Batch Loss = 0.0007309028493800151
1095, epoch_train_loss=0.0007309028493800151
Epoch 1096
Epoch 1096 :: Batch 0/1
Batch Loss = 0.0007309028493800151
1096, epoch_train_loss=0.0007309028493800151
Epoch 1097
Epoch 1097 :: Batch 0/1
Batch Loss = 0.0007309028493800151
1097, epoch_train_loss=0.0007309028493800151
Epoch 1098
Epoch 1098 :: Batch 0/1
Batch Loss = 0.0007309028493800147
1098, epoch_train_loss=0.0007309028493800147
Epoch 1099
Epoch 1099 :: Batch 0/1
Batch Loss = 0.0007309028493800147
1099, epoch_train_loss=0.0007309028493800147
Epoch 1100
Epoch 1100 :: Batch 0/1
Batch Loss = 0.0007309028493800147
1100, epoch_train_loss=0.0007309028493800147
Epoch 1101
Epoch 1101 :: Batch 0/1
Batch Loss = 0.0007309028493800147
1101, epoch_train_loss=0.0007309028493800147
Epoch 1102
Epoch 1102 :: Batch 0/1
Batch Loss = 0.0007309028493800147
1102, epoch_train_loss=0.0007309028493800147
Epoch 1103
Epoch 1103 :: Batch 0/1
Batch Loss = 0.0007309028493800147
1103, epoch_train_loss=0.0007309028493800147
Epoch 1104
Epoch 1104 :: Batch 0/1
Batch Loss = 0.0007309028493800147
1104, epoch_train_loss=0.0007309028493800147
Epoch 1105
Epoch 1105 :: Batch 0/1
Batch Loss = 0.0007309028493800146
1105, epoch_train_loss=0.0007309028493800146
Epoch 1106
Epoch 1106 :: Batch 0/1
Batch Loss = 0.0007309028493800145
1106, epoch_train_loss=0.0007309028493800145
Epoch 1107
Epoch 1107 :: Batch 0/1
Batch Loss = 0.0007309028493800145
1107, epoch_train_loss=0.0007309028493800145
Epoch 1108
Epoch 1108 :: Batch 0/1
Batch Loss = 0.0007309028493800145
1108, epoch_train_loss=0.0007309028493800145
Epoch 1109
Epoch 1109 :: Batch 0/1
Batch Loss = 0.0007309028493800145
1109, epoch_train_loss=0.0007309028493800145
Epoch 1110
Epoch 1110 :: Batch 0/1
Batch Loss = 0.0007309028493800145
1110, epoch_train_loss=0.0007309028493800145
Epoch 1111
Epoch 1111 :: Batch 0/1
Batch Loss = 0.0007309028493800145
1111, epoch_train_loss=0.0007309028493800145
Epoch 1112
Epoch 1112 :: Batch 0/1
Batch Loss = 0.0007309028493800145
1112, epoch_train_loss=0.0007309028493800145
Epoch 1113
Epoch 1113 :: Batch 0/1
Batch Loss = 0.0007309028493800145
1113, epoch_train_loss=0.0007309028493800145
Epoch 1114
Epoch 1114 :: Batch 0/1
Batch Loss = 0.0007309028493800144
1114, epoch_train_loss=0.0007309028493800144
Epoch 1115
Epoch 1115 :: Batch 0/1
Batch Loss = 0.0007309028493800143
1115, epoch_train_loss=0.0007309028493800143
Epoch 1116
Epoch 1116 :: Batch 0/1
Batch Loss = 0.0007309028493800143
1116, epoch_train_loss=0.0007309028493800143
Epoch 1117
Epoch 1117 :: Batch 0/1
Batch Loss = 0.0007309028493800143
1117, epoch_train_loss=0.0007309028493800143
Epoch 1118
Epoch 1118 :: Batch 0/1
Batch Loss = 0.0007309028493800143
1118, epoch_train_loss=0.0007309028493800143
Epoch 1119
Epoch 1119 :: Batch 0/1
Batch Loss = 0.0007309028493800143
1119, epoch_train_loss=0.0007309028493800143
Epoch 1120
Epoch 1120 :: Batch 0/1
Batch Loss = 0.0007309028493800143
1120, epoch_train_loss=0.0007309028493800143
Epoch 1121
Epoch 1121 :: Batch 0/1
Batch Loss = 0.0007309028493800143
1121, epoch_train_loss=0.0007309028493800143
Epoch 1122
Epoch 1122 :: Batch 0/1
Batch Loss = 0.0007309028493800142
1122, epoch_train_loss=0.0007309028493800142
Epoch 1123
Epoch 1123 :: Batch 0/1
Batch Loss = 0.0007309028493800141
1123, epoch_train_loss=0.0007309028493800141
Epoch 1124
Epoch 1124 :: Batch 0/1
Batch Loss = 0.0007309028493800141
1124, epoch_train_loss=0.0007309028493800141
Epoch 1125
Epoch 1125 :: Batch 0/1
Batch Loss = 0.0007309028493800141
1125, epoch_train_loss=0.0007309028493800141
Epoch 1126
Epoch 1126 :: Batch 0/1
Batch Loss = 0.0007309028493800141
1126, epoch_train_loss=0.0007309028493800141
Epoch 1127
Epoch 1127 :: Batch 0/1
Batch Loss = 0.0007309028493800141
1127, epoch_train_loss=0.0007309028493800141
Epoch 1128
Epoch 1128 :: Batch 0/1
Batch Loss = 0.0007309028493800141
1128, epoch_train_loss=0.0007309028493800141
Epoch 1129
Epoch 1129 :: Batch 0/1
Batch Loss = 0.0007309028493800141
1129, epoch_train_loss=0.0007309028493800141
Epoch 1130
Epoch 1130 :: Batch 0/1
Batch Loss = 0.0007309028493800141
1130, epoch_train_loss=0.0007309028493800141
Epoch 1131
Epoch 1131 :: Batch 0/1
Batch Loss = 0.0007309028493800141
1131, epoch_train_loss=0.0007309028493800141
Epoch 1132
Epoch 1132 :: Batch 0/1
Batch Loss = 0.000730902849380014
1132, epoch_train_loss=0.000730902849380014
Epoch 1133
Epoch 1133 :: Batch 0/1
Batch Loss = 0.000730902849380014
1133, epoch_train_loss=0.000730902849380014
Epoch 1134
Epoch 1134 :: Batch 0/1
Batch Loss = 0.0007309028493800138
1134, epoch_train_loss=0.0007309028493800138
Epoch 1135
Epoch 1135 :: Batch 0/1
Batch Loss = 0.0007309028493800138
1135, epoch_train_loss=0.0007309028493800138
Epoch 1136
Epoch 1136 :: Batch 0/1
Batch Loss = 0.0007309028493800138
1136, epoch_train_loss=0.0007309028493800138
Epoch 1137
Epoch 1137 :: Batch 0/1
Batch Loss = 0.0007309028493800138
1137, epoch_train_loss=0.0007309028493800138
Epoch 1138
Epoch 1138 :: Batch 0/1
Batch Loss = 0.0007309028493800138
1138, epoch_train_loss=0.0007309028493800138
Epoch 1139
Epoch 1139 :: Batch 0/1
Batch Loss = 0.0007309028493800138
1139, epoch_train_loss=0.0007309028493800138
Epoch 1140
Epoch 1140 :: Batch 0/1
Batch Loss = 0.0007309028493800138
1140, epoch_train_loss=0.0007309028493800138
Epoch 1141
Epoch 1141 :: Batch 0/1
Batch Loss = 0.0007309028493800137
1141, epoch_train_loss=0.0007309028493800137
Epoch 1142
Epoch 1142 :: Batch 0/1
Batch Loss = 0.0007309028493800137
1142, epoch_train_loss=0.0007309028493800137
Epoch 1143
Epoch 1143 :: Batch 0/1
Batch Loss = 0.0007309028493800136
1143, epoch_train_loss=0.0007309028493800136
Epoch 1144
Epoch 1144 :: Batch 0/1
Batch Loss = 0.0007309028493800136
1144, epoch_train_loss=0.0007309028493800136
Epoch 1145
Epoch 1145 :: Batch 0/1
Batch Loss = 0.0007309028493800136
1145, epoch_train_loss=0.0007309028493800136
Epoch 1146
Epoch 1146 :: Batch 0/1
Batch Loss = 0.0007309028493800136
1146, epoch_train_loss=0.0007309028493800136
Epoch 1147
Epoch 1147 :: Batch 0/1
Batch Loss = 0.0007309028493800136
1147, epoch_train_loss=0.0007309028493800136
Epoch 1148
Epoch 1148 :: Batch 0/1
Batch Loss = 0.0007309028493800136
1148, epoch_train_loss=0.0007309028493800136
Epoch 1149
Epoch 1149 :: Batch 0/1
Batch Loss = 0.0007309028493800134
1149, epoch_train_loss=0.0007309028493800134
Epoch 1150
Epoch 1150 :: Batch 0/1
Batch Loss = 0.0007309028493800134
1150, epoch_train_loss=0.0007309028493800134
Epoch 1151
Epoch 1151 :: Batch 0/1
Batch Loss = 0.0007309028493800133
1151, epoch_train_loss=0.0007309028493800133
Epoch 1152
Epoch 1152 :: Batch 0/1
Batch Loss = 0.0007309028493800133
1152, epoch_train_loss=0.0007309028493800133
Epoch 1153
Epoch 1153 :: Batch 0/1
Batch Loss = 0.0007309028493800133
1153, epoch_train_loss=0.0007309028493800133
Epoch 1154
Epoch 1154 :: Batch 0/1
Batch Loss = 0.0007309028493800133
1154, epoch_train_loss=0.0007309028493800133
Epoch 1155
Epoch 1155 :: Batch 0/1
Batch Loss = 0.0007309028493800133
1155, epoch_train_loss=0.0007309028493800133
Epoch 1156
Epoch 1156 :: Batch 0/1
Batch Loss = 0.0007309028493800133
1156, epoch_train_loss=0.0007309028493800133
Epoch 1157
Epoch 1157 :: Batch 0/1
Batch Loss = 0.0007309028493800132
1157, epoch_train_loss=0.0007309028493800132
Epoch 1158
Epoch 1158 :: Batch 0/1
Batch Loss = 0.0007309028493800132
1158, epoch_train_loss=0.0007309028493800132
Epoch 1159
Epoch 1159 :: Batch 0/1
Batch Loss = 0.0007309028493800132
1159, epoch_train_loss=0.0007309028493800132
Epoch 1160
Epoch 1160 :: Batch 0/1
Batch Loss = 0.0007309028493800131
1160, epoch_train_loss=0.0007309028493800131
Epoch 1161
Epoch 1161 :: Batch 0/1
Batch Loss = 0.0007309028493800131
1161, epoch_train_loss=0.0007309028493800131
Epoch 1162
Epoch 1162 :: Batch 0/1
Batch Loss = 0.0007309028493800131
1162, epoch_train_loss=0.0007309028493800131
Epoch 1163
Epoch 1163 :: Batch 0/1
Batch Loss = 0.0007309028493800131
1163, epoch_train_loss=0.0007309028493800131
Epoch 1164
Epoch 1164 :: Batch 0/1
Batch Loss = 0.000730902849380013
1164, epoch_train_loss=0.000730902849380013
Epoch 1165
Epoch 1165 :: Batch 0/1
Batch Loss = 0.000730902849380013
1165, epoch_train_loss=0.000730902849380013
Epoch 1166
Epoch 1166 :: Batch 0/1
Batch Loss = 0.000730902849380013
1166, epoch_train_loss=0.000730902849380013
Epoch 1167
Epoch 1167 :: Batch 0/1
Batch Loss = 0.000730902849380013
1167, epoch_train_loss=0.000730902849380013
Epoch 1168
Epoch 1168 :: Batch 0/1
Batch Loss = 0.000730902849380013
1168, epoch_train_loss=0.000730902849380013
Epoch 1169
Epoch 1169 :: Batch 0/1
Batch Loss = 0.0007309028493800129
1169, epoch_train_loss=0.0007309028493800129
Epoch 1170
Epoch 1170 :: Batch 0/1
Batch Loss = 0.0007309028493800129
1170, epoch_train_loss=0.0007309028493800129
Epoch 1171
Epoch 1171 :: Batch 0/1
Batch Loss = 0.0007309028493800129
1171, epoch_train_loss=0.0007309028493800129
Epoch 1172
Epoch 1172 :: Batch 0/1
Batch Loss = 0.0007309028493800129
1172, epoch_train_loss=0.0007309028493800129
Epoch 1173
Epoch 1173 :: Batch 0/1
Batch Loss = 0.0007309028493800127
1173, epoch_train_loss=0.0007309028493800127
Epoch 1174
Epoch 1174 :: Batch 0/1
Batch Loss = 0.0007309028493800127
1174, epoch_train_loss=0.0007309028493800127
Epoch 1175
Epoch 1175 :: Batch 0/1
Batch Loss = 0.0007309028493800127
1175, epoch_train_loss=0.0007309028493800127
Epoch 1176
Epoch 1176 :: Batch 0/1
Batch Loss = 0.0007309028493800127
1176, epoch_train_loss=0.0007309028493800127
Epoch 1177
Epoch 1177 :: Batch 0/1
Batch Loss = 0.0007309028493800127
1177, epoch_train_loss=0.0007309028493800127
Epoch 1178
Epoch 1178 :: Batch 0/1
Batch Loss = 0.0007309028493800127
1178, epoch_train_loss=0.0007309028493800127
Epoch 1179
Epoch 1179 :: Batch 0/1
Batch Loss = 0.0007309028493800126
1179, epoch_train_loss=0.0007309028493800126
Epoch 1180
Epoch 1180 :: Batch 0/1
Batch Loss = 0.0007309028493800126
1180, epoch_train_loss=0.0007309028493800126
Epoch 1181
Epoch 1181 :: Batch 0/1
Batch Loss = 0.0007309028493800126
1181, epoch_train_loss=0.0007309028493800126
Epoch 1182
Epoch 1182 :: Batch 0/1
Batch Loss = 0.0007309028493800126
1182, epoch_train_loss=0.0007309028493800126
Epoch 1183
Epoch 1183 :: Batch 0/1
Batch Loss = 0.0007309028493800125
1183, epoch_train_loss=0.0007309028493800125
Epoch 1184
Epoch 1184 :: Batch 0/1
Batch Loss = 0.0007309028493800125
1184, epoch_train_loss=0.0007309028493800125
Epoch 1185
Epoch 1185 :: Batch 0/1
Batch Loss = 0.0007309028493800124
1185, epoch_train_loss=0.0007309028493800124
Epoch 1186
Epoch 1186 :: Batch 0/1
Batch Loss = 0.0007309028493800124
1186, epoch_train_loss=0.0007309028493800124
Epoch 1187
Epoch 1187 :: Batch 0/1
Batch Loss = 0.0007309028493800124
1187, epoch_train_loss=0.0007309028493800124
Epoch 1188
Epoch 1188 :: Batch 0/1
Batch Loss = 0.0007309028493800124
1188, epoch_train_loss=0.0007309028493800124
Epoch 1189
Epoch 1189 :: Batch 0/1
Batch Loss = 0.0007309028493800124
1189, epoch_train_loss=0.0007309028493800124
Epoch 1190
Epoch 1190 :: Batch 0/1
Batch Loss = 0.0007309028493800124
1190, epoch_train_loss=0.0007309028493800124
Epoch 1191
Epoch 1191 :: Batch 0/1
Batch Loss = 0.0007309028493800124
1191, epoch_train_loss=0.0007309028493800124
Epoch 1192
Epoch 1192 :: Batch 0/1
Batch Loss = 0.0007309028493800124
1192, epoch_train_loss=0.0007309028493800124
Epoch 1193
Epoch 1193 :: Batch 0/1
Batch Loss = 0.0007309028493800123
1193, epoch_train_loss=0.0007309028493800123
Epoch 1194
Epoch 1194 :: Batch 0/1
Batch Loss = 0.0007309028493800121
1194, epoch_train_loss=0.0007309028493800121
Epoch 1195
Epoch 1195 :: Batch 0/1
Batch Loss = 0.0007309028493800121
1195, epoch_train_loss=0.0007309028493800121
Epoch 1196
Epoch 1196 :: Batch 0/1
Batch Loss = 0.0007309028493800121
1196, epoch_train_loss=0.0007309028493800121
Epoch 1197
Epoch 1197 :: Batch 0/1
Batch Loss = 0.0007309028493800121
1197, epoch_train_loss=0.0007309028493800121
Epoch 1198
Epoch 1198 :: Batch 0/1
Batch Loss = 0.0007309028493800121
1198, epoch_train_loss=0.0007309028493800121
Epoch 1199
Epoch 1199 :: Batch 0/1
Batch Loss = 0.0007309028493800121
1199, epoch_train_loss=0.0007309028493800121
Epoch 1200
Epoch 1200 :: Batch 0/1
Batch Loss = 0.000730902849380012
1200, epoch_train_loss=0.000730902849380012
Epoch 1201
Epoch 1201 :: Batch 0/1
Batch Loss = 0.000730902849380012
1201, epoch_train_loss=0.000730902849380012
Epoch 1202
Epoch 1202 :: Batch 0/1
Batch Loss = 0.0007309028493800119
1202, epoch_train_loss=0.0007309028493800119
Epoch 1203
Epoch 1203 :: Batch 0/1
Batch Loss = 0.0007309028493800119
1203, epoch_train_loss=0.0007309028493800119
Epoch 1204
Epoch 1204 :: Batch 0/1
Batch Loss = 0.0007309028493800119
1204, epoch_train_loss=0.0007309028493800119
Epoch 1205
Epoch 1205 :: Batch 0/1
Batch Loss = 0.0007309028493800119
1205, epoch_train_loss=0.0007309028493800119
Epoch 1206
Epoch 1206 :: Batch 0/1
Batch Loss = 0.0007309028493800119
1206, epoch_train_loss=0.0007309028493800119
Epoch 1207
Epoch 1207 :: Batch 0/1
Batch Loss = 0.0007309028493800119
1207, epoch_train_loss=0.0007309028493800119
Epoch 1208
Epoch 1208 :: Batch 0/1
Batch Loss = 0.0007309028493800118
1208, epoch_train_loss=0.0007309028493800118
Epoch 1209
Epoch 1209 :: Batch 0/1
Batch Loss = 0.0007309028493800117
1209, epoch_train_loss=0.0007309028493800117
Epoch 1210
Epoch 1210 :: Batch 0/1
Batch Loss = 0.0007309028493800117
1210, epoch_train_loss=0.0007309028493800117
Epoch 1211
Epoch 1211 :: Batch 0/1
Batch Loss = 0.0007309028493800117
1211, epoch_train_loss=0.0007309028493800117
Epoch 1212
Epoch 1212 :: Batch 0/1
Batch Loss = 0.0007309028493800117
1212, epoch_train_loss=0.0007309028493800117
Epoch 1213
Epoch 1213 :: Batch 0/1
Batch Loss = 0.0007309028493800117
1213, epoch_train_loss=0.0007309028493800117
Epoch 1214
Epoch 1214 :: Batch 0/1
Batch Loss = 0.0007309028493800117
1214, epoch_train_loss=0.0007309028493800117
Epoch 1215
Epoch 1215 :: Batch 0/1
Batch Loss = 0.0007309028493800117
1215, epoch_train_loss=0.0007309028493800117
Epoch 1216
Epoch 1216 :: Batch 0/1
Batch Loss = 0.0007309028493800115
1216, epoch_train_loss=0.0007309028493800115
Epoch 1217
Epoch 1217 :: Batch 0/1
Batch Loss = 0.0007309028493800115
1217, epoch_train_loss=0.0007309028493800115
Epoch 1218
Epoch 1218 :: Batch 0/1
Batch Loss = 0.0007309028493800115
1218, epoch_train_loss=0.0007309028493800115
Epoch 1219
Epoch 1219 :: Batch 0/1
Batch Loss = 0.0007309028493800114
1219, epoch_train_loss=0.0007309028493800114
Epoch 1220
Epoch 1220 :: Batch 0/1
Batch Loss = 0.0007309028493800114
1220, epoch_train_loss=0.0007309028493800114
Epoch 1221
Epoch 1221 :: Batch 0/1
Batch Loss = 0.0007309028493800114
1221, epoch_train_loss=0.0007309028493800114
Epoch 1222
Epoch 1222 :: Batch 0/1
Batch Loss = 0.0007309028493800114
1222, epoch_train_loss=0.0007309028493800114
Epoch 1223
Epoch 1223 :: Batch 0/1
Batch Loss = 0.0007309028493800114
1223, epoch_train_loss=0.0007309028493800114
Epoch 1224
Epoch 1224 :: Batch 0/1
Batch Loss = 0.0007309028493800113
1224, epoch_train_loss=0.0007309028493800113
Epoch 1225
Epoch 1225 :: Batch 0/1
Batch Loss = 0.0007309028493800113
1225, epoch_train_loss=0.0007309028493800113
Epoch 1226
Epoch 1226 :: Batch 0/1
Batch Loss = 0.0007309028493800113
1226, epoch_train_loss=0.0007309028493800113
Epoch 1227
Epoch 1227 :: Batch 0/1
Batch Loss = 0.0007309028493800112
1227, epoch_train_loss=0.0007309028493800112
Epoch 1228
Epoch 1228 :: Batch 0/1
Batch Loss = 0.0007309028493800112
1228, epoch_train_loss=0.0007309028493800112
Epoch 1229
Epoch 1229 :: Batch 0/1
Batch Loss = 0.0007309028493800112
1229, epoch_train_loss=0.0007309028493800112
Epoch 1230
Epoch 1230 :: Batch 0/1
Batch Loss = 0.0007309028493800112
1230, epoch_train_loss=0.0007309028493800112
Epoch 1231
Epoch 1231 :: Batch 0/1
Batch Loss = 0.0007309028493800112
1231, epoch_train_loss=0.0007309028493800112
Epoch 1232
Epoch 1232 :: Batch 0/1
Batch Loss = 0.0007309028493800112
1232, epoch_train_loss=0.0007309028493800112
Epoch 1233
Epoch 1233 :: Batch 0/1
Batch Loss = 0.0007309028493800111
1233, epoch_train_loss=0.0007309028493800111
Epoch 1234
Epoch 1234 :: Batch 0/1
Batch Loss = 0.0007309028493800111
1234, epoch_train_loss=0.0007309028493800111
Epoch 1235
Epoch 1235 :: Batch 0/1
Batch Loss = 0.000730902849380011
1235, epoch_train_loss=0.000730902849380011
Epoch 1236
Epoch 1236 :: Batch 0/1
Batch Loss = 0.000730902849380011
1236, epoch_train_loss=0.000730902849380011
Epoch 1237
Epoch 1237 :: Batch 0/1
Batch Loss = 0.000730902849380011
1237, epoch_train_loss=0.000730902849380011
Epoch 1238
Epoch 1238 :: Batch 0/1
Batch Loss = 0.000730902849380011
1238, epoch_train_loss=0.000730902849380011
Epoch 1239
Epoch 1239 :: Batch 0/1
Batch Loss = 0.000730902849380011
1239, epoch_train_loss=0.000730902849380011
Epoch 1240
Epoch 1240 :: Batch 0/1
Batch Loss = 0.0007309028493800108
1240, epoch_train_loss=0.0007309028493800108
Epoch 1241
Epoch 1241 :: Batch 0/1
Batch Loss = 0.0007309028493800107
1241, epoch_train_loss=0.0007309028493800107
Epoch 1242
Epoch 1242 :: Batch 0/1
Batch Loss = 0.0007309028493800107
1242, epoch_train_loss=0.0007309028493800107
Epoch 1243
Epoch 1243 :: Batch 0/1
Batch Loss = 0.0007309028493800107
1243, epoch_train_loss=0.0007309028493800107
Epoch 1244
Epoch 1244 :: Batch 0/1
Batch Loss = 0.0007309028493800107
1244, epoch_train_loss=0.0007309028493800107
Epoch 1245
Epoch 1245 :: Batch 0/1
Batch Loss = 0.0007309028493800107
1245, epoch_train_loss=0.0007309028493800107
Epoch 1246
Epoch 1246 :: Batch 0/1
Batch Loss = 0.0007309028493800107
1246, epoch_train_loss=0.0007309028493800107
Epoch 1247
Epoch 1247 :: Batch 0/1
Batch Loss = 0.0007309028493800107
1247, epoch_train_loss=0.0007309028493800107
Epoch 1248
Epoch 1248 :: Batch 0/1
Batch Loss = 0.0007309028493800107
1248, epoch_train_loss=0.0007309028493800107
Epoch 1249
Epoch 1249 :: Batch 0/1
Batch Loss = 0.0007309028493800106
1249, epoch_train_loss=0.0007309028493800106
Epoch 1250
Epoch 1250 :: Batch 0/1
Batch Loss = 0.0007309028493800104
1250, epoch_train_loss=0.0007309028493800104
Epoch 1251
Epoch 1251 :: Batch 0/1
Batch Loss = 0.0007309028493800104
1251, epoch_train_loss=0.0007309028493800104
Epoch 1252
Epoch 1252 :: Batch 0/1
Batch Loss = 0.0007309028493800104
1252, epoch_train_loss=0.0007309028493800104
Epoch 1253
Epoch 1253 :: Batch 0/1
Batch Loss = 0.0007309028493800104
1253, epoch_train_loss=0.0007309028493800104
Epoch 1254
Epoch 1254 :: Batch 0/1
Batch Loss = 0.0007309028493800104
1254, epoch_train_loss=0.0007309028493800104
Epoch 1255
Epoch 1255 :: Batch 0/1
Batch Loss = 0.0007309028493800104
1255, epoch_train_loss=0.0007309028493800104
Epoch 1256
Epoch 1256 :: Batch 0/1
Batch Loss = 0.0007309028493800104
1256, epoch_train_loss=0.0007309028493800104
Epoch 1257
Epoch 1257 :: Batch 0/1
Batch Loss = 0.0007309028493800104
1257, epoch_train_loss=0.0007309028493800104
Epoch 1258
Epoch 1258 :: Batch 0/1
Batch Loss = 0.0007309028493800103
1258, epoch_train_loss=0.0007309028493800103
Epoch 1259
Epoch 1259 :: Batch 0/1
Batch Loss = 0.0007309028493800103
1259, epoch_train_loss=0.0007309028493800103
Epoch 1260
Epoch 1260 :: Batch 0/1
Batch Loss = 0.0007309028493800102
1260, epoch_train_loss=0.0007309028493800102
Epoch 1261
Epoch 1261 :: Batch 0/1
Batch Loss = 0.0007309028493800102
1261, epoch_train_loss=0.0007309028493800102
Epoch 1262
Epoch 1262 :: Batch 0/1
Batch Loss = 0.0007309028493800102
1262, epoch_train_loss=0.0007309028493800102
Epoch 1263
Epoch 1263 :: Batch 0/1
Batch Loss = 0.0007309028493800102
1263, epoch_train_loss=0.0007309028493800102
Epoch 1264
Epoch 1264 :: Batch 0/1
Batch Loss = 0.0007309028493800102
1264, epoch_train_loss=0.0007309028493800102
Epoch 1265
Epoch 1265 :: Batch 0/1
Batch Loss = 0.00073090284938001
1265, epoch_train_loss=0.00073090284938001
Epoch 1266
Epoch 1266 :: Batch 0/1
Batch Loss = 0.00073090284938001
1266, epoch_train_loss=0.00073090284938001
Epoch 1267
Epoch 1267 :: Batch 0/1
Batch Loss = 0.00073090284938001
1267, epoch_train_loss=0.00073090284938001
Epoch 1268
Epoch 1268 :: Batch 0/1
Batch Loss = 0.00073090284938001
1268, epoch_train_loss=0.00073090284938001
Epoch 1269
Epoch 1269 :: Batch 0/1
Batch Loss = 0.00073090284938001
1269, epoch_train_loss=0.00073090284938001
Epoch 1270
Epoch 1270 :: Batch 0/1
Batch Loss = 0.00073090284938001
1270, epoch_train_loss=0.00073090284938001
Epoch 1271
Epoch 1271 :: Batch 0/1
Batch Loss = 0.00073090284938001
1271, epoch_train_loss=0.00073090284938001
Epoch 1272
Epoch 1272 :: Batch 0/1
Batch Loss = 0.00073090284938001
1272, epoch_train_loss=0.00073090284938001
Epoch 1273
Epoch 1273 :: Batch 0/1
Batch Loss = 0.0007309028493800099
1273, epoch_train_loss=0.0007309028493800099
Epoch 1274
Epoch 1274 :: Batch 0/1
Batch Loss = 0.0007309028493800098
1274, epoch_train_loss=0.0007309028493800098
Epoch 1275
Epoch 1275 :: Batch 0/1
Batch Loss = 0.0007309028493800098
1275, epoch_train_loss=0.0007309028493800098
Epoch 1276
Epoch 1276 :: Batch 0/1
Batch Loss = 0.0007309028493800098
1276, epoch_train_loss=0.0007309028493800098
Epoch 1277
Epoch 1277 :: Batch 0/1
Batch Loss = 0.0007309028493800098
1277, epoch_train_loss=0.0007309028493800098
Epoch 1278
Epoch 1278 :: Batch 0/1
Batch Loss = 0.0007309028493800098
1278, epoch_train_loss=0.0007309028493800098
Epoch 1279
Epoch 1279 :: Batch 0/1
Batch Loss = 0.0007309028493800098
1279, epoch_train_loss=0.0007309028493800098
Epoch 1280
Epoch 1280 :: Batch 0/1
Batch Loss = 0.0007309028493800098
1280, epoch_train_loss=0.0007309028493800098
Epoch 1281
Epoch 1281 :: Batch 0/1
Batch Loss = 0.0007309028493800096
1281, epoch_train_loss=0.0007309028493800096
Epoch 1282
Epoch 1282 :: Batch 0/1
Batch Loss = 0.0007309028493800095
1282, epoch_train_loss=0.0007309028493800095
Epoch 1283
Epoch 1283 :: Batch 0/1
Batch Loss = 0.0007309028493800095
1283, epoch_train_loss=0.0007309028493800095
Epoch 1284
Epoch 1284 :: Batch 0/1
Batch Loss = 0.0007309028493800095
1284, epoch_train_loss=0.0007309028493800095
Epoch 1285
Epoch 1285 :: Batch 0/1
Batch Loss = 0.0007309028493800095
1285, epoch_train_loss=0.0007309028493800095
Epoch 1286
Epoch 1286 :: Batch 0/1
Batch Loss = 0.0007309028493800095
1286, epoch_train_loss=0.0007309028493800095
Epoch 1287
Epoch 1287 :: Batch 0/1
Batch Loss = 0.0007309028493800095
1287, epoch_train_loss=0.0007309028493800095
Epoch 1288
Epoch 1288 :: Batch 0/1
Batch Loss = 0.0007309028493800095
1288, epoch_train_loss=0.0007309028493800095
Epoch 1289
Epoch 1289 :: Batch 0/1
Batch Loss = 0.0007309028493800095
1289, epoch_train_loss=0.0007309028493800095
Epoch 1290
Epoch 1290 :: Batch 0/1
Batch Loss = 0.0007309028493800092
1290, epoch_train_loss=0.0007309028493800092
Epoch 1291
Epoch 1291 :: Batch 0/1
Batch Loss = 0.0007309028493800092
1291, epoch_train_loss=0.0007309028493800092
Epoch 1292
Epoch 1292 :: Batch 0/1
Batch Loss = 0.0007309028493800092
1292, epoch_train_loss=0.0007309028493800092
Epoch 1293
Epoch 1293 :: Batch 0/1
Batch Loss = 0.0007309028493800092
1293, epoch_train_loss=0.0007309028493800092
Epoch 1294
Epoch 1294 :: Batch 0/1
Batch Loss = 0.0007309028493800092
1294, epoch_train_loss=0.0007309028493800092
Epoch 1295
Epoch 1295 :: Batch 0/1
Batch Loss = 0.0007309028493800092
1295, epoch_train_loss=0.0007309028493800092
Epoch 1296
Epoch 1296 :: Batch 0/1
Batch Loss = 0.0007309028493800092
1296, epoch_train_loss=0.0007309028493800092
Epoch 1297
Epoch 1297 :: Batch 0/1
Batch Loss = 0.0007309028493800092
1297, epoch_train_loss=0.0007309028493800092
Epoch 1298
Epoch 1298 :: Batch 0/1
Batch Loss = 0.0007309028493800091
1298, epoch_train_loss=0.0007309028493800091
Epoch 1299
Epoch 1299 :: Batch 0/1
Batch Loss = 0.000730902849380009
1299, epoch_train_loss=0.000730902849380009
Epoch 1300
Epoch 1300 :: Batch 0/1
Batch Loss = 0.000730902849380009
1300, epoch_train_loss=0.000730902849380009
Epoch 1301
Epoch 1301 :: Batch 0/1
Batch Loss = 0.000730902849380009
1301, epoch_train_loss=0.000730902849380009
Epoch 1302
Epoch 1302 :: Batch 0/1
Batch Loss = 0.000730902849380009
1302, epoch_train_loss=0.000730902849380009
Epoch 1303
Epoch 1303 :: Batch 0/1
Batch Loss = 0.000730902849380009
1303, epoch_train_loss=0.000730902849380009
Epoch 1304
Epoch 1304 :: Batch 0/1
Batch Loss = 0.000730902849380009
1304, epoch_train_loss=0.000730902849380009
Epoch 1305
Epoch 1305 :: Batch 0/1
Batch Loss = 0.0007309028493800089
1305, epoch_train_loss=0.0007309028493800089
Epoch 1306
Epoch 1306 :: Batch 0/1
Batch Loss = 0.0007309028493800089
1306, epoch_train_loss=0.0007309028493800089
Epoch 1307
Epoch 1307 :: Batch 0/1
Batch Loss = 0.0007309028493800088
1307, epoch_train_loss=0.0007309028493800088
Epoch 1308
Epoch 1308 :: Batch 0/1
Batch Loss = 0.0007309028493800088
1308, epoch_train_loss=0.0007309028493800088
Epoch 1309
Epoch 1309 :: Batch 0/1
Batch Loss = 0.0007309028493800088
1309, epoch_train_loss=0.0007309028493800088
Epoch 1310
Epoch 1310 :: Batch 0/1
Batch Loss = 0.0007309028493800088
1310, epoch_train_loss=0.0007309028493800088
Epoch 1311
Epoch 1311 :: Batch 0/1
Batch Loss = 0.0007309028493800088
1311, epoch_train_loss=0.0007309028493800088
Epoch 1312
Epoch 1312 :: Batch 0/1
Batch Loss = 0.0007309028493800087
1312, epoch_train_loss=0.0007309028493800087
Epoch 1313
Epoch 1313 :: Batch 0/1
Batch Loss = 0.0007309028493800087
1313, epoch_train_loss=0.0007309028493800087
Epoch 1314
Epoch 1314 :: Batch 0/1
Batch Loss = 0.0007309028493800086
1314, epoch_train_loss=0.0007309028493800086
Epoch 1315
Epoch 1315 :: Batch 0/1
Batch Loss = 0.0007309028493800086
1315, epoch_train_loss=0.0007309028493800086
Epoch 1316
Epoch 1316 :: Batch 0/1
Batch Loss = 0.0007309028493800086
1316, epoch_train_loss=0.0007309028493800086
Epoch 1317
Epoch 1317 :: Batch 0/1
Batch Loss = 0.0007309028493800086
1317, epoch_train_loss=0.0007309028493800086
Epoch 1318
Epoch 1318 :: Batch 0/1
Batch Loss = 0.0007309028493800086
1318, epoch_train_loss=0.0007309028493800086
Epoch 1319
Epoch 1319 :: Batch 0/1
Batch Loss = 0.0007309028493800085
1319, epoch_train_loss=0.0007309028493800085
Epoch 1320
Epoch 1320 :: Batch 0/1
Batch Loss = 0.0007309028493800085
1320, epoch_train_loss=0.0007309028493800085
Epoch 1321
Epoch 1321 :: Batch 0/1
Batch Loss = 0.0007309028493800083
1321, epoch_train_loss=0.0007309028493800083
Epoch 1322
Epoch 1322 :: Batch 0/1
Batch Loss = 0.0007309028493800083
1322, epoch_train_loss=0.0007309028493800083
Epoch 1323
Epoch 1323 :: Batch 0/1
Batch Loss = 0.0007309028493800083
1323, epoch_train_loss=0.0007309028493800083
Epoch 1324
Epoch 1324 :: Batch 0/1
Batch Loss = 0.0007309028493800083
1324, epoch_train_loss=0.0007309028493800083
Epoch 1325
Epoch 1325 :: Batch 0/1
Batch Loss = 0.0007309028493800083
1325, epoch_train_loss=0.0007309028493800083
Epoch 1326
Epoch 1326 :: Batch 0/1
Batch Loss = 0.0007309028493800083
1326, epoch_train_loss=0.0007309028493800083
Epoch 1327
Epoch 1327 :: Batch 0/1
Batch Loss = 0.0007309028493800083
1327, epoch_train_loss=0.0007309028493800083
Epoch 1328
Epoch 1328 :: Batch 0/1
Batch Loss = 0.0007309028493800081
1328, epoch_train_loss=0.0007309028493800081
Epoch 1329
Epoch 1329 :: Batch 0/1
Batch Loss = 0.0007309028493800081
1329, epoch_train_loss=0.0007309028493800081
Epoch 1330
Epoch 1330 :: Batch 0/1
Batch Loss = 0.000730902849380008
1330, epoch_train_loss=0.000730902849380008
Epoch 1331
Epoch 1331 :: Batch 0/1
Batch Loss = 0.000730902849380008
1331, epoch_train_loss=0.000730902849380008
Epoch 1332
Epoch 1332 :: Batch 0/1
Batch Loss = 0.000730902849380008
1332, epoch_train_loss=0.000730902849380008
Epoch 1333
Epoch 1333 :: Batch 0/1
Batch Loss = 0.000730902849380008
1333, epoch_train_loss=0.000730902849380008
Epoch 1334
Epoch 1334 :: Batch 0/1
Batch Loss = 0.000730902849380008
1334, epoch_train_loss=0.000730902849380008
Epoch 1335
Epoch 1335 :: Batch 0/1
Batch Loss = 0.0007309028493800079
1335, epoch_train_loss=0.0007309028493800079
Epoch 1336
Epoch 1336 :: Batch 0/1
Batch Loss = 0.0007309028493800079
1336, epoch_train_loss=0.0007309028493800079
Epoch 1337
Epoch 1337 :: Batch 0/1
Batch Loss = 0.0007309028493800079
1337, epoch_train_loss=0.0007309028493800079
Epoch 1338
Epoch 1338 :: Batch 0/1
Batch Loss = 0.0007309028493800078
1338, epoch_train_loss=0.0007309028493800078
Epoch 1339
Epoch 1339 :: Batch 0/1
Batch Loss = 0.0007309028493800078
1339, epoch_train_loss=0.0007309028493800078
Epoch 1340
Epoch 1340 :: Batch 0/1
Batch Loss = 0.0007309028493800078
1340, epoch_train_loss=0.0007309028493800078
Epoch 1341
Epoch 1341 :: Batch 0/1
Batch Loss = 0.0007309028493800078
1341, epoch_train_loss=0.0007309028493800078
Epoch 1342
Epoch 1342 :: Batch 0/1
Batch Loss = 0.0007309028493800077
1342, epoch_train_loss=0.0007309028493800077
Epoch 1343
Epoch 1343 :: Batch 0/1
Batch Loss = 0.0007309028493800077
1343, epoch_train_loss=0.0007309028493800077
Epoch 1344
Epoch 1344 :: Batch 0/1
Batch Loss = 0.0007309028493800077
1344, epoch_train_loss=0.0007309028493800077
Epoch 1345
Epoch 1345 :: Batch 0/1
Batch Loss = 0.0007309028493800076
1345, epoch_train_loss=0.0007309028493800076
Epoch 1346
Epoch 1346 :: Batch 0/1
Batch Loss = 0.0007309028493800076
1346, epoch_train_loss=0.0007309028493800076
Epoch 1347
Epoch 1347 :: Batch 0/1
Batch Loss = 0.0007309028493800076
1347, epoch_train_loss=0.0007309028493800076
Epoch 1348
Epoch 1348 :: Batch 0/1
Batch Loss = 0.0007309028493800076
1348, epoch_train_loss=0.0007309028493800076
Epoch 1349
Epoch 1349 :: Batch 0/1
Batch Loss = 0.0007309028493800076
1349, epoch_train_loss=0.0007309028493800076
Epoch 1350
Epoch 1350 :: Batch 0/1
Batch Loss = 0.0007309028493800075
1350, epoch_train_loss=0.0007309028493800075
Epoch 1351
Epoch 1351 :: Batch 0/1
Batch Loss = 0.0007309028493800075
1351, epoch_train_loss=0.0007309028493800075
Epoch 1352
Epoch 1352 :: Batch 0/1
Batch Loss = 0.0007309028493800075
1352, epoch_train_loss=0.0007309028493800075
Epoch 1353
Epoch 1353 :: Batch 0/1
Batch Loss = 0.0007309028493800074
1353, epoch_train_loss=0.0007309028493800074
Epoch 1354
Epoch 1354 :: Batch 0/1
Batch Loss = 0.0007309028493800074
1354, epoch_train_loss=0.0007309028493800074
Epoch 1355
Epoch 1355 :: Batch 0/1
Batch Loss = 0.0007309028493800074
1355, epoch_train_loss=0.0007309028493800074
Epoch 1356
Epoch 1356 :: Batch 0/1
Batch Loss = 0.0007309028493800074
1356, epoch_train_loss=0.0007309028493800074
Epoch 1357
Epoch 1357 :: Batch 0/1
Batch Loss = 0.0007309028493800074
1357, epoch_train_loss=0.0007309028493800074
Epoch 1358
Epoch 1358 :: Batch 0/1
Batch Loss = 0.0007309028493800073
1358, epoch_train_loss=0.0007309028493800073
Epoch 1359
Epoch 1359 :: Batch 0/1
Batch Loss = 0.0007309028493800073
1359, epoch_train_loss=0.0007309028493800073
Epoch 1360
Epoch 1360 :: Batch 0/1
Batch Loss = 0.0007309028493800073
1360, epoch_train_loss=0.0007309028493800073
Epoch 1361
Epoch 1361 :: Batch 0/1
Batch Loss = 0.0007309028493800073
1361, epoch_train_loss=0.0007309028493800073
Epoch 1362
Epoch 1362 :: Batch 0/1
Batch Loss = 0.0007309028493800072
1362, epoch_train_loss=0.0007309028493800072
Epoch 1363
Epoch 1363 :: Batch 0/1
Batch Loss = 0.0007309028493800072
1363, epoch_train_loss=0.0007309028493800072
Epoch 1364
Epoch 1364 :: Batch 0/1
Batch Loss = 0.0007309028493800072
1364, epoch_train_loss=0.0007309028493800072
Epoch 1365
Epoch 1365 :: Batch 0/1
Batch Loss = 0.0007309028493800072
1365, epoch_train_loss=0.0007309028493800072
Epoch 1366
Epoch 1366 :: Batch 0/1
Batch Loss = 0.0007309028493800069
1366, epoch_train_loss=0.0007309028493800069
Epoch 1367
Epoch 1367 :: Batch 0/1
Batch Loss = 0.0007309028493800069
1367, epoch_train_loss=0.0007309028493800069
Epoch 1368
Epoch 1368 :: Batch 0/1
Batch Loss = 0.0007309028493800068
1368, epoch_train_loss=0.0007309028493800068
Epoch 1369
Epoch 1369 :: Batch 0/1
Batch Loss = 0.0007309028493800068
1369, epoch_train_loss=0.0007309028493800068
Epoch 1370
Epoch 1370 :: Batch 0/1
Batch Loss = 0.0007309028493800068
1370, epoch_train_loss=0.0007309028493800068
Epoch 1371
Epoch 1371 :: Batch 0/1
Batch Loss = 0.0007309028493800068
1371, epoch_train_loss=0.0007309028493800068
Epoch 1372
Epoch 1372 :: Batch 0/1
Batch Loss = 0.0007309028493800067
1372, epoch_train_loss=0.0007309028493800067
Epoch 1373
Epoch 1373 :: Batch 0/1
Batch Loss = 0.0007309028493800067
1373, epoch_train_loss=0.0007309028493800067
Epoch 1374
Epoch 1374 :: Batch 0/1
Batch Loss = 0.0007309028493800067
1374, epoch_train_loss=0.0007309028493800067
Epoch 1375
Epoch 1375 :: Batch 0/1
Batch Loss = 0.0007309028493800067
1375, epoch_train_loss=0.0007309028493800067
Epoch 1376
Epoch 1376 :: Batch 0/1
Batch Loss = 0.0007309028493800067
1376, epoch_train_loss=0.0007309028493800067
Epoch 1377
Epoch 1377 :: Batch 0/1
Batch Loss = 0.0007309028493800066
1377, epoch_train_loss=0.0007309028493800066
Epoch 1378
Epoch 1378 :: Batch 0/1
Batch Loss = 0.0007309028493800066
1378, epoch_train_loss=0.0007309028493800066
Epoch 1379
Epoch 1379 :: Batch 0/1
Batch Loss = 0.0007309028493800066
1379, epoch_train_loss=0.0007309028493800066
Epoch 1380
Epoch 1380 :: Batch 0/1
Batch Loss = 0.0007309028493800066
1380, epoch_train_loss=0.0007309028493800066
Epoch 1381
Epoch 1381 :: Batch 0/1
Batch Loss = 0.0007309028493800065
1381, epoch_train_loss=0.0007309028493800065
Epoch 1382
Epoch 1382 :: Batch 0/1
Batch Loss = 0.0007309028493800065
1382, epoch_train_loss=0.0007309028493800065
Epoch 1383
Epoch 1383 :: Batch 0/1
Batch Loss = 0.0007309028493800065
1383, epoch_train_loss=0.0007309028493800065
Epoch 1384
Epoch 1384 :: Batch 0/1
Batch Loss = 0.0007309028493800064
1384, epoch_train_loss=0.0007309028493800064
Epoch 1385
Epoch 1385 :: Batch 0/1
Batch Loss = 0.0007309028493800064
1385, epoch_train_loss=0.0007309028493800064
Epoch 1386
Epoch 1386 :: Batch 0/1
Batch Loss = 0.0007309028493800064
1386, epoch_train_loss=0.0007309028493800064
Epoch 1387
Epoch 1387 :: Batch 0/1
Batch Loss = 0.0007309028493800064
1387, epoch_train_loss=0.0007309028493800064
Epoch 1388
Epoch 1388 :: Batch 0/1
Batch Loss = 0.0007309028493800063
1388, epoch_train_loss=0.0007309028493800063
Epoch 1389
Epoch 1389 :: Batch 0/1
Batch Loss = 0.0007309028493800063
1389, epoch_train_loss=0.0007309028493800063
Epoch 1390
Epoch 1390 :: Batch 0/1
Batch Loss = 0.0007309028493800063
1390, epoch_train_loss=0.0007309028493800063
Epoch 1391
Epoch 1391 :: Batch 0/1
Batch Loss = 0.0007309028493800062
1391, epoch_train_loss=0.0007309028493800062
Epoch 1392
Epoch 1392 :: Batch 0/1
Batch Loss = 0.0007309028493800062
1392, epoch_train_loss=0.0007309028493800062
Epoch 1393
Epoch 1393 :: Batch 0/1
Batch Loss = 0.0007309028493800062
1393, epoch_train_loss=0.0007309028493800062
Epoch 1394
Epoch 1394 :: Batch 0/1
Batch Loss = 0.0007309028493800062
1394, epoch_train_loss=0.0007309028493800062
Epoch 1395
Epoch 1395 :: Batch 0/1
Batch Loss = 0.0007309028493800061
1395, epoch_train_loss=0.0007309028493800061
Epoch 1396
Epoch 1396 :: Batch 0/1
Batch Loss = 0.0007309028493800059
1396, epoch_train_loss=0.0007309028493800059
Epoch 1397
Epoch 1397 :: Batch 0/1
Batch Loss = 0.0007309028493800059
1397, epoch_train_loss=0.0007309028493800059
Epoch 1398
Epoch 1398 :: Batch 0/1
Batch Loss = 0.0007309028493800059
1398, epoch_train_loss=0.0007309028493800059
Epoch 1399
Epoch 1399 :: Batch 0/1
Batch Loss = 0.0007309028493800059
1399, epoch_train_loss=0.0007309028493800059
Epoch 1400
Epoch 1400 :: Batch 0/1
Batch Loss = 0.0007309028493800059
1400, epoch_train_loss=0.0007309028493800059
Epoch 1401
Epoch 1401 :: Batch 0/1
Batch Loss = 0.0007309028493800059
1401, epoch_train_loss=0.0007309028493800059
Epoch 1402
Epoch 1402 :: Batch 0/1
Batch Loss = 0.0007309028493800056
1402, epoch_train_loss=0.0007309028493800056
Epoch 1403
Epoch 1403 :: Batch 0/1
Batch Loss = 0.0007309028493800056
1403, epoch_train_loss=0.0007309028493800056
Epoch 1404
Epoch 1404 :: Batch 0/1
Batch Loss = 0.0007309028493800056
1404, epoch_train_loss=0.0007309028493800056
Epoch 1405
Epoch 1405 :: Batch 0/1
Batch Loss = 0.0007309028493800056
1405, epoch_train_loss=0.0007309028493800056
Epoch 1406
Epoch 1406 :: Batch 0/1
Batch Loss = 0.0007309028493800056
1406, epoch_train_loss=0.0007309028493800056
Epoch 1407
Epoch 1407 :: Batch 0/1
Batch Loss = 0.0007309028493800056
1407, epoch_train_loss=0.0007309028493800056
Epoch 1408
Epoch 1408 :: Batch 0/1
Batch Loss = 0.0007309028493800056
1408, epoch_train_loss=0.0007309028493800056
Epoch 1409
Epoch 1409 :: Batch 0/1
Batch Loss = 0.0007309028493800056
1409, epoch_train_loss=0.0007309028493800056
Epoch 1410
Epoch 1410 :: Batch 0/1
Batch Loss = 0.0007309028493800055
1410, epoch_train_loss=0.0007309028493800055
Epoch 1411
Epoch 1411 :: Batch 0/1
Batch Loss = 0.0007309028493800055
1411, epoch_train_loss=0.0007309028493800055
Epoch 1412
Epoch 1412 :: Batch 0/1
Batch Loss = 0.0007309028493800055
1412, epoch_train_loss=0.0007309028493800055
Epoch 1413
Epoch 1413 :: Batch 0/1
Batch Loss = 0.0007309028493800054
1413, epoch_train_loss=0.0007309028493800054
Epoch 1414
Epoch 1414 :: Batch 0/1
Batch Loss = 0.0007309028493800054
1414, epoch_train_loss=0.0007309028493800054
Epoch 1415
Epoch 1415 :: Batch 0/1
Batch Loss = 0.0007309028493800054
1415, epoch_train_loss=0.0007309028493800054
Epoch 1416
Epoch 1416 :: Batch 0/1
Batch Loss = 0.0007309028493800054
1416, epoch_train_loss=0.0007309028493800054
Epoch 1417
Epoch 1417 :: Batch 0/1
Batch Loss = 0.0007309028493800054
1417, epoch_train_loss=0.0007309028493800054
Epoch 1418
Epoch 1418 :: Batch 0/1
Batch Loss = 0.0007309028493800053
1418, epoch_train_loss=0.0007309028493800053
Epoch 1419
Epoch 1419 :: Batch 0/1
Batch Loss = 0.0007309028493800052
1419, epoch_train_loss=0.0007309028493800052
Epoch 1420
Epoch 1420 :: Batch 0/1
Batch Loss = 0.0007309028493800052
1420, epoch_train_loss=0.0007309028493800052
Epoch 1421
Epoch 1421 :: Batch 0/1
Batch Loss = 0.0007309028493800052
1421, epoch_train_loss=0.0007309028493800052
Epoch 1422
Epoch 1422 :: Batch 0/1
Batch Loss = 0.0007309028493800052
1422, epoch_train_loss=0.0007309028493800052
Epoch 1423
Epoch 1423 :: Batch 0/1
Batch Loss = 0.0007309028493800052
1423, epoch_train_loss=0.0007309028493800052
Epoch 1424
Epoch 1424 :: Batch 0/1
Batch Loss = 0.0007309028493800052
1424, epoch_train_loss=0.0007309028493800052
Epoch 1425
Epoch 1425 :: Batch 0/1
Batch Loss = 0.0007309028493800051
1425, epoch_train_loss=0.0007309028493800051
Epoch 1426
Epoch 1426 :: Batch 0/1
Batch Loss = 0.0007309028493800051
1426, epoch_train_loss=0.0007309028493800051
Epoch 1427
Epoch 1427 :: Batch 0/1
Batch Loss = 0.000730902849380005
1427, epoch_train_loss=0.000730902849380005
Epoch 1428
Epoch 1428 :: Batch 0/1
Batch Loss = 0.000730902849380005
1428, epoch_train_loss=0.000730902849380005
Epoch 1429
Epoch 1429 :: Batch 0/1
Batch Loss = 0.000730902849380005
1429, epoch_train_loss=0.000730902849380005
Epoch 1430
Epoch 1430 :: Batch 0/1
Batch Loss = 0.000730902849380005
1430, epoch_train_loss=0.000730902849380005
Epoch 1431
Epoch 1431 :: Batch 0/1
Batch Loss = 0.000730902849380005
1431, epoch_train_loss=0.000730902849380005
Epoch 1432
Epoch 1432 :: Batch 0/1
Batch Loss = 0.0007309028493800048
1432, epoch_train_loss=0.0007309028493800048
Epoch 1433
Epoch 1433 :: Batch 0/1
Batch Loss = 0.0007309028493800048
1433, epoch_train_loss=0.0007309028493800048
Epoch 1434
Epoch 1434 :: Batch 0/1
Batch Loss = 0.0007309028493800047
1434, epoch_train_loss=0.0007309028493800047
Epoch 1435
Epoch 1435 :: Batch 0/1
Batch Loss = 0.0007309028493800047
1435, epoch_train_loss=0.0007309028493800047
Epoch 1436
Epoch 1436 :: Batch 0/1
Batch Loss = 0.0007309028493800047
1436, epoch_train_loss=0.0007309028493800047
Epoch 1437
Epoch 1437 :: Batch 0/1
Batch Loss = 0.0007309028493800047
1437, epoch_train_loss=0.0007309028493800047
Epoch 1438
Epoch 1438 :: Batch 0/1
Batch Loss = 0.0007309028493800047
1438, epoch_train_loss=0.0007309028493800047
Epoch 1439
Epoch 1439 :: Batch 0/1
Batch Loss = 0.0007309028493800046
1439, epoch_train_loss=0.0007309028493800046
Epoch 1440
Epoch 1440 :: Batch 0/1
Batch Loss = 0.0007309028493800046
1440, epoch_train_loss=0.0007309028493800046
Epoch 1441
Epoch 1441 :: Batch 0/1
Batch Loss = 0.0007309028493800046
1441, epoch_train_loss=0.0007309028493800046
Epoch 1442
Epoch 1442 :: Batch 0/1
Batch Loss = 0.0007309028493800046
1442, epoch_train_loss=0.0007309028493800046
Epoch 1443
Epoch 1443 :: Batch 0/1
Batch Loss = 0.0007309028493800044
1443, epoch_train_loss=0.0007309028493800044
Epoch 1444
Epoch 1444 :: Batch 0/1
Batch Loss = 0.0007309028493800044
1444, epoch_train_loss=0.0007309028493800044
Epoch 1445
Epoch 1445 :: Batch 0/1
Batch Loss = 0.0007309028493800044
1445, epoch_train_loss=0.0007309028493800044
Epoch 1446
Epoch 1446 :: Batch 0/1
Batch Loss = 0.0007309028493800043
1446, epoch_train_loss=0.0007309028493800043
Epoch 1447
Epoch 1447 :: Batch 0/1
Batch Loss = 0.0007309028493800043
1447, epoch_train_loss=0.0007309028493800043
Epoch 1448
Epoch 1448 :: Batch 0/1
Batch Loss = 0.0007309028493800043
1448, epoch_train_loss=0.0007309028493800043
Epoch 1449
Epoch 1449 :: Batch 0/1
Batch Loss = 0.0007309028493800042
1449, epoch_train_loss=0.0007309028493800042
Epoch 1450
Epoch 1450 :: Batch 0/1
Batch Loss = 0.0007309028493800042
1450, epoch_train_loss=0.0007309028493800042
Epoch 1451
Epoch 1451 :: Batch 0/1
Batch Loss = 0.0007309028493800042
1451, epoch_train_loss=0.0007309028493800042
Epoch 1452
Epoch 1452 :: Batch 0/1
Batch Loss = 0.0007309028493800042
1452, epoch_train_loss=0.0007309028493800042
Epoch 1453
Epoch 1453 :: Batch 0/1
Batch Loss = 0.0007309028493800042
1453, epoch_train_loss=0.0007309028493800042
Epoch 1454
Epoch 1454 :: Batch 0/1
Batch Loss = 0.0007309028493800041
1454, epoch_train_loss=0.0007309028493800041
Epoch 1455
Epoch 1455 :: Batch 0/1
Batch Loss = 0.000730902849380004
1455, epoch_train_loss=0.000730902849380004
Epoch 1456
Epoch 1456 :: Batch 0/1
Batch Loss = 0.000730902849380004
1456, epoch_train_loss=0.000730902849380004
Epoch 1457
Epoch 1457 :: Batch 0/1
Batch Loss = 0.000730902849380004
1457, epoch_train_loss=0.000730902849380004
Epoch 1458
Epoch 1458 :: Batch 0/1
Batch Loss = 0.000730902849380004
1458, epoch_train_loss=0.000730902849380004
Epoch 1459
Epoch 1459 :: Batch 0/1
Batch Loss = 0.000730902849380004
1459, epoch_train_loss=0.000730902849380004
Epoch 1460
Epoch 1460 :: Batch 0/1
Batch Loss = 0.0007309028493800039
1460, epoch_train_loss=0.0007309028493800039
Epoch 1461
Epoch 1461 :: Batch 0/1
Batch Loss = 0.0007309028493800038
1461, epoch_train_loss=0.0007309028493800038
Epoch 1462
Epoch 1462 :: Batch 0/1
Batch Loss = 0.0007309028493800038
1462, epoch_train_loss=0.0007309028493800038
Epoch 1463
Epoch 1463 :: Batch 0/1
Batch Loss = 0.0007309028493800038
1463, epoch_train_loss=0.0007309028493800038
Epoch 1464
Epoch 1464 :: Batch 0/1
Batch Loss = 0.0007309028493800038
1464, epoch_train_loss=0.0007309028493800038
Epoch 1465
Epoch 1465 :: Batch 0/1
Batch Loss = 0.0007309028493800038
1465, epoch_train_loss=0.0007309028493800038
Epoch 1466
Epoch 1466 :: Batch 0/1
Batch Loss = 0.0007309028493800038
1466, epoch_train_loss=0.0007309028493800038
Epoch 1467
Epoch 1467 :: Batch 0/1
Batch Loss = 0.0007309028493800036
1467, epoch_train_loss=0.0007309028493800036
Epoch 1468
Epoch 1468 :: Batch 0/1
Batch Loss = 0.0007309028493800035
1468, epoch_train_loss=0.0007309028493800035
Epoch 1469
Epoch 1469 :: Batch 0/1
Batch Loss = 0.0007309028493800035
1469, epoch_train_loss=0.0007309028493800035
Epoch 1470
Epoch 1470 :: Batch 0/1
Batch Loss = 0.0007309028493800035
1470, epoch_train_loss=0.0007309028493800035
Epoch 1471
Epoch 1471 :: Batch 0/1
Batch Loss = 0.0007309028493800035
1471, epoch_train_loss=0.0007309028493800035
Epoch 1472
Epoch 1472 :: Batch 0/1
Batch Loss = 0.0007309028493800035
1472, epoch_train_loss=0.0007309028493800035
Epoch 1473
Epoch 1473 :: Batch 0/1
Batch Loss = 0.0007309028493800035
1473, epoch_train_loss=0.0007309028493800035
Epoch 1474
Epoch 1474 :: Batch 0/1
Batch Loss = 0.0007309028493800035
1474, epoch_train_loss=0.0007309028493800035
Epoch 1475
Epoch 1475 :: Batch 0/1
Batch Loss = 0.0007309028493800034
1475, epoch_train_loss=0.0007309028493800034
Epoch 1476
Epoch 1476 :: Batch 0/1
Batch Loss = 0.0007309028493800034
1476, epoch_train_loss=0.0007309028493800034
Epoch 1477
Epoch 1477 :: Batch 0/1
Batch Loss = 0.0007309028493800033
1477, epoch_train_loss=0.0007309028493800033
Epoch 1478
Epoch 1478 :: Batch 0/1
Batch Loss = 0.0007309028493800033
1478, epoch_train_loss=0.0007309028493800033
Epoch 1479
Epoch 1479 :: Batch 0/1
Batch Loss = 0.0007309028493800033
1479, epoch_train_loss=0.0007309028493800033
Epoch 1480
Epoch 1480 :: Batch 0/1
Batch Loss = 0.0007309028493800033
1480, epoch_train_loss=0.0007309028493800033
Epoch 1481
Epoch 1481 :: Batch 0/1
Batch Loss = 0.0007309028493800033
1481, epoch_train_loss=0.0007309028493800033
Epoch 1482
Epoch 1482 :: Batch 0/1
Batch Loss = 0.0007309028493800033
1482, epoch_train_loss=0.0007309028493800033
Epoch 1483
Epoch 1483 :: Batch 0/1
Batch Loss = 0.0007309028493800031
1483, epoch_train_loss=0.0007309028493800031
Epoch 1484
Epoch 1484 :: Batch 0/1
Batch Loss = 0.000730902849380003
1484, epoch_train_loss=0.000730902849380003
Epoch 1485
Epoch 1485 :: Batch 0/1
Batch Loss = 0.000730902849380003
1485, epoch_train_loss=0.000730902849380003
Epoch 1486
Epoch 1486 :: Batch 0/1
Batch Loss = 0.000730902849380003
1486, epoch_train_loss=0.000730902849380003
Epoch 1487
Epoch 1487 :: Batch 0/1
Batch Loss = 0.000730902849380003
1487, epoch_train_loss=0.000730902849380003
Epoch 1488
Epoch 1488 :: Batch 0/1
Batch Loss = 0.0007309028493800029
1488, epoch_train_loss=0.0007309028493800029
Epoch 1489
Epoch 1489 :: Batch 0/1
Batch Loss = 0.0007309028493800029
1489, epoch_train_loss=0.0007309028493800029
Epoch 1490
Epoch 1490 :: Batch 0/1
Batch Loss = 0.0007309028493800029
1490, epoch_train_loss=0.0007309028493800029
Epoch 1491
Epoch 1491 :: Batch 0/1
Batch Loss = 0.0007309028493800028
1491, epoch_train_loss=0.0007309028493800028
Epoch 1492
Epoch 1492 :: Batch 0/1
Batch Loss = 0.0007309028493800028
1492, epoch_train_loss=0.0007309028493800028
Epoch 1493
Epoch 1493 :: Batch 0/1
Batch Loss = 0.0007309028493800028
1493, epoch_train_loss=0.0007309028493800028
Epoch 1494
Epoch 1494 :: Batch 0/1
Batch Loss = 0.0007309028493800028
1494, epoch_train_loss=0.0007309028493800028
Epoch 1495
Epoch 1495 :: Batch 0/1
Batch Loss = 0.0007309028493800027
1495, epoch_train_loss=0.0007309028493800027
Epoch 1496
Epoch 1496 :: Batch 0/1
Batch Loss = 0.0007309028493800027
1496, epoch_train_loss=0.0007309028493800027
Epoch 1497
Epoch 1497 :: Batch 0/1
Batch Loss = 0.0007309028493800027
1497, epoch_train_loss=0.0007309028493800027
Epoch 1498
Epoch 1498 :: Batch 0/1
Batch Loss = 0.0007309028493800025
1498, epoch_train_loss=0.0007309028493800025
Epoch 1499
Epoch 1499 :: Batch 0/1
Batch Loss = 0.0007309028493800025
1499, epoch_train_loss=0.0007309028493800025
Epoch 1500
Epoch 1500 :: Batch 0/1
Batch Loss = 0.0007309028493800025
1500, epoch_train_loss=0.0007309028493800025
Epoch 1501
Epoch 1501 :: Batch 0/1
Batch Loss = 0.0007309028493800025
1501, epoch_train_loss=0.0007309028493800025
Epoch 1502
Epoch 1502 :: Batch 0/1
Batch Loss = 0.0007309028493800024
1502, epoch_train_loss=0.0007309028493800024
Epoch 1503
Epoch 1503 :: Batch 0/1
Batch Loss = 0.0007309028493800024
1503, epoch_train_loss=0.0007309028493800024
Epoch 1504
Epoch 1504 :: Batch 0/1
Batch Loss = 0.0007309028493800024
1504, epoch_train_loss=0.0007309028493800024
Epoch 1505
Epoch 1505 :: Batch 0/1
Batch Loss = 0.0007309028493800023
1505, epoch_train_loss=0.0007309028493800023
Epoch 1506
Epoch 1506 :: Batch 0/1
Batch Loss = 0.0007309028493800023
1506, epoch_train_loss=0.0007309028493800023
Epoch 1507
Epoch 1507 :: Batch 0/1
Batch Loss = 0.0007309028493800023
1507, epoch_train_loss=0.0007309028493800023
Epoch 1508
Epoch 1508 :: Batch 0/1
Batch Loss = 0.0007309028493800022
1508, epoch_train_loss=0.0007309028493800022
Epoch 1509
Epoch 1509 :: Batch 0/1
Batch Loss = 0.0007309028493800022
1509, epoch_train_loss=0.0007309028493800022
Epoch 1510
Epoch 1510 :: Batch 0/1
Batch Loss = 0.0007309028493800022
1510, epoch_train_loss=0.0007309028493800022
Epoch 1511
Epoch 1511 :: Batch 0/1
Batch Loss = 0.0007309028493800022
1511, epoch_train_loss=0.0007309028493800022
Epoch 1512
Epoch 1512 :: Batch 0/1
Batch Loss = 0.0007309028493800021
1512, epoch_train_loss=0.0007309028493800021
Epoch 1513
Epoch 1513 :: Batch 0/1
Batch Loss = 0.0007309028493800021
1513, epoch_train_loss=0.0007309028493800021
Epoch 1514
Epoch 1514 :: Batch 0/1
Batch Loss = 0.000730902849380002
1514, epoch_train_loss=0.000730902849380002
Epoch 1515
Epoch 1515 :: Batch 0/1
Batch Loss = 0.000730902849380002
1515, epoch_train_loss=0.000730902849380002
Epoch 1516
Epoch 1516 :: Batch 0/1
Batch Loss = 0.000730902849380002
1516, epoch_train_loss=0.000730902849380002
Epoch 1517
Epoch 1517 :: Batch 0/1
Batch Loss = 0.000730902849380002
1517, epoch_train_loss=0.000730902849380002
Epoch 1518
Epoch 1518 :: Batch 0/1
Batch Loss = 0.000730902849380002
1518, epoch_train_loss=0.000730902849380002
Epoch 1519
Epoch 1519 :: Batch 0/1
Batch Loss = 0.000730902849380002
1519, epoch_train_loss=0.000730902849380002
Epoch 1520
Epoch 1520 :: Batch 0/1
Batch Loss = 0.0007309028493800018
1520, epoch_train_loss=0.0007309028493800018
Epoch 1521
Epoch 1521 :: Batch 0/1
Batch Loss = 0.0007309028493800018
1521, epoch_train_loss=0.0007309028493800018
Epoch 1522
Epoch 1522 :: Batch 0/1
Batch Loss = 0.0007309028493800018
1522, epoch_train_loss=0.0007309028493800018
Epoch 1523
Epoch 1523 :: Batch 0/1
Batch Loss = 0.0007309028493800017
1523, epoch_train_loss=0.0007309028493800017
Epoch 1524
Epoch 1524 :: Batch 0/1
Batch Loss = 0.0007309028493800017
1524, epoch_train_loss=0.0007309028493800017
Epoch 1525
Epoch 1525 :: Batch 0/1
Batch Loss = 0.0007309028493800017
1525, epoch_train_loss=0.0007309028493800017
Epoch 1526
Epoch 1526 :: Batch 0/1
Batch Loss = 0.0007309028493800016
1526, epoch_train_loss=0.0007309028493800016
Epoch 1527
Epoch 1527 :: Batch 0/1
Batch Loss = 0.0007309028493800016
1527, epoch_train_loss=0.0007309028493800016
Epoch 1528
Epoch 1528 :: Batch 0/1
Batch Loss = 0.0007309028493800015
1528, epoch_train_loss=0.0007309028493800015
Epoch 1529
Epoch 1529 :: Batch 0/1
Batch Loss = 0.0007309028493800015
1529, epoch_train_loss=0.0007309028493800015
Epoch 1530
Epoch 1530 :: Batch 0/1
Batch Loss = 0.0007309028493800013
1530, epoch_train_loss=0.0007309028493800013
Epoch 1531
Epoch 1531 :: Batch 0/1
Batch Loss = 0.0007309028493800013
1531, epoch_train_loss=0.0007309028493800013
Epoch 1532
Epoch 1532 :: Batch 0/1
Batch Loss = 0.0007309028493800013
1532, epoch_train_loss=0.0007309028493800013
Epoch 1533
Epoch 1533 :: Batch 0/1
Batch Loss = 0.0007309028493800013
1533, epoch_train_loss=0.0007309028493800013
Epoch 1534
Epoch 1534 :: Batch 0/1
Batch Loss = 0.0007309028493800013
1534, epoch_train_loss=0.0007309028493800013
Epoch 1535
Epoch 1535 :: Batch 0/1
Batch Loss = 0.0007309028493800012
1535, epoch_train_loss=0.0007309028493800012
Epoch 1536
Epoch 1536 :: Batch 0/1
Batch Loss = 0.0007309028493800012
1536, epoch_train_loss=0.0007309028493800012
Epoch 1537
Epoch 1537 :: Batch 0/1
Batch Loss = 0.0007309028493800011
1537, epoch_train_loss=0.0007309028493800011
Epoch 1538
Epoch 1538 :: Batch 0/1
Batch Loss = 0.0007309028493800011
1538, epoch_train_loss=0.0007309028493800011
Epoch 1539
Epoch 1539 :: Batch 0/1
Batch Loss = 0.0007309028493800011
1539, epoch_train_loss=0.0007309028493800011
Epoch 1540
Epoch 1540 :: Batch 0/1
Batch Loss = 0.0007309028493800011
1540, epoch_train_loss=0.0007309028493800011
Epoch 1541
Epoch 1541 :: Batch 0/1
Batch Loss = 0.0007309028493800011
1541, epoch_train_loss=0.0007309028493800011
Epoch 1542
Epoch 1542 :: Batch 0/1
Batch Loss = 0.000730902849380001
1542, epoch_train_loss=0.000730902849380001
Epoch 1543
Epoch 1543 :: Batch 0/1
Batch Loss = 0.000730902849380001
1543, epoch_train_loss=0.000730902849380001
Epoch 1544
Epoch 1544 :: Batch 0/1
Batch Loss = 0.000730902849380001
1544, epoch_train_loss=0.000730902849380001
Epoch 1545
Epoch 1545 :: Batch 0/1
Batch Loss = 0.000730902849380001
1545, epoch_train_loss=0.000730902849380001
Epoch 1546
Epoch 1546 :: Batch 0/1
Batch Loss = 0.0007309028493800009
1546, epoch_train_loss=0.0007309028493800009
Epoch 1547
Epoch 1547 :: Batch 0/1
Batch Loss = 0.0007309028493800009
1547, epoch_train_loss=0.0007309028493800009
Epoch 1548
Epoch 1548 :: Batch 0/1
Batch Loss = 0.0007309028493800009
1548, epoch_train_loss=0.0007309028493800009
Epoch 1549
Epoch 1549 :: Batch 0/1
Batch Loss = 0.0007309028493800009
1549, epoch_train_loss=0.0007309028493800009
Epoch 1550
Epoch 1550 :: Batch 0/1
Batch Loss = 0.0007309028493800008
1550, epoch_train_loss=0.0007309028493800008
Epoch 1551
Epoch 1551 :: Batch 0/1
Batch Loss = 0.0007309028493800008
1551, epoch_train_loss=0.0007309028493800008
Epoch 1552
Epoch 1552 :: Batch 0/1
Batch Loss = 0.0007309028493800008
1552, epoch_train_loss=0.0007309028493800008
Epoch 1553
Epoch 1553 :: Batch 0/1
Batch Loss = 0.0007309028493800006
1553, epoch_train_loss=0.0007309028493800006
Epoch 1554
Epoch 1554 :: Batch 0/1
Batch Loss = 0.0007309028493800006
1554, epoch_train_loss=0.0007309028493800006
Epoch 1555
Epoch 1555 :: Batch 0/1
Batch Loss = 0.0007309028493800006
1555, epoch_train_loss=0.0007309028493800006
Epoch 1556
Epoch 1556 :: Batch 0/1
Batch Loss = 0.0007309028493800006
1556, epoch_train_loss=0.0007309028493800006
Epoch 1557
Epoch 1557 :: Batch 0/1
Batch Loss = 0.0007309028493800006
1557, epoch_train_loss=0.0007309028493800006
Epoch 1558
Epoch 1558 :: Batch 0/1
Batch Loss = 0.0007309028493800005
1558, epoch_train_loss=0.0007309028493800005
Epoch 1559
Epoch 1559 :: Batch 0/1
Batch Loss = 0.0007309028493800004
1559, epoch_train_loss=0.0007309028493800004
Epoch 1560
Epoch 1560 :: Batch 0/1
Batch Loss = 0.0007309028493800004
1560, epoch_train_loss=0.0007309028493800004
Epoch 1561
Epoch 1561 :: Batch 0/1
Batch Loss = 0.0007309028493800004
1561, epoch_train_loss=0.0007309028493800004
Epoch 1562
Epoch 1562 :: Batch 0/1
Batch Loss = 0.0007309028493800004
1562, epoch_train_loss=0.0007309028493800004
Epoch 1563
Epoch 1563 :: Batch 0/1
Batch Loss = 0.0007309028493800004
1563, epoch_train_loss=0.0007309028493800004
Epoch 1564
Epoch 1564 :: Batch 0/1
Batch Loss = 0.0007309028493800004
1564, epoch_train_loss=0.0007309028493800004
Epoch 1565
Epoch 1565 :: Batch 0/1
Batch Loss = 0.0007309028493800001
1565, epoch_train_loss=0.0007309028493800001
Epoch 1566
Epoch 1566 :: Batch 0/1
Batch Loss = 0.0007309028493800001
1566, epoch_train_loss=0.0007309028493800001
Epoch 1567
Epoch 1567 :: Batch 0/1
Batch Loss = 0.0007309028493800001
1567, epoch_train_loss=0.0007309028493800001
Epoch 1568
Epoch 1568 :: Batch 0/1
Batch Loss = 0.0007309028493800001
1568, epoch_train_loss=0.0007309028493800001
Epoch 1569
Epoch 1569 :: Batch 0/1
Batch Loss = 0.0007309028493800001
1569, epoch_train_loss=0.0007309028493800001
Epoch 1570
Epoch 1570 :: Batch 0/1
Batch Loss = 0.00073090284938
1570, epoch_train_loss=0.00073090284938
Epoch 1571
Epoch 1571 :: Batch 0/1
Batch Loss = 0.00073090284938
1571, epoch_train_loss=0.00073090284938
Epoch 1572
Epoch 1572 :: Batch 0/1
Batch Loss = 0.00073090284938
1572, epoch_train_loss=0.00073090284938
Epoch 1573
Epoch 1573 :: Batch 0/1
Batch Loss = 0.0007309028493799999
1573, epoch_train_loss=0.0007309028493799999
Epoch 1574
Epoch 1574 :: Batch 0/1
Batch Loss = 0.0007309028493799999
1574, epoch_train_loss=0.0007309028493799999
Epoch 1575
Epoch 1575 :: Batch 0/1
Batch Loss = 0.0007309028493799998
1575, epoch_train_loss=0.0007309028493799998
Epoch 1576
Epoch 1576 :: Batch 0/1
Batch Loss = 0.0007309028493799998
1576, epoch_train_loss=0.0007309028493799998
Epoch 1577
Epoch 1577 :: Batch 0/1
Batch Loss = 0.0007309028493799997
1577, epoch_train_loss=0.0007309028493799997
Epoch 1578
Epoch 1578 :: Batch 0/1
Batch Loss = 0.0007309028493799997
1578, epoch_train_loss=0.0007309028493799997
Epoch 1579
Epoch 1579 :: Batch 0/1
Batch Loss = 0.0007309028493799997
1579, epoch_train_loss=0.0007309028493799997
Epoch 1580
Epoch 1580 :: Batch 0/1
Batch Loss = 0.0007309028493799997
1580, epoch_train_loss=0.0007309028493799997
Epoch 1581
Epoch 1581 :: Batch 0/1
Batch Loss = 0.0007309028493799997
1581, epoch_train_loss=0.0007309028493799997
Epoch 1582
Epoch 1582 :: Batch 0/1
Batch Loss = 0.0007309028493799997
1582, epoch_train_loss=0.0007309028493799997
Epoch 1583
Epoch 1583 :: Batch 0/1
Batch Loss = 0.0007309028493799997
1583, epoch_train_loss=0.0007309028493799997
Epoch 1584
Epoch 1584 :: Batch 0/1
Batch Loss = 0.0007309028493799996
1584, epoch_train_loss=0.0007309028493799996
Epoch 1585
Epoch 1585 :: Batch 0/1
Batch Loss = 0.0007309028493799995
1585, epoch_train_loss=0.0007309028493799995
Epoch 1586
Epoch 1586 :: Batch 0/1
Batch Loss = 0.0007309028493799995
1586, epoch_train_loss=0.0007309028493799995
Epoch 1587
Epoch 1587 :: Batch 0/1
Batch Loss = 0.0007309028493799995
1587, epoch_train_loss=0.0007309028493799995
Epoch 1588
Epoch 1588 :: Batch 0/1
Batch Loss = 0.0007309028493799995
1588, epoch_train_loss=0.0007309028493799995
Epoch 1589
Epoch 1589 :: Batch 0/1
Batch Loss = 0.0007309028493799995
1589, epoch_train_loss=0.0007309028493799995
Epoch 1590
Epoch 1590 :: Batch 0/1
Batch Loss = 0.0007309028493799995
1590, epoch_train_loss=0.0007309028493799995
Epoch 1591
Epoch 1591 :: Batch 0/1
Batch Loss = 0.0007309028493799992
1591, epoch_train_loss=0.0007309028493799992
Epoch 1592
Epoch 1592 :: Batch 0/1
Batch Loss = 0.0007309028493799992
1592, epoch_train_loss=0.0007309028493799992
Epoch 1593
Epoch 1593 :: Batch 0/1
Batch Loss = 0.0007309028493799992
1593, epoch_train_loss=0.0007309028493799992
Epoch 1594
Epoch 1594 :: Batch 0/1
Batch Loss = 0.0007309028493799992
1594, epoch_train_loss=0.0007309028493799992
Epoch 1595
Epoch 1595 :: Batch 0/1
Batch Loss = 0.0007309028493799992
1595, epoch_train_loss=0.0007309028493799992
Epoch 1596
Epoch 1596 :: Batch 0/1
Batch Loss = 0.000730902849379999
1596, epoch_train_loss=0.000730902849379999
Epoch 1597
Epoch 1597 :: Batch 0/1
Batch Loss = 0.0007309028493799989
1597, epoch_train_loss=0.0007309028493799989
Epoch 1598
Epoch 1598 :: Batch 0/1
Batch Loss = 0.0007309028493799989
1598, epoch_train_loss=0.0007309028493799989
Epoch 1599
Epoch 1599 :: Batch 0/1
Batch Loss = 0.0007309028493799989
1599, epoch_train_loss=0.0007309028493799989
Epoch 1600
Epoch 1600 :: Batch 0/1
Batch Loss = 0.0007309028493799989
1600, epoch_train_loss=0.0007309028493799989
Epoch 1601
Epoch 1601 :: Batch 0/1
Batch Loss = 0.0007309028493799989
1601, epoch_train_loss=0.0007309028493799989
Epoch 1602
Epoch 1602 :: Batch 0/1
Batch Loss = 0.0007309028493799988
1602, epoch_train_loss=0.0007309028493799988
Epoch 1603
Epoch 1603 :: Batch 0/1
Batch Loss = 0.0007309028493799987
1603, epoch_train_loss=0.0007309028493799987
Epoch 1604
Epoch 1604 :: Batch 0/1
Batch Loss = 0.0007309028493799987
1604, epoch_train_loss=0.0007309028493799987
Epoch 1605
Epoch 1605 :: Batch 0/1
Batch Loss = 0.0007309028493799987
1605, epoch_train_loss=0.0007309028493799987
Epoch 1606
Epoch 1606 :: Batch 0/1
Batch Loss = 0.0007309028493799987
1606, epoch_train_loss=0.0007309028493799987
Epoch 1607
Epoch 1607 :: Batch 0/1
Batch Loss = 0.0007309028493799987
1607, epoch_train_loss=0.0007309028493799987
Epoch 1608
Epoch 1608 :: Batch 0/1
Batch Loss = 0.0007309028493799987
1608, epoch_train_loss=0.0007309028493799987
Epoch 1609
Epoch 1609 :: Batch 0/1
Batch Loss = 0.0007309028493799986
1609, epoch_train_loss=0.0007309028493799986
Epoch 1610
Epoch 1610 :: Batch 0/1
Batch Loss = 0.0007309028493799985
1610, epoch_train_loss=0.0007309028493799985
Epoch 1611
Epoch 1611 :: Batch 0/1
Batch Loss = 0.0007309028493799985
1611, epoch_train_loss=0.0007309028493799985
Epoch 1612
Epoch 1612 :: Batch 0/1
Batch Loss = 0.0007309028493799985
1612, epoch_train_loss=0.0007309028493799985
Epoch 1613
Epoch 1613 :: Batch 0/1
Batch Loss = 0.0007309028493799985
1613, epoch_train_loss=0.0007309028493799985
Epoch 1614
Epoch 1614 :: Batch 0/1
Batch Loss = 0.0007309028493799985
1614, epoch_train_loss=0.0007309028493799985
Epoch 1615
Epoch 1615 :: Batch 0/1
Batch Loss = 0.0007309028493799985
1615, epoch_train_loss=0.0007309028493799985
Epoch 1616
Epoch 1616 :: Batch 0/1
Batch Loss = 0.0007309028493799983
1616, epoch_train_loss=0.0007309028493799983
Epoch 1617
Epoch 1617 :: Batch 0/1
Batch Loss = 0.0007309028493799983
1617, epoch_train_loss=0.0007309028493799983
Epoch 1618
Epoch 1618 :: Batch 0/1
Batch Loss = 0.0007309028493799983
1618, epoch_train_loss=0.0007309028493799983
Epoch 1619
Epoch 1619 :: Batch 0/1
Batch Loss = 0.0007309028493799983
1619, epoch_train_loss=0.0007309028493799983
Epoch 1620
Epoch 1620 :: Batch 0/1
Batch Loss = 0.0007309028493799983
1620, epoch_train_loss=0.0007309028493799983
Epoch 1621
Epoch 1621 :: Batch 0/1
Batch Loss = 0.0007309028493799983
1621, epoch_train_loss=0.0007309028493799983
Epoch 1622
Epoch 1622 :: Batch 0/1
Batch Loss = 0.0007309028493799982
1622, epoch_train_loss=0.0007309028493799982
Epoch 1623
Epoch 1623 :: Batch 0/1
Batch Loss = 0.0007309028493799982
1623, epoch_train_loss=0.0007309028493799982
Epoch 1624
Epoch 1624 :: Batch 0/1
Batch Loss = 0.0007309028493799979
1624, epoch_train_loss=0.0007309028493799979
Epoch 1625
Epoch 1625 :: Batch 0/1
Batch Loss = 0.0007309028493799979
1625, epoch_train_loss=0.0007309028493799979
Epoch 1626
Epoch 1626 :: Batch 0/1
Batch Loss = 0.0007309028493799979
1626, epoch_train_loss=0.0007309028493799979
Epoch 1627
Epoch 1627 :: Batch 0/1
Batch Loss = 0.0007309028493799979
1627, epoch_train_loss=0.0007309028493799979
Epoch 1628
Epoch 1628 :: Batch 0/1
Batch Loss = 0.0007309028493799978
1628, epoch_train_loss=0.0007309028493799978
Epoch 1629
Epoch 1629 :: Batch 0/1
Batch Loss = 0.0007309028493799977
1629, epoch_train_loss=0.0007309028493799977
Epoch 1630
Epoch 1630 :: Batch 0/1
Batch Loss = 0.0007309028493799977
1630, epoch_train_loss=0.0007309028493799977
Epoch 1631
Epoch 1631 :: Batch 0/1
Batch Loss = 0.0007309028493799977
1631, epoch_train_loss=0.0007309028493799977
Epoch 1632
Epoch 1632 :: Batch 0/1
Batch Loss = 0.0007309028493799977
1632, epoch_train_loss=0.0007309028493799977
Epoch 1633
Epoch 1633 :: Batch 0/1
Batch Loss = 0.0007309028493799977
1633, epoch_train_loss=0.0007309028493799977
Epoch 1634
Epoch 1634 :: Batch 0/1
Batch Loss = 0.0007309028493799977
1634, epoch_train_loss=0.0007309028493799977
Epoch 1635
Epoch 1635 :: Batch 0/1
Batch Loss = 0.0007309028493799976
1635, epoch_train_loss=0.0007309028493799976
Epoch 1636
Epoch 1636 :: Batch 0/1
Batch Loss = 0.0007309028493799976
1636, epoch_train_loss=0.0007309028493799976
Epoch 1637
Epoch 1637 :: Batch 0/1
Batch Loss = 0.0007309028493799975
1637, epoch_train_loss=0.0007309028493799975
Epoch 1638
Epoch 1638 :: Batch 0/1
Batch Loss = 0.0007309028493799975
1638, epoch_train_loss=0.0007309028493799975
Epoch 1639
Epoch 1639 :: Batch 0/1
Batch Loss = 0.0007309028493799975
1639, epoch_train_loss=0.0007309028493799975
Epoch 1640
Epoch 1640 :: Batch 0/1
Batch Loss = 0.0007309028493799975
1640, epoch_train_loss=0.0007309028493799975
Epoch 1641
Epoch 1641 :: Batch 0/1
Batch Loss = 0.0007309028493799974
1641, epoch_train_loss=0.0007309028493799974
Epoch 1642
Epoch 1642 :: Batch 0/1
Batch Loss = 0.0007309028493799974
1642, epoch_train_loss=0.0007309028493799974
Epoch 1643
Epoch 1643 :: Batch 0/1
Batch Loss = 0.0007309028493799974
1643, epoch_train_loss=0.0007309028493799974
Epoch 1644
Epoch 1644 :: Batch 0/1
Batch Loss = 0.0007309028493799973
1644, epoch_train_loss=0.0007309028493799973
Epoch 1645
Epoch 1645 :: Batch 0/1
Batch Loss = 0.0007309028493799973
1645, epoch_train_loss=0.0007309028493799973
Epoch 1646
Epoch 1646 :: Batch 0/1
Batch Loss = 0.0007309028493799972
1646, epoch_train_loss=0.0007309028493799972
Epoch 1647
Epoch 1647 :: Batch 0/1
Batch Loss = 0.0007309028493799972
1647, epoch_train_loss=0.0007309028493799972
Epoch 1648
Epoch 1648 :: Batch 0/1
Batch Loss = 0.0007309028493799972
1648, epoch_train_loss=0.0007309028493799972
Epoch 1649
Epoch 1649 :: Batch 0/1
Batch Loss = 0.0007309028493799972
1649, epoch_train_loss=0.0007309028493799972
Epoch 1650
Epoch 1650 :: Batch 0/1
Batch Loss = 0.0007309028493799971
1650, epoch_train_loss=0.0007309028493799971
Epoch 1651
Epoch 1651 :: Batch 0/1
Batch Loss = 0.0007309028493799971
1651, epoch_train_loss=0.0007309028493799971
Epoch 1652
Epoch 1652 :: Batch 0/1
Batch Loss = 0.0007309028493799971
1652, epoch_train_loss=0.0007309028493799971
Epoch 1653
Epoch 1653 :: Batch 0/1
Batch Loss = 0.0007309028493799971
1653, epoch_train_loss=0.0007309028493799971
Epoch 1654
Epoch 1654 :: Batch 0/1
Batch Loss = 0.0007309028493799971
1654, epoch_train_loss=0.0007309028493799971
Epoch 1655
Epoch 1655 :: Batch 0/1
Batch Loss = 0.0007309028493799969
1655, epoch_train_loss=0.0007309028493799969
Epoch 1656
Epoch 1656 :: Batch 0/1
Batch Loss = 0.0007309028493799967
1656, epoch_train_loss=0.0007309028493799967
Epoch 1657
Epoch 1657 :: Batch 0/1
Batch Loss = 0.0007309028493799967
1657, epoch_train_loss=0.0007309028493799967
Epoch 1658
Epoch 1658 :: Batch 0/1
Batch Loss = 0.0007309028493799967
1658, epoch_train_loss=0.0007309028493799967
Epoch 1659
Epoch 1659 :: Batch 0/1
Batch Loss = 0.0007309028493799967
1659, epoch_train_loss=0.0007309028493799967
Epoch 1660
Epoch 1660 :: Batch 0/1
Batch Loss = 0.0007309028493799966
1660, epoch_train_loss=0.0007309028493799966
Epoch 1661
Epoch 1661 :: Batch 0/1
Batch Loss = 0.0007309028493799966
1661, epoch_train_loss=0.0007309028493799966
Epoch 1662
Epoch 1662 :: Batch 0/1
Batch Loss = 0.0007309028493799965
1662, epoch_train_loss=0.0007309028493799965
Epoch 1663
Epoch 1663 :: Batch 0/1
Batch Loss = 0.0007309028493799965
1663, epoch_train_loss=0.0007309028493799965
Epoch 1664
Epoch 1664 :: Batch 0/1
Batch Loss = 0.0007309028493799965
1664, epoch_train_loss=0.0007309028493799965
Epoch 1665
Epoch 1665 :: Batch 0/1
Batch Loss = 0.0007309028493799965
1665, epoch_train_loss=0.0007309028493799965
Epoch 1666
Epoch 1666 :: Batch 0/1
Batch Loss = 0.0007309028493799964
1666, epoch_train_loss=0.0007309028493799964
Epoch 1667
Epoch 1667 :: Batch 0/1
Batch Loss = 0.0007309028493799964
1667, epoch_train_loss=0.0007309028493799964
Epoch 1668
Epoch 1668 :: Batch 0/1
Batch Loss = 0.0007309028493799964
1668, epoch_train_loss=0.0007309028493799964
Epoch 1669
Epoch 1669 :: Batch 0/1
Batch Loss = 0.0007309028493799964
1669, epoch_train_loss=0.0007309028493799964
Epoch 1670
Epoch 1670 :: Batch 0/1
Batch Loss = 0.0007309028493799963
1670, epoch_train_loss=0.0007309028493799963
Epoch 1671
Epoch 1671 :: Batch 0/1
Batch Loss = 0.0007309028493799963
1671, epoch_train_loss=0.0007309028493799963
Epoch 1672
Epoch 1672 :: Batch 0/1
Batch Loss = 0.0007309028493799962
1672, epoch_train_loss=0.0007309028493799962
Epoch 1673
Epoch 1673 :: Batch 0/1
Batch Loss = 0.0007309028493799962
1673, epoch_train_loss=0.0007309028493799962
Epoch 1674
Epoch 1674 :: Batch 0/1
Batch Loss = 0.0007309028493799962
1674, epoch_train_loss=0.0007309028493799962
Epoch 1675
Epoch 1675 :: Batch 0/1
Batch Loss = 0.0007309028493799961
1675, epoch_train_loss=0.0007309028493799961
Epoch 1676
Epoch 1676 :: Batch 0/1
Batch Loss = 0.0007309028493799961
1676, epoch_train_loss=0.0007309028493799961
Epoch 1677
Epoch 1677 :: Batch 0/1
Batch Loss = 0.0007309028493799961
1677, epoch_train_loss=0.0007309028493799961
Epoch 1678
Epoch 1678 :: Batch 0/1
Batch Loss = 0.0007309028493799961
1678, epoch_train_loss=0.0007309028493799961
Epoch 1679
Epoch 1679 :: Batch 0/1
Batch Loss = 0.000730902849379996
1679, epoch_train_loss=0.000730902849379996
Epoch 1680
Epoch 1680 :: Batch 0/1
Batch Loss = 0.000730902849379996
1680, epoch_train_loss=0.000730902849379996
Epoch 1681
Epoch 1681 :: Batch 0/1
Batch Loss = 0.000730902849379996
1681, epoch_train_loss=0.000730902849379996
Epoch 1682
Epoch 1682 :: Batch 0/1
Batch Loss = 0.0007309028493799959
1682, epoch_train_loss=0.0007309028493799959
Epoch 1683
Epoch 1683 :: Batch 0/1
Batch Loss = 0.0007309028493799959
1683, epoch_train_loss=0.0007309028493799959
Epoch 1684
Epoch 1684 :: Batch 0/1
Batch Loss = 0.0007309028493799959
1684, epoch_train_loss=0.0007309028493799959
Epoch 1685
Epoch 1685 :: Batch 0/1
Batch Loss = 0.0007309028493799959
1685, epoch_train_loss=0.0007309028493799959
Epoch 1686
Epoch 1686 :: Batch 0/1
Batch Loss = 0.0007309028493799957
1686, epoch_train_loss=0.0007309028493799957
Epoch 1687
Epoch 1687 :: Batch 0/1
Batch Loss = 0.0007309028493799956
1687, epoch_train_loss=0.0007309028493799956
Epoch 1688
Epoch 1688 :: Batch 0/1
Batch Loss = 0.0007309028493799956
1688, epoch_train_loss=0.0007309028493799956
Epoch 1689
Epoch 1689 :: Batch 0/1
Batch Loss = 0.0007309028493799956
1689, epoch_train_loss=0.0007309028493799956
Epoch 1690
Epoch 1690 :: Batch 0/1
Batch Loss = 0.0007309028493799956
1690, epoch_train_loss=0.0007309028493799956
Epoch 1691
Epoch 1691 :: Batch 0/1
Batch Loss = 0.0007309028493799954
1691, epoch_train_loss=0.0007309028493799954
Epoch 1692
Epoch 1692 :: Batch 0/1
Batch Loss = 0.0007309028493799954
1692, epoch_train_loss=0.0007309028493799954
Epoch 1693
Epoch 1693 :: Batch 0/1
Batch Loss = 0.0007309028493799953
1693, epoch_train_loss=0.0007309028493799953
Epoch 1694
Epoch 1694 :: Batch 0/1
Batch Loss = 0.0007309028493799953
1694, epoch_train_loss=0.0007309028493799953
Epoch 1695
Epoch 1695 :: Batch 0/1
Batch Loss = 0.0007309028493799953
1695, epoch_train_loss=0.0007309028493799953
Epoch 1696
Epoch 1696 :: Batch 0/1
Batch Loss = 0.0007309028493799953
1696, epoch_train_loss=0.0007309028493799953
Epoch 1697
Epoch 1697 :: Batch 0/1
Batch Loss = 0.0007309028493799952
1697, epoch_train_loss=0.0007309028493799952
Epoch 1698
Epoch 1698 :: Batch 0/1
Batch Loss = 0.0007309028493799952
1698, epoch_train_loss=0.0007309028493799952
Epoch 1699
Epoch 1699 :: Batch 0/1
Batch Loss = 0.0007309028493799951
1699, epoch_train_loss=0.0007309028493799951
Epoch 1700
Epoch 1700 :: Batch 0/1
Batch Loss = 0.0007309028493799951
1700, epoch_train_loss=0.0007309028493799951
Epoch 1701
Epoch 1701 :: Batch 0/1
Batch Loss = 0.0007309028493799951
1701, epoch_train_loss=0.0007309028493799951
Epoch 1702
Epoch 1702 :: Batch 0/1
Batch Loss = 0.0007309028493799951
1702, epoch_train_loss=0.0007309028493799951
Epoch 1703
Epoch 1703 :: Batch 0/1
Batch Loss = 0.0007309028493799951
1703, epoch_train_loss=0.0007309028493799951
Epoch 1704
Epoch 1704 :: Batch 0/1
Batch Loss = 0.0007309028493799951
1704, epoch_train_loss=0.0007309028493799951
Epoch 1705
Epoch 1705 :: Batch 0/1
Batch Loss = 0.000730902849379995
1705, epoch_train_loss=0.000730902849379995
Epoch 1706
Epoch 1706 :: Batch 0/1
Batch Loss = 0.0007309028493799949
1706, epoch_train_loss=0.0007309028493799949
Epoch 1707
Epoch 1707 :: Batch 0/1
Batch Loss = 0.0007309028493799949
1707, epoch_train_loss=0.0007309028493799949
Epoch 1708
Epoch 1708 :: Batch 0/1
Batch Loss = 0.0007309028493799949
1708, epoch_train_loss=0.0007309028493799949
Epoch 1709
Epoch 1709 :: Batch 0/1
Batch Loss = 0.0007309028493799949
1709, epoch_train_loss=0.0007309028493799949
Epoch 1710
Epoch 1710 :: Batch 0/1
Batch Loss = 0.0007309028493799949
1710, epoch_train_loss=0.0007309028493799949
Epoch 1711
Epoch 1711 :: Batch 0/1
Batch Loss = 0.0007309028493799948
1711, epoch_train_loss=0.0007309028493799948
Epoch 1712
Epoch 1712 :: Batch 0/1
Batch Loss = 0.0007309028493799946
1712, epoch_train_loss=0.0007309028493799946
Epoch 1713
Epoch 1713 :: Batch 0/1
Batch Loss = 0.0007309028493799946
1713, epoch_train_loss=0.0007309028493799946
Epoch 1714
Epoch 1714 :: Batch 0/1
Batch Loss = 0.0007309028493799946
1714, epoch_train_loss=0.0007309028493799946
Epoch 1715
Epoch 1715 :: Batch 0/1
Batch Loss = 0.0007309028493799946
1715, epoch_train_loss=0.0007309028493799946
Epoch 1716
Epoch 1716 :: Batch 0/1
Batch Loss = 0.0007309028493799946
1716, epoch_train_loss=0.0007309028493799946
Epoch 1717
Epoch 1717 :: Batch 0/1
Batch Loss = 0.0007309028493799945
1717, epoch_train_loss=0.0007309028493799945
Epoch 1718
Epoch 1718 :: Batch 0/1
Batch Loss = 0.0007309028493799944
1718, epoch_train_loss=0.0007309028493799944
Epoch 1719
Epoch 1719 :: Batch 0/1
Batch Loss = 0.0007309028493799944
1719, epoch_train_loss=0.0007309028493799944
Epoch 1720
Epoch 1720 :: Batch 0/1
Batch Loss = 0.0007309028493799944
1720, epoch_train_loss=0.0007309028493799944
Epoch 1721
Epoch 1721 :: Batch 0/1
Batch Loss = 0.0007309028493799944
1721, epoch_train_loss=0.0007309028493799944
Epoch 1722
Epoch 1722 :: Batch 0/1
Batch Loss = 0.0007309028493799941
1722, epoch_train_loss=0.0007309028493799941
Epoch 1723
Epoch 1723 :: Batch 0/1
Batch Loss = 0.0007309028493799941
1723, epoch_train_loss=0.0007309028493799941
Epoch 1724
Epoch 1724 :: Batch 0/1
Batch Loss = 0.0007309028493799941
1724, epoch_train_loss=0.0007309028493799941
Epoch 1725
Epoch 1725 :: Batch 0/1
Batch Loss = 0.0007309028493799941
1725, epoch_train_loss=0.0007309028493799941
Epoch 1726
Epoch 1726 :: Batch 0/1
Batch Loss = 0.0007309028493799941
1726, epoch_train_loss=0.0007309028493799941
Epoch 1727
Epoch 1727 :: Batch 0/1
Batch Loss = 0.0007309028493799941
1727, epoch_train_loss=0.0007309028493799941
Epoch 1728
Epoch 1728 :: Batch 0/1
Batch Loss = 0.000730902849379994
1728, epoch_train_loss=0.000730902849379994
Epoch 1729
Epoch 1729 :: Batch 0/1
Batch Loss = 0.000730902849379994
1729, epoch_train_loss=0.000730902849379994
Epoch 1730
Epoch 1730 :: Batch 0/1
Batch Loss = 0.0007309028493799939
1730, epoch_train_loss=0.0007309028493799939
Epoch 1731
Epoch 1731 :: Batch 0/1
Batch Loss = 0.0007309028493799939
1731, epoch_train_loss=0.0007309028493799939
Epoch 1732
Epoch 1732 :: Batch 0/1
Batch Loss = 0.0007309028493799939
1732, epoch_train_loss=0.0007309028493799939
Epoch 1733
Epoch 1733 :: Batch 0/1
Batch Loss = 0.0007309028493799939
1733, epoch_train_loss=0.0007309028493799939
Epoch 1734
Epoch 1734 :: Batch 0/1
Batch Loss = 0.0007309028493799938
1734, epoch_train_loss=0.0007309028493799938
Epoch 1735
Epoch 1735 :: Batch 0/1
Batch Loss = 0.0007309028493799938
1735, epoch_train_loss=0.0007309028493799938
Epoch 1736
Epoch 1736 :: Batch 0/1
Batch Loss = 0.0007309028493799938
1736, epoch_train_loss=0.0007309028493799938
Epoch 1737
Epoch 1737 :: Batch 0/1
Batch Loss = 0.0007309028493799937
1737, epoch_train_loss=0.0007309028493799937
Epoch 1738
Epoch 1738 :: Batch 0/1
Batch Loss = 0.0007309028493799937
1738, epoch_train_loss=0.0007309028493799937
Epoch 1739
Epoch 1739 :: Batch 0/1
Batch Loss = 0.0007309028493799937
1739, epoch_train_loss=0.0007309028493799937
Epoch 1740
Epoch 1740 :: Batch 0/1
Batch Loss = 0.0007309028493799937
1740, epoch_train_loss=0.0007309028493799937
Epoch 1741
Epoch 1741 :: Batch 0/1
Batch Loss = 0.0007309028493799934
1741, epoch_train_loss=0.0007309028493799934
Epoch 1742
Epoch 1742 :: Batch 0/1
Batch Loss = 0.0007309028493799934
1742, epoch_train_loss=0.0007309028493799934
Epoch 1743
Epoch 1743 :: Batch 0/1
Batch Loss = 0.0007309028493799934
1743, epoch_train_loss=0.0007309028493799934
Epoch 1744
Epoch 1744 :: Batch 0/1
Batch Loss = 0.0007309028493799934
1744, epoch_train_loss=0.0007309028493799934
Epoch 1745
Epoch 1745 :: Batch 0/1
Batch Loss = 0.0007309028493799934
1745, epoch_train_loss=0.0007309028493799934
Epoch 1746
Epoch 1746 :: Batch 0/1
Batch Loss = 0.0007309028493799934
1746, epoch_train_loss=0.0007309028493799934
Epoch 1747
Epoch 1747 :: Batch 0/1
Batch Loss = 0.0007309028493799932
1747, epoch_train_loss=0.0007309028493799932
Epoch 1748
Epoch 1748 :: Batch 0/1
Batch Loss = 0.0007309028493799932
1748, epoch_train_loss=0.0007309028493799932
Epoch 1749
Epoch 1749 :: Batch 0/1
Batch Loss = 0.0007309028493799932
1749, epoch_train_loss=0.0007309028493799932
Epoch 1750
Epoch 1750 :: Batch 0/1
Batch Loss = 0.0007309028493799932
1750, epoch_train_loss=0.0007309028493799932
Epoch 1751
Epoch 1751 :: Batch 0/1
Batch Loss = 0.0007309028493799932
1751, epoch_train_loss=0.0007309028493799932
Epoch 1752
Epoch 1752 :: Batch 0/1
Batch Loss = 0.000730902849379993
1752, epoch_train_loss=0.000730902849379993
Epoch 1753
Epoch 1753 :: Batch 0/1
Batch Loss = 0.000730902849379993
1753, epoch_train_loss=0.000730902849379993
Epoch 1754
Epoch 1754 :: Batch 0/1
Batch Loss = 0.000730902849379993
1754, epoch_train_loss=0.000730902849379993
Epoch 1755
Epoch 1755 :: Batch 0/1
Batch Loss = 0.000730902849379993
1755, epoch_train_loss=0.000730902849379993
Epoch 1756
Epoch 1756 :: Batch 0/1
Batch Loss = 0.000730902849379993
1756, epoch_train_loss=0.000730902849379993
Epoch 1757
Epoch 1757 :: Batch 0/1
Batch Loss = 0.000730902849379993
1757, epoch_train_loss=0.000730902849379993
Epoch 1758
Epoch 1758 :: Batch 0/1
Batch Loss = 0.0007309028493799927
1758, epoch_train_loss=0.0007309028493799927
Epoch 1759
Epoch 1759 :: Batch 0/1
Batch Loss = 0.0007309028493799927
1759, epoch_train_loss=0.0007309028493799927
Epoch 1760
Epoch 1760 :: Batch 0/1
Batch Loss = 0.0007309028493799927
1760, epoch_train_loss=0.0007309028493799927
Epoch 1761
Epoch 1761 :: Batch 0/1
Batch Loss = 0.0007309028493799927
1761, epoch_train_loss=0.0007309028493799927
Epoch 1762
Epoch 1762 :: Batch 0/1
Batch Loss = 0.0007309028493799927
1762, epoch_train_loss=0.0007309028493799927
Epoch 1763
Epoch 1763 :: Batch 0/1
Batch Loss = 0.0007309028493799927
1763, epoch_train_loss=0.0007309028493799927
Epoch 1764
Epoch 1764 :: Batch 0/1
Batch Loss = 0.0007309028493799927
1764, epoch_train_loss=0.0007309028493799927
Epoch 1765
Epoch 1765 :: Batch 0/1
Batch Loss = 0.0007309028493799927
1765, epoch_train_loss=0.0007309028493799927
Epoch 1766
Epoch 1766 :: Batch 0/1
Batch Loss = 0.0007309028493799925
1766, epoch_train_loss=0.0007309028493799925
Epoch 1767
Epoch 1767 :: Batch 0/1
Batch Loss = 0.0007309028493799925
1767, epoch_train_loss=0.0007309028493799925
Epoch 1768
Epoch 1768 :: Batch 0/1
Batch Loss = 0.0007309028493799925
1768, epoch_train_loss=0.0007309028493799925
Epoch 1769
Epoch 1769 :: Batch 0/1
Batch Loss = 0.0007309028493799925
1769, epoch_train_loss=0.0007309028493799925
Epoch 1770
Epoch 1770 :: Batch 0/1
Batch Loss = 0.0007309028493799925
1770, epoch_train_loss=0.0007309028493799925
Epoch 1771
Epoch 1771 :: Batch 0/1
Batch Loss = 0.0007309028493799923
1771, epoch_train_loss=0.0007309028493799923
Epoch 1772
Epoch 1772 :: Batch 0/1
Batch Loss = 0.0007309028493799922
1772, epoch_train_loss=0.0007309028493799922
Epoch 1773
Epoch 1773 :: Batch 0/1
Batch Loss = 0.0007309028493799922
1773, epoch_train_loss=0.0007309028493799922
Epoch 1774
Epoch 1774 :: Batch 0/1
Batch Loss = 0.0007309028493799922
1774, epoch_train_loss=0.0007309028493799922
Epoch 1775
Epoch 1775 :: Batch 0/1
Batch Loss = 0.0007309028493799922
1775, epoch_train_loss=0.0007309028493799922
Epoch 1776
Epoch 1776 :: Batch 0/1
Batch Loss = 0.0007309028493799922
1776, epoch_train_loss=0.0007309028493799922
Epoch 1777
Epoch 1777 :: Batch 0/1
Batch Loss = 0.0007309028493799921
1777, epoch_train_loss=0.0007309028493799921
Epoch 1778
Epoch 1778 :: Batch 0/1
Batch Loss = 0.000730902849379992
1778, epoch_train_loss=0.000730902849379992
Epoch 1779
Epoch 1779 :: Batch 0/1
Batch Loss = 0.000730902849379992
1779, epoch_train_loss=0.000730902849379992
Epoch 1780
Epoch 1780 :: Batch 0/1
Batch Loss = 0.000730902849379992
1780, epoch_train_loss=0.000730902849379992
Epoch 1781
Epoch 1781 :: Batch 0/1
Batch Loss = 0.0007309028493799919
1781, epoch_train_loss=0.0007309028493799919
Epoch 1782
Epoch 1782 :: Batch 0/1
Batch Loss = 0.0007309028493799919
1782, epoch_train_loss=0.0007309028493799919
Epoch 1783
Epoch 1783 :: Batch 0/1
Batch Loss = 0.0007309028493799919
1783, epoch_train_loss=0.0007309028493799919
Epoch 1784
Epoch 1784 :: Batch 0/1
Batch Loss = 0.0007309028493799918
1784, epoch_train_loss=0.0007309028493799918
Epoch 1785
Epoch 1785 :: Batch 0/1
Batch Loss = 0.0007309028493799918
1785, epoch_train_loss=0.0007309028493799918
Epoch 1786
Epoch 1786 :: Batch 0/1
Batch Loss = 0.0007309028493799917
1786, epoch_train_loss=0.0007309028493799917
Epoch 1787
Epoch 1787 :: Batch 0/1
Batch Loss = 0.0007309028493799917
1787, epoch_train_loss=0.0007309028493799917
Epoch 1788
Epoch 1788 :: Batch 0/1
Batch Loss = 0.0007309028493799917
1788, epoch_train_loss=0.0007309028493799917
Epoch 1789
Epoch 1789 :: Batch 0/1
Batch Loss = 0.0007309028493799917
1789, epoch_train_loss=0.0007309028493799917
Epoch 1790
Epoch 1790 :: Batch 0/1
Batch Loss = 0.0007309028493799915
1790, epoch_train_loss=0.0007309028493799915
Epoch 1791
Epoch 1791 :: Batch 0/1
Batch Loss = 0.0007309028493799915
1791, epoch_train_loss=0.0007309028493799915
Epoch 1792
Epoch 1792 :: Batch 0/1
Batch Loss = 0.0007309028493799915
1792, epoch_train_loss=0.0007309028493799915
Epoch 1793
Epoch 1793 :: Batch 0/1
Batch Loss = 0.0007309028493799914
1793, epoch_train_loss=0.0007309028493799914
Epoch 1794
Epoch 1794 :: Batch 0/1
Batch Loss = 0.0007309028493799914
1794, epoch_train_loss=0.0007309028493799914
Epoch 1795
Epoch 1795 :: Batch 0/1
Batch Loss = 0.0007309028493799914
1795, epoch_train_loss=0.0007309028493799914
Epoch 1796
Epoch 1796 :: Batch 0/1
Batch Loss = 0.0007309028493799913
1796, epoch_train_loss=0.0007309028493799913
Epoch 1797
Epoch 1797 :: Batch 0/1
Batch Loss = 0.0007309028493799913
1797, epoch_train_loss=0.0007309028493799913
Epoch 1798
Epoch 1798 :: Batch 0/1
Batch Loss = 0.0007309028493799913
1798, epoch_train_loss=0.0007309028493799913
Epoch 1799
Epoch 1799 :: Batch 0/1
Batch Loss = 0.0007309028493799911
1799, epoch_train_loss=0.0007309028493799911
Epoch 1800
Epoch 1800 :: Batch 0/1
Batch Loss = 0.0007309028493799911
1800, epoch_train_loss=0.0007309028493799911
Epoch 1801
Epoch 1801 :: Batch 0/1
Batch Loss = 0.000730902849379991
1801, epoch_train_loss=0.000730902849379991
Epoch 1802
Epoch 1802 :: Batch 0/1
Batch Loss = 0.000730902849379991
1802, epoch_train_loss=0.000730902849379991
Epoch 1803
Epoch 1803 :: Batch 0/1
Batch Loss = 0.000730902849379991
1803, epoch_train_loss=0.000730902849379991
Epoch 1804
Epoch 1804 :: Batch 0/1
Batch Loss = 0.0007309028493799909
1804, epoch_train_loss=0.0007309028493799909
Epoch 1805
Epoch 1805 :: Batch 0/1
Batch Loss = 0.0007309028493799909
1805, epoch_train_loss=0.0007309028493799909
Epoch 1806
Epoch 1806 :: Batch 0/1
Batch Loss = 0.0007309028493799909
1806, epoch_train_loss=0.0007309028493799909
Epoch 1807
Epoch 1807 :: Batch 0/1
Batch Loss = 0.0007309028493799908
1807, epoch_train_loss=0.0007309028493799908
Epoch 1808
Epoch 1808 :: Batch 0/1
Batch Loss = 0.0007309028493799908
1808, epoch_train_loss=0.0007309028493799908
Epoch 1809
Epoch 1809 :: Batch 0/1
Batch Loss = 0.0007309028493799908
1809, epoch_train_loss=0.0007309028493799908
Epoch 1810
Epoch 1810 :: Batch 0/1
Batch Loss = 0.0007309028493799907
1810, epoch_train_loss=0.0007309028493799907
Epoch 1811
Epoch 1811 :: Batch 0/1
Batch Loss = 0.0007309028493799907
1811, epoch_train_loss=0.0007309028493799907
Epoch 1812
Epoch 1812 :: Batch 0/1
Batch Loss = 0.0007309028493799906
1812, epoch_train_loss=0.0007309028493799906
Epoch 1813
Epoch 1813 :: Batch 0/1
Batch Loss = 0.0007309028493799906
1813, epoch_train_loss=0.0007309028493799906
Epoch 1814
Epoch 1814 :: Batch 0/1
Batch Loss = 0.0007309028493799906
1814, epoch_train_loss=0.0007309028493799906
Epoch 1815
Epoch 1815 :: Batch 0/1
Batch Loss = 0.0007309028493799906
1815, epoch_train_loss=0.0007309028493799906
Epoch 1816
Epoch 1816 :: Batch 0/1
Batch Loss = 0.0007309028493799905
1816, epoch_train_loss=0.0007309028493799905
Epoch 1817
Epoch 1817 :: Batch 0/1
Batch Loss = 0.0007309028493799905
1817, epoch_train_loss=0.0007309028493799905
Epoch 1818
Epoch 1818 :: Batch 0/1
Batch Loss = 0.0007309028493799903
1818, epoch_train_loss=0.0007309028493799903
Epoch 1819
Epoch 1819 :: Batch 0/1
Batch Loss = 0.0007309028493799903
1819, epoch_train_loss=0.0007309028493799903
Epoch 1820
Epoch 1820 :: Batch 0/1
Batch Loss = 0.0007309028493799903
1820, epoch_train_loss=0.0007309028493799903
Epoch 1821
Epoch 1821 :: Batch 0/1
Batch Loss = 0.0007309028493799903
1821, epoch_train_loss=0.0007309028493799903
Epoch 1822
Epoch 1822 :: Batch 0/1
Batch Loss = 0.0007309028493799902
1822, epoch_train_loss=0.0007309028493799902
Epoch 1823
Epoch 1823 :: Batch 0/1
Batch Loss = 0.0007309028493799902
1823, epoch_train_loss=0.0007309028493799902
Epoch 1824
Epoch 1824 :: Batch 0/1
Batch Loss = 0.0007309028493799902
1824, epoch_train_loss=0.0007309028493799902
Epoch 1825
Epoch 1825 :: Batch 0/1
Batch Loss = 0.00073090284937999
1825, epoch_train_loss=0.00073090284937999
Epoch 1826
Epoch 1826 :: Batch 0/1
Batch Loss = 0.00073090284937999
1826, epoch_train_loss=0.00073090284937999
Epoch 1827
Epoch 1827 :: Batch 0/1
Batch Loss = 0.00073090284937999
1827, epoch_train_loss=0.00073090284937999
Epoch 1828
Epoch 1828 :: Batch 0/1
Batch Loss = 0.0007309028493799899
1828, epoch_train_loss=0.0007309028493799899
Epoch 1829
Epoch 1829 :: Batch 0/1
Batch Loss = 0.0007309028493799899
1829, epoch_train_loss=0.0007309028493799899
Epoch 1830
Epoch 1830 :: Batch 0/1
Batch Loss = 0.0007309028493799899
1830, epoch_train_loss=0.0007309028493799899
Epoch 1831
Epoch 1831 :: Batch 0/1
Batch Loss = 0.0007309028493799898
1831, epoch_train_loss=0.0007309028493799898
Epoch 1832
Epoch 1832 :: Batch 0/1
Batch Loss = 0.0007309028493799898
1832, epoch_train_loss=0.0007309028493799898
Epoch 1833
Epoch 1833 :: Batch 0/1
Batch Loss = 0.0007309028493799898
1833, epoch_train_loss=0.0007309028493799898
Epoch 1834
Epoch 1834 :: Batch 0/1
Batch Loss = 0.0007309028493799897
1834, epoch_train_loss=0.0007309028493799897
Epoch 1835
Epoch 1835 :: Batch 0/1
Batch Loss = 0.0007309028493799896
1835, epoch_train_loss=0.0007309028493799896
Epoch 1836
Epoch 1836 :: Batch 0/1
Batch Loss = 0.0007309028493799896
1836, epoch_train_loss=0.0007309028493799896
Epoch 1837
Epoch 1837 :: Batch 0/1
Batch Loss = 0.0007309028493799896
1837, epoch_train_loss=0.0007309028493799896
Epoch 1838
Epoch 1838 :: Batch 0/1
Batch Loss = 0.0007309028493799896
1838, epoch_train_loss=0.0007309028493799896
Epoch 1839
Epoch 1839 :: Batch 0/1
Batch Loss = 0.0007309028493799895
1839, epoch_train_loss=0.0007309028493799895
Epoch 1840
Epoch 1840 :: Batch 0/1
Batch Loss = 0.0007309028493799894
1840, epoch_train_loss=0.0007309028493799894
Epoch 1841
Epoch 1841 :: Batch 0/1
Batch Loss = 0.0007309028493799894
1841, epoch_train_loss=0.0007309028493799894
Epoch 1842
Epoch 1842 :: Batch 0/1
Batch Loss = 0.0007309028493799894
1842, epoch_train_loss=0.0007309028493799894
Epoch 1843
Epoch 1843 :: Batch 0/1
Batch Loss = 0.0007309028493799894
1843, epoch_train_loss=0.0007309028493799894
Epoch 1844
Epoch 1844 :: Batch 0/1
Batch Loss = 0.0007309028493799894
1844, epoch_train_loss=0.0007309028493799894
Epoch 1845
Epoch 1845 :: Batch 0/1
Batch Loss = 0.0007309028493799893
1845, epoch_train_loss=0.0007309028493799893
Epoch 1846
Epoch 1846 :: Batch 0/1
Batch Loss = 0.0007309028493799893
1846, epoch_train_loss=0.0007309028493799893
Epoch 1847
Epoch 1847 :: Batch 0/1
Batch Loss = 0.0007309028493799892
1847, epoch_train_loss=0.0007309028493799892
Epoch 1848
Epoch 1848 :: Batch 0/1
Batch Loss = 0.0007309028493799892
1848, epoch_train_loss=0.0007309028493799892
Epoch 1849
Epoch 1849 :: Batch 0/1
Batch Loss = 0.0007309028493799892
1849, epoch_train_loss=0.0007309028493799892
Epoch 1850
Epoch 1850 :: Batch 0/1
Batch Loss = 0.0007309028493799892
1850, epoch_train_loss=0.0007309028493799892
Epoch 1851
Epoch 1851 :: Batch 0/1
Batch Loss = 0.000730902849379989
1851, epoch_train_loss=0.000730902849379989
Epoch 1852
Epoch 1852 :: Batch 0/1
Batch Loss = 0.0007309028493799888
1852, epoch_train_loss=0.0007309028493799888
Epoch 1853
Epoch 1853 :: Batch 0/1
Batch Loss = 0.0007309028493799888
1853, epoch_train_loss=0.0007309028493799888
Epoch 1854
Epoch 1854 :: Batch 0/1
Batch Loss = 0.0007309028493799888
1854, epoch_train_loss=0.0007309028493799888
Epoch 1855
Epoch 1855 :: Batch 0/1
Batch Loss = 0.0007309028493799888
1855, epoch_train_loss=0.0007309028493799888
Epoch 1856
Epoch 1856 :: Batch 0/1
Batch Loss = 0.0007309028493799888
1856, epoch_train_loss=0.0007309028493799888
Epoch 1857
Epoch 1857 :: Batch 0/1
Batch Loss = 0.0007309028493799887
1857, epoch_train_loss=0.0007309028493799887
Epoch 1858
Epoch 1858 :: Batch 0/1
Batch Loss = 0.0007309028493799886
1858, epoch_train_loss=0.0007309028493799886
Epoch 1859
Epoch 1859 :: Batch 0/1
Batch Loss = 0.0007309028493799886
1859, epoch_train_loss=0.0007309028493799886
Epoch 1860
Epoch 1860 :: Batch 0/1
Batch Loss = 0.0007309028493799886
1860, epoch_train_loss=0.0007309028493799886
Epoch 1861
Epoch 1861 :: Batch 0/1
Batch Loss = 0.0007309028493799886
1861, epoch_train_loss=0.0007309028493799886
Epoch 1862
Epoch 1862 :: Batch 0/1
Batch Loss = 0.0007309028493799886
1862, epoch_train_loss=0.0007309028493799886
Epoch 1863
Epoch 1863 :: Batch 0/1
Batch Loss = 0.0007309028493799884
1863, epoch_train_loss=0.0007309028493799884
Epoch 1864
Epoch 1864 :: Batch 0/1
Batch Loss = 0.0007309028493799884
1864, epoch_train_loss=0.0007309028493799884
Epoch 1865
Epoch 1865 :: Batch 0/1
Batch Loss = 0.0007309028493799884
1865, epoch_train_loss=0.0007309028493799884
Epoch 1866
Epoch 1866 :: Batch 0/1
Batch Loss = 0.0007309028493799884
1866, epoch_train_loss=0.0007309028493799884
Epoch 1867
Epoch 1867 :: Batch 0/1
Batch Loss = 0.0007309028493799884
1867, epoch_train_loss=0.0007309028493799884
Epoch 1868
Epoch 1868 :: Batch 0/1
Batch Loss = 0.0007309028493799883
1868, epoch_train_loss=0.0007309028493799883
Epoch 1869
Epoch 1869 :: Batch 0/1
Batch Loss = 0.0007309028493799882
1869, epoch_train_loss=0.0007309028493799882
Epoch 1870
Epoch 1870 :: Batch 0/1
Batch Loss = 0.0007309028493799882
1870, epoch_train_loss=0.0007309028493799882
Epoch 1871
Epoch 1871 :: Batch 0/1
Batch Loss = 0.0007309028493799882
1871, epoch_train_loss=0.0007309028493799882
Epoch 1872
Epoch 1872 :: Batch 0/1
Batch Loss = 0.0007309028493799882
1872, epoch_train_loss=0.0007309028493799882
Epoch 1873
Epoch 1873 :: Batch 0/1
Batch Loss = 0.0007309028493799882
1873, epoch_train_loss=0.0007309028493799882
Epoch 1874
Epoch 1874 :: Batch 0/1
Batch Loss = 0.000730902849379988
1874, epoch_train_loss=0.000730902849379988
Epoch 1875
Epoch 1875 :: Batch 0/1
Batch Loss = 0.000730902849379988
1875, epoch_train_loss=0.000730902849379988
Epoch 1876
Epoch 1876 :: Batch 0/1
Batch Loss = 0.000730902849379988
1876, epoch_train_loss=0.000730902849379988
Epoch 1877
Epoch 1877 :: Batch 0/1
Batch Loss = 0.000730902849379988
1877, epoch_train_loss=0.000730902849379988
Epoch 1878
Epoch 1878 :: Batch 0/1
Batch Loss = 0.000730902849379988
1878, epoch_train_loss=0.000730902849379988
Epoch 1879
Epoch 1879 :: Batch 0/1
Batch Loss = 0.000730902849379988
1879, epoch_train_loss=0.000730902849379988
Epoch 1880
Epoch 1880 :: Batch 0/1
Batch Loss = 0.0007309028493799876
1880, epoch_train_loss=0.0007309028493799876
Epoch 1881
Epoch 1881 :: Batch 0/1
Batch Loss = 0.0007309028493799876
1881, epoch_train_loss=0.0007309028493799876
Epoch 1882
Epoch 1882 :: Batch 0/1
Batch Loss = 0.0007309028493799876
1882, epoch_train_loss=0.0007309028493799876
Epoch 1883
Epoch 1883 :: Batch 0/1
Batch Loss = 0.0007309028493799876
1883, epoch_train_loss=0.0007309028493799876
Epoch 1884
Epoch 1884 :: Batch 0/1
Batch Loss = 0.0007309028493799876
1884, epoch_train_loss=0.0007309028493799876
Epoch 1885
Epoch 1885 :: Batch 0/1
Batch Loss = 0.0007309028493799876
1885, epoch_train_loss=0.0007309028493799876
Epoch 1886
Epoch 1886 :: Batch 0/1
Batch Loss = 0.0007309028493799874
1886, epoch_train_loss=0.0007309028493799874
Epoch 1887
Epoch 1887 :: Batch 0/1
Batch Loss = 0.0007309028493799874
1887, epoch_train_loss=0.0007309028493799874
Epoch 1888
Epoch 1888 :: Batch 0/1
Batch Loss = 0.0007309028493799874
1888, epoch_train_loss=0.0007309028493799874
Epoch 1889
Epoch 1889 :: Batch 0/1
Batch Loss = 0.0007309028493799874
1889, epoch_train_loss=0.0007309028493799874
Epoch 1890
Epoch 1890 :: Batch 0/1
Batch Loss = 0.0007309028493799873
1890, epoch_train_loss=0.0007309028493799873
Epoch 1891
Epoch 1891 :: Batch 0/1
Batch Loss = 0.0007309028493799872
1891, epoch_train_loss=0.0007309028493799872
Epoch 1892
Epoch 1892 :: Batch 0/1
Batch Loss = 0.0007309028493799872
1892, epoch_train_loss=0.0007309028493799872
Epoch 1893
Epoch 1893 :: Batch 0/1
Batch Loss = 0.0007309028493799872
1893, epoch_train_loss=0.0007309028493799872
Epoch 1894
Epoch 1894 :: Batch 0/1
Batch Loss = 0.0007309028493799872
1894, epoch_train_loss=0.0007309028493799872
Epoch 1895
Epoch 1895 :: Batch 0/1
Batch Loss = 0.0007309028493799871
1895, epoch_train_loss=0.0007309028493799871
Epoch 1896
Epoch 1896 :: Batch 0/1
Batch Loss = 0.0007309028493799871
1896, epoch_train_loss=0.0007309028493799871
Epoch 1897
Epoch 1897 :: Batch 0/1
Batch Loss = 0.0007309028493799871
1897, epoch_train_loss=0.0007309028493799871
Epoch 1898
Epoch 1898 :: Batch 0/1
Batch Loss = 0.000730902849379987
1898, epoch_train_loss=0.000730902849379987
Epoch 1899
Epoch 1899 :: Batch 0/1
Batch Loss = 0.000730902849379987
1899, epoch_train_loss=0.000730902849379987
Epoch 1900
Epoch 1900 :: Batch 0/1
Batch Loss = 0.000730902849379987
1900, epoch_train_loss=0.000730902849379987
Epoch 1901
Epoch 1901 :: Batch 0/1
Batch Loss = 0.0007309028493799869
1901, epoch_train_loss=0.0007309028493799869
Epoch 1902
Epoch 1902 :: Batch 0/1
Batch Loss = 0.0007309028493799869
1902, epoch_train_loss=0.0007309028493799869
Epoch 1903
Epoch 1903 :: Batch 0/1
Batch Loss = 0.0007309028493799867
1903, epoch_train_loss=0.0007309028493799867
Epoch 1904
Epoch 1904 :: Batch 0/1
Batch Loss = 0.0007309028493799867
1904, epoch_train_loss=0.0007309028493799867
Epoch 1905
Epoch 1905 :: Batch 0/1
Batch Loss = 0.0007309028493799867
1905, epoch_train_loss=0.0007309028493799867
Epoch 1906
Epoch 1906 :: Batch 0/1
Batch Loss = 0.0007309028493799867
1906, epoch_train_loss=0.0007309028493799867
Epoch 1907
Epoch 1907 :: Batch 0/1
Batch Loss = 0.0007309028493799866
1907, epoch_train_loss=0.0007309028493799866
Epoch 1908
Epoch 1908 :: Batch 0/1
Batch Loss = 0.0007309028493799864
1908, epoch_train_loss=0.0007309028493799864
Epoch 1909
Epoch 1909 :: Batch 0/1
Batch Loss = 0.0007309028493799864
1909, epoch_train_loss=0.0007309028493799864
Epoch 1910
Epoch 1910 :: Batch 0/1
Batch Loss = 0.0007309028493799864
1910, epoch_train_loss=0.0007309028493799864
Epoch 1911
Epoch 1911 :: Batch 0/1
Batch Loss = 0.0007309028493799864
1911, epoch_train_loss=0.0007309028493799864
Epoch 1912
Epoch 1912 :: Batch 0/1
Batch Loss = 0.0007309028493799864
1912, epoch_train_loss=0.0007309028493799864
Epoch 1913
Epoch 1913 :: Batch 0/1
Batch Loss = 0.0007309028493799863
1913, epoch_train_loss=0.0007309028493799863
Epoch 1914
Epoch 1914 :: Batch 0/1
Batch Loss = 0.0007309028493799862
1914, epoch_train_loss=0.0007309028493799862
Epoch 1915
Epoch 1915 :: Batch 0/1
Batch Loss = 0.0007309028493799862
1915, epoch_train_loss=0.0007309028493799862
Epoch 1916
Epoch 1916 :: Batch 0/1
Batch Loss = 0.0007309028493799862
1916, epoch_train_loss=0.0007309028493799862
Epoch 1917
Epoch 1917 :: Batch 0/1
Batch Loss = 0.0007309028493799861
1917, epoch_train_loss=0.0007309028493799861
Epoch 1918
Epoch 1918 :: Batch 0/1
Batch Loss = 0.0007309028493799861
1918, epoch_train_loss=0.0007309028493799861
Epoch 1919
Epoch 1919 :: Batch 0/1
Batch Loss = 0.000730902849379986
1919, epoch_train_loss=0.000730902849379986
Epoch 1920
Epoch 1920 :: Batch 0/1
Batch Loss = 0.000730902849379986
1920, epoch_train_loss=0.000730902849379986
Epoch 1921
Epoch 1921 :: Batch 0/1
Batch Loss = 0.000730902849379986
1921, epoch_train_loss=0.000730902849379986
Epoch 1922
Epoch 1922 :: Batch 0/1
Batch Loss = 0.0007309028493799859
1922, epoch_train_loss=0.0007309028493799859
Epoch 1923
Epoch 1923 :: Batch 0/1
Batch Loss = 0.0007309028493799859
1923, epoch_train_loss=0.0007309028493799859
Epoch 1924
Epoch 1924 :: Batch 0/1
Batch Loss = 0.0007309028493799859
1924, epoch_train_loss=0.0007309028493799859
Epoch 1925
Epoch 1925 :: Batch 0/1
Batch Loss = 0.0007309028493799858
1925, epoch_train_loss=0.0007309028493799858
Epoch 1926
Epoch 1926 :: Batch 0/1
Batch Loss = 0.0007309028493799858
1926, epoch_train_loss=0.0007309028493799858
Epoch 1927
Epoch 1927 :: Batch 0/1
Batch Loss = 0.0007309028493799858
1927, epoch_train_loss=0.0007309028493799858
Epoch 1928
Epoch 1928 :: Batch 0/1
Batch Loss = 0.0007309028493799857
1928, epoch_train_loss=0.0007309028493799857
Epoch 1929
Epoch 1929 :: Batch 0/1
Batch Loss = 0.0007309028493799857
1929, epoch_train_loss=0.0007309028493799857
Epoch 1930
Epoch 1930 :: Batch 0/1
Batch Loss = 0.0007309028493799855
1930, epoch_train_loss=0.0007309028493799855
Epoch 1931
Epoch 1931 :: Batch 0/1
Batch Loss = 0.0007309028493799855
1931, epoch_train_loss=0.0007309028493799855
Epoch 1932
Epoch 1932 :: Batch 0/1
Batch Loss = 0.0007309028493799855
1932, epoch_train_loss=0.0007309028493799855
Epoch 1933
Epoch 1933 :: Batch 0/1
Batch Loss = 0.0007309028493799855
1933, epoch_train_loss=0.0007309028493799855
Epoch 1934
Epoch 1934 :: Batch 0/1
Batch Loss = 0.0007309028493799854
1934, epoch_train_loss=0.0007309028493799854
Epoch 1935
Epoch 1935 :: Batch 0/1
Batch Loss = 0.0007309028493799854
1935, epoch_train_loss=0.0007309028493799854
Epoch 1936
Epoch 1936 :: Batch 0/1
Batch Loss = 0.0007309028493799853
1936, epoch_train_loss=0.0007309028493799853
Epoch 1937
Epoch 1937 :: Batch 0/1
Batch Loss = 0.0007309028493799853
1937, epoch_train_loss=0.0007309028493799853
Epoch 1938
Epoch 1938 :: Batch 0/1
Batch Loss = 0.0007309028493799853
1938, epoch_train_loss=0.0007309028493799853
Epoch 1939
Epoch 1939 :: Batch 0/1
Batch Loss = 0.0007309028493799853
1939, epoch_train_loss=0.0007309028493799853
Epoch 1940
Epoch 1940 :: Batch 0/1
Batch Loss = 0.0007309028493799851
1940, epoch_train_loss=0.0007309028493799851
Epoch 1941
Epoch 1941 :: Batch 0/1
Batch Loss = 0.0007309028493799851
1941, epoch_train_loss=0.0007309028493799851
Epoch 1942
Epoch 1942 :: Batch 0/1
Batch Loss = 0.000730902849379985
1942, epoch_train_loss=0.000730902849379985
Epoch 1943
Epoch 1943 :: Batch 0/1
Batch Loss = 0.000730902849379985
1943, epoch_train_loss=0.000730902849379985
Epoch 1944
Epoch 1944 :: Batch 0/1
Batch Loss = 0.0007309028493799849
1944, epoch_train_loss=0.0007309028493799849
Epoch 1945
Epoch 1945 :: Batch 0/1
Batch Loss = 0.0007309028493799849
1945, epoch_train_loss=0.0007309028493799849
Epoch 1946
Epoch 1946 :: Batch 0/1
Batch Loss = 0.0007309028493799848
1946, epoch_train_loss=0.0007309028493799848
Epoch 1947
Epoch 1947 :: Batch 0/1
Batch Loss = 0.0007309028493799848
1947, epoch_train_loss=0.0007309028493799848
Epoch 1948
Epoch 1948 :: Batch 0/1
Batch Loss = 0.0007309028493799848
1948, epoch_train_loss=0.0007309028493799848
Epoch 1949
Epoch 1949 :: Batch 0/1
Batch Loss = 0.0007309028493799848
1949, epoch_train_loss=0.0007309028493799848
Epoch 1950
Epoch 1950 :: Batch 0/1
Batch Loss = 0.0007309028493799848
1950, epoch_train_loss=0.0007309028493799848
Epoch 1951
Epoch 1951 :: Batch 0/1
Batch Loss = 0.0007309028493799846
1951, epoch_train_loss=0.0007309028493799846
Epoch 1952
Epoch 1952 :: Batch 0/1
Batch Loss = 0.0007309028493799846
1952, epoch_train_loss=0.0007309028493799846
Epoch 1953
Epoch 1953 :: Batch 0/1
Batch Loss = 0.0007309028493799846
1953, epoch_train_loss=0.0007309028493799846
Epoch 1954
Epoch 1954 :: Batch 0/1
Batch Loss = 0.0007309028493799846
1954, epoch_train_loss=0.0007309028493799846
Epoch 1955
Epoch 1955 :: Batch 0/1
Batch Loss = 0.0007309028493799844
1955, epoch_train_loss=0.0007309028493799844
Epoch 1956
Epoch 1956 :: Batch 0/1
Batch Loss = 0.0007309028493799844
1956, epoch_train_loss=0.0007309028493799844
Epoch 1957
Epoch 1957 :: Batch 0/1
Batch Loss = 0.0007309028493799844
1957, epoch_train_loss=0.0007309028493799844
Epoch 1958
Epoch 1958 :: Batch 0/1
Batch Loss = 0.0007309028493799843
1958, epoch_train_loss=0.0007309028493799843
Epoch 1959
Epoch 1959 :: Batch 0/1
Batch Loss = 0.0007309028493799843
1959, epoch_train_loss=0.0007309028493799843
Epoch 1960
Epoch 1960 :: Batch 0/1
Batch Loss = 0.0007309028493799843
1960, epoch_train_loss=0.0007309028493799843
Epoch 1961
Epoch 1961 :: Batch 0/1
Batch Loss = 0.0007309028493799842
1961, epoch_train_loss=0.0007309028493799842
Epoch 1962
Epoch 1962 :: Batch 0/1
Batch Loss = 0.0007309028493799841
1962, epoch_train_loss=0.0007309028493799841
Epoch 1963
Epoch 1963 :: Batch 0/1
Batch Loss = 0.0007309028493799841
1963, epoch_train_loss=0.0007309028493799841
Epoch 1964
Epoch 1964 :: Batch 0/1
Batch Loss = 0.0007309028493799841
1964, epoch_train_loss=0.0007309028493799841
Epoch 1965
Epoch 1965 :: Batch 0/1
Batch Loss = 0.000730902849379984
1965, epoch_train_loss=0.000730902849379984
Epoch 1966
Epoch 1966 :: Batch 0/1
Batch Loss = 0.000730902849379984
1966, epoch_train_loss=0.000730902849379984
Epoch 1967
Epoch 1967 :: Batch 0/1
Batch Loss = 0.0007309028493799838
1967, epoch_train_loss=0.0007309028493799838
Epoch 1968
Epoch 1968 :: Batch 0/1
Batch Loss = 0.0007309028493799838
1968, epoch_train_loss=0.0007309028493799838
Epoch 1969
Epoch 1969 :: Batch 0/1
Batch Loss = 0.0007309028493799838
1969, epoch_train_loss=0.0007309028493799838
Epoch 1970
Epoch 1970 :: Batch 0/1
Batch Loss = 0.0007309028493799838
1970, epoch_train_loss=0.0007309028493799838
Epoch 1971
Epoch 1971 :: Batch 0/1
Batch Loss = 0.0007309028493799837
1971, epoch_train_loss=0.0007309028493799837
Epoch 1972
Epoch 1972 :: Batch 0/1
Batch Loss = 0.0007309028493799836
1972, epoch_train_loss=0.0007309028493799836
Epoch 1973
Epoch 1973 :: Batch 0/1
Batch Loss = 0.0007309028493799836
1973, epoch_train_loss=0.0007309028493799836
Epoch 1974
Epoch 1974 :: Batch 0/1
Batch Loss = 0.0007309028493799836
1974, epoch_train_loss=0.0007309028493799836
Epoch 1975
Epoch 1975 :: Batch 0/1
Batch Loss = 0.0007309028493799836
1975, epoch_train_loss=0.0007309028493799836
Epoch 1976
Epoch 1976 :: Batch 0/1
Batch Loss = 0.0007309028493799836
1976, epoch_train_loss=0.0007309028493799836
Epoch 1977
Epoch 1977 :: Batch 0/1
Batch Loss = 0.0007309028493799834
1977, epoch_train_loss=0.0007309028493799834
Epoch 1978
Epoch 1978 :: Batch 0/1
Batch Loss = 0.0007309028493799834
1978, epoch_train_loss=0.0007309028493799834
Epoch 1979
Epoch 1979 :: Batch 0/1
Batch Loss = 0.0007309028493799834
1979, epoch_train_loss=0.0007309028493799834
Epoch 1980
Epoch 1980 :: Batch 0/1
Batch Loss = 0.0007309028493799834
1980, epoch_train_loss=0.0007309028493799834
Epoch 1981
Epoch 1981 :: Batch 0/1
Batch Loss = 0.0007309028493799832
1981, epoch_train_loss=0.0007309028493799832
Epoch 1982
Epoch 1982 :: Batch 0/1
Batch Loss = 0.0007309028493799831
1982, epoch_train_loss=0.0007309028493799831
Epoch 1983
Epoch 1983 :: Batch 0/1
Batch Loss = 0.0007309028493799831
1983, epoch_train_loss=0.0007309028493799831
Epoch 1984
Epoch 1984 :: Batch 0/1
Batch Loss = 0.0007309028493799831
1984, epoch_train_loss=0.0007309028493799831
Epoch 1985
Epoch 1985 :: Batch 0/1
Batch Loss = 0.0007309028493799831
1985, epoch_train_loss=0.0007309028493799831
Epoch 1986
Epoch 1986 :: Batch 0/1
Batch Loss = 0.0007309028493799831
1986, epoch_train_loss=0.0007309028493799831
Epoch 1987
Epoch 1987 :: Batch 0/1
Batch Loss = 0.0007309028493799831
1987, epoch_train_loss=0.0007309028493799831
Epoch 1988
Epoch 1988 :: Batch 0/1
Batch Loss = 0.0007309028493799829
1988, epoch_train_loss=0.0007309028493799829
Epoch 1989
Epoch 1989 :: Batch 0/1
Batch Loss = 0.0007309028493799829
1989, epoch_train_loss=0.0007309028493799829
Epoch 1990
Epoch 1990 :: Batch 0/1
Batch Loss = 0.0007309028493799829
1990, epoch_train_loss=0.0007309028493799829
Epoch 1991
Epoch 1991 :: Batch 0/1
Batch Loss = 0.0007309028493799829
1991, epoch_train_loss=0.0007309028493799829
Epoch 1992
Epoch 1992 :: Batch 0/1
Batch Loss = 0.0007309028493799829
1992, epoch_train_loss=0.0007309028493799829
Epoch 1993
Epoch 1993 :: Batch 0/1
Batch Loss = 0.0007309028493799827
1993, epoch_train_loss=0.0007309028493799827
Epoch 1994
Epoch 1994 :: Batch 0/1
Batch Loss = 0.0007309028493799827
1994, epoch_train_loss=0.0007309028493799827
Epoch 1995
Epoch 1995 :: Batch 0/1
Batch Loss = 0.0007309028493799827
1995, epoch_train_loss=0.0007309028493799827
Epoch 1996
Epoch 1996 :: Batch 0/1
Batch Loss = 0.0007309028493799827
1996, epoch_train_loss=0.0007309028493799827
Epoch 1997
Epoch 1997 :: Batch 0/1
Batch Loss = 0.0007309028493799825
1997, epoch_train_loss=0.0007309028493799825
Epoch 1998
Epoch 1998 :: Batch 0/1
Batch Loss = 0.0007309028493799824
1998, epoch_train_loss=0.0007309028493799824
Epoch 1999
Epoch 1999 :: Batch 0/1
Batch Loss = 0.0007309028493799824
1999, epoch_train_loss=0.0007309028493799824
Epoch 2000
Epoch 2000 :: Batch 0/1
Batch Loss = 0.0007309028493799824
2000, epoch_train_loss=0.0007309028493799824
Epoch 2001
Epoch 2001 :: Batch 0/1
Batch Loss = 0.0007309028493799824
2001, epoch_train_loss=0.0007309028493799824
Epoch 2002
Epoch 2002 :: Batch 0/1
Batch Loss = 0.0007309028493799824
2002, epoch_train_loss=0.0007309028493799824
Epoch 2003
Epoch 2003 :: Batch 0/1
Batch Loss = 0.0007309028493799821
2003, epoch_train_loss=0.0007309028493799821
Epoch 2004
Epoch 2004 :: Batch 0/1
Batch Loss = 0.0007309028493799821
2004, epoch_train_loss=0.0007309028493799821
Epoch 2005
Epoch 2005 :: Batch 0/1
Batch Loss = 0.0007309028493799821
2005, epoch_train_loss=0.0007309028493799821
Epoch 2006
Epoch 2006 :: Batch 0/1
Batch Loss = 0.0007309028493799821
2006, epoch_train_loss=0.0007309028493799821
Epoch 2007
Epoch 2007 :: Batch 0/1
Batch Loss = 0.0007309028493799821
2007, epoch_train_loss=0.0007309028493799821
Epoch 2008
Epoch 2008 :: Batch 0/1
Batch Loss = 0.0007309028493799819
2008, epoch_train_loss=0.0007309028493799819
Epoch 2009
Epoch 2009 :: Batch 0/1
Batch Loss = 0.0007309028493799819
2009, epoch_train_loss=0.0007309028493799819
Epoch 2010
Epoch 2010 :: Batch 0/1
Batch Loss = 0.0007309028493799819
2010, epoch_train_loss=0.0007309028493799819
Epoch 2011
Epoch 2011 :: Batch 0/1
Batch Loss = 0.0007309028493799819
2011, epoch_train_loss=0.0007309028493799819
Epoch 2012
Epoch 2012 :: Batch 0/1
Batch Loss = 0.0007309028493799819
2012, epoch_train_loss=0.0007309028493799819
Epoch 2013
Epoch 2013 :: Batch 0/1
Batch Loss = 0.0007309028493799817
2013, epoch_train_loss=0.0007309028493799817
Epoch 2014
Epoch 2014 :: Batch 0/1
Batch Loss = 0.0007309028493799817
2014, epoch_train_loss=0.0007309028493799817
Epoch 2015
Epoch 2015 :: Batch 0/1
Batch Loss = 0.0007309028493799817
2015, epoch_train_loss=0.0007309028493799817
Epoch 2016
Epoch 2016 :: Batch 0/1
Batch Loss = 0.0007309028493799817
2016, epoch_train_loss=0.0007309028493799817
Epoch 2017
Epoch 2017 :: Batch 0/1
Batch Loss = 0.0007309028493799817
2017, epoch_train_loss=0.0007309028493799817
Epoch 2018
Epoch 2018 :: Batch 0/1
Batch Loss = 0.0007309028493799816
2018, epoch_train_loss=0.0007309028493799816
Epoch 2019
Epoch 2019 :: Batch 0/1
Batch Loss = 0.0007309028493799815
2019, epoch_train_loss=0.0007309028493799815
Epoch 2020
Epoch 2020 :: Batch 0/1
Batch Loss = 0.0007309028493799815
2020, epoch_train_loss=0.0007309028493799815
Epoch 2021
Epoch 2021 :: Batch 0/1
Batch Loss = 0.0007309028493799815
2021, epoch_train_loss=0.0007309028493799815
Epoch 2022
Epoch 2022 :: Batch 0/1
Batch Loss = 0.0007309028493799814
2022, epoch_train_loss=0.0007309028493799814
Epoch 2023
Epoch 2023 :: Batch 0/1
Batch Loss = 0.0007309028493799814
2023, epoch_train_loss=0.0007309028493799814
Epoch 2024
Epoch 2024 :: Batch 0/1
Batch Loss = 0.0007309028493799812
2024, epoch_train_loss=0.0007309028493799812
Epoch 2025
Epoch 2025 :: Batch 0/1
Batch Loss = 0.0007309028493799812
2025, epoch_train_loss=0.0007309028493799812
Epoch 2026
Epoch 2026 :: Batch 0/1
Batch Loss = 0.0007309028493799812
2026, epoch_train_loss=0.0007309028493799812
Epoch 2027
Epoch 2027 :: Batch 0/1
Batch Loss = 0.0007309028493799812
2027, epoch_train_loss=0.0007309028493799812
Epoch 2028
Epoch 2028 :: Batch 0/1
Batch Loss = 0.0007309028493799811
2028, epoch_train_loss=0.0007309028493799811
Epoch 2029
Epoch 2029 :: Batch 0/1
Batch Loss = 0.0007309028493799811
2029, epoch_train_loss=0.0007309028493799811
Epoch 2030
Epoch 2030 :: Batch 0/1
Batch Loss = 0.0007309028493799809
2030, epoch_train_loss=0.0007309028493799809
Epoch 2031
Epoch 2031 :: Batch 0/1
Batch Loss = 0.0007309028493799809
2031, epoch_train_loss=0.0007309028493799809
Epoch 2032
Epoch 2032 :: Batch 0/1
Batch Loss = 0.0007309028493799809
2032, epoch_train_loss=0.0007309028493799809
Epoch 2033
Epoch 2033 :: Batch 0/1
Batch Loss = 0.0007309028493799808
2033, epoch_train_loss=0.0007309028493799808
Epoch 2034
Epoch 2034 :: Batch 0/1
Batch Loss = 0.0007309028493799808
2034, epoch_train_loss=0.0007309028493799808
Epoch 2035
Epoch 2035 :: Batch 0/1
Batch Loss = 0.0007309028493799808
2035, epoch_train_loss=0.0007309028493799808
Epoch 2036
Epoch 2036 :: Batch 0/1
Batch Loss = 0.0007309028493799807
2036, epoch_train_loss=0.0007309028493799807
Epoch 2037
Epoch 2037 :: Batch 0/1
Batch Loss = 0.0007309028493799807
2037, epoch_train_loss=0.0007309028493799807
Epoch 2038
Epoch 2038 :: Batch 0/1
Batch Loss = 0.0007309028493799806
2038, epoch_train_loss=0.0007309028493799806
Epoch 2039
Epoch 2039 :: Batch 0/1
Batch Loss = 0.0007309028493799806
2039, epoch_train_loss=0.0007309028493799806
Epoch 2040
Epoch 2040 :: Batch 0/1
Batch Loss = 0.0007309028493799806
2040, epoch_train_loss=0.0007309028493799806
Epoch 2041
Epoch 2041 :: Batch 0/1
Batch Loss = 0.0007309028493799805
2041, epoch_train_loss=0.0007309028493799805
Epoch 2042
Epoch 2042 :: Batch 0/1
Batch Loss = 0.0007309028493799805
2042, epoch_train_loss=0.0007309028493799805
Epoch 2043
Epoch 2043 :: Batch 0/1
Batch Loss = 0.0007309028493799804
2043, epoch_train_loss=0.0007309028493799804
Epoch 2044
Epoch 2044 :: Batch 0/1
Batch Loss = 0.0007309028493799804
2044, epoch_train_loss=0.0007309028493799804
Epoch 2045
Epoch 2045 :: Batch 0/1
Batch Loss = 0.0007309028493799804
2045, epoch_train_loss=0.0007309028493799804
Epoch 2046
Epoch 2046 :: Batch 0/1
Batch Loss = 0.0007309028493799803
2046, epoch_train_loss=0.0007309028493799803
Epoch 2047
Epoch 2047 :: Batch 0/1
Batch Loss = 0.0007309028493799803
2047, epoch_train_loss=0.0007309028493799803
Epoch 2048
Epoch 2048 :: Batch 0/1
Batch Loss = 0.0007309028493799803
2048, epoch_train_loss=0.0007309028493799803
Epoch 2049
Epoch 2049 :: Batch 0/1
Batch Loss = 0.0007309028493799802
2049, epoch_train_loss=0.0007309028493799802
Epoch 2050
Epoch 2050 :: Batch 0/1
Batch Loss = 0.0007309028493799802
2050, epoch_train_loss=0.0007309028493799802
Epoch 2051
Epoch 2051 :: Batch 0/1
Batch Loss = 0.00073090284937998
2051, epoch_train_loss=0.00073090284937998
Epoch 2052
Epoch 2052 :: Batch 0/1
Batch Loss = 0.00073090284937998
2052, epoch_train_loss=0.00073090284937998
Epoch 2053
Epoch 2053 :: Batch 0/1
Batch Loss = 0.0007309028493799798
2053, epoch_train_loss=0.0007309028493799798
Epoch 2054
Epoch 2054 :: Batch 0/1
Batch Loss = 0.0007309028493799798
2054, epoch_train_loss=0.0007309028493799798
Epoch 2055
Epoch 2055 :: Batch 0/1
Batch Loss = 0.0007309028493799798
2055, epoch_train_loss=0.0007309028493799798
Epoch 2056
Epoch 2056 :: Batch 0/1
Batch Loss = 0.0007309028493799797
2056, epoch_train_loss=0.0007309028493799797
Epoch 2057
Epoch 2057 :: Batch 0/1
Batch Loss = 0.0007309028493799797
2057, epoch_train_loss=0.0007309028493799797
Epoch 2058
Epoch 2058 :: Batch 0/1
Batch Loss = 0.0007309028493799797
2058, epoch_train_loss=0.0007309028493799797
Epoch 2059
Epoch 2059 :: Batch 0/1
Batch Loss = 0.0007309028493799796
2059, epoch_train_loss=0.0007309028493799796
Epoch 2060
Epoch 2060 :: Batch 0/1
Batch Loss = 0.0007309028493799796
2060, epoch_train_loss=0.0007309028493799796
Epoch 2061
Epoch 2061 :: Batch 0/1
Batch Loss = 0.0007309028493799795
2061, epoch_train_loss=0.0007309028493799795
Epoch 2062
Epoch 2062 :: Batch 0/1
Batch Loss = 0.0007309028493799795
2062, epoch_train_loss=0.0007309028493799795
Epoch 2063
Epoch 2063 :: Batch 0/1
Batch Loss = 0.0007309028493799794
2063, epoch_train_loss=0.0007309028493799794
Epoch 2064
Epoch 2064 :: Batch 0/1
Batch Loss = 0.0007309028493799794
2064, epoch_train_loss=0.0007309028493799794
Epoch 2065
Epoch 2065 :: Batch 0/1
Batch Loss = 0.0007309028493799793
2065, epoch_train_loss=0.0007309028493799793
Epoch 2066
Epoch 2066 :: Batch 0/1
Batch Loss = 0.0007309028493799793
2066, epoch_train_loss=0.0007309028493799793
Epoch 2067
Epoch 2067 :: Batch 0/1
Batch Loss = 0.0007309028493799793
2067, epoch_train_loss=0.0007309028493799793
Epoch 2068
Epoch 2068 :: Batch 0/1
Batch Loss = 0.0007309028493799793
2068, epoch_train_loss=0.0007309028493799793
Epoch 2069
Epoch 2069 :: Batch 0/1
Batch Loss = 0.0007309028493799792
2069, epoch_train_loss=0.0007309028493799792
Epoch 2070
Epoch 2070 :: Batch 0/1
Batch Loss = 0.0007309028493799791
2070, epoch_train_loss=0.0007309028493799791
Epoch 2071
Epoch 2071 :: Batch 0/1
Batch Loss = 0.0007309028493799791
2071, epoch_train_loss=0.0007309028493799791
Epoch 2072
Epoch 2072 :: Batch 0/1
Batch Loss = 0.0007309028493799791
2072, epoch_train_loss=0.0007309028493799791
Epoch 2073
Epoch 2073 :: Batch 0/1
Batch Loss = 0.0007309028493799791
2073, epoch_train_loss=0.0007309028493799791
Epoch 2074
Epoch 2074 :: Batch 0/1
Batch Loss = 0.0007309028493799791
2074, epoch_train_loss=0.0007309028493799791
Epoch 2075
Epoch 2075 :: Batch 0/1
Batch Loss = 0.0007309028493799787
2075, epoch_train_loss=0.0007309028493799787
Epoch 2076
Epoch 2076 :: Batch 0/1
Batch Loss = 0.0007309028493799787
2076, epoch_train_loss=0.0007309028493799787
Epoch 2077
Epoch 2077 :: Batch 0/1
Batch Loss = 0.0007309028493799787
2077, epoch_train_loss=0.0007309028493799787
Epoch 2078
Epoch 2078 :: Batch 0/1
Batch Loss = 0.0007309028493799787
2078, epoch_train_loss=0.0007309028493799787
Epoch 2079
Epoch 2079 :: Batch 0/1
Batch Loss = 0.0007309028493799785
2079, epoch_train_loss=0.0007309028493799785
Epoch 2080
Epoch 2080 :: Batch 0/1
Batch Loss = 0.0007309028493799785
2080, epoch_train_loss=0.0007309028493799785
Epoch 2081
Epoch 2081 :: Batch 0/1
Batch Loss = 0.0007309028493799785
2081, epoch_train_loss=0.0007309028493799785
Epoch 2082
Epoch 2082 :: Batch 0/1
Batch Loss = 0.0007309028493799785
2082, epoch_train_loss=0.0007309028493799785
Epoch 2083
Epoch 2083 :: Batch 0/1
Batch Loss = 0.0007309028493799784
2083, epoch_train_loss=0.0007309028493799784
Epoch 2084
Epoch 2084 :: Batch 0/1
Batch Loss = 0.0007309028493799783
2084, epoch_train_loss=0.0007309028493799783
Epoch 2085
Epoch 2085 :: Batch 0/1
Batch Loss = 0.0007309028493799783
2085, epoch_train_loss=0.0007309028493799783
Epoch 2086
Epoch 2086 :: Batch 0/1
Batch Loss = 0.0007309028493799783
2086, epoch_train_loss=0.0007309028493799783
Epoch 2087
Epoch 2087 :: Batch 0/1
Batch Loss = 0.0007309028493799783
2087, epoch_train_loss=0.0007309028493799783
Epoch 2088
Epoch 2088 :: Batch 0/1
Batch Loss = 0.0007309028493799783
2088, epoch_train_loss=0.0007309028493799783
Epoch 2089
Epoch 2089 :: Batch 0/1
Batch Loss = 0.0007309028493799783
2089, epoch_train_loss=0.0007309028493799783
Epoch 2090
Epoch 2090 :: Batch 0/1
Batch Loss = 0.0007309028493799781
2090, epoch_train_loss=0.0007309028493799781
Epoch 2091
Epoch 2091 :: Batch 0/1
Batch Loss = 0.0007309028493799781
2091, epoch_train_loss=0.0007309028493799781
Epoch 2092
Epoch 2092 :: Batch 0/1
Batch Loss = 0.0007309028493799781
2092, epoch_train_loss=0.0007309028493799781
Epoch 2093
Epoch 2093 :: Batch 0/1
Batch Loss = 0.0007309028493799781
2093, epoch_train_loss=0.0007309028493799781
Epoch 2094
Epoch 2094 :: Batch 0/1
Batch Loss = 0.0007309028493799781
2094, epoch_train_loss=0.0007309028493799781
Epoch 2095
Epoch 2095 :: Batch 0/1
Batch Loss = 0.0007309028493799781
2095, epoch_train_loss=0.0007309028493799781
Epoch 2096
Epoch 2096 :: Batch 0/1
Batch Loss = 0.0007309028493799779
2096, epoch_train_loss=0.0007309028493799779
Epoch 2097
Epoch 2097 :: Batch 0/1
Batch Loss = 0.0007309028493799779
2097, epoch_train_loss=0.0007309028493799779
Epoch 2098
Epoch 2098 :: Batch 0/1
Batch Loss = 0.0007309028493799779
2098, epoch_train_loss=0.0007309028493799779
Epoch 2099
Epoch 2099 :: Batch 0/1
Batch Loss = 0.0007309028493799779
2099, epoch_train_loss=0.0007309028493799779
Epoch 2100
Epoch 2100 :: Batch 0/1
Batch Loss = 0.0007309028493799776
2100, epoch_train_loss=0.0007309028493799776
Epoch 2101
Epoch 2101 :: Batch 0/1
Batch Loss = 0.0007309028493799776
2101, epoch_train_loss=0.0007309028493799776
Epoch 2102
Epoch 2102 :: Batch 0/1
Batch Loss = 0.0007309028493799776
2102, epoch_train_loss=0.0007309028493799776
Epoch 2103
Epoch 2103 :: Batch 0/1
Batch Loss = 0.0007309028493799774
2103, epoch_train_loss=0.0007309028493799774
Epoch 2104
Epoch 2104 :: Batch 0/1
Batch Loss = 0.0007309028493799774
2104, epoch_train_loss=0.0007309028493799774
Epoch 2105
Epoch 2105 :: Batch 0/1
Batch Loss = 0.0007309028493799773
2105, epoch_train_loss=0.0007309028493799773
Epoch 2106
Epoch 2106 :: Batch 0/1
Batch Loss = 0.0007309028493799773
2106, epoch_train_loss=0.0007309028493799773
Epoch 2107
Epoch 2107 :: Batch 0/1
Batch Loss = 0.0007309028493799773
2107, epoch_train_loss=0.0007309028493799773
Epoch 2108
Epoch 2108 :: Batch 0/1
Batch Loss = 0.0007309028493799772
2108, epoch_train_loss=0.0007309028493799772
Epoch 2109
Epoch 2109 :: Batch 0/1
Batch Loss = 0.0007309028493799772
2109, epoch_train_loss=0.0007309028493799772
Epoch 2110
Epoch 2110 :: Batch 0/1
Batch Loss = 0.0007309028493799771
2110, epoch_train_loss=0.0007309028493799771
Epoch 2111
Epoch 2111 :: Batch 0/1
Batch Loss = 0.0007309028493799771
2111, epoch_train_loss=0.0007309028493799771
Epoch 2112
Epoch 2112 :: Batch 0/1
Batch Loss = 0.0007309028493799771
2112, epoch_train_loss=0.0007309028493799771
Epoch 2113
Epoch 2113 :: Batch 0/1
Batch Loss = 0.000730902849379977
2113, epoch_train_loss=0.000730902849379977
Epoch 2114
Epoch 2114 :: Batch 0/1
Batch Loss = 0.0007309028493799769
2114, epoch_train_loss=0.0007309028493799769
Epoch 2115
Epoch 2115 :: Batch 0/1
Batch Loss = 0.0007309028493799769
2115, epoch_train_loss=0.0007309028493799769
Epoch 2116
Epoch 2116 :: Batch 0/1
Batch Loss = 0.0007309028493799769
2116, epoch_train_loss=0.0007309028493799769
Epoch 2117
Epoch 2117 :: Batch 0/1
Batch Loss = 0.0007309028493799769
2117, epoch_train_loss=0.0007309028493799769
Epoch 2118
Epoch 2118 :: Batch 0/1
Batch Loss = 0.0007309028493799768
2118, epoch_train_loss=0.0007309028493799768
Epoch 2119
Epoch 2119 :: Batch 0/1
Batch Loss = 0.0007309028493799767
2119, epoch_train_loss=0.0007309028493799767
Epoch 2120
Epoch 2120 :: Batch 0/1
Batch Loss = 0.0007309028493799767
2120, epoch_train_loss=0.0007309028493799767
Epoch 2121
Epoch 2121 :: Batch 0/1
Batch Loss = 0.0007309028493799767
2121, epoch_train_loss=0.0007309028493799767
Epoch 2122
Epoch 2122 :: Batch 0/1
Batch Loss = 0.0007309028493799767
2122, epoch_train_loss=0.0007309028493799767
Epoch 2123
Epoch 2123 :: Batch 0/1
Batch Loss = 0.0007309028493799765
2123, epoch_train_loss=0.0007309028493799765
Epoch 2124
Epoch 2124 :: Batch 0/1
Batch Loss = 0.0007309028493799765
2124, epoch_train_loss=0.0007309028493799765
Epoch 2125
Epoch 2125 :: Batch 0/1
Batch Loss = 0.0007309028493799764
2125, epoch_train_loss=0.0007309028493799764
Epoch 2126
Epoch 2126 :: Batch 0/1
Batch Loss = 0.0007309028493799764
2126, epoch_train_loss=0.0007309028493799764
Epoch 2127
Epoch 2127 :: Batch 0/1
Batch Loss = 0.0007309028493799764
2127, epoch_train_loss=0.0007309028493799764
Epoch 2128
Epoch 2128 :: Batch 0/1
Batch Loss = 0.0007309028493799764
2128, epoch_train_loss=0.0007309028493799764
Epoch 2129
Epoch 2129 :: Batch 0/1
Batch Loss = 0.0007309028493799761
2129, epoch_train_loss=0.0007309028493799761
Epoch 2130
Epoch 2130 :: Batch 0/1
Batch Loss = 0.0007309028493799761
2130, epoch_train_loss=0.0007309028493799761
Epoch 2131
Epoch 2131 :: Batch 0/1
Batch Loss = 0.0007309028493799761
2131, epoch_train_loss=0.0007309028493799761
Epoch 2132
Epoch 2132 :: Batch 0/1
Batch Loss = 0.0007309028493799761
2132, epoch_train_loss=0.0007309028493799761
Epoch 2133
Epoch 2133 :: Batch 0/1
Batch Loss = 0.000730902849379976
2133, epoch_train_loss=0.000730902849379976
Epoch 2134
Epoch 2134 :: Batch 0/1
Batch Loss = 0.000730902849379976
2134, epoch_train_loss=0.000730902849379976
Epoch 2135
Epoch 2135 :: Batch 0/1
Batch Loss = 0.0007309028493799759
2135, epoch_train_loss=0.0007309028493799759
Epoch 2136
Epoch 2136 :: Batch 0/1
Batch Loss = 0.0007309028493799759
2136, epoch_train_loss=0.0007309028493799759
Epoch 2137
Epoch 2137 :: Batch 0/1
Batch Loss = 0.0007309028493799759
2137, epoch_train_loss=0.0007309028493799759
Epoch 2138
Epoch 2138 :: Batch 0/1
Batch Loss = 0.0007309028493799758
2138, epoch_train_loss=0.0007309028493799758
Epoch 2139
Epoch 2139 :: Batch 0/1
Batch Loss = 0.0007309028493799758
2139, epoch_train_loss=0.0007309028493799758
Epoch 2140
Epoch 2140 :: Batch 0/1
Batch Loss = 0.0007309028493799757
2140, epoch_train_loss=0.0007309028493799757
Epoch 2141
Epoch 2141 :: Batch 0/1
Batch Loss = 0.0007309028493799757
2141, epoch_train_loss=0.0007309028493799757
Epoch 2142
Epoch 2142 :: Batch 0/1
Batch Loss = 0.0007309028493799757
2142, epoch_train_loss=0.0007309028493799757
Epoch 2143
Epoch 2143 :: Batch 0/1
Batch Loss = 0.0007309028493799757
2143, epoch_train_loss=0.0007309028493799757
Epoch 2144
Epoch 2144 :: Batch 0/1
Batch Loss = 0.0007309028493799756
2144, epoch_train_loss=0.0007309028493799756
Epoch 2145
Epoch 2145 :: Batch 0/1
Batch Loss = 0.0007309028493799755
2145, epoch_train_loss=0.0007309028493799755
Epoch 2146
Epoch 2146 :: Batch 0/1
Batch Loss = 0.0007309028493799755
2146, epoch_train_loss=0.0007309028493799755
Epoch 2147
Epoch 2147 :: Batch 0/1
Batch Loss = 0.0007309028493799753
2147, epoch_train_loss=0.0007309028493799753
Epoch 2148
Epoch 2148 :: Batch 0/1
Batch Loss = 0.0007309028493799753
2148, epoch_train_loss=0.0007309028493799753
Epoch 2149
Epoch 2149 :: Batch 0/1
Batch Loss = 0.0007309028493799753
2149, epoch_train_loss=0.0007309028493799753
Epoch 2150
Epoch 2150 :: Batch 0/1
Batch Loss = 0.0007309028493799752
2150, epoch_train_loss=0.0007309028493799752
Epoch 2151
Epoch 2151 :: Batch 0/1
Batch Loss = 0.0007309028493799752
2151, epoch_train_loss=0.0007309028493799752
Epoch 2152
Epoch 2152 :: Batch 0/1
Batch Loss = 0.0007309028493799751
2152, epoch_train_loss=0.0007309028493799751
Epoch 2153
Epoch 2153 :: Batch 0/1
Batch Loss = 0.0007309028493799751
2153, epoch_train_loss=0.0007309028493799751
Epoch 2154
Epoch 2154 :: Batch 0/1
Batch Loss = 0.000730902849379975
2154, epoch_train_loss=0.000730902849379975
Epoch 2155
Epoch 2155 :: Batch 0/1
Batch Loss = 0.000730902849379975
2155, epoch_train_loss=0.000730902849379975
Epoch 2156
Epoch 2156 :: Batch 0/1
Batch Loss = 0.0007309028493799748
2156, epoch_train_loss=0.0007309028493799748
Epoch 2157
Epoch 2157 :: Batch 0/1
Batch Loss = 0.0007309028493799748
2157, epoch_train_loss=0.0007309028493799748
Epoch 2158
Epoch 2158 :: Batch 0/1
Batch Loss = 0.0007309028493799748
2158, epoch_train_loss=0.0007309028493799748
Epoch 2159
Epoch 2159 :: Batch 0/1
Batch Loss = 0.0007309028493799748
2159, epoch_train_loss=0.0007309028493799748
Epoch 2160
Epoch 2160 :: Batch 0/1
Batch Loss = 0.0007309028493799747
2160, epoch_train_loss=0.0007309028493799747
Epoch 2161
Epoch 2161 :: Batch 0/1
Batch Loss = 0.0007309028493799747
2161, epoch_train_loss=0.0007309028493799747
Epoch 2162
Epoch 2162 :: Batch 0/1
Batch Loss = 0.0007309028493799746
2162, epoch_train_loss=0.0007309028493799746
Epoch 2163
Epoch 2163 :: Batch 0/1
Batch Loss = 0.0007309028493799746
2163, epoch_train_loss=0.0007309028493799746
Epoch 2164
Epoch 2164 :: Batch 0/1
Batch Loss = 0.0007309028493799745
2164, epoch_train_loss=0.0007309028493799745
Epoch 2165
Epoch 2165 :: Batch 0/1
Batch Loss = 0.0007309028493799745
2165, epoch_train_loss=0.0007309028493799745
Epoch 2166
Epoch 2166 :: Batch 0/1
Batch Loss = 0.0007309028493799744
2166, epoch_train_loss=0.0007309028493799744
Epoch 2167
Epoch 2167 :: Batch 0/1
Batch Loss = 0.0007309028493799744
2167, epoch_train_loss=0.0007309028493799744
Epoch 2168
Epoch 2168 :: Batch 0/1
Batch Loss = 0.0007309028493799742
2168, epoch_train_loss=0.0007309028493799742
Epoch 2169
Epoch 2169 :: Batch 0/1
Batch Loss = 0.0007309028493799742
2169, epoch_train_loss=0.0007309028493799742
Epoch 2170
Epoch 2170 :: Batch 0/1
Batch Loss = 0.0007309028493799742
2170, epoch_train_loss=0.0007309028493799742
Epoch 2171
Epoch 2171 :: Batch 0/1
Batch Loss = 0.0007309028493799741
2171, epoch_train_loss=0.0007309028493799741
Epoch 2172
Epoch 2172 :: Batch 0/1
Batch Loss = 0.000730902849379974
2172, epoch_train_loss=0.000730902849379974
Epoch 2173
Epoch 2173 :: Batch 0/1
Batch Loss = 0.000730902849379974
2173, epoch_train_loss=0.000730902849379974
Epoch 2174
Epoch 2174 :: Batch 0/1
Batch Loss = 0.000730902849379974
2174, epoch_train_loss=0.000730902849379974
Epoch 2175
Epoch 2175 :: Batch 0/1
Batch Loss = 0.000730902849379974
2175, epoch_train_loss=0.000730902849379974
Epoch 2176
Epoch 2176 :: Batch 0/1
Batch Loss = 0.0007309028493799739
2176, epoch_train_loss=0.0007309028493799739
Epoch 2177
Epoch 2177 :: Batch 0/1
Batch Loss = 0.0007309028493799738
2177, epoch_train_loss=0.0007309028493799738
Epoch 2178
Epoch 2178 :: Batch 0/1
Batch Loss = 0.0007309028493799738
2178, epoch_train_loss=0.0007309028493799738
Epoch 2179
Epoch 2179 :: Batch 0/1
Batch Loss = 0.0007309028493799738
2179, epoch_train_loss=0.0007309028493799738
Epoch 2180
Epoch 2180 :: Batch 0/1
Batch Loss = 0.0007309028493799738
2180, epoch_train_loss=0.0007309028493799738
Epoch 2181
Epoch 2181 :: Batch 0/1
Batch Loss = 0.0007309028493799737
2181, epoch_train_loss=0.0007309028493799737
Epoch 2182
Epoch 2182 :: Batch 0/1
Batch Loss = 0.0007309028493799735
2182, epoch_train_loss=0.0007309028493799735
Epoch 2183
Epoch 2183 :: Batch 0/1
Batch Loss = 0.0007309028493799735
2183, epoch_train_loss=0.0007309028493799735
Epoch 2184
Epoch 2184 :: Batch 0/1
Batch Loss = 0.0007309028493799735
2184, epoch_train_loss=0.0007309028493799735
Epoch 2185
Epoch 2185 :: Batch 0/1
Batch Loss = 0.0007309028493799735
2185, epoch_train_loss=0.0007309028493799735
Epoch 2186
Epoch 2186 :: Batch 0/1
Batch Loss = 0.0007309028493799735
2186, epoch_train_loss=0.0007309028493799735
Epoch 2187
Epoch 2187 :: Batch 0/1
Batch Loss = 0.0007309028493799733
2187, epoch_train_loss=0.0007309028493799733
Epoch 2188
Epoch 2188 :: Batch 0/1
Batch Loss = 0.0007309028493799733
2188, epoch_train_loss=0.0007309028493799733
Epoch 2189
Epoch 2189 :: Batch 0/1
Batch Loss = 0.0007309028493799733
2189, epoch_train_loss=0.0007309028493799733
Epoch 2190
Epoch 2190 :: Batch 0/1
Batch Loss = 0.0007309028493799733
2190, epoch_train_loss=0.0007309028493799733
Epoch 2191
Epoch 2191 :: Batch 0/1
Batch Loss = 0.0007309028493799733
2191, epoch_train_loss=0.0007309028493799733
Epoch 2192
Epoch 2192 :: Batch 0/1
Batch Loss = 0.0007309028493799732
2192, epoch_train_loss=0.0007309028493799732
Epoch 2193
Epoch 2193 :: Batch 0/1
Batch Loss = 0.000730902849379973
2193, epoch_train_loss=0.000730902849379973
Epoch 2194
Epoch 2194 :: Batch 0/1
Batch Loss = 0.000730902849379973
2194, epoch_train_loss=0.000730902849379973
Epoch 2195
Epoch 2195 :: Batch 0/1
Batch Loss = 0.0007309028493799729
2195, epoch_train_loss=0.0007309028493799729
Epoch 2196
Epoch 2196 :: Batch 0/1
Batch Loss = 0.0007309028493799728
2196, epoch_train_loss=0.0007309028493799728
Epoch 2197
Epoch 2197 :: Batch 0/1
Batch Loss = 0.0007309028493799728
2197, epoch_train_loss=0.0007309028493799728
Epoch 2198
Epoch 2198 :: Batch 0/1
Batch Loss = 0.0007309028493799728
2198, epoch_train_loss=0.0007309028493799728
Epoch 2199
Epoch 2199 :: Batch 0/1
Batch Loss = 0.0007309028493799728
2199, epoch_train_loss=0.0007309028493799728
Epoch 2200
Epoch 2200 :: Batch 0/1
Batch Loss = 0.0007309028493799727
2200, epoch_train_loss=0.0007309028493799727
Epoch 2201
Epoch 2201 :: Batch 0/1
Batch Loss = 0.0007309028493799726
2201, epoch_train_loss=0.0007309028493799726
Epoch 2202
Epoch 2202 :: Batch 0/1
Batch Loss = 0.0007309028493799726
2202, epoch_train_loss=0.0007309028493799726
Epoch 2203
Epoch 2203 :: Batch 0/1
Batch Loss = 0.0007309028493799726
2203, epoch_train_loss=0.0007309028493799726
Epoch 2204
Epoch 2204 :: Batch 0/1
Batch Loss = 0.0007309028493799726
2204, epoch_train_loss=0.0007309028493799726
Epoch 2205
Epoch 2205 :: Batch 0/1
Batch Loss = 0.0007309028493799725
2205, epoch_train_loss=0.0007309028493799725
Epoch 2206
Epoch 2206 :: Batch 0/1
Batch Loss = 0.0007309028493799724
2206, epoch_train_loss=0.0007309028493799724
Epoch 2207
Epoch 2207 :: Batch 0/1
Batch Loss = 0.0007309028493799724
2207, epoch_train_loss=0.0007309028493799724
Epoch 2208
Epoch 2208 :: Batch 0/1
Batch Loss = 0.0007309028493799724
2208, epoch_train_loss=0.0007309028493799724
Epoch 2209
Epoch 2209 :: Batch 0/1
Batch Loss = 0.0007309028493799724
2209, epoch_train_loss=0.0007309028493799724
Epoch 2210
Epoch 2210 :: Batch 0/1
Batch Loss = 0.0007309028493799722
2210, epoch_train_loss=0.0007309028493799722
Epoch 2211
Epoch 2211 :: Batch 0/1
Batch Loss = 0.0007309028493799721
2211, epoch_train_loss=0.0007309028493799721
Epoch 2212
Epoch 2212 :: Batch 0/1
Batch Loss = 0.0007309028493799721
2212, epoch_train_loss=0.0007309028493799721
Epoch 2213
Epoch 2213 :: Batch 0/1
Batch Loss = 0.0007309028493799721
2213, epoch_train_loss=0.0007309028493799721
Epoch 2214
Epoch 2214 :: Batch 0/1
Batch Loss = 0.0007309028493799721
2214, epoch_train_loss=0.0007309028493799721
Epoch 2215
Epoch 2215 :: Batch 0/1
Batch Loss = 0.0007309028493799718
2215, epoch_train_loss=0.0007309028493799718
Epoch 2216
Epoch 2216 :: Batch 0/1
Batch Loss = 0.0007309028493799718
2216, epoch_train_loss=0.0007309028493799718
Epoch 2217
Epoch 2217 :: Batch 0/1
Batch Loss = 0.0007309028493799718
2217, epoch_train_loss=0.0007309028493799718
Epoch 2218
Epoch 2218 :: Batch 0/1
Batch Loss = 0.0007309028493799718
2218, epoch_train_loss=0.0007309028493799718
Epoch 2219
Epoch 2219 :: Batch 0/1
Batch Loss = 0.0007309028493799716
2219, epoch_train_loss=0.0007309028493799716
Epoch 2220
Epoch 2220 :: Batch 0/1
Batch Loss = 0.0007309028493799716
2220, epoch_train_loss=0.0007309028493799716
Epoch 2221
Epoch 2221 :: Batch 0/1
Batch Loss = 0.0007309028493799716
2221, epoch_train_loss=0.0007309028493799716
Epoch 2222
Epoch 2222 :: Batch 0/1
Batch Loss = 0.0007309028493799716
2222, epoch_train_loss=0.0007309028493799716
Epoch 2223
Epoch 2223 :: Batch 0/1
Batch Loss = 0.0007309028493799716
2223, epoch_train_loss=0.0007309028493799716
Epoch 2224
Epoch 2224 :: Batch 0/1
Batch Loss = 0.0007309028493799714
2224, epoch_train_loss=0.0007309028493799714
Epoch 2225
Epoch 2225 :: Batch 0/1
Batch Loss = 0.0007309028493799714
2225, epoch_train_loss=0.0007309028493799714
Epoch 2226
Epoch 2226 :: Batch 0/1
Batch Loss = 0.0007309028493799714
2226, epoch_train_loss=0.0007309028493799714
Epoch 2227
Epoch 2227 :: Batch 0/1
Batch Loss = 0.0007309028493799714
2227, epoch_train_loss=0.0007309028493799714
Epoch 2228
Epoch 2228 :: Batch 0/1
Batch Loss = 0.0007309028493799714
2228, epoch_train_loss=0.0007309028493799714
Epoch 2229
Epoch 2229 :: Batch 0/1
Batch Loss = 0.0007309028493799712
2229, epoch_train_loss=0.0007309028493799712
Epoch 2230
Epoch 2230 :: Batch 0/1
Batch Loss = 0.0007309028493799712
2230, epoch_train_loss=0.0007309028493799712
Epoch 2231
Epoch 2231 :: Batch 0/1
Batch Loss = 0.0007309028493799712
2231, epoch_train_loss=0.0007309028493799712
Epoch 2232
Epoch 2232 :: Batch 0/1
Batch Loss = 0.0007309028493799712
2232, epoch_train_loss=0.0007309028493799712
Epoch 2233
Epoch 2233 :: Batch 0/1
Batch Loss = 0.000730902849379971
2233, epoch_train_loss=0.000730902849379971
Epoch 2234
Epoch 2234 :: Batch 0/1
Batch Loss = 0.0007309028493799708
2234, epoch_train_loss=0.0007309028493799708
Epoch 2235
Epoch 2235 :: Batch 0/1
Batch Loss = 0.0007309028493799708
2235, epoch_train_loss=0.0007309028493799708
Epoch 2236
Epoch 2236 :: Batch 0/1
Batch Loss = 0.0007309028493799708
2236, epoch_train_loss=0.0007309028493799708
Epoch 2237
Epoch 2237 :: Batch 0/1
Batch Loss = 0.0007309028493799706
2237, epoch_train_loss=0.0007309028493799706
Epoch 2238
Epoch 2238 :: Batch 0/1
Batch Loss = 0.0007309028493799706
2238, epoch_train_loss=0.0007309028493799706
Epoch 2239
Epoch 2239 :: Batch 0/1
Batch Loss = 0.0007309028493799706
2239, epoch_train_loss=0.0007309028493799706
Epoch 2240
Epoch 2240 :: Batch 0/1
Batch Loss = 0.0007309028493799706
2240, epoch_train_loss=0.0007309028493799706
Epoch 2241
Epoch 2241 :: Batch 0/1
Batch Loss = 0.0007309028493799705
2241, epoch_train_loss=0.0007309028493799705
Epoch 2242
Epoch 2242 :: Batch 0/1
Batch Loss = 0.0007309028493799705
2242, epoch_train_loss=0.0007309028493799705
Epoch 2243
Epoch 2243 :: Batch 0/1
Batch Loss = 0.0007309028493799704
2243, epoch_train_loss=0.0007309028493799704
Epoch 2244
Epoch 2244 :: Batch 0/1
Batch Loss = 0.0007309028493799704
2244, epoch_train_loss=0.0007309028493799704
Epoch 2245
Epoch 2245 :: Batch 0/1
Batch Loss = 0.0007309028493799704
2245, epoch_train_loss=0.0007309028493799704
Epoch 2246
Epoch 2246 :: Batch 0/1
Batch Loss = 0.0007309028493799703
2246, epoch_train_loss=0.0007309028493799703
Epoch 2247
Epoch 2247 :: Batch 0/1
Batch Loss = 0.0007309028493799703
2247, epoch_train_loss=0.0007309028493799703
Epoch 2248
Epoch 2248 :: Batch 0/1
Batch Loss = 0.0007309028493799702
2248, epoch_train_loss=0.0007309028493799702
Epoch 2249
Epoch 2249 :: Batch 0/1
Batch Loss = 0.0007309028493799702
2249, epoch_train_loss=0.0007309028493799702
Epoch 2250
Epoch 2250 :: Batch 0/1
Batch Loss = 0.0007309028493799701
2250, epoch_train_loss=0.0007309028493799701
Epoch 2251
Epoch 2251 :: Batch 0/1
Batch Loss = 0.0007309028493799701
2251, epoch_train_loss=0.0007309028493799701
Epoch 2252
Epoch 2252 :: Batch 0/1
Batch Loss = 0.00073090284937997
2252, epoch_train_loss=0.00073090284937997
Epoch 2253
Epoch 2253 :: Batch 0/1
Batch Loss = 0.00073090284937997
2253, epoch_train_loss=0.00073090284937997
Epoch 2254
Epoch 2254 :: Batch 0/1
Batch Loss = 0.00073090284937997
2254, epoch_train_loss=0.00073090284937997
Epoch 2255
Epoch 2255 :: Batch 0/1
Batch Loss = 0.0007309028493799699
2255, epoch_train_loss=0.0007309028493799699
Epoch 2256
Epoch 2256 :: Batch 0/1
Batch Loss = 0.0007309028493799699
2256, epoch_train_loss=0.0007309028493799699
Epoch 2257
Epoch 2257 :: Batch 0/1
Batch Loss = 0.0007309028493799699
2257, epoch_train_loss=0.0007309028493799699
Epoch 2258
Epoch 2258 :: Batch 0/1
Batch Loss = 0.0007309028493799696
2258, epoch_train_loss=0.0007309028493799696
Epoch 2259
Epoch 2259 :: Batch 0/1
Batch Loss = 0.0007309028493799695
2259, epoch_train_loss=0.0007309028493799695
Epoch 2260
Epoch 2260 :: Batch 0/1
Batch Loss = 0.0007309028493799695
2260, epoch_train_loss=0.0007309028493799695
Epoch 2261
Epoch 2261 :: Batch 0/1
Batch Loss = 0.0007309028493799695
2261, epoch_train_loss=0.0007309028493799695
Epoch 2262
Epoch 2262 :: Batch 0/1
Batch Loss = 0.0007309028493799694
2262, epoch_train_loss=0.0007309028493799694
Epoch 2263
Epoch 2263 :: Batch 0/1
Batch Loss = 0.0007309028493799694
2263, epoch_train_loss=0.0007309028493799694
Epoch 2264
Epoch 2264 :: Batch 0/1
Batch Loss = 0.0007309028493799693
2264, epoch_train_loss=0.0007309028493799693
Epoch 2265
Epoch 2265 :: Batch 0/1
Batch Loss = 0.0007309028493799693
2265, epoch_train_loss=0.0007309028493799693
Epoch 2266
Epoch 2266 :: Batch 0/1
Batch Loss = 0.0007309028493799693
2266, epoch_train_loss=0.0007309028493799693
Epoch 2267
Epoch 2267 :: Batch 0/1
Batch Loss = 0.0007309028493799692
2267, epoch_train_loss=0.0007309028493799692
Epoch 2268
Epoch 2268 :: Batch 0/1
Batch Loss = 0.0007309028493799691
2268, epoch_train_loss=0.0007309028493799691
Epoch 2269
Epoch 2269 :: Batch 0/1
Batch Loss = 0.0007309028493799691
2269, epoch_train_loss=0.0007309028493799691
Epoch 2270
Epoch 2270 :: Batch 0/1
Batch Loss = 0.0007309028493799691
2270, epoch_train_loss=0.0007309028493799691
Epoch 2271
Epoch 2271 :: Batch 0/1
Batch Loss = 0.000730902849379969
2271, epoch_train_loss=0.000730902849379969
Epoch 2272
Epoch 2272 :: Batch 0/1
Batch Loss = 0.000730902849379969
2272, epoch_train_loss=0.000730902849379969
Epoch 2273
Epoch 2273 :: Batch 0/1
Batch Loss = 0.0007309028493799689
2273, epoch_train_loss=0.0007309028493799689
Epoch 2274
Epoch 2274 :: Batch 0/1
Batch Loss = 0.0007309028493799689
2274, epoch_train_loss=0.0007309028493799689
Epoch 2275
Epoch 2275 :: Batch 0/1
Batch Loss = 0.0007309028493799689
2275, epoch_train_loss=0.0007309028493799689
Epoch 2276
Epoch 2276 :: Batch 0/1
Batch Loss = 0.0007309028493799688
2276, epoch_train_loss=0.0007309028493799688
Epoch 2277
Epoch 2277 :: Batch 0/1
Batch Loss = 0.0007309028493799688
2277, epoch_train_loss=0.0007309028493799688
Epoch 2278
Epoch 2278 :: Batch 0/1
Batch Loss = 0.0007309028493799686
2278, epoch_train_loss=0.0007309028493799686
Epoch 2279
Epoch 2279 :: Batch 0/1
Batch Loss = 0.0007309028493799686
2279, epoch_train_loss=0.0007309028493799686
Epoch 2280
Epoch 2280 :: Batch 0/1
Batch Loss = 0.0007309028493799684
2280, epoch_train_loss=0.0007309028493799684
Epoch 2281
Epoch 2281 :: Batch 0/1
Batch Loss = 0.0007309028493799684
2281, epoch_train_loss=0.0007309028493799684
Epoch 2282
Epoch 2282 :: Batch 0/1
Batch Loss = 0.0007309028493799683
2282, epoch_train_loss=0.0007309028493799683
Epoch 2283
Epoch 2283 :: Batch 0/1
Batch Loss = 0.0007309028493799682
2283, epoch_train_loss=0.0007309028493799682
Epoch 2284
Epoch 2284 :: Batch 0/1
Batch Loss = 0.0007309028493799682
2284, epoch_train_loss=0.0007309028493799682
Epoch 2285
Epoch 2285 :: Batch 0/1
Batch Loss = 0.0007309028493799682
2285, epoch_train_loss=0.0007309028493799682
Epoch 2286
Epoch 2286 :: Batch 0/1
Batch Loss = 0.0007309028493799682
2286, epoch_train_loss=0.0007309028493799682
Epoch 2287
Epoch 2287 :: Batch 0/1
Batch Loss = 0.000730902849379968
2287, epoch_train_loss=0.000730902849379968
Epoch 2288
Epoch 2288 :: Batch 0/1
Batch Loss = 0.000730902849379968
2288, epoch_train_loss=0.000730902849379968
Epoch 2289
Epoch 2289 :: Batch 0/1
Batch Loss = 0.000730902849379968
2289, epoch_train_loss=0.000730902849379968
Epoch 2290
Epoch 2290 :: Batch 0/1
Batch Loss = 0.000730902849379968
2290, epoch_train_loss=0.000730902849379968
Epoch 2291
Epoch 2291 :: Batch 0/1
Batch Loss = 0.0007309028493799678
2291, epoch_train_loss=0.0007309028493799678
Epoch 2292
Epoch 2292 :: Batch 0/1
Batch Loss = 0.0007309028493799678
2292, epoch_train_loss=0.0007309028493799678
Epoch 2293
Epoch 2293 :: Batch 0/1
Batch Loss = 0.0007309028493799678
2293, epoch_train_loss=0.0007309028493799678
Epoch 2294
Epoch 2294 :: Batch 0/1
Batch Loss = 0.0007309028493799678
2294, epoch_train_loss=0.0007309028493799678
Epoch 2295
Epoch 2295 :: Batch 0/1
Batch Loss = 0.0007309028493799678
2295, epoch_train_loss=0.0007309028493799678
Epoch 2296
Epoch 2296 :: Batch 0/1
Batch Loss = 0.0007309028493799677
2296, epoch_train_loss=0.0007309028493799677
Epoch 2297
Epoch 2297 :: Batch 0/1
Batch Loss = 0.0007309028493799676
2297, epoch_train_loss=0.0007309028493799676
Epoch 2298
Epoch 2298 :: Batch 0/1
Batch Loss = 0.0007309028493799676
2298, epoch_train_loss=0.0007309028493799676
Epoch 2299
Epoch 2299 :: Batch 0/1
Batch Loss = 0.0007309028493799676
2299, epoch_train_loss=0.0007309028493799676
Epoch 2300
Epoch 2300 :: Batch 0/1
Batch Loss = 0.0007309028493799676
2300, epoch_train_loss=0.0007309028493799676
Epoch 2301
Epoch 2301 :: Batch 0/1
Batch Loss = 0.0007309028493799673
2301, epoch_train_loss=0.0007309028493799673
Epoch 2302
Epoch 2302 :: Batch 0/1
Batch Loss = 0.0007309028493799673
2302, epoch_train_loss=0.0007309028493799673
Epoch 2303
Epoch 2303 :: Batch 0/1
Batch Loss = 0.0007309028493799673
2303, epoch_train_loss=0.0007309028493799673
Epoch 2304
Epoch 2304 :: Batch 0/1
Batch Loss = 0.0007309028493799673
2304, epoch_train_loss=0.0007309028493799673
Epoch 2305
Epoch 2305 :: Batch 0/1
Batch Loss = 0.0007309028493799673
2305, epoch_train_loss=0.0007309028493799673
Epoch 2306
Epoch 2306 :: Batch 0/1
Batch Loss = 0.000730902849379967
2306, epoch_train_loss=0.000730902849379967
Epoch 2307
Epoch 2307 :: Batch 0/1
Batch Loss = 0.000730902849379967
2307, epoch_train_loss=0.000730902849379967
Epoch 2308
Epoch 2308 :: Batch 0/1
Batch Loss = 0.000730902849379967
2308, epoch_train_loss=0.000730902849379967
Epoch 2309
Epoch 2309 :: Batch 0/1
Batch Loss = 0.000730902849379967
2309, epoch_train_loss=0.000730902849379967
Epoch 2310
Epoch 2310 :: Batch 0/1
Batch Loss = 0.0007309028493799668
2310, epoch_train_loss=0.0007309028493799668
Epoch 2311
Epoch 2311 :: Batch 0/1
Batch Loss = 0.0007309028493799668
2311, epoch_train_loss=0.0007309028493799668
Epoch 2312
Epoch 2312 :: Batch 0/1
Batch Loss = 0.0007309028493799668
2312, epoch_train_loss=0.0007309028493799668
Epoch 2313
Epoch 2313 :: Batch 0/1
Batch Loss = 0.0007309028493799667
2313, epoch_train_loss=0.0007309028493799667
Epoch 2314
Epoch 2314 :: Batch 0/1
Batch Loss = 0.0007309028493799667
2314, epoch_train_loss=0.0007309028493799667
Epoch 2315
Epoch 2315 :: Batch 0/1
Batch Loss = 0.0007309028493799666
2315, epoch_train_loss=0.0007309028493799666
Epoch 2316
Epoch 2316 :: Batch 0/1
Batch Loss = 0.0007309028493799666
2316, epoch_train_loss=0.0007309028493799666
Epoch 2317
Epoch 2317 :: Batch 0/1
Batch Loss = 0.0007309028493799666
2317, epoch_train_loss=0.0007309028493799666
Epoch 2318
Epoch 2318 :: Batch 0/1
Batch Loss = 0.0007309028493799665
2318, epoch_train_loss=0.0007309028493799665
Epoch 2319
Epoch 2319 :: Batch 0/1
Batch Loss = 0.0007309028493799663
2319, epoch_train_loss=0.0007309028493799663
Epoch 2320
Epoch 2320 :: Batch 0/1
Batch Loss = 0.0007309028493799663
2320, epoch_train_loss=0.0007309028493799663
Epoch 2321
Epoch 2321 :: Batch 0/1
Batch Loss = 0.0007309028493799663
2321, epoch_train_loss=0.0007309028493799663
Epoch 2322
Epoch 2322 :: Batch 0/1
Batch Loss = 0.0007309028493799662
2322, epoch_train_loss=0.0007309028493799662
Epoch 2323
Epoch 2323 :: Batch 0/1
Batch Loss = 0.0007309028493799661
2323, epoch_train_loss=0.0007309028493799661
Epoch 2324
Epoch 2324 :: Batch 0/1
Batch Loss = 0.0007309028493799661
2324, epoch_train_loss=0.0007309028493799661
Epoch 2325
Epoch 2325 :: Batch 0/1
Batch Loss = 0.0007309028493799661
2325, epoch_train_loss=0.0007309028493799661
Epoch 2326
Epoch 2326 :: Batch 0/1
Batch Loss = 0.0007309028493799661
2326, epoch_train_loss=0.0007309028493799661
Epoch 2327
Epoch 2327 :: Batch 0/1
Batch Loss = 0.000730902849379966
2327, epoch_train_loss=0.000730902849379966
Epoch 2328
Epoch 2328 :: Batch 0/1
Batch Loss = 0.0007309028493799658
2328, epoch_train_loss=0.0007309028493799658
Epoch 2329
Epoch 2329 :: Batch 0/1
Batch Loss = 0.0007309028493799658
2329, epoch_train_loss=0.0007309028493799658
Epoch 2330
Epoch 2330 :: Batch 0/1
Batch Loss = 0.0007309028493799658
2330, epoch_train_loss=0.0007309028493799658
Epoch 2331
Epoch 2331 :: Batch 0/1
Batch Loss = 0.0007309028493799658
2331, epoch_train_loss=0.0007309028493799658
Epoch 2332
Epoch 2332 :: Batch 0/1
Batch Loss = 0.0007309028493799656
2332, epoch_train_loss=0.0007309028493799656
Epoch 2333
Epoch 2333 :: Batch 0/1
Batch Loss = 0.0007309028493799656
2333, epoch_train_loss=0.0007309028493799656
Epoch 2334
Epoch 2334 :: Batch 0/1
Batch Loss = 0.0007309028493799656
2334, epoch_train_loss=0.0007309028493799656
Epoch 2335
Epoch 2335 :: Batch 0/1
Batch Loss = 0.0007309028493799655
2335, epoch_train_loss=0.0007309028493799655
Epoch 2336
Epoch 2336 :: Batch 0/1
Batch Loss = 0.0007309028493799654
2336, epoch_train_loss=0.0007309028493799654
Epoch 2337
Epoch 2337 :: Batch 0/1
Batch Loss = 0.0007309028493799654
2337, epoch_train_loss=0.0007309028493799654
Epoch 2338
Epoch 2338 :: Batch 0/1
Batch Loss = 0.0007309028493799654
2338, epoch_train_loss=0.0007309028493799654
Epoch 2339
Epoch 2339 :: Batch 0/1
Batch Loss = 0.0007309028493799654
2339, epoch_train_loss=0.0007309028493799654
Epoch 2340
Epoch 2340 :: Batch 0/1
Batch Loss = 0.0007309028493799654
2340, epoch_train_loss=0.0007309028493799654
Epoch 2341
Epoch 2341 :: Batch 0/1
Batch Loss = 0.0007309028493799651
2341, epoch_train_loss=0.0007309028493799651
Epoch 2342
Epoch 2342 :: Batch 0/1
Batch Loss = 0.0007309028493799651
2342, epoch_train_loss=0.0007309028493799651
Epoch 2343
Epoch 2343 :: Batch 0/1
Batch Loss = 0.0007309028493799651
2343, epoch_train_loss=0.0007309028493799651
Epoch 2344
Epoch 2344 :: Batch 0/1
Batch Loss = 0.000730902849379965
2344, epoch_train_loss=0.000730902849379965
Epoch 2345
Epoch 2345 :: Batch 0/1
Batch Loss = 0.000730902849379965
2345, epoch_train_loss=0.000730902849379965
Epoch 2346
Epoch 2346 :: Batch 0/1
Batch Loss = 0.0007309028493799649
2346, epoch_train_loss=0.0007309028493799649
Epoch 2347
Epoch 2347 :: Batch 0/1
Batch Loss = 0.0007309028493799649
2347, epoch_train_loss=0.0007309028493799649
Epoch 2348
Epoch 2348 :: Batch 0/1
Batch Loss = 0.0007309028493799648
2348, epoch_train_loss=0.0007309028493799648
Epoch 2349
Epoch 2349 :: Batch 0/1
Batch Loss = 0.0007309028493799648
2349, epoch_train_loss=0.0007309028493799648
Epoch 2350
Epoch 2350 :: Batch 0/1
Batch Loss = 0.0007309028493799647
2350, epoch_train_loss=0.0007309028493799647
Epoch 2351
Epoch 2351 :: Batch 0/1
Batch Loss = 0.0007309028493799647
2351, epoch_train_loss=0.0007309028493799647
Epoch 2352
Epoch 2352 :: Batch 0/1
Batch Loss = 0.0007309028493799645
2352, epoch_train_loss=0.0007309028493799645
Epoch 2353
Epoch 2353 :: Batch 0/1
Batch Loss = 0.0007309028493799645
2353, epoch_train_loss=0.0007309028493799645
Epoch 2354
Epoch 2354 :: Batch 0/1
Batch Loss = 0.0007309028493799645
2354, epoch_train_loss=0.0007309028493799645
Epoch 2355
Epoch 2355 :: Batch 0/1
Batch Loss = 0.0007309028493799644
2355, epoch_train_loss=0.0007309028493799644
Epoch 2356
Epoch 2356 :: Batch 0/1
Batch Loss = 0.0007309028493799644
2356, epoch_train_loss=0.0007309028493799644
Epoch 2357
Epoch 2357 :: Batch 0/1
Batch Loss = 0.0007309028493799643
2357, epoch_train_loss=0.0007309028493799643
Epoch 2358
Epoch 2358 :: Batch 0/1
Batch Loss = 0.0007309028493799643
2358, epoch_train_loss=0.0007309028493799643
Epoch 2359
Epoch 2359 :: Batch 0/1
Batch Loss = 0.0007309028493799642
2359, epoch_train_loss=0.0007309028493799642
Epoch 2360
Epoch 2360 :: Batch 0/1
Batch Loss = 0.000730902849379964
2360, epoch_train_loss=0.000730902849379964
Epoch 2361
Epoch 2361 :: Batch 0/1
Batch Loss = 0.000730902849379964
2361, epoch_train_loss=0.000730902849379964
Epoch 2362
Epoch 2362 :: Batch 0/1
Batch Loss = 0.000730902849379964
2362, epoch_train_loss=0.000730902849379964
Epoch 2363
Epoch 2363 :: Batch 0/1
Batch Loss = 0.0007309028493799639
2363, epoch_train_loss=0.0007309028493799639
Epoch 2364
Epoch 2364 :: Batch 0/1
Batch Loss = 0.0007309028493799639
2364, epoch_train_loss=0.0007309028493799639
Epoch 2365
Epoch 2365 :: Batch 0/1
Batch Loss = 0.0007309028493799638
2365, epoch_train_loss=0.0007309028493799638
Epoch 2366
Epoch 2366 :: Batch 0/1
Batch Loss = 0.0007309028493799638
2366, epoch_train_loss=0.0007309028493799638
Epoch 2367
Epoch 2367 :: Batch 0/1
Batch Loss = 0.0007309028493799638
2367, epoch_train_loss=0.0007309028493799638
Epoch 2368
Epoch 2368 :: Batch 0/1
Batch Loss = 0.0007309028493799637
2368, epoch_train_loss=0.0007309028493799637
Epoch 2369
Epoch 2369 :: Batch 0/1
Batch Loss = 0.0007309028493799637
2369, epoch_train_loss=0.0007309028493799637
Epoch 2370
Epoch 2370 :: Batch 0/1
Batch Loss = 0.0007309028493799636
2370, epoch_train_loss=0.0007309028493799636
Epoch 2371
Epoch 2371 :: Batch 0/1
Batch Loss = 0.0007309028493799635
2371, epoch_train_loss=0.0007309028493799635
Epoch 2372
Epoch 2372 :: Batch 0/1
Batch Loss = 0.0007309028493799635
2372, epoch_train_loss=0.0007309028493799635
Epoch 2373
Epoch 2373 :: Batch 0/1
Batch Loss = 0.0007309028493799635
2373, epoch_train_loss=0.0007309028493799635
Epoch 2374
Epoch 2374 :: Batch 0/1
Batch Loss = 0.0007309028493799634
2374, epoch_train_loss=0.0007309028493799634
Epoch 2375
Epoch 2375 :: Batch 0/1
Batch Loss = 0.0007309028493799634
2375, epoch_train_loss=0.0007309028493799634
Epoch 2376
Epoch 2376 :: Batch 0/1
Batch Loss = 0.0007309028493799632
2376, epoch_train_loss=0.0007309028493799632
Epoch 2377
Epoch 2377 :: Batch 0/1
Batch Loss = 0.0007309028493799632
2377, epoch_train_loss=0.0007309028493799632
Epoch 2378
Epoch 2378 :: Batch 0/1
Batch Loss = 0.0007309028493799632
2378, epoch_train_loss=0.0007309028493799632
Epoch 2379
Epoch 2379 :: Batch 0/1
Batch Loss = 0.0007309028493799631
2379, epoch_train_loss=0.0007309028493799631
Epoch 2380
Epoch 2380 :: Batch 0/1
Batch Loss = 0.0007309028493799631
2380, epoch_train_loss=0.0007309028493799631
Epoch 2381
Epoch 2381 :: Batch 0/1
Batch Loss = 0.000730902849379963
2381, epoch_train_loss=0.000730902849379963
Epoch 2382
Epoch 2382 :: Batch 0/1
Batch Loss = 0.000730902849379963
2382, epoch_train_loss=0.000730902849379963
Epoch 2383
Epoch 2383 :: Batch 0/1
Batch Loss = 0.0007309028493799628
2383, epoch_train_loss=0.0007309028493799628
Epoch 2384
Epoch 2384 :: Batch 0/1
Batch Loss = 0.0007309028493799627
2384, epoch_train_loss=0.0007309028493799627
Epoch 2385
Epoch 2385 :: Batch 0/1
Batch Loss = 0.0007309028493799627
2385, epoch_train_loss=0.0007309028493799627
Epoch 2386
Epoch 2386 :: Batch 0/1
Batch Loss = 0.0007309028493799627
2386, epoch_train_loss=0.0007309028493799627
Epoch 2387
Epoch 2387 :: Batch 0/1
Batch Loss = 0.0007309028493799627
2387, epoch_train_loss=0.0007309028493799627
Epoch 2388
Epoch 2388 :: Batch 0/1
Batch Loss = 0.0007309028493799625
2388, epoch_train_loss=0.0007309028493799625
Epoch 2389
Epoch 2389 :: Batch 0/1
Batch Loss = 0.0007309028493799625
2389, epoch_train_loss=0.0007309028493799625
Epoch 2390
Epoch 2390 :: Batch 0/1
Batch Loss = 0.0007309028493799625
2390, epoch_train_loss=0.0007309028493799625
Epoch 2391
Epoch 2391 :: Batch 0/1
Batch Loss = 0.0007309028493799625
2391, epoch_train_loss=0.0007309028493799625
Epoch 2392
Epoch 2392 :: Batch 0/1
Batch Loss = 0.0007309028493799624
2392, epoch_train_loss=0.0007309028493799624
Epoch 2393
Epoch 2393 :: Batch 0/1
Batch Loss = 0.0007309028493799623
2393, epoch_train_loss=0.0007309028493799623
Epoch 2394
Epoch 2394 :: Batch 0/1
Batch Loss = 0.0007309028493799623
2394, epoch_train_loss=0.0007309028493799623
Epoch 2395
Epoch 2395 :: Batch 0/1
Batch Loss = 0.0007309028493799623
2395, epoch_train_loss=0.0007309028493799623
Epoch 2396
Epoch 2396 :: Batch 0/1
Batch Loss = 0.0007309028493799623
2396, epoch_train_loss=0.0007309028493799623
Epoch 2397
Epoch 2397 :: Batch 0/1
Batch Loss = 0.0007309028493799622
2397, epoch_train_loss=0.0007309028493799622
Epoch 2398
Epoch 2398 :: Batch 0/1
Batch Loss = 0.000730902849379962
2398, epoch_train_loss=0.000730902849379962
Epoch 2399
Epoch 2399 :: Batch 0/1
Batch Loss = 0.000730902849379962
2399, epoch_train_loss=0.000730902849379962
Epoch 2400
Epoch 2400 :: Batch 0/1
Batch Loss = 0.000730902849379962
2400, epoch_train_loss=0.000730902849379962
Epoch 2401
Epoch 2401 :: Batch 0/1
Batch Loss = 0.0007309028493799617
2401, epoch_train_loss=0.0007309028493799617
Epoch 2402
Epoch 2402 :: Batch 0/1
Batch Loss = 0.0007309028493799617
2402, epoch_train_loss=0.0007309028493799617
Epoch 2403
Epoch 2403 :: Batch 0/1
Batch Loss = 0.0007309028493799617
2403, epoch_train_loss=0.0007309028493799617
Epoch 2404
Epoch 2404 :: Batch 0/1
Batch Loss = 0.0007309028493799617
2404, epoch_train_loss=0.0007309028493799617
Epoch 2405
Epoch 2405 :: Batch 0/1
Batch Loss = 0.0007309028493799615
2405, epoch_train_loss=0.0007309028493799615
Epoch 2406
Epoch 2406 :: Batch 0/1
Batch Loss = 0.0007309028493799615
2406, epoch_train_loss=0.0007309028493799615
Epoch 2407
Epoch 2407 :: Batch 0/1
Batch Loss = 0.0007309028493799615
2407, epoch_train_loss=0.0007309028493799615
Epoch 2408
Epoch 2408 :: Batch 0/1
Batch Loss = 0.0007309028493799615
2408, epoch_train_loss=0.0007309028493799615
Epoch 2409
Epoch 2409 :: Batch 0/1
Batch Loss = 0.0007309028493799614
2409, epoch_train_loss=0.0007309028493799614
Epoch 2410
Epoch 2410 :: Batch 0/1
Batch Loss = 0.0007309028493799613
2410, epoch_train_loss=0.0007309028493799613
Epoch 2411
Epoch 2411 :: Batch 0/1
Batch Loss = 0.0007309028493799613
2411, epoch_train_loss=0.0007309028493799613
Epoch 2412
Epoch 2412 :: Batch 0/1
Batch Loss = 0.0007309028493799613
2412, epoch_train_loss=0.0007309028493799613
Epoch 2413
Epoch 2413 :: Batch 0/1
Batch Loss = 0.0007309028493799613
2413, epoch_train_loss=0.0007309028493799613
Epoch 2414
Epoch 2414 :: Batch 0/1
Batch Loss = 0.0007309028493799611
2414, epoch_train_loss=0.0007309028493799611
Epoch 2415
Epoch 2415 :: Batch 0/1
Batch Loss = 0.0007309028493799611
2415, epoch_train_loss=0.0007309028493799611
Epoch 2416
Epoch 2416 :: Batch 0/1
Batch Loss = 0.0007309028493799611
2416, epoch_train_loss=0.0007309028493799611
Epoch 2417
Epoch 2417 :: Batch 0/1
Batch Loss = 0.0007309028493799611
2417, epoch_train_loss=0.0007309028493799611
Epoch 2418
Epoch 2418 :: Batch 0/1
Batch Loss = 0.0007309028493799609
2418, epoch_train_loss=0.0007309028493799609
Epoch 2419
Epoch 2419 :: Batch 0/1
Batch Loss = 0.0007309028493799609
2419, epoch_train_loss=0.0007309028493799609
Epoch 2420
Epoch 2420 :: Batch 0/1
Batch Loss = 0.0007309028493799609
2420, epoch_train_loss=0.0007309028493799609
Epoch 2421
Epoch 2421 :: Batch 0/1
Batch Loss = 0.0007309028493799609
2421, epoch_train_loss=0.0007309028493799609
Epoch 2422
Epoch 2422 :: Batch 0/1
Batch Loss = 0.0007309028493799606
2422, epoch_train_loss=0.0007309028493799606
Epoch 2423
Epoch 2423 :: Batch 0/1
Batch Loss = 0.0007309028493799605
2423, epoch_train_loss=0.0007309028493799605
Epoch 2424
Epoch 2424 :: Batch 0/1
Batch Loss = 0.0007309028493799605
2424, epoch_train_loss=0.0007309028493799605
Epoch 2425
Epoch 2425 :: Batch 0/1
Batch Loss = 0.0007309028493799605
2425, epoch_train_loss=0.0007309028493799605
Epoch 2426
Epoch 2426 :: Batch 0/1
Batch Loss = 0.0007309028493799603
2426, epoch_train_loss=0.0007309028493799603
Epoch 2427
Epoch 2427 :: Batch 0/1
Batch Loss = 0.0007309028493799603
2427, epoch_train_loss=0.0007309028493799603
Epoch 2428
Epoch 2428 :: Batch 0/1
Batch Loss = 0.0007309028493799603
2428, epoch_train_loss=0.0007309028493799603
Epoch 2429
Epoch 2429 :: Batch 0/1
Batch Loss = 0.0007309028493799603
2429, epoch_train_loss=0.0007309028493799603
Epoch 2430
Epoch 2430 :: Batch 0/1
Batch Loss = 0.0007309028493799602
2430, epoch_train_loss=0.0007309028493799602
Epoch 2431
Epoch 2431 :: Batch 0/1
Batch Loss = 0.0007309028493799601
2431, epoch_train_loss=0.0007309028493799601
Epoch 2432
Epoch 2432 :: Batch 0/1
Batch Loss = 0.0007309028493799601
2432, epoch_train_loss=0.0007309028493799601
Epoch 2433
Epoch 2433 :: Batch 0/1
Batch Loss = 0.0007309028493799601
2433, epoch_train_loss=0.0007309028493799601
Epoch 2434
Epoch 2434 :: Batch 0/1
Batch Loss = 0.00073090284937996
2434, epoch_train_loss=0.00073090284937996
Epoch 2435
Epoch 2435 :: Batch 0/1
Batch Loss = 0.0007309028493799599
2435, epoch_train_loss=0.0007309028493799599
Epoch 2436
Epoch 2436 :: Batch 0/1
Batch Loss = 0.0007309028493799599
2436, epoch_train_loss=0.0007309028493799599
Epoch 2437
Epoch 2437 :: Batch 0/1
Batch Loss = 0.0007309028493799599
2437, epoch_train_loss=0.0007309028493799599
Epoch 2438
Epoch 2438 :: Batch 0/1
Batch Loss = 0.0007309028493799598
2438, epoch_train_loss=0.0007309028493799598
Epoch 2439
Epoch 2439 :: Batch 0/1
Batch Loss = 0.0007309028493799597
2439, epoch_train_loss=0.0007309028493799597
Epoch 2440
Epoch 2440 :: Batch 0/1
Batch Loss = 0.0007309028493799597
2440, epoch_train_loss=0.0007309028493799597
Epoch 2441
Epoch 2441 :: Batch 0/1
Batch Loss = 0.0007309028493799597
2441, epoch_train_loss=0.0007309028493799597
Epoch 2442
Epoch 2442 :: Batch 0/1
Batch Loss = 0.0007309028493799594
2442, epoch_train_loss=0.0007309028493799594
Epoch 2443
Epoch 2443 :: Batch 0/1
Batch Loss = 0.0007309028493799593
2443, epoch_train_loss=0.0007309028493799593
Epoch 2444
Epoch 2444 :: Batch 0/1
Batch Loss = 0.0007309028493799593
2444, epoch_train_loss=0.0007309028493799593
Epoch 2445
Epoch 2445 :: Batch 0/1
Batch Loss = 0.0007309028493799593
2445, epoch_train_loss=0.0007309028493799593
Epoch 2446
Epoch 2446 :: Batch 0/1
Batch Loss = 0.0007309028493799592
2446, epoch_train_loss=0.0007309028493799592
Epoch 2447
Epoch 2447 :: Batch 0/1
Batch Loss = 0.0007309028493799592
2447, epoch_train_loss=0.0007309028493799592
Epoch 2448
Epoch 2448 :: Batch 0/1
Batch Loss = 0.0007309028493799591
2448, epoch_train_loss=0.0007309028493799591
Epoch 2449
Epoch 2449 :: Batch 0/1
Batch Loss = 0.0007309028493799591
2449, epoch_train_loss=0.0007309028493799591
Epoch 2450
Epoch 2450 :: Batch 0/1
Batch Loss = 0.000730902849379959
2450, epoch_train_loss=0.000730902849379959
Epoch 2451
Epoch 2451 :: Batch 0/1
Batch Loss = 0.000730902849379959
2451, epoch_train_loss=0.000730902849379959
Epoch 2452
Epoch 2452 :: Batch 0/1
Batch Loss = 0.0007309028493799589
2452, epoch_train_loss=0.0007309028493799589
Epoch 2453
Epoch 2453 :: Batch 0/1
Batch Loss = 0.0007309028493799589
2453, epoch_train_loss=0.0007309028493799589
Epoch 2454
Epoch 2454 :: Batch 0/1
Batch Loss = 0.0007309028493799588
2454, epoch_train_loss=0.0007309028493799588
Epoch 2455
Epoch 2455 :: Batch 0/1
Batch Loss = 0.0007309028493799588
2455, epoch_train_loss=0.0007309028493799588
Epoch 2456
Epoch 2456 :: Batch 0/1
Batch Loss = 0.0007309028493799588
2456, epoch_train_loss=0.0007309028493799588
Epoch 2457
Epoch 2457 :: Batch 0/1
Batch Loss = 0.0007309028493799587
2457, epoch_train_loss=0.0007309028493799587
Epoch 2458
Epoch 2458 :: Batch 0/1
Batch Loss = 0.0007309028493799587
2458, epoch_train_loss=0.0007309028493799587
Epoch 2459
Epoch 2459 :: Batch 0/1
Batch Loss = 0.0007309028493799584
2459, epoch_train_loss=0.0007309028493799584
Epoch 2460
Epoch 2460 :: Batch 0/1
Batch Loss = 0.0007309028493799584
2460, epoch_train_loss=0.0007309028493799584
Epoch 2461
Epoch 2461 :: Batch 0/1
Batch Loss = 0.0007309028493799584
2461, epoch_train_loss=0.0007309028493799584
Epoch 2462
Epoch 2462 :: Batch 0/1
Batch Loss = 0.0007309028493799584
2462, epoch_train_loss=0.0007309028493799584
Epoch 2463
Epoch 2463 :: Batch 0/1
Batch Loss = 0.0007309028493799583
2463, epoch_train_loss=0.0007309028493799583
Epoch 2464
Epoch 2464 :: Batch 0/1
Batch Loss = 0.0007309028493799581
2464, epoch_train_loss=0.0007309028493799581
Epoch 2465
Epoch 2465 :: Batch 0/1
Batch Loss = 0.0007309028493799581
2465, epoch_train_loss=0.0007309028493799581
Epoch 2466
Epoch 2466 :: Batch 0/1
Batch Loss = 0.0007309028493799581
2466, epoch_train_loss=0.0007309028493799581
Epoch 2467
Epoch 2467 :: Batch 0/1
Batch Loss = 0.000730902849379958
2467, epoch_train_loss=0.000730902849379958
Epoch 2468
Epoch 2468 :: Batch 0/1
Batch Loss = 0.000730902849379958
2468, epoch_train_loss=0.000730902849379958
Epoch 2469
Epoch 2469 :: Batch 0/1
Batch Loss = 0.0007309028493799579
2469, epoch_train_loss=0.0007309028493799579
Epoch 2470
Epoch 2470 :: Batch 0/1
Batch Loss = 0.0007309028493799578
2470, epoch_train_loss=0.0007309028493799578
Epoch 2471
Epoch 2471 :: Batch 0/1
Batch Loss = 0.0007309028493799577
2471, epoch_train_loss=0.0007309028493799577
Epoch 2472
Epoch 2472 :: Batch 0/1
Batch Loss = 0.0007309028493799577
2472, epoch_train_loss=0.0007309028493799577
Epoch 2473
Epoch 2473 :: Batch 0/1
Batch Loss = 0.0007309028493799577
2473, epoch_train_loss=0.0007309028493799577
Epoch 2474
Epoch 2474 :: Batch 0/1
Batch Loss = 0.0007309028493799577
2474, epoch_train_loss=0.0007309028493799577
Epoch 2475
Epoch 2475 :: Batch 0/1
Batch Loss = 0.0007309028493799576
2475, epoch_train_loss=0.0007309028493799576
Epoch 2476
Epoch 2476 :: Batch 0/1
Batch Loss = 0.0007309028493799575
2476, epoch_train_loss=0.0007309028493799575
Epoch 2477
Epoch 2477 :: Batch 0/1
Batch Loss = 0.0007309028493799575
2477, epoch_train_loss=0.0007309028493799575
Epoch 2478
Epoch 2478 :: Batch 0/1
Batch Loss = 0.0007309028493799575
2478, epoch_train_loss=0.0007309028493799575
Epoch 2479
Epoch 2479 :: Batch 0/1
Batch Loss = 0.0007309028493799574
2479, epoch_train_loss=0.0007309028493799574
Epoch 2480
Epoch 2480 :: Batch 0/1
Batch Loss = 0.0007309028493799572
2480, epoch_train_loss=0.0007309028493799572
Epoch 2481
Epoch 2481 :: Batch 0/1
Batch Loss = 0.0007309028493799572
2481, epoch_train_loss=0.0007309028493799572
Epoch 2482
Epoch 2482 :: Batch 0/1
Batch Loss = 0.0007309028493799572
2482, epoch_train_loss=0.0007309028493799572
Epoch 2483
Epoch 2483 :: Batch 0/1
Batch Loss = 0.0007309028493799571
2483, epoch_train_loss=0.0007309028493799571
Epoch 2484
Epoch 2484 :: Batch 0/1
Batch Loss = 0.000730902849379957
2484, epoch_train_loss=0.000730902849379957
Epoch 2485
Epoch 2485 :: Batch 0/1
Batch Loss = 0.000730902849379957
2485, epoch_train_loss=0.000730902849379957
Epoch 2486
Epoch 2486 :: Batch 0/1
Batch Loss = 0.000730902849379957
2486, epoch_train_loss=0.000730902849379957
Epoch 2487
Epoch 2487 :: Batch 0/1
Batch Loss = 0.0007309028493799568
2487, epoch_train_loss=0.0007309028493799568
Epoch 2488
Epoch 2488 :: Batch 0/1
Batch Loss = 0.0007309028493799568
2488, epoch_train_loss=0.0007309028493799568
Epoch 2489
Epoch 2489 :: Batch 0/1
Batch Loss = 0.0007309028493799567
2489, epoch_train_loss=0.0007309028493799567
Epoch 2490
Epoch 2490 :: Batch 0/1
Batch Loss = 0.0007309028493799567
2490, epoch_train_loss=0.0007309028493799567
Epoch 2491
Epoch 2491 :: Batch 0/1
Batch Loss = 0.0007309028493799567
2491, epoch_train_loss=0.0007309028493799567
Epoch 2492
Epoch 2492 :: Batch 0/1
Batch Loss = 0.0007309028493799566
2492, epoch_train_loss=0.0007309028493799566
Epoch 2493
Epoch 2493 :: Batch 0/1
Batch Loss = 0.0007309028493799565
2493, epoch_train_loss=0.0007309028493799565
Epoch 2494
Epoch 2494 :: Batch 0/1
Batch Loss = 0.0007309028493799565
2494, epoch_train_loss=0.0007309028493799565
Epoch 2495
Epoch 2495 :: Batch 0/1
Batch Loss = 0.0007309028493799564
2495, epoch_train_loss=0.0007309028493799564
Epoch 2496
Epoch 2496 :: Batch 0/1
Batch Loss = 0.0007309028493799563
2496, epoch_train_loss=0.0007309028493799563
Epoch 2497
Epoch 2497 :: Batch 0/1
Batch Loss = 0.0007309028493799563
2497, epoch_train_loss=0.0007309028493799563
Epoch 2498
Epoch 2498 :: Batch 0/1
Batch Loss = 0.0007309028493799563
2498, epoch_train_loss=0.0007309028493799563
Epoch 2499
Epoch 2499 :: Batch 0/1
Batch Loss = 0.000730902849379956
2499, epoch_train_loss=0.000730902849379956
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting "B3LYP_WITH_VWN5 = True" in pyscf_conf.py
  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802e680> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802e680> in UKS object of <class 'pyscf.dft.uks.UKS'>
WARNING: External module "mldftdat" required for non-local descriptor use.
<pyscf.gto.mole.Mole object at 0x7ffeb802e680> [['P', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb802caf0> [['N', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb802e740> [['H', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb802e8c0> [['Li', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb802e350> [['O', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb802ead0> [['Cl', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb802f8b0> [['Al', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb802d480> [['S', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb802ec80> [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb802e2f0> [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb802e4a0> [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb802e1a0> [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb802d9c0> [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb802ddb0> [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb802ce50> [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb802cd30> [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb802cb50> [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb802d030> [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb802f2e0> [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb802c070> [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb802dae0> [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb802f850> [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb802f550> [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb802e320> [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb802ee30> [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]] 5
<pyscf.gto.mole.Mole object at 0x7ffeb802dd20> [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb802fdc0> [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]] 5
mol:  [['P', array([0., 0., 0.])]]
converged SCF energy = -341.104145992717  <S^2> = 3.7502984  2S+1 = 4.0001492
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802caf0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802caf0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-3.48053232e-03 -9.22981806e-04 -2.09507924e-03 ... -1.11294850e+01
 -1.11294850e+01 -1.11294850e+01] = SCAN,
rho_a.shape=(6, 6320), rho_b.shape=(6, 6320)
fxc_a.shape=(6320,), fxc_b.shape=(6320,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12640), fxc.shape=(12640,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(2, 6320, 15)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12640, 15)
rho_filt.shape=(12640,)
mol:  [['N', array([0., 0., 0.])]]
converged SCF energy = -54.5289742046674  <S^2> = 3.7524945  2S+1 = 4.0012471
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802e740> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802e740> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5016, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.10256797e-03 -5.98013179e-04 -6.71209617e-05 ... -5.03581543e+00
 -5.03581543e+00 -5.03581543e+00] = SCAN,
rho_a.shape=(6, 5016), rho_b.shape=(6, 5016)
fxc_a.shape=(5016,), fxc_b.shape=(5016,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 10032), fxc.shape=(10032,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5016, 22)
rho shape (4, 4, 2, 5016)
rho_filt shape: (5016,)
get descriptors tdrho.shape=(2, 5016, 15)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(10032, 15)
rho_filt.shape=(10032,)
mol:  [['H', array([0., 0., 0.])]]
converged SCF energy = -0.499812984008539  <S^2> = 0.75  2S+1 = 2
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802e8c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802e8c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 7, 7)
ao.shape (10, 2440, 7)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.59173730e-03 -7.67300285e-04 -6.09330303e-05 ... -7.27736608e-01
 -7.27736608e-01 -7.27736608e-01] = SCAN,
rho_a.shape=(6, 2440), rho_b.shape=(6, 2440)
fxc_a.shape=(2440,), fxc_b.shape=(2440,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 2440), fxc.shape=(2440,)
get_data, dm shape = (2, 7, 7)
ao_eval.shape=(4, 2440, 7)
rho shape (4, 4, 2, 2440)
rho_filt shape: (2440,)
get descriptors tdrho.shape=(2, 2440, 15)
mol:  [['Li', array([0., 0., 0.])]]
converged SCF energy = -7.46006188627842  <S^2> = 0.75000049  2S+1 = 2.0000005
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802e350> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802e350> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 4592, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-6.71507910e-03 -1.45299376e-03 -1.45299376e-03 ... -1.46930969e-02
 -2.05021258e+00 -2.05021258e+00] = SCAN,
rho_a.shape=(6, 4592), rho_b.shape=(6, 4592)
fxc_a.shape=(4592,), fxc_b.shape=(4592,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 9184), fxc.shape=(9184,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 4592, 22)
rho shape (4, 4, 2, 4592)
rho_filt shape: (4592,)
get descriptors tdrho.shape=(2, 4592, 15)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(9184, 15)
rho_filt.shape=(9184,)
mol:  [['O', array([0., 0., 0.])]]
converged SCF energy = -75.0033774512088  <S^2> = 2.0027452  2S+1 = 3.0018296
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802ead0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802ead0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5040, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-7.08362723e-04 -1.23159784e-04 -6.18173540e-06 ... -5.78388652e+00
 -5.78388652e+00 -5.78388652e+00] = SCAN,
rho_a.shape=(6, 5040), rho_b.shape=(6, 5040)
fxc_a.shape=(5040,), fxc_b.shape=(5040,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 10080), fxc.shape=(10080,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5040, 22)
rho shape (4, 4, 2, 5040)
rho_filt shape: (5040,)
get descriptors tdrho.shape=(2, 5040, 15)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(10080, 15)
rho_filt.shape=(10080,)
mol:  [['Cl', array([0., 0., 0.])]]
converged SCF energy = -459.957577122153  <S^2> = 0.7516194  2S+1 = 2.0016187
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802f8b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802f8b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6152, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.70217241e-04 -1.00412833e-03 -3.55938008e-04 ... -1.26646370e+01
 -1.26646370e+01 -1.26646370e+01] = SCAN,
rho_a.shape=(6, 6152), rho_b.shape=(6, 6152)
fxc_a.shape=(6152,), fxc_b.shape=(6152,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12304), fxc.shape=(12304,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6152, 30)
rho shape (4, 4, 2, 6152)
rho_filt shape: (6152,)
get descriptors tdrho.shape=(2, 6152, 15)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12304, 15)
rho_filt.shape=(12304,)
mol:  [['Al', array([0., 0., 0.])]]
converged SCF energy = -242.226560989229  <S^2> = 0.75226414  2S+1 = 2.0022629
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802d480> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802d480> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6088, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.39564124e-02 -8.69586045e-03 -4.30171315e-03 ... -1.39781083e-04
 -1.04899399e-03 -7.75352742e-05] = SCAN,
rho_a.shape=(6, 6088), rho_b.shape=(6, 6088)
fxc_a.shape=(6088,), fxc_b.shape=(6088,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12176), fxc.shape=(12176,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6088, 30)
rho shape (4, 4, 2, 6088)
rho_filt shape: (6088,)
get descriptors tdrho.shape=(2, 6088, 15)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12176, 15)
rho_filt.shape=(12176,)
mol:  [['S', array([0., 0., 0.])]]
converged SCF energy = -397.938786809801  <S^2> = 2.0022329  2S+1 = 3.0014882
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802ec80> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802ec80> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.22130556e-03 -9.10168990e-04 -9.93795884e-04 ... -1.18982463e+01
 -1.18982463e+01 -1.18982463e+01] = SCAN,
rho_a.shape=(6, 6320), rho_b.shape=(6, 6320)
fxc_a.shape=(6320,), fxc_b.shape=(6320,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12640), fxc.shape=(12640,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(2, 6320, 15)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12640, 15)
rho_filt.shape=(12640,)
mol:  [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]]
converged SCF energy = -1.16580491182912  <S^2> = -4.4408921e-16  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802e2f0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802e2f0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 14, 14)
ao.shape (10, 4776, 14)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.31556415e-04 -9.72662915e-06 -3.66768501e-04 ... -5.54165573e-01
 -5.54165573e-01 -5.54165573e-01] = SCAN,
rho_a.shape=(6, 4776), rho_b.shape=(6, 4776)
fxc_a.shape=(4776,), fxc_b.shape=(4776,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 4776), fxc.shape=(4776,)
get_data, dm shape = (2, 14, 14)
ao_eval.shape=(4, 4776, 14)
rho shape (4, 4, 2, 4776)
rho_filt shape: (4776,)
get descriptors tdrho.shape=(2, 4776, 15)
mol:  [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]]
converged SCF energy = -109.439263799566  <S^2> = 1.3322676e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802e4a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802e4a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9848, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-9.68469910e-05 -9.84742277e-04 -2.59676148e-04 ... -2.39626668e-05
 -2.39626668e-05 -9.68469910e-05] = SCAN,
rho_a.shape=(6, 9848), rho_b.shape=(6, 9848)
fxc_a.shape=(9848,), fxc_b.shape=(9848,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9848), fxc.shape=(9848,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9848, 44)
rho shape (4, 4, 2, 9848)
rho_filt shape: (9848,)
get descriptors tdrho.shape=(2, 9848, 15)
mol:  [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]]
converged SCF energy = -107.339357395734  <S^2> = 8.8817842e-15  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802e1a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802e1a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9752, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.04987750e-03 -6.68953858e-04 -8.57556270e-04 ... -1.07485583e-03
 -8.01425698e-01 -8.01425698e-01] = SCAN,
rho_a.shape=(6, 9752), rho_b.shape=(6, 9752)
fxc_a.shape=(9752,), fxc_b.shape=(9752,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9752), fxc.shape=(9752,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9752, 44)
rho shape (4, 4, 2, 9752)
rho_filt shape: (9752,)
get descriptors tdrho.shape=(2, 9752, 15)
mol:  [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]]
converged SCF energy = -93.3377924465129  <S^2> = 4.0072923e-10  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802d9c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802d9c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 12256, 51)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.97917285e-04 -2.54412366e-05 -3.15182243e-05 ... -6.37386500e-01
 -6.37386500e-01 -6.37386500e-01] = SCAN,
rho_a.shape=(6, 12256), rho_b.shape=(6, 12256)
fxc_a.shape=(12256,), fxc_b.shape=(12256,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12256), fxc.shape=(12256,)
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 12256, 51)
rho shape (4, 4, 2, 12256)
rho_filt shape: (12256,)
get descriptors tdrho.shape=(2, 12256, 15)
mol:  [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]]
converged SCF energy = -188.456965322844  <S^2> = 2.1316282e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802ddb0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802ddb0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 66, 66)
ao.shape (10, 14920, 66)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.50217115e-04 -2.07520066e-04 -9.23619896e-04 ... -2.74295208e-06
 -4.27559894e+00 -4.27559894e+00] = SCAN,
rho_a.shape=(6, 14920), rho_b.shape=(6, 14920)
fxc_a.shape=(14920,), fxc_b.shape=(14920,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 14920), fxc.shape=(14920,)
get_data, dm shape = (2, 66, 66)
ao_eval.shape=(4, 14920, 66)
rho shape (4, 4, 2, 14920)
rho_filt shape: (14920,)
get descriptors tdrho.shape=(2, 14920, 15)
mol:  [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]]
converged SCF energy = -920.00560888896  <S^2> = 4.9737992e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802ce50> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802ce50> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12208, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00155834 -0.00091688 -0.00215831 ... -0.00091688 -0.41618507
 -0.41618507] = SCAN,
rho_a.shape=(6, 12208), rho_b.shape=(6, 12208)
fxc_a.shape=(12208,), fxc_b.shape=(12208,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12208), fxc.shape=(12208,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12208, 60)
rho shape (4, 4, 2, 12208)
rho_filt shape: (12208,)
get descriptors tdrho.shape=(2, 12208, 15)
mol:  [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]]
converged SCF energy = -199.394370591172  <S^2> = 1.2612134e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802cd30> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802cd30> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9824, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.92948614e-04 -1.95198688e-05 -1.16699802e-03 ... -4.89378340e-01
 -4.89378340e-01 -4.89378340e-01] = SCAN,
rho_a.shape=(6, 9824), rho_b.shape=(6, 9824)
fxc_a.shape=(9824,), fxc_b.shape=(9824,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9824), fxc.shape=(9824,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9824, 44)
rho shape (4, 4, 2, 9824)
rho_filt shape: (9824,)
get descriptors tdrho.shape=(2, 9824, 15)
mol:  [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]]
converged SCF energy = -150.214894518297  <S^2> = 1.0018599  2S+1 = 2.2377309
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802cb50> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802cb50> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9912, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-6.70678771e-04 -1.18855892e-04 -6.10662494e-06 ... -6.59150577e-01
 -6.59150577e-01 -6.59150577e-01] = SCAN,
rho_a.shape=(6, 9912), rho_b.shape=(6, 9912)
fxc_a.shape=(9912,), fxc_b.shape=(9912,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9912), fxc.shape=(9912,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9912, 44)
rho shape (4, 4, 2, 9912)
rho_filt shape: (9912,)
get descriptors tdrho.shape=(2, 9912, 15)
mol:  [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]]
converged SCF energy = -77.2435048346373  <S^2> = 1.4210855e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802d030> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802d030> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 58, 58)
ao.shape (10, 15208, 58)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.83270456e-05 -8.83270456e-05 -9.75839850e-04 ... -3.46719667e-05
 -3.31708644e-05 -3.31708644e-05] = SCAN,
rho_a.shape=(6, 15208), rho_b.shape=(6, 15208)
fxc_a.shape=(15208,), fxc_b.shape=(15208,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 15208), fxc.shape=(15208,)
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15208, 58)
rho shape (4, 4, 2, 15208)
rho_filt shape: (15208,)
get descriptors tdrho.shape=(2, 15208, 15)
mol:  [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]]
converged SCF energy = -113.221335689652  <S^2> = 6.5547567e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802f2e0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802f2e0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 10040, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-5.37000578e-04 -8.55494549e-04 -2.46853288e-03 ... -7.34251999e-01
 -7.34251999e-01 -7.34251999e-01] = SCAN,
rho_a.shape=(6, 10040), rho_b.shape=(6, 10040)
fxc_a.shape=(10040,), fxc_b.shape=(10040,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 10040), fxc.shape=(10040,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 10040, 44)
rho shape (4, 4, 2, 10040)
rho_filt shape: (10040,)
get descriptors tdrho.shape=(2, 10040, 15)
mol:  [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]]
converged SCF energy = -460.624592374078  <S^2> = 6.2172489e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802c070> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802c070> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 37, 37)
ao.shape (10, 8552, 37)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.38161177e-04 -1.81188367e-05 -2.37300299e-05 ... -2.83738108e+00
 -2.83738108e+00 -2.83738108e+00] = SCAN,
rho_a.shape=(6, 8552), rho_b.shape=(6, 8552)
fxc_a.shape=(8552,), fxc_b.shape=(8552,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 8552), fxc.shape=(8552,)
get_data, dm shape = (2, 37, 37)
ao_eval.shape=(4, 8552, 37)
rho shape (4, 4, 2, 8552)
rho_filt shape: (8552,)
get descriptors tdrho.shape=(2, 8552, 15)
mol:  [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]]
converged SCF energy = -8.04458854018922  <S^2> = 7.9047879e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802dae0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802dae0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 29, 29)
ao.shape (10, 6936, 29)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00113445 -0.00118624 -0.00118624 ... -0.48434639 -0.48434639
 -0.48434639] = SCAN,
rho_a.shape=(6, 6936), rho_b.shape=(6, 6936)
fxc_a.shape=(6936,), fxc_b.shape=(6936,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 6936), fxc.shape=(6936,)
get_data, dm shape = (2, 29, 29)
ao_eval.shape=(4, 6936, 29)
rho shape (4, 4, 2, 6936)
rho_filt shape: (6936,)
get descriptors tdrho.shape=(2, 6936, 15)
mol:  [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]]
converged SCF energy = -324.340512506578  <S^2> = 1.5864643e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802f850> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802f850> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 11536, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00297935 -0.00297935 -0.00407089 ... -0.00297935 -0.00297935
 -0.00407089] = SCAN,
rho_a.shape=(6, 11536), rho_b.shape=(6, 11536)
fxc_a.shape=(11536,), fxc_b.shape=(11536,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 11536), fxc.shape=(11536,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 11536, 60)
rho shape (4, 4, 2, 11536)
rho_filt shape: (11536,)
get descriptors tdrho.shape=(2, 11536, 15)
mol:  [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]]
converged SCF energy = -1622.57507845814  <S^2> = 8.31335e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802f550> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802f550> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 120, 120)
ao.shape (10, 24512, 120)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.61400996e-04 -4.90484900e-04 -2.56451718e-03 ... -9.59296113e+00
 -9.59296113e+00 -9.59296113e+00] = SCAN,
rho_a.shape=(6, 24512), rho_b.shape=(6, 24512)
fxc_a.shape=(24512,), fxc_b.shape=(24512,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 24512), fxc.shape=(24512,)
get_data, dm shape = (2, 120, 120)
ao_eval.shape=(4, 24512, 120)
rho shape (4, 4, 2, 24512)
rho_filt shape: (24512,)
get descriptors tdrho.shape=(2, 24512, 15)
mol:  [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]]
converged SCF energy = -342.979728469575  <S^2> = 2.5394797e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802e320> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802e320> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 13096, 51)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.28637920e-03 -4.32383300e-04 -3.74057272e-05 ... -1.91722770e+00
 -1.91722770e+00 -1.91722770e+00] = SCAN,
rho_a.shape=(6, 13096), rho_b.shape=(6, 13096)
fxc_a.shape=(13096,), fxc_b.shape=(13096,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 13096), fxc.shape=(13096,)
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 13096, 51)
rho shape (4, 4, 2, 13096)
rho_filt shape: (13096,)
get descriptors tdrho.shape=(2, 13096, 15)
mol:  [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]]
converged SCF energy = -578.565336626331  <S^2> = 1.0034707  2S+1 = 2.2391701
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802ee30> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802ee30> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12384, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.59951545e-04 -2.60672288e-04 -2.59277799e-04 ... -3.86943697e-01
 -3.86943697e-01 -3.86943697e-01] = SCAN,
rho_a.shape=(6, 12384), rho_b.shape=(6, 12384)
fxc_a.shape=(12384,), fxc_b.shape=(12384,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12384), fxc.shape=(12384,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12384, 60)
rho shape (4, 4, 2, 12384)
rho_filt shape: (12384,)
get descriptors tdrho.shape=(2, 12384, 15)
mol:  [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]]
converged SCF energy = -40.4598214864077  <S^2> = 3.1885605e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802dd20> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802dd20> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 50, 50)
ao.shape (10, 13936, 50)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.68439986e-04 -2.42462569e-04 -1.69927031e-05 ... -2.55230307e-05
 -2.55230307e-05 -2.55230307e-05] = SCAN,
rho_a.shape=(6, 13936), rho_b.shape=(6, 13936)
fxc_a.shape=(13936,), fxc_b.shape=(13936,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 13936), fxc.shape=(13936,)
get_data, dm shape = (2, 50, 50)
ao_eval.shape=(4, 13936, 50)
rho shape (4, 4, 2, 13936)
rho_filt shape: (13936,)
get descriptors tdrho.shape=(2, 13936, 15)
mol:  [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]]
converged SCF energy = -39.0756147483504  <S^2> = 6.1972649e-12  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802fdc0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802fdc0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 36, 36)
ao.shape (10, 9656, 36)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-7.67688751e-04 -4.57393214e-05 -2.02834191e-04 ... -1.14928924e+00
 -1.14928924e+00 -1.14928924e+00] = SCAN,
rho_a.shape=(6, 9656), rho_b.shape=(6, 9656)
fxc_a.shape=(9656,), fxc_b.shape=(9656,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9656), fxc.shape=(9656,)
get_data, dm shape = (2, 36, 36)
ao_eval.shape=(4, 9656, 36)
rho shape (4, 4, 2, 9656)
rho_filt shape: (9656,)
get descriptors tdrho.shape=(2, 9656, 15)
mol:  [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]]
converged SCF energy = -291.719272437819  <S^2> = 1.315037e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
New DM shape: (2, 58, 58)
ao.shape (10, 15256, 58)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.33850535e-04 -2.34903029e-04 -1.75623665e-05 ... -1.92891112e-05
 -1.92891112e-05 -1.92891112e-05] = SCAN,
rho_a.shape=(6, 15256), rho_b.shape=(6, 15256)
fxc_a.shape=(15256,), fxc_b.shape=(15256,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 15256), fxc.shape=(15256,)
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15256, 58)
rho shape (4, 4, 2, 15256)
rho_filt shape: (15256,)
get descriptors tdrho.shape=(2, 15256, 15)
localnet.spin_scaling: concatenating the data
first data shape = (10940, 15)
concatenated: tdrho.shape=(271719, 15)
PRE NAN FILT: tFxc.shape=(271719,), tdrho.shape=(271719, 15)
nan_filt_rho.shape=(271719,)
nan_filt_fxc.shape=(271719,)
tFxc.shape=(271719,), tdrho.shape=(271719, 15)
inp[0].shape = (271719, 15)
Epoch 0
Epoch 0 :: Batch 0/1
Batch Loss = 68545.19586576002
0, epoch_train_loss=68545.19586576002
Epoch 1
Epoch 1 :: Batch 0/1
Batch Loss = 1099979120.247849
1, epoch_train_loss=1099979120.247849
Epoch 2
Epoch 2 :: Batch 0/1
Batch Loss = 20846047.544015013
2, epoch_train_loss=20846047.544015013
Epoch 3
Epoch 3 :: Batch 0/1
Batch Loss = 37552277.50978654
3, epoch_train_loss=37552277.50978654
Epoch 4
Epoch 4 :: Batch 0/1
Batch Loss = 19650842.766405094
4, epoch_train_loss=19650842.766405094
Epoch 5
Epoch 5 :: Batch 0/1
Batch Loss = 6853089.7562551005
5, epoch_train_loss=6853089.7562551005
Epoch 6
Epoch 6 :: Batch 0/1
Batch Loss = 43990.78300337571
6, epoch_train_loss=43990.78300337571
Epoch 7
Epoch 7 :: Batch 0/1
Batch Loss = 684065.1899510405
7, epoch_train_loss=684065.1899510405
Epoch 8
Epoch 8 :: Batch 0/1
Batch Loss = 1719051.2879120235
8, epoch_train_loss=1719051.2879120235
Epoch 9
Epoch 9 :: Batch 0/1
Batch Loss = 881852.8977181717
9, epoch_train_loss=881852.8977181717
Epoch 10
Epoch 10 :: Batch 0/1
Batch Loss = 2840863.315076747
10, epoch_train_loss=2840863.315076747
Epoch 11
Epoch 11 :: Batch 0/1
Batch Loss = 80434.9315663787
11, epoch_train_loss=80434.9315663787
Epoch 12
Epoch 12 :: Batch 0/1
Batch Loss = 455264.5412772914
12, epoch_train_loss=455264.5412772914
Epoch 13
Epoch 13 :: Batch 0/1
Batch Loss = 448643.32234372426
13, epoch_train_loss=448643.32234372426
Epoch 14
Epoch 14 :: Batch 0/1
Batch Loss = 337341.5371944519
14, epoch_train_loss=337341.5371944519
Epoch 15
Epoch 15 :: Batch 0/1
Batch Loss = 183807.41873269135
15, epoch_train_loss=183807.41873269135
Epoch 16
Epoch 16 :: Batch 0/1
Batch Loss = 83577.06976967587
16, epoch_train_loss=83577.06976967587
Epoch 17
Epoch 17 :: Batch 0/1
Batch Loss = 23878.48292315991
17, epoch_train_loss=23878.48292315991
Epoch 18
Epoch 18 :: Batch 0/1
Batch Loss = 321.58326814281685
18, epoch_train_loss=321.58326814281685
Epoch 19
Epoch 19 :: Batch 0/1
Batch Loss = 12.966134399031976
19, epoch_train_loss=12.966134399031976
Epoch 20
Epoch 20 :: Batch 0/1
Batch Loss = 1.3265029931881853
20, epoch_train_loss=1.3265029931881853
Epoch 21
Epoch 21 :: Batch 0/1
Batch Loss = 1.4002594736137601
21, epoch_train_loss=1.4002594736137601
Epoch 22
Epoch 22 :: Batch 0/1
Batch Loss = 1.3613849412746102
22, epoch_train_loss=1.3613849412746102
Epoch 23
Epoch 23 :: Batch 0/1
Batch Loss = 1.456599238346084
23, epoch_train_loss=1.456599238346084
Epoch 24
Epoch 24 :: Batch 0/1
Batch Loss = 1.4598073314041153
24, epoch_train_loss=1.4598073314041153
Epoch 25
Epoch 25 :: Batch 0/1
Batch Loss = 1.4351131260418286
25, epoch_train_loss=1.4351131260418286
Epoch 26
Epoch 26 :: Batch 0/1
Batch Loss = 1.470252248920372
26, epoch_train_loss=1.470252248920372
Epoch 27
Epoch 27 :: Batch 0/1
Batch Loss = 1.4711539807633656
27, epoch_train_loss=1.4711539807633656
Epoch 28
Epoch 28 :: Batch 0/1
Batch Loss = 1.5513238138468681
28, epoch_train_loss=1.5513238138468681
Epoch 29
Epoch 29 :: Batch 0/1
Batch Loss = 1.4314256935073006
29, epoch_train_loss=1.4314256935073006
Epoch 30
Epoch 30 :: Batch 0/1
Batch Loss = 4.603274794753906
30, epoch_train_loss=4.603274794753906
Epoch 31
Epoch 31 :: Batch 0/1
Batch Loss = 175.76745520658093
31, epoch_train_loss=175.76745520658093
Epoch 32
Epoch 32 :: Batch 0/1
Batch Loss = 1.4694140722114915
32, epoch_train_loss=1.4694140722114915
Epoch 33
Epoch 33 :: Batch 0/1
Batch Loss = 1.479717956236247
33, epoch_train_loss=1.479717956236247
Epoch 34
Epoch 34 :: Batch 0/1
Batch Loss = 1.4740471868729252
34, epoch_train_loss=1.4740471868729252
Epoch 35
Epoch 35 :: Batch 0/1
Batch Loss = 1.4525701321175049
35, epoch_train_loss=1.4525701321175049
Epoch 36
Epoch 36 :: Batch 0/1
Batch Loss = 1.43743738853391
36, epoch_train_loss=1.43743738853391
Epoch 37
Epoch 37 :: Batch 0/1
Batch Loss = 1.4317480530567033
37, epoch_train_loss=1.4317480530567033
Epoch 38
Epoch 38 :: Batch 0/1
Batch Loss = 1.418272960933289
38, epoch_train_loss=1.418272960933289
Epoch 39
Epoch 39 :: Batch 0/1
Batch Loss = 1.3946156071849336
39, epoch_train_loss=1.3946156071849336
Epoch 40
Epoch 40 :: Batch 0/1
Batch Loss = 1.3728610352194435
40, epoch_train_loss=1.3728610352194435
Epoch 41
Epoch 41 :: Batch 0/1
Batch Loss = 3.3733032528924394
41, epoch_train_loss=3.3733032528924394
Epoch 42
Epoch 42 :: Batch 0/1
Batch Loss = 1.650429727460614
42, epoch_train_loss=1.650429727460614
Epoch 43
Epoch 43 :: Batch 0/1
Batch Loss = 2.232292918366002
43, epoch_train_loss=2.232292918366002
Epoch 44
Epoch 44 :: Batch 0/1
Batch Loss = 2.229032860533508
44, epoch_train_loss=2.229032860533508
Epoch 45
Epoch 45 :: Batch 0/1
Batch Loss = 1.8521394669447109
45, epoch_train_loss=1.8521394669447109
Epoch 46
Epoch 46 :: Batch 0/1
Batch Loss = 1.4276232923638879
46, epoch_train_loss=1.4276232923638879
Epoch 47
Epoch 47 :: Batch 0/1
Batch Loss = 1.2746561946488588
47, epoch_train_loss=1.2746561946488588
Epoch 48
Epoch 48 :: Batch 0/1
Batch Loss = 1.4750971883249033
48, epoch_train_loss=1.4750971883249033
Epoch 49
Epoch 49 :: Batch 0/1
Batch Loss = 1.7179084867723524
49, epoch_train_loss=1.7179084867723524
Epoch 50
Epoch 50 :: Batch 0/1
Batch Loss = 1.3825518018385736
50, epoch_train_loss=1.3825518018385736
Epoch 51
Epoch 51 :: Batch 0/1
Batch Loss = 1.854259600139774
51, epoch_train_loss=1.854259600139774
Epoch 52
Epoch 52 :: Batch 0/1
Batch Loss = 1.6480871270718422
52, epoch_train_loss=1.6480871270718422
Epoch 53
Epoch 53 :: Batch 0/1
Batch Loss = 1.3240496005179456
53, epoch_train_loss=1.3240496005179456
Epoch 54
Epoch 54 :: Batch 0/1
Batch Loss = 1.1917971688248372
54, epoch_train_loss=1.1917971688248372
Epoch 55
Epoch 55 :: Batch 0/1
Batch Loss = 1.272756986403936
55, epoch_train_loss=1.272756986403936
Epoch 56
Epoch 56 :: Batch 0/1
Batch Loss = 1.414660451840911
56, epoch_train_loss=1.414660451840911
Epoch 57
Epoch 57 :: Batch 0/1
Batch Loss = 1.4732207716650318
57, epoch_train_loss=1.4732207716650318
Epoch 58
Epoch 58 :: Batch 0/1
Batch Loss = 1.4022024999288893
58, epoch_train_loss=1.4022024999288893
Epoch 59
Epoch 59 :: Batch 0/1
Batch Loss = 1.2502356821936855
59, epoch_train_loss=1.2502356821936855
Epoch 60
Epoch 60 :: Batch 0/1
Batch Loss = 1.1182278171292177
60, epoch_train_loss=1.1182278171292177
Epoch 61
Epoch 61 :: Batch 0/1
Batch Loss = 1.0926194348240197
61, epoch_train_loss=1.0926194348240197
Epoch 62
Epoch 62 :: Batch 0/1
Batch Loss = 1.162725070503097
62, epoch_train_loss=1.162725070503097
Epoch 63
Epoch 63 :: Batch 0/1
Batch Loss = 1.2199556092315278
63, epoch_train_loss=1.2199556092315278
Epoch 64
Epoch 64 :: Batch 0/1
Batch Loss = 1.176239505417801
64, epoch_train_loss=1.176239505417801
Epoch 65
Epoch 65 :: Batch 0/1
Batch Loss = 1.0673762647126415
65, epoch_train_loss=1.0673762647126415
Epoch 66
Epoch 66 :: Batch 0/1
Batch Loss = 0.9920933026537855
66, epoch_train_loss=0.9920933026537855
Epoch 67
Epoch 67 :: Batch 0/1
Batch Loss = 0.9928211364419717
67, epoch_train_loss=0.9928211364419717
Epoch 68
Epoch 68 :: Batch 0/1
Batch Loss = 1.030672740798662
68, epoch_train_loss=1.030672740798662
Epoch 69
Epoch 69 :: Batch 0/1
Batch Loss = 1.041689900451841
69, epoch_train_loss=1.041689900451841
Epoch 70
Epoch 70 :: Batch 0/1
Batch Loss = 0.9987775874696976
70, epoch_train_loss=0.9987775874696976
Epoch 71
Epoch 71 :: Batch 0/1
Batch Loss = 0.9286988738598514
71, epoch_train_loss=0.9286988738598514
Epoch 72
Epoch 72 :: Batch 0/1
Batch Loss = 0.8833868018682097
72, epoch_train_loss=0.8833868018682097
Epoch 73
Epoch 73 :: Batch 0/1
Batch Loss = 0.8859268307064475
73, epoch_train_loss=0.8859268307064475
Epoch 74
Epoch 74 :: Batch 0/1
Batch Loss = 0.9028527095197223
74, epoch_train_loss=0.9028527095197223
Epoch 75
Epoch 75 :: Batch 0/1
Batch Loss = 79.97247736777028
75, epoch_train_loss=79.97247736777028
Epoch 76
Epoch 76 :: Batch 0/1
Batch Loss = 0.8384470736736459
76, epoch_train_loss=0.8384470736736459
Epoch 77
Epoch 77 :: Batch 0/1
Batch Loss = 0.8029762814172741
77, epoch_train_loss=0.8029762814172741
Epoch 78
Epoch 78 :: Batch 0/1
Batch Loss = 0.8062328527548641
78, epoch_train_loss=0.8062328527548641
Epoch 79
Epoch 79 :: Batch 0/1
Batch Loss = 0.8211559312724955
79, epoch_train_loss=0.8211559312724955
Epoch 80
Epoch 80 :: Batch 0/1
Batch Loss = 0.8148521344528471
80, epoch_train_loss=0.8148521344528471
Epoch 81
Epoch 81 :: Batch 0/1
Batch Loss = 0.7856554198983076
81, epoch_train_loss=0.7856554198983076
Epoch 82
Epoch 82 :: Batch 0/1
Batch Loss = 0.7655344781067493
82, epoch_train_loss=0.7655344781067493
Epoch 83
Epoch 83 :: Batch 0/1
Batch Loss = 0.7706681058055694
83, epoch_train_loss=0.7706681058055694
Epoch 84
Epoch 84 :: Batch 0/1
Batch Loss = 0.7764258695706177
84, epoch_train_loss=0.7764258695706177
Epoch 85
Epoch 85 :: Batch 0/1
Batch Loss = 0.7595091478663204
85, epoch_train_loss=0.7595091478663204
Epoch 86
Epoch 86 :: Batch 0/1
Batch Loss = 0.7333902177736783
86, epoch_train_loss=0.7333902177736783
Epoch 87
Epoch 87 :: Batch 0/1
Batch Loss = 0.723713469161862
87, epoch_train_loss=0.723713469161862
Epoch 88
Epoch 88 :: Batch 0/1
Batch Loss = 0.7257449064379183
88, epoch_train_loss=0.7257449064379183
Epoch 89
Epoch 89 :: Batch 0/1
Batch Loss = 0.7150822687664947
89, epoch_train_loss=0.7150822687664947
Epoch 90
Epoch 90 :: Batch 0/1
Batch Loss = 0.6920389249253188
90, epoch_train_loss=0.6920389249253188
Epoch 91
Epoch 91 :: Batch 0/1
Batch Loss = 0.6765336920360302
91, epoch_train_loss=0.6765336920360302
Epoch 92
Epoch 92 :: Batch 0/1
Batch Loss = 0.6721329799163375
92, epoch_train_loss=0.6721329799163375
Epoch 93
Epoch 93 :: Batch 0/1
Batch Loss = 0.6606366841129714
93, epoch_train_loss=0.6606366841129714
Epoch 94
Epoch 94 :: Batch 0/1
Batch Loss = 0.6387103395917312
94, epoch_train_loss=0.6387103395917312
Epoch 95
Epoch 95 :: Batch 0/1
Batch Loss = 0.622729156759351
95, epoch_train_loss=0.622729156759351
Epoch 96
Epoch 96 :: Batch 0/1
Batch Loss = 0.6146907120050479
96, epoch_train_loss=0.6146907120050479
Epoch 97
Epoch 97 :: Batch 0/1
Batch Loss = 0.6011844471914021
97, epoch_train_loss=0.6011844471914021
Epoch 98
Epoch 98 :: Batch 0/1
Batch Loss = 0.5808520581577167
98, epoch_train_loss=0.5808520581577167
Epoch 99
Epoch 99 :: Batch 0/1
Batch Loss = 0.5660718776385654
99, epoch_train_loss=0.5660718776385654
Epoch 100
Epoch 100 :: Batch 0/1
Batch Loss = 0.5561884127634681
100, epoch_train_loss=0.5561884127634681
Epoch 101
Epoch 101 :: Batch 0/1
Batch Loss = 0.540533833221549
101, epoch_train_loss=0.540533833221549
Epoch 102
Epoch 102 :: Batch 0/1
Batch Loss = 0.5224188101718251
102, epoch_train_loss=0.5224188101718251
Epoch 103
Epoch 103 :: Batch 0/1
Batch Loss = 0.5102209808008024
103, epoch_train_loss=0.5102209808008024
Epoch 104
Epoch 104 :: Batch 0/1
Batch Loss = 0.49856242518684746
104, epoch_train_loss=0.49856242518684746
Epoch 105
Epoch 105 :: Batch 0/1
Batch Loss = 0.4818100825568505
105, epoch_train_loss=0.4818100825568505
Epoch 106
Epoch 106 :: Batch 0/1
Batch Loss = 0.46705053636545035
106, epoch_train_loss=0.46705053636545035
Epoch 107
Epoch 107 :: Batch 0/1
Batch Loss = 0.45607852185690506
107, epoch_train_loss=0.45607852185690506
Epoch 108
Epoch 108 :: Batch 0/1
Batch Loss = 0.4419030670661225
108, epoch_train_loss=0.4419030670661225
Epoch 109
Epoch 109 :: Batch 0/1
Batch Loss = 0.4267895474950783
109, epoch_train_loss=0.4267895474950783
Epoch 110
Epoch 110 :: Batch 0/1
Batch Loss = 0.4155717938975717
110, epoch_train_loss=0.4155717938975717
Epoch 111
Epoch 111 :: Batch 0/1
Batch Loss = 0.4032016444568332
111, epoch_train_loss=0.4032016444568332
Epoch 112
Epoch 112 :: Batch 0/1
Batch Loss = 0.3888584833866506
112, epoch_train_loss=0.3888584833866506
Epoch 113
Epoch 113 :: Batch 0/1
Batch Loss = 0.3775919819910136
113, epoch_train_loss=0.3775919819910136
Epoch 114
Epoch 114 :: Batch 0/1
Batch Loss = 0.3663023880094236
114, epoch_train_loss=0.3663023880094236
Epoch 115
Epoch 115 :: Batch 0/1
Batch Loss = 0.35339136889430056
115, epoch_train_loss=0.35339136889430056
Epoch 116
Epoch 116 :: Batch 0/1
Batch Loss = 0.3429656769789783
116, epoch_train_loss=0.3429656769789783
Epoch 117
Epoch 117 :: Batch 0/1
Batch Loss = 0.3325146226473088
117, epoch_train_loss=0.3325146226473088
Epoch 118
Epoch 118 :: Batch 0/1
Batch Loss = 0.32097701334775136
118, epoch_train_loss=0.32097701334775136
Epoch 119
Epoch 119 :: Batch 0/1
Batch Loss = 0.3115996204457277
119, epoch_train_loss=0.3115996204457277
Epoch 120
Epoch 120 :: Batch 0/1
Batch Loss = 0.30173545488226755
120, epoch_train_loss=0.30173545488226755
Epoch 121
Epoch 121 :: Batch 0/1
Batch Loss = 0.291665760847419
121, epoch_train_loss=0.291665760847419
Epoch 122
Epoch 122 :: Batch 0/1
Batch Loss = 0.28343980823889564
122, epoch_train_loss=0.28343980823889564
Epoch 123
Epoch 123 :: Batch 0/1
Batch Loss = 0.2743656369668593
123, epoch_train_loss=0.2743656369668593
Epoch 124
Epoch 124 :: Batch 0/1
Batch Loss = 0.26600692034036283
124, epoch_train_loss=0.26600692034036283
Epoch 125
Epoch 125 :: Batch 0/1
Batch Loss = 0.2585589984376459
125, epoch_train_loss=0.2585589984376459
Epoch 126
Epoch 126 :: Batch 0/1
Batch Loss = 0.25063833729204305
126, epoch_train_loss=0.25063833729204305
Epoch 127
Epoch 127 :: Batch 0/1
Batch Loss = 0.2440735199483813
127, epoch_train_loss=0.2440735199483813
Epoch 128
Epoch 128 :: Batch 0/1
Batch Loss = 0.23719721436552052
128, epoch_train_loss=0.23719721436552052
Epoch 129
Epoch 129 :: Batch 0/1
Batch Loss = 0.23091878926968798
129, epoch_train_loss=0.23091878926968798
Epoch 130
Epoch 130 :: Batch 0/1
Batch Loss = 0.22525290825355915
130, epoch_train_loss=0.22525290825355915
Epoch 131
Epoch 131 :: Batch 0/1
Batch Loss = 0.2193936809235803
131, epoch_train_loss=0.2193936809235803
Epoch 132
Epoch 132 :: Batch 0/1
Batch Loss = 0.21456682674508396
132, epoch_train_loss=0.21456682674508396
Epoch 133
Epoch 133 :: Batch 0/1
Batch Loss = 0.20932320403491492
133, epoch_train_loss=0.20932320403491492
Epoch 134
Epoch 134 :: Batch 0/1
Batch Loss = 0.2049019852709274
134, epoch_train_loss=0.2049019852709274
Epoch 135
Epoch 135 :: Batch 0/1
Batch Loss = 0.2004407157052098
135, epoch_train_loss=0.2004407157052098
Epoch 136
Epoch 136 :: Batch 0/1
Batch Loss = 0.19635365646145528
136, epoch_train_loss=0.19635365646145528
Epoch 137
Epoch 137 :: Batch 0/1
Batch Loss = 0.1925922502866246
137, epoch_train_loss=0.1925922502866246
Epoch 138
Epoch 138 :: Batch 0/1
Batch Loss = 0.18881145181707512
138, epoch_train_loss=0.18881145181707512
Epoch 139
Epoch 139 :: Batch 0/1
Batch Loss = 0.18554669984209451
139, epoch_train_loss=0.18554669984209451
Epoch 140
Epoch 140 :: Batch 0/1
Batch Loss = 0.18210258866666715
140, epoch_train_loss=0.18210258866666715
Epoch 141
Epoch 141 :: Batch 0/1
Batch Loss = 0.1791783760135564
141, epoch_train_loss=0.1791783760135564
Epoch 142
Epoch 142 :: Batch 0/1
Batch Loss = 0.17604385514236734
142, epoch_train_loss=0.17604385514236734
Epoch 143
Epoch 143 :: Batch 0/1
Batch Loss = 0.17333502390599237
143, epoch_train_loss=0.17333502390599237
Epoch 144
Epoch 144 :: Batch 0/1
Batch Loss = 0.17046574941538617
144, epoch_train_loss=0.17046574941538617
Epoch 145
Epoch 145 :: Batch 0/1
Batch Loss = 0.167934512056858
145, epoch_train_loss=0.167934512056858
Epoch 146
Epoch 146 :: Batch 0/1
Batch Loss = 0.16531876733053136
146, epoch_train_loss=0.16531876733053136
Epoch 147
Epoch 147 :: Batch 0/1
Batch Loss = 0.16299430045077518
147, epoch_train_loss=0.16299430045077518
Epoch 148
Epoch 148 :: Batch 0/1
Batch Loss = 0.16060598713221375
148, epoch_train_loss=0.16060598713221375
Epoch 149
Epoch 149 :: Batch 0/1
Batch Loss = 0.1584552876221671
149, epoch_train_loss=0.1584552876221671
Epoch 150
Epoch 150 :: Batch 0/1
Batch Loss = 0.15623326924612832
150, epoch_train_loss=0.15623326924612832
Epoch 151
Epoch 151 :: Batch 0/1
Batch Loss = 0.15424880880606182
151, epoch_train_loss=0.15424880880606182
Epoch 152
Epoch 152 :: Batch 0/1
Batch Loss = 0.1522194106805223
152, epoch_train_loss=0.1522194106805223
Epoch 153
Epoch 153 :: Batch 0/1
Batch Loss = 0.1503897676199676
153, epoch_train_loss=0.1503897676199676
Epoch 154
Epoch 154 :: Batch 0/1
Batch Loss = 0.14851739215604676
154, epoch_train_loss=0.14851739215604676
Epoch 155
Epoch 155 :: Batch 0/1
Batch Loss = 0.14677982112470514
155, epoch_train_loss=0.14677982112470514
Epoch 156
Epoch 156 :: Batch 0/1
Batch Loss = 0.1450250603599733
156, epoch_train_loss=0.1450250603599733
Epoch 157
Epoch 157 :: Batch 0/1
Batch Loss = 0.14337498211615077
157, epoch_train_loss=0.14337498211615077
Epoch 158
Epoch 158 :: Batch 0/1
Batch Loss = 0.1417318396402752
158, epoch_train_loss=0.1417318396402752
Epoch 159
Epoch 159 :: Batch 0/1
Batch Loss = 0.14015811985360604
159, epoch_train_loss=0.14015811985360604
Epoch 160
Epoch 160 :: Batch 0/1
Batch Loss = 0.1386063571157863
160, epoch_train_loss=0.1386063571157863
Epoch 161
Epoch 161 :: Batch 0/1
Batch Loss = 0.13706417532142243
161, epoch_train_loss=0.13706417532142243
Epoch 162
Epoch 162 :: Batch 0/1
Batch Loss = 0.1355500085405984
162, epoch_train_loss=0.1355500085405984
Epoch 163
Epoch 163 :: Batch 0/1
Batch Loss = 0.13399934230455843
163, epoch_train_loss=0.13399934230455843
Epoch 164
Epoch 164 :: Batch 0/1
Batch Loss = 0.132504870976817
164, epoch_train_loss=0.132504870976817
Epoch 165
Epoch 165 :: Batch 0/1
Batch Loss = 0.13101641546233242
165, epoch_train_loss=0.13101641546233242
Epoch 166
Epoch 166 :: Batch 0/1
Batch Loss = 0.12957232799382074
166, epoch_train_loss=0.12957232799382074
Epoch 167
Epoch 167 :: Batch 0/1
Batch Loss = 0.12815464046354857
167, epoch_train_loss=0.12815464046354857
Epoch 168
Epoch 168 :: Batch 0/1
Batch Loss = 0.12682887129304274
168, epoch_train_loss=0.12682887129304274
Epoch 169
Epoch 169 :: Batch 0/1
Batch Loss = 0.12560309895120597
169, epoch_train_loss=0.12560309895120597
Epoch 170
Epoch 170 :: Batch 0/1
Batch Loss = 0.12447527856378868
170, epoch_train_loss=0.12447527856378868
Epoch 171
Epoch 171 :: Batch 0/1
Batch Loss = 0.12343630269268516
171, epoch_train_loss=0.12343630269268516
Epoch 172
Epoch 172 :: Batch 0/1
Batch Loss = 0.12238279846097878
172, epoch_train_loss=0.12238279846097878
Epoch 173
Epoch 173 :: Batch 0/1
Batch Loss = 0.12126213294368521
173, epoch_train_loss=0.12126213294368521
Epoch 174
Epoch 174 :: Batch 0/1
Batch Loss = 0.11948133121173386
174, epoch_train_loss=0.11948133121173386
Epoch 175
Epoch 175 :: Batch 0/1
Batch Loss = 0.11865357841071564
175, epoch_train_loss=0.11865357841071564
Epoch 176
Epoch 176 :: Batch 0/1
Batch Loss = 0.11849265896752137
176, epoch_train_loss=0.11849265896752137
Epoch 177
Epoch 177 :: Batch 0/1
Batch Loss = 0.11689664145977513
177, epoch_train_loss=0.11689664145977513
Epoch 178
Epoch 178 :: Batch 0/1
Batch Loss = 0.114739593014193
178, epoch_train_loss=0.114739593014193
Epoch 179
Epoch 179 :: Batch 0/1
Batch Loss = 0.11428500472476241
179, epoch_train_loss=0.11428500472476241
Epoch 180
Epoch 180 :: Batch 0/1
Batch Loss = 0.12473375557160118
180, epoch_train_loss=0.12473375557160118
Epoch 181
Epoch 181 :: Batch 0/1
Batch Loss = 0.19498923747529195
181, epoch_train_loss=0.19498923747529195
Epoch 182
Epoch 182 :: Batch 0/1
Batch Loss = 0.18891342505584727
182, epoch_train_loss=0.18891342505584727
Epoch 183
Epoch 183 :: Batch 0/1
Batch Loss = 0.2884449668901011
183, epoch_train_loss=0.2884449668901011
Epoch 184
Epoch 184 :: Batch 0/1
Batch Loss = 0.7375166914204437
184, epoch_train_loss=0.7375166914204437
Epoch 185
Epoch 185 :: Batch 0/1
Batch Loss = 0.16972894026840063
185, epoch_train_loss=0.16972894026840063
Epoch 186
Epoch 186 :: Batch 0/1
Batch Loss = 0.9072757819757291
186, epoch_train_loss=0.9072757819757291
Epoch 187
Epoch 187 :: Batch 0/1
Batch Loss = 0.41649412786294504
187, epoch_train_loss=0.41649412786294504
Epoch 188
Epoch 188 :: Batch 0/1
Batch Loss = 0.7222116088576795
188, epoch_train_loss=0.7222116088576795
Epoch 189
Epoch 189 :: Batch 0/1
Batch Loss = 0.16051400836681917
189, epoch_train_loss=0.16051400836681917
Epoch 190
Epoch 190 :: Batch 0/1
Batch Loss = 0.6866852871082721
190, epoch_train_loss=0.6866852871082721
Epoch 191
Epoch 191 :: Batch 0/1
Batch Loss = 0.17213471934160907
191, epoch_train_loss=0.17213471934160907
Epoch 192
Epoch 192 :: Batch 0/1
Batch Loss = 0.3828202754057609
192, epoch_train_loss=0.3828202754057609
Epoch 193
Epoch 193 :: Batch 0/1
Batch Loss = 0.301216139156376
193, epoch_train_loss=0.301216139156376
Epoch 194
Epoch 194 :: Batch 0/1
Batch Loss = 0.6696363494707064
194, epoch_train_loss=0.6696363494707064
Epoch 195
Epoch 195 :: Batch 0/1
Batch Loss = 0.29749487584498563
195, epoch_train_loss=0.29749487584498563
Epoch 196
Epoch 196 :: Batch 0/1
Batch Loss = 0.34690634739477977
196, epoch_train_loss=0.34690634739477977
Epoch 197
Epoch 197 :: Batch 0/1
Batch Loss = 0.15397323836301013
197, epoch_train_loss=0.15397323836301013
Epoch 198
Epoch 198 :: Batch 0/1
Batch Loss = 0.39956593454772904
198, epoch_train_loss=0.39956593454772904
Epoch 199
Epoch 199 :: Batch 0/1
Batch Loss = 0.20705087395347146
199, epoch_train_loss=0.20705087395347146
Epoch 200
Epoch 200 :: Batch 0/1
Batch Loss = 0.24485790486767128
200, epoch_train_loss=0.24485790486767128
Epoch 201
Epoch 201 :: Batch 0/1
Batch Loss = 0.3237215450232614
201, epoch_train_loss=0.3237215450232614
Epoch 202
Epoch 202 :: Batch 0/1
Batch Loss = 0.14935936852563303
202, epoch_train_loss=0.14935936852563303
Epoch 203
Epoch 203 :: Batch 0/1
Batch Loss = 0.2778198149590552
203, epoch_train_loss=0.2778198149590552
Epoch 204
Epoch 204 :: Batch 0/1
Batch Loss = 0.1888534248709536
204, epoch_train_loss=0.1888534248709536
Epoch 205
Epoch 205 :: Batch 0/1
Batch Loss = 0.17136074270936155
205, epoch_train_loss=0.17136074270936155
Epoch 206
Epoch 206 :: Batch 0/1
Batch Loss = 0.23301659838099736
206, epoch_train_loss=0.23301659838099736
Epoch 207
Epoch 207 :: Batch 0/1
Batch Loss = 0.12618211927710193
207, epoch_train_loss=0.12618211927710193
Epoch 208
Epoch 208 :: Batch 0/1
Batch Loss = 0.19546041148721133
208, epoch_train_loss=0.19546041148721133
Epoch 209
Epoch 209 :: Batch 0/1
Batch Loss = 0.15042265832330912
209, epoch_train_loss=0.15042265832330912
Epoch 210
Epoch 210 :: Batch 0/1
Batch Loss = 0.1415442643553549
210, epoch_train_loss=0.1415442643553549
Epoch 211
Epoch 211 :: Batch 0/1
Batch Loss = 0.177927641949637
211, epoch_train_loss=0.177927641949637
Epoch 212
Epoch 212 :: Batch 0/1
Batch Loss = 0.11288029593673111
212, epoch_train_loss=0.11288029593673111
Epoch 213
Epoch 213 :: Batch 0/1
Batch Loss = 0.16253457957619283
213, epoch_train_loss=0.16253457957619283
Epoch 214
Epoch 214 :: Batch 0/1
Batch Loss = 0.11754417944107315
214, epoch_train_loss=0.11754417944107315
Epoch 215
Epoch 215 :: Batch 0/1
Batch Loss = 0.12660915263329106
215, epoch_train_loss=0.12660915263329106
Epoch 216
Epoch 216 :: Batch 0/1
Batch Loss = 0.13194499178116625
216, epoch_train_loss=0.13194499178116625
Epoch 217
Epoch 217 :: Batch 0/1
Batch Loss = 0.10239309182382549
217, epoch_train_loss=0.10239309182382549
Epoch 218
Epoch 218 :: Batch 0/1
Batch Loss = 0.13220637204799293
218, epoch_train_loss=0.13220637204799293
Epoch 219
Epoch 219 :: Batch 0/1
Batch Loss = 0.09593978706284964
219, epoch_train_loss=0.09593978706284964
Epoch 220
Epoch 220 :: Batch 0/1
Batch Loss = 0.12061478679065395
220, epoch_train_loss=0.12061478679065395
Epoch 221
Epoch 221 :: Batch 0/1
Batch Loss = 0.10004950128649852
221, epoch_train_loss=0.10004950128649852
Epoch 222
Epoch 222 :: Batch 0/1
Batch Loss = 0.10402365725282628
222, epoch_train_loss=0.10402365725282628
Epoch 223
Epoch 223 :: Batch 0/1
Batch Loss = 0.10120577394526954
223, epoch_train_loss=0.10120577394526954
Epoch 224
Epoch 224 :: Batch 0/1
Batch Loss = 0.09226486612506962
224, epoch_train_loss=0.09226486612506962
Epoch 225
Epoch 225 :: Batch 0/1
Batch Loss = 0.1003473819070151
225, epoch_train_loss=0.1003473819070151
Epoch 226
Epoch 226 :: Batch 0/1
Batch Loss = 0.08340490472666526
226, epoch_train_loss=0.08340490472666526
Epoch 227
Epoch 227 :: Batch 0/1
Batch Loss = 0.09645644822342865
227, epoch_train_loss=0.09645644822342865
Epoch 228
Epoch 228 :: Batch 0/1
Batch Loss = 0.08003649131260732
228, epoch_train_loss=0.08003649131260732
Epoch 229
Epoch 229 :: Batch 0/1
Batch Loss = 0.09035724516305621
229, epoch_train_loss=0.09035724516305621
Epoch 230
Epoch 230 :: Batch 0/1
Batch Loss = 0.07950783014662256
230, epoch_train_loss=0.07950783014662256
Epoch 231
Epoch 231 :: Batch 0/1
Batch Loss = 0.08527641888623289
231, epoch_train_loss=0.08527641888623289
Epoch 232
Epoch 232 :: Batch 0/1
Batch Loss = 0.07828512534122463
232, epoch_train_loss=0.07828512534122463
Epoch 233
Epoch 233 :: Batch 0/1
Batch Loss = 0.0804405024089674
233, epoch_train_loss=0.0804405024089674
Epoch 234
Epoch 234 :: Batch 0/1
Batch Loss = 0.07751537627271869
234, epoch_train_loss=0.07751537627271869
Epoch 235
Epoch 235 :: Batch 0/1
Batch Loss = 0.07601534688078829
235, epoch_train_loss=0.07601534688078829
Epoch 236
Epoch 236 :: Batch 0/1
Batch Loss = 0.07568963958466268
236, epoch_train_loss=0.07568963958466268
Epoch 237
Epoch 237 :: Batch 0/1
Batch Loss = 0.07310299894830063
237, epoch_train_loss=0.07310299894830063
Epoch 238
Epoch 238 :: Batch 0/1
Batch Loss = 0.07376911311952312
238, epoch_train_loss=0.07376911311952312
Epoch 239
Epoch 239 :: Batch 0/1
Batch Loss = 0.07062506783944887
239, epoch_train_loss=0.07062506783944887
Epoch 240
Epoch 240 :: Batch 0/1
Batch Loss = 0.07207955712940764
240, epoch_train_loss=0.07207955712940764
Epoch 241
Epoch 241 :: Batch 0/1
Batch Loss = 0.06901718321729879
241, epoch_train_loss=0.06901718321729879
Epoch 242
Epoch 242 :: Batch 0/1
Batch Loss = 0.07023334044140149
242, epoch_train_loss=0.07023334044140149
Epoch 243
Epoch 243 :: Batch 0/1
Batch Loss = 0.06751994887900988
243, epoch_train_loss=0.06751994887900988
Epoch 244
Epoch 244 :: Batch 0/1
Batch Loss = 0.06828394805633575
244, epoch_train_loss=0.06828394805633575
Epoch 245
Epoch 245 :: Batch 0/1
Batch Loss = 0.06618351021428144
245, epoch_train_loss=0.06618351021428144
Epoch 246
Epoch 246 :: Batch 0/1
Batch Loss = 0.0664609010476542
246, epoch_train_loss=0.0664609010476542
Epoch 247
Epoch 247 :: Batch 0/1
Batch Loss = 0.06513404405424632
247, epoch_train_loss=0.06513404405424632
Epoch 248
Epoch 248 :: Batch 0/1
Batch Loss = 0.06479736904667664
248, epoch_train_loss=0.06479736904667664
Epoch 249
Epoch 249 :: Batch 0/1
Batch Loss = 0.06437110699342986
249, epoch_train_loss=0.06437110699342986
Epoch 250
Epoch 250 :: Batch 0/1
Batch Loss = 0.06341464799659824
250, epoch_train_loss=0.06341464799659824
Epoch 251
Epoch 251 :: Batch 0/1
Batch Loss = 0.06348484349298707
251, epoch_train_loss=0.06348484349298707
Epoch 252
Epoch 252 :: Batch 0/1
Batch Loss = 0.062140810845420794
252, epoch_train_loss=0.062140810845420794
Epoch 253
Epoch 253 :: Batch 0/1
Batch Loss = 0.0625646445570813
253, epoch_train_loss=0.0625646445570813
Epoch 254
Epoch 254 :: Batch 0/1
Batch Loss = 0.06100946816672895
254, epoch_train_loss=0.06100946816672895
Epoch 255
Epoch 255 :: Batch 0/1
Batch Loss = 0.06157658456120237
255, epoch_train_loss=0.06157658456120237
Epoch 256
Epoch 256 :: Batch 0/1
Batch Loss = 0.06013324991335395
256, epoch_train_loss=0.06013324991335395
Epoch 257
Epoch 257 :: Batch 0/1
Batch Loss = 0.06049631134800922
257, epoch_train_loss=0.06049631134800922
Epoch 258
Epoch 258 :: Batch 0/1
Batch Loss = 0.059429354284153725
258, epoch_train_loss=0.059429354284153725
Epoch 259
Epoch 259 :: Batch 0/1
Batch Loss = 0.05944041164461747
259, epoch_train_loss=0.05944041164461747
Epoch 260
Epoch 260 :: Batch 0/1
Batch Loss = 0.05881714720577365
260, epoch_train_loss=0.05881714720577365
Epoch 261
Epoch 261 :: Batch 0/1
Batch Loss = 0.058416267528861664
261, epoch_train_loss=0.058416267528861664
Epoch 262
Epoch 262 :: Batch 0/1
Batch Loss = 0.05820198303456429
262, epoch_train_loss=0.05820198303456429
Epoch 263
Epoch 263 :: Batch 0/1
Batch Loss = 0.0574878098610789
263, epoch_train_loss=0.0574878098610789
Epoch 264
Epoch 264 :: Batch 0/1
Batch Loss = 0.05748548499887531
264, epoch_train_loss=0.05748548499887531
Epoch 265
Epoch 265 :: Batch 0/1
Batch Loss = 0.05672204396138644
265, epoch_train_loss=0.05672204396138644
Epoch 266
Epoch 266 :: Batch 0/1
Batch Loss = 0.05669675605606112
266, epoch_train_loss=0.05669675605606112
Epoch 267
Epoch 267 :: Batch 0/1
Batch Loss = 0.05609954492225162
267, epoch_train_loss=0.05609954492225162
Epoch 268
Epoch 268 :: Batch 0/1
Batch Loss = 0.05589244989084861
268, epoch_train_loss=0.05589244989084861
Epoch 269
Epoch 269 :: Batch 0/1
Batch Loss = 0.05553197140586811
269, epoch_train_loss=0.05553197140586811
Epoch 270
Epoch 270 :: Batch 0/1
Batch Loss = 0.05511832609393721
270, epoch_train_loss=0.05511832609393721
Epoch 271
Epoch 271 :: Batch 0/1
Batch Loss = 0.05492603380413016
271, epoch_train_loss=0.05492603380413016
Epoch 272
Epoch 272 :: Batch 0/1
Batch Loss = 0.05441674387105492
272, epoch_train_loss=0.05441674387105492
Epoch 273
Epoch 273 :: Batch 0/1
Batch Loss = 0.05426524466265136
273, epoch_train_loss=0.05426524466265136
Epoch 274
Epoch 274 :: Batch 0/1
Batch Loss = 0.053810104298673805
274, epoch_train_loss=0.053810104298673805
Epoch 275
Epoch 275 :: Batch 0/1
Batch Loss = 0.053580227391141474
275, epoch_train_loss=0.053580227391141474
Epoch 276
Epoch 276 :: Batch 0/1
Batch Loss = 0.05325611178044782
276, epoch_train_loss=0.05325611178044782
Epoch 277
Epoch 277 :: Batch 0/1
Batch Loss = 0.052917611393170035
277, epoch_train_loss=0.052917611393170035
Epoch 278
Epoch 278 :: Batch 0/1
Batch Loss = 0.05269050520063411
278, epoch_train_loss=0.05269050520063411
Epoch 279
Epoch 279 :: Batch 0/1
Batch Loss = 0.052309016529621055
279, epoch_train_loss=0.052309016529621055
Epoch 280
Epoch 280 :: Batch 0/1
Batch Loss = 0.05209883813899608
280, epoch_train_loss=0.05209883813899608
Epoch 281
Epoch 281 :: Batch 0/1
Batch Loss = 0.051751039230463876
281, epoch_train_loss=0.051751039230463876
Epoch 282
Epoch 282 :: Batch 0/1
Batch Loss = 0.051498994889731774
282, epoch_train_loss=0.051498994889731774
Epoch 283
Epoch 283 :: Batch 0/1
Batch Loss = 0.051220809122720555
283, epoch_train_loss=0.051220809122720555
Epoch 284
Epoch 284 :: Batch 0/1
Batch Loss = 0.0509165585268349
284, epoch_train_loss=0.0509165585268349
Epoch 285
Epoch 285 :: Batch 0/1
Batch Loss = 0.050687036599047636
285, epoch_train_loss=0.050687036599047636
Epoch 286
Epoch 286 :: Batch 0/1
Batch Loss = 0.050371663090524876
286, epoch_train_loss=0.050371663090524876
Epoch 287
Epoch 287 :: Batch 0/1
Batch Loss = 0.05014007839268371
287, epoch_train_loss=0.05014007839268371
Epoch 288
Epoch 288 :: Batch 0/1
Batch Loss = 0.04985410089035121
288, epoch_train_loss=0.04985410089035121
Epoch 289
Epoch 289 :: Batch 0/1
Batch Loss = 0.04959056349838248
289, epoch_train_loss=0.04959056349838248
Epoch 290
Epoch 290 :: Batch 0/1
Batch Loss = 0.04934048563543503
290, epoch_train_loss=0.04934048563543503
Epoch 291
Epoch 291 :: Batch 0/1
Batch Loss = 0.049055978956713674
291, epoch_train_loss=0.049055978956713674
Epoch 292
Epoch 292 :: Batch 0/1
Batch Loss = 0.04881990686924319
292, epoch_train_loss=0.04881990686924319
Epoch 293
Epoch 293 :: Batch 0/1
Batch Loss = 0.04854384598352895
293, epoch_train_loss=0.04854384598352895
Epoch 294
Epoch 294 :: Batch 0/1
Batch Loss = 0.048299252829762364
294, epoch_train_loss=0.048299252829762364
Epoch 295
Epoch 295 :: Batch 0/1
Batch Loss = 0.04804574284640021
295, epoch_train_loss=0.04804574284640021
Epoch 296
Epoch 296 :: Batch 0/1
Batch Loss = 0.04778734940437204
296, epoch_train_loss=0.04778734940437204
Epoch 297
Epoch 297 :: Batch 0/1
Batch Loss = 0.04755089800796218
297, epoch_train_loss=0.04755089800796218
Epoch 298
Epoch 298 :: Batch 0/1
Batch Loss = 0.04729118332744918
298, epoch_train_loss=0.04729118332744918
Epoch 299
Epoch 299 :: Batch 0/1
Batch Loss = 0.0470566188727515
299, epoch_train_loss=0.0470566188727515
Epoch 300
Epoch 300 :: Batch 0/1
Batch Loss = 0.046810361188613196
300, epoch_train_loss=0.046810361188613196
Epoch 301
Epoch 301 :: Batch 0/1
Batch Loss = 0.046569428984826924
301, epoch_train_loss=0.046569428984826924
Epoch 302
Epoch 302 :: Batch 0/1
Batch Loss = 0.04633747942640512
302, epoch_train_loss=0.04633747942640512
Epoch 303
Epoch 303 :: Batch 0/1
Batch Loss = 0.046094328798868094
303, epoch_train_loss=0.046094328798868094
Epoch 304
Epoch 304 :: Batch 0/1
Batch Loss = 0.04586721456148212
304, epoch_train_loss=0.04586721456148212
Epoch 305
Epoch 305 :: Batch 0/1
Batch Loss = 0.04563128169131387
305, epoch_train_loss=0.04563128169131387
Epoch 306
Epoch 306 :: Batch 0/1
Batch Loss = 0.04540190400156083
306, epoch_train_loss=0.04540190400156083
Epoch 307
Epoch 307 :: Batch 0/1
Batch Loss = 0.04517653061082745
307, epoch_train_loss=0.04517653061082745
Epoch 308
Epoch 308 :: Batch 0/1
Batch Loss = 0.04494674155294651
308, epoch_train_loss=0.04494674155294651
Epoch 309
Epoch 309 :: Batch 0/1
Batch Loss = 0.04472692005648636
309, epoch_train_loss=0.04472692005648636
Epoch 310
Epoch 310 :: Batch 0/1
Batch Loss = 0.044502032530684715
310, epoch_train_loss=0.044502032530684715
Epoch 311
Epoch 311 :: Batch 0/1
Batch Loss = 0.044282250110892256
311, epoch_train_loss=0.044282250110892256
Epoch 312
Epoch 312 :: Batch 0/1
Batch Loss = 0.04406453277656674
312, epoch_train_loss=0.04406453277656674
Epoch 313
Epoch 313 :: Batch 0/1
Batch Loss = 0.04384497507402977
313, epoch_train_loss=0.04384497507402977
Epoch 314
Epoch 314 :: Batch 0/1
Batch Loss = 0.04363173494836361
314, epoch_train_loss=0.04363173494836361
Epoch 315
Epoch 315 :: Batch 0/1
Batch Loss = 0.043415971431746775
315, epoch_train_loss=0.043415971431746775
Epoch 316
Epoch 316 :: Batch 0/1
Batch Loss = 0.04320372526061671
316, epoch_train_loss=0.04320372526061671
Epoch 317
Epoch 317 :: Batch 0/1
Batch Loss = 0.04299327829065551
317, epoch_train_loss=0.04299327829065551
Epoch 318
Epoch 318 :: Batch 0/1
Batch Loss = 0.04278200296595969
318, epoch_train_loss=0.04278200296595969
Epoch 319
Epoch 319 :: Batch 0/1
Batch Loss = 0.04257509684426487
319, epoch_train_loss=0.04257509684426487
Epoch 320
Epoch 320 :: Batch 0/1
Batch Loss = 0.04236745011180096
320, epoch_train_loss=0.04236745011180096
Epoch 321
Epoch 321 :: Batch 0/1
Batch Loss = 0.042162275707101696
321, epoch_train_loss=0.042162275707101696
Epoch 322
Epoch 322 :: Batch 0/1
Batch Loss = 0.041959408609111876
322, epoch_train_loss=0.041959408609111876
Epoch 323
Epoch 323 :: Batch 0/1
Batch Loss = 0.04175661216508492
323, epoch_train_loss=0.04175661216508492
Epoch 324
Epoch 324 :: Batch 0/1
Batch Loss = 0.04155730481652606
324, epoch_train_loss=0.04155730481652606
Epoch 325
Epoch 325 :: Batch 0/1
Batch Loss = 0.0413586917093865
325, epoch_train_loss=0.0413586917093865
Epoch 326
Epoch 326 :: Batch 0/1
Batch Loss = 0.041161952443087464
326, epoch_train_loss=0.041161952443087464
Epoch 327
Epoch 327 :: Batch 0/1
Batch Loss = 0.04096784140689676
327, epoch_train_loss=0.04096784140689676
Epoch 328
Epoch 328 :: Batch 0/1
Batch Loss = 0.04077441311745162
328, epoch_train_loss=0.04077441311745162
Epoch 329
Epoch 329 :: Batch 0/1
Batch Loss = 0.040583694948350096
329, epoch_train_loss=0.040583694948350096
Epoch 330
Epoch 330 :: Batch 0/1
Batch Loss = 0.04039449404952059
330, epoch_train_loss=0.04039449404952059
Epoch 331
Epoch 331 :: Batch 0/1
Batch Loss = 0.04020671015223814
331, epoch_train_loss=0.04020671015223814
Epoch 332
Epoch 332 :: Batch 0/1
Batch Loss = 0.040021422977071994
332, epoch_train_loss=0.040021422977071994
Epoch 333
Epoch 333 :: Batch 0/1
Batch Loss = 0.03983727137129472
333, epoch_train_loss=0.03983727137129472
Epoch 334
Epoch 334 :: Batch 0/1
Batch Loss = 0.03965515371038255
334, epoch_train_loss=0.03965515371038255
Epoch 335
Epoch 335 :: Batch 0/1
Batch Loss = 0.03947496689377313
335, epoch_train_loss=0.03947496689377313
Epoch 336
Epoch 336 :: Batch 0/1
Batch Loss = 0.039296050805749695
336, epoch_train_loss=0.039296050805749695
Epoch 337
Epoch 337 :: Batch 0/1
Batch Loss = 0.03911926156856564
337, epoch_train_loss=0.03911926156856564
Epoch 338
Epoch 338 :: Batch 0/1
Batch Loss = 0.03894394912685386
338, epoch_train_loss=0.03894394912685386
Epoch 339
Epoch 339 :: Batch 0/1
Batch Loss = 0.03877015174299905
339, epoch_train_loss=0.03877015174299905
Epoch 340
Epoch 340 :: Batch 0/1
Batch Loss = 0.0385982632353171
340, epoch_train_loss=0.0385982632353171
Epoch 341
Epoch 341 :: Batch 0/1
Batch Loss = 0.03842767409796621
341, epoch_train_loss=0.03842767409796621
Epoch 342
Epoch 342 :: Batch 0/1
Batch Loss = 0.03825873513548282
342, epoch_train_loss=0.03825873513548282
Epoch 343
Epoch 343 :: Batch 0/1
Batch Loss = 0.03809141710768063
343, epoch_train_loss=0.03809141710768063
Epoch 344
Epoch 344 :: Batch 0/1
Batch Loss = 0.037925380021656754
344, epoch_train_loss=0.037925380021656754
Epoch 345
Epoch 345 :: Batch 0/1
Batch Loss = 0.037760979692344085
345, epoch_train_loss=0.037760979692344085
Epoch 346
Epoch 346 :: Batch 0/1
Batch Loss = 0.0375979944406594
346, epoch_train_loss=0.0375979944406594
Epoch 347
Epoch 347 :: Batch 0/1
Batch Loss = 0.03743634105768889
347, epoch_train_loss=0.03743634105768889
Epoch 348
Epoch 348 :: Batch 0/1
Batch Loss = 0.03727624253508093
348, epoch_train_loss=0.03727624253508093
Epoch 349
Epoch 349 :: Batch 0/1
Batch Loss = 0.03711744937353608
349, epoch_train_loss=0.03711744937353608
Epoch 350
Epoch 350 :: Batch 0/1
Batch Loss = 0.03696001647865253
350, epoch_train_loss=0.03696001647865253
Epoch 351
Epoch 351 :: Batch 0/1
Batch Loss = 0.03680402726488167
351, epoch_train_loss=0.03680402726488167
Epoch 352
Epoch 352 :: Batch 0/1
Batch Loss = 0.03664928737729933
352, epoch_train_loss=0.03664928737729933
Epoch 353
Epoch 353 :: Batch 0/1
Batch Loss = 0.036495895843287975
353, epoch_train_loss=0.036495895843287975
Epoch 354
Epoch 354 :: Batch 0/1
Batch Loss = 0.036343846867352265
354, epoch_train_loss=0.036343846867352265
Epoch 355
Epoch 355 :: Batch 0/1
Batch Loss = 0.03619301748659274
355, epoch_train_loss=0.03619301748659274
Epoch 356
Epoch 356 :: Batch 0/1
Batch Loss = 0.03604350042968
356, epoch_train_loss=0.03604350042968
Epoch 357
Epoch 357 :: Batch 0/1
Batch Loss = 0.03589524954075579
357, epoch_train_loss=0.03589524954075579
Epoch 358
Epoch 358 :: Batch 0/1
Batch Loss = 0.03574819791190302
358, epoch_train_loss=0.03574819791190302
Epoch 359
Epoch 359 :: Batch 0/1
Batch Loss = 0.035602416619519364
359, epoch_train_loss=0.035602416619519364
Epoch 360
Epoch 360 :: Batch 0/1
Batch Loss = 0.035457850818914
360, epoch_train_loss=0.035457850818914
Epoch 361
Epoch 361 :: Batch 0/1
Batch Loss = 0.035314468990474135
361, epoch_train_loss=0.035314468990474135
Epoch 362
Epoch 362 :: Batch 0/1
Batch Loss = 0.03517231911310487
362, epoch_train_loss=0.03517231911310487
Epoch 363
Epoch 363 :: Batch 0/1
Batch Loss = 0.035031348656659744
363, epoch_train_loss=0.035031348656659744
Epoch 364
Epoch 364 :: Batch 0/1
Batch Loss = 0.03489154448515659
364, epoch_train_loss=0.03489154448515659
Epoch 365
Epoch 365 :: Batch 0/1
Batch Loss = 0.03475293706683707
365, epoch_train_loss=0.03475293706683707
Epoch 366
Epoch 366 :: Batch 0/1
Batch Loss = 0.034615482588575296
366, epoch_train_loss=0.034615482588575296
Epoch 367
Epoch 367 :: Batch 0/1
Batch Loss = 0.03447917847484962
367, epoch_train_loss=0.03447917847484962
Epoch 368
Epoch 368 :: Batch 0/1
Batch Loss = 0.03434404376282456
368, epoch_train_loss=0.03434404376282456
Epoch 369
Epoch 369 :: Batch 0/1
Batch Loss = 0.03421004474686462
369, epoch_train_loss=0.03421004474686462
Epoch 370
Epoch 370 :: Batch 0/1
Batch Loss = 0.034077183787117486
370, epoch_train_loss=0.034077183787117486
Epoch 371
Epoch 371 :: Batch 0/1
Batch Loss = 0.03394547318842886
371, epoch_train_loss=0.03394547318842886
Epoch 372
Epoch 372 :: Batch 0/1
Batch Loss = 0.03381488917789224
372, epoch_train_loss=0.03381488917789224
Epoch 373
Epoch 373 :: Batch 0/1
Batch Loss = 0.03368543578998087
373, epoch_train_loss=0.03368543578998087
Epoch 374
Epoch 374 :: Batch 0/1
Batch Loss = 0.03355712221660379
374, epoch_train_loss=0.03355712221660379
Epoch 375
Epoch 375 :: Batch 0/1
Batch Loss = 0.03342993250390316
375, epoch_train_loss=0.03342993250390316
Epoch 376
Epoch 376 :: Batch 0/1
Batch Loss = 0.03330387008618522
376, epoch_train_loss=0.03330387008618522
Epoch 377
Epoch 377 :: Batch 0/1
Batch Loss = 0.03317894243413414
377, epoch_train_loss=0.03317894243413414
Epoch 378
Epoch 378 :: Batch 0/1
Batch Loss = 0.03305513842730964
378, epoch_train_loss=0.03305513842730964
Epoch 379
Epoch 379 :: Batch 0/1
Batch Loss = 0.0329324601261314
379, epoch_train_loss=0.0329324601261314
Epoch 380
Epoch 380 :: Batch 0/1
Batch Loss = 0.03281091393606081
380, epoch_train_loss=0.03281091393606081
Epoch 381
Epoch 381 :: Batch 0/1
Batch Loss = 0.0326904919966816
381, epoch_train_loss=0.0326904919966816
Epoch 382
Epoch 382 :: Batch 0/1
Batch Loss = 0.03257119510735557
382, epoch_train_loss=0.03257119510735557
Epoch 383
Epoch 383 :: Batch 0/1
Batch Loss = 0.032453028225828205
383, epoch_train_loss=0.032453028225828205
Epoch 384
Epoch 384 :: Batch 0/1
Batch Loss = 0.032335985539712696
384, epoch_train_loss=0.032335985539712696
Epoch 385
Epoch 385 :: Batch 0/1
Batch Loss = 0.03222006625069686
385, epoch_train_loss=0.03222006625069686
Epoch 386
Epoch 386 :: Batch 0/1
Batch Loss = 0.03210527345940572
386, epoch_train_loss=0.03210527345940572
Epoch 387
Epoch 387 :: Batch 0/1
Batch Loss = 0.03199160246666234
387, epoch_train_loss=0.03199160246666234
Epoch 388
Epoch 388 :: Batch 0/1
Batch Loss = 0.031879050383365616
388, epoch_train_loss=0.031879050383365616
Epoch 389
Epoch 389 :: Batch 0/1
Batch Loss = 0.031767617937199684
389, epoch_train_loss=0.031767617937199684
Epoch 390
Epoch 390 :: Batch 0/1
Batch Loss = 0.03165730040604854
390, epoch_train_loss=0.03165730040604854
Epoch 391
Epoch 391 :: Batch 0/1
Batch Loss = 0.031548092557265296
391, epoch_train_loss=0.031548092557265296
Epoch 392
Epoch 392 :: Batch 0/1
Batch Loss = 0.03143999244122271
392, epoch_train_loss=0.03143999244122271
Epoch 393
Epoch 393 :: Batch 0/1
Batch Loss = 0.031332994473754386
393, epoch_train_loss=0.031332994473754386
Epoch 394
Epoch 394 :: Batch 0/1
Batch Loss = 0.03122709136816551
394, epoch_train_loss=0.03122709136816551
Epoch 395
Epoch 395 :: Batch 0/1
Batch Loss = 0.031122278324566075
395, epoch_train_loss=0.031122278324566075
Epoch 396
Epoch 396 :: Batch 0/1
Batch Loss = 0.03101854852786055
396, epoch_train_loss=0.03101854852786055
Epoch 397
Epoch 397 :: Batch 0/1
Batch Loss = 0.03091589298065037
397, epoch_train_loss=0.03091589298065037
Epoch 398
Epoch 398 :: Batch 0/1
Batch Loss = 0.03081430414838687
398, epoch_train_loss=0.03081430414838687
Epoch 399
Epoch 399 :: Batch 0/1
Batch Loss = 0.030713773877124588
399, epoch_train_loss=0.030713773877124588
Epoch 400
Epoch 400 :: Batch 0/1
Batch Loss = 0.030614291860709186
400, epoch_train_loss=0.030614291860709186
Epoch 401
Epoch 401 :: Batch 0/1
Batch Loss = 0.030515848357060938
401, epoch_train_loss=0.030515848357060938
Epoch 402
Epoch 402 :: Batch 0/1
Batch Loss = 0.030418433887096617
402, epoch_train_loss=0.030418433887096617
Epoch 403
Epoch 403 :: Batch 0/1
Batch Loss = 0.030322037248922972
403, epoch_train_loss=0.030322037248922972
Epoch 404
Epoch 404 :: Batch 0/1
Batch Loss = 0.030226647183627265
404, epoch_train_loss=0.030226647183627265
Epoch 405
Epoch 405 :: Batch 0/1
Batch Loss = 0.03013225304956334
405, epoch_train_loss=0.03013225304956334
Epoch 406
Epoch 406 :: Batch 0/1
Batch Loss = 0.030038843278084666
406, epoch_train_loss=0.030038843278084666
Epoch 407
Epoch 407 :: Batch 0/1
Batch Loss = 0.029946405840568867
407, epoch_train_loss=0.029946405840568867
Epoch 408
Epoch 408 :: Batch 0/1
Batch Loss = 0.02985492934165324
408, epoch_train_loss=0.02985492934165324
Epoch 409
Epoch 409 :: Batch 0/1
Batch Loss = 0.029764402226518293
409, epoch_train_loss=0.029764402226518293
Epoch 410
Epoch 410 :: Batch 0/1
Batch Loss = 0.029674812448039815
410, epoch_train_loss=0.029674812448039815
Epoch 411
Epoch 411 :: Batch 0/1
Batch Loss = 0.02958614838938237
411, epoch_train_loss=0.02958614838938237
Epoch 412
Epoch 412 :: Batch 0/1
Batch Loss = 0.029498398782049486
412, epoch_train_loss=0.029498398782049486
Epoch 413
Epoch 413 :: Batch 0/1
Batch Loss = 0.029411552094577898
413, epoch_train_loss=0.029411552094577898
Epoch 414
Epoch 414 :: Batch 0/1
Batch Loss = 0.029325597009053857
414, epoch_train_loss=0.029325597009053857
Epoch 415
Epoch 415 :: Batch 0/1
Batch Loss = 0.02924052274586488
415, epoch_train_loss=0.02924052274586488
Epoch 416
Epoch 416 :: Batch 0/1
Batch Loss = 0.029156318593789123
416, epoch_train_loss=0.029156318593789123
Epoch 417
Epoch 417 :: Batch 0/1
Batch Loss = 0.029072973872051818
417, epoch_train_loss=0.029072973872051818
Epoch 418
Epoch 418 :: Batch 0/1
Batch Loss = 0.02899047839450996
418, epoch_train_loss=0.02899047839450996
Epoch 419
Epoch 419 :: Batch 0/1
Batch Loss = 0.028908822270540313
419, epoch_train_loss=0.028908822270540313
Epoch 420
Epoch 420 :: Batch 0/1
Batch Loss = 0.028827995660643594
420, epoch_train_loss=0.028827995660643594
Epoch 421
Epoch 421 :: Batch 0/1
Batch Loss = 0.02874798899173398
421, epoch_train_loss=0.02874798899173398
Epoch 422
Epoch 422 :: Batch 0/1
Batch Loss = 0.028668793035483458
422, epoch_train_loss=0.028668793035483458
Epoch 423
Epoch 423 :: Batch 0/1
Batch Loss = 0.028590398697875306
423, epoch_train_loss=0.028590398697875306
Epoch 424
Epoch 424 :: Batch 0/1
Batch Loss = 0.028512796979965505
424, epoch_train_loss=0.028512796979965505
Epoch 425
Epoch 425 :: Batch 0/1
Batch Loss = 0.028435979114180863
425, epoch_train_loss=0.028435979114180863
Epoch 426
Epoch 426 :: Batch 0/1
Batch Loss = 0.028359936506949936
426, epoch_train_loss=0.028359936506949936
Epoch 427
Epoch 427 :: Batch 0/1
Batch Loss = 0.028284660580386034
427, epoch_train_loss=0.028284660580386034
Epoch 428
Epoch 428 :: Batch 0/1
Batch Loss = 0.02821014282794157
428, epoch_train_loss=0.02821014282794157
Epoch 429
Epoch 429 :: Batch 0/1
Batch Loss = 0.028136374875046753
429, epoch_train_loss=0.028136374875046753
Epoch 430
Epoch 430 :: Batch 0/1
Batch Loss = 0.028063348358264652
430, epoch_train_loss=0.028063348358264652
Epoch 431
Epoch 431 :: Batch 0/1
Batch Loss = 0.02799105486590462
431, epoch_train_loss=0.02799105486590462
Epoch 432
Epoch 432 :: Batch 0/1
Batch Loss = 0.027919486031186352
432, epoch_train_loss=0.027919486031186352
Epoch 433
Epoch 433 :: Batch 0/1
Batch Loss = 0.027848633485006675
433, epoch_train_loss=0.027848633485006675
Epoch 434
Epoch 434 :: Batch 0/1
Batch Loss = 0.027778488794651352
434, epoch_train_loss=0.027778488794651352
Epoch 435
Epoch 435 :: Batch 0/1
Batch Loss = 0.027709043488526017
435, epoch_train_loss=0.027709043488526017
Epoch 436
Epoch 436 :: Batch 0/1
Batch Loss = 0.027640289063413304
436, epoch_train_loss=0.027640289063413304
Epoch 437
Epoch 437 :: Batch 0/1
Batch Loss = 0.02757221699035238
437, epoch_train_loss=0.02757221699035238
Epoch 438
Epoch 438 :: Batch 0/1
Batch Loss = 0.027504818664767443
438, epoch_train_loss=0.027504818664767443
Epoch 439
Epoch 439 :: Batch 0/1
Batch Loss = 0.027438085427464794
439, epoch_train_loss=0.027438085427464794
Epoch 440
Epoch 440 :: Batch 0/1
Batch Loss = 0.027372008606211685
440, epoch_train_loss=0.027372008606211685
Epoch 441
Epoch 441 :: Batch 0/1
Batch Loss = 0.027306579492589567
441, epoch_train_loss=0.027306579492589567
Epoch 442
Epoch 442 :: Batch 0/1
Batch Loss = 0.0272417893211882
442, epoch_train_loss=0.0272417893211882
Epoch 443
Epoch 443 :: Batch 0/1
Batch Loss = 0.027177629322150145
443, epoch_train_loss=0.027177629322150145
Epoch 444
Epoch 444 :: Batch 0/1
Batch Loss = 0.027114090713344616
444, epoch_train_loss=0.027114090713344616
Epoch 445
Epoch 445 :: Batch 0/1
Batch Loss = 0.027051164695367995
445, epoch_train_loss=0.027051164695367995
Epoch 446
Epoch 446 :: Batch 0/1
Batch Loss = 0.026988842469951018
446, epoch_train_loss=0.026988842469951018
Epoch 447
Epoch 447 :: Batch 0/1
Batch Loss = 0.02692711523380345
447, epoch_train_loss=0.02692711523380345
Epoch 448
Epoch 448 :: Batch 0/1
Batch Loss = 0.026865974207370434
448, epoch_train_loss=0.026865974207370434
Epoch 449
Epoch 449 :: Batch 0/1
Batch Loss = 0.026805410631379794
449, epoch_train_loss=0.026805410631379794
Epoch 450
Epoch 450 :: Batch 0/1
Batch Loss = 0.026745415763691834
450, epoch_train_loss=0.026745415763691834
Epoch 451
Epoch 451 :: Batch 0/1
Batch Loss = 0.026685980928397184
451, epoch_train_loss=0.026685980928397184
Epoch 452
Epoch 452 :: Batch 0/1
Batch Loss = 0.02662709751638198
452, epoch_train_loss=0.02662709751638198
Epoch 453
Epoch 453 :: Batch 0/1
Batch Loss = 0.026568757028188388
453, epoch_train_loss=0.026568757028188388
Epoch 454
Epoch 454 :: Batch 0/1
Batch Loss = 0.026510951105648724
454, epoch_train_loss=0.026510951105648724
Epoch 455
Epoch 455 :: Batch 0/1
Batch Loss = 0.026453671600386447
455, epoch_train_loss=0.026453671600386447
Epoch 456
Epoch 456 :: Batch 0/1
Batch Loss = 0.026396910663103824
456, epoch_train_loss=0.026396910663103824
Epoch 457
Epoch 457 :: Batch 0/1
Batch Loss = 0.026340660806010087
457, epoch_train_loss=0.026340660806010087
Epoch 458
Epoch 458 :: Batch 0/1
Batch Loss = 0.026284915021159014
458, epoch_train_loss=0.026284915021159014
Epoch 459
Epoch 459 :: Batch 0/1
Batch Loss = 0.02622966689612423
459, epoch_train_loss=0.02622966689612423
Epoch 460
Epoch 460 :: Batch 0/1
Batch Loss = 0.026174910671529095
460, epoch_train_loss=0.026174910671529095
Epoch 461
Epoch 461 :: Batch 0/1
Batch Loss = 0.02612064132099145
461, epoch_train_loss=0.02612064132099145
Epoch 462
Epoch 462 :: Batch 0/1
Batch Loss = 0.026066854511365953
462, epoch_train_loss=0.026066854511365953
Epoch 463
Epoch 463 :: Batch 0/1
Batch Loss = 0.026013546490378684
463, epoch_train_loss=0.026013546490378684
Epoch 464
Epoch 464 :: Batch 0/1
Batch Loss = 0.02596071385085847
464, epoch_train_loss=0.02596071385085847
Epoch 465
Epoch 465 :: Batch 0/1
Batch Loss = 0.02590835318623392
465, epoch_train_loss=0.02590835318623392
Epoch 466
Epoch 466 :: Batch 0/1
Batch Loss = 0.02585646066466791
466, epoch_train_loss=0.02585646066466791
Epoch 467
Epoch 467 :: Batch 0/1
Batch Loss = 0.02580503161699412
467, epoch_train_loss=0.02580503161699412
Epoch 468
Epoch 468 :: Batch 0/1
Batch Loss = 0.02575406013437957
468, epoch_train_loss=0.02575406013437957
Epoch 469
Epoch 469 :: Batch 0/1
Batch Loss = 0.025703538840576735
469, epoch_train_loss=0.025703538840576735
Epoch 470
Epoch 470 :: Batch 0/1
Batch Loss = 0.02565345878607632
470, epoch_train_loss=0.02565345878607632
Epoch 471
Epoch 471 :: Batch 0/1
Batch Loss = 0.02560380942392062
471, epoch_train_loss=0.02560380942392062
Epoch 472
Epoch 472 :: Batch 0/1
Batch Loss = 0.02555457859053052
472, epoch_train_loss=0.02555457859053052
Epoch 473
Epoch 473 :: Batch 0/1
Batch Loss = 0.02550575240624894
473, epoch_train_loss=0.02550575240624894
Epoch 474
Epoch 474 :: Batch 0/1
Batch Loss = 0.025457315008093236
474, epoch_train_loss=0.025457315008093236
Epoch 475
Epoch 475 :: Batch 0/1
Batch Loss = 0.025409248234990946
475, epoch_train_loss=0.025409248234990946
Epoch 476
Epoch 476 :: Batch 0/1
Batch Loss = 0.02536153160720038
476, epoch_train_loss=0.02536153160720038
Epoch 477
Epoch 477 :: Batch 0/1
Batch Loss = 0.02531414357359112
477, epoch_train_loss=0.02531414357359112
Epoch 478
Epoch 478 :: Batch 0/1
Batch Loss = 0.02526706594387518
478, epoch_train_loss=0.02526706594387518
Epoch 479
Epoch 479 :: Batch 0/1
Batch Loss = 0.025220294946767693
479, epoch_train_loss=0.025220294946767693
Epoch 480
Epoch 480 :: Batch 0/1
Batch Loss = 0.025173862047654334
480, epoch_train_loss=0.025173862047654334
Epoch 481
Epoch 481 :: Batch 0/1
Batch Loss = 0.025127858782765734
481, epoch_train_loss=0.025127858782765734
Epoch 482
Epoch 482 :: Batch 0/1
Batch Loss = 0.025082432601990244
482, epoch_train_loss=0.025082432601990244
Epoch 483
Epoch 483 :: Batch 0/1
Batch Loss = 0.025037695295326935
483, epoch_train_loss=0.025037695295326935
Epoch 484
Epoch 484 :: Batch 0/1
Batch Loss = 0.024993563256944098
484, epoch_train_loss=0.024993563256944098
Epoch 485
Epoch 485 :: Batch 0/1
Batch Loss = 0.024949743721303493
485, epoch_train_loss=0.024949743721303493
Epoch 486
Epoch 486 :: Batch 0/1
Batch Loss = 0.024906000045493155
486, epoch_train_loss=0.024906000045493155
Epoch 487
Epoch 487 :: Batch 0/1
Batch Loss = 0.024862369642300812
487, epoch_train_loss=0.024862369642300812
Epoch 488
Epoch 488 :: Batch 0/1
Batch Loss = 0.02481904859843155
488, epoch_train_loss=0.02481904859843155
Epoch 489
Epoch 489 :: Batch 0/1
Batch Loss = 0.024776200963792152
489, epoch_train_loss=0.024776200963792152
Epoch 490
Epoch 490 :: Batch 0/1
Batch Loss = 0.02473389719925881
490, epoch_train_loss=0.02473389719925881
Epoch 491
Epoch 491 :: Batch 0/1
Batch Loss = 0.02469206119955765
491, epoch_train_loss=0.02469206119955765
Epoch 492
Epoch 492 :: Batch 0/1
Batch Loss = 0.02465049754594633
492, epoch_train_loss=0.02465049754594633
Epoch 493
Epoch 493 :: Batch 0/1
Batch Loss = 0.024609089343931054
493, epoch_train_loss=0.024609089343931054
Epoch 494
Epoch 494 :: Batch 0/1
Batch Loss = 0.02456789562289797
494, epoch_train_loss=0.02456789562289797
Epoch 495
Epoch 495 :: Batch 0/1
Batch Loss = 0.02452701246805563
495, epoch_train_loss=0.02452701246805563
Epoch 496
Epoch 496 :: Batch 0/1
Batch Loss = 0.024486473329857913
496, epoch_train_loss=0.024486473329857913
Epoch 497
Epoch 497 :: Batch 0/1
Batch Loss = 0.02444630163371265
497, epoch_train_loss=0.02444630163371265
Epoch 498
Epoch 498 :: Batch 0/1
Batch Loss = 0.024406525011476964
498, epoch_train_loss=0.024406525011476964
Epoch 499
Epoch 499 :: Batch 0/1
Batch Loss = 0.024367104713126118
499, epoch_train_loss=0.024367104713126118
Epoch 500
Epoch 500 :: Batch 0/1
Batch Loss = 0.0243279490890438
500, epoch_train_loss=0.0243279490890438
Epoch 501
Epoch 501 :: Batch 0/1
Batch Loss = 0.02428902852929032
501, epoch_train_loss=0.02428902852929032
Epoch 502
Epoch 502 :: Batch 0/1
Batch Loss = 0.024250398953731718
502, epoch_train_loss=0.024250398953731718
Epoch 503
Epoch 503 :: Batch 0/1
Batch Loss = 0.024212103474208994
503, epoch_train_loss=0.024212103474208994
Epoch 504
Epoch 504 :: Batch 0/1
Batch Loss = 0.024174125459324586
504, epoch_train_loss=0.024174125459324586
Epoch 505
Epoch 505 :: Batch 0/1
Batch Loss = 0.02413643672603075
505, epoch_train_loss=0.02413643672603075
Epoch 506
Epoch 506 :: Batch 0/1
Batch Loss = 0.024099028907038765
506, epoch_train_loss=0.024099028907038765
Epoch 507
Epoch 507 :: Batch 0/1
Batch Loss = 0.02406189417991596
507, epoch_train_loss=0.02406189417991596
Epoch 508
Epoch 508 :: Batch 0/1
Batch Loss = 0.02402502048199046
508, epoch_train_loss=0.02402502048199046
Epoch 509
Epoch 509 :: Batch 0/1
Batch Loss = 0.02398841602515753
509, epoch_train_loss=0.02398841602515753
Epoch 510
Epoch 510 :: Batch 0/1
Batch Loss = 0.023952106243164507
510, epoch_train_loss=0.023952106243164507
Epoch 511
Epoch 511 :: Batch 0/1
Batch Loss = 0.023916099705194538
511, epoch_train_loss=0.023916099705194538
Epoch 512
Epoch 512 :: Batch 0/1
Batch Loss = 0.02388037768516758
512, epoch_train_loss=0.02388037768516758
Epoch 513
Epoch 513 :: Batch 0/1
Batch Loss = 0.023844918061876483
513, epoch_train_loss=0.023844918061876483
Epoch 514
Epoch 514 :: Batch 0/1
Batch Loss = 0.023809713388521253
514, epoch_train_loss=0.023809713388521253
Epoch 515
Epoch 515 :: Batch 0/1
Batch Loss = 0.02377476698209951
515, epoch_train_loss=0.02377476698209951
Epoch 516
Epoch 516 :: Batch 0/1
Batch Loss = 0.023740083958583603
516, epoch_train_loss=0.023740083958583603
Epoch 517
Epoch 517 :: Batch 0/1
Batch Loss = 0.02370566823399074
517, epoch_train_loss=0.02370566823399074
Epoch 518
Epoch 518 :: Batch 0/1
Batch Loss = 0.02367151868917303
518, epoch_train_loss=0.02367151868917303
Epoch 519
Epoch 519 :: Batch 0/1
Batch Loss = 0.023637625726355777
519, epoch_train_loss=0.023637625726355777
Epoch 520
Epoch 520 :: Batch 0/1
Batch Loss = 0.02360397713675823
520, epoch_train_loss=0.02360397713675823
Epoch 521
Epoch 521 :: Batch 0/1
Batch Loss = 0.023570569146393643
521, epoch_train_loss=0.023570569146393643
Epoch 522
Epoch 522 :: Batch 0/1
Batch Loss = 0.023537407323608123
522, epoch_train_loss=0.023537407323608123
Epoch 523
Epoch 523 :: Batch 0/1
Batch Loss = 0.023504496306490517
523, epoch_train_loss=0.023504496306490517
Epoch 524
Epoch 524 :: Batch 0/1
Batch Loss = 0.023471832908042677
524, epoch_train_loss=0.023471832908042677
Epoch 525
Epoch 525 :: Batch 0/1
Batch Loss = 0.02343941007490587
525, epoch_train_loss=0.02343941007490587
Epoch 526
Epoch 526 :: Batch 0/1
Batch Loss = 0.023407222546624088
526, epoch_train_loss=0.023407222546624088
Epoch 527
Epoch 527 :: Batch 0/1
Batch Loss = 0.023375267400982305
527, epoch_train_loss=0.023375267400982305
Epoch 528
Epoch 528 :: Batch 0/1
Batch Loss = 0.023343543088560032
528, epoch_train_loss=0.023343543088560032
Epoch 529
Epoch 529 :: Batch 0/1
Batch Loss = 0.023312049593094197
529, epoch_train_loss=0.023312049593094197
Epoch 530
Epoch 530 :: Batch 0/1
Batch Loss = 0.023280786366772572
530, epoch_train_loss=0.023280786366772572
Epoch 531
Epoch 531 :: Batch 0/1
Batch Loss = 0.02324974925690077
531, epoch_train_loss=0.02324974925690077
Epoch 532
Epoch 532 :: Batch 0/1
Batch Loss = 0.023218932027902203
532, epoch_train_loss=0.023218932027902203
Epoch 533
Epoch 533 :: Batch 0/1
Batch Loss = 0.02318833113672561
533, epoch_train_loss=0.02318833113672561
Epoch 534
Epoch 534 :: Batch 0/1
Batch Loss = 0.023157946495877022
534, epoch_train_loss=0.023157946495877022
Epoch 535
Epoch 535 :: Batch 0/1
Batch Loss = 0.023127777915743112
535, epoch_train_loss=0.023127777915743112
Epoch 536
Epoch 536 :: Batch 0/1
Batch Loss = 0.023097823102984683
536, epoch_train_loss=0.023097823102984683
Epoch 537
Epoch 537 :: Batch 0/1
Batch Loss = 0.02306807861949017
537, epoch_train_loss=0.02306807861949017
Epoch 538
Epoch 538 :: Batch 0/1
Batch Loss = 0.02303854086430652
538, epoch_train_loss=0.02303854086430652
Epoch 539
Epoch 539 :: Batch 0/1
Batch Loss = 0.023009206532890892
539, epoch_train_loss=0.023009206532890892
Epoch 540
Epoch 540 :: Batch 0/1
Batch Loss = 0.02298007353153892
540, epoch_train_loss=0.02298007353153892
Epoch 541
Epoch 541 :: Batch 0/1
Batch Loss = 0.02295114096613791
541, epoch_train_loss=0.02295114096613791
Epoch 542
Epoch 542 :: Batch 0/1
Batch Loss = 0.022922407396008905
542, epoch_train_loss=0.022922407396008905
Epoch 543
Epoch 543 :: Batch 0/1
Batch Loss = 0.022893869739176176
543, epoch_train_loss=0.022893869739176176
Epoch 544
Epoch 544 :: Batch 0/1
Batch Loss = 0.022865524443780722
544, epoch_train_loss=0.022865524443780722
Epoch 545
Epoch 545 :: Batch 0/1
Batch Loss = 0.02283736890188744
545, epoch_train_loss=0.02283736890188744
Epoch 546
Epoch 546 :: Batch 0/1
Batch Loss = 0.02280940131384879
546, epoch_train_loss=0.02280940131384879
Epoch 547
Epoch 547 :: Batch 0/1
Batch Loss = 0.02278161986016547
547, epoch_train_loss=0.02278161986016547
Epoch 548
Epoch 548 :: Batch 0/1
Batch Loss = 0.02275402248668588
548, epoch_train_loss=0.02275402248668588
Epoch 549
Epoch 549 :: Batch 0/1
Batch Loss = 0.0227266069519231
549, epoch_train_loss=0.0227266069519231
Epoch 550
Epoch 550 :: Batch 0/1
Batch Loss = 0.02269937080558984
550, epoch_train_loss=0.02269937080558984
Epoch 551
Epoch 551 :: Batch 0/1
Batch Loss = 0.022672311677772557
551, epoch_train_loss=0.022672311677772557
Epoch 552
Epoch 552 :: Batch 0/1
Batch Loss = 0.02264542768609245
552, epoch_train_loss=0.02264542768609245
Epoch 553
Epoch 553 :: Batch 0/1
Batch Loss = 0.022618717183241245
553, epoch_train_loss=0.022618717183241245
Epoch 554
Epoch 554 :: Batch 0/1
Batch Loss = 0.02259217818229128
554, epoch_train_loss=0.02259217818229128
Epoch 555
Epoch 555 :: Batch 0/1
Batch Loss = 0.022565808368951844
555, epoch_train_loss=0.022565808368951844
Epoch 556
Epoch 556 :: Batch 0/1
Batch Loss = 0.022539605613808692
556, epoch_train_loss=0.022539605613808692
Epoch 557
Epoch 557 :: Batch 0/1
Batch Loss = 0.022513568141834295
557, epoch_train_loss=0.022513568141834295
Epoch 558
Epoch 558 :: Batch 0/1
Batch Loss = 0.02248769428444289
558, epoch_train_loss=0.02248769428444289
Epoch 559
Epoch 559 :: Batch 0/1
Batch Loss = 0.02246198232924297
559, epoch_train_loss=0.02246198232924297
Epoch 560
Epoch 560 :: Batch 0/1
Batch Loss = 0.022436430492836196
560, epoch_train_loss=0.022436430492836196
Epoch 561
Epoch 561 :: Batch 0/1
Batch Loss = 0.022411036903353394
561, epoch_train_loss=0.022411036903353394
Epoch 562
Epoch 562 :: Batch 0/1
Batch Loss = 0.022385799719694774
562, epoch_train_loss=0.022385799719694774
Epoch 563
Epoch 563 :: Batch 0/1
Batch Loss = 0.02236071733861551
563, epoch_train_loss=0.02236071733861551
Epoch 564
Epoch 564 :: Batch 0/1
Batch Loss = 0.022335788365347106
564, epoch_train_loss=0.022335788365347106
Epoch 565
Epoch 565 :: Batch 0/1
Batch Loss = 0.02231101136590565
565, epoch_train_loss=0.02231101136590565
Epoch 566
Epoch 566 :: Batch 0/1
Batch Loss = 0.022286384768035895
566, epoch_train_loss=0.022286384768035895
Epoch 567
Epoch 567 :: Batch 0/1
Batch Loss = 0.022261907013128558
567, epoch_train_loss=0.022261907013128558
Epoch 568
Epoch 568 :: Batch 0/1
Batch Loss = 0.022237576653003105
568, epoch_train_loss=0.022237576653003105
Epoch 569
Epoch 569 :: Batch 0/1
Batch Loss = 0.022213392326693376
569, epoch_train_loss=0.022213392326693376
Epoch 570
Epoch 570 :: Batch 0/1
Batch Loss = 0.022189352737762975
570, epoch_train_loss=0.022189352737762975
Epoch 571
Epoch 571 :: Batch 0/1
Batch Loss = 0.022165456622863698
571, epoch_train_loss=0.022165456622863698
Epoch 572
Epoch 572 :: Batch 0/1
Batch Loss = 0.022141702707360083
572, epoch_train_loss=0.022141702707360083
Epoch 573
Epoch 573 :: Batch 0/1
Batch Loss = 0.022118089715575166
573, epoch_train_loss=0.022118089715575166
Epoch 574
Epoch 574 :: Batch 0/1
Batch Loss = 0.022094616446559245
574, epoch_train_loss=0.022094616446559245
Epoch 575
Epoch 575 :: Batch 0/1
Batch Loss = 0.02207128178303844
575, epoch_train_loss=0.02207128178303844
Epoch 576
Epoch 576 :: Batch 0/1
Batch Loss = 0.02204808462084849
576, epoch_train_loss=0.02204808462084849
Epoch 577
Epoch 577 :: Batch 0/1
Batch Loss = 0.022025023835238612
577, epoch_train_loss=0.022025023835238612
Epoch 578
Epoch 578 :: Batch 0/1
Batch Loss = 0.022002098317894472
578, epoch_train_loss=0.022002098317894472
Epoch 579
Epoch 579 :: Batch 0/1
Batch Loss = 0.021979307012455852
579, epoch_train_loss=0.021979307012455852
Epoch 580
Epoch 580 :: Batch 0/1
Batch Loss = 0.021956648883722795
580, epoch_train_loss=0.021956648883722795
Epoch 581
Epoch 581 :: Batch 0/1
Batch Loss = 0.02193412292956482
581, epoch_train_loss=0.02193412292956482
Epoch 582
Epoch 582 :: Batch 0/1
Batch Loss = 0.021911728138331008
582, epoch_train_loss=0.021911728138331008
Epoch 583
Epoch 583 :: Batch 0/1
Batch Loss = 0.02188946350624689
583, epoch_train_loss=0.02188946350624689
Epoch 584
Epoch 584 :: Batch 0/1
Batch Loss = 0.021867327994259812
584, epoch_train_loss=0.021867327994259812
Epoch 585
Epoch 585 :: Batch 0/1
Batch Loss = 0.021845320638490113
585, epoch_train_loss=0.021845320638490113
Epoch 586
Epoch 586 :: Batch 0/1
Batch Loss = 0.021823440459208247
586, epoch_train_loss=0.021823440459208247
Epoch 587
Epoch 587 :: Batch 0/1
Batch Loss = 0.021801686598371214
587, epoch_train_loss=0.021801686598371214
Epoch 588
Epoch 588 :: Batch 0/1
Batch Loss = 0.02178005816012385
588, epoch_train_loss=0.02178005816012385
Epoch 589
Epoch 589 :: Batch 0/1
Batch Loss = 0.021758554656061718
589, epoch_train_loss=0.021758554656061718
Epoch 590
Epoch 590 :: Batch 0/1
Batch Loss = 0.021737175893506475
590, epoch_train_loss=0.021737175893506475
Epoch 591
Epoch 591 :: Batch 0/1
Batch Loss = 0.021715923384330538
591, epoch_train_loss=0.021715923384330538
Epoch 592
Epoch 592 :: Batch 0/1
Batch Loss = 0.02169480120001382
592, epoch_train_loss=0.02169480120001382
Epoch 593
Epoch 593 :: Batch 0/1
Batch Loss = 0.021673821811279786
593, epoch_train_loss=0.021673821811279786
Epoch 594
Epoch 594 :: Batch 0/1
Batch Loss = 0.021653014571490322
594, epoch_train_loss=0.021653014571490322
Epoch 595
Epoch 595 :: Batch 0/1
Batch Loss = 0.021632455693288782
595, epoch_train_loss=0.021632455693288782
Epoch 596
Epoch 596 :: Batch 0/1
Batch Loss = 0.02161232833811672
596, epoch_train_loss=0.02161232833811672
Epoch 597
Epoch 597 :: Batch 0/1
Batch Loss = 0.021593099930553864
597, epoch_train_loss=0.021593099930553864
Epoch 598
Epoch 598 :: Batch 0/1
Batch Loss = 0.02157592143322989
598, epoch_train_loss=0.02157592143322989
Epoch 599
Epoch 599 :: Batch 0/1
Batch Loss = 0.02156378482021813
599, epoch_train_loss=0.02156378482021813
Epoch 600
Epoch 600 :: Batch 0/1
Batch Loss = 0.021564145536995965
600, epoch_train_loss=0.021564145536995965
Epoch 601
Epoch 601 :: Batch 0/1
Batch Loss = 0.021597169744407434
601, epoch_train_loss=0.021597169744407434
Epoch 602
Epoch 602 :: Batch 0/1
Batch Loss = 0.021712514023380983
602, epoch_train_loss=0.021712514023380983
Epoch 603
Epoch 603 :: Batch 0/1
Batch Loss = 0.022054819140280017
603, epoch_train_loss=0.022054819140280017
Epoch 604
Epoch 604 :: Batch 0/1
Batch Loss = 0.022957954627140124
604, epoch_train_loss=0.022957954627140124
Epoch 605
Epoch 605 :: Batch 0/1
Batch Loss = 0.02555272339468041
605, epoch_train_loss=0.02555272339468041
Epoch 606
Epoch 606 :: Batch 0/1
Batch Loss = 0.032010247828599604
606, epoch_train_loss=0.032010247828599604
Epoch 607
Epoch 607 :: Batch 0/1
Batch Loss = 0.051990517659621455
607, epoch_train_loss=0.051990517659621455
Epoch 608
Epoch 608 :: Batch 0/1
Batch Loss = 0.0968149973947656
608, epoch_train_loss=0.0968149973947656
Epoch 609
Epoch 609 :: Batch 0/1
Batch Loss = 0.22405471883295425
609, epoch_train_loss=0.22405471883295425
Epoch 610
Epoch 610 :: Batch 0/1
Batch Loss = 0.37173062555208586
610, epoch_train_loss=0.37173062555208586
Epoch 611
Epoch 611 :: Batch 0/1
Batch Loss = 0.5347935517616675
611, epoch_train_loss=0.5347935517616675
Epoch 612
Epoch 612 :: Batch 0/1
Batch Loss = 0.189149669218756
612, epoch_train_loss=0.189149669218756
Epoch 613
Epoch 613 :: Batch 0/1
Batch Loss = 0.04304440772682109
613, epoch_train_loss=0.04304440772682109
Epoch 614
Epoch 614 :: Batch 0/1
Batch Loss = 0.2401075335074447
614, epoch_train_loss=0.2401075335074447
Epoch 615
Epoch 615 :: Batch 0/1
Batch Loss = 0.12075889966282449
615, epoch_train_loss=0.12075889966282449
Epoch 616
Epoch 616 :: Batch 0/1
Batch Loss = 0.08419349949474726
616, epoch_train_loss=0.08419349949474726
Epoch 617
Epoch 617 :: Batch 0/1
Batch Loss = 0.16608830211647801
617, epoch_train_loss=0.16608830211647801
Epoch 618
Epoch 618 :: Batch 0/1
Batch Loss = 0.048138942500324015
618, epoch_train_loss=0.048138942500324015
Epoch 619
Epoch 619 :: Batch 0/1
Batch Loss = 0.14070684503907777
619, epoch_train_loss=0.14070684503907777
Epoch 620
Epoch 620 :: Batch 0/1
Batch Loss = 0.0647922821578013
620, epoch_train_loss=0.0647922821578013
Epoch 621
Epoch 621 :: Batch 0/1
Batch Loss = 0.08379973346951386
621, epoch_train_loss=0.08379973346951386
Epoch 622
Epoch 622 :: Batch 0/1
Batch Loss = 0.09119969944454755
622, epoch_train_loss=0.09119969944454755
Epoch 623
Epoch 623 :: Batch 0/1
Batch Loss = 0.039430265363384086
623, epoch_train_loss=0.039430265363384086
Epoch 624
Epoch 624 :: Batch 0/1
Batch Loss = 0.08658093039214987
624, epoch_train_loss=0.08658093039214987
Epoch 625
Epoch 625 :: Batch 0/1
Batch Loss = 0.03444934508122992
625, epoch_train_loss=0.03444934508122992
Epoch 626
Epoch 626 :: Batch 0/1
Batch Loss = 0.06701831098272515
626, epoch_train_loss=0.06701831098272515
Epoch 627
Epoch 627 :: Batch 0/1
Batch Loss = 0.052660646235747295
627, epoch_train_loss=0.052660646235747295
Epoch 628
Epoch 628 :: Batch 0/1
Batch Loss = 0.04212078453294796
628, epoch_train_loss=0.04212078453294796
Epoch 629
Epoch 629 :: Batch 0/1
Batch Loss = 0.0640776804846293
629, epoch_train_loss=0.0640776804846293
Epoch 630
Epoch 630 :: Batch 0/1
Batch Loss = 0.03130262964309164
630, epoch_train_loss=0.03130262964309164
Epoch 631
Epoch 631 :: Batch 0/1
Batch Loss = 0.053924930048368264
631, epoch_train_loss=0.053924930048368264
Epoch 632
Epoch 632 :: Batch 0/1
Batch Loss = 0.03964618782899242
632, epoch_train_loss=0.03964618782899242
Epoch 633
Epoch 633 :: Batch 0/1
Batch Loss = 0.0358837624881884
633, epoch_train_loss=0.0358837624881884
Epoch 634
Epoch 634 :: Batch 0/1
Batch Loss = 0.04873024283165739
634, epoch_train_loss=0.04873024283165739
Epoch 635
Epoch 635 :: Batch 0/1
Batch Loss = 0.029132111252724767
635, epoch_train_loss=0.029132111252724767
Epoch 636
Epoch 636 :: Batch 0/1
Batch Loss = 0.04372430407706148
636, epoch_train_loss=0.04372430407706148
Epoch 637
Epoch 637 :: Batch 0/1
Batch Loss = 0.03657315807406138
637, epoch_train_loss=0.03657315807406138
Epoch 638
Epoch 638 :: Batch 0/1
Batch Loss = 0.030621510728995847
638, epoch_train_loss=0.030621510728995847
Epoch 639
Epoch 639 :: Batch 0/1
Batch Loss = 0.041682761596092195
639, epoch_train_loss=0.041682761596092195
Epoch 640
Epoch 640 :: Batch 0/1
Batch Loss = 0.02833035018733458
640, epoch_train_loss=0.02833035018733458
Epoch 641
Epoch 641 :: Batch 0/1
Batch Loss = 0.03439111509516492
641, epoch_train_loss=0.03439111509516492
Epoch 642
Epoch 642 :: Batch 0/1
Batch Loss = 0.03419336993750046
642, epoch_train_loss=0.03419336993750046
Epoch 643
Epoch 643 :: Batch 0/1
Batch Loss = 0.02728072500541673
643, epoch_train_loss=0.02728072500541673
Epoch 644
Epoch 644 :: Batch 0/1
Batch Loss = 0.034505473470342954
644, epoch_train_loss=0.034505473470342954
Epoch 645
Epoch 645 :: Batch 0/1
Batch Loss = 0.028438316674158203
645, epoch_train_loss=0.028438316674158203
Epoch 646
Epoch 646 :: Batch 0/1
Batch Loss = 0.029024588820915367
646, epoch_train_loss=0.029024588820915367
Epoch 647
Epoch 647 :: Batch 0/1
Batch Loss = 0.031673922740651214
647, epoch_train_loss=0.031673922740651214
Epoch 648
Epoch 648 :: Batch 0/1
Batch Loss = 0.026551466875991175
648, epoch_train_loss=0.026551466875991175
Epoch 649
Epoch 649 :: Batch 0/1
Batch Loss = 0.02978816620250012
649, epoch_train_loss=0.02978816620250012
Epoch 650
Epoch 650 :: Batch 0/1
Batch Loss = 0.028986550805924802
650, epoch_train_loss=0.028986550805924802
Epoch 651
Epoch 651 :: Batch 0/1
Batch Loss = 0.02624456091497538
651, epoch_train_loss=0.02624456091497538
Epoch 652
Epoch 652 :: Batch 0/1
Batch Loss = 0.029587141621248123
652, epoch_train_loss=0.029587141621248123
Epoch 653
Epoch 653 :: Batch 0/1
Batch Loss = 0.026584451410797863
653, epoch_train_loss=0.026584451410797863
Epoch 654
Epoch 654 :: Batch 0/1
Batch Loss = 0.02666571221998002
654, epoch_train_loss=0.02666571221998002
Epoch 655
Epoch 655 :: Batch 0/1
Batch Loss = 0.027893967762590247
655, epoch_train_loss=0.027893967762590247
Epoch 656
Epoch 656 :: Batch 0/1
Batch Loss = 0.025397200795633696
656, epoch_train_loss=0.025397200795633696
Epoch 657
Epoch 657 :: Batch 0/1
Batch Loss = 0.026474200375414446
657, epoch_train_loss=0.026474200375414446
Epoch 658
Epoch 658 :: Batch 0/1
Batch Loss = 0.026267557325763743
658, epoch_train_loss=0.026267557325763743
Epoch 659
Epoch 659 :: Batch 0/1
Batch Loss = 0.024686605101838333
659, epoch_train_loss=0.024686605101838333
Epoch 660
Epoch 660 :: Batch 0/1
Batch Loss = 0.025947746089719514
660, epoch_train_loss=0.025947746089719514
Epoch 661
Epoch 661 :: Batch 0/1
Batch Loss = 0.024753384024440443
661, epoch_train_loss=0.024753384024440443
Epoch 662
Epoch 662 :: Batch 0/1
Batch Loss = 0.024374696978812165
662, epoch_train_loss=0.024374696978812165
Epoch 663
Epoch 663 :: Batch 0/1
Batch Loss = 0.024947624710692208
663, epoch_train_loss=0.024947624710692208
Epoch 664
Epoch 664 :: Batch 0/1
Batch Loss = 0.02376568706164303
664, epoch_train_loss=0.02376568706164303
Epoch 665
Epoch 665 :: Batch 0/1
Batch Loss = 0.023917084747141177
665, epoch_train_loss=0.023917084747141177
Epoch 666
Epoch 666 :: Batch 0/1
Batch Loss = 0.024002523901367822
666, epoch_train_loss=0.024002523901367822
Epoch 667
Epoch 667 :: Batch 0/1
Batch Loss = 0.023091666751013203
667, epoch_train_loss=0.023091666751013203
Epoch 668
Epoch 668 :: Batch 0/1
Batch Loss = 0.02349558997162623
668, epoch_train_loss=0.02349558997162623
Epoch 669
Epoch 669 :: Batch 0/1
Batch Loss = 0.023234615709406974
669, epoch_train_loss=0.023234615709406974
Epoch 670
Epoch 670 :: Batch 0/1
Batch Loss = 0.022713511162491932
670, epoch_train_loss=0.022713511162491932
Epoch 671
Epoch 671 :: Batch 0/1
Batch Loss = 0.023036753367779335
671, epoch_train_loss=0.023036753367779335
Epoch 672
Epoch 672 :: Batch 0/1
Batch Loss = 0.022710006210373515
672, epoch_train_loss=0.022710006210373515
Epoch 673
Epoch 673 :: Batch 0/1
Batch Loss = 0.02235930480583796
673, epoch_train_loss=0.02235930480583796
Epoch 674
Epoch 674 :: Batch 0/1
Batch Loss = 0.022592915839857362
674, epoch_train_loss=0.022592915839857362
Epoch 675
Epoch 675 :: Batch 0/1
Batch Loss = 0.022237339443593032
675, epoch_train_loss=0.022237339443593032
Epoch 676
Epoch 676 :: Batch 0/1
Batch Loss = 0.021996706766393113
676, epoch_train_loss=0.021996706766393113
Epoch 677
Epoch 677 :: Batch 0/1
Batch Loss = 0.022138430621368128
677, epoch_train_loss=0.022138430621368128
Epoch 678
Epoch 678 :: Batch 0/1
Batch Loss = 0.02189680307539482
678, epoch_train_loss=0.02189680307539482
Epoch 679
Epoch 679 :: Batch 0/1
Batch Loss = 0.021691489302632018
679, epoch_train_loss=0.021691489302632018
Epoch 680
Epoch 680 :: Batch 0/1
Batch Loss = 0.021827317520060976
680, epoch_train_loss=0.021827317520060976
Epoch 681
Epoch 681 :: Batch 0/1
Batch Loss = 0.021684566427470218
681, epoch_train_loss=0.021684566427470218
Epoch 682
Epoch 682 :: Batch 0/1
Batch Loss = 0.02150016591328555
682, epoch_train_loss=0.02150016591328555
Epoch 683
Epoch 683 :: Batch 0/1
Batch Loss = 0.021569841689855714
683, epoch_train_loss=0.021569841689855714
Epoch 684
Epoch 684 :: Batch 0/1
Batch Loss = 0.021528197154596633
684, epoch_train_loss=0.021528197154596633
Epoch 685
Epoch 685 :: Batch 0/1
Batch Loss = 0.021344277984814978
685, epoch_train_loss=0.021344277984814978
Epoch 686
Epoch 686 :: Batch 0/1
Batch Loss = 0.02135991057263839
686, epoch_train_loss=0.02135991057263839
Epoch 687
Epoch 687 :: Batch 0/1
Batch Loss = 0.021369595635974646
687, epoch_train_loss=0.021369595635974646
Epoch 688
Epoch 688 :: Batch 0/1
Batch Loss = 0.021239317936000864
688, epoch_train_loss=0.021239317936000864
Epoch 689
Epoch 689 :: Batch 0/1
Batch Loss = 0.02118745972539026
689, epoch_train_loss=0.02118745972539026
Epoch 690
Epoch 690 :: Batch 0/1
Batch Loss = 0.0212226725805743
690, epoch_train_loss=0.0212226725805743
Epoch 691
Epoch 691 :: Batch 0/1
Batch Loss = 0.021145773766188668
691, epoch_train_loss=0.021145773766188668
Epoch 692
Epoch 692 :: Batch 0/1
Batch Loss = 0.021068601315731628
692, epoch_train_loss=0.021068601315731628
Epoch 693
Epoch 693 :: Batch 0/1
Batch Loss = 0.021079308667247616
693, epoch_train_loss=0.021079308667247616
Epoch 694
Epoch 694 :: Batch 0/1
Batch Loss = 0.021061663307058588
694, epoch_train_loss=0.021061663307058588
Epoch 695
Epoch 695 :: Batch 0/1
Batch Loss = 0.02098169609001201
695, epoch_train_loss=0.02098169609001201
Epoch 696
Epoch 696 :: Batch 0/1
Batch Loss = 0.020958615148529158
696, epoch_train_loss=0.020958615148529158
Epoch 697
Epoch 697 :: Batch 0/1
Batch Loss = 0.020957535378353983
697, epoch_train_loss=0.020957535378353983
Epoch 698
Epoch 698 :: Batch 0/1
Batch Loss = 0.020906408372446107
698, epoch_train_loss=0.020906408372446107
Epoch 699
Epoch 699 :: Batch 0/1
Batch Loss = 0.020851278615785063
699, epoch_train_loss=0.020851278615785063
Epoch 700
Epoch 700 :: Batch 0/1
Batch Loss = 0.020841703564910843
700, epoch_train_loss=0.020841703564910843
Epoch 701
Epoch 701 :: Batch 0/1
Batch Loss = 0.02081695987001212
701, epoch_train_loss=0.02081695987001212
Epoch 702
Epoch 702 :: Batch 0/1
Batch Loss = 0.020763587095859647
702, epoch_train_loss=0.020763587095859647
Epoch 703
Epoch 703 :: Batch 0/1
Batch Loss = 0.020730610275609036
703, epoch_train_loss=0.020730610275609036
Epoch 704
Epoch 704 :: Batch 0/1
Batch Loss = 0.0207178648795452
704, epoch_train_loss=0.0207178648795452
Epoch 705
Epoch 705 :: Batch 0/1
Batch Loss = 0.020683690924535637
705, epoch_train_loss=0.020683690924535637
Epoch 706
Epoch 706 :: Batch 0/1
Batch Loss = 0.020641000898477816
706, epoch_train_loss=0.020641000898477816
Epoch 707
Epoch 707 :: Batch 0/1
Batch Loss = 0.02061864127023871
707, epoch_train_loss=0.02061864127023871
Epoch 708
Epoch 708 :: Batch 0/1
Batch Loss = 0.020599617042461823
708, epoch_train_loss=0.020599617042461823
Epoch 709
Epoch 709 :: Batch 0/1
Batch Loss = 0.02056438332035436
709, epoch_train_loss=0.02056438332035436
Epoch 710
Epoch 710 :: Batch 0/1
Batch Loss = 0.020529683945534977
710, epoch_train_loss=0.020529683945534977
Epoch 711
Epoch 711 :: Batch 0/1
Batch Loss = 0.020509189317317204
711, epoch_train_loss=0.020509189317317204
Epoch 712
Epoch 712 :: Batch 0/1
Batch Loss = 0.020486105450481164
712, epoch_train_loss=0.020486105450481164
Epoch 713
Epoch 713 :: Batch 0/1
Batch Loss = 0.02045354981382387
713, epoch_train_loss=0.02045354981382387
Epoch 714
Epoch 714 :: Batch 0/1
Batch Loss = 0.020424417735138332
714, epoch_train_loss=0.020424417735138332
Epoch 715
Epoch 715 :: Batch 0/1
Batch Loss = 0.020404181707839177
715, epoch_train_loss=0.020404181707839177
Epoch 716
Epoch 716 :: Batch 0/1
Batch Loss = 0.02038016073918583
716, epoch_train_loss=0.02038016073918583
Epoch 717
Epoch 717 :: Batch 0/1
Batch Loss = 0.02035077830172387
717, epoch_train_loss=0.02035077830172387
Epoch 718
Epoch 718 :: Batch 0/1
Batch Loss = 0.020324722019194705
718, epoch_train_loss=0.020324722019194705
Epoch 719
Epoch 719 :: Batch 0/1
Batch Loss = 0.020304119799542882
719, epoch_train_loss=0.020304119799542882
Epoch 720
Epoch 720 :: Batch 0/1
Batch Loss = 0.020280496532150912
720, epoch_train_loss=0.020280496532150912
Epoch 721
Epoch 721 :: Batch 0/1
Batch Loss = 0.02025363171467588
721, epoch_train_loss=0.02025363171467588
Epoch 722
Epoch 722 :: Batch 0/1
Batch Loss = 0.020229113636691547
722, epoch_train_loss=0.020229113636691547
Epoch 723
Epoch 723 :: Batch 0/1
Batch Loss = 0.020207925435595717
723, epoch_train_loss=0.020207925435595717
Epoch 724
Epoch 724 :: Batch 0/1
Batch Loss = 0.020184799798012822
724, epoch_train_loss=0.020184799798012822
Epoch 725
Epoch 725 :: Batch 0/1
Batch Loss = 0.020159457667183533
725, epoch_train_loss=0.020159457667183533
Epoch 726
Epoch 726 :: Batch 0/1
Batch Loss = 0.0201356398657238
726, epoch_train_loss=0.0201356398657238
Epoch 727
Epoch 727 :: Batch 0/1
Batch Loss = 0.020114195562319248
727, epoch_train_loss=0.020114195562319248
Epoch 728
Epoch 728 :: Batch 0/1
Batch Loss = 0.02009202835202173
728, epoch_train_loss=0.02009202835202173
Epoch 729
Epoch 729 :: Batch 0/1
Batch Loss = 0.020068221329945863
729, epoch_train_loss=0.020068221329945863
Epoch 730
Epoch 730 :: Batch 0/1
Batch Loss = 0.020045101221324836
730, epoch_train_loss=0.020045101221324836
Epoch 731
Epoch 731 :: Batch 0/1
Batch Loss = 0.020023607681627417
731, epoch_train_loss=0.020023607681627417
Epoch 732
Epoch 732 :: Batch 0/1
Batch Loss = 0.020002199737912348
732, epoch_train_loss=0.020002199737912348
Epoch 733
Epoch 733 :: Batch 0/1
Batch Loss = 0.01997962637216662
733, epoch_train_loss=0.01997962637216662
Epoch 734
Epoch 734 :: Batch 0/1
Batch Loss = 0.019957075795405975
734, epoch_train_loss=0.019957075795405975
Epoch 735
Epoch 735 :: Batch 0/1
Batch Loss = 0.01993554854089283
735, epoch_train_loss=0.01993554854089283
Epoch 736
Epoch 736 :: Batch 0/1
Batch Loss = 0.01991462552439573
736, epoch_train_loss=0.01991462552439573
Epoch 737
Epoch 737 :: Batch 0/1
Batch Loss = 0.019893130869059156
737, epoch_train_loss=0.019893130869059156
Epoch 738
Epoch 738 :: Batch 0/1
Batch Loss = 0.01987130164120407
738, epoch_train_loss=0.01987130164120407
Epoch 739
Epoch 739 :: Batch 0/1
Batch Loss = 0.019849913530762892
739, epoch_train_loss=0.019849913530762892
Epoch 740
Epoch 740 :: Batch 0/1
Batch Loss = 0.019829249360735976
740, epoch_train_loss=0.019829249360735976
Epoch 741
Epoch 741 :: Batch 0/1
Batch Loss = 0.01980865049199745
741, epoch_train_loss=0.01980865049199745
Epoch 742
Epoch 742 :: Batch 0/1
Batch Loss = 0.019787760161687325
742, epoch_train_loss=0.019787760161687325
Epoch 743
Epoch 743 :: Batch 0/1
Batch Loss = 0.019766884645932438
743, epoch_train_loss=0.019766884645932438
Epoch 744
Epoch 744 :: Batch 0/1
Batch Loss = 0.01974647669751973
744, epoch_train_loss=0.01974647669751973
Epoch 745
Epoch 745 :: Batch 0/1
Batch Loss = 0.019726496891071487
745, epoch_train_loss=0.019726496891071487
Epoch 746
Epoch 746 :: Batch 0/1
Batch Loss = 0.01970655984085333
746, epoch_train_loss=0.01970655984085333
Epoch 747
Epoch 747 :: Batch 0/1
Batch Loss = 0.019686551086650954
747, epoch_train_loss=0.019686551086650954
Epoch 748
Epoch 748 :: Batch 0/1
Batch Loss = 0.019666668851328385
748, epoch_train_loss=0.019666668851328385
Epoch 749
Epoch 749 :: Batch 0/1
Batch Loss = 0.01964717789638952
749, epoch_train_loss=0.01964717789638952
Epoch 750
Epoch 750 :: Batch 0/1
Batch Loss = 0.019628006649790964
750, epoch_train_loss=0.019628006649790964
Epoch 751
Epoch 751 :: Batch 0/1
Batch Loss = 0.019608985232814586
751, epoch_train_loss=0.019608985232814586
Epoch 752
Epoch 752 :: Batch 0/1
Batch Loss = 0.019590014956374507
752, epoch_train_loss=0.019590014956374507
Epoch 753
Epoch 753 :: Batch 0/1
Batch Loss = 0.01957123511898669
753, epoch_train_loss=0.01957123511898669
Epoch 754
Epoch 754 :: Batch 0/1
Batch Loss = 0.019552770101033703
754, epoch_train_loss=0.019552770101033703
Epoch 755
Epoch 755 :: Batch 0/1
Batch Loss = 0.019534626209422357
755, epoch_train_loss=0.019534626209422357
Epoch 756
Epoch 756 :: Batch 0/1
Batch Loss = 0.01951668961204044
756, epoch_train_loss=0.01951668961204044
Epoch 757
Epoch 757 :: Batch 0/1
Batch Loss = 0.019498893734130447
757, epoch_train_loss=0.019498893734130447
Epoch 758
Epoch 758 :: Batch 0/1
Batch Loss = 0.01948128331744379
758, epoch_train_loss=0.01948128331744379
Epoch 759
Epoch 759 :: Batch 0/1
Batch Loss = 0.019463936029681113
759, epoch_train_loss=0.019463936029681113
Epoch 760
Epoch 760 :: Batch 0/1
Batch Loss = 0.019446883296593876
760, epoch_train_loss=0.019446883296593876
Epoch 761
Epoch 761 :: Batch 0/1
Batch Loss = 0.01943006442058227
761, epoch_train_loss=0.01943006442058227
Epoch 762
Epoch 762 :: Batch 0/1
Batch Loss = 0.019413431736694575
762, epoch_train_loss=0.019413431736694575
Epoch 763
Epoch 763 :: Batch 0/1
Batch Loss = 0.01939697148293487
763, epoch_train_loss=0.01939697148293487
Epoch 764
Epoch 764 :: Batch 0/1
Batch Loss = 0.01938072982988531
764, epoch_train_loss=0.01938072982988531
Epoch 765
Epoch 765 :: Batch 0/1
Batch Loss = 0.019364727397087833
765, epoch_train_loss=0.019364727397087833
Epoch 766
Epoch 766 :: Batch 0/1
Batch Loss = 0.01934894963554019
766, epoch_train_loss=0.01934894963554019
Epoch 767
Epoch 767 :: Batch 0/1
Batch Loss = 0.019333354454928814
767, epoch_train_loss=0.019333354454928814
Epoch 768
Epoch 768 :: Batch 0/1
Batch Loss = 0.01931791556783976
768, epoch_train_loss=0.01931791556783976
Epoch 769
Epoch 769 :: Batch 0/1
Batch Loss = 0.0193026409434724
769, epoch_train_loss=0.0193026409434724
Epoch 770
Epoch 770 :: Batch 0/1
Batch Loss = 0.019287543057723372
770, epoch_train_loss=0.019287543057723372
Epoch 771
Epoch 771 :: Batch 0/1
Batch Loss = 0.01927262386582332
771, epoch_train_loss=0.01927262386582332
Epoch 772
Epoch 772 :: Batch 0/1
Batch Loss = 0.019257858232220686
772, epoch_train_loss=0.019257858232220686
Epoch 773
Epoch 773 :: Batch 0/1
Batch Loss = 0.019243223796723243
773, epoch_train_loss=0.019243223796723243
Epoch 774
Epoch 774 :: Batch 0/1
Batch Loss = 0.019228708444382892
774, epoch_train_loss=0.019228708444382892
Epoch 775
Epoch 775 :: Batch 0/1
Batch Loss = 0.019214317418516997
775, epoch_train_loss=0.019214317418516997
Epoch 776
Epoch 776 :: Batch 0/1
Batch Loss = 0.01920005384609689
776, epoch_train_loss=0.01920005384609689
Epoch 777
Epoch 777 :: Batch 0/1
Batch Loss = 0.01918591091121302
777, epoch_train_loss=0.01918591091121302
Epoch 778
Epoch 778 :: Batch 0/1
Batch Loss = 0.01917187351510565
778, epoch_train_loss=0.01917187351510565
Epoch 779
Epoch 779 :: Batch 0/1
Batch Loss = 0.019157927561999043
779, epoch_train_loss=0.019157927561999043
Epoch 780
Epoch 780 :: Batch 0/1
Batch Loss = 0.019144069501315933
780, epoch_train_loss=0.019144069501315933
Epoch 781
Epoch 781 :: Batch 0/1
Batch Loss = 0.019130299619903993
781, epoch_train_loss=0.019130299619903993
Epoch 782
Epoch 782 :: Batch 0/1
Batch Loss = 0.019116619047527313
782, epoch_train_loss=0.019116619047527313
Epoch 783
Epoch 783 :: Batch 0/1
Batch Loss = 0.019103021426824907
783, epoch_train_loss=0.019103021426824907
Epoch 784
Epoch 784 :: Batch 0/1
Batch Loss = 0.019089498740486505
784, epoch_train_loss=0.019089498740486505
Epoch 785
Epoch 785 :: Batch 0/1
Batch Loss = 0.019076044012056204
785, epoch_train_loss=0.019076044012056204
Epoch 786
Epoch 786 :: Batch 0/1
Batch Loss = 0.019062654968127323
786, epoch_train_loss=0.019062654968127323
Epoch 787
Epoch 787 :: Batch 0/1
Batch Loss = 0.019049332457834182
787, epoch_train_loss=0.019049332457834182
Epoch 788
Epoch 788 :: Batch 0/1
Batch Loss = 0.019036075830552893
788, epoch_train_loss=0.019036075830552893
Epoch 789
Epoch 789 :: Batch 0/1
Batch Loss = 0.019022882732581042
789, epoch_train_loss=0.019022882732581042
Epoch 790
Epoch 790 :: Batch 0/1
Batch Loss = 0.019009748321759043
790, epoch_train_loss=0.019009748321759043
Epoch 791
Epoch 791 :: Batch 0/1
Batch Loss = 0.01899666931532536
791, epoch_train_loss=0.01899666931532536
Epoch 792
Epoch 792 :: Batch 0/1
Batch Loss = 0.018983643983113554
792, epoch_train_loss=0.018983643983113554
Epoch 793
Epoch 793 :: Batch 0/1
Batch Loss = 0.018970672379307517
793, epoch_train_loss=0.018970672379307517
Epoch 794
Epoch 794 :: Batch 0/1
Batch Loss = 0.018957754502555175
794, epoch_train_loss=0.018957754502555175
Epoch 795
Epoch 795 :: Batch 0/1
Batch Loss = 0.018944889097658114
795, epoch_train_loss=0.018944889097658114
Epoch 796
Epoch 796 :: Batch 0/1
Batch Loss = 0.018932074232186617
796, epoch_train_loss=0.018932074232186617
Epoch 797
Epoch 797 :: Batch 0/1
Batch Loss = 0.01891930764594705
797, epoch_train_loss=0.01891930764594705
Epoch 798
Epoch 798 :: Batch 0/1
Batch Loss = 0.01890658817265488
798, epoch_train_loss=0.01890658817265488
Epoch 799
Epoch 799 :: Batch 0/1
Batch Loss = 0.018893915286174436
799, epoch_train_loss=0.018893915286174436
Epoch 800
Epoch 800 :: Batch 0/1
Batch Loss = 0.018881288922881018
800, epoch_train_loss=0.018881288922881018
Epoch 801
Epoch 801 :: Batch 0/1
Batch Loss = 0.01886870869886147
801, epoch_train_loss=0.01886870869886147
Epoch 802
Epoch 802 :: Batch 0/1
Batch Loss = 0.018856173696270725
802, epoch_train_loss=0.018856173696270725
Epoch 803
Epoch 803 :: Batch 0/1
Batch Loss = 0.018843682769566834
803, epoch_train_loss=0.018843682769566834
Epoch 804
Epoch 804 :: Batch 0/1
Batch Loss = 0.018831234771292588
804, epoch_train_loss=0.018831234771292588
Epoch 805
Epoch 805 :: Batch 0/1
Batch Loss = 0.018818829007397887
805, epoch_train_loss=0.018818829007397887
Epoch 806
Epoch 806 :: Batch 0/1
Batch Loss = 0.01880646506549721
806, epoch_train_loss=0.01880646506549721
Epoch 807
Epoch 807 :: Batch 0/1
Batch Loss = 0.018794142693471787
807, epoch_train_loss=0.018794142693471787
Epoch 808
Epoch 808 :: Batch 0/1
Batch Loss = 0.01878186155404896
808, epoch_train_loss=0.01878186155404896
Epoch 809
Epoch 809 :: Batch 0/1
Batch Loss = 0.01876962109487425
809, epoch_train_loss=0.01876962109487425
Epoch 810
Epoch 810 :: Batch 0/1
Batch Loss = 0.018757420669959272
810, epoch_train_loss=0.018757420669959272
Epoch 811
Epoch 811 :: Batch 0/1
Batch Loss = 0.018745259589860983
811, epoch_train_loss=0.018745259589860983
Epoch 812
Epoch 812 :: Batch 0/1
Batch Loss = 0.01873313730792455
812, epoch_train_loss=0.01873313730792455
Epoch 813
Epoch 813 :: Batch 0/1
Batch Loss = 0.01872105341746649
813, epoch_train_loss=0.01872105341746649
Epoch 814
Epoch 814 :: Batch 0/1
Batch Loss = 0.018709007573630042
814, epoch_train_loss=0.018709007573630042
Epoch 815
Epoch 815 :: Batch 0/1
Batch Loss = 0.018696999499070555
815, epoch_train_loss=0.018696999499070555
Epoch 816
Epoch 816 :: Batch 0/1
Batch Loss = 0.01868502881540125
816, epoch_train_loss=0.01868502881540125
Epoch 817
Epoch 817 :: Batch 0/1
Batch Loss = 0.0186730951375468
817, epoch_train_loss=0.0186730951375468
Epoch 818
Epoch 818 :: Batch 0/1
Batch Loss = 0.018661198003900113
818, epoch_train_loss=0.018661198003900113
Epoch 819
Epoch 819 :: Batch 0/1
Batch Loss = 0.018649336977761995
819, epoch_train_loss=0.018649336977761995
Epoch 820
Epoch 820 :: Batch 0/1
Batch Loss = 0.018637511667492914
820, epoch_train_loss=0.018637511667492914
Epoch 821
Epoch 821 :: Batch 0/1
Batch Loss = 0.01862572168123623
821, epoch_train_loss=0.01862572168123623
Epoch 822
Epoch 822 :: Batch 0/1
Batch Loss = 0.018613966726690096
822, epoch_train_loss=0.018613966726690096
Epoch 823
Epoch 823 :: Batch 0/1
Batch Loss = 0.018602246452648873
823, epoch_train_loss=0.018602246452648873
Epoch 824
Epoch 824 :: Batch 0/1
Batch Loss = 0.018590560579951895
824, epoch_train_loss=0.018590560579951895
Epoch 825
Epoch 825 :: Batch 0/1
Batch Loss = 0.018578908755852476
825, epoch_train_loss=0.018578908755852476
Epoch 826
Epoch 826 :: Batch 0/1
Batch Loss = 0.01856729065645318
826, epoch_train_loss=0.01856729065645318
Epoch 827
Epoch 827 :: Batch 0/1
Batch Loss = 0.018555705938130457
827, epoch_train_loss=0.018555705938130457
Epoch 828
Epoch 828 :: Batch 0/1
Batch Loss = 0.018544154242729306
828, epoch_train_loss=0.018544154242729306
Epoch 829
Epoch 829 :: Batch 0/1
Batch Loss = 0.018532635254790087
829, epoch_train_loss=0.018532635254790087
Epoch 830
Epoch 830 :: Batch 0/1
Batch Loss = 0.01852114862083339
830, epoch_train_loss=0.01852114862083339
Epoch 831
Epoch 831 :: Batch 0/1
Batch Loss = 0.018509694053092596
831, epoch_train_loss=0.018509694053092596
Epoch 832
Epoch 832 :: Batch 0/1
Batch Loss = 0.01849827121862198
832, epoch_train_loss=0.01849827121862198
Epoch 833
Epoch 833 :: Batch 0/1
Batch Loss = 0.01848687984004544
833, epoch_train_loss=0.01848687984004544
Epoch 834
Epoch 834 :: Batch 0/1
Batch Loss = 0.018475519600984546
834, epoch_train_loss=0.018475519600984546
Epoch 835
Epoch 835 :: Batch 0/1
Batch Loss = 0.018464190217544314
835, epoch_train_loss=0.018464190217544314
Epoch 836
Epoch 836 :: Batch 0/1
Batch Loss = 0.018452891385565592
836, epoch_train_loss=0.018452891385565592
Epoch 837
Epoch 837 :: Batch 0/1
Batch Loss = 0.018441622817319314
837, epoch_train_loss=0.018441622817319314
Epoch 838
Epoch 838 :: Batch 0/1
Batch Loss = 0.018430384216699007
838, epoch_train_loss=0.018430384216699007
Epoch 839
Epoch 839 :: Batch 0/1
Batch Loss = 0.018419175296035734
839, epoch_train_loss=0.018419175296035734
Epoch 840
Epoch 840 :: Batch 0/1
Batch Loss = 0.01840799577036399
840, epoch_train_loss=0.01840799577036399
Epoch 841
Epoch 841 :: Batch 0/1
Batch Loss = 0.01839684535385443
841, epoch_train_loss=0.01839684535385443
Epoch 842
Epoch 842 :: Batch 0/1
Batch Loss = 0.018385723773445245
842, epoch_train_loss=0.018385723773445245
Epoch 843
Epoch 843 :: Batch 0/1
Batch Loss = 0.018374630745551696
843, epoch_train_loss=0.018374630745551696
Epoch 844
Epoch 844 :: Batch 0/1
Batch Loss = 0.018363566012448405
844, epoch_train_loss=0.018363566012448405
Epoch 845
Epoch 845 :: Batch 0/1
Batch Loss = 0.01835252929146292
845, epoch_train_loss=0.01835252929146292
Epoch 846
Epoch 846 :: Batch 0/1
Batch Loss = 0.0183415203353323
846, epoch_train_loss=0.0183415203353323
Epoch 847
Epoch 847 :: Batch 0/1
Batch Loss = 0.01833053886609826
847, epoch_train_loss=0.01833053886609826
Epoch 848
Epoch 848 :: Batch 0/1
Batch Loss = 0.018319584642903904
848, epoch_train_loss=0.018319584642903904
Epoch 849
Epoch 849 :: Batch 0/1
Batch Loss = 0.01830865739246894
849, epoch_train_loss=0.01830865739246894
Epoch 850
Epoch 850 :: Batch 0/1
Batch Loss = 0.018297756881453884
850, epoch_train_loss=0.018297756881453884
Epoch 851
Epoch 851 :: Batch 0/1
Batch Loss = 0.018286882841842877
851, epoch_train_loss=0.018286882841842877
Epoch 852
Epoch 852 :: Batch 0/1
Batch Loss = 0.018276035047095747
852, epoch_train_loss=0.018276035047095747
Epoch 853
Epoch 853 :: Batch 0/1
Batch Loss = 0.01826521323393828
853, epoch_train_loss=0.01826521323393828
Epoch 854
Epoch 854 :: Batch 0/1
Batch Loss = 0.018254417184062025
854, epoch_train_loss=0.018254417184062025
Epoch 855
Epoch 855 :: Batch 0/1
Batch Loss = 0.01824364663688657
855, epoch_train_loss=0.01824364663688657
Epoch 856
Epoch 856 :: Batch 0/1
Batch Loss = 0.018232901387075933
856, epoch_train_loss=0.018232901387075933
Epoch 857
Epoch 857 :: Batch 0/1
Batch Loss = 0.0182221811770937
857, epoch_train_loss=0.0182221811770937
Epoch 858
Epoch 858 :: Batch 0/1
Batch Loss = 0.018211485820977424
858, epoch_train_loss=0.018211485820977424
Epoch 859
Epoch 859 :: Batch 0/1
Batch Loss = 0.018200815065174913
859, epoch_train_loss=0.018200815065174913
Epoch 860
Epoch 860 :: Batch 0/1
Batch Loss = 0.018190168758637066
860, epoch_train_loss=0.018190168758637066
Epoch 861
Epoch 861 :: Batch 0/1
Batch Loss = 0.01817954666501472
861, epoch_train_loss=0.01817954666501472
Epoch 862
Epoch 862 :: Batch 0/1
Batch Loss = 0.01816894870342287
862, epoch_train_loss=0.01816894870342287
Epoch 863
Epoch 863 :: Batch 0/1
Batch Loss = 0.01815837469657784
863, epoch_train_loss=0.01815837469657784
Epoch 864
Epoch 864 :: Batch 0/1
Batch Loss = 0.01814782473358782
864, epoch_train_loss=0.01814782473358782
Epoch 865
Epoch 865 :: Batch 0/1
Batch Loss = 0.018137298827116816
865, epoch_train_loss=0.018137298827116816
Epoch 866
Epoch 866 :: Batch 0/1
Batch Loss = 0.01812679752301134
866, epoch_train_loss=0.01812679752301134
Epoch 867
Epoch 867 :: Batch 0/1
Batch Loss = 0.018116321461545615
867, epoch_train_loss=0.018116321461545615
Epoch 868
Epoch 868 :: Batch 0/1
Batch Loss = 0.018105872526768485
868, epoch_train_loss=0.018105872526768485
Epoch 869
Epoch 869 :: Batch 0/1
Batch Loss = 0.01809545345316818
869, epoch_train_loss=0.01809545345316818
Epoch 870
Epoch 870 :: Batch 0/1
Batch Loss = 0.01808507033969387
870, epoch_train_loss=0.01808507033969387
Epoch 871
Epoch 871 :: Batch 0/1
Batch Loss = 0.018074733059111732
871, epoch_train_loss=0.018074733059111732
Epoch 872
Epoch 872 :: Batch 0/1
Batch Loss = 0.01806446183455874
872, epoch_train_loss=0.01806446183455874
Epoch 873
Epoch 873 :: Batch 0/1
Batch Loss = 0.01805429164797883
873, epoch_train_loss=0.01805429164797883
Epoch 874
Epoch 874 :: Batch 0/1
Batch Loss = 0.01804429267138816
874, epoch_train_loss=0.01804429267138816
Epoch 875
Epoch 875 :: Batch 0/1
Batch Loss = 0.018034591260981062
875, epoch_train_loss=0.018034591260981062
Epoch 876
Epoch 876 :: Batch 0/1
Batch Loss = 0.01802544341882386
876, epoch_train_loss=0.01802544341882386
Epoch 877
Epoch 877 :: Batch 0/1
Batch Loss = 0.01801731963709876
877, epoch_train_loss=0.01801731963709876
Epoch 878
Epoch 878 :: Batch 0/1
Batch Loss = 0.018011202592918996
878, epoch_train_loss=0.018011202592918996
Epoch 879
Epoch 879 :: Batch 0/1
Batch Loss = 0.018008899983550077
879, epoch_train_loss=0.018008899983550077
Epoch 880
Epoch 880 :: Batch 0/1
Batch Loss = 0.018014387574240058
880, epoch_train_loss=0.018014387574240058
Epoch 881
Epoch 881 :: Batch 0/1
Batch Loss = 0.01803478986052478
881, epoch_train_loss=0.01803478986052478
Epoch 882
Epoch 882 :: Batch 0/1
Batch Loss = 0.018087139700923214
882, epoch_train_loss=0.018087139700923214
Epoch 883
Epoch 883 :: Batch 0/1
Batch Loss = 0.018199780638095794
883, epoch_train_loss=0.018199780638095794
Epoch 884
Epoch 884 :: Batch 0/1
Batch Loss = 0.01845061403942947
884, epoch_train_loss=0.01845061403942947
Epoch 885
Epoch 885 :: Batch 0/1
Batch Loss = 0.01894889219423331
885, epoch_train_loss=0.01894889219423331
Epoch 886
Epoch 886 :: Batch 0/1
Batch Loss = 0.020077642942319552
886, epoch_train_loss=0.020077642942319552
Epoch 887
Epoch 887 :: Batch 0/1
Batch Loss = 0.022200264166280258
887, epoch_train_loss=0.022200264166280258
Epoch 888
Epoch 888 :: Batch 0/1
Batch Loss = 0.02730280508090868
888, epoch_train_loss=0.02730280508090868
Epoch 889
Epoch 889 :: Batch 0/1
Batch Loss = 0.03610230877495152
889, epoch_train_loss=0.03610230877495152
Epoch 890
Epoch 890 :: Batch 0/1
Batch Loss = 0.058151512030422604
890, epoch_train_loss=0.058151512030422604
Epoch 891
Epoch 891 :: Batch 0/1
Batch Loss = 0.0901419002300571
891, epoch_train_loss=0.0901419002300571
Epoch 892
Epoch 892 :: Batch 0/1
Batch Loss = 0.1609253265788944
892, epoch_train_loss=0.1609253265788944
Epoch 893
Epoch 893 :: Batch 0/1
Batch Loss = 0.20688096494031388
893, epoch_train_loss=0.20688096494031388
Epoch 894
Epoch 894 :: Batch 0/1
Batch Loss = 0.24988097961316083
894, epoch_train_loss=0.24988097961316083
Epoch 895
Epoch 895 :: Batch 0/1
Batch Loss = 0.13103203563792817
895, epoch_train_loss=0.13103203563792817
Epoch 896
Epoch 896 :: Batch 0/1
Batch Loss = 0.030609543579075537
896, epoch_train_loss=0.030609543579075537
Epoch 897
Epoch 897 :: Batch 0/1
Batch Loss = 0.05720556319758593
897, epoch_train_loss=0.05720556319758593
Epoch 898
Epoch 898 :: Batch 0/1
Batch Loss = 0.11668716867813576
898, epoch_train_loss=0.11668716867813576
Epoch 899
Epoch 899 :: Batch 0/1
Batch Loss = 0.0725114313796015
899, epoch_train_loss=0.0725114313796015
Epoch 900
Epoch 900 :: Batch 0/1
Batch Loss = 0.025168482107793197
900, epoch_train_loss=0.025168482107793197
Epoch 901
Epoch 901 :: Batch 0/1
Batch Loss = 0.06691227318817254
901, epoch_train_loss=0.06691227318817254
Epoch 902
Epoch 902 :: Batch 0/1
Batch Loss = 0.08883781564507125
902, epoch_train_loss=0.08883781564507125
Epoch 903
Epoch 903 :: Batch 0/1
Batch Loss = 0.03305000295916914
903, epoch_train_loss=0.03305000295916914
Epoch 904
Epoch 904 :: Batch 0/1
Batch Loss = 0.029173691141542127
904, epoch_train_loss=0.029173691141542127
Epoch 905
Epoch 905 :: Batch 0/1
Batch Loss = 0.06578510796455132
905, epoch_train_loss=0.06578510796455132
Epoch 906
Epoch 906 :: Batch 0/1
Batch Loss = 0.04694820128701145
906, epoch_train_loss=0.04694820128701145
Epoch 907
Epoch 907 :: Batch 0/1
Batch Loss = 0.020987551389242454
907, epoch_train_loss=0.020987551389242454
Epoch 908
Epoch 908 :: Batch 0/1
Batch Loss = 0.033020072943314355
908, epoch_train_loss=0.033020072943314355
Epoch 909
Epoch 909 :: Batch 0/1
Batch Loss = 0.04361643077829516
909, epoch_train_loss=0.04361643077829516
Epoch 910
Epoch 910 :: Batch 0/1
Batch Loss = 0.02826268471410996
910, epoch_train_loss=0.02826268471410996
Epoch 911
Epoch 911 :: Batch 0/1
Batch Loss = 0.022582200743018657
911, epoch_train_loss=0.022582200743018657
Epoch 912
Epoch 912 :: Batch 0/1
Batch Loss = 0.0357341056440197
912, epoch_train_loss=0.0357341056440197
Epoch 913
Epoch 913 :: Batch 0/1
Batch Loss = 0.03381485072923102
913, epoch_train_loss=0.03381485072923102
Epoch 914
Epoch 914 :: Batch 0/1
Batch Loss = 0.021043865020321156
914, epoch_train_loss=0.021043865020321156
Epoch 915
Epoch 915 :: Batch 0/1
Batch Loss = 0.028253469336664755
915, epoch_train_loss=0.028253469336664755
Epoch 916
Epoch 916 :: Batch 0/1
Batch Loss = 0.0327860816261495
916, epoch_train_loss=0.0327860816261495
Epoch 917
Epoch 917 :: Batch 0/1
Batch Loss = 0.021895422645521332
917, epoch_train_loss=0.021895422645521332
Epoch 918
Epoch 918 :: Batch 0/1
Batch Loss = 0.022195592842315183
918, epoch_train_loss=0.022195592842315183
Epoch 919
Epoch 919 :: Batch 0/1
Batch Loss = 0.02963505905782514
919, epoch_train_loss=0.02963505905782514
Epoch 920
Epoch 920 :: Batch 0/1
Batch Loss = 0.023095668267541277
920, epoch_train_loss=0.023095668267541277
Epoch 921
Epoch 921 :: Batch 0/1
Batch Loss = 0.019149550853660827
921, epoch_train_loss=0.019149550853660827
Epoch 922
Epoch 922 :: Batch 0/1
Batch Loss = 0.02403063698521911
922, epoch_train_loss=0.02403063698521911
Epoch 923
Epoch 923 :: Batch 0/1
Batch Loss = 0.02475589847802207
923, epoch_train_loss=0.02475589847802207
Epoch 924
Epoch 924 :: Batch 0/1
Batch Loss = 0.019953498115605418
924, epoch_train_loss=0.019953498115605418
Epoch 925
Epoch 925 :: Batch 0/1
Batch Loss = 0.019951063121554237
925, epoch_train_loss=0.019951063121554237
Epoch 926
Epoch 926 :: Batch 0/1
Batch Loss = 0.023386584867848233
926, epoch_train_loss=0.023386584867848233
Epoch 927
Epoch 927 :: Batch 0/1
Batch Loss = 0.021641888610653484
927, epoch_train_loss=0.021641888610653484
Epoch 928
Epoch 928 :: Batch 0/1
Batch Loss = 0.018767982986893878
928, epoch_train_loss=0.018767982986893878
Epoch 929
Epoch 929 :: Batch 0/1
Batch Loss = 0.020219821035133277
929, epoch_train_loss=0.020219821035133277
Epoch 930
Epoch 930 :: Batch 0/1
Batch Loss = 0.021708893858667904
930, epoch_train_loss=0.021708893858667904
Epoch 931
Epoch 931 :: Batch 0/1
Batch Loss = 0.01969593027139119
931, epoch_train_loss=0.01969593027139119
Epoch 932
Epoch 932 :: Batch 0/1
Batch Loss = 0.01818288351963421
932, epoch_train_loss=0.01818288351963421
Epoch 933
Epoch 933 :: Batch 0/1
Batch Loss = 0.019654865171175307
933, epoch_train_loss=0.019654865171175307
Epoch 934
Epoch 934 :: Batch 0/1
Batch Loss = 0.020472860597292173
934, epoch_train_loss=0.020472860597292173
Epoch 935
Epoch 935 :: Batch 0/1
Batch Loss = 0.019230567722624183
935, epoch_train_loss=0.019230567722624183
Epoch 936
Epoch 936 :: Batch 0/1
Batch Loss = 0.01809802554243058
936, epoch_train_loss=0.01809802554243058
Epoch 937
Epoch 937 :: Batch 0/1
Batch Loss = 0.018824550139701586
937, epoch_train_loss=0.018824550139701586
Epoch 938
Epoch 938 :: Batch 0/1
Batch Loss = 0.01972753804037367
938, epoch_train_loss=0.01972753804037367
Epoch 939
Epoch 939 :: Batch 0/1
Batch Loss = 0.019042948484486198
939, epoch_train_loss=0.019042948484486198
Epoch 940
Epoch 940 :: Batch 0/1
Batch Loss = 0.018078834535262663
940, epoch_train_loss=0.018078834535262663
Epoch 941
Epoch 941 :: Batch 0/1
Batch Loss = 0.01811670317874097
941, epoch_train_loss=0.01811670317874097
Epoch 942
Epoch 942 :: Batch 0/1
Batch Loss = 0.018807628383349116
942, epoch_train_loss=0.018807628383349116
Epoch 943
Epoch 943 :: Batch 0/1
Batch Loss = 0.018802622654077592
943, epoch_train_loss=0.018802622654077592
Epoch 944
Epoch 944 :: Batch 0/1
Batch Loss = 0.018117737173551706
944, epoch_train_loss=0.018117737173551706
Epoch 945
Epoch 945 :: Batch 0/1
Batch Loss = 0.017774264014721767
945, epoch_train_loss=0.017774264014721767
Epoch 946
Epoch 946 :: Batch 0/1
Batch Loss = 0.018096535086597473
946, epoch_train_loss=0.018096535086597473
Epoch 947
Epoch 947 :: Batch 0/1
Batch Loss = 0.018423928591472866
947, epoch_train_loss=0.018423928591472866
Epoch 948
Epoch 948 :: Batch 0/1
Batch Loss = 0.018191780516720234
948, epoch_train_loss=0.018191780516720234
Epoch 949
Epoch 949 :: Batch 0/1
Batch Loss = 0.017786924361088818
949, epoch_train_loss=0.017786924361088818
Epoch 950
Epoch 950 :: Batch 0/1
Batch Loss = 0.01772309957542678
950, epoch_train_loss=0.01772309957542678
Epoch 951
Epoch 951 :: Batch 0/1
Batch Loss = 0.017970716414272198
951, epoch_train_loss=0.017970716414272198
Epoch 952
Epoch 952 :: Batch 0/1
Batch Loss = 0.01806231094967853
952, epoch_train_loss=0.01806231094967853
Epoch 953
Epoch 953 :: Batch 0/1
Batch Loss = 0.017838554638344533
953, epoch_train_loss=0.017838554638344533
Epoch 954
Epoch 954 :: Batch 0/1
Batch Loss = 0.017611620203602484
954, epoch_train_loss=0.017611620203602484
Epoch 955
Epoch 955 :: Batch 0/1
Batch Loss = 0.01761669003644867
955, epoch_train_loss=0.01761669003644867
Epoch 956
Epoch 956 :: Batch 0/1
Batch Loss = 0.0177674698096722
956, epoch_train_loss=0.0177674698096722
Epoch 957
Epoch 957 :: Batch 0/1
Batch Loss = 0.017781472372568906
957, epoch_train_loss=0.017781472372568906
Epoch 958
Epoch 958 :: Batch 0/1
Batch Loss = 0.01763153224708571
958, epoch_train_loss=0.01763153224708571
Epoch 959
Epoch 959 :: Batch 0/1
Batch Loss = 0.0175108436000263
959, epoch_train_loss=0.0175108436000263
Epoch 960
Epoch 960 :: Batch 0/1
Batch Loss = 0.017534147461603852
960, epoch_train_loss=0.017534147461603852
Epoch 961
Epoch 961 :: Batch 0/1
Batch Loss = 0.017618398679677565
961, epoch_train_loss=0.017618398679677565
Epoch 962
Epoch 962 :: Batch 0/1
Batch Loss = 0.017596856075331128
962, epoch_train_loss=0.017596856075331128
Epoch 963
Epoch 963 :: Batch 0/1
Batch Loss = 0.017492822738068285
963, epoch_train_loss=0.017492822738068285
Epoch 964
Epoch 964 :: Batch 0/1
Batch Loss = 0.017426188447812085
964, epoch_train_loss=0.017426188447812085
Epoch 965
Epoch 965 :: Batch 0/1
Batch Loss = 0.01744663877734431
965, epoch_train_loss=0.01744663877734431
Epoch 966
Epoch 966 :: Batch 0/1
Batch Loss = 0.017487154914103156
966, epoch_train_loss=0.017487154914103156
Epoch 967
Epoch 967 :: Batch 0/1
Batch Loss = 0.017458744471777293
967, epoch_train_loss=0.017458744471777293
Epoch 968
Epoch 968 :: Batch 0/1
Batch Loss = 0.017391405987991604
968, epoch_train_loss=0.017391405987991604
Epoch 969
Epoch 969 :: Batch 0/1
Batch Loss = 0.01735311120672678
969, epoch_train_loss=0.01735311120672678
Epoch 970
Epoch 970 :: Batch 0/1
Batch Loss = 0.017366233473146614
970, epoch_train_loss=0.017366233473146614
Epoch 971
Epoch 971 :: Batch 0/1
Batch Loss = 0.017384825295862882
971, epoch_train_loss=0.017384825295862882
Epoch 972
Epoch 972 :: Batch 0/1
Batch Loss = 0.017360786079280675
972, epoch_train_loss=0.017360786079280675
Epoch 973
Epoch 973 :: Batch 0/1
Batch Loss = 0.017315445307393186
973, epoch_train_loss=0.017315445307393186
Epoch 974
Epoch 974 :: Batch 0/1
Batch Loss = 0.017287557372191238
974, epoch_train_loss=0.017287557372191238
Epoch 975
Epoch 975 :: Batch 0/1
Batch Loss = 0.01728986761655695
975, epoch_train_loss=0.01728986761655695
Epoch 976
Epoch 976 :: Batch 0/1
Batch Loss = 0.0172944741272773
976, epoch_train_loss=0.0172944741272773
Epoch 977
Epoch 977 :: Batch 0/1
Batch Loss = 0.01727496781953295
977, epoch_train_loss=0.01727496781953295
Epoch 978
Epoch 978 :: Batch 0/1
Batch Loss = 0.017243176199505703
978, epoch_train_loss=0.017243176199505703
Epoch 979
Epoch 979 :: Batch 0/1
Batch Loss = 0.017221067021198903
979, epoch_train_loss=0.017221067021198903
Epoch 980
Epoch 980 :: Batch 0/1
Batch Loss = 0.01721727873216279
980, epoch_train_loss=0.01721727873216279
Epoch 981
Epoch 981 :: Batch 0/1
Batch Loss = 0.017215694757208654
981, epoch_train_loss=0.017215694757208654
Epoch 982
Epoch 982 :: Batch 0/1
Batch Loss = 0.017200647545943106
982, epoch_train_loss=0.017200647545943106
Epoch 983
Epoch 983 :: Batch 0/1
Batch Loss = 0.017177081816433223
983, epoch_train_loss=0.017177081816433223
Epoch 984
Epoch 984 :: Batch 0/1
Batch Loss = 0.017157440047924882
984, epoch_train_loss=0.017157440047924882
Epoch 985
Epoch 985 :: Batch 0/1
Batch Loss = 0.01714861304558896
985, epoch_train_loss=0.01714861304558896
Epoch 986
Epoch 986 :: Batch 0/1
Batch Loss = 0.01714261029465955
986, epoch_train_loss=0.01714261029465955
Epoch 987
Epoch 987 :: Batch 0/1
Batch Loss = 0.01713003852504739
987, epoch_train_loss=0.01713003852504739
Epoch 988
Epoch 988 :: Batch 0/1
Batch Loss = 0.017111537224277065
988, epoch_train_loss=0.017111537224277065
Epoch 989
Epoch 989 :: Batch 0/1
Batch Loss = 0.017093494198293268
989, epoch_train_loss=0.017093494198293268
Epoch 990
Epoch 990 :: Batch 0/1
Batch Loss = 0.017081587719586833
990, epoch_train_loss=0.017081587719586833
Epoch 991
Epoch 991 :: Batch 0/1
Batch Loss = 0.017073128709636905
991, epoch_train_loss=0.017073128709636905
Epoch 992
Epoch 992 :: Batch 0/1
Batch Loss = 0.01706257595356853
992, epoch_train_loss=0.01706257595356853
Epoch 993
Epoch 993 :: Batch 0/1
Batch Loss = 0.017048056387555174
993, epoch_train_loss=0.017048056387555174
Epoch 994
Epoch 994 :: Batch 0/1
Batch Loss = 0.01703175899522998
994, epoch_train_loss=0.01703175899522998
Epoch 995
Epoch 995 :: Batch 0/1
Batch Loss = 0.017017884671945423
995, epoch_train_loss=0.017017884671945423
Epoch 996
Epoch 996 :: Batch 0/1
Batch Loss = 0.017006922209629283
996, epoch_train_loss=0.017006922209629283
Epoch 997
Epoch 997 :: Batch 0/1
Batch Loss = 0.016996531124054352
997, epoch_train_loss=0.016996531124054352
Epoch 998
Epoch 998 :: Batch 0/1
Batch Loss = 0.01698444576244409
998, epoch_train_loss=0.01698444576244409
Epoch 999
Epoch 999 :: Batch 0/1
Batch Loss = 0.016970249757286157
999, epoch_train_loss=0.016970249757286157
Epoch 1000
Epoch 1000 :: Batch 0/1
Batch Loss = 0.016956050590827495
1000, epoch_train_loss=0.016956050590827495
Epoch 1001
Epoch 1001 :: Batch 0/1
Batch Loss = 0.016943324961258853
1001, epoch_train_loss=0.016943324961258853
Epoch 1002
Epoch 1002 :: Batch 0/1
Batch Loss = 0.0169320217795241
1002, epoch_train_loss=0.0169320217795241
Epoch 1003
Epoch 1003 :: Batch 0/1
Batch Loss = 0.016920920058637295
1003, epoch_train_loss=0.016920920058637295
Epoch 1004
Epoch 1004 :: Batch 0/1
Batch Loss = 0.016908710767660366
1004, epoch_train_loss=0.016908710767660366
Epoch 1005
Epoch 1005 :: Batch 0/1
Batch Loss = 0.016895577894368016
1005, epoch_train_loss=0.016895577894368016
Epoch 1006
Epoch 1006 :: Batch 0/1
Batch Loss = 0.016882351826239837
1006, epoch_train_loss=0.016882351826239837
Epoch 1007
Epoch 1007 :: Batch 0/1
Batch Loss = 0.016869834203111674
1007, epoch_train_loss=0.016869834203111674
Epoch 1008
Epoch 1008 :: Batch 0/1
Batch Loss = 0.016858147353682136
1008, epoch_train_loss=0.016858147353682136
Epoch 1009
Epoch 1009 :: Batch 0/1
Batch Loss = 0.016846604067047785
1009, epoch_train_loss=0.016846604067047785
Epoch 1010
Epoch 1010 :: Batch 0/1
Batch Loss = 0.016834683246056508
1010, epoch_train_loss=0.016834683246056508
Epoch 1011
Epoch 1011 :: Batch 0/1
Batch Loss = 0.01682223341669248
1011, epoch_train_loss=0.01682223341669248
Epoch 1012
Epoch 1012 :: Batch 0/1
Batch Loss = 0.016809576411041423
1012, epoch_train_loss=0.016809576411041423
Epoch 1013
Epoch 1013 :: Batch 0/1
Batch Loss = 0.01679720374671006
1013, epoch_train_loss=0.01679720374671006
Epoch 1014
Epoch 1014 :: Batch 0/1
Batch Loss = 0.016785238978532388
1014, epoch_train_loss=0.016785238978532388
Epoch 1015
Epoch 1015 :: Batch 0/1
Batch Loss = 0.01677355455409465
1015, epoch_train_loss=0.01677355455409465
Epoch 1016
Epoch 1016 :: Batch 0/1
Batch Loss = 0.01676184469237074
1016, epoch_train_loss=0.01676184469237074
Epoch 1017
Epoch 1017 :: Batch 0/1
Batch Loss = 0.016749883037015172
1017, epoch_train_loss=0.016749883037015172
Epoch 1018
Epoch 1018 :: Batch 0/1
Batch Loss = 0.016737724754199323
1018, epoch_train_loss=0.016737724754199323
Epoch 1019
Epoch 1019 :: Batch 0/1
Batch Loss = 0.01672552664538123
1019, epoch_train_loss=0.01672552664538123
Epoch 1020
Epoch 1020 :: Batch 0/1
Batch Loss = 0.01671347453099025
1020, epoch_train_loss=0.01671347453099025
Epoch 1021
Epoch 1021 :: Batch 0/1
Batch Loss = 0.01670164421623916
1021, epoch_train_loss=0.01670164421623916
Epoch 1022
Epoch 1022 :: Batch 0/1
Batch Loss = 0.016689943059922406
1022, epoch_train_loss=0.016689943059922406
Epoch 1023
Epoch 1023 :: Batch 0/1
Batch Loss = 0.016678264236927497
1023, epoch_train_loss=0.016678264236927497
Epoch 1024
Epoch 1024 :: Batch 0/1
Batch Loss = 0.016666509145915883
1024, epoch_train_loss=0.016666509145915883
Epoch 1025
Epoch 1025 :: Batch 0/1
Batch Loss = 0.016654660075242377
1025, epoch_train_loss=0.016654660075242377
Epoch 1026
Epoch 1026 :: Batch 0/1
Batch Loss = 0.01664279043324299
1026, epoch_train_loss=0.01664279043324299
Epoch 1027
Epoch 1027 :: Batch 0/1
Batch Loss = 0.016630959877014
1027, epoch_train_loss=0.016630959877014
Epoch 1028
Epoch 1028 :: Batch 0/1
Batch Loss = 0.016619224485699798
1028, epoch_train_loss=0.016619224485699798
Epoch 1029
Epoch 1029 :: Batch 0/1
Batch Loss = 0.016607586015397195
1029, epoch_train_loss=0.016607586015397195
Epoch 1030
Epoch 1030 :: Batch 0/1
Batch Loss = 0.016596000814708457
1030, epoch_train_loss=0.016596000814708457
Epoch 1031
Epoch 1031 :: Batch 0/1
Batch Loss = 0.01658443472464889
1031, epoch_train_loss=0.01658443472464889
Epoch 1032
Epoch 1032 :: Batch 0/1
Batch Loss = 0.0165728520040826
1032, epoch_train_loss=0.0165728520040826
Epoch 1033
Epoch 1033 :: Batch 0/1
Batch Loss = 0.016561250320015715
1033, epoch_train_loss=0.016561250320015715
Epoch 1034
Epoch 1034 :: Batch 0/1
Batch Loss = 0.016549651864867152
1034, epoch_train_loss=0.016549651864867152
Epoch 1035
Epoch 1035 :: Batch 0/1
Batch Loss = 0.016538077381151886
1035, epoch_train_loss=0.016538077381151886
Epoch 1036
Epoch 1036 :: Batch 0/1
Batch Loss = 0.016526550338892165
1036, epoch_train_loss=0.016526550338892165
Epoch 1037
Epoch 1037 :: Batch 0/1
Batch Loss = 0.016515077763055695
1037, epoch_train_loss=0.016515077763055695
Epoch 1038
Epoch 1038 :: Batch 0/1
Batch Loss = 0.01650364999329567
1038, epoch_train_loss=0.01650364999329567
Epoch 1039
Epoch 1039 :: Batch 0/1
Batch Loss = 0.01649225770219312
1039, epoch_train_loss=0.01649225770219312
Epoch 1040
Epoch 1040 :: Batch 0/1
Batch Loss = 0.016480885993776666
1040, epoch_train_loss=0.016480885993776666
Epoch 1041
Epoch 1041 :: Batch 0/1
Batch Loss = 0.01646952656134101
1041, epoch_train_loss=0.01646952656134101
Epoch 1042
Epoch 1042 :: Batch 0/1
Batch Loss = 0.01645818131405748
1042, epoch_train_loss=0.01645818131405748
Epoch 1043
Epoch 1043 :: Batch 0/1
Batch Loss = 0.016446851007619747
1043, epoch_train_loss=0.016446851007619747
Epoch 1044
Epoch 1044 :: Batch 0/1
Batch Loss = 0.016435544179309786
1044, epoch_train_loss=0.016435544179309786
Epoch 1045
Epoch 1045 :: Batch 0/1
Batch Loss = 0.01642426703353478
1045, epoch_train_loss=0.01642426703353478
Epoch 1046
Epoch 1046 :: Batch 0/1
Batch Loss = 0.016413022618371735
1046, epoch_train_loss=0.016413022618371735
Epoch 1047
Epoch 1047 :: Batch 0/1
Batch Loss = 0.01640181403644169
1047, epoch_train_loss=0.01640181403644169
Epoch 1048
Epoch 1048 :: Batch 0/1
Batch Loss = 0.016390639662051006
1048, epoch_train_loss=0.016390639662051006
Epoch 1049
Epoch 1049 :: Batch 0/1
Batch Loss = 0.016379496634335508
1049, epoch_train_loss=0.016379496634335508
Epoch 1050
Epoch 1050 :: Batch 0/1
Batch Loss = 0.01636838321101612
1050, epoch_train_loss=0.01636838321101612
Epoch 1051
Epoch 1051 :: Batch 0/1
Batch Loss = 0.016357296383170957
1051, epoch_train_loss=0.016357296383170957
Epoch 1052
Epoch 1052 :: Batch 0/1
Batch Loss = 0.016346234945795724
1052, epoch_train_loss=0.016346234945795724
Epoch 1053
Epoch 1053 :: Batch 0/1
Batch Loss = 0.01633519938530639
1053, epoch_train_loss=0.01633519938530639
Epoch 1054
Epoch 1054 :: Batch 0/1
Batch Loss = 0.01632418888933787
1054, epoch_train_loss=0.01632418888933787
Epoch 1055
Epoch 1055 :: Batch 0/1
Batch Loss = 0.016313205113209547
1055, epoch_train_loss=0.016313205113209547
Epoch 1056
Epoch 1056 :: Batch 0/1
Batch Loss = 0.01630224870332118
1056, epoch_train_loss=0.01630224870332118
Epoch 1057
Epoch 1057 :: Batch 0/1
Batch Loss = 0.01629132043770165
1057, epoch_train_loss=0.01629132043770165
Epoch 1058
Epoch 1058 :: Batch 0/1
Batch Loss = 0.22070396779403825
1058, epoch_train_loss=0.22070396779403825
Epoch 1059
Epoch 1059 :: Batch 0/1
Batch Loss = 3.726225809454726
1059, epoch_train_loss=3.726225809454726
Epoch 1060
Epoch 1060 :: Batch 0/1
Batch Loss = 2.518989394261895
1060, epoch_train_loss=2.518989394261895
Epoch 1061
Epoch 1061 :: Batch 0/1
Batch Loss = 1.3458645283740611
1061, epoch_train_loss=1.3458645283740611
Epoch 1062
Epoch 1062 :: Batch 0/1
Batch Loss = 2.1096593189641832
1062, epoch_train_loss=2.1096593189641832
Epoch 1063
Epoch 1063 :: Batch 0/1
Batch Loss = 1.6095324289660817
1063, epoch_train_loss=1.6095324289660817
Epoch 1064
Epoch 1064 :: Batch 0/1
Batch Loss = 2.5745627268881526
1064, epoch_train_loss=2.5745627268881526
Epoch 1065
Epoch 1065 :: Batch 0/1
Batch Loss = 1.1364526462768607
1065, epoch_train_loss=1.1364526462768607
Epoch 1066
Epoch 1066 :: Batch 0/1
Batch Loss = 1.3352553402839722
1066, epoch_train_loss=1.3352553402839722
Epoch 1067
Epoch 1067 :: Batch 0/1
Batch Loss = 1.3056769777508084
1067, epoch_train_loss=1.3056769777508084
Epoch 1068
Epoch 1068 :: Batch 0/1
Batch Loss = 0.6400982260898433
1068, epoch_train_loss=0.6400982260898433
Epoch 1069
Epoch 1069 :: Batch 0/1
Batch Loss = 1.2713720746964707
1069, epoch_train_loss=1.2713720746964707
Epoch 1070
Epoch 1070 :: Batch 0/1
Batch Loss = 1.14399347823491
1070, epoch_train_loss=1.14399347823491
Epoch 1071
Epoch 1071 :: Batch 0/1
Batch Loss = 0.4902913870609856
1071, epoch_train_loss=0.4902913870609856
Epoch 1072
Epoch 1072 :: Batch 0/1
Batch Loss = 0.8484999520283907
1072, epoch_train_loss=0.8484999520283907
Epoch 1073
Epoch 1073 :: Batch 0/1
Batch Loss = 0.7478863553605373
1073, epoch_train_loss=0.7478863553605373
Epoch 1074
Epoch 1074 :: Batch 0/1
Batch Loss = 0.43108716199636155
1074, epoch_train_loss=0.43108716199636155
Epoch 1075
Epoch 1075 :: Batch 0/1
Batch Loss = 2797.5640714223396
1075, epoch_train_loss=2797.5640714223396
Epoch 1076
Epoch 1076 :: Batch 0/1
Batch Loss = 0.5484514642906484
1076, epoch_train_loss=0.5484514642906484
Epoch 1077
Epoch 1077 :: Batch 0/1
Batch Loss = 0.24502827308269318
1077, epoch_train_loss=0.24502827308269318
Epoch 1078
Epoch 1078 :: Batch 0/1
Batch Loss = 0.603341715224625
1078, epoch_train_loss=0.603341715224625
Epoch 1079
Epoch 1079 :: Batch 0/1
Batch Loss = 0.26762601155802657
1079, epoch_train_loss=0.26762601155802657
Epoch 1080
Epoch 1080 :: Batch 0/1
Batch Loss = 0.3443941469181448
1080, epoch_train_loss=0.3443941469181448
Epoch 1081
Epoch 1081 :: Batch 0/1
Batch Loss = 0.4592736535875555
1081, epoch_train_loss=0.4592736535875555
Epoch 1082
Epoch 1082 :: Batch 0/1
Batch Loss = 0.19868208558231282
1082, epoch_train_loss=0.19868208558231282
Epoch 1083
Epoch 1083 :: Batch 0/1
Batch Loss = 0.3834903357310602
1083, epoch_train_loss=0.3834903357310602
Epoch 1084
Epoch 1084 :: Batch 0/1
Batch Loss = 0.27019792737312337
1084, epoch_train_loss=0.27019792737312337
Epoch 1085
Epoch 1085 :: Batch 0/1
Batch Loss = 0.24079317942302
1085, epoch_train_loss=0.24079317942302
Epoch 1086
Epoch 1086 :: Batch 0/1
Batch Loss = 0.35077080016001305
1086, epoch_train_loss=0.35077080016001305
Epoch 1087
Epoch 1087 :: Batch 0/1
Batch Loss = 0.18272779580744408
1087, epoch_train_loss=0.18272779580744408
Epoch 1088
Epoch 1088 :: Batch 0/1
Batch Loss = 0.29216132793513566
1088, epoch_train_loss=0.29216132793513566
Epoch 1089
Epoch 1089 :: Batch 0/1
Batch Loss = 0.23038945887278137
1089, epoch_train_loss=0.23038945887278137
Epoch 1090
Epoch 1090 :: Batch 0/1
Batch Loss = 0.19271383095326813
1090, epoch_train_loss=0.19271383095326813
Epoch 1091
Epoch 1091 :: Batch 0/1
Batch Loss = 0.26903744133378166
1091, epoch_train_loss=0.26903744133378166
Epoch 1092
Epoch 1092 :: Batch 0/1
Batch Loss = 0.1712071976042057
1092, epoch_train_loss=0.1712071976042057
Epoch 1093
Epoch 1093 :: Batch 0/1
Batch Loss = 0.211526979104134
1093, epoch_train_loss=0.211526979104134
Epoch 1094
Epoch 1094 :: Batch 0/1
Batch Loss = 0.21224849667177667
1094, epoch_train_loss=0.21224849667177667
Epoch 1095
Epoch 1095 :: Batch 0/1
Batch Loss = 0.16015762646480322
1095, epoch_train_loss=0.16015762646480322
Epoch 1096
Epoch 1096 :: Batch 0/1
Batch Loss = 0.20914993569704243
1096, epoch_train_loss=0.20914993569704243
Epoch 1097
Epoch 1097 :: Batch 0/1
Batch Loss = 0.16677922052248348
1097, epoch_train_loss=0.16677922052248348
Epoch 1098
Epoch 1098 :: Batch 0/1
Batch Loss = 0.1673821299262232
1098, epoch_train_loss=0.1673821299262232
Epoch 1099
Epoch 1099 :: Batch 0/1
Batch Loss = 0.18153913310698358
1099, epoch_train_loss=0.18153913310698358
Epoch 1100
Epoch 1100 :: Batch 0/1
Batch Loss = 0.14813514876820277
1100, epoch_train_loss=0.14813514876820277
Epoch 1101
Epoch 1101 :: Batch 0/1
Batch Loss = 0.16947584125762793
1101, epoch_train_loss=0.16947584125762793
Epoch 1102
Epoch 1102 :: Batch 0/1
Batch Loss = 0.14816853263501095
1102, epoch_train_loss=0.14816853263501095
Epoch 1103
Epoch 1103 :: Batch 0/1
Batch Loss = 0.13672965903576192
1103, epoch_train_loss=0.13672965903576192
Epoch 1104
Epoch 1104 :: Batch 0/1
Batch Loss = 0.15722285655690177
1104, epoch_train_loss=0.15722285655690177
Epoch 1105
Epoch 1105 :: Batch 0/1
Batch Loss = 0.12874501576073252
1105, epoch_train_loss=0.12874501576073252
Epoch 1106
Epoch 1106 :: Batch 0/1
Batch Loss = 0.14136024170968114
1106, epoch_train_loss=0.14136024170968114
Epoch 1107
Epoch 1107 :: Batch 0/1
Batch Loss = 0.13593773675030887
1107, epoch_train_loss=0.13593773675030887
Epoch 1108
Epoch 1108 :: Batch 0/1
Batch Loss = 0.12844223369926638
1108, epoch_train_loss=0.12844223369926638
Epoch 1109
Epoch 1109 :: Batch 0/1
Batch Loss = 0.13682157470405776
1109, epoch_train_loss=0.13682157470405776
Epoch 1110
Epoch 1110 :: Batch 0/1
Batch Loss = 0.12257799218691155
1110, epoch_train_loss=0.12257799218691155
Epoch 1111
Epoch 1111 :: Batch 0/1
Batch Loss = 0.12949570922598533
1111, epoch_train_loss=0.12949570922598533
Epoch 1112
Epoch 1112 :: Batch 0/1
Batch Loss = 0.12360809200380857
1112, epoch_train_loss=0.12360809200380857
Epoch 1113
Epoch 1113 :: Batch 0/1
Batch Loss = 0.11912582490609777
1113, epoch_train_loss=0.11912582490609777
Epoch 1114
Epoch 1114 :: Batch 0/1
Batch Loss = 0.1220495604351361
1114, epoch_train_loss=0.1220495604351361
Epoch 1115
Epoch 1115 :: Batch 0/1
Batch Loss = 0.11177984748850746
1115, epoch_train_loss=0.11177984748850746
Epoch 1116
Epoch 1116 :: Batch 0/1
Batch Loss = 0.11523377632997454
1116, epoch_train_loss=0.11523377632997454
Epoch 1117
Epoch 1117 :: Batch 0/1
Batch Loss = 0.10795768426546139
1117, epoch_train_loss=0.10795768426546139
Epoch 1118
Epoch 1118 :: Batch 0/1
Batch Loss = 0.10795616436203605
1118, epoch_train_loss=0.10795616436203605
Epoch 1119
Epoch 1119 :: Batch 0/1
Batch Loss = 0.10850030527815474
1119, epoch_train_loss=0.10850030527815474
Epoch 1120
Epoch 1120 :: Batch 0/1
Batch Loss = 0.10256594405560028
1120, epoch_train_loss=0.10256594405560028
Epoch 1121
Epoch 1121 :: Batch 0/1
Batch Loss = 0.10554514283925918
1121, epoch_train_loss=0.10554514283925918
Epoch 1122
Epoch 1122 :: Batch 0/1
Batch Loss = 0.10049977171007864
1122, epoch_train_loss=0.10049977171007864
Epoch 1123
Epoch 1123 :: Batch 0/1
Batch Loss = 0.10131510700365795
1123, epoch_train_loss=0.10131510700365795
Epoch 1124
Epoch 1124 :: Batch 0/1
Batch Loss = 0.09945435629712622
1124, epoch_train_loss=0.09945435629712622
Epoch 1125
Epoch 1125 :: Batch 0/1
Batch Loss = 0.09704290641640123
1125, epoch_train_loss=0.09704290641640123
Epoch 1126
Epoch 1126 :: Batch 0/1
Batch Loss = 0.0962678045193987
1126, epoch_train_loss=0.0962678045193987
Epoch 1127
Epoch 1127 :: Batch 0/1
Batch Loss = 0.09207950062690226
1127, epoch_train_loss=0.09207950062690226
Epoch 1128
Epoch 1128 :: Batch 0/1
Batch Loss = 0.09292652320783683
1128, epoch_train_loss=0.09292652320783683
Epoch 1129
Epoch 1129 :: Batch 0/1
Batch Loss = 0.08900693741216859
1129, epoch_train_loss=0.08900693741216859
Epoch 1130
Epoch 1130 :: Batch 0/1
Batch Loss = 0.08935916220981756
1130, epoch_train_loss=0.08935916220981756
Epoch 1131
Epoch 1131 :: Batch 0/1
Batch Loss = 0.08624622850127676
1131, epoch_train_loss=0.08624622850127676
Epoch 1132
Epoch 1132 :: Batch 0/1
Batch Loss = 0.08467018872547992
1132, epoch_train_loss=0.08467018872547992
Epoch 1133
Epoch 1133 :: Batch 0/1
Batch Loss = 0.08345342149080631
1133, epoch_train_loss=0.08345342149080631
Epoch 1134
Epoch 1134 :: Batch 0/1
Batch Loss = 0.08083124985058282
1134, epoch_train_loss=0.08083124985058282
Epoch 1135
Epoch 1135 :: Batch 0/1
Batch Loss = 0.07980524934900234
1135, epoch_train_loss=0.07980524934900234
Epoch 1136
Epoch 1136 :: Batch 0/1
Batch Loss = 0.07736178311743289
1136, epoch_train_loss=0.07736178311743289
Epoch 1137
Epoch 1137 :: Batch 0/1
Batch Loss = 0.07623227645095695
1137, epoch_train_loss=0.07623227645095695
Epoch 1138
Epoch 1138 :: Batch 0/1
Batch Loss = 0.07345975091937283
1138, epoch_train_loss=0.07345975091937283
Epoch 1139
Epoch 1139 :: Batch 0/1
Batch Loss = 0.07180749777326217
1139, epoch_train_loss=0.07180749777326217
Epoch 1140
Epoch 1140 :: Batch 0/1
Batch Loss = 0.06912220807665657
1140, epoch_train_loss=0.06912220807665657
Epoch 1141
Epoch 1141 :: Batch 0/1
Batch Loss = 0.06701093037243203
1141, epoch_train_loss=0.06701093037243203
Epoch 1142
Epoch 1142 :: Batch 0/1
Batch Loss = 0.06552331244145093
1142, epoch_train_loss=0.06552331244145093
Epoch 1143
Epoch 1143 :: Batch 0/1
Batch Loss = 0.06378015414161618
1143, epoch_train_loss=0.06378015414161618
Epoch 1144
Epoch 1144 :: Batch 0/1
Batch Loss = 0.06293914218421193
1144, epoch_train_loss=0.06293914218421193
Epoch 1145
Epoch 1145 :: Batch 0/1
Batch Loss = 0.06116261142532537
1145, epoch_train_loss=0.06116261142532537
Epoch 1146
Epoch 1146 :: Batch 0/1
Batch Loss = 0.06073105980590017
1146, epoch_train_loss=0.06073105980590017
Epoch 1147
Epoch 1147 :: Batch 0/1
Batch Loss = 0.059772558902000096
1147, epoch_train_loss=0.059772558902000096
Epoch 1148
Epoch 1148 :: Batch 0/1
Batch Loss = 0.05940525841727062
1148, epoch_train_loss=0.05940525841727062
Epoch 1149
Epoch 1149 :: Batch 0/1
Batch Loss = 0.05791777596233904
1149, epoch_train_loss=0.05791777596233904
Epoch 1150
Epoch 1150 :: Batch 0/1
Batch Loss = 0.05731436583454073
1150, epoch_train_loss=0.05731436583454073
Epoch 1151
Epoch 1151 :: Batch 0/1
Batch Loss = 0.05665309447137372
1151, epoch_train_loss=0.05665309447137372
Epoch 1152
Epoch 1152 :: Batch 0/1
Batch Loss = 0.05706746799102495
1152, epoch_train_loss=0.05706746799102495
Epoch 1153
Epoch 1153 :: Batch 0/1
Batch Loss = 0.05699627451435784
1153, epoch_train_loss=0.05699627451435784
Epoch 1154
Epoch 1154 :: Batch 0/1
Batch Loss = 0.057225446669708344
1154, epoch_train_loss=0.057225446669708344
Epoch 1155
Epoch 1155 :: Batch 0/1
Batch Loss = 0.05664843993287546
1155, epoch_train_loss=0.05664843993287546
Epoch 1156
Epoch 1156 :: Batch 0/1
Batch Loss = 0.05622166924053443
1156, epoch_train_loss=0.05622166924053443
Epoch 1157
Epoch 1157 :: Batch 0/1
Batch Loss = 0.05570007808242564
1157, epoch_train_loss=0.05570007808242564
Epoch 1158
Epoch 1158 :: Batch 0/1
Batch Loss = 0.0554282157533484
1158, epoch_train_loss=0.0554282157533484
Epoch 1159
Epoch 1159 :: Batch 0/1
Batch Loss = 0.05535591357676107
1159, epoch_train_loss=0.05535591357676107
Epoch 1160
Epoch 1160 :: Batch 0/1
Batch Loss = 0.05497204657765736
1160, epoch_train_loss=0.05497204657765736
Epoch 1161
Epoch 1161 :: Batch 0/1
Batch Loss = 0.054677610069444126
1161, epoch_train_loss=0.054677610069444126
Epoch 1162
Epoch 1162 :: Batch 0/1
Batch Loss = 0.054027555911388025
1162, epoch_train_loss=0.054027555911388025
Epoch 1163
Epoch 1163 :: Batch 0/1
Batch Loss = 0.053698318090256186
1163, epoch_train_loss=0.053698318090256186
Epoch 1164
Epoch 1164 :: Batch 0/1
Batch Loss = 0.05324048347868759
1164, epoch_train_loss=0.05324048347868759
Epoch 1165
Epoch 1165 :: Batch 0/1
Batch Loss = 0.053117601432106726
1165, epoch_train_loss=0.053117601432106726
Epoch 1166
Epoch 1166 :: Batch 0/1
Batch Loss = 0.05299304189269252
1166, epoch_train_loss=0.05299304189269252
Epoch 1167
Epoch 1167 :: Batch 0/1
Batch Loss = 0.052851406485752865
1167, epoch_train_loss=0.052851406485752865
Epoch 1168
Epoch 1168 :: Batch 0/1
Batch Loss = 0.05272961180658106
1168, epoch_train_loss=0.05272961180658106
Epoch 1169
Epoch 1169 :: Batch 0/1
Batch Loss = 0.05259388718769585
1169, epoch_train_loss=0.05259388718769585
Epoch 1170
Epoch 1170 :: Batch 0/1
Batch Loss = 0.05259176406368327
1170, epoch_train_loss=0.05259176406368327
Epoch 1171
Epoch 1171 :: Batch 0/1
Batch Loss = 0.052450000582888186
1171, epoch_train_loss=0.052450000582888186
Epoch 1172
Epoch 1172 :: Batch 0/1
Batch Loss = 0.052322640256240414
1172, epoch_train_loss=0.052322640256240414
Epoch 1173
Epoch 1173 :: Batch 0/1
Batch Loss = 0.05202413985684391
1173, epoch_train_loss=0.05202413985684391
Epoch 1174
Epoch 1174 :: Batch 0/1
Batch Loss = 0.05178529225918675
1174, epoch_train_loss=0.05178529225918675
Epoch 1175
Epoch 1175 :: Batch 0/1
Batch Loss = 0.05154457266672147
1175, epoch_train_loss=0.05154457266672147
Epoch 1176
Epoch 1176 :: Batch 0/1
Batch Loss = 0.051345768472539174
1176, epoch_train_loss=0.051345768472539174
Epoch 1177
Epoch 1177 :: Batch 0/1
Batch Loss = 0.05116608281083308
1177, epoch_train_loss=0.05116608281083308
Epoch 1178
Epoch 1178 :: Batch 0/1
Batch Loss = 0.05098171122853081
1178, epoch_train_loss=0.05098171122853081
Epoch 1179
Epoch 1179 :: Batch 0/1
Batch Loss = 0.05089141706881221
1179, epoch_train_loss=0.05089141706881221
Epoch 1180
Epoch 1180 :: Batch 0/1
Batch Loss = 0.05078700916641554
1180, epoch_train_loss=0.05078700916641554
Epoch 1181
Epoch 1181 :: Batch 0/1
Batch Loss = 0.050713789657974295
1181, epoch_train_loss=0.050713789657974295
Epoch 1182
Epoch 1182 :: Batch 0/1
Batch Loss = 0.05056513070865762
1182, epoch_train_loss=0.05056513070865762
Epoch 1183
Epoch 1183 :: Batch 0/1
Batch Loss = 0.050392309432882886
1183, epoch_train_loss=0.050392309432882886
Epoch 1184
Epoch 1184 :: Batch 0/1
Batch Loss = 0.05020468000431055
1184, epoch_train_loss=0.05020468000431055
Epoch 1185
Epoch 1185 :: Batch 0/1
Batch Loss = 0.05013807661161316
1185, epoch_train_loss=0.05013807661161316
Epoch 1186
Epoch 1186 :: Batch 0/1
Batch Loss = 0.050022521492890645
1186, epoch_train_loss=0.050022521492890645
Epoch 1187
Epoch 1187 :: Batch 0/1
Batch Loss = 0.049822829523768444
1187, epoch_train_loss=0.049822829523768444
Epoch 1188
Epoch 1188 :: Batch 0/1
Batch Loss = 0.04971932040330738
1188, epoch_train_loss=0.04971932040330738
Epoch 1189
Epoch 1189 :: Batch 0/1
Batch Loss = 0.04964734785507899
1189, epoch_train_loss=0.04964734785507899
Epoch 1190
Epoch 1190 :: Batch 0/1
Batch Loss = 0.04958576485186202
1190, epoch_train_loss=0.04958576485186202
Epoch 1191
Epoch 1191 :: Batch 0/1
Batch Loss = 0.04950360649642856
1191, epoch_train_loss=0.04950360649642856
Epoch 1192
Epoch 1192 :: Batch 0/1
Batch Loss = 0.04941148800051946
1192, epoch_train_loss=0.04941148800051946
Epoch 1193
Epoch 1193 :: Batch 0/1
Batch Loss = 0.049298358820938824
1193, epoch_train_loss=0.049298358820938824
Epoch 1194
Epoch 1194 :: Batch 0/1
Batch Loss = 0.04917368590974991
1194, epoch_train_loss=0.04917368590974991
Epoch 1195
Epoch 1195 :: Batch 0/1
Batch Loss = 0.04906103240670236
1195, epoch_train_loss=0.04906103240670236
Epoch 1196
Epoch 1196 :: Batch 0/1
Batch Loss = 0.0489542411883285
1196, epoch_train_loss=0.0489542411883285
Epoch 1197
Epoch 1197 :: Batch 0/1
Batch Loss = 0.04886590789154015
1197, epoch_train_loss=0.04886590789154015
Epoch 1198
Epoch 1198 :: Batch 0/1
Batch Loss = 0.048806085643250015
1198, epoch_train_loss=0.048806085643250015
Epoch 1199
Epoch 1199 :: Batch 0/1
Batch Loss = 0.04873476890649619
1199, epoch_train_loss=0.04873476890649619
Epoch 1200
Epoch 1200 :: Batch 0/1
Batch Loss = 0.04863167133521686
1200, epoch_train_loss=0.04863167133521686
Epoch 1201
Epoch 1201 :: Batch 0/1
Batch Loss = 0.048547051464714996
1201, epoch_train_loss=0.048547051464714996
Epoch 1202
Epoch 1202 :: Batch 0/1
Batch Loss = 0.048481431412706666
1202, epoch_train_loss=0.048481431412706666
Epoch 1203
Epoch 1203 :: Batch 0/1
Batch Loss = 0.04841340727599063
1203, epoch_train_loss=0.04841340727599063
Epoch 1204
Epoch 1204 :: Batch 0/1
Batch Loss = 0.04833507979513906
1204, epoch_train_loss=0.04833507979513906
Epoch 1205
Epoch 1205 :: Batch 0/1
Batch Loss = 0.048252643512936774
1205, epoch_train_loss=0.048252643512936774
Epoch 1206
Epoch 1206 :: Batch 0/1
Batch Loss = 0.04817242888557013
1206, epoch_train_loss=0.04817242888557013
Epoch 1207
Epoch 1207 :: Batch 0/1
Batch Loss = 0.048093133599031085
1207, epoch_train_loss=0.048093133599031085
Epoch 1208
Epoch 1208 :: Batch 0/1
Batch Loss = 0.048016937494483235
1208, epoch_train_loss=0.048016937494483235
Epoch 1209
Epoch 1209 :: Batch 0/1
Batch Loss = 0.04794621403520005
1209, epoch_train_loss=0.04794621403520005
Epoch 1210
Epoch 1210 :: Batch 0/1
Batch Loss = 0.04787574230318308
1210, epoch_train_loss=0.04787574230318308
Epoch 1211
Epoch 1211 :: Batch 0/1
Batch Loss = 0.04779147798792263
1211, epoch_train_loss=0.04779147798792263
Epoch 1212
Epoch 1212 :: Batch 0/1
Batch Loss = 0.047689453759635875
1212, epoch_train_loss=0.047689453759635875
Epoch 1213
Epoch 1213 :: Batch 0/1
Batch Loss = 0.24060498047539447
1213, epoch_train_loss=0.24060498047539447
Epoch 1214
Epoch 1214 :: Batch 0/1
Batch Loss = 0.9137535306098469
1214, epoch_train_loss=0.9137535306098469
Epoch 1215
Epoch 1215 :: Batch 0/1
Batch Loss = 0.05470801593087976
1215, epoch_train_loss=0.05470801593087976
Epoch 1216
Epoch 1216 :: Batch 0/1
Batch Loss = 0.7669662077225872
1216, epoch_train_loss=0.7669662077225872
Epoch 1217
Epoch 1217 :: Batch 0/1
Batch Loss = 0.09658071505026483
1217, epoch_train_loss=0.09658071505026483
Epoch 1218
Epoch 1218 :: Batch 0/1
Batch Loss = 0.561605112709737
1218, epoch_train_loss=0.561605112709737
Epoch 1219
Epoch 1219 :: Batch 0/1
Batch Loss = 0.10873626685758608
1219, epoch_train_loss=0.10873626685758608
Epoch 1220
Epoch 1220 :: Batch 0/1
Batch Loss = 0.3561962691415847
1220, epoch_train_loss=0.3561962691415847
Epoch 1221
Epoch 1221 :: Batch 0/1
Batch Loss = 0.21174657433528243
1221, epoch_train_loss=0.21174657433528243
Epoch 1222
Epoch 1222 :: Batch 0/1
Batch Loss = 0.15244395355580978
1222, epoch_train_loss=0.15244395355580978
Epoch 1223
Epoch 1223 :: Batch 0/1
Batch Loss = 0.3296289985697524
1223, epoch_train_loss=0.3296289985697524
Epoch 1224
Epoch 1224 :: Batch 0/1
Batch Loss = 0.1407812270448204
1224, epoch_train_loss=0.1407812270448204
Epoch 1225
Epoch 1225 :: Batch 0/1
Batch Loss = 0.1736119576309442
1225, epoch_train_loss=0.1736119576309442
Epoch 1226
Epoch 1226 :: Batch 0/1
Batch Loss = 0.250528216204872
1226, epoch_train_loss=0.250528216204872
Epoch 1227
Epoch 1227 :: Batch 0/1
Batch Loss = 0.0961500623836686
1227, epoch_train_loss=0.0961500623836686
Epoch 1228
Epoch 1228 :: Batch 0/1
Batch Loss = 0.19002475720368048
1228, epoch_train_loss=0.19002475720368048
Epoch 1229
Epoch 1229 :: Batch 0/1
Batch Loss = 0.19140765824228387
1229, epoch_train_loss=0.19140765824228387
Epoch 1230
Epoch 1230 :: Batch 0/1
Batch Loss = 0.09542768349936408
1230, epoch_train_loss=0.09542768349936408
Epoch 1231
Epoch 1231 :: Batch 0/1
Batch Loss = 0.16945309942918638
1231, epoch_train_loss=0.16945309942918638
Epoch 1232
Epoch 1232 :: Batch 0/1
Batch Loss = 0.1353435271490723
1232, epoch_train_loss=0.1353435271490723
Epoch 1233
Epoch 1233 :: Batch 0/1
Batch Loss = 0.0999699426455232
1233, epoch_train_loss=0.0999699426455232
Epoch 1234
Epoch 1234 :: Batch 0/1
Batch Loss = 0.15702564715873638
1234, epoch_train_loss=0.15702564715873638
Epoch 1235
Epoch 1235 :: Batch 0/1
Batch Loss = 0.10845628894123924
1235, epoch_train_loss=0.10845628894123924
Epoch 1236
Epoch 1236 :: Batch 0/1
Batch Loss = 0.09877496232858837
1236, epoch_train_loss=0.09877496232858837
Epoch 1237
Epoch 1237 :: Batch 0/1
Batch Loss = 0.13530789378800048
1237, epoch_train_loss=0.13530789378800048
Epoch 1238
Epoch 1238 :: Batch 0/1
Batch Loss = 0.08439650370623941
1238, epoch_train_loss=0.08439650370623941
Epoch 1239
Epoch 1239 :: Batch 0/1
Batch Loss = 0.11254790399816966
1239, epoch_train_loss=0.11254790399816966
Epoch 1240
Epoch 1240 :: Batch 0/1
Batch Loss = 0.10743329305297805
1240, epoch_train_loss=0.10743329305297805
Epoch 1241
Epoch 1241 :: Batch 0/1
Batch Loss = 0.07754408726683325
1241, epoch_train_loss=0.07754408726683325
Epoch 1242
Epoch 1242 :: Batch 0/1
Batch Loss = 0.10635563788305057
1242, epoch_train_loss=0.10635563788305057
Epoch 1243
Epoch 1243 :: Batch 0/1
Batch Loss = 0.0801838391719786
1243, epoch_train_loss=0.0801838391719786
Epoch 1244
Epoch 1244 :: Batch 0/1
Batch Loss = 0.07686961995104553
1244, epoch_train_loss=0.07686961995104553
Epoch 1245
Epoch 1245 :: Batch 0/1
Batch Loss = 0.08721388742986223
1245, epoch_train_loss=0.08721388742986223
Epoch 1246
Epoch 1246 :: Batch 0/1
Batch Loss = 0.07034128903018115
1246, epoch_train_loss=0.07034128903018115
Epoch 1247
Epoch 1247 :: Batch 0/1
Batch Loss = 0.07669797003970526
1247, epoch_train_loss=0.07669797003970526
Epoch 1248
Epoch 1248 :: Batch 0/1
Batch Loss = 0.08122691654333342
1248, epoch_train_loss=0.08122691654333342
Epoch 1249
Epoch 1249 :: Batch 0/1
Batch Loss = 0.06845015656820116
1249, epoch_train_loss=0.06845015656820116
Epoch 1250
Epoch 1250 :: Batch 0/1
Batch Loss = 0.07137497018115398
1250, epoch_train_loss=0.07137497018115398
Epoch 1251
Epoch 1251 :: Batch 0/1
Batch Loss = 0.05904234006296672
1251, epoch_train_loss=0.05904234006296672
Epoch 1252
Epoch 1252 :: Batch 0/1
Batch Loss = 0.06510712780597214
1252, epoch_train_loss=0.06510712780597214
Epoch 1253
Epoch 1253 :: Batch 0/1
Batch Loss = 0.06146361746512663
1253, epoch_train_loss=0.06146361746512663
Epoch 1254
Epoch 1254 :: Batch 0/1
Batch Loss = 0.05265390339124962
1254, epoch_train_loss=0.05265390339124962
Epoch 1255
Epoch 1255 :: Batch 0/1
Batch Loss = 0.06212205226099639
1255, epoch_train_loss=0.06212205226099639
Epoch 1256
Epoch 1256 :: Batch 0/1
Batch Loss = 0.05544485835831682
1256, epoch_train_loss=0.05544485835831682
Epoch 1257
Epoch 1257 :: Batch 0/1
Batch Loss = 0.05140705140493876
1257, epoch_train_loss=0.05140705140493876
Epoch 1258
Epoch 1258 :: Batch 0/1
Batch Loss = 0.05084563109913958
1258, epoch_train_loss=0.05084563109913958
Epoch 1259
Epoch 1259 :: Batch 0/1
Batch Loss = 0.05209347398232308
1259, epoch_train_loss=0.05209347398232308
Epoch 1260
Epoch 1260 :: Batch 0/1
Batch Loss = 0.05084166265206681
1260, epoch_train_loss=0.05084166265206681
Epoch 1261
Epoch 1261 :: Batch 0/1
Batch Loss = 0.044664649167458825
1261, epoch_train_loss=0.044664649167458825
Epoch 1262
Epoch 1262 :: Batch 0/1
Batch Loss = 0.0503527029279788
1262, epoch_train_loss=0.0503527029279788
Epoch 1263
Epoch 1263 :: Batch 0/1
Batch Loss = 0.04714996526654568
1263, epoch_train_loss=0.04714996526654568
Epoch 1264
Epoch 1264 :: Batch 0/1
Batch Loss = 0.04564828605809838
1264, epoch_train_loss=0.04564828605809838
Epoch 1265
Epoch 1265 :: Batch 0/1
Batch Loss = 0.04648627552081758
1265, epoch_train_loss=0.04648627552081758
Epoch 1266
Epoch 1266 :: Batch 0/1
Batch Loss = 0.04648648638001201
1266, epoch_train_loss=0.04648648638001201
Epoch 1267
Epoch 1267 :: Batch 0/1
Batch Loss = 0.04563174759335977
1267, epoch_train_loss=0.04563174759335977
Epoch 1268
Epoch 1268 :: Batch 0/1
Batch Loss = 0.04301713027477585
1268, epoch_train_loss=0.04301713027477585
Epoch 1269
Epoch 1269 :: Batch 0/1
Batch Loss = 0.04634961964047186
1269, epoch_train_loss=0.04634961964047186
Epoch 1270
Epoch 1270 :: Batch 0/1
Batch Loss = 0.04416070609182495
1270, epoch_train_loss=0.04416070609182495
Epoch 1271
Epoch 1271 :: Batch 0/1
Batch Loss = 0.04387630635749137
1271, epoch_train_loss=0.04387630635749137
Epoch 1272
Epoch 1272 :: Batch 0/1
Batch Loss = 0.043288714212984795
1272, epoch_train_loss=0.043288714212984795
Epoch 1273
Epoch 1273 :: Batch 0/1
Batch Loss = 0.04417380253352246
1273, epoch_train_loss=0.04417380253352246
Epoch 1274
Epoch 1274 :: Batch 0/1
Batch Loss = 0.04222173287183595
1274, epoch_train_loss=0.04222173287183595
Epoch 1275
Epoch 1275 :: Batch 0/1
Batch Loss = 0.043371163404406665
1275, epoch_train_loss=0.043371163404406665
Epoch 1276
Epoch 1276 :: Batch 0/1
Batch Loss = 0.04304625212491126
1276, epoch_train_loss=0.04304625212491126
Epoch 1277
Epoch 1277 :: Batch 0/1
Batch Loss = 0.04224797372074584
1277, epoch_train_loss=0.04224797372074584
Epoch 1278
Epoch 1278 :: Batch 0/1
Batch Loss = 0.04243401688400922
1278, epoch_train_loss=0.04243401688400922
Epoch 1279
Epoch 1279 :: Batch 0/1
Batch Loss = 0.04248714610178916
1279, epoch_train_loss=0.04248714610178916
Epoch 1280
Epoch 1280 :: Batch 0/1
Batch Loss = 0.04215394617170494
1280, epoch_train_loss=0.04215394617170494
Epoch 1281
Epoch 1281 :: Batch 0/1
Batch Loss = 0.04203873790720291
1281, epoch_train_loss=0.04203873790720291
Epoch 1282
Epoch 1282 :: Batch 0/1
Batch Loss = 0.042487740454389904
1282, epoch_train_loss=0.042487740454389904
Epoch 1283
Epoch 1283 :: Batch 0/1
Batch Loss = 0.04169991916022716
1283, epoch_train_loss=0.04169991916022716
Epoch 1284
Epoch 1284 :: Batch 0/1
Batch Loss = 0.041928295799566834
1284, epoch_train_loss=0.041928295799566834
Epoch 1285
Epoch 1285 :: Batch 0/1
Batch Loss = 0.04176655514236814
1285, epoch_train_loss=0.04176655514236814
Epoch 1286
Epoch 1286 :: Batch 0/1
Batch Loss = 0.041850052737026126
1286, epoch_train_loss=0.041850052737026126
Epoch 1287
Epoch 1287 :: Batch 0/1
Batch Loss = 0.04124752268695648
1287, epoch_train_loss=0.04124752268695648
Epoch 1288
Epoch 1288 :: Batch 0/1
Batch Loss = 0.041539893930602624
1288, epoch_train_loss=0.041539893930602624
Epoch 1289
Epoch 1289 :: Batch 0/1
Batch Loss = 0.041182872900317996
1289, epoch_train_loss=0.041182872900317996
Epoch 1290
Epoch 1290 :: Batch 0/1
Batch Loss = 0.04107089505149279
1290, epoch_train_loss=0.04107089505149279
Epoch 1291
Epoch 1291 :: Batch 0/1
Batch Loss = 0.041165866416092135
1291, epoch_train_loss=0.041165866416092135
Epoch 1292
Epoch 1292 :: Batch 0/1
Batch Loss = 0.04100266595938025
1292, epoch_train_loss=0.04100266595938025
Epoch 1293
Epoch 1293 :: Batch 0/1
Batch Loss = 0.040924133518235574
1293, epoch_train_loss=0.040924133518235574
Epoch 1294
Epoch 1294 :: Batch 0/1
Batch Loss = 0.04101708325986956
1294, epoch_train_loss=0.04101708325986956
Epoch 1295
Epoch 1295 :: Batch 0/1
Batch Loss = 0.04077928705782844
1295, epoch_train_loss=0.04077928705782844
Epoch 1296
Epoch 1296 :: Batch 0/1
Batch Loss = 0.04089074851055689
1296, epoch_train_loss=0.04089074851055689
Epoch 1297
Epoch 1297 :: Batch 0/1
Batch Loss = 0.04080935455185459
1297, epoch_train_loss=0.04080935455185459
Epoch 1298
Epoch 1298 :: Batch 0/1
Batch Loss = 0.040620048868425115
1298, epoch_train_loss=0.040620048868425115
Epoch 1299
Epoch 1299 :: Batch 0/1
Batch Loss = 0.04069321673242678
1299, epoch_train_loss=0.04069321673242678
Epoch 1300
Epoch 1300 :: Batch 0/1
Batch Loss = 0.04049289085266354
1300, epoch_train_loss=0.04049289085266354
Epoch 1301
Epoch 1301 :: Batch 0/1
Batch Loss = 0.04050809188851799
1301, epoch_train_loss=0.04050809188851799
Epoch 1302
Epoch 1302 :: Batch 0/1
Batch Loss = 0.04042366762288017
1302, epoch_train_loss=0.04042366762288017
Epoch 1303
Epoch 1303 :: Batch 0/1
Batch Loss = 0.040361778601798175
1303, epoch_train_loss=0.040361778601798175
Epoch 1304
Epoch 1304 :: Batch 0/1
Batch Loss = 0.04033146975338252
1304, epoch_train_loss=0.04033146975338252
Epoch 1305
Epoch 1305 :: Batch 0/1
Batch Loss = 0.040281877940596504
1305, epoch_train_loss=0.040281877940596504
Epoch 1306
Epoch 1306 :: Batch 0/1
Batch Loss = 0.04022707646992101
1306, epoch_train_loss=0.04022707646992101
Epoch 1307
Epoch 1307 :: Batch 0/1
Batch Loss = 0.04021685460536974
1307, epoch_train_loss=0.04021685460536974
Epoch 1308
Epoch 1308 :: Batch 0/1
Batch Loss = 0.0400977442291412
1308, epoch_train_loss=0.0400977442291412
Epoch 1309
Epoch 1309 :: Batch 0/1
Batch Loss = 0.040101369940077736
1309, epoch_train_loss=0.040101369940077736
Epoch 1310
Epoch 1310 :: Batch 0/1
Batch Loss = 0.039987955660207825
1310, epoch_train_loss=0.039987955660207825
Epoch 1311
Epoch 1311 :: Batch 0/1
Batch Loss = 0.03998061966939375
1311, epoch_train_loss=0.03998061966939375
Epoch 1312
Epoch 1312 :: Batch 0/1
Batch Loss = 0.039914488397828005
1312, epoch_train_loss=0.039914488397828005
Epoch 1313
Epoch 1313 :: Batch 0/1
Batch Loss = 0.03987247209971629
1313, epoch_train_loss=0.03987247209971629
Epoch 1314
Epoch 1314 :: Batch 0/1
Batch Loss = 0.03984079948079992
1314, epoch_train_loss=0.03984079948079992
Epoch 1315
Epoch 1315 :: Batch 0/1
Batch Loss = 0.03978172034972048
1315, epoch_train_loss=0.03978172034972048
Epoch 1316
Epoch 1316 :: Batch 0/1
Batch Loss = 0.039755383776259305
1316, epoch_train_loss=0.039755383776259305
Epoch 1317
Epoch 1317 :: Batch 0/1
Batch Loss = 0.03970167264641065
1317, epoch_train_loss=0.03970167264641065
Epoch 1318
Epoch 1318 :: Batch 0/1
Batch Loss = 0.039655535947523614
1318, epoch_train_loss=0.039655535947523614
Epoch 1319
Epoch 1319 :: Batch 0/1
Batch Loss = 0.03961509666846661
1319, epoch_train_loss=0.03961509666846661
Epoch 1320
Epoch 1320 :: Batch 0/1
Batch Loss = 0.03956173875811671
1320, epoch_train_loss=0.03956173875811671
Epoch 1321
Epoch 1321 :: Batch 0/1
Batch Loss = 0.03953008357568119
1321, epoch_train_loss=0.03953008357568119
Epoch 1322
Epoch 1322 :: Batch 0/1
Batch Loss = 0.039484416001778815
1322, epoch_train_loss=0.039484416001778815
Epoch 1323
Epoch 1323 :: Batch 0/1
Batch Loss = 0.03944447144883339
1323, epoch_train_loss=0.03944447144883339
Epoch 1324
Epoch 1324 :: Batch 0/1
Batch Loss = 0.03940879901082099
1324, epoch_train_loss=0.03940879901082099
Epoch 1325
Epoch 1325 :: Batch 0/1
Batch Loss = 0.039359063285377534
1325, epoch_train_loss=0.039359063285377534
Epoch 1326
Epoch 1326 :: Batch 0/1
Batch Loss = 0.03932882330543874
1326, epoch_train_loss=0.03932882330543874
Epoch 1327
Epoch 1327 :: Batch 0/1
Batch Loss = 0.039278040513514446
1327, epoch_train_loss=0.039278040513514446
Epoch 1328
Epoch 1328 :: Batch 0/1
Batch Loss = 0.03924347581950586
1328, epoch_train_loss=0.03924347581950586
Epoch 1329
Epoch 1329 :: Batch 0/1
Batch Loss = 0.03919860344801895
1329, epoch_train_loss=0.03919860344801895
Epoch 1330
Epoch 1330 :: Batch 0/1
Batch Loss = 0.03915933118155867
1330, epoch_train_loss=0.03915933118155867
Epoch 1331
Epoch 1331 :: Batch 0/1
Batch Loss = 0.03912139562196537
1331, epoch_train_loss=0.03912139562196537
Epoch 1332
Epoch 1332 :: Batch 0/1
Batch Loss = 0.039081231777529445
1332, epoch_train_loss=0.039081231777529445
Epoch 1333
Epoch 1333 :: Batch 0/1
Batch Loss = 0.039042920612219305
1333, epoch_train_loss=0.039042920612219305
Epoch 1334
Epoch 1334 :: Batch 0/1
Batch Loss = 0.03900440097633965
1334, epoch_train_loss=0.03900440097633965
Epoch 1335
Epoch 1335 :: Batch 0/1
Batch Loss = 0.03896288311887529
1335, epoch_train_loss=0.03896288311887529
Epoch 1336
Epoch 1336 :: Batch 0/1
Batch Loss = 0.03892712121593779
1336, epoch_train_loss=0.03892712121593779
Epoch 1337
Epoch 1337 :: Batch 0/1
Batch Loss = 0.03888456790448307
1337, epoch_train_loss=0.03888456790448307
Epoch 1338
Epoch 1338 :: Batch 0/1
Batch Loss = 0.03884857944327777
1338, epoch_train_loss=0.03884857944327777
Epoch 1339
Epoch 1339 :: Batch 0/1
Batch Loss = 0.03880800642928834
1339, epoch_train_loss=0.03880800642928834
Epoch 1340
Epoch 1340 :: Batch 0/1
Batch Loss = 0.03877065544343335
1340, epoch_train_loss=0.03877065544343335
Epoch 1341
Epoch 1341 :: Batch 0/1
Batch Loss = 0.038733188162906884
1341, epoch_train_loss=0.038733188162906884
Epoch 1342
Epoch 1342 :: Batch 0/1
Batch Loss = 0.03869513212399631
1342, epoch_train_loss=0.03869513212399631
Epoch 1343
Epoch 1343 :: Batch 0/1
Batch Loss = 0.038658302853598246
1343, epoch_train_loss=0.038658302853598246
Epoch 1344
Epoch 1344 :: Batch 0/1
Batch Loss = 0.038620769080174935
1344, epoch_train_loss=0.038620769080174935
Epoch 1345
Epoch 1345 :: Batch 0/1
Batch Loss = 0.03858328781595205
1345, epoch_train_loss=0.03858328781595205
Epoch 1346
Epoch 1346 :: Batch 0/1
Batch Loss = 0.03854709972138722
1346, epoch_train_loss=0.03854709972138722
Epoch 1347
Epoch 1347 :: Batch 0/1
Batch Loss = 0.0385096049327475
1347, epoch_train_loss=0.0385096049327475
Epoch 1348
Epoch 1348 :: Batch 0/1
Batch Loss = 0.038473807033847
1348, epoch_train_loss=0.038473807033847
Epoch 1349
Epoch 1349 :: Batch 0/1
Batch Loss = 0.038437337875988666
1349, epoch_train_loss=0.038437337875988666
Epoch 1350
Epoch 1350 :: Batch 0/1
Batch Loss = 0.03840137585266657
1350, epoch_train_loss=0.03840137585266657
Epoch 1351
Epoch 1351 :: Batch 0/1
Batch Loss = 0.03836625396220254
1351, epoch_train_loss=0.03836625396220254
Epoch 1352
Epoch 1352 :: Batch 0/1
Batch Loss = 0.03833041930321305
1352, epoch_train_loss=0.03833041930321305
Epoch 1353
Epoch 1353 :: Batch 0/1
Batch Loss = 0.03829566331983145
1353, epoch_train_loss=0.03829566331983145
Epoch 1354
Epoch 1354 :: Batch 0/1
Batch Loss = 0.03826046632715406
1354, epoch_train_loss=0.03826046632715406
Epoch 1355
Epoch 1355 :: Batch 0/1
Batch Loss = 0.03822569343590285
1355, epoch_train_loss=0.03822569343590285
Epoch 1356
Epoch 1356 :: Batch 0/1
Batch Loss = 0.038191343411153864
1356, epoch_train_loss=0.038191343411153864
Epoch 1357
Epoch 1357 :: Batch 0/1
Batch Loss = 0.03815695860164624
1357, epoch_train_loss=0.03815695860164624
Epoch 1358
Epoch 1358 :: Batch 0/1
Batch Loss = 0.03812300822517051
1358, epoch_train_loss=0.03812300822517051
Epoch 1359
Epoch 1359 :: Batch 0/1
Batch Loss = 0.03808939478902058
1359, epoch_train_loss=0.03808939478902058
Epoch 1360
Epoch 1360 :: Batch 0/1
Batch Loss = 0.03805575161103304
1360, epoch_train_loss=0.03805575161103304
Epoch 1361
Epoch 1361 :: Batch 0/1
Batch Loss = 0.038022873546213046
1361, epoch_train_loss=0.038022873546213046
Epoch 1362
Epoch 1362 :: Batch 0/1
Batch Loss = 0.03798977542699641
1362, epoch_train_loss=0.03798977542699641
Epoch 1363
Epoch 1363 :: Batch 0/1
Batch Loss = 0.03795726386118261
1363, epoch_train_loss=0.03795726386118261
Epoch 1364
Epoch 1364 :: Batch 0/1
Batch Loss = 0.03792485095867595
1364, epoch_train_loss=0.03792485095867595
Epoch 1365
Epoch 1365 :: Batch 0/1
Batch Loss = 0.037892736042566384
1365, epoch_train_loss=0.037892736042566384
Epoch 1366
Epoch 1366 :: Batch 0/1
Batch Loss = 0.037860967128536704
1366, epoch_train_loss=0.037860967128536704
Epoch 1367
Epoch 1367 :: Batch 0/1
Batch Loss = 0.037829486946830373
1367, epoch_train_loss=0.037829486946830373
Epoch 1368
Epoch 1368 :: Batch 0/1
Batch Loss = 0.03779822176582873
1368, epoch_train_loss=0.03779822176582873
Epoch 1369
Epoch 1369 :: Batch 0/1
Batch Loss = 0.03776743693560782
1369, epoch_train_loss=0.03776743693560782
Epoch 1370
Epoch 1370 :: Batch 0/1
Batch Loss = 0.03773676762055087
1370, epoch_train_loss=0.03773676762055087
Epoch 1371
Epoch 1371 :: Batch 0/1
Batch Loss = 0.037706599457020526
1371, epoch_train_loss=0.037706599457020526
Epoch 1372
Epoch 1372 :: Batch 0/1
Batch Loss = 0.03767662763620718
1372, epoch_train_loss=0.03767662763620718
Epoch 1373
Epoch 1373 :: Batch 0/1
Batch Loss = 0.03764699115080638
1373, epoch_train_loss=0.03764699115080638
Epoch 1374
Epoch 1374 :: Batch 0/1
Batch Loss = 0.03761768010073791
1374, epoch_train_loss=0.03761768010073791
Epoch 1375
Epoch 1375 :: Batch 0/1
Batch Loss = 0.037588658443727684
1375, epoch_train_loss=0.037588658443727684
Epoch 1376
Epoch 1376 :: Batch 0/1
Batch Loss = 0.0375599461296542
1376, epoch_train_loss=0.0375599461296542
Epoch 1377
Epoch 1377 :: Batch 0/1
Batch Loss = 0.037531576021397506
1377, epoch_train_loss=0.037531576021397506
Epoch 1378
Epoch 1378 :: Batch 0/1
Batch Loss = 0.03750342804289183
1378, epoch_train_loss=0.03750342804289183
Epoch 1379
Epoch 1379 :: Batch 0/1
Batch Loss = 0.037475635662813034
1379, epoch_train_loss=0.037475635662813034
Epoch 1380
Epoch 1380 :: Batch 0/1
Batch Loss = 0.037448072587990315
1380, epoch_train_loss=0.037448072587990315
Epoch 1381
Epoch 1381 :: Batch 0/1
Batch Loss = 0.03742079041167207
1381, epoch_train_loss=0.03742079041167207
Epoch 1382
Epoch 1382 :: Batch 0/1
Batch Loss = 0.037393766646642165
1382, epoch_train_loss=0.037393766646642165
Epoch 1383
Epoch 1383 :: Batch 0/1
Batch Loss = 0.037366956317053654
1383, epoch_train_loss=0.037366956317053654
Epoch 1384
Epoch 1384 :: Batch 0/1
Batch Loss = 0.037340382608300735
1384, epoch_train_loss=0.037340382608300735
Epoch 1385
Epoch 1385 :: Batch 0/1
Batch Loss = 0.037314027671302534
1385, epoch_train_loss=0.037314027671302534
Epoch 1386
Epoch 1386 :: Batch 0/1
Batch Loss = 0.03728785920869741
1386, epoch_train_loss=0.03728785920869741
Epoch 1387
Epoch 1387 :: Batch 0/1
Batch Loss = 0.037261904572964154
1387, epoch_train_loss=0.037261904572964154
Epoch 1388
Epoch 1388 :: Batch 0/1
Batch Loss = 0.03723611340202823
1388, epoch_train_loss=0.03723611340202823
Epoch 1389
Epoch 1389 :: Batch 0/1
Batch Loss = 0.03721049693000068
1389, epoch_train_loss=0.03721049693000068
Epoch 1390
Epoch 1390 :: Batch 0/1
Batch Loss = 0.037185052711376725
1390, epoch_train_loss=0.037185052711376725
Epoch 1391
Epoch 1391 :: Batch 0/1
Batch Loss = 0.037159752130678694
1391, epoch_train_loss=0.037159752130678694
Epoch 1392
Epoch 1392 :: Batch 0/1
Batch Loss = 0.0371346124314862
1392, epoch_train_loss=0.0371346124314862
Epoch 1393
Epoch 1393 :: Batch 0/1
Batch Loss = 0.03710961454651475
1393, epoch_train_loss=0.03710961454651475
Epoch 1394
Epoch 1394 :: Batch 0/1
Batch Loss = 0.037084756915901844
1394, epoch_train_loss=0.037084756915901844
Epoch 1395
Epoch 1395 :: Batch 0/1
Batch Loss = 0.037060049159341434
1395, epoch_train_loss=0.037060049159341434
Epoch 1396
Epoch 1396 :: Batch 0/1
Batch Loss = 0.037035479769763444
1396, epoch_train_loss=0.037035479769763444
Epoch 1397
Epoch 1397 :: Batch 0/1
Batch Loss = 0.03701105258985017
1397, epoch_train_loss=0.03701105258985017
Epoch 1398
Epoch 1398 :: Batch 0/1
Batch Loss = 0.036986772930349784
1398, epoch_train_loss=0.036986772930349784
Epoch 1399
Epoch 1399 :: Batch 0/1
Batch Loss = 0.0369626288958221
1399, epoch_train_loss=0.0369626288958221
Epoch 1400
Epoch 1400 :: Batch 0/1
Batch Loss = 0.03693863461305788
1400, epoch_train_loss=0.03693863461305788
Epoch 1401
Epoch 1401 :: Batch 0/1
Batch Loss = 0.03691478109937342
1401, epoch_train_loss=0.03691478109937342
Epoch 1402
Epoch 1402 :: Batch 0/1
Batch Loss = 0.03689107032000024
1402, epoch_train_loss=0.03689107032000024
Epoch 1403
Epoch 1403 :: Batch 0/1
Batch Loss = 0.036867503309685086
1403, epoch_train_loss=0.036867503309685086
Epoch 1404
Epoch 1404 :: Batch 0/1
Batch Loss = 0.03684407627272162
1404, epoch_train_loss=0.03684407627272162
Epoch 1405
Epoch 1405 :: Batch 0/1
Batch Loss = 0.036820787351813086
1405, epoch_train_loss=0.036820787351813086
Epoch 1406
Epoch 1406 :: Batch 0/1
Batch Loss = 0.03679763811633278
1406, epoch_train_loss=0.03679763811633278
Epoch 1407
Epoch 1407 :: Batch 0/1
Batch Loss = 0.036774618136765184
1407, epoch_train_loss=0.036774618136765184
Epoch 1408
Epoch 1408 :: Batch 0/1
Batch Loss = 0.03675172964225189
1408, epoch_train_loss=0.03675172964225189
Epoch 1409
Epoch 1409 :: Batch 0/1
Batch Loss = 0.03672896317971771
1409, epoch_train_loss=0.03672896317971771
Epoch 1410
Epoch 1410 :: Batch 0/1
Batch Loss = 0.03670631417140648
1410, epoch_train_loss=0.03670631417140648
Epoch 1411
Epoch 1411 :: Batch 0/1
Batch Loss = 0.036683776103956706
1411, epoch_train_loss=0.036683776103956706
Epoch 1412
Epoch 1412 :: Batch 0/1
Batch Loss = 0.03666134039367118
1412, epoch_train_loss=0.03666134039367118
Epoch 1413
Epoch 1413 :: Batch 0/1
Batch Loss = 0.036638997131524055
1413, epoch_train_loss=0.036638997131524055
Epoch 1414
Epoch 1414 :: Batch 0/1
Batch Loss = 0.03661673865647605
1414, epoch_train_loss=0.03661673865647605
Epoch 1415
Epoch 1415 :: Batch 0/1
Batch Loss = 0.036594550055387746
1415, epoch_train_loss=0.036594550055387746
Epoch 1416
Epoch 1416 :: Batch 0/1
Batch Loss = 0.03657242103202311
1416, epoch_train_loss=0.03657242103202311
Epoch 1417
Epoch 1417 :: Batch 0/1
Batch Loss = 0.03655033459466992
1417, epoch_train_loss=0.03655033459466992
Epoch 1418
Epoch 1418 :: Batch 0/1
Batch Loss = 0.03652827386554477
1418, epoch_train_loss=0.03652827386554477
Epoch 1419
Epoch 1419 :: Batch 0/1
Batch Loss = 0.036506219201677674
1419, epoch_train_loss=0.036506219201677674
Epoch 1420
Epoch 1420 :: Batch 0/1
Batch Loss = 0.03648414797491502
1420, epoch_train_loss=0.03648414797491502
Epoch 1421
Epoch 1421 :: Batch 0/1
Batch Loss = 0.036462033818173256
1421, epoch_train_loss=0.036462033818173256
Epoch 1422
Epoch 1422 :: Batch 0/1
Batch Loss = 0.03643984929718714
1422, epoch_train_loss=0.03643984929718714
Epoch 1423
Epoch 1423 :: Batch 0/1
Batch Loss = 0.036417561369515745
1423, epoch_train_loss=0.036417561369515745
Epoch 1424
Epoch 1424 :: Batch 0/1
Batch Loss = 0.03639513757851264
1424, epoch_train_loss=0.03639513757851264
Epoch 1425
Epoch 1425 :: Batch 0/1
Batch Loss = 0.03637254195705777
1425, epoch_train_loss=0.03637254195705777
Epoch 1426
Epoch 1426 :: Batch 0/1
Batch Loss = 0.036349739932863175
1426, epoch_train_loss=0.036349739932863175
Epoch 1427
Epoch 1427 :: Batch 0/1
Batch Loss = 0.03632669993832249
1427, epoch_train_loss=0.03632669993832249
Epoch 1428
Epoch 1428 :: Batch 0/1
Batch Loss = 0.03630339638057375
1428, epoch_train_loss=0.03630339638057375
Epoch 1429
Epoch 1429 :: Batch 0/1
Batch Loss = 0.03627981223134748
1429, epoch_train_loss=0.03627981223134748
Epoch 1430
Epoch 1430 :: Batch 0/1
Batch Loss = 0.0362559428300785
1430, epoch_train_loss=0.0362559428300785
Epoch 1431
Epoch 1431 :: Batch 0/1
Batch Loss = 0.03623179668623913
1431, epoch_train_loss=0.03623179668623913
Epoch 1432
Epoch 1432 :: Batch 0/1
Batch Loss = 0.036207399630145604
1432, epoch_train_loss=0.036207399630145604
Epoch 1433
Epoch 1433 :: Batch 0/1
Batch Loss = 0.03618279167859681
1433, epoch_train_loss=0.03618279167859681
Epoch 1434
Epoch 1434 :: Batch 0/1
Batch Loss = 0.036158026805079534
1434, epoch_train_loss=0.036158026805079534
Epoch 1435
Epoch 1435 :: Batch 0/1
Batch Loss = 0.03613316875233402
1435, epoch_train_loss=0.03613316875233402
Epoch 1436
Epoch 1436 :: Batch 0/1
Batch Loss = 0.03610828879047263
1436, epoch_train_loss=0.03610828879047263
Epoch 1437
Epoch 1437 :: Batch 0/1
Batch Loss = 0.03608346219873008
1437, epoch_train_loss=0.03608346219873008
Epoch 1438
Epoch 1438 :: Batch 0/1
Batch Loss = 0.03605876286523458
1438, epoch_train_loss=0.03605876286523458
Epoch 1439
Epoch 1439 :: Batch 0/1
Batch Loss = 0.03603425907743177
1439, epoch_train_loss=0.03603425907743177
Epoch 1440
Epoch 1440 :: Batch 0/1
Batch Loss = 0.03601000933967767
1440, epoch_train_loss=0.03601000933967767
Epoch 1441
Epoch 1441 :: Batch 0/1
Batch Loss = 0.035986059889655884
1441, epoch_train_loss=0.035986059889655884
Epoch 1442
Epoch 1442 :: Batch 0/1
Batch Loss = 0.03596244238478714
1442, epoch_train_loss=0.03596244238478714
Epoch 1443
Epoch 1443 :: Batch 0/1
Batch Loss = 0.03593917294901389
1443, epoch_train_loss=0.03593917294901389
Epoch 1444
Epoch 1444 :: Batch 0/1
Batch Loss = 0.035916253818649324
1444, epoch_train_loss=0.035916253818649324
Epoch 1445
Epoch 1445 :: Batch 0/1
Batch Loss = 0.03589367533592282
1445, epoch_train_loss=0.03589367533592282
Epoch 1446
Epoch 1446 :: Batch 0/1
Batch Loss = 0.03587141932626576
1446, epoch_train_loss=0.03587141932626576
Epoch 1447
Epoch 1447 :: Batch 0/1
Batch Loss = 0.03584946130131727
1447, epoch_train_loss=0.03584946130131727
Epoch 1448
Epoch 1448 :: Batch 0/1
Batch Loss = 0.03582777354325046
1448, epoch_train_loss=0.03582777354325046
Epoch 1449
Epoch 1449 :: Batch 0/1
Batch Loss = 0.0358063267452778
1449, epoch_train_loss=0.0358063267452778
Epoch 1450
Epoch 1450 :: Batch 0/1
Batch Loss = 0.035785091733191436
1450, epoch_train_loss=0.035785091733191436
Epoch 1451
Epoch 1451 :: Batch 0/1
Batch Loss = 0.035764040449121126
1451, epoch_train_loss=0.035764040449121126
Epoch 1452
Epoch 1452 :: Batch 0/1
Batch Loss = 0.0357431468781585
1452, epoch_train_loss=0.0357431468781585
Epoch 1453
Epoch 1453 :: Batch 0/1
Batch Loss = 0.03572238775326655
1453, epoch_train_loss=0.03572238775326655
Epoch 1454
Epoch 1454 :: Batch 0/1
Batch Loss = 0.035701742754402375
1454, epoch_train_loss=0.035701742754402375
Epoch 1455
Epoch 1455 :: Batch 0/1
Batch Loss = 0.035681194843919344
1455, epoch_train_loss=0.035681194843919344
Epoch 1456
Epoch 1456 :: Batch 0/1
Batch Loss = 0.0356607299929818
1456, epoch_train_loss=0.0356607299929818
Epoch 1457
Epoch 1457 :: Batch 0/1
Batch Loss = 0.03564033706240901
1457, epoch_train_loss=0.03564033706240901
Epoch 1458
Epoch 1458 :: Batch 0/1
Batch Loss = 0.035620007212782724
1458, epoch_train_loss=0.035620007212782724
Epoch 1459
Epoch 1459 :: Batch 0/1
Batch Loss = 0.03559973350861912
1459, epoch_train_loss=0.03559973350861912
Epoch 1460
Epoch 1460 :: Batch 0/1
Batch Loss = 0.035579510366227834
1460, epoch_train_loss=0.035579510366227834
Epoch 1461
Epoch 1461 :: Batch 0/1
Batch Loss = 0.03555933310831858
1461, epoch_train_loss=0.03555933310831858
Epoch 1462
Epoch 1462 :: Batch 0/1
Batch Loss = 0.03553919754828951
1462, epoch_train_loss=0.03553919754828951
Epoch 1463
Epoch 1463 :: Batch 0/1
Batch Loss = 0.035519099528882186
1463, epoch_train_loss=0.035519099528882186
Epoch 1464
Epoch 1464 :: Batch 0/1
Batch Loss = 0.035499034501844176
1464, epoch_train_loss=0.035499034501844176
Epoch 1465
Epoch 1465 :: Batch 0/1
Batch Loss = 0.03547899706458419
1465, epoch_train_loss=0.03547899706458419
Epoch 1466
Epoch 1466 :: Batch 0/1
Batch Loss = 0.0354589805204547
1466, epoch_train_loss=0.0354589805204547
Epoch 1467
Epoch 1467 :: Batch 0/1
Batch Loss = 0.03543897644313818
1467, epoch_train_loss=0.03543897644313818
Epoch 1468
Epoch 1468 :: Batch 0/1
Batch Loss = 0.03541897410735686
1468, epoch_train_loss=0.03541897410735686
Epoch 1469
Epoch 1469 :: Batch 0/1
Batch Loss = 0.03539895994995935
1469, epoch_train_loss=0.03539895994995935
Epoch 1470
Epoch 1470 :: Batch 0/1
Batch Loss = 0.03537891683867076
1470, epoch_train_loss=0.03537891683867076
Epoch 1471
Epoch 1471 :: Batch 0/1
Batch Loss = 0.03535882329506061
1471, epoch_train_loss=0.03535882329506061
Epoch 1472
Epoch 1472 :: Batch 0/1
Batch Loss = 0.03533865230445314
1472, epoch_train_loss=0.03533865230445314
Epoch 1473
Epoch 1473 :: Batch 0/1
Batch Loss = 0.0353183697734111
1473, epoch_train_loss=0.0353183697734111
Epoch 1474
Epoch 1474 :: Batch 0/1
Batch Loss = 0.0352979323921502
1474, epoch_train_loss=0.0352979323921502
Epoch 1475
Epoch 1475 :: Batch 0/1
Batch Loss = 0.03527728479260499
1475, epoch_train_loss=0.03527728479260499
Epoch 1476
Epoch 1476 :: Batch 0/1
Batch Loss = 0.035256355644760436
1476, epoch_train_loss=0.035256355644760436
Epoch 1477
Epoch 1477 :: Batch 0/1
Batch Loss = 0.03523505237233676
1477, epoch_train_loss=0.03523505237233676
Epoch 1478
Epoch 1478 :: Batch 0/1
Batch Loss = 0.035213254239335516
1478, epoch_train_loss=0.035213254239335516
Epoch 1479
Epoch 1479 :: Batch 0/1
Batch Loss = 0.03519080388482647
1479, epoch_train_loss=0.03519080388482647
Epoch 1480
Epoch 1480 :: Batch 0/1
Batch Loss = 0.035167498014831274
1480, epoch_train_loss=0.035167498014831274
Epoch 1481
Epoch 1481 :: Batch 0/1
Batch Loss = 0.03514307984711548
1481, epoch_train_loss=0.03514307984711548
Epoch 1482
Epoch 1482 :: Batch 0/1
Batch Loss = 0.03511723909664818
1482, epoch_train_loss=0.03511723909664818
Epoch 1483
Epoch 1483 :: Batch 0/1
Batch Loss = 0.035089629945176386
1483, epoch_train_loss=0.035089629945176386
Epoch 1484
Epoch 1484 :: Batch 0/1
Batch Loss = 0.03505991932053477
1484, epoch_train_loss=0.03505991932053477
Epoch 1485
Epoch 1485 :: Batch 0/1
Batch Loss = 0.03502786811667808
1485, epoch_train_loss=0.03502786811667808
Epoch 1486
Epoch 1486 :: Batch 0/1
Batch Loss = 0.0349934185170605
1486, epoch_train_loss=0.0349934185170605
Epoch 1487
Epoch 1487 :: Batch 0/1
Batch Loss = 0.0349567286839744
1487, epoch_train_loss=0.0349567286839744
Epoch 1488
Epoch 1488 :: Batch 0/1
Batch Loss = 0.03491810780623566
1488, epoch_train_loss=0.03491810780623566
Epoch 1489
Epoch 1489 :: Batch 0/1
Batch Loss = 0.03487788002754381
1489, epoch_train_loss=0.03487788002754381
Epoch 1490
Epoch 1490 :: Batch 0/1
Batch Loss = 0.03483628943991152
1490, epoch_train_loss=0.03483628943991152
Epoch 1491
Epoch 1491 :: Batch 0/1
Batch Loss = 0.03479354344085406
1491, epoch_train_loss=0.03479354344085406
Epoch 1492
Epoch 1492 :: Batch 0/1
Batch Loss = 0.03474995708971779
1492, epoch_train_loss=0.03474995708971779
Epoch 1493
Epoch 1493 :: Batch 0/1
Batch Loss = 0.03470605521022397
1493, epoch_train_loss=0.03470605521022397
Epoch 1494
Epoch 1494 :: Batch 0/1
Batch Loss = 0.03466252091139759
1494, epoch_train_loss=0.03466252091139759
Epoch 1495
Epoch 1495 :: Batch 0/1
Batch Loss = 0.03462000452352485
1495, epoch_train_loss=0.03462000452352485
Epoch 1496
Epoch 1496 :: Batch 0/1
Batch Loss = 0.034578914845500235
1496, epoch_train_loss=0.034578914845500235
Epoch 1497
Epoch 1497 :: Batch 0/1
Batch Loss = 0.03453932405625618
1497, epoch_train_loss=0.03453932405625618
Epoch 1498
Epoch 1498 :: Batch 0/1
Batch Loss = 0.034501030252828684
1498, epoch_train_loss=0.034501030252828684
Epoch 1499
Epoch 1499 :: Batch 0/1
Batch Loss = 0.03446371397622994
1499, epoch_train_loss=0.03446371397622994
Epoch 1500
Epoch 1500 :: Batch 0/1
Batch Loss = 0.03442708226638911
1500, epoch_train_loss=0.03442708226638911
Epoch 1501
Epoch 1501 :: Batch 0/1
Batch Loss = 0.03439093291216047
1501, epoch_train_loss=0.03439093291216047
Epoch 1502
Epoch 1502 :: Batch 0/1
Batch Loss = 0.034355144518730715
1502, epoch_train_loss=0.034355144518730715
Epoch 1503
Epoch 1503 :: Batch 0/1
Batch Loss = 0.03431963950467525
1503, epoch_train_loss=0.03431963950467525
Epoch 1504
Epoch 1504 :: Batch 0/1
Batch Loss = 0.03428435803241308
1504, epoch_train_loss=0.03428435803241308
Epoch 1505
Epoch 1505 :: Batch 0/1
Batch Loss = 0.03424924970732012
1505, epoch_train_loss=0.03424924970732012
Epoch 1506
Epoch 1506 :: Batch 0/1
Batch Loss = 0.034214273391746826
1506, epoch_train_loss=0.034214273391746826
Epoch 1507
Epoch 1507 :: Batch 0/1
Batch Loss = 0.03417939827917611
1507, epoch_train_loss=0.03417939827917611
Epoch 1508
Epoch 1508 :: Batch 0/1
Batch Loss = 0.034144606325188585
1508, epoch_train_loss=0.034144606325188585
Epoch 1509
Epoch 1509 :: Batch 0/1
Batch Loss = 0.034109898833620846
1509, epoch_train_loss=0.034109898833620846
Epoch 1510
Epoch 1510 :: Batch 0/1
Batch Loss = 0.034075306598226676
1510, epoch_train_loss=0.034075306598226676
Epoch 1511
Epoch 1511 :: Batch 0/1
Batch Loss = 0.03404089748385808
1511, epoch_train_loss=0.03404089748385808
Epoch 1512
Epoch 1512 :: Batch 0/1
Batch Loss = 0.034006774122424206
1512, epoch_train_loss=0.034006774122424206
Epoch 1513
Epoch 1513 :: Batch 0/1
Batch Loss = 0.033973059448595286
1513, epoch_train_loss=0.033973059448595286
Epoch 1514
Epoch 1514 :: Batch 0/1
Batch Loss = 0.033939874808502433
1514, epoch_train_loss=0.033939874808502433
Epoch 1515
Epoch 1515 :: Batch 0/1
Batch Loss = 0.03390731943197082
1515, epoch_train_loss=0.03390731943197082
Epoch 1516
Epoch 1516 :: Batch 0/1
Batch Loss = 0.03387545896848896
1516, epoch_train_loss=0.03387545896848896
Epoch 1517
Epoch 1517 :: Batch 0/1
Batch Loss = 0.033844325538722214
1517, epoch_train_loss=0.033844325538722214
Epoch 1518
Epoch 1518 :: Batch 0/1
Batch Loss = 0.03381392545590324
1518, epoch_train_loss=0.03381392545590324
Epoch 1519
Epoch 1519 :: Batch 0/1
Batch Loss = 0.03378424761871614
1519, epoch_train_loss=0.03378424761871614
Epoch 1520
Epoch 1520 :: Batch 0/1
Batch Loss = 0.033755267453324855
1520, epoch_train_loss=0.033755267453324855
Epoch 1521
Epoch 1521 :: Batch 0/1
Batch Loss = 0.03372694648497012
1521, epoch_train_loss=0.03372694648497012
Epoch 1522
Epoch 1522 :: Batch 0/1
Batch Loss = 0.03369923180547961
1522, epoch_train_loss=0.03369923180547961
Epoch 1523
Epoch 1523 :: Batch 0/1
Batch Loss = 0.033672059538933596
1523, epoch_train_loss=0.033672059538933596
Epoch 1524
Epoch 1524 :: Batch 0/1
Batch Loss = 0.03364536266634593
1524, epoch_train_loss=0.03364536266634593
Epoch 1525
Epoch 1525 :: Batch 0/1
Batch Loss = 0.03361907976691077
1525, epoch_train_loss=0.03361907976691077
Epoch 1526
Epoch 1526 :: Batch 0/1
Batch Loss = 0.033593160283438846
1526, epoch_train_loss=0.033593160283438846
Epoch 1527
Epoch 1527 :: Batch 0/1
Batch Loss = 0.033567564396558364
1527, epoch_train_loss=0.033567564396558364
Epoch 1528
Epoch 1528 :: Batch 0/1
Batch Loss = 0.03354225909664951
1528, epoch_train_loss=0.03354225909664951
Epoch 1529
Epoch 1529 :: Batch 0/1
Batch Loss = 0.0335172138028379
1529, epoch_train_loss=0.0335172138028379
Epoch 1530
Epoch 1530 :: Batch 0/1
Batch Loss = 0.03349239813453719
1530, epoch_train_loss=0.03349239813453719
Epoch 1531
Epoch 1531 :: Batch 0/1
Batch Loss = 0.033467782217809314
1531, epoch_train_loss=0.033467782217809314
Epoch 1532
Epoch 1532 :: Batch 0/1
Batch Loss = 0.03344333814891202
1532, epoch_train_loss=0.03344333814891202
Epoch 1533
Epoch 1533 :: Batch 0/1
Batch Loss = 0.033419040886925946
1533, epoch_train_loss=0.033419040886925946
Epoch 1534
Epoch 1534 :: Batch 0/1
Batch Loss = 0.03339486777063617
1534, epoch_train_loss=0.03339486777063617
Epoch 1535
Epoch 1535 :: Batch 0/1
Batch Loss = 0.033370797199694324
1535, epoch_train_loss=0.033370797199694324
Epoch 1536
Epoch 1536 :: Batch 0/1
Batch Loss = 0.03334680759759707
1536, epoch_train_loss=0.03334680759759707
Epoch 1537
Epoch 1537 :: Batch 0/1
Batch Loss = 0.033322877511053034
1537, epoch_train_loss=0.033322877511053034
Epoch 1538
Epoch 1538 :: Batch 0/1
Batch Loss = 0.03329898679572996
1538, epoch_train_loss=0.03329898679572996
Epoch 1539
Epoch 1539 :: Batch 0/1
Batch Loss = 0.03327511807643531
1539, epoch_train_loss=0.03327511807643531
Epoch 1540
Epoch 1540 :: Batch 0/1
Batch Loss = 0.03325125758824814
1540, epoch_train_loss=0.03325125758824814
Epoch 1541
Epoch 1541 :: Batch 0/1
Batch Loss = 0.033227394934649074
1541, epoch_train_loss=0.033227394934649074
Epoch 1542
Epoch 1542 :: Batch 0/1
Batch Loss = 0.03320352195835948
1542, epoch_train_loss=0.03320352195835948
Epoch 1543
Epoch 1543 :: Batch 0/1
Batch Loss = 0.03317963131054797
1543, epoch_train_loss=0.03317963131054797
Epoch 1544
Epoch 1544 :: Batch 0/1
Batch Loss = 0.033155715246991915
1544, epoch_train_loss=0.033155715246991915
Epoch 1545
Epoch 1545 :: Batch 0/1
Batch Loss = 0.033131764902505745
1545, epoch_train_loss=0.033131764902505745
Epoch 1546
Epoch 1546 :: Batch 0/1
Batch Loss = 0.03310776998429229
1546, epoch_train_loss=0.03310776998429229
Epoch 1547
Epoch 1547 :: Batch 0/1
Batch Loss = 0.03308371860543259
1547, epoch_train_loss=0.03308371860543259
Epoch 1548
Epoch 1548 :: Batch 0/1
Batch Loss = 0.03305959702181731
1548, epoch_train_loss=0.03305959702181731
Epoch 1549
Epoch 1549 :: Batch 0/1
Batch Loss = 0.033035389221258
1549, epoch_train_loss=0.033035389221258
Epoch 1550
Epoch 1550 :: Batch 0/1
Batch Loss = 0.03301107646604222
1550, epoch_train_loss=0.03301107646604222
Epoch 1551
Epoch 1551 :: Batch 0/1
Batch Loss = 0.03298663699863536
1551, epoch_train_loss=0.03298663699863536
Epoch 1552
Epoch 1552 :: Batch 0/1
Batch Loss = 0.032962046034805216
1552, epoch_train_loss=0.032962046034805216
Epoch 1553
Epoch 1553 :: Batch 0/1
Batch Loss = 0.032937276073250636
1553, epoch_train_loss=0.032937276073250636
Epoch 1554
Epoch 1554 :: Batch 0/1
Batch Loss = 0.03291229737152557
1554, epoch_train_loss=0.03291229737152557
Epoch 1555
Epoch 1555 :: Batch 0/1
Batch Loss = 0.03288707847262463
1555, epoch_train_loss=0.03288707847262463
Epoch 1556
Epoch 1556 :: Batch 0/1
Batch Loss = 0.032861586726593936
1556, epoch_train_loss=0.032861586726593936
Epoch 1557
Epoch 1557 :: Batch 0/1
Batch Loss = 0.03283578884457363
1557, epoch_train_loss=0.03283578884457363
Epoch 1558
Epoch 1558 :: Batch 0/1
Batch Loss = 0.03280965158010988
1558, epoch_train_loss=0.03280965158010988
Epoch 1559
Epoch 1559 :: Batch 0/1
Batch Loss = 0.032783142707487105
1559, epoch_train_loss=0.032783142707487105
Epoch 1560
Epoch 1560 :: Batch 0/1
Batch Loss = 0.03275623236171515
1560, epoch_train_loss=0.03275623236171515
Epoch 1561
Epoch 1561 :: Batch 0/1
Batch Loss = 0.03272889479761046
1561, epoch_train_loss=0.03272889479761046
Epoch 1562
Epoch 1562 :: Batch 0/1
Batch Loss = 0.03270111051433569
1562, epoch_train_loss=0.03270111051433569
Epoch 1563
Epoch 1563 :: Batch 0/1
Batch Loss = 0.03267286868797999
1563, epoch_train_loss=0.03267286868797999
Epoch 1564
Epoch 1564 :: Batch 0/1
Batch Loss = 0.032644169758833504
1564, epoch_train_loss=0.032644169758833504
Epoch 1565
Epoch 1565 :: Batch 0/1
Batch Loss = 0.03261502800382901
1565, epoch_train_loss=0.03261502800382901
Epoch 1566
Epoch 1566 :: Batch 0/1
Batch Loss = 0.0325854737237741
1566, epoch_train_loss=0.0325854737237741
Epoch 1567
Epoch 1567 :: Batch 0/1
Batch Loss = 0.032555554615314015
1567, epoch_train_loss=0.032555554615314015
Epoch 1568
Epoch 1568 :: Batch 0/1
Batch Loss = 0.03252533577084476
1568, epoch_train_loss=0.03252533577084476
Epoch 1569
Epoch 1569 :: Batch 0/1
Batch Loss = 0.03249489784974468
1569, epoch_train_loss=0.03249489784974468
Epoch 1570
Epoch 1570 :: Batch 0/1
Batch Loss = 0.032464333286664614
1570, epoch_train_loss=0.032464333286664614
Epoch 1571
Epoch 1571 :: Batch 0/1
Batch Loss = 0.03243374084795356
1571, epoch_train_loss=0.03243374084795356
Epoch 1572
Epoch 1572 :: Batch 0/1
Batch Loss = 0.03240321930445347
1572, epoch_train_loss=0.03240321930445347
Epoch 1573
Epoch 1573 :: Batch 0/1
Batch Loss = 0.03237286122751446
1573, epoch_train_loss=0.03237286122751446
Epoch 1574
Epoch 1574 :: Batch 0/1
Batch Loss = 0.03234274777682914
1574, epoch_train_loss=0.03234274777682914
Epoch 1575
Epoch 1575 :: Batch 0/1
Batch Loss = 0.0323129449853043
1575, epoch_train_loss=0.0323129449853043
Epoch 1576
Epoch 1576 :: Batch 0/1
Batch Loss = 0.032283501689469814
1576, epoch_train_loss=0.032283501689469814
Epoch 1577
Epoch 1577 :: Batch 0/1
Batch Loss = 0.03225444905200127
1577, epoch_train_loss=0.03225444905200127
Epoch 1578
Epoch 1578 :: Batch 0/1
Batch Loss = 0.03222580159427517
1578, epoch_train_loss=0.03222580159427517
Epoch 1579
Epoch 1579 :: Batch 0/1
Batch Loss = 0.032197559625331953
1579, epoch_train_loss=0.032197559625331953
Epoch 1580
Epoch 1580 :: Batch 0/1
Batch Loss = 0.032169712706258555
1580, epoch_train_loss=0.032169712706258555
Epoch 1581
Epoch 1581 :: Batch 0/1
Batch Loss = 0.032142243566706585
1581, epoch_train_loss=0.032142243566706585
Epoch 1582
Epoch 1582 :: Batch 0/1
Batch Loss = 0.03211513170908042
1582, epoch_train_loss=0.03211513170908042
Epoch 1583
Epoch 1583 :: Batch 0/1
Batch Loss = 0.03208835604856544
1583, epoch_train_loss=0.03208835604856544
Epoch 1584
Epoch 1584 :: Batch 0/1
Batch Loss = 0.03206189637486947
1584, epoch_train_loss=0.03206189637486947
Epoch 1585
Epoch 1585 :: Batch 0/1
Batch Loss = 0.032035733795148034
1585, epoch_train_loss=0.032035733795148034
Epoch 1586
Epoch 1586 :: Batch 0/1
Batch Loss = 0.032009850609874164
1586, epoch_train_loss=0.032009850609874164
Epoch 1587
Epoch 1587 :: Batch 0/1
Batch Loss = 0.03198423004145621
1587, epoch_train_loss=0.03198423004145621
Epoch 1588
Epoch 1588 :: Batch 0/1
Batch Loss = 0.03195885607595215
1588, epoch_train_loss=0.03195885607595215
Epoch 1589
Epoch 1589 :: Batch 0/1
Batch Loss = 0.03193371342526619
1589, epoch_train_loss=0.03193371342526619
Epoch 1590
Epoch 1590 :: Batch 0/1
Batch Loss = 0.03190878752577714
1590, epoch_train_loss=0.03190878752577714
Epoch 1591
Epoch 1591 :: Batch 0/1
Batch Loss = 0.03188406451636689
1591, epoch_train_loss=0.03188406451636689
Epoch 1592
Epoch 1592 :: Batch 0/1
Batch Loss = 0.03185953116989736
1592, epoch_train_loss=0.03185953116989736
Epoch 1593
Epoch 1593 :: Batch 0/1
Batch Loss = 0.031835174838407476
1593, epoch_train_loss=0.031835174838407476
Epoch 1594
Epoch 1594 :: Batch 0/1
Batch Loss = 0.03181098344779007
1594, epoch_train_loss=0.03181098344779007
Epoch 1595
Epoch 1595 :: Batch 0/1
Batch Loss = 0.03178694556559976
1595, epoch_train_loss=0.03178694556559976
Epoch 1596
Epoch 1596 :: Batch 0/1
Batch Loss = 0.031763050482207934
1596, epoch_train_loss=0.031763050482207934
Epoch 1597
Epoch 1597 :: Batch 0/1
Batch Loss = 0.03173928826211768
1597, epoch_train_loss=0.03173928826211768
Epoch 1598
Epoch 1598 :: Batch 0/1
Batch Loss = 0.03171564973411088
1598, epoch_train_loss=0.03171564973411088
Epoch 1599
Epoch 1599 :: Batch 0/1
Batch Loss = 0.03169212643074704
1599, epoch_train_loss=0.03169212643074704
Epoch 1600
Epoch 1600 :: Batch 0/1
Batch Loss = 0.03166871051714302
1600, epoch_train_loss=0.03166871051714302
Epoch 1601
Epoch 1601 :: Batch 0/1
Batch Loss = 0.03164539475227705
1601, epoch_train_loss=0.03164539475227705
Epoch 1602
Epoch 1602 :: Batch 0/1
Batch Loss = 0.031622172505966925
1602, epoch_train_loss=0.031622172505966925
Epoch 1603
Epoch 1603 :: Batch 0/1
Batch Loss = 0.03159903782666457
1603, epoch_train_loss=0.03159903782666457
Epoch 1604
Epoch 1604 :: Batch 0/1
Batch Loss = 0.031575985535150286
1604, epoch_train_loss=0.031575985535150286
Epoch 1605
Epoch 1605 :: Batch 0/1
Batch Loss = 0.031553011314665154
1605, epoch_train_loss=0.031553011314665154
Epoch 1606
Epoch 1606 :: Batch 0/1
Batch Loss = 0.03153011177629579
1606, epoch_train_loss=0.03153011177629579
Epoch 1607
Epoch 1607 :: Batch 0/1
Batch Loss = 0.0315072844907399
1607, epoch_train_loss=0.0315072844907399
Epoch 1608
Epoch 1608 :: Batch 0/1
Batch Loss = 0.03148452798445167
1608, epoch_train_loss=0.03148452798445167
Epoch 1609
Epoch 1609 :: Batch 0/1
Batch Loss = 0.03146184171197792
1609, epoch_train_loss=0.03146184171197792
Epoch 1610
Epoch 1610 :: Batch 0/1
Batch Loss = 0.031439225988406046
1610, epoch_train_loss=0.031439225988406046
Epoch 1611
Epoch 1611 :: Batch 0/1
Batch Loss = 0.031416681901274085
1611, epoch_train_loss=0.031416681901274085
Epoch 1612
Epoch 1612 :: Batch 0/1
Batch Loss = 0.03139421119384836
1612, epoch_train_loss=0.03139421119384836
Epoch 1613
Epoch 1613 :: Batch 0/1
Batch Loss = 0.03137181613745975
1613, epoch_train_loss=0.03137181613745975
Epoch 1614
Epoch 1614 :: Batch 0/1
Batch Loss = 0.03134949938411515
1614, epoch_train_loss=0.03134949938411515
Epoch 1615
Epoch 1615 :: Batch 0/1
Batch Loss = 0.03132726383843369
1615, epoch_train_loss=0.03132726383843369
Epoch 1616
Epoch 1616 :: Batch 0/1
Batch Loss = 0.031305112532388234
1616, epoch_train_loss=0.031305112532388234
Epoch 1617
Epoch 1617 :: Batch 0/1
Batch Loss = 0.03128304852853518
1617, epoch_train_loss=0.03128304852853518
Epoch 1618
Epoch 1618 :: Batch 0/1
Batch Loss = 0.031261074844021026
1618, epoch_train_loss=0.031261074844021026
Epoch 1619
Epoch 1619 :: Batch 0/1
Batch Loss = 0.031239194393063094
1619, epoch_train_loss=0.031239194393063094
Epoch 1620
Epoch 1620 :: Batch 0/1
Batch Loss = 0.031217409951325843
1620, epoch_train_loss=0.031217409951325843
Epoch 1621
Epoch 1621 :: Batch 0/1
Batch Loss = 0.03119572413040866
1621, epoch_train_loss=0.03119572413040866
Epoch 1622
Epoch 1622 :: Batch 0/1
Batch Loss = 0.03117413935878382
1622, epoch_train_loss=0.03117413935878382
Epoch 1623
Epoch 1623 :: Batch 0/1
Batch Loss = 0.0311526578847988
1623, epoch_train_loss=0.0311526578847988
Epoch 1624
Epoch 1624 :: Batch 0/1
Batch Loss = 0.031131281773832068
1624, epoch_train_loss=0.031131281773832068
Epoch 1625
Epoch 1625 :: Batch 0/1
Batch Loss = 0.031110012924304327
1625, epoch_train_loss=0.031110012924304327
Epoch 1626
Epoch 1626 :: Batch 0/1
Batch Loss = 0.031088853071658606
1626, epoch_train_loss=0.031088853071658606
Epoch 1627
Epoch 1627 :: Batch 0/1
Batch Loss = 0.03106780380418123
1627, epoch_train_loss=0.03106780380418123
Epoch 1628
Epoch 1628 :: Batch 0/1
Batch Loss = 0.031046866561081833
1628, epoch_train_loss=0.031046866561081833
Epoch 1629
Epoch 1629 :: Batch 0/1
Batch Loss = 0.031026042637099693
1629, epoch_train_loss=0.031026042637099693
Epoch 1630
Epoch 1630 :: Batch 0/1
Batch Loss = 0.031005333175308965
1630, epoch_train_loss=0.031005333175308965
Epoch 1631
Epoch 1631 :: Batch 0/1
Batch Loss = 0.030984739159254034
1631, epoch_train_loss=0.030984739159254034
Epoch 1632
Epoch 1632 :: Batch 0/1
Batch Loss = 0.03096426139583018
1632, epoch_train_loss=0.03096426139583018
Epoch 1633
Epoch 1633 :: Batch 0/1
Batch Loss = 0.030943900506543724
1633, epoch_train_loss=0.030943900506543724
Epoch 1634
Epoch 1634 :: Batch 0/1
Batch Loss = 0.030923656911647664
1634, epoch_train_loss=0.030923656911647664
Epoch 1635
Epoch 1635 :: Batch 0/1
Batch Loss = 0.03090353081795674
1635, epoch_train_loss=0.03090353081795674
Epoch 1636
Epoch 1636 :: Batch 0/1
Batch Loss = 0.03088352220172988
1636, epoch_train_loss=0.03088352220172988
Epoch 1637
Epoch 1637 :: Batch 0/1
Batch Loss = 0.030863630803914025
1637, epoch_train_loss=0.030863630803914025
Epoch 1638
Epoch 1638 :: Batch 0/1
Batch Loss = 0.03084385612279955
1638, epoch_train_loss=0.03084385612279955
Epoch 1639
Epoch 1639 :: Batch 0/1
Batch Loss = 0.03082419741295522
1639, epoch_train_loss=0.03082419741295522
Epoch 1640
Epoch 1640 :: Batch 0/1
Batch Loss = 0.030804653688973442
1640, epoch_train_loss=0.030804653688973442
Epoch 1641
Epoch 1641 :: Batch 0/1
Batch Loss = 0.030785223735033902
1641, epoch_train_loss=0.030785223735033902
Epoch 1642
Epoch 1642 :: Batch 0/1
Batch Loss = 0.030765906111471127
1642, epoch_train_loss=0.030765906111471127
Epoch 1643
Epoch 1643 :: Batch 0/1
Batch Loss = 0.030746699171015846
1643, epoch_train_loss=0.030746699171015846
Epoch 1644
Epoch 1644 :: Batch 0/1
Batch Loss = 0.03072760107548181
1644, epoch_train_loss=0.03072760107548181
Epoch 1645
Epoch 1645 :: Batch 0/1
Batch Loss = 0.030708609805719284
1645, epoch_train_loss=0.030708609805719284
Epoch 1646
Epoch 1646 :: Batch 0/1
Batch Loss = 0.030689723170168003
1646, epoch_train_loss=0.030689723170168003
Epoch 1647
Epoch 1647 :: Batch 0/1
Batch Loss = 0.03067093881720254
1647, epoch_train_loss=0.03067093881720254
Epoch 1648
Epoch 1648 :: Batch 0/1
Batch Loss = 0.030652254244361205
1648, epoch_train_loss=0.030652254244361205
Epoch 1649
Epoch 1649 :: Batch 0/1
Batch Loss = 0.030633666796346465
1649, epoch_train_loss=0.030633666796346465
Epoch 1650
Epoch 1650 :: Batch 0/1
Batch Loss = 0.03061517366462124
1650, epoch_train_loss=0.03061517366462124
Epoch 1651
Epoch 1651 :: Batch 0/1
Batch Loss = 0.030596771878927037
1651, epoch_train_loss=0.030596771878927037
Epoch 1652
Epoch 1652 :: Batch 0/1
Batch Loss = 0.030578458300359372
1652, epoch_train_loss=0.030578458300359372
Epoch 1653
Epoch 1653 :: Batch 0/1
Batch Loss = 0.03056022959171359
1653, epoch_train_loss=0.03056022959171359
Epoch 1654
Epoch 1654 :: Batch 0/1
Batch Loss = 0.030542082200565344
1654, epoch_train_loss=0.030542082200565344
Epoch 1655
Epoch 1655 :: Batch 0/1
Batch Loss = 0.030524012316624865
1655, epoch_train_loss=0.030524012316624865
Epoch 1656
Epoch 1656 :: Batch 0/1
Batch Loss = 0.030506015823981093
1656, epoch_train_loss=0.030506015823981093
Epoch 1657
Epoch 1657 :: Batch 0/1
Batch Loss = 0.030488088233937966
1657, epoch_train_loss=0.030488088233937966
Epoch 1658
Epoch 1658 :: Batch 0/1
Batch Loss = 0.03047022460124805
1658, epoch_train_loss=0.03047022460124805
Epoch 1659
Epoch 1659 :: Batch 0/1
Batch Loss = 0.030452419401244123
1659, epoch_train_loss=0.030452419401244123
Epoch 1660
Epoch 1660 :: Batch 0/1
Batch Loss = 0.030434666369262897
1660, epoch_train_loss=0.030434666369262897
Epoch 1661
Epoch 1661 :: Batch 0/1
Batch Loss = 0.030416958285737334
1661, epoch_train_loss=0.030416958285737334
Epoch 1662
Epoch 1662 :: Batch 0/1
Batch Loss = 0.03039928666868815
1662, epoch_train_loss=0.03039928666868815
Epoch 1663
Epoch 1663 :: Batch 0/1
Batch Loss = 0.03038164135905613
1663, epoch_train_loss=0.03038164135905613
Epoch 1664
Epoch 1664 :: Batch 0/1
Batch Loss = 0.030364009923471894
1664, epoch_train_loss=0.030364009923471894
Epoch 1665
Epoch 1665 :: Batch 0/1
Batch Loss = 0.030346376805059916
1665, epoch_train_loss=0.030346376805059916
Epoch 1666
Epoch 1666 :: Batch 0/1
Batch Loss = 0.030328722089371176
1666, epoch_train_loss=0.030328722089371176
Epoch 1667
Epoch 1667 :: Batch 0/1
Batch Loss = 0.03031101966841825
1667, epoch_train_loss=0.03031101966841825
Epoch 1668
Epoch 1668 :: Batch 0/1
Batch Loss = 0.03029323445350908
1668, epoch_train_loss=0.03029323445350908
Epoch 1669
Epoch 1669 :: Batch 0/1
Batch Loss = 0.03027531806469819
1669, epoch_train_loss=0.03027531806469819
Epoch 1670
Epoch 1670 :: Batch 0/1
Batch Loss = 0.030257201962552834
1670, epoch_train_loss=0.030257201962552834
Epoch 1671
Epoch 1671 :: Batch 0/1
Batch Loss = 0.030238786248818043
1671, epoch_train_loss=0.030238786248818043
Epoch 1672
Epoch 1672 :: Batch 0/1
Batch Loss = 0.03021992086560349
1672, epoch_train_loss=0.03021992086560349
Epoch 1673
Epoch 1673 :: Batch 0/1
Batch Loss = 0.03020037316577905
1673, epoch_train_loss=0.03020037316577905
Epoch 1674
Epoch 1674 :: Batch 0/1
Batch Loss = 0.030179770602225868
1674, epoch_train_loss=0.030179770602225868
Epoch 1675
Epoch 1675 :: Batch 0/1
Batch Loss = 0.03015749773036349
1675, epoch_train_loss=0.03015749773036349
Epoch 1676
Epoch 1676 :: Batch 0/1
Batch Loss = 0.0301325119569128
1676, epoch_train_loss=0.0301325119569128
Epoch 1677
Epoch 1677 :: Batch 0/1
Batch Loss = 0.030103032260211514
1677, epoch_train_loss=0.030103032260211514
Epoch 1678
Epoch 1678 :: Batch 0/1
Batch Loss = 0.030066118030081967
1678, epoch_train_loss=0.030066118030081967
Epoch 1679
Epoch 1679 :: Batch 0/1
Batch Loss = 0.030017570141101787
1679, epoch_train_loss=0.030017570141101787
Epoch 1680
Epoch 1680 :: Batch 0/1
Batch Loss = 0.02995390080233216
1680, epoch_train_loss=0.02995390080233216
Epoch 1681
Epoch 1681 :: Batch 0/1
Batch Loss = 0.029878551359939217
1681, epoch_train_loss=0.029878551359939217
Epoch 1682
Epoch 1682 :: Batch 0/1
Batch Loss = 0.029803070888454478
1682, epoch_train_loss=0.029803070888454478
Epoch 1683
Epoch 1683 :: Batch 0/1
Batch Loss = 0.029725690475746615
1683, epoch_train_loss=0.029725690475746615
Epoch 1684
Epoch 1684 :: Batch 0/1
Batch Loss = 0.02963325973878528
1684, epoch_train_loss=0.02963325973878528
Epoch 1685
Epoch 1685 :: Batch 0/1
Batch Loss = 0.029542442640504363
1685, epoch_train_loss=0.029542442640504363
Epoch 1686
Epoch 1686 :: Batch 0/1
Batch Loss = 0.029474488392979555
1686, epoch_train_loss=0.029474488392979555
Epoch 1687
Epoch 1687 :: Batch 0/1
Batch Loss = 0.029415902204690307
1687, epoch_train_loss=0.029415902204690307
Epoch 1688
Epoch 1688 :: Batch 0/1
Batch Loss = 0.029357508169173308
1688, epoch_train_loss=0.029357508169173308
Epoch 1689
Epoch 1689 :: Batch 0/1
Batch Loss = 0.02931636027519106
1689, epoch_train_loss=0.02931636027519106
Epoch 1690
Epoch 1690 :: Batch 0/1
Batch Loss = 0.029292039650245422
1690, epoch_train_loss=0.029292039650245422
Epoch 1691
Epoch 1691 :: Batch 0/1
Batch Loss = 0.02926840102129514
1691, epoch_train_loss=0.02926840102129514
Epoch 1692
Epoch 1692 :: Batch 0/1
Batch Loss = 0.029248083876695884
1692, epoch_train_loss=0.029248083876695884
Epoch 1693
Epoch 1693 :: Batch 0/1
Batch Loss = 0.029236102522553636
1693, epoch_train_loss=0.029236102522553636
Epoch 1694
Epoch 1694 :: Batch 0/1
Batch Loss = 0.02922134415351493
1694, epoch_train_loss=0.02922134415351493
Epoch 1695
Epoch 1695 :: Batch 0/1
Batch Loss = 0.029199207444091286
1695, epoch_train_loss=0.029199207444091286
Epoch 1696
Epoch 1696 :: Batch 0/1
Batch Loss = 0.0291779823253653
1696, epoch_train_loss=0.0291779823253653
Epoch 1697
Epoch 1697 :: Batch 0/1
Batch Loss = 0.02915749555463059
1697, epoch_train_loss=0.02915749555463059
Epoch 1698
Epoch 1698 :: Batch 0/1
Batch Loss = 0.02913115406954865
1698, epoch_train_loss=0.02913115406954865
Epoch 1699
Epoch 1699 :: Batch 0/1
Batch Loss = 0.029101261002693737
1699, epoch_train_loss=0.029101261002693737
Epoch 1700
Epoch 1700 :: Batch 0/1
Batch Loss = 0.0290721141243666
1700, epoch_train_loss=0.0290721141243666
Epoch 1701
Epoch 1701 :: Batch 0/1
Batch Loss = 0.029040812226065898
1701, epoch_train_loss=0.029040812226065898
Epoch 1702
Epoch 1702 :: Batch 0/1
Batch Loss = 0.029006599428690532
1702, epoch_train_loss=0.029006599428690532
Epoch 1703
Epoch 1703 :: Batch 0/1
Batch Loss = 0.02897438863685451
1703, epoch_train_loss=0.02897438863685451
Epoch 1704
Epoch 1704 :: Batch 0/1
Batch Loss = 0.028944816260479195
1704, epoch_train_loss=0.028944816260479195
Epoch 1705
Epoch 1705 :: Batch 0/1
Batch Loss = 0.028914774826838907
1705, epoch_train_loss=0.028914774826838907
Epoch 1706
Epoch 1706 :: Batch 0/1
Batch Loss = 0.028885409975024968
1706, epoch_train_loss=0.028885409975024968
Epoch 1707
Epoch 1707 :: Batch 0/1
Batch Loss = 0.02885892394940717
1707, epoch_train_loss=0.02885892394940717
Epoch 1708
Epoch 1708 :: Batch 0/1
Batch Loss = 0.028833620443317366
1708, epoch_train_loss=0.028833620443317366
Epoch 1709
Epoch 1709 :: Batch 0/1
Batch Loss = 0.028808674487627296
1709, epoch_train_loss=0.028808674487627296
Epoch 1710
Epoch 1710 :: Batch 0/1
Batch Loss = 0.028785848950401448
1710, epoch_train_loss=0.028785848950401448
Epoch 1711
Epoch 1711 :: Batch 0/1
Batch Loss = 0.028764543513637855
1711, epoch_train_loss=0.028764543513637855
Epoch 1712
Epoch 1712 :: Batch 0/1
Batch Loss = 0.028742792687796642
1712, epoch_train_loss=0.028742792687796642
Epoch 1713
Epoch 1713 :: Batch 0/1
Batch Loss = 0.02872117582884882
1713, epoch_train_loss=0.02872117582884882
Epoch 1714
Epoch 1714 :: Batch 0/1
Batch Loss = 0.028700475033846005
1714, epoch_train_loss=0.028700475033846005
Epoch 1715
Epoch 1715 :: Batch 0/1
Batch Loss = 0.028679570311875527
1715, epoch_train_loss=0.028679570311875527
Epoch 1716
Epoch 1716 :: Batch 0/1
Batch Loss = 0.028658209795277348
1716, epoch_train_loss=0.028658209795277348
Epoch 1717
Epoch 1717 :: Batch 0/1
Batch Loss = 0.028637270045857696
1717, epoch_train_loss=0.028637270045857696
Epoch 1718
Epoch 1718 :: Batch 0/1
Batch Loss = 0.028616349839472373
1718, epoch_train_loss=0.028616349839472373
Epoch 1719
Epoch 1719 :: Batch 0/1
Batch Loss = 0.028594818234296344
1719, epoch_train_loss=0.028594818234296344
Epoch 1720
Epoch 1720 :: Batch 0/1
Batch Loss = 0.028573342146197105
1720, epoch_train_loss=0.028573342146197105
Epoch 1721
Epoch 1721 :: Batch 0/1
Batch Loss = 0.028552302801514703
1721, epoch_train_loss=0.028552302801514703
Epoch 1722
Epoch 1722 :: Batch 0/1
Batch Loss = 0.02853120972947403
1722, epoch_train_loss=0.02853120972947403
Epoch 1723
Epoch 1723 :: Batch 0/1
Batch Loss = 0.028510156988946392
1723, epoch_train_loss=0.028510156988946392
Epoch 1724
Epoch 1724 :: Batch 0/1
Batch Loss = 0.02848956887725601
1724, epoch_train_loss=0.02848956887725601
Epoch 1725
Epoch 1725 :: Batch 0/1
Batch Loss = 0.02846919058911317
1725, epoch_train_loss=0.02846919058911317
Epoch 1726
Epoch 1726 :: Batch 0/1
Batch Loss = 0.028448821058322218
1726, epoch_train_loss=0.028448821058322218
Epoch 1727
Epoch 1727 :: Batch 0/1
Batch Loss = 0.028428805885121967
1727, epoch_train_loss=0.028428805885121967
Epoch 1728
Epoch 1728 :: Batch 0/1
Batch Loss = 0.028409203669353428
1728, epoch_train_loss=0.028409203669353428
Epoch 1729
Epoch 1729 :: Batch 0/1
Batch Loss = 0.028389722397423784
1729, epoch_train_loss=0.028389722397423784
Epoch 1730
Epoch 1730 :: Batch 0/1
Batch Loss = 0.028370401146143646
1730, epoch_train_loss=0.028370401146143646
Epoch 1731
Epoch 1731 :: Batch 0/1
Batch Loss = 0.028351376223080798
1731, epoch_train_loss=0.028351376223080798
Epoch 1732
Epoch 1732 :: Batch 0/1
Batch Loss = 0.02833248011629379
1732, epoch_train_loss=0.02833248011629379
Epoch 1733
Epoch 1733 :: Batch 0/1
Batch Loss = 0.028313634268549628
1733, epoch_train_loss=0.028313634268549628
Epoch 1734
Epoch 1734 :: Batch 0/1
Batch Loss = 0.028294989476758535
1734, epoch_train_loss=0.028294989476758535
Epoch 1735
Epoch 1735 :: Batch 0/1
Batch Loss = 0.02827652105193098
1735, epoch_train_loss=0.02827652105193098
Epoch 1736
Epoch 1736 :: Batch 0/1
Batch Loss = 0.028258077366521146
1736, epoch_train_loss=0.028258077366521146
Epoch 1737
Epoch 1737 :: Batch 0/1
Batch Loss = 0.028239695044315494
1737, epoch_train_loss=0.028239695044315494
Epoch 1738
Epoch 1738 :: Batch 0/1
Batch Loss = 0.028221442793477484
1738, epoch_train_loss=0.028221442793477484
Epoch 1739
Epoch 1739 :: Batch 0/1
Batch Loss = 0.02820324864837606
1739, epoch_train_loss=0.02820324864837606
Epoch 1740
Epoch 1740 :: Batch 0/1
Batch Loss = 0.02818509608944276
1740, epoch_train_loss=0.02818509608944276
Epoch 1741
Epoch 1741 :: Batch 0/1
Batch Loss = 0.028167061050536575
1741, epoch_train_loss=0.028167061050536575
Epoch 1742
Epoch 1742 :: Batch 0/1
Batch Loss = 0.028149125172422573
1742, epoch_train_loss=0.028149125172422573
Epoch 1743
Epoch 1743 :: Batch 0/1
Batch Loss = 0.02813122959975199
1743, epoch_train_loss=0.02813122959975199
Epoch 1744
Epoch 1744 :: Batch 0/1
Batch Loss = 0.028113413418992903
1744, epoch_train_loss=0.028113413418992903
Epoch 1745
Epoch 1745 :: Batch 0/1
Batch Loss = 0.02809571529402295
1745, epoch_train_loss=0.02809571529402295
Epoch 1746
Epoch 1746 :: Batch 0/1
Batch Loss = 0.02807810048595547
1746, epoch_train_loss=0.02807810048595547
Epoch 1747
Epoch 1747 :: Batch 0/1
Batch Loss = 0.028060562910252045
1747, epoch_train_loss=0.028060562910252045
Epoch 1748
Epoch 1748 :: Batch 0/1
Batch Loss = 0.028043133362461413
1748, epoch_train_loss=0.028043133362461413
Epoch 1749
Epoch 1749 :: Batch 0/1
Batch Loss = 0.028025795420883123
1749, epoch_train_loss=0.028025795420883123
Epoch 1750
Epoch 1750 :: Batch 0/1
Batch Loss = 0.028008522620833365
1750, epoch_train_loss=0.028008522620833365
Epoch 1751
Epoch 1751 :: Batch 0/1
Batch Loss = 0.02799133448020427
1751, epoch_train_loss=0.02799133448020427
Epoch 1752
Epoch 1752 :: Batch 0/1
Batch Loss = 0.02797424006669887
1752, epoch_train_loss=0.02797424006669887
Epoch 1753
Epoch 1753 :: Batch 0/1
Batch Loss = 0.027957213446249067
1753, epoch_train_loss=0.027957213446249067
Epoch 1754
Epoch 1754 :: Batch 0/1
Batch Loss = 0.027940248309654084
1754, epoch_train_loss=0.027940248309654084
Epoch 1755
Epoch 1755 :: Batch 0/1
Batch Loss = 0.027923355869646706
1755, epoch_train_loss=0.027923355869646706
Epoch 1756
Epoch 1756 :: Batch 0/1
Batch Loss = 0.027906525590897346
1756, epoch_train_loss=0.027906525590897346
Epoch 1757
Epoch 1757 :: Batch 0/1
Batch Loss = 0.027889746490268912
1757, epoch_train_loss=0.027889746490268912
Epoch 1758
Epoch 1758 :: Batch 0/1
Batch Loss = 0.027873028426588658
1758, epoch_train_loss=0.027873028426588658
Epoch 1759
Epoch 1759 :: Batch 0/1
Batch Loss = 0.027856372818888128
1759, epoch_train_loss=0.027856372818888128
Epoch 1760
Epoch 1760 :: Batch 0/1
Batch Loss = 0.027839766967917536
1760, epoch_train_loss=0.027839766967917536
Epoch 1761
Epoch 1761 :: Batch 0/1
Batch Loss = 0.02782321102672943
1761, epoch_train_loss=0.02782321102672943
Epoch 1762
Epoch 1762 :: Batch 0/1
Batch Loss = 0.02780671233063351
1762, epoch_train_loss=0.02780671233063351
Epoch 1763
Epoch 1763 :: Batch 0/1
Batch Loss = 0.027790266669880995
1763, epoch_train_loss=0.027790266669880995
Epoch 1764
Epoch 1764 :: Batch 0/1
Batch Loss = 0.02777387006308854
1764, epoch_train_loss=0.02777387006308854
Epoch 1765
Epoch 1765 :: Batch 0/1
Batch Loss = 0.027757527047143946
1765, epoch_train_loss=0.027757527047143946
Epoch 1766
Epoch 1766 :: Batch 0/1
Batch Loss = 0.02774123669449329
1766, epoch_train_loss=0.02774123669449329
Epoch 1767
Epoch 1767 :: Batch 0/1
Batch Loss = 0.02772499266090334
1767, epoch_train_loss=0.02772499266090334
Epoch 1768
Epoch 1768 :: Batch 0/1
Batch Loss = 0.027708795519476175
1768, epoch_train_loss=0.027708795519476175
Epoch 1769
Epoch 1769 :: Batch 0/1
Batch Loss = 0.027692647669899734
1769, epoch_train_loss=0.027692647669899734
Epoch 1770
Epoch 1770 :: Batch 0/1
Batch Loss = 0.027676545108689953
1770, epoch_train_loss=0.027676545108689953
Epoch 1771
Epoch 1771 :: Batch 0/1
Batch Loss = 0.027660484491805165
1771, epoch_train_loss=0.027660484491805165
Epoch 1772
Epoch 1772 :: Batch 0/1
Batch Loss = 0.027644466556365245
1772, epoch_train_loss=0.027644466556365245
Epoch 1773
Epoch 1773 :: Batch 0/1
Batch Loss = 0.027628489429267145
1773, epoch_train_loss=0.027628489429267145
Epoch 1774
Epoch 1774 :: Batch 0/1
Batch Loss = 0.027612549450390017
1774, epoch_train_loss=0.027612549450390017
Epoch 1775
Epoch 1775 :: Batch 0/1
Batch Loss = 0.027596646411101426
1775, epoch_train_loss=0.027596646411101426
Epoch 1776
Epoch 1776 :: Batch 0/1
Batch Loss = 0.027580780272314084
1776, epoch_train_loss=0.027580780272314084
Epoch 1777
Epoch 1777 :: Batch 0/1
Batch Loss = 0.027564947986066193
1777, epoch_train_loss=0.027564947986066193
Epoch 1778
Epoch 1778 :: Batch 0/1
Batch Loss = 0.027549147501182064
1778, epoch_train_loss=0.027549147501182064
Epoch 1779
Epoch 1779 :: Batch 0/1
Batch Loss = 0.02753337884400225
1779, epoch_train_loss=0.02753337884400225
Epoch 1780
Epoch 1780 :: Batch 0/1
Batch Loss = 0.02751764065086741
1780, epoch_train_loss=0.02751764065086741
Epoch 1781
Epoch 1781 :: Batch 0/1
Batch Loss = 0.02750193078775865
1781, epoch_train_loss=0.02750193078775865
Epoch 1782
Epoch 1782 :: Batch 0/1
Batch Loss = 0.027486248617164602
1782, epoch_train_loss=0.027486248617164602
Epoch 1783
Epoch 1783 :: Batch 0/1
Batch Loss = 0.027470593269527204
1783, epoch_train_loss=0.027470593269527204
Epoch 1784
Epoch 1784 :: Batch 0/1
Batch Loss = 0.027454962563063734
1784, epoch_train_loss=0.027454962563063734
Epoch 1785
Epoch 1785 :: Batch 0/1
Batch Loss = 0.027439355042338565
1785, epoch_train_loss=0.027439355042338565
Epoch 1786
Epoch 1786 :: Batch 0/1
Batch Loss = 0.02742377007069453
1786, epoch_train_loss=0.02742377007069453
Epoch 1787
Epoch 1787 :: Batch 0/1
Batch Loss = 0.027408206092286745
1787, epoch_train_loss=0.027408206092286745
Epoch 1788
Epoch 1788 :: Batch 0/1
Batch Loss = 0.027392661194720183
1788, epoch_train_loss=0.027392661194720183
Epoch 1789
Epoch 1789 :: Batch 0/1
Batch Loss = 0.027377134199932485
1789, epoch_train_loss=0.027377134199932485
Epoch 1790
Epoch 1790 :: Batch 0/1
Batch Loss = 0.027361623782500878
1790, epoch_train_loss=0.027361623782500878
Epoch 1791
Epoch 1791 :: Batch 0/1
Batch Loss = 0.027346128098393624
1791, epoch_train_loss=0.027346128098393624
Epoch 1792
Epoch 1792 :: Batch 0/1
Batch Loss = 0.027330645718685678
1792, epoch_train_loss=0.027330645718685678
Epoch 1793
Epoch 1793 :: Batch 0/1
Batch Loss = 0.027315175488650967
1793, epoch_train_loss=0.027315175488650967
Epoch 1794
Epoch 1794 :: Batch 0/1
Batch Loss = 0.02729971576190475
1794, epoch_train_loss=0.02729971576190475
Epoch 1795
Epoch 1795 :: Batch 0/1
Batch Loss = 0.02728426483379493
1795, epoch_train_loss=0.02728426483379493
Epoch 1796
Epoch 1796 :: Batch 0/1
Batch Loss = 0.027268821412777436
1796, epoch_train_loss=0.027268821412777436
Epoch 1797
Epoch 1797 :: Batch 0/1
Batch Loss = 0.0272533841140667
1797, epoch_train_loss=0.0272533841140667
Epoch 1798
Epoch 1798 :: Batch 0/1
Batch Loss = 0.027237951314025836
1798, epoch_train_loss=0.027237951314025836
Epoch 1799
Epoch 1799 :: Batch 0/1
Batch Loss = 0.027222521585970175
1799, epoch_train_loss=0.027222521585970175
Epoch 1800
Epoch 1800 :: Batch 0/1
Batch Loss = 0.027207093584874012
1800, epoch_train_loss=0.027207093584874012
Epoch 1801
Epoch 1801 :: Batch 0/1
Batch Loss = 0.027191665740576798
1801, epoch_train_loss=0.027191665740576798
Epoch 1802
Epoch 1802 :: Batch 0/1
Batch Loss = 0.027176236518697442
1802, epoch_train_loss=0.027176236518697442
Epoch 1803
Epoch 1803 :: Batch 0/1
Batch Loss = 0.027160804582830288
1803, epoch_train_loss=0.027160804582830288
Epoch 1804
Epoch 1804 :: Batch 0/1
Batch Loss = 0.0271453685140377
1804, epoch_train_loss=0.0271453685140377
Epoch 1805
Epoch 1805 :: Batch 0/1
Batch Loss = 0.02712992678578934
1805, epoch_train_loss=0.02712992678578934
Epoch 1806
Epoch 1806 :: Batch 0/1
Batch Loss = 0.027114477989146877
1806, epoch_train_loss=0.027114477989146877
Epoch 1807
Epoch 1807 :: Batch 0/1
Batch Loss = 0.027099020764502765
1807, epoch_train_loss=0.027099020764502765
Epoch 1808
Epoch 1808 :: Batch 0/1
Batch Loss = 0.02708355367549029
1808, epoch_train_loss=0.02708355367549029
Epoch 1809
Epoch 1809 :: Batch 0/1
Batch Loss = 0.02706807533921851
1809, epoch_train_loss=0.02706807533921851
Epoch 1810
Epoch 1810 :: Batch 0/1
Batch Loss = 0.02705258447501059
1810, epoch_train_loss=0.02705258447501059
Epoch 1811
Epoch 1811 :: Batch 0/1
Batch Loss = 0.027037079772032513
1811, epoch_train_loss=0.027037079772032513
Epoch 1812
Epoch 1812 :: Batch 0/1
Batch Loss = 0.0270215599116378
1812, epoch_train_loss=0.0270215599116378
Epoch 1813
Epoch 1813 :: Batch 0/1
Batch Loss = 0.027006023679132487
1813, epoch_train_loss=0.027006023679132487
Epoch 1814
Epoch 1814 :: Batch 0/1
Batch Loss = 0.026990469914004483
1814, epoch_train_loss=0.026990469914004483
Epoch 1815
Epoch 1815 :: Batch 0/1
Batch Loss = 0.026974897454431882
1815, epoch_train_loss=0.026974897454431882
Epoch 1816
Epoch 1816 :: Batch 0/1
Batch Loss = 0.026959305202985992
1816, epoch_train_loss=0.026959305202985992
Epoch 1817
Epoch 1817 :: Batch 0/1
Batch Loss = 0.026943692147324467
1817, epoch_train_loss=0.026943692147324467
Epoch 1818
Epoch 1818 :: Batch 0/1
Batch Loss = 0.026928057307500544
1818, epoch_train_loss=0.026928057307500544
Epoch 1819
Epoch 1819 :: Batch 0/1
Batch Loss = 0.026912399758154373
1819, epoch_train_loss=0.026912399758154373
Epoch 1820
Epoch 1820 :: Batch 0/1
Batch Loss = 0.026896718673044377
1820, epoch_train_loss=0.026896718673044377
Epoch 1821
Epoch 1821 :: Batch 0/1
Batch Loss = 0.026881013301360904
1821, epoch_train_loss=0.026881013301360904
Epoch 1822
Epoch 1822 :: Batch 0/1
Batch Loss = 0.02686528294544433
1822, epoch_train_loss=0.02686528294544433
Epoch 1823
Epoch 1823 :: Batch 0/1
Batch Loss = 0.02684952700510146
1823, epoch_train_loss=0.02684952700510146
Epoch 1824
Epoch 1824 :: Batch 0/1
Batch Loss = 0.02683374498622242
1824, epoch_train_loss=0.02683374498622242
Epoch 1825
Epoch 1825 :: Batch 0/1
Batch Loss = 0.026817936476257083
1825, epoch_train_loss=0.026817936476257083
Epoch 1826
Epoch 1826 :: Batch 0/1
Batch Loss = 0.026802101166247806
1826, epoch_train_loss=0.026802101166247806
Epoch 1827
Epoch 1827 :: Batch 0/1
Batch Loss = 0.02678623886341655
1827, epoch_train_loss=0.02678623886341655
Epoch 1828
Epoch 1828 :: Batch 0/1
Batch Loss = 0.026770349485766703
1828, epoch_train_loss=0.026770349485766703
Epoch 1829
Epoch 1829 :: Batch 0/1
Batch Loss = 0.026754433056559182
1829, epoch_train_loss=0.026754433056559182
Epoch 1830
Epoch 1830 :: Batch 0/1
Batch Loss = 0.026738489728287882
1830, epoch_train_loss=0.026738489728287882
Epoch 1831
Epoch 1831 :: Batch 0/1
Batch Loss = 0.026722519787824372
1831, epoch_train_loss=0.026722519787824372
Epoch 1832
Epoch 1832 :: Batch 0/1
Batch Loss = 0.026706523641870095
1832, epoch_train_loss=0.026706523641870095
Epoch 1833
Epoch 1833 :: Batch 0/1
Batch Loss = 0.02669050182748204
1833, epoch_train_loss=0.02669050182748204
Epoch 1834
Epoch 1834 :: Batch 0/1
Batch Loss = 0.026674455025530116
1834, epoch_train_loss=0.026674455025530116
Epoch 1835
Epoch 1835 :: Batch 0/1
Batch Loss = 0.02665838404978987
1835, epoch_train_loss=0.02665838404978987
Epoch 1836
Epoch 1836 :: Batch 0/1
Batch Loss = 0.026642289847936797
1836, epoch_train_loss=0.026642289847936797
Epoch 1837
Epoch 1837 :: Batch 0/1
Batch Loss = 0.026626173513284234
1837, epoch_train_loss=0.026626173513284234
Epoch 1838
Epoch 1838 :: Batch 0/1
Batch Loss = 0.02661003628103168
1838, epoch_train_loss=0.02661003628103168
Epoch 1839
Epoch 1839 :: Batch 0/1
Batch Loss = 0.026593879517318727
1839, epoch_train_loss=0.026593879517318727
Epoch 1840
Epoch 1840 :: Batch 0/1
Batch Loss = 0.026577704727608873
1840, epoch_train_loss=0.026577704727608873
Epoch 1841
Epoch 1841 :: Batch 0/1
Batch Loss = 0.026561513552760527
1841, epoch_train_loss=0.026561513552760527
Epoch 1842
Epoch 1842 :: Batch 0/1
Batch Loss = 0.026545307760577324
1842, epoch_train_loss=0.026545307760577324
Epoch 1843
Epoch 1843 :: Batch 0/1
Batch Loss = 0.026529089245514472
1843, epoch_train_loss=0.026529089245514472
Epoch 1844
Epoch 1844 :: Batch 0/1
Batch Loss = 0.026512860021678547
1844, epoch_train_loss=0.026512860021678547
Epoch 1845
Epoch 1845 :: Batch 0/1
Batch Loss = 0.0264966222181467
1845, epoch_train_loss=0.0264966222181467
Epoch 1846
Epoch 1846 :: Batch 0/1
Batch Loss = 0.026480378068282455
1846, epoch_train_loss=0.026480378068282455
Epoch 1847
Epoch 1847 :: Batch 0/1
Batch Loss = 0.02646412990443328
1847, epoch_train_loss=0.02646412990443328
Epoch 1848
Epoch 1848 :: Batch 0/1
Batch Loss = 0.026447880150768672
1848, epoch_train_loss=0.026447880150768672
Epoch 1849
Epoch 1849 :: Batch 0/1
Batch Loss = 0.026431631307016954
1849, epoch_train_loss=0.026431631307016954
Epoch 1850
Epoch 1850 :: Batch 0/1
Batch Loss = 0.026415385943898844
1850, epoch_train_loss=0.026415385943898844
Epoch 1851
Epoch 1851 :: Batch 0/1
Batch Loss = 0.02639914668861659
1851, epoch_train_loss=0.02639914668861659
Epoch 1852
Epoch 1852 :: Batch 0/1
Batch Loss = 0.026382916216015004
1852, epoch_train_loss=0.026382916216015004
Epoch 1853
Epoch 1853 :: Batch 0/1
Batch Loss = 0.026366697234642033
1853, epoch_train_loss=0.026366697234642033
Epoch 1854
Epoch 1854 :: Batch 0/1
Batch Loss = 0.026350492475935367
1854, epoch_train_loss=0.026350492475935367
Epoch 1855
Epoch 1855 :: Batch 0/1
Batch Loss = 0.02633430468177389
1855, epoch_train_loss=0.02633430468177389
Epoch 1856
Epoch 1856 :: Batch 0/1
Batch Loss = 0.02631813659104071
1856, epoch_train_loss=0.02631813659104071
Epoch 1857
Epoch 1857 :: Batch 0/1
Batch Loss = 0.026301990928088777
1857, epoch_train_loss=0.026301990928088777
Epoch 1858
Epoch 1858 :: Batch 0/1
Batch Loss = 0.026285870391485056
1858, epoch_train_loss=0.026285870391485056
Epoch 1859
Epoch 1859 :: Batch 0/1
Batch Loss = 0.02626977764287849
1859, epoch_train_loss=0.02626977764287849
Epoch 1860
Epoch 1860 :: Batch 0/1
Batch Loss = 0.0262537152903797
1860, epoch_train_loss=0.0262537152903797
Epoch 1861
Epoch 1861 :: Batch 0/1
Batch Loss = 0.02623768588765253
1861, epoch_train_loss=0.02623768588765253
Epoch 1862
Epoch 1862 :: Batch 0/1
Batch Loss = 0.02622169191346428
1862, epoch_train_loss=0.02622169191346428
Epoch 1863
Epoch 1863 :: Batch 0/1
Batch Loss = 0.0262057357711543
1863, epoch_train_loss=0.0262057357711543
Epoch 1864
Epoch 1864 :: Batch 0/1
Batch Loss = 0.02618981977557139
1864, epoch_train_loss=0.02618981977557139
Epoch 1865
Epoch 1865 :: Batch 0/1
Batch Loss = 0.026173946147171716
1865, epoch_train_loss=0.026173946147171716
Epoch 1866
Epoch 1866 :: Batch 0/1
Batch Loss = 0.026158117005129197
1866, epoch_train_loss=0.026158117005129197
Epoch 1867
Epoch 1867 :: Batch 0/1
Batch Loss = 0.026142334361855604
1867, epoch_train_loss=0.026142334361855604
Epoch 1868
Epoch 1868 :: Batch 0/1
Batch Loss = 0.026126600118882282
1868, epoch_train_loss=0.026126600118882282
Epoch 1869
Epoch 1869 :: Batch 0/1
Batch Loss = 0.026110916062033283
1869, epoch_train_loss=0.026110916062033283
Epoch 1870
Epoch 1870 :: Batch 0/1
Batch Loss = 0.02609528386385645
1870, epoch_train_loss=0.02609528386385645
Epoch 1871
Epoch 1871 :: Batch 0/1
Batch Loss = 0.02607970507351447
1871, epoch_train_loss=0.02607970507351447
Epoch 1872
Epoch 1872 :: Batch 0/1
Batch Loss = 0.02606418112509529
1872, epoch_train_loss=0.02606418112509529
Epoch 1873
Epoch 1873 :: Batch 0/1
Batch Loss = 0.02604871333143923
1873, epoch_train_loss=0.02604871333143923
Epoch 1874
Epoch 1874 :: Batch 0/1
Batch Loss = 0.026033302890343744
1874, epoch_train_loss=0.026033302890343744
Epoch 1875
Epoch 1875 :: Batch 0/1
Batch Loss = 0.026017950882862337
1875, epoch_train_loss=0.026017950882862337
Epoch 1876
Epoch 1876 :: Batch 0/1
Batch Loss = 0.02600265827579643
1876, epoch_train_loss=0.02600265827579643
Epoch 1877
Epoch 1877 :: Batch 0/1
Batch Loss = 0.02598742592850579
1877, epoch_train_loss=0.02598742592850579
Epoch 1878
Epoch 1878 :: Batch 0/1
Batch Loss = 0.02597225459178433
1878, epoch_train_loss=0.02597225459178433
Epoch 1879
Epoch 1879 :: Batch 0/1
Batch Loss = 0.02595714491688149
1879, epoch_train_loss=0.02595714491688149
Epoch 1880
Epoch 1880 :: Batch 0/1
Batch Loss = 0.025942097456188983
1880, epoch_train_loss=0.025942097456188983
Epoch 1881
Epoch 1881 :: Batch 0/1
Batch Loss = 0.025927112668024773
1881, epoch_train_loss=0.025927112668024773
Epoch 1882
Epoch 1882 :: Batch 0/1
Batch Loss = 0.025912190925193083
1882, epoch_train_loss=0.025912190925193083
Epoch 1883
Epoch 1883 :: Batch 0/1
Batch Loss = 0.02589733251507329
1883, epoch_train_loss=0.02589733251507329
Epoch 1884
Epoch 1884 :: Batch 0/1
Batch Loss = 0.02588253764943807
1884, epoch_train_loss=0.02588253764943807
Epoch 1885
Epoch 1885 :: Batch 0/1
Batch Loss = 0.025867806464699987
1885, epoch_train_loss=0.025867806464699987
Epoch 1886
Epoch 1886 :: Batch 0/1
Batch Loss = 0.025853139030371022
1886, epoch_train_loss=0.025853139030371022
Epoch 1887
Epoch 1887 :: Batch 0/1
Batch Loss = 0.025838535353736647
1887, epoch_train_loss=0.025838535353736647
Epoch 1888
Epoch 1888 :: Batch 0/1
Batch Loss = 0.025823995382059043
1888, epoch_train_loss=0.025823995382059043
Epoch 1889
Epoch 1889 :: Batch 0/1
Batch Loss = 0.02580951900576255
1889, epoch_train_loss=0.02580951900576255
Epoch 1890
Epoch 1890 :: Batch 0/1
Batch Loss = 0.025795106069466896
1890, epoch_train_loss=0.025795106069466896
Epoch 1891
Epoch 1891 :: Batch 0/1
Batch Loss = 0.025780756368728258
1891, epoch_train_loss=0.025780756368728258
Epoch 1892
Epoch 1892 :: Batch 0/1
Batch Loss = 0.025766469653573276
1892, epoch_train_loss=0.025766469653573276
Epoch 1893
Epoch 1893 :: Batch 0/1
Batch Loss = 0.025752245638549522
1893, epoch_train_loss=0.025752245638549522
Epoch 1894
Epoch 1894 :: Batch 0/1
Batch Loss = 0.025738083997454905
1894, epoch_train_loss=0.025738083997454905
Epoch 1895
Epoch 1895 :: Batch 0/1
Batch Loss = 0.02572398437047568
1895, epoch_train_loss=0.02572398437047568
Epoch 1896
Epoch 1896 :: Batch 0/1
Batch Loss = 0.02570994636523658
1896, epoch_train_loss=0.02570994636523658
Epoch 1897
Epoch 1897 :: Batch 0/1
Batch Loss = 0.025695969559811217
1897, epoch_train_loss=0.025695969559811217
Epoch 1898
Epoch 1898 :: Batch 0/1
Batch Loss = 0.025682053500961362
1898, epoch_train_loss=0.025682053500961362
Epoch 1899
Epoch 1899 :: Batch 0/1
Batch Loss = 0.02566819770777816
1899, epoch_train_loss=0.02566819770777816
Epoch 1900
Epoch 1900 :: Batch 0/1
Batch Loss = 0.02565440167611424
1900, epoch_train_loss=0.02565440167611424
Epoch 1901
Epoch 1901 :: Batch 0/1
Batch Loss = 0.025640664872761596
1901, epoch_train_loss=0.025640664872761596
Epoch 1902
Epoch 1902 :: Batch 0/1
Batch Loss = 0.02562698674062821
1902, epoch_train_loss=0.02562698674062821
Epoch 1903
Epoch 1903 :: Batch 0/1
Batch Loss = 0.025613366698138712
1903, epoch_train_loss=0.025613366698138712
Epoch 1904
Epoch 1904 :: Batch 0/1
Batch Loss = 0.025599804139607704
1904, epoch_train_loss=0.025599804139607704
Epoch 1905
Epoch 1905 :: Batch 0/1
Batch Loss = 0.02558629843550811
1905, epoch_train_loss=0.02558629843550811
Epoch 1906
Epoch 1906 :: Batch 0/1
Batch Loss = 0.02557284893264855
1906, epoch_train_loss=0.02557284893264855
Epoch 1907
Epoch 1907 :: Batch 0/1
Batch Loss = 0.02555945495428135
1907, epoch_train_loss=0.02555945495428135
Epoch 1908
Epoch 1908 :: Batch 0/1
Batch Loss = 0.02554611580017762
1908, epoch_train_loss=0.02554611580017762
Epoch 1909
Epoch 1909 :: Batch 0/1
Batch Loss = 0.02553283074669358
1909, epoch_train_loss=0.02553283074669358
Epoch 1910
Epoch 1910 :: Batch 0/1
Batch Loss = 0.025519599046843186
1910, epoch_train_loss=0.025519599046843186
Epoch 1911
Epoch 1911 :: Batch 0/1
Batch Loss = 0.02550641992933769
1911, epoch_train_loss=0.02550641992933769
Epoch 1912
Epoch 1912 :: Batch 0/1
Batch Loss = 0.025493292601936682
1912, epoch_train_loss=0.025493292601936682
Epoch 1913
Epoch 1913 :: Batch 0/1
Batch Loss = 0.025480216248468992
1913, epoch_train_loss=0.025480216248468992
Epoch 1914
Epoch 1914 :: Batch 0/1
Batch Loss = 0.02546719003019779
1914, epoch_train_loss=0.02546719003019779
Epoch 1915
Epoch 1915 :: Batch 0/1
Batch Loss = 0.02545421308621214
1915, epoch_train_loss=0.02545421308621214
Epoch 1916
Epoch 1916 :: Batch 0/1
Batch Loss = 0.02544128453288246
1916, epoch_train_loss=0.02544128453288246
Epoch 1917
Epoch 1917 :: Batch 0/1
Batch Loss = 0.0254284034676016
1917, epoch_train_loss=0.0254284034676016
Epoch 1918
Epoch 1918 :: Batch 0/1
Batch Loss = 0.025415568965338104
1918, epoch_train_loss=0.025415568965338104
Epoch 1919
Epoch 1919 :: Batch 0/1
Batch Loss = 0.025402780083652456
1919, epoch_train_loss=0.025402780083652456
Epoch 1920
Epoch 1920 :: Batch 0/1
Batch Loss = 0.02539003586056108
1920, epoch_train_loss=0.02539003586056108
Epoch 1921
Epoch 1921 :: Batch 0/1
Batch Loss = 0.0253773353166864
1921, epoch_train_loss=0.0253773353166864
Epoch 1922
Epoch 1922 :: Batch 0/1
Batch Loss = 0.025364677455477304
1922, epoch_train_loss=0.025364677455477304
Epoch 1923
Epoch 1923 :: Batch 0/1
Batch Loss = 0.025352061266630595
1923, epoch_train_loss=0.025352061266630595
Epoch 1924
Epoch 1924 :: Batch 0/1
Batch Loss = 0.02533948572757343
1924, epoch_train_loss=0.02533948572757343
Epoch 1925
Epoch 1925 :: Batch 0/1
Batch Loss = 0.025326949802038666
1925, epoch_train_loss=0.025326949802038666
Epoch 1926
Epoch 1926 :: Batch 0/1
Batch Loss = 0.025314452445873478
1926, epoch_train_loss=0.025314452445873478
Epoch 1927
Epoch 1927 :: Batch 0/1
Batch Loss = 0.025301992605858525
1927, epoch_train_loss=0.025301992605858525
Epoch 1928
Epoch 1928 :: Batch 0/1
Batch Loss = 0.025289569221699414
1928, epoch_train_loss=0.025289569221699414
Epoch 1929
Epoch 1929 :: Batch 0/1
Batch Loss = 0.025277181231127453
1929, epoch_train_loss=0.025277181231127453
Epoch 1930
Epoch 1930 :: Batch 0/1
Batch Loss = 0.02526482756808166
1930, epoch_train_loss=0.02526482756808166
Epoch 1931
Epoch 1931 :: Batch 0/1
Batch Loss = 0.025252507168015297
1931, epoch_train_loss=0.025252507168015297
Epoch 1932
Epoch 1932 :: Batch 0/1
Batch Loss = 0.025240218969288478
1932, epoch_train_loss=0.025240218969288478
Epoch 1933
Epoch 1933 :: Batch 0/1
Batch Loss = 0.025227961916649465
1933, epoch_train_loss=0.025227961916649465
Epoch 1934
Epoch 1934 :: Batch 0/1
Batch Loss = 0.025215734960842548
1934, epoch_train_loss=0.025215734960842548
Epoch 1935
Epoch 1935 :: Batch 0/1
Batch Loss = 0.02520353706425025
1935, epoch_train_loss=0.02520353706425025
Epoch 1936
Epoch 1936 :: Batch 0/1
Batch Loss = 0.02519136720363134
1936, epoch_train_loss=0.02519136720363134
Epoch 1937
Epoch 1937 :: Batch 0/1
Batch Loss = 0.025179224369996808
1937, epoch_train_loss=0.025179224369996808
Epoch 1938
Epoch 1938 :: Batch 0/1
Batch Loss = 0.025167107574466597
1938, epoch_train_loss=0.025167107574466597
Epoch 1939
Epoch 1939 :: Batch 0/1
Batch Loss = 0.025155015850257027
1939, epoch_train_loss=0.025155015850257027
Epoch 1940
Epoch 1940 :: Batch 0/1
Batch Loss = 0.02514294825669445
1940, epoch_train_loss=0.02514294825669445
Epoch 1941
Epoch 1941 :: Batch 0/1
Batch Loss = 0.025130903879388405
1941, epoch_train_loss=0.025130903879388405
Epoch 1942
Epoch 1942 :: Batch 0/1
Batch Loss = 0.025118881836317733
1942, epoch_train_loss=0.025118881836317733
Epoch 1943
Epoch 1943 :: Batch 0/1
Batch Loss = 0.025106881280059126
1943, epoch_train_loss=0.025106881280059126
Epoch 1944
Epoch 1944 :: Batch 0/1
Batch Loss = 0.025094901401036953
1944, epoch_train_loss=0.025094901401036953
Epoch 1945
Epoch 1945 :: Batch 0/1
Batch Loss = 0.025082941430808737
1945, epoch_train_loss=0.025082941430808737
Epoch 1946
Epoch 1946 :: Batch 0/1
Batch Loss = 0.02507100064537654
1946, epoch_train_loss=0.02507100064537654
Epoch 1947
Epoch 1947 :: Batch 0/1
Batch Loss = 0.025059078368513975
1947, epoch_train_loss=0.025059078368513975
Epoch 1948
Epoch 1948 :: Batch 0/1
Batch Loss = 0.025047173975096558
1948, epoch_train_loss=0.025047173975096558
Epoch 1949
Epoch 1949 :: Batch 0/1
Batch Loss = 0.025035286894421856
1949, epoch_train_loss=0.025035286894421856
Epoch 1950
Epoch 1950 :: Batch 0/1
Batch Loss = 0.025023416613504913
1950, epoch_train_loss=0.025023416613504913
Epoch 1951
Epoch 1951 :: Batch 0/1
Batch Loss = 0.02501156268033152
1951, epoch_train_loss=0.02501156268033152
Epoch 1952
Epoch 1952 :: Batch 0/1
Batch Loss = 0.02499972470705022
1952, epoch_train_loss=0.02499972470705022
Epoch 1953
Epoch 1953 :: Batch 0/1
Batch Loss = 0.0249879023730819
1953, epoch_train_loss=0.0249879023730819
Epoch 1954
Epoch 1954 :: Batch 0/1
Batch Loss = 0.024976095428123665
1954, epoch_train_loss=0.024976095428123665
Epoch 1955
Epoch 1955 :: Batch 0/1
Batch Loss = 0.024964303694068515
1955, epoch_train_loss=0.024964303694068515
Epoch 1956
Epoch 1956 :: Batch 0/1
Batch Loss = 0.02495252707058204
1956, epoch_train_loss=0.02495252707058204
Epoch 1957
Epoch 1957 :: Batch 0/1
Batch Loss = 0.02494076553476807
1957, epoch_train_loss=0.02494076553476807
Epoch 1958
Epoch 1958 :: Batch 0/1
Batch Loss = 0.02492901914443636
1958, epoch_train_loss=0.02492901914443636
Epoch 1959
Epoch 1959 :: Batch 0/1
Batch Loss = 0.024917288039218853
1959, epoch_train_loss=0.024917288039218853
Epoch 1960
Epoch 1960 :: Batch 0/1
Batch Loss = 0.024905572445200554
1960, epoch_train_loss=0.024905572445200554
Epoch 1961
Epoch 1961 :: Batch 0/1
Batch Loss = 0.02489387267353965
1961, epoch_train_loss=0.02489387267353965
Epoch 1962
Epoch 1962 :: Batch 0/1
Batch Loss = 0.024882189121594796
1962, epoch_train_loss=0.024882189121594796
Epoch 1963
Epoch 1963 :: Batch 0/1
Batch Loss = 0.024870522276501394
1963, epoch_train_loss=0.024870522276501394
Epoch 1964
Epoch 1964 :: Batch 0/1
Batch Loss = 0.02485887271265957
1964, epoch_train_loss=0.02485887271265957
Epoch 1965
Epoch 1965 :: Batch 0/1
Batch Loss = 0.024847241092562124
1965, epoch_train_loss=0.024847241092562124
Epoch 1966
Epoch 1966 :: Batch 0/1
Batch Loss = 0.024835628165261688
1966, epoch_train_loss=0.024835628165261688
Epoch 1967
Epoch 1967 :: Batch 0/1
Batch Loss = 0.024824034768095016
1967, epoch_train_loss=0.024824034768095016
Epoch 1968
Epoch 1968 :: Batch 0/1
Batch Loss = 0.024812461821304203
1968, epoch_train_loss=0.024812461821304203
Epoch 1969
Epoch 1969 :: Batch 0/1
Batch Loss = 0.024800910328688648
1969, epoch_train_loss=0.024800910328688648
Epoch 1970
Epoch 1970 :: Batch 0/1
Batch Loss = 0.024789381374871702
1970, epoch_train_loss=0.024789381374871702
Epoch 1971
Epoch 1971 :: Batch 0/1
Batch Loss = 0.02477787611920573
1971, epoch_train_loss=0.02477787611920573
Epoch 1972
Epoch 1972 :: Batch 0/1
Batch Loss = 0.02476639579563561
1972, epoch_train_loss=0.02476639579563561
Epoch 1973
Epoch 1973 :: Batch 0/1
Batch Loss = 0.024754941705462805
1973, epoch_train_loss=0.024754941705462805
Epoch 1974
Epoch 1974 :: Batch 0/1
Batch Loss = 0.024743515212356394
1974, epoch_train_loss=0.024743515212356394
Epoch 1975
Epoch 1975 :: Batch 0/1
Batch Loss = 0.024732117738665505
1975, epoch_train_loss=0.024732117738665505
Epoch 1976
Epoch 1976 :: Batch 0/1
Batch Loss = 0.02472075075842914
1976, epoch_train_loss=0.02472075075842914
Epoch 1977
Epoch 1977 :: Batch 0/1
Batch Loss = 0.024709415790829678
1977, epoch_train_loss=0.024709415790829678
Epoch 1978
Epoch 1978 :: Batch 0/1
Batch Loss = 0.02469811439411434
1978, epoch_train_loss=0.02469811439411434
Epoch 1979
Epoch 1979 :: Batch 0/1
Batch Loss = 0.024686848156369137
1979, epoch_train_loss=0.024686848156369137
Epoch 1980
Epoch 1980 :: Batch 0/1
Batch Loss = 0.024675618687797683
1980, epoch_train_loss=0.024675618687797683
Epoch 1981
Epoch 1981 :: Batch 0/1
Batch Loss = 0.024664427614529548
1981, epoch_train_loss=0.024664427614529548
Epoch 1982
Epoch 1982 :: Batch 0/1
Batch Loss = 0.024653276569473453
1982, epoch_train_loss=0.024653276569473453
Epoch 1983
Epoch 1983 :: Batch 0/1
Batch Loss = 0.024642167183943606
1983, epoch_train_loss=0.024642167183943606
Epoch 1984
Epoch 1984 :: Batch 0/1
Batch Loss = 0.02463110108011458
1984, epoch_train_loss=0.02463110108011458
Epoch 1985
Epoch 1985 :: Batch 0/1
Batch Loss = 0.024620079859903672
1985, epoch_train_loss=0.024620079859903672
Epoch 1986
Epoch 1986 :: Batch 0/1
Batch Loss = 0.024609105099334276
1986, epoch_train_loss=0.024609105099334276
Epoch 1987
Epoch 1987 :: Batch 0/1
Batch Loss = 0.024598178339477018
1987, epoch_train_loss=0.024598178339477018
Epoch 1988
Epoch 1988 :: Batch 0/1
Batch Loss = 0.02458730107854685
1988, epoch_train_loss=0.02458730107854685
Epoch 1989
Epoch 1989 :: Batch 0/1
Batch Loss = 0.024576474764333042
1989, epoch_train_loss=0.024576474764333042
Epoch 1990
Epoch 1990 :: Batch 0/1
Batch Loss = 0.024565700787036515
1990, epoch_train_loss=0.024565700787036515
Epoch 1991
Epoch 1991 :: Batch 0/1
Batch Loss = 0.024554980472583445
1991, epoch_train_loss=0.024554980472583445
Epoch 1992
Epoch 1992 :: Batch 0/1
Batch Loss = 0.024544315076478283
1992, epoch_train_loss=0.024544315076478283
Epoch 1993
Epoch 1993 :: Batch 0/1
Batch Loss = 0.02453370577825188
1993, epoch_train_loss=0.02453370577825188
Epoch 1994
Epoch 1994 :: Batch 0/1
Batch Loss = 0.02452315367655233
1994, epoch_train_loss=0.02452315367655233
Epoch 1995
Epoch 1995 :: Batch 0/1
Batch Loss = 0.02451265978406883
1995, epoch_train_loss=0.02451265978406883
Epoch 1996
Epoch 1996 :: Batch 0/1
Batch Loss = 0.024502225027412165
1996, epoch_train_loss=0.024502225027412165
Epoch 1997
Epoch 1997 :: Batch 0/1
Batch Loss = 0.02449185023922729
1997, epoch_train_loss=0.02449185023922729
Epoch 1998
Epoch 1998 :: Batch 0/1
Batch Loss = 0.02448153615871608
1998, epoch_train_loss=0.02448153615871608
Epoch 1999
Epoch 1999 :: Batch 0/1
Batch Loss = 0.024471283431988993
1999, epoch_train_loss=0.024471283431988993
Epoch 2000
Epoch 2000 :: Batch 0/1
Batch Loss = 0.02446109260896927
2000, epoch_train_loss=0.02446109260896927
Epoch 2001
Epoch 2001 :: Batch 0/1
Batch Loss = 0.024450964144368332
2001, epoch_train_loss=0.024450964144368332
Epoch 2002
Epoch 2002 :: Batch 0/1
Batch Loss = 0.024440898397688312
2002, epoch_train_loss=0.024440898397688312
Epoch 2003
Epoch 2003 :: Batch 0/1
Batch Loss = 0.02443089563794632
2003, epoch_train_loss=0.02443089563794632
Epoch 2004
Epoch 2004 :: Batch 0/1
Batch Loss = 0.024420956040047005
2004, epoch_train_loss=0.024420956040047005
Epoch 2005
Epoch 2005 :: Batch 0/1
Batch Loss = 0.024411079692298325
2005, epoch_train_loss=0.024411079692298325
Epoch 2006
Epoch 2006 :: Batch 0/1
Batch Loss = 0.02440126659949846
2006, epoch_train_loss=0.02440126659949846
Epoch 2007
Epoch 2007 :: Batch 0/1
Batch Loss = 0.024391516681739053
2007, epoch_train_loss=0.024391516681739053
Epoch 2008
Epoch 2008 :: Batch 0/1
Batch Loss = 0.024381829783172996
2008, epoch_train_loss=0.024381829783172996
Epoch 2009
Epoch 2009 :: Batch 0/1
Batch Loss = 0.024372205675490068
2009, epoch_train_loss=0.024372205675490068
Epoch 2010
Epoch 2010 :: Batch 0/1
Batch Loss = 0.024362644060915137
2010, epoch_train_loss=0.024362644060915137
Epoch 2011
Epoch 2011 :: Batch 0/1
Batch Loss = 0.02435314457700707
2011, epoch_train_loss=0.02435314457700707
Epoch 2012
Epoch 2012 :: Batch 0/1
Batch Loss = 0.024343706803895696
2012, epoch_train_loss=0.024343706803895696
Epoch 2013
Epoch 2013 :: Batch 0/1
Batch Loss = 0.024334330266215124
2013, epoch_train_loss=0.024334330266215124
Epoch 2014
Epoch 2014 :: Batch 0/1
Batch Loss = 0.024325014441214
2014, epoch_train_loss=0.024325014441214
Epoch 2015
Epoch 2015 :: Batch 0/1
Batch Loss = 0.024315758760750022
2015, epoch_train_loss=0.024315758760750022
Epoch 2016
Epoch 2016 :: Batch 0/1
Batch Loss = 0.0243065626185407
2016, epoch_train_loss=0.0243065626185407
Epoch 2017
Epoch 2017 :: Batch 0/1
Batch Loss = 0.02429742537501117
2017, epoch_train_loss=0.02429742537501117
Epoch 2018
Epoch 2018 :: Batch 0/1
Batch Loss = 0.02428834635979631
2018, epoch_train_loss=0.02428834635979631
Epoch 2019
Epoch 2019 :: Batch 0/1
Batch Loss = 0.02427932487925792
2019, epoch_train_loss=0.02427932487925792
Epoch 2020
Epoch 2020 :: Batch 0/1
Batch Loss = 0.0242703602178902
2020, epoch_train_loss=0.0242703602178902
Epoch 2021
Epoch 2021 :: Batch 0/1
Batch Loss = 0.02426145164464934
2021, epoch_train_loss=0.02426145164464934
Epoch 2022
Epoch 2022 :: Batch 0/1
Batch Loss = 0.024252598416811738
2022, epoch_train_loss=0.024252598416811738
Epoch 2023
Epoch 2023 :: Batch 0/1
Batch Loss = 0.02424379978143702
2023, epoch_train_loss=0.02424379978143702
Epoch 2024
Epoch 2024 :: Batch 0/1
Batch Loss = 0.024235054981591564
2024, epoch_train_loss=0.024235054981591564
Epoch 2025
Epoch 2025 :: Batch 0/1
Batch Loss = 0.024226363255866766
2025, epoch_train_loss=0.024226363255866766
Epoch 2026
Epoch 2026 :: Batch 0/1
Batch Loss = 0.02421772384613453
2026, epoch_train_loss=0.02421772384613453
Epoch 2027
Epoch 2027 :: Batch 0/1
Batch Loss = 0.024209135995127112
2027, epoch_train_loss=0.024209135995127112
Epoch 2028
Epoch 2028 :: Batch 0/1
Batch Loss = 0.024200598951513556
2028, epoch_train_loss=0.024200598951513556
Epoch 2029
Epoch 2029 :: Batch 0/1
Batch Loss = 0.02419211197118376
2029, epoch_train_loss=0.02419211197118376
Epoch 2030
Epoch 2030 :: Batch 0/1
Batch Loss = 0.024183674318984053
2030, epoch_train_loss=0.024183674318984053
Epoch 2031
Epoch 2031 :: Batch 0/1
Batch Loss = 0.024175285270889017
2031, epoch_train_loss=0.024175285270889017
Epoch 2032
Epoch 2032 :: Batch 0/1
Batch Loss = 0.024166944112543033
2032, epoch_train_loss=0.024166944112543033
Epoch 2033
Epoch 2033 :: Batch 0/1
Batch Loss = 0.024158650145061362
2033, epoch_train_loss=0.024158650145061362
Epoch 2034
Epoch 2034 :: Batch 0/1
Batch Loss = 0.024150402680459294
2034, epoch_train_loss=0.024150402680459294
Epoch 2035
Epoch 2035 :: Batch 0/1
Batch Loss = 0.024142201047693806
2035, epoch_train_loss=0.024142201047693806
Epoch 2036
Epoch 2036 :: Batch 0/1
Batch Loss = 0.024134044587796662
2036, epoch_train_loss=0.024134044587796662
Epoch 2037
Epoch 2037 :: Batch 0/1
Batch Loss = 0.02412593265953173
2037, epoch_train_loss=0.02412593265953173
Epoch 2038
Epoch 2038 :: Batch 0/1
Batch Loss = 0.02411786463429374
2038, epoch_train_loss=0.02411786463429374
Epoch 2039
Epoch 2039 :: Batch 0/1
Batch Loss = 0.024109839900795446
2039, epoch_train_loss=0.024109839900795446
Epoch 2040
Epoch 2040 :: Batch 0/1
Batch Loss = 0.024101857862394924
2040, epoch_train_loss=0.024101857862394924
Epoch 2041
Epoch 2041 :: Batch 0/1
Batch Loss = 0.02409391793762324
2041, epoch_train_loss=0.02409391793762324
Epoch 2042
Epoch 2042 :: Batch 0/1
Batch Loss = 0.024086019559972426
2042, epoch_train_loss=0.024086019559972426
Epoch 2043
Epoch 2043 :: Batch 0/1
Batch Loss = 0.02407816217761564
2043, epoch_train_loss=0.02407816217761564
Epoch 2044
Epoch 2044 :: Batch 0/1
Batch Loss = 0.02407034525307147
2044, epoch_train_loss=0.02407034525307147
Epoch 2045
Epoch 2045 :: Batch 0/1
Batch Loss = 0.024062568262192812
2045, epoch_train_loss=0.024062568262192812
Epoch 2046
Epoch 2046 :: Batch 0/1
Batch Loss = 0.02405483069564739
2046, epoch_train_loss=0.02405483069564739
Epoch 2047
Epoch 2047 :: Batch 0/1
Batch Loss = 0.02404713205595047
2047, epoch_train_loss=0.02404713205595047
Epoch 2048
Epoch 2048 :: Batch 0/1
Batch Loss = 0.024039471860138908
2048, epoch_train_loss=0.024039471860138908
Epoch 2049
Epoch 2049 :: Batch 0/1
Batch Loss = 0.024031849634932056
2049, epoch_train_loss=0.024031849634932056
Epoch 2050
Epoch 2050 :: Batch 0/1
Batch Loss = 0.02402426492061735
2050, epoch_train_loss=0.02402426492061735
Epoch 2051
Epoch 2051 :: Batch 0/1
Batch Loss = 0.02401671726808604
2051, epoch_train_loss=0.02401671726808604
Epoch 2052
Epoch 2052 :: Batch 0/1
Batch Loss = 0.024009206238377646
2052, epoch_train_loss=0.024009206238377646
Epoch 2053
Epoch 2053 :: Batch 0/1
Batch Loss = 0.0240017314040516
2053, epoch_train_loss=0.0240017314040516
Epoch 2054
Epoch 2054 :: Batch 0/1
Batch Loss = 0.023994292346299465
2054, epoch_train_loss=0.023994292346299465
Epoch 2055
Epoch 2055 :: Batch 0/1
Batch Loss = 0.023986888656939764
2055, epoch_train_loss=0.023986888656939764
Epoch 2056
Epoch 2056 :: Batch 0/1
Batch Loss = 0.023979519936182365
2056, epoch_train_loss=0.023979519936182365
Epoch 2057
Epoch 2057 :: Batch 0/1
Batch Loss = 0.023972185792242817
2057, epoch_train_loss=0.023972185792242817
Epoch 2058
Epoch 2058 :: Batch 0/1
Batch Loss = 0.023964885842159767
2058, epoch_train_loss=0.023964885842159767
Epoch 2059
Epoch 2059 :: Batch 0/1
Batch Loss = 0.02395761971142232
2059, epoch_train_loss=0.02395761971142232
Epoch 2060
Epoch 2060 :: Batch 0/1
Batch Loss = 0.02395038703185719
2060, epoch_train_loss=0.02395038703185719
Epoch 2061
Epoch 2061 :: Batch 0/1
Batch Loss = 0.02394318744366937
2061, epoch_train_loss=0.02394318744366937
Epoch 2062
Epoch 2062 :: Batch 0/1
Batch Loss = 0.02393602059279098
2062, epoch_train_loss=0.02393602059279098
Epoch 2063
Epoch 2063 :: Batch 0/1
Batch Loss = 0.023928886132360015
2063, epoch_train_loss=0.023928886132360015
Epoch 2064
Epoch 2064 :: Batch 0/1
Batch Loss = 0.023921783722444894
2064, epoch_train_loss=0.023921783722444894
Epoch 2065
Epoch 2065 :: Batch 0/1
Batch Loss = 0.023914713028065512
2065, epoch_train_loss=0.023914713028065512
Epoch 2066
Epoch 2066 :: Batch 0/1
Batch Loss = 0.02390767372070112
2066, epoch_train_loss=0.02390767372070112
Epoch 2067
Epoch 2067 :: Batch 0/1
Batch Loss = 0.023900665478065526
2067, epoch_train_loss=0.023900665478065526
Epoch 2068
Epoch 2068 :: Batch 0/1
Batch Loss = 0.02389368798219857
2068, epoch_train_loss=0.02389368798219857
Epoch 2069
Epoch 2069 :: Batch 0/1
Batch Loss = 0.02388674092099607
2069, epoch_train_loss=0.02388674092099607
Epoch 2070
Epoch 2070 :: Batch 0/1
Batch Loss = 0.023879823987465975
2070, epoch_train_loss=0.023879823987465975
Epoch 2071
Epoch 2071 :: Batch 0/1
Batch Loss = 0.023872936880128395
2071, epoch_train_loss=0.023872936880128395
Epoch 2072
Epoch 2072 :: Batch 0/1
Batch Loss = 0.02386607930063043
2072, epoch_train_loss=0.02386607930063043
Epoch 2073
Epoch 2073 :: Batch 0/1
Batch Loss = 0.023859250957521602
2073, epoch_train_loss=0.023859250957521602
Epoch 2074
Epoch 2074 :: Batch 0/1
Batch Loss = 0.0238524515622193
2074, epoch_train_loss=0.0238524515622193
Epoch 2075
Epoch 2075 :: Batch 0/1
Batch Loss = 0.023845680831120784
2075, epoch_train_loss=0.023845680831120784
Epoch 2076
Epoch 2076 :: Batch 0/1
Batch Loss = 0.02383893848493453
2076, epoch_train_loss=0.02383893848493453
Epoch 2077
Epoch 2077 :: Batch 0/1
Batch Loss = 0.023832224248577137
2077, epoch_train_loss=0.023832224248577137
Epoch 2078
Epoch 2078 :: Batch 0/1
Batch Loss = 0.023825537851076935
2078, epoch_train_loss=0.023825537851076935
Epoch 2079
Epoch 2079 :: Batch 0/1
Batch Loss = 0.02381887902548402
2079, epoch_train_loss=0.02381887902548402
Epoch 2080
Epoch 2080 :: Batch 0/1
Batch Loss = 0.02381224750878591
2080, epoch_train_loss=0.02381224750878591
Epoch 2081
Epoch 2081 :: Batch 0/1
Batch Loss = 0.023805643041828604
2081, epoch_train_loss=0.023805643041828604
Epoch 2082
Epoch 2082 :: Batch 0/1
Batch Loss = 0.023799065369242235
2082, epoch_train_loss=0.023799065369242235
Epoch 2083
Epoch 2083 :: Batch 0/1
Batch Loss = 0.023792514239371426
2083, epoch_train_loss=0.023792514239371426
Epoch 2084
Epoch 2084 :: Batch 0/1
Batch Loss = 0.023785989403679455
2084, epoch_train_loss=0.023785989403679455
Epoch 2085
Epoch 2085 :: Batch 0/1
Batch Loss = 0.023779490618279497
2085, epoch_train_loss=0.023779490618279497
Epoch 2086
Epoch 2086 :: Batch 0/1
Batch Loss = 0.02377301764280515
2086, epoch_train_loss=0.02377301764280515
Epoch 2087
Epoch 2087 :: Batch 0/1
Batch Loss = 0.023766570239307356
2087, epoch_train_loss=0.023766570239307356
Epoch 2088
Epoch 2088 :: Batch 0/1
Batch Loss = 0.023760148173263947
2088, epoch_train_loss=0.023760148173263947
Epoch 2089
Epoch 2089 :: Batch 0/1
Batch Loss = 0.023753751214568873
2089, epoch_train_loss=0.023753751214568873
Epoch 2090
Epoch 2090 :: Batch 0/1
Batch Loss = 0.023747379135394996
2090, epoch_train_loss=0.023747379135394996
Epoch 2091
Epoch 2091 :: Batch 0/1
Batch Loss = 0.023741031712229646
2091, epoch_train_loss=0.023741031712229646
Epoch 2092
Epoch 2092 :: Batch 0/1
Batch Loss = 0.02373470872427148
2092, epoch_train_loss=0.02373470872427148
Epoch 2093
Epoch 2093 :: Batch 0/1
Batch Loss = 0.023728409953909273
2093, epoch_train_loss=0.023728409953909273
Epoch 2094
Epoch 2094 :: Batch 0/1
Batch Loss = 0.02372213518617176
2094, epoch_train_loss=0.02372213518617176
Epoch 2095
Epoch 2095 :: Batch 0/1
Batch Loss = 0.023715884210221434
2095, epoch_train_loss=0.023715884210221434
Epoch 2096
Epoch 2096 :: Batch 0/1
Batch Loss = 0.02370965681727494
2096, epoch_train_loss=0.02370965681727494
Epoch 2097
Epoch 2097 :: Batch 0/1
Batch Loss = 0.023703452802600798
2097, epoch_train_loss=0.023703452802600798
Epoch 2098
Epoch 2098 :: Batch 0/1
Batch Loss = 0.023697271963959515
2098, epoch_train_loss=0.023697271963959515
Epoch 2099
Epoch 2099 :: Batch 0/1
Batch Loss = 0.023691114102077967
2099, epoch_train_loss=0.023691114102077967
Epoch 2100
Epoch 2100 :: Batch 0/1
Batch Loss = 0.023684979020116733
2100, epoch_train_loss=0.023684979020116733
Epoch 2101
Epoch 2101 :: Batch 0/1
Batch Loss = 0.023678866524638877
2101, epoch_train_loss=0.023678866524638877
Epoch 2102
Epoch 2102 :: Batch 0/1
Batch Loss = 0.02367277642556871
2102, epoch_train_loss=0.02367277642556871
Epoch 2103
Epoch 2103 :: Batch 0/1
Batch Loss = 0.023666708534671177
2103, epoch_train_loss=0.023666708534671177
Epoch 2104
Epoch 2104 :: Batch 0/1
Batch Loss = 0.02366066266750212
2104, epoch_train_loss=0.02366066266750212
Epoch 2105
Epoch 2105 :: Batch 0/1
Batch Loss = 0.023654638641401025
2105, epoch_train_loss=0.023654638641401025
Epoch 2106
Epoch 2106 :: Batch 0/1
Batch Loss = 0.023648636277427325
2106, epoch_train_loss=0.023648636277427325
Epoch 2107
Epoch 2107 :: Batch 0/1
Batch Loss = 0.02364265539836734
2107, epoch_train_loss=0.02364265539836734
Epoch 2108
Epoch 2108 :: Batch 0/1
Batch Loss = 0.023636695830172045
2108, epoch_train_loss=0.023636695830172045
Epoch 2109
Epoch 2109 :: Batch 0/1
Batch Loss = 0.02363075740191712
2109, epoch_train_loss=0.02363075740191712
Epoch 2110
Epoch 2110 :: Batch 0/1
Batch Loss = 0.0236248399443193
2110, epoch_train_loss=0.0236248399443193
Epoch 2111
Epoch 2111 :: Batch 0/1
Batch Loss = 0.02361894329115812
2111, epoch_train_loss=0.02361894329115812
Epoch 2112
Epoch 2112 :: Batch 0/1
Batch Loss = 0.023613067278757525
2112, epoch_train_loss=0.023613067278757525
Epoch 2113
Epoch 2113 :: Batch 0/1
Batch Loss = 0.02360721174595279
2113, epoch_train_loss=0.02360721174595279
Epoch 2114
Epoch 2114 :: Batch 0/1
Batch Loss = 0.023601376534057084
2114, epoch_train_loss=0.023601376534057084
Epoch 2115
Epoch 2115 :: Batch 0/1
Batch Loss = 0.023595561487300576
2115, epoch_train_loss=0.023595561487300576
Epoch 2116
Epoch 2116 :: Batch 0/1
Batch Loss = 0.023589766451374516
2116, epoch_train_loss=0.023589766451374516
Epoch 2117
Epoch 2117 :: Batch 0/1
Batch Loss = 0.02358399127482063
2117, epoch_train_loss=0.02358399127482063
Epoch 2118
Epoch 2118 :: Batch 0/1
Batch Loss = 0.023578235808520313
2118, epoch_train_loss=0.023578235808520313
Epoch 2119
Epoch 2119 :: Batch 0/1
Batch Loss = 0.02357249990565894
2119, epoch_train_loss=0.02357249990565894
Epoch 2120
Epoch 2120 :: Batch 0/1
Batch Loss = 0.02356678342168951
2120, epoch_train_loss=0.02356678342168951
Epoch 2121
Epoch 2121 :: Batch 0/1
Batch Loss = 0.023561086214295972
2121, epoch_train_loss=0.023561086214295972
Epoch 2122
Epoch 2122 :: Batch 0/1
Batch Loss = 0.023555408143355997
2122, epoch_train_loss=0.023555408143355997
Epoch 2123
Epoch 2123 :: Batch 0/1
Batch Loss = 0.023549749070903153
2123, epoch_train_loss=0.023549749070903153
Epoch 2124
Epoch 2124 :: Batch 0/1
Batch Loss = 0.02354410886108879
2124, epoch_train_loss=0.02354410886108879
Epoch 2125
Epoch 2125 :: Batch 0/1
Batch Loss = 0.023538487380143264
2125, epoch_train_loss=0.023538487380143264
Epoch 2126
Epoch 2126 :: Batch 0/1
Batch Loss = 0.02353288449633689
2126, epoch_train_loss=0.02353288449633689
Epoch 2127
Epoch 2127 :: Batch 0/1
Batch Loss = 0.023527300079940326
2127, epoch_train_loss=0.023527300079940326
Epoch 2128
Epoch 2128 :: Batch 0/1
Batch Loss = 0.023521734003184653
2128, epoch_train_loss=0.023521734003184653
Epoch 2129
Epoch 2129 :: Batch 0/1
Batch Loss = 0.023516186140221048
2129, epoch_train_loss=0.023516186140221048
Epoch 2130
Epoch 2130 :: Batch 0/1
Batch Loss = 0.023510656367080114
2130, epoch_train_loss=0.023510656367080114
Epoch 2131
Epoch 2131 :: Batch 0/1
Batch Loss = 0.02350514456118224
2131, epoch_train_loss=0.02350514456118224
Epoch 2132
Epoch 2132 :: Batch 0/1
Batch Loss = 0.023499650602644785
2132, epoch_train_loss=0.023499650602644785
Epoch 2133
Epoch 2133 :: Batch 0/1
Batch Loss = 0.02349417437288984
2133, epoch_train_loss=0.02349417437288984
Epoch 2134
Epoch 2134 :: Batch 0/1
Batch Loss = 0.0234887157550536
2134, epoch_train_loss=0.0234887157550536
Epoch 2135
Epoch 2135 :: Batch 0/1
Batch Loss = 0.023483274633944576
2135, epoch_train_loss=0.023483274633944576
Epoch 2136
Epoch 2136 :: Batch 0/1
Batch Loss = 0.023477850896001615
2136, epoch_train_loss=0.023477850896001615
Epoch 2137
Epoch 2137 :: Batch 0/1
Batch Loss = 0.023472444428811766
2137, epoch_train_loss=0.023472444428811766
Epoch 2138
Epoch 2138 :: Batch 0/1
Batch Loss = 0.023467055122391157
2138, epoch_train_loss=0.023467055122391157
Epoch 2139
Epoch 2139 :: Batch 0/1
Batch Loss = 0.023461682867817888
2139, epoch_train_loss=0.023461682867817888
Epoch 2140
Epoch 2140 :: Batch 0/1
Batch Loss = 0.0234563275571965
2140, epoch_train_loss=0.0234563275571965
Epoch 2141
Epoch 2141 :: Batch 0/1
Batch Loss = 0.02345098908492669
2141, epoch_train_loss=0.02345098908492669
Epoch 2142
Epoch 2142 :: Batch 0/1
Batch Loss = 0.023445667345915595
2142, epoch_train_loss=0.023445667345915595
Epoch 2143
Epoch 2143 :: Batch 0/1
Batch Loss = 0.02344036223684502
2143, epoch_train_loss=0.02344036223684502
Epoch 2144
Epoch 2144 :: Batch 0/1
Batch Loss = 0.023435073656124142
2144, epoch_train_loss=0.023435073656124142
Epoch 2145
Epoch 2145 :: Batch 0/1
Batch Loss = 0.023429801502983648
2145, epoch_train_loss=0.023429801502983648
Epoch 2146
Epoch 2146 :: Batch 0/1
Batch Loss = 0.02342454567786954
2146, epoch_train_loss=0.02342454567786954
Epoch 2147
Epoch 2147 :: Batch 0/1
Batch Loss = 0.023419306081976128
2147, epoch_train_loss=0.023419306081976128
Epoch 2148
Epoch 2148 :: Batch 0/1
Batch Loss = 0.023414082618063613
2148, epoch_train_loss=0.023414082618063613
Epoch 2149
Epoch 2149 :: Batch 0/1
Batch Loss = 0.023408875189988604
2149, epoch_train_loss=0.023408875189988604
Epoch 2150
Epoch 2150 :: Batch 0/1
Batch Loss = 0.023403683703088533
2150, epoch_train_loss=0.023403683703088533
Epoch 2151
Epoch 2151 :: Batch 0/1
Batch Loss = 0.02339850806287259
2151, epoch_train_loss=0.02339850806287259
Epoch 2152
Epoch 2152 :: Batch 0/1
Batch Loss = 0.02339334817667779
2152, epoch_train_loss=0.02339334817667779
Epoch 2153
Epoch 2153 :: Batch 0/1
Batch Loss = 0.02338820395194382
2153, epoch_train_loss=0.02338820395194382
Epoch 2154
Epoch 2154 :: Batch 0/1
Batch Loss = 0.02338307529744239
2154, epoch_train_loss=0.02338307529744239
Epoch 2155
Epoch 2155 :: Batch 0/1
Batch Loss = 0.023377962122819416
2155, epoch_train_loss=0.023377962122819416
Epoch 2156
Epoch 2156 :: Batch 0/1
Batch Loss = 0.02337286433856044
2156, epoch_train_loss=0.02337286433856044
Epoch 2157
Epoch 2157 :: Batch 0/1
Batch Loss = 0.023367781856371124
2157, epoch_train_loss=0.023367781856371124
Epoch 2158
Epoch 2158 :: Batch 0/1
Batch Loss = 0.023362714588312548
2158, epoch_train_loss=0.023362714588312548
Epoch 2159
Epoch 2159 :: Batch 0/1
Batch Loss = 0.023357662446774913
2159, epoch_train_loss=0.023357662446774913
Epoch 2160
Epoch 2160 :: Batch 0/1
Batch Loss = 0.023352625344862505
2160, epoch_train_loss=0.023352625344862505
Epoch 2161
Epoch 2161 :: Batch 0/1
Batch Loss = 0.02334760319718217
2161, epoch_train_loss=0.02334760319718217
Epoch 2162
Epoch 2162 :: Batch 0/1
Batch Loss = 0.02334259591898655
2162, epoch_train_loss=0.02334259591898655
Epoch 2163
Epoch 2163 :: Batch 0/1
Batch Loss = 0.023337603425331827
2163, epoch_train_loss=0.023337603425331827
Epoch 2164
Epoch 2164 :: Batch 0/1
Batch Loss = 0.02333262563227869
2164, epoch_train_loss=0.02333262563227869
Epoch 2165
Epoch 2165 :: Batch 0/1
Batch Loss = 0.02332766245645598
2165, epoch_train_loss=0.02332766245645598
Epoch 2166
Epoch 2166 :: Batch 0/1
Batch Loss = 0.023322713815035305
2166, epoch_train_loss=0.023322713815035305
Epoch 2167
Epoch 2167 :: Batch 0/1
Batch Loss = 0.02331777962570687
2167, epoch_train_loss=0.02331777962570687
Epoch 2168
Epoch 2168 :: Batch 0/1
Batch Loss = 0.023312859806656143
2168, epoch_train_loss=0.023312859806656143
Epoch 2169
Epoch 2169 :: Batch 0/1
Batch Loss = 0.02330795427654174
2169, epoch_train_loss=0.02330795427654174
Epoch 2170
Epoch 2170 :: Batch 0/1
Batch Loss = 0.023303062954474426
2170, epoch_train_loss=0.023303062954474426
Epoch 2171
Epoch 2171 :: Batch 0/1
Batch Loss = 0.023298185759996936
2171, epoch_train_loss=0.023298185759996936
Epoch 2172
Epoch 2172 :: Batch 0/1
Batch Loss = 0.023293322613065198
2172, epoch_train_loss=0.023293322613065198
Epoch 2173
Epoch 2173 :: Batch 0/1
Batch Loss = 0.023288473433634913
2173, epoch_train_loss=0.023288473433634913
Epoch 2174
Epoch 2174 :: Batch 0/1
Batch Loss = 0.023283638143227505
2174, epoch_train_loss=0.023283638143227505
Epoch 2175
Epoch 2175 :: Batch 0/1
Batch Loss = 0.023278816662146113
2175, epoch_train_loss=0.023278816662146113
Epoch 2176
Epoch 2176 :: Batch 0/1
Batch Loss = 0.02327400891222664
2176, epoch_train_loss=0.02327400891222664
Epoch 2177
Epoch 2177 :: Batch 0/1
Batch Loss = 0.023269214815244608
2177, epoch_train_loss=0.023269214815244608
Epoch 2178
Epoch 2178 :: Batch 0/1
Batch Loss = 0.023264434293297782
2178, epoch_train_loss=0.023264434293297782
Epoch 2179
Epoch 2179 :: Batch 0/1
Batch Loss = 0.023259667268405827
2179, epoch_train_loss=0.023259667268405827
Epoch 2180
Epoch 2180 :: Batch 0/1
Batch Loss = 0.0232549136636684
2180, epoch_train_loss=0.0232549136636684
Epoch 2181
Epoch 2181 :: Batch 0/1
Batch Loss = 0.023250173402085537
2181, epoch_train_loss=0.023250173402085537
Epoch 2182
Epoch 2182 :: Batch 0/1
Batch Loss = 0.023245446406554383
2182, epoch_train_loss=0.023245446406554383
Epoch 2183
Epoch 2183 :: Batch 0/1
Batch Loss = 0.023240732601020435
2183, epoch_train_loss=0.023240732601020435
Epoch 2184
Epoch 2184 :: Batch 0/1
Batch Loss = 0.023236031909310853
2184, epoch_train_loss=0.023236031909310853
Epoch 2185
Epoch 2185 :: Batch 0/1
Batch Loss = 0.02323134425513375
2185, epoch_train_loss=0.02323134425513375
Epoch 2186
Epoch 2186 :: Batch 0/1
Batch Loss = 0.023226669563223105
2186, epoch_train_loss=0.023226669563223105
Epoch 2187
Epoch 2187 :: Batch 0/1
Batch Loss = 0.023222007757803684
2187, epoch_train_loss=0.023222007757803684
Epoch 2188
Epoch 2188 :: Batch 0/1
Batch Loss = 0.023217358764115636
2188, epoch_train_loss=0.023217358764115636
Epoch 2189
Epoch 2189 :: Batch 0/1
Batch Loss = 0.023212722507267945
2189, epoch_train_loss=0.023212722507267945
Epoch 2190
Epoch 2190 :: Batch 0/1
Batch Loss = 0.023208098912242188
2190, epoch_train_loss=0.023208098912242188
Epoch 2191
Epoch 2191 :: Batch 0/1
Batch Loss = 0.023203487904649742
2191, epoch_train_loss=0.023203487904649742
Epoch 2192
Epoch 2192 :: Batch 0/1
Batch Loss = 0.023198889410727796
2192, epoch_train_loss=0.023198889410727796
Epoch 2193
Epoch 2193 :: Batch 0/1
Batch Loss = 0.02319430335621286
2193, epoch_train_loss=0.02319430335621286
Epoch 2194
Epoch 2194 :: Batch 0/1
Batch Loss = 0.023189729667845677
2194, epoch_train_loss=0.023189729667845677
Epoch 2195
Epoch 2195 :: Batch 0/1
Batch Loss = 0.023185168271875167
2195, epoch_train_loss=0.023185168271875167
Epoch 2196
Epoch 2196 :: Batch 0/1
Batch Loss = 0.023180619095185304
2196, epoch_train_loss=0.023180619095185304
Epoch 2197
Epoch 2197 :: Batch 0/1
Batch Loss = 0.023176082065294604
2197, epoch_train_loss=0.023176082065294604
Epoch 2198
Epoch 2198 :: Batch 0/1
Batch Loss = 0.023171557109247922
2198, epoch_train_loss=0.023171557109247922
Epoch 2199
Epoch 2199 :: Batch 0/1
Batch Loss = 0.02316704415473618
2199, epoch_train_loss=0.02316704415473618
Epoch 2200
Epoch 2200 :: Batch 0/1
Batch Loss = 0.02316254312973015
2200, epoch_train_loss=0.02316254312973015
Epoch 2201
Epoch 2201 :: Batch 0/1
Batch Loss = 0.023158053962853333
2201, epoch_train_loss=0.023158053962853333
Epoch 2202
Epoch 2202 :: Batch 0/1
Batch Loss = 0.023153576582287436
2202, epoch_train_loss=0.023153576582287436
Epoch 2203
Epoch 2203 :: Batch 0/1
Batch Loss = 0.02314911091688247
2203, epoch_train_loss=0.02314911091688247
Epoch 2204
Epoch 2204 :: Batch 0/1
Batch Loss = 0.023144656895796232
2204, epoch_train_loss=0.023144656895796232
Epoch 2205
Epoch 2205 :: Batch 0/1
Batch Loss = 0.0231402144485024
2205, epoch_train_loss=0.0231402144485024
Epoch 2206
Epoch 2206 :: Batch 0/1
Batch Loss = 0.023135783505160778
2206, epoch_train_loss=0.023135783505160778
Epoch 2207
Epoch 2207 :: Batch 0/1
Batch Loss = 0.023131363995177748
2207, epoch_train_loss=0.023131363995177748
Epoch 2208
Epoch 2208 :: Batch 0/1
Batch Loss = 0.023126955849748634
2208, epoch_train_loss=0.023126955849748634
Epoch 2209
Epoch 2209 :: Batch 0/1
Batch Loss = 0.023122558998976625
2209, epoch_train_loss=0.023122558998976625
Epoch 2210
Epoch 2210 :: Batch 0/1
Batch Loss = 0.023118173374765294
2210, epoch_train_loss=0.023118173374765294
Epoch 2211
Epoch 2211 :: Batch 0/1
Batch Loss = 0.02311379890795251
2211, epoch_train_loss=0.02311379890795251
Epoch 2212
Epoch 2212 :: Batch 0/1
Batch Loss = 0.023109435531188893
2212, epoch_train_loss=0.023109435531188893
Epoch 2213
Epoch 2213 :: Batch 0/1
Batch Loss = 0.023105083176086408
2213, epoch_train_loss=0.023105083176086408
Epoch 2214
Epoch 2214 :: Batch 0/1
Batch Loss = 0.023100741776082793
2214, epoch_train_loss=0.023100741776082793
Epoch 2215
Epoch 2215 :: Batch 0/1
Batch Loss = 0.023096411263604618
2215, epoch_train_loss=0.023096411263604618
Epoch 2216
Epoch 2216 :: Batch 0/1
Batch Loss = 0.02309209157256466
2216, epoch_train_loss=0.02309209157256466
Epoch 2217
Epoch 2217 :: Batch 0/1
Batch Loss = 0.02308778263695174
2217, epoch_train_loss=0.02308778263695174
Epoch 2218
Epoch 2218 :: Batch 0/1
Batch Loss = 0.023083484391196226
2218, epoch_train_loss=0.023083484391196226
Epoch 2219
Epoch 2219 :: Batch 0/1
Batch Loss = 0.023079196770180496
2219, epoch_train_loss=0.023079196770180496
Epoch 2220
Epoch 2220 :: Batch 0/1
Batch Loss = 0.023074919709249114
2220, epoch_train_loss=0.023074919709249114
Epoch 2221
Epoch 2221 :: Batch 0/1
Batch Loss = 0.023070653144219196
2221, epoch_train_loss=0.023070653144219196
Epoch 2222
Epoch 2222 :: Batch 0/1
Batch Loss = 0.02306639701104263
2222, epoch_train_loss=0.02306639701104263
Epoch 2223
Epoch 2223 :: Batch 0/1
Batch Loss = 0.023062151246861208
2223, epoch_train_loss=0.023062151246861208
Epoch 2224
Epoch 2224 :: Batch 0/1
Batch Loss = 0.02305791578862394
2224, epoch_train_loss=0.02305791578862394
Epoch 2225
Epoch 2225 :: Batch 0/1
Batch Loss = 0.023053690574486186
2225, epoch_train_loss=0.023053690574486186
Epoch 2226
Epoch 2226 :: Batch 0/1
Batch Loss = 0.023049475542777875
2226, epoch_train_loss=0.023049475542777875
Epoch 2227
Epoch 2227 :: Batch 0/1
Batch Loss = 0.02304527063236056
2227, epoch_train_loss=0.02304527063236056
Epoch 2228
Epoch 2228 :: Batch 0/1
Batch Loss = 0.02304107578263671
2228, epoch_train_loss=0.02304107578263671
Epoch 2229
Epoch 2229 :: Batch 0/1
Batch Loss = 0.0230368909332166
2229, epoch_train_loss=0.0230368909332166
Epoch 2230
Epoch 2230 :: Batch 0/1
Batch Loss = 0.023032716024955005
2230, epoch_train_loss=0.023032716024955005
Epoch 2231
Epoch 2231 :: Batch 0/1
Batch Loss = 0.023028550998590105
2231, epoch_train_loss=0.023028550998590105
Epoch 2232
Epoch 2232 :: Batch 0/1
Batch Loss = 0.023024395795778488
2232, epoch_train_loss=0.023024395795778488
Epoch 2233
Epoch 2233 :: Batch 0/1
Batch Loss = 0.023020250359099645
2233, epoch_train_loss=0.023020250359099645
Epoch 2234
Epoch 2234 :: Batch 0/1
Batch Loss = 0.023016114631045404
2234, epoch_train_loss=0.023016114631045404
Epoch 2235
Epoch 2235 :: Batch 0/1
Batch Loss = 0.023011988555384475
2235, epoch_train_loss=0.023011988555384475
Epoch 2236
Epoch 2236 :: Batch 0/1
Batch Loss = 0.023007872076152416
2236, epoch_train_loss=0.023007872076152416
Epoch 2237
Epoch 2237 :: Batch 0/1
Batch Loss = 0.02300376513766208
2237, epoch_train_loss=0.02300376513766208
Epoch 2238
Epoch 2238 :: Batch 0/1
Batch Loss = 0.022999667685184088
2238, epoch_train_loss=0.022999667685184088
Epoch 2239
Epoch 2239 :: Batch 0/1
Batch Loss = 0.02299557966494944
2239, epoch_train_loss=0.02299557966494944
Epoch 2240
Epoch 2240 :: Batch 0/1
Batch Loss = 0.02299150102315104
2240, epoch_train_loss=0.02299150102315104
Epoch 2241
Epoch 2241 :: Batch 0/1
Batch Loss = 0.022987431706954845
2241, epoch_train_loss=0.022987431706954845
Epoch 2242
Epoch 2242 :: Batch 0/1
Batch Loss = 0.022983371664501637
2242, epoch_train_loss=0.022983371664501637
Epoch 2243
Epoch 2243 :: Batch 0/1
Batch Loss = 0.02297932084391412
2243, epoch_train_loss=0.02297932084391412
Epoch 2244
Epoch 2244 :: Batch 0/1
Batch Loss = 0.022975279194300268
2244, epoch_train_loss=0.022975279194300268
Epoch 2245
Epoch 2245 :: Batch 0/1
Batch Loss = 0.0229712466657541
2245, epoch_train_loss=0.0229712466657541
Epoch 2246
Epoch 2246 :: Batch 0/1
Batch Loss = 0.022967223208039025
2246, epoch_train_loss=0.022967223208039025
Epoch 2247
Epoch 2247 :: Batch 0/1
Batch Loss = 0.02296320877290089
2247, epoch_train_loss=0.02296320877290089
Epoch 2248
Epoch 2248 :: Batch 0/1
Batch Loss = 0.022959203311763036
2248, epoch_train_loss=0.022959203311763036
Epoch 2249
Epoch 2249 :: Batch 0/1
Batch Loss = 0.02295520677704716
2249, epoch_train_loss=0.02295520677704716
Epoch 2250
Epoch 2250 :: Batch 0/1
Batch Loss = 0.02295121912184594
2250, epoch_train_loss=0.02295121912184594
Epoch 2251
Epoch 2251 :: Batch 0/1
Batch Loss = 0.022947240299924787
2251, epoch_train_loss=0.022947240299924787
Epoch 2252
Epoch 2252 :: Batch 0/1
Batch Loss = 0.02294327026604835
2252, epoch_train_loss=0.02294327026604835
Epoch 2253
Epoch 2253 :: Batch 0/1
Batch Loss = 0.022939308974681293
2253, epoch_train_loss=0.022939308974681293
Epoch 2254
Epoch 2254 :: Batch 0/1
Batch Loss = 0.02293535638194013
2254, epoch_train_loss=0.02293535638194013
Epoch 2255
Epoch 2255 :: Batch 0/1
Batch Loss = 0.02293141244461524
2255, epoch_train_loss=0.02293141244461524
Epoch 2256
Epoch 2256 :: Batch 0/1
Batch Loss = 0.022927477119526554
2256, epoch_train_loss=0.022927477119526554
Epoch 2257
Epoch 2257 :: Batch 0/1
Batch Loss = 0.022923550364172565
2257, epoch_train_loss=0.022923550364172565
Epoch 2258
Epoch 2258 :: Batch 0/1
Batch Loss = 0.022919632137371113
2258, epoch_train_loss=0.022919632137371113
Epoch 2259
Epoch 2259 :: Batch 0/1
Batch Loss = 0.022915722398291635
2259, epoch_train_loss=0.022915722398291635
Epoch 2260
Epoch 2260 :: Batch 0/1
Batch Loss = 0.022911821107095737
2260, epoch_train_loss=0.022911821107095737
Epoch 2261
Epoch 2261 :: Batch 0/1
Batch Loss = 0.0229079282236572
2261, epoch_train_loss=0.0229079282236572
Epoch 2262
Epoch 2262 :: Batch 0/1
Batch Loss = 0.022904043709477715
2262, epoch_train_loss=0.022904043709477715
Epoch 2263
Epoch 2263 :: Batch 0/1
Batch Loss = 0.022900167526405474
2263, epoch_train_loss=0.022900167526405474
Epoch 2264
Epoch 2264 :: Batch 0/1
Batch Loss = 0.02289629963695213
2264, epoch_train_loss=0.02289629963695213
Epoch 2265
Epoch 2265 :: Batch 0/1
Batch Loss = 0.02289244000428968
2265, epoch_train_loss=0.02289244000428968
Epoch 2266
Epoch 2266 :: Batch 0/1
Batch Loss = 0.022888588591931623
2266, epoch_train_loss=0.022888588591931623
Epoch 2267
Epoch 2267 :: Batch 0/1
Batch Loss = 0.022884745364676534
2267, epoch_train_loss=0.022884745364676534
Epoch 2268
Epoch 2268 :: Batch 0/1
Batch Loss = 0.02288091028765557
2268, epoch_train_loss=0.02288091028765557
Epoch 2269
Epoch 2269 :: Batch 0/1
Batch Loss = 0.022877083326331135
2269, epoch_train_loss=0.022877083326331135
Epoch 2270
Epoch 2270 :: Batch 0/1
Batch Loss = 0.02287326444743372
2270, epoch_train_loss=0.02287326444743372
Epoch 2271
Epoch 2271 :: Batch 0/1
Batch Loss = 0.022869453618014395
2271, epoch_train_loss=0.022869453618014395
Epoch 2272
Epoch 2272 :: Batch 0/1
Batch Loss = 0.02286565080544267
2272, epoch_train_loss=0.02286565080544267
Epoch 2273
Epoch 2273 :: Batch 0/1
Batch Loss = 0.02286185597833664
2273, epoch_train_loss=0.02286185597833664
Epoch 2274
Epoch 2274 :: Batch 0/1
Batch Loss = 0.022858069105310307
2274, epoch_train_loss=0.022858069105310307
Epoch 2275
Epoch 2275 :: Batch 0/1
Batch Loss = 0.02285429015621202
2275, epoch_train_loss=0.02285429015621202
Epoch 2276
Epoch 2276 :: Batch 0/1
Batch Loss = 0.02285051910118538
2276, epoch_train_loss=0.02285051910118538
Epoch 2277
Epoch 2277 :: Batch 0/1
Batch Loss = 0.022846755910665744
2277, epoch_train_loss=0.022846755910665744
Epoch 2278
Epoch 2278 :: Batch 0/1
Batch Loss = 0.02284300055629971
2278, epoch_train_loss=0.02284300055629971
Epoch 2279
Epoch 2279 :: Batch 0/1
Batch Loss = 0.0228392530097039
2279, epoch_train_loss=0.0228392530097039
Epoch 2280
Epoch 2280 :: Batch 0/1
Batch Loss = 0.02283551324338321
2280, epoch_train_loss=0.02283551324338321
Epoch 2281
Epoch 2281 :: Batch 0/1
Batch Loss = 0.022831781230720546
2281, epoch_train_loss=0.022831781230720546
Epoch 2282
Epoch 2282 :: Batch 0/1
Batch Loss = 0.02282805694505077
2282, epoch_train_loss=0.02282805694505077
Epoch 2283
Epoch 2283 :: Batch 0/1
Batch Loss = 0.022824340360877254
2283, epoch_train_loss=0.022824340360877254
Epoch 2284
Epoch 2284 :: Batch 0/1
Batch Loss = 0.022820631452642244
2284, epoch_train_loss=0.022820631452642244
Epoch 2285
Epoch 2285 :: Batch 0/1
Batch Loss = 0.02281693019563477
2285, epoch_train_loss=0.02281693019563477
Epoch 2286
Epoch 2286 :: Batch 0/1
Batch Loss = 0.022813236565979552
2286, epoch_train_loss=0.022813236565979552
Epoch 2287
Epoch 2287 :: Batch 0/1
Batch Loss = 0.02280955053971935
2287, epoch_train_loss=0.02280955053971935
Epoch 2288
Epoch 2288 :: Batch 0/1
Batch Loss = 0.0228058720937168
2288, epoch_train_loss=0.0228058720937168
Epoch 2289
Epoch 2289 :: Batch 0/1
Batch Loss = 0.022802201205642903
2289, epoch_train_loss=0.022802201205642903
Epoch 2290
Epoch 2290 :: Batch 0/1
Batch Loss = 0.022798537853064436
2290, epoch_train_loss=0.022798537853064436
Epoch 2291
Epoch 2291 :: Batch 0/1
Batch Loss = 0.02279488201433986
2291, epoch_train_loss=0.02279488201433986
Epoch 2292
Epoch 2292 :: Batch 0/1
Batch Loss = 0.022791233668308323
2292, epoch_train_loss=0.022791233668308323
Epoch 2293
Epoch 2293 :: Batch 0/1
Batch Loss = 0.022787592794280522
2293, epoch_train_loss=0.022787592794280522
Epoch 2294
Epoch 2294 :: Batch 0/1
Batch Loss = 0.02278395937202938
2294, epoch_train_loss=0.02278395937202938
Epoch 2295
Epoch 2295 :: Batch 0/1
Batch Loss = 0.022780333381780694
2295, epoch_train_loss=0.022780333381780694
Epoch 2296
Epoch 2296 :: Batch 0/1
Batch Loss = 0.02277671480420357
2296, epoch_train_loss=0.02277671480420357
Epoch 2297
Epoch 2297 :: Batch 0/1
Batch Loss = 0.02277310362040086
2297, epoch_train_loss=0.02277310362040086
Epoch 2298
Epoch 2298 :: Batch 0/1
Batch Loss = 0.022769499811899428
2298, epoch_train_loss=0.022769499811899428
Epoch 2299
Epoch 2299 :: Batch 0/1
Batch Loss = 0.022765903360640327
2299, epoch_train_loss=0.022765903360640327
Epoch 2300
Epoch 2300 :: Batch 0/1
Batch Loss = 0.02276231424896893
2300, epoch_train_loss=0.02276231424896893
Epoch 2301
Epoch 2301 :: Batch 0/1
Batch Loss = 0.02275873245962492
2301, epoch_train_loss=0.02275873245962492
Epoch 2302
Epoch 2302 :: Batch 0/1
Batch Loss = 0.022755157975732215
2302, epoch_train_loss=0.022755157975732215
Epoch 2303
Epoch 2303 :: Batch 0/1
Batch Loss = 0.022751590780788776
2303, epoch_train_loss=0.022751590780788776
Epoch 2304
Epoch 2304 :: Batch 0/1
Batch Loss = 0.022748030858656416
2304, epoch_train_loss=0.022748030858656416
Epoch 2305
Epoch 2305 :: Batch 0/1
Batch Loss = 0.0227444781935504
2305, epoch_train_loss=0.0227444781935504
Epoch 2306
Epoch 2306 :: Batch 0/1
Batch Loss = 0.022740932770029126
2306, epoch_train_loss=0.022740932770029126
Epoch 2307
Epoch 2307 :: Batch 0/1
Batch Loss = 0.02273739457298353
2307, epoch_train_loss=0.02273739457298353
Epoch 2308
Epoch 2308 :: Batch 0/1
Batch Loss = 0.022733863587626635
2308, epoch_train_loss=0.022733863587626635
Epoch 2309
Epoch 2309 :: Batch 0/1
Batch Loss = 0.022730339799482854
2309, epoch_train_loss=0.022730339799482854
Epoch 2310
Epoch 2310 :: Batch 0/1
Batch Loss = 0.022726823194377275
2310, epoch_train_loss=0.022726823194377275
Epoch 2311
Epoch 2311 :: Batch 0/1
Batch Loss = 0.02272331375813693
2311, epoch_train_loss=0.02272331375813693
Epoch 2312
Epoch 2312 :: Batch 0/1
Batch Loss = 0.022719811477444864
2312, epoch_train_loss=0.022719811477444864
Epoch 2313
Epoch 2313 :: Batch 0/1
Batch Loss = 0.02271631633896329
2313, epoch_train_loss=0.02271631633896329
Epoch 2314
Epoch 2314 :: Batch 0/1
Batch Loss = 0.02271282832932532
2314, epoch_train_loss=0.02271282832932532
Epoch 2315
Epoch 2315 :: Batch 0/1
Batch Loss = 0.022709347435983605
2315, epoch_train_loss=0.022709347435983605
Epoch 2316
Epoch 2316 :: Batch 0/1
Batch Loss = 0.022705873646053407
2316, epoch_train_loss=0.022705873646053407
Epoch 2317
Epoch 2317 :: Batch 0/1
Batch Loss = 0.022702406947444905
2317, epoch_train_loss=0.022702406947444905
Epoch 2318
Epoch 2318 :: Batch 0/1
Batch Loss = 0.022698947327710696
2318, epoch_train_loss=0.022698947327710696
Epoch 2319
Epoch 2319 :: Batch 0/1
Batch Loss = 0.02269549477517337
2319, epoch_train_loss=0.02269549477517337
Epoch 2320
Epoch 2320 :: Batch 0/1
Batch Loss = 0.022692049278060282
2320, epoch_train_loss=0.022692049278060282
Epoch 2321
Epoch 2321 :: Batch 0/1
Batch Loss = 0.022688610824776963
2321, epoch_train_loss=0.022688610824776963
Epoch 2322
Epoch 2322 :: Batch 0/1
Batch Loss = 0.022685179403613902
2322, epoch_train_loss=0.022685179403613902
Epoch 2323
Epoch 2323 :: Batch 0/1
Batch Loss = 0.022681755003299803
2323, epoch_train_loss=0.022681755003299803
Epoch 2324
Epoch 2324 :: Batch 0/1
Batch Loss = 0.02267833761298736
2324, epoch_train_loss=0.02267833761298736
Epoch 2325
Epoch 2325 :: Batch 0/1
Batch Loss = 0.0226749272213987
2325, epoch_train_loss=0.0226749272213987
Epoch 2326
Epoch 2326 :: Batch 0/1
Batch Loss = 0.02267152381765729
2326, epoch_train_loss=0.02267152381765729
Epoch 2327
Epoch 2327 :: Batch 0/1
Batch Loss = 0.02266812739127341
2327, epoch_train_loss=0.02266812739127341
Epoch 2328
Epoch 2328 :: Batch 0/1
Batch Loss = 0.022664737931572854
2328, epoch_train_loss=0.022664737931572854
Epoch 2329
Epoch 2329 :: Batch 0/1
Batch Loss = 0.022661355427687626
2329, epoch_train_loss=0.022661355427687626
Epoch 2330
Epoch 2330 :: Batch 0/1
Batch Loss = 0.022657979869100897
2330, epoch_train_loss=0.022657979869100897
Epoch 2331
Epoch 2331 :: Batch 0/1
Batch Loss = 0.022654611245355616
2331, epoch_train_loss=0.022654611245355616
Epoch 2332
Epoch 2332 :: Batch 0/1
Batch Loss = 0.022651249546042394
2332, epoch_train_loss=0.022651249546042394
Epoch 2333
Epoch 2333 :: Batch 0/1
Batch Loss = 0.022647894761062798
2333, epoch_train_loss=0.022647894761062798
Epoch 2334
Epoch 2334 :: Batch 0/1
Batch Loss = 0.022644546879789524
2334, epoch_train_loss=0.022644546879789524
Epoch 2335
Epoch 2335 :: Batch 0/1
Batch Loss = 0.022641205891882583
2335, epoch_train_loss=0.022641205891882583
Epoch 2336
Epoch 2336 :: Batch 0/1
Batch Loss = 0.022637871787000507
2336, epoch_train_loss=0.022637871787000507
Epoch 2337
Epoch 2337 :: Batch 0/1
Batch Loss = 0.022634544554788003
2337, epoch_train_loss=0.022634544554788003
Epoch 2338
Epoch 2338 :: Batch 0/1
Batch Loss = 0.02263122418513627
2338, epoch_train_loss=0.02263122418513627
Epoch 2339
Epoch 2339 :: Batch 0/1
Batch Loss = 0.022627910667351373
2339, epoch_train_loss=0.022627910667351373
Epoch 2340
Epoch 2340 :: Batch 0/1
Batch Loss = 0.02262460399096197
2340, epoch_train_loss=0.02262460399096197
Epoch 2341
Epoch 2341 :: Batch 0/1
Batch Loss = 0.022621304145433167
2341, epoch_train_loss=0.022621304145433167
Epoch 2342
Epoch 2342 :: Batch 0/1
Batch Loss = 0.022618011120154095
2342, epoch_train_loss=0.022618011120154095
Epoch 2343
Epoch 2343 :: Batch 0/1
Batch Loss = 0.022614724904425475
2343, epoch_train_loss=0.022614724904425475
Epoch 2344
Epoch 2344 :: Batch 0/1
Batch Loss = 0.02261144548744718
2344, epoch_train_loss=0.02261144548744718
Epoch 2345
Epoch 2345 :: Batch 0/1
Batch Loss = 0.022608172858305853
2345, epoch_train_loss=0.022608172858305853
Epoch 2346
Epoch 2346 :: Batch 0/1
Batch Loss = 0.02260490700596256
2346, epoch_train_loss=0.02260490700596256
Epoch 2347
Epoch 2347 :: Batch 0/1
Batch Loss = 0.022601647919240415
2347, epoch_train_loss=0.022601647919240415
Epoch 2348
Epoch 2348 :: Batch 0/1
Batch Loss = 0.022598395586812394
2348, epoch_train_loss=0.022598395586812394
Epoch 2349
Epoch 2349 :: Batch 0/1
Batch Loss = 0.02259514999718909
2349, epoch_train_loss=0.02259514999718909
Epoch 2350
Epoch 2350 :: Batch 0/1
Batch Loss = 0.022591911138706577
2350, epoch_train_loss=0.022591911138706577
Epoch 2351
Epoch 2351 :: Batch 0/1
Batch Loss = 0.02258867899951441
2351, epoch_train_loss=0.02258867899951441
Epoch 2352
Epoch 2352 :: Batch 0/1
Batch Loss = 0.022585453567298557
2352, epoch_train_loss=0.022585453567298557
Epoch 2353
Epoch 2353 :: Batch 0/1
Batch Loss = 0.022582234830065863
2353, epoch_train_loss=0.022582234830065863
Epoch 2354
Epoch 2354 :: Batch 0/1
Batch Loss = 0.02257902277533522
2354, epoch_train_loss=0.02257902277533522
Epoch 2355
Epoch 2355 :: Batch 0/1
Batch Loss = 0.022575817390391963
2355, epoch_train_loss=0.022575817390391963
Epoch 2356
Epoch 2356 :: Batch 0/1
Batch Loss = 0.0225726186620135
2356, epoch_train_loss=0.0225726186620135
Epoch 2357
Epoch 2357 :: Batch 0/1
Batch Loss = 0.022569426577248027
2357, epoch_train_loss=0.022569426577248027
Epoch 2358
Epoch 2358 :: Batch 0/1
Batch Loss = 0.022566241122612647
2358, epoch_train_loss=0.022566241122612647
Epoch 2359
Epoch 2359 :: Batch 0/1
Batch Loss = 0.02256306228408506
2359, epoch_train_loss=0.02256306228408506
Epoch 2360
Epoch 2360 :: Batch 0/1
Batch Loss = 0.02255989004787798
2360, epoch_train_loss=0.02255989004787798
Epoch 2361
Epoch 2361 :: Batch 0/1
Batch Loss = 0.022556724399642784
2361, epoch_train_loss=0.022556724399642784
Epoch 2362
Epoch 2362 :: Batch 0/1
Batch Loss = 0.02255356532446187
2362, epoch_train_loss=0.02255356532446187
Epoch 2363
Epoch 2363 :: Batch 0/1
Batch Loss = 0.022550412807618988
2363, epoch_train_loss=0.022550412807618988
Epoch 2364
Epoch 2364 :: Batch 0/1
Batch Loss = 0.022547266833549764
2364, epoch_train_loss=0.022547266833549764
Epoch 2365
Epoch 2365 :: Batch 0/1
Batch Loss = 0.02254412738687013
2365, epoch_train_loss=0.02254412738687013
Epoch 2366
Epoch 2366 :: Batch 0/1
Batch Loss = 0.02254099445158935
2366, epoch_train_loss=0.02254099445158935
Epoch 2367
Epoch 2367 :: Batch 0/1
Batch Loss = 0.022537868011103707
2367, epoch_train_loss=0.022537868011103707
Epoch 2368
Epoch 2368 :: Batch 0/1
Batch Loss = 0.02253474804896058
2368, epoch_train_loss=0.02253474804896058
Epoch 2369
Epoch 2369 :: Batch 0/1
Batch Loss = 0.02253163454782099
2369, epoch_train_loss=0.02253163454782099
Epoch 2370
Epoch 2370 :: Batch 0/1
Batch Loss = 0.02252852749022369
2370, epoch_train_loss=0.02252852749022369
Epoch 2371
Epoch 2371 :: Batch 0/1
Batch Loss = 0.022525426858575427
2371, epoch_train_loss=0.022525426858575427
Epoch 2372
Epoch 2372 :: Batch 0/1
Batch Loss = 0.0225223326343779
2372, epoch_train_loss=0.0225223326343779
Epoch 2373
Epoch 2373 :: Batch 0/1
Batch Loss = 0.02251924479924236
2373, epoch_train_loss=0.02251924479924236
Epoch 2374
Epoch 2374 :: Batch 0/1
Batch Loss = 0.02251616333386451
2374, epoch_train_loss=0.02251616333386451
Epoch 2375
Epoch 2375 :: Batch 0/1
Batch Loss = 0.022513088218782753
2375, epoch_train_loss=0.022513088218782753
Epoch 2376
Epoch 2376 :: Batch 0/1
Batch Loss = 0.022510019434370657
2376, epoch_train_loss=0.022510019434370657
Epoch 2377
Epoch 2377 :: Batch 0/1
Batch Loss = 0.022506956959821606
2377, epoch_train_loss=0.022506956959821606
Epoch 2378
Epoch 2378 :: Batch 0/1
Batch Loss = 0.02250390077491227
2378, epoch_train_loss=0.02250390077491227
Epoch 2379
Epoch 2379 :: Batch 0/1
Batch Loss = 0.022500850858232186
2379, epoch_train_loss=0.022500850858232186
Epoch 2380
Epoch 2380 :: Batch 0/1
Batch Loss = 0.02249780718843996
2380, epoch_train_loss=0.02249780718843996
Epoch 2381
Epoch 2381 :: Batch 0/1
Batch Loss = 0.022494769743005592
2381, epoch_train_loss=0.022494769743005592
Epoch 2382
Epoch 2382 :: Batch 0/1
Batch Loss = 0.022491738499961597
2382, epoch_train_loss=0.022491738499961597
Epoch 2383
Epoch 2383 :: Batch 0/1
Batch Loss = 0.022488713436148635
2383, epoch_train_loss=0.022488713436148635
Epoch 2384
Epoch 2384 :: Batch 0/1
Batch Loss = 0.022485694528215
2384, epoch_train_loss=0.022485694528215
Epoch 2385
Epoch 2385 :: Batch 0/1
Batch Loss = 0.02248268175236528
2385, epoch_train_loss=0.02248268175236528
Epoch 2386
Epoch 2386 :: Batch 0/1
Batch Loss = 0.022479675084359736
2386, epoch_train_loss=0.022479675084359736
Epoch 2387
Epoch 2387 :: Batch 0/1
Batch Loss = 0.02247667449951435
2387, epoch_train_loss=0.02247667449951435
Epoch 2388
Epoch 2388 :: Batch 0/1
Batch Loss = 0.02247367997270106
2388, epoch_train_loss=0.02247367997270106
Epoch 2389
Epoch 2389 :: Batch 0/1
Batch Loss = 0.022470691478348712
2389, epoch_train_loss=0.022470691478348712
Epoch 2390
Epoch 2390 :: Batch 0/1
Batch Loss = 0.02246770899044433
2390, epoch_train_loss=0.02246770899044433
Epoch 2391
Epoch 2391 :: Batch 0/1
Batch Loss = 0.022464732482534878
2391, epoch_train_loss=0.022464732482534878
Epoch 2392
Epoch 2392 :: Batch 0/1
Batch Loss = 0.02246176192772947
2392, epoch_train_loss=0.02246176192772947
Epoch 2393
Epoch 2393 :: Batch 0/1
Batch Loss = 0.02245879729870199
2393, epoch_train_loss=0.02245879729870199
Epoch 2394
Epoch 2394 :: Batch 0/1
Batch Loss = 0.022455838567694165
2394, epoch_train_loss=0.022455838567694165
Epoch 2395
Epoch 2395 :: Batch 0/1
Batch Loss = 0.022452885706519
2395, epoch_train_loss=0.022452885706519
Epoch 2396
Epoch 2396 :: Batch 0/1
Batch Loss = 0.02244993868656465
2396, epoch_train_loss=0.02244993868656465
Epoch 2397
Epoch 2397 :: Batch 0/1
Batch Loss = 0.022446997478556413
2397, epoch_train_loss=0.022446997478556413
Epoch 2398
Epoch 2398 :: Batch 0/1
Batch Loss = 0.022444062053288787
2398, epoch_train_loss=0.022444062053288787
Epoch 2399
Epoch 2399 :: Batch 0/1
Batch Loss = 0.022441132380902208
2399, epoch_train_loss=0.022441132380902208
Epoch 2400
Epoch 2400 :: Batch 0/1
Batch Loss = 0.022438208431131236
2400, epoch_train_loss=0.022438208431131236
Epoch 2401
Epoch 2401 :: Batch 0/1
Batch Loss = 0.022435290173069746
2401, epoch_train_loss=0.022435290173069746
Epoch 2402
Epoch 2402 :: Batch 0/1
Batch Loss = 0.02243237757589875
2402, epoch_train_loss=0.02243237757589875
Epoch 2403
Epoch 2403 :: Batch 0/1
Batch Loss = 0.02242947060816979
2403, epoch_train_loss=0.02242947060816979
Epoch 2404
Epoch 2404 :: Batch 0/1
Batch Loss = 0.02242656923781338
2404, epoch_train_loss=0.02242656923781338
Epoch 2405
Epoch 2405 :: Batch 0/1
Batch Loss = 0.022423673432863448
2405, epoch_train_loss=0.022423673432863448
Epoch 2406
Epoch 2406 :: Batch 0/1
Batch Loss = 0.022420783160507364
2406, epoch_train_loss=0.022420783160507364
Epoch 2407
Epoch 2407 :: Batch 0/1
Batch Loss = 0.02241789838804781
2407, epoch_train_loss=0.02241789838804781
Epoch 2408
Epoch 2408 :: Batch 0/1
Batch Loss = 0.0224150190821939
2408, epoch_train_loss=0.0224150190821939
Epoch 2409
Epoch 2409 :: Batch 0/1
Batch Loss = 0.022412145209070446
2409, epoch_train_loss=0.022412145209070446
Epoch 2410
Epoch 2410 :: Batch 0/1
Batch Loss = 0.022409276734936482
2410, epoch_train_loss=0.022409276734936482
Epoch 2411
Epoch 2411 :: Batch 0/1
Batch Loss = 0.02240641362524464
2411, epoch_train_loss=0.02240641362524464
Epoch 2412
Epoch 2412 :: Batch 0/1
Batch Loss = 0.02240355584559474
2412, epoch_train_loss=0.02240355584559474
Epoch 2413
Epoch 2413 :: Batch 0/1
Batch Loss = 0.02240070336103182
2413, epoch_train_loss=0.02240070336103182
Epoch 2414
Epoch 2414 :: Batch 0/1
Batch Loss = 0.022397856135820817
2414, epoch_train_loss=0.022397856135820817
Epoch 2415
Epoch 2415 :: Batch 0/1
Batch Loss = 0.022395014134863455
2415, epoch_train_loss=0.022395014134863455
Epoch 2416
Epoch 2416 :: Batch 0/1
Batch Loss = 0.022392177322294517
2416, epoch_train_loss=0.022392177322294517
Epoch 2417
Epoch 2417 :: Batch 0/1
Batch Loss = 0.022389345661726394
2417, epoch_train_loss=0.022389345661726394
Epoch 2418
Epoch 2418 :: Batch 0/1
Batch Loss = 0.022386519116723954
2418, epoch_train_loss=0.022386519116723954
Epoch 2419
Epoch 2419 :: Batch 0/1
Batch Loss = 0.022383697650809723
2419, epoch_train_loss=0.022383697650809723
Epoch 2420
Epoch 2420 :: Batch 0/1
Batch Loss = 0.022380881226771725
2420, epoch_train_loss=0.022380881226771725
Epoch 2421
Epoch 2421 :: Batch 0/1
Batch Loss = 0.022378069807369543
2421, epoch_train_loss=0.022378069807369543
Epoch 2422
Epoch 2422 :: Batch 0/1
Batch Loss = 0.022375263355338227
2422, epoch_train_loss=0.022375263355338227
Epoch 2423
Epoch 2423 :: Batch 0/1
Batch Loss = 0.02237246183246891
2423, epoch_train_loss=0.02237246183246891
Epoch 2424
Epoch 2424 :: Batch 0/1
Batch Loss = 0.022369665201233683
2424, epoch_train_loss=0.022369665201233683
Epoch 2425
Epoch 2425 :: Batch 0/1
Batch Loss = 0.02236687342317296
2425, epoch_train_loss=0.02236687342317296
Epoch 2426
Epoch 2426 :: Batch 0/1
Batch Loss = 0.02236408645982404
2426, epoch_train_loss=0.02236408645982404
Epoch 2427
Epoch 2427 :: Batch 0/1
Batch Loss = 0.022361304272496337
2427, epoch_train_loss=0.022361304272496337
Epoch 2428
Epoch 2428 :: Batch 0/1
Batch Loss = 0.022358526822273135
2428, epoch_train_loss=0.022358526822273135
Epoch 2429
Epoch 2429 :: Batch 0/1
Batch Loss = 0.022355754070247542
2429, epoch_train_loss=0.022355754070247542
Epoch 2430
Epoch 2430 :: Batch 0/1
Batch Loss = 0.02235298597660636
2430, epoch_train_loss=0.02235298597660636
Epoch 2431
Epoch 2431 :: Batch 0/1
Batch Loss = 0.022350222502012025
2431, epoch_train_loss=0.022350222502012025
Epoch 2432
Epoch 2432 :: Batch 0/1
Batch Loss = 0.0223474636069091
2432, epoch_train_loss=0.0223474636069091
Epoch 2433
Epoch 2433 :: Batch 0/1
Batch Loss = 0.022344709251082638
2433, epoch_train_loss=0.022344709251082638
Epoch 2434
Epoch 2434 :: Batch 0/1
Batch Loss = 0.022341959394102204
2434, epoch_train_loss=0.022341959394102204
Epoch 2435
Epoch 2435 :: Batch 0/1
Batch Loss = 0.022339213995795706
2435, epoch_train_loss=0.022339213995795706
Epoch 2436
Epoch 2436 :: Batch 0/1
Batch Loss = 0.022336473015767428
2436, epoch_train_loss=0.022336473015767428
Epoch 2437
Epoch 2437 :: Batch 0/1
Batch Loss = 0.022333736412982887
2437, epoch_train_loss=0.022333736412982887
Epoch 2438
Epoch 2438 :: Batch 0/1
Batch Loss = 0.02233100414617349
2438, epoch_train_loss=0.02233100414617349
Epoch 2439
Epoch 2439 :: Batch 0/1
Batch Loss = 0.022328276174353883
2439, epoch_train_loss=0.022328276174353883
Epoch 2440
Epoch 2440 :: Batch 0/1
Batch Loss = 0.022325552456053027
2440, epoch_train_loss=0.022325552456053027
Epoch 2441
Epoch 2441 :: Batch 0/1
Batch Loss = 0.02232283294966783
2441, epoch_train_loss=0.02232283294966783
Epoch 2442
Epoch 2442 :: Batch 0/1
Batch Loss = 0.022320117613297063
2442, epoch_train_loss=0.022320117613297063
Epoch 2443
Epoch 2443 :: Batch 0/1
Batch Loss = 0.02231740640476764
2443, epoch_train_loss=0.02231740640476764
Epoch 2444
Epoch 2444 :: Batch 0/1
Batch Loss = 0.022314699282011435
2444, epoch_train_loss=0.022314699282011435
Epoch 2445
Epoch 2445 :: Batch 0/1
Batch Loss = 0.022311996202902685
2445, epoch_train_loss=0.022311996202902685
Epoch 2446
Epoch 2446 :: Batch 0/1
Batch Loss = 0.022309297124987932
2446, epoch_train_loss=0.022309297124987932
Epoch 2447
Epoch 2447 :: Batch 0/1
Batch Loss = 0.022306602006591528
2447, epoch_train_loss=0.022306602006591528
Epoch 2448
Epoch 2448 :: Batch 0/1
Batch Loss = 0.022303910805897765
2448, epoch_train_loss=0.022303910805897765
Epoch 2449
Epoch 2449 :: Batch 0/1
Batch Loss = 0.022301223484024312
2449, epoch_train_loss=0.022301223484024312
Epoch 2450
Epoch 2450 :: Batch 0/1
Batch Loss = 0.022298540004656465
2450, epoch_train_loss=0.022298540004656465
Epoch 2451
Epoch 2451 :: Batch 0/1
Batch Loss = 0.022295860340656833
2451, epoch_train_loss=0.022295860340656833
Epoch 2452
Epoch 2452 :: Batch 0/1
Batch Loss = 0.02229318447746797
2452, epoch_train_loss=0.02229318447746797
Epoch 2453
Epoch 2453 :: Batch 0/1
Batch Loss = 0.022290512434489384
2453, epoch_train_loss=0.022290512434489384
Epoch 2454
Epoch 2454 :: Batch 0/1
Batch Loss = 0.02228784428769672
2454, epoch_train_loss=0.02228784428769672
Epoch 2455
Epoch 2455 :: Batch 0/1
Batch Loss = 0.02228518024303942
2455, epoch_train_loss=0.02228518024303942
Epoch 2456
Epoch 2456 :: Batch 0/1
Batch Loss = 0.02228252074508459
2456, epoch_train_loss=0.02228252074508459
Epoch 2457
Epoch 2457 :: Batch 0/1
Batch Loss = 0.022279866755522276
2457, epoch_train_loss=0.022279866755522276
Epoch 2458
Epoch 2458 :: Batch 0/1
Batch Loss = 0.022277220237275634
2458, epoch_train_loss=0.022277220237275634
Epoch 2459
Epoch 2459 :: Batch 0/1
Batch Loss = 0.02227458526190806
2459, epoch_train_loss=0.02227458526190806
Epoch 2460
Epoch 2460 :: Batch 0/1
Batch Loss = 0.02227197012693271
2460, epoch_train_loss=0.02227197012693271
Epoch 2461
Epoch 2461 :: Batch 0/1
Batch Loss = 0.022269391923079883
2461, epoch_train_loss=0.022269391923079883
Epoch 2462
Epoch 2462 :: Batch 0/1
Batch Loss = 0.022266885766618952
2462, epoch_train_loss=0.022266885766618952
Epoch 2463
Epoch 2463 :: Batch 0/1
Batch Loss = 0.022264524131385376
2463, epoch_train_loss=0.022264524131385376
Epoch 2464
Epoch 2464 :: Batch 0/1
Batch Loss = 0.022262457462120615
2464, epoch_train_loss=0.022262457462120615
Epoch 2465
Epoch 2465 :: Batch 0/1
Batch Loss = 0.02226099743009006
2465, epoch_train_loss=0.02226099743009006
Epoch 2466
Epoch 2466 :: Batch 0/1
Batch Loss = 0.022260798314869542
2466, epoch_train_loss=0.022260798314869542
Epoch 2467
Epoch 2467 :: Batch 0/1
Batch Loss = 0.022263218804901192
2467, epoch_train_loss=0.022263218804901192
Epoch 2468
Epoch 2468 :: Batch 0/1
Batch Loss = 0.022271146142864063
2468, epoch_train_loss=0.022271146142864063
Epoch 2469
Epoch 2469 :: Batch 0/1
Batch Loss = 0.022290559408194823
2469, epoch_train_loss=0.022290559408194823
Epoch 2470
Epoch 2470 :: Batch 0/1
Batch Loss = 0.022334295494256754
2470, epoch_train_loss=0.022334295494256754
Epoch 2471
Epoch 2471 :: Batch 0/1
Batch Loss = 0.022428509983306
2471, epoch_train_loss=0.022428509983306
Epoch 2472
Epoch 2472 :: Batch 0/1
Batch Loss = 0.022628395321891405
2472, epoch_train_loss=0.022628395321891405
Epoch 2473
Epoch 2473 :: Batch 0/1
Batch Loss = 0.023041115152005975
2473, epoch_train_loss=0.023041115152005975
Epoch 2474
Epoch 2474 :: Batch 0/1
Batch Loss = 0.02384341988032703
2474, epoch_train_loss=0.02384341988032703
Epoch 2475
Epoch 2475 :: Batch 0/1
Batch Loss = 0.025371014740996418
2475, epoch_train_loss=0.025371014740996418
Epoch 2476
Epoch 2476 :: Batch 0/1
Batch Loss = 0.027609393268936012
2476, epoch_train_loss=0.027609393268936012
Epoch 2477
Epoch 2477 :: Batch 0/1
Batch Loss = 0.031376061805565784
2477, epoch_train_loss=0.031376061805565784
Epoch 2478
Epoch 2478 :: Batch 0/1
Batch Loss = 0.03675999690783097
2478, epoch_train_loss=0.03675999690783097
Epoch 2479
Epoch 2479 :: Batch 0/1
Batch Loss = 0.04310224326181092
2479, epoch_train_loss=0.04310224326181092
Epoch 2480
Epoch 2480 :: Batch 0/1
Batch Loss = 0.05019113177373663
2480, epoch_train_loss=0.05019113177373663
Epoch 2481
Epoch 2481 :: Batch 0/1
Batch Loss = 0.052557796709281804
2481, epoch_train_loss=0.052557796709281804
Epoch 2482
Epoch 2482 :: Batch 0/1
Batch Loss = 0.050789975135383233
2482, epoch_train_loss=0.050789975135383233
Epoch 2483
Epoch 2483 :: Batch 0/1
Batch Loss = 0.03992067715364966
2483, epoch_train_loss=0.03992067715364966
Epoch 2484
Epoch 2484 :: Batch 0/1
Batch Loss = 0.028676967830777472
2484, epoch_train_loss=0.028676967830777472
Epoch 2485
Epoch 2485 :: Batch 0/1
Batch Loss = 0.022822597230937026
2485, epoch_train_loss=0.022822597230937026
Epoch 2486
Epoch 2486 :: Batch 0/1
Batch Loss = 0.025293603105070218
2486, epoch_train_loss=0.025293603105070218
Epoch 2487
Epoch 2487 :: Batch 0/1
Batch Loss = 0.03148822971359888
2487, epoch_train_loss=0.03148822971359888
Epoch 2488
Epoch 2488 :: Batch 0/1
Batch Loss = 0.03308827561599173
2488, epoch_train_loss=0.03308827561599173
Epoch 2489
Epoch 2489 :: Batch 0/1
Batch Loss = 0.02828246481140224
2489, epoch_train_loss=0.02828246481140224
Epoch 2490
Epoch 2490 :: Batch 0/1
Batch Loss = 0.023823081184225728
2490, epoch_train_loss=0.023823081184225728
Epoch 2491
Epoch 2491 :: Batch 0/1
Batch Loss = 0.024325438844213638
2491, epoch_train_loss=0.024325438844213638
Epoch 2492
Epoch 2492 :: Batch 0/1
Batch Loss = 0.029125584033729675
2492, epoch_train_loss=0.029125584033729675
Epoch 2493
Epoch 2493 :: Batch 0/1
Batch Loss = 0.029497290601382665
2493, epoch_train_loss=0.029497290601382665
Epoch 2494
Epoch 2494 :: Batch 0/1
Batch Loss = 0.02671349951656324
2494, epoch_train_loss=0.02671349951656324
Epoch 2495
Epoch 2495 :: Batch 0/1
Batch Loss = 0.023704272974888613
2495, epoch_train_loss=0.023704272974888613
Epoch 2496
Epoch 2496 :: Batch 0/1
Batch Loss = 0.02328116466511044
2496, epoch_train_loss=0.02328116466511044
Epoch 2497
Epoch 2497 :: Batch 0/1
Batch Loss = 0.026172711052791325
2497, epoch_train_loss=0.026172711052791325
Epoch 2498
Epoch 2498 :: Batch 0/1
Batch Loss = 0.027286769188847832
2498, epoch_train_loss=0.027286769188847832
Epoch 2499
Epoch 2499 :: Batch 0/1
Batch Loss = 0.027250567261208947
2499, epoch_train_loss=0.027250567261208947
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting "B3LYP_WITH_VWN5 = True" in pyscf_conf.py
  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0a92d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0a92d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
WARNING: External module "mldftdat" required for non-local descriptor use.
<pyscf.gto.mole.Mole object at 0x7ffeac0a92d0> [['P', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac0a9ab0> [['N', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac0a9cc0> [['H', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac0a9b40> [['Li', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac0a9bd0> [['O', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac0a9e70> [['Cl', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac0aa080> [['Al', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac0a9e10> [['S', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac0aa470> [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0aa4a0> [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0aa5c0> [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0aa8f0> [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeac0aab00> [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeac0aa890> [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0aada0> [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0aadd0> [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0aaa40> [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeac0ab070> [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0aaf80> [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0aaef0> [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0ab220> [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0ab430> [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeac0ab1c0> [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeac0ab6d0> [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0ab370> [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]] 5
<pyscf.gto.mole.Mole object at 0x7ffeac0ab820> [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeac0abb50> [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]] 5
mol:  [['P', array([0., 0., 0.])]]
converged SCF energy = -341.104145992717  <S^2> = 3.7502984  2S+1 = 4.0001492
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0a9ab0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0a9ab0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-3.48053232e-03 -9.22981806e-04 -2.09507924e-03 ... -1.11294850e+01
 -1.11294850e+01 -1.11294850e+01] = SCAN,
rho_a.shape=(6, 6320), rho_b.shape=(6, 6320)
fxc_a.shape=(6320,), fxc_b.shape=(6320,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12640), fxc.shape=(12640,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(2, 6320, 15)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12640, 15)
rho_filt.shape=(12640,)
mol:  [['N', array([0., 0., 0.])]]
converged SCF energy = -54.5289742046674  <S^2> = 3.7524945  2S+1 = 4.0012471
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0a9cc0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0a9cc0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5016, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.10256797e-03 -5.98013179e-04 -6.71209617e-05 ... -5.03581543e+00
 -5.03581543e+00 -5.03581543e+00] = SCAN,
rho_a.shape=(6, 5016), rho_b.shape=(6, 5016)
fxc_a.shape=(5016,), fxc_b.shape=(5016,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 10032), fxc.shape=(10032,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5016, 22)
rho shape (4, 4, 2, 5016)
rho_filt shape: (5016,)
get descriptors tdrho.shape=(2, 5016, 15)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(10032, 15)
rho_filt.shape=(10032,)
mol:  [['H', array([0., 0., 0.])]]
converged SCF energy = -0.499812984008539  <S^2> = 0.75  2S+1 = 2
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0a9b40> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0a9b40> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 7, 7)
ao.shape (10, 2440, 7)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.59173730e-03 -7.67300285e-04 -6.09330303e-05 ... -7.27736608e-01
 -7.27736608e-01 -7.27736608e-01] = SCAN,
rho_a.shape=(6, 2440), rho_b.shape=(6, 2440)
fxc_a.shape=(2440,), fxc_b.shape=(2440,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 2440), fxc.shape=(2440,)
get_data, dm shape = (2, 7, 7)
ao_eval.shape=(4, 2440, 7)
rho shape (4, 4, 2, 2440)
rho_filt shape: (2440,)
get descriptors tdrho.shape=(2, 2440, 15)
mol:  [['Li', array([0., 0., 0.])]]
converged SCF energy = -7.46006188627842  <S^2> = 0.75000049  2S+1 = 2.0000005
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0a9bd0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0a9bd0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 4592, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-6.71507910e-03 -1.45299376e-03 -1.45299376e-03 ... -1.46930969e-02
 -2.05021258e+00 -2.05021258e+00] = SCAN,
rho_a.shape=(6, 4592), rho_b.shape=(6, 4592)
fxc_a.shape=(4592,), fxc_b.shape=(4592,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 9184), fxc.shape=(9184,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 4592, 22)
rho shape (4, 4, 2, 4592)
rho_filt shape: (4592,)
get descriptors tdrho.shape=(2, 4592, 15)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(9184, 15)
rho_filt.shape=(9184,)
mol:  [['O', array([0., 0., 0.])]]
SCF not converged.
SCF energy = -75.0033774338371 after 50 cycles  <S^2> = 2.0027403  2S+1 = 3.0018263
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0a9e70> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0a9e70> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5040, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-7.08461346e-04 -1.23177614e-04 -6.18265752e-06 ... -5.78388659e+00
 -5.78388659e+00 -5.78388659e+00] = SCAN,
rho_a.shape=(6, 5040), rho_b.shape=(6, 5040)
fxc_a.shape=(5040,), fxc_b.shape=(5040,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 10080), fxc.shape=(10080,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5040, 22)
rho shape (4, 4, 2, 5040)
rho_filt shape: (5040,)
get descriptors tdrho.shape=(2, 5040, 15)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(10080, 15)
rho_filt.shape=(10080,)
mol:  [['Cl', array([0., 0., 0.])]]
converged SCF energy = -459.957577121563  <S^2> = 0.7516194  2S+1 = 2.0016187
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0aa080> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0aa080> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6152, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-5.92787549e-04 -9.51826916e-04 -3.33326666e-04 ... -1.26646370e+01
 -1.26646370e+01 -1.26646370e+01] = SCAN,
rho_a.shape=(6, 6152), rho_b.shape=(6, 6152)
fxc_a.shape=(6152,), fxc_b.shape=(6152,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12304), fxc.shape=(12304,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6152, 30)
rho shape (4, 4, 2, 6152)
rho_filt shape: (6152,)
get descriptors tdrho.shape=(2, 6152, 15)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12304, 15)
rho_filt.shape=(12304,)
mol:  [['Al', array([0., 0., 0.])]]
converged SCF energy = -242.226560990731  <S^2> = 0.75226414  2S+1 = 2.0022629
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0a9e10> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0a9e10> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6088, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.02415743 -0.01542239 -0.00781769 ... -0.0001393  -0.00172046
 -0.00012585] = SCAN,
rho_a.shape=(6, 6088), rho_b.shape=(6, 6088)
fxc_a.shape=(6088,), fxc_b.shape=(6088,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12176), fxc.shape=(12176,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6088, 30)
rho shape (4, 4, 2, 6088)
rho_filt shape: (6088,)
get descriptors tdrho.shape=(2, 6088, 15)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12176, 15)
rho_filt.shape=(12176,)
mol:  [['S', array([0., 0., 0.])]]
converged SCF energy = -397.938786806904  <S^2> = 2.0022329  2S+1 = 3.0014882
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0aa470> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0aa470> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.43071587e-03 -7.97253589e-04 -9.75867133e-04 ... -1.18982463e+01
 -1.18982463e+01 -1.18982463e+01] = SCAN,
rho_a.shape=(6, 6320), rho_b.shape=(6, 6320)
fxc_a.shape=(6320,), fxc_b.shape=(6320,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12640), fxc.shape=(12640,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(2, 6320, 15)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12640, 15)
rho_filt.shape=(12640,)
mol:  [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]]
converged SCF energy = -1.16580491182912  <S^2> = -8.8817842e-16  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0aa4a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0aa4a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 14, 14)
ao.shape (10, 4776, 14)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.31556415e-04 -9.72662915e-06 -3.66768501e-04 ... -5.54165573e-01
 -5.54165573e-01 -5.54165573e-01] = SCAN,
rho_a.shape=(6, 4776), rho_b.shape=(6, 4776)
fxc_a.shape=(4776,), fxc_b.shape=(4776,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 4776), fxc.shape=(4776,)
get_data, dm shape = (2, 14, 14)
ao_eval.shape=(4, 4776, 14)
rho shape (4, 4, 2, 4776)
rho_filt shape: (4776,)
get descriptors tdrho.shape=(2, 4776, 15)
mol:  [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]]
converged SCF energy = -109.439263799566  <S^2> = 1.5099033e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0aa5c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0aa5c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9848, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-9.68469910e-05 -9.84742277e-04 -2.59676148e-04 ... -2.39626668e-05
 -2.39626668e-05 -9.68469910e-05] = SCAN,
rho_a.shape=(6, 9848), rho_b.shape=(6, 9848)
fxc_a.shape=(9848,), fxc_b.shape=(9848,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9848), fxc.shape=(9848,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9848, 44)
rho shape (4, 4, 2, 9848)
rho_filt shape: (9848,)
get descriptors tdrho.shape=(2, 9848, 15)
mol:  [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]]
converged SCF energy = -107.339357395734  <S^2> = 1.7763568e-15  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0aa8f0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0aa8f0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9752, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.04987750e-03 -6.68953858e-04 -8.57556270e-04 ... -1.07485583e-03
 -8.01425698e-01 -8.01425698e-01] = SCAN,
rho_a.shape=(6, 9752), rho_b.shape=(6, 9752)
fxc_a.shape=(9752,), fxc_b.shape=(9752,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9752), fxc.shape=(9752,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9752, 44)
rho shape (4, 4, 2, 9752)
rho_filt shape: (9752,)
get descriptors tdrho.shape=(2, 9752, 15)
mol:  [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]]
converged SCF energy = -93.337792446513  <S^2> = 4.0072834e-10  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0aab00> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0aab00> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 12256, 51)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.97917285e-04 -2.54412366e-05 -3.15182243e-05 ... -6.37386500e-01
 -6.37386500e-01 -6.37386500e-01] = SCAN,
rho_a.shape=(6, 12256), rho_b.shape=(6, 12256)
fxc_a.shape=(12256,), fxc_b.shape=(12256,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12256), fxc.shape=(12256,)
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 12256, 51)
rho shape (4, 4, 2, 12256)
rho_filt shape: (12256,)
get descriptors tdrho.shape=(2, 12256, 15)
mol:  [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]]
converged SCF energy = -188.456965322844  <S^2> = 2.1316282e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0aa890> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0aa890> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 66, 66)
ao.shape (10, 14920, 66)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.50217115e-04 -2.07520066e-04 -9.23619896e-04 ... -2.74295208e-06
 -4.27559894e+00 -4.27559894e+00] = SCAN,
rho_a.shape=(6, 14920), rho_b.shape=(6, 14920)
fxc_a.shape=(14920,), fxc_b.shape=(14920,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 14920), fxc.shape=(14920,)
get_data, dm shape = (2, 66, 66)
ao_eval.shape=(4, 14920, 66)
rho shape (4, 4, 2, 14920)
rho_filt shape: (14920,)
get descriptors tdrho.shape=(2, 14920, 15)
mol:  [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]]
converged SCF energy = -920.005608888959  <S^2> = 4.9027449e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0aada0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0aada0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12208, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00155834 -0.00091688 -0.00215831 ... -0.00091688 -0.41618507
 -0.41618507] = SCAN,
rho_a.shape=(6, 12208), rho_b.shape=(6, 12208)
fxc_a.shape=(12208,), fxc_b.shape=(12208,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12208), fxc.shape=(12208,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12208, 60)
rho shape (4, 4, 2, 12208)
rho_filt shape: (12208,)
get descriptors tdrho.shape=(2, 12208, 15)
mol:  [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]]
converged SCF energy = -199.394370591172  <S^2> = 1.2612134e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0aadd0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0aadd0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9824, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.92948614e-04 -1.95198688e-05 -1.16699802e-03 ... -4.89378340e-01
 -4.89378340e-01 -4.89378340e-01] = SCAN,
rho_a.shape=(6, 9824), rho_b.shape=(6, 9824)
fxc_a.shape=(9824,), fxc_b.shape=(9824,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9824), fxc.shape=(9824,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9824, 44)
rho shape (4, 4, 2, 9824)
rho_filt shape: (9824,)
get descriptors tdrho.shape=(2, 9824, 15)
mol:  [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]]
converged SCF energy = -150.214894490397  <S^2> = 1.0018599  2S+1 = 2.2377309
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0aaa40> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0aaa40> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9912, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.22200160e-04 -1.46866652e-04 -7.59096596e-06 ... -6.59150638e-01
 -6.59150638e-01 -6.59150638e-01] = SCAN,
rho_a.shape=(6, 9912), rho_b.shape=(6, 9912)
fxc_a.shape=(9912,), fxc_b.shape=(9912,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9912), fxc.shape=(9912,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9912, 44)
rho shape (4, 4, 2, 9912)
rho_filt shape: (9912,)
get descriptors tdrho.shape=(2, 9912, 15)
mol:  [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]]
converged SCF energy = -77.2435048346372  <S^2> = 7.9936058e-15  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0ab070> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0ab070> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 58, 58)
ao.shape (10, 15208, 58)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.83270456e-05 -8.83270456e-05 -9.75839850e-04 ... -3.46719667e-05
 -3.31708644e-05 -3.31708644e-05] = SCAN,
rho_a.shape=(6, 15208), rho_b.shape=(6, 15208)
fxc_a.shape=(15208,), fxc_b.shape=(15208,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 15208), fxc.shape=(15208,)
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15208, 58)
rho shape (4, 4, 2, 15208)
rho_filt shape: (15208,)
get descriptors tdrho.shape=(2, 15208, 15)
mol:  [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]]
converged SCF energy = -113.221335689652  <S^2> = 6.5991657e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0aaf80> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0aaf80> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 10040, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-5.37000578e-04 -8.55494549e-04 -2.46853288e-03 ... -7.34251999e-01
 -7.34251999e-01 -7.34251999e-01] = SCAN,
rho_a.shape=(6, 10040), rho_b.shape=(6, 10040)
fxc_a.shape=(10040,), fxc_b.shape=(10040,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 10040), fxc.shape=(10040,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 10040, 44)
rho shape (4, 4, 2, 10040)
rho_filt shape: (10040,)
get descriptors tdrho.shape=(2, 10040, 15)
mol:  [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]]
converged SCF energy = -460.624592374078  <S^2> = 6.5725203e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0aaef0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0aaef0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 37, 37)
ao.shape (10, 8552, 37)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.38161177e-04 -1.81188367e-05 -2.37300299e-05 ... -2.83738108e+00
 -2.83738108e+00 -2.83738108e+00] = SCAN,
rho_a.shape=(6, 8552), rho_b.shape=(6, 8552)
fxc_a.shape=(8552,), fxc_b.shape=(8552,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 8552), fxc.shape=(8552,)
get_data, dm shape = (2, 37, 37)
ao_eval.shape=(4, 8552, 37)
rho shape (4, 4, 2, 8552)
rho_filt shape: (8552,)
get descriptors tdrho.shape=(2, 8552, 15)
mol:  [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]]
converged SCF energy = -8.04458854018922  <S^2> = 7.6827433e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0ab220> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0ab220> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 29, 29)
ao.shape (10, 6936, 29)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00113445 -0.00118624 -0.00118624 ... -0.48434639 -0.48434639
 -0.48434639] = SCAN,
rho_a.shape=(6, 6936), rho_b.shape=(6, 6936)
fxc_a.shape=(6936,), fxc_b.shape=(6936,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 6936), fxc.shape=(6936,)
get_data, dm shape = (2, 29, 29)
ao_eval.shape=(4, 6936, 29)
rho shape (4, 4, 2, 6936)
rho_filt shape: (6936,)
get descriptors tdrho.shape=(2, 6936, 15)
mol:  [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]]
converged SCF energy = -324.340512506578  <S^2> = 1.5864643e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0ab430> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0ab430> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 11536, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00297935 -0.00297935 -0.00407089 ... -0.00297935 -0.00297935
 -0.00407089] = SCAN,
rho_a.shape=(6, 11536), rho_b.shape=(6, 11536)
fxc_a.shape=(11536,), fxc_b.shape=(11536,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 11536), fxc.shape=(11536,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 11536, 60)
rho shape (4, 4, 2, 11536)
rho_filt shape: (11536,)
get descriptors tdrho.shape=(2, 11536, 15)
mol:  [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]]
converged SCF energy = -1622.57507845814  <S^2> = 8.31335e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0ab1c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0ab1c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 120, 120)
ao.shape (10, 24512, 120)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.61400996e-04 -4.90484900e-04 -2.56451718e-03 ... -9.59296113e+00
 -9.59296113e+00 -9.59296113e+00] = SCAN,
rho_a.shape=(6, 24512), rho_b.shape=(6, 24512)
fxc_a.shape=(24512,), fxc_b.shape=(24512,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 24512), fxc.shape=(24512,)
get_data, dm shape = (2, 120, 120)
ao_eval.shape=(4, 24512, 120)
rho shape (4, 4, 2, 24512)
rho_filt shape: (24512,)
get descriptors tdrho.shape=(2, 24512, 15)
mol:  [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]]
converged SCF energy = -342.979728469576  <S^2> = 2.5391245e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0ab6d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0ab6d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 13096, 51)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.28637920e-03 -4.32383300e-04 -3.74057272e-05 ... -1.91722770e+00
 -1.91722770e+00 -1.91722770e+00] = SCAN,
rho_a.shape=(6, 13096), rho_b.shape=(6, 13096)
fxc_a.shape=(13096,), fxc_b.shape=(13096,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 13096), fxc.shape=(13096,)
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 13096, 51)
rho shape (4, 4, 2, 13096)
rho_filt shape: (13096,)
get descriptors tdrho.shape=(2, 13096, 15)
mol:  [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]]
converged SCF energy = -578.565336680792  <S^2> = 1.0034708  2S+1 = 2.2391702
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0ab370> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0ab370> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12384, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.60030106e-04 -2.60763242e-04 -2.59345009e-04 ... -3.86943856e-01
 -3.86943856e-01 -3.86943856e-01] = SCAN,
rho_a.shape=(6, 12384), rho_b.shape=(6, 12384)
fxc_a.shape=(12384,), fxc_b.shape=(12384,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12384), fxc.shape=(12384,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12384, 60)
rho shape (4, 4, 2, 12384)
rho_filt shape: (12384,)
get descriptors tdrho.shape=(2, 12384, 15)
mol:  [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]]
converged SCF energy = -40.4598214864075  <S^2> = 3.2152059e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0ab820> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0ab820> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 50, 50)
ao.shape (10, 13936, 50)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.68439986e-04 -2.42462569e-04 -1.69927031e-05 ... -2.55230307e-05
 -2.55230307e-05 -2.55230307e-05] = SCAN,
rho_a.shape=(6, 13936), rho_b.shape=(6, 13936)
fxc_a.shape=(13936,), fxc_b.shape=(13936,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 13936), fxc.shape=(13936,)
get_data, dm shape = (2, 50, 50)
ao_eval.shape=(4, 13936, 50)
rho shape (4, 4, 2, 13936)
rho_filt shape: (13936,)
get descriptors tdrho.shape=(2, 13936, 15)
mol:  [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]]
converged SCF energy = -39.0756147483504  <S^2> = 6.1985972e-12  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0abb50> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0abb50> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 36, 36)
ao.shape (10, 9656, 36)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-7.67688751e-04 -4.57393214e-05 -2.02834191e-04 ... -1.14928924e+00
 -1.14928924e+00 -1.14928924e+00] = SCAN,
rho_a.shape=(6, 9656), rho_b.shape=(6, 9656)
fxc_a.shape=(9656,), fxc_b.shape=(9656,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9656), fxc.shape=(9656,)
get_data, dm shape = (2, 36, 36)
ao_eval.shape=(4, 9656, 36)
rho shape (4, 4, 2, 9656)
rho_filt shape: (9656,)
get descriptors tdrho.shape=(2, 9656, 15)
mol:  [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]]
converged SCF energy = -291.719272437818  <S^2> = 1.3148593e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
New DM shape: (2, 58, 58)
ao.shape (10, 15256, 58)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.33850535e-04 -2.34903029e-04 -1.75623665e-05 ... -1.92891112e-05
 -1.92891112e-05 -1.92891112e-05] = SCAN,
rho_a.shape=(6, 15256), rho_b.shape=(6, 15256)
fxc_a.shape=(15256,), fxc_b.shape=(15256,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 15256), fxc.shape=(15256,)
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15256, 58)
rho shape (4, 4, 2, 15256)
rho_filt shape: (15256,)
get descriptors tdrho.shape=(2, 15256, 15)
localnet.spin_scaling: concatenating the data
first data shape = (10940, 15)
concatenated: tdrho.shape=(271711, 15)
PRE NAN FILT: tFxc.shape=(271711,), tdrho.shape=(271711, 15)
nan_filt_rho.shape=(271711,)
nan_filt_fxc.shape=(271711,)
tFxc.shape=(271711,), tdrho.shape=(271711, 15)
inp[0].shape = (271711, 15)
Epoch 0
Epoch 0 :: Batch 0/1
Batch Loss = 3699743.2558365576
0, epoch_train_loss=3699743.2558365576
Epoch 1
Epoch 1 :: Batch 0/1
Batch Loss = 1177019.576781283
1, epoch_train_loss=1177019.576781283
Epoch 2
Epoch 2 :: Batch 0/1
Batch Loss = 40306.67906831451
2, epoch_train_loss=40306.67906831451
Epoch 3
Epoch 3 :: Batch 0/1
Batch Loss = 121.68337334688559
3, epoch_train_loss=121.68337334688559
Epoch 4
Epoch 4 :: Batch 0/1
Batch Loss = 4.9895876559841605
4, epoch_train_loss=4.9895876559841605
Epoch 5
Epoch 5 :: Batch 0/1
Batch Loss = 131749.7147468941
5, epoch_train_loss=131749.7147468941
Epoch 6
Epoch 6 :: Batch 0/1
Batch Loss = 4.892774506721112
6, epoch_train_loss=4.892774506721112
Epoch 7
Epoch 7 :: Batch 0/1
Batch Loss = 4.887172154077316
7, epoch_train_loss=4.887172154077316
Epoch 8
Epoch 8 :: Batch 0/1
Batch Loss = 4.87666419893181
8, epoch_train_loss=4.87666419893181
Epoch 9
Epoch 9 :: Batch 0/1
Batch Loss = 4.86456249666527
9, epoch_train_loss=4.86456249666527
Epoch 10
Epoch 10 :: Batch 0/1
Batch Loss = 6.116837298646306
10, epoch_train_loss=6.116837298646306
Epoch 11
Epoch 11 :: Batch 0/1
Batch Loss = 4.84949714390274
11, epoch_train_loss=4.84949714390274
Epoch 12
Epoch 12 :: Batch 0/1
Batch Loss = 1225.160344363706
12, epoch_train_loss=1225.160344363706
Epoch 13
Epoch 13 :: Batch 0/1
Batch Loss = 4.841874421135796
13, epoch_train_loss=4.841874421135796
Epoch 14
Epoch 14 :: Batch 0/1
Batch Loss = 95746.56003026351
14, epoch_train_loss=95746.56003026351
Epoch 15
Epoch 15 :: Batch 0/1
Batch Loss = 4.829492153071134
15, epoch_train_loss=4.829492153071134
Epoch 16
Epoch 16 :: Batch 0/1
Batch Loss = 4.822976854624891
16, epoch_train_loss=4.822976854624891
Epoch 17
Epoch 17 :: Batch 0/1
Batch Loss = 4.815536275442883
17, epoch_train_loss=4.815536275442883
Epoch 18
Epoch 18 :: Batch 0/1
Batch Loss = 4.807145731448691
18, epoch_train_loss=4.807145731448691
Epoch 19
Epoch 19 :: Batch 0/1
Batch Loss = 4.798322996860483
19, epoch_train_loss=4.798322996860483
Epoch 20
Epoch 20 :: Batch 0/1
Batch Loss = 4.793805317699544
20, epoch_train_loss=4.793805317699544
Epoch 21
Epoch 21 :: Batch 0/1
Batch Loss = 4.786190435160839
21, epoch_train_loss=4.786190435160839
Epoch 22
Epoch 22 :: Batch 0/1
Batch Loss = 4.778255030039568
22, epoch_train_loss=4.778255030039568
Epoch 23
Epoch 23 :: Batch 0/1
Batch Loss = 4.772361173939514
23, epoch_train_loss=4.772361173939514
Epoch 24
Epoch 24 :: Batch 0/1
Batch Loss = 4.766833326460552
24, epoch_train_loss=4.766833326460552
Epoch 25
Epoch 25 :: Batch 0/1
Batch Loss = 4.760734841743288
25, epoch_train_loss=4.760734841743288
Epoch 26
Epoch 26 :: Batch 0/1
Batch Loss = 4.753627268624399
26, epoch_train_loss=4.753627268624399
Epoch 27
Epoch 27 :: Batch 0/1
Batch Loss = 4.746353344981553
27, epoch_train_loss=4.746353344981553
Epoch 28
Epoch 28 :: Batch 0/1
Batch Loss = 4.743257944321003
28, epoch_train_loss=4.743257944321003
Epoch 29
Epoch 29 :: Batch 0/1
Batch Loss = 4.735489577097866
29, epoch_train_loss=4.735489577097866
Epoch 30
Epoch 30 :: Batch 0/1
Batch Loss = 4.729648354120268
30, epoch_train_loss=4.729648354120268
Epoch 31
Epoch 31 :: Batch 0/1
Batch Loss = 4.725362132601068
31, epoch_train_loss=4.725362132601068
Epoch 32
Epoch 32 :: Batch 0/1
Batch Loss = 4.7194884674146325
32, epoch_train_loss=4.7194884674146325
Epoch 33
Epoch 33 :: Batch 0/1
Batch Loss = 4.712513275293906
33, epoch_train_loss=4.712513275293906
Epoch 34
Epoch 34 :: Batch 0/1
Batch Loss = 4.709370743204591
34, epoch_train_loss=4.709370743204591
Epoch 35
Epoch 35 :: Batch 0/1
Batch Loss = 4.701593312332152
35, epoch_train_loss=4.701593312332152
Epoch 36
Epoch 36 :: Batch 0/1
Batch Loss = 4.697139220087985
36, epoch_train_loss=4.697139220087985
Epoch 37
Epoch 37 :: Batch 0/1
Batch Loss = 4.691830451286655
37, epoch_train_loss=4.691830451286655
Epoch 38
Epoch 38 :: Batch 0/1
Batch Loss = 4.684869332205356
38, epoch_train_loss=4.684869332205356
Epoch 39
Epoch 39 :: Batch 0/1
Batch Loss = 4.682295015545981
39, epoch_train_loss=4.682295015545981
Epoch 40
Epoch 40 :: Batch 0/1
Batch Loss = 4.676177567518402
40, epoch_train_loss=4.676177567518402
Epoch 41
Epoch 41 :: Batch 0/1
Batch Loss = 4.6729896460553535
41, epoch_train_loss=4.6729896460553535
Epoch 42
Epoch 42 :: Batch 0/1
Batch Loss = 4.66545385536515
42, epoch_train_loss=4.66545385536515
Epoch 43
Epoch 43 :: Batch 0/1
Batch Loss = 400.89839694540206
43, epoch_train_loss=400.89839694540206
Epoch 44
Epoch 44 :: Batch 0/1
Batch Loss = 4.683487253900072
44, epoch_train_loss=4.683487253900072
Epoch 45
Epoch 45 :: Batch 0/1
Batch Loss = 4.691822317199304
45, epoch_train_loss=4.691822317199304
Epoch 46
Epoch 46 :: Batch 0/1
Batch Loss = 4.69116320609
46, epoch_train_loss=4.69116320609
Epoch 47
Epoch 47 :: Batch 0/1
Batch Loss = 4.688645158884864
47, epoch_train_loss=4.688645158884864
Epoch 48
Epoch 48 :: Batch 0/1
Batch Loss = 4.685625130220342
48, epoch_train_loss=4.685625130220342
Epoch 49
Epoch 49 :: Batch 0/1
Batch Loss = 4.682452340049806
49, epoch_train_loss=4.682452340049806
Epoch 50
Epoch 50 :: Batch 0/1
Batch Loss = 4.679242109174091
50, epoch_train_loss=4.679242109174091
Epoch 51
Epoch 51 :: Batch 0/1
Batch Loss = 4.6760370907223185
51, epoch_train_loss=4.6760370907223185
Epoch 52
Epoch 52 :: Batch 0/1
Batch Loss = 4.672853878420872
52, epoch_train_loss=4.672853878420872
Epoch 53
Epoch 53 :: Batch 0/1
Batch Loss = 4.669698848279109
53, epoch_train_loss=4.669698848279109
Epoch 54
Epoch 54 :: Batch 0/1
Batch Loss = 4.66657419035305
54, epoch_train_loss=4.66657419035305
Epoch 55
Epoch 55 :: Batch 0/1
Batch Loss = 4.663480264481601
55, epoch_train_loss=4.663480264481601
Epoch 56
Epoch 56 :: Batch 0/1
Batch Loss = 4.660416627249632
56, epoch_train_loss=4.660416627249632
Epoch 57
Epoch 57 :: Batch 0/1
Batch Loss = 4.6573825079198015
57, epoch_train_loss=4.6573825079198015
Epoch 58
Epoch 58 :: Batch 0/1
Batch Loss = 4.654377035996442
58, epoch_train_loss=4.654377035996442
Epoch 59
Epoch 59 :: Batch 0/1
Batch Loss = 4.651399348544531
59, epoch_train_loss=4.651399348544531
Epoch 60
Epoch 60 :: Batch 0/1
Batch Loss = 4.6484486415001305
60, epoch_train_loss=4.6484486415001305
Epoch 61
Epoch 61 :: Batch 0/1
Batch Loss = 4.645524190327415
61, epoch_train_loss=4.645524190327415
Epoch 62
Epoch 62 :: Batch 0/1
Batch Loss = 4.642625357165707
62, epoch_train_loss=4.642625357165707
Epoch 63
Epoch 63 :: Batch 0/1
Batch Loss = 4.639751588370885
63, epoch_train_loss=4.639751588370885
Epoch 64
Epoch 64 :: Batch 0/1
Batch Loss = 4.636902410646138
64, epoch_train_loss=4.636902410646138
Epoch 65
Epoch 65 :: Batch 0/1
Batch Loss = 4.634077424282984
65, epoch_train_loss=4.634077424282984
Epoch 66
Epoch 66 :: Batch 0/1
Batch Loss = 4.631276296926799
66, epoch_train_loss=4.631276296926799
Epoch 67
Epoch 67 :: Batch 0/1
Batch Loss = 4.628498756081895
67, epoch_train_loss=4.628498756081895
Epoch 68
Epoch 68 :: Batch 0/1
Batch Loss = 4.625744582750696
68, epoch_train_loss=4.625744582750696
Epoch 69
Epoch 69 :: Batch 0/1
Batch Loss = 4.623013605274793
69, epoch_train_loss=4.623013605274793
Epoch 70
Epoch 70 :: Batch 0/1
Batch Loss = 4.620305693441498
70, epoch_train_loss=4.620305693441498
Epoch 71
Epoch 71 :: Batch 0/1
Batch Loss = 4.617620753874326
71, epoch_train_loss=4.617620753874326
Epoch 72
Epoch 72 :: Batch 0/1
Batch Loss = 4.61495872466934
72, epoch_train_loss=4.61495872466934
Epoch 73
Epoch 73 :: Batch 0/1
Batch Loss = 4.612319571088243
73, epoch_train_loss=4.612319571088243
Epoch 74
Epoch 74 :: Batch 0/1
Batch Loss = 4.609703282243602
74, epoch_train_loss=4.609703282243602
Epoch 75
Epoch 75 :: Batch 0/1
Batch Loss = 4.607109866975548
75, epoch_train_loss=4.607109866975548
Epoch 76
Epoch 76 :: Batch 0/1
Batch Loss = 4.604539350678355
76, epoch_train_loss=4.604539350678355
Epoch 77
Epoch 77 :: Batch 0/1
Batch Loss = 4.60199177282303
77, epoch_train_loss=4.60199177282303
Epoch 78
Epoch 78 :: Batch 0/1
Batch Loss = 4.59946718437408
78, epoch_train_loss=4.59946718437408
Epoch 79
Epoch 79 :: Batch 0/1
Batch Loss = 4.596965645097942
79, epoch_train_loss=4.596965645097942
Epoch 80
Epoch 80 :: Batch 0/1
Batch Loss = 4.594487221499701
80, epoch_train_loss=4.594487221499701
Epoch 81
Epoch 81 :: Batch 0/1
Batch Loss = 4.592031985525195
81, epoch_train_loss=4.592031985525195
Epoch 82
Epoch 82 :: Batch 0/1
Batch Loss = 4.589600012165606
82, epoch_train_loss=4.589600012165606
Epoch 83
Epoch 83 :: Batch 0/1
Batch Loss = 4.58719137871426
83, epoch_train_loss=4.58719137871426
Epoch 84
Epoch 84 :: Batch 0/1
Batch Loss = 4.584806162727892
84, epoch_train_loss=4.584806162727892
Epoch 85
Epoch 85 :: Batch 0/1
Batch Loss = 4.582444441405293
85, epoch_train_loss=4.582444441405293
Epoch 86
Epoch 86 :: Batch 0/1
Batch Loss = 4.5801062903746725
86, epoch_train_loss=4.5801062903746725
Epoch 87
Epoch 87 :: Batch 0/1
Batch Loss = 4.577791782957005
87, epoch_train_loss=4.577791782957005
Epoch 88
Epoch 88 :: Batch 0/1
Batch Loss = 4.575500988848555
88, epoch_train_loss=4.575500988848555
Epoch 89
Epoch 89 :: Batch 0/1
Batch Loss = 4.573233973945361
89, epoch_train_loss=4.573233973945361
Epoch 90
Epoch 90 :: Batch 0/1
Batch Loss = 4.570990799727533
90, epoch_train_loss=4.570990799727533
Epoch 91
Epoch 91 :: Batch 0/1
Batch Loss = 4.568771522230804
91, epoch_train_loss=4.568771522230804
Epoch 92
Epoch 92 :: Batch 0/1
Batch Loss = 4.566576192102611
92, epoch_train_loss=4.566576192102611
Epoch 93
Epoch 93 :: Batch 0/1
Batch Loss = 4.564404854049064
93, epoch_train_loss=4.564404854049064
Epoch 94
Epoch 94 :: Batch 0/1
Batch Loss = 4.56225754667238
94, epoch_train_loss=4.56225754667238
Epoch 95
Epoch 95 :: Batch 0/1
Batch Loss = 4.560134301721767
95, epoch_train_loss=4.560134301721767
Epoch 96
Epoch 96 :: Batch 0/1
Batch Loss = 4.55803514436145
96, epoch_train_loss=4.55803514436145
Epoch 97
Epoch 97 :: Batch 0/1
Batch Loss = 4.555960092835617
97, epoch_train_loss=4.555960092835617
Epoch 98
Epoch 98 :: Batch 0/1
Batch Loss = 4.553909158336713
98, epoch_train_loss=4.553909158336713
Epoch 99
Epoch 99 :: Batch 0/1
Batch Loss = 4.551882344910234
99, epoch_train_loss=4.551882344910234
Epoch 100
Epoch 100 :: Batch 0/1
Batch Loss = 4.549879649392426
100, epoch_train_loss=4.549879649392426
Epoch 101
Epoch 101 :: Batch 0/1
Batch Loss = 4.547901061377526
101, epoch_train_loss=4.547901061377526
Epoch 102
Epoch 102 :: Batch 0/1
Batch Loss = 4.545946563211524
102, epoch_train_loss=4.545946563211524
Epoch 103
Epoch 103 :: Batch 0/1
Batch Loss = 4.544016130009697
103, epoch_train_loss=4.544016130009697
Epoch 104
Epoch 104 :: Batch 0/1
Batch Loss = 4.542109729695386
104, epoch_train_loss=4.542109729695386
Epoch 105
Epoch 105 :: Batch 0/1
Batch Loss = 4.5402273229178025
105, epoch_train_loss=4.5402273229178025
Epoch 106
Epoch 106 :: Batch 0/1
Batch Loss = 4.53836886354964
106, epoch_train_loss=4.53836886354964
Epoch 107
Epoch 107 :: Batch 0/1
Batch Loss = 4.536534298350615
107, epoch_train_loss=4.536534298350615
Epoch 108
Epoch 108 :: Batch 0/1
Batch Loss = 4.5347235672133595
108, epoch_train_loss=4.5347235672133595
Epoch 109
Epoch 109 :: Batch 0/1
Batch Loss = 4.53293660314512
109, epoch_train_loss=4.53293660314512
Epoch 110
Epoch 110 :: Batch 0/1
Batch Loss = 4.531173332794246
110, epoch_train_loss=4.531173332794246
Epoch 111
Epoch 111 :: Batch 0/1
Batch Loss = 4.529433676179276
111, epoch_train_loss=4.529433676179276
Epoch 112
Epoch 112 :: Batch 0/1
Batch Loss = 4.5277175468387645
112, epoch_train_loss=4.5277175468387645
Epoch 113
Epoch 113 :: Batch 0/1
Batch Loss = 4.526024852365363
113, epoch_train_loss=4.526024852365363
Epoch 114
Epoch 114 :: Batch 0/1
Batch Loss = 4.524355494170964
114, epoch_train_loss=4.524355494170964
Epoch 115
Epoch 115 :: Batch 0/1
Batch Loss = 4.522709367776056
115, epoch_train_loss=4.522709367776056
Epoch 116
Epoch 116 :: Batch 0/1
Batch Loss = 4.521086362972524
116, epoch_train_loss=4.521086362972524
Epoch 117
Epoch 117 :: Batch 0/1
Batch Loss = 4.5194863638707545
117, epoch_train_loss=4.5194863638707545
Epoch 118
Epoch 118 :: Batch 0/1
Batch Loss = 4.51790924931092
118, epoch_train_loss=4.51790924931092
Epoch 119
Epoch 119 :: Batch 0/1
Batch Loss = 4.516354893025662
119, epoch_train_loss=4.516354893025662
Epoch 120
Epoch 120 :: Batch 0/1
Batch Loss = 4.514823163462554
120, epoch_train_loss=4.514823163462554
Epoch 121
Epoch 121 :: Batch 0/1
Batch Loss = 4.513313924421911
121, epoch_train_loss=4.513313924421911
Epoch 122
Epoch 122 :: Batch 0/1
Batch Loss = 4.511827034772371
122, epoch_train_loss=4.511827034772371
Epoch 123
Epoch 123 :: Batch 0/1
Batch Loss = 4.51036234907584
123, epoch_train_loss=4.51036234907584
Epoch 124
Epoch 124 :: Batch 0/1
Batch Loss = 4.508919717316236
124, epoch_train_loss=4.508919717316236
Epoch 125
Epoch 125 :: Batch 0/1
Batch Loss = 4.507498985510222
125, epoch_train_loss=4.507498985510222
Epoch 126
Epoch 126 :: Batch 0/1
Batch Loss = 4.506099995447593
126, epoch_train_loss=4.506099995447593
Epoch 127
Epoch 127 :: Batch 0/1
Batch Loss = 4.504722585184327
127, epoch_train_loss=4.504722585184327
Epoch 128
Epoch 128 :: Batch 0/1
Batch Loss = 4.503366589103352
128, epoch_train_loss=4.503366589103352
Epoch 129
Epoch 129 :: Batch 0/1
Batch Loss = 4.502031838181285
129, epoch_train_loss=4.502031838181285
Epoch 130
Epoch 130 :: Batch 0/1
Batch Loss = 4.500718159853525
130, epoch_train_loss=4.500718159853525
Epoch 131
Epoch 131 :: Batch 0/1
Batch Loss = 4.499425378480874
131, epoch_train_loss=4.499425378480874
Epoch 132
Epoch 132 :: Batch 0/1
Batch Loss = 4.498153315408967
132, epoch_train_loss=4.498153315408967
Epoch 133
Epoch 133 :: Batch 0/1
Batch Loss = 4.496901789127217
133, epoch_train_loss=4.496901789127217
Epoch 134
Epoch 134 :: Batch 0/1
Batch Loss = 4.49567061551685
134, epoch_train_loss=4.49567061551685
Epoch 135
Epoch 135 :: Batch 0/1
Batch Loss = 4.494459607727495
135, epoch_train_loss=4.494459607727495
Epoch 136
Epoch 136 :: Batch 0/1
Batch Loss = 4.493268576608118
136, epoch_train_loss=4.493268576608118
Epoch 137
Epoch 137 :: Batch 0/1
Batch Loss = 4.492097330761357
137, epoch_train_loss=4.492097330761357
Epoch 138
Epoch 138 :: Batch 0/1
Batch Loss = 4.49094567668905
138, epoch_train_loss=4.49094567668905
Epoch 139
Epoch 139 :: Batch 0/1
Batch Loss = 4.489813418934865
139, epoch_train_loss=4.489813418934865
Epoch 140
Epoch 140 :: Batch 0/1
Batch Loss = 4.488700360223965
140, epoch_train_loss=4.488700360223965
Epoch 141
Epoch 141 :: Batch 0/1
Batch Loss = 4.4876063015996435
141, epoch_train_loss=4.4876063015996435
Epoch 142
Epoch 142 :: Batch 0/1
Batch Loss = 4.48653104255695
142, epoch_train_loss=4.48653104255695
Epoch 143
Epoch 143 :: Batch 0/1
Batch Loss = 4.485474381173226
143, epoch_train_loss=4.485474381173226
Epoch 144
Epoch 144 :: Batch 0/1
Batch Loss = 4.484436114235552
144, epoch_train_loss=4.484436114235552
Epoch 145
Epoch 145 :: Batch 0/1
Batch Loss = 4.483416037289216
145, epoch_train_loss=4.483416037289216
Epoch 146
Epoch 146 :: Batch 0/1
Batch Loss = 4.482413945063419
146, epoch_train_loss=4.482413945063419
Epoch 147
Epoch 147 :: Batch 0/1
Batch Loss = 4.481429631058183
147, epoch_train_loss=4.481429631058183
Epoch 148
Epoch 148 :: Batch 0/1
Batch Loss = 4.480462888187401
148, epoch_train_loss=4.480462888187401
Epoch 149
Epoch 149 :: Batch 0/1
Batch Loss = 4.479513508590012
149, epoch_train_loss=4.479513508590012
Epoch 150
Epoch 150 :: Batch 0/1
Batch Loss = 4.478581283814207
150, epoch_train_loss=4.478581283814207
Epoch 151
Epoch 151 :: Batch 0/1
Batch Loss = 4.477666004855102
151, epoch_train_loss=4.477666004855102
Epoch 152
Epoch 152 :: Batch 0/1
Batch Loss = 4.4767674624638305
152, epoch_train_loss=4.4767674624638305
Epoch 153
Epoch 153 :: Batch 0/1
Batch Loss = 4.475885447038781
153, epoch_train_loss=4.475885447038781
Epoch 154
Epoch 154 :: Batch 0/1
Batch Loss = 4.475019748727991
154, epoch_train_loss=4.475019748727991
Epoch 155
Epoch 155 :: Batch 0/1
Batch Loss = 4.47417015771806
155, epoch_train_loss=4.47417015771806
Epoch 156
Epoch 156 :: Batch 0/1
Batch Loss = 4.47333646412763
156, epoch_train_loss=4.47333646412763
Epoch 157
Epoch 157 :: Batch 0/1
Batch Loss = 4.472518458100666
157, epoch_train_loss=4.472518458100666
Epoch 158
Epoch 158 :: Batch 0/1
Batch Loss = 4.471715930075818
158, epoch_train_loss=4.471715930075818
Epoch 159
Epoch 159 :: Batch 0/1
Batch Loss = 4.4709286706819595
159, epoch_train_loss=4.4709286706819595
Epoch 160
Epoch 160 :: Batch 0/1
Batch Loss = 4.470156470822563
160, epoch_train_loss=4.470156470822563
Epoch 161
Epoch 161 :: Batch 0/1
Batch Loss = 4.469399121926165
161, epoch_train_loss=4.469399121926165
Epoch 162
Epoch 162 :: Batch 0/1
Batch Loss = 4.468656415788482
162, epoch_train_loss=4.468656415788482
Epoch 163
Epoch 163 :: Batch 0/1
Batch Loss = 4.467928144815687
163, epoch_train_loss=4.467928144815687
Epoch 164
Epoch 164 :: Batch 0/1
Batch Loss = 4.467214102087251
164, epoch_train_loss=4.467214102087251
Epoch 165
Epoch 165 :: Batch 0/1
Batch Loss = 4.466514081259858
165, epoch_train_loss=4.466514081259858
Epoch 166
Epoch 166 :: Batch 0/1
Batch Loss = 4.465827876843641
166, epoch_train_loss=4.465827876843641
Epoch 167
Epoch 167 :: Batch 0/1
Batch Loss = 4.465155284052039
167, epoch_train_loss=4.465155284052039
Epoch 168
Epoch 168 :: Batch 0/1
Batch Loss = 4.464496099015171
168, epoch_train_loss=4.464496099015171
Epoch 169
Epoch 169 :: Batch 0/1
Batch Loss = 4.4638501187812425
169, epoch_train_loss=4.4638501187812425
Epoch 170
Epoch 170 :: Batch 0/1
Batch Loss = 4.463217141367929
170, epoch_train_loss=4.463217141367929
Epoch 171
Epoch 171 :: Batch 0/1
Batch Loss = 4.46259696585721
171, epoch_train_loss=4.46259696585721
Epoch 172
Epoch 172 :: Batch 0/1
Batch Loss = 4.461989392301248
172, epoch_train_loss=4.461989392301248
Epoch 173
Epoch 173 :: Batch 0/1
Batch Loss = 4.461394221951459
173, epoch_train_loss=4.461394221951459
Epoch 174
Epoch 174 :: Batch 0/1
Batch Loss = 4.460811257073428
174, epoch_train_loss=4.460811257073428
Epoch 175
Epoch 175 :: Batch 0/1
Batch Loss = 4.460240301294615
175, epoch_train_loss=4.460240301294615
Epoch 176
Epoch 176 :: Batch 0/1
Batch Loss = 4.45968115933135
176, epoch_train_loss=4.45968115933135
Epoch 177
Epoch 177 :: Batch 0/1
Batch Loss = 4.459133637196088
177, epoch_train_loss=4.459133637196088
Epoch 178
Epoch 178 :: Batch 0/1
Batch Loss = 4.4585975421826625
178, epoch_train_loss=4.4585975421826625
Epoch 179
Epoch 179 :: Batch 0/1
Batch Loss = 4.458072682892553
179, epoch_train_loss=4.458072682892553
Epoch 180
Epoch 180 :: Batch 0/1
Batch Loss = 4.457558869258286
180, epoch_train_loss=4.457558869258286
Epoch 181
Epoch 181 :: Batch 0/1
Batch Loss = 4.457055912563865
181, epoch_train_loss=4.457055912563865
Epoch 182
Epoch 182 :: Batch 0/1
Batch Loss = 4.456563625462233
182, epoch_train_loss=4.456563625462233
Epoch 183
Epoch 183 :: Batch 0/1
Batch Loss = 4.456081821989771
183, epoch_train_loss=4.456081821989771
Epoch 184
Epoch 184 :: Batch 0/1
Batch Loss = 4.455610317577759
184, epoch_train_loss=4.455610317577759
Epoch 185
Epoch 185 :: Batch 0/1
Batch Loss = 4.45514892906087
185, epoch_train_loss=4.45514892906087
Epoch 186
Epoch 186 :: Batch 0/1
Batch Loss = 4.454697474649044
186, epoch_train_loss=4.454697474649044
Epoch 187
Epoch 187 :: Batch 0/1
Batch Loss = 4.454255774064982
187, epoch_train_loss=4.454255774064982
Epoch 188
Epoch 188 :: Batch 0/1
Batch Loss = 4.453823648308607
188, epoch_train_loss=4.453823648308607
Epoch 189
Epoch 189 :: Batch 0/1
Batch Loss = 4.453400919887492
189, epoch_train_loss=4.453400919887492
Epoch 190
Epoch 190 :: Batch 0/1
Batch Loss = 4.452987412647591
190, epoch_train_loss=4.452987412647591
Epoch 191
Epoch 191 :: Batch 0/1
Batch Loss = 4.452582951892066
191, epoch_train_loss=4.452582951892066
Epoch 192
Epoch 192 :: Batch 0/1
Batch Loss = 4.452187364276713
192, epoch_train_loss=4.452187364276713
Epoch 193
Epoch 193 :: Batch 0/1
Batch Loss = 4.451800477830184
193, epoch_train_loss=4.451800477830184
Epoch 194
Epoch 194 :: Batch 0/1
Batch Loss = 4.451422121941133
194, epoch_train_loss=4.451422121941133
Epoch 195
Epoch 195 :: Batch 0/1
Batch Loss = 4.451052127317193
195, epoch_train_loss=4.451052127317193
Epoch 196
Epoch 196 :: Batch 0/1
Batch Loss = 4.450690326055162
196, epoch_train_loss=4.450690326055162
Epoch 197
Epoch 197 :: Batch 0/1
Batch Loss = 4.450336551517235
197, epoch_train_loss=4.450336551517235
Epoch 198
Epoch 198 :: Batch 0/1
Batch Loss = 4.449990638428177
198, epoch_train_loss=4.449990638428177
Epoch 199
Epoch 199 :: Batch 0/1
Batch Loss = 4.44965242278506
199, epoch_train_loss=4.44965242278506
Epoch 200
Epoch 200 :: Batch 0/1
Batch Loss = 4.449321741853111
200, epoch_train_loss=4.449321741853111
Epoch 201
Epoch 201 :: Batch 0/1
Batch Loss = 4.448998434236416
201, epoch_train_loss=4.448998434236416
Epoch 202
Epoch 202 :: Batch 0/1
Batch Loss = 4.448682339776929
202, epoch_train_loss=4.448682339776929
Epoch 203
Epoch 203 :: Batch 0/1
Batch Loss = 4.448373299628092
203, epoch_train_loss=4.448373299628092
Epoch 204
Epoch 204 :: Batch 0/1
Batch Loss = 4.448071156251457
204, epoch_train_loss=4.448071156251457
Epoch 205
Epoch 205 :: Batch 0/1
Batch Loss = 4.447775753343916
205, epoch_train_loss=4.447775753343916
Epoch 206
Epoch 206 :: Batch 0/1
Batch Loss = 4.447486935895094
206, epoch_train_loss=4.447486935895094
Epoch 207
Epoch 207 :: Batch 0/1
Batch Loss = 4.447204550161415
207, epoch_train_loss=4.447204550161415
Epoch 208
Epoch 208 :: Batch 0/1
Batch Loss = 4.446928443562451
208, epoch_train_loss=4.446928443562451
Epoch 209
Epoch 209 :: Batch 0/1
Batch Loss = 4.446658464687463
209, epoch_train_loss=4.446658464687463
Epoch 210
Epoch 210 :: Batch 0/1
Batch Loss = 4.446394463219175
210, epoch_train_loss=4.446394463219175
Epoch 211
Epoch 211 :: Batch 0/1
Batch Loss = 4.4461362897946675
211, epoch_train_loss=4.4461362897946675
Epoch 212
Epoch 212 :: Batch 0/1
Batch Loss = 4.445883796004712
212, epoch_train_loss=4.445883796004712
Epoch 213
Epoch 213 :: Batch 0/1
Batch Loss = 4.445636834389424
213, epoch_train_loss=4.445636834389424
Epoch 214
Epoch 214 :: Batch 0/1
Batch Loss = 4.445395258497372
214, epoch_train_loss=4.445395258497372
Epoch 215
Epoch 215 :: Batch 0/1
Batch Loss = 4.445158923227805
215, epoch_train_loss=4.445158923227805
Epoch 216
Epoch 216 :: Batch 0/1
Batch Loss = 4.444927685437433
216, epoch_train_loss=4.444927685437433
Epoch 217
Epoch 217 :: Batch 0/1
Batch Loss = 4.4447014046557545
217, epoch_train_loss=4.4447014046557545
Epoch 218
Epoch 218 :: Batch 0/1
Batch Loss = 4.444479944142862
218, epoch_train_loss=4.444479944142862
Epoch 219
Epoch 219 :: Batch 0/1
Batch Loss = 4.444263171717993
219, epoch_train_loss=4.444263171717993
Epoch 220
Epoch 220 :: Batch 0/1
Batch Loss = 4.444050959886836
220, epoch_train_loss=4.444050959886836
Epoch 221
Epoch 221 :: Batch 0/1
Batch Loss = 4.443843184424148
221, epoch_train_loss=4.443843184424148
Epoch 222
Epoch 222 :: Batch 0/1
Batch Loss = 4.443639720583453
222, epoch_train_loss=4.443639720583453
Epoch 223
Epoch 223 :: Batch 0/1
Batch Loss = 4.443440436658681
223, epoch_train_loss=4.443440436658681
Epoch 224
Epoch 224 :: Batch 0/1
Batch Loss = 4.4432451859523345
224, epoch_train_loss=4.4432451859523345
Epoch 225
Epoch 225 :: Batch 0/1
Batch Loss = 4.443053800070992
225, epoch_train_loss=4.443053800070992
Epoch 226
Epoch 226 :: Batch 0/1
Batch Loss = 4.442866087831968
226, epoch_train_loss=4.442866087831968
Epoch 227
Epoch 227 :: Batch 0/1
Batch Loss = 4.442681843304963
227, epoch_train_loss=4.442681843304963
Epoch 228
Epoch 228 :: Batch 0/1
Batch Loss = 4.442500862789821
228, epoch_train_loss=4.442500862789821
Epoch 229
Epoch 229 :: Batch 0/1
Batch Loss = 4.442322964755201
229, epoch_train_loss=4.442322964755201
Epoch 230
Epoch 230 :: Batch 0/1
Batch Loss = 4.442148003387589
230, epoch_train_loss=4.442148003387589
Epoch 231
Epoch 231 :: Batch 0/1
Batch Loss = 4.441975867978488
231, epoch_train_loss=4.441975867978488
Epoch 232
Epoch 232 :: Batch 0/1
Batch Loss = 4.44180646757217
232, epoch_train_loss=4.44180646757217
Epoch 233
Epoch 233 :: Batch 0/1
Batch Loss = 4.441639708280145
233, epoch_train_loss=4.441639708280145
Epoch 234
Epoch 234 :: Batch 0/1
Batch Loss = 4.441475473721809
234, epoch_train_loss=4.441475473721809
Epoch 235
Epoch 235 :: Batch 0/1
Batch Loss = 4.44131361615577
235, epoch_train_loss=4.44131361615577
Epoch 236
Epoch 236 :: Batch 0/1
Batch Loss = 4.441153959292187
236, epoch_train_loss=4.441153959292187
Epoch 237
Epoch 237 :: Batch 0/1
Batch Loss = 4.440996308883688
237, epoch_train_loss=4.440996308883688
Epoch 238
Epoch 238 :: Batch 0/1
Batch Loss = 4.440840465911558
238, epoch_train_loss=4.440840465911558
Epoch 239
Epoch 239 :: Batch 0/1
Batch Loss = 4.440686238463662
239, epoch_train_loss=4.440686238463662
Epoch 240
Epoch 240 :: Batch 0/1
Batch Loss = 4.440533450516759
240, epoch_train_loss=4.440533450516759
Epoch 241
Epoch 241 :: Batch 0/1
Batch Loss = 4.440381947416707
241, epoch_train_loss=4.440381947416707
Epoch 242
Epoch 242 :: Batch 0/1
Batch Loss = 4.440231598265224
242, epoch_train_loss=4.440231598265224
Epoch 243
Epoch 243 :: Batch 0/1
Batch Loss = 4.440082296055193
243, epoch_train_loss=4.440082296055193
Epoch 244
Epoch 244 :: Batch 0/1
Batch Loss = 4.439933956679308
244, epoch_train_loss=4.439933956679308
Epoch 245
Epoch 245 :: Batch 0/1
Batch Loss = 4.4397865182864065
245, epoch_train_loss=4.4397865182864065
Epoch 246
Epoch 246 :: Batch 0/1
Batch Loss = 4.43963994287609
246, epoch_train_loss=4.43963994287609
Epoch 247
Epoch 247 :: Batch 0/1
Batch Loss = 4.439494221113482
247, epoch_train_loss=4.439494221113482
Epoch 248
Epoch 248 :: Batch 0/1
Batch Loss = 4.439349483007543
248, epoch_train_loss=4.439349483007543
Epoch 249
Epoch 249 :: Batch 0/1
Batch Loss = 4.4392052746697015
249, epoch_train_loss=4.4392052746697015
Epoch 250
Epoch 250 :: Batch 0/1
Batch Loss = 4.439063698448056
250, epoch_train_loss=4.439063698448056
Epoch 251
Epoch 251 :: Batch 0/1
Batch Loss = 4.438910450426525
251, epoch_train_loss=4.438910450426525
Epoch 252
Epoch 252 :: Batch 0/1
Batch Loss = 4.438784000941762
252, epoch_train_loss=4.438784000941762
Epoch 253
Epoch 253 :: Batch 0/1
Batch Loss = 4.4386388626038125
253, epoch_train_loss=4.4386388626038125
Epoch 254
Epoch 254 :: Batch 0/1
Batch Loss = 4.4385155877439715
254, epoch_train_loss=4.4385155877439715
Epoch 255
Epoch 255 :: Batch 0/1
Batch Loss = 4.438373041515556
255, epoch_train_loss=4.438373041515556
Epoch 256
Epoch 256 :: Batch 0/1
Batch Loss = 4.438247177053621
256, epoch_train_loss=4.438247177053621
Epoch 257
Epoch 257 :: Batch 0/1
Batch Loss = 4.438115685502971
257, epoch_train_loss=4.438115685502971
Epoch 258
Epoch 258 :: Batch 0/1
Batch Loss = 4.4379916103136665
258, epoch_train_loss=4.4379916103136665
Epoch 259
Epoch 259 :: Batch 0/1
Batch Loss = 4.437872047583913
259, epoch_train_loss=4.437872047583913
Epoch 260
Epoch 260 :: Batch 0/1
Batch Loss = 4.437745632373382
260, epoch_train_loss=4.437745632373382
Epoch 261
Epoch 261 :: Batch 0/1
Batch Loss = 4.437637479861076
261, epoch_train_loss=4.437637479861076
Epoch 262
Epoch 262 :: Batch 0/1
Batch Loss = 4.437512232374354
262, epoch_train_loss=4.437512232374354
Epoch 263
Epoch 263 :: Batch 0/1
Batch Loss = 4.437408139503713
263, epoch_train_loss=4.437408139503713
Epoch 264
Epoch 264 :: Batch 0/1
Batch Loss = 4.437287960260676
264, epoch_train_loss=4.437287960260676
Epoch 265
Epoch 265 :: Batch 0/1
Batch Loss = 4.437183905262787
265, epoch_train_loss=4.437183905262787
Epoch 266
Epoch 266 :: Batch 0/1
Batch Loss = 4.437070766043373
266, epoch_train_loss=4.437070766043373
Epoch 267
Epoch 267 :: Batch 0/1
Batch Loss = 4.4369659573630145
267, epoch_train_loss=4.4369659573630145
Epoch 268
Epoch 268 :: Batch 0/1
Batch Loss = 4.436859421399644
268, epoch_train_loss=4.436859421399644
Epoch 269
Epoch 269 :: Batch 0/1
Batch Loss = 4.436753573816491
269, epoch_train_loss=4.436753573816491
Epoch 270
Epoch 270 :: Batch 0/1
Batch Loss = 4.436651697828218
270, epoch_train_loss=4.436651697828218
Epoch 271
Epoch 271 :: Batch 0/1
Batch Loss = 4.436545929898802
271, epoch_train_loss=4.436545929898802
Epoch 272
Epoch 272 :: Batch 0/1
Batch Loss = 4.436446183592307
272, epoch_train_loss=4.436446183592307
Epoch 273
Epoch 273 :: Batch 0/1
Batch Loss = 4.436341317569456
273, epoch_train_loss=4.436341317569456
Epoch 274
Epoch 274 :: Batch 0/1
Batch Loss = 4.436242533791655
274, epoch_train_loss=4.436242533791655
Epoch 275
Epoch 275 :: Batch 0/1
Batch Loss = 4.436138441328772
275, epoch_train_loss=4.436138441328772
Epoch 276
Epoch 276 :: Batch 0/1
Batch Loss = 4.436039350707352
276, epoch_train_loss=4.436039350707352
Epoch 277
Epoch 277 :: Batch 0/1
Batch Loss = 4.43593640339637
277, epoch_train_loss=4.43593640339637
Epoch 278
Epoch 278 :: Batch 0/1
Batch Loss = 4.435836340618204
278, epoch_train_loss=4.435836340618204
Epoch 279
Epoch 279 :: Batch 0/1
Batch Loss = 4.435734843403367
279, epoch_train_loss=4.435734843403367
Epoch 280
Epoch 280 :: Batch 0/1
Batch Loss = 4.435632616757823
280, epoch_train_loss=4.435632616757823
Epoch 281
Epoch 281 :: Batch 0/1
Batch Loss = 4.435532808600994
281, epoch_train_loss=4.435532808600994
Epoch 282
Epoch 282 :: Batch 0/1
Batch Loss = 4.435428602086908
282, epoch_train_loss=4.435428602086908
Epoch 283
Epoch 283 :: Batch 0/1
Batch Loss = 4.435327694415846
283, epoch_train_loss=4.435327694415846
Epoch 284
Epoch 284 :: Batch 0/1
Batch Loss = 4.4352251720162235
284, epoch_train_loss=4.4352251720162235
Epoch 285
Epoch 285 :: Batch 0/1
Batch Loss = 4.435117834627955
285, epoch_train_loss=4.435117834627955
Epoch 286
Epoch 286 :: Batch 0/1
Batch Loss = 4.435009942647144
286, epoch_train_loss=4.435009942647144
Epoch 287
Epoch 287 :: Batch 0/1
Batch Loss = 4.43490211827096
287, epoch_train_loss=4.43490211827096
Epoch 288
Epoch 288 :: Batch 0/1
Batch Loss = 4.434791337233564
288, epoch_train_loss=4.434791337233564
Epoch 289
Epoch 289 :: Batch 0/1
Batch Loss = 4.4346742502936465
289, epoch_train_loss=4.4346742502936465
Epoch 290
Epoch 290 :: Batch 0/1
Batch Loss = 4.43455389451239
290, epoch_train_loss=4.43455389451239
Epoch 291
Epoch 291 :: Batch 0/1
Batch Loss = 4.434426856576574
291, epoch_train_loss=4.434426856576574
Epoch 292
Epoch 292 :: Batch 0/1
Batch Loss = 4.434296149748185
292, epoch_train_loss=4.434296149748185
Epoch 293
Epoch 293 :: Batch 0/1
Batch Loss = 4.434157911534018
293, epoch_train_loss=4.434157911534018
Epoch 294
Epoch 294 :: Batch 0/1
Batch Loss = 4.434010470069025
294, epoch_train_loss=4.434010470069025
Epoch 295
Epoch 295 :: Batch 0/1
Batch Loss = 4.433853165335989
295, epoch_train_loss=4.433853165335989
Epoch 296
Epoch 296 :: Batch 0/1
Batch Loss = 4.43367634767434
296, epoch_train_loss=4.43367634767434
Epoch 297
Epoch 297 :: Batch 0/1
Batch Loss = 4.4334922418989
297, epoch_train_loss=4.4334922418989
Epoch 298
Epoch 298 :: Batch 0/1
Batch Loss = 4.433298132448219
298, epoch_train_loss=4.433298132448219
Epoch 299
Epoch 299 :: Batch 0/1
Batch Loss = 4.433137517368497
299, epoch_train_loss=4.433137517368497
Epoch 300
Epoch 300 :: Batch 0/1
Batch Loss = 4.432901007975127
300, epoch_train_loss=4.432901007975127
Epoch 301
Epoch 301 :: Batch 0/1
Batch Loss = 4.43269953641908
301, epoch_train_loss=4.43269953641908
Epoch 302
Epoch 302 :: Batch 0/1
Batch Loss = 4.4324731633250165
302, epoch_train_loss=4.4324731633250165
Epoch 303
Epoch 303 :: Batch 0/1
Batch Loss = 4.432223055328396
303, epoch_train_loss=4.432223055328396
Epoch 304
Epoch 304 :: Batch 0/1
Batch Loss = 4.431955376663027
304, epoch_train_loss=4.431955376663027
Epoch 305
Epoch 305 :: Batch 0/1
Batch Loss = 4.4316793556458975
305, epoch_train_loss=4.4316793556458975
Epoch 306
Epoch 306 :: Batch 0/1
Batch Loss = 4.431405990565526
306, epoch_train_loss=4.431405990565526
Epoch 307
Epoch 307 :: Batch 0/1
Batch Loss = 4.431144290040514
307, epoch_train_loss=4.431144290040514
Epoch 308
Epoch 308 :: Batch 0/1
Batch Loss = 4.430893826369256
308, epoch_train_loss=4.430893826369256
Epoch 309
Epoch 309 :: Batch 0/1
Batch Loss = 4.430650107892807
309, epoch_train_loss=4.430650107892807
Epoch 310
Epoch 310 :: Batch 0/1
Batch Loss = 4.430431518175165
310, epoch_train_loss=4.430431518175165
Epoch 311
Epoch 311 :: Batch 0/1
Batch Loss = 4.430232489761166
311, epoch_train_loss=4.430232489761166
Epoch 312
Epoch 312 :: Batch 0/1
Batch Loss = 4.4299805003333015
312, epoch_train_loss=4.4299805003333015
Epoch 313
Epoch 313 :: Batch 0/1
Batch Loss = 4.429663262876607
313, epoch_train_loss=4.429663262876607
Epoch 314
Epoch 314 :: Batch 0/1
Batch Loss = 4.429288717274844
314, epoch_train_loss=4.429288717274844
Epoch 315
Epoch 315 :: Batch 0/1
Batch Loss = 4.428854259461915
315, epoch_train_loss=4.428854259461915
Epoch 316
Epoch 316 :: Batch 0/1
Batch Loss = 4.4284009541672305
316, epoch_train_loss=4.4284009541672305
Epoch 317
Epoch 317 :: Batch 0/1
Batch Loss = 4.427959218257796
317, epoch_train_loss=4.427959218257796
Epoch 318
Epoch 318 :: Batch 0/1
Batch Loss = 4.427492100758616
318, epoch_train_loss=4.427492100758616
Epoch 319
Epoch 319 :: Batch 0/1
Batch Loss = 4.4270102996418075
319, epoch_train_loss=4.4270102996418075
Epoch 320
Epoch 320 :: Batch 0/1
Batch Loss = 4.426457590063054
320, epoch_train_loss=4.426457590063054
Epoch 321
Epoch 321 :: Batch 0/1
Batch Loss = 4.425787037658988
321, epoch_train_loss=4.425787037658988
Epoch 322
Epoch 322 :: Batch 0/1
Batch Loss = 20.928004518204375
322, epoch_train_loss=20.928004518204375
Epoch 323
Epoch 323 :: Batch 0/1
Batch Loss = 4.4270418960989115
323, epoch_train_loss=4.4270418960989115
Epoch 324
Epoch 324 :: Batch 0/1
Batch Loss = 4.42567431078055
324, epoch_train_loss=4.42567431078055
Epoch 325
Epoch 325 :: Batch 0/1
Batch Loss = 4.425859953179911
325, epoch_train_loss=4.425859953179911
Epoch 326
Epoch 326 :: Batch 0/1
Batch Loss = 4.424780595749624
326, epoch_train_loss=4.424780595749624
Epoch 327
Epoch 327 :: Batch 0/1
Batch Loss = 4.421934498709171
327, epoch_train_loss=4.421934498709171
Epoch 328
Epoch 328 :: Batch 0/1
Batch Loss = 4.417568339605395
328, epoch_train_loss=4.417568339605395
Epoch 329
Epoch 329 :: Batch 0/1
Batch Loss = 4.415706969377575
329, epoch_train_loss=4.415706969377575
Epoch 330
Epoch 330 :: Batch 0/1
Batch Loss = 4.421974545363781
330, epoch_train_loss=4.421974545363781
Epoch 331
Epoch 331 :: Batch 0/1
Batch Loss = 4.416493480926442
331, epoch_train_loss=4.416493480926442
Epoch 332
Epoch 332 :: Batch 0/1
Batch Loss = 4.421696179205323
332, epoch_train_loss=4.421696179205323
Epoch 333
Epoch 333 :: Batch 0/1
Batch Loss = 4.422143000241407
333, epoch_train_loss=4.422143000241407
Epoch 334
Epoch 334 :: Batch 0/1
Batch Loss = 4.418060317788332
334, epoch_train_loss=4.418060317788332
Epoch 335
Epoch 335 :: Batch 0/1
Batch Loss = 4.4184101254551535
335, epoch_train_loss=4.4184101254551535
Epoch 336
Epoch 336 :: Batch 0/1
Batch Loss = 4.4181358815307785
336, epoch_train_loss=4.4181358815307785
Epoch 337
Epoch 337 :: Batch 0/1
Batch Loss = 4.416608999796563
337, epoch_train_loss=4.416608999796563
Epoch 338
Epoch 338 :: Batch 0/1
Batch Loss = 4.418739578060846
338, epoch_train_loss=4.418739578060846
Epoch 339
Epoch 339 :: Batch 0/1
Batch Loss = 4.41695009041379
339, epoch_train_loss=4.41695009041379
Epoch 340
Epoch 340 :: Batch 0/1
Batch Loss = 4.413557207851824
340, epoch_train_loss=4.413557207851824
Epoch 341
Epoch 341 :: Batch 0/1
Batch Loss = 4.417854421881183
341, epoch_train_loss=4.417854421881183
Epoch 342
Epoch 342 :: Batch 0/1
Batch Loss = 4.413319434165096
342, epoch_train_loss=4.413319434165096
Epoch 343
Epoch 343 :: Batch 0/1
Batch Loss = 4.415582162405307
343, epoch_train_loss=4.415582162405307
Epoch 344
Epoch 344 :: Batch 0/1
Batch Loss = 4.415498352368168
344, epoch_train_loss=4.415498352368168
Epoch 345
Epoch 345 :: Batch 0/1
Batch Loss = 4.413004304925689
345, epoch_train_loss=4.413004304925689
Epoch 346
Epoch 346 :: Batch 0/1
Batch Loss = 4.4138138857481595
346, epoch_train_loss=4.4138138857481595
Epoch 347
Epoch 347 :: Batch 0/1
Batch Loss = 4.412382112105437
347, epoch_train_loss=4.412382112105437
Epoch 348
Epoch 348 :: Batch 0/1
Batch Loss = 4.411696262858261
348, epoch_train_loss=4.411696262858261
Epoch 349
Epoch 349 :: Batch 0/1
Batch Loss = 4.412099385195506
349, epoch_train_loss=4.412099385195506
Epoch 350
Epoch 350 :: Batch 0/1
Batch Loss = 4.410030886834343
350, epoch_train_loss=4.410030886834343
Epoch 351
Epoch 351 :: Batch 0/1
Batch Loss = 4.41016289509871
351, epoch_train_loss=4.41016289509871
Epoch 352
Epoch 352 :: Batch 0/1
Batch Loss = 4.408266024879288
352, epoch_train_loss=4.408266024879288
Epoch 353
Epoch 353 :: Batch 0/1
Batch Loss = 4.4080330871980165
353, epoch_train_loss=4.4080330871980165
Epoch 354
Epoch 354 :: Batch 0/1
Batch Loss = 4.407017533045034
354, epoch_train_loss=4.407017533045034
Epoch 355
Epoch 355 :: Batch 0/1
Batch Loss = 4.405756597882425
355, epoch_train_loss=4.405756597882425
Epoch 356
Epoch 356 :: Batch 0/1
Batch Loss = 4.404909314455715
356, epoch_train_loss=4.404909314455715
Epoch 357
Epoch 357 :: Batch 0/1
Batch Loss = 4.404123866405383
357, epoch_train_loss=4.404123866405383
Epoch 358
Epoch 358 :: Batch 0/1
Batch Loss = 4.4029658130888105
358, epoch_train_loss=4.4029658130888105
Epoch 359
Epoch 359 :: Batch 0/1
Batch Loss = 4.402243765873807
359, epoch_train_loss=4.402243765873807
Epoch 360
Epoch 360 :: Batch 0/1
Batch Loss = 4.400532219660672
360, epoch_train_loss=4.400532219660672
Epoch 361
Epoch 361 :: Batch 0/1
Batch Loss = 4.399987669240419
361, epoch_train_loss=4.399987669240419
Epoch 362
Epoch 362 :: Batch 0/1
Batch Loss = 4.398565177708195
362, epoch_train_loss=4.398565177708195
Epoch 363
Epoch 363 :: Batch 0/1
Batch Loss = 4.3975123102654745
363, epoch_train_loss=4.3975123102654745
Epoch 364
Epoch 364 :: Batch 0/1
Batch Loss = 4.3978641822784965
364, epoch_train_loss=4.3978641822784965
Epoch 365
Epoch 365 :: Batch 0/1
Batch Loss = 4.855563571877567
365, epoch_train_loss=4.855563571877567
Epoch 366
Epoch 366 :: Batch 0/1
Batch Loss = 4.435540499144627
366, epoch_train_loss=4.435540499144627
Epoch 367
Epoch 367 :: Batch 0/1
Batch Loss = 4.437296277437395
367, epoch_train_loss=4.437296277437395
Epoch 368
Epoch 368 :: Batch 0/1
Batch Loss = 4.4374622762690334
368, epoch_train_loss=4.4374622762690334
Epoch 369
Epoch 369 :: Batch 0/1
Batch Loss = 4.437492403192177
369, epoch_train_loss=4.437492403192177
Epoch 370
Epoch 370 :: Batch 0/1
Batch Loss = 4.437503302503435
370, epoch_train_loss=4.437503302503435
Epoch 371
Epoch 371 :: Batch 0/1
Batch Loss = 4.4375085327759125
371, epoch_train_loss=4.4375085327759125
Epoch 372
Epoch 372 :: Batch 0/1
Batch Loss = 4.43751223318477
372, epoch_train_loss=4.43751223318477
Epoch 373
Epoch 373 :: Batch 0/1
Batch Loss = 4.437515759528019
373, epoch_train_loss=4.437515759528019
Epoch 374
Epoch 374 :: Batch 0/1
Batch Loss = 4.4375196166869015
374, epoch_train_loss=4.4375196166869015
Epoch 375
Epoch 375 :: Batch 0/1
Batch Loss = 4.4375241450174014
375, epoch_train_loss=4.4375241450174014
Epoch 376
Epoch 376 :: Batch 0/1
Batch Loss = 4.437529653940997
376, epoch_train_loss=4.437529653940997
Epoch 377
Epoch 377 :: Batch 0/1
Batch Loss = 4.437536346411799
377, epoch_train_loss=4.437536346411799
Epoch 378
Epoch 378 :: Batch 0/1
Batch Loss = 4.437544094688751
378, epoch_train_loss=4.437544094688751
Epoch 379
Epoch 379 :: Batch 0/1
Batch Loss = 4.437552152669181
379, epoch_train_loss=4.437552152669181
Epoch 380
Epoch 380 :: Batch 0/1
Batch Loss = 4.437559052014162
380, epoch_train_loss=4.437559052014162
Epoch 381
Epoch 381 :: Batch 0/1
Batch Loss = 4.437562963451778
381, epoch_train_loss=4.437562963451778
Epoch 382
Epoch 382 :: Batch 0/1
Batch Loss = 4.437562558449885
382, epoch_train_loss=4.437562558449885
Epoch 383
Epoch 383 :: Batch 0/1
Batch Loss = 4.437557751987264
383, epoch_train_loss=4.437557751987264
Epoch 384
Epoch 384 :: Batch 0/1
Batch Loss = 4.437549705922859
384, epoch_train_loss=4.437549705922859
Epoch 385
Epoch 385 :: Batch 0/1
Batch Loss = 4.437540125655149
385, epoch_train_loss=4.437540125655149
Epoch 386
Epoch 386 :: Batch 0/1
Batch Loss = 4.437530464794513
386, epoch_train_loss=4.437530464794513
Epoch 387
Epoch 387 :: Batch 0/1
Batch Loss = 4.437521558856186
387, epoch_train_loss=4.437521558856186
Epoch 388
Epoch 388 :: Batch 0/1
Batch Loss = 4.437513693534635
388, epoch_train_loss=4.437513693534635
Epoch 389
Epoch 389 :: Batch 0/1
Batch Loss = 4.4375068347416855
389, epoch_train_loss=4.4375068347416855
Epoch 390
Epoch 390 :: Batch 0/1
Batch Loss = 4.437500822510164
390, epoch_train_loss=4.437500822510164
Epoch 391
Epoch 391 :: Batch 0/1
Batch Loss = 4.437495479265988
391, epoch_train_loss=4.437495479265988
Epoch 392
Epoch 392 :: Batch 0/1
Batch Loss = 4.437490653500361
392, epoch_train_loss=4.437490653500361
Epoch 393
Epoch 393 :: Batch 0/1
Batch Loss = 4.437486229341856
393, epoch_train_loss=4.437486229341856
Epoch 394
Epoch 394 :: Batch 0/1
Batch Loss = 4.437482122516377
394, epoch_train_loss=4.437482122516377
Epoch 395
Epoch 395 :: Batch 0/1
Batch Loss = 4.437478272897505
395, epoch_train_loss=4.437478272897505
Epoch 396
Epoch 396 :: Batch 0/1
Batch Loss = 4.437474637623296
396, epoch_train_loss=4.437474637623296
Epoch 397
Epoch 397 :: Batch 0/1
Batch Loss = 4.437471185821958
397, epoch_train_loss=4.437471185821958
Epoch 398
Epoch 398 :: Batch 0/1
Batch Loss = 4.437467894878108
398, epoch_train_loss=4.437467894878108
Epoch 399
Epoch 399 :: Batch 0/1
Batch Loss = 4.437464747880498
399, epoch_train_loss=4.437464747880498
Epoch 400
Epoch 400 :: Batch 0/1
Batch Loss = 4.437461731898636
400, epoch_train_loss=4.437461731898636
Epoch 401
Epoch 401 :: Batch 0/1
Batch Loss = 4.437458836816298
401, epoch_train_loss=4.437458836816298
Epoch 402
Epoch 402 :: Batch 0/1
Batch Loss = 4.437456054535627
402, epoch_train_loss=4.437456054535627
Epoch 403
Epoch 403 :: Batch 0/1
Batch Loss = 4.437453378424925
403, epoch_train_loss=4.437453378424925
Epoch 404
Epoch 404 :: Batch 0/1
Batch Loss = 4.437450802932095
404, epoch_train_loss=4.437450802932095
Epoch 405
Epoch 405 :: Batch 0/1
Batch Loss = 4.437448323308749
405, epoch_train_loss=4.437448323308749
Epoch 406
Epoch 406 :: Batch 0/1
Batch Loss = 4.43744593541073
406, epoch_train_loss=4.43744593541073
Epoch 407
Epoch 407 :: Batch 0/1
Batch Loss = 4.437443635552867
407, epoch_train_loss=4.437443635552867
Epoch 408
Epoch 408 :: Batch 0/1
Batch Loss = 4.437441420402571
408, epoch_train_loss=4.437441420402571
Epoch 409
Epoch 409 :: Batch 0/1
Batch Loss = 4.437439286899845
409, epoch_train_loss=4.437439286899845
Epoch 410
Epoch 410 :: Batch 0/1
Batch Loss = 4.437437232198936
410, epoch_train_loss=4.437437232198936
Epoch 411
Epoch 411 :: Batch 0/1
Batch Loss = 4.437435253623594
411, epoch_train_loss=4.437435253623594
Epoch 412
Epoch 412 :: Batch 0/1
Batch Loss = 4.437433348634586
412, epoch_train_loss=4.437433348634586
Epoch 413
Epoch 413 :: Batch 0/1
Batch Loss = 4.437431514804909
413, epoch_train_loss=4.437431514804909
Epoch 414
Epoch 414 :: Batch 0/1
Batch Loss = 4.437429749801429
414, epoch_train_loss=4.437429749801429
Epoch 415
Epoch 415 :: Batch 0/1
Batch Loss = 4.43742805137125
415, epoch_train_loss=4.43742805137125
Epoch 416
Epoch 416 :: Batch 0/1
Batch Loss = 4.437426417331767
416, epoch_train_loss=4.437426417331767
Epoch 417
Epoch 417 :: Batch 0/1
Batch Loss = 4.437424845562907
417, epoch_train_loss=4.437424845562907
Epoch 418
Epoch 418 :: Batch 0/1
Batch Loss = 4.437423334002148
418, epoch_train_loss=4.437423334002148
Epoch 419
Epoch 419 :: Batch 0/1
Batch Loss = 4.437421880640527
419, epoch_train_loss=4.437421880640527
Epoch 420
Epoch 420 :: Batch 0/1
Batch Loss = 4.437420483519999
420, epoch_train_loss=4.437420483519999
Epoch 421
Epoch 421 :: Batch 0/1
Batch Loss = 4.437419140731187
421, epoch_train_loss=4.437419140731187
Epoch 422
Epoch 422 :: Batch 0/1
Batch Loss = 4.437417850412375
422, epoch_train_loss=4.437417850412375
Epoch 423
Epoch 423 :: Batch 0/1
Batch Loss = 4.4374166107484765
423, epoch_train_loss=4.4374166107484765
Epoch 424
Epoch 424 :: Batch 0/1
Batch Loss = 4.437415419970388
424, epoch_train_loss=4.437415419970388
Epoch 425
Epoch 425 :: Batch 0/1
Batch Loss = 4.437414276354545
425, epoch_train_loss=4.437414276354545
Epoch 426
Epoch 426 :: Batch 0/1
Batch Loss = 4.437413178222582
426, epoch_train_loss=4.437413178222582
Epoch 427
Epoch 427 :: Batch 0/1
Batch Loss = 4.43741212394104
427, epoch_train_loss=4.43741212394104
Epoch 428
Epoch 428 :: Batch 0/1
Batch Loss = 4.437411111921103
428, epoch_train_loss=4.437411111921103
Epoch 429
Epoch 429 :: Batch 0/1
Batch Loss = 4.437410140618242
429, epoch_train_loss=4.437410140618242
Epoch 430
Epoch 430 :: Batch 0/1
Batch Loss = 4.437409208532134
430, epoch_train_loss=4.437409208532134
Epoch 431
Epoch 431 :: Batch 0/1
Batch Loss = 4.437408314206066
431, epoch_train_loss=4.437408314206066
Epoch 432
Epoch 432 :: Batch 0/1
Batch Loss = 4.437407456226615
432, epoch_train_loss=4.437407456226615
Epoch 433
Epoch 433 :: Batch 0/1
Batch Loss = 4.437406633223181
433, epoch_train_loss=4.437406633223181
Epoch 434
Epoch 434 :: Batch 0/1
Batch Loss = 4.437405843867416
434, epoch_train_loss=4.437405843867416
Epoch 435
Epoch 435 :: Batch 0/1
Batch Loss = 4.4374050868728245
435, epoch_train_loss=4.4374050868728245
Epoch 436
Epoch 436 :: Batch 0/1
Batch Loss = 4.437404360993962
436, epoch_train_loss=4.437404360993962
Epoch 437
Epoch 437 :: Batch 0/1
Batch Loss = 4.4374036650258155
437, epoch_train_loss=4.4374036650258155
Epoch 438
Epoch 438 :: Batch 0/1
Batch Loss = 4.4374029978030975
438, epoch_train_loss=4.4374029978030975
Epoch 439
Epoch 439 :: Batch 0/1
Batch Loss = 4.437402358199483
439, epoch_train_loss=4.437402358199483
Epoch 440
Epoch 440 :: Batch 0/1
Batch Loss = 4.437401745126779
440, epoch_train_loss=4.437401745126779
Epoch 441
Epoch 441 :: Batch 0/1
Batch Loss = 4.437401157534238
441, epoch_train_loss=4.437401157534238
Epoch 442
Epoch 442 :: Batch 0/1
Batch Loss = 4.437400594407534
442, epoch_train_loss=4.437400594407534
Epoch 443
Epoch 443 :: Batch 0/1
Batch Loss = 4.43740005476804
443, epoch_train_loss=4.43740005476804
Epoch 444
Epoch 444 :: Batch 0/1
Batch Loss = 4.43739953767193
444, epoch_train_loss=4.43739953767193
Epoch 445
Epoch 445 :: Batch 0/1
Batch Loss = 4.4373990422091785
445, epoch_train_loss=4.4373990422091785
Epoch 446
Epoch 446 :: Batch 0/1
Batch Loss = 4.437398567502815
446, epoch_train_loss=4.437398567502815
Epoch 447
Epoch 447 :: Batch 0/1
Batch Loss = 4.437398112707907
447, epoch_train_loss=4.437398112707907
Epoch 448
Epoch 448 :: Batch 0/1
Batch Loss = 4.437397677010659
448, epoch_train_loss=4.437397677010659
Epoch 449
Epoch 449 :: Batch 0/1
Batch Loss = 4.437397259627616
449, epoch_train_loss=4.437397259627616
Epoch 450
Epoch 450 :: Batch 0/1
Batch Loss = 4.437396859804633
450, epoch_train_loss=4.437396859804633
Epoch 451
Epoch 451 :: Batch 0/1
Batch Loss = 4.437396476816083
451, epoch_train_loss=4.437396476816083
Epoch 452
Epoch 452 :: Batch 0/1
Batch Loss = 4.437396109963936
452, epoch_train_loss=4.437396109963936
Epoch 453
Epoch 453 :: Batch 0/1
Batch Loss = 4.437395758576928
453, epoch_train_loss=4.437395758576928
Epoch 454
Epoch 454 :: Batch 0/1
Batch Loss = 4.437395422009611
454, epoch_train_loss=4.437395422009611
Epoch 455
Epoch 455 :: Batch 0/1
Batch Loss = 4.437395099641593
455, epoch_train_loss=4.437395099641593
Epoch 456
Epoch 456 :: Batch 0/1
Batch Loss = 4.437394790876712
456, epoch_train_loss=4.437394790876712
Epoch 457
Epoch 457 :: Batch 0/1
Batch Loss = 4.437394495142139
457, epoch_train_loss=4.437394495142139
Epoch 458
Epoch 458 :: Batch 0/1
Batch Loss = 4.437394211887664
458, epoch_train_loss=4.437394211887664
Epoch 459
Epoch 459 :: Batch 0/1
Batch Loss = 4.4373939405849105
459, epoch_train_loss=4.4373939405849105
Epoch 460
Epoch 460 :: Batch 0/1
Batch Loss = 4.437393680726489
460, epoch_train_loss=4.437393680726489
Epoch 461
Epoch 461 :: Batch 0/1
Batch Loss = 4.437393431825381
461, epoch_train_loss=4.437393431825381
Epoch 462
Epoch 462 :: Batch 0/1
Batch Loss = 4.437393193414129
462, epoch_train_loss=4.437393193414129
Epoch 463
Epoch 463 :: Batch 0/1
Batch Loss = 4.437392965044179
463, epoch_train_loss=4.437392965044179
Epoch 464
Epoch 464 :: Batch 0/1
Batch Loss = 4.437392746285128
464, epoch_train_loss=4.437392746285128
Epoch 465
Epoch 465 :: Batch 0/1
Batch Loss = 4.437392536724121
465, epoch_train_loss=4.437392536724121
Epoch 466
Epoch 466 :: Batch 0/1
Batch Loss = 4.437392335965171
466, epoch_train_loss=4.437392335965171
Epoch 467
Epoch 467 :: Batch 0/1
Batch Loss = 4.437392143628585
467, epoch_train_loss=4.437392143628585
Epoch 468
Epoch 468 :: Batch 0/1
Batch Loss = 4.437391959350264
468, epoch_train_loss=4.437391959350264
Epoch 469
Epoch 469 :: Batch 0/1
Batch Loss = 4.437391782781166
469, epoch_train_loss=4.437391782781166
Epoch 470
Epoch 470 :: Batch 0/1
Batch Loss = 4.437391613586754
470, epoch_train_loss=4.437391613586754
Epoch 471
Epoch 471 :: Batch 0/1
Batch Loss = 4.437391451446413
471, epoch_train_loss=4.437391451446413
Epoch 472
Epoch 472 :: Batch 0/1
Batch Loss = 4.437391296052881
472, epoch_train_loss=4.437391296052881
Epoch 473
Epoch 473 :: Batch 0/1
Batch Loss = 4.4373911471118035
473, epoch_train_loss=4.4373911471118035
Epoch 474
Epoch 474 :: Batch 0/1
Batch Loss = 4.437391004341183
474, epoch_train_loss=4.437391004341183
Epoch 475
Epoch 475 :: Batch 0/1
Batch Loss = 4.4373908674709055
475, epoch_train_loss=4.4373908674709055
Epoch 476
Epoch 476 :: Batch 0/1
Batch Loss = 4.437390736242265
476, epoch_train_loss=4.437390736242265
Epoch 477
Epoch 477 :: Batch 0/1
Batch Loss = 4.437390610407544
477, epoch_train_loss=4.437390610407544
Epoch 478
Epoch 478 :: Batch 0/1
Batch Loss = 4.437390489729542
478, epoch_train_loss=4.437390489729542
Epoch 479
Epoch 479 :: Batch 0/1
Batch Loss = 4.43739037398116
479, epoch_train_loss=4.43739037398116
Epoch 480
Epoch 480 :: Batch 0/1
Batch Loss = 4.4373902629450255
480, epoch_train_loss=4.4373902629450255
Epoch 481
Epoch 481 :: Batch 0/1
Batch Loss = 4.4373901564130644
481, epoch_train_loss=4.4373901564130644
Epoch 482
Epoch 482 :: Batch 0/1
Batch Loss = 4.437390054186137
482, epoch_train_loss=4.437390054186137
Epoch 483
Epoch 483 :: Batch 0/1
Batch Loss = 4.437389956073694
483, epoch_train_loss=4.437389956073694
Epoch 484
Epoch 484 :: Batch 0/1
Batch Loss = 4.437389861893401
484, epoch_train_loss=4.437389861893401
Epoch 485
Epoch 485 :: Batch 0/1
Batch Loss = 4.437389771470806
485, epoch_train_loss=4.437389771470806
Epoch 486
Epoch 486 :: Batch 0/1
Batch Loss = 4.437389684639038
486, epoch_train_loss=4.437389684639038
Epoch 487
Epoch 487 :: Batch 0/1
Batch Loss = 4.437389601238468
487, epoch_train_loss=4.437389601238468
Epoch 488
Epoch 488 :: Batch 0/1
Batch Loss = 4.437389521116422
488, epoch_train_loss=4.437389521116422
Epoch 489
Epoch 489 :: Batch 0/1
Batch Loss = 4.437389444126901
489, epoch_train_loss=4.437389444126901
Epoch 490
Epoch 490 :: Batch 0/1
Batch Loss = 4.437389370130279
490, epoch_train_loss=4.437389370130279
Epoch 491
Epoch 491 :: Batch 0/1
Batch Loss = 4.437389298993067
491, epoch_train_loss=4.437389298993067
Epoch 492
Epoch 492 :: Batch 0/1
Batch Loss = 4.437389230587626
492, epoch_train_loss=4.437389230587626
Epoch 493
Epoch 493 :: Batch 0/1
Batch Loss = 4.437389164791945
493, epoch_train_loss=4.437389164791945
Epoch 494
Epoch 494 :: Batch 0/1
Batch Loss = 4.437389101489393
494, epoch_train_loss=4.437389101489393
Epoch 495
Epoch 495 :: Batch 0/1
Batch Loss = 4.437389040568492
495, epoch_train_loss=4.437389040568492
Epoch 496
Epoch 496 :: Batch 0/1
Batch Loss = 4.437388981922703
496, epoch_train_loss=4.437388981922703
Epoch 497
Epoch 497 :: Batch 0/1
Batch Loss = 4.437388925450201
497, epoch_train_loss=4.437388925450201
Epoch 498
Epoch 498 :: Batch 0/1
Batch Loss = 4.437388871053706
498, epoch_train_loss=4.437388871053706
Epoch 499
Epoch 499 :: Batch 0/1
Batch Loss = 4.437388818640253
499, epoch_train_loss=4.437388818640253
Epoch 500
Epoch 500 :: Batch 0/1
Batch Loss = 4.4373887681210284
500, epoch_train_loss=4.4373887681210284
Epoch 501
Epoch 501 :: Batch 0/1
Batch Loss = 4.437388719411187
501, epoch_train_loss=4.437388719411187
Epoch 502
Epoch 502 :: Batch 0/1
Batch Loss = 4.4373886724296785
502, epoch_train_loss=4.4373886724296785
Epoch 503
Epoch 503 :: Batch 0/1
Batch Loss = 4.437388627099089
503, epoch_train_loss=4.437388627099089
Epoch 504
Epoch 504 :: Batch 0/1
Batch Loss = 4.43738858334547
504, epoch_train_loss=4.43738858334547
Epoch 505
Epoch 505 :: Batch 0/1
Batch Loss = 4.43738854109821
505, epoch_train_loss=4.43738854109821
Epoch 506
Epoch 506 :: Batch 0/1
Batch Loss = 4.437388500289866
506, epoch_train_loss=4.437388500289866
Epoch 507
Epoch 507 :: Batch 0/1
Batch Loss = 4.437388460856044
507, epoch_train_loss=4.437388460856044
Epoch 508
Epoch 508 :: Batch 0/1
Batch Loss = 4.437388422735252
508, epoch_train_loss=4.437388422735252
Epoch 509
Epoch 509 :: Batch 0/1
Batch Loss = 4.437388385868787
509, epoch_train_loss=4.437388385868787
Epoch 510
Epoch 510 :: Batch 0/1
Batch Loss = 4.437388350200589
510, epoch_train_loss=4.437388350200589
Epoch 511
Epoch 511 :: Batch 0/1
Batch Loss = 4.437388315677153
511, epoch_train_loss=4.437388315677153
Epoch 512
Epoch 512 :: Batch 0/1
Batch Loss = 4.437388282247398
512, epoch_train_loss=4.437388282247398
Epoch 513
Epoch 513 :: Batch 0/1
Batch Loss = 4.437388249862564
513, epoch_train_loss=4.437388249862564
Epoch 514
Epoch 514 :: Batch 0/1
Batch Loss = 4.437388218476106
514, epoch_train_loss=4.437388218476106
Epoch 515
Epoch 515 :: Batch 0/1
Batch Loss = 4.437388188043607
515, epoch_train_loss=4.437388188043607
Epoch 516
Epoch 516 :: Batch 0/1
Batch Loss = 4.437388158522665
516, epoch_train_loss=4.437388158522665
Epoch 517
Epoch 517 :: Batch 0/1
Batch Loss = 4.437388129872816
517, epoch_train_loss=4.437388129872816
Epoch 518
Epoch 518 :: Batch 0/1
Batch Loss = 4.437388102055447
518, epoch_train_loss=4.437388102055447
Epoch 519
Epoch 519 :: Batch 0/1
Batch Loss = 4.437388075033707
519, epoch_train_loss=4.437388075033707
Epoch 520
Epoch 520 :: Batch 0/1
Batch Loss = 4.437388048772432
520, epoch_train_loss=4.437388048772432
Epoch 521
Epoch 521 :: Batch 0/1
Batch Loss = 4.437388023238065
521, epoch_train_loss=4.437388023238065
Epoch 522
Epoch 522 :: Batch 0/1
Batch Loss = 4.437387998398589
522, epoch_train_loss=4.437387998398589
Epoch 523
Epoch 523 :: Batch 0/1
Batch Loss = 4.4373879742234505
523, epoch_train_loss=4.4373879742234505
Epoch 524
Epoch 524 :: Batch 0/1
Batch Loss = 4.437387950683494
524, epoch_train_loss=4.437387950683494
Epoch 525
Epoch 525 :: Batch 0/1
Batch Loss = 4.437387927750908
525, epoch_train_loss=4.437387927750908
Epoch 526
Epoch 526 :: Batch 0/1
Batch Loss = 4.437387905399154
526, epoch_train_loss=4.437387905399154
Epoch 527
Epoch 527 :: Batch 0/1
Batch Loss = 4.437387883602906
527, epoch_train_loss=4.437387883602906
Epoch 528
Epoch 528 :: Batch 0/1
Batch Loss = 4.437387862338003
528, epoch_train_loss=4.437387862338003
Epoch 529
Epoch 529 :: Batch 0/1
Batch Loss = 4.437387841581393
529, epoch_train_loss=4.437387841581393
Epoch 530
Epoch 530 :: Batch 0/1
Batch Loss = 4.437387821311078
530, epoch_train_loss=4.437387821311078
Epoch 531
Epoch 531 :: Batch 0/1
Batch Loss = 4.4373878015060715
531, epoch_train_loss=4.4373878015060715
Epoch 532
Epoch 532 :: Batch 0/1
Batch Loss = 4.43738778214634
532, epoch_train_loss=4.43738778214634
Epoch 533
Epoch 533 :: Batch 0/1
Batch Loss = 4.4373877632127785
533, epoch_train_loss=4.4373877632127785
Epoch 534
Epoch 534 :: Batch 0/1
Batch Loss = 4.437387744687148
534, epoch_train_loss=4.437387744687148
Epoch 535
Epoch 535 :: Batch 0/1
Batch Loss = 4.437387726552044
535, epoch_train_loss=4.437387726552044
Epoch 536
Epoch 536 :: Batch 0/1
Batch Loss = 4.437387708790856
536, epoch_train_loss=4.437387708790856
Epoch 537
Epoch 537 :: Batch 0/1
Batch Loss = 4.437387691387734
537, epoch_train_loss=4.437387691387734
Epoch 538
Epoch 538 :: Batch 0/1
Batch Loss = 4.4373876743275416
538, epoch_train_loss=4.4373876743275416
Epoch 539
Epoch 539 :: Batch 0/1
Batch Loss = 4.4373876575958375
539, epoch_train_loss=4.4373876575958375
Epoch 540
Epoch 540 :: Batch 0/1
Batch Loss = 4.43738764117883
540, epoch_train_loss=4.43738764117883
Epoch 541
Epoch 541 :: Batch 0/1
Batch Loss = 4.437387625063351
541, epoch_train_loss=4.437387625063351
Epoch 542
Epoch 542 :: Batch 0/1
Batch Loss = 4.437387609236826
542, epoch_train_loss=4.437387609236826
Epoch 543
Epoch 543 :: Batch 0/1
Batch Loss = 4.437387593687247
543, epoch_train_loss=4.437387593687247
Epoch 544
Epoch 544 :: Batch 0/1
Batch Loss = 4.437387578403147
544, epoch_train_loss=4.437387578403147
Epoch 545
Epoch 545 :: Batch 0/1
Batch Loss = 4.43738756337356
545, epoch_train_loss=4.43738756337356
Epoch 546
Epoch 546 :: Batch 0/1
Batch Loss = 4.4373875485880205
546, epoch_train_loss=4.4373875485880205
Epoch 547
Epoch 547 :: Batch 0/1
Batch Loss = 4.43738753403652
547, epoch_train_loss=4.43738753403652
Epoch 548
Epoch 548 :: Batch 0/1
Batch Loss = 4.437387519709489
548, epoch_train_loss=4.437387519709489
Epoch 549
Epoch 549 :: Batch 0/1
Batch Loss = 4.437387505597778
549, epoch_train_loss=4.437387505597778
Epoch 550
Epoch 550 :: Batch 0/1
Batch Loss = 4.437387491692641
550, epoch_train_loss=4.437387491692641
Epoch 551
Epoch 551 :: Batch 0/1
Batch Loss = 4.4373874779857
551, epoch_train_loss=4.4373874779857
Epoch 552
Epoch 552 :: Batch 0/1
Batch Loss = 4.437387464468952
552, epoch_train_loss=4.437387464468952
Epoch 553
Epoch 553 :: Batch 0/1
Batch Loss = 4.437387451134721
553, epoch_train_loss=4.437387451134721
Epoch 554
Epoch 554 :: Batch 0/1
Batch Loss = 4.437387437975662
554, epoch_train_loss=4.437387437975662
Epoch 555
Epoch 555 :: Batch 0/1
Batch Loss = 4.437387424984737
555, epoch_train_loss=4.437387424984737
Epoch 556
Epoch 556 :: Batch 0/1
Batch Loss = 4.437387412155202
556, epoch_train_loss=4.437387412155202
Epoch 557
Epoch 557 :: Batch 0/1
Batch Loss = 4.437387399480584
557, epoch_train_loss=4.437387399480584
Epoch 558
Epoch 558 :: Batch 0/1
Batch Loss = 4.4373873869546765
558, epoch_train_loss=4.4373873869546765
Epoch 559
Epoch 559 :: Batch 0/1
Batch Loss = 4.437387374571519
559, epoch_train_loss=4.437387374571519
Epoch 560
Epoch 560 :: Batch 0/1
Batch Loss = 4.437387362325391
560, epoch_train_loss=4.437387362325391
Epoch 561
Epoch 561 :: Batch 0/1
Batch Loss = 4.437387350210789
561, epoch_train_loss=4.437387350210789
Epoch 562
Epoch 562 :: Batch 0/1
Batch Loss = 4.437387338222421
562, epoch_train_loss=4.437387338222421
Epoch 563
Epoch 563 :: Batch 0/1
Batch Loss = 4.4373873263551955
563, epoch_train_loss=4.4373873263551955
Epoch 564
Epoch 564 :: Batch 0/1
Batch Loss = 4.437387314604208
564, epoch_train_loss=4.437387314604208
Epoch 565
Epoch 565 :: Batch 0/1
Batch Loss = 4.437387302964727
565, epoch_train_loss=4.437387302964727
Epoch 566
Epoch 566 :: Batch 0/1
Batch Loss = 4.437387291432192
566, epoch_train_loss=4.437387291432192
Epoch 567
Epoch 567 :: Batch 0/1
Batch Loss = 4.437387280002197
567, epoch_train_loss=4.437387280002197
Epoch 568
Epoch 568 :: Batch 0/1
Batch Loss = 4.437387268670482
568, epoch_train_loss=4.437387268670482
Epoch 569
Epoch 569 :: Batch 0/1
Batch Loss = 4.437387257432926
569, epoch_train_loss=4.437387257432926
Epoch 570
Epoch 570 :: Batch 0/1
Batch Loss = 4.437387246285536
570, epoch_train_loss=4.437387246285536
Epoch 571
Epoch 571 :: Batch 0/1
Batch Loss = 4.437387235224442
571, epoch_train_loss=4.437387235224442
Epoch 572
Epoch 572 :: Batch 0/1
Batch Loss = 4.437387224245882
572, epoch_train_loss=4.437387224245882
Epoch 573
Epoch 573 :: Batch 0/1
Batch Loss = 4.437387213346202
573, epoch_train_loss=4.437387213346202
Epoch 574
Epoch 574 :: Batch 0/1
Batch Loss = 4.437387202521846
574, epoch_train_loss=4.437387202521846
Epoch 575
Epoch 575 :: Batch 0/1
Batch Loss = 4.437387191769343
575, epoch_train_loss=4.437387191769343
Epoch 576
Epoch 576 :: Batch 0/1
Batch Loss = 4.437387181085306
576, epoch_train_loss=4.437387181085306
Epoch 577
Epoch 577 :: Batch 0/1
Batch Loss = 4.437387170466429
577, epoch_train_loss=4.437387170466429
Epoch 578
Epoch 578 :: Batch 0/1
Batch Loss = 4.437387159909465
578, epoch_train_loss=4.437387159909465
Epoch 579
Epoch 579 :: Batch 0/1
Batch Loss = 4.43738714941124
579, epoch_train_loss=4.43738714941124
Epoch 580
Epoch 580 :: Batch 0/1
Batch Loss = 4.437387138968627
580, epoch_train_loss=4.437387138968627
Epoch 581
Epoch 581 :: Batch 0/1
Batch Loss = 4.437387128578554
581, epoch_train_loss=4.437387128578554
Epoch 582
Epoch 582 :: Batch 0/1
Batch Loss = 4.437387118237993
582, epoch_train_loss=4.437387118237993
Epoch 583
Epoch 583 :: Batch 0/1
Batch Loss = 4.437387107943952
583, epoch_train_loss=4.437387107943952
Epoch 584
Epoch 584 :: Batch 0/1
Batch Loss = 4.437387097693472
584, epoch_train_loss=4.437387097693472
Epoch 585
Epoch 585 :: Batch 0/1
Batch Loss = 4.437387087483621
585, epoch_train_loss=4.437387087483621
Epoch 586
Epoch 586 :: Batch 0/1
Batch Loss = 4.43738707731149
586, epoch_train_loss=4.43738707731149
Epoch 587
Epoch 587 :: Batch 0/1
Batch Loss = 4.4373870671741855
587, epoch_train_loss=4.4373870671741855
Epoch 588
Epoch 588 :: Batch 0/1
Batch Loss = 4.437387057068821
588, epoch_train_loss=4.437387057068821
Epoch 589
Epoch 589 :: Batch 0/1
Batch Loss = 4.4373870469925185
589, epoch_train_loss=4.4373870469925185
Epoch 590
Epoch 590 :: Batch 0/1
Batch Loss = 4.437387036942405
590, epoch_train_loss=4.437387036942405
Epoch 591
Epoch 591 :: Batch 0/1
Batch Loss = 4.4373870269155935
591, epoch_train_loss=4.4373870269155935
Epoch 592
Epoch 592 :: Batch 0/1
Batch Loss = 4.437387016909195
592, epoch_train_loss=4.437387016909195
Epoch 593
Epoch 593 :: Batch 0/1
Batch Loss = 4.437387006920301
593, epoch_train_loss=4.437387006920301
Epoch 594
Epoch 594 :: Batch 0/1
Batch Loss = 4.437386996945988
594, epoch_train_loss=4.437386996945988
Epoch 595
Epoch 595 :: Batch 0/1
Batch Loss = 4.437386986983304
595, epoch_train_loss=4.437386986983304
Epoch 596
Epoch 596 :: Batch 0/1
Batch Loss = 4.437386977029273
596, epoch_train_loss=4.437386977029273
Epoch 597
Epoch 597 :: Batch 0/1
Batch Loss = 4.43738696708088
597, epoch_train_loss=4.43738696708088
Epoch 598
Epoch 598 :: Batch 0/1
Batch Loss = 4.437386957135076
598, epoch_train_loss=4.437386957135076
Epoch 599
Epoch 599 :: Batch 0/1
Batch Loss = 4.437386947188769
599, epoch_train_loss=4.437386947188769
Epoch 600
Epoch 600 :: Batch 0/1
Batch Loss = 4.437386937238818
600, epoch_train_loss=4.437386937238818
Epoch 601
Epoch 601 :: Batch 0/1
Batch Loss = 4.437386927282032
601, epoch_train_loss=4.437386927282032
Epoch 602
Epoch 602 :: Batch 0/1
Batch Loss = 4.437386917315162
602, epoch_train_loss=4.437386917315162
Epoch 603
Epoch 603 :: Batch 0/1
Batch Loss = 4.437386907334902
603, epoch_train_loss=4.437386907334902
Epoch 604
Epoch 604 :: Batch 0/1
Batch Loss = 4.437386897337884
604, epoch_train_loss=4.437386897337884
Epoch 605
Epoch 605 :: Batch 0/1
Batch Loss = 4.437386887320666
605, epoch_train_loss=4.437386887320666
Epoch 606
Epoch 606 :: Batch 0/1
Batch Loss = 4.437386877279739
606, epoch_train_loss=4.437386877279739
Epoch 607
Epoch 607 :: Batch 0/1
Batch Loss = 4.437386867211521
607, epoch_train_loss=4.437386867211521
Epoch 608
Epoch 608 :: Batch 0/1
Batch Loss = 4.437386857112348
608, epoch_train_loss=4.437386857112348
Epoch 609
Epoch 609 :: Batch 0/1
Batch Loss = 4.437386846978478
609, epoch_train_loss=4.437386846978478
Epoch 610
Epoch 610 :: Batch 0/1
Batch Loss = 4.437386836806084
610, epoch_train_loss=4.437386836806084
Epoch 611
Epoch 611 :: Batch 0/1
Batch Loss = 4.437386826591257
611, epoch_train_loss=4.437386826591257
Epoch 612
Epoch 612 :: Batch 0/1
Batch Loss = 4.437386816329997
612, epoch_train_loss=4.437386816329997
Epoch 613
Epoch 613 :: Batch 0/1
Batch Loss = 4.437386806018225
613, epoch_train_loss=4.437386806018225
Epoch 614
Epoch 614 :: Batch 0/1
Batch Loss = 4.4373867956517605
614, epoch_train_loss=4.4373867956517605
Epoch 615
Epoch 615 :: Batch 0/1
Batch Loss = 4.437386785226351
615, epoch_train_loss=4.437386785226351
Epoch 616
Epoch 616 :: Batch 0/1
Batch Loss = 4.437386774737645
616, epoch_train_loss=4.437386774737645
Epoch 617
Epoch 617 :: Batch 0/1
Batch Loss = 4.437386764181214
617, epoch_train_loss=4.437386764181214
Epoch 618
Epoch 618 :: Batch 0/1
Batch Loss = 4.4373867535525475
618, epoch_train_loss=4.4373867535525475
Epoch 619
Epoch 619 :: Batch 0/1
Batch Loss = 4.437386742847059
619, epoch_train_loss=4.437386742847059
Epoch 620
Epoch 620 :: Batch 0/1
Batch Loss = 4.43738673206009
620, epoch_train_loss=4.43738673206009
Epoch 621
Epoch 621 :: Batch 0/1
Batch Loss = 4.437386721186925
621, epoch_train_loss=4.437386721186925
Epoch 622
Epoch 622 :: Batch 0/1
Batch Loss = 4.43738671022279
622, epoch_train_loss=4.43738671022279
Epoch 623
Epoch 623 :: Batch 0/1
Batch Loss = 4.437386699162873
623, epoch_train_loss=4.437386699162873
Epoch 624
Epoch 624 :: Batch 0/1
Batch Loss = 4.4373866880023325
624, epoch_train_loss=4.4373866880023325
Epoch 625
Epoch 625 :: Batch 0/1
Batch Loss = 4.437386676736315
625, epoch_train_loss=4.437386676736315
Epoch 626
Epoch 626 :: Batch 0/1
Batch Loss = 4.437386665359975
626, epoch_train_loss=4.437386665359975
Epoch 627
Epoch 627 :: Batch 0/1
Batch Loss = 4.4373866538684945
627, epoch_train_loss=4.4373866538684945
Epoch 628
Epoch 628 :: Batch 0/1
Batch Loss = 4.437386642257107
628, epoch_train_loss=4.437386642257107
Epoch 629
Epoch 629 :: Batch 0/1
Batch Loss = 4.437386630521131
629, epoch_train_loss=4.437386630521131
Epoch 630
Epoch 630 :: Batch 0/1
Batch Loss = 4.4373866186559985
630, epoch_train_loss=4.4373866186559985
Epoch 631
Epoch 631 :: Batch 0/1
Batch Loss = 4.4373866066572845
631, epoch_train_loss=4.4373866066572845
Epoch 632
Epoch 632 :: Batch 0/1
Batch Loss = 4.437386594520765
632, epoch_train_loss=4.437386594520765
Epoch 633
Epoch 633 :: Batch 0/1
Batch Loss = 4.437386582242445
633, epoch_train_loss=4.437386582242445
Epoch 634
Epoch 634 :: Batch 0/1
Batch Loss = 4.4373865698186155
634, epoch_train_loss=4.4373865698186155
Epoch 635
Epoch 635 :: Batch 0/1
Batch Loss = 4.43738655724591
635, epoch_train_loss=4.43738655724591
Epoch 636
Epoch 636 :: Batch 0/1
Batch Loss = 4.43738654452136
636, epoch_train_loss=4.43738654452136
Epoch 637
Epoch 637 :: Batch 0/1
Batch Loss = 4.437386531642467
637, epoch_train_loss=4.437386531642467
Epoch 638
Epoch 638 :: Batch 0/1
Batch Loss = 4.437386518607266
638, epoch_train_loss=4.437386518607266
Epoch 639
Epoch 639 :: Batch 0/1
Batch Loss = 4.437386505414402
639, epoch_train_loss=4.437386505414402
Epoch 640
Epoch 640 :: Batch 0/1
Batch Loss = 4.437386492063222
640, epoch_train_loss=4.437386492063222
Epoch 641
Epoch 641 :: Batch 0/1
Batch Loss = 4.43738647855384
641, epoch_train_loss=4.43738647855384
Epoch 642
Epoch 642 :: Batch 0/1
Batch Loss = 4.437386464887247
642, epoch_train_loss=4.437386464887247
Epoch 643
Epoch 643 :: Batch 0/1
Batch Loss = 4.437386451065389
643, epoch_train_loss=4.437386451065389
Epoch 644
Epoch 644 :: Batch 0/1
Batch Loss = 4.437386437091269
644, epoch_train_loss=4.437386437091269
Epoch 645
Epoch 645 :: Batch 0/1
Batch Loss = 4.437386422969039
645, epoch_train_loss=4.437386422969039
Epoch 646
Epoch 646 :: Batch 0/1
Batch Loss = 4.437386408704105
646, epoch_train_loss=4.437386408704105
Epoch 647
Epoch 647 :: Batch 0/1
Batch Loss = 4.437386394303209
647, epoch_train_loss=4.437386394303209
Epoch 648
Epoch 648 :: Batch 0/1
Batch Loss = 4.437386379774526
648, epoch_train_loss=4.437386379774526
Epoch 649
Epoch 649 :: Batch 0/1
Batch Loss = 4.437386365127749
649, epoch_train_loss=4.437386365127749
Epoch 650
Epoch 650 :: Batch 0/1
Batch Loss = 4.437386350374161
650, epoch_train_loss=4.437386350374161
Epoch 651
Epoch 651 :: Batch 0/1
Batch Loss = 4.437386335526704
651, epoch_train_loss=4.437386335526704
Epoch 652
Epoch 652 :: Batch 0/1
Batch Loss = 4.437386320600025
652, epoch_train_loss=4.437386320600025
Epoch 653
Epoch 653 :: Batch 0/1
Batch Loss = 4.437386305610512
653, epoch_train_loss=4.437386305610512
Epoch 654
Epoch 654 :: Batch 0/1
Batch Loss = 4.437386290576303
654, epoch_train_loss=4.437386290576303
Epoch 655
Epoch 655 :: Batch 0/1
Batch Loss = 4.437386275517281
655, epoch_train_loss=4.437386275517281
Epoch 656
Epoch 656 :: Batch 0/1
Batch Loss = 4.437386260455026
656, epoch_train_loss=4.437386260455026
Epoch 657
Epoch 657 :: Batch 0/1
Batch Loss = 4.437386245412756
657, epoch_train_loss=4.437386245412756
Epoch 658
Epoch 658 :: Batch 0/1
Batch Loss = 4.437386230415226
658, epoch_train_loss=4.437386230415226
Epoch 659
Epoch 659 :: Batch 0/1
Batch Loss = 4.437386215488588
659, epoch_train_loss=4.437386215488588
Epoch 660
Epoch 660 :: Batch 0/1
Batch Loss = 4.437386200660226
660, epoch_train_loss=4.437386200660226
Epoch 661
Epoch 661 :: Batch 0/1
Batch Loss = 4.43738618595855
661, epoch_train_loss=4.43738618595855
Epoch 662
Epoch 662 :: Batch 0/1
Batch Loss = 4.4373861714127525
662, epoch_train_loss=4.4373861714127525
Epoch 663
Epoch 663 :: Batch 0/1
Batch Loss = 4.43738615705253
663, epoch_train_loss=4.43738615705253
Epoch 664
Epoch 664 :: Batch 0/1
Batch Loss = 4.437386142907783
664, epoch_train_loss=4.437386142907783
Epoch 665
Epoch 665 :: Batch 0/1
Batch Loss = 4.437386129008264
665, epoch_train_loss=4.437386129008264
Epoch 666
Epoch 666 :: Batch 0/1
Batch Loss = 4.437386115383232
666, epoch_train_loss=4.437386115383232
Epoch 667
Epoch 667 :: Batch 0/1
Batch Loss = 4.437386102061064
667, epoch_train_loss=4.437386102061064
Epoch 668
Epoch 668 :: Batch 0/1
Batch Loss = 4.43738608906887
668, epoch_train_loss=4.43738608906887
Epoch 669
Epoch 669 :: Batch 0/1
Batch Loss = 4.437386076432106
669, epoch_train_loss=4.437386076432106
Epoch 670
Epoch 670 :: Batch 0/1
Batch Loss = 4.437386064174186
670, epoch_train_loss=4.437386064174186
Epoch 671
Epoch 671 :: Batch 0/1
Batch Loss = 4.4373860523161115
671, epoch_train_loss=4.4373860523161115
Epoch 672
Epoch 672 :: Batch 0/1
Batch Loss = 4.437386040876136
672, epoch_train_loss=4.437386040876136
Epoch 673
Epoch 673 :: Batch 0/1
Batch Loss = 4.437386029869458
673, epoch_train_loss=4.437386029869458
Epoch 674
Epoch 674 :: Batch 0/1
Batch Loss = 4.437386019307958
674, epoch_train_loss=4.437386019307958
Epoch 675
Epoch 675 :: Batch 0/1
Batch Loss = 4.437386009199985
675, epoch_train_loss=4.437386009199985
Epoch 676
Epoch 676 :: Batch 0/1
Batch Loss = 4.43738599955022
676, epoch_train_loss=4.43738599955022
Epoch 677
Epoch 677 :: Batch 0/1
Batch Loss = 4.437385990359586
677, epoch_train_loss=4.437385990359586
Epoch 678
Epoch 678 :: Batch 0/1
Batch Loss = 4.437385981625241
678, epoch_train_loss=4.437385981625241
Epoch 679
Epoch 679 :: Batch 0/1
Batch Loss = 4.437385973340637
679, epoch_train_loss=4.437385973340637
Epoch 680
Epoch 680 :: Batch 0/1
Batch Loss = 4.437385965495656
680, epoch_train_loss=4.437385965495656
Epoch 681
Epoch 681 :: Batch 0/1
Batch Loss = 4.437385958076806
681, epoch_train_loss=4.437385958076806
Epoch 682
Epoch 682 :: Batch 0/1
Batch Loss = 4.437385951067506
682, epoch_train_loss=4.437385951067506
Epoch 683
Epoch 683 :: Batch 0/1
Batch Loss = 4.437385944448397
683, epoch_train_loss=4.437385944448397
Epoch 684
Epoch 684 :: Batch 0/1
Batch Loss = 4.437385938197727
684, epoch_train_loss=4.437385938197727
Epoch 685
Epoch 685 :: Batch 0/1
Batch Loss = 4.437385932291775
685, epoch_train_loss=4.437385932291775
Epoch 686
Epoch 686 :: Batch 0/1
Batch Loss = 4.437385926705279
686, epoch_train_loss=4.437385926705279
Epoch 687
Epoch 687 :: Batch 0/1
Batch Loss = 4.437385921411918
687, epoch_train_loss=4.437385921411918
Epoch 688
Epoch 688 :: Batch 0/1
Batch Loss = 4.437385916384773
688, epoch_train_loss=4.437385916384773
Epoch 689
Epoch 689 :: Batch 0/1
Batch Loss = 4.437385911596772
689, epoch_train_loss=4.437385911596772
Epoch 690
Epoch 690 :: Batch 0/1
Batch Loss = 4.437385907021142
690, epoch_train_loss=4.437385907021142
Epoch 691
Epoch 691 :: Batch 0/1
Batch Loss = 4.437385902631797
691, epoch_train_loss=4.437385902631797
Epoch 692
Epoch 692 :: Batch 0/1
Batch Loss = 4.437385898403715
692, epoch_train_loss=4.437385898403715
Epoch 693
Epoch 693 :: Batch 0/1
Batch Loss = 4.437385894313237
693, epoch_train_loss=4.437385894313237
Epoch 694
Epoch 694 :: Batch 0/1
Batch Loss = 4.437385890338339
694, epoch_train_loss=4.437385890338339
Epoch 695
Epoch 695 :: Batch 0/1
Batch Loss = 4.437385886458823
695, epoch_train_loss=4.437385886458823
Epoch 696
Epoch 696 :: Batch 0/1
Batch Loss = 4.437385882656463
696, epoch_train_loss=4.437385882656463
Epoch 697
Epoch 697 :: Batch 0/1
Batch Loss = 4.43738587891509
697, epoch_train_loss=4.43738587891509
Epoch 698
Epoch 698 :: Batch 0/1
Batch Loss = 4.437385875220615
698, epoch_train_loss=4.437385875220615
Epoch 699
Epoch 699 :: Batch 0/1
Batch Loss = 4.437385871561011
699, epoch_train_loss=4.437385871561011
Epoch 700
Epoch 700 :: Batch 0/1
Batch Loss = 4.437385867926244
700, epoch_train_loss=4.437385867926244
Epoch 701
Epoch 701 :: Batch 0/1
Batch Loss = 4.437385864308171
701, epoch_train_loss=4.437385864308171
Epoch 702
Epoch 702 :: Batch 0/1
Batch Loss = 4.437385860700394
702, epoch_train_loss=4.437385860700394
Epoch 703
Epoch 703 :: Batch 0/1
Batch Loss = 4.437385857098114
703, epoch_train_loss=4.437385857098114
Epoch 704
Epoch 704 :: Batch 0/1
Batch Loss = 4.437385853497941
704, epoch_train_loss=4.437385853497941
Epoch 705
Epoch 705 :: Batch 0/1
Batch Loss = 4.43738584989772
705, epoch_train_loss=4.43738584989772
Epoch 706
Epoch 706 :: Batch 0/1
Batch Loss = 4.437385846296346
706, epoch_train_loss=4.437385846296346
Epoch 707
Epoch 707 :: Batch 0/1
Batch Loss = 4.437385842693572
707, epoch_train_loss=4.437385842693572
Epoch 708
Epoch 708 :: Batch 0/1
Batch Loss = 4.4373858390898455
708, epoch_train_loss=4.4373858390898455
Epoch 709
Epoch 709 :: Batch 0/1
Batch Loss = 4.4373858354861415
709, epoch_train_loss=4.4373858354861415
Epoch 710
Epoch 710 :: Batch 0/1
Batch Loss = 4.437385831883816
710, epoch_train_loss=4.437385831883816
Epoch 711
Epoch 711 :: Batch 0/1
Batch Loss = 4.43738582828448
711, epoch_train_loss=4.43738582828448
Epoch 712
Epoch 712 :: Batch 0/1
Batch Loss = 4.437385824689882
712, epoch_train_loss=4.437385824689882
Epoch 713
Epoch 713 :: Batch 0/1
Batch Loss = 4.437385821101817
713, epoch_train_loss=4.437385821101817
Epoch 714
Epoch 714 :: Batch 0/1
Batch Loss = 4.437385817522047
714, epoch_train_loss=4.437385817522047
Epoch 715
Epoch 715 :: Batch 0/1
Batch Loss = 4.437385813952248
715, epoch_train_loss=4.437385813952248
Epoch 716
Epoch 716 :: Batch 0/1
Batch Loss = 4.437385810393954
716, epoch_train_loss=4.437385810393954
Epoch 717
Epoch 717 :: Batch 0/1
Batch Loss = 4.437385806848535
717, epoch_train_loss=4.437385806848535
Epoch 718
Epoch 718 :: Batch 0/1
Batch Loss = 4.437385803317177
718, epoch_train_loss=4.437385803317177
Epoch 719
Epoch 719 :: Batch 0/1
Batch Loss = 4.437385799800865
719, epoch_train_loss=4.437385799800865
Epoch 720
Epoch 720 :: Batch 0/1
Batch Loss = 4.437385796300394
720, epoch_train_loss=4.437385796300394
Epoch 721
Epoch 721 :: Batch 0/1
Batch Loss = 4.437385792816369
721, epoch_train_loss=4.437385792816369
Epoch 722
Epoch 722 :: Batch 0/1
Batch Loss = 4.437385789349216
722, epoch_train_loss=4.437385789349216
Epoch 723
Epoch 723 :: Batch 0/1
Batch Loss = 4.437385785899197
723, epoch_train_loss=4.437385785899197
Epoch 724
Epoch 724 :: Batch 0/1
Batch Loss = 4.437385782466433
724, epoch_train_loss=4.437385782466433
Epoch 725
Epoch 725 :: Batch 0/1
Batch Loss = 4.437385779050913
725, epoch_train_loss=4.437385779050913
Epoch 726
Epoch 726 :: Batch 0/1
Batch Loss = 4.437385775652525
726, epoch_train_loss=4.437385775652525
Epoch 727
Epoch 727 :: Batch 0/1
Batch Loss = 4.437385772271065
727, epoch_train_loss=4.437385772271065
Epoch 728
Epoch 728 :: Batch 0/1
Batch Loss = 4.437385768906261
728, epoch_train_loss=4.437385768906261
Epoch 729
Epoch 729 :: Batch 0/1
Batch Loss = 4.437385765557787
729, epoch_train_loss=4.437385765557787
Epoch 730
Epoch 730 :: Batch 0/1
Batch Loss = 4.437385762225284
730, epoch_train_loss=4.437385762225284
Epoch 731
Epoch 731 :: Batch 0/1
Batch Loss = 4.4373857589083645
731, epoch_train_loss=4.4373857589083645
Epoch 732
Epoch 732 :: Batch 0/1
Batch Loss = 4.437385755606633
732, epoch_train_loss=4.437385755606633
Epoch 733
Epoch 733 :: Batch 0/1
Batch Loss = 4.437385752319692
733, epoch_train_loss=4.437385752319692
Epoch 734
Epoch 734 :: Batch 0/1
Batch Loss = 4.437385749047148
734, epoch_train_loss=4.437385749047148
Epoch 735
Epoch 735 :: Batch 0/1
Batch Loss = 4.437385745788626
735, epoch_train_loss=4.437385745788626
Epoch 736
Epoch 736 :: Batch 0/1
Batch Loss = 4.437385742543768
736, epoch_train_loss=4.437385742543768
Epoch 737
Epoch 737 :: Batch 0/1
Batch Loss = 4.437385739312235
737, epoch_train_loss=4.437385739312235
Epoch 738
Epoch 738 :: Batch 0/1
Batch Loss = 4.437385736093719
738, epoch_train_loss=4.437385736093719
Epoch 739
Epoch 739 :: Batch 0/1
Batch Loss = 4.437385732887935
739, epoch_train_loss=4.437385732887935
Epoch 740
Epoch 740 :: Batch 0/1
Batch Loss = 4.437385729694624
740, epoch_train_loss=4.437385729694624
Epoch 741
Epoch 741 :: Batch 0/1
Batch Loss = 4.4373857265135515
741, epoch_train_loss=4.4373857265135515
Epoch 742
Epoch 742 :: Batch 0/1
Batch Loss = 4.437385723344514
742, epoch_train_loss=4.437385723344514
Epoch 743
Epoch 743 :: Batch 0/1
Batch Loss = 4.437385720187327
743, epoch_train_loss=4.437385720187327
Epoch 744
Epoch 744 :: Batch 0/1
Batch Loss = 4.43738571704183
744, epoch_train_loss=4.43738571704183
Epoch 745
Epoch 745 :: Batch 0/1
Batch Loss = 4.437385713907884
745, epoch_train_loss=4.437385713907884
Epoch 746
Epoch 746 :: Batch 0/1
Batch Loss = 4.437385710785363
746, epoch_train_loss=4.437385710785363
Epoch 747
Epoch 747 :: Batch 0/1
Batch Loss = 4.437385707674165
747, epoch_train_loss=4.437385707674165
Epoch 748
Epoch 748 :: Batch 0/1
Batch Loss = 4.437385704574197
748, epoch_train_loss=4.437385704574197
Epoch 749
Epoch 749 :: Batch 0/1
Batch Loss = 4.437385701485377
749, epoch_train_loss=4.437385701485377
Epoch 750
Epoch 750 :: Batch 0/1
Batch Loss = 4.437385698407636
750, epoch_train_loss=4.437385698407636
Epoch 751
Epoch 751 :: Batch 0/1
Batch Loss = 4.437385695340905
751, epoch_train_loss=4.437385695340905
Epoch 752
Epoch 752 :: Batch 0/1
Batch Loss = 4.437385692285134
752, epoch_train_loss=4.437385692285134
Epoch 753
Epoch 753 :: Batch 0/1
Batch Loss = 4.4373856892402666
753, epoch_train_loss=4.4373856892402666
Epoch 754
Epoch 754 :: Batch 0/1
Batch Loss = 4.437385686206257
754, epoch_train_loss=4.437385686206257
Epoch 755
Epoch 755 :: Batch 0/1
Batch Loss = 4.437385683183055
755, epoch_train_loss=4.437385683183055
Epoch 756
Epoch 756 :: Batch 0/1
Batch Loss = 4.437385680170619
756, epoch_train_loss=4.437385680170619
Epoch 757
Epoch 757 :: Batch 0/1
Batch Loss = 4.4373856771689
757, epoch_train_loss=4.4373856771689
Epoch 758
Epoch 758 :: Batch 0/1
Batch Loss = 4.4373856741778575
758, epoch_train_loss=4.4373856741778575
Epoch 759
Epoch 759 :: Batch 0/1
Batch Loss = 4.437385671197444
759, epoch_train_loss=4.437385671197444
Epoch 760
Epoch 760 :: Batch 0/1
Batch Loss = 4.437385668227612
760, epoch_train_loss=4.437385668227612
Epoch 761
Epoch 761 :: Batch 0/1
Batch Loss = 4.437385665268317
761, epoch_train_loss=4.437385665268317
Epoch 762
Epoch 762 :: Batch 0/1
Batch Loss = 4.437385662319506
762, epoch_train_loss=4.437385662319506
Epoch 763
Epoch 763 :: Batch 0/1
Batch Loss = 4.437385659381131
763, epoch_train_loss=4.437385659381131
Epoch 764
Epoch 764 :: Batch 0/1
Batch Loss = 4.4373856564531415
764, epoch_train_loss=4.4373856564531415
Epoch 765
Epoch 765 :: Batch 0/1
Batch Loss = 4.437385653535482
765, epoch_train_loss=4.437385653535482
Epoch 766
Epoch 766 :: Batch 0/1
Batch Loss = 4.437385650628099
766, epoch_train_loss=4.437385650628099
Epoch 767
Epoch 767 :: Batch 0/1
Batch Loss = 4.437385647730937
767, epoch_train_loss=4.437385647730937
Epoch 768
Epoch 768 :: Batch 0/1
Batch Loss = 4.437385644843941
768, epoch_train_loss=4.437385644843941
Epoch 769
Epoch 769 :: Batch 0/1
Batch Loss = 4.437385641967053
769, epoch_train_loss=4.437385641967053
Epoch 770
Epoch 770 :: Batch 0/1
Batch Loss = 4.437385639100216
770, epoch_train_loss=4.437385639100216
Epoch 771
Epoch 771 :: Batch 0/1
Batch Loss = 4.437385636243371
771, epoch_train_loss=4.437385636243371
Epoch 772
Epoch 772 :: Batch 0/1
Batch Loss = 4.4373856333964605
772, epoch_train_loss=4.4373856333964605
Epoch 773
Epoch 773 :: Batch 0/1
Batch Loss = 4.437385630559426
773, epoch_train_loss=4.437385630559426
Epoch 774
Epoch 774 :: Batch 0/1
Batch Loss = 4.43738562773221
774, epoch_train_loss=4.43738562773221
Epoch 775
Epoch 775 :: Batch 0/1
Batch Loss = 4.437385624914753
775, epoch_train_loss=4.437385624914753
Epoch 776
Epoch 776 :: Batch 0/1
Batch Loss = 4.437385622106999
776, epoch_train_loss=4.437385622106999
Epoch 777
Epoch 777 :: Batch 0/1
Batch Loss = 4.437385619308889
777, epoch_train_loss=4.437385619308889
Epoch 778
Epoch 778 :: Batch 0/1
Batch Loss = 4.43738561652037
778, epoch_train_loss=4.43738561652037
Epoch 779
Epoch 779 :: Batch 0/1
Batch Loss = 4.437385613741379
779, epoch_train_loss=4.437385613741379
Epoch 780
Epoch 780 :: Batch 0/1
Batch Loss = 4.437385610971867
780, epoch_train_loss=4.437385610971867
Epoch 781
Epoch 781 :: Batch 0/1
Batch Loss = 4.437385608211773
781, epoch_train_loss=4.437385608211773
Epoch 782
Epoch 782 :: Batch 0/1
Batch Loss = 4.437385605461047
782, epoch_train_loss=4.437385605461047
Epoch 783
Epoch 783 :: Batch 0/1
Batch Loss = 4.437385602719635
783, epoch_train_loss=4.437385602719635
Epoch 784
Epoch 784 :: Batch 0/1
Batch Loss = 4.43738559998748
784, epoch_train_loss=4.43738559998748
Epoch 785
Epoch 785 :: Batch 0/1
Batch Loss = 4.437385597264533
785, epoch_train_loss=4.437385597264533
Epoch 786
Epoch 786 :: Batch 0/1
Batch Loss = 4.437385594550741
786, epoch_train_loss=4.437385594550741
Epoch 787
Epoch 787 :: Batch 0/1
Batch Loss = 4.437385591846054
787, epoch_train_loss=4.437385591846054
Epoch 788
Epoch 788 :: Batch 0/1
Batch Loss = 4.437385589150418
788, epoch_train_loss=4.437385589150418
Epoch 789
Epoch 789 :: Batch 0/1
Batch Loss = 4.437385586463783
789, epoch_train_loss=4.437385586463783
Epoch 790
Epoch 790 :: Batch 0/1
Batch Loss = 4.437385583786104
790, epoch_train_loss=4.437385583786104
Epoch 791
Epoch 791 :: Batch 0/1
Batch Loss = 4.4373855811173275
791, epoch_train_loss=4.4373855811173275
Epoch 792
Epoch 792 :: Batch 0/1
Batch Loss = 4.437385578457405
792, epoch_train_loss=4.437385578457405
Epoch 793
Epoch 793 :: Batch 0/1
Batch Loss = 4.4373855758062914
793, epoch_train_loss=4.4373855758062914
Epoch 794
Epoch 794 :: Batch 0/1
Batch Loss = 4.437385573163936
794, epoch_train_loss=4.437385573163936
Epoch 795
Epoch 795 :: Batch 0/1
Batch Loss = 4.437385570530292
795, epoch_train_loss=4.437385570530292
Epoch 796
Epoch 796 :: Batch 0/1
Batch Loss = 4.437385567905313
796, epoch_train_loss=4.437385567905313
Epoch 797
Epoch 797 :: Batch 0/1
Batch Loss = 4.437385565288951
797, epoch_train_loss=4.437385565288951
Epoch 798
Epoch 798 :: Batch 0/1
Batch Loss = 4.437385562681163
798, epoch_train_loss=4.437385562681163
Epoch 799
Epoch 799 :: Batch 0/1
Batch Loss = 4.4373855600819
799, epoch_train_loss=4.4373855600819
Epoch 800
Epoch 800 :: Batch 0/1
Batch Loss = 4.437385557491117
800, epoch_train_loss=4.437385557491117
Epoch 801
Epoch 801 :: Batch 0/1
Batch Loss = 4.4373855549087695
801, epoch_train_loss=4.4373855549087695
Epoch 802
Epoch 802 :: Batch 0/1
Batch Loss = 4.437385552334812
802, epoch_train_loss=4.437385552334812
Epoch 803
Epoch 803 :: Batch 0/1
Batch Loss = 4.437385549769199
803, epoch_train_loss=4.437385549769199
Epoch 804
Epoch 804 :: Batch 0/1
Batch Loss = 4.437385547211889
804, epoch_train_loss=4.437385547211889
Epoch 805
Epoch 805 :: Batch 0/1
Batch Loss = 4.437385544662833
805, epoch_train_loss=4.437385544662833
Epoch 806
Epoch 806 :: Batch 0/1
Batch Loss = 4.437385542121992
806, epoch_train_loss=4.437385542121992
Epoch 807
Epoch 807 :: Batch 0/1
Batch Loss = 4.437385539589322
807, epoch_train_loss=4.437385539589322
Epoch 808
Epoch 808 :: Batch 0/1
Batch Loss = 4.437385537064777
808, epoch_train_loss=4.437385537064777
Epoch 809
Epoch 809 :: Batch 0/1
Batch Loss = 4.437385534548314
809, epoch_train_loss=4.437385534548314
Epoch 810
Epoch 810 :: Batch 0/1
Batch Loss = 4.437385532039894
810, epoch_train_loss=4.437385532039894
Epoch 811
Epoch 811 :: Batch 0/1
Batch Loss = 4.437385529539473
811, epoch_train_loss=4.437385529539473
Epoch 812
Epoch 812 :: Batch 0/1
Batch Loss = 4.437385527047008
812, epoch_train_loss=4.437385527047008
Epoch 813
Epoch 813 :: Batch 0/1
Batch Loss = 4.437385524562456
813, epoch_train_loss=4.437385524562456
Epoch 814
Epoch 814 :: Batch 0/1
Batch Loss = 4.437385522085778
814, epoch_train_loss=4.437385522085778
Epoch 815
Epoch 815 :: Batch 0/1
Batch Loss = 4.437385519616931
815, epoch_train_loss=4.437385519616931
Epoch 816
Epoch 816 :: Batch 0/1
Batch Loss = 4.437385517155875
816, epoch_train_loss=4.437385517155875
Epoch 817
Epoch 817 :: Batch 0/1
Batch Loss = 4.4373855147025685
817, epoch_train_loss=4.4373855147025685
Epoch 818
Epoch 818 :: Batch 0/1
Batch Loss = 4.437385512256973
818, epoch_train_loss=4.437385512256973
Epoch 819
Epoch 819 :: Batch 0/1
Batch Loss = 4.437385509819046
819, epoch_train_loss=4.437385509819046
Epoch 820
Epoch 820 :: Batch 0/1
Batch Loss = 4.437385507388748
820, epoch_train_loss=4.437385507388748
Epoch 821
Epoch 821 :: Batch 0/1
Batch Loss = 4.43738550496604
821, epoch_train_loss=4.43738550496604
Epoch 822
Epoch 822 :: Batch 0/1
Batch Loss = 4.437385502550882
822, epoch_train_loss=4.437385502550882
Epoch 823
Epoch 823 :: Batch 0/1
Batch Loss = 4.437385500143235
823, epoch_train_loss=4.437385500143235
Epoch 824
Epoch 824 :: Batch 0/1
Batch Loss = 4.437385497743061
824, epoch_train_loss=4.437385497743061
Epoch 825
Epoch 825 :: Batch 0/1
Batch Loss = 4.437385495350321
825, epoch_train_loss=4.437385495350321
Epoch 826
Epoch 826 :: Batch 0/1
Batch Loss = 4.437385492964977
826, epoch_train_loss=4.437385492964977
Epoch 827
Epoch 827 :: Batch 0/1
Batch Loss = 4.4373854905869905
827, epoch_train_loss=4.4373854905869905
Epoch 828
Epoch 828 :: Batch 0/1
Batch Loss = 4.437385488216322
828, epoch_train_loss=4.437385488216322
Epoch 829
Epoch 829 :: Batch 0/1
Batch Loss = 4.437385485852937
829, epoch_train_loss=4.437385485852937
Epoch 830
Epoch 830 :: Batch 0/1
Batch Loss = 4.437385483496796
830, epoch_train_loss=4.437385483496796
Epoch 831
Epoch 831 :: Batch 0/1
Batch Loss = 4.437385481147865
831, epoch_train_loss=4.437385481147865
Epoch 832
Epoch 832 :: Batch 0/1
Batch Loss = 4.437385478806101
832, epoch_train_loss=4.437385478806101
Epoch 833
Epoch 833 :: Batch 0/1
Batch Loss = 4.4373854764714755
833, epoch_train_loss=4.4373854764714755
Epoch 834
Epoch 834 :: Batch 0/1
Batch Loss = 4.437385474143946
834, epoch_train_loss=4.437385474143946
Epoch 835
Epoch 835 :: Batch 0/1
Batch Loss = 4.437385471823479
835, epoch_train_loss=4.437385471823479
Epoch 836
Epoch 836 :: Batch 0/1
Batch Loss = 4.437385469510038
836, epoch_train_loss=4.437385469510038
Epoch 837
Epoch 837 :: Batch 0/1
Batch Loss = 4.437385467203588
837, epoch_train_loss=4.437385467203588
Epoch 838
Epoch 838 :: Batch 0/1
Batch Loss = 4.437385464904093
838, epoch_train_loss=4.437385464904093
Epoch 839
Epoch 839 :: Batch 0/1
Batch Loss = 4.437385462611519
839, epoch_train_loss=4.437385462611519
Epoch 840
Epoch 840 :: Batch 0/1
Batch Loss = 4.437385460325829
840, epoch_train_loss=4.437385460325829
Epoch 841
Epoch 841 :: Batch 0/1
Batch Loss = 4.4373854580469905
841, epoch_train_loss=4.4373854580469905
Epoch 842
Epoch 842 :: Batch 0/1
Batch Loss = 4.437385455774967
842, epoch_train_loss=4.437385455774967
Epoch 843
Epoch 843 :: Batch 0/1
Batch Loss = 4.437385453509727
843, epoch_train_loss=4.437385453509727
Epoch 844
Epoch 844 :: Batch 0/1
Batch Loss = 4.437385451251232
844, epoch_train_loss=4.437385451251232
Epoch 845
Epoch 845 :: Batch 0/1
Batch Loss = 4.437385448999455
845, epoch_train_loss=4.437385448999455
Epoch 846
Epoch 846 :: Batch 0/1
Batch Loss = 4.437385446754356
846, epoch_train_loss=4.437385446754356
Epoch 847
Epoch 847 :: Batch 0/1
Batch Loss = 4.437385444515905
847, epoch_train_loss=4.437385444515905
Epoch 848
Epoch 848 :: Batch 0/1
Batch Loss = 4.437385442284068
848, epoch_train_loss=4.437385442284068
Epoch 849
Epoch 849 :: Batch 0/1
Batch Loss = 4.4373854400588115
849, epoch_train_loss=4.4373854400588115
Epoch 850
Epoch 850 :: Batch 0/1
Batch Loss = 4.437385437840104
850, epoch_train_loss=4.437385437840104
Epoch 851
Epoch 851 :: Batch 0/1
Batch Loss = 4.437385435627911
851, epoch_train_loss=4.437385435627911
Epoch 852
Epoch 852 :: Batch 0/1
Batch Loss = 4.437385433422202
852, epoch_train_loss=4.437385433422202
Epoch 853
Epoch 853 :: Batch 0/1
Batch Loss = 4.437385431222947
853, epoch_train_loss=4.437385431222947
Epoch 854
Epoch 854 :: Batch 0/1
Batch Loss = 4.437385429030109
854, epoch_train_loss=4.437385429030109
Epoch 855
Epoch 855 :: Batch 0/1
Batch Loss = 4.437385426843658
855, epoch_train_loss=4.437385426843658
Epoch 856
Epoch 856 :: Batch 0/1
Batch Loss = 4.437385424663565
856, epoch_train_loss=4.437385424663565
Epoch 857
Epoch 857 :: Batch 0/1
Batch Loss = 4.437385422489797
857, epoch_train_loss=4.437385422489797
Epoch 858
Epoch 858 :: Batch 0/1
Batch Loss = 4.4373854203223235
858, epoch_train_loss=4.4373854203223235
Epoch 859
Epoch 859 :: Batch 0/1
Batch Loss = 4.437385418161112
859, epoch_train_loss=4.437385418161112
Epoch 860
Epoch 860 :: Batch 0/1
Batch Loss = 4.437385416006133
860, epoch_train_loss=4.437385416006133
Epoch 861
Epoch 861 :: Batch 0/1
Batch Loss = 4.437385413857355
861, epoch_train_loss=4.437385413857355
Epoch 862
Epoch 862 :: Batch 0/1
Batch Loss = 4.43738541171475
862, epoch_train_loss=4.43738541171475
Epoch 863
Epoch 863 :: Batch 0/1
Batch Loss = 4.437385409578285
863, epoch_train_loss=4.437385409578285
Epoch 864
Epoch 864 :: Batch 0/1
Batch Loss = 4.437385407447931
864, epoch_train_loss=4.437385407447931
Epoch 865
Epoch 865 :: Batch 0/1
Batch Loss = 4.43738540532366
865, epoch_train_loss=4.43738540532366
Epoch 866
Epoch 866 :: Batch 0/1
Batch Loss = 4.4373854032054405
866, epoch_train_loss=4.4373854032054405
Epoch 867
Epoch 867 :: Batch 0/1
Batch Loss = 4.437385401093243
867, epoch_train_loss=4.437385401093243
Epoch 868
Epoch 868 :: Batch 0/1
Batch Loss = 4.437385398987039
868, epoch_train_loss=4.437385398987039
Epoch 869
Epoch 869 :: Batch 0/1
Batch Loss = 4.4373853968868
869, epoch_train_loss=4.4373853968868
Epoch 870
Epoch 870 :: Batch 0/1
Batch Loss = 4.4373853947924955
870, epoch_train_loss=4.4373853947924955
Epoch 871
Epoch 871 :: Batch 0/1
Batch Loss = 4.437385392704098
871, epoch_train_loss=4.437385392704098
Epoch 872
Epoch 872 :: Batch 0/1
Batch Loss = 4.4373853906215786
872, epoch_train_loss=4.4373853906215786
Epoch 873
Epoch 873 :: Batch 0/1
Batch Loss = 4.437385388544908
873, epoch_train_loss=4.437385388544908
Epoch 874
Epoch 874 :: Batch 0/1
Batch Loss = 4.437385386474061
874, epoch_train_loss=4.437385386474061
Epoch 875
Epoch 875 :: Batch 0/1
Batch Loss = 4.437385384409007
875, epoch_train_loss=4.437385384409007
Epoch 876
Epoch 876 :: Batch 0/1
Batch Loss = 4.437385382349718
876, epoch_train_loss=4.437385382349718
Epoch 877
Epoch 877 :: Batch 0/1
Batch Loss = 4.43738538029617
877, epoch_train_loss=4.43738538029617
Epoch 878
Epoch 878 :: Batch 0/1
Batch Loss = 4.4373853782483295
878, epoch_train_loss=4.4373853782483295
Epoch 879
Epoch 879 :: Batch 0/1
Batch Loss = 4.437385376206173
879, epoch_train_loss=4.437385376206173
Epoch 880
Epoch 880 :: Batch 0/1
Batch Loss = 4.437385374169673
880, epoch_train_loss=4.437385374169673
Epoch 881
Epoch 881 :: Batch 0/1
Batch Loss = 4.437385372138802
881, epoch_train_loss=4.437385372138802
Epoch 882
Epoch 882 :: Batch 0/1
Batch Loss = 4.437385370113534
882, epoch_train_loss=4.437385370113534
Epoch 883
Epoch 883 :: Batch 0/1
Batch Loss = 4.4373853680938415
883, epoch_train_loss=4.4373853680938415
Epoch 884
Epoch 884 :: Batch 0/1
Batch Loss = 4.437385366079698
884, epoch_train_loss=4.437385366079698
Epoch 885
Epoch 885 :: Batch 0/1
Batch Loss = 4.437385364071077
885, epoch_train_loss=4.437385364071077
Epoch 886
Epoch 886 :: Batch 0/1
Batch Loss = 4.437385362067953
886, epoch_train_loss=4.437385362067953
Epoch 887
Epoch 887 :: Batch 0/1
Batch Loss = 4.4373853600703
887, epoch_train_loss=4.4373853600703
Epoch 888
Epoch 888 :: Batch 0/1
Batch Loss = 4.43738535807809
888, epoch_train_loss=4.43738535807809
Epoch 889
Epoch 889 :: Batch 0/1
Batch Loss = 4.4373853560912995
889, epoch_train_loss=4.4373853560912995
Epoch 890
Epoch 890 :: Batch 0/1
Batch Loss = 4.437385354109903
890, epoch_train_loss=4.437385354109903
Epoch 891
Epoch 891 :: Batch 0/1
Batch Loss = 4.437385352133873
891, epoch_train_loss=4.437385352133873
Epoch 892
Epoch 892 :: Batch 0/1
Batch Loss = 4.437385350163187
892, epoch_train_loss=4.437385350163187
Epoch 893
Epoch 893 :: Batch 0/1
Batch Loss = 4.437385348197818
893, epoch_train_loss=4.437385348197818
Epoch 894
Epoch 894 :: Batch 0/1
Batch Loss = 4.4373853462377415
894, epoch_train_loss=4.4373853462377415
Epoch 895
Epoch 895 :: Batch 0/1
Batch Loss = 4.437385344282933
895, epoch_train_loss=4.437385344282933
Epoch 896
Epoch 896 :: Batch 0/1
Batch Loss = 4.437385342333366
896, epoch_train_loss=4.437385342333366
Epoch 897
Epoch 897 :: Batch 0/1
Batch Loss = 4.437385340389018
897, epoch_train_loss=4.437385340389018
Epoch 898
Epoch 898 :: Batch 0/1
Batch Loss = 4.4373853384498645
898, epoch_train_loss=4.4373853384498645
Epoch 899
Epoch 899 :: Batch 0/1
Batch Loss = 4.43738533651588
899, epoch_train_loss=4.43738533651588
Epoch 900
Epoch 900 :: Batch 0/1
Batch Loss = 4.437385334587041
900, epoch_train_loss=4.437385334587041
Epoch 901
Epoch 901 :: Batch 0/1
Batch Loss = 4.437385332663323
901, epoch_train_loss=4.437385332663323
Epoch 902
Epoch 902 :: Batch 0/1
Batch Loss = 4.437385330744703
902, epoch_train_loss=4.437385330744703
Epoch 903
Epoch 903 :: Batch 0/1
Batch Loss = 4.437385328831157
903, epoch_train_loss=4.437385328831157
Epoch 904
Epoch 904 :: Batch 0/1
Batch Loss = 4.43738532692266
904, epoch_train_loss=4.43738532692266
Epoch 905
Epoch 905 :: Batch 0/1
Batch Loss = 4.437385325019193
905, epoch_train_loss=4.437385325019193
Epoch 906
Epoch 906 :: Batch 0/1
Batch Loss = 4.4373853231207265
906, epoch_train_loss=4.4373853231207265
Epoch 907
Epoch 907 :: Batch 0/1
Batch Loss = 4.437385321227242
907, epoch_train_loss=4.437385321227242
Epoch 908
Epoch 908 :: Batch 0/1
Batch Loss = 4.437385319338714
908, epoch_train_loss=4.437385319338714
Epoch 909
Epoch 909 :: Batch 0/1
Batch Loss = 4.437385317455122
909, epoch_train_loss=4.437385317455122
Epoch 910
Epoch 910 :: Batch 0/1
Batch Loss = 4.43738531557644
910, epoch_train_loss=4.43738531557644
Epoch 911
Epoch 911 :: Batch 0/1
Batch Loss = 4.437385313702646
911, epoch_train_loss=4.437385313702646
Epoch 912
Epoch 912 :: Batch 0/1
Batch Loss = 4.43738531183372
912, epoch_train_loss=4.43738531183372
Epoch 913
Epoch 913 :: Batch 0/1
Batch Loss = 4.437385309969638
913, epoch_train_loss=4.437385309969638
Epoch 914
Epoch 914 :: Batch 0/1
Batch Loss = 4.437385308110378
914, epoch_train_loss=4.437385308110378
Epoch 915
Epoch 915 :: Batch 0/1
Batch Loss = 4.4373853062559165
915, epoch_train_loss=4.4373853062559165
Epoch 916
Epoch 916 :: Batch 0/1
Batch Loss = 4.437385304406233
916, epoch_train_loss=4.437385304406233
Epoch 917
Epoch 917 :: Batch 0/1
Batch Loss = 4.437385302561305
917, epoch_train_loss=4.437385302561305
Epoch 918
Epoch 918 :: Batch 0/1
Batch Loss = 4.437385300721112
918, epoch_train_loss=4.437385300721112
Epoch 919
Epoch 919 :: Batch 0/1
Batch Loss = 4.437385298885631
919, epoch_train_loss=4.437385298885631
Epoch 920
Epoch 920 :: Batch 0/1
Batch Loss = 4.437385297054839
920, epoch_train_loss=4.437385297054839
Epoch 921
Epoch 921 :: Batch 0/1
Batch Loss = 4.437385295228718
921, epoch_train_loss=4.437385295228718
Epoch 922
Epoch 922 :: Batch 0/1
Batch Loss = 4.437385293407243
922, epoch_train_loss=4.437385293407243
Epoch 923
Epoch 923 :: Batch 0/1
Batch Loss = 4.437385291590397
923, epoch_train_loss=4.437385291590397
Epoch 924
Epoch 924 :: Batch 0/1
Batch Loss = 4.437385289778155
924, epoch_train_loss=4.437385289778155
Epoch 925
Epoch 925 :: Batch 0/1
Batch Loss = 4.437385287970499
925, epoch_train_loss=4.437385287970499
Epoch 926
Epoch 926 :: Batch 0/1
Batch Loss = 4.437385286167407
926, epoch_train_loss=4.437385286167407
Epoch 927
Epoch 927 :: Batch 0/1
Batch Loss = 4.437385284368857
927, epoch_train_loss=4.437385284368857
Epoch 928
Epoch 928 :: Batch 0/1
Batch Loss = 4.43738528257483
928, epoch_train_loss=4.43738528257483
Epoch 929
Epoch 929 :: Batch 0/1
Batch Loss = 4.437385280785305
929, epoch_train_loss=4.437385280785305
Epoch 930
Epoch 930 :: Batch 0/1
Batch Loss = 4.437385279000264
930, epoch_train_loss=4.437385279000264
Epoch 931
Epoch 931 :: Batch 0/1
Batch Loss = 4.437385277219682
931, epoch_train_loss=4.437385277219682
Epoch 932
Epoch 932 :: Batch 0/1
Batch Loss = 4.4373852754435426
932, epoch_train_loss=4.4373852754435426
Epoch 933
Epoch 933 :: Batch 0/1
Batch Loss = 4.437385273671822
933, epoch_train_loss=4.437385273671822
Epoch 934
Epoch 934 :: Batch 0/1
Batch Loss = 4.437385271904506
934, epoch_train_loss=4.437385271904506
Epoch 935
Epoch 935 :: Batch 0/1
Batch Loss = 4.43738527014157
935, epoch_train_loss=4.43738527014157
Epoch 936
Epoch 936 :: Batch 0/1
Batch Loss = 4.437385268382995
936, epoch_train_loss=4.437385268382995
Epoch 937
Epoch 937 :: Batch 0/1
Batch Loss = 4.4373852666287625
937, epoch_train_loss=4.4373852666287625
Epoch 938
Epoch 938 :: Batch 0/1
Batch Loss = 4.437385264878855
938, epoch_train_loss=4.437385264878855
Epoch 939
Epoch 939 :: Batch 0/1
Batch Loss = 4.437385263133247
939, epoch_train_loss=4.437385263133247
Epoch 940
Epoch 940 :: Batch 0/1
Batch Loss = 4.437385261391924
940, epoch_train_loss=4.437385261391924
Epoch 941
Epoch 941 :: Batch 0/1
Batch Loss = 4.437385259654867
941, epoch_train_loss=4.437385259654867
Epoch 942
Epoch 942 :: Batch 0/1
Batch Loss = 4.437385257922055
942, epoch_train_loss=4.437385257922055
Epoch 943
Epoch 943 :: Batch 0/1
Batch Loss = 4.437385256193471
943, epoch_train_loss=4.437385256193471
Epoch 944
Epoch 944 :: Batch 0/1
Batch Loss = 4.437385254469093
944, epoch_train_loss=4.437385254469093
Epoch 945
Epoch 945 :: Batch 0/1
Batch Loss = 4.4373852527489035
945, epoch_train_loss=4.4373852527489035
Epoch 946
Epoch 946 :: Batch 0/1
Batch Loss = 4.437385251032885
946, epoch_train_loss=4.437385251032885
Epoch 947
Epoch 947 :: Batch 0/1
Batch Loss = 4.437385249321019
947, epoch_train_loss=4.437385249321019
Epoch 948
Epoch 948 :: Batch 0/1
Batch Loss = 4.437385247613285
948, epoch_train_loss=4.437385247613285
Epoch 949
Epoch 949 :: Batch 0/1
Batch Loss = 4.437385245909668
949, epoch_train_loss=4.437385245909668
Epoch 950
Epoch 950 :: Batch 0/1
Batch Loss = 4.437385244210145
950, epoch_train_loss=4.437385244210145
Epoch 951
Epoch 951 :: Batch 0/1
Batch Loss = 4.437385242514702
951, epoch_train_loss=4.437385242514702
Epoch 952
Epoch 952 :: Batch 0/1
Batch Loss = 4.437385240823317
952, epoch_train_loss=4.437385240823317
Epoch 953
Epoch 953 :: Batch 0/1
Batch Loss = 4.437385239135976
953, epoch_train_loss=4.437385239135976
Epoch 954
Epoch 954 :: Batch 0/1
Batch Loss = 4.437385237452659
954, epoch_train_loss=4.437385237452659
Epoch 955
Epoch 955 :: Batch 0/1
Batch Loss = 4.437385235773348
955, epoch_train_loss=4.437385235773348
Epoch 956
Epoch 956 :: Batch 0/1
Batch Loss = 4.437385234098026
956, epoch_train_loss=4.437385234098026
Epoch 957
Epoch 957 :: Batch 0/1
Batch Loss = 4.437385232426673
957, epoch_train_loss=4.437385232426673
Epoch 958
Epoch 958 :: Batch 0/1
Batch Loss = 4.4373852307592765
958, epoch_train_loss=4.4373852307592765
Epoch 959
Epoch 959 :: Batch 0/1
Batch Loss = 4.437385229095815
959, epoch_train_loss=4.437385229095815
Epoch 960
Epoch 960 :: Batch 0/1
Batch Loss = 4.4373852274362715
960, epoch_train_loss=4.4373852274362715
Epoch 961
Epoch 961 :: Batch 0/1
Batch Loss = 4.437385225780631
961, epoch_train_loss=4.437385225780631
Epoch 962
Epoch 962 :: Batch 0/1
Batch Loss = 4.437385224128874
962, epoch_train_loss=4.437385224128874
Epoch 963
Epoch 963 :: Batch 0/1
Batch Loss = 4.437385222480984
963, epoch_train_loss=4.437385222480984
Epoch 964
Epoch 964 :: Batch 0/1
Batch Loss = 4.437385220836943
964, epoch_train_loss=4.437385220836943
Epoch 965
Epoch 965 :: Batch 0/1
Batch Loss = 4.437385219196736
965, epoch_train_loss=4.437385219196736
Epoch 966
Epoch 966 :: Batch 0/1
Batch Loss = 4.437385217560346
966, epoch_train_loss=4.437385217560346
Epoch 967
Epoch 967 :: Batch 0/1
Batch Loss = 4.437385215927755
967, epoch_train_loss=4.437385215927755
Epoch 968
Epoch 968 :: Batch 0/1
Batch Loss = 4.4373852142989465
968, epoch_train_loss=4.4373852142989465
Epoch 969
Epoch 969 :: Batch 0/1
Batch Loss = 4.437385212673904
969, epoch_train_loss=4.437385212673904
Epoch 970
Epoch 970 :: Batch 0/1
Batch Loss = 4.437385211052612
970, epoch_train_loss=4.437385211052612
Epoch 971
Epoch 971 :: Batch 0/1
Batch Loss = 4.437385209435053
971, epoch_train_loss=4.437385209435053
Epoch 972
Epoch 972 :: Batch 0/1
Batch Loss = 4.437385207821212
972, epoch_train_loss=4.437385207821212
Epoch 973
Epoch 973 :: Batch 0/1
Batch Loss = 4.437385206211071
973, epoch_train_loss=4.437385206211071
Epoch 974
Epoch 974 :: Batch 0/1
Batch Loss = 4.437385204604615
974, epoch_train_loss=4.437385204604615
Epoch 975
Epoch 975 :: Batch 0/1
Batch Loss = 4.437385203001829
975, epoch_train_loss=4.437385203001829
Epoch 976
Epoch 976 :: Batch 0/1
Batch Loss = 4.4373852014026935
976, epoch_train_loss=4.4373852014026935
Epoch 977
Epoch 977 :: Batch 0/1
Batch Loss = 4.437385199807194
977, epoch_train_loss=4.437385199807194
Epoch 978
Epoch 978 :: Batch 0/1
Batch Loss = 4.437385198215316
978, epoch_train_loss=4.437385198215316
Epoch 979
Epoch 979 :: Batch 0/1
Batch Loss = 4.437385196627044
979, epoch_train_loss=4.437385196627044
Epoch 980
Epoch 980 :: Batch 0/1
Batch Loss = 4.437385195042362
980, epoch_train_loss=4.437385195042362
Epoch 981
Epoch 981 :: Batch 0/1
Batch Loss = 4.437385193461251
981, epoch_train_loss=4.437385193461251
Epoch 982
Epoch 982 :: Batch 0/1
Batch Loss = 4.437385191883701
982, epoch_train_loss=4.437385191883701
Epoch 983
Epoch 983 :: Batch 0/1
Batch Loss = 4.437385190309692
983, epoch_train_loss=4.437385190309692
Epoch 984
Epoch 984 :: Batch 0/1
Batch Loss = 4.437385188739211
984, epoch_train_loss=4.437385188739211
Epoch 985
Epoch 985 :: Batch 0/1
Batch Loss = 4.437385187172242
985, epoch_train_loss=4.437385187172242
Epoch 986
Epoch 986 :: Batch 0/1
Batch Loss = 4.43738518560877
986, epoch_train_loss=4.43738518560877
Epoch 987
Epoch 987 :: Batch 0/1
Batch Loss = 4.437385184048779
987, epoch_train_loss=4.437385184048779
Epoch 988
Epoch 988 :: Batch 0/1
Batch Loss = 4.437385182492255
988, epoch_train_loss=4.437385182492255
Epoch 989
Epoch 989 :: Batch 0/1
Batch Loss = 4.4373851809391835
989, epoch_train_loss=4.4373851809391835
Epoch 990
Epoch 990 :: Batch 0/1
Batch Loss = 4.437385179389548
990, epoch_train_loss=4.437385179389548
Epoch 991
Epoch 991 :: Batch 0/1
Batch Loss = 4.437385177843334
991, epoch_train_loss=4.437385177843334
Epoch 992
Epoch 992 :: Batch 0/1
Batch Loss = 4.437385176300527
992, epoch_train_loss=4.437385176300527
Epoch 993
Epoch 993 :: Batch 0/1
Batch Loss = 4.437385174761112
993, epoch_train_loss=4.437385174761112
Epoch 994
Epoch 994 :: Batch 0/1
Batch Loss = 4.437385173225076
994, epoch_train_loss=4.437385173225076
Epoch 995
Epoch 995 :: Batch 0/1
Batch Loss = 4.437385171692401
995, epoch_train_loss=4.437385171692401
Epoch 996
Epoch 996 :: Batch 0/1
Batch Loss = 4.437385170163076
996, epoch_train_loss=4.437385170163076
Epoch 997
Epoch 997 :: Batch 0/1
Batch Loss = 4.437385168637085
997, epoch_train_loss=4.437385168637085
Epoch 998
Epoch 998 :: Batch 0/1
Batch Loss = 4.437385167114415
998, epoch_train_loss=4.437385167114415
Epoch 999
Epoch 999 :: Batch 0/1
Batch Loss = 4.437385165595047
999, epoch_train_loss=4.437385165595047
Epoch 1000
Epoch 1000 :: Batch 0/1
Batch Loss = 4.437385164078973
1000, epoch_train_loss=4.437385164078973
Epoch 1001
Epoch 1001 :: Batch 0/1
Batch Loss = 4.437385162566176
1001, epoch_train_loss=4.437385162566176
Epoch 1002
Epoch 1002 :: Batch 0/1
Batch Loss = 4.437385161056642
1002, epoch_train_loss=4.437385161056642
Epoch 1003
Epoch 1003 :: Batch 0/1
Batch Loss = 4.437385159550357
1003, epoch_train_loss=4.437385159550357
Epoch 1004
Epoch 1004 :: Batch 0/1
Batch Loss = 4.437385158047307
1004, epoch_train_loss=4.437385158047307
Epoch 1005
Epoch 1005 :: Batch 0/1
Batch Loss = 4.437385156547478
1005, epoch_train_loss=4.437385156547478
Epoch 1006
Epoch 1006 :: Batch 0/1
Batch Loss = 4.437385155050855
1006, epoch_train_loss=4.437385155050855
Epoch 1007
Epoch 1007 :: Batch 0/1
Batch Loss = 4.437385153557427
1007, epoch_train_loss=4.437385153557427
Epoch 1008
Epoch 1008 :: Batch 0/1
Batch Loss = 4.437385152067179
1008, epoch_train_loss=4.437385152067179
Epoch 1009
Epoch 1009 :: Batch 0/1
Batch Loss = 4.437385150580097
1009, epoch_train_loss=4.437385150580097
Epoch 1010
Epoch 1010 :: Batch 0/1
Batch Loss = 4.437385149096167
1010, epoch_train_loss=4.437385149096167
Epoch 1011
Epoch 1011 :: Batch 0/1
Batch Loss = 4.437385147615378
1011, epoch_train_loss=4.437385147615378
Epoch 1012
Epoch 1012 :: Batch 0/1
Batch Loss = 4.4373851461377125
1012, epoch_train_loss=4.4373851461377125
Epoch 1013
Epoch 1013 :: Batch 0/1
Batch Loss = 4.4373851446631605
1013, epoch_train_loss=4.4373851446631605
Epoch 1014
Epoch 1014 :: Batch 0/1
Batch Loss = 4.437385143191708
1014, epoch_train_loss=4.437385143191708
Epoch 1015
Epoch 1015 :: Batch 0/1
Batch Loss = 4.4373851417233405
1015, epoch_train_loss=4.4373851417233405
Epoch 1016
Epoch 1016 :: Batch 0/1
Batch Loss = 4.437385140258046
1016, epoch_train_loss=4.437385140258046
Epoch 1017
Epoch 1017 :: Batch 0/1
Batch Loss = 4.437385138795811
1017, epoch_train_loss=4.437385138795811
Epoch 1018
Epoch 1018 :: Batch 0/1
Batch Loss = 4.437385137336622
1018, epoch_train_loss=4.437385137336622
Epoch 1019
Epoch 1019 :: Batch 0/1
Batch Loss = 4.437385135880467
1019, epoch_train_loss=4.437385135880467
Epoch 1020
Epoch 1020 :: Batch 0/1
Batch Loss = 4.437385134427332
1020, epoch_train_loss=4.437385134427332
Epoch 1021
Epoch 1021 :: Batch 0/1
Batch Loss = 4.437385132977206
1021, epoch_train_loss=4.437385132977206
Epoch 1022
Epoch 1022 :: Batch 0/1
Batch Loss = 4.437385131530074
1022, epoch_train_loss=4.437385131530074
Epoch 1023
Epoch 1023 :: Batch 0/1
Batch Loss = 4.437385130085923
1023, epoch_train_loss=4.437385130085923
Epoch 1024
Epoch 1024 :: Batch 0/1
Batch Loss = 4.437385128644742
1024, epoch_train_loss=4.437385128644742
Epoch 1025
Epoch 1025 :: Batch 0/1
Batch Loss = 4.437385127206517
1025, epoch_train_loss=4.437385127206517
Epoch 1026
Epoch 1026 :: Batch 0/1
Batch Loss = 4.437385125771237
1026, epoch_train_loss=4.437385125771237
Epoch 1027
Epoch 1027 :: Batch 0/1
Batch Loss = 4.4373851243388875
1027, epoch_train_loss=4.4373851243388875
Epoch 1028
Epoch 1028 :: Batch 0/1
Batch Loss = 4.4373851229094585
1028, epoch_train_loss=4.4373851229094585
Epoch 1029
Epoch 1029 :: Batch 0/1
Batch Loss = 4.437385121482935
1029, epoch_train_loss=4.437385121482935
Epoch 1030
Epoch 1030 :: Batch 0/1
Batch Loss = 4.437385120059306
1030, epoch_train_loss=4.437385120059306
Epoch 1031
Epoch 1031 :: Batch 0/1
Batch Loss = 4.4373851186385584
1031, epoch_train_loss=4.4373851186385584
Epoch 1032
Epoch 1032 :: Batch 0/1
Batch Loss = 4.437385117220682
1032, epoch_train_loss=4.437385117220682
Epoch 1033
Epoch 1033 :: Batch 0/1
Batch Loss = 4.437385115805662
1033, epoch_train_loss=4.437385115805662
Epoch 1034
Epoch 1034 :: Batch 0/1
Batch Loss = 4.437385114393487
1034, epoch_train_loss=4.437385114393487
Epoch 1035
Epoch 1035 :: Batch 0/1
Batch Loss = 4.4373851129841455
1035, epoch_train_loss=4.4373851129841455
Epoch 1036
Epoch 1036 :: Batch 0/1
Batch Loss = 4.437385111577626
1036, epoch_train_loss=4.437385111577626
Epoch 1037
Epoch 1037 :: Batch 0/1
Batch Loss = 4.437385110173914
1037, epoch_train_loss=4.437385110173914
Epoch 1038
Epoch 1038 :: Batch 0/1
Batch Loss = 4.437385108773001
1038, epoch_train_loss=4.437385108773001
Epoch 1039
Epoch 1039 :: Batch 0/1
Batch Loss = 4.4373851073748725
1039, epoch_train_loss=4.4373851073748725
Epoch 1040
Epoch 1040 :: Batch 0/1
Batch Loss = 4.437385105979518
1040, epoch_train_loss=4.437385105979518
Epoch 1041
Epoch 1041 :: Batch 0/1
Batch Loss = 4.437385104586925
1041, epoch_train_loss=4.437385104586925
Epoch 1042
Epoch 1042 :: Batch 0/1
Batch Loss = 4.437385103197082
1042, epoch_train_loss=4.437385103197082
Epoch 1043
Epoch 1043 :: Batch 0/1
Batch Loss = 4.437385101809977
1043, epoch_train_loss=4.437385101809977
Epoch 1044
Epoch 1044 :: Batch 0/1
Batch Loss = 4.437385100425599
1044, epoch_train_loss=4.437385100425599
Epoch 1045
Epoch 1045 :: Batch 0/1
Batch Loss = 4.437385099043937
1045, epoch_train_loss=4.437385099043937
Epoch 1046
Epoch 1046 :: Batch 0/1
Batch Loss = 4.437385097664979
1046, epoch_train_loss=4.437385097664979
Epoch 1047
Epoch 1047 :: Batch 0/1
Batch Loss = 4.437385096288712
1047, epoch_train_loss=4.437385096288712
Epoch 1048
Epoch 1048 :: Batch 0/1
Batch Loss = 4.437385094915126
1048, epoch_train_loss=4.437385094915126
Epoch 1049
Epoch 1049 :: Batch 0/1
Batch Loss = 4.4373850935442105
1049, epoch_train_loss=4.4373850935442105
Epoch 1050
Epoch 1050 :: Batch 0/1
Batch Loss = 4.437385092175952
1050, epoch_train_loss=4.437385092175952
Epoch 1051
Epoch 1051 :: Batch 0/1
Batch Loss = 4.43738509081034
1051, epoch_train_loss=4.43738509081034
Epoch 1052
Epoch 1052 :: Batch 0/1
Batch Loss = 4.437385089447365
1052, epoch_train_loss=4.437385089447365
Epoch 1053
Epoch 1053 :: Batch 0/1
Batch Loss = 4.437385088087014
1053, epoch_train_loss=4.437385088087014
Epoch 1054
Epoch 1054 :: Batch 0/1
Batch Loss = 4.437385086729276
1054, epoch_train_loss=4.437385086729276
Epoch 1055
Epoch 1055 :: Batch 0/1
Batch Loss = 4.437385085374141
1055, epoch_train_loss=4.437385085374141
Epoch 1056
Epoch 1056 :: Batch 0/1
Batch Loss = 4.437385084021596
1056, epoch_train_loss=4.437385084021596
Epoch 1057
Epoch 1057 :: Batch 0/1
Batch Loss = 4.437385082671632
1057, epoch_train_loss=4.437385082671632
Epoch 1058
Epoch 1058 :: Batch 0/1
Batch Loss = 4.437385081324236
1058, epoch_train_loss=4.437385081324236
Epoch 1059
Epoch 1059 :: Batch 0/1
Batch Loss = 4.437385079979401
1059, epoch_train_loss=4.437385079979401
Epoch 1060
Epoch 1060 :: Batch 0/1
Batch Loss = 4.437385078637113
1060, epoch_train_loss=4.437385078637113
Epoch 1061
Epoch 1061 :: Batch 0/1
Batch Loss = 4.43738507729736
1061, epoch_train_loss=4.43738507729736
Epoch 1062
Epoch 1062 :: Batch 0/1
Batch Loss = 4.437385075960135
1062, epoch_train_loss=4.437385075960135
Epoch 1063
Epoch 1063 :: Batch 0/1
Batch Loss = 4.437385074625424
1063, epoch_train_loss=4.437385074625424
Epoch 1064
Epoch 1064 :: Batch 0/1
Batch Loss = 4.437385073293218
1064, epoch_train_loss=4.437385073293218
Epoch 1065
Epoch 1065 :: Batch 0/1
Batch Loss = 4.437385071963506
1065, epoch_train_loss=4.437385071963506
Epoch 1066
Epoch 1066 :: Batch 0/1
Batch Loss = 4.437385070636278
1066, epoch_train_loss=4.437385070636278
Epoch 1067
Epoch 1067 :: Batch 0/1
Batch Loss = 4.4373850693115235
1067, epoch_train_loss=4.4373850693115235
Epoch 1068
Epoch 1068 :: Batch 0/1
Batch Loss = 4.437385067989229
1068, epoch_train_loss=4.437385067989229
Epoch 1069
Epoch 1069 :: Batch 0/1
Batch Loss = 4.437385066669389
1069, epoch_train_loss=4.437385066669389
Epoch 1070
Epoch 1070 :: Batch 0/1
Batch Loss = 4.43738506535199
1070, epoch_train_loss=4.43738506535199
Epoch 1071
Epoch 1071 :: Batch 0/1
Batch Loss = 4.437385064037022
1071, epoch_train_loss=4.437385064037022
Epoch 1072
Epoch 1072 :: Batch 0/1
Batch Loss = 4.437385062724475
1072, epoch_train_loss=4.437385062724475
Epoch 1073
Epoch 1073 :: Batch 0/1
Batch Loss = 4.437385061414339
1073, epoch_train_loss=4.437385061414339
Epoch 1074
Epoch 1074 :: Batch 0/1
Batch Loss = 4.437385060106604
1074, epoch_train_loss=4.437385060106604
Epoch 1075
Epoch 1075 :: Batch 0/1
Batch Loss = 4.437385058801259
1075, epoch_train_loss=4.437385058801259
Epoch 1076
Epoch 1076 :: Batch 0/1
Batch Loss = 4.437385057498294
1076, epoch_train_loss=4.437385057498294
Epoch 1077
Epoch 1077 :: Batch 0/1
Batch Loss = 4.437385056197699
1077, epoch_train_loss=4.437385056197699
Epoch 1078
Epoch 1078 :: Batch 0/1
Batch Loss = 4.437385054899465
1078, epoch_train_loss=4.437385054899465
Epoch 1079
Epoch 1079 :: Batch 0/1
Batch Loss = 4.437385053603579
1079, epoch_train_loss=4.437385053603579
Epoch 1080
Epoch 1080 :: Batch 0/1
Batch Loss = 4.437385052310035
1080, epoch_train_loss=4.437385052310035
Epoch 1081
Epoch 1081 :: Batch 0/1
Batch Loss = 4.437385051018819
1081, epoch_train_loss=4.437385051018819
Epoch 1082
Epoch 1082 :: Batch 0/1
Batch Loss = 4.437385049729926
1082, epoch_train_loss=4.437385049729926
Epoch 1083
Epoch 1083 :: Batch 0/1
Batch Loss = 4.437385048443342
1083, epoch_train_loss=4.437385048443342
Epoch 1084
Epoch 1084 :: Batch 0/1
Batch Loss = 4.437385047159059
1084, epoch_train_loss=4.437385047159059
Epoch 1085
Epoch 1085 :: Batch 0/1
Batch Loss = 4.4373850458770665
1085, epoch_train_loss=4.4373850458770665
Epoch 1086
Epoch 1086 :: Batch 0/1
Batch Loss = 4.437385044597353
1086, epoch_train_loss=4.437385044597353
Epoch 1087
Epoch 1087 :: Batch 0/1
Batch Loss = 4.437385043319915
1087, epoch_train_loss=4.437385043319915
Epoch 1088
Epoch 1088 :: Batch 0/1
Batch Loss = 4.437385042044735
1088, epoch_train_loss=4.437385042044735
Epoch 1089
Epoch 1089 :: Batch 0/1
Batch Loss = 4.437385040771808
1089, epoch_train_loss=4.437385040771808
Epoch 1090
Epoch 1090 :: Batch 0/1
Batch Loss = 4.437385039501124
1090, epoch_train_loss=4.437385039501124
Epoch 1091
Epoch 1091 :: Batch 0/1
Batch Loss = 4.437385038232672
1091, epoch_train_loss=4.437385038232672
Epoch 1092
Epoch 1092 :: Batch 0/1
Batch Loss = 4.437385036966445
1092, epoch_train_loss=4.437385036966445
Epoch 1093
Epoch 1093 :: Batch 0/1
Batch Loss = 4.437385035702432
1093, epoch_train_loss=4.437385035702432
Epoch 1094
Epoch 1094 :: Batch 0/1
Batch Loss = 4.437385034440622
1094, epoch_train_loss=4.437385034440622
Epoch 1095
Epoch 1095 :: Batch 0/1
Batch Loss = 4.437385033181008
1095, epoch_train_loss=4.437385033181008
Epoch 1096
Epoch 1096 :: Batch 0/1
Batch Loss = 4.43738503192358
1096, epoch_train_loss=4.43738503192358
Epoch 1097
Epoch 1097 :: Batch 0/1
Batch Loss = 4.437385030668327
1097, epoch_train_loss=4.437385030668327
Epoch 1098
Epoch 1098 :: Batch 0/1
Batch Loss = 4.4373850294152435
1098, epoch_train_loss=4.4373850294152435
Epoch 1099
Epoch 1099 :: Batch 0/1
Batch Loss = 4.437385028164318
1099, epoch_train_loss=4.437385028164318
Epoch 1100
Epoch 1100 :: Batch 0/1
Batch Loss = 4.437385026915539
1100, epoch_train_loss=4.437385026915539
Epoch 1101
Epoch 1101 :: Batch 0/1
Batch Loss = 4.437385025668901
1101, epoch_train_loss=4.437385025668901
Epoch 1102
Epoch 1102 :: Batch 0/1
Batch Loss = 4.437385024424393
1102, epoch_train_loss=4.437385024424393
Epoch 1103
Epoch 1103 :: Batch 0/1
Batch Loss = 4.437385023182005
1103, epoch_train_loss=4.437385023182005
Epoch 1104
Epoch 1104 :: Batch 0/1
Batch Loss = 4.437385021941731
1104, epoch_train_loss=4.437385021941731
Epoch 1105
Epoch 1105 :: Batch 0/1
Batch Loss = 4.437385020703561
1105, epoch_train_loss=4.437385020703561
Epoch 1106
Epoch 1106 :: Batch 0/1
Batch Loss = 4.437385019467483
1106, epoch_train_loss=4.437385019467483
Epoch 1107
Epoch 1107 :: Batch 0/1
Batch Loss = 4.437385018233492
1107, epoch_train_loss=4.437385018233492
Epoch 1108
Epoch 1108 :: Batch 0/1
Batch Loss = 4.437385017001575
1108, epoch_train_loss=4.437385017001575
Epoch 1109
Epoch 1109 :: Batch 0/1
Batch Loss = 4.437385015771728
1109, epoch_train_loss=4.437385015771728
Epoch 1110
Epoch 1110 :: Batch 0/1
Batch Loss = 4.4373850145439375
1110, epoch_train_loss=4.4373850145439375
Epoch 1111
Epoch 1111 :: Batch 0/1
Batch Loss = 4.437385013318198
1111, epoch_train_loss=4.437385013318198
Epoch 1112
Epoch 1112 :: Batch 0/1
Batch Loss = 4.4373850120944995
1112, epoch_train_loss=4.4373850120944995
Epoch 1113
Epoch 1113 :: Batch 0/1
Batch Loss = 4.437385010872832
1113, epoch_train_loss=4.437385010872832
Epoch 1114
Epoch 1114 :: Batch 0/1
Batch Loss = 4.43738500965319
1114, epoch_train_loss=4.43738500965319
Epoch 1115
Epoch 1115 :: Batch 0/1
Batch Loss = 4.437385008435561
1115, epoch_train_loss=4.437385008435561
Epoch 1116
Epoch 1116 :: Batch 0/1
Batch Loss = 4.437385007219938
1116, epoch_train_loss=4.437385007219938
Epoch 1117
Epoch 1117 :: Batch 0/1
Batch Loss = 4.437385006006312
1117, epoch_train_loss=4.437385006006312
Epoch 1118
Epoch 1118 :: Batch 0/1
Batch Loss = 4.437385004794677
1118, epoch_train_loss=4.437385004794677
Epoch 1119
Epoch 1119 :: Batch 0/1
Batch Loss = 4.437385003585021
1119, epoch_train_loss=4.437385003585021
Epoch 1120
Epoch 1120 :: Batch 0/1
Batch Loss = 4.437385002377336
1120, epoch_train_loss=4.437385002377336
Epoch 1121
Epoch 1121 :: Batch 0/1
Batch Loss = 4.437385001171615
1121, epoch_train_loss=4.437385001171615
Epoch 1122
Epoch 1122 :: Batch 0/1
Batch Loss = 4.437384999967848
1122, epoch_train_loss=4.437384999967848
Epoch 1123
Epoch 1123 :: Batch 0/1
Batch Loss = 4.437384998766028
1123, epoch_train_loss=4.437384998766028
Epoch 1124
Epoch 1124 :: Batch 0/1
Batch Loss = 4.437384997566146
1124, epoch_train_loss=4.437384997566146
Epoch 1125
Epoch 1125 :: Batch 0/1
Batch Loss = 4.437384996368194
1125, epoch_train_loss=4.437384996368194
Epoch 1126
Epoch 1126 :: Batch 0/1
Batch Loss = 4.437384995172161
1126, epoch_train_loss=4.437384995172161
Epoch 1127
Epoch 1127 :: Batch 0/1
Batch Loss = 4.437384993978043
1127, epoch_train_loss=4.437384993978043
Epoch 1128
Epoch 1128 :: Batch 0/1
Batch Loss = 4.437384992785828
1128, epoch_train_loss=4.437384992785828
Epoch 1129
Epoch 1129 :: Batch 0/1
Batch Loss = 4.43738499159551
1129, epoch_train_loss=4.43738499159551
Epoch 1130
Epoch 1130 :: Batch 0/1
Batch Loss = 4.437384990407081
1130, epoch_train_loss=4.437384990407081
Epoch 1131
Epoch 1131 :: Batch 0/1
Batch Loss = 4.437384989220531
1131, epoch_train_loss=4.437384989220531
Epoch 1132
Epoch 1132 :: Batch 0/1
Batch Loss = 4.437384988035852
1132, epoch_train_loss=4.437384988035852
Epoch 1133
Epoch 1133 :: Batch 0/1
Batch Loss = 4.437384986853037
1133, epoch_train_loss=4.437384986853037
Epoch 1134
Epoch 1134 :: Batch 0/1
Batch Loss = 4.437384985672077
1134, epoch_train_loss=4.437384985672077
Epoch 1135
Epoch 1135 :: Batch 0/1
Batch Loss = 4.437384984492965
1135, epoch_train_loss=4.437384984492965
Epoch 1136
Epoch 1136 :: Batch 0/1
Batch Loss = 4.437384983315693
1136, epoch_train_loss=4.437384983315693
Epoch 1137
Epoch 1137 :: Batch 0/1
Batch Loss = 4.43738498214025
1137, epoch_train_loss=4.43738498214025
Epoch 1138
Epoch 1138 :: Batch 0/1
Batch Loss = 4.437384980966632
1138, epoch_train_loss=4.437384980966632
Epoch 1139
Epoch 1139 :: Batch 0/1
Batch Loss = 4.437384979794829
1139, epoch_train_loss=4.437384979794829
Epoch 1140
Epoch 1140 :: Batch 0/1
Batch Loss = 4.437384978624834
1140, epoch_train_loss=4.437384978624834
Epoch 1141
Epoch 1141 :: Batch 0/1
Batch Loss = 4.437384977456637
1141, epoch_train_loss=4.437384977456637
Epoch 1142
Epoch 1142 :: Batch 0/1
Batch Loss = 4.437384976290233
1142, epoch_train_loss=4.437384976290233
Epoch 1143
Epoch 1143 :: Batch 0/1
Batch Loss = 4.43738497512561
1143, epoch_train_loss=4.43738497512561
Epoch 1144
Epoch 1144 :: Batch 0/1
Batch Loss = 4.437384973962766
1144, epoch_train_loss=4.437384973962766
Epoch 1145
Epoch 1145 :: Batch 0/1
Batch Loss = 4.437384972801689
1145, epoch_train_loss=4.437384972801689
Epoch 1146
Epoch 1146 :: Batch 0/1
Batch Loss = 4.4373849716423726
1146, epoch_train_loss=4.4373849716423726
Epoch 1147
Epoch 1147 :: Batch 0/1
Batch Loss = 4.437384970484807
1147, epoch_train_loss=4.437384970484807
Epoch 1148
Epoch 1148 :: Batch 0/1
Batch Loss = 4.437384969328987
1148, epoch_train_loss=4.437384969328987
Epoch 1149
Epoch 1149 :: Batch 0/1
Batch Loss = 4.437384968174905
1149, epoch_train_loss=4.437384968174905
Epoch 1150
Epoch 1150 :: Batch 0/1
Batch Loss = 4.437384967022552
1150, epoch_train_loss=4.437384967022552
Epoch 1151
Epoch 1151 :: Batch 0/1
Batch Loss = 4.43738496587192
1151, epoch_train_loss=4.43738496587192
Epoch 1152
Epoch 1152 :: Batch 0/1
Batch Loss = 4.437384964723003
1152, epoch_train_loss=4.437384964723003
Epoch 1153
Epoch 1153 :: Batch 0/1
Batch Loss = 4.437384963575792
1153, epoch_train_loss=4.437384963575792
Epoch 1154
Epoch 1154 :: Batch 0/1
Batch Loss = 4.43738496243028
1154, epoch_train_loss=4.43738496243028
Epoch 1155
Epoch 1155 :: Batch 0/1
Batch Loss = 4.43738496128646
1155, epoch_train_loss=4.43738496128646
Epoch 1156
Epoch 1156 :: Batch 0/1
Batch Loss = 4.437384960144324
1156, epoch_train_loss=4.437384960144324
Epoch 1157
Epoch 1157 :: Batch 0/1
Batch Loss = 4.437384959003864
1157, epoch_train_loss=4.437384959003864
Epoch 1158
Epoch 1158 :: Batch 0/1
Batch Loss = 4.437384957865072
1158, epoch_train_loss=4.437384957865072
Epoch 1159
Epoch 1159 :: Batch 0/1
Batch Loss = 4.437384956727943
1159, epoch_train_loss=4.437384956727943
Epoch 1160
Epoch 1160 :: Batch 0/1
Batch Loss = 4.437384955592467
1160, epoch_train_loss=4.437384955592467
Epoch 1161
Epoch 1161 :: Batch 0/1
Batch Loss = 4.437384954458639
1161, epoch_train_loss=4.437384954458639
Epoch 1162
Epoch 1162 :: Batch 0/1
Batch Loss = 4.43738495332645
1162, epoch_train_loss=4.43738495332645
Epoch 1163
Epoch 1163 :: Batch 0/1
Batch Loss = 4.437384952195892
1163, epoch_train_loss=4.437384952195892
Epoch 1164
Epoch 1164 :: Batch 0/1
Batch Loss = 4.43738495106696
1164, epoch_train_loss=4.43738495106696
Epoch 1165
Epoch 1165 :: Batch 0/1
Batch Loss = 4.437384949939645
1165, epoch_train_loss=4.437384949939645
Epoch 1166
Epoch 1166 :: Batch 0/1
Batch Loss = 4.43738494881394
1166, epoch_train_loss=4.43738494881394
Epoch 1167
Epoch 1167 :: Batch 0/1
Batch Loss = 4.4373849476898375
1167, epoch_train_loss=4.4373849476898375
Epoch 1168
Epoch 1168 :: Batch 0/1
Batch Loss = 4.437384946567331
1168, epoch_train_loss=4.437384946567331
Epoch 1169
Epoch 1169 :: Batch 0/1
Batch Loss = 4.4373849454464125
1169, epoch_train_loss=4.4373849454464125
Epoch 1170
Epoch 1170 :: Batch 0/1
Batch Loss = 4.437384944327077
1170, epoch_train_loss=4.437384944327077
Epoch 1171
Epoch 1171 :: Batch 0/1
Batch Loss = 4.437384943209314
1171, epoch_train_loss=4.437384943209314
Epoch 1172
Epoch 1172 :: Batch 0/1
Batch Loss = 4.437384942093119
1172, epoch_train_loss=4.437384942093119
Epoch 1173
Epoch 1173 :: Batch 0/1
Batch Loss = 4.437384940978482
1173, epoch_train_loss=4.437384940978482
Epoch 1174
Epoch 1174 :: Batch 0/1
Batch Loss = 4.4373849398653995
1174, epoch_train_loss=4.4373849398653995
Epoch 1175
Epoch 1175 :: Batch 0/1
Batch Loss = 4.437384938753861
1175, epoch_train_loss=4.437384938753861
Epoch 1176
Epoch 1176 :: Batch 0/1
Batch Loss = 4.437384937643864
1176, epoch_train_loss=4.437384937643864
Epoch 1177
Epoch 1177 :: Batch 0/1
Batch Loss = 4.437384936535396
1177, epoch_train_loss=4.437384936535396
Epoch 1178
Epoch 1178 :: Batch 0/1
Batch Loss = 4.4373849354284545
1178, epoch_train_loss=4.4373849354284545
Epoch 1179
Epoch 1179 :: Batch 0/1
Batch Loss = 4.437384934323029
1179, epoch_train_loss=4.437384934323029
Epoch 1180
Epoch 1180 :: Batch 0/1
Batch Loss = 4.437384933219115
1180, epoch_train_loss=4.437384933219115
Epoch 1181
Epoch 1181 :: Batch 0/1
Batch Loss = 4.4373849321167045
1181, epoch_train_loss=4.4373849321167045
Epoch 1182
Epoch 1182 :: Batch 0/1
Batch Loss = 4.437384931015791
1182, epoch_train_loss=4.437384931015791
Epoch 1183
Epoch 1183 :: Batch 0/1
Batch Loss = 4.437384929916369
1183, epoch_train_loss=4.437384929916369
Epoch 1184
Epoch 1184 :: Batch 0/1
Batch Loss = 4.437384928818427
1184, epoch_train_loss=4.437384928818427
Epoch 1185
Epoch 1185 :: Batch 0/1
Batch Loss = 4.437384927721965
1185, epoch_train_loss=4.437384927721965
Epoch 1186
Epoch 1186 :: Batch 0/1
Batch Loss = 4.43738492662697
1186, epoch_train_loss=4.43738492662697
Epoch 1187
Epoch 1187 :: Batch 0/1
Batch Loss = 4.437384925533438
1187, epoch_train_loss=4.437384925533438
Epoch 1188
Epoch 1188 :: Batch 0/1
Batch Loss = 4.437384924441362
1188, epoch_train_loss=4.437384924441362
Epoch 1189
Epoch 1189 :: Batch 0/1
Batch Loss = 4.437384923350735
1189, epoch_train_loss=4.437384923350735
Epoch 1190
Epoch 1190 :: Batch 0/1
Batch Loss = 4.4373849222615505
1190, epoch_train_loss=4.4373849222615505
Epoch 1191
Epoch 1191 :: Batch 0/1
Batch Loss = 4.437384921173803
1191, epoch_train_loss=4.437384921173803
Epoch 1192
Epoch 1192 :: Batch 0/1
Batch Loss = 4.4373849200874815
1192, epoch_train_loss=4.4373849200874815
Epoch 1193
Epoch 1193 :: Batch 0/1
Batch Loss = 4.437384919002584
1193, epoch_train_loss=4.437384919002584
Epoch 1194
Epoch 1194 :: Batch 0/1
Batch Loss = 4.437384917919102
1194, epoch_train_loss=4.437384917919102
Epoch 1195
Epoch 1195 :: Batch 0/1
Batch Loss = 4.437384916837029
1195, epoch_train_loss=4.437384916837029
Epoch 1196
Epoch 1196 :: Batch 0/1
Batch Loss = 4.4373849157563585
1196, epoch_train_loss=4.4373849157563585
Epoch 1197
Epoch 1197 :: Batch 0/1
Batch Loss = 4.437384914677083
1197, epoch_train_loss=4.437384914677083
Epoch 1198
Epoch 1198 :: Batch 0/1
Batch Loss = 4.437384913599198
1198, epoch_train_loss=4.437384913599198
Epoch 1199
Epoch 1199 :: Batch 0/1
Batch Loss = 4.437384912522694
1199, epoch_train_loss=4.437384912522694
Epoch 1200
Epoch 1200 :: Batch 0/1
Batch Loss = 4.437384911447566
1200, epoch_train_loss=4.437384911447566
Epoch 1201
Epoch 1201 :: Batch 0/1
Batch Loss = 4.437384910373809
1201, epoch_train_loss=4.437384910373809
Epoch 1202
Epoch 1202 :: Batch 0/1
Batch Loss = 4.437384909301414
1202, epoch_train_loss=4.437384909301414
Epoch 1203
Epoch 1203 :: Batch 0/1
Batch Loss = 4.437384908230375
1203, epoch_train_loss=4.437384908230375
Epoch 1204
Epoch 1204 :: Batch 0/1
Batch Loss = 4.437384907160687
1204, epoch_train_loss=4.437384907160687
Epoch 1205
Epoch 1205 :: Batch 0/1
Batch Loss = 4.437384906092342
1205, epoch_train_loss=4.437384906092342
Epoch 1206
Epoch 1206 :: Batch 0/1
Batch Loss = 4.4373849050253344
1206, epoch_train_loss=4.4373849050253344
Epoch 1207
Epoch 1207 :: Batch 0/1
Batch Loss = 4.437384903959658
1207, epoch_train_loss=4.437384903959658
Epoch 1208
Epoch 1208 :: Batch 0/1
Batch Loss = 4.437384902895305
1208, epoch_train_loss=4.437384902895305
Epoch 1209
Epoch 1209 :: Batch 0/1
Batch Loss = 4.43738490183227
1209, epoch_train_loss=4.43738490183227
Epoch 1210
Epoch 1210 :: Batch 0/1
Batch Loss = 4.437384900770547
1210, epoch_train_loss=4.437384900770547
Epoch 1211
Epoch 1211 :: Batch 0/1
Batch Loss = 4.437384899710129
1211, epoch_train_loss=4.437384899710129
Epoch 1212
Epoch 1212 :: Batch 0/1
Batch Loss = 4.43738489865101
1212, epoch_train_loss=4.43738489865101
Epoch 1213
Epoch 1213 :: Batch 0/1
Batch Loss = 4.437384897593184
1213, epoch_train_loss=4.437384897593184
Epoch 1214
Epoch 1214 :: Batch 0/1
Batch Loss = 4.437384896536644
1214, epoch_train_loss=4.437384896536644
Epoch 1215
Epoch 1215 :: Batch 0/1
Batch Loss = 4.437384895481384
1215, epoch_train_loss=4.437384895481384
Epoch 1216
Epoch 1216 :: Batch 0/1
Batch Loss = 4.437384894427397
1216, epoch_train_loss=4.437384894427397
Epoch 1217
Epoch 1217 :: Batch 0/1
Batch Loss = 4.437384893374678
1217, epoch_train_loss=4.437384893374678
Epoch 1218
Epoch 1218 :: Batch 0/1
Batch Loss = 4.437384892323222
1218, epoch_train_loss=4.437384892323222
Epoch 1219
Epoch 1219 :: Batch 0/1
Batch Loss = 4.437384891273019
1219, epoch_train_loss=4.437384891273019
Epoch 1220
Epoch 1220 :: Batch 0/1
Batch Loss = 4.437384890224067
1220, epoch_train_loss=4.437384890224067
Epoch 1221
Epoch 1221 :: Batch 0/1
Batch Loss = 4.437384889176355
1221, epoch_train_loss=4.437384889176355
Epoch 1222
Epoch 1222 :: Batch 0/1
Batch Loss = 4.437384888129881
1222, epoch_train_loss=4.437384888129881
Epoch 1223
Epoch 1223 :: Batch 0/1
Batch Loss = 4.4373848870846375
1223, epoch_train_loss=4.4373848870846375
Epoch 1224
Epoch 1224 :: Batch 0/1
Batch Loss = 4.437384886040619
1224, epoch_train_loss=4.437384886040619
Epoch 1225
Epoch 1225 :: Batch 0/1
Batch Loss = 4.437384884997817
1225, epoch_train_loss=4.437384884997817
Epoch 1226
Epoch 1226 :: Batch 0/1
Batch Loss = 4.437384883956229
1226, epoch_train_loss=4.437384883956229
Epoch 1227
Epoch 1227 :: Batch 0/1
Batch Loss = 4.437384882915846
1227, epoch_train_loss=4.437384882915846
Epoch 1228
Epoch 1228 :: Batch 0/1
Batch Loss = 4.437384881876664
1228, epoch_train_loss=4.437384881876664
Epoch 1229
Epoch 1229 :: Batch 0/1
Batch Loss = 4.4373848808386755
1229, epoch_train_loss=4.4373848808386755
Epoch 1230
Epoch 1230 :: Batch 0/1
Batch Loss = 4.437384879801874
1230, epoch_train_loss=4.437384879801874
Epoch 1231
Epoch 1231 :: Batch 0/1
Batch Loss = 4.437384878766255
1231, epoch_train_loss=4.437384878766255
Epoch 1232
Epoch 1232 :: Batch 0/1
Batch Loss = 4.437384877731812
1232, epoch_train_loss=4.437384877731812
Epoch 1233
Epoch 1233 :: Batch 0/1
Batch Loss = 4.437384876698538
1233, epoch_train_loss=4.437384876698538
Epoch 1234
Epoch 1234 :: Batch 0/1
Batch Loss = 4.43738487566643
1234, epoch_train_loss=4.43738487566643
Epoch 1235
Epoch 1235 :: Batch 0/1
Batch Loss = 4.43738487463548
1235, epoch_train_loss=4.43738487463548
Epoch 1236
Epoch 1236 :: Batch 0/1
Batch Loss = 4.43738487360568
1236, epoch_train_loss=4.43738487360568
Epoch 1237
Epoch 1237 :: Batch 0/1
Batch Loss = 4.437384872577029
1237, epoch_train_loss=4.437384872577029
Epoch 1238
Epoch 1238 :: Batch 0/1
Batch Loss = 4.437384871549515
1238, epoch_train_loss=4.437384871549515
Epoch 1239
Epoch 1239 :: Batch 0/1
Batch Loss = 4.437384870523138
1239, epoch_train_loss=4.437384870523138
Epoch 1240
Epoch 1240 :: Batch 0/1
Batch Loss = 4.437384869497888
1240, epoch_train_loss=4.437384869497888
Epoch 1241
Epoch 1241 :: Batch 0/1
Batch Loss = 4.437384868473762
1241, epoch_train_loss=4.437384868473762
Epoch 1242
Epoch 1242 :: Batch 0/1
Batch Loss = 4.437384867450751
1242, epoch_train_loss=4.437384867450751
Epoch 1243
Epoch 1243 :: Batch 0/1
Batch Loss = 4.437384866428853
1243, epoch_train_loss=4.437384866428853
Epoch 1244
Epoch 1244 :: Batch 0/1
Batch Loss = 4.437384865408058
1244, epoch_train_loss=4.437384865408058
Epoch 1245
Epoch 1245 :: Batch 0/1
Batch Loss = 4.437384864388365
1245, epoch_train_loss=4.437384864388365
Epoch 1246
Epoch 1246 :: Batch 0/1
Batch Loss = 4.437384863369765
1246, epoch_train_loss=4.437384863369765
Epoch 1247
Epoch 1247 :: Batch 0/1
Batch Loss = 4.437384862352251
1247, epoch_train_loss=4.437384862352251
Epoch 1248
Epoch 1248 :: Batch 0/1
Batch Loss = 4.437384861335819
1248, epoch_train_loss=4.437384861335819
Epoch 1249
Epoch 1249 :: Batch 0/1
Batch Loss = 4.437384860320464
1249, epoch_train_loss=4.437384860320464
Epoch 1250
Epoch 1250 :: Batch 0/1
Batch Loss = 4.43738485930618
1250, epoch_train_loss=4.43738485930618
Epoch 1251
Epoch 1251 :: Batch 0/1
Batch Loss = 4.437384858292961
1251, epoch_train_loss=4.437384858292961
Epoch 1252
Epoch 1252 :: Batch 0/1
Batch Loss = 4.437384857280801
1252, epoch_train_loss=4.437384857280801
Epoch 1253
Epoch 1253 :: Batch 0/1
Batch Loss = 4.437384856269694
1253, epoch_train_loss=4.437384856269694
Epoch 1254
Epoch 1254 :: Batch 0/1
Batch Loss = 4.437384855259635
1254, epoch_train_loss=4.437384855259635
Epoch 1255
Epoch 1255 :: Batch 0/1
Batch Loss = 4.437384854250618
1255, epoch_train_loss=4.437384854250618
Epoch 1256
Epoch 1256 :: Batch 0/1
Batch Loss = 4.437384853242637
1256, epoch_train_loss=4.437384853242637
Epoch 1257
Epoch 1257 :: Batch 0/1
Batch Loss = 4.437384852235687
1257, epoch_train_loss=4.437384852235687
Epoch 1258
Epoch 1258 :: Batch 0/1
Batch Loss = 4.437384851229762
1258, epoch_train_loss=4.437384851229762
Epoch 1259
Epoch 1259 :: Batch 0/1
Batch Loss = 4.437384850224857
1259, epoch_train_loss=4.437384850224857
Epoch 1260
Epoch 1260 :: Batch 0/1
Batch Loss = 4.437384849220966
1260, epoch_train_loss=4.437384849220966
Epoch 1261
Epoch 1261 :: Batch 0/1
Batch Loss = 4.437384848218084
1261, epoch_train_loss=4.437384848218084
Epoch 1262
Epoch 1262 :: Batch 0/1
Batch Loss = 4.437384847216203
1262, epoch_train_loss=4.437384847216203
Epoch 1263
Epoch 1263 :: Batch 0/1
Batch Loss = 4.43738484621532
1263, epoch_train_loss=4.43738484621532
Epoch 1264
Epoch 1264 :: Batch 0/1
Batch Loss = 4.437384845215429
1264, epoch_train_loss=4.437384845215429
Epoch 1265
Epoch 1265 :: Batch 0/1
Batch Loss = 4.437384844216524
1265, epoch_train_loss=4.437384844216524
Epoch 1266
Epoch 1266 :: Batch 0/1
Batch Loss = 4.437384843218599
1266, epoch_train_loss=4.437384843218599
Epoch 1267
Epoch 1267 :: Batch 0/1
Batch Loss = 4.437384842221649
1267, epoch_train_loss=4.437384842221649
Epoch 1268
Epoch 1268 :: Batch 0/1
Batch Loss = 4.4373848412256685
1268, epoch_train_loss=4.4373848412256685
Epoch 1269
Epoch 1269 :: Batch 0/1
Batch Loss = 4.437384840230654
1269, epoch_train_loss=4.437384840230654
Epoch 1270
Epoch 1270 :: Batch 0/1
Batch Loss = 4.4373848392365955
1270, epoch_train_loss=4.4373848392365955
Epoch 1271
Epoch 1271 :: Batch 0/1
Batch Loss = 4.43738483824349
1271, epoch_train_loss=4.43738483824349
Epoch 1272
Epoch 1272 :: Batch 0/1
Batch Loss = 4.437384837251334
1272, epoch_train_loss=4.437384837251334
Epoch 1273
Epoch 1273 :: Batch 0/1
Batch Loss = 4.43738483626012
1273, epoch_train_loss=4.43738483626012
Epoch 1274
Epoch 1274 :: Batch 0/1
Batch Loss = 4.4373848352698415
1274, epoch_train_loss=4.4373848352698415
Epoch 1275
Epoch 1275 :: Batch 0/1
Batch Loss = 4.437384834280495
1275, epoch_train_loss=4.437384834280495
Epoch 1276
Epoch 1276 :: Batch 0/1
Batch Loss = 4.437384833292075
1276, epoch_train_loss=4.437384833292075
Epoch 1277
Epoch 1277 :: Batch 0/1
Batch Loss = 4.437384832304574
1277, epoch_train_loss=4.437384832304574
Epoch 1278
Epoch 1278 :: Batch 0/1
Batch Loss = 4.437384831317989
1278, epoch_train_loss=4.437384831317989
Epoch 1279
Epoch 1279 :: Batch 0/1
Batch Loss = 4.437384830332314
1279, epoch_train_loss=4.437384830332314
Epoch 1280
Epoch 1280 :: Batch 0/1
Batch Loss = 4.437384829347542
1280, epoch_train_loss=4.437384829347542
Epoch 1281
Epoch 1281 :: Batch 0/1
Batch Loss = 4.437384828363671
1281, epoch_train_loss=4.437384828363671
Epoch 1282
Epoch 1282 :: Batch 0/1
Batch Loss = 4.437384827380693
1282, epoch_train_loss=4.437384827380693
Epoch 1283
Epoch 1283 :: Batch 0/1
Batch Loss = 4.437384826398603
1283, epoch_train_loss=4.437384826398603
Epoch 1284
Epoch 1284 :: Batch 0/1
Batch Loss = 4.437384825417396
1284, epoch_train_loss=4.437384825417396
Epoch 1285
Epoch 1285 :: Batch 0/1
Batch Loss = 4.437384824437066
1285, epoch_train_loss=4.437384824437066
Epoch 1286
Epoch 1286 :: Batch 0/1
Batch Loss = 4.437384823457609
1286, epoch_train_loss=4.437384823457609
Epoch 1287
Epoch 1287 :: Batch 0/1
Batch Loss = 4.43738482247902
1287, epoch_train_loss=4.43738482247902
Epoch 1288
Epoch 1288 :: Batch 0/1
Batch Loss = 4.437384821501292
1288, epoch_train_loss=4.437384821501292
Epoch 1289
Epoch 1289 :: Batch 0/1
Batch Loss = 4.437384820524421
1289, epoch_train_loss=4.437384820524421
Epoch 1290
Epoch 1290 :: Batch 0/1
Batch Loss = 4.4373848195484005
1290, epoch_train_loss=4.4373848195484005
Epoch 1291
Epoch 1291 :: Batch 0/1
Batch Loss = 4.437384818573224
1291, epoch_train_loss=4.437384818573224
Epoch 1292
Epoch 1292 :: Batch 0/1
Batch Loss = 4.4373848175988915
1292, epoch_train_loss=4.4373848175988915
Epoch 1293
Epoch 1293 :: Batch 0/1
Batch Loss = 4.437384816625393
1293, epoch_train_loss=4.437384816625393
Epoch 1294
Epoch 1294 :: Batch 0/1
Batch Loss = 4.4373848156527265
1294, epoch_train_loss=4.4373848156527265
Epoch 1295
Epoch 1295 :: Batch 0/1
Batch Loss = 4.437384814680883
1295, epoch_train_loss=4.437384814680883
Epoch 1296
Epoch 1296 :: Batch 0/1
Batch Loss = 4.437384813709859
1296, epoch_train_loss=4.437384813709859
Epoch 1297
Epoch 1297 :: Batch 0/1
Batch Loss = 4.437384812739651
1297, epoch_train_loss=4.437384812739651
Epoch 1298
Epoch 1298 :: Batch 0/1
Batch Loss = 4.437384811770252
1298, epoch_train_loss=4.437384811770252
Epoch 1299
Epoch 1299 :: Batch 0/1
Batch Loss = 4.437384810801658
1299, epoch_train_loss=4.437384810801658
Epoch 1300
Epoch 1300 :: Batch 0/1
Batch Loss = 4.437384809833862
1300, epoch_train_loss=4.437384809833862
Epoch 1301
Epoch 1301 :: Batch 0/1
Batch Loss = 4.43738480886686
1301, epoch_train_loss=4.43738480886686
Epoch 1302
Epoch 1302 :: Batch 0/1
Batch Loss = 4.437384807900647
1302, epoch_train_loss=4.437384807900647
Epoch 1303
Epoch 1303 :: Batch 0/1
Batch Loss = 4.437384806935218
1303, epoch_train_loss=4.437384806935218
Epoch 1304
Epoch 1304 :: Batch 0/1
Batch Loss = 4.437384805970567
1304, epoch_train_loss=4.437384805970567
Epoch 1305
Epoch 1305 :: Batch 0/1
Batch Loss = 4.43738480500669
1305, epoch_train_loss=4.43738480500669
Epoch 1306
Epoch 1306 :: Batch 0/1
Batch Loss = 4.437384804043582
1306, epoch_train_loss=4.437384804043582
Epoch 1307
Epoch 1307 :: Batch 0/1
Batch Loss = 4.437384803081236
1307, epoch_train_loss=4.437384803081236
Epoch 1308
Epoch 1308 :: Batch 0/1
Batch Loss = 4.437384802119647
1308, epoch_train_loss=4.437384802119647
Epoch 1309
Epoch 1309 :: Batch 0/1
Batch Loss = 4.437384801158813
1309, epoch_train_loss=4.437384801158813
Epoch 1310
Epoch 1310 :: Batch 0/1
Batch Loss = 4.437384800198726
1310, epoch_train_loss=4.437384800198726
Epoch 1311
Epoch 1311 :: Batch 0/1
Batch Loss = 4.437384799239382
1311, epoch_train_loss=4.437384799239382
Epoch 1312
Epoch 1312 :: Batch 0/1
Batch Loss = 4.437384798280774
1312, epoch_train_loss=4.437384798280774
Epoch 1313
Epoch 1313 :: Batch 0/1
Batch Loss = 4.437384797322901
1313, epoch_train_loss=4.437384797322901
Epoch 1314
Epoch 1314 :: Batch 0/1
Batch Loss = 4.4373847963657544
1314, epoch_train_loss=4.4373847963657544
Epoch 1315
Epoch 1315 :: Batch 0/1
Batch Loss = 4.43738479540933
1315, epoch_train_loss=4.43738479540933
Epoch 1316
Epoch 1316 :: Batch 0/1
Batch Loss = 4.437384794453625
1316, epoch_train_loss=4.437384794453625
Epoch 1317
Epoch 1317 :: Batch 0/1
Batch Loss = 4.437384793498631
1317, epoch_train_loss=4.437384793498631
Epoch 1318
Epoch 1318 :: Batch 0/1
Batch Loss = 4.437384792544345
1318, epoch_train_loss=4.437384792544345
Epoch 1319
Epoch 1319 :: Batch 0/1
Batch Loss = 4.437384791590762
1319, epoch_train_loss=4.437384791590762
Epoch 1320
Epoch 1320 :: Batch 0/1
Batch Loss = 4.437384790637874
1320, epoch_train_loss=4.437384790637874
Epoch 1321
Epoch 1321 :: Batch 0/1
Batch Loss = 4.43738478968568
1321, epoch_train_loss=4.43738478968568
Epoch 1322
Epoch 1322 :: Batch 0/1
Batch Loss = 4.437384788734174
1322, epoch_train_loss=4.437384788734174
Epoch 1323
Epoch 1323 :: Batch 0/1
Batch Loss = 4.437384787783349
1323, epoch_train_loss=4.437384787783349
Epoch 1324
Epoch 1324 :: Batch 0/1
Batch Loss = 4.437384786833204
1324, epoch_train_loss=4.437384786833204
Epoch 1325
Epoch 1325 :: Batch 0/1
Batch Loss = 4.437384785883729
1325, epoch_train_loss=4.437384785883729
Epoch 1326
Epoch 1326 :: Batch 0/1
Batch Loss = 4.437384784934924
1326, epoch_train_loss=4.437384784934924
Epoch 1327
Epoch 1327 :: Batch 0/1
Batch Loss = 4.43738478398678
1327, epoch_train_loss=4.43738478398678
Epoch 1328
Epoch 1328 :: Batch 0/1
Batch Loss = 4.437384783039294
1328, epoch_train_loss=4.437384783039294
Epoch 1329
Epoch 1329 :: Batch 0/1
Batch Loss = 4.43738478209246
1329, epoch_train_loss=4.43738478209246
Epoch 1330
Epoch 1330 :: Batch 0/1
Batch Loss = 4.437384781146275
1330, epoch_train_loss=4.437384781146275
Epoch 1331
Epoch 1331 :: Batch 0/1
Batch Loss = 4.437384780200733
1331, epoch_train_loss=4.437384780200733
Epoch 1332
Epoch 1332 :: Batch 0/1
Batch Loss = 4.437384779255828
1332, epoch_train_loss=4.437384779255828
Epoch 1333
Epoch 1333 :: Batch 0/1
Batch Loss = 4.437384778311556
1333, epoch_train_loss=4.437384778311556
Epoch 1334
Epoch 1334 :: Batch 0/1
Batch Loss = 4.437384777367914
1334, epoch_train_loss=4.437384777367914
Epoch 1335
Epoch 1335 :: Batch 0/1
Batch Loss = 4.437384776424892
1335, epoch_train_loss=4.437384776424892
Epoch 1336
Epoch 1336 :: Batch 0/1
Batch Loss = 4.437384775482491
1336, epoch_train_loss=4.437384775482491
Epoch 1337
Epoch 1337 :: Batch 0/1
Batch Loss = 4.437384774540703
1337, epoch_train_loss=4.437384774540703
Epoch 1338
Epoch 1338 :: Batch 0/1
Batch Loss = 4.437384773599522
1338, epoch_train_loss=4.437384773599522
Epoch 1339
Epoch 1339 :: Batch 0/1
Batch Loss = 4.4373847726589455
1339, epoch_train_loss=4.4373847726589455
Epoch 1340
Epoch 1340 :: Batch 0/1
Batch Loss = 4.4373847717189685
1340, epoch_train_loss=4.4373847717189685
Epoch 1341
Epoch 1341 :: Batch 0/1
Batch Loss = 4.437384770779586
1341, epoch_train_loss=4.437384770779586
Epoch 1342
Epoch 1342 :: Batch 0/1
Batch Loss = 4.437384769840791
1342, epoch_train_loss=4.437384769840791
Epoch 1343
Epoch 1343 :: Batch 0/1
Batch Loss = 4.4373847689025805
1343, epoch_train_loss=4.4373847689025805
Epoch 1344
Epoch 1344 :: Batch 0/1
Batch Loss = 4.437384767964949
1344, epoch_train_loss=4.437384767964949
Epoch 1345
Epoch 1345 :: Batch 0/1
Batch Loss = 4.4373847670278925
1345, epoch_train_loss=4.4373847670278925
Epoch 1346
Epoch 1346 :: Batch 0/1
Batch Loss = 4.437384766091407
1346, epoch_train_loss=4.437384766091407
Epoch 1347
Epoch 1347 :: Batch 0/1
Batch Loss = 4.437384765155485
1347, epoch_train_loss=4.437384765155485
Epoch 1348
Epoch 1348 :: Batch 0/1
Batch Loss = 4.437384764220123
1348, epoch_train_loss=4.437384764220123
Epoch 1349
Epoch 1349 :: Batch 0/1
Batch Loss = 4.4373847632853165
1349, epoch_train_loss=4.4373847632853165
Epoch 1350
Epoch 1350 :: Batch 0/1
Batch Loss = 4.43738476235106
1350, epoch_train_loss=4.43738476235106
Epoch 1351
Epoch 1351 :: Batch 0/1
Batch Loss = 4.437384761417349
1351, epoch_train_loss=4.437384761417349
Epoch 1352
Epoch 1352 :: Batch 0/1
Batch Loss = 4.437384760484181
1352, epoch_train_loss=4.437384760484181
Epoch 1353
Epoch 1353 :: Batch 0/1
Batch Loss = 4.437384759551546
1353, epoch_train_loss=4.437384759551546
Epoch 1354
Epoch 1354 :: Batch 0/1
Batch Loss = 4.4373847586194435
1354, epoch_train_loss=4.4373847586194435
Epoch 1355
Epoch 1355 :: Batch 0/1
Batch Loss = 4.437384757687868
1355, epoch_train_loss=4.437384757687868
Epoch 1356
Epoch 1356 :: Batch 0/1
Batch Loss = 4.437384756756813
1356, epoch_train_loss=4.437384756756813
Epoch 1357
Epoch 1357 :: Batch 0/1
Batch Loss = 4.437384755826276
1357, epoch_train_loss=4.437384755826276
Epoch 1358
Epoch 1358 :: Batch 0/1
Batch Loss = 4.437384754896249
1358, epoch_train_loss=4.437384754896249
Epoch 1359
Epoch 1359 :: Batch 0/1
Batch Loss = 4.437384753966732
1359, epoch_train_loss=4.437384753966732
Epoch 1360
Epoch 1360 :: Batch 0/1
Batch Loss = 4.4373847530377155
1360, epoch_train_loss=4.4373847530377155
Epoch 1361
Epoch 1361 :: Batch 0/1
Batch Loss = 4.437384752109198
1361, epoch_train_loss=4.437384752109198
Epoch 1362
Epoch 1362 :: Batch 0/1
Batch Loss = 4.437384751181173
1362, epoch_train_loss=4.437384751181173
Epoch 1363
Epoch 1363 :: Batch 0/1
Batch Loss = 4.437384750253637
1363, epoch_train_loss=4.437384750253637
Epoch 1364
Epoch 1364 :: Batch 0/1
Batch Loss = 4.437384749326584
1364, epoch_train_loss=4.437384749326584
Epoch 1365
Epoch 1365 :: Batch 0/1
Batch Loss = 4.437384748400009
1365, epoch_train_loss=4.437384748400009
Epoch 1366
Epoch 1366 :: Batch 0/1
Batch Loss = 4.437384747473908
1366, epoch_train_loss=4.437384747473908
Epoch 1367
Epoch 1367 :: Batch 0/1
Batch Loss = 4.437384746548277
1367, epoch_train_loss=4.437384746548277
Epoch 1368
Epoch 1368 :: Batch 0/1
Batch Loss = 4.437384745623111
1368, epoch_train_loss=4.437384745623111
Epoch 1369
Epoch 1369 :: Batch 0/1
Batch Loss = 4.437384744698403
1369, epoch_train_loss=4.437384744698403
Epoch 1370
Epoch 1370 :: Batch 0/1
Batch Loss = 4.437384743774152
1370, epoch_train_loss=4.437384743774152
Epoch 1371
Epoch 1371 :: Batch 0/1
Batch Loss = 4.437384742850351
1371, epoch_train_loss=4.437384742850351
Epoch 1372
Epoch 1372 :: Batch 0/1
Batch Loss = 4.437384741926995
1372, epoch_train_loss=4.437384741926995
Epoch 1373
Epoch 1373 :: Batch 0/1
Batch Loss = 4.437384741004081
1373, epoch_train_loss=4.437384741004081
Epoch 1374
Epoch 1374 :: Batch 0/1
Batch Loss = 4.437384740081602
1374, epoch_train_loss=4.437384740081602
Epoch 1375
Epoch 1375 :: Batch 0/1
Batch Loss = 4.437384739159556
1375, epoch_train_loss=4.437384739159556
Epoch 1376
Epoch 1376 :: Batch 0/1
Batch Loss = 4.4373847382379354
1376, epoch_train_loss=4.4373847382379354
Epoch 1377
Epoch 1377 :: Batch 0/1
Batch Loss = 4.437384737316738
1377, epoch_train_loss=4.437384737316738
Epoch 1378
Epoch 1378 :: Batch 0/1
Batch Loss = 4.437384736395956
1378, epoch_train_loss=4.437384736395956
Epoch 1379
Epoch 1379 :: Batch 0/1
Batch Loss = 4.437384735475589
1379, epoch_train_loss=4.437384735475589
Epoch 1380
Epoch 1380 :: Batch 0/1
Batch Loss = 4.437384734555629
1380, epoch_train_loss=4.437384734555629
Epoch 1381
Epoch 1381 :: Batch 0/1
Batch Loss = 4.437384733636073
1381, epoch_train_loss=4.437384733636073
Epoch 1382
Epoch 1382 :: Batch 0/1
Batch Loss = 4.437384732716915
1382, epoch_train_loss=4.437384732716915
Epoch 1383
Epoch 1383 :: Batch 0/1
Batch Loss = 4.437384731798152
1383, epoch_train_loss=4.437384731798152
Epoch 1384
Epoch 1384 :: Batch 0/1
Batch Loss = 4.437384730879777
1384, epoch_train_loss=4.437384730879777
Epoch 1385
Epoch 1385 :: Batch 0/1
Batch Loss = 4.437384729961788
1385, epoch_train_loss=4.437384729961788
Epoch 1386
Epoch 1386 :: Batch 0/1
Batch Loss = 4.437384729044178
1386, epoch_train_loss=4.437384729044178
Epoch 1387
Epoch 1387 :: Batch 0/1
Batch Loss = 4.4373847281269425
1387, epoch_train_loss=4.4373847281269425
Epoch 1388
Epoch 1388 :: Batch 0/1
Batch Loss = 4.4373847272100795
1388, epoch_train_loss=4.4373847272100795
Epoch 1389
Epoch 1389 :: Batch 0/1
Batch Loss = 4.437384726293583
1389, epoch_train_loss=4.437384726293583
Epoch 1390
Epoch 1390 :: Batch 0/1
Batch Loss = 4.437384725377445
1390, epoch_train_loss=4.437384725377445
Epoch 1391
Epoch 1391 :: Batch 0/1
Batch Loss = 4.437384724461665
1391, epoch_train_loss=4.437384724461665
Epoch 1392
Epoch 1392 :: Batch 0/1
Batch Loss = 4.437384723546238
1392, epoch_train_loss=4.437384723546238
Epoch 1393
Epoch 1393 :: Batch 0/1
Batch Loss = 4.4373847226311565
1393, epoch_train_loss=4.4373847226311565
Epoch 1394
Epoch 1394 :: Batch 0/1
Batch Loss = 4.437384721716419
1394, epoch_train_loss=4.437384721716419
Epoch 1395
Epoch 1395 :: Batch 0/1
Batch Loss = 4.43738472080202
1395, epoch_train_loss=4.43738472080202
Epoch 1396
Epoch 1396 :: Batch 0/1
Batch Loss = 4.437384719887953
1396, epoch_train_loss=4.437384719887953
Epoch 1397
Epoch 1397 :: Batch 0/1
Batch Loss = 4.437384718974215
1397, epoch_train_loss=4.437384718974215
Epoch 1398
Epoch 1398 :: Batch 0/1
Batch Loss = 4.437384718060801
1398, epoch_train_loss=4.437384718060801
Epoch 1399
Epoch 1399 :: Batch 0/1
Batch Loss = 4.437384717147707
1399, epoch_train_loss=4.437384717147707
Epoch 1400
Epoch 1400 :: Batch 0/1
Batch Loss = 4.437384716234928
1400, epoch_train_loss=4.437384716234928
Epoch 1401
Epoch 1401 :: Batch 0/1
Batch Loss = 4.437384715322459
1401, epoch_train_loss=4.437384715322459
Epoch 1402
Epoch 1402 :: Batch 0/1
Batch Loss = 4.437384714410294
1402, epoch_train_loss=4.437384714410294
Epoch 1403
Epoch 1403 :: Batch 0/1
Batch Loss = 4.437384713498433
1403, epoch_train_loss=4.437384713498433
Epoch 1404
Epoch 1404 :: Batch 0/1
Batch Loss = 4.437384712586866
1404, epoch_train_loss=4.437384712586866
Epoch 1405
Epoch 1405 :: Batch 0/1
Batch Loss = 4.43738471167559
1405, epoch_train_loss=4.43738471167559
Epoch 1406
Epoch 1406 :: Batch 0/1
Batch Loss = 4.4373847107646025
1406, epoch_train_loss=4.4373847107646025
Epoch 1407
Epoch 1407 :: Batch 0/1
Batch Loss = 4.437384709853897
1407, epoch_train_loss=4.437384709853897
Epoch 1408
Epoch 1408 :: Batch 0/1
Batch Loss = 4.437384708943469
1408, epoch_train_loss=4.437384708943469
Epoch 1409
Epoch 1409 :: Batch 0/1
Batch Loss = 4.4373847080333135
1409, epoch_train_loss=4.4373847080333135
Epoch 1410
Epoch 1410 :: Batch 0/1
Batch Loss = 4.437384707123428
1410, epoch_train_loss=4.437384707123428
Epoch 1411
Epoch 1411 :: Batch 0/1
Batch Loss = 4.437384706213805
1411, epoch_train_loss=4.437384706213805
Epoch 1412
Epoch 1412 :: Batch 0/1
Batch Loss = 4.437384705304442
1412, epoch_train_loss=4.437384705304442
Epoch 1413
Epoch 1413 :: Batch 0/1
Batch Loss = 4.437384704395333
1413, epoch_train_loss=4.437384704395333
Epoch 1414
Epoch 1414 :: Batch 0/1
Batch Loss = 4.437384703486475
1414, epoch_train_loss=4.437384703486475
Epoch 1415
Epoch 1415 :: Batch 0/1
Batch Loss = 4.4373847025778606
1415, epoch_train_loss=4.4373847025778606
Epoch 1416
Epoch 1416 :: Batch 0/1
Batch Loss = 4.437384701669488
1416, epoch_train_loss=4.437384701669488
Epoch 1417
Epoch 1417 :: Batch 0/1
Batch Loss = 4.437384700761353
1417, epoch_train_loss=4.437384700761353
Epoch 1418
Epoch 1418 :: Batch 0/1
Batch Loss = 4.437384699853446
1418, epoch_train_loss=4.437384699853446
Epoch 1419
Epoch 1419 :: Batch 0/1
Batch Loss = 4.437384698945768
1419, epoch_train_loss=4.437384698945768
Epoch 1420
Epoch 1420 :: Batch 0/1
Batch Loss = 4.437384698038311
1420, epoch_train_loss=4.437384698038311
Epoch 1421
Epoch 1421 :: Batch 0/1
Batch Loss = 4.4373846971310735
1421, epoch_train_loss=4.4373846971310735
Epoch 1422
Epoch 1422 :: Batch 0/1
Batch Loss = 4.437384696224048
1422, epoch_train_loss=4.437384696224048
Epoch 1423
Epoch 1423 :: Batch 0/1
Batch Loss = 4.43738469531723
1423, epoch_train_loss=4.43738469531723
Epoch 1424
Epoch 1424 :: Batch 0/1
Batch Loss = 4.437384694410617
1424, epoch_train_loss=4.437384694410617
Epoch 1425
Epoch 1425 :: Batch 0/1
Batch Loss = 4.437384693504203
1425, epoch_train_loss=4.437384693504203
Epoch 1426
Epoch 1426 :: Batch 0/1
Batch Loss = 4.437384692597982
1426, epoch_train_loss=4.437384692597982
Epoch 1427
Epoch 1427 :: Batch 0/1
Batch Loss = 4.437384691691952
1427, epoch_train_loss=4.437384691691952
Epoch 1428
Epoch 1428 :: Batch 0/1
Batch Loss = 4.437384690786107
1428, epoch_train_loss=4.437384690786107
Epoch 1429
Epoch 1429 :: Batch 0/1
Batch Loss = 4.437384689880444
1429, epoch_train_loss=4.437384689880444
Epoch 1430
Epoch 1430 :: Batch 0/1
Batch Loss = 4.437384688974956
1430, epoch_train_loss=4.437384688974956
Epoch 1431
Epoch 1431 :: Batch 0/1
Batch Loss = 4.437384688069639
1431, epoch_train_loss=4.437384688069639
Epoch 1432
Epoch 1432 :: Batch 0/1
Batch Loss = 4.437384687164489
1432, epoch_train_loss=4.437384687164489
Epoch 1433
Epoch 1433 :: Batch 0/1
Batch Loss = 4.437384686259501
1433, epoch_train_loss=4.437384686259501
Epoch 1434
Epoch 1434 :: Batch 0/1
Batch Loss = 4.437384685354671
1434, epoch_train_loss=4.437384685354671
Epoch 1435
Epoch 1435 :: Batch 0/1
Batch Loss = 4.437384684449994
1435, epoch_train_loss=4.437384684449994
Epoch 1436
Epoch 1436 :: Batch 0/1
Batch Loss = 4.4373846835454644
1436, epoch_train_loss=4.4373846835454644
Epoch 1437
Epoch 1437 :: Batch 0/1
Batch Loss = 4.437384682641079
1437, epoch_train_loss=4.437384682641079
Epoch 1438
Epoch 1438 :: Batch 0/1
Batch Loss = 4.437384681736832
1438, epoch_train_loss=4.437384681736832
Epoch 1439
Epoch 1439 :: Batch 0/1
Batch Loss = 4.437384680832721
1439, epoch_train_loss=4.437384680832721
Epoch 1440
Epoch 1440 :: Batch 0/1
Batch Loss = 4.437384679928738
1440, epoch_train_loss=4.437384679928738
Epoch 1441
Epoch 1441 :: Batch 0/1
Batch Loss = 4.4373846790248805
1441, epoch_train_loss=4.4373846790248805
Epoch 1442
Epoch 1442 :: Batch 0/1
Batch Loss = 4.437384678121145
1442, epoch_train_loss=4.437384678121145
Epoch 1443
Epoch 1443 :: Batch 0/1
Batch Loss = 4.437384677217523
1443, epoch_train_loss=4.437384677217523
Epoch 1444
Epoch 1444 :: Batch 0/1
Batch Loss = 4.437384676314014
1444, epoch_train_loss=4.437384676314014
Epoch 1445
Epoch 1445 :: Batch 0/1
Batch Loss = 4.437384675410609
1445, epoch_train_loss=4.437384675410609
Epoch 1446
Epoch 1446 :: Batch 0/1
Batch Loss = 4.437384674507309
1446, epoch_train_loss=4.437384674507309
Epoch 1447
Epoch 1447 :: Batch 0/1
Batch Loss = 4.437384673604106
1447, epoch_train_loss=4.437384673604106
Epoch 1448
Epoch 1448 :: Batch 0/1
Batch Loss = 4.437384672700994
1448, epoch_train_loss=4.437384672700994
Epoch 1449
Epoch 1449 :: Batch 0/1
Batch Loss = 4.437384671797972
1449, epoch_train_loss=4.437384671797972
Epoch 1450
Epoch 1450 :: Batch 0/1
Batch Loss = 4.437384670895031
1450, epoch_train_loss=4.437384670895031
Epoch 1451
Epoch 1451 :: Batch 0/1
Batch Loss = 4.43738466999217
1451, epoch_train_loss=4.43738466999217
Epoch 1452
Epoch 1452 :: Batch 0/1
Batch Loss = 4.437384669089384
1452, epoch_train_loss=4.437384669089384
Epoch 1453
Epoch 1453 :: Batch 0/1
Batch Loss = 4.437384668186666
1453, epoch_train_loss=4.437384668186666
Epoch 1454
Epoch 1454 :: Batch 0/1
Batch Loss = 4.437384667284014
1454, epoch_train_loss=4.437384667284014
Epoch 1455
Epoch 1455 :: Batch 0/1
Batch Loss = 4.43738466638142
1455, epoch_train_loss=4.43738466638142
Epoch 1456
Epoch 1456 :: Batch 0/1
Batch Loss = 4.437384665478884
1456, epoch_train_loss=4.437384665478884
Epoch 1457
Epoch 1457 :: Batch 0/1
Batch Loss = 4.437384664576397
1457, epoch_train_loss=4.437384664576397
Epoch 1458
Epoch 1458 :: Batch 0/1
Batch Loss = 4.437384663673957
1458, epoch_train_loss=4.437384663673957
Epoch 1459
Epoch 1459 :: Batch 0/1
Batch Loss = 4.437384662771559
1459, epoch_train_loss=4.437384662771559
Epoch 1460
Epoch 1460 :: Batch 0/1
Batch Loss = 4.4373846618691974
1460, epoch_train_loss=4.4373846618691974
Epoch 1461
Epoch 1461 :: Batch 0/1
Batch Loss = 4.437384660966868
1461, epoch_train_loss=4.437384660966868
Epoch 1462
Epoch 1462 :: Batch 0/1
Batch Loss = 4.437384660064566
1462, epoch_train_loss=4.437384660064566
Epoch 1463
Epoch 1463 :: Batch 0/1
Batch Loss = 4.437384659162286
1463, epoch_train_loss=4.437384659162286
Epoch 1464
Epoch 1464 :: Batch 0/1
Batch Loss = 4.437384658260025
1464, epoch_train_loss=4.437384658260025
Epoch 1465
Epoch 1465 :: Batch 0/1
Batch Loss = 4.437384657357778
1465, epoch_train_loss=4.437384657357778
Epoch 1466
Epoch 1466 :: Batch 0/1
Batch Loss = 4.4373846564555395
1466, epoch_train_loss=4.4373846564555395
Epoch 1467
Epoch 1467 :: Batch 0/1
Batch Loss = 4.437384655553304
1467, epoch_train_loss=4.437384655553304
Epoch 1468
Epoch 1468 :: Batch 0/1
Batch Loss = 4.437384654651069
1468, epoch_train_loss=4.437384654651069
Epoch 1469
Epoch 1469 :: Batch 0/1
Batch Loss = 4.437384653748829
1469, epoch_train_loss=4.437384653748829
Epoch 1470
Epoch 1470 :: Batch 0/1
Batch Loss = 4.437384652846578
1470, epoch_train_loss=4.437384652846578
Epoch 1471
Epoch 1471 :: Batch 0/1
Batch Loss = 4.4373846519443125
1471, epoch_train_loss=4.4373846519443125
Epoch 1472
Epoch 1472 :: Batch 0/1
Batch Loss = 4.437384651042028
1472, epoch_train_loss=4.437384651042028
Epoch 1473
Epoch 1473 :: Batch 0/1
Batch Loss = 4.43738465013972
1473, epoch_train_loss=4.43738465013972
Epoch 1474
Epoch 1474 :: Batch 0/1
Batch Loss = 4.437384649237382
1474, epoch_train_loss=4.437384649237382
Epoch 1475
Epoch 1475 :: Batch 0/1
Batch Loss = 4.437384648335011
1475, epoch_train_loss=4.437384648335011
Epoch 1476
Epoch 1476 :: Batch 0/1
Batch Loss = 4.437384647432602
1476, epoch_train_loss=4.437384647432602
Epoch 1477
Epoch 1477 :: Batch 0/1
Batch Loss = 4.437384646530149
1477, epoch_train_loss=4.437384646530149
Epoch 1478
Epoch 1478 :: Batch 0/1
Batch Loss = 4.437384645627651
1478, epoch_train_loss=4.437384645627651
Epoch 1479
Epoch 1479 :: Batch 0/1
Batch Loss = 4.437384644725099
1479, epoch_train_loss=4.437384644725099
Epoch 1480
Epoch 1480 :: Batch 0/1
Batch Loss = 4.437384643822489
1480, epoch_train_loss=4.437384643822489
Epoch 1481
Epoch 1481 :: Batch 0/1
Batch Loss = 4.4373846429198185
1481, epoch_train_loss=4.4373846429198185
Epoch 1482
Epoch 1482 :: Batch 0/1
Batch Loss = 4.43738464201708
1482, epoch_train_loss=4.43738464201708
Epoch 1483
Epoch 1483 :: Batch 0/1
Batch Loss = 4.437384641114272
1483, epoch_train_loss=4.437384641114272
Epoch 1484
Epoch 1484 :: Batch 0/1
Batch Loss = 4.437384640211387
1484, epoch_train_loss=4.437384640211387
Epoch 1485
Epoch 1485 :: Batch 0/1
Batch Loss = 4.437384639308421
1485, epoch_train_loss=4.437384639308421
Epoch 1486
Epoch 1486 :: Batch 0/1
Batch Loss = 4.437384638405369
1486, epoch_train_loss=4.437384638405369
Epoch 1487
Epoch 1487 :: Batch 0/1
Batch Loss = 4.437384637502227
1487, epoch_train_loss=4.437384637502227
Epoch 1488
Epoch 1488 :: Batch 0/1
Batch Loss = 4.43738463659899
1488, epoch_train_loss=4.43738463659899
Epoch 1489
Epoch 1489 :: Batch 0/1
Batch Loss = 4.437384635695653
1489, epoch_train_loss=4.437384635695653
Epoch 1490
Epoch 1490 :: Batch 0/1
Batch Loss = 4.4373846347922115
1490, epoch_train_loss=4.4373846347922115
Epoch 1491
Epoch 1491 :: Batch 0/1
Batch Loss = 4.437384633888661
1491, epoch_train_loss=4.437384633888661
Epoch 1492
Epoch 1492 :: Batch 0/1
Batch Loss = 4.437384632984997
1492, epoch_train_loss=4.437384632984997
Epoch 1493
Epoch 1493 :: Batch 0/1
Batch Loss = 4.437384632081211
1493, epoch_train_loss=4.437384632081211
Epoch 1494
Epoch 1494 :: Batch 0/1
Batch Loss = 4.437384631177304
1494, epoch_train_loss=4.437384631177304
Epoch 1495
Epoch 1495 :: Batch 0/1
Batch Loss = 4.437384630273267
1495, epoch_train_loss=4.437384630273267
Epoch 1496
Epoch 1496 :: Batch 0/1
Batch Loss = 4.437384629369097
1496, epoch_train_loss=4.437384629369097
Epoch 1497
Epoch 1497 :: Batch 0/1
Batch Loss = 4.437384628464789
1497, epoch_train_loss=4.437384628464789
Epoch 1498
Epoch 1498 :: Batch 0/1
Batch Loss = 4.437384627560338
1498, epoch_train_loss=4.437384627560338
Epoch 1499
Epoch 1499 :: Batch 0/1
Batch Loss = 4.437384626655739
1499, epoch_train_loss=4.437384626655739
Epoch 1500
Epoch 1500 :: Batch 0/1
Batch Loss = 4.437384625750987
1500, epoch_train_loss=4.437384625750987
Epoch 1501
Epoch 1501 :: Batch 0/1
Batch Loss = 4.4373846248460795
1501, epoch_train_loss=4.4373846248460795
Epoch 1502
Epoch 1502 :: Batch 0/1
Batch Loss = 4.437384623941007
1502, epoch_train_loss=4.437384623941007
Epoch 1503
Epoch 1503 :: Batch 0/1
Batch Loss = 4.4373846230357685
1503, epoch_train_loss=4.4373846230357685
Epoch 1504
Epoch 1504 :: Batch 0/1
Batch Loss = 4.437384622130357
1504, epoch_train_loss=4.437384622130357
Epoch 1505
Epoch 1505 :: Batch 0/1
Batch Loss = 4.43738462122477
1505, epoch_train_loss=4.43738462122477
Epoch 1506
Epoch 1506 :: Batch 0/1
Batch Loss = 4.437384620319001
1506, epoch_train_loss=4.437384620319001
Epoch 1507
Epoch 1507 :: Batch 0/1
Batch Loss = 4.437384619413044
1507, epoch_train_loss=4.437384619413044
Epoch 1508
Epoch 1508 :: Batch 0/1
Batch Loss = 4.437384618506896
1508, epoch_train_loss=4.437384618506896
Epoch 1509
Epoch 1509 :: Batch 0/1
Batch Loss = 4.437384617600553
1509, epoch_train_loss=4.437384617600553
Epoch 1510
Epoch 1510 :: Batch 0/1
Batch Loss = 4.437384616694008
1510, epoch_train_loss=4.437384616694008
Epoch 1511
Epoch 1511 :: Batch 0/1
Batch Loss = 4.4373846157872565
1511, epoch_train_loss=4.4373846157872565
Epoch 1512
Epoch 1512 :: Batch 0/1
Batch Loss = 4.437384614880294
1512, epoch_train_loss=4.437384614880294
Epoch 1513
Epoch 1513 :: Batch 0/1
Batch Loss = 4.437384613973116
1513, epoch_train_loss=4.437384613973116
Epoch 1514
Epoch 1514 :: Batch 0/1
Batch Loss = 4.437384613065716
1514, epoch_train_loss=4.437384613065716
Epoch 1515
Epoch 1515 :: Batch 0/1
Batch Loss = 4.437384612158092
1515, epoch_train_loss=4.437384612158092
Epoch 1516
Epoch 1516 :: Batch 0/1
Batch Loss = 4.437384611250236
1516, epoch_train_loss=4.437384611250236
Epoch 1517
Epoch 1517 :: Batch 0/1
Batch Loss = 4.437384610342144
1517, epoch_train_loss=4.437384610342144
Epoch 1518
Epoch 1518 :: Batch 0/1
Batch Loss = 4.437384609433812
1518, epoch_train_loss=4.437384609433812
Epoch 1519
Epoch 1519 :: Batch 0/1
Batch Loss = 4.437384608525234
1519, epoch_train_loss=4.437384608525234
Epoch 1520
Epoch 1520 :: Batch 0/1
Batch Loss = 4.437384607616407
1520, epoch_train_loss=4.437384607616407
Epoch 1521
Epoch 1521 :: Batch 0/1
Batch Loss = 4.437384606707323
1521, epoch_train_loss=4.437384606707323
Epoch 1522
Epoch 1522 :: Batch 0/1
Batch Loss = 4.4373846057979796
1522, epoch_train_loss=4.4373846057979796
Epoch 1523
Epoch 1523 :: Batch 0/1
Batch Loss = 4.43738460488837
1523, epoch_train_loss=4.43738460488837
Epoch 1524
Epoch 1524 :: Batch 0/1
Batch Loss = 4.437384603978491
1524, epoch_train_loss=4.437384603978491
Epoch 1525
Epoch 1525 :: Batch 0/1
Batch Loss = 4.4373846030683355
1525, epoch_train_loss=4.4373846030683355
Epoch 1526
Epoch 1526 :: Batch 0/1
Batch Loss = 4.437384602157901
1526, epoch_train_loss=4.437384602157901
Epoch 1527
Epoch 1527 :: Batch 0/1
Batch Loss = 4.437384601247179
1527, epoch_train_loss=4.437384601247179
Epoch 1528
Epoch 1528 :: Batch 0/1
Batch Loss = 4.437384600336168
1528, epoch_train_loss=4.437384600336168
Epoch 1529
Epoch 1529 :: Batch 0/1
Batch Loss = 4.4373845994248615
1529, epoch_train_loss=4.4373845994248615
Epoch 1530
Epoch 1530 :: Batch 0/1
Batch Loss = 4.437384598513254
1530, epoch_train_loss=4.437384598513254
Epoch 1531
Epoch 1531 :: Batch 0/1
Batch Loss = 4.4373845976013415
1531, epoch_train_loss=4.4373845976013415
Epoch 1532
Epoch 1532 :: Batch 0/1
Batch Loss = 4.437384596689118
1532, epoch_train_loss=4.437384596689118
Epoch 1533
Epoch 1533 :: Batch 0/1
Batch Loss = 4.43738459577658
1533, epoch_train_loss=4.43738459577658
Epoch 1534
Epoch 1534 :: Batch 0/1
Batch Loss = 4.437384594863721
1534, epoch_train_loss=4.437384594863721
Epoch 1535
Epoch 1535 :: Batch 0/1
Batch Loss = 4.437384593950536
1535, epoch_train_loss=4.437384593950536
Epoch 1536
Epoch 1536 :: Batch 0/1
Batch Loss = 4.437384593037019
1536, epoch_train_loss=4.437384593037019
Epoch 1537
Epoch 1537 :: Batch 0/1
Batch Loss = 4.437384592123167
1537, epoch_train_loss=4.437384592123167
Epoch 1538
Epoch 1538 :: Batch 0/1
Batch Loss = 4.437384591208975
1538, epoch_train_loss=4.437384591208975
Epoch 1539
Epoch 1539 :: Batch 0/1
Batch Loss = 4.437384590294436
1539, epoch_train_loss=4.437384590294436
Epoch 1540
Epoch 1540 :: Batch 0/1
Batch Loss = 4.437384589379546
1540, epoch_train_loss=4.437384589379546
Epoch 1541
Epoch 1541 :: Batch 0/1
Batch Loss = 4.437384588464299
1541, epoch_train_loss=4.437384588464299
Epoch 1542
Epoch 1542 :: Batch 0/1
Batch Loss = 4.437384587548692
1542, epoch_train_loss=4.437384587548692
Epoch 1543
Epoch 1543 :: Batch 0/1
Batch Loss = 4.437384586632717
1543, epoch_train_loss=4.437384586632717
Epoch 1544
Epoch 1544 :: Batch 0/1
Batch Loss = 4.43738458571637
1544, epoch_train_loss=4.43738458571637
Epoch 1545
Epoch 1545 :: Batch 0/1
Batch Loss = 4.437384584799648
1545, epoch_train_loss=4.437384584799648
Epoch 1546
Epoch 1546 :: Batch 0/1
Batch Loss = 4.437384583882541
1546, epoch_train_loss=4.437384583882541
Epoch 1547
Epoch 1547 :: Batch 0/1
Batch Loss = 4.4373845829650485
1547, epoch_train_loss=4.4373845829650485
Epoch 1548
Epoch 1548 :: Batch 0/1
Batch Loss = 4.437384582047164
1548, epoch_train_loss=4.437384582047164
Epoch 1549
Epoch 1549 :: Batch 0/1
Batch Loss = 4.437384581128882
1549, epoch_train_loss=4.437384581128882
Epoch 1550
Epoch 1550 :: Batch 0/1
Batch Loss = 4.437384580210195
1550, epoch_train_loss=4.437384580210195
Epoch 1551
Epoch 1551 :: Batch 0/1
Batch Loss = 4.437384579291101
1551, epoch_train_loss=4.437384579291101
Epoch 1552
Epoch 1552 :: Batch 0/1
Batch Loss = 4.437384578371594
1552, epoch_train_loss=4.437384578371594
Epoch 1553
Epoch 1553 :: Batch 0/1
Batch Loss = 4.4373845774516685
1553, epoch_train_loss=4.4373845774516685
Epoch 1554
Epoch 1554 :: Batch 0/1
Batch Loss = 4.4373845765313185
1554, epoch_train_loss=4.4373845765313185
Epoch 1555
Epoch 1555 :: Batch 0/1
Batch Loss = 4.43738457561054
1555, epoch_train_loss=4.43738457561054
Epoch 1556
Epoch 1556 :: Batch 0/1
Batch Loss = 4.437384574689326
1556, epoch_train_loss=4.437384574689326
Epoch 1557
Epoch 1557 :: Batch 0/1
Batch Loss = 4.437384573767673
1557, epoch_train_loss=4.437384573767673
Epoch 1558
Epoch 1558 :: Batch 0/1
Batch Loss = 4.437384572845576
1558, epoch_train_loss=4.437384572845576
Epoch 1559
Epoch 1559 :: Batch 0/1
Batch Loss = 4.437384571923028
1559, epoch_train_loss=4.437384571923028
Epoch 1560
Epoch 1560 :: Batch 0/1
Batch Loss = 4.437384571000023
1560, epoch_train_loss=4.437384571000023
Epoch 1561
Epoch 1561 :: Batch 0/1
Batch Loss = 4.437384570076559
1561, epoch_train_loss=4.437384570076559
Epoch 1562
Epoch 1562 :: Batch 0/1
Batch Loss = 4.437384569152629
1562, epoch_train_loss=4.437384569152629
Epoch 1563
Epoch 1563 :: Batch 0/1
Batch Loss = 4.437384568228225
1563, epoch_train_loss=4.437384568228225
Epoch 1564
Epoch 1564 :: Batch 0/1
Batch Loss = 4.437384567303346
1564, epoch_train_loss=4.437384567303346
Epoch 1565
Epoch 1565 :: Batch 0/1
Batch Loss = 4.437384566377985
1565, epoch_train_loss=4.437384566377985
Epoch 1566
Epoch 1566 :: Batch 0/1
Batch Loss = 4.437384565452135
1566, epoch_train_loss=4.437384565452135
Epoch 1567
Epoch 1567 :: Batch 0/1
Batch Loss = 4.437384564525793
1567, epoch_train_loss=4.437384564525793
Epoch 1568
Epoch 1568 :: Batch 0/1
Batch Loss = 4.437384563598952
1568, epoch_train_loss=4.437384563598952
Epoch 1569
Epoch 1569 :: Batch 0/1
Batch Loss = 4.437384562671607
1569, epoch_train_loss=4.437384562671607
Epoch 1570
Epoch 1570 :: Batch 0/1
Batch Loss = 4.437384561743753
1570, epoch_train_loss=4.437384561743753
Epoch 1571
Epoch 1571 :: Batch 0/1
Batch Loss = 4.437384560815384
1571, epoch_train_loss=4.437384560815384
Epoch 1572
Epoch 1572 :: Batch 0/1
Batch Loss = 4.437384559886494
1572, epoch_train_loss=4.437384559886494
Epoch 1573
Epoch 1573 :: Batch 0/1
Batch Loss = 4.4373845589570795
1573, epoch_train_loss=4.4373845589570795
Epoch 1574
Epoch 1574 :: Batch 0/1
Batch Loss = 4.437384558027134
1574, epoch_train_loss=4.437384558027134
Epoch 1575
Epoch 1575 :: Batch 0/1
Batch Loss = 4.43738455709665
1575, epoch_train_loss=4.43738455709665
Epoch 1576
Epoch 1576 :: Batch 0/1
Batch Loss = 4.437384556165624
1576, epoch_train_loss=4.437384556165624
Epoch 1577
Epoch 1577 :: Batch 0/1
Batch Loss = 4.437384555234051
1577, epoch_train_loss=4.437384555234051
Epoch 1578
Epoch 1578 :: Batch 0/1
Batch Loss = 4.437384554301926
1578, epoch_train_loss=4.437384554301926
Epoch 1579
Epoch 1579 :: Batch 0/1
Batch Loss = 4.437384553369241
1579, epoch_train_loss=4.437384553369241
Epoch 1580
Epoch 1580 :: Batch 0/1
Batch Loss = 4.437384552435992
1580, epoch_train_loss=4.437384552435992
Epoch 1581
Epoch 1581 :: Batch 0/1
Batch Loss = 4.437384551502172
1581, epoch_train_loss=4.437384551502172
Epoch 1582
Epoch 1582 :: Batch 0/1
Batch Loss = 4.437384550567778
1582, epoch_train_loss=4.437384550567778
Epoch 1583
Epoch 1583 :: Batch 0/1
Batch Loss = 4.437384549632804
1583, epoch_train_loss=4.437384549632804
Epoch 1584
Epoch 1584 :: Batch 0/1
Batch Loss = 4.43738454869724
1584, epoch_train_loss=4.43738454869724
Epoch 1585
Epoch 1585 :: Batch 0/1
Batch Loss = 4.437384547761086
1585, epoch_train_loss=4.437384547761086
Epoch 1586
Epoch 1586 :: Batch 0/1
Batch Loss = 4.437384546824333
1586, epoch_train_loss=4.437384546824333
Epoch 1587
Epoch 1587 :: Batch 0/1
Batch Loss = 4.437384545886977
1587, epoch_train_loss=4.437384545886977
Epoch 1588
Epoch 1588 :: Batch 0/1
Batch Loss = 4.437384544949011
1588, epoch_train_loss=4.437384544949011
Epoch 1589
Epoch 1589 :: Batch 0/1
Batch Loss = 4.437384544010431
1589, epoch_train_loss=4.437384544010431
Epoch 1590
Epoch 1590 :: Batch 0/1
Batch Loss = 4.43738454307123
1590, epoch_train_loss=4.43738454307123
Epoch 1591
Epoch 1591 :: Batch 0/1
Batch Loss = 4.437384542131404
1591, epoch_train_loss=4.437384542131404
Epoch 1592
Epoch 1592 :: Batch 0/1
Batch Loss = 4.437384541190944
1592, epoch_train_loss=4.437384541190944
Epoch 1593
Epoch 1593 :: Batch 0/1
Batch Loss = 4.437384540249848
1593, epoch_train_loss=4.437384540249848
Epoch 1594
Epoch 1594 :: Batch 0/1
Batch Loss = 4.437384539308106
1594, epoch_train_loss=4.437384539308106
Epoch 1595
Epoch 1595 :: Batch 0/1
Batch Loss = 4.437384538365717
1595, epoch_train_loss=4.437384538365717
Epoch 1596
Epoch 1596 :: Batch 0/1
Batch Loss = 4.4373845374226715
1596, epoch_train_loss=4.4373845374226715
Epoch 1597
Epoch 1597 :: Batch 0/1
Batch Loss = 4.437384536478966
1597, epoch_train_loss=4.437384536478966
Epoch 1598
Epoch 1598 :: Batch 0/1
Batch Loss = 4.437384535534594
1598, epoch_train_loss=4.437384535534594
Epoch 1599
Epoch 1599 :: Batch 0/1
Batch Loss = 4.437384534589549
1599, epoch_train_loss=4.437384534589549
Epoch 1600
Epoch 1600 :: Batch 0/1
Batch Loss = 4.437384533643827
1600, epoch_train_loss=4.437384533643827
Epoch 1601
Epoch 1601 :: Batch 0/1
Batch Loss = 4.437384532697419
1601, epoch_train_loss=4.437384532697419
Epoch 1602
Epoch 1602 :: Batch 0/1
Batch Loss = 4.437384531750322
1602, epoch_train_loss=4.437384531750322
Epoch 1603
Epoch 1603 :: Batch 0/1
Batch Loss = 4.437384530802529
1603, epoch_train_loss=4.437384530802529
Epoch 1604
Epoch 1604 :: Batch 0/1
Batch Loss = 4.437384529854033
1604, epoch_train_loss=4.437384529854033
Epoch 1605
Epoch 1605 :: Batch 0/1
Batch Loss = 4.43738452890483
1605, epoch_train_loss=4.43738452890483
Epoch 1606
Epoch 1606 :: Batch 0/1
Batch Loss = 4.437384527954913
1606, epoch_train_loss=4.437384527954913
Epoch 1607
Epoch 1607 :: Batch 0/1
Batch Loss = 4.437384527004278
1607, epoch_train_loss=4.437384527004278
Epoch 1608
Epoch 1608 :: Batch 0/1
Batch Loss = 4.437384526052916
1608, epoch_train_loss=4.437384526052916
Epoch 1609
Epoch 1609 :: Batch 0/1
Batch Loss = 4.437384525100823
1609, epoch_train_loss=4.437384525100823
Epoch 1610
Epoch 1610 :: Batch 0/1
Batch Loss = 4.4373845241479914
1610, epoch_train_loss=4.4373845241479914
Epoch 1611
Epoch 1611 :: Batch 0/1
Batch Loss = 4.437384523194418
1611, epoch_train_loss=4.437384523194418
Epoch 1612
Epoch 1612 :: Batch 0/1
Batch Loss = 4.437384522240093
1612, epoch_train_loss=4.437384522240093
Epoch 1613
Epoch 1613 :: Batch 0/1
Batch Loss = 4.437384521285014
1613, epoch_train_loss=4.437384521285014
Epoch 1614
Epoch 1614 :: Batch 0/1
Batch Loss = 4.437384520329172
1614, epoch_train_loss=4.437384520329172
Epoch 1615
Epoch 1615 :: Batch 0/1
Batch Loss = 4.4373845193725625
1615, epoch_train_loss=4.4373845193725625
Epoch 1616
Epoch 1616 :: Batch 0/1
Batch Loss = 4.43738451841518
1616, epoch_train_loss=4.43738451841518
Epoch 1617
Epoch 1617 :: Batch 0/1
Batch Loss = 4.4373845174570175
1617, epoch_train_loss=4.4373845174570175
Epoch 1618
Epoch 1618 :: Batch 0/1
Batch Loss = 4.437384516498067
1618, epoch_train_loss=4.437384516498067
Epoch 1619
Epoch 1619 :: Batch 0/1
Batch Loss = 4.437384515538327
1619, epoch_train_loss=4.437384515538327
Epoch 1620
Epoch 1620 :: Batch 0/1
Batch Loss = 4.437384514577786
1620, epoch_train_loss=4.437384514577786
Epoch 1621
Epoch 1621 :: Batch 0/1
Batch Loss = 4.4373845136164425
1621, epoch_train_loss=4.4373845136164425
Epoch 1622
Epoch 1622 :: Batch 0/1
Batch Loss = 4.437384512654287
1622, epoch_train_loss=4.437384512654287
Epoch 1623
Epoch 1623 :: Batch 0/1
Batch Loss = 4.437384511691314
1623, epoch_train_loss=4.437384511691314
Epoch 1624
Epoch 1624 :: Batch 0/1
Batch Loss = 4.437384510727518
1624, epoch_train_loss=4.437384510727518
Epoch 1625
Epoch 1625 :: Batch 0/1
Batch Loss = 4.437384509762892
1625, epoch_train_loss=4.437384509762892
Epoch 1626
Epoch 1626 :: Batch 0/1
Batch Loss = 4.4373845087974315
1626, epoch_train_loss=4.4373845087974315
Epoch 1627
Epoch 1627 :: Batch 0/1
Batch Loss = 4.437384507831127
1627, epoch_train_loss=4.437384507831127
Epoch 1628
Epoch 1628 :: Batch 0/1
Batch Loss = 4.4373845068639755
1628, epoch_train_loss=4.4373845068639755
Epoch 1629
Epoch 1629 :: Batch 0/1
Batch Loss = 4.437384505895969
1629, epoch_train_loss=4.437384505895969
Epoch 1630
Epoch 1630 :: Batch 0/1
Batch Loss = 4.437384504927101
1630, epoch_train_loss=4.437384504927101
Epoch 1631
Epoch 1631 :: Batch 0/1
Batch Loss = 4.4373845039573645
1631, epoch_train_loss=4.4373845039573645
Epoch 1632
Epoch 1632 :: Batch 0/1
Batch Loss = 4.437384502986755
1632, epoch_train_loss=4.437384502986755
Epoch 1633
Epoch 1633 :: Batch 0/1
Batch Loss = 4.437384502015266
1633, epoch_train_loss=4.437384502015266
Epoch 1634
Epoch 1634 :: Batch 0/1
Batch Loss = 4.437384501042889
1634, epoch_train_loss=4.437384501042889
Epoch 1635
Epoch 1635 :: Batch 0/1
Batch Loss = 4.437384500069618
1635, epoch_train_loss=4.437384500069618
Epoch 1636
Epoch 1636 :: Batch 0/1
Batch Loss = 4.43738449909545
1636, epoch_train_loss=4.43738449909545
Epoch 1637
Epoch 1637 :: Batch 0/1
Batch Loss = 4.437384498120375
1637, epoch_train_loss=4.437384498120375
Epoch 1638
Epoch 1638 :: Batch 0/1
Batch Loss = 4.437384497144386
1638, epoch_train_loss=4.437384497144386
Epoch 1639
Epoch 1639 :: Batch 0/1
Batch Loss = 4.4373844961674775
1639, epoch_train_loss=4.4373844961674775
Epoch 1640
Epoch 1640 :: Batch 0/1
Batch Loss = 4.437384495189644
1640, epoch_train_loss=4.437384495189644
Epoch 1641
Epoch 1641 :: Batch 0/1
Batch Loss = 4.437384494210879
1641, epoch_train_loss=4.437384494210879
Epoch 1642
Epoch 1642 :: Batch 0/1
Batch Loss = 4.437384493231173
1642, epoch_train_loss=4.437384493231173
Epoch 1643
Epoch 1643 :: Batch 0/1
Batch Loss = 4.437384492250523
1643, epoch_train_loss=4.437384492250523
Epoch 1644
Epoch 1644 :: Batch 0/1
Batch Loss = 4.4373844912689195
1644, epoch_train_loss=4.4373844912689195
Epoch 1645
Epoch 1645 :: Batch 0/1
Batch Loss = 4.437384490286358
1645, epoch_train_loss=4.437384490286358
Epoch 1646
Epoch 1646 :: Batch 0/1
Batch Loss = 4.43738448930283
1646, epoch_train_loss=4.43738448930283
Epoch 1647
Epoch 1647 :: Batch 0/1
Batch Loss = 4.43738448831833
1647, epoch_train_loss=4.43738448831833
Epoch 1648
Epoch 1648 :: Batch 0/1
Batch Loss = 4.437384487332853
1648, epoch_train_loss=4.437384487332853
Epoch 1649
Epoch 1649 :: Batch 0/1
Batch Loss = 4.437384486346387
1649, epoch_train_loss=4.437384486346387
Epoch 1650
Epoch 1650 :: Batch 0/1
Batch Loss = 4.437384485358932
1650, epoch_train_loss=4.437384485358932
Epoch 1651
Epoch 1651 :: Batch 0/1
Batch Loss = 4.437384484370474
1651, epoch_train_loss=4.437384484370474
Epoch 1652
Epoch 1652 :: Batch 0/1
Batch Loss = 4.437384483381012
1652, epoch_train_loss=4.437384483381012
Epoch 1653
Epoch 1653 :: Batch 0/1
Batch Loss = 4.437384482390538
1653, epoch_train_loss=4.437384482390538
Epoch 1654
Epoch 1654 :: Batch 0/1
Batch Loss = 4.437384481399042
1654, epoch_train_loss=4.437384481399042
Epoch 1655
Epoch 1655 :: Batch 0/1
Batch Loss = 4.43738448040652
1655, epoch_train_loss=4.43738448040652
Epoch 1656
Epoch 1656 :: Batch 0/1
Batch Loss = 4.437384479412964
1656, epoch_train_loss=4.437384479412964
Epoch 1657
Epoch 1657 :: Batch 0/1
Batch Loss = 4.437384478418368
1657, epoch_train_loss=4.437384478418368
Epoch 1658
Epoch 1658 :: Batch 0/1
Batch Loss = 4.437384477422724
1658, epoch_train_loss=4.437384477422724
Epoch 1659
Epoch 1659 :: Batch 0/1
Batch Loss = 4.437384476426026
1659, epoch_train_loss=4.437384476426026
Epoch 1660
Epoch 1660 :: Batch 0/1
Batch Loss = 4.437384475428266
1660, epoch_train_loss=4.437384475428266
Epoch 1661
Epoch 1661 :: Batch 0/1
Batch Loss = 4.437384474429437
1661, epoch_train_loss=4.437384474429437
Epoch 1662
Epoch 1662 :: Batch 0/1
Batch Loss = 4.437384473429534
1662, epoch_train_loss=4.437384473429534
Epoch 1663
Epoch 1663 :: Batch 0/1
Batch Loss = 4.437384472428548
1663, epoch_train_loss=4.437384472428548
Epoch 1664
Epoch 1664 :: Batch 0/1
Batch Loss = 4.437384471426472
1664, epoch_train_loss=4.437384471426472
Epoch 1665
Epoch 1665 :: Batch 0/1
Batch Loss = 4.437384470423298
1665, epoch_train_loss=4.437384470423298
Epoch 1666
Epoch 1666 :: Batch 0/1
Batch Loss = 4.437384469419021
1666, epoch_train_loss=4.437384469419021
Epoch 1667
Epoch 1667 :: Batch 0/1
Batch Loss = 4.437384468413632
1667, epoch_train_loss=4.437384468413632
Epoch 1668
Epoch 1668 :: Batch 0/1
Batch Loss = 4.437384467407126
1668, epoch_train_loss=4.437384467407126
Epoch 1669
Epoch 1669 :: Batch 0/1
Batch Loss = 4.437384466399493
1669, epoch_train_loss=4.437384466399493
Epoch 1670
Epoch 1670 :: Batch 0/1
Batch Loss = 4.437384465390727
1670, epoch_train_loss=4.437384465390727
Epoch 1671
Epoch 1671 :: Batch 0/1
Batch Loss = 4.437384464380822
1671, epoch_train_loss=4.437384464380822
Epoch 1672
Epoch 1672 :: Batch 0/1
Batch Loss = 4.437384463369769
1672, epoch_train_loss=4.437384463369769
Epoch 1673
Epoch 1673 :: Batch 0/1
Batch Loss = 4.437384462357561
1673, epoch_train_loss=4.437384462357561
Epoch 1674
Epoch 1674 :: Batch 0/1
Batch Loss = 4.437384461344192
1674, epoch_train_loss=4.437384461344192
Epoch 1675
Epoch 1675 :: Batch 0/1
Batch Loss = 4.437384460329652
1675, epoch_train_loss=4.437384460329652
Epoch 1676
Epoch 1676 :: Batch 0/1
Batch Loss = 4.437384459313935
1676, epoch_train_loss=4.437384459313935
Epoch 1677
Epoch 1677 :: Batch 0/1
Batch Loss = 4.437384458297035
1677, epoch_train_loss=4.437384458297035
Epoch 1678
Epoch 1678 :: Batch 0/1
Batch Loss = 4.437384457278942
1678, epoch_train_loss=4.437384457278942
Epoch 1679
Epoch 1679 :: Batch 0/1
Batch Loss = 4.437384456259649
1679, epoch_train_loss=4.437384456259649
Epoch 1680
Epoch 1680 :: Batch 0/1
Batch Loss = 4.43738445523915
1680, epoch_train_loss=4.43738445523915
Epoch 1681
Epoch 1681 :: Batch 0/1
Batch Loss = 4.437384454217437
1681, epoch_train_loss=4.437384454217437
Epoch 1682
Epoch 1682 :: Batch 0/1
Batch Loss = 4.437384453194502
1682, epoch_train_loss=4.437384453194502
Epoch 1683
Epoch 1683 :: Batch 0/1
Batch Loss = 4.437384452170337
1683, epoch_train_loss=4.437384452170337
Epoch 1684
Epoch 1684 :: Batch 0/1
Batch Loss = 4.4373844511449345
1684, epoch_train_loss=4.4373844511449345
Epoch 1685
Epoch 1685 :: Batch 0/1
Batch Loss = 4.437384450118287
1685, epoch_train_loss=4.437384450118287
Epoch 1686
Epoch 1686 :: Batch 0/1
Batch Loss = 4.437384449090388
1686, epoch_train_loss=4.437384449090388
Epoch 1687
Epoch 1687 :: Batch 0/1
Batch Loss = 4.437384448061229
1687, epoch_train_loss=4.437384448061229
Epoch 1688
Epoch 1688 :: Batch 0/1
Batch Loss = 4.4373844470308
1688, epoch_train_loss=4.4373844470308
Epoch 1689
Epoch 1689 :: Batch 0/1
Batch Loss = 4.437384445999097
1689, epoch_train_loss=4.437384445999097
Epoch 1690
Epoch 1690 :: Batch 0/1
Batch Loss = 4.43738444496611
1690, epoch_train_loss=4.43738444496611
Epoch 1691
Epoch 1691 :: Batch 0/1
Batch Loss = 4.4373844439318315
1691, epoch_train_loss=4.4373844439318315
Epoch 1692
Epoch 1692 :: Batch 0/1
Batch Loss = 4.437384442896253
1692, epoch_train_loss=4.437384442896253
Epoch 1693
Epoch 1693 :: Batch 0/1
Batch Loss = 4.437384441859368
1693, epoch_train_loss=4.437384441859368
Epoch 1694
Epoch 1694 :: Batch 0/1
Batch Loss = 4.437384440821167
1694, epoch_train_loss=4.437384440821167
Epoch 1695
Epoch 1695 :: Batch 0/1
Batch Loss = 4.437384439781644
1695, epoch_train_loss=4.437384439781644
Epoch 1696
Epoch 1696 :: Batch 0/1
Batch Loss = 4.43738443874079
1696, epoch_train_loss=4.43738443874079
Epoch 1697
Epoch 1697 :: Batch 0/1
Batch Loss = 4.437384437698597
1697, epoch_train_loss=4.437384437698597
Epoch 1698
Epoch 1698 :: Batch 0/1
Batch Loss = 4.437384436655056
1698, epoch_train_loss=4.437384436655056
Epoch 1699
Epoch 1699 :: Batch 0/1
Batch Loss = 4.437384435610159
1699, epoch_train_loss=4.437384435610159
Epoch 1700
Epoch 1700 :: Batch 0/1
Batch Loss = 4.4373844345639
1700, epoch_train_loss=4.4373844345639
Epoch 1701
Epoch 1701 :: Batch 0/1
Batch Loss = 4.437384433516269
1701, epoch_train_loss=4.437384433516269
Epoch 1702
Epoch 1702 :: Batch 0/1
Batch Loss = 4.43738443246726
1702, epoch_train_loss=4.43738443246726
Epoch 1703
Epoch 1703 :: Batch 0/1
Batch Loss = 4.437384431416862
1703, epoch_train_loss=4.437384431416862
Epoch 1704
Epoch 1704 :: Batch 0/1
Batch Loss = 4.437384430365067
1704, epoch_train_loss=4.437384430365067
Epoch 1705
Epoch 1705 :: Batch 0/1
Batch Loss = 4.437384429311868
1705, epoch_train_loss=4.437384429311868
Epoch 1706
Epoch 1706 :: Batch 0/1
Batch Loss = 4.437384428257258
1706, epoch_train_loss=4.437384428257258
Epoch 1707
Epoch 1707 :: Batch 0/1
Batch Loss = 4.437384427201224
1707, epoch_train_loss=4.437384427201224
Epoch 1708
Epoch 1708 :: Batch 0/1
Batch Loss = 4.437384426143763
1708, epoch_train_loss=4.437384426143763
Epoch 1709
Epoch 1709 :: Batch 0/1
Batch Loss = 4.437384425084864
1709, epoch_train_loss=4.437384425084864
Epoch 1710
Epoch 1710 :: Batch 0/1
Batch Loss = 4.4373844240245175
1710, epoch_train_loss=4.4373844240245175
Epoch 1711
Epoch 1711 :: Batch 0/1
Batch Loss = 4.437384422962717
1711, epoch_train_loss=4.437384422962717
Epoch 1712
Epoch 1712 :: Batch 0/1
Batch Loss = 4.437384421899454
1712, epoch_train_loss=4.437384421899454
Epoch 1713
Epoch 1713 :: Batch 0/1
Batch Loss = 4.437384420834718
1713, epoch_train_loss=4.437384420834718
Epoch 1714
Epoch 1714 :: Batch 0/1
Batch Loss = 4.437384419768502
1714, epoch_train_loss=4.437384419768502
Epoch 1715
Epoch 1715 :: Batch 0/1
Batch Loss = 4.4373844187007965
1715, epoch_train_loss=4.4373844187007965
Epoch 1716
Epoch 1716 :: Batch 0/1
Batch Loss = 4.437384417631594
1716, epoch_train_loss=4.437384417631594
Epoch 1717
Epoch 1717 :: Batch 0/1
Batch Loss = 4.437384416560886
1717, epoch_train_loss=4.437384416560886
Epoch 1718
Epoch 1718 :: Batch 0/1
Batch Loss = 4.437384415488662
1718, epoch_train_loss=4.437384415488662
Epoch 1719
Epoch 1719 :: Batch 0/1
Batch Loss = 4.437384414414914
1719, epoch_train_loss=4.437384414414914
Epoch 1720
Epoch 1720 :: Batch 0/1
Batch Loss = 4.437384413339633
1720, epoch_train_loss=4.437384413339633
Epoch 1721
Epoch 1721 :: Batch 0/1
Batch Loss = 4.437384412262812
1721, epoch_train_loss=4.437384412262812
Epoch 1722
Epoch 1722 :: Batch 0/1
Batch Loss = 4.43738441118444
1722, epoch_train_loss=4.43738441118444
Epoch 1723
Epoch 1723 :: Batch 0/1
Batch Loss = 4.437384410104508
1723, epoch_train_loss=4.437384410104508
Epoch 1724
Epoch 1724 :: Batch 0/1
Batch Loss = 4.437384409023009
1724, epoch_train_loss=4.437384409023009
Epoch 1725
Epoch 1725 :: Batch 0/1
Batch Loss = 4.437384407939932
1725, epoch_train_loss=4.437384407939932
Epoch 1726
Epoch 1726 :: Batch 0/1
Batch Loss = 4.43738440685527
1726, epoch_train_loss=4.43738440685527
Epoch 1727
Epoch 1727 :: Batch 0/1
Batch Loss = 4.437384405769012
1727, epoch_train_loss=4.437384405769012
Epoch 1728
Epoch 1728 :: Batch 0/1
Batch Loss = 4.43738440468115
1728, epoch_train_loss=4.43738440468115
Epoch 1729
Epoch 1729 :: Batch 0/1
Batch Loss = 4.4373844035916745
1729, epoch_train_loss=4.4373844035916745
Epoch 1730
Epoch 1730 :: Batch 0/1
Batch Loss = 4.437384402500577
1730, epoch_train_loss=4.437384402500577
Epoch 1731
Epoch 1731 :: Batch 0/1
Batch Loss = 4.437384401407845
1731, epoch_train_loss=4.437384401407845
Epoch 1732
Epoch 1732 :: Batch 0/1
Batch Loss = 4.437384400313475
1732, epoch_train_loss=4.437384400313475
Epoch 1733
Epoch 1733 :: Batch 0/1
Batch Loss = 4.4373843992174535
1733, epoch_train_loss=4.4373843992174535
Epoch 1734
Epoch 1734 :: Batch 0/1
Batch Loss = 4.4373843981197725
1734, epoch_train_loss=4.4373843981197725
Epoch 1735
Epoch 1735 :: Batch 0/1
Batch Loss = 4.437384397020422
1735, epoch_train_loss=4.437384397020422
Epoch 1736
Epoch 1736 :: Batch 0/1
Batch Loss = 4.437384395919393
1736, epoch_train_loss=4.437384395919393
Epoch 1737
Epoch 1737 :: Batch 0/1
Batch Loss = 4.437384394816675
1737, epoch_train_loss=4.437384394816675
Epoch 1738
Epoch 1738 :: Batch 0/1
Batch Loss = 4.437384393712261
1738, epoch_train_loss=4.437384393712261
Epoch 1739
Epoch 1739 :: Batch 0/1
Batch Loss = 4.43738439260614
1739, epoch_train_loss=4.43738439260614
Epoch 1740
Epoch 1740 :: Batch 0/1
Batch Loss = 4.437384391498302
1740, epoch_train_loss=4.437384391498302
Epoch 1741
Epoch 1741 :: Batch 0/1
Batch Loss = 4.437384390388738
1741, epoch_train_loss=4.437384390388738
Epoch 1742
Epoch 1742 :: Batch 0/1
Batch Loss = 4.437384389277438
1742, epoch_train_loss=4.437384389277438
Epoch 1743
Epoch 1743 :: Batch 0/1
Batch Loss = 4.437384388164392
1743, epoch_train_loss=4.437384388164392
Epoch 1744
Epoch 1744 :: Batch 0/1
Batch Loss = 4.4373843870495895
1744, epoch_train_loss=4.4373843870495895
Epoch 1745
Epoch 1745 :: Batch 0/1
Batch Loss = 4.437384385933024
1745, epoch_train_loss=4.437384385933024
Epoch 1746
Epoch 1746 :: Batch 0/1
Batch Loss = 4.437384384814683
1746, epoch_train_loss=4.437384384814683
Epoch 1747
Epoch 1747 :: Batch 0/1
Batch Loss = 4.437384383694556
1747, epoch_train_loss=4.437384383694556
Epoch 1748
Epoch 1748 :: Batch 0/1
Batch Loss = 4.437384382572636
1748, epoch_train_loss=4.437384382572636
Epoch 1749
Epoch 1749 :: Batch 0/1
Batch Loss = 4.4373843814489105
1749, epoch_train_loss=4.4373843814489105
Epoch 1750
Epoch 1750 :: Batch 0/1
Batch Loss = 4.43738438032337
1750, epoch_train_loss=4.43738438032337
Epoch 1751
Epoch 1751 :: Batch 0/1
Batch Loss = 4.437384379196003
1751, epoch_train_loss=4.437384379196003
Epoch 1752
Epoch 1752 :: Batch 0/1
Batch Loss = 4.437384378066804
1752, epoch_train_loss=4.437384378066804
Epoch 1753
Epoch 1753 :: Batch 0/1
Batch Loss = 4.437384376935759
1753, epoch_train_loss=4.437384376935759
Epoch 1754
Epoch 1754 :: Batch 0/1
Batch Loss = 4.437384375802857
1754, epoch_train_loss=4.437384375802857
Epoch 1755
Epoch 1755 :: Batch 0/1
Batch Loss = 4.437384374668091
1755, epoch_train_loss=4.437384374668091
Epoch 1756
Epoch 1756 :: Batch 0/1
Batch Loss = 4.437384373531448
1756, epoch_train_loss=4.437384373531448
Epoch 1757
Epoch 1757 :: Batch 0/1
Batch Loss = 4.43738437239292
1757, epoch_train_loss=4.43738437239292
Epoch 1758
Epoch 1758 :: Batch 0/1
Batch Loss = 4.437384371252494
1758, epoch_train_loss=4.437384371252494
Epoch 1759
Epoch 1759 :: Batch 0/1
Batch Loss = 4.437384370110163
1759, epoch_train_loss=4.437384370110163
Epoch 1760
Epoch 1760 :: Batch 0/1
Batch Loss = 4.437384368965912
1760, epoch_train_loss=4.437384368965912
Epoch 1761
Epoch 1761 :: Batch 0/1
Batch Loss = 4.437384367819735
1761, epoch_train_loss=4.437384367819735
Epoch 1762
Epoch 1762 :: Batch 0/1
Batch Loss = 4.437384366671618
1762, epoch_train_loss=4.437384366671618
Epoch 1763
Epoch 1763 :: Batch 0/1
Batch Loss = 4.4373843655215515
1763, epoch_train_loss=4.4373843655215515
Epoch 1764
Epoch 1764 :: Batch 0/1
Batch Loss = 4.437384364369527
1764, epoch_train_loss=4.437384364369527
Epoch 1765
Epoch 1765 :: Batch 0/1
Batch Loss = 4.437384363215529
1765, epoch_train_loss=4.437384363215529
Epoch 1766
Epoch 1766 :: Batch 0/1
Batch Loss = 4.437384362059552
1766, epoch_train_loss=4.437384362059552
Epoch 1767
Epoch 1767 :: Batch 0/1
Batch Loss = 4.437384360901581
1767, epoch_train_loss=4.437384360901581
Epoch 1768
Epoch 1768 :: Batch 0/1
Batch Loss = 4.437384359741608
1768, epoch_train_loss=4.437384359741608
Epoch 1769
Epoch 1769 :: Batch 0/1
Batch Loss = 4.43738435857962
1769, epoch_train_loss=4.43738435857962
Epoch 1770
Epoch 1770 :: Batch 0/1
Batch Loss = 4.437384357415606
1770, epoch_train_loss=4.437384357415606
Epoch 1771
Epoch 1771 :: Batch 0/1
Batch Loss = 4.437384356249557
1771, epoch_train_loss=4.437384356249557
Epoch 1772
Epoch 1772 :: Batch 0/1
Batch Loss = 4.437384355081461
1772, epoch_train_loss=4.437384355081461
Epoch 1773
Epoch 1773 :: Batch 0/1
Batch Loss = 4.437384353911305
1773, epoch_train_loss=4.437384353911305
Epoch 1774
Epoch 1774 :: Batch 0/1
Batch Loss = 4.437384352739081
1774, epoch_train_loss=4.437384352739081
Epoch 1775
Epoch 1775 :: Batch 0/1
Batch Loss = 4.437384351564775
1775, epoch_train_loss=4.437384351564775
Epoch 1776
Epoch 1776 :: Batch 0/1
Batch Loss = 4.437384350388378
1776, epoch_train_loss=4.437384350388378
Epoch 1777
Epoch 1777 :: Batch 0/1
Batch Loss = 4.437384349209876
1777, epoch_train_loss=4.437384349209876
Epoch 1778
Epoch 1778 :: Batch 0/1
Batch Loss = 4.4373843480292585
1778, epoch_train_loss=4.4373843480292585
Epoch 1779
Epoch 1779 :: Batch 0/1
Batch Loss = 4.437384346846517
1779, epoch_train_loss=4.437384346846517
Epoch 1780
Epoch 1780 :: Batch 0/1
Batch Loss = 4.437384345661636
1780, epoch_train_loss=4.437384345661636
Epoch 1781
Epoch 1781 :: Batch 0/1
Batch Loss = 4.437384344474605
1781, epoch_train_loss=4.437384344474605
Epoch 1782
Epoch 1782 :: Batch 0/1
Batch Loss = 4.437384343285414
1782, epoch_train_loss=4.437384343285414
Epoch 1783
Epoch 1783 :: Batch 0/1
Batch Loss = 4.437384342094049
1783, epoch_train_loss=4.437384342094049
Epoch 1784
Epoch 1784 :: Batch 0/1
Batch Loss = 4.4373843409005
1784, epoch_train_loss=4.4373843409005
Epoch 1785
Epoch 1785 :: Batch 0/1
Batch Loss = 4.437384339704753
1785, epoch_train_loss=4.437384339704753
Epoch 1786
Epoch 1786 :: Batch 0/1
Batch Loss = 4.437384338506799
1786, epoch_train_loss=4.437384338506799
Epoch 1787
Epoch 1787 :: Batch 0/1
Batch Loss = 4.437384337306623
1787, epoch_train_loss=4.437384337306623
Epoch 1788
Epoch 1788 :: Batch 0/1
Batch Loss = 4.437384336104217
1788, epoch_train_loss=4.437384336104217
Epoch 1789
Epoch 1789 :: Batch 0/1
Batch Loss = 4.437384334899565
1789, epoch_train_loss=4.437384334899565
Epoch 1790
Epoch 1790 :: Batch 0/1
Batch Loss = 4.437384333692657
1790, epoch_train_loss=4.437384333692657
Epoch 1791
Epoch 1791 :: Batch 0/1
Batch Loss = 4.437384332483481
1791, epoch_train_loss=4.437384332483481
Epoch 1792
Epoch 1792 :: Batch 0/1
Batch Loss = 4.437384331272023
1792, epoch_train_loss=4.437384331272023
Epoch 1793
Epoch 1793 :: Batch 0/1
Batch Loss = 4.437384330058272
1793, epoch_train_loss=4.437384330058272
Epoch 1794
Epoch 1794 :: Batch 0/1
Batch Loss = 4.437384328842216
1794, epoch_train_loss=4.437384328842216
Epoch 1795
Epoch 1795 :: Batch 0/1
Batch Loss = 4.437384327623841
1795, epoch_train_loss=4.437384327623841
Epoch 1796
Epoch 1796 :: Batch 0/1
Batch Loss = 4.437384326403136
1796, epoch_train_loss=4.437384326403136
Epoch 1797
Epoch 1797 :: Batch 0/1
Batch Loss = 4.437384325180089
1797, epoch_train_loss=4.437384325180089
Epoch 1798
Epoch 1798 :: Batch 0/1
Batch Loss = 4.437384323954685
1798, epoch_train_loss=4.437384323954685
Epoch 1799
Epoch 1799 :: Batch 0/1
Batch Loss = 4.437384322726913
1799, epoch_train_loss=4.437384322726913
Epoch 1800
Epoch 1800 :: Batch 0/1
Batch Loss = 4.4373843214967605
1800, epoch_train_loss=4.4373843214967605
Epoch 1801
Epoch 1801 :: Batch 0/1
Batch Loss = 4.437384320264214
1801, epoch_train_loss=4.437384320264214
Epoch 1802
Epoch 1802 :: Batch 0/1
Batch Loss = 4.437384319029261
1802, epoch_train_loss=4.437384319029261
Epoch 1803
Epoch 1803 :: Batch 0/1
Batch Loss = 4.437384317791888
1803, epoch_train_loss=4.437384317791888
Epoch 1804
Epoch 1804 :: Batch 0/1
Batch Loss = 4.437384316552082
1804, epoch_train_loss=4.437384316552082
Epoch 1805
Epoch 1805 :: Batch 0/1
Batch Loss = 4.437384315309831
1805, epoch_train_loss=4.437384315309831
Epoch 1806
Epoch 1806 :: Batch 0/1
Batch Loss = 4.437384314065121
1806, epoch_train_loss=4.437384314065121
Epoch 1807
Epoch 1807 :: Batch 0/1
Batch Loss = 4.43738431281794
1807, epoch_train_loss=4.43738431281794
Epoch 1808
Epoch 1808 :: Batch 0/1
Batch Loss = 4.437384311568272
1808, epoch_train_loss=4.437384311568272
Epoch 1809
Epoch 1809 :: Batch 0/1
Batch Loss = 4.437384310316106
1809, epoch_train_loss=4.437384310316106
Epoch 1810
Epoch 1810 :: Batch 0/1
Batch Loss = 4.437384309061429
1810, epoch_train_loss=4.437384309061429
Epoch 1811
Epoch 1811 :: Batch 0/1
Batch Loss = 4.437384307804224
1811, epoch_train_loss=4.437384307804224
Epoch 1812
Epoch 1812 :: Batch 0/1
Batch Loss = 4.437384306544482
1812, epoch_train_loss=4.437384306544482
Epoch 1813
Epoch 1813 :: Batch 0/1
Batch Loss = 4.437384305282187
1813, epoch_train_loss=4.437384305282187
Epoch 1814
Epoch 1814 :: Batch 0/1
Batch Loss = 4.437384304017326
1814, epoch_train_loss=4.437384304017326
Epoch 1815
Epoch 1815 :: Batch 0/1
Batch Loss = 4.437384302749884
1815, epoch_train_loss=4.437384302749884
Epoch 1816
Epoch 1816 :: Batch 0/1
Batch Loss = 4.437384301479847
1816, epoch_train_loss=4.437384301479847
Epoch 1817
Epoch 1817 :: Batch 0/1
Batch Loss = 4.437384300207204
1817, epoch_train_loss=4.437384300207204
Epoch 1818
Epoch 1818 :: Batch 0/1
Batch Loss = 4.4373842989319385
1818, epoch_train_loss=4.4373842989319385
Epoch 1819
Epoch 1819 :: Batch 0/1
Batch Loss = 4.437384297654036
1819, epoch_train_loss=4.437384297654036
Epoch 1820
Epoch 1820 :: Batch 0/1
Batch Loss = 4.4373842963734855
1820, epoch_train_loss=4.4373842963734855
Epoch 1821
Epoch 1821 :: Batch 0/1
Batch Loss = 4.43738429509027
1821, epoch_train_loss=4.43738429509027
Epoch 1822
Epoch 1822 :: Batch 0/1
Batch Loss = 4.437384293804376
1822, epoch_train_loss=4.437384293804376
Epoch 1823
Epoch 1823 :: Batch 0/1
Batch Loss = 4.437384292515788
1823, epoch_train_loss=4.437384292515788
Epoch 1824
Epoch 1824 :: Batch 0/1
Batch Loss = 4.437384291224495
1824, epoch_train_loss=4.437384291224495
Epoch 1825
Epoch 1825 :: Batch 0/1
Batch Loss = 4.437384289930478
1825, epoch_train_loss=4.437384289930478
Epoch 1826
Epoch 1826 :: Batch 0/1
Batch Loss = 4.437384288633725
1826, epoch_train_loss=4.437384288633725
Epoch 1827
Epoch 1827 :: Batch 0/1
Batch Loss = 4.437384287334221
1827, epoch_train_loss=4.437384287334221
Epoch 1828
Epoch 1828 :: Batch 0/1
Batch Loss = 4.437384286031952
1828, epoch_train_loss=4.437384286031952
Epoch 1829
Epoch 1829 :: Batch 0/1
Batch Loss = 4.437384284726901
1829, epoch_train_loss=4.437384284726901
Epoch 1830
Epoch 1830 :: Batch 0/1
Batch Loss = 4.437384283419055
1830, epoch_train_loss=4.437384283419055
Epoch 1831
Epoch 1831 :: Batch 0/1
Batch Loss = 4.437384282108399
1831, epoch_train_loss=4.437384282108399
Epoch 1832
Epoch 1832 :: Batch 0/1
Batch Loss = 4.437384280794916
1832, epoch_train_loss=4.437384280794916
Epoch 1833
Epoch 1833 :: Batch 0/1
Batch Loss = 4.437384279478594
1833, epoch_train_loss=4.437384279478594
Epoch 1834
Epoch 1834 :: Batch 0/1
Batch Loss = 4.437384278159414
1834, epoch_train_loss=4.437384278159414
Epoch 1835
Epoch 1835 :: Batch 0/1
Batch Loss = 4.437384276837364
1835, epoch_train_loss=4.437384276837364
Epoch 1836
Epoch 1836 :: Batch 0/1
Batch Loss = 4.437384275512427
1836, epoch_train_loss=4.437384275512427
Epoch 1837
Epoch 1837 :: Batch 0/1
Batch Loss = 4.437384274184588
1837, epoch_train_loss=4.437384274184588
Epoch 1838
Epoch 1838 :: Batch 0/1
Batch Loss = 4.437384272853829
1838, epoch_train_loss=4.437384272853829
Epoch 1839
Epoch 1839 :: Batch 0/1
Batch Loss = 4.43738427152014
1839, epoch_train_loss=4.43738427152014
Epoch 1840
Epoch 1840 :: Batch 0/1
Batch Loss = 4.437384270183499
1840, epoch_train_loss=4.437384270183499
Epoch 1841
Epoch 1841 :: Batch 0/1
Batch Loss = 4.437384268843894
1841, epoch_train_loss=4.437384268843894
Epoch 1842
Epoch 1842 :: Batch 0/1
Batch Loss = 4.437384267501308
1842, epoch_train_loss=4.437384267501308
Epoch 1843
Epoch 1843 :: Batch 0/1
Batch Loss = 4.437384266155725
1843, epoch_train_loss=4.437384266155725
Epoch 1844
Epoch 1844 :: Batch 0/1
Batch Loss = 4.437384264807129
1844, epoch_train_loss=4.437384264807129
Epoch 1845
Epoch 1845 :: Batch 0/1
Batch Loss = 4.437384263455504
1845, epoch_train_loss=4.437384263455504
Epoch 1846
Epoch 1846 :: Batch 0/1
Batch Loss = 4.437384262100832
1846, epoch_train_loss=4.437384262100832
Epoch 1847
Epoch 1847 :: Batch 0/1
Batch Loss = 4.4373842607431
1847, epoch_train_loss=4.4373842607431
Epoch 1848
Epoch 1848 :: Batch 0/1
Batch Loss = 4.437384259382288
1848, epoch_train_loss=4.437384259382288
Epoch 1849
Epoch 1849 :: Batch 0/1
Batch Loss = 4.4373842580183815
1849, epoch_train_loss=4.4373842580183815
Epoch 1850
Epoch 1850 :: Batch 0/1
Batch Loss = 4.437384256651364
1850, epoch_train_loss=4.437384256651364
Epoch 1851
Epoch 1851 :: Batch 0/1
Batch Loss = 4.437384255281217
1851, epoch_train_loss=4.437384255281217
Epoch 1852
Epoch 1852 :: Batch 0/1
Batch Loss = 4.437384253907926
1852, epoch_train_loss=4.437384253907926
Epoch 1853
Epoch 1853 :: Batch 0/1
Batch Loss = 4.437384252531473
1853, epoch_train_loss=4.437384252531473
Epoch 1854
Epoch 1854 :: Batch 0/1
Batch Loss = 4.437384251151839
1854, epoch_train_loss=4.437384251151839
Epoch 1855
Epoch 1855 :: Batch 0/1
Batch Loss = 4.437384249769009
1855, epoch_train_loss=4.437384249769009
Epoch 1856
Epoch 1856 :: Batch 0/1
Batch Loss = 4.437384248382966
1856, epoch_train_loss=4.437384248382966
Epoch 1857
Epoch 1857 :: Batch 0/1
Batch Loss = 4.437384246993692
1857, epoch_train_loss=4.437384246993692
Epoch 1858
Epoch 1858 :: Batch 0/1
Batch Loss = 4.43738424560117
1858, epoch_train_loss=4.43738424560117
Epoch 1859
Epoch 1859 :: Batch 0/1
Batch Loss = 4.437384244205382
1859, epoch_train_loss=4.437384244205382
Epoch 1860
Epoch 1860 :: Batch 0/1
Batch Loss = 4.43738424280631
1860, epoch_train_loss=4.43738424280631
Epoch 1861
Epoch 1861 :: Batch 0/1
Batch Loss = 4.4373842414039375
1861, epoch_train_loss=4.4373842414039375
Epoch 1862
Epoch 1862 :: Batch 0/1
Batch Loss = 4.437384239998246
1862, epoch_train_loss=4.437384239998246
Epoch 1863
Epoch 1863 :: Batch 0/1
Batch Loss = 4.4373842385892175
1863, epoch_train_loss=4.4373842385892175
Epoch 1864
Epoch 1864 :: Batch 0/1
Batch Loss = 4.4373842371768335
1864, epoch_train_loss=4.4373842371768335
Epoch 1865
Epoch 1865 :: Batch 0/1
Batch Loss = 4.437384235761076
1865, epoch_train_loss=4.437384235761076
Epoch 1866
Epoch 1866 :: Batch 0/1
Batch Loss = 4.437384234341928
1866, epoch_train_loss=4.437384234341928
Epoch 1867
Epoch 1867 :: Batch 0/1
Batch Loss = 4.437384232919369
1867, epoch_train_loss=4.437384232919369
Epoch 1868
Epoch 1868 :: Batch 0/1
Batch Loss = 4.4373842314933825
1868, epoch_train_loss=4.4373842314933825
Epoch 1869
Epoch 1869 :: Batch 0/1
Batch Loss = 4.437384230063949
1869, epoch_train_loss=4.437384230063949
Epoch 1870
Epoch 1870 :: Batch 0/1
Batch Loss = 4.43738422863105
1870, epoch_train_loss=4.43738422863105
Epoch 1871
Epoch 1871 :: Batch 0/1
Batch Loss = 4.437384227194667
1871, epoch_train_loss=4.437384227194667
Epoch 1872
Epoch 1872 :: Batch 0/1
Batch Loss = 4.437384225754781
1872, epoch_train_loss=4.437384225754781
Epoch 1873
Epoch 1873 :: Batch 0/1
Batch Loss = 4.437384224311372
1873, epoch_train_loss=4.437384224311372
Epoch 1874
Epoch 1874 :: Batch 0/1
Batch Loss = 4.437384222864422
1874, epoch_train_loss=4.437384222864422
Epoch 1875
Epoch 1875 :: Batch 0/1
Batch Loss = 4.437384221413911
1875, epoch_train_loss=4.437384221413911
Epoch 1876
Epoch 1876 :: Batch 0/1
Batch Loss = 4.437384219959821
1876, epoch_train_loss=4.437384219959821
Epoch 1877
Epoch 1877 :: Batch 0/1
Batch Loss = 4.437384218502131
1877, epoch_train_loss=4.437384218502131
Epoch 1878
Epoch 1878 :: Batch 0/1
Batch Loss = 4.4373842170408215
1878, epoch_train_loss=4.4373842170408215
Epoch 1879
Epoch 1879 :: Batch 0/1
Batch Loss = 4.437384215575873
1879, epoch_train_loss=4.437384215575873
Epoch 1880
Epoch 1880 :: Batch 0/1
Batch Loss = 4.437384214107267
1880, epoch_train_loss=4.437384214107267
Epoch 1881
Epoch 1881 :: Batch 0/1
Batch Loss = 4.437384212634981
1881, epoch_train_loss=4.437384212634981
Epoch 1882
Epoch 1882 :: Batch 0/1
Batch Loss = 4.437384211158997
1882, epoch_train_loss=4.437384211158997
Epoch 1883
Epoch 1883 :: Batch 0/1
Batch Loss = 4.437384209679296
1883, epoch_train_loss=4.437384209679296
Epoch 1884
Epoch 1884 :: Batch 0/1
Batch Loss = 4.437384208195853
1884, epoch_train_loss=4.437384208195853
Epoch 1885
Epoch 1885 :: Batch 0/1
Batch Loss = 4.437384206708653
1885, epoch_train_loss=4.437384206708653
Epoch 1886
Epoch 1886 :: Batch 0/1
Batch Loss = 4.437384205217671
1886, epoch_train_loss=4.437384205217671
Epoch 1887
Epoch 1887 :: Batch 0/1
Batch Loss = 4.437384203722889
1887, epoch_train_loss=4.437384203722889
Epoch 1888
Epoch 1888 :: Batch 0/1
Batch Loss = 4.437384202224285
1888, epoch_train_loss=4.437384202224285
Epoch 1889
Epoch 1889 :: Batch 0/1
Batch Loss = 4.437384200721839
1889, epoch_train_loss=4.437384200721839
Epoch 1890
Epoch 1890 :: Batch 0/1
Batch Loss = 4.43738419921553
1890, epoch_train_loss=4.43738419921553
Epoch 1891
Epoch 1891 :: Batch 0/1
Batch Loss = 4.4373841977053345
1891, epoch_train_loss=4.4373841977053345
Epoch 1892
Epoch 1892 :: Batch 0/1
Batch Loss = 4.4373841961912355
1892, epoch_train_loss=4.4373841961912355
Epoch 1893
Epoch 1893 :: Batch 0/1
Batch Loss = 4.437384194673205
1893, epoch_train_loss=4.437384194673205
Epoch 1894
Epoch 1894 :: Batch 0/1
Batch Loss = 4.437384193151229
1894, epoch_train_loss=4.437384193151229
Epoch 1895
Epoch 1895 :: Batch 0/1
Batch Loss = 4.43738419162528
1895, epoch_train_loss=4.43738419162528
Epoch 1896
Epoch 1896 :: Batch 0/1
Batch Loss = 4.437384190095339
1896, epoch_train_loss=4.437384190095339
Epoch 1897
Epoch 1897 :: Batch 0/1
Batch Loss = 4.437384188561384
1897, epoch_train_loss=4.437384188561384
Epoch 1898
Epoch 1898 :: Batch 0/1
Batch Loss = 4.437384187023392
1898, epoch_train_loss=4.437384187023392
Epoch 1899
Epoch 1899 :: Batch 0/1
Batch Loss = 4.437384185481339
1899, epoch_train_loss=4.437384185481339
Epoch 1900
Epoch 1900 :: Batch 0/1
Batch Loss = 4.4373841839352055
1900, epoch_train_loss=4.4373841839352055
Epoch 1901
Epoch 1901 :: Batch 0/1
Batch Loss = 4.437384182384969
1901, epoch_train_loss=4.437384182384969
Epoch 1902
Epoch 1902 :: Batch 0/1
Batch Loss = 4.4373841808306045
1902, epoch_train_loss=4.4373841808306045
Epoch 1903
Epoch 1903 :: Batch 0/1
Batch Loss = 4.43738417927209
1903, epoch_train_loss=4.43738417927209
Epoch 1904
Epoch 1904 :: Batch 0/1
Batch Loss = 4.437384177709405
1904, epoch_train_loss=4.437384177709405
Epoch 1905
Epoch 1905 :: Batch 0/1
Batch Loss = 4.437384176142523
1905, epoch_train_loss=4.437384176142523
Epoch 1906
Epoch 1906 :: Batch 0/1
Batch Loss = 4.437384174571423
1906, epoch_train_loss=4.437384174571423
Epoch 1907
Epoch 1907 :: Batch 0/1
Batch Loss = 4.437384172996079
1907, epoch_train_loss=4.437384172996079
Epoch 1908
Epoch 1908 :: Batch 0/1
Batch Loss = 4.437384171416471
1908, epoch_train_loss=4.437384171416471
Epoch 1909
Epoch 1909 :: Batch 0/1
Batch Loss = 4.437384169832572
1909, epoch_train_loss=4.437384169832572
Epoch 1910
Epoch 1910 :: Batch 0/1
Batch Loss = 4.4373841682443596
1910, epoch_train_loss=4.4373841682443596
Epoch 1911
Epoch 1911 :: Batch 0/1
Batch Loss = 4.4373841666518095
1911, epoch_train_loss=4.4373841666518095
Epoch 1912
Epoch 1912 :: Batch 0/1
Batch Loss = 4.437384165054898
1912, epoch_train_loss=4.437384165054898
Epoch 1913
Epoch 1913 :: Batch 0/1
Batch Loss = 4.4373841634536015
1913, epoch_train_loss=4.4373841634536015
Epoch 1914
Epoch 1914 :: Batch 0/1
Batch Loss = 4.437384161847894
1914, epoch_train_loss=4.437384161847894
Epoch 1915
Epoch 1915 :: Batch 0/1
Batch Loss = 4.437384160237753
1915, epoch_train_loss=4.437384160237753
Epoch 1916
Epoch 1916 :: Batch 0/1
Batch Loss = 4.437384158623148
1916, epoch_train_loss=4.437384158623148
Epoch 1917
Epoch 1917 :: Batch 0/1
Batch Loss = 4.437384157004062
1917, epoch_train_loss=4.437384157004062
Epoch 1918
Epoch 1918 :: Batch 0/1
Batch Loss = 4.437384155380464
1918, epoch_train_loss=4.437384155380464
Epoch 1919
Epoch 1919 :: Batch 0/1
Batch Loss = 4.4373841537523315
1919, epoch_train_loss=4.4373841537523315
Epoch 1920
Epoch 1920 :: Batch 0/1
Batch Loss = 4.437384152119638
1920, epoch_train_loss=4.437384152119638
Epoch 1921
Epoch 1921 :: Batch 0/1
Batch Loss = 4.43738415048236
1921, epoch_train_loss=4.43738415048236
Epoch 1922
Epoch 1922 :: Batch 0/1
Batch Loss = 4.437384148840468
1922, epoch_train_loss=4.437384148840468
Epoch 1923
Epoch 1923 :: Batch 0/1
Batch Loss = 4.437384147193939
1923, epoch_train_loss=4.437384147193939
Epoch 1924
Epoch 1924 :: Batch 0/1
Batch Loss = 4.437384145542745
1924, epoch_train_loss=4.437384145542745
Epoch 1925
Epoch 1925 :: Batch 0/1
Batch Loss = 4.437384143886861
1925, epoch_train_loss=4.437384143886861
Epoch 1926
Epoch 1926 :: Batch 0/1
Batch Loss = 4.437384142226261
1926, epoch_train_loss=4.437384142226261
Epoch 1927
Epoch 1927 :: Batch 0/1
Batch Loss = 4.4373841405609165
1927, epoch_train_loss=4.4373841405609165
Epoch 1928
Epoch 1928 :: Batch 0/1
Batch Loss = 4.4373841388908035
1928, epoch_train_loss=4.4373841388908035
Epoch 1929
Epoch 1929 :: Batch 0/1
Batch Loss = 4.437384137215892
1929, epoch_train_loss=4.437384137215892
Epoch 1930
Epoch 1930 :: Batch 0/1
Batch Loss = 4.437384135536155
1930, epoch_train_loss=4.437384135536155
Epoch 1931
Epoch 1931 :: Batch 0/1
Batch Loss = 4.437384133851569
1931, epoch_train_loss=4.437384133851569
Epoch 1932
Epoch 1932 :: Batch 0/1
Batch Loss = 4.437384132162102
1932, epoch_train_loss=4.437384132162102
Epoch 1933
Epoch 1933 :: Batch 0/1
Batch Loss = 4.437384130467728
1933, epoch_train_loss=4.437384130467728
Epoch 1934
Epoch 1934 :: Batch 0/1
Batch Loss = 4.43738412876842
1934, epoch_train_loss=4.43738412876842
Epoch 1935
Epoch 1935 :: Batch 0/1
Batch Loss = 4.437384127064147
1935, epoch_train_loss=4.437384127064147
Epoch 1936
Epoch 1936 :: Batch 0/1
Batch Loss = 4.437384125354887
1936, epoch_train_loss=4.437384125354887
Epoch 1937
Epoch 1937 :: Batch 0/1
Batch Loss = 4.437384123640605
1937, epoch_train_loss=4.437384123640605
Epoch 1938
Epoch 1938 :: Batch 0/1
Batch Loss = 4.4373841219212755
1938, epoch_train_loss=4.4373841219212755
Epoch 1939
Epoch 1939 :: Batch 0/1
Batch Loss = 4.437384120196868
1939, epoch_train_loss=4.437384120196868
Epoch 1940
Epoch 1940 :: Batch 0/1
Batch Loss = 4.437384118467355
1940, epoch_train_loss=4.437384118467355
Epoch 1941
Epoch 1941 :: Batch 0/1
Batch Loss = 4.437384116732706
1941, epoch_train_loss=4.437384116732706
Epoch 1942
Epoch 1942 :: Batch 0/1
Batch Loss = 4.437384114992893
1942, epoch_train_loss=4.437384114992893
Epoch 1943
Epoch 1943 :: Batch 0/1
Batch Loss = 4.4373841132478855
1943, epoch_train_loss=4.4373841132478855
Epoch 1944
Epoch 1944 :: Batch 0/1
Batch Loss = 4.4373841114976535
1944, epoch_train_loss=4.4373841114976535
Epoch 1945
Epoch 1945 :: Batch 0/1
Batch Loss = 4.437384109742169
1945, epoch_train_loss=4.437384109742169
Epoch 1946
Epoch 1946 :: Batch 0/1
Batch Loss = 4.437384107981398
1946, epoch_train_loss=4.437384107981398
Epoch 1947
Epoch 1947 :: Batch 0/1
Batch Loss = 4.437384106215313
1947, epoch_train_loss=4.437384106215313
Epoch 1948
Epoch 1948 :: Batch 0/1
Batch Loss = 4.437384104443883
1948, epoch_train_loss=4.437384104443883
Epoch 1949
Epoch 1949 :: Batch 0/1
Batch Loss = 4.4373841026670755
1949, epoch_train_loss=4.4373841026670755
Epoch 1950
Epoch 1950 :: Batch 0/1
Batch Loss = 4.437384100884862
1950, epoch_train_loss=4.437384100884862
Epoch 1951
Epoch 1951 :: Batch 0/1
Batch Loss = 4.437384099097209
1951, epoch_train_loss=4.437384099097209
Epoch 1952
Epoch 1952 :: Batch 0/1
Batch Loss = 4.437384097304086
1952, epoch_train_loss=4.437384097304086
Epoch 1953
Epoch 1953 :: Batch 0/1
Batch Loss = 4.437384095505462
1953, epoch_train_loss=4.437384095505462
Epoch 1954
Epoch 1954 :: Batch 0/1
Batch Loss = 4.437384093701303
1954, epoch_train_loss=4.437384093701303
Epoch 1955
Epoch 1955 :: Batch 0/1
Batch Loss = 4.437384091891579
1955, epoch_train_loss=4.437384091891579
Epoch 1956
Epoch 1956 :: Batch 0/1
Batch Loss = 4.437384090076255
1956, epoch_train_loss=4.437384090076255
Epoch 1957
Epoch 1957 :: Batch 0/1
Batch Loss = 4.4373840882553015
1957, epoch_train_loss=4.4373840882553015
Epoch 1958
Epoch 1958 :: Batch 0/1
Batch Loss = 4.437384086428684
1958, epoch_train_loss=4.437384086428684
Epoch 1959
Epoch 1959 :: Batch 0/1
Batch Loss = 4.43738408459637
1959, epoch_train_loss=4.43738408459637
Epoch 1960
Epoch 1960 :: Batch 0/1
Batch Loss = 4.437384082758324
1960, epoch_train_loss=4.437384082758324
Epoch 1961
Epoch 1961 :: Batch 0/1
Batch Loss = 4.437384080914516
1961, epoch_train_loss=4.437384080914516
Epoch 1962
Epoch 1962 :: Batch 0/1
Batch Loss = 4.43738407906491
1962, epoch_train_loss=4.43738407906491
Epoch 1963
Epoch 1963 :: Batch 0/1
Batch Loss = 4.437384077209473
1963, epoch_train_loss=4.437384077209473
Epoch 1964
Epoch 1964 :: Batch 0/1
Batch Loss = 4.43738407534817
1964, epoch_train_loss=4.43738407534817
Epoch 1965
Epoch 1965 :: Batch 0/1
Batch Loss = 4.437384073480968
1965, epoch_train_loss=4.437384073480968
Epoch 1966
Epoch 1966 :: Batch 0/1
Batch Loss = 4.4373840716078305
1966, epoch_train_loss=4.4373840716078305
Epoch 1967
Epoch 1967 :: Batch 0/1
Batch Loss = 4.437384069728722
1967, epoch_train_loss=4.437384069728722
Epoch 1968
Epoch 1968 :: Batch 0/1
Batch Loss = 4.43738406784361
1968, epoch_train_loss=4.43738406784361
Epoch 1969
Epoch 1969 :: Batch 0/1
Batch Loss = 4.4373840659524575
1969, epoch_train_loss=4.4373840659524575
Epoch 1970
Epoch 1970 :: Batch 0/1
Batch Loss = 4.437384064055228
1970, epoch_train_loss=4.437384064055228
Epoch 1971
Epoch 1971 :: Batch 0/1
Batch Loss = 4.437384062151885
1971, epoch_train_loss=4.437384062151885
Epoch 1972
Epoch 1972 :: Batch 0/1
Batch Loss = 4.437384060242396
1972, epoch_train_loss=4.437384060242396
Epoch 1973
Epoch 1973 :: Batch 0/1
Batch Loss = 4.437384058326721
1973, epoch_train_loss=4.437384058326721
Epoch 1974
Epoch 1974 :: Batch 0/1
Batch Loss = 4.437384056404825
1974, epoch_train_loss=4.437384056404825
Epoch 1975
Epoch 1975 :: Batch 0/1
Batch Loss = 4.437384054476668
1975, epoch_train_loss=4.437384054476668
Epoch 1976
Epoch 1976 :: Batch 0/1
Batch Loss = 4.4373840525422175
1976, epoch_train_loss=4.4373840525422175
Epoch 1977
Epoch 1977 :: Batch 0/1
Batch Loss = 4.437384050601432
1977, epoch_train_loss=4.437384050601432
Epoch 1978
Epoch 1978 :: Batch 0/1
Batch Loss = 4.437384048654275
1978, epoch_train_loss=4.437384048654275
Epoch 1979
Epoch 1979 :: Batch 0/1
Batch Loss = 4.43738404670071
1979, epoch_train_loss=4.43738404670071
Epoch 1980
Epoch 1980 :: Batch 0/1
Batch Loss = 4.437384044740695
1980, epoch_train_loss=4.437384044740695
Epoch 1981
Epoch 1981 :: Batch 0/1
Batch Loss = 4.437384042774195
1981, epoch_train_loss=4.437384042774195
Epoch 1982
Epoch 1982 :: Batch 0/1
Batch Loss = 4.437384040801168
1982, epoch_train_loss=4.437384040801168
Epoch 1983
Epoch 1983 :: Batch 0/1
Batch Loss = 4.437384038821578
1983, epoch_train_loss=4.437384038821578
Epoch 1984
Epoch 1984 :: Batch 0/1
Batch Loss = 4.4373840368353825
1984, epoch_train_loss=4.4373840368353825
Epoch 1985
Epoch 1985 :: Batch 0/1
Batch Loss = 4.437384034842544
1985, epoch_train_loss=4.437384034842544
Epoch 1986
Epoch 1986 :: Batch 0/1
Batch Loss = 4.43738403284302
1986, epoch_train_loss=4.43738403284302
Epoch 1987
Epoch 1987 :: Batch 0/1
Batch Loss = 4.437384030836772
1987, epoch_train_loss=4.437384030836772
Epoch 1988
Epoch 1988 :: Batch 0/1
Batch Loss = 4.437384028823757
1988, epoch_train_loss=4.437384028823757
Epoch 1989
Epoch 1989 :: Batch 0/1
Batch Loss = 4.4373840268039375
1989, epoch_train_loss=4.4373840268039375
Epoch 1990
Epoch 1990 :: Batch 0/1
Batch Loss = 4.437384024777269
1990, epoch_train_loss=4.437384024777269
Epoch 1991
Epoch 1991 :: Batch 0/1
Batch Loss = 4.4373840227437125
1991, epoch_train_loss=4.4373840227437125
Epoch 1992
Epoch 1992 :: Batch 0/1
Batch Loss = 4.437384020703225
1992, epoch_train_loss=4.437384020703225
Epoch 1993
Epoch 1993 :: Batch 0/1
Batch Loss = 4.437384018655762
1993, epoch_train_loss=4.437384018655762
Epoch 1994
Epoch 1994 :: Batch 0/1
Batch Loss = 4.437384016601285
1994, epoch_train_loss=4.437384016601285
Epoch 1995
Epoch 1995 :: Batch 0/1
Batch Loss = 4.437384014539748
1995, epoch_train_loss=4.437384014539748
Epoch 1996
Epoch 1996 :: Batch 0/1
Batch Loss = 4.437384012471109
1996, epoch_train_loss=4.437384012471109
Epoch 1997
Epoch 1997 :: Batch 0/1
Batch Loss = 4.437384010395324
1997, epoch_train_loss=4.437384010395324
Epoch 1998
Epoch 1998 :: Batch 0/1
Batch Loss = 4.4373840083123515
1998, epoch_train_loss=4.4373840083123515
Epoch 1999
Epoch 1999 :: Batch 0/1
Batch Loss = 4.4373840062221435
1999, epoch_train_loss=4.4373840062221435
Epoch 2000
Epoch 2000 :: Batch 0/1
Batch Loss = 4.437384004124657
2000, epoch_train_loss=4.437384004124657
Epoch 2001
Epoch 2001 :: Batch 0/1
Batch Loss = 4.437384002019848
2001, epoch_train_loss=4.437384002019848
Epoch 2002
Epoch 2002 :: Batch 0/1
Batch Loss = 4.437383999907672
2002, epoch_train_loss=4.437383999907672
Epoch 2003
Epoch 2003 :: Batch 0/1
Batch Loss = 4.43738399778808
2003, epoch_train_loss=4.43738399778808
Epoch 2004
Epoch 2004 :: Batch 0/1
Batch Loss = 4.437383995661029
2004, epoch_train_loss=4.437383995661029
Epoch 2005
Epoch 2005 :: Batch 0/1
Batch Loss = 4.437383993526473
2005, epoch_train_loss=4.437383993526473
Epoch 2006
Epoch 2006 :: Batch 0/1
Batch Loss = 4.4373839913843645
2006, epoch_train_loss=4.4373839913843645
Epoch 2007
Epoch 2007 :: Batch 0/1
Batch Loss = 4.437383989234656
2007, epoch_train_loss=4.437383989234656
Epoch 2008
Epoch 2008 :: Batch 0/1
Batch Loss = 4.4373839870773
2008, epoch_train_loss=4.4373839870773
Epoch 2009
Epoch 2009 :: Batch 0/1
Batch Loss = 4.437383984912251
2009, epoch_train_loss=4.437383984912251
Epoch 2010
Epoch 2010 :: Batch 0/1
Batch Loss = 4.437383982739458
2010, epoch_train_loss=4.437383982739458
Epoch 2011
Epoch 2011 :: Batch 0/1
Batch Loss = 4.4373839805588755
2011, epoch_train_loss=4.4373839805588755
Epoch 2012
Epoch 2012 :: Batch 0/1
Batch Loss = 4.437383978370452
2012, epoch_train_loss=4.437383978370452
Epoch 2013
Epoch 2013 :: Batch 0/1
Batch Loss = 4.43738397617414
2013, epoch_train_loss=4.43738397617414
Epoch 2014
Epoch 2014 :: Batch 0/1
Batch Loss = 4.43738397396989
2014, epoch_train_loss=4.43738397396989
Epoch 2015
Epoch 2015 :: Batch 0/1
Batch Loss = 4.437383971757652
2015, epoch_train_loss=4.437383971757652
Epoch 2016
Epoch 2016 :: Batch 0/1
Batch Loss = 4.437383969537374
2016, epoch_train_loss=4.437383969537374
Epoch 2017
Epoch 2017 :: Batch 0/1
Batch Loss = 4.437383967309006
2017, epoch_train_loss=4.437383967309006
Epoch 2018
Epoch 2018 :: Batch 0/1
Batch Loss = 4.437383965072499
2018, epoch_train_loss=4.437383965072499
Epoch 2019
Epoch 2019 :: Batch 0/1
Batch Loss = 4.437383962827799
2019, epoch_train_loss=4.437383962827799
Epoch 2020
Epoch 2020 :: Batch 0/1
Batch Loss = 4.437383960574855
2020, epoch_train_loss=4.437383960574855
Epoch 2021
Epoch 2021 :: Batch 0/1
Batch Loss = 4.4373839583136165
2021, epoch_train_loss=4.4373839583136165
Epoch 2022
Epoch 2022 :: Batch 0/1
Batch Loss = 4.437383956044026
2022, epoch_train_loss=4.437383956044026
Epoch 2023
Epoch 2023 :: Batch 0/1
Batch Loss = 4.437383953766035
2023, epoch_train_loss=4.437383953766035
Epoch 2024
Epoch 2024 :: Batch 0/1
Batch Loss = 4.437383951479587
2024, epoch_train_loss=4.437383951479587
Epoch 2025
Epoch 2025 :: Batch 0/1
Batch Loss = 4.437383949184629
2025, epoch_train_loss=4.437383949184629
Epoch 2026
Epoch 2026 :: Batch 0/1
Batch Loss = 4.437383946881107
2026, epoch_train_loss=4.437383946881107
Epoch 2027
Epoch 2027 :: Batch 0/1
Batch Loss = 4.437383944568965
2027, epoch_train_loss=4.437383944568965
Epoch 2028
Epoch 2028 :: Batch 0/1
Batch Loss = 4.437383942248149
2028, epoch_train_loss=4.437383942248149
Epoch 2029
Epoch 2029 :: Batch 0/1
Batch Loss = 4.437383939918602
2029, epoch_train_loss=4.437383939918602
Epoch 2030
Epoch 2030 :: Batch 0/1
Batch Loss = 4.437383937580268
2030, epoch_train_loss=4.437383937580268
Epoch 2031
Epoch 2031 :: Batch 0/1
Batch Loss = 4.4373839352330915
2031, epoch_train_loss=4.4373839352330915
Epoch 2032
Epoch 2032 :: Batch 0/1
Batch Loss = 4.437383932877013
2032, epoch_train_loss=4.437383932877013
Epoch 2033
Epoch 2033 :: Batch 0/1
Batch Loss = 4.437383930511979
2033, epoch_train_loss=4.437383930511979
Epoch 2034
Epoch 2034 :: Batch 0/1
Batch Loss = 4.437383928137926
2034, epoch_train_loss=4.437383928137926
Epoch 2035
Epoch 2035 :: Batch 0/1
Batch Loss = 4.4373839257548
2035, epoch_train_loss=4.4373839257548
Epoch 2036
Epoch 2036 :: Batch 0/1
Batch Loss = 4.437383923362539
2036, epoch_train_loss=4.437383923362539
Epoch 2037
Epoch 2037 :: Batch 0/1
Batch Loss = 4.437383920961085
2037, epoch_train_loss=4.437383920961085
Epoch 2038
Epoch 2038 :: Batch 0/1
Batch Loss = 4.437383918550378
2038, epoch_train_loss=4.437383918550378
Epoch 2039
Epoch 2039 :: Batch 0/1
Batch Loss = 4.437383916130358
2039, epoch_train_loss=4.437383916130358
Epoch 2040
Epoch 2040 :: Batch 0/1
Batch Loss = 4.437383913700962
2040, epoch_train_loss=4.437383913700962
Epoch 2041
Epoch 2041 :: Batch 0/1
Batch Loss = 4.43738391126213
2041, epoch_train_loss=4.43738391126213
Epoch 2042
Epoch 2042 :: Batch 0/1
Batch Loss = 4.437383908813801
2042, epoch_train_loss=4.437383908813801
Epoch 2043
Epoch 2043 :: Batch 0/1
Batch Loss = 4.437383906355909
2043, epoch_train_loss=4.437383906355909
Epoch 2044
Epoch 2044 :: Batch 0/1
Batch Loss = 4.437383903888394
2044, epoch_train_loss=4.437383903888394
Epoch 2045
Epoch 2045 :: Batch 0/1
Batch Loss = 4.437383901411192
2045, epoch_train_loss=4.437383901411192
Epoch 2046
Epoch 2046 :: Batch 0/1
Batch Loss = 4.437383898924239
2046, epoch_train_loss=4.437383898924239
Epoch 2047
Epoch 2047 :: Batch 0/1
Batch Loss = 4.43738389642747
2047, epoch_train_loss=4.43738389642747
Epoch 2048
Epoch 2048 :: Batch 0/1
Batch Loss = 4.43738389392082
2048, epoch_train_loss=4.43738389392082
Epoch 2049
Epoch 2049 :: Batch 0/1
Batch Loss = 4.437383891404223
2049, epoch_train_loss=4.437383891404223
Epoch 2050
Epoch 2050 :: Batch 0/1
Batch Loss = 4.437383888877613
2050, epoch_train_loss=4.437383888877613
Epoch 2051
Epoch 2051 :: Batch 0/1
Batch Loss = 4.437383886340923
2051, epoch_train_loss=4.437383886340923
Epoch 2052
Epoch 2052 :: Batch 0/1
Batch Loss = 4.4373838837940855
2052, epoch_train_loss=4.4373838837940855
Epoch 2053
Epoch 2053 :: Batch 0/1
Batch Loss = 4.437383881237033
2053, epoch_train_loss=4.437383881237033
Epoch 2054
Epoch 2054 :: Batch 0/1
Batch Loss = 4.4373838786696975
2054, epoch_train_loss=4.4373838786696975
Epoch 2055
Epoch 2055 :: Batch 0/1
Batch Loss = 4.437383876092009
2055, epoch_train_loss=4.437383876092009
Epoch 2056
Epoch 2056 :: Batch 0/1
Batch Loss = 4.437383873503898
2056, epoch_train_loss=4.437383873503898
Epoch 2057
Epoch 2057 :: Batch 0/1
Batch Loss = 4.437383870905294
2057, epoch_train_loss=4.437383870905294
Epoch 2058
Epoch 2058 :: Batch 0/1
Batch Loss = 4.437383868296127
2058, epoch_train_loss=4.437383868296127
Epoch 2059
Epoch 2059 :: Batch 0/1
Batch Loss = 4.437383865676326
2059, epoch_train_loss=4.437383865676326
Epoch 2060
Epoch 2060 :: Batch 0/1
Batch Loss = 4.437383863045816
2060, epoch_train_loss=4.437383863045816
Epoch 2061
Epoch 2061 :: Batch 0/1
Batch Loss = 4.437383860404527
2061, epoch_train_loss=4.437383860404527
Epoch 2062
Epoch 2062 :: Batch 0/1
Batch Loss = 4.437383857752385
2062, epoch_train_loss=4.437383857752385
Epoch 2063
Epoch 2063 :: Batch 0/1
Batch Loss = 4.437383855089316
2063, epoch_train_loss=4.437383855089316
Epoch 2064
Epoch 2064 :: Batch 0/1
Batch Loss = 4.437383852415247
2064, epoch_train_loss=4.437383852415247
Epoch 2065
Epoch 2065 :: Batch 0/1
Batch Loss = 4.437383849730098
2065, epoch_train_loss=4.437383849730098
Epoch 2066
Epoch 2066 :: Batch 0/1
Batch Loss = 4.437383847033798
2066, epoch_train_loss=4.437383847033798
Epoch 2067
Epoch 2067 :: Batch 0/1
Batch Loss = 4.437383844326268
2067, epoch_train_loss=4.437383844326268
Epoch 2068
Epoch 2068 :: Batch 0/1
Batch Loss = 4.437383841607431
2068, epoch_train_loss=4.437383841607431
Epoch 2069
Epoch 2069 :: Batch 0/1
Batch Loss = 4.437383838877212
2069, epoch_train_loss=4.437383838877212
Epoch 2070
Epoch 2070 :: Batch 0/1
Batch Loss = 4.437383836135527
2070, epoch_train_loss=4.437383836135527
Epoch 2071
Epoch 2071 :: Batch 0/1
Batch Loss = 4.4373838333823015
2071, epoch_train_loss=4.4373838333823015
Epoch 2072
Epoch 2072 :: Batch 0/1
Batch Loss = 4.437383830617454
2072, epoch_train_loss=4.437383830617454
Epoch 2073
Epoch 2073 :: Batch 0/1
Batch Loss = 4.437383827840902
2073, epoch_train_loss=4.437383827840902
Epoch 2074
Epoch 2074 :: Batch 0/1
Batch Loss = 4.437383825052567
2074, epoch_train_loss=4.437383825052567
Epoch 2075
Epoch 2075 :: Batch 0/1
Batch Loss = 4.4373838222523645
2075, epoch_train_loss=4.4373838222523645
Epoch 2076
Epoch 2076 :: Batch 0/1
Batch Loss = 4.437383819440213
2076, epoch_train_loss=4.437383819440213
Epoch 2077
Epoch 2077 :: Batch 0/1
Batch Loss = 4.4373838166160295
2077, epoch_train_loss=4.4373838166160295
Epoch 2078
Epoch 2078 :: Batch 0/1
Batch Loss = 4.437383813779728
2078, epoch_train_loss=4.437383813779728
Epoch 2079
Epoch 2079 :: Batch 0/1
Batch Loss = 4.437383810931225
2079, epoch_train_loss=4.437383810931225
Epoch 2080
Epoch 2080 :: Batch 0/1
Batch Loss = 4.437383808070433
2080, epoch_train_loss=4.437383808070433
Epoch 2081
Epoch 2081 :: Batch 0/1
Batch Loss = 4.437383805197267
2081, epoch_train_loss=4.437383805197267
Epoch 2082
Epoch 2082 :: Batch 0/1
Batch Loss = 4.437383802311638
2082, epoch_train_loss=4.437383802311638
Epoch 2083
Epoch 2083 :: Batch 0/1
Batch Loss = 4.4373837994134595
2083, epoch_train_loss=4.4373837994134595
Epoch 2084
Epoch 2084 :: Batch 0/1
Batch Loss = 4.437383796502642
2084, epoch_train_loss=4.437383796502642
Epoch 2085
Epoch 2085 :: Batch 0/1
Batch Loss = 4.437383793579095
2085, epoch_train_loss=4.437383793579095
Epoch 2086
Epoch 2086 :: Batch 0/1
Batch Loss = 4.43738379064273
2086, epoch_train_loss=4.43738379064273
Epoch 2087
Epoch 2087 :: Batch 0/1
Batch Loss = 4.437383787693453
2087, epoch_train_loss=4.437383787693453
Epoch 2088
Epoch 2088 :: Batch 0/1
Batch Loss = 4.437383784731173
2088, epoch_train_loss=4.437383784731173
Epoch 2089
Epoch 2089 :: Batch 0/1
Batch Loss = 4.4373837817557975
2089, epoch_train_loss=4.4373837817557975
Epoch 2090
Epoch 2090 :: Batch 0/1
Batch Loss = 4.437383778767231
2090, epoch_train_loss=4.437383778767231
Epoch 2091
Epoch 2091 :: Batch 0/1
Batch Loss = 4.4373837757653805
2091, epoch_train_loss=4.4373837757653805
Epoch 2092
Epoch 2092 :: Batch 0/1
Batch Loss = 4.437383772750149
2092, epoch_train_loss=4.437383772750149
Epoch 2093
Epoch 2093 :: Batch 0/1
Batch Loss = 4.437383769721439
2093, epoch_train_loss=4.437383769721439
Epoch 2094
Epoch 2094 :: Batch 0/1
Batch Loss = 4.437383766679155
2094, epoch_train_loss=4.437383766679155
Epoch 2095
Epoch 2095 :: Batch 0/1
Batch Loss = 4.4373837636232
2095, epoch_train_loss=4.4373837636232
Epoch 2096
Epoch 2096 :: Batch 0/1
Batch Loss = 4.437383760553471
2096, epoch_train_loss=4.437383760553471
Epoch 2097
Epoch 2097 :: Batch 0/1
Batch Loss = 4.4373837574698705
2097, epoch_train_loss=4.4373837574698705
Epoch 2098
Epoch 2098 :: Batch 0/1
Batch Loss = 4.437383754372298
2098, epoch_train_loss=4.437383754372298
Epoch 2099
Epoch 2099 :: Batch 0/1
Batch Loss = 4.437383751260648
2099, epoch_train_loss=4.437383751260648
Epoch 2100
Epoch 2100 :: Batch 0/1
Batch Loss = 4.43738374813482
2100, epoch_train_loss=4.43738374813482
Epoch 2101
Epoch 2101 :: Batch 0/1
Batch Loss = 4.43738374499471
2101, epoch_train_loss=4.43738374499471
Epoch 2102
Epoch 2102 :: Batch 0/1
Batch Loss = 4.437383741840213
2102, epoch_train_loss=4.437383741840213
Epoch 2103
Epoch 2103 :: Batch 0/1
Batch Loss = 4.4373837386712225
2103, epoch_train_loss=4.4373837386712225
Epoch 2104
Epoch 2104 :: Batch 0/1
Batch Loss = 4.437383735487631
2104, epoch_train_loss=4.437383735487631
Epoch 2105
Epoch 2105 :: Batch 0/1
Batch Loss = 4.437383732289333
2105, epoch_train_loss=4.437383732289333
Epoch 2106
Epoch 2106 :: Batch 0/1
Batch Loss = 4.437383729076217
2106, epoch_train_loss=4.437383729076217
Epoch 2107
Epoch 2107 :: Batch 0/1
Batch Loss = 4.437383725848174
2107, epoch_train_loss=4.437383725848174
Epoch 2108
Epoch 2108 :: Batch 0/1
Batch Loss = 4.437383722605094
2108, epoch_train_loss=4.437383722605094
Epoch 2109
Epoch 2109 :: Batch 0/1
Batch Loss = 4.437383719346862
2109, epoch_train_loss=4.437383719346862
Epoch 2110
Epoch 2110 :: Batch 0/1
Batch Loss = 4.4373837160733665
2110, epoch_train_loss=4.4373837160733665
Epoch 2111
Epoch 2111 :: Batch 0/1
Batch Loss = 4.437383712784493
2111, epoch_train_loss=4.437383712784493
Epoch 2112
Epoch 2112 :: Batch 0/1
Batch Loss = 4.4373837094801285
2112, epoch_train_loss=4.4373837094801285
Epoch 2113
Epoch 2113 :: Batch 0/1
Batch Loss = 4.437383706160152
2113, epoch_train_loss=4.437383706160152
Epoch 2114
Epoch 2114 :: Batch 0/1
Batch Loss = 4.437383702824449
2114, epoch_train_loss=4.437383702824449
Epoch 2115
Epoch 2115 :: Batch 0/1
Batch Loss = 4.4373836994729015
2115, epoch_train_loss=4.4373836994729015
Epoch 2116
Epoch 2116 :: Batch 0/1
Batch Loss = 4.437383696105387
2116, epoch_train_loss=4.437383696105387
Epoch 2117
Epoch 2117 :: Batch 0/1
Batch Loss = 4.437383692721786
2117, epoch_train_loss=4.437383692721786
Epoch 2118
Epoch 2118 :: Batch 0/1
Batch Loss = 4.437383689321975
2118, epoch_train_loss=4.437383689321975
Epoch 2119
Epoch 2119 :: Batch 0/1
Batch Loss = 4.437383685905832
2119, epoch_train_loss=4.437383685905832
Epoch 2120
Epoch 2120 :: Batch 0/1
Batch Loss = 4.437383682473231
2120, epoch_train_loss=4.437383682473231
Epoch 2121
Epoch 2121 :: Batch 0/1
Batch Loss = 4.437383679024049
2121, epoch_train_loss=4.437383679024049
Epoch 2122
Epoch 2122 :: Batch 0/1
Batch Loss = 4.437383675558154
2122, epoch_train_loss=4.437383675558154
Epoch 2123
Epoch 2123 :: Batch 0/1
Batch Loss = 4.437383672075421
2123, epoch_train_loss=4.437383672075421
Epoch 2124
Epoch 2124 :: Batch 0/1
Batch Loss = 4.43738366857572
2124, epoch_train_loss=4.43738366857572
Epoch 2125
Epoch 2125 :: Batch 0/1
Batch Loss = 4.43738366505892
2125, epoch_train_loss=4.43738366505892
Epoch 2126
Epoch 2126 :: Batch 0/1
Batch Loss = 4.437383661524888
2126, epoch_train_loss=4.437383661524888
Epoch 2127
Epoch 2127 :: Batch 0/1
Batch Loss = 4.43738365797349
2127, epoch_train_loss=4.43738365797349
Epoch 2128
Epoch 2128 :: Batch 0/1
Batch Loss = 4.437383654404594
2128, epoch_train_loss=4.437383654404594
Epoch 2129
Epoch 2129 :: Batch 0/1
Batch Loss = 4.437383650818061
2129, epoch_train_loss=4.437383650818061
Epoch 2130
Epoch 2130 :: Batch 0/1
Batch Loss = 4.437383647213755
2130, epoch_train_loss=4.437383647213755
Epoch 2131
Epoch 2131 :: Batch 0/1
Batch Loss = 4.437383643591535
2131, epoch_train_loss=4.437383643591535
Epoch 2132
Epoch 2132 :: Batch 0/1
Batch Loss = 4.437383639951261
2132, epoch_train_loss=4.437383639951261
Epoch 2133
Epoch 2133 :: Batch 0/1
Batch Loss = 4.437383636292795
2133, epoch_train_loss=4.437383636292795
Epoch 2134
Epoch 2134 :: Batch 0/1
Batch Loss = 4.437383632615988
2134, epoch_train_loss=4.437383632615988
Epoch 2135
Epoch 2135 :: Batch 0/1
Batch Loss = 4.437383628920698
2135, epoch_train_loss=4.437383628920698
Epoch 2136
Epoch 2136 :: Batch 0/1
Batch Loss = 4.437383625206781
2136, epoch_train_loss=4.437383625206781
Epoch 2137
Epoch 2137 :: Batch 0/1
Batch Loss = 4.437383621474085
2137, epoch_train_loss=4.437383621474085
Epoch 2138
Epoch 2138 :: Batch 0/1
Batch Loss = 4.437383617722464
2138, epoch_train_loss=4.437383617722464
Epoch 2139
Epoch 2139 :: Batch 0/1
Batch Loss = 4.437383613951766
2139, epoch_train_loss=4.437383613951766
Epoch 2140
Epoch 2140 :: Batch 0/1
Batch Loss = 4.437383610161839
2140, epoch_train_loss=4.437383610161839
Epoch 2141
Epoch 2141 :: Batch 0/1
Batch Loss = 4.437383606352528
2141, epoch_train_loss=4.437383606352528
Epoch 2142
Epoch 2142 :: Batch 0/1
Batch Loss = 4.4373836025236795
2142, epoch_train_loss=4.4373836025236795
Epoch 2143
Epoch 2143 :: Batch 0/1
Batch Loss = 4.437383598675135
2143, epoch_train_loss=4.437383598675135
Epoch 2144
Epoch 2144 :: Batch 0/1
Batch Loss = 4.437383594806736
2144, epoch_train_loss=4.437383594806736
Epoch 2145
Epoch 2145 :: Batch 0/1
Batch Loss = 4.437383590918324
2145, epoch_train_loss=4.437383590918324
Epoch 2146
Epoch 2146 :: Batch 0/1
Batch Loss = 4.437383587009735
2146, epoch_train_loss=4.437383587009735
Epoch 2147
Epoch 2147 :: Batch 0/1
Batch Loss = 4.4373835830808055
2147, epoch_train_loss=4.4373835830808055
Epoch 2148
Epoch 2148 :: Batch 0/1
Batch Loss = 4.43738357913137
2148, epoch_train_loss=4.43738357913137
Epoch 2149
Epoch 2149 :: Batch 0/1
Batch Loss = 4.437383575161261
2149, epoch_train_loss=4.437383575161261
Epoch 2150
Epoch 2150 :: Batch 0/1
Batch Loss = 4.437383571170312
2150, epoch_train_loss=4.437383571170312
Epoch 2151
Epoch 2151 :: Batch 0/1
Batch Loss = 4.437383567158349
2151, epoch_train_loss=4.437383567158349
Epoch 2152
Epoch 2152 :: Batch 0/1
Batch Loss = 4.437383563125203
2152, epoch_train_loss=4.437383563125203
Epoch 2153
Epoch 2153 :: Batch 0/1
Batch Loss = 4.437383559070695
2153, epoch_train_loss=4.437383559070695
Epoch 2154
Epoch 2154 :: Batch 0/1
Batch Loss = 4.4373835549946525
2154, epoch_train_loss=4.4373835549946525
Epoch 2155
Epoch 2155 :: Batch 0/1
Batch Loss = 4.437383550896897
2155, epoch_train_loss=4.437383550896897
Epoch 2156
Epoch 2156 :: Batch 0/1
Batch Loss = 4.437383546777245
2156, epoch_train_loss=4.437383546777245
Epoch 2157
Epoch 2157 :: Batch 0/1
Batch Loss = 4.437383542635519
2157, epoch_train_loss=4.437383542635519
Epoch 2158
Epoch 2158 :: Batch 0/1
Batch Loss = 4.437383538471535
2158, epoch_train_loss=4.437383538471535
Epoch 2159
Epoch 2159 :: Batch 0/1
Batch Loss = 4.437383534285105
2159, epoch_train_loss=4.437383534285105
Epoch 2160
Epoch 2160 :: Batch 0/1
Batch Loss = 4.43738353007604
2160, epoch_train_loss=4.43738353007604
Epoch 2161
Epoch 2161 :: Batch 0/1
Batch Loss = 4.437383525844151
2161, epoch_train_loss=4.437383525844151
Epoch 2162
Epoch 2162 :: Batch 0/1
Batch Loss = 4.437383521589248
2162, epoch_train_loss=4.437383521589248
Epoch 2163
Epoch 2163 :: Batch 0/1
Batch Loss = 4.437383517311135
2163, epoch_train_loss=4.437383517311135
Epoch 2164
Epoch 2164 :: Batch 0/1
Batch Loss = 4.437383513009617
2164, epoch_train_loss=4.437383513009617
Epoch 2165
Epoch 2165 :: Batch 0/1
Batch Loss = 4.437383508684493
2165, epoch_train_loss=4.437383508684493
Epoch 2166
Epoch 2166 :: Batch 0/1
Batch Loss = 4.437383504335566
2166, epoch_train_loss=4.437383504335566
Epoch 2167
Epoch 2167 :: Batch 0/1
Batch Loss = 4.437383499962631
2167, epoch_train_loss=4.437383499962631
Epoch 2168
Epoch 2168 :: Batch 0/1
Batch Loss = 4.437383495565482
2168, epoch_train_loss=4.437383495565482
Epoch 2169
Epoch 2169 :: Batch 0/1
Batch Loss = 4.4373834911439145
2169, epoch_train_loss=4.4373834911439145
Epoch 2170
Epoch 2170 :: Batch 0/1
Batch Loss = 4.437383486697716
2170, epoch_train_loss=4.437383486697716
Epoch 2171
Epoch 2171 :: Batch 0/1
Batch Loss = 4.437383482226679
2171, epoch_train_loss=4.437383482226679
Epoch 2172
Epoch 2172 :: Batch 0/1
Batch Loss = 4.437383477730583
2172, epoch_train_loss=4.437383477730583
Epoch 2173
Epoch 2173 :: Batch 0/1
Batch Loss = 4.4373834732092154
2173, epoch_train_loss=4.4373834732092154
Epoch 2174
Epoch 2174 :: Batch 0/1
Batch Loss = 4.437383468662357
2174, epoch_train_loss=4.437383468662357
Epoch 2175
Epoch 2175 :: Batch 0/1
Batch Loss = 4.437383464089785
2175, epoch_train_loss=4.437383464089785
Epoch 2176
Epoch 2176 :: Batch 0/1
Batch Loss = 4.437383459491276
2176, epoch_train_loss=4.437383459491276
Epoch 2177
Epoch 2177 :: Batch 0/1
Batch Loss = 4.437383454866601
2177, epoch_train_loss=4.437383454866601
Epoch 2178
Epoch 2178 :: Batch 0/1
Batch Loss = 4.437383450215534
2178, epoch_train_loss=4.437383450215534
Epoch 2179
Epoch 2179 :: Batch 0/1
Batch Loss = 4.437383445537843
2179, epoch_train_loss=4.437383445537843
Epoch 2180
Epoch 2180 :: Batch 0/1
Batch Loss = 4.4373834408332895
2180, epoch_train_loss=4.4373834408332895
Epoch 2181
Epoch 2181 :: Batch 0/1
Batch Loss = 4.4373834361016415
2181, epoch_train_loss=4.4373834361016415
Epoch 2182
Epoch 2182 :: Batch 0/1
Batch Loss = 4.437383431342656
2182, epoch_train_loss=4.437383431342656
Epoch 2183
Epoch 2183 :: Batch 0/1
Batch Loss = 4.4373834265560905
2183, epoch_train_loss=4.4373834265560905
Epoch 2184
Epoch 2184 :: Batch 0/1
Batch Loss = 4.437383421741702
2184, epoch_train_loss=4.437383421741702
Epoch 2185
Epoch 2185 :: Batch 0/1
Batch Loss = 4.437383416899237
2185, epoch_train_loss=4.437383416899237
Epoch 2186
Epoch 2186 :: Batch 0/1
Batch Loss = 4.437383412028451
2186, epoch_train_loss=4.437383412028451
Epoch 2187
Epoch 2187 :: Batch 0/1
Batch Loss = 4.437383407129086
2187, epoch_train_loss=4.437383407129086
Epoch 2188
Epoch 2188 :: Batch 0/1
Batch Loss = 4.437383402200884
2188, epoch_train_loss=4.437383402200884
Epoch 2189
Epoch 2189 :: Batch 0/1
Batch Loss = 4.437383397243586
2189, epoch_train_loss=4.437383397243586
Epoch 2190
Epoch 2190 :: Batch 0/1
Batch Loss = 4.437383392256931
2190, epoch_train_loss=4.437383392256931
Epoch 2191
Epoch 2191 :: Batch 0/1
Batch Loss = 4.437383387240649
2191, epoch_train_loss=4.437383387240649
Epoch 2192
Epoch 2192 :: Batch 0/1
Batch Loss = 4.437383382194474
2192, epoch_train_loss=4.437383382194474
Epoch 2193
Epoch 2193 :: Batch 0/1
Batch Loss = 4.437383377118131
2193, epoch_train_loss=4.437383377118131
Epoch 2194
Epoch 2194 :: Batch 0/1
Batch Loss = 4.437383372011347
2194, epoch_train_loss=4.437383372011347
Epoch 2195
Epoch 2195 :: Batch 0/1
Batch Loss = 4.4373833668738385
2195, epoch_train_loss=4.4373833668738385
Epoch 2196
Epoch 2196 :: Batch 0/1
Batch Loss = 4.4373833617053275
2196, epoch_train_loss=4.4373833617053275
Epoch 2197
Epoch 2197 :: Batch 0/1
Batch Loss = 4.437383356505524
2197, epoch_train_loss=4.437383356505524
Epoch 2198
Epoch 2198 :: Batch 0/1
Batch Loss = 4.437383351274143
2198, epoch_train_loss=4.437383351274143
Epoch 2199
Epoch 2199 :: Batch 0/1
Batch Loss = 4.437383346010889
2199, epoch_train_loss=4.437383346010889
Epoch 2200
Epoch 2200 :: Batch 0/1
Batch Loss = 4.437383340715468
2200, epoch_train_loss=4.437383340715468
Epoch 2201
Epoch 2201 :: Batch 0/1
Batch Loss = 4.437383335387577
2201, epoch_train_loss=4.437383335387577
Epoch 2202
Epoch 2202 :: Batch 0/1
Batch Loss = 4.437383330026916
2202, epoch_train_loss=4.437383330026916
Epoch 2203
Epoch 2203 :: Batch 0/1
Batch Loss = 4.437383324633175
2203, epoch_train_loss=4.437383324633175
Epoch 2204
Epoch 2204 :: Batch 0/1
Batch Loss = 4.4373833192060435
2204, epoch_train_loss=4.4373833192060435
Epoch 2205
Epoch 2205 :: Batch 0/1
Batch Loss = 4.437383313745209
2205, epoch_train_loss=4.437383313745209
Epoch 2206
Epoch 2206 :: Batch 0/1
Batch Loss = 4.43738330825035
2206, epoch_train_loss=4.43738330825035
Epoch 2207
Epoch 2207 :: Batch 0/1
Batch Loss = 4.437383302721144
2207, epoch_train_loss=4.437383302721144
Epoch 2208
Epoch 2208 :: Batch 0/1
Batch Loss = 4.437383297157266
2208, epoch_train_loss=4.437383297157266
Epoch 2209
Epoch 2209 :: Batch 0/1
Batch Loss = 4.437383291558388
2209, epoch_train_loss=4.437383291558388
Epoch 2210
Epoch 2210 :: Batch 0/1
Batch Loss = 4.43738328592417
2210, epoch_train_loss=4.43738328592417
Epoch 2211
Epoch 2211 :: Batch 0/1
Batch Loss = 4.437383280254275
2211, epoch_train_loss=4.437383280254275
Epoch 2212
Epoch 2212 :: Batch 0/1
Batch Loss = 4.437383274548362
2212, epoch_train_loss=4.437383274548362
Epoch 2213
Epoch 2213 :: Batch 0/1
Batch Loss = 4.437383268806081
2213, epoch_train_loss=4.437383268806081
Epoch 2214
Epoch 2214 :: Batch 0/1
Batch Loss = 4.4373832630270815
2214, epoch_train_loss=4.4373832630270815
Epoch 2215
Epoch 2215 :: Batch 0/1
Batch Loss = 4.437383257211006
2215, epoch_train_loss=4.437383257211006
Epoch 2216
Epoch 2216 :: Batch 0/1
Batch Loss = 4.437383251357496
2216, epoch_train_loss=4.437383251357496
Epoch 2217
Epoch 2217 :: Batch 0/1
Batch Loss = 4.437383245466185
2217, epoch_train_loss=4.437383245466185
Epoch 2218
Epoch 2218 :: Batch 0/1
Batch Loss = 4.437383239536703
2218, epoch_train_loss=4.437383239536703
Epoch 2219
Epoch 2219 :: Batch 0/1
Batch Loss = 4.437383233568673
2219, epoch_train_loss=4.437383233568673
Epoch 2220
Epoch 2220 :: Batch 0/1
Batch Loss = 4.43738322756172
2220, epoch_train_loss=4.43738322756172
Epoch 2221
Epoch 2221 :: Batch 0/1
Batch Loss = 4.437383221515459
2221, epoch_train_loss=4.437383221515459
Epoch 2222
Epoch 2222 :: Batch 0/1
Batch Loss = 4.437383215429496
2222, epoch_train_loss=4.437383215429496
Epoch 2223
Epoch 2223 :: Batch 0/1
Batch Loss = 4.437383209303439
2223, epoch_train_loss=4.437383209303439
Epoch 2224
Epoch 2224 :: Batch 0/1
Batch Loss = 4.437383203136892
2224, epoch_train_loss=4.437383203136892
Epoch 2225
Epoch 2225 :: Batch 0/1
Batch Loss = 4.437383196929446
2225, epoch_train_loss=4.437383196929446
Epoch 2226
Epoch 2226 :: Batch 0/1
Batch Loss = 4.437383190680691
2226, epoch_train_loss=4.437383190680691
Epoch 2227
Epoch 2227 :: Batch 0/1
Batch Loss = 4.437383184390216
2227, epoch_train_loss=4.437383184390216
Epoch 2228
Epoch 2228 :: Batch 0/1
Batch Loss = 4.437383178057596
2228, epoch_train_loss=4.437383178057596
Epoch 2229
Epoch 2229 :: Batch 0/1
Batch Loss = 4.437383171682406
2229, epoch_train_loss=4.437383171682406
Epoch 2230
Epoch 2230 :: Batch 0/1
Batch Loss = 4.437383165264215
2230, epoch_train_loss=4.437383165264215
Epoch 2231
Epoch 2231 :: Batch 0/1
Batch Loss = 4.437383158802586
2231, epoch_train_loss=4.437383158802586
Epoch 2232
Epoch 2232 :: Batch 0/1
Batch Loss = 4.437383152297071
2232, epoch_train_loss=4.437383152297071
Epoch 2233
Epoch 2233 :: Batch 0/1
Batch Loss = 4.437383145747224
2233, epoch_train_loss=4.437383145747224
Epoch 2234
Epoch 2234 :: Batch 0/1
Batch Loss = 4.437383139152588
2234, epoch_train_loss=4.437383139152588
Epoch 2235
Epoch 2235 :: Batch 0/1
Batch Loss = 4.437383132512705
2235, epoch_train_loss=4.437383132512705
Epoch 2236
Epoch 2236 :: Batch 0/1
Batch Loss = 4.437383125827104
2236, epoch_train_loss=4.437383125827104
Epoch 2237
Epoch 2237 :: Batch 0/1
Batch Loss = 4.437383119095311
2237, epoch_train_loss=4.437383119095311
Epoch 2238
Epoch 2238 :: Batch 0/1
Batch Loss = 4.437383112316844
2238, epoch_train_loss=4.437383112316844
Epoch 2239
Epoch 2239 :: Batch 0/1
Batch Loss = 4.437383105491221
2239, epoch_train_loss=4.437383105491221
Epoch 2240
Epoch 2240 :: Batch 0/1
Batch Loss = 4.4373830986179446
2240, epoch_train_loss=4.4373830986179446
Epoch 2241
Epoch 2241 :: Batch 0/1
Batch Loss = 4.437383091696515
2241, epoch_train_loss=4.437383091696515
Epoch 2242
Epoch 2242 :: Batch 0/1
Batch Loss = 4.437383084726425
2242, epoch_train_loss=4.437383084726425
Epoch 2243
Epoch 2243 :: Batch 0/1
Batch Loss = 4.4373830777071595
2243, epoch_train_loss=4.4373830777071595
Epoch 2244
Epoch 2244 :: Batch 0/1
Batch Loss = 4.437383070638198
2244, epoch_train_loss=4.437383070638198
Epoch 2245
Epoch 2245 :: Batch 0/1
Batch Loss = 4.437383063519011
2245, epoch_train_loss=4.437383063519011
Epoch 2246
Epoch 2246 :: Batch 0/1
Batch Loss = 4.437383056349063
2246, epoch_train_loss=4.437383056349063
Epoch 2247
Epoch 2247 :: Batch 0/1
Batch Loss = 4.437383049127811
2247, epoch_train_loss=4.437383049127811
Epoch 2248
Epoch 2248 :: Batch 0/1
Batch Loss = 4.437383041854703
2248, epoch_train_loss=4.437383041854703
Epoch 2249
Epoch 2249 :: Batch 0/1
Batch Loss = 4.437383034529182
2249, epoch_train_loss=4.437383034529182
Epoch 2250
Epoch 2250 :: Batch 0/1
Batch Loss = 4.437383027150676
2250, epoch_train_loss=4.437383027150676
Epoch 2251
Epoch 2251 :: Batch 0/1
Batch Loss = 4.437383019718617
2251, epoch_train_loss=4.437383019718617
Epoch 2252
Epoch 2252 :: Batch 0/1
Batch Loss = 4.437383012232417
2252, epoch_train_loss=4.437383012232417
Epoch 2253
Epoch 2253 :: Batch 0/1
Batch Loss = 4.437383004691488
2253, epoch_train_loss=4.437383004691488
Epoch 2254
Epoch 2254 :: Batch 0/1
Batch Loss = 4.437382997095228
2254, epoch_train_loss=4.437382997095228
Epoch 2255
Epoch 2255 :: Batch 0/1
Batch Loss = 4.43738298944303
2255, epoch_train_loss=4.43738298944303
Epoch 2256
Epoch 2256 :: Batch 0/1
Batch Loss = 4.437382981734277
2256, epoch_train_loss=4.437382981734277
Epoch 2257
Epoch 2257 :: Batch 0/1
Batch Loss = 4.437382973968341
2257, epoch_train_loss=4.437382973968341
Epoch 2258
Epoch 2258 :: Batch 0/1
Batch Loss = 4.437382966144589
2258, epoch_train_loss=4.437382966144589
Epoch 2259
Epoch 2259 :: Batch 0/1
Batch Loss = 4.4373829582623765
2259, epoch_train_loss=4.4373829582623765
Epoch 2260
Epoch 2260 :: Batch 0/1
Batch Loss = 4.43738295032105
2260, epoch_train_loss=4.43738295032105
Epoch 2261
Epoch 2261 :: Batch 0/1
Batch Loss = 4.437382942319946
2261, epoch_train_loss=4.437382942319946
Epoch 2262
Epoch 2262 :: Batch 0/1
Batch Loss = 4.437382934258391
2262, epoch_train_loss=4.437382934258391
Epoch 2263
Epoch 2263 :: Batch 0/1
Batch Loss = 4.437382926135702
2263, epoch_train_loss=4.437382926135702
Epoch 2264
Epoch 2264 :: Batch 0/1
Batch Loss = 4.437382917951187
2264, epoch_train_loss=4.437382917951187
Epoch 2265
Epoch 2265 :: Batch 0/1
Batch Loss = 4.437382909704142
2265, epoch_train_loss=4.437382909704142
Epoch 2266
Epoch 2266 :: Batch 0/1
Batch Loss = 4.4373829013938515
2266, epoch_train_loss=4.4373829013938515
Epoch 2267
Epoch 2267 :: Batch 0/1
Batch Loss = 4.437382893019593
2267, epoch_train_loss=4.437382893019593
Epoch 2268
Epoch 2268 :: Batch 0/1
Batch Loss = 4.437382884580632
2268, epoch_train_loss=4.437382884580632
Epoch 2269
Epoch 2269 :: Batch 0/1
Batch Loss = 4.43738287607622
2269, epoch_train_loss=4.43738287607622
Epoch 2270
Epoch 2270 :: Batch 0/1
Batch Loss = 4.4373828675056
2270, epoch_train_loss=4.4373828675056
Epoch 2271
Epoch 2271 :: Batch 0/1
Batch Loss = 4.437382858868004
2271, epoch_train_loss=4.437382858868004
Epoch 2272
Epoch 2272 :: Batch 0/1
Batch Loss = 4.437382850162649
2272, epoch_train_loss=4.437382850162649
Epoch 2273
Epoch 2273 :: Batch 0/1
Batch Loss = 4.437382841388743
2273, epoch_train_loss=4.437382841388743
Epoch 2274
Epoch 2274 :: Batch 0/1
Batch Loss = 4.437382832545481
2274, epoch_train_loss=4.437382832545481
Epoch 2275
Epoch 2275 :: Batch 0/1
Batch Loss = 4.4373828236320465
2275, epoch_train_loss=4.4373828236320465
Epoch 2276
Epoch 2276 :: Batch 0/1
Batch Loss = 4.437382814647608
2276, epoch_train_loss=4.437382814647608
Epoch 2277
Epoch 2277 :: Batch 0/1
Batch Loss = 4.437382805591324
2277, epoch_train_loss=4.437382805591324
Epoch 2278
Epoch 2278 :: Batch 0/1
Batch Loss = 4.4373827964623365
2278, epoch_train_loss=4.4373827964623365
Epoch 2279
Epoch 2279 :: Batch 0/1
Batch Loss = 4.437382787259778
2279, epoch_train_loss=4.437382787259778
Epoch 2280
Epoch 2280 :: Batch 0/1
Batch Loss = 4.437382777982764
2280, epoch_train_loss=4.437382777982764
Epoch 2281
Epoch 2281 :: Batch 0/1
Batch Loss = 4.437382768630398
2281, epoch_train_loss=4.437382768630398
Epoch 2282
Epoch 2282 :: Batch 0/1
Batch Loss = 4.437382759201769
2282, epoch_train_loss=4.437382759201769
Epoch 2283
Epoch 2283 :: Batch 0/1
Batch Loss = 4.437382749695953
2283, epoch_train_loss=4.437382749695953
Epoch 2284
Epoch 2284 :: Batch 0/1
Batch Loss = 4.437382740112007
2284, epoch_train_loss=4.437382740112007
Epoch 2285
Epoch 2285 :: Batch 0/1
Batch Loss = 4.437382730448976
2285, epoch_train_loss=4.437382730448976
Epoch 2286
Epoch 2286 :: Batch 0/1
Batch Loss = 4.437382720705891
2286, epoch_train_loss=4.437382720705891
Epoch 2287
Epoch 2287 :: Batch 0/1
Batch Loss = 4.437382710881765
2287, epoch_train_loss=4.437382710881765
Epoch 2288
Epoch 2288 :: Batch 0/1
Batch Loss = 4.437382700975595
2288, epoch_train_loss=4.437382700975595
Epoch 2289
Epoch 2289 :: Batch 0/1
Batch Loss = 4.437382690986364
2289, epoch_train_loss=4.437382690986364
Epoch 2290
Epoch 2290 :: Batch 0/1
Batch Loss = 4.437382680913034
2290, epoch_train_loss=4.437382680913034
Epoch 2291
Epoch 2291 :: Batch 0/1
Batch Loss = 4.437382670754556
2291, epoch_train_loss=4.437382670754556
Epoch 2292
Epoch 2292 :: Batch 0/1
Batch Loss = 4.437382660509861
2292, epoch_train_loss=4.437382660509861
Epoch 2293
Epoch 2293 :: Batch 0/1
Batch Loss = 4.437382650177859
2293, epoch_train_loss=4.437382650177859
Epoch 2294
Epoch 2294 :: Batch 0/1
Batch Loss = 4.4373826397574465
2294, epoch_train_loss=4.4373826397574465
Epoch 2295
Epoch 2295 :: Batch 0/1
Batch Loss = 4.437382629247502
2295, epoch_train_loss=4.437382629247502
Epoch 2296
Epoch 2296 :: Batch 0/1
Batch Loss = 4.43738261864688
2296, epoch_train_loss=4.43738261864688
Epoch 2297
Epoch 2297 :: Batch 0/1
Batch Loss = 4.4373826079544205
2297, epoch_train_loss=4.4373826079544205
Epoch 2298
Epoch 2298 :: Batch 0/1
Batch Loss = 4.437382597168946
2298, epoch_train_loss=4.437382597168946
Epoch 2299
Epoch 2299 :: Batch 0/1
Batch Loss = 4.437382586289251
2299, epoch_train_loss=4.437382586289251
Epoch 2300
Epoch 2300 :: Batch 0/1
Batch Loss = 4.43738257531412
2300, epoch_train_loss=4.43738257531412
Epoch 2301
Epoch 2301 :: Batch 0/1
Batch Loss = 4.437382564242307
2301, epoch_train_loss=4.437382564242307
Epoch 2302
Epoch 2302 :: Batch 0/1
Batch Loss = 4.43738255307255
2302, epoch_train_loss=4.43738255307255
Epoch 2303
Epoch 2303 :: Batch 0/1
Batch Loss = 4.437382541803565
2303, epoch_train_loss=4.437382541803565
Epoch 2304
Epoch 2304 :: Batch 0/1
Batch Loss = 4.4373825304340455
2304, epoch_train_loss=4.4373825304340455
Epoch 2305
Epoch 2305 :: Batch 0/1
Batch Loss = 4.437382518962661
2305, epoch_train_loss=4.437382518962661
Epoch 2306
Epoch 2306 :: Batch 0/1
Batch Loss = 4.437382507388061
2306, epoch_train_loss=4.437382507388061
Epoch 2307
Epoch 2307 :: Batch 0/1
Batch Loss = 4.437382495708868
2307, epoch_train_loss=4.437382495708868
Epoch 2308
Epoch 2308 :: Batch 0/1
Batch Loss = 4.437382483923681
2308, epoch_train_loss=4.437382483923681
Epoch 2309
Epoch 2309 :: Batch 0/1
Batch Loss = 4.437382472031079
2309, epoch_train_loss=4.437382472031079
Epoch 2310
Epoch 2310 :: Batch 0/1
Batch Loss = 4.437382460029607
2310, epoch_train_loss=4.437382460029607
Epoch 2311
Epoch 2311 :: Batch 0/1
Batch Loss = 4.437382447917791
2311, epoch_train_loss=4.437382447917791
Epoch 2312
Epoch 2312 :: Batch 0/1
Batch Loss = 4.43738243569413
2312, epoch_train_loss=4.43738243569413
Epoch 2313
Epoch 2313 :: Batch 0/1
Batch Loss = 4.437382423357097
2313, epoch_train_loss=4.437382423357097
Epoch 2314
Epoch 2314 :: Batch 0/1
Batch Loss = 4.437382410905132
2314, epoch_train_loss=4.437382410905132
Epoch 2315
Epoch 2315 :: Batch 0/1
Batch Loss = 4.437382398336651
2315, epoch_train_loss=4.437382398336651
Epoch 2316
Epoch 2316 :: Batch 0/1
Batch Loss = 4.437382385650042
2316, epoch_train_loss=4.437382385650042
Epoch 2317
Epoch 2317 :: Batch 0/1
Batch Loss = 4.437382372843663
2317, epoch_train_loss=4.437382372843663
Epoch 2318
Epoch 2318 :: Batch 0/1
Batch Loss = 4.437382359915841
2318, epoch_train_loss=4.437382359915841
Epoch 2319
Epoch 2319 :: Batch 0/1
Batch Loss = 4.4373823468648705
2319, epoch_train_loss=4.4373823468648705
Epoch 2320
Epoch 2320 :: Batch 0/1
Batch Loss = 4.437382333689022
2320, epoch_train_loss=4.437382333689022
Epoch 2321
Epoch 2321 :: Batch 0/1
Batch Loss = 4.437382320386524
2321, epoch_train_loss=4.437382320386524
Epoch 2322
Epoch 2322 :: Batch 0/1
Batch Loss = 4.437382306955579
2322, epoch_train_loss=4.437382306955579
Epoch 2323
Epoch 2323 :: Batch 0/1
Batch Loss = 4.437382293394352
2323, epoch_train_loss=4.437382293394352
Epoch 2324
Epoch 2324 :: Batch 0/1
Batch Loss = 4.437382279700977
2324, epoch_train_loss=4.437382279700977
Epoch 2325
Epoch 2325 :: Batch 0/1
Batch Loss = 4.437382265873549
2325, epoch_train_loss=4.437382265873549
Epoch 2326
Epoch 2326 :: Batch 0/1
Batch Loss = 4.4373822519101305
2326, epoch_train_loss=4.4373822519101305
Epoch 2327
Epoch 2327 :: Batch 0/1
Batch Loss = 4.437382237808746
2327, epoch_train_loss=4.437382237808746
Epoch 2328
Epoch 2328 :: Batch 0/1
Batch Loss = 4.43738222356738
2328, epoch_train_loss=4.43738222356738
Epoch 2329
Epoch 2329 :: Batch 0/1
Batch Loss = 4.4373822091839825
2329, epoch_train_loss=4.4373822091839825
Epoch 2330
Epoch 2330 :: Batch 0/1
Batch Loss = 4.437382194656458
2330, epoch_train_loss=4.437382194656458
Epoch 2331
Epoch 2331 :: Batch 0/1
Batch Loss = 4.4373821799826745
2331, epoch_train_loss=4.4373821799826745
Epoch 2332
Epoch 2332 :: Batch 0/1
Batch Loss = 4.437382165160459
2332, epoch_train_loss=4.437382165160459
Epoch 2333
Epoch 2333 :: Batch 0/1
Batch Loss = 4.437382150187594
2333, epoch_train_loss=4.437382150187594
Epoch 2334
Epoch 2334 :: Batch 0/1
Batch Loss = 4.437382135061821
2334, epoch_train_loss=4.437382135061821
Epoch 2335
Epoch 2335 :: Batch 0/1
Batch Loss = 4.437382119780832
2335, epoch_train_loss=4.437382119780832
Epoch 2336
Epoch 2336 :: Batch 0/1
Batch Loss = 4.4373821043422765
2336, epoch_train_loss=4.4373821043422765
Epoch 2337
Epoch 2337 :: Batch 0/1
Batch Loss = 4.437382088743759
2337, epoch_train_loss=4.437382088743759
Epoch 2338
Epoch 2338 :: Batch 0/1
Batch Loss = 4.437382072982832
2338, epoch_train_loss=4.437382072982832
Epoch 2339
Epoch 2339 :: Batch 0/1
Batch Loss = 4.437382057057001
2339, epoch_train_loss=4.437382057057001
Epoch 2340
Epoch 2340 :: Batch 0/1
Batch Loss = 4.43738204096372
2340, epoch_train_loss=4.43738204096372
Epoch 2341
Epoch 2341 :: Batch 0/1
Batch Loss = 4.437382024700391
2341, epoch_train_loss=4.437382024700391
Epoch 2342
Epoch 2342 :: Batch 0/1
Batch Loss = 4.437382008264365
2342, epoch_train_loss=4.437382008264365
Epoch 2343
Epoch 2343 :: Batch 0/1
Batch Loss = 4.437381991652935
2343, epoch_train_loss=4.437381991652935
Epoch 2344
Epoch 2344 :: Batch 0/1
Batch Loss = 4.43738197486334
2344, epoch_train_loss=4.43738197486334
Epoch 2345
Epoch 2345 :: Batch 0/1
Batch Loss = 4.4373819578927645
2345, epoch_train_loss=4.4373819578927645
Epoch 2346
Epoch 2346 :: Batch 0/1
Batch Loss = 4.437381940738328
2346, epoch_train_loss=4.437381940738328
Epoch 2347
Epoch 2347 :: Batch 0/1
Batch Loss = 4.437381923397096
2347, epoch_train_loss=4.437381923397096
Epoch 2348
Epoch 2348 :: Batch 0/1
Batch Loss = 4.437381905866069
2348, epoch_train_loss=4.437381905866069
Epoch 2349
Epoch 2349 :: Batch 0/1
Batch Loss = 4.437381888142183
2349, epoch_train_loss=4.437381888142183
Epoch 2350
Epoch 2350 :: Batch 0/1
Batch Loss = 4.43738187022231
2350, epoch_train_loss=4.43738187022231
Epoch 2351
Epoch 2351 :: Batch 0/1
Batch Loss = 4.4373818521032575
2351, epoch_train_loss=4.4373818521032575
Epoch 2352
Epoch 2352 :: Batch 0/1
Batch Loss = 4.4373818337817585
2352, epoch_train_loss=4.4373818337817585
Epoch 2353
Epoch 2353 :: Batch 0/1
Batch Loss = 4.437381815254479
2353, epoch_train_loss=4.437381815254479
Epoch 2354
Epoch 2354 :: Batch 0/1
Batch Loss = 4.437381796518015
2354, epoch_train_loss=4.437381796518015
Epoch 2355
Epoch 2355 :: Batch 0/1
Batch Loss = 4.437381777568885
2355, epoch_train_loss=4.437381777568885
Epoch 2356
Epoch 2356 :: Batch 0/1
Batch Loss = 4.437381758403529
2356, epoch_train_loss=4.437381758403529
Epoch 2357
Epoch 2357 :: Batch 0/1
Batch Loss = 4.437381739018312
2357, epoch_train_loss=4.437381739018312
Epoch 2358
Epoch 2358 :: Batch 0/1
Batch Loss = 4.437381719409519
2358, epoch_train_loss=4.437381719409519
Epoch 2359
Epoch 2359 :: Batch 0/1
Batch Loss = 4.437381699573344
2359, epoch_train_loss=4.437381699573344
Epoch 2360
Epoch 2360 :: Batch 0/1
Batch Loss = 4.437381679505909
2360, epoch_train_loss=4.437381679505909
Epoch 2361
Epoch 2361 :: Batch 0/1
Batch Loss = 4.437381659203235
2361, epoch_train_loss=4.437381659203235
Epoch 2362
Epoch 2362 :: Batch 0/1
Batch Loss = 4.437381638661264
2362, epoch_train_loss=4.437381638661264
Epoch 2363
Epoch 2363 :: Batch 0/1
Batch Loss = 4.437381617875835
2363, epoch_train_loss=4.437381617875835
Epoch 2364
Epoch 2364 :: Batch 0/1
Batch Loss = 4.437381596842699
2364, epoch_train_loss=4.437381596842699
Epoch 2365
Epoch 2365 :: Batch 0/1
Batch Loss = 4.437381575557507
2365, epoch_train_loss=4.437381575557507
Epoch 2366
Epoch 2366 :: Batch 0/1
Batch Loss = 4.437381554015809
2366, epoch_train_loss=4.437381554015809
Epoch 2367
Epoch 2367 :: Batch 0/1
Batch Loss = 4.437381532213046
2367, epoch_train_loss=4.437381532213046
Epoch 2368
Epoch 2368 :: Batch 0/1
Batch Loss = 4.437381510144561
2368, epoch_train_loss=4.437381510144561
Epoch 2369
Epoch 2369 :: Batch 0/1
Batch Loss = 4.43738148780558
2369, epoch_train_loss=4.43738148780558
Epoch 2370
Epoch 2370 :: Batch 0/1
Batch Loss = 4.4373814651912165
2370, epoch_train_loss=4.4373814651912165
Epoch 2371
Epoch 2371 :: Batch 0/1
Batch Loss = 4.437381442296472
2371, epoch_train_loss=4.437381442296472
Epoch 2372
Epoch 2372 :: Batch 0/1
Batch Loss = 4.437381419116222
2372, epoch_train_loss=4.437381419116222
Epoch 2373
Epoch 2373 :: Batch 0/1
Batch Loss = 4.437381395645221
2373, epoch_train_loss=4.437381395645221
Epoch 2374
Epoch 2374 :: Batch 0/1
Batch Loss = 4.4373813718780974
2374, epoch_train_loss=4.4373813718780974
Epoch 2375
Epoch 2375 :: Batch 0/1
Batch Loss = 4.437381347809342
2375, epoch_train_loss=4.437381347809342
Epoch 2376
Epoch 2376 :: Batch 0/1
Batch Loss = 4.437381323433319
2376, epoch_train_loss=4.437381323433319
Epoch 2377
Epoch 2377 :: Batch 0/1
Batch Loss = 4.437381298744242
2377, epoch_train_loss=4.437381298744242
Epoch 2378
Epoch 2378 :: Batch 0/1
Batch Loss = 4.4373812737361895
2378, epoch_train_loss=4.4373812737361895
Epoch 2379
Epoch 2379 :: Batch 0/1
Batch Loss = 4.437381248403088
2379, epoch_train_loss=4.437381248403088
Epoch 2380
Epoch 2380 :: Batch 0/1
Batch Loss = 4.437381222738712
2380, epoch_train_loss=4.437381222738712
Epoch 2381
Epoch 2381 :: Batch 0/1
Batch Loss = 4.437381196736673
2381, epoch_train_loss=4.437381196736673
Epoch 2382
Epoch 2382 :: Batch 0/1
Batch Loss = 4.437381170390427
2382, epoch_train_loss=4.437381170390427
Epoch 2383
Epoch 2383 :: Batch 0/1
Batch Loss = 4.437381143693256
2383, epoch_train_loss=4.437381143693256
Epoch 2384
Epoch 2384 :: Batch 0/1
Batch Loss = 4.43738111663827
2384, epoch_train_loss=4.43738111663827
Epoch 2385
Epoch 2385 :: Batch 0/1
Batch Loss = 4.437381089218397
2385, epoch_train_loss=4.437381089218397
Epoch 2386
Epoch 2386 :: Batch 0/1
Batch Loss = 4.437381061426383
2386, epoch_train_loss=4.437381061426383
Epoch 2387
Epoch 2387 :: Batch 0/1
Batch Loss = 4.437381033254781
2387, epoch_train_loss=4.437381033254781
Epoch 2388
Epoch 2388 :: Batch 0/1
Batch Loss = 4.437381004695945
2388, epoch_train_loss=4.437381004695945
Epoch 2389
Epoch 2389 :: Batch 0/1
Batch Loss = 4.437380975742028
2389, epoch_train_loss=4.437380975742028
Epoch 2390
Epoch 2390 :: Batch 0/1
Batch Loss = 4.437380946384965
2390, epoch_train_loss=4.437380946384965
Epoch 2391
Epoch 2391 :: Batch 0/1
Batch Loss = 4.43738091661648
2391, epoch_train_loss=4.43738091661648
Epoch 2392
Epoch 2392 :: Batch 0/1
Batch Loss = 4.437380886428067
2392, epoch_train_loss=4.437380886428067
Epoch 2393
Epoch 2393 :: Batch 0/1
Batch Loss = 4.437380855810985
2393, epoch_train_loss=4.437380855810985
Epoch 2394
Epoch 2394 :: Batch 0/1
Batch Loss = 4.437380824756254
2394, epoch_train_loss=4.437380824756254
Epoch 2395
Epoch 2395 :: Batch 0/1
Batch Loss = 4.437380793254641
2395, epoch_train_loss=4.437380793254641
Epoch 2396
Epoch 2396 :: Batch 0/1
Batch Loss = 4.437380761296654
2396, epoch_train_loss=4.437380761296654
Epoch 2397
Epoch 2397 :: Batch 0/1
Batch Loss = 4.437380728872534
2397, epoch_train_loss=4.437380728872534
Epoch 2398
Epoch 2398 :: Batch 0/1
Batch Loss = 4.4373806959722435
2398, epoch_train_loss=4.4373806959722435
Epoch 2399
Epoch 2399 :: Batch 0/1
Batch Loss = 4.4373806625854595
2399, epoch_train_loss=4.4373806625854595
Epoch 2400
Epoch 2400 :: Batch 0/1
Batch Loss = 4.437380628701556
2400, epoch_train_loss=4.437380628701556
Epoch 2401
Epoch 2401 :: Batch 0/1
Batch Loss = 4.437380594309599
2401, epoch_train_loss=4.437380594309599
Epoch 2402
Epoch 2402 :: Batch 0/1
Batch Loss = 4.437380559398337
2402, epoch_train_loss=4.437380559398337
Epoch 2403
Epoch 2403 :: Batch 0/1
Batch Loss = 4.437380523956182
2403, epoch_train_loss=4.437380523956182
Epoch 2404
Epoch 2404 :: Batch 0/1
Batch Loss = 4.437380487971201
2404, epoch_train_loss=4.437380487971201
Epoch 2405
Epoch 2405 :: Batch 0/1
Batch Loss = 4.43738045143111
2405, epoch_train_loss=4.43738045143111
Epoch 2406
Epoch 2406 :: Batch 0/1
Batch Loss = 4.437380414323244
2406, epoch_train_loss=4.437380414323244
Epoch 2407
Epoch 2407 :: Batch 0/1
Batch Loss = 4.43738037663456
2407, epoch_train_loss=4.43738037663456
Epoch 2408
Epoch 2408 :: Batch 0/1
Batch Loss = 4.43738033835161
2408, epoch_train_loss=4.43738033835161
Epoch 2409
Epoch 2409 :: Batch 0/1
Batch Loss = 4.4373802994605285
2409, epoch_train_loss=4.4373802994605285
Epoch 2410
Epoch 2410 :: Batch 0/1
Batch Loss = 4.437380259947024
2410, epoch_train_loss=4.437380259947024
Epoch 2411
Epoch 2411 :: Batch 0/1
Batch Loss = 4.437380219796347
2411, epoch_train_loss=4.437380219796347
Epoch 2412
Epoch 2412 :: Batch 0/1
Batch Loss = 4.43738017899329
2412, epoch_train_loss=4.43738017899329
Epoch 2413
Epoch 2413 :: Batch 0/1
Batch Loss = 4.437380137522151
2413, epoch_train_loss=4.437380137522151
Epoch 2414
Epoch 2414 :: Batch 0/1
Batch Loss = 4.437380095366719
2414, epoch_train_loss=4.437380095366719
Epoch 2415
Epoch 2415 :: Batch 0/1
Batch Loss = 4.437380052510271
2415, epoch_train_loss=4.437380052510271
Epoch 2416
Epoch 2416 :: Batch 0/1
Batch Loss = 4.437380008935525
2416, epoch_train_loss=4.437380008935525
Epoch 2417
Epoch 2417 :: Batch 0/1
Batch Loss = 4.437379964624627
2417, epoch_train_loss=4.437379964624627
Epoch 2418
Epoch 2418 :: Batch 0/1
Batch Loss = 4.437379919559129
2418, epoch_train_loss=4.437379919559129
Epoch 2419
Epoch 2419 :: Batch 0/1
Batch Loss = 4.437379873719967
2419, epoch_train_loss=4.437379873719967
Epoch 2420
Epoch 2420 :: Batch 0/1
Batch Loss = 4.437379827087423
2420, epoch_train_loss=4.437379827087423
Epoch 2421
Epoch 2421 :: Batch 0/1
Batch Loss = 4.437379779641106
2421, epoch_train_loss=4.437379779641106
Epoch 2422
Epoch 2422 :: Batch 0/1
Batch Loss = 4.437379731359927
2422, epoch_train_loss=4.437379731359927
Epoch 2423
Epoch 2423 :: Batch 0/1
Batch Loss = 4.437379682222047
2423, epoch_train_loss=4.437379682222047
Epoch 2424
Epoch 2424 :: Batch 0/1
Batch Loss = 4.437379632204875
2424, epoch_train_loss=4.437379632204875
Epoch 2425
Epoch 2425 :: Batch 0/1
Batch Loss = 4.437379581285005
2425, epoch_train_loss=4.437379581285005
Epoch 2426
Epoch 2426 :: Batch 0/1
Batch Loss = 4.437379529438197
2426, epoch_train_loss=4.437379529438197
Epoch 2427
Epoch 2427 :: Batch 0/1
Batch Loss = 4.437379476639333
2427, epoch_train_loss=4.437379476639333
Epoch 2428
Epoch 2428 :: Batch 0/1
Batch Loss = 4.437379422862376
2428, epoch_train_loss=4.437379422862376
Epoch 2429
Epoch 2429 :: Batch 0/1
Batch Loss = 4.4373793680803315
2429, epoch_train_loss=4.4373793680803315
Epoch 2430
Epoch 2430 :: Batch 0/1
Batch Loss = 4.437379312265188
2430, epoch_train_loss=4.437379312265188
Epoch 2431
Epoch 2431 :: Batch 0/1
Batch Loss = 4.4373792553878895
2431, epoch_train_loss=4.4373792553878895
Epoch 2432
Epoch 2432 :: Batch 0/1
Batch Loss = 4.437379197418276
2432, epoch_train_loss=4.437379197418276
Epoch 2433
Epoch 2433 :: Batch 0/1
Batch Loss = 4.437379138325019
2433, epoch_train_loss=4.437379138325019
Epoch 2434
Epoch 2434 :: Batch 0/1
Batch Loss = 4.4373790780755735
2434, epoch_train_loss=4.4373790780755735
Epoch 2435
Epoch 2435 :: Batch 0/1
Batch Loss = 4.437379016636125
2435, epoch_train_loss=4.437379016636125
Epoch 2436
Epoch 2436 :: Batch 0/1
Batch Loss = 4.43737895397152
2436, epoch_train_loss=4.43737895397152
Epoch 2437
Epoch 2437 :: Batch 0/1
Batch Loss = 4.437378890045186
2437, epoch_train_loss=4.437378890045186
Epoch 2438
Epoch 2438 :: Batch 0/1
Batch Loss = 4.437378824819059
2438, epoch_train_loss=4.437378824819059
Epoch 2439
Epoch 2439 :: Batch 0/1
Batch Loss = 4.43737875825354
2439, epoch_train_loss=4.43737875825354
Epoch 2440
Epoch 2440 :: Batch 0/1
Batch Loss = 4.437378690307368
2440, epoch_train_loss=4.437378690307368
Epoch 2441
Epoch 2441 :: Batch 0/1
Batch Loss = 4.437378620937562
2441, epoch_train_loss=4.437378620937562
Epoch 2442
Epoch 2442 :: Batch 0/1
Batch Loss = 4.437378550099312
2442, epoch_train_loss=4.437378550099312
Epoch 2443
Epoch 2443 :: Batch 0/1
Batch Loss = 4.437378477745879
2443, epoch_train_loss=4.437378477745879
Epoch 2444
Epoch 2444 :: Batch 0/1
Batch Loss = 4.437378403828507
2444, epoch_train_loss=4.437378403828507
Epoch 2445
Epoch 2445 :: Batch 0/1
Batch Loss = 4.437378328296289
2445, epoch_train_loss=4.437378328296289
Epoch 2446
Epoch 2446 :: Batch 0/1
Batch Loss = 4.43737825109605
2446, epoch_train_loss=4.43737825109605
Epoch 2447
Epoch 2447 :: Batch 0/1
Batch Loss = 4.437378172172222
2447, epoch_train_loss=4.437378172172222
Epoch 2448
Epoch 2448 :: Batch 0/1
Batch Loss = 4.43737809146669
2448, epoch_train_loss=4.43737809146669
Epoch 2449
Epoch 2449 :: Batch 0/1
Batch Loss = 4.4373780089186585
2449, epoch_train_loss=4.4373780089186585
Epoch 2450
Epoch 2450 :: Batch 0/1
Batch Loss = 4.4373779244644895
2450, epoch_train_loss=4.4373779244644895
Epoch 2451
Epoch 2451 :: Batch 0/1
Batch Loss = 4.437377838037521
2451, epoch_train_loss=4.437377838037521
Epoch 2452
Epoch 2452 :: Batch 0/1
Batch Loss = 4.437377749567877
2452, epoch_train_loss=4.437377749567877
Epoch 2453
Epoch 2453 :: Batch 0/1
Batch Loss = 4.437377658982266
2453, epoch_train_loss=4.437377658982266
Epoch 2454
Epoch 2454 :: Batch 0/1
Batch Loss = 4.437377566203785
2454, epoch_train_loss=4.437377566203785
Epoch 2455
Epoch 2455 :: Batch 0/1
Batch Loss = 4.437377471151655
2455, epoch_train_loss=4.437377471151655
Epoch 2456
Epoch 2456 :: Batch 0/1
Batch Loss = 4.437377373740992
2456, epoch_train_loss=4.437377373740992
Epoch 2457
Epoch 2457 :: Batch 0/1
Batch Loss = 4.4373772738825314
2457, epoch_train_loss=4.4373772738825314
Epoch 2458
Epoch 2458 :: Batch 0/1
Batch Loss = 4.43737717148231
2458, epoch_train_loss=4.43737717148231
Epoch 2459
Epoch 2459 :: Batch 0/1
Batch Loss = 4.437377066441373
2459, epoch_train_loss=4.437377066441373
Epoch 2460
Epoch 2460 :: Batch 0/1
Batch Loss = 4.437376958655397
2460, epoch_train_loss=4.437376958655397
Epoch 2461
Epoch 2461 :: Batch 0/1
Batch Loss = 4.4373768480143365
2461, epoch_train_loss=4.4373768480143365
Epoch 2462
Epoch 2462 :: Batch 0/1
Batch Loss = 4.4373767344019805
2462, epoch_train_loss=4.4373767344019805
Epoch 2463
Epoch 2463 :: Batch 0/1
Batch Loss = 4.437376617695535
2463, epoch_train_loss=4.437376617695535
Epoch 2464
Epoch 2464 :: Batch 0/1
Batch Loss = 4.437376497765094
2464, epoch_train_loss=4.437376497765094
Epoch 2465
Epoch 2465 :: Batch 0/1
Batch Loss = 4.43737637447312
2465, epoch_train_loss=4.43737637447312
Epoch 2466
Epoch 2466 :: Batch 0/1
Batch Loss = 4.437376247673852
2466, epoch_train_loss=4.437376247673852
Epoch 2467
Epoch 2467 :: Batch 0/1
Batch Loss = 4.4373761172126525
2467, epoch_train_loss=4.4373761172126525
Epoch 2468
Epoch 2468 :: Batch 0/1
Batch Loss = 4.437375982925291
2468, epoch_train_loss=4.437375982925291
Epoch 2469
Epoch 2469 :: Batch 0/1
Batch Loss = 4.437375844637168
2469, epoch_train_loss=4.437375844637168
Epoch 2470
Epoch 2470 :: Batch 0/1
Batch Loss = 4.437375702162417
2470, epoch_train_loss=4.437375702162417
Epoch 2471
Epoch 2471 :: Batch 0/1
Batch Loss = 4.43737555530297
2471, epoch_train_loss=4.43737555530297
Epoch 2472
Epoch 2472 :: Batch 0/1
Batch Loss = 4.437375403847487
2472, epoch_train_loss=4.437375403847487
Epoch 2473
Epoch 2473 :: Batch 0/1
Batch Loss = 4.437375247570174
2473, epoch_train_loss=4.437375247570174
Epoch 2474
Epoch 2474 :: Batch 0/1
Batch Loss = 4.437375086229476
2474, epoch_train_loss=4.437375086229476
Epoch 2475
Epoch 2475 :: Batch 0/1
Batch Loss = 4.437374919566574
2475, epoch_train_loss=4.437374919566574
Epoch 2476
Epoch 2476 :: Batch 0/1
Batch Loss = 4.437374747303784
2476, epoch_train_loss=4.437374747303784
Epoch 2477
Epoch 2477 :: Batch 0/1
Batch Loss = 4.437374569142709
2477, epoch_train_loss=4.437374569142709
Epoch 2478
Epoch 2478 :: Batch 0/1
Batch Loss = 4.437374384762169
2478, epoch_train_loss=4.437374384762169
Epoch 2479
Epoch 2479 :: Batch 0/1
Batch Loss = 4.437374193815878
2479, epoch_train_loss=4.437374193815878
Epoch 2480
Epoch 2480 :: Batch 0/1
Batch Loss = 4.437373995929824
2480, epoch_train_loss=4.437373995929824
Epoch 2481
Epoch 2481 :: Batch 0/1
Batch Loss = 4.437373790699288
2481, epoch_train_loss=4.437373790699288
Epoch 2482
Epoch 2482 :: Batch 0/1
Batch Loss = 4.437373577685452
2482, epoch_train_loss=4.437373577685452
Epoch 2483
Epoch 2483 :: Batch 0/1
Batch Loss = 4.437373356411603
2483, epoch_train_loss=4.437373356411603
Epoch 2484
Epoch 2484 :: Batch 0/1
Batch Loss = 4.437373126358679
2484, epoch_train_loss=4.437373126358679
Epoch 2485
Epoch 2485 :: Batch 0/1
Batch Loss = 4.437372886960241
2485, epoch_train_loss=4.437372886960241
Epoch 2486
Epoch 2486 :: Batch 0/1
Batch Loss = 4.437372637596661
2486, epoch_train_loss=4.437372637596661
Epoch 2487
Epoch 2487 :: Batch 0/1
Batch Loss = 4.437372377588399
2487, epoch_train_loss=4.437372377588399
Epoch 2488
Epoch 2488 :: Batch 0/1
Batch Loss = 4.437372106188266
2488, epoch_train_loss=4.437372106188266
Epoch 2489
Epoch 2489 :: Batch 0/1
Batch Loss = 4.43737182257232
2489, epoch_train_loss=4.43737182257232
Epoch 2490
Epoch 2490 :: Batch 0/1
Batch Loss = 4.437371525829274
2490, epoch_train_loss=4.437371525829274
Epoch 2491
Epoch 2491 :: Batch 0/1
Batch Loss = 4.4373712149480555
2491, epoch_train_loss=4.4373712149480555
Epoch 2492
Epoch 2492 :: Batch 0/1
Batch Loss = 4.437370888803125
2492, epoch_train_loss=4.437370888803125
Epoch 2493
Epoch 2493 :: Batch 0/1
Batch Loss = 4.4373705461370285
2493, epoch_train_loss=4.4373705461370285
Epoch 2494
Epoch 2494 :: Batch 0/1
Batch Loss = 4.4373701855397405
2494, epoch_train_loss=4.4373701855397405
Epoch 2495
Epoch 2495 :: Batch 0/1
Batch Loss = 4.437369805423686
2495, epoch_train_loss=4.437369805423686
Epoch 2496
Epoch 2496 :: Batch 0/1
Batch Loss = 4.437369403993913
2496, epoch_train_loss=4.437369403993913
Epoch 2497
Epoch 2497 :: Batch 0/1
Batch Loss = 4.4373689792116755
2497, epoch_train_loss=4.4373689792116755
Epoch 2498
Epoch 2498 :: Batch 0/1
Batch Loss = 4.437368528750294
2498, epoch_train_loss=4.437368528750294
Epoch 2499
Epoch 2499 :: Batch 0/1
Batch Loss = 4.437368049940704
2499, epoch_train_loss=4.437368049940704
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting "B3LYP_WITH_VWN5 = True" in pyscf_conf.py
  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0302b60> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0302b60> in UKS object of <class 'pyscf.dft.uks.UKS'>
WARNING: External module "mldftdat" required for non-local descriptor use.
<pyscf.gto.mole.Mole object at 0x7ffeb0302b60> [['P', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb03006d0> [['N', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb0300fa0> [['H', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb0302e30> [['Li', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb0301900> [['O', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb0302530> [['Cl', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb0301690> [['Al', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb0300580> [['S', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb0300ca0> [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb0301390> [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb0302ef0> [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb0302770> [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb03024a0> [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb03008b0> [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb00a4160> [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb00a4b20> [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb00a6c20> [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb00a6c50> [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb00a79d0> [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb00a7580> [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb00a72b0> [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb00a6590> [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb00a5690> [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb00a5f90> [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb00a4700> [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]] 5
<pyscf.gto.mole.Mole object at 0x7ffeb02d0820> [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb02d3580> [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]] 5
mol:  [['P', array([0., 0., 0.])]]
converged SCF energy = -341.104145992717  <S^2> = 3.7502984  2S+1 = 4.0001492
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb03006d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb03006d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-3.47389956e-03 -8.82676818e-04 -2.08411238e-03 ... -1.11301603e+01
 -1.11301603e+01 -1.11301603e+01] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 16)
mol:  [['N', array([0., 0., 0.])]]
converged SCF energy = -54.5289742046674  <S^2> = 3.7524945  2S+1 = 4.0012471
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0300fa0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0300fa0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5016, 22)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.07670570e-03 -5.92340671e-04 -6.66573372e-05 ... -5.03679786e+00
 -5.03679786e+00 -5.03679786e+00] = SCAN,
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5016, 22)
rho shape (4, 4, 2, 5016)
rho_filt shape: (5016,)
get descriptors tdrho.shape=(5016, 16)
mol:  [['H', array([0., 0., 0.])]]
converged SCF energy = -0.499812984008539  <S^2> = 0.75  2S+1 = 2
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0302e30> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0302e30> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 7, 7)
ao.shape (10, 2440, 7)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.59173730e-03 -7.67300285e-04 -6.09330303e-05 ... -7.27736608e-01
 -7.27736608e-01 -7.27736608e-01] = SCAN,
get_data, dm shape = (2, 7, 7)
ao_eval.shape=(4, 2440, 7)
rho shape (4, 4, 2, 2440)
rho_filt shape: (2440,)
get descriptors tdrho.shape=(2440, 16)
mol:  [['Li', array([0., 0., 0.])]]
converged SCF energy = -7.46006188627842  <S^2> = 0.75000049  2S+1 = 2.0000005
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0301900> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0301900> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 4592, 22)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-6.71503005e-03 -1.44519923e-03 -1.44519923e-03 ... -1.46899070e-02
 -2.03947707e+00 -2.03947707e+00] = SCAN,
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 4592, 22)
rho shape (4, 4, 2, 4592)
rho_filt shape: (4592,)
get descriptors tdrho.shape=(4592, 16)
mol:  [['O', array([0., 0., 0.])]]
converged SCF energy = -75.0033774438863  <S^2> = 2.0027452  2S+1 = 3.0018296
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0302530> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0302530> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5040, 22)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-7.48511638e-04 -1.24324024e-04 -6.20777460e-06 ... -5.78449376e+00
 -5.78449376e+00 -5.78449376e+00] = SCAN,
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5040, 22)
rho shape (4, 4, 2, 5040)
rho_filt shape: (5040,)
get descriptors tdrho.shape=(5040, 16)
mol:  [['Cl', array([0., 0., 0.])]]
converged SCF energy = -459.95757712413  <S^2> = 0.75161941  2S+1 = 2.0016188
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0301690> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0301690> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6152, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-5.81963844e-04 -9.06839735e-04 -3.08241277e-04 ... -1.26648275e+01
 -1.26648275e+01 -1.26648275e+01] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6152, 30)
rho shape (4, 4, 2, 6152)
rho_filt shape: (6152,)
get descriptors tdrho.shape=(6152, 16)
mol:  [['Al', array([0., 0., 0.])]]
converged SCF energy = -242.226560989234  <S^2> = 0.75226414  2S+1 = 2.0022629
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0300580> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0300580> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6088, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.33089483e-02 -8.48878485e-03 -4.25204071e-03 ... -1.37643420e-04
 -1.02964531e-03 -7.41775908e-05] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6088, 30)
rho shape (4, 4, 2, 6088)
rho_filt shape: (6088,)
get descriptors tdrho.shape=(6088, 16)
mol:  [['S', array([0., 0., 0.])]]
converged SCF energy = -397.938786807052  <S^2> = 2.0022329  2S+1 = 3.0014882
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0300ca0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0300ca0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.59289362e-03 -7.51450947e-04 -9.06608871e-04 ... -1.18986567e+01
 -1.18986567e+01 -1.18986567e+01] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 16)
mol:  [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]]
converged SCF energy = -1.16580491182912  <S^2> = 4.4408921e-16  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0301390> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0301390> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 14, 14)
ao.shape (10, 4776, 14)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.31557088e-04 -9.73828620e-06 -3.66768667e-04 ... -5.54165574e-01
 -5.54165574e-01 -5.54165574e-01] = SCAN,
get_data, dm shape = (2, 14, 14)
ao_eval.shape=(4, 4776, 14)
rho shape (4, 4, 2, 4776)
rho_filt shape: (4776,)
get descriptors tdrho.shape=(4776, 16)
mol:  [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]]
converged SCF energy = -109.439263799566  <S^2> = 1.9539925e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0302ef0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0302ef0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9848, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-9.68474977e-05 -9.84742592e-04 -2.59676393e-04 ... -2.39645778e-05
 -2.39645778e-05 -9.68474977e-05] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9848, 44)
rho shape (4, 4, 2, 9848)
rho_filt shape: (9848,)
get descriptors tdrho.shape=(9848, 16)
mol:  [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]]
converged SCF energy = -107.339357395734  <S^2> = 7.1054274e-15  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0302770> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0302770> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9752, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.04987770e-03 -6.68954111e-04 -8.57556562e-04 ... -1.07485605e-03
 -8.01425702e-01 -8.01425702e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9752, 44)
rho shape (4, 4, 2, 9752)
rho_filt shape: (9752,)
get descriptors tdrho.shape=(9752, 16)
mol:  [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]]
converged SCF energy = -93.337792446513  <S^2> = 4.0072923e-10  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb03024a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb03024a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 12256, 51)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.97917639e-04 -2.54437615e-05 -3.15202008e-05 ... -6.37386388e-01
 -6.37386388e-01 -6.37386388e-01] = SCAN,
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 12256, 51)
rho shape (4, 4, 2, 12256)
rho_filt shape: (12256,)
get descriptors tdrho.shape=(12256, 16)
mol:  [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]]
converged SCF energy = -188.456965322843  <S^2> = 1.5987212e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb03008b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb03008b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 66, 66)
ao.shape (10, 14920, 66)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.50217343e-04 -2.07520331e-04 -9.23619961e-04 ... -2.76182455e-06
 -4.27559894e+00 -4.27559894e+00] = SCAN,
get_data, dm shape = (2, 66, 66)
ao_eval.shape=(4, 14920, 66)
rho shape (4, 4, 2, 14920)
rho_filt shape: (14920,)
get descriptors tdrho.shape=(14920, 16)
mol:  [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]]
converged SCF energy = -920.005608888958  <S^2> = 5.0448534e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb00a4160> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb00a4160> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12208, 60)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-0.00155834 -0.00091688 -0.00215831 ... -0.00091688 -0.41618506
 -0.41618506] = SCAN,
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12208, 60)
rho shape (4, 4, 2, 12208)
rho_filt shape: (12208,)
get descriptors tdrho.shape=(12208, 16)
mol:  [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]]
converged SCF energy = -199.394370591172  <S^2> = 1.2967405e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb00a4b20> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb00a4b20> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9824, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.92948752e-04 -1.95215890e-05 -1.16699780e-03 ... -4.89378326e-01
 -4.89378326e-01 -4.89378326e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9824, 44)
rho shape (4, 4, 2, 9824)
rho_filt shape: (9824,)
get descriptors tdrho.shape=(9824, 16)
mol:  [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]]
converged SCF energy = -150.214894533261  <S^2> = 1.0018599  2S+1 = 2.2377309
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb00a6c20> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb00a6c20> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9912, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-6.91772546e-04 -1.23036460e-04 -6.34270046e-06 ... -6.59150605e-01
 -6.59150605e-01 -6.59150605e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9912, 44)
rho shape (4, 4, 2, 9912)
rho_filt shape: (9912,)
get descriptors tdrho.shape=(9912, 16)
mol:  [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]]
converged SCF energy = -77.2435048346374  <S^2> = 9.7699626e-15  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb00a6c50> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb00a6c50> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 58, 58)
ao.shape (10, 15208, 58)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-8.83278187e-05 -8.83278187e-05 -9.75839793e-04 ... -3.46740731e-05
 -3.31729009e-05 -3.31729009e-05] = SCAN,
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15208, 58)
rho shape (4, 4, 2, 15208)
rho_filt shape: (15208,)
get descriptors tdrho.shape=(15208, 16)
mol:  [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]]
converged SCF energy = -113.221335689652  <S^2> = 6.5547567e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb00a79d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb00a79d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 10040, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-5.37000596e-04 -8.55494373e-04 -2.46853248e-03 ... -7.34251993e-01
 -7.34251993e-01 -7.34251993e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 10040, 44)
rho shape (4, 4, 2, 10040)
rho_filt shape: (10040,)
get descriptors tdrho.shape=(10040, 16)
mol:  [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]]
converged SCF energy = -460.624592374077  <S^2> = 7.283063e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb00a7580> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb00a7580> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 37, 37)
ao.shape (10, 8552, 37)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.38161478e-04 -1.81223966e-05 -2.37327566e-05 ... -2.83738108e+00
 -2.83738108e+00 -2.83738108e+00] = SCAN,
get_data, dm shape = (2, 37, 37)
ao_eval.shape=(4, 8552, 37)
rho shape (4, 4, 2, 8552)
rho_filt shape: (8552,)
get descriptors tdrho.shape=(8552, 16)
mol:  [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]]
converged SCF energy = -8.04458854018922  <S^2> = 7.7937656e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb00a72b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb00a72b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 29, 29)
ao.shape (10, 6936, 29)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-0.00113445 -0.00118624 -0.00118624 ... -0.48434639 -0.48434639
 -0.48434639] = SCAN,
get_data, dm shape = (2, 29, 29)
ao_eval.shape=(4, 6936, 29)
rho shape (4, 4, 2, 6936)
rho_filt shape: (6936,)
get descriptors tdrho.shape=(6936, 16)
mol:  [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]]
converged SCF energy = -324.340512506578  <S^2> = 1.5855761e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb00a6590> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb00a6590> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 11536, 60)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-0.00297936 -0.00297936 -0.00407091 ... -0.00297936 -0.00297936
 -0.00407091] = SCAN,
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 11536, 60)
rho shape (4, 4, 2, 11536)
rho_filt shape: (11536,)
get descriptors tdrho.shape=(11536, 16)
mol:  [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]]
converged SCF energy = -1622.57507845814  <S^2> = 8.2422957e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb00a5690> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb00a5690> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 120, 120)
ao.shape (10, 24512, 120)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.61401455e-04 -4.90485117e-04 -2.56451688e-03 ... -9.59296114e+00
 -9.59296114e+00 -9.59296114e+00] = SCAN,
get_data, dm shape = (2, 120, 120)
ao_eval.shape=(4, 24512, 120)
rho shape (4, 4, 2, 24512)
rho_filt shape: (24512,)
get descriptors tdrho.shape=(24512, 16)
mol:  [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]]
converged SCF energy = -342.979728469575  <S^2> = 2.5393021e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb00a5f90> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb00a5f90> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 13096, 51)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.28637187e-03 -4.32380890e-04 -3.74072638e-05 ... -1.91722763e+00
 -1.91722763e+00 -1.91722763e+00] = SCAN,
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 13096, 51)
rho shape (4, 4, 2, 13096)
rho_filt shape: (13096,)
get descriptors tdrho.shape=(13096, 16)
mol:  [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]]
converged SCF energy = -578.565336641349  <S^2> = 1.0034707  2S+1 = 2.2391701
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb00a4700> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb00a4700> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12384, 60)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.60118913e-04 -2.60152326e-04 -2.60145797e-04 ... -3.86943856e-01
 -3.86943856e-01 -3.86943856e-01] = SCAN,
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12384, 60)
rho shape (4, 4, 2, 12384)
rho_filt shape: (12384,)
get descriptors tdrho.shape=(12384, 16)
mol:  [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]]
converged SCF energy = -40.4598214864075  <S^2> = 3.1974423e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb02d0820> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb02d0820> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 50, 50)
ao.shape (10, 13936, 50)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-8.68439856e-04 -2.42462783e-04 -1.69965237e-05 ... -2.55256081e-05
 -2.55256081e-05 -2.55256081e-05] = SCAN,
get_data, dm shape = (2, 50, 50)
ao_eval.shape=(4, 13936, 50)
rho shape (4, 4, 2, 13936)
rho_filt shape: (13936,)
get descriptors tdrho.shape=(13936, 16)
mol:  [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]]
converged SCF energy = -39.0756147483504  <S^2> = 6.1994854e-12  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb02d3580> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb02d3580> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 36, 36)
ao.shape (10, 9656, 36)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-7.67691257e-04 -4.57409182e-05 -2.02835243e-04 ... -1.14928928e+00
 -1.14928928e+00 -1.14928928e+00] = SCAN,
get_data, dm shape = (2, 36, 36)
ao_eval.shape=(4, 9656, 36)
rho shape (4, 4, 2, 9656)
rho_filt shape: (9656,)
get descriptors tdrho.shape=(9656, 16)
mol:  [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]]
converged SCF energy = -291.719272437818  <S^2> = 1.3155699e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
New DM shape: (2, 58, 58)
ao.shape (10, 15256, 58)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-8.33847724e-04 -2.34902391e-04 -1.75660753e-05 ... -1.92925750e-05
 -1.92925750e-05 -1.92925750e-05] = SCAN,
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15256, 58)
rho shape (4, 4, 2, 15256)
rho_filt shape: (15256,)
get descriptors tdrho.shape=(15256, 16)
PRE NAN FILT: tFxc.shape=(237021,), tdrho.shape=(237021, 16)
nan_filt_rho.shape=(237021,)
nan_filt_fxc.shape=(237021,)
tFxc.shape=(237021,), tdrho.shape=(237021, 16)
inp[0].shape = (237021, 16)
Epoch 0
Epoch 0 :: Batch 0/1
Batch Loss = 948345.9307054718
0, epoch_train_loss=948345.9307054718
Epoch 1
Epoch 1 :: Batch 0/1
Batch Loss = 195434780.0766344
1, epoch_train_loss=195434780.0766344
Epoch 2
Epoch 2 :: Batch 0/1
Batch Loss = 1025850.2795476998
2, epoch_train_loss=1025850.2795476998
Epoch 3
Epoch 3 :: Batch 0/1
Batch Loss = 3465183.9635517616
3, epoch_train_loss=3465183.9635517616
Epoch 4
Epoch 4 :: Batch 0/1
Batch Loss = 1199258.2530317523
4, epoch_train_loss=1199258.2530317523
Epoch 5
Epoch 5 :: Batch 0/1
Batch Loss = 20151.15847826155
5, epoch_train_loss=20151.15847826155
Epoch 6
Epoch 6 :: Batch 0/1
Batch Loss = 489267.042108189
6, epoch_train_loss=489267.042108189
Epoch 7
Epoch 7 :: Batch 0/1
Batch Loss = 239103.97742016215
7, epoch_train_loss=239103.97742016215
Epoch 8
Epoch 8 :: Batch 0/1
Batch Loss = 6338.903719167407
8, epoch_train_loss=6338.903719167407
Epoch 9
Epoch 9 :: Batch 0/1
Batch Loss = 316537.8251132697
9, epoch_train_loss=316537.8251132697
Epoch 10
Epoch 10 :: Batch 0/1
Batch Loss = 238371.0024466091
10, epoch_train_loss=238371.0024466091
Epoch 11
Epoch 11 :: Batch 0/1
Batch Loss = 3078.4310029064454
11, epoch_train_loss=3078.4310029064454
Epoch 12
Epoch 12 :: Batch 0/1
Batch Loss = 139691.63698100514
12, epoch_train_loss=139691.63698100514
Epoch 13
Epoch 13 :: Batch 0/1
Batch Loss = 206202.34663829062
13, epoch_train_loss=206202.34663829062
Epoch 14
Epoch 14 :: Batch 0/1
Batch Loss = 40627.151785628404
14, epoch_train_loss=40627.151785628404
Epoch 15
Epoch 15 :: Batch 0/1
Batch Loss = 24323.978769579568
15, epoch_train_loss=24323.978769579568
Epoch 16
Epoch 16 :: Batch 0/1
Batch Loss = 156782.53540405838
16, epoch_train_loss=156782.53540405838
Epoch 17
Epoch 17 :: Batch 0/1
Batch Loss = 110462.28436017521
17, epoch_train_loss=110462.28436017521
Epoch 18
Epoch 18 :: Batch 0/1
Batch Loss = 1826.3885013312336
18, epoch_train_loss=1826.3885013312336
Epoch 19
Epoch 19 :: Batch 0/1
Batch Loss = 149031.1362740771
19, epoch_train_loss=149031.1362740771
Epoch 20
Epoch 20 :: Batch 0/1
Batch Loss = 44406.713993938756
20, epoch_train_loss=44406.713993938756
Epoch 21
Epoch 21 :: Batch 0/1
Batch Loss = 124.2951630801103
21, epoch_train_loss=124.2951630801103
Epoch 22
Epoch 22 :: Batch 0/1
Batch Loss = 25350.61795811554
22, epoch_train_loss=25350.61795811554
Epoch 23
Epoch 23 :: Batch 0/1
Batch Loss = 35580.44080260917
23, epoch_train_loss=35580.44080260917
Epoch 24
Epoch 24 :: Batch 0/1
Batch Loss = 14226.055018143832
24, epoch_train_loss=14226.055018143832
Epoch 25
Epoch 25 :: Batch 0/1
Batch Loss = 28.081132745451605
25, epoch_train_loss=28.081132745451605
Epoch 26
Epoch 26 :: Batch 0/1
Batch Loss = 11999.18164341706
26, epoch_train_loss=11999.18164341706
Epoch 27
Epoch 27 :: Batch 0/1
Batch Loss = 20600.886626169977
27, epoch_train_loss=20600.886626169977
Epoch 28
Epoch 28 :: Batch 0/1
Batch Loss = 4763.719956228747
28, epoch_train_loss=4763.719956228747
Epoch 29
Epoch 29 :: Batch 0/1
Batch Loss = 159601.95819635122
29, epoch_train_loss=159601.95819635122
Epoch 30
Epoch 30 :: Batch 0/1
Batch Loss = 266476.4807880246
30, epoch_train_loss=266476.4807880246
Epoch 31
Epoch 31 :: Batch 0/1
Batch Loss = 110141.7663468405
31, epoch_train_loss=110141.7663468405
Epoch 32
Epoch 32 :: Batch 0/1
Batch Loss = 10272.411496191415
32, epoch_train_loss=10272.411496191415
Epoch 33
Epoch 33 :: Batch 0/1
Batch Loss = 195882.26791943112
33, epoch_train_loss=195882.26791943112
Epoch 34
Epoch 34 :: Batch 0/1
Batch Loss = 125173.24637645084
34, epoch_train_loss=125173.24637645084
Epoch 35
Epoch 35 :: Batch 0/1
Batch Loss = 202.67832443914915
35, epoch_train_loss=202.67832443914915
Epoch 36
Epoch 36 :: Batch 0/1
Batch Loss = 107144.88111443404
36, epoch_train_loss=107144.88111443404
Epoch 37
Epoch 37 :: Batch 0/1
Batch Loss = 132527.20612626418
37, epoch_train_loss=132527.20612626418
Epoch 38
Epoch 38 :: Batch 0/1
Batch Loss = 21021.24722834454
38, epoch_train_loss=21021.24722834454
Epoch 39
Epoch 39 :: Batch 0/1
Batch Loss = 17768.83334751158
39, epoch_train_loss=17768.83334751158
Epoch 40
Epoch 40 :: Batch 0/1
Batch Loss = 72725.03964918798
40, epoch_train_loss=72725.03964918798
Epoch 41
Epoch 41 :: Batch 0/1
Batch Loss = 53759.83565036699
41, epoch_train_loss=53759.83565036699
Epoch 42
Epoch 42 :: Batch 0/1
Batch Loss = 9908.34880734348
42, epoch_train_loss=9908.34880734348
Epoch 43
Epoch 43 :: Batch 0/1
Batch Loss = 3077.7808484474263
43, epoch_train_loss=3077.7808484474263
Epoch 44
Epoch 44 :: Batch 0/1
Batch Loss = 28371.997089167267
44, epoch_train_loss=28371.997089167267
Epoch 45
Epoch 45 :: Batch 0/1
Batch Loss = 41507.90979693575
45, epoch_train_loss=41507.90979693575
Epoch 46
Epoch 46 :: Batch 0/1
Batch Loss = 25908.962943145594
46, epoch_train_loss=25908.962943145594
Epoch 47
Epoch 47 :: Batch 0/1
Batch Loss = 4491.902761588399
47, epoch_train_loss=4491.902761588399
Epoch 48
Epoch 48 :: Batch 0/1
Batch Loss = 1517.5807005901813
48, epoch_train_loss=1517.5807005901813
Epoch 49
Epoch 49 :: Batch 0/1
Batch Loss = 14223.912091365914
49, epoch_train_loss=14223.912091365914
Epoch 50
Epoch 50 :: Batch 0/1
Batch Loss = 22509.730329587717
50, epoch_train_loss=22509.730329587717
Epoch 51
Epoch 51 :: Batch 0/1
Batch Loss = 16286.98609504705
51, epoch_train_loss=16286.98609504705
Epoch 52
Epoch 52 :: Batch 0/1
Batch Loss = 4489.249867700858
52, epoch_train_loss=4489.249867700858
Epoch 53
Epoch 53 :: Batch 0/1
Batch Loss = 54.747300464488916
53, epoch_train_loss=54.747300464488916
Epoch 54
Epoch 54 :: Batch 0/1
Batch Loss = 4952.316435753548
54, epoch_train_loss=4952.316435753548
Epoch 55
Epoch 55 :: Batch 0/1
Batch Loss = 11082.069339747733
55, epoch_train_loss=11082.069339747733
Epoch 56
Epoch 56 :: Batch 0/1
Batch Loss = 11122.208964909121
56, epoch_train_loss=11122.208964909121
Epoch 57
Epoch 57 :: Batch 0/1
Batch Loss = 5672.393016810875
57, epoch_train_loss=5672.393016810875
Epoch 58
Epoch 58 :: Batch 0/1
Batch Loss = 734.8616158726917
58, epoch_train_loss=734.8616158726917
Epoch 59
Epoch 59 :: Batch 0/1
Batch Loss = 560.6300354143394
59, epoch_train_loss=560.6300354143394
Epoch 60
Epoch 60 :: Batch 0/1
Batch Loss = 3876.060689423383
60, epoch_train_loss=3876.060689423383
Epoch 61
Epoch 61 :: Batch 0/1
Batch Loss = 6295.415861767492
61, epoch_train_loss=6295.415861767492
Epoch 62
Epoch 62 :: Batch 0/1
Batch Loss = 5265.151064641797
62, epoch_train_loss=5265.151064641797
Epoch 63
Epoch 63 :: Batch 0/1
Batch Loss = 2168.4936685421994
63, epoch_train_loss=2168.4936685421994
Epoch 64
Epoch 64 :: Batch 0/1
Batch Loss = 116.93973615477273
64, epoch_train_loss=116.93973615477273
Epoch 65
Epoch 65 :: Batch 0/1
Batch Loss = 588.489628717186
65, epoch_train_loss=588.489628717186
Epoch 66
Epoch 66 :: Batch 0/1
Batch Loss = 2397.221900083931
66, epoch_train_loss=2397.221900083931
Epoch 67
Epoch 67 :: Batch 0/1
Batch Loss = 3378.447631583282
67, epoch_train_loss=3378.447631583282
Epoch 68
Epoch 68 :: Batch 0/1
Batch Loss = 2601.548817932775
68, epoch_train_loss=2601.548817932775
Epoch 69
Epoch 69 :: Batch 0/1
Batch Loss = 973.3193356218261
69, epoch_train_loss=973.3193356218261
Epoch 70
Epoch 70 :: Batch 0/1
Batch Loss = 28.837468457352283
70, epoch_train_loss=28.837468457352283
Epoch 71
Epoch 71 :: Batch 0/1
Batch Loss = 387.87324352916335
71, epoch_train_loss=387.87324352916335
Epoch 72
Epoch 72 :: Batch 0/1
Batch Loss = 1352.62795485037
72, epoch_train_loss=1352.62795485037
Epoch 73
Epoch 73 :: Batch 0/1
Batch Loss = 1802.9259984208777
73, epoch_train_loss=1802.9259984208777
Epoch 74
Epoch 74 :: Batch 0/1
Batch Loss = 1322.184962973027
74, epoch_train_loss=1322.184962973027
Epoch 75
Epoch 75 :: Batch 0/1
Batch Loss = 453.60337883579126
75, epoch_train_loss=453.60337883579126
Epoch 76
Epoch 76 :: Batch 0/1
Batch Loss = 6.4303075266988685
76, epoch_train_loss=6.4303075266988685
Epoch 77
Epoch 77 :: Batch 0/1
Batch Loss = 249.15609345549214
77, epoch_train_loss=249.15609345549214
Epoch 78
Epoch 78 :: Batch 0/1
Batch Loss = 764.191647990343
78, epoch_train_loss=764.191647990343
Epoch 79
Epoch 79 :: Batch 0/1
Batch Loss = 966.1493081308611
79, epoch_train_loss=966.1493081308611
Epoch 80
Epoch 80 :: Batch 0/1
Batch Loss = 676.212947458365
80, epoch_train_loss=676.212947458365
Epoch 81
Epoch 81 :: Batch 0/1
Batch Loss = 212.47731551772554
81, epoch_train_loss=212.47731551772554
Epoch 82
Epoch 82 :: Batch 0/1
Batch Loss = 1.9447802035056434
82, epoch_train_loss=1.9447802035056434
Epoch 83
Epoch 83 :: Batch 0/1
Batch Loss = 160.24015989024016
83, epoch_train_loss=160.24015989024016
Epoch 84
Epoch 84 :: Batch 0/1
Batch Loss = 437.4150797797438
84, epoch_train_loss=437.4150797797438
Epoch 85
Epoch 85 :: Batch 0/1
Batch Loss = 518.9743991489462
85, epoch_train_loss=518.9743991489462
Epoch 86
Epoch 86 :: Batch 0/1
Batch Loss = 336.524163551822
86, epoch_train_loss=336.524163551822
Epoch 87
Epoch 87 :: Batch 0/1
Batch Loss = 88.88838574132075
87, epoch_train_loss=88.88838574132075
Epoch 88
Epoch 88 :: Batch 0/1
Batch Loss = 3.298189781394758
88, epoch_train_loss=3.298189781394758
Epoch 89
Epoch 89 :: Batch 0/1
Batch Loss = 112.23452252696738
89, epoch_train_loss=112.23452252696738
Epoch 90
Epoch 90 :: Batch 0/1
Batch Loss = 257.4065676151006
90, epoch_train_loss=257.4065676151006
Epoch 91
Epoch 91 :: Batch 0/1
Batch Loss = 276.12024333722337
91, epoch_train_loss=276.12024333722337
Epoch 92
Epoch 92 :: Batch 0/1
Batch Loss = 158.84637366268706
92, epoch_train_loss=158.84637366268706
Epoch 93
Epoch 93 :: Batch 0/1
Batch Loss = 31.184130454419037
93, epoch_train_loss=31.184130454419037
Epoch 94
Epoch 94 :: Batch 0/1
Batch Loss = 7.716486736096677
94, epoch_train_loss=7.716486736096677
Epoch 95
Epoch 95 :: Batch 0/1
Batch Loss = 81.76035606570996
95, epoch_train_loss=81.76035606570996
Epoch 96
Epoch 96 :: Batch 0/1
Batch Loss = 152.9036789725876
96, epoch_train_loss=152.9036789725876
Epoch 97
Epoch 97 :: Batch 0/1
Batch Loss = 142.4264002227953
97, epoch_train_loss=142.4264002227953
Epoch 98
Epoch 98 :: Batch 0/1
Batch Loss = 67.58751546556017
98, epoch_train_loss=67.58751546556017
Epoch 99
Epoch 99 :: Batch 0/1
Batch Loss = 7.4791352967904
99, epoch_train_loss=7.4791352967904
Epoch 100
Epoch 100 :: Batch 0/1
Batch Loss = 12.964546664017787
100, epoch_train_loss=12.964546664017787
Epoch 101
Epoch 101 :: Batch 0/1
Batch Loss = 60.620060713843436
101, epoch_train_loss=60.620060713843436
Epoch 102
Epoch 102 :: Batch 0/1
Batch Loss = 89.38016981221594
102, epoch_train_loss=89.38016981221594
Epoch 103
Epoch 103 :: Batch 0/1
Batch Loss = 68.72956818663575
103, epoch_train_loss=68.72956818663575
Epoch 104
Epoch 104 :: Batch 0/1
Batch Loss = 24.20721230436052
104, epoch_train_loss=24.20721230436052
Epoch 105
Epoch 105 :: Batch 0/1
Batch Loss = 1.5489064018105605
105, epoch_train_loss=1.5489064018105605
Epoch 106
Epoch 106 :: Batch 0/1
Batch Loss = 16.380250934701092
106, epoch_train_loss=16.380250934701092
Epoch 107
Epoch 107 :: Batch 0/1
Batch Loss = 43.35764179049081
107, epoch_train_loss=43.35764179049081
Epoch 108
Epoch 108 :: Batch 0/1
Batch Loss = 49.41778548510161
108, epoch_train_loss=49.41778548510161
Epoch 109
Epoch 109 :: Batch 0/1
Batch Loss = 29.37923637329522
109, epoch_train_loss=29.37923637329522
Epoch 110
Epoch 110 :: Batch 0/1
Batch Loss = 6.429227509839315
110, epoch_train_loss=6.429227509839315
Epoch 111
Epoch 111 :: Batch 0/1
Batch Loss = 2.8886011057276284
111, epoch_train_loss=2.8886011057276284
Epoch 112
Epoch 112 :: Batch 0/1
Batch Loss = 16.92571304879709
112, epoch_train_loss=16.92571304879709
Epoch 113
Epoch 113 :: Batch 0/1
Batch Loss = 28.745903429719007
113, epoch_train_loss=28.745903429719007
Epoch 114
Epoch 114 :: Batch 0/1
Batch Loss = 24.61411079857207
114, epoch_train_loss=24.61411079857207
Epoch 115
Epoch 115 :: Batch 0/1
Batch Loss = 10.333550022840432
115, epoch_train_loss=10.333550022840432
Epoch 116
Epoch 116 :: Batch 0/1
Batch Loss = 1.6241392064142792
116, epoch_train_loss=1.6241392064142792
Epoch 117
Epoch 117 :: Batch 0/1
Batch Loss = 5.570735425007186
117, epoch_train_loss=5.570735425007186
Epoch 118
Epoch 118 :: Batch 0/1
Batch Loss = 14.529552016946072
118, epoch_train_loss=14.529552016946072
Epoch 119
Epoch 119 :: Batch 0/1
Batch Loss = 16.91699767507957
119, epoch_train_loss=16.91699767507957
Epoch 120
Epoch 120 :: Batch 0/1
Batch Loss = 10.435675027380078
120, epoch_train_loss=10.435675027380078
Epoch 121
Epoch 121 :: Batch 0/1
Batch Loss = 2.9321290547454772
121, epoch_train_loss=2.9321290547454772
Epoch 122
Epoch 122 :: Batch 0/1
Batch Loss = 2.0375294543785123
122, epoch_train_loss=2.0375294543785123
Epoch 123
Epoch 123 :: Batch 0/1
Batch Loss = 6.849147030796192
123, epoch_train_loss=6.849147030796192
Epoch 124
Epoch 124 :: Batch 0/1
Batch Loss = 10.42993405658191
124, epoch_train_loss=10.42993405658191
Epoch 125
Epoch 125 :: Batch 0/1
Batch Loss = 8.502288962040002
125, epoch_train_loss=8.502288962040002
Epoch 126
Epoch 126 :: Batch 0/1
Batch Loss = 3.705892809287624
126, epoch_train_loss=3.705892809287624
Epoch 127
Epoch 127 :: Batch 0/1
Batch Loss = 1.4415268813674011
127, epoch_train_loss=1.4415268813674011
Epoch 128
Epoch 128 :: Batch 0/1
Batch Loss = 3.3624202039790565
128, epoch_train_loss=3.3624202039790565
Epoch 129
Epoch 129 :: Batch 0/1
Batch Loss = 6.183349907236096
129, epoch_train_loss=6.183349907236096
Epoch 130
Epoch 130 :: Batch 0/1
Batch Loss = 6.232504528240285
130, epoch_train_loss=6.232504528240285
Epoch 131
Epoch 131 :: Batch 0/1
Batch Loss = 3.6561831564752114
131, epoch_train_loss=3.6561831564752114
Epoch 132
Epoch 132 :: Batch 0/1
Batch Loss = 1.5753930593838745
132, epoch_train_loss=1.5753930593838745
Epoch 133
Epoch 133 :: Batch 0/1
Batch Loss = 2.012590977933451
133, epoch_train_loss=2.012590977933451
Epoch 134
Epoch 134 :: Batch 0/1
Batch Loss = 3.7826977345878405
134, epoch_train_loss=3.7826977345878405
Epoch 135
Epoch 135 :: Batch 0/1
Batch Loss = 4.4128881479412945
135, epoch_train_loss=4.4128881479412945
Epoch 136
Epoch 136 :: Batch 0/1
Batch Loss = 3.202879822903407
136, epoch_train_loss=3.202879822903407
Epoch 137
Epoch 137 :: Batch 0/1
Batch Loss = 1.7149677122350895
137, epoch_train_loss=1.7149677122350895
Epoch 138
Epoch 138 :: Batch 0/1
Batch Loss = 1.567237098419909
138, epoch_train_loss=1.567237098419909
Epoch 139
Epoch 139 :: Batch 0/1
Batch Loss = 2.5478518813642244
139, epoch_train_loss=2.5478518813642244
Epoch 140
Epoch 140 :: Batch 0/1
Batch Loss = 3.1843345058846886
140, epoch_train_loss=3.1843345058846886
Epoch 141
Epoch 141 :: Batch 0/1
Batch Loss = 2.6833634654339678
141, epoch_train_loss=2.6833634654339678
Epoch 142
Epoch 142 :: Batch 0/1
Batch Loss = 1.7377042615259155
142, epoch_train_loss=1.7377042615259155
Epoch 143
Epoch 143 :: Batch 0/1
Batch Loss = 1.4472979142757698
143, epoch_train_loss=1.4472979142757698
Epoch 144
Epoch 144 :: Batch 0/1
Batch Loss = 1.950183325022915
144, epoch_train_loss=1.950183325022915
Epoch 145
Epoch 145 :: Batch 0/1
Batch Loss = 2.429373714270729
145, epoch_train_loss=2.429373714270729
Epoch 146
Epoch 146 :: Batch 0/1
Batch Loss = 2.2505022949328386
146, epoch_train_loss=2.2505022949328386
Epoch 147
Epoch 147 :: Batch 0/1
Batch Loss = 1.6837989047376274
147, epoch_train_loss=1.6837989047376274
Epoch 148
Epoch 148 :: Batch 0/1
Batch Loss = 1.4198393484016327
148, epoch_train_loss=1.4198393484016327
Epoch 149
Epoch 149 :: Batch 0/1
Batch Loss = 1.6665408264515535
149, epoch_train_loss=1.6665408264515535
Epoch 150
Epoch 150 :: Batch 0/1
Batch Loss = 1.9845731129835842
150, epoch_train_loss=1.9845731129835842
Epoch 151
Epoch 151 :: Batch 0/1
Batch Loss = 1.9321509863340447
151, epoch_train_loss=1.9321509863340447
Epoch 152
Epoch 152 :: Batch 0/1
Batch Loss = 1.6001913464162691
152, epoch_train_loss=1.6001913464162691
Epoch 153
Epoch 153 :: Batch 0/1
Batch Loss = 1.4069542063325473
153, epoch_train_loss=1.4069542063325473
Epoch 154
Epoch 154 :: Batch 0/1
Batch Loss = 1.5293453788885196
154, epoch_train_loss=1.5293453788885196
Epoch 155
Epoch 155 :: Batch 0/1
Batch Loss = 1.7295308406049121
155, epoch_train_loss=1.7295308406049121
Epoch 156
Epoch 156 :: Batch 0/1
Batch Loss = 1.7205579342607609
156, epoch_train_loss=1.7205579342607609
Epoch 157
Epoch 157 :: Batch 0/1
Batch Loss = 1.527523479879777
157, epoch_train_loss=1.527523479879777
Epoch 158
Epoch 158 :: Batch 0/1
Batch Loss = 1.4004265824519824
158, epoch_train_loss=1.4004265824519824
Epoch 159
Epoch 159 :: Batch 0/1
Batch Loss = 1.4644795107994935
159, epoch_train_loss=1.4644795107994935
Epoch 160
Epoch 160 :: Batch 0/1
Batch Loss = 1.586875127210701
160, epoch_train_loss=1.586875127210701
Epoch 161
Epoch 161 :: Batch 0/1
Batch Loss = 1.5879951112042572
161, epoch_train_loss=1.5879951112042572
Epoch 162
Epoch 162 :: Batch 0/1
Batch Loss = 1.473754765433643
162, epoch_train_loss=1.473754765433643
Epoch 163
Epoch 163 :: Batch 0/1
Batch Loss = 1.3947886921603778
163, epoch_train_loss=1.3947886921603778
Epoch 164
Epoch 164 :: Batch 0/1
Batch Loss = 1.431285799144805
164, epoch_train_loss=1.431285799144805
Epoch 165
Epoch 165 :: Batch 0/1
Batch Loss = 1.50456258224398
165, epoch_train_loss=1.50456258224398
Epoch 166
Epoch 166 :: Batch 0/1
Batch Loss = 1.5047859626299225
166, epoch_train_loss=1.5047859626299225
Epoch 167
Epoch 167 :: Batch 0/1
Batch Loss = 1.4355048600212945
167, epoch_train_loss=1.4355048600212945
Epoch 168
Epoch 168 :: Batch 0/1
Batch Loss = 1.3886991849185968
168, epoch_train_loss=1.3886991849185968
Epoch 169
Epoch 169 :: Batch 0/1
Batch Loss = 1.4117102568097069
169, epoch_train_loss=1.4117102568097069
Epoch 170
Epoch 170 :: Batch 0/1
Batch Loss = 1.4547356829321096
170, epoch_train_loss=1.4547356829321096
Epoch 171
Epoch 171 :: Batch 0/1
Batch Loss = 1.4519888192862687
171, epoch_train_loss=1.4519888192862687
Epoch 172
Epoch 172 :: Batch 0/1
Batch Loss = 1.4089915994478623
172, epoch_train_loss=1.4089915994478623
Epoch 173
Epoch 173 :: Batch 0/1
Batch Loss = 1.3825903534129789
173, epoch_train_loss=1.3825903534129789
Epoch 174
Epoch 174 :: Batch 0/1
Batch Loss = 1.3982764257167883
174, epoch_train_loss=1.3982764257167883
Epoch 175
Epoch 175 :: Batch 0/1
Batch Loss = 1.4227843948411105
175, epoch_train_loss=1.4227843948411105
Epoch 176
Epoch 176 :: Batch 0/1
Batch Loss = 1.4178187642148485
176, epoch_train_loss=1.4178187642148485
Epoch 177
Epoch 177 :: Batch 0/1
Batch Loss = 1.3907762760574252
177, epoch_train_loss=1.3907762760574252
Epoch 178
Epoch 178 :: Batch 0/1
Batch Loss = 1.3768313413234006
178, epoch_train_loss=1.3768313413234006
Epoch 179
Epoch 179 :: Batch 0/1
Batch Loss = 1.3878690058285734
179, epoch_train_loss=1.3878690058285734
Epoch 180
Epoch 180 :: Batch 0/1
Batch Loss = 1.4010595215782886
180, epoch_train_loss=1.4010595215782886
Epoch 181
Epoch 181 :: Batch 0/1
Batch Loss = 1.3951324044337696
181, epoch_train_loss=1.3951324044337696
Epoch 182
Epoch 182 :: Batch 0/1
Batch Loss = 1.3781365773396685
182, epoch_train_loss=1.3781365773396685
Epoch 183
Epoch 183 :: Batch 0/1
Batch Loss = 1.3714854890071388
183, epoch_train_loss=1.3714854890071388
Epoch 184
Epoch 184 :: Batch 0/1
Batch Loss = 1.3791301585497533
184, epoch_train_loss=1.3791301585497533
Epoch 185
Epoch 185 :: Batch 0/1
Batch Loss = 1.3854445551320553
185, epoch_train_loss=1.3854445551320553
Epoch 186
Epoch 186 :: Batch 0/1
Batch Loss = 1.379572353075692
186, epoch_train_loss=1.379572353075692
Epoch 187
Epoch 187 :: Batch 0/1
Batch Loss = 1.3690634338629974
187, epoch_train_loss=1.3690634338629974
Epoch 188
Epoch 188 :: Batch 0/1
Batch Loss = 1.3664192256643364
188, epoch_train_loss=1.3664192256643364
Epoch 189
Epoch 189 :: Batch 0/1
Batch Loss = 1.3713926156125333
189, epoch_train_loss=1.3713926156125333
Epoch 190
Epoch 190 :: Batch 0/1
Batch Loss = 1.373614527133829
190, epoch_train_loss=1.373614527133829
Epoch 191
Epoch 191 :: Batch 0/1
Batch Loss = 1.3684137900709066
191, epoch_train_loss=1.3684137900709066
Epoch 192
Epoch 192 :: Batch 0/1
Batch Loss = 1.36210047324817
192, epoch_train_loss=1.36210047324817
Epoch 193
Epoch 193 :: Batch 0/1
Batch Loss = 1.3614095703069777
193, epoch_train_loss=1.3614095703069777
Epoch 194
Epoch 194 :: Batch 0/1
Batch Loss = 1.3642559600677064
194, epoch_train_loss=1.3642559600677064
Epoch 195
Epoch 195 :: Batch 0/1
Batch Loss = 1.3641675924023395
195, epoch_train_loss=1.3641675924023395
Epoch 196
Epoch 196 :: Batch 0/1
Batch Loss = 1.3599203715945083
196, epoch_train_loss=1.3599203715945083
Epoch 197
Epoch 197 :: Batch 0/1
Batch Loss = 1.3562597511765568
197, epoch_train_loss=1.3562597511765568
Epoch 198
Epoch 198 :: Batch 0/1
Batch Loss = 1.3562965869814845
198, epoch_train_loss=1.3562965869814845
Epoch 199
Epoch 199 :: Batch 0/1
Batch Loss = 1.357515701618448
199, epoch_train_loss=1.357515701618448
Epoch 200
Epoch 200 :: Batch 0/1
Batch Loss = 1.3562629460181301
200, epoch_train_loss=1.3562629460181301
Epoch 201
Epoch 201 :: Batch 0/1
Batch Loss = 1.3530036102189391
201, epoch_train_loss=1.3530036102189391
Epoch 202
Epoch 202 :: Batch 0/1
Batch Loss = 1.350926773424137
202, epoch_train_loss=1.350926773424137
Epoch 203
Epoch 203 :: Batch 0/1
Batch Loss = 1.3510155225167564
203, epoch_train_loss=1.3510155225167564
Epoch 204
Epoch 204 :: Batch 0/1
Batch Loss = 1.3510779374597082
204, epoch_train_loss=1.3510779374597082
Epoch 205
Epoch 205 :: Batch 0/1
Batch Loss = 1.349385817437582
205, epoch_train_loss=1.349385817437582
Epoch 206
Epoch 206 :: Batch 0/1
Batch Loss = 1.3470069175420594
206, epoch_train_loss=1.3470069175420594
Epoch 207
Epoch 207 :: Batch 0/1
Batch Loss = 1.3457976464933326
207, epoch_train_loss=1.3457976464933326
Epoch 208
Epoch 208 :: Batch 0/1
Batch Loss = 1.3456469884655629
208, epoch_train_loss=1.3456469884655629
Epoch 209
Epoch 209 :: Batch 0/1
Batch Loss = 1.3449858816130018
209, epoch_train_loss=1.3449858816130018
Epoch 210
Epoch 210 :: Batch 0/1
Batch Loss = 1.3432663966887974
210, epoch_train_loss=1.3432663966887974
Epoch 211
Epoch 211 :: Batch 0/1
Batch Loss = 1.3415510412441836
211, epoch_train_loss=1.3415510412441836
Epoch 212
Epoch 212 :: Batch 0/1
Batch Loss = 1.340711146466684
212, epoch_train_loss=1.340711146466684
Epoch 213
Epoch 213 :: Batch 0/1
Batch Loss = 1.3402212037208225
213, epoch_train_loss=1.3402212037208225
Epoch 214
Epoch 214 :: Batch 0/1
Batch Loss = 1.3391610690261955
214, epoch_train_loss=1.3391610690261955
Epoch 215
Epoch 215 :: Batch 0/1
Batch Loss = 1.3376015746949221
215, epoch_train_loss=1.3376015746949221
Epoch 216
Epoch 216 :: Batch 0/1
Batch Loss = 1.336322093982238
216, epoch_train_loss=1.336322093982238
Epoch 217
Epoch 217 :: Batch 0/1
Batch Loss = 1.3355624492353861
217, epoch_train_loss=1.3355624492353861
Epoch 218
Epoch 218 :: Batch 0/1
Batch Loss = 1.334775522939295
218, epoch_train_loss=1.334775522939295
Epoch 219
Epoch 219 :: Batch 0/1
Batch Loss = 1.333558539711087
219, epoch_train_loss=1.333558539711087
Epoch 220
Epoch 220 :: Batch 0/1
Batch Loss = 1.332208099002233
220, epoch_train_loss=1.332208099002233
Epoch 221
Epoch 221 :: Batch 0/1
Batch Loss = 1.3311629403692014
221, epoch_train_loss=1.3311629403692014
Epoch 222
Epoch 222 :: Batch 0/1
Batch Loss = 1.3303410521788943
222, epoch_train_loss=1.3303410521788943
Epoch 223
Epoch 223 :: Batch 0/1
Batch Loss = 1.3293533538813094
223, epoch_train_loss=1.3293533538813094
Epoch 224
Epoch 224 :: Batch 0/1
Batch Loss = 1.3281261815207777
224, epoch_train_loss=1.3281261815207777
Epoch 225
Epoch 225 :: Batch 0/1
Batch Loss = 1.3269510186110696
225, epoch_train_loss=1.3269510186110696
Epoch 226
Epoch 226 :: Batch 0/1
Batch Loss = 1.3259910868098534
226, epoch_train_loss=1.3259910868098534
Epoch 227
Epoch 227 :: Batch 0/1
Batch Loss = 1.3250650759512541
227, epoch_train_loss=1.3250650759512541
Epoch 228
Epoch 228 :: Batch 0/1
Batch Loss = 1.3239761175084583
228, epoch_train_loss=1.3239761175084583
Epoch 229
Epoch 229 :: Batch 0/1
Batch Loss = 1.322804268668439
229, epoch_train_loss=1.322804268668439
Epoch 230
Epoch 230 :: Batch 0/1
Batch Loss = 1.321737627927012
230, epoch_train_loss=1.321737627927012
Epoch 231
Epoch 231 :: Batch 0/1
Batch Loss = 1.3207742617169538
231, epoch_train_loss=1.3207742617169538
Epoch 232
Epoch 232 :: Batch 0/1
Batch Loss = 1.3197595404068811
232, epoch_train_loss=1.3197595404068811
Epoch 233
Epoch 233 :: Batch 0/1
Batch Loss = 1.318643410634913
233, epoch_train_loss=1.318643410634913
Epoch 234
Epoch 234 :: Batch 0/1
Batch Loss = 1.317535594834125
234, epoch_train_loss=1.317535594834125
Epoch 235
Epoch 235 :: Batch 0/1
Batch Loss = 1.3165126586683207
235, epoch_train_loss=1.3165126586683207
Epoch 236
Epoch 236 :: Batch 0/1
Batch Loss = 1.3155095140446005
236, epoch_train_loss=1.3155095140446005
Epoch 237
Epoch 237 :: Batch 0/1
Batch Loss = 1.3144425710014604
237, epoch_train_loss=1.3144425710014604
Epoch 238
Epoch 238 :: Batch 0/1
Batch Loss = 1.3133383617918877
238, epoch_train_loss=1.3133383617918877
Epoch 239
Epoch 239 :: Batch 0/1
Batch Loss = 1.3122734145253845
239, epoch_train_loss=1.3122734145253845
Epoch 240
Epoch 240 :: Batch 0/1
Batch Loss = 1.3112497402557641
240, epoch_train_loss=1.3112497402557641
Epoch 241
Epoch 241 :: Batch 0/1
Batch Loss = 1.3102052907866524
241, epoch_train_loss=1.3102052907866524
Epoch 242
Epoch 242 :: Batch 0/1
Batch Loss = 1.3091193554144802
242, epoch_train_loss=1.3091193554144802
Epoch 243
Epoch 243 :: Batch 0/1
Batch Loss = 1.308036248706804
243, epoch_train_loss=1.308036248706804
Epoch 244
Epoch 244 :: Batch 0/1
Batch Loss = 1.3069860215443623
244, epoch_train_loss=1.3069860215443623
Epoch 245
Epoch 245 :: Batch 0/1
Batch Loss = 1.3059412364330445
245, epoch_train_loss=1.3059412364330445
Epoch 246
Epoch 246 :: Batch 0/1
Batch Loss = 1.3048691798057521
246, epoch_train_loss=1.3048691798057521
Epoch 247
Epoch 247 :: Batch 0/1
Batch Loss = 1.3037826897051925
247, epoch_train_loss=1.3037826897051925
Epoch 248
Epoch 248 :: Batch 0/1
Batch Loss = 1.3027121409290856
248, epoch_train_loss=1.3027121409290856
Epoch 249
Epoch 249 :: Batch 0/1
Batch Loss = 1.301655678385652
249, epoch_train_loss=1.301655678385652
Epoch 250
Epoch 250 :: Batch 0/1
Batch Loss = 1.3005879856500495
250, epoch_train_loss=1.3005879856500495
Epoch 251
Epoch 251 :: Batch 0/1
Batch Loss = 1.2995035671334978
251, epoch_train_loss=1.2995035671334978
Epoch 252
Epoch 252 :: Batch 0/1
Batch Loss = 1.2984215268627153
252, epoch_train_loss=1.2984215268627153
Epoch 253
Epoch 253 :: Batch 0/1
Batch Loss = 1.2973514770983137
253, epoch_train_loss=1.2973514770983137
Epoch 254
Epoch 254 :: Batch 0/1
Batch Loss = 1.2962805186250255
254, epoch_train_loss=1.2962805186250255
Epoch 255
Epoch 255 :: Batch 0/1
Batch Loss = 1.295197482971929
255, epoch_train_loss=1.295197482971929
Epoch 256
Epoch 256 :: Batch 0/1
Batch Loss = 1.2941101397534491
256, epoch_train_loss=1.2941101397534491
Epoch 257
Epoch 257 :: Batch 0/1
Batch Loss = 1.2930298477336475
257, epoch_train_loss=1.2930298477336475
Epoch 258
Epoch 258 :: Batch 0/1
Batch Loss = 1.2919532607779274
258, epoch_train_loss=1.2919532607779274
Epoch 259
Epoch 259 :: Batch 0/1
Batch Loss = 1.290870215851997
259, epoch_train_loss=1.290870215851997
Epoch 260
Epoch 260 :: Batch 0/1
Batch Loss = 1.2897807885049164
260, epoch_train_loss=1.2897807885049164
Epoch 261
Epoch 261 :: Batch 0/1
Batch Loss = 1.2886931498895997
261, epoch_train_loss=1.2886931498895997
Epoch 262
Epoch 262 :: Batch 0/1
Batch Loss = 1.287609077279703
262, epoch_train_loss=1.287609077279703
Epoch 263
Epoch 263 :: Batch 0/1
Batch Loss = 1.2865221656068868
263, epoch_train_loss=1.2865221656068868
Epoch 264
Epoch 264 :: Batch 0/1
Batch Loss = 1.285429317125055
264, epoch_train_loss=1.285429317125055
Epoch 265
Epoch 265 :: Batch 0/1
Batch Loss = 1.2843424972450155
265, epoch_train_loss=1.2843424972450155
Epoch 266
Epoch 266 :: Batch 0/1
Batch Loss = 1.283247999461231
266, epoch_train_loss=1.283247999461231
Epoch 267
Epoch 267 :: Batch 0/1
Batch Loss = 1.2821535983794778
267, epoch_train_loss=1.2821535983794778
Epoch 268
Epoch 268 :: Batch 0/1
Batch Loss = 1.2810616197957239
268, epoch_train_loss=1.2810616197957239
Epoch 269
Epoch 269 :: Batch 0/1
Batch Loss = 1.279968512181322
269, epoch_train_loss=1.279968512181322
Epoch 270
Epoch 270 :: Batch 0/1
Batch Loss = 1.2788768319291113
270, epoch_train_loss=1.2788768319291113
Epoch 271
Epoch 271 :: Batch 0/1
Batch Loss = 1.2777853776419115
271, epoch_train_loss=1.2777853776419115
Epoch 272
Epoch 272 :: Batch 0/1
Batch Loss = 1.2766907525226545
272, epoch_train_loss=1.2766907525226545
Epoch 273
Epoch 273 :: Batch 0/1
Batch Loss = 1.2755923144847605
273, epoch_train_loss=1.2755923144847605
Epoch 274
Epoch 274 :: Batch 0/1
Batch Loss = 1.2744921714609343
274, epoch_train_loss=1.2744921714609343
Epoch 275
Epoch 275 :: Batch 0/1
Batch Loss = 1.2733972547011856
275, epoch_train_loss=1.2733972547011856
Epoch 276
Epoch 276 :: Batch 0/1
Batch Loss = 1.2723064512032602
276, epoch_train_loss=1.2723064512032602
Epoch 277
Epoch 277 :: Batch 0/1
Batch Loss = 1.2712096466213398
277, epoch_train_loss=1.2712096466213398
Epoch 278
Epoch 278 :: Batch 0/1
Batch Loss = 1.2701222414717597
278, epoch_train_loss=1.2701222414717597
Epoch 279
Epoch 279 :: Batch 0/1
Batch Loss = 1.2690328863079046
279, epoch_train_loss=1.2690328863079046
Epoch 280
Epoch 280 :: Batch 0/1
Batch Loss = 1.2679398321960982
280, epoch_train_loss=1.2679398321960982
Epoch 281
Epoch 281 :: Batch 0/1
Batch Loss = 1.2668482584543026
281, epoch_train_loss=1.2668482584543026
Epoch 282
Epoch 282 :: Batch 0/1
Batch Loss = 1.2657661580773212
282, epoch_train_loss=1.2657661580773212
Epoch 283
Epoch 283 :: Batch 0/1
Batch Loss = 1.2646797024810423
283, epoch_train_loss=1.2646797024810423
Epoch 284
Epoch 284 :: Batch 0/1
Batch Loss = 1.2636018909180338
284, epoch_train_loss=1.2636018909180338
Epoch 285
Epoch 285 :: Batch 0/1
Batch Loss = 1.2625160678180314
285, epoch_train_loss=1.2625160678180314
Epoch 286
Epoch 286 :: Batch 0/1
Batch Loss = 1.2614320869315343
286, epoch_train_loss=1.2614320869315343
Epoch 287
Epoch 287 :: Batch 0/1
Batch Loss = 1.2603534939802297
287, epoch_train_loss=1.2603534939802297
Epoch 288
Epoch 288 :: Batch 0/1
Batch Loss = 1.2592798366877065
288, epoch_train_loss=1.2592798366877065
Epoch 289
Epoch 289 :: Batch 0/1
Batch Loss = 1.2582058254448751
289, epoch_train_loss=1.2582058254448751
Epoch 290
Epoch 290 :: Batch 0/1
Batch Loss = 1.2571261674251737
290, epoch_train_loss=1.2571261674251737
Epoch 291
Epoch 291 :: Batch 0/1
Batch Loss = 1.256060576680393
291, epoch_train_loss=1.256060576680393
Epoch 292
Epoch 292 :: Batch 0/1
Batch Loss = 1.2549903148700188
292, epoch_train_loss=1.2549903148700188
Epoch 293
Epoch 293 :: Batch 0/1
Batch Loss = 1.2539238106483894
293, epoch_train_loss=1.2539238106483894
Epoch 294
Epoch 294 :: Batch 0/1
Batch Loss = 1.2528529932470187
294, epoch_train_loss=1.2528529932470187
Epoch 295
Epoch 295 :: Batch 0/1
Batch Loss = 1.2517880491412106
295, epoch_train_loss=1.2517880491412106
Epoch 296
Epoch 296 :: Batch 0/1
Batch Loss = 1.2507302021416329
296, epoch_train_loss=1.2507302021416329
Epoch 297
Epoch 297 :: Batch 0/1
Batch Loss = 1.249665479679289
297, epoch_train_loss=1.249665479679289
Epoch 298
Epoch 298 :: Batch 0/1
Batch Loss = 1.2486158176464206
298, epoch_train_loss=1.2486158176464206
Epoch 299
Epoch 299 :: Batch 0/1
Batch Loss = 1.2475523832596946
299, epoch_train_loss=1.2475523832596946
Epoch 300
Epoch 300 :: Batch 0/1
Batch Loss = 1.246511536999383
300, epoch_train_loss=1.246511536999383
Epoch 301
Epoch 301 :: Batch 0/1
Batch Loss = 1.2454582380577526
301, epoch_train_loss=1.2454582380577526
Epoch 302
Epoch 302 :: Batch 0/1
Batch Loss = 1.2443934537198071
302, epoch_train_loss=1.2443934537198071
Epoch 303
Epoch 303 :: Batch 0/1
Batch Loss = 1.2433462573336613
303, epoch_train_loss=1.2433462573336613
Epoch 304
Epoch 304 :: Batch 0/1
Batch Loss = 1.242305722725185
304, epoch_train_loss=1.242305722725185
Epoch 305
Epoch 305 :: Batch 0/1
Batch Loss = 1.2412542364788746
305, epoch_train_loss=1.2412542364788746
Epoch 306
Epoch 306 :: Batch 0/1
Batch Loss = 1.2402085870827346
306, epoch_train_loss=1.2402085870827346
Epoch 307
Epoch 307 :: Batch 0/1
Batch Loss = 1.2391648403744369
307, epoch_train_loss=1.2391648403744369
Epoch 308
Epoch 308 :: Batch 0/1
Batch Loss = 1.2380508453564543
308, epoch_train_loss=1.2380508453564543
Epoch 309
Epoch 309 :: Batch 0/1
Batch Loss = 1.2370848095063096
309, epoch_train_loss=1.2370848095063096
Epoch 310
Epoch 310 :: Batch 0/1
Batch Loss = 1.236171614514512
310, epoch_train_loss=1.236171614514512
Epoch 311
Epoch 311 :: Batch 0/1
Batch Loss = 1.2352157208459709
311, epoch_train_loss=1.2352157208459709
Epoch 312
Epoch 312 :: Batch 0/1
Batch Loss = 1.2341996757831126
312, epoch_train_loss=1.2341996757831126
Epoch 313
Epoch 313 :: Batch 0/1
Batch Loss = 1.2331245268799336
313, epoch_train_loss=1.2331245268799336
Epoch 314
Epoch 314 :: Batch 0/1
Batch Loss = 1.2319980695450792
314, epoch_train_loss=1.2319980695450792
Epoch 315
Epoch 315 :: Batch 0/1
Batch Loss = 1.2307671274251017
315, epoch_train_loss=1.2307671274251017
Epoch 316
Epoch 316 :: Batch 0/1
Batch Loss = 1.2301175969824412
316, epoch_train_loss=1.2301175969824412
Epoch 317
Epoch 317 :: Batch 0/1
Batch Loss = 1.2288120073888644
317, epoch_train_loss=1.2288120073888644
Epoch 318
Epoch 318 :: Batch 0/1
Batch Loss = 1.2278906321995802
318, epoch_train_loss=1.2278906321995802
Epoch 319
Epoch 319 :: Batch 0/1
Batch Loss = 1.2270288080980043
319, epoch_train_loss=1.2270288080980043
Epoch 320
Epoch 320 :: Batch 0/1
Batch Loss = 1.226093130529501
320, epoch_train_loss=1.226093130529501
Epoch 321
Epoch 321 :: Batch 0/1
Batch Loss = 1.2250778785133467
321, epoch_train_loss=1.2250778785133467
Epoch 322
Epoch 322 :: Batch 0/1
Batch Loss = 1.2239889997807087
322, epoch_train_loss=1.2239889997807087
Epoch 323
Epoch 323 :: Batch 0/1
Batch Loss = 1.2228415303867246
323, epoch_train_loss=1.2228415303867246
Epoch 324
Epoch 324 :: Batch 0/1
Batch Loss = 1.2216543983981267
324, epoch_train_loss=1.2216543983981267
Epoch 325
Epoch 325 :: Batch 0/1
Batch Loss = 1.2208631518465383
325, epoch_train_loss=1.2208631518465383
Epoch 326
Epoch 326 :: Batch 0/1
Batch Loss = 1.2196327589718168
326, epoch_train_loss=1.2196327589718168
Epoch 327
Epoch 327 :: Batch 0/1
Batch Loss = 1.2187953168167942
327, epoch_train_loss=1.2187953168167942
Epoch 328
Epoch 328 :: Batch 0/1
Batch Loss = 1.2178797924761942
328, epoch_train_loss=1.2178797924761942
Epoch 329
Epoch 329 :: Batch 0/1
Batch Loss = 1.2168998991951168
329, epoch_train_loss=1.2168998991951168
Epoch 330
Epoch 330 :: Batch 0/1
Batch Loss = 1.215838085174371
330, epoch_train_loss=1.215838085174371
Epoch 331
Epoch 331 :: Batch 0/1
Batch Loss = 1.2146420019632667
331, epoch_train_loss=1.2146420019632667
Epoch 332
Epoch 332 :: Batch 0/1
Batch Loss = 1.2139666572243824
332, epoch_train_loss=1.2139666572243824
Epoch 333
Epoch 333 :: Batch 0/1
Batch Loss = 1.212673855754683
333, epoch_train_loss=1.212673855754683
Epoch 334
Epoch 334 :: Batch 0/1
Batch Loss = 1.2118855918584879
334, epoch_train_loss=1.2118855918584879
Epoch 335
Epoch 335 :: Batch 0/1
Batch Loss = 1.2110102740668114
335, epoch_train_loss=1.2110102740668114
Epoch 336
Epoch 336 :: Batch 0/1
Batch Loss = 1.2100265093073415
336, epoch_train_loss=1.2100265093073415
Epoch 337
Epoch 337 :: Batch 0/1
Batch Loss = 1.2089402017220425
337, epoch_train_loss=1.2089402017220425
Epoch 338
Epoch 338 :: Batch 0/1
Batch Loss = 1.2077318683533607
338, epoch_train_loss=1.2077318683533607
Epoch 339
Epoch 339 :: Batch 0/1
Batch Loss = 1.2070359431174216
339, epoch_train_loss=1.2070359431174216
Epoch 340
Epoch 340 :: Batch 0/1
Batch Loss = 1.2057785448467275
340, epoch_train_loss=1.2057785448467275
Epoch 341
Epoch 341 :: Batch 0/1
Batch Loss = 1.205007236240273
341, epoch_train_loss=1.205007236240273
Epoch 342
Epoch 342 :: Batch 0/1
Batch Loss = 1.2040958408172808
342, epoch_train_loss=1.2040958408172808
Epoch 343
Epoch 343 :: Batch 0/1
Batch Loss = 1.203075584529179
343, epoch_train_loss=1.203075584529179
Epoch 344
Epoch 344 :: Batch 0/1
Batch Loss = 1.2018990475095295
344, epoch_train_loss=1.2018990475095295
Epoch 345
Epoch 345 :: Batch 0/1
Batch Loss = 1.2012139647637365
345, epoch_train_loss=1.2012139647637365
Epoch 346
Epoch 346 :: Batch 0/1
Batch Loss = 1.1999747648098933
346, epoch_train_loss=1.1999747648098933
Epoch 347
Epoch 347 :: Batch 0/1
Batch Loss = 1.1992264247620492
347, epoch_train_loss=1.1992264247620492
Epoch 348
Epoch 348 :: Batch 0/1
Batch Loss = 1.1983581098911682
348, epoch_train_loss=1.1983581098911682
Epoch 349
Epoch 349 :: Batch 0/1
Batch Loss = 1.1973545067499525
349, epoch_train_loss=1.1973545067499525
Epoch 350
Epoch 350 :: Batch 0/1
Batch Loss = 1.196163843516428
350, epoch_train_loss=1.196163843516428
Epoch 351
Epoch 351 :: Batch 0/1
Batch Loss = 1.1959739148317858
351, epoch_train_loss=1.1959739148317858
Epoch 352
Epoch 352 :: Batch 0/1
Batch Loss = 1.1944325677253618
352, epoch_train_loss=1.1944325677253618
Epoch 353
Epoch 353 :: Batch 0/1
Batch Loss = 1.1936094328698457
353, epoch_train_loss=1.1936094328698457
Epoch 354
Epoch 354 :: Batch 0/1
Batch Loss = 1.1930024513492596
354, epoch_train_loss=1.1930024513492596
Epoch 355
Epoch 355 :: Batch 0/1
Batch Loss = 1.1921788784686045
355, epoch_train_loss=1.1921788784686045
Epoch 356
Epoch 356 :: Batch 0/1
Batch Loss = 1.1911221636782574
356, epoch_train_loss=1.1911221636782574
Epoch 357
Epoch 357 :: Batch 0/1
Batch Loss = 1.1898795411884135
357, epoch_train_loss=1.1898795411884135
Epoch 358
Epoch 358 :: Batch 0/1
Batch Loss = 1.1885209099855132
358, epoch_train_loss=1.1885209099855132
Epoch 359
Epoch 359 :: Batch 0/1
Batch Loss = 1.1882732969857017
359, epoch_train_loss=1.1882732969857017
Epoch 360
Epoch 360 :: Batch 0/1
Batch Loss = 1.186694174811074
360, epoch_train_loss=1.186694174811074
Epoch 361
Epoch 361 :: Batch 0/1
Batch Loss = 1.1860829213110036
361, epoch_train_loss=1.1860829213110036
Epoch 362
Epoch 362 :: Batch 0/1
Batch Loss = 1.185402408610569
362, epoch_train_loss=1.185402408610569
Epoch 363
Epoch 363 :: Batch 0/1
Batch Loss = 1.184504449908914
363, epoch_train_loss=1.184504449908914
Epoch 364
Epoch 364 :: Batch 0/1
Batch Loss = 1.1833947878142717
364, epoch_train_loss=1.1833947878142717
Epoch 365
Epoch 365 :: Batch 0/1
Batch Loss = 1.182186834898088
365, epoch_train_loss=1.182186834898088
Epoch 366
Epoch 366 :: Batch 0/1
Batch Loss = 1.181085185692559
366, epoch_train_loss=1.181085185692559
Epoch 367
Epoch 367 :: Batch 0/1
Batch Loss = 1.1801755474477031
367, epoch_train_loss=1.1801755474477031
Epoch 368
Epoch 368 :: Batch 0/1
Batch Loss = 1.1793894333360464
368, epoch_train_loss=1.1793894333360464
Epoch 369
Epoch 369 :: Batch 0/1
Batch Loss = 1.1783014190509606
369, epoch_train_loss=1.1783014190509606
Epoch 370
Epoch 370 :: Batch 0/1
Batch Loss = 1.177351475128096
370, epoch_train_loss=1.177351475128096
Epoch 371
Epoch 371 :: Batch 0/1
Batch Loss = 1.176600286218592
371, epoch_train_loss=1.176600286218592
Epoch 372
Epoch 372 :: Batch 0/1
Batch Loss = 1.175856516360825
372, epoch_train_loss=1.175856516360825
Epoch 373
Epoch 373 :: Batch 0/1
Batch Loss = 1.1747114694080394
373, epoch_train_loss=1.1747114694080394
Epoch 374
Epoch 374 :: Batch 0/1
Batch Loss = 1.1739050371686706
374, epoch_train_loss=1.1739050371686706
Epoch 375
Epoch 375 :: Batch 0/1
Batch Loss = 1.1731897842667385
375, epoch_train_loss=1.1731897842667385
Epoch 376
Epoch 376 :: Batch 0/1
Batch Loss = 1.1722690721084372
376, epoch_train_loss=1.1722690721084372
Epoch 377
Epoch 377 :: Batch 0/1
Batch Loss = 1.1711560549195774
377, epoch_train_loss=1.1711560549195774
Epoch 378
Epoch 378 :: Batch 0/1
Batch Loss = 1.1699964621651646
378, epoch_train_loss=1.1699964621651646
Epoch 379
Epoch 379 :: Batch 0/1
Batch Loss = 1.1692435526895721
379, epoch_train_loss=1.1692435526895721
Epoch 380
Epoch 380 :: Batch 0/1
Batch Loss = 1.168144013000495
380, epoch_train_loss=1.168144013000495
Epoch 381
Epoch 381 :: Batch 0/1
Batch Loss = 1.1674382365298324
381, epoch_train_loss=1.1674382365298324
Epoch 382
Epoch 382 :: Batch 0/1
Batch Loss = 1.166517906635393
382, epoch_train_loss=1.166517906635393
Epoch 383
Epoch 383 :: Batch 0/1
Batch Loss = 1.1656186594068292
383, epoch_train_loss=1.1656186594068292
Epoch 384
Epoch 384 :: Batch 0/1
Batch Loss = 1.1645196209064461
384, epoch_train_loss=1.1645196209064461
Epoch 385
Epoch 385 :: Batch 0/1
Batch Loss = 1.1656423591262615
385, epoch_train_loss=1.1656423591262615
Epoch 386
Epoch 386 :: Batch 0/1
Batch Loss = 1.1631017146498812
386, epoch_train_loss=1.1631017146498812
Epoch 387
Epoch 387 :: Batch 0/1
Batch Loss = 1.1623196131717248
387, epoch_train_loss=1.1623196131717248
Epoch 388
Epoch 388 :: Batch 0/1
Batch Loss = 1.162106724158691
388, epoch_train_loss=1.162106724158691
Epoch 389
Epoch 389 :: Batch 0/1
Batch Loss = 1.1615267503035056
389, epoch_train_loss=1.1615267503035056
Epoch 390
Epoch 390 :: Batch 0/1
Batch Loss = 1.1605069433109987
390, epoch_train_loss=1.1605069433109987
Epoch 391
Epoch 391 :: Batch 0/1
Batch Loss = 1.159124233576213
391, epoch_train_loss=1.159124233576213
Epoch 392
Epoch 392 :: Batch 0/1
Batch Loss = 1.1575703084574183
392, epoch_train_loss=1.1575703084574183
Epoch 393
Epoch 393 :: Batch 0/1
Batch Loss = 1.1568588123352714
393, epoch_train_loss=1.1568588123352714
Epoch 394
Epoch 394 :: Batch 0/1
Batch Loss = 1.1561485818178532
394, epoch_train_loss=1.1561485818178532
Epoch 395
Epoch 395 :: Batch 0/1
Batch Loss = 1.1596800936054836
395, epoch_train_loss=1.1596800936054836
Epoch 396
Epoch 396 :: Batch 0/1
Batch Loss = 1.1543034070465605
396, epoch_train_loss=1.1543034070465605
Epoch 397
Epoch 397 :: Batch 0/1
Batch Loss = 1.1536449743266919
397, epoch_train_loss=1.1536449743266919
Epoch 398
Epoch 398 :: Batch 0/1
Batch Loss = 1.1527025661970023
398, epoch_train_loss=1.1527025661970023
Epoch 399
Epoch 399 :: Batch 0/1
Batch Loss = 1.151531635561158
399, epoch_train_loss=1.151531635561158
Epoch 400
Epoch 400 :: Batch 0/1
Batch Loss = 1.1502035290846517
400, epoch_train_loss=1.1502035290846517
Epoch 401
Epoch 401 :: Batch 0/1
Batch Loss = 1.15029824973057
401, epoch_train_loss=1.15029824973057
Epoch 402
Epoch 402 :: Batch 0/1
Batch Loss = 1.148543760900994
402, epoch_train_loss=1.148543760900994
Epoch 403
Epoch 403 :: Batch 0/1
Batch Loss = 1.1479843078641399
403, epoch_train_loss=1.1479843078641399
Epoch 404
Epoch 404 :: Batch 0/1
Batch Loss = 1.1474510178700614
404, epoch_train_loss=1.1474510178700614
Epoch 405
Epoch 405 :: Batch 0/1
Batch Loss = 1.1466539327346532
405, epoch_train_loss=1.1466539327346532
Epoch 406
Epoch 406 :: Batch 0/1
Batch Loss = 1.1455905834985736
406, epoch_train_loss=1.1455905834985736
Epoch 407
Epoch 407 :: Batch 0/1
Batch Loss = 1.1443389783426343
407, epoch_train_loss=1.1443389783426343
Epoch 408
Epoch 408 :: Batch 0/1
Batch Loss = 1.1433222707787534
408, epoch_train_loss=1.1433222707787534
Epoch 409
Epoch 409 :: Batch 0/1
Batch Loss = 1.142668896565482
409, epoch_train_loss=1.142668896565482
Epoch 410
Epoch 410 :: Batch 0/1
Batch Loss = 1.1414859697347077
410, epoch_train_loss=1.1414859697347077
Epoch 411
Epoch 411 :: Batch 0/1
Batch Loss = 1.142320641900165
411, epoch_train_loss=1.142320641900165
Epoch 412
Epoch 412 :: Batch 0/1
Batch Loss = 1.140035179270052
412, epoch_train_loss=1.140035179270052
Epoch 413
Epoch 413 :: Batch 0/1
Batch Loss = 1.1392152188779203
413, epoch_train_loss=1.1392152188779203
Epoch 414
Epoch 414 :: Batch 0/1
Batch Loss = 1.138950534562507
414, epoch_train_loss=1.138950534562507
Epoch 415
Epoch 415 :: Batch 0/1
Batch Loss = 1.1383611981464183
415, epoch_train_loss=1.1383611981464183
Epoch 416
Epoch 416 :: Batch 0/1
Batch Loss = 1.1373620522097818
416, epoch_train_loss=1.1373620522097818
Epoch 417
Epoch 417 :: Batch 0/1
Batch Loss = 1.136037400266293
417, epoch_train_loss=1.136037400266293
Epoch 418
Epoch 418 :: Batch 0/1
Batch Loss = 1.134568979509274
418, epoch_train_loss=1.134568979509274
Epoch 419
Epoch 419 :: Batch 0/1
Batch Loss = 1.1339409427321225
419, epoch_train_loss=1.1339409427321225
Epoch 420
Epoch 420 :: Batch 0/1
Batch Loss = 1.1331077826944247
420, epoch_train_loss=1.1331077826944247
Epoch 421
Epoch 421 :: Batch 0/1
Batch Loss = 1.1318417135740617
421, epoch_train_loss=1.1318417135740617
Epoch 422
Epoch 422 :: Batch 0/1
Batch Loss = 1.1313343464044592
422, epoch_train_loss=1.1313343464044592
Epoch 423
Epoch 423 :: Batch 0/1
Batch Loss = 1.1306119698806392
423, epoch_train_loss=1.1306119698806392
Epoch 424
Epoch 424 :: Batch 0/1
Batch Loss = 1.1296138066135906
424, epoch_train_loss=1.1296138066135906
Epoch 425
Epoch 425 :: Batch 0/1
Batch Loss = 1.128406738462711
425, epoch_train_loss=1.128406738462711
Epoch 426
Epoch 426 :: Batch 0/1
Batch Loss = 1.1272193823142407
426, epoch_train_loss=1.1272193823142407
Epoch 427
Epoch 427 :: Batch 0/1
Batch Loss = 1.1267195918428379
427, epoch_train_loss=1.1267195918428379
Epoch 428
Epoch 428 :: Batch 0/1
Batch Loss = 1.1254023571752723
428, epoch_train_loss=1.1254023571752723
Epoch 429
Epoch 429 :: Batch 0/1
Batch Loss = 1.1247616579252244
429, epoch_train_loss=1.1247616579252244
Epoch 430
Epoch 430 :: Batch 0/1
Batch Loss = 1.1239364563532626
430, epoch_train_loss=1.1239364563532626
Epoch 431
Epoch 431 :: Batch 0/1
Batch Loss = 1.1229716781845882
431, epoch_train_loss=1.1229716781845882
Epoch 432
Epoch 432 :: Batch 0/1
Batch Loss = 1.1218546279046346
432, epoch_train_loss=1.1218546279046346
Epoch 433
Epoch 433 :: Batch 0/1
Batch Loss = 1.1212119686604654
433, epoch_train_loss=1.1212119686604654
Epoch 434
Epoch 434 :: Batch 0/1
Batch Loss = 1.1200890788735618
434, epoch_train_loss=1.1200890788735618
Epoch 435
Epoch 435 :: Batch 0/1
Batch Loss = 1.1194258865672335
435, epoch_train_loss=1.1194258865672335
Epoch 436
Epoch 436 :: Batch 0/1
Batch Loss = 1.1186417091069614
436, epoch_train_loss=1.1186417091069614
Epoch 437
Epoch 437 :: Batch 0/1
Batch Loss = 1.1176997236684063
437, epoch_train_loss=1.1176997236684063
Epoch 438
Epoch 438 :: Batch 0/1
Batch Loss = 1.116562683117565
438, epoch_train_loss=1.116562683117565
Epoch 439
Epoch 439 :: Batch 0/1
Batch Loss = 1.1160873625595822
439, epoch_train_loss=1.1160873625595822
Epoch 440
Epoch 440 :: Batch 0/1
Batch Loss = 1.1148535504157686
440, epoch_train_loss=1.1148535504157686
Epoch 441
Epoch 441 :: Batch 0/1
Batch Loss = 1.1141786500072337
441, epoch_train_loss=1.1141786500072337
Epoch 442
Epoch 442 :: Batch 0/1
Batch Loss = 1.1135034299689384
442, epoch_train_loss=1.1135034299689384
Epoch 443
Epoch 443 :: Batch 0/1
Batch Loss = 1.1125832137664642
443, epoch_train_loss=1.1125832137664642
Epoch 444
Epoch 444 :: Batch 0/1
Batch Loss = 1.1114760567421638
444, epoch_train_loss=1.1114760567421638
Epoch 445
Epoch 445 :: Batch 0/1
Batch Loss = 1.1104009693947559
445, epoch_train_loss=1.1104009693947559
Epoch 446
Epoch 446 :: Batch 0/1
Batch Loss = 1.1096189269736119
446, epoch_train_loss=1.1096189269736119
Epoch 447
Epoch 447 :: Batch 0/1
Batch Loss = 1.1086200002033002
447, epoch_train_loss=1.1086200002033002
Epoch 448
Epoch 448 :: Batch 0/1
Batch Loss = 1.1077696115335771
448, epoch_train_loss=1.1077696115335771
Epoch 449
Epoch 449 :: Batch 0/1
Batch Loss = 1.1081916798994584
449, epoch_train_loss=1.1081916798994584
Epoch 450
Epoch 450 :: Batch 0/1
Batch Loss = 1.1063763833373792
450, epoch_train_loss=1.1063763833373792
Epoch 451
Epoch 451 :: Batch 0/1
Batch Loss = 1.1054884934013016
451, epoch_train_loss=1.1054884934013016
Epoch 452
Epoch 452 :: Batch 0/1
Batch Loss = 1.1051861275583572
452, epoch_train_loss=1.1051861275583572
Epoch 453
Epoch 453 :: Batch 0/1
Batch Loss = 1.1045010026242756
453, epoch_train_loss=1.1045010026242756
Epoch 454
Epoch 454 :: Batch 0/1
Batch Loss = 1.1034061899587053
454, epoch_train_loss=1.1034061899587053
Epoch 455
Epoch 455 :: Batch 0/1
Batch Loss = 1.1020093451208761
455, epoch_train_loss=1.1020093451208761
Epoch 456
Epoch 456 :: Batch 0/1
Batch Loss = 1.1006760314949067
456, epoch_train_loss=1.1006760314949067
Epoch 457
Epoch 457 :: Batch 0/1
Batch Loss = 1.1005307967675453
457, epoch_train_loss=1.1005307967675453
Epoch 458
Epoch 458 :: Batch 0/1
Batch Loss = 1.0988722781806564
458, epoch_train_loss=1.0988722781806564
Epoch 459
Epoch 459 :: Batch 0/1
Batch Loss = 1.0983620469087179
459, epoch_train_loss=1.0983620469087179
Epoch 460
Epoch 460 :: Batch 0/1
Batch Loss = 1.0976777841799792
460, epoch_train_loss=1.0976777841799792
Epoch 461
Epoch 461 :: Batch 0/1
Batch Loss = 1.0967278399456313
461, epoch_train_loss=1.0967278399456313
Epoch 462
Epoch 462 :: Batch 0/1
Batch Loss = 1.0956001914154916
462, epoch_train_loss=1.0956001914154916
Epoch 463
Epoch 463 :: Batch 0/1
Batch Loss = 1.0945296161470215
463, epoch_train_loss=1.0945296161470215
Epoch 464
Epoch 464 :: Batch 0/1
Batch Loss = 1.093804190366974
464, epoch_train_loss=1.093804190366974
Epoch 465
Epoch 465 :: Batch 0/1
Batch Loss = 1.0927176059538084
465, epoch_train_loss=1.0927176059538084
Epoch 466
Epoch 466 :: Batch 0/1
Batch Loss = 1.0920233186892734
466, epoch_train_loss=1.0920233186892734
Epoch 467
Epoch 467 :: Batch 0/1
Batch Loss = 1.0910804096113433
467, epoch_train_loss=1.0910804096113433
Epoch 468
Epoch 468 :: Batch 0/1
Batch Loss = 1.0901402699079925
468, epoch_train_loss=1.0901402699079925
Epoch 469
Epoch 469 :: Batch 0/1
Batch Loss = 1.0893758573021457
469, epoch_train_loss=1.0893758573021457
Epoch 470
Epoch 470 :: Batch 0/1
Batch Loss = 1.088589139292386
470, epoch_train_loss=1.088589139292386
Epoch 471
Epoch 471 :: Batch 0/1
Batch Loss = 1.0877120150186452
471, epoch_train_loss=1.0877120150186452
Epoch 472
Epoch 472 :: Batch 0/1
Batch Loss = 1.086702613637432
472, epoch_train_loss=1.086702613637432
Epoch 473
Epoch 473 :: Batch 0/1
Batch Loss = 1.0855633943815257
473, epoch_train_loss=1.0855633943815257
Epoch 474
Epoch 474 :: Batch 0/1
Batch Loss = 1.0848756451135393
474, epoch_train_loss=1.0848756451135393
Epoch 475
Epoch 475 :: Batch 0/1
Batch Loss = 1.0837867306784714
475, epoch_train_loss=1.0837867306784714
Epoch 476
Epoch 476 :: Batch 0/1
Batch Loss = 1.083139139081162
476, epoch_train_loss=1.083139139081162
Epoch 477
Epoch 477 :: Batch 0/1
Batch Loss = 1.0824288074983717
477, epoch_train_loss=1.0824288074983717
Epoch 478
Epoch 478 :: Batch 0/1
Batch Loss = 1.0815722315272636
478, epoch_train_loss=1.0815722315272636
Epoch 479
Epoch 479 :: Batch 0/1
Batch Loss = 1.0805343315318443
479, epoch_train_loss=1.0805343315318443
Epoch 480
Epoch 480 :: Batch 0/1
Batch Loss = 1.0793301794081018
480, epoch_train_loss=1.0793301794081018
Epoch 481
Epoch 481 :: Batch 0/1
Batch Loss = 1.0788267772031568
481, epoch_train_loss=1.0788267772031568
Epoch 482
Epoch 482 :: Batch 0/1
Batch Loss = 1.0775791934878387
482, epoch_train_loss=1.0775791934878387
Epoch 483
Epoch 483 :: Batch 0/1
Batch Loss = 1.0769531526885092
483, epoch_train_loss=1.0769531526885092
Epoch 484
Epoch 484 :: Batch 0/1
Batch Loss = 1.0762083251154047
484, epoch_train_loss=1.0762083251154047
Epoch 485
Epoch 485 :: Batch 0/1
Batch Loss = 1.075287503573609
485, epoch_train_loss=1.075287503573609
Epoch 486
Epoch 486 :: Batch 0/1
Batch Loss = 1.0742446675803088
486, epoch_train_loss=1.0742446675803088
Epoch 487
Epoch 487 :: Batch 0/1
Batch Loss = 1.0731036459999947
487, epoch_train_loss=1.0731036459999947
Epoch 488
Epoch 488 :: Batch 0/1
Batch Loss = 1.0724148621416696
488, epoch_train_loss=1.0724148621416696
Epoch 489
Epoch 489 :: Batch 0/1
Batch Loss = 1.0713337656481465
489, epoch_train_loss=1.0713337656481465
Epoch 490
Epoch 490 :: Batch 0/1
Batch Loss = 1.070684214185384
490, epoch_train_loss=1.070684214185384
Epoch 491
Epoch 491 :: Batch 0/1
Batch Loss = 1.0699768819721716
491, epoch_train_loss=1.0699768819721716
Epoch 492
Epoch 492 :: Batch 0/1
Batch Loss = 1.0691239702334712
492, epoch_train_loss=1.0691239702334712
Epoch 493
Epoch 493 :: Batch 0/1
Batch Loss = 1.0680913384032802
493, epoch_train_loss=1.0680913384032802
Epoch 494
Epoch 494 :: Batch 0/1
Batch Loss = 1.066871661455661
494, epoch_train_loss=1.066871661455661
Epoch 495
Epoch 495 :: Batch 0/1
Batch Loss = 1.066517121357838
495, epoch_train_loss=1.066517121357838
Epoch 496
Epoch 496 :: Batch 0/1
Batch Loss = 1.0651450568824137
496, epoch_train_loss=1.0651450568824137
Epoch 497
Epoch 497 :: Batch 0/1
Batch Loss = 1.064490563247323
497, epoch_train_loss=1.064490563247323
Epoch 498
Epoch 498 :: Batch 0/1
Batch Loss = 1.063814036575445
498, epoch_train_loss=1.063814036575445
Epoch 499
Epoch 499 :: Batch 0/1
Batch Loss = 1.0629266482448694
499, epoch_train_loss=1.0629266482448694
Epoch 500
Epoch 500 :: Batch 0/1
Batch Loss = 1.0618359065422591
500, epoch_train_loss=1.0618359065422591
Epoch 501
Epoch 501 :: Batch 0/1
Batch Loss = 1.0605733238919408
501, epoch_train_loss=1.0605733238919408
Epoch 502
Epoch 502 :: Batch 0/1
Batch Loss = 1.0603327259043862
502, epoch_train_loss=1.0603327259043862
Epoch 503
Epoch 503 :: Batch 0/1
Batch Loss = 1.0588565682075193
503, epoch_train_loss=1.0588565682075193
Epoch 504
Epoch 504 :: Batch 0/1
Batch Loss = 1.0581604483722802
504, epoch_train_loss=1.0581604483722802
Epoch 505
Epoch 505 :: Batch 0/1
Batch Loss = 1.0575136141832517
505, epoch_train_loss=1.0575136141832517
Epoch 506
Epoch 506 :: Batch 0/1
Batch Loss = 1.0566410583022754
506, epoch_train_loss=1.0566410583022754
Epoch 507
Epoch 507 :: Batch 0/1
Batch Loss = 1.0555192495592733
507, epoch_train_loss=1.0555192495592733
Epoch 508
Epoch 508 :: Batch 0/1
Batch Loss = 1.0543633034468574
508, epoch_train_loss=1.0543633034468574
Epoch 509
Epoch 509 :: Batch 0/1
Batch Loss = 1.0547340758547505
509, epoch_train_loss=1.0547340758547505
Epoch 510
Epoch 510 :: Batch 0/1
Batch Loss = 1.0536825278774538
510, epoch_train_loss=1.0536825278774538
Epoch 511
Epoch 511 :: Batch 0/1
Batch Loss = 1.0515789926278638
511, epoch_train_loss=1.0515789926278638
Epoch 512
Epoch 512 :: Batch 0/1
Batch Loss = 1.0513070528374646
512, epoch_train_loss=1.0513070528374646
Epoch 513
Epoch 513 :: Batch 0/1
Batch Loss = 1.0509063064738577
513, epoch_train_loss=1.0509063064738577
Epoch 514
Epoch 514 :: Batch 0/1
Batch Loss = 1.0500247111949588
514, epoch_train_loss=1.0500247111949588
Epoch 515
Epoch 515 :: Batch 0/1
Batch Loss = 1.0486813155465782
515, epoch_train_loss=1.0486813155465782
Epoch 516
Epoch 516 :: Batch 0/1
Batch Loss = 1.0470122371785897
516, epoch_train_loss=1.0470122371785897
Epoch 517
Epoch 517 :: Batch 0/1
Batch Loss = 1.045489021245843
517, epoch_train_loss=1.045489021245843
Epoch 518
Epoch 518 :: Batch 0/1
Batch Loss = 1.106030393346716
518, epoch_train_loss=1.106030393346716
Epoch 519
Epoch 519 :: Batch 0/1
Batch Loss = 1.057959373934051
519, epoch_train_loss=1.057959373934051
Epoch 520
Epoch 520 :: Batch 0/1
Batch Loss = 1.0734729389551234
520, epoch_train_loss=1.0734729389551234
Epoch 521
Epoch 521 :: Batch 0/1
Batch Loss = 1.0756419360614358
521, epoch_train_loss=1.0756419360614358
Epoch 522
Epoch 522 :: Batch 0/1
Batch Loss = 1364.2115679063697
522, epoch_train_loss=1364.2115679063697
Epoch 523
Epoch 523 :: Batch 0/1
Batch Loss = 663.5371324343167
523, epoch_train_loss=663.5371324343167
Epoch 524
Epoch 524 :: Batch 0/1
Batch Loss = 941.70275499499
524, epoch_train_loss=941.70275499499
Epoch 525
Epoch 525 :: Batch 0/1
Batch Loss = 179.0029099181776
525, epoch_train_loss=179.0029099181776
Epoch 526
Epoch 526 :: Batch 0/1
Batch Loss = 137.46155488574524
526, epoch_train_loss=137.46155488574524
Epoch 527
Epoch 527 :: Batch 0/1
Batch Loss = 673.1114585927137
527, epoch_train_loss=673.1114585927137
Epoch 528
Epoch 528 :: Batch 0/1
Batch Loss = 419.78831609728576
528, epoch_train_loss=419.78831609728576
Epoch 529
Epoch 529 :: Batch 0/1
Batch Loss = 1.2998071410720125
529, epoch_train_loss=1.2998071410720125
Epoch 530
Epoch 530 :: Batch 0/1
Batch Loss = 16574.177546657054
530, epoch_train_loss=16574.177546657054
Epoch 531
Epoch 531 :: Batch 0/1
Batch Loss = 565.4180511502178
531, epoch_train_loss=565.4180511502178
Epoch 532
Epoch 532 :: Batch 0/1
Batch Loss = 33.154997702074795
532, epoch_train_loss=33.154997702074795
Epoch 533
Epoch 533 :: Batch 0/1
Batch Loss = 390.5262767017463
533, epoch_train_loss=390.5262767017463
Epoch 534
Epoch 534 :: Batch 0/1
Batch Loss = 773.7638488563623
534, epoch_train_loss=773.7638488563623
Epoch 535
Epoch 535 :: Batch 0/1
Batch Loss = 175.46826619740267
535, epoch_train_loss=175.46826619740267
Epoch 536
Epoch 536 :: Batch 0/1
Batch Loss = 123.5109717708227
536, epoch_train_loss=123.5109717708227
Epoch 537
Epoch 537 :: Batch 0/1
Batch Loss = 584.462870687039
537, epoch_train_loss=584.462870687039
Epoch 538
Epoch 538 :: Batch 0/1
Batch Loss = 254.37267603149343
538, epoch_train_loss=254.37267603149343
Epoch 539
Epoch 539 :: Batch 0/1
Batch Loss = 29.43157750155803
539, epoch_train_loss=29.43157750155803
Epoch 540
Epoch 540 :: Batch 0/1
Batch Loss = 398.06337097305044
540, epoch_train_loss=398.06337097305044
Epoch 541
Epoch 541 :: Batch 0/1
Batch Loss = 254.54049242966656
541, epoch_train_loss=254.54049242966656
Epoch 542
Epoch 542 :: Batch 0/1
Batch Loss = 5.809940750878399
542, epoch_train_loss=5.809940750878399
Epoch 543
Epoch 543 :: Batch 0/1
Batch Loss = 269.12581610662113
543, epoch_train_loss=269.12581610662113
Epoch 544
Epoch 544 :: Batch 0/1
Batch Loss = 219.62880825886367
544, epoch_train_loss=219.62880825886367
Epoch 545
Epoch 545 :: Batch 0/1
Batch Loss = 2.768129616492254
545, epoch_train_loss=2.768129616492254
Epoch 546
Epoch 546 :: Batch 0/1
Batch Loss = 184.92366644773506
546, epoch_train_loss=184.92366644773506
Epoch 547
Epoch 547 :: Batch 0/1
Batch Loss = 175.05029457142146
547, epoch_train_loss=175.05029457142146
Epoch 548
Epoch 548 :: Batch 0/1
Batch Loss = 3.8390382879414573
548, epoch_train_loss=3.8390382879414573
Epoch 549
Epoch 549 :: Batch 0/1
Batch Loss = 134.69059525927764
549, epoch_train_loss=134.69059525927764
Epoch 550
Epoch 550 :: Batch 0/1
Batch Loss = 139.25964686706843
550, epoch_train_loss=139.25964686706843
Epoch 551
Epoch 551 :: Batch 0/1
Batch Loss = 6.492802163454382
551, epoch_train_loss=6.492802163454382
Epoch 552
Epoch 552 :: Batch 0/1
Batch Loss = 96.36365090674325
552, epoch_train_loss=96.36365090674325
Epoch 553
Epoch 553 :: Batch 0/1
Batch Loss = 98.80169114962416
553, epoch_train_loss=98.80169114962416
Epoch 554
Epoch 554 :: Batch 0/1
Batch Loss = 2.65111285865953
554, epoch_train_loss=2.65111285865953
Epoch 555
Epoch 555 :: Batch 0/1
Batch Loss = 71.55408737496074
555, epoch_train_loss=71.55408737496074
Epoch 556
Epoch 556 :: Batch 0/1
Batch Loss = 71.34635788162018
556, epoch_train_loss=71.34635788162018
Epoch 557
Epoch 557 :: Batch 0/1
Batch Loss = 1.8810385256996076
557, epoch_train_loss=1.8810385256996076
Epoch 558
Epoch 558 :: Batch 0/1
Batch Loss = 54.44277664915118
558, epoch_train_loss=54.44277664915118
Epoch 559
Epoch 559 :: Batch 0/1
Batch Loss = 50.5882554188302
559, epoch_train_loss=50.5882554188302
Epoch 560
Epoch 560 :: Batch 0/1
Batch Loss = 1.483071395994258
560, epoch_train_loss=1.483071395994258
Epoch 561
Epoch 561 :: Batch 0/1
Batch Loss = 42.003261156483276
561, epoch_train_loss=42.003261156483276
Epoch 562
Epoch 562 :: Batch 0/1
Batch Loss = 35.4694253990155
562, epoch_train_loss=35.4694253990155
Epoch 563
Epoch 563 :: Batch 0/1
Batch Loss = 104.74155051676969
563, epoch_train_loss=104.74155051676969
Epoch 564
Epoch 564 :: Batch 0/1
Batch Loss = 32.79895275393549
564, epoch_train_loss=32.79895275393549
Epoch 565
Epoch 565 :: Batch 0/1
Batch Loss = 25.82816905917964
565, epoch_train_loss=25.82816905917964
Epoch 566
Epoch 566 :: Batch 0/1
Batch Loss = 2.057853491067137
566, epoch_train_loss=2.057853491067137
Epoch 567
Epoch 567 :: Batch 0/1
Batch Loss = 25.60097853782198
567, epoch_train_loss=25.60097853782198
Epoch 568
Epoch 568 :: Batch 0/1
Batch Loss = 16.48837356072222
568, epoch_train_loss=16.48837356072222
Epoch 569
Epoch 569 :: Batch 0/1
Batch Loss = 2.3771601950453953
569, epoch_train_loss=2.3771601950453953
Epoch 570
Epoch 570 :: Batch 0/1
Batch Loss = 20.346349930497826
570, epoch_train_loss=20.346349930497826
Epoch 571
Epoch 571 :: Batch 0/1
Batch Loss = 11.116842416415732
571, epoch_train_loss=11.116842416415732
Epoch 572
Epoch 572 :: Batch 0/1
Batch Loss = 2.462307623339425
572, epoch_train_loss=2.462307623339425
Epoch 573
Epoch 573 :: Batch 0/1
Batch Loss = 15.560186124214082
573, epoch_train_loss=15.560186124214082
Epoch 574
Epoch 574 :: Batch 0/1
Batch Loss = 7.1576376065366505
574, epoch_train_loss=7.1576376065366505
Epoch 575
Epoch 575 :: Batch 0/1
Batch Loss = 2.6863545491167558
575, epoch_train_loss=2.6863545491167558
Epoch 576
Epoch 576 :: Batch 0/1
Batch Loss = 12.03013541958767
576, epoch_train_loss=12.03013541958767
Epoch 577
Epoch 577 :: Batch 0/1
Batch Loss = 4.675364794782145
577, epoch_train_loss=4.675364794782145
Epoch 578
Epoch 578 :: Batch 0/1
Batch Loss = 2.8850864594951546
578, epoch_train_loss=2.8850864594951546
Epoch 579
Epoch 579 :: Batch 0/1
Batch Loss = 9.259573472851935
579, epoch_train_loss=9.259573472851935
Epoch 580
Epoch 580 :: Batch 0/1
Batch Loss = 3.094063362268844
580, epoch_train_loss=3.094063362268844
Epoch 581
Epoch 581 :: Batch 0/1
Batch Loss = 2.9833511384151183
581, epoch_train_loss=2.9833511384151183
Epoch 582
Epoch 582 :: Batch 0/1
Batch Loss = 7.085894096389189
582, epoch_train_loss=7.085894096389189
Epoch 583
Epoch 583 :: Batch 0/1
Batch Loss = 2.1373813757436073
583, epoch_train_loss=2.1373813757436073
Epoch 584
Epoch 584 :: Batch 0/1
Batch Loss = 2.979394820858988
584, epoch_train_loss=2.979394820858988
Epoch 585
Epoch 585 :: Batch 0/1
Batch Loss = 5.402385383872196
585, epoch_train_loss=5.402385383872196
Epoch 586
Epoch 586 :: Batch 0/1
Batch Loss = 1.5975942383073718
586, epoch_train_loss=1.5975942383073718
Epoch 587
Epoch 587 :: Batch 0/1
Batch Loss = 2.891451129328561
587, epoch_train_loss=2.891451129328561
Epoch 588
Epoch 588 :: Batch 0/1
Batch Loss = 4.117997925067689
588, epoch_train_loss=4.117997925067689
Epoch 589
Epoch 589 :: Batch 0/1
Batch Loss = 1.3282421607765496
589, epoch_train_loss=1.3282421607765496
Epoch 590
Epoch 590 :: Batch 0/1
Batch Loss = 2.7379770880917853
590, epoch_train_loss=2.7379770880917853
Epoch 591
Epoch 591 :: Batch 0/1
Batch Loss = 3.158409631183786
591, epoch_train_loss=3.158409631183786
Epoch 592
Epoch 592 :: Batch 0/1
Batch Loss = 1.2227089031920808
592, epoch_train_loss=1.2227089031920808
Epoch 593
Epoch 593 :: Batch 0/1
Batch Loss = 2.543183045436252
593, epoch_train_loss=2.543183045436252
Epoch 594
Epoch 594 :: Batch 0/1
Batch Loss = 2.4599121463788287
594, epoch_train_loss=2.4599121463788287
Epoch 595
Epoch 595 :: Batch 0/1
Batch Loss = 1.208739447975982
595, epoch_train_loss=1.208739447975982
Epoch 596
Epoch 596 :: Batch 0/1
Batch Loss = 2.328130684515825
596, epoch_train_loss=2.328130684515825
Epoch 597
Epoch 597 :: Batch 0/1
Batch Loss = 1.9676609918777368
597, epoch_train_loss=1.9676609918777368
Epoch 598
Epoch 598 :: Batch 0/1
Batch Loss = 1.23771498078485
598, epoch_train_loss=1.23771498078485
Epoch 599
Epoch 599 :: Batch 0/1
Batch Loss = 2.111382308042155
599, epoch_train_loss=2.111382308042155
Epoch 600
Epoch 600 :: Batch 0/1
Batch Loss = 1.6347188136433868
600, epoch_train_loss=1.6347188136433868
Epoch 601
Epoch 601 :: Batch 0/1
Batch Loss = 1.278921903465698
601, epoch_train_loss=1.278921903465698
Epoch 602
Epoch 602 :: Batch 0/1
Batch Loss = 1.9069209664384685
602, epoch_train_loss=1.9069209664384685
Epoch 603
Epoch 603 :: Batch 0/1
Batch Loss = 1.4208262298938734
603, epoch_train_loss=1.4208262298938734
Epoch 604
Epoch 604 :: Batch 0/1
Batch Loss = 1.3149171364156047
604, epoch_train_loss=1.3149171364156047
Epoch 605
Epoch 605 :: Batch 0/1
Batch Loss = 1.7244196453886766
605, epoch_train_loss=1.7244196453886766
Epoch 606
Epoch 606 :: Batch 0/1
Batch Loss = 1.2928619289300296
606, epoch_train_loss=1.2928619289300296
Epoch 607
Epoch 607 :: Batch 0/1
Batch Loss = 1.3372531362541622
607, epoch_train_loss=1.3372531362541622
Epoch 608
Epoch 608 :: Batch 0/1
Batch Loss = 1.5695648683419863
608, epoch_train_loss=1.5695648683419863
Epoch 609
Epoch 609 :: Batch 0/1
Batch Loss = 1.2238334876120547
609, epoch_train_loss=1.2238334876120547
Epoch 610
Epoch 610 :: Batch 0/1
Batch Loss = 1.343910247244722
610, epoch_train_loss=1.343910247244722
Epoch 611
Epoch 611 :: Batch 0/1
Batch Loss = 1.4444697360324248
611, epoch_train_loss=1.4444697360324248
Epoch 612
Epoch 612 :: Batch 0/1
Batch Loss = 1.1928079318955536
612, epoch_train_loss=1.1928079318955536
Epoch 613
Epoch 613 :: Batch 0/1
Batch Loss = 1.336473492205424
613, epoch_train_loss=1.336473492205424
Epoch 614
Epoch 614 :: Batch 0/1
Batch Loss = 1.3484414471642565
614, epoch_train_loss=1.3484414471642565
Epoch 615
Epoch 615 :: Batch 0/1
Batch Loss = 1.1840025247334927
615, epoch_train_loss=1.1840025247334927
Epoch 616
Epoch 616 :: Batch 0/1
Batch Loss = 1.3185493225006217
616, epoch_train_loss=1.3185493225006217
Epoch 617
Epoch 617 :: Batch 0/1
Batch Loss = 1.2786991420091782
617, epoch_train_loss=1.2786991420091782
Epoch 618
Epoch 618 :: Batch 0/1
Batch Loss = 1.1862313949025067
618, epoch_train_loss=1.1862313949025067
Epoch 619
Epoch 619 :: Batch 0/1
Batch Loss = 1.2943167512861633
619, epoch_train_loss=1.2943167512861633
Epoch 620
Epoch 620 :: Batch 0/1
Batch Loss = 1.2311607600714543
620, epoch_train_loss=1.2311607600714543
Epoch 621
Epoch 621 :: Batch 0/1
Batch Loss = 1.1921111287652066
621, epoch_train_loss=1.1921111287652066
Epoch 622
Epoch 622 :: Batch 0/1
Batch Loss = 1.2677221800364957
622, epoch_train_loss=1.2677221800364957
Epoch 623
Epoch 623 :: Batch 0/1
Batch Loss = 1.2011755738635808
623, epoch_train_loss=1.2011755738635808
Epoch 624
Epoch 624 :: Batch 0/1
Batch Loss = 1.1972880814738318
624, epoch_train_loss=1.1972880814738318
Epoch 625
Epoch 625 :: Batch 0/1
Batch Loss = 1.2419464391301176
625, epoch_train_loss=1.2419464391301176
Epoch 626
Epoch 626 :: Batch 0/1
Batch Loss = 1.1840806178998127
626, epoch_train_loss=1.1840806178998127
Epoch 627
Epoch 627 :: Batch 0/1
Batch Loss = 1.1996973530133166
627, epoch_train_loss=1.1996973530133166
Epoch 628
Epoch 628 :: Batch 0/1
Batch Loss = 1.2191769267697565
628, epoch_train_loss=1.2191769267697565
Epoch 629
Epoch 629 :: Batch 0/1
Batch Loss = 1.175664993273245
629, epoch_train_loss=1.175664993273245
Epoch 630
Epoch 630 :: Batch 0/1
Batch Loss = 1.198836416653747
630, epoch_train_loss=1.198836416653747
Epoch 631
Epoch 631 :: Batch 0/1
Batch Loss = 1.2005998997088416
631, epoch_train_loss=1.2005998997088416
Epoch 632
Epoch 632 :: Batch 0/1
Batch Loss = 1.172429450089674
632, epoch_train_loss=1.172429450089674
Epoch 633
Epoch 633 :: Batch 0/1
Batch Loss = 1.19516906169496
633, epoch_train_loss=1.19516906169496
Epoch 634
Epoch 634 :: Batch 0/1
Batch Loss = 1.1865306223100853
634, epoch_train_loss=1.1865306223100853
Epoch 635
Epoch 635 :: Batch 0/1
Batch Loss = 1.1717247813013763
635, epoch_train_loss=1.1717247813013763
Epoch 636
Epoch 636 :: Batch 0/1
Batch Loss = 1.1896190479061572
636, epoch_train_loss=1.1896190479061572
Epoch 637
Epoch 637 :: Batch 0/1
Batch Loss = 1.176630481058131
637, epoch_train_loss=1.176630481058131
Epoch 638
Epoch 638 :: Batch 0/1
Batch Loss = 1.1717479572439193
638, epoch_train_loss=1.1717479572439193
Epoch 639
Epoch 639 :: Batch 0/1
Batch Loss = 1.1832070844571454
639, epoch_train_loss=1.1832070844571454
Epoch 640
Epoch 640 :: Batch 0/1
Batch Loss = 1.170153687199472
640, epoch_train_loss=1.170153687199472
Epoch 641
Epoch 641 :: Batch 0/1
Batch Loss = 1.1714458169252358
641, epoch_train_loss=1.1714458169252358
Epoch 642
Epoch 642 :: Batch 0/1
Batch Loss = 1.176810936545193
642, epoch_train_loss=1.176810936545193
Epoch 643
Epoch 643 :: Batch 0/1
Batch Loss = 1.1661766728427319
643, epoch_train_loss=1.1661766728427319
Epoch 644
Epoch 644 :: Batch 0/1
Batch Loss = 1.170360908880463
644, epoch_train_loss=1.170360908880463
Epoch 645
Epoch 645 :: Batch 0/1
Batch Loss = 1.1710426661908222
645, epoch_train_loss=1.1710426661908222
Epoch 646
Epoch 646 :: Batch 0/1
Batch Loss = 1.1637886047524086
646, epoch_train_loss=1.1637886047524086
Epoch 647
Epoch 647 :: Batch 0/1
Batch Loss = 1.168449923497897
647, epoch_train_loss=1.168449923497897
Epoch 648
Epoch 648 :: Batch 0/1
Batch Loss = 1.1662200174739605
648, epoch_train_loss=1.1662200174739605
Epoch 649
Epoch 649 :: Batch 0/1
Batch Loss = 1.1622197666037406
649, epoch_train_loss=1.1622197666037406
Epoch 650
Epoch 650 :: Batch 0/1
Batch Loss = 1.1659106634795833
650, epoch_train_loss=1.1659106634795833
Epoch 651
Epoch 651 :: Batch 0/1
Batch Loss = 1.162402967853665
651, epoch_train_loss=1.162402967853665
Epoch 652
Epoch 652 :: Batch 0/1
Batch Loss = 1.1609092835809829
652, epoch_train_loss=1.1609092835809829
Epoch 653
Epoch 653 :: Batch 0/1
Batch Loss = 1.1630351258656968
653, epoch_train_loss=1.1630351258656968
Epoch 654
Epoch 654 :: Batch 0/1
Batch Loss = 1.159467074676867
654, epoch_train_loss=1.159467074676867
Epoch 655
Epoch 655 :: Batch 0/1
Batch Loss = 1.159517090702495
655, epoch_train_loss=1.159517090702495
Epoch 656
Epoch 656 :: Batch 0/1
Batch Loss = 1.160103702382704
656, epoch_train_loss=1.160103702382704
Epoch 657
Epoch 657 :: Batch 0/1
Batch Loss = 1.1571882219937997
657, epoch_train_loss=1.1571882219937997
Epoch 658
Epoch 658 :: Batch 0/1
Batch Loss = 1.1578947949579608
658, epoch_train_loss=1.1578947949579608
Epoch 659
Epoch 659 :: Batch 0/1
Batch Loss = 1.1573230764621836
659, epoch_train_loss=1.1573230764621836
Epoch 660
Epoch 660 :: Batch 0/1
Batch Loss = 1.1553183323533154
660, epoch_train_loss=1.1553183323533154
Epoch 661
Epoch 661 :: Batch 0/1
Batch Loss = 1.1560332140912675
661, epoch_train_loss=1.1560332140912675
Epoch 662
Epoch 662 :: Batch 0/1
Batch Loss = 1.1548036422006704
662, epoch_train_loss=1.1548036422006704
Epoch 663
Epoch 663 :: Batch 0/1
Batch Loss = 1.1536409297508792
663, epoch_train_loss=1.1536409297508792
Epoch 664
Epoch 664 :: Batch 0/1
Batch Loss = 1.154003204195935
664, epoch_train_loss=1.154003204195935
Epoch 665
Epoch 665 :: Batch 0/1
Batch Loss = 1.1525675000618312
665, epoch_train_loss=1.1525675000618312
Epoch 666
Epoch 666 :: Batch 0/1
Batch Loss = 1.1520021327207377
666, epoch_train_loss=1.1520021327207377
Epoch 667
Epoch 667 :: Batch 0/1
Batch Loss = 1.1519035518177054
667, epoch_train_loss=1.1519035518177054
Epoch 668
Epoch 668 :: Batch 0/1
Batch Loss = 1.1505736870567018
668, epoch_train_loss=1.1505736870567018
Epoch 669
Epoch 669 :: Batch 0/1
Batch Loss = 1.1503165903876016
669, epoch_train_loss=1.1503165903876016
Epoch 670
Epoch 670 :: Batch 0/1
Batch Loss = 1.149821185281249
670, epoch_train_loss=1.149821185281249
Epoch 671
Epoch 671 :: Batch 0/1
Batch Loss = 1.1487479247343655
671, epoch_train_loss=1.1487479247343655
Epoch 672
Epoch 672 :: Batch 0/1
Batch Loss = 1.1485571500175014
672, epoch_train_loss=1.1485571500175014
Epoch 673
Epoch 673 :: Batch 0/1
Batch Loss = 1.1478122444211172
673, epoch_train_loss=1.1478122444211172
Epoch 674
Epoch 674 :: Batch 0/1
Batch Loss = 1.1470128829872301
674, epoch_train_loss=1.1470128829872301
Epoch 675
Epoch 675 :: Batch 0/1
Batch Loss = 1.1467370034479456
675, epoch_train_loss=1.1467370034479456
Epoch 676
Epoch 676 :: Batch 0/1
Batch Loss = 1.14589806892906
676, epoch_train_loss=1.14589806892906
Epoch 677
Epoch 677 :: Batch 0/1
Batch Loss = 1.1453069031348695
677, epoch_train_loss=1.1453069031348695
Epoch 678
Epoch 678 :: Batch 0/1
Batch Loss = 1.144887604952968
678, epoch_train_loss=1.144887604952968
Epoch 679
Epoch 679 :: Batch 0/1
Batch Loss = 1.1440711443651268
679, epoch_train_loss=1.1440711443651268
Epoch 680
Epoch 680 :: Batch 0/1
Batch Loss = 1.1435920753961428
680, epoch_train_loss=1.1435920753961428
Epoch 681
Epoch 681 :: Batch 0/1
Batch Loss = 1.1430408368998257
681, epoch_train_loss=1.1430408368998257
Epoch 682
Epoch 682 :: Batch 0/1
Batch Loss = 1.1423068952269402
682, epoch_train_loss=1.1423068952269402
Epoch 683
Epoch 683 :: Batch 0/1
Batch Loss = 1.1418535053435812
683, epoch_train_loss=1.1418535053435812
Epoch 684
Epoch 684 :: Batch 0/1
Batch Loss = 1.1412183306123191
684, epoch_train_loss=1.1412183306123191
Epoch 685
Epoch 685 :: Batch 0/1
Batch Loss = 1.1405761174761857
685, epoch_train_loss=1.1405761174761857
Epoch 686
Epoch 686 :: Batch 0/1
Batch Loss = 1.1400929911520683
686, epoch_train_loss=1.1400929911520683
Epoch 687
Epoch 687 :: Batch 0/1
Batch Loss = 1.1394280762779816
687, epoch_train_loss=1.1394280762779816
Epoch 688
Epoch 688 :: Batch 0/1
Batch Loss = 1.138854267543668
688, epoch_train_loss=1.138854267543668
Epoch 689
Epoch 689 :: Batch 0/1
Batch Loss = 1.1383206393187375
689, epoch_train_loss=1.1383206393187375
Epoch 690
Epoch 690 :: Batch 0/1
Batch Loss = 1.1376665161804036
690, epoch_train_loss=1.1376665161804036
Epoch 691
Epoch 691 :: Batch 0/1
Batch Loss = 1.137126084124709
691, epoch_train_loss=1.137126084124709
Epoch 692
Epoch 692 :: Batch 0/1
Batch Loss = 1.1365474782162595
692, epoch_train_loss=1.1365474782162595
Epoch 693
Epoch 693 :: Batch 0/1
Batch Loss = 1.135923458062527
693, epoch_train_loss=1.135923458062527
Epoch 694
Epoch 694 :: Batch 0/1
Batch Loss = 1.1353857483657712
694, epoch_train_loss=1.1353857483657712
Epoch 695
Epoch 695 :: Batch 0/1
Batch Loss = 1.1347807491833133
695, epoch_train_loss=1.1347807491833133
Epoch 696
Epoch 696 :: Batch 0/1
Batch Loss = 1.1341873526333552
696, epoch_train_loss=1.1341873526333552
Epoch 697
Epoch 697 :: Batch 0/1
Batch Loss = 1.1336344786691508
697, epoch_train_loss=1.1336344786691508
Epoch 698
Epoch 698 :: Batch 0/1
Batch Loss = 1.1330229907357685
698, epoch_train_loss=1.1330229907357685
Epoch 699
Epoch 699 :: Batch 0/1
Batch Loss = 1.1324499672346842
699, epoch_train_loss=1.1324499672346842
Epoch 700
Epoch 700 :: Batch 0/1
Batch Loss = 1.1318776702714017
700, epoch_train_loss=1.1318776702714017
Epoch 701
Epoch 701 :: Batch 0/1
Batch Loss = 1.1312731685870179
701, epoch_train_loss=1.1312731685870179
Epoch 702
Epoch 702 :: Batch 0/1
Batch Loss = 1.13070701875786
702, epoch_train_loss=1.13070701875786
Epoch 703
Epoch 703 :: Batch 0/1
Batch Loss = 1.1301194530211014
703, epoch_train_loss=1.1301194530211014
Epoch 704
Epoch 704 :: Batch 0/1
Batch Loss = 1.1295262107149697
704, epoch_train_loss=1.1295262107149697
Epoch 705
Epoch 705 :: Batch 0/1
Batch Loss = 1.128956477014005
705, epoch_train_loss=1.128956477014005
Epoch 706
Epoch 706 :: Batch 0/1
Batch Loss = 1.1283617466152527
706, epoch_train_loss=1.1283617466152527
Epoch 707
Epoch 707 :: Batch 0/1
Batch Loss = 1.1277781038236865
707, epoch_train_loss=1.1277781038236865
Epoch 708
Epoch 708 :: Batch 0/1
Batch Loss = 1.127200737612871
708, epoch_train_loss=1.127200737612871
Epoch 709
Epoch 709 :: Batch 0/1
Batch Loss = 1.1266065574110788
709, epoch_train_loss=1.1266065574110788
Epoch 710
Epoch 710 :: Batch 0/1
Batch Loss = 1.1260282326966997
710, epoch_train_loss=1.1260282326966997
Epoch 711
Epoch 711 :: Batch 0/1
Batch Loss = 1.12544443170926
711, epoch_train_loss=1.12544443170926
Epoch 712
Epoch 712 :: Batch 0/1
Batch Loss = 1.1248553247170856
712, epoch_train_loss=1.1248553247170856
Epoch 713
Epoch 713 :: Batch 0/1
Batch Loss = 1.1242783167150245
713, epoch_train_loss=1.1242783167150245
Epoch 714
Epoch 714 :: Batch 0/1
Batch Loss = 1.1236920113815403
714, epoch_train_loss=1.1236920113815403
Epoch 715
Epoch 715 :: Batch 0/1
Batch Loss = 1.1231092052985538
715, epoch_train_loss=1.1231092052985538
Epoch 716
Epoch 716 :: Batch 0/1
Batch Loss = 1.1225317207250762
716, epoch_train_loss=1.1225317207250762
Epoch 717
Epoch 717 :: Batch 0/1
Batch Loss = 1.1219472881381618
717, epoch_train_loss=1.1219472881381618
Epoch 718
Epoch 718 :: Batch 0/1
Batch Loss = 1.1213701026540255
718, epoch_train_loss=1.1213701026540255
Epoch 719
Epoch 719 :: Batch 0/1
Batch Loss = 1.1207927618993978
719, epoch_train_loss=1.1207927618993978
Epoch 720
Epoch 720 :: Batch 0/1
Batch Loss = 1.120213328079674
720, epoch_train_loss=1.120213328079674
Epoch 721
Epoch 721 :: Batch 0/1
Batch Loss = 1.1196405202770932
721, epoch_train_loss=1.1196405202770932
Epoch 722
Epoch 722 :: Batch 0/1
Batch Loss = 1.1190652339310252
722, epoch_train_loss=1.1190652339310252
Epoch 723
Epoch 723 :: Batch 0/1
Batch Loss = 1.1184920433857042
723, epoch_train_loss=1.1184920433857042
Epoch 724
Epoch 724 :: Batch 0/1
Batch Loss = 1.1179228314474279
724, epoch_train_loss=1.1179228314474279
Epoch 725
Epoch 725 :: Batch 0/1
Batch Loss = 1.1173518211847955
725, epoch_train_loss=1.1173518211847955
Epoch 726
Epoch 726 :: Batch 0/1
Batch Loss = 1.1167851591937057
726, epoch_train_loss=1.1167851591937057
Epoch 727
Epoch 727 :: Batch 0/1
Batch Loss = 1.1162202853760066
727, epoch_train_loss=1.1162202853760066
Epoch 728
Epoch 728 :: Batch 0/1
Batch Loss = 1.115656026015079
728, epoch_train_loss=1.115656026015079
Epoch 729
Epoch 729 :: Batch 0/1
Batch Loss = 1.1150965497754852
729, epoch_train_loss=1.1150965497754852
Epoch 730
Epoch 730 :: Batch 0/1
Batch Loss = 1.1145381111933343
730, epoch_train_loss=1.1145381111933343
Epoch 731
Epoch 731 :: Batch 0/1
Batch Loss = 1.1139825824164886
731, epoch_train_loss=1.1139825824164886
Epoch 732
Epoch 732 :: Batch 0/1
Batch Loss = 1.1134310475266116
732, epoch_train_loss=1.1134310475266116
Epoch 733
Epoch 733 :: Batch 0/1
Batch Loss = 1.1128809483093935
733, epoch_train_loss=1.1128809483093935
Epoch 734
Epoch 734 :: Batch 0/1
Batch Loss = 1.1123349295473715
734, epoch_train_loss=1.1123349295473715
Epoch 735
Epoch 735 :: Batch 0/1
Batch Loss = 1.1117919172696322
735, epoch_train_loss=1.1117919172696322
Epoch 736
Epoch 736 :: Batch 0/1
Batch Loss = 1.1112513139954785
736, epoch_train_loss=1.1112513139954785
Epoch 737
Epoch 737 :: Batch 0/1
Batch Loss = 1.1107148996316951
737, epoch_train_loss=1.1107148996316951
Epoch 738
Epoch 738 :: Batch 0/1
Batch Loss = 1.1101810006223887
738, epoch_train_loss=1.1101810006223887
Epoch 739
Epoch 739 :: Batch 0/1
Batch Loss = 1.1096504055287728
739, epoch_train_loss=1.1096504055287728
Epoch 740
Epoch 740 :: Batch 0/1
Batch Loss = 1.1091235299382702
740, epoch_train_loss=1.1091235299382702
Epoch 741
Epoch 741 :: Batch 0/1
Batch Loss = 1.1085992506334938
741, epoch_train_loss=1.1085992506334938
Epoch 742
Epoch 742 :: Batch 0/1
Batch Loss = 1.1080787089364914
742, epoch_train_loss=1.1080787089364914
Epoch 743
Epoch 743 :: Batch 0/1
Batch Loss = 1.1075613747936706
743, epoch_train_loss=1.1075613747936706
Epoch 744
Epoch 744 :: Batch 0/1
Batch Loss = 1.1070470258700038
744, epoch_train_loss=1.1070470258700038
Epoch 745
Epoch 745 :: Batch 0/1
Batch Loss = 1.1065363973977798
745, epoch_train_loss=1.1065363973977798
Epoch 746
Epoch 746 :: Batch 0/1
Batch Loss = 1.106028737969852
746, epoch_train_loss=1.106028737969852
Epoch 747
Epoch 747 :: Batch 0/1
Batch Loss = 1.1055244362810135
747, epoch_train_loss=1.1055244362810135
Epoch 748
Epoch 748 :: Batch 0/1
Batch Loss = 1.1050236064959185
748, epoch_train_loss=1.1050236064959185
Epoch 749
Epoch 749 :: Batch 0/1
Batch Loss = 1.1045257797251002
749, epoch_train_loss=1.1045257797251002
Epoch 750
Epoch 750 :: Batch 0/1
Batch Loss = 1.1040314445220016
750, epoch_train_loss=1.1040314445220016
Epoch 751
Epoch 751 :: Batch 0/1
Batch Loss = 1.1035402903209197
751, epoch_train_loss=1.1035402903209197
Epoch 752
Epoch 752 :: Batch 0/1
Batch Loss = 1.103052230514384
752, epoch_train_loss=1.103052230514384
Epoch 753
Epoch 753 :: Batch 0/1
Batch Loss = 1.1025674939358745
753, epoch_train_loss=1.1025674939358745
Epoch 754
Epoch 754 :: Batch 0/1
Batch Loss = 1.1020856747515249
754, epoch_train_loss=1.1020856747515249
Epoch 755
Epoch 755 :: Batch 0/1
Batch Loss = 1.1016069010603367
755, epoch_train_loss=1.1016069010603367
Epoch 756
Epoch 756 :: Batch 0/1
Batch Loss = 1.1011310904207796
756, epoch_train_loss=1.1011310904207796
Epoch 757
Epoch 757 :: Batch 0/1
Batch Loss = 1.1006579852975154
757, epoch_train_loss=1.1006579852975154
Epoch 758
Epoch 758 :: Batch 0/1
Batch Loss = 1.1001877234979338
758, epoch_train_loss=1.1001877234979338
Epoch 759
Epoch 759 :: Batch 0/1
Batch Loss = 1.0997200728642955
759, epoch_train_loss=1.0997200728642955
Epoch 760
Epoch 760 :: Batch 0/1
Batch Loss = 1.0992549780211236
760, epoch_train_loss=1.0992549780211236
Epoch 761
Epoch 761 :: Batch 0/1
Batch Loss = 1.098792466377249
761, epoch_train_loss=1.098792466377249
Epoch 762
Epoch 762 :: Batch 0/1
Batch Loss = 1.0983323256132735
762, epoch_train_loss=1.0983323256132735
Epoch 763
Epoch 763 :: Batch 0/1
Batch Loss = 1.097874599336074
763, epoch_train_loss=1.097874599336074
Epoch 764
Epoch 764 :: Batch 0/1
Batch Loss = 1.0974191903690438
764, epoch_train_loss=1.0974191903690438
Epoch 765
Epoch 765 :: Batch 0/1
Batch Loss = 1.0969659766169357
765, epoch_train_loss=1.0969659766169357
Epoch 766
Epoch 766 :: Batch 0/1
Batch Loss = 1.0965149778333019
766, epoch_train_loss=1.0965149778333019
Epoch 767
Epoch 767 :: Batch 0/1
Batch Loss = 1.0960660416165162
767, epoch_train_loss=1.0960660416165162
Epoch 768
Epoch 768 :: Batch 0/1
Batch Loss = 1.0956191264858144
768, epoch_train_loss=1.0956191264858144
Epoch 769
Epoch 769 :: Batch 0/1
Batch Loss = 1.095174186512808
769, epoch_train_loss=1.095174186512808
Epoch 770
Epoch 770 :: Batch 0/1
Batch Loss = 1.0947310993386086
770, epoch_train_loss=1.0947310993386086
Epoch 771
Epoch 771 :: Batch 0/1
Batch Loss = 1.094289859567132
771, epoch_train_loss=1.094289859567132
Epoch 772
Epoch 772 :: Batch 0/1
Batch Loss = 1.0938503828681234
772, epoch_train_loss=1.0938503828681234
Epoch 773
Epoch 773 :: Batch 0/1
Batch Loss = 1.093412610547392
773, epoch_train_loss=1.093412610547392
Epoch 774
Epoch 774 :: Batch 0/1
Batch Loss = 1.0929765287533921
774, epoch_train_loss=1.0929765287533921
Epoch 775
Epoch 775 :: Batch 0/1
Batch Loss = 1.0925420566747153
775, epoch_train_loss=1.0925420566747153
Epoch 776
Epoch 776 :: Batch 0/1
Batch Loss = 1.0921091786029211
776, epoch_train_loss=1.0921091786029211
Epoch 777
Epoch 777 :: Batch 0/1
Batch Loss = 1.091677857035734
777, epoch_train_loss=1.091677857035734
Epoch 778
Epoch 778 :: Batch 0/1
Batch Loss = 1.0912480411554557
778, epoch_train_loss=1.0912480411554557
Epoch 779
Epoch 779 :: Batch 0/1
Batch Loss = 1.0908197261832688
779, epoch_train_loss=1.0908197261832688
Epoch 780
Epoch 780 :: Batch 0/1
Batch Loss = 1.090392867265089
780, epoch_train_loss=1.090392867265089
Epoch 781
Epoch 781 :: Batch 0/1
Batch Loss = 1.0899674471089476
781, epoch_train_loss=1.0899674471089476
Epoch 782
Epoch 782 :: Batch 0/1
Batch Loss = 1.0895434548797942
782, epoch_train_loss=1.0895434548797942
Epoch 783
Epoch 783 :: Batch 0/1
Batch Loss = 1.089120858759764
783, epoch_train_loss=1.089120858759764
Epoch 784
Epoch 784 :: Batch 0/1
Batch Loss = 1.0886996584218847
784, epoch_train_loss=1.0886996584218847
Epoch 785
Epoch 785 :: Batch 0/1
Batch Loss = 1.0882798344426277
785, epoch_train_loss=1.0882798344426277
Epoch 786
Epoch 786 :: Batch 0/1
Batch Loss = 1.0878613717340448
786, epoch_train_loss=1.0878613717340448
Epoch 787
Epoch 787 :: Batch 0/1
Batch Loss = 1.0874442669082949
787, epoch_train_loss=1.0874442669082949
Epoch 788
Epoch 788 :: Batch 0/1
Batch Loss = 1.0870284978221192
788, epoch_train_loss=1.0870284978221192
Epoch 789
Epoch 789 :: Batch 0/1
Batch Loss = 1.0866140578444463
789, epoch_train_loss=1.0866140578444463
Epoch 790
Epoch 790 :: Batch 0/1
Batch Loss = 1.0862009338459508
790, epoch_train_loss=1.0862009338459508
Epoch 791
Epoch 791 :: Batch 0/1
Batch Loss = 1.0857891084135298
791, epoch_train_loss=1.0857891084135298
Epoch 792
Epoch 792 :: Batch 0/1
Batch Loss = 1.0853785751003606
792, epoch_train_loss=1.0853785751003606
Epoch 793
Epoch 793 :: Batch 0/1
Batch Loss = 1.0849693161914775
793, epoch_train_loss=1.0849693161914775
Epoch 794
Epoch 794 :: Batch 0/1
Batch Loss = 1.0845613206958509
794, epoch_train_loss=1.0845613206958509
Epoch 795
Epoch 795 :: Batch 0/1
Batch Loss = 1.0841545777263217
795, epoch_train_loss=1.0841545777263217
Epoch 796
Epoch 796 :: Batch 0/1
Batch Loss = 1.0837490705263015
796, epoch_train_loss=1.0837490705263015
Epoch 797
Epoch 797 :: Batch 0/1
Batch Loss = 1.0833447894758237
797, epoch_train_loss=1.0833447894758237
Epoch 798
Epoch 798 :: Batch 0/1
Batch Loss = 1.082941718662659
798, epoch_train_loss=1.082941718662659
Epoch 799
Epoch 799 :: Batch 0/1
Batch Loss = 1.0825398433351363
799, epoch_train_loss=1.0825398433351363
Epoch 800
Epoch 800 :: Batch 0/1
Batch Loss = 1.0821391504616333
800, epoch_train_loss=1.0821391504616333
Epoch 801
Epoch 801 :: Batch 0/1
Batch Loss = 1.0817396220191255
801, epoch_train_loss=1.0817396220191255
Epoch 802
Epoch 802 :: Batch 0/1
Batch Loss = 1.0813412440506653
802, epoch_train_loss=1.0813412440506653
Epoch 803
Epoch 803 :: Batch 0/1
Batch Loss = 1.0809440000597015
803, epoch_train_loss=1.0809440000597015
Epoch 804
Epoch 804 :: Batch 0/1
Batch Loss = 1.0805478732612885
804, epoch_train_loss=1.0805478732612885
Epoch 805
Epoch 805 :: Batch 0/1
Batch Loss = 1.080152849155391
805, epoch_train_loss=1.080152849155391
Epoch 806
Epoch 806 :: Batch 0/1
Batch Loss = 1.0797589104421523
806, epoch_train_loss=1.0797589104421523
Epoch 807
Epoch 807 :: Batch 0/1
Batch Loss = 1.0793660420892004
807, epoch_train_loss=1.0793660420892004
Epoch 808
Epoch 808 :: Batch 0/1
Batch Loss = 1.078974228505017
808, epoch_train_loss=1.078974228505017
Epoch 809
Epoch 809 :: Batch 0/1
Batch Loss = 1.0785834532932863
809, epoch_train_loss=1.0785834532932863
Epoch 810
Epoch 810 :: Batch 0/1
Batch Loss = 1.0781937018691987
810, epoch_train_loss=1.0781937018691987
Epoch 811
Epoch 811 :: Batch 0/1
Batch Loss = 1.077804957950325
811, epoch_train_loss=1.077804957950325
Epoch 812
Epoch 812 :: Batch 0/1
Batch Loss = 1.0774172063236966
812, epoch_train_loss=1.0774172063236966
Epoch 813
Epoch 813 :: Batch 0/1
Batch Loss = 1.077030431949232
813, epoch_train_loss=1.077030431949232
Epoch 814
Epoch 814 :: Batch 0/1
Batch Loss = 1.076644619007361
814, epoch_train_loss=1.076644619007361
Epoch 815
Epoch 815 :: Batch 0/1
Batch Loss = 1.0762597530573181
815, epoch_train_loss=1.0762597530573181
Epoch 816
Epoch 816 :: Batch 0/1
Batch Loss = 1.0758758190344697
816, epoch_train_loss=1.0758758190344697
Epoch 817
Epoch 817 :: Batch 0/1
Batch Loss = 1.07549280229143
817, epoch_train_loss=1.07549280229143
Epoch 818
Epoch 818 :: Batch 0/1
Batch Loss = 1.0751106887806658
818, epoch_train_loss=1.0751106887806658
Epoch 819
Epoch 819 :: Batch 0/1
Batch Loss = 1.074729464044839
819, epoch_train_loss=1.074729464044839
Epoch 820
Epoch 820 :: Batch 0/1
Batch Loss = 1.0743491144772905
820, epoch_train_loss=1.0743491144772905
Epoch 821
Epoch 821 :: Batch 0/1
Batch Loss = 1.073969626313603
821, epoch_train_loss=1.073969626313603
Epoch 822
Epoch 822 :: Batch 0/1
Batch Loss = 1.0735909859596677
822, epoch_train_loss=1.0735909859596677
Epoch 823
Epoch 823 :: Batch 0/1
Batch Loss = 1.0732131803495273
823, epoch_train_loss=1.0732131803495273
Epoch 824
Epoch 824 :: Batch 0/1
Batch Loss = 1.0728361962178405
824, epoch_train_loss=1.0728361962178405
Epoch 825
Epoch 825 :: Batch 0/1
Batch Loss = 1.0724600208727324
825, epoch_train_loss=1.0724600208727324
Epoch 826
Epoch 826 :: Batch 0/1
Batch Loss = 1.0720846417769938
826, epoch_train_loss=1.0720846417769938
Epoch 827
Epoch 827 :: Batch 0/1
Batch Loss = 1.0717100467033607
827, epoch_train_loss=1.0717100467033607
Epoch 828
Epoch 828 :: Batch 0/1
Batch Loss = 1.0713362241377271
828, epoch_train_loss=1.0713362241377271
Epoch 829
Epoch 829 :: Batch 0/1
Batch Loss = 1.0709631627678706
829, epoch_train_loss=1.0709631627678706
Epoch 830
Epoch 830 :: Batch 0/1
Batch Loss = 1.0705908520930008
830, epoch_train_loss=1.0705908520930008
Epoch 831
Epoch 831 :: Batch 0/1
Batch Loss = 1.0702192821157235
831, epoch_train_loss=1.0702192821157235
Epoch 832
Epoch 832 :: Batch 0/1
Batch Loss = 1.0698484432083588
832, epoch_train_loss=1.0698484432083588
Epoch 833
Epoch 833 :: Batch 0/1
Batch Loss = 1.0694783261499126
833, epoch_train_loss=1.0694783261499126
Epoch 834
Epoch 834 :: Batch 0/1
Batch Loss = 1.0691089213760432
834, epoch_train_loss=1.0691089213760432
Epoch 835
Epoch 835 :: Batch 0/1
Batch Loss = 1.0687402190837576
835, epoch_train_loss=1.0687402190837576
Epoch 836
Epoch 836 :: Batch 0/1
Batch Loss = 1.068372208578729
836, epoch_train_loss=1.068372208578729
Epoch 837
Epoch 837 :: Batch 0/1
Batch Loss = 1.0680048782993803
837, epoch_train_loss=1.0680048782993803
Epoch 838
Epoch 838 :: Batch 0/1
Batch Loss = 1.0676382158873585
838, epoch_train_loss=1.0676382158873585
Epoch 839
Epoch 839 :: Batch 0/1
Batch Loss = 1.0672722086724167
839, epoch_train_loss=1.0672722086724167
Epoch 840
Epoch 840 :: Batch 0/1
Batch Loss = 1.0669068442454475
840, epoch_train_loss=1.0669068442454475
Epoch 841
Epoch 841 :: Batch 0/1
Batch Loss = 1.0665421112595623
841, epoch_train_loss=1.0665421112595623
Epoch 842
Epoch 842 :: Batch 0/1
Batch Loss = 1.0661779994973928
842, epoch_train_loss=1.0661779994973928
Epoch 843
Epoch 843 :: Batch 0/1
Batch Loss = 1.0658145001647847
843, epoch_train_loss=1.0658145001647847
Epoch 844
Epoch 844 :: Batch 0/1
Batch Loss = 1.0654516053848455
844, epoch_train_loss=1.0654516053848455
Epoch 845
Epoch 845 :: Batch 0/1
Batch Loss = 1.0650893079293504
845, epoch_train_loss=1.0650893079293504
Epoch 846
Epoch 846 :: Batch 0/1
Batch Loss = 1.0647276008830266
846, epoch_train_loss=1.0647276008830266
Epoch 847
Epoch 847 :: Batch 0/1
Batch Loss = 1.0643664773956596
847, epoch_train_loss=1.0643664773956596
Epoch 848
Epoch 848 :: Batch 0/1
Batch Loss = 1.064005930654396
848, epoch_train_loss=1.064005930654396
Epoch 849
Epoch 849 :: Batch 0/1
Batch Loss = 1.0636459537676666
849, epoch_train_loss=1.0636459537676666
Epoch 850
Epoch 850 :: Batch 0/1
Batch Loss = 1.063286539905742
850, epoch_train_loss=1.063286539905742
Epoch 851
Epoch 851 :: Batch 0/1
Batch Loss = 1.0629276822159288
851, epoch_train_loss=1.0629276822159288
Epoch 852
Epoch 852 :: Batch 0/1
Batch Loss = 1.0625693739322073
852, epoch_train_loss=1.0625693739322073
Epoch 853
Epoch 853 :: Batch 0/1
Batch Loss = 1.062211608353667
853, epoch_train_loss=1.062211608353667
Epoch 854
Epoch 854 :: Batch 0/1
Batch Loss = 1.0618543788162123
854, epoch_train_loss=1.0618543788162123
Epoch 855
Epoch 855 :: Batch 0/1
Batch Loss = 1.0614976788012473
855, epoch_train_loss=1.0614976788012473
Epoch 856
Epoch 856 :: Batch 0/1
Batch Loss = 1.0611415018265604
856, epoch_train_loss=1.0611415018265604
Epoch 857
Epoch 857 :: Batch 0/1
Batch Loss = 1.0607858415371372
857, epoch_train_loss=1.0607858415371372
Epoch 858
Epoch 858 :: Batch 0/1
Batch Loss = 1.0604306916469952
858, epoch_train_loss=1.0604306916469952
Epoch 859
Epoch 859 :: Batch 0/1
Batch Loss = 1.0600760459149599
859, epoch_train_loss=1.0600760459149599
Epoch 860
Epoch 860 :: Batch 0/1
Batch Loss = 1.0597218982373662
860, epoch_train_loss=1.0597218982373662
Epoch 861
Epoch 861 :: Batch 0/1
Batch Loss = 1.0593682425380895
861, epoch_train_loss=1.0593682425380895
Epoch 862
Epoch 862 :: Batch 0/1
Batch Loss = 1.059015072837483
862, epoch_train_loss=1.059015072837483
Epoch 863
Epoch 863 :: Batch 0/1
Batch Loss = 1.0586623832653836
863, epoch_train_loss=1.0586623832653836
Epoch 864
Epoch 864 :: Batch 0/1
Batch Loss = 1.0583101679629752
864, epoch_train_loss=1.0583101679629752
Epoch 865
Epoch 865 :: Batch 0/1
Batch Loss = 1.0579584211950703
865, epoch_train_loss=1.0579584211950703
Epoch 866
Epoch 866 :: Batch 0/1
Batch Loss = 1.0576071372379576
866, epoch_train_loss=1.0576071372379576
Epoch 867
Epoch 867 :: Batch 0/1
Batch Loss = 1.057256310449651
867, epoch_train_loss=1.057256310449651
Epoch 868
Epoch 868 :: Batch 0/1
Batch Loss = 1.0569059352471466
868, epoch_train_loss=1.0569059352471466
Epoch 869
Epoch 869 :: Batch 0/1
Batch Loss = 1.056556006120429
869, epoch_train_loss=1.056556006120429
Epoch 870
Epoch 870 :: Batch 0/1
Batch Loss = 1.0562065175507698
870, epoch_train_loss=1.0562065175507698
Epoch 871
Epoch 871 :: Batch 0/1
Batch Loss = 1.0558574640871448
871, epoch_train_loss=1.0558574640871448
Epoch 872
Epoch 872 :: Batch 0/1
Batch Loss = 1.0555088403104071
872, epoch_train_loss=1.0555088403104071
Epoch 873
Epoch 873 :: Batch 0/1
Batch Loss = 1.0551606408607053
873, epoch_train_loss=1.0551606408607053
Epoch 874
Epoch 874 :: Batch 0/1
Batch Loss = 1.0548128603202247
874, epoch_train_loss=1.0548128603202247
Epoch 875
Epoch 875 :: Batch 0/1
Batch Loss = 1.0544654933990598
875, epoch_train_loss=1.0544654933990598
Epoch 876
Epoch 876 :: Batch 0/1
Batch Loss = 1.05411853474221
876, epoch_train_loss=1.05411853474221
Epoch 877
Epoch 877 :: Batch 0/1
Batch Loss = 1.0537719790299236
877, epoch_train_loss=1.0537719790299236
Epoch 878
Epoch 878 :: Batch 0/1
Batch Loss = 1.0534258209491125
878, epoch_train_loss=1.0534258209491125
Epoch 879
Epoch 879 :: Batch 0/1
Batch Loss = 1.053080055185554
879, epoch_train_loss=1.053080055185554
Epoch 880
Epoch 880 :: Batch 0/1
Batch Loss = 1.0527346764209535
880, epoch_train_loss=1.0527346764209535
Epoch 881
Epoch 881 :: Batch 0/1
Batch Loss = 1.0523896793281053
881, epoch_train_loss=1.0523896793281053
Epoch 882
Epoch 882 :: Batch 0/1
Batch Loss = 1.0520450585653394
882, epoch_train_loss=1.0520450585653394
Epoch 883
Epoch 883 :: Batch 0/1
Batch Loss = 1.051700808774012
883, epoch_train_loss=1.051700808774012
Epoch 884
Epoch 884 :: Batch 0/1
Batch Loss = 1.0513569245724943
884, epoch_train_loss=1.0513569245724943
Epoch 885
Epoch 885 :: Batch 0/1
Batch Loss = 1.0510134005532243
885, epoch_train_loss=1.0510134005532243
Epoch 886
Epoch 886 :: Batch 0/1
Batch Loss = 1.050670231251794
886, epoch_train_loss=1.050670231251794
Epoch 887
Epoch 887 :: Batch 0/1
Batch Loss = 1.0503274112218035
887, epoch_train_loss=1.0503274112218035
Epoch 888
Epoch 888 :: Batch 0/1
Batch Loss = 1.049984934952009
888, epoch_train_loss=1.049984934952009
Epoch 889
Epoch 889 :: Batch 0/1
Batch Loss = 1.0496427968872841
889, epoch_train_loss=1.0496427968872841
Epoch 890
Epoch 890 :: Batch 0/1
Batch Loss = 1.0493009914246207
890, epoch_train_loss=1.0493009914246207
Epoch 891
Epoch 891 :: Batch 0/1
Batch Loss = 1.0489595128813114
891, epoch_train_loss=1.0489595128813114
Epoch 892
Epoch 892 :: Batch 0/1
Batch Loss = 1.0486183555683977
892, epoch_train_loss=1.0486183555683977
Epoch 893
Epoch 893 :: Batch 0/1
Batch Loss = 1.0482775137062235
893, epoch_train_loss=1.0482775137062235
Epoch 894
Epoch 894 :: Batch 0/1
Batch Loss = 1.0479369814180437
894, epoch_train_loss=1.0479369814180437
Epoch 895
Epoch 895 :: Batch 0/1
Batch Loss = 1.047596752803842
895, epoch_train_loss=1.047596752803842
Epoch 896
Epoch 896 :: Batch 0/1
Batch Loss = 1.0472568218561051
896, epoch_train_loss=1.0472568218561051
Epoch 897
Epoch 897 :: Batch 0/1
Batch Loss = 1.0469171824561072
897, epoch_train_loss=1.0469171824561072
Epoch 898
Epoch 898 :: Batch 0/1
Batch Loss = 1.0465778284246663
898, epoch_train_loss=1.0465778284246663
Epoch 899
Epoch 899 :: Batch 0/1
Batch Loss = 1.0462387535219935
899, epoch_train_loss=1.0462387535219935
Epoch 900
Epoch 900 :: Batch 0/1
Batch Loss = 1.045899951398792
900, epoch_train_loss=1.045899951398792
Epoch 901
Epoch 901 :: Batch 0/1
Batch Loss = 1.0455614156027329
901, epoch_train_loss=1.0455614156027329
Epoch 902
Epoch 902 :: Batch 0/1
Batch Loss = 1.0452231396673046
902, epoch_train_loss=1.0452231396673046
Epoch 903
Epoch 903 :: Batch 0/1
Batch Loss = 1.0448851170215767
903, epoch_train_loss=1.0448851170215767
Epoch 904
Epoch 904 :: Batch 0/1
Batch Loss = 1.0445473411129496
904, epoch_train_loss=1.0445473411129496
Epoch 905
Epoch 905 :: Batch 0/1
Batch Loss = 1.0442098053251119
905, epoch_train_loss=1.0442098053251119
Epoch 906
Epoch 906 :: Batch 0/1
Batch Loss = 1.0438725030818197
906, epoch_train_loss=1.0438725030818197
Epoch 907
Epoch 907 :: Batch 0/1
Batch Loss = 1.0435354278485478
907, epoch_train_loss=1.0435354278485478
Epoch 908
Epoch 908 :: Batch 0/1
Batch Loss = 1.0431985731865452
908, epoch_train_loss=1.0431985731865452
Epoch 909
Epoch 909 :: Batch 0/1
Batch Loss = 1.0428619327001305
909, epoch_train_loss=1.0428619327001305
Epoch 910
Epoch 910 :: Batch 0/1
Batch Loss = 1.0425255001344895
910, epoch_train_loss=1.0425255001344895
Epoch 911
Epoch 911 :: Batch 0/1
Batch Loss = 1.0421892693849952
911, epoch_train_loss=1.0421892693849952
Epoch 912
Epoch 912 :: Batch 0/1
Batch Loss = 1.0418532344148468
912, epoch_train_loss=1.0418532344148468
Epoch 913
Epoch 913 :: Batch 0/1
Batch Loss = 1.0415173893116156
913, epoch_train_loss=1.0415173893116156
Epoch 914
Epoch 914 :: Batch 0/1
Batch Loss = 1.0411817282470877
914, epoch_train_loss=1.0411817282470877
Epoch 915
Epoch 915 :: Batch 0/1
Batch Loss = 1.040846245341413
915, epoch_train_loss=1.040846245341413
Epoch 916
Epoch 916 :: Batch 0/1
Batch Loss = 1.0405109346677484
916, epoch_train_loss=1.0405109346677484
Epoch 917
Epoch 917 :: Batch 0/1
Batch Loss = 1.0401757901428952
917, epoch_train_loss=1.0401757901428952
Epoch 918
Epoch 918 :: Batch 0/1
Batch Loss = 1.0398408054400279
918, epoch_train_loss=1.0398408054400279
Epoch 919
Epoch 919 :: Batch 0/1
Batch Loss = 1.039505973904907
919, epoch_train_loss=1.039505973904907
Epoch 920
Epoch 920 :: Batch 0/1
Batch Loss = 1.039171288483218
920, epoch_train_loss=1.039171288483218
Epoch 921
Epoch 921 :: Batch 0/1
Batch Loss = 1.0388367416663158
921, epoch_train_loss=1.0388367416663158
Epoch 922
Epoch 922 :: Batch 0/1
Batch Loss = 1.0385023254621233
922, epoch_train_loss=1.0385023254621233
Epoch 923
Epoch 923 :: Batch 0/1
Batch Loss = 1.0381680313961448
923, epoch_train_loss=1.0381680313961448
Epoch 924
Epoch 924 :: Batch 0/1
Batch Loss = 1.0378338505453124
924, epoch_train_loss=1.0378338505453124
Epoch 925
Epoch 925 :: Batch 0/1
Batch Loss = 1.0374997736039608
925, epoch_train_loss=1.0374997736039608
Epoch 926
Epoch 926 :: Batch 0/1
Batch Loss = 1.0371657909774326
926, epoch_train_loss=1.0371657909774326
Epoch 927
Epoch 927 :: Batch 0/1
Batch Loss = 1.0368318928953888
927, epoch_train_loss=1.0368318928953888
Epoch 928
Epoch 928 :: Batch 0/1
Batch Loss = 1.0364980695342563
928, epoch_train_loss=1.0364980695342563
Epoch 929
Epoch 929 :: Batch 0/1
Batch Loss = 1.036164311111733
929, epoch_train_loss=1.036164311111733
Epoch 930
Epoch 930 :: Batch 0/1
Batch Loss = 1.0358306080719832
930, epoch_train_loss=1.0358306080719832
Epoch 931
Epoch 931 :: Batch 0/1
Batch Loss = 1.035496951095227
931, epoch_train_loss=1.035496951095227
Epoch 932
Epoch 932 :: Batch 0/1
Batch Loss = 1.0351633311593809
932, epoch_train_loss=1.0351633311593809
Epoch 933
Epoch 933 :: Batch 0/1
Batch Loss = 1.034829739652157
933, epoch_train_loss=1.034829739652157
Epoch 934
Epoch 934 :: Batch 0/1
Batch Loss = 1.034496168302863
934, epoch_train_loss=1.034496168302863
Epoch 935
Epoch 935 :: Batch 0/1
Batch Loss = 1.0341626091699847
935, epoch_train_loss=1.0341626091699847
Epoch 936
Epoch 936 :: Batch 0/1
Batch Loss = 1.0338290546895597
936, epoch_train_loss=1.0338290546895597
Epoch 937
Epoch 937 :: Batch 0/1
Batch Loss = 1.0334954975575292
937, epoch_train_loss=1.0334954975575292
Epoch 938
Epoch 938 :: Batch 0/1
Batch Loss = 1.0331619307094306
938, epoch_train_loss=1.0331619307094306
Epoch 939
Epoch 939 :: Batch 0/1
Batch Loss = 1.0328283472469906
939, epoch_train_loss=1.0328283472469906
Epoch 940
Epoch 940 :: Batch 0/1
Batch Loss = 1.0324947404699203
940, epoch_train_loss=1.0324947404699203
Epoch 941
Epoch 941 :: Batch 0/1
Batch Loss = 1.032161103731211
941, epoch_train_loss=1.032161103731211
Epoch 942
Epoch 942 :: Batch 0/1
Batch Loss = 1.031827430504558
942, epoch_train_loss=1.031827430504558
Epoch 943
Epoch 943 :: Batch 0/1
Batch Loss = 1.0314937142512335
943, epoch_train_loss=1.0314937142512335
Epoch 944
Epoch 944 :: Batch 0/1
Batch Loss = 1.0311599484737877
944, epoch_train_loss=1.0311599484737877
Epoch 945
Epoch 945 :: Batch 0/1
Batch Loss = 1.0308261266978858
945, epoch_train_loss=1.0308261266978858
Epoch 946
Epoch 946 :: Batch 0/1
Batch Loss = 1.0304922424074907
946, epoch_train_loss=1.0304922424074907
Epoch 947
Epoch 947 :: Batch 0/1
Batch Loss = 1.0301582890360486
947, epoch_train_loss=1.0301582890360486
Epoch 948
Epoch 948 :: Batch 0/1
Batch Loss = 1.02982426001297
948, epoch_train_loss=1.02982426001297
Epoch 949
Epoch 949 :: Batch 0/1
Batch Loss = 1.0294901487612342
949, epoch_train_loss=1.0294901487612342
Epoch 950
Epoch 950 :: Batch 0/1
Batch Loss = 1.0291559486193802
950, epoch_train_loss=1.0291559486193802
Epoch 951
Epoch 951 :: Batch 0/1
Batch Loss = 1.0288216529199898
951, epoch_train_loss=1.0288216529199898
Epoch 952
Epoch 952 :: Batch 0/1
Batch Loss = 1.0284872549912205
952, epoch_train_loss=1.0284872549912205
Epoch 953
Epoch 953 :: Batch 0/1
Batch Loss = 1.0281527480807453
953, epoch_train_loss=1.0281527480807453
Epoch 954
Epoch 954 :: Batch 0/1
Batch Loss = 1.0278181254348588
954, epoch_train_loss=1.0278181254348588
Epoch 955
Epoch 955 :: Batch 0/1
Batch Loss = 1.027483380273694
955, epoch_train_loss=1.027483380273694
Epoch 956
Epoch 956 :: Batch 0/1
Batch Loss = 1.0271485057921055
956, epoch_train_loss=1.0271485057921055
Epoch 957
Epoch 957 :: Batch 0/1
Batch Loss = 1.0268134951866117
957, epoch_train_loss=1.0268134951866117
Epoch 958
Epoch 958 :: Batch 0/1
Batch Loss = 1.0264783415792416
958, epoch_train_loss=1.0264783415792416
Epoch 959
Epoch 959 :: Batch 0/1
Batch Loss = 1.0261430380986876
959, epoch_train_loss=1.0261430380986876
Epoch 960
Epoch 960 :: Batch 0/1
Batch Loss = 1.025807577834622
960, epoch_train_loss=1.025807577834622
Epoch 961
Epoch 961 :: Batch 0/1
Batch Loss = 1.0254719539524135
961, epoch_train_loss=1.0254719539524135
Epoch 962
Epoch 962 :: Batch 0/1
Batch Loss = 1.025136159554107
962, epoch_train_loss=1.025136159554107
Epoch 963
Epoch 963 :: Batch 0/1
Batch Loss = 1.0248001878066066
963, epoch_train_loss=1.0248001878066066
Epoch 964
Epoch 964 :: Batch 0/1
Batch Loss = 1.0244640319471114
964, epoch_train_loss=1.0244640319471114
Epoch 965
Epoch 965 :: Batch 0/1
Batch Loss = 1.0241276853194452
965, epoch_train_loss=1.0241276853194452
Epoch 966
Epoch 966 :: Batch 0/1
Batch Loss = 1.023791141409819
966, epoch_train_loss=1.023791141409819
Epoch 967
Epoch 967 :: Batch 0/1
Batch Loss = 1.0234543938728866
967, epoch_train_loss=1.0234543938728866
Epoch 968
Epoch 968 :: Batch 0/1
Batch Loss = 1.0231174365099158
968, epoch_train_loss=1.0231174365099158
Epoch 969
Epoch 969 :: Batch 0/1
Batch Loss = 1.022780263343563
969, epoch_train_loss=1.022780263343563
Epoch 970
Epoch 970 :: Batch 0/1
Batch Loss = 1.0224428683639457
970, epoch_train_loss=1.0224428683639457
Epoch 971
Epoch 971 :: Batch 0/1
Batch Loss = 1.0221052456017936
971, epoch_train_loss=1.0221052456017936
Epoch 972
Epoch 972 :: Batch 0/1
Batch Loss = 1.0217673888910552
972, epoch_train_loss=1.0217673888910552
Epoch 973
Epoch 973 :: Batch 0/1
Batch Loss = 1.0214292917643182
973, epoch_train_loss=1.0214292917643182
Epoch 974
Epoch 974 :: Batch 0/1
Batch Loss = 1.021090947324563
974, epoch_train_loss=1.021090947324563
Epoch 975
Epoch 975 :: Batch 0/1
Batch Loss = 1.020752348268608
975, epoch_train_loss=1.020752348268608
Epoch 976
Epoch 976 :: Batch 0/1
Batch Loss = 1.0204134868038952
976, epoch_train_loss=1.0204134868038952
Epoch 977
Epoch 977 :: Batch 0/1
Batch Loss = 1.0200743546847673
977, epoch_train_loss=1.0200743546847673
Epoch 978
Epoch 978 :: Batch 0/1
Batch Loss = 1.0197349433511635
978, epoch_train_loss=1.0197349433511635
Epoch 979
Epoch 979 :: Batch 0/1
Batch Loss = 1.01939524391522
979, epoch_train_loss=1.01939524391522
Epoch 980
Epoch 980 :: Batch 0/1
Batch Loss = 1.0190552472191252
980, epoch_train_loss=1.0190552472191252
Epoch 981
Epoch 981 :: Batch 0/1
Batch Loss = 1.018714943958088
981, epoch_train_loss=1.018714943958088
Epoch 982
Epoch 982 :: Batch 0/1
Batch Loss = 1.018374324629946
982, epoch_train_loss=1.018374324629946
Epoch 983
Epoch 983 :: Batch 0/1
Batch Loss = 1.0180333795503997
983, epoch_train_loss=1.0180333795503997
Epoch 984
Epoch 984 :: Batch 0/1
Batch Loss = 1.0176920989374634
984, epoch_train_loss=1.0176920989374634
Epoch 985
Epoch 985 :: Batch 0/1
Batch Loss = 1.0173504728038574
985, epoch_train_loss=1.0173504728038574
Epoch 986
Epoch 986 :: Batch 0/1
Batch Loss = 1.017008491030243
986, epoch_train_loss=1.017008491030243
Epoch 987
Epoch 987 :: Batch 0/1
Batch Loss = 1.0166661433571564
987, epoch_train_loss=1.0166661433571564
Epoch 988
Epoch 988 :: Batch 0/1
Batch Loss = 1.0163234192967179
988, epoch_train_loss=1.0163234192967179
Epoch 989
Epoch 989 :: Batch 0/1
Batch Loss = 1.0159803082308472
989, epoch_train_loss=1.0159803082308472
Epoch 990
Epoch 990 :: Batch 0/1
Batch Loss = 1.0156367992982065
990, epoch_train_loss=1.0156367992982065
Epoch 991
Epoch 991 :: Batch 0/1
Batch Loss = 1.0152928814688496
991, epoch_train_loss=1.0152928814688496
Epoch 992
Epoch 992 :: Batch 0/1
Batch Loss = 1.014948543540822
992, epoch_train_loss=1.014948543540822
Epoch 993
Epoch 993 :: Batch 0/1
Batch Loss = 1.01460377405795
993, epoch_train_loss=1.01460377405795
Epoch 994
Epoch 994 :: Batch 0/1
Batch Loss = 1.0142585614158088
994, epoch_train_loss=1.0142585614158088
Epoch 995
Epoch 995 :: Batch 0/1
Batch Loss = 1.013912893727988
995, epoch_train_loss=1.013912893727988
Epoch 996
Epoch 996 :: Batch 0/1
Batch Loss = 1.0135667590146147
996, epoch_train_loss=1.0135667590146147
Epoch 997
Epoch 997 :: Batch 0/1
Batch Loss = 1.0132201450165674
997, epoch_train_loss=1.0132201450165674
Epoch 998
Epoch 998 :: Batch 0/1
Batch Loss = 1.012873039305862
998, epoch_train_loss=1.012873039305862
Epoch 999
Epoch 999 :: Batch 0/1
Batch Loss = 1.0125254292637678
999, epoch_train_loss=1.0125254292637678
Epoch 1000
Epoch 1000 :: Batch 0/1
Batch Loss = 1.012177302088406
1000, epoch_train_loss=1.012177302088406
Epoch 1001
Epoch 1001 :: Batch 0/1
Batch Loss = 1.0118286448331748
1001, epoch_train_loss=1.0118286448331748
Epoch 1002
Epoch 1002 :: Batch 0/1
Batch Loss = 1.0114794443148616
1002, epoch_train_loss=1.0114794443148616
Epoch 1003
Epoch 1003 :: Batch 0/1
Batch Loss = 1.0111296873258282
1003, epoch_train_loss=1.0111296873258282
Epoch 1004
Epoch 1004 :: Batch 0/1
Batch Loss = 1.0107793604474513
1004, epoch_train_loss=1.0107793604474513
Epoch 1005
Epoch 1005 :: Batch 0/1
Batch Loss = 1.0104284503063279
1005, epoch_train_loss=1.0104284503063279
Epoch 1006
Epoch 1006 :: Batch 0/1
Batch Loss = 1.0100769434054608
1006, epoch_train_loss=1.0100769434054608
Epoch 1007
Epoch 1007 :: Batch 0/1
Batch Loss = 1.009724826401327
1007, epoch_train_loss=1.009724826401327
Epoch 1008
Epoch 1008 :: Batch 0/1
Batch Loss = 1.0093720859555553
1008, epoch_train_loss=1.0093720859555553
Epoch 1009
Epoch 1009 :: Batch 0/1
Batch Loss = 1.0090187090070655
1009, epoch_train_loss=1.0090187090070655
Epoch 1010
Epoch 1010 :: Batch 0/1
Batch Loss = 1.0086646827543244
1010, epoch_train_loss=1.0086646827543244
Epoch 1011
Epoch 1011 :: Batch 0/1
Batch Loss = 1.008309994784373
1011, epoch_train_loss=1.008309994784373
Epoch 1012
Epoch 1012 :: Batch 0/1
Batch Loss = 1.0079546331837481
1012, epoch_train_loss=1.0079546331837481
Epoch 1013
Epoch 1013 :: Batch 0/1
Batch Loss = 1.0075985866574022
1013, epoch_train_loss=1.0075985866574022
Epoch 1014
Epoch 1014 :: Batch 0/1
Batch Loss = 1.0072418446540514
1014, epoch_train_loss=1.0072418446540514
Epoch 1015
Epoch 1015 :: Batch 0/1
Batch Loss = 1.0068843974683208
1015, epoch_train_loss=1.0068843974683208
Epoch 1016
Epoch 1016 :: Batch 0/1
Batch Loss = 1.0065262364562222
1016, epoch_train_loss=1.0065262364562222
Epoch 1017
Epoch 1017 :: Batch 0/1
Batch Loss = 1.006167354056181
1017, epoch_train_loss=1.006167354056181
Epoch 1018
Epoch 1018 :: Batch 0/1
Batch Loss = 1.0058077440572806
1018, epoch_train_loss=1.0058077440572806
Epoch 1019
Epoch 1019 :: Batch 0/1
Batch Loss = 1.0054474015269248
1019, epoch_train_loss=1.0054474015269248
Epoch 1020
Epoch 1020 :: Batch 0/1
Batch Loss = 1.00508632311959
1020, epoch_train_loss=1.00508632311959
Epoch 1021
Epoch 1021 :: Batch 0/1
Batch Loss = 1.0047245070653783
1021, epoch_train_loss=1.0047245070653783
Epoch 1022
Epoch 1022 :: Batch 0/1
Batch Loss = 1.004361953253175
1022, epoch_train_loss=1.004361953253175
Epoch 1023
Epoch 1023 :: Batch 0/1
Batch Loss = 1.0039986633778375
1023, epoch_train_loss=1.0039986633778375
Epoch 1024
Epoch 1024 :: Batch 0/1
Batch Loss = 1.0036346408658345
1024, epoch_train_loss=1.0036346408658345
Epoch 1025
Epoch 1025 :: Batch 0/1
Batch Loss = 1.0032698909982771
1025, epoch_train_loss=1.0032698909982771
Epoch 1026
Epoch 1026 :: Batch 0/1
Batch Loss = 1.0029044207763267
1026, epoch_train_loss=1.0029044207763267
Epoch 1027
Epoch 1027 :: Batch 0/1
Batch Loss = 1.0025382389498916
1027, epoch_train_loss=1.0025382389498916
Epoch 1028
Epoch 1028 :: Batch 0/1
Batch Loss = 1.0021713559243062
1028, epoch_train_loss=1.0021713559243062
Epoch 1029
Epoch 1029 :: Batch 0/1
Batch Loss = 1.0018037835705131
1029, epoch_train_loss=1.0018037835705131
Epoch 1030
Epoch 1030 :: Batch 0/1
Batch Loss = 1.0014355350513624
1030, epoch_train_loss=1.0014355350513624
Epoch 1031
Epoch 1031 :: Batch 0/1
Batch Loss = 1.0010666246644642
1031, epoch_train_loss=1.0010666246644642
Epoch 1032
Epoch 1032 :: Batch 0/1
Batch Loss = 1.000697067588215
1032, epoch_train_loss=1.000697067588215
Epoch 1033
Epoch 1033 :: Batch 0/1
Batch Loss = 1.0003268795036968
1033, epoch_train_loss=1.0003268795036968
Epoch 1034
Epoch 1034 :: Batch 0/1
Batch Loss = 0.9999560763545207
1034, epoch_train_loss=0.9999560763545207
Epoch 1035
Epoch 1035 :: Batch 0/1
Batch Loss = 0.999584673991648
1035, epoch_train_loss=0.999584673991648
Epoch 1036
Epoch 1036 :: Batch 0/1
Batch Loss = 0.999212687708863
1036, epoch_train_loss=0.999212687708863
Epoch 1037
Epoch 1037 :: Batch 0/1
Batch Loss = 0.9988401319368165
1037, epoch_train_loss=0.9988401319368165
Epoch 1038
Epoch 1038 :: Batch 0/1
Batch Loss = 0.9984670198457057
1038, epoch_train_loss=0.9984670198457057
Epoch 1039
Epoch 1039 :: Batch 0/1
Batch Loss = 0.9980933628372526
1039, epoch_train_loss=0.9980933628372526
Epoch 1040
Epoch 1040 :: Batch 0/1
Batch Loss = 0.9977191703726437
1040, epoch_train_loss=0.9977191703726437
Epoch 1041
Epoch 1041 :: Batch 0/1
Batch Loss = 0.99734444942256
1041, epoch_train_loss=0.99734444942256
Epoch 1042
Epoch 1042 :: Batch 0/1
Batch Loss = 0.9969692042764897
1042, epoch_train_loss=0.9969692042764897
Epoch 1043
Epoch 1043 :: Batch 0/1
Batch Loss = 0.9965934362573232
1043, epoch_train_loss=0.9965934362573232
Epoch 1044
Epoch 1044 :: Batch 0/1
Batch Loss = 0.9962171435248808
1044, epoch_train_loss=0.9962171435248808
Epoch 1045
Epoch 1045 :: Batch 0/1
Batch Loss = 0.9958403209763032
1045, epoch_train_loss=0.9958403209763032
Epoch 1046
Epoch 1046 :: Batch 0/1
Batch Loss = 0.9954629600731699
1046, epoch_train_loss=0.9954629600731699
Epoch 1047
Epoch 1047 :: Batch 0/1
Batch Loss = 0.9950850490384678
1047, epoch_train_loss=0.9950850490384678
Epoch 1048
Epoch 1048 :: Batch 0/1
Batch Loss = 0.9947065728390523
1048, epoch_train_loss=0.9947065728390523
Epoch 1049
Epoch 1049 :: Batch 0/1
Batch Loss = 0.9943275134226623
1049, epoch_train_loss=0.9943275134226623
Epoch 1050
Epoch 1050 :: Batch 0/1
Batch Loss = 0.9939478498559189
1050, epoch_train_loss=0.9939478498559189
Epoch 1051
Epoch 1051 :: Batch 0/1
Batch Loss = 0.9935675588337695
1051, epoch_train_loss=0.9935675588337695
Epoch 1052
Epoch 1052 :: Batch 0/1
Batch Loss = 0.993186614970426
1052, epoch_train_loss=0.993186614970426
Epoch 1053
Epoch 1053 :: Batch 0/1
Batch Loss = 0.9928049913151523
1053, epoch_train_loss=0.9928049913151523
Epoch 1054
Epoch 1054 :: Batch 0/1
Batch Loss = 0.9924226599194136
1054, epoch_train_loss=0.9924226599194136
Epoch 1055
Epoch 1055 :: Batch 0/1
Batch Loss = 0.9920395924607668
1055, epoch_train_loss=0.9920395924607668
Epoch 1056
Epoch 1056 :: Batch 0/1
Batch Loss = 0.9916557610799244
1056, epoch_train_loss=0.9916557610799244
Epoch 1057
Epoch 1057 :: Batch 0/1
Batch Loss = 0.9912711391407831
1057, epoch_train_loss=0.9912711391407831
Epoch 1058
Epoch 1058 :: Batch 0/1
Batch Loss = 0.990885702215609
1058, epoch_train_loss=0.990885702215609
Epoch 1059
Epoch 1059 :: Batch 0/1
Batch Loss = 0.9904994291435646
1059, epoch_train_loss=0.9904994291435646
Epoch 1060
Epoch 1060 :: Batch 0/1
Batch Loss = 0.9901123031452083
1060, epoch_train_loss=0.9901123031452083
Epoch 1061
Epoch 1061 :: Batch 0/1
Batch Loss = 0.9897243131052206
1061, epoch_train_loss=0.9897243131052206
Epoch 1062
Epoch 1062 :: Batch 0/1
Batch Loss = 0.9893354546248436
1062, epoch_train_loss=0.9893354546248436
Epoch 1063
Epoch 1063 :: Batch 0/1
Batch Loss = 0.9889457311937221
1063, epoch_train_loss=0.9889457311937221
Epoch 1064
Epoch 1064 :: Batch 0/1
Batch Loss = 0.9885551548888145
1064, epoch_train_loss=0.9885551548888145
Epoch 1065
Epoch 1065 :: Batch 0/1
Batch Loss = 0.9881637467600854
1065, epoch_train_loss=0.9881637467600854
Epoch 1066
Epoch 1066 :: Batch 0/1
Batch Loss = 0.9877715365299381
1066, epoch_train_loss=0.9877715365299381
Epoch 1067
Epoch 1067 :: Batch 0/1
Batch Loss = 0.9873785615724407
1067, epoch_train_loss=0.9873785615724407
Epoch 1068
Epoch 1068 :: Batch 0/1
Batch Loss = 0.9869848648732447
1068, epoch_train_loss=0.9869848648732447
Epoch 1069
Epoch 1069 :: Batch 0/1
Batch Loss = 0.9865904918695151
1069, epoch_train_loss=0.9865904918695151
Epoch 1070
Epoch 1070 :: Batch 0/1
Batch Loss = 0.9861954865205527
1070, epoch_train_loss=0.9861954865205527
Epoch 1071
Epoch 1071 :: Batch 0/1
Batch Loss = 0.9857998863288489
1071, epoch_train_loss=0.9857998863288489
Epoch 1072
Epoch 1072 :: Batch 0/1
Batch Loss = 0.985403717116383
1072, epoch_train_loss=0.985403717116383
Epoch 1073
Epoch 1073 :: Batch 0/1
Batch Loss = 0.9850069874796183
1073, epoch_train_loss=0.9850069874796183
Epoch 1074
Epoch 1074 :: Batch 0/1
Batch Loss = 0.9846096838675363
1074, epoch_train_loss=0.9846096838675363
Epoch 1075
Epoch 1075 :: Batch 0/1
Batch Loss = 0.9842117661847984
1075, epoch_train_loss=0.9842117661847984
Epoch 1076
Epoch 1076 :: Batch 0/1
Batch Loss = 0.9838131646193022
1076, epoch_train_loss=0.9838131646193022
Epoch 1077
Epoch 1077 :: Batch 0/1
Batch Loss = 0.9834137773388604
1077, epoch_train_loss=0.9834137773388604
Epoch 1078
Epoch 1078 :: Batch 0/1
Batch Loss = 0.9830134688843027
1078, epoch_train_loss=0.9830134688843027
Epoch 1079
Epoch 1079 :: Batch 0/1
Batch Loss = 0.9826120685968645
1079, epoch_train_loss=0.9826120685968645
Epoch 1080
Epoch 1080 :: Batch 0/1
Batch Loss = 0.9822093679114592
1080, epoch_train_loss=0.9822093679114592
Epoch 1081
Epoch 1081 :: Batch 0/1
Batch Loss = 0.9818051153884634
1081, epoch_train_loss=0.9818051153884634
Epoch 1082
Epoch 1082 :: Batch 0/1
Batch Loss = 0.9813990070323708
1082, epoch_train_loss=0.9813990070323708
Epoch 1083
Epoch 1083 :: Batch 0/1
Batch Loss = 0.9809906685668952
1083, epoch_train_loss=0.9809906685668952
Epoch 1084
Epoch 1084 :: Batch 0/1
Batch Loss = 0.9805796231668201
1084, epoch_train_loss=0.9805796231668201
Epoch 1085
Epoch 1085 :: Batch 0/1
Batch Loss = 0.9801652332005535
1085, epoch_train_loss=0.9801652332005535
Epoch 1086
Epoch 1086 :: Batch 0/1
Batch Loss = 0.9797465937379529
1086, epoch_train_loss=0.9797465937379529
Epoch 1087
Epoch 1087 :: Batch 0/1
Batch Loss = 0.9793223378683853
1087, epoch_train_loss=0.9793223378683853
Epoch 1088
Epoch 1088 :: Batch 0/1
Batch Loss = 0.9788902821929985
1088, epoch_train_loss=0.9788902821929985
Epoch 1089
Epoch 1089 :: Batch 0/1
Batch Loss = 0.9784467886972626
1089, epoch_train_loss=0.9784467886972626
Epoch 1090
Epoch 1090 :: Batch 0/1
Batch Loss = 0.9779856464545326
1090, epoch_train_loss=0.9779856464545326
Epoch 1091
Epoch 1091 :: Batch 0/1
Batch Loss = 0.9774962673288182
1091, epoch_train_loss=0.9774962673288182
Epoch 1092
Epoch 1092 :: Batch 0/1
Batch Loss = 0.9769616348079131
1092, epoch_train_loss=0.9769616348079131
Epoch 1093
Epoch 1093 :: Batch 0/1
Batch Loss = 0.9763603891999115
1093, epoch_train_loss=0.9763603891999115
Epoch 1094
Epoch 1094 :: Batch 0/1
Batch Loss = 0.9756870333173441
1094, epoch_train_loss=0.9756870333173441
Epoch 1095
Epoch 1095 :: Batch 0/1
Batch Loss = 0.9749578313374851
1095, epoch_train_loss=0.9749578313374851
Epoch 1096
Epoch 1096 :: Batch 0/1
Batch Loss = 0.9738967240407562
1096, epoch_train_loss=0.9738967240407562
Epoch 1097
Epoch 1097 :: Batch 0/1
Batch Loss = 0.9697446246926748
1097, epoch_train_loss=0.9697446246926748
Epoch 1098
Epoch 1098 :: Batch 0/1
Batch Loss = 1.029514647851286
1098, epoch_train_loss=1.029514647851286
Epoch 1099
Epoch 1099 :: Batch 0/1
Batch Loss = 0.9683522527025834
1099, epoch_train_loss=0.9683522527025834
Epoch 1100
Epoch 1100 :: Batch 0/1
Batch Loss = 0.9716158172346949
1100, epoch_train_loss=0.9716158172346949
Epoch 1101
Epoch 1101 :: Batch 0/1
Batch Loss = 0.971573420298209
1101, epoch_train_loss=0.971573420298209
Epoch 1102
Epoch 1102 :: Batch 0/1
Batch Loss = 0.9714365144009637
1102, epoch_train_loss=0.9714365144009637
Epoch 1103
Epoch 1103 :: Batch 0/1
Batch Loss = 0.9712602343127396
1103, epoch_train_loss=0.9712602343127396
Epoch 1104
Epoch 1104 :: Batch 0/1
Batch Loss = 0.9710289234135764
1104, epoch_train_loss=0.9710289234135764
Epoch 1105
Epoch 1105 :: Batch 0/1
Batch Loss = 0.9707779651646319
1105, epoch_train_loss=0.9707779651646319
Epoch 1106
Epoch 1106 :: Batch 0/1
Batch Loss = 0.9705304604089302
1106, epoch_train_loss=0.9705304604089302
Epoch 1107
Epoch 1107 :: Batch 0/1
Batch Loss = 0.9702917994269892
1107, epoch_train_loss=0.9702917994269892
Epoch 1108
Epoch 1108 :: Batch 0/1
Batch Loss = 0.9700488772553785
1108, epoch_train_loss=0.9700488772553785
Epoch 1109
Epoch 1109 :: Batch 0/1
Batch Loss = 0.9697733519356992
1109, epoch_train_loss=0.9697733519356992
Epoch 1110
Epoch 1110 :: Batch 0/1
Batch Loss = 0.9694399246260484
1110, epoch_train_loss=0.9694399246260484
Epoch 1111
Epoch 1111 :: Batch 0/1
Batch Loss = 0.969040948304964
1111, epoch_train_loss=0.969040948304964
Epoch 1112
Epoch 1112 :: Batch 0/1
Batch Loss = 0.9685916288751171
1112, epoch_train_loss=0.9685916288751171
Epoch 1113
Epoch 1113 :: Batch 0/1
Batch Loss = 0.9681183610977374
1113, epoch_train_loss=0.9681183610977374
Epoch 1114
Epoch 1114 :: Batch 0/1
Batch Loss = 0.967643291830784
1114, epoch_train_loss=0.967643291830784
Epoch 1115
Epoch 1115 :: Batch 0/1
Batch Loss = 0.9671756878165972
1115, epoch_train_loss=0.9671756878165972
Epoch 1116
Epoch 1116 :: Batch 0/1
Batch Loss = 0.9667128777424643
1116, epoch_train_loss=0.9667128777424643
Epoch 1117
Epoch 1117 :: Batch 0/1
Batch Loss = 0.9662485432780266
1117, epoch_train_loss=0.9662485432780266
Epoch 1118
Epoch 1118 :: Batch 0/1
Batch Loss = 0.9657791039000166
1118, epoch_train_loss=0.9657791039000166
Epoch 1119
Epoch 1119 :: Batch 0/1
Batch Loss = 0.965307580078081
1119, epoch_train_loss=0.965307580078081
Epoch 1120
Epoch 1120 :: Batch 0/1
Batch Loss = 0.9648409361814361
1120, epoch_train_loss=0.9648409361814361
Epoch 1121
Epoch 1121 :: Batch 0/1
Batch Loss = 0.9643857154432356
1121, epoch_train_loss=0.9643857154432356
Epoch 1122
Epoch 1122 :: Batch 0/1
Batch Loss = 0.9639430117746359
1122, epoch_train_loss=0.9639430117746359
Epoch 1123
Epoch 1123 :: Batch 0/1
Batch Loss = 0.9635074819478846
1123, epoch_train_loss=0.9635074819478846
Epoch 1124
Epoch 1124 :: Batch 0/1
Batch Loss = 0.9630703776960232
1124, epoch_train_loss=0.9630703776960232
Epoch 1125
Epoch 1125 :: Batch 0/1
Batch Loss = 0.9626244357284208
1125, epoch_train_loss=0.9626244357284208
Epoch 1126
Epoch 1126 :: Batch 0/1
Batch Loss = 0.9621675648229001
1126, epoch_train_loss=0.9621675648229001
Epoch 1127
Epoch 1127 :: Batch 0/1
Batch Loss = 0.9617024632500653
1127, epoch_train_loss=0.9617024632500653
Epoch 1128
Epoch 1128 :: Batch 0/1
Batch Loss = 0.9612339447531548
1128, epoch_train_loss=0.9612339447531548
Epoch 1129
Epoch 1129 :: Batch 0/1
Batch Loss = 0.9607651368185567
1129, epoch_train_loss=0.9607651368185567
Epoch 1130
Epoch 1130 :: Batch 0/1
Batch Loss = 0.9602960455597754
1130, epoch_train_loss=0.9602960455597754
Epoch 1131
Epoch 1131 :: Batch 0/1
Batch Loss = 0.9598242214889682
1131, epoch_train_loss=0.9598242214889682
Epoch 1132
Epoch 1132 :: Batch 0/1
Batch Loss = 0.9593472512314933
1132, epoch_train_loss=0.9593472512314933
Epoch 1133
Epoch 1133 :: Batch 0/1
Batch Loss = 0.9588646753065089
1133, epoch_train_loss=0.9588646753065089
Epoch 1134
Epoch 1134 :: Batch 0/1
Batch Loss = 0.9583783068632589
1134, epoch_train_loss=0.9583783068632589
Epoch 1135
Epoch 1135 :: Batch 0/1
Batch Loss = 0.9578908708108472
1135, epoch_train_loss=0.9578908708108472
Epoch 1136
Epoch 1136 :: Batch 0/1
Batch Loss = 0.957403898559002
1136, epoch_train_loss=0.957403898559002
Epoch 1137
Epoch 1137 :: Batch 0/1
Batch Loss = 0.956916645817732
1137, epoch_train_loss=0.956916645817732
Epoch 1138
Epoch 1138 :: Batch 0/1
Batch Loss = 0.9564264893106879
1138, epoch_train_loss=0.9564264893106879
Epoch 1139
Epoch 1139 :: Batch 0/1
Batch Loss = 0.9559307802976981
1139, epoch_train_loss=0.9559307802976981
Epoch 1140
Epoch 1140 :: Batch 0/1
Batch Loss = 0.955428323036194
1140, epoch_train_loss=0.955428323036194
Epoch 1141
Epoch 1141 :: Batch 0/1
Batch Loss = 0.9549196493219346
1141, epoch_train_loss=0.9549196493219346
Epoch 1142
Epoch 1142 :: Batch 0/1
Batch Loss = 0.9544058364033037
1142, epoch_train_loss=0.9544058364033037
Epoch 1143
Epoch 1143 :: Batch 0/1
Batch Loss = 0.9538871836965651
1143, epoch_train_loss=0.9538871836965651
Epoch 1144
Epoch 1144 :: Batch 0/1
Batch Loss = 0.9533626851777031
1144, epoch_train_loss=0.9533626851777031
Epoch 1145
Epoch 1145 :: Batch 0/1
Batch Loss = 0.9528305095869667
1145, epoch_train_loss=0.9528305095869667
Epoch 1146
Epoch 1146 :: Batch 0/1
Batch Loss = 0.952288867577577
1146, epoch_train_loss=0.952288867577577
Epoch 1147
Epoch 1147 :: Batch 0/1
Batch Loss = 0.9517363889056889
1147, epoch_train_loss=0.9517363889056889
Epoch 1148
Epoch 1148 :: Batch 0/1
Batch Loss = 0.9511717642876357
1148, epoch_train_loss=0.9511717642876357
Epoch 1149
Epoch 1149 :: Batch 0/1
Batch Loss = 0.9505927122704919
1149, epoch_train_loss=0.9505927122704919
Epoch 1150
Epoch 1150 :: Batch 0/1
Batch Loss = 0.9499946218523843
1150, epoch_train_loss=0.9499946218523843
Epoch 1151
Epoch 1151 :: Batch 0/1
Batch Loss = 0.9493671649411004
1151, epoch_train_loss=0.9493671649411004
Epoch 1152
Epoch 1152 :: Batch 0/1
Batch Loss = 0.9486793748165271
1152, epoch_train_loss=0.9486793748165271
Epoch 1153
Epoch 1153 :: Batch 0/1
Batch Loss = 0.9478849428650054
1153, epoch_train_loss=0.9478849428650054
Epoch 1154
Epoch 1154 :: Batch 0/1
Batch Loss = 0.9468759198443102
1154, epoch_train_loss=0.9468759198443102
Epoch 1155
Epoch 1155 :: Batch 0/1
Batch Loss = 848.7158544575902
1155, epoch_train_loss=848.7158544575902
Epoch 1156
Epoch 1156 :: Batch 0/1
Batch Loss = 1639.0235257921
1156, epoch_train_loss=1639.0235257921
Epoch 1157
Epoch 1157 :: Batch 0/1
Batch Loss = 121.09701461438988
1157, epoch_train_loss=121.09701461438988
Epoch 1158
Epoch 1158 :: Batch 0/1
Batch Loss = 1118.9777076915057
1158, epoch_train_loss=1118.9777076915057
Epoch 1159
Epoch 1159 :: Batch 0/1
Batch Loss = 357.5121191000192
1159, epoch_train_loss=357.5121191000192
Epoch 1160
Epoch 1160 :: Batch 0/1
Batch Loss = 614.8264418478241
1160, epoch_train_loss=614.8264418478241
Epoch 1161
Epoch 1161 :: Batch 0/1
Batch Loss = 575.7270853889089
1161, epoch_train_loss=575.7270853889089
Epoch 1162
Epoch 1162 :: Batch 0/1
Batch Loss = 236.24804995744037
1162, epoch_train_loss=236.24804995744037
Epoch 1163
Epoch 1163 :: Batch 0/1
Batch Loss = 675.7595329875128
1163, epoch_train_loss=675.7595329875128
Epoch 1164
Epoch 1164 :: Batch 0/1
Batch Loss = 39.164394973374776
1164, epoch_train_loss=39.164394973374776
Epoch 1165
Epoch 1165 :: Batch 0/1
Batch Loss = 633.2774078325355
1165, epoch_train_loss=633.2774078325355
Epoch 1166
Epoch 1166 :: Batch 0/1
Batch Loss = 5.50823326226961
1166, epoch_train_loss=5.50823326226961
Epoch 1167
Epoch 1167 :: Batch 0/1
Batch Loss = 482.4821075103087
1167, epoch_train_loss=482.4821075103087
Epoch 1168
Epoch 1168 :: Batch 0/1
Batch Loss = 76.43408504873278
1168, epoch_train_loss=76.43408504873278
Epoch 1169
Epoch 1169 :: Batch 0/1
Batch Loss = 288.71672336192586
1169, epoch_train_loss=288.71672336192586
Epoch 1170
Epoch 1170 :: Batch 0/1
Batch Loss = 175.82057468923475
1170, epoch_train_loss=175.82057468923475
Epoch 1171
Epoch 1171 :: Batch 0/1
Batch Loss = 121.4062677661997
1171, epoch_train_loss=121.4062677661997
Epoch 1172
Epoch 1172 :: Batch 0/1
Batch Loss = 239.24680757499502
1172, epoch_train_loss=239.24680757499502
Epoch 1173
Epoch 1173 :: Batch 0/1
Batch Loss = 24.267904253506863
1173, epoch_train_loss=24.267904253506863
Epoch 1174
Epoch 1174 :: Batch 0/1
Batch Loss = 239.9004247319274
1174, epoch_train_loss=239.9004247319274
Epoch 1175
Epoch 1175 :: Batch 0/1
Batch Loss = 1.6255850029946581
1175, epoch_train_loss=1.6255850029946581
Epoch 1176
Epoch 1176 :: Batch 0/1
Batch Loss = 186.60632523946563
1176, epoch_train_loss=186.60632523946563
Epoch 1177
Epoch 1177 :: Batch 0/1
Batch Loss = 29.166496723653083
1177, epoch_train_loss=29.166496723653083
Epoch 1178
Epoch 1178 :: Batch 0/1
Batch Loss = 110.07095372870339
1178, epoch_train_loss=110.07095372870339
Epoch 1179
Epoch 1179 :: Batch 0/1
Batch Loss = 70.54794606750617
1179, epoch_train_loss=70.54794606750617
Epoch 1180
Epoch 1180 :: Batch 0/1
Batch Loss = 43.64885404438187
1180, epoch_train_loss=43.64885404438187
Epoch 1181
Epoch 1181 :: Batch 0/1
Batch Loss = 95.91915614767476
1181, epoch_train_loss=95.91915614767476
Epoch 1182
Epoch 1182 :: Batch 0/1
Batch Loss = 7.3340045456905845
1182, epoch_train_loss=7.3340045456905845
Epoch 1183
Epoch 1183 :: Batch 0/1
Batch Loss = 93.80381779930538
1183, epoch_train_loss=93.80381779930538
Epoch 1184
Epoch 1184 :: Batch 0/1
Batch Loss = 2.097653170753252
1184, epoch_train_loss=2.097653170753252
Epoch 1185
Epoch 1185 :: Batch 0/1
Batch Loss = 69.80856314013025
1185, epoch_train_loss=69.80856314013025
Epoch 1186
Epoch 1186 :: Batch 0/1
Batch Loss = 15.748918603106318
1186, epoch_train_loss=15.748918603106318
Epoch 1187
Epoch 1187 :: Batch 0/1
Batch Loss = 38.452690225612265
1187, epoch_train_loss=38.452690225612265
Epoch 1188
Epoch 1188 :: Batch 0/1
Batch Loss = 32.332319046409246
1188, epoch_train_loss=32.332319046409246
Epoch 1189
Epoch 1189 :: Batch 0/1
Batch Loss = 13.556191975873833
1189, epoch_train_loss=13.556191975873833
Epoch 1190
Epoch 1190 :: Batch 0/1
Batch Loss = 40.42273911853381
1190, epoch_train_loss=40.42273911853381
Epoch 1191
Epoch 1191 :: Batch 0/1
Batch Loss = 1.94624375961074
1191, epoch_train_loss=1.94624375961074
Epoch 1192
Epoch 1192 :: Batch 0/1
Batch Loss = 36.99742392306804
1192, epoch_train_loss=36.99742392306804
Epoch 1193
Epoch 1193 :: Batch 0/1
Batch Loss = 2.375768250225553
1193, epoch_train_loss=2.375768250225553
Epoch 1194
Epoch 1194 :: Batch 0/1
Batch Loss = 25.807407419581843
1194, epoch_train_loss=25.807407419581843
Epoch 1195
Epoch 1195 :: Batch 0/1
Batch Loss = 8.882338949234386
1195, epoch_train_loss=8.882338949234386
Epoch 1196
Epoch 1196 :: Batch 0/1
Batch Loss = 13.224737873167388
1196, epoch_train_loss=13.224737873167388
Epoch 1197
Epoch 1197 :: Batch 0/1
Batch Loss = 15.078841633926523
1197, epoch_train_loss=15.078841633926523
Epoch 1198
Epoch 1198 :: Batch 0/1
Batch Loss = 4.338213930135678
1198, epoch_train_loss=4.338213930135678
Epoch 1199
Epoch 1199 :: Batch 0/1
Batch Loss = 17.15699693675704
1199, epoch_train_loss=17.15699693675704
Epoch 1200
Epoch 1200 :: Batch 0/1
Batch Loss = 1.0040216431057416
1200, epoch_train_loss=1.0040216431057416
Epoch 1201
Epoch 1201 :: Batch 0/1
Batch Loss = 14.749453797352743
1201, epoch_train_loss=14.749453797352743
Epoch 1202
Epoch 1202 :: Batch 0/1
Batch Loss = 2.0400654999679797
1202, epoch_train_loss=2.0400654999679797
Epoch 1203
Epoch 1203 :: Batch 0/1
Batch Loss = 9.833618538692642
1203, epoch_train_loss=9.833618538692642
Epoch 1204
Epoch 1204 :: Batch 0/1
Batch Loss = 4.835938004088927
1204, epoch_train_loss=4.835938004088927
Epoch 1205
Epoch 1205 :: Batch 0/1
Batch Loss = 4.956657083988297
1205, epoch_train_loss=4.956657083988297
Epoch 1206
Epoch 1206 :: Batch 0/1
Batch Loss = 7.025878779552638
1206, epoch_train_loss=7.025878779552638
Epoch 1207
Epoch 1207 :: Batch 0/1
Batch Loss = 1.851867485709217
1207, epoch_train_loss=1.851867485709217
Epoch 1208
Epoch 1208 :: Batch 0/1
Batch Loss = 7.439787165857183
1208, epoch_train_loss=7.439787165857183
Epoch 1209
Epoch 1209 :: Batch 0/1
Batch Loss = 0.9315174241968527
1209, epoch_train_loss=0.9315174241968527
Epoch 1210
Epoch 1210 :: Batch 0/1
Batch Loss = 6.196263615337689
1210, epoch_train_loss=6.196263615337689
Epoch 1211
Epoch 1211 :: Batch 0/1
Batch Loss = 1.549274137109958
1211, epoch_train_loss=1.549274137109958
Epoch 1212
Epoch 1212 :: Batch 0/1
Batch Loss = 4.169533643299661
1212, epoch_train_loss=4.169533643299661
Epoch 1213
Epoch 1213 :: Batch 0/1
Batch Loss = 2.663086524506372
1213, epoch_train_loss=2.663086524506372
Epoch 1214
Epoch 1214 :: Batch 0/1
Batch Loss = 2.3145592812549065
1214, epoch_train_loss=2.3145592812549065
Epoch 1215
Epoch 1215 :: Batch 0/1
Batch Loss = 3.4301382577925583
1215, epoch_train_loss=3.4301382577925583
Epoch 1216
Epoch 1216 :: Batch 0/1
Batch Loss = 1.2087977686541083
1216, epoch_train_loss=1.2087977686541083
Epoch 1217
Epoch 1217 :: Batch 0/1
Batch Loss = 3.492288337315669
1217, epoch_train_loss=3.492288337315669
Epoch 1218
Epoch 1218 :: Batch 0/1
Batch Loss = 0.9315747025510471
1218, epoch_train_loss=0.9315747025510471
Epoch 1219
Epoch 1219 :: Batch 0/1
Batch Loss = 2.9518710243935193
1219, epoch_train_loss=2.9518710243935193
Epoch 1220
Epoch 1220 :: Batch 0/1
Batch Loss = 1.202149752027097
1220, epoch_train_loss=1.202149752027097
Epoch 1221
Epoch 1221 :: Batch 0/1
Batch Loss = 2.1545813783708803
1221, epoch_train_loss=2.1545813783708803
Epoch 1222
Epoch 1222 :: Batch 0/1
Batch Loss = 1.627771673230537
1222, epoch_train_loss=1.627771673230537
Epoch 1223
Epoch 1223 :: Batch 0/1
Batch Loss = 1.4483796395566275
1223, epoch_train_loss=1.4483796395566275
Epoch 1224
Epoch 1224 :: Batch 0/1
Batch Loss = 1.9079413739223219
1224, epoch_train_loss=1.9079413739223219
Epoch 1225
Epoch 1225 :: Batch 0/1
Batch Loss = 1.0324401544897568
1225, epoch_train_loss=1.0324401544897568
Epoch 1226
Epoch 1226 :: Batch 0/1
Batch Loss = 1.923028638143531
1226, epoch_train_loss=1.923028638143531
Epoch 1227
Epoch 1227 :: Batch 0/1
Batch Loss = 0.9260160639463874
1227, epoch_train_loss=0.9260160639463874
Epoch 1228
Epoch 1228 :: Batch 0/1
Batch Loss = 1.7161469155524396
1228, epoch_train_loss=1.7161469155524396
Epoch 1229
Epoch 1229 :: Batch 0/1
Batch Loss = 1.0234158486100695
1229, epoch_train_loss=1.0234158486100695
Epoch 1230
Epoch 1230 :: Batch 0/1
Batch Loss = 1.4138176043877255
1230, epoch_train_loss=1.4138176043877255
Epoch 1231
Epoch 1231 :: Batch 0/1
Batch Loss = 1.1816586026564357
1231, epoch_train_loss=1.1816586026564357
Epoch 1232
Epoch 1232 :: Batch 0/1
Batch Loss = 1.1420272548034107
1232, epoch_train_loss=1.1420272548034107
Epoch 1233
Epoch 1233 :: Batch 0/1
Batch Loss = 1.2905670581573068
1233, epoch_train_loss=1.2905670581573068
Epoch 1234
Epoch 1234 :: Batch 0/1
Batch Loss = 0.9749842460000535
1234, epoch_train_loss=0.9749842460000535
Epoch 1235
Epoch 1235 :: Batch 0/1
Batch Loss = 1.3047389621588645
1235, epoch_train_loss=1.3047389621588645
Epoch 1236
Epoch 1236 :: Batch 0/1
Batch Loss = 0.9220002270086318
1236, epoch_train_loss=0.9220002270086318
Epoch 1237
Epoch 1237 :: Batch 0/1
Batch Loss = 1.235718969988368
1237, epoch_train_loss=1.235718969988368
Epoch 1238
Epoch 1238 :: Batch 0/1
Batch Loss = 0.9490425173016876
1238, epoch_train_loss=0.9490425173016876
Epoch 1239
Epoch 1239 :: Batch 0/1
Batch Loss = 1.1258583029292184
1239, epoch_train_loss=1.1258583029292184
Epoch 1240
Epoch 1240 :: Batch 0/1
Batch Loss = 1.0053608876763378
1240, epoch_train_loss=1.0053608876763378
Epoch 1241
Epoch 1241 :: Batch 0/1
Batch Loss = 1.0209265823757276
1241, epoch_train_loss=1.0209265823757276
Epoch 1242
Epoch 1242 :: Batch 0/1
Batch Loss = 1.0498249105117567
1242, epoch_train_loss=1.0498249105117567
Epoch 1243
Epoch 1243 :: Batch 0/1
Batch Loss = 0.9500554998622004
1243, epoch_train_loss=0.9500554998622004
Epoch 1244
Epoch 1244 :: Batch 0/1
Batch Loss = 1.062315722682472
1244, epoch_train_loss=1.062315722682472
Epoch 1245
Epoch 1245 :: Batch 0/1
Batch Loss = 0.9206311171140844
1245, epoch_train_loss=0.9206311171140844
Epoch 1246
Epoch 1246 :: Batch 0/1
Batch Loss = 1.0433353157528775
1246, epoch_train_loss=1.0433353157528775
Epoch 1247
Epoch 1247 :: Batch 0/1
Batch Loss = 0.9232042007468619
1247, epoch_train_loss=0.9232042007468619
Epoch 1248
Epoch 1248 :: Batch 0/1
Batch Loss = 1.0059459349447193
1248, epoch_train_loss=1.0059459349447193
Epoch 1249
Epoch 1249 :: Batch 0/1
Batch Loss = 0.941001755410697
1249, epoch_train_loss=0.941001755410697
Epoch 1250
Epoch 1250 :: Batch 0/1
Batch Loss = 0.9660932434561866
1250, epoch_train_loss=0.9660932434561866
Epoch 1251
Epoch 1251 :: Batch 0/1
Batch Loss = 0.9585972121096337
1251, epoch_train_loss=0.9585972121096337
Epoch 1252
Epoch 1252 :: Batch 0/1
Batch Loss = 0.9355611517745884
1252, epoch_train_loss=0.9355611517745884
Epoch 1253
Epoch 1253 :: Batch 0/1
Batch Loss = 0.9668509991381697
1253, epoch_train_loss=0.9668509991381697
Epoch 1254
Epoch 1254 :: Batch 0/1
Batch Loss = 0.919121718491489
1254, epoch_train_loss=0.919121718491489
Epoch 1255
Epoch 1255 :: Batch 0/1
Batch Loss = 0.9637606550186825
1255, epoch_train_loss=0.9637606550186825
Epoch 1256
Epoch 1256 :: Batch 0/1
Batch Loss = 0.915306389812927
1256, epoch_train_loss=0.915306389812927
Epoch 1257
Epoch 1257 :: Batch 0/1
Batch Loss = 0.9524318249747328
1257, epoch_train_loss=0.9524318249747328
Epoch 1258
Epoch 1258 :: Batch 0/1
Batch Loss = 0.9191525624985697
1258, epoch_train_loss=0.9191525624985697
Epoch 1259
Epoch 1259 :: Batch 0/1
Batch Loss = 0.9380226246614294
1259, epoch_train_loss=0.9380226246614294
Epoch 1260
Epoch 1260 :: Batch 0/1
Batch Loss = 0.925153105148086
1260, epoch_train_loss=0.925153105148086
Epoch 1261
Epoch 1261 :: Batch 0/1
Batch Loss = 0.9251383347649415
1261, epoch_train_loss=0.9251383347649415
Epoch 1262
Epoch 1262 :: Batch 0/1
Batch Loss = 0.9292977609199105
1262, epoch_train_loss=0.9292977609199105
Epoch 1263
Epoch 1263 :: Batch 0/1
Batch Loss = 0.9164088621193814
1263, epoch_train_loss=0.9164088621193814
Epoch 1264
Epoch 1264 :: Batch 0/1
Batch Loss = 0.9298426782616762
1264, epoch_train_loss=0.9298426782616762
Epoch 1265
Epoch 1265 :: Batch 0/1
Batch Loss = 0.9122897571935141
1265, epoch_train_loss=0.9122897571935141
Epoch 1266
Epoch 1266 :: Batch 0/1
Batch Loss = 0.927019867847901
1266, epoch_train_loss=0.927019867847901
Epoch 1267
Epoch 1267 :: Batch 0/1
Batch Loss = 0.9116859089950589
1267, epoch_train_loss=0.9116859089950589
Epoch 1268
Epoch 1268 :: Batch 0/1
Batch Loss = 0.922199978695962
1268, epoch_train_loss=0.922199978695962
Epoch 1269
Epoch 1269 :: Batch 0/1
Batch Loss = 0.9128613600571557
1269, epoch_train_loss=0.9128613600571557
Epoch 1270
Epoch 1270 :: Batch 0/1
Batch Loss = 0.9169980634234637
1270, epoch_train_loss=0.9169980634234637
Epoch 1271
Epoch 1271 :: Batch 0/1
Batch Loss = 0.9142233700126554
1271, epoch_train_loss=0.9142233700126554
Epoch 1272
Epoch 1272 :: Batch 0/1
Batch Loss = 0.9126405411291236
1272, epoch_train_loss=0.9126405411291236
Epoch 1273
Epoch 1273 :: Batch 0/1
Batch Loss = 0.914766961418773
1273, epoch_train_loss=0.914766961418773
Epoch 1274
Epoch 1274 :: Batch 0/1
Batch Loss = 0.9097022108759324
1274, epoch_train_loss=0.9097022108759324
Epoch 1275
Epoch 1275 :: Batch 0/1
Batch Loss = 0.9141631566617809
1275, epoch_train_loss=0.9141631566617809
Epoch 1276
Epoch 1276 :: Batch 0/1
Batch Loss = 0.9081551937479289
1276, epoch_train_loss=0.9081551937479289
Epoch 1277
Epoch 1277 :: Batch 0/1
Batch Loss = 0.9126013865691412
1277, epoch_train_loss=0.9126013865691412
Epoch 1278
Epoch 1278 :: Batch 0/1
Batch Loss = 0.907592039395628
1278, epoch_train_loss=0.907592039395628
Epoch 1279
Epoch 1279 :: Batch 0/1
Batch Loss = 0.9105297970068565
1279, epoch_train_loss=0.9105297970068565
Epoch 1280
Epoch 1280 :: Batch 0/1
Batch Loss = 0.907485539962607
1280, epoch_train_loss=0.907485539962607
Epoch 1281
Epoch 1281 :: Batch 0/1
Batch Loss = 0.9084149577700721
1281, epoch_train_loss=0.9084149577700721
Epoch 1282
Epoch 1282 :: Batch 0/1
Batch Loss = 0.9073881479900878
1282, epoch_train_loss=0.9073881479900878
Epoch 1283
Epoch 1283 :: Batch 0/1
Batch Loss = 0.906588578676147
1283, epoch_train_loss=0.906588578676147
Epoch 1284
Epoch 1284 :: Batch 0/1
Batch Loss = 0.9070322806896236
1284, epoch_train_loss=0.9070322806896236
Epoch 1285
Epoch 1285 :: Batch 0/1
Batch Loss = 0.9051943285879612
1285, epoch_train_loss=0.9051943285879612
Epoch 1286
Epoch 1286 :: Batch 0/1
Batch Loss = 0.906339227505468
1286, epoch_train_loss=0.906339227505468
Epoch 1287
Epoch 1287 :: Batch 0/1
Batch Loss = 0.9042120528678157
1287, epoch_train_loss=0.9042120528678157
Epoch 1288
Epoch 1288 :: Batch 0/1
Batch Loss = 0.9053688482702511
1288, epoch_train_loss=0.9053688482702511
Epoch 1289
Epoch 1289 :: Batch 0/1
Batch Loss = 0.9035221221351492
1289, epoch_train_loss=0.9035221221351492
Epoch 1290
Epoch 1290 :: Batch 0/1
Batch Loss = 0.9042482137426167
1290, epoch_train_loss=0.9042482137426167
Epoch 1291
Epoch 1291 :: Batch 0/1
Batch Loss = 0.9029748058740525
1291, epoch_train_loss=0.9029748058740525
Epoch 1292
Epoch 1292 :: Batch 0/1
Batch Loss = 0.9031083283927117
1292, epoch_train_loss=0.9031083283927117
Epoch 1293
Epoch 1293 :: Batch 0/1
Batch Loss = 0.9024423723005477
1293, epoch_train_loss=0.9024423723005477
Epoch 1294
Epoch 1294 :: Batch 0/1
Batch Loss = 0.9020441494298063
1294, epoch_train_loss=0.9020441494298063
Epoch 1295
Epoch 1295 :: Batch 0/1
Batch Loss = 0.9018453790789512
1295, epoch_train_loss=0.9018453790789512
Epoch 1296
Epoch 1296 :: Batch 0/1
Batch Loss = 0.9011003792401875
1296, epoch_train_loss=0.9011003792401875
Epoch 1297
Epoch 1297 :: Batch 0/1
Batch Loss = 0.9011555324927956
1297, epoch_train_loss=0.9011555324927956
Epoch 1298
Epoch 1298 :: Batch 0/1
Batch Loss = 0.9002769869558295
1298, epoch_train_loss=0.9002769869558295
Epoch 1299
Epoch 1299 :: Batch 0/1
Batch Loss = 0.9003834531803198
1299, epoch_train_loss=0.9003834531803198
Epoch 1300
Epoch 1300 :: Batch 0/1
Batch Loss = 0.8995451491378441
1300, epoch_train_loss=0.8995451491378441
Epoch 1301
Epoch 1301 :: Batch 0/1
Batch Loss = 0.8995605622054964
1301, epoch_train_loss=0.8995605622054964
Epoch 1302
Epoch 1302 :: Batch 0/1
Batch Loss = 0.8988651267306588
1302, epoch_train_loss=0.8988651267306588
Epoch 1303
Epoch 1303 :: Batch 0/1
Batch Loss = 0.8987223740439251
1303, epoch_train_loss=0.8987223740439251
Epoch 1304
Epoch 1304 :: Batch 0/1
Batch Loss = 0.8982003522127067
1304, epoch_train_loss=0.8982003522127067
Epoch 1305
Epoch 1305 :: Batch 0/1
Batch Loss = 0.8978971897482257
1305, epoch_train_loss=0.8978971897482257
Epoch 1306
Epoch 1306 :: Batch 0/1
Batch Loss = 0.8975253748967843
1306, epoch_train_loss=0.8975253748967843
Epoch 1307
Epoch 1307 :: Batch 0/1
Batch Loss = 0.8971010996405636
1307, epoch_train_loss=0.8971010996405636
Epoch 1308
Epoch 1308 :: Batch 0/1
Batch Loss = 0.8968279239816099
1308, epoch_train_loss=0.8968279239816099
Epoch 1309
Epoch 1309 :: Batch 0/1
Batch Loss = 0.8963381498056183
1309, epoch_train_loss=0.8963381498056183
Epoch 1310
Epoch 1310 :: Batch 0/1
Batch Loss = 0.8961068146268345
1310, epoch_train_loss=0.8961068146268345
Epoch 1311
Epoch 1311 :: Batch 0/1
Batch Loss = 0.8956035996918215
1311, epoch_train_loss=0.8956035996918215
Epoch 1312
Epoch 1312 :: Batch 0/1
Batch Loss = 0.8953678698004832
1312, epoch_train_loss=0.8953678698004832
Epoch 1313
Epoch 1313 :: Batch 0/1
Batch Loss = 0.894888197989276
1313, epoch_train_loss=0.894888197989276
Epoch 1314
Epoch 1314 :: Batch 0/1
Batch Loss = 0.8946196690085931
1314, epoch_train_loss=0.8946196690085931
Epoch 1315
Epoch 1315 :: Batch 0/1
Batch Loss = 0.8941819812893524
1315, epoch_train_loss=0.8941819812893524
Epoch 1316
Epoch 1316 :: Batch 0/1
Batch Loss = 0.8938702666042472
1316, epoch_train_loss=0.8938702666042472
Epoch 1317
Epoch 1317 :: Batch 0/1
Batch Loss = 0.893476810130114
1317, epoch_train_loss=0.893476810130114
Epoch 1318
Epoch 1318 :: Batch 0/1
Batch Loss = 0.8931253177333102
1318, epoch_train_loss=0.8931253177333102
Epoch 1319
Epoch 1319 :: Batch 0/1
Batch Loss = 0.8927674982925539
1319, epoch_train_loss=0.8927674982925539
Epoch 1320
Epoch 1320 :: Batch 0/1
Batch Loss = 0.8923875163137088
1320, epoch_train_loss=0.8923875163137088
Epoch 1321
Epoch 1321 :: Batch 0/1
Batch Loss = 0.892051805764945
1321, epoch_train_loss=0.892051805764945
Epoch 1322
Epoch 1322 :: Batch 0/1
Batch Loss = 0.8916569666171256
1322, epoch_train_loss=0.8916569666171256
Epoch 1323
Epoch 1323 :: Batch 0/1
Batch Loss = 0.8913297521843486
1323, epoch_train_loss=0.8913297521843486
Epoch 1324
Epoch 1324 :: Batch 0/1
Batch Loss = 0.8909320606335464
1324, epoch_train_loss=0.8909320606335464
Epoch 1325
Epoch 1325 :: Batch 0/1
Batch Loss = 0.8906026839124174
1325, epoch_train_loss=0.8906026839124174
Epoch 1326
Epoch 1326 :: Batch 0/1
Batch Loss = 0.8902102993486141
1326, epoch_train_loss=0.8902102993486141
Epoch 1327
Epoch 1327 :: Batch 0/1
Batch Loss = 0.8898720415406051
1327, epoch_train_loss=0.8898720415406051
Epoch 1328
Epoch 1328 :: Batch 0/1
Batch Loss = 0.8894885860850754
1328, epoch_train_loss=0.8894885860850754
Epoch 1329
Epoch 1329 :: Batch 0/1
Batch Loss = 0.8891386090511121
1329, epoch_train_loss=0.8891386090511121
Epoch 1330
Epoch 1330 :: Batch 0/1
Batch Loss = 0.8887643931664282
1330, epoch_train_loss=0.8887643931664282
Epoch 1331
Epoch 1331 :: Batch 0/1
Batch Loss = 0.8884034562733147
1331, epoch_train_loss=0.8884034562733147
Epoch 1332
Epoch 1332 :: Batch 0/1
Batch Loss = 0.8880368116049252
1332, epoch_train_loss=0.8880368116049252
Epoch 1333
Epoch 1333 :: Batch 0/1
Batch Loss = 0.8876673725968955
1333, epoch_train_loss=0.8876673725968955
Epoch 1334
Epoch 1334 :: Batch 0/1
Batch Loss = 0.8873052129564221
1334, epoch_train_loss=0.8873052129564221
Epoch 1335
Epoch 1335 :: Batch 0/1
Batch Loss = 0.8869301981545405
1335, epoch_train_loss=0.8869301981545405
Epoch 1336
Epoch 1336 :: Batch 0/1
Batch Loss = 0.8865693993942416
1336, epoch_train_loss=0.8865693993942416
Epoch 1337
Epoch 1337 :: Batch 0/1
Batch Loss = 0.886191600428788
1337, epoch_train_loss=0.886191600428788
Epoch 1338
Epoch 1338 :: Batch 0/1
Batch Loss = 0.8858297056249709
1338, epoch_train_loss=0.8858297056249709
Epoch 1339
Epoch 1339 :: Batch 0/1
Batch Loss = 0.8854513017900337
1339, epoch_train_loss=0.8854513017900337
Epoch 1340
Epoch 1340 :: Batch 0/1
Batch Loss = 0.8850868316081447
1340, epoch_train_loss=0.8850868316081447
Epoch 1341
Epoch 1341 :: Batch 0/1
Batch Loss = 0.8847092614422843
1341, epoch_train_loss=0.8847092614422843
Epoch 1342
Epoch 1342 :: Batch 0/1
Batch Loss = 0.8843417303040356
1342, epoch_train_loss=0.8843417303040356
Epoch 1343
Epoch 1343 :: Batch 0/1
Batch Loss = 0.8839657608821903
1343, epoch_train_loss=0.8839657608821903
Epoch 1344
Epoch 1344 :: Batch 0/1
Batch Loss = 0.8835954660002525
1344, epoch_train_loss=0.8835954660002525
Epoch 1345
Epoch 1345 :: Batch 0/1
Batch Loss = 0.8832213299721138
1345, epoch_train_loss=0.8832213299721138
Epoch 1346
Epoch 1346 :: Batch 0/1
Batch Loss = 0.8828490156723718
1346, epoch_train_loss=0.8828490156723718
Epoch 1347
Epoch 1347 :: Batch 0/1
Batch Loss = 0.8824765517041061
1347, epoch_train_loss=0.8824765517041061
Epoch 1348
Epoch 1348 :: Batch 0/1
Batch Loss = 0.8821030751174149
1348, epoch_train_loss=0.8821030751174149
Epoch 1349
Epoch 1349 :: Batch 0/1
Batch Loss = 0.8817318701448379
1349, epoch_train_loss=0.8817318701448379
Epoch 1350
Epoch 1350 :: Batch 0/1
Batch Loss = 0.8813579705349206
1350, epoch_train_loss=0.8813579705349206
Epoch 1351
Epoch 1351 :: Batch 0/1
Batch Loss = 0.8809875108262848
1351, epoch_train_loss=0.8809875108262848
Epoch 1352
Epoch 1352 :: Batch 0/1
Batch Loss = 0.8806137139131293
1352, epoch_train_loss=0.8806137139131293
Epoch 1353
Epoch 1353 :: Batch 0/1
Batch Loss = 0.8802435267021282
1353, epoch_train_loss=0.8802435267021282
Epoch 1354
Epoch 1354 :: Batch 0/1
Batch Loss = 0.8798701528816436
1354, epoch_train_loss=0.8798701528816436
Epoch 1355
Epoch 1355 :: Batch 0/1
Batch Loss = 0.8794999087098833
1355, epoch_train_loss=0.8794999087098833
Epoch 1356
Epoch 1356 :: Batch 0/1
Batch Loss = 0.8791271292121842
1356, epoch_train_loss=0.8791271292121842
Epoch 1357
Epoch 1357 :: Batch 0/1
Batch Loss = 0.8787566826101493
1357, epoch_train_loss=0.8787566826101493
Epoch 1358
Epoch 1358 :: Batch 0/1
Batch Loss = 0.8783845741991461
1358, epoch_train_loss=0.8783845741991461
Epoch 1359
Epoch 1359 :: Batch 0/1
Batch Loss = 0.8780139398605102
1359, epoch_train_loss=0.8780139398605102
Epoch 1360
Epoch 1360 :: Batch 0/1
Batch Loss = 0.8776425172377917
1360, epoch_train_loss=0.8776425172377917
Epoch 1361
Epoch 1361 :: Batch 0/1
Batch Loss = 0.8772718174454284
1361, epoch_train_loss=0.8772718174454284
Epoch 1362
Epoch 1362 :: Batch 0/1
Batch Loss = 0.8769010706330568
1362, epoch_train_loss=0.8769010706330568
Epoch 1363
Epoch 1363 :: Batch 0/1
Batch Loss = 0.8765304968829256
1363, epoch_train_loss=0.8765304968829256
Epoch 1364
Epoch 1364 :: Batch 0/1
Batch Loss = 0.8761604030503146
1364, epoch_train_loss=0.8761604030503146
Epoch 1365
Epoch 1365 :: Batch 0/1
Batch Loss = 0.8757900899712786
1365, epoch_train_loss=0.8757900899712786
Epoch 1366
Epoch 1366 :: Batch 0/1
Batch Loss = 0.87542044713358
1366, epoch_train_loss=0.87542044713358
Epoch 1367
Epoch 1367 :: Batch 0/1
Batch Loss = 0.8750502193850501
1367, epoch_train_loss=0.8750502193850501
Epoch 1368
Epoch 1368 :: Batch 0/1
Batch Loss = 0.8746805211023586
1368, epoch_train_loss=0.8746805211023586
Epoch 1369
Epoch 1369 :: Batch 0/1
Batch Loss = 0.8743099516365656
1369, epoch_train_loss=0.8743099516365656
Epoch 1370
Epoch 1370 :: Batch 0/1
Batch Loss = 0.8739396577998667
1370, epoch_train_loss=0.8739396577998667
Epoch 1371
Epoch 1371 :: Batch 0/1
Batch Loss = 0.873568384038467
1371, epoch_train_loss=0.873568384038467
Epoch 1372
Epoch 1372 :: Batch 0/1
Batch Loss = 0.8731971311857605
1372, epoch_train_loss=0.8731971311857605
Epoch 1373
Epoch 1373 :: Batch 0/1
Batch Loss = 0.8728248792436362
1373, epoch_train_loss=0.8728248792436362
Epoch 1374
Epoch 1374 :: Batch 0/1
Batch Loss = 0.8724523253127667
1374, epoch_train_loss=0.8724523253127667
Epoch 1375
Epoch 1375 :: Batch 0/1
Batch Loss = 0.8720786690856175
1375, epoch_train_loss=0.8720786690856175
Epoch 1376
Epoch 1376 :: Batch 0/1
Batch Loss = 0.8717042398707142
1376, epoch_train_loss=0.8717042398707142
Epoch 1377
Epoch 1377 :: Batch 0/1
Batch Loss = 0.8713284265205482
1377, epoch_train_loss=0.8713284265205482
Epoch 1378
Epoch 1378 :: Batch 0/1
Batch Loss = 0.8709512110778143
1378, epoch_train_loss=0.8709512110778143
Epoch 1379
Epoch 1379 :: Batch 0/1
Batch Loss = 0.8705721110212373
1379, epoch_train_loss=0.8705721110212373
Epoch 1380
Epoch 1380 :: Batch 0/1
Batch Loss = 0.8701907784330668
1380, epoch_train_loss=0.8701907784330668
Epoch 1381
Epoch 1381 :: Batch 0/1
Batch Loss = 0.8698067311130908
1381, epoch_train_loss=0.8698067311130908
Epoch 1382
Epoch 1382 :: Batch 0/1
Batch Loss = 0.8694192559514725
1382, epoch_train_loss=0.8694192559514725
Epoch 1383
Epoch 1383 :: Batch 0/1
Batch Loss = 0.8690276815560525
1383, epoch_train_loss=0.8690276815560525
Epoch 1384
Epoch 1384 :: Batch 0/1
Batch Loss = 0.8686308407436711
1384, epoch_train_loss=0.8686308407436711
Epoch 1385
Epoch 1385 :: Batch 0/1
Batch Loss = 0.868227672941041
1385, epoch_train_loss=0.868227672941041
Epoch 1386
Epoch 1386 :: Batch 0/1
Batch Loss = 0.8678165153755787
1386, epoch_train_loss=0.8678165153755787
Epoch 1387
Epoch 1387 :: Batch 0/1
Batch Loss = 0.8673959506855966
1387, epoch_train_loss=0.8673959506855966
Epoch 1388
Epoch 1388 :: Batch 0/1
Batch Loss = 0.866964275856024
1388, epoch_train_loss=0.866964275856024
Epoch 1389
Epoch 1389 :: Batch 0/1
Batch Loss = 0.8665206979495979
1389, epoch_train_loss=0.8665206979495979
Epoch 1390
Epoch 1390 :: Batch 0/1
Batch Loss = 0.8660652787666826
1390, epoch_train_loss=0.8660652787666826
Epoch 1391
Epoch 1391 :: Batch 0/1
Batch Loss = 0.8656002331171166
1391, epoch_train_loss=0.8656002331171166
Epoch 1392
Epoch 1392 :: Batch 0/1
Batch Loss = 0.8651295290901432
1392, epoch_train_loss=0.8651295290901432
Epoch 1393
Epoch 1393 :: Batch 0/1
Batch Loss = 0.8646585842716523
1393, epoch_train_loss=0.8646585842716523
Epoch 1394
Epoch 1394 :: Batch 0/1
Batch Loss = 0.8641920899480687
1394, epoch_train_loss=0.8641920899480687
Epoch 1395
Epoch 1395 :: Batch 0/1
Batch Loss = 0.8637327554072415
1395, epoch_train_loss=0.8637327554072415
Epoch 1396
Epoch 1396 :: Batch 0/1
Batch Loss = 0.8632808029488304
1396, epoch_train_loss=0.8632808029488304
Epoch 1397
Epoch 1397 :: Batch 0/1
Batch Loss = 0.8628352702011122
1397, epoch_train_loss=0.8628352702011122
Epoch 1398
Epoch 1398 :: Batch 0/1
Batch Loss = 0.8623950133124786
1398, epoch_train_loss=0.8623950133124786
Epoch 1399
Epoch 1399 :: Batch 0/1
Batch Loss = 0.8619592472558288
1399, epoch_train_loss=0.8619592472558288
Epoch 1400
Epoch 1400 :: Batch 0/1
Batch Loss = 0.8615274177603356
1400, epoch_train_loss=0.8615274177603356
Epoch 1401
Epoch 1401 :: Batch 0/1
Batch Loss = 0.8610990613022914
1401, epoch_train_loss=0.8610990613022914
Epoch 1402
Epoch 1402 :: Batch 0/1
Batch Loss = 0.8606737457807974
1402, epoch_train_loss=0.8606737457807974
Epoch 1403
Epoch 1403 :: Batch 0/1
Batch Loss = 0.8602510082978057
1403, epoch_train_loss=0.8602510082978057
Epoch 1404
Epoch 1404 :: Batch 0/1
Batch Loss = 0.8598304217351407
1404, epoch_train_loss=0.8598304217351407
Epoch 1405
Epoch 1405 :: Batch 0/1
Batch Loss = 0.8594115355963178
1405, epoch_train_loss=0.8594115355963178
Epoch 1406
Epoch 1406 :: Batch 0/1
Batch Loss = 0.8589939763460128
1406, epoch_train_loss=0.8589939763460128
Epoch 1407
Epoch 1407 :: Batch 0/1
Batch Loss = 0.8585773693836426
1407, epoch_train_loss=0.8585773693836426
Epoch 1408
Epoch 1408 :: Batch 0/1
Batch Loss = 0.8581614290094651
1408, epoch_train_loss=0.8581614290094651
Epoch 1409
Epoch 1409 :: Batch 0/1
Batch Loss = 0.8577458669242609
1409, epoch_train_loss=0.8577458669242609
Epoch 1410
Epoch 1410 :: Batch 0/1
Batch Loss = 0.857330473927927
1410, epoch_train_loss=0.857330473927927
Epoch 1411
Epoch 1411 :: Batch 0/1
Batch Loss = 0.8569150447361956
1411, epoch_train_loss=0.8569150447361956
Epoch 1412
Epoch 1412 :: Batch 0/1
Batch Loss = 0.8564994571693818
1412, epoch_train_loss=0.8564994571693818
Epoch 1413
Epoch 1413 :: Batch 0/1
Batch Loss = 0.8560836137032258
1413, epoch_train_loss=0.8560836137032258
Epoch 1414
Epoch 1414 :: Batch 0/1
Batch Loss = 0.8556675010533304
1414, epoch_train_loss=0.8556675010533304
Epoch 1415
Epoch 1415 :: Batch 0/1
Batch Loss = 0.855251137390957
1415, epoch_train_loss=0.855251137390957
Epoch 1416
Epoch 1416 :: Batch 0/1
Batch Loss = 0.8548346068699594
1416, epoch_train_loss=0.8548346068699594
Epoch 1417
Epoch 1417 :: Batch 0/1
Batch Loss = 0.8544180131749354
1417, epoch_train_loss=0.8544180131749354
Epoch 1418
Epoch 1418 :: Batch 0/1
Batch Loss = 0.8540014942878347
1418, epoch_train_loss=0.8540014942878347
Epoch 1419
Epoch 1419 :: Batch 0/1
Batch Loss = 0.8535851849745697
1419, epoch_train_loss=0.8535851849745697
Epoch 1420
Epoch 1420 :: Batch 0/1
Batch Loss = 0.853169219403669
1420, epoch_train_loss=0.853169219403669
Epoch 1421
Epoch 1421 :: Batch 0/1
Batch Loss = 0.852753706031755
1421, epoch_train_loss=0.852753706031755
Epoch 1422
Epoch 1422 :: Batch 0/1
Batch Loss = 0.8523387256847581
1422, epoch_train_loss=0.8523387256847581
Epoch 1423
Epoch 1423 :: Batch 0/1
Batch Loss = 0.8519243197761747
1423, epoch_train_loss=0.8519243197761747
Epoch 1424
Epoch 1424 :: Batch 0/1
Batch Loss = 0.8515104877943409
1424, epoch_train_loss=0.8515104877943409
Epoch 1425
Epoch 1425 :: Batch 0/1
Batch Loss = 0.8510971869564831
1425, epoch_train_loss=0.8510971869564831
Epoch 1426
Epoch 1426 :: Batch 0/1
Batch Loss = 0.8506843318333193
1426, epoch_train_loss=0.8506843318333193
Epoch 1427
Epoch 1427 :: Batch 0/1
Batch Loss = 0.8502718035323947
1427, epoch_train_loss=0.8502718035323947
Epoch 1428
Epoch 1428 :: Batch 0/1
Batch Loss = 0.8498594509522145
1428, epoch_train_loss=0.8498594509522145
Epoch 1429
Epoch 1429 :: Batch 0/1
Batch Loss = 0.8494471025869459
1429, epoch_train_loss=0.8494471025869459
Epoch 1430
Epoch 1430 :: Batch 0/1
Batch Loss = 0.8490345630902015
1430, epoch_train_loss=0.8490345630902015
Epoch 1431
Epoch 1431 :: Batch 0/1
Batch Loss = 0.8486216185639172
1431, epoch_train_loss=0.8486216185639172
Epoch 1432
Epoch 1432 :: Batch 0/1
Batch Loss = 0.8482080230068216
1432, epoch_train_loss=0.8482080230068216
Epoch 1433
Epoch 1433 :: Batch 0/1
Batch Loss = 0.8477934923527526
1433, epoch_train_loss=0.8477934923527526
Epoch 1434
Epoch 1434 :: Batch 0/1
Batch Loss = 0.8473776799037542
1434, epoch_train_loss=0.8473776799037542
Epoch 1435
Epoch 1435 :: Batch 0/1
Batch Loss = 0.8469601595793919
1435, epoch_train_loss=0.8469601595793919
Epoch 1436
Epoch 1436 :: Batch 0/1
Batch Loss = 0.8465403956939334
1436, epoch_train_loss=0.8465403956939334
Epoch 1437
Epoch 1437 :: Batch 0/1
Batch Loss = 0.8461177287630879
1437, epoch_train_loss=0.8461177287630879
Epoch 1438
Epoch 1438 :: Batch 0/1
Batch Loss = 0.8456913719901034
1438, epoch_train_loss=0.8456913719901034
Epoch 1439
Epoch 1439 :: Batch 0/1
Batch Loss = 0.8452604665198602
1439, epoch_train_loss=0.8452604665198602
Epoch 1440
Epoch 1440 :: Batch 0/1
Batch Loss = 0.8448242251063242
1440, epoch_train_loss=0.8448242251063242
Epoch 1441
Epoch 1441 :: Batch 0/1
Batch Loss = 0.8443822287806033
1441, epoch_train_loss=0.8443822287806033
Epoch 1442
Epoch 1442 :: Batch 0/1
Batch Loss = 0.8439348590902462
1442, epoch_train_loss=0.8439348590902462
Epoch 1443
Epoch 1443 :: Batch 0/1
Batch Loss = 0.8434837207639961
1443, epoch_train_loss=0.8434837207639961
Epoch 1444
Epoch 1444 :: Batch 0/1
Batch Loss = 0.8430316911573676
1444, epoch_train_loss=0.8430316911573676
Epoch 1445
Epoch 1445 :: Batch 0/1
Batch Loss = 0.8425822833859687
1445, epoch_train_loss=0.8425822833859687
Epoch 1446
Epoch 1446 :: Batch 0/1
Batch Loss = 0.8421384904491953
1446, epoch_train_loss=0.8421384904491953
Epoch 1447
Epoch 1447 :: Batch 0/1
Batch Loss = 0.8417018523857394
1447, epoch_train_loss=0.8417018523857394
Epoch 1448
Epoch 1448 :: Batch 0/1
Batch Loss = 0.8412723338977879
1448, epoch_train_loss=0.8412723338977879
Epoch 1449
Epoch 1449 :: Batch 0/1
Batch Loss = 0.8408488726284956
1449, epoch_train_loss=0.8408488726284956
Epoch 1450
Epoch 1450 :: Batch 0/1
Batch Loss = 0.8404300398422656
1450, epoch_train_loss=0.8404300398422656
Epoch 1451
Epoch 1451 :: Batch 0/1
Batch Loss = 0.8400144578526022
1451, epoch_train_loss=0.8400144578526022
Epoch 1452
Epoch 1452 :: Batch 0/1
Batch Loss = 0.8396009563370387
1452, epoch_train_loss=0.8396009563370387
Epoch 1453
Epoch 1453 :: Batch 0/1
Batch Loss = 0.8391885950941652
1453, epoch_train_loss=0.8391885950941652
Epoch 1454
Epoch 1454 :: Batch 0/1
Batch Loss = 0.8387766542181138
1454, epoch_train_loss=0.8387766542181138
Epoch 1455
Epoch 1455 :: Batch 0/1
Batch Loss = 0.838364627410992
1455, epoch_train_loss=0.838364627410992
Epoch 1456
Epoch 1456 :: Batch 0/1
Batch Loss = 0.8379522128071875
1456, epoch_train_loss=0.8379522128071875
Epoch 1457
Epoch 1457 :: Batch 0/1
Batch Loss = 0.8375392880932476
1457, epoch_train_loss=0.8375392880932476
Epoch 1458
Epoch 1458 :: Batch 0/1
Batch Loss = 0.8371258686168467
1458, epoch_train_loss=0.8371258686168467
Epoch 1459
Epoch 1459 :: Batch 0/1
Batch Loss = 0.8367120571346968
1459, epoch_train_loss=0.8367120571346968
Epoch 1460
Epoch 1460 :: Batch 0/1
Batch Loss = 0.8362979977769697
1460, epoch_train_loss=0.8362979977769697
Epoch 1461
Epoch 1461 :: Batch 0/1
Batch Loss = 0.8358838413648221
1461, epoch_train_loss=0.8358838413648221
Epoch 1462
Epoch 1462 :: Batch 0/1
Batch Loss = 0.8354697245675832
1462, epoch_train_loss=0.8354697245675832
Epoch 1463
Epoch 1463 :: Batch 0/1
Batch Loss = 0.8350557593135425
1463, epoch_train_loss=0.8350557593135425
Epoch 1464
Epoch 1464 :: Batch 0/1
Batch Loss = 0.8346420294581453
1464, epoch_train_loss=0.8346420294581453
Epoch 1465
Epoch 1465 :: Batch 0/1
Batch Loss = 0.8342285905859048
1465, epoch_train_loss=0.8342285905859048
Epoch 1466
Epoch 1466 :: Batch 0/1
Batch Loss = 0.8338154717945099
1466, epoch_train_loss=0.8338154717945099
Epoch 1467
Epoch 1467 :: Batch 0/1
Batch Loss = 0.8334026783915954
1467, epoch_train_loss=0.8334026783915954
Epoch 1468
Epoch 1468 :: Batch 0/1
Batch Loss = 0.8329901957711646
1468, epoch_train_loss=0.8329901957711646
Epoch 1469
Epoch 1469 :: Batch 0/1
Batch Loss = 0.8325779939761754
1469, epoch_train_loss=0.8325779939761754
Epoch 1470
Epoch 1470 :: Batch 0/1
Batch Loss = 0.8321660327848005
1470, epoch_train_loss=0.8321660327848005
Epoch 1471
Epoch 1471 :: Batch 0/1
Batch Loss = 0.8317542669820313
1471, epoch_train_loss=0.8317542669820313
Epoch 1472
Epoch 1472 :: Batch 0/1
Batch Loss = 0.8313426514637253
1472, epoch_train_loss=0.8313426514637253
Epoch 1473
Epoch 1473 :: Batch 0/1
Batch Loss = 0.8309311456009562
1473, epoch_train_loss=0.8309311456009562
Epoch 1474
Epoch 1474 :: Batch 0/1
Batch Loss = 0.8305197163722052
1474, epoch_train_loss=0.8305197163722052
Epoch 1475
Epoch 1475 :: Batch 0/1
Batch Loss = 0.8301083401573968
1475, epoch_train_loss=0.8301083401573968
Epoch 1476
Epoch 1476 :: Batch 0/1
Batch Loss = 0.8296970030218707
1476, epoch_train_loss=0.8296970030218707
Epoch 1477
Epoch 1477 :: Batch 0/1
Batch Loss = 0.8292856997719577
1477, epoch_train_loss=0.8292856997719577
Epoch 1478
Epoch 1478 :: Batch 0/1
Batch Loss = 0.8288744319604539
1478, epoch_train_loss=0.8288744319604539
Epoch 1479
Epoch 1479 :: Batch 0/1
Batch Loss = 0.8284632057787461
1479, epoch_train_loss=0.8284632057787461
Epoch 1480
Epoch 1480 :: Batch 0/1
Batch Loss = 0.82805202961175
1480, epoch_train_loss=0.82805202961175
Epoch 1481
Epoch 1481 :: Batch 0/1
Batch Loss = 0.8276409118203095
1481, epoch_train_loss=0.8276409118203095
Epoch 1482
Epoch 1482 :: Batch 0/1
Batch Loss = 0.8272298589634614
1482, epoch_train_loss=0.8272298589634614
Epoch 1483
Epoch 1483 :: Batch 0/1
Batch Loss = 0.826818874651838
1483, epoch_train_loss=0.826818874651838
Epoch 1484
Epoch 1484 :: Batch 0/1
Batch Loss = 0.826407958928476
1484, epoch_train_loss=0.826407958928476
Epoch 1485
Epoch 1485 :: Batch 0/1
Batch Loss = 0.8259971082671225
1485, epoch_train_loss=0.8259971082671225
Epoch 1486
Epoch 1486 :: Batch 0/1
Batch Loss = 0.8255863161657134
1486, epoch_train_loss=0.8255863161657134
Epoch 1487
Epoch 1487 :: Batch 0/1
Batch Loss = 0.8251755737706697
1487, epoch_train_loss=0.8251755737706697
Epoch 1488
Epoch 1488 :: Batch 0/1
Batch Loss = 0.8247648707666786
1488, epoch_train_loss=0.8247648707666786
Epoch 1489
Epoch 1489 :: Batch 0/1
Batch Loss = 0.8243541963085441
1489, epoch_train_loss=0.8243541963085441
Epoch 1490
Epoch 1490 :: Batch 0/1
Batch Loss = 0.8239435396421151
1490, epoch_train_loss=0.8239435396421151
Epoch 1491
Epoch 1491 :: Batch 0/1
Batch Loss = 0.823532890553432
1491, epoch_train_loss=0.823532890553432
Epoch 1492
Epoch 1492 :: Batch 0/1
Batch Loss = 0.8231222396801673
1492, epoch_train_loss=0.8231222396801673
Epoch 1493
Epoch 1493 :: Batch 0/1
Batch Loss = 0.8227115783764261
1493, epoch_train_loss=0.8227115783764261
Epoch 1494
Epoch 1494 :: Batch 0/1
Batch Loss = 0.8223008987320265
1494, epoch_train_loss=0.8223008987320265
Epoch 1495
Epoch 1495 :: Batch 0/1
Batch Loss = 0.8218901934817257
1495, epoch_train_loss=0.8218901934817257
Epoch 1496
Epoch 1496 :: Batch 0/1
Batch Loss = 0.821479455878336
1496, epoch_train_loss=0.821479455878336
Epoch 1497
Epoch 1497 :: Batch 0/1
Batch Loss = 0.8210686798264315
1497, epoch_train_loss=0.8210686798264315
Epoch 1498
Epoch 1498 :: Batch 0/1
Batch Loss = 0.8206578597769187
1498, epoch_train_loss=0.8206578597769187
Epoch 1499
Epoch 1499 :: Batch 0/1
Batch Loss = 0.8202469908163214
1499, epoch_train_loss=0.8202469908163214
Epoch 1500
Epoch 1500 :: Batch 0/1
Batch Loss = 0.8198360686097992
1500, epoch_train_loss=0.8198360686097992
Epoch 1501
Epoch 1501 :: Batch 0/1
Batch Loss = 0.8194250891689054
1501, epoch_train_loss=0.8194250891689054
Epoch 1502
Epoch 1502 :: Batch 0/1
Batch Loss = 0.8190140487828522
1502, epoch_train_loss=0.8190140487828522
Epoch 1503
Epoch 1503 :: Batch 0/1
Batch Loss = 0.8186029436383002
1503, epoch_train_loss=0.8186029436383002
Epoch 1504
Epoch 1504 :: Batch 0/1
Batch Loss = 0.8181917696577891
1504, epoch_train_loss=0.8181917696577891
Epoch 1505
Epoch 1505 :: Batch 0/1
Batch Loss = 0.8177805222234134
1505, epoch_train_loss=0.8177805222234134
Epoch 1506
Epoch 1506 :: Batch 0/1
Batch Loss = 0.8173691959703624
1506, epoch_train_loss=0.8173691959703624
Epoch 1507
Epoch 1507 :: Batch 0/1
Batch Loss = 0.8169577846460833
1507, epoch_train_loss=0.8169577846460833
Epoch 1508
Epoch 1508 :: Batch 0/1
Batch Loss = 0.8165462808615066
1508, epoch_train_loss=0.8165462808615066
Epoch 1509
Epoch 1509 :: Batch 0/1
Batch Loss = 0.816134676033568
1509, epoch_train_loss=0.816134676033568
Epoch 1510
Epoch 1510 :: Batch 0/1
Batch Loss = 0.8157229601587749
1510, epoch_train_loss=0.8157229601587749
Epoch 1511
Epoch 1511 :: Batch 0/1
Batch Loss = 0.815311121528767
1511, epoch_train_loss=0.815311121528767
Epoch 1512
Epoch 1512 :: Batch 0/1
Batch Loss = 0.8148991462896944
1512, epoch_train_loss=0.8148991462896944
Epoch 1513
Epoch 1513 :: Batch 0/1
Batch Loss = 0.8144870177901894
1513, epoch_train_loss=0.8144870177901894
Epoch 1514
Epoch 1514 :: Batch 0/1
Batch Loss = 0.8140747154045845
1514, epoch_train_loss=0.8140747154045845
Epoch 1515
Epoch 1515 :: Batch 0/1
Batch Loss = 0.8136622131801293
1515, epoch_train_loss=0.8136622131801293
Epoch 1516
Epoch 1516 :: Batch 0/1
Batch Loss = 0.8132494771849256
1516, epoch_train_loss=0.8132494771849256
Epoch 1517
Epoch 1517 :: Batch 0/1
Batch Loss = 0.8128364620927687
1517, epoch_train_loss=0.8128364620927687
Epoch 1518
Epoch 1518 :: Batch 0/1
Batch Loss = 0.8124231054163147
1518, epoch_train_loss=0.8124231054163147
Epoch 1519
Epoch 1519 :: Batch 0/1
Batch Loss = 0.8120093186678781
1519, epoch_train_loss=0.8120093186678781
Epoch 1520
Epoch 1520 :: Batch 0/1
Batch Loss = 0.8115949731217447
1520, epoch_train_loss=0.8115949731217447
Epoch 1521
Epoch 1521 :: Batch 0/1
Batch Loss = 0.811179876523287
1521, epoch_train_loss=0.811179876523287
Epoch 1522
Epoch 1522 :: Batch 0/1
Batch Loss = 0.810763734305182
1522, epoch_train_loss=0.810763734305182
Epoch 1523
Epoch 1523 :: Batch 0/1
Batch Loss = 0.8103460844735944
1523, epoch_train_loss=0.8103460844735944
Epoch 1524
Epoch 1524 :: Batch 0/1
Batch Loss = 0.8099261877628684
1524, epoch_train_loss=0.8099261877628684
Epoch 1525
Epoch 1525 :: Batch 0/1
Batch Loss = 0.8095028473000224
1525, epoch_train_loss=0.8095028473000224
Epoch 1526
Epoch 1526 :: Batch 0/1
Batch Loss = 0.80907413418449
1526, epoch_train_loss=0.80907413418449
Epoch 1527
Epoch 1527 :: Batch 0/1
Batch Loss = 0.8086370552368014
1527, epoch_train_loss=0.8086370552368014
Epoch 1528
Epoch 1528 :: Batch 0/1
Batch Loss = 0.8081874445842183
1528, epoch_train_loss=0.8081874445842183
Epoch 1529
Epoch 1529 :: Batch 0/1
Batch Loss = 0.8077209524810559
1529, epoch_train_loss=0.8077209524810559
Epoch 1530
Epoch 1530 :: Batch 0/1
Batch Loss = 0.8072364164196046
1530, epoch_train_loss=0.8072364164196046
Epoch 1531
Epoch 1531 :: Batch 0/1
Batch Loss = 0.806740749287721
1531, epoch_train_loss=0.806740749287721
Epoch 1532
Epoch 1532 :: Batch 0/1
Batch Loss = 0.8062507256053759
1532, epoch_train_loss=0.8062507256053759
Epoch 1533
Epoch 1533 :: Batch 0/1
Batch Loss = 0.8057897432558497
1533, epoch_train_loss=0.8057897432558497
Epoch 1534
Epoch 1534 :: Batch 0/1
Batch Loss = 0.8053739000112434
1534, epoch_train_loss=0.8053739000112434
Epoch 1535
Epoch 1535 :: Batch 0/1
Batch Loss = 0.804990819768754
1535, epoch_train_loss=0.804990819768754
Epoch 1536
Epoch 1536 :: Batch 0/1
Batch Loss = 0.8046090038540656
1536, epoch_train_loss=0.8046090038540656
Epoch 1537
Epoch 1537 :: Batch 0/1
Batch Loss = 0.8042046244875162
1537, epoch_train_loss=0.8042046244875162
Epoch 1538
Epoch 1538 :: Batch 0/1
Batch Loss = 0.8037703434583786
1538, epoch_train_loss=0.8037703434583786
Epoch 1539
Epoch 1539 :: Batch 0/1
Batch Loss = 0.8033121803375846
1539, epoch_train_loss=0.8033121803375846
Epoch 1540
Epoch 1540 :: Batch 0/1
Batch Loss = 0.8028451809660234
1540, epoch_train_loss=0.8028451809660234
Epoch 1541
Epoch 1541 :: Batch 0/1
Batch Loss = 0.8023875134192902
1541, epoch_train_loss=0.8023875134192902
Epoch 1542
Epoch 1542 :: Batch 0/1
Batch Loss = 0.8019508279384995
1542, epoch_train_loss=0.8019508279384995
Epoch 1543
Epoch 1543 :: Batch 0/1
Batch Loss = 0.8015332094500172
1543, epoch_train_loss=0.8015332094500172
Epoch 1544
Epoch 1544 :: Batch 0/1
Batch Loss = 0.8011231990452624
1544, epoch_train_loss=0.8011231990452624
Epoch 1545
Epoch 1545 :: Batch 0/1
Batch Loss = 0.8007092699804036
1545, epoch_train_loss=0.8007092699804036
Epoch 1546
Epoch 1546 :: Batch 0/1
Batch Loss = 0.8002850546082145
1546, epoch_train_loss=0.8002850546082145
Epoch 1547
Epoch 1547 :: Batch 0/1
Batch Loss = 0.7998498657870305
1547, epoch_train_loss=0.7998498657870305
Epoch 1548
Epoch 1548 :: Batch 0/1
Batch Loss = 0.7994075051896742
1548, epoch_train_loss=0.7994075051896742
Epoch 1549
Epoch 1549 :: Batch 0/1
Batch Loss = 0.798964478791013
1549, epoch_train_loss=0.798964478791013
Epoch 1550
Epoch 1550 :: Batch 0/1
Batch Loss = 0.7985272880780017
1550, epoch_train_loss=0.7985272880780017
Epoch 1551
Epoch 1551 :: Batch 0/1
Batch Loss = 0.7980990306486264
1551, epoch_train_loss=0.7980990306486264
Epoch 1552
Epoch 1552 :: Batch 0/1
Batch Loss = 0.7976775264984218
1552, epoch_train_loss=0.7976775264984218
Epoch 1553
Epoch 1553 :: Batch 0/1
Batch Loss = 0.7972570826460271
1553, epoch_train_loss=0.7972570826460271
Epoch 1554
Epoch 1554 :: Batch 0/1
Batch Loss = 0.7968324555726508
1554, epoch_train_loss=0.7968324555726508
Epoch 1555
Epoch 1555 :: Batch 0/1
Batch Loss = 0.796401705759232
1555, epoch_train_loss=0.796401705759232
Epoch 1556
Epoch 1556 :: Batch 0/1
Batch Loss = 0.795966542474826
1556, epoch_train_loss=0.795966542474826
Epoch 1557
Epoch 1557 :: Batch 0/1
Batch Loss = 0.7955307576620841
1557, epoch_train_loss=0.7955307576620841
Epoch 1558
Epoch 1558 :: Batch 0/1
Batch Loss = 0.7950978844453913
1558, epoch_train_loss=0.7950978844453913
Epoch 1559
Epoch 1559 :: Batch 0/1
Batch Loss = 0.7946693065760599
1559, epoch_train_loss=0.7946693065760599
Epoch 1560
Epoch 1560 :: Batch 0/1
Batch Loss = 0.794243846074429
1560, epoch_train_loss=0.794243846074429
Epoch 1561
Epoch 1561 :: Batch 0/1
Batch Loss = 0.7938188910175447
1561, epoch_train_loss=0.7938188910175447
Epoch 1562
Epoch 1562 :: Batch 0/1
Batch Loss = 0.7933920773430139
1562, epoch_train_loss=0.7933920773430139
Epoch 1563
Epoch 1563 :: Batch 0/1
Batch Loss = 0.792962457703788
1563, epoch_train_loss=0.792962457703788
Epoch 1564
Epoch 1564 :: Batch 0/1
Batch Loss = 0.7925307169948241
1564, epoch_train_loss=0.7925307169948241
Epoch 1565
Epoch 1565 :: Batch 0/1
Batch Loss = 0.7920985490644845
1565, epoch_train_loss=0.7920985490644845
Epoch 1566
Epoch 1566 :: Batch 0/1
Batch Loss = 0.7916676067021926
1566, epoch_train_loss=0.7916676067021926
Epoch 1567
Epoch 1567 :: Batch 0/1
Batch Loss = 0.7912385981586955
1567, epoch_train_loss=0.7912385981586955
Epoch 1568
Epoch 1568 :: Batch 0/1
Batch Loss = 0.7908110324831483
1568, epoch_train_loss=0.7908110324831483
Epoch 1569
Epoch 1569 :: Batch 0/1
Batch Loss = 0.7903837001642021
1569, epoch_train_loss=0.7903837001642021
Epoch 1570
Epoch 1570 :: Batch 0/1
Batch Loss = 0.7899554843384768
1570, epoch_train_loss=0.7899554843384768
Epoch 1571
Epoch 1571 :: Batch 0/1
Batch Loss = 0.7895259594849404
1571, epoch_train_loss=0.7895259594849404
Epoch 1572
Epoch 1572 :: Batch 0/1
Batch Loss = 0.7890954872483719
1572, epoch_train_loss=0.7890954872483719
Epoch 1573
Epoch 1573 :: Batch 0/1
Batch Loss = 0.7886648662190832
1573, epoch_train_loss=0.7886648662190832
Epoch 1574
Epoch 1574 :: Batch 0/1
Batch Loss = 0.7882348104664831
1574, epoch_train_loss=0.7882348104664831
Epoch 1575
Epoch 1575 :: Batch 0/1
Batch Loss = 0.7878055680335903
1575, epoch_train_loss=0.7878055680335903
Epoch 1576
Epoch 1576 :: Batch 0/1
Batch Loss = 0.7873768651487452
1576, epoch_train_loss=0.7873768651487452
Epoch 1577
Epoch 1577 :: Batch 0/1
Batch Loss = 0.786948151671037
1577, epoch_train_loss=0.786948151671037
Epoch 1578
Epoch 1578 :: Batch 0/1
Batch Loss = 0.786518957638072
1578, epoch_train_loss=0.786518957638072
Epoch 1579
Epoch 1579 :: Batch 0/1
Batch Loss = 0.7860891430993995
1579, epoch_train_loss=0.7860891430993995
Epoch 1580
Epoch 1580 :: Batch 0/1
Batch Loss = 0.7856589213169194
1580, epoch_train_loss=0.7856589213169194
Epoch 1581
Epoch 1581 :: Batch 0/1
Batch Loss = 0.785228679950902
1581, epoch_train_loss=0.785228679950902
Epoch 1582
Epoch 1582 :: Batch 0/1
Batch Loss = 0.78479873200403
1582, epoch_train_loss=0.78479873200403
Epoch 1583
Epoch 1583 :: Batch 0/1
Batch Loss = 0.7843691498103634
1583, epoch_train_loss=0.7843691498103634
Epoch 1584
Epoch 1584 :: Batch 0/1
Batch Loss = 0.7839397664358387
1584, epoch_train_loss=0.7839397664358387
Epoch 1585
Epoch 1585 :: Batch 0/1
Batch Loss = 0.783510315342235
1585, epoch_train_loss=0.783510315342235
Epoch 1586
Epoch 1586 :: Batch 0/1
Batch Loss = 0.7830806019884413
1586, epoch_train_loss=0.7830806019884413
Epoch 1587
Epoch 1587 :: Batch 0/1
Batch Loss = 0.7826506025787798
1587, epoch_train_loss=0.7826506025787798
Epoch 1588
Epoch 1588 :: Batch 0/1
Batch Loss = 0.7822204482937046
1588, epoch_train_loss=0.7822204482937046
Epoch 1589
Epoch 1589 :: Batch 0/1
Batch Loss = 0.7817903245923872
1589, epoch_train_loss=0.7817903245923872
Epoch 1590
Epoch 1590 :: Batch 0/1
Batch Loss = 0.7813603568641222
1590, epoch_train_loss=0.7813603568641222
Epoch 1591
Epoch 1591 :: Batch 0/1
Batch Loss = 0.7809305502664894
1591, epoch_train_loss=0.7809305502664894
Epoch 1592
Epoch 1592 :: Batch 0/1
Batch Loss = 0.7805008099901031
1592, epoch_train_loss=0.7805008099901031
Epoch 1593
Epoch 1593 :: Batch 0/1
Batch Loss = 0.7800710172094987
1593, epoch_train_loss=0.7800710172094987
Epoch 1594
Epoch 1594 :: Batch 0/1
Batch Loss = 0.779641105762136
1594, epoch_train_loss=0.779641105762136
Epoch 1595
Epoch 1595 :: Batch 0/1
Batch Loss = 0.7792110933974267
1595, epoch_train_loss=0.7792110933974267
Epoch 1596
Epoch 1596 :: Batch 0/1
Batch Loss = 0.778781056608253
1596, epoch_train_loss=0.778781056608253
Epoch 1597
Epoch 1597 :: Batch 0/1
Batch Loss = 0.7783510750211927
1597, epoch_train_loss=0.7783510750211927
Epoch 1598
Epoch 1598 :: Batch 0/1
Batch Loss = 0.7779211849728968
1598, epoch_train_loss=0.7779211849728968
Epoch 1599
Epoch 1599 :: Batch 0/1
Batch Loss = 0.7774913684413955
1599, epoch_train_loss=0.7774913684413955
Epoch 1600
Epoch 1600 :: Batch 0/1
Batch Loss = 0.7770615765944173
1600, epoch_train_loss=0.7770615765944173
Epoch 1601
Epoch 1601 :: Batch 0/1
Batch Loss = 0.7766317666996406
1601, epoch_train_loss=0.7766317666996406
Epoch 1602
Epoch 1602 :: Batch 0/1
Batch Loss = 0.7762019279903258
1602, epoch_train_loss=0.7762019279903258
Epoch 1603
Epoch 1603 :: Batch 0/1
Batch Loss = 0.7757720837221955
1603, epoch_train_loss=0.7757720837221955
Epoch 1604
Epoch 1604 :: Batch 0/1
Batch Loss = 0.7753422728363623
1604, epoch_train_loss=0.7753422728363623
Epoch 1605
Epoch 1605 :: Batch 0/1
Batch Loss = 0.7749125255844742
1605, epoch_train_loss=0.7749125255844742
Epoch 1606
Epoch 1606 :: Batch 0/1
Batch Loss = 0.7744828484226242
1606, epoch_train_loss=0.7744828484226242
Epoch 1607
Epoch 1607 :: Batch 0/1
Batch Loss = 0.7740532255779269
1607, epoch_train_loss=0.7740532255779269
Epoch 1608
Epoch 1608 :: Batch 0/1
Batch Loss = 0.7736236335779689
1608, epoch_train_loss=0.7736236335779689
Epoch 1609
Epoch 1609 :: Batch 0/1
Batch Loss = 0.7731940579450792
1609, epoch_train_loss=0.7731940579450792
Epoch 1610
Epoch 1610 :: Batch 0/1
Batch Loss = 0.7727645012768114
1610, epoch_train_loss=0.7727645012768114
Epoch 1611
Epoch 1611 :: Batch 0/1
Batch Loss = 0.7723349795939662
1611, epoch_train_loss=0.7723349795939662
Epoch 1612
Epoch 1612 :: Batch 0/1
Batch Loss = 0.7719055112525911
1612, epoch_train_loss=0.7719055112525911
Epoch 1613
Epoch 1613 :: Batch 0/1
Batch Loss = 0.7714761067787678
1613, epoch_train_loss=0.7714761067787678
Epoch 1614
Epoch 1614 :: Batch 0/1
Batch Loss = 0.7710467652413737
1614, epoch_train_loss=0.7710467652413737
Epoch 1615
Epoch 1615 :: Batch 0/1
Batch Loss = 0.7706174783002204
1615, epoch_train_loss=0.7706174783002204
Epoch 1616
Epoch 1616 :: Batch 0/1
Batch Loss = 0.7701882375209078
1616, epoch_train_loss=0.7701882375209078
Epoch 1617
Epoch 1617 :: Batch 0/1
Batch Loss = 0.7697590402659732
1617, epoch_train_loss=0.7697590402659732
Epoch 1618
Epoch 1618 :: Batch 0/1
Batch Loss = 0.7693298909396475
1618, epoch_train_loss=0.7693298909396475
Epoch 1619
Epoch 1619 :: Batch 0/1
Batch Loss = 0.7689007978089036
1619, epoch_train_loss=0.7689007978089036
Epoch 1620
Epoch 1620 :: Batch 0/1
Batch Loss = 0.7684717681927602
1620, epoch_train_loss=0.7684717681927602
Epoch 1621
Epoch 1621 :: Batch 0/1
Batch Loss = 0.7680428049574592
1621, epoch_train_loss=0.7680428049574592
Epoch 1622
Epoch 1622 :: Batch 0/1
Batch Loss = 0.7676139060208022
1622, epoch_train_loss=0.7676139060208022
Epoch 1623
Epoch 1623 :: Batch 0/1
Batch Loss = 0.7671850668526033
1623, epoch_train_loss=0.7671850668526033
Epoch 1624
Epoch 1624 :: Batch 0/1
Batch Loss = 0.7667562836038317
1624, epoch_train_loss=0.7667562836038317
Epoch 1625
Epoch 1625 :: Batch 0/1
Batch Loss = 0.7663275554095952
1625, epoch_train_loss=0.7663275554095952
Epoch 1626
Epoch 1626 :: Batch 0/1
Batch Loss = 0.7658988843572619
1626, epoch_train_loss=0.7658988843572619
Epoch 1627
Epoch 1627 :: Batch 0/1
Batch Loss = 0.7654702740835445
1627, epoch_train_loss=0.7654702740835445
Epoch 1628
Epoch 1628 :: Batch 0/1
Batch Loss = 0.7650417276997857
1628, epoch_train_loss=0.7650417276997857
Epoch 1629
Epoch 1629 :: Batch 0/1
Batch Loss = 0.7646132467202542
1629, epoch_train_loss=0.7646132467202542
Epoch 1630
Epoch 1630 :: Batch 0/1
Batch Loss = 0.7641848310526006
1630, epoch_train_loss=0.7641848310526006
Epoch 1631
Epoch 1631 :: Batch 0/1
Batch Loss = 0.7637564799028641
1631, epoch_train_loss=0.7637564799028641
Epoch 1632
Epoch 1632 :: Batch 0/1
Batch Loss = 0.7633281926508927
1632, epoch_train_loss=0.7633281926508927
Epoch 1633
Epoch 1633 :: Batch 0/1
Batch Loss = 0.7628999694207017
1633, epoch_train_loss=0.7628999694207017
Epoch 1634
Epoch 1634 :: Batch 0/1
Batch Loss = 0.7624718108148518
1634, epoch_train_loss=0.7624718108148518
Epoch 1635
Epoch 1635 :: Batch 0/1
Batch Loss = 0.7620437172511623
1635, epoch_train_loss=0.7620437172511623
Epoch 1636
Epoch 1636 :: Batch 0/1
Batch Loss = 0.7616156881238718
1636, epoch_train_loss=0.7616156881238718
Epoch 1637
Epoch 1637 :: Batch 0/1
Batch Loss = 0.761187721476539
1637, epoch_train_loss=0.761187721476539
Epoch 1638
Epoch 1638 :: Batch 0/1
Batch Loss = 0.7607598139440905
1638, epoch_train_loss=0.7607598139440905
Epoch 1639
Epoch 1639 :: Batch 0/1
Batch Loss = 0.7603319609907194
1639, epoch_train_loss=0.7603319609907194
Epoch 1640
Epoch 1640 :: Batch 0/1
Batch Loss = 0.759904157153777
1640, epoch_train_loss=0.759904157153777
Epoch 1641
Epoch 1641 :: Batch 0/1
Batch Loss = 0.7594763961181977
1641, epoch_train_loss=0.7594763961181977
Epoch 1642
Epoch 1642 :: Batch 0/1
Batch Loss = 0.7590486703802125
1642, epoch_train_loss=0.7590486703802125
Epoch 1643
Epoch 1643 :: Batch 0/1
Batch Loss = 0.7586209708704795
1643, epoch_train_loss=0.7586209708704795
Epoch 1644
Epoch 1644 :: Batch 0/1
Batch Loss = 0.7581932864886747
1644, epoch_train_loss=0.7581932864886747
Epoch 1645
Epoch 1645 :: Batch 0/1
Batch Loss = 0.7577656035124659
1645, epoch_train_loss=0.7577656035124659
Epoch 1646
Epoch 1646 :: Batch 0/1
Batch Loss = 0.7573379050485037
1646, epoch_train_loss=0.7573379050485037
Epoch 1647
Epoch 1647 :: Batch 0/1
Batch Loss = 0.7569101706307027
1647, epoch_train_loss=0.7569101706307027
Epoch 1648
Epoch 1648 :: Batch 0/1
Batch Loss = 0.7564823757744302
1648, epoch_train_loss=0.7564823757744302
Epoch 1649
Epoch 1649 :: Batch 0/1
Batch Loss = 0.7560544912785733
1649, epoch_train_loss=0.7560544912785733
Epoch 1650
Epoch 1650 :: Batch 0/1
Batch Loss = 0.7556264825842604
1650, epoch_train_loss=0.7556264825842604
Epoch 1651
Epoch 1651 :: Batch 0/1
Batch Loss = 0.7551983090221305
1651, epoch_train_loss=0.7551983090221305
Epoch 1652
Epoch 1652 :: Batch 0/1
Batch Loss = 0.7547699232391487
1652, epoch_train_loss=0.7547699232391487
Epoch 1653
Epoch 1653 :: Batch 0/1
Batch Loss = 0.7543412703032614
1653, epoch_train_loss=0.7543412703032614
Epoch 1654
Epoch 1654 :: Batch 0/1
Batch Loss = 0.7539122873642257
1654, epoch_train_loss=0.7539122873642257
Epoch 1655
Epoch 1655 :: Batch 0/1
Batch Loss = 0.7534829030097512
1655, epoch_train_loss=0.7534829030097512
Epoch 1656
Epoch 1656 :: Batch 0/1
Batch Loss = 0.7530530368148488
1656, epoch_train_loss=0.7530530368148488
Epoch 1657
Epoch 1657 :: Batch 0/1
Batch Loss = 0.7526225987244967
1657, epoch_train_loss=0.7526225987244967
Epoch 1658
Epoch 1658 :: Batch 0/1
Batch Loss = 0.7521914882347798
1658, epoch_train_loss=0.7521914882347798
Epoch 1659
Epoch 1659 :: Batch 0/1
Batch Loss = 0.7517595926035107
1659, epoch_train_loss=0.7517595926035107
Epoch 1660
Epoch 1660 :: Batch 0/1
Batch Loss = 0.7513267837262313
1660, epoch_train_loss=0.7513267837262313
Epoch 1661
Epoch 1661 :: Batch 0/1
Batch Loss = 0.7508929127215178
1661, epoch_train_loss=0.7508929127215178
Epoch 1662
Epoch 1662 :: Batch 0/1
Batch Loss = 0.7504578006404499
1662, epoch_train_loss=0.7504578006404499
Epoch 1663
Epoch 1663 :: Batch 0/1
Batch Loss = 0.7500212243201793
1663, epoch_train_loss=0.7500212243201793
Epoch 1664
Epoch 1664 :: Batch 0/1
Batch Loss = 0.7495828950770145
1664, epoch_train_loss=0.7495828950770145
Epoch 1665
Epoch 1665 :: Batch 0/1
Batch Loss = 0.7491424283403637
1665, epoch_train_loss=0.7491424283403637
Epoch 1666
Epoch 1666 :: Batch 0/1
Batch Loss = 0.7486993011742032
1666, epoch_train_loss=0.7486993011742032
Epoch 1667
Epoch 1667 :: Batch 0/1
Batch Loss = 0.7482527936734227
1667, epoch_train_loss=0.7482527936734227
Epoch 1668
Epoch 1668 :: Batch 0/1
Batch Loss = 0.747801910958566
1668, epoch_train_loss=0.747801910958566
Epoch 1669
Epoch 1669 :: Batch 0/1
Batch Loss = 0.7473452900664146
1669, epoch_train_loss=0.7473452900664146
Epoch 1670
Epoch 1670 :: Batch 0/1
Batch Loss = 0.74688112207822
1670, epoch_train_loss=0.74688112207822
Epoch 1671
Epoch 1671 :: Batch 0/1
Batch Loss = 0.7464071960017142
1671, epoch_train_loss=0.7464071960017142
Epoch 1672
Epoch 1672 :: Batch 0/1
Batch Loss = 0.745921330388957
1672, epoch_train_loss=0.745921330388957
Epoch 1673
Epoch 1673 :: Batch 0/1
Batch Loss = 0.745422653019291
1673, epoch_train_loss=0.745422653019291
Epoch 1674
Epoch 1674 :: Batch 0/1
Batch Loss = 0.7449139456621233
1674, epoch_train_loss=0.7449139456621233
Epoch 1675
Epoch 1675 :: Batch 0/1
Batch Loss = 0.7444035577607458
1675, epoch_train_loss=0.7444035577607458
Epoch 1676
Epoch 1676 :: Batch 0/1
Batch Loss = 0.7439030044344438
1676, epoch_train_loss=0.7439030044344438
Epoch 1677
Epoch 1677 :: Batch 0/1
Batch Loss = 0.7434192880169401
1677, epoch_train_loss=0.7434192880169401
Epoch 1678
Epoch 1678 :: Batch 0/1
Batch Loss = 0.7429495075499786
1678, epoch_train_loss=0.7429495075499786
Epoch 1679
Epoch 1679 :: Batch 0/1
Batch Loss = 0.7424840064309123
1679, epoch_train_loss=0.7424840064309123
Epoch 1680
Epoch 1680 :: Batch 0/1
Batch Loss = 0.7420135884383565
1680, epoch_train_loss=0.7420135884383565
Epoch 1681
Epoch 1681 :: Batch 0/1
Batch Loss = 0.741534527254071
1681, epoch_train_loss=0.741534527254071
Epoch 1682
Epoch 1682 :: Batch 0/1
Batch Loss = 0.7410502171884825
1682, epoch_train_loss=0.7410502171884825
Epoch 1683
Epoch 1683 :: Batch 0/1
Batch Loss = 0.7405702363996524
1683, epoch_train_loss=0.7405702363996524
Epoch 1684
Epoch 1684 :: Batch 0/1
Batch Loss = 0.740105902226564
1684, epoch_train_loss=0.740105902226564
Epoch 1685
Epoch 1685 :: Batch 0/1
Batch Loss = 0.7396615623318273
1685, epoch_train_loss=0.7396615623318273
Epoch 1686
Epoch 1686 :: Batch 0/1
Batch Loss = 0.7392295623103973
1686, epoch_train_loss=0.7392295623103973
Epoch 1687
Epoch 1687 :: Batch 0/1
Batch Loss = 0.738798262040112
1687, epoch_train_loss=0.738798262040112
Epoch 1688
Epoch 1688 :: Batch 0/1
Batch Loss = 0.7383621828688273
1688, epoch_train_loss=0.7383621828688273
Epoch 1689
Epoch 1689 :: Batch 0/1
Batch Loss = 0.7379224282061624
1689, epoch_train_loss=0.7379224282061624
Epoch 1690
Epoch 1690 :: Batch 0/1
Batch Loss = 0.7374823605963678
1690, epoch_train_loss=0.7374823605963678
Epoch 1691
Epoch 1691 :: Batch 0/1
Batch Loss = 0.7370446144818232
1691, epoch_train_loss=0.7370446144818232
Epoch 1692
Epoch 1692 :: Batch 0/1
Batch Loss = 0.736610216642356
1692, epoch_train_loss=0.736610216642356
Epoch 1693
Epoch 1693 :: Batch 0/1
Batch Loss = 0.7361788343430855
1693, epoch_train_loss=0.7361788343430855
Epoch 1694
Epoch 1694 :: Batch 0/1
Batch Loss = 0.7357493492628133
1694, epoch_train_loss=0.7357493492628133
Epoch 1695
Epoch 1695 :: Batch 0/1
Batch Loss = 0.7353203965984176
1695, epoch_train_loss=0.7353203965984176
Epoch 1696
Epoch 1696 :: Batch 0/1
Batch Loss = 0.7348907836242747
1696, epoch_train_loss=0.7348907836242747
Epoch 1697
Epoch 1697 :: Batch 0/1
Batch Loss = 0.734459770096528
1697, epoch_train_loss=0.734459770096528
Epoch 1698
Epoch 1698 :: Batch 0/1
Batch Loss = 0.7340271737945734
1698, epoch_train_loss=0.7340271737945734
Epoch 1699
Epoch 1699 :: Batch 0/1
Batch Loss = 0.7335932853864472
1699, epoch_train_loss=0.7335932853864472
Epoch 1700
Epoch 1700 :: Batch 0/1
Batch Loss = 0.733158651265382
1700, epoch_train_loss=0.733158651265382
Epoch 1701
Epoch 1701 :: Batch 0/1
Batch Loss = 0.7327238402355696
1701, epoch_train_loss=0.7327238402355696
Epoch 1702
Epoch 1702 :: Batch 0/1
Batch Loss = 0.7322892888733494
1702, epoch_train_loss=0.7322892888733494
Epoch 1703
Epoch 1703 :: Batch 0/1
Batch Loss = 0.7318552393962425
1703, epoch_train_loss=0.7318552393962425
Epoch 1704
Epoch 1704 :: Batch 0/1
Batch Loss = 0.731421728707717
1704, epoch_train_loss=0.731421728707717
Epoch 1705
Epoch 1705 :: Batch 0/1
Batch Loss = 0.7309885985768686
1705, epoch_train_loss=0.7309885985768686
Epoch 1706
Epoch 1706 :: Batch 0/1
Batch Loss = 0.7305555405123244
1706, epoch_train_loss=0.7305555405123244
Epoch 1707
Epoch 1707 :: Batch 0/1
Batch Loss = 0.7301222027988642
1707, epoch_train_loss=0.7301222027988642
Epoch 1708
Epoch 1708 :: Batch 0/1
Batch Loss = 0.7296883462234793
1708, epoch_train_loss=0.7296883462234793
Epoch 1709
Epoch 1709 :: Batch 0/1
Batch Loss = 0.7292539813264032
1709, epoch_train_loss=0.7292539813264032
Epoch 1710
Epoch 1710 :: Batch 0/1
Batch Loss = 0.7288194042406395
1710, epoch_train_loss=0.7288194042406395
Epoch 1711
Epoch 1711 :: Batch 0/1
Batch Loss = 0.7283850879660919
1711, epoch_train_loss=0.7283850879660919
Epoch 1712
Epoch 1712 :: Batch 0/1
Batch Loss = 0.7279514632951042
1712, epoch_train_loss=0.7279514632951042
Epoch 1713
Epoch 1713 :: Batch 0/1
Batch Loss = 0.7275187012776982
1713, epoch_train_loss=0.7275187012776982
Epoch 1714
Epoch 1714 :: Batch 0/1
Batch Loss = 0.7270866331826742
1714, epoch_train_loss=0.7270866331826742
Epoch 1715
Epoch 1715 :: Batch 0/1
Batch Loss = 0.7266548668599496
1715, epoch_train_loss=0.7266548668599496
Epoch 1716
Epoch 1716 :: Batch 0/1
Batch Loss = 0.726223023289278
1716, epoch_train_loss=0.726223023289278
Epoch 1717
Epoch 1717 :: Batch 0/1
Batch Loss = 0.7257909396399674
1717, epoch_train_loss=0.7257909396399674
Epoch 1718
Epoch 1718 :: Batch 0/1
Batch Loss = 0.7253587231995619
1718, epoch_train_loss=0.7253587231995619
Epoch 1719
Epoch 1719 :: Batch 0/1
Batch Loss = 0.7249266476506591
1719, epoch_train_loss=0.7249266476506591
Epoch 1720
Epoch 1720 :: Batch 0/1
Batch Loss = 0.724494976934964
1720, epoch_train_loss=0.724494976934964
Epoch 1721
Epoch 1721 :: Batch 0/1
Batch Loss = 0.7240638278817855
1721, epoch_train_loss=0.7240638278817855
Epoch 1722
Epoch 1722 :: Batch 0/1
Batch Loss = 0.7236331391758093
1722, epoch_train_loss=0.7236331391758093
Epoch 1723
Epoch 1723 :: Batch 0/1
Batch Loss = 0.7232027430264788
1723, epoch_train_loss=0.7232027430264788
Epoch 1724
Epoch 1724 :: Batch 0/1
Batch Loss = 0.722772482188679
1724, epoch_train_loss=0.722772482188679
Epoch 1725
Epoch 1725 :: Batch 0/1
Batch Loss = 0.7223423038666722
1725, epoch_train_loss=0.7223423038666722
Epoch 1726
Epoch 1726 :: Batch 0/1
Batch Loss = 0.7219122842190793
1726, epoch_train_loss=0.7219122842190793
Epoch 1727
Epoch 1727 :: Batch 0/1
Batch Loss = 0.721482580691413
1727, epoch_train_loss=0.721482580691413
Epoch 1728
Epoch 1728 :: Batch 0/1
Batch Loss = 0.7210533479902591
1728, epoch_train_loss=0.7210533479902591
Epoch 1729
Epoch 1729 :: Batch 0/1
Batch Loss = 0.7206246703830098
1729, epoch_train_loss=0.7206246703830098
Epoch 1730
Epoch 1730 :: Batch 0/1
Batch Loss = 0.7201965443509312
1730, epoch_train_loss=0.7201965443509312
Epoch 1731
Epoch 1731 :: Batch 0/1
Batch Loss = 0.7197689086241019
1731, epoch_train_loss=0.7197689086241019
Epoch 1732
Epoch 1732 :: Batch 0/1
Batch Loss = 0.7193416926481976
1732, epoch_train_loss=0.7193416926481976
Epoch 1733
Epoch 1733 :: Batch 0/1
Batch Loss = 0.7189148537239108
1733, epoch_train_loss=0.7189148537239108
Epoch 1734
Epoch 1734 :: Batch 0/1
Batch Loss = 0.7184883881237274
1734, epoch_train_loss=0.7184883881237274
Epoch 1735
Epoch 1735 :: Batch 0/1
Batch Loss = 0.7180623175469225
1735, epoch_train_loss=0.7180623175469225
Epoch 1736
Epoch 1736 :: Batch 0/1
Batch Loss = 0.7176366628610277
1736, epoch_train_loss=0.7176366628610277
Epoch 1737
Epoch 1737 :: Batch 0/1
Batch Loss = 0.717211421314041
1737, epoch_train_loss=0.717211421314041
Epoch 1738
Epoch 1738 :: Batch 0/1
Batch Loss = 0.7167865589632473
1738, epoch_train_loss=0.7167865589632473
Epoch 1739
Epoch 1739 :: Batch 0/1
Batch Loss = 0.7163620203102476
1739, epoch_train_loss=0.7163620203102476
Epoch 1740
Epoch 1740 :: Batch 0/1
Batch Loss = 0.7159377466771703
1740, epoch_train_loss=0.7159377466771703
Epoch 1741
Epoch 1741 :: Batch 0/1
Batch Loss = 0.715513692358308
1741, epoch_train_loss=0.715513692358308
Epoch 1742
Epoch 1742 :: Batch 0/1
Batch Loss = 0.7150898320337878
1742, epoch_train_loss=0.7150898320337878
Epoch 1743
Epoch 1743 :: Batch 0/1
Batch Loss = 0.7146661582588818
1743, epoch_train_loss=0.7146661582588818
Epoch 1744
Epoch 1744 :: Batch 0/1
Batch Loss = 0.7142426724410957
1744, epoch_train_loss=0.7142426724410957
Epoch 1745
Epoch 1745 :: Batch 0/1
Batch Loss = 0.7138193742819157
1745, epoch_train_loss=0.7138193742819157
Epoch 1746
Epoch 1746 :: Batch 0/1
Batch Loss = 0.7133962553551874
1746, epoch_train_loss=0.7133962553551874
Epoch 1747
Epoch 1747 :: Batch 0/1
Batch Loss = 0.7129732993938025
1747, epoch_train_loss=0.7129732993938025
Epoch 1748
Epoch 1748 :: Batch 0/1
Batch Loss = 0.7125504883945956
1748, epoch_train_loss=0.7125504883945956
Epoch 1749
Epoch 1749 :: Batch 0/1
Batch Loss = 0.7121278096391893
1749, epoch_train_loss=0.7121278096391893
Epoch 1750
Epoch 1750 :: Batch 0/1
Batch Loss = 0.7117052600837872
1750, epoch_train_loss=0.7117052600837872
Epoch 1751
Epoch 1751 :: Batch 0/1
Batch Loss = 0.7112828458596753
1751, epoch_train_loss=0.7112828458596753
Epoch 1752
Epoch 1752 :: Batch 0/1
Batch Loss = 0.7108605779014028
1752, epoch_train_loss=0.7108605779014028
Epoch 1753
Epoch 1753 :: Batch 0/1
Batch Loss = 0.7104384660780152
1753, epoch_train_loss=0.7104384660780152
Epoch 1754
Epoch 1754 :: Batch 0/1
Batch Loss = 0.7100165145769121
1754, epoch_train_loss=0.7100165145769121
Epoch 1755
Epoch 1755 :: Batch 0/1
Batch Loss = 0.7095947212850089
1755, epoch_train_loss=0.7095947212850089
Epoch 1756
Epoch 1756 :: Batch 0/1
Batch Loss = 0.7091730807464064
1756, epoch_train_loss=0.7091730807464064
Epoch 1757
Epoch 1757 :: Batch 0/1
Batch Loss = 0.7087515885078841
1757, epoch_train_loss=0.7087515885078841
Epoch 1758
Epoch 1758 :: Batch 0/1
Batch Loss = 0.7083302436925615
1758, epoch_train_loss=0.7083302436925615
Epoch 1759
Epoch 1759 :: Batch 0/1
Batch Loss = 0.7079090487003875
1759, epoch_train_loss=0.7079090487003875
Epoch 1760
Epoch 1760 :: Batch 0/1
Batch Loss = 0.7074880062233009
1760, epoch_train_loss=0.7074880062233009
Epoch 1761
Epoch 1761 :: Batch 0/1
Batch Loss = 0.7070671161680915
1761, epoch_train_loss=0.7070671161680915
Epoch 1762
Epoch 1762 :: Batch 0/1
Batch Loss = 0.7066463739711346
1762, epoch_train_loss=0.7066463739711346
Epoch 1763
Epoch 1763 :: Batch 0/1
Batch Loss = 0.7062257716540342
1763, epoch_train_loss=0.7062257716540342
Epoch 1764
Epoch 1764 :: Batch 0/1
Batch Loss = 0.7058053004667937
1764, epoch_train_loss=0.7058053004667937
Epoch 1765
Epoch 1765 :: Batch 0/1
Batch Loss = 0.7053849534789355
1765, epoch_train_loss=0.7053849534789355
Epoch 1766
Epoch 1766 :: Batch 0/1
Batch Loss = 0.7049647263275934
1766, epoch_train_loss=0.7049647263275934
Epoch 1767
Epoch 1767 :: Batch 0/1
Batch Loss = 0.7045446162177809
1767, epoch_train_loss=0.7045446162177809
Epoch 1768
Epoch 1768 :: Batch 0/1
Batch Loss = 0.7041246199348399
1768, epoch_train_loss=0.7041246199348399
Epoch 1769
Epoch 1769 :: Batch 0/1
Batch Loss = 0.7037047324849474
1769, epoch_train_loss=0.7037047324849474
Epoch 1770
Epoch 1770 :: Batch 0/1
Batch Loss = 0.7032849470961974
1770, epoch_train_loss=0.7032849470961974
Epoch 1771
Epoch 1771 :: Batch 0/1
Batch Loss = 0.7028652563464858
1771, epoch_train_loss=0.7028652563464858
Epoch 1772
Epoch 1772 :: Batch 0/1
Batch Loss = 0.702445653566798
1772, epoch_train_loss=0.702445653566798
Epoch 1773
Epoch 1773 :: Batch 0/1
Batch Loss = 0.7020261336936747
1773, epoch_train_loss=0.7020261336936747
Epoch 1774
Epoch 1774 :: Batch 0/1
Batch Loss = 0.7016066933133622
1774, epoch_train_loss=0.7016066933133622
Epoch 1775
Epoch 1775 :: Batch 0/1
Batch Loss = 0.7011873303048318
1775, epoch_train_loss=0.7011873303048318
Epoch 1776
Epoch 1776 :: Batch 0/1
Batch Loss = 0.7007680433276346
1776, epoch_train_loss=0.7007680433276346
Epoch 1777
Epoch 1777 :: Batch 0/1
Batch Loss = 0.7003488301365071
1777, epoch_train_loss=0.7003488301365071
Epoch 1778
Epoch 1778 :: Batch 0/1
Batch Loss = 0.6999296835955128
1778, epoch_train_loss=0.6999296835955128
Epoch 1779
Epoch 1779 :: Batch 0/1
Batch Loss = 0.6995105881571958
1779, epoch_train_loss=0.6995105881571958
Epoch 1780
Epoch 1780 :: Batch 0/1
Batch Loss = 0.699091523530599
1780, epoch_train_loss=0.699091523530599
Epoch 1781
Epoch 1781 :: Batch 0/1
Batch Loss = 0.6986724760480566
1781, epoch_train_loss=0.6986724760480566
Epoch 1782
Epoch 1782 :: Batch 0/1
Batch Loss = 0.6982534465145487
1782, epoch_train_loss=0.6982534465145487
Epoch 1783
Epoch 1783 :: Batch 0/1
Batch Loss = 0.6978344454471688
1783, epoch_train_loss=0.6978344454471688
Epoch 1784
Epoch 1784 :: Batch 0/1
Batch Loss = 0.6974154813247242
1784, epoch_train_loss=0.6974154813247242
Epoch 1785
Epoch 1785 :: Batch 0/1
Batch Loss = 0.6969965549738211
1785, epoch_train_loss=0.6969965549738211
Epoch 1786
Epoch 1786 :: Batch 0/1
Batch Loss = 0.6965776623932259
1786, epoch_train_loss=0.6965776623932259
Epoch 1787
Epoch 1787 :: Batch 0/1
Batch Loss = 0.6961587989053141
1787, epoch_train_loss=0.6961587989053141
Epoch 1788
Epoch 1788 :: Batch 0/1
Batch Loss = 0.69573996077393
1788, epoch_train_loss=0.69573996077393
Epoch 1789
Epoch 1789 :: Batch 0/1
Batch Loss = 0.6953211449267478
1789, epoch_train_loss=0.6953211449267478
Epoch 1790
Epoch 1790 :: Batch 0/1
Batch Loss = 0.6949023481494404
1790, epoch_train_loss=0.6949023481494404
Epoch 1791
Epoch 1791 :: Batch 0/1
Batch Loss = 0.6944835670021099
1791, epoch_train_loss=0.6944835670021099
Epoch 1792
Epoch 1792 :: Batch 0/1
Batch Loss = 0.6940647978896429
1792, epoch_train_loss=0.6940647978896429
Epoch 1793
Epoch 1793 :: Batch 0/1
Batch Loss = 0.6936460373534741
1793, epoch_train_loss=0.6936460373534741
Epoch 1794
Epoch 1794 :: Batch 0/1
Batch Loss = 0.6932272821546118
1794, epoch_train_loss=0.6932272821546118
Epoch 1795
Epoch 1795 :: Batch 0/1
Batch Loss = 0.6928085292752761
1795, epoch_train_loss=0.6928085292752761
Epoch 1796
Epoch 1796 :: Batch 0/1
Batch Loss = 0.6923897757857121
1796, epoch_train_loss=0.6923897757857121
Epoch 1797
Epoch 1797 :: Batch 0/1
Batch Loss = 0.6919710186817427
1797, epoch_train_loss=0.6919710186817427
Epoch 1798
Epoch 1798 :: Batch 0/1
Batch Loss = 0.6915522550420675
1798, epoch_train_loss=0.6915522550420675
Epoch 1799
Epoch 1799 :: Batch 0/1
Batch Loss = 0.6911334821417341
1799, epoch_train_loss=0.6911334821417341
Epoch 1800
Epoch 1800 :: Batch 0/1
Batch Loss = 0.6907146974638914
1800, epoch_train_loss=0.6907146974638914
Epoch 1801
Epoch 1801 :: Batch 0/1
Batch Loss = 0.6902958987041474
1801, epoch_train_loss=0.6902958987041474
Epoch 1802
Epoch 1802 :: Batch 0/1
Batch Loss = 0.6898770837968954
1802, epoch_train_loss=0.6898770837968954
Epoch 1803
Epoch 1803 :: Batch 0/1
Batch Loss = 0.6894582508524931
1803, epoch_train_loss=0.6894582508524931
Epoch 1804
Epoch 1804 :: Batch 0/1
Batch Loss = 0.6890393981736773
1804, epoch_train_loss=0.6890393981736773
Epoch 1805
Epoch 1805 :: Batch 0/1
Batch Loss = 0.6886205243428273
1805, epoch_train_loss=0.6886205243428273
Epoch 1806
Epoch 1806 :: Batch 0/1
Batch Loss = 0.68820162821158
1806, epoch_train_loss=0.68820162821158
Epoch 1807
Epoch 1807 :: Batch 0/1
Batch Loss = 0.6877827089324093
1807, epoch_train_loss=0.6877827089324093
Epoch 1808
Epoch 1808 :: Batch 0/1
Batch Loss = 0.6873637660291888
1808, epoch_train_loss=0.6873637660291888
Epoch 1809
Epoch 1809 :: Batch 0/1
Batch Loss = 0.6869447993586693
1809, epoch_train_loss=0.6869447993586693
Epoch 1810
Epoch 1810 :: Batch 0/1
Batch Loss = 0.6865258091209314
1810, epoch_train_loss=0.6865258091209314
Epoch 1811
Epoch 1811 :: Batch 0/1
Batch Loss = 0.6861067958923524
1811, epoch_train_loss=0.6861067958923524
Epoch 1812
Epoch 1812 :: Batch 0/1
Batch Loss = 0.6856877606687639
1812, epoch_train_loss=0.6856877606687639
Epoch 1813
Epoch 1813 :: Batch 0/1
Batch Loss = 0.6852687048997065
1813, epoch_train_loss=0.6852687048997065
Epoch 1814
Epoch 1814 :: Batch 0/1
Batch Loss = 0.6848496305031655
1814, epoch_train_loss=0.6848496305031655
Epoch 1815
Epoch 1815 :: Batch 0/1
Batch Loss = 0.6844305398982629
1815, epoch_train_loss=0.6844305398982629
Epoch 1816
Epoch 1816 :: Batch 0/1
Batch Loss = 0.6840114359003819
1816, epoch_train_loss=0.6840114359003819
Epoch 1817
Epoch 1817 :: Batch 0/1
Batch Loss = 0.6835923217935614
1817, epoch_train_loss=0.6835923217935614
Epoch 1818
Epoch 1818 :: Batch 0/1
Batch Loss = 0.683173201414231
1818, epoch_train_loss=0.683173201414231
Epoch 1819
Epoch 1819 :: Batch 0/1
Batch Loss = 0.6827540791040032
1819, epoch_train_loss=0.6827540791040032
Epoch 1820
Epoch 1820 :: Batch 0/1
Batch Loss = 0.6823349596213815
1820, epoch_train_loss=0.6823349596213815
Epoch 1821
Epoch 1821 :: Batch 0/1
Batch Loss = 0.6819158483427606
1821, epoch_train_loss=0.6819158483427606
Epoch 1822
Epoch 1822 :: Batch 0/1
Batch Loss = 0.6814967511144845
1822, epoch_train_loss=0.6814967511144845
Epoch 1823
Epoch 1823 :: Batch 0/1
Batch Loss = 0.6810776742688275
1823, epoch_train_loss=0.6810776742688275
Epoch 1824
Epoch 1824 :: Batch 0/1
Batch Loss = 0.680658624608337
1824, epoch_train_loss=0.680658624608337
Epoch 1825
Epoch 1825 :: Batch 0/1
Batch Loss = 0.6802396093600848
1825, epoch_train_loss=0.6802396093600848
Epoch 1826
Epoch 1826 :: Batch 0/1
Batch Loss = 0.6798206362985701
1826, epoch_train_loss=0.6798206362985701
Epoch 1827
Epoch 1827 :: Batch 0/1
Batch Loss = 0.679401713493269
1827, epoch_train_loss=0.679401713493269
Epoch 1828
Epoch 1828 :: Batch 0/1
Batch Loss = 0.6789828495203934
1828, epoch_train_loss=0.6789828495203934
Epoch 1829
Epoch 1829 :: Batch 0/1
Batch Loss = 0.6785640532976639
1829, epoch_train_loss=0.6785640532976639
Epoch 1830
Epoch 1830 :: Batch 0/1
Batch Loss = 0.6781453340512498
1830, epoch_train_loss=0.6781453340512498
Epoch 1831
Epoch 1831 :: Batch 0/1
Batch Loss = 0.6777267013842625
1831, epoch_train_loss=0.6777267013842625
Epoch 1832
Epoch 1832 :: Batch 0/1
Batch Loss = 0.677308165144154
1832, epoch_train_loss=0.677308165144154
Epoch 1833
Epoch 1833 :: Batch 0/1
Batch Loss = 0.6768897353901039
1833, epoch_train_loss=0.6768897353901039
Epoch 1834
Epoch 1834 :: Batch 0/1
Batch Loss = 0.676471422457703
1834, epoch_train_loss=0.676471422457703
Epoch 1835
Epoch 1835 :: Batch 0/1
Batch Loss = 0.6760532368169989
1835, epoch_train_loss=0.6760532368169989
Epoch 1836
Epoch 1836 :: Batch 0/1
Batch Loss = 0.675635189027972
1836, epoch_train_loss=0.675635189027972
Epoch 1837
Epoch 1837 :: Batch 0/1
Batch Loss = 0.6752172897618762
1837, epoch_train_loss=0.6752172897618762
Epoch 1838
Epoch 1838 :: Batch 0/1
Batch Loss = 0.6747995497559558
1838, epoch_train_loss=0.6747995497559558
Epoch 1839
Epoch 1839 :: Batch 0/1
Batch Loss = 0.6743819796691753
1839, epoch_train_loss=0.6743819796691753
Epoch 1840
Epoch 1840 :: Batch 0/1
Batch Loss = 0.673964590175152
1840, epoch_train_loss=0.673964590175152
Epoch 1841
Epoch 1841 :: Batch 0/1
Batch Loss = 0.6735473918217624
1841, epoch_train_loss=0.6735473918217624
Epoch 1842
Epoch 1842 :: Batch 0/1
Batch Loss = 0.6731303949941598
1842, epoch_train_loss=0.6731303949941598
Epoch 1843
Epoch 1843 :: Batch 0/1
Batch Loss = 0.6727136099819337
1843, epoch_train_loss=0.6727136099819337
Epoch 1844
Epoch 1844 :: Batch 0/1
Batch Loss = 0.6722970468156733
1844, epoch_train_loss=0.6722970468156733
Epoch 1845
Epoch 1845 :: Batch 0/1
Batch Loss = 0.6718807153427158
1845, epoch_train_loss=0.6718807153427158
Epoch 1846
Epoch 1846 :: Batch 0/1
Batch Loss = 0.6714646251722112
1846, epoch_train_loss=0.6714646251722112
Epoch 1847
Epoch 1847 :: Batch 0/1
Batch Loss = 0.6710487856564136
1847, epoch_train_loss=0.6710487856564136
Epoch 1848
Epoch 1848 :: Batch 0/1
Batch Loss = 0.6706332059076676
1848, epoch_train_loss=0.6706332059076676
Epoch 1849
Epoch 1849 :: Batch 0/1
Batch Loss = 0.6702178946841882
1849, epoch_train_loss=0.6702178946841882
Epoch 1850
Epoch 1850 :: Batch 0/1
Batch Loss = 0.6698028605133991
1850, epoch_train_loss=0.6698028605133991
Epoch 1851
Epoch 1851 :: Batch 0/1
Batch Loss = 0.6693881115518633
1851, epoch_train_loss=0.6693881115518633
Epoch 1852
Epoch 1852 :: Batch 0/1
Batch Loss = 0.6689736556835858
1852, epoch_train_loss=0.6689736556835858
Epoch 1853
Epoch 1853 :: Batch 0/1
Batch Loss = 0.6685595004887304
1853, epoch_train_loss=0.6685595004887304
Epoch 1854
Epoch 1854 :: Batch 0/1
Batch Loss = 0.6681456532495461
1854, epoch_train_loss=0.6681456532495461
Epoch 1855
Epoch 1855 :: Batch 0/1
Batch Loss = 0.6677321209598256
1855, epoch_train_loss=0.6677321209598256
Epoch 1856
Epoch 1856 :: Batch 0/1
Batch Loss = 0.6673189103377719
1856, epoch_train_loss=0.6673189103377719
Epoch 1857
Epoch 1857 :: Batch 0/1
Batch Loss = 0.6669060278420033
1857, epoch_train_loss=0.6669060278420033
Epoch 1858
Epoch 1858 :: Batch 0/1
Batch Loss = 0.6664934796900599
1858, epoch_train_loss=0.6664934796900599
Epoch 1859
Epoch 1859 :: Batch 0/1
Batch Loss = 0.6660812719115737
1859, epoch_train_loss=0.6660812719115737
Epoch 1860
Epoch 1860 :: Batch 0/1
Batch Loss = 0.6656694102358407
1860, epoch_train_loss=0.6656694102358407
Epoch 1861
Epoch 1861 :: Batch 0/1
Batch Loss = 0.6652579003438656
1861, epoch_train_loss=0.6652579003438656
Epoch 1862
Epoch 1862 :: Batch 0/1
Batch Loss = 0.66484674762163
1862, epoch_train_loss=0.66484674762163
Epoch 1863
Epoch 1863 :: Batch 0/1
Batch Loss = 0.6644359574094333
1863, epoch_train_loss=0.6644359574094333
Epoch 1864
Epoch 1864 :: Batch 0/1
Batch Loss = 0.6640255348851165
1864, epoch_train_loss=0.6640255348851165
Epoch 1865
Epoch 1865 :: Batch 0/1
Batch Loss = 0.6636154851113126
1865, epoch_train_loss=0.6636154851113126
Epoch 1866
Epoch 1866 :: Batch 0/1
Batch Loss = 0.6632058130474217
1866, epoch_train_loss=0.6632058130474217
Epoch 1867
Epoch 1867 :: Batch 0/1
Batch Loss = 0.6627965235589897
1867, epoch_train_loss=0.6627965235589897
Epoch 1868
Epoch 1868 :: Batch 0/1
Batch Loss = 0.6623876214240699
1868, epoch_train_loss=0.6623876214240699
Epoch 1869
Epoch 1869 :: Batch 0/1
Batch Loss = 0.6619791113033422
1869, epoch_train_loss=0.6619791113033422
Epoch 1870
Epoch 1870 :: Batch 0/1
Batch Loss = 0.6615709978711527
1870, epoch_train_loss=0.6615709978711527
Epoch 1871
Epoch 1871 :: Batch 0/1
Batch Loss = 0.6611632855808411
1871, epoch_train_loss=0.6611632855808411
Epoch 1872
Epoch 1872 :: Batch 0/1
Batch Loss = 0.6607559788867339
1872, epoch_train_loss=0.6607559788867339
Epoch 1873
Epoch 1873 :: Batch 0/1
Batch Loss = 0.6603490820677341
1873, epoch_train_loss=0.6603490820677341
Epoch 1874
Epoch 1874 :: Batch 0/1
Batch Loss = 0.6599425993429535
1874, epoch_train_loss=0.6599425993429535
Epoch 1875
Epoch 1875 :: Batch 0/1
Batch Loss = 0.6595365347545326
1875, epoch_train_loss=0.6595365347545326
Epoch 1876
Epoch 1876 :: Batch 0/1
Batch Loss = 0.6591308921465652
1876, epoch_train_loss=0.6591308921465652
Epoch 1877
Epoch 1877 :: Batch 0/1
Batch Loss = 0.658725675240104
1877, epoch_train_loss=0.658725675240104
Epoch 1878
Epoch 1878 :: Batch 0/1
Batch Loss = 0.658320887478101
1878, epoch_train_loss=0.658320887478101
Epoch 1879
Epoch 1879 :: Batch 0/1
Batch Loss = 0.6579165321311818
1879, epoch_train_loss=0.6579165321311818
Epoch 1880
Epoch 1880 :: Batch 0/1
Batch Loss = 0.6575126121749361
1880, epoch_train_loss=0.6575126121749361
Epoch 1881
Epoch 1881 :: Batch 0/1
Batch Loss = 0.6571091302660997
1881, epoch_train_loss=0.6571091302660997
Epoch 1882
Epoch 1882 :: Batch 0/1
Batch Loss = 0.6567060888175523
1882, epoch_train_loss=0.6567060888175523
Epoch 1883
Epoch 1883 :: Batch 0/1
Batch Loss = 0.6563034898475878
1883, epoch_train_loss=0.6563034898475878
Epoch 1884
Epoch 1884 :: Batch 0/1
Batch Loss = 0.655901335091681
1884, epoch_train_loss=0.655901335091681
Epoch 1885
Epoch 1885 :: Batch 0/1
Batch Loss = 0.6554996258573904
1885, epoch_train_loss=0.6554996258573904
Epoch 1886
Epoch 1886 :: Batch 0/1
Batch Loss = 0.655098363109776
1886, epoch_train_loss=0.655098363109776
Epoch 1887
Epoch 1887 :: Batch 0/1
Batch Loss = 0.6546975474630609
1887, epoch_train_loss=0.6546975474630609
Epoch 1888
Epoch 1888 :: Batch 0/1
Batch Loss = 0.6542971791118012
1888, epoch_train_loss=0.6542971791118012
Epoch 1889
Epoch 1889 :: Batch 0/1
Batch Loss = 0.6538972577988792
1889, epoch_train_loss=0.6538972577988792
Epoch 1890
Epoch 1890 :: Batch 0/1
Batch Loss = 0.6534977830132157
1890, epoch_train_loss=0.6534977830132157
Epoch 1891
Epoch 1891 :: Batch 0/1
Batch Loss = 0.6530987538041773
1891, epoch_train_loss=0.6530987538041773
Epoch 1892
Epoch 1892 :: Batch 0/1
Batch Loss = 0.6527001687934858
1892, epoch_train_loss=0.6527001687934858
Epoch 1893
Epoch 1893 :: Batch 0/1
Batch Loss = 0.6523020263827339
1893, epoch_train_loss=0.6523020263827339
Epoch 1894
Epoch 1894 :: Batch 0/1
Batch Loss = 0.6519043245454171
1894, epoch_train_loss=0.6519043245454171
Epoch 1895
Epoch 1895 :: Batch 0/1
Batch Loss = 0.6515070609746111
1895, epoch_train_loss=0.6515070609746111
Epoch 1896
Epoch 1896 :: Batch 0/1
Batch Loss = 0.6511102330712721
1896, epoch_train_loss=0.6511102330712721
Epoch 1897
Epoch 1897 :: Batch 0/1
Batch Loss = 0.6507138379974875
1897, epoch_train_loss=0.6507138379974875
Epoch 1898
Epoch 1898 :: Batch 0/1
Batch Loss = 0.6503178726021497
1898, epoch_train_loss=0.6503178726021497
Epoch 1899
Epoch 1899 :: Batch 0/1
Batch Loss = 0.6499223335065587
1899, epoch_train_loss=0.6499223335065587
Epoch 1900
Epoch 1900 :: Batch 0/1
Batch Loss = 0.6495272172209828
1900, epoch_train_loss=0.6495272172209828
Epoch 1901
Epoch 1901 :: Batch 0/1
Batch Loss = 0.6491325199733424
1901, epoch_train_loss=0.6491325199733424
Epoch 1902
Epoch 1902 :: Batch 0/1
Batch Loss = 0.6487389582340294
1902, epoch_train_loss=0.6487389582340294
Epoch 1903
Epoch 1903 :: Batch 0/1
Batch Loss = 0.8168832077699923
1903, epoch_train_loss=0.8168832077699923
Epoch 1904
Epoch 1904 :: Batch 0/1
Batch Loss = 0.6480548713266387
1904, epoch_train_loss=0.6480548713266387
Epoch 1905
Epoch 1905 :: Batch 0/1
Batch Loss = 0.6478963741314859
1905, epoch_train_loss=0.6478963741314859
Epoch 1906
Epoch 1906 :: Batch 0/1
Batch Loss = 0.647693313653167
1906, epoch_train_loss=0.647693313653167
Epoch 1907
Epoch 1907 :: Batch 0/1
Batch Loss = 0.6474465326651241
1907, epoch_train_loss=0.6474465326651241
Epoch 1908
Epoch 1908 :: Batch 0/1
Batch Loss = 0.6471698929338843
1908, epoch_train_loss=0.6471698929338843
Epoch 1909
Epoch 1909 :: Batch 0/1
Batch Loss = 0.6468700130224229
1909, epoch_train_loss=0.6468700130224229
Epoch 1910
Epoch 1910 :: Batch 0/1
Batch Loss = 0.6465568383771286
1910, epoch_train_loss=0.6465568383771286
Epoch 1911
Epoch 1911 :: Batch 0/1
Batch Loss = 0.6462319855366359
1911, epoch_train_loss=0.6462319855366359
Epoch 1912
Epoch 1912 :: Batch 0/1
Batch Loss = 0.6458914797416819
1912, epoch_train_loss=0.6458914797416819
Epoch 1913
Epoch 1913 :: Batch 0/1
Batch Loss = 0.6455370240729783
1913, epoch_train_loss=0.6455370240729783
Epoch 1914
Epoch 1914 :: Batch 0/1
Batch Loss = 0.6451713450976417
1914, epoch_train_loss=0.6451713450976417
Epoch 1915
Epoch 1915 :: Batch 0/1
Batch Loss = 0.6447915813321806
1915, epoch_train_loss=0.6447915813321806
Epoch 1916
Epoch 1916 :: Batch 0/1
Batch Loss = 0.6443965205066166
1916, epoch_train_loss=0.6443965205066166
Epoch 1917
Epoch 1917 :: Batch 0/1
Batch Loss = 0.6439918409143296
1917, epoch_train_loss=0.6439918409143296
Epoch 1918
Epoch 1918 :: Batch 0/1
Batch Loss = 0.6435813770472433
1918, epoch_train_loss=0.6435813770472433
Epoch 1919
Epoch 1919 :: Batch 0/1
Batch Loss = 0.643161920591761
1919, epoch_train_loss=0.643161920591761
Epoch 1920
Epoch 1920 :: Batch 0/1
Batch Loss = 0.642735105265893
1920, epoch_train_loss=0.642735105265893
Epoch 1921
Epoch 1921 :: Batch 0/1
Batch Loss = 0.6423161285568104
1921, epoch_train_loss=0.6423161285568104
Epoch 1922
Epoch 1922 :: Batch 0/1
Batch Loss = 0.6419158667324474
1922, epoch_train_loss=0.6419158667324474
Epoch 1923
Epoch 1923 :: Batch 0/1
Batch Loss = 0.6415218116608555
1923, epoch_train_loss=0.6415218116608555
Epoch 1924
Epoch 1924 :: Batch 0/1
Batch Loss = 0.6411181798537955
1924, epoch_train_loss=0.6411181798537955
Epoch 1925
Epoch 1925 :: Batch 0/1
Batch Loss = 0.6407067880884447
1925, epoch_train_loss=0.6407067880884447
Epoch 1926
Epoch 1926 :: Batch 0/1
Batch Loss = 0.6402974687256117
1926, epoch_train_loss=0.6402974687256117
Epoch 1927
Epoch 1927 :: Batch 0/1
Batch Loss = 0.6398975687156732
1927, epoch_train_loss=0.6398975687156732
Epoch 1928
Epoch 1928 :: Batch 0/1
Batch Loss = 0.6395068933108535
1928, epoch_train_loss=0.6395068933108535
Epoch 1929
Epoch 1929 :: Batch 0/1
Batch Loss = 0.6391183615206639
1929, epoch_train_loss=0.6391183615206639
Epoch 1930
Epoch 1930 :: Batch 0/1
Batch Loss = 0.6387262136472056
1930, epoch_train_loss=0.6387262136472056
Epoch 1931
Epoch 1931 :: Batch 0/1
Batch Loss = 0.6383321981616863
1931, epoch_train_loss=0.6383321981616863
Epoch 1932
Epoch 1932 :: Batch 0/1
Batch Loss = 0.637941926760299
1932, epoch_train_loss=0.637941926760299
Epoch 1933
Epoch 1933 :: Batch 0/1
Batch Loss = 0.6375561993464177
1933, epoch_train_loss=0.6375561993464177
Epoch 1934
Epoch 1934 :: Batch 0/1
Batch Loss = 0.6371699368293009
1934, epoch_train_loss=0.6371699368293009
Epoch 1935
Epoch 1935 :: Batch 0/1
Batch Loss = 0.6367797324689084
1935, epoch_train_loss=0.6367797324689084
Epoch 1936
Epoch 1936 :: Batch 0/1
Batch Loss = 0.6363868588242593
1936, epoch_train_loss=0.6363868588242593
Epoch 1937
Epoch 1937 :: Batch 0/1
Batch Loss = 0.6359928379604908
1937, epoch_train_loss=0.6359928379604908
Epoch 1938
Epoch 1938 :: Batch 0/1
Batch Loss = 0.6355987346490781
1938, epoch_train_loss=0.6355987346490781
Epoch 1939
Epoch 1939 :: Batch 0/1
Batch Loss = 0.6352072151264363
1939, epoch_train_loss=0.6352072151264363
Epoch 1940
Epoch 1940 :: Batch 0/1
Batch Loss = 0.6348195708336588
1940, epoch_train_loss=0.6348195708336588
Epoch 1941
Epoch 1941 :: Batch 0/1
Batch Loss = 0.6344332169827439
1941, epoch_train_loss=0.6344332169827439
Epoch 1942
Epoch 1942 :: Batch 0/1
Batch Loss = 0.6340451955524617
1942, epoch_train_loss=0.6340451955524617
Epoch 1943
Epoch 1943 :: Batch 0/1
Batch Loss = 0.6336551794268257
1943, epoch_train_loss=0.6336551794268257
Epoch 1944
Epoch 1944 :: Batch 0/1
Batch Loss = 0.6332639546489641
1944, epoch_train_loss=0.6332639546489641
Epoch 1945
Epoch 1945 :: Batch 0/1
Batch Loss = 0.6328720610166506
1945, epoch_train_loss=0.6328720610166506
Epoch 1946
Epoch 1946 :: Batch 0/1
Batch Loss = 0.6324806241265095
1946, epoch_train_loss=0.6324806241265095
Epoch 1947
Epoch 1947 :: Batch 0/1
Batch Loss = 0.6320909578886615
1947, epoch_train_loss=0.6320909578886615
Epoch 1948
Epoch 1948 :: Batch 0/1
Batch Loss = 0.6317029799829559
1948, epoch_train_loss=0.6317029799829559
Epoch 1949
Epoch 1949 :: Batch 0/1
Batch Loss = 0.6313156916113833
1949, epoch_train_loss=0.6313156916113833
Epoch 1950
Epoch 1950 :: Batch 0/1
Batch Loss = 0.630928618813556
1950, epoch_train_loss=0.630928618813556
Epoch 1951
Epoch 1951 :: Batch 0/1
Batch Loss = 0.6305415764290181
1951, epoch_train_loss=0.6305415764290181
Epoch 1952
Epoch 1952 :: Batch 0/1
Batch Loss = 0.6301541247562048
1952, epoch_train_loss=0.6301541247562048
Epoch 1953
Epoch 1953 :: Batch 0/1
Batch Loss = 0.6297661480867637
1953, epoch_train_loss=0.6297661480867637
Epoch 1954
Epoch 1954 :: Batch 0/1
Batch Loss = 0.6293780429315922
1954, epoch_train_loss=0.6293780429315922
Epoch 1955
Epoch 1955 :: Batch 0/1
Batch Loss = 0.6289900423355934
1955, epoch_train_loss=0.6289900423355934
Epoch 1956
Epoch 1956 :: Batch 0/1
Batch Loss = 0.6286021514465137
1956, epoch_train_loss=0.6286021514465137
Epoch 1957
Epoch 1957 :: Batch 0/1
Batch Loss = 0.6282146212839876
1957, epoch_train_loss=0.6282146212839876
Epoch 1958
Epoch 1958 :: Batch 0/1
Batch Loss = 0.6278277279831174
1958, epoch_train_loss=0.6278277279831174
Epoch 1959
Epoch 1959 :: Batch 0/1
Batch Loss = 0.6274412686102274
1959, epoch_train_loss=0.6274412686102274
Epoch 1960
Epoch 1960 :: Batch 0/1
Batch Loss = 0.6270548157762501
1960, epoch_train_loss=0.6270548157762501
Epoch 1961
Epoch 1961 :: Batch 0/1
Batch Loss = 0.6266682044136475
1961, epoch_train_loss=0.6266682044136475
Epoch 1962
Epoch 1962 :: Batch 0/1
Batch Loss = 0.6262814765300333
1962, epoch_train_loss=0.6262814765300333
Epoch 1963
Epoch 1963 :: Batch 0/1
Batch Loss = 0.6258947475943804
1963, epoch_train_loss=0.6258947475943804
Epoch 1964
Epoch 1964 :: Batch 0/1
Batch Loss = 0.6255082797944489
1964, epoch_train_loss=0.6255082797944489
Epoch 1965
Epoch 1965 :: Batch 0/1
Batch Loss = 0.6251223048514105
1965, epoch_train_loss=0.6251223048514105
Epoch 1966
Epoch 1966 :: Batch 0/1
Batch Loss = 0.6247367335995224
1966, epoch_train_loss=0.6247367335995224
Epoch 1967
Epoch 1967 :: Batch 0/1
Batch Loss = 0.6243512951955358
1967, epoch_train_loss=0.6243512951955358
Epoch 1968
Epoch 1968 :: Batch 0/1
Batch Loss = 0.6239658642489538
1968, epoch_train_loss=0.6239658642489538
Epoch 1969
Epoch 1969 :: Batch 0/1
Batch Loss = 0.6235804673294397
1969, epoch_train_loss=0.6235804673294397
Epoch 1970
Epoch 1970 :: Batch 0/1
Batch Loss = 0.6231951414564179
1970, epoch_train_loss=0.6231951414564179
Epoch 1971
Epoch 1971 :: Batch 0/1
Batch Loss = 0.6228099465328415
1971, epoch_train_loss=0.6228099465328415
Epoch 1972
Epoch 1972 :: Batch 0/1
Batch Loss = 0.6224249672191621
1972, epoch_train_loss=0.6224249672191621
Epoch 1973
Epoch 1973 :: Batch 0/1
Batch Loss = 0.6220402092446066
1973, epoch_train_loss=0.6220402092446066
Epoch 1974
Epoch 1974 :: Batch 0/1
Batch Loss = 0.6216556072184553
1974, epoch_train_loss=0.6216556072184553
Epoch 1975
Epoch 1975 :: Batch 0/1
Batch Loss = 0.6212711343010007
1975, epoch_train_loss=0.6212711343010007
Epoch 1976
Epoch 1976 :: Batch 0/1
Batch Loss = 0.6208867923982424
1976, epoch_train_loss=0.6208867923982424
Epoch 1977
Epoch 1977 :: Batch 0/1
Batch Loss = 0.6205025389580977
1977, epoch_train_loss=0.6205025389580977
Epoch 1978
Epoch 1978 :: Batch 0/1
Batch Loss = 0.6201183241646033
1978, epoch_train_loss=0.6201183241646033
Epoch 1979
Epoch 1979 :: Batch 0/1
Batch Loss = 0.6197341585561299
1979, epoch_train_loss=0.6197341585561299
Epoch 1980
Epoch 1980 :: Batch 0/1
Batch Loss = 0.6193500853786645
1980, epoch_train_loss=0.6193500853786645
Epoch 1981
Epoch 1981 :: Batch 0/1
Batch Loss = 0.6189661292927231
1981, epoch_train_loss=0.6189661292927231
Epoch 1982
Epoch 1982 :: Batch 0/1
Batch Loss = 0.6185822989234869
1982, epoch_train_loss=0.6185822989234869
Epoch 1983
Epoch 1983 :: Batch 0/1
Batch Loss = 0.6181985921890438
1983, epoch_train_loss=0.6181985921890438
Epoch 1984
Epoch 1984 :: Batch 0/1
Batch Loss = 0.6178149829151639
1984, epoch_train_loss=0.6178149829151639
Epoch 1985
Epoch 1985 :: Batch 0/1
Batch Loss = 0.6174314398400832
1985, epoch_train_loss=0.6174314398400832
Epoch 1986
Epoch 1986 :: Batch 0/1
Batch Loss = 0.6170479631697733
1986, epoch_train_loss=0.6170479631697733
Epoch 1987
Epoch 1987 :: Batch 0/1
Batch Loss = 0.616664577190361
1987, epoch_train_loss=0.616664577190361
Epoch 1988
Epoch 1988 :: Batch 0/1
Batch Loss = 0.6162812981092358
1988, epoch_train_loss=0.6162812981092358
Epoch 1989
Epoch 1989 :: Batch 0/1
Batch Loss = 0.6158981303580977
1989, epoch_train_loss=0.6158981303580977
Epoch 1990
Epoch 1990 :: Batch 0/1
Batch Loss = 0.6155150786531413
1990, epoch_train_loss=0.6155150786531413
Epoch 1991
Epoch 1991 :: Batch 0/1
Batch Loss = 0.6151321463766641
1991, epoch_train_loss=0.6151321463766641
Epoch 1992
Epoch 1992 :: Batch 0/1
Batch Loss = 0.6147493361018571
1992, epoch_train_loss=0.6147493361018571
Epoch 1993
Epoch 1993 :: Batch 0/1
Batch Loss = 0.6143666629792935
1993, epoch_train_loss=0.6143666629792935
Epoch 1994
Epoch 1994 :: Batch 0/1
Batch Loss = 0.6139841568498196
1994, epoch_train_loss=0.6139841568498196
Epoch 1995
Epoch 1995 :: Batch 0/1
Batch Loss = 0.6136018467005752
1995, epoch_train_loss=0.6136018467005752
Epoch 1996
Epoch 1996 :: Batch 0/1
Batch Loss = 0.6132197532352037
1996, epoch_train_loss=0.6132197532352037
Epoch 1997
Epoch 1997 :: Batch 0/1
Batch Loss = 0.6128378977209924
1997, epoch_train_loss=0.6128378977209924
Epoch 1998
Epoch 1998 :: Batch 0/1
Batch Loss = 0.612456309410272
1998, epoch_train_loss=0.612456309410272
Epoch 1999
Epoch 1999 :: Batch 0/1
Batch Loss = 0.6120750227410492
1999, epoch_train_loss=0.6120750227410492
Epoch 2000
Epoch 2000 :: Batch 0/1
Batch Loss = 0.6116940728257717
2000, epoch_train_loss=0.6116940728257717
Epoch 2001
Epoch 2001 :: Batch 0/1
Batch Loss = 0.6113134931670996
2001, epoch_train_loss=0.6113134931670996
Epoch 2002
Epoch 2002 :: Batch 0/1
Batch Loss = 0.6109333121994519
2002, epoch_train_loss=0.6109333121994519
Epoch 2003
Epoch 2003 :: Batch 0/1
Batch Loss = 0.6105535509917988
2003, epoch_train_loss=0.6105535509917988
Epoch 2004
Epoch 2004 :: Batch 0/1
Batch Loss = 0.6101742251022251
2004, epoch_train_loss=0.6101742251022251
Epoch 2005
Epoch 2005 :: Batch 0/1
Batch Loss = 0.6097953473224298
2005, epoch_train_loss=0.6097953473224298
Epoch 2006
Epoch 2006 :: Batch 0/1
Batch Loss = 0.6094169258579663
2006, epoch_train_loss=0.6094169258579663
Epoch 2007
Epoch 2007 :: Batch 0/1
Batch Loss = 0.6090389609020689
2007, epoch_train_loss=0.6090389609020689
Epoch 2008
Epoch 2008 :: Batch 0/1
Batch Loss = 0.6086614453323775
2008, epoch_train_loss=0.6086614453323775
Epoch 2009
Epoch 2009 :: Batch 0/1
Batch Loss = 0.6082843680253582
2009, epoch_train_loss=0.6082843680253582
Epoch 2010
Epoch 2010 :: Batch 0/1
Batch Loss = 0.6079077160220548
2010, epoch_train_loss=0.6079077160220548
Epoch 2011
Epoch 2011 :: Batch 0/1
Batch Loss = 0.6075314753013954
2011, epoch_train_loss=0.6075314753013954
Epoch 2012
Epoch 2012 :: Batch 0/1
Batch Loss = 0.6071556322594612
2012, epoch_train_loss=0.6071556322594612
Epoch 2013
Epoch 2013 :: Batch 0/1
Batch Loss = 0.6067801754707665
2013, epoch_train_loss=0.6067801754707665
Epoch 2014
Epoch 2014 :: Batch 0/1
Batch Loss = 0.6064050964768779
2014, epoch_train_loss=0.6064050964768779
Epoch 2015
Epoch 2015 :: Batch 0/1
Batch Loss = 0.6060303891115614
2015, epoch_train_loss=0.6060303891115614
Epoch 2016
Epoch 2016 :: Batch 0/1
Batch Loss = 0.6056560492124917
2016, epoch_train_loss=0.6056560492124917
Epoch 2017
Epoch 2017 :: Batch 0/1
Batch Loss = 0.6052820746622753
2017, epoch_train_loss=0.6052820746622753
Epoch 2018
Epoch 2018 :: Batch 0/1
Batch Loss = 0.6049084648924491
2018, epoch_train_loss=0.6049084648924491
Epoch 2019
Epoch 2019 :: Batch 0/1
Batch Loss = 0.6045352200902027
2019, epoch_train_loss=0.6045352200902027
Epoch 2020
Epoch 2020 :: Batch 0/1
Batch Loss = 0.6041623404571348
2020, epoch_train_loss=0.6041623404571348
Epoch 2021
Epoch 2021 :: Batch 0/1
Batch Loss = 0.6037898261013273
2021, epoch_train_loss=0.6037898261013273
Epoch 2022
Epoch 2022 :: Batch 0/1
Batch Loss = 0.6034176765627686
2022, epoch_train_loss=0.6034176765627686
Epoch 2023
Epoch 2023 :: Batch 0/1
Batch Loss = 0.6030458898081215
2023, epoch_train_loss=0.6030458898081215
Epoch 2024
Epoch 2024 :: Batch 0/1
Batch Loss = 0.6026744617379849
2024, epoch_train_loss=0.6026744617379849
Epoch 2025
Epoch 2025 :: Batch 0/1
Batch Loss = 0.6023033862615209
2025, epoch_train_loss=0.6023033862615209
Epoch 2026
Epoch 2026 :: Batch 0/1
Batch Loss = 0.6019326562379878
2026, epoch_train_loss=0.6019326562379878
Epoch 2027
Epoch 2027 :: Batch 0/1
Batch Loss = 0.6015622639051538
2027, epoch_train_loss=0.6015622639051538
Epoch 2028
Epoch 2028 :: Batch 0/1
Batch Loss = 0.601192201351468
2028, epoch_train_loss=0.601192201351468
Epoch 2029
Epoch 2029 :: Batch 0/1
Batch Loss = 0.6008224606495024
2029, epoch_train_loss=0.6008224606495024
Epoch 2030
Epoch 2030 :: Batch 0/1
Batch Loss = 0.6004530339375012
2030, epoch_train_loss=0.6004530339375012
Epoch 2031
Epoch 2031 :: Batch 0/1
Batch Loss = 0.6000839135728245
2031, epoch_train_loss=0.6000839135728245
Epoch 2032
Epoch 2032 :: Batch 0/1
Batch Loss = 0.5997150922892045
2032, epoch_train_loss=0.5997150922892045
Epoch 2033
Epoch 2033 :: Batch 0/1
Batch Loss = 0.5993465637405013
2033, epoch_train_loss=0.5993465637405013
Epoch 2034
Epoch 2034 :: Batch 0/1
Batch Loss = 0.5989783224061673
2034, epoch_train_loss=0.5989783224061673
Epoch 2035
Epoch 2035 :: Batch 0/1
Batch Loss = 0.5986103637218481
2035, epoch_train_loss=0.5986103637218481
Epoch 2036
Epoch 2036 :: Batch 0/1
Batch Loss = 0.5982426836714493
2036, epoch_train_loss=0.5982426836714493
Epoch 2037
Epoch 2037 :: Batch 0/1
Batch Loss = 0.5978752789462972
2037, epoch_train_loss=0.5978752789462972
Epoch 2038
Epoch 2038 :: Batch 0/1
Batch Loss = 0.5975081466515528
2038, epoch_train_loss=0.5975081466515528
Epoch 2039
Epoch 2039 :: Batch 0/1
Batch Loss = 0.5971412843918956
2039, epoch_train_loss=0.5971412843918956
Epoch 2040
Epoch 2040 :: Batch 0/1
Batch Loss = 0.5967746900008661
2040, epoch_train_loss=0.5967746900008661
Epoch 2041
Epoch 2041 :: Batch 0/1
Batch Loss = 0.5964083614583139
2041, epoch_train_loss=0.5964083614583139
Epoch 2042
Epoch 2042 :: Batch 0/1
Batch Loss = 0.5960422967988951
2042, epoch_train_loss=0.5960422967988951
Epoch 2043
Epoch 2043 :: Batch 0/1
Batch Loss = 0.5956764940180731
2043, epoch_train_loss=0.5956764940180731
Epoch 2044
Epoch 2044 :: Batch 0/1
Batch Loss = 0.5953109510236413
2044, epoch_train_loss=0.5953109510236413
Epoch 2045
Epoch 2045 :: Batch 0/1
Batch Loss = 0.5949456656367986
2045, epoch_train_loss=0.5949456656367986
Epoch 2046
Epoch 2046 :: Batch 0/1
Batch Loss = 0.5945806357359862
2046, epoch_train_loss=0.5945806357359862
Epoch 2047
Epoch 2047 :: Batch 0/1
Batch Loss = 0.5942158591569645
2047, epoch_train_loss=0.5942158591569645
Epoch 2048
Epoch 2048 :: Batch 0/1
Batch Loss = 0.593851333846558
2048, epoch_train_loss=0.593851333846558
Epoch 2049
Epoch 2049 :: Batch 0/1
Batch Loss = 0.5934870576967248
2049, epoch_train_loss=0.5934870576967248
Epoch 2050
Epoch 2050 :: Batch 0/1
Batch Loss = 0.5931230288151188
2050, epoch_train_loss=0.5931230288151188
Epoch 2051
Epoch 2051 :: Batch 0/1
Batch Loss = 0.5927592454461438
2051, epoch_train_loss=0.5927592454461438
Epoch 2052
Epoch 2052 :: Batch 0/1
Batch Loss = 0.5923957059834504
2052, epoch_train_loss=0.5923957059834504
Epoch 2053
Epoch 2053 :: Batch 0/1
Batch Loss = 0.5920324090729445
2053, epoch_train_loss=0.5920324090729445
Epoch 2054
Epoch 2054 :: Batch 0/1
Batch Loss = 0.591669353515895
2054, epoch_train_loss=0.591669353515895
Epoch 2055
Epoch 2055 :: Batch 0/1
Batch Loss = 0.5913065383950431
2055, epoch_train_loss=0.5913065383950431
Epoch 2056
Epoch 2056 :: Batch 0/1
Batch Loss = 0.5909439629740597
2056, epoch_train_loss=0.5909439629740597
Epoch 2057
Epoch 2057 :: Batch 0/1
Batch Loss = 0.5905816266737758
2057, epoch_train_loss=0.5905816266737758
Epoch 2058
Epoch 2058 :: Batch 0/1
Batch Loss = 0.5902195290992526
2058, epoch_train_loss=0.5902195290992526
Epoch 2059
Epoch 2059 :: Batch 0/1
Batch Loss = 0.58985767000378
2059, epoch_train_loss=0.58985767000378
Epoch 2060
Epoch 2060 :: Batch 0/1
Batch Loss = 0.5894960491754938
2060, epoch_train_loss=0.5894960491754938
Epoch 2061
Epoch 2061 :: Batch 0/1
Batch Loss = 0.5891346665497879
2061, epoch_train_loss=0.5891346665497879
Epoch 2062
Epoch 2062 :: Batch 0/1
Batch Loss = 0.588773522093877
2062, epoch_train_loss=0.588773522093877
Epoch 2063
Epoch 2063 :: Batch 0/1
Batch Loss = 0.5884126158869132
2063, epoch_train_loss=0.5884126158869132
Epoch 2064
Epoch 2064 :: Batch 0/1
Batch Loss = 0.5880519481044243
2064, epoch_train_loss=0.5880519481044243
Epoch 2065
Epoch 2065 :: Batch 0/1
Batch Loss = 0.5876915189216709
2065, epoch_train_loss=0.5876915189216709
Epoch 2066
Epoch 2066 :: Batch 0/1
Batch Loss = 0.5873313286025637
2066, epoch_train_loss=0.5873313286025637
Epoch 2067
Epoch 2067 :: Batch 0/1
Batch Loss = 0.5869713775018209
2067, epoch_train_loss=0.5869713775018209
Epoch 2068
Epoch 2068 :: Batch 0/1
Batch Loss = 0.5866116659763142
2068, epoch_train_loss=0.5866116659763142
Epoch 2069
Epoch 2069 :: Batch 0/1
Batch Loss = 0.5862521944705553
2069, epoch_train_loss=0.5862521944705553
Epoch 2070
Epoch 2070 :: Batch 0/1
Batch Loss = 0.5858929634831519
2070, epoch_train_loss=0.5858929634831519
Epoch 2071
Epoch 2071 :: Batch 0/1
Batch Loss = 0.5855339735871037
2071, epoch_train_loss=0.5855339735871037
Epoch 2072
Epoch 2072 :: Batch 0/1
Batch Loss = 0.5851752253010525
2072, epoch_train_loss=0.5851752253010525
Epoch 2073
Epoch 2073 :: Batch 0/1
Batch Loss = 0.5848167192815164
2073, epoch_train_loss=0.5848167192815164
Epoch 2074
Epoch 2074 :: Batch 0/1
Batch Loss = 0.5844584561063617
2074, epoch_train_loss=0.5844584561063617
Epoch 2075
Epoch 2075 :: Batch 0/1
Batch Loss = 0.5841004363761945
2075, epoch_train_loss=0.5841004363761945
Epoch 2076
Epoch 2076 :: Batch 0/1
Batch Loss = 0.5837426606683194
2076, epoch_train_loss=0.5837426606683194
Epoch 2077
Epoch 2077 :: Batch 0/1
Batch Loss = 0.5833851295214966
2077, epoch_train_loss=0.5833851295214966
Epoch 2078
Epoch 2078 :: Batch 0/1
Batch Loss = 0.5830278434226953
2078, epoch_train_loss=0.5830278434226953
Epoch 2079
Epoch 2079 :: Batch 0/1
Batch Loss = 0.5826708027942359
2079, epoch_train_loss=0.5826708027942359
Epoch 2080
Epoch 2080 :: Batch 0/1
Batch Loss = 0.582314007981688
2080, epoch_train_loss=0.582314007981688
Epoch 2081
Epoch 2081 :: Batch 0/1
Batch Loss = 0.5819574592442179
2081, epoch_train_loss=0.5819574592442179
Epoch 2082
Epoch 2082 :: Batch 0/1
Batch Loss = 0.581601156747353
2082, epoch_train_loss=0.581601156747353
Epoch 2083
Epoch 2083 :: Batch 0/1
Batch Loss = 0.5812451005568992
2083, epoch_train_loss=0.5812451005568992
Epoch 2084
Epoch 2084 :: Batch 0/1
Batch Loss = 0.580889290604982
2084, epoch_train_loss=0.580889290604982
Epoch 2085
Epoch 2085 :: Batch 0/1
Batch Loss = 0.5805337267734879
2085, epoch_train_loss=0.5805337267734879
Epoch 2086
Epoch 2086 :: Batch 0/1
Batch Loss = 0.5801784088340791
2086, epoch_train_loss=0.5801784088340791
Epoch 2087
Epoch 2087 :: Batch 0/1
Batch Loss = 0.579823336389054
2087, epoch_train_loss=0.579823336389054
Epoch 2088
Epoch 2088 :: Batch 0/1
Batch Loss = 0.5794685089287079
2088, epoch_train_loss=0.5794685089287079
Epoch 2089
Epoch 2089 :: Batch 0/1
Batch Loss = 0.5791139258898248
2089, epoch_train_loss=0.5791139258898248
Epoch 2090
Epoch 2090 :: Batch 0/1
Batch Loss = 0.578759586541887
2090, epoch_train_loss=0.578759586541887
Epoch 2091
Epoch 2091 :: Batch 0/1
Batch Loss = 0.5784054901039936
2091, epoch_train_loss=0.5784054901039936
Epoch 2092
Epoch 2092 :: Batch 0/1
Batch Loss = 0.5780516356599289
2092, epoch_train_loss=0.5780516356599289
Epoch 2093
Epoch 2093 :: Batch 0/1
Batch Loss = 0.5776980221892221
2093, epoch_train_loss=0.5776980221892221
Epoch 2094
Epoch 2094 :: Batch 0/1
Batch Loss = 0.5773446485413977
2094, epoch_train_loss=0.5773446485413977
Epoch 2095
Epoch 2095 :: Batch 0/1
Batch Loss = 0.5769915135257431
2095, epoch_train_loss=0.5769915135257431
Epoch 2096
Epoch 2096 :: Batch 0/1
Batch Loss = 0.5766386158003236
2096, epoch_train_loss=0.5766386158003236
Epoch 2097
Epoch 2097 :: Batch 0/1
Batch Loss = 0.5762859539913726
2097, epoch_train_loss=0.5762859539913726
Epoch 2098
Epoch 2098 :: Batch 0/1
Batch Loss = 0.5759335266117503
2098, epoch_train_loss=0.5759335266117503
Epoch 2099
Epoch 2099 :: Batch 0/1
Batch Loss = 0.5755813320942695
2099, epoch_train_loss=0.5755813320942695
Epoch 2100
Epoch 2100 :: Batch 0/1
Batch Loss = 0.5752293687677815
2100, epoch_train_loss=0.5752293687677815
Epoch 2101
Epoch 2101 :: Batch 0/1
Batch Loss = 0.574877634919502
2101, epoch_train_loss=0.574877634919502
Epoch 2102
Epoch 2102 :: Batch 0/1
Batch Loss = 0.5745261287999969
2102, epoch_train_loss=0.5745261287999969
Epoch 2103
Epoch 2103 :: Batch 0/1
Batch Loss = 0.5741748485421939
2103, epoch_train_loss=0.5741748485421939
Epoch 2104
Epoch 2104 :: Batch 0/1
Batch Loss = 0.5738237922803904
2104, epoch_train_loss=0.5738237922803904
Epoch 2105
Epoch 2105 :: Batch 0/1
Batch Loss = 0.5734729580403423
2105, epoch_train_loss=0.5734729580403423
Epoch 2106
Epoch 2106 :: Batch 0/1
Batch Loss = 0.5731223438577892
2106, epoch_train_loss=0.5731223438577892
Epoch 2107
Epoch 2107 :: Batch 0/1
Batch Loss = 0.572771947668176
2107, epoch_train_loss=0.572771947668176
Epoch 2108
Epoch 2108 :: Batch 0/1
Batch Loss = 0.57242176739596
2108, epoch_train_loss=0.57242176739596
Epoch 2109
Epoch 2109 :: Batch 0/1
Batch Loss = 0.5720718009579112
2109, epoch_train_loss=0.5720718009579112
Epoch 2110
Epoch 2110 :: Batch 0/1
Batch Loss = 0.571722046180688
2110, epoch_train_loss=0.571722046180688
Epoch 2111
Epoch 2111 :: Batch 0/1
Batch Loss = 0.5713725008890238
2111, epoch_train_loss=0.5713725008890238
Epoch 2112
Epoch 2112 :: Batch 0/1
Batch Loss = 0.5710231628795782
2112, epoch_train_loss=0.5710231628795782
Epoch 2113
Epoch 2113 :: Batch 0/1
Batch Loss = 0.5706740299229702
2113, epoch_train_loss=0.5706740299229702
Epoch 2114
Epoch 2114 :: Batch 0/1
Batch Loss = 0.5703250997653851
2114, epoch_train_loss=0.5703250997653851
Epoch 2115
Epoch 2115 :: Batch 0/1
Batch Loss = 0.5699763701580876
2115, epoch_train_loss=0.5699763701580876
Epoch 2116
Epoch 2116 :: Batch 0/1
Batch Loss = 0.5696278387728831
2116, epoch_train_loss=0.5696278387728831
Epoch 2117
Epoch 2117 :: Batch 0/1
Batch Loss = 0.5692795032874919
2117, epoch_train_loss=0.5692795032874919
Epoch 2118
Epoch 2118 :: Batch 0/1
Batch Loss = 0.5689313613566591
2118, epoch_train_loss=0.5689313613566591
Epoch 2119
Epoch 2119 :: Batch 0/1
Batch Loss = 0.568583410610935
2119, epoch_train_loss=0.568583410610935
Epoch 2120
Epoch 2120 :: Batch 0/1
Batch Loss = 0.5682356486546005
2120, epoch_train_loss=0.5682356486546005
Epoch 2121
Epoch 2121 :: Batch 0/1
Batch Loss = 0.5678880730626125
2121, epoch_train_loss=0.5678880730626125
Epoch 2122
Epoch 2122 :: Batch 0/1
Batch Loss = 0.5675406813763947
2122, epoch_train_loss=0.5675406813763947
Epoch 2123
Epoch 2123 :: Batch 0/1
Batch Loss = 0.5671934710982591
2123, epoch_train_loss=0.5671934710982591
Epoch 2124
Epoch 2124 :: Batch 0/1
Batch Loss = 0.5668464396842015
2124, epoch_train_loss=0.5668464396842015
Epoch 2125
Epoch 2125 :: Batch 0/1
Batch Loss = 0.5664995845347216
2125, epoch_train_loss=0.5664995845347216
Epoch 2126
Epoch 2126 :: Batch 0/1
Batch Loss = 0.5661529029832432
2126, epoch_train_loss=0.5661529029832432
Epoch 2127
Epoch 2127 :: Batch 0/1
Batch Loss = 0.5658063922816398
2127, epoch_train_loss=0.5658063922816398
Epoch 2128
Epoch 2128 :: Batch 0/1
Batch Loss = 0.5654600495822933
2128, epoch_train_loss=0.5654600495822933
Epoch 2129
Epoch 2129 :: Batch 0/1
Batch Loss = 0.5651138719159431
2129, epoch_train_loss=0.5651138719159431
Epoch 2130
Epoch 2130 :: Batch 0/1
Batch Loss = 0.5647678561643891
2130, epoch_train_loss=0.5647678561643891
Epoch 2131
Epoch 2131 :: Batch 0/1
Batch Loss = 0.5644219989987196
2131, epoch_train_loss=0.5644219989987196
Epoch 2132
Epoch 2132 :: Batch 0/1
Batch Loss = 0.564076296922528
2132, epoch_train_loss=0.564076296922528
Epoch 2133
Epoch 2133 :: Batch 0/1
Batch Loss = 0.5637307461364803
2133, epoch_train_loss=0.5637307461364803
Epoch 2134
Epoch 2134 :: Batch 0/1
Batch Loss = 0.5633853425038605
2134, epoch_train_loss=0.5633853425038605
Epoch 2135
Epoch 2135 :: Batch 0/1
Batch Loss = 0.5630400814733009
2135, epoch_train_loss=0.5630400814733009
Epoch 2136
Epoch 2136 :: Batch 0/1
Batch Loss = 0.5626949579833166
2136, epoch_train_loss=0.5626949579833166
Epoch 2137
Epoch 2137 :: Batch 0/1
Batch Loss = 0.5623499663160063
2137, epoch_train_loss=0.5623499663160063
Epoch 2138
Epoch 2138 :: Batch 0/1
Batch Loss = 0.5620051000347148
2138, epoch_train_loss=0.5620051000347148
Epoch 2139
Epoch 2139 :: Batch 0/1
Batch Loss = 0.5616603517171231
2139, epoch_train_loss=0.5616603517171231
Epoch 2140
Epoch 2140 :: Batch 0/1
Batch Loss = 0.5613157127275121
2140, epoch_train_loss=0.5613157127275121
Epoch 2141
Epoch 2141 :: Batch 0/1
Batch Loss = 0.5609711730159938
2141, epoch_train_loss=0.5609711730159938
Epoch 2142
Epoch 2142 :: Batch 0/1
Batch Loss = 0.5606267206485047
2142, epoch_train_loss=0.5606267206485047
Epoch 2143
Epoch 2143 :: Batch 0/1
Batch Loss = 0.5602823414413676
2143, epoch_train_loss=0.5602823414413676
Epoch 2144
Epoch 2144 :: Batch 0/1
Batch Loss = 0.559938018394879
2144, epoch_train_loss=0.559938018394879
Epoch 2145
Epoch 2145 :: Batch 0/1
Batch Loss = 0.5595937309234609
2145, epoch_train_loss=0.5595937309234609
Epoch 2146
Epoch 2146 :: Batch 0/1
Batch Loss = 0.5592494539852564
2146, epoch_train_loss=0.5592494539852564
Epoch 2147
Epoch 2147 :: Batch 0/1
Batch Loss = 0.5589051569265424
2147, epoch_train_loss=0.5589051569265424
Epoch 2148
Epoch 2148 :: Batch 0/1
Batch Loss = 0.5585608021319517
2148, epoch_train_loss=0.5585608021319517
Epoch 2149
Epoch 2149 :: Batch 0/1
Batch Loss = 0.558216343262793
2149, epoch_train_loss=0.558216343262793
Epoch 2150
Epoch 2150 :: Batch 0/1
Batch Loss = 0.5578717231862071
2150, epoch_train_loss=0.5578717231862071
Epoch 2151
Epoch 2151 :: Batch 0/1
Batch Loss = 0.5575268714238792
2151, epoch_train_loss=0.5575268714238792
Epoch 2152
Epoch 2152 :: Batch 0/1
Batch Loss = 0.557181701504119
2152, epoch_train_loss=0.557181701504119
Epoch 2153
Epoch 2153 :: Batch 0/1
Batch Loss = 0.5568361080203825
2153, epoch_train_loss=0.5568361080203825
Epoch 2154
Epoch 2154 :: Batch 0/1
Batch Loss = 0.5564899643066374
2154, epoch_train_loss=0.5564899643066374
Epoch 2155
Epoch 2155 :: Batch 0/1
Batch Loss = 0.5561431213326973
2155, epoch_train_loss=0.5561431213326973
Epoch 2156
Epoch 2156 :: Batch 0/1
Batch Loss = 0.5557954094809656
2156, epoch_train_loss=0.5557954094809656
Epoch 2157
Epoch 2157 :: Batch 0/1
Batch Loss = 0.5554466454565409
2157, epoch_train_loss=0.5554466454565409
Epoch 2158
Epoch 2158 :: Batch 0/1
Batch Loss = 0.5550966470527816
2158, epoch_train_loss=0.5550966470527816
Epoch 2159
Epoch 2159 :: Batch 0/1
Batch Loss = 0.5547452585535543
2159, epoch_train_loss=0.5547452585535543
Epoch 2160
Epoch 2160 :: Batch 0/1
Batch Loss = 0.554392387357798
2160, epoch_train_loss=0.554392387357798
Epoch 2161
Epoch 2161 :: Batch 0/1
Batch Loss = 0.5540380475101765
2161, epoch_train_loss=0.5540380475101765
Epoch 2162
Epoch 2162 :: Batch 0/1
Batch Loss = 0.5536823980087523
2162, epoch_train_loss=0.5536823980087523
Epoch 2163
Epoch 2163 :: Batch 0/1
Batch Loss = 0.55332575690135
2163, epoch_train_loss=0.55332575690135
Epoch 2164
Epoch 2164 :: Batch 0/1
Batch Loss = 0.5529685724908108
2164, epoch_train_loss=0.5529685724908108
Epoch 2165
Epoch 2165 :: Batch 0/1
Batch Loss = 0.5526113463685741
2165, epoch_train_loss=0.5526113463685741
Epoch 2166
Epoch 2166 :: Batch 0/1
Batch Loss = 0.5522545278438309
2166, epoch_train_loss=0.5522545278438309
Epoch 2167
Epoch 2167 :: Batch 0/1
Batch Loss = 0.5518984191013679
2167, epoch_train_loss=0.5518984191013679
Epoch 2168
Epoch 2168 :: Batch 0/1
Batch Loss = 0.5515431294817189
2168, epoch_train_loss=0.5515431294817189
Epoch 2169
Epoch 2169 :: Batch 0/1
Batch Loss = 0.5511885942935563
2169, epoch_train_loss=0.5511885942935563
Epoch 2170
Epoch 2170 :: Batch 0/1
Batch Loss = 0.5508346443911231
2170, epoch_train_loss=0.5508346443911231
Epoch 2171
Epoch 2171 :: Batch 0/1
Batch Loss = 0.5504810946088611
2171, epoch_train_loss=0.5504810946088611
Epoch 2172
Epoch 2172 :: Batch 0/1
Batch Loss = 0.550127817915322
2172, epoch_train_loss=0.550127817915322
Epoch 2173
Epoch 2173 :: Batch 0/1
Batch Loss = 0.5497747836525367
2173, epoch_train_loss=0.5497747836525367
Epoch 2174
Epoch 2174 :: Batch 0/1
Batch Loss = 0.5494220554729554
2174, epoch_train_loss=0.5494220554729554
Epoch 2175
Epoch 2175 :: Batch 0/1
Batch Loss = 0.5490697594654415
2175, epoch_train_loss=0.5490697594654415
Epoch 2176
Epoch 2176 :: Batch 0/1
Batch Loss = 0.5487180421422235
2176, epoch_train_loss=0.5487180421422235
Epoch 2177
Epoch 2177 :: Batch 0/1
Batch Loss = 0.5483670343215431
2177, epoch_train_loss=0.5483670343215431
Epoch 2178
Epoch 2178 :: Batch 0/1
Batch Loss = 0.5480168288971349
2178, epoch_train_loss=0.5480168288971349
Epoch 2179
Epoch 2179 :: Batch 0/1
Batch Loss = 0.5476674710820358
2179, epoch_train_loss=0.5476674710820358
Epoch 2180
Epoch 2180 :: Batch 0/1
Batch Loss = 0.5473189573617292
2180, epoch_train_loss=0.5473189573617292
Epoch 2181
Epoch 2181 :: Batch 0/1
Batch Loss = 0.5469712409049662
2181, epoch_train_loss=0.5469712409049662
Epoch 2182
Epoch 2182 :: Batch 0/1
Batch Loss = 0.5466242429785254
2182, epoch_train_loss=0.5466242429785254
Epoch 2183
Epoch 2183 :: Batch 0/1
Batch Loss = 0.546277867714424
2183, epoch_train_loss=0.546277867714424
Epoch 2184
Epoch 2184 :: Batch 0/1
Batch Loss = 0.5459320149260974
2184, epoch_train_loss=0.5459320149260974
Epoch 2185
Epoch 2185 :: Batch 0/1
Batch Loss = 0.5455865868840093
2185, epoch_train_loss=0.5455865868840093
Epoch 2186
Epoch 2186 :: Batch 0/1
Batch Loss = 0.545241488419332
2186, epoch_train_loss=0.545241488419332
Epoch 2187
Epoch 2187 :: Batch 0/1
Batch Loss = 0.5448966228682837
2187, epoch_train_loss=0.5448966228682837
Epoch 2188
Epoch 2188 :: Batch 0/1
Batch Loss = 0.5445518874895707
2188, epoch_train_loss=0.5445518874895707
Epoch 2189
Epoch 2189 :: Batch 0/1
Batch Loss = 0.5442071690838258
2189, epoch_train_loss=0.5442071690838258
Epoch 2190
Epoch 2190 :: Batch 0/1
Batch Loss = 0.543862339456728
2190, epoch_train_loss=0.543862339456728
Epoch 2191
Epoch 2191 :: Batch 0/1
Batch Loss = 0.5435172501073293
2191, epoch_train_loss=0.5435172501073293
Epoch 2192
Epoch 2192 :: Batch 0/1
Batch Loss = 0.5431717262269664
2192, epoch_train_loss=0.5431717262269664
Epoch 2193
Epoch 2193 :: Batch 0/1
Batch Loss = 0.5428255600821242
2193, epoch_train_loss=0.5428255600821242
Epoch 2194
Epoch 2194 :: Batch 0/1
Batch Loss = 0.5424785036453849
2194, epoch_train_loss=0.5424785036453849
Epoch 2195
Epoch 2195 :: Batch 0/1
Batch Loss = 0.5421302605400056
2195, epoch_train_loss=0.5421302605400056
Epoch 2196
Epoch 2196 :: Batch 0/1
Batch Loss = 0.5417804811625757
2196, epoch_train_loss=0.5417804811625757
Epoch 2197
Epoch 2197 :: Batch 0/1
Batch Loss = 0.541428767879304
2197, epoch_train_loss=0.541428767879304
Epoch 2198
Epoch 2198 :: Batch 0/1
Batch Loss = 0.5410747003882431
2198, epoch_train_loss=0.5410747003882431
Epoch 2199
Epoch 2199 :: Batch 0/1
Batch Loss = 0.5407178921185346
2199, epoch_train_loss=0.5407178921185346
Epoch 2200
Epoch 2200 :: Batch 0/1
Batch Loss = 0.5403580827290951
2200, epoch_train_loss=0.5403580827290951
Epoch 2201
Epoch 2201 :: Batch 0/1
Batch Loss = 0.5399952555778796
2201, epoch_train_loss=0.5399952555778796
Epoch 2202
Epoch 2202 :: Batch 0/1
Batch Loss = 0.5396297392094921
2202, epoch_train_loss=0.5396297392094921
Epoch 2203
Epoch 2203 :: Batch 0/1
Batch Loss = 0.5392622258126833
2203, epoch_train_loss=0.5392622258126833
Epoch 2204
Epoch 2204 :: Batch 0/1
Batch Loss = 0.5388936511032694
2204, epoch_train_loss=0.5388936511032694
Epoch 2205
Epoch 2205 :: Batch 0/1
Batch Loss = 0.5385249533503126
2205, epoch_train_loss=0.5385249533503126
Epoch 2206
Epoch 2206 :: Batch 0/1
Batch Loss = 0.5381568196535707
2206, epoch_train_loss=0.5381568196535707
Epoch 2207
Epoch 2207 :: Batch 0/1
Batch Loss = 0.5377895472281626
2207, epoch_train_loss=0.5377895472281626
Epoch 2208
Epoch 2208 :: Batch 0/1
Batch Loss = 0.5374230702628985
2208, epoch_train_loss=0.5374230702628985
Epoch 2209
Epoch 2209 :: Batch 0/1
Batch Loss = 0.5370571021951466
2209, epoch_train_loss=0.5370571021951466
Epoch 2210
Epoch 2210 :: Batch 0/1
Batch Loss = 0.5366913050401522
2210, epoch_train_loss=0.5366913050401522
Epoch 2211
Epoch 2211 :: Batch 0/1
Batch Loss = 0.5363254181840802
2211, epoch_train_loss=0.5363254181840802
Epoch 2212
Epoch 2212 :: Batch 0/1
Batch Loss = 0.5359593230093377
2212, epoch_train_loss=0.5359593230093377
Epoch 2213
Epoch 2213 :: Batch 0/1
Batch Loss = 0.535593048910587
2213, epoch_train_loss=0.535593048910587
Epoch 2214
Epoch 2214 :: Batch 0/1
Batch Loss = 0.5352267423512855
2214, epoch_train_loss=0.5352267423512855
Epoch 2215
Epoch 2215 :: Batch 0/1
Batch Loss = 0.5348606200516559
2215, epoch_train_loss=0.5348606200516559
Epoch 2216
Epoch 2216 :: Batch 0/1
Batch Loss = 0.5344949236058726
2216, epoch_train_loss=0.5344949236058726
Epoch 2217
Epoch 2217 :: Batch 0/1
Batch Loss = 0.5341298837832735
2217, epoch_train_loss=0.5341298837832735
Epoch 2218
Epoch 2218 :: Batch 0/1
Batch Loss = 0.5337656982445883
2218, epoch_train_loss=0.5337656982445883
Epoch 2219
Epoch 2219 :: Batch 0/1
Batch Loss = 0.533402522051421
2219, epoch_train_loss=0.533402522051421
Epoch 2220
Epoch 2220 :: Batch 0/1
Batch Loss = 0.5330404684224576
2220, epoch_train_loss=0.5330404684224576
Epoch 2221
Epoch 2221 :: Batch 0/1
Batch Loss = 0.5326796162335408
2221, epoch_train_loss=0.5326796162335408
Epoch 2222
Epoch 2222 :: Batch 0/1
Batch Loss = 0.5323200206463371
2222, epoch_train_loss=0.5323200206463371
Epoch 2223
Epoch 2223 :: Batch 0/1
Batch Loss = 0.5319617237356736
2223, epoch_train_loss=0.5319617237356736
Epoch 2224
Epoch 2224 :: Batch 0/1
Batch Loss = 0.5316047621767912
2224, epoch_train_loss=0.5316047621767912
Epoch 2225
Epoch 2225 :: Batch 0/1
Batch Loss = 0.5312491710520817
2225, epoch_train_loss=0.5312491710520817
Epoch 2226
Epoch 2226 :: Batch 0/1
Batch Loss = 0.5308949835087459
2226, epoch_train_loss=0.5308949835087459
Epoch 2227
Epoch 2227 :: Batch 0/1
Batch Loss = 0.5305422279304572
2227, epoch_train_loss=0.5305422279304572
Epoch 2228
Epoch 2228 :: Batch 0/1
Batch Loss = 0.5301909241938895
2228, epoch_train_loss=0.5301909241938895
Epoch 2229
Epoch 2229 :: Batch 0/1
Batch Loss = 0.5298410804767526
2229, epoch_train_loss=0.5298410804767526
Epoch 2230
Epoch 2230 :: Batch 0/1
Batch Loss = 0.5294926917085137
2230, epoch_train_loss=0.5294926917085137
Epoch 2231
Epoch 2231 :: Batch 0/1
Batch Loss = 0.529145739719346
2231, epoch_train_loss=0.529145739719346
Epoch 2232
Epoch 2232 :: Batch 0/1
Batch Loss = 0.5288001952363371
2232, epoch_train_loss=0.5288001952363371
Epoch 2233
Epoch 2233 :: Batch 0/1
Batch Loss = 0.5284560205904987
2233, epoch_train_loss=0.5284560205904987
Epoch 2234
Epoch 2234 :: Batch 0/1
Batch Loss = 0.528113172077024
2234, epoch_train_loss=0.528113172077024
Epoch 2235
Epoch 2235 :: Batch 0/1
Batch Loss = 0.5277716016802715
2235, epoch_train_loss=0.5277716016802715
Epoch 2236
Epoch 2236 :: Batch 0/1
Batch Loss = 0.5274312580112583
2236, epoch_train_loss=0.5274312580112583
Epoch 2237
Epoch 2237 :: Batch 0/1
Batch Loss = 0.5270920873107384
2237, epoch_train_loss=0.5270920873107384
Epoch 2238
Epoch 2238 :: Batch 0/1
Batch Loss = 0.5267540346997167
2238, epoch_train_loss=0.5267540346997167
Epoch 2239
Epoch 2239 :: Batch 0/1
Batch Loss = 0.5264170453245172
2239, epoch_train_loss=0.5264170453245172
Epoch 2240
Epoch 2240 :: Batch 0/1
Batch Loss = 0.5260810650978117
2240, epoch_train_loss=0.5260810650978117
Epoch 2241
Epoch 2241 :: Batch 0/1
Batch Loss = 0.5257460412922347
2241, epoch_train_loss=0.5257460412922347
Epoch 2242
Epoch 2242 :: Batch 0/1
Batch Loss = 0.5254119229520317
2242, epoch_train_loss=0.5254119229520317
Epoch 2243
Epoch 2243 :: Batch 0/1
Batch Loss = 0.5250786612884127
2243, epoch_train_loss=0.5250786612884127
Epoch 2244
Epoch 2244 :: Batch 0/1
Batch Loss = 0.5247462102830472
2244, epoch_train_loss=0.5247462102830472
Epoch 2245
Epoch 2245 :: Batch 0/1
Batch Loss = 0.5244145271368491
2245, epoch_train_loss=0.5244145271368491
Epoch 2246
Epoch 2246 :: Batch 0/1
Batch Loss = 0.5240835724717312
2246, epoch_train_loss=0.5240835724717312
Epoch 2247
Epoch 2247 :: Batch 0/1
Batch Loss = 0.5237533106712091
2247, epoch_train_loss=0.5237533106712091
Epoch 2248
Epoch 2248 :: Batch 0/1
Batch Loss = 0.5234237096409737
2248, epoch_train_loss=0.5234237096409737
Epoch 2249
Epoch 2249 :: Batch 0/1
Batch Loss = 0.5230947406638125
2249, epoch_train_loss=0.5230947406638125
Epoch 2250
Epoch 2250 :: Batch 0/1
Batch Loss = 0.5227663780414501
2250, epoch_train_loss=0.5227663780414501
Epoch 2251
Epoch 2251 :: Batch 0/1
Batch Loss = 0.522438598863956
2251, epoch_train_loss=0.522438598863956
Epoch 2252
Epoch 2252 :: Batch 0/1
Batch Loss = 0.5221113828889298
2252, epoch_train_loss=0.5221113828889298
Epoch 2253
Epoch 2253 :: Batch 0/1
Batch Loss = 0.5217847118655993
2253, epoch_train_loss=0.5217847118655993
Epoch 2254
Epoch 2254 :: Batch 0/1
Batch Loss = 0.5214585673602716
2254, epoch_train_loss=0.5214585673602716
Epoch 2255
Epoch 2255 :: Batch 0/1
Batch Loss = 0.5211329238230519
2255, epoch_train_loss=0.5211329238230519
Epoch 2256
Epoch 2256 :: Batch 0/1
Batch Loss = 0.5208077311183397
2256, epoch_train_loss=0.5208077311183397
Epoch 2257
Epoch 2257 :: Batch 0/1
Batch Loss = 0.520482867930271
2257, epoch_train_loss=0.520482867930271
Epoch 2258
Epoch 2258 :: Batch 0/1
Batch Loss = 0.5201580239505978
2258, epoch_train_loss=0.5201580239505978
Epoch 2259
Epoch 2259 :: Batch 0/1
Batch Loss = 0.5198325365435479
2259, epoch_train_loss=0.5198325365435479
Epoch 2260
Epoch 2260 :: Batch 0/1
Batch Loss = 0.5195056138162417
2260, epoch_train_loss=0.5195056138162417
Epoch 2261
Epoch 2261 :: Batch 0/1
Batch Loss = 0.5191783814313552
2261, epoch_train_loss=0.5191783814313552
Epoch 2262
Epoch 2262 :: Batch 0/1
Batch Loss = 0.5188561401872441
2262, epoch_train_loss=0.5188561401872441
Epoch 2263
Epoch 2263 :: Batch 0/1
Batch Loss = 0.5185394294459239
2263, epoch_train_loss=0.5185394294459239
Epoch 2264
Epoch 2264 :: Batch 0/1
Batch Loss = 0.5182231575261588
2264, epoch_train_loss=0.5182231575261588
Epoch 2265
Epoch 2265 :: Batch 0/1
Batch Loss = 0.5179058109642246
2265, epoch_train_loss=0.5179058109642246
Epoch 2266
Epoch 2266 :: Batch 0/1
Batch Loss = 0.5175877944306824
2266, epoch_train_loss=0.5175877944306824
Epoch 2267
Epoch 2267 :: Batch 0/1
Batch Loss = 0.517269530191504
2267, epoch_train_loss=0.517269530191504
Epoch 2268
Epoch 2268 :: Batch 0/1
Batch Loss = 0.516951231321983
2268, epoch_train_loss=0.516951231321983
Epoch 2269
Epoch 2269 :: Batch 0/1
Batch Loss = 0.516632969860322
2269, epoch_train_loss=0.516632969860322
Epoch 2270
Epoch 2270 :: Batch 0/1
Batch Loss = 0.5163147247178718
2270, epoch_train_loss=0.5163147247178718
Epoch 2271
Epoch 2271 :: Batch 0/1
Batch Loss = 0.5159963899952293
2271, epoch_train_loss=0.5159963899952293
Epoch 2272
Epoch 2272 :: Batch 0/1
Batch Loss = 0.515677794420936
2272, epoch_train_loss=0.515677794420936
Epoch 2273
Epoch 2273 :: Batch 0/1
Batch Loss = 0.5153588338146604
2273, epoch_train_loss=0.5153588338146604
Epoch 2274
Epoch 2274 :: Batch 0/1
Batch Loss = 0.5150398855833264
2274, epoch_train_loss=0.5150398855833264
Epoch 2275
Epoch 2275 :: Batch 0/1
Batch Loss = 0.5147222508511552
2275, epoch_train_loss=0.5147222508511552
Epoch 2276
Epoch 2276 :: Batch 0/1
Batch Loss = 0.5144070304812889
2276, epoch_train_loss=0.5144070304812889
Epoch 2277
Epoch 2277 :: Batch 0/1
Batch Loss = 0.5140932870030962
2277, epoch_train_loss=0.5140932870030962
Epoch 2278
Epoch 2278 :: Batch 0/1
Batch Loss = 0.513779358209053
2278, epoch_train_loss=0.513779358209053
Epoch 2279
Epoch 2279 :: Batch 0/1
Batch Loss = 0.5134645025314906
2279, epoch_train_loss=0.5134645025314906
Epoch 2280
Epoch 2280 :: Batch 0/1
Batch Loss = 0.5131487722297924
2280, epoch_train_loss=0.5131487722297924
Epoch 2281
Epoch 2281 :: Batch 0/1
Batch Loss = 0.5128327360741686
2281, epoch_train_loss=0.5128327360741686
Epoch 2282
Epoch 2282 :: Batch 0/1
Batch Loss = 0.51251733642842
2282, epoch_train_loss=0.51251733642842
Epoch 2283
Epoch 2283 :: Batch 0/1
Batch Loss = 0.5122032380122352
2283, epoch_train_loss=0.5122032380122352
Epoch 2284
Epoch 2284 :: Batch 0/1
Batch Loss = 0.5118901786315281
2284, epoch_train_loss=0.5118901786315281
Epoch 2285
Epoch 2285 :: Batch 0/1
Batch Loss = 0.5115774281336855
2285, epoch_train_loss=0.5115774281336855
Epoch 2286
Epoch 2286 :: Batch 0/1
Batch Loss = 0.5112645025354867
2286, epoch_train_loss=0.5112645025354867
Epoch 2287
Epoch 2287 :: Batch 0/1
Batch Loss = 0.5109512809877585
2287, epoch_train_loss=0.5109512809877585
Epoch 2288
Epoch 2288 :: Batch 0/1
Batch Loss = 0.5106378912155504
2288, epoch_train_loss=0.5106378912155504
Epoch 2289
Epoch 2289 :: Batch 0/1
Batch Loss = 0.5103246400714945
2289, epoch_train_loss=0.5103246400714945
Epoch 2290
Epoch 2290 :: Batch 0/1
Batch Loss = 0.5100118989807448
2290, epoch_train_loss=0.5100118989807448
Epoch 2291
Epoch 2291 :: Batch 0/1
Batch Loss = 0.5096998487774147
2291, epoch_train_loss=0.5096998487774147
Epoch 2292
Epoch 2292 :: Batch 0/1
Batch Loss = 0.5093883080484515
2292, epoch_train_loss=0.5093883080484515
Epoch 2293
Epoch 2293 :: Batch 0/1
Batch Loss = 0.5090769095960548
2293, epoch_train_loss=0.5090769095960548
Epoch 2294
Epoch 2294 :: Batch 0/1
Batch Loss = 0.5087654122642885
2294, epoch_train_loss=0.5087654122642885
Epoch 2295
Epoch 2295 :: Batch 0/1
Batch Loss = 0.5084538342115097
2295, epoch_train_loss=0.5084538342115097
Epoch 2296
Epoch 2296 :: Batch 0/1
Batch Loss = 0.5081423851436218
2296, epoch_train_loss=0.5081423851436218
Epoch 2297
Epoch 2297 :: Batch 0/1
Batch Loss = 0.5078312878661544
2297, epoch_train_loss=0.5078312878661544
Epoch 2298
Epoch 2298 :: Batch 0/1
Batch Loss = 0.5075206100438482
2298, epoch_train_loss=0.5075206100438482
Epoch 2299
Epoch 2299 :: Batch 0/1
Batch Loss = 0.5072102440044591
2299, epoch_train_loss=0.5072102440044591
Epoch 2300
Epoch 2300 :: Batch 0/1
Batch Loss = 0.5069000255715148
2300, epoch_train_loss=0.5069000255715148
Epoch 2301
Epoch 2301 :: Batch 0/1
Batch Loss = 0.5065898498210402
2301, epoch_train_loss=0.5065898498210402
Epoch 2302
Epoch 2302 :: Batch 0/1
Batch Loss = 0.5062797095061892
2302, epoch_train_loss=0.5062797095061892
Epoch 2303
Epoch 2303 :: Batch 0/1
Batch Loss = 0.5059696741786691
2303, epoch_train_loss=0.5059696741786691
Epoch 2304
Epoch 2304 :: Batch 0/1
Batch Loss = 0.5056598347518348
2304, epoch_train_loss=0.5056598347518348
Epoch 2305
Epoch 2305 :: Batch 0/1
Batch Loss = 0.5053502355196718
2305, epoch_train_loss=0.5053502355196718
Epoch 2306
Epoch 2306 :: Batch 0/1
Batch Loss = 0.5050408375091737
2306, epoch_train_loss=0.5050408375091737
Epoch 2307
Epoch 2307 :: Batch 0/1
Batch Loss = 0.5047315467293871
2307, epoch_train_loss=0.5047315467293871
Epoch 2308
Epoch 2308 :: Batch 0/1
Batch Loss = 0.5044222808219972
2308, epoch_train_loss=0.5044222808219972
Epoch 2309
Epoch 2309 :: Batch 0/1
Batch Loss = 0.5041130143823783
2309, epoch_train_loss=0.5041130143823783
Epoch 2310
Epoch 2310 :: Batch 0/1
Batch Loss = 0.5038037729997973
2310, epoch_train_loss=0.5038037729997973
Epoch 2311
Epoch 2311 :: Batch 0/1
Batch Loss = 0.503494590563721
2311, epoch_train_loss=0.503494590563721
Epoch 2312
Epoch 2312 :: Batch 0/1
Batch Loss = 0.5031854665843734
2312, epoch_train_loss=0.5031854665843734
Epoch 2313
Epoch 2313 :: Batch 0/1
Batch Loss = 0.5028763535915002
2313, epoch_train_loss=0.5028763535915002
Epoch 2314
Epoch 2314 :: Batch 0/1
Batch Loss = 0.5025671766123875
2314, epoch_train_loss=0.5025671766123875
Epoch 2315
Epoch 2315 :: Batch 0/1
Batch Loss = 0.5022578619457725
2315, epoch_train_loss=0.5022578619457725
Epoch 2316
Epoch 2316 :: Batch 0/1
Batch Loss = 0.5019483528271428
2316, epoch_train_loss=0.5019483528271428
Epoch 2317
Epoch 2317 :: Batch 0/1
Batch Loss = 0.5016386052468879
2317, epoch_train_loss=0.5016386052468879
Epoch 2318
Epoch 2318 :: Batch 0/1
Batch Loss = 0.5013285685566528
2318, epoch_train_loss=0.5013285685566528
Epoch 2319
Epoch 2319 :: Batch 0/1
Batch Loss = 0.5010181629001506
2319, epoch_train_loss=0.5010181629001506
Epoch 2320
Epoch 2320 :: Batch 0/1
Batch Loss = 0.5007072655246532
2320, epoch_train_loss=0.5007072655246532
Epoch 2321
Epoch 2321 :: Batch 0/1
Batch Loss = 0.5003957107304356
2321, epoch_train_loss=0.5003957107304356
Epoch 2322
Epoch 2322 :: Batch 0/1
Batch Loss = 0.5000832955941327
2322, epoch_train_loss=0.5000832955941327
Epoch 2323
Epoch 2323 :: Batch 0/1
Batch Loss = 0.499769778285845
2323, epoch_train_loss=0.499769778285845
Epoch 2324
Epoch 2324 :: Batch 0/1
Batch Loss = 0.4994548602969187
2324, epoch_train_loss=0.4994548602969187
Epoch 2325
Epoch 2325 :: Batch 0/1
Batch Loss = 0.49913815312548154
2325, epoch_train_loss=0.49913815312548154
Epoch 2326
Epoch 2326 :: Batch 0/1
Batch Loss = 0.4988191359868982
2326, epoch_train_loss=0.4988191359868982
Epoch 2327
Epoch 2327 :: Batch 0/1
Batch Loss = 0.49849710931208613
2327, epoch_train_loss=0.49849710931208613
Epoch 2328
Epoch 2328 :: Batch 0/1
Batch Loss = 0.4981711428886889
2328, epoch_train_loss=0.4981711428886889
Epoch 2329
Epoch 2329 :: Batch 0/1
Batch Loss = 0.49784001313390513
2329, epoch_train_loss=0.49784001313390513
Epoch 2330
Epoch 2330 :: Batch 0/1
Batch Loss = 0.49750212700514973
2330, epoch_train_loss=0.49750212700514973
Epoch 2331
Epoch 2331 :: Batch 0/1
Batch Loss = 0.49715544318480603
2331, epoch_train_loss=0.49715544318480603
Epoch 2332
Epoch 2332 :: Batch 0/1
Batch Loss = 0.49679742537340055
2332, epoch_train_loss=0.49679742537340055
Epoch 2333
Epoch 2333 :: Batch 0/1
Batch Loss = 0.4964250932542047
2333, epoch_train_loss=0.4964250932542047
Epoch 2334
Epoch 2334 :: Batch 0/1
Batch Loss = 0.49603524672386007
2334, epoch_train_loss=0.49603524672386007
Epoch 2335
Epoch 2335 :: Batch 0/1
Batch Loss = 0.4956248297423229
2335, epoch_train_loss=0.4956248297423229
Epoch 2336
Epoch 2336 :: Batch 0/1
Batch Loss = 0.495190968628605
2336, epoch_train_loss=0.495190968628605
Epoch 2337
Epoch 2337 :: Batch 0/1
Batch Loss = 0.49472936608937607
2337, epoch_train_loss=0.49472936608937607
Epoch 2338
Epoch 2338 :: Batch 0/1
Batch Loss = 0.4942289968850585
2338, epoch_train_loss=0.4942289968850585
Epoch 2339
Epoch 2339 :: Batch 0/1
Batch Loss = 0.4936615581382134
2339, epoch_train_loss=0.4936615581382134
Epoch 2340
Epoch 2340 :: Batch 0/1
Batch Loss = 0.49296764611444044
2340, epoch_train_loss=0.49296764611444044
Epoch 2341
Epoch 2341 :: Batch 0/1
Batch Loss = 0.4920595370583521
2341, epoch_train_loss=0.4920595370583521
Epoch 2342
Epoch 2342 :: Batch 0/1
Batch Loss = 0.4908952481046486
2342, epoch_train_loss=0.4908952481046486
Epoch 2343
Epoch 2343 :: Batch 0/1
Batch Loss = 0.48953255932973594
2343, epoch_train_loss=0.48953255932973594
Epoch 2344
Epoch 2344 :: Batch 0/1
Batch Loss = 0.4878304632115839
2344, epoch_train_loss=0.4878304632115839
Epoch 2345
Epoch 2345 :: Batch 0/1
Batch Loss = 0.48629396129901087
2345, epoch_train_loss=0.48629396129901087
Epoch 2346
Epoch 2346 :: Batch 0/1
Batch Loss = 0.4859808751691351
2346, epoch_train_loss=0.4859808751691351
Epoch 2347
Epoch 2347 :: Batch 0/1
Batch Loss = 0.48643396488952334
2347, epoch_train_loss=0.48643396488952334
Epoch 2348
Epoch 2348 :: Batch 0/1
Batch Loss = 0.4867115877804318
2348, epoch_train_loss=0.4867115877804318
Epoch 2349
Epoch 2349 :: Batch 0/1
Batch Loss = 0.48642739709331145
2349, epoch_train_loss=0.48642739709331145
Epoch 2350
Epoch 2350 :: Batch 0/1
Batch Loss = 0.4856062907372263
2350, epoch_train_loss=0.4856062907372263
Epoch 2351
Epoch 2351 :: Batch 0/1
Batch Loss = 0.4845188546423074
2351, epoch_train_loss=0.4845188546423074
Epoch 2352
Epoch 2352 :: Batch 0/1
Batch Loss = 0.4832791484947191
2352, epoch_train_loss=0.4832791484947191
Epoch 2353
Epoch 2353 :: Batch 0/1
Batch Loss = 0.4823152046557923
2353, epoch_train_loss=0.4823152046557923
Epoch 2354
Epoch 2354 :: Batch 0/1
Batch Loss = 0.48205990211722605
2354, epoch_train_loss=0.48205990211722605
Epoch 2355
Epoch 2355 :: Batch 0/1
Batch Loss = 0.4821022272423426
2355, epoch_train_loss=0.4821022272423426
Epoch 2356
Epoch 2356 :: Batch 0/1
Batch Loss = 0.4819626233536019
2356, epoch_train_loss=0.4819626233536019
Epoch 2357
Epoch 2357 :: Batch 0/1
Batch Loss = 0.4814984988914252
2357, epoch_train_loss=0.4814984988914252
Epoch 2358
Epoch 2358 :: Batch 0/1
Batch Loss = 0.4808170853987788
2358, epoch_train_loss=0.4808170853987788
Epoch 2359
Epoch 2359 :: Batch 0/1
Batch Loss = 0.4801207996712383
2359, epoch_train_loss=0.4801207996712383
Epoch 2360
Epoch 2360 :: Batch 0/1
Batch Loss = 0.4794909005568938
2360, epoch_train_loss=0.4794909005568938
Epoch 2361
Epoch 2361 :: Batch 0/1
Batch Loss = 0.47900929878701015
2361, epoch_train_loss=0.47900929878701015
Epoch 2362
Epoch 2362 :: Batch 0/1
Batch Loss = 0.47876324440606555
2362, epoch_train_loss=0.47876324440606555
Epoch 2363
Epoch 2363 :: Batch 0/1
Batch Loss = 0.47861975242681964
2363, epoch_train_loss=0.47861975242681964
Epoch 2364
Epoch 2364 :: Batch 0/1
Batch Loss = 0.4783645748495336
2364, epoch_train_loss=0.4783645748495336
Epoch 2365
Epoch 2365 :: Batch 0/1
Batch Loss = 0.4779527270485941
2365, epoch_train_loss=0.4779527270485941
Epoch 2366
Epoch 2366 :: Batch 0/1
Batch Loss = 0.47745153559986403
2366, epoch_train_loss=0.47745153559986403
Epoch 2367
Epoch 2367 :: Batch 0/1
Batch Loss = 0.4769773038655667
2367, epoch_train_loss=0.4769773038655667
Epoch 2368
Epoch 2368 :: Batch 0/1
Batch Loss = 0.4766090259305642
2368, epoch_train_loss=0.4766090259305642
Epoch 2369
Epoch 2369 :: Batch 0/1
Batch Loss = 0.4763110526162381
2369, epoch_train_loss=0.4763110526162381
Epoch 2370
Epoch 2370 :: Batch 0/1
Batch Loss = 0.47607488687485044
2370, epoch_train_loss=0.47607488687485044
Epoch 2371
Epoch 2371 :: Batch 0/1
Batch Loss = 0.4758325976399181
2371, epoch_train_loss=0.4758325976399181
Epoch 2372
Epoch 2372 :: Batch 0/1
Batch Loss = 0.47551854016853806
2372, epoch_train_loss=0.47551854016853806
Epoch 2373
Epoch 2373 :: Batch 0/1
Batch Loss = 0.475154860457199
2373, epoch_train_loss=0.475154860457199
Epoch 2374
Epoch 2374 :: Batch 0/1
Batch Loss = 0.47477903313065845
2374, epoch_train_loss=0.47477903313065845
Epoch 2375
Epoch 2375 :: Batch 0/1
Batch Loss = 0.4744366187912678
2375, epoch_train_loss=0.4744366187912678
Epoch 2376
Epoch 2376 :: Batch 0/1
Batch Loss = 0.47414258295387796
2376, epoch_train_loss=0.47414258295387796
Epoch 2377
Epoch 2377 :: Batch 0/1
Batch Loss = 0.4738767549098944
2377, epoch_train_loss=0.4738767549098944
Epoch 2378
Epoch 2378 :: Batch 0/1
Batch Loss = 0.47362240712971754
2378, epoch_train_loss=0.47362240712971754
Epoch 2379
Epoch 2379 :: Batch 0/1
Batch Loss = 0.4733548870419699
2379, epoch_train_loss=0.4733548870419699
Epoch 2380
Epoch 2380 :: Batch 0/1
Batch Loss = 0.47305204278330065
2380, epoch_train_loss=0.47305204278330065
Epoch 2381
Epoch 2381 :: Batch 0/1
Batch Loss = 0.47272877545807596
2381, epoch_train_loss=0.47272877545807596
Epoch 2382
Epoch 2382 :: Batch 0/1
Batch Loss = 0.47241362554069055
2382, epoch_train_loss=0.47241362554069055
Epoch 2383
Epoch 2383 :: Batch 0/1
Batch Loss = 0.4721242720298267
2383, epoch_train_loss=0.4721242720298267
Epoch 2384
Epoch 2384 :: Batch 0/1
Batch Loss = 0.4718579756887659
2384, epoch_train_loss=0.4718579756887659
Epoch 2385
Epoch 2385 :: Batch 0/1
Batch Loss = 0.4715931564572601
2385, epoch_train_loss=0.4715931564572601
Epoch 2386
Epoch 2386 :: Batch 0/1
Batch Loss = 0.4713205135333633
2386, epoch_train_loss=0.4713205135333633
Epoch 2387
Epoch 2387 :: Batch 0/1
Batch Loss = 0.4710399261287654
2387, epoch_train_loss=0.4710399261287654
Epoch 2388
Epoch 2388 :: Batch 0/1
Batch Loss = 0.4707490195141335
2388, epoch_train_loss=0.4707490195141335
Epoch 2389
Epoch 2389 :: Batch 0/1
Batch Loss = 0.4704546261069707
2389, epoch_train_loss=0.4704546261069707
Epoch 2390
Epoch 2390 :: Batch 0/1
Batch Loss = 0.4701680200345446
2390, epoch_train_loss=0.4701680200345446
Epoch 2391
Epoch 2391 :: Batch 0/1
Batch Loss = 0.46989439798263605
2391, epoch_train_loss=0.46989439798263605
Epoch 2392
Epoch 2392 :: Batch 0/1
Batch Loss = 0.46962934178769006
2392, epoch_train_loss=0.46962934178769006
Epoch 2393
Epoch 2393 :: Batch 0/1
Batch Loss = 0.46936079131245745
2393, epoch_train_loss=0.46936079131245745
Epoch 2394
Epoch 2394 :: Batch 0/1
Batch Loss = 0.46908356716721783
2394, epoch_train_loss=0.46908356716721783
Epoch 2395
Epoch 2395 :: Batch 0/1
Batch Loss = 0.46880291791017964
2395, epoch_train_loss=0.46880291791017964
Epoch 2396
Epoch 2396 :: Batch 0/1
Batch Loss = 0.4685231519400016
2396, epoch_train_loss=0.4685231519400016
Epoch 2397
Epoch 2397 :: Batch 0/1
Batch Loss = 0.46824647885530957
2397, epoch_train_loss=0.46824647885530957
Epoch 2398
Epoch 2398 :: Batch 0/1
Batch Loss = 0.46797441138812024
2398, epoch_train_loss=0.46797441138812024
Epoch 2399
Epoch 2399 :: Batch 0/1
Batch Loss = 0.4677058648937364
2399, epoch_train_loss=0.4677058648937364
Epoch 2400
Epoch 2400 :: Batch 0/1
Batch Loss = 0.4674379733580016
2400, epoch_train_loss=0.4674379733580016
Epoch 2401
Epoch 2401 :: Batch 0/1
Batch Loss = 0.46716696764146576
2401, epoch_train_loss=0.46716696764146576
Epoch 2402
Epoch 2402 :: Batch 0/1
Batch Loss = 0.4668917756467676
2402, epoch_train_loss=0.4668917756467676
Epoch 2403
Epoch 2403 :: Batch 0/1
Batch Loss = 0.4666161053794037
2403, epoch_train_loss=0.4666161053794037
Epoch 2404
Epoch 2404 :: Batch 0/1
Batch Loss = 0.4663431560347604
2404, epoch_train_loss=0.4663431560347604
Epoch 2405
Epoch 2405 :: Batch 0/1
Batch Loss = 0.46607282524294313
2405, epoch_train_loss=0.46607282524294313
Epoch 2406
Epoch 2406 :: Batch 0/1
Batch Loss = 0.4658036294886369
2406, epoch_train_loss=0.4658036294886369
Epoch 2407
Epoch 2407 :: Batch 0/1
Batch Loss = 0.4655337236729873
2407, epoch_train_loss=0.4655337236729873
Epoch 2408
Epoch 2408 :: Batch 0/1
Batch Loss = 0.4652623361501541
2408, epoch_train_loss=0.4652623361501541
Epoch 2409
Epoch 2409 :: Batch 0/1
Batch Loss = 0.46498967107497063
2409, epoch_train_loss=0.46498967107497063
Epoch 2410
Epoch 2410 :: Batch 0/1
Batch Loss = 0.4647163182328032
2410, epoch_train_loss=0.4647163182328032
Epoch 2411
Epoch 2411 :: Batch 0/1
Batch Loss = 0.46444380286445547
2411, epoch_train_loss=0.46444380286445547
Epoch 2412
Epoch 2412 :: Batch 0/1
Batch Loss = 0.46417284636667105
2412, epoch_train_loss=0.46417284636667105
Epoch 2413
Epoch 2413 :: Batch 0/1
Batch Loss = 0.4639025743778256
2413, epoch_train_loss=0.4639025743778256
Epoch 2414
Epoch 2414 :: Batch 0/1
Batch Loss = 0.4636319002358139
2414, epoch_train_loss=0.4636319002358139
Epoch 2415
Epoch 2415 :: Batch 0/1
Batch Loss = 0.4633601707785313
2415, epoch_train_loss=0.4633601707785313
Epoch 2416
Epoch 2416 :: Batch 0/1
Batch Loss = 0.4630877562270965
2416, epoch_train_loss=0.4630877562270965
Epoch 2417
Epoch 2417 :: Batch 0/1
Batch Loss = 0.46281548700065717
2417, epoch_train_loss=0.46281548700065717
Epoch 2418
Epoch 2418 :: Batch 0/1
Batch Loss = 0.46254373551639005
2418, epoch_train_loss=0.46254373551639005
Epoch 2419
Epoch 2419 :: Batch 0/1
Batch Loss = 0.4622726419607284
2419, epoch_train_loss=0.4622726419607284
Epoch 2420
Epoch 2420 :: Batch 0/1
Batch Loss = 0.4620019039186807
2420, epoch_train_loss=0.4620019039186807
Epoch 2421
Epoch 2421 :: Batch 0/1
Batch Loss = 0.4617310039529862
2421, epoch_train_loss=0.4617310039529862
Epoch 2422
Epoch 2422 :: Batch 0/1
Batch Loss = 0.46145976779121806
2422, epoch_train_loss=0.46145976779121806
Epoch 2423
Epoch 2423 :: Batch 0/1
Batch Loss = 0.46118828535988665
2423, epoch_train_loss=0.46118828535988665
Epoch 2424
Epoch 2424 :: Batch 0/1
Batch Loss = 0.4609169291454851
2424, epoch_train_loss=0.4609169291454851
Epoch 2425
Epoch 2425 :: Batch 0/1
Batch Loss = 0.46064601457687854
2425, epoch_train_loss=0.46064601457687854
Epoch 2426
Epoch 2426 :: Batch 0/1
Batch Loss = 0.46037549163492375
2426, epoch_train_loss=0.46037549163492375
Epoch 2427
Epoch 2427 :: Batch 0/1
Batch Loss = 0.4601051771425995
2427, epoch_train_loss=0.4601051771425995
Epoch 2428
Epoch 2428 :: Batch 0/1
Batch Loss = 0.45983483220591653
2428, epoch_train_loss=0.45983483220591653
Epoch 2429
Epoch 2429 :: Batch 0/1
Batch Loss = 0.4595643689767026
2429, epoch_train_loss=0.4595643689767026
Epoch 2430
Epoch 2430 :: Batch 0/1
Batch Loss = 0.4592939362663282
2430, epoch_train_loss=0.4592939362663282
Epoch 2431
Epoch 2431 :: Batch 0/1
Batch Loss = 0.45902367280686013
2431, epoch_train_loss=0.45902367280686013
Epoch 2432
Epoch 2432 :: Batch 0/1
Batch Loss = 0.4587536697860029
2432, epoch_train_loss=0.4587536697860029
Epoch 2433
Epoch 2433 :: Batch 0/1
Batch Loss = 0.4584839152154786
2433, epoch_train_loss=0.4584839152154786
Epoch 2434
Epoch 2434 :: Batch 0/1
Batch Loss = 0.4582143157500598
2434, epoch_train_loss=0.4582143157500598
Epoch 2435
Epoch 2435 :: Batch 0/1
Batch Loss = 0.4579448085218306
2435, epoch_train_loss=0.4579448085218306
Epoch 2436
Epoch 2436 :: Batch 0/1
Batch Loss = 0.45767537704435385
2436, epoch_train_loss=0.45767537704435385
Epoch 2437
Epoch 2437 :: Batch 0/1
Batch Loss = 0.4574087423512058
2437, epoch_train_loss=0.4574087423512058
Epoch 2438
Epoch 2438 :: Batch 0/1
Batch Loss = 0.4571808571511631
2438, epoch_train_loss=0.4571808571511631
Epoch 2439
Epoch 2439 :: Batch 0/1
Batch Loss = 0.457006118786518
2439, epoch_train_loss=0.457006118786518
Epoch 2440
Epoch 2440 :: Batch 0/1
Batch Loss = 0.4567209754661452
2440, epoch_train_loss=0.4567209754661452
Epoch 2441
Epoch 2441 :: Batch 0/1
Batch Loss = 0.456446094996109
2441, epoch_train_loss=0.456446094996109
Epoch 2442
Epoch 2442 :: Batch 0/1
Batch Loss = 0.4562281895855555
2442, epoch_train_loss=0.4562281895855555
Epoch 2443
Epoch 2443 :: Batch 0/1
Batch Loss = 0.4560149210821826
2443, epoch_train_loss=0.4560149210821826
Epoch 2444
Epoch 2444 :: Batch 0/1
Batch Loss = 0.45579340689210124
2444, epoch_train_loss=0.45579340689210124
Epoch 2445
Epoch 2445 :: Batch 0/1
Batch Loss = 0.4555610842435582
2445, epoch_train_loss=0.4555610842435582
Epoch 2446
Epoch 2446 :: Batch 0/1
Batch Loss = 0.45531774277799814
2446, epoch_train_loss=0.45531774277799814
Epoch 2447
Epoch 2447 :: Batch 0/1
Batch Loss = 0.45506541375641807
2447, epoch_train_loss=0.45506541375641807
Epoch 2448
Epoch 2448 :: Batch 0/1
Batch Loss = 0.45480942623068565
2448, epoch_train_loss=0.45480942623068565
Epoch 2449
Epoch 2449 :: Batch 0/1
Batch Loss = 0.454563733443795
2449, epoch_train_loss=0.454563733443795
Epoch 2450
Epoch 2450 :: Batch 0/1
Batch Loss = 0.4543468206351298
2450, epoch_train_loss=0.4543468206351298
Epoch 2451
Epoch 2451 :: Batch 0/1
Batch Loss = 0.4541183994625634
2451, epoch_train_loss=0.4541183994625634
Epoch 2452
Epoch 2452 :: Batch 0/1
Batch Loss = 0.4538613862912293
2452, epoch_train_loss=0.4538613862912293
Epoch 2453
Epoch 2453 :: Batch 0/1
Batch Loss = 0.4536227408409401
2453, epoch_train_loss=0.4536227408409401
Epoch 2454
Epoch 2454 :: Batch 0/1
Batch Loss = 0.45339670841329677
2454, epoch_train_loss=0.45339670841329677
Epoch 2455
Epoch 2455 :: Batch 0/1
Batch Loss = 0.4531700756236886
2455, epoch_train_loss=0.4531700756236886
Epoch 2456
Epoch 2456 :: Batch 0/1
Batch Loss = 0.4529396398602026
2456, epoch_train_loss=0.4529396398602026
Epoch 2457
Epoch 2457 :: Batch 0/1
Batch Loss = 0.45270562224030997
2457, epoch_train_loss=0.45270562224030997
Epoch 2458
Epoch 2458 :: Batch 0/1
Batch Loss = 0.4524694066698853
2458, epoch_train_loss=0.4524694066698853
Epoch 2459
Epoch 2459 :: Batch 0/1
Batch Loss = 0.4522332569982472
2459, epoch_train_loss=0.4522332569982472
Epoch 2460
Epoch 2460 :: Batch 0/1
Batch Loss = 0.4520009095247841
2460, epoch_train_loss=0.4520009095247841
Epoch 2461
Epoch 2461 :: Batch 0/1
Batch Loss = 0.4517745538833602
2461, epoch_train_loss=0.4517745538833602
Epoch 2462
Epoch 2462 :: Batch 0/1
Batch Loss = 0.4515466603276232
2462, epoch_train_loss=0.4515466603276232
Epoch 2463
Epoch 2463 :: Batch 0/1
Batch Loss = 0.45131149748642246
2463, epoch_train_loss=0.45131149748642246
Epoch 2464
Epoch 2464 :: Batch 0/1
Batch Loss = 0.4510772527420874
2464, epoch_train_loss=0.4510772527420874
Epoch 2465
Epoch 2465 :: Batch 0/1
Batch Loss = 0.4508478631685533
2465, epoch_train_loss=0.4508478631685533
Epoch 2466
Epoch 2466 :: Batch 0/1
Batch Loss = 0.4506202125117353
2466, epoch_train_loss=0.4506202125117353
Epoch 2467
Epoch 2467 :: Batch 0/1
Batch Loss = 0.45039157460691664
2467, epoch_train_loss=0.45039157460691664
Epoch 2468
Epoch 2468 :: Batch 0/1
Batch Loss = 0.4501612748383363
2468, epoch_train_loss=0.4501612748383363
Epoch 2469
Epoch 2469 :: Batch 0/1
Batch Loss = 0.4499302317555988
2469, epoch_train_loss=0.4499302317555988
Epoch 2470
Epoch 2470 :: Batch 0/1
Batch Loss = 0.44970051835948893
2470, epoch_train_loss=0.44970051835948893
Epoch 2471
Epoch 2471 :: Batch 0/1
Batch Loss = 0.4494737513629359
2471, epoch_train_loss=0.4494737513629359
Epoch 2472
Epoch 2472 :: Batch 0/1
Batch Loss = 0.4492478601359069
2472, epoch_train_loss=0.4492478601359069
Epoch 2473
Epoch 2473 :: Batch 0/1
Batch Loss = 0.4490195832324096
2473, epoch_train_loss=0.4490195832324096
Epoch 2474
Epoch 2474 :: Batch 0/1
Batch Loss = 0.44879074125455104
2474, epoch_train_loss=0.44879074125455104
Epoch 2475
Epoch 2475 :: Batch 0/1
Batch Loss = 0.448564010438449
2475, epoch_train_loss=0.448564010438449
Epoch 2476
Epoch 2476 :: Batch 0/1
Batch Loss = 0.44833858575406926
2476, epoch_train_loss=0.44833858575406926
Epoch 2477
Epoch 2477 :: Batch 0/1
Batch Loss = 0.44811272206680197
2477, epoch_train_loss=0.44811272206680197
Epoch 2478
Epoch 2478 :: Batch 0/1
Batch Loss = 0.4478859169402852
2478, epoch_train_loss=0.4478859169402852
Epoch 2479
Epoch 2479 :: Batch 0/1
Batch Loss = 0.44765901039280165
2479, epoch_train_loss=0.44765901039280165
Epoch 2480
Epoch 2480 :: Batch 0/1
Batch Loss = 0.4474331962679967
2480, epoch_train_loss=0.4474331962679967
Epoch 2481
Epoch 2481 :: Batch 0/1
Batch Loss = 0.4472085152071613
2481, epoch_train_loss=0.4472085152071613
Epoch 2482
Epoch 2482 :: Batch 0/1
Batch Loss = 0.44698365810722646
2482, epoch_train_loss=0.44698365810722646
Epoch 2483
Epoch 2483 :: Batch 0/1
Batch Loss = 0.44675815659756657
2483, epoch_train_loss=0.44675815659756657
Epoch 2484
Epoch 2484 :: Batch 0/1
Batch Loss = 0.44653302959497615
2484, epoch_train_loss=0.44653302959497615
Epoch 2485
Epoch 2485 :: Batch 0/1
Batch Loss = 0.44630883890245854
2485, epoch_train_loss=0.44630883890245854
Epoch 2486
Epoch 2486 :: Batch 0/1
Batch Loss = 0.4460851209192806
2486, epoch_train_loss=0.4460851209192806
Epoch 2487
Epoch 2487 :: Batch 0/1
Batch Loss = 0.4458613059459418
2487, epoch_train_loss=0.4458613059459418
Epoch 2488
Epoch 2488 :: Batch 0/1
Batch Loss = 0.44563734370667313
2488, epoch_train_loss=0.44563734370667313
Epoch 2489
Epoch 2489 :: Batch 0/1
Batch Loss = 0.44541363962846336
2489, epoch_train_loss=0.44541363962846336
Epoch 2490
Epoch 2490 :: Batch 0/1
Batch Loss = 0.4451905117641484
2490, epoch_train_loss=0.4451905117641484
Epoch 2491
Epoch 2491 :: Batch 0/1
Batch Loss = 0.44496774428254277
2491, epoch_train_loss=0.44496774428254277
Epoch 2492
Epoch 2492 :: Batch 0/1
Batch Loss = 0.4447449098390958
2492, epoch_train_loss=0.4447449098390958
Epoch 2493
Epoch 2493 :: Batch 0/1
Batch Loss = 0.4445220313921381
2493, epoch_train_loss=0.4445220313921381
Epoch 2494
Epoch 2494 :: Batch 0/1
Batch Loss = 0.4442994515894364
2494, epoch_train_loss=0.4442994515894364
Epoch 2495
Epoch 2495 :: Batch 0/1
Batch Loss = 0.44407726128138797
2495, epoch_train_loss=0.44407726128138797
Epoch 2496
Epoch 2496 :: Batch 0/1
Batch Loss = 0.44385525405157333
2496, epoch_train_loss=0.44385525405157333
Epoch 2497
Epoch 2497 :: Batch 0/1
Batch Loss = 0.44363325937618303
2497, epoch_train_loss=0.44363325937618303
Epoch 2498
Epoch 2498 :: Batch 0/1
Batch Loss = 0.44341133526908555
2498, epoch_train_loss=0.44341133526908555
Epoch 2499
Epoch 2499 :: Batch 0/1
Batch Loss = 0.4431896540584056
2499, epoch_train_loss=0.4431896540584056
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting "B3LYP_WITH_VWN5 = True" in pyscf_conf.py
  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '
Warning: <pyscf.gto.mole.Mole object at 0x7ffec40a4a30> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec40a4a30> in UKS object of <class 'pyscf.dft.uks.UKS'>
WARNING: External module "mldftdat" required for non-local descriptor use.
<pyscf.gto.mole.Mole object at 0x7ffec40a4a30> [['P', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec42f9c90> [['N', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec42f9ea0> [['H', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec42fb7f0> [['Li', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec42fb580> [['O', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec42f8610> [['Cl', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec42f8e50> [['Al', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec42fb3a0> [['S', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec42fa380> [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec42fb6d0> [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec42f9870> [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec42f8730> [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]] 3
<pyscf.gto.mole.Mole object at 0x7ffec42f9480> [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]] 3
<pyscf.gto.mole.Mole object at 0x7ffec42fb250> [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec42d3490> [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec42d0c70> [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec42d2620> [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]] 4
<pyscf.gto.mole.Mole object at 0x7ffec42d3400> [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec42d31c0> [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec42d1240> [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec42d0700> [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec42d0ac0> [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]] 4
<pyscf.gto.mole.Mole object at 0x7ffec42d09d0> [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]] 4
<pyscf.gto.mole.Mole object at 0x7ffec42d13c0> [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec42d24d0> [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]] 5
<pyscf.gto.mole.Mole object at 0x7ffec42d13f0> [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]] 3
<pyscf.gto.mole.Mole object at 0x7ffec42d14e0> [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]] 5
mol:  [['P', array([0., 0., 0.])]]
converged SCF energy = -341.104145992717  <S^2> = 3.7502984  2S+1 = 4.0001492
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42f9c90> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42f9c90> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-3.47389956e-03 -8.82676818e-04 -2.08411238e-03 ... -1.11301603e+01
 -1.11301603e+01 -1.11301603e+01] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 16)
mol:  [['N', array([0., 0., 0.])]]
converged SCF energy = -54.5289742046675  <S^2> = 3.7524945  2S+1 = 4.0012471
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42f9ea0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42f9ea0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5016, 22)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.07670570e-03 -5.92340671e-04 -6.66573372e-05 ... -5.03679786e+00
 -5.03679786e+00 -5.03679786e+00] = SCAN,
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5016, 22)
rho shape (4, 4, 2, 5016)
rho_filt shape: (5016,)
get descriptors tdrho.shape=(5016, 16)
mol:  [['H', array([0., 0., 0.])]]
converged SCF energy = -0.499812984008539  <S^2> = 0.75  2S+1 = 2
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42fb7f0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42fb7f0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 7, 7)
ao.shape (10, 2440, 7)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.59173730e-03 -7.67300285e-04 -6.09330303e-05 ... -7.27736608e-01
 -7.27736608e-01 -7.27736608e-01] = SCAN,
get_data, dm shape = (2, 7, 7)
ao_eval.shape=(4, 2440, 7)
rho shape (4, 4, 2, 2440)
rho_filt shape: (2440,)
get descriptors tdrho.shape=(2440, 16)
mol:  [['Li', array([0., 0., 0.])]]
converged SCF energy = -7.46006188627841  <S^2> = 0.75000049  2S+1 = 2.0000005
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42fb580> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42fb580> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 4592, 22)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-6.71503005e-03 -1.44519923e-03 -1.44519923e-03 ... -1.46899070e-02
 -2.03947707e+00 -2.03947707e+00] = SCAN,
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 4592, 22)
rho shape (4, 4, 2, 4592)
rho_filt shape: (4592,)
get descriptors tdrho.shape=(4592, 16)
mol:  [['O', array([0., 0., 0.])]]
converged SCF energy = -75.0033802925613  <S^2> = 2.0027445  2S+1 = 3.0018291
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42f8610> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42f8610> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5040, 22)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-6.33488417e-04 -1.08917461e-04 -5.45248598e-06 ... -5.78449347e+00
 -5.78449347e+00 -5.78449347e+00] = SCAN,
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5040, 22)
rho shape (4, 4, 2, 5040)
rho_filt shape: (5040,)
get descriptors tdrho.shape=(5040, 16)
mol:  [['Cl', array([0., 0., 0.])]]
converged SCF energy = -459.957577121955  <S^2> = 0.75161941  2S+1 = 2.0016188
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42f8e50> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42f8e50> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6152, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-9.59197305e-04 -9.57081192e-04 -3.30728589e-04 ... -1.26648275e+01
 -1.26648275e+01 -1.26648275e+01] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6152, 30)
rho shape (4, 4, 2, 6152)
rho_filt shape: (6152,)
get descriptors tdrho.shape=(6152, 16)
mol:  [['Al', array([0., 0., 0.])]]
converged SCF energy = -242.226560989243  <S^2> = 0.75226414  2S+1 = 2.0022629
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42fb3a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42fb3a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6088, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.33118550e-02 -8.49068703e-03 -4.25301188e-03 ... -1.37659916e-04
 -1.02991814e-03 -7.41975204e-05] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6088, 30)
rho shape (4, 4, 2, 6088)
rho_filt shape: (6088,)
get descriptors tdrho.shape=(6088, 16)
mol:  [['S', array([0., 0., 0.])]]
converged SCF energy = -397.938786830939  <S^2> = 2.0022329  2S+1 = 3.0014882
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42fa380> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42fa380> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.68269746e-03 -7.17875172e-04 -9.02884636e-04 ... -1.18986567e+01
 -1.18986567e+01 -1.18986567e+01] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 16)
mol:  [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]]
converged SCF energy = -1.16580491182912  <S^2> = 4.4408921e-16  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42fb6d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42fb6d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 14, 14)
ao.shape (10, 4776, 14)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.31557088e-04 -9.73828620e-06 -3.66768667e-04 ... -5.54165574e-01
 -5.54165574e-01 -5.54165574e-01] = SCAN,
get_data, dm shape = (2, 14, 14)
ao_eval.shape=(4, 4776, 14)
rho shape (4, 4, 2, 4776)
rho_filt shape: (4776,)
get descriptors tdrho.shape=(4776, 16)
mol:  [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]]
converged SCF energy = -109.439263799566  <S^2> = 1.4210855e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42f9870> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42f9870> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9848, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-9.68474977e-05 -9.84742592e-04 -2.59676393e-04 ... -2.39645778e-05
 -2.39645778e-05 -9.68474977e-05] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9848, 44)
rho shape (4, 4, 2, 9848)
rho_filt shape: (9848,)
get descriptors tdrho.shape=(9848, 16)
mol:  [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]]
converged SCF energy = -107.339357395734  <S^2> = 1.7763568e-15  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42f8730> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42f8730> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9752, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.04987770e-03 -6.68954111e-04 -8.57556562e-04 ... -1.07485605e-03
 -8.01425702e-01 -8.01425702e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9752, 44)
rho shape (4, 4, 2, 9752)
rho_filt shape: (9752,)
get descriptors tdrho.shape=(9752, 16)
mol:  [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]]
converged SCF energy = -93.3377924465131  <S^2> = 4.0073189e-10  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42f9480> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42f9480> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 12256, 51)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.97917639e-04 -2.54437615e-05 -3.15202008e-05 ... -6.37386388e-01
 -6.37386388e-01 -6.37386388e-01] = SCAN,
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 12256, 51)
rho shape (4, 4, 2, 12256)
rho_filt shape: (12256,)
get descriptors tdrho.shape=(12256, 16)
mol:  [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]]
converged SCF energy = -188.456965322843  <S^2> = 1.7763568e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42fb250> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42fb250> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 66, 66)
ao.shape (10, 14920, 66)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.50217343e-04 -2.07520331e-04 -9.23619961e-04 ... -2.76182455e-06
 -4.27559894e+00 -4.27559894e+00] = SCAN,
get_data, dm shape = (2, 66, 66)
ao_eval.shape=(4, 14920, 66)
rho shape (4, 4, 2, 14920)
rho_filt shape: (14920,)
get descriptors tdrho.shape=(14920, 16)
mol:  [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]]
converged SCF energy = -920.00560888896  <S^2> = 5.0093263e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42d3490> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42d3490> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12208, 60)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-0.00155834 -0.00091688 -0.00215831 ... -0.00091688 -0.41618506
 -0.41618506] = SCAN,
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12208, 60)
rho shape (4, 4, 2, 12208)
rho_filt shape: (12208,)
get descriptors tdrho.shape=(12208, 16)
mol:  [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]]
converged SCF energy = -199.394370591172  <S^2> = 1.1723955e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42d0c70> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42d0c70> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9824, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.92948752e-04 -1.95215890e-05 -1.16699780e-03 ... -4.89378326e-01
 -4.89378326e-01 -4.89378326e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9824, 44)
rho shape (4, 4, 2, 9824)
rho_filt shape: (9824,)
get descriptors tdrho.shape=(9824, 16)
mol:  [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]]
converged SCF energy = -150.214894435364  <S^2> = 1.0018598  2S+1 = 2.2377308
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42d2620> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42d2620> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9912, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-7.75399118e-04 -1.38913432e-04 -7.19465430e-06 ... -6.59150673e-01
 -6.59150673e-01 -6.59150673e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9912, 44)
rho shape (4, 4, 2, 9912)
rho_filt shape: (9912,)
get descriptors tdrho.shape=(9912, 16)
mol:  [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]]
converged SCF energy = -77.2435048346374  <S^2> = 7.1054274e-15  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42d3400> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42d3400> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 58, 58)
ao.shape (10, 15208, 58)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-8.83278187e-05 -8.83278187e-05 -9.75839793e-04 ... -3.46740731e-05
 -3.31729009e-05 -3.31729009e-05] = SCAN,
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15208, 58)
rho shape (4, 4, 2, 15208)
rho_filt shape: (15208,)
get descriptors tdrho.shape=(15208, 16)
mol:  [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]]
converged SCF energy = -113.221335689652  <S^2> = 6.5902839e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42d31c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42d31c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 10040, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-5.37000596e-04 -8.55494373e-04 -2.46853248e-03 ... -7.34251993e-01
 -7.34251993e-01 -7.34251993e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 10040, 44)
rho shape (4, 4, 2, 10040)
rho_filt shape: (10040,)
get descriptors tdrho.shape=(10040, 16)
mol:  [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]]
converged SCF energy = -460.624592374077  <S^2> = 7.1054274e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42d1240> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42d1240> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 37, 37)
ao.shape (10, 8552, 37)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.38161478e-04 -1.81223966e-05 -2.37327566e-05 ... -2.83738108e+00
 -2.83738108e+00 -2.83738108e+00] = SCAN,
get_data, dm shape = (2, 37, 37)
ao_eval.shape=(4, 8552, 37)
rho shape (4, 4, 2, 8552)
rho_filt shape: (8552,)
get descriptors tdrho.shape=(8552, 16)
mol:  [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]]
converged SCF energy = -8.04458854018922  <S^2> = 7.9047879e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42d0700> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42d0700> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 29, 29)
ao.shape (10, 6936, 29)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-0.00113445 -0.00118624 -0.00118624 ... -0.48434639 -0.48434639
 -0.48434639] = SCAN,
get_data, dm shape = (2, 29, 29)
ao_eval.shape=(4, 6936, 29)
rho shape (4, 4, 2, 6936)
rho_filt shape: (6936,)
get descriptors tdrho.shape=(6936, 16)
mol:  [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]]
converged SCF energy = -324.340512506578  <S^2> = 1.5855761e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42d0ac0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42d0ac0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 11536, 60)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-0.00297936 -0.00297936 -0.00407091 ... -0.00297936 -0.00297936
 -0.00407091] = SCAN,
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 11536, 60)
rho shape (4, 4, 2, 11536)
rho_filt shape: (11536,)
get descriptors tdrho.shape=(11536, 16)
mol:  [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]]
converged SCF energy = -1622.57507845814  <S^2> = 8.2422957e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42d09d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42d09d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 120, 120)
ao.shape (10, 24512, 120)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.61401455e-04 -4.90485117e-04 -2.56451688e-03 ... -9.59296114e+00
 -9.59296114e+00 -9.59296114e+00] = SCAN,
get_data, dm shape = (2, 120, 120)
ao_eval.shape=(4, 24512, 120)
rho shape (4, 4, 2, 24512)
rho_filt shape: (24512,)
get descriptors tdrho.shape=(24512, 16)
mol:  [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]]
converged SCF energy = -342.979728469574  <S^2> = 2.5394797e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42d13c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42d13c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 13096, 51)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.28637187e-03 -4.32380890e-04 -3.74072638e-05 ... -1.91722763e+00
 -1.91722763e+00 -1.91722763e+00] = SCAN,
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 13096, 51)
rho shape (4, 4, 2, 13096)
rho_filt shape: (13096,)
get descriptors tdrho.shape=(13096, 16)
mol:  [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]]
converged SCF energy = -578.565335809629  <S^2> = 1.0034705  2S+1 = 2.2391699
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42d24d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42d24d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12384, 60)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.60128931e-04 -2.60147640e-04 -2.60153280e-04 ... -3.86944099e-01
 -3.86944099e-01 -3.86944099e-01] = SCAN,
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12384, 60)
rho shape (4, 4, 2, 12384)
rho_filt shape: (12384,)
get descriptors tdrho.shape=(12384, 16)
mol:  [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]]
converged SCF energy = -40.4598214864075  <S^2> = 3.2063241e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42d13f0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42d13f0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 50, 50)
ao.shape (10, 13936, 50)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-8.68439856e-04 -2.42462783e-04 -1.69965237e-05 ... -2.55256081e-05
 -2.55256081e-05 -2.55256081e-05] = SCAN,
get_data, dm shape = (2, 50, 50)
ao_eval.shape=(4, 13936, 50)
rho shape (4, 4, 2, 13936)
rho_filt shape: (13936,)
get descriptors tdrho.shape=(13936, 16)
mol:  [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]]
converged SCF energy = -39.0756147483504  <S^2> = 6.1990413e-12  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffec42d14e0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec42d14e0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 36, 36)
ao.shape (10, 9656, 36)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-7.67691257e-04 -4.57409182e-05 -2.02835243e-04 ... -1.14928928e+00
 -1.14928928e+00 -1.14928928e+00] = SCAN,
get_data, dm shape = (2, 36, 36)
ao_eval.shape=(4, 9656, 36)
rho shape (4, 4, 2, 9656)
rho_filt shape: (9656,)
get descriptors tdrho.shape=(9656, 16)
mol:  [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]]
converged SCF energy = -291.719272437819  <S^2> = 1.3155699e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
New DM shape: (2, 58, 58)
ao.shape (10, 15256, 58)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-8.33847724e-04 -2.34902391e-04 -1.75660753e-05 ... -1.92925750e-05
 -1.92925750e-05 -1.92925750e-05] = SCAN,
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15256, 58)
rho shape (4, 4, 2, 15256)
rho_filt shape: (15256,)
get descriptors tdrho.shape=(15256, 16)
PRE NAN FILT: tFxc.shape=(237013,), tdrho.shape=(237013, 16)
nan_filt_rho.shape=(237013,)
nan_filt_fxc.shape=(237013,)
tFxc.shape=(237013,), tdrho.shape=(237013, 16)
inp[0].shape = (237013, 16)
Epoch 0
Epoch 0 :: Batch 0/1
Batch Loss = 8138533.367451707
0, epoch_train_loss=8138533.367451707
Epoch 1
Epoch 1 :: Batch 0/1
Batch Loss = 224640.31196927596
1, epoch_train_loss=224640.31196927596
Epoch 2
Epoch 2 :: Batch 0/1
Batch Loss = 13.87681175902535
2, epoch_train_loss=13.87681175902535
Epoch 3
Epoch 3 :: Batch 0/1
Batch Loss = 41403.43961970564
3, epoch_train_loss=41403.43961970564
Epoch 4
Epoch 4 :: Batch 0/1
Batch Loss = 1581881.4125262697
4, epoch_train_loss=1581881.4125262697
Epoch 5
Epoch 5 :: Batch 0/1
Batch Loss = 3.5972837558675677
5, epoch_train_loss=3.5972837558675677
Epoch 6
Epoch 6 :: Batch 0/1
Batch Loss = 4.6128574933811795
6, epoch_train_loss=4.6128574933811795
Epoch 7
Epoch 7 :: Batch 0/1
Batch Loss = 4.595274100140608
7, epoch_train_loss=4.595274100140608
Epoch 8
Epoch 8 :: Batch 0/1
Batch Loss = 4.570941706783204
8, epoch_train_loss=4.570941706783204
Epoch 9
Epoch 9 :: Batch 0/1
Batch Loss = 23.055359111362627
9, epoch_train_loss=23.055359111362627
Epoch 10
Epoch 10 :: Batch 0/1
Batch Loss = 4.57041983435946
10, epoch_train_loss=4.57041983435946
Epoch 11
Epoch 11 :: Batch 0/1
Batch Loss = 4.568188167607608
11, epoch_train_loss=4.568188167607608
Epoch 12
Epoch 12 :: Batch 0/1
Batch Loss = 4.564368591164169
12, epoch_train_loss=4.564368591164169
Epoch 13
Epoch 13 :: Batch 0/1
Batch Loss = 4.560433871884191
13, epoch_train_loss=4.560433871884191
Epoch 14
Epoch 14 :: Batch 0/1
Batch Loss = 4.556496522628868
14, epoch_train_loss=4.556496522628868
Epoch 15
Epoch 15 :: Batch 0/1
Batch Loss = 4.5527079637264025
15, epoch_train_loss=4.5527079637264025
Epoch 16
Epoch 16 :: Batch 0/1
Batch Loss = 4.549097368922558
16, epoch_train_loss=4.549097368922558
Epoch 17
Epoch 17 :: Batch 0/1
Batch Loss = 4.54567279333928
17, epoch_train_loss=4.54567279333928
Epoch 18
Epoch 18 :: Batch 0/1
Batch Loss = 4.5424494236288995
18, epoch_train_loss=4.5424494236288995
Epoch 19
Epoch 19 :: Batch 0/1
Batch Loss = 4.5393970312645635
19, epoch_train_loss=4.5393970312645635
Epoch 20
Epoch 20 :: Batch 0/1
Batch Loss = 4.536462998593283
20, epoch_train_loss=4.536462998593283
Epoch 21
Epoch 21 :: Batch 0/1
Batch Loss = 4.533621099880256
21, epoch_train_loss=4.533621099880256
Epoch 22
Epoch 22 :: Batch 0/1
Batch Loss = 4.531134198057755
22, epoch_train_loss=4.531134198057755
Epoch 23
Epoch 23 :: Batch 0/1
Batch Loss = 4.528311194268081
23, epoch_train_loss=4.528311194268081
Epoch 24
Epoch 24 :: Batch 0/1
Batch Loss = 4.525843472638485
24, epoch_train_loss=4.525843472638485
Epoch 25
Epoch 25 :: Batch 0/1
Batch Loss = 4.523411620077204
25, epoch_train_loss=4.523411620077204
Epoch 26
Epoch 26 :: Batch 0/1
Batch Loss = 4.520996699668694
26, epoch_train_loss=4.520996699668694
Epoch 27
Epoch 27 :: Batch 0/1
Batch Loss = 4.518603331087248
27, epoch_train_loss=4.518603331087248
Epoch 28
Epoch 28 :: Batch 0/1
Batch Loss = 4.516228681944123
28, epoch_train_loss=4.516228681944123
Epoch 29
Epoch 29 :: Batch 0/1
Batch Loss = 4.513861087727075
29, epoch_train_loss=4.513861087727075
Epoch 30
Epoch 30 :: Batch 0/1
Batch Loss = 4.511472561270985
30, epoch_train_loss=4.511472561270985
Epoch 31
Epoch 31 :: Batch 0/1
Batch Loss = 4.509017791738431
31, epoch_train_loss=4.509017791738431
Epoch 32
Epoch 32 :: Batch 0/1
Batch Loss = 4.5067914320060325
32, epoch_train_loss=4.5067914320060325
Epoch 33
Epoch 33 :: Batch 0/1
Batch Loss = 4.504348414978327
33, epoch_train_loss=4.504348414978327
Epoch 34
Epoch 34 :: Batch 0/1
Batch Loss = 4.502092429322698
34, epoch_train_loss=4.502092429322698
Epoch 35
Epoch 35 :: Batch 0/1
Batch Loss = 4.499842148618271
35, epoch_train_loss=4.499842148618271
Epoch 36
Epoch 36 :: Batch 0/1
Batch Loss = 4.497511612544938
36, epoch_train_loss=4.497511612544938
Epoch 37
Epoch 37 :: Batch 0/1
Batch Loss = 4.495234158256584
37, epoch_train_loss=4.495234158256584
Epoch 38
Epoch 38 :: Batch 0/1
Batch Loss = 4.492971899021547
38, epoch_train_loss=4.492971899021547
Epoch 39
Epoch 39 :: Batch 0/1
Batch Loss = 4.490705784684569
39, epoch_train_loss=4.490705784684569
Epoch 40
Epoch 40 :: Batch 0/1
Batch Loss = 4.488481682018958
40, epoch_train_loss=4.488481682018958
Epoch 41
Epoch 41 :: Batch 0/1
Batch Loss = 4.486172691975136
41, epoch_train_loss=4.486172691975136
Epoch 42
Epoch 42 :: Batch 0/1
Batch Loss = 4.4839949698978305
42, epoch_train_loss=4.4839949698978305
Epoch 43
Epoch 43 :: Batch 0/1
Batch Loss = 4.481674881560087
43, epoch_train_loss=4.481674881560087
Epoch 44
Epoch 44 :: Batch 0/1
Batch Loss = 4.479472299794413
44, epoch_train_loss=4.479472299794413
Epoch 45
Epoch 45 :: Batch 0/1
Batch Loss = 4.477178729138626
45, epoch_train_loss=4.477178729138626
Epoch 46
Epoch 46 :: Batch 0/1
Batch Loss = 4.474987218915251
46, epoch_train_loss=4.474987218915251
Epoch 47
Epoch 47 :: Batch 0/1
Batch Loss = 4.472681892777465
47, epoch_train_loss=4.472681892777465
Epoch 48
Epoch 48 :: Batch 0/1
Batch Loss = 4.47046988572222
48, epoch_train_loss=4.47046988572222
Epoch 49
Epoch 49 :: Batch 0/1
Batch Loss = 4.468185463443041
49, epoch_train_loss=4.468185463443041
Epoch 50
Epoch 50 :: Batch 0/1
Batch Loss = 4.4659924219787905
50, epoch_train_loss=4.4659924219787905
Epoch 51
Epoch 51 :: Batch 0/1
Batch Loss = 4.463706715628831
51, epoch_train_loss=4.463706715628831
Epoch 52
Epoch 52 :: Batch 0/1
Batch Loss = 4.46148614608527
52, epoch_train_loss=4.46148614608527
Epoch 53
Epoch 53 :: Batch 0/1
Batch Loss = 4.459198866432476
53, epoch_train_loss=4.459198866432476
Epoch 54
Epoch 54 :: Batch 0/1
Batch Loss = 4.457006627904572
54, epoch_train_loss=4.457006627904572
Epoch 55
Epoch 55 :: Batch 0/1
Batch Loss = 4.4547403152905565
55, epoch_train_loss=4.4547403152905565
Epoch 56
Epoch 56 :: Batch 0/1
Batch Loss = 4.45251675680082
56, epoch_train_loss=4.45251675680082
Epoch 57
Epoch 57 :: Batch 0/1
Batch Loss = 4.450220815632685
57, epoch_train_loss=4.450220815632685
Epoch 58
Epoch 58 :: Batch 0/1
Batch Loss = 4.448040387465877
58, epoch_train_loss=4.448040387465877
Epoch 59
Epoch 59 :: Batch 0/1
Batch Loss = 4.445784412888096
59, epoch_train_loss=4.445784412888096
Epoch 60
Epoch 60 :: Batch 0/1
Batch Loss = 4.443573214671024
60, epoch_train_loss=4.443573214671024
Epoch 61
Epoch 61 :: Batch 0/1
Batch Loss = 4.441255338456312
61, epoch_train_loss=4.441255338456312
Epoch 62
Epoch 62 :: Batch 0/1
Batch Loss = 4.4391362969707036
62, epoch_train_loss=4.4391362969707036
Epoch 63
Epoch 63 :: Batch 0/1
Batch Loss = 4.43685994759759
63, epoch_train_loss=4.43685994759759
Epoch 64
Epoch 64 :: Batch 0/1
Batch Loss = 4.434702949127458
64, epoch_train_loss=4.434702949127458
Epoch 65
Epoch 65 :: Batch 0/1
Batch Loss = 4.432339628858255
65, epoch_train_loss=4.432339628858255
Epoch 66
Epoch 66 :: Batch 0/1
Batch Loss = 4.430307825621228
66, epoch_train_loss=4.430307825621228
Epoch 67
Epoch 67 :: Batch 0/1
Batch Loss = 4.42792605179554
67, epoch_train_loss=4.42792605179554
Epoch 68
Epoch 68 :: Batch 0/1
Batch Loss = 4.425832507698532
68, epoch_train_loss=4.425832507698532
Epoch 69
Epoch 69 :: Batch 0/1
Batch Loss = 4.423510335044642
69, epoch_train_loss=4.423510335044642
Epoch 70
Epoch 70 :: Batch 0/1
Batch Loss = 4.42130840382861
70, epoch_train_loss=4.42130840382861
Epoch 71
Epoch 71 :: Batch 0/1
Batch Loss = 4.419056786040238
71, epoch_train_loss=4.419056786040238
Epoch 72
Epoch 72 :: Batch 0/1
Batch Loss = 4.416873652654918
72, epoch_train_loss=4.416873652654918
Epoch 73
Epoch 73 :: Batch 0/1
Batch Loss = 4.41470432443617
73, epoch_train_loss=4.41470432443617
Epoch 74
Epoch 74 :: Batch 0/1
Batch Loss = 4.412441175870114
74, epoch_train_loss=4.412441175870114
Epoch 75
Epoch 75 :: Batch 0/1
Batch Loss = 4.410350126280424
75, epoch_train_loss=4.410350126280424
Epoch 76
Epoch 76 :: Batch 0/1
Batch Loss = 4.408087422930508
76, epoch_train_loss=4.408087422930508
Epoch 77
Epoch 77 :: Batch 0/1
Batch Loss = 4.40597541943493
77, epoch_train_loss=4.40597541943493
Epoch 78
Epoch 78 :: Batch 0/1
Batch Loss = 4.403751845250099
78, epoch_train_loss=4.403751845250099
Epoch 79
Epoch 79 :: Batch 0/1
Batch Loss = 4.401633439807304
79, epoch_train_loss=4.401633439807304
Epoch 80
Epoch 80 :: Batch 0/1
Batch Loss = 4.399429505698238
80, epoch_train_loss=4.399429505698238
Epoch 81
Epoch 81 :: Batch 0/1
Batch Loss = 4.39731205286385
81, epoch_train_loss=4.39731205286385
Epoch 82
Epoch 82 :: Batch 0/1
Batch Loss = 4.395162877018285
82, epoch_train_loss=4.395162877018285
Epoch 83
Epoch 83 :: Batch 0/1
Batch Loss = 4.3930253453760715
83, epoch_train_loss=4.3930253453760715
Epoch 84
Epoch 84 :: Batch 0/1
Batch Loss = 4.390909635554538
84, epoch_train_loss=4.390909635554538
Epoch 85
Epoch 85 :: Batch 0/1
Batch Loss = 4.388789305412558
85, epoch_train_loss=4.388789305412558
Epoch 86
Epoch 86 :: Batch 0/1
Batch Loss = 4.386693788867471
86, epoch_train_loss=4.386693788867471
Epoch 87
Epoch 87 :: Batch 0/1
Batch Loss = 4.3845774820346675
87, epoch_train_loss=4.3845774820346675
Epoch 88
Epoch 88 :: Batch 0/1
Batch Loss = 4.382510345424029
88, epoch_train_loss=4.382510345424029
Epoch 89
Epoch 89 :: Batch 0/1
Batch Loss = 4.38041582238007
89, epoch_train_loss=4.38041582238007
Epoch 90
Epoch 90 :: Batch 0/1
Batch Loss = 4.378359187424582
90, epoch_train_loss=4.378359187424582
Epoch 91
Epoch 91 :: Batch 0/1
Batch Loss = 4.376281969344607
91, epoch_train_loss=4.376281969344607
Epoch 92
Epoch 92 :: Batch 0/1
Batch Loss = 4.374249323532403
92, epoch_train_loss=4.374249323532403
Epoch 93
Epoch 93 :: Batch 0/1
Batch Loss = 4.372198074512023
93, epoch_train_loss=4.372198074512023
Epoch 94
Epoch 94 :: Batch 0/1
Batch Loss = 4.370175806280111
94, epoch_train_loss=4.370175806280111
Epoch 95
Epoch 95 :: Batch 0/1
Batch Loss = 4.368146129120967
95, epoch_train_loss=4.368146129120967
Epoch 96
Epoch 96 :: Batch 0/1
Batch Loss = 4.366143178652927
96, epoch_train_loss=4.366143178652927
Epoch 97
Epoch 97 :: Batch 0/1
Batch Loss = 4.364144378392223
97, epoch_train_loss=4.364144378392223
Epoch 98
Epoch 98 :: Batch 0/1
Batch Loss = 4.362155177797004
98, epoch_train_loss=4.362155177797004
Epoch 99
Epoch 99 :: Batch 0/1
Batch Loss = 4.360181216229633
99, epoch_train_loss=4.360181216229633
Epoch 100
Epoch 100 :: Batch 0/1
Batch Loss = 4.358207327385364
100, epoch_train_loss=4.358207327385364
Epoch 101
Epoch 101 :: Batch 0/1
Batch Loss = 4.356261241702484
101, epoch_train_loss=4.356261241702484
Epoch 102
Epoch 102 :: Batch 0/1
Batch Loss = 4.354310370870647
102, epoch_train_loss=4.354310370870647
Epoch 103
Epoch 103 :: Batch 0/1
Batch Loss = 4.352388564811874
103, epoch_train_loss=4.352388564811874
Epoch 104
Epoch 104 :: Batch 0/1
Batch Loss = 4.350462005486428
104, epoch_train_loss=4.350462005486428
Epoch 105
Epoch 105 :: Batch 0/1
Batch Loss = 4.3485573353765625
105, epoch_train_loss=4.3485573353765625
Epoch 106
Epoch 106 :: Batch 0/1
Batch Loss = 4.346660776533225
106, epoch_train_loss=4.346660776533225
Epoch 107
Epoch 107 :: Batch 0/1
Batch Loss = 4.344774315610535
107, epoch_train_loss=4.344774315610535
Epoch 108
Epoch 108 :: Batch 0/1
Batch Loss = 4.342906908366668
108, epoch_train_loss=4.342906908366668
Epoch 109
Epoch 109 :: Batch 0/1
Batch Loss = 4.3410433409367615
109, epoch_train_loss=4.3410433409367615
Epoch 110
Epoch 110 :: Batch 0/1
Batch Loss = 4.339201154264798
110, epoch_train_loss=4.339201154264798
Epoch 111
Epoch 111 :: Batch 0/1
Batch Loss = 4.337364685496242
111, epoch_train_loss=4.337364685496242
Epoch 112
Epoch 112 :: Batch 0/1
Batch Loss = 4.335542557052738
112, epoch_train_loss=4.335542557052738
Epoch 113
Epoch 113 :: Batch 0/1
Batch Loss = 4.333736109533345
113, epoch_train_loss=4.333736109533345
Epoch 114
Epoch 114 :: Batch 0/1
Batch Loss = 4.331936366339099
114, epoch_train_loss=4.331936366339099
Epoch 115
Epoch 115 :: Batch 0/1
Batch Loss = 4.330155018800478
115, epoch_train_loss=4.330155018800478
Epoch 116
Epoch 116 :: Batch 0/1
Batch Loss = 4.328383979915126
116, epoch_train_loss=4.328383979915126
Epoch 117
Epoch 117 :: Batch 0/1
Batch Loss = 4.326625320271481
117, epoch_train_loss=4.326625320271481
Epoch 118
Epoch 118 :: Batch 0/1
Batch Loss = 4.324882621704504
118, epoch_train_loss=4.324882621704504
Epoch 119
Epoch 119 :: Batch 0/1
Batch Loss = 4.323149790360985
119, epoch_train_loss=4.323149790360985
Epoch 120
Epoch 120 :: Batch 0/1
Batch Loss = 4.3214322190067564
120, epoch_train_loss=4.3214322190067564
Epoch 121
Epoch 121 :: Batch 0/1
Batch Loss = 4.319727951416821
121, epoch_train_loss=4.319727951416821
Epoch 122
Epoch 122 :: Batch 0/1
Batch Loss = 4.318034983850769
122, epoch_train_loss=4.318034983850769
Epoch 123
Epoch 123 :: Batch 0/1
Batch Loss = 4.316357759385502
123, epoch_train_loss=4.316357759385502
Epoch 124
Epoch 124 :: Batch 0/1
Batch Loss = 4.314692798169162
124, epoch_train_loss=4.314692798169162
Epoch 125
Epoch 125 :: Batch 0/1
Batch Loss = 4.313040182475796
125, epoch_train_loss=4.313040182475796
Epoch 126
Epoch 126 :: Batch 0/1
Batch Loss = 4.311403145993612
126, epoch_train_loss=4.311403145993612
Epoch 127
Epoch 127 :: Batch 0/1
Batch Loss = 4.309778340295448
127, epoch_train_loss=4.309778340295448
Epoch 128
Epoch 128 :: Batch 0/1
Batch Loss = 4.30816616212826
128, epoch_train_loss=4.30816616212826
Epoch 129
Epoch 129 :: Batch 0/1
Batch Loss = 4.306569426232821
129, epoch_train_loss=4.306569426232821
Epoch 130
Epoch 130 :: Batch 0/1
Batch Loss = 4.304985329511471
130, epoch_train_loss=4.304985329511471
Epoch 131
Epoch 131 :: Batch 0/1
Batch Loss = 4.303413655072794
131, epoch_train_loss=4.303413655072794
Epoch 132
Epoch 132 :: Batch 0/1
Batch Loss = 4.301857247239711
132, epoch_train_loss=4.301857247239711
Epoch 133
Epoch 133 :: Batch 0/1
Batch Loss = 4.300314144232325
133, epoch_train_loss=4.300314144232325
Epoch 134
Epoch 134 :: Batch 0/1
Batch Loss = 4.298783175817882
134, epoch_train_loss=4.298783175817882
Epoch 135
Epoch 135 :: Batch 0/1
Batch Loss = 4.297266965869167
135, epoch_train_loss=4.297266965869167
Epoch 136
Epoch 136 :: Batch 0/1
Batch Loss = 4.295764806449673
136, epoch_train_loss=4.295764806449673
Epoch 137
Epoch 137 :: Batch 0/1
Batch Loss = 4.294274920721195
137, epoch_train_loss=4.294274920721195
Epoch 138
Epoch 138 :: Batch 0/1
Batch Loss = 4.292798863289751
138, epoch_train_loss=4.292798863289751
Epoch 139
Epoch 139 :: Batch 0/1
Batch Loss = 4.291337199892787
139, epoch_train_loss=4.291337199892787
Epoch 140
Epoch 140 :: Batch 0/1
Batch Loss = 4.290071361828146
140, epoch_train_loss=4.290071361828146
Epoch 141
Epoch 141 :: Batch 0/1
Batch Loss = 4.290407887404106
141, epoch_train_loss=4.290407887404106
Epoch 142
Epoch 142 :: Batch 0/1
Batch Loss = 4.289446941627961
142, epoch_train_loss=4.289446941627961
Epoch 143
Epoch 143 :: Batch 0/1
Batch Loss = 4.288208395476245
143, epoch_train_loss=4.288208395476245
Epoch 144
Epoch 144 :: Batch 0/1
Batch Loss = 4.286917462946826
144, epoch_train_loss=4.286917462946826
Epoch 145
Epoch 145 :: Batch 0/1
Batch Loss = 4.285610603101555
145, epoch_train_loss=4.285610603101555
Epoch 146
Epoch 146 :: Batch 0/1
Batch Loss = 4.2843014634327865
146, epoch_train_loss=4.2843014634327865
Epoch 147
Epoch 147 :: Batch 0/1
Batch Loss = 4.282996822505276
147, epoch_train_loss=4.282996822505276
Epoch 148
Epoch 148 :: Batch 0/1
Batch Loss = 4.28170037273406
148, epoch_train_loss=4.28170037273406
Epoch 149
Epoch 149 :: Batch 0/1
Batch Loss = 4.280414201504855
149, epoch_train_loss=4.280414201504855
Epoch 150
Epoch 150 :: Batch 0/1
Batch Loss = 4.279139511627666
150, epoch_train_loss=4.279139511627666
Epoch 151
Epoch 151 :: Batch 0/1
Batch Loss = 4.277877002085338
151, epoch_train_loss=4.277877002085338
Epoch 152
Epoch 152 :: Batch 0/1
Batch Loss = 4.276627077377623
152, epoch_train_loss=4.276627077377623
Epoch 153
Epoch 153 :: Batch 0/1
Batch Loss = 4.275389965119245
153, epoch_train_loss=4.275389965119245
Epoch 154
Epoch 154 :: Batch 0/1
Batch Loss = 4.27416578419903
154, epoch_train_loss=4.27416578419903
Epoch 155
Epoch 155 :: Batch 0/1
Batch Loss = 4.272954585444738
155, epoch_train_loss=4.272954585444738
Epoch 156
Epoch 156 :: Batch 0/1
Batch Loss = 4.271756375865249
156, epoch_train_loss=4.271756375865249
Epoch 157
Epoch 157 :: Batch 0/1
Batch Loss = 4.27057113388406
157, epoch_train_loss=4.27057113388406
Epoch 158
Epoch 158 :: Batch 0/1
Batch Loss = 4.269398819264885
158, epoch_train_loss=4.269398819264885
Epoch 159
Epoch 159 :: Batch 0/1
Batch Loss = 4.268239379083944
159, epoch_train_loss=4.268239379083944
Epoch 160
Epoch 160 :: Batch 0/1
Batch Loss = 4.267092751831499
160, epoch_train_loss=4.267092751831499
Epoch 161
Epoch 161 :: Batch 0/1
Batch Loss = 4.265958870395362
161, epoch_train_loss=4.265958870395362
Epoch 162
Epoch 162 :: Batch 0/1
Batch Loss = 4.264837663549033
162, epoch_train_loss=4.264837663549033
Epoch 163
Epoch 163 :: Batch 0/1
Batch Loss = 4.263729057448379
163, epoch_train_loss=4.263729057448379
Epoch 164
Epoch 164 :: Batch 0/1
Batch Loss = 4.262632976469792
164, epoch_train_loss=4.262632976469792
Epoch 165
Epoch 165 :: Batch 0/1
Batch Loss = 4.261549343536105
165, epoch_train_loss=4.261549343536105
Epoch 166
Epoch 166 :: Batch 0/1
Batch Loss = 4.260478080832813
166, epoch_train_loss=4.260478080832813
Epoch 167
Epoch 167 :: Batch 0/1
Batch Loss = 4.259419109743782
167, epoch_train_loss=4.259419109743782
Epoch 168
Epoch 168 :: Batch 0/1
Batch Loss = 4.25837235126595
168, epoch_train_loss=4.25837235126595
Epoch 169
Epoch 169 :: Batch 0/1
Batch Loss = 4.2573377260384575
169, epoch_train_loss=4.2573377260384575
Epoch 170
Epoch 170 :: Batch 0/1
Batch Loss = 4.256315154410727
170, epoch_train_loss=4.256315154410727
Epoch 171
Epoch 171 :: Batch 0/1
Batch Loss = 4.25530455655678
171, epoch_train_loss=4.25530455655678
Epoch 172
Epoch 172 :: Batch 0/1
Batch Loss = 4.254305852266649
172, epoch_train_loss=4.254305852266649
Epoch 173
Epoch 173 :: Batch 0/1
Batch Loss = 4.2533189612551725
173, epoch_train_loss=4.2533189612551725
Epoch 174
Epoch 174 :: Batch 0/1
Batch Loss = 4.25234380278756
174, epoch_train_loss=4.25234380278756
Epoch 175
Epoch 175 :: Batch 0/1
Batch Loss = 4.251380296187431
175, epoch_train_loss=4.251380296187431
Epoch 176
Epoch 176 :: Batch 0/1
Batch Loss = 4.250428360313273
176, epoch_train_loss=4.250428360313273
Epoch 177
Epoch 177 :: Batch 0/1
Batch Loss = 4.249487913840112
177, epoch_train_loss=4.249487913840112
Epoch 178
Epoch 178 :: Batch 0/1
Batch Loss = 4.248558875175271
178, epoch_train_loss=4.248558875175271
Epoch 179
Epoch 179 :: Batch 0/1
Batch Loss = 4.247641162448377
179, epoch_train_loss=4.247641162448377
Epoch 180
Epoch 180 :: Batch 0/1
Batch Loss = 4.24673469350287
180, epoch_train_loss=4.24673469350287
Epoch 181
Epoch 181 :: Batch 0/1
Batch Loss = 4.245839385889264
181, epoch_train_loss=4.245839385889264
Epoch 182
Epoch 182 :: Batch 0/1
Batch Loss = 4.244955156860382
182, epoch_train_loss=4.244955156860382
Epoch 183
Epoch 183 :: Batch 0/1
Batch Loss = 4.244081923368627
183, epoch_train_loss=4.244081923368627
Epoch 184
Epoch 184 :: Batch 0/1
Batch Loss = 4.243219602065272
184, epoch_train_loss=4.243219602065272
Epoch 185
Epoch 185 :: Batch 0/1
Batch Loss = 4.242368109301731
185, epoch_train_loss=4.242368109301731
Epoch 186
Epoch 186 :: Batch 0/1
Batch Loss = 4.241527361069973
186, epoch_train_loss=4.241527361069973
Epoch 187
Epoch 187 :: Batch 0/1
Batch Loss = 4.24069727325903
187, epoch_train_loss=4.24069727325903
Epoch 188
Epoch 188 :: Batch 0/1
Batch Loss = 4.239877761222306
188, epoch_train_loss=4.239877761222306
Epoch 189
Epoch 189 :: Batch 0/1
Batch Loss = 4.239068740223122
189, epoch_train_loss=4.239068740223122
Epoch 190
Epoch 190 :: Batch 0/1
Batch Loss = 4.238270125135236
190, epoch_train_loss=4.238270125135236
Epoch 191
Epoch 191 :: Batch 0/1
Batch Loss = 4.237481830696233
191, epoch_train_loss=4.237481830696233
Epoch 192
Epoch 192 :: Batch 0/1
Batch Loss = 4.236703771338028
192, epoch_train_loss=4.236703771338028
Epoch 193
Epoch 193 :: Batch 0/1
Batch Loss = 4.235935861260652
193, epoch_train_loss=4.235935861260652
Epoch 194
Epoch 194 :: Batch 0/1
Batch Loss = 4.235178014446049
194, epoch_train_loss=4.235178014446049
Epoch 195
Epoch 195 :: Batch 0/1
Batch Loss = 4.234430144616788
195, epoch_train_loss=4.234430144616788
Epoch 196
Epoch 196 :: Batch 0/1
Batch Loss = 4.233692165420017
196, epoch_train_loss=4.233692165420017
Epoch 197
Epoch 197 :: Batch 0/1
Batch Loss = 4.232963990218901
197, epoch_train_loss=4.232963990218901
Epoch 198
Epoch 198 :: Batch 0/1
Batch Loss = 4.232245532330199
198, epoch_train_loss=4.232245532330199
Epoch 199
Epoch 199 :: Batch 0/1
Batch Loss = 4.231536704875337
199, epoch_train_loss=4.231536704875337
Epoch 200
Epoch 200 :: Batch 0/1
Batch Loss = 4.2308374208006105
200, epoch_train_loss=4.2308374208006105
Epoch 201
Epoch 201 :: Batch 0/1
Batch Loss = 4.230147593052088
201, epoch_train_loss=4.230147593052088
Epoch 202
Epoch 202 :: Batch 0/1
Batch Loss = 4.229467134383019
202, epoch_train_loss=4.229467134383019
Epoch 203
Epoch 203 :: Batch 0/1
Batch Loss = 4.228795957527953
203, epoch_train_loss=4.228795957527953
Epoch 204
Epoch 204 :: Batch 0/1
Batch Loss = 4.2281339752167915
204, epoch_train_loss=4.2281339752167915
Epoch 205
Epoch 205 :: Batch 0/1
Batch Loss = 4.227481100042572
205, epoch_train_loss=4.227481100042572
Epoch 206
Epoch 206 :: Batch 0/1
Batch Loss = 4.226837244629133
206, epoch_train_loss=4.226837244629133
Epoch 207
Epoch 207 :: Batch 0/1
Batch Loss = 4.22620232164495
207, epoch_train_loss=4.22620232164495
Epoch 208
Epoch 208 :: Batch 0/1
Batch Loss = 4.22557624367652
208, epoch_train_loss=4.22557624367652
Epoch 209
Epoch 209 :: Batch 0/1
Batch Loss = 4.224958923389299
209, epoch_train_loss=4.224958923389299
Epoch 210
Epoch 210 :: Batch 0/1
Batch Loss = 4.224350273540874
210, epoch_train_loss=4.224350273540874
Epoch 211
Epoch 211 :: Batch 0/1
Batch Loss = 4.223750206859371
211, epoch_train_loss=4.223750206859371
Epoch 212
Epoch 212 :: Batch 0/1
Batch Loss = 4.223158636197445
212, epoch_train_loss=4.223158636197445
Epoch 213
Epoch 213 :: Batch 0/1
Batch Loss = 4.222575474544411
213, epoch_train_loss=4.222575474544411
Epoch 214
Epoch 214 :: Batch 0/1
Batch Loss = 4.222000634909131
214, epoch_train_loss=4.222000634909131
Epoch 215
Epoch 215 :: Batch 0/1
Batch Loss = 4.2214340304245415
215, epoch_train_loss=4.2214340304245415
Epoch 216
Epoch 216 :: Batch 0/1
Batch Loss = 4.220875574486379
216, epoch_train_loss=4.220875574486379
Epoch 217
Epoch 217 :: Batch 0/1
Batch Loss = 4.220325180511033
217, epoch_train_loss=4.220325180511033
Epoch 218
Epoch 218 :: Batch 0/1
Batch Loss = 4.219782762118381
218, epoch_train_loss=4.219782762118381
Epoch 219
Epoch 219 :: Batch 0/1
Batch Loss = 4.2192482331008145
219, epoch_train_loss=4.2192482331008145
Epoch 220
Epoch 220 :: Batch 0/1
Batch Loss = 4.2187215074342275
220, epoch_train_loss=4.2187215074342275
Epoch 221
Epoch 221 :: Batch 0/1
Batch Loss = 4.218202499288241
221, epoch_train_loss=4.218202499288241
Epoch 222
Epoch 222 :: Batch 0/1
Batch Loss = 4.21769112303561
222, epoch_train_loss=4.21769112303561
Epoch 223
Epoch 223 :: Batch 0/1
Batch Loss = 4.217187293260715
223, epoch_train_loss=4.217187293260715
Epoch 224
Epoch 224 :: Batch 0/1
Batch Loss = 4.21669092476712
224, epoch_train_loss=4.21669092476712
Epoch 225
Epoch 225 :: Batch 0/1
Batch Loss = 4.2162019325840445
225, epoch_train_loss=4.2162019325840445
Epoch 226
Epoch 226 :: Batch 0/1
Batch Loss = 4.215720231971755
226, epoch_train_loss=4.215720231971755
Epoch 227
Epoch 227 :: Batch 0/1
Batch Loss = 4.2152457383902
227, epoch_train_loss=4.2152457383902
Epoch 228
Epoch 228 :: Batch 0/1
Batch Loss = 4.214778367609064
228, epoch_train_loss=4.214778367609064
Epoch 229
Epoch 229 :: Batch 0/1
Batch Loss = 4.2143180356009236
229, epoch_train_loss=4.2143180356009236
Epoch 230
Epoch 230 :: Batch 0/1
Batch Loss = 4.213864658577213
230, epoch_train_loss=4.213864658577213
Epoch 231
Epoch 231 :: Batch 0/1
Batch Loss = 4.213418152986065
231, epoch_train_loss=4.213418152986065
Epoch 232
Epoch 232 :: Batch 0/1
Batch Loss = 4.212978435475255
232, epoch_train_loss=4.212978435475255
Epoch 233
Epoch 233 :: Batch 0/1
Batch Loss = 4.212545422985192
233, epoch_train_loss=4.212545422985192
Epoch 234
Epoch 234 :: Batch 0/1
Batch Loss = 4.212119032639716
234, epoch_train_loss=4.212119032639716
Epoch 235
Epoch 235 :: Batch 0/1
Batch Loss = 4.211699181767953
235, epoch_train_loss=4.211699181767953
Epoch 236
Epoch 236 :: Batch 0/1
Batch Loss = 4.211285787889278
236, epoch_train_loss=4.211285787889278
Epoch 237
Epoch 237 :: Batch 0/1
Batch Loss = 4.210878768664165
237, epoch_train_loss=4.210878768664165
Epoch 238
Epoch 238 :: Batch 0/1
Batch Loss = 4.210478041933386
238, epoch_train_loss=4.210478041933386
Epoch 239
Epoch 239 :: Batch 0/1
Batch Loss = 4.2100835256881215
239, epoch_train_loss=4.2100835256881215
Epoch 240
Epoch 240 :: Batch 0/1
Batch Loss = 4.20969513794745
240, epoch_train_loss=4.20969513794745
Epoch 241
Epoch 241 :: Batch 0/1
Batch Loss = 4.209312796837237
241, epoch_train_loss=4.209312796837237
Epoch 242
Epoch 242 :: Batch 0/1
Batch Loss = 4.208936420454388
242, epoch_train_loss=4.208936420454388
Epoch 243
Epoch 243 :: Batch 0/1
Batch Loss = 4.208565926813019
243, epoch_train_loss=4.208565926813019
Epoch 244
Epoch 244 :: Batch 0/1
Batch Loss = 4.208201233862122
244, epoch_train_loss=4.208201233862122
Epoch 245
Epoch 245 :: Batch 0/1
Batch Loss = 4.207842259295915
245, epoch_train_loss=4.207842259295915
Epoch 246
Epoch 246 :: Batch 0/1
Batch Loss = 4.207488920544379
246, epoch_train_loss=4.207488920544379
Epoch 247
Epoch 247 :: Batch 0/1
Batch Loss = 4.20714113463549
247, epoch_train_loss=4.20714113463549
Epoch 248
Epoch 248 :: Batch 0/1
Batch Loss = 4.206798818090408
248, epoch_train_loss=4.206798818090408
Epoch 249
Epoch 249 :: Batch 0/1
Batch Loss = 4.206461886690807
249, epoch_train_loss=4.206461886690807
Epoch 250
Epoch 250 :: Batch 0/1
Batch Loss = 4.206130255371667
250, epoch_train_loss=4.206130255371667
Epoch 251
Epoch 251 :: Batch 0/1
Batch Loss = 4.205803837997395
251, epoch_train_loss=4.205803837997395
Epoch 252
Epoch 252 :: Batch 0/1
Batch Loss = 4.205482547020789
252, epoch_train_loss=4.205482547020789
Epoch 253
Epoch 253 :: Batch 0/1
Batch Loss = 4.205166293234632
253, epoch_train_loss=4.205166293234632
Epoch 254
Epoch 254 :: Batch 0/1
Batch Loss = 4.204854985377714
254, epoch_train_loss=4.204854985377714
Epoch 255
Epoch 255 :: Batch 0/1
Batch Loss = 4.20454852958524
255, epoch_train_loss=4.20454852958524
Epoch 256
Epoch 256 :: Batch 0/1
Batch Loss = 4.204246828871154
256, epoch_train_loss=4.204246828871154
Epoch 257
Epoch 257 :: Batch 0/1
Batch Loss = 4.203949782376183
257, epoch_train_loss=4.203949782376183
Epoch 258
Epoch 258 :: Batch 0/1
Batch Loss = 4.203657284461235
258, epoch_train_loss=4.203657284461235
Epoch 259
Epoch 259 :: Batch 0/1
Batch Loss = 4.203369223570249
259, epoch_train_loss=4.203369223570249
Epoch 260
Epoch 260 :: Batch 0/1
Batch Loss = 4.203085480792767
260, epoch_train_loss=4.203085480792767
Epoch 261
Epoch 261 :: Batch 0/1
Batch Loss = 4.202805928032003
261, epoch_train_loss=4.202805928032003
Epoch 262
Epoch 262 :: Batch 0/1
Batch Loss = 4.202530425649743
262, epoch_train_loss=4.202530425649743
Epoch 263
Epoch 263 :: Batch 0/1
Batch Loss = 4.202258819410694
263, epoch_train_loss=4.202258819410694
Epoch 264
Epoch 264 :: Batch 0/1
Batch Loss = 4.201990936478886
264, epoch_train_loss=4.201990936478886
Epoch 265
Epoch 265 :: Batch 0/1
Batch Loss = 4.20172658011688
265, epoch_train_loss=4.20172658011688
Epoch 266
Epoch 266 :: Batch 0/1
Batch Loss = 4.201465522588075
266, epoch_train_loss=4.201465522588075
Epoch 267
Epoch 267 :: Batch 0/1
Batch Loss = 4.201207495536258
267, epoch_train_loss=4.201207495536258
Epoch 268
Epoch 268 :: Batch 0/1
Batch Loss = 4.200952176770014
268, epoch_train_loss=4.200952176770014
Epoch 269
Epoch 269 :: Batch 0/1
Batch Loss = 4.200699171817977
269, epoch_train_loss=4.200699171817977
Epoch 270
Epoch 270 :: Batch 0/1
Batch Loss = 4.200447987863512
270, epoch_train_loss=4.200447987863512
Epoch 271
Epoch 271 :: Batch 0/1
Batch Loss = 4.200197995941169
271, epoch_train_loss=4.200197995941169
Epoch 272
Epoch 272 :: Batch 0/1
Batch Loss = 4.199948375197025
272, epoch_train_loss=4.199948375197025
Epoch 273
Epoch 273 :: Batch 0/1
Batch Loss = 4.199698028329682
273, epoch_train_loss=4.199698028329682
Epoch 274
Epoch 274 :: Batch 0/1
Batch Loss = 4.199445449389112
274, epoch_train_loss=4.199445449389112
Epoch 275
Epoch 275 :: Batch 0/1
Batch Loss = 4.1991885086497485
275, epoch_train_loss=4.1991885086497485
Epoch 276
Epoch 276 :: Batch 0/1
Batch Loss = 4.1989240855028775
276, epoch_train_loss=4.1989240855028775
Epoch 277
Epoch 277 :: Batch 0/1
Batch Loss = 4.198647400198089
277, epoch_train_loss=4.198647400198089
Epoch 278
Epoch 278 :: Batch 0/1
Batch Loss = 4.1983506890974684
278, epoch_train_loss=4.1983506890974684
Epoch 279
Epoch 279 :: Batch 0/1
Batch Loss = 4.198020249365224
279, epoch_train_loss=4.198020249365224
Epoch 280
Epoch 280 :: Batch 0/1
Batch Loss = 4.197628639046248
280, epoch_train_loss=4.197628639046248
Epoch 281
Epoch 281 :: Batch 0/1
Batch Loss = 4.197107135241952
281, epoch_train_loss=4.197107135241952
Epoch 282
Epoch 282 :: Batch 0/1
Batch Loss = 4.197033448661041
282, epoch_train_loss=4.197033448661041
Epoch 283
Epoch 283 :: Batch 0/1
Batch Loss = 4.196436309228829
283, epoch_train_loss=4.196436309228829
Epoch 284
Epoch 284 :: Batch 0/1
Batch Loss = 4.196230479617025
284, epoch_train_loss=4.196230479617025
Epoch 285
Epoch 285 :: Batch 0/1
Batch Loss = 4.331940655315312
285, epoch_train_loss=4.331940655315312
Epoch 286
Epoch 286 :: Batch 0/1
Batch Loss = 4.19614030005377
286, epoch_train_loss=4.19614030005377
Epoch 287
Epoch 287 :: Batch 0/1
Batch Loss = 4.196273125119545
287, epoch_train_loss=4.196273125119545
Epoch 288
Epoch 288 :: Batch 0/1
Batch Loss = 4.196276351591138
288, epoch_train_loss=4.196276351591138
Epoch 289
Epoch 289 :: Batch 0/1
Batch Loss = 4.19620872935805
289, epoch_train_loss=4.19620872935805
Epoch 290
Epoch 290 :: Batch 0/1
Batch Loss = 4.196099609898375
290, epoch_train_loss=4.196099609898375
Epoch 291
Epoch 291 :: Batch 0/1
Batch Loss = 4.1959657709931495
291, epoch_train_loss=4.1959657709931495
Epoch 292
Epoch 292 :: Batch 0/1
Batch Loss = 4.195817253477464
292, epoch_train_loss=4.195817253477464
Epoch 293
Epoch 293 :: Batch 0/1
Batch Loss = 4.195660256153713
293, epoch_train_loss=4.195660256153713
Epoch 294
Epoch 294 :: Batch 0/1
Batch Loss = 4.195498700530657
294, epoch_train_loss=4.195498700530657
Epoch 295
Epoch 295 :: Batch 0/1
Batch Loss = 4.19533511682343
295, epoch_train_loss=4.19533511682343
Epoch 296
Epoch 296 :: Batch 0/1
Batch Loss = 4.195171162483347
296, epoch_train_loss=4.195171162483347
Epoch 297
Epoch 297 :: Batch 0/1
Batch Loss = 4.195007933346603
297, epoch_train_loss=4.195007933346603
Epoch 298
Epoch 298 :: Batch 0/1
Batch Loss = 4.194846154391515
298, epoch_train_loss=4.194846154391515
Epoch 299
Epoch 299 :: Batch 0/1
Batch Loss = 4.194686299486442
299, epoch_train_loss=4.194686299486442
Epoch 300
Epoch 300 :: Batch 0/1
Batch Loss = 4.194528668627193
300, epoch_train_loss=4.194528668627193
Epoch 301
Epoch 301 :: Batch 0/1
Batch Loss = 4.1943734393386425
301, epoch_train_loss=4.1943734393386425
Epoch 302
Epoch 302 :: Batch 0/1
Batch Loss = 4.194220702089874
302, epoch_train_loss=4.194220702089874
Epoch 303
Epoch 303 :: Batch 0/1
Batch Loss = 4.194070485508281
303, epoch_train_loss=4.194070485508281
Epoch 304
Epoch 304 :: Batch 0/1
Batch Loss = 4.193922774737891
304, epoch_train_loss=4.193922774737891
Epoch 305
Epoch 305 :: Batch 0/1
Batch Loss = 4.1937775249659675
305, epoch_train_loss=4.1937775249659675
Epoch 306
Epoch 306 :: Batch 0/1
Batch Loss = 4.19363467147944
306, epoch_train_loss=4.19363467147944
Epoch 307
Epoch 307 :: Batch 0/1
Batch Loss = 4.1934941372525065
307, epoch_train_loss=4.1934941372525065
Epoch 308
Epoch 308 :: Batch 0/1
Batch Loss = 4.193355838478348
308, epoch_train_loss=4.193355838478348
Epoch 309
Epoch 309 :: Batch 0/1
Batch Loss = 4.19321968862721
309, epoch_train_loss=4.19321968862721
Epoch 310
Epoch 310 :: Batch 0/1
Batch Loss = 4.193085601216037
310, epoch_train_loss=4.193085601216037
Epoch 311
Epoch 311 :: Batch 0/1
Batch Loss = 4.192953491402782
311, epoch_train_loss=4.192953491402782
Epoch 312
Epoch 312 :: Batch 0/1
Batch Loss = 4.192823276474736
312, epoch_train_loss=4.192823276474736
Epoch 313
Epoch 313 :: Batch 0/1
Batch Loss = 4.192694875035023
313, epoch_train_loss=4.192694875035023
Epoch 314
Epoch 314 :: Batch 0/1
Batch Loss = 4.192568205117728
314, epoch_train_loss=4.192568205117728
Epoch 315
Epoch 315 :: Batch 0/1
Batch Loss = 4.1924431811545455
315, epoch_train_loss=4.1924431811545455
Epoch 316
Epoch 316 :: Batch 0/1
Batch Loss = 4.192319710115384
316, epoch_train_loss=4.192319710115384
Epoch 317
Epoch 317 :: Batch 0/1
Batch Loss = 4.192197687562455
317, epoch_train_loss=4.192197687562455
Epoch 318
Epoch 318 :: Batch 0/1
Batch Loss = 4.192076994241169
318, epoch_train_loss=4.192076994241169
Epoch 319
Epoch 319 :: Batch 0/1
Batch Loss = 4.191957494094258
319, epoch_train_loss=4.191957494094258
Epoch 320
Epoch 320 :: Batch 0/1
Batch Loss = 4.191839034574803
320, epoch_train_loss=4.191839034574803
Epoch 321
Epoch 321 :: Batch 0/1
Batch Loss = 4.191721449296248
321, epoch_train_loss=4.191721449296248
Epoch 322
Epoch 322 :: Batch 0/1
Batch Loss = 4.191604562504425
322, epoch_train_loss=4.191604562504425
Epoch 323
Epoch 323 :: Batch 0/1
Batch Loss = 4.191488193980976
323, epoch_train_loss=4.191488193980976
Epoch 324
Epoch 324 :: Batch 0/1
Batch Loss = 4.191372162429909
324, epoch_train_loss=4.191372162429909
Epoch 325
Epoch 325 :: Batch 0/1
Batch Loss = 4.191256285425129
325, epoch_train_loss=4.191256285425129
Epoch 326
Epoch 326 :: Batch 0/1
Batch Loss = 4.191140375104236
326, epoch_train_loss=4.191140375104236
Epoch 327
Epoch 327 :: Batch 0/1
Batch Loss = 4.191024229890621
327, epoch_train_loss=4.191024229890621
Epoch 328
Epoch 328 :: Batch 0/1
Batch Loss = 4.190907624058151
328, epoch_train_loss=4.190907624058151
Epoch 329
Epoch 329 :: Batch 0/1
Batch Loss = 4.190790297310933
329, epoch_train_loss=4.190790297310933
Epoch 330
Epoch 330 :: Batch 0/1
Batch Loss = 4.190671946284312
330, epoch_train_loss=4.190671946284312
Epoch 331
Epoch 331 :: Batch 0/1
Batch Loss = 4.190552218869091
331, epoch_train_loss=4.190552218869091
Epoch 332
Epoch 332 :: Batch 0/1
Batch Loss = 4.190430710764398
332, epoch_train_loss=4.190430710764398
Epoch 333
Epoch 333 :: Batch 0/1
Batch Loss = 4.190306962721403
333, epoch_train_loss=4.190306962721403
Epoch 334
Epoch 334 :: Batch 0/1
Batch Loss = 4.190180456270763
334, epoch_train_loss=4.190180456270763
Epoch 335
Epoch 335 :: Batch 0/1
Batch Loss = 4.190050605871266
335, epoch_train_loss=4.190050605871266
Epoch 336
Epoch 336 :: Batch 0/1
Batch Loss = 4.189916745678927
336, epoch_train_loss=4.189916745678927
Epoch 337
Epoch 337 :: Batch 0/1
Batch Loss = 4.189778110043375
337, epoch_train_loss=4.189778110043375
Epoch 338
Epoch 338 :: Batch 0/1
Batch Loss = 4.189633807792907
338, epoch_train_loss=4.189633807792907
Epoch 339
Epoch 339 :: Batch 0/1
Batch Loss = 4.189482791446623
339, epoch_train_loss=4.189482791446623
Epoch 340
Epoch 340 :: Batch 0/1
Batch Loss = 4.1893238239801445
340, epoch_train_loss=4.1893238239801445
Epoch 341
Epoch 341 :: Batch 0/1
Batch Loss = 4.1891554469207595
341, epoch_train_loss=4.1891554469207595
Epoch 342
Epoch 342 :: Batch 0/1
Batch Loss = 4.188975957311274
342, epoch_train_loss=4.188975957311274
Epoch 343
Epoch 343 :: Batch 0/1
Batch Loss = 4.188783409367942
343, epoch_train_loss=4.188783409367942
Epoch 344
Epoch 344 :: Batch 0/1
Batch Loss = 4.1885756824619005
344, epoch_train_loss=4.1885756824619005
Epoch 345
Epoch 345 :: Batch 0/1
Batch Loss = 4.188350709924277
345, epoch_train_loss=4.188350709924277
Epoch 346
Epoch 346 :: Batch 0/1
Batch Loss = 4.188107029543422
346, epoch_train_loss=4.188107029543422
Epoch 347
Epoch 347 :: Batch 0/1
Batch Loss = 4.187843824592427
347, epoch_train_loss=4.187843824592427
Epoch 348
Epoch 348 :: Batch 0/1
Batch Loss = 2351.734836810379
348, epoch_train_loss=2351.734836810379
Epoch 349
Epoch 349 :: Batch 0/1
Batch Loss = 4.187838491998322
349, epoch_train_loss=4.187838491998322
Epoch 350
Epoch 350 :: Batch 0/1
Batch Loss = 4.188293971445205
350, epoch_train_loss=4.188293971445205
Epoch 351
Epoch 351 :: Batch 0/1
Batch Loss = 4.188752862870654
351, epoch_train_loss=4.188752862870654
Epoch 352
Epoch 352 :: Batch 0/1
Batch Loss = 4.189126666711651
352, epoch_train_loss=4.189126666711651
Epoch 353
Epoch 353 :: Batch 0/1
Batch Loss = 4.18939727303927
353, epoch_train_loss=4.18939727303927
Epoch 354
Epoch 354 :: Batch 0/1
Batch Loss = 4.189576514222547
354, epoch_train_loss=4.189576514222547
Epoch 355
Epoch 355 :: Batch 0/1
Batch Loss = 4.189687176192858
355, epoch_train_loss=4.189687176192858
Epoch 356
Epoch 356 :: Batch 0/1
Batch Loss = 4.189751049826378
356, epoch_train_loss=4.189751049826378
Epoch 357
Epoch 357 :: Batch 0/1
Batch Loss = 4.189784035626077
357, epoch_train_loss=4.189784035626077
Epoch 358
Epoch 358 :: Batch 0/1
Batch Loss = 4.189796093520116
358, epoch_train_loss=4.189796093520116
Epoch 359
Epoch 359 :: Batch 0/1
Batch Loss = 4.1897928751307045
359, epoch_train_loss=4.1897928751307045
Epoch 360
Epoch 360 :: Batch 0/1
Batch Loss = 4.189777300626521
360, epoch_train_loss=4.189777300626521
Epoch 361
Epoch 361 :: Batch 0/1
Batch Loss = 4.189750599825971
361, epoch_train_loss=4.189750599825971
Epoch 362
Epoch 362 :: Batch 0/1
Batch Loss = 4.189712867617233
362, epoch_train_loss=4.189712867617233
Epoch 363
Epoch 363 :: Batch 0/1
Batch Loss = 4.189663290264026
363, epoch_train_loss=4.189663290264026
Epoch 364
Epoch 364 :: Batch 0/1
Batch Loss = 4.189600165973996
364, epoch_train_loss=4.189600165973996
Epoch 365
Epoch 365 :: Batch 0/1
Batch Loss = 4.189520798286495
365, epoch_train_loss=4.189520798286495
Epoch 366
Epoch 366 :: Batch 0/1
Batch Loss = 4.1894213275190335
366, epoch_train_loss=4.1894213275190335
Epoch 367
Epoch 367 :: Batch 0/1
Batch Loss = 4.189296608783538
367, epoch_train_loss=4.189296608783538
Epoch 368
Epoch 368 :: Batch 0/1
Batch Loss = 4.18914039182122
368, epoch_train_loss=4.18914039182122
Epoch 369
Epoch 369 :: Batch 0/1
Batch Loss = 4.188946410448122
369, epoch_train_loss=4.188946410448122
Epoch 370
Epoch 370 :: Batch 0/1
Batch Loss = 4.188711718630422
370, epoch_train_loss=4.188711718630422
Epoch 371
Epoch 371 :: Batch 0/1
Batch Loss = 4.1884448112672334
371, epoch_train_loss=4.1884448112672334
Epoch 372
Epoch 372 :: Batch 0/1
Batch Loss = 4.188181867591066
372, epoch_train_loss=4.188181867591066
Epoch 373
Epoch 373 :: Batch 0/1
Batch Loss = 4.188009031505865
373, epoch_train_loss=4.188009031505865
Epoch 374
Epoch 374 :: Batch 0/1
Batch Loss = 4.188048452983214
374, epoch_train_loss=4.188048452983214
Epoch 375
Epoch 375 :: Batch 0/1
Batch Loss = 4.1882487228677086
375, epoch_train_loss=4.1882487228677086
Epoch 376
Epoch 376 :: Batch 0/1
Batch Loss = 4.188246603294709
376, epoch_train_loss=4.188246603294709
Epoch 377
Epoch 377 :: Batch 0/1
Batch Loss = 4.18798324224391
377, epoch_train_loss=4.18798324224391
Epoch 378
Epoch 378 :: Batch 0/1
Batch Loss = 4.187689954391522
378, epoch_train_loss=4.187689954391522
Epoch 379
Epoch 379 :: Batch 0/1
Batch Loss = 4.187524485353387
379, epoch_train_loss=4.187524485353387
Epoch 380
Epoch 380 :: Batch 0/1
Batch Loss = 4.187481179208751
380, epoch_train_loss=4.187481179208751
Epoch 381
Epoch 381 :: Batch 0/1
Batch Loss = 4.187481110617023
381, epoch_train_loss=4.187481110617023
Epoch 382
Epoch 382 :: Batch 0/1
Batch Loss = 4.187461586570247
382, epoch_train_loss=4.187461586570247
Epoch 383
Epoch 383 :: Batch 0/1
Batch Loss = 4.187393886593443
383, epoch_train_loss=4.187393886593443
Epoch 384
Epoch 384 :: Batch 0/1
Batch Loss = 4.187270861614773
384, epoch_train_loss=4.187270861614773
Epoch 385
Epoch 385 :: Batch 0/1
Batch Loss = 4.187098442770268
385, epoch_train_loss=4.187098442770268
Epoch 386
Epoch 386 :: Batch 0/1
Batch Loss = 4.186896515617861
386, epoch_train_loss=4.186896515617861
Epoch 387
Epoch 387 :: Batch 0/1
Batch Loss = 4.1867026930836655
387, epoch_train_loss=4.1867026930836655
Epoch 388
Epoch 388 :: Batch 0/1
Batch Loss = 4.186560961104865
388, epoch_train_loss=4.186560961104865
Epoch 389
Epoch 389 :: Batch 0/1
Batch Loss = 4.1864720786319
389, epoch_train_loss=4.1864720786319
Epoch 390
Epoch 390 :: Batch 0/1
Batch Loss = 4.18634949492356
390, epoch_train_loss=4.18634949492356
Epoch 391
Epoch 391 :: Batch 0/1
Batch Loss = 4.186107493246167
391, epoch_train_loss=4.186107493246167
Epoch 392
Epoch 392 :: Batch 0/1
Batch Loss = 4.185768565493076
392, epoch_train_loss=4.185768565493076
Epoch 393
Epoch 393 :: Batch 0/1
Batch Loss = 4.185393076333671
393, epoch_train_loss=4.185393076333671
Epoch 394
Epoch 394 :: Batch 0/1
Batch Loss = 4.184980606601499
394, epoch_train_loss=4.184980606601499
Epoch 395
Epoch 395 :: Batch 0/1
Batch Loss = 4.184477327431636
395, epoch_train_loss=4.184477327431636
Epoch 396
Epoch 396 :: Batch 0/1
Batch Loss = 4.183883513112376
396, epoch_train_loss=4.183883513112376
Epoch 397
Epoch 397 :: Batch 0/1
Batch Loss = 4.1835165408311035
397, epoch_train_loss=4.1835165408311035
Epoch 398
Epoch 398 :: Batch 0/1
Batch Loss = 4.184069833434669
398, epoch_train_loss=4.184069833434669
Epoch 399
Epoch 399 :: Batch 0/1
Batch Loss = 4.184026994678068
399, epoch_train_loss=4.184026994678068
Epoch 400
Epoch 400 :: Batch 0/1
Batch Loss = 4.183379319597971
400, epoch_train_loss=4.183379319597971
Epoch 401
Epoch 401 :: Batch 0/1
Batch Loss = 4.182960346738614
401, epoch_train_loss=4.182960346738614
Epoch 402
Epoch 402 :: Batch 0/1
Batch Loss = 4.182881111052564
402, epoch_train_loss=4.182881111052564
Epoch 403
Epoch 403 :: Batch 0/1
Batch Loss = 4.1829042287630545
403, epoch_train_loss=4.1829042287630545
Epoch 404
Epoch 404 :: Batch 0/1
Batch Loss = 4.182888825665948
404, epoch_train_loss=4.182888825665948
Epoch 405
Epoch 405 :: Batch 0/1
Batch Loss = 4.182791373200891
405, epoch_train_loss=4.182791373200891
Epoch 406
Epoch 406 :: Batch 0/1
Batch Loss = 4.182594947672332
406, epoch_train_loss=4.182594947672332
Epoch 407
Epoch 407 :: Batch 0/1
Batch Loss = 4.182306337587413
407, epoch_train_loss=4.182306337587413
Epoch 408
Epoch 408 :: Batch 0/1
Batch Loss = 4.181971952178624
408, epoch_train_loss=4.181971952178624
Epoch 409
Epoch 409 :: Batch 0/1
Batch Loss = 4.181681413736792
409, epoch_train_loss=4.181681413736792
Epoch 410
Epoch 410 :: Batch 0/1
Batch Loss = 4.181521636373855
410, epoch_train_loss=4.181521636373855
Epoch 411
Epoch 411 :: Batch 0/1
Batch Loss = 4.181434724768755
411, epoch_train_loss=4.181434724768755
Epoch 412
Epoch 412 :: Batch 0/1
Batch Loss = 4.181188158056961
412, epoch_train_loss=4.181188158056961
Epoch 413
Epoch 413 :: Batch 0/1
Batch Loss = 4.180680454654307
413, epoch_train_loss=4.180680454654307
Epoch 414
Epoch 414 :: Batch 0/1
Batch Loss = 4.180002266740203
414, epoch_train_loss=4.180002266740203
Epoch 415
Epoch 415 :: Batch 0/1
Batch Loss = 4.179226763174594
415, epoch_train_loss=4.179226763174594
Epoch 416
Epoch 416 :: Batch 0/1
Batch Loss = 4.178562932240649
416, epoch_train_loss=4.178562932240649
Epoch 417
Epoch 417 :: Batch 0/1
Batch Loss = 4.1787725483657105
417, epoch_train_loss=4.1787725483657105
Epoch 418
Epoch 418 :: Batch 0/1
Batch Loss = 4.179464056464287
418, epoch_train_loss=4.179464056464287
Epoch 419
Epoch 419 :: Batch 0/1
Batch Loss = 4.178976912326755
419, epoch_train_loss=4.178976912326755
Epoch 420
Epoch 420 :: Batch 0/1
Batch Loss = 4.178244094759942
420, epoch_train_loss=4.178244094759942
Epoch 421
Epoch 421 :: Batch 0/1
Batch Loss = 4.177951139610786
421, epoch_train_loss=4.177951139610786
Epoch 422
Epoch 422 :: Batch 0/1
Batch Loss = 4.177958929679214
422, epoch_train_loss=4.177958929679214
Epoch 423
Epoch 423 :: Batch 0/1
Batch Loss = 4.177974052446838
423, epoch_train_loss=4.177974052446838
Epoch 424
Epoch 424 :: Batch 0/1
Batch Loss = 4.177815772198148
424, epoch_train_loss=4.177815772198148
Epoch 425
Epoch 425 :: Batch 0/1
Batch Loss = 4.177586639226732
425, epoch_train_loss=4.177586639226732
Epoch 426
Epoch 426 :: Batch 0/1
Batch Loss = 4.177514868091972
426, epoch_train_loss=4.177514868091972
Epoch 427
Epoch 427 :: Batch 0/1
Batch Loss = 4.177007638823784
427, epoch_train_loss=4.177007638823784
Epoch 428
Epoch 428 :: Batch 0/1
Batch Loss = 4.176868228955302
428, epoch_train_loss=4.176868228955302
Epoch 429
Epoch 429 :: Batch 0/1
Batch Loss = 4.176810543122819
429, epoch_train_loss=4.176810543122819
Epoch 430
Epoch 430 :: Batch 0/1
Batch Loss = 4.176835359297357
430, epoch_train_loss=4.176835359297357
Epoch 431
Epoch 431 :: Batch 0/1
Batch Loss = 4.176785215459272
431, epoch_train_loss=4.176785215459272
Epoch 432
Epoch 432 :: Batch 0/1
Batch Loss = 4.176530639013104
432, epoch_train_loss=4.176530639013104
Epoch 433
Epoch 433 :: Batch 0/1
Batch Loss = 4.176230149501798
433, epoch_train_loss=4.176230149501798
Epoch 434
Epoch 434 :: Batch 0/1
Batch Loss = 4.176182824624458
434, epoch_train_loss=4.176182824624458
Epoch 435
Epoch 435 :: Batch 0/1
Batch Loss = 4.176158009958285
435, epoch_train_loss=4.176158009958285
Epoch 436
Epoch 436 :: Batch 0/1
Batch Loss = 4.175950384291662
436, epoch_train_loss=4.175950384291662
Epoch 437
Epoch 437 :: Batch 0/1
Batch Loss = 4.175864157238011
437, epoch_train_loss=4.175864157238011
Epoch 438
Epoch 438 :: Batch 0/1
Batch Loss = 4.175824041011892
438, epoch_train_loss=4.175824041011892
Epoch 439
Epoch 439 :: Batch 0/1
Batch Loss = 4.175702391544947
439, epoch_train_loss=4.175702391544947
Epoch 440
Epoch 440 :: Batch 0/1
Batch Loss = 4.175473552294584
440, epoch_train_loss=4.175473552294584
Epoch 441
Epoch 441 :: Batch 0/1
Batch Loss = 4.175154452297608
441, epoch_train_loss=4.175154452297608
Epoch 442
Epoch 442 :: Batch 0/1
Batch Loss = 4.1748529570749175
442, epoch_train_loss=4.1748529570749175
Epoch 443
Epoch 443 :: Batch 0/1
Batch Loss = 4.174857860763198
443, epoch_train_loss=4.174857860763198
Epoch 444
Epoch 444 :: Batch 0/1
Batch Loss = 4.174226892705315
444, epoch_train_loss=4.174226892705315
Epoch 445
Epoch 445 :: Batch 0/1
Batch Loss = 4.174008350699424
445, epoch_train_loss=4.174008350699424
Epoch 446
Epoch 446 :: Batch 0/1
Batch Loss = 4.173340664935324
446, epoch_train_loss=4.173340664935324
Epoch 447
Epoch 447 :: Batch 0/1
Batch Loss = 4.173558252403504
447, epoch_train_loss=4.173558252403504
Epoch 448
Epoch 448 :: Batch 0/1
Batch Loss = 4.173337500414297
448, epoch_train_loss=4.173337500414297
Epoch 449
Epoch 449 :: Batch 0/1
Batch Loss = 4.173511298702781
449, epoch_train_loss=4.173511298702781
Epoch 450
Epoch 450 :: Batch 0/1
Batch Loss = 4.172342799390687
450, epoch_train_loss=4.172342799390687
Epoch 451
Epoch 451 :: Batch 0/1
Batch Loss = 4.174326502119151
451, epoch_train_loss=4.174326502119151
Epoch 452
Epoch 452 :: Batch 0/1
Batch Loss = 4.172392112093673
452, epoch_train_loss=4.172392112093673
Epoch 453
Epoch 453 :: Batch 0/1
Batch Loss = 4.173745566254925
453, epoch_train_loss=4.173745566254925
Epoch 454
Epoch 454 :: Batch 0/1
Batch Loss = 4.173995141404906
454, epoch_train_loss=4.173995141404906
Epoch 455
Epoch 455 :: Batch 0/1
Batch Loss = 4.17334915822438
455, epoch_train_loss=4.17334915822438
Epoch 456
Epoch 456 :: Batch 0/1
Batch Loss = 4.1717022058697895
456, epoch_train_loss=4.1717022058697895
Epoch 457
Epoch 457 :: Batch 0/1
Batch Loss = 4.1737094191702235
457, epoch_train_loss=4.1737094191702235
Epoch 458
Epoch 458 :: Batch 0/1
Batch Loss = 4.171219513937446
458, epoch_train_loss=4.171219513937446
Epoch 459
Epoch 459 :: Batch 0/1
Batch Loss = 4.171944166949004
459, epoch_train_loss=4.171944166949004
Epoch 460
Epoch 460 :: Batch 0/1
Batch Loss = 4.172277989479603
460, epoch_train_loss=4.172277989479603
Epoch 461
Epoch 461 :: Batch 0/1
Batch Loss = 4.1715611455295045
461, epoch_train_loss=4.1715611455295045
Epoch 462
Epoch 462 :: Batch 0/1
Batch Loss = 4.170703280977581
462, epoch_train_loss=4.170703280977581
Epoch 463
Epoch 463 :: Batch 0/1
Batch Loss = 4.171884422560694
463, epoch_train_loss=4.171884422560694
Epoch 464
Epoch 464 :: Batch 0/1
Batch Loss = 4.170357522729504
464, epoch_train_loss=4.170357522729504
Epoch 465
Epoch 465 :: Batch 0/1
Batch Loss = 4.170814642727249
465, epoch_train_loss=4.170814642727249
Epoch 466
Epoch 466 :: Batch 0/1
Batch Loss = 4.170834673833788
466, epoch_train_loss=4.170834673833788
Epoch 467
Epoch 467 :: Batch 0/1
Batch Loss = 4.170020464885307
467, epoch_train_loss=4.170020464885307
Epoch 468
Epoch 468 :: Batch 0/1
Batch Loss = 4.16988171486251
468, epoch_train_loss=4.16988171486251
Epoch 469
Epoch 469 :: Batch 0/1
Batch Loss = 4.1698303534011405
469, epoch_train_loss=4.1698303534011405
Epoch 470
Epoch 470 :: Batch 0/1
Batch Loss = 4.169131882803229
470, epoch_train_loss=4.169131882803229
Epoch 471
Epoch 471 :: Batch 0/1
Batch Loss = 4.169294518158311
471, epoch_train_loss=4.169294518158311
Epoch 472
Epoch 472 :: Batch 0/1
Batch Loss = 4.168575717307149
472, epoch_train_loss=4.168575717307149
Epoch 473
Epoch 473 :: Batch 0/1
Batch Loss = 4.1679425436759105
473, epoch_train_loss=4.1679425436759105
Epoch 474
Epoch 474 :: Batch 0/1
Batch Loss = 4.167221894960547
474, epoch_train_loss=4.167221894960547
Epoch 475
Epoch 475 :: Batch 0/1
Batch Loss = 4.166938202206899
475, epoch_train_loss=4.166938202206899
Epoch 476
Epoch 476 :: Batch 0/1
Batch Loss = 4.164801689077553
476, epoch_train_loss=4.164801689077553
Epoch 477
Epoch 477 :: Batch 0/1
Batch Loss = 4.193858588189648
477, epoch_train_loss=4.193858588189648
Epoch 478
Epoch 478 :: Batch 0/1
Batch Loss = 4.173123051210194
478, epoch_train_loss=4.173123051210194
Epoch 479
Epoch 479 :: Batch 0/1
Batch Loss = 4.174388285548832
479, epoch_train_loss=4.174388285548832
Epoch 480
Epoch 480 :: Batch 0/1
Batch Loss = 4.174544440294532
480, epoch_train_loss=4.174544440294532
Epoch 481
Epoch 481 :: Batch 0/1
Batch Loss = 4.174530257985727
481, epoch_train_loss=4.174530257985727
Epoch 482
Epoch 482 :: Batch 0/1
Batch Loss = 4.174516056646213
482, epoch_train_loss=4.174516056646213
Epoch 483
Epoch 483 :: Batch 0/1
Batch Loss = 4.174528120221756
483, epoch_train_loss=4.174528120221756
Epoch 484
Epoch 484 :: Batch 0/1
Batch Loss = 4.174516978597597
484, epoch_train_loss=4.174516978597597
Epoch 485
Epoch 485 :: Batch 0/1
Batch Loss = 4.174470778890935
485, epoch_train_loss=4.174470778890935
Epoch 486
Epoch 486 :: Batch 0/1
Batch Loss = 4.174429618072921
486, epoch_train_loss=4.174429618072921
Epoch 487
Epoch 487 :: Batch 0/1
Batch Loss = 4.174408948033107
487, epoch_train_loss=4.174408948033107
Epoch 488
Epoch 488 :: Batch 0/1
Batch Loss = 4.174385683580232
488, epoch_train_loss=4.174385683580232
Epoch 489
Epoch 489 :: Batch 0/1
Batch Loss = 4.1743432817576425
489, epoch_train_loss=4.1743432817576425
Epoch 490
Epoch 490 :: Batch 0/1
Batch Loss = 4.17429426266694
490, epoch_train_loss=4.17429426266694
Epoch 491
Epoch 491 :: Batch 0/1
Batch Loss = 4.174257756027135
491, epoch_train_loss=4.174257756027135
Epoch 492
Epoch 492 :: Batch 0/1
Batch Loss = 4.174228569715193
492, epoch_train_loss=4.174228569715193
Epoch 493
Epoch 493 :: Batch 0/1
Batch Loss = 4.174187261420206
493, epoch_train_loss=4.174187261420206
Epoch 494
Epoch 494 :: Batch 0/1
Batch Loss = 4.174134616058712
494, epoch_train_loss=4.174134616058712
Epoch 495
Epoch 495 :: Batch 0/1
Batch Loss = 4.174086514895347
495, epoch_train_loss=4.174086514895347
Epoch 496
Epoch 496 :: Batch 0/1
Batch Loss = 4.174044589263935
496, epoch_train_loss=4.174044589263935
Epoch 497
Epoch 497 :: Batch 0/1
Batch Loss = 4.1739969135716155
497, epoch_train_loss=4.1739969135716155
Epoch 498
Epoch 498 :: Batch 0/1
Batch Loss = 4.173939047775988
498, epoch_train_loss=4.173939047775988
Epoch 499
Epoch 499 :: Batch 0/1
Batch Loss = 4.173879396225091
499, epoch_train_loss=4.173879396225091
Epoch 500
Epoch 500 :: Batch 0/1
Batch Loss = 4.1738245746237475
500, epoch_train_loss=4.1738245746237475
Epoch 501
Epoch 501 :: Batch 0/1
Batch Loss = 4.173769187117117
501, epoch_train_loss=4.173769187117117
Epoch 502
Epoch 502 :: Batch 0/1
Batch Loss = 4.173706191415208
502, epoch_train_loss=4.173706191415208
Epoch 503
Epoch 503 :: Batch 0/1
Batch Loss = 4.17363851213153
503, epoch_train_loss=4.17363851213153
Epoch 504
Epoch 504 :: Batch 0/1
Batch Loss = 4.173571662417394
504, epoch_train_loss=4.173571662417394
Epoch 505
Epoch 505 :: Batch 0/1
Batch Loss = 4.17350379686118
505, epoch_train_loss=4.17350379686118
Epoch 506
Epoch 506 :: Batch 0/1
Batch Loss = 4.1734301888623015
506, epoch_train_loss=4.1734301888623015
Epoch 507
Epoch 507 :: Batch 0/1
Batch Loss = 4.17335104182997
507, epoch_train_loss=4.17335104182997
Epoch 508
Epoch 508 :: Batch 0/1
Batch Loss = 4.173269235714161
508, epoch_train_loss=4.173269235714161
Epoch 509
Epoch 509 :: Batch 0/1
Batch Loss = 4.173183585456694
509, epoch_train_loss=4.173183585456694
Epoch 510
Epoch 510 :: Batch 0/1
Batch Loss = 4.173089723872269
510, epoch_train_loss=4.173089723872269
Epoch 511
Epoch 511 :: Batch 0/1
Batch Loss = 4.172985487472796
511, epoch_train_loss=4.172985487472796
Epoch 512
Epoch 512 :: Batch 0/1
Batch Loss = 4.172869327848376
512, epoch_train_loss=4.172869327848376
Epoch 513
Epoch 513 :: Batch 0/1
Batch Loss = 4.172734109764102
513, epoch_train_loss=4.172734109764102
Epoch 514
Epoch 514 :: Batch 0/1
Batch Loss = 4.172563741369238
514, epoch_train_loss=4.172563741369238
Epoch 515
Epoch 515 :: Batch 0/1
Batch Loss = 4.172324726232088
515, epoch_train_loss=4.172324726232088
Epoch 516
Epoch 516 :: Batch 0/1
Batch Loss = 4.171932537503275
516, epoch_train_loss=4.171932537503275
Epoch 517
Epoch 517 :: Batch 0/1
Batch Loss = 4.171207203299258
517, epoch_train_loss=4.171207203299258
Epoch 518
Epoch 518 :: Batch 0/1
Batch Loss = 4.170095509232778
518, epoch_train_loss=4.170095509232778
Epoch 519
Epoch 519 :: Batch 0/1
Batch Loss = 4.169600898790523
519, epoch_train_loss=4.169600898790523
Epoch 520
Epoch 520 :: Batch 0/1
Batch Loss = 4.170795579281952
520, epoch_train_loss=4.170795579281952
Epoch 521
Epoch 521 :: Batch 0/1
Batch Loss = 4.17009225648247
521, epoch_train_loss=4.17009225648247
Epoch 522
Epoch 522 :: Batch 0/1
Batch Loss = 4.169405356590657
522, epoch_train_loss=4.169405356590657
Epoch 523
Epoch 523 :: Batch 0/1
Batch Loss = 4.1693908625689575
523, epoch_train_loss=4.1693908625689575
Epoch 524
Epoch 524 :: Batch 0/1
Batch Loss = 4.1695569654836255
524, epoch_train_loss=4.1695569654836255
Epoch 525
Epoch 525 :: Batch 0/1
Batch Loss = 4.169644731760155
525, epoch_train_loss=4.169644731760155
Epoch 526
Epoch 526 :: Batch 0/1
Batch Loss = 4.169619509322021
526, epoch_train_loss=4.169619509322021
Epoch 527
Epoch 527 :: Batch 0/1
Batch Loss = 4.16940295738053
527, epoch_train_loss=4.16940295738053
Epoch 528
Epoch 528 :: Batch 0/1
Batch Loss = 4.1690342432345195
528, epoch_train_loss=4.1690342432345195
Epoch 529
Epoch 529 :: Batch 0/1
Batch Loss = 4.1687366145918165
529, epoch_train_loss=4.1687366145918165
Epoch 530
Epoch 530 :: Batch 0/1
Batch Loss = 4.168704378190781
530, epoch_train_loss=4.168704378190781
Epoch 531
Epoch 531 :: Batch 0/1
Batch Loss = 4.168849482622813
531, epoch_train_loss=4.168849482622813
Epoch 532
Epoch 532 :: Batch 0/1
Batch Loss = 4.168802735382583
532, epoch_train_loss=4.168802735382583
Epoch 533
Epoch 533 :: Batch 0/1
Batch Loss = 4.168521245252641
533, epoch_train_loss=4.168521245252641
Epoch 534
Epoch 534 :: Batch 0/1
Batch Loss = 4.168287687822384
534, epoch_train_loss=4.168287687822384
Epoch 535
Epoch 535 :: Batch 0/1
Batch Loss = 4.168218874638565
535, epoch_train_loss=4.168218874638565
Epoch 536
Epoch 536 :: Batch 0/1
Batch Loss = 4.1682293451975445
536, epoch_train_loss=4.1682293451975445
Epoch 537
Epoch 537 :: Batch 0/1
Batch Loss = 4.168202787913247
537, epoch_train_loss=4.168202787913247
Epoch 538
Epoch 538 :: Batch 0/1
Batch Loss = 4.168084369201547
538, epoch_train_loss=4.168084369201547
Epoch 539
Epoch 539 :: Batch 0/1
Batch Loss = 4.167899473323863
539, epoch_train_loss=4.167899473323863
Epoch 540
Epoch 540 :: Batch 0/1
Batch Loss = 4.16772718346796
540, epoch_train_loss=4.16772718346796
Epoch 541
Epoch 541 :: Batch 0/1
Batch Loss = 4.167638511048384
541, epoch_train_loss=4.167638511048384
Epoch 542
Epoch 542 :: Batch 0/1
Batch Loss = 4.16761279423234
542, epoch_train_loss=4.16761279423234
Epoch 543
Epoch 543 :: Batch 0/1
Batch Loss = 4.167543273952995
543, epoch_train_loss=4.167543273952995
Epoch 544
Epoch 544 :: Batch 0/1
Batch Loss = 4.167394678270874
544, epoch_train_loss=4.167394678270874
Epoch 545
Epoch 545 :: Batch 0/1
Batch Loss = 4.167245891751686
545, epoch_train_loss=4.167245891751686
Epoch 546
Epoch 546 :: Batch 0/1
Batch Loss = 4.167151721666825
546, epoch_train_loss=4.167151721666825
Epoch 547
Epoch 547 :: Batch 0/1
Batch Loss = 4.167088999300555
547, epoch_train_loss=4.167088999300555
Epoch 548
Epoch 548 :: Batch 0/1
Batch Loss = 4.167012155559053
548, epoch_train_loss=4.167012155559053
Epoch 549
Epoch 549 :: Batch 0/1
Batch Loss = 4.166898170779705
549, epoch_train_loss=4.166898170779705
Epoch 550
Epoch 550 :: Batch 0/1
Batch Loss = 4.16676266633955
550, epoch_train_loss=4.16676266633955
Epoch 551
Epoch 551 :: Batch 0/1
Batch Loss = 4.166644608766224
551, epoch_train_loss=4.166644608766224
Epoch 552
Epoch 552 :: Batch 0/1
Batch Loss = 4.166559724120836
552, epoch_train_loss=4.166559724120836
Epoch 553
Epoch 553 :: Batch 0/1
Batch Loss = 4.166479485486448
553, epoch_train_loss=4.166479485486448
Epoch 554
Epoch 554 :: Batch 0/1
Batch Loss = 4.166371344801688
554, epoch_train_loss=4.166371344801688
Epoch 555
Epoch 555 :: Batch 0/1
Batch Loss = 4.166244511708839
555, epoch_train_loss=4.166244511708839
Epoch 556
Epoch 556 :: Batch 0/1
Batch Loss = 4.166128441961394
556, epoch_train_loss=4.166128441961394
Epoch 557
Epoch 557 :: Batch 0/1
Batch Loss = 4.16603024121954
557, epoch_train_loss=4.16603024121954
Epoch 558
Epoch 558 :: Batch 0/1
Batch Loss = 4.165934294256284
558, epoch_train_loss=4.165934294256284
Epoch 559
Epoch 559 :: Batch 0/1
Batch Loss = 4.165823724087755
559, epoch_train_loss=4.165823724087755
Epoch 560
Epoch 560 :: Batch 0/1
Batch Loss = 4.165698918255993
560, epoch_train_loss=4.165698918255993
Epoch 561
Epoch 561 :: Batch 0/1
Batch Loss = 4.165578176037607
561, epoch_train_loss=4.165578176037607
Epoch 562
Epoch 562 :: Batch 0/1
Batch Loss = 4.165471745709208
562, epoch_train_loss=4.165471745709208
Epoch 563
Epoch 563 :: Batch 0/1
Batch Loss = 4.1653647019474604
563, epoch_train_loss=4.1653647019474604
Epoch 564
Epoch 564 :: Batch 0/1
Batch Loss = 4.16524285101262
564, epoch_train_loss=4.16524285101262
Epoch 565
Epoch 565 :: Batch 0/1
Batch Loss = 4.165115414132089
565, epoch_train_loss=4.165115414132089
Epoch 566
Epoch 566 :: Batch 0/1
Batch Loss = 4.16499495709578
566, epoch_train_loss=4.16499495709578
Epoch 567
Epoch 567 :: Batch 0/1
Batch Loss = 4.164879489105769
567, epoch_train_loss=4.164879489105769
Epoch 568
Epoch 568 :: Batch 0/1
Batch Loss = 4.164760097540474
568, epoch_train_loss=4.164760097540474
Epoch 569
Epoch 569 :: Batch 0/1
Batch Loss = 4.164632916623608
569, epoch_train_loss=4.164632916623608
Epoch 570
Epoch 570 :: Batch 0/1
Batch Loss = 4.164503588382432
570, epoch_train_loss=4.164503588382432
Epoch 571
Epoch 571 :: Batch 0/1
Batch Loss = 4.16437919051561
571, epoch_train_loss=4.16437919051561
Epoch 572
Epoch 572 :: Batch 0/1
Batch Loss = 4.164256930810768
572, epoch_train_loss=4.164256930810768
Epoch 573
Epoch 573 :: Batch 0/1
Batch Loss = 4.164129726509524
573, epoch_train_loss=4.164129726509524
Epoch 574
Epoch 574 :: Batch 0/1
Batch Loss = 4.163998704541153
574, epoch_train_loss=4.163998704541153
Epoch 575
Epoch 575 :: Batch 0/1
Batch Loss = 4.163870063627119
575, epoch_train_loss=4.163870063627119
Epoch 576
Epoch 576 :: Batch 0/1
Batch Loss = 4.163744540696988
576, epoch_train_loss=4.163744540696988
Epoch 577
Epoch 577 :: Batch 0/1
Batch Loss = 4.163617336032626
577, epoch_train_loss=4.163617336032626
Epoch 578
Epoch 578 :: Batch 0/1
Batch Loss = 4.16348635602534
578, epoch_train_loss=4.16348635602534
Epoch 579
Epoch 579 :: Batch 0/1
Batch Loss = 4.163355664968859
579, epoch_train_loss=4.163355664968859
Epoch 580
Epoch 580 :: Batch 0/1
Batch Loss = 4.163227978630766
580, epoch_train_loss=4.163227978630766
Epoch 581
Epoch 581 :: Batch 0/1
Batch Loss = 4.163099924120438
581, epoch_train_loss=4.163099924120438
Epoch 582
Epoch 582 :: Batch 0/1
Batch Loss = 4.162969022311522
582, epoch_train_loss=4.162969022311522
Epoch 583
Epoch 583 :: Batch 0/1
Batch Loss = 4.162837643761623
583, epoch_train_loss=4.162837643761623
Epoch 584
Epoch 584 :: Batch 0/1
Batch Loss = 4.162707837669299
584, epoch_train_loss=4.162707837669299
Epoch 585
Epoch 585 :: Batch 0/1
Batch Loss = 4.162577819074984
585, epoch_train_loss=4.162577819074984
Epoch 586
Epoch 586 :: Batch 0/1
Batch Loss = 4.162445375348525
586, epoch_train_loss=4.162445375348525
Epoch 587
Epoch 587 :: Batch 0/1
Batch Loss = 4.162311873950557
587, epoch_train_loss=4.162311873950557
Epoch 588
Epoch 588 :: Batch 0/1
Batch Loss = 4.162179009866453
588, epoch_train_loss=4.162179009866453
Epoch 589
Epoch 589 :: Batch 0/1
Batch Loss = 4.162045357750645
589, epoch_train_loss=4.162045357750645
Epoch 590
Epoch 590 :: Batch 0/1
Batch Loss = 4.161909429149195
590, epoch_train_loss=4.161909429149195
Epoch 591
Epoch 591 :: Batch 0/1
Batch Loss = 4.16177203172605
591, epoch_train_loss=4.16177203172605
Epoch 592
Epoch 592 :: Batch 0/1
Batch Loss = 4.161634228684068
592, epoch_train_loss=4.161634228684068
Epoch 593
Epoch 593 :: Batch 0/1
Batch Loss = 4.161494980616358
593, epoch_train_loss=4.161494980616358
Epoch 594
Epoch 594 :: Batch 0/1
Batch Loss = 4.16135300826488
594, epoch_train_loss=4.16135300826488
Epoch 595
Epoch 595 :: Batch 0/1
Batch Loss = 4.1612093321469645
595, epoch_train_loss=4.1612093321469645
Epoch 596
Epoch 596 :: Batch 0/1
Batch Loss = 4.1610644742322
596, epoch_train_loss=4.1610644742322
Epoch 597
Epoch 597 :: Batch 0/1
Batch Loss = 4.160917146443206
597, epoch_train_loss=4.160917146443206
Epoch 598
Epoch 598 :: Batch 0/1
Batch Loss = 4.1607670367842475
598, epoch_train_loss=4.1607670367842475
Epoch 599
Epoch 599 :: Batch 0/1
Batch Loss = 4.160614858705479
599, epoch_train_loss=4.160614858705479
Epoch 600
Epoch 600 :: Batch 0/1
Batch Loss = 4.160460532117936
600, epoch_train_loss=4.160460532117936
Epoch 601
Epoch 601 :: Batch 0/1
Batch Loss = 4.1603032049555395
601, epoch_train_loss=4.1603032049555395
Epoch 602
Epoch 602 :: Batch 0/1
Batch Loss = 4.160142998365825
602, epoch_train_loss=4.160142998365825
Epoch 603
Epoch 603 :: Batch 0/1
Batch Loss = 4.159980495434835
603, epoch_train_loss=4.159980495434835
Epoch 604
Epoch 604 :: Batch 0/1
Batch Loss = 4.159815263827583
604, epoch_train_loss=4.159815263827583
Epoch 605
Epoch 605 :: Batch 0/1
Batch Loss = 4.159646973831083
605, epoch_train_loss=4.159646973831083
Epoch 606
Epoch 606 :: Batch 0/1
Batch Loss = 4.159476159630476
606, epoch_train_loss=4.159476159630476
Epoch 607
Epoch 607 :: Batch 0/1
Batch Loss = 4.159303045142912
607, epoch_train_loss=4.159303045142912
Epoch 608
Epoch 608 :: Batch 0/1
Batch Loss = 4.159127296922446
608, epoch_train_loss=4.159127296922446
Epoch 609
Epoch 609 :: Batch 0/1
Batch Loss = 4.1589492975903575
609, epoch_train_loss=4.1589492975903575
Epoch 610
Epoch 610 :: Batch 0/1
Batch Loss = 4.158769591619861
610, epoch_train_loss=4.158769591619861
Epoch 611
Epoch 611 :: Batch 0/1
Batch Loss = 4.158588122243656
611, epoch_train_loss=4.158588122243656
Epoch 612
Epoch 612 :: Batch 0/1
Batch Loss = 4.158405110529129
612, epoch_train_loss=4.158405110529129
Epoch 613
Epoch 613 :: Batch 0/1
Batch Loss = 4.158221196769491
613, epoch_train_loss=4.158221196769491
Epoch 614
Epoch 614 :: Batch 0/1
Batch Loss = 4.15803653945444
614, epoch_train_loss=4.15803653945444
Epoch 615
Epoch 615 :: Batch 0/1
Batch Loss = 4.157851300364968
615, epoch_train_loss=4.157851300364968
Epoch 616
Epoch 616 :: Batch 0/1
Batch Loss = 4.157665985272639
616, epoch_train_loss=4.157665985272639
Epoch 617
Epoch 617 :: Batch 0/1
Batch Loss = 4.157480823604942
617, epoch_train_loss=4.157480823604942
Epoch 618
Epoch 618 :: Batch 0/1
Batch Loss = 4.1572958329476295
618, epoch_train_loss=4.1572958329476295
Epoch 619
Epoch 619 :: Batch 0/1
Batch Loss = 4.157111396821799
619, epoch_train_loss=4.157111396821799
Epoch 620
Epoch 620 :: Batch 0/1
Batch Loss = 4.156927654310088
620, epoch_train_loss=4.156927654310088
Epoch 621
Epoch 621 :: Batch 0/1
Batch Loss = 4.156744572921404
621, epoch_train_loss=4.156744572921404
Epoch 622
Epoch 622 :: Batch 0/1
Batch Loss = 4.156562376613212
622, epoch_train_loss=4.156562376613212
Epoch 623
Epoch 623 :: Batch 0/1
Batch Loss = 4.156381098483032
623, epoch_train_loss=4.156381098483032
Epoch 624
Epoch 624 :: Batch 0/1
Batch Loss = 4.156200639523299
624, epoch_train_loss=4.156200639523299
Epoch 625
Epoch 625 :: Batch 0/1
Batch Loss = 4.156021108805241
625, epoch_train_loss=4.156021108805241
Epoch 626
Epoch 626 :: Batch 0/1
Batch Loss = 4.155842413701792
626, epoch_train_loss=4.155842413701792
Epoch 627
Epoch 627 :: Batch 0/1
Batch Loss = 4.155664397561516
627, epoch_train_loss=4.155664397561516
Epoch 628
Epoch 628 :: Batch 0/1
Batch Loss = 4.155487021595308
628, epoch_train_loss=4.155487021595308
Epoch 629
Epoch 629 :: Batch 0/1
Batch Loss = 4.155310058282066
629, epoch_train_loss=4.155310058282066
Epoch 630
Epoch 630 :: Batch 0/1
Batch Loss = 4.155133275583004
630, epoch_train_loss=4.155133275583004
Epoch 631
Epoch 631 :: Batch 0/1
Batch Loss = 4.154956503104115
631, epoch_train_loss=4.154956503104115
Epoch 632
Epoch 632 :: Batch 0/1
Batch Loss = 4.154779398380912
632, epoch_train_loss=4.154779398380912
Epoch 633
Epoch 633 :: Batch 0/1
Batch Loss = 4.154601704284271
633, epoch_train_loss=4.154601704284271
Epoch 634
Epoch 634 :: Batch 0/1
Batch Loss = 4.154423147110941
634, epoch_train_loss=4.154423147110941
Epoch 635
Epoch 635 :: Batch 0/1
Batch Loss = 4.154243385661884
635, epoch_train_loss=4.154243385661884
Epoch 636
Epoch 636 :: Batch 0/1
Batch Loss = 4.154062186559725
636, epoch_train_loss=4.154062186559725
Epoch 637
Epoch 637 :: Batch 0/1
Batch Loss = 4.153879276100528
637, epoch_train_loss=4.153879276100528
Epoch 638
Epoch 638 :: Batch 0/1
Batch Loss = 4.153694415629235
638, epoch_train_loss=4.153694415629235
Epoch 639
Epoch 639 :: Batch 0/1
Batch Loss = 4.153507459623557
639, epoch_train_loss=4.153507459623557
Epoch 640
Epoch 640 :: Batch 0/1
Batch Loss = 4.153318228827562
640, epoch_train_loss=4.153318228827562
Epoch 641
Epoch 641 :: Batch 0/1
Batch Loss = 4.153126654151174
641, epoch_train_loss=4.153126654151174
Epoch 642
Epoch 642 :: Batch 0/1
Batch Loss = 4.152932675643288
642, epoch_train_loss=4.152932675643288
Epoch 643
Epoch 643 :: Batch 0/1
Batch Loss = 4.152736261508275
643, epoch_train_loss=4.152736261508275
Epoch 644
Epoch 644 :: Batch 0/1
Batch Loss = 4.1525374357974085
644, epoch_train_loss=4.1525374357974085
Epoch 645
Epoch 645 :: Batch 0/1
Batch Loss = 4.152336187767837
645, epoch_train_loss=4.152336187767837
Epoch 646
Epoch 646 :: Batch 0/1
Batch Loss = 4.152132546261404
646, epoch_train_loss=4.152132546261404
Epoch 647
Epoch 647 :: Batch 0/1
Batch Loss = 4.151926494318022
647, epoch_train_loss=4.151926494318022
Epoch 648
Epoch 648 :: Batch 0/1
Batch Loss = 4.1517180005578735
648, epoch_train_loss=4.1517180005578735
Epoch 649
Epoch 649 :: Batch 0/1
Batch Loss = 4.151507006936736
649, epoch_train_loss=4.151507006936736
Epoch 650
Epoch 650 :: Batch 0/1
Batch Loss = 4.151293405643219
650, epoch_train_loss=4.151293405643219
Epoch 651
Epoch 651 :: Batch 0/1
Batch Loss = 4.1510770898842635
651, epoch_train_loss=4.1510770898842635
Epoch 652
Epoch 652 :: Batch 0/1
Batch Loss = 4.150857912494454
652, epoch_train_loss=4.150857912494454
Epoch 653
Epoch 653 :: Batch 0/1
Batch Loss = 4.150635753861377
653, epoch_train_loss=4.150635753861377
Epoch 654
Epoch 654 :: Batch 0/1
Batch Loss = 4.15041049238294
654, epoch_train_loss=4.15041049238294
Epoch 655
Epoch 655 :: Batch 0/1
Batch Loss = 4.150182056419041
655, epoch_train_loss=4.150182056419041
Epoch 656
Epoch 656 :: Batch 0/1
Batch Loss = 4.149950411810379
656, epoch_train_loss=4.149950411810379
Epoch 657
Epoch 657 :: Batch 0/1
Batch Loss = 4.149715578135651
657, epoch_train_loss=4.149715578135651
Epoch 658
Epoch 658 :: Batch 0/1
Batch Loss = 4.149477621984539
658, epoch_train_loss=4.149477621984539
Epoch 659
Epoch 659 :: Batch 0/1
Batch Loss = 4.149236646609365
659, epoch_train_loss=4.149236646609365
Epoch 660
Epoch 660 :: Batch 0/1
Batch Loss = 4.1489927905415005
660, epoch_train_loss=4.1489927905415005
Epoch 661
Epoch 661 :: Batch 0/1
Batch Loss = 4.1487461978095945
661, epoch_train_loss=4.1487461978095945
Epoch 662
Epoch 662 :: Batch 0/1
Batch Loss = 4.148497024320402
662, epoch_train_loss=4.148497024320402
Epoch 663
Epoch 663 :: Batch 0/1
Batch Loss = 4.1482454121885795
663, epoch_train_loss=4.1482454121885795
Epoch 664
Epoch 664 :: Batch 0/1
Batch Loss = 4.147991505705462
664, epoch_train_loss=4.147991505705462
Epoch 665
Epoch 665 :: Batch 0/1
Batch Loss = 4.147735432968048
665, epoch_train_loss=4.147735432968048
Epoch 666
Epoch 666 :: Batch 0/1
Batch Loss = 4.147477328337459
666, epoch_train_loss=4.147477328337459
Epoch 667
Epoch 667 :: Batch 0/1
Batch Loss = 4.147217326460741
667, epoch_train_loss=4.147217326460741
Epoch 668
Epoch 668 :: Batch 0/1
Batch Loss = 4.146955621574512
668, epoch_train_loss=4.146955621574512
Epoch 669
Epoch 669 :: Batch 0/1
Batch Loss = 4.146692578558733
669, epoch_train_loss=4.146692578558733
Epoch 670
Epoch 670 :: Batch 0/1
Batch Loss = 4.14642952334687
670, epoch_train_loss=4.14642952334687
Epoch 671
Epoch 671 :: Batch 0/1
Batch Loss = 4.146173195818591
671, epoch_train_loss=4.146173195818591
Epoch 672
Epoch 672 :: Batch 0/1
Batch Loss = 4.145956440849903
672, epoch_train_loss=4.145956440849903
Epoch 673
Epoch 673 :: Batch 0/1
Batch Loss = 4.145930936890162
673, epoch_train_loss=4.145930936890162
Epoch 674
Epoch 674 :: Batch 0/1
Batch Loss = 4.145724197483173
674, epoch_train_loss=4.145724197483173
Epoch 675
Epoch 675 :: Batch 0/1
Batch Loss = 4.145588916000661
675, epoch_train_loss=4.145588916000661
Epoch 676
Epoch 676 :: Batch 0/1
Batch Loss = 4.145009222520201
676, epoch_train_loss=4.145009222520201
Epoch 677
Epoch 677 :: Batch 0/1
Batch Loss = 4.1448675060509
677, epoch_train_loss=4.1448675060509
Epoch 678
Epoch 678 :: Batch 0/1
Batch Loss = 4.144540164967123
678, epoch_train_loss=4.144540164967123
Epoch 679
Epoch 679 :: Batch 0/1
Batch Loss = 4.144308050123068
679, epoch_train_loss=4.144308050123068
Epoch 680
Epoch 680 :: Batch 0/1
Batch Loss = 4.1441936154331165
680, epoch_train_loss=4.1441936154331165
Epoch 681
Epoch 681 :: Batch 0/1
Batch Loss = 4.143606079351964
681, epoch_train_loss=4.143606079351964
Epoch 682
Epoch 682 :: Batch 0/1
Batch Loss = 4.143754822886166
682, epoch_train_loss=4.143754822886166
Epoch 683
Epoch 683 :: Batch 0/1
Batch Loss = 4.14308244684521
683, epoch_train_loss=4.14308244684521
Epoch 684
Epoch 684 :: Batch 0/1
Batch Loss = 4.142914467870834
684, epoch_train_loss=4.142914467870834
Epoch 685
Epoch 685 :: Batch 0/1
Batch Loss = 4.142492826643922
685, epoch_train_loss=4.142492826643922
Epoch 686
Epoch 686 :: Batch 0/1
Batch Loss = 4.142219238038437
686, epoch_train_loss=4.142219238038437
Epoch 687
Epoch 687 :: Batch 0/1
Batch Loss = 4.141898992190747
687, epoch_train_loss=4.141898992190747
Epoch 688
Epoch 688 :: Batch 0/1
Batch Loss = 4.14157779115066
688, epoch_train_loss=4.14157779115066
Epoch 689
Epoch 689 :: Batch 0/1
Batch Loss = 4.141114730050663
689, epoch_train_loss=4.141114730050663
Epoch 690
Epoch 690 :: Batch 0/1
Batch Loss = 4.140873637121513
690, epoch_train_loss=4.140873637121513
Epoch 691
Epoch 691 :: Batch 0/1
Batch Loss = 4.140420067193889
691, epoch_train_loss=4.140420067193889
Epoch 692
Epoch 692 :: Batch 0/1
Batch Loss = 4.1401453304368525
692, epoch_train_loss=4.1401453304368525
Epoch 693
Epoch 693 :: Batch 0/1
Batch Loss = 4.139645608610872
693, epoch_train_loss=4.139645608610872
Epoch 694
Epoch 694 :: Batch 0/1
Batch Loss = 4.13931615587973
694, epoch_train_loss=4.13931615587973
Epoch 695
Epoch 695 :: Batch 0/1
Batch Loss = 4.138820354168171
695, epoch_train_loss=4.138820354168171
Epoch 696
Epoch 696 :: Batch 0/1
Batch Loss = 4.138444547690843
696, epoch_train_loss=4.138444547690843
Epoch 697
Epoch 697 :: Batch 0/1
Batch Loss = 4.138011023420727
697, epoch_train_loss=4.138011023420727
Epoch 698
Epoch 698 :: Batch 0/1
Batch Loss = 4.137542851430762
698, epoch_train_loss=4.137542851430762
Epoch 699
Epoch 699 :: Batch 0/1
Batch Loss = 4.137130007662874
699, epoch_train_loss=4.137130007662874
Epoch 700
Epoch 700 :: Batch 0/1
Batch Loss = 4.136595373607464
700, epoch_train_loss=4.136595373607464
Epoch 701
Epoch 701 :: Batch 0/1
Batch Loss = 4.136215288738488
701, epoch_train_loss=4.136215288738488
Epoch 702
Epoch 702 :: Batch 0/1
Batch Loss = 4.135656085672247
702, epoch_train_loss=4.135656085672247
Epoch 703
Epoch 703 :: Batch 0/1
Batch Loss = 4.1352721298031785
703, epoch_train_loss=4.1352721298031785
Epoch 704
Epoch 704 :: Batch 0/1
Batch Loss = 4.134736666614001
704, epoch_train_loss=4.134736666614001
Epoch 705
Epoch 705 :: Batch 0/1
Batch Loss = 4.134319518908011
705, epoch_train_loss=4.134319518908011
Epoch 706
Epoch 706 :: Batch 0/1
Batch Loss = 4.133888189237224
706, epoch_train_loss=4.133888189237224
Epoch 707
Epoch 707 :: Batch 0/1
Batch Loss = 4.133492048148981
707, epoch_train_loss=4.133492048148981
Epoch 708
Epoch 708 :: Batch 0/1
Batch Loss = 4.13328468891949
708, epoch_train_loss=4.13328468891949
Epoch 709
Epoch 709 :: Batch 0/1
Batch Loss = 4.13349881790681
709, epoch_train_loss=4.13349881790681
Epoch 710
Epoch 710 :: Batch 0/1
Batch Loss = 4.132690064949145
710, epoch_train_loss=4.132690064949145
Epoch 711
Epoch 711 :: Batch 0/1
Batch Loss = 4.133002007307138
711, epoch_train_loss=4.133002007307138
Epoch 712
Epoch 712 :: Batch 0/1
Batch Loss = 4.131615765589226
712, epoch_train_loss=4.131615765589226
Epoch 713
Epoch 713 :: Batch 0/1
Batch Loss = 4.132679113994051
713, epoch_train_loss=4.132679113994051
Epoch 714
Epoch 714 :: Batch 0/1
Batch Loss = 4.132543038208447
714, epoch_train_loss=4.132543038208447
Epoch 715
Epoch 715 :: Batch 0/1
Batch Loss = 4.132178590311158
715, epoch_train_loss=4.132178590311158
Epoch 716
Epoch 716 :: Batch 0/1
Batch Loss = 4.130651262634777
716, epoch_train_loss=4.130651262634777
Epoch 717
Epoch 717 :: Batch 0/1
Batch Loss = 4.130353866847206
717, epoch_train_loss=4.130353866847206
Epoch 718
Epoch 718 :: Batch 0/1
Batch Loss = 4.129682970176517
718, epoch_train_loss=4.129682970176517
Epoch 719
Epoch 719 :: Batch 0/1
Batch Loss = 4.129883585994538
719, epoch_train_loss=4.129883585994538
Epoch 720
Epoch 720 :: Batch 0/1
Batch Loss = 4.1287535371727015
720, epoch_train_loss=4.1287535371727015
Epoch 721
Epoch 721 :: Batch 0/1
Batch Loss = 4.128419515011773
721, epoch_train_loss=4.128419515011773
Epoch 722
Epoch 722 :: Batch 0/1
Batch Loss = 4.1288248224128195
722, epoch_train_loss=4.1288248224128195
Epoch 723
Epoch 723 :: Batch 0/1
Batch Loss = 4.127666752630846
723, epoch_train_loss=4.127666752630846
Epoch 724
Epoch 724 :: Batch 0/1
Batch Loss = 4.128263316294956
724, epoch_train_loss=4.128263316294956
Epoch 725
Epoch 725 :: Batch 0/1
Batch Loss = 4.128501017924678
725, epoch_train_loss=4.128501017924678
Epoch 726
Epoch 726 :: Batch 0/1
Batch Loss = 4.127703690419674
726, epoch_train_loss=4.127703690419674
Epoch 727
Epoch 727 :: Batch 0/1
Batch Loss = 4.127147769079229
727, epoch_train_loss=4.127147769079229
Epoch 728
Epoch 728 :: Batch 0/1
Batch Loss = 4.125468451699774
728, epoch_train_loss=4.125468451699774
Epoch 729
Epoch 729 :: Batch 0/1
Batch Loss = 4.126461323563024
729, epoch_train_loss=4.126461323563024
Epoch 730
Epoch 730 :: Batch 0/1
Batch Loss = 4.125107963727363
730, epoch_train_loss=4.125107963727363
Epoch 731
Epoch 731 :: Batch 0/1
Batch Loss = 4.12555379749266
731, epoch_train_loss=4.12555379749266
Epoch 732
Epoch 732 :: Batch 0/1
Batch Loss = 4.124513266192742
732, epoch_train_loss=4.124513266192742
Epoch 733
Epoch 733 :: Batch 0/1
Batch Loss = 4.12453815319488
733, epoch_train_loss=4.12453815319488
Epoch 734
Epoch 734 :: Batch 0/1
Batch Loss = 4.123523936298022
734, epoch_train_loss=4.123523936298022
Epoch 735
Epoch 735 :: Batch 0/1
Batch Loss = 4.123192996014576
735, epoch_train_loss=4.123192996014576
Epoch 736
Epoch 736 :: Batch 0/1
Batch Loss = 4.123304133036348
736, epoch_train_loss=4.123304133036348
Epoch 737
Epoch 737 :: Batch 0/1
Batch Loss = 4.122484318286314
737, epoch_train_loss=4.122484318286314
Epoch 738
Epoch 738 :: Batch 0/1
Batch Loss = 4.1224745177098745
738, epoch_train_loss=4.1224745177098745
Epoch 739
Epoch 739 :: Batch 0/1
Batch Loss = 4.121741210644788
739, epoch_train_loss=4.121741210644788
Epoch 740
Epoch 740 :: Batch 0/1
Batch Loss = 4.12187788060551
740, epoch_train_loss=4.12187788060551
Epoch 741
Epoch 741 :: Batch 0/1
Batch Loss = 4.121371872678244
741, epoch_train_loss=4.121371872678244
Epoch 742
Epoch 742 :: Batch 0/1
Batch Loss = 4.120857568812794
742, epoch_train_loss=4.120857568812794
Epoch 743
Epoch 743 :: Batch 0/1
Batch Loss = 4.121083641197957
743, epoch_train_loss=4.121083641197957
Epoch 744
Epoch 744 :: Batch 0/1
Batch Loss = 4.120828903997135
744, epoch_train_loss=4.120828903997135
Epoch 745
Epoch 745 :: Batch 0/1
Batch Loss = 4.120619542527817
745, epoch_train_loss=4.120619542527817
Epoch 746
Epoch 746 :: Batch 0/1
Batch Loss = 4.119561733001208
746, epoch_train_loss=4.119561733001208
Epoch 747
Epoch 747 :: Batch 0/1
Batch Loss = 4.119376683673059
747, epoch_train_loss=4.119376683673059
Epoch 748
Epoch 748 :: Batch 0/1
Batch Loss = 4.119293090830141
748, epoch_train_loss=4.119293090830141
Epoch 749
Epoch 749 :: Batch 0/1
Batch Loss = 4.118548335660854
749, epoch_train_loss=4.118548335660854
Epoch 750
Epoch 750 :: Batch 0/1
Batch Loss = 4.118145635886882
750, epoch_train_loss=4.118145635886882
Epoch 751
Epoch 751 :: Batch 0/1
Batch Loss = 4.117664855549521
751, epoch_train_loss=4.117664855549521
Epoch 752
Epoch 752 :: Batch 0/1
Batch Loss = 4.117391382378613
752, epoch_train_loss=4.117391382378613
Epoch 753
Epoch 753 :: Batch 0/1
Batch Loss = 4.117198068989407
753, epoch_train_loss=4.117198068989407
Epoch 754
Epoch 754 :: Batch 0/1
Batch Loss = 4.116731628174013
754, epoch_train_loss=4.116731628174013
Epoch 755
Epoch 755 :: Batch 0/1
Batch Loss = 4.116498748521784
755, epoch_train_loss=4.116498748521784
Epoch 756
Epoch 756 :: Batch 0/1
Batch Loss = 4.116139091409769
756, epoch_train_loss=4.116139091409769
Epoch 757
Epoch 757 :: Batch 0/1
Batch Loss = 4.115821182573949
757, epoch_train_loss=4.115821182573949
Epoch 758
Epoch 758 :: Batch 0/1
Batch Loss = 4.115721705594417
758, epoch_train_loss=4.115721705594417
Epoch 759
Epoch 759 :: Batch 0/1
Batch Loss = 4.115381284036291
759, epoch_train_loss=4.115381284036291
Epoch 760
Epoch 760 :: Batch 0/1
Batch Loss = 4.115157439802103
760, epoch_train_loss=4.115157439802103
Epoch 761
Epoch 761 :: Batch 0/1
Batch Loss = 4.114954866421842
761, epoch_train_loss=4.114954866421842
Epoch 762
Epoch 762 :: Batch 0/1
Batch Loss = 4.114477978981651
762, epoch_train_loss=4.114477978981651
Epoch 763
Epoch 763 :: Batch 0/1
Batch Loss = 4.11429599172129
763, epoch_train_loss=4.11429599172129
Epoch 764
Epoch 764 :: Batch 0/1
Batch Loss = 4.113915719508362
764, epoch_train_loss=4.113915719508362
Epoch 765
Epoch 765 :: Batch 0/1
Batch Loss = 4.1137387539950145
765, epoch_train_loss=4.1137387539950145
Epoch 766
Epoch 766 :: Batch 0/1
Batch Loss = 4.1135858216722685
766, epoch_train_loss=4.1135858216722685
Epoch 767
Epoch 767 :: Batch 0/1
Batch Loss = 4.113330397302695
767, epoch_train_loss=4.113330397302695
Epoch 768
Epoch 768 :: Batch 0/1
Batch Loss = 4.113383036085348
768, epoch_train_loss=4.113383036085348
Epoch 769
Epoch 769 :: Batch 0/1
Batch Loss = 4.112991131285252
769, epoch_train_loss=4.112991131285252
Epoch 770
Epoch 770 :: Batch 0/1
Batch Loss = 4.113033280146411
770, epoch_train_loss=4.113033280146411
Epoch 771
Epoch 771 :: Batch 0/1
Batch Loss = 4.112611819202242
771, epoch_train_loss=4.112611819202242
Epoch 772
Epoch 772 :: Batch 0/1
Batch Loss = 4.112675301371479
772, epoch_train_loss=4.112675301371479
Epoch 773
Epoch 773 :: Batch 0/1
Batch Loss = 4.112073448310002
773, epoch_train_loss=4.112073448310002
Epoch 774
Epoch 774 :: Batch 0/1
Batch Loss = 4.112325665984697
774, epoch_train_loss=4.112325665984697
Epoch 775
Epoch 775 :: Batch 0/1
Batch Loss = 4.111228555671341
775, epoch_train_loss=4.111228555671341
Epoch 776
Epoch 776 :: Batch 0/1
Batch Loss = 4.111261417330567
776, epoch_train_loss=4.111261417330567
Epoch 777
Epoch 777 :: Batch 0/1
Batch Loss = 4.110617746855588
777, epoch_train_loss=4.110617746855588
Epoch 778
Epoch 778 :: Batch 0/1
Batch Loss = 4.111082376876273
778, epoch_train_loss=4.111082376876273
Epoch 779
Epoch 779 :: Batch 0/1
Batch Loss = 4.112253707789734
779, epoch_train_loss=4.112253707789734
Epoch 780
Epoch 780 :: Batch 0/1
Batch Loss = 4.112278931004388
780, epoch_train_loss=4.112278931004388
Epoch 781
Epoch 781 :: Batch 0/1
Batch Loss = 4.114438242768539
781, epoch_train_loss=4.114438242768539
Epoch 782
Epoch 782 :: Batch 0/1
Batch Loss = 4.1135258926931035
782, epoch_train_loss=4.1135258926931035
Epoch 783
Epoch 783 :: Batch 0/1
Batch Loss = 4.111322154078255
783, epoch_train_loss=4.111322154078255
Epoch 784
Epoch 784 :: Batch 0/1
Batch Loss = 4.11238677624191
784, epoch_train_loss=4.11238677624191
Epoch 785
Epoch 785 :: Batch 0/1
Batch Loss = 4.108848383098361
785, epoch_train_loss=4.108848383098361
Epoch 786
Epoch 786 :: Batch 0/1
Batch Loss = 4.112732443819755
786, epoch_train_loss=4.112732443819755
Epoch 787
Epoch 787 :: Batch 0/1
Batch Loss = 4.115593717607623
787, epoch_train_loss=4.115593717607623
Epoch 788
Epoch 788 :: Batch 0/1
Batch Loss = 4.116926798514766
788, epoch_train_loss=4.116926798514766
Epoch 789
Epoch 789 :: Batch 0/1
Batch Loss = 4.115923321233934
789, epoch_train_loss=4.115923321233934
Epoch 790
Epoch 790 :: Batch 0/1
Batch Loss = 4.1126219585284165
790, epoch_train_loss=4.1126219585284165
Epoch 791
Epoch 791 :: Batch 0/1
Batch Loss = 4.115277254885507
791, epoch_train_loss=4.115277254885507
Epoch 792
Epoch 792 :: Batch 0/1
Batch Loss = 4.1155653526881535
792, epoch_train_loss=4.1155653526881535
Epoch 793
Epoch 793 :: Batch 0/1
Batch Loss = 4.1148273596469895
793, epoch_train_loss=4.1148273596469895
Epoch 794
Epoch 794 :: Batch 0/1
Batch Loss = 4.123061742491873
794, epoch_train_loss=4.123061742491873
Epoch 795
Epoch 795 :: Batch 0/1
Batch Loss = 4.125010995560562
795, epoch_train_loss=4.125010995560562
Epoch 796
Epoch 796 :: Batch 0/1
Batch Loss = 4.122401897797232
796, epoch_train_loss=4.122401897797232
Epoch 797
Epoch 797 :: Batch 0/1
Batch Loss = 4.10534735845709
797, epoch_train_loss=4.10534735845709
Epoch 798
Epoch 798 :: Batch 0/1
Batch Loss = 4.135134609773321
798, epoch_train_loss=4.135134609773321
Epoch 799
Epoch 799 :: Batch 0/1
Batch Loss = 4.145924960984783
799, epoch_train_loss=4.145924960984783
Epoch 800
Epoch 800 :: Batch 0/1
Batch Loss = 4.159666205242927
800, epoch_train_loss=4.159666205242927
Epoch 801
Epoch 801 :: Batch 0/1
Batch Loss = 4.157242829462223
801, epoch_train_loss=4.157242829462223
Epoch 802
Epoch 802 :: Batch 0/1
Batch Loss = 4.161750086223624
802, epoch_train_loss=4.161750086223624
Epoch 803
Epoch 803 :: Batch 0/1
Batch Loss = 4.154527997164108
803, epoch_train_loss=4.154527997164108
Epoch 804
Epoch 804 :: Batch 0/1
Batch Loss = 4.154527802964212
804, epoch_train_loss=4.154527802964212
Epoch 805
Epoch 805 :: Batch 0/1
Batch Loss = 4.150962599292613
805, epoch_train_loss=4.150962599292613
Epoch 806
Epoch 806 :: Batch 0/1
Batch Loss = 4.161287293162374
806, epoch_train_loss=4.161287293162374
Epoch 807
Epoch 807 :: Batch 0/1
Batch Loss = 4.150829016355072
807, epoch_train_loss=4.150829016355072
Epoch 808
Epoch 808 :: Batch 0/1
Batch Loss = 4.155167154473075
808, epoch_train_loss=4.155167154473075
Epoch 809
Epoch 809 :: Batch 0/1
Batch Loss = 4.156569255318852
809, epoch_train_loss=4.156569255318852
Epoch 810
Epoch 810 :: Batch 0/1
Batch Loss = 4.156876867630512
810, epoch_train_loss=4.156876867630512
Epoch 811
Epoch 811 :: Batch 0/1
Batch Loss = 4.156220522323595
811, epoch_train_loss=4.156220522323595
Epoch 812
Epoch 812 :: Batch 0/1
Batch Loss = 4.155383646734709
812, epoch_train_loss=4.155383646734709
Epoch 813
Epoch 813 :: Batch 0/1
Batch Loss = 4.153852139196016
813, epoch_train_loss=4.153852139196016
Epoch 814
Epoch 814 :: Batch 0/1
Batch Loss = 4.151659487471575
814, epoch_train_loss=4.151659487471575
Epoch 815
Epoch 815 :: Batch 0/1
Batch Loss = 4.147575668167449
815, epoch_train_loss=4.147575668167449
Epoch 816
Epoch 816 :: Batch 0/1
Batch Loss = 4.14270183443607
816, epoch_train_loss=4.14270183443607
Epoch 817
Epoch 817 :: Batch 0/1
Batch Loss = 4.15538712166561
817, epoch_train_loss=4.15538712166561
Epoch 818
Epoch 818 :: Batch 0/1
Batch Loss = 4.149046852186281
818, epoch_train_loss=4.149046852186281
Epoch 819
Epoch 819 :: Batch 0/1
Batch Loss = 4.151645573763286
819, epoch_train_loss=4.151645573763286
Epoch 820
Epoch 820 :: Batch 0/1
Batch Loss = 4.1554971126885505
820, epoch_train_loss=4.1554971126885505
Epoch 821
Epoch 821 :: Batch 0/1
Batch Loss = 4.1525389671069135
821, epoch_train_loss=4.1525389671069135
Epoch 822
Epoch 822 :: Batch 0/1
Batch Loss = 4.15387347483452
822, epoch_train_loss=4.15387347483452
Epoch 823
Epoch 823 :: Batch 0/1
Batch Loss = 4.15072189098356
823, epoch_train_loss=4.15072189098356
Epoch 824
Epoch 824 :: Batch 0/1
Batch Loss = 4.152031819521649
824, epoch_train_loss=4.152031819521649
Epoch 825
Epoch 825 :: Batch 0/1
Batch Loss = 4.147927748483303
825, epoch_train_loss=4.147927748483303
Epoch 826
Epoch 826 :: Batch 0/1
Batch Loss = 4.144056714389133
826, epoch_train_loss=4.144056714389133
Epoch 827
Epoch 827 :: Batch 0/1
Batch Loss = 4.156425560208578
827, epoch_train_loss=4.156425560208578
Epoch 828
Epoch 828 :: Batch 0/1
Batch Loss = 4.14267598100002
828, epoch_train_loss=4.14267598100002
Epoch 829
Epoch 829 :: Batch 0/1
Batch Loss = 4.146512325257489
829, epoch_train_loss=4.146512325257489
Epoch 830
Epoch 830 :: Batch 0/1
Batch Loss = 4.149833037553724
830, epoch_train_loss=4.149833037553724
Epoch 831
Epoch 831 :: Batch 0/1
Batch Loss = 4.149444503189733
831, epoch_train_loss=4.149444503189733
Epoch 832
Epoch 832 :: Batch 0/1
Batch Loss = 4.1496470664654215
832, epoch_train_loss=4.1496470664654215
Epoch 833
Epoch 833 :: Batch 0/1
Batch Loss = 4.1481452117348505
833, epoch_train_loss=4.1481452117348505
Epoch 834
Epoch 834 :: Batch 0/1
Batch Loss = 4.146884680321872
834, epoch_train_loss=4.146884680321872
Epoch 835
Epoch 835 :: Batch 0/1
Batch Loss = 4.143551349929421
835, epoch_train_loss=4.143551349929421
Epoch 836
Epoch 836 :: Batch 0/1
Batch Loss = 4.140428172958441
836, epoch_train_loss=4.140428172958441
Epoch 837
Epoch 837 :: Batch 0/1
Batch Loss = 4.144748796561737
837, epoch_train_loss=4.144748796561737
Epoch 838
Epoch 838 :: Batch 0/1
Batch Loss = 4.140094948160219
838, epoch_train_loss=4.140094948160219
Epoch 839
Epoch 839 :: Batch 0/1
Batch Loss = 4.141025061052255
839, epoch_train_loss=4.141025061052255
Epoch 840
Epoch 840 :: Batch 0/1
Batch Loss = 4.142618173855116
840, epoch_train_loss=4.142618173855116
Epoch 841
Epoch 841 :: Batch 0/1
Batch Loss = 4.140651679021926
841, epoch_train_loss=4.140651679021926
Epoch 842
Epoch 842 :: Batch 0/1
Batch Loss = 4.139401131195388
842, epoch_train_loss=4.139401131195388
Epoch 843
Epoch 843 :: Batch 0/1
Batch Loss = 4.140881110943094
843, epoch_train_loss=4.140881110943094
Epoch 844
Epoch 844 :: Batch 0/1
Batch Loss = 4.1383922375913285
844, epoch_train_loss=4.1383922375913285
Epoch 845
Epoch 845 :: Batch 0/1
Batch Loss = 4.138374082798065
845, epoch_train_loss=4.138374082798065
Epoch 846
Epoch 846 :: Batch 0/1
Batch Loss = 4.138583386447442
846, epoch_train_loss=4.138583386447442
Epoch 847
Epoch 847 :: Batch 0/1
Batch Loss = 4.137300186178329
847, epoch_train_loss=4.137300186178329
Epoch 848
Epoch 848 :: Batch 0/1
Batch Loss = 4.136428974978332
848, epoch_train_loss=4.136428974978332
Epoch 849
Epoch 849 :: Batch 0/1
Batch Loss = 4.1368247955929265
849, epoch_train_loss=4.1368247955929265
Epoch 850
Epoch 850 :: Batch 0/1
Batch Loss = 4.134950801163028
850, epoch_train_loss=4.134950801163028
Epoch 851
Epoch 851 :: Batch 0/1
Batch Loss = 4.134191516106541
851, epoch_train_loss=4.134191516106541
Epoch 852
Epoch 852 :: Batch 0/1
Batch Loss = 4.132886596623438
852, epoch_train_loss=4.132886596623438
Epoch 853
Epoch 853 :: Batch 0/1
Batch Loss = 4.129688754630523
853, epoch_train_loss=4.129688754630523
Epoch 854
Epoch 854 :: Batch 0/1
Batch Loss = 4.12571396937529
854, epoch_train_loss=4.12571396937529
Epoch 855
Epoch 855 :: Batch 0/1
Batch Loss = 4.126468513833639
855, epoch_train_loss=4.126468513833639
Epoch 856
Epoch 856 :: Batch 0/1
Batch Loss = 4.126323661929421
856, epoch_train_loss=4.126323661929421
Epoch 857
Epoch 857 :: Batch 0/1
Batch Loss = 4.12514361084696
857, epoch_train_loss=4.12514361084696
Epoch 858
Epoch 858 :: Batch 0/1
Batch Loss = 4.126387807950543
858, epoch_train_loss=4.126387807950543
Epoch 859
Epoch 859 :: Batch 0/1
Batch Loss = 4.121624960109454
859, epoch_train_loss=4.121624960109454
Epoch 860
Epoch 860 :: Batch 0/1
Batch Loss = 4.123884692891588
860, epoch_train_loss=4.123884692891588
Epoch 861
Epoch 861 :: Batch 0/1
Batch Loss = 4.120614442195918
861, epoch_train_loss=4.120614442195918
Epoch 862
Epoch 862 :: Batch 0/1
Batch Loss = 4.119509287275976
862, epoch_train_loss=4.119509287275976
Epoch 863
Epoch 863 :: Batch 0/1
Batch Loss = 4.120656047184827
863, epoch_train_loss=4.120656047184827
Epoch 864
Epoch 864 :: Batch 0/1
Batch Loss = 4.1180086768733135
864, epoch_train_loss=4.1180086768733135
Epoch 865
Epoch 865 :: Batch 0/1
Batch Loss = 4.115797195669166
865, epoch_train_loss=4.115797195669166
Epoch 866
Epoch 866 :: Batch 0/1
Batch Loss = 4.114034846049528
866, epoch_train_loss=4.114034846049528
Epoch 867
Epoch 867 :: Batch 0/1
Batch Loss = 4.108966262992782
867, epoch_train_loss=4.108966262992782
Epoch 868
Epoch 868 :: Batch 0/1
Batch Loss = 4.101290492426899
868, epoch_train_loss=4.101290492426899
Epoch 869
Epoch 869 :: Batch 0/1
Batch Loss = 4.127437386250134
869, epoch_train_loss=4.127437386250134
Epoch 870
Epoch 870 :: Batch 0/1
Batch Loss = 4.114737393150851
870, epoch_train_loss=4.114737393150851
Epoch 871
Epoch 871 :: Batch 0/1
Batch Loss = 4.115377281918346
871, epoch_train_loss=4.115377281918346
Epoch 872
Epoch 872 :: Batch 0/1
Batch Loss = 4.129764888770079
872, epoch_train_loss=4.129764888770079
Epoch 873
Epoch 873 :: Batch 0/1
Batch Loss = 4.134454333728921
873, epoch_train_loss=4.134454333728921
Epoch 874
Epoch 874 :: Batch 0/1
Batch Loss = 4.145394061391944
874, epoch_train_loss=4.145394061391944
Epoch 875
Epoch 875 :: Batch 0/1
Batch Loss = 4.141744256921381
875, epoch_train_loss=4.141744256921381
Epoch 876
Epoch 876 :: Batch 0/1
Batch Loss = 4.131617293963329
876, epoch_train_loss=4.131617293963329
Epoch 877
Epoch 877 :: Batch 0/1
Batch Loss = 4.1508337034108855
877, epoch_train_loss=4.1508337034108855
Epoch 878
Epoch 878 :: Batch 0/1
Batch Loss = 4.141233252274655
878, epoch_train_loss=4.141233252274655
Epoch 879
Epoch 879 :: Batch 0/1
Batch Loss = 4.149602644137937
879, epoch_train_loss=4.149602644137937
Epoch 880
Epoch 880 :: Batch 0/1
Batch Loss = 4.147793631059753
880, epoch_train_loss=4.147793631059753
Epoch 881
Epoch 881 :: Batch 0/1
Batch Loss = 4.151160886610656
881, epoch_train_loss=4.151160886610656
Epoch 882
Epoch 882 :: Batch 0/1
Batch Loss = 4.147525873109792
882, epoch_train_loss=4.147525873109792
Epoch 883
Epoch 883 :: Batch 0/1
Batch Loss = 4.143691935459314
883, epoch_train_loss=4.143691935459314
Epoch 884
Epoch 884 :: Batch 0/1
Batch Loss = 4.148884834577309
884, epoch_train_loss=4.148884834577309
Epoch 885
Epoch 885 :: Batch 0/1
Batch Loss = 4.1485355618835404
885, epoch_train_loss=4.1485355618835404
Epoch 886
Epoch 886 :: Batch 0/1
Batch Loss = 4.145526601644682
886, epoch_train_loss=4.145526601644682
Epoch 887
Epoch 887 :: Batch 0/1
Batch Loss = 4.14044559742716
887, epoch_train_loss=4.14044559742716
Epoch 888
Epoch 888 :: Batch 0/1
Batch Loss = 4.147661202957366
888, epoch_train_loss=4.147661202957366
Epoch 889
Epoch 889 :: Batch 0/1
Batch Loss = 4.138744179102653
889, epoch_train_loss=4.138744179102653
Epoch 890
Epoch 890 :: Batch 0/1
Batch Loss = 4.141115480718576
890, epoch_train_loss=4.141115480718576
Epoch 891
Epoch 891 :: Batch 0/1
Batch Loss = 4.140192042378384
891, epoch_train_loss=4.140192042378384
Epoch 892
Epoch 892 :: Batch 0/1
Batch Loss = 4.137850861937077
892, epoch_train_loss=4.137850861937077
Epoch 893
Epoch 893 :: Batch 0/1
Batch Loss = 4.137372536075348
893, epoch_train_loss=4.137372536075348
Epoch 894
Epoch 894 :: Batch 0/1
Batch Loss = 4.1391406148342575
894, epoch_train_loss=4.1391406148342575
Epoch 895
Epoch 895 :: Batch 0/1
Batch Loss = 4.1386819622520905
895, epoch_train_loss=4.1386819622520905
Epoch 896
Epoch 896 :: Batch 0/1
Batch Loss = 4.135514301237109
896, epoch_train_loss=4.135514301237109
Epoch 897
Epoch 897 :: Batch 0/1
Batch Loss = 4.136055482662208
897, epoch_train_loss=4.136055482662208
Epoch 898
Epoch 898 :: Batch 0/1
Batch Loss = 4.135432145828089
898, epoch_train_loss=4.135432145828089
Epoch 899
Epoch 899 :: Batch 0/1
Batch Loss = 4.134660560286048
899, epoch_train_loss=4.134660560286048
Epoch 900
Epoch 900 :: Batch 0/1
Batch Loss = 4.1313078883245895
900, epoch_train_loss=4.1313078883245895
Epoch 901
Epoch 901 :: Batch 0/1
Batch Loss = 4.132509133226632
901, epoch_train_loss=4.132509133226632
Epoch 902
Epoch 902 :: Batch 0/1
Batch Loss = 4.127541953736153
902, epoch_train_loss=4.127541953736153
Epoch 903
Epoch 903 :: Batch 0/1
Batch Loss = 4.122443466346534
903, epoch_train_loss=4.122443466346534
Epoch 904
Epoch 904 :: Batch 0/1
Batch Loss = 4.124152336096085
904, epoch_train_loss=4.124152336096085
Epoch 905
Epoch 905 :: Batch 0/1
Batch Loss = 4.1194978725855025
905, epoch_train_loss=4.1194978725855025
Epoch 906
Epoch 906 :: Batch 0/1
Batch Loss = 4.121079203561982
906, epoch_train_loss=4.121079203561982
Epoch 907
Epoch 907 :: Batch 0/1
Batch Loss = 4.120811151714366
907, epoch_train_loss=4.120811151714366
Epoch 908
Epoch 908 :: Batch 0/1
Batch Loss = 4.119800391659483
908, epoch_train_loss=4.119800391659483
Epoch 909
Epoch 909 :: Batch 0/1
Batch Loss = 4.117419403357059
909, epoch_train_loss=4.117419403357059
Epoch 910
Epoch 910 :: Batch 0/1
Batch Loss = 4.120113635538624
910, epoch_train_loss=4.120113635538624
Epoch 911
Epoch 911 :: Batch 0/1
Batch Loss = 4.118726101316944
911, epoch_train_loss=4.118726101316944
Epoch 912
Epoch 912 :: Batch 0/1
Batch Loss = 4.115536480132419
912, epoch_train_loss=4.115536480132419
Epoch 913
Epoch 913 :: Batch 0/1
Batch Loss = 4.118916416865986
913, epoch_train_loss=4.118916416865986
Epoch 914
Epoch 914 :: Batch 0/1
Batch Loss = 4.115888450210699
914, epoch_train_loss=4.115888450210699
Epoch 915
Epoch 915 :: Batch 0/1
Batch Loss = 4.11516079452442
915, epoch_train_loss=4.11516079452442
Epoch 916
Epoch 916 :: Batch 0/1
Batch Loss = 4.116213227060083
916, epoch_train_loss=4.116213227060083
Epoch 917
Epoch 917 :: Batch 0/1
Batch Loss = 4.113291317030757
917, epoch_train_loss=4.113291317030757
Epoch 918
Epoch 918 :: Batch 0/1
Batch Loss = 4.113961901543363
918, epoch_train_loss=4.113961901543363
Epoch 919
Epoch 919 :: Batch 0/1
Batch Loss = 4.114104367168898
919, epoch_train_loss=4.114104367168898
Epoch 920
Epoch 920 :: Batch 0/1
Batch Loss = 4.112503021123279
920, epoch_train_loss=4.112503021123279
Epoch 921
Epoch 921 :: Batch 0/1
Batch Loss = 4.112611938822231
921, epoch_train_loss=4.112611938822231
Epoch 922
Epoch 922 :: Batch 0/1
Batch Loss = 4.11300265518572
922, epoch_train_loss=4.11300265518572
Epoch 923
Epoch 923 :: Batch 0/1
Batch Loss = 4.111708461101279
923, epoch_train_loss=4.111708461101279
Epoch 924
Epoch 924 :: Batch 0/1
Batch Loss = 4.112101359091567
924, epoch_train_loss=4.112101359091567
Epoch 925
Epoch 925 :: Batch 0/1
Batch Loss = 4.111791208634712
925, epoch_train_loss=4.111791208634712
Epoch 926
Epoch 926 :: Batch 0/1
Batch Loss = 4.111343119565132
926, epoch_train_loss=4.111343119565132
Epoch 927
Epoch 927 :: Batch 0/1
Batch Loss = 4.111321169153278
927, epoch_train_loss=4.111321169153278
Epoch 928
Epoch 928 :: Batch 0/1
Batch Loss = 4.11095154002591
928, epoch_train_loss=4.11095154002591
Epoch 929
Epoch 929 :: Batch 0/1
Batch Loss = 4.11089038615375
929, epoch_train_loss=4.11089038615375
Epoch 930
Epoch 930 :: Batch 0/1
Batch Loss = 4.1105445806127685
930, epoch_train_loss=4.1105445806127685
Epoch 931
Epoch 931 :: Batch 0/1
Batch Loss = 4.110397687547173
931, epoch_train_loss=4.110397687547173
Epoch 932
Epoch 932 :: Batch 0/1
Batch Loss = 4.110263484609051
932, epoch_train_loss=4.110263484609051
Epoch 933
Epoch 933 :: Batch 0/1
Batch Loss = 4.1098770538933085
933, epoch_train_loss=4.1098770538933085
Epoch 934
Epoch 934 :: Batch 0/1
Batch Loss = 4.109900205062561
934, epoch_train_loss=4.109900205062561
Epoch 935
Epoch 935 :: Batch 0/1
Batch Loss = 4.109527908342904
935, epoch_train_loss=4.109527908342904
Epoch 936
Epoch 936 :: Batch 0/1
Batch Loss = 4.1093337593371215
936, epoch_train_loss=4.1093337593371215
Epoch 937
Epoch 937 :: Batch 0/1
Batch Loss = 4.10927361413144
937, epoch_train_loss=4.10927361413144
Epoch 938
Epoch 938 :: Batch 0/1
Batch Loss = 4.108900221952337
938, epoch_train_loss=4.108900221952337
Epoch 939
Epoch 939 :: Batch 0/1
Batch Loss = 4.108799240692496
939, epoch_train_loss=4.108799240692496
Epoch 940
Epoch 940 :: Batch 0/1
Batch Loss = 4.108668127086057
940, epoch_train_loss=4.108668127086057
Epoch 941
Epoch 941 :: Batch 0/1
Batch Loss = 4.10835936790824
941, epoch_train_loss=4.10835936790824
Epoch 942
Epoch 942 :: Batch 0/1
Batch Loss = 4.108263239284111
942, epoch_train_loss=4.108263239284111
Epoch 943
Epoch 943 :: Batch 0/1
Batch Loss = 4.108082512833032
943, epoch_train_loss=4.108082512833032
Epoch 944
Epoch 944 :: Batch 0/1
Batch Loss = 4.107831130562437
944, epoch_train_loss=4.107831130562437
Epoch 945
Epoch 945 :: Batch 0/1
Batch Loss = 4.107692915162352
945, epoch_train_loss=4.107692915162352
Epoch 946
Epoch 946 :: Batch 0/1
Batch Loss = 4.107489873791321
946, epoch_train_loss=4.107489873791321
Epoch 947
Epoch 947 :: Batch 0/1
Batch Loss = 4.107258604737969
947, epoch_train_loss=4.107258604737969
Epoch 948
Epoch 948 :: Batch 0/1
Batch Loss = 4.1070836880069805
948, epoch_train_loss=4.1070836880069805
Epoch 949
Epoch 949 :: Batch 0/1
Batch Loss = 4.106874530181416
949, epoch_train_loss=4.106874530181416
Epoch 950
Epoch 950 :: Batch 0/1
Batch Loss = 4.106646323036917
950, epoch_train_loss=4.106646323036917
Epoch 951
Epoch 951 :: Batch 0/1
Batch Loss = 4.106440524972084
951, epoch_train_loss=4.106440524972084
Epoch 952
Epoch 952 :: Batch 0/1
Batch Loss = 4.10622754223539
952, epoch_train_loss=4.10622754223539
Epoch 953
Epoch 953 :: Batch 0/1
Batch Loss = 4.10598584530882
953, epoch_train_loss=4.10598584530882
Epoch 954
Epoch 954 :: Batch 0/1
Batch Loss = 4.105769177012496
954, epoch_train_loss=4.105769177012496
Epoch 955
Epoch 955 :: Batch 0/1
Batch Loss = 4.105532825744178
955, epoch_train_loss=4.105532825744178
Epoch 956
Epoch 956 :: Batch 0/1
Batch Loss = 4.1052704313906325
956, epoch_train_loss=4.1052704313906325
Epoch 957
Epoch 957 :: Batch 0/1
Batch Loss = 4.105035630767235
957, epoch_train_loss=4.105035630767235
Epoch 958
Epoch 958 :: Batch 0/1
Batch Loss = 4.104758662520629
958, epoch_train_loss=4.104758662520629
Epoch 959
Epoch 959 :: Batch 0/1
Batch Loss = 4.104475908604915
959, epoch_train_loss=4.104475908604915
Epoch 960
Epoch 960 :: Batch 0/1
Batch Loss = 4.104197417759261
960, epoch_train_loss=4.104197417759261
Epoch 961
Epoch 961 :: Batch 0/1
Batch Loss = 4.103870946010491
961, epoch_train_loss=4.103870946010491
Epoch 962
Epoch 962 :: Batch 0/1
Batch Loss = 4.103546847917377
962, epoch_train_loss=4.103546847917377
Epoch 963
Epoch 963 :: Batch 0/1
Batch Loss = 4.103191372052027
963, epoch_train_loss=4.103191372052027
Epoch 964
Epoch 964 :: Batch 0/1
Batch Loss = 4.102788858076712
964, epoch_train_loss=4.102788858076712
Epoch 965
Epoch 965 :: Batch 0/1
Batch Loss = 4.102353515661783
965, epoch_train_loss=4.102353515661783
Epoch 966
Epoch 966 :: Batch 0/1
Batch Loss = 4.101835444590098
966, epoch_train_loss=4.101835444590098
Epoch 967
Epoch 967 :: Batch 0/1
Batch Loss = 4.101214862234231
967, epoch_train_loss=4.101214862234231
Epoch 968
Epoch 968 :: Batch 0/1
Batch Loss = 4.1004202320724135
968, epoch_train_loss=4.1004202320724135
Epoch 969
Epoch 969 :: Batch 0/1
Batch Loss = 4.099312916853823
969, epoch_train_loss=4.099312916853823
Epoch 970
Epoch 970 :: Batch 0/1
Batch Loss = 4.097594195544109
970, epoch_train_loss=4.097594195544109
Epoch 971
Epoch 971 :: Batch 0/1
Batch Loss = 4.094618391303133
971, epoch_train_loss=4.094618391303133
Epoch 972
Epoch 972 :: Batch 0/1
Batch Loss = 4.090249715335722
972, epoch_train_loss=4.090249715335722
Epoch 973
Epoch 973 :: Batch 0/1
Batch Loss = 4.095155308095197
973, epoch_train_loss=4.095155308095197
Epoch 974
Epoch 974 :: Batch 0/1
Batch Loss = 4.12595649605079
974, epoch_train_loss=4.12595649605079
Epoch 975
Epoch 975 :: Batch 0/1
Batch Loss = 4.088663699262208
975, epoch_train_loss=4.088663699262208
Epoch 976
Epoch 976 :: Batch 0/1
Batch Loss = 4.08720980531158
976, epoch_train_loss=4.08720980531158
Epoch 977
Epoch 977 :: Batch 0/1
Batch Loss = 4.087561639137038
977, epoch_train_loss=4.087561639137038
Epoch 978
Epoch 978 :: Batch 0/1
Batch Loss = 4.086808227696
978, epoch_train_loss=4.086808227696
Epoch 979
Epoch 979 :: Batch 0/1
Batch Loss = 4.08624320283567
979, epoch_train_loss=4.08624320283567
Epoch 980
Epoch 980 :: Batch 0/1
Batch Loss = 4.086019293665212
980, epoch_train_loss=4.086019293665212
Epoch 981
Epoch 981 :: Batch 0/1
Batch Loss = 4.085430775754123
981, epoch_train_loss=4.085430775754123
Epoch 982
Epoch 982 :: Batch 0/1
Batch Loss = 4.085169701970231
982, epoch_train_loss=4.085169701970231
Epoch 983
Epoch 983 :: Batch 0/1
Batch Loss = 4.084543400795348
983, epoch_train_loss=4.084543400795348
Epoch 984
Epoch 984 :: Batch 0/1
Batch Loss = 4.084459527151865
984, epoch_train_loss=4.084459527151865
Epoch 985
Epoch 985 :: Batch 0/1
Batch Loss = 4.08371788686089
985, epoch_train_loss=4.08371788686089
Epoch 986
Epoch 986 :: Batch 0/1
Batch Loss = 4.083549475307228
986, epoch_train_loss=4.083549475307228
Epoch 987
Epoch 987 :: Batch 0/1
Batch Loss = 4.083181985943436
987, epoch_train_loss=4.083181985943436
Epoch 988
Epoch 988 :: Batch 0/1
Batch Loss = 4.082844819570076
988, epoch_train_loss=4.082844819570076
Epoch 989
Epoch 989 :: Batch 0/1
Batch Loss = 4.082567376140218
989, epoch_train_loss=4.082567376140218
Epoch 990
Epoch 990 :: Batch 0/1
Batch Loss = 4.08251446276903
990, epoch_train_loss=4.08251446276903
Epoch 991
Epoch 991 :: Batch 0/1
Batch Loss = 4.082314057820886
991, epoch_train_loss=4.082314057820886
Epoch 992
Epoch 992 :: Batch 0/1
Batch Loss = 4.082161341805731
992, epoch_train_loss=4.082161341805731
Epoch 993
Epoch 993 :: Batch 0/1
Batch Loss = 4.082049834733845
993, epoch_train_loss=4.082049834733845
Epoch 994
Epoch 994 :: Batch 0/1
Batch Loss = 4.081785876439246
994, epoch_train_loss=4.081785876439246
Epoch 995
Epoch 995 :: Batch 0/1
Batch Loss = 4.08177326127761
995, epoch_train_loss=4.08177326127761
Epoch 996
Epoch 996 :: Batch 0/1
Batch Loss = 4.081274538988585
996, epoch_train_loss=4.081274538988585
Epoch 997
Epoch 997 :: Batch 0/1
Batch Loss = 4.081318932196435
997, epoch_train_loss=4.081318932196435
Epoch 998
Epoch 998 :: Batch 0/1
Batch Loss = 4.080952068399741
998, epoch_train_loss=4.080952068399741
Epoch 999
Epoch 999 :: Batch 0/1
Batch Loss = 4.080878660571253
999, epoch_train_loss=4.080878660571253
Epoch 1000
Epoch 1000 :: Batch 0/1
Batch Loss = 4.080739114885338
1000, epoch_train_loss=4.080739114885338
Epoch 1001
Epoch 1001 :: Batch 0/1
Batch Loss = 4.080483333736028
1001, epoch_train_loss=4.080483333736028
Epoch 1002
Epoch 1002 :: Batch 0/1
Batch Loss = 4.080472409831881
1002, epoch_train_loss=4.080472409831881
Epoch 1003
Epoch 1003 :: Batch 0/1
Batch Loss = 4.080220989723355
1003, epoch_train_loss=4.080220989723355
Epoch 1004
Epoch 1004 :: Batch 0/1
Batch Loss = 4.080105784671127
1004, epoch_train_loss=4.080105784671127
Epoch 1005
Epoch 1005 :: Batch 0/1
Batch Loss = 4.079916922661253
1005, epoch_train_loss=4.079916922661253
Epoch 1006
Epoch 1006 :: Batch 0/1
Batch Loss = 4.079769180166748
1006, epoch_train_loss=4.079769180166748
Epoch 1007
Epoch 1007 :: Batch 0/1
Batch Loss = 4.079578415463485
1007, epoch_train_loss=4.079578415463485
Epoch 1008
Epoch 1008 :: Batch 0/1
Batch Loss = 4.079501394617883
1008, epoch_train_loss=4.079501394617883
Epoch 1009
Epoch 1009 :: Batch 0/1
Batch Loss = 4.0792577910635
1009, epoch_train_loss=4.0792577910635
Epoch 1010
Epoch 1010 :: Batch 0/1
Batch Loss = 4.079191377705493
1010, epoch_train_loss=4.079191377705493
Epoch 1011
Epoch 1011 :: Batch 0/1
Batch Loss = 4.079001599487612
1011, epoch_train_loss=4.079001599487612
Epoch 1012
Epoch 1012 :: Batch 0/1
Batch Loss = 4.078855900272155
1012, epoch_train_loss=4.078855900272155
Epoch 1013
Epoch 1013 :: Batch 0/1
Batch Loss = 4.078715845701481
1013, epoch_train_loss=4.078715845701481
Epoch 1014
Epoch 1014 :: Batch 0/1
Batch Loss = 4.078537300904008
1014, epoch_train_loss=4.078537300904008
Epoch 1015
Epoch 1015 :: Batch 0/1
Batch Loss = 4.078379783360747
1015, epoch_train_loss=4.078379783360747
Epoch 1016
Epoch 1016 :: Batch 0/1
Batch Loss = 4.078237849943671
1016, epoch_train_loss=4.078237849943671
Epoch 1017
Epoch 1017 :: Batch 0/1
Batch Loss = 4.078073255054051
1017, epoch_train_loss=4.078073255054051
Epoch 1018
Epoch 1018 :: Batch 0/1
Batch Loss = 4.077926816576318
1018, epoch_train_loss=4.077926816576318
Epoch 1019
Epoch 1019 :: Batch 0/1
Batch Loss = 4.0777991046987845
1019, epoch_train_loss=4.0777991046987845
Epoch 1020
Epoch 1020 :: Batch 0/1
Batch Loss = 4.077630410617049
1020, epoch_train_loss=4.077630410617049
Epoch 1021
Epoch 1021 :: Batch 0/1
Batch Loss = 4.077514082243207
1021, epoch_train_loss=4.077514082243207
Epoch 1022
Epoch 1022 :: Batch 0/1
Batch Loss = 4.077356595909238
1022, epoch_train_loss=4.077356595909238
Epoch 1023
Epoch 1023 :: Batch 0/1
Batch Loss = 4.077218820704001
1023, epoch_train_loss=4.077218820704001
Epoch 1024
Epoch 1024 :: Batch 0/1
Batch Loss = 4.077075421079286
1024, epoch_train_loss=4.077075421079286
Epoch 1025
Epoch 1025 :: Batch 0/1
Batch Loss = 4.076942978358928
1025, epoch_train_loss=4.076942978358928
Epoch 1026
Epoch 1026 :: Batch 0/1
Batch Loss = 4.07679097505033
1026, epoch_train_loss=4.07679097505033
Epoch 1027
Epoch 1027 :: Batch 0/1
Batch Loss = 4.076669215738474
1027, epoch_train_loss=4.076669215738474
Epoch 1028
Epoch 1028 :: Batch 0/1
Batch Loss = 4.076522925239632
1028, epoch_train_loss=4.076522925239632
Epoch 1029
Epoch 1029 :: Batch 0/1
Batch Loss = 4.07638688443732
1029, epoch_train_loss=4.07638688443732
Epoch 1030
Epoch 1030 :: Batch 0/1
Batch Loss = 4.076252511598104
1030, epoch_train_loss=4.076252511598104
Epoch 1031
Epoch 1031 :: Batch 0/1
Batch Loss = 4.076115300316526
1031, epoch_train_loss=4.076115300316526
Epoch 1032
Epoch 1032 :: Batch 0/1
Batch Loss = 4.075973627385629
1032, epoch_train_loss=4.075973627385629
Epoch 1033
Epoch 1033 :: Batch 0/1
Batch Loss = 4.075844166546484
1033, epoch_train_loss=4.075844166546484
Epoch 1034
Epoch 1034 :: Batch 0/1
Batch Loss = 4.075706154895245
1034, epoch_train_loss=4.075706154895245
Epoch 1035
Epoch 1035 :: Batch 0/1
Batch Loss = 4.075570605509192
1035, epoch_train_loss=4.075570605509192
Epoch 1036
Epoch 1036 :: Batch 0/1
Batch Loss = 4.075441243602555
1036, epoch_train_loss=4.075441243602555
Epoch 1037
Epoch 1037 :: Batch 0/1
Batch Loss = 4.075307170190287
1037, epoch_train_loss=4.075307170190287
Epoch 1038
Epoch 1038 :: Batch 0/1
Batch Loss = 4.0751737970472455
1038, epoch_train_loss=4.0751737970472455
Epoch 1039
Epoch 1039 :: Batch 0/1
Batch Loss = 4.075042560864589
1039, epoch_train_loss=4.075042560864589
Epoch 1040
Epoch 1040 :: Batch 0/1
Batch Loss = 4.074911059760542
1040, epoch_train_loss=4.074911059760542
Epoch 1041
Epoch 1041 :: Batch 0/1
Batch Loss = 4.074775457789397
1041, epoch_train_loss=4.074775457789397
Epoch 1042
Epoch 1042 :: Batch 0/1
Batch Loss = 4.07464620498783
1042, epoch_train_loss=4.07464620498783
Epoch 1043
Epoch 1043 :: Batch 0/1
Batch Loss = 4.074513142171622
1043, epoch_train_loss=4.074513142171622
Epoch 1044
Epoch 1044 :: Batch 0/1
Batch Loss = 4.0743806666347355
1044, epoch_train_loss=4.0743806666347355
Epoch 1045
Epoch 1045 :: Batch 0/1
Batch Loss = 4.074248815263611
1045, epoch_train_loss=4.074248815263611
Epoch 1046
Epoch 1046 :: Batch 0/1
Batch Loss = 4.074118453052215
1046, epoch_train_loss=4.074118453052215
Epoch 1047
Epoch 1047 :: Batch 0/1
Batch Loss = 4.073983935550468
1047, epoch_train_loss=4.073983935550468
Epoch 1048
Epoch 1048 :: Batch 0/1
Batch Loss = 4.073852601790932
1048, epoch_train_loss=4.073852601790932
Epoch 1049
Epoch 1049 :: Batch 0/1
Batch Loss = 4.073719756525626
1049, epoch_train_loss=4.073719756525626
Epoch 1050
Epoch 1050 :: Batch 0/1
Batch Loss = 4.0735863805489405
1050, epoch_train_loss=4.0735863805489405
Epoch 1051
Epoch 1051 :: Batch 0/1
Batch Loss = 4.073451922383786
1051, epoch_train_loss=4.073451922383786
Epoch 1052
Epoch 1052 :: Batch 0/1
Batch Loss = 4.073319340019398
1052, epoch_train_loss=4.073319340019398
Epoch 1053
Epoch 1053 :: Batch 0/1
Batch Loss = 4.073183660218202
1053, epoch_train_loss=4.073183660218202
Epoch 1054
Epoch 1054 :: Batch 0/1
Batch Loss = 4.073048464863982
1054, epoch_train_loss=4.073048464863982
Epoch 1055
Epoch 1055 :: Batch 0/1
Batch Loss = 4.0729123442660935
1055, epoch_train_loss=4.0729123442660935
Epoch 1056
Epoch 1056 :: Batch 0/1
Batch Loss = 4.072775805823648
1056, epoch_train_loss=4.072775805823648
Epoch 1057
Epoch 1057 :: Batch 0/1
Batch Loss = 4.0726370198162725
1057, epoch_train_loss=4.0726370198162725
Epoch 1058
Epoch 1058 :: Batch 0/1
Batch Loss = 4.07249842530185
1058, epoch_train_loss=4.07249842530185
Epoch 1059
Epoch 1059 :: Batch 0/1
Batch Loss = 4.072358307316339
1059, epoch_train_loss=4.072358307316339
Epoch 1060
Epoch 1060 :: Batch 0/1
Batch Loss = 4.0722168638345675
1060, epoch_train_loss=4.0722168638345675
Epoch 1061
Epoch 1061 :: Batch 0/1
Batch Loss = 4.072073580003265
1061, epoch_train_loss=4.072073580003265
Epoch 1062
Epoch 1062 :: Batch 0/1
Batch Loss = 4.071929566290736
1062, epoch_train_loss=4.071929566290736
Epoch 1063
Epoch 1063 :: Batch 0/1
Batch Loss = 4.07178311226414
1063, epoch_train_loss=4.07178311226414
Epoch 1064
Epoch 1064 :: Batch 0/1
Batch Loss = 4.071634828294456
1064, epoch_train_loss=4.071634828294456
Epoch 1065
Epoch 1065 :: Batch 0/1
Batch Loss = 4.07148442341893
1065, epoch_train_loss=4.07148442341893
Epoch 1066
Epoch 1066 :: Batch 0/1
Batch Loss = 4.071332392664558
1066, epoch_train_loss=4.071332392664558
Epoch 1067
Epoch 1067 :: Batch 0/1
Batch Loss = 4.071177772232805
1067, epoch_train_loss=4.071177772232805
Epoch 1068
Epoch 1068 :: Batch 0/1
Batch Loss = 4.071021502569084
1068, epoch_train_loss=4.071021502569084
Epoch 1069
Epoch 1069 :: Batch 0/1
Batch Loss = 4.070863671278404
1069, epoch_train_loss=4.070863671278404
Epoch 1070
Epoch 1070 :: Batch 0/1
Batch Loss = 4.070705143434798
1070, epoch_train_loss=4.070705143434798
Epoch 1071
Epoch 1071 :: Batch 0/1
Batch Loss = 4.070546228894465
1071, epoch_train_loss=4.070546228894465
Epoch 1072
Epoch 1072 :: Batch 0/1
Batch Loss = 4.070388784966967
1072, epoch_train_loss=4.070388784966967
Epoch 1073
Epoch 1073 :: Batch 0/1
Batch Loss = 4.070234326647382
1073, epoch_train_loss=4.070234326647382
Epoch 1074
Epoch 1074 :: Batch 0/1
Batch Loss = 4.070084948756895
1074, epoch_train_loss=4.070084948756895
Epoch 1075
Epoch 1075 :: Batch 0/1
Batch Loss = 4.069942213938164
1075, epoch_train_loss=4.069942213938164
Epoch 1076
Epoch 1076 :: Batch 0/1
Batch Loss = 4.069807110727599
1076, epoch_train_loss=4.069807110727599
Epoch 1077
Epoch 1077 :: Batch 0/1
Batch Loss = 4.069677388552876
1077, epoch_train_loss=4.069677388552876
Epoch 1078
Epoch 1078 :: Batch 0/1
Batch Loss = 4.069547981281612
1078, epoch_train_loss=4.069547981281612
Epoch 1079
Epoch 1079 :: Batch 0/1
Batch Loss = 4.069413626733804
1079, epoch_train_loss=4.069413626733804
Epoch 1080
Epoch 1080 :: Batch 0/1
Batch Loss = 4.069273932862351
1080, epoch_train_loss=4.069273932862351
Epoch 1081
Epoch 1081 :: Batch 0/1
Batch Loss = 4.069132317517631
1081, epoch_train_loss=4.069132317517631
Epoch 1082
Epoch 1082 :: Batch 0/1
Batch Loss = 4.0689919701385255
1082, epoch_train_loss=4.0689919701385255
Epoch 1083
Epoch 1083 :: Batch 0/1
Batch Loss = 4.068852560349902
1083, epoch_train_loss=4.068852560349902
Epoch 1084
Epoch 1084 :: Batch 0/1
Batch Loss = 4.068711699375647
1084, epoch_train_loss=4.068711699375647
Epoch 1085
Epoch 1085 :: Batch 0/1
Batch Loss = 4.0685670570321415
1085, epoch_train_loss=4.0685670570321415
Epoch 1086
Epoch 1086 :: Batch 0/1
Batch Loss = 4.068418357085593
1086, epoch_train_loss=4.068418357085593
Epoch 1087
Epoch 1087 :: Batch 0/1
Batch Loss = 4.068267319388012
1087, epoch_train_loss=4.068267319388012
Epoch 1088
Epoch 1088 :: Batch 0/1
Batch Loss = 4.068116933901008
1088, epoch_train_loss=4.068116933901008
Epoch 1089
Epoch 1089 :: Batch 0/1
Batch Loss = 4.067969788987206
1089, epoch_train_loss=4.067969788987206
Epoch 1090
Epoch 1090 :: Batch 0/1
Batch Loss = 4.067827105054889
1090, epoch_train_loss=4.067827105054889
Epoch 1091
Epoch 1091 :: Batch 0/1
Batch Loss = 4.067688452156419
1091, epoch_train_loss=4.067688452156419
Epoch 1092
Epoch 1092 :: Batch 0/1
Batch Loss = 4.067552477100447
1092, epoch_train_loss=4.067552477100447
Epoch 1093
Epoch 1093 :: Batch 0/1
Batch Loss = 4.06741792596182
1093, epoch_train_loss=4.06741792596182
Epoch 1094
Epoch 1094 :: Batch 0/1
Batch Loss = 4.067284257786342
1094, epoch_train_loss=4.067284257786342
Epoch 1095
Epoch 1095 :: Batch 0/1
Batch Loss = 4.067151655744573
1095, epoch_train_loss=4.067151655744573
Epoch 1096
Epoch 1096 :: Batch 0/1
Batch Loss = 4.06702053750884
1096, epoch_train_loss=4.06702053750884
Epoch 1097
Epoch 1097 :: Batch 0/1
Batch Loss = 4.066891147424277
1097, epoch_train_loss=4.066891147424277
Epoch 1098
Epoch 1098 :: Batch 0/1
Batch Loss = 4.06676331719863
1098, epoch_train_loss=4.06676331719863
Epoch 1099
Epoch 1099 :: Batch 0/1
Batch Loss = 4.066636576573189
1099, epoch_train_loss=4.066636576573189
Epoch 1100
Epoch 1100 :: Batch 0/1
Batch Loss = 4.066510314674417
1100, epoch_train_loss=4.066510314674417
Epoch 1101
Epoch 1101 :: Batch 0/1
Batch Loss = 4.066384074827382
1101, epoch_train_loss=4.066384074827382
Epoch 1102
Epoch 1102 :: Batch 0/1
Batch Loss = 4.0662576676808895
1102, epoch_train_loss=4.0662576676808895
Epoch 1103
Epoch 1103 :: Batch 0/1
Batch Loss = 4.066131250821739
1103, epoch_train_loss=4.066131250821739
Epoch 1104
Epoch 1104 :: Batch 0/1
Batch Loss = 4.066005164015304
1104, epoch_train_loss=4.066005164015304
Epoch 1105
Epoch 1105 :: Batch 0/1
Batch Loss = 4.065879762206005
1105, epoch_train_loss=4.065879762206005
Epoch 1106
Epoch 1106 :: Batch 0/1
Batch Loss = 4.0657551674996055
1106, epoch_train_loss=4.0657551674996055
Epoch 1107
Epoch 1107 :: Batch 0/1
Batch Loss = 4.0656312130010175
1107, epoch_train_loss=4.0656312130010175
Epoch 1108
Epoch 1108 :: Batch 0/1
Batch Loss = 4.065507493350032
1108, epoch_train_loss=4.065507493350032
Epoch 1109
Epoch 1109 :: Batch 0/1
Batch Loss = 4.065383615993949
1109, epoch_train_loss=4.065383615993949
Epoch 1110
Epoch 1110 :: Batch 0/1
Batch Loss = 4.065259372952187
1110, epoch_train_loss=4.065259372952187
Epoch 1111
Epoch 1111 :: Batch 0/1
Batch Loss = 4.065134797086872
1111, epoch_train_loss=4.065134797086872
Epoch 1112
Epoch 1112 :: Batch 0/1
Batch Loss = 4.065010005018453
1112, epoch_train_loss=4.065010005018453
Epoch 1113
Epoch 1113 :: Batch 0/1
Batch Loss = 4.06488504419138
1113, epoch_train_loss=4.06488504419138
Epoch 1114
Epoch 1114 :: Batch 0/1
Batch Loss = 4.06475982318025
1114, epoch_train_loss=4.06475982318025
Epoch 1115
Epoch 1115 :: Batch 0/1
Batch Loss = 4.064634172967154
1115, epoch_train_loss=4.064634172967154
Epoch 1116
Epoch 1116 :: Batch 0/1
Batch Loss = 4.064507943099834
1116, epoch_train_loss=4.064507943099834
Epoch 1117
Epoch 1117 :: Batch 0/1
Batch Loss = 4.064381047929522
1117, epoch_train_loss=4.064381047929522
Epoch 1118
Epoch 1118 :: Batch 0/1
Batch Loss = 4.0642534603461105
1118, epoch_train_loss=4.0642534603461105
Epoch 1119
Epoch 1119 :: Batch 0/1
Batch Loss = 4.064125167167792
1119, epoch_train_loss=4.064125167167792
Epoch 1120
Epoch 1120 :: Batch 0/1
Batch Loss = 4.063996155076622
1120, epoch_train_loss=4.063996155076622
Epoch 1121
Epoch 1121 :: Batch 0/1
Batch Loss = 4.063866403906206
1121, epoch_train_loss=4.063866403906206
Epoch 1122
Epoch 1122 :: Batch 0/1
Batch Loss = 4.063735888755257
1122, epoch_train_loss=4.063735888755257
Epoch 1123
Epoch 1123 :: Batch 0/1
Batch Loss = 4.0636045561127
1123, epoch_train_loss=4.0636045561127
Epoch 1124
Epoch 1124 :: Batch 0/1
Batch Loss = 4.063472319646809
1124, epoch_train_loss=4.063472319646809
Epoch 1125
Epoch 1125 :: Batch 0/1
Batch Loss = 4.063339081557765
1125, epoch_train_loss=4.063339081557765
Epoch 1126
Epoch 1126 :: Batch 0/1
Batch Loss = 4.0632047881932785
1126, epoch_train_loss=4.0632047881932785
Epoch 1127
Epoch 1127 :: Batch 0/1
Batch Loss = 4.063069454330665
1127, epoch_train_loss=4.063069454330665
Epoch 1128
Epoch 1128 :: Batch 0/1
Batch Loss = 4.062933143516141
1128, epoch_train_loss=4.062933143516141
Epoch 1129
Epoch 1129 :: Batch 0/1
Batch Loss = 4.062795908510171
1129, epoch_train_loss=4.062795908510171
Epoch 1130
Epoch 1130 :: Batch 0/1
Batch Loss = 4.062657759090405
1130, epoch_train_loss=4.062657759090405
Epoch 1131
Epoch 1131 :: Batch 0/1
Batch Loss = 4.062518674607608
1131, epoch_train_loss=4.062518674607608
Epoch 1132
Epoch 1132 :: Batch 0/1
Batch Loss = 4.062378648754811
1132, epoch_train_loss=4.062378648754811
Epoch 1133
Epoch 1133 :: Batch 0/1
Batch Loss = 4.062237716183299
1133, epoch_train_loss=4.062237716183299
Epoch 1134
Epoch 1134 :: Batch 0/1
Batch Loss = 4.0620959404295105
1134, epoch_train_loss=4.0620959404295105
Epoch 1135
Epoch 1135 :: Batch 0/1
Batch Loss = 4.061953381650183
1135, epoch_train_loss=4.061953381650183
Epoch 1136
Epoch 1136 :: Batch 0/1
Batch Loss = 4.061810074035201
1136, epoch_train_loss=4.061810074035201
Epoch 1137
Epoch 1137 :: Batch 0/1
Batch Loss = 4.061666031091342
1137, epoch_train_loss=4.061666031091342
Epoch 1138
Epoch 1138 :: Batch 0/1
Batch Loss = 4.061521260842729
1138, epoch_train_loss=4.061521260842729
Epoch 1139
Epoch 1139 :: Batch 0/1
Batch Loss = 4.061375777402265
1139, epoch_train_loss=4.061375777402265
Epoch 1140
Epoch 1140 :: Batch 0/1
Batch Loss = 4.061229597040255
1140, epoch_train_loss=4.061229597040255
Epoch 1141
Epoch 1141 :: Batch 0/1
Batch Loss = 4.061082733131836
1141, epoch_train_loss=4.061082733131836
Epoch 1142
Epoch 1142 :: Batch 0/1
Batch Loss = 4.060935193782675
1142, epoch_train_loss=4.060935193782675
Epoch 1143
Epoch 1143 :: Batch 0/1
Batch Loss = 4.060786985869835
1143, epoch_train_loss=4.060786985869835
Epoch 1144
Epoch 1144 :: Batch 0/1
Batch Loss = 4.060638115462969
1144, epoch_train_loss=4.060638115462969
Epoch 1145
Epoch 1145 :: Batch 0/1
Batch Loss = 4.060488584577516
1145, epoch_train_loss=4.060488584577516
Epoch 1146
Epoch 1146 :: Batch 0/1
Batch Loss = 4.060338386969741
1146, epoch_train_loss=4.060338386969741
Epoch 1147
Epoch 1147 :: Batch 0/1
Batch Loss = 4.060187508454785
1147, epoch_train_loss=4.060187508454785
Epoch 1148
Epoch 1148 :: Batch 0/1
Batch Loss = 4.060035932369246
1148, epoch_train_loss=4.060035932369246
Epoch 1149
Epoch 1149 :: Batch 0/1
Batch Loss = 4.059883643541494
1149, epoch_train_loss=4.059883643541494
Epoch 1150
Epoch 1150 :: Batch 0/1
Batch Loss = 4.059730628204674
1150, epoch_train_loss=4.059730628204674
Epoch 1151
Epoch 1151 :: Batch 0/1
Batch Loss = 4.059576868572904
1151, epoch_train_loss=4.059576868572904
Epoch 1152
Epoch 1152 :: Batch 0/1
Batch Loss = 4.059422339452617
1152, epoch_train_loss=4.059422339452617
Epoch 1153
Epoch 1153 :: Batch 0/1
Batch Loss = 4.05926700829169
1153, epoch_train_loss=4.05926700829169
Epoch 1154
Epoch 1154 :: Batch 0/1
Batch Loss = 4.059110841126484
1154, epoch_train_loss=4.059110841126484
Epoch 1155
Epoch 1155 :: Batch 0/1
Batch Loss = 4.058953807092244
1155, epoch_train_loss=4.058953807092244
Epoch 1156
Epoch 1156 :: Batch 0/1
Batch Loss = 4.058795883125132
1156, epoch_train_loss=4.058795883125132
Epoch 1157
Epoch 1157 :: Batch 0/1
Batch Loss = 4.058637053667611
1157, epoch_train_loss=4.058637053667611
Epoch 1158
Epoch 1158 :: Batch 0/1
Batch Loss = 4.05847731817412
1158, epoch_train_loss=4.05847731817412
Epoch 1159
Epoch 1159 :: Batch 0/1
Batch Loss = 4.05831670003877
1159, epoch_train_loss=4.05831670003877
Epoch 1160
Epoch 1160 :: Batch 0/1
Batch Loss = 4.058155277381123
1160, epoch_train_loss=4.058155277381123
Epoch 1161
Epoch 1161 :: Batch 0/1
Batch Loss = 4.05799324752187
1161, epoch_train_loss=4.05799324752187
Epoch 1162
Epoch 1162 :: Batch 0/1
Batch Loss = 4.057831007583132
1162, epoch_train_loss=4.057831007583132
Epoch 1163
Epoch 1163 :: Batch 0/1
Batch Loss = 4.057669634077186
1163, epoch_train_loss=4.057669634077186
Epoch 1164
Epoch 1164 :: Batch 0/1
Batch Loss = 4.057510909417268
1164, epoch_train_loss=4.057510909417268
Epoch 1165
Epoch 1165 :: Batch 0/1
Batch Loss = 4.0573614082162575
1165, epoch_train_loss=4.0573614082162575
Epoch 1166
Epoch 1166 :: Batch 0/1
Batch Loss = 4.057228573336929
1166, epoch_train_loss=4.057228573336929
Epoch 1167
Epoch 1167 :: Batch 0/1
Batch Loss = 4.057160022500978
1167, epoch_train_loss=4.057160022500978
Epoch 1168
Epoch 1168 :: Batch 0/1
Batch Loss = 4.057162456026078
1168, epoch_train_loss=4.057162456026078
Epoch 1169
Epoch 1169 :: Batch 0/1
Batch Loss = 4.057620688843088
1169, epoch_train_loss=4.057620688843088
Epoch 1170
Epoch 1170 :: Batch 0/1
Batch Loss = 4.057912669028649
1170, epoch_train_loss=4.057912669028649
Epoch 1171
Epoch 1171 :: Batch 0/1
Batch Loss = 4.060536439238305
1171, epoch_train_loss=4.060536439238305
Epoch 1172
Epoch 1172 :: Batch 0/1
Batch Loss = 4.057809186484474
1172, epoch_train_loss=4.057809186484474
Epoch 1173
Epoch 1173 :: Batch 0/1
Batch Loss = 4.057546263380526
1173, epoch_train_loss=4.057546263380526
Epoch 1174
Epoch 1174 :: Batch 0/1
Batch Loss = 4.0566218790304145
1174, epoch_train_loss=4.0566218790304145
Epoch 1175
Epoch 1175 :: Batch 0/1
Batch Loss = 4.056696426823624
1175, epoch_train_loss=4.056696426823624
Epoch 1176
Epoch 1176 :: Batch 0/1
Batch Loss = 4.056253210519767
1176, epoch_train_loss=4.056253210519767
Epoch 1177
Epoch 1177 :: Batch 0/1
Batch Loss = 4.056539299428762
1177, epoch_train_loss=4.056539299428762
Epoch 1178
Epoch 1178 :: Batch 0/1
Batch Loss = 4.056426591734651
1178, epoch_train_loss=4.056426591734651
Epoch 1179
Epoch 1179 :: Batch 0/1
Batch Loss = 4.058152275716061
1179, epoch_train_loss=4.058152275716061
Epoch 1180
Epoch 1180 :: Batch 0/1
Batch Loss = 4.056500522632538
1180, epoch_train_loss=4.056500522632538
Epoch 1181
Epoch 1181 :: Batch 0/1
Batch Loss = 4.056295871373468
1181, epoch_train_loss=4.056295871373468
Epoch 1182
Epoch 1182 :: Batch 0/1
Batch Loss = 4.056052543214226
1182, epoch_train_loss=4.056052543214226
Epoch 1183
Epoch 1183 :: Batch 0/1
Batch Loss = 4.057110394944314
1183, epoch_train_loss=4.057110394944314
Epoch 1184
Epoch 1184 :: Batch 0/1
Batch Loss = 4.056703627367309
1184, epoch_train_loss=4.056703627367309
Epoch 1185
Epoch 1185 :: Batch 0/1
Batch Loss = 4.0562288085509675
1185, epoch_train_loss=4.0562288085509675
Epoch 1186
Epoch 1186 :: Batch 0/1
Batch Loss = 4.057209136762443
1186, epoch_train_loss=4.057209136762443
Epoch 1187
Epoch 1187 :: Batch 0/1
Batch Loss = 4.058382631078426
1187, epoch_train_loss=4.058382631078426
Epoch 1188
Epoch 1188 :: Batch 0/1
Batch Loss = 4.06116582928926
1188, epoch_train_loss=4.06116582928926
Epoch 1189
Epoch 1189 :: Batch 0/1
Batch Loss = 4.059105437504075
1189, epoch_train_loss=4.059105437504075
Epoch 1190
Epoch 1190 :: Batch 0/1
Batch Loss = 4.0721927699659926
1190, epoch_train_loss=4.0721927699659926
Epoch 1191
Epoch 1191 :: Batch 0/1
Batch Loss = 4.080822421162153
1191, epoch_train_loss=4.080822421162153
Epoch 1192
Epoch 1192 :: Batch 0/1
Batch Loss = 4.082166674244192
1192, epoch_train_loss=4.082166674244192
Epoch 1193
Epoch 1193 :: Batch 0/1
Batch Loss = 4.101736665003634
1193, epoch_train_loss=4.101736665003634
Epoch 1194
Epoch 1194 :: Batch 0/1
Batch Loss = 4.101374313498787
1194, epoch_train_loss=4.101374313498787
Epoch 1195
Epoch 1195 :: Batch 0/1
Batch Loss = 4.110026691059836
1195, epoch_train_loss=4.110026691059836
Epoch 1196
Epoch 1196 :: Batch 0/1
Batch Loss = 4.117971321767548
1196, epoch_train_loss=4.117971321767548
Epoch 1197
Epoch 1197 :: Batch 0/1
Batch Loss = 4.112993765323327
1197, epoch_train_loss=4.112993765323327
Epoch 1198
Epoch 1198 :: Batch 0/1
Batch Loss = 4.110087640982428
1198, epoch_train_loss=4.110087640982428
Epoch 1199
Epoch 1199 :: Batch 0/1
Batch Loss = 4.102334766100312
1199, epoch_train_loss=4.102334766100312
Epoch 1200
Epoch 1200 :: Batch 0/1
Batch Loss = 4.100160349920237
1200, epoch_train_loss=4.100160349920237
Epoch 1201
Epoch 1201 :: Batch 0/1
Batch Loss = 4.095839361981853
1201, epoch_train_loss=4.095839361981853
Epoch 1202
Epoch 1202 :: Batch 0/1
Batch Loss = 4.106235276286815
1202, epoch_train_loss=4.106235276286815
Epoch 1203
Epoch 1203 :: Batch 0/1
Batch Loss = 4.100904299631893
1203, epoch_train_loss=4.100904299631893
Epoch 1204
Epoch 1204 :: Batch 0/1
Batch Loss = 4.097923459906157
1204, epoch_train_loss=4.097923459906157
Epoch 1205
Epoch 1205 :: Batch 0/1
Batch Loss = 4.089776082510519
1205, epoch_train_loss=4.089776082510519
Epoch 1206
Epoch 1206 :: Batch 0/1
Batch Loss = 4.099333751757333
1206, epoch_train_loss=4.099333751757333
Epoch 1207
Epoch 1207 :: Batch 0/1
Batch Loss = 4.095270436833147
1207, epoch_train_loss=4.095270436833147
Epoch 1208
Epoch 1208 :: Batch 0/1
Batch Loss = 4.097170982600664
1208, epoch_train_loss=4.097170982600664
Epoch 1209
Epoch 1209 :: Batch 0/1
Batch Loss = 4.085592273300855
1209, epoch_train_loss=4.085592273300855
Epoch 1210
Epoch 1210 :: Batch 0/1
Batch Loss = 4.100080996747243
1210, epoch_train_loss=4.100080996747243
Epoch 1211
Epoch 1211 :: Batch 0/1
Batch Loss = 4.090554308921475
1211, epoch_train_loss=4.090554308921475
Epoch 1212
Epoch 1212 :: Batch 0/1
Batch Loss = 4.101632642287935
1212, epoch_train_loss=4.101632642287935
Epoch 1213
Epoch 1213 :: Batch 0/1
Batch Loss = 4.103766967695391
1213, epoch_train_loss=4.103766967695391
Epoch 1214
Epoch 1214 :: Batch 0/1
Batch Loss = 4.096988234692217
1214, epoch_train_loss=4.096988234692217
Epoch 1215
Epoch 1215 :: Batch 0/1
Batch Loss = 4.083259392831994
1215, epoch_train_loss=4.083259392831994
Epoch 1216
Epoch 1216 :: Batch 0/1
Batch Loss = 4.084347540677775
1216, epoch_train_loss=4.084347540677775
Epoch 1217
Epoch 1217 :: Batch 0/1
Batch Loss = 4.0707914227294415
1217, epoch_train_loss=4.0707914227294415
Epoch 1218
Epoch 1218 :: Batch 0/1
Batch Loss = 4.083936333551042
1218, epoch_train_loss=4.083936333551042
Epoch 1219
Epoch 1219 :: Batch 0/1
Batch Loss = 4.078370211860224
1219, epoch_train_loss=4.078370211860224
Epoch 1220
Epoch 1220 :: Batch 0/1
Batch Loss = 4.078301057624068
1220, epoch_train_loss=4.078301057624068
Epoch 1221
Epoch 1221 :: Batch 0/1
Batch Loss = 4.083104744796982
1221, epoch_train_loss=4.083104744796982
Epoch 1222
Epoch 1222 :: Batch 0/1
Batch Loss = 4.062471583988977
1222, epoch_train_loss=4.062471583988977
Epoch 1223
Epoch 1223 :: Batch 0/1
Batch Loss = 4.065231691578827
1223, epoch_train_loss=4.065231691578827
Epoch 1224
Epoch 1224 :: Batch 0/1
Batch Loss = 4.063435676258804
1224, epoch_train_loss=4.063435676258804
Epoch 1225
Epoch 1225 :: Batch 0/1
Batch Loss = 4.066251266915814
1225, epoch_train_loss=4.066251266915814
Epoch 1226
Epoch 1226 :: Batch 0/1
Batch Loss = 4.061061852743673
1226, epoch_train_loss=4.061061852743673
Epoch 1227
Epoch 1227 :: Batch 0/1
Batch Loss = 4.064031854040452
1227, epoch_train_loss=4.064031854040452
Epoch 1228
Epoch 1228 :: Batch 0/1
Batch Loss = 4.0636161464350105
1228, epoch_train_loss=4.0636161464350105
Epoch 1229
Epoch 1229 :: Batch 0/1
Batch Loss = 4.072192780125245
1229, epoch_train_loss=4.072192780125245
Epoch 1230
Epoch 1230 :: Batch 0/1
Batch Loss = 4.065681590667283
1230, epoch_train_loss=4.065681590667283
Epoch 1231
Epoch 1231 :: Batch 0/1
Batch Loss = 4.062746979291496
1231, epoch_train_loss=4.062746979291496
Epoch 1232
Epoch 1232 :: Batch 0/1
Batch Loss = 4.053791690652766
1232, epoch_train_loss=4.053791690652766
Epoch 1233
Epoch 1233 :: Batch 0/1
Batch Loss = 4.062160048803954
1233, epoch_train_loss=4.062160048803954
Epoch 1234
Epoch 1234 :: Batch 0/1
Batch Loss = 4.063266534110748
1234, epoch_train_loss=4.063266534110748
Epoch 1235
Epoch 1235 :: Batch 0/1
Batch Loss = 4.063566525514261
1235, epoch_train_loss=4.063566525514261
Epoch 1236
Epoch 1236 :: Batch 0/1
Batch Loss = 4.052680412428316
1236, epoch_train_loss=4.052680412428316
Epoch 1237
Epoch 1237 :: Batch 0/1
Batch Loss = 4.0663166502409736
1237, epoch_train_loss=4.0663166502409736
Epoch 1238
Epoch 1238 :: Batch 0/1
Batch Loss = 4.071131636954798
1238, epoch_train_loss=4.071131636954798
Epoch 1239
Epoch 1239 :: Batch 0/1
Batch Loss = 4.082800883435272
1239, epoch_train_loss=4.082800883435272
Epoch 1240
Epoch 1240 :: Batch 0/1
Batch Loss = 4.07422081211251
1240, epoch_train_loss=4.07422081211251
Epoch 1241
Epoch 1241 :: Batch 0/1
Batch Loss = 4.0673842358365295
1241, epoch_train_loss=4.0673842358365295
Epoch 1242
Epoch 1242 :: Batch 0/1
Batch Loss = 4.08402777469363
1242, epoch_train_loss=4.08402777469363
Epoch 1243
Epoch 1243 :: Batch 0/1
Batch Loss = 4.077116204550269
1243, epoch_train_loss=4.077116204550269
Epoch 1244
Epoch 1244 :: Batch 0/1
Batch Loss = 4.092826037106396
1244, epoch_train_loss=4.092826037106396
Epoch 1245
Epoch 1245 :: Batch 0/1
Batch Loss = 4.0959660329769765
1245, epoch_train_loss=4.0959660329769765
Epoch 1246
Epoch 1246 :: Batch 0/1
Batch Loss = 4.097855719590716
1246, epoch_train_loss=4.097855719590716
Epoch 1247
Epoch 1247 :: Batch 0/1
Batch Loss = 4.085415145325983
1247, epoch_train_loss=4.085415145325983
Epoch 1248
Epoch 1248 :: Batch 0/1
Batch Loss = 4.091302051826744
1248, epoch_train_loss=4.091302051826744
Epoch 1249
Epoch 1249 :: Batch 0/1
Batch Loss = 4.0847309417200846
1249, epoch_train_loss=4.0847309417200846
Epoch 1250
Epoch 1250 :: Batch 0/1
Batch Loss = 4.086488422715927
1250, epoch_train_loss=4.086488422715927
Epoch 1251
Epoch 1251 :: Batch 0/1
Batch Loss = 4.085891448995302
1251, epoch_train_loss=4.085891448995302
Epoch 1252
Epoch 1252 :: Batch 0/1
Batch Loss = 4.0774881186453955
1252, epoch_train_loss=4.0774881186453955
Epoch 1253
Epoch 1253 :: Batch 0/1
Batch Loss = 4.075906153088955
1253, epoch_train_loss=4.075906153088955
Epoch 1254
Epoch 1254 :: Batch 0/1
Batch Loss = 4.07612667006424
1254, epoch_train_loss=4.07612667006424
Epoch 1255
Epoch 1255 :: Batch 0/1
Batch Loss = 4.070516295255909
1255, epoch_train_loss=4.070516295255909
Epoch 1256
Epoch 1256 :: Batch 0/1
Batch Loss = 4.069419275820107
1256, epoch_train_loss=4.069419275820107
Epoch 1257
Epoch 1257 :: Batch 0/1
Batch Loss = 4.056991574570543
1257, epoch_train_loss=4.056991574570543
Epoch 1258
Epoch 1258 :: Batch 0/1
Batch Loss = 4.0834743280990375
1258, epoch_train_loss=4.0834743280990375
Epoch 1259
Epoch 1259 :: Batch 0/1
Batch Loss = 4.06755564123797
1259, epoch_train_loss=4.06755564123797
Epoch 1260
Epoch 1260 :: Batch 0/1
Batch Loss = 4.0806017193324795
1260, epoch_train_loss=4.0806017193324795
Epoch 1261
Epoch 1261 :: Batch 0/1
Batch Loss = 4.082991399688981
1261, epoch_train_loss=4.082991399688981
Epoch 1262
Epoch 1262 :: Batch 0/1
Batch Loss = 4.088244606421659
1262, epoch_train_loss=4.088244606421659
Epoch 1263
Epoch 1263 :: Batch 0/1
Batch Loss = 4.09090344417302
1263, epoch_train_loss=4.09090344417302
Epoch 1264
Epoch 1264 :: Batch 0/1
Batch Loss = 4.086409120554056
1264, epoch_train_loss=4.086409120554056
Epoch 1265
Epoch 1265 :: Batch 0/1
Batch Loss = 4.0835304554867555
1265, epoch_train_loss=4.0835304554867555
Epoch 1266
Epoch 1266 :: Batch 0/1
Batch Loss = 4.0786781542345665
1266, epoch_train_loss=4.0786781542345665
Epoch 1267
Epoch 1267 :: Batch 0/1
Batch Loss = 4.071034005101487
1267, epoch_train_loss=4.071034005101487
Epoch 1268
Epoch 1268 :: Batch 0/1
Batch Loss = 4.067635652823367
1268, epoch_train_loss=4.067635652823367
Epoch 1269
Epoch 1269 :: Batch 0/1
Batch Loss = 4.068904731252062
1269, epoch_train_loss=4.068904731252062
Epoch 1270
Epoch 1270 :: Batch 0/1
Batch Loss = 4.056919484585916
1270, epoch_train_loss=4.056919484585916
Epoch 1271
Epoch 1271 :: Batch 0/1
Batch Loss = 4.061350325203523
1271, epoch_train_loss=4.061350325203523
Epoch 1272
Epoch 1272 :: Batch 0/1
Batch Loss = 4.060486264483143
1272, epoch_train_loss=4.060486264483143
Epoch 1273
Epoch 1273 :: Batch 0/1
Batch Loss = 4.058095818228204
1273, epoch_train_loss=4.058095818228204
Epoch 1274
Epoch 1274 :: Batch 0/1
Batch Loss = 4.058040870458393
1274, epoch_train_loss=4.058040870458393
Epoch 1275
Epoch 1275 :: Batch 0/1
Batch Loss = 4.052817014083633
1275, epoch_train_loss=4.052817014083633
Epoch 1276
Epoch 1276 :: Batch 0/1
Batch Loss = 4.051407836266261
1276, epoch_train_loss=4.051407836266261
Epoch 1277
Epoch 1277 :: Batch 0/1
Batch Loss = 4.056112718123475
1277, epoch_train_loss=4.056112718123475
Epoch 1278
Epoch 1278 :: Batch 0/1
Batch Loss = 4.052883539961821
1278, epoch_train_loss=4.052883539961821
Epoch 1279
Epoch 1279 :: Batch 0/1
Batch Loss = 4.051769753944481
1279, epoch_train_loss=4.051769753944481
Epoch 1280
Epoch 1280 :: Batch 0/1
Batch Loss = 4.050318785945734
1280, epoch_train_loss=4.050318785945734
Epoch 1281
Epoch 1281 :: Batch 0/1
Batch Loss = 4.046013080250595
1281, epoch_train_loss=4.046013080250595
Epoch 1282
Epoch 1282 :: Batch 0/1
Batch Loss = 4.044990046250402
1282, epoch_train_loss=4.044990046250402
Epoch 1283
Epoch 1283 :: Batch 0/1
Batch Loss = 4.046317519814881
1283, epoch_train_loss=4.046317519814881
Epoch 1284
Epoch 1284 :: Batch 0/1
Batch Loss = 4.0448306634380105
1284, epoch_train_loss=4.0448306634380105
Epoch 1285
Epoch 1285 :: Batch 0/1
Batch Loss = 4.043377885379924
1285, epoch_train_loss=4.043377885379924
Epoch 1286
Epoch 1286 :: Batch 0/1
Batch Loss = 4.046248356604461
1286, epoch_train_loss=4.046248356604461
Epoch 1287
Epoch 1287 :: Batch 0/1
Batch Loss = 4.042926182374452
1287, epoch_train_loss=4.042926182374452
Epoch 1288
Epoch 1288 :: Batch 0/1
Batch Loss = 4.042746045311599
1288, epoch_train_loss=4.042746045311599
Epoch 1289
Epoch 1289 :: Batch 0/1
Batch Loss = 4.042015824277293
1289, epoch_train_loss=4.042015824277293
Epoch 1290
Epoch 1290 :: Batch 0/1
Batch Loss = 4.039927111751127
1290, epoch_train_loss=4.039927111751127
Epoch 1291
Epoch 1291 :: Batch 0/1
Batch Loss = 4.040266938962408
1291, epoch_train_loss=4.040266938962408
Epoch 1292
Epoch 1292 :: Batch 0/1
Batch Loss = 4.040211472858915
1292, epoch_train_loss=4.040211472858915
Epoch 1293
Epoch 1293 :: Batch 0/1
Batch Loss = 4.040443876283514
1293, epoch_train_loss=4.040443876283514
Epoch 1294
Epoch 1294 :: Batch 0/1
Batch Loss = 4.038936029594549
1294, epoch_train_loss=4.038936029594549
Epoch 1295
Epoch 1295 :: Batch 0/1
Batch Loss = 4.039713052483404
1295, epoch_train_loss=4.039713052483404
Epoch 1296
Epoch 1296 :: Batch 0/1
Batch Loss = 4.038075331009618
1296, epoch_train_loss=4.038075331009618
Epoch 1297
Epoch 1297 :: Batch 0/1
Batch Loss = 4.037824491192474
1297, epoch_train_loss=4.037824491192474
Epoch 1298
Epoch 1298 :: Batch 0/1
Batch Loss = 4.03707816126988
1298, epoch_train_loss=4.03707816126988
Epoch 1299
Epoch 1299 :: Batch 0/1
Batch Loss = 4.037532985277004
1299, epoch_train_loss=4.037532985277004
Epoch 1300
Epoch 1300 :: Batch 0/1
Batch Loss = 4.036622241078421
1300, epoch_train_loss=4.036622241078421
Epoch 1301
Epoch 1301 :: Batch 0/1
Batch Loss = 4.037068398632383
1301, epoch_train_loss=4.037068398632383
Epoch 1302
Epoch 1302 :: Batch 0/1
Batch Loss = 4.036316281522512
1302, epoch_train_loss=4.036316281522512
Epoch 1303
Epoch 1303 :: Batch 0/1
Batch Loss = 4.03590194386484
1303, epoch_train_loss=4.03590194386484
Epoch 1304
Epoch 1304 :: Batch 0/1
Batch Loss = 4.0353381588540715
1304, epoch_train_loss=4.0353381588540715
Epoch 1305
Epoch 1305 :: Batch 0/1
Batch Loss = 4.03524659007045
1305, epoch_train_loss=4.03524659007045
Epoch 1306
Epoch 1306 :: Batch 0/1
Batch Loss = 4.034865182920258
1306, epoch_train_loss=4.034865182920258
Epoch 1307
Epoch 1307 :: Batch 0/1
Batch Loss = 4.034660729809039
1307, epoch_train_loss=4.034660729809039
Epoch 1308
Epoch 1308 :: Batch 0/1
Batch Loss = 4.034454036935936
1308, epoch_train_loss=4.034454036935936
Epoch 1309
Epoch 1309 :: Batch 0/1
Batch Loss = 4.034153194241891
1309, epoch_train_loss=4.034153194241891
Epoch 1310
Epoch 1310 :: Batch 0/1
Batch Loss = 4.033700966079137
1310, epoch_train_loss=4.033700966079137
Epoch 1311
Epoch 1311 :: Batch 0/1
Batch Loss = 4.0334477946519804
1311, epoch_train_loss=4.0334477946519804
Epoch 1312
Epoch 1312 :: Batch 0/1
Batch Loss = 4.033113197671912
1312, epoch_train_loss=4.033113197671912
Epoch 1313
Epoch 1313 :: Batch 0/1
Batch Loss = 4.032850005432742
1313, epoch_train_loss=4.032850005432742
Epoch 1314
Epoch 1314 :: Batch 0/1
Batch Loss = 4.032698349079433
1314, epoch_train_loss=4.032698349079433
Epoch 1315
Epoch 1315 :: Batch 0/1
Batch Loss = 4.03240941682418
1315, epoch_train_loss=4.03240941682418
Epoch 1316
Epoch 1316 :: Batch 0/1
Batch Loss = 4.032102333098722
1316, epoch_train_loss=4.032102333098722
Epoch 1317
Epoch 1317 :: Batch 0/1
Batch Loss = 4.031787243133026
1317, epoch_train_loss=4.031787243133026
Epoch 1318
Epoch 1318 :: Batch 0/1
Batch Loss = 4.0314990202227285
1318, epoch_train_loss=4.0314990202227285
Epoch 1319
Epoch 1319 :: Batch 0/1
Batch Loss = 4.031103470960397
1319, epoch_train_loss=4.031103470960397
Epoch 1320
Epoch 1320 :: Batch 0/1
Batch Loss = 4.0309199612509135
1320, epoch_train_loss=4.0309199612509135
Epoch 1321
Epoch 1321 :: Batch 0/1
Batch Loss = 4.0305710179692325
1321, epoch_train_loss=4.0305710179692325
Epoch 1322
Epoch 1322 :: Batch 0/1
Batch Loss = 4.030312189936984
1322, epoch_train_loss=4.030312189936984
Epoch 1323
Epoch 1323 :: Batch 0/1
Batch Loss = 4.029987209087852
1323, epoch_train_loss=4.029987209087852
Epoch 1324
Epoch 1324 :: Batch 0/1
Batch Loss = 4.029697743336949
1324, epoch_train_loss=4.029697743336949
Epoch 1325
Epoch 1325 :: Batch 0/1
Batch Loss = 4.029380639398421
1325, epoch_train_loss=4.029380639398421
Epoch 1326
Epoch 1326 :: Batch 0/1
Batch Loss = 4.029140718251304
1326, epoch_train_loss=4.029140718251304
Epoch 1327
Epoch 1327 :: Batch 0/1
Batch Loss = 4.02883813475051
1327, epoch_train_loss=4.02883813475051
Epoch 1328
Epoch 1328 :: Batch 0/1
Batch Loss = 4.028614649379576
1328, epoch_train_loss=4.028614649379576
Epoch 1329
Epoch 1329 :: Batch 0/1
Batch Loss = 4.028338385607892
1329, epoch_train_loss=4.028338385607892
Epoch 1330
Epoch 1330 :: Batch 0/1
Batch Loss = 4.028103657011049
1330, epoch_train_loss=4.028103657011049
Epoch 1331
Epoch 1331 :: Batch 0/1
Batch Loss = 4.027871295965742
1331, epoch_train_loss=4.027871295965742
Epoch 1332
Epoch 1332 :: Batch 0/1
Batch Loss = 4.027668209256484
1332, epoch_train_loss=4.027668209256484
Epoch 1333
Epoch 1333 :: Batch 0/1
Batch Loss = 4.027453254601836
1333, epoch_train_loss=4.027453254601836
Epoch 1334
Epoch 1334 :: Batch 0/1
Batch Loss = 4.027265441062476
1334, epoch_train_loss=4.027265441062476
Epoch 1335
Epoch 1335 :: Batch 0/1
Batch Loss = 4.027029495437
1335, epoch_train_loss=4.027029495437
Epoch 1336
Epoch 1336 :: Batch 0/1
Batch Loss = 4.026799498627122
1336, epoch_train_loss=4.026799498627122
Epoch 1337
Epoch 1337 :: Batch 0/1
Batch Loss = 4.026551605530468
1337, epoch_train_loss=4.026551605530468
Epoch 1338
Epoch 1338 :: Batch 0/1
Batch Loss = 4.026289528902921
1338, epoch_train_loss=4.026289528902921
Epoch 1339
Epoch 1339 :: Batch 0/1
Batch Loss = 4.026041788927865
1339, epoch_train_loss=4.026041788927865
Epoch 1340
Epoch 1340 :: Batch 0/1
Batch Loss = 4.025794682802592
1340, epoch_train_loss=4.025794682802592
Epoch 1341
Epoch 1341 :: Batch 0/1
Batch Loss = 4.025552918161089
1341, epoch_train_loss=4.025552918161089
Epoch 1342
Epoch 1342 :: Batch 0/1
Batch Loss = 4.025317439122548
1342, epoch_train_loss=4.025317439122548
Epoch 1343
Epoch 1343 :: Batch 0/1
Batch Loss = 4.0250879232403065
1343, epoch_train_loss=4.0250879232403065
Epoch 1344
Epoch 1344 :: Batch 0/1
Batch Loss = 4.024853456418072
1344, epoch_train_loss=4.024853456418072
Epoch 1345
Epoch 1345 :: Batch 0/1
Batch Loss = 4.024635655864172
1345, epoch_train_loss=4.024635655864172
Epoch 1346
Epoch 1346 :: Batch 0/1
Batch Loss = 4.024410719994062
1346, epoch_train_loss=4.024410719994062
Epoch 1347
Epoch 1347 :: Batch 0/1
Batch Loss = 4.024195185957625
1347, epoch_train_loss=4.024195185957625
Epoch 1348
Epoch 1348 :: Batch 0/1
Batch Loss = 4.023972222009807
1348, epoch_train_loss=4.023972222009807
Epoch 1349
Epoch 1349 :: Batch 0/1
Batch Loss = 4.023754355648973
1349, epoch_train_loss=4.023754355648973
Epoch 1350
Epoch 1350 :: Batch 0/1
Batch Loss = 4.023527216932263
1350, epoch_train_loss=4.023527216932263
Epoch 1351
Epoch 1351 :: Batch 0/1
Batch Loss = 4.023309974409129
1351, epoch_train_loss=4.023309974409129
Epoch 1352
Epoch 1352 :: Batch 0/1
Batch Loss = 4.023089088170515
1352, epoch_train_loss=4.023089088170515
Epoch 1353
Epoch 1353 :: Batch 0/1
Batch Loss = 4.0228774945271235
1353, epoch_train_loss=4.0228774945271235
Epoch 1354
Epoch 1354 :: Batch 0/1
Batch Loss = 4.022665263029991
1354, epoch_train_loss=4.022665263029991
Epoch 1355
Epoch 1355 :: Batch 0/1
Batch Loss = 4.022458750568758
1355, epoch_train_loss=4.022458750568758
Epoch 1356
Epoch 1356 :: Batch 0/1
Batch Loss = 4.0222493299885524
1356, epoch_train_loss=4.0222493299885524
Epoch 1357
Epoch 1357 :: Batch 0/1
Batch Loss = 4.0220438320640985
1357, epoch_train_loss=4.0220438320640985
Epoch 1358
Epoch 1358 :: Batch 0/1
Batch Loss = 4.021837063735719
1358, epoch_train_loss=4.021837063735719
Epoch 1359
Epoch 1359 :: Batch 0/1
Batch Loss = 4.021630905099457
1359, epoch_train_loss=4.021630905099457
Epoch 1360
Epoch 1360 :: Batch 0/1
Batch Loss = 4.0214253576918555
1360, epoch_train_loss=4.0214253576918555
Epoch 1361
Epoch 1361 :: Batch 0/1
Batch Loss = 4.021217192639874
1361, epoch_train_loss=4.021217192639874
Epoch 1362
Epoch 1362 :: Batch 0/1
Batch Loss = 4.0210092330871845
1362, epoch_train_loss=4.0210092330871845
Epoch 1363
Epoch 1363 :: Batch 0/1
Batch Loss = 4.020798745099064
1363, epoch_train_loss=4.020798745099064
Epoch 1364
Epoch 1364 :: Batch 0/1
Batch Loss = 4.020588874212495
1364, epoch_train_loss=4.020588874212495
Epoch 1365
Epoch 1365 :: Batch 0/1
Batch Loss = 4.02037721672949
1365, epoch_train_loss=4.02037721672949
Epoch 1366
Epoch 1366 :: Batch 0/1
Batch Loss = 4.020166254212833
1366, epoch_train_loss=4.020166254212833
Epoch 1367
Epoch 1367 :: Batch 0/1
Batch Loss = 4.019953005625183
1367, epoch_train_loss=4.019953005625183
Epoch 1368
Epoch 1368 :: Batch 0/1
Batch Loss = 4.019739893893436
1368, epoch_train_loss=4.019739893893436
Epoch 1369
Epoch 1369 :: Batch 0/1
Batch Loss = 4.019524504692323
1369, epoch_train_loss=4.019524504692323
Epoch 1370
Epoch 1370 :: Batch 0/1
Batch Loss = 4.019309142300657
1370, epoch_train_loss=4.019309142300657
Epoch 1371
Epoch 1371 :: Batch 0/1
Batch Loss = 4.019092561811885
1371, epoch_train_loss=4.019092561811885
Epoch 1372
Epoch 1372 :: Batch 0/1
Batch Loss = 4.018875005062062
1372, epoch_train_loss=4.018875005062062
Epoch 1373
Epoch 1373 :: Batch 0/1
Batch Loss = 4.018656859089404
1373, epoch_train_loss=4.018656859089404
Epoch 1374
Epoch 1374 :: Batch 0/1
Batch Loss = 4.018436543076511
1374, epoch_train_loss=4.018436543076511
Epoch 1375
Epoch 1375 :: Batch 0/1
Batch Loss = 4.018215350812549
1375, epoch_train_loss=4.018215350812549
Epoch 1376
Epoch 1376 :: Batch 0/1
Batch Loss = 4.017992039129742
1376, epoch_train_loss=4.017992039129742
Epoch 1377
Epoch 1377 :: Batch 0/1
Batch Loss = 4.017767142319255
1377, epoch_train_loss=4.017767142319255
Epoch 1378
Epoch 1378 :: Batch 0/1
Batch Loss = 4.0175399258820494
1378, epoch_train_loss=4.0175399258820494
Epoch 1379
Epoch 1379 :: Batch 0/1
Batch Loss = 4.017310357243495
1379, epoch_train_loss=4.017310357243495
Epoch 1380
Epoch 1380 :: Batch 0/1
Batch Loss = 4.017077513002718
1380, epoch_train_loss=4.017077513002718
Epoch 1381
Epoch 1381 :: Batch 0/1
Batch Loss = 4.016841950730814
1381, epoch_train_loss=4.016841950730814
Epoch 1382
Epoch 1382 :: Batch 0/1
Batch Loss = 4.016603575762131
1382, epoch_train_loss=4.016603575762131
Epoch 1383
Epoch 1383 :: Batch 0/1
Batch Loss = 4.0163634513156765
1383, epoch_train_loss=4.0163634513156765
Epoch 1384
Epoch 1384 :: Batch 0/1
Batch Loss = 4.016123012447916
1384, epoch_train_loss=4.016123012447916
Epoch 1385
Epoch 1385 :: Batch 0/1
Batch Loss = 4.015882902983131
1385, epoch_train_loss=4.015882902983131
Epoch 1386
Epoch 1386 :: Batch 0/1
Batch Loss = 4.015644421896143
1386, epoch_train_loss=4.015644421896143
Epoch 1387
Epoch 1387 :: Batch 0/1
Batch Loss = 4.01540729384065
1387, epoch_train_loss=4.01540729384065
Epoch 1388
Epoch 1388 :: Batch 0/1
Batch Loss = 4.015171571217045
1388, epoch_train_loss=4.015171571217045
Epoch 1389
Epoch 1389 :: Batch 0/1
Batch Loss = 4.0149366312672266
1389, epoch_train_loss=4.0149366312672266
Epoch 1390
Epoch 1390 :: Batch 0/1
Batch Loss = 4.01470206163907
1390, epoch_train_loss=4.01470206163907
Epoch 1391
Epoch 1391 :: Batch 0/1
Batch Loss = 4.0144676542333535
1391, epoch_train_loss=4.0144676542333535
Epoch 1392
Epoch 1392 :: Batch 0/1
Batch Loss = 4.014233214693695
1392, epoch_train_loss=4.014233214693695
Epoch 1393
Epoch 1393 :: Batch 0/1
Batch Loss = 4.013999115628041
1393, epoch_train_loss=4.013999115628041
Epoch 1394
Epoch 1394 :: Batch 0/1
Batch Loss = 4.0137654482833725
1394, epoch_train_loss=4.0137654482833725
Epoch 1395
Epoch 1395 :: Batch 0/1
Batch Loss = 4.013532554470729
1395, epoch_train_loss=4.013532554470729
Epoch 1396
Epoch 1396 :: Batch 0/1
Batch Loss = 4.013300331927302
1396, epoch_train_loss=4.013300331927302
Epoch 1397
Epoch 1397 :: Batch 0/1
Batch Loss = 4.013068461897343
1397, epoch_train_loss=4.013068461897343
Epoch 1398
Epoch 1398 :: Batch 0/1
Batch Loss = 4.0128364910360235
1398, epoch_train_loss=4.0128364910360235
Epoch 1399
Epoch 1399 :: Batch 0/1
Batch Loss = 4.01260380337322
1399, epoch_train_loss=4.01260380337322
Epoch 1400
Epoch 1400 :: Batch 0/1
Batch Loss = 4.012370058290847
1400, epoch_train_loss=4.012370058290847
Epoch 1401
Epoch 1401 :: Batch 0/1
Batch Loss = 4.012134914602701
1401, epoch_train_loss=4.012134914602701
Epoch 1402
Epoch 1402 :: Batch 0/1
Batch Loss = 4.011898303327571
1402, epoch_train_loss=4.011898303327571
Epoch 1403
Epoch 1403 :: Batch 0/1
Batch Loss = 4.011660121134817
1403, epoch_train_loss=4.011660121134817
Epoch 1404
Epoch 1404 :: Batch 0/1
Batch Loss = 4.011420205049392
1404, epoch_train_loss=4.011420205049392
Epoch 1405
Epoch 1405 :: Batch 0/1
Batch Loss = 4.01117842940122
1405, epoch_train_loss=4.01117842940122
Epoch 1406
Epoch 1406 :: Batch 0/1
Batch Loss = 4.010934560680169
1406, epoch_train_loss=4.010934560680169
Epoch 1407
Epoch 1407 :: Batch 0/1
Batch Loss = 4.010688647769804
1407, epoch_train_loss=4.010688647769804
Epoch 1408
Epoch 1408 :: Batch 0/1
Batch Loss = 4.010441011136263
1408, epoch_train_loss=4.010441011136263
Epoch 1409
Epoch 1409 :: Batch 0/1
Batch Loss = 4.010192414128817
1409, epoch_train_loss=4.010192414128817
Epoch 1410
Epoch 1410 :: Batch 0/1
Batch Loss = 4.009944066546098
1410, epoch_train_loss=4.009944066546098
Epoch 1411
Epoch 1411 :: Batch 0/1
Batch Loss = 4.009697239389881
1411, epoch_train_loss=4.009697239389881
Epoch 1412
Epoch 1412 :: Batch 0/1
Batch Loss = 4.009453094367337
1412, epoch_train_loss=4.009453094367337
Epoch 1413
Epoch 1413 :: Batch 0/1
Batch Loss = 4.00921231989684
1413, epoch_train_loss=4.00921231989684
Epoch 1414
Epoch 1414 :: Batch 0/1
Batch Loss = 4.008975068118794
1414, epoch_train_loss=4.008975068118794
Epoch 1415
Epoch 1415 :: Batch 0/1
Batch Loss = 4.008741059104039
1415, epoch_train_loss=4.008741059104039
Epoch 1416
Epoch 1416 :: Batch 0/1
Batch Loss = 4.008509679344916
1416, epoch_train_loss=4.008509679344916
Epoch 1417
Epoch 1417 :: Batch 0/1
Batch Loss = 4.008280278607728
1417, epoch_train_loss=4.008280278607728
Epoch 1418
Epoch 1418 :: Batch 0/1
Batch Loss = 4.008052244598758
1418, epoch_train_loss=4.008052244598758
Epoch 1419
Epoch 1419 :: Batch 0/1
Batch Loss = 4.00782504417457
1419, epoch_train_loss=4.00782504417457
Epoch 1420
Epoch 1420 :: Batch 0/1
Batch Loss = 4.007598263803916
1420, epoch_train_loss=4.007598263803916
Epoch 1421
Epoch 1421 :: Batch 0/1
Batch Loss = 4.007371562766569
1421, epoch_train_loss=4.007371562766569
Epoch 1422
Epoch 1422 :: Batch 0/1
Batch Loss = 4.007144719411015
1422, epoch_train_loss=4.007144719411015
Epoch 1423
Epoch 1423 :: Batch 0/1
Batch Loss = 4.0069175814427425
1423, epoch_train_loss=4.0069175814427425
Epoch 1424
Epoch 1424 :: Batch 0/1
Batch Loss = 4.006690050834761
1424, epoch_train_loss=4.006690050834761
Epoch 1425
Epoch 1425 :: Batch 0/1
Batch Loss = 4.006462115693089
1425, epoch_train_loss=4.006462115693089
Epoch 1426
Epoch 1426 :: Batch 0/1
Batch Loss = 4.006233816258762
1426, epoch_train_loss=4.006233816258762
Epoch 1427
Epoch 1427 :: Batch 0/1
Batch Loss = 4.006005260802808
1427, epoch_train_loss=4.006005260802808
Epoch 1428
Epoch 1428 :: Batch 0/1
Batch Loss = 4.005776583155506
1428, epoch_train_loss=4.005776583155506
Epoch 1429
Epoch 1429 :: Batch 0/1
Batch Loss = 4.005547882262217
1429, epoch_train_loss=4.005547882262217
Epoch 1430
Epoch 1430 :: Batch 0/1
Batch Loss = 4.005319203464518
1430, epoch_train_loss=4.005319203464518
Epoch 1431
Epoch 1431 :: Batch 0/1
Batch Loss = 4.0050905442068325
1431, epoch_train_loss=4.0050905442068325
Epoch 1432
Epoch 1432 :: Batch 0/1
Batch Loss = 4.004861968310825
1432, epoch_train_loss=4.004861968310825
Epoch 1433
Epoch 1433 :: Batch 0/1
Batch Loss = 4.004633756470255
1433, epoch_train_loss=4.004633756470255
Epoch 1434
Epoch 1434 :: Batch 0/1
Batch Loss = 4.004406477248176
1434, epoch_train_loss=4.004406477248176
Epoch 1435
Epoch 1435 :: Batch 0/1
Batch Loss = 4.004181013258883
1435, epoch_train_loss=4.004181013258883
Epoch 1436
Epoch 1436 :: Batch 0/1
Batch Loss = 4.0039584436711335
1436, epoch_train_loss=4.0039584436711335
Epoch 1437
Epoch 1437 :: Batch 0/1
Batch Loss = 4.003739238018616
1437, epoch_train_loss=4.003739238018616
Epoch 1438
Epoch 1438 :: Batch 0/1
Batch Loss = 4.003522726960677
1438, epoch_train_loss=4.003522726960677
Epoch 1439
Epoch 1439 :: Batch 0/1
Batch Loss = 4.0033078885204585
1439, epoch_train_loss=4.0033078885204585
Epoch 1440
Epoch 1440 :: Batch 0/1
Batch Loss = 4.00309405960497
1440, epoch_train_loss=4.00309405960497
Epoch 1441
Epoch 1441 :: Batch 0/1
Batch Loss = 4.002880945978225
1441, epoch_train_loss=4.002880945978225
Epoch 1442
Epoch 1442 :: Batch 0/1
Batch Loss = 4.002668414704442
1442, epoch_train_loss=4.002668414704442
Epoch 1443
Epoch 1443 :: Batch 0/1
Batch Loss = 4.0024563863123355
1443, epoch_train_loss=4.0024563863123355
Epoch 1444
Epoch 1444 :: Batch 0/1
Batch Loss = 4.002244800679322
1444, epoch_train_loss=4.002244800679322
Epoch 1445
Epoch 1445 :: Batch 0/1
Batch Loss = 4.002033604287488
1445, epoch_train_loss=4.002033604287488
Epoch 1446
Epoch 1446 :: Batch 0/1
Batch Loss = 4.001822749595641
1446, epoch_train_loss=4.001822749595641
Epoch 1447
Epoch 1447 :: Batch 0/1
Batch Loss = 4.00161219342994
1447, epoch_train_loss=4.00161219342994
Epoch 1448
Epoch 1448 :: Batch 0/1
Batch Loss = 4.0014018957851345
1448, epoch_train_loss=4.0014018957851345
Epoch 1449
Epoch 1449 :: Batch 0/1
Batch Loss = 4.001191820279181
1449, epoch_train_loss=4.001191820279181
Epoch 1450
Epoch 1450 :: Batch 0/1
Batch Loss = 4.000981937746628
1450, epoch_train_loss=4.000981937746628
Epoch 1451
Epoch 1451 :: Batch 0/1
Batch Loss = 4.000772226976528
1451, epoch_train_loss=4.000772226976528
Epoch 1452
Epoch 1452 :: Batch 0/1
Batch Loss = 4.000562675993372
1452, epoch_train_loss=4.000562675993372
Epoch 1453
Epoch 1453 :: Batch 0/1
Batch Loss = 4.000353283379783
1453, epoch_train_loss=4.000353283379783
Epoch 1454
Epoch 1454 :: Batch 0/1
Batch Loss = 4.000144056075318
1454, epoch_train_loss=4.000144056075318
Epoch 1455
Epoch 1455 :: Batch 0/1
Batch Loss = 3.99993500791183
1455, epoch_train_loss=3.99993500791183
Epoch 1456
Epoch 1456 :: Batch 0/1
Batch Loss = 3.999726156114123
1456, epoch_train_loss=3.999726156114123
Epoch 1457
Epoch 1457 :: Batch 0/1
Batch Loss = 3.99951751556654
1457, epoch_train_loss=3.99951751556654
Epoch 1458
Epoch 1458 :: Batch 0/1
Batch Loss = 3.999309092452641
1458, epoch_train_loss=3.999309092452641
Epoch 1459
Epoch 1459 :: Batch 0/1
Batch Loss = 3.999100879582465
1459, epoch_train_loss=3.999100879582465
Epoch 1460
Epoch 1460 :: Batch 0/1
Batch Loss = 3.9988928528625576
1460, epoch_train_loss=3.9988928528625576
Epoch 1461
Epoch 1461 :: Batch 0/1
Batch Loss = 3.9986849703319196
1461, epoch_train_loss=3.9986849703319196
Epoch 1462
Epoch 1462 :: Batch 0/1
Batch Loss = 3.9984771743872844
1462, epoch_train_loss=3.9984771743872844
Epoch 1463
Epoch 1463 :: Batch 0/1
Batch Loss = 3.998269393577938
1463, epoch_train_loss=3.998269393577938
Epoch 1464
Epoch 1464 :: Batch 0/1
Batch Loss = 3.998061545452534
1464, epoch_train_loss=3.998061545452534
Epoch 1465
Epoch 1465 :: Batch 0/1
Batch Loss = 3.997853538987666
1465, epoch_train_loss=3.997853538987666
Epoch 1466
Epoch 1466 :: Batch 0/1
Batch Loss = 3.9976452750284195
1466, epoch_train_loss=3.9976452750284195
Epoch 1467
Epoch 1467 :: Batch 0/1
Batch Loss = 3.9974366447518457
1467, epoch_train_loss=3.9974366447518457
Epoch 1468
Epoch 1468 :: Batch 0/1
Batch Loss = 3.9972275249272604
1468, epoch_train_loss=3.9972275249272604
Epoch 1469
Epoch 1469 :: Batch 0/1
Batch Loss = 3.9970177691944215
1469, epoch_train_loss=3.9970177691944215
Epoch 1470
Epoch 1470 :: Batch 0/1
Batch Loss = 3.996807194839272
1470, epoch_train_loss=3.996807194839272
Epoch 1471
Epoch 1471 :: Batch 0/1
Batch Loss = 3.99659556588122
1471, epoch_train_loss=3.99659556588122
Epoch 1472
Epoch 1472 :: Batch 0/1
Batch Loss = 3.996382571940569
1472, epoch_train_loss=3.996382571940569
Epoch 1473
Epoch 1473 :: Batch 0/1
Batch Loss = 3.996167804594772
1473, epoch_train_loss=3.996167804594772
Epoch 1474
Epoch 1474 :: Batch 0/1
Batch Loss = 3.9959507328977493
1474, epoch_train_loss=3.9959507328977493
Epoch 1475
Epoch 1475 :: Batch 0/1
Batch Loss = 3.9957306832594823
1475, epoch_train_loss=3.9957306832594823
Epoch 1476
Epoch 1476 :: Batch 0/1
Batch Loss = 3.995506833400185
1476, epoch_train_loss=3.995506833400185
Epoch 1477
Epoch 1477 :: Batch 0/1
Batch Loss = 3.995278238515784
1477, epoch_train_loss=3.995278238515784
Epoch 1478
Epoch 1478 :: Batch 0/1
Batch Loss = 3.9950439164972793
1478, epoch_train_loss=3.9950439164972793
Epoch 1479
Epoch 1479 :: Batch 0/1
Batch Loss = 3.994803016895772
1479, epoch_train_loss=3.994803016895772
Epoch 1480
Epoch 1480 :: Batch 0/1
Batch Loss = 3.9945550671282937
1480, epoch_train_loss=3.9945550671282937
Epoch 1481
Epoch 1481 :: Batch 0/1
Batch Loss = 3.9943002282831914
1481, epoch_train_loss=3.9943002282831914
Epoch 1482
Epoch 1482 :: Batch 0/1
Batch Loss = 3.9940394455459214
1482, epoch_train_loss=3.9940394455459214
Epoch 1483
Epoch 1483 :: Batch 0/1
Batch Loss = 3.9937744142438243
1483, epoch_train_loss=3.9937744142438243
Epoch 1484
Epoch 1484 :: Batch 0/1
Batch Loss = 3.993507345973696
1484, epoch_train_loss=3.993507345973696
Epoch 1485
Epoch 1485 :: Batch 0/1
Batch Loss = 3.993240489552101
1485, epoch_train_loss=3.993240489552101
Epoch 1486
Epoch 1486 :: Batch 0/1
Batch Loss = 3.992975390045517
1486, epoch_train_loss=3.992975390045517
Epoch 1487
Epoch 1487 :: Batch 0/1
Batch Loss = 3.992712221661133
1487, epoch_train_loss=3.992712221661133
Epoch 1488
Epoch 1488 :: Batch 0/1
Batch Loss = 3.9924498146111698
1488, epoch_train_loss=3.9924498146111698
Epoch 1489
Epoch 1489 :: Batch 0/1
Batch Loss = 3.992186545844156
1489, epoch_train_loss=3.992186545844156
Epoch 1490
Epoch 1490 :: Batch 0/1
Batch Loss = 3.99192147666765
1490, epoch_train_loss=3.99192147666765
Epoch 1491
Epoch 1491 :: Batch 0/1
Batch Loss = 3.9916548905311338
1491, epoch_train_loss=3.9916548905311338
Epoch 1492
Epoch 1492 :: Batch 0/1
Batch Loss = 3.9913878197450323
1492, epoch_train_loss=3.9913878197450323
Epoch 1493
Epoch 1493 :: Batch 0/1
Batch Loss = 3.9911209489967727
1493, epoch_train_loss=3.9911209489967727
Epoch 1494
Epoch 1494 :: Batch 0/1
Batch Loss = 3.9908539049889864
1494, epoch_train_loss=3.9908539049889864
Epoch 1495
Epoch 1495 :: Batch 0/1
Batch Loss = 3.9905854828684104
1495, epoch_train_loss=3.9905854828684104
Epoch 1496
Epoch 1496 :: Batch 0/1
Batch Loss = 3.9903142467410024
1496, epoch_train_loss=3.9903142467410024
Epoch 1497
Epoch 1497 :: Batch 0/1
Batch Loss = 3.9900387309487795
1497, epoch_train_loss=3.9900387309487795
Epoch 1498
Epoch 1498 :: Batch 0/1
Batch Loss = 3.989757223187047
1498, epoch_train_loss=3.989757223187047
Epoch 1499
Epoch 1499 :: Batch 0/1
Batch Loss = 3.9894677461511296
1499, epoch_train_loss=3.9894677461511296
Epoch 1500
Epoch 1500 :: Batch 0/1
Batch Loss = 3.9891688323007966
1500, epoch_train_loss=3.9891688323007966
Epoch 1501
Epoch 1501 :: Batch 0/1
Batch Loss = 3.988861038182836
1501, epoch_train_loss=3.988861038182836
Epoch 1502
Epoch 1502 :: Batch 0/1
Batch Loss = 3.988548204345956
1502, epoch_train_loss=3.988548204345956
Epoch 1503
Epoch 1503 :: Batch 0/1
Batch Loss = 3.9882370296419998
1503, epoch_train_loss=3.9882370296419998
Epoch 1504
Epoch 1504 :: Batch 0/1
Batch Loss = 3.9879345867350677
1504, epoch_train_loss=3.9879345867350677
Epoch 1505
Epoch 1505 :: Batch 0/1
Batch Loss = 3.987645244173104
1505, epoch_train_loss=3.987645244173104
Epoch 1506
Epoch 1506 :: Batch 0/1
Batch Loss = 3.9873690101561046
1506, epoch_train_loss=3.9873690101561046
Epoch 1507
Epoch 1507 :: Batch 0/1
Batch Loss = 3.987102335744485
1507, epoch_train_loss=3.987102335744485
Epoch 1508
Epoch 1508 :: Batch 0/1
Batch Loss = 3.986840513945712
1508, epoch_train_loss=3.986840513945712
Epoch 1509
Epoch 1509 :: Batch 0/1
Batch Loss = 3.9865796229524304
1509, epoch_train_loss=3.9865796229524304
Epoch 1510
Epoch 1510 :: Batch 0/1
Batch Loss = 3.9863171749403516
1510, epoch_train_loss=3.9863171749403516
Epoch 1511
Epoch 1511 :: Batch 0/1
Batch Loss = 3.986052069777916
1511, epoch_train_loss=3.986052069777916
Epoch 1512
Epoch 1512 :: Batch 0/1
Batch Loss = 3.9857843600113863
1512, epoch_train_loss=3.9857843600113863
Epoch 1513
Epoch 1513 :: Batch 0/1
Batch Loss = 3.9855148645245473
1513, epoch_train_loss=3.9855148645245473
Epoch 1514
Epoch 1514 :: Batch 0/1
Batch Loss = 3.985244678473294
1514, epoch_train_loss=3.985244678473294
Epoch 1515
Epoch 1515 :: Batch 0/1
Batch Loss = 3.9849747575790375
1515, epoch_train_loss=3.9849747575790375
Epoch 1516
Epoch 1516 :: Batch 0/1
Batch Loss = 3.984705653130712
1516, epoch_train_loss=3.984705653130712
Epoch 1517
Epoch 1517 :: Batch 0/1
Batch Loss = 3.984437350452654
1517, epoch_train_loss=3.984437350452654
Epoch 1518
Epoch 1518 :: Batch 0/1
Batch Loss = 3.9841692141136935
1518, epoch_train_loss=3.9841692141136935
Epoch 1519
Epoch 1519 :: Batch 0/1
Batch Loss = 3.983900114327451
1519, epoch_train_loss=3.983900114327451
Epoch 1520
Epoch 1520 :: Batch 0/1
Batch Loss = 3.9836287399787182
1520, epoch_train_loss=3.9836287399787182
Epoch 1521
Epoch 1521 :: Batch 0/1
Batch Loss = 3.9833539680868872
1521, epoch_train_loss=3.9833539680868872
Epoch 1522
Epoch 1522 :: Batch 0/1
Batch Loss = 3.9830751182381836
1522, epoch_train_loss=3.9830751182381836
Epoch 1523
Epoch 1523 :: Batch 0/1
Batch Loss = 3.9827920319533963
1523, epoch_train_loss=3.9827920319533963
Epoch 1524
Epoch 1524 :: Batch 0/1
Batch Loss = 3.982505041982563
1524, epoch_train_loss=3.982505041982563
Epoch 1525
Epoch 1525 :: Batch 0/1
Batch Loss = 3.9822148741443666
1525, epoch_train_loss=3.9822148741443666
Epoch 1526
Epoch 1526 :: Batch 0/1
Batch Loss = 3.9819224610787765
1526, epoch_train_loss=3.9819224610787765
Epoch 1527
Epoch 1527 :: Batch 0/1
Batch Loss = 3.981628732872626
1527, epoch_train_loss=3.981628732872626
Epoch 1528
Epoch 1528 :: Batch 0/1
Batch Loss = 3.9813345241867064
1528, epoch_train_loss=3.9813345241867064
Epoch 1529
Epoch 1529 :: Batch 0/1
Batch Loss = 3.9810405980615817
1529, epoch_train_loss=3.9810405980615817
Epoch 1530
Epoch 1530 :: Batch 0/1
Batch Loss = 3.9807476293454758
1530, epoch_train_loss=3.9807476293454758
Epoch 1531
Epoch 1531 :: Batch 0/1
Batch Loss = 3.9804560824445985
1531, epoch_train_loss=3.9804560824445985
Epoch 1532
Epoch 1532 :: Batch 0/1
Batch Loss = 3.98016612688982
1532, epoch_train_loss=3.98016612688982
Epoch 1533
Epoch 1533 :: Batch 0/1
Batch Loss = 3.979877692884215
1533, epoch_train_loss=3.979877692884215
Epoch 1534
Epoch 1534 :: Batch 0/1
Batch Loss = 3.979590543279946
1534, epoch_train_loss=3.979590543279946
Epoch 1535
Epoch 1535 :: Batch 0/1
Batch Loss = 3.9793042398101806
1535, epoch_train_loss=3.9793042398101806
Epoch 1536
Epoch 1536 :: Batch 0/1
Batch Loss = 3.979018107994659
1536, epoch_train_loss=3.979018107994659
Epoch 1537
Epoch 1537 :: Batch 0/1
Batch Loss = 3.978731362667059
1537, epoch_train_loss=3.978731362667059
Epoch 1538
Epoch 1538 :: Batch 0/1
Batch Loss = 3.9784433588597623
1538, epoch_train_loss=3.9784433588597623
Epoch 1539
Epoch 1539 :: Batch 0/1
Batch Loss = 3.9781537812968235
1539, epoch_train_loss=3.9781537812968235
Epoch 1540
Epoch 1540 :: Batch 0/1
Batch Loss = 3.9778626333809757
1540, epoch_train_loss=3.9778626333809757
Epoch 1541
Epoch 1541 :: Batch 0/1
Batch Loss = 3.977570044380856
1541, epoch_train_loss=3.977570044380856
Epoch 1542
Epoch 1542 :: Batch 0/1
Batch Loss = 3.9772760526545414
1542, epoch_train_loss=3.9772760526545414
Epoch 1543
Epoch 1543 :: Batch 0/1
Batch Loss = 3.976980547150551
1543, epoch_train_loss=3.976980547150551
Epoch 1544
Epoch 1544 :: Batch 0/1
Batch Loss = 3.976683428894457
1544, epoch_train_loss=3.976683428894457
Epoch 1545
Epoch 1545 :: Batch 0/1
Batch Loss = 3.9763848959455297
1545, epoch_train_loss=3.9763848959455297
Epoch 1546
Epoch 1546 :: Batch 0/1
Batch Loss = 3.9760855509184494
1546, epoch_train_loss=3.9760855509184494
Epoch 1547
Epoch 1547 :: Batch 0/1
Batch Loss = 3.9757860100001863
1547, epoch_train_loss=3.9757860100001863
Epoch 1548
Epoch 1548 :: Batch 0/1
Batch Loss = 3.9754862998082943
1548, epoch_train_loss=3.9754862998082943
Epoch 1549
Epoch 1549 :: Batch 0/1
Batch Loss = 3.9751854062193153
1549, epoch_train_loss=3.9751854062193153
Epoch 1550
Epoch 1550 :: Batch 0/1
Batch Loss = 3.97488085499937
1550, epoch_train_loss=3.97488085499937
Epoch 1551
Epoch 1551 :: Batch 0/1
Batch Loss = 3.9745690361742994
1551, epoch_train_loss=3.9745690361742994
Epoch 1552
Epoch 1552 :: Batch 0/1
Batch Loss = 3.9742507699657392
1552, epoch_train_loss=3.9742507699657392
Epoch 1553
Epoch 1553 :: Batch 0/1
Batch Loss = 3.9739393185577327
1553, epoch_train_loss=3.9739393185577327
Epoch 1554
Epoch 1554 :: Batch 0/1
Batch Loss = 3.9736415079104637
1554, epoch_train_loss=3.9736415079104637
Epoch 1555
Epoch 1555 :: Batch 0/1
Batch Loss = 3.973347601968172
1555, epoch_train_loss=3.973347601968172
Epoch 1556
Epoch 1556 :: Batch 0/1
Batch Loss = 3.9730402833849605
1556, epoch_train_loss=3.9730402833849605
Epoch 1557
Epoch 1557 :: Batch 0/1
Batch Loss = 3.972767114513198
1557, epoch_train_loss=3.972767114513198
Epoch 1558
Epoch 1558 :: Batch 0/1
Batch Loss = 3.9725514630316794
1558, epoch_train_loss=3.9725514630316794
Epoch 1559
Epoch 1559 :: Batch 0/1
Batch Loss = 3.9726051805293148
1559, epoch_train_loss=3.9726051805293148
Epoch 1560
Epoch 1560 :: Batch 0/1
Batch Loss = 3.9727945853304223
1560, epoch_train_loss=3.9727945853304223
Epoch 1561
Epoch 1561 :: Batch 0/1
Batch Loss = 3.9748668245999546
1561, epoch_train_loss=3.9748668245999546
Epoch 1562
Epoch 1562 :: Batch 0/1
Batch Loss = 3.973031286314816
1562, epoch_train_loss=3.973031286314816
Epoch 1563
Epoch 1563 :: Batch 0/1
Batch Loss = 3.9735924385046926
1563, epoch_train_loss=3.9735924385046926
Epoch 1564
Epoch 1564 :: Batch 0/1
Batch Loss = 3.972015181611057
1564, epoch_train_loss=3.972015181611057
Epoch 1565
Epoch 1565 :: Batch 0/1
Batch Loss = 3.971824352337279
1565, epoch_train_loss=3.971824352337279
Epoch 1566
Epoch 1566 :: Batch 0/1
Batch Loss = 3.97108280527928
1566, epoch_train_loss=3.97108280527928
Epoch 1567
Epoch 1567 :: Batch 0/1
Batch Loss = 3.970762663647622
1567, epoch_train_loss=3.970762663647622
Epoch 1568
Epoch 1568 :: Batch 0/1
Batch Loss = 3.970388063511566
1568, epoch_train_loss=3.970388063511566
Epoch 1569
Epoch 1569 :: Batch 0/1
Batch Loss = 3.9700537271061296
1569, epoch_train_loss=3.9700537271061296
Epoch 1570
Epoch 1570 :: Batch 0/1
Batch Loss = 3.970067939821559
1570, epoch_train_loss=3.970067939821559
Epoch 1571
Epoch 1571 :: Batch 0/1
Batch Loss = 3.9704604304209123
1571, epoch_train_loss=3.9704604304209123
Epoch 1572
Epoch 1572 :: Batch 0/1
Batch Loss = 3.9708486179916815
1572, epoch_train_loss=3.9708486179916815
Epoch 1573
Epoch 1573 :: Batch 0/1
Batch Loss = 3.9732691560470754
1573, epoch_train_loss=3.9732691560470754
Epoch 1574
Epoch 1574 :: Batch 0/1
Batch Loss = 3.971761006403472
1574, epoch_train_loss=3.971761006403472
Epoch 1575
Epoch 1575 :: Batch 0/1
Batch Loss = 3.9717221158695857
1575, epoch_train_loss=3.9717221158695857
Epoch 1576
Epoch 1576 :: Batch 0/1
Batch Loss = 3.9757342233444595
1576, epoch_train_loss=3.9757342233444595
Epoch 1577
Epoch 1577 :: Batch 0/1
Batch Loss = 3.9689737547687183
1577, epoch_train_loss=3.9689737547687183
Epoch 1578
Epoch 1578 :: Batch 0/1
Batch Loss = 3.969511150098579
1578, epoch_train_loss=3.969511150098579
Epoch 1579
Epoch 1579 :: Batch 0/1
Batch Loss = 3.96763159981172
1579, epoch_train_loss=3.96763159981172
Epoch 1580
Epoch 1580 :: Batch 0/1
Batch Loss = 3.9742796606097777
1580, epoch_train_loss=3.9742796606097777
Epoch 1581
Epoch 1581 :: Batch 0/1
Batch Loss = 3.989918079708191
1581, epoch_train_loss=3.989918079708191
Epoch 1582
Epoch 1582 :: Batch 0/1
Batch Loss = 3.9922643649106604
1582, epoch_train_loss=3.9922643649106604
Epoch 1583
Epoch 1583 :: Batch 0/1
Batch Loss = 3.98705985572247
1583, epoch_train_loss=3.98705985572247
Epoch 1584
Epoch 1584 :: Batch 0/1
Batch Loss = 3.9764700710869354
1584, epoch_train_loss=3.9764700710869354
Epoch 1585
Epoch 1585 :: Batch 0/1
Batch Loss = 3.981393097267974
1585, epoch_train_loss=3.981393097267974
Epoch 1586
Epoch 1586 :: Batch 0/1
Batch Loss = 3.9759806461819784
1586, epoch_train_loss=3.9759806461819784
Epoch 1587
Epoch 1587 :: Batch 0/1
Batch Loss = 3.977905531728394
1587, epoch_train_loss=3.977905531728394
Epoch 1588
Epoch 1588 :: Batch 0/1
Batch Loss = 3.9884927006165363
1588, epoch_train_loss=3.9884927006165363
Epoch 1589
Epoch 1589 :: Batch 0/1
Batch Loss = 3.9938689878984484
1589, epoch_train_loss=3.9938689878984484
Epoch 1590
Epoch 1590 :: Batch 0/1
Batch Loss = 3.9707538175952326
1590, epoch_train_loss=3.9707538175952326
Epoch 1591
Epoch 1591 :: Batch 0/1
Batch Loss = 4.024419928512787
1591, epoch_train_loss=4.024419928512787
Epoch 1592
Epoch 1592 :: Batch 0/1
Batch Loss = 4.033124464298081
1592, epoch_train_loss=4.033124464298081
Epoch 1593
Epoch 1593 :: Batch 0/1
Batch Loss = 4.059052805055043
1593, epoch_train_loss=4.059052805055043
Epoch 1594
Epoch 1594 :: Batch 0/1
Batch Loss = 4.065158505460129
1594, epoch_train_loss=4.065158505460129
Epoch 1595
Epoch 1595 :: Batch 0/1
Batch Loss = 4.070277305993264
1595, epoch_train_loss=4.070277305993264
Epoch 1596
Epoch 1596 :: Batch 0/1
Batch Loss = 4.074897153334139
1596, epoch_train_loss=4.074897153334139
Epoch 1597
Epoch 1597 :: Batch 0/1
Batch Loss = 4.07128077225068
1597, epoch_train_loss=4.07128077225068
Epoch 1598
Epoch 1598 :: Batch 0/1
Batch Loss = 4.069999996378359
1598, epoch_train_loss=4.069999996378359
Epoch 1599
Epoch 1599 :: Batch 0/1
Batch Loss = 4.0678775311474125
1599, epoch_train_loss=4.0678775311474125
Epoch 1600
Epoch 1600 :: Batch 0/1
Batch Loss = 4.064612014908486
1600, epoch_train_loss=4.064612014908486
Epoch 1601
Epoch 1601 :: Batch 0/1
Batch Loss = 4.06788816820559
1601, epoch_train_loss=4.06788816820559
Epoch 1602
Epoch 1602 :: Batch 0/1
Batch Loss = 4.065126251085152
1602, epoch_train_loss=4.065126251085152
Epoch 1603
Epoch 1603 :: Batch 0/1
Batch Loss = 4.060565803330497
1603, epoch_train_loss=4.060565803330497
Epoch 1604
Epoch 1604 :: Batch 0/1
Batch Loss = 4.061150583622331
1604, epoch_train_loss=4.061150583622331
Epoch 1605
Epoch 1605 :: Batch 0/1
Batch Loss = 4.061112925583226
1605, epoch_train_loss=4.061112925583226
Epoch 1606
Epoch 1606 :: Batch 0/1
Batch Loss = 4.060566806781588
1606, epoch_train_loss=4.060566806781588
Epoch 1607
Epoch 1607 :: Batch 0/1
Batch Loss = 4.058842745159605
1607, epoch_train_loss=4.058842745159605
Epoch 1608
Epoch 1608 :: Batch 0/1
Batch Loss = 4.056130178505436
1608, epoch_train_loss=4.056130178505436
Epoch 1609
Epoch 1609 :: Batch 0/1
Batch Loss = 4.053090434056793
1609, epoch_train_loss=4.053090434056793
Epoch 1610
Epoch 1610 :: Batch 0/1
Batch Loss = 4.0529158757297825
1610, epoch_train_loss=4.0529158757297825
Epoch 1611
Epoch 1611 :: Batch 0/1
Batch Loss = 4.053155746171835
1611, epoch_train_loss=4.053155746171835
Epoch 1612
Epoch 1612 :: Batch 0/1
Batch Loss = 4.049822293024751
1612, epoch_train_loss=4.049822293024751
Epoch 1613
Epoch 1613 :: Batch 0/1
Batch Loss = 4.048033747890198
1613, epoch_train_loss=4.048033747890198
Epoch 1614
Epoch 1614 :: Batch 0/1
Batch Loss = 4.047709734925739
1614, epoch_train_loss=4.047709734925739
Epoch 1615
Epoch 1615 :: Batch 0/1
Batch Loss = 4.046934122191656
1615, epoch_train_loss=4.046934122191656
Epoch 1616
Epoch 1616 :: Batch 0/1
Batch Loss = 4.044950140970814
1616, epoch_train_loss=4.044950140970814
Epoch 1617
Epoch 1617 :: Batch 0/1
Batch Loss = 4.043002048331613
1617, epoch_train_loss=4.043002048331613
Epoch 1618
Epoch 1618 :: Batch 0/1
Batch Loss = 4.04182218984208
1618, epoch_train_loss=4.04182218984208
Epoch 1619
Epoch 1619 :: Batch 0/1
Batch Loss = 4.041163743713702
1619, epoch_train_loss=4.041163743713702
Epoch 1620
Epoch 1620 :: Batch 0/1
Batch Loss = 4.039642556518016
1620, epoch_train_loss=4.039642556518016
Epoch 1621
Epoch 1621 :: Batch 0/1
Batch Loss = 4.0380925676557515
1621, epoch_train_loss=4.0380925676557515
Epoch 1622
Epoch 1622 :: Batch 0/1
Batch Loss = 4.037150979669
1622, epoch_train_loss=4.037150979669
Epoch 1623
Epoch 1623 :: Batch 0/1
Batch Loss = 4.036244359316273
1623, epoch_train_loss=4.036244359316273
Epoch 1624
Epoch 1624 :: Batch 0/1
Batch Loss = 4.035146826903436
1624, epoch_train_loss=4.035146826903436
Epoch 1625
Epoch 1625 :: Batch 0/1
Batch Loss = 4.0335777976774585
1625, epoch_train_loss=4.0335777976774585
Epoch 1626
Epoch 1626 :: Batch 0/1
Batch Loss = 4.0326281691645125
1626, epoch_train_loss=4.0326281691645125
Epoch 1627
Epoch 1627 :: Batch 0/1
Batch Loss = 4.040508373359836
1627, epoch_train_loss=4.040508373359836
Epoch 1628
Epoch 1628 :: Batch 0/1
Batch Loss = 4.056344188393374
1628, epoch_train_loss=4.056344188393374
Epoch 1629
Epoch 1629 :: Batch 0/1
Batch Loss = 4.060028125113264
1629, epoch_train_loss=4.060028125113264
Epoch 1630
Epoch 1630 :: Batch 0/1
Batch Loss = 4.061275243177783
1630, epoch_train_loss=4.061275243177783
Epoch 1631
Epoch 1631 :: Batch 0/1
Batch Loss = 4.061482462188759
1631, epoch_train_loss=4.061482462188759
Epoch 1632
Epoch 1632 :: Batch 0/1
Batch Loss = 4.061310912001233
1632, epoch_train_loss=4.061310912001233
Epoch 1633
Epoch 1633 :: Batch 0/1
Batch Loss = 4.06197053692213
1633, epoch_train_loss=4.06197053692213
Epoch 1634
Epoch 1634 :: Batch 0/1
Batch Loss = 4.06224422012446
1634, epoch_train_loss=4.06224422012446
Epoch 1635
Epoch 1635 :: Batch 0/1
Batch Loss = 4.061616223106656
1635, epoch_train_loss=4.061616223106656
Epoch 1636
Epoch 1636 :: Batch 0/1
Batch Loss = 4.05932868502616
1636, epoch_train_loss=4.05932868502616
Epoch 1637
Epoch 1637 :: Batch 0/1
Batch Loss = 4.060477840082213
1637, epoch_train_loss=4.060477840082213
Epoch 1638
Epoch 1638 :: Batch 0/1
Batch Loss = 4.058576475123708
1638, epoch_train_loss=4.058576475123708
Epoch 1639
Epoch 1639 :: Batch 0/1
Batch Loss = 4.057908168320412
1639, epoch_train_loss=4.057908168320412
Epoch 1640
Epoch 1640 :: Batch 0/1
Batch Loss = 4.056658929721132
1640, epoch_train_loss=4.056658929721132
Epoch 1641
Epoch 1641 :: Batch 0/1
Batch Loss = 4.055742949596697
1641, epoch_train_loss=4.055742949596697
Epoch 1642
Epoch 1642 :: Batch 0/1
Batch Loss = 4.054337490337708
1642, epoch_train_loss=4.054337490337708
Epoch 1643
Epoch 1643 :: Batch 0/1
Batch Loss = 4.053444082935643
1643, epoch_train_loss=4.053444082935643
Epoch 1644
Epoch 1644 :: Batch 0/1
Batch Loss = 4.051249725671444
1644, epoch_train_loss=4.051249725671444
Epoch 1645
Epoch 1645 :: Batch 0/1
Batch Loss = 4.050287221714616
1645, epoch_train_loss=4.050287221714616
Epoch 1646
Epoch 1646 :: Batch 0/1
Batch Loss = 4.0471847796498235
1646, epoch_train_loss=4.0471847796498235
Epoch 1647
Epoch 1647 :: Batch 0/1
Batch Loss = 4.044142149910406
1647, epoch_train_loss=4.044142149910406
Epoch 1648
Epoch 1648 :: Batch 0/1
Batch Loss = 4.034356660244181
1648, epoch_train_loss=4.034356660244181
Epoch 1649
Epoch 1649 :: Batch 0/1
Batch Loss = 4.942828259466111
1649, epoch_train_loss=4.942828259466111
Epoch 1650
Epoch 1650 :: Batch 0/1
Batch Loss = 4.069548620030387
1650, epoch_train_loss=4.069548620030387
Epoch 1651
Epoch 1651 :: Batch 0/1
Batch Loss = 4.077871371247147
1651, epoch_train_loss=4.077871371247147
Epoch 1652
Epoch 1652 :: Batch 0/1
Batch Loss = 4.07770135348355
1652, epoch_train_loss=4.07770135348355
Epoch 1653
Epoch 1653 :: Batch 0/1
Batch Loss = 4.077658379124758
1653, epoch_train_loss=4.077658379124758
Epoch 1654
Epoch 1654 :: Batch 0/1
Batch Loss = 4.0778704739889795
1654, epoch_train_loss=4.0778704739889795
Epoch 1655
Epoch 1655 :: Batch 0/1
Batch Loss = 4.078610979746398
1655, epoch_train_loss=4.078610979746398
Epoch 1656
Epoch 1656 :: Batch 0/1
Batch Loss = 4.079022510879784
1656, epoch_train_loss=4.079022510879784
Epoch 1657
Epoch 1657 :: Batch 0/1
Batch Loss = 4.077590350241811
1657, epoch_train_loss=4.077590350241811
Epoch 1658
Epoch 1658 :: Batch 0/1
Batch Loss = 4.074758327100858
1658, epoch_train_loss=4.074758327100858
Epoch 1659
Epoch 1659 :: Batch 0/1
Batch Loss = 4.071938824746846
1659, epoch_train_loss=4.071938824746846
Epoch 1660
Epoch 1660 :: Batch 0/1
Batch Loss = 4.070577914547869
1660, epoch_train_loss=4.070577914547869
Epoch 1661
Epoch 1661 :: Batch 0/1
Batch Loss = 4.071256894970154
1661, epoch_train_loss=4.071256894970154
Epoch 1662
Epoch 1662 :: Batch 0/1
Batch Loss = 4.071140626983014
1662, epoch_train_loss=4.071140626983014
Epoch 1663
Epoch 1663 :: Batch 0/1
Batch Loss = 4.069308892237394
1663, epoch_train_loss=4.069308892237394
Epoch 1664
Epoch 1664 :: Batch 0/1
Batch Loss = 4.0689203255252995
1664, epoch_train_loss=4.0689203255252995
Epoch 1665
Epoch 1665 :: Batch 0/1
Batch Loss = 4.069114622291744
1665, epoch_train_loss=4.069114622291744
Epoch 1666
Epoch 1666 :: Batch 0/1
Batch Loss = 4.068757168593439
1666, epoch_train_loss=4.068757168593439
Epoch 1667
Epoch 1667 :: Batch 0/1
Batch Loss = 4.067912899400542
1667, epoch_train_loss=4.067912899400542
Epoch 1668
Epoch 1668 :: Batch 0/1
Batch Loss = 4.067646276436891
1668, epoch_train_loss=4.067646276436891
Epoch 1669
Epoch 1669 :: Batch 0/1
Batch Loss = 4.0678029336961545
1669, epoch_train_loss=4.0678029336961545
Epoch 1670
Epoch 1670 :: Batch 0/1
Batch Loss = 4.068609123687147
1670, epoch_train_loss=4.068609123687147
Epoch 1671
Epoch 1671 :: Batch 0/1
Batch Loss = 4.06910557038898
1671, epoch_train_loss=4.06910557038898
Epoch 1672
Epoch 1672 :: Batch 0/1
Batch Loss = 4.069306803603964
1672, epoch_train_loss=4.069306803603964
Epoch 1673
Epoch 1673 :: Batch 0/1
Batch Loss = 4.069902320028067
1673, epoch_train_loss=4.069902320028067
Epoch 1674
Epoch 1674 :: Batch 0/1
Batch Loss = 4.070250553707998
1674, epoch_train_loss=4.070250553707998
Epoch 1675
Epoch 1675 :: Batch 0/1
Batch Loss = 4.0699478600919345
1675, epoch_train_loss=4.0699478600919345
Epoch 1676
Epoch 1676 :: Batch 0/1
Batch Loss = 4.069656706277467
1676, epoch_train_loss=4.069656706277467
Epoch 1677
Epoch 1677 :: Batch 0/1
Batch Loss = 4.069594351872423
1677, epoch_train_loss=4.069594351872423
Epoch 1678
Epoch 1678 :: Batch 0/1
Batch Loss = 4.069149379883778
1678, epoch_train_loss=4.069149379883778
Epoch 1679
Epoch 1679 :: Batch 0/1
Batch Loss = 4.068486374388156
1679, epoch_train_loss=4.068486374388156
Epoch 1680
Epoch 1680 :: Batch 0/1
Batch Loss = 4.067947733432507
1680, epoch_train_loss=4.067947733432507
Epoch 1681
Epoch 1681 :: Batch 0/1
Batch Loss = 4.067358706479841
1681, epoch_train_loss=4.067358706479841
Epoch 1682
Epoch 1682 :: Batch 0/1
Batch Loss = 4.066580852785891
1682, epoch_train_loss=4.066580852785891
Epoch 1683
Epoch 1683 :: Batch 0/1
Batch Loss = 4.065710159668781
1683, epoch_train_loss=4.065710159668781
Epoch 1684
Epoch 1684 :: Batch 0/1
Batch Loss = 4.064908546423003
1684, epoch_train_loss=4.064908546423003
Epoch 1685
Epoch 1685 :: Batch 0/1
Batch Loss = 4.064162518085532
1685, epoch_train_loss=4.064162518085532
Epoch 1686
Epoch 1686 :: Batch 0/1
Batch Loss = 4.063326850791029
1686, epoch_train_loss=4.063326850791029
Epoch 1687
Epoch 1687 :: Batch 0/1
Batch Loss = 4.06241640322496
1687, epoch_train_loss=4.06241640322496
Epoch 1688
Epoch 1688 :: Batch 0/1
Batch Loss = 4.061542873222464
1688, epoch_train_loss=4.061542873222464
Epoch 1689
Epoch 1689 :: Batch 0/1
Batch Loss = 4.060701137180293
1689, epoch_train_loss=4.060701137180293
Epoch 1690
Epoch 1690 :: Batch 0/1
Batch Loss = 4.059819136892075
1690, epoch_train_loss=4.059819136892075
Epoch 1691
Epoch 1691 :: Batch 0/1
Batch Loss = 4.058934640841768
1691, epoch_train_loss=4.058934640841768
Epoch 1692
Epoch 1692 :: Batch 0/1
Batch Loss = 4.0582312837579
1692, epoch_train_loss=4.0582312837579
Epoch 1693
Epoch 1693 :: Batch 0/1
Batch Loss = 4.057676978545333
1693, epoch_train_loss=4.057676978545333
Epoch 1694
Epoch 1694 :: Batch 0/1
Batch Loss = 4.056961739081586
1694, epoch_train_loss=4.056961739081586
Epoch 1695
Epoch 1695 :: Batch 0/1
Batch Loss = 4.056404836958662
1695, epoch_train_loss=4.056404836958662
Epoch 1696
Epoch 1696 :: Batch 0/1
Batch Loss = 4.0558187491019
1696, epoch_train_loss=4.0558187491019
Epoch 1697
Epoch 1697 :: Batch 0/1
Batch Loss = 4.055019049617932
1697, epoch_train_loss=4.055019049617932
Epoch 1698
Epoch 1698 :: Batch 0/1
Batch Loss = 4.054409439221114
1698, epoch_train_loss=4.054409439221114
Epoch 1699
Epoch 1699 :: Batch 0/1
Batch Loss = 4.053636221923787
1699, epoch_train_loss=4.053636221923787
Epoch 1700
Epoch 1700 :: Batch 0/1
Batch Loss = 4.052930995885493
1700, epoch_train_loss=4.052930995885493
Epoch 1701
Epoch 1701 :: Batch 0/1
Batch Loss = 4.052240879053764
1701, epoch_train_loss=4.052240879053764
Epoch 1702
Epoch 1702 :: Batch 0/1
Batch Loss = 4.0514705605631685
1702, epoch_train_loss=4.0514705605631685
Epoch 1703
Epoch 1703 :: Batch 0/1
Batch Loss = 4.050863621391025
1703, epoch_train_loss=4.050863621391025
Epoch 1704
Epoch 1704 :: Batch 0/1
Batch Loss = 4.0501381306949
1704, epoch_train_loss=4.0501381306949
Epoch 1705
Epoch 1705 :: Batch 0/1
Batch Loss = 4.049542589270866
1705, epoch_train_loss=4.049542589270866
Epoch 1706
Epoch 1706 :: Batch 0/1
Batch Loss = 4.048846142506133
1706, epoch_train_loss=4.048846142506133
Epoch 1707
Epoch 1707 :: Batch 0/1
Batch Loss = 4.048288123012981
1707, epoch_train_loss=4.048288123012981
Epoch 1708
Epoch 1708 :: Batch 0/1
Batch Loss = 4.0476296897227195
1708, epoch_train_loss=4.0476296897227195
Epoch 1709
Epoch 1709 :: Batch 0/1
Batch Loss = 4.047071683727113
1709, epoch_train_loss=4.047071683727113
Epoch 1710
Epoch 1710 :: Batch 0/1
Batch Loss = 4.046563278171734
1710, epoch_train_loss=4.046563278171734
Epoch 1711
Epoch 1711 :: Batch 0/1
Batch Loss = 4.0460067118087615
1711, epoch_train_loss=4.0460067118087615
Epoch 1712
Epoch 1712 :: Batch 0/1
Batch Loss = 4.0452953932225375
1712, epoch_train_loss=4.0452953932225375
Epoch 1713
Epoch 1713 :: Batch 0/1
Batch Loss = 4.044647859772291
1713, epoch_train_loss=4.044647859772291
Epoch 1714
Epoch 1714 :: Batch 0/1
Batch Loss = 4.044047759122751
1714, epoch_train_loss=4.044047759122751
Epoch 1715
Epoch 1715 :: Batch 0/1
Batch Loss = 4.043455329410117
1715, epoch_train_loss=4.043455329410117
Epoch 1716
Epoch 1716 :: Batch 0/1
Batch Loss = 4.042910386531369
1716, epoch_train_loss=4.042910386531369
Epoch 1717
Epoch 1717 :: Batch 0/1
Batch Loss = 4.042319644502752
1717, epoch_train_loss=4.042319644502752
Epoch 1718
Epoch 1718 :: Batch 0/1
Batch Loss = 4.041751563909982
1718, epoch_train_loss=4.041751563909982
Epoch 1719
Epoch 1719 :: Batch 0/1
Batch Loss = 4.041240446176552
1719, epoch_train_loss=4.041240446176552
Epoch 1720
Epoch 1720 :: Batch 0/1
Batch Loss = 4.040752072290162
1720, epoch_train_loss=4.040752072290162
Epoch 1721
Epoch 1721 :: Batch 0/1
Batch Loss = 4.040396153720907
1721, epoch_train_loss=4.040396153720907
Epoch 1722
Epoch 1722 :: Batch 0/1
Batch Loss = 4.0397829784289145
1722, epoch_train_loss=4.0397829784289145
Epoch 1723
Epoch 1723 :: Batch 0/1
Batch Loss = 4.039181580442549
1723, epoch_train_loss=4.039181580442549
Epoch 1724
Epoch 1724 :: Batch 0/1
Batch Loss = 4.038315422743416
1724, epoch_train_loss=4.038315422743416
Epoch 1725
Epoch 1725 :: Batch 0/1
Batch Loss = 4.037996743305465
1725, epoch_train_loss=4.037996743305465
Epoch 1726
Epoch 1726 :: Batch 0/1
Batch Loss = 4.037486224505161
1726, epoch_train_loss=4.037486224505161
Epoch 1727
Epoch 1727 :: Batch 0/1
Batch Loss = 4.036568306458237
1727, epoch_train_loss=4.036568306458237
Epoch 1728
Epoch 1728 :: Batch 0/1
Batch Loss = 4.036379951978533
1728, epoch_train_loss=4.036379951978533
Epoch 1729
Epoch 1729 :: Batch 0/1
Batch Loss = 4.035458388295229
1729, epoch_train_loss=4.035458388295229
Epoch 1730
Epoch 1730 :: Batch 0/1
Batch Loss = 4.034934580016406
1730, epoch_train_loss=4.034934580016406
Epoch 1731
Epoch 1731 :: Batch 0/1
Batch Loss = 4.0342193920094696
1731, epoch_train_loss=4.0342193920094696
Epoch 1732
Epoch 1732 :: Batch 0/1
Batch Loss = 4.033485511343721
1732, epoch_train_loss=4.033485511343721
Epoch 1733
Epoch 1733 :: Batch 0/1
Batch Loss = 4.032915237350259
1733, epoch_train_loss=4.032915237350259
Epoch 1734
Epoch 1734 :: Batch 0/1
Batch Loss = 4.032041707449188
1734, epoch_train_loss=4.032041707449188
Epoch 1735
Epoch 1735 :: Batch 0/1
Batch Loss = 4.031506722796415
1735, epoch_train_loss=4.031506722796415
Epoch 1736
Epoch 1736 :: Batch 0/1
Batch Loss = 4.030518126073487
1736, epoch_train_loss=4.030518126073487
Epoch 1737
Epoch 1737 :: Batch 0/1
Batch Loss = 4.029849462166509
1737, epoch_train_loss=4.029849462166509
Epoch 1738
Epoch 1738 :: Batch 0/1
Batch Loss = 4.028891409138566
1738, epoch_train_loss=4.028891409138566
Epoch 1739
Epoch 1739 :: Batch 0/1
Batch Loss = 4.028046342105545
1739, epoch_train_loss=4.028046342105545
Epoch 1740
Epoch 1740 :: Batch 0/1
Batch Loss = 4.0270520350893335
1740, epoch_train_loss=4.0270520350893335
Epoch 1741
Epoch 1741 :: Batch 0/1
Batch Loss = 4.02600249532746
1741, epoch_train_loss=4.02600249532746
Epoch 1742
Epoch 1742 :: Batch 0/1
Batch Loss = 4.024897939611654
1742, epoch_train_loss=4.024897939611654
Epoch 1743
Epoch 1743 :: Batch 0/1
Batch Loss = 4.023632407407757
1743, epoch_train_loss=4.023632407407757
Epoch 1744
Epoch 1744 :: Batch 0/1
Batch Loss = 4.022338350713491
1744, epoch_train_loss=4.022338350713491
Epoch 1745
Epoch 1745 :: Batch 0/1
Batch Loss = 4.020824775095763
1745, epoch_train_loss=4.020824775095763
Epoch 1746
Epoch 1746 :: Batch 0/1
Batch Loss = 4.019196167441518
1746, epoch_train_loss=4.019196167441518
Epoch 1747
Epoch 1747 :: Batch 0/1
Batch Loss = 4.017201055399739
1747, epoch_train_loss=4.017201055399739
Epoch 1748
Epoch 1748 :: Batch 0/1
Batch Loss = 4.015023598674905
1748, epoch_train_loss=4.015023598674905
Epoch 1749
Epoch 1749 :: Batch 0/1
Batch Loss = 4.0125051810695185
1749, epoch_train_loss=4.0125051810695185
Epoch 1750
Epoch 1750 :: Batch 0/1
Batch Loss = 4.009976577834519
1750, epoch_train_loss=4.009976577834519
Epoch 1751
Epoch 1751 :: Batch 0/1
Batch Loss = 4.007550666016282
1751, epoch_train_loss=4.007550666016282
Epoch 1752
Epoch 1752 :: Batch 0/1
Batch Loss = 4.00535285362651
1752, epoch_train_loss=4.00535285362651
Epoch 1753
Epoch 1753 :: Batch 0/1
Batch Loss = 4.002738618568055
1753, epoch_train_loss=4.002738618568055
Epoch 1754
Epoch 1754 :: Batch 0/1
Batch Loss = 3.999602612083794
1754, epoch_train_loss=3.999602612083794
Epoch 1755
Epoch 1755 :: Batch 0/1
Batch Loss = 3.9969540279477975
1755, epoch_train_loss=3.9969540279477975
Epoch 1756
Epoch 1756 :: Batch 0/1
Batch Loss = 3.9950797984676334
1756, epoch_train_loss=3.9950797984676334
Epoch 1757
Epoch 1757 :: Batch 0/1
Batch Loss = 3.993421797935861
1757, epoch_train_loss=3.993421797935861
Epoch 1758
Epoch 1758 :: Batch 0/1
Batch Loss = 3.991632757098339
1758, epoch_train_loss=3.991632757098339
Epoch 1759
Epoch 1759 :: Batch 0/1
Batch Loss = 3.990402577283706
1759, epoch_train_loss=3.990402577283706
Epoch 1760
Epoch 1760 :: Batch 0/1
Batch Loss = 3.989732509272422
1760, epoch_train_loss=3.989732509272422
Epoch 1761
Epoch 1761 :: Batch 0/1
Batch Loss = 3.9885618808231964
1761, epoch_train_loss=3.9885618808231964
Epoch 1762
Epoch 1762 :: Batch 0/1
Batch Loss = 3.987682559278811
1762, epoch_train_loss=3.987682559278811
Epoch 1763
Epoch 1763 :: Batch 0/1
Batch Loss = 3.987198852665689
1763, epoch_train_loss=3.987198852665689
Epoch 1764
Epoch 1764 :: Batch 0/1
Batch Loss = 3.986329493526599
1764, epoch_train_loss=3.986329493526599
Epoch 1765
Epoch 1765 :: Batch 0/1
Batch Loss = 3.9855774119955765
1765, epoch_train_loss=3.9855774119955765
Epoch 1766
Epoch 1766 :: Batch 0/1
Batch Loss = 3.9851057499836773
1766, epoch_train_loss=3.9851057499836773
Epoch 1767
Epoch 1767 :: Batch 0/1
Batch Loss = 3.9842550921362574
1767, epoch_train_loss=3.9842550921362574
Epoch 1768
Epoch 1768 :: Batch 0/1
Batch Loss = 3.9835902892069086
1768, epoch_train_loss=3.9835902892069086
Epoch 1769
Epoch 1769 :: Batch 0/1
Batch Loss = 3.9830032792014665
1769, epoch_train_loss=3.9830032792014665
Epoch 1770
Epoch 1770 :: Batch 0/1
Batch Loss = 3.9822076604202743
1770, epoch_train_loss=3.9822076604202743
Epoch 1771
Epoch 1771 :: Batch 0/1
Batch Loss = 3.981556938173121
1771, epoch_train_loss=3.981556938173121
Epoch 1772
Epoch 1772 :: Batch 0/1
Batch Loss = 3.9809172694742636
1772, epoch_train_loss=3.9809172694742636
Epoch 1773
Epoch 1773 :: Batch 0/1
Batch Loss = 3.9801519048859255
1773, epoch_train_loss=3.9801519048859255
Epoch 1774
Epoch 1774 :: Batch 0/1
Batch Loss = 3.9795207616745167
1774, epoch_train_loss=3.9795207616745167
Epoch 1775
Epoch 1775 :: Batch 0/1
Batch Loss = 3.978885648541095
1775, epoch_train_loss=3.978885648541095
Epoch 1776
Epoch 1776 :: Batch 0/1
Batch Loss = 3.9781694664013294
1776, epoch_train_loss=3.9781694664013294
Epoch 1777
Epoch 1777 :: Batch 0/1
Batch Loss = 3.9775397620363884
1777, epoch_train_loss=3.9775397620363884
Epoch 1778
Epoch 1778 :: Batch 0/1
Batch Loss = 3.976880904103394
1778, epoch_train_loss=3.976880904103394
Epoch 1779
Epoch 1779 :: Batch 0/1
Batch Loss = 3.9761731669566984
1779, epoch_train_loss=3.9761731669566984
Epoch 1780
Epoch 1780 :: Batch 0/1
Batch Loss = 3.975545467393046
1780, epoch_train_loss=3.975545467393046
Epoch 1781
Epoch 1781 :: Batch 0/1
Batch Loss = 3.9748378438187157
1781, epoch_train_loss=3.9748378438187157
Epoch 1782
Epoch 1782 :: Batch 0/1
Batch Loss = 3.974148538960982
1782, epoch_train_loss=3.974148538960982
Epoch 1783
Epoch 1783 :: Batch 0/1
Batch Loss = 3.9735190619862477
1783, epoch_train_loss=3.9735190619862477
Epoch 1784
Epoch 1784 :: Batch 0/1
Batch Loss = 3.9728843328490795
1784, epoch_train_loss=3.9728843328490795
Epoch 1785
Epoch 1785 :: Batch 0/1
Batch Loss = 3.9722591211575895
1785, epoch_train_loss=3.9722591211575895
Epoch 1786
Epoch 1786 :: Batch 0/1
Batch Loss = 3.971686840324896
1786, epoch_train_loss=3.971686840324896
Epoch 1787
Epoch 1787 :: Batch 0/1
Batch Loss = 3.971062680266134
1787, epoch_train_loss=3.971062680266134
Epoch 1788
Epoch 1788 :: Batch 0/1
Batch Loss = 3.970478259500287
1788, epoch_train_loss=3.970478259500287
Epoch 1789
Epoch 1789 :: Batch 0/1
Batch Loss = 3.9699253445669362
1789, epoch_train_loss=3.9699253445669362
Epoch 1790
Epoch 1790 :: Batch 0/1
Batch Loss = 3.9693722596015393
1790, epoch_train_loss=3.9693722596015393
Epoch 1791
Epoch 1791 :: Batch 0/1
Batch Loss = 3.9688560201971774
1791, epoch_train_loss=3.9688560201971774
Epoch 1792
Epoch 1792 :: Batch 0/1
Batch Loss = 3.9683654121941054
1792, epoch_train_loss=3.9683654121941054
Epoch 1793
Epoch 1793 :: Batch 0/1
Batch Loss = 3.9678576535197236
1793, epoch_train_loss=3.9678576535197236
Epoch 1794
Epoch 1794 :: Batch 0/1
Batch Loss = 3.9673743490200173
1794, epoch_train_loss=3.9673743490200173
Epoch 1795
Epoch 1795 :: Batch 0/1
Batch Loss = 3.9668778553662447
1795, epoch_train_loss=3.9668778553662447
Epoch 1796
Epoch 1796 :: Batch 0/1
Batch Loss = 3.9663672516957127
1796, epoch_train_loss=3.9663672516957127
Epoch 1797
Epoch 1797 :: Batch 0/1
Batch Loss = 3.9658773016752584
1797, epoch_train_loss=3.9658773016752584
Epoch 1798
Epoch 1798 :: Batch 0/1
Batch Loss = 3.9653874800009015
1798, epoch_train_loss=3.9653874800009015
Epoch 1799
Epoch 1799 :: Batch 0/1
Batch Loss = 3.964905712520933
1799, epoch_train_loss=3.964905712520933
Epoch 1800
Epoch 1800 :: Batch 0/1
Batch Loss = 3.9644357217493074
1800, epoch_train_loss=3.9644357217493074
Epoch 1801
Epoch 1801 :: Batch 0/1
Batch Loss = 3.963958659331732
1801, epoch_train_loss=3.963958659331732
Epoch 1802
Epoch 1802 :: Batch 0/1
Batch Loss = 3.9634782829802884
1802, epoch_train_loss=3.9634782829802884
Epoch 1803
Epoch 1803 :: Batch 0/1
Batch Loss = 3.963004604177521
1803, epoch_train_loss=3.963004604177521
Epoch 1804
Epoch 1804 :: Batch 0/1
Batch Loss = 3.9625249368265223
1804, epoch_train_loss=3.9625249368265223
Epoch 1805
Epoch 1805 :: Batch 0/1
Batch Loss = 3.9620484885515137
1805, epoch_train_loss=3.9620484885515137
Epoch 1806
Epoch 1806 :: Batch 0/1
Batch Loss = 3.961576616139332
1806, epoch_train_loss=3.961576616139332
Epoch 1807
Epoch 1807 :: Batch 0/1
Batch Loss = 3.961097383304526
1807, epoch_train_loss=3.961097383304526
Epoch 1808
Epoch 1808 :: Batch 0/1
Batch Loss = 3.9606182601844147
1808, epoch_train_loss=3.9606182601844147
Epoch 1809
Epoch 1809 :: Batch 0/1
Batch Loss = 3.9601314534631853
1809, epoch_train_loss=3.9601314534631853
Epoch 1810
Epoch 1810 :: Batch 0/1
Batch Loss = 3.9596362210562672
1810, epoch_train_loss=3.9596362210562672
Epoch 1811
Epoch 1811 :: Batch 0/1
Batch Loss = 3.9591429431770586
1811, epoch_train_loss=3.9591429431770586
Epoch 1812
Epoch 1812 :: Batch 0/1
Batch Loss = 3.9586464910839974
1812, epoch_train_loss=3.9586464910839974
Epoch 1813
Epoch 1813 :: Batch 0/1
Batch Loss = 3.9581515840763926
1813, epoch_train_loss=3.9581515840763926
Epoch 1814
Epoch 1814 :: Batch 0/1
Batch Loss = 3.957660656545118
1814, epoch_train_loss=3.957660656545118
Epoch 1815
Epoch 1815 :: Batch 0/1
Batch Loss = 3.957160837564801
1815, epoch_train_loss=3.957160837564801
Epoch 1816
Epoch 1816 :: Batch 0/1
Batch Loss = 3.9566555688007674
1816, epoch_train_loss=3.9566555688007674
Epoch 1817
Epoch 1817 :: Batch 0/1
Batch Loss = 3.9561458495673163
1817, epoch_train_loss=3.9561458495673163
Epoch 1818
Epoch 1818 :: Batch 0/1
Batch Loss = 3.95563352627668
1818, epoch_train_loss=3.95563352627668
Epoch 1819
Epoch 1819 :: Batch 0/1
Batch Loss = 3.955123754120879
1819, epoch_train_loss=3.955123754120879
Epoch 1820
Epoch 1820 :: Batch 0/1
Batch Loss = 3.9546150984637176
1820, epoch_train_loss=3.9546150984637176
Epoch 1821
Epoch 1821 :: Batch 0/1
Batch Loss = 3.954110671155833
1821, epoch_train_loss=3.954110671155833
Epoch 1822
Epoch 1822 :: Batch 0/1
Batch Loss = 3.9536134719123677
1822, epoch_train_loss=3.9536134719123677
Epoch 1823
Epoch 1823 :: Batch 0/1
Batch Loss = 3.9531225305618705
1823, epoch_train_loss=3.9531225305618705
Epoch 1824
Epoch 1824 :: Batch 0/1
Batch Loss = 3.952640111997177
1824, epoch_train_loss=3.952640111997177
Epoch 1825
Epoch 1825 :: Batch 0/1
Batch Loss = 3.95216399729876
1825, epoch_train_loss=3.95216399729876
Epoch 1826
Epoch 1826 :: Batch 0/1
Batch Loss = 3.9516912925420846
1826, epoch_train_loss=3.9516912925420846
Epoch 1827
Epoch 1827 :: Batch 0/1
Batch Loss = 3.9512199692076173
1827, epoch_train_loss=3.9512199692076173
Epoch 1828
Epoch 1828 :: Batch 0/1
Batch Loss = 3.950746272653529
1828, epoch_train_loss=3.950746272653529
Epoch 1829
Epoch 1829 :: Batch 0/1
Batch Loss = 3.950271396552008
1829, epoch_train_loss=3.950271396552008
Epoch 1830
Epoch 1830 :: Batch 0/1
Batch Loss = 3.949796002017868
1830, epoch_train_loss=3.949796002017868
Epoch 1831
Epoch 1831 :: Batch 0/1
Batch Loss = 3.9493207428984776
1831, epoch_train_loss=3.9493207428984776
Epoch 1832
Epoch 1832 :: Batch 0/1
Batch Loss = 3.948847050925807
1832, epoch_train_loss=3.948847050925807
Epoch 1833
Epoch 1833 :: Batch 0/1
Batch Loss = 3.9483811281374734
1833, epoch_train_loss=3.9483811281374734
Epoch 1834
Epoch 1834 :: Batch 0/1
Batch Loss = 3.9479330017319314
1834, epoch_train_loss=3.9479330017319314
Epoch 1835
Epoch 1835 :: Batch 0/1
Batch Loss = 3.947562341684461
1835, epoch_train_loss=3.947562341684461
Epoch 1836
Epoch 1836 :: Batch 0/1
Batch Loss = 3.947306245244115
1836, epoch_train_loss=3.947306245244115
Epoch 1837
Epoch 1837 :: Batch 0/1
Batch Loss = 3.947385207173449
1837, epoch_train_loss=3.947385207173449
Epoch 1838
Epoch 1838 :: Batch 0/1
Batch Loss = 3.946384983610193
1838, epoch_train_loss=3.946384983610193
Epoch 1839
Epoch 1839 :: Batch 0/1
Batch Loss = 3.945494354517406
1839, epoch_train_loss=3.945494354517406
Epoch 1840
Epoch 1840 :: Batch 0/1
Batch Loss = 3.9451991262967914
1840, epoch_train_loss=3.9451991262967914
Epoch 1841
Epoch 1841 :: Batch 0/1
Batch Loss = 3.9447719750127743
1841, epoch_train_loss=3.9447719750127743
Epoch 1842
Epoch 1842 :: Batch 0/1
Batch Loss = 3.9440567815590013
1842, epoch_train_loss=3.9440567815590013
Epoch 1843
Epoch 1843 :: Batch 0/1
Batch Loss = 3.9435029823563923
1843, epoch_train_loss=3.9435029823563923
Epoch 1844
Epoch 1844 :: Batch 0/1
Batch Loss = 3.943162972493261
1844, epoch_train_loss=3.943162972493261
Epoch 1845
Epoch 1845 :: Batch 0/1
Batch Loss = 3.942579498162346
1845, epoch_train_loss=3.942579498162346
Epoch 1846
Epoch 1846 :: Batch 0/1
Batch Loss = 3.9419554607258185
1846, epoch_train_loss=3.9419554607258185
Epoch 1847
Epoch 1847 :: Batch 0/1
Batch Loss = 3.941618153818415
1847, epoch_train_loss=3.941618153818415
Epoch 1848
Epoch 1848 :: Batch 0/1
Batch Loss = 3.9411061361830324
1848, epoch_train_loss=3.9411061361830324
Epoch 1849
Epoch 1849 :: Batch 0/1
Batch Loss = 3.9404305588111135
1849, epoch_train_loss=3.9404305588111135
Epoch 1850
Epoch 1850 :: Batch 0/1
Batch Loss = 3.9400924641867126
1850, epoch_train_loss=3.9400924641867126
Epoch 1851
Epoch 1851 :: Batch 0/1
Batch Loss = 3.9395892337153082
1851, epoch_train_loss=3.9395892337153082
Epoch 1852
Epoch 1852 :: Batch 0/1
Batch Loss = 3.9389280256840338
1852, epoch_train_loss=3.9389280256840338
Epoch 1853
Epoch 1853 :: Batch 0/1
Batch Loss = 3.9386009136862294
1853, epoch_train_loss=3.9386009136862294
Epoch 1854
Epoch 1854 :: Batch 0/1
Batch Loss = 3.9380672686376723
1854, epoch_train_loss=3.9380672686376723
Epoch 1855
Epoch 1855 :: Batch 0/1
Batch Loss = 3.937410486377569
1855, epoch_train_loss=3.937410486377569
Epoch 1856
Epoch 1856 :: Batch 0/1
Batch Loss = 3.937100091041111
1856, epoch_train_loss=3.937100091041111
Epoch 1857
Epoch 1857 :: Batch 0/1
Batch Loss = 3.936600025137618
1857, epoch_train_loss=3.936600025137618
Epoch 1858
Epoch 1858 :: Batch 0/1
Batch Loss = 3.935918925095993
1858, epoch_train_loss=3.935918925095993
Epoch 1859
Epoch 1859 :: Batch 0/1
Batch Loss = 3.9356212976719616
1859, epoch_train_loss=3.9356212976719616
Epoch 1860
Epoch 1860 :: Batch 0/1
Batch Loss = 3.9351676119650794
1860, epoch_train_loss=3.9351676119650794
Epoch 1861
Epoch 1861 :: Batch 0/1
Batch Loss = 3.93445973519825
1861, epoch_train_loss=3.93445973519825
Epoch 1862
Epoch 1862 :: Batch 0/1
Batch Loss = 3.9341758954786865
1862, epoch_train_loss=3.9341758954786865
Epoch 1863
Epoch 1863 :: Batch 0/1
Batch Loss = 3.933783074270747
1863, epoch_train_loss=3.933783074270747
Epoch 1864
Epoch 1864 :: Batch 0/1
Batch Loss = 3.9330307434558835
1864, epoch_train_loss=3.9330307434558835
Epoch 1865
Epoch 1865 :: Batch 0/1
Batch Loss = 3.9327458379288434
1865, epoch_train_loss=3.9327458379288434
Epoch 1866
Epoch 1866 :: Batch 0/1
Batch Loss = 3.9324136533513943
1866, epoch_train_loss=3.9324136533513943
Epoch 1867
Epoch 1867 :: Batch 0/1
Batch Loss = 3.9316072580547092
1867, epoch_train_loss=3.9316072580547092
Epoch 1868
Epoch 1868 :: Batch 0/1
Batch Loss = 3.9312833038879793
1868, epoch_train_loss=3.9312833038879793
Epoch 1869
Epoch 1869 :: Batch 0/1
Batch Loss = 3.931026279922511
1869, epoch_train_loss=3.931026279922511
Epoch 1870
Epoch 1870 :: Batch 0/1
Batch Loss = 3.9301991574747364
1870, epoch_train_loss=3.9301991574747364
Epoch 1871
Epoch 1871 :: Batch 0/1
Batch Loss = 3.9298007686721856
1871, epoch_train_loss=3.9298007686721856
Epoch 1872
Epoch 1872 :: Batch 0/1
Batch Loss = 3.9295669202931434
1872, epoch_train_loss=3.9295669202931434
Epoch 1873
Epoch 1873 :: Batch 0/1
Batch Loss = 3.9287889912772807
1873, epoch_train_loss=3.9287889912772807
Epoch 1874
Epoch 1874 :: Batch 0/1
Batch Loss = 3.9283008138315356
1874, epoch_train_loss=3.9283008138315356
Epoch 1875
Epoch 1875 :: Batch 0/1
Batch Loss = 3.9280412757053647
1875, epoch_train_loss=3.9280412757053647
Epoch 1876
Epoch 1876 :: Batch 0/1
Batch Loss = 3.9273910056468946
1876, epoch_train_loss=3.9273910056468946
Epoch 1877
Epoch 1877 :: Batch 0/1
Batch Loss = 3.9268309649924467
1877, epoch_train_loss=3.9268309649924467
Epoch 1878
Epoch 1878 :: Batch 0/1
Batch Loss = 3.926496410839558
1878, epoch_train_loss=3.926496410839558
Epoch 1879
Epoch 1879 :: Batch 0/1
Batch Loss = 3.925993784262531
1879, epoch_train_loss=3.925993784262531
Epoch 1880
Epoch 1880 :: Batch 0/1
Batch Loss = 3.925414439747082
1880, epoch_train_loss=3.925414439747082
Epoch 1881
Epoch 1881 :: Batch 0/1
Batch Loss = 3.9249587628829286
1881, epoch_train_loss=3.9249587628829286
Epoch 1882
Epoch 1882 :: Batch 0/1
Batch Loss = 3.9245522622895987
1882, epoch_train_loss=3.9245522622895987
Epoch 1883
Epoch 1883 :: Batch 0/1
Batch Loss = 3.9240536213637083
1883, epoch_train_loss=3.9240536213637083
Epoch 1884
Epoch 1884 :: Batch 0/1
Batch Loss = 3.9235088129601734
1884, epoch_train_loss=3.9235088129601734
Epoch 1885
Epoch 1885 :: Batch 0/1
Batch Loss = 3.923060732536787
1885, epoch_train_loss=3.923060732536787
Epoch 1886
Epoch 1886 :: Batch 0/1
Batch Loss = 3.922636546285956
1886, epoch_train_loss=3.922636546285956
Epoch 1887
Epoch 1887 :: Batch 0/1
Batch Loss = 3.9221193168574833
1887, epoch_train_loss=3.9221193168574833
Epoch 1888
Epoch 1888 :: Batch 0/1
Batch Loss = 3.9215887271913488
1888, epoch_train_loss=3.9215887271913488
Epoch 1889
Epoch 1889 :: Batch 0/1
Batch Loss = 3.9211094708827807
1889, epoch_train_loss=3.9211094708827807
Epoch 1890
Epoch 1890 :: Batch 0/1
Batch Loss = 3.9206681676365913
1890, epoch_train_loss=3.9206681676365913
Epoch 1891
Epoch 1891 :: Batch 0/1
Batch Loss = 3.9202245924545163
1891, epoch_train_loss=3.9202245924545163
Epoch 1892
Epoch 1892 :: Batch 0/1
Batch Loss = 3.9197376582522003
1892, epoch_train_loss=3.9197376582522003
Epoch 1893
Epoch 1893 :: Batch 0/1
Batch Loss = 3.9192411857548
1893, epoch_train_loss=3.9192411857548
Epoch 1894
Epoch 1894 :: Batch 0/1
Batch Loss = 3.9187284411136027
1894, epoch_train_loss=3.9187284411136027
Epoch 1895
Epoch 1895 :: Batch 0/1
Batch Loss = 3.9182275717526185
1895, epoch_train_loss=3.9182275717526185
Epoch 1896
Epoch 1896 :: Batch 0/1
Batch Loss = 3.9177320719101614
1896, epoch_train_loss=3.9177320719101614
Epoch 1897
Epoch 1897 :: Batch 0/1
Batch Loss = 3.9172429498227235
1897, epoch_train_loss=3.9172429498227235
Epoch 1898
Epoch 1898 :: Batch 0/1
Batch Loss = 3.9167581131443505
1898, epoch_train_loss=3.9167581131443505
Epoch 1899
Epoch 1899 :: Batch 0/1
Batch Loss = 3.9162812526710225
1899, epoch_train_loss=3.9162812526710225
Epoch 1900
Epoch 1900 :: Batch 0/1
Batch Loss = 3.9158271010687793
1900, epoch_train_loss=3.9158271010687793
Epoch 1901
Epoch 1901 :: Batch 0/1
Batch Loss = 3.915470455869152
1901, epoch_train_loss=3.915470455869152
Epoch 1902
Epoch 1902 :: Batch 0/1
Batch Loss = 3.915380318640598
1902, epoch_train_loss=3.915380318640598
Epoch 1903
Epoch 1903 :: Batch 0/1
Batch Loss = 3.9164128715306568
1903, epoch_train_loss=3.9164128715306568
Epoch 1904
Epoch 1904 :: Batch 0/1
Batch Loss = 3.9164447540520424
1904, epoch_train_loss=3.9164447540520424
Epoch 1905
Epoch 1905 :: Batch 0/1
Batch Loss = 3.9160067190969636
1905, epoch_train_loss=3.9160067190969636
Epoch 1906
Epoch 1906 :: Batch 0/1
Batch Loss = 3.9130774524732894
1906, epoch_train_loss=3.9130774524732894
Epoch 1907
Epoch 1907 :: Batch 0/1
Batch Loss = 3.9154214139001793
1907, epoch_train_loss=3.9154214139001793
Epoch 1908
Epoch 1908 :: Batch 0/1
Batch Loss = 3.9140878853668117
1908, epoch_train_loss=3.9140878853668117
Epoch 1909
Epoch 1909 :: Batch 0/1
Batch Loss = 3.9127044203527857
1909, epoch_train_loss=3.9127044203527857
Epoch 1910
Epoch 1910 :: Batch 0/1
Batch Loss = 3.9135903278288153
1910, epoch_train_loss=3.9135903278288153
Epoch 1911
Epoch 1911 :: Batch 0/1
Batch Loss = 3.910567050840641
1911, epoch_train_loss=3.910567050840641
Epoch 1912
Epoch 1912 :: Batch 0/1
Batch Loss = 3.9116139654105475
1912, epoch_train_loss=3.9116139654105475
Epoch 1913
Epoch 1913 :: Batch 0/1
Batch Loss = 3.909517669694892
1913, epoch_train_loss=3.909517669694892
Epoch 1914
Epoch 1914 :: Batch 0/1
Batch Loss = 3.9106510048550547
1914, epoch_train_loss=3.9106510048550547
Epoch 1915
Epoch 1915 :: Batch 0/1
Batch Loss = 3.908721690906842
1915, epoch_train_loss=3.908721690906842
Epoch 1916
Epoch 1916 :: Batch 0/1
Batch Loss = 3.909088557600172
1916, epoch_train_loss=3.909088557600172
Epoch 1917
Epoch 1917 :: Batch 0/1
Batch Loss = 3.907565062917144
1917, epoch_train_loss=3.907565062917144
Epoch 1918
Epoch 1918 :: Batch 0/1
Batch Loss = 3.907823471714534
1918, epoch_train_loss=3.907823471714534
Epoch 1919
Epoch 1919 :: Batch 0/1
Batch Loss = 3.9071838881779617
1919, epoch_train_loss=3.9071838881779617
Epoch 1920
Epoch 1920 :: Batch 0/1
Batch Loss = 3.906464193348828
1920, epoch_train_loss=3.906464193348828
Epoch 1921
Epoch 1921 :: Batch 0/1
Batch Loss = 3.9062039806802313
1921, epoch_train_loss=3.9062039806802313
Epoch 1922
Epoch 1922 :: Batch 0/1
Batch Loss = 3.905059851304026
1922, epoch_train_loss=3.905059851304026
Epoch 1923
Epoch 1923 :: Batch 0/1
Batch Loss = 3.9050325139620434
1923, epoch_train_loss=3.9050325139620434
Epoch 1924
Epoch 1924 :: Batch 0/1
Batch Loss = 3.9040190499693805
1924, epoch_train_loss=3.9040190499693805
Epoch 1925
Epoch 1925 :: Batch 0/1
Batch Loss = 3.9040168804671636
1925, epoch_train_loss=3.9040168804671636
Epoch 1926
Epoch 1926 :: Batch 0/1
Batch Loss = 3.9028948968168504
1926, epoch_train_loss=3.9028948968168504
Epoch 1927
Epoch 1927 :: Batch 0/1
Batch Loss = 3.902733306118041
1927, epoch_train_loss=3.902733306118041
Epoch 1928
Epoch 1928 :: Batch 0/1
Batch Loss = 3.90183866018485
1928, epoch_train_loss=3.90183866018485
Epoch 1929
Epoch 1929 :: Batch 0/1
Batch Loss = 3.9014718764585927
1929, epoch_train_loss=3.9014718764585927
Epoch 1930
Epoch 1930 :: Batch 0/1
Batch Loss = 3.90089589449359
1930, epoch_train_loss=3.90089589449359
Epoch 1931
Epoch 1931 :: Batch 0/1
Batch Loss = 3.9003000748780545
1931, epoch_train_loss=3.9003000748780545
Epoch 1932
Epoch 1932 :: Batch 0/1
Batch Loss = 3.8999202773482016
1932, epoch_train_loss=3.8999202773482016
Epoch 1933
Epoch 1933 :: Batch 0/1
Batch Loss = 3.899126661258701
1933, epoch_train_loss=3.899126661258701
Epoch 1934
Epoch 1934 :: Batch 0/1
Batch Loss = 3.8988275226680456
1934, epoch_train_loss=3.8988275226680456
Epoch 1935
Epoch 1935 :: Batch 0/1
Batch Loss = 3.8981814598263216
1935, epoch_train_loss=3.8981814598263216
Epoch 1936
Epoch 1936 :: Batch 0/1
Batch Loss = 3.8976310410341313
1936, epoch_train_loss=3.8976310410341313
Epoch 1937
Epoch 1937 :: Batch 0/1
Batch Loss = 3.897221031821863
1937, epoch_train_loss=3.897221031821863
Epoch 1938
Epoch 1938 :: Batch 0/1
Batch Loss = 3.8965664702547014
1938, epoch_train_loss=3.8965664702547014
Epoch 1939
Epoch 1939 :: Batch 0/1
Batch Loss = 3.896153000146754
1939, epoch_train_loss=3.896153000146754
Epoch 1940
Epoch 1940 :: Batch 0/1
Batch Loss = 3.8955921167753873
1940, epoch_train_loss=3.8955921167753873
Epoch 1941
Epoch 1941 :: Batch 0/1
Batch Loss = 3.89499558057992
1941, epoch_train_loss=3.89499558057992
Epoch 1942
Epoch 1942 :: Batch 0/1
Batch Loss = 3.894574102238892
1942, epoch_train_loss=3.894574102238892
Epoch 1943
Epoch 1943 :: Batch 0/1
Batch Loss = 3.89396334692213
1943, epoch_train_loss=3.89396334692213
Epoch 1944
Epoch 1944 :: Batch 0/1
Batch Loss = 3.8933910681343615
1944, epoch_train_loss=3.8933910681343615
Epoch 1945
Epoch 1945 :: Batch 0/1
Batch Loss = 3.892934908606668
1945, epoch_train_loss=3.892934908606668
Epoch 1946
Epoch 1946 :: Batch 0/1
Batch Loss = 3.8923224312579943
1946, epoch_train_loss=3.8923224312579943
Epoch 1947
Epoch 1947 :: Batch 0/1
Batch Loss = 3.8917415264650432
1947, epoch_train_loss=3.8917415264650432
Epoch 1948
Epoch 1948 :: Batch 0/1
Batch Loss = 3.8912716242472833
1948, epoch_train_loss=3.8912716242472833
Epoch 1949
Epoch 1949 :: Batch 0/1
Batch Loss = 3.890684492362128
1949, epoch_train_loss=3.890684492362128
Epoch 1950
Epoch 1950 :: Batch 0/1
Batch Loss = 3.8900749406618473
1950, epoch_train_loss=3.8900749406618473
Epoch 1951
Epoch 1951 :: Batch 0/1
Batch Loss = 3.889597075607054
1951, epoch_train_loss=3.889597075607054
Epoch 1952
Epoch 1952 :: Batch 0/1
Batch Loss = 3.8890632146152027
1952, epoch_train_loss=3.8890632146152027
Epoch 1953
Epoch 1953 :: Batch 0/1
Batch Loss = 3.8884223219916425
1953, epoch_train_loss=3.8884223219916425
Epoch 1954
Epoch 1954 :: Batch 0/1
Batch Loss = 3.887891811599492
1954, epoch_train_loss=3.887891811599492
Epoch 1955
Epoch 1955 :: Batch 0/1
Batch Loss = 3.8873954073643135
1955, epoch_train_loss=3.8873954073643135
Epoch 1956
Epoch 1956 :: Batch 0/1
Batch Loss = 3.886783677539608
1956, epoch_train_loss=3.886783677539608
Epoch 1957
Epoch 1957 :: Batch 0/1
Batch Loss = 3.8861803192422957
1957, epoch_train_loss=3.8861803192422957
Epoch 1958
Epoch 1958 :: Batch 0/1
Batch Loss = 3.8856438812803313
1958, epoch_train_loss=3.8856438812803313
Epoch 1959
Epoch 1959 :: Batch 0/1
Batch Loss = 3.88509711318658
1959, epoch_train_loss=3.88509711318658
Epoch 1960
Epoch 1960 :: Batch 0/1
Batch Loss = 3.884500625412559
1960, epoch_train_loss=3.884500625412559
Epoch 1961
Epoch 1961 :: Batch 0/1
Batch Loss = 3.88387181539911
1961, epoch_train_loss=3.88387181539911
Epoch 1962
Epoch 1962 :: Batch 0/1
Batch Loss = 3.883266676518802
1962, epoch_train_loss=3.883266676518802
Epoch 1963
Epoch 1963 :: Batch 0/1
Batch Loss = 3.8826836637053828
1963, epoch_train_loss=3.8826836637053828
Epoch 1964
Epoch 1964 :: Batch 0/1
Batch Loss = 3.8820846290253166
1964, epoch_train_loss=3.8820846290253166
Epoch 1965
Epoch 1965 :: Batch 0/1
Batch Loss = 3.8814590036190726
1965, epoch_train_loss=3.8814590036190726
Epoch 1966
Epoch 1966 :: Batch 0/1
Batch Loss = 3.8807988075031417
1966, epoch_train_loss=3.8807988075031417
Epoch 1967
Epoch 1967 :: Batch 0/1
Batch Loss = 3.880132926166461
1967, epoch_train_loss=3.880132926166461
Epoch 1968
Epoch 1968 :: Batch 0/1
Batch Loss = 3.879465054871531
1968, epoch_train_loss=3.879465054871531
Epoch 1969
Epoch 1969 :: Batch 0/1
Batch Loss = 3.8788007181144475
1969, epoch_train_loss=3.8788007181144475
Epoch 1970
Epoch 1970 :: Batch 0/1
Batch Loss = 3.8781386690562325
1970, epoch_train_loss=3.8781386690562325
Epoch 1971
Epoch 1971 :: Batch 0/1
Batch Loss = 3.8774737132406636
1971, epoch_train_loss=3.8774737132406636
Epoch 1972
Epoch 1972 :: Batch 0/1
Batch Loss = 3.876811118719944
1972, epoch_train_loss=3.876811118719944
Epoch 1973
Epoch 1973 :: Batch 0/1
Batch Loss = 3.8761447429022424
1973, epoch_train_loss=3.8761447429022424
Epoch 1974
Epoch 1974 :: Batch 0/1
Batch Loss = 3.875502908763856
1974, epoch_train_loss=3.875502908763856
Epoch 1975
Epoch 1975 :: Batch 0/1
Batch Loss = 3.8748704926494897
1975, epoch_train_loss=3.8748704926494897
Epoch 1976
Epoch 1976 :: Batch 0/1
Batch Loss = 3.874351325000583
1976, epoch_train_loss=3.874351325000583
Epoch 1977
Epoch 1977 :: Batch 0/1
Batch Loss = 3.873831017054233
1977, epoch_train_loss=3.873831017054233
Epoch 1978
Epoch 1978 :: Batch 0/1
Batch Loss = 3.8736868276076852
1978, epoch_train_loss=3.8736868276076852
Epoch 1979
Epoch 1979 :: Batch 0/1
Batch Loss = 3.873089936467682
1979, epoch_train_loss=3.873089936467682
Epoch 1980
Epoch 1980 :: Batch 0/1
Batch Loss = 3.873022336937917
1980, epoch_train_loss=3.873022336937917
Epoch 1981
Epoch 1981 :: Batch 0/1
Batch Loss = 3.871490650552366
1981, epoch_train_loss=3.871490650552366
Epoch 1982
Epoch 1982 :: Batch 0/1
Batch Loss = 3.8703489316635364
1982, epoch_train_loss=3.8703489316635364
Epoch 1983
Epoch 1983 :: Batch 0/1
Batch Loss = 3.8695483086028766
1983, epoch_train_loss=3.8695483086028766
Epoch 1984
Epoch 1984 :: Batch 0/1
Batch Loss = 3.8691618065976168
1984, epoch_train_loss=3.8691618065976168
Epoch 1985
Epoch 1985 :: Batch 0/1
Batch Loss = 3.8691208665792503
1985, epoch_train_loss=3.8691208665792503
Epoch 1986
Epoch 1986 :: Batch 0/1
Batch Loss = 3.868373370623998
1986, epoch_train_loss=3.868373370623998
Epoch 1987
Epoch 1987 :: Batch 0/1
Batch Loss = 3.867767348498874
1987, epoch_train_loss=3.867767348498874
Epoch 1988
Epoch 1988 :: Batch 0/1
Batch Loss = 3.8664666129385172
1988, epoch_train_loss=3.8664666129385172
Epoch 1989
Epoch 1989 :: Batch 0/1
Batch Loss = 3.865520588087244
1989, epoch_train_loss=3.865520588087244
Epoch 1990
Epoch 1990 :: Batch 0/1
Batch Loss = 3.864756277600612
1990, epoch_train_loss=3.864756277600612
Epoch 1991
Epoch 1991 :: Batch 0/1
Batch Loss = 3.8642809839303687
1991, epoch_train_loss=3.8642809839303687
Epoch 1992
Epoch 1992 :: Batch 0/1
Batch Loss = 3.863953114760087
1992, epoch_train_loss=3.863953114760087
Epoch 1993
Epoch 1993 :: Batch 0/1
Batch Loss = 3.863210556785977
1993, epoch_train_loss=3.863210556785977
Epoch 1994
Epoch 1994 :: Batch 0/1
Batch Loss = 3.8624636107385264
1994, epoch_train_loss=3.8624636107385264
Epoch 1995
Epoch 1995 :: Batch 0/1
Batch Loss = 3.861348899943931
1995, epoch_train_loss=3.861348899943931
Epoch 1996
Epoch 1996 :: Batch 0/1
Batch Loss = 3.8603730492453407
1996, epoch_train_loss=3.8603730492453407
Epoch 1997
Epoch 1997 :: Batch 0/1
Batch Loss = 3.859758650820241
1997, epoch_train_loss=3.859758650820241
Epoch 1998
Epoch 1998 :: Batch 0/1
Batch Loss = 3.858833174586577
1998, epoch_train_loss=3.858833174586577
Epoch 1999
Epoch 1999 :: Batch 0/1
Batch Loss = 3.857938592181686
1999, epoch_train_loss=3.857938592181686
Epoch 2000
Epoch 2000 :: Batch 0/1
Batch Loss = 3.856864462383823
2000, epoch_train_loss=3.856864462383823
Epoch 2001
Epoch 2001 :: Batch 0/1
Batch Loss = 3.855844013535382
2001, epoch_train_loss=3.855844013535382
Epoch 2002
Epoch 2002 :: Batch 0/1
Batch Loss = 3.8548336722541863
2002, epoch_train_loss=3.8548336722541863
Epoch 2003
Epoch 2003 :: Batch 0/1
Batch Loss = 3.8535664291707317
2003, epoch_train_loss=3.8535664291707317
Epoch 2004
Epoch 2004 :: Batch 0/1
Batch Loss = 3.8523358451383927
2004, epoch_train_loss=3.8523358451383927
Epoch 2005
Epoch 2005 :: Batch 0/1
Batch Loss = 3.8513988843880806
2005, epoch_train_loss=3.8513988843880806
Epoch 2006
Epoch 2006 :: Batch 0/1
Batch Loss = 3.8504598406028143
2006, epoch_train_loss=3.8504598406028143
Epoch 2007
Epoch 2007 :: Batch 0/1
Batch Loss = 3.8496625851325157
2007, epoch_train_loss=3.8496625851325157
Epoch 2008
Epoch 2008 :: Batch 0/1
Batch Loss = 3.84883981403269
2008, epoch_train_loss=3.84883981403269
Epoch 2009
Epoch 2009 :: Batch 0/1
Batch Loss = 3.848004021317187
2009, epoch_train_loss=3.848004021317187
Epoch 2010
Epoch 2010 :: Batch 0/1
Batch Loss = 3.847211458752714
2010, epoch_train_loss=3.847211458752714
Epoch 2011
Epoch 2011 :: Batch 0/1
Batch Loss = 3.8464955429605
2011, epoch_train_loss=3.8464955429605
Epoch 2012
Epoch 2012 :: Batch 0/1
Batch Loss = 3.846141094142
2012, epoch_train_loss=3.846141094142
Epoch 2013
Epoch 2013 :: Batch 0/1
Batch Loss = 3.846528998925306
2013, epoch_train_loss=3.846528998925306
Epoch 2014
Epoch 2014 :: Batch 0/1
Batch Loss = 3.8455653970738157
2014, epoch_train_loss=3.8455653970738157
Epoch 2015
Epoch 2015 :: Batch 0/1
Batch Loss = 3.843903039723498
2015, epoch_train_loss=3.843903039723498
Epoch 2016
Epoch 2016 :: Batch 0/1
Batch Loss = 3.842314044421541
2016, epoch_train_loss=3.842314044421541
Epoch 2017
Epoch 2017 :: Batch 0/1
Batch Loss = 3.8425564389429394
2017, epoch_train_loss=3.8425564389429394
Epoch 2018
Epoch 2018 :: Batch 0/1
Batch Loss = 3.8413971615565727
2018, epoch_train_loss=3.8413971615565727
Epoch 2019
Epoch 2019 :: Batch 0/1
Batch Loss = 3.839764596110808
2019, epoch_train_loss=3.839764596110808
Epoch 2020
Epoch 2020 :: Batch 0/1
Batch Loss = 3.8395194577231995
2020, epoch_train_loss=3.8395194577231995
Epoch 2021
Epoch 2021 :: Batch 0/1
Batch Loss = 3.8388852415753196
2021, epoch_train_loss=3.8388852415753196
Epoch 2022
Epoch 2022 :: Batch 0/1
Batch Loss = 3.837803700190238
2022, epoch_train_loss=3.837803700190238
Epoch 2023
Epoch 2023 :: Batch 0/1
Batch Loss = 3.83640519797456
2023, epoch_train_loss=3.83640519797456
Epoch 2024
Epoch 2024 :: Batch 0/1
Batch Loss = 3.8355287972711563
2024, epoch_train_loss=3.8355287972711563
Epoch 2025
Epoch 2025 :: Batch 0/1
Batch Loss = 3.8347131044213962
2025, epoch_train_loss=3.8347131044213962
Epoch 2026
Epoch 2026 :: Batch 0/1
Batch Loss = 3.8337189014933903
2026, epoch_train_loss=3.8337189014933903
Epoch 2027
Epoch 2027 :: Batch 0/1
Batch Loss = 3.832945551094478
2027, epoch_train_loss=3.832945551094478
Epoch 2028
Epoch 2028 :: Batch 0/1
Batch Loss = 3.831747022882901
2028, epoch_train_loss=3.831747022882901
Epoch 2029
Epoch 2029 :: Batch 0/1
Batch Loss = 3.830870602316235
2029, epoch_train_loss=3.830870602316235
Epoch 2030
Epoch 2030 :: Batch 0/1
Batch Loss = 3.829762107720336
2030, epoch_train_loss=3.829762107720336
Epoch 2031
Epoch 2031 :: Batch 0/1
Batch Loss = 3.8288681216093865
2031, epoch_train_loss=3.8288681216093865
Epoch 2032
Epoch 2032 :: Batch 0/1
Batch Loss = 3.828138716949573
2032, epoch_train_loss=3.828138716949573
Epoch 2033
Epoch 2033 :: Batch 0/1
Batch Loss = 3.8288732026839303
2033, epoch_train_loss=3.8288732026839303
Epoch 2034
Epoch 2034 :: Batch 0/1
Batch Loss = 3.83321229913149
2034, epoch_train_loss=3.83321229913149
Epoch 2035
Epoch 2035 :: Batch 0/1
Batch Loss = 3.830107254170583
2035, epoch_train_loss=3.830107254170583
Epoch 2036
Epoch 2036 :: Batch 0/1
Batch Loss = 3.8261715521858752
2036, epoch_train_loss=3.8261715521858752
Epoch 2037
Epoch 2037 :: Batch 0/1
Batch Loss = 3.8297609981836342
2037, epoch_train_loss=3.8297609981836342
Epoch 2038
Epoch 2038 :: Batch 0/1
Batch Loss = 3.8269879723954316
2038, epoch_train_loss=3.8269879723954316
Epoch 2039
Epoch 2039 :: Batch 0/1
Batch Loss = 3.8294823216963163
2039, epoch_train_loss=3.8294823216963163
Epoch 2040
Epoch 2040 :: Batch 0/1
Batch Loss = 3.8267927130528196
2040, epoch_train_loss=3.8267927130528196
Epoch 2041
Epoch 2041 :: Batch 0/1
Batch Loss = 3.8268023758110705
2041, epoch_train_loss=3.8268023758110705
Epoch 2042
Epoch 2042 :: Batch 0/1
Batch Loss = 3.824282572531667
2042, epoch_train_loss=3.824282572531667
Epoch 2043
Epoch 2043 :: Batch 0/1
Batch Loss = 3.8210935508031354
2043, epoch_train_loss=3.8210935508031354
Epoch 2044
Epoch 2044 :: Batch 0/1
Batch Loss = 3.821566835454696
2044, epoch_train_loss=3.821566835454696
Epoch 2045
Epoch 2045 :: Batch 0/1
Batch Loss = 3.822266229806216
2045, epoch_train_loss=3.822266229806216
Epoch 2046
Epoch 2046 :: Batch 0/1
Batch Loss = 3.8230103408389517
2046, epoch_train_loss=3.8230103408389517
Epoch 2047
Epoch 2047 :: Batch 0/1
Batch Loss = 3.8199868580890386
2047, epoch_train_loss=3.8199868580890386
Epoch 2048
Epoch 2048 :: Batch 0/1
Batch Loss = 3.8222416637039944
2048, epoch_train_loss=3.8222416637039944
Epoch 2049
Epoch 2049 :: Batch 0/1
Batch Loss = 3.819172516913987
2049, epoch_train_loss=3.819172516913987
Epoch 2050
Epoch 2050 :: Batch 0/1
Batch Loss = 3.8182137097755953
2050, epoch_train_loss=3.8182137097755953
Epoch 2051
Epoch 2051 :: Batch 0/1
Batch Loss = 3.8158344977780834
2051, epoch_train_loss=3.8158344977780834
Epoch 2052
Epoch 2052 :: Batch 0/1
Batch Loss = 3.815202883895107
2052, epoch_train_loss=3.815202883895107
Epoch 2053
Epoch 2053 :: Batch 0/1
Batch Loss = 3.8147451611958636
2053, epoch_train_loss=3.8147451611958636
Epoch 2054
Epoch 2054 :: Batch 0/1
Batch Loss = 3.8129117180333223
2054, epoch_train_loss=3.8129117180333223
Epoch 2055
Epoch 2055 :: Batch 0/1
Batch Loss = 3.809606914689911
2055, epoch_train_loss=3.809606914689911
Epoch 2056
Epoch 2056 :: Batch 0/1
Batch Loss = 3.8064629922676843
2056, epoch_train_loss=3.8064629922676843
Epoch 2057
Epoch 2057 :: Batch 0/1
Batch Loss = 3.805553395472096
2057, epoch_train_loss=3.805553395472096
Epoch 2058
Epoch 2058 :: Batch 0/1
Batch Loss = 3.8053463416197197
2058, epoch_train_loss=3.8053463416197197
Epoch 2059
Epoch 2059 :: Batch 0/1
Batch Loss = 3.8049255199564307
2059, epoch_train_loss=3.8049255199564307
Epoch 2060
Epoch 2060 :: Batch 0/1
Batch Loss = 3.801657680571567
2060, epoch_train_loss=3.801657680571567
Epoch 2061
Epoch 2061 :: Batch 0/1
Batch Loss = 3.7988258443346967
2061, epoch_train_loss=3.7988258443346967
Epoch 2062
Epoch 2062 :: Batch 0/1
Batch Loss = 3.798003844265205
2062, epoch_train_loss=3.798003844265205
Epoch 2063
Epoch 2063 :: Batch 0/1
Batch Loss = 3.7969465752489415
2063, epoch_train_loss=3.7969465752489415
Epoch 2064
Epoch 2064 :: Batch 0/1
Batch Loss = 3.794469766045274
2064, epoch_train_loss=3.794469766045274
Epoch 2065
Epoch 2065 :: Batch 0/1
Batch Loss = 3.791579636239898
2065, epoch_train_loss=3.791579636239898
Epoch 2066
Epoch 2066 :: Batch 0/1
Batch Loss = 3.788678897846349
2066, epoch_train_loss=3.788678897846349
Epoch 2067
Epoch 2067 :: Batch 0/1
Batch Loss = 3.784572087401378
2067, epoch_train_loss=3.784572087401378
Epoch 2068
Epoch 2068 :: Batch 0/1
Batch Loss = 3.7617826774008254
2068, epoch_train_loss=3.7617826774008254
Epoch 2069
Epoch 2069 :: Batch 0/1
Batch Loss = 4.772493420497297
2069, epoch_train_loss=4.772493420497297
Epoch 2070
Epoch 2070 :: Batch 0/1
Batch Loss = 4.049082577136959
2070, epoch_train_loss=4.049082577136959
Epoch 2071
Epoch 2071 :: Batch 0/1
Batch Loss = 3.8726115222519866
2071, epoch_train_loss=3.8726115222519866
Epoch 2072
Epoch 2072 :: Batch 0/1
Batch Loss = 3.8493419834580656
2072, epoch_train_loss=3.8493419834580656
Epoch 2073
Epoch 2073 :: Batch 0/1
Batch Loss = 3.8595521908194588
2073, epoch_train_loss=3.8595521908194588
Epoch 2074
Epoch 2074 :: Batch 0/1
Batch Loss = 3.839344279753797
2074, epoch_train_loss=3.839344279753797
Epoch 2075
Epoch 2075 :: Batch 0/1
Batch Loss = 3.8240593349725613
2075, epoch_train_loss=3.8240593349725613
Epoch 2076
Epoch 2076 :: Batch 0/1
Batch Loss = 3.8166165991461036
2076, epoch_train_loss=3.8166165991461036
Epoch 2077
Epoch 2077 :: Batch 0/1
Batch Loss = 3.8134751035882757
2077, epoch_train_loss=3.8134751035882757
Epoch 2078
Epoch 2078 :: Batch 0/1
Batch Loss = 3.812933890618076
2078, epoch_train_loss=3.812933890618076
Epoch 2079
Epoch 2079 :: Batch 0/1
Batch Loss = 3.8142700202745026
2079, epoch_train_loss=3.8142700202745026
Epoch 2080
Epoch 2080 :: Batch 0/1
Batch Loss = 3.816568382846278
2080, epoch_train_loss=3.816568382846278
Epoch 2081
Epoch 2081 :: Batch 0/1
Batch Loss = 3.8178662531860326
2081, epoch_train_loss=3.8178662531860326
Epoch 2082
Epoch 2082 :: Batch 0/1
Batch Loss = 3.817604727644191
2082, epoch_train_loss=3.817604727644191
Epoch 2083
Epoch 2083 :: Batch 0/1
Batch Loss = 3.8155508239930924
2083, epoch_train_loss=3.8155508239930924
Epoch 2084
Epoch 2084 :: Batch 0/1
Batch Loss = 3.812370421756772
2084, epoch_train_loss=3.812370421756772
Epoch 2085
Epoch 2085 :: Batch 0/1
Batch Loss = 3.8093643220959246
2085, epoch_train_loss=3.8093643220959246
Epoch 2086
Epoch 2086 :: Batch 0/1
Batch Loss = 3.806467005205465
2086, epoch_train_loss=3.806467005205465
Epoch 2087
Epoch 2087 :: Batch 0/1
Batch Loss = 3.803914060574243
2087, epoch_train_loss=3.803914060574243
Epoch 2088
Epoch 2088 :: Batch 0/1
Batch Loss = 3.801636489734368
2088, epoch_train_loss=3.801636489734368
Epoch 2089
Epoch 2089 :: Batch 0/1
Batch Loss = 3.7996168797262633
2089, epoch_train_loss=3.7996168797262633
Epoch 2090
Epoch 2090 :: Batch 0/1
Batch Loss = 3.7982038415938617
2090, epoch_train_loss=3.7982038415938617
Epoch 2091
Epoch 2091 :: Batch 0/1
Batch Loss = 3.7970619078127483
2091, epoch_train_loss=3.7970619078127483
Epoch 2092
Epoch 2092 :: Batch 0/1
Batch Loss = 3.7961955788253796
2092, epoch_train_loss=3.7961955788253796
Epoch 2093
Epoch 2093 :: Batch 0/1
Batch Loss = 3.795130432525851
2093, epoch_train_loss=3.795130432525851
Epoch 2094
Epoch 2094 :: Batch 0/1
Batch Loss = 3.793648067431557
2094, epoch_train_loss=3.793648067431557
Epoch 2095
Epoch 2095 :: Batch 0/1
Batch Loss = 3.7917854894948677
2095, epoch_train_loss=3.7917854894948677
Epoch 2096
Epoch 2096 :: Batch 0/1
Batch Loss = 3.789419233826866
2096, epoch_train_loss=3.789419233826866
Epoch 2097
Epoch 2097 :: Batch 0/1
Batch Loss = 3.78666257346797
2097, epoch_train_loss=3.78666257346797
Epoch 2098
Epoch 2098 :: Batch 0/1
Batch Loss = 3.7836845288449616
2098, epoch_train_loss=3.7836845288449616
Epoch 2099
Epoch 2099 :: Batch 0/1
Batch Loss = 3.781230103456217
2099, epoch_train_loss=3.781230103456217
Epoch 2100
Epoch 2100 :: Batch 0/1
Batch Loss = 3.7802600339657024
2100, epoch_train_loss=3.7802600339657024
Epoch 2101
Epoch 2101 :: Batch 0/1
Batch Loss = 3.7787984390097993
2101, epoch_train_loss=3.7787984390097993
Epoch 2102
Epoch 2102 :: Batch 0/1
Batch Loss = 3.776410037609128
2102, epoch_train_loss=3.776410037609128
Epoch 2103
Epoch 2103 :: Batch 0/1
Batch Loss = 3.774975219414844
2103, epoch_train_loss=3.774975219414844
Epoch 2104
Epoch 2104 :: Batch 0/1
Batch Loss = 3.774092571625974
2104, epoch_train_loss=3.774092571625974
Epoch 2105
Epoch 2105 :: Batch 0/1
Batch Loss = 3.7730372655801983
2105, epoch_train_loss=3.7730372655801983
Epoch 2106
Epoch 2106 :: Batch 0/1
Batch Loss = 3.7715162228298835
2106, epoch_train_loss=3.7715162228298835
Epoch 2107
Epoch 2107 :: Batch 0/1
Batch Loss = 3.7696172512048984
2107, epoch_train_loss=3.7696172512048984
Epoch 2108
Epoch 2108 :: Batch 0/1
Batch Loss = 3.767646102514759
2108, epoch_train_loss=3.767646102514759
Epoch 2109
Epoch 2109 :: Batch 0/1
Batch Loss = 3.7659577796343444
2109, epoch_train_loss=3.7659577796343444
Epoch 2110
Epoch 2110 :: Batch 0/1
Batch Loss = 3.764630537136954
2110, epoch_train_loss=3.764630537136954
Epoch 2111
Epoch 2111 :: Batch 0/1
Batch Loss = 3.7630776075793317
2111, epoch_train_loss=3.7630776075793317
Epoch 2112
Epoch 2112 :: Batch 0/1
Batch Loss = 3.761172337707705
2112, epoch_train_loss=3.761172337707705
Epoch 2113
Epoch 2113 :: Batch 0/1
Batch Loss = 3.7593814736933786
2113, epoch_train_loss=3.7593814736933786
Epoch 2114
Epoch 2114 :: Batch 0/1
Batch Loss = 3.757798206399715
2114, epoch_train_loss=3.757798206399715
Epoch 2115
Epoch 2115 :: Batch 0/1
Batch Loss = 3.7563257176662503
2115, epoch_train_loss=3.7563257176662503
Epoch 2116
Epoch 2116 :: Batch 0/1
Batch Loss = 3.7548182391150395
2116, epoch_train_loss=3.7548182391150395
Epoch 2117
Epoch 2117 :: Batch 0/1
Batch Loss = 3.753193083448151
2117, epoch_train_loss=3.753193083448151
Epoch 2118
Epoch 2118 :: Batch 0/1
Batch Loss = 3.751539929606101
2118, epoch_train_loss=3.751539929606101
Epoch 2119
Epoch 2119 :: Batch 0/1
Batch Loss = 3.7499853643713164
2119, epoch_train_loss=3.7499853643713164
Epoch 2120
Epoch 2120 :: Batch 0/1
Batch Loss = 3.748570347324167
2120, epoch_train_loss=3.748570347324167
Epoch 2121
Epoch 2121 :: Batch 0/1
Batch Loss = 3.747144679850708
2121, epoch_train_loss=3.747144679850708
Epoch 2122
Epoch 2122 :: Batch 0/1
Batch Loss = 3.745655032546384
2122, epoch_train_loss=3.745655032546384
Epoch 2123
Epoch 2123 :: Batch 0/1
Batch Loss = 3.744211564932315
2123, epoch_train_loss=3.744211564932315
Epoch 2124
Epoch 2124 :: Batch 0/1
Batch Loss = 3.7427183841925236
2124, epoch_train_loss=3.7427183841925236
Epoch 2125
Epoch 2125 :: Batch 0/1
Batch Loss = 3.741034058371317
2125, epoch_train_loss=3.741034058371317
Epoch 2126
Epoch 2126 :: Batch 0/1
Batch Loss = 3.7391975350063817
2126, epoch_train_loss=3.7391975350063817
Epoch 2127
Epoch 2127 :: Batch 0/1
Batch Loss = 3.7372877665928104
2127, epoch_train_loss=3.7372877665928104
Epoch 2128
Epoch 2128 :: Batch 0/1
Batch Loss = 3.7353863340742306
2128, epoch_train_loss=3.7353863340742306
Epoch 2129
Epoch 2129 :: Batch 0/1
Batch Loss = 3.733527393832851
2129, epoch_train_loss=3.733527393832851
Epoch 2130
Epoch 2130 :: Batch 0/1
Batch Loss = 3.7316949589379482
2130, epoch_train_loss=3.7316949589379482
Epoch 2131
Epoch 2131 :: Batch 0/1
Batch Loss = 3.7299056687874894
2131, epoch_train_loss=3.7299056687874894
Epoch 2132
Epoch 2132 :: Batch 0/1
Batch Loss = 3.728208730098006
2132, epoch_train_loss=3.728208730098006
Epoch 2133
Epoch 2133 :: Batch 0/1
Batch Loss = 3.726502905336557
2133, epoch_train_loss=3.726502905336557
Epoch 2134
Epoch 2134 :: Batch 0/1
Batch Loss = 3.7245373796393
2134, epoch_train_loss=3.7245373796393
Epoch 2135
Epoch 2135 :: Batch 0/1
Batch Loss = 3.7221564398948854
2135, epoch_train_loss=3.7221564398948854
Epoch 2136
Epoch 2136 :: Batch 0/1
Batch Loss = 3.7191921055290726
2136, epoch_train_loss=3.7191921055290726
Epoch 2137
Epoch 2137 :: Batch 0/1
Batch Loss = 3.7156428180092154
2137, epoch_train_loss=3.7156428180092154
Epoch 2138
Epoch 2138 :: Batch 0/1
Batch Loss = 3.7115199197112325
2138, epoch_train_loss=3.7115199197112325
Epoch 2139
Epoch 2139 :: Batch 0/1
Batch Loss = 3.7072879619484302
2139, epoch_train_loss=3.7072879619484302
Epoch 2140
Epoch 2140 :: Batch 0/1
Batch Loss = 3.7039121583605983
2140, epoch_train_loss=3.7039121583605983
Epoch 2141
Epoch 2141 :: Batch 0/1
Batch Loss = 3.7023708984173362
2141, epoch_train_loss=3.7023708984173362
Epoch 2142
Epoch 2142 :: Batch 0/1
Batch Loss = 3.7018749332480367
2142, epoch_train_loss=3.7018749332480367
Epoch 2143
Epoch 2143 :: Batch 0/1
Batch Loss = 3.6999376288399843
2143, epoch_train_loss=3.6999376288399843
Epoch 2144
Epoch 2144 :: Batch 0/1
Batch Loss = 3.696261824243849
2144, epoch_train_loss=3.696261824243849
Epoch 2145
Epoch 2145 :: Batch 0/1
Batch Loss = 3.6921079972230073
2145, epoch_train_loss=3.6921079972230073
Epoch 2146
Epoch 2146 :: Batch 0/1
Batch Loss = 3.6886833735353184
2146, epoch_train_loss=3.6886833735353184
Epoch 2147
Epoch 2147 :: Batch 0/1
Batch Loss = 3.686680065683649
2147, epoch_train_loss=3.686680065683649
Epoch 2148
Epoch 2148 :: Batch 0/1
Batch Loss = 3.685334815595501
2148, epoch_train_loss=3.685334815595501
Epoch 2149
Epoch 2149 :: Batch 0/1
Batch Loss = 3.6835331497917685
2149, epoch_train_loss=3.6835331497917685
Epoch 2150
Epoch 2150 :: Batch 0/1
Batch Loss = 3.681271127671356
2150, epoch_train_loss=3.681271127671356
Epoch 2151
Epoch 2151 :: Batch 0/1
Batch Loss = 3.678687629824675
2151, epoch_train_loss=3.678687629824675
Epoch 2152
Epoch 2152 :: Batch 0/1
Batch Loss = 3.675752518230655
2152, epoch_train_loss=3.675752518230655
Epoch 2153
Epoch 2153 :: Batch 0/1
Batch Loss = 3.6723349939663135
2153, epoch_train_loss=3.6723349939663135
Epoch 2154
Epoch 2154 :: Batch 0/1
Batch Loss = 3.6690637397160417
2154, epoch_train_loss=3.6690637397160417
Epoch 2155
Epoch 2155 :: Batch 0/1
Batch Loss = 3.666725398673572
2155, epoch_train_loss=3.666725398673572
Epoch 2156
Epoch 2156 :: Batch 0/1
Batch Loss = 3.664755374026903
2156, epoch_train_loss=3.664755374026903
Epoch 2157
Epoch 2157 :: Batch 0/1
Batch Loss = 3.6623032201425296
2157, epoch_train_loss=3.6623032201425296
Epoch 2158
Epoch 2158 :: Batch 0/1
Batch Loss = 3.6591646407896152
2158, epoch_train_loss=3.6591646407896152
Epoch 2159
Epoch 2159 :: Batch 0/1
Batch Loss = 3.655842004187358
2159, epoch_train_loss=3.655842004187358
Epoch 2160
Epoch 2160 :: Batch 0/1
Batch Loss = 3.6528327912754244
2160, epoch_train_loss=3.6528327912754244
Epoch 2161
Epoch 2161 :: Batch 0/1
Batch Loss = 3.6501095680158993
2161, epoch_train_loss=3.6501095680158993
Epoch 2162
Epoch 2162 :: Batch 0/1
Batch Loss = 3.647112076205732
2162, epoch_train_loss=3.647112076205732
Epoch 2163
Epoch 2163 :: Batch 0/1
Batch Loss = 3.6434645895561593
2163, epoch_train_loss=3.6434645895561593
Epoch 2164
Epoch 2164 :: Batch 0/1
Batch Loss = 3.639650477620124
2164, epoch_train_loss=3.639650477620124
Epoch 2165
Epoch 2165 :: Batch 0/1
Batch Loss = 3.6361290872559753
2165, epoch_train_loss=3.6361290872559753
Epoch 2166
Epoch 2166 :: Batch 0/1
Batch Loss = 3.6329619829912825
2166, epoch_train_loss=3.6329619829912825
Epoch 2167
Epoch 2167 :: Batch 0/1
Batch Loss = 3.629642998936276
2167, epoch_train_loss=3.629642998936276
Epoch 2168
Epoch 2168 :: Batch 0/1
Batch Loss = 3.6258351227135357
2168, epoch_train_loss=3.6258351227135357
Epoch 2169
Epoch 2169 :: Batch 0/1
Batch Loss = 3.6219075988481184
2169, epoch_train_loss=3.6219075988481184
Epoch 2170
Epoch 2170 :: Batch 0/1
Batch Loss = 3.618000088853748
2170, epoch_train_loss=3.618000088853748
Epoch 2171
Epoch 2171 :: Batch 0/1
Batch Loss = 3.613808028728284
2171, epoch_train_loss=3.613808028728284
Epoch 2172
Epoch 2172 :: Batch 0/1
Batch Loss = 3.6091150964128795
2172, epoch_train_loss=3.6091150964128795
Epoch 2173
Epoch 2173 :: Batch 0/1
Batch Loss = 3.6036259335854224
2173, epoch_train_loss=3.6036259335854224
Epoch 2174
Epoch 2174 :: Batch 0/1
Batch Loss = 3.597987996044949
2174, epoch_train_loss=3.597987996044949
Epoch 2175
Epoch 2175 :: Batch 0/1
Batch Loss = 3.592495940584369
2175, epoch_train_loss=3.592495940584369
Epoch 2176
Epoch 2176 :: Batch 0/1
Batch Loss = 3.586487695037677
2176, epoch_train_loss=3.586487695037677
Epoch 2177
Epoch 2177 :: Batch 0/1
Batch Loss = 3.579299222989536
2177, epoch_train_loss=3.579299222989536
Epoch 2178
Epoch 2178 :: Batch 0/1
Batch Loss = 3.5712810840275075
2178, epoch_train_loss=3.5712810840275075
Epoch 2179
Epoch 2179 :: Batch 0/1
Batch Loss = 3.5626746180014557
2179, epoch_train_loss=3.5626746180014557
Epoch 2180
Epoch 2180 :: Batch 0/1
Batch Loss = 3.554155192662044
2180, epoch_train_loss=3.554155192662044
Epoch 2181
Epoch 2181 :: Batch 0/1
Batch Loss = 3.545301614966771
2181, epoch_train_loss=3.545301614966771
Epoch 2182
Epoch 2182 :: Batch 0/1
Batch Loss = 3.5359459530008173
2182, epoch_train_loss=3.5359459530008173
Epoch 2183
Epoch 2183 :: Batch 0/1
Batch Loss = 3.5263486007060383
2183, epoch_train_loss=3.5263486007060383
Epoch 2184
Epoch 2184 :: Batch 0/1
Batch Loss = 3.516215507941386
2184, epoch_train_loss=3.516215507941386
Epoch 2185
Epoch 2185 :: Batch 0/1
Batch Loss = 3.5055350268607866
2185, epoch_train_loss=3.5055350268607866
Epoch 2186
Epoch 2186 :: Batch 0/1
Batch Loss = 3.494339849379206
2186, epoch_train_loss=3.494339849379206
Epoch 2187
Epoch 2187 :: Batch 0/1
Batch Loss = 3.4820015773361344
2187, epoch_train_loss=3.4820015773361344
Epoch 2188
Epoch 2188 :: Batch 0/1
Batch Loss = 3.4693275893392594
2188, epoch_train_loss=3.4693275893392594
Epoch 2189
Epoch 2189 :: Batch 0/1
Batch Loss = 3.4565630638818905
2189, epoch_train_loss=3.4565630638818905
Epoch 2190
Epoch 2190 :: Batch 0/1
Batch Loss = 3.441640784526815
2190, epoch_train_loss=3.441640784526815
Epoch 2191
Epoch 2191 :: Batch 0/1
Batch Loss = 3.427590349362025
2191, epoch_train_loss=3.427590349362025
Epoch 2192
Epoch 2192 :: Batch 0/1
Batch Loss = 3.4146800645810402
2192, epoch_train_loss=3.4146800645810402
Epoch 2193
Epoch 2193 :: Batch 0/1
Batch Loss = 3.4093454089489494
2193, epoch_train_loss=3.4093454089489494
Epoch 2194
Epoch 2194 :: Batch 0/1
Batch Loss = 3.399536432158777
2194, epoch_train_loss=3.399536432158777
Epoch 2195
Epoch 2195 :: Batch 0/1
Batch Loss = 3.501752524167045
2195, epoch_train_loss=3.501752524167045
Epoch 2196
Epoch 2196 :: Batch 0/1
Batch Loss = 3.425151673056363
2196, epoch_train_loss=3.425151673056363
Epoch 2197
Epoch 2197 :: Batch 0/1
Batch Loss = 3.4427812094371113
2197, epoch_train_loss=3.4427812094371113
Epoch 2198
Epoch 2198 :: Batch 0/1
Batch Loss = 3.4552388544222175
2198, epoch_train_loss=3.4552388544222175
Epoch 2199
Epoch 2199 :: Batch 0/1
Batch Loss = 3.440470919958381
2199, epoch_train_loss=3.440470919958381
Epoch 2200
Epoch 2200 :: Batch 0/1
Batch Loss = 3.44289765254762
2200, epoch_train_loss=3.44289765254762
Epoch 2201
Epoch 2201 :: Batch 0/1
Batch Loss = 3.438698943924319
2201, epoch_train_loss=3.438698943924319
Epoch 2202
Epoch 2202 :: Batch 0/1
Batch Loss = 3.44100911047369
2202, epoch_train_loss=3.44100911047369
Epoch 2203
Epoch 2203 :: Batch 0/1
Batch Loss = 3.4436727940636085
2203, epoch_train_loss=3.4436727940636085
Epoch 2204
Epoch 2204 :: Batch 0/1
Batch Loss = 3.444993731303922
2204, epoch_train_loss=3.444993731303922
Epoch 2205
Epoch 2205 :: Batch 0/1
Batch Loss = 3.4435685819902
2205, epoch_train_loss=3.4435685819902
Epoch 2206
Epoch 2206 :: Batch 0/1
Batch Loss = 3.4408147877109605
2206, epoch_train_loss=3.4408147877109605
Epoch 2207
Epoch 2207 :: Batch 0/1
Batch Loss = 3.4386708897661076
2207, epoch_train_loss=3.4386708897661076
Epoch 2208
Epoch 2208 :: Batch 0/1
Batch Loss = 3.4356744884611596
2208, epoch_train_loss=3.4356744884611596
Epoch 2209
Epoch 2209 :: Batch 0/1
Batch Loss = 3.43125230904173
2209, epoch_train_loss=3.43125230904173
Epoch 2210
Epoch 2210 :: Batch 0/1
Batch Loss = 3.429485863838873
2210, epoch_train_loss=3.429485863838873
Epoch 2211
Epoch 2211 :: Batch 0/1
Batch Loss = 3.4272283211814893
2211, epoch_train_loss=3.4272283211814893
Epoch 2212
Epoch 2212 :: Batch 0/1
Batch Loss = 3.4234293500713497
2212, epoch_train_loss=3.4234293500713497
Epoch 2213
Epoch 2213 :: Batch 0/1
Batch Loss = 3.4200496468273376
2213, epoch_train_loss=3.4200496468273376
Epoch 2214
Epoch 2214 :: Batch 0/1
Batch Loss = 3.416014720171927
2214, epoch_train_loss=3.416014720171927
Epoch 2215
Epoch 2215 :: Batch 0/1
Batch Loss = 3.41192323262758
2215, epoch_train_loss=3.41192323262758
Epoch 2216
Epoch 2216 :: Batch 0/1
Batch Loss = 3.4079872211674425
2216, epoch_train_loss=3.4079872211674425
Epoch 2217
Epoch 2217 :: Batch 0/1
Batch Loss = 3.4044428868603682
2217, epoch_train_loss=3.4044428868603682
Epoch 2218
Epoch 2218 :: Batch 0/1
Batch Loss = 3.4007294084852173
2218, epoch_train_loss=3.4007294084852173
Epoch 2219
Epoch 2219 :: Batch 0/1
Batch Loss = 3.3962555461436774
2219, epoch_train_loss=3.3962555461436774
Epoch 2220
Epoch 2220 :: Batch 0/1
Batch Loss = 3.3921707787457076
2220, epoch_train_loss=3.3921707787457076
Epoch 2221
Epoch 2221 :: Batch 0/1
Batch Loss = 3.388837264227229
2221, epoch_train_loss=3.388837264227229
Epoch 2222
Epoch 2222 :: Batch 0/1
Batch Loss = 3.3847237607544893
2222, epoch_train_loss=3.3847237607544893
Epoch 2223
Epoch 2223 :: Batch 0/1
Batch Loss = 3.3809526183228753
2223, epoch_train_loss=3.3809526183228753
Epoch 2224
Epoch 2224 :: Batch 0/1
Batch Loss = 3.377171925193078
2224, epoch_train_loss=3.377171925193078
Epoch 2225
Epoch 2225 :: Batch 0/1
Batch Loss = 3.3731906631736637
2225, epoch_train_loss=3.3731906631736637
Epoch 2226
Epoch 2226 :: Batch 0/1
Batch Loss = 3.3694233420010433
2226, epoch_train_loss=3.3694233420010433
Epoch 2227
Epoch 2227 :: Batch 0/1
Batch Loss = 3.3659805189498453
2227, epoch_train_loss=3.3659805189498453
Epoch 2228
Epoch 2228 :: Batch 0/1
Batch Loss = 3.3620780149996548
2228, epoch_train_loss=3.3620780149996548
Epoch 2229
Epoch 2229 :: Batch 0/1
Batch Loss = 3.358165966546179
2229, epoch_train_loss=3.358165966546179
Epoch 2230
Epoch 2230 :: Batch 0/1
Batch Loss = 3.354360622028554
2230, epoch_train_loss=3.354360622028554
Epoch 2231
Epoch 2231 :: Batch 0/1
Batch Loss = 3.35069742574459
2231, epoch_train_loss=3.35069742574459
Epoch 2232
Epoch 2232 :: Batch 0/1
Batch Loss = 3.3469412986627742
2232, epoch_train_loss=3.3469412986627742
Epoch 2233
Epoch 2233 :: Batch 0/1
Batch Loss = 3.34334912894639
2233, epoch_train_loss=3.34334912894639
Epoch 2234
Epoch 2234 :: Batch 0/1
Batch Loss = 3.3404353794249513
2234, epoch_train_loss=3.3404353794249513
Epoch 2235
Epoch 2235 :: Batch 0/1
Batch Loss = 3.337597042074797
2235, epoch_train_loss=3.337597042074797
Epoch 2236
Epoch 2236 :: Batch 0/1
Batch Loss = 3.333232222407075
2236, epoch_train_loss=3.333232222407075
Epoch 2237
Epoch 2237 :: Batch 0/1
Batch Loss = 3.327455639928619
2237, epoch_train_loss=3.327455639928619
Epoch 2238
Epoch 2238 :: Batch 0/1
Batch Loss = 3.323555614713496
2238, epoch_train_loss=3.323555614713496
Epoch 2239
Epoch 2239 :: Batch 0/1
Batch Loss = 3.3213540808254796
2239, epoch_train_loss=3.3213540808254796
Epoch 2240
Epoch 2240 :: Batch 0/1
Batch Loss = 3.3170102502394183
2240, epoch_train_loss=3.3170102502394183
Epoch 2241
Epoch 2241 :: Batch 0/1
Batch Loss = 3.311968569691765
2241, epoch_train_loss=3.311968569691765
Epoch 2242
Epoch 2242 :: Batch 0/1
Batch Loss = 3.3095919430905756
2242, epoch_train_loss=3.3095919430905756
Epoch 2243
Epoch 2243 :: Batch 0/1
Batch Loss = 3.3075270135544654
2243, epoch_train_loss=3.3075270135544654
Epoch 2244
Epoch 2244 :: Batch 0/1
Batch Loss = 3.303679156471392
2244, epoch_train_loss=3.303679156471392
Epoch 2245
Epoch 2245 :: Batch 0/1
Batch Loss = 3.300271481222146
2245, epoch_train_loss=3.300271481222146
Epoch 2246
Epoch 2246 :: Batch 0/1
Batch Loss = 3.2978678598724365
2246, epoch_train_loss=3.2978678598724365
Epoch 2247
Epoch 2247 :: Batch 0/1
Batch Loss = 3.2946486413512286
2247, epoch_train_loss=3.2946486413512286
Epoch 2248
Epoch 2248 :: Batch 0/1
Batch Loss = 3.290599648532467
2248, epoch_train_loss=3.290599648532467
Epoch 2249
Epoch 2249 :: Batch 0/1
Batch Loss = 3.287572987624124
2249, epoch_train_loss=3.287572987624124
Epoch 2250
Epoch 2250 :: Batch 0/1
Batch Loss = 3.2852609755332773
2250, epoch_train_loss=3.2852609755332773
Epoch 2251
Epoch 2251 :: Batch 0/1
Batch Loss = 3.282402389526552
2251, epoch_train_loss=3.282402389526552
Epoch 2252
Epoch 2252 :: Batch 0/1
Batch Loss = 3.2785564295856933
2252, epoch_train_loss=3.2785564295856933
Epoch 2253
Epoch 2253 :: Batch 0/1
Batch Loss = 3.2751740136010046
2253, epoch_train_loss=3.2751740136010046
Epoch 2254
Epoch 2254 :: Batch 0/1
Batch Loss = 3.272310206657847
2254, epoch_train_loss=3.272310206657847
Epoch 2255
Epoch 2255 :: Batch 0/1
Batch Loss = 3.269817497152964
2255, epoch_train_loss=3.269817497152964
Epoch 2256
Epoch 2256 :: Batch 0/1
Batch Loss = 3.2668161554559774
2256, epoch_train_loss=3.2668161554559774
Epoch 2257
Epoch 2257 :: Batch 0/1
Batch Loss = 3.263028063521715
2257, epoch_train_loss=3.263028063521715
Epoch 2258
Epoch 2258 :: Batch 0/1
Batch Loss = 3.2591341184877174
2258, epoch_train_loss=3.2591341184877174
Epoch 2259
Epoch 2259 :: Batch 0/1
Batch Loss = 3.2559441769844417
2259, epoch_train_loss=3.2559441769844417
Epoch 2260
Epoch 2260 :: Batch 0/1
Batch Loss = 3.2533561307688497
2260, epoch_train_loss=3.2533561307688497
Epoch 2261
Epoch 2261 :: Batch 0/1
Batch Loss = 3.2513593303424932
2261, epoch_train_loss=3.2513593303424932
Epoch 2262
Epoch 2262 :: Batch 0/1
Batch Loss = 3.2497470989552144
2262, epoch_train_loss=3.2497470989552144
Epoch 2263
Epoch 2263 :: Batch 0/1
Batch Loss = 3.247889854807321
2263, epoch_train_loss=3.247889854807321
Epoch 2264
Epoch 2264 :: Batch 0/1
Batch Loss = 3.2445461640703632
2264, epoch_train_loss=3.2445461640703632
Epoch 2265
Epoch 2265 :: Batch 0/1
Batch Loss = 3.240358161678886
2265, epoch_train_loss=3.240358161678886
Epoch 2266
Epoch 2266 :: Batch 0/1
Batch Loss = 3.23635550873305
2266, epoch_train_loss=3.23635550873305
Epoch 2267
Epoch 2267 :: Batch 0/1
Batch Loss = 3.2323720142512036
2267, epoch_train_loss=3.2323720142512036
Epoch 2268
Epoch 2268 :: Batch 0/1
Batch Loss = 3.229091158742499
2268, epoch_train_loss=3.229091158742499
Epoch 2269
Epoch 2269 :: Batch 0/1
Batch Loss = 3.225963555789819
2269, epoch_train_loss=3.225963555789819
Epoch 2270
Epoch 2270 :: Batch 0/1
Batch Loss = 3.222338547495327
2270, epoch_train_loss=3.222338547495327
Epoch 2271
Epoch 2271 :: Batch 0/1
Batch Loss = 3.2181530019860523
2271, epoch_train_loss=3.2181530019860523
Epoch 2272
Epoch 2272 :: Batch 0/1
Batch Loss = 3.2148694404221483
2272, epoch_train_loss=3.2148694404221483
Epoch 2273
Epoch 2273 :: Batch 0/1
Batch Loss = 3.212972359415418
2273, epoch_train_loss=3.212972359415418
Epoch 2274
Epoch 2274 :: Batch 0/1
Batch Loss = 3.2114760858536755
2274, epoch_train_loss=3.2114760858536755
Epoch 2275
Epoch 2275 :: Batch 0/1
Batch Loss = 3.2092799498798774
2275, epoch_train_loss=3.2092799498798774
Epoch 2276
Epoch 2276 :: Batch 0/1
Batch Loss = 3.205838545583806
2276, epoch_train_loss=3.205838545583806
Epoch 2277
Epoch 2277 :: Batch 0/1
Batch Loss = 3.202336831158399
2277, epoch_train_loss=3.202336831158399
Epoch 2278
Epoch 2278 :: Batch 0/1
Batch Loss = 3.1998273985935533
2278, epoch_train_loss=3.1998273985935533
Epoch 2279
Epoch 2279 :: Batch 0/1
Batch Loss = 3.198494226687985
2279, epoch_train_loss=3.198494226687985
Epoch 2280
Epoch 2280 :: Batch 0/1
Batch Loss = 3.19710656466519
2280, epoch_train_loss=3.19710656466519
Epoch 2281
Epoch 2281 :: Batch 0/1
Batch Loss = 3.194401932596231
2281, epoch_train_loss=3.194401932596231
Epoch 2282
Epoch 2282 :: Batch 0/1
Batch Loss = 3.1906837676865165
2282, epoch_train_loss=3.1906837676865165
Epoch 2283
Epoch 2283 :: Batch 0/1
Batch Loss = 3.187742958901062
2283, epoch_train_loss=3.187742958901062
Epoch 2284
Epoch 2284 :: Batch 0/1
Batch Loss = 3.1859647346468822
2284, epoch_train_loss=3.1859647346468822
Epoch 2285
Epoch 2285 :: Batch 0/1
Batch Loss = 3.1844281650799906
2285, epoch_train_loss=3.1844281650799906
Epoch 2286
Epoch 2286 :: Batch 0/1
Batch Loss = 3.182201652741117
2286, epoch_train_loss=3.182201652741117
Epoch 2287
Epoch 2287 :: Batch 0/1
Batch Loss = 3.1789150729464413
2287, epoch_train_loss=3.1789150729464413
Epoch 2288
Epoch 2288 :: Batch 0/1
Batch Loss = 3.1757493082063735
2288, epoch_train_loss=3.1757493082063735
Epoch 2289
Epoch 2289 :: Batch 0/1
Batch Loss = 3.172862529481158
2289, epoch_train_loss=3.172862529481158
Epoch 2290
Epoch 2290 :: Batch 0/1
Batch Loss = 3.1702320656138405
2290, epoch_train_loss=3.1702320656138405
Epoch 2291
Epoch 2291 :: Batch 0/1
Batch Loss = 3.1678049778525863
2291, epoch_train_loss=3.1678049778525863
Epoch 2292
Epoch 2292 :: Batch 0/1
Batch Loss = 3.165689555089856
2292, epoch_train_loss=3.165689555089856
Epoch 2293
Epoch 2293 :: Batch 0/1
Batch Loss = 3.1647660113681177
2293, epoch_train_loss=3.1647660113681177
Epoch 2294
Epoch 2294 :: Batch 0/1
Batch Loss = 3.170156320806688
2294, epoch_train_loss=3.170156320806688
Epoch 2295
Epoch 2295 :: Batch 0/1
Batch Loss = 3.200380866984022
2295, epoch_train_loss=3.200380866984022
Epoch 2296
Epoch 2296 :: Batch 0/1
Batch Loss = 3.309397658760724
2296, epoch_train_loss=3.309397658760724
Epoch 2297
Epoch 2297 :: Batch 0/1
Batch Loss = 3.3013684313400575
2297, epoch_train_loss=3.3013684313400575
Epoch 2298
Epoch 2298 :: Batch 0/1
Batch Loss = 3.160831929303048
2298, epoch_train_loss=3.160831929303048
Epoch 2299
Epoch 2299 :: Batch 0/1
Batch Loss = 3.212696662996147
2299, epoch_train_loss=3.212696662996147
Epoch 2300
Epoch 2300 :: Batch 0/1
Batch Loss = 3.192686808085002
2300, epoch_train_loss=3.192686808085002
Epoch 2301
Epoch 2301 :: Batch 0/1
Batch Loss = 3.1570518875104767
2301, epoch_train_loss=3.1570518875104767
Epoch 2302
Epoch 2302 :: Batch 0/1
Batch Loss = 3.184368569121186
2302, epoch_train_loss=3.184368569121186
Epoch 2303
Epoch 2303 :: Batch 0/1
Batch Loss = 3.164192566054413
2303, epoch_train_loss=3.164192566054413
Epoch 2304
Epoch 2304 :: Batch 0/1
Batch Loss = 3.1607917774856262
2304, epoch_train_loss=3.1607917774856262
Epoch 2305
Epoch 2305 :: Batch 0/1
Batch Loss = 3.168788313241255
2305, epoch_train_loss=3.168788313241255
Epoch 2306
Epoch 2306 :: Batch 0/1
Batch Loss = 3.161711422474054
2306, epoch_train_loss=3.161711422474054
Epoch 2307
Epoch 2307 :: Batch 0/1
Batch Loss = 3.151328731251044
2307, epoch_train_loss=3.151328731251044
Epoch 2308
Epoch 2308 :: Batch 0/1
Batch Loss = 3.1463086428805433
2308, epoch_train_loss=3.1463086428805433
Epoch 2309
Epoch 2309 :: Batch 0/1
Batch Loss = 3.1546860113863464
2309, epoch_train_loss=3.1546860113863464
Epoch 2310
Epoch 2310 :: Batch 0/1
Batch Loss = 3.1377111576887717
2310, epoch_train_loss=3.1377111576887717
Epoch 2311
Epoch 2311 :: Batch 0/1
Batch Loss = 3.135508915993404
2311, epoch_train_loss=3.135508915993404
Epoch 2312
Epoch 2312 :: Batch 0/1
Batch Loss = 3.129814384087146
2312, epoch_train_loss=3.129814384087146
Epoch 2313
Epoch 2313 :: Batch 0/1
Batch Loss = 3.1279310116080237
2313, epoch_train_loss=3.1279310116080237
Epoch 2314
Epoch 2314 :: Batch 0/1
Batch Loss = 3.1253840423354604
2314, epoch_train_loss=3.1253840423354604
Epoch 2315
Epoch 2315 :: Batch 0/1
Batch Loss = 3.116612460434652
2315, epoch_train_loss=3.116612460434652
Epoch 2316
Epoch 2316 :: Batch 0/1
Batch Loss = 3.1145895449771333
2316, epoch_train_loss=3.1145895449771333
Epoch 2317
Epoch 2317 :: Batch 0/1
Batch Loss = 3.108882400745361
2317, epoch_train_loss=3.108882400745361
Epoch 2318
Epoch 2318 :: Batch 0/1
Batch Loss = 3.1094028135029754
2318, epoch_train_loss=3.1094028135029754
Epoch 2319
Epoch 2319 :: Batch 0/1
Batch Loss = 3.104435601760996
2319, epoch_train_loss=3.104435601760996
Epoch 2320
Epoch 2320 :: Batch 0/1
Batch Loss = 3.101252165300571
2320, epoch_train_loss=3.101252165300571
Epoch 2321
Epoch 2321 :: Batch 0/1
Batch Loss = 3.099888355000004
2321, epoch_train_loss=3.099888355000004
Epoch 2322
Epoch 2322 :: Batch 0/1
Batch Loss = 3.0955980473936733
2322, epoch_train_loss=3.0955980473936733
Epoch 2323
Epoch 2323 :: Batch 0/1
Batch Loss = 3.094330795560245
2323, epoch_train_loss=3.094330795560245
Epoch 2324
Epoch 2324 :: Batch 0/1
Batch Loss = 3.0909549742250997
2324, epoch_train_loss=3.0909549742250997
Epoch 2325
Epoch 2325 :: Batch 0/1
Batch Loss = 3.088023004645939
2325, epoch_train_loss=3.088023004645939
Epoch 2326
Epoch 2326 :: Batch 0/1
Batch Loss = 3.0859433108231986
2326, epoch_train_loss=3.0859433108231986
Epoch 2327
Epoch 2327 :: Batch 0/1
Batch Loss = 3.082010876119692
2327, epoch_train_loss=3.082010876119692
Epoch 2328
Epoch 2328 :: Batch 0/1
Batch Loss = 3.07957227148405
2328, epoch_train_loss=3.07957227148405
Epoch 2329
Epoch 2329 :: Batch 0/1
Batch Loss = 3.0749011690785513
2329, epoch_train_loss=3.0749011690785513
Epoch 2330
Epoch 2330 :: Batch 0/1
Batch Loss = 3.0708536268581663
2330, epoch_train_loss=3.0708536268581663
Epoch 2331
Epoch 2331 :: Batch 0/1
Batch Loss = 3.0685272509670485
2331, epoch_train_loss=3.0685272509670485
Epoch 2332
Epoch 2332 :: Batch 0/1
Batch Loss = 3.0642733697474136
2332, epoch_train_loss=3.0642733697474136
Epoch 2333
Epoch 2333 :: Batch 0/1
Batch Loss = 3.0601505655657424
2333, epoch_train_loss=3.0601505655657424
Epoch 2334
Epoch 2334 :: Batch 0/1
Batch Loss = 3.0573464693797523
2334, epoch_train_loss=3.0573464693797523
Epoch 2335
Epoch 2335 :: Batch 0/1
Batch Loss = 3.053742202175878
2335, epoch_train_loss=3.053742202175878
Epoch 2336
Epoch 2336 :: Batch 0/1
Batch Loss = 3.0512001147153422
2336, epoch_train_loss=3.0512001147153422
Epoch 2337
Epoch 2337 :: Batch 0/1
Batch Loss = 3.049095253245187
2337, epoch_train_loss=3.049095253245187
Epoch 2338
Epoch 2338 :: Batch 0/1
Batch Loss = 3.0459268320523956
2338, epoch_train_loss=3.0459268320523956
Epoch 2339
Epoch 2339 :: Batch 0/1
Batch Loss = 3.042813181876823
2339, epoch_train_loss=3.042813181876823
Epoch 2340
Epoch 2340 :: Batch 0/1
Batch Loss = 3.040938509744483
2340, epoch_train_loss=3.040938509744483
Epoch 2341
Epoch 2341 :: Batch 0/1
Batch Loss = 3.0387942317511656
2341, epoch_train_loss=3.0387942317511656
Epoch 2342
Epoch 2342 :: Batch 0/1
Batch Loss = 3.036321121199583
2342, epoch_train_loss=3.036321121199583
Epoch 2343
Epoch 2343 :: Batch 0/1
Batch Loss = 3.0345520269306663
2343, epoch_train_loss=3.0345520269306663
Epoch 2344
Epoch 2344 :: Batch 0/1
Batch Loss = 3.032496176349156
2344, epoch_train_loss=3.032496176349156
Epoch 2345
Epoch 2345 :: Batch 0/1
Batch Loss = 3.0296614238275428
2345, epoch_train_loss=3.0296614238275428
Epoch 2346
Epoch 2346 :: Batch 0/1
Batch Loss = 3.027082168190506
2346, epoch_train_loss=3.027082168190506
Epoch 2347
Epoch 2347 :: Batch 0/1
Batch Loss = 3.092617994325547
2347, epoch_train_loss=3.092617994325547
Epoch 2348
Epoch 2348 :: Batch 0/1
Batch Loss = 3.0639595472164802
2348, epoch_train_loss=3.0639595472164802
Epoch 2349
Epoch 2349 :: Batch 0/1
Batch Loss = 3.075924418937182
2349, epoch_train_loss=3.075924418937182
Epoch 2350
Epoch 2350 :: Batch 0/1
Batch Loss = 3.0781055208773513
2350, epoch_train_loss=3.0781055208773513
Epoch 2351
Epoch 2351 :: Batch 0/1
Batch Loss = 3.0780193093748807
2351, epoch_train_loss=3.0780193093748807
Epoch 2352
Epoch 2352 :: Batch 0/1
Batch Loss = 3.0747067716265497
2352, epoch_train_loss=3.0747067716265497
Epoch 2353
Epoch 2353 :: Batch 0/1
Batch Loss = 3.070422347465826
2353, epoch_train_loss=3.070422347465826
Epoch 2354
Epoch 2354 :: Batch 0/1
Batch Loss = 3.068243176272896
2354, epoch_train_loss=3.068243176272896
Epoch 2355
Epoch 2355 :: Batch 0/1
Batch Loss = 3.064850321834581
2355, epoch_train_loss=3.064850321834581
Epoch 2356
Epoch 2356 :: Batch 0/1
Batch Loss = 3.058897505560064
2356, epoch_train_loss=3.058897505560064
Epoch 2357
Epoch 2357 :: Batch 0/1
Batch Loss = 3.771354814498769
2357, epoch_train_loss=3.771354814498769
Epoch 2358
Epoch 2358 :: Batch 0/1
Batch Loss = 3.6964321466346717
2358, epoch_train_loss=3.6964321466346717
Epoch 2359
Epoch 2359 :: Batch 0/1
Batch Loss = 3.785342166450021
2359, epoch_train_loss=3.785342166450021
Epoch 2360
Epoch 2360 :: Batch 0/1
Batch Loss = 3.756460274240352
2360, epoch_train_loss=3.756460274240352
Epoch 2361
Epoch 2361 :: Batch 0/1
Batch Loss = 3.7423387388270615
2361, epoch_train_loss=3.7423387388270615
Epoch 2362
Epoch 2362 :: Batch 0/1
Batch Loss = 3.739474797982982
2362, epoch_train_loss=3.739474797982982
Epoch 2363
Epoch 2363 :: Batch 0/1
Batch Loss = 3.7382787635987285
2363, epoch_train_loss=3.7382787635987285
Epoch 2364
Epoch 2364 :: Batch 0/1
Batch Loss = 3.737043271774957
2364, epoch_train_loss=3.737043271774957
Epoch 2365
Epoch 2365 :: Batch 0/1
Batch Loss = 3.731403704002526
2365, epoch_train_loss=3.731403704002526
Epoch 2366
Epoch 2366 :: Batch 0/1
Batch Loss = 3.721274622763186
2366, epoch_train_loss=3.721274622763186
Epoch 2367
Epoch 2367 :: Batch 0/1
Batch Loss = 3.70407507235771
2367, epoch_train_loss=3.70407507235771
Epoch 2368
Epoch 2368 :: Batch 0/1
Batch Loss = 3.6792445604525708
2368, epoch_train_loss=3.6792445604525708
Epoch 2369
Epoch 2369 :: Batch 0/1
Batch Loss = 3.651083196884065
2369, epoch_train_loss=3.651083196884065
Epoch 2370
Epoch 2370 :: Batch 0/1
Batch Loss = 3.6254840135782254
2370, epoch_train_loss=3.6254840135782254
Epoch 2371
Epoch 2371 :: Batch 0/1
Batch Loss = 3.6042206697569714
2371, epoch_train_loss=3.6042206697569714
Epoch 2372
Epoch 2372 :: Batch 0/1
Batch Loss = 3.5877051438121295
2372, epoch_train_loss=3.5877051438121295
Epoch 2373
Epoch 2373 :: Batch 0/1
Batch Loss = 3.5755017077997895
2373, epoch_train_loss=3.5755017077997895
Epoch 2374
Epoch 2374 :: Batch 0/1
Batch Loss = 3.5637052485007734
2374, epoch_train_loss=3.5637052485007734
Epoch 2375
Epoch 2375 :: Batch 0/1
Batch Loss = 3.5530887058235296
2375, epoch_train_loss=3.5530887058235296
Epoch 2376
Epoch 2376 :: Batch 0/1
Batch Loss = 3.540499940110433
2376, epoch_train_loss=3.540499940110433
Epoch 2377
Epoch 2377 :: Batch 0/1
Batch Loss = 3.528136663465841
2377, epoch_train_loss=3.528136663465841
Epoch 2378
Epoch 2378 :: Batch 0/1
Batch Loss = 3.5162908801299557
2378, epoch_train_loss=3.5162908801299557
Epoch 2379
Epoch 2379 :: Batch 0/1
Batch Loss = 3.492654559648313
2379, epoch_train_loss=3.492654559648313
Epoch 2380
Epoch 2380 :: Batch 0/1
Batch Loss = 6.645222774306775
2380, epoch_train_loss=6.645222774306775
Epoch 2381
Epoch 2381 :: Batch 0/1
Batch Loss = 3.5096864743270975
2381, epoch_train_loss=3.5096864743270975
Epoch 2382
Epoch 2382 :: Batch 0/1
Batch Loss = 3.5028281488056328
2382, epoch_train_loss=3.5028281488056328
Epoch 2383
Epoch 2383 :: Batch 0/1
Batch Loss = 3.4940185752818347
2383, epoch_train_loss=3.4940185752818347
Epoch 2384
Epoch 2384 :: Batch 0/1
Batch Loss = 3.4838513797366573
2384, epoch_train_loss=3.4838513797366573
Epoch 2385
Epoch 2385 :: Batch 0/1
Batch Loss = 3.4928309455087025
2385, epoch_train_loss=3.4928309455087025
Epoch 2386
Epoch 2386 :: Batch 0/1
Batch Loss = 3.484427192303308
2386, epoch_train_loss=3.484427192303308
Epoch 2387
Epoch 2387 :: Batch 0/1
Batch Loss = 3.4850302997298432
2387, epoch_train_loss=3.4850302997298432
Epoch 2388
Epoch 2388 :: Batch 0/1
Batch Loss = 3.4812785641006836
2388, epoch_train_loss=3.4812785641006836
Epoch 2389
Epoch 2389 :: Batch 0/1
Batch Loss = 3.4765577588185366
2389, epoch_train_loss=3.4765577588185366
Epoch 2390
Epoch 2390 :: Batch 0/1
Batch Loss = 3.471983148843435
2390, epoch_train_loss=3.471983148843435
Epoch 2391
Epoch 2391 :: Batch 0/1
Batch Loss = 3.4681765051074196
2391, epoch_train_loss=3.4681765051074196
Epoch 2392
Epoch 2392 :: Batch 0/1
Batch Loss = 3.4641515488669636
2392, epoch_train_loss=3.4641515488669636
Epoch 2393
Epoch 2393 :: Batch 0/1
Batch Loss = 3.4591071029830363
2393, epoch_train_loss=3.4591071029830363
Epoch 2394
Epoch 2394 :: Batch 0/1
Batch Loss = 3.456494939625504
2394, epoch_train_loss=3.456494939625504
Epoch 2395
Epoch 2395 :: Batch 0/1
Batch Loss = 3.457831294748196
2395, epoch_train_loss=3.457831294748196
Epoch 2396
Epoch 2396 :: Batch 0/1
Batch Loss = 3.489798505887384
2396, epoch_train_loss=3.489798505887384
Epoch 2397
Epoch 2397 :: Batch 0/1
Batch Loss = 3.7700935446329424
2397, epoch_train_loss=3.7700935446329424
Epoch 2398
Epoch 2398 :: Batch 0/1
Batch Loss = 3.8917683458190906
2398, epoch_train_loss=3.8917683458190906
Epoch 2399
Epoch 2399 :: Batch 0/1
Batch Loss = 3.4670173725819575
2399, epoch_train_loss=3.4670173725819575
Epoch 2400
Epoch 2400 :: Batch 0/1
Batch Loss = 3.483287851294216
2400, epoch_train_loss=3.483287851294216
Epoch 2401
Epoch 2401 :: Batch 0/1
Batch Loss = 3.5254313199618985
2401, epoch_train_loss=3.5254313199618985
Epoch 2402
Epoch 2402 :: Batch 0/1
Batch Loss = 3.5417672079348677
2402, epoch_train_loss=3.5417672079348677
Epoch 2403
Epoch 2403 :: Batch 0/1
Batch Loss = 3.5392612077537824
2403, epoch_train_loss=3.5392612077537824
Epoch 2404
Epoch 2404 :: Batch 0/1
Batch Loss = 3.5363984815271987
2404, epoch_train_loss=3.5363984815271987
Epoch 2405
Epoch 2405 :: Batch 0/1
Batch Loss = 3.521739454877358
2405, epoch_train_loss=3.521739454877358
Epoch 2406
Epoch 2406 :: Batch 0/1
Batch Loss = 3.504612381175788
2406, epoch_train_loss=3.504612381175788
Epoch 2407
Epoch 2407 :: Batch 0/1
Batch Loss = 3.5035927284188197
2407, epoch_train_loss=3.5035927284188197
Epoch 2408
Epoch 2408 :: Batch 0/1
Batch Loss = 3.5134105675479037
2408, epoch_train_loss=3.5134105675479037
Epoch 2409
Epoch 2409 :: Batch 0/1
Batch Loss = 3.5139980318820765
2409, epoch_train_loss=3.5139980318820765
Epoch 2410
Epoch 2410 :: Batch 0/1
Batch Loss = 3.5147153173274313
2410, epoch_train_loss=3.5147153173274313
Epoch 2411
Epoch 2411 :: Batch 0/1
Batch Loss = 3.5157430579736566
2411, epoch_train_loss=3.5157430579736566
Epoch 2412
Epoch 2412 :: Batch 0/1
Batch Loss = 3.5149030950594278
2412, epoch_train_loss=3.5149030950594278
Epoch 2413
Epoch 2413 :: Batch 0/1
Batch Loss = 3.5107654083215247
2413, epoch_train_loss=3.5107654083215247
Epoch 2414
Epoch 2414 :: Batch 0/1
Batch Loss = 3.5038248891828476
2414, epoch_train_loss=3.5038248891828476
Epoch 2415
Epoch 2415 :: Batch 0/1
Batch Loss = 3.494140198779866
2415, epoch_train_loss=3.494140198779866
Epoch 2416
Epoch 2416 :: Batch 0/1
Batch Loss = 3.485102917950933
2416, epoch_train_loss=3.485102917950933
Epoch 2417
Epoch 2417 :: Batch 0/1
Batch Loss = 3.48176947609074
2417, epoch_train_loss=3.48176947609074
Epoch 2418
Epoch 2418 :: Batch 0/1
Batch Loss = 3.4827238218327503
2418, epoch_train_loss=3.4827238218327503
Epoch 2419
Epoch 2419 :: Batch 0/1
Batch Loss = 3.48472938549628
2419, epoch_train_loss=3.48472938549628
Epoch 2420
Epoch 2420 :: Batch 0/1
Batch Loss = 3.4852673893173143
2420, epoch_train_loss=3.4852673893173143
Epoch 2421
Epoch 2421 :: Batch 0/1
Batch Loss = 3.4833719644365946
2421, epoch_train_loss=3.4833719644365946
Epoch 2422
Epoch 2422 :: Batch 0/1
Batch Loss = 3.4791819870320695
2422, epoch_train_loss=3.4791819870320695
Epoch 2423
Epoch 2423 :: Batch 0/1
Batch Loss = 3.473881523942639
2423, epoch_train_loss=3.473881523942639
Epoch 2424
Epoch 2424 :: Batch 0/1
Batch Loss = 3.4691428902644277
2424, epoch_train_loss=3.4691428902644277
Epoch 2425
Epoch 2425 :: Batch 0/1
Batch Loss = 3.466071733285688
2425, epoch_train_loss=3.466071733285688
Epoch 2426
Epoch 2426 :: Batch 0/1
Batch Loss = 3.464465248925455
2426, epoch_train_loss=3.464465248925455
Epoch 2427
Epoch 2427 :: Batch 0/1
Batch Loss = 3.4635744926111265
2427, epoch_train_loss=3.4635744926111265
Epoch 2428
Epoch 2428 :: Batch 0/1
Batch Loss = 3.4626186344390906
2428, epoch_train_loss=3.4626186344390906
Epoch 2429
Epoch 2429 :: Batch 0/1
Batch Loss = 3.461068949214421
2429, epoch_train_loss=3.461068949214421
Epoch 2430
Epoch 2430 :: Batch 0/1
Batch Loss = 3.458825847737396
2430, epoch_train_loss=3.458825847737396
Epoch 2431
Epoch 2431 :: Batch 0/1
Batch Loss = 3.456141701495477
2431, epoch_train_loss=3.456141701495477
Epoch 2432
Epoch 2432 :: Batch 0/1
Batch Loss = 3.453444952901016
2432, epoch_train_loss=3.453444952901016
Epoch 2433
Epoch 2433 :: Batch 0/1
Batch Loss = 3.4509550943634575
2433, epoch_train_loss=3.4509550943634575
Epoch 2434
Epoch 2434 :: Batch 0/1
Batch Loss = 3.448896147302656
2434, epoch_train_loss=3.448896147302656
Epoch 2435
Epoch 2435 :: Batch 0/1
Batch Loss = 3.4473375377473063
2435, epoch_train_loss=3.4473375377473063
Epoch 2436
Epoch 2436 :: Batch 0/1
Batch Loss = 3.4459548001361067
2436, epoch_train_loss=3.4459548001361067
Epoch 2437
Epoch 2437 :: Batch 0/1
Batch Loss = 3.444347331661417
2437, epoch_train_loss=3.444347331661417
Epoch 2438
Epoch 2438 :: Batch 0/1
Batch Loss = 3.442471924912624
2438, epoch_train_loss=3.442471924912624
Epoch 2439
Epoch 2439 :: Batch 0/1
Batch Loss = 3.440390689891881
2439, epoch_train_loss=3.440390689891881
Epoch 2440
Epoch 2440 :: Batch 0/1
Batch Loss = 3.438267162033691
2440, epoch_train_loss=3.438267162033691
Epoch 2441
Epoch 2441 :: Batch 0/1
Batch Loss = 3.4362371144680486
2441, epoch_train_loss=3.4362371144680486
Epoch 2442
Epoch 2442 :: Batch 0/1
Batch Loss = 3.434447651337986
2442, epoch_train_loss=3.434447651337986
Epoch 2443
Epoch 2443 :: Batch 0/1
Batch Loss = 3.4328808730000113
2443, epoch_train_loss=3.4328808730000113
Epoch 2444
Epoch 2444 :: Batch 0/1
Batch Loss = 3.4314846816651086
2444, epoch_train_loss=3.4314846816651086
Epoch 2445
Epoch 2445 :: Batch 0/1
Batch Loss = 3.4301238551888766
2445, epoch_train_loss=3.4301238551888766
Epoch 2446
Epoch 2446 :: Batch 0/1
Batch Loss = 3.428664943354429
2446, epoch_train_loss=3.428664943354429
Epoch 2447
Epoch 2447 :: Batch 0/1
Batch Loss = 3.4269949608464936
2447, epoch_train_loss=3.4269949608464936
Epoch 2448
Epoch 2448 :: Batch 0/1
Batch Loss = 3.4252322360399057
2448, epoch_train_loss=3.4252322360399057
Epoch 2449
Epoch 2449 :: Batch 0/1
Batch Loss = 3.423464496334617
2449, epoch_train_loss=3.423464496334617
Epoch 2450
Epoch 2450 :: Batch 0/1
Batch Loss = 3.4217563308586145
2450, epoch_train_loss=3.4217563308586145
Epoch 2451
Epoch 2451 :: Batch 0/1
Batch Loss = 3.420108678632131
2451, epoch_train_loss=3.420108678632131
Epoch 2452
Epoch 2452 :: Batch 0/1
Batch Loss = 3.41859129894267
2452, epoch_train_loss=3.41859129894267
Epoch 2453
Epoch 2453 :: Batch 0/1
Batch Loss = 3.41713921613436
2453, epoch_train_loss=3.41713921613436
Epoch 2454
Epoch 2454 :: Batch 0/1
Batch Loss = 3.4156948350338605
2454, epoch_train_loss=3.4156948350338605
Epoch 2455
Epoch 2455 :: Batch 0/1
Batch Loss = 3.41422981499139
2455, epoch_train_loss=3.41422981499139
Epoch 2456
Epoch 2456 :: Batch 0/1
Batch Loss = 3.412748566010179
2456, epoch_train_loss=3.412748566010179
Epoch 2457
Epoch 2457 :: Batch 0/1
Batch Loss = 3.4112512380750917
2457, epoch_train_loss=3.4112512380750917
Epoch 2458
Epoch 2458 :: Batch 0/1
Batch Loss = 3.4097742706665177
2458, epoch_train_loss=3.4097742706665177
Epoch 2459
Epoch 2459 :: Batch 0/1
Batch Loss = 3.408407684603883
2459, epoch_train_loss=3.408407684603883
Epoch 2460
Epoch 2460 :: Batch 0/1
Batch Loss = 3.4070663171294875
2460, epoch_train_loss=3.4070663171294875
Epoch 2461
Epoch 2461 :: Batch 0/1
Batch Loss = 3.4057446664097375
2461, epoch_train_loss=3.4057446664097375
Epoch 2462
Epoch 2462 :: Batch 0/1
Batch Loss = 3.4044247637236555
2462, epoch_train_loss=3.4044247637236555
Epoch 2463
Epoch 2463 :: Batch 0/1
Batch Loss = 3.4031138683682287
2463, epoch_train_loss=3.4031138683682287
Epoch 2464
Epoch 2464 :: Batch 0/1
Batch Loss = 3.401791233753592
2464, epoch_train_loss=3.401791233753592
Epoch 2465
Epoch 2465 :: Batch 0/1
Batch Loss = 3.4004828637204656
2465, epoch_train_loss=3.4004828637204656
Epoch 2466
Epoch 2466 :: Batch 0/1
Batch Loss = 3.399151841727073
2466, epoch_train_loss=3.399151841727073
Epoch 2467
Epoch 2467 :: Batch 0/1
Batch Loss = 3.3978495045184647
2467, epoch_train_loss=3.3978495045184647
Epoch 2468
Epoch 2468 :: Batch 0/1
Batch Loss = 3.3966124384066148
2468, epoch_train_loss=3.3966124384066148
Epoch 2469
Epoch 2469 :: Batch 0/1
Batch Loss = 3.3953960301307737
2469, epoch_train_loss=3.3953960301307737
Epoch 2470
Epoch 2470 :: Batch 0/1
Batch Loss = 3.394198805911677
2470, epoch_train_loss=3.394198805911677
Epoch 2471
Epoch 2471 :: Batch 0/1
Batch Loss = 3.3929960488746107
2471, epoch_train_loss=3.3929960488746107
Epoch 2472
Epoch 2472 :: Batch 0/1
Batch Loss = 3.391776250593915
2472, epoch_train_loss=3.391776250593915
Epoch 2473
Epoch 2473 :: Batch 0/1
Batch Loss = 3.390566533332521
2473, epoch_train_loss=3.390566533332521
Epoch 2474
Epoch 2474 :: Batch 0/1
Batch Loss = 3.389393183743293
2474, epoch_train_loss=3.389393183743293
Epoch 2475
Epoch 2475 :: Batch 0/1
Batch Loss = 3.388268360997624
2475, epoch_train_loss=3.388268360997624
Epoch 2476
Epoch 2476 :: Batch 0/1
Batch Loss = 3.387172627853687
2476, epoch_train_loss=3.387172627853687
Epoch 2477
Epoch 2477 :: Batch 0/1
Batch Loss = 3.386101730740721
2477, epoch_train_loss=3.386101730740721
Epoch 2478
Epoch 2478 :: Batch 0/1
Batch Loss = 3.3850405086298805
2478, epoch_train_loss=3.3850405086298805
Epoch 2479
Epoch 2479 :: Batch 0/1
Batch Loss = 3.38397445226062
2479, epoch_train_loss=3.38397445226062
Epoch 2480
Epoch 2480 :: Batch 0/1
Batch Loss = 3.3829160025385776
2480, epoch_train_loss=3.3829160025385776
Epoch 2481
Epoch 2481 :: Batch 0/1
Batch Loss = 3.381832788507461
2481, epoch_train_loss=3.381832788507461
Epoch 2482
Epoch 2482 :: Batch 0/1
Batch Loss = 3.3807255059152284
2482, epoch_train_loss=3.3807255059152284
Epoch 2483
Epoch 2483 :: Batch 0/1
Batch Loss = 3.3795777369994258
2483, epoch_train_loss=3.3795777369994258
Epoch 2484
Epoch 2484 :: Batch 0/1
Batch Loss = 3.378421057106975
2484, epoch_train_loss=3.378421057106975
Epoch 2485
Epoch 2485 :: Batch 0/1
Batch Loss = 3.3772510413349353
2485, epoch_train_loss=3.3772510413349353
Epoch 2486
Epoch 2486 :: Batch 0/1
Batch Loss = 3.3761008936360413
2486, epoch_train_loss=3.3761008936360413
Epoch 2487
Epoch 2487 :: Batch 0/1
Batch Loss = 3.374966277544708
2487, epoch_train_loss=3.374966277544708
Epoch 2488
Epoch 2488 :: Batch 0/1
Batch Loss = 3.3738546527145274
2488, epoch_train_loss=3.3738546527145274
Epoch 2489
Epoch 2489 :: Batch 0/1
Batch Loss = 3.3727780739515216
2489, epoch_train_loss=3.3727780739515216
Epoch 2490
Epoch 2490 :: Batch 0/1
Batch Loss = 3.371700577258213
2490, epoch_train_loss=3.371700577258213
Epoch 2491
Epoch 2491 :: Batch 0/1
Batch Loss = 3.37061672635764
2491, epoch_train_loss=3.37061672635764
Epoch 2492
Epoch 2492 :: Batch 0/1
Batch Loss = 3.369517604802585
2492, epoch_train_loss=3.369517604802585
Epoch 2493
Epoch 2493 :: Batch 0/1
Batch Loss = 3.3684425774773823
2493, epoch_train_loss=3.3684425774773823
Epoch 2494
Epoch 2494 :: Batch 0/1
Batch Loss = 3.367386340771009
2494, epoch_train_loss=3.367386340771009
Epoch 2495
Epoch 2495 :: Batch 0/1
Batch Loss = 3.3663243278475363
2495, epoch_train_loss=3.3663243278475363
Epoch 2496
Epoch 2496 :: Batch 0/1
Batch Loss = 3.365247318280369
2496, epoch_train_loss=3.365247318280369
Epoch 2497
Epoch 2497 :: Batch 0/1
Batch Loss = 3.364148169310152
2497, epoch_train_loss=3.364148169310152
Epoch 2498
Epoch 2498 :: Batch 0/1
Batch Loss = 3.363078332806685
2498, epoch_train_loss=3.363078332806685
Epoch 2499
Epoch 2499 :: Batch 0/1
Batch Loss = 3.362035518933991
2499, epoch_train_loss=3.362035518933991
