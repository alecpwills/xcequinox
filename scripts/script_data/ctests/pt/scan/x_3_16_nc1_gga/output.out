/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting "B3LYP_WITH_VWN5 = True" in pyscf_conf.py
  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8043520> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8043520> in UKS object of <class 'pyscf.dft.uks.UKS'>
<pyscf.gto.mole.Mole object at 0x7ffeb8043520> [['P', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb8042710> [['N', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb8040b50> [['H', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb8042ef0> [['Li', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb8043e50> [['O', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb80426b0> [['Cl', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb80427a0> [['Al', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb8042ce0> [['S', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb8041d80> [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb80422f0> [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb80408b0> [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb8041a80> [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb8041450> [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb8040dc0> [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb8043df0> [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb8041db0> [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb8040c40> [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb8041810> [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb8040310> [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb8042800> [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb8043880> [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb80420e0> [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb8043fa0> [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb8041960> [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb8043a30> [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]] 5
<pyscf.gto.mole.Mole object at 0x7ffeb80437f0> [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb8041690> [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]] 5
mol:  [['P', array([0., 0., 0.])]]
converged SCF energy = -341.104145992717  <S^2> = 3.7502984  2S+1 = 4.0001492
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8042710> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8042710> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-3.47389956e-03 -8.82676818e-04 -2.08411238e-03 ... -1.11301603e+01
 -1.11301603e+01 -1.11301603e+01] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 3)
mol:  [['N', array([0., 0., 0.])]]
converged SCF energy = -54.5289742046675  <S^2> = 3.7524945  2S+1 = 4.0012471
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8040b50> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8040b50> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5016, 22)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.07670570e-03 -5.92340671e-04 -6.66573372e-05 ... -5.03679786e+00
 -5.03679786e+00 -5.03679786e+00] = SCAN,
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5016, 22)
rho shape (4, 4, 2, 5016)
rho_filt shape: (5016,)
get descriptors tdrho.shape=(5016, 3)
mol:  [['H', array([0., 0., 0.])]]
converged SCF energy = -0.499812984008539  <S^2> = 0.75  2S+1 = 2
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8042ef0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8042ef0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 7, 7)
ao.shape (10, 2440, 7)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.59173730e-03 -7.67300285e-04 -6.09330303e-05 ... -7.27736608e-01
 -7.27736608e-01 -7.27736608e-01] = SCAN,
get_data, dm shape = (2, 7, 7)
ao_eval.shape=(4, 2440, 7)
rho shape (4, 4, 2, 2440)
rho_filt shape: (2440,)
get descriptors tdrho.shape=(2440, 3)
mol:  [['Li', array([0., 0., 0.])]]
converged SCF energy = -7.46006188627842  <S^2> = 0.75000049  2S+1 = 2.0000005
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8043e50> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8043e50> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 4592, 22)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-6.71503005e-03 -1.44519923e-03 -1.44519923e-03 ... -1.46899070e-02
 -2.03947707e+00 -2.03947707e+00] = SCAN,
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 4592, 22)
rho shape (4, 4, 2, 4592)
rho_filt shape: (4592,)
get descriptors tdrho.shape=(4592, 3)
mol:  [['O', array([0., 0., 0.])]]
converged SCF energy = -75.0033834885785  <S^2> = 2.0027437  2S+1 = 3.0018286
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb80426b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb80426b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5040, 22)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.03909418e-03 -1.69662637e-04 -8.02681466e-06 ... -5.78449312e+00
 -5.78449312e+00 -5.78449312e+00] = SCAN,
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5040, 22)
rho shape (4, 4, 2, 5040)
rho_filt shape: (5040,)
get descriptors tdrho.shape=(5040, 3)
mol:  [['Cl', array([0., 0., 0.])]]
converged SCF energy = -459.957577120879  <S^2> = 0.7516194  2S+1 = 2.0016187
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb80427a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb80427a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6152, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-7.06820868e-04 -9.17979579e-04 -3.13830971e-04 ... -1.26648275e+01
 -1.26648275e+01 -1.26648275e+01] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6152, 30)
rho shape (4, 4, 2, 6152)
rho_filt shape: (6152,)
get descriptors tdrho.shape=(6152, 3)
mol:  [['Al', array([0., 0., 0.])]]
converged SCF energy = -242.226560993883  <S^2> = 0.75226414  2S+1 = 2.0022629
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8042ce0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8042ce0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6088, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-0.02356543 -0.0151998  -0.00773866 ... -0.00013664 -0.00169117
 -0.00012232] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6088, 30)
rho shape (4, 4, 2, 6088)
rho_filt shape: (6088,)
get descriptors tdrho.shape=(6088, 3)
mol:  [['S', array([0., 0., 0.])]]
converged SCF energy = -397.938786807112  <S^2> = 2.0022329  2S+1 = 3.0014882
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8041d80> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8041d80> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.59239226e-03 -7.51631110e-04 -9.06634250e-04 ... -1.18986567e+01
 -1.18986567e+01 -1.18986567e+01] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 3)
mol:  [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]]
converged SCF energy = -1.16580491182912  <S^2> = 2.220446e-16  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb80422f0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb80422f0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 14, 14)
ao.shape (10, 4776, 14)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.31557088e-04 -9.73828620e-06 -3.66768667e-04 ... -5.54165574e-01
 -5.54165574e-01 -5.54165574e-01] = SCAN,
get_data, dm shape = (2, 14, 14)
ao_eval.shape=(4, 4776, 14)
rho shape (4, 4, 2, 4776)
rho_filt shape: (4776,)
get descriptors tdrho.shape=(4776, 3)
mol:  [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]]
converged SCF energy = -109.439263799566  <S^2> = 1.7763568e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb80408b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb80408b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9848, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-9.68474977e-05 -9.84742592e-04 -2.59676393e-04 ... -2.39645778e-05
 -2.39645778e-05 -9.68474977e-05] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9848, 44)
rho shape (4, 4, 2, 9848)
rho_filt shape: (9848,)
get descriptors tdrho.shape=(9848, 3)
mol:  [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]]
converged SCF energy = -107.339357395734  <S^2> = 8.8817842e-15  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8041a80> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8041a80> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9752, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.04987770e-03 -6.68954111e-04 -8.57556562e-04 ... -1.07485605e-03
 -8.01425702e-01 -8.01425702e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9752, 44)
rho shape (4, 4, 2, 9752)
rho_filt shape: (9752,)
get descriptors tdrho.shape=(9752, 3)
mol:  [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]]
converged SCF energy = -93.3377924465129  <S^2> = 4.0072834e-10  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8041450> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8041450> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 12256, 51)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.97917639e-04 -2.54437615e-05 -3.15202008e-05 ... -6.37386388e-01
 -6.37386388e-01 -6.37386388e-01] = SCAN,
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 12256, 51)
rho shape (4, 4, 2, 12256)
rho_filt shape: (12256,)
get descriptors tdrho.shape=(12256, 3)
mol:  [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]]
converged SCF energy = -188.456965322844  <S^2> = 1.7763568e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8040dc0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8040dc0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 66, 66)
ao.shape (10, 14920, 66)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.50217343e-04 -2.07520331e-04 -9.23619961e-04 ... -2.76182455e-06
 -4.27559894e+00 -4.27559894e+00] = SCAN,
get_data, dm shape = (2, 66, 66)
ao_eval.shape=(4, 14920, 66)
rho shape (4, 4, 2, 14920)
rho_filt shape: (14920,)
get descriptors tdrho.shape=(14920, 3)
mol:  [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]]
converged SCF energy = -920.00560888896  <S^2> = 5.0093263e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8043df0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8043df0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12208, 60)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-0.00155834 -0.00091688 -0.00215831 ... -0.00091688 -0.41618506
 -0.41618506] = SCAN,
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12208, 60)
rho shape (4, 4, 2, 12208)
rho_filt shape: (12208,)
get descriptors tdrho.shape=(12208, 3)
mol:  [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]]
converged SCF energy = -199.394370591172  <S^2> = 1.2079227e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8041db0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8041db0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9824, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.92948752e-04 -1.95215890e-05 -1.16699780e-03 ... -4.89378326e-01
 -4.89378326e-01 -4.89378326e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9824, 44)
rho shape (4, 4, 2, 9824)
rho_filt shape: (9824,)
get descriptors tdrho.shape=(9824, 3)
mol:  [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]]
converged SCF energy = -150.214894485937  <S^2> = 1.0018599  2S+1 = 2.2377309
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8040c40> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8040c40> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9912, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-7.55964964e-04 -1.35225323e-04 -6.99690653e-06 ... -6.59150642e-01
 -6.59150642e-01 -6.59150642e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9912, 44)
rho shape (4, 4, 2, 9912)
rho_filt shape: (9912,)
get descriptors tdrho.shape=(9912, 3)
mol:  [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]]
converged SCF energy = -77.2435048346375  <S^2> = 1.2434498e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8041810> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8041810> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 58, 58)
ao.shape (10, 15208, 58)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-8.83278187e-05 -8.83278187e-05 -9.75839793e-04 ... -3.46740731e-05
 -3.31729009e-05 -3.31729009e-05] = SCAN,
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15208, 58)
rho shape (4, 4, 2, 15208)
rho_filt shape: (15208,)
get descriptors tdrho.shape=(15208, 3)
mol:  [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]]
converged SCF energy = -113.221335689652  <S^2> = 6.5725203e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8040310> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8040310> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 10040, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-5.37000596e-04 -8.55494373e-04 -2.46853248e-03 ... -7.34251993e-01
 -7.34251993e-01 -7.34251993e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 10040, 44)
rho shape (4, 4, 2, 10040)
rho_filt shape: (10040,)
get descriptors tdrho.shape=(10040, 3)
mol:  [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]]
converged SCF energy = -460.624592374077  <S^2> = 6.9277917e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8042800> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8042800> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 37, 37)
ao.shape (10, 8552, 37)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.38161478e-04 -1.81223966e-05 -2.37327566e-05 ... -2.83738108e+00
 -2.83738108e+00 -2.83738108e+00] = SCAN,
get_data, dm shape = (2, 37, 37)
ao_eval.shape=(4, 8552, 37)
rho shape (4, 4, 2, 8552)
rho_filt shape: (8552,)
get descriptors tdrho.shape=(8552, 3)
mol:  [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]]
converged SCF energy = -8.04458854018922  <S^2> = 7.5495166e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8043880> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8043880> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 29, 29)
ao.shape (10, 6936, 29)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-0.00113445 -0.00118624 -0.00118624 ... -0.48434639 -0.48434639
 -0.48434639] = SCAN,
get_data, dm shape = (2, 29, 29)
ao_eval.shape=(4, 6936, 29)
rho shape (4, 4, 2, 6936)
rho_filt shape: (6936,)
get descriptors tdrho.shape=(6936, 3)
mol:  [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]]
converged SCF energy = -324.340512506578  <S^2> = 1.5862867e-11  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb80420e0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb80420e0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 11536, 60)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-0.00297936 -0.00297936 -0.00407091 ... -0.00297936 -0.00297936
 -0.00407091] = SCAN,
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 11536, 60)
rho shape (4, 4, 2, 11536)
rho_filt shape: (11536,)
get descriptors tdrho.shape=(11536, 3)
mol:  [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]]
converged SCF energy = -1622.57507845814  <S^2> = 8.2778229e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8043fa0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8043fa0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 120, 120)
ao.shape (10, 24512, 120)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.61401455e-04 -4.90485117e-04 -2.56451688e-03 ... -9.59296114e+00
 -9.59296114e+00 -9.59296114e+00] = SCAN,
get_data, dm shape = (2, 120, 120)
ao_eval.shape=(4, 24512, 120)
rho shape (4, 4, 2, 24512)
rho_filt shape: (24512,)
get descriptors tdrho.shape=(24512, 3)
mol:  [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]]
converged SCF energy = -342.979728469575  <S^2> = 2.5389468e-11  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8041960> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8041960> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 13096, 51)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.28637187e-03 -4.32380890e-04 -3.74072638e-05 ... -1.91722763e+00
 -1.91722763e+00 -1.91722763e+00] = SCAN,
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 13096, 51)
rho shape (4, 4, 2, 13096)
rho_filt shape: (13096,)
get descriptors tdrho.shape=(13096, 3)
mol:  [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]]
converged SCF energy = -578.565336196082  <S^2> = 1.0034705  2S+1 = 2.2391699
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8043a30> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8043a30> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12384, 60)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.60122985e-04 -2.60140242e-04 -2.60157890e-04 ... -3.86943961e-01
 -3.86943961e-01 -3.86943961e-01] = SCAN,
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12384, 60)
rho shape (4, 4, 2, 12384)
rho_filt shape: (12384,)
get descriptors tdrho.shape=(12384, 3)
mol:  [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]]
converged SCF energy = -40.4598214864076  <S^2> = 3.2152059e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb80437f0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb80437f0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 50, 50)
ao.shape (10, 13936, 50)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-8.68439856e-04 -2.42462783e-04 -1.69965237e-05 ... -2.55256081e-05
 -2.55256081e-05 -2.55256081e-05] = SCAN,
get_data, dm shape = (2, 50, 50)
ao_eval.shape=(4, 13936, 50)
rho shape (4, 4, 2, 13936)
rho_filt shape: (13936,)
get descriptors tdrho.shape=(13936, 3)
mol:  [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]]
converged SCF energy = -39.0756147483503  <S^2> = 6.2079231e-12  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb8041690> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb8041690> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 36, 36)
ao.shape (10, 9656, 36)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-7.67691257e-04 -4.57409182e-05 -2.02835243e-04 ... -1.14928928e+00
 -1.14928928e+00 -1.14928928e+00] = SCAN,
get_data, dm shape = (2, 36, 36)
ao_eval.shape=(4, 9656, 36)
rho shape (4, 4, 2, 9656)
rho_filt shape: (9656,)
get descriptors tdrho.shape=(9656, 3)
mol:  [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]]
converged SCF energy = -291.719272437819  <S^2> = 1.3152146e-11  2S+1 = 1
New DM shape: (2, 58, 58)
ao.shape (10, 15256, 58)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-8.33847724e-04 -2.34902391e-04 -1.75660753e-05 ... -1.92925750e-05
 -1.92925750e-05 -1.92925750e-05] = SCAN,
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15256, 58)
rho shape (4, 4, 2, 15256)
rho_filt shape: (15256,)
get descriptors tdrho.shape=(15256, 3)
PRE NAN FILT: tFxc.shape=(237007,), tdrho.shape=(237007, 3)
nan_filt_rho.shape=(237007,)
nan_filt_fxc.shape=(237007,)
tFxc.shape=(237007,), tdrho.shape=(237007, 3)
inp[0].shape = (237007, 3)
Epoch 0
Epoch 0 :: Batch 0/1
Batch Loss = 4.706672885115587
0, epoch_train_loss=4.706672885115587
Epoch 1
Epoch 1 :: Batch 0/1
Batch Loss = 4.37526545012383
1, epoch_train_loss=4.37526545012383
Epoch 2
Epoch 2 :: Batch 0/1
Batch Loss = 4.142400353165158
2, epoch_train_loss=4.142400353165158
Epoch 3
Epoch 3 :: Batch 0/1
Batch Loss = 3.9709986751182185
3, epoch_train_loss=3.9709986751182185
Epoch 4
Epoch 4 :: Batch 0/1
Batch Loss = 3.675724393394951
4, epoch_train_loss=3.675724393394951
Epoch 5
Epoch 5 :: Batch 0/1
Batch Loss = 3.331158510145481
5, epoch_train_loss=3.331158510145481
Epoch 6
Epoch 6 :: Batch 0/1
Batch Loss = 2.8801972160643627
6, epoch_train_loss=2.8801972160643627
Epoch 7
Epoch 7 :: Batch 0/1
Batch Loss = 2.2928337961793295
7, epoch_train_loss=2.2928337961793295
Epoch 8
Epoch 8 :: Batch 0/1
Batch Loss = 1.651488739853929
8, epoch_train_loss=1.651488739853929
Epoch 9
Epoch 9 :: Batch 0/1
Batch Loss = 1.0348127251183505
9, epoch_train_loss=1.0348127251183505
Epoch 10
Epoch 10 :: Batch 0/1
Batch Loss = 0.7465447943417074
10, epoch_train_loss=0.7465447943417074
Epoch 11
Epoch 11 :: Batch 0/1
Batch Loss = 0.5359701901231063
11, epoch_train_loss=0.5359701901231063
Epoch 12
Epoch 12 :: Batch 0/1
Batch Loss = 2.310116940867905
12, epoch_train_loss=2.310116940867905
Epoch 13
Epoch 13 :: Batch 0/1
Batch Loss = 0.8591543063196277
13, epoch_train_loss=0.8591543063196277
Epoch 14
Epoch 14 :: Batch 0/1
Batch Loss = 2.0282523522812013
14, epoch_train_loss=2.0282523522812013
Epoch 15
Epoch 15 :: Batch 0/1
Batch Loss = 0.9494735916580025
15, epoch_train_loss=0.9494735916580025
Epoch 16
Epoch 16 :: Batch 0/1
Batch Loss = 0.2686231792008657
16, epoch_train_loss=0.2686231792008657
Epoch 17
Epoch 17 :: Batch 0/1
Batch Loss = 0.7365044330197834
17, epoch_train_loss=0.7365044330197834
Epoch 18
Epoch 18 :: Batch 0/1
Batch Loss = 1.0015294505381875
18, epoch_train_loss=1.0015294505381875
Epoch 19
Epoch 19 :: Batch 0/1
Batch Loss = 0.682112568077853
19, epoch_train_loss=0.682112568077853
Epoch 20
Epoch 20 :: Batch 0/1
Batch Loss = 0.3340192629180512
20, epoch_train_loss=0.3340192629180512
Epoch 21
Epoch 21 :: Batch 0/1
Batch Loss = 0.2237435919696783
21, epoch_train_loss=0.2237435919696783
Epoch 22
Epoch 22 :: Batch 0/1
Batch Loss = 0.2519311282991045
22, epoch_train_loss=0.2519311282991045
Epoch 23
Epoch 23 :: Batch 0/1
Batch Loss = 0.2950587841810439
23, epoch_train_loss=0.2950587841810439
Epoch 24
Epoch 24 :: Batch 0/1
Batch Loss = 0.3091723881479731
24, epoch_train_loss=0.3091723881479731
Epoch 25
Epoch 25 :: Batch 0/1
Batch Loss = 0.2922317809680241
25, epoch_train_loss=0.2922317809680241
Epoch 26
Epoch 26 :: Batch 0/1
Batch Loss = 0.259664098028499
26, epoch_train_loss=0.259664098028499
Epoch 27
Epoch 27 :: Batch 0/1
Batch Loss = 0.22592406880877394
27, epoch_train_loss=0.22592406880877394
Epoch 28
Epoch 28 :: Batch 0/1
Batch Loss = 0.19499452989001428
28, epoch_train_loss=0.19499452989001428
Epoch 29
Epoch 29 :: Batch 0/1
Batch Loss = 0.1655885391896601
29, epoch_train_loss=0.1655885391896601
Epoch 30
Epoch 30 :: Batch 0/1
Batch Loss = 0.13437716091132407
30, epoch_train_loss=0.13437716091132407
Epoch 31
Epoch 31 :: Batch 0/1
Batch Loss = 0.10389102903630715
31, epoch_train_loss=0.10389102903630715
Epoch 32
Epoch 32 :: Batch 0/1
Batch Loss = 0.07951684925464585
32, epoch_train_loss=0.07951684925464585
Epoch 33
Epoch 33 :: Batch 0/1
Batch Loss = 0.06440666908046404
33, epoch_train_loss=0.06440666908046404
Epoch 34
Epoch 34 :: Batch 0/1
Batch Loss = 0.05611804325735374
34, epoch_train_loss=0.05611804325735374
Epoch 35
Epoch 35 :: Batch 0/1
Batch Loss = 0.049076792336434596
35, epoch_train_loss=0.049076792336434596
Epoch 36
Epoch 36 :: Batch 0/1
Batch Loss = 0.04334747344604983
36, epoch_train_loss=0.04334747344604983
Epoch 37
Epoch 37 :: Batch 0/1
Batch Loss = 0.04121777024075691
37, epoch_train_loss=0.04121777024075691
Epoch 38
Epoch 38 :: Batch 0/1
Batch Loss = 0.04276055151634837
38, epoch_train_loss=0.04276055151634837
Epoch 39
Epoch 39 :: Batch 0/1
Batch Loss = 0.04455190728888158
39, epoch_train_loss=0.04455190728888158
Epoch 40
Epoch 40 :: Batch 0/1
Batch Loss = 0.04689802870486714
40, epoch_train_loss=0.04689802870486714
Epoch 41
Epoch 41 :: Batch 0/1
Batch Loss = 0.04977566717804471
41, epoch_train_loss=0.04977566717804471
Epoch 42
Epoch 42 :: Batch 0/1
Batch Loss = 0.0502792173125385
42, epoch_train_loss=0.0502792173125385
Epoch 43
Epoch 43 :: Batch 0/1
Batch Loss = 0.04765506569841325
43, epoch_train_loss=0.04765506569841325
Epoch 44
Epoch 44 :: Batch 0/1
Batch Loss = 0.045848529750028566
44, epoch_train_loss=0.045848529750028566
Epoch 45
Epoch 45 :: Batch 0/1
Batch Loss = 0.044451476979637736
45, epoch_train_loss=0.044451476979637736
Epoch 46
Epoch 46 :: Batch 0/1
Batch Loss = 0.040730691982278515
46, epoch_train_loss=0.040730691982278515
Epoch 47
Epoch 47 :: Batch 0/1
Batch Loss = 0.03561260169575463
47, epoch_train_loss=0.03561260169575463
Epoch 48
Epoch 48 :: Batch 0/1
Batch Loss = 0.03151391348733185
48, epoch_train_loss=0.03151391348733185
Epoch 49
Epoch 49 :: Batch 0/1
Batch Loss = 0.027824646556459606
49, epoch_train_loss=0.027824646556459606
Epoch 50
Epoch 50 :: Batch 0/1
Batch Loss = 0.02424455594132879
50, epoch_train_loss=0.02424455594132879
Epoch 51
Epoch 51 :: Batch 0/1
Batch Loss = 0.021494640026010047
51, epoch_train_loss=0.021494640026010047
Epoch 52
Epoch 52 :: Batch 0/1
Batch Loss = 0.019317411453162462
52, epoch_train_loss=0.019317411453162462
Epoch 53
Epoch 53 :: Batch 0/1
Batch Loss = 0.016943235581507225
53, epoch_train_loss=0.016943235581507225
Epoch 54
Epoch 54 :: Batch 0/1
Batch Loss = 0.015262748222477944
54, epoch_train_loss=0.015262748222477944
Epoch 55
Epoch 55 :: Batch 0/1
Batch Loss = 0.014775821878192603
55, epoch_train_loss=0.014775821878192603
Epoch 56
Epoch 56 :: Batch 0/1
Batch Loss = 0.014598898394070144
56, epoch_train_loss=0.014598898394070144
Epoch 57
Epoch 57 :: Batch 0/1
Batch Loss = 0.014044760168592916
57, epoch_train_loss=0.014044760168592916
Epoch 58
Epoch 58 :: Batch 0/1
Batch Loss = 0.013740033269389591
58, epoch_train_loss=0.013740033269389591
Epoch 59
Epoch 59 :: Batch 0/1
Batch Loss = 0.013990727872110427
59, epoch_train_loss=0.013990727872110427
Epoch 60
Epoch 60 :: Batch 0/1
Batch Loss = 0.014275814880163365
60, epoch_train_loss=0.014275814880163365
Epoch 61
Epoch 61 :: Batch 0/1
Batch Loss = 0.014364562987060441
61, epoch_train_loss=0.014364562987060441
Epoch 62
Epoch 62 :: Batch 0/1
Batch Loss = 0.014371811515629634
62, epoch_train_loss=0.014371811515629634
Epoch 63
Epoch 63 :: Batch 0/1
Batch Loss = 0.014281110070441003
63, epoch_train_loss=0.014281110070441003
Epoch 64
Epoch 64 :: Batch 0/1
Batch Loss = 0.01397077357230004
64, epoch_train_loss=0.01397077357230004
Epoch 65
Epoch 65 :: Batch 0/1
Batch Loss = 0.013632879236406038
65, epoch_train_loss=0.013632879236406038
Epoch 66
Epoch 66 :: Batch 0/1
Batch Loss = 0.013286886928282934
66, epoch_train_loss=0.013286886928282934
Epoch 67
Epoch 67 :: Batch 0/1
Batch Loss = 0.012679715403746582
67, epoch_train_loss=0.012679715403746582
Epoch 68
Epoch 68 :: Batch 0/1
Batch Loss = 0.011913742587590561
68, epoch_train_loss=0.011913742587590561
Epoch 69
Epoch 69 :: Batch 0/1
Batch Loss = 0.01131243631891669
69, epoch_train_loss=0.01131243631891669
Epoch 70
Epoch 70 :: Batch 0/1
Batch Loss = 0.010872272345979336
70, epoch_train_loss=0.010872272345979336
Epoch 71
Epoch 71 :: Batch 0/1
Batch Loss = 0.010359763880580544
71, epoch_train_loss=0.010359763880580544
Epoch 72
Epoch 72 :: Batch 0/1
Batch Loss = 0.009861371461910243
72, epoch_train_loss=0.009861371461910243
Epoch 73
Epoch 73 :: Batch 0/1
Batch Loss = 0.009561359580977353
73, epoch_train_loss=0.009561359580977353
Epoch 74
Epoch 74 :: Batch 0/1
Batch Loss = 0.00936087755194238
74, epoch_train_loss=0.00936087755194238
Epoch 75
Epoch 75 :: Batch 0/1
Batch Loss = 0.00918555970497664
75, epoch_train_loss=0.00918555970497664
Epoch 76
Epoch 76 :: Batch 0/1
Batch Loss = 0.009071803975590206
76, epoch_train_loss=0.009071803975590206
Epoch 77
Epoch 77 :: Batch 0/1
Batch Loss = 0.008964375907292441
77, epoch_train_loss=0.008964375907292441
Epoch 78
Epoch 78 :: Batch 0/1
Batch Loss = 0.008843278208640547
78, epoch_train_loss=0.008843278208640547
Epoch 79
Epoch 79 :: Batch 0/1
Batch Loss = 0.008776957636473078
79, epoch_train_loss=0.008776957636473078
Epoch 80
Epoch 80 :: Batch 0/1
Batch Loss = 0.008714385108710565
80, epoch_train_loss=0.008714385108710565
Epoch 81
Epoch 81 :: Batch 0/1
Batch Loss = 0.008564942427302743
81, epoch_train_loss=0.008564942427302743
Epoch 82
Epoch 82 :: Batch 0/1
Batch Loss = 0.008399479925229061
82, epoch_train_loss=0.008399479925229061
Epoch 83
Epoch 83 :: Batch 0/1
Batch Loss = 0.008271326482271324
83, epoch_train_loss=0.008271326482271324
Epoch 84
Epoch 84 :: Batch 0/1
Batch Loss = 0.008107129903830673
84, epoch_train_loss=0.008107129903830673
Epoch 85
Epoch 85 :: Batch 0/1
Batch Loss = 0.007905246789826454
85, epoch_train_loss=0.007905246789826454
Epoch 86
Epoch 86 :: Batch 0/1
Batch Loss = 0.007713131507757237
86, epoch_train_loss=0.007713131507757237
Epoch 87
Epoch 87 :: Batch 0/1
Batch Loss = 0.007518356896496365
87, epoch_train_loss=0.007518356896496365
Epoch 88
Epoch 88 :: Batch 0/1
Batch Loss = 0.00733286090816102
88, epoch_train_loss=0.00733286090816102
Epoch 89
Epoch 89 :: Batch 0/1
Batch Loss = 0.007175274378296427
89, epoch_train_loss=0.007175274378296427
Epoch 90
Epoch 90 :: Batch 0/1
Batch Loss = 0.007017156918502014
90, epoch_train_loss=0.007017156918502014
Epoch 91
Epoch 91 :: Batch 0/1
Batch Loss = 0.006870652093975891
91, epoch_train_loss=0.006870652093975891
Epoch 92
Epoch 92 :: Batch 0/1
Batch Loss = 0.006766839501953259
92, epoch_train_loss=0.006766839501953259
Epoch 93
Epoch 93 :: Batch 0/1
Batch Loss = 0.006674137109693149
93, epoch_train_loss=0.006674137109693149
Epoch 94
Epoch 94 :: Batch 0/1
Batch Loss = 0.006572038715285856
94, epoch_train_loss=0.006572038715285856
Epoch 95
Epoch 95 :: Batch 0/1
Batch Loss = 0.006482802864259815
95, epoch_train_loss=0.006482802864259815
Epoch 96
Epoch 96 :: Batch 0/1
Batch Loss = 0.006401116480910606
96, epoch_train_loss=0.006401116480910606
Epoch 97
Epoch 97 :: Batch 0/1
Batch Loss = 0.0063115207394339986
97, epoch_train_loss=0.0063115207394339986
Epoch 98
Epoch 98 :: Batch 0/1
Batch Loss = 0.006220007379625134
98, epoch_train_loss=0.006220007379625134
Epoch 99
Epoch 99 :: Batch 0/1
Batch Loss = 0.006123288950249584
99, epoch_train_loss=0.006123288950249584
Epoch 100
Epoch 100 :: Batch 0/1
Batch Loss = 0.006020386543612835
100, epoch_train_loss=0.006020386543612835
Epoch 101
Epoch 101 :: Batch 0/1
Batch Loss = 0.005923270464818507
101, epoch_train_loss=0.005923270464818507
Epoch 102
Epoch 102 :: Batch 0/1
Batch Loss = 0.00582599448165437
102, epoch_train_loss=0.00582599448165437
Epoch 103
Epoch 103 :: Batch 0/1
Batch Loss = 0.005722552296933557
103, epoch_train_loss=0.005722552296933557
Epoch 104
Epoch 104 :: Batch 0/1
Batch Loss = 0.00562610665265841
104, epoch_train_loss=0.00562610665265841
Epoch 105
Epoch 105 :: Batch 0/1
Batch Loss = 0.005537798988035902
105, epoch_train_loss=0.005537798988035902
Epoch 106
Epoch 106 :: Batch 0/1
Batch Loss = 0.005450470447987149
106, epoch_train_loss=0.005450470447987149
Epoch 107
Epoch 107 :: Batch 0/1
Batch Loss = 0.005368525339959493
107, epoch_train_loss=0.005368525339959493
Epoch 108
Epoch 108 :: Batch 0/1
Batch Loss = 0.005292652981791746
108, epoch_train_loss=0.005292652981791746
Epoch 109
Epoch 109 :: Batch 0/1
Batch Loss = 0.005220786500730331
109, epoch_train_loss=0.005220786500730331
Epoch 110
Epoch 110 :: Batch 0/1
Batch Loss = 0.005154724320866941
110, epoch_train_loss=0.005154724320866941
Epoch 111
Epoch 111 :: Batch 0/1
Batch Loss = 0.005089675771539307
111, epoch_train_loss=0.005089675771539307
Epoch 112
Epoch 112 :: Batch 0/1
Batch Loss = 0.005023849838493707
112, epoch_train_loss=0.005023849838493707
Epoch 113
Epoch 113 :: Batch 0/1
Batch Loss = 0.004961614403083206
113, epoch_train_loss=0.004961614403083206
Epoch 114
Epoch 114 :: Batch 0/1
Batch Loss = 0.0048997633433819935
114, epoch_train_loss=0.0048997633433819935
Epoch 115
Epoch 115 :: Batch 0/1
Batch Loss = 0.00483624046331843
115, epoch_train_loss=0.00483624046331843
Epoch 116
Epoch 116 :: Batch 0/1
Batch Loss = 0.004774706548202516
116, epoch_train_loss=0.004774706548202516
Epoch 117
Epoch 117 :: Batch 0/1
Batch Loss = 0.0047147930777043335
117, epoch_train_loss=0.0047147930777043335
Epoch 118
Epoch 118 :: Batch 0/1
Batch Loss = 0.004656405094958222
118, epoch_train_loss=0.004656405094958222
Epoch 119
Epoch 119 :: Batch 0/1
Batch Loss = 0.004600565655054397
119, epoch_train_loss=0.004600565655054397
Epoch 120
Epoch 120 :: Batch 0/1
Batch Loss = 0.004546320980213042
120, epoch_train_loss=0.004546320980213042
Epoch 121
Epoch 121 :: Batch 0/1
Batch Loss = 0.004495122911281517
121, epoch_train_loss=0.004495122911281517
Epoch 122
Epoch 122 :: Batch 0/1
Batch Loss = 0.004446934868303264
122, epoch_train_loss=0.004446934868303264
Epoch 123
Epoch 123 :: Batch 0/1
Batch Loss = 0.0043995579129386155
123, epoch_train_loss=0.0043995579129386155
Epoch 124
Epoch 124 :: Batch 0/1
Batch Loss = 0.004354179016666128
124, epoch_train_loss=0.004354179016666128
Epoch 125
Epoch 125 :: Batch 0/1
Batch Loss = 0.004310997189672507
125, epoch_train_loss=0.004310997189672507
Epoch 126
Epoch 126 :: Batch 0/1
Batch Loss = 0.004268561884481188
126, epoch_train_loss=0.004268561884481188
Epoch 127
Epoch 127 :: Batch 0/1
Batch Loss = 0.004227164611844878
127, epoch_train_loss=0.004227164611844878
Epoch 128
Epoch 128 :: Batch 0/1
Batch Loss = 0.004186520023342047
128, epoch_train_loss=0.004186520023342047
Epoch 129
Epoch 129 :: Batch 0/1
Batch Loss = 0.004146586667772544
129, epoch_train_loss=0.004146586667772544
Epoch 130
Epoch 130 :: Batch 0/1
Batch Loss = 0.004107676762717358
130, epoch_train_loss=0.004107676762717358
Epoch 131
Epoch 131 :: Batch 0/1
Batch Loss = 0.00406913828334035
131, epoch_train_loss=0.00406913828334035
Epoch 132
Epoch 132 :: Batch 0/1
Batch Loss = 0.0040316469529024625
132, epoch_train_loss=0.0040316469529024625
Epoch 133
Epoch 133 :: Batch 0/1
Batch Loss = 0.00399566705774069
133, epoch_train_loss=0.00399566705774069
Epoch 134
Epoch 134 :: Batch 0/1
Batch Loss = 0.003960569565787518
134, epoch_train_loss=0.003960569565787518
Epoch 135
Epoch 135 :: Batch 0/1
Batch Loss = 0.0039267410074969806
135, epoch_train_loss=0.0039267410074969806
Epoch 136
Epoch 136 :: Batch 0/1
Batch Loss = 0.0038942597143355752
136, epoch_train_loss=0.0038942597143355752
Epoch 137
Epoch 137 :: Batch 0/1
Batch Loss = 0.0038629201529696245
137, epoch_train_loss=0.0038629201529696245
Epoch 138
Epoch 138 :: Batch 0/1
Batch Loss = 0.003832849249837209
138, epoch_train_loss=0.003832849249837209
Epoch 139
Epoch 139 :: Batch 0/1
Batch Loss = 0.0038040793033702574
139, epoch_train_loss=0.0038040793033702574
Epoch 140
Epoch 140 :: Batch 0/1
Batch Loss = 0.003777695936567504
140, epoch_train_loss=0.003777695936567504
Epoch 141
Epoch 141 :: Batch 0/1
Batch Loss = 0.0037561701113379745
141, epoch_train_loss=0.0037561701113379745
Epoch 142
Epoch 142 :: Batch 0/1
Batch Loss = 0.0037456407218430456
142, epoch_train_loss=0.0037456407218430456
Epoch 143
Epoch 143 :: Batch 0/1
Batch Loss = 0.0037659198289833923
143, epoch_train_loss=0.0037659198289833923
Epoch 144
Epoch 144 :: Batch 0/1
Batch Loss = 0.003866128626946981
144, epoch_train_loss=0.003866128626946981
Epoch 145
Epoch 145 :: Batch 0/1
Batch Loss = 0.004206041719729668
145, epoch_train_loss=0.004206041719729668
Epoch 146
Epoch 146 :: Batch 0/1
Batch Loss = 0.005055542970264785
146, epoch_train_loss=0.005055542970264785
Epoch 147
Epoch 147 :: Batch 0/1
Batch Loss = 0.007173002312356127
147, epoch_train_loss=0.007173002312356127
Epoch 148
Epoch 148 :: Batch 0/1
Batch Loss = 0.00887627175728729
148, epoch_train_loss=0.00887627175728729
Epoch 149
Epoch 149 :: Batch 0/1
Batch Loss = 0.00962217248440791
149, epoch_train_loss=0.00962217248440791
Epoch 150
Epoch 150 :: Batch 0/1
Batch Loss = 0.005054565167760891
150, epoch_train_loss=0.005054565167760891
Epoch 151
Epoch 151 :: Batch 0/1
Batch Loss = 0.0037166003089605945
151, epoch_train_loss=0.0037166003089605945
Epoch 152
Epoch 152 :: Batch 0/1
Batch Loss = 0.006312664174401687
152, epoch_train_loss=0.006312664174401687
Epoch 153
Epoch 153 :: Batch 0/1
Batch Loss = 0.005919343183084972
153, epoch_train_loss=0.005919343183084972
Epoch 154
Epoch 154 :: Batch 0/1
Batch Loss = 0.0037355903834039236
154, epoch_train_loss=0.0037355903834039236
Epoch 155
Epoch 155 :: Batch 0/1
Batch Loss = 0.004045932900348945
155, epoch_train_loss=0.004045932900348945
Epoch 156
Epoch 156 :: Batch 0/1
Batch Loss = 0.005200088290257076
156, epoch_train_loss=0.005200088290257076
Epoch 157
Epoch 157 :: Batch 0/1
Batch Loss = 0.004167300844703697
157, epoch_train_loss=0.004167300844703697
Epoch 158
Epoch 158 :: Batch 0/1
Batch Loss = 0.0034800697741439283
158, epoch_train_loss=0.0034800697741439283
Epoch 159
Epoch 159 :: Batch 0/1
Batch Loss = 0.00458588780184108
159, epoch_train_loss=0.00458588780184108
Epoch 160
Epoch 160 :: Batch 0/1
Batch Loss = 0.004231016476095254
160, epoch_train_loss=0.004231016476095254
Epoch 161
Epoch 161 :: Batch 0/1
Batch Loss = 0.0033543428079798065
161, epoch_train_loss=0.0033543428079798065
Epoch 162
Epoch 162 :: Batch 0/1
Batch Loss = 0.004214588820680752
162, epoch_train_loss=0.004214588820680752
Epoch 163
Epoch 163 :: Batch 0/1
Batch Loss = 0.004102466271122931
163, epoch_train_loss=0.004102466271122931
Epoch 164
Epoch 164 :: Batch 0/1
Batch Loss = 0.0033032846848768914
164, epoch_train_loss=0.0033032846848768914
Epoch 165
Epoch 165 :: Batch 0/1
Batch Loss = 0.0039865052050977266
165, epoch_train_loss=0.0039865052050977266
Epoch 166
Epoch 166 :: Batch 0/1
Batch Loss = 0.003912206469001362
166, epoch_train_loss=0.003912206469001362
Epoch 167
Epoch 167 :: Batch 0/1
Batch Loss = 0.0032616316206143126
167, epoch_train_loss=0.0032616316206143126
Epoch 168
Epoch 168 :: Batch 0/1
Batch Loss = 0.00382833380289201
168, epoch_train_loss=0.00382833380289201
Epoch 169
Epoch 169 :: Batch 0/1
Batch Loss = 0.0037279735615482488
169, epoch_train_loss=0.0037279735615482488
Epoch 170
Epoch 170 :: Batch 0/1
Batch Loss = 0.003223730602781152
170, epoch_train_loss=0.003223730602781152
Epoch 171
Epoch 171 :: Batch 0/1
Batch Loss = 0.0036946122872275147
171, epoch_train_loss=0.0036946122872275147
Epoch 172
Epoch 172 :: Batch 0/1
Batch Loss = 0.003566476310847463
172, epoch_train_loss=0.003566476310847463
Epoch 173
Epoch 173 :: Batch 0/1
Batch Loss = 0.0031894487502386482
173, epoch_train_loss=0.0031894487502386482
Epoch 174
Epoch 174 :: Batch 0/1
Batch Loss = 0.003574370898366277
174, epoch_train_loss=0.003574370898366277
Epoch 175
Epoch 175 :: Batch 0/1
Batch Loss = 0.003434431729185232
175, epoch_train_loss=0.003434431729185232
Epoch 176
Epoch 176 :: Batch 0/1
Batch Loss = 0.003155596995058139
176, epoch_train_loss=0.003155596995058139
Epoch 177
Epoch 177 :: Batch 0/1
Batch Loss = 0.0034626643157126785
177, epoch_train_loss=0.0034626643157126785
Epoch 178
Epoch 178 :: Batch 0/1
Batch Loss = 0.003331713633272643
178, epoch_train_loss=0.003331713633272643
Epoch 179
Epoch 179 :: Batch 0/1
Batch Loss = 0.0031211833373098008
179, epoch_train_loss=0.0031211833373098008
Epoch 180
Epoch 180 :: Batch 0/1
Batch Loss = 0.0033606774986805316
180, epoch_train_loss=0.0033606774986805316
Epoch 181
Epoch 181 :: Batch 0/1
Batch Loss = 0.0032517298291996305
181, epoch_train_loss=0.0032517298291996305
Epoch 182
Epoch 182 :: Batch 0/1
Batch Loss = 0.0030867478238555874
182, epoch_train_loss=0.0030867478238555874
Epoch 183
Epoch 183 :: Batch 0/1
Batch Loss = 0.003270657305629986
183, epoch_train_loss=0.003270657305629986
Epoch 184
Epoch 184 :: Batch 0/1
Batch Loss = 0.003190331054305483
184, epoch_train_loss=0.003190331054305483
Epoch 185
Epoch 185 :: Batch 0/1
Batch Loss = 0.003052964120236713
185, epoch_train_loss=0.003052964120236713
Epoch 186
Epoch 186 :: Batch 0/1
Batch Loss = 0.0031911580557018324
186, epoch_train_loss=0.0031911580557018324
Epoch 187
Epoch 187 :: Batch 0/1
Batch Loss = 0.0031417446808497365
187, epoch_train_loss=0.0031417446808497365
Epoch 188
Epoch 188 :: Batch 0/1
Batch Loss = 0.0030219069713360064
188, epoch_train_loss=0.0030219069713360064
Epoch 189
Epoch 189 :: Batch 0/1
Batch Loss = 0.0031213751125579382
189, epoch_train_loss=0.0031213751125579382
Epoch 190
Epoch 190 :: Batch 0/1
Batch Loss = 0.003101181581103402
190, epoch_train_loss=0.003101181581103402
Epoch 191
Epoch 191 :: Batch 0/1
Batch Loss = 0.00299501944509147
191, epoch_train_loss=0.00299501944509147
Epoch 192
Epoch 192 :: Batch 0/1
Batch Loss = 0.0030595585719264988
192, epoch_train_loss=0.0030595585719264988
Epoch 193
Epoch 193 :: Batch 0/1
Batch Loss = 0.003064624526760412
193, epoch_train_loss=0.003064624526760412
Epoch 194
Epoch 194 :: Batch 0/1
Batch Loss = 0.0029733306896211676
194, epoch_train_loss=0.0029733306896211676
Epoch 195
Epoch 195 :: Batch 0/1
Batch Loss = 0.0030051284162075378
195, epoch_train_loss=0.0030051284162075378
Epoch 196
Epoch 196 :: Batch 0/1
Batch Loss = 0.003028305765247362
196, epoch_train_loss=0.003028305765247362
Epoch 197
Epoch 197 :: Batch 0/1
Batch Loss = 0.0029565381216553896
197, epoch_train_loss=0.0029565381216553896
Epoch 198
Epoch 198 :: Batch 0/1
Batch Loss = 0.0029589745771236966
198, epoch_train_loss=0.0029589745771236966
Epoch 199
Epoch 199 :: Batch 0/1
Batch Loss = 0.0029902023246775053
199, epoch_train_loss=0.0029902023246775053
Epoch 200
Epoch 200 :: Batch 0/1
Batch Loss = 0.0029429013285952347
200, epoch_train_loss=0.0029429013285952347
Epoch 201
Epoch 201 :: Batch 0/1
Batch Loss = 0.0029226308957219483
201, epoch_train_loss=0.0029226308957219483
Epoch 202
Epoch 202 :: Batch 0/1
Batch Loss = 0.002950049682705898
202, epoch_train_loss=0.002950049682705898
Epoch 203
Epoch 203 :: Batch 0/1
Batch Loss = 0.0029289441371807405
203, epoch_train_loss=0.0029289441371807405
Epoch 204
Epoch 204 :: Batch 0/1
Batch Loss = 0.0028975060735225593
204, epoch_train_loss=0.0028975060735225593
Epoch 205
Epoch 205 :: Batch 0/1
Batch Loss = 0.0029107720221946175
205, epoch_train_loss=0.0029107720221946175
Epoch 206
Epoch 206 :: Batch 0/1
Batch Loss = 0.0029107614866681402
206, epoch_train_loss=0.0029107614866681402
Epoch 207
Epoch 207 :: Batch 0/1
Batch Loss = 0.0028821508875816154
207, epoch_train_loss=0.0028821508875816154
Epoch 208
Epoch 208 :: Batch 0/1
Batch Loss = 0.0028773716762997233
208, epoch_train_loss=0.0028773716762997233
Epoch 209
Epoch 209 :: Batch 0/1
Batch Loss = 0.002886282402732331
209, epoch_train_loss=0.002886282402732331
Epoch 210
Epoch 210 :: Batch 0/1
Batch Loss = 0.002871052431686547
210, epoch_train_loss=0.002871052431686547
Epoch 211
Epoch 211 :: Batch 0/1
Batch Loss = 0.00285416801871046
211, epoch_train_loss=0.00285416801871046
Epoch 212
Epoch 212 :: Batch 0/1
Batch Loss = 0.002858146589830645
212, epoch_train_loss=0.002858146589830645
Epoch 213
Epoch 213 :: Batch 0/1
Batch Loss = 0.0028567627162242506
213, epoch_train_loss=0.0028567627162242506
Epoch 214
Epoch 214 :: Batch 0/1
Batch Loss = 0.0028403308472794184
214, epoch_train_loss=0.0028403308472794184
Epoch 215
Epoch 215 :: Batch 0/1
Batch Loss = 0.002833170959404111
215, epoch_train_loss=0.002833170959404111
Epoch 216
Epoch 216 :: Batch 0/1
Batch Loss = 0.002835879821341297
216, epoch_train_loss=0.002835879821341297
Epoch 217
Epoch 217 :: Batch 0/1
Batch Loss = 0.002828977586462532
217, epoch_train_loss=0.002828977586462532
Epoch 218
Epoch 218 :: Batch 0/1
Batch Loss = 0.002816630900498993
218, epoch_train_loss=0.002816630900498993
Epoch 219
Epoch 219 :: Batch 0/1
Batch Loss = 0.002813202507302782
219, epoch_train_loss=0.002813202507302782
Epoch 220
Epoch 220 :: Batch 0/1
Batch Loss = 0.0028130235988132993
220, epoch_train_loss=0.0028130235988132993
Epoch 221
Epoch 221 :: Batch 0/1
Batch Loss = 0.002805534810518622
221, epoch_train_loss=0.002805534810518622
Epoch 222
Epoch 222 :: Batch 0/1
Batch Loss = 0.002796370153332564
222, epoch_train_loss=0.002796370153332564
Epoch 223
Epoch 223 :: Batch 0/1
Batch Loss = 0.0027935204670611782
223, epoch_train_loss=0.0027935204670611782
Epoch 224
Epoch 224 :: Batch 0/1
Batch Loss = 0.0027918416340758914
224, epoch_train_loss=0.0027918416340758914
Epoch 225
Epoch 225 :: Batch 0/1
Batch Loss = 0.0027851986088478485
225, epoch_train_loss=0.0027851986088478485
Epoch 226
Epoch 226 :: Batch 0/1
Batch Loss = 0.0027778698017280816
226, epoch_train_loss=0.0027778698017280816
Epoch 227
Epoch 227 :: Batch 0/1
Batch Loss = 0.0027746328900409034
227, epoch_train_loss=0.0027746328900409034
Epoch 228
Epoch 228 :: Batch 0/1
Batch Loss = 0.002772339105124067
228, epoch_train_loss=0.002772339105124067
Epoch 229
Epoch 229 :: Batch 0/1
Batch Loss = 0.0027670261572550784
229, epoch_train_loss=0.0027670261572550784
Epoch 230
Epoch 230 :: Batch 0/1
Batch Loss = 0.00276074080450935
230, epoch_train_loss=0.00276074080450935
Epoch 231
Epoch 231 :: Batch 0/1
Batch Loss = 0.00275690679335904
231, epoch_train_loss=0.00275690679335904
Epoch 232
Epoch 232 :: Batch 0/1
Batch Loss = 0.0027543798110366406
232, epoch_train_loss=0.0027543798110366406
Epoch 233
Epoch 233 :: Batch 0/1
Batch Loss = 0.002750262953742235
233, epoch_train_loss=0.002750262953742235
Epoch 234
Epoch 234 :: Batch 0/1
Batch Loss = 0.002744887723323504
234, epoch_train_loss=0.002744887723323504
Epoch 235
Epoch 235 :: Batch 0/1
Batch Loss = 0.00274055636818976
235, epoch_train_loss=0.00274055636818976
Epoch 236
Epoch 236 :: Batch 0/1
Batch Loss = 0.0027376430646574683
236, epoch_train_loss=0.0027376430646574683
Epoch 237
Epoch 237 :: Batch 0/1
Batch Loss = 0.0027344469029240495
237, epoch_train_loss=0.0027344469029240495
Epoch 238
Epoch 238 :: Batch 0/1
Batch Loss = 0.0027301122074831767
238, epoch_train_loss=0.0027301122074831767
Epoch 239
Epoch 239 :: Batch 0/1
Batch Loss = 0.0027256843871628226
239, epoch_train_loss=0.0027256843871628226
Epoch 240
Epoch 240 :: Batch 0/1
Batch Loss = 0.0027222026003537233
240, epoch_train_loss=0.0027222026003537233
Epoch 241
Epoch 241 :: Batch 0/1
Batch Loss = 0.002719290268052577
241, epoch_train_loss=0.002719290268052577
Epoch 242
Epoch 242 :: Batch 0/1
Batch Loss = 0.002715974739906322
242, epoch_train_loss=0.002715974739906322
Epoch 243
Epoch 243 :: Batch 0/1
Batch Loss = 0.0027120784016663175
243, epoch_train_loss=0.0027120784016663175
Epoch 244
Epoch 244 :: Batch 0/1
Batch Loss = 0.0027082758064047993
244, epoch_train_loss=0.0027082758064047993
Epoch 245
Epoch 245 :: Batch 0/1
Batch Loss = 0.0027050468914124105
245, epoch_train_loss=0.0027050468914124105
Epoch 246
Epoch 246 :: Batch 0/1
Batch Loss = 0.002702145764527466
246, epoch_train_loss=0.002702145764527466
Epoch 247
Epoch 247 :: Batch 0/1
Batch Loss = 0.002699058083144931
247, epoch_train_loss=0.002699058083144931
Epoch 248
Epoch 248 :: Batch 0/1
Batch Loss = 0.0026956457801011243
248, epoch_train_loss=0.0026956457801011243
Epoch 249
Epoch 249 :: Batch 0/1
Batch Loss = 0.0026922188985299424
249, epoch_train_loss=0.0026922188985299424
Epoch 250
Epoch 250 :: Batch 0/1
Batch Loss = 0.002689085138338243
250, epoch_train_loss=0.002689085138338243
Epoch 251
Epoch 251 :: Batch 0/1
Batch Loss = 0.0026862303052643363
251, epoch_train_loss=0.0026862303052643363
Epoch 252
Epoch 252 :: Batch 0/1
Batch Loss = 0.0026834087938726898
252, epoch_train_loss=0.0026834087938726898
Epoch 253
Epoch 253 :: Batch 0/1
Batch Loss = 0.002680438688659814
253, epoch_train_loss=0.002680438688659814
Epoch 254
Epoch 254 :: Batch 0/1
Batch Loss = 0.0026773623822206184
254, epoch_train_loss=0.0026773623822206184
Epoch 255
Epoch 255 :: Batch 0/1
Batch Loss = 0.0026743457423988536
255, epoch_train_loss=0.0026743457423988536
Epoch 256
Epoch 256 :: Batch 0/1
Batch Loss = 0.0026715030828285457
256, epoch_train_loss=0.0026715030828285457
Epoch 257
Epoch 257 :: Batch 0/1
Batch Loss = 0.0026688101023015897
257, epoch_train_loss=0.0026688101023015897
Epoch 258
Epoch 258 :: Batch 0/1
Batch Loss = 0.002666158939132579
258, epoch_train_loss=0.002666158939132579
Epoch 259
Epoch 259 :: Batch 0/1
Batch Loss = 0.0026634654837012924
259, epoch_train_loss=0.0026634654837012924
Epoch 260
Epoch 260 :: Batch 0/1
Batch Loss = 0.0026607235557353113
260, epoch_train_loss=0.0026607235557353113
Epoch 261
Epoch 261 :: Batch 0/1
Batch Loss = 0.0026579905764220237
261, epoch_train_loss=0.0026579905764220237
Epoch 262
Epoch 262 :: Batch 0/1
Batch Loss = 0.002655327480419372
262, epoch_train_loss=0.002655327480419372
Epoch 263
Epoch 263 :: Batch 0/1
Batch Loss = 0.0026527591044860944
263, epoch_train_loss=0.0026527591044860944
Epoch 264
Epoch 264 :: Batch 0/1
Batch Loss = 0.002650267768852842
264, epoch_train_loss=0.002650267768852842
Epoch 265
Epoch 265 :: Batch 0/1
Batch Loss = 0.002647815544454635
265, epoch_train_loss=0.002647815544454635
Epoch 266
Epoch 266 :: Batch 0/1
Batch Loss = 0.0026453716738950965
266, epoch_train_loss=0.0026453716738950965
Epoch 267
Epoch 267 :: Batch 0/1
Batch Loss = 0.0026429249513234797
267, epoch_train_loss=0.0026429249513234797
Epoch 268
Epoch 268 :: Batch 0/1
Batch Loss = 0.0026404844333208595
268, epoch_train_loss=0.0026404844333208595
Epoch 269
Epoch 269 :: Batch 0/1
Batch Loss = 0.0026380667749187573
269, epoch_train_loss=0.0026380667749187573
Epoch 270
Epoch 270 :: Batch 0/1
Batch Loss = 0.0026356877523952345
270, epoch_train_loss=0.0026356877523952345
Epoch 271
Epoch 271 :: Batch 0/1
Batch Loss = 0.0026333549422994586
271, epoch_train_loss=0.0026333549422994586
Epoch 272
Epoch 272 :: Batch 0/1
Batch Loss = 0.0026310681579454795
272, epoch_train_loss=0.0026310681579454795
Epoch 273
Epoch 273 :: Batch 0/1
Batch Loss = 0.002628821991439991
273, epoch_train_loss=0.002628821991439991
Epoch 274
Epoch 274 :: Batch 0/1
Batch Loss = 0.0026266093343248714
274, epoch_train_loss=0.0026266093343248714
Epoch 275
Epoch 275 :: Batch 0/1
Batch Loss = 0.0026244243793186596
275, epoch_train_loss=0.0026244243793186596
Epoch 276
Epoch 276 :: Batch 0/1
Batch Loss = 0.0026222631409935117
276, epoch_train_loss=0.0026222631409935117
Epoch 277
Epoch 277 :: Batch 0/1
Batch Loss = 0.0026201244423112173
277, epoch_train_loss=0.0026201244423112173
Epoch 278
Epoch 278 :: Batch 0/1
Batch Loss = 0.0026180080091278038
278, epoch_train_loss=0.0026180080091278038
Epoch 279
Epoch 279 :: Batch 0/1
Batch Loss = 0.0026159153834537253
279, epoch_train_loss=0.0026159153834537253
Epoch 280
Epoch 280 :: Batch 0/1
Batch Loss = 0.0026138476207527577
280, epoch_train_loss=0.0026138476207527577
Epoch 281
Epoch 281 :: Batch 0/1
Batch Loss = 0.0026118070709709703
281, epoch_train_loss=0.0026118070709709703
Epoch 282
Epoch 282 :: Batch 0/1
Batch Loss = 0.002609795398472633
282, epoch_train_loss=0.002609795398472633
Epoch 283
Epoch 283 :: Batch 0/1
Batch Loss = 0.002607816373501825
283, epoch_train_loss=0.002607816373501825
Epoch 284
Epoch 284 :: Batch 0/1
Batch Loss = 0.0026058741362303454
284, epoch_train_loss=0.0026058741362303454
Epoch 285
Epoch 285 :: Batch 0/1
Batch Loss = 0.0026039778512153607
285, epoch_train_loss=0.0026039778512153607
Epoch 286
Epoch 286 :: Batch 0/1
Batch Loss = 0.0026021406559336544
286, epoch_train_loss=0.0026021406559336544
Epoch 287
Epoch 287 :: Batch 0/1
Batch Loss = 0.0026003900228970226
287, epoch_train_loss=0.0026003900228970226
Epoch 288
Epoch 288 :: Batch 0/1
Batch Loss = 0.002598769946833383
288, epoch_train_loss=0.002598769946833383
Epoch 289
Epoch 289 :: Batch 0/1
Batch Loss = 0.002597372351162934
289, epoch_train_loss=0.002597372351162934
Epoch 290
Epoch 290 :: Batch 0/1
Batch Loss = 0.0025963532759942097
290, epoch_train_loss=0.0025963532759942097
Epoch 291
Epoch 291 :: Batch 0/1
Batch Loss = 0.0025960515386201123
291, epoch_train_loss=0.0025960515386201123
Epoch 292
Epoch 292 :: Batch 0/1
Batch Loss = 0.0025970600018439417
292, epoch_train_loss=0.0025970600018439417
Epoch 293
Epoch 293 :: Batch 0/1
Batch Loss = 0.00260075370345792
293, epoch_train_loss=0.00260075370345792
Epoch 294
Epoch 294 :: Batch 0/1
Batch Loss = 0.002609538252759885
294, epoch_train_loss=0.002609538252759885
Epoch 295
Epoch 295 :: Batch 0/1
Batch Loss = 0.002629556349302064
295, epoch_train_loss=0.002629556349302064
Epoch 296
Epoch 296 :: Batch 0/1
Batch Loss = 0.002671009341782374
296, epoch_train_loss=0.002671009341782374
Epoch 297
Epoch 297 :: Batch 0/1
Batch Loss = 0.002764003507039318
297, epoch_train_loss=0.002764003507039318
Epoch 298
Epoch 298 :: Batch 0/1
Batch Loss = 0.0029505529319348587
298, epoch_train_loss=0.0029505529319348587
Epoch 299
Epoch 299 :: Batch 0/1
Batch Loss = 0.0033883410971713717
299, epoch_train_loss=0.0033883410971713717
Epoch 300
Epoch 300 :: Batch 0/1
Batch Loss = 0.00419213805801978
300, epoch_train_loss=0.00419213805801978
Epoch 301
Epoch 301 :: Batch 0/1
Batch Loss = 0.006130469836578118
301, epoch_train_loss=0.006130469836578118
Epoch 302
Epoch 302 :: Batch 0/1
Batch Loss = 0.008479062663643084
302, epoch_train_loss=0.008479062663643084
Epoch 303
Epoch 303 :: Batch 0/1
Batch Loss = 0.013261801380459752
303, epoch_train_loss=0.013261801380459752
Epoch 304
Epoch 304 :: Batch 0/1
Batch Loss = 0.011483951821998835
304, epoch_train_loss=0.011483951821998835
Epoch 305
Epoch 305 :: Batch 0/1
Batch Loss = 0.008277968779056396
305, epoch_train_loss=0.008277968779056396
Epoch 306
Epoch 306 :: Batch 0/1
Batch Loss = 0.0029353747892851135
306, epoch_train_loss=0.0029353747892851135
Epoch 307
Epoch 307 :: Batch 0/1
Batch Loss = 0.004371265370802119
307, epoch_train_loss=0.004371265370802119
Epoch 308
Epoch 308 :: Batch 0/1
Batch Loss = 0.008190846142591461
308, epoch_train_loss=0.008190846142591461
Epoch 309
Epoch 309 :: Batch 0/1
Batch Loss = 0.004840094821825097
309, epoch_train_loss=0.004840094821825097
Epoch 310
Epoch 310 :: Batch 0/1
Batch Loss = 0.002664388568386046
310, epoch_train_loss=0.002664388568386046
Epoch 311
Epoch 311 :: Batch 0/1
Batch Loss = 0.0049984495955926805
311, epoch_train_loss=0.0049984495955926805
Epoch 312
Epoch 312 :: Batch 0/1
Batch Loss = 0.004272333496141179
312, epoch_train_loss=0.004272333496141179
Epoch 313
Epoch 313 :: Batch 0/1
Batch Loss = 0.0026139412402043805
313, epoch_train_loss=0.0026139412402043805
Epoch 314
Epoch 314 :: Batch 0/1
Batch Loss = 0.0039037230755729663
314, epoch_train_loss=0.0039037230755729663
Epoch 315
Epoch 315 :: Batch 0/1
Batch Loss = 0.0036100799646332556
315, epoch_train_loss=0.0036100799646332556
Epoch 316
Epoch 316 :: Batch 0/1
Batch Loss = 0.0026241985491665034
316, epoch_train_loss=0.0026241985491665034
Epoch 317
Epoch 317 :: Batch 0/1
Batch Loss = 0.003485705665868762
317, epoch_train_loss=0.003485705665868762
Epoch 318
Epoch 318 :: Batch 0/1
Batch Loss = 0.0030794014502903244
318, epoch_train_loss=0.0030794014502903244
Epoch 319
Epoch 319 :: Batch 0/1
Batch Loss = 0.0026921005396842784
319, epoch_train_loss=0.0026921005396842784
Epoch 320
Epoch 320 :: Batch 0/1
Batch Loss = 0.003275017544425201
320, epoch_train_loss=0.003275017544425201
Epoch 321
Epoch 321 :: Batch 0/1
Batch Loss = 0.0027526785541254132
321, epoch_train_loss=0.0027526785541254132
Epoch 322
Epoch 322 :: Batch 0/1
Batch Loss = 0.0028031143466835805
322, epoch_train_loss=0.0028031143466835805
Epoch 323
Epoch 323 :: Batch 0/1
Batch Loss = 0.0030625437376372235
323, epoch_train_loss=0.0030625437376372235
Epoch 324
Epoch 324 :: Batch 0/1
Batch Loss = 0.0026146979828863184
324, epoch_train_loss=0.0026146979828863184
Epoch 325
Epoch 325 :: Batch 0/1
Batch Loss = 0.0028878235212816974
325, epoch_train_loss=0.0028878235212816974
Epoch 326
Epoch 326 :: Batch 0/1
Batch Loss = 0.002850810769010934
326, epoch_train_loss=0.002850810769010934
Epoch 327
Epoch 327 :: Batch 0/1
Batch Loss = 0.002602838065601405
327, epoch_train_loss=0.002602838065601405
Epoch 328
Epoch 328 :: Batch 0/1
Batch Loss = 0.0028839443995766147
328, epoch_train_loss=0.0028839443995766147
Epoch 329
Epoch 329 :: Batch 0/1
Batch Loss = 0.0026899982757287912
329, epoch_train_loss=0.0026899982757287912
Epoch 330
Epoch 330 :: Batch 0/1
Batch Loss = 0.0026392675186681478
330, epoch_train_loss=0.0026392675186681478
Epoch 331
Epoch 331 :: Batch 0/1
Batch Loss = 0.0028048747984425826
331, epoch_train_loss=0.0028048747984425826
Epoch 332
Epoch 332 :: Batch 0/1
Batch Loss = 0.002597555486392704
332, epoch_train_loss=0.002597555486392704
Epoch 333
Epoch 333 :: Batch 0/1
Batch Loss = 0.002667882471369032
333, epoch_train_loss=0.002667882471369032
Epoch 334
Epoch 334 :: Batch 0/1
Batch Loss = 0.002706305829504037
334, epoch_train_loss=0.002706305829504037
Epoch 335
Epoch 335 :: Batch 0/1
Batch Loss = 0.0025636630705495717
335, epoch_train_loss=0.0025636630705495717
Epoch 336
Epoch 336 :: Batch 0/1
Batch Loss = 0.0026664385575397953
336, epoch_train_loss=0.0026664385575397953
Epoch 337
Epoch 337 :: Batch 0/1
Batch Loss = 0.002627868721261105
337, epoch_train_loss=0.002627868721261105
Epoch 338
Epoch 338 :: Batch 0/1
Batch Loss = 0.002560724671675947
338, epoch_train_loss=0.002560724671675947
Epoch 339
Epoch 339 :: Batch 0/1
Batch Loss = 0.0026439490506633394
339, epoch_train_loss=0.0026439490506633394
Epoch 340
Epoch 340 :: Batch 0/1
Batch Loss = 0.002577902263821248
340, epoch_train_loss=0.002577902263821248
Epoch 341
Epoch 341 :: Batch 0/1
Batch Loss = 0.002565708188148924
341, epoch_train_loss=0.002565708188148924
Epoch 342
Epoch 342 :: Batch 0/1
Batch Loss = 0.002614932536582832
342, epoch_train_loss=0.002614932536582832
Epoch 343
Epoch 343 :: Batch 0/1
Batch Loss = 0.0025521963593441157
343, epoch_train_loss=0.0025521963593441157
Epoch 344
Epoch 344 :: Batch 0/1
Batch Loss = 0.002567163122436121
344, epoch_train_loss=0.002567163122436121
Epoch 345
Epoch 345 :: Batch 0/1
Batch Loss = 0.002587629189016236
345, epoch_train_loss=0.002587629189016236
Epoch 346
Epoch 346 :: Batch 0/1
Batch Loss = 0.0025397746073766085
346, epoch_train_loss=0.0025397746073766085
Epoch 347
Epoch 347 :: Batch 0/1
Batch Loss = 0.002563868865292172
347, epoch_train_loss=0.002563868865292172
Epoch 348
Epoch 348 :: Batch 0/1
Batch Loss = 0.0025663639430992336
348, epoch_train_loss=0.0025663639430992336
Epoch 349
Epoch 349 :: Batch 0/1
Batch Loss = 0.002533487802130205
349, epoch_train_loss=0.002533487802130205
Epoch 350
Epoch 350 :: Batch 0/1
Batch Loss = 0.0025569163453857888
350, epoch_train_loss=0.0025569163453857888
Epoch 351
Epoch 351 :: Batch 0/1
Batch Loss = 0.0025510303461346823
351, epoch_train_loss=0.0025510303461346823
Epoch 352
Epoch 352 :: Batch 0/1
Batch Loss = 0.002529301286273609
352, epoch_train_loss=0.002529301286273609
Epoch 353
Epoch 353 :: Batch 0/1
Batch Loss = 0.002548471420069849
353, epoch_train_loss=0.002548471420069849
Epoch 354
Epoch 354 :: Batch 0/1
Batch Loss = 0.0025399415439424753
354, epoch_train_loss=0.0025399415439424753
Epoch 355
Epoch 355 :: Batch 0/1
Batch Loss = 0.002525552964610503
355, epoch_train_loss=0.002525552964610503
Epoch 356
Epoch 356 :: Batch 0/1
Batch Loss = 0.002540043761118838
356, epoch_train_loss=0.002540043761118838
Epoch 357
Epoch 357 :: Batch 0/1
Batch Loss = 0.002531793047869276
357, epoch_train_loss=0.002531793047869276
Epoch 358
Epoch 358 :: Batch 0/1
Batch Loss = 0.0025216822084604376
358, epoch_train_loss=0.0025216822084604376
Epoch 359
Epoch 359 :: Batch 0/1
Batch Loss = 0.0025321643394848924
359, epoch_train_loss=0.0025321643394848924
Epoch 360
Epoch 360 :: Batch 0/1
Batch Loss = 0.002525366852970749
360, epoch_train_loss=0.002525366852970749
Epoch 361
Epoch 361 :: Batch 0/1
Batch Loss = 0.0025177472442095925
361, epoch_train_loss=0.0025177472442095925
Epoch 362
Epoch 362 :: Batch 0/1
Batch Loss = 0.002525143570838097
362, epoch_train_loss=0.002525143570838097
Epoch 363
Epoch 363 :: Batch 0/1
Batch Loss = 0.0025200108527400986
363, epoch_train_loss=0.0025200108527400986
Epoch 364
Epoch 364 :: Batch 0/1
Batch Loss = 0.0025137886998518873
364, epoch_train_loss=0.0025137886998518873
Epoch 365
Epoch 365 :: Batch 0/1
Batch Loss = 0.0025188871807077778
365, epoch_train_loss=0.0025188871807077778
Epoch 366
Epoch 366 :: Batch 0/1
Batch Loss = 0.0025153421751311633
366, epoch_train_loss=0.0025153421751311633
Epoch 367
Epoch 367 :: Batch 0/1
Batch Loss = 0.002509987716530749
367, epoch_train_loss=0.002509987716530749
Epoch 368
Epoch 368 :: Batch 0/1
Batch Loss = 0.002513315621179955
368, epoch_train_loss=0.002513315621179955
Epoch 369
Epoch 369 :: Batch 0/1
Batch Loss = 0.002511086324599453
369, epoch_train_loss=0.002511086324599453
Epoch 370
Epoch 370 :: Batch 0/1
Batch Loss = 0.0025063746180452465
370, epoch_train_loss=0.0025063746180452465
Epoch 371
Epoch 371 :: Batch 0/1
Batch Loss = 0.0025082919974272503
371, epoch_train_loss=0.0025082919974272503
Epoch 372
Epoch 372 :: Batch 0/1
Batch Loss = 0.002507110655114162
372, epoch_train_loss=0.002507110655114162
Epoch 373
Epoch 373 :: Batch 0/1
Batch Loss = 0.0025029991710628736
373, epoch_train_loss=0.0025029991710628736
Epoch 374
Epoch 374 :: Batch 0/1
Batch Loss = 0.0025037414255420455
374, epoch_train_loss=0.0025037414255420455
Epoch 375
Epoch 375 :: Batch 0/1
Batch Loss = 0.002503288935666927
375, epoch_train_loss=0.002503288935666927
Epoch 376
Epoch 376 :: Batch 0/1
Batch Loss = 0.002499831408438172
376, epoch_train_loss=0.002499831408438172
Epoch 377
Epoch 377 :: Batch 0/1
Batch Loss = 0.00249961700564871
377, epoch_train_loss=0.00249961700564871
Epoch 378
Epoch 378 :: Batch 0/1
Batch Loss = 0.002499581429574548
378, epoch_train_loss=0.002499581429574548
Epoch 379
Epoch 379 :: Batch 0/1
Batch Loss = 0.0024968334100352444
379, epoch_train_loss=0.0024968334100352444
Epoch 380
Epoch 380 :: Batch 0/1
Batch Loss = 0.0024958842858399566
380, epoch_train_loss=0.0024958842858399566
Epoch 381
Epoch 381 :: Batch 0/1
Batch Loss = 0.0024959615462234928
381, epoch_train_loss=0.0024959615462234928
Epoch 382
Epoch 382 :: Batch 0/1
Batch Loss = 0.0024939371133599884
382, epoch_train_loss=0.0024939371133599884
Epoch 383
Epoch 383 :: Batch 0/1
Batch Loss = 0.0024925186934208307
383, epoch_train_loss=0.0024925186934208307
Epoch 384
Epoch 384 :: Batch 0/1
Batch Loss = 0.00249245564380159
384, epoch_train_loss=0.00249245564380159
Epoch 385
Epoch 385 :: Batch 0/1
Batch Loss = 0.0024910807186283475
385, epoch_train_loss=0.0024910807186283475
Epoch 386
Epoch 386 :: Batch 0/1
Batch Loss = 0.00248946542701146
386, epoch_train_loss=0.00248946542701146
Epoch 387
Epoch 387 :: Batch 0/1
Batch Loss = 0.00248909495336685
387, epoch_train_loss=0.00248909495336685
Epoch 388
Epoch 388 :: Batch 0/1
Batch Loss = 0.0024882094599190667
388, epoch_train_loss=0.0024882094599190667
Epoch 389
Epoch 389 :: Batch 0/1
Batch Loss = 0.002486652790491209
389, epoch_train_loss=0.002486652790491209
Epoch 390
Epoch 390 :: Batch 0/1
Batch Loss = 0.0024859276066220557
390, epoch_train_loss=0.0024859276066220557
Epoch 391
Epoch 391 :: Batch 0/1
Batch Loss = 0.0024853088007600437
391, epoch_train_loss=0.0024853088007600437
Epoch 392
Epoch 392 :: Batch 0/1
Batch Loss = 0.0024839862869667387
392, epoch_train_loss=0.0024839862869667387
Epoch 393
Epoch 393 :: Batch 0/1
Batch Loss = 0.002482979584858429
393, epoch_train_loss=0.002482979584858429
Epoch 394
Epoch 394 :: Batch 0/1
Batch Loss = 0.0024824033392596046
394, epoch_train_loss=0.0024824033392596046
Epoch 395
Epoch 395 :: Batch 0/1
Batch Loss = 0.0024813751728728554
395, epoch_train_loss=0.0024813751728728554
Epoch 396
Epoch 396 :: Batch 0/1
Batch Loss = 0.0024802452525763976
396, epoch_train_loss=0.0024802452525763976
Epoch 397
Epoch 397 :: Batch 0/1
Batch Loss = 0.0024795533248375707
397, epoch_train_loss=0.0024795533248375707
Epoch 398
Epoch 398 :: Batch 0/1
Batch Loss = 0.002478757674421751
398, epoch_train_loss=0.002478757674421751
Epoch 399
Epoch 399 :: Batch 0/1
Batch Loss = 0.0024776749736607133
399, epoch_train_loss=0.0024776749736607133
Epoch 400
Epoch 400 :: Batch 0/1
Batch Loss = 0.002476824867683045
400, epoch_train_loss=0.002476824867683045
Epoch 401
Epoch 401 :: Batch 0/1
Batch Loss = 0.0024761248510009166
401, epoch_train_loss=0.0024761248510009166
Epoch 402
Epoch 402 :: Batch 0/1
Batch Loss = 0.0024751919960035416
402, epoch_train_loss=0.0024751919960035416
Epoch 403
Epoch 403 :: Batch 0/1
Batch Loss = 0.002474250952834302
403, epoch_train_loss=0.002474250952834302
Epoch 404
Epoch 404 :: Batch 0/1
Batch Loss = 0.0024735183306698483
404, epoch_train_loss=0.0024735183306698483
Epoch 405
Epoch 405 :: Batch 0/1
Batch Loss = 0.0024727308892225288
405, epoch_train_loss=0.0024727308892225288
Epoch 406
Epoch 406 :: Batch 0/1
Batch Loss = 0.0024718122042357
406, epoch_train_loss=0.0024718122042357
Epoch 407
Epoch 407 :: Batch 0/1
Batch Loss = 0.0024709994173857993
407, epoch_train_loss=0.0024709994173857993
Epoch 408
Epoch 408 :: Batch 0/1
Batch Loss = 0.0024702731647380763
408, epoch_train_loss=0.0024702731647380763
Epoch 409
Epoch 409 :: Batch 0/1
Batch Loss = 0.002469449202730205
409, epoch_train_loss=0.002469449202730205
Epoch 410
Epoch 410 :: Batch 0/1
Batch Loss = 0.0024686000367747404
410, epoch_train_loss=0.0024686000367747404
Epoch 411
Epoch 411 :: Batch 0/1
Batch Loss = 0.0024678524184947558
411, epoch_train_loss=0.0024678524184947558
Epoch 412
Epoch 412 :: Batch 0/1
Batch Loss = 0.00246711129995003
412, epoch_train_loss=0.00246711129995003
Epoch 413
Epoch 413 :: Batch 0/1
Batch Loss = 0.00246630010717539
413, epoch_train_loss=0.00246630010717539
Epoch 414
Epoch 414 :: Batch 0/1
Batch Loss = 0.0024655154432999767
414, epoch_train_loss=0.0024655154432999767
Epoch 415
Epoch 415 :: Batch 0/1
Batch Loss = 0.0024647946899042407
415, epoch_train_loss=0.0024647946899042407
Epoch 416
Epoch 416 :: Batch 0/1
Batch Loss = 0.00246405128338866
416, epoch_train_loss=0.00246405128338866
Epoch 417
Epoch 417 :: Batch 0/1
Batch Loss = 0.0024632740711813413
417, epoch_train_loss=0.0024632740711813413
Epoch 418
Epoch 418 :: Batch 0/1
Batch Loss = 0.002462534459903937
418, epoch_train_loss=0.002462534459903937
Epoch 419
Epoch 419 :: Batch 0/1
Batch Loss = 0.0024618278795880823
419, epoch_train_loss=0.0024618278795880823
Epoch 420
Epoch 420 :: Batch 0/1
Batch Loss = 0.0024610974763062264
420, epoch_train_loss=0.0024610974763062264
Epoch 421
Epoch 421 :: Batch 0/1
Batch Loss = 0.002460355877517894
421, epoch_train_loss=0.002460355877517894
Epoch 422
Epoch 422 :: Batch 0/1
Batch Loss = 0.002459647048390512
422, epoch_train_loss=0.002459647048390512
Epoch 423
Epoch 423 :: Batch 0/1
Batch Loss = 0.0024589543836909533
423, epoch_train_loss=0.0024589543836909533
Epoch 424
Epoch 424 :: Batch 0/1
Batch Loss = 0.002458244536516126
424, epoch_train_loss=0.002458244536516126
Epoch 425
Epoch 425 :: Batch 0/1
Batch Loss = 0.002457534528933536
425, epoch_train_loss=0.002457534528933536
Epoch 426
Epoch 426 :: Batch 0/1
Batch Loss = 0.0024568488842883
426, epoch_train_loss=0.0024568488842883
Epoch 427
Epoch 427 :: Batch 0/1
Batch Loss = 0.0024561721777225385
427, epoch_train_loss=0.0024561721777225385
Epoch 428
Epoch 428 :: Batch 0/1
Batch Loss = 0.002455485797854111
428, epoch_train_loss=0.002455485797854111
Epoch 429
Epoch 429 :: Batch 0/1
Batch Loss = 0.002454802417490205
429, epoch_train_loss=0.002454802417490205
Epoch 430
Epoch 430 :: Batch 0/1
Batch Loss = 0.0024541366332203438
430, epoch_train_loss=0.0024541366332203438
Epoch 431
Epoch 431 :: Batch 0/1
Batch Loss = 0.002453477951006527
431, epoch_train_loss=0.002453477951006527
Epoch 432
Epoch 432 :: Batch 0/1
Batch Loss = 0.0024528144366506095
432, epoch_train_loss=0.0024528144366506095
Epoch 433
Epoch 433 :: Batch 0/1
Batch Loss = 0.0024521543715735243
433, epoch_train_loss=0.0024521543715735243
Epoch 434
Epoch 434 :: Batch 0/1
Batch Loss = 0.002451507285737422
434, epoch_train_loss=0.002451507285737422
Epoch 435
Epoch 435 :: Batch 0/1
Batch Loss = 0.0024508669125336787
435, epoch_train_loss=0.0024508669125336787
Epoch 436
Epoch 436 :: Batch 0/1
Batch Loss = 0.0024502251677020206
436, epoch_train_loss=0.0024502251677020206
Epoch 437
Epoch 437 :: Batch 0/1
Batch Loss = 0.0024495862723459038
437, epoch_train_loss=0.0024495862723459038
Epoch 438
Epoch 438 :: Batch 0/1
Batch Loss = 0.00244895712369504
438, epoch_train_loss=0.00244895712369504
Epoch 439
Epoch 439 :: Batch 0/1
Batch Loss = 0.002448334775036047
439, epoch_train_loss=0.002448334775036047
Epoch 440
Epoch 440 :: Batch 0/1
Batch Loss = 0.0024477132951420846
440, epoch_train_loss=0.0024477132951420846
Epoch 441
Epoch 441 :: Batch 0/1
Batch Loss = 0.002447094312285426
441, epoch_train_loss=0.002447094312285426
Epoch 442
Epoch 442 :: Batch 0/1
Batch Loss = 0.0024464824717370377
442, epoch_train_loss=0.0024464824717370377
Epoch 443
Epoch 443 :: Batch 0/1
Batch Loss = 0.0024458772692383165
443, epoch_train_loss=0.0024458772692383165
Epoch 444
Epoch 444 :: Batch 0/1
Batch Loss = 0.0024452748460024177
444, epoch_train_loss=0.0024452748460024177
Epoch 445
Epoch 445 :: Batch 0/1
Batch Loss = 0.0024446746232426597
445, epoch_train_loss=0.0024446746232426597
Epoch 446
Epoch 446 :: Batch 0/1
Batch Loss = 0.002444079622205596
446, epoch_train_loss=0.002444079622205596
Epoch 447
Epoch 447 :: Batch 0/1
Batch Loss = 0.0024434908013758936
447, epoch_train_loss=0.0024434908013758936
Epoch 448
Epoch 448 :: Batch 0/1
Batch Loss = 0.0024429058925085575
448, epoch_train_loss=0.0024429058925085575
Epoch 449
Epoch 449 :: Batch 0/1
Batch Loss = 0.0024423235409013317
449, epoch_train_loss=0.0024423235409013317
Epoch 450
Epoch 450 :: Batch 0/1
Batch Loss = 0.002441745016537825
450, epoch_train_loss=0.002441745016537825
Epoch 451
Epoch 451 :: Batch 0/1
Batch Loss = 0.0024411717611198115
451, epoch_train_loss=0.0024411717611198115
Epoch 452
Epoch 452 :: Batch 0/1
Batch Loss = 0.0024406030568003274
452, epoch_train_loss=0.0024406030568003274
Epoch 453
Epoch 453 :: Batch 0/1
Batch Loss = 0.0024400374594511745
453, epoch_train_loss=0.0024400374594511745
Epoch 454
Epoch 454 :: Batch 0/1
Batch Loss = 0.002439475049914281
454, epoch_train_loss=0.002439475049914281
Epoch 455
Epoch 455 :: Batch 0/1
Batch Loss = 0.0024389168231384595
455, epoch_train_loss=0.0024389168231384595
Epoch 456
Epoch 456 :: Batch 0/1
Batch Loss = 0.002438363094071589
456, epoch_train_loss=0.002438363094071589
Epoch 457
Epoch 457 :: Batch 0/1
Batch Loss = 0.0024378130894337196
457, epoch_train_loss=0.0024378130894337196
Epoch 458
Epoch 458 :: Batch 0/1
Batch Loss = 0.002437266142017741
458, epoch_train_loss=0.002437266142017741
Epoch 459
Epoch 459 :: Batch 0/1
Batch Loss = 0.0024367226137586755
459, epoch_train_loss=0.0024367226137586755
Epoch 460
Epoch 460 :: Batch 0/1
Batch Loss = 0.0024361830354161372
460, epoch_train_loss=0.0024361830354161372
Epoch 461
Epoch 461 :: Batch 0/1
Batch Loss = 0.002435647329207884
461, epoch_train_loss=0.002435647329207884
Epoch 462
Epoch 462 :: Batch 0/1
Batch Loss = 0.002435114983428557
462, epoch_train_loss=0.002435114983428557
Epoch 463
Epoch 463 :: Batch 0/1
Batch Loss = 0.0024345857133342775
463, epoch_train_loss=0.0024345857133342775
Epoch 464
Epoch 464 :: Batch 0/1
Batch Loss = 0.0024340597808762353
464, epoch_train_loss=0.0024340597808762353
Epoch 465
Epoch 465 :: Batch 0/1
Batch Loss = 0.0024335374298976066
465, epoch_train_loss=0.0024335374298976066
Epoch 466
Epoch 466 :: Batch 0/1
Batch Loss = 0.002433018554069197
466, epoch_train_loss=0.002433018554069197
Epoch 467
Epoch 467 :: Batch 0/1
Batch Loss = 0.002432502843361574
467, epoch_train_loss=0.002432502843361574
Epoch 468
Epoch 468 :: Batch 0/1
Batch Loss = 0.002431990126745696
468, epoch_train_loss=0.002431990126745696
Epoch 469
Epoch 469 :: Batch 0/1
Batch Loss = 0.002431480548390157
469, epoch_train_loss=0.002431480548390157
Epoch 470
Epoch 470 :: Batch 0/1
Batch Loss = 0.0024309742207171098
470, epoch_train_loss=0.0024309742207171098
Epoch 471
Epoch 471 :: Batch 0/1
Batch Loss = 0.002430471077456006
471, epoch_train_loss=0.002430471077456006
Epoch 472
Epoch 472 :: Batch 0/1
Batch Loss = 0.002429970938315689
472, epoch_train_loss=0.002429970938315689
Epoch 473
Epoch 473 :: Batch 0/1
Batch Loss = 0.0024294736638770967
473, epoch_train_loss=0.0024294736638770967
Epoch 474
Epoch 474 :: Batch 0/1
Batch Loss = 0.0024289793002657037
474, epoch_train_loss=0.0024289793002657037
Epoch 475
Epoch 475 :: Batch 0/1
Batch Loss = 0.0024284879010385884
475, epoch_train_loss=0.0024284879010385884
Epoch 476
Epoch 476 :: Batch 0/1
Batch Loss = 0.0024279994439931553
476, epoch_train_loss=0.0024279994439931553
Epoch 477
Epoch 477 :: Batch 0/1
Batch Loss = 0.0024275138226338274
477, epoch_train_loss=0.0024275138226338274
Epoch 478
Epoch 478 :: Batch 0/1
Batch Loss = 0.0024270309215448675
478, epoch_train_loss=0.0024270309215448675
Epoch 479
Epoch 479 :: Batch 0/1
Batch Loss = 0.0024265507238002094
479, epoch_train_loss=0.0024265507238002094
Epoch 480
Epoch 480 :: Batch 0/1
Batch Loss = 0.0024260732350403417
480, epoch_train_loss=0.0024260732350403417
Epoch 481
Epoch 481 :: Batch 0/1
Batch Loss = 0.0024255984593866283
481, epoch_train_loss=0.0024255984593866283
Epoch 482
Epoch 482 :: Batch 0/1
Batch Loss = 0.0024251263412356323
482, epoch_train_loss=0.0024251263412356323
Epoch 483
Epoch 483 :: Batch 0/1
Batch Loss = 0.0024246567940249095
483, epoch_train_loss=0.0024246567940249095
Epoch 484
Epoch 484 :: Batch 0/1
Batch Loss = 0.002424189770771565
484, epoch_train_loss=0.002424189770771565
Epoch 485
Epoch 485 :: Batch 0/1
Batch Loss = 0.002423725239022711
485, epoch_train_loss=0.002423725239022711
Epoch 486
Epoch 486 :: Batch 0/1
Batch Loss = 0.002423263198663012
486, epoch_train_loss=0.002423263198663012
Epoch 487
Epoch 487 :: Batch 0/1
Batch Loss = 0.0024228036227528383
487, epoch_train_loss=0.0024228036227528383
Epoch 488
Epoch 488 :: Batch 0/1
Batch Loss = 0.0024223464619060487
488, epoch_train_loss=0.0024223464619060487
Epoch 489
Epoch 489 :: Batch 0/1
Batch Loss = 0.0024218916655136306
489, epoch_train_loss=0.0024218916655136306
Epoch 490
Epoch 490 :: Batch 0/1
Batch Loss = 0.002421439182352431
490, epoch_train_loss=0.002421439182352431
Epoch 491
Epoch 491 :: Batch 0/1
Batch Loss = 0.0024209889921443235
491, epoch_train_loss=0.0024209889921443235
Epoch 492
Epoch 492 :: Batch 0/1
Batch Loss = 0.0024205410683235994
492, epoch_train_loss=0.0024205410683235994
Epoch 493
Epoch 493 :: Batch 0/1
Batch Loss = 0.0024200953878289327
493, epoch_train_loss=0.0024200953878289327
Epoch 494
Epoch 494 :: Batch 0/1
Batch Loss = 0.002419651913614131
494, epoch_train_loss=0.002419651913614131
Epoch 495
Epoch 495 :: Batch 0/1
Batch Loss = 0.0024192106003756175
495, epoch_train_loss=0.0024192106003756175
Epoch 496
Epoch 496 :: Batch 0/1
Batch Loss = 0.0024187714138761223
496, epoch_train_loss=0.0024187714138761223
Epoch 497
Epoch 497 :: Batch 0/1
Batch Loss = 0.0024183343164949664
497, epoch_train_loss=0.0024183343164949664
Epoch 498
Epoch 498 :: Batch 0/1
Batch Loss = 0.0024178992866765946
498, epoch_train_loss=0.0024178992866765946
Epoch 499
Epoch 499 :: Batch 0/1
Batch Loss = 0.002417466294931064
499, epoch_train_loss=0.002417466294931064
Epoch 500
Epoch 500 :: Batch 0/1
Batch Loss = 0.002417035312968949
500, epoch_train_loss=0.002417035312968949
Epoch 501
Epoch 501 :: Batch 0/1
Batch Loss = 0.002416606308170728
501, epoch_train_loss=0.002416606308170728
Epoch 502
Epoch 502 :: Batch 0/1
Batch Loss = 0.0024161792441318034
502, epoch_train_loss=0.0024161792441318034
Epoch 503
Epoch 503 :: Batch 0/1
Batch Loss = 0.0024157540917435208
503, epoch_train_loss=0.0024157540917435208
Epoch 504
Epoch 504 :: Batch 0/1
Batch Loss = 0.0024153308177654468
504, epoch_train_loss=0.0024153308177654468
Epoch 505
Epoch 505 :: Batch 0/1
Batch Loss = 0.0024149093986387616
505, epoch_train_loss=0.0024149093986387616
Epoch 506
Epoch 506 :: Batch 0/1
Batch Loss = 0.0024144898049953357
506, epoch_train_loss=0.0024144898049953357
Epoch 507
Epoch 507 :: Batch 0/1
Batch Loss = 0.002414072010976981
507, epoch_train_loss=0.002414072010976981
Epoch 508
Epoch 508 :: Batch 0/1
Batch Loss = 0.0024136559878796724
508, epoch_train_loss=0.0024136559878796724
Epoch 509
Epoch 509 :: Batch 0/1
Batch Loss = 0.0024132417056870136
509, epoch_train_loss=0.0024132417056870136
Epoch 510
Epoch 510 :: Batch 0/1
Batch Loss = 0.0024128291375577504
510, epoch_train_loss=0.0024128291375577504
Epoch 511
Epoch 511 :: Batch 0/1
Batch Loss = 0.002412418253568494
511, epoch_train_loss=0.002412418253568494
Epoch 512
Epoch 512 :: Batch 0/1
Batch Loss = 0.002412009029761175
512, epoch_train_loss=0.002412009029761175
Epoch 513
Epoch 513 :: Batch 0/1
Batch Loss = 0.0024116014383023915
513, epoch_train_loss=0.0024116014383023915
Epoch 514
Epoch 514 :: Batch 0/1
Batch Loss = 0.0024111954559099327
514, epoch_train_loss=0.0024111954559099327
Epoch 515
Epoch 515 :: Batch 0/1
Batch Loss = 0.002410791056255843
515, epoch_train_loss=0.002410791056255843
Epoch 516
Epoch 516 :: Batch 0/1
Batch Loss = 0.0024103882146993792
516, epoch_train_loss=0.0024103882146993792
Epoch 517
Epoch 517 :: Batch 0/1
Batch Loss = 0.0024099869062577848
517, epoch_train_loss=0.0024099869062577848
Epoch 518
Epoch 518 :: Batch 0/1
Batch Loss = 0.0024095871052636202
518, epoch_train_loss=0.0024095871052636202
Epoch 519
Epoch 519 :: Batch 0/1
Batch Loss = 0.0024091887884186963
519, epoch_train_loss=0.0024091887884186963
Epoch 520
Epoch 520 :: Batch 0/1
Batch Loss = 0.0024087919302797065
520, epoch_train_loss=0.0024087919302797065
Epoch 521
Epoch 521 :: Batch 0/1
Batch Loss = 0.002408396508972879
521, epoch_train_loss=0.002408396508972879
Epoch 522
Epoch 522 :: Batch 0/1
Batch Loss = 0.002408002499936529
522, epoch_train_loss=0.002408002499936529
Epoch 523
Epoch 523 :: Batch 0/1
Batch Loss = 0.002407609881950817
523, epoch_train_loss=0.002407609881950817
Epoch 524
Epoch 524 :: Batch 0/1
Batch Loss = 0.0024072186314630806
524, epoch_train_loss=0.0024072186314630806
Epoch 525
Epoch 525 :: Batch 0/1
Batch Loss = 0.0024068287273442523
525, epoch_train_loss=0.0024068287273442523
Epoch 526
Epoch 526 :: Batch 0/1
Batch Loss = 0.002406440147256806
526, epoch_train_loss=0.002406440147256806
Epoch 527
Epoch 527 :: Batch 0/1
Batch Loss = 0.002406052869830982
527, epoch_train_loss=0.002406052869830982
Epoch 528
Epoch 528 :: Batch 0/1
Batch Loss = 0.0024056668739423073
528, epoch_train_loss=0.0024056668739423073
Epoch 529
Epoch 529 :: Batch 0/1
Batch Loss = 0.0024052821382561083
529, epoch_train_loss=0.0024052821382561083
Epoch 530
Epoch 530 :: Batch 0/1
Batch Loss = 0.002404898642734296
530, epoch_train_loss=0.002404898642734296
Epoch 531
Epoch 531 :: Batch 0/1
Batch Loss = 0.002404516366349743
531, epoch_train_loss=0.002404516366349743
Epoch 532
Epoch 532 :: Batch 0/1
Batch Loss = 0.0024041352899077236
532, epoch_train_loss=0.0024041352899077236
Epoch 533
Epoch 533 :: Batch 0/1
Batch Loss = 0.0024037553928696023
533, epoch_train_loss=0.0024037553928696023
Epoch 534
Epoch 534 :: Batch 0/1
Batch Loss = 0.00240337665686079
534, epoch_train_loss=0.00240337665686079
Epoch 535
Epoch 535 :: Batch 0/1
Batch Loss = 0.0024029990617601485
535, epoch_train_loss=0.0024029990617601485
Epoch 536
Epoch 536 :: Batch 0/1
Batch Loss = 0.0024026225898656456
536, epoch_train_loss=0.0024026225898656456
Epoch 537
Epoch 537 :: Batch 0/1
Batch Loss = 0.002402247221678453
537, epoch_train_loss=0.002402247221678453
Epoch 538
Epoch 538 :: Batch 0/1
Batch Loss = 0.0024018729399777994
538, epoch_train_loss=0.0024018729399777994
Epoch 539
Epoch 539 :: Batch 0/1
Batch Loss = 0.002401499725920015
539, epoch_train_loss=0.002401499725920015
Epoch 540
Epoch 540 :: Batch 0/1
Batch Loss = 0.002401127562786065
540, epoch_train_loss=0.002401127562786065
Epoch 541
Epoch 541 :: Batch 0/1
Batch Loss = 0.0024007564322750443
541, epoch_train_loss=0.0024007564322750443
Epoch 542
Epoch 542 :: Batch 0/1
Batch Loss = 0.002400386318247421
542, epoch_train_loss=0.002400386318247421
Epoch 543
Epoch 543 :: Batch 0/1
Batch Loss = 0.002400017202923862
543, epoch_train_loss=0.002400017202923862
Epoch 544
Epoch 544 :: Batch 0/1
Batch Loss = 0.0023996490707978903
544, epoch_train_loss=0.0023996490707978903
Epoch 545
Epoch 545 :: Batch 0/1
Batch Loss = 0.0023992819044786886
545, epoch_train_loss=0.0023992819044786886
Epoch 546
Epoch 546 :: Batch 0/1
Batch Loss = 0.0023989156891665504
546, epoch_train_loss=0.0023989156891665504
Epoch 547
Epoch 547 :: Batch 0/1
Batch Loss = 0.002398550407901863
547, epoch_train_loss=0.002398550407901863
Epoch 548
Epoch 548 :: Batch 0/1
Batch Loss = 0.002398186046697004
548, epoch_train_loss=0.002398186046697004
Epoch 549
Epoch 549 :: Batch 0/1
Batch Loss = 0.0023978225887898645
549, epoch_train_loss=0.0023978225887898645
Epoch 550
Epoch 550 :: Batch 0/1
Batch Loss = 0.0023974600214202117
550, epoch_train_loss=0.0023974600214202117
Epoch 551
Epoch 551 :: Batch 0/1
Batch Loss = 0.0023970983280626897
551, epoch_train_loss=0.0023970983280626897
Epoch 552
Epoch 552 :: Batch 0/1
Batch Loss = 0.002396737497784791
552, epoch_train_loss=0.002396737497784791
Epoch 553
Epoch 553 :: Batch 0/1
Batch Loss = 0.002396377514418806
553, epoch_train_loss=0.002396377514418806
Epoch 554
Epoch 554 :: Batch 0/1
Batch Loss = 0.002396018370448117
554, epoch_train_loss=0.002396018370448117
Epoch 555
Epoch 555 :: Batch 0/1
Batch Loss = 0.002395660051403748
555, epoch_train_loss=0.002395660051403748
Epoch 556
Epoch 556 :: Batch 0/1
Batch Loss = 0.0023953025574847044
556, epoch_train_loss=0.0023953025574847044
Epoch 557
Epoch 557 :: Batch 0/1
Batch Loss = 0.0023949458809331887
557, epoch_train_loss=0.0023949458809331887
Epoch 558
Epoch 558 :: Batch 0/1
Batch Loss = 0.0023945900427321818
558, epoch_train_loss=0.0023945900427321818
Epoch 559
Epoch 559 :: Batch 0/1
Batch Loss = 0.0023942350610874174
559, epoch_train_loss=0.0023942350610874174
Epoch 560
Epoch 560 :: Batch 0/1
Batch Loss = 0.002393881020875267
560, epoch_train_loss=0.002393881020875267
Epoch 561
Epoch 561 :: Batch 0/1
Batch Loss = 0.002393528038565907
561, epoch_train_loss=0.002393528038565907
Epoch 562
Epoch 562 :: Batch 0/1
Batch Loss = 0.0023931764153167392
562, epoch_train_loss=0.0023931764153167392
Epoch 563
Epoch 563 :: Batch 0/1
Batch Loss = 0.002392826643582968
563, epoch_train_loss=0.002392826643582968
Epoch 564
Epoch 564 :: Batch 0/1
Batch Loss = 0.0023924798136241163
564, epoch_train_loss=0.0023924798136241163
Epoch 565
Epoch 565 :: Batch 0/1
Batch Loss = 0.0023921378919135013
565, epoch_train_loss=0.0023921378919135013
Epoch 566
Epoch 566 :: Batch 0/1
Batch Loss = 0.0023918050291765547
566, epoch_train_loss=0.0023918050291765547
Epoch 567
Epoch 567 :: Batch 0/1
Batch Loss = 0.002391489166507561
567, epoch_train_loss=0.002391489166507561
Epoch 568
Epoch 568 :: Batch 0/1
Batch Loss = 0.0023912069705638375
568, epoch_train_loss=0.0023912069705638375
Epoch 569
Epoch 569 :: Batch 0/1
Batch Loss = 0.002390991505425971
569, epoch_train_loss=0.002390991505425971
Epoch 570
Epoch 570 :: Batch 0/1
Batch Loss = 0.002390913208423768
570, epoch_train_loss=0.002390913208423768
Epoch 571
Epoch 571 :: Batch 0/1
Batch Loss = 0.0023911147478220683
571, epoch_train_loss=0.0023911147478220683
Epoch 572
Epoch 572 :: Batch 0/1
Batch Loss = 0.0023919090031286373
572, epoch_train_loss=0.0023919090031286373
Epoch 573
Epoch 573 :: Batch 0/1
Batch Loss = 0.002393933191057896
573, epoch_train_loss=0.002393933191057896
Epoch 574
Epoch 574 :: Batch 0/1
Batch Loss = 0.002398649220702141
574, epoch_train_loss=0.002398649220702141
Epoch 575
Epoch 575 :: Batch 0/1
Batch Loss = 0.0024089820769093214
575, epoch_train_loss=0.0024089820769093214
Epoch 576
Epoch 576 :: Batch 0/1
Batch Loss = 0.0024321409760133354
576, epoch_train_loss=0.0024321409760133354
Epoch 577
Epoch 577 :: Batch 0/1
Batch Loss = 0.002481644723358046
577, epoch_train_loss=0.002481644723358046
Epoch 578
Epoch 578 :: Batch 0/1
Batch Loss = 0.0025951333672910907
578, epoch_train_loss=0.0025951333672910907
Epoch 579
Epoch 579 :: Batch 0/1
Batch Loss = 0.002831357631471235
579, epoch_train_loss=0.002831357631471235
Epoch 580
Epoch 580 :: Batch 0/1
Batch Loss = 0.003394965666862638
580, epoch_train_loss=0.003394965666862638
Epoch 581
Epoch 581 :: Batch 0/1
Batch Loss = 0.004452588396889764
581, epoch_train_loss=0.004452588396889764
Epoch 582
Epoch 582 :: Batch 0/1
Batch Loss = 0.007022771664028597
582, epoch_train_loss=0.007022771664028597
Epoch 583
Epoch 583 :: Batch 0/1
Batch Loss = 0.010087875232275351
583, epoch_train_loss=0.010087875232275351
Epoch 584
Epoch 584 :: Batch 0/1
Batch Loss = 0.01632585141542773
584, epoch_train_loss=0.01632585141542773
Epoch 585
Epoch 585 :: Batch 0/1
Batch Loss = 0.013130883611907167
585, epoch_train_loss=0.013130883611907167
Epoch 586
Epoch 586 :: Batch 0/1
Batch Loss = 0.00817218940687325
586, epoch_train_loss=0.00817218940687325
Epoch 587
Epoch 587 :: Batch 0/1
Batch Loss = 0.002481971513365299
587, epoch_train_loss=0.002481971513365299
Epoch 588
Epoch 588 :: Batch 0/1
Batch Loss = 0.0059693123103805685
588, epoch_train_loss=0.0059693123103805685
Epoch 589
Epoch 589 :: Batch 0/1
Batch Loss = 0.009376797414238247
589, epoch_train_loss=0.009376797414238247
Epoch 590
Epoch 590 :: Batch 0/1
Batch Loss = 0.0033644081170686223
590, epoch_train_loss=0.0033644081170686223
Epoch 591
Epoch 591 :: Batch 0/1
Batch Loss = 0.004482856741859753
591, epoch_train_loss=0.004482856741859753
Epoch 592
Epoch 592 :: Batch 0/1
Batch Loss = 0.006969052925056166
592, epoch_train_loss=0.006969052925056166
Epoch 593
Epoch 593 :: Batch 0/1
Batch Loss = 0.0026372434008387607
593, epoch_train_loss=0.0026372434008387607
Epoch 594
Epoch 594 :: Batch 0/1
Batch Loss = 0.005345806924044885
594, epoch_train_loss=0.005345806924044885
Epoch 595
Epoch 595 :: Batch 0/1
Batch Loss = 0.005225742015003028
595, epoch_train_loss=0.005225742015003028
Epoch 596
Epoch 596 :: Batch 0/1
Batch Loss = 0.0027569580339809586
596, epoch_train_loss=0.0027569580339809586
Epoch 597
Epoch 597 :: Batch 0/1
Batch Loss = 0.005599906210482866
597, epoch_train_loss=0.005599906210482866
Epoch 598
Epoch 598 :: Batch 0/1
Batch Loss = 0.003121543540892133
598, epoch_train_loss=0.003121543540892133
Epoch 599
Epoch 599 :: Batch 0/1
Batch Loss = 0.003911070102379367
599, epoch_train_loss=0.003911070102379367
Epoch 600
Epoch 600 :: Batch 0/1
Batch Loss = 0.003945733369649518
600, epoch_train_loss=0.003945733369649518
Epoch 601
Epoch 601 :: Batch 0/1
Batch Loss = 0.002729022831349283
601, epoch_train_loss=0.002729022831349283
Epoch 602
Epoch 602 :: Batch 0/1
Batch Loss = 0.004061124237001431
602, epoch_train_loss=0.004061124237001431
Epoch 603
Epoch 603 :: Batch 0/1
Batch Loss = 0.00255926817211114
603, epoch_train_loss=0.00255926817211114
Epoch 604
Epoch 604 :: Batch 0/1
Batch Loss = 0.0035981302987843303
604, epoch_train_loss=0.0035981302987843303
Epoch 605
Epoch 605 :: Batch 0/1
Batch Loss = 0.002940389350500169
605, epoch_train_loss=0.002940389350500169
Epoch 606
Epoch 606 :: Batch 0/1
Batch Loss = 0.002926079836203499
606, epoch_train_loss=0.002926079836203499
Epoch 607
Epoch 607 :: Batch 0/1
Batch Loss = 0.003225713687808959
607, epoch_train_loss=0.003225713687808959
Epoch 608
Epoch 608 :: Batch 0/1
Batch Loss = 0.0025270189236834876
608, epoch_train_loss=0.0025270189236834876
Epoch 609
Epoch 609 :: Batch 0/1
Batch Loss = 0.003208627121885823
609, epoch_train_loss=0.003208627121885823
Epoch 610
Epoch 610 :: Batch 0/1
Batch Loss = 0.0025070475835204543
610, epoch_train_loss=0.0025070475835204543
Epoch 611
Epoch 611 :: Batch 0/1
Batch Loss = 0.002916341600334866
611, epoch_train_loss=0.002916341600334866
Epoch 612
Epoch 612 :: Batch 0/1
Batch Loss = 0.0027309692617973266
612, epoch_train_loss=0.0027309692617973266
Epoch 613
Epoch 613 :: Batch 0/1
Batch Loss = 0.002585207584790193
613, epoch_train_loss=0.002585207584790193
Epoch 614
Epoch 614 :: Batch 0/1
Batch Loss = 0.0028477649996376387
614, epoch_train_loss=0.0028477649996376387
Epoch 615
Epoch 615 :: Batch 0/1
Batch Loss = 0.002446663613598175
615, epoch_train_loss=0.002446663613598175
Epoch 616
Epoch 616 :: Batch 0/1
Batch Loss = 0.002782521808762872
616, epoch_train_loss=0.002782521808762872
Epoch 617
Epoch 617 :: Batch 0/1
Batch Loss = 0.002505523566393655
617, epoch_train_loss=0.002505523566393655
Epoch 618
Epoch 618 :: Batch 0/1
Batch Loss = 0.002579335749189286
618, epoch_train_loss=0.002579335749189286
Epoch 619
Epoch 619 :: Batch 0/1
Batch Loss = 0.002629589915599612
619, epoch_train_loss=0.002629589915599612
Epoch 620
Epoch 620 :: Batch 0/1
Batch Loss = 0.002447280643548692
620, epoch_train_loss=0.002447280643548692
Epoch 621
Epoch 621 :: Batch 0/1
Batch Loss = 0.002636672554397832
621, epoch_train_loss=0.002636672554397832
Epoch 622
Epoch 622 :: Batch 0/1
Batch Loss = 0.0024380269198797846
622, epoch_train_loss=0.0024380269198797846
Epoch 623
Epoch 623 :: Batch 0/1
Batch Loss = 0.002555865665021292
623, epoch_train_loss=0.002555865665021292
Epoch 624
Epoch 624 :: Batch 0/1
Batch Loss = 0.002501233771323496
624, epoch_train_loss=0.002501233771323496
Epoch 625
Epoch 625 :: Batch 0/1
Batch Loss = 0.002449607560618624
625, epoch_train_loss=0.002449607560618624
Epoch 626
Epoch 626 :: Batch 0/1
Batch Loss = 0.002539542769347536
626, epoch_train_loss=0.002539542769347536
Epoch 627
Epoch 627 :: Batch 0/1
Batch Loss = 0.002419721963754193
627, epoch_train_loss=0.002419721963754193
Epoch 628
Epoch 628 :: Batch 0/1
Batch Loss = 0.002509028857585239
628, epoch_train_loss=0.002509028857585239
Epoch 629
Epoch 629 :: Batch 0/1
Batch Loss = 0.00244045037342566
629, epoch_train_loss=0.00244045037342566
Epoch 630
Epoch 630 :: Batch 0/1
Batch Loss = 0.00245378168855338
630, epoch_train_loss=0.00245378168855338
Epoch 631
Epoch 631 :: Batch 0/1
Batch Loss = 0.0024740063870144534
631, epoch_train_loss=0.0024740063870144534
Epoch 632
Epoch 632 :: Batch 0/1
Batch Loss = 0.002414153670349372
632, epoch_train_loss=0.002414153670349372
Epoch 633
Epoch 633 :: Batch 0/1
Batch Loss = 0.0024739573352045964
633, epoch_train_loss=0.0024739573352045964
Epoch 634
Epoch 634 :: Batch 0/1
Batch Loss = 0.002422834503819228
634, epoch_train_loss=0.002422834503819228
Epoch 635
Epoch 635 :: Batch 0/1
Batch Loss = 0.0024512182057995716
635, epoch_train_loss=0.0024512182057995716
Epoch 636
Epoch 636 :: Batch 0/1
Batch Loss = 0.002451156960429638
636, epoch_train_loss=0.002451156960429638
Epoch 637
Epoch 637 :: Batch 0/1
Batch Loss = 0.0024416931900477527
637, epoch_train_loss=0.0024416931900477527
Epoch 638
Epoch 638 :: Batch 0/1
Batch Loss = 0.0024885680095311346
638, epoch_train_loss=0.0024885680095311346
Epoch 639
Epoch 639 :: Batch 0/1
Batch Loss = 0.0024807021854962885
639, epoch_train_loss=0.0024807021854962885
Epoch 640
Epoch 640 :: Batch 0/1
Batch Loss = 0.0025476117962779346
640, epoch_train_loss=0.0025476117962779346
Epoch 641
Epoch 641 :: Batch 0/1
Batch Loss = 0.002611092199063204
641, epoch_train_loss=0.002611092199063204
Epoch 642
Epoch 642 :: Batch 0/1
Batch Loss = 0.002718045814331871
642, epoch_train_loss=0.002718045814331871
Epoch 643
Epoch 643 :: Batch 0/1
Batch Loss = 0.0029246905190335646
643, epoch_train_loss=0.0029246905190335646
Epoch 644
Epoch 644 :: Batch 0/1
Batch Loss = 0.003171684968797507
644, epoch_train_loss=0.003171684968797507
Epoch 645
Epoch 645 :: Batch 0/1
Batch Loss = 0.003639213362082098
645, epoch_train_loss=0.003639213362082098
Epoch 646
Epoch 646 :: Batch 0/1
Batch Loss = 0.004113732488814044
646, epoch_train_loss=0.004113732488814044
Epoch 647
Epoch 647 :: Batch 0/1
Batch Loss = 0.004854260386677534
647, epoch_train_loss=0.004854260386677534
Epoch 648
Epoch 648 :: Batch 0/1
Batch Loss = 0.0051678181131703
648, epoch_train_loss=0.0051678181131703
Epoch 649
Epoch 649 :: Batch 0/1
Batch Loss = 0.005432239927133103
649, epoch_train_loss=0.005432239927133103
Epoch 650
Epoch 650 :: Batch 0/1
Batch Loss = 0.004633575825703879
650, epoch_train_loss=0.004633575825703879
Epoch 651
Epoch 651 :: Batch 0/1
Batch Loss = 0.003674656862682582
651, epoch_train_loss=0.003674656862682582
Epoch 652
Epoch 652 :: Batch 0/1
Batch Loss = 0.0026937374650302186
652, epoch_train_loss=0.0026937374650302186
Epoch 653
Epoch 653 :: Batch 0/1
Batch Loss = 0.002402883995413765
653, epoch_train_loss=0.002402883995413765
Epoch 654
Epoch 654 :: Batch 0/1
Batch Loss = 0.0027777449998893852
654, epoch_train_loss=0.0027777449998893852
Epoch 655
Epoch 655 :: Batch 0/1
Batch Loss = 0.0033168120308697266
655, epoch_train_loss=0.0033168120308697266
Epoch 656
Epoch 656 :: Batch 0/1
Batch Loss = 0.0035635677122280015
656, epoch_train_loss=0.0035635677122280015
Epoch 657
Epoch 657 :: Batch 0/1
Batch Loss = 0.003168520679283817
657, epoch_train_loss=0.003168520679283817
Epoch 658
Epoch 658 :: Batch 0/1
Batch Loss = 0.002649629979825751
658, epoch_train_loss=0.002649629979825751
Epoch 659
Epoch 659 :: Batch 0/1
Batch Loss = 0.0023883280940587
659, epoch_train_loss=0.0023883280940587
Epoch 660
Epoch 660 :: Batch 0/1
Batch Loss = 0.0025601181046557907
660, epoch_train_loss=0.0025601181046557907
Epoch 661
Epoch 661 :: Batch 0/1
Batch Loss = 0.002876175454611348
661, epoch_train_loss=0.002876175454611348
Epoch 662
Epoch 662 :: Batch 0/1
Batch Loss = 0.0029342514568066664
662, epoch_train_loss=0.0029342514568066664
Epoch 663
Epoch 663 :: Batch 0/1
Batch Loss = 0.002731272662173258
663, epoch_train_loss=0.002731272662173258
Epoch 664
Epoch 664 :: Batch 0/1
Batch Loss = 0.0024542309320092766
664, epoch_train_loss=0.0024542309320092766
Epoch 665
Epoch 665 :: Batch 0/1
Batch Loss = 0.0023936624225106688
665, epoch_train_loss=0.0023936624225106688
Epoch 666
Epoch 666 :: Batch 0/1
Batch Loss = 0.0025388224895645595
666, epoch_train_loss=0.0025388224895645595
Epoch 667
Epoch 667 :: Batch 0/1
Batch Loss = 0.002680620066416615
667, epoch_train_loss=0.002680620066416615
Epoch 668
Epoch 668 :: Batch 0/1
Batch Loss = 0.002671078039855646
668, epoch_train_loss=0.002671078039855646
Epoch 669
Epoch 669 :: Batch 0/1
Batch Loss = 0.0025123378558542333
669, epoch_train_loss=0.0025123378558542333
Epoch 670
Epoch 670 :: Batch 0/1
Batch Loss = 0.0023919039415731687
670, epoch_train_loss=0.0023919039415731687
Epoch 671
Epoch 671 :: Batch 0/1
Batch Loss = 0.002401996114837087
671, epoch_train_loss=0.002401996114837087
Epoch 672
Epoch 672 :: Batch 0/1
Batch Loss = 0.0024970745144769877
672, epoch_train_loss=0.0024970745144769877
Epoch 673
Epoch 673 :: Batch 0/1
Batch Loss = 0.0025573272874669724
673, epoch_train_loss=0.0025573272874669724
Epoch 674
Epoch 674 :: Batch 0/1
Batch Loss = 0.002511782008697373
674, epoch_train_loss=0.002511782008697373
Epoch 675
Epoch 675 :: Batch 0/1
Batch Loss = 0.0024252697672631396
675, epoch_train_loss=0.0024252697672631396
Epoch 676
Epoch 676 :: Batch 0/1
Batch Loss = 0.0023761128015446237
676, epoch_train_loss=0.0023761128015446237
Epoch 677
Epoch 677 :: Batch 0/1
Batch Loss = 0.002400064674049289
677, epoch_train_loss=0.002400064674049289
Epoch 678
Epoch 678 :: Batch 0/1
Batch Loss = 0.002453724986432624
678, epoch_train_loss=0.002453724986432624
Epoch 679
Epoch 679 :: Batch 0/1
Batch Loss = 0.0024722491627132397
679, epoch_train_loss=0.0024722491627132397
Epoch 680
Epoch 680 :: Batch 0/1
Batch Loss = 0.002443676472418615
680, epoch_train_loss=0.002443676472418615
Epoch 681
Epoch 681 :: Batch 0/1
Batch Loss = 0.002394655213053472
681, epoch_train_loss=0.002394655213053472
Epoch 682
Epoch 682 :: Batch 0/1
Batch Loss = 0.0023724504108838945
682, epoch_train_loss=0.0023724504108838945
Epoch 683
Epoch 683 :: Batch 0/1
Batch Loss = 0.002387795995613118
683, epoch_train_loss=0.002387795995613118
Epoch 684
Epoch 684 :: Batch 0/1
Batch Loss = 0.002415273883254867
684, epoch_train_loss=0.002415273883254867
Epoch 685
Epoch 685 :: Batch 0/1
Batch Loss = 0.0024262797930614424
685, epoch_train_loss=0.0024262797930614424
Epoch 686
Epoch 686 :: Batch 0/1
Batch Loss = 0.00240937057077901
686, epoch_train_loss=0.00240937057077901
Epoch 687
Epoch 687 :: Batch 0/1
Batch Loss = 0.0023835137850562486
687, epoch_train_loss=0.0023835137850562486
Epoch 688
Epoch 688 :: Batch 0/1
Batch Loss = 0.002369602730100847
688, epoch_train_loss=0.002369602730100847
Epoch 689
Epoch 689 :: Batch 0/1
Batch Loss = 0.002374738689802522
689, epoch_train_loss=0.002374738689802522
Epoch 690
Epoch 690 :: Batch 0/1
Batch Loss = 0.0023894598731360292
690, epoch_train_loss=0.0023894598731360292
Epoch 691
Epoch 691 :: Batch 0/1
Batch Loss = 0.00239739043369343
691, epoch_train_loss=0.00239739043369343
Epoch 692
Epoch 692 :: Batch 0/1
Batch Loss = 0.0023922448589008684
692, epoch_train_loss=0.0023922448589008684
Epoch 693
Epoch 693 :: Batch 0/1
Batch Loss = 0.0023787505664304813
693, epoch_train_loss=0.0023787505664304813
Epoch 694
Epoch 694 :: Batch 0/1
Batch Loss = 0.002367885968455059
694, epoch_train_loss=0.002367885968455059
Epoch 695
Epoch 695 :: Batch 0/1
Batch Loss = 0.0023663980335186293
695, epoch_train_loss=0.0023663980335186293
Epoch 696
Epoch 696 :: Batch 0/1
Batch Loss = 0.002372427979195067
696, epoch_train_loss=0.002372427979195067
Epoch 697
Epoch 697 :: Batch 0/1
Batch Loss = 0.0023788811223690105
697, epoch_train_loss=0.0023788811223690105
Epoch 698
Epoch 698 :: Batch 0/1
Batch Loss = 0.00237992243351699
698, epoch_train_loss=0.00237992243351699
Epoch 699
Epoch 699 :: Batch 0/1
Batch Loss = 0.0023750359763056233
699, epoch_train_loss=0.0023750359763056233
Epoch 700
Epoch 700 :: Batch 0/1
Batch Loss = 0.0023678477827495884
700, epoch_train_loss=0.0023678477827495884
Epoch 701
Epoch 701 :: Batch 0/1
Batch Loss = 0.002363231789176532
701, epoch_train_loss=0.002363231789176532
Epoch 702
Epoch 702 :: Batch 0/1
Batch Loss = 0.0023629938957764668
702, epoch_train_loss=0.0023629938957764668
Epoch 703
Epoch 703 :: Batch 0/1
Batch Loss = 0.0023658819823048653
703, epoch_train_loss=0.0023658819823048653
Epoch 704
Epoch 704 :: Batch 0/1
Batch Loss = 0.002368749361673167
704, epoch_train_loss=0.002368749361673167
Epoch 705
Epoch 705 :: Batch 0/1
Batch Loss = 0.0023690720025125243
705, epoch_train_loss=0.0023690720025125243
Epoch 706
Epoch 706 :: Batch 0/1
Batch Loss = 0.002366789693739393
706, epoch_train_loss=0.002366789693739393
Epoch 707
Epoch 707 :: Batch 0/1
Batch Loss = 0.002363181771269442
707, epoch_train_loss=0.002363181771269442
Epoch 708
Epoch 708 :: Batch 0/1
Batch Loss = 0.0023603080800469775
708, epoch_train_loss=0.0023603080800469775
Epoch 709
Epoch 709 :: Batch 0/1
Batch Loss = 0.0023592966939831343
709, epoch_train_loss=0.0023592966939831343
Epoch 710
Epoch 710 :: Batch 0/1
Batch Loss = 0.0023599182748821924
710, epoch_train_loss=0.0023599182748821924
Epoch 711
Epoch 711 :: Batch 0/1
Batch Loss = 0.0023611784521402625
711, epoch_train_loss=0.0023611784521402625
Epoch 712
Epoch 712 :: Batch 0/1
Batch Loss = 0.002361873211469065
712, epoch_train_loss=0.002361873211469065
Epoch 713
Epoch 713 :: Batch 0/1
Batch Loss = 0.0023614230793952196
713, epoch_train_loss=0.0023614230793952196
Epoch 714
Epoch 714 :: Batch 0/1
Batch Loss = 0.0023599740671596155
714, epoch_train_loss=0.0023599740671596155
Epoch 715
Epoch 715 :: Batch 0/1
Batch Loss = 0.002358163986616583
715, epoch_train_loss=0.002358163986616583
Epoch 716
Epoch 716 :: Batch 0/1
Batch Loss = 0.0023566764483767756
716, epoch_train_loss=0.0023566764483767756
Epoch 717
Epoch 717 :: Batch 0/1
Batch Loss = 0.002355921909165237
717, epoch_train_loss=0.002355921909165237
Epoch 718
Epoch 718 :: Batch 0/1
Batch Loss = 0.0023558344358213116
718, epoch_train_loss=0.0023558344358213116
Epoch 719
Epoch 719 :: Batch 0/1
Batch Loss = 0.0023561064620929665
719, epoch_train_loss=0.0023561064620929665
Epoch 720
Epoch 720 :: Batch 0/1
Batch Loss = 0.002356332597642857
720, epoch_train_loss=0.002356332597642857
Epoch 721
Epoch 721 :: Batch 0/1
Batch Loss = 0.002356203428100451
721, epoch_train_loss=0.002356203428100451
Epoch 722
Epoch 722 :: Batch 0/1
Batch Loss = 0.0023556783580470355
722, epoch_train_loss=0.0023556783580470355
Epoch 723
Epoch 723 :: Batch 0/1
Batch Loss = 0.0023548387566715095
723, epoch_train_loss=0.0023548387566715095
Epoch 724
Epoch 724 :: Batch 0/1
Batch Loss = 0.00235390023364593
724, epoch_train_loss=0.00235390023364593
Epoch 725
Epoch 725 :: Batch 0/1
Batch Loss = 0.0023530647200387124
725, epoch_train_loss=0.0023530647200387124
Epoch 726
Epoch 726 :: Batch 0/1
Batch Loss = 0.00235244203357783
726, epoch_train_loss=0.00235244203357783
Epoch 727
Epoch 727 :: Batch 0/1
Batch Loss = 0.0023520580091259953
727, epoch_train_loss=0.0023520580091259953
Epoch 728
Epoch 728 :: Batch 0/1
Batch Loss = 0.002351854744044017
728, epoch_train_loss=0.002351854744044017
Epoch 729
Epoch 729 :: Batch 0/1
Batch Loss = 0.002351722917216072
729, epoch_train_loss=0.002351722917216072
Epoch 730
Epoch 730 :: Batch 0/1
Batch Loss = 0.00235157274082107
730, epoch_train_loss=0.00235157274082107
Epoch 731
Epoch 731 :: Batch 0/1
Batch Loss = 0.002351340300856791
731, epoch_train_loss=0.002351340300856791
Epoch 732
Epoch 732 :: Batch 0/1
Batch Loss = 0.0023509929887146706
732, epoch_train_loss=0.0023509929887146706
Epoch 733
Epoch 733 :: Batch 0/1
Batch Loss = 0.002350553746713512
733, epoch_train_loss=0.002350553746713512
Epoch 734
Epoch 734 :: Batch 0/1
Batch Loss = 0.0023500474272293245
734, epoch_train_loss=0.0023500474272293245
Epoch 735
Epoch 735 :: Batch 0/1
Batch Loss = 0.0023495167458779565
735, epoch_train_loss=0.0023495167458779565
Epoch 736
Epoch 736 :: Batch 0/1
Batch Loss = 0.002349000011108356
736, epoch_train_loss=0.002349000011108356
Epoch 737
Epoch 737 :: Batch 0/1
Batch Loss = 0.0023485197266939505
737, epoch_train_loss=0.0023485197266939505
Epoch 738
Epoch 738 :: Batch 0/1
Batch Loss = 0.0023480873299234063
738, epoch_train_loss=0.0023480873299234063
Epoch 739
Epoch 739 :: Batch 0/1
Batch Loss = 0.0023477050505026917
739, epoch_train_loss=0.0023477050505026917
Epoch 740
Epoch 740 :: Batch 0/1
Batch Loss = 0.002347361919392195
740, epoch_train_loss=0.002347361919392195
Epoch 741
Epoch 741 :: Batch 0/1
Batch Loss = 0.0023470484420148453
741, epoch_train_loss=0.0023470484420148453
Epoch 742
Epoch 742 :: Batch 0/1
Batch Loss = 0.0023467541171485052
742, epoch_train_loss=0.0023467541171485052
Epoch 743
Epoch 743 :: Batch 0/1
Batch Loss = 0.002346467093120139
743, epoch_train_loss=0.002346467093120139
Epoch 744
Epoch 744 :: Batch 0/1
Batch Loss = 0.0023461823597011613
744, epoch_train_loss=0.0023461823597011613
Epoch 745
Epoch 745 :: Batch 0/1
Batch Loss = 0.0023458948052957175
745, epoch_train_loss=0.0023458948052957175
Epoch 746
Epoch 746 :: Batch 0/1
Batch Loss = 0.002345602655102263
746, epoch_train_loss=0.002345602655102263
Epoch 747
Epoch 747 :: Batch 0/1
Batch Loss = 0.002345306833769487
747, epoch_train_loss=0.002345306833769487
Epoch 748
Epoch 748 :: Batch 0/1
Batch Loss = 0.002345009907758944
748, epoch_train_loss=0.002345009907758944
Epoch 749
Epoch 749 :: Batch 0/1
Batch Loss = 0.0023447124787834865
749, epoch_train_loss=0.0023447124787834865
Epoch 750
Epoch 750 :: Batch 0/1
Batch Loss = 0.0023444206698092955
750, epoch_train_loss=0.0023444206698092955
Epoch 751
Epoch 751 :: Batch 0/1
Batch Loss = 0.0023441357435119866
751, epoch_train_loss=0.0023441357435119866
Epoch 752
Epoch 752 :: Batch 0/1
Batch Loss = 0.002343864629948602
752, epoch_train_loss=0.002343864629948602
Epoch 753
Epoch 753 :: Batch 0/1
Batch Loss = 0.0023436114359380966
753, epoch_train_loss=0.0023436114359380966
Epoch 754
Epoch 754 :: Batch 0/1
Batch Loss = 0.002343386345746328
754, epoch_train_loss=0.002343386345746328
Epoch 755
Epoch 755 :: Batch 0/1
Batch Loss = 0.0023431964196126615
755, epoch_train_loss=0.0023431964196126615
Epoch 756
Epoch 756 :: Batch 0/1
Batch Loss = 0.002343062214242754
756, epoch_train_loss=0.002343062214242754
Epoch 757
Epoch 757 :: Batch 0/1
Batch Loss = 0.0023429997097400086
757, epoch_train_loss=0.0023429997097400086
Epoch 758
Epoch 758 :: Batch 0/1
Batch Loss = 0.002343050901193853
758, epoch_train_loss=0.002343050901193853
Epoch 759
Epoch 759 :: Batch 0/1
Batch Loss = 0.0023432551142189843
759, epoch_train_loss=0.0023432551142189843
Epoch 760
Epoch 760 :: Batch 0/1
Batch Loss = 0.002343706753167461
760, epoch_train_loss=0.002343706753167461
Epoch 761
Epoch 761 :: Batch 0/1
Batch Loss = 0.0023445000610844407
761, epoch_train_loss=0.0023445000610844407
Epoch 762
Epoch 762 :: Batch 0/1
Batch Loss = 0.0023458654545986377
762, epoch_train_loss=0.0023458654545986377
Epoch 763
Epoch 763 :: Batch 0/1
Batch Loss = 0.0023480332509323216
763, epoch_train_loss=0.0023480332509323216
Epoch 764
Epoch 764 :: Batch 0/1
Batch Loss = 0.002351599010610571
764, epoch_train_loss=0.002351599010610571
Epoch 765
Epoch 765 :: Batch 0/1
Batch Loss = 0.0023571362061081966
765, epoch_train_loss=0.0023571362061081966
Epoch 766
Epoch 766 :: Batch 0/1
Batch Loss = 0.0023662824286495936
766, epoch_train_loss=0.0023662824286495936
Epoch 767
Epoch 767 :: Batch 0/1
Batch Loss = 0.002380460411673979
767, epoch_train_loss=0.002380460411673979
Epoch 768
Epoch 768 :: Batch 0/1
Batch Loss = 0.0024044548499648735
768, epoch_train_loss=0.0024044548499648735
Epoch 769
Epoch 769 :: Batch 0/1
Batch Loss = 0.0024416186568499954
769, epoch_train_loss=0.0024416186568499954
Epoch 770
Epoch 770 :: Batch 0/1
Batch Loss = 0.0025067400530521086
770, epoch_train_loss=0.0025067400530521086
Epoch 771
Epoch 771 :: Batch 0/1
Batch Loss = 0.002606164937221209
771, epoch_train_loss=0.002606164937221209
Epoch 772
Epoch 772 :: Batch 0/1
Batch Loss = 0.002787633666602363
772, epoch_train_loss=0.002787633666602363
Epoch 773
Epoch 773 :: Batch 0/1
Batch Loss = 0.003051151677269925
773, epoch_train_loss=0.003051151677269925
Epoch 774
Epoch 774 :: Batch 0/1
Batch Loss = 0.003551678248070548
774, epoch_train_loss=0.003551678248070548
Epoch 775
Epoch 775 :: Batch 0/1
Batch Loss = 0.004183791557126584
775, epoch_train_loss=0.004183791557126584
Epoch 776
Epoch 776 :: Batch 0/1
Batch Loss = 0.005407112085531877
776, epoch_train_loss=0.005407112085531877
Epoch 777
Epoch 777 :: Batch 0/1
Batch Loss = 0.006430458151568585
777, epoch_train_loss=0.006430458151568585
Epoch 778
Epoch 778 :: Batch 0/1
Batch Loss = 0.008318542114186327
778, epoch_train_loss=0.008318542114186327
Epoch 779
Epoch 779 :: Batch 0/1
Batch Loss = 0.008177941678919777
779, epoch_train_loss=0.008177941678919777
Epoch 780
Epoch 780 :: Batch 0/1
Batch Loss = 0.008106651910320027
780, epoch_train_loss=0.008106651910320027
Epoch 781
Epoch 781 :: Batch 0/1
Batch Loss = 0.005356002806602693
781, epoch_train_loss=0.005356002806602693
Epoch 782
Epoch 782 :: Batch 0/1
Batch Loss = 0.003218386874124358
782, epoch_train_loss=0.003218386874124358
Epoch 783
Epoch 783 :: Batch 0/1
Batch Loss = 0.0023491350445349426
783, epoch_train_loss=0.0023491350445349426
Epoch 784
Epoch 784 :: Batch 0/1
Batch Loss = 0.0031304986251191086
784, epoch_train_loss=0.0031304986251191086
Epoch 785
Epoch 785 :: Batch 0/1
Batch Loss = 0.004590984961303052
785, epoch_train_loss=0.004590984961303052
Epoch 786
Epoch 786 :: Batch 0/1
Batch Loss = 0.004903513661602084
786, epoch_train_loss=0.004903513661602084
Epoch 787
Epoch 787 :: Batch 0/1
Batch Loss = 0.004332797616762776
787, epoch_train_loss=0.004332797616762776
Epoch 788
Epoch 788 :: Batch 0/1
Batch Loss = 0.002935044272554036
788, epoch_train_loss=0.002935044272554036
Epoch 789
Epoch 789 :: Batch 0/1
Batch Loss = 0.0023525219416546082
789, epoch_train_loss=0.0023525219416546082
Epoch 790
Epoch 790 :: Batch 0/1
Batch Loss = 0.002842687417063309
790, epoch_train_loss=0.002842687417063309
Epoch 791
Epoch 791 :: Batch 0/1
Batch Loss = 0.003510794128542925
791, epoch_train_loss=0.003510794128542925
Epoch 792
Epoch 792 :: Batch 0/1
Batch Loss = 0.0036080309574970447
792, epoch_train_loss=0.0036080309574970447
Epoch 793
Epoch 793 :: Batch 0/1
Batch Loss = 0.0028891205539657173
793, epoch_train_loss=0.0028891205539657173
Epoch 794
Epoch 794 :: Batch 0/1
Batch Loss = 0.0023744121421424816
794, epoch_train_loss=0.0023744121421424816
Epoch 795
Epoch 795 :: Batch 0/1
Batch Loss = 0.0024992644685276617
795, epoch_train_loss=0.0024992644685276617
Epoch 796
Epoch 796 :: Batch 0/1
Batch Loss = 0.0029349323549432943
796, epoch_train_loss=0.0029349323549432943
Epoch 797
Epoch 797 :: Batch 0/1
Batch Loss = 0.0031247428022821906
797, epoch_train_loss=0.0031247428022821906
Epoch 798
Epoch 798 :: Batch 0/1
Batch Loss = 0.0027784694379835626
798, epoch_train_loss=0.0027784694379835626
Epoch 799
Epoch 799 :: Batch 0/1
Batch Loss = 0.002407939027265111
799, epoch_train_loss=0.002407939027265111
Epoch 800
Epoch 800 :: Batch 0/1
Batch Loss = 0.002376666025528301
800, epoch_train_loss=0.002376666025528301
Epoch 801
Epoch 801 :: Batch 0/1
Batch Loss = 0.0026216345118526157
801, epoch_train_loss=0.0026216345118526157
Epoch 802
Epoch 802 :: Batch 0/1
Batch Loss = 0.002787813171427864
802, epoch_train_loss=0.002787813171427864
Epoch 803
Epoch 803 :: Batch 0/1
Batch Loss = 0.002638837017956499
803, epoch_train_loss=0.002638837017956499
Epoch 804
Epoch 804 :: Batch 0/1
Batch Loss = 0.0024093643452505055
804, epoch_train_loss=0.0024093643452505055
Epoch 805
Epoch 805 :: Batch 0/1
Batch Loss = 0.0023404968329598444
805, epoch_train_loss=0.0023404968329598444
Epoch 806
Epoch 806 :: Batch 0/1
Batch Loss = 0.00246288652353161
806, epoch_train_loss=0.00246288652353161
Epoch 807
Epoch 807 :: Batch 0/1
Batch Loss = 0.0025904854445994636
807, epoch_train_loss=0.0025904854445994636
Epoch 808
Epoch 808 :: Batch 0/1
Batch Loss = 0.0025506959579835553
808, epoch_train_loss=0.0025506959579835553
Epoch 809
Epoch 809 :: Batch 0/1
Batch Loss = 0.0024176736750892336
809, epoch_train_loss=0.0024176736750892336
Epoch 810
Epoch 810 :: Batch 0/1
Batch Loss = 0.002334923858553344
810, epoch_train_loss=0.002334923858553344
Epoch 811
Epoch 811 :: Batch 0/1
Batch Loss = 0.0023760282391072037
811, epoch_train_loss=0.0023760282391072037
Epoch 812
Epoch 812 :: Batch 0/1
Batch Loss = 0.0024620052159851423
812, epoch_train_loss=0.0024620052159851423
Epoch 813
Epoch 813 :: Batch 0/1
Batch Loss = 0.0024750258861860456
813, epoch_train_loss=0.0024750258861860456
Epoch 814
Epoch 814 :: Batch 0/1
Batch Loss = 0.0024121779958707562
814, epoch_train_loss=0.0024121779958707562
Epoch 815
Epoch 815 :: Batch 0/1
Batch Loss = 0.00234324733166881
815, epoch_train_loss=0.00234324733166881
Epoch 816
Epoch 816 :: Batch 0/1
Batch Loss = 0.002338294650255999
816, epoch_train_loss=0.002338294650255999
Epoch 817
Epoch 817 :: Batch 0/1
Batch Loss = 0.0023838540168142222
817, epoch_train_loss=0.0023838540168142222
Epoch 818
Epoch 818 :: Batch 0/1
Batch Loss = 0.002415387183548278
818, epoch_train_loss=0.002415387183548278
Epoch 819
Epoch 819 :: Batch 0/1
Batch Loss = 0.0023998872702983886
819, epoch_train_loss=0.0023998872702983886
Epoch 820
Epoch 820 :: Batch 0/1
Batch Loss = 0.0023548556065688874
820, epoch_train_loss=0.0023548556065688874
Epoch 821
Epoch 821 :: Batch 0/1
Batch Loss = 0.0023296740559810517
821, epoch_train_loss=0.0023296740559810517
Epoch 822
Epoch 822 :: Batch 0/1
Batch Loss = 0.002341894448383033
822, epoch_train_loss=0.002341894448383033
Epoch 823
Epoch 823 :: Batch 0/1
Batch Loss = 0.0023681167895060707
823, epoch_train_loss=0.0023681167895060707
Epoch 824
Epoch 824 :: Batch 0/1
Batch Loss = 0.002376862007733674
824, epoch_train_loss=0.002376862007733674
Epoch 825
Epoch 825 :: Batch 0/1
Batch Loss = 0.002358667395151435
825, epoch_train_loss=0.002358667395151435
Epoch 826
Epoch 826 :: Batch 0/1
Batch Loss = 0.0023350260294692123
826, epoch_train_loss=0.0023350260294692123
Epoch 827
Epoch 827 :: Batch 0/1
Batch Loss = 0.0023270393912373606
827, epoch_train_loss=0.0023270393912373606
Epoch 828
Epoch 828 :: Batch 0/1
Batch Loss = 0.0023373355001255216
828, epoch_train_loss=0.0023373355001255216
Epoch 829
Epoch 829 :: Batch 0/1
Batch Loss = 0.002350809320535559
829, epoch_train_loss=0.002350809320535559
Epoch 830
Epoch 830 :: Batch 0/1
Batch Loss = 0.0023519246378651756
830, epoch_train_loss=0.0023519246378651756
Epoch 831
Epoch 831 :: Batch 0/1
Batch Loss = 0.0023408590520465154
831, epoch_train_loss=0.0023408590520465154
Epoch 832
Epoch 832 :: Batch 0/1
Batch Loss = 0.002328302850861738
832, epoch_train_loss=0.002328302850861738
Epoch 833
Epoch 833 :: Batch 0/1
Batch Loss = 0.002324949984724407
833, epoch_train_loss=0.002324949984724407
Epoch 834
Epoch 834 :: Batch 0/1
Batch Loss = 0.0023308461252518026
834, epoch_train_loss=0.0023308461252518026
Epoch 835
Epoch 835 :: Batch 0/1
Batch Loss = 0.0023377478687392
835, epoch_train_loss=0.0023377478687392
Epoch 836
Epoch 836 :: Batch 0/1
Batch Loss = 0.002338501585532342
836, epoch_train_loss=0.002338501585532342
Epoch 837
Epoch 837 :: Batch 0/1
Batch Loss = 0.002332446895174805
837, epoch_train_loss=0.002332446895174805
Epoch 838
Epoch 838 :: Batch 0/1
Batch Loss = 0.002325413007822359
838, epoch_train_loss=0.002325413007822359
Epoch 839
Epoch 839 :: Batch 0/1
Batch Loss = 0.0023226828328987113
839, epoch_train_loss=0.0023226828328987113
Epoch 840
Epoch 840 :: Batch 0/1
Batch Loss = 0.0023250022692791294
840, epoch_train_loss=0.0023250022692791294
Epoch 841
Epoch 841 :: Batch 0/1
Batch Loss = 0.0023288039877479615
841, epoch_train_loss=0.0023288039877479615
Epoch 842
Epoch 842 :: Batch 0/1
Batch Loss = 0.002329984786480223
842, epoch_train_loss=0.002329984786480223
Epoch 843
Epoch 843 :: Batch 0/1
Batch Loss = 0.002327595965522205
843, epoch_train_loss=0.002327595965522205
Epoch 844
Epoch 844 :: Batch 0/1
Batch Loss = 0.0023236000280220472
844, epoch_train_loss=0.0023236000280220472
Epoch 845
Epoch 845 :: Batch 0/1
Batch Loss = 0.0023209804791662495
845, epoch_train_loss=0.0023209804791662495
Epoch 846
Epoch 846 :: Batch 0/1
Batch Loss = 0.0023209948626987335
846, epoch_train_loss=0.0023209948626987335
Epoch 847
Epoch 847 :: Batch 0/1
Batch Loss = 0.0023226720231795273
847, epoch_train_loss=0.0023226720231795273
Epoch 848
Epoch 848 :: Batch 0/1
Batch Loss = 0.0023240411757721596
848, epoch_train_loss=0.0023240411757721596
Epoch 849
Epoch 849 :: Batch 0/1
Batch Loss = 0.0023237434697895346
849, epoch_train_loss=0.0023237434697895346
Epoch 850
Epoch 850 :: Batch 0/1
Batch Loss = 0.002321990837237447
850, epoch_train_loss=0.002321990837237447
Epoch 851
Epoch 851 :: Batch 0/1
Batch Loss = 0.0023199296767530708
851, epoch_train_loss=0.0023199296767530708
Epoch 852
Epoch 852 :: Batch 0/1
Batch Loss = 0.0023187416789108082
852, epoch_train_loss=0.0023187416789108082
Epoch 853
Epoch 853 :: Batch 0/1
Batch Loss = 0.002318760122262348
853, epoch_train_loss=0.002318760122262348
Epoch 854
Epoch 854 :: Batch 0/1
Batch Loss = 0.002319450541415721
854, epoch_train_loss=0.002319450541415721
Epoch 855
Epoch 855 :: Batch 0/1
Batch Loss = 0.002319959851698584
855, epoch_train_loss=0.002319959851698584
Epoch 856
Epoch 856 :: Batch 0/1
Batch Loss = 0.002319719892865299
856, epoch_train_loss=0.002319719892865299
Epoch 857
Epoch 857 :: Batch 0/1
Batch Loss = 0.002318793091441579
857, epoch_train_loss=0.002318793091441579
Epoch 858
Epoch 858 :: Batch 0/1
Batch Loss = 0.0023176408547630816
858, epoch_train_loss=0.0023176408547630816
Epoch 859
Epoch 859 :: Batch 0/1
Batch Loss = 0.0023167938772964245
859, epoch_train_loss=0.0023167938772964245
Epoch 860
Epoch 860 :: Batch 0/1
Batch Loss = 0.0023164846906129147
860, epoch_train_loss=0.0023164846906129147
Epoch 861
Epoch 861 :: Batch 0/1
Batch Loss = 0.002316582959046454
861, epoch_train_loss=0.002316582959046454
Epoch 862
Epoch 862 :: Batch 0/1
Batch Loss = 0.002316753962198294
862, epoch_train_loss=0.002316753962198294
Epoch 863
Epoch 863 :: Batch 0/1
Batch Loss = 0.002316694153062429
863, epoch_train_loss=0.002316694153062429
Epoch 864
Epoch 864 :: Batch 0/1
Batch Loss = 0.0023163066593531885
864, epoch_train_loss=0.0023163066593531885
Epoch 865
Epoch 865 :: Batch 0/1
Batch Loss = 0.0023156865515387742
865, epoch_train_loss=0.0023156865515387742
Epoch 866
Epoch 866 :: Batch 0/1
Batch Loss = 0.002315042525318792
866, epoch_train_loss=0.002315042525318792
Epoch 867
Epoch 867 :: Batch 0/1
Batch Loss = 0.002314546949895938
867, epoch_train_loss=0.002314546949895938
Epoch 868
Epoch 868 :: Batch 0/1
Batch Loss = 0.002314265399401525
868, epoch_train_loss=0.002314265399401525
Epoch 869
Epoch 869 :: Batch 0/1
Batch Loss = 0.0023141460942880596
869, epoch_train_loss=0.0023141460942880596
Epoch 870
Epoch 870 :: Batch 0/1
Batch Loss = 0.002314071256339997
870, epoch_train_loss=0.002314071256339997
Epoch 871
Epoch 871 :: Batch 0/1
Batch Loss = 0.0023139294401874552
871, epoch_train_loss=0.0023139294401874552
Epoch 872
Epoch 872 :: Batch 0/1
Batch Loss = 0.00231366286038404
872, epoch_train_loss=0.00231366286038404
Epoch 873
Epoch 873 :: Batch 0/1
Batch Loss = 0.002313287961775816
873, epoch_train_loss=0.002313287961775816
Epoch 874
Epoch 874 :: Batch 0/1
Batch Loss = 0.0023128638182791632
874, epoch_train_loss=0.0023128638182791632
Epoch 875
Epoch 875 :: Batch 0/1
Batch Loss = 0.0023124610868033306
875, epoch_train_loss=0.0023124610868033306
Epoch 876
Epoch 876 :: Batch 0/1
Batch Loss = 0.0023121246973643057
876, epoch_train_loss=0.0023121246973643057
Epoch 877
Epoch 877 :: Batch 0/1
Batch Loss = 0.002311864099227514
877, epoch_train_loss=0.002311864099227514
Epoch 878
Epoch 878 :: Batch 0/1
Batch Loss = 0.002311658202281217
878, epoch_train_loss=0.002311658202281217
Epoch 879
Epoch 879 :: Batch 0/1
Batch Loss = 0.0023114709554923722
879, epoch_train_loss=0.0023114709554923722
Epoch 880
Epoch 880 :: Batch 0/1
Batch Loss = 0.002311268522638563
880, epoch_train_loss=0.002311268522638563
Epoch 881
Epoch 881 :: Batch 0/1
Batch Loss = 0.0023110295780742595
881, epoch_train_loss=0.0023110295780742595
Epoch 882
Epoch 882 :: Batch 0/1
Batch Loss = 0.002310751633572654
882, epoch_train_loss=0.002310751633572654
Epoch 883
Epoch 883 :: Batch 0/1
Batch Loss = 0.0023104454104683194
883, epoch_train_loss=0.0023104454104683194
Epoch 884
Epoch 884 :: Batch 0/1
Batch Loss = 0.002310130055094726
884, epoch_train_loss=0.002310130055094726
Epoch 885
Epoch 885 :: Batch 0/1
Batch Loss = 0.0023098225978437285
885, epoch_train_loss=0.0023098225978437285
Epoch 886
Epoch 886 :: Batch 0/1
Batch Loss = 0.0023095342148679236
886, epoch_train_loss=0.0023095342148679236
Epoch 887
Epoch 887 :: Batch 0/1
Batch Loss = 0.002309267992337003
887, epoch_train_loss=0.002309267992337003
Epoch 888
Epoch 888 :: Batch 0/1
Batch Loss = 0.002309020748013513
888, epoch_train_loss=0.002309020748013513
Epoch 889
Epoch 889 :: Batch 0/1
Batch Loss = 0.0023087855208602753
889, epoch_train_loss=0.0023087855208602753
Epoch 890
Epoch 890 :: Batch 0/1
Batch Loss = 0.002308554099556632
890, epoch_train_loss=0.002308554099556632
Epoch 891
Epoch 891 :: Batch 0/1
Batch Loss = 0.0023083195279474744
891, epoch_train_loss=0.0023083195279474744
Epoch 892
Epoch 892 :: Batch 0/1
Batch Loss = 0.0023080772581729084
892, epoch_train_loss=0.0023080772581729084
Epoch 893
Epoch 893 :: Batch 0/1
Batch Loss = 0.0023078262033534567
893, epoch_train_loss=0.0023078262033534567
Epoch 894
Epoch 894 :: Batch 0/1
Batch Loss = 0.0023075672704391034
894, epoch_train_loss=0.0023075672704391034
Epoch 895
Epoch 895 :: Batch 0/1
Batch Loss = 0.0023073031448600854
895, epoch_train_loss=0.0023073031448600854
Epoch 896
Epoch 896 :: Batch 0/1
Batch Loss = 0.00230703643847608
896, epoch_train_loss=0.00230703643847608
Epoch 897
Epoch 897 :: Batch 0/1
Batch Loss = 0.0023067698906582886
897, epoch_train_loss=0.0023067698906582886
Epoch 898
Epoch 898 :: Batch 0/1
Batch Loss = 0.0023065054323393916
898, epoch_train_loss=0.0023065054323393916
Epoch 899
Epoch 899 :: Batch 0/1
Batch Loss = 0.0023062444238739683
899, epoch_train_loss=0.0023062444238739683
Epoch 900
Epoch 900 :: Batch 0/1
Batch Loss = 0.0023059872604913345
900, epoch_train_loss=0.0023059872604913345
Epoch 901
Epoch 901 :: Batch 0/1
Batch Loss = 0.0023057337659921906
901, epoch_train_loss=0.0023057337659921906
Epoch 902
Epoch 902 :: Batch 0/1
Batch Loss = 0.002305483378756237
902, epoch_train_loss=0.002305483378756237
Epoch 903
Epoch 903 :: Batch 0/1
Batch Loss = 0.0023052354726260035
903, epoch_train_loss=0.0023052354726260035
Epoch 904
Epoch 904 :: Batch 0/1
Batch Loss = 0.0023049894377666865
904, epoch_train_loss=0.0023049894377666865
Epoch 905
Epoch 905 :: Batch 0/1
Batch Loss = 0.0023047446793956136
905, epoch_train_loss=0.0023047446793956136
Epoch 906
Epoch 906 :: Batch 0/1
Batch Loss = 0.0023045007423271196
906, epoch_train_loss=0.0023045007423271196
Epoch 907
Epoch 907 :: Batch 0/1
Batch Loss = 0.0023042572974189567
907, epoch_train_loss=0.0023042572974189567
Epoch 908
Epoch 908 :: Batch 0/1
Batch Loss = 0.0023040142790319925
908, epoch_train_loss=0.0023040142790319925
Epoch 909
Epoch 909 :: Batch 0/1
Batch Loss = 0.0023037716490508665
909, epoch_train_loss=0.0023037716490508665
Epoch 910
Epoch 910 :: Batch 0/1
Batch Loss = 0.0023035295545892395
910, epoch_train_loss=0.0023035295545892395
Epoch 911
Epoch 911 :: Batch 0/1
Batch Loss = 0.002303288074361849
911, epoch_train_loss=0.002303288074361849
Epoch 912
Epoch 912 :: Batch 0/1
Batch Loss = 0.00230304755363644
912, epoch_train_loss=0.00230304755363644
Epoch 913
Epoch 913 :: Batch 0/1
Batch Loss = 0.0023028082737581957
913, epoch_train_loss=0.0023028082737581957
Epoch 914
Epoch 914 :: Batch 0/1
Batch Loss = 0.0023025708467425405
914, epoch_train_loss=0.0023025708467425405
Epoch 915
Epoch 915 :: Batch 0/1
Batch Loss = 0.002302335780304382
915, epoch_train_loss=0.002302335780304382
Epoch 916
Epoch 916 :: Batch 0/1
Batch Loss = 0.0023021041144753443
916, epoch_train_loss=0.0023021041144753443
Epoch 917
Epoch 917 :: Batch 0/1
Batch Loss = 0.0023018769018928855
917, epoch_train_loss=0.0023018769018928855
Epoch 918
Epoch 918 :: Batch 0/1
Batch Loss = 0.0023016561869536926
918, epoch_train_loss=0.0023016561869536926
Epoch 919
Epoch 919 :: Batch 0/1
Batch Loss = 0.002301444247291891
919, epoch_train_loss=0.002301444247291891
Epoch 920
Epoch 920 :: Batch 0/1
Batch Loss = 0.002301245305677542
920, epoch_train_loss=0.002301245305677542
Epoch 921
Epoch 921 :: Batch 0/1
Batch Loss = 0.0023010644302491074
921, epoch_train_loss=0.0023010644302491074
Epoch 922
Epoch 922 :: Batch 0/1
Batch Loss = 0.0023009109886646243
922, epoch_train_loss=0.0023009109886646243
Epoch 923
Epoch 923 :: Batch 0/1
Batch Loss = 0.0023007967192761073
923, epoch_train_loss=0.0023007967192761073
Epoch 924
Epoch 924 :: Batch 0/1
Batch Loss = 0.0023007435094826827
924, epoch_train_loss=0.0023007435094826827
Epoch 925
Epoch 925 :: Batch 0/1
Batch Loss = 0.002300779323143328
925, epoch_train_loss=0.002300779323143328
Epoch 926
Epoch 926 :: Batch 0/1
Batch Loss = 0.0023009579034972763
926, epoch_train_loss=0.0023009579034972763
Epoch 927
Epoch 927 :: Batch 0/1
Batch Loss = 0.002301347978451767
927, epoch_train_loss=0.002301347978451767
Epoch 928
Epoch 928 :: Batch 0/1
Batch Loss = 0.0023020881951517237
928, epoch_train_loss=0.0023020881951517237
Epoch 929
Epoch 929 :: Batch 0/1
Batch Loss = 0.0023033521357623443
929, epoch_train_loss=0.0023033521357623443
Epoch 930
Epoch 930 :: Batch 0/1
Batch Loss = 0.0023055157614603953
930, epoch_train_loss=0.0023055157614603953
Epoch 931
Epoch 931 :: Batch 0/1
Batch Loss = 0.002309026805763278
931, epoch_train_loss=0.002309026805763278
Epoch 932
Epoch 932 :: Batch 0/1
Batch Loss = 0.002314959066994596
932, epoch_train_loss=0.002314959066994596
Epoch 933
Epoch 933 :: Batch 0/1
Batch Loss = 0.002324475432279194
933, epoch_train_loss=0.002324475432279194
Epoch 934
Epoch 934 :: Batch 0/1
Batch Loss = 0.00234081350088257
934, epoch_train_loss=0.00234081350088257
Epoch 935
Epoch 935 :: Batch 0/1
Batch Loss = 0.0023669181954957133
935, epoch_train_loss=0.0023669181954957133
Epoch 936
Epoch 936 :: Batch 0/1
Batch Loss = 0.0024131115461877573
936, epoch_train_loss=0.0024131115461877573
Epoch 937
Epoch 937 :: Batch 0/1
Batch Loss = 0.002486041540672556
937, epoch_train_loss=0.002486041540672556
Epoch 938
Epoch 938 :: Batch 0/1
Batch Loss = 0.002620402440684554
938, epoch_train_loss=0.002620402440684554
Epoch 939
Epoch 939 :: Batch 0/1
Batch Loss = 0.002824759000127785
939, epoch_train_loss=0.002824759000127785
Epoch 940
Epoch 940 :: Batch 0/1
Batch Loss = 0.0032198082828921453
940, epoch_train_loss=0.0032198082828921453
Epoch 941
Epoch 941 :: Batch 0/1
Batch Loss = 0.00376390306584904
941, epoch_train_loss=0.00376390306584904
Epoch 942
Epoch 942 :: Batch 0/1
Batch Loss = 0.0048668857190747
942, epoch_train_loss=0.0048668857190747
Epoch 943
Epoch 943 :: Batch 0/1
Batch Loss = 0.006027824151277751
943, epoch_train_loss=0.006027824151277751
Epoch 944
Epoch 944 :: Batch 0/1
Batch Loss = 0.008433440465055375
944, epoch_train_loss=0.008433440465055375
Epoch 945
Epoch 945 :: Batch 0/1
Batch Loss = 0.009339858427344909
945, epoch_train_loss=0.009339858427344909
Epoch 946
Epoch 946 :: Batch 0/1
Batch Loss = 0.011333110181904556
946, epoch_train_loss=0.011333110181904556
Epoch 947
Epoch 947 :: Batch 0/1
Batch Loss = 0.008645788698111747
947, epoch_train_loss=0.008645788698111747
Epoch 948
Epoch 948 :: Batch 0/1
Batch Loss = 0.0061629297489358215
948, epoch_train_loss=0.0061629297489358215
Epoch 949
Epoch 949 :: Batch 0/1
Batch Loss = 0.0030876045066946384
949, epoch_train_loss=0.0030876045066946384
Epoch 950
Epoch 950 :: Batch 0/1
Batch Loss = 0.002398913581683584
950, epoch_train_loss=0.002398913581683584
Epoch 951
Epoch 951 :: Batch 0/1
Batch Loss = 0.003865583306167074
951, epoch_train_loss=0.003865583306167074
Epoch 952
Epoch 952 :: Batch 0/1
Batch Loss = 0.005432536081756448
952, epoch_train_loss=0.005432536081756448
Epoch 953
Epoch 953 :: Batch 0/1
Batch Loss = 0.006168415237770794
953, epoch_train_loss=0.006168415237770794
Epoch 954
Epoch 954 :: Batch 0/1
Batch Loss = 0.0043872025411950556
954, epoch_train_loss=0.0043872025411950556
Epoch 955
Epoch 955 :: Batch 0/1
Batch Loss = 0.002749182064580619
955, epoch_train_loss=0.002749182064580619
Epoch 956
Epoch 956 :: Batch 0/1
Batch Loss = 0.0024213926734863713
956, epoch_train_loss=0.0024213926734863713
Epoch 957
Epoch 957 :: Batch 0/1
Batch Loss = 0.0033764821089378306
957, epoch_train_loss=0.0033764821089378306
Epoch 958
Epoch 958 :: Batch 0/1
Batch Loss = 0.004229634578036199
958, epoch_train_loss=0.004229634578036199
Epoch 959
Epoch 959 :: Batch 0/1
Batch Loss = 0.0035935000636118813
959, epoch_train_loss=0.0035935000636118813
Epoch 960
Epoch 960 :: Batch 0/1
Batch Loss = 0.0026435948421848748
960, epoch_train_loss=0.0026435948421848748
Epoch 961
Epoch 961 :: Batch 0/1
Batch Loss = 0.0023486875088272923
961, epoch_train_loss=0.0023486875088272923
Epoch 962
Epoch 962 :: Batch 0/1
Batch Loss = 0.002877018630770295
962, epoch_train_loss=0.002877018630770295
Epoch 963
Epoch 963 :: Batch 0/1
Batch Loss = 0.0034156043251523187
963, epoch_train_loss=0.0034156043251523187
Epoch 964
Epoch 964 :: Batch 0/1
Batch Loss = 0.0031256267250386904
964, epoch_train_loss=0.0031256267250386904
Epoch 965
Epoch 965 :: Batch 0/1
Batch Loss = 0.0025507138612336153
965, epoch_train_loss=0.0025507138612336153
Epoch 966
Epoch 966 :: Batch 0/1
Batch Loss = 0.0023223341893485253
966, epoch_train_loss=0.0023223341893485253
Epoch 967
Epoch 967 :: Batch 0/1
Batch Loss = 0.002619580498350685
967, epoch_train_loss=0.002619580498350685
Epoch 968
Epoch 968 :: Batch 0/1
Batch Loss = 0.0029414028084329474
968, epoch_train_loss=0.0029414028084329474
Epoch 969
Epoch 969 :: Batch 0/1
Batch Loss = 0.0027902711480580085
969, epoch_train_loss=0.0027902711480580085
Epoch 970
Epoch 970 :: Batch 0/1
Batch Loss = 0.0024510807710692003
970, epoch_train_loss=0.0024510807710692003
Epoch 971
Epoch 971 :: Batch 0/1
Batch Loss = 0.002306055946531434
971, epoch_train_loss=0.002306055946531434
Epoch 972
Epoch 972 :: Batch 0/1
Batch Loss = 0.0024690948770180726
972, epoch_train_loss=0.0024690948770180726
Epoch 973
Epoch 973 :: Batch 0/1
Batch Loss = 0.002670074855721177
973, epoch_train_loss=0.002670074855721177
Epoch 974
Epoch 974 :: Batch 0/1
Batch Loss = 0.0026164053255353066
974, epoch_train_loss=0.0026164053255353066
Epoch 975
Epoch 975 :: Batch 0/1
Batch Loss = 0.0024192803253588646
975, epoch_train_loss=0.0024192803253588646
Epoch 976
Epoch 976 :: Batch 0/1
Batch Loss = 0.0023013926120874265
976, epoch_train_loss=0.0023013926120874265
Epoch 977
Epoch 977 :: Batch 0/1
Batch Loss = 0.002373569398098176
977, epoch_train_loss=0.002373569398098176
Epoch 978
Epoch 978 :: Batch 0/1
Batch Loss = 0.0025016972435923652
978, epoch_train_loss=0.0025016972435923652
Epoch 979
Epoch 979 :: Batch 0/1
Batch Loss = 0.0025030844394306018
979, epoch_train_loss=0.0025030844394306018
Epoch 980
Epoch 980 :: Batch 0/1
Batch Loss = 0.0023982797854339996
980, epoch_train_loss=0.0023982797854339996
Epoch 981
Epoch 981 :: Batch 0/1
Batch Loss = 0.0023052760656577737
981, epoch_train_loss=0.0023052760656577737
Epoch 982
Epoch 982 :: Batch 0/1
Batch Loss = 0.002318229163883178
982, epoch_train_loss=0.002318229163883178
Epoch 983
Epoch 983 :: Batch 0/1
Batch Loss = 0.0023931726361554957
983, epoch_train_loss=0.0023931726361554957
Epoch 984
Epoch 984 :: Batch 0/1
Batch Loss = 0.00242337500314226
984, epoch_train_loss=0.00242337500314226
Epoch 985
Epoch 985 :: Batch 0/1
Batch Loss = 0.0023806137909527856
985, epoch_train_loss=0.0023806137909527856
Epoch 986
Epoch 986 :: Batch 0/1
Batch Loss = 0.002313578825308719
986, epoch_train_loss=0.002313578825308719
Epoch 987
Epoch 987 :: Batch 0/1
Batch Loss = 0.0022959158512316094
987, epoch_train_loss=0.0022959158512316094
Epoch 988
Epoch 988 :: Batch 0/1
Batch Loss = 0.002330943676184302
988, epoch_train_loss=0.002330943676184302
Epoch 989
Epoch 989 :: Batch 0/1
Batch Loss = 0.002364555000818802
989, epoch_train_loss=0.002364555000818802
Epoch 990
Epoch 990 :: Batch 0/1
Batch Loss = 0.0023583627702263694
990, epoch_train_loss=0.0023583627702263694
Epoch 991
Epoch 991 :: Batch 0/1
Batch Loss = 0.0023196668313192016
991, epoch_train_loss=0.0023196668313192016
Epoch 992
Epoch 992 :: Batch 0/1
Batch Loss = 0.0022924779826074305
992, epoch_train_loss=0.0022924779826074305
Epoch 993
Epoch 993 :: Batch 0/1
Batch Loss = 0.0022984514427653374
993, epoch_train_loss=0.0022984514427653374
Epoch 994
Epoch 994 :: Batch 0/1
Batch Loss = 0.0023216462813701
994, epoch_train_loss=0.0023216462813701
Epoch 995
Epoch 995 :: Batch 0/1
Batch Loss = 0.0023325203301681583
995, epoch_train_loss=0.0023325203301681583
Epoch 996
Epoch 996 :: Batch 0/1
Batch Loss = 0.0023185622215193013
996, epoch_train_loss=0.0023185622215193013
Epoch 997
Epoch 997 :: Batch 0/1
Batch Loss = 0.0022971628312363573
997, epoch_train_loss=0.0022971628312363573
Epoch 998
Epoch 998 :: Batch 0/1
Batch Loss = 0.002288657742772717
998, epoch_train_loss=0.002288657742772717
Epoch 999
Epoch 999 :: Batch 0/1
Batch Loss = 0.002297222225359541
999, epoch_train_loss=0.002297222225359541
Epoch 1000
Epoch 1000 :: Batch 0/1
Batch Loss = 0.0023094992474815036
1000, epoch_train_loss=0.0023094992474815036
Epoch 1001
Epoch 1001 :: Batch 0/1
Batch Loss = 0.002310887913721421
1001, epoch_train_loss=0.002310887913721421
Epoch 1002
Epoch 1002 :: Batch 0/1
Batch Loss = 0.0023011311047112
1002, epoch_train_loss=0.0023011311047112
Epoch 1003
Epoch 1003 :: Batch 0/1
Batch Loss = 0.0022899667098144265
1003, epoch_train_loss=0.0022899667098144265
Epoch 1004
Epoch 1004 :: Batch 0/1
Batch Loss = 0.0022871245053163332
1004, epoch_train_loss=0.0022871245053163332
Epoch 1005
Epoch 1005 :: Batch 0/1
Batch Loss = 0.002292463690118596
1005, epoch_train_loss=0.002292463690118596
Epoch 1006
Epoch 1006 :: Batch 0/1
Batch Loss = 0.002298327531678201
1006, epoch_train_loss=0.002298327531678201
Epoch 1007
Epoch 1007 :: Batch 0/1
Batch Loss = 0.002298447454400801
1007, epoch_train_loss=0.002298447454400801
Epoch 1008
Epoch 1008 :: Batch 0/1
Batch Loss = 0.0022928079654311413
1008, epoch_train_loss=0.0022928079654311413
Epoch 1009
Epoch 1009 :: Batch 0/1
Batch Loss = 0.0022869267084206176
1009, epoch_train_loss=0.0022869267084206176
Epoch 1010
Epoch 1010 :: Batch 0/1
Batch Loss = 0.002285198566473169
1010, epoch_train_loss=0.002285198566473169
Epoch 1011
Epoch 1011 :: Batch 0/1
Batch Loss = 0.002287621103067383
1011, epoch_train_loss=0.002287621103067383
Epoch 1012
Epoch 1012 :: Batch 0/1
Batch Loss = 0.0022907186762076547
1012, epoch_train_loss=0.0022907186762076547
Epoch 1013
Epoch 1013 :: Batch 0/1
Batch Loss = 0.0022911406312825107
1013, epoch_train_loss=0.0022911406312825107
Epoch 1014
Epoch 1014 :: Batch 0/1
Batch Loss = 0.0022886158882096546
1014, epoch_train_loss=0.0022886158882096546
Epoch 1015
Epoch 1015 :: Batch 0/1
Batch Loss = 0.00228522137680981
1015, epoch_train_loss=0.00228522137680981
Epoch 1016
Epoch 1016 :: Batch 0/1
Batch Loss = 0.0022834403205138757
1016, epoch_train_loss=0.0022834403205138757
Epoch 1017
Epoch 1017 :: Batch 0/1
Batch Loss = 0.0022839433408064565
1017, epoch_train_loss=0.0022839433408064565
Epoch 1018
Epoch 1018 :: Batch 0/1
Batch Loss = 0.002285496451648398
1018, epoch_train_loss=0.002285496451648398
Epoch 1019
Epoch 1019 :: Batch 0/1
Batch Loss = 0.0022863462774465396
1019, epoch_train_loss=0.0022863462774465396
Epoch 1020
Epoch 1020 :: Batch 0/1
Batch Loss = 0.002285612960056523
1020, epoch_train_loss=0.002285612960056523
Epoch 1021
Epoch 1021 :: Batch 0/1
Batch Loss = 0.002283869645102682
1021, epoch_train_loss=0.002283869645102682
Epoch 1022
Epoch 1022 :: Batch 0/1
Batch Loss = 0.0022822855198258215
1022, epoch_train_loss=0.0022822855198258215
Epoch 1023
Epoch 1023 :: Batch 0/1
Batch Loss = 0.0022817079406638316
1023, epoch_train_loss=0.0022817079406638316
Epoch 1024
Epoch 1024 :: Batch 0/1
Batch Loss = 0.002282076277045183
1024, epoch_train_loss=0.002282076277045183
Epoch 1025
Epoch 1025 :: Batch 0/1
Batch Loss = 0.0022826791555562603
1025, epoch_train_loss=0.0022826791555562603
Epoch 1026
Epoch 1026 :: Batch 0/1
Batch Loss = 0.0022828184163725413
1026, epoch_train_loss=0.0022828184163725413
Epoch 1027
Epoch 1027 :: Batch 0/1
Batch Loss = 0.0022822618227919377
1027, epoch_train_loss=0.0022822618227919377
Epoch 1028
Epoch 1028 :: Batch 0/1
Batch Loss = 0.0022813261531933046
1028, epoch_train_loss=0.0022813261531933046
Epoch 1029
Epoch 1029 :: Batch 0/1
Batch Loss = 0.0022804910326731935
1029, epoch_train_loss=0.0022804910326731935
Epoch 1030
Epoch 1030 :: Batch 0/1
Batch Loss = 0.0022800772313844777
1030, epoch_train_loss=0.0022800772313844777
Epoch 1031
Epoch 1031 :: Batch 0/1
Batch Loss = 0.0022800698780178087
1031, epoch_train_loss=0.0022800698780178087
Epoch 1032
Epoch 1032 :: Batch 0/1
Batch Loss = 0.002280211201640291
1032, epoch_train_loss=0.002280211201640291
Epoch 1033
Epoch 1033 :: Batch 0/1
Batch Loss = 0.0022802157602190166
1033, epoch_train_loss=0.0022802157602190166
Epoch 1034
Epoch 1034 :: Batch 0/1
Batch Loss = 0.002279939730808903
1034, epoch_train_loss=0.002279939730808903
Epoch 1035
Epoch 1035 :: Batch 0/1
Batch Loss = 0.00227944997622363
1035, epoch_train_loss=0.00227944997622363
Epoch 1036
Epoch 1036 :: Batch 0/1
Batch Loss = 0.0022789218485909867
1036, epoch_train_loss=0.0022789218485909867
Epoch 1037
Epoch 1037 :: Batch 0/1
Batch Loss = 0.0022785216857633315
1037, epoch_train_loss=0.0022785216857633315
Epoch 1038
Epoch 1038 :: Batch 0/1
Batch Loss = 0.0022783038241122543
1038, epoch_train_loss=0.0022783038241122543
Epoch 1039
Epoch 1039 :: Batch 0/1
Batch Loss = 0.0022782096378337084
1039, epoch_train_loss=0.0022782096378337084
Epoch 1040
Epoch 1040 :: Batch 0/1
Batch Loss = 0.0022781296526374795
1040, epoch_train_loss=0.0022781296526374795
Epoch 1041
Epoch 1041 :: Batch 0/1
Batch Loss = 0.0022779757414903177
1041, epoch_train_loss=0.0022779757414903177
Epoch 1042
Epoch 1042 :: Batch 0/1
Batch Loss = 0.002277724487418641
1042, epoch_train_loss=0.002277724487418641
Epoch 1043
Epoch 1043 :: Batch 0/1
Batch Loss = 0.0022774056748406605
1043, epoch_train_loss=0.0022774056748406605
Epoch 1044
Epoch 1044 :: Batch 0/1
Batch Loss = 0.0022770780430628143
1044, epoch_train_loss=0.0022770780430628143
Epoch 1045
Epoch 1045 :: Batch 0/1
Batch Loss = 0.0022767898615713702
1045, epoch_train_loss=0.0022767898615713702
Epoch 1046
Epoch 1046 :: Batch 0/1
Batch Loss = 0.0022765611883971176
1046, epoch_train_loss=0.0022765611883971176
Epoch 1047
Epoch 1047 :: Batch 0/1
Batch Loss = 0.002276379493976958
1047, epoch_train_loss=0.002276379493976958
Epoch 1048
Epoch 1048 :: Batch 0/1
Batch Loss = 0.0022762138950299937
1048, epoch_train_loss=0.0022762138950299937
Epoch 1049
Epoch 1049 :: Batch 0/1
Batch Loss = 0.002276034918203049
1049, epoch_train_loss=0.002276034918203049
Epoch 1050
Epoch 1050 :: Batch 0/1
Batch Loss = 0.002275827688775657
1050, epoch_train_loss=0.002275827688775657
Epoch 1051
Epoch 1051 :: Batch 0/1
Batch Loss = 0.002275595902362767
1051, epoch_train_loss=0.002275595902362767
Epoch 1052
Epoch 1052 :: Batch 0/1
Batch Loss = 0.002275352312547485
1052, epoch_train_loss=0.002275352312547485
Epoch 1053
Epoch 1053 :: Batch 0/1
Batch Loss = 0.00227511150971175
1053, epoch_train_loss=0.00227511150971175
Epoch 1054
Epoch 1054 :: Batch 0/1
Batch Loss = 0.0022748824027727667
1054, epoch_train_loss=0.0022748824027727667
Epoch 1055
Epoch 1055 :: Batch 0/1
Batch Loss = 0.002274667662934935
1055, epoch_train_loss=0.002274667662934935
Epoch 1056
Epoch 1056 :: Batch 0/1
Batch Loss = 0.0022744639089939114
1056, epoch_train_loss=0.0022744639089939114
Epoch 1057
Epoch 1057 :: Batch 0/1
Batch Loss = 0.002274264789022468
1057, epoch_train_loss=0.002274264789022468
Epoch 1058
Epoch 1058 :: Batch 0/1
Batch Loss = 0.002274064109474455
1058, epoch_train_loss=0.002274064109474455
Epoch 1059
Epoch 1059 :: Batch 0/1
Batch Loss = 0.002273858564048509
1059, epoch_train_loss=0.002273858564048509
Epoch 1060
Epoch 1060 :: Batch 0/1
Batch Loss = 0.002273648717245032
1060, epoch_train_loss=0.002273648717245032
Epoch 1061
Epoch 1061 :: Batch 0/1
Batch Loss = 0.002273436921748828
1061, epoch_train_loss=0.002273436921748828
Epoch 1062
Epoch 1062 :: Batch 0/1
Batch Loss = 0.0022732259833932907
1062, epoch_train_loss=0.0022732259833932907
Epoch 1063
Epoch 1063 :: Batch 0/1
Batch Loss = 0.002273017391681368
1063, epoch_train_loss=0.002273017391681368
Epoch 1064
Epoch 1064 :: Batch 0/1
Batch Loss = 0.0022728117883705373
1064, epoch_train_loss=0.0022728117883705373
Epoch 1065
Epoch 1065 :: Batch 0/1
Batch Loss = 0.0022726086590251276
1065, epoch_train_loss=0.0022726086590251276
Epoch 1066
Epoch 1066 :: Batch 0/1
Batch Loss = 0.002272406924394795
1066, epoch_train_loss=0.002272406924394795
Epoch 1067
Epoch 1067 :: Batch 0/1
Batch Loss = 0.0022722050253816527
1067, epoch_train_loss=0.0022722050253816527
Epoch 1068
Epoch 1068 :: Batch 0/1
Batch Loss = 0.0022720018862160836
1068, epoch_train_loss=0.0022720018862160836
Epoch 1069
Epoch 1069 :: Batch 0/1
Batch Loss = 0.0022717972055184515
1069, epoch_train_loss=0.0022717972055184515
Epoch 1070
Epoch 1070 :: Batch 0/1
Batch Loss = 0.002271591456899383
1070, epoch_train_loss=0.002271591456899383
Epoch 1071
Epoch 1071 :: Batch 0/1
Batch Loss = 0.0022713854535591715
1071, epoch_train_loss=0.0022713854535591715
Epoch 1072
Epoch 1072 :: Batch 0/1
Batch Loss = 0.002271179935075746
1072, epoch_train_loss=0.002271179935075746
Epoch 1073
Epoch 1073 :: Batch 0/1
Batch Loss = 0.0022709756303657107
1073, epoch_train_loss=0.0022709756303657107
Epoch 1074
Epoch 1074 :: Batch 0/1
Batch Loss = 0.002270773042757263
1074, epoch_train_loss=0.002270773042757263
Epoch 1075
Epoch 1075 :: Batch 0/1
Batch Loss = 0.002270572483058502
1075, epoch_train_loss=0.002270572483058502
Epoch 1076
Epoch 1076 :: Batch 0/1
Batch Loss = 0.0022703738158478073
1076, epoch_train_loss=0.0022703738158478073
Epoch 1077
Epoch 1077 :: Batch 0/1
Batch Loss = 0.002270176719299034
1077, epoch_train_loss=0.002270176719299034
Epoch 1078
Epoch 1078 :: Batch 0/1
Batch Loss = 0.0022699807676397703
1078, epoch_train_loss=0.0022699807676397703
Epoch 1079
Epoch 1079 :: Batch 0/1
Batch Loss = 0.002269785668167975
1079, epoch_train_loss=0.002269785668167975
Epoch 1080
Epoch 1080 :: Batch 0/1
Batch Loss = 0.0022695911381278022
1080, epoch_train_loss=0.0022695911381278022
Epoch 1081
Epoch 1081 :: Batch 0/1
Batch Loss = 0.002269396995802544
1081, epoch_train_loss=0.002269396995802544
Epoch 1082
Epoch 1082 :: Batch 0/1
Batch Loss = 0.0022692031134197523
1082, epoch_train_loss=0.0022692031134197523
Epoch 1083
Epoch 1083 :: Batch 0/1
Batch Loss = 0.0022690095884713808
1083, epoch_train_loss=0.0022690095884713808
Epoch 1084
Epoch 1084 :: Batch 0/1
Batch Loss = 0.002268816601245039
1084, epoch_train_loss=0.002268816601245039
Epoch 1085
Epoch 1085 :: Batch 0/1
Batch Loss = 0.0022686244950021056
1085, epoch_train_loss=0.0022686244950021056
Epoch 1086
Epoch 1086 :: Batch 0/1
Batch Loss = 0.0022684336108169333
1086, epoch_train_loss=0.0022684336108169333
Epoch 1087
Epoch 1087 :: Batch 0/1
Batch Loss = 0.002268244550975477
1087, epoch_train_loss=0.002268244550975477
Epoch 1088
Epoch 1088 :: Batch 0/1
Batch Loss = 0.002268058025817609
1088, epoch_train_loss=0.002268058025817609
Epoch 1089
Epoch 1089 :: Batch 0/1
Batch Loss = 0.0022678752286638074
1089, epoch_train_loss=0.0022678752286638074
Epoch 1090
Epoch 1090 :: Batch 0/1
Batch Loss = 0.0022676975897245755
1090, epoch_train_loss=0.0022676975897245755
Epoch 1091
Epoch 1091 :: Batch 0/1
Batch Loss = 0.002267527543152848
1091, epoch_train_loss=0.002267527543152848
Epoch 1092
Epoch 1092 :: Batch 0/1
Batch Loss = 0.0022673682902808164
1092, epoch_train_loss=0.0022673682902808164
Epoch 1093
Epoch 1093 :: Batch 0/1
Batch Loss = 0.0022672253944296455
1093, epoch_train_loss=0.0022672253944296455
Epoch 1094
Epoch 1094 :: Batch 0/1
Batch Loss = 0.0022671064729629787
1094, epoch_train_loss=0.0022671064729629787
Epoch 1095
Epoch 1095 :: Batch 0/1
Batch Loss = 0.0022670248741884684
1095, epoch_train_loss=0.0022670248741884684
Epoch 1096
Epoch 1096 :: Batch 0/1
Batch Loss = 0.002266999431313962
1096, epoch_train_loss=0.002266999431313962
Epoch 1097
Epoch 1097 :: Batch 0/1
Batch Loss = 0.0022670639712615635
1097, epoch_train_loss=0.0022670639712615635
Epoch 1098
Epoch 1098 :: Batch 0/1
Batch Loss = 0.0022672669416062352
1098, epoch_train_loss=0.0022672669416062352
Epoch 1099
Epoch 1099 :: Batch 0/1
Batch Loss = 0.002267698335094638
1099, epoch_train_loss=0.002267698335094638
Epoch 1100
Epoch 1100 :: Batch 0/1
Batch Loss = 0.002268487064341192
1100, epoch_train_loss=0.002268487064341192
Epoch 1101
Epoch 1101 :: Batch 0/1
Batch Loss = 0.0022698844693077877
1101, epoch_train_loss=0.0022698844693077877
Epoch 1102
Epoch 1102 :: Batch 0/1
Batch Loss = 0.0022722441596173685
1102, epoch_train_loss=0.0022722441596173685
Epoch 1103
Epoch 1103 :: Batch 0/1
Batch Loss = 0.002276303149704488
1103, epoch_train_loss=0.002276303149704488
Epoch 1104
Epoch 1104 :: Batch 0/1
Batch Loss = 0.0022830518099032446
1104, epoch_train_loss=0.0022830518099032446
Epoch 1105
Epoch 1105 :: Batch 0/1
Batch Loss = 0.0022947584741124347
1105, epoch_train_loss=0.0022947584741124347
Epoch 1106
Epoch 1106 :: Batch 0/1
Batch Loss = 0.0023141966603172207
1106, epoch_train_loss=0.0023141966603172207
Epoch 1107
Epoch 1107 :: Batch 0/1
Batch Loss = 0.002348669705292758
1107, epoch_train_loss=0.002348669705292758
Epoch 1108
Epoch 1108 :: Batch 0/1
Batch Loss = 0.0024055275575091367
1108, epoch_train_loss=0.0024055275575091367
Epoch 1109
Epoch 1109 :: Batch 0/1
Batch Loss = 0.00250902172306753
1109, epoch_train_loss=0.00250902172306753
Epoch 1110
Epoch 1110 :: Batch 0/1
Batch Loss = 0.0026742034184652624
1110, epoch_train_loss=0.0026742034184652624
Epoch 1111
Epoch 1111 :: Batch 0/1
Batch Loss = 0.0029802418095446442
1111, epoch_train_loss=0.0029802418095446442
Epoch 1112
Epoch 1112 :: Batch 0/1
Batch Loss = 0.0034191722121754337
1112, epoch_train_loss=0.0034191722121754337
Epoch 1113
Epoch 1113 :: Batch 0/1
Batch Loss = 0.004211348520344843
1113, epoch_train_loss=0.004211348520344843
Epoch 1114
Epoch 1114 :: Batch 0/1
Batch Loss = 0.005014264881615676
1114, epoch_train_loss=0.005014264881615676
Epoch 1115
Epoch 1115 :: Batch 0/1
Batch Loss = 0.006203542727619867
1115, epoch_train_loss=0.006203542727619867
Epoch 1116
Epoch 1116 :: Batch 0/1
Batch Loss = 0.0061704248499430585
1116, epoch_train_loss=0.0061704248499430585
Epoch 1117
Epoch 1117 :: Batch 0/1
Batch Loss = 0.005685521435115774
1117, epoch_train_loss=0.005685521435115774
Epoch 1118
Epoch 1118 :: Batch 0/1
Batch Loss = 0.004057545424412312
1118, epoch_train_loss=0.004057545424412312
Epoch 1119
Epoch 1119 :: Batch 0/1
Batch Loss = 0.003726502980380048
1119, epoch_train_loss=0.003726502980380048
Epoch 1120
Epoch 1120 :: Batch 0/1
Batch Loss = 0.005002414538685064
1120, epoch_train_loss=0.005002414538685064
Epoch 1121
Epoch 1121 :: Batch 0/1
Batch Loss = 0.006815628261044516
1121, epoch_train_loss=0.006815628261044516
Epoch 1122
Epoch 1122 :: Batch 0/1
Batch Loss = 0.0072377368360966214
1122, epoch_train_loss=0.0072377368360966214
Epoch 1123
Epoch 1123 :: Batch 0/1
Batch Loss = 0.008126161949872571
1123, epoch_train_loss=0.008126161949872571
Epoch 1124
Epoch 1124 :: Batch 0/1
Batch Loss = 0.008335906704677964
1124, epoch_train_loss=0.008335906704677964
Epoch 1125
Epoch 1125 :: Batch 0/1
Batch Loss = 0.009305129251839045
1125, epoch_train_loss=0.009305129251839045
Epoch 1126
Epoch 1126 :: Batch 0/1
Batch Loss = 0.006451884790679345
1126, epoch_train_loss=0.006451884790679345
Epoch 1127
Epoch 1127 :: Batch 0/1
Batch Loss = 0.004279580929343492
1127, epoch_train_loss=0.004279580929343492
Epoch 1128
Epoch 1128 :: Batch 0/1
Batch Loss = 0.0031122371641523743
1128, epoch_train_loss=0.0031122371641523743
Epoch 1129
Epoch 1129 :: Batch 0/1
Batch Loss = 0.0028494543273745168
1129, epoch_train_loss=0.0028494543273745168
Epoch 1130
Epoch 1130 :: Batch 0/1
Batch Loss = 0.0032959937474442594
1130, epoch_train_loss=0.0032959937474442594
Epoch 1131
Epoch 1131 :: Batch 0/1
Batch Loss = 0.004358839423029898
1131, epoch_train_loss=0.004358839423029898
Epoch 1132
Epoch 1132 :: Batch 0/1
Batch Loss = 0.005238926376639679
1132, epoch_train_loss=0.005238926376639679
Epoch 1133
Epoch 1133 :: Batch 0/1
Batch Loss = 0.004337570479645734
1133, epoch_train_loss=0.004337570479645734
Epoch 1134
Epoch 1134 :: Batch 0/1
Batch Loss = 0.0031812184853793795
1134, epoch_train_loss=0.0031812184853793795
Epoch 1135
Epoch 1135 :: Batch 0/1
Batch Loss = 0.0025967077819513
1135, epoch_train_loss=0.0025967077819513
Epoch 1136
Epoch 1136 :: Batch 0/1
Batch Loss = 0.0027091892078748426
1136, epoch_train_loss=0.0027091892078748426
Epoch 1137
Epoch 1137 :: Batch 0/1
Batch Loss = 0.0031260345332179544
1137, epoch_train_loss=0.0031260345332179544
Epoch 1138
Epoch 1138 :: Batch 0/1
Batch Loss = 0.0035078435142439587
1138, epoch_train_loss=0.0035078435142439587
Epoch 1139
Epoch 1139 :: Batch 0/1
Batch Loss = 0.0035670312275052686
1139, epoch_train_loss=0.0035670312275052686
Epoch 1140
Epoch 1140 :: Batch 0/1
Batch Loss = 0.002887126642497822
1140, epoch_train_loss=0.002887126642497822
Epoch 1141
Epoch 1141 :: Batch 0/1
Batch Loss = 0.00236049602550995
1141, epoch_train_loss=0.00236049602550995
Epoch 1142
Epoch 1142 :: Batch 0/1
Batch Loss = 0.0024264407455139945
1142, epoch_train_loss=0.0024264407455139945
Epoch 1143
Epoch 1143 :: Batch 0/1
Batch Loss = 0.0027489085550464034
1143, epoch_train_loss=0.0027489085550464034
Epoch 1144
Epoch 1144 :: Batch 0/1
Batch Loss = 0.002925756606381281
1144, epoch_train_loss=0.002925756606381281
Epoch 1145
Epoch 1145 :: Batch 0/1
Batch Loss = 0.0028301298105761897
1145, epoch_train_loss=0.0028301298105761897
Epoch 1146
Epoch 1146 :: Batch 0/1
Batch Loss = 0.002628661085402254
1146, epoch_train_loss=0.002628661085402254
Epoch 1147
Epoch 1147 :: Batch 0/1
Batch Loss = 0.0023731579769917426
1147, epoch_train_loss=0.0023731579769917426
Epoch 1148
Epoch 1148 :: Batch 0/1
Batch Loss = 0.0023106151716322256
1148, epoch_train_loss=0.0023106151716322256
Epoch 1149
Epoch 1149 :: Batch 0/1
Batch Loss = 0.0025016693392311633
1149, epoch_train_loss=0.0025016693392311633
Epoch 1150
Epoch 1150 :: Batch 0/1
Batch Loss = 0.002665103527583532
1150, epoch_train_loss=0.002665103527583532
Epoch 1151
Epoch 1151 :: Batch 0/1
Batch Loss = 0.002629031783381464
1151, epoch_train_loss=0.002629031783381464
Epoch 1152
Epoch 1152 :: Batch 0/1
Batch Loss = 0.0024703771741288946
1152, epoch_train_loss=0.0024703771741288946
Epoch 1153
Epoch 1153 :: Batch 0/1
Batch Loss = 0.002362519338926022
1153, epoch_train_loss=0.002362519338926022
Epoch 1154
Epoch 1154 :: Batch 0/1
Batch Loss = 0.0023143578121984395
1154, epoch_train_loss=0.0023143578121984395
Epoch 1155
Epoch 1155 :: Batch 0/1
Batch Loss = 0.002342183305796666
1155, epoch_train_loss=0.002342183305796666
Epoch 1156
Epoch 1156 :: Batch 0/1
Batch Loss = 0.0024365672664260632
1156, epoch_train_loss=0.0024365672664260632
Epoch 1157
Epoch 1157 :: Batch 0/1
Batch Loss = 0.002485525549800487
1157, epoch_train_loss=0.002485525549800487
Epoch 1158
Epoch 1158 :: Batch 0/1
Batch Loss = 0.0024355420559668706
1158, epoch_train_loss=0.0024355420559668706
Epoch 1159
Epoch 1159 :: Batch 0/1
Batch Loss = 0.0023388542718958726
1159, epoch_train_loss=0.0023388542718958726
Epoch 1160
Epoch 1160 :: Batch 0/1
Batch Loss = 0.002293262676575455
1160, epoch_train_loss=0.002293262676575455
Epoch 1161
Epoch 1161 :: Batch 0/1
Batch Loss = 0.002297548847632549
1161, epoch_train_loss=0.002297548847632549
Epoch 1162
Epoch 1162 :: Batch 0/1
Batch Loss = 0.0023219589360373187
1162, epoch_train_loss=0.0023219589360373187
Epoch 1163
Epoch 1163 :: Batch 0/1
Batch Loss = 0.0023568022445147425
1163, epoch_train_loss=0.0023568022445147425
Epoch 1164
Epoch 1164 :: Batch 0/1
Batch Loss = 0.0023722311677130347
1164, epoch_train_loss=0.0023722311677130347
Epoch 1165
Epoch 1165 :: Batch 0/1
Batch Loss = 0.0023459284529086088
1165, epoch_train_loss=0.0023459284529086088
Epoch 1166
Epoch 1166 :: Batch 0/1
Batch Loss = 0.0022967127258085845
1166, epoch_train_loss=0.0022967127258085845
Epoch 1167
Epoch 1167 :: Batch 0/1
Batch Loss = 0.002273034382982519
1167, epoch_train_loss=0.002273034382982519
Epoch 1168
Epoch 1168 :: Batch 0/1
Batch Loss = 0.002281018581343181
1168, epoch_train_loss=0.002281018581343181
Epoch 1169
Epoch 1169 :: Batch 0/1
Batch Loss = 0.002296857737729652
1169, epoch_train_loss=0.002296857737729652
Epoch 1170
Epoch 1170 :: Batch 0/1
Batch Loss = 0.002309175728982602
1170, epoch_train_loss=0.002309175728982602
Epoch 1171
Epoch 1171 :: Batch 0/1
Batch Loss = 0.002314732377635793
1171, epoch_train_loss=0.002314732377635793
Epoch 1172
Epoch 1172 :: Batch 0/1
Batch Loss = 0.00230717758995666
1172, epoch_train_loss=0.00230717758995666
Epoch 1173
Epoch 1173 :: Batch 0/1
Batch Loss = 0.002285128812747775
1173, epoch_train_loss=0.002285128812747775
Epoch 1174
Epoch 1174 :: Batch 0/1
Batch Loss = 0.00226784057651169
1174, epoch_train_loss=0.00226784057651169
Epoch 1175
Epoch 1175 :: Batch 0/1
Batch Loss = 0.0022671488935053423
1175, epoch_train_loss=0.0022671488935053423
Epoch 1176
Epoch 1176 :: Batch 0/1
Batch Loss = 0.002275582838032452
1176, epoch_train_loss=0.002275582838032452
Epoch 1177
Epoch 1177 :: Batch 0/1
Batch Loss = 0.0022826311240543265
1177, epoch_train_loss=0.0022826311240543265
Epoch 1178
Epoch 1178 :: Batch 0/1
Batch Loss = 0.0022861106477298546
1178, epoch_train_loss=0.0022861106477298546
Epoch 1179
Epoch 1179 :: Batch 0/1
Batch Loss = 0.0022859560356982673
1179, epoch_train_loss=0.0022859560356982673
Epoch 1180
Epoch 1180 :: Batch 0/1
Batch Loss = 0.0022788427528102925
1180, epoch_train_loss=0.0022788427528102925
Epoch 1181
Epoch 1181 :: Batch 0/1
Batch Loss = 0.002268487101928125
1181, epoch_train_loss=0.002268487101928125
Epoch 1182
Epoch 1182 :: Batch 0/1
Batch Loss = 0.0022626154355856235
1182, epoch_train_loss=0.0022626154355856235
Epoch 1183
Epoch 1183 :: Batch 0/1
Batch Loss = 0.0022635570005762294
1183, epoch_train_loss=0.0022635570005762294
Epoch 1184
Epoch 1184 :: Batch 0/1
Batch Loss = 0.002266894488025567
1184, epoch_train_loss=0.002266894488025567
Epoch 1185
Epoch 1185 :: Batch 0/1
Batch Loss = 0.0022694588319170077
1185, epoch_train_loss=0.0022694588319170077
Epoch 1186
Epoch 1186 :: Batch 0/1
Batch Loss = 0.0022713872767115783
1186, epoch_train_loss=0.0022713872767115783
Epoch 1187
Epoch 1187 :: Batch 0/1
Batch Loss = 0.0022715830926670327
1187, epoch_train_loss=0.0022715830926670327
Epoch 1188
Epoch 1188 :: Batch 0/1
Batch Loss = 0.0022687105712579972
1188, epoch_train_loss=0.0022687105712579972
Epoch 1189
Epoch 1189 :: Batch 0/1
Batch Loss = 0.0022639662748457005
1189, epoch_train_loss=0.0022639662748457005
Epoch 1190
Epoch 1190 :: Batch 0/1
Batch Loss = 0.002260731328645432
1190, epoch_train_loss=0.002260731328645432
Epoch 1191
Epoch 1191 :: Batch 0/1
Batch Loss = 0.0022599723153879227
1191, epoch_train_loss=0.0022599723153879227
Epoch 1192
Epoch 1192 :: Batch 0/1
Batch Loss = 0.002260353683109968
1192, epoch_train_loss=0.002260353683109968
Epoch 1193
Epoch 1193 :: Batch 0/1
Batch Loss = 0.00226108678673971
1193, epoch_train_loss=0.00226108678673971
Epoch 1194
Epoch 1194 :: Batch 0/1
Batch Loss = 0.002262282324259464
1194, epoch_train_loss=0.002262282324259464
Epoch 1195
Epoch 1195 :: Batch 0/1
Batch Loss = 0.0022633729103511196
1195, epoch_train_loss=0.0022633729103511196
Epoch 1196
Epoch 1196 :: Batch 0/1
Batch Loss = 0.0022631703842073596
1196, epoch_train_loss=0.0022631703842073596
Epoch 1197
Epoch 1197 :: Batch 0/1
Batch Loss = 0.0022617227656227808
1197, epoch_train_loss=0.0022617227656227808
Epoch 1198
Epoch 1198 :: Batch 0/1
Batch Loss = 0.002260049931246157
1198, epoch_train_loss=0.002260049931246157
Epoch 1199
Epoch 1199 :: Batch 0/1
Batch Loss = 0.002258831379931919
1199, epoch_train_loss=0.002258831379931919
Epoch 1200
Epoch 1200 :: Batch 0/1
Batch Loss = 0.0022578870347992656
1200, epoch_train_loss=0.0022578870347992656
Epoch 1201
Epoch 1201 :: Batch 0/1
Batch Loss = 0.002257182538249888
1201, epoch_train_loss=0.002257182538249888
Epoch 1202
Epoch 1202 :: Batch 0/1
Batch Loss = 0.0022570402414760484
1202, epoch_train_loss=0.0022570402414760484
Epoch 1203
Epoch 1203 :: Batch 0/1
Batch Loss = 0.0022574588277384858
1203, epoch_train_loss=0.0022574588277384858
Epoch 1204
Epoch 1204 :: Batch 0/1
Batch Loss = 0.0022579260086566318
1204, epoch_train_loss=0.0022579260086566318
Epoch 1205
Epoch 1205 :: Batch 0/1
Batch Loss = 0.002258047902107535
1205, epoch_train_loss=0.002258047902107535
Epoch 1206
Epoch 1206 :: Batch 0/1
Batch Loss = 0.002257928913386152
1206, epoch_train_loss=0.002257928913386152
Epoch 1207
Epoch 1207 :: Batch 0/1
Batch Loss = 0.0022577168603886448
1207, epoch_train_loss=0.0022577168603886448
Epoch 1208
Epoch 1208 :: Batch 0/1
Batch Loss = 0.002257347722312382
1208, epoch_train_loss=0.002257347722312382
Epoch 1209
Epoch 1209 :: Batch 0/1
Batch Loss = 0.0022567275552309064
1209, epoch_train_loss=0.0022567275552309064
Epoch 1210
Epoch 1210 :: Batch 0/1
Batch Loss = 0.0022560399505184756
1210, epoch_train_loss=0.0022560399505184756
Epoch 1211
Epoch 1211 :: Batch 0/1
Batch Loss = 0.0022555075770695817
1211, epoch_train_loss=0.0022555075770695817
Epoch 1212
Epoch 1212 :: Batch 0/1
Batch Loss = 0.002255149867735147
1212, epoch_train_loss=0.002255149867735147
Epoch 1213
Epoch 1213 :: Batch 0/1
Batch Loss = 0.0022548519329400837
1213, epoch_train_loss=0.0022548519329400837
Epoch 1214
Epoch 1214 :: Batch 0/1
Batch Loss = 0.0022546002394626184
1214, epoch_train_loss=0.0022546002394626184
Epoch 1215
Epoch 1215 :: Batch 0/1
Batch Loss = 0.0022544692120704166
1215, epoch_train_loss=0.0022544692120704166
Epoch 1216
Epoch 1216 :: Batch 0/1
Batch Loss = 0.0022544494130036148
1216, epoch_train_loss=0.0022544494130036148
Epoch 1217
Epoch 1217 :: Batch 0/1
Batch Loss = 0.0022544315086933915
1217, epoch_train_loss=0.0022544315086933915
Epoch 1218
Epoch 1218 :: Batch 0/1
Batch Loss = 0.0022543479725633
1218, epoch_train_loss=0.0022543479725633
Epoch 1219
Epoch 1219 :: Batch 0/1
Batch Loss = 0.00225423543158893
1219, epoch_train_loss=0.00225423543158893
Epoch 1220
Epoch 1220 :: Batch 0/1
Batch Loss = 0.002254129277085261
1220, epoch_train_loss=0.002254129277085261
Epoch 1221
Epoch 1221 :: Batch 0/1
Batch Loss = 0.0022540046692850414
1221, epoch_train_loss=0.0022540046692850414
Epoch 1222
Epoch 1222 :: Batch 0/1
Batch Loss = 0.002253816782167422
1222, epoch_train_loss=0.002253816782167422
Epoch 1223
Epoch 1223 :: Batch 0/1
Batch Loss = 0.002253585616479857
1223, epoch_train_loss=0.002253585616479857
Epoch 1224
Epoch 1224 :: Batch 0/1
Batch Loss = 0.0022533551105149187
1224, epoch_train_loss=0.0022533551105149187
Epoch 1225
Epoch 1225 :: Batch 0/1
Batch Loss = 0.002253141670996649
1225, epoch_train_loss=0.002253141670996649
Epoch 1226
Epoch 1226 :: Batch 0/1
Batch Loss = 0.002252918509074657
1226, epoch_train_loss=0.002252918509074657
Epoch 1227
Epoch 1227 :: Batch 0/1
Batch Loss = 0.002252678414250086
1227, epoch_train_loss=0.002252678414250086
Epoch 1228
Epoch 1228 :: Batch 0/1
Batch Loss = 0.0022524407130862954
1228, epoch_train_loss=0.0022524407130862954
Epoch 1229
Epoch 1229 :: Batch 0/1
Batch Loss = 0.002252227348298742
1229, epoch_train_loss=0.002252227348298742
Epoch 1230
Epoch 1230 :: Batch 0/1
Batch Loss = 0.0022520290063563616
1230, epoch_train_loss=0.0022520290063563616
Epoch 1231
Epoch 1231 :: Batch 0/1
Batch Loss = 0.0022518290985253067
1231, epoch_train_loss=0.0022518290985253067
Epoch 1232
Epoch 1232 :: Batch 0/1
Batch Loss = 0.0022516263224951097
1232, epoch_train_loss=0.0022516263224951097
Epoch 1233
Epoch 1233 :: Batch 0/1
Batch Loss = 0.0022514353928185476
1233, epoch_train_loss=0.0022514353928185476
Epoch 1234
Epoch 1234 :: Batch 0/1
Batch Loss = 0.0022512608674018927
1234, epoch_train_loss=0.0022512608674018927
Epoch 1235
Epoch 1235 :: Batch 0/1
Batch Loss = 0.0022510943413072584
1235, epoch_train_loss=0.0022510943413072584
Epoch 1236
Epoch 1236 :: Batch 0/1
Batch Loss = 0.0022509267674563414
1236, epoch_train_loss=0.0022509267674563414
Epoch 1237
Epoch 1237 :: Batch 0/1
Batch Loss = 0.0022507631693149104
1237, epoch_train_loss=0.0022507631693149104
Epoch 1238
Epoch 1238 :: Batch 0/1
Batch Loss = 0.0022506121769179537
1238, epoch_train_loss=0.0022506121769179537
Epoch 1239
Epoch 1239 :: Batch 0/1
Batch Loss = 0.002250477250278497
1239, epoch_train_loss=0.002250477250278497
Epoch 1240
Epoch 1240 :: Batch 0/1
Batch Loss = 0.0022503541713599726
1240, epoch_train_loss=0.0022503541713599726
Epoch 1241
Epoch 1241 :: Batch 0/1
Batch Loss = 0.0022502451488711312
1241, epoch_train_loss=0.0022502451488711312
Epoch 1242
Epoch 1242 :: Batch 0/1
Batch Loss = 0.002250158887434498
1242, epoch_train_loss=0.002250158887434498
Epoch 1243
Epoch 1243 :: Batch 0/1
Batch Loss = 0.002250111773351926
1243, epoch_train_loss=0.002250111773351926
Epoch 1244
Epoch 1244 :: Batch 0/1
Batch Loss = 0.002250115615022638
1244, epoch_train_loss=0.002250115615022638
Epoch 1245
Epoch 1245 :: Batch 0/1
Batch Loss = 0.002250193092253632
1245, epoch_train_loss=0.002250193092253632
Epoch 1246
Epoch 1246 :: Batch 0/1
Batch Loss = 0.002250371085885765
1246, epoch_train_loss=0.002250371085885765
Epoch 1247
Epoch 1247 :: Batch 0/1
Batch Loss = 0.0022507126555500363
1247, epoch_train_loss=0.0022507126555500363
Epoch 1248
Epoch 1248 :: Batch 0/1
Batch Loss = 0.002251285801084229
1248, epoch_train_loss=0.002251285801084229
Epoch 1249
Epoch 1249 :: Batch 0/1
Batch Loss = 0.002252237069836035
1249, epoch_train_loss=0.002252237069836035
Epoch 1250
Epoch 1250 :: Batch 0/1
Batch Loss = 0.002253719406803965
1250, epoch_train_loss=0.002253719406803965
Epoch 1251
Epoch 1251 :: Batch 0/1
Batch Loss = 0.0022561064718842434
1251, epoch_train_loss=0.0022561064718842434
Epoch 1252
Epoch 1252 :: Batch 0/1
Batch Loss = 0.002259768792892261
1252, epoch_train_loss=0.002259768792892261
Epoch 1253
Epoch 1253 :: Batch 0/1
Batch Loss = 0.0022657110988412725
1253, epoch_train_loss=0.0022657110988412725
Epoch 1254
Epoch 1254 :: Batch 0/1
Batch Loss = 0.0022748037353056407
1254, epoch_train_loss=0.0022748037353056407
Epoch 1255
Epoch 1255 :: Batch 0/1
Batch Loss = 0.0022898792654260916
1255, epoch_train_loss=0.0022898792654260916
Epoch 1256
Epoch 1256 :: Batch 0/1
Batch Loss = 0.002312888630070256
1256, epoch_train_loss=0.002312888630070256
Epoch 1257
Epoch 1257 :: Batch 0/1
Batch Loss = 0.0023522841800448736
1257, epoch_train_loss=0.0023522841800448736
Epoch 1258
Epoch 1258 :: Batch 0/1
Batch Loss = 0.0024117349101873916
1258, epoch_train_loss=0.0024117349101873916
Epoch 1259
Epoch 1259 :: Batch 0/1
Batch Loss = 0.0025178532953016764
1259, epoch_train_loss=0.0025178532953016764
Epoch 1260
Epoch 1260 :: Batch 0/1
Batch Loss = 0.002672805048697934
1260, epoch_train_loss=0.002672805048697934
Epoch 1261
Epoch 1261 :: Batch 0/1
Batch Loss = 0.0029641300570256504
1261, epoch_train_loss=0.0029641300570256504
Epoch 1262
Epoch 1262 :: Batch 0/1
Batch Loss = 0.0033567823977555078
1262, epoch_train_loss=0.0033567823977555078
Epoch 1263
Epoch 1263 :: Batch 0/1
Batch Loss = 0.004141377134444541
1263, epoch_train_loss=0.004141377134444541
Epoch 1264
Epoch 1264 :: Batch 0/1
Batch Loss = 0.005012709101202413
1264, epoch_train_loss=0.005012709101202413
Epoch 1265
Epoch 1265 :: Batch 0/1
Batch Loss = 0.006867115339279683
1265, epoch_train_loss=0.006867115339279683
Epoch 1266
Epoch 1266 :: Batch 0/1
Batch Loss = 0.008051322698065622
1266, epoch_train_loss=0.008051322698065622
Epoch 1267
Epoch 1267 :: Batch 0/1
Batch Loss = 0.010806364104706465
1267, epoch_train_loss=0.010806364104706465
Epoch 1268
Epoch 1268 :: Batch 0/1
Batch Loss = 0.010042758953977821
1268, epoch_train_loss=0.010042758953977821
Epoch 1269
Epoch 1269 :: Batch 0/1
Batch Loss = 0.010056358433014554
1269, epoch_train_loss=0.010056358433014554
Epoch 1270
Epoch 1270 :: Batch 0/1
Batch Loss = 0.006205917109997467
1270, epoch_train_loss=0.006205917109997467
Epoch 1271
Epoch 1271 :: Batch 0/1
Batch Loss = 0.003490611610478366
1271, epoch_train_loss=0.003490611610478366
Epoch 1272
Epoch 1272 :: Batch 0/1
Batch Loss = 0.0022969263882334236
1272, epoch_train_loss=0.0022969263882334236
Epoch 1273
Epoch 1273 :: Batch 0/1
Batch Loss = 0.003156154452708251
1273, epoch_train_loss=0.003156154452708251
Epoch 1274
Epoch 1274 :: Batch 0/1
Batch Loss = 0.00504144184287083
1274, epoch_train_loss=0.00504144184287083
Epoch 1275
Epoch 1275 :: Batch 0/1
Batch Loss = 0.005674763921503007
1275, epoch_train_loss=0.005674763921503007
Epoch 1276
Epoch 1276 :: Batch 0/1
Batch Loss = 0.005384647871471863
1276, epoch_train_loss=0.005384647871471863
Epoch 1277
Epoch 1277 :: Batch 0/1
Batch Loss = 0.003504670319356104
1277, epoch_train_loss=0.003504670319356104
Epoch 1278
Epoch 1278 :: Batch 0/1
Batch Loss = 0.0024090405012894872
1278, epoch_train_loss=0.0024090405012894872
Epoch 1279
Epoch 1279 :: Batch 0/1
Batch Loss = 0.0027095061849813796
1279, epoch_train_loss=0.0027095061849813796
Epoch 1280
Epoch 1280 :: Batch 0/1
Batch Loss = 0.003626817134489108
1280, epoch_train_loss=0.003626817134489108
Epoch 1281
Epoch 1281 :: Batch 0/1
Batch Loss = 0.004139150802680834
1281, epoch_train_loss=0.004139150802680834
Epoch 1282
Epoch 1282 :: Batch 0/1
Batch Loss = 0.0034263477781690506
1282, epoch_train_loss=0.0034263477781690506
Epoch 1283
Epoch 1283 :: Batch 0/1
Batch Loss = 0.0026088013160002503
1283, epoch_train_loss=0.0026088013160002503
Epoch 1284
Epoch 1284 :: Batch 0/1
Batch Loss = 0.0023437782728173624
1284, epoch_train_loss=0.0023437782728173624
Epoch 1285
Epoch 1285 :: Batch 0/1
Batch Loss = 0.0027302722943737968
1285, epoch_train_loss=0.0027302722943737968
Epoch 1286
Epoch 1286 :: Batch 0/1
Batch Loss = 0.003192483149400377
1286, epoch_train_loss=0.003192483149400377
Epoch 1287
Epoch 1287 :: Batch 0/1
Batch Loss = 0.0030543513313847632
1287, epoch_train_loss=0.0030543513313847632
Epoch 1288
Epoch 1288 :: Batch 0/1
Batch Loss = 0.002628463789013822
1288, epoch_train_loss=0.002628463789013822
Epoch 1289
Epoch 1289 :: Batch 0/1
Batch Loss = 0.002342016765708125
1289, epoch_train_loss=0.002342016765708125
Epoch 1290
Epoch 1290 :: Batch 0/1
Batch Loss = 0.002451267652689115
1290, epoch_train_loss=0.002451267652689115
Epoch 1291
Epoch 1291 :: Batch 0/1
Batch Loss = 0.002725561239352317
1291, epoch_train_loss=0.002725561239352317
Epoch 1292
Epoch 1292 :: Batch 0/1
Batch Loss = 0.0027771559199530594
1292, epoch_train_loss=0.0027771559199530594
Epoch 1293
Epoch 1293 :: Batch 0/1
Batch Loss = 0.002614731870521784
1293, epoch_train_loss=0.002614731870521784
Epoch 1294
Epoch 1294 :: Batch 0/1
Batch Loss = 0.002386240243058275
1294, epoch_train_loss=0.002386240243058275
Epoch 1295
Epoch 1295 :: Batch 0/1
Batch Loss = 0.002323255451985513
1295, epoch_train_loss=0.002323255451985513
Epoch 1296
Epoch 1296 :: Batch 0/1
Batch Loss = 0.002422451232997051
1296, epoch_train_loss=0.002422451232997051
Epoch 1297
Epoch 1297 :: Batch 0/1
Batch Loss = 0.002515268056383336
1297, epoch_train_loss=0.002515268056383336
Epoch 1298
Epoch 1298 :: Batch 0/1
Batch Loss = 0.0025089293385834157
1298, epoch_train_loss=0.0025089293385834157
Epoch 1299
Epoch 1299 :: Batch 0/1
Batch Loss = 0.002404232756345822
1299, epoch_train_loss=0.002404232756345822
Epoch 1300
Epoch 1300 :: Batch 0/1
Batch Loss = 0.002318895305843684
1300, epoch_train_loss=0.002318895305843684
Epoch 1301
Epoch 1301 :: Batch 0/1
Batch Loss = 0.002308638484116183
1301, epoch_train_loss=0.002308638484116183
Epoch 1302
Epoch 1302 :: Batch 0/1
Batch Loss = 0.002351340217560054
1302, epoch_train_loss=0.002351340217560054
Epoch 1303
Epoch 1303 :: Batch 0/1
Batch Loss = 0.002393383782353455
1303, epoch_train_loss=0.002393383782353455
Epoch 1304
Epoch 1304 :: Batch 0/1
Batch Loss = 0.0023856666499501972
1304, epoch_train_loss=0.0023856666499501972
Epoch 1305
Epoch 1305 :: Batch 0/1
Batch Loss = 0.0023440911846707908
1305, epoch_train_loss=0.0023440911846707908
Epoch 1306
Epoch 1306 :: Batch 0/1
Batch Loss = 0.0022986496682670787
1306, epoch_train_loss=0.0022986496682670787
Epoch 1307
Epoch 1307 :: Batch 0/1
Batch Loss = 0.0022804310378494804
1307, epoch_train_loss=0.0022804310378494804
Epoch 1308
Epoch 1308 :: Batch 0/1
Batch Loss = 0.002293641497853681
1308, epoch_train_loss=0.002293641497853681
Epoch 1309
Epoch 1309 :: Batch 0/1
Batch Loss = 0.0023163558046511556
1309, epoch_train_loss=0.0023163558046511556
Epoch 1310
Epoch 1310 :: Batch 0/1
Batch Loss = 0.0023269793795526105
1310, epoch_train_loss=0.0023269793795526105
Epoch 1311
Epoch 1311 :: Batch 0/1
Batch Loss = 0.0023121900240821135
1311, epoch_train_loss=0.0023121900240821135
Epoch 1312
Epoch 1312 :: Batch 0/1
Batch Loss = 0.0022841220970780895
1312, epoch_train_loss=0.0022841220970780895
Epoch 1313
Epoch 1313 :: Batch 0/1
Batch Loss = 0.0022624412315515835
1313, epoch_train_loss=0.0022624412315515835
Epoch 1314
Epoch 1314 :: Batch 0/1
Batch Loss = 0.0022598424290527484
1314, epoch_train_loss=0.0022598424290527484
Epoch 1315
Epoch 1315 :: Batch 0/1
Batch Loss = 0.0022730836403531763
1315, epoch_train_loss=0.0022730836403531763
Epoch 1316
Epoch 1316 :: Batch 0/1
Batch Loss = 0.002286267905864793
1316, epoch_train_loss=0.002286267905864793
Epoch 1317
Epoch 1317 :: Batch 0/1
Batch Loss = 0.00228724987740541
1317, epoch_train_loss=0.00228724987740541
Epoch 1318
Epoch 1318 :: Batch 0/1
Batch Loss = 0.002274156486031822
1318, epoch_train_loss=0.002274156486031822
Epoch 1319
Epoch 1319 :: Batch 0/1
Batch Loss = 0.0022571276090897823
1319, epoch_train_loss=0.0022571276090897823
Epoch 1320
Epoch 1320 :: Batch 0/1
Batch Loss = 0.0022480652273773544
1320, epoch_train_loss=0.0022480652273773544
Epoch 1321
Epoch 1321 :: Batch 0/1
Batch Loss = 0.002251116611420225
1321, epoch_train_loss=0.002251116611420225
Epoch 1322
Epoch 1322 :: Batch 0/1
Batch Loss = 0.002260940322672213
1322, epoch_train_loss=0.002260940322672213
Epoch 1323
Epoch 1323 :: Batch 0/1
Batch Loss = 0.002267648422494784
1323, epoch_train_loss=0.002267648422494784
Epoch 1324
Epoch 1324 :: Batch 0/1
Batch Loss = 0.0022655076888158402
1324, epoch_train_loss=0.0022655076888158402
Epoch 1325
Epoch 1325 :: Batch 0/1
Batch Loss = 0.0022562398475271246
1325, epoch_train_loss=0.0022562398475271246
Epoch 1326
Epoch 1326 :: Batch 0/1
Batch Loss = 0.0022468453801529013
1326, epoch_train_loss=0.0022468453801529013
Epoch 1327
Epoch 1327 :: Batch 0/1
Batch Loss = 0.002243374571424747
1327, epoch_train_loss=0.002243374571424747
Epoch 1328
Epoch 1328 :: Batch 0/1
Batch Loss = 0.00224652578578557
1328, epoch_train_loss=0.00224652578578557
Epoch 1329
Epoch 1329 :: Batch 0/1
Batch Loss = 0.0022521860673755225
1329, epoch_train_loss=0.0022521860673755225
Epoch 1330
Epoch 1330 :: Batch 0/1
Batch Loss = 0.002255275449668354
1330, epoch_train_loss=0.002255275449668354
Epoch 1331
Epoch 1331 :: Batch 0/1
Batch Loss = 0.0022536131393623812
1331, epoch_train_loss=0.0022536131393623812
Epoch 1332
Epoch 1332 :: Batch 0/1
Batch Loss = 0.0022486550904067468
1332, epoch_train_loss=0.0022486550904067468
Epoch 1333
Epoch 1333 :: Batch 0/1
Batch Loss = 0.0022437803993997306
1333, epoch_train_loss=0.0022437803993997306
Epoch 1334
Epoch 1334 :: Batch 0/1
Batch Loss = 0.0022416369726912066
1334, epoch_train_loss=0.0022416369726912066
Epoch 1335
Epoch 1335 :: Batch 0/1
Batch Loss = 0.002242644027882164
1335, epoch_train_loss=0.002242644027882164
Epoch 1336
Epoch 1336 :: Batch 0/1
Batch Loss = 0.0022452335166410968
1336, epoch_train_loss=0.0022452335166410968
Epoch 1337
Epoch 1337 :: Batch 0/1
Batch Loss = 0.002247210523211456
1337, epoch_train_loss=0.002247210523211456
Epoch 1338
Epoch 1338 :: Batch 0/1
Batch Loss = 0.0022472049147144024
1338, epoch_train_loss=0.0022472049147144024
Epoch 1339
Epoch 1339 :: Batch 0/1
Batch Loss = 0.002245305912465936
1339, epoch_train_loss=0.002245305912465936
Epoch 1340
Epoch 1340 :: Batch 0/1
Batch Loss = 0.002242723013449429
1340, epoch_train_loss=0.002242723013449429
Epoch 1341
Epoch 1341 :: Batch 0/1
Batch Loss = 0.0022408185595287054
1341, epoch_train_loss=0.0022408185595287054
Epoch 1342
Epoch 1342 :: Batch 0/1
Batch Loss = 0.002240291121649915
1342, epoch_train_loss=0.002240291121649915
Epoch 1343
Epoch 1343 :: Batch 0/1
Batch Loss = 0.0022409346130947863
1343, epoch_train_loss=0.0022409346130947863
Epoch 1344
Epoch 1344 :: Batch 0/1
Batch Loss = 0.0022419893598140833
1344, epoch_train_loss=0.0022419893598140833
Epoch 1345
Epoch 1345 :: Batch 0/1
Batch Loss = 0.00224267539203151
1345, epoch_train_loss=0.00224267539203151
Epoch 1346
Epoch 1346 :: Batch 0/1
Batch Loss = 0.0022425855200878825
1346, epoch_train_loss=0.0022425855200878825
Epoch 1347
Epoch 1347 :: Batch 0/1
Batch Loss = 0.0022417797175378587
1347, epoch_train_loss=0.0022417797175378587
Epoch 1348
Epoch 1348 :: Batch 0/1
Batch Loss = 0.0022406396857168847
1348, epoch_train_loss=0.0022406396857168847
Epoch 1349
Epoch 1349 :: Batch 0/1
Batch Loss = 0.0022396398412509394
1349, epoch_train_loss=0.0022396398412509394
Epoch 1350
Epoch 1350 :: Batch 0/1
Batch Loss = 0.0022390948149835273
1350, epoch_train_loss=0.0022390948149835273
Epoch 1351
Epoch 1351 :: Batch 0/1
Batch Loss = 0.00223904987002981
1351, epoch_train_loss=0.00223904987002981
Epoch 1352
Epoch 1352 :: Batch 0/1
Batch Loss = 0.0022393154905801928
1352, epoch_train_loss=0.0022393154905801928
Epoch 1353
Epoch 1353 :: Batch 0/1
Batch Loss = 0.0022396140058165335
1353, epoch_train_loss=0.0022396140058165335
Epoch 1354
Epoch 1354 :: Batch 0/1
Batch Loss = 0.0022397267256659237
1354, epoch_train_loss=0.0022397267256659237
Epoch 1355
Epoch 1355 :: Batch 0/1
Batch Loss = 0.002239566759755314
1355, epoch_train_loss=0.002239566759755314
Epoch 1356
Epoch 1356 :: Batch 0/1
Batch Loss = 0.002239173589704638
1356, epoch_train_loss=0.002239173589704638
Epoch 1357
Epoch 1357 :: Batch 0/1
Batch Loss = 0.002238662929737611
1357, epoch_train_loss=0.002238662929737611
Epoch 1358
Epoch 1358 :: Batch 0/1
Batch Loss = 0.0022381756353945345
1358, epoch_train_loss=0.0022381756353945345
Epoch 1359
Epoch 1359 :: Batch 0/1
Batch Loss = 0.00223781584222428
1359, epoch_train_loss=0.00223781584222428
Epoch 1360
Epoch 1360 :: Batch 0/1
Batch Loss = 0.0022376223213297357
1360, epoch_train_loss=0.0022376223213297357
Epoch 1361
Epoch 1361 :: Batch 0/1
Batch Loss = 0.0022375635350979596
1361, epoch_train_loss=0.0022375635350979596
Epoch 1362
Epoch 1362 :: Batch 0/1
Batch Loss = 0.0022375700533521755
1362, epoch_train_loss=0.0022375700533521755
Epoch 1363
Epoch 1363 :: Batch 0/1
Batch Loss = 0.002237570950642083
1363, epoch_train_loss=0.002237570950642083
Epoch 1364
Epoch 1364 :: Batch 0/1
Batch Loss = 0.0022375167173367947
1364, epoch_train_loss=0.0022375167173367947
Epoch 1365
Epoch 1365 :: Batch 0/1
Batch Loss = 0.0022373862982134813
1365, epoch_train_loss=0.0022373862982134813
Epoch 1366
Epoch 1366 :: Batch 0/1
Batch Loss = 0.002237183548633503
1366, epoch_train_loss=0.002237183548633503
Epoch 1367
Epoch 1367 :: Batch 0/1
Batch Loss = 0.002236934634948042
1367, epoch_train_loss=0.002236934634948042
Epoch 1368
Epoch 1368 :: Batch 0/1
Batch Loss = 0.0022366737100198014
1368, epoch_train_loss=0.0022366737100198014
Epoch 1369
Epoch 1369 :: Batch 0/1
Batch Loss = 0.0022364323402784287
1369, epoch_train_loss=0.0022364323402784287
Epoch 1370
Epoch 1370 :: Batch 0/1
Batch Loss = 0.002236227449182111
1370, epoch_train_loss=0.002236227449182111
Epoch 1371
Epoch 1371 :: Batch 0/1
Batch Loss = 0.002236063042461596
1371, epoch_train_loss=0.002236063042461596
Epoch 1372
Epoch 1372 :: Batch 0/1
Batch Loss = 0.0022359334846005874
1372, epoch_train_loss=0.0022359334846005874
Epoch 1373
Epoch 1373 :: Batch 0/1
Batch Loss = 0.002235828266596721
1373, epoch_train_loss=0.002235828266596721
Epoch 1374
Epoch 1374 :: Batch 0/1
Batch Loss = 0.0022357337541506045
1374, epoch_train_loss=0.0022357337541506045
Epoch 1375
Epoch 1375 :: Batch 0/1
Batch Loss = 0.002235636507553922
1375, epoch_train_loss=0.002235636507553922
Epoch 1376
Epoch 1376 :: Batch 0/1
Batch Loss = 0.0022355272858961615
1376, epoch_train_loss=0.0022355272858961615
Epoch 1377
Epoch 1377 :: Batch 0/1
Batch Loss = 0.0022354021550366563
1377, epoch_train_loss=0.0022354021550366563
Epoch 1378
Epoch 1378 :: Batch 0/1
Batch Loss = 0.002235262063780521
1378, epoch_train_loss=0.002235262063780521
Epoch 1379
Epoch 1379 :: Batch 0/1
Batch Loss = 0.0022351092199931867
1379, epoch_train_loss=0.0022351092199931867
Epoch 1380
Epoch 1380 :: Batch 0/1
Batch Loss = 0.002234947671898435
1380, epoch_train_loss=0.002234947671898435
Epoch 1381
Epoch 1381 :: Batch 0/1
Batch Loss = 0.0022347816750307036
1381, epoch_train_loss=0.0022347816750307036
Epoch 1382
Epoch 1382 :: Batch 0/1
Batch Loss = 0.002234616269398354
1382, epoch_train_loss=0.002234616269398354
Epoch 1383
Epoch 1383 :: Batch 0/1
Batch Loss = 0.0022344545951479664
1383, epoch_train_loss=0.0022344545951479664
Epoch 1384
Epoch 1384 :: Batch 0/1
Batch Loss = 0.0022342984866186976
1384, epoch_train_loss=0.0022342984866186976
Epoch 1385
Epoch 1385 :: Batch 0/1
Batch Loss = 0.002234148266763514
1385, epoch_train_loss=0.002234148266763514
Epoch 1386
Epoch 1386 :: Batch 0/1
Batch Loss = 0.002234004143525826
1386, epoch_train_loss=0.002234004143525826
Epoch 1387
Epoch 1387 :: Batch 0/1
Batch Loss = 0.0022338656773145836
1387, epoch_train_loss=0.0022338656773145836
Epoch 1388
Epoch 1388 :: Batch 0/1
Batch Loss = 0.0022337318708154806
1388, epoch_train_loss=0.0022337318708154806
Epoch 1389
Epoch 1389 :: Batch 0/1
Batch Loss = 0.002233601382936214
1389, epoch_train_loss=0.002233601382936214
Epoch 1390
Epoch 1390 :: Batch 0/1
Batch Loss = 0.0022334731355372808
1390, epoch_train_loss=0.0022334731355372808
Epoch 1391
Epoch 1391 :: Batch 0/1
Batch Loss = 0.00223334654143935
1391, epoch_train_loss=0.00223334654143935
Epoch 1392
Epoch 1392 :: Batch 0/1
Batch Loss = 0.0022332210648534634
1392, epoch_train_loss=0.0022332210648534634
Epoch 1393
Epoch 1393 :: Batch 0/1
Batch Loss = 0.0022330962859056663
1393, epoch_train_loss=0.0022330962859056663
Epoch 1394
Epoch 1394 :: Batch 0/1
Batch Loss = 0.0022329718531422806
1394, epoch_train_loss=0.0022329718531422806
Epoch 1395
Epoch 1395 :: Batch 0/1
Batch Loss = 0.0022328479899889507
1395, epoch_train_loss=0.0022328479899889507
Epoch 1396
Epoch 1396 :: Batch 0/1
Batch Loss = 0.002232724915261324
1396, epoch_train_loss=0.002232724915261324
Epoch 1397
Epoch 1397 :: Batch 0/1
Batch Loss = 0.002232603160279409
1397, epoch_train_loss=0.002232603160279409
Epoch 1398
Epoch 1398 :: Batch 0/1
Batch Loss = 0.0022324830079614435
1398, epoch_train_loss=0.0022324830079614435
Epoch 1399
Epoch 1399 :: Batch 0/1
Batch Loss = 0.0022323653937589712
1399, epoch_train_loss=0.0022323653937589712
Epoch 1400
Epoch 1400 :: Batch 0/1
Batch Loss = 0.002232251220057982
1400, epoch_train_loss=0.002232251220057982
Epoch 1401
Epoch 1401 :: Batch 0/1
Batch Loss = 0.0022321421620194733
1401, epoch_train_loss=0.0022321421620194733
Epoch 1402
Epoch 1402 :: Batch 0/1
Batch Loss = 0.0022320398540553146
1402, epoch_train_loss=0.0022320398540553146
Epoch 1403
Epoch 1403 :: Batch 0/1
Batch Loss = 0.0022319472502411205
1403, epoch_train_loss=0.0022319472502411205
Epoch 1404
Epoch 1404 :: Batch 0/1
Batch Loss = 0.002231867925831036
1404, epoch_train_loss=0.002231867925831036
Epoch 1405
Epoch 1405 :: Batch 0/1
Batch Loss = 0.0022318079100361395
1405, epoch_train_loss=0.0022318079100361395
Epoch 1406
Epoch 1406 :: Batch 0/1
Batch Loss = 0.002231774953250408
1406, epoch_train_loss=0.002231774953250408
Epoch 1407
Epoch 1407 :: Batch 0/1
Batch Loss = 0.002231781678221746
1407, epoch_train_loss=0.002231781678221746
Epoch 1408
Epoch 1408 :: Batch 0/1
Batch Loss = 0.0022318456207086695
1408, epoch_train_loss=0.0022318456207086695
Epoch 1409
Epoch 1409 :: Batch 0/1
Batch Loss = 0.0022319950405396724
1409, epoch_train_loss=0.0022319950405396724
Epoch 1410
Epoch 1410 :: Batch 0/1
Batch Loss = 0.0022322709851537942
1410, epoch_train_loss=0.0022322709851537942
Epoch 1411
Epoch 1411 :: Batch 0/1
Batch Loss = 0.002232739206652328
1411, epoch_train_loss=0.002232739206652328
Epoch 1412
Epoch 1412 :: Batch 0/1
Batch Loss = 0.0022334990237409423
1412, epoch_train_loss=0.0022334990237409423
Epoch 1413
Epoch 1413 :: Batch 0/1
Batch Loss = 0.002234709951060591
1413, epoch_train_loss=0.002234709951060591
Epoch 1414
Epoch 1414 :: Batch 0/1
Batch Loss = 0.002236621359808131
1414, epoch_train_loss=0.002236621359808131
Epoch 1415
Epoch 1415 :: Batch 0/1
Batch Loss = 0.0022396346388024306
1415, epoch_train_loss=0.0022396346388024306
Epoch 1416
Epoch 1416 :: Batch 0/1
Batch Loss = 0.0022443991752001554
1416, epoch_train_loss=0.0022443991752001554
Epoch 1417
Epoch 1417 :: Batch 0/1
Batch Loss = 0.0022519585947279506
1417, epoch_train_loss=0.0022519585947279506
Epoch 1418
Epoch 1418 :: Batch 0/1
Batch Loss = 0.002264065491360011
1418, epoch_train_loss=0.002264065491360011
Epoch 1419
Epoch 1419 :: Batch 0/1
Batch Loss = 0.0022835024883841627
1419, epoch_train_loss=0.0022835024883841627
Epoch 1420
Epoch 1420 :: Batch 0/1
Batch Loss = 0.00231515040789402
1420, epoch_train_loss=0.00231515040789402
Epoch 1421
Epoch 1421 :: Batch 0/1
Batch Loss = 0.002366498262762405
1421, epoch_train_loss=0.002366498262762405
Epoch 1422
Epoch 1422 :: Batch 0/1
Batch Loss = 0.0024513926581602157
1422, epoch_train_loss=0.0024513926581602157
Epoch 1423
Epoch 1423 :: Batch 0/1
Batch Loss = 0.0025894108969588938
1423, epoch_train_loss=0.0025894108969588938
Epoch 1424
Epoch 1424 :: Batch 0/1
Batch Loss = 0.0028190928202074753
1424, epoch_train_loss=0.0028190928202074753
Epoch 1425
Epoch 1425 :: Batch 0/1
Batch Loss = 0.0031850014933312537
1425, epoch_train_loss=0.0031850014933312537
Epoch 1426
Epoch 1426 :: Batch 0/1
Batch Loss = 0.003780768001097919
1426, epoch_train_loss=0.003780768001097919
Epoch 1427
Epoch 1427 :: Batch 0/1
Batch Loss = 0.004663005394027628
1427, epoch_train_loss=0.004663005394027628
Epoch 1428
Epoch 1428 :: Batch 0/1
Batch Loss = 0.005953483706560116
1428, epoch_train_loss=0.005953483706560116
Epoch 1429
Epoch 1429 :: Batch 0/1
Batch Loss = 0.007564151091457207
1429, epoch_train_loss=0.007564151091457207
Epoch 1430
Epoch 1430 :: Batch 0/1
Batch Loss = 0.009177238053988603
1430, epoch_train_loss=0.009177238053988603
Epoch 1431
Epoch 1431 :: Batch 0/1
Batch Loss = 0.011015852999576557
1431, epoch_train_loss=0.011015852999576557
Epoch 1432
Epoch 1432 :: Batch 0/1
Batch Loss = 0.010989143846396071
1432, epoch_train_loss=0.010989143846396071
Epoch 1433
Epoch 1433 :: Batch 0/1
Batch Loss = 0.01261443596439892
1433, epoch_train_loss=0.01261443596439892
Epoch 1434
Epoch 1434 :: Batch 0/1
Batch Loss = 0.009813202375547234
1434, epoch_train_loss=0.009813202375547234
Epoch 1435
Epoch 1435 :: Batch 0/1
Batch Loss = 0.008106951010215272
1435, epoch_train_loss=0.008106951010215272
Epoch 1436
Epoch 1436 :: Batch 0/1
Batch Loss = 0.004339303032756505
1436, epoch_train_loss=0.004339303032756505
Epoch 1437
Epoch 1437 :: Batch 0/1
Batch Loss = 0.0025471140549057562
1437, epoch_train_loss=0.0025471140549057562
Epoch 1438
Epoch 1438 :: Batch 0/1
Batch Loss = 0.0034499702557296416
1438, epoch_train_loss=0.0034499702557296416
Epoch 1439
Epoch 1439 :: Batch 0/1
Batch Loss = 0.0056142003633163655
1439, epoch_train_loss=0.0056142003633163655
Epoch 1440
Epoch 1440 :: Batch 0/1
Batch Loss = 0.007371122560916687
1440, epoch_train_loss=0.007371122560916687
Epoch 1441
Epoch 1441 :: Batch 0/1
Batch Loss = 0.005625337300532054
1441, epoch_train_loss=0.005625337300532054
Epoch 1442
Epoch 1442 :: Batch 0/1
Batch Loss = 0.00352944383566176
1442, epoch_train_loss=0.00352944383566176
Epoch 1443
Epoch 1443 :: Batch 0/1
Batch Loss = 0.002483154449730982
1443, epoch_train_loss=0.002483154449730982
Epoch 1444
Epoch 1444 :: Batch 0/1
Batch Loss = 0.003297794782616672
1444, epoch_train_loss=0.003297794782616672
Epoch 1445
Epoch 1445 :: Batch 0/1
Batch Loss = 0.004495625362586987
1445, epoch_train_loss=0.004495625362586987
Epoch 1446
Epoch 1446 :: Batch 0/1
Batch Loss = 0.004208465409698875
1446, epoch_train_loss=0.004208465409698875
Epoch 1447
Epoch 1447 :: Batch 0/1
Batch Loss = 0.0033601427350774017
1447, epoch_train_loss=0.0033601427350774017
Epoch 1448
Epoch 1448 :: Batch 0/1
Batch Loss = 0.0026107249288431543
1448, epoch_train_loss=0.0026107249288431543
Epoch 1449
Epoch 1449 :: Batch 0/1
Batch Loss = 0.0026645868790653687
1449, epoch_train_loss=0.0026645868790653687
Epoch 1450
Epoch 1450 :: Batch 0/1
Batch Loss = 0.0031688594021990115
1450, epoch_train_loss=0.0031688594021990115
Epoch 1451
Epoch 1451 :: Batch 0/1
Batch Loss = 0.0033059049603945344
1451, epoch_train_loss=0.0033059049603945344
Epoch 1452
Epoch 1452 :: Batch 0/1
Batch Loss = 0.0030555962128897113
1452, epoch_train_loss=0.0030555962128897113
Epoch 1453
Epoch 1453 :: Batch 0/1
Batch Loss = 0.0026035244251474505
1453, epoch_train_loss=0.0026035244251474505
Epoch 1454
Epoch 1454 :: Batch 0/1
Batch Loss = 0.0024618038856093712
1454, epoch_train_loss=0.0024618038856093712
Epoch 1455
Epoch 1455 :: Batch 0/1
Batch Loss = 0.002673821880088273
1455, epoch_train_loss=0.002673821880088273
Epoch 1456
Epoch 1456 :: Batch 0/1
Batch Loss = 0.002888257984551883
1456, epoch_train_loss=0.002888257984551883
Epoch 1457
Epoch 1457 :: Batch 0/1
Batch Loss = 0.002877885025274568
1457, epoch_train_loss=0.002877885025274568
Epoch 1458
Epoch 1458 :: Batch 0/1
Batch Loss = 0.0025702030251500163
1458, epoch_train_loss=0.0025702030251500163
Epoch 1459
Epoch 1459 :: Batch 0/1
Batch Loss = 0.002320200880907924
1459, epoch_train_loss=0.002320200880907924
Epoch 1460
Epoch 1460 :: Batch 0/1
Batch Loss = 0.0023736912818142913
1460, epoch_train_loss=0.0023736912818142913
Epoch 1461
Epoch 1461 :: Batch 0/1
Batch Loss = 0.0025946139225985153
1461, epoch_train_loss=0.0025946139225985153
Epoch 1462
Epoch 1462 :: Batch 0/1
Batch Loss = 0.0026775148446229313
1462, epoch_train_loss=0.0026775148446229313
Epoch 1463
Epoch 1463 :: Batch 0/1
Batch Loss = 0.0024948820286798965
1463, epoch_train_loss=0.0024948820286798965
Epoch 1464
Epoch 1464 :: Batch 0/1
Batch Loss = 0.002287770195470413
1464, epoch_train_loss=0.002287770195470413
Epoch 1465
Epoch 1465 :: Batch 0/1
Batch Loss = 0.0022750624987194295
1465, epoch_train_loss=0.0022750624987194295
Epoch 1466
Epoch 1466 :: Batch 0/1
Batch Loss = 0.0024140195468460414
1466, epoch_train_loss=0.0024140195468460414
Epoch 1467
Epoch 1467 :: Batch 0/1
Batch Loss = 0.0025018578140627323
1467, epoch_train_loss=0.0025018578140627323
Epoch 1468
Epoch 1468 :: Batch 0/1
Batch Loss = 0.0024351290563468385
1468, epoch_train_loss=0.0024351290563468385
Epoch 1469
Epoch 1469 :: Batch 0/1
Batch Loss = 0.002319656082866658
1469, epoch_train_loss=0.002319656082866658
Epoch 1470
Epoch 1470 :: Batch 0/1
Batch Loss = 0.0022665513657582547
1470, epoch_train_loss=0.0022665513657582547
Epoch 1471
Epoch 1471 :: Batch 0/1
Batch Loss = 0.0022945891284853522
1471, epoch_train_loss=0.0022945891284853522
Epoch 1472
Epoch 1472 :: Batch 0/1
Batch Loss = 0.0023410603289068296
1472, epoch_train_loss=0.0023410603289068296
Epoch 1473
Epoch 1473 :: Batch 0/1
Batch Loss = 0.0023564631139760778
1473, epoch_train_loss=0.0023564631139760778
Epoch 1474
Epoch 1474 :: Batch 0/1
Batch Loss = 0.0023409919385905744
1474, epoch_train_loss=0.0023409919385905744
Epoch 1475
Epoch 1475 :: Batch 0/1
Batch Loss = 0.0023004228864550013
1475, epoch_train_loss=0.0023004228864550013
Epoch 1476
Epoch 1476 :: Batch 0/1
Batch Loss = 0.002261496784873332
1476, epoch_train_loss=0.002261496784873332
Epoch 1477
Epoch 1477 :: Batch 0/1
Batch Loss = 0.002249175120409301
1477, epoch_train_loss=0.002249175120409301
Epoch 1478
Epoch 1478 :: Batch 0/1
Batch Loss = 0.002272423418688985
1478, epoch_train_loss=0.002272423418688985
Epoch 1479
Epoch 1479 :: Batch 0/1
Batch Loss = 0.002304443882933887
1479, epoch_train_loss=0.002304443882933887
Epoch 1480
Epoch 1480 :: Batch 0/1
Batch Loss = 0.0023049586121009122
1480, epoch_train_loss=0.0023049586121009122
Epoch 1481
Epoch 1481 :: Batch 0/1
Batch Loss = 0.0022739030204288594
1481, epoch_train_loss=0.0022739030204288594
Epoch 1482
Epoch 1482 :: Batch 0/1
Batch Loss = 0.002241183881999593
1482, epoch_train_loss=0.002241183881999593
Epoch 1483
Epoch 1483 :: Batch 0/1
Batch Loss = 0.002236141637637674
1483, epoch_train_loss=0.002236141637637674
Epoch 1484
Epoch 1484 :: Batch 0/1
Batch Loss = 0.002253591246371708
1484, epoch_train_loss=0.002253591246371708
Epoch 1485
Epoch 1485 :: Batch 0/1
Batch Loss = 0.0022688448593402694
1485, epoch_train_loss=0.0022688448593402694
Epoch 1486
Epoch 1486 :: Batch 0/1
Batch Loss = 0.002268746541674494
1486, epoch_train_loss=0.002268746541674494
Epoch 1487
Epoch 1487 :: Batch 0/1
Batch Loss = 0.002256683925369063
1487, epoch_train_loss=0.002256683925369063
Epoch 1488
Epoch 1488 :: Batch 0/1
Batch Loss = 0.0022446698521354745
1488, epoch_train_loss=0.0022446698521354745
Epoch 1489
Epoch 1489 :: Batch 0/1
Batch Loss = 0.002237720207066552
1489, epoch_train_loss=0.002237720207066552
Epoch 1490
Epoch 1490 :: Batch 0/1
Batch Loss = 0.0022368010793441633
1490, epoch_train_loss=0.0022368010793441633
Epoch 1491
Epoch 1491 :: Batch 0/1
Batch Loss = 0.0022412441063865717
1491, epoch_train_loss=0.0022412441063865717
Epoch 1492
Epoch 1492 :: Batch 0/1
Batch Loss = 0.002247485790057083
1492, epoch_train_loss=0.002247485790057083
Epoch 1493
Epoch 1493 :: Batch 0/1
Batch Loss = 0.0022499339967735335
1493, epoch_train_loss=0.0022499339967735335
Epoch 1494
Epoch 1494 :: Batch 0/1
Batch Loss = 0.002244806272984596
1494, epoch_train_loss=0.002244806272984596
Epoch 1495
Epoch 1495 :: Batch 0/1
Batch Loss = 0.002235900722188538
1495, epoch_train_loss=0.002235900722188538
Epoch 1496
Epoch 1496 :: Batch 0/1
Batch Loss = 0.0022303255067595337
1496, epoch_train_loss=0.0022303255067595337
Epoch 1497
Epoch 1497 :: Batch 0/1
Batch Loss = 0.002231557843451964
1497, epoch_train_loss=0.002231557843451964
Epoch 1498
Epoch 1498 :: Batch 0/1
Batch Loss = 0.002236429601714551
1498, epoch_train_loss=0.002236429601714551
Epoch 1499
Epoch 1499 :: Batch 0/1
Batch Loss = 0.0022394750919625846
1499, epoch_train_loss=0.0022394750919625846
Epoch 1500
Epoch 1500 :: Batch 0/1
Batch Loss = 0.00223849863821858
1500, epoch_train_loss=0.00223849863821858
Epoch 1501
Epoch 1501 :: Batch 0/1
Batch Loss = 0.0022350763218204955
1501, epoch_train_loss=0.0022350763218204955
Epoch 1502
Epoch 1502 :: Batch 0/1
Batch Loss = 0.0022320006852830556
1502, epoch_train_loss=0.0022320006852830556
Epoch 1503
Epoch 1503 :: Batch 0/1
Batch Loss = 0.002230375255882552
1503, epoch_train_loss=0.002230375255882552
Epoch 1504
Epoch 1504 :: Batch 0/1
Batch Loss = 0.0022300949450978435
1504, epoch_train_loss=0.0022300949450978435
Epoch 1505
Epoch 1505 :: Batch 0/1
Batch Loss = 0.0022307660140850247
1505, epoch_train_loss=0.0022307660140850247
Epoch 1506
Epoch 1506 :: Batch 0/1
Batch Loss = 0.002231925883848027
1506, epoch_train_loss=0.002231925883848027
Epoch 1507
Epoch 1507 :: Batch 0/1
Batch Loss = 0.002232784356213804
1507, epoch_train_loss=0.002232784356213804
Epoch 1508
Epoch 1508 :: Batch 0/1
Batch Loss = 0.0022324134041503928
1508, epoch_train_loss=0.0022324134041503928
Epoch 1509
Epoch 1509 :: Batch 0/1
Batch Loss = 0.00223079764459964
1509, epoch_train_loss=0.00223079764459964
Epoch 1510
Epoch 1510 :: Batch 0/1
Batch Loss = 0.0022288587670622048
1510, epoch_train_loss=0.0022288587670622048
Epoch 1511
Epoch 1511 :: Batch 0/1
Batch Loss = 0.002227795597178743
1511, epoch_train_loss=0.002227795597178743
Epoch 1512
Epoch 1512 :: Batch 0/1
Batch Loss = 0.002227990752438706
1512, epoch_train_loss=0.002227990752438706
Epoch 1513
Epoch 1513 :: Batch 0/1
Batch Loss = 0.0022288455019687558
1513, epoch_train_loss=0.0022288455019687558
Epoch 1514
Epoch 1514 :: Batch 0/1
Batch Loss = 0.002229463860321067
1514, epoch_train_loss=0.002229463860321067
Epoch 1515
Epoch 1515 :: Batch 0/1
Batch Loss = 0.0022293862713814987
1515, epoch_train_loss=0.0022293862713814987
Epoch 1516
Epoch 1516 :: Batch 0/1
Batch Loss = 0.0022287904582445773
1516, epoch_train_loss=0.0022287904582445773
Epoch 1517
Epoch 1517 :: Batch 0/1
Batch Loss = 0.0022280454137482038
1517, epoch_train_loss=0.0022280454137482038
Epoch 1518
Epoch 1518 :: Batch 0/1
Batch Loss = 0.0022274223841537563
1518, epoch_train_loss=0.0022274223841537563
Epoch 1519
Epoch 1519 :: Batch 0/1
Batch Loss = 0.0022269840652807656
1519, epoch_train_loss=0.0022269840652807656
Epoch 1520
Epoch 1520 :: Batch 0/1
Batch Loss = 0.002226744688253486
1520, epoch_train_loss=0.002226744688253486
Epoch 1521
Epoch 1521 :: Batch 0/1
Batch Loss = 0.002226724872422204
1521, epoch_train_loss=0.002226724872422204
Epoch 1522
Epoch 1522 :: Batch 0/1
Batch Loss = 0.0022268749326323964
1522, epoch_train_loss=0.0022268749326323964
Epoch 1523
Epoch 1523 :: Batch 0/1
Batch Loss = 0.002227032474051411
1523, epoch_train_loss=0.002227032474051411
Epoch 1524
Epoch 1524 :: Batch 0/1
Batch Loss = 0.002226994914682679
1524, epoch_train_loss=0.002226994914682679
Epoch 1525
Epoch 1525 :: Batch 0/1
Batch Loss = 0.0022266897805864453
1525, epoch_train_loss=0.0022266897805864453
Epoch 1526
Epoch 1526 :: Batch 0/1
Batch Loss = 0.002226220395506345
1526, epoch_train_loss=0.002226220395506345
Epoch 1527
Epoch 1527 :: Batch 0/1
Batch Loss = 0.0022257865562568912
1527, epoch_train_loss=0.0022257865562568912
Epoch 1528
Epoch 1528 :: Batch 0/1
Batch Loss = 0.0022255186247250127
1528, epoch_train_loss=0.0022255186247250127
Epoch 1529
Epoch 1529 :: Batch 0/1
Batch Loss = 0.0022254133524697477
1529, epoch_train_loss=0.0022254133524697477
Epoch 1530
Epoch 1530 :: Batch 0/1
Batch Loss = 0.002225380871106784
1530, epoch_train_loss=0.002225380871106784
Epoch 1531
Epoch 1531 :: Batch 0/1
Batch Loss = 0.002225342900208887
1531, epoch_train_loss=0.002225342900208887
Epoch 1532
Epoch 1532 :: Batch 0/1
Batch Loss = 0.0022252759035601096
1532, epoch_train_loss=0.0022252759035601096
Epoch 1533
Epoch 1533 :: Batch 0/1
Batch Loss = 0.002225181891301062
1533, epoch_train_loss=0.002225181891301062
Epoch 1534
Epoch 1534 :: Batch 0/1
Batch Loss = 0.0022250564244529927
1534, epoch_train_loss=0.0022250564244529927
Epoch 1535
Epoch 1535 :: Batch 0/1
Batch Loss = 0.002224881024033698
1535, epoch_train_loss=0.002224881024033698
Epoch 1536
Epoch 1536 :: Batch 0/1
Batch Loss = 0.0022246588172648075
1536, epoch_train_loss=0.0022246588172648075
Epoch 1537
Epoch 1537 :: Batch 0/1
Batch Loss = 0.0022244219592712318
1537, epoch_train_loss=0.0022244219592712318
Epoch 1538
Epoch 1538 :: Batch 0/1
Batch Loss = 0.002224216509459172
1538, epoch_train_loss=0.002224216509459172
Epoch 1539
Epoch 1539 :: Batch 0/1
Batch Loss = 0.0022240676971762153
1539, epoch_train_loss=0.0022240676971762153
Epoch 1540
Epoch 1540 :: Batch 0/1
Batch Loss = 0.0022239676311500427
1540, epoch_train_loss=0.0022239676311500427
Epoch 1541
Epoch 1541 :: Batch 0/1
Batch Loss = 0.0022238873054626546
1541, epoch_train_loss=0.0022238873054626546
Epoch 1542
Epoch 1542 :: Batch 0/1
Batch Loss = 0.0022237998871326727
1542, epoch_train_loss=0.0022237998871326727
Epoch 1543
Epoch 1543 :: Batch 0/1
Batch Loss = 0.002223695040018841
1543, epoch_train_loss=0.002223695040018841
Epoch 1544
Epoch 1544 :: Batch 0/1
Batch Loss = 0.0022235751407549487
1544, epoch_train_loss=0.0022235751407549487
Epoch 1545
Epoch 1545 :: Batch 0/1
Batch Loss = 0.0022234472835317644
1545, epoch_train_loss=0.0022234472835317644
Epoch 1546
Epoch 1546 :: Batch 0/1
Batch Loss = 0.002223312664245646
1546, epoch_train_loss=0.002223312664245646
Epoch 1547
Epoch 1547 :: Batch 0/1
Batch Loss = 0.0022231697893936196
1547, epoch_train_loss=0.0022231697893936196
Epoch 1548
Epoch 1548 :: Batch 0/1
Batch Loss = 0.002223018167495711
1548, epoch_train_loss=0.002223018167495711
Epoch 1549
Epoch 1549 :: Batch 0/1
Batch Loss = 0.0022228639327576687
1549, epoch_train_loss=0.0022228639327576687
Epoch 1550
Epoch 1550 :: Batch 0/1
Batch Loss = 0.002222716420145964
1550, epoch_train_loss=0.002222716420145964
Epoch 1551
Epoch 1551 :: Batch 0/1
Batch Loss = 0.002222582420173259
1551, epoch_train_loss=0.002222582420173259
Epoch 1552
Epoch 1552 :: Batch 0/1
Batch Loss = 0.0022224614207260636
1552, epoch_train_loss=0.0022224614207260636
Epoch 1553
Epoch 1553 :: Batch 0/1
Batch Loss = 0.002222347674803591
1553, epoch_train_loss=0.002222347674803591
Epoch 1554
Epoch 1554 :: Batch 0/1
Batch Loss = 0.0022222351085056822
1554, epoch_train_loss=0.0022222351085056822
Epoch 1555
Epoch 1555 :: Batch 0/1
Batch Loss = 0.002222121023332488
1555, epoch_train_loss=0.002222121023332488
Epoch 1556
Epoch 1556 :: Batch 0/1
Batch Loss = 0.0022220060862987124
1556, epoch_train_loss=0.0022220060862987124
Epoch 1557
Epoch 1557 :: Batch 0/1
Batch Loss = 0.002221891355459311
1557, epoch_train_loss=0.002221891355459311
Epoch 1558
Epoch 1558 :: Batch 0/1
Batch Loss = 0.002221777039664916
1558, epoch_train_loss=0.002221777039664916
Epoch 1559
Epoch 1559 :: Batch 0/1
Batch Loss = 0.002221661555527714
1559, epoch_train_loss=0.002221661555527714
Epoch 1560
Epoch 1560 :: Batch 0/1
Batch Loss = 0.002221543561478367
1560, epoch_train_loss=0.002221543561478367
Epoch 1561
Epoch 1561 :: Batch 0/1
Batch Loss = 0.0022214221929413446
1561, epoch_train_loss=0.0022214221929413446
Epoch 1562
Epoch 1562 :: Batch 0/1
Batch Loss = 0.002221298545263264
1562, epoch_train_loss=0.002221298545263264
Epoch 1563
Epoch 1563 :: Batch 0/1
Batch Loss = 0.0022211741650490944
1563, epoch_train_loss=0.0022211741650490944
Epoch 1564
Epoch 1564 :: Batch 0/1
Batch Loss = 0.0022210508896811686
1564, epoch_train_loss=0.0022210508896811686
Epoch 1565
Epoch 1565 :: Batch 0/1
Batch Loss = 0.0022209290103178026
1565, epoch_train_loss=0.0022209290103178026
Epoch 1566
Epoch 1566 :: Batch 0/1
Batch Loss = 0.0022208081639864146
1566, epoch_train_loss=0.0022208081639864146
Epoch 1567
Epoch 1567 :: Batch 0/1
Batch Loss = 0.0022206874024545626
1567, epoch_train_loss=0.0022206874024545626
Epoch 1568
Epoch 1568 :: Batch 0/1
Batch Loss = 0.0022205665853720497
1568, epoch_train_loss=0.0022205665853720497
Epoch 1569
Epoch 1569 :: Batch 0/1
Batch Loss = 0.0022204459523810557
1569, epoch_train_loss=0.0022204459523810557
Epoch 1570
Epoch 1570 :: Batch 0/1
Batch Loss = 0.002220326166070455
1570, epoch_train_loss=0.002220326166070455
Epoch 1571
Epoch 1571 :: Batch 0/1
Batch Loss = 0.002220207467299454
1571, epoch_train_loss=0.002220207467299454
Epoch 1572
Epoch 1572 :: Batch 0/1
Batch Loss = 0.0022200899251583494
1572, epoch_train_loss=0.0022200899251583494
Epoch 1573
Epoch 1573 :: Batch 0/1
Batch Loss = 0.0022199731339379004
1573, epoch_train_loss=0.0022199731339379004
Epoch 1574
Epoch 1574 :: Batch 0/1
Batch Loss = 0.002219856793258941
1574, epoch_train_loss=0.002219856793258941
Epoch 1575
Epoch 1575 :: Batch 0/1
Batch Loss = 0.0022197405593546056
1575, epoch_train_loss=0.0022197405593546056
Epoch 1576
Epoch 1576 :: Batch 0/1
Batch Loss = 0.002219624535365343
1576, epoch_train_loss=0.002219624535365343
Epoch 1577
Epoch 1577 :: Batch 0/1
Batch Loss = 0.002219508889349385
1577, epoch_train_loss=0.002219508889349385
Epoch 1578
Epoch 1578 :: Batch 0/1
Batch Loss = 0.0022193939796785337
1578, epoch_train_loss=0.0022193939796785337
Epoch 1579
Epoch 1579 :: Batch 0/1
Batch Loss = 0.0022192798685448127
1579, epoch_train_loss=0.0022192798685448127
Epoch 1580
Epoch 1580 :: Batch 0/1
Batch Loss = 0.002219166648622027
1580, epoch_train_loss=0.002219166648622027
Epoch 1581
Epoch 1581 :: Batch 0/1
Batch Loss = 0.002219054225095045
1581, epoch_train_loss=0.002219054225095045
Epoch 1582
Epoch 1582 :: Batch 0/1
Batch Loss = 0.00221894280932904
1582, epoch_train_loss=0.00221894280932904
Epoch 1583
Epoch 1583 :: Batch 0/1
Batch Loss = 0.0022188325888032687
1583, epoch_train_loss=0.0022188325888032687
Epoch 1584
Epoch 1584 :: Batch 0/1
Batch Loss = 0.0022187242228788404
1584, epoch_train_loss=0.0022187242228788404
Epoch 1585
Epoch 1585 :: Batch 0/1
Batch Loss = 0.002218618387464668
1585, epoch_train_loss=0.002218618387464668
Epoch 1586
Epoch 1586 :: Batch 0/1
Batch Loss = 0.0022185164651329897
1586, epoch_train_loss=0.0022185164651329897
Epoch 1587
Epoch 1587 :: Batch 0/1
Batch Loss = 0.0022184199444513363
1587, epoch_train_loss=0.0022184199444513363
Epoch 1588
Epoch 1588 :: Batch 0/1
Batch Loss = 0.002218331650030609
1588, epoch_train_loss=0.002218331650030609
Epoch 1589
Epoch 1589 :: Batch 0/1
Batch Loss = 0.0022182550534531934
1589, epoch_train_loss=0.0022182550534531934
Epoch 1590
Epoch 1590 :: Batch 0/1
Batch Loss = 0.002218196734419013
1590, epoch_train_loss=0.002218196734419013
Epoch 1591
Epoch 1591 :: Batch 0/1
Batch Loss = 0.0022181654213408626
1591, epoch_train_loss=0.0022181654213408626
Epoch 1592
Epoch 1592 :: Batch 0/1
Batch Loss = 0.0022181773041295805
1592, epoch_train_loss=0.0022181773041295805
Epoch 1593
Epoch 1593 :: Batch 0/1
Batch Loss = 0.0022182544633179914
1593, epoch_train_loss=0.0022182544633179914
Epoch 1594
Epoch 1594 :: Batch 0/1
Batch Loss = 0.0022184382669913484
1594, epoch_train_loss=0.0022184382669913484
Epoch 1595
Epoch 1595 :: Batch 0/1
Batch Loss = 0.0022187855680212945
1595, epoch_train_loss=0.0022187855680212945
Epoch 1596
Epoch 1596 :: Batch 0/1
Batch Loss = 0.0022194068009168794
1596, epoch_train_loss=0.0022194068009168794
Epoch 1597
Epoch 1597 :: Batch 0/1
Batch Loss = 0.002220451375362022
1597, epoch_train_loss=0.002220451375362022
Epoch 1598
Epoch 1598 :: Batch 0/1
Batch Loss = 0.0022222279227844715
1598, epoch_train_loss=0.0022222279227844715
Epoch 1599
Epoch 1599 :: Batch 0/1
Batch Loss = 0.002225135396236867
1599, epoch_train_loss=0.002225135396236867
Epoch 1600
Epoch 1600 :: Batch 0/1
Batch Loss = 0.002230078408933856
1600, epoch_train_loss=0.002230078408933856
Epoch 1601
Epoch 1601 :: Batch 0/1
Batch Loss = 0.0022381256827586726
1601, epoch_train_loss=0.0022381256827586726
Epoch 1602
Epoch 1602 :: Batch 0/1
Batch Loss = 0.0022520669411187485
1602, epoch_train_loss=0.0022520669411187485
Epoch 1603
Epoch 1603 :: Batch 0/1
Batch Loss = 0.0022746972291577804
1603, epoch_train_loss=0.0022746972291577804
Epoch 1604
Epoch 1604 :: Batch 0/1
Batch Loss = 0.0023151077344569014
1604, epoch_train_loss=0.0023151077344569014
Epoch 1605
Epoch 1605 :: Batch 0/1
Batch Loss = 0.002379940743340927
1605, epoch_train_loss=0.002379940743340927
Epoch 1606
Epoch 1606 :: Batch 0/1
Batch Loss = 0.002500582307402369
1606, epoch_train_loss=0.002500582307402369
Epoch 1607
Epoch 1607 :: Batch 0/1
Batch Loss = 0.00268764602755629
1607, epoch_train_loss=0.00268764602755629
Epoch 1608
Epoch 1608 :: Batch 0/1
Batch Loss = 0.0030551175238811984
1608, epoch_train_loss=0.0030551175238811984
Epoch 1609
Epoch 1609 :: Batch 0/1
Batch Loss = 0.0035786770151872076
1609, epoch_train_loss=0.0035786770151872076
Epoch 1610
Epoch 1610 :: Batch 0/1
Batch Loss = 0.004679014070628262
1610, epoch_train_loss=0.004679014070628262
Epoch 1611
Epoch 1611 :: Batch 0/1
Batch Loss = 0.005946923316744656
1611, epoch_train_loss=0.005946923316744656
Epoch 1612
Epoch 1612 :: Batch 0/1
Batch Loss = 0.00881588860990417
1612, epoch_train_loss=0.00881588860990417
Epoch 1613
Epoch 1613 :: Batch 0/1
Batch Loss = 0.01053950730869568
1613, epoch_train_loss=0.01053950730869568
Epoch 1614
Epoch 1614 :: Batch 0/1
Batch Loss = 0.01492757057733744
1614, epoch_train_loss=0.01492757057733744
Epoch 1615
Epoch 1615 :: Batch 0/1
Batch Loss = 0.013031782052579514
1615, epoch_train_loss=0.013031782052579514
Epoch 1616
Epoch 1616 :: Batch 0/1
Batch Loss = 0.012370847013192824
1616, epoch_train_loss=0.012370847013192824
Epoch 1617
Epoch 1617 :: Batch 0/1
Batch Loss = 0.006856811071428571
1617, epoch_train_loss=0.006856811071428571
Epoch 1618
Epoch 1618 :: Batch 0/1
Batch Loss = 0.0036865906611294828
1618, epoch_train_loss=0.0036865906611294828
Epoch 1619
Epoch 1619 :: Batch 0/1
Batch Loss = 0.003510836377624978
1619, epoch_train_loss=0.003510836377624978
Epoch 1620
Epoch 1620 :: Batch 0/1
Batch Loss = 0.005278661602468898
1620, epoch_train_loss=0.005278661602468898
Epoch 1621
Epoch 1621 :: Batch 0/1
Batch Loss = 0.007463352768295785
1621, epoch_train_loss=0.007463352768295785
Epoch 1622
Epoch 1622 :: Batch 0/1
Batch Loss = 0.0065247465414713
1622, epoch_train_loss=0.0065247465414713
Epoch 1623
Epoch 1623 :: Batch 0/1
Batch Loss = 0.004891582110226128
1623, epoch_train_loss=0.004891582110226128
Epoch 1624
Epoch 1624 :: Batch 0/1
Batch Loss = 0.0032259074158347083
1624, epoch_train_loss=0.0032259074158347083
Epoch 1625
Epoch 1625 :: Batch 0/1
Batch Loss = 0.003476984597783881
1625, epoch_train_loss=0.003476984597783881
Epoch 1626
Epoch 1626 :: Batch 0/1
Batch Loss = 0.004702037550721557
1626, epoch_train_loss=0.004702037550721557
Epoch 1627
Epoch 1627 :: Batch 0/1
Batch Loss = 0.004581736680719568
1627, epoch_train_loss=0.004581736680719568
Epoch 1628
Epoch 1628 :: Batch 0/1
Batch Loss = 0.0037345050033165536
1628, epoch_train_loss=0.0037345050033165536
Epoch 1629
Epoch 1629 :: Batch 0/1
Batch Loss = 0.0028160625948621676
1629, epoch_train_loss=0.0028160625948621676
Epoch 1630
Epoch 1630 :: Batch 0/1
Batch Loss = 0.0029110042007615836
1630, epoch_train_loss=0.0029110042007615836
Epoch 1631
Epoch 1631 :: Batch 0/1
Batch Loss = 0.0035679757581104457
1631, epoch_train_loss=0.0035679757581104457
Epoch 1632
Epoch 1632 :: Batch 0/1
Batch Loss = 0.003545849315193825
1632, epoch_train_loss=0.003545849315193825
Epoch 1633
Epoch 1633 :: Batch 0/1
Batch Loss = 0.002954257025910148
1633, epoch_train_loss=0.002954257025910148
Epoch 1634
Epoch 1634 :: Batch 0/1
Batch Loss = 0.002465656856917017
1634, epoch_train_loss=0.002465656856917017
Epoch 1635
Epoch 1635 :: Batch 0/1
Batch Loss = 0.0027176682477332336
1635, epoch_train_loss=0.0027176682477332336
Epoch 1636
Epoch 1636 :: Batch 0/1
Batch Loss = 0.003208462386359345
1636, epoch_train_loss=0.003208462386359345
Epoch 1637
Epoch 1637 :: Batch 0/1
Batch Loss = 0.003125350262314336
1637, epoch_train_loss=0.003125350262314336
Epoch 1638
Epoch 1638 :: Batch 0/1
Batch Loss = 0.0026589120360844035
1638, epoch_train_loss=0.0026589120360844035
Epoch 1639
Epoch 1639 :: Batch 0/1
Batch Loss = 0.0023365423833098388
1639, epoch_train_loss=0.0023365423833098388
Epoch 1640
Epoch 1640 :: Batch 0/1
Batch Loss = 0.0025068191524460232
1640, epoch_train_loss=0.0025068191524460232
Epoch 1641
Epoch 1641 :: Batch 0/1
Batch Loss = 0.0028178420241244495
1641, epoch_train_loss=0.0028178420241244495
Epoch 1642
Epoch 1642 :: Batch 0/1
Batch Loss = 0.002761377191990281
1642, epoch_train_loss=0.002761377191990281
Epoch 1643
Epoch 1643 :: Batch 0/1
Batch Loss = 0.0024535399745719494
1643, epoch_train_loss=0.0024535399745719494
Epoch 1644
Epoch 1644 :: Batch 0/1
Batch Loss = 0.0022735360509736928
1644, epoch_train_loss=0.0022735360509736928
Epoch 1645
Epoch 1645 :: Batch 0/1
Batch Loss = 0.0023990030421482305
1645, epoch_train_loss=0.0023990030421482305
Epoch 1646
Epoch 1646 :: Batch 0/1
Batch Loss = 0.0025925126475162966
1646, epoch_train_loss=0.0025925126475162966
Epoch 1647
Epoch 1647 :: Batch 0/1
Batch Loss = 0.0025666822421884037
1647, epoch_train_loss=0.0025666822421884037
Epoch 1648
Epoch 1648 :: Batch 0/1
Batch Loss = 0.0023823119728012252
1648, epoch_train_loss=0.0023823119728012252
Epoch 1649
Epoch 1649 :: Batch 0/1
Batch Loss = 0.0022568386450403473
1649, epoch_train_loss=0.0022568386450403473
Epoch 1650
Epoch 1650 :: Batch 0/1
Batch Loss = 0.0023132837183824483
1650, epoch_train_loss=0.0023132837183824483
Epoch 1651
Epoch 1651 :: Batch 0/1
Batch Loss = 0.002430445339672423
1651, epoch_train_loss=0.002430445339672423
Epoch 1652
Epoch 1652 :: Batch 0/1
Batch Loss = 0.0024377275980605015
1652, epoch_train_loss=0.0024377275980605015
Epoch 1653
Epoch 1653 :: Batch 0/1
Batch Loss = 0.0023389211292627107
1653, epoch_train_loss=0.0023389211292627107
Epoch 1654
Epoch 1654 :: Batch 0/1
Batch Loss = 0.0022498188220520412
1654, epoch_train_loss=0.0022498188220520412
Epoch 1655
Epoch 1655 :: Batch 0/1
Batch Loss = 0.0022601887404197388
1655, epoch_train_loss=0.0022601887404197388
Epoch 1656
Epoch 1656 :: Batch 0/1
Batch Loss = 0.002325625733539039
1656, epoch_train_loss=0.002325625733539039
Epoch 1657
Epoch 1657 :: Batch 0/1
Batch Loss = 0.0023514582462489005
1657, epoch_train_loss=0.0023514582462489005
Epoch 1658
Epoch 1658 :: Batch 0/1
Batch Loss = 0.0023119296506127138
1658, epoch_train_loss=0.0023119296506127138
Epoch 1659
Epoch 1659 :: Batch 0/1
Batch Loss = 0.002253336108335929
1659, epoch_train_loss=0.002253336108335929
Epoch 1660
Epoch 1660 :: Batch 0/1
Batch Loss = 0.002239283708037304
1660, epoch_train_loss=0.002239283708037304
Epoch 1661
Epoch 1661 :: Batch 0/1
Batch Loss = 0.002267874734618205
1661, epoch_train_loss=0.002267874734618205
Epoch 1662
Epoch 1662 :: Batch 0/1
Batch Loss = 0.0022926001027076624
1662, epoch_train_loss=0.0022926001027076624
Epoch 1663
Epoch 1663 :: Batch 0/1
Batch Loss = 0.002284270291076511
1663, epoch_train_loss=0.002284270291076511
Epoch 1664
Epoch 1664 :: Batch 0/1
Batch Loss = 0.002253409336811337
1664, epoch_train_loss=0.002253409336811337
Epoch 1665
Epoch 1665 :: Batch 0/1
Batch Loss = 0.002235659051780878
1665, epoch_train_loss=0.002235659051780878
Epoch 1666
Epoch 1666 :: Batch 0/1
Batch Loss = 0.002242460866771257
1666, epoch_train_loss=0.002242460866771257
Epoch 1667
Epoch 1667 :: Batch 0/1
Batch Loss = 0.002256543118182041
1667, epoch_train_loss=0.002256543118182041
Epoch 1668
Epoch 1668 :: Batch 0/1
Batch Loss = 0.0022583114036028033
1668, epoch_train_loss=0.0022583114036028033
Epoch 1669
Epoch 1669 :: Batch 0/1
Batch Loss = 0.0022445946169075055
1669, epoch_train_loss=0.0022445946169075055
Epoch 1670
Epoch 1670 :: Batch 0/1
Batch Loss = 0.002231404129446525
1670, epoch_train_loss=0.002231404129446525
Epoch 1671
Epoch 1671 :: Batch 0/1
Batch Loss = 0.002230664655072183
1671, epoch_train_loss=0.002230664655072183
Epoch 1672
Epoch 1672 :: Batch 0/1
Batch Loss = 0.0022382400375761608
1672, epoch_train_loss=0.0022382400375761608
Epoch 1673
Epoch 1673 :: Batch 0/1
Batch Loss = 0.0022425804414300966
1673, epoch_train_loss=0.0022425804414300966
Epoch 1674
Epoch 1674 :: Batch 0/1
Batch Loss = 0.002237186802103472
1674, epoch_train_loss=0.002237186802103472
Epoch 1675
Epoch 1675 :: Batch 0/1
Batch Loss = 0.0022276911134621017
1675, epoch_train_loss=0.0022276911134621017
Epoch 1676
Epoch 1676 :: Batch 0/1
Batch Loss = 0.002223424402560025
1676, epoch_train_loss=0.002223424402560025
Epoch 1677
Epoch 1677 :: Batch 0/1
Batch Loss = 0.002226607122442163
1677, epoch_train_loss=0.002226607122442163
Epoch 1678
Epoch 1678 :: Batch 0/1
Batch Loss = 0.0022318661145668494
1678, epoch_train_loss=0.0022318661145668494
Epoch 1679
Epoch 1679 :: Batch 0/1
Batch Loss = 0.0022326398545870763
1679, epoch_train_loss=0.0022326398545870763
Epoch 1680
Epoch 1680 :: Batch 0/1
Batch Loss = 0.0022277526235553384
1680, epoch_train_loss=0.0022277526235553384
Epoch 1681
Epoch 1681 :: Batch 0/1
Batch Loss = 0.002221863787397998
1681, epoch_train_loss=0.002221863787397998
Epoch 1682
Epoch 1682 :: Batch 0/1
Batch Loss = 0.0022198751397142437
1682, epoch_train_loss=0.0022198751397142437
Epoch 1683
Epoch 1683 :: Batch 0/1
Batch Loss = 0.0022222892929801056
1683, epoch_train_loss=0.0022222892929801056
Epoch 1684
Epoch 1684 :: Batch 0/1
Batch Loss = 0.0022258371948356333
1684, epoch_train_loss=0.0022258371948356333
Epoch 1685
Epoch 1685 :: Batch 0/1
Batch Loss = 0.0022266457561980414
1685, epoch_train_loss=0.0022266457561980414
Epoch 1686
Epoch 1686 :: Batch 0/1
Batch Loss = 0.002223929829579695
1686, epoch_train_loss=0.002223929829579695
Epoch 1687
Epoch 1687 :: Batch 0/1
Batch Loss = 0.0022201800313069193
1687, epoch_train_loss=0.0022201800313069193
Epoch 1688
Epoch 1688 :: Batch 0/1
Batch Loss = 0.0022181479984967707
1688, epoch_train_loss=0.0022181479984967707
Epoch 1689
Epoch 1689 :: Batch 0/1
Batch Loss = 0.002218916668678959
1689, epoch_train_loss=0.002218916668678959
Epoch 1690
Epoch 1690 :: Batch 0/1
Batch Loss = 0.0022210401513978718
1690, epoch_train_loss=0.0022210401513978718
Epoch 1691
Epoch 1691 :: Batch 0/1
Batch Loss = 0.0022222599701490022
1691, epoch_train_loss=0.0022222599701490022
Epoch 1692
Epoch 1692 :: Batch 0/1
Batch Loss = 0.002221581799959939
1692, epoch_train_loss=0.002221581799959939
Epoch 1693
Epoch 1693 :: Batch 0/1
Batch Loss = 0.0022195596568425125
1693, epoch_train_loss=0.0022195596568425125
Epoch 1694
Epoch 1694 :: Batch 0/1
Batch Loss = 0.0022177675508319567
1694, epoch_train_loss=0.0022177675508319567
Epoch 1695
Epoch 1695 :: Batch 0/1
Batch Loss = 0.0022172445392684903
1695, epoch_train_loss=0.0022172445392684903
Epoch 1696
Epoch 1696 :: Batch 0/1
Batch Loss = 0.0022178557521950262
1696, epoch_train_loss=0.0022178557521950262
Epoch 1697
Epoch 1697 :: Batch 0/1
Batch Loss = 0.002218760312422366
1697, epoch_train_loss=0.002218760312422366
Epoch 1698
Epoch 1698 :: Batch 0/1
Batch Loss = 0.002219067471710674
1698, epoch_train_loss=0.002219067471710674
Epoch 1699
Epoch 1699 :: Batch 0/1
Batch Loss = 0.002218546876673052
1699, epoch_train_loss=0.002218546876673052
Epoch 1700
Epoch 1700 :: Batch 0/1
Batch Loss = 0.0022175868213593465
1700, epoch_train_loss=0.0022175868213593465
Epoch 1701
Epoch 1701 :: Batch 0/1
Batch Loss = 0.0022167637017822595
1701, epoch_train_loss=0.0022167637017822595
Epoch 1702
Epoch 1702 :: Batch 0/1
Batch Loss = 0.0022164659566728754
1702, epoch_train_loss=0.0022164659566728754
Epoch 1703
Epoch 1703 :: Batch 0/1
Batch Loss = 0.0022166261123244967
1703, epoch_train_loss=0.0022166261123244967
Epoch 1704
Epoch 1704 :: Batch 0/1
Batch Loss = 0.0022168866490493494
1704, epoch_train_loss=0.0022168866490493494
Epoch 1705
Epoch 1705 :: Batch 0/1
Batch Loss = 0.0022169328079372722
1705, epoch_train_loss=0.0022169328079372722
Epoch 1706
Epoch 1706 :: Batch 0/1
Batch Loss = 0.0022166602973723655
1706, epoch_train_loss=0.0022166602973723655
Epoch 1707
Epoch 1707 :: Batch 0/1
Batch Loss = 0.0022162252886113935
1707, epoch_train_loss=0.0022162252886113935
Epoch 1708
Epoch 1708 :: Batch 0/1
Batch Loss = 0.0022158564245731566
1708, epoch_train_loss=0.0022158564245731566
Epoch 1709
Epoch 1709 :: Batch 0/1
Batch Loss = 0.002215661540325881
1709, epoch_train_loss=0.002215661540325881
Epoch 1710
Epoch 1710 :: Batch 0/1
Batch Loss = 0.0022156185795807674
1710, epoch_train_loss=0.0022156185795807674
Epoch 1711
Epoch 1711 :: Batch 0/1
Batch Loss = 0.0022156037634058763
1711, epoch_train_loss=0.0022156037634058763
Epoch 1712
Epoch 1712 :: Batch 0/1
Batch Loss = 0.0022155145308829457
1712, epoch_train_loss=0.0022155145308829457
Epoch 1713
Epoch 1713 :: Batch 0/1
Batch Loss = 0.002215334257048413
1713, epoch_train_loss=0.002215334257048413
Epoch 1714
Epoch 1714 :: Batch 0/1
Batch Loss = 0.0022151022961754826
1714, epoch_train_loss=0.0022151022961754826
Epoch 1715
Epoch 1715 :: Batch 0/1
Batch Loss = 0.002214898569465901
1715, epoch_train_loss=0.002214898569465901
Epoch 1716
Epoch 1716 :: Batch 0/1
Batch Loss = 0.002214766396452622
1716, epoch_train_loss=0.002214766396452622
Epoch 1717
Epoch 1717 :: Batch 0/1
Batch Loss = 0.0022146925623665985
1717, epoch_train_loss=0.0022146925623665985
Epoch 1718
Epoch 1718 :: Batch 0/1
Batch Loss = 0.0022146320920748506
1718, epoch_train_loss=0.0022146320920748506
Epoch 1719
Epoch 1719 :: Batch 0/1
Batch Loss = 0.0022145298863519443
1719, epoch_train_loss=0.0022145298863519443
Epoch 1720
Epoch 1720 :: Batch 0/1
Batch Loss = 0.002214373993173184
1720, epoch_train_loss=0.002214373993173184
Epoch 1721
Epoch 1721 :: Batch 0/1
Batch Loss = 0.002214187760521836
1721, epoch_train_loss=0.002214187760521836
Epoch 1722
Epoch 1722 :: Batch 0/1
Batch Loss = 0.002214008428728042
1722, epoch_train_loss=0.002214008428728042
Epoch 1723
Epoch 1723 :: Batch 0/1
Batch Loss = 0.002213868259843851
1723, epoch_train_loss=0.002213868259843851
Epoch 1724
Epoch 1724 :: Batch 0/1
Batch Loss = 0.0022137708343727357
1724, epoch_train_loss=0.0022137708343727357
Epoch 1725
Epoch 1725 :: Batch 0/1
Batch Loss = 0.0022136998335867447
1725, epoch_train_loss=0.0022136998335867447
Epoch 1726
Epoch 1726 :: Batch 0/1
Batch Loss = 0.002213628808477509
1726, epoch_train_loss=0.002213628808477509
Epoch 1727
Epoch 1727 :: Batch 0/1
Batch Loss = 0.0022135328938781723
1727, epoch_train_loss=0.0022135328938781723
Epoch 1728
Epoch 1728 :: Batch 0/1
Batch Loss = 0.002213405392741373
1728, epoch_train_loss=0.002213405392741373
Epoch 1729
Epoch 1729 :: Batch 0/1
Batch Loss = 0.002213255871941755
1729, epoch_train_loss=0.002213255871941755
Epoch 1730
Epoch 1730 :: Batch 0/1
Batch Loss = 0.0022131026909767058
1730, epoch_train_loss=0.0022131026909767058
Epoch 1731
Epoch 1731 :: Batch 0/1
Batch Loss = 0.002212964329576771
1731, epoch_train_loss=0.002212964329576771
Epoch 1732
Epoch 1732 :: Batch 0/1
Batch Loss = 0.002212846944783214
1732, epoch_train_loss=0.002212846944783214
Epoch 1733
Epoch 1733 :: Batch 0/1
Batch Loss = 0.0022127486286968264
1733, epoch_train_loss=0.0022127486286968264
Epoch 1734
Epoch 1734 :: Batch 0/1
Batch Loss = 0.0022126602530224446
1734, epoch_train_loss=0.0022126602530224446
Epoch 1735
Epoch 1735 :: Batch 0/1
Batch Loss = 0.00221257039434016
1735, epoch_train_loss=0.00221257039434016
Epoch 1736
Epoch 1736 :: Batch 0/1
Batch Loss = 0.002212471917853492
1736, epoch_train_loss=0.002212471917853492
Epoch 1737
Epoch 1737 :: Batch 0/1
Batch Loss = 0.002212361298615418
1737, epoch_train_loss=0.002212361298615418
Epoch 1738
Epoch 1738 :: Batch 0/1
Batch Loss = 0.0022122419742329
1738, epoch_train_loss=0.0022122419742329
Epoch 1739
Epoch 1739 :: Batch 0/1
Batch Loss = 0.002212119817262538
1739, epoch_train_loss=0.002212119817262538
Epoch 1740
Epoch 1740 :: Batch 0/1
Batch Loss = 0.002212000091562233
1740, epoch_train_loss=0.002212000091562233
Epoch 1741
Epoch 1741 :: Batch 0/1
Batch Loss = 0.0022118861991271927
1741, epoch_train_loss=0.0022118861991271927
Epoch 1742
Epoch 1742 :: Batch 0/1
Batch Loss = 0.002211778323135938
1742, epoch_train_loss=0.002211778323135938
Epoch 1743
Epoch 1743 :: Batch 0/1
Batch Loss = 0.00221167489930268
1743, epoch_train_loss=0.00221167489930268
Epoch 1744
Epoch 1744 :: Batch 0/1
Batch Loss = 0.0022115735084520926
1744, epoch_train_loss=0.0022115735084520926
Epoch 1745
Epoch 1745 :: Batch 0/1
Batch Loss = 0.002211471231754035
1745, epoch_train_loss=0.002211471231754035
Epoch 1746
Epoch 1746 :: Batch 0/1
Batch Loss = 0.002211366650995314
1746, epoch_train_loss=0.002211366650995314
Epoch 1747
Epoch 1747 :: Batch 0/1
Batch Loss = 0.0022112595680171417
1747, epoch_train_loss=0.0022112595680171417
Epoch 1748
Epoch 1748 :: Batch 0/1
Batch Loss = 0.0022111508257855396
1748, epoch_train_loss=0.0022111508257855396
Epoch 1749
Epoch 1749 :: Batch 0/1
Batch Loss = 0.0022110419705578302
1749, epoch_train_loss=0.0022110419705578302
Epoch 1750
Epoch 1750 :: Batch 0/1
Batch Loss = 0.0022109338658301138
1750, epoch_train_loss=0.0022109338658301138
Epoch 1751
Epoch 1751 :: Batch 0/1
Batch Loss = 0.0022108274588907495
1751, epoch_train_loss=0.0022108274588907495
Epoch 1752
Epoch 1752 :: Batch 0/1
Batch Loss = 0.0022107229378944297
1752, epoch_train_loss=0.0022107229378944297
Epoch 1753
Epoch 1753 :: Batch 0/1
Batch Loss = 0.0022106200188807285
1753, epoch_train_loss=0.0022106200188807285
Epoch 1754
Epoch 1754 :: Batch 0/1
Batch Loss = 0.002210518119003785
1754, epoch_train_loss=0.002210518119003785
Epoch 1755
Epoch 1755 :: Batch 0/1
Batch Loss = 0.002210416418082504
1755, epoch_train_loss=0.002210416418082504
Epoch 1756
Epoch 1756 :: Batch 0/1
Batch Loss = 0.0022103144468729402
1756, epoch_train_loss=0.0022103144468729402
Epoch 1757
Epoch 1757 :: Batch 0/1
Batch Loss = 0.0022102119405280805
1757, epoch_train_loss=0.0022102119405280805
Epoch 1758
Epoch 1758 :: Batch 0/1
Batch Loss = 0.0022101087741598618
1758, epoch_train_loss=0.0022101087741598618
Epoch 1759
Epoch 1759 :: Batch 0/1
Batch Loss = 0.0022100051095581643
1759, epoch_train_loss=0.0022100051095581643
Epoch 1760
Epoch 1760 :: Batch 0/1
Batch Loss = 0.002209901144623281
1760, epoch_train_loss=0.002209901144623281
Epoch 1761
Epoch 1761 :: Batch 0/1
Batch Loss = 0.0022097972369756547
1761, epoch_train_loss=0.0022097972369756547
Epoch 1762
Epoch 1762 :: Batch 0/1
Batch Loss = 0.0022096936678632864
1762, epoch_train_loss=0.0022096936678632864
Epoch 1763
Epoch 1763 :: Batch 0/1
Batch Loss = 0.002209590586319659
1763, epoch_train_loss=0.002209590586319659
Epoch 1764
Epoch 1764 :: Batch 0/1
Batch Loss = 0.002209488113378654
1764, epoch_train_loss=0.002209488113378654
Epoch 1765
Epoch 1765 :: Batch 0/1
Batch Loss = 0.002209386310073171
1765, epoch_train_loss=0.002209386310073171
Epoch 1766
Epoch 1766 :: Batch 0/1
Batch Loss = 0.0022092851619849034
1766, epoch_train_loss=0.0022092851619849034
Epoch 1767
Epoch 1767 :: Batch 0/1
Batch Loss = 0.0022091847044505143
1767, epoch_train_loss=0.0022091847044505143
Epoch 1768
Epoch 1768 :: Batch 0/1
Batch Loss = 0.002209084830996262
1768, epoch_train_loss=0.002209084830996262
Epoch 1769
Epoch 1769 :: Batch 0/1
Batch Loss = 0.0022089856724237214
1769, epoch_train_loss=0.0022089856724237214
Epoch 1770
Epoch 1770 :: Batch 0/1
Batch Loss = 0.002208887243732271
1770, epoch_train_loss=0.002208887243732271
Epoch 1771
Epoch 1771 :: Batch 0/1
Batch Loss = 0.002208789827286735
1771, epoch_train_loss=0.002208789827286735
Epoch 1772
Epoch 1772 :: Batch 0/1
Batch Loss = 0.002208693624929928
1772, epoch_train_loss=0.002208693624929928
Epoch 1773
Epoch 1773 :: Batch 0/1
Batch Loss = 0.002208599212163528
1773, epoch_train_loss=0.002208599212163528
Epoch 1774
Epoch 1774 :: Batch 0/1
Batch Loss = 0.0022085071805196727
1774, epoch_train_loss=0.0022085071805196727
Epoch 1775
Epoch 1775 :: Batch 0/1
Batch Loss = 0.002208418738245091
1775, epoch_train_loss=0.002208418738245091
Epoch 1776
Epoch 1776 :: Batch 0/1
Batch Loss = 0.002208335240384595
1776, epoch_train_loss=0.002208335240384595
Epoch 1777
Epoch 1777 :: Batch 0/1
Batch Loss = 0.0022082592472630917
1777, epoch_train_loss=0.0022082592472630917
Epoch 1778
Epoch 1778 :: Batch 0/1
Batch Loss = 0.0022081939603988848
1778, epoch_train_loss=0.0022081939603988848
Epoch 1779
Epoch 1779 :: Batch 0/1
Batch Loss = 0.0022081451025793685
1779, epoch_train_loss=0.0022081451025793685
Epoch 1780
Epoch 1780 :: Batch 0/1
Batch Loss = 0.0022081203581823376
1780, epoch_train_loss=0.0022081203581823376
Epoch 1781
Epoch 1781 :: Batch 0/1
Batch Loss = 0.0022081331268712376
1781, epoch_train_loss=0.0022081331268712376
Epoch 1782
Epoch 1782 :: Batch 0/1
Batch Loss = 0.0022082022717249358
1782, epoch_train_loss=0.0022082022717249358
Epoch 1783
Epoch 1783 :: Batch 0/1
Batch Loss = 0.002208360695315728
1783, epoch_train_loss=0.002208360695315728
Epoch 1784
Epoch 1784 :: Batch 0/1
Batch Loss = 0.0022086558831131767
1784, epoch_train_loss=0.0022086558831131767
Epoch 1785
Epoch 1785 :: Batch 0/1
Batch Loss = 0.002209172008215675
1785, epoch_train_loss=0.002209172008215675
Epoch 1786
Epoch 1786 :: Batch 0/1
Batch Loss = 0.002210031839503121
1786, epoch_train_loss=0.002210031839503121
Epoch 1787
Epoch 1787 :: Batch 0/1
Batch Loss = 0.0022114593966912083
1787, epoch_train_loss=0.0022114593966912083
Epoch 1788
Epoch 1788 :: Batch 0/1
Batch Loss = 0.002213780303575207
1788, epoch_train_loss=0.002213780303575207
Epoch 1789
Epoch 1789 :: Batch 0/1
Batch Loss = 0.0022176145607840513
1789, epoch_train_loss=0.0022176145607840513
Epoch 1790
Epoch 1790 :: Batch 0/1
Batch Loss = 0.00222384503515827
1790, epoch_train_loss=0.00222384503515827
Epoch 1791
Epoch 1791 :: Batch 0/1
Batch Loss = 0.002234256618569519
1791, epoch_train_loss=0.002234256618569519
Epoch 1792
Epoch 1792 :: Batch 0/1
Batch Loss = 0.002251277005589098
1792, epoch_train_loss=0.002251277005589098
Epoch 1793
Epoch 1793 :: Batch 0/1
Batch Loss = 0.002280252430142389
1793, epoch_train_loss=0.002280252430142389
Epoch 1794
Epoch 1794 :: Batch 0/1
Batch Loss = 0.002327842164988428
1794, epoch_train_loss=0.002327842164988428
Epoch 1795
Epoch 1795 :: Batch 0/1
Batch Loss = 0.002410680913078749
1795, epoch_train_loss=0.002410680913078749
Epoch 1796
Epoch 1796 :: Batch 0/1
Batch Loss = 0.002546228466334358
1796, epoch_train_loss=0.002546228466334358
Epoch 1797
Epoch 1797 :: Batch 0/1
Batch Loss = 0.0027877366134156454
1797, epoch_train_loss=0.0027877366134156454
Epoch 1798
Epoch 1798 :: Batch 0/1
Batch Loss = 0.0031731210592678676
1798, epoch_train_loss=0.0031731210592678676
Epoch 1799
Epoch 1799 :: Batch 0/1
Batch Loss = 0.0038724104891815735
1799, epoch_train_loss=0.0038724104891815735
Epoch 1800
Epoch 1800 :: Batch 0/1
Batch Loss = 0.004902592716586772
1800, epoch_train_loss=0.004902592716586772
Epoch 1801
Epoch 1801 :: Batch 0/1
Batch Loss = 0.006764816437578153
1801, epoch_train_loss=0.006764816437578153
Epoch 1802
Epoch 1802 :: Batch 0/1
Batch Loss = 0.008921187738232944
1802, epoch_train_loss=0.008921187738232944
Epoch 1803
Epoch 1803 :: Batch 0/1
Batch Loss = 0.012619936557487627
1803, epoch_train_loss=0.012619936557487627
Epoch 1804
Epoch 1804 :: Batch 0/1
Batch Loss = 0.014200595618164325
1804, epoch_train_loss=0.014200595618164325
Epoch 1805
Epoch 1805 :: Batch 0/1
Batch Loss = 0.01703242285528862
1805, epoch_train_loss=0.01703242285528862
Epoch 1806
Epoch 1806 :: Batch 0/1
Batch Loss = 0.012492623923948402
1806, epoch_train_loss=0.012492623923948402
Epoch 1807
Epoch 1807 :: Batch 0/1
Batch Loss = 0.009171215613372224
1807, epoch_train_loss=0.009171215613372224
Epoch 1808
Epoch 1808 :: Batch 0/1
Batch Loss = 0.004203398600771007
1808, epoch_train_loss=0.004203398600771007
Epoch 1809
Epoch 1809 :: Batch 0/1
Batch Loss = 0.0028900237847863087
1809, epoch_train_loss=0.0028900237847863087
Epoch 1810
Epoch 1810 :: Batch 0/1
Batch Loss = 0.004593700023999304
1810, epoch_train_loss=0.004593700023999304
Epoch 1811
Epoch 1811 :: Batch 0/1
Batch Loss = 0.006787063371116983
1811, epoch_train_loss=0.006787063371116983
Epoch 1812
Epoch 1812 :: Batch 0/1
Batch Loss = 0.008424560946181277
1812, epoch_train_loss=0.008424560946181277
Epoch 1813
Epoch 1813 :: Batch 0/1
Batch Loss = 0.006050996553571519
1813, epoch_train_loss=0.006050996553571519
Epoch 1814
Epoch 1814 :: Batch 0/1
Batch Loss = 0.003673112366801833
1814, epoch_train_loss=0.003673112366801833
Epoch 1815
Epoch 1815 :: Batch 0/1
Batch Loss = 0.0027112189059331983
1815, epoch_train_loss=0.0027112189059331983
Epoch 1816
Epoch 1816 :: Batch 0/1
Batch Loss = 0.003939477596289783
1816, epoch_train_loss=0.003939477596289783
Epoch 1817
Epoch 1817 :: Batch 0/1
Batch Loss = 0.00549543552725259
1817, epoch_train_loss=0.00549543552725259
Epoch 1818
Epoch 1818 :: Batch 0/1
Batch Loss = 0.004699673012467585
1818, epoch_train_loss=0.004699673012467585
Epoch 1819
Epoch 1819 :: Batch 0/1
Batch Loss = 0.003263761621968267
1819, epoch_train_loss=0.003263761621968267
Epoch 1820
Epoch 1820 :: Batch 0/1
Batch Loss = 0.002432061710983355
1820, epoch_train_loss=0.002432061710983355
Epoch 1821
Epoch 1821 :: Batch 0/1
Batch Loss = 0.003001218511764917
1821, epoch_train_loss=0.003001218511764917
Epoch 1822
Epoch 1822 :: Batch 0/1
Batch Loss = 0.003933471976768382
1822, epoch_train_loss=0.003933471976768382
Epoch 1823
Epoch 1823 :: Batch 0/1
Batch Loss = 0.003619556273800071
1823, epoch_train_loss=0.003619556273800071
Epoch 1824
Epoch 1824 :: Batch 0/1
Batch Loss = 0.0027486840630670186
1824, epoch_train_loss=0.0027486840630670186
Epoch 1825
Epoch 1825 :: Batch 0/1
Batch Loss = 0.0023300527499652743
1825, epoch_train_loss=0.0023300527499652743
Epoch 1826
Epoch 1826 :: Batch 0/1
Batch Loss = 0.0027974374650706684
1826, epoch_train_loss=0.0027974374650706684
Epoch 1827
Epoch 1827 :: Batch 0/1
Batch Loss = 0.0033796689685752254
1827, epoch_train_loss=0.0033796689685752254
Epoch 1828
Epoch 1828 :: Batch 0/1
Batch Loss = 0.0031633831429132317
1828, epoch_train_loss=0.0031633831429132317
Epoch 1829
Epoch 1829 :: Batch 0/1
Batch Loss = 0.002590356441352463
1829, epoch_train_loss=0.002590356441352463
Epoch 1830
Epoch 1830 :: Batch 0/1
Batch Loss = 0.002277558381562805
1830, epoch_train_loss=0.002277558381562805
Epoch 1831
Epoch 1831 :: Batch 0/1
Batch Loss = 0.0025292793962491578
1831, epoch_train_loss=0.0025292793962491578
Epoch 1832
Epoch 1832 :: Batch 0/1
Batch Loss = 0.00288082507552831
1832, epoch_train_loss=0.00288082507552831
Epoch 1833
Epoch 1833 :: Batch 0/1
Batch Loss = 0.0027674086088606865
1833, epoch_train_loss=0.0027674086088606865
Epoch 1834
Epoch 1834 :: Batch 0/1
Batch Loss = 0.0024257664563172867
1834, epoch_train_loss=0.0024257664563172867
Epoch 1835
Epoch 1835 :: Batch 0/1
Batch Loss = 0.002253969561435206
1835, epoch_train_loss=0.002253969561435206
Epoch 1836
Epoch 1836 :: Batch 0/1
Batch Loss = 0.00240853783655864
1836, epoch_train_loss=0.00240853783655864
Epoch 1837
Epoch 1837 :: Batch 0/1
Batch Loss = 0.002620618288681115
1837, epoch_train_loss=0.002620618288681115
Epoch 1838
Epoch 1838 :: Batch 0/1
Batch Loss = 0.002573316983500391
1838, epoch_train_loss=0.002573316983500391
Epoch 1839
Epoch 1839 :: Batch 0/1
Batch Loss = 0.0023732941784320796
1839, epoch_train_loss=0.0023732941784320796
Epoch 1840
Epoch 1840 :: Batch 0/1
Batch Loss = 0.0022429195536660643
1840, epoch_train_loss=0.0022429195536660643
Epoch 1841
Epoch 1841 :: Batch 0/1
Batch Loss = 0.002308134338470625
1841, epoch_train_loss=0.002308134338470625
Epoch 1842
Epoch 1842 :: Batch 0/1
Batch Loss = 0.0024364153471021248
1842, epoch_train_loss=0.0024364153471021248
Epoch 1843
Epoch 1843 :: Batch 0/1
Batch Loss = 0.0024398433437072984
1843, epoch_train_loss=0.0024398433437072984
Epoch 1844
Epoch 1844 :: Batch 0/1
Batch Loss = 0.0023389778682080965
1844, epoch_train_loss=0.0023389778682080965
Epoch 1845
Epoch 1845 :: Batch 0/1
Batch Loss = 0.002242624007965227
1845, epoch_train_loss=0.002242624007965227
Epoch 1846
Epoch 1846 :: Batch 0/1
Batch Loss = 0.0022450580060101224
1846, epoch_train_loss=0.0022450580060101224
Epoch 1847
Epoch 1847 :: Batch 0/1
Batch Loss = 0.0023120631454863747
1847, epoch_train_loss=0.0023120631454863747
Epoch 1848
Epoch 1848 :: Batch 0/1
Batch Loss = 0.0023469514803647284
1848, epoch_train_loss=0.0023469514803647284
Epoch 1849
Epoch 1849 :: Batch 0/1
Batch Loss = 0.002318702358888177
1849, epoch_train_loss=0.002318702358888177
Epoch 1850
Epoch 1850 :: Batch 0/1
Batch Loss = 0.0022563985922987666
1850, epoch_train_loss=0.0022563985922987666
Epoch 1851
Epoch 1851 :: Batch 0/1
Batch Loss = 0.002224937839557428
1851, epoch_train_loss=0.002224937839557428
Epoch 1852
Epoch 1852 :: Batch 0/1
Batch Loss = 0.002241233682341446
1852, epoch_train_loss=0.002241233682341446
Epoch 1853
Epoch 1853 :: Batch 0/1
Batch Loss = 0.002274066851306878
1853, epoch_train_loss=0.002274066851306878
Epoch 1854
Epoch 1854 :: Batch 0/1
Batch Loss = 0.0022881888395170896
1854, epoch_train_loss=0.0022881888395170896
Epoch 1855
Epoch 1855 :: Batch 0/1
Batch Loss = 0.0022689412034015873
1855, epoch_train_loss=0.0022689412034015873
Epoch 1856
Epoch 1856 :: Batch 0/1
Batch Loss = 0.0022383100380820157
1856, epoch_train_loss=0.0022383100380820157
Epoch 1857
Epoch 1857 :: Batch 0/1
Batch Loss = 0.002220554558772152
1857, epoch_train_loss=0.002220554558772152
Epoch 1858
Epoch 1858 :: Batch 0/1
Batch Loss = 0.002225228742186801
1858, epoch_train_loss=0.002225228742186801
Epoch 1859
Epoch 1859 :: Batch 0/1
Batch Loss = 0.0022418947483561803
1859, epoch_train_loss=0.0022418947483561803
Epoch 1860
Epoch 1860 :: Batch 0/1
Batch Loss = 0.0022519036084696817
1860, epoch_train_loss=0.0022519036084696817
Epoch 1861
Epoch 1861 :: Batch 0/1
Batch Loss = 0.0022470256387117633
1861, epoch_train_loss=0.0022470256387117633
Epoch 1862
Epoch 1862 :: Batch 0/1
Batch Loss = 0.0022311843196123355
1862, epoch_train_loss=0.0022311843196123355
Epoch 1863
Epoch 1863 :: Batch 0/1
Batch Loss = 0.002217441355949174
1863, epoch_train_loss=0.002217441355949174
Epoch 1864
Epoch 1864 :: Batch 0/1
Batch Loss = 0.0022147895484707593
1864, epoch_train_loss=0.0022147895484707593
Epoch 1865
Epoch 1865 :: Batch 0/1
Batch Loss = 0.002222317678497469
1865, epoch_train_loss=0.002222317678497469
Epoch 1866
Epoch 1866 :: Batch 0/1
Batch Loss = 0.002231119249264396
1866, epoch_train_loss=0.002231119249264396
Epoch 1867
Epoch 1867 :: Batch 0/1
Batch Loss = 0.002232416391765524
1867, epoch_train_loss=0.002232416391765524
Epoch 1868
Epoch 1868 :: Batch 0/1
Batch Loss = 0.002225406811090379
1868, epoch_train_loss=0.002225406811090379
Epoch 1869
Epoch 1869 :: Batch 0/1
Batch Loss = 0.0022158277146090435
1869, epoch_train_loss=0.0022158277146090435
Epoch 1870
Epoch 1870 :: Batch 0/1
Batch Loss = 0.0022110823532788515
1870, epoch_train_loss=0.0022110823532788515
Epoch 1871
Epoch 1871 :: Batch 0/1
Batch Loss = 0.0022133076398729535
1871, epoch_train_loss=0.0022133076398729535
Epoch 1872
Epoch 1872 :: Batch 0/1
Batch Loss = 0.002218777775606739
1872, epoch_train_loss=0.002218777775606739
Epoch 1873
Epoch 1873 :: Batch 0/1
Batch Loss = 0.002222014583586485
1873, epoch_train_loss=0.002222014583586485
Epoch 1874
Epoch 1874 :: Batch 0/1
Batch Loss = 0.0022202050210523476
1874, epoch_train_loss=0.0022202050210523476
Epoch 1875
Epoch 1875 :: Batch 0/1
Batch Loss = 0.0022152982767888
1875, epoch_train_loss=0.0022152982767888
Epoch 1876
Epoch 1876 :: Batch 0/1
Batch Loss = 0.0022110758823445493
1876, epoch_train_loss=0.0022110758823445493
Epoch 1877
Epoch 1877 :: Batch 0/1
Batch Loss = 0.002210135821422317
1877, epoch_train_loss=0.002210135821422317
Epoch 1878
Epoch 1878 :: Batch 0/1
Batch Loss = 0.0022121421453610117
1878, epoch_train_loss=0.0022121421453610117
Epoch 1879
Epoch 1879 :: Batch 0/1
Batch Loss = 0.0022146586057630695
1879, epoch_train_loss=0.0022146586057630695
Epoch 1880
Epoch 1880 :: Batch 0/1
Batch Loss = 0.002215481115767852
1880, epoch_train_loss=0.002215481115767852
Epoch 1881
Epoch 1881 :: Batch 0/1
Batch Loss = 0.0022140402282627594
1881, epoch_train_loss=0.0022140402282627594
Epoch 1882
Epoch 1882 :: Batch 0/1
Batch Loss = 0.0022115549773377647
1882, epoch_train_loss=0.0022115549773377647
Epoch 1883
Epoch 1883 :: Batch 0/1
Batch Loss = 0.0022096954758762926
1883, epoch_train_loss=0.0022096954758762926
Epoch 1884
Epoch 1884 :: Batch 0/1
Batch Loss = 0.0022093513608226474
1884, epoch_train_loss=0.0022093513608226474
Epoch 1885
Epoch 1885 :: Batch 0/1
Batch Loss = 0.0022101947881197517
1885, epoch_train_loss=0.0022101947881197517
Epoch 1886
Epoch 1886 :: Batch 0/1
Batch Loss = 0.0022111976919740027
1886, epoch_train_loss=0.0022111976919740027
Epoch 1887
Epoch 1887 :: Batch 0/1
Batch Loss = 0.0022114852757134937
1887, epoch_train_loss=0.0022114852757134937
Epoch 1888
Epoch 1888 :: Batch 0/1
Batch Loss = 0.002210864023000643
1888, epoch_train_loss=0.002210864023000643
Epoch 1889
Epoch 1889 :: Batch 0/1
Batch Loss = 0.0022097683274949864
1889, epoch_train_loss=0.0022097683274949864
Epoch 1890
Epoch 1890 :: Batch 0/1
Batch Loss = 0.002208789782490896
1890, epoch_train_loss=0.002208789782490896
Epoch 1891
Epoch 1891 :: Batch 0/1
Batch Loss = 0.002208338681300433
1891, epoch_train_loss=0.002208338681300433
Epoch 1892
Epoch 1892 :: Batch 0/1
Batch Loss = 0.002208425166165105
1892, epoch_train_loss=0.002208425166165105
Epoch 1893
Epoch 1893 :: Batch 0/1
Batch Loss = 0.002208778213450147
1893, epoch_train_loss=0.002208778213450147
Epoch 1894
Epoch 1894 :: Batch 0/1
Batch Loss = 0.002209032014769994
1894, epoch_train_loss=0.002209032014769994
Epoch 1895
Epoch 1895 :: Batch 0/1
Batch Loss = 0.0022089430671553803
1895, epoch_train_loss=0.0022089430671553803
Epoch 1896
Epoch 1896 :: Batch 0/1
Batch Loss = 0.0022085263838071256
1896, epoch_train_loss=0.0022085263838071256
Epoch 1897
Epoch 1897 :: Batch 0/1
Batch Loss = 0.002207959343072994
1897, epoch_train_loss=0.002207959343072994
Epoch 1898
Epoch 1898 :: Batch 0/1
Batch Loss = 0.002207486263492187
1898, epoch_train_loss=0.002207486263492187
Epoch 1899
Epoch 1899 :: Batch 0/1
Batch Loss = 0.002207246545998464
1899, epoch_train_loss=0.002207246545998464
Epoch 1900
Epoch 1900 :: Batch 0/1
Batch Loss = 0.0022072347487867208
1900, epoch_train_loss=0.0022072347487867208
Epoch 1901
Epoch 1901 :: Batch 0/1
Batch Loss = 0.002207338473378436
1901, epoch_train_loss=0.002207338473378436
Epoch 1902
Epoch 1902 :: Batch 0/1
Batch Loss = 0.0022074032612895114
1902, epoch_train_loss=0.0022074032612895114
Epoch 1903
Epoch 1903 :: Batch 0/1
Batch Loss = 0.002207326657940224
1903, epoch_train_loss=0.002207326657940224
Epoch 1904
Epoch 1904 :: Batch 0/1
Batch Loss = 0.002207089549962408
1904, epoch_train_loss=0.002207089549962408
Epoch 1905
Epoch 1905 :: Batch 0/1
Batch Loss = 0.002206765321757952
1905, epoch_train_loss=0.002206765321757952
Epoch 1906
Epoch 1906 :: Batch 0/1
Batch Loss = 0.0022064614452234443
1906, epoch_train_loss=0.0022064614452234443
Epoch 1907
Epoch 1907 :: Batch 0/1
Batch Loss = 0.0022062580169875962
1907, epoch_train_loss=0.0022062580169875962
Epoch 1908
Epoch 1908 :: Batch 0/1
Batch Loss = 0.0022061698253929424
1908, epoch_train_loss=0.0022061698253929424
Epoch 1909
Epoch 1909 :: Batch 0/1
Batch Loss = 0.0022061536054802764
1909, epoch_train_loss=0.0022061536054802764
Epoch 1910
Epoch 1910 :: Batch 0/1
Batch Loss = 0.0022061412889618675
1910, epoch_train_loss=0.0022061412889618675
Epoch 1911
Epoch 1911 :: Batch 0/1
Batch Loss = 0.0022060788858891327
1911, epoch_train_loss=0.0022060788858891327
Epoch 1912
Epoch 1912 :: Batch 0/1
Batch Loss = 0.0022059474242941664
1912, epoch_train_loss=0.0022059474242941664
Epoch 1913
Epoch 1913 :: Batch 0/1
Batch Loss = 0.002205761305022591
1913, epoch_train_loss=0.002205761305022591
Epoch 1914
Epoch 1914 :: Batch 0/1
Batch Loss = 0.0022055605293392236
1914, epoch_train_loss=0.0022055605293392236
Epoch 1915
Epoch 1915 :: Batch 0/1
Batch Loss = 0.002205381784586621
1915, epoch_train_loss=0.002205381784586621
Epoch 1916
Epoch 1916 :: Batch 0/1
Batch Loss = 0.0022052462997049103
1916, epoch_train_loss=0.0022052462997049103
Epoch 1917
Epoch 1917 :: Batch 0/1
Batch Loss = 0.002205150627700343
1917, epoch_train_loss=0.002205150627700343
Epoch 1918
Epoch 1918 :: Batch 0/1
Batch Loss = 0.0022050760390540766
1918, epoch_train_loss=0.0022050760390540766
Epoch 1919
Epoch 1919 :: Batch 0/1
Batch Loss = 0.0022050020140014968
1919, epoch_train_loss=0.0022050020140014968
Epoch 1920
Epoch 1920 :: Batch 0/1
Batch Loss = 0.002204912714409805
1920, epoch_train_loss=0.002204912714409805
Epoch 1921
Epoch 1921 :: Batch 0/1
Batch Loss = 0.002204803451501796
1921, epoch_train_loss=0.002204803451501796
Epoch 1922
Epoch 1922 :: Batch 0/1
Batch Loss = 0.0022046765775054536
1922, epoch_train_loss=0.0022046765775054536
Epoch 1923
Epoch 1923 :: Batch 0/1
Batch Loss = 0.0022045410472524
1923, epoch_train_loss=0.0022045410472524
Epoch 1924
Epoch 1924 :: Batch 0/1
Batch Loss = 0.0022044065499848063
1924, epoch_train_loss=0.0022044065499848063
Epoch 1925
Epoch 1925 :: Batch 0/1
Batch Loss = 0.002204280000290226
1925, epoch_train_loss=0.002204280000290226
Epoch 1926
Epoch 1926 :: Batch 0/1
Batch Loss = 0.002204163949956339
1926, epoch_train_loss=0.002204163949956339
Epoch 1927
Epoch 1927 :: Batch 0/1
Batch Loss = 0.0022040574344543607
1927, epoch_train_loss=0.0022040574344543607
Epoch 1928
Epoch 1928 :: Batch 0/1
Batch Loss = 0.0022039576392802428
1928, epoch_train_loss=0.0022039576392802428
Epoch 1929
Epoch 1929 :: Batch 0/1
Batch Loss = 0.00220386113995378
1929, epoch_train_loss=0.00220386113995378
Epoch 1930
Epoch 1930 :: Batch 0/1
Batch Loss = 0.002203764304824951
1930, epoch_train_loss=0.002203764304824951
Epoch 1931
Epoch 1931 :: Batch 0/1
Batch Loss = 0.0022036640934581953
1931, epoch_train_loss=0.0022036640934581953
Epoch 1932
Epoch 1932 :: Batch 0/1
Batch Loss = 0.0022035593431564424
1932, epoch_train_loss=0.0022035593431564424
Epoch 1933
Epoch 1933 :: Batch 0/1
Batch Loss = 0.0022034500071915035
1933, epoch_train_loss=0.0022034500071915035
Epoch 1934
Epoch 1934 :: Batch 0/1
Batch Loss = 0.0022033376473457977
1934, epoch_train_loss=0.0022033376473457977
Epoch 1935
Epoch 1935 :: Batch 0/1
Batch Loss = 0.002203223754950007
1935, epoch_train_loss=0.002203223754950007
Epoch 1936
Epoch 1936 :: Batch 0/1
Batch Loss = 0.0022031103697766037
1936, epoch_train_loss=0.0022031103697766037
Epoch 1937
Epoch 1937 :: Batch 0/1
Batch Loss = 0.0022029991365204965
1937, epoch_train_loss=0.0022029991365204965
Epoch 1938
Epoch 1938 :: Batch 0/1
Batch Loss = 0.002202891080891243
1938, epoch_train_loss=0.002202891080891243
Epoch 1939
Epoch 1939 :: Batch 0/1
Batch Loss = 0.0022027863156891714
1939, epoch_train_loss=0.0022027863156891714
Epoch 1940
Epoch 1940 :: Batch 0/1
Batch Loss = 0.002202684100617439
1940, epoch_train_loss=0.002202684100617439
Epoch 1941
Epoch 1941 :: Batch 0/1
Batch Loss = 0.0022025834861269353
1941, epoch_train_loss=0.0022025834861269353
Epoch 1942
Epoch 1942 :: Batch 0/1
Batch Loss = 0.0022024834517491584
1942, epoch_train_loss=0.0022024834517491584
Epoch 1943
Epoch 1943 :: Batch 0/1
Batch Loss = 0.0022023831666061738
1943, epoch_train_loss=0.0022023831666061738
Epoch 1944
Epoch 1944 :: Batch 0/1
Batch Loss = 0.002202281998385543
1944, epoch_train_loss=0.002202281998385543
Epoch 1945
Epoch 1945 :: Batch 0/1
Batch Loss = 0.0022021797973722336
1945, epoch_train_loss=0.0022021797973722336
Epoch 1946
Epoch 1946 :: Batch 0/1
Batch Loss = 0.0022020766425195053
1946, epoch_train_loss=0.0022020766425195053
Epoch 1947
Epoch 1947 :: Batch 0/1
Batch Loss = 0.002201972964367618
1947, epoch_train_loss=0.002201972964367618
Epoch 1948
Epoch 1948 :: Batch 0/1
Batch Loss = 0.0022018689600245703
1948, epoch_train_loss=0.0022018689600245703
Epoch 1949
Epoch 1949 :: Batch 0/1
Batch Loss = 0.0022017650067439856
1949, epoch_train_loss=0.0022017650067439856
Epoch 1950
Epoch 1950 :: Batch 0/1
Batch Loss = 0.0022016612180931774
1950, epoch_train_loss=0.0022016612180931774
Epoch 1951
Epoch 1951 :: Batch 0/1
Batch Loss = 0.0022015578389898544
1951, epoch_train_loss=0.0022015578389898544
Epoch 1952
Epoch 1952 :: Batch 0/1
Batch Loss = 0.002201454847835346
1952, epoch_train_loss=0.002201454847835346
Epoch 1953
Epoch 1953 :: Batch 0/1
Batch Loss = 0.002201352281710465
1953, epoch_train_loss=0.002201352281710465
Epoch 1954
Epoch 1954 :: Batch 0/1
Batch Loss = 0.002201250071683786
1954, epoch_train_loss=0.002201250071683786
Epoch 1955
Epoch 1955 :: Batch 0/1
Batch Loss = 0.002201148262504192
1955, epoch_train_loss=0.002201148262504192
Epoch 1956
Epoch 1956 :: Batch 0/1
Batch Loss = 0.0022010468115485115
1956, epoch_train_loss=0.0022010468115485115
Epoch 1957
Epoch 1957 :: Batch 0/1
Batch Loss = 0.002200945726981218
1957, epoch_train_loss=0.002200945726981218
Epoch 1958
Epoch 1958 :: Batch 0/1
Batch Loss = 0.00220084495003225
1958, epoch_train_loss=0.00220084495003225
Epoch 1959
Epoch 1959 :: Batch 0/1
Batch Loss = 0.002200744485886677
1959, epoch_train_loss=0.002200744485886677
Epoch 1960
Epoch 1960 :: Batch 0/1
Batch Loss = 0.0022006443097728327
1960, epoch_train_loss=0.0022006443097728327
Epoch 1961
Epoch 1961 :: Batch 0/1
Batch Loss = 0.0022005443853052474
1961, epoch_train_loss=0.0022005443853052474
Epoch 1962
Epoch 1962 :: Batch 0/1
Batch Loss = 0.002200444688002637
1962, epoch_train_loss=0.002200444688002637
Epoch 1963
Epoch 1963 :: Batch 0/1
Batch Loss = 0.0022003451699770123
1963, epoch_train_loss=0.0022003451699770123
Epoch 1964
Epoch 1964 :: Batch 0/1
Batch Loss = 0.0022002458550249753
1964, epoch_train_loss=0.0022002458550249753
Epoch 1965
Epoch 1965 :: Batch 0/1
Batch Loss = 0.0022001467044232012
1965, epoch_train_loss=0.0022001467044232012
Epoch 1966
Epoch 1966 :: Batch 0/1
Batch Loss = 0.0022000477563812897
1966, epoch_train_loss=0.0022000477563812897
Epoch 1967
Epoch 1967 :: Batch 0/1
Batch Loss = 0.0021999489836359008
1967, epoch_train_loss=0.0021999489836359008
Epoch 1968
Epoch 1968 :: Batch 0/1
Batch Loss = 0.0021998504643718424
1968, epoch_train_loss=0.0021998504643718424
Epoch 1969
Epoch 1969 :: Batch 0/1
Batch Loss = 0.002199752194271011
1969, epoch_train_loss=0.002199752194271011
Epoch 1970
Epoch 1970 :: Batch 0/1
Batch Loss = 0.0021996542844360486
1970, epoch_train_loss=0.0021996542844360486
Epoch 1971
Epoch 1971 :: Batch 0/1
Batch Loss = 0.0021995567483135
1971, epoch_train_loss=0.0021995567483135
Epoch 1972
Epoch 1972 :: Batch 0/1
Batch Loss = 0.0021994597714816893
1972, epoch_train_loss=0.0021994597714816893
Epoch 1973
Epoch 1973 :: Batch 0/1
Batch Loss = 0.002199363435565255
1973, epoch_train_loss=0.002199363435565255
Epoch 1974
Epoch 1974 :: Batch 0/1
Batch Loss = 0.00219926806287874
1974, epoch_train_loss=0.00219926806287874
Epoch 1975
Epoch 1975 :: Batch 0/1
Batch Loss = 0.0021991738822241596
1975, epoch_train_loss=0.0021991738822241596
Epoch 1976
Epoch 1976 :: Batch 0/1
Batch Loss = 0.0021990815124731306
1976, epoch_train_loss=0.0021990815124731306
Epoch 1977
Epoch 1977 :: Batch 0/1
Batch Loss = 0.0021989915571035807
1977, epoch_train_loss=0.0021989915571035807
Epoch 1978
Epoch 1978 :: Batch 0/1
Batch Loss = 0.0021989053073985703
1978, epoch_train_loss=0.0021989053073985703
Epoch 1979
Epoch 1979 :: Batch 0/1
Batch Loss = 0.002198824254914781
1979, epoch_train_loss=0.002198824254914781
Epoch 1980
Epoch 1980 :: Batch 0/1
Batch Loss = 0.0021987512693839497
1980, epoch_train_loss=0.0021987512693839497
Epoch 1981
Epoch 1981 :: Batch 0/1
Batch Loss = 0.002198690040157637
1981, epoch_train_loss=0.002198690040157637
Epoch 1982
Epoch 1982 :: Batch 0/1
Batch Loss = 0.0021986473031749437
1982, epoch_train_loss=0.0021986473031749437
Epoch 1983
Epoch 1983 :: Batch 0/1
Batch Loss = 0.002198632280600317
1983, epoch_train_loss=0.002198632280600317
Epoch 1984
Epoch 1984 :: Batch 0/1
Batch Loss = 0.0021986615353103826
1984, epoch_train_loss=0.0021986615353103826
Epoch 1985
Epoch 1985 :: Batch 0/1
Batch Loss = 0.0021987585913551874
1985, epoch_train_loss=0.0021987585913551874
Epoch 1986
Epoch 1986 :: Batch 0/1
Batch Loss = 0.0021989660110711042
1986, epoch_train_loss=0.0021989660110711042
Epoch 1987
Epoch 1987 :: Batch 0/1
Batch Loss = 0.0021993451623280847
1987, epoch_train_loss=0.0021993451623280847
Epoch 1988
Epoch 1988 :: Batch 0/1
Batch Loss = 0.0022000100943892105
1988, epoch_train_loss=0.0022000100943892105
Epoch 1989
Epoch 1989 :: Batch 0/1
Batch Loss = 0.0022011244698069876
1989, epoch_train_loss=0.0022011244698069876
Epoch 1990
Epoch 1990 :: Batch 0/1
Batch Loss = 0.0022030068904247387
1990, epoch_train_loss=0.0022030068904247387
Epoch 1991
Epoch 1991 :: Batch 0/1
Batch Loss = 0.0022061021133002396
1991, epoch_train_loss=0.0022061021133002396
Epoch 1992
Epoch 1992 :: Batch 0/1
Batch Loss = 0.0022113392711401864
1992, epoch_train_loss=0.0022113392711401864
Epoch 1993
Epoch 1993 :: Batch 0/1
Batch Loss = 0.002219940353410122
1993, epoch_train_loss=0.002219940353410122
Epoch 1994
Epoch 1994 :: Batch 0/1
Batch Loss = 0.002234741898940895
1994, epoch_train_loss=0.002234741898940895
Epoch 1995
Epoch 1995 :: Batch 0/1
Batch Loss = 0.0022590811036517933
1995, epoch_train_loss=0.0022590811036517933
Epoch 1996
Epoch 1996 :: Batch 0/1
Batch Loss = 0.0023020557404151852
1996, epoch_train_loss=0.0023020557404151852
Epoch 1997
Epoch 1997 :: Batch 0/1
Batch Loss = 0.0023723599910397274
1997, epoch_train_loss=0.0023723599910397274
Epoch 1998
Epoch 1998 :: Batch 0/1
Batch Loss = 0.002500713706237622
1998, epoch_train_loss=0.002500713706237622
Epoch 1999
Epoch 1999 :: Batch 0/1
Batch Loss = 0.002706106767157819
1999, epoch_train_loss=0.002706106767157819
Epoch 2000
Epoch 2000 :: Batch 0/1
Batch Loss = 0.0030970687050743038
2000, epoch_train_loss=0.0030970687050743038
Epoch 2001
Epoch 2001 :: Batch 0/1
Batch Loss = 0.0036858842539704896
2001, epoch_train_loss=0.0036858842539704896
Epoch 2002
Epoch 2002 :: Batch 0/1
Batch Loss = 0.004861113667561487
2002, epoch_train_loss=0.004861113667561487
Epoch 2003
Epoch 2003 :: Batch 0/1
Batch Loss = 0.006373653891679656
2003, epoch_train_loss=0.006373653891679656
Epoch 2004
Epoch 2004 :: Batch 0/1
Batch Loss = 0.009505953698059724
2004, epoch_train_loss=0.009505953698059724
Epoch 2005
Epoch 2005 :: Batch 0/1
Batch Loss = 0.012038744602716103
2005, epoch_train_loss=0.012038744602716103
Epoch 2006
Epoch 2006 :: Batch 0/1
Batch Loss = 0.017228044712560724
2006, epoch_train_loss=0.017228044712560724
Epoch 2007
Epoch 2007 :: Batch 0/1
Batch Loss = 0.016285915678833095
2007, epoch_train_loss=0.016285915678833095
Epoch 2008
Epoch 2008 :: Batch 0/1
Batch Loss = 0.01584233415839243
2008, epoch_train_loss=0.01584233415839243
Epoch 2009
Epoch 2009 :: Batch 0/1
Batch Loss = 0.008305858387147862
2009, epoch_train_loss=0.008305858387147862
Epoch 2010
Epoch 2010 :: Batch 0/1
Batch Loss = 0.0033780800087754013
2010, epoch_train_loss=0.0033780800087754013
Epoch 2011
Epoch 2011 :: Batch 0/1
Batch Loss = 0.0026756293176592844
2011, epoch_train_loss=0.0026756293176592844
Epoch 2012
Epoch 2012 :: Batch 0/1
Batch Loss = 0.0056489632596812426
2012, epoch_train_loss=0.0056489632596812426
Epoch 2013
Epoch 2013 :: Batch 0/1
Batch Loss = 0.008959720010507394
2013, epoch_train_loss=0.008959720010507394
Epoch 2014
Epoch 2014 :: Batch 0/1
Batch Loss = 0.0077819777035632864
2014, epoch_train_loss=0.0077819777035632864
Epoch 2015
Epoch 2015 :: Batch 0/1
Batch Loss = 0.0053534051762837796
2015, epoch_train_loss=0.0053534051762837796
Epoch 2016
Epoch 2016 :: Batch 0/1
Batch Loss = 0.002913614049664491
2016, epoch_train_loss=0.002913614049664491
Epoch 2017
Epoch 2017 :: Batch 0/1
Batch Loss = 0.003427851593309788
2017, epoch_train_loss=0.003427851593309788
Epoch 2018
Epoch 2018 :: Batch 0/1
Batch Loss = 0.005392489072531621
2018, epoch_train_loss=0.005392489072531621
Epoch 2019
Epoch 2019 :: Batch 0/1
Batch Loss = 0.005501127476068375
2019, epoch_train_loss=0.005501127476068375
Epoch 2020
Epoch 2020 :: Batch 0/1
Batch Loss = 0.004318926230510517
2020, epoch_train_loss=0.004318926230510517
Epoch 2021
Epoch 2021 :: Batch 0/1
Batch Loss = 0.0028001982091187838
2021, epoch_train_loss=0.0028001982091187838
Epoch 2022
Epoch 2022 :: Batch 0/1
Batch Loss = 0.002749979855215848
2022, epoch_train_loss=0.002749979855215848
Epoch 2023
Epoch 2023 :: Batch 0/1
Batch Loss = 0.003694571191997145
2023, epoch_train_loss=0.003694571191997145
Epoch 2024
Epoch 2024 :: Batch 0/1
Batch Loss = 0.004017750671258449
2024, epoch_train_loss=0.004017750671258449
Epoch 2025
Epoch 2025 :: Batch 0/1
Batch Loss = 0.003413622951511707
2025, epoch_train_loss=0.003413622951511707
Epoch 2026
Epoch 2026 :: Batch 0/1
Batch Loss = 0.002537694086927489
2026, epoch_train_loss=0.002537694086927489
Epoch 2027
Epoch 2027 :: Batch 0/1
Batch Loss = 0.0025384030742372095
2027, epoch_train_loss=0.0025384030742372095
Epoch 2028
Epoch 2028 :: Batch 0/1
Batch Loss = 0.003136791480636266
2028, epoch_train_loss=0.003136791480636266
Epoch 2029
Epoch 2029 :: Batch 0/1
Batch Loss = 0.0033926752952357572
2029, epoch_train_loss=0.0033926752952357572
Epoch 2030
Epoch 2030 :: Batch 0/1
Batch Loss = 0.003121503012432965
2030, epoch_train_loss=0.003121503012432965
Epoch 2031
Epoch 2031 :: Batch 0/1
Batch Loss = 0.002600814961859006
2031, epoch_train_loss=0.002600814961859006
Epoch 2032
Epoch 2032 :: Batch 0/1
Batch Loss = 0.0024355100312286166
2032, epoch_train_loss=0.0024355100312286166
Epoch 2033
Epoch 2033 :: Batch 0/1
Batch Loss = 0.002635538443243685
2033, epoch_train_loss=0.002635538443243685
Epoch 2034
Epoch 2034 :: Batch 0/1
Batch Loss = 0.0028230539394334423
2034, epoch_train_loss=0.0028230539394334423
Epoch 2035
Epoch 2035 :: Batch 0/1
Batch Loss = 0.0027717939404585633
2035, epoch_train_loss=0.0027717939404585633
Epoch 2036
Epoch 2036 :: Batch 0/1
Batch Loss = 0.0025192852771598064
2036, epoch_train_loss=0.0025192852771598064
Epoch 2037
Epoch 2037 :: Batch 0/1
Batch Loss = 0.0023658788270931653
2037, epoch_train_loss=0.0023658788270931653
Epoch 2038
Epoch 2038 :: Batch 0/1
Batch Loss = 0.0024093209209292347
2038, epoch_train_loss=0.0024093209209292347
Epoch 2039
Epoch 2039 :: Batch 0/1
Batch Loss = 0.002536564341595237
2039, epoch_train_loss=0.002536564341595237
Epoch 2040
Epoch 2040 :: Batch 0/1
Batch Loss = 0.002574526773054761
2040, epoch_train_loss=0.002574526773054761
Epoch 2041
Epoch 2041 :: Batch 0/1
Batch Loss = 0.002446444935254206
2041, epoch_train_loss=0.002446444935254206
Epoch 2042
Epoch 2042 :: Batch 0/1
Batch Loss = 0.002315758817351708
2042, epoch_train_loss=0.002315758817351708
Epoch 2043
Epoch 2043 :: Batch 0/1
Batch Loss = 0.002309669330647378
2043, epoch_train_loss=0.002309669330647378
Epoch 2044
Epoch 2044 :: Batch 0/1
Batch Loss = 0.002401261776292122
2044, epoch_train_loss=0.002401261776292122
Epoch 2045
Epoch 2045 :: Batch 0/1
Batch Loss = 0.0024541484130808115
2045, epoch_train_loss=0.0024541484130808115
Epoch 2046
Epoch 2046 :: Batch 0/1
Batch Loss = 0.002384855586207059
2046, epoch_train_loss=0.002384855586207059
Epoch 2047
Epoch 2047 :: Batch 0/1
Batch Loss = 0.0022814529793658005
2047, epoch_train_loss=0.0022814529793658005
Epoch 2048
Epoch 2048 :: Batch 0/1
Batch Loss = 0.002251189703371996
2048, epoch_train_loss=0.002251189703371996
Epoch 2049
Epoch 2049 :: Batch 0/1
Batch Loss = 0.0023097021746925294
2049, epoch_train_loss=0.0023097021746925294
Epoch 2050
Epoch 2050 :: Batch 0/1
Batch Loss = 0.0023625999829335483
2050, epoch_train_loss=0.0023625999829335483
Epoch 2051
Epoch 2051 :: Batch 0/1
Batch Loss = 0.0023325602450195108
2051, epoch_train_loss=0.0023325602450195108
Epoch 2052
Epoch 2052 :: Batch 0/1
Batch Loss = 0.002261323367659496
2052, epoch_train_loss=0.002261323367659496
Epoch 2053
Epoch 2053 :: Batch 0/1
Batch Loss = 0.0022263283453982452
2053, epoch_train_loss=0.0022263283453982452
Epoch 2054
Epoch 2054 :: Batch 0/1
Batch Loss = 0.0022557329362234584
2054, epoch_train_loss=0.0022557329362234584
Epoch 2055
Epoch 2055 :: Batch 0/1
Batch Loss = 0.002297794903326409
2055, epoch_train_loss=0.002297794903326409
Epoch 2056
Epoch 2056 :: Batch 0/1
Batch Loss = 0.0022957665888732885
2056, epoch_train_loss=0.0022957665888732885
Epoch 2057
Epoch 2057 :: Batch 0/1
Batch Loss = 0.0022563327915446204
2057, epoch_train_loss=0.0022563327915446204
Epoch 2058
Epoch 2058 :: Batch 0/1
Batch Loss = 0.002220775723203395
2058, epoch_train_loss=0.002220775723203395
Epoch 2059
Epoch 2059 :: Batch 0/1
Batch Loss = 0.002222171254304485
2059, epoch_train_loss=0.002222171254304485
Epoch 2060
Epoch 2060 :: Batch 0/1
Batch Loss = 0.00224594414244663
2060, epoch_train_loss=0.00224594414244663
Epoch 2061
Epoch 2061 :: Batch 0/1
Batch Loss = 0.002259263256652812
2061, epoch_train_loss=0.002259263256652812
Epoch 2062
Epoch 2062 :: Batch 0/1
Batch Loss = 0.0022493150517743474
2062, epoch_train_loss=0.0022493150517743474
Epoch 2063
Epoch 2063 :: Batch 0/1
Batch Loss = 0.002226544091562693
2063, epoch_train_loss=0.002226544091562693
Epoch 2064
Epoch 2064 :: Batch 0/1
Batch Loss = 0.0022134284132607754
2064, epoch_train_loss=0.0022134284132607754
Epoch 2065
Epoch 2065 :: Batch 0/1
Batch Loss = 0.002215863817843639
2065, epoch_train_loss=0.002215863817843639
Epoch 2066
Epoch 2066 :: Batch 0/1
Batch Loss = 0.0022254739839513565
2066, epoch_train_loss=0.0022254739839513565
Epoch 2067
Epoch 2067 :: Batch 0/1
Batch Loss = 0.002230870297796448
2067, epoch_train_loss=0.002230870297796448
Epoch 2068
Epoch 2068 :: Batch 0/1
Batch Loss = 0.002226816460053401
2068, epoch_train_loss=0.002226816460053401
Epoch 2069
Epoch 2069 :: Batch 0/1
Batch Loss = 0.0022187256183962955
2069, epoch_train_loss=0.0022187256183962955
Epoch 2070
Epoch 2070 :: Batch 0/1
Batch Loss = 0.0022118227736376366
2070, epoch_train_loss=0.0022118227736376366
Epoch 2071
Epoch 2071 :: Batch 0/1
Batch Loss = 0.002209436500537012
2071, epoch_train_loss=0.002209436500537012
Epoch 2072
Epoch 2072 :: Batch 0/1
Batch Loss = 0.002211217174546243
2072, epoch_train_loss=0.002211217174546243
Epoch 2073
Epoch 2073 :: Batch 0/1
Batch Loss = 0.002214458950877151
2073, epoch_train_loss=0.002214458950877151
Epoch 2074
Epoch 2074 :: Batch 0/1
Batch Loss = 0.002216781050661767
2074, epoch_train_loss=0.002216781050661767
Epoch 2075
Epoch 2075 :: Batch 0/1
Batch Loss = 0.002215690535587813
2075, epoch_train_loss=0.002215690535587813
Epoch 2076
Epoch 2076 :: Batch 0/1
Batch Loss = 0.002211558637792644
2076, epoch_train_loss=0.002211558637792644
Epoch 2077
Epoch 2077 :: Batch 0/1
Batch Loss = 0.002206915349867272
2077, epoch_train_loss=0.002206915349867272
Epoch 2078
Epoch 2078 :: Batch 0/1
Batch Loss = 0.0022047356328639195
2078, epoch_train_loss=0.0022047356328639195
Epoch 2079
Epoch 2079 :: Batch 0/1
Batch Loss = 0.0022061855389337915
2079, epoch_train_loss=0.0022061855389337915
Epoch 2080
Epoch 2080 :: Batch 0/1
Batch Loss = 0.0022092067129124053
2080, epoch_train_loss=0.0022092067129124053
Epoch 2081
Epoch 2081 :: Batch 0/1
Batch Loss = 0.0022108214854769494
2081, epoch_train_loss=0.0022108214854769494
Epoch 2082
Epoch 2082 :: Batch 0/1
Batch Loss = 0.00220960667155268
2082, epoch_train_loss=0.00220960667155268
Epoch 2083
Epoch 2083 :: Batch 0/1
Batch Loss = 0.0022066226101940326
2083, epoch_train_loss=0.0022066226101940326
Epoch 2084
Epoch 2084 :: Batch 0/1
Batch Loss = 0.0022041388188320677
2084, epoch_train_loss=0.0022041388188320677
Epoch 2085
Epoch 2085 :: Batch 0/1
Batch Loss = 0.0022034692344467663
2085, epoch_train_loss=0.0022034692344467663
Epoch 2086
Epoch 2086 :: Batch 0/1
Batch Loss = 0.0022042957248386307
2086, epoch_train_loss=0.0022042957248386307
Epoch 2087
Epoch 2087 :: Batch 0/1
Batch Loss = 0.0022054329167626495
2087, epoch_train_loss=0.0022054329167626495
Epoch 2088
Epoch 2088 :: Batch 0/1
Batch Loss = 0.002205846509318298
2088, epoch_train_loss=0.002205846509318298
Epoch 2089
Epoch 2089 :: Batch 0/1
Batch Loss = 0.002205390542154155
2089, epoch_train_loss=0.002205390542154155
Epoch 2090
Epoch 2090 :: Batch 0/1
Batch Loss = 0.0022044583966142964
2090, epoch_train_loss=0.0022044583966142964
Epoch 2091
Epoch 2091 :: Batch 0/1
Batch Loss = 0.002203501208341178
2091, epoch_train_loss=0.002203501208341178
Epoch 2092
Epoch 2092 :: Batch 0/1
Batch Loss = 0.002202855718868901
2092, epoch_train_loss=0.002202855718868901
Epoch 2093
Epoch 2093 :: Batch 0/1
Batch Loss = 0.0022025760774245684
2093, epoch_train_loss=0.0022025760774245684
Epoch 2094
Epoch 2094 :: Batch 0/1
Batch Loss = 0.0022026579146978025
2094, epoch_train_loss=0.0022026579146978025
Epoch 2095
Epoch 2095 :: Batch 0/1
Batch Loss = 0.0022029353300632246
2095, epoch_train_loss=0.0022029353300632246
Epoch 2096
Epoch 2096 :: Batch 0/1
Batch Loss = 0.0022031505676221763
2096, epoch_train_loss=0.0022031505676221763
Epoch 2097
Epoch 2097 :: Batch 0/1
Batch Loss = 0.002203090502647799
2097, epoch_train_loss=0.002203090502647799
Epoch 2098
Epoch 2098 :: Batch 0/1
Batch Loss = 0.0022026485686616323
2098, epoch_train_loss=0.0022026485686616323
Epoch 2099
Epoch 2099 :: Batch 0/1
Batch Loss = 0.002202019527220305
2099, epoch_train_loss=0.002202019527220305
Epoch 2100
Epoch 2100 :: Batch 0/1
Batch Loss = 0.0022014908978629497
2100, epoch_train_loss=0.0022014908978629497
Epoch 2101
Epoch 2101 :: Batch 0/1
Batch Loss = 0.002201274813624425
2101, epoch_train_loss=0.002201274813624425
Epoch 2102
Epoch 2102 :: Batch 0/1
Batch Loss = 0.002201363996176111
2102, epoch_train_loss=0.002201363996176111
Epoch 2103
Epoch 2103 :: Batch 0/1
Batch Loss = 0.0022015381787927357
2103, epoch_train_loss=0.0022015381787927357
Epoch 2104
Epoch 2104 :: Batch 0/1
Batch Loss = 0.002201591707317602
2104, epoch_train_loss=0.002201591707317602
Epoch 2105
Epoch 2105 :: Batch 0/1
Batch Loss = 0.0022014336834092056
2105, epoch_train_loss=0.0022014336834092056
Epoch 2106
Epoch 2106 :: Batch 0/1
Batch Loss = 0.00220113121701067
2106, epoch_train_loss=0.00220113121701067
Epoch 2107
Epoch 2107 :: Batch 0/1
Batch Loss = 0.0022008136458253344
2107, epoch_train_loss=0.0022008136458253344
Epoch 2108
Epoch 2108 :: Batch 0/1
Batch Loss = 0.002200565708460562
2108, epoch_train_loss=0.002200565708460562
Epoch 2109
Epoch 2109 :: Batch 0/1
Batch Loss = 0.0022004137341260386
2109, epoch_train_loss=0.0022004137341260386
Epoch 2110
Epoch 2110 :: Batch 0/1
Batch Loss = 0.002200327684584673
2110, epoch_train_loss=0.002200327684584673
Epoch 2111
Epoch 2111 :: Batch 0/1
Batch Loss = 0.0022002677428593054
2111, epoch_train_loss=0.0022002677428593054
Epoch 2112
Epoch 2112 :: Batch 0/1
Batch Loss = 0.00220021012274841
2112, epoch_train_loss=0.00220021012274841
Epoch 2113
Epoch 2113 :: Batch 0/1
Batch Loss = 0.0022001352707062202
2113, epoch_train_loss=0.0022001352707062202
Epoch 2114
Epoch 2114 :: Batch 0/1
Batch Loss = 0.002200034259160119
2114, epoch_train_loss=0.002200034259160119
Epoch 2115
Epoch 2115 :: Batch 0/1
Batch Loss = 0.002199898134847226
2115, epoch_train_loss=0.002199898134847226
Epoch 2116
Epoch 2116 :: Batch 0/1
Batch Loss = 0.0021997283592993786
2116, epoch_train_loss=0.0021997283592993786
Epoch 2117
Epoch 2117 :: Batch 0/1
Batch Loss = 0.0021995487315046103
2117, epoch_train_loss=0.0021995487315046103
Epoch 2118
Epoch 2118 :: Batch 0/1
Batch Loss = 0.0021993872327416442
2118, epoch_train_loss=0.0021993872327416442
Epoch 2119
Epoch 2119 :: Batch 0/1
Batch Loss = 0.002199268145971775
2119, epoch_train_loss=0.002199268145971775
Epoch 2120
Epoch 2120 :: Batch 0/1
Batch Loss = 0.0021991891634044786
2120, epoch_train_loss=0.0021991891634044786
Epoch 2121
Epoch 2121 :: Batch 0/1
Batch Loss = 0.002199128003155352
2121, epoch_train_loss=0.002199128003155352
Epoch 2122
Epoch 2122 :: Batch 0/1
Batch Loss = 0.002199059717134317
2122, epoch_train_loss=0.002199059717134317
Epoch 2123
Epoch 2123 :: Batch 0/1
Batch Loss = 0.002198967012340979
2123, epoch_train_loss=0.002198967012340979
Epoch 2124
Epoch 2124 :: Batch 0/1
Batch Loss = 0.0021988519679068024
2124, epoch_train_loss=0.0021988519679068024
Epoch 2125
Epoch 2125 :: Batch 0/1
Batch Loss = 0.0021987243865560736
2125, epoch_train_loss=0.0021987243865560736
Epoch 2126
Epoch 2126 :: Batch 0/1
Batch Loss = 0.0021985969310566074
2126, epoch_train_loss=0.0021985969310566074
Epoch 2127
Epoch 2127 :: Batch 0/1
Batch Loss = 0.0021984776973627594
2127, epoch_train_loss=0.0021984776973627594
Epoch 2128
Epoch 2128 :: Batch 0/1
Batch Loss = 0.0021983673404190485
2128, epoch_train_loss=0.0021983673404190485
Epoch 2129
Epoch 2129 :: Batch 0/1
Batch Loss = 0.002198264416177569
2129, epoch_train_loss=0.002198264416177569
Epoch 2130
Epoch 2130 :: Batch 0/1
Batch Loss = 0.002198166211990099
2130, epoch_train_loss=0.002198166211990099
Epoch 2131
Epoch 2131 :: Batch 0/1
Batch Loss = 0.0021980718861006163
2131, epoch_train_loss=0.0021980718861006163
Epoch 2132
Epoch 2132 :: Batch 0/1
Batch Loss = 0.0021979810909224635
2132, epoch_train_loss=0.0021979810909224635
Epoch 2133
Epoch 2133 :: Batch 0/1
Batch Loss = 0.0021978912519348423
2133, epoch_train_loss=0.0021978912519348423
Epoch 2134
Epoch 2134 :: Batch 0/1
Batch Loss = 0.0021977992960812522
2134, epoch_train_loss=0.0021977992960812522
Epoch 2135
Epoch 2135 :: Batch 0/1
Batch Loss = 0.0021977021911653745
2135, epoch_train_loss=0.0021977021911653745
Epoch 2136
Epoch 2136 :: Batch 0/1
Batch Loss = 0.002197599328500306
2136, epoch_train_loss=0.002197599328500306
Epoch 2137
Epoch 2137 :: Batch 0/1
Batch Loss = 0.002197493030424936
2137, epoch_train_loss=0.002197493030424936
Epoch 2138
Epoch 2138 :: Batch 0/1
Batch Loss = 0.0021973860737506514
2138, epoch_train_loss=0.0021973860737506514
Epoch 2139
Epoch 2139 :: Batch 0/1
Batch Loss = 0.002197281746543024
2139, epoch_train_loss=0.002197281746543024
Epoch 2140
Epoch 2140 :: Batch 0/1
Batch Loss = 0.0021971811684721033
2140, epoch_train_loss=0.0021971811684721033
Epoch 2141
Epoch 2141 :: Batch 0/1
Batch Loss = 0.002197084199060835
2141, epoch_train_loss=0.002197084199060835
Epoch 2142
Epoch 2142 :: Batch 0/1
Batch Loss = 0.0021969894983082418
2142, epoch_train_loss=0.0021969894983082418
Epoch 2143
Epoch 2143 :: Batch 0/1
Batch Loss = 0.0021968954229087873
2143, epoch_train_loss=0.0021968954229087873
Epoch 2144
Epoch 2144 :: Batch 0/1
Batch Loss = 0.002196801398682927
2144, epoch_train_loss=0.002196801398682927
Epoch 2145
Epoch 2145 :: Batch 0/1
Batch Loss = 0.0021967071583370193
2145, epoch_train_loss=0.0021967071583370193
Epoch 2146
Epoch 2146 :: Batch 0/1
Batch Loss = 0.0021966130121878454
2146, epoch_train_loss=0.0021966130121878454
Epoch 2147
Epoch 2147 :: Batch 0/1
Batch Loss = 0.0021965188701228247
2147, epoch_train_loss=0.0021965188701228247
Epoch 2148
Epoch 2148 :: Batch 0/1
Batch Loss = 0.002196424454327754
2148, epoch_train_loss=0.002196424454327754
Epoch 2149
Epoch 2149 :: Batch 0/1
Batch Loss = 0.002196329476840674
2149, epoch_train_loss=0.002196329476840674
Epoch 2150
Epoch 2150 :: Batch 0/1
Batch Loss = 0.0021962337478682184
2150, epoch_train_loss=0.0021962337478682184
Epoch 2151
Epoch 2151 :: Batch 0/1
Batch Loss = 0.002196137363512178
2151, epoch_train_loss=0.002196137363512178
Epoch 2152
Epoch 2152 :: Batch 0/1
Batch Loss = 0.0021960407192570977
2152, epoch_train_loss=0.0021960407192570977
Epoch 2153
Epoch 2153 :: Batch 0/1
Batch Loss = 0.002195944209837121
2153, epoch_train_loss=0.002195944209837121
Epoch 2154
Epoch 2154 :: Batch 0/1
Batch Loss = 0.0021958483221475814
2154, epoch_train_loss=0.0021958483221475814
Epoch 2155
Epoch 2155 :: Batch 0/1
Batch Loss = 0.002195753090406084
2155, epoch_train_loss=0.002195753090406084
Epoch 2156
Epoch 2156 :: Batch 0/1
Batch Loss = 0.0021956584885537535
2156, epoch_train_loss=0.0021956584885537535
Epoch 2157
Epoch 2157 :: Batch 0/1
Batch Loss = 0.002195564273824214
2157, epoch_train_loss=0.002195564273824214
Epoch 2158
Epoch 2158 :: Batch 0/1
Batch Loss = 0.0021954703034755066
2158, epoch_train_loss=0.0021954703034755066
Epoch 2159
Epoch 2159 :: Batch 0/1
Batch Loss = 0.0021953765216719536
2159, epoch_train_loss=0.0021953765216719536
Epoch 2160
Epoch 2160 :: Batch 0/1
Batch Loss = 0.0021952829066307377
2160, epoch_train_loss=0.0021952829066307377
Epoch 2161
Epoch 2161 :: Batch 0/1
Batch Loss = 0.0021951895374377173
2161, epoch_train_loss=0.0021951895374377173
Epoch 2162
Epoch 2162 :: Batch 0/1
Batch Loss = 0.002195096440907315
2162, epoch_train_loss=0.002195096440907315
Epoch 2163
Epoch 2163 :: Batch 0/1
Batch Loss = 0.0021950036340588246
2163, epoch_train_loss=0.0021950036340588246
Epoch 2164
Epoch 2164 :: Batch 0/1
Batch Loss = 0.0021949110503166227
2164, epoch_train_loss=0.0021949110503166227
Epoch 2165
Epoch 2165 :: Batch 0/1
Batch Loss = 0.002194818605809532
2165, epoch_train_loss=0.002194818605809532
Epoch 2166
Epoch 2166 :: Batch 0/1
Batch Loss = 0.002194726228916317
2166, epoch_train_loss=0.002194726228916317
Epoch 2167
Epoch 2167 :: Batch 0/1
Batch Loss = 0.0021946339171783503
2167, epoch_train_loss=0.0021946339171783503
Epoch 2168
Epoch 2168 :: Batch 0/1
Batch Loss = 0.002194541662226504
2168, epoch_train_loss=0.002194541662226504
Epoch 2169
Epoch 2169 :: Batch 0/1
Batch Loss = 0.0021944495423123493
2169, epoch_train_loss=0.0021944495423123493
Epoch 2170
Epoch 2170 :: Batch 0/1
Batch Loss = 0.002194357561421009
2170, epoch_train_loss=0.002194357561421009
Epoch 2171
Epoch 2171 :: Batch 0/1
Batch Loss = 0.0021942658200443895
2171, epoch_train_loss=0.0021942658200443895
Epoch 2172
Epoch 2172 :: Batch 0/1
Batch Loss = 0.0021941742998957178
2172, epoch_train_loss=0.0021941742998957178
Epoch 2173
Epoch 2173 :: Batch 0/1
Batch Loss = 0.0021940830757645542
2173, epoch_train_loss=0.0021940830757645542
Epoch 2174
Epoch 2174 :: Batch 0/1
Batch Loss = 0.002193992123910844
2174, epoch_train_loss=0.002193992123910844
Epoch 2175
Epoch 2175 :: Batch 0/1
Batch Loss = 0.0021939015492523323
2175, epoch_train_loss=0.0021939015492523323
Epoch 2176
Epoch 2176 :: Batch 0/1
Batch Loss = 0.0021938113877706163
2176, epoch_train_loss=0.0021938113877706163
Epoch 2177
Epoch 2177 :: Batch 0/1
Batch Loss = 0.002193721837097862
2177, epoch_train_loss=0.002193721837097862
Epoch 2178
Epoch 2178 :: Batch 0/1
Batch Loss = 0.002193633021971957
2178, epoch_train_loss=0.002193633021971957
Epoch 2179
Epoch 2179 :: Batch 0/1
Batch Loss = 0.0021935453080888885
2179, epoch_train_loss=0.0021935453080888885
Epoch 2180
Epoch 2180 :: Batch 0/1
Batch Loss = 0.0021934589850423104
2180, epoch_train_loss=0.0021934589850423104
Epoch 2181
Epoch 2181 :: Batch 0/1
Batch Loss = 0.0021933747301168833
2181, epoch_train_loss=0.0021933747301168833
Epoch 2182
Epoch 2182 :: Batch 0/1
Batch Loss = 0.002193293182764167
2182, epoch_train_loss=0.002193293182764167
Epoch 2183
Epoch 2183 :: Batch 0/1
Batch Loss = 0.00219321567936573
2183, epoch_train_loss=0.00219321567936573
Epoch 2184
Epoch 2184 :: Batch 0/1
Batch Loss = 0.0021931436839319435
2184, epoch_train_loss=0.0021931436839319435
Epoch 2185
Epoch 2185 :: Batch 0/1
Batch Loss = 0.002193080011818201
2185, epoch_train_loss=0.002193080011818201
Epoch 2186
Epoch 2186 :: Batch 0/1
Batch Loss = 0.00219302802867765
2186, epoch_train_loss=0.00219302802867765
Epoch 2187
Epoch 2187 :: Batch 0/1
Batch Loss = 0.0021929939576411594
2187, epoch_train_loss=0.0021929939576411594
Epoch 2188
Epoch 2188 :: Batch 0/1
Batch Loss = 0.002192985610815614
2188, epoch_train_loss=0.002192985610815614
Epoch 2189
Epoch 2189 :: Batch 0/1
Batch Loss = 0.0021930173507695803
2189, epoch_train_loss=0.0021930173507695803
Epoch 2190
Epoch 2190 :: Batch 0/1
Batch Loss = 0.0021931075955515206
2190, epoch_train_loss=0.0021931075955515206
Epoch 2191
Epoch 2191 :: Batch 0/1
Batch Loss = 0.0021932908744070516
2191, epoch_train_loss=0.0021932908744070516
Epoch 2192
Epoch 2192 :: Batch 0/1
Batch Loss = 0.0021936114218505157
2192, epoch_train_loss=0.0021936114218505157
Epoch 2193
Epoch 2193 :: Batch 0/1
Batch Loss = 0.0021941557169101074
2193, epoch_train_loss=0.0021941557169101074
Epoch 2194
Epoch 2194 :: Batch 0/1
Batch Loss = 0.002195031775164604
2194, epoch_train_loss=0.002195031775164604
Epoch 2195
Epoch 2195 :: Batch 0/1
Batch Loss = 0.0021964653264011087
2195, epoch_train_loss=0.0021964653264011087
Epoch 2196
Epoch 2196 :: Batch 0/1
Batch Loss = 0.0021987229656529285
2196, epoch_train_loss=0.0021987229656529285
Epoch 2197
Epoch 2197 :: Batch 0/1
Batch Loss = 0.002202420011364557
2197, epoch_train_loss=0.002202420011364557
Epoch 2198
Epoch 2198 :: Batch 0/1
Batch Loss = 0.002208213591018973
2198, epoch_train_loss=0.002208213591018973
Epoch 2199
Epoch 2199 :: Batch 0/1
Batch Loss = 0.0022178612754308446
2199, epoch_train_loss=0.0022178612754308446
Epoch 2200
Epoch 2200 :: Batch 0/1
Batch Loss = 0.00223293967403213
2200, epoch_train_loss=0.00223293967403213
Epoch 2201
Epoch 2201 :: Batch 0/1
Batch Loss = 0.0022587314581660927
2201, epoch_train_loss=0.0022587314581660927
Epoch 2202
Epoch 2202 :: Batch 0/1
Batch Loss = 0.002298683862830616
2202, epoch_train_loss=0.002298683862830616
Epoch 2203
Epoch 2203 :: Batch 0/1
Batch Loss = 0.002369560242856893
2203, epoch_train_loss=0.002369560242856893
Epoch 2204
Epoch 2204 :: Batch 0/1
Batch Loss = 0.002476681140468739
2204, epoch_train_loss=0.002476681140468739
Epoch 2205
Epoch 2205 :: Batch 0/1
Batch Loss = 0.0026761121684201502
2205, epoch_train_loss=0.0026761121684201502
Epoch 2206
Epoch 2206 :: Batch 0/1
Batch Loss = 0.0029607757087138525
2206, epoch_train_loss=0.0029607757087138525
Epoch 2207
Epoch 2207 :: Batch 0/1
Batch Loss = 0.0035251899608768193
2207, epoch_train_loss=0.0035251899608768193
Epoch 2208
Epoch 2208 :: Batch 0/1
Batch Loss = 0.004233551654105118
2208, epoch_train_loss=0.004233551654105118
Epoch 2209
Epoch 2209 :: Batch 0/1
Batch Loss = 0.005752662636556825
2209, epoch_train_loss=0.005752662636556825
Epoch 2210
Epoch 2210 :: Batch 0/1
Batch Loss = 0.007145912326612722
2210, epoch_train_loss=0.007145912326612722
Epoch 2211
Epoch 2211 :: Batch 0/1
Batch Loss = 0.010438832236353928
2211, epoch_train_loss=0.010438832236353928
Epoch 2212
Epoch 2212 :: Batch 0/1
Batch Loss = 0.011420713910218424
2212, epoch_train_loss=0.011420713910218424
Epoch 2213
Epoch 2213 :: Batch 0/1
Batch Loss = 0.014784345710979217
2213, epoch_train_loss=0.014784345710979217
Epoch 2214
Epoch 2214 :: Batch 0/1
Batch Loss = 0.011716025755514548
2214, epoch_train_loss=0.011716025755514548
Epoch 2215
Epoch 2215 :: Batch 0/1
Batch Loss = 0.009706182615804877
2215, epoch_train_loss=0.009706182615804877
Epoch 2216
Epoch 2216 :: Batch 0/1
Batch Loss = 0.0055023669818458935
2216, epoch_train_loss=0.0055023669818458935
Epoch 2217
Epoch 2217 :: Batch 0/1
Batch Loss = 0.003307556565166974
2217, epoch_train_loss=0.003307556565166974
Epoch 2218
Epoch 2218 :: Batch 0/1
Batch Loss = 0.0035143159450240617
2218, epoch_train_loss=0.0035143159450240617
Epoch 2219
Epoch 2219 :: Batch 0/1
Batch Loss = 0.005026301575851021
2219, epoch_train_loss=0.005026301575851021
Epoch 2220
Epoch 2220 :: Batch 0/1
Batch Loss = 0.006983052719353317
2220, epoch_train_loss=0.006983052719353317
Epoch 2221
Epoch 2221 :: Batch 0/1
Batch Loss = 0.006391636449777403
2221, epoch_train_loss=0.006391636449777403
Epoch 2222
Epoch 2222 :: Batch 0/1
Batch Loss = 0.004977527755441166
2222, epoch_train_loss=0.004977527755441166
Epoch 2223
Epoch 2223 :: Batch 0/1
Batch Loss = 0.0031913307173652085
2223, epoch_train_loss=0.0031913307173652085
Epoch 2224
Epoch 2224 :: Batch 0/1
Batch Loss = 0.0029064675304309117
2224, epoch_train_loss=0.0029064675304309117
Epoch 2225
Epoch 2225 :: Batch 0/1
Batch Loss = 0.0039741417517218285
2225, epoch_train_loss=0.0039741417517218285
Epoch 2226
Epoch 2226 :: Batch 0/1
Batch Loss = 0.004682853496195892
2226, epoch_train_loss=0.004682853496195892
Epoch 2227
Epoch 2227 :: Batch 0/1
Batch Loss = 0.004718598626005357
2227, epoch_train_loss=0.004718598626005357
Epoch 2228
Epoch 2228 :: Batch 0/1
Batch Loss = 0.0035940606274897573
2228, epoch_train_loss=0.0035940606274897573
Epoch 2229
Epoch 2229 :: Batch 0/1
Batch Loss = 0.0026910774645680364
2229, epoch_train_loss=0.0026910774645680364
Epoch 2230
Epoch 2230 :: Batch 0/1
Batch Loss = 0.0026209074977429307
2230, epoch_train_loss=0.0026209074977429307
Epoch 2231
Epoch 2231 :: Batch 0/1
Batch Loss = 0.0030866973938353773
2231, epoch_train_loss=0.0030866973938353773
Epoch 2232
Epoch 2232 :: Batch 0/1
Batch Loss = 0.0034280442676350486
2232, epoch_train_loss=0.0034280442676350486
Epoch 2233
Epoch 2233 :: Batch 0/1
Batch Loss = 0.0031250582852864147
2233, epoch_train_loss=0.0031250582852864147
Epoch 2234
Epoch 2234 :: Batch 0/1
Batch Loss = 0.00269314658245864
2234, epoch_train_loss=0.00269314658245864
Epoch 2235
Epoch 2235 :: Batch 0/1
Batch Loss = 0.0024750608951190035
2235, epoch_train_loss=0.0024750608951190035
Epoch 2236
Epoch 2236 :: Batch 0/1
Batch Loss = 0.002573399319129164
2236, epoch_train_loss=0.002573399319129164
Epoch 2237
Epoch 2237 :: Batch 0/1
Batch Loss = 0.0027983650847270528
2237, epoch_train_loss=0.0027983650847270528
Epoch 2238
Epoch 2238 :: Batch 0/1
Batch Loss = 0.0028151682150484593
2238, epoch_train_loss=0.0028151682150484593
Epoch 2239
Epoch 2239 :: Batch 0/1
Batch Loss = 0.0026597981341174985
2239, epoch_train_loss=0.0026597981341174985
Epoch 2240
Epoch 2240 :: Batch 0/1
Batch Loss = 0.002456156125936517
2240, epoch_train_loss=0.002456156125936517
Epoch 2241
Epoch 2241 :: Batch 0/1
Batch Loss = 0.0023973742850174184
2241, epoch_train_loss=0.0023973742850174184
Epoch 2242
Epoch 2242 :: Batch 0/1
Batch Loss = 0.0024792074100111086
2242, epoch_train_loss=0.0024792074100111086
Epoch 2243
Epoch 2243 :: Batch 0/1
Batch Loss = 0.0025523675691824497
2243, epoch_train_loss=0.0025523675691824497
Epoch 2244
Epoch 2244 :: Batch 0/1
Batch Loss = 0.0025505874889173684
2244, epoch_train_loss=0.0025505874889173684
Epoch 2245
Epoch 2245 :: Batch 0/1
Batch Loss = 0.002451719443514234
2245, epoch_train_loss=0.002451719443514234
Epoch 2246
Epoch 2246 :: Batch 0/1
Batch Loss = 0.002358666138552432
2246, epoch_train_loss=0.002358666138552432
Epoch 2247
Epoch 2247 :: Batch 0/1
Batch Loss = 0.0023289698591246037
2247, epoch_train_loss=0.0023289698591246037
Epoch 2248
Epoch 2248 :: Batch 0/1
Batch Loss = 0.00234786008501174
2248, epoch_train_loss=0.00234786008501174
Epoch 2249
Epoch 2249 :: Batch 0/1
Batch Loss = 0.0023756517625574862
2249, epoch_train_loss=0.0023756517625574862
Epoch 2250
Epoch 2250 :: Batch 0/1
Batch Loss = 0.0023705207408766214
2250, epoch_train_loss=0.0023705207408766214
Epoch 2251
Epoch 2251 :: Batch 0/1
Batch Loss = 0.0023422963078148034
2251, epoch_train_loss=0.0023422963078148034
Epoch 2252
Epoch 2252 :: Batch 0/1
Batch Loss = 0.0023034352854437307
2252, epoch_train_loss=0.0023034352854437307
Epoch 2253
Epoch 2253 :: Batch 0/1
Batch Loss = 0.0022738611992247767
2253, epoch_train_loss=0.0022738611992247767
Epoch 2254
Epoch 2254 :: Batch 0/1
Batch Loss = 0.002270813510642783
2254, epoch_train_loss=0.002270813510642783
Epoch 2255
Epoch 2255 :: Batch 0/1
Batch Loss = 0.002285450660407528
2255, epoch_train_loss=0.002285450660407528
Epoch 2256
Epoch 2256 :: Batch 0/1
Batch Loss = 0.00229945426426632
2256, epoch_train_loss=0.00229945426426632
Epoch 2257
Epoch 2257 :: Batch 0/1
Batch Loss = 0.002291301431391656
2257, epoch_train_loss=0.002291301431391656
Epoch 2258
Epoch 2258 :: Batch 0/1
Batch Loss = 0.002263087786906759
2258, epoch_train_loss=0.002263087786906759
Epoch 2259
Epoch 2259 :: Batch 0/1
Batch Loss = 0.002235581113056196
2259, epoch_train_loss=0.002235581113056196
Epoch 2260
Epoch 2260 :: Batch 0/1
Batch Loss = 0.002226799987925973
2260, epoch_train_loss=0.002226799987925973
Epoch 2261
Epoch 2261 :: Batch 0/1
Batch Loss = 0.0022393240128042078
2261, epoch_train_loss=0.0022393240128042078
Epoch 2262
Epoch 2262 :: Batch 0/1
Batch Loss = 0.002255937007211264
2262, epoch_train_loss=0.002255937007211264
Epoch 2263
Epoch 2263 :: Batch 0/1
Batch Loss = 0.0022583725002570617
2263, epoch_train_loss=0.0022583725002570617
Epoch 2264
Epoch 2264 :: Batch 0/1
Batch Loss = 0.002241750454295055
2264, epoch_train_loss=0.002241750454295055
Epoch 2265
Epoch 2265 :: Batch 0/1
Batch Loss = 0.0022176081412014718
2265, epoch_train_loss=0.0022176081412014718
Epoch 2266
Epoch 2266 :: Batch 0/1
Batch Loss = 0.002203723064742624
2266, epoch_train_loss=0.002203723064742624
Epoch 2267
Epoch 2267 :: Batch 0/1
Batch Loss = 0.0022075149984051474
2267, epoch_train_loss=0.0022075149984051474
Epoch 2268
Epoch 2268 :: Batch 0/1
Batch Loss = 0.0022217226331311332
2268, epoch_train_loss=0.0022217226331311332
Epoch 2269
Epoch 2269 :: Batch 0/1
Batch Loss = 0.002231705191756288
2269, epoch_train_loss=0.002231705191756288
Epoch 2270
Epoch 2270 :: Batch 0/1
Batch Loss = 0.0022283783992104564
2270, epoch_train_loss=0.0022283783992104564
Epoch 2271
Epoch 2271 :: Batch 0/1
Batch Loss = 0.002214799542485862
2271, epoch_train_loss=0.002214799542485862
Epoch 2272
Epoch 2272 :: Batch 0/1
Batch Loss = 0.002201154115170006
2272, epoch_train_loss=0.002201154115170006
Epoch 2273
Epoch 2273 :: Batch 0/1
Batch Loss = 0.0021960039072011833
2273, epoch_train_loss=0.0021960039072011833
Epoch 2274
Epoch 2274 :: Batch 0/1
Batch Loss = 0.0022000626232355296
2274, epoch_train_loss=0.0022000626232355296
Epoch 2275
Epoch 2275 :: Batch 0/1
Batch Loss = 0.0022075665657715566
2275, epoch_train_loss=0.0022075665657715566
Epoch 2276
Epoch 2276 :: Batch 0/1
Batch Loss = 0.0022118894938006977
2276, epoch_train_loss=0.0022118894938006977
Epoch 2277
Epoch 2277 :: Batch 0/1
Batch Loss = 0.0022100989932451734
2277, epoch_train_loss=0.0022100989932451734
Epoch 2278
Epoch 2278 :: Batch 0/1
Batch Loss = 0.0022043032286677183
2278, epoch_train_loss=0.0022043032286677183
Epoch 2279
Epoch 2279 :: Batch 0/1
Batch Loss = 0.002198787929035721
2279, epoch_train_loss=0.002198787929035721
Epoch 2280
Epoch 2280 :: Batch 0/1
Batch Loss = 0.002196471015759867
2280, epoch_train_loss=0.002196471015759867
Epoch 2281
Epoch 2281 :: Batch 0/1
Batch Loss = 0.0021972730706960064
2281, epoch_train_loss=0.0021972730706960064
Epoch 2282
Epoch 2282 :: Batch 0/1
Batch Loss = 0.002198966751344162
2282, epoch_train_loss=0.002198966751344162
Epoch 2283
Epoch 2283 :: Batch 0/1
Batch Loss = 0.002199627029281628
2283, epoch_train_loss=0.002199627029281628
Epoch 2284
Epoch 2284 :: Batch 0/1
Batch Loss = 0.0021989139930408863
2284, epoch_train_loss=0.0021989139930408863
Epoch 2285
Epoch 2285 :: Batch 0/1
Batch Loss = 0.0021978023908936412
2285, epoch_train_loss=0.0021978023908936412
Epoch 2286
Epoch 2286 :: Batch 0/1
Batch Loss = 0.0021971945962335902
2286, epoch_train_loss=0.0021971945962335902
Epoch 2287
Epoch 2287 :: Batch 0/1
Batch Loss = 0.0021970734032497876
2287, epoch_train_loss=0.0021970734032497876
Epoch 2288
Epoch 2288 :: Batch 0/1
Batch Loss = 0.0021968872044204626
2288, epoch_train_loss=0.0021968872044204626
Epoch 2289
Epoch 2289 :: Batch 0/1
Batch Loss = 0.0021961127618964965
2289, epoch_train_loss=0.0021961127618964965
Epoch 2290
Epoch 2290 :: Batch 0/1
Batch Loss = 0.002194885601258761
2290, epoch_train_loss=0.002194885601258761
Epoch 2291
Epoch 2291 :: Batch 0/1
Batch Loss = 0.0021937710368651617
2291, epoch_train_loss=0.0021937710368651617
Epoch 2292
Epoch 2292 :: Batch 0/1
Batch Loss = 0.002193352758205899
2292, epoch_train_loss=0.002193352758205899
Epoch 2293
Epoch 2293 :: Batch 0/1
Batch Loss = 0.0021937583251394515
2293, epoch_train_loss=0.0021937583251394515
Epoch 2294
Epoch 2294 :: Batch 0/1
Batch Loss = 0.00219455638766458
2294, epoch_train_loss=0.00219455638766458
Epoch 2295
Epoch 2295 :: Batch 0/1
Batch Loss = 0.002195088917272061
2295, epoch_train_loss=0.002195088917272061
Epoch 2296
Epoch 2296 :: Batch 0/1
Batch Loss = 0.0021948902131839953
2296, epoch_train_loss=0.0021948902131839953
Epoch 2297
Epoch 2297 :: Batch 0/1
Batch Loss = 0.002194047926286668
2297, epoch_train_loss=0.002194047926286668
Epoch 2298
Epoch 2298 :: Batch 0/1
Batch Loss = 0.0021929959487951268
2298, epoch_train_loss=0.0021929959487951268
Epoch 2299
Epoch 2299 :: Batch 0/1
Batch Loss = 0.0021922253994104334
2299, epoch_train_loss=0.0021922253994104334
Epoch 2300
Epoch 2300 :: Batch 0/1
Batch Loss = 0.002191948032830521
2300, epoch_train_loss=0.002191948032830521
Epoch 2301
Epoch 2301 :: Batch 0/1
Batch Loss = 0.002192067714437992
2301, epoch_train_loss=0.002192067714437992
Epoch 2302
Epoch 2302 :: Batch 0/1
Batch Loss = 0.002192319756662847
2302, epoch_train_loss=0.002192319756662847
Epoch 2303
Epoch 2303 :: Batch 0/1
Batch Loss = 0.0021924541442382268
2303, epoch_train_loss=0.0021924541442382268
Epoch 2304
Epoch 2304 :: Batch 0/1
Batch Loss = 0.002192365767061524
2304, epoch_train_loss=0.002192365767061524
Epoch 2305
Epoch 2305 :: Batch 0/1
Batch Loss = 0.002192100132612793
2305, epoch_train_loss=0.002192100132612793
Epoch 2306
Epoch 2306 :: Batch 0/1
Batch Loss = 0.0021917984923503124
2306, epoch_train_loss=0.0021917984923503124
Epoch 2307
Epoch 2307 :: Batch 0/1
Batch Loss = 0.0021915663362146094
2307, epoch_train_loss=0.0021915663362146094
Epoch 2308
Epoch 2308 :: Batch 0/1
Batch Loss = 0.002191424333539684
2308, epoch_train_loss=0.002191424333539684
Epoch 2309
Epoch 2309 :: Batch 0/1
Batch Loss = 0.0021913119245755546
2309, epoch_train_loss=0.0021913119245755546
Epoch 2310
Epoch 2310 :: Batch 0/1
Batch Loss = 0.0021911635456547565
2310, epoch_train_loss=0.0021911635456547565
Epoch 2311
Epoch 2311 :: Batch 0/1
Batch Loss = 0.002190958088770582
2311, epoch_train_loss=0.002190958088770582
Epoch 2312
Epoch 2312 :: Batch 0/1
Batch Loss = 0.0021907235489423494
2312, epoch_train_loss=0.0021907235489423494
Epoch 2313
Epoch 2313 :: Batch 0/1
Batch Loss = 0.0021905119024171094
2313, epoch_train_loss=0.0021905119024171094
Epoch 2314
Epoch 2314 :: Batch 0/1
Batch Loss = 0.0021903658421261914
2314, epoch_train_loss=0.0021903658421261914
Epoch 2315
Epoch 2315 :: Batch 0/1
Batch Loss = 0.002190297653968022
2315, epoch_train_loss=0.002190297653968022
Epoch 2316
Epoch 2316 :: Batch 0/1
Batch Loss = 0.0021902810542459993
2316, epoch_train_loss=0.0021902810542459993
Epoch 2317
Epoch 2317 :: Batch 0/1
Batch Loss = 0.0021902667287011643
2317, epoch_train_loss=0.0021902667287011643
Epoch 2318
Epoch 2318 :: Batch 0/1
Batch Loss = 0.002190209937674605
2318, epoch_train_loss=0.002190209937674605
Epoch 2319
Epoch 2319 :: Batch 0/1
Batch Loss = 0.002190094864581936
2319, epoch_train_loss=0.002190094864581936
Epoch 2320
Epoch 2320 :: Batch 0/1
Batch Loss = 0.0021899354798918915
2320, epoch_train_loss=0.0021899354798918915
Epoch 2321
Epoch 2321 :: Batch 0/1
Batch Loss = 0.0021897608130776063
2321, epoch_train_loss=0.0021897608130776063
Epoch 2322
Epoch 2322 :: Batch 0/1
Batch Loss = 0.002189596653587073
2322, epoch_train_loss=0.002189596653587073
Epoch 2323
Epoch 2323 :: Batch 0/1
Batch Loss = 0.0021894576437121987
2323, epoch_train_loss=0.0021894576437121987
Epoch 2324
Epoch 2324 :: Batch 0/1
Batch Loss = 0.002189343616464671
2324, epoch_train_loss=0.002189343616464671
Epoch 2325
Epoch 2325 :: Batch 0/1
Batch Loss = 0.002189244287743698
2325, epoch_train_loss=0.002189244287743698
Epoch 2326
Epoch 2326 :: Batch 0/1
Batch Loss = 0.0021891450504565825
2326, epoch_train_loss=0.0021891450504565825
Epoch 2327
Epoch 2327 :: Batch 0/1
Batch Loss = 0.0021890371867043274
2327, epoch_train_loss=0.0021890371867043274
Epoch 2328
Epoch 2328 :: Batch 0/1
Batch Loss = 0.0021889208606400154
2328, epoch_train_loss=0.0021889208606400154
Epoch 2329
Epoch 2329 :: Batch 0/1
Batch Loss = 0.0021888031827920922
2329, epoch_train_loss=0.0021888031827920922
Epoch 2330
Epoch 2330 :: Batch 0/1
Batch Loss = 0.0021886915934798907
2330, epoch_train_loss=0.0021886915934798907
Epoch 2331
Epoch 2331 :: Batch 0/1
Batch Loss = 0.002188590367031519
2331, epoch_train_loss=0.002188590367031519
Epoch 2332
Epoch 2332 :: Batch 0/1
Batch Loss = 0.0021884997650470513
2332, epoch_train_loss=0.0021884997650470513
Epoch 2333
Epoch 2333 :: Batch 0/1
Batch Loss = 0.002188416385643001
2333, epoch_train_loss=0.002188416385643001
Epoch 2334
Epoch 2334 :: Batch 0/1
Batch Loss = 0.0021883349148737047
2334, epoch_train_loss=0.0021883349148737047
Epoch 2335
Epoch 2335 :: Batch 0/1
Batch Loss = 0.002188249677946889
2335, epoch_train_loss=0.002188249677946889
Epoch 2336
Epoch 2336 :: Batch 0/1
Batch Loss = 0.0021881582470312778
2336, epoch_train_loss=0.0021881582470312778
Epoch 2337
Epoch 2337 :: Batch 0/1
Batch Loss = 0.0021880609314059787
2337, epoch_train_loss=0.0021880609314059787
Epoch 2338
Epoch 2338 :: Batch 0/1
Batch Loss = 0.0021879608715557647
2338, epoch_train_loss=0.0021879608715557647
Epoch 2339
Epoch 2339 :: Batch 0/1
Batch Loss = 0.00218786069456436
2339, epoch_train_loss=0.00218786069456436
Epoch 2340
Epoch 2340 :: Batch 0/1
Batch Loss = 0.0021877631323296842
2340, epoch_train_loss=0.0021877631323296842
Epoch 2341
Epoch 2341 :: Batch 0/1
Batch Loss = 0.002187669180475039
2341, epoch_train_loss=0.002187669180475039
Epoch 2342
Epoch 2342 :: Batch 0/1
Batch Loss = 0.0021875793976851125
2342, epoch_train_loss=0.0021875793976851125
Epoch 2343
Epoch 2343 :: Batch 0/1
Batch Loss = 0.0021874925030011465
2343, epoch_train_loss=0.0021874925030011465
Epoch 2344
Epoch 2344 :: Batch 0/1
Batch Loss = 0.0021874075908261386
2344, epoch_train_loss=0.0021874075908261386
Epoch 2345
Epoch 2345 :: Batch 0/1
Batch Loss = 0.0021873236330553028
2345, epoch_train_loss=0.0021873236330553028
Epoch 2346
Epoch 2346 :: Batch 0/1
Batch Loss = 0.002187241371908866
2346, epoch_train_loss=0.002187241371908866
Epoch 2347
Epoch 2347 :: Batch 0/1
Batch Loss = 0.002187161576376485
2347, epoch_train_loss=0.002187161576376485
Epoch 2348
Epoch 2348 :: Batch 0/1
Batch Loss = 0.0021870866382150754
2348, epoch_train_loss=0.0021870866382150754
Epoch 2349
Epoch 2349 :: Batch 0/1
Batch Loss = 0.0021870185770220167
2349, epoch_train_loss=0.0021870185770220167
Epoch 2350
Epoch 2350 :: Batch 0/1
Batch Loss = 0.0021869615711717027
2350, epoch_train_loss=0.0021869615711717027
Epoch 2351
Epoch 2351 :: Batch 0/1
Batch Loss = 0.0021869188716406346
2351, epoch_train_loss=0.0021869188716406346
Epoch 2352
Epoch 2352 :: Batch 0/1
Batch Loss = 0.0021868970909298567
2352, epoch_train_loss=0.0021868970909298567
Epoch 2353
Epoch 2353 :: Batch 0/1
Batch Loss = 0.0021869018362378366
2353, epoch_train_loss=0.0021869018362378366
Epoch 2354
Epoch 2354 :: Batch 0/1
Batch Loss = 0.0021869460601699744
2354, epoch_train_loss=0.0021869460601699744
Epoch 2355
Epoch 2355 :: Batch 0/1
Batch Loss = 0.0021870420512977227
2355, epoch_train_loss=0.0021870420512977227
Epoch 2356
Epoch 2356 :: Batch 0/1
Batch Loss = 0.0021872176323456723
2356, epoch_train_loss=0.0021872176323456723
Epoch 2357
Epoch 2357 :: Batch 0/1
Batch Loss = 0.002187500061915885
2357, epoch_train_loss=0.002187500061915885
Epoch 2358
Epoch 2358 :: Batch 0/1
Batch Loss = 0.0021879515311370975
2358, epoch_train_loss=0.0021879515311370975
Epoch 2359
Epoch 2359 :: Batch 0/1
Batch Loss = 0.002188632953938065
2359, epoch_train_loss=0.002188632953938065
Epoch 2360
Epoch 2360 :: Batch 0/1
Batch Loss = 0.002189688430358836
2360, epoch_train_loss=0.002189688430358836
Epoch 2361
Epoch 2361 :: Batch 0/1
Batch Loss = 0.0021912543872473285
2361, epoch_train_loss=0.0021912543872473285
Epoch 2362
Epoch 2362 :: Batch 0/1
Batch Loss = 0.0021936797043879156
2362, epoch_train_loss=0.0021936797043879156
Epoch 2363
Epoch 2363 :: Batch 0/1
Batch Loss = 0.002197271538333683
2363, epoch_train_loss=0.002197271538333683
Epoch 2364
Epoch 2364 :: Batch 0/1
Batch Loss = 0.0022029165214088645
2364, epoch_train_loss=0.0022029165214088645
Epoch 2365
Epoch 2365 :: Batch 0/1
Batch Loss = 0.0022112962297159105
2365, epoch_train_loss=0.0022112962297159105
Epoch 2366
Epoch 2366 :: Batch 0/1
Batch Loss = 0.002224779117349481
2366, epoch_train_loss=0.002224779117349481
Epoch 2367
Epoch 2367 :: Batch 0/1
Batch Loss = 0.0022447917603537247
2367, epoch_train_loss=0.0022447917603537247
Epoch 2368
Epoch 2368 :: Batch 0/1
Batch Loss = 0.002277998516775213
2368, epoch_train_loss=0.002277998516775213
Epoch 2369
Epoch 2369 :: Batch 0/1
Batch Loss = 0.0023268686421581775
2369, epoch_train_loss=0.0023268686421581775
Epoch 2370
Epoch 2370 :: Batch 0/1
Batch Loss = 0.0024111669731246028
2370, epoch_train_loss=0.0024111669731246028
Epoch 2371
Epoch 2371 :: Batch 0/1
Batch Loss = 0.002532114790362064
2371, epoch_train_loss=0.002532114790362064
Epoch 2372
Epoch 2372 :: Batch 0/1
Batch Loss = 0.0027511583954941674
2372, epoch_train_loss=0.0027511583954941674
Epoch 2373
Epoch 2373 :: Batch 0/1
Batch Loss = 0.0030475280907597017
2373, epoch_train_loss=0.0030475280907597017
Epoch 2374
Epoch 2374 :: Batch 0/1
Batch Loss = 0.003617492409647974
2374, epoch_train_loss=0.003617492409647974
Epoch 2375
Epoch 2375 :: Batch 0/1
Batch Loss = 0.0042949367603751926
2375, epoch_train_loss=0.0042949367603751926
Epoch 2376
Epoch 2376 :: Batch 0/1
Batch Loss = 0.005692262458137181
2376, epoch_train_loss=0.005692262458137181
Epoch 2377
Epoch 2377 :: Batch 0/1
Batch Loss = 0.006914074759207488
2377, epoch_train_loss=0.006914074759207488
Epoch 2378
Epoch 2378 :: Batch 0/1
Batch Loss = 0.009645030519086911
2378, epoch_train_loss=0.009645030519086911
Epoch 2379
Epoch 2379 :: Batch 0/1
Batch Loss = 0.010484186629805775
2379, epoch_train_loss=0.010484186629805775
Epoch 2380
Epoch 2380 :: Batch 0/1
Batch Loss = 0.012993941746313476
2380, epoch_train_loss=0.012993941746313476
Epoch 2381
Epoch 2381 :: Batch 0/1
Batch Loss = 0.010729243462181912
2381, epoch_train_loss=0.010729243462181912
Epoch 2382
Epoch 2382 :: Batch 0/1
Batch Loss = 0.00892619723389296
2382, epoch_train_loss=0.00892619723389296
Epoch 2383
Epoch 2383 :: Batch 0/1
Batch Loss = 0.0051260750447503865
2383, epoch_train_loss=0.0051260750447503865
Epoch 2384
Epoch 2384 :: Batch 0/1
Batch Loss = 0.002793473919288769
2384, epoch_train_loss=0.002793473919288769
Epoch 2385
Epoch 2385 :: Batch 0/1
Batch Loss = 0.002614838407671342
2385, epoch_train_loss=0.002614838407671342
Epoch 2386
Epoch 2386 :: Batch 0/1
Batch Loss = 0.00407788875007098
2386, epoch_train_loss=0.00407788875007098
Epoch 2387
Epoch 2387 :: Batch 0/1
Batch Loss = 0.005985553525858055
2387, epoch_train_loss=0.005985553525858055
Epoch 2388
Epoch 2388 :: Batch 0/1
Batch Loss = 0.005945049842225218
2388, epoch_train_loss=0.005945049842225218
Epoch 2389
Epoch 2389 :: Batch 0/1
Batch Loss = 0.004839488713407012
2389, epoch_train_loss=0.004839488713407012
Epoch 2390
Epoch 2390 :: Batch 0/1
Batch Loss = 0.003008618566874659
2390, epoch_train_loss=0.003008618566874659
Epoch 2391
Epoch 2391 :: Batch 0/1
Batch Loss = 0.0023123556891637173
2391, epoch_train_loss=0.0023123556891637173
Epoch 2392
Epoch 2392 :: Batch 0/1
Batch Loss = 0.0029863014416010793
2392, epoch_train_loss=0.0029863014416010793
Epoch 2393
Epoch 2393 :: Batch 0/1
Batch Loss = 0.003992195658799285
2393, epoch_train_loss=0.003992195658799285
Epoch 2394
Epoch 2394 :: Batch 0/1
Batch Loss = 0.0045398474482653516
2394, epoch_train_loss=0.0045398474482653516
Epoch 2395
Epoch 2395 :: Batch 0/1
Batch Loss = 0.0038708306336221166
2395, epoch_train_loss=0.0038708306336221166
Epoch 2396
Epoch 2396 :: Batch 0/1
Batch Loss = 0.002917269976077948
2396, epoch_train_loss=0.002917269976077948
Epoch 2397
Epoch 2397 :: Batch 0/1
Batch Loss = 0.002299811033359539
2397, epoch_train_loss=0.002299811033359539
Epoch 2398
Epoch 2398 :: Batch 0/1
Batch Loss = 0.002425641867985307
2398, epoch_train_loss=0.002425641867985307
Epoch 2399
Epoch 2399 :: Batch 0/1
Batch Loss = 0.002972514763597339
2399, epoch_train_loss=0.002972514763597339
Epoch 2400
Epoch 2400 :: Batch 0/1
Batch Loss = 0.003268482165007875
2400, epoch_train_loss=0.003268482165007875
Epoch 2401
Epoch 2401 :: Batch 0/1
Batch Loss = 0.0031510183030140724
2401, epoch_train_loss=0.0031510183030140724
Epoch 2402
Epoch 2402 :: Batch 0/1
Batch Loss = 0.0026645897428944727
2402, epoch_train_loss=0.0026645897428944727
Epoch 2403
Epoch 2403 :: Batch 0/1
Batch Loss = 0.002291559952395561
2403, epoch_train_loss=0.002291559952395561
Epoch 2404
Epoch 2404 :: Batch 0/1
Batch Loss = 0.0022695545948493525
2404, epoch_train_loss=0.0022695545948493525
Epoch 2405
Epoch 2405 :: Batch 0/1
Batch Loss = 0.0025180252025112284
2405, epoch_train_loss=0.0025180252025112284
Epoch 2406
Epoch 2406 :: Batch 0/1
Batch Loss = 0.0027524995700082913
2406, epoch_train_loss=0.0027524995700082913
Epoch 2407
Epoch 2407 :: Batch 0/1
Batch Loss = 0.002713219848062095
2407, epoch_train_loss=0.002713219848062095
Epoch 2408
Epoch 2408 :: Batch 0/1
Batch Loss = 0.00249164806301354
2408, epoch_train_loss=0.00249164806301354
Epoch 2409
Epoch 2409 :: Batch 0/1
Batch Loss = 0.002271480999799016
2409, epoch_train_loss=0.002271480999799016
Epoch 2410
Epoch 2410 :: Batch 0/1
Batch Loss = 0.0022292579868399658
2410, epoch_train_loss=0.0022292579868399658
Epoch 2411
Epoch 2411 :: Batch 0/1
Batch Loss = 0.0023465944879727313
2411, epoch_train_loss=0.0023465944879727313
Epoch 2412
Epoch 2412 :: Batch 0/1
Batch Loss = 0.002476300744767311
2412, epoch_train_loss=0.002476300744767311
Epoch 2413
Epoch 2413 :: Batch 0/1
Batch Loss = 0.0025056374116862084
2413, epoch_train_loss=0.0025056374116862084
Epoch 2414
Epoch 2414 :: Batch 0/1
Batch Loss = 0.002402526415949448
2414, epoch_train_loss=0.002402526415949448
Epoch 2415
Epoch 2415 :: Batch 0/1
Batch Loss = 0.002275676614025621
2415, epoch_train_loss=0.002275676614025621
Epoch 2416
Epoch 2416 :: Batch 0/1
Batch Loss = 0.0022153538000291126
2416, epoch_train_loss=0.0022153538000291126
Epoch 2417
Epoch 2417 :: Batch 0/1
Batch Loss = 0.0022499202498010447
2417, epoch_train_loss=0.0022499202498010447
Epoch 2418
Epoch 2418 :: Batch 0/1
Batch Loss = 0.0023251086971556325
2418, epoch_train_loss=0.0023251086971556325
Epoch 2419
Epoch 2419 :: Batch 0/1
Batch Loss = 0.0023630174185902456
2419, epoch_train_loss=0.0023630174185902456
Epoch 2420
Epoch 2420 :: Batch 0/1
Batch Loss = 0.002340090316630095
2420, epoch_train_loss=0.002340090316630095
Epoch 2421
Epoch 2421 :: Batch 0/1
Batch Loss = 0.0022740017358415126
2421, epoch_train_loss=0.0022740017358415126
Epoch 2422
Epoch 2422 :: Batch 0/1
Batch Loss = 0.0022216255023741046
2422, epoch_train_loss=0.0022216255023741046
Epoch 2423
Epoch 2423 :: Batch 0/1
Batch Loss = 0.0022105428254812227
2423, epoch_train_loss=0.0022105428254812227
Epoch 2424
Epoch 2424 :: Batch 0/1
Batch Loss = 0.0022350633634600383
2424, epoch_train_loss=0.0022350633634600383
Epoch 2425
Epoch 2425 :: Batch 0/1
Batch Loss = 0.0022665944377491362
2425, epoch_train_loss=0.0022665944377491362
Epoch 2426
Epoch 2426 :: Batch 0/1
Batch Loss = 0.002277636043207269
2426, epoch_train_loss=0.002277636043207269
Epoch 2427
Epoch 2427 :: Batch 0/1
Batch Loss = 0.0022644878921637083
2427, epoch_train_loss=0.0022644878921637083
Epoch 2428
Epoch 2428 :: Batch 0/1
Batch Loss = 0.002234733343301559
2428, epoch_train_loss=0.002234733343301559
Epoch 2429
Epoch 2429 :: Batch 0/1
Batch Loss = 0.0022093203671426274
2429, epoch_train_loss=0.0022093203671426274
Epoch 2430
Epoch 2430 :: Batch 0/1
Batch Loss = 0.002199436005707309
2430, epoch_train_loss=0.002199436005707309
Epoch 2431
Epoch 2431 :: Batch 0/1
Batch Loss = 0.002205944420949225
2431, epoch_train_loss=0.002205944420949225
Epoch 2432
Epoch 2432 :: Batch 0/1
Batch Loss = 0.0022199436169244363
2432, epoch_train_loss=0.0022199436169244363
Epoch 2433
Epoch 2433 :: Batch 0/1
Batch Loss = 0.0022299789729272965
2433, epoch_train_loss=0.0022299789729272965
Epoch 2434
Epoch 2434 :: Batch 0/1
Batch Loss = 0.0022308810240568764
2434, epoch_train_loss=0.0022308810240568764
Epoch 2435
Epoch 2435 :: Batch 0/1
Batch Loss = 0.0022214476942349435
2435, epoch_train_loss=0.0022214476942349435
Epoch 2436
Epoch 2436 :: Batch 0/1
Batch Loss = 0.0022082724867474863
2436, epoch_train_loss=0.0022082724867474863
Epoch 2437
Epoch 2437 :: Batch 0/1
Batch Loss = 0.002196639981130038
2437, epoch_train_loss=0.002196639981130038
Epoch 2438
Epoch 2438 :: Batch 0/1
Batch Loss = 0.0021912467252677903
2438, epoch_train_loss=0.0021912467252677903
Epoch 2439
Epoch 2439 :: Batch 0/1
Batch Loss = 0.0021927089881815806
2439, epoch_train_loss=0.0021927089881815806
Epoch 2440
Epoch 2440 :: Batch 0/1
Batch Loss = 0.0021984059072552705
2440, epoch_train_loss=0.0021984059072552705
Epoch 2441
Epoch 2441 :: Batch 0/1
Batch Loss = 0.0022044499209379785
2441, epoch_train_loss=0.0022044499209379785
Epoch 2442
Epoch 2442 :: Batch 0/1
Batch Loss = 0.002207272216163819
2442, epoch_train_loss=0.002207272216163819
Epoch 2443
Epoch 2443 :: Batch 0/1
Batch Loss = 0.0022062443097655357
2443, epoch_train_loss=0.0022062443097655357
Epoch 2444
Epoch 2444 :: Batch 0/1
Batch Loss = 0.002201630036928651
2444, epoch_train_loss=0.002201630036928651
Epoch 2445
Epoch 2445 :: Batch 0/1
Batch Loss = 0.002195910927032533
2445, epoch_train_loss=0.002195910927032533
Epoch 2446
Epoch 2446 :: Batch 0/1
Batch Loss = 0.002190931575966225
2446, epoch_train_loss=0.002190931575966225
Epoch 2447
Epoch 2447 :: Batch 0/1
Batch Loss = 0.0021882266344382625
2447, epoch_train_loss=0.0021882266344382625
Epoch 2448
Epoch 2448 :: Batch 0/1
Batch Loss = 0.002188039484991145
2448, epoch_train_loss=0.002188039484991145
Epoch 2449
Epoch 2449 :: Batch 0/1
Batch Loss = 0.0021896476465717574
2449, epoch_train_loss=0.0021896476465717574
Epoch 2450
Epoch 2450 :: Batch 0/1
Batch Loss = 0.002191900436033977
2450, epoch_train_loss=0.002191900436033977
Epoch 2451
Epoch 2451 :: Batch 0/1
Batch Loss = 0.0021937141866330673
2451, epoch_train_loss=0.0021937141866330673
Epoch 2452
Epoch 2452 :: Batch 0/1
Batch Loss = 0.0021945819367740066
2452, epoch_train_loss=0.0021945819367740066
Epoch 2453
Epoch 2453 :: Batch 0/1
Batch Loss = 0.0021942200931743417
2453, epoch_train_loss=0.0021942200931743417
Epoch 2454
Epoch 2454 :: Batch 0/1
Batch Loss = 0.0021929812333351083
2454, epoch_train_loss=0.0021929812333351083
Epoch 2455
Epoch 2455 :: Batch 0/1
Batch Loss = 0.0021911671036817515
2455, epoch_train_loss=0.0021911671036817515
Epoch 2456
Epoch 2456 :: Batch 0/1
Batch Loss = 0.00218933338853974
2456, epoch_train_loss=0.00218933338853974
Epoch 2457
Epoch 2457 :: Batch 0/1
Batch Loss = 0.002187808523686855
2457, epoch_train_loss=0.002187808523686855
Epoch 2458
Epoch 2458 :: Batch 0/1
Batch Loss = 0.002186821148953244
2458, epoch_train_loss=0.002186821148953244
Epoch 2459
Epoch 2459 :: Batch 0/1
Batch Loss = 0.002186424023253646
2459, epoch_train_loss=0.002186424023253646
Epoch 2460
Epoch 2460 :: Batch 0/1
Batch Loss = 0.002186545870607351
2460, epoch_train_loss=0.002186545870607351
Epoch 2461
Epoch 2461 :: Batch 0/1
Batch Loss = 0.0021870110902661513
2461, epoch_train_loss=0.0021870110902661513
Epoch 2462
Epoch 2462 :: Batch 0/1
Batch Loss = 0.0021875836334125285
2462, epoch_train_loss=0.0021875836334125285
Epoch 2463
Epoch 2463 :: Batch 0/1
Batch Loss = 0.0021880607120678642
2463, epoch_train_loss=0.0021880607120678642
Epoch 2464
Epoch 2464 :: Batch 0/1
Batch Loss = 0.0021882916561393392
2464, epoch_train_loss=0.0021882916561393392
Epoch 2465
Epoch 2465 :: Batch 0/1
Batch Loss = 0.0021882531580045147
2465, epoch_train_loss=0.0021882531580045147
Epoch 2466
Epoch 2466 :: Batch 0/1
Batch Loss = 0.002187943910635144
2466, epoch_train_loss=0.002187943910635144
Epoch 2467
Epoch 2467 :: Batch 0/1
Batch Loss = 0.002187460264986688
2467, epoch_train_loss=0.002187460264986688
Epoch 2468
Epoch 2468 :: Batch 0/1
Batch Loss = 0.002186877358186437
2468, epoch_train_loss=0.002186877358186437
Epoch 2469
Epoch 2469 :: Batch 0/1
Batch Loss = 0.0021863043185826898
2469, epoch_train_loss=0.0021863043185826898
Epoch 2470
Epoch 2470 :: Batch 0/1
Batch Loss = 0.0021857935277135275
2470, epoch_train_loss=0.0021857935277135275
Epoch 2471
Epoch 2471 :: Batch 0/1
Batch Loss = 0.002185381633651323
2471, epoch_train_loss=0.002185381633651323
Epoch 2472
Epoch 2472 :: Batch 0/1
Batch Loss = 0.002185074338728618
2472, epoch_train_loss=0.002185074338728618
Epoch 2473
Epoch 2473 :: Batch 0/1
Batch Loss = 0.0021848704096453417
2473, epoch_train_loss=0.0021848704096453417
Epoch 2474
Epoch 2474 :: Batch 0/1
Batch Loss = 0.0021847564418988564
2474, epoch_train_loss=0.0021847564418988564
Epoch 2475
Epoch 2475 :: Batch 0/1
Batch Loss = 0.002184710304705701
2475, epoch_train_loss=0.002184710304705701
Epoch 2476
Epoch 2476 :: Batch 0/1
Batch Loss = 0.002184707293292144
2476, epoch_train_loss=0.002184707293292144
Epoch 2477
Epoch 2477 :: Batch 0/1
Batch Loss = 0.002184724740979214
2477, epoch_train_loss=0.002184724740979214
Epoch 2478
Epoch 2478 :: Batch 0/1
Batch Loss = 0.0021847449355649855
2478, epoch_train_loss=0.0021847449355649855
Epoch 2479
Epoch 2479 :: Batch 0/1
Batch Loss = 0.0021847489697592714
2479, epoch_train_loss=0.0021847489697592714
Epoch 2480
Epoch 2480 :: Batch 0/1
Batch Loss = 0.0021847283950775336
2480, epoch_train_loss=0.0021847283950775336
Epoch 2481
Epoch 2481 :: Batch 0/1
Batch Loss = 0.0021846757294838324
2481, epoch_train_loss=0.0021846757294838324
Epoch 2482
Epoch 2482 :: Batch 0/1
Batch Loss = 0.0021845986539170024
2482, epoch_train_loss=0.0021845986539170024
Epoch 2483
Epoch 2483 :: Batch 0/1
Batch Loss = 0.0021844974342704717
2483, epoch_train_loss=0.0021844974342704717
Epoch 2484
Epoch 2484 :: Batch 0/1
Batch Loss = 0.002184382539891264
2484, epoch_train_loss=0.002184382539891264
Epoch 2485
Epoch 2485 :: Batch 0/1
Batch Loss = 0.0021842546292252475
2485, epoch_train_loss=0.0021842546292252475
Epoch 2486
Epoch 2486 :: Batch 0/1
Batch Loss = 0.002184123965593654
2486, epoch_train_loss=0.002184123965593654
Epoch 2487
Epoch 2487 :: Batch 0/1
Batch Loss = 0.0021839899818300655
2487, epoch_train_loss=0.0021839899818300655
Epoch 2488
Epoch 2488 :: Batch 0/1
Batch Loss = 0.0021838584647513658
2488, epoch_train_loss=0.0021838584647513658
Epoch 2489
Epoch 2489 :: Batch 0/1
Batch Loss = 0.002183727052009957
2489, epoch_train_loss=0.002183727052009957
Epoch 2490
Epoch 2490 :: Batch 0/1
Batch Loss = 0.0021836010774378936
2490, epoch_train_loss=0.0021836010774378936
Epoch 2491
Epoch 2491 :: Batch 0/1
Batch Loss = 0.00218347953253365
2491, epoch_train_loss=0.00218347953253365
Epoch 2492
Epoch 2492 :: Batch 0/1
Batch Loss = 0.0021833668346548257
2492, epoch_train_loss=0.0021833668346548257
Epoch 2493
Epoch 2493 :: Batch 0/1
Batch Loss = 0.0021832612601474484
2493, epoch_train_loss=0.0021832612601474484
Epoch 2494
Epoch 2494 :: Batch 0/1
Batch Loss = 0.0021831670348321877
2494, epoch_train_loss=0.0021831670348321877
Epoch 2495
Epoch 2495 :: Batch 0/1
Batch Loss = 0.0021830829377655664
2495, epoch_train_loss=0.0021830829377655664
Epoch 2496
Epoch 2496 :: Batch 0/1
Batch Loss = 0.002183013372100551
2496, epoch_train_loss=0.002183013372100551
Epoch 2497
Epoch 2497 :: Batch 0/1
Batch Loss = 0.0021829568773824404
2497, epoch_train_loss=0.0021829568773824404
Epoch 2498
Epoch 2498 :: Batch 0/1
Batch Loss = 0.0021829197328107988
2498, epoch_train_loss=0.0021829197328107988
Epoch 2499
Epoch 2499 :: Batch 0/1
Batch Loss = 0.0021829025096306498
2499, epoch_train_loss=0.0021829025096306498
