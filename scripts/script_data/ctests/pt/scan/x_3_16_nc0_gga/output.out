/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting "B3LYP_WITH_VWN5 = True" in pyscf_conf.py
  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '
Warning: <pyscf.gto.mole.Mole object at 0x7ffef00426e0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef00426e0> in UKS object of <class 'pyscf.dft.uks.UKS'>
<pyscf.gto.mole.Mole object at 0x7ffef00426e0> [['P', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffef0042d70> [['N', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffef00433d0> [['H', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffef0042a40> [['Li', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffef00424d0> [['O', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffef0042110> [['Cl', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffef0042650> [['Al', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffef0042200> [['S', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffef00417e0> [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef0041a80> [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef0041a20> [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef0040190> [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]] 3
<pyscf.gto.mole.Mole object at 0x7ffef0040af0> [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]] 3
<pyscf.gto.mole.Mole object at 0x7ffef0040d60> [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef0040e20> [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef00418a0> [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef0040a60> [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]] 4
<pyscf.gto.mole.Mole object at 0x7ffef00432b0> [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef0043070> [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef00435b0> [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef0041f60> [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef0042b60> [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]] 4
<pyscf.gto.mole.Mole object at 0x7ffef0043280> [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]] 4
<pyscf.gto.mole.Mole object at 0x7ffef0043940> [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]] 2
<pyscf.gto.mole.Mole object at 0x7ffef0043430> [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]] 5
<pyscf.gto.mole.Mole object at 0x7ffef0043d30> [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]] 3
<pyscf.gto.mole.Mole object at 0x7ffef0043d60> [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]] 5
mol:  [['P', array([0., 0., 0.])]]
converged SCF energy = -341.104145992717  <S^2> = 3.7502984  2S+1 = 4.0001492
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0042d70> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0042d70> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-3.47389956e-03 -8.82676818e-04 -2.08411238e-03 ... -1.11301603e+01
 -1.11301603e+01 -1.11301603e+01] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 3)
mol:  [['N', array([0., 0., 0.])]]
converged SCF energy = -54.5289742046675  <S^2> = 3.7524945  2S+1 = 4.0012471
Warning: <pyscf.gto.mole.Mole object at 0x7ffef00433d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef00433d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5016, 22)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.07670570e-03 -5.92340671e-04 -6.66573372e-05 ... -5.03679786e+00
 -5.03679786e+00 -5.03679786e+00] = SCAN,
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5016, 22)
rho shape (4, 4, 2, 5016)
rho_filt shape: (5016,)
get descriptors tdrho.shape=(5016, 3)
mol:  [['H', array([0., 0., 0.])]]
converged SCF energy = -0.49981298400854  <S^2> = 0.75  2S+1 = 2
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0042a40> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0042a40> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 7, 7)
ao.shape (10, 2440, 7)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.59173730e-03 -7.67300285e-04 -6.09330303e-05 ... -7.27736608e-01
 -7.27736608e-01 -7.27736608e-01] = SCAN,
get_data, dm shape = (2, 7, 7)
ao_eval.shape=(4, 2440, 7)
rho shape (4, 4, 2, 2440)
rho_filt shape: (2440,)
get descriptors tdrho.shape=(2440, 3)
mol:  [['Li', array([0., 0., 0.])]]
converged SCF energy = -7.46006188627842  <S^2> = 0.75000049  2S+1 = 2.0000005
Warning: <pyscf.gto.mole.Mole object at 0x7ffef00424d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef00424d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 4592, 22)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-6.71503005e-03 -1.44519923e-03 -1.44519923e-03 ... -1.46899070e-02
 -2.03947707e+00 -2.03947707e+00] = SCAN,
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 4592, 22)
rho shape (4, 4, 2, 4592)
rho_filt shape: (4592,)
get descriptors tdrho.shape=(4592, 3)
mol:  [['O', array([0., 0., 0.])]]
converged SCF energy = -75.003377443081  <S^2> = 2.0027453  2S+1 = 3.0018296
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0042110> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0042110> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5040, 22)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-7.50108809e-04 -1.24504028e-04 -6.21637250e-06 ... -5.78449381e+00
 -5.78449381e+00 -5.78449381e+00] = SCAN,
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5040, 22)
rho shape (4, 4, 2, 5040)
rho_filt shape: (5040,)
get descriptors tdrho.shape=(5040, 3)
mol:  [['Cl', array([0., 0., 0.])]]
converged SCF energy = -459.957577121376  <S^2> = 0.7516194  2S+1 = 2.0016187
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0042650> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0042650> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6152, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.06046800e-03 -9.79646678e-04 -3.40075764e-04 ... -1.26648275e+01
 -1.26648275e+01 -1.26648275e+01] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6152, 30)
rho shape (4, 4, 2, 6152)
rho_filt shape: (6152,)
get descriptors tdrho.shape=(6152, 3)
mol:  [['Al', array([0., 0., 0.])]]
converged SCF energy = -242.226560983335  <S^2> = 0.75226414  2S+1 = 2.0022629
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0042200> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0042200> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6088, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-0.02415715 -0.01560148 -0.00795622 ... -0.000139   -0.00176837
 -0.00012769] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6088, 30)
rho shape (4, 4, 2, 6088)
rho_filt shape: (6088,)
get descriptors tdrho.shape=(6088, 3)
mol:  [['S', array([0., 0., 0.])]]
converged SCF energy = -397.938786821141  <S^2> = 2.0022329  2S+1 = 3.0014882
Warning: <pyscf.gto.mole.Mole object at 0x7ffef00417e0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef00417e0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.08470554e-03 -9.04965301e-04 -9.58520064e-04 ... -1.18986567e+01
 -1.18986567e+01 -1.18986567e+01] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 3)
mol:  [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]]
converged SCF energy = -1.16580491182912  <S^2> = -4.4408921e-16  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0041a80> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0041a80> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 14, 14)
ao.shape (10, 4776, 14)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.31557088e-04 -9.73828620e-06 -3.66768667e-04 ... -5.54165574e-01
 -5.54165574e-01 -5.54165574e-01] = SCAN,
get_data, dm shape = (2, 14, 14)
ao_eval.shape=(4, 4776, 14)
rho shape (4, 4, 2, 4776)
rho_filt shape: (4776,)
get descriptors tdrho.shape=(4776, 3)
mol:  [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]]
converged SCF energy = -109.439263799566  <S^2> = 1.0658141e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0041a20> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0041a20> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9848, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-9.68474977e-05 -9.84742592e-04 -2.59676393e-04 ... -2.39645778e-05
 -2.39645778e-05 -9.68474977e-05] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9848, 44)
rho shape (4, 4, 2, 9848)
rho_filt shape: (9848,)
get descriptors tdrho.shape=(9848, 3)
mol:  [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]]
converged SCF energy = -107.339357395734  <S^2> = -3.5527137e-15  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0040190> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0040190> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9752, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.04987770e-03 -6.68954111e-04 -8.57556562e-04 ... -1.07485605e-03
 -8.01425702e-01 -8.01425702e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9752, 44)
rho shape (4, 4, 2, 9752)
rho_filt shape: (9752,)
get descriptors tdrho.shape=(9752, 3)
mol:  [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]]
converged SCF energy = -93.3377924465132  <S^2> = 4.0072834e-10  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0040af0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0040af0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 12256, 51)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.97917639e-04 -2.54437615e-05 -3.15202008e-05 ... -6.37386388e-01
 -6.37386388e-01 -6.37386388e-01] = SCAN,
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 12256, 51)
rho shape (4, 4, 2, 12256)
rho_filt shape: (12256,)
get descriptors tdrho.shape=(12256, 3)
mol:  [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]]
converged SCF energy = -188.456965322844  <S^2> = 2.3092639e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0040d60> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0040d60> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 66, 66)
ao.shape (10, 14920, 66)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.50217343e-04 -2.07520331e-04 -9.23619961e-04 ... -2.76182455e-06
 -4.27559894e+00 -4.27559894e+00] = SCAN,
get_data, dm shape = (2, 66, 66)
ao_eval.shape=(4, 14920, 66)
rho shape (4, 4, 2, 14920)
rho_filt shape: (14920,)
get descriptors tdrho.shape=(14920, 3)
mol:  [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]]
converged SCF energy = -920.005608888959  <S^2> = 5.0093263e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0040e20> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0040e20> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12208, 60)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-0.00155834 -0.00091688 -0.00215831 ... -0.00091688 -0.41618506
 -0.41618506] = SCAN,
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12208, 60)
rho shape (4, 4, 2, 12208)
rho_filt shape: (12208,)
get descriptors tdrho.shape=(12208, 3)
mol:  [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]]
converged SCF energy = -199.394370591172  <S^2> = 1.1901591e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef00418a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef00418a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9824, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.92948752e-04 -1.95215890e-05 -1.16699780e-03 ... -4.89378326e-01
 -4.89378326e-01 -4.89378326e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9824, 44)
rho shape (4, 4, 2, 9824)
rho_filt shape: (9824,)
get descriptors tdrho.shape=(9824, 3)
mol:  [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]]
converged SCF energy = -150.214894541876  <S^2> = 1.0018599  2S+1 = 2.2377309
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0040a60> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0040a60> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9912, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-6.48686938e-04 -1.14848257e-04 -5.90272062e-06 ... -6.59150583e-01
 -6.59150583e-01 -6.59150583e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9912, 44)
rho shape (4, 4, 2, 9912)
rho_filt shape: (9912,)
get descriptors tdrho.shape=(9912, 3)
mol:  [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]]
converged SCF energy = -77.2435048346375  <S^2> = 8.8817842e-15  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef00432b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef00432b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 58, 58)
ao.shape (10, 15208, 58)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-8.83278187e-05 -8.83278187e-05 -9.75839793e-04 ... -3.46740731e-05
 -3.31729009e-05 -3.31729009e-05] = SCAN,
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15208, 58)
rho shape (4, 4, 2, 15208)
rho_filt shape: (15208,)
get descriptors tdrho.shape=(15208, 3)
mol:  [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]]
converged SCF energy = -113.221335689652  <S^2> = 6.5281114e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0043070> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0043070> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 10040, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-5.37000596e-04 -8.55494373e-04 -2.46853248e-03 ... -7.34251993e-01
 -7.34251993e-01 -7.34251993e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 10040, 44)
rho shape (4, 4, 2, 10040)
rho_filt shape: (10040,)
get descriptors tdrho.shape=(10040, 3)
mol:  [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]]
converged SCF energy = -460.624592374078  <S^2> = 6.2172489e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef00435b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef00435b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 37, 37)
ao.shape (10, 8552, 37)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.38161478e-04 -1.81223966e-05 -2.37327566e-05 ... -2.83738108e+00
 -2.83738108e+00 -2.83738108e+00] = SCAN,
get_data, dm shape = (2, 37, 37)
ao_eval.shape=(4, 8552, 37)
rho shape (4, 4, 2, 8552)
rho_filt shape: (8552,)
get descriptors tdrho.shape=(8552, 3)
mol:  [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]]
converged SCF energy = -8.04458854018922  <S^2> = 7.7049478e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0041f60> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0041f60> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 29, 29)
ao.shape (10, 6936, 29)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-0.00113445 -0.00118624 -0.00118624 ... -0.48434639 -0.48434639
 -0.48434639] = SCAN,
get_data, dm shape = (2, 29, 29)
ao_eval.shape=(4, 6936, 29)
rho shape (4, 4, 2, 6936)
rho_filt shape: (6936,)
get descriptors tdrho.shape=(6936, 3)
mol:  [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]]
converged SCF energy = -324.340512506578  <S^2> = 1.5868196e-11  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0042b60> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0042b60> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 11536, 60)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-0.00297936 -0.00297936 -0.00407091 ... -0.00297936 -0.00297936
 -0.00407091] = SCAN,
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 11536, 60)
rho shape (4, 4, 2, 11536)
rho_filt shape: (11536,)
get descriptors tdrho.shape=(11536, 3)
mol:  [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]]
converged SCF energy = -1622.57507845815  <S^2> = 8.31335e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0043280> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0043280> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 120, 120)
ao.shape (10, 24512, 120)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.61401455e-04 -4.90485117e-04 -2.56451688e-03 ... -9.59296114e+00
 -9.59296114e+00 -9.59296114e+00] = SCAN,
get_data, dm shape = (2, 120, 120)
ao_eval.shape=(4, 24512, 120)
rho shape (4, 4, 2, 24512)
rho_filt shape: (24512,)
get descriptors tdrho.shape=(24512, 3)
mol:  [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]]
converged SCF energy = -342.979728469575  <S^2> = 2.5387692e-11  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0043940> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0043940> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 13096, 51)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.28637187e-03 -4.32380890e-04 -3.74072638e-05 ... -1.91722763e+00
 -1.91722763e+00 -1.91722763e+00] = SCAN,
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 13096, 51)
rho shape (4, 4, 2, 13096)
rho_filt shape: (13096,)
get descriptors tdrho.shape=(13096, 3)
mol:  [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]]
converged SCF energy = -578.565336647371  <S^2> = 1.0034707  2S+1 = 2.2391701
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0043430> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0043430> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12384, 60)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.60118839e-04 -2.60152115e-04 -2.60145915e-04 ... -3.86943823e-01
 -3.86943823e-01 -3.86943823e-01] = SCAN,
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12384, 60)
rho shape (4, 4, 2, 12384)
rho_filt shape: (12384,)
get descriptors tdrho.shape=(12384, 3)
mol:  [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]]
converged SCF energy = -40.4598214864075  <S^2> = 3.1974423e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0043d30> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0043d30> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 50, 50)
ao.shape (10, 13936, 50)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-8.68439856e-04 -2.42462783e-04 -1.69965237e-05 ... -2.55256081e-05
 -2.55256081e-05 -2.55256081e-05] = SCAN,
get_data, dm shape = (2, 50, 50)
ao_eval.shape=(4, 13936, 50)
rho shape (4, 4, 2, 13936)
rho_filt shape: (13936,)
get descriptors tdrho.shape=(13936, 3)
mol:  [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]]
converged SCF energy = -39.0756147483503  <S^2> = 6.2017058e-12  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffef0043d60> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffef0043d60> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 36, 36)
ao.shape (10, 9656, 36)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-7.67691257e-04 -4.57409182e-05 -2.02835243e-04 ... -1.14928928e+00
 -1.14928928e+00 -1.14928928e+00] = SCAN,
get_data, dm shape = (2, 36, 36)
ao_eval.shape=(4, 9656, 36)
rho shape (4, 4, 2, 9656)
rho_filt shape: (9656,)
get descriptors tdrho.shape=(9656, 3)
mol:  [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]]
converged SCF energy = -291.719272437818  <S^2> = 1.3153922e-11  2S+1 = 1
New DM shape: (2, 58, 58)
ao.shape (10, 15256, 58)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-8.33847724e-04 -2.34902391e-04 -1.75660753e-05 ... -1.92925750e-05
 -1.92925750e-05 -1.92925750e-05] = SCAN,
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15256, 58)
rho shape (4, 4, 2, 15256)
rho_filt shape: (15256,)
get descriptors tdrho.shape=(15256, 3)
PRE NAN FILT: tFxc.shape=(237015,), tdrho.shape=(237015, 3)
nan_filt_rho.shape=(237015,)
nan_filt_fxc.shape=(237015,)
tFxc.shape=(237015,), tdrho.shape=(237015, 3)
inp[0].shape = (237015, 3)
Epoch 0
Epoch 0 :: Batch 0/1
Batch Loss = 5.264168430749761
0, epoch_train_loss=5.264168430749761
Epoch 1
Epoch 1 :: Batch 0/1
Batch Loss = 4.842611944604443
1, epoch_train_loss=4.842611944604443
Epoch 2
Epoch 2 :: Batch 0/1
Batch Loss = 4.260537727536809
2, epoch_train_loss=4.260537727536809
Epoch 3
Epoch 3 :: Batch 0/1
Batch Loss = 3.287587854363113
3, epoch_train_loss=3.287587854363113
Epoch 4
Epoch 4 :: Batch 0/1
Batch Loss = 1.9199244450613997
4, epoch_train_loss=1.9199244450613997
Epoch 5
Epoch 5 :: Batch 0/1
Batch Loss = 1.1158059126402138
5, epoch_train_loss=1.1158059126402138
Epoch 6
Epoch 6 :: Batch 0/1
Batch Loss = 1.8611285689622656
6, epoch_train_loss=1.8611285689622656
Epoch 7
Epoch 7 :: Batch 0/1
Batch Loss = 0.9068584823954158
7, epoch_train_loss=0.9068584823954158
Epoch 8
Epoch 8 :: Batch 0/1
Batch Loss = 0.46517238455863585
8, epoch_train_loss=0.46517238455863585
Epoch 9
Epoch 9 :: Batch 0/1
Batch Loss = 0.8194326463523506
9, epoch_train_loss=0.8194326463523506
Epoch 10
Epoch 10 :: Batch 0/1
Batch Loss = 0.8013306457988211
10, epoch_train_loss=0.8013306457988211
Epoch 11
Epoch 11 :: Batch 0/1
Batch Loss = 0.4861076527408097
11, epoch_train_loss=0.4861076527408097
Epoch 12
Epoch 12 :: Batch 0/1
Batch Loss = 0.23691950274132825
12, epoch_train_loss=0.23691950274132825
Epoch 13
Epoch 13 :: Batch 0/1
Batch Loss = 0.2752623086448307
13, epoch_train_loss=0.2752623086448307
Epoch 14
Epoch 14 :: Batch 0/1
Batch Loss = 0.43282922053774014
14, epoch_train_loss=0.43282922053774014
Epoch 15
Epoch 15 :: Batch 0/1
Batch Loss = 0.3327205661050184
15, epoch_train_loss=0.3327205661050184
Epoch 16
Epoch 16 :: Batch 0/1
Batch Loss = 0.18177770928355802
16, epoch_train_loss=0.18177770928355802
Epoch 17
Epoch 17 :: Batch 0/1
Batch Loss = 0.2183379112253501
17, epoch_train_loss=0.2183379112253501
Epoch 18
Epoch 18 :: Batch 0/1
Batch Loss = 0.2877038170341054
18, epoch_train_loss=0.2877038170341054
Epoch 19
Epoch 19 :: Batch 0/1
Batch Loss = 0.2504436233664692
19, epoch_train_loss=0.2504436233664692
Epoch 20
Epoch 20 :: Batch 0/1
Batch Loss = 0.15884962764334876
20, epoch_train_loss=0.15884962764334876
Epoch 21
Epoch 21 :: Batch 0/1
Batch Loss = 0.12971795794529217
21, epoch_train_loss=0.12971795794529217
Epoch 22
Epoch 22 :: Batch 0/1
Batch Loss = 0.16801183863796376
22, epoch_train_loss=0.16801183863796376
Epoch 23
Epoch 23 :: Batch 0/1
Batch Loss = 0.15697890533632658
23, epoch_train_loss=0.15697890533632658
Epoch 24
Epoch 24 :: Batch 0/1
Batch Loss = 0.0978436717223136
24, epoch_train_loss=0.0978436717223136
Epoch 25
Epoch 25 :: Batch 0/1
Batch Loss = 0.09269149188862225
25, epoch_train_loss=0.09269149188862225
Epoch 26
Epoch 26 :: Batch 0/1
Batch Loss = 0.10929786859063678
26, epoch_train_loss=0.10929786859063678
Epoch 27
Epoch 27 :: Batch 0/1
Batch Loss = 0.08491006164007338
27, epoch_train_loss=0.08491006164007338
Epoch 28
Epoch 28 :: Batch 0/1
Batch Loss = 0.06297634187157407
28, epoch_train_loss=0.06297634187157407
Epoch 29
Epoch 29 :: Batch 0/1
Batch Loss = 0.0641360975610738
29, epoch_train_loss=0.0641360975610738
Epoch 30
Epoch 30 :: Batch 0/1
Batch Loss = 0.06551933550155421
30, epoch_train_loss=0.06551933550155421
Epoch 31
Epoch 31 :: Batch 0/1
Batch Loss = 0.05256026552777695
31, epoch_train_loss=0.05256026552777695
Epoch 32
Epoch 32 :: Batch 0/1
Batch Loss = 0.04966140065035392
32, epoch_train_loss=0.04966140065035392
Epoch 33
Epoch 33 :: Batch 0/1
Batch Loss = 0.04923275431507563
33, epoch_train_loss=0.04923275431507563
Epoch 34
Epoch 34 :: Batch 0/1
Batch Loss = 0.0372179039019891
34, epoch_train_loss=0.0372179039019891
Epoch 35
Epoch 35 :: Batch 0/1
Batch Loss = 0.041236076349237295
35, epoch_train_loss=0.041236076349237295
Epoch 36
Epoch 36 :: Batch 0/1
Batch Loss = 0.040124755953567605
36, epoch_train_loss=0.040124755953567605
Epoch 37
Epoch 37 :: Batch 0/1
Batch Loss = 0.028120065986711924
37, epoch_train_loss=0.028120065986711924
Epoch 38
Epoch 38 :: Batch 0/1
Batch Loss = 0.026660643804528678
38, epoch_train_loss=0.026660643804528678
Epoch 39
Epoch 39 :: Batch 0/1
Batch Loss = 0.032530367021770645
39, epoch_train_loss=0.032530367021770645
Epoch 40
Epoch 40 :: Batch 0/1
Batch Loss = 0.028962472676195124
40, epoch_train_loss=0.028962472676195124
Epoch 41
Epoch 41 :: Batch 0/1
Batch Loss = 0.019705632560211083
41, epoch_train_loss=0.019705632560211083
Epoch 42
Epoch 42 :: Batch 0/1
Batch Loss = 0.022542999737530812
42, epoch_train_loss=0.022542999737530812
Epoch 43
Epoch 43 :: Batch 0/1
Batch Loss = 0.025191067940182253
43, epoch_train_loss=0.025191067940182253
Epoch 44
Epoch 44 :: Batch 0/1
Batch Loss = 0.015759911376571827
44, epoch_train_loss=0.015759911376571827
Epoch 45
Epoch 45 :: Batch 0/1
Batch Loss = 0.013651682772940174
45, epoch_train_loss=0.013651682772940174
Epoch 46
Epoch 46 :: Batch 0/1
Batch Loss = 0.017282922052917298
46, epoch_train_loss=0.017282922052917298
Epoch 47
Epoch 47 :: Batch 0/1
Batch Loss = 0.014624243681895419
47, epoch_train_loss=0.014624243681895419
Epoch 48
Epoch 48 :: Batch 0/1
Batch Loss = 0.012675937488693716
48, epoch_train_loss=0.012675937488693716
Epoch 49
Epoch 49 :: Batch 0/1
Batch Loss = 0.011971975188403657
49, epoch_train_loss=0.011971975188403657
Epoch 50
Epoch 50 :: Batch 0/1
Batch Loss = 0.011775303399553377
50, epoch_train_loss=0.011775303399553377
Epoch 51
Epoch 51 :: Batch 0/1
Batch Loss = 0.013148078182381572
51, epoch_train_loss=0.013148078182381572
Epoch 52
Epoch 52 :: Batch 0/1
Batch Loss = 0.010179074942669909
52, epoch_train_loss=0.010179074942669909
Epoch 53
Epoch 53 :: Batch 0/1
Batch Loss = 0.009163656936372347
53, epoch_train_loss=0.009163656936372347
Epoch 54
Epoch 54 :: Batch 0/1
Batch Loss = 0.011714456783138831
54, epoch_train_loss=0.011714456783138831
Epoch 55
Epoch 55 :: Batch 0/1
Batch Loss = 0.008957871781227974
55, epoch_train_loss=0.008957871781227974
Epoch 56
Epoch 56 :: Batch 0/1
Batch Loss = 0.0076245620459814995
56, epoch_train_loss=0.0076245620459814995
Epoch 57
Epoch 57 :: Batch 0/1
Batch Loss = 0.008729032546093678
57, epoch_train_loss=0.008729032546093678
Epoch 58
Epoch 58 :: Batch 0/1
Batch Loss = 0.008172442397735949
58, epoch_train_loss=0.008172442397735949
Epoch 59
Epoch 59 :: Batch 0/1
Batch Loss = 0.007677612069090301
59, epoch_train_loss=0.007677612069090301
Epoch 60
Epoch 60 :: Batch 0/1
Batch Loss = 0.006424251548927065
60, epoch_train_loss=0.006424251548927065
Epoch 61
Epoch 61 :: Batch 0/1
Batch Loss = 0.006726874779509375
61, epoch_train_loss=0.006726874779509375
Epoch 62
Epoch 62 :: Batch 0/1
Batch Loss = 0.007400901982000552
62, epoch_train_loss=0.007400901982000552
Epoch 63
Epoch 63 :: Batch 0/1
Batch Loss = 0.005798124001216757
63, epoch_train_loss=0.005798124001216757
Epoch 64
Epoch 64 :: Batch 0/1
Batch Loss = 0.005881218473708902
64, epoch_train_loss=0.005881218473708902
Epoch 65
Epoch 65 :: Batch 0/1
Batch Loss = 0.0062777409925462956
65, epoch_train_loss=0.0062777409925462956
Epoch 66
Epoch 66 :: Batch 0/1
Batch Loss = 0.00564356408862438
66, epoch_train_loss=0.00564356408862438
Epoch 67
Epoch 67 :: Batch 0/1
Batch Loss = 0.005309046000708101
67, epoch_train_loss=0.005309046000708101
Epoch 68
Epoch 68 :: Batch 0/1
Batch Loss = 0.004861280208348285
68, epoch_train_loss=0.004861280208348285
Epoch 69
Epoch 69 :: Batch 0/1
Batch Loss = 0.005372515346634793
69, epoch_train_loss=0.005372515346634793
Epoch 70
Epoch 70 :: Batch 0/1
Batch Loss = 0.004799488123500118
70, epoch_train_loss=0.004799488123500118
Epoch 71
Epoch 71 :: Batch 0/1
Batch Loss = 0.004345128897236065
71, epoch_train_loss=0.004345128897236065
Epoch 72
Epoch 72 :: Batch 0/1
Batch Loss = 0.00480895717802759
72, epoch_train_loss=0.00480895717802759
Epoch 73
Epoch 73 :: Batch 0/1
Batch Loss = 0.004411857697913576
73, epoch_train_loss=0.004411857697913576
Epoch 74
Epoch 74 :: Batch 0/1
Batch Loss = 0.004246086452653641
74, epoch_train_loss=0.004246086452653641
Epoch 75
Epoch 75 :: Batch 0/1
Batch Loss = 0.004162073255441237
75, epoch_train_loss=0.004162073255441237
Epoch 76
Epoch 76 :: Batch 0/1
Batch Loss = 0.004263656992201127
76, epoch_train_loss=0.004263656992201127
Epoch 77
Epoch 77 :: Batch 0/1
Batch Loss = 0.00395783405182726
77, epoch_train_loss=0.00395783405182726
Epoch 78
Epoch 78 :: Batch 0/1
Batch Loss = 0.0037883774628827546
78, epoch_train_loss=0.0037883774628827546
Epoch 79
Epoch 79 :: Batch 0/1
Batch Loss = 0.004018765182683092
79, epoch_train_loss=0.004018765182683092
Epoch 80
Epoch 80 :: Batch 0/1
Batch Loss = 0.003657718874449423
80, epoch_train_loss=0.003657718874449423
Epoch 81
Epoch 81 :: Batch 0/1
Batch Loss = 0.003648785999509069
81, epoch_train_loss=0.003648785999509069
Epoch 82
Epoch 82 :: Batch 0/1
Batch Loss = 0.0036304712349884403
82, epoch_train_loss=0.0036304712349884403
Epoch 83
Epoch 83 :: Batch 0/1
Batch Loss = 0.0036017256764745587
83, epoch_train_loss=0.0036017256764745587
Epoch 84
Epoch 84 :: Batch 0/1
Batch Loss = 0.003439650743977725
84, epoch_train_loss=0.003439650743977725
Epoch 85
Epoch 85 :: Batch 0/1
Batch Loss = 0.0034987340926678303
85, epoch_train_loss=0.0034987340926678303
Epoch 86
Epoch 86 :: Batch 0/1
Batch Loss = 0.0034589458124062327
86, epoch_train_loss=0.0034589458124062327
Epoch 87
Epoch 87 :: Batch 0/1
Batch Loss = 0.0032894687473175284
87, epoch_train_loss=0.0032894687473175284
Epoch 88
Epoch 88 :: Batch 0/1
Batch Loss = 0.0033786012708796305
88, epoch_train_loss=0.0033786012708796305
Epoch 89
Epoch 89 :: Batch 0/1
Batch Loss = 0.0032877759334124785
89, epoch_train_loss=0.0032877759334124785
Epoch 90
Epoch 90 :: Batch 0/1
Batch Loss = 0.0032432164502961386
90, epoch_train_loss=0.0032432164502961386
Epoch 91
Epoch 91 :: Batch 0/1
Batch Loss = 0.003216466485964104
91, epoch_train_loss=0.003216466485964104
Epoch 92
Epoch 92 :: Batch 0/1
Batch Loss = 0.0032326871364667647
92, epoch_train_loss=0.0032326871364667647
Epoch 93
Epoch 93 :: Batch 0/1
Batch Loss = 0.003124145354219701
93, epoch_train_loss=0.003124145354219701
Epoch 94
Epoch 94 :: Batch 0/1
Batch Loss = 0.003168505443701392
94, epoch_train_loss=0.003168505443701392
Epoch 95
Epoch 95 :: Batch 0/1
Batch Loss = 0.0031219981261837123
95, epoch_train_loss=0.0031219981261837123
Epoch 96
Epoch 96 :: Batch 0/1
Batch Loss = 0.0030779608280918033
96, epoch_train_loss=0.0030779608280918033
Epoch 97
Epoch 97 :: Batch 0/1
Batch Loss = 0.003084333598488895
97, epoch_train_loss=0.003084333598488895
Epoch 98
Epoch 98 :: Batch 0/1
Batch Loss = 0.0030518526149054784
98, epoch_train_loss=0.0030518526149054784
Epoch 99
Epoch 99 :: Batch 0/1
Batch Loss = 0.0030133181086865186
99, epoch_train_loss=0.0030133181086865186
Epoch 100
Epoch 100 :: Batch 0/1
Batch Loss = 0.003020015061210859
100, epoch_train_loss=0.003020015061210859
Epoch 101
Epoch 101 :: Batch 0/1
Batch Loss = 0.0029916956215732685
101, epoch_train_loss=0.0029916956215732685
Epoch 102
Epoch 102 :: Batch 0/1
Batch Loss = 0.002967208779264172
102, epoch_train_loss=0.002967208779264172
Epoch 103
Epoch 103 :: Batch 0/1
Batch Loss = 0.0029753543498933547
103, epoch_train_loss=0.0029753543498933547
Epoch 104
Epoch 104 :: Batch 0/1
Batch Loss = 0.0029345964385568126
104, epoch_train_loss=0.0029345964385568126
Epoch 105
Epoch 105 :: Batch 0/1
Batch Loss = 0.0029338059123324646
105, epoch_train_loss=0.0029338059123324646
Epoch 106
Epoch 106 :: Batch 0/1
Batch Loss = 0.00292092417302796
106, epoch_train_loss=0.00292092417302796
Epoch 107
Epoch 107 :: Batch 0/1
Batch Loss = 0.0028988180152809974
107, epoch_train_loss=0.0028988180152809974
Epoch 108
Epoch 108 :: Batch 0/1
Batch Loss = 0.0028934892609085937
108, epoch_train_loss=0.0028934892609085937
Epoch 109
Epoch 109 :: Batch 0/1
Batch Loss = 0.002881343330291263
109, epoch_train_loss=0.002881343330291263
Epoch 110
Epoch 110 :: Batch 0/1
Batch Loss = 0.0028623890569313467
110, epoch_train_loss=0.0028623890569313467
Epoch 111
Epoch 111 :: Batch 0/1
Batch Loss = 0.00286430872401898
111, epoch_train_loss=0.00286430872401898
Epoch 112
Epoch 112 :: Batch 0/1
Batch Loss = 0.002843192579372503
112, epoch_train_loss=0.002843192579372503
Epoch 113
Epoch 113 :: Batch 0/1
Batch Loss = 0.002839670658595629
113, epoch_train_loss=0.002839670658595629
Epoch 114
Epoch 114 :: Batch 0/1
Batch Loss = 0.0028301194914717737
114, epoch_train_loss=0.0028301194914717737
Epoch 115
Epoch 115 :: Batch 0/1
Batch Loss = 0.00281595107133103
115, epoch_train_loss=0.00281595107133103
Epoch 116
Epoch 116 :: Batch 0/1
Batch Loss = 0.0028129602951132266
116, epoch_train_loss=0.0028129602951132266
Epoch 117
Epoch 117 :: Batch 0/1
Batch Loss = 0.0028001338046006415
117, epoch_train_loss=0.0028001338046006415
Epoch 118
Epoch 118 :: Batch 0/1
Batch Loss = 0.002794159865011015
118, epoch_train_loss=0.002794159865011015
Epoch 119
Epoch 119 :: Batch 0/1
Batch Loss = 0.0027871873490029806
119, epoch_train_loss=0.0027871873490029806
Epoch 120
Epoch 120 :: Batch 0/1
Batch Loss = 0.002777342189407005
120, epoch_train_loss=0.002777342189407005
Epoch 121
Epoch 121 :: Batch 0/1
Batch Loss = 0.0027725398120596752
121, epoch_train_loss=0.0027725398120596752
Epoch 122
Epoch 122 :: Batch 0/1
Batch Loss = 0.002764784308278445
122, epoch_train_loss=0.002764784308278445
Epoch 123
Epoch 123 :: Batch 0/1
Batch Loss = 0.0027569708481949935
123, epoch_train_loss=0.0027569708481949935
Epoch 124
Epoch 124 :: Batch 0/1
Batch Loss = 0.002753038616642414
124, epoch_train_loss=0.002753038616642414
Epoch 125
Epoch 125 :: Batch 0/1
Batch Loss = 0.0027428695045296817
125, epoch_train_loss=0.0027428695045296817
Epoch 126
Epoch 126 :: Batch 0/1
Batch Loss = 0.0027407096369176363
126, epoch_train_loss=0.0027407096369176363
Epoch 127
Epoch 127 :: Batch 0/1
Batch Loss = 0.002731559306549914
127, epoch_train_loss=0.002731559306549914
Epoch 128
Epoch 128 :: Batch 0/1
Batch Loss = 0.002728143694126682
128, epoch_train_loss=0.002728143694126682
Epoch 129
Epoch 129 :: Batch 0/1
Batch Loss = 0.0027218075255783635
129, epoch_train_loss=0.0027218075255783635
Epoch 130
Epoch 130 :: Batch 0/1
Batch Loss = 0.002716125474030905
130, epoch_train_loss=0.002716125474030905
Epoch 131
Epoch 131 :: Batch 0/1
Batch Loss = 0.0027119570337696
131, epoch_train_loss=0.0027119570337696
Epoch 132
Epoch 132 :: Batch 0/1
Batch Loss = 0.002705338431185671
132, epoch_train_loss=0.002705338431185671
Epoch 133
Epoch 133 :: Batch 0/1
Batch Loss = 0.002702068207841463
133, epoch_train_loss=0.002702068207841463
Epoch 134
Epoch 134 :: Batch 0/1
Batch Loss = 0.0026958135153776703
134, epoch_train_loss=0.0026958135153776703
Epoch 135
Epoch 135 :: Batch 0/1
Batch Loss = 0.0026921363352725904
135, epoch_train_loss=0.0026921363352725904
Epoch 136
Epoch 136 :: Batch 0/1
Batch Loss = 0.002687052479553805
136, epoch_train_loss=0.002687052479553805
Epoch 137
Epoch 137 :: Batch 0/1
Batch Loss = 0.002682768968484091
137, epoch_train_loss=0.002682768968484091
Epoch 138
Epoch 138 :: Batch 0/1
Batch Loss = 0.002678685982355489
138, epoch_train_loss=0.002678685982355489
Epoch 139
Epoch 139 :: Batch 0/1
Batch Loss = 0.002673898525147225
139, epoch_train_loss=0.002673898525147225
Epoch 140
Epoch 140 :: Batch 0/1
Batch Loss = 0.0026703784844537607
140, epoch_train_loss=0.0026703784844537607
Epoch 141
Epoch 141 :: Batch 0/1
Batch Loss = 0.0026656160456125164
141, epoch_train_loss=0.0026656160456125164
Epoch 142
Epoch 142 :: Batch 0/1
Batch Loss = 0.0026622558844503004
142, epoch_train_loss=0.0026622558844503004
Epoch 143
Epoch 143 :: Batch 0/1
Batch Loss = 0.0026579225223336334
143, epoch_train_loss=0.0026579225223336334
Epoch 144
Epoch 144 :: Batch 0/1
Batch Loss = 0.0026544112574252605
144, epoch_train_loss=0.0026544112574252605
Epoch 145
Epoch 145 :: Batch 0/1
Batch Loss = 0.002650611308777834
145, epoch_train_loss=0.002650611308777834
Epoch 146
Epoch 146 :: Batch 0/1
Batch Loss = 0.0026468454522611674
146, epoch_train_loss=0.0026468454522611674
Epoch 147
Epoch 147 :: Batch 0/1
Batch Loss = 0.0026435619311543797
147, epoch_train_loss=0.0026435619311543797
Epoch 148
Epoch 148 :: Batch 0/1
Batch Loss = 0.0026396917300710895
148, epoch_train_loss=0.0026396917300710895
Epoch 149
Epoch 149 :: Batch 0/1
Batch Loss = 0.0026366452667872923
149, epoch_train_loss=0.0026366452667872923
Epoch 150
Epoch 150 :: Batch 0/1
Batch Loss = 0.002632918111133411
150, epoch_train_loss=0.002632918111133411
Epoch 151
Epoch 151 :: Batch 0/1
Batch Loss = 0.002629919572918571
151, epoch_train_loss=0.002629919572918571
Epoch 152
Epoch 152 :: Batch 0/1
Batch Loss = 0.0026265270208998136
152, epoch_train_loss=0.0026265270208998136
Epoch 153
Epoch 153 :: Batch 0/1
Batch Loss = 0.00262343696897739
153, epoch_train_loss=0.00262343696897739
Epoch 154
Epoch 154 :: Batch 0/1
Batch Loss = 0.0026203529077554995
154, epoch_train_loss=0.0026203529077554995
Epoch 155
Epoch 155 :: Batch 0/1
Batch Loss = 0.0026172432358509143
155, epoch_train_loss=0.0026172432358509143
Epoch 156
Epoch 156 :: Batch 0/1
Batch Loss = 0.0026143361367327437
156, epoch_train_loss=0.0026143361367327437
Epoch 157
Epoch 157 :: Batch 0/1
Batch Loss = 0.0026113463410200447
157, epoch_train_loss=0.0026113463410200447
Epoch 158
Epoch 158 :: Batch 0/1
Batch Loss = 0.0026085163408892486
158, epoch_train_loss=0.0026085163408892486
Epoch 159
Epoch 159 :: Batch 0/1
Batch Loss = 0.0026056811693686934
159, epoch_train_loss=0.0026056811693686934
Epoch 160
Epoch 160 :: Batch 0/1
Batch Loss = 0.0026029266116815897
160, epoch_train_loss=0.0026029266116815897
Epoch 161
Epoch 161 :: Batch 0/1
Batch Loss = 0.0026001969152414325
161, epoch_train_loss=0.0026001969152414325
Epoch 162
Epoch 162 :: Batch 0/1
Batch Loss = 0.0025975752384686714
162, epoch_train_loss=0.0025975752384686714
Epoch 163
Epoch 163 :: Batch 0/1
Batch Loss = 0.0025948862809778922
163, epoch_train_loss=0.0025948862809778922
Epoch 164
Epoch 164 :: Batch 0/1
Batch Loss = 0.0025923955774586897
164, epoch_train_loss=0.0025923955774586897
Epoch 165
Epoch 165 :: Batch 0/1
Batch Loss = 0.002589787794098142
165, epoch_train_loss=0.002589787794098142
Epoch 166
Epoch 166 :: Batch 0/1
Batch Loss = 0.002587374128428188
166, epoch_train_loss=0.002587374128428188
Epoch 167
Epoch 167 :: Batch 0/1
Batch Loss = 0.0025848938630940147
167, epoch_train_loss=0.0025848938630940147
Epoch 168
Epoch 168 :: Batch 0/1
Batch Loss = 0.0025825229082568077
168, epoch_train_loss=0.0025825229082568077
Epoch 169
Epoch 169 :: Batch 0/1
Batch Loss = 0.002580156101195065
169, epoch_train_loss=0.002580156101195065
Epoch 170
Epoch 170 :: Batch 0/1
Batch Loss = 0.002577854010627267
170, epoch_train_loss=0.002577854010627267
Epoch 171
Epoch 171 :: Batch 0/1
Batch Loss = 0.002575558066559009
171, epoch_train_loss=0.002575558066559009
Epoch 172
Epoch 172 :: Batch 0/1
Batch Loss = 0.0025733502150724853
172, epoch_train_loss=0.0025733502150724853
Epoch 173
Epoch 173 :: Batch 0/1
Batch Loss = 0.0025711148504106414
173, epoch_train_loss=0.0025711148504106414
Epoch 174
Epoch 174 :: Batch 0/1
Batch Loss = 0.002568978765832513
174, epoch_train_loss=0.002568978765832513
Epoch 175
Epoch 175 :: Batch 0/1
Batch Loss = 0.002566831982566889
175, epoch_train_loss=0.002566831982566889
Epoch 176
Epoch 176 :: Batch 0/1
Batch Loss = 0.0025647443023302094
176, epoch_train_loss=0.0025647443023302094
Epoch 177
Epoch 177 :: Batch 0/1
Batch Loss = 0.002562676705264384
177, epoch_train_loss=0.002562676705264384
Epoch 178
Epoch 178 :: Batch 0/1
Batch Loss = 0.0025606499169149814
178, epoch_train_loss=0.0025606499169149814
Epoch 179
Epoch 179 :: Batch 0/1
Batch Loss = 0.002558638544858864
179, epoch_train_loss=0.002558638544858864
Epoch 180
Epoch 180 :: Batch 0/1
Batch Loss = 0.002556683154172338
180, epoch_train_loss=0.002556683154172338
Epoch 181
Epoch 181 :: Batch 0/1
Batch Loss = 0.0025547267561671907
181, epoch_train_loss=0.0025547267561671907
Epoch 182
Epoch 182 :: Batch 0/1
Batch Loss = 0.0025528252703539444
182, epoch_train_loss=0.0025528252703539444
Epoch 183
Epoch 183 :: Batch 0/1
Batch Loss = 0.0025509341561705846
183, epoch_train_loss=0.0025509341561705846
Epoch 184
Epoch 184 :: Batch 0/1
Batch Loss = 0.0025490791805192307
184, epoch_train_loss=0.0025490791805192307
Epoch 185
Epoch 185 :: Batch 0/1
Batch Loss = 0.0025472433960919065
185, epoch_train_loss=0.0025472433960919065
Epoch 186
Epoch 186 :: Batch 0/1
Batch Loss = 0.0025454419686410635
186, epoch_train_loss=0.0025454419686410635
Epoch 187
Epoch 187 :: Batch 0/1
Batch Loss = 0.0025436510908546854
187, epoch_train_loss=0.0025436510908546854
Epoch 188
Epoch 188 :: Batch 0/1
Batch Loss = 0.002541900670871501
188, epoch_train_loss=0.002541900670871501
Epoch 189
Epoch 189 :: Batch 0/1
Batch Loss = 0.0025401600915272438
189, epoch_train_loss=0.0025401600915272438
Epoch 190
Epoch 190 :: Batch 0/1
Batch Loss = 0.002538449336096916
190, epoch_train_loss=0.002538449336096916
Epoch 191
Epoch 191 :: Batch 0/1
Batch Loss = 0.0025367573296770757
191, epoch_train_loss=0.0025367573296770757
Epoch 192
Epoch 192 :: Batch 0/1
Batch Loss = 0.002535089206520361
192, epoch_train_loss=0.002535089206520361
Epoch 193
Epoch 193 :: Batch 0/1
Batch Loss = 0.0025334368283017605
193, epoch_train_loss=0.0025334368283017605
Epoch 194
Epoch 194 :: Batch 0/1
Batch Loss = 0.002531810724787885
194, epoch_train_loss=0.002531810724787885
Epoch 195
Epoch 195 :: Batch 0/1
Batch Loss = 0.0025301992103315273
195, epoch_train_loss=0.0025301992103315273
Epoch 196
Epoch 196 :: Batch 0/1
Batch Loss = 0.002528608204031608
196, epoch_train_loss=0.002528608204031608
Epoch 197
Epoch 197 :: Batch 0/1
Batch Loss = 0.0025270370362772522
197, epoch_train_loss=0.0025270370362772522
Epoch 198
Epoch 198 :: Batch 0/1
Batch Loss = 0.002525481624816246
198, epoch_train_loss=0.002525481624816246
Epoch 199
Epoch 199 :: Batch 0/1
Batch Loss = 0.002523944531351222
199, epoch_train_loss=0.002523944531351222
Epoch 200
Epoch 200 :: Batch 0/1
Batch Loss = 0.002522424198419668
200, epoch_train_loss=0.002522424198419668
Epoch 201
Epoch 201 :: Batch 0/1
Batch Loss = 0.002520921108989294
201, epoch_train_loss=0.002520921108989294
Epoch 202
Epoch 202 :: Batch 0/1
Batch Loss = 0.0025194317200584347
202, epoch_train_loss=0.0025194317200584347
Epoch 203
Epoch 203 :: Batch 0/1
Batch Loss = 0.002517961256426785
203, epoch_train_loss=0.002517961256426785
Epoch 204
Epoch 204 :: Batch 0/1
Batch Loss = 0.0025165031471669932
204, epoch_train_loss=0.0025165031471669932
Epoch 205
Epoch 205 :: Batch 0/1
Batch Loss = 0.002515061309592394
205, epoch_train_loss=0.002515061309592394
Epoch 206
Epoch 206 :: Batch 0/1
Batch Loss = 0.002513633253620751
206, epoch_train_loss=0.002513633253620751
Epoch 207
Epoch 207 :: Batch 0/1
Batch Loss = 0.0025122199805008285
207, epoch_train_loss=0.0025122199805008285
Epoch 208
Epoch 208 :: Batch 0/1
Batch Loss = 0.002510819169245551
208, epoch_train_loss=0.002510819169245551
Epoch 209
Epoch 209 :: Batch 0/1
Batch Loss = 0.0025094326257510003
209, epoch_train_loss=0.0025094326257510003
Epoch 210
Epoch 210 :: Batch 0/1
Batch Loss = 0.00250805926072434
210, epoch_train_loss=0.00250805926072434
Epoch 211
Epoch 211 :: Batch 0/1
Batch Loss = 0.002506697678136566
211, epoch_train_loss=0.002506697678136566
Epoch 212
Epoch 212 :: Batch 0/1
Batch Loss = 0.002505349418629606
212, epoch_train_loss=0.002505349418629606
Epoch 213
Epoch 213 :: Batch 0/1
Batch Loss = 0.0025040127821182568
213, epoch_train_loss=0.0025040127821182568
Epoch 214
Epoch 214 :: Batch 0/1
Batch Loss = 0.0025026883782024566
214, epoch_train_loss=0.0025026883782024566
Epoch 215
Epoch 215 :: Batch 0/1
Batch Loss = 0.002501374902634027
215, epoch_train_loss=0.002501374902634027
Epoch 216
Epoch 216 :: Batch 0/1
Batch Loss = 0.0025000735473042217
216, epoch_train_loss=0.0025000735473042217
Epoch 217
Epoch 217 :: Batch 0/1
Batch Loss = 0.002498782816938359
217, epoch_train_loss=0.002498782816938359
Epoch 218
Epoch 218 :: Batch 0/1
Batch Loss = 0.0024975029006900447
218, epoch_train_loss=0.0024975029006900447
Epoch 219
Epoch 219 :: Batch 0/1
Batch Loss = 0.002496233874189214
219, epoch_train_loss=0.002496233874189214
Epoch 220
Epoch 220 :: Batch 0/1
Batch Loss = 0.002494975021176445
220, epoch_train_loss=0.002494975021176445
Epoch 221
Epoch 221 :: Batch 0/1
Batch Loss = 0.0024937264876614827
221, epoch_train_loss=0.0024937264876614827
Epoch 222
Epoch 222 :: Batch 0/1
Batch Loss = 0.0024924876652814166
222, epoch_train_loss=0.0024924876652814166
Epoch 223
Epoch 223 :: Batch 0/1
Batch Loss = 0.0024912590220606433
223, epoch_train_loss=0.0024912590220606433
Epoch 224
Epoch 224 :: Batch 0/1
Batch Loss = 0.0024900396806826425
224, epoch_train_loss=0.0024900396806826425
Epoch 225
Epoch 225 :: Batch 0/1
Batch Loss = 0.002488829783092623
225, epoch_train_loss=0.002488829783092623
Epoch 226
Epoch 226 :: Batch 0/1
Batch Loss = 0.0024876292107767466
226, epoch_train_loss=0.0024876292107767466
Epoch 227
Epoch 227 :: Batch 0/1
Batch Loss = 0.002486437612061102
227, epoch_train_loss=0.002486437612061102
Epoch 228
Epoch 228 :: Batch 0/1
Batch Loss = 0.002485254976467691
228, epoch_train_loss=0.002485254976467691
Epoch 229
Epoch 229 :: Batch 0/1
Batch Loss = 0.002484080906232134
229, epoch_train_loss=0.002484080906232134
Epoch 230
Epoch 230 :: Batch 0/1
Batch Loss = 0.0024829156098486894
230, epoch_train_loss=0.0024829156098486894
Epoch 231
Epoch 231 :: Batch 0/1
Batch Loss = 0.002481758643384842
231, epoch_train_loss=0.002481758643384842
Epoch 232
Epoch 232 :: Batch 0/1
Batch Loss = 0.002480609904764928
232, epoch_train_loss=0.002480609904764928
Epoch 233
Epoch 233 :: Batch 0/1
Batch Loss = 0.0024794693513062187
233, epoch_train_loss=0.0024794693513062187
Epoch 234
Epoch 234 :: Batch 0/1
Batch Loss = 0.002478336754086419
234, epoch_train_loss=0.002478336754086419
Epoch 235
Epoch 235 :: Batch 0/1
Batch Loss = 0.0024772120702564694
235, epoch_train_loss=0.0024772120702564694
Epoch 236
Epoch 236 :: Batch 0/1
Batch Loss = 0.00247609500595372
236, epoch_train_loss=0.00247609500595372
Epoch 237
Epoch 237 :: Batch 0/1
Batch Loss = 0.0024749856031970594
237, epoch_train_loss=0.0024749856031970594
Epoch 238
Epoch 238 :: Batch 0/1
Batch Loss = 0.0024738836913659958
238, epoch_train_loss=0.0024738836913659958
Epoch 239
Epoch 239 :: Batch 0/1
Batch Loss = 0.002472789072677137
239, epoch_train_loss=0.002472789072677137
Epoch 240
Epoch 240 :: Batch 0/1
Batch Loss = 0.00247170171332857
240, epoch_train_loss=0.00247170171332857
Epoch 241
Epoch 241 :: Batch 0/1
Batch Loss = 0.002470621441232413
241, epoch_train_loss=0.002470621441232413
Epoch 242
Epoch 242 :: Batch 0/1
Batch Loss = 0.002469548210572619
242, epoch_train_loss=0.002469548210572619
Epoch 243
Epoch 243 :: Batch 0/1
Batch Loss = 0.002468481844532698
243, epoch_train_loss=0.002468481844532698
Epoch 244
Epoch 244 :: Batch 0/1
Batch Loss = 0.00246742223440551
244, epoch_train_loss=0.00246742223440551
Epoch 245
Epoch 245 :: Batch 0/1
Batch Loss = 0.002466369329190861
245, epoch_train_loss=0.002466369329190861
Epoch 246
Epoch 246 :: Batch 0/1
Batch Loss = 0.0024653229665506224
246, epoch_train_loss=0.0024653229665506224
Epoch 247
Epoch 247 :: Batch 0/1
Batch Loss = 0.0024642830796410293
247, epoch_train_loss=0.0024642830796410293
Epoch 248
Epoch 248 :: Batch 0/1
Batch Loss = 0.0024632495426091167
248, epoch_train_loss=0.0024632495426091167
Epoch 249
Epoch 249 :: Batch 0/1
Batch Loss = 0.0024622222561325826
249, epoch_train_loss=0.0024622222561325826
Epoch 250
Epoch 250 :: Batch 0/1
Batch Loss = 0.002461201155797538
250, epoch_train_loss=0.002461201155797538
Epoch 251
Epoch 251 :: Batch 0/1
Batch Loss = 0.0024601861085543765
251, epoch_train_loss=0.0024601861085543765
Epoch 252
Epoch 252 :: Batch 0/1
Batch Loss = 0.002459177034935745
252, epoch_train_loss=0.002459177034935745
Epoch 253
Epoch 253 :: Batch 0/1
Batch Loss = 0.0024581738378878794
253, epoch_train_loss=0.0024581738378878794
Epoch 254
Epoch 254 :: Batch 0/1
Batch Loss = 0.0024571764266094896
254, epoch_train_loss=0.0024571764266094896
Epoch 255
Epoch 255 :: Batch 0/1
Batch Loss = 0.0024561847266248026
255, epoch_train_loss=0.0024561847266248026
Epoch 256
Epoch 256 :: Batch 0/1
Batch Loss = 0.0024551986220426774
256, epoch_train_loss=0.0024551986220426774
Epoch 257
Epoch 257 :: Batch 0/1
Batch Loss = 0.0024542180385235704
257, epoch_train_loss=0.0024542180385235704
Epoch 258
Epoch 258 :: Batch 0/1
Batch Loss = 0.0024532428958243023
258, epoch_train_loss=0.0024532428958243023
Epoch 259
Epoch 259 :: Batch 0/1
Batch Loss = 0.0024522730965332823
259, epoch_train_loss=0.0024522730965332823
Epoch 260
Epoch 260 :: Batch 0/1
Batch Loss = 0.002451308570978259
260, epoch_train_loss=0.002451308570978259
Epoch 261
Epoch 261 :: Batch 0/1
Batch Loss = 0.0024503492278591093
261, epoch_train_loss=0.0024503492278591093
Epoch 262
Epoch 262 :: Batch 0/1
Batch Loss = 0.002449394982533134
262, epoch_train_loss=0.002449394982533134
Epoch 263
Epoch 263 :: Batch 0/1
Batch Loss = 0.002448445765691789
263, epoch_train_loss=0.002448445765691789
Epoch 264
Epoch 264 :: Batch 0/1
Batch Loss = 0.002447501493914168
264, epoch_train_loss=0.002447501493914168
Epoch 265
Epoch 265 :: Batch 0/1
Batch Loss = 0.0024465620907987654
265, epoch_train_loss=0.0024465620907987654
Epoch 266
Epoch 266 :: Batch 0/1
Batch Loss = 0.0024456274796685407
266, epoch_train_loss=0.0024456274796685407
Epoch 267
Epoch 267 :: Batch 0/1
Batch Loss = 0.0024446975822156264
267, epoch_train_loss=0.0024446975822156264
Epoch 268
Epoch 268 :: Batch 0/1
Batch Loss = 0.0024437723278563244
268, epoch_train_loss=0.0024437723278563244
Epoch 269
Epoch 269 :: Batch 0/1
Batch Loss = 0.0024428516424538755
269, epoch_train_loss=0.0024428516424538755
Epoch 270
Epoch 270 :: Batch 0/1
Batch Loss = 0.0024419354539223542
270, epoch_train_loss=0.0024419354539223542
Epoch 271
Epoch 271 :: Batch 0/1
Batch Loss = 0.0024410236926128296
271, epoch_train_loss=0.0024410236926128296
Epoch 272
Epoch 272 :: Batch 0/1
Batch Loss = 0.002440116284745117
272, epoch_train_loss=0.002440116284745117
Epoch 273
Epoch 273 :: Batch 0/1
Batch Loss = 0.0024392131628424847
273, epoch_train_loss=0.0024392131628424847
Epoch 274
Epoch 274 :: Batch 0/1
Batch Loss = 0.002438314261547817
274, epoch_train_loss=0.002438314261547817
Epoch 275
Epoch 275 :: Batch 0/1
Batch Loss = 0.0024374195104877533
275, epoch_train_loss=0.0024374195104877533
Epoch 276
Epoch 276 :: Batch 0/1
Batch Loss = 0.002436528846533721
276, epoch_train_loss=0.002436528846533721
Epoch 277
Epoch 277 :: Batch 0/1
Batch Loss = 0.0024356422053430004
277, epoch_train_loss=0.0024356422053430004
Epoch 278
Epoch 278 :: Batch 0/1
Batch Loss = 0.002434759519414316
278, epoch_train_loss=0.002434759519414316
Epoch 279
Epoch 279 :: Batch 0/1
Batch Loss = 0.0024338807289329663
279, epoch_train_loss=0.0024338807289329663
Epoch 280
Epoch 280 :: Batch 0/1
Batch Loss = 0.002433005772042164
280, epoch_train_loss=0.002433005772042164
Epoch 281
Epoch 281 :: Batch 0/1
Batch Loss = 0.0024321345861762125
281, epoch_train_loss=0.0024321345861762125
Epoch 282
Epoch 282 :: Batch 0/1
Batch Loss = 0.0024312671135349145
282, epoch_train_loss=0.0024312671135349145
Epoch 283
Epoch 283 :: Batch 0/1
Batch Loss = 0.0024304032948231683
283, epoch_train_loss=0.0024304032948231683
Epoch 284
Epoch 284 :: Batch 0/1
Batch Loss = 0.002429543070616938
284, epoch_train_loss=0.002429543070616938
Epoch 285
Epoch 285 :: Batch 0/1
Batch Loss = 0.0024286863853559613
285, epoch_train_loss=0.0024286863853559613
Epoch 286
Epoch 286 :: Batch 0/1
Batch Loss = 0.002427833183543582
286, epoch_train_loss=0.002427833183543582
Epoch 287
Epoch 287 :: Batch 0/1
Batch Loss = 0.002426983408643376
287, epoch_train_loss=0.002426983408643376
Epoch 288
Epoch 288 :: Batch 0/1
Batch Loss = 0.0024261370083698253
288, epoch_train_loss=0.0024261370083698253
Epoch 289
Epoch 289 :: Batch 0/1
Batch Loss = 0.0024252939294155474
289, epoch_train_loss=0.0024252939294155474
Epoch 290
Epoch 290 :: Batch 0/1
Batch Loss = 0.002424454118728338
290, epoch_train_loss=0.002424454118728338
Epoch 291
Epoch 291 :: Batch 0/1
Batch Loss = 0.002423617526209478
291, epoch_train_loss=0.002423617526209478
Epoch 292
Epoch 292 :: Batch 0/1
Batch Loss = 0.002422784101449756
292, epoch_train_loss=0.002422784101449756
Epoch 293
Epoch 293 :: Batch 0/1
Batch Loss = 0.0024219537944205677
293, epoch_train_loss=0.0024219537944205677
Epoch 294
Epoch 294 :: Batch 0/1
Batch Loss = 0.002421126557147961
294, epoch_train_loss=0.002421126557147961
Epoch 295
Epoch 295 :: Batch 0/1
Batch Loss = 0.0024203023423416898
295, epoch_train_loss=0.0024203023423416898
Epoch 296
Epoch 296 :: Batch 0/1
Batch Loss = 0.002419481102290627
296, epoch_train_loss=0.002419481102290627
Epoch 297
Epoch 297 :: Batch 0/1
Batch Loss = 0.002418662792277649
297, epoch_train_loss=0.002418662792277649
Epoch 298
Epoch 298 :: Batch 0/1
Batch Loss = 0.0024178473671013843
298, epoch_train_loss=0.0024178473671013843
Epoch 299
Epoch 299 :: Batch 0/1
Batch Loss = 0.0024170347822661084
299, epoch_train_loss=0.0024170347822661084
Epoch 300
Epoch 300 :: Batch 0/1
Batch Loss = 0.0024162249950641685
300, epoch_train_loss=0.0024162249950641685
Epoch 301
Epoch 301 :: Batch 0/1
Batch Loss = 0.002415417963227585
301, epoch_train_loss=0.002415417963227585
Epoch 302
Epoch 302 :: Batch 0/1
Batch Loss = 0.002414613644706935
302, epoch_train_loss=0.002414613644706935
Epoch 303
Epoch 303 :: Batch 0/1
Batch Loss = 0.0024138119991628506
303, epoch_train_loss=0.0024138119991628506
Epoch 304
Epoch 304 :: Batch 0/1
Batch Loss = 0.0024130129867322996
304, epoch_train_loss=0.0024130129867322996
Epoch 305
Epoch 305 :: Batch 0/1
Batch Loss = 0.0024122165678877043
305, epoch_train_loss=0.0024122165678877043
Epoch 306
Epoch 306 :: Batch 0/1
Batch Loss = 0.002411422704655624
306, epoch_train_loss=0.002411422704655624
Epoch 307
Epoch 307 :: Batch 0/1
Batch Loss = 0.002410631359207805
307, epoch_train_loss=0.002410631359207805
Epoch 308
Epoch 308 :: Batch 0/1
Batch Loss = 0.0024098424948000075
308, epoch_train_loss=0.0024098424948000075
Epoch 309
Epoch 309 :: Batch 0/1
Batch Loss = 0.0024090560753541705
309, epoch_train_loss=0.0024090560753541705
Epoch 310
Epoch 310 :: Batch 0/1
Batch Loss = 0.002408272065692093
310, epoch_train_loss=0.002408272065692093
Epoch 311
Epoch 311 :: Batch 0/1
Batch Loss = 0.0024074904307056196
311, epoch_train_loss=0.0024074904307056196
Epoch 312
Epoch 312 :: Batch 0/1
Batch Loss = 0.0024067111372825085
312, epoch_train_loss=0.0024067111372825085
Epoch 313
Epoch 313 :: Batch 0/1
Batch Loss = 0.0024059341513618703
313, epoch_train_loss=0.0024059341513618703
Epoch 314
Epoch 314 :: Batch 0/1
Batch Loss = 0.00240515944134021
314, epoch_train_loss=0.00240515944134021
Epoch 315
Epoch 315 :: Batch 0/1
Batch Loss = 0.0024043869748322268
315, epoch_train_loss=0.0024043869748322268
Epoch 316
Epoch 316 :: Batch 0/1
Batch Loss = 0.0024036167218728075
316, epoch_train_loss=0.0024036167218728075
Epoch 317
Epoch 317 :: Batch 0/1
Batch Loss = 0.002402848651241443
317, epoch_train_loss=0.002402848651241443
Epoch 318
Epoch 318 :: Batch 0/1
Batch Loss = 0.002402082736800354
318, epoch_train_loss=0.002402082736800354
Epoch 319
Epoch 319 :: Batch 0/1
Batch Loss = 0.002401318949347422
319, epoch_train_loss=0.002401318949347422
Epoch 320
Epoch 320 :: Batch 0/1
Batch Loss = 0.0024005572708633302
320, epoch_train_loss=0.0024005572708633302
Epoch 321
Epoch 321 :: Batch 0/1
Batch Loss = 0.0023997976820879126
321, epoch_train_loss=0.0023997976820879126
Epoch 322
Epoch 322 :: Batch 0/1
Batch Loss = 0.0023990401930833872
322, epoch_train_loss=0.0023990401930833872
Epoch 323
Epoch 323 :: Batch 0/1
Batch Loss = 0.0023982848327865755
323, epoch_train_loss=0.0023982848327865755
Epoch 324
Epoch 324 :: Batch 0/1
Batch Loss = 0.002397531737039308
324, epoch_train_loss=0.002397531737039308
Epoch 325
Epoch 325 :: Batch 0/1
Batch Loss = 0.002396781191775653
325, epoch_train_loss=0.002396781191775653
Epoch 326
Epoch 326 :: Batch 0/1
Batch Loss = 0.0023960339711566847
326, epoch_train_loss=0.0023960339711566847
Epoch 327
Epoch 327 :: Batch 0/1
Batch Loss = 0.0023952918036646196
327, epoch_train_loss=0.0023952918036646196
Epoch 328
Epoch 328 :: Batch 0/1
Batch Loss = 0.002394559020517567
328, epoch_train_loss=0.002394559020517567
Epoch 329
Epoch 329 :: Batch 0/1
Batch Loss = 0.0023938458971869488
329, epoch_train_loss=0.0023938458971869488
Epoch 330
Epoch 330 :: Batch 0/1
Batch Loss = 0.0023931781463881986
330, epoch_train_loss=0.0023931781463881986
Epoch 331
Epoch 331 :: Batch 0/1
Batch Loss = 0.002392619780129241
331, epoch_train_loss=0.002392619780129241
Epoch 332
Epoch 332 :: Batch 0/1
Batch Loss = 0.00239233392565616
332, epoch_train_loss=0.00239233392565616
Epoch 333
Epoch 333 :: Batch 0/1
Batch Loss = 0.0023927440240703005
333, epoch_train_loss=0.0023927440240703005
Epoch 334
Epoch 334 :: Batch 0/1
Batch Loss = 0.002394952111399022
334, epoch_train_loss=0.002394952111399022
Epoch 335
Epoch 335 :: Batch 0/1
Batch Loss = 0.0024019435863707316
335, epoch_train_loss=0.0024019435863707316
Epoch 336
Epoch 336 :: Batch 0/1
Batch Loss = 0.002421595020878287
336, epoch_train_loss=0.002421595020878287
Epoch 337
Epoch 337 :: Batch 0/1
Batch Loss = 0.0024764055360754138
337, epoch_train_loss=0.0024764055360754138
Epoch 338
Epoch 338 :: Batch 0/1
Batch Loss = 0.0026250539224395948
338, epoch_train_loss=0.0026250539224395948
Epoch 339
Epoch 339 :: Batch 0/1
Batch Loss = 0.0030490730078724247
339, epoch_train_loss=0.0030490730078724247
Epoch 340
Epoch 340 :: Batch 0/1
Batch Loss = 0.004178394697914092
340, epoch_train_loss=0.004178394697914092
Epoch 341
Epoch 341 :: Batch 0/1
Batch Loss = 0.007523474862247664
341, epoch_train_loss=0.007523474862247664
Epoch 342
Epoch 342 :: Batch 0/1
Batch Loss = 0.01515992109371771
342, epoch_train_loss=0.01515992109371771
Epoch 343
Epoch 343 :: Batch 0/1
Batch Loss = 0.03606035529386668
343, epoch_train_loss=0.03606035529386668
Epoch 344
Epoch 344 :: Batch 0/1
Batch Loss = 0.05063967104737191
344, epoch_train_loss=0.05063967104737191
Epoch 345
Epoch 345 :: Batch 0/1
Batch Loss = 0.06574242655694222
345, epoch_train_loss=0.06574242655694222
Epoch 346
Epoch 346 :: Batch 0/1
Batch Loss = 0.013489986844202141
346, epoch_train_loss=0.013489986844202141
Epoch 347
Epoch 347 :: Batch 0/1
Batch Loss = 0.009504978816089698
347, epoch_train_loss=0.009504978816089698
Epoch 348
Epoch 348 :: Batch 0/1
Batch Loss = 0.03712060974138437
348, epoch_train_loss=0.03712060974138437
Epoch 349
Epoch 349 :: Batch 0/1
Batch Loss = 0.010005401519978595
349, epoch_train_loss=0.010005401519978595
Epoch 350
Epoch 350 :: Batch 0/1
Batch Loss = 0.009071742048097034
350, epoch_train_loss=0.009071742048097034
Epoch 351
Epoch 351 :: Batch 0/1
Batch Loss = 0.023586457551757232
351, epoch_train_loss=0.023586457551757232
Epoch 352
Epoch 352 :: Batch 0/1
Batch Loss = 0.003208052856148783
352, epoch_train_loss=0.003208052856148783
Epoch 353
Epoch 353 :: Batch 0/1
Batch Loss = 0.016632167895024773
353, epoch_train_loss=0.016632167895024773
Epoch 354
Epoch 354 :: Batch 0/1
Batch Loss = 0.014057156327332093
354, epoch_train_loss=0.014057156327332093
Epoch 355
Epoch 355 :: Batch 0/1
Batch Loss = 0.005727318674631837
355, epoch_train_loss=0.005727318674631837
Epoch 356
Epoch 356 :: Batch 0/1
Batch Loss = 0.0179441012256849
356, epoch_train_loss=0.0179441012256849
Epoch 357
Epoch 357 :: Batch 0/1
Batch Loss = 0.0037263745865075597
357, epoch_train_loss=0.0037263745865075597
Epoch 358
Epoch 358 :: Batch 0/1
Batch Loss = 0.011869409542308465
358, epoch_train_loss=0.011869409542308465
Epoch 359
Epoch 359 :: Batch 0/1
Batch Loss = 0.005995181063118494
359, epoch_train_loss=0.005995181063118494
Epoch 360
Epoch 360 :: Batch 0/1
Batch Loss = 0.00700632675969634
360, epoch_train_loss=0.00700632675969634
Epoch 361
Epoch 361 :: Batch 0/1
Batch Loss = 0.00854768948321894
361, epoch_train_loss=0.00854768948321894
Epoch 362
Epoch 362 :: Batch 0/1
Batch Loss = 0.004456467736862928
362, epoch_train_loss=0.004456467736862928
Epoch 363
Epoch 363 :: Batch 0/1
Batch Loss = 0.009233940944738454
363, epoch_train_loss=0.009233940944738454
Epoch 364
Epoch 364 :: Batch 0/1
Batch Loss = 0.003248733034699689
364, epoch_train_loss=0.003248733034699689
Epoch 365
Epoch 365 :: Batch 0/1
Batch Loss = 0.007867507437366139
365, epoch_train_loss=0.007867507437366139
Epoch 366
Epoch 366 :: Batch 0/1
Batch Loss = 0.003438309122333203
366, epoch_train_loss=0.003438309122333203
Epoch 367
Epoch 367 :: Batch 0/1
Batch Loss = 0.006535784025489164
367, epoch_train_loss=0.006535784025489164
Epoch 368
Epoch 368 :: Batch 0/1
Batch Loss = 0.004295807550528917
368, epoch_train_loss=0.004295807550528917
Epoch 369
Epoch 369 :: Batch 0/1
Batch Loss = 0.004934099766645576
369, epoch_train_loss=0.004934099766645576
Epoch 370
Epoch 370 :: Batch 0/1
Batch Loss = 0.004818673768999113
370, epoch_train_loss=0.004818673768999113
Epoch 371
Epoch 371 :: Batch 0/1
Batch Loss = 0.003632536471710466
371, epoch_train_loss=0.003632536471710466
Epoch 372
Epoch 372 :: Batch 0/1
Batch Loss = 0.005029467266580505
372, epoch_train_loss=0.005029467266580505
Epoch 373
Epoch 373 :: Batch 0/1
Batch Loss = 0.0030525949595377904
373, epoch_train_loss=0.0030525949595377904
Epoch 374
Epoch 374 :: Batch 0/1
Batch Loss = 0.004885177522226478
374, epoch_train_loss=0.004885177522226478
Epoch 375
Epoch 375 :: Batch 0/1
Batch Loss = 0.002940512487384111
375, epoch_train_loss=0.002940512487384111
Epoch 376
Epoch 376 :: Batch 0/1
Batch Loss = 0.0041952070364086845
376, epoch_train_loss=0.0041952070364086845
Epoch 377
Epoch 377 :: Batch 0/1
Batch Loss = 0.0031578074406231967
377, epoch_train_loss=0.0031578074406231967
Epoch 378
Epoch 378 :: Batch 0/1
Batch Loss = 0.0034624373881098234
378, epoch_train_loss=0.0034624373881098234
Epoch 379
Epoch 379 :: Batch 0/1
Batch Loss = 0.0034896651584397885
379, epoch_train_loss=0.0034896651584397885
Epoch 380
Epoch 380 :: Batch 0/1
Batch Loss = 0.0028919407083704204
380, epoch_train_loss=0.0028919407083704204
Epoch 381
Epoch 381 :: Batch 0/1
Batch Loss = 0.0036039976227197662
381, epoch_train_loss=0.0036039976227197662
Epoch 382
Epoch 382 :: Batch 0/1
Batch Loss = 0.002644765203834388
382, epoch_train_loss=0.002644765203834388
Epoch 383
Epoch 383 :: Batch 0/1
Batch Loss = 0.003355157414289927
383, epoch_train_loss=0.003355157414289927
Epoch 384
Epoch 384 :: Batch 0/1
Batch Loss = 0.002733575473222179
384, epoch_train_loss=0.002733575473222179
Epoch 385
Epoch 385 :: Batch 0/1
Batch Loss = 0.002961365614449891
385, epoch_train_loss=0.002961365614449891
Epoch 386
Epoch 386 :: Batch 0/1
Batch Loss = 0.0029447295901413387
386, epoch_train_loss=0.0029447295901413387
Epoch 387
Epoch 387 :: Batch 0/1
Batch Loss = 0.0026270896232553336
387, epoch_train_loss=0.0026270896232553336
Epoch 388
Epoch 388 :: Batch 0/1
Batch Loss = 0.003015739369939336
388, epoch_train_loss=0.003015739369939336
Epoch 389
Epoch 389 :: Batch 0/1
Batch Loss = 0.002527249823239593
389, epoch_train_loss=0.002527249823239593
Epoch 390
Epoch 390 :: Batch 0/1
Batch Loss = 0.002840542060563554
390, epoch_train_loss=0.002840542060563554
Epoch 391
Epoch 391 :: Batch 0/1
Batch Loss = 0.0026292510022686666
391, epoch_train_loss=0.0026292510022686666
Epoch 392
Epoch 392 :: Batch 0/1
Batch Loss = 0.002595592335815504
392, epoch_train_loss=0.002595592335815504
Epoch 393
Epoch 393 :: Batch 0/1
Batch Loss = 0.0027314885220148175
393, epoch_train_loss=0.0027314885220148175
Epoch 394
Epoch 394 :: Batch 0/1
Batch Loss = 0.0024683996483057624
394, epoch_train_loss=0.0024683996483057624
Epoch 395
Epoch 395 :: Batch 0/1
Batch Loss = 0.0026890369698043177
395, epoch_train_loss=0.0026890369698043177
Epoch 396
Epoch 396 :: Batch 0/1
Batch Loss = 0.0025094394543384206
396, epoch_train_loss=0.0025094394543384206
Epoch 397
Epoch 397 :: Batch 0/1
Batch Loss = 0.0025349997372069283
397, epoch_train_loss=0.0025349997372069283
Epoch 398
Epoch 398 :: Batch 0/1
Batch Loss = 0.0025880845807126967
398, epoch_train_loss=0.0025880845807126967
Epoch 399
Epoch 399 :: Batch 0/1
Batch Loss = 0.0024362821168223015
399, epoch_train_loss=0.0024362821168223015
Epoch 400
Epoch 400 :: Batch 0/1
Batch Loss = 0.0025666938326824435
400, epoch_train_loss=0.0025666938326824435
Epoch 401
Epoch 401 :: Batch 0/1
Batch Loss = 0.002457194481286233
401, epoch_train_loss=0.002457194481286233
Epoch 402
Epoch 402 :: Batch 0/1
Batch Loss = 0.002470892671900676
402, epoch_train_loss=0.002470892671900676
Epoch 403
Epoch 403 :: Batch 0/1
Batch Loss = 0.002507715207392791
403, epoch_train_loss=0.002507715207392791
Epoch 404
Epoch 404 :: Batch 0/1
Batch Loss = 0.002412433175696386
404, epoch_train_loss=0.002412433175696386
Epoch 405
Epoch 405 :: Batch 0/1
Batch Loss = 0.002490512787489886
405, epoch_train_loss=0.002490512787489886
Epoch 406
Epoch 406 :: Batch 0/1
Batch Loss = 0.002433245801290372
406, epoch_train_loss=0.002433245801290372
Epoch 407
Epoch 407 :: Batch 0/1
Batch Loss = 0.002424791612525799
407, epoch_train_loss=0.002424791612525799
Epoch 408
Epoch 408 :: Batch 0/1
Batch Loss = 0.002460972692001785
408, epoch_train_loss=0.002460972692001785
Epoch 409
Epoch 409 :: Batch 0/1
Batch Loss = 0.00239764561834565
409, epoch_train_loss=0.00239764561834565
Epoch 410
Epoch 410 :: Batch 0/1
Batch Loss = 0.002435141454401
410, epoch_train_loss=0.002435141454401
Epoch 411
Epoch 411 :: Batch 0/1
Batch Loss = 0.002418492849063808
411, epoch_train_loss=0.002418492849063808
Epoch 412
Epoch 412 :: Batch 0/1
Batch Loss = 0.0023941204157283506
412, epoch_train_loss=0.0023941204157283506
Epoch 413
Epoch 413 :: Batch 0/1
Batch Loss = 0.002425781741295165
413, epoch_train_loss=0.002425781741295165
Epoch 414
Epoch 414 :: Batch 0/1
Batch Loss = 0.0023897570114156997
414, epoch_train_loss=0.0023897570114156997
Epoch 415
Epoch 415 :: Batch 0/1
Batch Loss = 0.0023991934848858615
415, epoch_train_loss=0.0023991934848858615
Epoch 416
Epoch 416 :: Batch 0/1
Batch Loss = 0.0024047816265078347
416, epoch_train_loss=0.0024047816265078347
Epoch 417
Epoch 417 :: Batch 0/1
Batch Loss = 0.0023790745190175786
417, epoch_train_loss=0.0023790745190175786
Epoch 418
Epoch 418 :: Batch 0/1
Batch Loss = 0.00239784078391586
418, epoch_train_loss=0.00239784078391586
Epoch 419
Epoch 419 :: Batch 0/1
Batch Loss = 0.002385563670532011
419, epoch_train_loss=0.002385563670532011
Epoch 420
Epoch 420 :: Batch 0/1
Batch Loss = 0.0023769840982411044
420, epoch_train_loss=0.0023769840982411044
Epoch 421
Epoch 421 :: Batch 0/1
Batch Loss = 0.0023898299884276297
421, epoch_train_loss=0.0023898299884276297
Epoch 422
Epoch 422 :: Batch 0/1
Batch Loss = 0.0023730717957104403
422, epoch_train_loss=0.0023730717957104403
Epoch 423
Epoch 423 :: Batch 0/1
Batch Loss = 0.002376344651148799
423, epoch_train_loss=0.002376344651148799
Epoch 424
Epoch 424 :: Batch 0/1
Batch Loss = 0.002379263562697272
424, epoch_train_loss=0.002379263562697272
Epoch 425
Epoch 425 :: Batch 0/1
Batch Loss = 0.0023667593244224494
425, epoch_train_loss=0.0023667593244224494
Epoch 426
Epoch 426 :: Batch 0/1
Batch Loss = 0.0023736700956251996
426, epoch_train_loss=0.0023736700956251996
Epoch 427
Epoch 427 :: Batch 0/1
Batch Loss = 0.0023699139571056985
427, epoch_train_loss=0.0023699139571056985
Epoch 428
Epoch 428 :: Batch 0/1
Batch Loss = 0.002363498600273056
428, epoch_train_loss=0.002363498600273056
Epoch 429
Epoch 429 :: Batch 0/1
Batch Loss = 0.0023692366224024744
429, epoch_train_loss=0.0023692366224024744
Epoch 430
Epoch 430 :: Batch 0/1
Batch Loss = 0.0023629043214882755
430, epoch_train_loss=0.0023629043214882755
Epoch 431
Epoch 431 :: Batch 0/1
Batch Loss = 0.0023611546578095827
431, epoch_train_loss=0.0023611546578095827
Epoch 432
Epoch 432 :: Batch 0/1
Batch Loss = 0.0023640552356110314
432, epoch_train_loss=0.0023640552356110314
Epoch 433
Epoch 433 :: Batch 0/1
Batch Loss = 0.0023579879564935013
433, epoch_train_loss=0.0023579879564935013
Epoch 434
Epoch 434 :: Batch 0/1
Batch Loss = 0.0023586816721121455
434, epoch_train_loss=0.0023586816721121455
Epoch 435
Epoch 435 :: Batch 0/1
Batch Loss = 0.0023590794500147993
435, epoch_train_loss=0.0023590794500147993
Epoch 436
Epoch 436 :: Batch 0/1
Batch Loss = 0.0023544507509881906
436, epoch_train_loss=0.0023544507509881906
Epoch 437
Epoch 437 :: Batch 0/1
Batch Loss = 0.0023558501166754775
437, epoch_train_loss=0.0023558501166754775
Epoch 438
Epoch 438 :: Batch 0/1
Batch Loss = 0.002354727671668521
438, epoch_train_loss=0.002354727671668521
Epoch 439
Epoch 439 :: Batch 0/1
Batch Loss = 0.0023515634977297065
439, epoch_train_loss=0.0023515634977297065
Epoch 440
Epoch 440 :: Batch 0/1
Batch Loss = 0.002352823207897507
440, epoch_train_loss=0.002352823207897507
Epoch 441
Epoch 441 :: Batch 0/1
Batch Loss = 0.002350981748899202
441, epoch_train_loss=0.002350981748899202
Epoch 442
Epoch 442 :: Batch 0/1
Batch Loss = 0.0023489880580074896
442, epoch_train_loss=0.0023489880580074896
Epoch 443
Epoch 443 :: Batch 0/1
Batch Loss = 0.0023497359774756143
443, epoch_train_loss=0.0023497359774756143
Epoch 444
Epoch 444 :: Batch 0/1
Batch Loss = 0.0023477719548977496
444, epoch_train_loss=0.0023477719548977496
Epoch 445
Epoch 445 :: Batch 0/1
Batch Loss = 0.0023464802986820554
445, epoch_train_loss=0.0023464802986820554
Epoch 446
Epoch 446 :: Batch 0/1
Batch Loss = 0.002346767752327485
446, epoch_train_loss=0.002346767752327485
Epoch 447
Epoch 447 :: Batch 0/1
Batch Loss = 0.0023449100082733047
447, epoch_train_loss=0.0023449100082733047
Epoch 448
Epoch 448 :: Batch 0/1
Batch Loss = 0.0023440318710418154
448, epoch_train_loss=0.0023440318710418154
Epoch 449
Epoch 449 :: Batch 0/1
Batch Loss = 0.002343937423364047
449, epoch_train_loss=0.002343937423364047
Epoch 450
Epoch 450 :: Batch 0/1
Batch Loss = 0.0023423005857094396
450, epoch_train_loss=0.0023423005857094396
Epoch 451
Epoch 451 :: Batch 0/1
Batch Loss = 0.0023416023250740926
451, epoch_train_loss=0.0023416023250740926
Epoch 452
Epoch 452 :: Batch 0/1
Batch Loss = 0.0023412659223552563
452, epoch_train_loss=0.0023412659223552563
Epoch 453
Epoch 453 :: Batch 0/1
Batch Loss = 0.002339843611997516
453, epoch_train_loss=0.002339843611997516
Epoch 454
Epoch 454 :: Batch 0/1
Batch Loss = 0.0023392143287454367
454, epoch_train_loss=0.0023392143287454367
Epoch 455
Epoch 455 :: Batch 0/1
Batch Loss = 0.002338733386187465
455, epoch_train_loss=0.002338733386187465
Epoch 456
Epoch 456 :: Batch 0/1
Batch Loss = 0.0023374903393868
456, epoch_train_loss=0.0023374903393868
Epoch 457
Epoch 457 :: Batch 0/1
Batch Loss = 0.002336876861456545
457, epoch_train_loss=0.002336876861456545
Epoch 458
Epoch 458 :: Batch 0/1
Batch Loss = 0.0023363133236262114
458, epoch_train_loss=0.0023363133236262114
Epoch 459
Epoch 459 :: Batch 0/1
Batch Loss = 0.002335219150082598
459, epoch_train_loss=0.002335219150082598
Epoch 460
Epoch 460 :: Batch 0/1
Batch Loss = 0.0023345893756333207
460, epoch_train_loss=0.0023345893756333207
Epoch 461
Epoch 461 :: Batch 0/1
Batch Loss = 0.002333992487087833
461, epoch_train_loss=0.002333992487087833
Epoch 462
Epoch 462 :: Batch 0/1
Batch Loss = 0.00233300409753023
462, epoch_train_loss=0.00233300409753023
Epoch 463
Epoch 463 :: Batch 0/1
Batch Loss = 0.0023323583908369887
463, epoch_train_loss=0.0023323583908369887
Epoch 464
Epoch 464 :: Batch 0/1
Batch Loss = 0.002331744111511598
464, epoch_train_loss=0.002331744111511598
Epoch 465
Epoch 465 :: Batch 0/1
Batch Loss = 0.0023308425126716595
465, epoch_train_loss=0.0023308425126716595
Epoch 466
Epoch 466 :: Batch 0/1
Batch Loss = 0.0023301782938644788
466, epoch_train_loss=0.0023301782938644788
Epoch 467
Epoch 467 :: Batch 0/1
Batch Loss = 0.00232956059611506
467, epoch_train_loss=0.00232956059611506
Epoch 468
Epoch 468 :: Batch 0/1
Batch Loss = 0.0023287257646675634
468, epoch_train_loss=0.0023287257646675634
Epoch 469
Epoch 469 :: Batch 0/1
Batch Loss = 0.0023280477676498133
469, epoch_train_loss=0.0023280477676498133
Epoch 470
Epoch 470 :: Batch 0/1
Batch Loss = 0.00232743131037507
470, epoch_train_loss=0.00232743131037507
Epoch 471
Epoch 471 :: Batch 0/1
Batch Loss = 0.002326649274093208
471, epoch_train_loss=0.002326649274093208
Epoch 472
Epoch 472 :: Batch 0/1
Batch Loss = 0.0023259642642818974
472, epoch_train_loss=0.0023259642642818974
Epoch 473
Epoch 473 :: Batch 0/1
Batch Loss = 0.0023253471206958185
473, epoch_train_loss=0.0023253471206958185
Epoch 474
Epoch 474 :: Batch 0/1
Batch Loss = 0.0023246102021967007
474, epoch_train_loss=0.0023246102021967007
Epoch 475
Epoch 475 :: Batch 0/1
Batch Loss = 0.0023239211957370707
475, epoch_train_loss=0.0023239211957370707
Epoch 476
Epoch 476 :: Batch 0/1
Batch Loss = 0.0023233040949046965
476, epoch_train_loss=0.0023233040949046965
Epoch 477
Epoch 477 :: Batch 0/1
Batch Loss = 0.0023226035638017344
477, epoch_train_loss=0.0023226035638017344
Epoch 478
Epoch 478 :: Batch 0/1
Batch Loss = 0.0023219172124380053
478, epoch_train_loss=0.0023219172124380053
Epoch 479
Epoch 479 :: Batch 0/1
Batch Loss = 0.0023212978786241348
479, epoch_train_loss=0.0023212978786241348
Epoch 480
Epoch 480 :: Batch 0/1
Batch Loss = 0.0023206275204543463
480, epoch_train_loss=0.0023206275204543463
Epoch 481
Epoch 481 :: Batch 0/1
Batch Loss = 0.0023199488686270206
481, epoch_train_loss=0.0023199488686270206
Epoch 482
Epoch 482 :: Batch 0/1
Batch Loss = 0.002319326012713202
482, epoch_train_loss=0.002319326012713202
Epoch 483
Epoch 483 :: Batch 0/1
Batch Loss = 0.002318680059761509
483, epoch_train_loss=0.002318680059761509
Epoch 484
Epoch 484 :: Batch 0/1
Batch Loss = 0.002318012679852375
484, epoch_train_loss=0.002318012679852375
Epoch 485
Epoch 485 :: Batch 0/1
Batch Loss = 0.0023173870149424997
485, epoch_train_loss=0.0023173870149424997
Epoch 486
Epoch 486 :: Batch 0/1
Batch Loss = 0.0023167584988559556
486, epoch_train_loss=0.0023167584988559556
Epoch 487
Epoch 487 :: Batch 0/1
Batch Loss = 0.0023161059130747245
487, epoch_train_loss=0.0023161059130747245
Epoch 488
Epoch 488 :: Batch 0/1
Batch Loss = 0.002315478676266003
488, epoch_train_loss=0.002315478676266003
Epoch 489
Epoch 489 :: Batch 0/1
Batch Loss = 0.002314862175461148
489, epoch_train_loss=0.002314862175461148
Epoch 490
Epoch 490 :: Batch 0/1
Batch Loss = 0.0023142256959636375
490, epoch_train_loss=0.0023142256959636375
Epoch 491
Epoch 491 :: Batch 0/1
Batch Loss = 0.0023135997118033293
491, epoch_train_loss=0.0023135997118033293
Epoch 492
Epoch 492 :: Batch 0/1
Batch Loss = 0.002312990629088588
492, epoch_train_loss=0.002312990629088588
Epoch 493
Epoch 493 :: Batch 0/1
Batch Loss = 0.00231236954450388
493, epoch_train_loss=0.00231236954450388
Epoch 494
Epoch 494 :: Batch 0/1
Batch Loss = 0.00231174853039175
494, epoch_train_loss=0.00231174853039175
Epoch 495
Epoch 495 :: Batch 0/1
Batch Loss = 0.0023111435521551453
495, epoch_train_loss=0.0023111435521551453
Epoch 496
Epoch 496 :: Batch 0/1
Batch Loss = 0.002310535851171465
496, epoch_train_loss=0.002310535851171465
Epoch 497
Epoch 497 :: Batch 0/1
Batch Loss = 0.0023099227989202623
497, epoch_train_loss=0.0023099227989202623
Epoch 498
Epoch 498 :: Batch 0/1
Batch Loss = 0.0023093209011184894
498, epoch_train_loss=0.0023093209011184894
Epoch 499
Epoch 499 :: Batch 0/1
Batch Loss = 0.0023087233828559257
499, epoch_train_loss=0.0023087233828559257
Epoch 500
Epoch 500 :: Batch 0/1
Batch Loss = 0.002308120289530301
500, epoch_train_loss=0.002308120289530301
Epoch 501
Epoch 501 :: Batch 0/1
Batch Loss = 0.002307522296994452
501, epoch_train_loss=0.002307522296994452
Epoch 502
Epoch 502 :: Batch 0/1
Batch Loss = 0.0023069317934494705
502, epoch_train_loss=0.0023069317934494705
Epoch 503
Epoch 503 :: Batch 0/1
Batch Loss = 0.0023063390367893863
503, epoch_train_loss=0.0023063390367893863
Epoch 504
Epoch 504 :: Batch 0/1
Batch Loss = 0.0023057466528954144
504, epoch_train_loss=0.0023057466528954144
Epoch 505
Epoch 505 :: Batch 0/1
Batch Loss = 0.0023051612189111618
505, epoch_train_loss=0.0023051612189111618
Epoch 506
Epoch 506 :: Batch 0/1
Batch Loss = 0.0023045775032203607
506, epoch_train_loss=0.0023045775032203607
Epoch 507
Epoch 507 :: Batch 0/1
Batch Loss = 0.0023039924691114
507, epoch_train_loss=0.0023039924691114
Epoch 508
Epoch 508 :: Batch 0/1
Batch Loss = 0.0023034116452498622
508, epoch_train_loss=0.0023034116452498622
Epoch 509
Epoch 509 :: Batch 0/1
Batch Loss = 0.0023028349792998544
509, epoch_train_loss=0.0023028349792998544
Epoch 510
Epoch 510 :: Batch 0/1
Batch Loss = 0.002302258048295505
510, epoch_train_loss=0.002302258048295505
Epoch 511
Epoch 511 :: Batch 0/1
Batch Loss = 0.0023016825762995205
511, epoch_train_loss=0.0023016825762995205
Epoch 512
Epoch 512 :: Batch 0/1
Batch Loss = 0.0023011114352450603
512, epoch_train_loss=0.0023011114352450603
Epoch 513
Epoch 513 :: Batch 0/1
Batch Loss = 0.002300542055280686
513, epoch_train_loss=0.002300542055280686
Epoch 514
Epoch 514 :: Batch 0/1
Batch Loss = 0.002299973021807358
514, epoch_train_loss=0.002299973021807358
Epoch 515
Epoch 515 :: Batch 0/1
Batch Loss = 0.0022994068996154654
515, epoch_train_loss=0.0022994068996154654
Epoch 516
Epoch 516 :: Batch 0/1
Batch Loss = 0.0022988438352476563
516, epoch_train_loss=0.0022988438352476563
Epoch 517
Epoch 517 :: Batch 0/1
Batch Loss = 0.002298281701278502
517, epoch_train_loss=0.002298281701278502
Epoch 518
Epoch 518 :: Batch 0/1
Batch Loss = 0.0022977210125116666
518, epoch_train_loss=0.0022977210125116666
Epoch 519
Epoch 519 :: Batch 0/1
Batch Loss = 0.002297163282657789
519, epoch_train_loss=0.002297163282657789
Epoch 520
Epoch 520 :: Batch 0/1
Batch Loss = 0.0022966075852712477
520, epoch_train_loss=0.0022966075852712477
Epoch 521
Epoch 521 :: Batch 0/1
Batch Loss = 0.002296052934974286
521, epoch_train_loss=0.002296052934974286
Epoch 522
Epoch 522 :: Batch 0/1
Batch Loss = 0.0022955003223584397
522, epoch_train_loss=0.0022955003223584397
Epoch 523
Epoch 523 :: Batch 0/1
Batch Loss = 0.0022949502210798662
523, epoch_train_loss=0.0022949502210798662
Epoch 524
Epoch 524 :: Batch 0/1
Batch Loss = 0.0022944016886809067
524, epoch_train_loss=0.0022944016886809067
Epoch 525
Epoch 525 :: Batch 0/1
Batch Loss = 0.0022938545245680913
525, epoch_train_loss=0.0022938545245680913
Epoch 526
Epoch 526 :: Batch 0/1
Batch Loss = 0.0022933095133604365
526, epoch_train_loss=0.0022933095133604365
Epoch 527
Epoch 527 :: Batch 0/1
Batch Loss = 0.0022927665966492954
527, epoch_train_loss=0.0022927665966492954
Epoch 528
Epoch 528 :: Batch 0/1
Batch Loss = 0.0022922251189319053
528, epoch_train_loss=0.0022922251189319053
Epoch 529
Epoch 529 :: Batch 0/1
Batch Loss = 0.002291685239917719
529, epoch_train_loss=0.002291685239917719
Epoch 530
Epoch 530 :: Batch 0/1
Batch Loss = 0.0022911474092282558
530, epoch_train_loss=0.0022911474092282558
Epoch 531
Epoch 531 :: Batch 0/1
Batch Loss = 0.0022906113983437106
531, epoch_train_loss=0.0022906113983437106
Epoch 532
Epoch 532 :: Batch 0/1
Batch Loss = 0.0022900768503525083
532, epoch_train_loss=0.0022900768503525083
Epoch 533
Epoch 533 :: Batch 0/1
Batch Loss = 0.0022895439747461026
533, epoch_train_loss=0.0022895439747461026
Epoch 534
Epoch 534 :: Batch 0/1
Batch Loss = 0.002289013006086717
534, epoch_train_loss=0.002289013006086717
Epoch 535
Epoch 535 :: Batch 0/1
Batch Loss = 0.0022884837129096298
535, epoch_train_loss=0.0022884837129096298
Epoch 536
Epoch 536 :: Batch 0/1
Batch Loss = 0.0022879559124255873
536, epoch_train_loss=0.0022879559124255873
Epoch 537
Epoch 537 :: Batch 0/1
Batch Loss = 0.0022874297842745794
537, epoch_train_loss=0.0022874297842745794
Epoch 538
Epoch 538 :: Batch 0/1
Batch Loss = 0.002286905433213736
538, epoch_train_loss=0.002286905433213736
Epoch 539
Epoch 539 :: Batch 0/1
Batch Loss = 0.002286382681868855
539, epoch_train_loss=0.002286382681868855
Epoch 540
Epoch 540 :: Batch 0/1
Batch Loss = 0.0022858614372125233
540, epoch_train_loss=0.0022858614372125233
Epoch 541
Epoch 541 :: Batch 0/1
Batch Loss = 0.002285341823685126
541, epoch_train_loss=0.002285341823685126
Epoch 542
Epoch 542 :: Batch 0/1
Batch Loss = 0.0022848238878133385
542, epoch_train_loss=0.0022848238878133385
Epoch 543
Epoch 543 :: Batch 0/1
Batch Loss = 0.0022843075086065538
543, epoch_train_loss=0.0022843075086065538
Epoch 544
Epoch 544 :: Batch 0/1
Batch Loss = 0.0022837926281870824
544, epoch_train_loss=0.0022837926281870824
Epoch 545
Epoch 545 :: Batch 0/1
Batch Loss = 0.002283279327726892
545, epoch_train_loss=0.002283279327726892
Epoch 546
Epoch 546 :: Batch 0/1
Batch Loss = 0.0022827676324672897
546, epoch_train_loss=0.0022827676324672897
Epoch 547
Epoch 547 :: Batch 0/1
Batch Loss = 0.0022822574582662372
547, epoch_train_loss=0.0022822574582662372
Epoch 548
Epoch 548 :: Batch 0/1
Batch Loss = 0.002281748767662206
548, epoch_train_loss=0.002281748767662206
Epoch 549
Epoch 549 :: Batch 0/1
Batch Loss = 0.0022812416053518314
549, epoch_train_loss=0.0022812416053518314
Epoch 550
Epoch 550 :: Batch 0/1
Batch Loss = 0.0022807359890046138
550, epoch_train_loss=0.0022807359890046138
Epoch 551
Epoch 551 :: Batch 0/1
Batch Loss = 0.0022802318667623604
551, epoch_train_loss=0.0022802318667623604
Epoch 552
Epoch 552 :: Batch 0/1
Batch Loss = 0.0022797292032184146
552, epoch_train_loss=0.0022797292032184146
Epoch 553
Epoch 553 :: Batch 0/1
Batch Loss = 0.0022792280237840614
553, epoch_train_loss=0.0022792280237840614
Epoch 554
Epoch 554 :: Batch 0/1
Batch Loss = 0.002278728341856184
554, epoch_train_loss=0.002278728341856184
Epoch 555
Epoch 555 :: Batch 0/1
Batch Loss = 0.0022782301252899507
555, epoch_train_loss=0.0022782301252899507
Epoch 556
Epoch 556 :: Batch 0/1
Batch Loss = 0.0022777333456594933
556, epoch_train_loss=0.0022777333456594933
Epoch 557
Epoch 557 :: Batch 0/1
Batch Loss = 0.002277238009582513
557, epoch_train_loss=0.002277238009582513
Epoch 558
Epoch 558 :: Batch 0/1
Batch Loss = 0.002276744129484128
558, epoch_train_loss=0.002276744129484128
Epoch 559
Epoch 559 :: Batch 0/1
Batch Loss = 0.002276251687979613
559, epoch_train_loss=0.002276251687979613
Epoch 560
Epoch 560 :: Batch 0/1
Batch Loss = 0.0022757606597008186
560, epoch_train_loss=0.0022757606597008186
Epoch 561
Epoch 561 :: Batch 0/1
Batch Loss = 0.0022752710426022864
561, epoch_train_loss=0.0022752710426022864
Epoch 562
Epoch 562 :: Batch 0/1
Batch Loss = 0.002274782843538894
562, epoch_train_loss=0.002274782843538894
Epoch 563
Epoch 563 :: Batch 0/1
Batch Loss = 0.0022742960551255997
563, epoch_train_loss=0.0022742960551255997
Epoch 564
Epoch 564 :: Batch 0/1
Batch Loss = 0.0022738106588613646
564, epoch_train_loss=0.0022738106588613646
Epoch 565
Epoch 565 :: Batch 0/1
Batch Loss = 0.002273326645237453
565, epoch_train_loss=0.002273326645237453
Epoch 566
Epoch 566 :: Batch 0/1
Batch Loss = 0.0022728440170054903
566, epoch_train_loss=0.0022728440170054903
Epoch 567
Epoch 567 :: Batch 0/1
Batch Loss = 0.0022723627716717564
567, epoch_train_loss=0.0022723627716717564
Epoch 568
Epoch 568 :: Batch 0/1
Batch Loss = 0.002271882897100527
568, epoch_train_loss=0.002271882897100527
Epoch 569
Epoch 569 :: Batch 0/1
Batch Loss = 0.002271404382474414
569, epoch_train_loss=0.002271404382474414
Epoch 570
Epoch 570 :: Batch 0/1
Batch Loss = 0.00227092722454205
570, epoch_train_loss=0.00227092722454205
Epoch 571
Epoch 571 :: Batch 0/1
Batch Loss = 0.0022704514227470645
571, epoch_train_loss=0.0022704514227470645
Epoch 572
Epoch 572 :: Batch 0/1
Batch Loss = 0.0022699769705029193
572, epoch_train_loss=0.0022699769705029193
Epoch 573
Epoch 573 :: Batch 0/1
Batch Loss = 0.002269503857721035
573, epoch_train_loss=0.002269503857721035
Epoch 574
Epoch 574 :: Batch 0/1
Batch Loss = 0.0022690320784459353
574, epoch_train_loss=0.0022690320784459353
Epoch 575
Epoch 575 :: Batch 0/1
Batch Loss = 0.0022685616301927193
575, epoch_train_loss=0.0022685616301927193
Epoch 576
Epoch 576 :: Batch 0/1
Batch Loss = 0.0022680925097840242
576, epoch_train_loss=0.0022680925097840242
Epoch 577
Epoch 577 :: Batch 0/1
Batch Loss = 0.002267624710353926
577, epoch_train_loss=0.002267624710353926
Epoch 578
Epoch 578 :: Batch 0/1
Batch Loss = 0.002267158224586079
578, epoch_train_loss=0.002267158224586079
Epoch 579
Epoch 579 :: Batch 0/1
Batch Loss = 0.002266693048383399
579, epoch_train_loss=0.002266693048383399
Epoch 580
Epoch 580 :: Batch 0/1
Batch Loss = 0.002266229178650713
580, epoch_train_loss=0.002266229178650713
Epoch 581
Epoch 581 :: Batch 0/1
Batch Loss = 0.002265766611279557
581, epoch_train_loss=0.002265766611279557
Epoch 582
Epoch 582 :: Batch 0/1
Batch Loss = 0.002265305340454139
582, epoch_train_loss=0.002265305340454139
Epoch 583
Epoch 583 :: Batch 0/1
Batch Loss = 0.002264845360508232
583, epoch_train_loss=0.002264845360508232
Epoch 584
Epoch 584 :: Batch 0/1
Batch Loss = 0.0022643866677352657
584, epoch_train_loss=0.0022643866677352657
Epoch 585
Epoch 585 :: Batch 0/1
Batch Loss = 0.0022639292587839357
585, epoch_train_loss=0.0022639292587839357
Epoch 586
Epoch 586 :: Batch 0/1
Batch Loss = 0.0022634731295746334
586, epoch_train_loss=0.0022634731295746334
Epoch 587
Epoch 587 :: Batch 0/1
Batch Loss = 0.002263018275220396
587, epoch_train_loss=0.002263018275220396
Epoch 588
Epoch 588 :: Batch 0/1
Batch Loss = 0.002262564691031382
588, epoch_train_loss=0.002262564691031382
Epoch 589
Epoch 589 :: Batch 0/1
Batch Loss = 0.0022621123735172654
589, epoch_train_loss=0.0022621123735172654
Epoch 590
Epoch 590 :: Batch 0/1
Batch Loss = 0.0022616613192512794
590, epoch_train_loss=0.0022616613192512794
Epoch 591
Epoch 591 :: Batch 0/1
Batch Loss = 0.0022612115245258697
591, epoch_train_loss=0.0022612115245258697
Epoch 592
Epoch 592 :: Batch 0/1
Batch Loss = 0.0022607629851979626
592, epoch_train_loss=0.0022607629851979626
Epoch 593
Epoch 593 :: Batch 0/1
Batch Loss = 0.0022603156971220183
593, epoch_train_loss=0.0022603156971220183
Epoch 594
Epoch 594 :: Batch 0/1
Batch Loss = 0.0022598696569101286
594, epoch_train_loss=0.0022598696569101286
Epoch 595
Epoch 595 :: Batch 0/1
Batch Loss = 0.0022594248612120478
595, epoch_train_loss=0.0022594248612120478
Epoch 596
Epoch 596 :: Batch 0/1
Batch Loss = 0.0022589813066767276
596, epoch_train_loss=0.0022589813066767276
Epoch 597
Epoch 597 :: Batch 0/1
Batch Loss = 0.0022585389896605866
597, epoch_train_loss=0.0022585389896605866
Epoch 598
Epoch 598 :: Batch 0/1
Batch Loss = 0.0022580979064691893
598, epoch_train_loss=0.0022580979064691893
Epoch 599
Epoch 599 :: Batch 0/1
Batch Loss = 0.0022576580537877135
599, epoch_train_loss=0.0022576580537877135
Epoch 600
Epoch 600 :: Batch 0/1
Batch Loss = 0.002257219428422036
600, epoch_train_loss=0.002257219428422036
Epoch 601
Epoch 601 :: Batch 0/1
Batch Loss = 0.002256782027267844
601, epoch_train_loss=0.002256782027267844
Epoch 602
Epoch 602 :: Batch 0/1
Batch Loss = 0.0022563458470070993
602, epoch_train_loss=0.0022563458470070993
Epoch 603
Epoch 603 :: Batch 0/1
Batch Loss = 0.0022559108842873463
603, epoch_train_loss=0.0022559108842873463
Epoch 604
Epoch 604 :: Batch 0/1
Batch Loss = 0.0022554771359431175
604, epoch_train_loss=0.0022554771359431175
Epoch 605
Epoch 605 :: Batch 0/1
Batch Loss = 0.0022550445988045063
605, epoch_train_loss=0.0022550445988045063
Epoch 606
Epoch 606 :: Batch 0/1
Batch Loss = 0.0022546132699332116
606, epoch_train_loss=0.0022546132699332116
Epoch 607
Epoch 607 :: Batch 0/1
Batch Loss = 0.0022541831462764084
607, epoch_train_loss=0.0022541831462764084
Epoch 608
Epoch 608 :: Batch 0/1
Batch Loss = 0.0022537542247033538
608, epoch_train_loss=0.0022537542247033538
Epoch 609
Epoch 609 :: Batch 0/1
Batch Loss = 0.0022533265021958434
609, epoch_train_loss=0.0022533265021958434
Epoch 610
Epoch 610 :: Batch 0/1
Batch Loss = 0.0022528999756671847
610, epoch_train_loss=0.0022528999756671847
Epoch 611
Epoch 611 :: Batch 0/1
Batch Loss = 0.0022524746422598546
611, epoch_train_loss=0.0022524746422598546
Epoch 612
Epoch 612 :: Batch 0/1
Batch Loss = 0.002252050499000455
612, epoch_train_loss=0.002252050499000455
Epoch 613
Epoch 613 :: Batch 0/1
Batch Loss = 0.0022516275429682207
613, epoch_train_loss=0.0022516275429682207
Epoch 614
Epoch 614 :: Batch 0/1
Batch Loss = 0.002251205771258386
614, epoch_train_loss=0.002251205771258386
Epoch 615
Epoch 615 :: Batch 0/1
Batch Loss = 0.0022507851809003333
615, epoch_train_loss=0.0022507851809003333
Epoch 616
Epoch 616 :: Batch 0/1
Batch Loss = 0.0022503657689745766
616, epoch_train_loss=0.0022503657689745766
Epoch 617
Epoch 617 :: Batch 0/1
Batch Loss = 0.0022499475325734883
617, epoch_train_loss=0.0022499475325734883
Epoch 618
Epoch 618 :: Batch 0/1
Batch Loss = 0.0022495304688520024
618, epoch_train_loss=0.0022495304688520024
Epoch 619
Epoch 619 :: Batch 0/1
Batch Loss = 0.0022491145749210318
619, epoch_train_loss=0.0022491145749210318
Epoch 620
Epoch 620 :: Batch 0/1
Batch Loss = 0.0022486998479218415
620, epoch_train_loss=0.0022486998479218415
Epoch 621
Epoch 621 :: Batch 0/1
Batch Loss = 0.0022482862849390233
621, epoch_train_loss=0.0022482862849390233
Epoch 622
Epoch 622 :: Batch 0/1
Batch Loss = 0.002247873883055345
622, epoch_train_loss=0.002247873883055345
Epoch 623
Epoch 623 :: Batch 0/1
Batch Loss = 0.0022474626394079975
623, epoch_train_loss=0.0022474626394079975
Epoch 624
Epoch 624 :: Batch 0/1
Batch Loss = 0.0022470525510954607
624, epoch_train_loss=0.0022470525510954607
Epoch 625
Epoch 625 :: Batch 0/1
Batch Loss = 0.0022466436152682625
625, epoch_train_loss=0.0022466436152682625
Epoch 626
Epoch 626 :: Batch 0/1
Batch Loss = 0.002246235828984837
626, epoch_train_loss=0.002246235828984837
Epoch 627
Epoch 627 :: Batch 0/1
Batch Loss = 0.0022458291893209326
627, epoch_train_loss=0.0022458291893209326
Epoch 628
Epoch 628 :: Batch 0/1
Batch Loss = 0.0022454236933514664
628, epoch_train_loss=0.0022454236933514664
Epoch 629
Epoch 629 :: Batch 0/1
Batch Loss = 0.0022450193381187335
629, epoch_train_loss=0.0022450193381187335
Epoch 630
Epoch 630 :: Batch 0/1
Batch Loss = 0.002244616120687689
630, epoch_train_loss=0.002244616120687689
Epoch 631
Epoch 631 :: Batch 0/1
Batch Loss = 0.0022442140380801003
631, epoch_train_loss=0.0022442140380801003
Epoch 632
Epoch 632 :: Batch 0/1
Batch Loss = 0.0022438130873227015
632, epoch_train_loss=0.0022438130873227015
Epoch 633
Epoch 633 :: Batch 0/1
Batch Loss = 0.0022434132654096397
633, epoch_train_loss=0.0022434132654096397
Epoch 634
Epoch 634 :: Batch 0/1
Batch Loss = 0.0022430145693140293
634, epoch_train_loss=0.0022430145693140293
Epoch 635
Epoch 635 :: Batch 0/1
Batch Loss = 0.002242616995992573
635, epoch_train_loss=0.002242616995992573
Epoch 636
Epoch 636 :: Batch 0/1
Batch Loss = 0.0022422205423383917
636, epoch_train_loss=0.0022422205423383917
Epoch 637
Epoch 637 :: Batch 0/1
Batch Loss = 0.0022418252053310822
637, epoch_train_loss=0.0022418252053310822
Epoch 638
Epoch 638 :: Batch 0/1
Batch Loss = 0.002241430981785495
638, epoch_train_loss=0.002241430981785495
Epoch 639
Epoch 639 :: Batch 0/1
Batch Loss = 0.0022410378686023615
639, epoch_train_loss=0.0022410378686023615
Epoch 640
Epoch 640 :: Batch 0/1
Batch Loss = 0.0022406458626105254
640, epoch_train_loss=0.0022406458626105254
Epoch 641
Epoch 641 :: Batch 0/1
Batch Loss = 0.002240254960618506
641, epoch_train_loss=0.002240254960618506
Epoch 642
Epoch 642 :: Batch 0/1
Batch Loss = 0.0022398651593741607
642, epoch_train_loss=0.0022398651593741607
Epoch 643
Epoch 643 :: Batch 0/1
Batch Loss = 0.0022394764556517392
643, epoch_train_loss=0.0022394764556517392
Epoch 644
Epoch 644 :: Batch 0/1
Batch Loss = 0.0022390888461715957
644, epoch_train_loss=0.0022390888461715957
Epoch 645
Epoch 645 :: Batch 0/1
Batch Loss = 0.002238702327583709
645, epoch_train_loss=0.002238702327583709
Epoch 646
Epoch 646 :: Batch 0/1
Batch Loss = 0.002238316896578382
646, epoch_train_loss=0.002238316896578382
Epoch 647
Epoch 647 :: Batch 0/1
Batch Loss = 0.0022379325497703184
647, epoch_train_loss=0.0022379325497703184
Epoch 648
Epoch 648 :: Batch 0/1
Batch Loss = 0.0022375492837238596
648, epoch_train_loss=0.0022375492837238596
Epoch 649
Epoch 649 :: Batch 0/1
Batch Loss = 0.002237167095018537
649, epoch_train_loss=0.002237167095018537
Epoch 650
Epoch 650 :: Batch 0/1
Batch Loss = 0.002236785980146068
650, epoch_train_loss=0.002236785980146068
Epoch 651
Epoch 651 :: Batch 0/1
Batch Loss = 0.0022364059355888454
651, epoch_train_loss=0.0022364059355888454
Epoch 652
Epoch 652 :: Batch 0/1
Batch Loss = 0.0022360269578226904
652, epoch_train_loss=0.0022360269578226904
Epoch 653
Epoch 653 :: Batch 0/1
Batch Loss = 0.0022356490432303916
653, epoch_train_loss=0.0022356490432303916
Epoch 654
Epoch 654 :: Batch 0/1
Batch Loss = 0.002235272188210777
654, epoch_train_loss=0.002235272188210777
Epoch 655
Epoch 655 :: Batch 0/1
Batch Loss = 0.0022348963891025554
655, epoch_train_loss=0.0022348963891025554
Epoch 656
Epoch 656 :: Batch 0/1
Batch Loss = 0.002234521642169727
656, epoch_train_loss=0.002234521642169727
Epoch 657
Epoch 657 :: Batch 0/1
Batch Loss = 0.0022341479436747053
657, epoch_train_loss=0.0022341479436747053
Epoch 658
Epoch 658 :: Batch 0/1
Batch Loss = 0.0022337752898337836
658, epoch_train_loss=0.0022337752898337836
Epoch 659
Epoch 659 :: Batch 0/1
Batch Loss = 0.002233403676859465
659, epoch_train_loss=0.002233403676859465
Epoch 660
Epoch 660 :: Batch 0/1
Batch Loss = 0.002233033100861439
660, epoch_train_loss=0.002233033100861439
Epoch 661
Epoch 661 :: Batch 0/1
Batch Loss = 0.0022326635579439677
661, epoch_train_loss=0.0022326635579439677
Epoch 662
Epoch 662 :: Batch 0/1
Batch Loss = 0.0022322950441949424
662, epoch_train_loss=0.0022322950441949424
Epoch 663
Epoch 663 :: Batch 0/1
Batch Loss = 0.0022319275555806547
663, epoch_train_loss=0.0022319275555806547
Epoch 664
Epoch 664 :: Batch 0/1
Batch Loss = 0.002231561088138101
664, epoch_train_loss=0.002231561088138101
Epoch 665
Epoch 665 :: Batch 0/1
Batch Loss = 0.0022311956377800935
665, epoch_train_loss=0.0022311956377800935
Epoch 666
Epoch 666 :: Batch 0/1
Batch Loss = 0.002230831200435081
666, epoch_train_loss=0.002230831200435081
Epoch 667
Epoch 667 :: Batch 0/1
Batch Loss = 0.002230467771906647
667, epoch_train_loss=0.002230467771906647
Epoch 668
Epoch 668 :: Batch 0/1
Batch Loss = 0.0022301053480709818
668, epoch_train_loss=0.0022301053480709818
Epoch 669
Epoch 669 :: Batch 0/1
Batch Loss = 0.0022297439246502584
669, epoch_train_loss=0.0022297439246502584
Epoch 670
Epoch 670 :: Batch 0/1
Batch Loss = 0.002229383497440285
670, epoch_train_loss=0.002229383497440285
Epoch 671
Epoch 671 :: Batch 0/1
Batch Loss = 0.0022290240621098824
671, epoch_train_loss=0.0022290240621098824
Epoch 672
Epoch 672 :: Batch 0/1
Batch Loss = 0.0022286656143191926
672, epoch_train_loss=0.0022286656143191926
Epoch 673
Epoch 673 :: Batch 0/1
Batch Loss = 0.00222830814965675
673, epoch_train_loss=0.00222830814965675
Epoch 674
Epoch 674 :: Batch 0/1
Batch Loss = 0.002227951663757886
674, epoch_train_loss=0.002227951663757886
Epoch 675
Epoch 675 :: Batch 0/1
Batch Loss = 0.0022275961521036488
675, epoch_train_loss=0.0022275961521036488
Epoch 676
Epoch 676 :: Batch 0/1
Batch Loss = 0.0022272416102503646
676, epoch_train_loss=0.0022272416102503646
Epoch 677
Epoch 677 :: Batch 0/1
Batch Loss = 0.00222688803360022
677, epoch_train_loss=0.00222688803360022
Epoch 678
Epoch 678 :: Batch 0/1
Batch Loss = 0.002226535417604746
678, epoch_train_loss=0.002226535417604746
Epoch 679
Epoch 679 :: Batch 0/1
Batch Loss = 0.00222618375764235
679, epoch_train_loss=0.00222618375764235
Epoch 680
Epoch 680 :: Batch 0/1
Batch Loss = 0.002225833049061806
680, epoch_train_loss=0.002225833049061806
Epoch 681
Epoch 681 :: Batch 0/1
Batch Loss = 0.0022254832871375792
681, epoch_train_loss=0.0022254832871375792
Epoch 682
Epoch 682 :: Batch 0/1
Batch Loss = 0.0022251344671716925
682, epoch_train_loss=0.0022251344671716925
Epoch 683
Epoch 683 :: Batch 0/1
Batch Loss = 0.0022247865843893736
683, epoch_train_loss=0.0022247865843893736
Epoch 684
Epoch 684 :: Batch 0/1
Batch Loss = 0.0022244396339948765
684, epoch_train_loss=0.0022244396339948765
Epoch 685
Epoch 685 :: Batch 0/1
Batch Loss = 0.002224093611138688
685, epoch_train_loss=0.002224093611138688
Epoch 686
Epoch 686 :: Batch 0/1
Batch Loss = 0.0022237485109321167
686, epoch_train_loss=0.0022237485109321167
Epoch 687
Epoch 687 :: Batch 0/1
Batch Loss = 0.002223404328479064
687, epoch_train_loss=0.002223404328479064
Epoch 688
Epoch 688 :: Batch 0/1
Batch Loss = 0.002223061058831484
688, epoch_train_loss=0.002223061058831484
Epoch 689
Epoch 689 :: Batch 0/1
Batch Loss = 0.0022227186970240247
689, epoch_train_loss=0.0022227186970240247
Epoch 690
Epoch 690 :: Batch 0/1
Batch Loss = 0.002222377238090104
690, epoch_train_loss=0.002222377238090104
Epoch 691
Epoch 691 :: Batch 0/1
Batch Loss = 0.0022220366769550577
691, epoch_train_loss=0.0022220366769550577
Epoch 692
Epoch 692 :: Batch 0/1
Batch Loss = 0.0022216970086710195
692, epoch_train_loss=0.0022216970086710195
Epoch 693
Epoch 693 :: Batch 0/1
Batch Loss = 0.0022213582281507918
693, epoch_train_loss=0.0022213582281507918
Epoch 694
Epoch 694 :: Batch 0/1
Batch Loss = 0.00222102033052388
694, epoch_train_loss=0.00222102033052388
Epoch 695
Epoch 695 :: Batch 0/1
Batch Loss = 0.0022206833109080543
695, epoch_train_loss=0.0022206833109080543
Epoch 696
Epoch 696 :: Batch 0/1
Batch Loss = 0.0022203471648794904
696, epoch_train_loss=0.0022203471648794904
Epoch 697
Epoch 697 :: Batch 0/1
Batch Loss = 0.0022200118883403345
697, epoch_train_loss=0.0022200118883403345
Epoch 698
Epoch 698 :: Batch 0/1
Batch Loss = 0.0022196774786706276
698, epoch_train_loss=0.0022196774786706276
Epoch 699
Epoch 699 :: Batch 0/1
Batch Loss = 0.0022193439349979943
699, epoch_train_loss=0.0022193439349979943
Epoch 700
Epoch 700 :: Batch 0/1
Batch Loss = 0.00221901126165349
700, epoch_train_loss=0.00221901126165349
Epoch 701
Epoch 701 :: Batch 0/1
Batch Loss = 0.0022186794709237573
701, epoch_train_loss=0.0022186794709237573
Epoch 702
Epoch 702 :: Batch 0/1
Batch Loss = 0.0022183485945205363
702, epoch_train_loss=0.0022183485945205363
Epoch 703
Epoch 703 :: Batch 0/1
Batch Loss = 0.0022180186993415105
703, epoch_train_loss=0.0022180186993415105
Epoch 704
Epoch 704 :: Batch 0/1
Batch Loss = 0.002217689930780014
704, epoch_train_loss=0.002217689930780014
Epoch 705
Epoch 705 :: Batch 0/1
Batch Loss = 0.0022173625892459464
705, epoch_train_loss=0.0022173625892459464
Epoch 706
Epoch 706 :: Batch 0/1
Batch Loss = 0.0022170373117418453
706, epoch_train_loss=0.0022170373117418453
Epoch 707
Epoch 707 :: Batch 0/1
Batch Loss = 0.0022167154351844464
707, epoch_train_loss=0.0022167154351844464
Epoch 708
Epoch 708 :: Batch 0/1
Batch Loss = 0.0022163998105908136
708, epoch_train_loss=0.0022163998105908136
Epoch 709
Epoch 709 :: Batch 0/1
Batch Loss = 0.0022160965523888958
709, epoch_train_loss=0.0022160965523888958
Epoch 710
Epoch 710 :: Batch 0/1
Batch Loss = 0.0022158188733747944
710, epoch_train_loss=0.0022158188733747944
Epoch 711
Epoch 711 :: Batch 0/1
Batch Loss = 0.0022155957694438774
711, epoch_train_loss=0.0022155957694438774
Epoch 712
Epoch 712 :: Batch 0/1
Batch Loss = 0.0022154907621575364
712, epoch_train_loss=0.0022154907621575364
Epoch 713
Epoch 713 :: Batch 0/1
Batch Loss = 0.0022156467733729316
713, epoch_train_loss=0.0022156467733729316
Epoch 714
Epoch 714 :: Batch 0/1
Batch Loss = 0.002216380134836214
714, epoch_train_loss=0.002216380134836214
Epoch 715
Epoch 715 :: Batch 0/1
Batch Loss = 0.0022184242766443425
715, epoch_train_loss=0.0022184242766443425
Epoch 716
Epoch 716 :: Batch 0/1
Batch Loss = 0.0022234050615161163
716, epoch_train_loss=0.0022234050615161163
Epoch 717
Epoch 717 :: Batch 0/1
Batch Loss = 0.002235252925779334
717, epoch_train_loss=0.002235252925779334
Epoch 718
Epoch 718 :: Batch 0/1
Batch Loss = 0.002262524767486638
718, epoch_train_loss=0.002262524767486638
Epoch 719
Epoch 719 :: Batch 0/1
Batch Loss = 0.002327353190521142
719, epoch_train_loss=0.002327353190521142
Epoch 720
Epoch 720 :: Batch 0/1
Batch Loss = 0.0024749227134362155
720, epoch_train_loss=0.0024749227134362155
Epoch 721
Epoch 721 :: Batch 0/1
Batch Loss = 0.0028374171103047203
721, epoch_train_loss=0.0028374171103047203
Epoch 722
Epoch 722 :: Batch 0/1
Batch Loss = 0.0036385204394815303
722, epoch_train_loss=0.0036385204394815303
Epoch 723
Epoch 723 :: Batch 0/1
Batch Loss = 0.005713264675193052
723, epoch_train_loss=0.005713264675193052
Epoch 724
Epoch 724 :: Batch 0/1
Batch Loss = 0.009775184422219061
724, epoch_train_loss=0.009775184422219061
Epoch 725
Epoch 725 :: Batch 0/1
Batch Loss = 0.020853594866916864
725, epoch_train_loss=0.020853594866916864
Epoch 726
Epoch 726 :: Batch 0/1
Batch Loss = 0.03348155721977079
726, epoch_train_loss=0.03348155721977079
Epoch 727
Epoch 727 :: Batch 0/1
Batch Loss = 0.06580231812818269
727, epoch_train_loss=0.06580231812818269
Epoch 728
Epoch 728 :: Batch 0/1
Batch Loss = 0.04545237117827233
728, epoch_train_loss=0.04545237117827233
Epoch 729
Epoch 729 :: Batch 0/1
Batch Loss = 0.028022283637432
729, epoch_train_loss=0.028022283637432
Epoch 730
Epoch 730 :: Batch 0/1
Batch Loss = 0.0031255901371061616
730, epoch_train_loss=0.0031255901371061616
Epoch 731
Epoch 731 :: Batch 0/1
Batch Loss = 0.01390215304637881
731, epoch_train_loss=0.01390215304637881
Epoch 732
Epoch 732 :: Batch 0/1
Batch Loss = 0.03548901662790046
732, epoch_train_loss=0.03548901662790046
Epoch 733
Epoch 733 :: Batch 0/1
Batch Loss = 0.012256177399300147
733, epoch_train_loss=0.012256177399300147
Epoch 734
Epoch 734 :: Batch 0/1
Batch Loss = 0.0034609495946481966
734, epoch_train_loss=0.0034609495946481966
Epoch 735
Epoch 735 :: Batch 0/1
Batch Loss = 0.016670627390984877
735, epoch_train_loss=0.016670627390984877
Epoch 736
Epoch 736 :: Batch 0/1
Batch Loss = 0.01056451144213071
736, epoch_train_loss=0.01056451144213071
Epoch 737
Epoch 737 :: Batch 0/1
Batch Loss = 0.0026045381205628423
737, epoch_train_loss=0.0026045381205628423
Epoch 738
Epoch 738 :: Batch 0/1
Batch Loss = 0.008662950051649961
738, epoch_train_loss=0.008662950051649961
Epoch 739
Epoch 739 :: Batch 0/1
Batch Loss = 0.007663529983526354
739, epoch_train_loss=0.007663529983526354
Epoch 740
Epoch 740 :: Batch 0/1
Batch Loss = 0.0025680878163807357
740, epoch_train_loss=0.0025680878163807357
Epoch 741
Epoch 741 :: Batch 0/1
Batch Loss = 0.005881387929928564
741, epoch_train_loss=0.005881387929928564
Epoch 742
Epoch 742 :: Batch 0/1
Batch Loss = 0.005453457129236932
742, epoch_train_loss=0.005453457129236932
Epoch 743
Epoch 743 :: Batch 0/1
Batch Loss = 0.002672296834582386
743, epoch_train_loss=0.002672296834582386
Epoch 744
Epoch 744 :: Batch 0/1
Batch Loss = 0.0047746758536326515
744, epoch_train_loss=0.0047746758536326515
Epoch 745
Epoch 745 :: Batch 0/1
Batch Loss = 0.004189255840669483
745, epoch_train_loss=0.004189255840669483
Epoch 746
Epoch 746 :: Batch 0/1
Batch Loss = 0.0026779202346171147
746, epoch_train_loss=0.0026779202346171147
Epoch 747
Epoch 747 :: Batch 0/1
Batch Loss = 0.004242564915557719
747, epoch_train_loss=0.004242564915557719
Epoch 748
Epoch 748 :: Batch 0/1
Batch Loss = 0.003413566720235297
748, epoch_train_loss=0.003413566720235297
Epoch 749
Epoch 749 :: Batch 0/1
Batch Loss = 0.002753603614355082
749, epoch_train_loss=0.002753603614355082
Epoch 750
Epoch 750 :: Batch 0/1
Batch Loss = 0.0038025347433658903
750, epoch_train_loss=0.0038025347433658903
Epoch 751
Epoch 751 :: Batch 0/1
Batch Loss = 0.002928281963078587
751, epoch_train_loss=0.002928281963078587
Epoch 752
Epoch 752 :: Batch 0/1
Batch Loss = 0.00283834059938891
752, epoch_train_loss=0.00283834059938891
Epoch 753
Epoch 753 :: Batch 0/1
Batch Loss = 0.003425450254249431
753, epoch_train_loss=0.003425450254249431
Epoch 754
Epoch 754 :: Batch 0/1
Batch Loss = 0.0026803713697216075
754, epoch_train_loss=0.0026803713697216075
Epoch 755
Epoch 755 :: Batch 0/1
Batch Loss = 0.002818283815608675
755, epoch_train_loss=0.002818283815608675
Epoch 756
Epoch 756 :: Batch 0/1
Batch Loss = 0.0031559620306053416
756, epoch_train_loss=0.0031559620306053416
Epoch 757
Epoch 757 :: Batch 0/1
Batch Loss = 0.0025315119113734244
757, epoch_train_loss=0.0025315119113734244
Epoch 758
Epoch 758 :: Batch 0/1
Batch Loss = 0.002779281733840015
758, epoch_train_loss=0.002779281733840015
Epoch 759
Epoch 759 :: Batch 0/1
Batch Loss = 0.0029235494336010545
759, epoch_train_loss=0.0029235494336010545
Epoch 760
Epoch 760 :: Batch 0/1
Batch Loss = 0.00246322921015791
760, epoch_train_loss=0.00246322921015791
Epoch 761
Epoch 761 :: Batch 0/1
Batch Loss = 0.002700561645844831
761, epoch_train_loss=0.002700561645844831
Epoch 762
Epoch 762 :: Batch 0/1
Batch Loss = 0.0027549819396358146
762, epoch_train_loss=0.0027549819396358146
Epoch 763
Epoch 763 :: Batch 0/1
Batch Loss = 0.002413958513940789
763, epoch_train_loss=0.002413958513940789
Epoch 764
Epoch 764 :: Batch 0/1
Batch Loss = 0.0026012913436016236
764, epoch_train_loss=0.0026012913436016236
Epoch 765
Epoch 765 :: Batch 0/1
Batch Loss = 0.002644637274739348
765, epoch_train_loss=0.002644637274739348
Epoch 766
Epoch 766 :: Batch 0/1
Batch Loss = 0.002365646687339413
766, epoch_train_loss=0.002365646687339413
Epoch 767
Epoch 767 :: Batch 0/1
Batch Loss = 0.002514600799585241
767, epoch_train_loss=0.002514600799585241
Epoch 768
Epoch 768 :: Batch 0/1
Batch Loss = 0.002554646015061332
768, epoch_train_loss=0.002554646015061332
Epoch 769
Epoch 769 :: Batch 0/1
Batch Loss = 0.0023395743340908107
769, epoch_train_loss=0.0023395743340908107
Epoch 770
Epoch 770 :: Batch 0/1
Batch Loss = 0.002434514197801812
770, epoch_train_loss=0.002434514197801812
Epoch 771
Epoch 771 :: Batch 0/1
Batch Loss = 0.002487917688246002
771, epoch_train_loss=0.002487917688246002
Epoch 772
Epoch 772 :: Batch 0/1
Batch Loss = 0.002325863084876677
772, epoch_train_loss=0.002325863084876677
Epoch 773
Epoch 773 :: Batch 0/1
Batch Loss = 0.002365702381374205
773, epoch_train_loss=0.002365702381374205
Epoch 774
Epoch 774 :: Batch 0/1
Batch Loss = 0.0024356596314865516
774, epoch_train_loss=0.0024356596314865516
Epoch 775
Epoch 775 :: Batch 0/1
Batch Loss = 0.0023152111434514497
775, epoch_train_loss=0.0023152111434514497
Epoch 776
Epoch 776 :: Batch 0/1
Batch Loss = 0.002318773994243018
776, epoch_train_loss=0.002318773994243018
Epoch 777
Epoch 777 :: Batch 0/1
Batch Loss = 0.0023872458878928314
777, epoch_train_loss=0.0023872458878928314
Epoch 778
Epoch 778 :: Batch 0/1
Batch Loss = 0.0023117270283832176
778, epoch_train_loss=0.0023117270283832176
Epoch 779
Epoch 779 :: Batch 0/1
Batch Loss = 0.0022865372519161294
779, epoch_train_loss=0.0022865372519161294
Epoch 780
Epoch 780 :: Batch 0/1
Batch Loss = 0.0023430500429236453
780, epoch_train_loss=0.0023430500429236453
Epoch 781
Epoch 781 :: Batch 0/1
Batch Loss = 0.002310541025716627
781, epoch_train_loss=0.002310541025716627
Epoch 782
Epoch 782 :: Batch 0/1
Batch Loss = 0.002267194798899215
782, epoch_train_loss=0.002267194798899215
Epoch 783
Epoch 783 :: Batch 0/1
Batch Loss = 0.0023054717096976625
783, epoch_train_loss=0.0023054717096976625
Epoch 784
Epoch 784 :: Batch 0/1
Batch Loss = 0.002303356364201441
784, epoch_train_loss=0.002303356364201441
Epoch 785
Epoch 785 :: Batch 0/1
Batch Loss = 0.0022608349292580106
785, epoch_train_loss=0.0022608349292580106
Epoch 786
Epoch 786 :: Batch 0/1
Batch Loss = 0.002274844250362737
786, epoch_train_loss=0.002274844250362737
Epoch 787
Epoch 787 :: Batch 0/1
Batch Loss = 0.0022903872181519523
787, epoch_train_loss=0.0022903872181519523
Epoch 788
Epoch 788 :: Batch 0/1
Batch Loss = 0.002261881075843242
788, epoch_train_loss=0.002261881075843242
Epoch 789
Epoch 789 :: Batch 0/1
Batch Loss = 0.0022539885146829913
789, epoch_train_loss=0.0022539885146829913
Epoch 790
Epoch 790 :: Batch 0/1
Batch Loss = 0.002273542778490861
790, epoch_train_loss=0.002273542778490861
Epoch 791
Epoch 791 :: Batch 0/1
Batch Loss = 0.0022627107912724354
791, epoch_train_loss=0.0022627107912724354
Epoch 792
Epoch 792 :: Batch 0/1
Batch Loss = 0.002244763196061089
792, epoch_train_loss=0.002244763196061089
Epoch 793
Epoch 793 :: Batch 0/1
Batch Loss = 0.0022559540088564114
793, epoch_train_loss=0.0022559540088564114
Epoch 794
Epoch 794 :: Batch 0/1
Batch Loss = 0.0022591218828074294
794, epoch_train_loss=0.0022591218828074294
Epoch 795
Epoch 795 :: Batch 0/1
Batch Loss = 0.002243376076171339
795, epoch_train_loss=0.002243376076171339
Epoch 796
Epoch 796 :: Batch 0/1
Batch Loss = 0.002242044676785584
796, epoch_train_loss=0.002242044676785584
Epoch 797
Epoch 797 :: Batch 0/1
Batch Loss = 0.0022506317317104
797, epoch_train_loss=0.0022506317317104
Epoch 798
Epoch 798 :: Batch 0/1
Batch Loss = 0.0022436241220113134
798, epoch_train_loss=0.0022436241220113134
Epoch 799
Epoch 799 :: Batch 0/1
Batch Loss = 0.0022349241592513986
799, epoch_train_loss=0.0022349241592513986
Epoch 800
Epoch 800 :: Batch 0/1
Batch Loss = 0.002240138553278867
800, epoch_train_loss=0.002240138553278867
Epoch 801
Epoch 801 :: Batch 0/1
Batch Loss = 0.002241212751799328
801, epoch_train_loss=0.002241212751799328
Epoch 802
Epoch 802 :: Batch 0/1
Batch Loss = 0.0022331709009381267
802, epoch_train_loss=0.0022331709009381267
Epoch 803
Epoch 803 :: Batch 0/1
Batch Loss = 0.002231627186578915
803, epoch_train_loss=0.002231627186578915
Epoch 804
Epoch 804 :: Batch 0/1
Batch Loss = 0.002235408897912496
804, epoch_train_loss=0.002235408897912496
Epoch 805
Epoch 805 :: Batch 0/1
Batch Loss = 0.0022324943752615564
805, epoch_train_loss=0.0022324943752615564
Epoch 806
Epoch 806 :: Batch 0/1
Batch Loss = 0.002227271604165866
806, epoch_train_loss=0.002227271604165866
Epoch 807
Epoch 807 :: Batch 0/1
Batch Loss = 0.0022286065553816393
807, epoch_train_loss=0.0022286065553816393
Epoch 808
Epoch 808 :: Batch 0/1
Batch Loss = 0.0022297558533410184
808, epoch_train_loss=0.0022297558533410184
Epoch 809
Epoch 809 :: Batch 0/1
Batch Loss = 0.002225827690202595
809, epoch_train_loss=0.002225827690202595
Epoch 810
Epoch 810 :: Batch 0/1
Batch Loss = 0.0022236400409034796
810, epoch_train_loss=0.0022236400409034796
Epoch 811
Epoch 811 :: Batch 0/1
Batch Loss = 0.002225045782924466
811, epoch_train_loss=0.002225045782924466
Epoch 812
Epoch 812 :: Batch 0/1
Batch Loss = 0.0022243767233757175
812, epoch_train_loss=0.0022243767233757175
Epoch 813
Epoch 813 :: Batch 0/1
Batch Loss = 0.002221269628584515
813, epoch_train_loss=0.002221269628584515
Epoch 814
Epoch 814 :: Batch 0/1
Batch Loss = 0.0022205699784289767
814, epoch_train_loss=0.0022205699784289767
Epoch 815
Epoch 815 :: Batch 0/1
Batch Loss = 0.002221374524269228
815, epoch_train_loss=0.002221374524269228
Epoch 816
Epoch 816 :: Batch 0/1
Batch Loss = 0.002219901667555709
816, epoch_train_loss=0.002219901667555709
Epoch 817
Epoch 817 :: Batch 0/1
Batch Loss = 0.0022178479755417093
817, epoch_train_loss=0.0022178479755417093
Epoch 818
Epoch 818 :: Batch 0/1
Batch Loss = 0.0022177132324871815
818, epoch_train_loss=0.0022177132324871815
Epoch 819
Epoch 819 :: Batch 0/1
Batch Loss = 0.002217792249447182
819, epoch_train_loss=0.002217792249447182
Epoch 820
Epoch 820 :: Batch 0/1
Batch Loss = 0.002216328737654338
820, epoch_train_loss=0.002216328737654338
Epoch 821
Epoch 821 :: Batch 0/1
Batch Loss = 0.002214954603372844
821, epoch_train_loss=0.002214954603372844
Epoch 822
Epoch 822 :: Batch 0/1
Batch Loss = 0.0022149127350334337
822, epoch_train_loss=0.0022149127350334337
Epoch 823
Epoch 823 :: Batch 0/1
Batch Loss = 0.0022145856881810237
823, epoch_train_loss=0.0022145856881810237
Epoch 824
Epoch 824 :: Batch 0/1
Batch Loss = 0.0022132902502546344
824, epoch_train_loss=0.0022132902502546344
Epoch 825
Epoch 825 :: Batch 0/1
Batch Loss = 0.0022123799413189763
825, epoch_train_loss=0.0022123799413189763
Epoch 826
Epoch 826 :: Batch 0/1
Batch Loss = 0.0022122117084192156
826, epoch_train_loss=0.0022122117084192156
Epoch 827
Epoch 827 :: Batch 0/1
Batch Loss = 0.0022117248205863007
827, epoch_train_loss=0.0022117248205863007
Epoch 828
Epoch 828 :: Batch 0/1
Batch Loss = 0.002210666037646926
828, epoch_train_loss=0.002210666037646926
Epoch 829
Epoch 829 :: Batch 0/1
Batch Loss = 0.0022099540278343022
829, epoch_train_loss=0.0022099540278343022
Epoch 830
Epoch 830 :: Batch 0/1
Batch Loss = 0.002209708073009218
830, epoch_train_loss=0.002209708073009218
Epoch 831
Epoch 831 :: Batch 0/1
Batch Loss = 0.0022091604869324535
831, epoch_train_loss=0.0022091604869324535
Epoch 832
Epoch 832 :: Batch 0/1
Batch Loss = 0.0022082960714792208
832, epoch_train_loss=0.0022082960714792208
Epoch 833
Epoch 833 :: Batch 0/1
Batch Loss = 0.002207692370319118
833, epoch_train_loss=0.002207692370319118
Epoch 834
Epoch 834 :: Batch 0/1
Batch Loss = 0.0022073628580310117
834, epoch_train_loss=0.0022073628580310117
Epoch 835
Epoch 835 :: Batch 0/1
Batch Loss = 0.0022068399045232765
835, epoch_train_loss=0.0022068399045232765
Epoch 836
Epoch 836 :: Batch 0/1
Batch Loss = 0.002206102852485096
836, epoch_train_loss=0.002206102852485096
Epoch 837
Epoch 837 :: Batch 0/1
Batch Loss = 0.0022055516643243174
837, epoch_train_loss=0.0022055516643243174
Epoch 838
Epoch 838 :: Batch 0/1
Batch Loss = 0.0022051825345012384
838, epoch_train_loss=0.0022051825345012384
Epoch 839
Epoch 839 :: Batch 0/1
Batch Loss = 0.0022046830158354075
839, epoch_train_loss=0.0022046830158354075
Epoch 840
Epoch 840 :: Batch 0/1
Batch Loss = 0.0022040509174380218
840, epoch_train_loss=0.0022040509174380218
Epoch 841
Epoch 841 :: Batch 0/1
Batch Loss = 0.0022035250689793513
841, epoch_train_loss=0.0022035250689793513
Epoch 842
Epoch 842 :: Batch 0/1
Batch Loss = 0.002203131621544867
842, epoch_train_loss=0.002203131621544867
Epoch 843
Epoch 843 :: Batch 0/1
Batch Loss = 0.0022026682098628693
843, epoch_train_loss=0.0022026682098628693
Epoch 844
Epoch 844 :: Batch 0/1
Batch Loss = 0.0022021039546249346
844, epoch_train_loss=0.0022021039546249346
Epoch 845
Epoch 845 :: Batch 0/1
Batch Loss = 0.0022016036440026003
845, epoch_train_loss=0.0022016036440026003
Epoch 846
Epoch 846 :: Batch 0/1
Batch Loss = 0.002201195630564176
846, epoch_train_loss=0.002201195630564176
Epoch 847
Epoch 847 :: Batch 0/1
Batch Loss = 0.00220075896707766
847, epoch_train_loss=0.00220075896707766
Epoch 848
Epoch 848 :: Batch 0/1
Batch Loss = 0.0022002539958338426
848, epoch_train_loss=0.0022002539958338426
Epoch 849
Epoch 849 :: Batch 0/1
Batch Loss = 0.0021997709312438006
849, epoch_train_loss=0.0021997709312438006
Epoch 850
Epoch 850 :: Batch 0/1
Batch Loss = 0.0021993575869527714
850, epoch_train_loss=0.0021993575869527714
Epoch 851
Epoch 851 :: Batch 0/1
Batch Loss = 0.0021989420953097446
851, epoch_train_loss=0.0021989420953097446
Epoch 852
Epoch 852 :: Batch 0/1
Batch Loss = 0.0021984792351416745
852, epoch_train_loss=0.0021984792351416745
Epoch 853
Epoch 853 :: Batch 0/1
Batch Loss = 0.0021980183644613608
853, epoch_train_loss=0.0021980183644613608
Epoch 854
Epoch 854 :: Batch 0/1
Batch Loss = 0.0021976010869311185
854, epoch_train_loss=0.0021976010869311185
Epoch 855
Epoch 855 :: Batch 0/1
Batch Loss = 0.002197199384135725
855, epoch_train_loss=0.002197199384135725
Epoch 856
Epoch 856 :: Batch 0/1
Batch Loss = 0.0021967709457863765
856, epoch_train_loss=0.0021967709457863765
Epoch 857
Epoch 857 :: Batch 0/1
Batch Loss = 0.0021963302982292944
857, epoch_train_loss=0.0021963302982292944
Epoch 858
Epoch 858 :: Batch 0/1
Batch Loss = 0.002195915990484026
858, epoch_train_loss=0.002195915990484026
Epoch 859
Epoch 859 :: Batch 0/1
Batch Loss = 0.00219552182030694
859, epoch_train_loss=0.00219552182030694
Epoch 860
Epoch 860 :: Batch 0/1
Batch Loss = 0.0021951173539720824
860, epoch_train_loss=0.0021951173539720824
Epoch 861
Epoch 861 :: Batch 0/1
Batch Loss = 0.0021946995169689737
861, epoch_train_loss=0.0021946995169689737
Epoch 862
Epoch 862 :: Batch 0/1
Batch Loss = 0.0021942908722908153
862, epoch_train_loss=0.0021942908722908153
Epoch 863
Epoch 863 :: Batch 0/1
Batch Loss = 0.002193901120814846
863, epoch_train_loss=0.002193901120814846
Epoch 864
Epoch 864 :: Batch 0/1
Batch Loss = 0.0021935132517488976
864, epoch_train_loss=0.0021935132517488976
Epoch 865
Epoch 865 :: Batch 0/1
Batch Loss = 0.002193115197933901
865, epoch_train_loss=0.002193115197933901
Epoch 866
Epoch 866 :: Batch 0/1
Batch Loss = 0.002192717278960945
866, epoch_train_loss=0.002192717278960945
Epoch 867
Epoch 867 :: Batch 0/1
Batch Loss = 0.002192330987936837
867, epoch_train_loss=0.002192330987936837
Epoch 868
Epoch 868 :: Batch 0/1
Batch Loss = 0.002191952624725464
868, epoch_train_loss=0.002191952624725464
Epoch 869
Epoch 869 :: Batch 0/1
Batch Loss = 0.0021915713171487784
869, epoch_train_loss=0.0021915713171487784
Epoch 870
Epoch 870 :: Batch 0/1
Batch Loss = 0.0021911857715339925
870, epoch_train_loss=0.0021911857715339925
Epoch 871
Epoch 871 :: Batch 0/1
Batch Loss = 0.0021908049207242145
871, epoch_train_loss=0.0021908049207242145
Epoch 872
Epoch 872 :: Batch 0/1
Batch Loss = 0.002190432307811582
872, epoch_train_loss=0.002190432307811582
Epoch 873
Epoch 873 :: Batch 0/1
Batch Loss = 0.0021900620458255625
873, epoch_train_loss=0.0021900620458255625
Epoch 874
Epoch 874 :: Batch 0/1
Batch Loss = 0.002189689318778132
874, epoch_train_loss=0.002189689318778132
Epoch 875
Epoch 875 :: Batch 0/1
Batch Loss = 0.0021893164337140826
875, epoch_train_loss=0.0021893164337140826
Epoch 876
Epoch 876 :: Batch 0/1
Batch Loss = 0.002188948351314821
876, epoch_train_loss=0.002188948351314821
Epoch 877
Epoch 877 :: Batch 0/1
Batch Loss = 0.0021885851395307835
877, epoch_train_loss=0.0021885851395307835
Epoch 878
Epoch 878 :: Batch 0/1
Batch Loss = 0.002188222566401648
878, epoch_train_loss=0.002188222566401648
Epoch 879
Epoch 879 :: Batch 0/1
Batch Loss = 0.002187858987700222
879, epoch_train_loss=0.002187858987700222
Epoch 880
Epoch 880 :: Batch 0/1
Batch Loss = 0.002187496769071676
880, epoch_train_loss=0.002187496769071676
Epoch 881
Epoch 881 :: Batch 0/1
Batch Loss = 0.002187138179000932
881, epoch_train_loss=0.002187138179000932
Epoch 882
Epoch 882 :: Batch 0/1
Batch Loss = 0.002186782543426724
882, epoch_train_loss=0.002186782543426724
Epoch 883
Epoch 883 :: Batch 0/1
Batch Loss = 0.0021864274205839257
883, epoch_train_loss=0.0021864274205839257
Epoch 884
Epoch 884 :: Batch 0/1
Batch Loss = 0.002186072178581373
884, epoch_train_loss=0.002186072178581373
Epoch 885
Epoch 885 :: Batch 0/1
Batch Loss = 0.0021857185087264014
885, epoch_train_loss=0.0021857185087264014
Epoch 886
Epoch 886 :: Batch 0/1
Batch Loss = 0.002185367511434107
886, epoch_train_loss=0.002185367511434107
Epoch 887
Epoch 887 :: Batch 0/1
Batch Loss = 0.0021850184714327725
887, epoch_train_loss=0.0021850184714327725
Epoch 888
Epoch 888 :: Batch 0/1
Batch Loss = 0.00218467009451925
888, epoch_train_loss=0.00218467009451925
Epoch 889
Epoch 889 :: Batch 0/1
Batch Loss = 0.002184322094158611
889, epoch_train_loss=0.002184322094158611
Epoch 890
Epoch 890 :: Batch 0/1
Batch Loss = 0.0021839754101645086
890, epoch_train_loss=0.0021839754101645086
Epoch 891
Epoch 891 :: Batch 0/1
Batch Loss = 0.0021836306901198733
891, epoch_train_loss=0.0021836306901198733
Epoch 892
Epoch 892 :: Batch 0/1
Batch Loss = 0.0021832874836043683
892, epoch_train_loss=0.0021832874836043683
Epoch 893
Epoch 893 :: Batch 0/1
Batch Loss = 0.0021829450496103388
893, epoch_train_loss=0.0021829450496103388
Epoch 894
Epoch 894 :: Batch 0/1
Batch Loss = 0.0021826031835019345
894, epoch_train_loss=0.0021826031835019345
Epoch 895
Epoch 895 :: Batch 0/1
Batch Loss = 0.0021822623434574694
895, epoch_train_loss=0.0021822623434574694
Epoch 896
Epoch 896 :: Batch 0/1
Batch Loss = 0.002181922964941827
896, epoch_train_loss=0.002181922964941827
Epoch 897
Epoch 897 :: Batch 0/1
Batch Loss = 0.002181584843905694
897, epoch_train_loss=0.002181584843905694
Epoch 898
Epoch 898 :: Batch 0/1
Batch Loss = 0.0021812475238875513
898, epoch_train_loss=0.0021812475238875513
Epoch 899
Epoch 899 :: Batch 0/1
Batch Loss = 0.0021809108353785284
899, epoch_train_loss=0.0021809108353785284
Epoch 900
Epoch 900 :: Batch 0/1
Batch Loss = 0.002180574951592601
900, epoch_train_loss=0.002180574951592601
Epoch 901
Epoch 901 :: Batch 0/1
Batch Loss = 0.0021802401409731745
901, epoch_train_loss=0.0021802401409731745
Epoch 902
Epoch 902 :: Batch 0/1
Batch Loss = 0.002179906397459718
902, epoch_train_loss=0.002179906397459718
Epoch 903
Epoch 903 :: Batch 0/1
Batch Loss = 0.00217957345498947
903, epoch_train_loss=0.00217957345498947
Epoch 904
Epoch 904 :: Batch 0/1
Batch Loss = 0.002179241137986573
904, epoch_train_loss=0.002179241137986573
Epoch 905
Epoch 905 :: Batch 0/1
Batch Loss = 0.00217890948448714
905, epoch_train_loss=0.00217890948448714
Epoch 906
Epoch 906 :: Batch 0/1
Batch Loss = 0.0021785786321191487
906, epoch_train_loss=0.0021785786321191487
Epoch 907
Epoch 907 :: Batch 0/1
Batch Loss = 0.002178248643023207
907, epoch_train_loss=0.002178248643023207
Epoch 908
Epoch 908 :: Batch 0/1
Batch Loss = 0.002177919414047663
908, epoch_train_loss=0.002177919414047663
Epoch 909
Epoch 909 :: Batch 0/1
Batch Loss = 0.0021775907991848113
909, epoch_train_loss=0.0021775907991848113
Epoch 910
Epoch 910 :: Batch 0/1
Batch Loss = 0.002177262752716493
910, epoch_train_loss=0.002177262752716493
Epoch 911
Epoch 911 :: Batch 0/1
Batch Loss = 0.00217693532298177
911, epoch_train_loss=0.00217693532298177
Epoch 912
Epoch 912 :: Batch 0/1
Batch Loss = 0.0021766085689980917
912, epoch_train_loss=0.0021766085689980917
Epoch 913
Epoch 913 :: Batch 0/1
Batch Loss = 0.002176282481277477
913, epoch_train_loss=0.002176282481277477
Epoch 914
Epoch 914 :: Batch 0/1
Batch Loss = 0.002175956978599133
914, epoch_train_loss=0.002175956978599133
Epoch 915
Epoch 915 :: Batch 0/1
Batch Loss = 0.0021756319897232586
915, epoch_train_loss=0.0021756319897232586
Epoch 916
Epoch 916 :: Batch 0/1
Batch Loss = 0.002175307502077546
916, epoch_train_loss=0.002175307502077546
Epoch 917
Epoch 917 :: Batch 0/1
Batch Loss = 0.0021749835376451555
917, epoch_train_loss=0.0021749835376451555
Epoch 918
Epoch 918 :: Batch 0/1
Batch Loss = 0.0021746601155436395
918, epoch_train_loss=0.0021746601155436395
Epoch 919
Epoch 919 :: Batch 0/1
Batch Loss = 0.002174337217254037
919, epoch_train_loss=0.002174337217254037
Epoch 920
Epoch 920 :: Batch 0/1
Batch Loss = 0.0021740147914119825
920, epoch_train_loss=0.0021740147914119825
Epoch 921
Epoch 921 :: Batch 0/1
Batch Loss = 0.0021736927994190406
921, epoch_train_loss=0.0021736927994190406
Epoch 922
Epoch 922 :: Batch 0/1
Batch Loss = 0.0021733712299994235
922, epoch_train_loss=0.0021733712299994235
Epoch 923
Epoch 923 :: Batch 0/1
Batch Loss = 0.002173050087120863
923, epoch_train_loss=0.002173050087120863
Epoch 924
Epoch 924 :: Batch 0/1
Batch Loss = 0.0021727293750829955
924, epoch_train_loss=0.0021727293750829955
Epoch 925
Epoch 925 :: Batch 0/1
Batch Loss = 0.002172409078988412
925, epoch_train_loss=0.002172409078988412
Epoch 926
Epoch 926 :: Batch 0/1
Batch Loss = 0.0021720891684246747
926, epoch_train_loss=0.0021720891684246747
Epoch 927
Epoch 927 :: Batch 0/1
Batch Loss = 0.0021717696175063425
927, epoch_train_loss=0.0021717696175063425
Epoch 928
Epoch 928 :: Batch 0/1
Batch Loss = 0.0021714504118565603
928, epoch_train_loss=0.0021714504118565603
Epoch 929
Epoch 929 :: Batch 0/1
Batch Loss = 0.002171131546606149
929, epoch_train_loss=0.002171131546606149
Epoch 930
Epoch 930 :: Batch 0/1
Batch Loss = 0.0021708130195525767
930, epoch_train_loss=0.0021708130195525767
Epoch 931
Epoch 931 :: Batch 0/1
Batch Loss = 0.002170494820937778
931, epoch_train_loss=0.002170494820937778
Epoch 932
Epoch 932 :: Batch 0/1
Batch Loss = 0.0021701769328237954
932, epoch_train_loss=0.0021701769328237954
Epoch 933
Epoch 933 :: Batch 0/1
Batch Loss = 0.002169859336441534
933, epoch_train_loss=0.002169859336441534
Epoch 934
Epoch 934 :: Batch 0/1
Batch Loss = 0.002169542016721017
934, epoch_train_loss=0.002169542016721017
Epoch 935
Epoch 935 :: Batch 0/1
Batch Loss = 0.002169224964039793
935, epoch_train_loss=0.002169224964039793
Epoch 936
Epoch 936 :: Batch 0/1
Batch Loss = 0.0021689081726324923
936, epoch_train_loss=0.0021689081726324923
Epoch 937
Epoch 937 :: Batch 0/1
Batch Loss = 0.0021685916352481064
937, epoch_train_loss=0.0021685916352481064
Epoch 938
Epoch 938 :: Batch 0/1
Batch Loss = 0.0021682753410056372
938, epoch_train_loss=0.0021682753410056372
Epoch 939
Epoch 939 :: Batch 0/1
Batch Loss = 0.0021679592769787946
939, epoch_train_loss=0.0021679592769787946
Epoch 940
Epoch 940 :: Batch 0/1
Batch Loss = 0.0021676434298620187
940, epoch_train_loss=0.0021676434298620187
Epoch 941
Epoch 941 :: Batch 0/1
Batch Loss = 0.0021673277884567458
941, epoch_train_loss=0.0021673277884567458
Epoch 942
Epoch 942 :: Batch 0/1
Batch Loss = 0.00216701234449939
942, epoch_train_loss=0.00216701234449939
Epoch 943
Epoch 943 :: Batch 0/1
Batch Loss = 0.0021666970904717694
943, epoch_train_loss=0.0021666970904717694
Epoch 944
Epoch 944 :: Batch 0/1
Batch Loss = 0.002166382018643228
944, epoch_train_loss=0.002166382018643228
Epoch 945
Epoch 945 :: Batch 0/1
Batch Loss = 0.002166067120201131
945, epoch_train_loss=0.002166067120201131
Epoch 946
Epoch 946 :: Batch 0/1
Batch Loss = 0.002165752385085948
946, epoch_train_loss=0.002165752385085948
Epoch 947
Epoch 947 :: Batch 0/1
Batch Loss = 0.002165437803240875
947, epoch_train_loss=0.002165437803240875
Epoch 948
Epoch 948 :: Batch 0/1
Batch Loss = 0.0021651233655036873
948, epoch_train_loss=0.0021651233655036873
Epoch 949
Epoch 949 :: Batch 0/1
Batch Loss = 0.002164809063683171
949, epoch_train_loss=0.002164809063683171
Epoch 950
Epoch 950 :: Batch 0/1
Batch Loss = 0.0021644948903067345
950, epoch_train_loss=0.0021644948903067345
Epoch 951
Epoch 951 :: Batch 0/1
Batch Loss = 0.0021641808383297246
951, epoch_train_loss=0.0021641808383297246
Epoch 952
Epoch 952 :: Batch 0/1
Batch Loss = 0.0021638669003185514
952, epoch_train_loss=0.0021638669003185514
Epoch 953
Epoch 953 :: Batch 0/1
Batch Loss = 0.002163553068418038
953, epoch_train_loss=0.002163553068418038
Epoch 954
Epoch 954 :: Batch 0/1
Batch Loss = 0.0021632393347289797
954, epoch_train_loss=0.0021632393347289797
Epoch 955
Epoch 955 :: Batch 0/1
Batch Loss = 0.002162925691358588
955, epoch_train_loss=0.002162925691358588
Epoch 956
Epoch 956 :: Batch 0/1
Batch Loss = 0.0021626121308730454
956, epoch_train_loss=0.0021626121308730454
Epoch 957
Epoch 957 :: Batch 0/1
Batch Loss = 0.0021622986463932957
957, epoch_train_loss=0.0021622986463932957
Epoch 958
Epoch 958 :: Batch 0/1
Batch Loss = 0.002161985231268445
958, epoch_train_loss=0.002161985231268445
Epoch 959
Epoch 959 :: Batch 0/1
Batch Loss = 0.002161671879004414
959, epoch_train_loss=0.002161671879004414
Epoch 960
Epoch 960 :: Batch 0/1
Batch Loss = 0.002161358583151213
960, epoch_train_loss=0.002161358583151213
Epoch 961
Epoch 961 :: Batch 0/1
Batch Loss = 0.0021610453372578856
961, epoch_train_loss=0.0021610453372578856
Epoch 962
Epoch 962 :: Batch 0/1
Batch Loss = 0.0021607321347946623
962, epoch_train_loss=0.0021607321347946623
Epoch 963
Epoch 963 :: Batch 0/1
Batch Loss = 0.0021604189693783865
963, epoch_train_loss=0.0021604189693783865
Epoch 964
Epoch 964 :: Batch 0/1
Batch Loss = 0.0021601058347523014
964, epoch_train_loss=0.0021601058347523014
Epoch 965
Epoch 965 :: Batch 0/1
Batch Loss = 0.0021597927248361804
965, epoch_train_loss=0.0021597927248361804
Epoch 966
Epoch 966 :: Batch 0/1
Batch Loss = 0.002159479633774282
966, epoch_train_loss=0.002159479633774282
Epoch 967
Epoch 967 :: Batch 0/1
Batch Loss = 0.0021591665558792056
967, epoch_train_loss=0.0021591665558792056
Epoch 968
Epoch 968 :: Batch 0/1
Batch Loss = 0.0021588534855260925
968, epoch_train_loss=0.0021588534855260925
Epoch 969
Epoch 969 :: Batch 0/1
Batch Loss = 0.002158540417272882
969, epoch_train_loss=0.002158540417272882
Epoch 970
Epoch 970 :: Batch 0/1
Batch Loss = 0.002158227345642649
970, epoch_train_loss=0.002158227345642649
Epoch 971
Epoch 971 :: Batch 0/1
Batch Loss = 0.0021579142652833434
971, epoch_train_loss=0.0021579142652833434
Epoch 972
Epoch 972 :: Batch 0/1
Batch Loss = 0.002157601170896978
972, epoch_train_loss=0.002157601170896978
Epoch 973
Epoch 973 :: Batch 0/1
Batch Loss = 0.0021572880572765024
973, epoch_train_loss=0.0021572880572765024
Epoch 974
Epoch 974 :: Batch 0/1
Batch Loss = 0.0021569749192734797
974, epoch_train_loss=0.0021569749192734797
Epoch 975
Epoch 975 :: Batch 0/1
Batch Loss = 0.0021566617518977717
975, epoch_train_loss=0.0021566617518977717
Epoch 976
Epoch 976 :: Batch 0/1
Batch Loss = 0.002156348550267198
976, epoch_train_loss=0.002156348550267198
Epoch 977
Epoch 977 :: Batch 0/1
Batch Loss = 0.0021560353095547657
977, epoch_train_loss=0.0021560353095547657
Epoch 978
Epoch 978 :: Batch 0/1
Batch Loss = 0.0021557220250622566
978, epoch_train_loss=0.0021557220250622566
Epoch 979
Epoch 979 :: Batch 0/1
Batch Loss = 0.0021554086921544927
979, epoch_train_loss=0.0021554086921544927
Epoch 980
Epoch 980 :: Batch 0/1
Batch Loss = 0.002155095306234184
980, epoch_train_loss=0.002155095306234184
Epoch 981
Epoch 981 :: Batch 0/1
Batch Loss = 0.0021547818628276866
981, epoch_train_loss=0.0021547818628276866
Epoch 982
Epoch 982 :: Batch 0/1
Batch Loss = 0.002154468357523666
982, epoch_train_loss=0.002154468357523666
Epoch 983
Epoch 983 :: Batch 0/1
Batch Loss = 0.0021541547859442495
983, epoch_train_loss=0.0021541547859442495
Epoch 984
Epoch 984 :: Batch 0/1
Batch Loss = 0.0021538411438186463
984, epoch_train_loss=0.0021538411438186463
Epoch 985
Epoch 985 :: Batch 0/1
Batch Loss = 0.0021535274269061525
985, epoch_train_loss=0.0021535274269061525
Epoch 986
Epoch 986 :: Batch 0/1
Batch Loss = 0.0021532136310543136
986, epoch_train_loss=0.0021532136310543136
Epoch 987
Epoch 987 :: Batch 0/1
Batch Loss = 0.0021528997521954103
987, epoch_train_loss=0.0021528997521954103
Epoch 988
Epoch 988 :: Batch 0/1
Batch Loss = 0.0021525857862900156
988, epoch_train_loss=0.0021525857862900156
Epoch 989
Epoch 989 :: Batch 0/1
Batch Loss = 0.002152271729401345
989, epoch_train_loss=0.002152271729401345
Epoch 990
Epoch 990 :: Batch 0/1
Batch Loss = 0.0021519575776041576
990, epoch_train_loss=0.0021519575776041576
Epoch 991
Epoch 991 :: Batch 0/1
Batch Loss = 0.0021516433270608475
991, epoch_train_loss=0.0021516433270608475
Epoch 992
Epoch 992 :: Batch 0/1
Batch Loss = 0.002151328974013057
992, epoch_train_loss=0.002151328974013057
Epoch 993
Epoch 993 :: Batch 0/1
Batch Loss = 0.0021510145147000826
993, epoch_train_loss=0.0021510145147000826
Epoch 994
Epoch 994 :: Batch 0/1
Batch Loss = 0.002150699945473497
994, epoch_train_loss=0.002150699945473497
Epoch 995
Epoch 995 :: Batch 0/1
Batch Loss = 0.0021503852626565617
995, epoch_train_loss=0.0021503852626565617
Epoch 996
Epoch 996 :: Batch 0/1
Batch Loss = 0.002150070462724941
996, epoch_train_loss=0.002150070462724941
Epoch 997
Epoch 997 :: Batch 0/1
Batch Loss = 0.0021497555421240108
997, epoch_train_loss=0.0021497555421240108
Epoch 998
Epoch 998 :: Batch 0/1
Batch Loss = 0.0021494404973711246
998, epoch_train_loss=0.0021494404973711246
Epoch 999
Epoch 999 :: Batch 0/1
Batch Loss = 0.00214912532502328
999, epoch_train_loss=0.00214912532502328
Epoch 1000
Epoch 1000 :: Batch 0/1
Batch Loss = 0.002148810021693091
1000, epoch_train_loss=0.002148810021693091
Epoch 1001
Epoch 1001 :: Batch 0/1
Batch Loss = 0.002148494584053351
1001, epoch_train_loss=0.002148494584053351
Epoch 1002
Epoch 1002 :: Batch 0/1
Batch Loss = 0.0021481790087536576
1002, epoch_train_loss=0.0021481790087536576
Epoch 1003
Epoch 1003 :: Batch 0/1
Batch Loss = 0.002147863292572427
1003, epoch_train_loss=0.002147863292572427
Epoch 1004
Epoch 1004 :: Batch 0/1
Batch Loss = 0.0021475474322357095
1004, epoch_train_loss=0.0021475474322357095
Epoch 1005
Epoch 1005 :: Batch 0/1
Batch Loss = 0.0021472314245997337
1005, epoch_train_loss=0.0021472314245997337
Epoch 1006
Epoch 1006 :: Batch 0/1
Batch Loss = 0.002146915266472749
1006, epoch_train_loss=0.002146915266472749
Epoch 1007
Epoch 1007 :: Batch 0/1
Batch Loss = 0.002146598954783948
1007, epoch_train_loss=0.002146598954783948
Epoch 1008
Epoch 1008 :: Batch 0/1
Batch Loss = 0.0021462824864187913
1008, epoch_train_loss=0.0021462824864187913
Epoch 1009
Epoch 1009 :: Batch 0/1
Batch Loss = 0.0021459658583556167
1009, epoch_train_loss=0.0021459658583556167
Epoch 1010
Epoch 1010 :: Batch 0/1
Batch Loss = 0.0021456490676079855
1010, epoch_train_loss=0.0021456490676079855
Epoch 1011
Epoch 1011 :: Batch 0/1
Batch Loss = 0.0021453321112092946
1011, epoch_train_loss=0.0021453321112092946
Epoch 1012
Epoch 1012 :: Batch 0/1
Batch Loss = 0.002145014986270207
1012, epoch_train_loss=0.002145014986270207
Epoch 1013
Epoch 1013 :: Batch 0/1
Batch Loss = 0.0021446976899213268
1013, epoch_train_loss=0.0021446976899213268
Epoch 1014
Epoch 1014 :: Batch 0/1
Batch Loss = 0.002144380219415579
1014, epoch_train_loss=0.002144380219415579
Epoch 1015
Epoch 1015 :: Batch 0/1
Batch Loss = 0.002144062572025632
1015, epoch_train_loss=0.002144062572025632
Epoch 1016
Epoch 1016 :: Batch 0/1
Batch Loss = 0.0021437447452926084
1016, epoch_train_loss=0.0021437447452926084
Epoch 1017
Epoch 1017 :: Batch 0/1
Batch Loss = 0.002143426736849631
1017, epoch_train_loss=0.002143426736849631
Epoch 1018
Epoch 1018 :: Batch 0/1
Batch Loss = 0.0021431085448810456
1018, epoch_train_loss=0.0021431085448810456
Epoch 1019
Epoch 1019 :: Batch 0/1
Batch Loss = 0.002142790167932859
1019, epoch_train_loss=0.002142790167932859
Epoch 1020
Epoch 1020 :: Batch 0/1
Batch Loss = 0.0021424716058187038
1020, epoch_train_loss=0.0021424716058187038
Epoch 1021
Epoch 1021 :: Batch 0/1
Batch Loss = 0.0021421528598159897
1021, epoch_train_loss=0.0021421528598159897
Epoch 1022
Epoch 1022 :: Batch 0/1
Batch Loss = 0.0021418339344620625
1022, epoch_train_loss=0.0021418339344620625
Epoch 1023
Epoch 1023 :: Batch 0/1
Batch Loss = 0.0021415148391765153
1023, epoch_train_loss=0.0021415148391765153
Epoch 1024
Epoch 1024 :: Batch 0/1
Batch Loss = 0.0021411955930739214
1024, epoch_train_loss=0.0021411955930739214
Epoch 1025
Epoch 1025 :: Batch 0/1
Batch Loss = 0.0021408762311759624
1025, epoch_train_loss=0.0021408762311759624
Epoch 1026
Epoch 1026 :: Batch 0/1
Batch Loss = 0.002140556819059492
1026, epoch_train_loss=0.002140556819059492
Epoch 1027
Epoch 1027 :: Batch 0/1
Batch Loss = 0.002140237475451827
1027, epoch_train_loss=0.002140237475451827
Epoch 1028
Epoch 1028 :: Batch 0/1
Batch Loss = 0.0021399184198262313
1028, epoch_train_loss=0.0021399184198262313
Epoch 1029
Epoch 1029 :: Batch 0/1
Batch Loss = 0.0021396000544833885
1029, epoch_train_loss=0.0021396000544833885
Epoch 1030
Epoch 1030 :: Batch 0/1
Batch Loss = 0.002139283126870697
1030, epoch_train_loss=0.002139283126870697
Epoch 1031
Epoch 1031 :: Batch 0/1
Batch Loss = 0.0021389690332632594
1031, epoch_train_loss=0.0021389690332632594
Epoch 1032
Epoch 1032 :: Batch 0/1
Batch Loss = 0.002138660396680985
1032, epoch_train_loss=0.002138660396680985
Epoch 1033
Epoch 1033 :: Batch 0/1
Batch Loss = 0.0021383622186697646
1033, epoch_train_loss=0.0021383622186697646
Epoch 1034
Epoch 1034 :: Batch 0/1
Batch Loss = 0.002138084007398204
1034, epoch_train_loss=0.002138084007398204
Epoch 1035
Epoch 1035 :: Batch 0/1
Batch Loss = 0.002137844297754333
1035, epoch_train_loss=0.002137844297754333
Epoch 1036
Epoch 1036 :: Batch 0/1
Batch Loss = 0.002137678681555408
1036, epoch_train_loss=0.002137678681555408
Epoch 1037
Epoch 1037 :: Batch 0/1
Batch Loss = 0.002137658293366078
1037, epoch_train_loss=0.002137658293366078
Epoch 1038
Epoch 1038 :: Batch 0/1
Batch Loss = 0.0021379204794558493
1038, epoch_train_loss=0.0021379204794558493
Epoch 1039
Epoch 1039 :: Batch 0/1
Batch Loss = 0.002138748338280583
1039, epoch_train_loss=0.002138748338280583
Epoch 1040
Epoch 1040 :: Batch 0/1
Batch Loss = 0.002140686509783967
1040, epoch_train_loss=0.002140686509783967
Epoch 1041
Epoch 1041 :: Batch 0/1
Batch Loss = 0.002144906245857585
1041, epoch_train_loss=0.002144906245857585
Epoch 1042
Epoch 1042 :: Batch 0/1
Batch Loss = 0.002153613494084828
1042, epoch_train_loss=0.002153613494084828
Epoch 1043
Epoch 1043 :: Batch 0/1
Batch Loss = 0.0021718703042283545
1043, epoch_train_loss=0.0021718703042283545
Epoch 1044
Epoch 1044 :: Batch 0/1
Batch Loss = 0.002208685685350083
1044, epoch_train_loss=0.002208685685350083
Epoch 1045
Epoch 1045 :: Batch 0/1
Batch Loss = 0.002287103190848292
1045, epoch_train_loss=0.002287103190848292
Epoch 1046
Epoch 1046 :: Batch 0/1
Batch Loss = 0.002443046504490824
1046, epoch_train_loss=0.002443046504490824
Epoch 1047
Epoch 1047 :: Batch 0/1
Batch Loss = 0.0027881570608190966
1047, epoch_train_loss=0.0027881570608190966
Epoch 1048
Epoch 1048 :: Batch 0/1
Batch Loss = 0.0034483902480590377
1048, epoch_train_loss=0.0034483902480590377
Epoch 1049
Epoch 1049 :: Batch 0/1
Batch Loss = 0.00499905761753441
1049, epoch_train_loss=0.00499905761753441
Epoch 1050
Epoch 1050 :: Batch 0/1
Batch Loss = 0.007630953788559568
1050, epoch_train_loss=0.007630953788559568
Epoch 1051
Epoch 1051 :: Batch 0/1
Batch Loss = 0.014301743752646973
1051, epoch_train_loss=0.014301743752646973
Epoch 1052
Epoch 1052 :: Batch 0/1
Batch Loss = 0.021824573699819338
1052, epoch_train_loss=0.021824573699819338
Epoch 1053
Epoch 1053 :: Batch 0/1
Batch Loss = 0.04193355352491649
1053, epoch_train_loss=0.04193355352491649
Epoch 1054
Epoch 1054 :: Batch 0/1
Batch Loss = 0.04018673064331579
1054, epoch_train_loss=0.04018673064331579
Epoch 1055
Epoch 1055 :: Batch 0/1
Batch Loss = 0.04612677464738705
1055, epoch_train_loss=0.04612677464738705
Epoch 1056
Epoch 1056 :: Batch 0/1
Batch Loss = 0.015903084659844094
1056, epoch_train_loss=0.015903084659844094
Epoch 1057
Epoch 1057 :: Batch 0/1
Batch Loss = 0.002519858650271485
1057, epoch_train_loss=0.002519858650271485
Epoch 1058
Epoch 1058 :: Batch 0/1
Batch Loss = 0.009432643397206357
1058, epoch_train_loss=0.009432643397206357
Epoch 1059
Epoch 1059 :: Batch 0/1
Batch Loss = 0.019837994188092355
1059, epoch_train_loss=0.019837994188092355
Epoch 1060
Epoch 1060 :: Batch 0/1
Batch Loss = 0.023743820224113112
1060, epoch_train_loss=0.023743820224113112
Epoch 1061
Epoch 1061 :: Batch 0/1
Batch Loss = 0.006789226697133328
1061, epoch_train_loss=0.006789226697133328
Epoch 1062
Epoch 1062 :: Batch 0/1
Batch Loss = 0.004003859217174397
1062, epoch_train_loss=0.004003859217174397
Epoch 1063
Epoch 1063 :: Batch 0/1
Batch Loss = 0.013991333318745875
1063, epoch_train_loss=0.013991333318745875
Epoch 1064
Epoch 1064 :: Batch 0/1
Batch Loss = 0.01121536250222564
1064, epoch_train_loss=0.01121536250222564
Epoch 1065
Epoch 1065 :: Batch 0/1
Batch Loss = 0.0038480565946136095
1065, epoch_train_loss=0.0038480565946136095
Epoch 1066
Epoch 1066 :: Batch 0/1
Batch Loss = 0.003738829918218031
1066, epoch_train_loss=0.003738829918218031
Epoch 1067
Epoch 1067 :: Batch 0/1
Batch Loss = 0.008584062135466957
1067, epoch_train_loss=0.008584062135466957
Epoch 1068
Epoch 1068 :: Batch 0/1
Batch Loss = 0.007854637592531518
1068, epoch_train_loss=0.007854637592531518
Epoch 1069
Epoch 1069 :: Batch 0/1
Batch Loss = 0.0026064755916773096
1069, epoch_train_loss=0.0026064755916773096
Epoch 1070
Epoch 1070 :: Batch 0/1
Batch Loss = 0.005052196204812585
1070, epoch_train_loss=0.005052196204812585
Epoch 1071
Epoch 1071 :: Batch 0/1
Batch Loss = 0.008243637732059245
1071, epoch_train_loss=0.008243637732059245
Epoch 1072
Epoch 1072 :: Batch 0/1
Batch Loss = 0.003826561773859244
1072, epoch_train_loss=0.003826561773859244
Epoch 1073
Epoch 1073 :: Batch 0/1
Batch Loss = 0.0029065906893263518
1073, epoch_train_loss=0.0029065906893263518
Epoch 1074
Epoch 1074 :: Batch 0/1
Batch Loss = 0.006049509974310252
1074, epoch_train_loss=0.006049509974310252
Epoch 1075
Epoch 1075 :: Batch 0/1
Batch Loss = 0.00444466957565251
1075, epoch_train_loss=0.00444466957565251
Epoch 1076
Epoch 1076 :: Batch 0/1
Batch Loss = 0.002410874764602912
1076, epoch_train_loss=0.002410874764602912
Epoch 1077
Epoch 1077 :: Batch 0/1
Batch Loss = 0.0040094948903827655
1077, epoch_train_loss=0.0040094948903827655
Epoch 1078
Epoch 1078 :: Batch 0/1
Batch Loss = 0.004307698806997755
1078, epoch_train_loss=0.004307698806997755
Epoch 1079
Epoch 1079 :: Batch 0/1
Batch Loss = 0.002653915890795209
1079, epoch_train_loss=0.002653915890795209
Epoch 1080
Epoch 1080 :: Batch 0/1
Batch Loss = 0.002806673548628702
1080, epoch_train_loss=0.002806673548628702
Epoch 1081
Epoch 1081 :: Batch 0/1
Batch Loss = 0.0038425464266253887
1081, epoch_train_loss=0.0038425464266253887
Epoch 1082
Epoch 1082 :: Batch 0/1
Batch Loss = 0.003110071349169938
1082, epoch_train_loss=0.003110071349169938
Epoch 1083
Epoch 1083 :: Batch 0/1
Batch Loss = 0.0023602834492566097
1083, epoch_train_loss=0.0023602834492566097
Epoch 1084
Epoch 1084 :: Batch 0/1
Batch Loss = 0.0032019935235898652
1084, epoch_train_loss=0.0032019935235898652
Epoch 1085
Epoch 1085 :: Batch 0/1
Batch Loss = 0.003306924335201781
1085, epoch_train_loss=0.003306924335201781
Epoch 1086
Epoch 1086 :: Batch 0/1
Batch Loss = 0.0023839559834792513
1086, epoch_train_loss=0.0023839559834792513
Epoch 1087
Epoch 1087 :: Batch 0/1
Batch Loss = 0.00265780046357178
1087, epoch_train_loss=0.00265780046357178
Epoch 1088
Epoch 1088 :: Batch 0/1
Batch Loss = 0.003177521655010163
1088, epoch_train_loss=0.003177521655010163
Epoch 1089
Epoch 1089 :: Batch 0/1
Batch Loss = 0.0025663425166873983
1089, epoch_train_loss=0.0025663425166873983
Epoch 1090
Epoch 1090 :: Batch 0/1
Batch Loss = 0.002325460296173729
1090, epoch_train_loss=0.002325460296173729
Epoch 1091
Epoch 1091 :: Batch 0/1
Batch Loss = 0.002803100268808618
1091, epoch_train_loss=0.002803100268808618
Epoch 1092
Epoch 1092 :: Batch 0/1
Batch Loss = 0.0026834082746754894
1092, epoch_train_loss=0.0026834082746754894
Epoch 1093
Epoch 1093 :: Batch 0/1
Batch Loss = 0.0022700215784567926
1093, epoch_train_loss=0.0022700215784567926
Epoch 1094
Epoch 1094 :: Batch 0/1
Batch Loss = 0.002429925392781301
1094, epoch_train_loss=0.002429925392781301
Epoch 1095
Epoch 1095 :: Batch 0/1
Batch Loss = 0.0026347849392882013
1095, epoch_train_loss=0.0026347849392882013
Epoch 1096
Epoch 1096 :: Batch 0/1
Batch Loss = 0.002384232083829501
1096, epoch_train_loss=0.002384232083829501
Epoch 1097
Epoch 1097 :: Batch 0/1
Batch Loss = 0.002238352136850619
1097, epoch_train_loss=0.002238352136850619
Epoch 1098
Epoch 1098 :: Batch 0/1
Batch Loss = 0.0024463732022902834
1098, epoch_train_loss=0.0024463732022902834
Epoch 1099
Epoch 1099 :: Batch 0/1
Batch Loss = 0.002463495469464106
1099, epoch_train_loss=0.002463495469464106
Epoch 1100
Epoch 1100 :: Batch 0/1
Batch Loss = 0.0022426172591664397
1100, epoch_train_loss=0.0022426172591664397
Epoch 1101
Epoch 1101 :: Batch 0/1
Batch Loss = 0.0022609272869217696
1101, epoch_train_loss=0.0022609272869217696
Epoch 1102
Epoch 1102 :: Batch 0/1
Batch Loss = 0.0024024583631923227
1102, epoch_train_loss=0.0024024583631923227
Epoch 1103
Epoch 1103 :: Batch 0/1
Batch Loss = 0.002316235443203872
1103, epoch_train_loss=0.002316235443203872
Epoch 1104
Epoch 1104 :: Batch 0/1
Batch Loss = 0.0021948545430304856
1104, epoch_train_loss=0.0021948545430304856
Epoch 1105
Epoch 1105 :: Batch 0/1
Batch Loss = 0.002260435571913651
1105, epoch_train_loss=0.002260435571913651
Epoch 1106
Epoch 1106 :: Batch 0/1
Batch Loss = 0.002320400586746288
1106, epoch_train_loss=0.002320400586746288
Epoch 1107
Epoch 1107 :: Batch 0/1
Batch Loss = 0.0022368635556778253
1107, epoch_train_loss=0.0022368635556778253
Epoch 1108
Epoch 1108 :: Batch 0/1
Batch Loss = 0.0021806867263581367
1108, epoch_train_loss=0.0021806867263581367
Epoch 1109
Epoch 1109 :: Batch 0/1
Batch Loss = 0.0022410739357999047
1109, epoch_train_loss=0.0022410739357999047
Epoch 1110
Epoch 1110 :: Batch 0/1
Batch Loss = 0.002264508719514469
1110, epoch_train_loss=0.002264508719514469
Epoch 1111
Epoch 1111 :: Batch 0/1
Batch Loss = 0.002197010299542486
1111, epoch_train_loss=0.002197010299542486
Epoch 1112
Epoch 1112 :: Batch 0/1
Batch Loss = 0.0021727285959480917
1112, epoch_train_loss=0.0021727285959480917
Epoch 1113
Epoch 1113 :: Batch 0/1
Batch Loss = 0.0022164100347625616
1113, epoch_train_loss=0.0022164100347625616
Epoch 1114
Epoch 1114 :: Batch 0/1
Batch Loss = 0.0022204216728243104
1114, epoch_train_loss=0.0022204216728243104
Epoch 1115
Epoch 1115 :: Batch 0/1
Batch Loss = 0.002175471756932758
1115, epoch_train_loss=0.002175471756932758
Epoch 1116
Epoch 1116 :: Batch 0/1
Batch Loss = 0.002163934057044396
1116, epoch_train_loss=0.002163934057044396
Epoch 1117
Epoch 1117 :: Batch 0/1
Batch Loss = 0.002192267636738587
1117, epoch_train_loss=0.002192267636738587
Epoch 1118
Epoch 1118 :: Batch 0/1
Batch Loss = 0.0021936693268252536
1118, epoch_train_loss=0.0021936693268252536
Epoch 1119
Epoch 1119 :: Batch 0/1
Batch Loss = 0.00216278595372064
1119, epoch_train_loss=0.00216278595372064
Epoch 1120
Epoch 1120 :: Batch 0/1
Batch Loss = 0.0021549961035457563
1120, epoch_train_loss=0.0021549961035457563
Epoch 1121
Epoch 1121 :: Batch 0/1
Batch Loss = 0.002173514435902278
1121, epoch_train_loss=0.002173514435902278
Epoch 1122
Epoch 1122 :: Batch 0/1
Batch Loss = 0.0021744036326535577
1122, epoch_train_loss=0.0021744036326535577
Epoch 1123
Epoch 1123 :: Batch 0/1
Batch Loss = 0.0021542330237184866
1123, epoch_train_loss=0.0021542330237184866
Epoch 1124
Epoch 1124 :: Batch 0/1
Batch Loss = 0.0021470482207353557
1124, epoch_train_loss=0.0021470482207353557
Epoch 1125
Epoch 1125 :: Batch 0/1
Batch Loss = 0.0021584855954803993
1125, epoch_train_loss=0.0021584855954803993
Epoch 1126
Epoch 1126 :: Batch 0/1
Batch Loss = 0.0021611709772293816
1126, epoch_train_loss=0.0021611709772293816
Epoch 1127
Epoch 1127 :: Batch 0/1
Batch Loss = 0.002147840644239683
1127, epoch_train_loss=0.002147840644239683
Epoch 1128
Epoch 1128 :: Batch 0/1
Batch Loss = 0.0021407485702899225
1128, epoch_train_loss=0.0021407485702899225
Epoch 1129
Epoch 1129 :: Batch 0/1
Batch Loss = 0.0021473264456517197
1129, epoch_train_loss=0.0021473264456517197
Epoch 1130
Epoch 1130 :: Batch 0/1
Batch Loss = 0.0021505018618377327
1130, epoch_train_loss=0.0021505018618377327
Epoch 1131
Epoch 1131 :: Batch 0/1
Batch Loss = 0.0021426201733903676
1131, epoch_train_loss=0.0021426201733903676
Epoch 1132
Epoch 1132 :: Batch 0/1
Batch Loss = 0.002135973928504098
1132, epoch_train_loss=0.002135973928504098
Epoch 1133
Epoch 1133 :: Batch 0/1
Batch Loss = 0.002138610003528821
1133, epoch_train_loss=0.002138610003528821
Epoch 1134
Epoch 1134 :: Batch 0/1
Batch Loss = 0.002141878318945687
1134, epoch_train_loss=0.002141878318945687
Epoch 1135
Epoch 1135 :: Batch 0/1
Batch Loss = 0.002137936866657986
1135, epoch_train_loss=0.002137936866657986
Epoch 1136
Epoch 1136 :: Batch 0/1
Batch Loss = 0.002132296997557887
1136, epoch_train_loss=0.002132296997557887
Epoch 1137
Epoch 1137 :: Batch 0/1
Batch Loss = 0.0021321079205993956
1137, epoch_train_loss=0.0021321079205993956
Epoch 1138
Epoch 1138 :: Batch 0/1
Batch Loss = 0.0021346152045517695
1138, epoch_train_loss=0.0021346152045517695
Epoch 1139
Epoch 1139 :: Batch 0/1
Batch Loss = 0.0021334506197240844
1139, epoch_train_loss=0.0021334506197240844
Epoch 1140
Epoch 1140 :: Batch 0/1
Batch Loss = 0.002129251039652598
1140, epoch_train_loss=0.002129251039652598
Epoch 1141
Epoch 1141 :: Batch 0/1
Batch Loss = 0.0021273960444550153
1141, epoch_train_loss=0.0021273960444550153
Epoch 1142
Epoch 1142 :: Batch 0/1
Batch Loss = 0.0021286571597657214
1142, epoch_train_loss=0.0021286571597657214
Epoch 1143
Epoch 1143 :: Batch 0/1
Batch Loss = 0.0021288770734220475
1143, epoch_train_loss=0.0021288770734220475
Epoch 1144
Epoch 1144 :: Batch 0/1
Batch Loss = 0.002126369825228328
1144, epoch_train_loss=0.002126369825228328
Epoch 1145
Epoch 1145 :: Batch 0/1
Batch Loss = 0.0021239636061379666
1145, epoch_train_loss=0.0021239636061379666
Epoch 1146
Epoch 1146 :: Batch 0/1
Batch Loss = 0.0021238474540472138
1146, epoch_train_loss=0.0021238474540472138
Epoch 1147
Epoch 1147 :: Batch 0/1
Batch Loss = 0.002124365267959816
1147, epoch_train_loss=0.002124365267959816
Epoch 1148
Epoch 1148 :: Batch 0/1
Batch Loss = 0.0021232840717360845
1148, epoch_train_loss=0.0021232840717360845
Epoch 1149
Epoch 1149 :: Batch 0/1
Batch Loss = 0.002121235805829297
1149, epoch_train_loss=0.002121235805829297
Epoch 1150
Epoch 1150 :: Batch 0/1
Batch Loss = 0.002120141927127775
1150, epoch_train_loss=0.002120141927127775
Epoch 1151
Epoch 1151 :: Batch 0/1
Batch Loss = 0.002120201284062706
1151, epoch_train_loss=0.002120201284062706
Epoch 1152
Epoch 1152 :: Batch 0/1
Batch Loss = 0.0021199537376216456
1152, epoch_train_loss=0.0021199537376216456
Epoch 1153
Epoch 1153 :: Batch 0/1
Batch Loss = 0.0021186725726127517
1153, epoch_train_loss=0.0021186725726127517
Epoch 1154
Epoch 1154 :: Batch 0/1
Batch Loss = 0.0021172664199334166
1154, epoch_train_loss=0.0021172664199334166
Epoch 1155
Epoch 1155 :: Batch 0/1
Batch Loss = 0.0021166796581629237
1155, epoch_train_loss=0.0021166796581629237
Epoch 1156
Epoch 1156 :: Batch 0/1
Batch Loss = 0.002116537850991462
1156, epoch_train_loss=0.002116537850991462
Epoch 1157
Epoch 1157 :: Batch 0/1
Batch Loss = 0.002115927292697054
1157, epoch_train_loss=0.002115927292697054
Epoch 1158
Epoch 1158 :: Batch 0/1
Batch Loss = 0.0021147875004369305
1158, epoch_train_loss=0.0021147875004369305
Epoch 1159
Epoch 1159 :: Batch 0/1
Batch Loss = 0.0021138180638020526
1159, epoch_train_loss=0.0021138180638020526
Epoch 1160
Epoch 1160 :: Batch 0/1
Batch Loss = 0.0021133508947920082
1160, epoch_train_loss=0.0021133508947920082
Epoch 1161
Epoch 1161 :: Batch 0/1
Batch Loss = 0.002112992119326081
1161, epoch_train_loss=0.002112992119326081
Epoch 1162
Epoch 1162 :: Batch 0/1
Batch Loss = 0.0021123008455036533
1162, epoch_train_loss=0.0021123008455036533
Epoch 1163
Epoch 1163 :: Batch 0/1
Batch Loss = 0.00211136567363979
1163, epoch_train_loss=0.00211136567363979
Epoch 1164
Epoch 1164 :: Batch 0/1
Batch Loss = 0.0021105952812717063
1164, epoch_train_loss=0.0021105952812717063
Epoch 1165
Epoch 1165 :: Batch 0/1
Batch Loss = 0.0021101181241878135
1165, epoch_train_loss=0.0021101181241878135
Epoch 1166
Epoch 1166 :: Batch 0/1
Batch Loss = 0.0021096603848228973
1166, epoch_train_loss=0.0021096603848228973
Epoch 1167
Epoch 1167 :: Batch 0/1
Batch Loss = 0.002108984596235557
1167, epoch_train_loss=0.002108984596235557
Epoch 1168
Epoch 1168 :: Batch 0/1
Batch Loss = 0.002108192590231608
1168, epoch_train_loss=0.002108192590231608
Epoch 1169
Epoch 1169 :: Batch 0/1
Batch Loss = 0.002107521031739103
1169, epoch_train_loss=0.002107521031739103
Epoch 1170
Epoch 1170 :: Batch 0/1
Batch Loss = 0.0021070113600874804
1170, epoch_train_loss=0.0021070113600874804
Epoch 1171
Epoch 1171 :: Batch 0/1
Batch Loss = 0.0021065052975143307
1171, epoch_train_loss=0.0021065052975143307
Epoch 1172
Epoch 1172 :: Batch 0/1
Batch Loss = 0.0021058775637509907
1172, epoch_train_loss=0.0021058775637509907
Epoch 1173
Epoch 1173 :: Batch 0/1
Batch Loss = 0.0021051799996717807
1173, epoch_train_loss=0.0021051799996717807
Epoch 1174
Epoch 1174 :: Batch 0/1
Batch Loss = 0.0021045476456798165
1174, epoch_train_loss=0.0021045476456798165
Epoch 1175
Epoch 1175 :: Batch 0/1
Batch Loss = 0.0021040165627107263
1175, epoch_train_loss=0.0021040165627107263
Epoch 1176
Epoch 1176 :: Batch 0/1
Batch Loss = 0.0021034963100455923
1176, epoch_train_loss=0.0021034963100455923
Epoch 1177
Epoch 1177 :: Batch 0/1
Batch Loss = 0.002102907068419959
1177, epoch_train_loss=0.002102907068419959
Epoch 1178
Epoch 1178 :: Batch 0/1
Batch Loss = 0.00210227225218402
1178, epoch_train_loss=0.00210227225218402
Epoch 1179
Epoch 1179 :: Batch 0/1
Batch Loss = 0.0021016668762119345
1179, epoch_train_loss=0.0021016668762119345
Epoch 1180
Epoch 1180 :: Batch 0/1
Batch Loss = 0.0021011208083264715
1180, epoch_train_loss=0.0021011208083264715
Epoch 1181
Epoch 1181 :: Batch 0/1
Batch Loss = 0.002100594933999234
1181, epoch_train_loss=0.002100594933999234
Epoch 1182
Epoch 1182 :: Batch 0/1
Batch Loss = 0.002100037835079683
1182, epoch_train_loss=0.002100037835079683
Epoch 1183
Epoch 1183 :: Batch 0/1
Batch Loss = 0.0020994462144858727
1183, epoch_train_loss=0.0020994462144858727
Epoch 1184
Epoch 1184 :: Batch 0/1
Batch Loss = 0.002098860858642937
1184, epoch_train_loss=0.002098860858642937
Epoch 1185
Epoch 1185 :: Batch 0/1
Batch Loss = 0.002098310172003301
1185, epoch_train_loss=0.002098310172003301
Epoch 1186
Epoch 1186 :: Batch 0/1
Batch Loss = 0.002097780995525628
1186, epoch_train_loss=0.002097780995525628
Epoch 1187
Epoch 1187 :: Batch 0/1
Batch Loss = 0.0020972429152271094
1187, epoch_train_loss=0.0020972429152271094
Epoch 1188
Epoch 1188 :: Batch 0/1
Batch Loss = 0.0020966839797069976
1188, epoch_train_loss=0.0020966839797069976
Epoch 1189
Epoch 1189 :: Batch 0/1
Batch Loss = 0.0020961189207128286
1189, epoch_train_loss=0.0020961189207128286
Epoch 1190
Epoch 1190 :: Batch 0/1
Batch Loss = 0.002095568832825534
1190, epoch_train_loss=0.002095568832825534
Epoch 1191
Epoch 1191 :: Batch 0/1
Batch Loss = 0.0020950376105896788
1191, epoch_train_loss=0.0020950376105896788
Epoch 1192
Epoch 1192 :: Batch 0/1
Batch Loss = 0.0020945104122196056
1192, epoch_train_loss=0.0020945104122196056
Epoch 1193
Epoch 1193 :: Batch 0/1
Batch Loss = 0.00209397350870404
1193, epoch_train_loss=0.00209397350870404
Epoch 1194
Epoch 1194 :: Batch 0/1
Batch Loss = 0.002093428230365468
1194, epoch_train_loss=0.002093428230365468
Epoch 1195
Epoch 1195 :: Batch 0/1
Batch Loss = 0.002092885722795149
1195, epoch_train_loss=0.002092885722795149
Epoch 1196
Epoch 1196 :: Batch 0/1
Batch Loss = 0.0020923540074660927
1196, epoch_train_loss=0.0020923540074660927
Epoch 1197
Epoch 1197 :: Batch 0/1
Batch Loss = 0.0020918309701652643
1197, epoch_train_loss=0.0020918309701652643
Epoch 1198
Epoch 1198 :: Batch 0/1
Batch Loss = 0.0020913078977863194
1198, epoch_train_loss=0.0020913078977863194
Epoch 1199
Epoch 1199 :: Batch 0/1
Batch Loss = 0.0020907793957954802
1199, epoch_train_loss=0.0020907793957954802
Epoch 1200
Epoch 1200 :: Batch 0/1
Batch Loss = 0.0020902482278432268
1200, epoch_train_loss=0.0020902482278432268
Epoch 1201
Epoch 1201 :: Batch 0/1
Batch Loss = 0.002089720627849452
1201, epoch_train_loss=0.002089720627849452
Epoch 1202
Epoch 1202 :: Batch 0/1
Batch Loss = 0.00208919953357855
1202, epoch_train_loss=0.00208919953357855
Epoch 1203
Epoch 1203 :: Batch 0/1
Batch Loss = 0.0020886828073266687
1203, epoch_train_loss=0.0020886828073266687
Epoch 1204
Epoch 1204 :: Batch 0/1
Batch Loss = 0.0020881660813493533
1204, epoch_train_loss=0.0020881660813493533
Epoch 1205
Epoch 1205 :: Batch 0/1
Batch Loss = 0.002087647006774197
1205, epoch_train_loss=0.002087647006774197
Epoch 1206
Epoch 1206 :: Batch 0/1
Batch Loss = 0.00208712722741715
1206, epoch_train_loss=0.00208712722741715
Epoch 1207
Epoch 1207 :: Batch 0/1
Batch Loss = 0.0020866099278116844
1207, epoch_train_loss=0.0020866099278116844
Epoch 1208
Epoch 1208 :: Batch 0/1
Batch Loss = 0.002086096507964308
1208, epoch_train_loss=0.002086096507964308
Epoch 1209
Epoch 1209 :: Batch 0/1
Batch Loss = 0.0020855858642767305
1209, epoch_train_loss=0.0020855858642767305
Epoch 1210
Epoch 1210 :: Batch 0/1
Batch Loss = 0.002085075874841368
1210, epoch_train_loss=0.002085075874841368
Epoch 1211
Epoch 1211 :: Batch 0/1
Batch Loss = 0.002084565239291583
1211, epoch_train_loss=0.002084565239291583
Epoch 1212
Epoch 1212 :: Batch 0/1
Batch Loss = 0.002084054472075927
1212, epoch_train_loss=0.002084054472075927
Epoch 1213
Epoch 1213 :: Batch 0/1
Batch Loss = 0.002083545136464352
1213, epoch_train_loss=0.002083545136464352
Epoch 1214
Epoch 1214 :: Batch 0/1
Batch Loss = 0.0020830381765814467
1214, epoch_train_loss=0.0020830381765814467
Epoch 1215
Epoch 1215 :: Batch 0/1
Batch Loss = 0.002082533309671363
1215, epoch_train_loss=0.002082533309671363
Epoch 1216
Epoch 1216 :: Batch 0/1
Batch Loss = 0.002082029549197373
1216, epoch_train_loss=0.002082029549197373
Epoch 1217
Epoch 1217 :: Batch 0/1
Batch Loss = 0.0020815260387503478
1217, epoch_train_loss=0.0020815260387503478
Epoch 1218
Epoch 1218 :: Batch 0/1
Batch Loss = 0.0020810226883484183
1218, epoch_train_loss=0.0020810226883484183
Epoch 1219
Epoch 1219 :: Batch 0/1
Batch Loss = 0.002080520132149408
1219, epoch_train_loss=0.002080520132149408
Epoch 1220
Epoch 1220 :: Batch 0/1
Batch Loss = 0.002080019011769027
1220, epoch_train_loss=0.002080019011769027
Epoch 1221
Epoch 1221 :: Batch 0/1
Batch Loss = 0.0020795194639064785
1221, epoch_train_loss=0.0020795194639064785
Epoch 1222
Epoch 1222 :: Batch 0/1
Batch Loss = 0.0020790211610781217
1222, epoch_train_loss=0.0020790211610781217
Epoch 1223
Epoch 1223 :: Batch 0/1
Batch Loss = 0.002078523612316329
1223, epoch_train_loss=0.002078523612316329
Epoch 1224
Epoch 1224 :: Batch 0/1
Batch Loss = 0.002078026524777653
1224, epoch_train_loss=0.002078026524777653
Epoch 1225
Epoch 1225 :: Batch 0/1
Batch Loss = 0.002077529993048491
1225, epoch_train_loss=0.002077529993048491
Epoch 1226
Epoch 1226 :: Batch 0/1
Batch Loss = 0.0020770343216728036
1226, epoch_train_loss=0.0020770343216728036
Epoch 1227
Epoch 1227 :: Batch 0/1
Batch Loss = 0.002076539741482117
1227, epoch_train_loss=0.002076539741482117
Epoch 1228
Epoch 1228 :: Batch 0/1
Batch Loss = 0.002076046282151391
1228, epoch_train_loss=0.002076046282151391
Epoch 1229
Epoch 1229 :: Batch 0/1
Batch Loss = 0.002075553775549256
1229, epoch_train_loss=0.002075553775549256
Epoch 1230
Epoch 1230 :: Batch 0/1
Batch Loss = 0.0020750619891748705
1230, epoch_train_loss=0.0020750619891748705
Epoch 1231
Epoch 1231 :: Batch 0/1
Batch Loss = 0.002074570799506504
1231, epoch_train_loss=0.002074570799506504
Epoch 1232
Epoch 1232 :: Batch 0/1
Batch Loss = 0.0020740802396942717
1232, epoch_train_loss=0.0020740802396942717
Epoch 1233
Epoch 1233 :: Batch 0/1
Batch Loss = 0.0020735904229800517
1233, epoch_train_loss=0.0020735904229800517
Epoch 1234
Epoch 1234 :: Batch 0/1
Batch Loss = 0.002073101458399133
1234, epoch_train_loss=0.002073101458399133
Epoch 1235
Epoch 1235 :: Batch 0/1
Batch Loss = 0.0020726133765277734
1235, epoch_train_loss=0.0020726133765277734
Epoch 1236
Epoch 1236 :: Batch 0/1
Batch Loss = 0.0020721261124929235
1236, epoch_train_loss=0.0020721261124929235
Epoch 1237
Epoch 1237 :: Batch 0/1
Batch Loss = 0.002071639567894488
1237, epoch_train_loss=0.002071639567894488
Epoch 1238
Epoch 1238 :: Batch 0/1
Batch Loss = 0.002071153671401844
1238, epoch_train_loss=0.002071153671401844
Epoch 1239
Epoch 1239 :: Batch 0/1
Batch Loss = 0.0020706683993833882
1239, epoch_train_loss=0.0020706683993833882
Epoch 1240
Epoch 1240 :: Batch 0/1
Batch Loss = 0.002070183779765209
1240, epoch_train_loss=0.002070183779765209
Epoch 1241
Epoch 1241 :: Batch 0/1
Batch Loss = 0.0020696998635831958
1241, epoch_train_loss=0.0020696998635831958
Epoch 1242
Epoch 1242 :: Batch 0/1
Batch Loss = 0.0020692166789858827
1242, epoch_train_loss=0.0020692166789858827
Epoch 1243
Epoch 1243 :: Batch 0/1
Batch Loss = 0.002068734220157997
1243, epoch_train_loss=0.002068734220157997
Epoch 1244
Epoch 1244 :: Batch 0/1
Batch Loss = 0.002068252457725784
1244, epoch_train_loss=0.002068252457725784
Epoch 1245
Epoch 1245 :: Batch 0/1
Batch Loss = 0.0020677713500106675
1245, epoch_train_loss=0.0020677713500106675
Epoch 1246
Epoch 1246 :: Batch 0/1
Batch Loss = 0.0020672908645166257
1246, epoch_train_loss=0.0020672908645166257
Epoch 1247
Epoch 1247 :: Batch 0/1
Batch Loss = 0.0020668109932058558
1247, epoch_train_loss=0.0020668109932058558
Epoch 1248
Epoch 1248 :: Batch 0/1
Batch Loss = 0.00206633174239867
1248, epoch_train_loss=0.00206633174239867
Epoch 1249
Epoch 1249 :: Batch 0/1
Batch Loss = 0.002065853124942077
1249, epoch_train_loss=0.002065853124942077
Epoch 1250
Epoch 1250 :: Batch 0/1
Batch Loss = 0.002065375152672351
1250, epoch_train_loss=0.002065375152672351
Epoch 1251
Epoch 1251 :: Batch 0/1
Batch Loss = 0.0020648978259577796
1251, epoch_train_loss=0.0020648978259577796
Epoch 1252
Epoch 1252 :: Batch 0/1
Batch Loss = 0.002064421133676572
1252, epoch_train_loss=0.002064421133676572
Epoch 1253
Epoch 1253 :: Batch 0/1
Batch Loss = 0.0020639450609343936
1253, epoch_train_loss=0.0020639450609343936
Epoch 1254
Epoch 1254 :: Batch 0/1
Batch Loss = 0.0020634695928654413
1254, epoch_train_loss=0.0020634695928654413
Epoch 1255
Epoch 1255 :: Batch 0/1
Batch Loss = 0.0020629947185287563
1255, epoch_train_loss=0.0020629947185287563
Epoch 1256
Epoch 1256 :: Batch 0/1
Batch Loss = 0.0020625204346455565
1256, epoch_train_loss=0.0020625204346455565
Epoch 1257
Epoch 1257 :: Batch 0/1
Batch Loss = 0.0020620467417966286
1257, epoch_train_loss=0.0020620467417966286
Epoch 1258
Epoch 1258 :: Batch 0/1
Batch Loss = 0.0020615736425836835
1258, epoch_train_loss=0.0020615736425836835
Epoch 1259
Epoch 1259 :: Batch 0/1
Batch Loss = 0.002061101139333287
1259, epoch_train_loss=0.002061101139333287
Epoch 1260
Epoch 1260 :: Batch 0/1
Batch Loss = 0.002060629231687431
1260, epoch_train_loss=0.002060629231687431
Epoch 1261
Epoch 1261 :: Batch 0/1
Batch Loss = 0.0020601579160088113
1261, epoch_train_loss=0.0020601579160088113
Epoch 1262
Epoch 1262 :: Batch 0/1
Batch Loss = 0.002059687187349535
1262, epoch_train_loss=0.002059687187349535
Epoch 1263
Epoch 1263 :: Batch 0/1
Batch Loss = 0.002059217040229573
1263, epoch_train_loss=0.002059217040229573
Epoch 1264
Epoch 1264 :: Batch 0/1
Batch Loss = 0.0020587474694776535
1264, epoch_train_loss=0.0020587474694776535
Epoch 1265
Epoch 1265 :: Batch 0/1
Batch Loss = 0.0020582784719933587
1265, epoch_train_loss=0.0020582784719933587
Epoch 1266
Epoch 1266 :: Batch 0/1
Batch Loss = 0.002057810046098319
1266, epoch_train_loss=0.002057810046098319
Epoch 1267
Epoch 1267 :: Batch 0/1
Batch Loss = 0.002057342191277683
1267, epoch_train_loss=0.002057342191277683
Epoch 1268
Epoch 1268 :: Batch 0/1
Batch Loss = 0.002056874907627796
1268, epoch_train_loss=0.002056874907627796
Epoch 1269
Epoch 1269 :: Batch 0/1
Batch Loss = 0.0020564081956669336
1269, epoch_train_loss=0.0020564081956669336
Epoch 1270
Epoch 1270 :: Batch 0/1
Batch Loss = 0.0020559420550569827
1270, epoch_train_loss=0.0020559420550569827
Epoch 1271
Epoch 1271 :: Batch 0/1
Batch Loss = 0.0020554764854209106
1271, epoch_train_loss=0.0020554764854209106
Epoch 1272
Epoch 1272 :: Batch 0/1
Batch Loss = 0.0020550114860765066
1272, epoch_train_loss=0.0020550114860765066
Epoch 1273
Epoch 1273 :: Batch 0/1
Batch Loss = 0.002054547055943369
1273, epoch_train_loss=0.002054547055943369
Epoch 1274
Epoch 1274 :: Batch 0/1
Batch Loss = 0.002054083194044535
1274, epoch_train_loss=0.002054083194044535
Epoch 1275
Epoch 1275 :: Batch 0/1
Batch Loss = 0.002053619899647443
1275, epoch_train_loss=0.002053619899647443
Epoch 1276
Epoch 1276 :: Batch 0/1
Batch Loss = 0.002053157172195468
1276, epoch_train_loss=0.002053157172195468
Epoch 1277
Epoch 1277 :: Batch 0/1
Batch Loss = 0.0020526950115329
1277, epoch_train_loss=0.0020526950115329
Epoch 1278
Epoch 1278 :: Batch 0/1
Batch Loss = 0.002052233418092984
1278, epoch_train_loss=0.002052233418092984
Epoch 1279
Epoch 1279 :: Batch 0/1
Batch Loss = 0.0020517723922135376
1279, epoch_train_loss=0.0020517723922135376
Epoch 1280
Epoch 1280 :: Batch 0/1
Batch Loss = 0.002051311934764952
1280, epoch_train_loss=0.002051311934764952
Epoch 1281
Epoch 1281 :: Batch 0/1
Batch Loss = 0.0020508520466770716
1281, epoch_train_loss=0.0020508520466770716
Epoch 1282
Epoch 1282 :: Batch 0/1
Batch Loss = 0.002050392728900035
1282, epoch_train_loss=0.002050392728900035
Epoch 1283
Epoch 1283 :: Batch 0/1
Batch Loss = 0.002049933982569661
1283, epoch_train_loss=0.002049933982569661
Epoch 1284
Epoch 1284 :: Batch 0/1
Batch Loss = 0.002049475808882521
1284, epoch_train_loss=0.002049475808882521
Epoch 1285
Epoch 1285 :: Batch 0/1
Batch Loss = 0.002049018209043874
1285, epoch_train_loss=0.002049018209043874
Epoch 1286
Epoch 1286 :: Batch 0/1
Batch Loss = 0.002048561184384945
1286, epoch_train_loss=0.002048561184384945
Epoch 1287
Epoch 1287 :: Batch 0/1
Batch Loss = 0.0020481047362850015
1287, epoch_train_loss=0.0020481047362850015
Epoch 1288
Epoch 1288 :: Batch 0/1
Batch Loss = 0.0020476488662311125
1288, epoch_train_loss=0.0020476488662311125
Epoch 1289
Epoch 1289 :: Batch 0/1
Batch Loss = 0.0020471935758811766
1289, epoch_train_loss=0.0020471935758811766
Epoch 1290
Epoch 1290 :: Batch 0/1
Batch Loss = 0.002046738866921219
1290, epoch_train_loss=0.002046738866921219
Epoch 1291
Epoch 1291 :: Batch 0/1
Batch Loss = 0.0020462847412093346
1291, epoch_train_loss=0.0020462847412093346
Epoch 1292
Epoch 1292 :: Batch 0/1
Batch Loss = 0.0020458312006757483
1292, epoch_train_loss=0.0020458312006757483
Epoch 1293
Epoch 1293 :: Batch 0/1
Batch Loss = 0.0020453782473772446
1293, epoch_train_loss=0.0020453782473772446
Epoch 1294
Epoch 1294 :: Batch 0/1
Batch Loss = 0.0020449258834884017
1294, epoch_train_loss=0.0020449258834884017
Epoch 1295
Epoch 1295 :: Batch 0/1
Batch Loss = 0.002044474111239752
1295, epoch_train_loss=0.002044474111239752
Epoch 1296
Epoch 1296 :: Batch 0/1
Batch Loss = 0.002044022932972083
1296, epoch_train_loss=0.002044022932972083
Epoch 1297
Epoch 1297 :: Batch 0/1
Batch Loss = 0.00204357235116049
1297, epoch_train_loss=0.00204357235116049
Epoch 1298
Epoch 1298 :: Batch 0/1
Batch Loss = 0.002043122368296215
1298, epoch_train_loss=0.002043122368296215
Epoch 1299
Epoch 1299 :: Batch 0/1
Batch Loss = 0.002042672987078718
1299, epoch_train_loss=0.002042672987078718
Epoch 1300
Epoch 1300 :: Batch 0/1
Batch Loss = 0.002042224210107661
1300, epoch_train_loss=0.002042224210107661
Epoch 1301
Epoch 1301 :: Batch 0/1
Batch Loss = 0.00204177604022873
1301, epoch_train_loss=0.00204177604022873
Epoch 1302
Epoch 1302 :: Batch 0/1
Batch Loss = 0.002041328480312761
1302, epoch_train_loss=0.002041328480312761
Epoch 1303
Epoch 1303 :: Batch 0/1
Batch Loss = 0.002040881533399025
1303, epoch_train_loss=0.002040881533399025
Epoch 1304
Epoch 1304 :: Batch 0/1
Batch Loss = 0.0020404352024553482
1304, epoch_train_loss=0.0020404352024553482
Epoch 1305
Epoch 1305 :: Batch 0/1
Batch Loss = 0.002039989490762818
1305, epoch_train_loss=0.002039989490762818
Epoch 1306
Epoch 1306 :: Batch 0/1
Batch Loss = 0.002039544401595759
1306, epoch_train_loss=0.002039544401595759
Epoch 1307
Epoch 1307 :: Batch 0/1
Batch Loss = 0.002039099938514856
1307, epoch_train_loss=0.002039099938514856
Epoch 1308
Epoch 1308 :: Batch 0/1
Batch Loss = 0.00203865610521866
1308, epoch_train_loss=0.00203865610521866
Epoch 1309
Epoch 1309 :: Batch 0/1
Batch Loss = 0.0020382129057724485
1309, epoch_train_loss=0.0020382129057724485
Epoch 1310
Epoch 1310 :: Batch 0/1
Batch Loss = 0.002037770344698624
1310, epoch_train_loss=0.002037770344698624
Epoch 1311
Epoch 1311 :: Batch 0/1
Batch Loss = 0.0020373284271653267
1311, epoch_train_loss=0.0020373284271653267
Epoch 1312
Epoch 1312 :: Batch 0/1
Batch Loss = 0.0020368871592677824
1312, epoch_train_loss=0.0020368871592677824
Epoch 1313
Epoch 1313 :: Batch 0/1
Batch Loss = 0.00203644654889215
1313, epoch_train_loss=0.00203644654889215
Epoch 1314
Epoch 1314 :: Batch 0/1
Batch Loss = 0.002036006606014403
1314, epoch_train_loss=0.002036006606014403
Epoch 1315
Epoch 1315 :: Batch 0/1
Batch Loss = 0.002035567344696846
1315, epoch_train_loss=0.002035567344696846
Epoch 1316
Epoch 1316 :: Batch 0/1
Batch Loss = 0.002035128784980087
1316, epoch_train_loss=0.002035128784980087
Epoch 1317
Epoch 1317 :: Batch 0/1
Batch Loss = 0.002034690956838624
1317, epoch_train_loss=0.002034690956838624
Epoch 1318
Epoch 1318 :: Batch 0/1
Batch Loss = 0.002034253906219584
1318, epoch_train_loss=0.002034253906219584
Epoch 1319
Epoch 1319 :: Batch 0/1
Batch Loss = 0.0020338177057633196
1319, epoch_train_loss=0.0020338177057633196
Epoch 1320
Epoch 1320 :: Batch 0/1
Batch Loss = 0.0020333824718693255
1320, epoch_train_loss=0.0020333824718693255
Epoch 1321
Epoch 1321 :: Batch 0/1
Batch Loss = 0.00203294839422566
1321, epoch_train_loss=0.00203294839422566
Epoch 1322
Epoch 1322 :: Batch 0/1
Batch Loss = 0.0020325157873412763
1322, epoch_train_loss=0.0020325157873412763
Epoch 1323
Epoch 1323 :: Batch 0/1
Batch Loss = 0.002032085174143032
1323, epoch_train_loss=0.002032085174143032
Epoch 1324
Epoch 1324 :: Batch 0/1
Batch Loss = 0.002031657443555482
1324, epoch_train_loss=0.002031657443555482
Epoch 1325
Epoch 1325 :: Batch 0/1
Batch Loss = 0.002031234097345664
1325, epoch_train_loss=0.002031234097345664
Epoch 1326
Epoch 1326 :: Batch 0/1
Batch Loss = 0.002030817745548902
1326, epoch_train_loss=0.002030817745548902
Epoch 1327
Epoch 1327 :: Batch 0/1
Batch Loss = 0.002030412853621007
1327, epoch_train_loss=0.002030412853621007
Epoch 1328
Epoch 1328 :: Batch 0/1
Batch Loss = 0.0020300273581322645
1328, epoch_train_loss=0.0020300273581322645
Epoch 1329
Epoch 1329 :: Batch 0/1
Batch Loss = 0.0020296749627132
1329, epoch_train_loss=0.0020296749627132
Epoch 1330
Epoch 1330 :: Batch 0/1
Batch Loss = 0.0020293806359080735
1330, epoch_train_loss=0.0020293806359080735
Epoch 1331
Epoch 1331 :: Batch 0/1
Batch Loss = 0.002029187678870386
1331, epoch_train_loss=0.002029187678870386
Epoch 1332
Epoch 1332 :: Batch 0/1
Batch Loss = 0.0020291773797373736
1332, epoch_train_loss=0.0020291773797373736
Epoch 1333
Epoch 1333 :: Batch 0/1
Batch Loss = 0.0020294902409497472
1333, epoch_train_loss=0.0020294902409497472
Epoch 1334
Epoch 1334 :: Batch 0/1
Batch Loss = 0.0020304004991066487
1334, epoch_train_loss=0.0020304004991066487
Epoch 1335
Epoch 1335 :: Batch 0/1
Batch Loss = 0.002032374395223442
1335, epoch_train_loss=0.002032374395223442
Epoch 1336
Epoch 1336 :: Batch 0/1
Batch Loss = 0.0020363733701363696
1336, epoch_train_loss=0.0020363733701363696
Epoch 1337
Epoch 1337 :: Batch 0/1
Batch Loss = 0.0020439681819880997
1337, epoch_train_loss=0.0020439681819880997
Epoch 1338
Epoch 1338 :: Batch 0/1
Batch Loss = 0.0020586792584978563
1338, epoch_train_loss=0.0020586792584978563
Epoch 1339
Epoch 1339 :: Batch 0/1
Batch Loss = 0.002085800431766735
1339, epoch_train_loss=0.002085800431766735
Epoch 1340
Epoch 1340 :: Batch 0/1
Batch Loss = 0.002138898509844436
1340, epoch_train_loss=0.002138898509844436
Epoch 1341
Epoch 1341 :: Batch 0/1
Batch Loss = 0.0022352120399731926
1341, epoch_train_loss=0.0022352120399731926
Epoch 1342
Epoch 1342 :: Batch 0/1
Batch Loss = 0.002430274772641784
1342, epoch_train_loss=0.002430274772641784
Epoch 1343
Epoch 1343 :: Batch 0/1
Batch Loss = 0.0027728624940199153
1343, epoch_train_loss=0.0027728624940199153
Epoch 1344
Epoch 1344 :: Batch 0/1
Batch Loss = 0.003505219734898366
1344, epoch_train_loss=0.003505219734898366
Epoch 1345
Epoch 1345 :: Batch 0/1
Batch Loss = 0.00468862073366473
1345, epoch_train_loss=0.00468862073366473
Epoch 1346
Epoch 1346 :: Batch 0/1
Batch Loss = 0.007419197050615724
1346, epoch_train_loss=0.007419197050615724
Epoch 1347
Epoch 1347 :: Batch 0/1
Batch Loss = 0.010929358567869975
1347, epoch_train_loss=0.010929358567869975
Epoch 1348
Epoch 1348 :: Batch 0/1
Batch Loss = 0.01980005833069474
1348, epoch_train_loss=0.01980005833069474
Epoch 1349
Epoch 1349 :: Batch 0/1
Batch Loss = 0.024867166048472916
1349, epoch_train_loss=0.024867166048472916
Epoch 1350
Epoch 1350 :: Batch 0/1
Batch Loss = 0.039918933423699995
1350, epoch_train_loss=0.039918933423699995
Epoch 1351
Epoch 1351 :: Batch 0/1
Batch Loss = 0.029163241783095813
1351, epoch_train_loss=0.029163241783095813
Epoch 1352
Epoch 1352 :: Batch 0/1
Batch Loss = 0.022657105445077694
1352, epoch_train_loss=0.022657105445077694
Epoch 1353
Epoch 1353 :: Batch 0/1
Batch Loss = 0.006508876234956117
1353, epoch_train_loss=0.006508876234956117
Epoch 1354
Epoch 1354 :: Batch 0/1
Batch Loss = 0.002572207024577513
1354, epoch_train_loss=0.002572207024577513
Epoch 1355
Epoch 1355 :: Batch 0/1
Batch Loss = 0.009652199061716011
1355, epoch_train_loss=0.009652199061716011
Epoch 1356
Epoch 1356 :: Batch 0/1
Batch Loss = 0.015259753624766661
1356, epoch_train_loss=0.015259753624766661
Epoch 1357
Epoch 1357 :: Batch 0/1
Batch Loss = 0.01696005971138415
1357, epoch_train_loss=0.01696005971138415
Epoch 1358
Epoch 1358 :: Batch 0/1
Batch Loss = 0.006606198613729897
1358, epoch_train_loss=0.006606198613729897
Epoch 1359
Epoch 1359 :: Batch 0/1
Batch Loss = 0.0023202645798056096
1359, epoch_train_loss=0.0023202645798056096
Epoch 1360
Epoch 1360 :: Batch 0/1
Batch Loss = 0.00654368479120551
1360, epoch_train_loss=0.00654368479120551
Epoch 1361
Epoch 1361 :: Batch 0/1
Batch Loss = 0.009822615140445782
1361, epoch_train_loss=0.009822615140445782
Epoch 1362
Epoch 1362 :: Batch 0/1
Batch Loss = 0.008359409590139357
1362, epoch_train_loss=0.008359409590139357
Epoch 1363
Epoch 1363 :: Batch 0/1
Batch Loss = 0.002893673991908959
1363, epoch_train_loss=0.002893673991908959
Epoch 1364
Epoch 1364 :: Batch 0/1
Batch Loss = 0.003143509753123645
1364, epoch_train_loss=0.003143509753123645
Epoch 1365
Epoch 1365 :: Batch 0/1
Batch Loss = 0.0069756184681421494
1365, epoch_train_loss=0.0069756184681421494
Epoch 1366
Epoch 1366 :: Batch 0/1
Batch Loss = 0.006286651886872792
1366, epoch_train_loss=0.006286651886872792
Epoch 1367
Epoch 1367 :: Batch 0/1
Batch Loss = 0.003366870255662386
1367, epoch_train_loss=0.003366870255662386
Epoch 1368
Epoch 1368 :: Batch 0/1
Batch Loss = 0.0023181807299998045
1368, epoch_train_loss=0.0023181807299998045
Epoch 1369
Epoch 1369 :: Batch 0/1
Batch Loss = 0.004299808529153861
1369, epoch_train_loss=0.004299808529153861
Epoch 1370
Epoch 1370 :: Batch 0/1
Batch Loss = 0.005375855051330397
1370, epoch_train_loss=0.005375855051330397
Epoch 1371
Epoch 1371 :: Batch 0/1
Batch Loss = 0.003202587071346962
1371, epoch_train_loss=0.003202587071346962
Epoch 1372
Epoch 1372 :: Batch 0/1
Batch Loss = 0.0022147083138483134
1372, epoch_train_loss=0.0022147083138483134
Epoch 1373
Epoch 1373 :: Batch 0/1
Batch Loss = 0.003573582927801305
1373, epoch_train_loss=0.003573582927801305
Epoch 1374
Epoch 1374 :: Batch 0/1
Batch Loss = 0.0040443768913268514
1374, epoch_train_loss=0.0040443768913268514
Epoch 1375
Epoch 1375 :: Batch 0/1
Batch Loss = 0.002927891173548454
1375, epoch_train_loss=0.002927891173548454
Epoch 1376
Epoch 1376 :: Batch 0/1
Batch Loss = 0.0021533788047064526
1376, epoch_train_loss=0.0021533788047064526
Epoch 1377
Epoch 1377 :: Batch 0/1
Batch Loss = 0.0029371350051074826
1377, epoch_train_loss=0.0029371350051074826
Epoch 1378
Epoch 1378 :: Batch 0/1
Batch Loss = 0.003497892349656823
1378, epoch_train_loss=0.003497892349656823
Epoch 1379
Epoch 1379 :: Batch 0/1
Batch Loss = 0.002650846617012545
1379, epoch_train_loss=0.002650846617012545
Epoch 1380
Epoch 1380 :: Batch 0/1
Batch Loss = 0.002149744273349456
1380, epoch_train_loss=0.002149744273349456
Epoch 1381
Epoch 1381 :: Batch 0/1
Batch Loss = 0.0026769045292431965
1381, epoch_train_loss=0.0026769045292431965
Epoch 1382
Epoch 1382 :: Batch 0/1
Batch Loss = 0.002958553253614855
1382, epoch_train_loss=0.002958553253614855
Epoch 1383
Epoch 1383 :: Batch 0/1
Batch Loss = 0.0024989881114779993
1383, epoch_train_loss=0.0024989881114779993
Epoch 1384
Epoch 1384 :: Batch 0/1
Batch Loss = 0.0021208579701449824
1384, epoch_train_loss=0.0021208579701449824
Epoch 1385
Epoch 1385 :: Batch 0/1
Batch Loss = 0.0024177815349068076
1385, epoch_train_loss=0.0024177815349068076
Epoch 1386
Epoch 1386 :: Batch 0/1
Batch Loss = 0.0026984982138992613
1386, epoch_train_loss=0.0026984982138992613
Epoch 1387
Epoch 1387 :: Batch 0/1
Batch Loss = 0.0023865938681298642
1387, epoch_train_loss=0.0023865938681298642
Epoch 1388
Epoch 1388 :: Batch 0/1
Batch Loss = 0.0021030815130081825
1388, epoch_train_loss=0.0021030815130081825
Epoch 1389
Epoch 1389 :: Batch 0/1
Batch Loss = 0.0022513152026413523
1389, epoch_train_loss=0.0022513152026413523
Epoch 1390
Epoch 1390 :: Batch 0/1
Batch Loss = 0.002448703529785067
1390, epoch_train_loss=0.002448703529785067
Epoch 1391
Epoch 1391 :: Batch 0/1
Batch Loss = 0.0023298483725538812
1391, epoch_train_loss=0.0023298483725538812
Epoch 1392
Epoch 1392 :: Batch 0/1
Batch Loss = 0.0021005309145845576
1392, epoch_train_loss=0.0021005309145845576
Epoch 1393
Epoch 1393 :: Batch 0/1
Batch Loss = 0.0021317926301311256
1393, epoch_train_loss=0.0021317926301311256
Epoch 1394
Epoch 1394 :: Batch 0/1
Batch Loss = 0.0023001703546842323
1394, epoch_train_loss=0.0023001703546842323
Epoch 1395
Epoch 1395 :: Batch 0/1
Batch Loss = 0.002270199069843084
1395, epoch_train_loss=0.002270199069843084
Epoch 1396
Epoch 1396 :: Batch 0/1
Batch Loss = 0.002107692806318767
1396, epoch_train_loss=0.002107692806318767
Epoch 1397
Epoch 1397 :: Batch 0/1
Batch Loss = 0.002064927963273498
1397, epoch_train_loss=0.002064927963273498
Epoch 1398
Epoch 1398 :: Batch 0/1
Batch Loss = 0.002166320532065716
1398, epoch_train_loss=0.002166320532065716
Epoch 1399
Epoch 1399 :: Batch 0/1
Batch Loss = 0.002208585761983342
1399, epoch_train_loss=0.002208585761983342
Epoch 1400
Epoch 1400 :: Batch 0/1
Batch Loss = 0.002116671629228988
1400, epoch_train_loss=0.002116671629228988
Epoch 1401
Epoch 1401 :: Batch 0/1
Batch Loss = 0.002045603032462519
1401, epoch_train_loss=0.002045603032462519
Epoch 1402
Epoch 1402 :: Batch 0/1
Batch Loss = 0.002080481574890656
1402, epoch_train_loss=0.002080481574890656
Epoch 1403
Epoch 1403 :: Batch 0/1
Batch Loss = 0.0021362168403113698
1403, epoch_train_loss=0.0021362168403113698
Epoch 1404
Epoch 1404 :: Batch 0/1
Batch Loss = 0.002117246266956354
1404, epoch_train_loss=0.002117246266956354
Epoch 1405
Epoch 1405 :: Batch 0/1
Batch Loss = 0.0020517278145898293
1405, epoch_train_loss=0.0020517278145898293
Epoch 1406
Epoch 1406 :: Batch 0/1
Batch Loss = 0.0020349769804965945
1406, epoch_train_loss=0.0020349769804965945
Epoch 1407
Epoch 1407 :: Batch 0/1
Batch Loss = 0.002074209598180442
1407, epoch_train_loss=0.002074209598180442
Epoch 1408
Epoch 1408 :: Batch 0/1
Batch Loss = 0.0020923839535504215
1408, epoch_train_loss=0.0020923839535504215
Epoch 1409
Epoch 1409 :: Batch 0/1
Batch Loss = 0.002061751889225044
1409, epoch_train_loss=0.002061751889225044
Epoch 1410
Epoch 1410 :: Batch 0/1
Batch Loss = 0.002026720766592674
1410, epoch_train_loss=0.002026720766592674
Epoch 1411
Epoch 1411 :: Batch 0/1
Batch Loss = 0.002031094385953095
1411, epoch_train_loss=0.002031094385953095
Epoch 1412
Epoch 1412 :: Batch 0/1
Batch Loss = 0.0020551209617216334
1412, epoch_train_loss=0.0020551209617216334
Epoch 1413
Epoch 1413 :: Batch 0/1
Batch Loss = 0.002057714939907716
1413, epoch_train_loss=0.002057714939907716
Epoch 1414
Epoch 1414 :: Batch 0/1
Batch Loss = 0.002035431868184426
1414, epoch_train_loss=0.002035431868184426
Epoch 1415
Epoch 1415 :: Batch 0/1
Batch Loss = 0.002017189567690205
1415, epoch_train_loss=0.002017189567690205
Epoch 1416
Epoch 1416 :: Batch 0/1
Batch Loss = 0.002022756541325096
1416, epoch_train_loss=0.002022756541325096
Epoch 1417
Epoch 1417 :: Batch 0/1
Batch Loss = 0.0020369396969133523
1417, epoch_train_loss=0.0020369396969133523
Epoch 1418
Epoch 1418 :: Batch 0/1
Batch Loss = 0.002035939208373482
1418, epoch_train_loss=0.002035939208373482
Epoch 1419
Epoch 1419 :: Batch 0/1
Batch Loss = 0.0020212513640148126
1419, epoch_train_loss=0.0020212513640148126
Epoch 1420
Epoch 1420 :: Batch 0/1
Batch Loss = 0.002011243107263474
1420, epoch_train_loss=0.002011243107263474
Epoch 1421
Epoch 1421 :: Batch 0/1
Batch Loss = 0.0020145712800801172
1421, epoch_train_loss=0.0020145712800801172
Epoch 1422
Epoch 1422 :: Batch 0/1
Batch Loss = 0.0020224323854200373
1422, epoch_train_loss=0.0020224323854200373
Epoch 1423
Epoch 1423 :: Batch 0/1
Batch Loss = 0.0020221368166754654
1423, epoch_train_loss=0.0020221368166754654
Epoch 1424
Epoch 1424 :: Batch 0/1
Batch Loss = 0.0020136641624518077
1424, epoch_train_loss=0.0020136641624518077
Epoch 1425
Epoch 1425 :: Batch 0/1
Batch Loss = 0.002006552624143805
1425, epoch_train_loss=0.002006552624143805
Epoch 1426
Epoch 1426 :: Batch 0/1
Batch Loss = 0.0020075753811963147
1426, epoch_train_loss=0.0020075753811963147
Epoch 1427
Epoch 1427 :: Batch 0/1
Batch Loss = 0.0020121994618487967
1427, epoch_train_loss=0.0020121994618487967
Epoch 1428
Epoch 1428 :: Batch 0/1
Batch Loss = 0.002012733458082733
1428, epoch_train_loss=0.002012733458082733
Epoch 1429
Epoch 1429 :: Batch 0/1
Batch Loss = 0.002008076048884725
1429, epoch_train_loss=0.002008076048884725
Epoch 1430
Epoch 1430 :: Batch 0/1
Batch Loss = 0.002003073866743089
1430, epoch_train_loss=0.002003073866743089
Epoch 1431
Epoch 1431 :: Batch 0/1
Batch Loss = 0.002002095881306429
1431, epoch_train_loss=0.002002095881306429
Epoch 1432
Epoch 1432 :: Batch 0/1
Batch Loss = 0.00200437261701248
1432, epoch_train_loss=0.00200437261701248
Epoch 1433
Epoch 1433 :: Batch 0/1
Batch Loss = 0.0020056276589182753
1433, epoch_train_loss=0.0020056276589182753
Epoch 1434
Epoch 1434 :: Batch 0/1
Batch Loss = 0.002003547480266812
1434, epoch_train_loss=0.002003547480266812
Epoch 1435
Epoch 1435 :: Batch 0/1
Batch Loss = 0.0020000646146622588
1435, epoch_train_loss=0.0020000646146622588
Epoch 1436
Epoch 1436 :: Batch 0/1
Batch Loss = 0.001998252974351365
1436, epoch_train_loss=0.001998252974351365
Epoch 1437
Epoch 1437 :: Batch 0/1
Batch Loss = 0.001998770871050895
1437, epoch_train_loss=0.001998770871050895
Epoch 1438
Epoch 1438 :: Batch 0/1
Batch Loss = 0.001999758790299081
1438, epoch_train_loss=0.001999758790299081
Epoch 1439
Epoch 1439 :: Batch 0/1
Batch Loss = 0.0019993422626037247
1439, epoch_train_loss=0.0019993422626037247
Epoch 1440
Epoch 1440 :: Batch 0/1
Batch Loss = 0.0019973444477771754
1440, epoch_train_loss=0.0019973444477771754
Epoch 1441
Epoch 1441 :: Batch 0/1
Batch Loss = 0.001995395557641433
1441, epoch_train_loss=0.001995395557641433
Epoch 1442
Epoch 1442 :: Batch 0/1
Batch Loss = 0.0019947279618109707
1442, epoch_train_loss=0.0019947279618109707
Epoch 1443
Epoch 1443 :: Batch 0/1
Batch Loss = 0.0019950505484077454
1443, epoch_train_loss=0.0019950505484077454
Epoch 1444
Epoch 1444 :: Batch 0/1
Batch Loss = 0.0019951848932072648
1444, epoch_train_loss=0.0019951848932072648
Epoch 1445
Epoch 1445 :: Batch 0/1
Batch Loss = 0.0019944212228718526
1445, epoch_train_loss=0.0019944212228718526
Epoch 1446
Epoch 1446 :: Batch 0/1
Batch Loss = 0.001993041355027201
1446, epoch_train_loss=0.001993041355027201
Epoch 1447
Epoch 1447 :: Batch 0/1
Batch Loss = 0.001991883196519765
1447, epoch_train_loss=0.001991883196519765
Epoch 1448
Epoch 1448 :: Batch 0/1
Batch Loss = 0.001991447078997128
1448, epoch_train_loss=0.001991447078997128
Epoch 1449
Epoch 1449 :: Batch 0/1
Batch Loss = 0.001991441569556444
1449, epoch_train_loss=0.001991441569556444
Epoch 1450
Epoch 1450 :: Batch 0/1
Batch Loss = 0.0019912521674290183
1450, epoch_train_loss=0.0019912521674290183
Epoch 1451
Epoch 1451 :: Batch 0/1
Batch Loss = 0.001990575619538114
1451, epoch_train_loss=0.001990575619538114
Epoch 1452
Epoch 1452 :: Batch 0/1
Batch Loss = 0.0019896135595332152
1452, epoch_train_loss=0.0019896135595332152
Epoch 1453
Epoch 1453 :: Batch 0/1
Batch Loss = 0.0019887939486790984
1453, epoch_train_loss=0.0019887939486790984
Epoch 1454
Epoch 1454 :: Batch 0/1
Batch Loss = 0.0019883569372278324
1454, epoch_train_loss=0.0019883569372278324
Epoch 1455
Epoch 1455 :: Batch 0/1
Batch Loss = 0.001988145480972209
1455, epoch_train_loss=0.001988145480972209
Epoch 1456
Epoch 1456 :: Batch 0/1
Batch Loss = 0.001987859014504186
1456, epoch_train_loss=0.001987859014504186
Epoch 1457
Epoch 1457 :: Batch 0/1
Batch Loss = 0.0019873250567572056
1457, epoch_train_loss=0.0019873250567572056
Epoch 1458
Epoch 1458 :: Batch 0/1
Batch Loss = 0.0019866273788891418
1458, epoch_train_loss=0.0019866273788891418
Epoch 1459
Epoch 1459 :: Batch 0/1
Batch Loss = 0.0019859704242451036
1459, epoch_train_loss=0.0019859704242451036
Epoch 1460
Epoch 1460 :: Batch 0/1
Batch Loss = 0.0019854989071803678
1460, epoch_train_loss=0.0019854989071803678
Epoch 1461
Epoch 1461 :: Batch 0/1
Batch Loss = 0.0019851722712660707
1461, epoch_train_loss=0.0019851722712660707
Epoch 1462
Epoch 1462 :: Batch 0/1
Batch Loss = 0.0019848517798717867
1462, epoch_train_loss=0.0019848517798717867
Epoch 1463
Epoch 1463 :: Batch 0/1
Batch Loss = 0.00198442855023996
1463, epoch_train_loss=0.00198442855023996
Epoch 1464
Epoch 1464 :: Batch 0/1
Batch Loss = 0.0019838952102148154
1464, epoch_train_loss=0.0019838952102148154
Epoch 1465
Epoch 1465 :: Batch 0/1
Batch Loss = 0.001983341350531572
1465, epoch_train_loss=0.001983341350531572
Epoch 1466
Epoch 1466 :: Batch 0/1
Batch Loss = 0.0019828610898299185
1466, epoch_train_loss=0.0019828610898299185
Epoch 1467
Epoch 1467 :: Batch 0/1
Batch Loss = 0.0019824735292402677
1467, epoch_train_loss=0.0019824735292402677
Epoch 1468
Epoch 1468 :: Batch 0/1
Batch Loss = 0.0019821266445900487
1468, epoch_train_loss=0.0019821266445900487
Epoch 1469
Epoch 1469 :: Batch 0/1
Batch Loss = 0.001981757804501691
1469, epoch_train_loss=0.001981757804501691
Epoch 1470
Epoch 1470 :: Batch 0/1
Batch Loss = 0.001981331560427665
1470, epoch_train_loss=0.001981331560427665
Epoch 1471
Epoch 1471 :: Batch 0/1
Batch Loss = 0.001980866393832048
1471, epoch_train_loss=0.001980866393832048
Epoch 1472
Epoch 1472 :: Batch 0/1
Batch Loss = 0.001980409672543139
1472, epoch_train_loss=0.001980409672543139
Epoch 1473
Epoch 1473 :: Batch 0/1
Batch Loss = 0.0019799948789904448
1473, epoch_train_loss=0.0019799948789904448
Epoch 1474
Epoch 1474 :: Batch 0/1
Batch Loss = 0.001979620480226146
1474, epoch_train_loss=0.001979620480226146
Epoch 1475
Epoch 1475 :: Batch 0/1
Batch Loss = 0.0019792640868673984
1475, epoch_train_loss=0.0019792640868673984
Epoch 1476
Epoch 1476 :: Batch 0/1
Batch Loss = 0.00197889527680797
1476, epoch_train_loss=0.00197889527680797
Epoch 1477
Epoch 1477 :: Batch 0/1
Batch Loss = 0.0019785006554669336
1477, epoch_train_loss=0.0019785006554669336
Epoch 1478
Epoch 1478 :: Batch 0/1
Batch Loss = 0.001978090251680665
1478, epoch_train_loss=0.001978090251680665
Epoch 1479
Epoch 1479 :: Batch 0/1
Batch Loss = 0.00197768443731503
1479, epoch_train_loss=0.00197768443731503
Epoch 1480
Epoch 1480 :: Batch 0/1
Batch Loss = 0.0019772974914770507
1480, epoch_train_loss=0.0019772974914770507
Epoch 1481
Epoch 1481 :: Batch 0/1
Batch Loss = 0.0019769317729443022
1481, epoch_train_loss=0.0019769317729443022
Epoch 1482
Epoch 1482 :: Batch 0/1
Batch Loss = 0.001976577646302012
1482, epoch_train_loss=0.001976577646302012
Epoch 1483
Epoch 1483 :: Batch 0/1
Batch Loss = 0.0019762216791231924
1483, epoch_train_loss=0.0019762216791231924
Epoch 1484
Epoch 1484 :: Batch 0/1
Batch Loss = 0.001975856956240758
1484, epoch_train_loss=0.001975856956240758
Epoch 1485
Epoch 1485 :: Batch 0/1
Batch Loss = 0.001975484004185293
1485, epoch_train_loss=0.001975484004185293
Epoch 1486
Epoch 1486 :: Batch 0/1
Batch Loss = 0.001975110127181213
1486, epoch_train_loss=0.001975110127181213
Epoch 1487
Epoch 1487 :: Batch 0/1
Batch Loss = 0.001974743256161772
1487, epoch_train_loss=0.001974743256161772
Epoch 1488
Epoch 1488 :: Batch 0/1
Batch Loss = 0.001974387105622182
1488, epoch_train_loss=0.001974387105622182
Epoch 1489
Epoch 1489 :: Batch 0/1
Batch Loss = 0.0019740396180915514
1489, epoch_train_loss=0.0019740396180915514
Epoch 1490
Epoch 1490 :: Batch 0/1
Batch Loss = 0.001973696428685885
1490, epoch_train_loss=0.001973696428685885
Epoch 1491
Epoch 1491 :: Batch 0/1
Batch Loss = 0.0019733524460421874
1491, epoch_train_loss=0.0019733524460421874
Epoch 1492
Epoch 1492 :: Batch 0/1
Batch Loss = 0.0019730054592296937
1492, epoch_train_loss=0.0019730054592296937
Epoch 1493
Epoch 1493 :: Batch 0/1
Batch Loss = 0.0019726566621473384
1493, epoch_train_loss=0.0019726566621473384
Epoch 1494
Epoch 1494 :: Batch 0/1
Batch Loss = 0.00197230881116532
1494, epoch_train_loss=0.00197230881116532
Epoch 1495
Epoch 1495 :: Batch 0/1
Batch Loss = 0.0019719645853078296
1495, epoch_train_loss=0.0019719645853078296
Epoch 1496
Epoch 1496 :: Batch 0/1
Batch Loss = 0.0019716257002222472
1496, epoch_train_loss=0.0019716257002222472
Epoch 1497
Epoch 1497 :: Batch 0/1
Batch Loss = 0.001971291661102662
1497, epoch_train_loss=0.001971291661102662
Epoch 1498
Epoch 1498 :: Batch 0/1
Batch Loss = 0.001970960575253981
1498, epoch_train_loss=0.001970960575253981
Epoch 1499
Epoch 1499 :: Batch 0/1
Batch Loss = 0.0019706307316259123
1499, epoch_train_loss=0.0019706307316259123
Epoch 1500
Epoch 1500 :: Batch 0/1
Batch Loss = 0.0019703009647770754
1500, epoch_train_loss=0.0019703009647770754
Epoch 1501
Epoch 1501 :: Batch 0/1
Batch Loss = 0.001969971073842756
1501, epoch_train_loss=0.001969971073842756
Epoch 1502
Epoch 1502 :: Batch 0/1
Batch Loss = 0.0019696417968610843
1502, epoch_train_loss=0.0019696417968610843
Epoch 1503
Epoch 1503 :: Batch 0/1
Batch Loss = 0.0019693142034224705
1503, epoch_train_loss=0.0019693142034224705
Epoch 1504
Epoch 1504 :: Batch 0/1
Batch Loss = 0.0019689890773219696
1504, epoch_train_loss=0.0019689890773219696
Epoch 1505
Epoch 1505 :: Batch 0/1
Batch Loss = 0.0019686668097797016
1505, epoch_train_loss=0.0019686668097797016
Epoch 1506
Epoch 1506 :: Batch 0/1
Batch Loss = 0.0019683470759433563
1506, epoch_train_loss=0.0019683470759433563
Epoch 1507
Epoch 1507 :: Batch 0/1
Batch Loss = 0.00196802931650805
1507, epoch_train_loss=0.00196802931650805
Epoch 1508
Epoch 1508 :: Batch 0/1
Batch Loss = 0.001967712946365072
1508, epoch_train_loss=0.001967712946365072
Epoch 1509
Epoch 1509 :: Batch 0/1
Batch Loss = 0.0019673975212670786
1509, epoch_train_loss=0.0019673975212670786
Epoch 1510
Epoch 1510 :: Batch 0/1
Batch Loss = 0.0019670828429827437
1510, epoch_train_loss=0.0019670828429827437
Epoch 1511
Epoch 1511 :: Batch 0/1
Batch Loss = 0.001966769102570485
1511, epoch_train_loss=0.001966769102570485
Epoch 1512
Epoch 1512 :: Batch 0/1
Batch Loss = 0.001966456533229117
1512, epoch_train_loss=0.001966456533229117
Epoch 1513
Epoch 1513 :: Batch 0/1
Batch Loss = 0.0019661453997647928
1513, epoch_train_loss=0.0019661453997647928
Epoch 1514
Epoch 1514 :: Batch 0/1
Batch Loss = 0.0019658359102576674
1514, epoch_train_loss=0.0019658359102576674
Epoch 1515
Epoch 1515 :: Batch 0/1
Batch Loss = 0.0019655280921806696
1515, epoch_train_loss=0.0019655280921806696
Epoch 1516
Epoch 1516 :: Batch 0/1
Batch Loss = 0.0019652218482709934
1516, epoch_train_loss=0.0019652218482709934
Epoch 1517
Epoch 1517 :: Batch 0/1
Batch Loss = 0.0019649170323052583
1517, epoch_train_loss=0.0019649170323052583
Epoch 1518
Epoch 1518 :: Batch 0/1
Batch Loss = 0.0019646134609274655
1518, epoch_train_loss=0.0019646134609274655
Epoch 1519
Epoch 1519 :: Batch 0/1
Batch Loss = 0.001964310968768182
1519, epoch_train_loss=0.001964310968768182
Epoch 1520
Epoch 1520 :: Batch 0/1
Batch Loss = 0.0019640094910409354
1520, epoch_train_loss=0.0019640094910409354
Epoch 1521
Epoch 1521 :: Batch 0/1
Batch Loss = 0.0019637089828090392
1521, epoch_train_loss=0.0019637089828090392
Epoch 1522
Epoch 1522 :: Batch 0/1
Batch Loss = 0.001963409475811928
1522, epoch_train_loss=0.001963409475811928
Epoch 1523
Epoch 1523 :: Batch 0/1
Batch Loss = 0.001963111012664506
1523, epoch_train_loss=0.001963111012664506
Epoch 1524
Epoch 1524 :: Batch 0/1
Batch Loss = 0.0019628136370530022
1524, epoch_train_loss=0.0019628136370530022
Epoch 1525
Epoch 1525 :: Batch 0/1
Batch Loss = 0.001962517375170744
1525, epoch_train_loss=0.001962517375170744
Epoch 1526
Epoch 1526 :: Batch 0/1
Batch Loss = 0.001962222250255906
1526, epoch_train_loss=0.001962222250255906
Epoch 1527
Epoch 1527 :: Batch 0/1
Batch Loss = 0.001961928233442565
1527, epoch_train_loss=0.001961928233442565
Epoch 1528
Epoch 1528 :: Batch 0/1
Batch Loss = 0.001961635294573483
1528, epoch_train_loss=0.001961635294573483
Epoch 1529
Epoch 1529 :: Batch 0/1
Batch Loss = 0.0019613433975770975
1529, epoch_train_loss=0.0019613433975770975
Epoch 1530
Epoch 1530 :: Batch 0/1
Batch Loss = 0.001961052494940119
1530, epoch_train_loss=0.001961052494940119
Epoch 1531
Epoch 1531 :: Batch 0/1
Batch Loss = 0.0019607625448478253
1531, epoch_train_loss=0.0019607625448478253
Epoch 1532
Epoch 1532 :: Batch 0/1
Batch Loss = 0.001960473514510039
1532, epoch_train_loss=0.001960473514510039
Epoch 1533
Epoch 1533 :: Batch 0/1
Batch Loss = 0.0019601853785243973
1533, epoch_train_loss=0.0019601853785243973
Epoch 1534
Epoch 1534 :: Batch 0/1
Batch Loss = 0.0019598981172358892
1534, epoch_train_loss=0.0019598981172358892
Epoch 1535
Epoch 1535 :: Batch 0/1
Batch Loss = 0.0019596117210641806
1535, epoch_train_loss=0.0019596117210641806
Epoch 1536
Epoch 1536 :: Batch 0/1
Batch Loss = 0.0019593261743468416
1536, epoch_train_loss=0.0019593261743468416
Epoch 1537
Epoch 1537 :: Batch 0/1
Batch Loss = 0.0019590414747544514
1537, epoch_train_loss=0.0019590414747544514
Epoch 1538
Epoch 1538 :: Batch 0/1
Batch Loss = 0.0019587576141483275
1538, epoch_train_loss=0.0019587576141483275
Epoch 1539
Epoch 1539 :: Batch 0/1
Batch Loss = 0.001958474584412856
1539, epoch_train_loss=0.001958474584412856
Epoch 1540
Epoch 1540 :: Batch 0/1
Batch Loss = 0.0019581923742210093
1540, epoch_train_loss=0.0019581923742210093
Epoch 1541
Epoch 1541 :: Batch 0/1
Batch Loss = 0.001957910977778031
1541, epoch_train_loss=0.001957910977778031
Epoch 1542
Epoch 1542 :: Batch 0/1
Batch Loss = 0.0019576303802598875
1542, epoch_train_loss=0.0019576303802598875
Epoch 1543
Epoch 1543 :: Batch 0/1
Batch Loss = 0.001957350571734577
1543, epoch_train_loss=0.001957350571734577
Epoch 1544
Epoch 1544 :: Batch 0/1
Batch Loss = 0.0019570715385622962
1544, epoch_train_loss=0.0019570715385622962
Epoch 1545
Epoch 1545 :: Batch 0/1
Batch Loss = 0.001956793268856948
1545, epoch_train_loss=0.001956793268856948
Epoch 1546
Epoch 1546 :: Batch 0/1
Batch Loss = 0.0019565157503782108
1546, epoch_train_loss=0.0019565157503782108
Epoch 1547
Epoch 1547 :: Batch 0/1
Batch Loss = 0.0019562389717316616
1547, epoch_train_loss=0.0019562389717316616
Epoch 1548
Epoch 1548 :: Batch 0/1
Batch Loss = 0.0019559629194479053
1548, epoch_train_loss=0.0019559629194479053
Epoch 1549
Epoch 1549 :: Batch 0/1
Batch Loss = 0.001955687584007821
1549, epoch_train_loss=0.001955687584007821
Epoch 1550
Epoch 1550 :: Batch 0/1
Batch Loss = 0.001955412954671863
1550, epoch_train_loss=0.001955412954671863
Epoch 1551
Epoch 1551 :: Batch 0/1
Batch Loss = 0.0019551390205798523
1551, epoch_train_loss=0.0019551390205798523
Epoch 1552
Epoch 1552 :: Batch 0/1
Batch Loss = 0.001954865772536858
1552, epoch_train_loss=0.001954865772536858
Epoch 1553
Epoch 1553 :: Batch 0/1
Batch Loss = 0.0019545932010634464
1553, epoch_train_loss=0.0019545932010634464
Epoch 1554
Epoch 1554 :: Batch 0/1
Batch Loss = 0.001954321297588446
1554, epoch_train_loss=0.001954321297588446
Epoch 1555
Epoch 1555 :: Batch 0/1
Batch Loss = 0.001954050053613019
1555, epoch_train_loss=0.001954050053613019
Epoch 1556
Epoch 1556 :: Batch 0/1
Batch Loss = 0.001953779461196136
1556, epoch_train_loss=0.001953779461196136
Epoch 1557
Epoch 1557 :: Batch 0/1
Batch Loss = 0.0019535095122276457
1557, epoch_train_loss=0.0019535095122276457
Epoch 1558
Epoch 1558 :: Batch 0/1
Batch Loss = 0.001953240200497435
1558, epoch_train_loss=0.001953240200497435
Epoch 1559
Epoch 1559 :: Batch 0/1
Batch Loss = 0.0019529715183213543
1559, epoch_train_loss=0.0019529715183213543
Epoch 1560
Epoch 1560 :: Batch 0/1
Batch Loss = 0.0019527034603054624
1560, epoch_train_loss=0.0019527034603054624
Epoch 1561
Epoch 1561 :: Batch 0/1
Batch Loss = 0.0019524360203973913
1561, epoch_train_loss=0.0019524360203973913
Epoch 1562
Epoch 1562 :: Batch 0/1
Batch Loss = 0.0019521691953887695
1562, epoch_train_loss=0.0019521691953887695
Epoch 1563
Epoch 1563 :: Batch 0/1
Batch Loss = 0.0019519029810988336
1563, epoch_train_loss=0.0019519029810988336
Epoch 1564
Epoch 1564 :: Batch 0/1
Batch Loss = 0.0019516373778915143
1564, epoch_train_loss=0.0019516373778915143
Epoch 1565
Epoch 1565 :: Batch 0/1
Batch Loss = 0.0019513723859512484
1565, epoch_train_loss=0.0019513723859512484
Epoch 1566
Epoch 1566 :: Batch 0/1
Batch Loss = 0.0019511080131822814
1566, epoch_train_loss=0.0019511080131822814
Epoch 1567
Epoch 1567 :: Batch 0/1
Batch Loss = 0.0019508442700210495
1567, epoch_train_loss=0.0019508442700210495
Epoch 1568
Epoch 1568 :: Batch 0/1
Batch Loss = 0.001950581181344277
1568, epoch_train_loss=0.001950581181344277
Epoch 1569
Epoch 1569 :: Batch 0/1
Batch Loss = 0.0019503187818396296
1569, epoch_train_loss=0.0019503187818396296
Epoch 1570
Epoch 1570 :: Batch 0/1
Batch Loss = 0.0019500571377049458
1570, epoch_train_loss=0.0019500571377049458
Epoch 1571
Epoch 1571 :: Batch 0/1
Batch Loss = 0.0019497963452591175
1571, epoch_train_loss=0.0019497963452591175
Epoch 1572
Epoch 1572 :: Batch 0/1
Batch Loss = 0.0019495365740092115
1572, epoch_train_loss=0.0019495365740092115
Epoch 1573
Epoch 1573 :: Batch 0/1
Batch Loss = 0.0019492780812882657
1573, epoch_train_loss=0.0019492780812882657
Epoch 1574
Epoch 1574 :: Batch 0/1
Batch Loss = 0.001949021307450979
1574, epoch_train_loss=0.001949021307450979
Epoch 1575
Epoch 1575 :: Batch 0/1
Batch Loss = 0.0019487669420231463
1575, epoch_train_loss=0.0019487669420231463
Epoch 1576
Epoch 1576 :: Batch 0/1
Batch Loss = 0.0019485161613165948
1576, epoch_train_loss=0.0019485161613165948
Epoch 1577
Epoch 1577 :: Batch 0/1
Batch Loss = 0.0019482708500032841
1577, epoch_train_loss=0.0019482708500032841
Epoch 1578
Epoch 1578 :: Batch 0/1
Batch Loss = 0.0019480342511731875
1578, epoch_train_loss=0.0019480342511731875
Epoch 1579
Epoch 1579 :: Batch 0/1
Batch Loss = 0.001947811648482521
1579, epoch_train_loss=0.001947811648482521
Epoch 1580
Epoch 1580 :: Batch 0/1
Batch Loss = 0.0019476122828511692
1580, epoch_train_loss=0.0019476122828511692
Epoch 1581
Epoch 1581 :: Batch 0/1
Batch Loss = 0.0019474513678726828
1581, epoch_train_loss=0.0019474513678726828
Epoch 1582
Epoch 1582 :: Batch 0/1
Batch Loss = 0.0019473561322841506
1582, epoch_train_loss=0.0019473561322841506
Epoch 1583
Epoch 1583 :: Batch 0/1
Batch Loss = 0.0019473715441527205
1583, epoch_train_loss=0.0019473715441527205
Epoch 1584
Epoch 1584 :: Batch 0/1
Batch Loss = 0.001947580622968901
1584, epoch_train_loss=0.001947580622968901
Epoch 1585
Epoch 1585 :: Batch 0/1
Batch Loss = 0.0019481195336794417
1585, epoch_train_loss=0.0019481195336794417
Epoch 1586
Epoch 1586 :: Batch 0/1
Batch Loss = 0.0019492505960775905
1586, epoch_train_loss=0.0019492505960775905
Epoch 1587
Epoch 1587 :: Batch 0/1
Batch Loss = 0.0019513945083105488
1587, epoch_train_loss=0.0019513945083105488
Epoch 1588
Epoch 1588 :: Batch 0/1
Batch Loss = 0.0019554127258044306
1588, epoch_train_loss=0.0019554127258044306
Epoch 1589
Epoch 1589 :: Batch 0/1
Batch Loss = 0.001962621463543491
1589, epoch_train_loss=0.001962621463543491
Epoch 1590
Epoch 1590 :: Batch 0/1
Batch Loss = 0.001975975253503541
1590, epoch_train_loss=0.001975975253503541
Epoch 1591
Epoch 1591 :: Batch 0/1
Batch Loss = 0.001999580608007552
1591, epoch_train_loss=0.001999580608007552
Epoch 1592
Epoch 1592 :: Batch 0/1
Batch Loss = 0.0020440968265342517
1592, epoch_train_loss=0.0020440968265342517
Epoch 1593
Epoch 1593 :: Batch 0/1
Batch Loss = 0.002121854018421634
1593, epoch_train_loss=0.002121854018421634
Epoch 1594
Epoch 1594 :: Batch 0/1
Batch Loss = 0.00227364849730281
1594, epoch_train_loss=0.00227364849730281
Epoch 1595
Epoch 1595 :: Batch 0/1
Batch Loss = 0.002531510840924978
1595, epoch_train_loss=0.002531510840924978
Epoch 1596
Epoch 1596 :: Batch 0/1
Batch Loss = 0.003061756172857006
1596, epoch_train_loss=0.003061756172857006
Epoch 1597
Epoch 1597 :: Batch 0/1
Batch Loss = 0.0039016136333574598
1597, epoch_train_loss=0.0039016136333574598
Epoch 1598
Epoch 1598 :: Batch 0/1
Batch Loss = 0.005760841089527918
1598, epoch_train_loss=0.005760841089527918
Epoch 1599
Epoch 1599 :: Batch 0/1
Batch Loss = 0.00822340882106714
1599, epoch_train_loss=0.00822340882106714
Epoch 1600
Epoch 1600 :: Batch 0/1
Batch Loss = 0.014218773681852323
1600, epoch_train_loss=0.014218773681852323
Epoch 1601
Epoch 1601 :: Batch 0/1
Batch Loss = 0.018852270414475972
1601, epoch_train_loss=0.018852270414475972
Epoch 1602
Epoch 1602 :: Batch 0/1
Batch Loss = 0.031753136625788096
1602, epoch_train_loss=0.031753136625788096
Epoch 1603
Epoch 1603 :: Batch 0/1
Batch Loss = 0.028663227376398308
1603, epoch_train_loss=0.028663227376398308
Epoch 1604
Epoch 1604 :: Batch 0/1
Batch Loss = 0.031499116592621515
1604, epoch_train_loss=0.031499116592621515
Epoch 1605
Epoch 1605 :: Batch 0/1
Batch Loss = 0.014099537902285362
1605, epoch_train_loss=0.014099537902285362
Epoch 1606
Epoch 1606 :: Batch 0/1
Batch Loss = 0.00404548496714823
1606, epoch_train_loss=0.00404548496714823
Epoch 1607
Epoch 1607 :: Batch 0/1
Batch Loss = 0.0028729767408873005
1607, epoch_train_loss=0.0028729767408873005
Epoch 1608
Epoch 1608 :: Batch 0/1
Batch Loss = 0.009228139585014119
1608, epoch_train_loss=0.009228139585014119
Epoch 1609
Epoch 1609 :: Batch 0/1
Batch Loss = 0.017038512956359858
1609, epoch_train_loss=0.017038512956359858
Epoch 1610
Epoch 1610 :: Batch 0/1
Batch Loss = 0.012233975917518994
1610, epoch_train_loss=0.012233975917518994
Epoch 1611
Epoch 1611 :: Batch 0/1
Batch Loss = 0.006040794953839919
1611, epoch_train_loss=0.006040794953839919
Epoch 1612
Epoch 1612 :: Batch 0/1
Batch Loss = 0.0021876019403279726
1612, epoch_train_loss=0.0021876019403279726
Epoch 1613
Epoch 1613 :: Batch 0/1
Batch Loss = 0.005185886262452933
1613, epoch_train_loss=0.005185886262452933
Epoch 1614
Epoch 1614 :: Batch 0/1
Batch Loss = 0.009884822494753831
1614, epoch_train_loss=0.009884822494753831
Epoch 1615
Epoch 1615 :: Batch 0/1
Batch Loss = 0.0074793413004776614
1615, epoch_train_loss=0.0074793413004776614
Epoch 1616
Epoch 1616 :: Batch 0/1
Batch Loss = 0.0035996202734004173
1616, epoch_train_loss=0.0035996202734004173
Epoch 1617
Epoch 1617 :: Batch 0/1
Batch Loss = 0.002237874375593751
1617, epoch_train_loss=0.002237874375593751
Epoch 1618
Epoch 1618 :: Batch 0/1
Batch Loss = 0.004644535389695514
1618, epoch_train_loss=0.004644535389695514
Epoch 1619
Epoch 1619 :: Batch 0/1
Batch Loss = 0.006788252615282211
1619, epoch_train_loss=0.006788252615282211
Epoch 1620
Epoch 1620 :: Batch 0/1
Batch Loss = 0.004437591464949528
1620, epoch_train_loss=0.004437591464949528
Epoch 1621
Epoch 1621 :: Batch 0/1
Batch Loss = 0.0022206320846421836
1621, epoch_train_loss=0.0022206320846421836
Epoch 1622
Epoch 1622 :: Batch 0/1
Batch Loss = 0.0028224553561660847
1622, epoch_train_loss=0.0028224553561660847
Epoch 1623
Epoch 1623 :: Batch 0/1
Batch Loss = 0.004369015592279394
1623, epoch_train_loss=0.004369015592279394
Epoch 1624
Epoch 1624 :: Batch 0/1
Batch Loss = 0.004233983671266505
1624, epoch_train_loss=0.004233983671266505
Epoch 1625
Epoch 1625 :: Batch 0/1
Batch Loss = 0.0024809562760084343
1625, epoch_train_loss=0.0024809562760084343
Epoch 1626
Epoch 1626 :: Batch 0/1
Batch Loss = 0.0022598646164718153
1626, epoch_train_loss=0.0022598646164718153
Epoch 1627
Epoch 1627 :: Batch 0/1
Batch Loss = 0.0034093603432926826
1627, epoch_train_loss=0.0034093603432926826
Epoch 1628
Epoch 1628 :: Batch 0/1
Batch Loss = 0.0035354743814214992
1628, epoch_train_loss=0.0035354743814214992
Epoch 1629
Epoch 1629 :: Batch 0/1
Batch Loss = 0.0026183782409900744
1629, epoch_train_loss=0.0026183782409900744
Epoch 1630
Epoch 1630 :: Batch 0/1
Batch Loss = 0.0020857570523884223
1630, epoch_train_loss=0.0020857570523884223
Epoch 1631
Epoch 1631 :: Batch 0/1
Batch Loss = 0.0026486235440637417
1631, epoch_train_loss=0.0026486235440637417
Epoch 1632
Epoch 1632 :: Batch 0/1
Batch Loss = 0.0031377921506886924
1632, epoch_train_loss=0.0031377921506886924
Epoch 1633
Epoch 1633 :: Batch 0/1
Batch Loss = 0.0026033150479741396
1633, epoch_train_loss=0.0026033150479741396
Epoch 1634
Epoch 1634 :: Batch 0/1
Batch Loss = 0.002069804324086922
1634, epoch_train_loss=0.002069804324086922
Epoch 1635
Epoch 1635 :: Batch 0/1
Batch Loss = 0.002255952499670361
1635, epoch_train_loss=0.002255952499670361
Epoch 1636
Epoch 1636 :: Batch 0/1
Batch Loss = 0.002646891438065895
1636, epoch_train_loss=0.002646891438065895
Epoch 1637
Epoch 1637 :: Batch 0/1
Batch Loss = 0.0025663636110219296
1637, epoch_train_loss=0.0025663636110219296
Epoch 1638
Epoch 1638 :: Batch 0/1
Batch Loss = 0.002133787020848942
1638, epoch_train_loss=0.002133787020848942
Epoch 1639
Epoch 1639 :: Batch 0/1
Batch Loss = 0.0020548649355292756
1639, epoch_train_loss=0.0020548649355292756
Epoch 1640
Epoch 1640 :: Batch 0/1
Batch Loss = 0.002328968985276247
1640, epoch_train_loss=0.002328968985276247
Epoch 1641
Epoch 1641 :: Batch 0/1
Batch Loss = 0.0024148877528075395
1641, epoch_train_loss=0.0024148877528075395
Epoch 1642
Epoch 1642 :: Batch 0/1
Batch Loss = 0.002198718361272672
1642, epoch_train_loss=0.002198718361272672
Epoch 1643
Epoch 1643 :: Batch 0/1
Batch Loss = 0.0020030440219248823
1643, epoch_train_loss=0.0020030440219248823
Epoch 1644
Epoch 1644 :: Batch 0/1
Batch Loss = 0.0020900804008666864
1644, epoch_train_loss=0.0020900804008666864
Epoch 1645
Epoch 1645 :: Batch 0/1
Batch Loss = 0.002257660576699576
1645, epoch_train_loss=0.002257660576699576
Epoch 1646
Epoch 1646 :: Batch 0/1
Batch Loss = 0.002210310645067702
1646, epoch_train_loss=0.002210310645067702
Epoch 1647
Epoch 1647 :: Batch 0/1
Batch Loss = 0.002042360435229295
1647, epoch_train_loss=0.002042360435229295
Epoch 1648
Epoch 1648 :: Batch 0/1
Batch Loss = 0.001984423180136806
1648, epoch_train_loss=0.001984423180136806
Epoch 1649
Epoch 1649 :: Batch 0/1
Batch Loss = 0.0020780194148305735
1649, epoch_train_loss=0.0020780194148305735
Epoch 1650
Epoch 1650 :: Batch 0/1
Batch Loss = 0.00215001264061826
1650, epoch_train_loss=0.00215001264061826
Epoch 1651
Epoch 1651 :: Batch 0/1
Batch Loss = 0.0020851737360635695
1651, epoch_train_loss=0.0020851737360635695
Epoch 1652
Epoch 1652 :: Batch 0/1
Batch Loss = 0.0019869012623928077
1652, epoch_train_loss=0.0019869012623928077
Epoch 1653
Epoch 1653 :: Batch 0/1
Batch Loss = 0.0019772662379402063
1653, epoch_train_loss=0.0019772662379402063
Epoch 1654
Epoch 1654 :: Batch 0/1
Batch Loss = 0.00204124042608631
1654, epoch_train_loss=0.00204124042608631
Epoch 1655
Epoch 1655 :: Batch 0/1
Batch Loss = 0.0020721025735245487
1655, epoch_train_loss=0.0020721025735245487
Epoch 1656
Epoch 1656 :: Batch 0/1
Batch Loss = 0.002023259308349751
1656, epoch_train_loss=0.002023259308349751
Epoch 1657
Epoch 1657 :: Batch 0/1
Batch Loss = 0.0019675394286324467
1657, epoch_train_loss=0.0019675394286324467
Epoch 1658
Epoch 1658 :: Batch 0/1
Batch Loss = 0.00196679719072333
1658, epoch_train_loss=0.00196679719072333
Epoch 1659
Epoch 1659 :: Batch 0/1
Batch Loss = 0.002004246857203664
1659, epoch_train_loss=0.002004246857203664
Epoch 1660
Epoch 1660 :: Batch 0/1
Batch Loss = 0.0020214756345827895
1660, epoch_train_loss=0.0020214756345827895
Epoch 1661
Epoch 1661 :: Batch 0/1
Batch Loss = 0.001994262885817312
1661, epoch_train_loss=0.001994262885817312
Epoch 1662
Epoch 1662 :: Batch 0/1
Batch Loss = 0.0019595489895512398
1662, epoch_train_loss=0.0019595489895512398
Epoch 1663
Epoch 1663 :: Batch 0/1
Batch Loss = 0.001955008190523935
1663, epoch_train_loss=0.001955008190523935
Epoch 1664
Epoch 1664 :: Batch 0/1
Batch Loss = 0.001975852179879542
1664, epoch_train_loss=0.001975852179879542
Epoch 1665
Epoch 1665 :: Batch 0/1
Batch Loss = 0.0019896424511396784
1665, epoch_train_loss=0.0019896424511396784
Epoch 1666
Epoch 1666 :: Batch 0/1
Batch Loss = 0.001977820601008862
1666, epoch_train_loss=0.001977820601008862
Epoch 1667
Epoch 1667 :: Batch 0/1
Batch Loss = 0.0019560235032948974
1667, epoch_train_loss=0.0019560235032948974
Epoch 1668
Epoch 1668 :: Batch 0/1
Batch Loss = 0.0019471253530352656
1668, epoch_train_loss=0.0019471253530352656
Epoch 1669
Epoch 1669 :: Batch 0/1
Batch Loss = 0.0019563767587261176
1669, epoch_train_loss=0.0019563767587261176
Epoch 1670
Epoch 1670 :: Batch 0/1
Batch Loss = 0.0019677103684501767
1670, epoch_train_loss=0.0019677103684501767
Epoch 1671
Epoch 1671 :: Batch 0/1
Batch Loss = 0.0019657387660913044
1671, epoch_train_loss=0.0019657387660913044
Epoch 1672
Epoch 1672 :: Batch 0/1
Batch Loss = 0.001953543381651769
1672, epoch_train_loss=0.001953543381651769
Epoch 1673
Epoch 1673 :: Batch 0/1
Batch Loss = 0.0019438857590995371
1673, epoch_train_loss=0.0019438857590995371
Epoch 1674
Epoch 1674 :: Batch 0/1
Batch Loss = 0.0019447094478258228
1674, epoch_train_loss=0.0019447094478258228
Epoch 1675
Epoch 1675 :: Batch 0/1
Batch Loss = 0.0019517188709350275
1675, epoch_train_loss=0.0019517188709350275
Epoch 1676
Epoch 1676 :: Batch 0/1
Batch Loss = 0.00195487143577356
1676, epoch_train_loss=0.00195487143577356
Epoch 1677
Epoch 1677 :: Batch 0/1
Batch Loss = 0.0019504759874994062
1677, epoch_train_loss=0.0019504759874994062
Epoch 1678
Epoch 1678 :: Batch 0/1
Batch Loss = 0.0019430941366080244
1678, epoch_train_loss=0.0019430941366080244
Epoch 1679
Epoch 1679 :: Batch 0/1
Batch Loss = 0.0019394941064681329
1679, epoch_train_loss=0.0019394941064681329
Epoch 1680
Epoch 1680 :: Batch 0/1
Batch Loss = 0.0019413029935665227
1680, epoch_train_loss=0.0019413029935665227
Epoch 1681
Epoch 1681 :: Batch 0/1
Batch Loss = 0.00194478247240264
1681, epoch_train_loss=0.00194478247240264
Epoch 1682
Epoch 1682 :: Batch 0/1
Batch Loss = 0.0019453400231102635
1682, epoch_train_loss=0.0019453400231102635
Epoch 1683
Epoch 1683 :: Batch 0/1
Batch Loss = 0.001942031780076188
1683, epoch_train_loss=0.001942031780076188
Epoch 1684
Epoch 1684 :: Batch 0/1
Batch Loss = 0.0019380167420149337
1684, epoch_train_loss=0.0019380167420149337
Epoch 1685
Epoch 1685 :: Batch 0/1
Batch Loss = 0.0019363452511544376
1685, epoch_train_loss=0.0019363452511544376
Epoch 1686
Epoch 1686 :: Batch 0/1
Batch Loss = 0.001937375189764446
1686, epoch_train_loss=0.001937375189764446
Epoch 1687
Epoch 1687 :: Batch 0/1
Batch Loss = 0.0019390428791281466
1687, epoch_train_loss=0.0019390428791281466
Epoch 1688
Epoch 1688 :: Batch 0/1
Batch Loss = 0.0019390613436179342
1688, epoch_train_loss=0.0019390613436179342
Epoch 1689
Epoch 1689 :: Batch 0/1
Batch Loss = 0.0019371546082509305
1689, epoch_train_loss=0.0019371546082509305
Epoch 1690
Epoch 1690 :: Batch 0/1
Batch Loss = 0.0019348087652621688
1690, epoch_train_loss=0.0019348087652621688
Epoch 1691
Epoch 1691 :: Batch 0/1
Batch Loss = 0.0019336015327643748
1691, epoch_train_loss=0.0019336015327643748
Epoch 1692
Epoch 1692 :: Batch 0/1
Batch Loss = 0.0019338559674368936
1692, epoch_train_loss=0.0019338559674368936
Epoch 1693
Epoch 1693 :: Batch 0/1
Batch Loss = 0.0019346332742276374
1693, epoch_train_loss=0.0019346332742276374
Epoch 1694
Epoch 1694 :: Batch 0/1
Batch Loss = 0.0019347183951062667
1694, epoch_train_loss=0.0019347183951062667
Epoch 1695
Epoch 1695 :: Batch 0/1
Batch Loss = 0.0019337229924029538
1695, epoch_train_loss=0.0019337229924029538
Epoch 1696
Epoch 1696 :: Batch 0/1
Batch Loss = 0.0019323019668949276
1696, epoch_train_loss=0.0019323019668949276
Epoch 1697
Epoch 1697 :: Batch 0/1
Batch Loss = 0.0019312952803975018
1697, epoch_train_loss=0.0019312952803975018
Epoch 1698
Epoch 1698 :: Batch 0/1
Batch Loss = 0.0019310711883887752
1698, epoch_train_loss=0.0019310711883887752
Epoch 1699
Epoch 1699 :: Batch 0/1
Batch Loss = 0.0019313106483268227
1699, epoch_train_loss=0.0019313106483268227
Epoch 1700
Epoch 1700 :: Batch 0/1
Batch Loss = 0.0019313878404243101
1700, epoch_train_loss=0.0019313878404243101
Epoch 1701
Epoch 1701 :: Batch 0/1
Batch Loss = 0.0019309664859856373
1701, epoch_train_loss=0.0019309664859856373
Epoch 1702
Epoch 1702 :: Batch 0/1
Batch Loss = 0.0019301691340283067
1702, epoch_train_loss=0.0019301691340283067
Epoch 1703
Epoch 1703 :: Batch 0/1
Batch Loss = 0.0019293659396237097
1703, epoch_train_loss=0.0019293659396237097
Epoch 1704
Epoch 1704 :: Batch 0/1
Batch Loss = 0.001928873073858378
1704, epoch_train_loss=0.001928873073858378
Epoch 1705
Epoch 1705 :: Batch 0/1
Batch Loss = 0.0019287292383119796
1705, epoch_train_loss=0.0019287292383119796
Epoch 1706
Epoch 1706 :: Batch 0/1
Batch Loss = 0.0019287101784109108
1706, epoch_train_loss=0.0019287101784109108
Epoch 1707
Epoch 1707 :: Batch 0/1
Batch Loss = 0.0019285566624601483
1707, epoch_train_loss=0.0019285566624601483
Epoch 1708
Epoch 1708 :: Batch 0/1
Batch Loss = 0.0019281666208910073
1708, epoch_train_loss=0.0019281666208910073
Epoch 1709
Epoch 1709 :: Batch 0/1
Batch Loss = 0.0019276165387343016
1709, epoch_train_loss=0.0019276165387343016
Epoch 1710
Epoch 1710 :: Batch 0/1
Batch Loss = 0.0019270978947354056
1710, epoch_train_loss=0.0019270978947354056
Epoch 1711
Epoch 1711 :: Batch 0/1
Batch Loss = 0.0019267399501411001
1711, epoch_train_loss=0.0019267399501411001
Epoch 1712
Epoch 1712 :: Batch 0/1
Batch Loss = 0.001926538413763968
1712, epoch_train_loss=0.001926538413763968
Epoch 1713
Epoch 1713 :: Batch 0/1
Batch Loss = 0.0019263961305182801
1713, epoch_train_loss=0.0019263961305182801
Epoch 1714
Epoch 1714 :: Batch 0/1
Batch Loss = 0.0019261992893949997
1714, epoch_train_loss=0.0019261992893949997
Epoch 1715
Epoch 1715 :: Batch 0/1
Batch Loss = 0.0019258909592480115
1715, epoch_train_loss=0.0019258909592480115
Epoch 1716
Epoch 1716 :: Batch 0/1
Batch Loss = 0.001925504997611336
1716, epoch_train_loss=0.001925504997611336
Epoch 1717
Epoch 1717 :: Batch 0/1
Batch Loss = 0.0019251189937639554
1717, epoch_train_loss=0.0019251189937639554
Epoch 1718
Epoch 1718 :: Batch 0/1
Batch Loss = 0.0019247955413473814
1718, epoch_train_loss=0.0019247955413473814
Epoch 1719
Epoch 1719 :: Batch 0/1
Batch Loss = 0.0019245504864595102
1719, epoch_train_loss=0.0019245504864595102
Epoch 1720
Epoch 1720 :: Batch 0/1
Batch Loss = 0.0019243500489478599
1720, epoch_train_loss=0.0019243500489478599
Epoch 1721
Epoch 1721 :: Batch 0/1
Batch Loss = 0.0019241429891223847
1721, epoch_train_loss=0.0019241429891223847
Epoch 1722
Epoch 1722 :: Batch 0/1
Batch Loss = 0.0019238992083578709
1722, epoch_train_loss=0.0019238992083578709
Epoch 1723
Epoch 1723 :: Batch 0/1
Batch Loss = 0.001923613490366804
1723, epoch_train_loss=0.001923613490366804
Epoch 1724
Epoch 1724 :: Batch 0/1
Batch Loss = 0.0019233073122936104
1724, epoch_train_loss=0.0019233073122936104
Epoch 1725
Epoch 1725 :: Batch 0/1
Batch Loss = 0.001923011449012084
1725, epoch_train_loss=0.001923011449012084
Epoch 1726
Epoch 1726 :: Batch 0/1
Batch Loss = 0.0019227455897410592
1726, epoch_train_loss=0.0019227455897410592
Epoch 1727
Epoch 1727 :: Batch 0/1
Batch Loss = 0.0019225106458658314
1727, epoch_train_loss=0.0019225106458658314
Epoch 1728
Epoch 1728 :: Batch 0/1
Batch Loss = 0.0019222940331581294
1728, epoch_train_loss=0.0019222940331581294
Epoch 1729
Epoch 1729 :: Batch 0/1
Batch Loss = 0.001922076379647303
1729, epoch_train_loss=0.001922076379647303
Epoch 1730
Epoch 1730 :: Batch 0/1
Batch Loss = 0.0019218450562438448
1730, epoch_train_loss=0.0019218450562438448
Epoch 1731
Epoch 1731 :: Batch 0/1
Batch Loss = 0.001921597913860585
1731, epoch_train_loss=0.001921597913860585
Epoch 1732
Epoch 1732 :: Batch 0/1
Batch Loss = 0.0019213407723008068
1732, epoch_train_loss=0.0019213407723008068
Epoch 1733
Epoch 1733 :: Batch 0/1
Batch Loss = 0.0019210848027942729
1733, epoch_train_loss=0.0019210848027942729
Epoch 1734
Epoch 1734 :: Batch 0/1
Batch Loss = 0.0019208398491508886
1734, epoch_train_loss=0.0019208398491508886
Epoch 1735
Epoch 1735 :: Batch 0/1
Batch Loss = 0.0019206085824174835
1735, epoch_train_loss=0.0019206085824174835
Epoch 1736
Epoch 1736 :: Batch 0/1
Batch Loss = 0.0019203885169209887
1736, epoch_train_loss=0.0019203885169209887
Epoch 1737
Epoch 1737 :: Batch 0/1
Batch Loss = 0.001920173509184059
1737, epoch_train_loss=0.001920173509184059
Epoch 1738
Epoch 1738 :: Batch 0/1
Batch Loss = 0.0019199572375272183
1738, epoch_train_loss=0.0019199572375272183
Epoch 1739
Epoch 1739 :: Batch 0/1
Batch Loss = 0.00191973625319413
1739, epoch_train_loss=0.00191973625319413
Epoch 1740
Epoch 1740 :: Batch 0/1
Batch Loss = 0.0019195104056728667
1740, epoch_train_loss=0.0019195104056728667
Epoch 1741
Epoch 1741 :: Batch 0/1
Batch Loss = 0.0019192818506800925
1741, epoch_train_loss=0.0019192818506800925
Epoch 1742
Epoch 1742 :: Batch 0/1
Batch Loss = 0.0019190546182804205
1742, epoch_train_loss=0.0019190546182804205
Epoch 1743
Epoch 1743 :: Batch 0/1
Batch Loss = 0.0019188315378914121
1743, epoch_train_loss=0.0019188315378914121
Epoch 1744
Epoch 1744 :: Batch 0/1
Batch Loss = 0.0019186138129835492
1744, epoch_train_loss=0.0019186138129835492
Epoch 1745
Epoch 1745 :: Batch 0/1
Batch Loss = 0.0019184012493275984
1745, epoch_train_loss=0.0019184012493275984
Epoch 1746
Epoch 1746 :: Batch 0/1
Batch Loss = 0.0019181923735173238
1746, epoch_train_loss=0.0019181923735173238
Epoch 1747
Epoch 1747 :: Batch 0/1
Batch Loss = 0.0019179852136113422
1747, epoch_train_loss=0.0019179852136113422
Epoch 1748
Epoch 1748 :: Batch 0/1
Batch Loss = 0.0019177783110940752
1748, epoch_train_loss=0.0019177783110940752
Epoch 1749
Epoch 1749 :: Batch 0/1
Batch Loss = 0.0019175707109974828
1749, epoch_train_loss=0.0019175707109974828
Epoch 1750
Epoch 1750 :: Batch 0/1
Batch Loss = 0.0019173624486051282
1750, epoch_train_loss=0.0019173624486051282
Epoch 1751
Epoch 1751 :: Batch 0/1
Batch Loss = 0.0019171542062086614
1751, epoch_train_loss=0.0019171542062086614
Epoch 1752
Epoch 1752 :: Batch 0/1
Batch Loss = 0.0019169467224898764
1752, epoch_train_loss=0.0019169467224898764
Epoch 1753
Epoch 1753 :: Batch 0/1
Batch Loss = 0.0019167408213279463
1753, epoch_train_loss=0.0019167408213279463
Epoch 1754
Epoch 1754 :: Batch 0/1
Batch Loss = 0.0019165371386088551
1754, epoch_train_loss=0.0019165371386088551
Epoch 1755
Epoch 1755 :: Batch 0/1
Batch Loss = 0.0019163357863656956
1755, epoch_train_loss=0.0019163357863656956
Epoch 1756
Epoch 1756 :: Batch 0/1
Batch Loss = 0.0019161366533168328
1756, epoch_train_loss=0.0019161366533168328
Epoch 1757
Epoch 1757 :: Batch 0/1
Batch Loss = 0.0019159393910306887
1757, epoch_train_loss=0.0019159393910306887
Epoch 1758
Epoch 1758 :: Batch 0/1
Batch Loss = 0.0019157435710726127
1758, epoch_train_loss=0.0019157435710726127
Epoch 1759
Epoch 1759 :: Batch 0/1
Batch Loss = 0.0019155488496885806
1759, epoch_train_loss=0.0019155488496885806
Epoch 1760
Epoch 1760 :: Batch 0/1
Batch Loss = 0.0019153549648323151
1760, epoch_train_loss=0.0019153549648323151
Epoch 1761
Epoch 1761 :: Batch 0/1
Batch Loss = 0.0019151617479995907
1761, epoch_train_loss=0.0019151617479995907
Epoch 1762
Epoch 1762 :: Batch 0/1
Batch Loss = 0.0019149692611978657
1762, epoch_train_loss=0.0019149692611978657
Epoch 1763
Epoch 1763 :: Batch 0/1
Batch Loss = 0.0019147775656114783
1763, epoch_train_loss=0.0019147775656114783
Epoch 1764
Epoch 1764 :: Batch 0/1
Batch Loss = 0.0019145867667488673
1764, epoch_train_loss=0.0019145867667488673
Epoch 1765
Epoch 1765 :: Batch 0/1
Batch Loss = 0.0019143969987998612
1765, epoch_train_loss=0.0019143969987998612
Epoch 1766
Epoch 1766 :: Batch 0/1
Batch Loss = 0.0019142083549258313
1766, epoch_train_loss=0.0019142083549258313
Epoch 1767
Epoch 1767 :: Batch 0/1
Batch Loss = 0.0019140208921517136
1767, epoch_train_loss=0.0019140208921517136
Epoch 1768
Epoch 1768 :: Batch 0/1
Batch Loss = 0.0019138346429157013
1768, epoch_train_loss=0.0019138346429157013
Epoch 1769
Epoch 1769 :: Batch 0/1
Batch Loss = 0.0019136495848972711
1769, epoch_train_loss=0.0019136495848972711
Epoch 1770
Epoch 1770 :: Batch 0/1
Batch Loss = 0.0019134657035830888
1770, epoch_train_loss=0.0019134657035830888
Epoch 1771
Epoch 1771 :: Batch 0/1
Batch Loss = 0.0019132829761271124
1771, epoch_train_loss=0.0019132829761271124
Epoch 1772
Epoch 1772 :: Batch 0/1
Batch Loss = 0.0019131013476048943
1772, epoch_train_loss=0.0019131013476048943
Epoch 1773
Epoch 1773 :: Batch 0/1
Batch Loss = 0.001912920792329476
1773, epoch_train_loss=0.001912920792329476
Epoch 1774
Epoch 1774 :: Batch 0/1
Batch Loss = 0.0019127412871421917
1774, epoch_train_loss=0.0019127412871421917
Epoch 1775
Epoch 1775 :: Batch 0/1
Batch Loss = 0.0019125628039320304
1775, epoch_train_loss=0.0019125628039320304
Epoch 1776
Epoch 1776 :: Batch 0/1
Batch Loss = 0.0019123853309777632
1776, epoch_train_loss=0.0019123853309777632
Epoch 1777
Epoch 1777 :: Batch 0/1
Batch Loss = 0.0019122088537128904
1777, epoch_train_loss=0.0019122088537128904
Epoch 1778
Epoch 1778 :: Batch 0/1
Batch Loss = 0.0019120333666431488
1778, epoch_train_loss=0.0019120333666431488
Epoch 1779
Epoch 1779 :: Batch 0/1
Batch Loss = 0.0019118588703039945
1779, epoch_train_loss=0.0019118588703039945
Epoch 1780
Epoch 1780 :: Batch 0/1
Batch Loss = 0.0019116853649812932
1780, epoch_train_loss=0.0019116853649812932
Epoch 1781
Epoch 1781 :: Batch 0/1
Batch Loss = 0.0019115128496290104
1781, epoch_train_loss=0.0019115128496290104
Epoch 1782
Epoch 1782 :: Batch 0/1
Batch Loss = 0.0019113413480205386
1782, epoch_train_loss=0.0019113413480205386
Epoch 1783
Epoch 1783 :: Batch 0/1
Batch Loss = 0.0019111708685643823
1783, epoch_train_loss=0.0019111708685643823
Epoch 1784
Epoch 1784 :: Batch 0/1
Batch Loss = 0.0019110014494811303
1784, epoch_train_loss=0.0019110014494811303
Epoch 1785
Epoch 1785 :: Batch 0/1
Batch Loss = 0.0019108331279731714
1785, epoch_train_loss=0.0019108331279731714
Epoch 1786
Epoch 1786 :: Batch 0/1
Batch Loss = 0.0019106659843432935
1786, epoch_train_loss=0.0019106659843432935
Epoch 1787
Epoch 1787 :: Batch 0/1
Batch Loss = 0.001910500112207855
1787, epoch_train_loss=0.001910500112207855
Epoch 1788
Epoch 1788 :: Batch 0/1
Batch Loss = 0.0019103356886142685
1788, epoch_train_loss=0.0019103356886142685
Epoch 1789
Epoch 1789 :: Batch 0/1
Batch Loss = 0.001910172939998648
1789, epoch_train_loss=0.001910172939998648
Epoch 1790
Epoch 1790 :: Batch 0/1
Batch Loss = 0.0019100122751463484
1790, epoch_train_loss=0.0019100122751463484
Epoch 1791
Epoch 1791 :: Batch 0/1
Batch Loss = 0.0019098542653820884
1791, epoch_train_loss=0.0019098542653820884
Epoch 1792
Epoch 1792 :: Batch 0/1
Batch Loss = 0.0019096998902163743
1792, epoch_train_loss=0.0019096998902163743
Epoch 1793
Epoch 1793 :: Batch 0/1
Batch Loss = 0.0019095506175686083
1793, epoch_train_loss=0.0019095506175686083
Epoch 1794
Epoch 1794 :: Batch 0/1
Batch Loss = 0.0019094089473586122
1794, epoch_train_loss=0.0019094089473586122
Epoch 1795
Epoch 1795 :: Batch 0/1
Batch Loss = 0.0019092787831124676
1795, epoch_train_loss=0.0019092787831124676
Epoch 1796
Epoch 1796 :: Batch 0/1
Batch Loss = 0.00190916683887636
1796, epoch_train_loss=0.00190916683887636
Epoch 1797
Epoch 1797 :: Batch 0/1
Batch Loss = 0.001909083931239855
1797, epoch_train_loss=0.001909083931239855
Epoch 1798
Epoch 1798 :: Batch 0/1
Batch Loss = 0.0019090490407549312
1798, epoch_train_loss=0.0019090490407549312
Epoch 1799
Epoch 1799 :: Batch 0/1
Batch Loss = 0.0019090934178774802
1799, epoch_train_loss=0.0019090934178774802
Epoch 1800
Epoch 1800 :: Batch 0/1
Batch Loss = 0.0019092734891669957
1800, epoch_train_loss=0.0019092734891669957
Epoch 1801
Epoch 1801 :: Batch 0/1
Batch Loss = 0.0019096832220239339
1801, epoch_train_loss=0.0019096832220239339
Epoch 1802
Epoch 1802 :: Batch 0/1
Batch Loss = 0.0019104990120940258
1802, epoch_train_loss=0.0019104990120940258
Epoch 1803
Epoch 1803 :: Batch 0/1
Batch Loss = 0.0019120135914146672
1803, epoch_train_loss=0.0019120135914146672
Epoch 1804
Epoch 1804 :: Batch 0/1
Batch Loss = 0.0019148054473885658
1804, epoch_train_loss=0.0019148054473885658
Epoch 1805
Epoch 1805 :: Batch 0/1
Batch Loss = 0.0019198069026375585
1805, epoch_train_loss=0.0019198069026375585
Epoch 1806
Epoch 1806 :: Batch 0/1
Batch Loss = 0.0019289957970747974
1806, epoch_train_loss=0.0019289957970747974
Epoch 1807
Epoch 1807 :: Batch 0/1
Batch Loss = 0.001945303726240871
1807, epoch_train_loss=0.001945303726240871
Epoch 1808
Epoch 1808 :: Batch 0/1
Batch Loss = 0.001975599957882795
1808, epoch_train_loss=0.001975599957882795
Epoch 1809
Epoch 1809 :: Batch 0/1
Batch Loss = 0.0020279761047283617
1809, epoch_train_loss=0.0020279761047283617
Epoch 1810
Epoch 1810 :: Batch 0/1
Batch Loss = 0.0021244440094155
1810, epoch_train_loss=0.0021244440094155
Epoch 1811
Epoch 1811 :: Batch 0/1
Batch Loss = 0.0022751479101232315
1811, epoch_train_loss=0.0022751479101232315
Epoch 1812
Epoch 1812 :: Batch 0/1
Batch Loss = 0.0025244831126542
1812, epoch_train_loss=0.0025244831126542
Epoch 1813
Epoch 1813 :: Batch 0/1
Batch Loss = 0.0028095058185612734
1813, epoch_train_loss=0.0028095058185612734
Epoch 1814
Epoch 1814 :: Batch 0/1
Batch Loss = 0.003115190214609123
1814, epoch_train_loss=0.003115190214609123
Epoch 1815
Epoch 1815 :: Batch 0/1
Batch Loss = 0.003358326596439367
1815, epoch_train_loss=0.003358326596439367
Epoch 1816
Epoch 1816 :: Batch 0/1
Batch Loss = 0.00365994374094289
1816, epoch_train_loss=0.00365994374094289
Epoch 1817
Epoch 1817 :: Batch 0/1
Batch Loss = 0.004740170531016581
1817, epoch_train_loss=0.004740170531016581
Epoch 1818
Epoch 1818 :: Batch 0/1
Batch Loss = 0.006215799125164456
1818, epoch_train_loss=0.006215799125164456
Epoch 1819
Epoch 1819 :: Batch 0/1
Batch Loss = 0.009338688914926828
1819, epoch_train_loss=0.009338688914926828
Epoch 1820
Epoch 1820 :: Batch 0/1
Batch Loss = 0.011322842370731466
1820, epoch_train_loss=0.011322842370731466
Epoch 1821
Epoch 1821 :: Batch 0/1
Batch Loss = 0.016717862366982028
1821, epoch_train_loss=0.016717862366982028
Epoch 1822
Epoch 1822 :: Batch 0/1
Batch Loss = 0.016972220763100305
1822, epoch_train_loss=0.016972220763100305
Epoch 1823
Epoch 1823 :: Batch 0/1
Batch Loss = 0.02092856219296423
1823, epoch_train_loss=0.02092856219296423
Epoch 1824
Epoch 1824 :: Batch 0/1
Batch Loss = 0.014674017354029262
1824, epoch_train_loss=0.014674017354029262
Epoch 1825
Epoch 1825 :: Batch 0/1
Batch Loss = 0.010481310416995319
1825, epoch_train_loss=0.010481310416995319
Epoch 1826
Epoch 1826 :: Batch 0/1
Batch Loss = 0.004421111653502861
1826, epoch_train_loss=0.004421111653502861
Epoch 1827
Epoch 1827 :: Batch 0/1
Batch Loss = 0.002156943238837478
1827, epoch_train_loss=0.002156943238837478
Epoch 1828
Epoch 1828 :: Batch 0/1
Batch Loss = 0.003403582517136995
1828, epoch_train_loss=0.003403582517136995
Epoch 1829
Epoch 1829 :: Batch 0/1
Batch Loss = 0.006136539614000003
1829, epoch_train_loss=0.006136539614000003
Epoch 1830
Epoch 1830 :: Batch 0/1
Batch Loss = 0.008993923120525708
1830, epoch_train_loss=0.008993923120525708
Epoch 1831
Epoch 1831 :: Batch 0/1
Batch Loss = 0.007414799383883241
1831, epoch_train_loss=0.007414799383883241
Epoch 1832
Epoch 1832 :: Batch 0/1
Batch Loss = 0.005015472098062624
1832, epoch_train_loss=0.005015472098062624
Epoch 1833
Epoch 1833 :: Batch 0/1
Batch Loss = 0.0024189421037100482
1833, epoch_train_loss=0.0024189421037100482
Epoch 1834
Epoch 1834 :: Batch 0/1
Batch Loss = 0.002262835422864594
1834, epoch_train_loss=0.002262835422864594
Epoch 1835
Epoch 1835 :: Batch 0/1
Batch Loss = 0.003895168223251995
1835, epoch_train_loss=0.003895168223251995
Epoch 1836
Epoch 1836 :: Batch 0/1
Batch Loss = 0.00505812086174719
1836, epoch_train_loss=0.00505812086174719
Epoch 1837
Epoch 1837 :: Batch 0/1
Batch Loss = 0.005204297268971824
1837, epoch_train_loss=0.005204297268971824
Epoch 1838
Epoch 1838 :: Batch 0/1
Batch Loss = 0.003515944453650269
1838, epoch_train_loss=0.003515944453650269
Epoch 1839
Epoch 1839 :: Batch 0/1
Batch Loss = 0.002193700295020581
1839, epoch_train_loss=0.002193700295020581
Epoch 1840
Epoch 1840 :: Batch 0/1
Batch Loss = 0.002156028384578064
1840, epoch_train_loss=0.002156028384578064
Epoch 1841
Epoch 1841 :: Batch 0/1
Batch Loss = 0.0030570947336000963
1841, epoch_train_loss=0.0030570947336000963
Epoch 1842
Epoch 1842 :: Batch 0/1
Batch Loss = 0.0038035952301520453
1842, epoch_train_loss=0.0038035952301520453
Epoch 1843
Epoch 1843 :: Batch 0/1
Batch Loss = 0.0032812257984700466
1843, epoch_train_loss=0.0032812257984700466
Epoch 1844
Epoch 1844 :: Batch 0/1
Batch Loss = 0.0024119970607644424
1844, epoch_train_loss=0.0024119970607644424
Epoch 1845
Epoch 1845 :: Batch 0/1
Batch Loss = 0.0019769239507082784
1845, epoch_train_loss=0.0019769239507082784
Epoch 1846
Epoch 1846 :: Batch 0/1
Batch Loss = 0.0023208461035964605
1846, epoch_train_loss=0.0023208461035964605
Epoch 1847
Epoch 1847 :: Batch 0/1
Batch Loss = 0.002851864154570415
1847, epoch_train_loss=0.002851864154570415
Epoch 1848
Epoch 1848 :: Batch 0/1
Batch Loss = 0.0028671945169598738
1848, epoch_train_loss=0.0028671945169598738
Epoch 1849
Epoch 1849 :: Batch 0/1
Batch Loss = 0.0024914803905425682
1849, epoch_train_loss=0.0024914803905425682
Epoch 1850
Epoch 1850 :: Batch 0/1
Batch Loss = 0.0020525487921356717
1850, epoch_train_loss=0.0020525487921356717
Epoch 1851
Epoch 1851 :: Batch 0/1
Batch Loss = 0.0020015424361260726
1851, epoch_train_loss=0.0020015424361260726
Epoch 1852
Epoch 1852 :: Batch 0/1
Batch Loss = 0.0023076325129671015
1852, epoch_train_loss=0.0023076325129671015
Epoch 1853
Epoch 1853 :: Batch 0/1
Batch Loss = 0.0025179261667557847
1853, epoch_train_loss=0.0025179261667557847
Epoch 1854
Epoch 1854 :: Batch 0/1
Batch Loss = 0.0024396887153015565
1854, epoch_train_loss=0.0024396887153015565
Epoch 1855
Epoch 1855 :: Batch 0/1
Batch Loss = 0.0021299234447023107
1855, epoch_train_loss=0.0021299234447023107
Epoch 1856
Epoch 1856 :: Batch 0/1
Batch Loss = 0.001955274427360679
1856, epoch_train_loss=0.001955274427360679
Epoch 1857
Epoch 1857 :: Batch 0/1
Batch Loss = 0.002018847245845316
1857, epoch_train_loss=0.002018847245845316
Epoch 1858
Epoch 1858 :: Batch 0/1
Batch Loss = 0.0021871586313285986
1858, epoch_train_loss=0.0021871586313285986
Epoch 1859
Epoch 1859 :: Batch 0/1
Batch Loss = 0.0022640129582000776
1859, epoch_train_loss=0.0022640129582000776
Epoch 1860
Epoch 1860 :: Batch 0/1
Batch Loss = 0.0021663687810443257
1860, epoch_train_loss=0.0021663687810443257
Epoch 1861
Epoch 1861 :: Batch 0/1
Batch Loss = 0.002016832100295294
1861, epoch_train_loss=0.002016832100295294
Epoch 1862
Epoch 1862 :: Batch 0/1
Batch Loss = 0.0019381413421156375
1862, epoch_train_loss=0.0019381413421156375
Epoch 1863
Epoch 1863 :: Batch 0/1
Batch Loss = 0.001980406794663193
1863, epoch_train_loss=0.001980406794663193
Epoch 1864
Epoch 1864 :: Batch 0/1
Batch Loss = 0.0020787640155613186
1864, epoch_train_loss=0.0020787640155613186
Epoch 1865
Epoch 1865 :: Batch 0/1
Batch Loss = 0.0021106186576554426
1865, epoch_train_loss=0.0021106186576554426
Epoch 1866
Epoch 1866 :: Batch 0/1
Batch Loss = 0.002059300345565004
1866, epoch_train_loss=0.002059300345565004
Epoch 1867
Epoch 1867 :: Batch 0/1
Batch Loss = 0.0019713866214361923
1867, epoch_train_loss=0.0019713866214361923
Epoch 1868
Epoch 1868 :: Batch 0/1
Batch Loss = 0.0019286165362378013
1868, epoch_train_loss=0.0019286165362378013
Epoch 1869
Epoch 1869 :: Batch 0/1
Batch Loss = 0.0019492584704257506
1869, epoch_train_loss=0.0019492584704257506
Epoch 1870
Epoch 1870 :: Batch 0/1
Batch Loss = 0.0019979352404135985
1870, epoch_train_loss=0.0019979352404135985
Epoch 1871
Epoch 1871 :: Batch 0/1
Batch Loss = 0.00202437153787045
1871, epoch_train_loss=0.00202437153787045
Epoch 1872
Epoch 1872 :: Batch 0/1
Batch Loss = 0.002005783800165293
1872, epoch_train_loss=0.002005783800165293
Epoch 1873
Epoch 1873 :: Batch 0/1
Batch Loss = 0.0019629168201569726
1873, epoch_train_loss=0.0019629168201569726
Epoch 1874
Epoch 1874 :: Batch 0/1
Batch Loss = 0.0019269563053309414
1874, epoch_train_loss=0.0019269563053309414
Epoch 1875
Epoch 1875 :: Batch 0/1
Batch Loss = 0.0019227870437367113
1875, epoch_train_loss=0.0019227870437367113
Epoch 1876
Epoch 1876 :: Batch 0/1
Batch Loss = 0.0019453550772437944
1876, epoch_train_loss=0.0019453550772437944
Epoch 1877
Epoch 1877 :: Batch 0/1
Batch Loss = 0.0019671649206975464
1877, epoch_train_loss=0.0019671649206975464
Epoch 1878
Epoch 1878 :: Batch 0/1
Batch Loss = 0.0019714566154195386
1878, epoch_train_loss=0.0019714566154195386
Epoch 1879
Epoch 1879 :: Batch 0/1
Batch Loss = 0.0019543491490604304
1879, epoch_train_loss=0.0019543491490604304
Epoch 1880
Epoch 1880 :: Batch 0/1
Batch Loss = 0.0019316759854435032
1880, epoch_train_loss=0.0019316759854435032
Epoch 1881
Epoch 1881 :: Batch 0/1
Batch Loss = 0.001917668074703898
1881, epoch_train_loss=0.001917668074703898
Epoch 1882
Epoch 1882 :: Batch 0/1
Batch Loss = 0.0019183818707022907
1882, epoch_train_loss=0.0019183818707022907
Epoch 1883
Epoch 1883 :: Batch 0/1
Batch Loss = 0.0019284020102500732
1883, epoch_train_loss=0.0019284020102500732
Epoch 1884
Epoch 1884 :: Batch 0/1
Batch Loss = 0.0019387003316750444
1884, epoch_train_loss=0.0019387003316750444
Epoch 1885
Epoch 1885 :: Batch 0/1
Batch Loss = 0.0019409768430190147
1885, epoch_train_loss=0.0019409768430190147
Epoch 1886
Epoch 1886 :: Batch 0/1
Batch Loss = 0.0019332343595340302
1886, epoch_train_loss=0.0019332343595340302
Epoch 1887
Epoch 1887 :: Batch 0/1
Batch Loss = 0.0019218728631336492
1887, epoch_train_loss=0.0019218728631336492
Epoch 1888
Epoch 1888 :: Batch 0/1
Batch Loss = 0.001913582983102056
1888, epoch_train_loss=0.001913582983102056
Epoch 1889
Epoch 1889 :: Batch 0/1
Batch Loss = 0.00191184341846345
1889, epoch_train_loss=0.00191184341846345
Epoch 1890
Epoch 1890 :: Batch 0/1
Batch Loss = 0.0019156679655798258
1890, epoch_train_loss=0.0019156679655798258
Epoch 1891
Epoch 1891 :: Batch 0/1
Batch Loss = 0.0019205030422806918
1891, epoch_train_loss=0.0019205030422806918
Epoch 1892
Epoch 1892 :: Batch 0/1
Batch Loss = 0.0019230827180094507
1892, epoch_train_loss=0.0019230827180094507
Epoch 1893
Epoch 1893 :: Batch 0/1
Batch Loss = 0.001921772289831289
1893, epoch_train_loss=0.001921772289831289
Epoch 1894
Epoch 1894 :: Batch 0/1
Batch Loss = 0.001917456874589002
1894, epoch_train_loss=0.001917456874589002
Epoch 1895
Epoch 1895 :: Batch 0/1
Batch Loss = 0.001912247017890967
1895, epoch_train_loss=0.001912247017890967
Epoch 1896
Epoch 1896 :: Batch 0/1
Batch Loss = 0.0019088829768902755
1896, epoch_train_loss=0.0019088829768902755
Epoch 1897
Epoch 1897 :: Batch 0/1
Batch Loss = 0.0019082488170900761
1897, epoch_train_loss=0.0019082488170900761
Epoch 1898
Epoch 1898 :: Batch 0/1
Batch Loss = 0.0019098177790862314
1898, epoch_train_loss=0.0019098177790862314
Epoch 1899
Epoch 1899 :: Batch 0/1
Batch Loss = 0.0019118819934601011
1899, epoch_train_loss=0.0019118819934601011
Epoch 1900
Epoch 1900 :: Batch 0/1
Batch Loss = 0.001912906578059294
1900, epoch_train_loss=0.001912906578059294
Epoch 1901
Epoch 1901 :: Batch 0/1
Batch Loss = 0.0019125059070385958
1901, epoch_train_loss=0.0019125059070385958
Epoch 1902
Epoch 1902 :: Batch 0/1
Batch Loss = 0.0019108296785663094
1902, epoch_train_loss=0.0019108296785663094
Epoch 1903
Epoch 1903 :: Batch 0/1
Batch Loss = 0.0019085481413849714
1903, epoch_train_loss=0.0019085481413849714
Epoch 1904
Epoch 1904 :: Batch 0/1
Batch Loss = 0.0019065608394955662
1904, epoch_train_loss=0.0019065608394955662
Epoch 1905
Epoch 1905 :: Batch 0/1
Batch Loss = 0.001905457813565285
1905, epoch_train_loss=0.001905457813565285
Epoch 1906
Epoch 1906 :: Batch 0/1
Batch Loss = 0.0019053551788314703
1906, epoch_train_loss=0.0019053551788314703
Epoch 1907
Epoch 1907 :: Batch 0/1
Batch Loss = 0.0019059314122237489
1907, epoch_train_loss=0.0019059314122237489
Epoch 1908
Epoch 1908 :: Batch 0/1
Batch Loss = 0.001906582944814266
1908, epoch_train_loss=0.001906582944814266
Epoch 1909
Epoch 1909 :: Batch 0/1
Batch Loss = 0.001906859175392098
1909, epoch_train_loss=0.001906859175392098
Epoch 1910
Epoch 1910 :: Batch 0/1
Batch Loss = 0.001906652771959952
1910, epoch_train_loss=0.001906652771959952
Epoch 1911
Epoch 1911 :: Batch 0/1
Batch Loss = 0.0019059556443097815
1911, epoch_train_loss=0.0019059556443097815
Epoch 1912
Epoch 1912 :: Batch 0/1
Batch Loss = 0.001905008714008394
1912, epoch_train_loss=0.001905008714008394
Epoch 1913
Epoch 1913 :: Batch 0/1
Batch Loss = 0.001904057946800483
1913, epoch_train_loss=0.001904057946800483
Epoch 1914
Epoch 1914 :: Batch 0/1
Batch Loss = 0.0019032944041125167
1914, epoch_train_loss=0.0019032944041125167
Epoch 1915
Epoch 1915 :: Batch 0/1
Batch Loss = 0.0019028543857515377
1915, epoch_train_loss=0.0019028543857515377
Epoch 1916
Epoch 1916 :: Batch 0/1
Batch Loss = 0.0019027283905385477
1916, epoch_train_loss=0.0019027283905385477
Epoch 1917
Epoch 1917 :: Batch 0/1
Batch Loss = 0.0019027851284293708
1917, epoch_train_loss=0.0019027851284293708
Epoch 1918
Epoch 1918 :: Batch 0/1
Batch Loss = 0.0019028873623654163
1918, epoch_train_loss=0.0019028873623654163
Epoch 1919
Epoch 1919 :: Batch 0/1
Batch Loss = 0.0019029147618626061
1919, epoch_train_loss=0.0019029147618626061
Epoch 1920
Epoch 1920 :: Batch 0/1
Batch Loss = 0.0019028000790645189
1920, epoch_train_loss=0.0019028000790645189
Epoch 1921
Epoch 1921 :: Batch 0/1
Batch Loss = 0.001902563791085945
1921, epoch_train_loss=0.001902563791085945
Epoch 1922
Epoch 1922 :: Batch 0/1
Batch Loss = 0.0019022069560996118
1922, epoch_train_loss=0.0019022069560996118
Epoch 1923
Epoch 1923 :: Batch 0/1
Batch Loss = 0.0019017798405714839
1923, epoch_train_loss=0.0019017798405714839
Epoch 1924
Epoch 1924 :: Batch 0/1
Batch Loss = 0.00190134567264058
1924, epoch_train_loss=0.00190134567264058
Epoch 1925
Epoch 1925 :: Batch 0/1
Batch Loss = 0.0019009579924548622
1925, epoch_train_loss=0.0019009579924548622
Epoch 1926
Epoch 1926 :: Batch 0/1
Batch Loss = 0.0019006467998015067
1926, epoch_train_loss=0.0019006467998015067
Epoch 1927
Epoch 1927 :: Batch 0/1
Batch Loss = 0.001900418749719897
1927, epoch_train_loss=0.001900418749719897
Epoch 1928
Epoch 1928 :: Batch 0/1
Batch Loss = 0.001900248674263263
1928, epoch_train_loss=0.001900248674263263
Epoch 1929
Epoch 1929 :: Batch 0/1
Batch Loss = 0.001900125515674206
1929, epoch_train_loss=0.001900125515674206
Epoch 1930
Epoch 1930 :: Batch 0/1
Batch Loss = 0.0019000270762570726
1930, epoch_train_loss=0.0019000270762570726
Epoch 1931
Epoch 1931 :: Batch 0/1
Batch Loss = 0.0018999324875451909
1931, epoch_train_loss=0.0018999324875451909
Epoch 1932
Epoch 1932 :: Batch 0/1
Batch Loss = 0.0018998295112246815
1932, epoch_train_loss=0.0018998295112246815
Epoch 1933
Epoch 1933 :: Batch 0/1
Batch Loss = 0.0018997060118671233
1933, epoch_train_loss=0.0018997060118671233
Epoch 1934
Epoch 1934 :: Batch 0/1
Batch Loss = 0.00189955563430199
1934, epoch_train_loss=0.00189955563430199
Epoch 1935
Epoch 1935 :: Batch 0/1
Batch Loss = 0.0018993847805808199
1935, epoch_train_loss=0.0018993847805808199
Epoch 1936
Epoch 1936 :: Batch 0/1
Batch Loss = 0.0018991980695567791
1936, epoch_train_loss=0.0018991980695567791
Epoch 1937
Epoch 1937 :: Batch 0/1
Batch Loss = 0.0018990032049426525
1937, epoch_train_loss=0.0018990032049426525
Epoch 1938
Epoch 1938 :: Batch 0/1
Batch Loss = 0.0018988055022354218
1938, epoch_train_loss=0.0018988055022354218
Epoch 1939
Epoch 1939 :: Batch 0/1
Batch Loss = 0.0018986057747853546
1939, epoch_train_loss=0.0018986057747853546
Epoch 1940
Epoch 1940 :: Batch 0/1
Batch Loss = 0.0018984090651011328
1940, epoch_train_loss=0.0018984090651011328
Epoch 1941
Epoch 1941 :: Batch 0/1
Batch Loss = 0.0018982200332459513
1941, epoch_train_loss=0.0018982200332459513
Epoch 1942
Epoch 1942 :: Batch 0/1
Batch Loss = 0.001898038565530367
1942, epoch_train_loss=0.001898038565530367
Epoch 1943
Epoch 1943 :: Batch 0/1
Batch Loss = 0.0018978665324420707
1943, epoch_train_loss=0.0018978665324420707
Epoch 1944
Epoch 1944 :: Batch 0/1
Batch Loss = 0.0018977019231950743
1944, epoch_train_loss=0.0018977019231950743
Epoch 1945
Epoch 1945 :: Batch 0/1
Batch Loss = 0.0018975435273815948
1945, epoch_train_loss=0.0018975435273815948
Epoch 1946
Epoch 1946 :: Batch 0/1
Batch Loss = 0.0018973909380512169
1946, epoch_train_loss=0.0018973909380512169
Epoch 1947
Epoch 1947 :: Batch 0/1
Batch Loss = 0.001897243263418489
1947, epoch_train_loss=0.001897243263418489
Epoch 1948
Epoch 1948 :: Batch 0/1
Batch Loss = 0.0018971000574197758
1948, epoch_train_loss=0.0018971000574197758
Epoch 1949
Epoch 1949 :: Batch 0/1
Batch Loss = 0.0018969615387010338
1949, epoch_train_loss=0.0018969615387010338
Epoch 1950
Epoch 1950 :: Batch 0/1
Batch Loss = 0.0018968254719924124
1950, epoch_train_loss=0.0018968254719924124
Epoch 1951
Epoch 1951 :: Batch 0/1
Batch Loss = 0.0018966917029625773
1951, epoch_train_loss=0.0018966917029625773
Epoch 1952
Epoch 1952 :: Batch 0/1
Batch Loss = 0.00189655996447344
1952, epoch_train_loss=0.00189655996447344
Epoch 1953
Epoch 1953 :: Batch 0/1
Batch Loss = 0.0018964301085831329
1953, epoch_train_loss=0.0018964301085831329
Epoch 1954
Epoch 1954 :: Batch 0/1
Batch Loss = 0.0018963024466852319
1954, epoch_train_loss=0.0018963024466852319
Epoch 1955
Epoch 1955 :: Batch 0/1
Batch Loss = 0.001896176824172958
1955, epoch_train_loss=0.001896176824172958
Epoch 1956
Epoch 1956 :: Batch 0/1
Batch Loss = 0.0018960529400301627
1956, epoch_train_loss=0.0018960529400301627
Epoch 1957
Epoch 1957 :: Batch 0/1
Batch Loss = 0.001895931147355267
1957, epoch_train_loss=0.001895931147355267
Epoch 1958
Epoch 1958 :: Batch 0/1
Batch Loss = 0.0018958111693004253
1958, epoch_train_loss=0.0018958111693004253
Epoch 1959
Epoch 1959 :: Batch 0/1
Batch Loss = 0.0018956935058621674
1959, epoch_train_loss=0.0018956935058621674
Epoch 1960
Epoch 1960 :: Batch 0/1
Batch Loss = 0.0018955789760123959
1960, epoch_train_loss=0.0018955789760123959
Epoch 1961
Epoch 1961 :: Batch 0/1
Batch Loss = 0.001895468170960896
1961, epoch_train_loss=0.001895468170960896
Epoch 1962
Epoch 1962 :: Batch 0/1
Batch Loss = 0.0018953621869381295
1962, epoch_train_loss=0.0018953621869381295
Epoch 1963
Epoch 1963 :: Batch 0/1
Batch Loss = 0.0018952626056296668
1963, epoch_train_loss=0.0018952626056296668
Epoch 1964
Epoch 1964 :: Batch 0/1
Batch Loss = 0.001895171629807734
1964, epoch_train_loss=0.001895171629807734
Epoch 1965
Epoch 1965 :: Batch 0/1
Batch Loss = 0.001895093286270628
1965, epoch_train_loss=0.001895093286270628
Epoch 1966
Epoch 1966 :: Batch 0/1
Batch Loss = 0.001895032980929934
1966, epoch_train_loss=0.001895032980929934
Epoch 1967
Epoch 1967 :: Batch 0/1
Batch Loss = 0.0018949999675566616
1967, epoch_train_loss=0.0018949999675566616
Epoch 1968
Epoch 1968 :: Batch 0/1
Batch Loss = 0.001895007483001541
1968, epoch_train_loss=0.001895007483001541
Epoch 1969
Epoch 1969 :: Batch 0/1
Batch Loss = 0.0018950780724891732
1969, epoch_train_loss=0.0018950780724891732
Epoch 1970
Epoch 1970 :: Batch 0/1
Batch Loss = 0.0018952438196639533
1970, epoch_train_loss=0.0018952438196639533
Epoch 1971
Epoch 1971 :: Batch 0/1
Batch Loss = 0.0018955627438266686
1971, epoch_train_loss=0.0018955627438266686
Epoch 1972
Epoch 1972 :: Batch 0/1
Batch Loss = 0.001896115799094274
1972, epoch_train_loss=0.001896115799094274
Epoch 1973
Epoch 1973 :: Batch 0/1
Batch Loss = 0.0018970572656210952
1973, epoch_train_loss=0.0018970572656210952
Epoch 1974
Epoch 1974 :: Batch 0/1
Batch Loss = 0.0018985960705219802
1974, epoch_train_loss=0.0018985960705219802
Epoch 1975
Epoch 1975 :: Batch 0/1
Batch Loss = 0.0019011593807029858
1975, epoch_train_loss=0.0019011593807029858
Epoch 1976
Epoch 1976 :: Batch 0/1
Batch Loss = 0.0019052946821075305
1976, epoch_train_loss=0.0019052946821075305
Epoch 1977
Epoch 1977 :: Batch 0/1
Batch Loss = 0.0019122392276811546
1977, epoch_train_loss=0.0019122392276811546
Epoch 1978
Epoch 1978 :: Batch 0/1
Batch Loss = 0.0019234313121069717
1978, epoch_train_loss=0.0019234313121069717
Epoch 1979
Epoch 1979 :: Batch 0/1
Batch Loss = 0.0019426343091214703
1979, epoch_train_loss=0.0019426343091214703
Epoch 1980
Epoch 1980 :: Batch 0/1
Batch Loss = 0.00197353786680976
1980, epoch_train_loss=0.00197353786680976
Epoch 1981
Epoch 1981 :: Batch 0/1
Batch Loss = 0.0020282333409147825
1981, epoch_train_loss=0.0020282333409147825
Epoch 1982
Epoch 1982 :: Batch 0/1
Batch Loss = 0.0021153534977574047
1982, epoch_train_loss=0.0021153534977574047
Epoch 1983
Epoch 1983 :: Batch 0/1
Batch Loss = 0.002276114702854841
1983, epoch_train_loss=0.002276114702854841
Epoch 1984
Epoch 1984 :: Batch 0/1
Batch Loss = 0.0025245073574043145
1984, epoch_train_loss=0.0025245073574043145
Epoch 1985
Epoch 1985 :: Batch 0/1
Batch Loss = 0.0030093467683935246
1985, epoch_train_loss=0.0030093467683935246
Epoch 1986
Epoch 1986 :: Batch 0/1
Batch Loss = 0.003705938212528734
1986, epoch_train_loss=0.003705938212528734
Epoch 1987
Epoch 1987 :: Batch 0/1
Batch Loss = 0.005172073306570774
1987, epoch_train_loss=0.005172073306570774
Epoch 1988
Epoch 1988 :: Batch 0/1
Batch Loss = 0.006946921025994317
1988, epoch_train_loss=0.006946921025994317
Epoch 1989
Epoch 1989 :: Batch 0/1
Batch Loss = 0.011063815225573096
1989, epoch_train_loss=0.011063815225573096
Epoch 1990
Epoch 1990 :: Batch 0/1
Batch Loss = 0.014204478257998242
1990, epoch_train_loss=0.014204478257998242
Epoch 1991
Epoch 1991 :: Batch 0/1
Batch Loss = 0.02262121581484874
1991, epoch_train_loss=0.02262121581484874
Epoch 1992
Epoch 1992 :: Batch 0/1
Batch Loss = 0.022237975998130773
1992, epoch_train_loss=0.022237975998130773
Epoch 1993
Epoch 1993 :: Batch 0/1
Batch Loss = 0.02695666822908549
1993, epoch_train_loss=0.02695666822908549
Epoch 1994
Epoch 1994 :: Batch 0/1
Batch Loss = 0.015933733592032408
1994, epoch_train_loss=0.015933733592032408
Epoch 1995
Epoch 1995 :: Batch 0/1
Batch Loss = 0.008387311357912833
1995, epoch_train_loss=0.008387311357912833
Epoch 1996
Epoch 1996 :: Batch 0/1
Batch Loss = 0.0025300969000391776
1996, epoch_train_loss=0.0025300969000391776
Epoch 1997
Epoch 1997 :: Batch 0/1
Batch Loss = 0.003279943581410062
1997, epoch_train_loss=0.003279943581410062
Epoch 1998
Epoch 1998 :: Batch 0/1
Batch Loss = 0.008366939123423376
1998, epoch_train_loss=0.008366939123423376
Epoch 1999
Epoch 1999 :: Batch 0/1
Batch Loss = 0.01063766232220364
1999, epoch_train_loss=0.01063766232220364
Epoch 2000
Epoch 2000 :: Batch 0/1
Batch Loss = 0.0106735946139826
2000, epoch_train_loss=0.0106735946139826
Epoch 2001
Epoch 2001 :: Batch 0/1
Batch Loss = 0.0051179560104827534
2001, epoch_train_loss=0.0051179560104827534
Epoch 2002
Epoch 2002 :: Batch 0/1
Batch Loss = 0.002143219656450679
2002, epoch_train_loss=0.002143219656450679
Epoch 2003
Epoch 2003 :: Batch 0/1
Batch Loss = 0.0032425168671900853
2003, epoch_train_loss=0.0032425168671900853
Epoch 2004
Epoch 2004 :: Batch 0/1
Batch Loss = 0.005920036642710635
2004, epoch_train_loss=0.005920036642710635
Epoch 2005
Epoch 2005 :: Batch 0/1
Batch Loss = 0.007385487968992633
2005, epoch_train_loss=0.007385487968992633
Epoch 2006
Epoch 2006 :: Batch 0/1
Batch Loss = 0.004796080888979863
2006, epoch_train_loss=0.004796080888979863
Epoch 2007
Epoch 2007 :: Batch 0/1
Batch Loss = 0.002433036574773767
2007, epoch_train_loss=0.002433036574773767
Epoch 2008
Epoch 2008 :: Batch 0/1
Batch Loss = 0.0022555890115961534
2008, epoch_train_loss=0.0022555890115961534
Epoch 2009
Epoch 2009 :: Batch 0/1
Batch Loss = 0.0038435375117134626
2009, epoch_train_loss=0.0038435375117134626
Epoch 2010
Epoch 2010 :: Batch 0/1
Batch Loss = 0.004921085980760483
2010, epoch_train_loss=0.004921085980760483
Epoch 2011
Epoch 2011 :: Batch 0/1
Batch Loss = 0.0036514444959365325
2011, epoch_train_loss=0.0036514444959365325
Epoch 2012
Epoch 2012 :: Batch 0/1
Batch Loss = 0.002231654181300633
2012, epoch_train_loss=0.002231654181300633
Epoch 2013
Epoch 2013 :: Batch 0/1
Batch Loss = 0.0021624945188436105
2013, epoch_train_loss=0.0021624945188436105
Epoch 2014
Epoch 2014 :: Batch 0/1
Batch Loss = 0.0031532771953324845
2014, epoch_train_loss=0.0031532771953324845
Epoch 2015
Epoch 2015 :: Batch 0/1
Batch Loss = 0.0037008587363970137
2015, epoch_train_loss=0.0037008587363970137
Epoch 2016
Epoch 2016 :: Batch 0/1
Batch Loss = 0.002887745727818321
2016, epoch_train_loss=0.002887745727818321
Epoch 2017
Epoch 2017 :: Batch 0/1
Batch Loss = 0.002095201924235095
2017, epoch_train_loss=0.002095201924235095
Epoch 2018
Epoch 2018 :: Batch 0/1
Batch Loss = 0.0021651016179846305
2018, epoch_train_loss=0.0021651016179846305
Epoch 2019
Epoch 2019 :: Batch 0/1
Batch Loss = 0.0027291116862631225
2019, epoch_train_loss=0.0027291116862631225
Epoch 2020
Epoch 2020 :: Batch 0/1
Batch Loss = 0.002912963671640411
2020, epoch_train_loss=0.002912963671640411
Epoch 2021
Epoch 2021 :: Batch 0/1
Batch Loss = 0.0024350938709710506
2021, epoch_train_loss=0.0024350938709710506
Epoch 2022
Epoch 2022 :: Batch 0/1
Batch Loss = 0.0020457544713898164
2022, epoch_train_loss=0.0020457544713898164
Epoch 2023
Epoch 2023 :: Batch 0/1
Batch Loss = 0.002123303137392002
2023, epoch_train_loss=0.002123303137392002
Epoch 2024
Epoch 2024 :: Batch 0/1
Batch Loss = 0.0024085089081554607
2024, epoch_train_loss=0.0024085089081554607
Epoch 2025
Epoch 2025 :: Batch 0/1
Batch Loss = 0.0024781744154994093
2025, epoch_train_loss=0.0024781744154994093
Epoch 2026
Epoch 2026 :: Batch 0/1
Batch Loss = 0.002234171386401829
2026, epoch_train_loss=0.002234171386401829
Epoch 2027
Epoch 2027 :: Batch 0/1
Batch Loss = 0.0020365315189364128
2027, epoch_train_loss=0.0020365315189364128
Epoch 2028
Epoch 2028 :: Batch 0/1
Batch Loss = 0.0020521612267871037
2028, epoch_train_loss=0.0020521612267871037
Epoch 2029
Epoch 2029 :: Batch 0/1
Batch Loss = 0.002189925324746876
2029, epoch_train_loss=0.002189925324746876
Epoch 2030
Epoch 2030 :: Batch 0/1
Batch Loss = 0.0022353751737367944
2030, epoch_train_loss=0.0022353751737367944
Epoch 2031
Epoch 2031 :: Batch 0/1
Batch Loss = 0.0021292385899970814
2031, epoch_train_loss=0.0021292385899970814
Epoch 2032
Epoch 2032 :: Batch 0/1
Batch Loss = 0.002021231710600498
2032, epoch_train_loss=0.002021231710600498
Epoch 2033
Epoch 2033 :: Batch 0/1
Batch Loss = 0.001989023592044375
2033, epoch_train_loss=0.001989023592044375
Epoch 2034
Epoch 2034 :: Batch 0/1
Batch Loss = 0.0020344056505030355
2034, epoch_train_loss=0.0020344056505030355
Epoch 2035
Epoch 2035 :: Batch 0/1
Batch Loss = 0.0020824938476293846
2035, epoch_train_loss=0.0020824938476293846
Epoch 2036
Epoch 2036 :: Batch 0/1
Batch Loss = 0.0020715771639714265
2036, epoch_train_loss=0.0020715771639714265
Epoch 2037
Epoch 2037 :: Batch 0/1
Batch Loss = 0.002015948595047191
2037, epoch_train_loss=0.002015948595047191
Epoch 2038
Epoch 2038 :: Batch 0/1
Batch Loss = 0.0019531825439776406
2038, epoch_train_loss=0.0019531825439776406
Epoch 2039
Epoch 2039 :: Batch 0/1
Batch Loss = 0.0019456355039828546
2039, epoch_train_loss=0.0019456355039828546
Epoch 2040
Epoch 2040 :: Batch 0/1
Batch Loss = 0.001991643329555272
2040, epoch_train_loss=0.001991643329555272
Epoch 2041
Epoch 2041 :: Batch 0/1
Batch Loss = 0.0020251661425478547
2041, epoch_train_loss=0.0020251661425478547
Epoch 2042
Epoch 2042 :: Batch 0/1
Batch Loss = 0.0020029176807088924
2042, epoch_train_loss=0.0020029176807088924
Epoch 2043
Epoch 2043 :: Batch 0/1
Batch Loss = 0.0019414314111442735
2043, epoch_train_loss=0.0019414314111442735
Epoch 2044
Epoch 2044 :: Batch 0/1
Batch Loss = 0.0019138724658458075
2044, epoch_train_loss=0.0019138724658458075
Epoch 2045
Epoch 2045 :: Batch 0/1
Batch Loss = 0.0019377444571182269
2045, epoch_train_loss=0.0019377444571182269
Epoch 2046
Epoch 2046 :: Batch 0/1
Batch Loss = 0.001971439429173567
2046, epoch_train_loss=0.001971439429173567
Epoch 2047
Epoch 2047 :: Batch 0/1
Batch Loss = 0.001973240945129421
2047, epoch_train_loss=0.001973240945129421
Epoch 2048
Epoch 2048 :: Batch 0/1
Batch Loss = 0.0019426760206086285
2048, epoch_train_loss=0.0019426760206086285
Epoch 2049
Epoch 2049 :: Batch 0/1
Batch Loss = 0.0019175590410673586
2049, epoch_train_loss=0.0019175590410673586
Epoch 2050
Epoch 2050 :: Batch 0/1
Batch Loss = 0.0019158720520436108
2050, epoch_train_loss=0.0019158720520436108
Epoch 2051
Epoch 2051 :: Batch 0/1
Batch Loss = 0.0019264500194994265
2051, epoch_train_loss=0.0019264500194994265
Epoch 2052
Epoch 2052 :: Batch 0/1
Batch Loss = 0.0019342579058589562
2052, epoch_train_loss=0.0019342579058589562
Epoch 2053
Epoch 2053 :: Batch 0/1
Batch Loss = 0.0019329740725206647
2053, epoch_train_loss=0.0019329740725206647
Epoch 2054
Epoch 2054 :: Batch 0/1
Batch Loss = 0.001926480432050051
2054, epoch_train_loss=0.001926480432050051
Epoch 2055
Epoch 2055 :: Batch 0/1
Batch Loss = 0.0019171352218415331
2055, epoch_train_loss=0.0019171352218415331
Epoch 2056
Epoch 2056 :: Batch 0/1
Batch Loss = 0.0019086383926382028
2056, epoch_train_loss=0.0019086383926382028
Epoch 2057
Epoch 2057 :: Batch 0/1
Batch Loss = 0.0019068027760527463
2057, epoch_train_loss=0.0019068027760527463
Epoch 2058
Epoch 2058 :: Batch 0/1
Batch Loss = 0.0019127948729170143
2058, epoch_train_loss=0.0019127948729170143
Epoch 2059
Epoch 2059 :: Batch 0/1
Batch Loss = 0.0019194103209094425
2059, epoch_train_loss=0.0019194103209094425
Epoch 2060
Epoch 2060 :: Batch 0/1
Batch Loss = 0.0019180273679693975
2060, epoch_train_loss=0.0019180273679693975
Epoch 2061
Epoch 2061 :: Batch 0/1
Batch Loss = 0.0019095124892568266
2061, epoch_train_loss=0.0019095124892568266
Epoch 2062
Epoch 2062 :: Batch 0/1
Batch Loss = 0.0019016230332972413
2062, epoch_train_loss=0.0019016230332972413
Epoch 2063
Epoch 2063 :: Batch 0/1
Batch Loss = 0.0019005203036465868
2063, epoch_train_loss=0.0019005203036465868
Epoch 2064
Epoch 2064 :: Batch 0/1
Batch Loss = 0.001904389238368711
2064, epoch_train_loss=0.001904389238368711
Epoch 2065
Epoch 2065 :: Batch 0/1
Batch Loss = 0.0019075054166358134
2065, epoch_train_loss=0.0019075054166358134
Epoch 2066
Epoch 2066 :: Batch 0/1
Batch Loss = 0.0019070354144374684
2066, epoch_train_loss=0.0019070354144374684
Epoch 2067
Epoch 2067 :: Batch 0/1
Batch Loss = 0.0019042019337740278
2067, epoch_train_loss=0.0019042019337740278
Epoch 2068
Epoch 2068 :: Batch 0/1
Batch Loss = 0.001901325031818844
2068, epoch_train_loss=0.001901325031818844
Epoch 2069
Epoch 2069 :: Batch 0/1
Batch Loss = 0.0018993151793005026
2069, epoch_train_loss=0.0018993151793005026
Epoch 2070
Epoch 2070 :: Batch 0/1
Batch Loss = 0.0018982471272494395
2070, epoch_train_loss=0.0018982471272494395
Epoch 2071
Epoch 2071 :: Batch 0/1
Batch Loss = 0.0018982541843837355
2071, epoch_train_loss=0.0018982541843837355
Epoch 2072
Epoch 2072 :: Batch 0/1
Batch Loss = 0.0018993303467581886
2072, epoch_train_loss=0.0018993303467581886
Epoch 2073
Epoch 2073 :: Batch 0/1
Batch Loss = 0.0019003715997015255
2073, epoch_train_loss=0.0019003715997015255
Epoch 2074
Epoch 2074 :: Batch 0/1
Batch Loss = 0.0018999467294431227
2074, epoch_train_loss=0.0018999467294431227
Epoch 2075
Epoch 2075 :: Batch 0/1
Batch Loss = 0.0018979946269828604
2075, epoch_train_loss=0.0018979946269828604
Epoch 2076
Epoch 2076 :: Batch 0/1
Batch Loss = 0.001895783244727947
2076, epoch_train_loss=0.001895783244727947
Epoch 2077
Epoch 2077 :: Batch 0/1
Batch Loss = 0.0018947892489758027
2077, epoch_train_loss=0.0018947892489758027
Epoch 2078
Epoch 2078 :: Batch 0/1
Batch Loss = 0.0018951642335693097
2078, epoch_train_loss=0.0018951642335693097
Epoch 2079
Epoch 2079 :: Batch 0/1
Batch Loss = 0.0018958817224023425
2079, epoch_train_loss=0.0018958817224023425
Epoch 2080
Epoch 2080 :: Batch 0/1
Batch Loss = 0.0018960967411362597
2080, epoch_train_loss=0.0018960967411362597
Epoch 2081
Epoch 2081 :: Batch 0/1
Batch Loss = 0.0018956743318797296
2081, epoch_train_loss=0.0018956743318797296
Epoch 2082
Epoch 2082 :: Batch 0/1
Batch Loss = 0.0018949941707752378
2082, epoch_train_loss=0.0018949941707752378
Epoch 2083
Epoch 2083 :: Batch 0/1
Batch Loss = 0.001894302455641144
2083, epoch_train_loss=0.001894302455641144
Epoch 2084
Epoch 2084 :: Batch 0/1
Batch Loss = 0.0018936412766903286
2084, epoch_train_loss=0.0018936412766903286
Epoch 2085
Epoch 2085 :: Batch 0/1
Batch Loss = 0.001893076456107087
2085, epoch_train_loss=0.001893076456107087
Epoch 2086
Epoch 2086 :: Batch 0/1
Batch Loss = 0.00189278940117362
2086, epoch_train_loss=0.00189278940117362
Epoch 2087
Epoch 2087 :: Batch 0/1
Batch Loss = 0.0018928365917717917
2087, epoch_train_loss=0.0018928365917717917
Epoch 2088
Epoch 2088 :: Batch 0/1
Batch Loss = 0.0018930306464719326
2088, epoch_train_loss=0.0018930306464719326
Epoch 2089
Epoch 2089 :: Batch 0/1
Batch Loss = 0.0018930601415677133
2089, epoch_train_loss=0.0018930601415677133
Epoch 2090
Epoch 2090 :: Batch 0/1
Batch Loss = 0.0018927399457777554
2090, epoch_train_loss=0.0018927399457777554
Epoch 2091
Epoch 2091 :: Batch 0/1
Batch Loss = 0.0018921921987972356
2091, epoch_train_loss=0.0018921921987972356
Epoch 2092
Epoch 2092 :: Batch 0/1
Batch Loss = 0.0018916691689532035
2092, epoch_train_loss=0.0018916691689532035
Epoch 2093
Epoch 2093 :: Batch 0/1
Batch Loss = 0.0018913375406546375
2093, epoch_train_loss=0.0018913375406546375
Epoch 2094
Epoch 2094 :: Batch 0/1
Batch Loss = 0.0018911802592448095
2094, epoch_train_loss=0.0018911802592448095
Epoch 2095
Epoch 2095 :: Batch 0/1
Batch Loss = 0.0018910863954271799
2095, epoch_train_loss=0.0018910863954271799
Epoch 2096
Epoch 2096 :: Batch 0/1
Batch Loss = 0.0018909859395847262
2096, epoch_train_loss=0.0018909859395847262
Epoch 2097
Epoch 2097 :: Batch 0/1
Batch Loss = 0.0018908827527921996
2097, epoch_train_loss=0.0018908827527921996
Epoch 2098
Epoch 2098 :: Batch 0/1
Batch Loss = 0.0018907746742293779
2098, epoch_train_loss=0.0018907746742293779
Epoch 2099
Epoch 2099 :: Batch 0/1
Batch Loss = 0.00189063205773856
2099, epoch_train_loss=0.00189063205773856
Epoch 2100
Epoch 2100 :: Batch 0/1
Batch Loss = 0.0018904233052264392
2100, epoch_train_loss=0.0018904233052264392
Epoch 2101
Epoch 2101 :: Batch 0/1
Batch Loss = 0.0018901519278682348
2101, epoch_train_loss=0.0018901519278682348
Epoch 2102
Epoch 2102 :: Batch 0/1
Batch Loss = 0.0018898753756041544
2102, epoch_train_loss=0.0018898753756041544
Epoch 2103
Epoch 2103 :: Batch 0/1
Batch Loss = 0.0018896528501798641
2103, epoch_train_loss=0.0018896528501798641
Epoch 2104
Epoch 2104 :: Batch 0/1
Batch Loss = 0.0018895069218309685
2104, epoch_train_loss=0.0018895069218309685
Epoch 2105
Epoch 2105 :: Batch 0/1
Batch Loss = 0.0018894138207703063
2105, epoch_train_loss=0.0018894138207703063
Epoch 2106
Epoch 2106 :: Batch 0/1
Batch Loss = 0.0018893268312474988
2106, epoch_train_loss=0.0018893268312474988
Epoch 2107
Epoch 2107 :: Batch 0/1
Batch Loss = 0.00188921877113986
2107, epoch_train_loss=0.00188921877113986
Epoch 2108
Epoch 2108 :: Batch 0/1
Batch Loss = 0.0018890915879660748
2108, epoch_train_loss=0.0018890915879660748
Epoch 2109
Epoch 2109 :: Batch 0/1
Batch Loss = 0.0018889567538413563
2109, epoch_train_loss=0.0018889567538413563
Epoch 2110
Epoch 2110 :: Batch 0/1
Batch Loss = 0.0018888194027466558
2110, epoch_train_loss=0.0018888194027466558
Epoch 2111
Epoch 2111 :: Batch 0/1
Batch Loss = 0.0018886747658745898
2111, epoch_train_loss=0.0018886747658745898
Epoch 2112
Epoch 2112 :: Batch 0/1
Batch Loss = 0.0018885185120999014
2112, epoch_train_loss=0.0018885185120999014
Epoch 2113
Epoch 2113 :: Batch 0/1
Batch Loss = 0.0018883569208009991
2113, epoch_train_loss=0.0018883569208009991
Epoch 2114
Epoch 2114 :: Batch 0/1
Batch Loss = 0.0018882022787506418
2114, epoch_train_loss=0.0018882022787506418
Epoch 2115
Epoch 2115 :: Batch 0/1
Batch Loss = 0.001888065705251907
2115, epoch_train_loss=0.001888065705251907
Epoch 2116
Epoch 2116 :: Batch 0/1
Batch Loss = 0.0018879498107572331
2116, epoch_train_loss=0.0018879498107572331
Epoch 2117
Epoch 2117 :: Batch 0/1
Batch Loss = 0.0018878462242470815
2117, epoch_train_loss=0.0018878462242470815
Epoch 2118
Epoch 2118 :: Batch 0/1
Batch Loss = 0.0018877446287302454
2118, epoch_train_loss=0.0018877446287302454
Epoch 2119
Epoch 2119 :: Batch 0/1
Batch Loss = 0.0018876395609586654
2119, epoch_train_loss=0.0018876395609586654
Epoch 2120
Epoch 2120 :: Batch 0/1
Batch Loss = 0.0018875313729239702
2120, epoch_train_loss=0.0018875313729239702
Epoch 2121
Epoch 2121 :: Batch 0/1
Batch Loss = 0.0018874234150337268
2121, epoch_train_loss=0.0018874234150337268
Epoch 2122
Epoch 2122 :: Batch 0/1
Batch Loss = 0.0018873168243070562
2122, epoch_train_loss=0.0018873168243070562
Epoch 2123
Epoch 2123 :: Batch 0/1
Batch Loss = 0.001887210109277734
2123, epoch_train_loss=0.001887210109277734
Epoch 2124
Epoch 2124 :: Batch 0/1
Batch Loss = 0.001887101296421404
2124, epoch_train_loss=0.001887101296421404
Epoch 2125
Epoch 2125 :: Batch 0/1
Batch Loss = 0.001886989010530401
2125, epoch_train_loss=0.001886989010530401
Epoch 2126
Epoch 2126 :: Batch 0/1
Batch Loss = 0.0018868748964581726
2126, epoch_train_loss=0.0018868748964581726
Epoch 2127
Epoch 2127 :: Batch 0/1
Batch Loss = 0.001886762064218622
2127, epoch_train_loss=0.001886762064218622
Epoch 2128
Epoch 2128 :: Batch 0/1
Batch Loss = 0.0018866529141587758
2128, epoch_train_loss=0.0018866529141587758
Epoch 2129
Epoch 2129 :: Batch 0/1
Batch Loss = 0.001886547962587041
2129, epoch_train_loss=0.001886547962587041
Epoch 2130
Epoch 2130 :: Batch 0/1
Batch Loss = 0.0018864459849206095
2130, epoch_train_loss=0.0018864459849206095
Epoch 2131
Epoch 2131 :: Batch 0/1
Batch Loss = 0.0018863455102333163
2131, epoch_train_loss=0.0018863455102333163
Epoch 2132
Epoch 2132 :: Batch 0/1
Batch Loss = 0.0018862460098640946
2132, epoch_train_loss=0.0018862460098640946
Epoch 2133
Epoch 2133 :: Batch 0/1
Batch Loss = 0.0018861476296465126
2133, epoch_train_loss=0.0018861476296465126
Epoch 2134
Epoch 2134 :: Batch 0/1
Batch Loss = 0.0018860511559669962
2134, epoch_train_loss=0.0018860511559669962
Epoch 2135
Epoch 2135 :: Batch 0/1
Batch Loss = 0.001885957147739149
2135, epoch_train_loss=0.001885957147739149
Epoch 2136
Epoch 2136 :: Batch 0/1
Batch Loss = 0.001885865346536862
2136, epoch_train_loss=0.001885865346536862
Epoch 2137
Epoch 2137 :: Batch 0/1
Batch Loss = 0.0018857750210884592
2137, epoch_train_loss=0.0018857750210884592
Epoch 2138
Epoch 2138 :: Batch 0/1
Batch Loss = 0.001885685429939378
2138, epoch_train_loss=0.001885685429939378
Epoch 2139
Epoch 2139 :: Batch 0/1
Batch Loss = 0.001885596258981081
2139, epoch_train_loss=0.001885596258981081
Epoch 2140
Epoch 2140 :: Batch 0/1
Batch Loss = 0.0018855077282597568
2140, epoch_train_loss=0.0018855077282597568
Epoch 2141
Epoch 2141 :: Batch 0/1
Batch Loss = 0.0018854201906016394
2141, epoch_train_loss=0.0018854201906016394
Epoch 2142
Epoch 2142 :: Batch 0/1
Batch Loss = 0.0018853340210631037
2142, epoch_train_loss=0.0018853340210631037
Epoch 2143
Epoch 2143 :: Batch 0/1
Batch Loss = 0.0018852494172670864
2143, epoch_train_loss=0.0018852494172670864
Epoch 2144
Epoch 2144 :: Batch 0/1
Batch Loss = 0.0018851662060814641
2144, epoch_train_loss=0.0018851662060814641
Epoch 2145
Epoch 2145 :: Batch 0/1
Batch Loss = 0.0018850843045816925
2145, epoch_train_loss=0.0018850843045816925
Epoch 2146
Epoch 2146 :: Batch 0/1
Batch Loss = 0.001885003666381756
2146, epoch_train_loss=0.001885003666381756
Epoch 2147
Epoch 2147 :: Batch 0/1
Batch Loss = 0.0018849246406228064
2147, epoch_train_loss=0.0018849246406228064
Epoch 2148
Epoch 2148 :: Batch 0/1
Batch Loss = 0.0018848476846695117
2148, epoch_train_loss=0.0018848476846695117
Epoch 2149
Epoch 2149 :: Batch 0/1
Batch Loss = 0.0018847735803727262
2149, epoch_train_loss=0.0018847735803727262
Epoch 2150
Epoch 2150 :: Batch 0/1
Batch Loss = 0.0018847031360270023
2150, epoch_train_loss=0.0018847031360270023
Epoch 2151
Epoch 2151 :: Batch 0/1
Batch Loss = 0.001884637611566414
2151, epoch_train_loss=0.001884637611566414
Epoch 2152
Epoch 2152 :: Batch 0/1
Batch Loss = 0.0018845783868312795
2152, epoch_train_loss=0.0018845783868312795
Epoch 2153
Epoch 2153 :: Batch 0/1
Batch Loss = 0.0018845279578719126
2153, epoch_train_loss=0.0018845279578719126
Epoch 2154
Epoch 2154 :: Batch 0/1
Batch Loss = 0.001884489514960156
2154, epoch_train_loss=0.001884489514960156
Epoch 2155
Epoch 2155 :: Batch 0/1
Batch Loss = 0.0018844686802833752
2155, epoch_train_loss=0.0018844686802833752
Epoch 2156
Epoch 2156 :: Batch 0/1
Batch Loss = 0.0018844727567106656
2156, epoch_train_loss=0.0018844727567106656
Epoch 2157
Epoch 2157 :: Batch 0/1
Batch Loss = 0.001884514504988978
2157, epoch_train_loss=0.001884514504988978
Epoch 2158
Epoch 2158 :: Batch 0/1
Batch Loss = 0.00188461055014232
2158, epoch_train_loss=0.00188461055014232
Epoch 2159
Epoch 2159 :: Batch 0/1
Batch Loss = 0.00188479077333422
2159, epoch_train_loss=0.00188479077333422
Epoch 2160
Epoch 2160 :: Batch 0/1
Batch Loss = 0.001885093945807371
2160, epoch_train_loss=0.001885093945807371
Epoch 2161
Epoch 2161 :: Batch 0/1
Batch Loss = 0.0018855930517213476
2161, epoch_train_loss=0.0018855930517213476
Epoch 2162
Epoch 2162 :: Batch 0/1
Batch Loss = 0.0018863810490539867
2162, epoch_train_loss=0.0018863810490539867
Epoch 2163
Epoch 2163 :: Batch 0/1
Batch Loss = 0.0018876437943862662
2163, epoch_train_loss=0.0018876437943862662
Epoch 2164
Epoch 2164 :: Batch 0/1
Batch Loss = 0.0018896083720186133
2164, epoch_train_loss=0.0018896083720186133
Epoch 2165
Epoch 2165 :: Batch 0/1
Batch Loss = 0.0018927684045305058
2165, epoch_train_loss=0.0018927684045305058
Epoch 2166
Epoch 2166 :: Batch 0/1
Batch Loss = 0.0018976840448020687
2166, epoch_train_loss=0.0018976840448020687
Epoch 2167
Epoch 2167 :: Batch 0/1
Batch Loss = 0.001905726862682255
2167, epoch_train_loss=0.001905726862682255
Epoch 2168
Epoch 2168 :: Batch 0/1
Batch Loss = 0.0019182702959677617
2168, epoch_train_loss=0.0019182702959677617
Epoch 2169
Epoch 2169 :: Batch 0/1
Batch Loss = 0.0019393198129943907
2169, epoch_train_loss=0.0019393198129943907
Epoch 2170
Epoch 2170 :: Batch 0/1
Batch Loss = 0.0019721062756023187
2170, epoch_train_loss=0.0019721062756023187
Epoch 2171
Epoch 2171 :: Batch 0/1
Batch Loss = 0.0020289776509792995
2171, epoch_train_loss=0.0020289776509792995
Epoch 2172
Epoch 2172 :: Batch 0/1
Batch Loss = 0.0021165235400312107
2172, epoch_train_loss=0.0021165235400312107
Epoch 2173
Epoch 2173 :: Batch 0/1
Batch Loss = 0.002275048031638437
2173, epoch_train_loss=0.002275048031638437
Epoch 2174
Epoch 2174 :: Batch 0/1
Batch Loss = 0.002511241353294247
2174, epoch_train_loss=0.002511241353294247
Epoch 2175
Epoch 2175 :: Batch 0/1
Batch Loss = 0.002963896609665061
2175, epoch_train_loss=0.002963896609665061
Epoch 2176
Epoch 2176 :: Batch 0/1
Batch Loss = 0.0035896190872293935
2176, epoch_train_loss=0.0035896190872293935
Epoch 2177
Epoch 2177 :: Batch 0/1
Batch Loss = 0.004881927868685866
2177, epoch_train_loss=0.004881927868685866
Epoch 2178
Epoch 2178 :: Batch 0/1
Batch Loss = 0.006387677073594115
2178, epoch_train_loss=0.006387677073594115
Epoch 2179
Epoch 2179 :: Batch 0/1
Batch Loss = 0.009811542220067955
2179, epoch_train_loss=0.009811542220067955
Epoch 2180
Epoch 2180 :: Batch 0/1
Batch Loss = 0.012371503705133664
2180, epoch_train_loss=0.012371503705133664
Epoch 2181
Epoch 2181 :: Batch 0/1
Batch Loss = 0.019105767321212816
2181, epoch_train_loss=0.019105767321212816
Epoch 2182
Epoch 2182 :: Batch 0/1
Batch Loss = 0.01911548766941648
2182, epoch_train_loss=0.01911548766941648
Epoch 2183
Epoch 2183 :: Batch 0/1
Batch Loss = 0.023416290557618864
2183, epoch_train_loss=0.023416290557618864
Epoch 2184
Epoch 2184 :: Batch 0/1
Batch Loss = 0.015010234314296383
2184, epoch_train_loss=0.015010234314296383
Epoch 2185
Epoch 2185 :: Batch 0/1
Batch Loss = 0.009272078905512397
2185, epoch_train_loss=0.009272078905512397
Epoch 2186
Epoch 2186 :: Batch 0/1
Batch Loss = 0.003222988519784696
2186, epoch_train_loss=0.003222988519784696
Epoch 2187
Epoch 2187 :: Batch 0/1
Batch Loss = 0.00230730306687336
2187, epoch_train_loss=0.00230730306687336
Epoch 2188
Epoch 2188 :: Batch 0/1
Batch Loss = 0.005492831186993704
2188, epoch_train_loss=0.005492831186993704
Epoch 2189
Epoch 2189 :: Batch 0/1
Batch Loss = 0.008483476565026953
2189, epoch_train_loss=0.008483476565026953
Epoch 2190
Epoch 2190 :: Batch 0/1
Batch Loss = 0.010337094122638175
2190, epoch_train_loss=0.010337094122638175
Epoch 2191
Epoch 2191 :: Batch 0/1
Batch Loss = 0.006542947775223238
2191, epoch_train_loss=0.006542947775223238
Epoch 2192
Epoch 2192 :: Batch 0/1
Batch Loss = 0.003214994365753739
2192, epoch_train_loss=0.003214994365753739
Epoch 2193
Epoch 2193 :: Batch 0/1
Batch Loss = 0.0020236104994008487
2193, epoch_train_loss=0.0020236104994008487
Epoch 2194
Epoch 2194 :: Batch 0/1
Batch Loss = 0.0036132604597672287
2194, epoch_train_loss=0.0036132604597672287
Epoch 2195
Epoch 2195 :: Batch 0/1
Batch Loss = 0.0060175491927090975
2195, epoch_train_loss=0.0060175491927090975
Epoch 2196
Epoch 2196 :: Batch 0/1
Batch Loss = 0.00585320723771457
2196, epoch_train_loss=0.00585320723771457
Epoch 2197
Epoch 2197 :: Batch 0/1
Batch Loss = 0.0044031789486099265
2197, epoch_train_loss=0.0044031789486099265
Epoch 2198
Epoch 2198 :: Batch 0/1
Batch Loss = 0.0025316347604626152
2198, epoch_train_loss=0.0025316347604626152
Epoch 2199
Epoch 2199 :: Batch 0/1
Batch Loss = 0.0023740604286118746
2199, epoch_train_loss=0.0023740604286118746
Epoch 2200
Epoch 2200 :: Batch 0/1
Batch Loss = 0.0036473525984594105
2200, epoch_train_loss=0.0036473525984594105
Epoch 2201
Epoch 2201 :: Batch 0/1
Batch Loss = 0.004241339803619333
2201, epoch_train_loss=0.004241339803619333
Epoch 2202
Epoch 2202 :: Batch 0/1
Batch Loss = 0.003809348842007846
2202, epoch_train_loss=0.003809348842007846
Epoch 2203
Epoch 2203 :: Batch 0/1
Batch Loss = 0.0024722052513528807
2203, epoch_train_loss=0.0024722052513528807
Epoch 2204
Epoch 2204 :: Batch 0/1
Batch Loss = 0.0019608351915404157
2204, epoch_train_loss=0.0019608351915404157
Epoch 2205
Epoch 2205 :: Batch 0/1
Batch Loss = 0.00252244819220637
2205, epoch_train_loss=0.00252244819220637
Epoch 2206
Epoch 2206 :: Batch 0/1
Batch Loss = 0.0031748220606948574
2206, epoch_train_loss=0.0031748220606948574
Epoch 2207
Epoch 2207 :: Batch 0/1
Batch Loss = 0.0032200217688394452
2207, epoch_train_loss=0.0032200217688394452
Epoch 2208
Epoch 2208 :: Batch 0/1
Batch Loss = 0.0024742887655364726
2208, epoch_train_loss=0.0024742887655364726
Epoch 2209
Epoch 2209 :: Batch 0/1
Batch Loss = 0.0019620460698524926
2209, epoch_train_loss=0.0019620460698524926
Epoch 2210
Epoch 2210 :: Batch 0/1
Batch Loss = 0.0021090772161478084
2210, epoch_train_loss=0.0021090772161478084
Epoch 2211
Epoch 2211 :: Batch 0/1
Batch Loss = 0.0025643371661562167
2211, epoch_train_loss=0.0025643371661562167
Epoch 2212
Epoch 2212 :: Batch 0/1
Batch Loss = 0.0027677955255422993
2212, epoch_train_loss=0.0027677955255422993
Epoch 2213
Epoch 2213 :: Batch 0/1
Batch Loss = 0.0024296732735433913
2213, epoch_train_loss=0.0024296732735433913
Epoch 2214
Epoch 2214 :: Batch 0/1
Batch Loss = 0.0020386700143522752
2214, epoch_train_loss=0.0020386700143522752
Epoch 2215
Epoch 2215 :: Batch 0/1
Batch Loss = 0.0019461086138897153
2215, epoch_train_loss=0.0019461086138897153
Epoch 2216
Epoch 2216 :: Batch 0/1
Batch Loss = 0.0021668208273393753
2216, epoch_train_loss=0.0021668208273393753
Epoch 2217
Epoch 2217 :: Batch 0/1
Batch Loss = 0.0023858149238349087
2217, epoch_train_loss=0.0023858149238349087
Epoch 2218
Epoch 2218 :: Batch 0/1
Batch Loss = 0.0023124057742897686
2218, epoch_train_loss=0.0023124057742897686
Epoch 2219
Epoch 2219 :: Batch 0/1
Batch Loss = 0.002089292558207757
2219, epoch_train_loss=0.002089292558207757
Epoch 2220
Epoch 2220 :: Batch 0/1
Batch Loss = 0.0019280527786858233
2220, epoch_train_loss=0.0019280527786858233
Epoch 2221
Epoch 2221 :: Batch 0/1
Batch Loss = 0.0019710991332558014
2221, epoch_train_loss=0.0019710991332558014
Epoch 2222
Epoch 2222 :: Batch 0/1
Batch Loss = 0.0021223267591842527
2222, epoch_train_loss=0.0021223267591842527
Epoch 2223
Epoch 2223 :: Batch 0/1
Batch Loss = 0.0021772567151094563
2223, epoch_train_loss=0.0021772567151094563
Epoch 2224
Epoch 2224 :: Batch 0/1
Batch Loss = 0.002096221640618597
2224, epoch_train_loss=0.002096221640618597
Epoch 2225
Epoch 2225 :: Batch 0/1
Batch Loss = 0.0019615788315387687
2225, epoch_train_loss=0.0019615788315387687
Epoch 2226
Epoch 2226 :: Batch 0/1
Batch Loss = 0.001911333748167616
2226, epoch_train_loss=0.001911333748167616
Epoch 2227
Epoch 2227 :: Batch 0/1
Batch Loss = 0.0019663891364023536
2227, epoch_train_loss=0.0019663891364023536
Epoch 2228
Epoch 2228 :: Batch 0/1
Batch Loss = 0.0020405537980009124
2228, epoch_train_loss=0.0020405537980009124
Epoch 2229
Epoch 2229 :: Batch 0/1
Batch Loss = 0.002054005975593572
2229, epoch_train_loss=0.002054005975593572
Epoch 2230
Epoch 2230 :: Batch 0/1
Batch Loss = 0.0019941133800627326
2230, epoch_train_loss=0.0019941133800627326
Epoch 2231
Epoch 2231 :: Batch 0/1
Batch Loss = 0.00192950090531106
2231, epoch_train_loss=0.00192950090531106
Epoch 2232
Epoch 2232 :: Batch 0/1
Batch Loss = 0.0019085496513736495
2232, epoch_train_loss=0.0019085496513736495
Epoch 2233
Epoch 2233 :: Batch 0/1
Batch Loss = 0.0019355991233165053
2233, epoch_train_loss=0.0019355991233165053
Epoch 2234
Epoch 2234 :: Batch 0/1
Batch Loss = 0.001972518309750125
2234, epoch_train_loss=0.001972518309750125
Epoch 2235
Epoch 2235 :: Batch 0/1
Batch Loss = 0.00197897127464854
2235, epoch_train_loss=0.00197897127464854
Epoch 2236
Epoch 2236 :: Batch 0/1
Batch Loss = 0.001955585122080402
2236, epoch_train_loss=0.001955585122080402
Epoch 2237
Epoch 2237 :: Batch 0/1
Batch Loss = 0.0019208881535291264
2237, epoch_train_loss=0.0019208881535291264
Epoch 2238
Epoch 2238 :: Batch 0/1
Batch Loss = 0.0019023303083864091
2238, epoch_train_loss=0.0019023303083864091
Epoch 2239
Epoch 2239 :: Batch 0/1
Batch Loss = 0.0019085308118448065
2239, epoch_train_loss=0.0019085308118448065
Epoch 2240
Epoch 2240 :: Batch 0/1
Batch Loss = 0.0019261686795851132
2240, epoch_train_loss=0.0019261686795851132
Epoch 2241
Epoch 2241 :: Batch 0/1
Batch Loss = 0.0019377722081427514
2241, epoch_train_loss=0.0019377722081427514
Epoch 2242
Epoch 2242 :: Batch 0/1
Batch Loss = 0.0019328758236000746
2242, epoch_train_loss=0.0019328758236000746
Epoch 2243
Epoch 2243 :: Batch 0/1
Batch Loss = 0.0019162986132380151
2243, epoch_train_loss=0.0019162986132380151
Epoch 2244
Epoch 2244 :: Batch 0/1
Batch Loss = 0.001899924326876266
2244, epoch_train_loss=0.001899924326876266
Epoch 2245
Epoch 2245 :: Batch 0/1
Batch Loss = 0.0018947219850557109
2245, epoch_train_loss=0.0018947219850557109
Epoch 2246
Epoch 2246 :: Batch 0/1
Batch Loss = 0.0019009917169151367
2246, epoch_train_loss=0.0019009917169151367
Epoch 2247
Epoch 2247 :: Batch 0/1
Batch Loss = 0.0019109987093950227
2247, epoch_train_loss=0.0019109987093950227
Epoch 2248
Epoch 2248 :: Batch 0/1
Batch Loss = 0.001915441757805652
2248, epoch_train_loss=0.001915441757805652
Epoch 2249
Epoch 2249 :: Batch 0/1
Batch Loss = 0.00191009711246666
2249, epoch_train_loss=0.00191009711246666
Epoch 2250
Epoch 2250 :: Batch 0/1
Batch Loss = 0.0019001791338354558
2250, epoch_train_loss=0.0019001791338354558
Epoch 2251
Epoch 2251 :: Batch 0/1
Batch Loss = 0.0018922997275315327
2251, epoch_train_loss=0.0018922997275315327
Epoch 2252
Epoch 2252 :: Batch 0/1
Batch Loss = 0.0018906423269881047
2252, epoch_train_loss=0.0018906423269881047
Epoch 2253
Epoch 2253 :: Batch 0/1
Batch Loss = 0.0018944201297829643
2253, epoch_train_loss=0.0018944201297829643
Epoch 2254
Epoch 2254 :: Batch 0/1
Batch Loss = 0.0018990346816085504
2254, epoch_train_loss=0.0018990346816085504
Epoch 2255
Epoch 2255 :: Batch 0/1
Batch Loss = 0.0019006971990389028
2255, epoch_train_loss=0.0019006971990389028
Epoch 2256
Epoch 2256 :: Batch 0/1
Batch Loss = 0.0018983462153845689
2256, epoch_train_loss=0.0018983462153845689
Epoch 2257
Epoch 2257 :: Batch 0/1
Batch Loss = 0.0018938624754793634
2257, epoch_train_loss=0.0018938624754793634
Epoch 2258
Epoch 2258 :: Batch 0/1
Batch Loss = 0.0018899571133266656
2258, epoch_train_loss=0.0018899571133266656
Epoch 2259
Epoch 2259 :: Batch 0/1
Batch Loss = 0.0018883425501652967
2259, epoch_train_loss=0.0018883425501652967
Epoch 2260
Epoch 2260 :: Batch 0/1
Batch Loss = 0.0018888563132839814
2260, epoch_train_loss=0.0018888563132839814
Epoch 2261
Epoch 2261 :: Batch 0/1
Batch Loss = 0.0018904948710336273
2261, epoch_train_loss=0.0018904948710336273
Epoch 2262
Epoch 2262 :: Batch 0/1
Batch Loss = 0.00189186963258493
2262, epoch_train_loss=0.00189186963258493
Epoch 2263
Epoch 2263 :: Batch 0/1
Batch Loss = 0.0018919535980352392
2263, epoch_train_loss=0.0018919535980352392
Epoch 2264
Epoch 2264 :: Batch 0/1
Batch Loss = 0.0018907386377009887
2264, epoch_train_loss=0.0018907386377009887
Epoch 2265
Epoch 2265 :: Batch 0/1
Batch Loss = 0.001888684338350637
2265, epoch_train_loss=0.001888684338350637
Epoch 2266
Epoch 2266 :: Batch 0/1
Batch Loss = 0.0018867940895357983
2266, epoch_train_loss=0.0018867940895357983
Epoch 2267
Epoch 2267 :: Batch 0/1
Batch Loss = 0.0018858168891465864
2267, epoch_train_loss=0.0018858168891465864
Epoch 2268
Epoch 2268 :: Batch 0/1
Batch Loss = 0.0018858654120332797
2268, epoch_train_loss=0.0018858654120332797
Epoch 2269
Epoch 2269 :: Batch 0/1
Batch Loss = 0.001886586156080286
2269, epoch_train_loss=0.001886586156080286
Epoch 2270
Epoch 2270 :: Batch 0/1
Batch Loss = 0.0018872976493063255
2270, epoch_train_loss=0.0018872976493063255
Epoch 2271
Epoch 2271 :: Batch 0/1
Batch Loss = 0.0018874334767576492
2271, epoch_train_loss=0.0018874334767576492
Epoch 2272
Epoch 2272 :: Batch 0/1
Batch Loss = 0.001886877780688241
2272, epoch_train_loss=0.001886877780688241
Epoch 2273
Epoch 2273 :: Batch 0/1
Batch Loss = 0.0018859070579444576
2273, epoch_train_loss=0.0018859070579444576
Epoch 2274
Epoch 2274 :: Batch 0/1
Batch Loss = 0.0018849185293104112
2274, epoch_train_loss=0.0018849185293104112
Epoch 2275
Epoch 2275 :: Batch 0/1
Batch Loss = 0.0018842546619636448
2275, epoch_train_loss=0.0018842546619636448
Epoch 2276
Epoch 2276 :: Batch 0/1
Batch Loss = 0.001883980004881481
2276, epoch_train_loss=0.001883980004881481
Epoch 2277
Epoch 2277 :: Batch 0/1
Batch Loss = 0.0018840136885075662
2277, epoch_train_loss=0.0018840136885075662
Epoch 2278
Epoch 2278 :: Batch 0/1
Batch Loss = 0.001884182730543219
2278, epoch_train_loss=0.001884182730543219
Epoch 2279
Epoch 2279 :: Batch 0/1
Batch Loss = 0.001884291127596673
2279, epoch_train_loss=0.001884291127596673
Epoch 2280
Epoch 2280 :: Batch 0/1
Batch Loss = 0.0018842554083171273
2280, epoch_train_loss=0.0018842554083171273
Epoch 2281
Epoch 2281 :: Batch 0/1
Batch Loss = 0.00188403939021247
2281, epoch_train_loss=0.00188403939021247
Epoch 2282
Epoch 2282 :: Batch 0/1
Batch Loss = 0.001883675799566287
2282, epoch_train_loss=0.001883675799566287
Epoch 2283
Epoch 2283 :: Batch 0/1
Batch Loss = 0.0018832436593024702
2283, epoch_train_loss=0.0018832436593024702
Epoch 2284
Epoch 2284 :: Batch 0/1
Batch Loss = 0.0018828292916440678
2284, epoch_train_loss=0.0018828292916440678
Epoch 2285
Epoch 2285 :: Batch 0/1
Batch Loss = 0.001882513277387541
2285, epoch_train_loss=0.001882513277387541
Epoch 2286
Epoch 2286 :: Batch 0/1
Batch Loss = 0.001882334859641002
2286, epoch_train_loss=0.001882334859641002
Epoch 2287
Epoch 2287 :: Batch 0/1
Batch Loss = 0.0018822665849023154
2287, epoch_train_loss=0.0018822665849023154
Epoch 2288
Epoch 2288 :: Batch 0/1
Batch Loss = 0.0018822606575292498
2288, epoch_train_loss=0.0018822606575292498
Epoch 2289
Epoch 2289 :: Batch 0/1
Batch Loss = 0.0018822541669760776
2289, epoch_train_loss=0.0018822541669760776
Epoch 2290
Epoch 2290 :: Batch 0/1
Batch Loss = 0.001882194294503155
2290, epoch_train_loss=0.001882194294503155
Epoch 2291
Epoch 2291 :: Batch 0/1
Batch Loss = 0.0018820708604205649
2291, epoch_train_loss=0.0018820708604205649
Epoch 2292
Epoch 2292 :: Batch 0/1
Batch Loss = 0.0018818906347086155
2292, epoch_train_loss=0.0018818906347086155
Epoch 2293
Epoch 2293 :: Batch 0/1
Batch Loss = 0.0018816815630297475
2293, epoch_train_loss=0.0018816815630297475
Epoch 2294
Epoch 2294 :: Batch 0/1
Batch Loss = 0.001881469339703248
2294, epoch_train_loss=0.001881469339703248
Epoch 2295
Epoch 2295 :: Batch 0/1
Batch Loss = 0.001881268809168632
2295, epoch_train_loss=0.001881268809168632
Epoch 2296
Epoch 2296 :: Batch 0/1
Batch Loss = 0.0018810926057111353
2296, epoch_train_loss=0.0018810926057111353
Epoch 2297
Epoch 2297 :: Batch 0/1
Batch Loss = 0.0018809450279431044
2297, epoch_train_loss=0.0018809450279431044
Epoch 2298
Epoch 2298 :: Batch 0/1
Batch Loss = 0.0018808230318703768
2298, epoch_train_loss=0.0018808230318703768
Epoch 2299
Epoch 2299 :: Batch 0/1
Batch Loss = 0.0018807253074992617
2299, epoch_train_loss=0.0018807253074992617
Epoch 2300
Epoch 2300 :: Batch 0/1
Batch Loss = 0.0018806438582292487
2300, epoch_train_loss=0.0018806438582292487
Epoch 2301
Epoch 2301 :: Batch 0/1
Batch Loss = 0.0018805692062219424
2301, epoch_train_loss=0.0018805692062219424
Epoch 2302
Epoch 2302 :: Batch 0/1
Batch Loss = 0.001880492777280987
2302, epoch_train_loss=0.001880492777280987
Epoch 2303
Epoch 2303 :: Batch 0/1
Batch Loss = 0.0018804057414845625
2303, epoch_train_loss=0.0018804057414845625
Epoch 2304
Epoch 2304 :: Batch 0/1
Batch Loss = 0.0018803073571171827
2304, epoch_train_loss=0.0018803073571171827
Epoch 2305
Epoch 2305 :: Batch 0/1
Batch Loss = 0.0018801981858812629
2305, epoch_train_loss=0.0018801981858812629
Epoch 2306
Epoch 2306 :: Batch 0/1
Batch Loss = 0.0018800814395381264
2306, epoch_train_loss=0.0018800814395381264
Epoch 2307
Epoch 2307 :: Batch 0/1
Batch Loss = 0.0018799621530787305
2307, epoch_train_loss=0.0018799621530787305
Epoch 2308
Epoch 2308 :: Batch 0/1
Batch Loss = 0.0018798434305456787
2308, epoch_train_loss=0.0018798434305456787
Epoch 2309
Epoch 2309 :: Batch 0/1
Batch Loss = 0.0018797267258585113
2309, epoch_train_loss=0.0018797267258585113
Epoch 2310
Epoch 2310 :: Batch 0/1
Batch Loss = 0.0018796133551238678
2310, epoch_train_loss=0.0018796133551238678
Epoch 2311
Epoch 2311 :: Batch 0/1
Batch Loss = 0.0018795029704668361
2311, epoch_train_loss=0.0018795029704668361
Epoch 2312
Epoch 2312 :: Batch 0/1
Batch Loss = 0.0018793966604485114
2312, epoch_train_loss=0.0018793966604485114
Epoch 2313
Epoch 2313 :: Batch 0/1
Batch Loss = 0.0018792945223995885
2313, epoch_train_loss=0.0018792945223995885
Epoch 2314
Epoch 2314 :: Batch 0/1
Batch Loss = 0.001879196529329784
2314, epoch_train_loss=0.001879196529329784
Epoch 2315
Epoch 2315 :: Batch 0/1
Batch Loss = 0.0018791031130691422
2315, epoch_train_loss=0.0018791031130691422
Epoch 2316
Epoch 2316 :: Batch 0/1
Batch Loss = 0.0018790133488830972
2316, epoch_train_loss=0.0018790133488830972
Epoch 2317
Epoch 2317 :: Batch 0/1
Batch Loss = 0.0018789263520417108
2317, epoch_train_loss=0.0018789263520417108
Epoch 2318
Epoch 2318 :: Batch 0/1
Batch Loss = 0.0018788414153431605
2318, epoch_train_loss=0.0018788414153431605
Epoch 2319
Epoch 2319 :: Batch 0/1
Batch Loss = 0.0018787577300065064
2319, epoch_train_loss=0.0018787577300065064
Epoch 2320
Epoch 2320 :: Batch 0/1
Batch Loss = 0.0018786752495676827
2320, epoch_train_loss=0.0018786752495676827
Epoch 2321
Epoch 2321 :: Batch 0/1
Batch Loss = 0.0018785938547412845
2321, epoch_train_loss=0.0018785938547412845
Epoch 2322
Epoch 2322 :: Batch 0/1
Batch Loss = 0.0018785136451545737
2322, epoch_train_loss=0.0018785136451545737
Epoch 2323
Epoch 2323 :: Batch 0/1
Batch Loss = 0.0018784350906561852
2323, epoch_train_loss=0.0018784350906561852
Epoch 2324
Epoch 2324 :: Batch 0/1
Batch Loss = 0.0018783580497529464
2324, epoch_train_loss=0.0018783580497529464
Epoch 2325
Epoch 2325 :: Batch 0/1
Batch Loss = 0.0018782827602805883
2325, epoch_train_loss=0.0018782827602805883
Epoch 2326
Epoch 2326 :: Batch 0/1
Batch Loss = 0.0018782093288960144
2326, epoch_train_loss=0.0018782093288960144
Epoch 2327
Epoch 2327 :: Batch 0/1
Batch Loss = 0.0018781380729055326
2327, epoch_train_loss=0.0018781380729055326
Epoch 2328
Epoch 2328 :: Batch 0/1
Batch Loss = 0.0018780694538875643
2328, epoch_train_loss=0.0018780694538875643
Epoch 2329
Epoch 2329 :: Batch 0/1
Batch Loss = 0.0018780043414255818
2329, epoch_train_loss=0.0018780043414255818
Epoch 2330
Epoch 2330 :: Batch 0/1
Batch Loss = 0.0018779437104860252
2330, epoch_train_loss=0.0018779437104860252
Epoch 2331
Epoch 2331 :: Batch 0/1
Batch Loss = 0.001877889439070553
2331, epoch_train_loss=0.001877889439070553
Epoch 2332
Epoch 2332 :: Batch 0/1
Batch Loss = 0.0018778432952758065
2332, epoch_train_loss=0.0018778432952758065
Epoch 2333
Epoch 2333 :: Batch 0/1
Batch Loss = 0.0018778087745172209
2333, epoch_train_loss=0.0018778087745172209
Epoch 2334
Epoch 2334 :: Batch 0/1
Batch Loss = 0.0018777897237595134
2334, epoch_train_loss=0.0018777897237595134
Epoch 2335
Epoch 2335 :: Batch 0/1
Batch Loss = 0.0018777931812706404
2335, epoch_train_loss=0.0018777931812706404
Epoch 2336
Epoch 2336 :: Batch 0/1
Batch Loss = 0.0018778275046945766
2336, epoch_train_loss=0.0018778275046945766
Epoch 2337
Epoch 2337 :: Batch 0/1
Batch Loss = 0.001877908085398995
2337, epoch_train_loss=0.001877908085398995
Epoch 2338
Epoch 2338 :: Batch 0/1
Batch Loss = 0.001878053410529449
2338, epoch_train_loss=0.001878053410529449
Epoch 2339
Epoch 2339 :: Batch 0/1
Batch Loss = 0.0018782984386243532
2339, epoch_train_loss=0.0018782984386243532
Epoch 2340
Epoch 2340 :: Batch 0/1
Batch Loss = 0.0018786848269636786
2340, epoch_train_loss=0.0018786848269636786
Epoch 2341
Epoch 2341 :: Batch 0/1
Batch Loss = 0.0018792951161035072
2341, epoch_train_loss=0.0018792951161035072
Epoch 2342
Epoch 2342 :: Batch 0/1
Batch Loss = 0.001880225426543723
2342, epoch_train_loss=0.001880225426543723
Epoch 2343
Epoch 2343 :: Batch 0/1
Batch Loss = 0.0018816786584954944
2343, epoch_train_loss=0.0018816786584954944
Epoch 2344
Epoch 2344 :: Batch 0/1
Batch Loss = 0.0018838799845889629
2344, epoch_train_loss=0.0018838799845889629
Epoch 2345
Epoch 2345 :: Batch 0/1
Batch Loss = 0.0018873505434354085
2345, epoch_train_loss=0.0018873505434354085
Epoch 2346
Epoch 2346 :: Batch 0/1
Batch Loss = 0.0018926213919107187
2346, epoch_train_loss=0.0018926213919107187
Epoch 2347
Epoch 2347 :: Batch 0/1
Batch Loss = 0.0019010949254142932
2347, epoch_train_loss=0.0019010949254142932
Epoch 2348
Epoch 2348 :: Batch 0/1
Batch Loss = 0.0019140088999883363
2348, epoch_train_loss=0.0019140088999883363
Epoch 2349
Epoch 2349 :: Batch 0/1
Batch Loss = 0.0019353293207369866
2349, epoch_train_loss=0.0019353293207369866
Epoch 2350
Epoch 2350 :: Batch 0/1
Batch Loss = 0.0019677862515301574
2350, epoch_train_loss=0.0019677862515301574
Epoch 2351
Epoch 2351 :: Batch 0/1
Batch Loss = 0.0020232146312376747
2351, epoch_train_loss=0.0020232146312376747
Epoch 2352
Epoch 2352 :: Batch 0/1
Batch Loss = 0.0021066187023654945
2352, epoch_train_loss=0.0021066187023654945
Epoch 2353
Epoch 2353 :: Batch 0/1
Batch Loss = 0.002255339918108323
2353, epoch_train_loss=0.002255339918108323
Epoch 2354
Epoch 2354 :: Batch 0/1
Batch Loss = 0.002472085532060657
2354, epoch_train_loss=0.002472085532060657
Epoch 2355
Epoch 2355 :: Batch 0/1
Batch Loss = 0.002880909329281688
2355, epoch_train_loss=0.002880909329281688
Epoch 2356
Epoch 2356 :: Batch 0/1
Batch Loss = 0.0034352774650744065
2356, epoch_train_loss=0.0034352774650744065
Epoch 2357
Epoch 2357 :: Batch 0/1
Batch Loss = 0.004560493508297083
2357, epoch_train_loss=0.004560493508297083
Epoch 2358
Epoch 2358 :: Batch 0/1
Batch Loss = 0.005860030231469195
2358, epoch_train_loss=0.005860030231469195
Epoch 2359
Epoch 2359 :: Batch 0/1
Batch Loss = 0.008758032827618424
2359, epoch_train_loss=0.008758032827618424
Epoch 2360
Epoch 2360 :: Batch 0/1
Batch Loss = 0.011000508765645943
2360, epoch_train_loss=0.011000508765645943
Epoch 2361
Epoch 2361 :: Batch 0/1
Batch Loss = 0.01674661953966741
2361, epoch_train_loss=0.01674661953966741
Epoch 2362
Epoch 2362 :: Batch 0/1
Batch Loss = 0.01729310830848741
2362, epoch_train_loss=0.01729310830848741
Epoch 2363
Epoch 2363 :: Batch 0/1
Batch Loss = 0.0218227939264093
2363, epoch_train_loss=0.0218227939264093
Epoch 2364
Epoch 2364 :: Batch 0/1
Batch Loss = 0.015197987279400063
2364, epoch_train_loss=0.015197987279400063
Epoch 2365
Epoch 2365 :: Batch 0/1
Batch Loss = 0.010830347196890336
2365, epoch_train_loss=0.010830347196890336
Epoch 2366
Epoch 2366 :: Batch 0/1
Batch Loss = 0.004312564246191081
2366, epoch_train_loss=0.004312564246191081
Epoch 2367
Epoch 2367 :: Batch 0/1
Batch Loss = 0.0020547983623477265
2367, epoch_train_loss=0.0020547983623477265
Epoch 2368
Epoch 2368 :: Batch 0/1
Batch Loss = 0.0038457554933508165
2368, epoch_train_loss=0.0038457554933508165
Epoch 2369
Epoch 2369 :: Batch 0/1
Batch Loss = 0.006967572043427888
2369, epoch_train_loss=0.006967572043427888
Epoch 2370
Epoch 2370 :: Batch 0/1
Batch Loss = 0.009811889769923808
2370, epoch_train_loss=0.009811889769923808
Epoch 2371
Epoch 2371 :: Batch 0/1
Batch Loss = 0.0075231721854109555
2371, epoch_train_loss=0.0075231721854109555
Epoch 2372
Epoch 2372 :: Batch 0/1
Batch Loss = 0.004770627902597027
2372, epoch_train_loss=0.004770627902597027
Epoch 2373
Epoch 2373 :: Batch 0/1
Batch Loss = 0.0023668150314425777
2373, epoch_train_loss=0.0023668150314425777
Epoch 2374
Epoch 2374 :: Batch 0/1
Batch Loss = 0.0026515466182766567
2374, epoch_train_loss=0.0026515466182766567
Epoch 2375
Epoch 2375 :: Batch 0/1
Batch Loss = 0.004634950994088471
2375, epoch_train_loss=0.004634950994088471
Epoch 2376
Epoch 2376 :: Batch 0/1
Batch Loss = 0.005715099578045012
2376, epoch_train_loss=0.005715099578045012
Epoch 2377
Epoch 2377 :: Batch 0/1
Batch Loss = 0.005384772214261545
2377, epoch_train_loss=0.005384772214261545
Epoch 2378
Epoch 2378 :: Batch 0/1
Batch Loss = 0.003283205793301084
2378, epoch_train_loss=0.003283205793301084
Epoch 2379
Epoch 2379 :: Batch 0/1
Batch Loss = 0.002009893378023529
2379, epoch_train_loss=0.002009893378023529
Epoch 2380
Epoch 2380 :: Batch 0/1
Batch Loss = 0.002338974669259327
2380, epoch_train_loss=0.002338974669259327
Epoch 2381
Epoch 2381 :: Batch 0/1
Batch Loss = 0.003422416781685204
2381, epoch_train_loss=0.003422416781685204
Epoch 2382
Epoch 2382 :: Batch 0/1
Batch Loss = 0.003985579379153998
2382, epoch_train_loss=0.003985579379153998
Epoch 2383
Epoch 2383 :: Batch 0/1
Batch Loss = 0.003156640014058631
2383, epoch_train_loss=0.003156640014058631
Epoch 2384
Epoch 2384 :: Batch 0/1
Batch Loss = 0.002216188244372576
2384, epoch_train_loss=0.002216188244372576
Epoch 2385
Epoch 2385 :: Batch 0/1
Batch Loss = 0.001997703668790037
2385, epoch_train_loss=0.001997703668790037
Epoch 2386
Epoch 2386 :: Batch 0/1
Batch Loss = 0.0025158195374889803
2386, epoch_train_loss=0.0025158195374889803
Epoch 2387
Epoch 2387 :: Batch 0/1
Batch Loss = 0.0029962258684572253
2387, epoch_train_loss=0.0029962258684572253
Epoch 2388
Epoch 2388 :: Batch 0/1
Batch Loss = 0.002786836140742965
2388, epoch_train_loss=0.002786836140742965
Epoch 2389
Epoch 2389 :: Batch 0/1
Batch Loss = 0.0023068625410522623
2389, epoch_train_loss=0.0023068625410522623
Epoch 2390
Epoch 2390 :: Batch 0/1
Batch Loss = 0.002008437640078258
2390, epoch_train_loss=0.002008437640078258
Epoch 2391
Epoch 2391 :: Batch 0/1
Batch Loss = 0.002133554162472901
2391, epoch_train_loss=0.002133554162472901
Epoch 2392
Epoch 2392 :: Batch 0/1
Batch Loss = 0.002388696867077585
2392, epoch_train_loss=0.002388696867077585
Epoch 2393
Epoch 2393 :: Batch 0/1
Batch Loss = 0.002453468368255207
2393, epoch_train_loss=0.002453468368255207
Epoch 2394
Epoch 2394 :: Batch 0/1
Batch Loss = 0.002329948148888683
2394, epoch_train_loss=0.002329948148888683
Epoch 2395
Epoch 2395 :: Batch 0/1
Batch Loss = 0.0021027889541770355
2395, epoch_train_loss=0.0021027889541770355
Epoch 2396
Epoch 2396 :: Batch 0/1
Batch Loss = 0.0019925046040572053
2396, epoch_train_loss=0.0019925046040572053
Epoch 2397
Epoch 2397 :: Batch 0/1
Batch Loss = 0.0020291941581450736
2397, epoch_train_loss=0.0020291941581450736
Epoch 2398
Epoch 2398 :: Batch 0/1
Batch Loss = 0.002149506448163756
2398, epoch_train_loss=0.002149506448163756
Epoch 2399
Epoch 2399 :: Batch 0/1
Batch Loss = 0.0022280288530322438
2399, epoch_train_loss=0.0022280288530322438
Epoch 2400
Epoch 2400 :: Batch 0/1
Batch Loss = 0.002139740309574015
2400, epoch_train_loss=0.002139740309574015
Epoch 2401
Epoch 2401 :: Batch 0/1
Batch Loss = 0.0019860128506431294
2401, epoch_train_loss=0.0019860128506431294
Epoch 2402
Epoch 2402 :: Batch 0/1
Batch Loss = 0.0019070937229013897
2402, epoch_train_loss=0.0019070937229013897
Epoch 2403
Epoch 2403 :: Batch 0/1
Batch Loss = 0.001970120043073962
2403, epoch_train_loss=0.001970120043073962
Epoch 2404
Epoch 2404 :: Batch 0/1
Batch Loss = 0.0020731052509566092
2404, epoch_train_loss=0.0020731052509566092
Epoch 2405
Epoch 2405 :: Batch 0/1
Batch Loss = 0.002079412749456319
2405, epoch_train_loss=0.002079412749456319
Epoch 2406
Epoch 2406 :: Batch 0/1
Batch Loss = 0.0020060272775270335
2406, epoch_train_loss=0.0020060272775270335
Epoch 2407
Epoch 2407 :: Batch 0/1
Batch Loss = 0.0019313336709639735
2407, epoch_train_loss=0.0019313336709639735
Epoch 2408
Epoch 2408 :: Batch 0/1
Batch Loss = 0.0019156780674054155
2408, epoch_train_loss=0.0019156780674054155
Epoch 2409
Epoch 2409 :: Batch 0/1
Batch Loss = 0.001941017151100868
2409, epoch_train_loss=0.001941017151100868
Epoch 2410
Epoch 2410 :: Batch 0/1
Batch Loss = 0.001968478524178245
2410, epoch_train_loss=0.001968478524178245
Epoch 2411
Epoch 2411 :: Batch 0/1
Batch Loss = 0.0019826650353479284
2411, epoch_train_loss=0.0019826650353479284
Epoch 2412
Epoch 2412 :: Batch 0/1
Batch Loss = 0.001969164787792003
2412, epoch_train_loss=0.001969164787792003
Epoch 2413
Epoch 2413 :: Batch 0/1
Batch Loss = 0.0019355070719614972
2413, epoch_train_loss=0.0019355070719614972
Epoch 2414
Epoch 2414 :: Batch 0/1
Batch Loss = 0.0019017700810737947
2414, epoch_train_loss=0.0019017700810737947
Epoch 2415
Epoch 2415 :: Batch 0/1
Batch Loss = 0.00189612886112209
2415, epoch_train_loss=0.00189612886112209
Epoch 2416
Epoch 2416 :: Batch 0/1
Batch Loss = 0.00191946106317253
2416, epoch_train_loss=0.00191946106317253
Epoch 2417
Epoch 2417 :: Batch 0/1
Batch Loss = 0.0019414232712090744
2417, epoch_train_loss=0.0019414232712090744
Epoch 2418
Epoch 2418 :: Batch 0/1
Batch Loss = 0.0019392560020861625
2418, epoch_train_loss=0.0019392560020861625
Epoch 2419
Epoch 2419 :: Batch 0/1
Batch Loss = 0.0019177089182328814
2419, epoch_train_loss=0.0019177089182328814
Epoch 2420
Epoch 2420 :: Batch 0/1
Batch Loss = 0.0018990583588187875
2420, epoch_train_loss=0.0018990583588187875
Epoch 2421
Epoch 2421 :: Batch 0/1
Batch Loss = 0.001893007245680525
2421, epoch_train_loss=0.001893007245680525
Epoch 2422
Epoch 2422 :: Batch 0/1
Batch Loss = 0.0018960965473606107
2422, epoch_train_loss=0.0018960965473606107
Epoch 2423
Epoch 2423 :: Batch 0/1
Batch Loss = 0.0019025715065613166
2423, epoch_train_loss=0.0019025715065613166
Epoch 2424
Epoch 2424 :: Batch 0/1
Batch Loss = 0.0019090494375205048
2424, epoch_train_loss=0.0019090494375205048
Epoch 2425
Epoch 2425 :: Batch 0/1
Batch Loss = 0.0019108400394281335
2425, epoch_train_loss=0.0019108400394281335
Epoch 2426
Epoch 2426 :: Batch 0/1
Batch Loss = 0.0019034368401208705
2426, epoch_train_loss=0.0019034368401208705
Epoch 2427
Epoch 2427 :: Batch 0/1
Batch Loss = 0.001891889961597147
2427, epoch_train_loss=0.001891889961597147
Epoch 2428
Epoch 2428 :: Batch 0/1
Batch Loss = 0.0018847912571454215
2428, epoch_train_loss=0.0018847912571454215
Epoch 2429
Epoch 2429 :: Batch 0/1
Batch Loss = 0.001886267912592312
2429, epoch_train_loss=0.001886267912592312
Epoch 2430
Epoch 2430 :: Batch 0/1
Batch Loss = 0.0018917647734195712
2430, epoch_train_loss=0.0018917647734195712
Epoch 2431
Epoch 2431 :: Batch 0/1
Batch Loss = 0.0018949605419683968
2431, epoch_train_loss=0.0018949605419683968
Epoch 2432
Epoch 2432 :: Batch 0/1
Batch Loss = 0.0018947101338864525
2432, epoch_train_loss=0.0018947101338864525
Epoch 2433
Epoch 2433 :: Batch 0/1
Batch Loss = 0.0018922456013511202
2433, epoch_train_loss=0.0018922456013511202
Epoch 2434
Epoch 2434 :: Batch 0/1
Batch Loss = 0.0018887283872231788
2434, epoch_train_loss=0.0018887283872231788
Epoch 2435
Epoch 2435 :: Batch 0/1
Batch Loss = 0.001884705436145893
2435, epoch_train_loss=0.001884705436145893
Epoch 2436
Epoch 2436 :: Batch 0/1
Batch Loss = 0.0018819845579353647
2436, epoch_train_loss=0.0018819845579353647
Epoch 2437
Epoch 2437 :: Batch 0/1
Batch Loss = 0.001882282190726985
2437, epoch_train_loss=0.001882282190726985
Epoch 2438
Epoch 2438 :: Batch 0/1
Batch Loss = 0.0018847286257721151
2438, epoch_train_loss=0.0018847286257721151
Epoch 2439
Epoch 2439 :: Batch 0/1
Batch Loss = 0.0018866406250121477
2439, epoch_train_loss=0.0018866406250121477
Epoch 2440
Epoch 2440 :: Batch 0/1
Batch Loss = 0.0018864525407048748
2440, epoch_train_loss=0.0018864525407048748
Epoch 2441
Epoch 2441 :: Batch 0/1
Batch Loss = 0.0018847990140700081
2441, epoch_train_loss=0.0018847990140700081
Epoch 2442
Epoch 2442 :: Batch 0/1
Batch Loss = 0.0018828713037661034
2442, epoch_train_loss=0.0018828713037661034
Epoch 2443
Epoch 2443 :: Batch 0/1
Batch Loss = 0.0018812955780990702
2443, epoch_train_loss=0.0018812955780990702
Epoch 2444
Epoch 2444 :: Batch 0/1
Batch Loss = 0.0018800910281108023
2444, epoch_train_loss=0.0018800910281108023
Epoch 2445
Epoch 2445 :: Batch 0/1
Batch Loss = 0.0018795539899636648
2445, epoch_train_loss=0.0018795539899636648
Epoch 2446
Epoch 2446 :: Batch 0/1
Batch Loss = 0.0018799113909379844
2446, epoch_train_loss=0.0018799113909379844
Epoch 2447
Epoch 2447 :: Batch 0/1
Batch Loss = 0.001880774137450236
2447, epoch_train_loss=0.001880774137450236
Epoch 2448
Epoch 2448 :: Batch 0/1
Batch Loss = 0.0018813359823611506
2448, epoch_train_loss=0.0018813359823611506
Epoch 2449
Epoch 2449 :: Batch 0/1
Batch Loss = 0.0018810471745825384
2449, epoch_train_loss=0.0018810471745825384
Epoch 2450
Epoch 2450 :: Batch 0/1
Batch Loss = 0.001880177248584103
2450, epoch_train_loss=0.001880177248584103
Epoch 2451
Epoch 2451 :: Batch 0/1
Batch Loss = 0.0018792556466133467
2451, epoch_train_loss=0.0018792556466133467
Epoch 2452
Epoch 2452 :: Batch 0/1
Batch Loss = 0.0018785389507133916
2452, epoch_train_loss=0.0018785389507133916
Epoch 2453
Epoch 2453 :: Batch 0/1
Batch Loss = 0.0018779871070053232
2453, epoch_train_loss=0.0018779871070053232
Epoch 2454
Epoch 2454 :: Batch 0/1
Batch Loss = 0.0018776022521330416
2454, epoch_train_loss=0.0018776022521330416
Epoch 2455
Epoch 2455 :: Batch 0/1
Batch Loss = 0.0018775075008388993
2455, epoch_train_loss=0.0018775075008388993
Epoch 2456
Epoch 2456 :: Batch 0/1
Batch Loss = 0.001877698078413665
2456, epoch_train_loss=0.001877698078413665
Epoch 2457
Epoch 2457 :: Batch 0/1
Batch Loss = 0.0018779205288763623
2457, epoch_train_loss=0.0018779205288763623
Epoch 2458
Epoch 2458 :: Batch 0/1
Batch Loss = 0.0018779226228113515
2458, epoch_train_loss=0.0018779226228113515
Epoch 2459
Epoch 2459 :: Batch 0/1
Batch Loss = 0.0018776789295971528
2459, epoch_train_loss=0.0018776789295971528
Epoch 2460
Epoch 2460 :: Batch 0/1
Batch Loss = 0.0018773135332682672
2460, epoch_train_loss=0.0018773135332682672
Epoch 2461
Epoch 2461 :: Batch 0/1
Batch Loss = 0.0018769520965477055
2461, epoch_train_loss=0.0018769520965477055
Epoch 2462
Epoch 2462 :: Batch 0/1
Batch Loss = 0.0018766043313637164
2462, epoch_train_loss=0.0018766043313637164
Epoch 2463
Epoch 2463 :: Batch 0/1
Batch Loss = 0.0018762654784182365
2463, epoch_train_loss=0.0018762654784182365
Epoch 2464
Epoch 2464 :: Batch 0/1
Batch Loss = 0.0018759772962265175
2464, epoch_train_loss=0.0018759772962265175
Epoch 2465
Epoch 2465 :: Batch 0/1
Batch Loss = 0.0018758008597895362
2465, epoch_train_loss=0.0018758008597895362
Epoch 2466
Epoch 2466 :: Batch 0/1
Batch Loss = 0.0018757452237712486
2466, epoch_train_loss=0.0018757452237712486
Epoch 2467
Epoch 2467 :: Batch 0/1
Batch Loss = 0.0018757387804219329
2467, epoch_train_loss=0.0018757387804219329
Epoch 2468
Epoch 2468 :: Batch 0/1
Batch Loss = 0.0018757017678186572
2468, epoch_train_loss=0.0018757017678186572
Epoch 2469
Epoch 2469 :: Batch 0/1
Batch Loss = 0.0018756150035226392
2469, epoch_train_loss=0.0018756150035226392
Epoch 2470
Epoch 2470 :: Batch 0/1
Batch Loss = 0.001875503894743855
2470, epoch_train_loss=0.001875503894743855
Epoch 2471
Epoch 2471 :: Batch 0/1
Batch Loss = 0.001875382267492624
2471, epoch_train_loss=0.001875382267492624
Epoch 2472
Epoch 2472 :: Batch 0/1
Batch Loss = 0.0018752372202490255
2472, epoch_train_loss=0.0018752372202490255
Epoch 2473
Epoch 2473 :: Batch 0/1
Batch Loss = 0.001875053928056109
2473, epoch_train_loss=0.001875053928056109
Epoch 2474
Epoch 2474 :: Batch 0/1
Batch Loss = 0.001874851240774438
2474, epoch_train_loss=0.001874851240774438
Epoch 2475
Epoch 2475 :: Batch 0/1
Batch Loss = 0.0018746588865064088
2475, epoch_train_loss=0.0018746588865064088
Epoch 2476
Epoch 2476 :: Batch 0/1
Batch Loss = 0.0018744966783218677
2476, epoch_train_loss=0.0018744966783218677
Epoch 2477
Epoch 2477 :: Batch 0/1
Batch Loss = 0.0018743607702641284
2477, epoch_train_loss=0.0018743607702641284
Epoch 2478
Epoch 2478 :: Batch 0/1
Batch Loss = 0.0018742362950980589
2478, epoch_train_loss=0.0018742362950980589
Epoch 2479
Epoch 2479 :: Batch 0/1
Batch Loss = 0.001874117538501533
2479, epoch_train_loss=0.001874117538501533
Epoch 2480
Epoch 2480 :: Batch 0/1
Batch Loss = 0.0018740102398110347
2480, epoch_train_loss=0.0018740102398110347
Epoch 2481
Epoch 2481 :: Batch 0/1
Batch Loss = 0.0018739200809413901
2481, epoch_train_loss=0.0018739200809413901
Epoch 2482
Epoch 2482 :: Batch 0/1
Batch Loss = 0.0018738419528530697
2482, epoch_train_loss=0.0018738419528530697
Epoch 2483
Epoch 2483 :: Batch 0/1
Batch Loss = 0.0018737631260164523
2483, epoch_train_loss=0.0018737631260164523
Epoch 2484
Epoch 2484 :: Batch 0/1
Batch Loss = 0.0018736769724446798
2484, epoch_train_loss=0.0018736769724446798
Epoch 2485
Epoch 2485 :: Batch 0/1
Batch Loss = 0.0018735866034183622
2485, epoch_train_loss=0.0018735866034183622
Epoch 2486
Epoch 2486 :: Batch 0/1
Batch Loss = 0.0018734969244778039
2486, epoch_train_loss=0.0018734969244778039
Epoch 2487
Epoch 2487 :: Batch 0/1
Batch Loss = 0.0018734100323014931
2487, epoch_train_loss=0.0018734100323014931
Epoch 2488
Epoch 2488 :: Batch 0/1
Batch Loss = 0.0018733221087472859
2488, epoch_train_loss=0.0018733221087472859
Epoch 2489
Epoch 2489 :: Batch 0/1
Batch Loss = 0.001873230538558377
2489, epoch_train_loss=0.001873230538558377
Epoch 2490
Epoch 2490 :: Batch 0/1
Batch Loss = 0.0018731352808038356
2490, epoch_train_loss=0.0018731352808038356
Epoch 2491
Epoch 2491 :: Batch 0/1
Batch Loss = 0.0018730403346260969
2491, epoch_train_loss=0.0018730403346260969
Epoch 2492
Epoch 2492 :: Batch 0/1
Batch Loss = 0.0018729489340710604
2492, epoch_train_loss=0.0018729489340710604
Epoch 2493
Epoch 2493 :: Batch 0/1
Batch Loss = 0.0018728614303706024
2493, epoch_train_loss=0.0018728614303706024
Epoch 2494
Epoch 2494 :: Batch 0/1
Batch Loss = 0.0018727753071369733
2494, epoch_train_loss=0.0018727753071369733
Epoch 2495
Epoch 2495 :: Batch 0/1
Batch Loss = 0.0018726901099666276
2495, epoch_train_loss=0.0018726901099666276
Epoch 2496
Epoch 2496 :: Batch 0/1
Batch Loss = 0.0018726066530556271
2496, epoch_train_loss=0.0018726066530556271
Epoch 2497
Epoch 2497 :: Batch 0/1
Batch Loss = 0.0018725279435559373
2497, epoch_train_loss=0.0018725279435559373
Epoch 2498
Epoch 2498 :: Batch 0/1
Batch Loss = 0.001872454938888012
2498, epoch_train_loss=0.001872454938888012
Epoch 2499
Epoch 2499 :: Batch 0/1
Batch Loss = 0.0018723886456018845
2499, epoch_train_loss=0.0018723886456018845
