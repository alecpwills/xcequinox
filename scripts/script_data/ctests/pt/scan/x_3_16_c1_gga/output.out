/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting "B3LYP_WITH_VWN5 = True" in pyscf_conf.py
  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01fbc40> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01fbc40> in UKS object of <class 'pyscf.dft.uks.UKS'>
<pyscf.gto.mole.Mole object at 0x7ffeb01fbc40> [['P', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb01fafe0> [['N', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb01fb910> [['H', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb01fbd00> [['Li', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb01fbf70> [['O', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb01fbee0> [['Cl', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb01f8f10> [['Al', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb01fb010> [['S', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb01fb1c0> [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb01fb400> [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb01f8850> [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb01f86d0> [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb01f82e0> [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb01f89a0> [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb01f9210> [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb01f8070> [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb01f9270> [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb01f9180> [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb01f8b80> [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb01f9de0> [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb01f9a50> [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb01f97b0> [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb01fae00> [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb01faa70> [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb01faa10> [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]] 5
<pyscf.gto.mole.Mole object at 0x7ffeb01f81c0> [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb01f8040> [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]] 5
mol:  [['P', array([0., 0., 0.])]]
converged SCF energy = -341.104145992717  <S^2> = 3.7502984  2S+1 = 4.0001492
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01fafe0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01fafe0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-3.48053232e-03 -9.22981806e-04 -2.09507924e-03 ... -1.11294850e+01
 -1.11294850e+01 -1.11294850e+01] = SCAN,
rho_a.shape=(6, 6320), rho_b.shape=(6, 6320)
fxc_a.shape=(6320,), fxc_b.shape=(6320,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12640), fxc.shape=(12640,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(2, 6320, 2)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12640, 2)
rho_filt.shape=(12640,)
mol:  [['N', array([0., 0., 0.])]]
converged SCF energy = -54.5289742046674  <S^2> = 3.7524945  2S+1 = 4.0012471
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01fb910> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01fb910> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5016, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.10256797e-03 -5.98013179e-04 -6.71209617e-05 ... -5.03581543e+00
 -5.03581543e+00 -5.03581543e+00] = SCAN,
rho_a.shape=(6, 5016), rho_b.shape=(6, 5016)
fxc_a.shape=(5016,), fxc_b.shape=(5016,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 10032), fxc.shape=(10032,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5016, 22)
rho shape (4, 4, 2, 5016)
rho_filt shape: (5016,)
get descriptors tdrho.shape=(2, 5016, 2)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(10032, 2)
rho_filt.shape=(10032,)
mol:  [['H', array([0., 0., 0.])]]
converged SCF energy = -0.499812984008539  <S^2> = 0.75  2S+1 = 2
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01fbd00> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01fbd00> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 7, 7)
ao.shape (10, 2440, 7)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.59173730e-03 -7.67300285e-04 -6.09330303e-05 ... -7.27736608e-01
 -7.27736608e-01 -7.27736608e-01] = SCAN,
rho_a.shape=(6, 2440), rho_b.shape=(6, 2440)
fxc_a.shape=(2440,), fxc_b.shape=(2440,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 2440), fxc.shape=(2440,)
get_data, dm shape = (2, 7, 7)
ao_eval.shape=(4, 2440, 7)
rho shape (4, 4, 2, 2440)
rho_filt shape: (2440,)
get descriptors tdrho.shape=(2, 2440, 2)
mol:  [['Li', array([0., 0., 0.])]]
converged SCF energy = -7.46006188627842  <S^2> = 0.75000049  2S+1 = 2.0000005
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01fbf70> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01fbf70> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 4592, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-6.71507910e-03 -1.45299376e-03 -1.45299376e-03 ... -1.46930969e-02
 -2.05021258e+00 -2.05021258e+00] = SCAN,
rho_a.shape=(6, 4592), rho_b.shape=(6, 4592)
fxc_a.shape=(4592,), fxc_b.shape=(4592,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 9184), fxc.shape=(9184,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 4592, 22)
rho shape (4, 4, 2, 4592)
rho_filt shape: (4592,)
get descriptors tdrho.shape=(2, 4592, 2)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(9184, 2)
rho_filt.shape=(9184,)
mol:  [['O', array([0., 0., 0.])]]
converged SCF energy = -75.0033779898163  <S^2> = 2.0027451  2S+1 = 3.0018295
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01fbee0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01fbee0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5040, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-7.82967892e-04 -1.36569375e-04 -6.87281809e-06 ... -5.78388653e+00
 -5.78388653e+00 -5.78388653e+00] = SCAN,
rho_a.shape=(6, 5040), rho_b.shape=(6, 5040)
fxc_a.shape=(5040,), fxc_b.shape=(5040,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 10080), fxc.shape=(10080,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5040, 22)
rho shape (4, 4, 2, 5040)
rho_filt shape: (5040,)
get descriptors tdrho.shape=(2, 5040, 2)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(10080, 2)
rho_filt.shape=(10080,)
mol:  [['Cl', array([0., 0., 0.])]]
converged SCF energy = -459.957577125462  <S^2> = 0.75161942  2S+1 = 2.0016188
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f8f10> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f8f10> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6152, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-5.04351627e-04 -9.37831486e-04 -3.26514884e-04 ... -1.26646370e+01
 -1.26646370e+01 -1.26646370e+01] = SCAN,
rho_a.shape=(6, 6152), rho_b.shape=(6, 6152)
fxc_a.shape=(6152,), fxc_b.shape=(6152,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12304), fxc.shape=(12304,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6152, 30)
rho shape (4, 4, 2, 6152)
rho_filt shape: (6152,)
get descriptors tdrho.shape=(2, 6152, 2)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12304, 2)
rho_filt.shape=(12304,)
mol:  [['Al', array([0., 0., 0.])]]
converged SCF energy = -242.226560996606  <S^2> = 0.75226414  2S+1 = 2.0022629
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01fb010> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01fb010> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6088, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.02206819 -0.01402453 -0.00706429 ... -0.00012851 -0.0014074
 -0.00010362] = SCAN,
rho_a.shape=(6, 6088), rho_b.shape=(6, 6088)
fxc_a.shape=(6088,), fxc_b.shape=(6088,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12176), fxc.shape=(12176,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6088, 30)
rho shape (4, 4, 2, 6088)
rho_filt shape: (6088,)
get descriptors tdrho.shape=(2, 6088, 2)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12176, 2)
rho_filt.shape=(12176,)
mol:  [['S', array([0., 0., 0.])]]
converged SCF energy = -397.938786815927  <S^2> = 2.0022329  2S+1 = 3.0014882
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01fb1c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01fb1c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.08992216e-03 -9.66918801e-04 -1.00386860e-03 ... -1.18982463e+01
 -1.18982463e+01 -1.18982463e+01] = SCAN,
rho_a.shape=(6, 6320), rho_b.shape=(6, 6320)
fxc_a.shape=(6320,), fxc_b.shape=(6320,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12640), fxc.shape=(12640,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(2, 6320, 2)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12640, 2)
rho_filt.shape=(12640,)
mol:  [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]]
converged SCF energy = -1.16580491182912  <S^2> = 0  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01fb400> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01fb400> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 14, 14)
ao.shape (10, 4776, 14)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.31556415e-04 -9.72662915e-06 -3.66768501e-04 ... -5.54165573e-01
 -5.54165573e-01 -5.54165573e-01] = SCAN,
rho_a.shape=(6, 4776), rho_b.shape=(6, 4776)
fxc_a.shape=(4776,), fxc_b.shape=(4776,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 4776), fxc.shape=(4776,)
get_data, dm shape = (2, 14, 14)
ao_eval.shape=(4, 4776, 14)
rho shape (4, 4, 2, 4776)
rho_filt shape: (4776,)
get descriptors tdrho.shape=(2, 4776, 2)
mol:  [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]]
converged SCF energy = -109.439263799566  <S^2> = 8.8817842e-15  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f8850> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f8850> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9848, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-9.68469910e-05 -9.84742277e-04 -2.59676148e-04 ... -2.39626668e-05
 -2.39626668e-05 -9.68469910e-05] = SCAN,
rho_a.shape=(6, 9848), rho_b.shape=(6, 9848)
fxc_a.shape=(9848,), fxc_b.shape=(9848,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9848), fxc.shape=(9848,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9848, 44)
rho shape (4, 4, 2, 9848)
rho_filt shape: (9848,)
get descriptors tdrho.shape=(2, 9848, 2)
mol:  [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]]
converged SCF energy = -107.339357395734  <S^2> = 2.6645353e-15  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f86d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f86d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9752, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.04987750e-03 -6.68953858e-04 -8.57556270e-04 ... -1.07485583e-03
 -8.01425698e-01 -8.01425698e-01] = SCAN,
rho_a.shape=(6, 9752), rho_b.shape=(6, 9752)
fxc_a.shape=(9752,), fxc_b.shape=(9752,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9752), fxc.shape=(9752,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9752, 44)
rho shape (4, 4, 2, 9752)
rho_filt shape: (9752,)
get descriptors tdrho.shape=(2, 9752, 2)
mol:  [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]]
converged SCF energy = -93.3377924465133  <S^2> = 4.0073012e-10  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f82e0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f82e0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 12256, 51)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.97917285e-04 -2.54412366e-05 -3.15182243e-05 ... -6.37386500e-01
 -6.37386500e-01 -6.37386500e-01] = SCAN,
rho_a.shape=(6, 12256), rho_b.shape=(6, 12256)
fxc_a.shape=(12256,), fxc_b.shape=(12256,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12256), fxc.shape=(12256,)
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 12256, 51)
rho shape (4, 4, 2, 12256)
rho_filt shape: (12256,)
get descriptors tdrho.shape=(2, 12256, 2)
mol:  [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]]
converged SCF energy = -188.456965322844  <S^2> = 1.2434498e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f89a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f89a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 66, 66)
ao.shape (10, 14920, 66)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.50217115e-04 -2.07520066e-04 -9.23619896e-04 ... -2.74295208e-06
 -4.27559894e+00 -4.27559894e+00] = SCAN,
rho_a.shape=(6, 14920), rho_b.shape=(6, 14920)
fxc_a.shape=(14920,), fxc_b.shape=(14920,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 14920), fxc.shape=(14920,)
get_data, dm shape = (2, 66, 66)
ao_eval.shape=(4, 14920, 66)
rho shape (4, 4, 2, 14920)
rho_filt shape: (14920,)
get descriptors tdrho.shape=(2, 14920, 2)
mol:  [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]]
converged SCF energy = -920.00560888896  <S^2> = 5.0093263e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f9210> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f9210> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12208, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00155834 -0.00091688 -0.00215831 ... -0.00091688 -0.41618507
 -0.41618507] = SCAN,
rho_a.shape=(6, 12208), rho_b.shape=(6, 12208)
fxc_a.shape=(12208,), fxc_b.shape=(12208,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12208), fxc.shape=(12208,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12208, 60)
rho shape (4, 4, 2, 12208)
rho_filt shape: (12208,)
get descriptors tdrho.shape=(2, 12208, 2)
mol:  [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]]
converged SCF energy = -199.394370591172  <S^2> = 1.1546319e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f8070> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f8070> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9824, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.92948614e-04 -1.95198688e-05 -1.16699802e-03 ... -4.89378340e-01
 -4.89378340e-01 -4.89378340e-01] = SCAN,
rho_a.shape=(6, 9824), rho_b.shape=(6, 9824)
fxc_a.shape=(9824,), fxc_b.shape=(9824,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9824), fxc.shape=(9824,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9824, 44)
rho shape (4, 4, 2, 9824)
rho_filt shape: (9824,)
get descriptors tdrho.shape=(2, 9824, 2)
mol:  [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]]
converged SCF energy = -150.214894466814  <S^2> = 1.0018598  2S+1 = 2.2377309
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f9270> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f9270> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9912, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-3.38737750e-04 -5.76881261e-05 -2.88355696e-06 ... -6.59150650e-01
 -6.59150650e-01 -6.59150650e-01] = SCAN,
rho_a.shape=(6, 9912), rho_b.shape=(6, 9912)
fxc_a.shape=(9912,), fxc_b.shape=(9912,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9912), fxc.shape=(9912,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9912, 44)
rho shape (4, 4, 2, 9912)
rho_filt shape: (9912,)
get descriptors tdrho.shape=(2, 9912, 2)
mol:  [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]]
converged SCF energy = -77.2435048346374  <S^2> = 8.8817842e-15  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f9180> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f9180> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 58, 58)
ao.shape (10, 15208, 58)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.83270456e-05 -8.83270456e-05 -9.75839850e-04 ... -3.46719667e-05
 -3.31708644e-05 -3.31708644e-05] = SCAN,
rho_a.shape=(6, 15208), rho_b.shape=(6, 15208)
fxc_a.shape=(15208,), fxc_b.shape=(15208,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 15208), fxc.shape=(15208,)
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15208, 58)
rho shape (4, 4, 2, 15208)
rho_filt shape: (15208,)
get descriptors tdrho.shape=(2, 15208, 2)
mol:  [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]]
converged SCF energy = -113.221335689652  <S^2> = 6.6613381e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f8b80> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f8b80> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 10040, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-5.37000578e-04 -8.55494549e-04 -2.46853288e-03 ... -7.34251999e-01
 -7.34251999e-01 -7.34251999e-01] = SCAN,
rho_a.shape=(6, 10040), rho_b.shape=(6, 10040)
fxc_a.shape=(10040,), fxc_b.shape=(10040,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 10040), fxc.shape=(10040,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 10040, 44)
rho shape (4, 4, 2, 10040)
rho_filt shape: (10040,)
get descriptors tdrho.shape=(2, 10040, 2)
mol:  [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]]
converged SCF energy = -460.624592374078  <S^2> = 6.5725203e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f9de0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f9de0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 37, 37)
ao.shape (10, 8552, 37)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.38161177e-04 -1.81188367e-05 -2.37300299e-05 ... -2.83738108e+00
 -2.83738108e+00 -2.83738108e+00] = SCAN,
rho_a.shape=(6, 8552), rho_b.shape=(6, 8552)
fxc_a.shape=(8552,), fxc_b.shape=(8552,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 8552), fxc.shape=(8552,)
get_data, dm shape = (2, 37, 37)
ao_eval.shape=(4, 8552, 37)
rho shape (4, 4, 2, 8552)
rho_filt shape: (8552,)
get descriptors tdrho.shape=(2, 8552, 2)
mol:  [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]]
converged SCF energy = -8.04458854018922  <S^2> = 7.8825835e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f9a50> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f9a50> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 29, 29)
ao.shape (10, 6936, 29)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00113445 -0.00118624 -0.00118624 ... -0.48434639 -0.48434639
 -0.48434639] = SCAN,
rho_a.shape=(6, 6936), rho_b.shape=(6, 6936)
fxc_a.shape=(6936,), fxc_b.shape=(6936,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 6936), fxc.shape=(6936,)
get_data, dm shape = (2, 29, 29)
ao_eval.shape=(4, 6936, 29)
rho shape (4, 4, 2, 6936)
rho_filt shape: (6936,)
get descriptors tdrho.shape=(2, 6936, 2)
mol:  [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]]
converged SCF energy = -324.340512506578  <S^2> = 1.5866419e-11  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f97b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f97b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 11536, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00297935 -0.00297935 -0.00407089 ... -0.00297935 -0.00297935
 -0.00407089] = SCAN,
rho_a.shape=(6, 11536), rho_b.shape=(6, 11536)
fxc_a.shape=(11536,), fxc_b.shape=(11536,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 11536), fxc.shape=(11536,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 11536, 60)
rho shape (4, 4, 2, 11536)
rho_filt shape: (11536,)
get descriptors tdrho.shape=(2, 11536, 2)
mol:  [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]]
converged SCF energy = -1622.57507845815  <S^2> = 8.2422957e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01fae00> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01fae00> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 120, 120)
ao.shape (10, 24512, 120)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.61400996e-04 -4.90484900e-04 -2.56451718e-03 ... -9.59296113e+00
 -9.59296113e+00 -9.59296113e+00] = SCAN,
rho_a.shape=(6, 24512), rho_b.shape=(6, 24512)
fxc_a.shape=(24512,), fxc_b.shape=(24512,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 24512), fxc.shape=(24512,)
get_data, dm shape = (2, 120, 120)
ao_eval.shape=(4, 24512, 120)
rho shape (4, 4, 2, 24512)
rho_filt shape: (24512,)
get descriptors tdrho.shape=(2, 24512, 2)
mol:  [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]]
converged SCF energy = -342.979728469575  <S^2> = 2.5393021e-11  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01faa70> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01faa70> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 13096, 51)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.28637920e-03 -4.32383300e-04 -3.74057272e-05 ... -1.91722770e+00
 -1.91722770e+00 -1.91722770e+00] = SCAN,
rho_a.shape=(6, 13096), rho_b.shape=(6, 13096)
fxc_a.shape=(13096,), fxc_b.shape=(13096,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 13096), fxc.shape=(13096,)
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 13096, 51)
rho shape (4, 4, 2, 13096)
rho_filt shape: (13096,)
get descriptors tdrho.shape=(2, 13096, 2)
mol:  [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]]
converged SCF energy = -578.565336532766  <S^2> = 1.0034706  2S+1 = 2.23917
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01faa10> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01faa10> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12384, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.59860160e-04 -2.60557229e-04 -2.59209644e-04 ... -3.86943883e-01
 -3.86943883e-01 -3.86943883e-01] = SCAN,
rho_a.shape=(6, 12384), rho_b.shape=(6, 12384)
fxc_a.shape=(12384,), fxc_b.shape=(12384,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12384), fxc.shape=(12384,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12384, 60)
rho shape (4, 4, 2, 12384)
rho_filt shape: (12384,)
get descriptors tdrho.shape=(2, 12384, 2)
mol:  [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]]
converged SCF energy = -40.4598214864076  <S^2> = 3.2063241e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f81c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f81c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 50, 50)
ao.shape (10, 13936, 50)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.68439986e-04 -2.42462569e-04 -1.69927031e-05 ... -2.55230307e-05
 -2.55230307e-05 -2.55230307e-05] = SCAN,
rho_a.shape=(6, 13936), rho_b.shape=(6, 13936)
fxc_a.shape=(13936,), fxc_b.shape=(13936,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 13936), fxc.shape=(13936,)
get_data, dm shape = (2, 50, 50)
ao_eval.shape=(4, 13936, 50)
rho shape (4, 4, 2, 13936)
rho_filt shape: (13936,)
get descriptors tdrho.shape=(2, 13936, 2)
mol:  [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]]
converged SCF energy = -39.0756147483504  <S^2> = 6.1985972e-12  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb01f8040> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb01f8040> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 36, 36)
ao.shape (10, 9656, 36)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-7.67688751e-04 -4.57393214e-05 -2.02834191e-04 ... -1.14928924e+00
 -1.14928924e+00 -1.14928924e+00] = SCAN,
rho_a.shape=(6, 9656), rho_b.shape=(6, 9656)
fxc_a.shape=(9656,), fxc_b.shape=(9656,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9656), fxc.shape=(9656,)
get_data, dm shape = (2, 36, 36)
ao_eval.shape=(4, 9656, 36)
rho shape (4, 4, 2, 9656)
rho_filt shape: (9656,)
get descriptors tdrho.shape=(2, 9656, 2)
mol:  [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]]
converged SCF energy = -291.719272437819  <S^2> = 1.3155699e-11  2S+1 = 1
New DM shape: (2, 58, 58)
ao.shape (10, 15256, 58)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.33850535e-04 -2.34903029e-04 -1.75623665e-05 ... -1.92891112e-05
 -1.92891112e-05 -1.92891112e-05] = SCAN,
rho_a.shape=(6, 15256), rho_b.shape=(6, 15256)
fxc_a.shape=(15256,), fxc_b.shape=(15256,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 15256), fxc.shape=(15256,)
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15256, 58)
rho shape (4, 4, 2, 15256)
rho_filt shape: (15256,)
get descriptors tdrho.shape=(2, 15256, 2)
localnet.spin_scaling: concatenating the data
first data shape = (10940, 2)
concatenated: tdrho.shape=(271683, 2)
PRE NAN FILT: tFxc.shape=(271683,), tdrho.shape=(271683, 2)
nan_filt_rho.shape=(271683,)
nan_filt_fxc.shape=(271683,)
tFxc.shape=(271683,), tdrho.shape=(271683, 2)
inp[0].shape = (271683, 1)
Epoch 0
Epoch 0 :: Batch 0/1
Batch Loss = 5.007322712275149
0, epoch_train_loss=5.007322712275149
Epoch 1
Epoch 1 :: Batch 0/1
Batch Loss = 4.725931369358813
1, epoch_train_loss=4.725931369358813
Epoch 2
Epoch 2 :: Batch 0/1
Batch Loss = 4.468119621708431
2, epoch_train_loss=4.468119621708431
Epoch 3
Epoch 3 :: Batch 0/1
Batch Loss = 4.555171173724723
3, epoch_train_loss=4.555171173724723
Epoch 4
Epoch 4 :: Batch 0/1
Batch Loss = 4.352907316237606
4, epoch_train_loss=4.352907316237606
Epoch 5
Epoch 5 :: Batch 0/1
Batch Loss = 4.340992423860173
5, epoch_train_loss=4.340992423860173
Epoch 6
Epoch 6 :: Batch 0/1
Batch Loss = 4.323959301602664
6, epoch_train_loss=4.323959301602664
Epoch 7
Epoch 7 :: Batch 0/1
Batch Loss = 4.245180346194352
7, epoch_train_loss=4.245180346194352
Epoch 8
Epoch 8 :: Batch 0/1
Batch Loss = 4.058470793709646
8, epoch_train_loss=4.058470793709646
Epoch 9
Epoch 9 :: Batch 0/1
Batch Loss = 3.8770248975133663
9, epoch_train_loss=3.8770248975133663
Epoch 10
Epoch 10 :: Batch 0/1
Batch Loss = 3.735937102757979
10, epoch_train_loss=3.735937102757979
Epoch 11
Epoch 11 :: Batch 0/1
Batch Loss = 3.5929874591399455
11, epoch_train_loss=3.5929874591399455
Epoch 12
Epoch 12 :: Batch 0/1
Batch Loss = 3.612843023024947
12, epoch_train_loss=3.612843023024947
Epoch 13
Epoch 13 :: Batch 0/1
Batch Loss = 3.573591386372523
13, epoch_train_loss=3.573591386372523
Epoch 14
Epoch 14 :: Batch 0/1
Batch Loss = 3.5462790806631324
14, epoch_train_loss=3.5462790806631324
Epoch 15
Epoch 15 :: Batch 0/1
Batch Loss = 3.5070768073313796
15, epoch_train_loss=3.5070768073313796
Epoch 16
Epoch 16 :: Batch 0/1
Batch Loss = 3.527329011025727
16, epoch_train_loss=3.527329011025727
Epoch 17
Epoch 17 :: Batch 0/1
Batch Loss = 3.5227714451867653
17, epoch_train_loss=3.5227714451867653
Epoch 18
Epoch 18 :: Batch 0/1
Batch Loss = 3.54826053927043
18, epoch_train_loss=3.54826053927043
Epoch 19
Epoch 19 :: Batch 0/1
Batch Loss = 3.5384083891304927
19, epoch_train_loss=3.5384083891304927
Epoch 20
Epoch 20 :: Batch 0/1
Batch Loss = 3.543765708188258
20, epoch_train_loss=3.543765708188258
Epoch 21
Epoch 21 :: Batch 0/1
Batch Loss = 3.529327982508311
21, epoch_train_loss=3.529327982508311
Epoch 22
Epoch 22 :: Batch 0/1
Batch Loss = 3.526204823857636
22, epoch_train_loss=3.526204823857636
Epoch 23
Epoch 23 :: Batch 0/1
Batch Loss = 3.519303526278334
23, epoch_train_loss=3.519303526278334
Epoch 24
Epoch 24 :: Batch 0/1
Batch Loss = 3.508146747059195
24, epoch_train_loss=3.508146747059195
Epoch 25
Epoch 25 :: Batch 0/1
Batch Loss = 3.5117778767808754
25, epoch_train_loss=3.5117778767808754
Epoch 26
Epoch 26 :: Batch 0/1
Batch Loss = 3.4961474897780085
26, epoch_train_loss=3.4961474897780085
Epoch 27
Epoch 27 :: Batch 0/1
Batch Loss = 3.498380252970597
27, epoch_train_loss=3.498380252970597
Epoch 28
Epoch 28 :: Batch 0/1
Batch Loss = 3.483909074909587
28, epoch_train_loss=3.483909074909587
Epoch 29
Epoch 29 :: Batch 0/1
Batch Loss = 3.478809945447218
29, epoch_train_loss=3.478809945447218
Epoch 30
Epoch 30 :: Batch 0/1
Batch Loss = 3.4815014165202194
30, epoch_train_loss=3.4815014165202194
Epoch 31
Epoch 31 :: Batch 0/1
Batch Loss = 3.4808288896997364
31, epoch_train_loss=3.4808288896997364
Epoch 32
Epoch 32 :: Batch 0/1
Batch Loss = 3.48765514149603
32, epoch_train_loss=3.48765514149603
Epoch 33
Epoch 33 :: Batch 0/1
Batch Loss = 3.4898383316933717
33, epoch_train_loss=3.4898383316933717
Epoch 34
Epoch 34 :: Batch 0/1
Batch Loss = 3.4828713572526677
34, epoch_train_loss=3.4828713572526677
Epoch 35
Epoch 35 :: Batch 0/1
Batch Loss = 3.4799288889932463
35, epoch_train_loss=3.4799288889932463
Epoch 36
Epoch 36 :: Batch 0/1
Batch Loss = 3.4802724883241734
36, epoch_train_loss=3.4802724883241734
Epoch 37
Epoch 37 :: Batch 0/1
Batch Loss = 3.478158388789075
37, epoch_train_loss=3.478158388789075
Epoch 38
Epoch 38 :: Batch 0/1
Batch Loss = 3.479708646612321
38, epoch_train_loss=3.479708646612321
Epoch 39
Epoch 39 :: Batch 0/1
Batch Loss = 3.4787920593292196
39, epoch_train_loss=3.4787920593292196
Epoch 40
Epoch 40 :: Batch 0/1
Batch Loss = 3.4734988480416082
40, epoch_train_loss=3.4734988480416082
Epoch 41
Epoch 41 :: Batch 0/1
Batch Loss = 3.472410855764886
41, epoch_train_loss=3.472410855764886
Epoch 42
Epoch 42 :: Batch 0/1
Batch Loss = 3.470878646711614
42, epoch_train_loss=3.470878646711614
Epoch 43
Epoch 43 :: Batch 0/1
Batch Loss = 3.4705617614490154
43, epoch_train_loss=3.4705617614490154
Epoch 44
Epoch 44 :: Batch 0/1
Batch Loss = 3.472665682525324
44, epoch_train_loss=3.472665682525324
Epoch 45
Epoch 45 :: Batch 0/1
Batch Loss = 3.4713122399656084
45, epoch_train_loss=3.4713122399656084
Epoch 46
Epoch 46 :: Batch 0/1
Batch Loss = 3.471173910500866
46, epoch_train_loss=3.471173910500866
Epoch 47
Epoch 47 :: Batch 0/1
Batch Loss = 3.469537223422625
47, epoch_train_loss=3.469537223422625
Epoch 48
Epoch 48 :: Batch 0/1
Batch Loss = 3.467571276476427
48, epoch_train_loss=3.467571276476427
Epoch 49
Epoch 49 :: Batch 0/1
Batch Loss = 3.467638428422523
49, epoch_train_loss=3.467638428422523
Epoch 50
Epoch 50 :: Batch 0/1
Batch Loss = 3.4664844104549903
50, epoch_train_loss=3.4664844104549903
Epoch 51
Epoch 51 :: Batch 0/1
Batch Loss = 3.467180409383873
51, epoch_train_loss=3.467180409383873
Epoch 52
Epoch 52 :: Batch 0/1
Batch Loss = 3.46592894909796
52, epoch_train_loss=3.46592894909796
Epoch 53
Epoch 53 :: Batch 0/1
Batch Loss = 3.4651446344399774
53, epoch_train_loss=3.4651446344399774
Epoch 54
Epoch 54 :: Batch 0/1
Batch Loss = 3.464299262617409
54, epoch_train_loss=3.464299262617409
Epoch 55
Epoch 55 :: Batch 0/1
Batch Loss = 3.4638250972855156
55, epoch_train_loss=3.4638250972855156
Epoch 56
Epoch 56 :: Batch 0/1
Batch Loss = 3.4644683499391173
56, epoch_train_loss=3.4644683499391173
Epoch 57
Epoch 57 :: Batch 0/1
Batch Loss = 3.464131414102608
57, epoch_train_loss=3.464131414102608
Epoch 58
Epoch 58 :: Batch 0/1
Batch Loss = 3.464203249560176
58, epoch_train_loss=3.464203249560176
Epoch 59
Epoch 59 :: Batch 0/1
Batch Loss = 3.4630970508344125
59, epoch_train_loss=3.4630970508344125
Epoch 60
Epoch 60 :: Batch 0/1
Batch Loss = 3.462751623241384
60, epoch_train_loss=3.462751623241384
Epoch 61
Epoch 61 :: Batch 0/1
Batch Loss = 3.462298320018333
61, epoch_train_loss=3.462298320018333
Epoch 62
Epoch 62 :: Batch 0/1
Batch Loss = 3.4623980191928863
62, epoch_train_loss=3.4623980191928863
Epoch 63
Epoch 63 :: Batch 0/1
Batch Loss = 3.462258876592279
63, epoch_train_loss=3.462258876592279
Epoch 64
Epoch 64 :: Batch 0/1
Batch Loss = 3.4619964404370633
64, epoch_train_loss=3.4619964404370633
Epoch 65
Epoch 65 :: Batch 0/1
Batch Loss = 3.461763534877499
65, epoch_train_loss=3.461763534877499
Epoch 66
Epoch 66 :: Batch 0/1
Batch Loss = 3.461663867728888
66, epoch_train_loss=3.461663867728888
Epoch 67
Epoch 67 :: Batch 0/1
Batch Loss = 3.4618220492040184
67, epoch_train_loss=3.4618220492040184
Epoch 68
Epoch 68 :: Batch 0/1
Batch Loss = 3.4618272702460424
68, epoch_train_loss=3.4618272702460424
Epoch 69
Epoch 69 :: Batch 0/1
Batch Loss = 3.4617727017379076
69, epoch_train_loss=3.4617727017379076
Epoch 70
Epoch 70 :: Batch 0/1
Batch Loss = 3.4614878634500466
70, epoch_train_loss=3.4614878634500466
Epoch 71
Epoch 71 :: Batch 0/1
Batch Loss = 3.4613850903491667
71, epoch_train_loss=3.4613850903491667
Epoch 72
Epoch 72 :: Batch 0/1
Batch Loss = 3.461361346592083
72, epoch_train_loss=3.461361346592083
Epoch 73
Epoch 73 :: Batch 0/1
Batch Loss = 3.461505281602975
73, epoch_train_loss=3.461505281602975
Epoch 74
Epoch 74 :: Batch 0/1
Batch Loss = 3.461509674371552
74, epoch_train_loss=3.461509674371552
Epoch 75
Epoch 75 :: Batch 0/1
Batch Loss = 3.461453146380113
75, epoch_train_loss=3.461453146380113
Epoch 76
Epoch 76 :: Batch 0/1
Batch Loss = 3.4613593711690336
76, epoch_train_loss=3.4613593711690336
Epoch 77
Epoch 77 :: Batch 0/1
Batch Loss = 3.4613441932642304
77, epoch_train_loss=3.4613441932642304
Epoch 78
Epoch 78 :: Batch 0/1
Batch Loss = 3.461359095771189
78, epoch_train_loss=3.461359095771189
Epoch 79
Epoch 79 :: Batch 0/1
Batch Loss = 3.4612998503126855
79, epoch_train_loss=3.4612998503126855
Epoch 80
Epoch 80 :: Batch 0/1
Batch Loss = 3.461228574211181
80, epoch_train_loss=3.461228574211181
Epoch 81
Epoch 81 :: Batch 0/1
Batch Loss = 3.461121418169396
81, epoch_train_loss=3.461121418169396
Epoch 82
Epoch 82 :: Batch 0/1
Batch Loss = 3.4611489355540566
82, epoch_train_loss=3.4611489355540566
Epoch 83
Epoch 83 :: Batch 0/1
Batch Loss = 3.4611338109493066
83, epoch_train_loss=3.4611338109493066
Epoch 84
Epoch 84 :: Batch 0/1
Batch Loss = 3.461168509714028
84, epoch_train_loss=3.461168509714028
Epoch 85
Epoch 85 :: Batch 0/1
Batch Loss = 3.461083678441317
85, epoch_train_loss=3.461083678441317
Epoch 86
Epoch 86 :: Batch 0/1
Batch Loss = 3.4610652671762927
86, epoch_train_loss=3.4610652671762927
Epoch 87
Epoch 87 :: Batch 0/1
Batch Loss = 3.4610293367068246
87, epoch_train_loss=3.4610293367068246
Epoch 88
Epoch 88 :: Batch 0/1
Batch Loss = 3.461042905587468
88, epoch_train_loss=3.461042905587468
Epoch 89
Epoch 89 :: Batch 0/1
Batch Loss = 3.461033227039697
89, epoch_train_loss=3.461033227039697
Epoch 90
Epoch 90 :: Batch 0/1
Batch Loss = 3.460998160172275
90, epoch_train_loss=3.460998160172275
Epoch 91
Epoch 91 :: Batch 0/1
Batch Loss = 3.4609892247430434
91, epoch_train_loss=3.4609892247430434
Epoch 92
Epoch 92 :: Batch 0/1
Batch Loss = 3.4609682511094713
92, epoch_train_loss=3.4609682511094713
Epoch 93
Epoch 93 :: Batch 0/1
Batch Loss = 3.4609867621270896
93, epoch_train_loss=3.4609867621270896
Epoch 94
Epoch 94 :: Batch 0/1
Batch Loss = 3.460960458268864
94, epoch_train_loss=3.460960458268864
Epoch 95
Epoch 95 :: Batch 0/1
Batch Loss = 3.4609411606463656
95, epoch_train_loss=3.4609411606463656
Epoch 96
Epoch 96 :: Batch 0/1
Batch Loss = 3.4609152453379832
96, epoch_train_loss=3.4609152453379832
Epoch 97
Epoch 97 :: Batch 0/1
Batch Loss = 3.460904742803658
97, epoch_train_loss=3.460904742803658
Epoch 98
Epoch 98 :: Batch 0/1
Batch Loss = 3.4609122618065946
98, epoch_train_loss=3.4609122618065946
Epoch 99
Epoch 99 :: Batch 0/1
Batch Loss = 3.4608982685398595
99, epoch_train_loss=3.4608982685398595
Epoch 100
Epoch 100 :: Batch 0/1
Batch Loss = 3.4608945452361857
100, epoch_train_loss=3.4608945452361857
Epoch 101
Epoch 101 :: Batch 0/1
Batch Loss = 3.4608805612412636
101, epoch_train_loss=3.4608805612412636
Epoch 102
Epoch 102 :: Batch 0/1
Batch Loss = 3.460881050142096
102, epoch_train_loss=3.460881050142096
Epoch 103
Epoch 103 :: Batch 0/1
Batch Loss = 3.4608820411560433
103, epoch_train_loss=3.4608820411560433
Epoch 104
Epoch 104 :: Batch 0/1
Batch Loss = 3.460870457718894
104, epoch_train_loss=3.460870457718894
Epoch 105
Epoch 105 :: Batch 0/1
Batch Loss = 3.4608646971638612
105, epoch_train_loss=3.4608646971638612
Epoch 106
Epoch 106 :: Batch 0/1
Batch Loss = 3.4608540319313725
106, epoch_train_loss=3.4608540319313725
Epoch 107
Epoch 107 :: Batch 0/1
Batch Loss = 3.460854065623817
107, epoch_train_loss=3.460854065623817
Epoch 108
Epoch 108 :: Batch 0/1
Batch Loss = 3.4608520218010446
108, epoch_train_loss=3.4608520218010446
Epoch 109
Epoch 109 :: Batch 0/1
Batch Loss = 3.4608422582582463
109, epoch_train_loss=3.4608422582582463
Epoch 110
Epoch 110 :: Batch 0/1
Batch Loss = 3.4608359413820713
110, epoch_train_loss=3.4608359413820713
Epoch 111
Epoch 111 :: Batch 0/1
Batch Loss = 3.460826521379151
111, epoch_train_loss=3.460826521379151
Epoch 112
Epoch 112 :: Batch 0/1
Batch Loss = 3.4608222505520265
112, epoch_train_loss=3.4608222505520265
Epoch 113
Epoch 113 :: Batch 0/1
Batch Loss = 3.4608170231618605
113, epoch_train_loss=3.4608170231618605
Epoch 114
Epoch 114 :: Batch 0/1
Batch Loss = 3.460807358425429
114, epoch_train_loss=3.460807358425429
Epoch 115
Epoch 115 :: Batch 0/1
Batch Loss = 3.4608019844077615
115, epoch_train_loss=3.4608019844077615
Epoch 116
Epoch 116 :: Batch 0/1
Batch Loss = 3.460796303480262
116, epoch_train_loss=3.460796303480262
Epoch 117
Epoch 117 :: Batch 0/1
Batch Loss = 3.46079177092659
117, epoch_train_loss=3.46079177092659
Epoch 118
Epoch 118 :: Batch 0/1
Batch Loss = 3.4607873363123685
118, epoch_train_loss=3.4607873363123685
Epoch 119
Epoch 119 :: Batch 0/1
Batch Loss = 3.4607792416326992
119, epoch_train_loss=3.4607792416326992
Epoch 120
Epoch 120 :: Batch 0/1
Batch Loss = 3.460773375820659
120, epoch_train_loss=3.460773375820659
Epoch 121
Epoch 121 :: Batch 0/1
Batch Loss = 3.460768741467981
121, epoch_train_loss=3.460768741467981
Epoch 122
Epoch 122 :: Batch 0/1
Batch Loss = 3.4607630328990333
122, epoch_train_loss=3.4607630328990333
Epoch 123
Epoch 123 :: Batch 0/1
Batch Loss = 3.460758160828723
123, epoch_train_loss=3.460758160828723
Epoch 124
Epoch 124 :: Batch 0/1
Batch Loss = 3.4607523151141684
124, epoch_train_loss=3.4607523151141684
Epoch 125
Epoch 125 :: Batch 0/1
Batch Loss = 3.4607466820280646
125, epoch_train_loss=3.4607466820280646
Epoch 126
Epoch 126 :: Batch 0/1
Batch Loss = 3.460742611529605
126, epoch_train_loss=3.460742611529605
Epoch 127
Epoch 127 :: Batch 0/1
Batch Loss = 3.4607373200961153
127, epoch_train_loss=3.4607373200961153
Epoch 128
Epoch 128 :: Batch 0/1
Batch Loss = 3.460731490956447
128, epoch_train_loss=3.460731490956447
Epoch 129
Epoch 129 :: Batch 0/1
Batch Loss = 3.4607265738692905
129, epoch_train_loss=3.4607265738692905
Epoch 130
Epoch 130 :: Batch 0/1
Batch Loss = 3.460721595971433
130, epoch_train_loss=3.460721595971433
Epoch 131
Epoch 131 :: Batch 0/1
Batch Loss = 3.4607169414714685
131, epoch_train_loss=3.4607169414714685
Epoch 132
Epoch 132 :: Batch 0/1
Batch Loss = 3.4607124487850776
132, epoch_train_loss=3.4607124487850776
Epoch 133
Epoch 133 :: Batch 0/1
Batch Loss = 3.4607071825075275
133, epoch_train_loss=3.4607071825075275
Epoch 134
Epoch 134 :: Batch 0/1
Batch Loss = 3.4607021365186266
134, epoch_train_loss=3.4607021365186266
Epoch 135
Epoch 135 :: Batch 0/1
Batch Loss = 3.4606976387139383
135, epoch_train_loss=3.4606976387139383
Epoch 136
Epoch 136 :: Batch 0/1
Batch Loss = 3.460692654504116
136, epoch_train_loss=3.460692654504116
Epoch 137
Epoch 137 :: Batch 0/1
Batch Loss = 3.4606874493056754
137, epoch_train_loss=3.4606874493056754
Epoch 138
Epoch 138 :: Batch 0/1
Batch Loss = 3.460682565665862
138, epoch_train_loss=3.460682565665862
Epoch 139
Epoch 139 :: Batch 0/1
Batch Loss = 3.4606776602105946
139, epoch_train_loss=3.4606776602105946
Epoch 140
Epoch 140 :: Batch 0/1
Batch Loss = 3.4606727255736502
140, epoch_train_loss=3.4606727255736502
Epoch 141
Epoch 141 :: Batch 0/1
Batch Loss = 3.460667860073061
141, epoch_train_loss=3.460667860073061
Epoch 142
Epoch 142 :: Batch 0/1
Batch Loss = 3.46066278329692
142, epoch_train_loss=3.46066278329692
Epoch 143
Epoch 143 :: Batch 0/1
Batch Loss = 3.4606576527698345
143, epoch_train_loss=3.4606576527698345
Epoch 144
Epoch 144 :: Batch 0/1
Batch Loss = 3.460652787423323
144, epoch_train_loss=3.460652787423323
Epoch 145
Epoch 145 :: Batch 0/1
Batch Loss = 3.4606479271636017
145, epoch_train_loss=3.4606479271636017
Epoch 146
Epoch 146 :: Batch 0/1
Batch Loss = 3.460642861056636
146, epoch_train_loss=3.460642861056636
Epoch 147
Epoch 147 :: Batch 0/1
Batch Loss = 3.4606378497644887
147, epoch_train_loss=3.4606378497644887
Epoch 148
Epoch 148 :: Batch 0/1
Batch Loss = 3.460632986732035
148, epoch_train_loss=3.460632986732035
Epoch 149
Epoch 149 :: Batch 0/1
Batch Loss = 3.460628045499017
149, epoch_train_loss=3.460628045499017
Epoch 150
Epoch 150 :: Batch 0/1
Batch Loss = 3.460622983546495
150, epoch_train_loss=3.460622983546495
Epoch 151
Epoch 151 :: Batch 0/1
Batch Loss = 3.4606179548221054
151, epoch_train_loss=3.4606179548221054
Epoch 152
Epoch 152 :: Batch 0/1
Batch Loss = 3.46061296697881
152, epoch_train_loss=3.46061296697881
Epoch 153
Epoch 153 :: Batch 0/1
Batch Loss = 3.4606079418503994
153, epoch_train_loss=3.4606079418503994
Epoch 154
Epoch 154 :: Batch 0/1
Batch Loss = 3.46060289824217
154, epoch_train_loss=3.46060289824217
Epoch 155
Epoch 155 :: Batch 0/1
Batch Loss = 3.4605978486804805
155, epoch_train_loss=3.4605978486804805
Epoch 156
Epoch 156 :: Batch 0/1
Batch Loss = 3.460592759307752
156, epoch_train_loss=3.460592759307752
Epoch 157
Epoch 157 :: Batch 0/1
Batch Loss = 3.460587639612673
157, epoch_train_loss=3.460587639612673
Epoch 158
Epoch 158 :: Batch 0/1
Batch Loss = 3.460582513293513
158, epoch_train_loss=3.460582513293513
Epoch 159
Epoch 159 :: Batch 0/1
Batch Loss = 3.4605773662772887
159, epoch_train_loss=3.4605773662772887
Epoch 160
Epoch 160 :: Batch 0/1
Batch Loss = 3.460572175612133
160, epoch_train_loss=3.460572175612133
Epoch 161
Epoch 161 :: Batch 0/1
Batch Loss = 3.460566952317764
161, epoch_train_loss=3.460566952317764
Epoch 162
Epoch 162 :: Batch 0/1
Batch Loss = 3.460561721224339
162, epoch_train_loss=3.460561721224339
Epoch 163
Epoch 163 :: Batch 0/1
Batch Loss = 3.460556460015034
163, epoch_train_loss=3.460556460015034
Epoch 164
Epoch 164 :: Batch 0/1
Batch Loss = 3.4605511383800165
164, epoch_train_loss=3.4605511383800165
Epoch 165
Epoch 165 :: Batch 0/1
Batch Loss = 3.460545771039578
165, epoch_train_loss=3.460545771039578
Epoch 166
Epoch 166 :: Batch 0/1
Batch Loss = 3.460540376517682
166, epoch_train_loss=3.460540376517682
Epoch 167
Epoch 167 :: Batch 0/1
Batch Loss = 3.460534952730459
167, epoch_train_loss=3.460534952730459
Epoch 168
Epoch 168 :: Batch 0/1
Batch Loss = 3.4605294818747447
168, epoch_train_loss=3.4605294818747447
Epoch 169
Epoch 169 :: Batch 0/1
Batch Loss = 3.4605239555575658
169, epoch_train_loss=3.4605239555575658
Epoch 170
Epoch 170 :: Batch 0/1
Batch Loss = 3.460518385949347
170, epoch_train_loss=3.460518385949347
Epoch 171
Epoch 171 :: Batch 0/1
Batch Loss = 3.4605127703224903
171, epoch_train_loss=3.4605127703224903
Epoch 172
Epoch 172 :: Batch 0/1
Batch Loss = 3.460507101229044
172, epoch_train_loss=3.460507101229044
Epoch 173
Epoch 173 :: Batch 0/1
Batch Loss = 3.4605013760475964
173, epoch_train_loss=3.4605013760475964
Epoch 174
Epoch 174 :: Batch 0/1
Batch Loss = 3.460495591764708
174, epoch_train_loss=3.460495591764708
Epoch 175
Epoch 175 :: Batch 0/1
Batch Loss = 3.460489750128574
175, epoch_train_loss=3.460489750128574
Epoch 176
Epoch 176 :: Batch 0/1
Batch Loss = 3.4604838443457027
176, epoch_train_loss=3.4604838443457027
Epoch 177
Epoch 177 :: Batch 0/1
Batch Loss = 3.4604778739192104
177, epoch_train_loss=3.4604778739192104
Epoch 178
Epoch 178 :: Batch 0/1
Batch Loss = 3.460471838864654
178, epoch_train_loss=3.460471838864654
Epoch 179
Epoch 179 :: Batch 0/1
Batch Loss = 3.460465730696823
179, epoch_train_loss=3.460465730696823
Epoch 180
Epoch 180 :: Batch 0/1
Batch Loss = 3.4604595455535985
180, epoch_train_loss=3.4604595455535985
Epoch 181
Epoch 181 :: Batch 0/1
Batch Loss = 3.460453278125439
181, epoch_train_loss=3.460453278125439
Epoch 182
Epoch 182 :: Batch 0/1
Batch Loss = 3.4604469326396425
182, epoch_train_loss=3.4604469326396425
Epoch 183
Epoch 183 :: Batch 0/1
Batch Loss = 3.4604405062519974
183, epoch_train_loss=3.4604405062519974
Epoch 184
Epoch 184 :: Batch 0/1
Batch Loss = 3.4604339907777875
184, epoch_train_loss=3.4604339907777875
Epoch 185
Epoch 185 :: Batch 0/1
Batch Loss = 3.4604273810270647
185, epoch_train_loss=3.4604273810270647
Epoch 186
Epoch 186 :: Batch 0/1
Batch Loss = 3.4604206709196728
186, epoch_train_loss=3.4604206709196728
Epoch 187
Epoch 187 :: Batch 0/1
Batch Loss = 3.4604138610793065
187, epoch_train_loss=3.4604138610793065
Epoch 188
Epoch 188 :: Batch 0/1
Batch Loss = 3.460406943341556
188, epoch_train_loss=3.460406943341556
Epoch 189
Epoch 189 :: Batch 0/1
Batch Loss = 3.4603999145669895
189, epoch_train_loss=3.4603999145669895
Epoch 190
Epoch 190 :: Batch 0/1
Batch Loss = 3.4603927687449096
190, epoch_train_loss=3.4603927687449096
Epoch 191
Epoch 191 :: Batch 0/1
Batch Loss = 3.4603855026549475
191, epoch_train_loss=3.4603855026549475
Epoch 192
Epoch 192 :: Batch 0/1
Batch Loss = 3.460378110125574
192, epoch_train_loss=3.460378110125574
Epoch 193
Epoch 193 :: Batch 0/1
Batch Loss = 3.4603705832001954
193, epoch_train_loss=3.4603705832001954
Epoch 194
Epoch 194 :: Batch 0/1
Batch Loss = 3.4603629174780814
194, epoch_train_loss=3.4603629174780814
Epoch 195
Epoch 195 :: Batch 0/1
Batch Loss = 3.460355104961436
195, epoch_train_loss=3.460355104961436
Epoch 196
Epoch 196 :: Batch 0/1
Batch Loss = 3.4603471416709586
196, epoch_train_loss=3.4603471416709586
Epoch 197
Epoch 197 :: Batch 0/1
Batch Loss = 3.4603390158460137
197, epoch_train_loss=3.4603390158460137
Epoch 198
Epoch 198 :: Batch 0/1
Batch Loss = 3.4603307240817496
198, epoch_train_loss=3.4603307240817496
Epoch 199
Epoch 199 :: Batch 0/1
Batch Loss = 3.4603222540791525
199, epoch_train_loss=3.4603222540791525
Epoch 200
Epoch 200 :: Batch 0/1
Batch Loss = 3.460313604555494
200, epoch_train_loss=3.460313604555494
Epoch 201
Epoch 201 :: Batch 0/1
Batch Loss = 3.4603047600494485
201, epoch_train_loss=3.4603047600494485
Epoch 202
Epoch 202 :: Batch 0/1
Batch Loss = 3.4602957246875397
202, epoch_train_loss=3.4602957246875397
Epoch 203
Epoch 203 :: Batch 0/1
Batch Loss = 3.4602864835753
203, epoch_train_loss=3.4602864835753
Epoch 204
Epoch 204 :: Batch 0/1
Batch Loss = 3.460277062930482
204, epoch_train_loss=3.460277062930482
Epoch 205
Epoch 205 :: Batch 0/1
Batch Loss = 3.4602674679242744
205, epoch_train_loss=3.4602674679242744
Epoch 206
Epoch 206 :: Batch 0/1
Batch Loss = 3.460257827088614
206, epoch_train_loss=3.460257827088614
Epoch 207
Epoch 207 :: Batch 0/1
Batch Loss = 3.4602483242595024
207, epoch_train_loss=3.4602483242595024
Epoch 208
Epoch 208 :: Batch 0/1
Batch Loss = 3.460239688384952
208, epoch_train_loss=3.460239688384952
Epoch 209
Epoch 209 :: Batch 0/1
Batch Loss = 3.460233460893505
209, epoch_train_loss=3.460233460893505
Epoch 210
Epoch 210 :: Batch 0/1
Batch Loss = 3.460234627480026
210, epoch_train_loss=3.460234627480026
Epoch 211
Epoch 211 :: Batch 0/1
Batch Loss = 3.460255565572842
211, epoch_train_loss=3.460255565572842
Epoch 212
Epoch 212 :: Batch 0/1
Batch Loss = 3.4603369312545715
212, epoch_train_loss=3.4603369312545715
Epoch 213
Epoch 213 :: Batch 0/1
Batch Loss = 3.4605828089515427
213, epoch_train_loss=3.4605828089515427
Epoch 214
Epoch 214 :: Batch 0/1
Batch Loss = 3.4613780965518766
214, epoch_train_loss=3.4613780965518766
Epoch 215
Epoch 215 :: Batch 0/1
Batch Loss = 3.463531332660586
215, epoch_train_loss=3.463531332660586
Epoch 216
Epoch 216 :: Batch 0/1
Batch Loss = 3.4706991669955913
216, epoch_train_loss=3.4706991669955913
Epoch 217
Epoch 217 :: Batch 0/1
Batch Loss = 3.4818734867223182
217, epoch_train_loss=3.4818734867223182
Epoch 218
Epoch 218 :: Batch 0/1
Batch Loss = 3.5082991171218807
218, epoch_train_loss=3.5082991171218807
Epoch 219
Epoch 219 :: Batch 0/1
Batch Loss = 3.4735465250298057
219, epoch_train_loss=3.4735465250298057
Epoch 220
Epoch 220 :: Batch 0/1
Batch Loss = 3.4609646496135795
220, epoch_train_loss=3.4609646496135795
Epoch 221
Epoch 221 :: Batch 0/1
Batch Loss = 3.475037225424407
221, epoch_train_loss=3.475037225424407
Epoch 222
Epoch 222 :: Batch 0/1
Batch Loss = 3.465904635149995
222, epoch_train_loss=3.465904635149995
Epoch 223
Epoch 223 :: Batch 0/1
Batch Loss = 3.461776676377158
223, epoch_train_loss=3.461776676377158
Epoch 224
Epoch 224 :: Batch 0/1
Batch Loss = 3.468891159284167
224, epoch_train_loss=3.468891159284167
Epoch 225
Epoch 225 :: Batch 0/1
Batch Loss = 3.461210573999223
225, epoch_train_loss=3.461210573999223
Epoch 226
Epoch 226 :: Batch 0/1
Batch Loss = 3.4649100315828316
226, epoch_train_loss=3.4649100315828316
Epoch 227
Epoch 227 :: Batch 0/1
Batch Loss = 3.4656875850036166
227, epoch_train_loss=3.4656875850036166
Epoch 228
Epoch 228 :: Batch 0/1
Batch Loss = 3.461077152449312
228, epoch_train_loss=3.461077152449312
Epoch 229
Epoch 229 :: Batch 0/1
Batch Loss = 3.4666591378497302
229, epoch_train_loss=3.4666591378497302
Epoch 230
Epoch 230 :: Batch 0/1
Batch Loss = 3.461879409543762
230, epoch_train_loss=3.461879409543762
Epoch 231
Epoch 231 :: Batch 0/1
Batch Loss = 3.4629696279526594
231, epoch_train_loss=3.4629696279526594
Epoch 232
Epoch 232 :: Batch 0/1
Batch Loss = 3.4636326331981393
232, epoch_train_loss=3.4636326331981393
Epoch 233
Epoch 233 :: Batch 0/1
Batch Loss = 3.460550986513873
233, epoch_train_loss=3.460550986513873
Epoch 234
Epoch 234 :: Batch 0/1
Batch Loss = 3.4633606786632725
234, epoch_train_loss=3.4633606786632725
Epoch 235
Epoch 235 :: Batch 0/1
Batch Loss = 3.4605195582967974
235, epoch_train_loss=3.4605195582967974
Epoch 236
Epoch 236 :: Batch 0/1
Batch Loss = 3.4622042363537906
236, epoch_train_loss=3.4622042363537906
Epoch 237
Epoch 237 :: Batch 0/1
Batch Loss = 3.4614737090658796
237, epoch_train_loss=3.4614737090658796
Epoch 238
Epoch 238 :: Batch 0/1
Batch Loss = 3.4607571919530806
238, epoch_train_loss=3.4607571919530806
Epoch 239
Epoch 239 :: Batch 0/1
Batch Loss = 3.4620025846331415
239, epoch_train_loss=3.4620025846331415
Epoch 240
Epoch 240 :: Batch 0/1
Batch Loss = 3.4601373839739877
240, epoch_train_loss=3.4601373839739877
Epoch 241
Epoch 241 :: Batch 0/1
Batch Loss = 3.4614723360941806
241, epoch_train_loss=3.4614723360941806
Epoch 242
Epoch 242 :: Batch 0/1
Batch Loss = 3.46040600035756
242, epoch_train_loss=3.46040600035756
Epoch 243
Epoch 243 :: Batch 0/1
Batch Loss = 3.460666893953141
243, epoch_train_loss=3.460666893953141
Epoch 244
Epoch 244 :: Batch 0/1
Batch Loss = 3.4608560846254144
244, epoch_train_loss=3.4608560846254144
Epoch 245
Epoch 245 :: Batch 0/1
Batch Loss = 3.460118835581647
245, epoch_train_loss=3.460118835581647
Epoch 246
Epoch 246 :: Batch 0/1
Batch Loss = 3.460946026378722
246, epoch_train_loss=3.460946026378722
Epoch 247
Epoch 247 :: Batch 0/1
Batch Loss = 3.460054019889899
247, epoch_train_loss=3.460054019889899
Epoch 248
Epoch 248 :: Batch 0/1
Batch Loss = 3.4605245002944085
248, epoch_train_loss=3.4605245002944085
Epoch 249
Epoch 249 :: Batch 0/1
Batch Loss = 3.4602422103210633
249, epoch_train_loss=3.4602422103210633
Epoch 250
Epoch 250 :: Batch 0/1
Batch Loss = 3.4600510954202996
250, epoch_train_loss=3.4600510954202996
Epoch 251
Epoch 251 :: Batch 0/1
Batch Loss = 3.460351467509535
251, epoch_train_loss=3.460351467509535
Epoch 252
Epoch 252 :: Batch 0/1
Batch Loss = 3.459834931000647
252, epoch_train_loss=3.459834931000647
Epoch 253
Epoch 253 :: Batch 0/1
Batch Loss = 3.4602161222553103
253, epoch_train_loss=3.4602161222553103
Epoch 254
Epoch 254 :: Batch 0/1
Batch Loss = 3.459868250038606
254, epoch_train_loss=3.459868250038606
Epoch 255
Epoch 255 :: Batch 0/1
Batch Loss = 3.4598789888087564
255, epoch_train_loss=3.4598789888087564
Epoch 256
Epoch 256 :: Batch 0/1
Batch Loss = 3.4599193427236052
256, epoch_train_loss=3.4599193427236052
Epoch 257
Epoch 257 :: Batch 0/1
Batch Loss = 3.4596234545893663
257, epoch_train_loss=3.4596234545893663
Epoch 258
Epoch 258 :: Batch 0/1
Batch Loss = 3.45981724226443
258, epoch_train_loss=3.45981724226443
Epoch 259
Epoch 259 :: Batch 0/1
Batch Loss = 3.459538212761431
259, epoch_train_loss=3.459538212761431
Epoch 260
Epoch 260 :: Batch 0/1
Batch Loss = 3.4595947489211456
260, epoch_train_loss=3.4595947489211456
Epoch 261
Epoch 261 :: Batch 0/1
Batch Loss = 3.459512931384978
261, epoch_train_loss=3.459512931384978
Epoch 262
Epoch 262 :: Batch 0/1
Batch Loss = 3.4593590856812093
262, epoch_train_loss=3.4593590856812093
Epoch 263
Epoch 263 :: Batch 0/1
Batch Loss = 3.459420424527558
263, epoch_train_loss=3.459420424527558
Epoch 264
Epoch 264 :: Batch 0/1
Batch Loss = 3.459198253900498
264, epoch_train_loss=3.459198253900498
Epoch 265
Epoch 265 :: Batch 0/1
Batch Loss = 3.459214900085859
265, epoch_train_loss=3.459214900085859
Epoch 266
Epoch 266 :: Batch 0/1
Batch Loss = 3.459080817628907
266, epoch_train_loss=3.459080817628907
Epoch 267
Epoch 267 :: Batch 0/1
Batch Loss = 3.458956834177448
267, epoch_train_loss=3.458956834177448
Epoch 268
Epoch 268 :: Batch 0/1
Batch Loss = 3.4589174336051123
268, epoch_train_loss=3.4589174336051123
Epoch 269
Epoch 269 :: Batch 0/1
Batch Loss = 3.4587166604167723
269, epoch_train_loss=3.4587166604167723
Epoch 270
Epoch 270 :: Batch 0/1
Batch Loss = 3.4586570502337373
270, epoch_train_loss=3.4586570502337373
Epoch 271
Epoch 271 :: Batch 0/1
Batch Loss = 3.4584891500117365
271, epoch_train_loss=3.4584891500117365
Epoch 272
Epoch 272 :: Batch 0/1
Batch Loss = 3.4583205898699805
272, epoch_train_loss=3.4583205898699805
Epoch 273
Epoch 273 :: Batch 0/1
Batch Loss = 3.4581978178593924
273, epoch_train_loss=3.4581978178593924
Epoch 274
Epoch 274 :: Batch 0/1
Batch Loss = 3.4579599197556328
274, epoch_train_loss=3.4579599197556328
Epoch 275
Epoch 275 :: Batch 0/1
Batch Loss = 3.457785929819231
275, epoch_train_loss=3.457785929819231
Epoch 276
Epoch 276 :: Batch 0/1
Batch Loss = 3.457559131342105
276, epoch_train_loss=3.457559131342105
Epoch 277
Epoch 277 :: Batch 0/1
Batch Loss = 3.457272860061667
277, epoch_train_loss=3.457272860061667
Epoch 278
Epoch 278 :: Batch 0/1
Batch Loss = 3.4570212496014547
278, epoch_train_loss=3.4570212496014547
Epoch 279
Epoch 279 :: Batch 0/1
Batch Loss = 3.4566810859046173
279, epoch_train_loss=3.4566810859046173
Epoch 280
Epoch 280 :: Batch 0/1
Batch Loss = 3.456297334531731
280, epoch_train_loss=3.456297334531731
Epoch 281
Epoch 281 :: Batch 0/1
Batch Loss = 3.455905543010752
281, epoch_train_loss=3.455905543010752
Epoch 282
Epoch 282 :: Batch 0/1
Batch Loss = 3.455411275292426
282, epoch_train_loss=3.455411275292426
Epoch 283
Epoch 283 :: Batch 0/1
Batch Loss = 3.454835130016231
283, epoch_train_loss=3.454835130016231
Epoch 284
Epoch 284 :: Batch 0/1
Batch Loss = 3.45420096230081
284, epoch_train_loss=3.45420096230081
Epoch 285
Epoch 285 :: Batch 0/1
Batch Loss = 3.4534532237518856
285, epoch_train_loss=3.4534532237518856
Epoch 286
Epoch 286 :: Batch 0/1
Batch Loss = 3.45255920092649
286, epoch_train_loss=3.45255920092649
Epoch 287
Epoch 287 :: Batch 0/1
Batch Loss = 3.4514817166123484
287, epoch_train_loss=3.4514817166123484
Epoch 288
Epoch 288 :: Batch 0/1
Batch Loss = 3.450212002662079
288, epoch_train_loss=3.450212002662079
Epoch 289
Epoch 289 :: Batch 0/1
Batch Loss = 3.448707661950026
289, epoch_train_loss=3.448707661950026
Epoch 290
Epoch 290 :: Batch 0/1
Batch Loss = 3.4471764442914736
290, epoch_train_loss=3.4471764442914736
Epoch 291
Epoch 291 :: Batch 0/1
Batch Loss = 3.448148008658797
291, epoch_train_loss=3.448148008658797
Epoch 292
Epoch 292 :: Batch 0/1
Batch Loss = 3.4818972557025165
292, epoch_train_loss=3.4818972557025165
Epoch 293
Epoch 293 :: Batch 0/1
Batch Loss = 3.634186649148055
293, epoch_train_loss=3.634186649148055
Epoch 294
Epoch 294 :: Batch 0/1
Batch Loss = 3.684786505215952
294, epoch_train_loss=3.684786505215952
Epoch 295
Epoch 295 :: Batch 0/1
Batch Loss = 3.7622093276977884
295, epoch_train_loss=3.7622093276977884
Epoch 296
Epoch 296 :: Batch 0/1
Batch Loss = 3.608786305577716
296, epoch_train_loss=3.608786305577716
Epoch 297
Epoch 297 :: Batch 0/1
Batch Loss = 3.537548118128851
297, epoch_train_loss=3.537548118128851
Epoch 298
Epoch 298 :: Batch 0/1
Batch Loss = 3.555085249552687
298, epoch_train_loss=3.555085249552687
Epoch 299
Epoch 299 :: Batch 0/1
Batch Loss = 3.542878453513595
299, epoch_train_loss=3.542878453513595
Epoch 300
Epoch 300 :: Batch 0/1
Batch Loss = 3.5220723379384746
300, epoch_train_loss=3.5220723379384746
Epoch 301
Epoch 301 :: Batch 0/1
Batch Loss = 3.5307568422442066
301, epoch_train_loss=3.5307568422442066
Epoch 302
Epoch 302 :: Batch 0/1
Batch Loss = 3.5166046147235037
302, epoch_train_loss=3.5166046147235037
Epoch 303
Epoch 303 :: Batch 0/1
Batch Loss = 3.4867335039912644
303, epoch_train_loss=3.4867335039912644
Epoch 304
Epoch 304 :: Batch 0/1
Batch Loss = 3.529329894646821
304, epoch_train_loss=3.529329894646821
Epoch 305
Epoch 305 :: Batch 0/1
Batch Loss = 3.4920351955204785
305, epoch_train_loss=3.4920351955204785
Epoch 306
Epoch 306 :: Batch 0/1
Batch Loss = 3.502921108584798
306, epoch_train_loss=3.502921108584798
Epoch 307
Epoch 307 :: Batch 0/1
Batch Loss = 3.5031975885206896
307, epoch_train_loss=3.5031975885206896
Epoch 308
Epoch 308 :: Batch 0/1
Batch Loss = 3.4843986651877326
308, epoch_train_loss=3.4843986651877326
Epoch 309
Epoch 309 :: Batch 0/1
Batch Loss = 3.484225600987753
309, epoch_train_loss=3.484225600987753
Epoch 310
Epoch 310 :: Batch 0/1
Batch Loss = 3.495851080580093
310, epoch_train_loss=3.495851080580093
Epoch 311
Epoch 311 :: Batch 0/1
Batch Loss = 3.4859185918540927
311, epoch_train_loss=3.4859185918540927
Epoch 312
Epoch 312 :: Batch 0/1
Batch Loss = 3.478455670122929
312, epoch_train_loss=3.478455670122929
Epoch 313
Epoch 313 :: Batch 0/1
Batch Loss = 3.4815778937360005
313, epoch_train_loss=3.4815778937360005
Epoch 314
Epoch 314 :: Batch 0/1
Batch Loss = 3.487702187436508
314, epoch_train_loss=3.487702187436508
Epoch 315
Epoch 315 :: Batch 0/1
Batch Loss = 3.481804755033523
315, epoch_train_loss=3.481804755033523
Epoch 316
Epoch 316 :: Batch 0/1
Batch Loss = 3.4750316314481267
316, epoch_train_loss=3.4750316314481267
Epoch 317
Epoch 317 :: Batch 0/1
Batch Loss = 3.4779070982404514
317, epoch_train_loss=3.4779070982404514
Epoch 318
Epoch 318 :: Batch 0/1
Batch Loss = 3.4796936265764216
318, epoch_train_loss=3.4796936265764216
Epoch 319
Epoch 319 :: Batch 0/1
Batch Loss = 3.475547977361535
319, epoch_train_loss=3.475547977361535
Epoch 320
Epoch 320 :: Batch 0/1
Batch Loss = 3.470685236003202
320, epoch_train_loss=3.470685236003202
Epoch 321
Epoch 321 :: Batch 0/1
Batch Loss = 3.47291636640255
321, epoch_train_loss=3.47291636640255
Epoch 322
Epoch 322 :: Batch 0/1
Batch Loss = 3.4744826035959817
322, epoch_train_loss=3.4744826035959817
Epoch 323
Epoch 323 :: Batch 0/1
Batch Loss = 3.4709506691412684
323, epoch_train_loss=3.4709506691412684
Epoch 324
Epoch 324 :: Batch 0/1
Batch Loss = 3.468337971336467
324, epoch_train_loss=3.468337971336467
Epoch 325
Epoch 325 :: Batch 0/1
Batch Loss = 3.468917654358473
325, epoch_train_loss=3.468917654358473
Epoch 326
Epoch 326 :: Batch 0/1
Batch Loss = 3.470148052920897
326, epoch_train_loss=3.470148052920897
Epoch 327
Epoch 327 :: Batch 0/1
Batch Loss = 3.467662019815485
327, epoch_train_loss=3.467662019815485
Epoch 328
Epoch 328 :: Batch 0/1
Batch Loss = 3.465209749957859
328, epoch_train_loss=3.465209749957859
Epoch 329
Epoch 329 :: Batch 0/1
Batch Loss = 3.4663311225356375
329, epoch_train_loss=3.4663311225356375
Epoch 330
Epoch 330 :: Batch 0/1
Batch Loss = 3.466118880705338
330, epoch_train_loss=3.466118880705338
Epoch 331
Epoch 331 :: Batch 0/1
Batch Loss = 3.464648000976578
331, epoch_train_loss=3.464648000976578
Epoch 332
Epoch 332 :: Batch 0/1
Batch Loss = 3.463375693810385
332, epoch_train_loss=3.463375693810385
Epoch 333
Epoch 333 :: Batch 0/1
Batch Loss = 3.4637235737125738
333, epoch_train_loss=3.4637235737125738
Epoch 334
Epoch 334 :: Batch 0/1
Batch Loss = 3.46424031737177
334, epoch_train_loss=3.46424031737177
Epoch 335
Epoch 335 :: Batch 0/1
Batch Loss = 3.462800257524589
335, epoch_train_loss=3.462800257524589
Epoch 336
Epoch 336 :: Batch 0/1
Batch Loss = 3.4622811544023553
336, epoch_train_loss=3.4622811544023553
Epoch 337
Epoch 337 :: Batch 0/1
Batch Loss = 3.46237103035612
337, epoch_train_loss=3.46237103035612
Epoch 338
Epoch 338 :: Batch 0/1
Batch Loss = 3.4627025900623134
338, epoch_train_loss=3.4627025900623134
Epoch 339
Epoch 339 :: Batch 0/1
Batch Loss = 3.4618445707783096
339, epoch_train_loss=3.4618445707783096
Epoch 340
Epoch 340 :: Batch 0/1
Batch Loss = 3.4614979300258835
340, epoch_train_loss=3.4614979300258835
Epoch 341
Epoch 341 :: Batch 0/1
Batch Loss = 3.4616780567950514
341, epoch_train_loss=3.4616780567950514
Epoch 342
Epoch 342 :: Batch 0/1
Batch Loss = 3.4619160020315007
342, epoch_train_loss=3.4619160020315007
Epoch 343
Epoch 343 :: Batch 0/1
Batch Loss = 3.461323982667124
343, epoch_train_loss=3.461323982667124
Epoch 344
Epoch 344 :: Batch 0/1
Batch Loss = 3.4610664689514596
344, epoch_train_loss=3.4610664689514596
Epoch 345
Epoch 345 :: Batch 0/1
Batch Loss = 3.4611706114748078
345, epoch_train_loss=3.4611706114748078
Epoch 346
Epoch 346 :: Batch 0/1
Batch Loss = 3.4612117055520044
346, epoch_train_loss=3.4612117055520044
Epoch 347
Epoch 347 :: Batch 0/1
Batch Loss = 3.460853940843136
347, epoch_train_loss=3.460853940843136
Epoch 348
Epoch 348 :: Batch 0/1
Batch Loss = 3.4607316875062306
348, epoch_train_loss=3.4607316875062306
Epoch 349
Epoch 349 :: Batch 0/1
Batch Loss = 3.460833910664094
349, epoch_train_loss=3.460833910664094
Epoch 350
Epoch 350 :: Batch 0/1
Batch Loss = 3.4608285976267514
350, epoch_train_loss=3.4608285976267514
Epoch 351
Epoch 351 :: Batch 0/1
Batch Loss = 3.4605539776522365
351, epoch_train_loss=3.4605539776522365
Epoch 352
Epoch 352 :: Batch 0/1
Batch Loss = 3.4604543681213973
352, epoch_train_loss=3.4604543681213973
Epoch 353
Epoch 353 :: Batch 0/1
Batch Loss = 3.460518721974499
353, epoch_train_loss=3.460518721974499
Epoch 354
Epoch 354 :: Batch 0/1
Batch Loss = 3.460494191198601
354, epoch_train_loss=3.460494191198601
Epoch 355
Epoch 355 :: Batch 0/1
Batch Loss = 3.4603278115877254
355, epoch_train_loss=3.4603278115877254
Epoch 356
Epoch 356 :: Batch 0/1
Batch Loss = 3.4602823914325396
356, epoch_train_loss=3.4602823914325396
Epoch 357
Epoch 357 :: Batch 0/1
Batch Loss = 3.460318218722061
357, epoch_train_loss=3.460318218722061
Epoch 358
Epoch 358 :: Batch 0/1
Batch Loss = 3.460245355791084
358, epoch_train_loss=3.460245355791084
Epoch 359
Epoch 359 :: Batch 0/1
Batch Loss = 3.4600960341574623
359, epoch_train_loss=3.4600960341574623
Epoch 360
Epoch 360 :: Batch 0/1
Batch Loss = 3.4600488637739444
360, epoch_train_loss=3.4600488637739444
Epoch 361
Epoch 361 :: Batch 0/1
Batch Loss = 3.4600439301218056
361, epoch_train_loss=3.4600439301218056
Epoch 362
Epoch 362 :: Batch 0/1
Batch Loss = 3.4599642271497486
362, epoch_train_loss=3.4599642271497486
Epoch 363
Epoch 363 :: Batch 0/1
Batch Loss = 3.459856747871366
363, epoch_train_loss=3.459856747871366
Epoch 364
Epoch 364 :: Batch 0/1
Batch Loss = 3.4598182002591136
364, epoch_train_loss=3.4598182002591136
Epoch 365
Epoch 365 :: Batch 0/1
Batch Loss = 3.4597829540586944
365, epoch_train_loss=3.4597829540586944
Epoch 366
Epoch 366 :: Batch 0/1
Batch Loss = 3.459691393052243
366, epoch_train_loss=3.459691393052243
Epoch 367
Epoch 367 :: Batch 0/1
Batch Loss = 3.4596002399272248
367, epoch_train_loss=3.4596002399272248
Epoch 368
Epoch 368 :: Batch 0/1
Batch Loss = 3.4595688530716107
368, epoch_train_loss=3.4595688530716107
Epoch 369
Epoch 369 :: Batch 0/1
Batch Loss = 3.4595249726678063
369, epoch_train_loss=3.4595249726678063
Epoch 370
Epoch 370 :: Batch 0/1
Batch Loss = 3.459447770551383
370, epoch_train_loss=3.459447770551383
Epoch 371
Epoch 371 :: Batch 0/1
Batch Loss = 3.459379549376373
371, epoch_train_loss=3.459379549376373
Epoch 372
Epoch 372 :: Batch 0/1
Batch Loss = 3.459351303040458
372, epoch_train_loss=3.459351303040458
Epoch 373
Epoch 373 :: Batch 0/1
Batch Loss = 3.459292778786534
373, epoch_train_loss=3.459292778786534
Epoch 374
Epoch 374 :: Batch 0/1
Batch Loss = 3.4592244762331403
374, epoch_train_loss=3.4592244762331403
Epoch 375
Epoch 375 :: Batch 0/1
Batch Loss = 3.459170368242019
375, epoch_train_loss=3.459170368242019
Epoch 376
Epoch 376 :: Batch 0/1
Batch Loss = 3.4591371030102915
376, epoch_train_loss=3.4591371030102915
Epoch 377
Epoch 377 :: Batch 0/1
Batch Loss = 3.4590745521103745
377, epoch_train_loss=3.4590745521103745
Epoch 378
Epoch 378 :: Batch 0/1
Batch Loss = 3.459016723998861
378, epoch_train_loss=3.459016723998861
Epoch 379
Epoch 379 :: Batch 0/1
Batch Loss = 3.4589693972772806
379, epoch_train_loss=3.4589693972772806
Epoch 380
Epoch 380 :: Batch 0/1
Batch Loss = 3.458921163325461
380, epoch_train_loss=3.458921163325461
Epoch 381
Epoch 381 :: Batch 0/1
Batch Loss = 3.4588568129765167
381, epoch_train_loss=3.4588568129765167
Epoch 382
Epoch 382 :: Batch 0/1
Batch Loss = 3.4588002850281065
382, epoch_train_loss=3.4588002850281065
Epoch 383
Epoch 383 :: Batch 0/1
Batch Loss = 3.4587536887068366
383, epoch_train_loss=3.4587536887068366
Epoch 384
Epoch 384 :: Batch 0/1
Batch Loss = 3.4586938298165264
384, epoch_train_loss=3.4586938298165264
Epoch 385
Epoch 385 :: Batch 0/1
Batch Loss = 3.4586332588886153
385, epoch_train_loss=3.4586332588886153
Epoch 386
Epoch 386 :: Batch 0/1
Batch Loss = 3.4585774158568054
386, epoch_train_loss=3.4585774158568054
Epoch 387
Epoch 387 :: Batch 0/1
Batch Loss = 3.4585217711219856
387, epoch_train_loss=3.4585217711219856
Epoch 388
Epoch 388 :: Batch 0/1
Batch Loss = 3.4584581162857853
388, epoch_train_loss=3.4584581162857853
Epoch 389
Epoch 389 :: Batch 0/1
Batch Loss = 3.4583956944624092
389, epoch_train_loss=3.4583956944624092
Epoch 390
Epoch 390 :: Batch 0/1
Batch Loss = 3.4583394947579613
390, epoch_train_loss=3.4583394947579613
Epoch 391
Epoch 391 :: Batch 0/1
Batch Loss = 3.458275675303649
391, epoch_train_loss=3.458275675303649
Epoch 392
Epoch 392 :: Batch 0/1
Batch Loss = 3.4582090000336394
392, epoch_train_loss=3.4582090000336394
Epoch 393
Epoch 393 :: Batch 0/1
Batch Loss = 3.458146963468151
393, epoch_train_loss=3.458146963468151
Epoch 394
Epoch 394 :: Batch 0/1
Batch Loss = 3.458081471853088
394, epoch_train_loss=3.458081471853088
Epoch 395
Epoch 395 :: Batch 0/1
Batch Loss = 3.458011754940321
395, epoch_train_loss=3.458011754940321
Epoch 396
Epoch 396 :: Batch 0/1
Batch Loss = 3.457944251486373
396, epoch_train_loss=3.457944251486373
Epoch 397
Epoch 397 :: Batch 0/1
Batch Loss = 3.4578765154932345
397, epoch_train_loss=3.4578765154932345
Epoch 398
Epoch 398 :: Batch 0/1
Batch Loss = 3.4578042229454433
398, epoch_train_loss=3.4578042229454433
Epoch 399
Epoch 399 :: Batch 0/1
Batch Loss = 3.4577310878197767
399, epoch_train_loss=3.4577310878197767
Epoch 400
Epoch 400 :: Batch 0/1
Batch Loss = 3.4576586893480714
400, epoch_train_loss=3.4576586893480714
Epoch 401
Epoch 401 :: Batch 0/1
Batch Loss = 3.4575829322186076
401, epoch_train_loss=3.4575829322186076
Epoch 402
Epoch 402 :: Batch 0/1
Batch Loss = 3.4575042673140035
402, epoch_train_loss=3.4575042673140035
Epoch 403
Epoch 403 :: Batch 0/1
Batch Loss = 3.457425965298816
403, epoch_train_loss=3.457425965298816
Epoch 404
Epoch 404 :: Batch 0/1
Batch Loss = 3.4573460617992846
404, epoch_train_loss=3.4573460617992846
Epoch 405
Epoch 405 :: Batch 0/1
Batch Loss = 3.45726214368181
405, epoch_train_loss=3.45726214368181
Epoch 406
Epoch 406 :: Batch 0/1
Batch Loss = 3.4571759777831654
406, epoch_train_loss=3.4571759777831654
Epoch 407
Epoch 407 :: Batch 0/1
Batch Loss = 3.457088294529006
407, epoch_train_loss=3.457088294529006
Epoch 408
Epoch 408 :: Batch 0/1
Batch Loss = 3.4569972458574534
408, epoch_train_loss=3.4569972458574534
Epoch 409
Epoch 409 :: Batch 0/1
Batch Loss = 3.456903444846692
409, epoch_train_loss=3.456903444846692
Epoch 410
Epoch 410 :: Batch 0/1
Batch Loss = 3.456807677880811
410, epoch_train_loss=3.456807677880811
Epoch 411
Epoch 411 :: Batch 0/1
Batch Loss = 3.4567083218095878
411, epoch_train_loss=3.4567083218095878
Epoch 412
Epoch 412 :: Batch 0/1
Batch Loss = 3.45660549908206
412, epoch_train_loss=3.45660549908206
Epoch 413
Epoch 413 :: Batch 0/1
Batch Loss = 3.456500741829821
413, epoch_train_loss=3.456500741829821
Epoch 414
Epoch 414 :: Batch 0/1
Batch Loss = 3.456395438232018
414, epoch_train_loss=3.456395438232018
Epoch 415
Epoch 415 :: Batch 0/1
Batch Loss = 3.4562959095513794
415, epoch_train_loss=3.4562959095513794
Epoch 416
Epoch 416 :: Batch 0/1
Batch Loss = 3.456229657720748
416, epoch_train_loss=3.456229657720748
Epoch 417
Epoch 417 :: Batch 0/1
Batch Loss = 3.4563039040783567
417, epoch_train_loss=3.4563039040783567
Epoch 418
Epoch 418 :: Batch 0/1
Batch Loss = 3.456912166521648
418, epoch_train_loss=3.456912166521648
Epoch 419
Epoch 419 :: Batch 0/1
Batch Loss = 3.459483513611296
419, epoch_train_loss=3.459483513611296
Epoch 420
Epoch 420 :: Batch 0/1
Batch Loss = 3.4646758314050756
420, epoch_train_loss=3.4646758314050756
Epoch 421
Epoch 421 :: Batch 0/1
Batch Loss = 3.466557896109587
421, epoch_train_loss=3.466557896109587
Epoch 422
Epoch 422 :: Batch 0/1
Batch Loss = 3.458293046528261
422, epoch_train_loss=3.458293046528261
Epoch 423
Epoch 423 :: Batch 0/1
Batch Loss = 3.4569771519216332
423, epoch_train_loss=3.4569771519216332
Epoch 424
Epoch 424 :: Batch 0/1
Batch Loss = 3.4614217859532266
424, epoch_train_loss=3.4614217859532266
Epoch 425
Epoch 425 :: Batch 0/1
Batch Loss = 3.45594557599543
425, epoch_train_loss=3.45594557599543
Epoch 426
Epoch 426 :: Batch 0/1
Batch Loss = 3.4579210084685936
426, epoch_train_loss=3.4579210084685936
Epoch 427
Epoch 427 :: Batch 0/1
Batch Loss = 3.457300116270288
427, epoch_train_loss=3.457300116270288
Epoch 428
Epoch 428 :: Batch 0/1
Batch Loss = 3.4552868320635106
428, epoch_train_loss=3.4552868320635106
Epoch 429
Epoch 429 :: Batch 0/1
Batch Loss = 3.457308872304219
429, epoch_train_loss=3.457308872304219
Epoch 430
Epoch 430 :: Batch 0/1
Batch Loss = 3.4543901121935794
430, epoch_train_loss=3.4543901121935794
Epoch 431
Epoch 431 :: Batch 0/1
Batch Loss = 3.456451969877829
431, epoch_train_loss=3.456451969877829
Epoch 432
Epoch 432 :: Batch 0/1
Batch Loss = 3.4543633021748064
432, epoch_train_loss=3.4543633021748064
Epoch 433
Epoch 433 :: Batch 0/1
Batch Loss = 3.4548715532613277
433, epoch_train_loss=3.4548715532613277
Epoch 434
Epoch 434 :: Batch 0/1
Batch Loss = 3.4543961318314205
434, epoch_train_loss=3.4543961318314205
Epoch 435
Epoch 435 :: Batch 0/1
Batch Loss = 3.453443267137927
435, epoch_train_loss=3.453443267137927
Epoch 436
Epoch 436 :: Batch 0/1
Batch Loss = 3.4540328263434286
436, epoch_train_loss=3.4540328263434286
Epoch 437
Epoch 437 :: Batch 0/1
Batch Loss = 3.4524501933731706
437, epoch_train_loss=3.4524501933731706
Epoch 438
Epoch 438 :: Batch 0/1
Batch Loss = 3.453192439040684
438, epoch_train_loss=3.453192439040684
Epoch 439
Epoch 439 :: Batch 0/1
Batch Loss = 3.4517981929813493
439, epoch_train_loss=3.4517981929813493
Epoch 440
Epoch 440 :: Batch 0/1
Batch Loss = 3.4519445811094833
440, epoch_train_loss=3.4519445811094833
Epoch 441
Epoch 441 :: Batch 0/1
Batch Loss = 3.4512387173338315
441, epoch_train_loss=3.4512387173338315
Epoch 442
Epoch 442 :: Batch 0/1
Batch Loss = 3.450565841394119
442, epoch_train_loss=3.450565841394119
Epoch 443
Epoch 443 :: Batch 0/1
Batch Loss = 3.4504552037021265
443, epoch_train_loss=3.4504552037021265
Epoch 444
Epoch 444 :: Batch 0/1
Batch Loss = 3.449240797160528
444, epoch_train_loss=3.449240797160528
Epoch 445
Epoch 445 :: Batch 0/1
Batch Loss = 3.449209244300915
445, epoch_train_loss=3.449209244300915
Epoch 446
Epoch 446 :: Batch 0/1
Batch Loss = 3.4481959320748174
446, epoch_train_loss=3.4481959320748174
Epoch 447
Epoch 447 :: Batch 0/1
Batch Loss = 3.447394280594696
447, epoch_train_loss=3.447394280594696
Epoch 448
Epoch 448 :: Batch 0/1
Batch Loss = 3.446939710999653
448, epoch_train_loss=3.446939710999653
Epoch 449
Epoch 449 :: Batch 0/1
Batch Loss = 3.4456038265037683
449, epoch_train_loss=3.4456038265037683
Epoch 450
Epoch 450 :: Batch 0/1
Batch Loss = 3.4448359147109415
450, epoch_train_loss=3.4448359147109415
Epoch 451
Epoch 451 :: Batch 0/1
Batch Loss = 3.443881120402403
451, epoch_train_loss=3.443881120402403
Epoch 452
Epoch 452 :: Batch 0/1
Batch Loss = 3.4423937190845484
452, epoch_train_loss=3.4423937190845484
Epoch 453
Epoch 453 :: Batch 0/1
Batch Loss = 3.441314567958872
453, epoch_train_loss=3.441314567958872
Epoch 454
Epoch 454 :: Batch 0/1
Batch Loss = 3.4399910524444195
454, epoch_train_loss=3.4399910524444195
Epoch 455
Epoch 455 :: Batch 0/1
Batch Loss = 3.438214144303009
455, epoch_train_loss=3.438214144303009
Epoch 456
Epoch 456 :: Batch 0/1
Batch Loss = 3.4365966402807833
456, epoch_train_loss=3.4365966402807833
Epoch 457
Epoch 457 :: Batch 0/1
Batch Loss = 3.434945831999068
457, epoch_train_loss=3.434945831999068
Epoch 458
Epoch 458 :: Batch 0/1
Batch Loss = 3.4328599396196418
458, epoch_train_loss=3.4328599396196418
Epoch 459
Epoch 459 :: Batch 0/1
Batch Loss = 3.4305561230317774
459, epoch_train_loss=3.4305561230317774
Epoch 460
Epoch 460 :: Batch 0/1
Batch Loss = 3.4282968321052043
460, epoch_train_loss=3.4282968321052043
Epoch 461
Epoch 461 :: Batch 0/1
Batch Loss = 3.4259804287906386
461, epoch_train_loss=3.4259804287906386
Epoch 462
Epoch 462 :: Batch 0/1
Batch Loss = 3.423554698031941
462, epoch_train_loss=3.423554698031941
Epoch 463
Epoch 463 :: Batch 0/1
Batch Loss = 3.4211918556084506
463, epoch_train_loss=3.4211918556084506
Epoch 464
Epoch 464 :: Batch 0/1
Batch Loss = 3.4194211033167066
464, epoch_train_loss=3.4194211033167066
Epoch 465
Epoch 465 :: Batch 0/1
Batch Loss = 3.4223576460659086
465, epoch_train_loss=3.4223576460659086
Epoch 466
Epoch 466 :: Batch 0/1
Batch Loss = 3.4351207362957585
466, epoch_train_loss=3.4351207362957585
Epoch 467
Epoch 467 :: Batch 0/1
Batch Loss = 3.4861444852651267
467, epoch_train_loss=3.4861444852651267
Epoch 468
Epoch 468 :: Batch 0/1
Batch Loss = 3.4558612523167107
468, epoch_train_loss=3.4558612523167107
Epoch 469
Epoch 469 :: Batch 0/1
Batch Loss = 3.4653073750309877
469, epoch_train_loss=3.4653073750309877
Epoch 470
Epoch 470 :: Batch 0/1
Batch Loss = 3.4131480437852404
470, epoch_train_loss=3.4131480437852404
Epoch 471
Epoch 471 :: Batch 0/1
Batch Loss = 3.6695330399292057
471, epoch_train_loss=3.6695330399292057
Epoch 472
Epoch 472 :: Batch 0/1
Batch Loss = 3.6580260281119337
472, epoch_train_loss=3.6580260281119337
Epoch 473
Epoch 473 :: Batch 0/1
Batch Loss = 3.631997171683586
473, epoch_train_loss=3.631997171683586
Epoch 474
Epoch 474 :: Batch 0/1
Batch Loss = 3.9648268460627016
474, epoch_train_loss=3.9648268460627016
Epoch 475
Epoch 475 :: Batch 0/1
Batch Loss = 3.794750565082176
475, epoch_train_loss=3.794750565082176
Epoch 476
Epoch 476 :: Batch 0/1
Batch Loss = 3.596232386372983
476, epoch_train_loss=3.596232386372983
Epoch 477
Epoch 477 :: Batch 0/1
Batch Loss = 3.8329757490733956
477, epoch_train_loss=3.8329757490733956
Epoch 478
Epoch 478 :: Batch 0/1
Batch Loss = 3.9071958295051474
478, epoch_train_loss=3.9071958295051474
Epoch 479
Epoch 479 :: Batch 0/1
Batch Loss = 3.830580415192252
479, epoch_train_loss=3.830580415192252
Epoch 480
Epoch 480 :: Batch 0/1
Batch Loss = 3.6865281179847504
480, epoch_train_loss=3.6865281179847504
Epoch 481
Epoch 481 :: Batch 0/1
Batch Loss = 3.737797742805704
481, epoch_train_loss=3.737797742805704
Epoch 482
Epoch 482 :: Batch 0/1
Batch Loss = 3.567125745584261
482, epoch_train_loss=3.567125745584261
Epoch 483
Epoch 483 :: Batch 0/1
Batch Loss = 3.6090009787916713
483, epoch_train_loss=3.6090009787916713
Epoch 484
Epoch 484 :: Batch 0/1
Batch Loss = 3.550996428655788
484, epoch_train_loss=3.550996428655788
Epoch 485
Epoch 485 :: Batch 0/1
Batch Loss = 3.7016222858685275
485, epoch_train_loss=3.7016222858685275
Epoch 486
Epoch 486 :: Batch 0/1
Batch Loss = 3.49979537207917
486, epoch_train_loss=3.49979537207917
Epoch 487
Epoch 487 :: Batch 0/1
Batch Loss = 3.5932068943590356
487, epoch_train_loss=3.5932068943590356
Epoch 488
Epoch 488 :: Batch 0/1
Batch Loss = 3.4786080130275345
488, epoch_train_loss=3.4786080130275345
Epoch 489
Epoch 489 :: Batch 0/1
Batch Loss = 3.580541063593241
489, epoch_train_loss=3.580541063593241
Epoch 490
Epoch 490 :: Batch 0/1
Batch Loss = 3.501203728805
490, epoch_train_loss=3.501203728805
Epoch 491
Epoch 491 :: Batch 0/1
Batch Loss = 3.553227756437489
491, epoch_train_loss=3.553227756437489
Epoch 492
Epoch 492 :: Batch 0/1
Batch Loss = 3.4911962559380596
492, epoch_train_loss=3.4911962559380596
Epoch 493
Epoch 493 :: Batch 0/1
Batch Loss = 3.504009054337825
493, epoch_train_loss=3.504009054337825
Epoch 494
Epoch 494 :: Batch 0/1
Batch Loss = 3.5006268332065247
494, epoch_train_loss=3.5006268332065247
Epoch 495
Epoch 495 :: Batch 0/1
Batch Loss = 3.4914634941072804
495, epoch_train_loss=3.4914634941072804
Epoch 496
Epoch 496 :: Batch 0/1
Batch Loss = 3.5163279326218166
496, epoch_train_loss=3.5163279326218166
Epoch 497
Epoch 497 :: Batch 0/1
Batch Loss = 3.475901786199841
497, epoch_train_loss=3.475901786199841
Epoch 498
Epoch 498 :: Batch 0/1
Batch Loss = 3.502720164522465
498, epoch_train_loss=3.502720164522465
Epoch 499
Epoch 499 :: Batch 0/1
Batch Loss = 3.471311008552075
499, epoch_train_loss=3.471311008552075
Epoch 500
Epoch 500 :: Batch 0/1
Batch Loss = 3.4875557809276274
500, epoch_train_loss=3.4875557809276274
Epoch 501
Epoch 501 :: Batch 0/1
Batch Loss = 3.48770784041176
501, epoch_train_loss=3.48770784041176
Epoch 502
Epoch 502 :: Batch 0/1
Batch Loss = 3.473002438894389
502, epoch_train_loss=3.473002438894389
Epoch 503
Epoch 503 :: Batch 0/1
Batch Loss = 3.485376651375775
503, epoch_train_loss=3.485376651375775
Epoch 504
Epoch 504 :: Batch 0/1
Batch Loss = 3.468025842999998
504, epoch_train_loss=3.468025842999998
Epoch 505
Epoch 505 :: Batch 0/1
Batch Loss = 3.473919184721204
505, epoch_train_loss=3.473919184721204
Epoch 506
Epoch 506 :: Batch 0/1
Batch Loss = 3.477166588913152
506, epoch_train_loss=3.477166588913152
Epoch 507
Epoch 507 :: Batch 0/1
Batch Loss = 3.468957164524569
507, epoch_train_loss=3.468957164524569
Epoch 508
Epoch 508 :: Batch 0/1
Batch Loss = 3.477403206828191
508, epoch_train_loss=3.477403206828191
Epoch 509
Epoch 509 :: Batch 0/1
Batch Loss = 3.465065871412775
509, epoch_train_loss=3.465065871412775
Epoch 510
Epoch 510 :: Batch 0/1
Batch Loss = 3.4696808160286015
510, epoch_train_loss=3.4696808160286015
Epoch 511
Epoch 511 :: Batch 0/1
Batch Loss = 3.469330795142517
511, epoch_train_loss=3.469330795142517
Epoch 512
Epoch 512 :: Batch 0/1
Batch Loss = 3.4653822522485007
512, epoch_train_loss=3.4653822522485007
Epoch 513
Epoch 513 :: Batch 0/1
Batch Loss = 3.4704503728673326
513, epoch_train_loss=3.4704503728673326
Epoch 514
Epoch 514 :: Batch 0/1
Batch Loss = 3.464242727946543
514, epoch_train_loss=3.464242727946543
Epoch 515
Epoch 515 :: Batch 0/1
Batch Loss = 3.465176554064167
515, epoch_train_loss=3.465176554064167
Epoch 516
Epoch 516 :: Batch 0/1
Batch Loss = 3.4660887124521254
516, epoch_train_loss=3.4660887124521254
Epoch 517
Epoch 517 :: Batch 0/1
Batch Loss = 3.463310562991324
517, epoch_train_loss=3.463310562991324
Epoch 518
Epoch 518 :: Batch 0/1
Batch Loss = 3.4670055162258167
518, epoch_train_loss=3.4670055162258167
Epoch 519
Epoch 519 :: Batch 0/1
Batch Loss = 3.4632099234925757
519, epoch_train_loss=3.4632099234925757
Epoch 520
Epoch 520 :: Batch 0/1
Batch Loss = 3.463857948527304
520, epoch_train_loss=3.463857948527304
Epoch 521
Epoch 521 :: Batch 0/1
Batch Loss = 3.4634959976829376
521, epoch_train_loss=3.4634959976829376
Epoch 522
Epoch 522 :: Batch 0/1
Batch Loss = 3.4620686117194777
522, epoch_train_loss=3.4620686117194777
Epoch 523
Epoch 523 :: Batch 0/1
Batch Loss = 3.4642210367575474
523, epoch_train_loss=3.4642210367575474
Epoch 524
Epoch 524 :: Batch 0/1
Batch Loss = 3.462016663120821
524, epoch_train_loss=3.462016663120821
Epoch 525
Epoch 525 :: Batch 0/1
Batch Loss = 3.462758123340894
525, epoch_train_loss=3.462758123340894
Epoch 526
Epoch 526 :: Batch 0/1
Batch Loss = 3.4620910555229503
526, epoch_train_loss=3.4620910555229503
Epoch 527
Epoch 527 :: Batch 0/1
Batch Loss = 3.461403619066516
527, epoch_train_loss=3.461403619066516
Epoch 528
Epoch 528 :: Batch 0/1
Batch Loss = 3.462533293788943
528, epoch_train_loss=3.462533293788943
Epoch 529
Epoch 529 :: Batch 0/1
Batch Loss = 3.4612057607717412
529, epoch_train_loss=3.4612057607717412
Epoch 530
Epoch 530 :: Batch 0/1
Batch Loss = 3.4619770384030075
530, epoch_train_loss=3.4619770384030075
Epoch 531
Epoch 531 :: Batch 0/1
Batch Loss = 3.460944638195085
531, epoch_train_loss=3.460944638195085
Epoch 532
Epoch 532 :: Batch 0/1
Batch Loss = 3.4610208177048785
532, epoch_train_loss=3.4610208177048785
Epoch 533
Epoch 533 :: Batch 0/1
Batch Loss = 3.4612137224734263
533, epoch_train_loss=3.4612137224734263
Epoch 534
Epoch 534 :: Batch 0/1
Batch Loss = 3.4607363471214163
534, epoch_train_loss=3.4607363471214163
Epoch 535
Epoch 535 :: Batch 0/1
Batch Loss = 3.46127649929058
535, epoch_train_loss=3.46127649929058
Epoch 536
Epoch 536 :: Batch 0/1
Batch Loss = 3.460530438201626
536, epoch_train_loss=3.460530438201626
Epoch 537
Epoch 537 :: Batch 0/1
Batch Loss = 3.4608191139571605
537, epoch_train_loss=3.4608191139571605
Epoch 538
Epoch 538 :: Batch 0/1
Batch Loss = 3.460579079408116
538, epoch_train_loss=3.460579079408116
Epoch 539
Epoch 539 :: Batch 0/1
Batch Loss = 3.460565869623061
539, epoch_train_loss=3.460565869623061
Epoch 540
Epoch 540 :: Batch 0/1
Batch Loss = 3.4607226915220535
540, epoch_train_loss=3.4607226915220535
Epoch 541
Epoch 541 :: Batch 0/1
Batch Loss = 3.460335203775419
541, epoch_train_loss=3.460335203775419
Epoch 542
Epoch 542 :: Batch 0/1
Batch Loss = 3.4605673075540375
542, epoch_train_loss=3.4605673075540375
Epoch 543
Epoch 543 :: Batch 0/1
Batch Loss = 3.460229340130649
543, epoch_train_loss=3.460229340130649
Epoch 544
Epoch 544 :: Batch 0/1
Batch Loss = 3.4604469374643423
544, epoch_train_loss=3.4604469374643423
Epoch 545
Epoch 545 :: Batch 0/1
Batch Loss = 3.4603408935548674
545, epoch_train_loss=3.4603408935548674
Epoch 546
Epoch 546 :: Batch 0/1
Batch Loss = 3.460317909956645
546, epoch_train_loss=3.460317909956645
Epoch 547
Epoch 547 :: Batch 0/1
Batch Loss = 3.4603275612050557
547, epoch_train_loss=3.4603275612050557
Epoch 548
Epoch 548 :: Batch 0/1
Batch Loss = 3.460150847324926
548, epoch_train_loss=3.460150847324926
Epoch 549
Epoch 549 :: Batch 0/1
Batch Loss = 3.4602794732315414
549, epoch_train_loss=3.4602794732315414
Epoch 550
Epoch 550 :: Batch 0/1
Batch Loss = 3.4601276371624627
550, epoch_train_loss=3.4601276371624627
Epoch 551
Epoch 551 :: Batch 0/1
Batch Loss = 3.4602335805010576
551, epoch_train_loss=3.4602335805010576
Epoch 552
Epoch 552 :: Batch 0/1
Batch Loss = 3.4601111852576847
552, epoch_train_loss=3.4601111852576847
Epoch 553
Epoch 553 :: Batch 0/1
Batch Loss = 3.4601212455182546
553, epoch_train_loss=3.4601212455182546
Epoch 554
Epoch 554 :: Batch 0/1
Batch Loss = 3.460095054224874
554, epoch_train_loss=3.460095054224874
Epoch 555
Epoch 555 :: Batch 0/1
Batch Loss = 3.46006142766826
555, epoch_train_loss=3.46006142766826
Epoch 556
Epoch 556 :: Batch 0/1
Batch Loss = 3.4601014166311472
556, epoch_train_loss=3.4601014166311472
Epoch 557
Epoch 557 :: Batch 0/1
Batch Loss = 3.460016780195863
557, epoch_train_loss=3.460016780195863
Epoch 558
Epoch 558 :: Batch 0/1
Batch Loss = 3.4600502945570533
558, epoch_train_loss=3.4600502945570533
Epoch 559
Epoch 559 :: Batch 0/1
Batch Loss = 3.4599697676845467
559, epoch_train_loss=3.4599697676845467
Epoch 560
Epoch 560 :: Batch 0/1
Batch Loss = 3.4600013558497515
560, epoch_train_loss=3.4600013558497515
Epoch 561
Epoch 561 :: Batch 0/1
Batch Loss = 3.4599632002460567
561, epoch_train_loss=3.4599632002460567
Epoch 562
Epoch 562 :: Batch 0/1
Batch Loss = 3.4599636202107473
562, epoch_train_loss=3.4599636202107473
Epoch 563
Epoch 563 :: Batch 0/1
Batch Loss = 3.4599439455319034
563, epoch_train_loss=3.4599439455319034
Epoch 564
Epoch 564 :: Batch 0/1
Batch Loss = 3.4599131081736636
564, epoch_train_loss=3.4599131081736636
Epoch 565
Epoch 565 :: Batch 0/1
Batch Loss = 3.4599176952570554
565, epoch_train_loss=3.4599176952570554
Epoch 566
Epoch 566 :: Batch 0/1
Batch Loss = 3.4598828462200273
566, epoch_train_loss=3.4598828462200273
Epoch 567
Epoch 567 :: Batch 0/1
Batch Loss = 3.4598943457195843
567, epoch_train_loss=3.4598943457195843
Epoch 568
Epoch 568 :: Batch 0/1
Batch Loss = 3.459854768581282
568, epoch_train_loss=3.459854768581282
Epoch 569
Epoch 569 :: Batch 0/1
Batch Loss = 3.459856298548439
569, epoch_train_loss=3.459856298548439
Epoch 570
Epoch 570 :: Batch 0/1
Batch Loss = 3.459827385668547
570, epoch_train_loss=3.459827385668547
Epoch 571
Epoch 571 :: Batch 0/1
Batch Loss = 3.4598246827195065
571, epoch_train_loss=3.4598246827195065
Epoch 572
Epoch 572 :: Batch 0/1
Batch Loss = 3.4598096847124626
572, epoch_train_loss=3.4598096847124626
Epoch 573
Epoch 573 :: Batch 0/1
Batch Loss = 3.459795232769386
573, epoch_train_loss=3.459795232769386
Epoch 574
Epoch 574 :: Batch 0/1
Batch Loss = 3.4597842822212757
574, epoch_train_loss=3.4597842822212757
Epoch 575
Epoch 575 :: Batch 0/1
Batch Loss = 3.4597628599900188
575, epoch_train_loss=3.4597628599900188
Epoch 576
Epoch 576 :: Batch 0/1
Batch Loss = 3.459758429070477
576, epoch_train_loss=3.459758429070477
Epoch 577
Epoch 577 :: Batch 0/1
Batch Loss = 3.459738248853882
577, epoch_train_loss=3.459738248853882
Epoch 578
Epoch 578 :: Batch 0/1
Batch Loss = 3.4597340604463613
578, epoch_train_loss=3.4597340604463613
Epoch 579
Epoch 579 :: Batch 0/1
Batch Loss = 3.459713641992537
579, epoch_train_loss=3.459713641992537
Epoch 580
Epoch 580 :: Batch 0/1
Batch Loss = 3.4597060271616855
580, epoch_train_loss=3.4597060271616855
Epoch 581
Epoch 581 :: Batch 0/1
Batch Loss = 3.459689818778158
581, epoch_train_loss=3.459689818778158
Epoch 582
Epoch 582 :: Batch 0/1
Batch Loss = 3.4596807504904517
582, epoch_train_loss=3.4596807504904517
Epoch 583
Epoch 583 :: Batch 0/1
Batch Loss = 3.459668055729736
583, epoch_train_loss=3.459668055729736
Epoch 584
Epoch 584 :: Batch 0/1
Batch Loss = 3.459655152553867
584, epoch_train_loss=3.459655152553867
Epoch 585
Epoch 585 :: Batch 0/1
Batch Loss = 3.4596437474506825
585, epoch_train_loss=3.4596437474506825
Epoch 586
Epoch 586 :: Batch 0/1
Batch Loss = 3.459629527582761
586, epoch_train_loss=3.459629527582761
Epoch 587
Epoch 587 :: Batch 0/1
Batch Loss = 3.459620476458465
587, epoch_train_loss=3.459620476458465
Epoch 588
Epoch 588 :: Batch 0/1
Batch Loss = 3.4596064567320224
588, epoch_train_loss=3.4596064567320224
Epoch 589
Epoch 589 :: Batch 0/1
Batch Loss = 3.4595972310897336
589, epoch_train_loss=3.4595972310897336
Epoch 590
Epoch 590 :: Batch 0/1
Batch Loss = 3.4595828177273087
590, epoch_train_loss=3.4595828177273087
Epoch 591
Epoch 591 :: Batch 0/1
Batch Loss = 3.459572924804648
591, epoch_train_loss=3.459572924804648
Epoch 592
Epoch 592 :: Batch 0/1
Batch Loss = 3.459559747953431
592, epoch_train_loss=3.459559747953431
Epoch 593
Epoch 593 :: Batch 0/1
Batch Loss = 3.459549576048556
593, epoch_train_loss=3.459549576048556
Epoch 594
Epoch 594 :: Batch 0/1
Batch Loss = 3.4595372627893672
594, epoch_train_loss=3.4595372627893672
Epoch 595
Epoch 595 :: Batch 0/1
Batch Loss = 3.4595260489074997
595, epoch_train_loss=3.4595260489074997
Epoch 596
Epoch 596 :: Batch 0/1
Batch Loss = 3.4595143997303013
596, epoch_train_loss=3.4595143997303013
Epoch 597
Epoch 597 :: Batch 0/1
Batch Loss = 3.4595029897198586
597, epoch_train_loss=3.4595029897198586
Epoch 598
Epoch 598 :: Batch 0/1
Batch Loss = 3.4594922622749977
598, epoch_train_loss=3.4594922622749977
Epoch 599
Epoch 599 :: Batch 0/1
Batch Loss = 3.4594806439290386
599, epoch_train_loss=3.4594806439290386
Epoch 600
Epoch 600 :: Batch 0/1
Batch Loss = 3.459470012545785
600, epoch_train_loss=3.459470012545785
Epoch 601
Epoch 601 :: Batch 0/1
Batch Loss = 3.4594581913545537
601, epoch_train_loss=3.4594581913545537
Epoch 602
Epoch 602 :: Batch 0/1
Batch Loss = 3.4594478056195337
602, epoch_train_loss=3.4594478056195337
Epoch 603
Epoch 603 :: Batch 0/1
Batch Loss = 3.4594363595820936
603, epoch_train_loss=3.4594363595820936
Epoch 604
Epoch 604 :: Batch 0/1
Batch Loss = 3.459426141041551
604, epoch_train_loss=3.459426141041551
Epoch 605
Epoch 605 :: Batch 0/1
Batch Loss = 3.4594148709569863
605, epoch_train_loss=3.4594148709569863
Epoch 606
Epoch 606 :: Batch 0/1
Batch Loss = 3.459404556623595
606, epoch_train_loss=3.459404556623595
Epoch 607
Epoch 607 :: Batch 0/1
Batch Loss = 3.4593935867345573
607, epoch_train_loss=3.4593935867345573
Epoch 608
Epoch 608 :: Batch 0/1
Batch Loss = 3.45938337608187
608, epoch_train_loss=3.45938337608187
Epoch 609
Epoch 609 :: Batch 0/1
Batch Loss = 3.459372755998772
609, epoch_train_loss=3.459372755998772
Epoch 610
Epoch 610 :: Batch 0/1
Batch Loss = 3.4593625180735708
610, epoch_train_loss=3.4593625180735708
Epoch 611
Epoch 611 :: Batch 0/1
Batch Loss = 3.459352079647801
611, epoch_train_loss=3.459352079647801
Epoch 612
Epoch 612 :: Batch 0/1
Batch Loss = 3.459341882749813
612, epoch_train_loss=3.459341882749813
Epoch 613
Epoch 613 :: Batch 0/1
Batch Loss = 3.459331749169481
613, epoch_train_loss=3.459331749169481
Epoch 614
Epoch 614 :: Batch 0/1
Batch Loss = 3.459321697973688
614, epoch_train_loss=3.459321697973688
Epoch 615
Epoch 615 :: Batch 0/1
Batch Loss = 3.459311760154801
615, epoch_train_loss=3.459311760154801
Epoch 616
Epoch 616 :: Batch 0/1
Batch Loss = 3.4593017697455326
616, epoch_train_loss=3.4593017697455326
Epoch 617
Epoch 617 :: Batch 0/1
Batch Loss = 3.4592919912283038
617, epoch_train_loss=3.4592919912283038
Epoch 618
Epoch 618 :: Batch 0/1
Batch Loss = 3.459282163344976
618, epoch_train_loss=3.459282163344976
Epoch 619
Epoch 619 :: Batch 0/1
Batch Loss = 3.4592725830991085
619, epoch_train_loss=3.4592725830991085
Epoch 620
Epoch 620 :: Batch 0/1
Batch Loss = 3.4592629027678115
620, epoch_train_loss=3.4592629027678115
Epoch 621
Epoch 621 :: Batch 0/1
Batch Loss = 3.459253447640148
621, epoch_train_loss=3.459253447640148
Epoch 622
Epoch 622 :: Batch 0/1
Batch Loss = 3.459243909412582
622, epoch_train_loss=3.459243909412582
Epoch 623
Epoch 623 :: Batch 0/1
Batch Loss = 3.4592346111328487
623, epoch_train_loss=3.4592346111328487
Epoch 624
Epoch 624 :: Batch 0/1
Batch Loss = 3.459225252604957
624, epoch_train_loss=3.459225252604957
Epoch 625
Epoch 625 :: Batch 0/1
Batch Loss = 3.4592160935086507
625, epoch_train_loss=3.4592160935086507
Epoch 626
Epoch 626 :: Batch 0/1
Batch Loss = 3.459206876472845
626, epoch_train_loss=3.459206876472845
Epoch 627
Epoch 627 :: Batch 0/1
Batch Loss = 3.4591978406930313
627, epoch_train_loss=3.4591978406930313
Epoch 628
Epoch 628 :: Batch 0/1
Batch Loss = 3.459188785942396
628, epoch_train_loss=3.459188785942396
Epoch 629
Epoch 629 :: Batch 0/1
Batch Loss = 3.4591798968502
629, epoch_train_loss=3.4591798968502
Epoch 630
Epoch 630 :: Batch 0/1
Batch Loss = 3.459170996205262
630, epoch_train_loss=3.459170996205262
Epoch 631
Epoch 631 :: Batch 0/1
Batch Loss = 3.4591622268257844
631, epoch_train_loss=3.4591622268257844
Epoch 632
Epoch 632 :: Batch 0/1
Batch Loss = 3.4591534612616535
632, epoch_train_loss=3.4591534612616535
Epoch 633
Epoch 633 :: Batch 0/1
Batch Loss = 3.4591448196714003
633, epoch_train_loss=3.4591448196714003
Epoch 634
Epoch 634 :: Batch 0/1
Batch Loss = 3.4591361997276273
634, epoch_train_loss=3.4591361997276273
Epoch 635
Epoch 635 :: Batch 0/1
Batch Loss = 3.459127684916967
635, epoch_train_loss=3.459127684916967
Epoch 636
Epoch 636 :: Batch 0/1
Batch Loss = 3.4591191945769713
636, epoch_train_loss=3.4591191945769713
Epoch 637
Epoch 637 :: Batch 0/1
Batch Loss = 3.4591107974707773
637, epoch_train_loss=3.4591107974707773
Epoch 638
Epoch 638 :: Batch 0/1
Batch Loss = 3.4591024362633056
638, epoch_train_loss=3.4591024362633056
Epoch 639
Epoch 639 :: Batch 0/1
Batch Loss = 3.45909416140844
639, epoch_train_loss=3.45909416140844
Epoch 640
Epoch 640 :: Batch 0/1
Batch Loss = 3.4590859235463154
640, epoch_train_loss=3.4590859235463154
Epoch 641
Epoch 641 :: Batch 0/1
Batch Loss = 3.4590777588468797
641, epoch_train_loss=3.4590777588468797
Epoch 642
Epoch 642 :: Batch 0/1
Batch Loss = 3.459069631741584
642, epoch_train_loss=3.459069631741584
Epoch 643
Epoch 643 :: Batch 0/1
Batch Loss = 3.459061574031043
643, epoch_train_loss=3.459061574031043
Epoch 644
Epoch 644 :: Batch 0/1
Batch Loss = 3.459053557966211
644, epoch_train_loss=3.459053557966211
Epoch 645
Epoch 645 :: Batch 0/1
Batch Loss = 3.4590456045953584
645, epoch_train_loss=3.4590456045953584
Epoch 646
Epoch 646 :: Batch 0/1
Batch Loss = 3.459037689086338
646, epoch_train_loss=3.459037689086338
Epoch 647
Epoch 647 :: Batch 0/1
Batch Loss = 3.459029829717287
647, epoch_train_loss=3.459029829717287
Epoch 648
Epoch 648 :: Batch 0/1
Batch Loss = 3.4590220088888546
648, epoch_train_loss=3.4590220088888546
Epoch 649
Epoch 649 :: Batch 0/1
Batch Loss = 3.4590142408119045
649, epoch_train_loss=3.4590142408119045
Epoch 650
Epoch 650 :: Batch 0/1
Batch Loss = 3.459006508195171
650, epoch_train_loss=3.459006508195171
Epoch 651
Epoch 651 :: Batch 0/1
Batch Loss = 3.45899882184825
651, epoch_train_loss=3.45899882184825
Epoch 652
Epoch 652 :: Batch 0/1
Batch Loss = 3.458991168731487
652, epoch_train_loss=3.458991168731487
Epoch 653
Epoch 653 :: Batch 0/1
Batch Loss = 3.4589835589925557
653, epoch_train_loss=3.4589835589925557
Epoch 654
Epoch 654 :: Batch 0/1
Batch Loss = 3.45897598070228
654, epoch_train_loss=3.45897598070228
Epoch 655
Epoch 655 :: Batch 0/1
Batch Loss = 3.4589684412135178
655, epoch_train_loss=3.4589684412135178
Epoch 656
Epoch 656 :: Batch 0/1
Batch Loss = 3.458960929786258
656, epoch_train_loss=3.458960929786258
Epoch 657
Epoch 657 :: Batch 0/1
Batch Loss = 3.458953452902279
657, epoch_train_loss=3.458953452902279
Epoch 658
Epoch 658 :: Batch 0/1
Batch Loss = 3.4589460015316837
658, epoch_train_loss=3.4589460015316837
Epoch 659
Epoch 659 :: Batch 0/1
Batch Loss = 3.4589385812813087
659, epoch_train_loss=3.4589385812813087
Epoch 660
Epoch 660 :: Batch 0/1
Batch Loss = 3.4589311837920547
660, epoch_train_loss=3.4589311837920547
Epoch 661
Epoch 661 :: Batch 0/1
Batch Loss = 3.458923813096866
661, epoch_train_loss=3.458923813096866
Epoch 662
Epoch 662 :: Batch 0/1
Batch Loss = 3.4589164618189545
662, epoch_train_loss=3.4589164618189545
Epoch 663
Epoch 663 :: Batch 0/1
Batch Loss = 3.4589091339322717
663, epoch_train_loss=3.4589091339322717
Epoch 664
Epoch 664 :: Batch 0/1
Batch Loss = 3.458901823236843
664, epoch_train_loss=3.458901823236843
Epoch 665
Epoch 665 :: Batch 0/1
Batch Loss = 3.4588945323106697
665, epoch_train_loss=3.4588945323106697
Epoch 666
Epoch 666 :: Batch 0/1
Batch Loss = 3.45888725525502
666, epoch_train_loss=3.45888725525502
Epoch 667
Epoch 667 :: Batch 0/1
Batch Loss = 3.4588799943745507
667, epoch_train_loss=3.4588799943745507
Epoch 668
Epoch 668 :: Batch 0/1
Batch Loss = 3.4588727450578824
668, epoch_train_loss=3.4588727450578824
Epoch 669
Epoch 669 :: Batch 0/1
Batch Loss = 3.458865508771708
669, epoch_train_loss=3.458865508771708
Epoch 670
Epoch 670 :: Batch 0/1
Batch Loss = 3.4588582811339226
670, epoch_train_loss=3.4588582811339226
Epoch 671
Epoch 671 :: Batch 0/1
Batch Loss = 3.45885106302934
671, epoch_train_loss=3.45885106302934
Epoch 672
Epoch 672 :: Batch 0/1
Batch Loss = 3.4588438509907005
672, epoch_train_loss=3.4588438509907005
Epoch 673
Epoch 673 :: Batch 0/1
Batch Loss = 3.458836645423058
673, epoch_train_loss=3.458836645423058
Epoch 674
Epoch 674 :: Batch 0/1
Batch Loss = 3.458829443351129
674, epoch_train_loss=3.458829443351129
Epoch 675
Epoch 675 :: Batch 0/1
Batch Loss = 3.45882224470562
675, epoch_train_loss=3.45882224470562
Epoch 676
Epoch 676 :: Batch 0/1
Batch Loss = 3.458815047004314
676, epoch_train_loss=3.458815047004314
Epoch 677
Epoch 677 :: Batch 0/1
Batch Loss = 3.458807849692625
677, epoch_train_loss=3.458807849692625
Epoch 678
Epoch 678 :: Batch 0/1
Batch Loss = 3.458800650776446
678, epoch_train_loss=3.458800650776446
Epoch 679
Epoch 679 :: Batch 0/1
Batch Loss = 3.458793449482566
679, epoch_train_loss=3.458793449482566
Epoch 680
Epoch 680 :: Batch 0/1
Batch Loss = 3.4587862441106667
680, epoch_train_loss=3.4587862441106667
Epoch 681
Epoch 681 :: Batch 0/1
Batch Loss = 3.4587790334535216
681, epoch_train_loss=3.4587790334535216
Epoch 682
Epoch 682 :: Batch 0/1
Batch Loss = 3.4587718161539365
682, epoch_train_loss=3.4587718161539365
Epoch 683
Epoch 683 :: Batch 0/1
Batch Loss = 3.4587645910030456
683, epoch_train_loss=3.4587645910030456
Epoch 684
Epoch 684 :: Batch 0/1
Batch Loss = 3.458757356855793
684, epoch_train_loss=3.458757356855793
Epoch 685
Epoch 685 :: Batch 0/1
Batch Loss = 3.4587501122165834
685, epoch_train_loss=3.4587501122165834
Epoch 686
Epoch 686 :: Batch 0/1
Batch Loss = 3.458742856061321
686, epoch_train_loss=3.458742856061321
Epoch 687
Epoch 687 :: Batch 0/1
Batch Loss = 3.4587355869355316
687, epoch_train_loss=3.4587355869355316
Epoch 688
Epoch 688 :: Batch 0/1
Batch Loss = 3.4587283039044436
688, epoch_train_loss=3.4587283039044436
Epoch 689
Epoch 689 :: Batch 0/1
Batch Loss = 3.458721005417097
689, epoch_train_loss=3.458721005417097
Epoch 690
Epoch 690 :: Batch 0/1
Batch Loss = 3.458713690561741
690, epoch_train_loss=3.458713690561741
Epoch 691
Epoch 691 :: Batch 0/1
Batch Loss = 3.4587063578573267
691, epoch_train_loss=3.4587063578573267
Epoch 692
Epoch 692 :: Batch 0/1
Batch Loss = 3.4586990063679623
692, epoch_train_loss=3.4586990063679623
Epoch 693
Epoch 693 :: Batch 0/1
Batch Loss = 3.45869163464596
693, epoch_train_loss=3.45869163464596
Epoch 694
Epoch 694 :: Batch 0/1
Batch Loss = 3.458684241750077
694, epoch_train_loss=3.458684241750077
Epoch 695
Epoch 695 :: Batch 0/1
Batch Loss = 3.458676826314625
695, epoch_train_loss=3.458676826314625
Epoch 696
Epoch 696 :: Batch 0/1
Batch Loss = 3.4586693872912204
696, epoch_train_loss=3.4586693872912204
Epoch 697
Epoch 697 :: Batch 0/1
Batch Loss = 3.458661923374091
697, epoch_train_loss=3.458661923374091
Epoch 698
Epoch 698 :: Batch 0/1
Batch Loss = 3.4586544335065064
698, epoch_train_loss=3.4586544335065064
Epoch 699
Epoch 699 :: Batch 0/1
Batch Loss = 3.4586469164571443
699, epoch_train_loss=3.4586469164571443
Epoch 700
Epoch 700 :: Batch 0/1
Batch Loss = 3.4586393710615035
700, epoch_train_loss=3.4586393710615035
Epoch 701
Epoch 701 :: Batch 0/1
Batch Loss = 3.458631796131444
701, epoch_train_loss=3.458631796131444
Epoch 702
Epoch 702 :: Batch 0/1
Batch Loss = 3.4586241905025497
702, epoch_train_loss=3.4586241905025497
Epoch 703
Epoch 703 :: Batch 0/1
Batch Loss = 3.4586165530292208
703, epoch_train_loss=3.4586165530292208
Epoch 704
Epoch 704 :: Batch 0/1
Batch Loss = 3.458608882480212
704, epoch_train_loss=3.458608882480212
Epoch 705
Epoch 705 :: Batch 0/1
Batch Loss = 3.4586011777241383
705, epoch_train_loss=3.4586011777241383
Epoch 706
Epoch 706 :: Batch 0/1
Batch Loss = 3.4585934375349425
706, epoch_train_loss=3.4585934375349425
Epoch 707
Epoch 707 :: Batch 0/1
Batch Loss = 3.4585856607702268
707, epoch_train_loss=3.4585856607702268
Epoch 708
Epoch 708 :: Batch 0/1
Batch Loss = 3.458577846174604
708, epoch_train_loss=3.458577846174604
Epoch 709
Epoch 709 :: Batch 0/1
Batch Loss = 3.4585699925990596
709, epoch_train_loss=3.4585699925990596
Epoch 710
Epoch 710 :: Batch 0/1
Batch Loss = 3.4585620987987493
710, epoch_train_loss=3.4585620987987493
Epoch 711
Epoch 711 :: Batch 0/1
Batch Loss = 3.4585541635815304
711, epoch_train_loss=3.4585541635815304
Epoch 712
Epoch 712 :: Batch 0/1
Batch Loss = 3.458546185694476
712, epoch_train_loss=3.458546185694476
Epoch 713
Epoch 713 :: Batch 0/1
Batch Loss = 3.4585381639344397
713, epoch_train_loss=3.4585381639344397
Epoch 714
Epoch 714 :: Batch 0/1
Batch Loss = 3.4585300970537207
714, epoch_train_loss=3.4585300970537207
Epoch 715
Epoch 715 :: Batch 0/1
Batch Loss = 3.458521983793156
715, epoch_train_loss=3.458521983793156
Epoch 716
Epoch 716 :: Batch 0/1
Batch Loss = 3.458513822893097
716, epoch_train_loss=3.458513822893097
Epoch 717
Epoch 717 :: Batch 0/1
Batch Loss = 3.458505613080316
717, epoch_train_loss=3.458505613080316
Epoch 718
Epoch 718 :: Batch 0/1
Batch Loss = 3.4584973530771594
718, epoch_train_loss=3.4584973530771594
Epoch 719
Epoch 719 :: Batch 0/1
Batch Loss = 3.4584890415624003
719, epoch_train_loss=3.4584890415624003
Epoch 720
Epoch 720 :: Batch 0/1
Batch Loss = 3.4584806772388315
720, epoch_train_loss=3.4584806772388315
Epoch 721
Epoch 721 :: Batch 0/1
Batch Loss = 3.458472258770884
721, epoch_train_loss=3.458472258770884
Epoch 722
Epoch 722 :: Batch 0/1
Batch Loss = 3.4584637848224484
722, epoch_train_loss=3.4584637848224484
Epoch 723
Epoch 723 :: Batch 0/1
Batch Loss = 3.4584552540214926
723, epoch_train_loss=3.4584552540214926
Epoch 724
Epoch 724 :: Batch 0/1
Batch Loss = 3.458446665006191
724, epoch_train_loss=3.458446665006191
Epoch 725
Epoch 725 :: Batch 0/1
Batch Loss = 3.4584380163799766
725, epoch_train_loss=3.4584380163799766
Epoch 726
Epoch 726 :: Batch 0/1
Batch Loss = 3.458429306727905
726, epoch_train_loss=3.458429306727905
Epoch 727
Epoch 727 :: Batch 0/1
Batch Loss = 3.4584205346174306
727, epoch_train_loss=3.4584205346174306
Epoch 728
Epoch 728 :: Batch 0/1
Batch Loss = 3.4584116986030153
728, epoch_train_loss=3.4584116986030153
Epoch 729
Epoch 729 :: Batch 0/1
Batch Loss = 3.458402797213583
729, epoch_train_loss=3.458402797213583
Epoch 730
Epoch 730 :: Batch 0/1
Batch Loss = 3.458393828947434
730, epoch_train_loss=3.458393828947434
Epoch 731
Epoch 731 :: Batch 0/1
Batch Loss = 3.4583847922959507
731, epoch_train_loss=3.4583847922959507
Epoch 732
Epoch 732 :: Batch 0/1
Batch Loss = 3.4583756857208523
732, epoch_train_loss=3.4583756857208523
Epoch 733
Epoch 733 :: Batch 0/1
Batch Loss = 3.4583665076589822
733, epoch_train_loss=3.4583665076589822
Epoch 734
Epoch 734 :: Batch 0/1
Batch Loss = 3.4583572565161336
734, epoch_train_loss=3.4583572565161336
Epoch 735
Epoch 735 :: Batch 0/1
Batch Loss = 3.4583479306846856
735, epoch_train_loss=3.4583479306846856
Epoch 736
Epoch 736 :: Batch 0/1
Batch Loss = 3.4583385285240262
736, epoch_train_loss=3.4583385285240262
Epoch 737
Epoch 737 :: Batch 0/1
Batch Loss = 3.4583290483599916
737, epoch_train_loss=3.4583290483599916
Epoch 738
Epoch 738 :: Batch 0/1
Batch Loss = 3.458319488494752
738, epoch_train_loss=3.458319488494752
Epoch 739
Epoch 739 :: Batch 0/1
Batch Loss = 3.458309847202152
739, epoch_train_loss=3.458309847202152
Epoch 740
Epoch 740 :: Batch 0/1
Batch Loss = 3.4583001227228736
740, epoch_train_loss=3.4583001227228736
Epoch 741
Epoch 741 :: Batch 0/1
Batch Loss = 3.458290313259006
741, epoch_train_loss=3.458290313259006
Epoch 742
Epoch 742 :: Batch 0/1
Batch Loss = 3.45828041698989
742, epoch_train_loss=3.45828041698989
Epoch 743
Epoch 743 :: Batch 0/1
Batch Loss = 3.4582704320561817
743, epoch_train_loss=3.4582704320561817
Epoch 744
Epoch 744 :: Batch 0/1
Batch Loss = 3.458260356559912
744, epoch_train_loss=3.458260356559912
Epoch 745
Epoch 745 :: Batch 0/1
Batch Loss = 3.4582501885659
745, epoch_train_loss=3.4582501885659
Epoch 746
Epoch 746 :: Batch 0/1
Batch Loss = 3.458239926106894
746, epoch_train_loss=3.458239926106894
Epoch 747
Epoch 747 :: Batch 0/1
Batch Loss = 3.4582295671723102
747, epoch_train_loss=3.4582295671723102
Epoch 748
Epoch 748 :: Batch 0/1
Batch Loss = 3.458219109706641
748, epoch_train_loss=3.458219109706641
Epoch 749
Epoch 749 :: Batch 0/1
Batch Loss = 3.4582085516172127
749, epoch_train_loss=3.4582085516172127
Epoch 750
Epoch 750 :: Batch 0/1
Batch Loss = 3.458197890769619
750, epoch_train_loss=3.458197890769619
Epoch 751
Epoch 751 :: Batch 0/1
Batch Loss = 3.4581871249789105
751, epoch_train_loss=3.4581871249789105
Epoch 752
Epoch 752 :: Batch 0/1
Batch Loss = 3.4581762520129264
752, epoch_train_loss=3.4581762520129264
Epoch 753
Epoch 753 :: Batch 0/1
Batch Loss = 3.4581652695969716
753, epoch_train_loss=3.4581652695969716
Epoch 754
Epoch 754 :: Batch 0/1
Batch Loss = 3.4581541754042817
754, epoch_train_loss=3.4581541754042817
Epoch 755
Epoch 755 :: Batch 0/1
Batch Loss = 3.4581429670528365
755, epoch_train_loss=3.4581429670528365
Epoch 756
Epoch 756 :: Batch 0/1
Batch Loss = 3.4581316421102057
756, epoch_train_loss=3.4581316421102057
Epoch 757
Epoch 757 :: Batch 0/1
Batch Loss = 3.458120198091607
757, epoch_train_loss=3.458120198091607
Epoch 758
Epoch 758 :: Batch 0/1
Batch Loss = 3.4581086324519044
758, epoch_train_loss=3.4581086324519044
Epoch 759
Epoch 759 :: Batch 0/1
Batch Loss = 3.458096942585491
759, epoch_train_loss=3.458096942585491
Epoch 760
Epoch 760 :: Batch 0/1
Batch Loss = 3.4580851258298213
760, epoch_train_loss=3.4580851258298213
Epoch 761
Epoch 761 :: Batch 0/1
Batch Loss = 3.4580731794606767
761, epoch_train_loss=3.4580731794606767
Epoch 762
Epoch 762 :: Batch 0/1
Batch Loss = 3.458061100684544
762, epoch_train_loss=3.458061100684544
Epoch 763
Epoch 763 :: Batch 0/1
Batch Loss = 3.4580488866425894
763, epoch_train_loss=3.4580488866425894
Epoch 764
Epoch 764 :: Batch 0/1
Batch Loss = 3.45803653440982
764, epoch_train_loss=3.45803653440982
Epoch 765
Epoch 765 :: Batch 0/1
Batch Loss = 3.4580240409868943
765, epoch_train_loss=3.4580240409868943
Epoch 766
Epoch 766 :: Batch 0/1
Batch Loss = 3.458011403298753
766, epoch_train_loss=3.458011403298753
Epoch 767
Epoch 767 :: Batch 0/1
Batch Loss = 3.4579986181967586
767, epoch_train_loss=3.4579986181967586
Epoch 768
Epoch 768 :: Batch 0/1
Batch Loss = 3.4579856824553747
768, epoch_train_loss=3.4579856824553747
Epoch 769
Epoch 769 :: Batch 0/1
Batch Loss = 3.4579725927633014
769, epoch_train_loss=3.4579725927633014
Epoch 770
Epoch 770 :: Batch 0/1
Batch Loss = 3.4579593457250883
770, epoch_train_loss=3.4579593457250883
Epoch 771
Epoch 771 :: Batch 0/1
Batch Loss = 3.457945937863123
771, epoch_train_loss=3.457945937863123
Epoch 772
Epoch 772 :: Batch 0/1
Batch Loss = 3.457932365605432
772, epoch_train_loss=3.457932365605432
Epoch 773
Epoch 773 :: Batch 0/1
Batch Loss = 3.4579186252873613
773, epoch_train_loss=3.4579186252873613
Epoch 774
Epoch 774 :: Batch 0/1
Batch Loss = 3.4579047131491794
774, epoch_train_loss=3.4579047131491794
Epoch 775
Epoch 775 :: Batch 0/1
Batch Loss = 3.457890625334293
775, epoch_train_loss=3.457890625334293
Epoch 776
Epoch 776 :: Batch 0/1
Batch Loss = 3.457876357879522
776, epoch_train_loss=3.457876357879522
Epoch 777
Epoch 777 :: Batch 0/1
Batch Loss = 3.457861906715449
777, epoch_train_loss=3.457861906715449
Epoch 778
Epoch 778 :: Batch 0/1
Batch Loss = 3.4578472676666463
778, epoch_train_loss=3.4578472676666463
Epoch 779
Epoch 779 :: Batch 0/1
Batch Loss = 3.45783243644351
779, epoch_train_loss=3.45783243644351
Epoch 780
Epoch 780 :: Batch 0/1
Batch Loss = 3.4578174086355453
780, epoch_train_loss=3.4578174086355453
Epoch 781
Epoch 781 :: Batch 0/1
Batch Loss = 3.457802179714993
781, epoch_train_loss=3.457802179714993
Epoch 782
Epoch 782 :: Batch 0/1
Batch Loss = 3.4577867450303637
782, epoch_train_loss=3.4577867450303637
Epoch 783
Epoch 783 :: Batch 0/1
Batch Loss = 3.4577710997959197
783, epoch_train_loss=3.4577710997959197
Epoch 784
Epoch 784 :: Batch 0/1
Batch Loss = 3.4577552390956283
784, epoch_train_loss=3.4577552390956283
Epoch 785
Epoch 785 :: Batch 0/1
Batch Loss = 3.4577391578749586
785, epoch_train_loss=3.4577391578749586
Epoch 786
Epoch 786 :: Batch 0/1
Batch Loss = 3.457722850935856
786, epoch_train_loss=3.457722850935856
Epoch 787
Epoch 787 :: Batch 0/1
Batch Loss = 3.4577063129315184
787, epoch_train_loss=3.4577063129315184
Epoch 788
Epoch 788 :: Batch 0/1
Batch Loss = 3.457689538366152
788, epoch_train_loss=3.457689538366152
Epoch 789
Epoch 789 :: Batch 0/1
Batch Loss = 3.4576725215834605
789, epoch_train_loss=3.4576725215834605
Epoch 790
Epoch 790 :: Batch 0/1
Batch Loss = 3.457655256763061
790, epoch_train_loss=3.457655256763061
Epoch 791
Epoch 791 :: Batch 0/1
Batch Loss = 3.4576377379170666
791, epoch_train_loss=3.4576377379170666
Epoch 792
Epoch 792 :: Batch 0/1
Batch Loss = 3.4576199588847927
792, epoch_train_loss=3.4576199588847927
Epoch 793
Epoch 793 :: Batch 0/1
Batch Loss = 3.4576019133234497
793, epoch_train_loss=3.4576019133234497
Epoch 794
Epoch 794 :: Batch 0/1
Batch Loss = 3.4575835947033973
794, epoch_train_loss=3.4575835947033973
Epoch 795
Epoch 795 :: Batch 0/1
Batch Loss = 3.457564996304483
795, epoch_train_loss=3.457564996304483
Epoch 796
Epoch 796 :: Batch 0/1
Batch Loss = 3.4575461112068697
796, epoch_train_loss=3.4575461112068697
Epoch 797
Epoch 797 :: Batch 0/1
Batch Loss = 3.4575269322831153
797, epoch_train_loss=3.4575269322831153
Epoch 798
Epoch 798 :: Batch 0/1
Batch Loss = 3.457507452194321
798, epoch_train_loss=3.457507452194321
Epoch 799
Epoch 799 :: Batch 0/1
Batch Loss = 3.4574876633824343
799, epoch_train_loss=3.4574876633824343
Epoch 800
Epoch 800 :: Batch 0/1
Batch Loss = 3.4574675580600753
800, epoch_train_loss=3.4574675580600753
Epoch 801
Epoch 801 :: Batch 0/1
Batch Loss = 3.457447128204776
801, epoch_train_loss=3.457447128204776
Epoch 802
Epoch 802 :: Batch 0/1
Batch Loss = 3.4574263655526885
802, epoch_train_loss=3.4574263655526885
Epoch 803
Epoch 803 :: Batch 0/1
Batch Loss = 3.457405261586046
803, epoch_train_loss=3.457405261586046
Epoch 804
Epoch 804 :: Batch 0/1
Batch Loss = 3.457383807531107
804, epoch_train_loss=3.457383807531107
Epoch 805
Epoch 805 :: Batch 0/1
Batch Loss = 3.457361994340617
805, epoch_train_loss=3.457361994340617
Epoch 806
Epoch 806 :: Batch 0/1
Batch Loss = 3.4573398126948764
806, epoch_train_loss=3.4573398126948764
Epoch 807
Epoch 807 :: Batch 0/1
Batch Loss = 3.4573172529840055
807, epoch_train_loss=3.4573172529840055
Epoch 808
Epoch 808 :: Batch 0/1
Batch Loss = 3.45729430530047
808, epoch_train_loss=3.45729430530047
Epoch 809
Epoch 809 :: Batch 0/1
Batch Loss = 3.457270959434816
809, epoch_train_loss=3.457270959434816
Epoch 810
Epoch 810 :: Batch 0/1
Batch Loss = 3.4572472048577128
810, epoch_train_loss=3.4572472048577128
Epoch 811
Epoch 811 :: Batch 0/1
Batch Loss = 3.457223030710219
811, epoch_train_loss=3.457223030710219
Epoch 812
Epoch 812 :: Batch 0/1
Batch Loss = 3.457198425800168
812, epoch_train_loss=3.457198425800168
Epoch 813
Epoch 813 :: Batch 0/1
Batch Loss = 3.457173378583366
813, epoch_train_loss=3.457173378583366
Epoch 814
Epoch 814 :: Batch 0/1
Batch Loss = 3.4571478771512383
814, epoch_train_loss=3.4571478771512383
Epoch 815
Epoch 815 :: Batch 0/1
Batch Loss = 3.4571219092248624
815, epoch_train_loss=3.4571219092248624
Epoch 816
Epoch 816 :: Batch 0/1
Batch Loss = 3.4570954621439274
816, epoch_train_loss=3.4570954621439274
Epoch 817
Epoch 817 :: Batch 0/1
Batch Loss = 3.457068522840135
817, epoch_train_loss=3.457068522840135
Epoch 818
Epoch 818 :: Batch 0/1
Batch Loss = 3.4570410778378338
818, epoch_train_loss=3.4570410778378338
Epoch 819
Epoch 819 :: Batch 0/1
Batch Loss = 3.457013113239088
819, epoch_train_loss=3.457013113239088
Epoch 820
Epoch 820 :: Batch 0/1
Batch Loss = 3.4569846147012675
820, epoch_train_loss=3.4569846147012675
Epoch 821
Epoch 821 :: Batch 0/1
Batch Loss = 3.456955567432018
821, epoch_train_loss=3.456955567432018
Epoch 822
Epoch 822 :: Batch 0/1
Batch Loss = 3.456925956167046
822, epoch_train_loss=3.456925956167046
Epoch 823
Epoch 823 :: Batch 0/1
Batch Loss = 3.4568957651597723
823, epoch_train_loss=3.4568957651597723
Epoch 824
Epoch 824 :: Batch 0/1
Batch Loss = 3.4568649781654583
824, epoch_train_loss=3.4568649781654583
Epoch 825
Epoch 825 :: Batch 0/1
Batch Loss = 3.4568335784196687
825, epoch_train_loss=3.4568335784196687
Epoch 826
Epoch 826 :: Batch 0/1
Batch Loss = 3.4568015486277153
826, epoch_train_loss=3.4568015486277153
Epoch 827
Epoch 827 :: Batch 0/1
Batch Loss = 3.456768870944053
827, epoch_train_loss=3.456768870944053
Epoch 828
Epoch 828 :: Batch 0/1
Batch Loss = 3.4567355269587607
828, epoch_train_loss=3.4567355269587607
Epoch 829
Epoch 829 :: Batch 0/1
Batch Loss = 3.4567014976717556
829, epoch_train_loss=3.4567014976717556
Epoch 830
Epoch 830 :: Batch 0/1
Batch Loss = 3.456666763480203
830, epoch_train_loss=3.456666763480203
Epoch 831
Epoch 831 :: Batch 0/1
Batch Loss = 3.456631304157798
831, epoch_train_loss=3.456631304157798
Epoch 832
Epoch 832 :: Batch 0/1
Batch Loss = 3.456595098835756
832, epoch_train_loss=3.456595098835756
Epoch 833
Epoch 833 :: Batch 0/1
Batch Loss = 3.456558125983869
833, epoch_train_loss=3.456558125983869
Epoch 834
Epoch 834 :: Batch 0/1
Batch Loss = 3.4565203633784667
834, epoch_train_loss=3.4565203633784667
Epoch 835
Epoch 835 :: Batch 0/1
Batch Loss = 3.4564817881021144
835, epoch_train_loss=3.4564817881021144
Epoch 836
Epoch 836 :: Batch 0/1
Batch Loss = 3.4564423764970105
836, epoch_train_loss=3.4564423764970105
Epoch 837
Epoch 837 :: Batch 0/1
Batch Loss = 3.4564021041655804
837, epoch_train_loss=3.4564021041655804
Epoch 838
Epoch 838 :: Batch 0/1
Batch Loss = 3.456360945923863
838, epoch_train_loss=3.456360945923863
Epoch 839
Epoch 839 :: Batch 0/1
Batch Loss = 3.456318875801256
839, epoch_train_loss=3.456318875801256
Epoch 840
Epoch 840 :: Batch 0/1
Batch Loss = 3.456275866989111
840, epoch_train_loss=3.456275866989111
Epoch 841
Epoch 841 :: Batch 0/1
Batch Loss = 3.456231891841608
841, epoch_train_loss=3.456231891841608
Epoch 842
Epoch 842 :: Batch 0/1
Batch Loss = 3.4561869218217147
842, epoch_train_loss=3.4561869218217147
Epoch 843
Epoch 843 :: Batch 0/1
Batch Loss = 3.456140927495279
843, epoch_train_loss=3.456140927495279
Epoch 844
Epoch 844 :: Batch 0/1
Batch Loss = 3.4560938784886153
844, epoch_train_loss=3.4560938784886153
Epoch 845
Epoch 845 :: Batch 0/1
Batch Loss = 3.4560457434607423
845, epoch_train_loss=3.4560457434607423
Epoch 846
Epoch 846 :: Batch 0/1
Batch Loss = 3.4559964900693294
846, epoch_train_loss=3.4559964900693294
Epoch 847
Epoch 847 :: Batch 0/1
Batch Loss = 3.455946084936368
847, epoch_train_loss=3.455946084936368
Epoch 848
Epoch 848 :: Batch 0/1
Batch Loss = 3.4558944936114204
848, epoch_train_loss=3.4558944936114204
Epoch 849
Epoch 849 :: Batch 0/1
Batch Loss = 3.455841680526775
849, epoch_train_loss=3.455841680526775
Epoch 850
Epoch 850 :: Batch 0/1
Batch Loss = 3.45578760896725
850, epoch_train_loss=3.45578760896725
Epoch 851
Epoch 851 :: Batch 0/1
Batch Loss = 3.4557322410084024
851, epoch_train_loss=3.4557322410084024
Epoch 852
Epoch 852 :: Batch 0/1
Batch Loss = 3.4556755374823744
852, epoch_train_loss=3.4556755374823744
Epoch 853
Epoch 853 :: Batch 0/1
Batch Loss = 3.45561745791107
853, epoch_train_loss=3.45561745791107
Epoch 854
Epoch 854 :: Batch 0/1
Batch Loss = 3.455557960446873
854, epoch_train_loss=3.455557960446873
Epoch 855
Epoch 855 :: Batch 0/1
Batch Loss = 3.4554970018203126
855, epoch_train_loss=3.4554970018203126
Epoch 856
Epoch 856 :: Batch 0/1
Batch Loss = 3.4554345372498854
856, epoch_train_loss=3.4554345372498854
Epoch 857
Epoch 857 :: Batch 0/1
Batch Loss = 3.4553705203822376
857, epoch_train_loss=3.4553705203822376
Epoch 858
Epoch 858 :: Batch 0/1
Batch Loss = 3.455304903189631
858, epoch_train_loss=3.455304903189631
Epoch 859
Epoch 859 :: Batch 0/1
Batch Loss = 3.4552376358721637
859, epoch_train_loss=3.4552376358721637
Epoch 860
Epoch 860 :: Batch 0/1
Batch Loss = 3.4551686667660997
860, epoch_train_loss=3.4551686667660997
Epoch 861
Epoch 861 :: Batch 0/1
Batch Loss = 3.455097942202991
861, epoch_train_loss=3.455097942202991
Epoch 862
Epoch 862 :: Batch 0/1
Batch Loss = 3.455025406392613
862, epoch_train_loss=3.455025406392613
Epoch 863
Epoch 863 :: Batch 0/1
Batch Loss = 3.4549510012770908
863, epoch_train_loss=3.4549510012770908
Epoch 864
Epoch 864 :: Batch 0/1
Batch Loss = 3.454874666350364
864, epoch_train_loss=3.454874666350364
Epoch 865
Epoch 865 :: Batch 0/1
Batch Loss = 3.4547963385008336
865, epoch_train_loss=3.4547963385008336
Epoch 866
Epoch 866 :: Batch 0/1
Batch Loss = 3.454715951789208
866, epoch_train_loss=3.454715951789208
Epoch 867
Epoch 867 :: Batch 0/1
Batch Loss = 3.454633437246271
867, epoch_train_loss=3.454633437246271
Epoch 868
Epoch 868 :: Batch 0/1
Batch Loss = 3.454548722625519
868, epoch_train_loss=3.454548722625519
Epoch 869
Epoch 869 :: Batch 0/1
Batch Loss = 3.4544617321452735
869, epoch_train_loss=3.4544617321452735
Epoch 870
Epoch 870 :: Batch 0/1
Batch Loss = 3.454372386177944
870, epoch_train_loss=3.454372386177944
Epoch 871
Epoch 871 :: Batch 0/1
Batch Loss = 3.4542806009527705
871, epoch_train_loss=3.4542806009527705
Epoch 872
Epoch 872 :: Batch 0/1
Batch Loss = 3.4541862882041636
872, epoch_train_loss=3.4541862882041636
Epoch 873
Epoch 873 :: Batch 0/1
Batch Loss = 3.454089354803187
873, epoch_train_loss=3.454089354803187
Epoch 874
Epoch 874 :: Batch 0/1
Batch Loss = 3.453989702320374
874, epoch_train_loss=3.453989702320374
Epoch 875
Epoch 875 :: Batch 0/1
Batch Loss = 3.4538872266378675
875, epoch_train_loss=3.4538872266378675
Epoch 876
Epoch 876 :: Batch 0/1
Batch Loss = 3.4537818174193404
876, epoch_train_loss=3.4537818174193404
Epoch 877
Epoch 877 :: Batch 0/1
Batch Loss = 3.4536733576272436
877, epoch_train_loss=3.4536733576272436
Epoch 878
Epoch 878 :: Batch 0/1
Batch Loss = 3.453561722961865
878, epoch_train_loss=3.453561722961865
Epoch 879
Epoch 879 :: Batch 0/1
Batch Loss = 3.4534467812655354
879, epoch_train_loss=3.4534467812655354
Epoch 880
Epoch 880 :: Batch 0/1
Batch Loss = 3.453328391882525
880, epoch_train_loss=3.453328391882525
Epoch 881
Epoch 881 :: Batch 0/1
Batch Loss = 3.4532064049701443
881, epoch_train_loss=3.4532064049701443
Epoch 882
Epoch 882 :: Batch 0/1
Batch Loss = 3.453080660757686
882, epoch_train_loss=3.453080660757686
Epoch 883
Epoch 883 :: Batch 0/1
Batch Loss = 3.452950988749255
883, epoch_train_loss=3.452950988749255
Epoch 884
Epoch 884 :: Batch 0/1
Batch Loss = 3.4528172068641796
884, epoch_train_loss=3.4528172068641796
Epoch 885
Epoch 885 :: Batch 0/1
Batch Loss = 3.452679120509979
885, epoch_train_loss=3.452679120509979
Epoch 886
Epoch 886 :: Batch 0/1
Batch Loss = 3.452536521569718
886, epoch_train_loss=3.452536521569718
Epoch 887
Epoch 887 :: Batch 0/1
Batch Loss = 3.452389187350206
887, epoch_train_loss=3.452389187350206
Epoch 888
Epoch 888 :: Batch 0/1
Batch Loss = 3.4522368793745986
888, epoch_train_loss=3.4522368793745986
Epoch 889
Epoch 889 :: Batch 0/1
Batch Loss = 3.452079342116454
889, epoch_train_loss=3.452079342116454
Epoch 890
Epoch 890 :: Batch 0/1
Batch Loss = 3.4519163016113317
890, epoch_train_loss=3.4519163016113317
Epoch 891
Epoch 891 :: Batch 0/1
Batch Loss = 3.4517474639321786
891, epoch_train_loss=3.4517474639321786
Epoch 892
Epoch 892 :: Batch 0/1
Batch Loss = 3.451572513580016
892, epoch_train_loss=3.451572513580016
Epoch 893
Epoch 893 :: Batch 0/1
Batch Loss = 3.451391111648784
893, epoch_train_loss=3.451391111648784
Epoch 894
Epoch 894 :: Batch 0/1
Batch Loss = 3.451202893862723
894, epoch_train_loss=3.451202893862723
Epoch 895
Epoch 895 :: Batch 0/1
Batch Loss = 3.451007468481092
895, epoch_train_loss=3.451007468481092
Epoch 896
Epoch 896 :: Batch 0/1
Batch Loss = 3.450804413925709
896, epoch_train_loss=3.450804413925709
Epoch 897
Epoch 897 :: Batch 0/1
Batch Loss = 3.4505932762413822
897, epoch_train_loss=3.4505932762413822
Epoch 898
Epoch 898 :: Batch 0/1
Batch Loss = 3.450373566370296
898, epoch_train_loss=3.450373566370296
Epoch 899
Epoch 899 :: Batch 0/1
Batch Loss = 3.450144757165542
899, epoch_train_loss=3.450144757165542
Epoch 900
Epoch 900 :: Batch 0/1
Batch Loss = 3.449906280124486
900, epoch_train_loss=3.449906280124486
Epoch 901
Epoch 901 :: Batch 0/1
Batch Loss = 3.4496575219021772
901, epoch_train_loss=3.4496575219021772
Epoch 902
Epoch 902 :: Batch 0/1
Batch Loss = 3.4493978206297315
902, epoch_train_loss=3.4493978206297315
Epoch 903
Epoch 903 :: Batch 0/1
Batch Loss = 3.449126461845891
903, epoch_train_loss=3.449126461845891
Epoch 904
Epoch 904 :: Batch 0/1
Batch Loss = 3.448842674341502
904, epoch_train_loss=3.448842674341502
Epoch 905
Epoch 905 :: Batch 0/1
Batch Loss = 3.4485456256137494
905, epoch_train_loss=3.4485456256137494
Epoch 906
Epoch 906 :: Batch 0/1
Batch Loss = 3.4482344172736332
906, epoch_train_loss=3.4482344172736332
Epoch 907
Epoch 907 :: Batch 0/1
Batch Loss = 3.447908080221548
907, epoch_train_loss=3.447908080221548
Epoch 908
Epoch 908 :: Batch 0/1
Batch Loss = 3.447565569791695
908, epoch_train_loss=3.447565569791695
Epoch 909
Epoch 909 :: Batch 0/1
Batch Loss = 3.4472057608055744
909, epoch_train_loss=3.4472057608055744
Epoch 910
Epoch 910 :: Batch 0/1
Batch Loss = 3.446827442920427
910, epoch_train_loss=3.446827442920427
Epoch 911
Epoch 911 :: Batch 0/1
Batch Loss = 3.4464293162114945
911, epoch_train_loss=3.4464293162114945
Epoch 912
Epoch 912 :: Batch 0/1
Batch Loss = 3.4460099871704184
912, epoch_train_loss=3.4460099871704184
Epoch 913
Epoch 913 :: Batch 0/1
Batch Loss = 3.445567965659551
913, epoch_train_loss=3.445567965659551
Epoch 914
Epoch 914 :: Batch 0/1
Batch Loss = 3.445101662882487
914, epoch_train_loss=3.445101662882487
Epoch 915
Epoch 915 :: Batch 0/1
Batch Loss = 3.444609390724393
915, epoch_train_loss=3.444609390724393
Epoch 916
Epoch 916 :: Batch 0/1
Batch Loss = 3.4440893632354608
916, epoch_train_loss=3.4440893632354608
Epoch 917
Epoch 917 :: Batch 0/1
Batch Loss = 3.4435397004211654
917, epoch_train_loss=3.4435397004211654
Epoch 918
Epoch 918 :: Batch 0/1
Batch Loss = 3.4429584350637628
918, epoch_train_loss=3.4429584350637628
Epoch 919
Epoch 919 :: Batch 0/1
Batch Loss = 3.4423435231140784
919, epoch_train_loss=3.4423435231140784
Epoch 920
Epoch 920 :: Batch 0/1
Batch Loss = 3.441692858195019
920, epoch_train_loss=3.441692858195019
Epoch 921
Epoch 921 :: Batch 0/1
Batch Loss = 3.4410042906635097
921, epoch_train_loss=3.4410042906635097
Epoch 922
Epoch 922 :: Batch 0/1
Batch Loss = 3.4402756514843356
922, epoch_train_loss=3.4402756514843356
Epoch 923
Epoch 923 :: Batch 0/1
Batch Loss = 3.439504780860464
923, epoch_train_loss=3.439504780860464
Epoch 924
Epoch 924 :: Batch 0/1
Batch Loss = 3.438689561108535
924, epoch_train_loss=3.438689561108535
Epoch 925
Epoch 925 :: Batch 0/1
Batch Loss = 3.4378279526674866
925, epoch_train_loss=3.4378279526674866
Epoch 926
Epoch 926 :: Batch 0/1
Batch Loss = 3.4369180313847587
926, epoch_train_loss=3.4369180313847587
Epoch 927
Epoch 927 :: Batch 0/1
Batch Loss = 3.435958024410224
927, epoch_train_loss=3.435958024410224
Epoch 928
Epoch 928 :: Batch 0/1
Batch Loss = 3.4349463412161905
928, epoch_train_loss=3.4349463412161905
Epoch 929
Epoch 929 :: Batch 0/1
Batch Loss = 3.433881595572545
929, epoch_train_loss=3.433881595572545
Epoch 930
Epoch 930 :: Batch 0/1
Batch Loss = 3.4327626145126686
930, epoch_train_loss=3.4327626145126686
Epoch 931
Epoch 931 :: Batch 0/1
Batch Loss = 3.431588429501083
931, epoch_train_loss=3.431588429501083
Epoch 932
Epoch 932 :: Batch 0/1
Batch Loss = 3.430358247427604
932, epoch_train_loss=3.430358247427604
Epoch 933
Epoch 933 :: Batch 0/1
Batch Loss = 3.4290714005020257
933, epoch_train_loss=3.4290714005020257
Epoch 934
Epoch 934 :: Batch 0/1
Batch Loss = 3.427727274724169
934, epoch_train_loss=3.427727274724169
Epoch 935
Epoch 935 :: Batch 0/1
Batch Loss = 3.426325222109903
935, epoch_train_loss=3.426325222109903
Epoch 936
Epoch 936 :: Batch 0/1
Batch Loss = 3.4248644582689014
936, epoch_train_loss=3.4248644582689014
Epoch 937
Epoch 937 :: Batch 0/1
Batch Loss = 3.4233439566987216
937, epoch_train_loss=3.4233439566987216
Epoch 938
Epoch 938 :: Batch 0/1
Batch Loss = 3.4217623196980296
938, epoch_train_loss=3.4217623196980296
Epoch 939
Epoch 939 :: Batch 0/1
Batch Loss = 3.420117692011528
939, epoch_train_loss=3.420117692011528
Epoch 940
Epoch 940 :: Batch 0/1
Batch Loss = 3.4184075019049796
940, epoch_train_loss=3.4184075019049796
Epoch 941
Epoch 941 :: Batch 0/1
Batch Loss = 3.4166288281177186
941, epoch_train_loss=3.4166288281177186
Epoch 942
Epoch 942 :: Batch 0/1
Batch Loss = 3.414778364391753
942, epoch_train_loss=3.414778364391753
Epoch 943
Epoch 943 :: Batch 0/1
Batch Loss = 3.412886391982353
943, epoch_train_loss=3.412886391982353
Epoch 944
Epoch 944 :: Batch 0/1
Batch Loss = 3.411513108536133
944, epoch_train_loss=3.411513108536133
Epoch 945
Epoch 945 :: Batch 0/1
Batch Loss = 3.4213741202870422
945, epoch_train_loss=3.4213741202870422
Epoch 946
Epoch 946 :: Batch 0/1
Batch Loss = 3.481764041781436
946, epoch_train_loss=3.481764041781436
Epoch 947
Epoch 947 :: Batch 0/1
Batch Loss = 3.6063655804872847
947, epoch_train_loss=3.6063655804872847
Epoch 948
Epoch 948 :: Batch 0/1
Batch Loss = 3.697407855832237
948, epoch_train_loss=3.697407855832237
Epoch 949
Epoch 949 :: Batch 0/1
Batch Loss = 3.4314631722922098
949, epoch_train_loss=3.4314631722922098
Epoch 950
Epoch 950 :: Batch 0/1
Batch Loss = 3.601986455087197
950, epoch_train_loss=3.601986455087197
Epoch 951
Epoch 951 :: Batch 0/1
Batch Loss = 3.5020913290989175
951, epoch_train_loss=3.5020913290989175
Epoch 952
Epoch 952 :: Batch 0/1
Batch Loss = 3.5307494607830265
952, epoch_train_loss=3.5307494607830265
Epoch 953
Epoch 953 :: Batch 0/1
Batch Loss = 3.4669511227312357
953, epoch_train_loss=3.4669511227312357
Epoch 954
Epoch 954 :: Batch 0/1
Batch Loss = 3.5129587533366045
954, epoch_train_loss=3.5129587533366045
Epoch 955
Epoch 955 :: Batch 0/1
Batch Loss = 3.4366236805594927
955, epoch_train_loss=3.4366236805594927
Epoch 956
Epoch 956 :: Batch 0/1
Batch Loss = 3.4981511543390655
956, epoch_train_loss=3.4981511543390655
Epoch 957
Epoch 957 :: Batch 0/1
Batch Loss = 3.429133753528415
957, epoch_train_loss=3.429133753528415
Epoch 958
Epoch 958 :: Batch 0/1
Batch Loss = 3.4651780924373456
958, epoch_train_loss=3.4651780924373456
Epoch 959
Epoch 959 :: Batch 0/1
Batch Loss = 3.440707003750926
959, epoch_train_loss=3.440707003750926
Epoch 960
Epoch 960 :: Batch 0/1
Batch Loss = 3.424362488607202
960, epoch_train_loss=3.424362488607202
Epoch 961
Epoch 961 :: Batch 0/1
Batch Loss = 3.4483154928273794
961, epoch_train_loss=3.4483154928273794
Epoch 962
Epoch 962 :: Batch 0/1
Batch Loss = 3.409654964609612
962, epoch_train_loss=3.409654964609612
Epoch 963
Epoch 963 :: Batch 0/1
Batch Loss = 3.42886332909592
963, epoch_train_loss=3.42886332909592
Epoch 964
Epoch 964 :: Batch 0/1
Batch Loss = 3.415390398087023
964, epoch_train_loss=3.415390398087023
Epoch 965
Epoch 965 :: Batch 0/1
Batch Loss = 3.402621288867108
965, epoch_train_loss=3.402621288867108
Epoch 966
Epoch 966 :: Batch 0/1
Batch Loss = 3.4167548698337886
966, epoch_train_loss=3.4167548698337886
Epoch 967
Epoch 967 :: Batch 0/1
Batch Loss = 3.396821065289483
967, epoch_train_loss=3.396821065289483
Epoch 968
Epoch 968 :: Batch 0/1
Batch Loss = 3.3913427589807745
968, epoch_train_loss=3.3913427589807745
Epoch 969
Epoch 969 :: Batch 0/1
Batch Loss = 3.392787425049588
969, epoch_train_loss=3.392787425049588
Epoch 970
Epoch 970 :: Batch 0/1
Batch Loss = 3.3752225665227247
970, epoch_train_loss=3.3752225665227247
Epoch 971
Epoch 971 :: Batch 0/1
Batch Loss = 3.3824636785217868
971, epoch_train_loss=3.3824636785217868
Epoch 972
Epoch 972 :: Batch 0/1
Batch Loss = 3.3703059749423856
972, epoch_train_loss=3.3703059749423856
Epoch 973
Epoch 973 :: Batch 0/1
Batch Loss = 3.3596804307801293
973, epoch_train_loss=3.3596804307801293
Epoch 974
Epoch 974 :: Batch 0/1
Batch Loss = 3.358699391898243
974, epoch_train_loss=3.358699391898243
Epoch 975
Epoch 975 :: Batch 0/1
Batch Loss = 3.346435524624123
975, epoch_train_loss=3.346435524624123
Epoch 976
Epoch 976 :: Batch 0/1
Batch Loss = 3.345376561565116
976, epoch_train_loss=3.345376561565116
Epoch 977
Epoch 977 :: Batch 0/1
Batch Loss = 3.333348944445203
977, epoch_train_loss=3.333348944445203
Epoch 978
Epoch 978 :: Batch 0/1
Batch Loss = 3.3274415723051862
978, epoch_train_loss=3.3274415723051862
Epoch 979
Epoch 979 :: Batch 0/1
Batch Loss = 3.3206339021326823
979, epoch_train_loss=3.3206339021326823
Epoch 980
Epoch 980 :: Batch 0/1
Batch Loss = 3.3089326005109383
980, epoch_train_loss=3.3089326005109383
Epoch 981
Epoch 981 :: Batch 0/1
Batch Loss = 3.306265356601297
981, epoch_train_loss=3.306265356601297
Epoch 982
Epoch 982 :: Batch 0/1
Batch Loss = 3.2941778905957366
982, epoch_train_loss=3.2941778905957366
Epoch 983
Epoch 983 :: Batch 0/1
Batch Loss = 3.2897519468879026
983, epoch_train_loss=3.2897519468879026
Epoch 984
Epoch 984 :: Batch 0/1
Batch Loss = 3.2783572375817354
984, epoch_train_loss=3.2783572375817354
Epoch 985
Epoch 985 :: Batch 0/1
Batch Loss = 3.274726303907039
985, epoch_train_loss=3.274726303907039
Epoch 986
Epoch 986 :: Batch 0/1
Batch Loss = 3.263902031558067
986, epoch_train_loss=3.263902031558067
Epoch 987
Epoch 987 :: Batch 0/1
Batch Loss = 3.2601225651070878
987, epoch_train_loss=3.2601225651070878
Epoch 988
Epoch 988 :: Batch 0/1
Batch Loss = 3.2513740655371666
988, epoch_train_loss=3.2513740655371666
Epoch 989
Epoch 989 :: Batch 0/1
Batch Loss = 3.245065517988256
989, epoch_train_loss=3.245065517988256
Epoch 990
Epoch 990 :: Batch 0/1
Batch Loss = 3.2409201035345414
990, epoch_train_loss=3.2409201035345414
Epoch 991
Epoch 991 :: Batch 0/1
Batch Loss = 3.233823911287612
991, epoch_train_loss=3.233823911287612
Epoch 992
Epoch 992 :: Batch 0/1
Batch Loss = 3.227874221452866
992, epoch_train_loss=3.227874221452866
Epoch 993
Epoch 993 :: Batch 0/1
Batch Loss = 3.2270710836505763
993, epoch_train_loss=3.2270710836505763
Epoch 994
Epoch 994 :: Batch 0/1
Batch Loss = 3.22126322262008
994, epoch_train_loss=3.22126322262008
Epoch 995
Epoch 995 :: Batch 0/1
Batch Loss = 3.215338652775539
995, epoch_train_loss=3.215338652775539
Epoch 996
Epoch 996 :: Batch 0/1
Batch Loss = 3.21316405903403
996, epoch_train_loss=3.21316405903403
Epoch 997
Epoch 997 :: Batch 0/1
Batch Loss = 3.212149647321792
997, epoch_train_loss=3.212149647321792
Epoch 998
Epoch 998 :: Batch 0/1
Batch Loss = 3.2129210660485668
998, epoch_train_loss=3.2129210660485668
Epoch 999
Epoch 999 :: Batch 0/1
Batch Loss = 3.218667784374083
999, epoch_train_loss=3.218667784374083
Epoch 1000
Epoch 1000 :: Batch 0/1
Batch Loss = 3.233224464852494
1000, epoch_train_loss=3.233224464852494
Epoch 1001
Epoch 1001 :: Batch 0/1
Batch Loss = 3.2609612175995046
1001, epoch_train_loss=3.2609612175995046
Epoch 1002
Epoch 1002 :: Batch 0/1
Batch Loss = 3.2779328342379355
1002, epoch_train_loss=3.2779328342379355
Epoch 1003
Epoch 1003 :: Batch 0/1
Batch Loss = 3.271201670075859
1003, epoch_train_loss=3.271201670075859
Epoch 1004
Epoch 1004 :: Batch 0/1
Batch Loss = 3.2191399566571977
1004, epoch_train_loss=3.2191399566571977
Epoch 1005
Epoch 1005 :: Batch 0/1
Batch Loss = 3.200316891154578
1005, epoch_train_loss=3.200316891154578
Epoch 1006
Epoch 1006 :: Batch 0/1
Batch Loss = 3.220673253058622
1006, epoch_train_loss=3.220673253058622
Epoch 1007
Epoch 1007 :: Batch 0/1
Batch Loss = 3.2469123342868227
1007, epoch_train_loss=3.2469123342868227
Epoch 1008
Epoch 1008 :: Batch 0/1
Batch Loss = 3.232357871942881
1008, epoch_train_loss=3.232357871942881
Epoch 1009
Epoch 1009 :: Batch 0/1
Batch Loss = 3.1993529099779505
1009, epoch_train_loss=3.1993529099779505
Epoch 1010
Epoch 1010 :: Batch 0/1
Batch Loss = 3.1983893628539986
1010, epoch_train_loss=3.1983893628539986
Epoch 1011
Epoch 1011 :: Batch 0/1
Batch Loss = 3.217556702336346
1011, epoch_train_loss=3.217556702336346
Epoch 1012
Epoch 1012 :: Batch 0/1
Batch Loss = 3.2140797321472174
1012, epoch_train_loss=3.2140797321472174
Epoch 1013
Epoch 1013 :: Batch 0/1
Batch Loss = 3.1933981401454927
1013, epoch_train_loss=3.1933981401454927
Epoch 1014
Epoch 1014 :: Batch 0/1
Batch Loss = 3.1949509801413463
1014, epoch_train_loss=3.1949509801413463
Epoch 1015
Epoch 1015 :: Batch 0/1
Batch Loss = 3.208069438486704
1015, epoch_train_loss=3.208069438486704
Epoch 1016
Epoch 1016 :: Batch 0/1
Batch Loss = 3.200726629784094
1016, epoch_train_loss=3.200726629784094
Epoch 1017
Epoch 1017 :: Batch 0/1
Batch Loss = 3.188595650363019
1017, epoch_train_loss=3.188595650363019
Epoch 1018
Epoch 1018 :: Batch 0/1
Batch Loss = 3.1935097603086646
1018, epoch_train_loss=3.1935097603086646
Epoch 1019
Epoch 1019 :: Batch 0/1
Batch Loss = 3.1997408015912563
1019, epoch_train_loss=3.1997408015912563
Epoch 1020
Epoch 1020 :: Batch 0/1
Batch Loss = 3.1913084041042024
1020, epoch_train_loss=3.1913084041042024
Epoch 1021
Epoch 1021 :: Batch 0/1
Batch Loss = 3.1856498673223244
1021, epoch_train_loss=3.1856498673223244
Epoch 1022
Epoch 1022 :: Batch 0/1
Batch Loss = 3.1909201494915065
1022, epoch_train_loss=3.1909201494915065
Epoch 1023
Epoch 1023 :: Batch 0/1
Batch Loss = 3.192620985051016
1023, epoch_train_loss=3.192620985051016
Epoch 1024
Epoch 1024 :: Batch 0/1
Batch Loss = 3.1860984863947444
1024, epoch_train_loss=3.1860984863947444
Epoch 1025
Epoch 1025 :: Batch 0/1
Batch Loss = 3.182504823566133
1025, epoch_train_loss=3.182504823566133
Epoch 1026
Epoch 1026 :: Batch 0/1
Batch Loss = 3.1861084748582704
1026, epoch_train_loss=3.1861084748582704
Epoch 1027
Epoch 1027 :: Batch 0/1
Batch Loss = 3.187746752094781
1027, epoch_train_loss=3.187746752094781
Epoch 1028
Epoch 1028 :: Batch 0/1
Batch Loss = 3.1832565241668016
1028, epoch_train_loss=3.1832565241668016
Epoch 1029
Epoch 1029 :: Batch 0/1
Batch Loss = 3.179463128820237
1029, epoch_train_loss=3.179463128820237
Epoch 1030
Epoch 1030 :: Batch 0/1
Batch Loss = 3.1805102376254446
1030, epoch_train_loss=3.1805102376254446
Epoch 1031
Epoch 1031 :: Batch 0/1
Batch Loss = 3.1827022836050425
1031, epoch_train_loss=3.1827022836050425
Epoch 1032
Epoch 1032 :: Batch 0/1
Batch Loss = 3.182376008382036
1032, epoch_train_loss=3.182376008382036
Epoch 1033
Epoch 1033 :: Batch 0/1
Batch Loss = 3.178960056954841
1033, epoch_train_loss=3.178960056954841
Epoch 1034
Epoch 1034 :: Batch 0/1
Batch Loss = 3.1763702556353577
1034, epoch_train_loss=3.1763702556353577
Epoch 1035
Epoch 1035 :: Batch 0/1
Batch Loss = 3.176163919803092
1035, epoch_train_loss=3.176163919803092
Epoch 1036
Epoch 1036 :: Batch 0/1
Batch Loss = 3.1772819761813835
1036, epoch_train_loss=3.1772819761813835
Epoch 1037
Epoch 1037 :: Batch 0/1
Batch Loss = 3.1783534081528297
1037, epoch_train_loss=3.1783534081528297
Epoch 1038
Epoch 1038 :: Batch 0/1
Batch Loss = 3.17771418816014
1038, epoch_train_loss=3.17771418816014
Epoch 1039
Epoch 1039 :: Batch 0/1
Batch Loss = 3.1764690167528533
1039, epoch_train_loss=3.1764690167528533
Epoch 1040
Epoch 1040 :: Batch 0/1
Batch Loss = 3.174356912566251
1040, epoch_train_loss=3.174356912566251
Epoch 1041
Epoch 1041 :: Batch 0/1
Batch Loss = 3.172668634910752
1041, epoch_train_loss=3.172668634910752
Epoch 1042
Epoch 1042 :: Batch 0/1
Batch Loss = 3.171419362031535
1042, epoch_train_loss=3.171419362031535
Epoch 1043
Epoch 1043 :: Batch 0/1
Batch Loss = 3.170618841107961
1043, epoch_train_loss=3.170618841107961
Epoch 1044
Epoch 1044 :: Batch 0/1
Batch Loss = 3.17019652864355
1044, epoch_train_loss=3.17019652864355
Epoch 1045
Epoch 1045 :: Batch 0/1
Batch Loss = 3.1700319512741593
1045, epoch_train_loss=3.1700319512741593
Epoch 1046
Epoch 1046 :: Batch 0/1
Batch Loss = 3.170248037240788
1046, epoch_train_loss=3.170248037240788
Epoch 1047
Epoch 1047 :: Batch 0/1
Batch Loss = 3.1707989982146447
1047, epoch_train_loss=3.1707989982146447
Epoch 1048
Epoch 1048 :: Batch 0/1
Batch Loss = 3.1727293988927805
1048, epoch_train_loss=3.1727293988927805
Epoch 1049
Epoch 1049 :: Batch 0/1
Batch Loss = 3.1752766105077614
1049, epoch_train_loss=3.1752766105077614
Epoch 1050
Epoch 1050 :: Batch 0/1
Batch Loss = 3.1829192764029655
1050, epoch_train_loss=3.1829192764029655
Epoch 1051
Epoch 1051 :: Batch 0/1
Batch Loss = 3.187726021957647
1051, epoch_train_loss=3.187726021957647
Epoch 1052
Epoch 1052 :: Batch 0/1
Batch Loss = 3.2049312064167412
1052, epoch_train_loss=3.2049312064167412
Epoch 1053
Epoch 1053 :: Batch 0/1
Batch Loss = 3.1957553869326984
1053, epoch_train_loss=3.1957553869326984
Epoch 1054
Epoch 1054 :: Batch 0/1
Batch Loss = 3.1947206465275313
1054, epoch_train_loss=3.1947206465275313
Epoch 1055
Epoch 1055 :: Batch 0/1
Batch Loss = 3.174301881817224
1055, epoch_train_loss=3.174301881817224
Epoch 1056
Epoch 1056 :: Batch 0/1
Batch Loss = 3.163930219202851
1056, epoch_train_loss=3.163930219202851
Epoch 1057
Epoch 1057 :: Batch 0/1
Batch Loss = 3.162116725181424
1057, epoch_train_loss=3.162116725181424
Epoch 1058
Epoch 1058 :: Batch 0/1
Batch Loss = 3.1674576615812673
1058, epoch_train_loss=3.1674576615812673
Epoch 1059
Epoch 1059 :: Batch 0/1
Batch Loss = 3.176188064717722
1059, epoch_train_loss=3.176188064717722
Epoch 1060
Epoch 1060 :: Batch 0/1
Batch Loss = 3.1734357155740778
1060, epoch_train_loss=3.1734357155740778
Epoch 1061
Epoch 1061 :: Batch 0/1
Batch Loss = 3.1689333803434896
1061, epoch_train_loss=3.1689333803434896
Epoch 1062
Epoch 1062 :: Batch 0/1
Batch Loss = 3.1605924982223983
1062, epoch_train_loss=3.1605924982223983
Epoch 1063
Epoch 1063 :: Batch 0/1
Batch Loss = 3.157507962647901
1063, epoch_train_loss=3.157507962647901
Epoch 1064
Epoch 1064 :: Batch 0/1
Batch Loss = 3.159487742358137
1064, epoch_train_loss=3.159487742358137
Epoch 1065
Epoch 1065 :: Batch 0/1
Batch Loss = 3.1626903827746413
1065, epoch_train_loss=3.1626903827746413
Epoch 1066
Epoch 1066 :: Batch 0/1
Batch Loss = 3.1656765604824164
1066, epoch_train_loss=3.1656765604824164
Epoch 1067
Epoch 1067 :: Batch 0/1
Batch Loss = 3.1621602395751682
1067, epoch_train_loss=3.1621602395751682
Epoch 1068
Epoch 1068 :: Batch 0/1
Batch Loss = 3.1582003989789484
1068, epoch_train_loss=3.1582003989789484
Epoch 1069
Epoch 1069 :: Batch 0/1
Batch Loss = 3.1542433976074924
1069, epoch_train_loss=3.1542433976074924
Epoch 1070
Epoch 1070 :: Batch 0/1
Batch Loss = 3.1529273689383777
1070, epoch_train_loss=3.1529273689383777
Epoch 1071
Epoch 1071 :: Batch 0/1
Batch Loss = 3.153880073635242
1071, epoch_train_loss=3.153880073635242
Epoch 1072
Epoch 1072 :: Batch 0/1
Batch Loss = 3.155246266394806
1072, epoch_train_loss=3.155246266394806
Epoch 1073
Epoch 1073 :: Batch 0/1
Batch Loss = 3.1566559888177763
1073, epoch_train_loss=3.1566559888177763
Epoch 1074
Epoch 1074 :: Batch 0/1
Batch Loss = 3.1554006934454013
1074, epoch_train_loss=3.1554006934454013
Epoch 1075
Epoch 1075 :: Batch 0/1
Batch Loss = 3.153820939007699
1075, epoch_train_loss=3.153820939007699
Epoch 1076
Epoch 1076 :: Batch 0/1
Batch Loss = 3.151011482628687
1076, epoch_train_loss=3.151011482628687
Epoch 1077
Epoch 1077 :: Batch 0/1
Batch Loss = 3.149054691642549
1077, epoch_train_loss=3.149054691642549
Epoch 1078
Epoch 1078 :: Batch 0/1
Batch Loss = 3.1479843161244525
1078, epoch_train_loss=3.1479843161244525
Epoch 1079
Epoch 1079 :: Batch 0/1
Batch Loss = 3.1477751739333195
1079, epoch_train_loss=3.1477751739333195
Epoch 1080
Epoch 1080 :: Batch 0/1
Batch Loss = 3.148145789255417
1080, epoch_train_loss=3.148145789255417
Epoch 1081
Epoch 1081 :: Batch 0/1
Batch Loss = 3.1484803240662873
1081, epoch_train_loss=3.1484803240662873
Epoch 1082
Epoch 1082 :: Batch 0/1
Batch Loss = 3.14881609045762
1082, epoch_train_loss=3.14881609045762
Epoch 1083
Epoch 1083 :: Batch 0/1
Batch Loss = 3.1481829450880903
1083, epoch_train_loss=3.1481829450880903
Epoch 1084
Epoch 1084 :: Batch 0/1
Batch Loss = 3.147511602638153
1084, epoch_train_loss=3.147511602638153
Epoch 1085
Epoch 1085 :: Batch 0/1
Batch Loss = 3.1461704271585456
1085, epoch_train_loss=3.1461704271585456
Epoch 1086
Epoch 1086 :: Batch 0/1
Batch Loss = 3.144978173680293
1086, epoch_train_loss=3.144978173680293
Epoch 1087
Epoch 1087 :: Batch 0/1
Batch Loss = 3.1438525129418586
1087, epoch_train_loss=3.1438525129418586
Epoch 1088
Epoch 1088 :: Batch 0/1
Batch Loss = 3.1430921693839644
1088, epoch_train_loss=3.1430921693839644
Epoch 1089
Epoch 1089 :: Batch 0/1
Batch Loss = 3.142678279971744
1089, epoch_train_loss=3.142678279971744
Epoch 1090
Epoch 1090 :: Batch 0/1
Batch Loss = 3.142521611833225
1090, epoch_train_loss=3.142521611833225
Epoch 1091
Epoch 1091 :: Batch 0/1
Batch Loss = 3.1424664924473538
1091, epoch_train_loss=3.1424664924473538
Epoch 1092
Epoch 1092 :: Batch 0/1
Batch Loss = 3.1422982831599726
1092, epoch_train_loss=3.1422982831599726
Epoch 1093
Epoch 1093 :: Batch 0/1
Batch Loss = 3.142026204707607
1093, epoch_train_loss=3.142026204707607
Epoch 1094
Epoch 1094 :: Batch 0/1
Batch Loss = 3.14149716448132
1094, epoch_train_loss=3.14149716448132
Epoch 1095
Epoch 1095 :: Batch 0/1
Batch Loss = 3.1409126117330812
1095, epoch_train_loss=3.1409126117330812
Epoch 1096
Epoch 1096 :: Batch 0/1
Batch Loss = 3.140299930035326
1096, epoch_train_loss=3.140299930035326
Epoch 1097
Epoch 1097 :: Batch 0/1
Batch Loss = 3.1397990848565636
1097, epoch_train_loss=3.1397990848565636
Epoch 1098
Epoch 1098 :: Batch 0/1
Batch Loss = 3.1394336370333815
1098, epoch_train_loss=3.1394336370333815
Epoch 1099
Epoch 1099 :: Batch 0/1
Batch Loss = 3.1391918568255233
1099, epoch_train_loss=3.1391918568255233
Epoch 1100
Epoch 1100 :: Batch 0/1
Batch Loss = 3.1390226070268064
1100, epoch_train_loss=3.1390226070268064
Epoch 1101
Epoch 1101 :: Batch 0/1
Batch Loss = 3.138855381020323
1101, epoch_train_loss=3.138855381020323
Epoch 1102
Epoch 1102 :: Batch 0/1
Batch Loss = 3.138655000181683
1102, epoch_train_loss=3.138655000181683
Epoch 1103
Epoch 1103 :: Batch 0/1
Batch Loss = 3.138375269896585
1103, epoch_train_loss=3.138375269896585
Epoch 1104
Epoch 1104 :: Batch 0/1
Batch Loss = 3.138061818020721
1104, epoch_train_loss=3.138061818020721
Epoch 1105
Epoch 1105 :: Batch 0/1
Batch Loss = 3.137708218735333
1105, epoch_train_loss=3.137708218735333
Epoch 1106
Epoch 1106 :: Batch 0/1
Batch Loss = 3.1373637731554047
1106, epoch_train_loss=3.1373637731554047
Epoch 1107
Epoch 1107 :: Batch 0/1
Batch Loss = 3.1370430035632135
1107, epoch_train_loss=3.1370430035632135
Epoch 1108
Epoch 1108 :: Batch 0/1
Batch Loss = 3.1367587428790644
1108, epoch_train_loss=3.1367587428790644
Epoch 1109
Epoch 1109 :: Batch 0/1
Batch Loss = 3.136509215868794
1109, epoch_train_loss=3.136509215868794
Epoch 1110
Epoch 1110 :: Batch 0/1
Batch Loss = 3.1362846122454915
1110, epoch_train_loss=3.1362846122454915
Epoch 1111
Epoch 1111 :: Batch 0/1
Batch Loss = 3.1360735209898647
1111, epoch_train_loss=3.1360735209898647
Epoch 1112
Epoch 1112 :: Batch 0/1
Batch Loss = 3.135860746922077
1112, epoch_train_loss=3.135860746922077
Epoch 1113
Epoch 1113 :: Batch 0/1
Batch Loss = 3.1356410128508823
1113, epoch_train_loss=3.1356410128508823
Epoch 1114
Epoch 1114 :: Batch 0/1
Batch Loss = 3.135400223739216
1114, epoch_train_loss=3.135400223739216
Epoch 1115
Epoch 1115 :: Batch 0/1
Batch Loss = 3.1351453743855267
1115, epoch_train_loss=3.1351453743855267
Epoch 1116
Epoch 1116 :: Batch 0/1
Batch Loss = 3.1348632095804567
1116, epoch_train_loss=3.1348632095804567
Epoch 1117
Epoch 1117 :: Batch 0/1
Batch Loss = 3.134566266096383
1117, epoch_train_loss=3.134566266096383
Epoch 1118
Epoch 1118 :: Batch 0/1
Batch Loss = 3.134241561464544
1118, epoch_train_loss=3.134241561464544
Epoch 1119
Epoch 1119 :: Batch 0/1
Batch Loss = 3.133901652974253
1119, epoch_train_loss=3.133901652974253
Epoch 1120
Epoch 1120 :: Batch 0/1
Batch Loss = 3.133535537806499
1120, epoch_train_loss=3.133535537806499
Epoch 1121
Epoch 1121 :: Batch 0/1
Batch Loss = 3.133151432933733
1121, epoch_train_loss=3.133151432933733
Epoch 1122
Epoch 1122 :: Batch 0/1
Batch Loss = 3.132740830745058
1122, epoch_train_loss=3.132740830745058
Epoch 1123
Epoch 1123 :: Batch 0/1
Batch Loss = 3.132309063518459
1123, epoch_train_loss=3.132309063518459
Epoch 1124
Epoch 1124 :: Batch 0/1
Batch Loss = 3.1318502210150867
1124, epoch_train_loss=3.1318502210150867
Epoch 1125
Epoch 1125 :: Batch 0/1
Batch Loss = 3.1313671888230137
1125, epoch_train_loss=3.1313671888230137
Epoch 1126
Epoch 1126 :: Batch 0/1
Batch Loss = 3.1308569761348917
1126, epoch_train_loss=3.1308569761348917
Epoch 1127
Epoch 1127 :: Batch 0/1
Batch Loss = 3.1303224289906857
1127, epoch_train_loss=3.1303224289906857
Epoch 1128
Epoch 1128 :: Batch 0/1
Batch Loss = 3.1297627301755724
1128, epoch_train_loss=3.1297627301755724
Epoch 1129
Epoch 1129 :: Batch 0/1
Batch Loss = 3.1291817064059497
1129, epoch_train_loss=3.1291817064059497
Epoch 1130
Epoch 1130 :: Batch 0/1
Batch Loss = 3.128580937648447
1130, epoch_train_loss=3.128580937648447
Epoch 1131
Epoch 1131 :: Batch 0/1
Batch Loss = 3.127966052304373
1131, epoch_train_loss=3.127966052304373
Epoch 1132
Epoch 1132 :: Batch 0/1
Batch Loss = 3.1273409322367125
1132, epoch_train_loss=3.1273409322367125
Epoch 1133
Epoch 1133 :: Batch 0/1
Batch Loss = 3.1267165234094003
1133, epoch_train_loss=3.1267165234094003
Epoch 1134
Epoch 1134 :: Batch 0/1
Batch Loss = 3.126101594397117
1134, epoch_train_loss=3.126101594397117
Epoch 1135
Epoch 1135 :: Batch 0/1
Batch Loss = 3.1255279586851787
1135, epoch_train_loss=3.1255279586851787
Epoch 1136
Epoch 1136 :: Batch 0/1
Batch Loss = 3.1250205624635945
1136, epoch_train_loss=3.1250205624635945
Epoch 1137
Epoch 1137 :: Batch 0/1
Batch Loss = 3.124709175129435
1137, epoch_train_loss=3.124709175129435
Epoch 1138
Epoch 1138 :: Batch 0/1
Batch Loss = 3.1246714109914735
1138, epoch_train_loss=3.1246714109914735
Epoch 1139
Epoch 1139 :: Batch 0/1
Batch Loss = 3.1255633649362697
1139, epoch_train_loss=3.1255633649362697
Epoch 1140
Epoch 1140 :: Batch 0/1
Batch Loss = 3.1274146824231805
1140, epoch_train_loss=3.1274146824231805
Epoch 1141
Epoch 1141 :: Batch 0/1
Batch Loss = 3.13382502019301
1141, epoch_train_loss=3.13382502019301
Epoch 1142
Epoch 1142 :: Batch 0/1
Batch Loss = 3.1406832381834167
1142, epoch_train_loss=3.1406832381834167
Epoch 1143
Epoch 1143 :: Batch 0/1
Batch Loss = 3.1637810288162482
1143, epoch_train_loss=3.1637810288162482
Epoch 1144
Epoch 1144 :: Batch 0/1
Batch Loss = 3.1577205405284188
1144, epoch_train_loss=3.1577205405284188
Epoch 1145
Epoch 1145 :: Batch 0/1
Batch Loss = 3.162191028898289
1145, epoch_train_loss=3.162191028898289
Epoch 1146
Epoch 1146 :: Batch 0/1
Batch Loss = 3.13081737944441
1146, epoch_train_loss=3.13081737944441
Epoch 1147
Epoch 1147 :: Batch 0/1
Batch Loss = 3.1181314348819273
1147, epoch_train_loss=3.1181314348819273
Epoch 1148
Epoch 1148 :: Batch 0/1
Batch Loss = 3.1238669838181177
1148, epoch_train_loss=3.1238669838181177
Epoch 1149
Epoch 1149 :: Batch 0/1
Batch Loss = 3.1345619602775283
1149, epoch_train_loss=3.1345619602775283
Epoch 1150
Epoch 1150 :: Batch 0/1
Batch Loss = 3.1436739093062798
1150, epoch_train_loss=3.1436739093062798
Epoch 1151
Epoch 1151 :: Batch 0/1
Batch Loss = 3.1237039770720774
1151, epoch_train_loss=3.1237039770720774
Epoch 1152
Epoch 1152 :: Batch 0/1
Batch Loss = 3.1152757426686986
1152, epoch_train_loss=3.1152757426686986
Epoch 1153
Epoch 1153 :: Batch 0/1
Batch Loss = 3.122846480253213
1153, epoch_train_loss=3.122846480253213
Epoch 1154
Epoch 1154 :: Batch 0/1
Batch Loss = 3.1292449527775803
1154, epoch_train_loss=3.1292449527775803
Epoch 1155
Epoch 1155 :: Batch 0/1
Batch Loss = 3.1305137155761735
1155, epoch_train_loss=3.1305137155761735
Epoch 1156
Epoch 1156 :: Batch 0/1
Batch Loss = 3.1149293511222123
1156, epoch_train_loss=3.1149293511222123
Epoch 1157
Epoch 1157 :: Batch 0/1
Batch Loss = 3.113911220493741
1157, epoch_train_loss=3.113911220493741
Epoch 1158
Epoch 1158 :: Batch 0/1
Batch Loss = 3.1234499965643154
1158, epoch_train_loss=3.1234499965643154
Epoch 1159
Epoch 1159 :: Batch 0/1
Batch Loss = 3.1245782497690233
1159, epoch_train_loss=3.1245782497690233
Epoch 1160
Epoch 1160 :: Batch 0/1
Batch Loss = 3.1187090533876365
1160, epoch_train_loss=3.1187090533876365
Epoch 1161
Epoch 1161 :: Batch 0/1
Batch Loss = 3.1108871738074493
1161, epoch_train_loss=3.1108871738074493
Epoch 1162
Epoch 1162 :: Batch 0/1
Batch Loss = 3.111324593476948
1162, epoch_train_loss=3.111324593476948
Epoch 1163
Epoch 1163 :: Batch 0/1
Batch Loss = 3.118200463650476
1163, epoch_train_loss=3.118200463650476
Epoch 1164
Epoch 1164 :: Batch 0/1
Batch Loss = 3.116564838691679
1164, epoch_train_loss=3.116564838691679
Epoch 1165
Epoch 1165 :: Batch 0/1
Batch Loss = 3.11361563711964
1165, epoch_train_loss=3.11361563711964
Epoch 1166
Epoch 1166 :: Batch 0/1
Batch Loss = 3.108570272053572
1166, epoch_train_loss=3.108570272053572
Epoch 1167
Epoch 1167 :: Batch 0/1
Batch Loss = 3.1071185332089986
1167, epoch_train_loss=3.1071185332089986
Epoch 1168
Epoch 1168 :: Batch 0/1
Batch Loss = 3.1082224999721655
1168, epoch_train_loss=3.1082224999721655
Epoch 1169
Epoch 1169 :: Batch 0/1
Batch Loss = 3.1099710072958153
1169, epoch_train_loss=3.1099710072958153
Epoch 1170
Epoch 1170 :: Batch 0/1
Batch Loss = 3.1120229616087656
1170, epoch_train_loss=3.1120229616087656
Epoch 1171
Epoch 1171 :: Batch 0/1
Batch Loss = 3.110043608008415
1171, epoch_train_loss=3.110043608008415
Epoch 1172
Epoch 1172 :: Batch 0/1
Batch Loss = 3.1080680465093176
1172, epoch_train_loss=3.1080680465093176
Epoch 1173
Epoch 1173 :: Batch 0/1
Batch Loss = 3.1056597632201353
1173, epoch_train_loss=3.1056597632201353
Epoch 1174
Epoch 1174 :: Batch 0/1
Batch Loss = 3.1041652176343275
1174, epoch_train_loss=3.1041652176343275
Epoch 1175
Epoch 1175 :: Batch 0/1
Batch Loss = 3.1032506161182254
1175, epoch_train_loss=3.1032506161182254
Epoch 1176
Epoch 1176 :: Batch 0/1
Batch Loss = 3.102452706664586
1176, epoch_train_loss=3.102452706664586
Epoch 1177
Epoch 1177 :: Batch 0/1
Batch Loss = 3.1019524174755295
1177, epoch_train_loss=3.1019524174755295
Epoch 1178
Epoch 1178 :: Batch 0/1
Batch Loss = 3.1014115676284493
1178, epoch_train_loss=3.1014115676284493
Epoch 1179
Epoch 1179 :: Batch 0/1
Batch Loss = 3.1009679418392673
1179, epoch_train_loss=3.1009679418392673
Epoch 1180
Epoch 1180 :: Batch 0/1
Batch Loss = 3.10076447152772
1180, epoch_train_loss=3.10076447152772
Epoch 1181
Epoch 1181 :: Batch 0/1
Batch Loss = 3.101779982972959
1181, epoch_train_loss=3.101779982972959
Epoch 1182
Epoch 1182 :: Batch 0/1
Batch Loss = 3.1092741652999787
1182, epoch_train_loss=3.1092741652999787
Epoch 1183
Epoch 1183 :: Batch 0/1
Batch Loss = 3.1595645276401356
1183, epoch_train_loss=3.1595645276401356
Epoch 1184
Epoch 1184 :: Batch 0/1
Batch Loss = 3.1739632523945644
1184, epoch_train_loss=3.1739632523945644
Epoch 1185
Epoch 1185 :: Batch 0/1
Batch Loss = 3.3059588071792976
1185, epoch_train_loss=3.3059588071792976
Epoch 1186
Epoch 1186 :: Batch 0/1
Batch Loss = 3.206284738006663
1186, epoch_train_loss=3.206284738006663
Epoch 1187
Epoch 1187 :: Batch 0/1
Batch Loss = 3.2263427288433246
1187, epoch_train_loss=3.2263427288433246
Epoch 1188
Epoch 1188 :: Batch 0/1
Batch Loss = 3.1803716452663253
1188, epoch_train_loss=3.1803716452663253
Epoch 1189
Epoch 1189 :: Batch 0/1
Batch Loss = 3.167235719736527
1189, epoch_train_loss=3.167235719736527
Epoch 1190
Epoch 1190 :: Batch 0/1
Batch Loss = 3.1825678531321056
1190, epoch_train_loss=3.1825678531321056
Epoch 1191
Epoch 1191 :: Batch 0/1
Batch Loss = 3.1388781278899835
1191, epoch_train_loss=3.1388781278899835
Epoch 1192
Epoch 1192 :: Batch 0/1
Batch Loss = 3.165835192154303
1192, epoch_train_loss=3.165835192154303
Epoch 1193
Epoch 1193 :: Batch 0/1
Batch Loss = 3.1270991492051263
1193, epoch_train_loss=3.1270991492051263
Epoch 1194
Epoch 1194 :: Batch 0/1
Batch Loss = 3.1355094746245946
1194, epoch_train_loss=3.1355094746245946
Epoch 1195
Epoch 1195 :: Batch 0/1
Batch Loss = 3.146403155579898
1195, epoch_train_loss=3.146403155579898
Epoch 1196
Epoch 1196 :: Batch 0/1
Batch Loss = 3.136773723847204
1196, epoch_train_loss=3.136773723847204
Epoch 1197
Epoch 1197 :: Batch 0/1
Batch Loss = 3.1368327725885514
1197, epoch_train_loss=3.1368327725885514
Epoch 1198
Epoch 1198 :: Batch 0/1
Batch Loss = 3.1138688991758223
1198, epoch_train_loss=3.1138688991758223
Epoch 1199
Epoch 1199 :: Batch 0/1
Batch Loss = 3.139426667478912
1199, epoch_train_loss=3.139426667478912
Epoch 1200
Epoch 1200 :: Batch 0/1
Batch Loss = 3.1098474311957807
1200, epoch_train_loss=3.1098474311957807
Epoch 1201
Epoch 1201 :: Batch 0/1
Batch Loss = 3.1334778854430394
1201, epoch_train_loss=3.1334778854430394
Epoch 1202
Epoch 1202 :: Batch 0/1
Batch Loss = 3.1194441981338983
1202, epoch_train_loss=3.1194441981338983
Epoch 1203
Epoch 1203 :: Batch 0/1
Batch Loss = 3.116925652437877
1203, epoch_train_loss=3.116925652437877
Epoch 1204
Epoch 1204 :: Batch 0/1
Batch Loss = 3.1289115105907523
1204, epoch_train_loss=3.1289115105907523
Epoch 1205
Epoch 1205 :: Batch 0/1
Batch Loss = 3.1076511644736735
1205, epoch_train_loss=3.1076511644736735
Epoch 1206
Epoch 1206 :: Batch 0/1
Batch Loss = 3.120365536627168
1206, epoch_train_loss=3.120365536627168
Epoch 1207
Epoch 1207 :: Batch 0/1
Batch Loss = 3.107135065567896
1207, epoch_train_loss=3.107135065567896
Epoch 1208
Epoch 1208 :: Batch 0/1
Batch Loss = 3.113521668337286
1208, epoch_train_loss=3.113521668337286
Epoch 1209
Epoch 1209 :: Batch 0/1
Batch Loss = 3.1168261449963928
1209, epoch_train_loss=3.1168261449963928
Epoch 1210
Epoch 1210 :: Batch 0/1
Batch Loss = 3.103506942751995
1210, epoch_train_loss=3.103506942751995
Epoch 1211
Epoch 1211 :: Batch 0/1
Batch Loss = 3.1166021085774043
1211, epoch_train_loss=3.1166021085774043
Epoch 1212
Epoch 1212 :: Batch 0/1
Batch Loss = 3.1091559990770774
1212, epoch_train_loss=3.1091559990770774
Epoch 1213
Epoch 1213 :: Batch 0/1
Batch Loss = 3.103613741594107
1213, epoch_train_loss=3.103613741594107
Epoch 1214
Epoch 1214 :: Batch 0/1
Batch Loss = 3.118821382730923
1214, epoch_train_loss=3.118821382730923
Epoch 1215
Epoch 1215 :: Batch 0/1
Batch Loss = 3.111707487872558
1215, epoch_train_loss=3.111707487872558
Epoch 1216
Epoch 1216 :: Batch 0/1
Batch Loss = 3.1022442262141285
1216, epoch_train_loss=3.1022442262141285
Epoch 1217
Epoch 1217 :: Batch 0/1
Batch Loss = 3.122527143877946
1217, epoch_train_loss=3.122527143877946
Epoch 1218
Epoch 1218 :: Batch 0/1
Batch Loss = 3.128526934620698
1218, epoch_train_loss=3.128526934620698
Epoch 1219
Epoch 1219 :: Batch 0/1
Batch Loss = 3.097592380817833
1219, epoch_train_loss=3.097592380817833
Epoch 1220
Epoch 1220 :: Batch 0/1
Batch Loss = 3.1586796764439025
1220, epoch_train_loss=3.1586796764439025
Epoch 1221
Epoch 1221 :: Batch 0/1
Batch Loss = 3.2258125592409153
1221, epoch_train_loss=3.2258125592409153
Epoch 1222
Epoch 1222 :: Batch 0/1
Batch Loss = 3.206409219222261
1222, epoch_train_loss=3.206409219222261
Epoch 1223
Epoch 1223 :: Batch 0/1
Batch Loss = 3.1514278179339743
1223, epoch_train_loss=3.1514278179339743
Epoch 1224
Epoch 1224 :: Batch 0/1
Batch Loss = 3.2134510328671197
1224, epoch_train_loss=3.2134510328671197
Epoch 1225
Epoch 1225 :: Batch 0/1
Batch Loss = 3.2532167430244745
1225, epoch_train_loss=3.2532167430244745
Epoch 1226
Epoch 1226 :: Batch 0/1
Batch Loss = 3.224417617512074
1226, epoch_train_loss=3.224417617512074
Epoch 1227
Epoch 1227 :: Batch 0/1
Batch Loss = 3.1823772218963664
1227, epoch_train_loss=3.1823772218963664
Epoch 1228
Epoch 1228 :: Batch 0/1
Batch Loss = 3.229870050926792
1228, epoch_train_loss=3.229870050926792
Epoch 1229
Epoch 1229 :: Batch 0/1
Batch Loss = 3.12898424729485
1229, epoch_train_loss=3.12898424729485
Epoch 1230
Epoch 1230 :: Batch 0/1
Batch Loss = 3.221963778005218
1230, epoch_train_loss=3.221963778005218
Epoch 1231
Epoch 1231 :: Batch 0/1
Batch Loss = 3.1341886256746774
1231, epoch_train_loss=3.1341886256746774
Epoch 1232
Epoch 1232 :: Batch 0/1
Batch Loss = 3.1754221676545167
1232, epoch_train_loss=3.1754221676545167
Epoch 1233
Epoch 1233 :: Batch 0/1
Batch Loss = 3.1472613674824563
1233, epoch_train_loss=3.1472613674824563
Epoch 1234
Epoch 1234 :: Batch 0/1
Batch Loss = 3.1625283405794407
1234, epoch_train_loss=3.1625283405794407
Epoch 1235
Epoch 1235 :: Batch 0/1
Batch Loss = 3.133225847002716
1235, epoch_train_loss=3.133225847002716
Epoch 1236
Epoch 1236 :: Batch 0/1
Batch Loss = 3.1480285390236036
1236, epoch_train_loss=3.1480285390236036
Epoch 1237
Epoch 1237 :: Batch 0/1
Batch Loss = 3.149371774719465
1237, epoch_train_loss=3.149371774719465
Epoch 1238
Epoch 1238 :: Batch 0/1
Batch Loss = 3.120707778050776
1238, epoch_train_loss=3.120707778050776
Epoch 1239
Epoch 1239 :: Batch 0/1
Batch Loss = 3.145290057690471
1239, epoch_train_loss=3.145290057690471
Epoch 1240
Epoch 1240 :: Batch 0/1
Batch Loss = 3.123184108903351
1240, epoch_train_loss=3.123184108903351
Epoch 1241
Epoch 1241 :: Batch 0/1
Batch Loss = 3.130172942981581
1241, epoch_train_loss=3.130172942981581
Epoch 1242
Epoch 1242 :: Batch 0/1
Batch Loss = 3.1186587621121196
1242, epoch_train_loss=3.1186587621121196
Epoch 1243
Epoch 1243 :: Batch 0/1
Batch Loss = 3.131323986778815
1243, epoch_train_loss=3.131323986778815
Epoch 1244
Epoch 1244 :: Batch 0/1
Batch Loss = 3.120137986246822
1244, epoch_train_loss=3.120137986246822
Epoch 1245
Epoch 1245 :: Batch 0/1
Batch Loss = 3.1163436766992687
1245, epoch_train_loss=3.1163436766992687
Epoch 1246
Epoch 1246 :: Batch 0/1
Batch Loss = 3.12438732968506
1246, epoch_train_loss=3.12438732968506
Epoch 1247
Epoch 1247 :: Batch 0/1
Batch Loss = 3.1106925274048396
1247, epoch_train_loss=3.1106925274048396
Epoch 1248
Epoch 1248 :: Batch 0/1
Batch Loss = 3.11666508864025
1248, epoch_train_loss=3.11666508864025
Epoch 1249
Epoch 1249 :: Batch 0/1
Batch Loss = 3.1104018286092683
1249, epoch_train_loss=3.1104018286092683
Epoch 1250
Epoch 1250 :: Batch 0/1
Batch Loss = 3.1147353019582162
1250, epoch_train_loss=3.1147353019582162
Epoch 1251
Epoch 1251 :: Batch 0/1
Batch Loss = 3.1053887899982247
1251, epoch_train_loss=3.1053887899982247
Epoch 1252
Epoch 1252 :: Batch 0/1
Batch Loss = 3.113278265456126
1252, epoch_train_loss=3.113278265456126
Epoch 1253
Epoch 1253 :: Batch 0/1
Batch Loss = 3.1031884890359476
1253, epoch_train_loss=3.1031884890359476
Epoch 1254
Epoch 1254 :: Batch 0/1
Batch Loss = 3.108394997835762
1254, epoch_train_loss=3.108394997835762
Epoch 1255
Epoch 1255 :: Batch 0/1
Batch Loss = 3.105653101739237
1255, epoch_train_loss=3.105653101739237
Epoch 1256
Epoch 1256 :: Batch 0/1
Batch Loss = 3.1040381754372643
1256, epoch_train_loss=3.1040381754372643
Epoch 1257
Epoch 1257 :: Batch 0/1
Batch Loss = 3.1040586907138956
1257, epoch_train_loss=3.1040586907138956
Epoch 1258
Epoch 1258 :: Batch 0/1
Batch Loss = 3.0991695731880853
1258, epoch_train_loss=3.0991695731880853
Epoch 1259
Epoch 1259 :: Batch 0/1
Batch Loss = 3.101988216350737
1259, epoch_train_loss=3.101988216350737
Epoch 1260
Epoch 1260 :: Batch 0/1
Batch Loss = 3.095809251657454
1260, epoch_train_loss=3.095809251657454
Epoch 1261
Epoch 1261 :: Batch 0/1
Batch Loss = 3.099258905162042
1261, epoch_train_loss=3.099258905162042
Epoch 1262
Epoch 1262 :: Batch 0/1
Batch Loss = 3.1004432539243005
1262, epoch_train_loss=3.1004432539243005
Epoch 1263
Epoch 1263 :: Batch 0/1
Batch Loss = 3.0935246806842076
1263, epoch_train_loss=3.0935246806842076
Epoch 1264
Epoch 1264 :: Batch 0/1
Batch Loss = 3.0993638210594967
1264, epoch_train_loss=3.0993638210594967
Epoch 1265
Epoch 1265 :: Batch 0/1
Batch Loss = 3.1006358956721676
1265, epoch_train_loss=3.1006358956721676
Epoch 1266
Epoch 1266 :: Batch 0/1
Batch Loss = 3.091222604657889
1266, epoch_train_loss=3.091222604657889
Epoch 1267
Epoch 1267 :: Batch 0/1
Batch Loss = 3.100049528697141
1267, epoch_train_loss=3.100049528697141
Epoch 1268
Epoch 1268 :: Batch 0/1
Batch Loss = 3.115472120318894
1268, epoch_train_loss=3.115472120318894
Epoch 1269
Epoch 1269 :: Batch 0/1
Batch Loss = 3.0915512026592737
1269, epoch_train_loss=3.0915512026592737
Epoch 1270
Epoch 1270 :: Batch 0/1
Batch Loss = 3.0983575596691426
1270, epoch_train_loss=3.0983575596691426
Epoch 1271
Epoch 1271 :: Batch 0/1
Batch Loss = 3.1248275910276626
1271, epoch_train_loss=3.1248275910276626
Epoch 1272
Epoch 1272 :: Batch 0/1
Batch Loss = 3.0900603400774793
1272, epoch_train_loss=3.0900603400774793
Epoch 1273
Epoch 1273 :: Batch 0/1
Batch Loss = 3.100415239878503
1273, epoch_train_loss=3.100415239878503
Epoch 1274
Epoch 1274 :: Batch 0/1
Batch Loss = 3.1488882899291286
1274, epoch_train_loss=3.1488882899291286
Epoch 1275
Epoch 1275 :: Batch 0/1
Batch Loss = 3.0886323268711324
1275, epoch_train_loss=3.0886323268711324
Epoch 1276
Epoch 1276 :: Batch 0/1
Batch Loss = 3.178289462561752
1276, epoch_train_loss=3.178289462561752
Epoch 1277
Epoch 1277 :: Batch 0/1
Batch Loss = 3.2980944182132323
1277, epoch_train_loss=3.2980944182132323
Epoch 1278
Epoch 1278 :: Batch 0/1
Batch Loss = 3.3424290389378783
1278, epoch_train_loss=3.3424290389378783
Epoch 1279
Epoch 1279 :: Batch 0/1
Batch Loss = 3.27118544056946
1279, epoch_train_loss=3.27118544056946
Epoch 1280
Epoch 1280 :: Batch 0/1
Batch Loss = 3.2380476631459105
1280, epoch_train_loss=3.2380476631459105
Epoch 1281
Epoch 1281 :: Batch 0/1
Batch Loss = 3.1825364280432926
1281, epoch_train_loss=3.1825364280432926
Epoch 1282
Epoch 1282 :: Batch 0/1
Batch Loss = 3.2301107688282724
1282, epoch_train_loss=3.2301107688282724
Epoch 1283
Epoch 1283 :: Batch 0/1
Batch Loss = 3.2467527885110226
1283, epoch_train_loss=3.2467527885110226
Epoch 1284
Epoch 1284 :: Batch 0/1
Batch Loss = 3.17240908861051
1284, epoch_train_loss=3.17240908861051
Epoch 1285
Epoch 1285 :: Batch 0/1
Batch Loss = 3.1502649005854337
1285, epoch_train_loss=3.1502649005854337
Epoch 1286
Epoch 1286 :: Batch 0/1
Batch Loss = 3.222880761768881
1286, epoch_train_loss=3.222880761768881
Epoch 1287
Epoch 1287 :: Batch 0/1
Batch Loss = 3.137052143778913
1287, epoch_train_loss=3.137052143778913
Epoch 1288
Epoch 1288 :: Batch 0/1
Batch Loss = 3.1353219593521886
1288, epoch_train_loss=3.1353219593521886
Epoch 1289
Epoch 1289 :: Batch 0/1
Batch Loss = 3.15129100299123
1289, epoch_train_loss=3.15129100299123
Epoch 1290
Epoch 1290 :: Batch 0/1
Batch Loss = 3.1671263442110917
1290, epoch_train_loss=3.1671263442110917
Epoch 1291
Epoch 1291 :: Batch 0/1
Batch Loss = 3.1264510881088228
1291, epoch_train_loss=3.1264510881088228
Epoch 1292
Epoch 1292 :: Batch 0/1
Batch Loss = 3.1402154029722973
1292, epoch_train_loss=3.1402154029722973
Epoch 1293
Epoch 1293 :: Batch 0/1
Batch Loss = 3.150790678650439
1293, epoch_train_loss=3.150790678650439
Epoch 1294
Epoch 1294 :: Batch 0/1
Batch Loss = 3.144412556079731
1294, epoch_train_loss=3.144412556079731
Epoch 1295
Epoch 1295 :: Batch 0/1
Batch Loss = 3.1133103910970474
1295, epoch_train_loss=3.1133103910970474
Epoch 1296
Epoch 1296 :: Batch 0/1
Batch Loss = 3.1313055697763423
1296, epoch_train_loss=3.1313055697763423
Epoch 1297
Epoch 1297 :: Batch 0/1
Batch Loss = 3.1403211728880454
1297, epoch_train_loss=3.1403211728880454
Epoch 1298
Epoch 1298 :: Batch 0/1
Batch Loss = 3.1091798123370915
1298, epoch_train_loss=3.1091798123370915
Epoch 1299
Epoch 1299 :: Batch 0/1
Batch Loss = 3.1081946788686454
1299, epoch_train_loss=3.1081946788686454
Epoch 1300
Epoch 1300 :: Batch 0/1
Batch Loss = 3.112895114246473
1300, epoch_train_loss=3.112895114246473
Epoch 1301
Epoch 1301 :: Batch 0/1
Batch Loss = 3.117887787239444
1301, epoch_train_loss=3.117887787239444
Epoch 1302
Epoch 1302 :: Batch 0/1
Batch Loss = 3.1009268114882227
1302, epoch_train_loss=3.1009268114882227
Epoch 1303
Epoch 1303 :: Batch 0/1
Batch Loss = 3.1026890612823697
1303, epoch_train_loss=3.1026890612823697
Epoch 1304
Epoch 1304 :: Batch 0/1
Batch Loss = 3.1075534035159413
1304, epoch_train_loss=3.1075534035159413
Epoch 1305
Epoch 1305 :: Batch 0/1
Batch Loss = 3.0995668448544333
1305, epoch_train_loss=3.0995668448544333
Epoch 1306
Epoch 1306 :: Batch 0/1
Batch Loss = 3.097257226432904
1306, epoch_train_loss=3.097257226432904
Epoch 1307
Epoch 1307 :: Batch 0/1
Batch Loss = 3.096110858332098
1307, epoch_train_loss=3.096110858332098
Epoch 1308
Epoch 1308 :: Batch 0/1
Batch Loss = 3.0953411101457022
1308, epoch_train_loss=3.0953411101457022
Epoch 1309
Epoch 1309 :: Batch 0/1
Batch Loss = 3.093001163253031
1309, epoch_train_loss=3.093001163253031
Epoch 1310
Epoch 1310 :: Batch 0/1
Batch Loss = 3.091276840337134
1310, epoch_train_loss=3.091276840337134
Epoch 1311
Epoch 1311 :: Batch 0/1
Batch Loss = 3.0911694502452907
1311, epoch_train_loss=3.0911694502452907
Epoch 1312
Epoch 1312 :: Batch 0/1
Batch Loss = 3.0900949674968388
1312, epoch_train_loss=3.0900949674968388
Epoch 1313
Epoch 1313 :: Batch 0/1
Batch Loss = 3.0880553899535195
1313, epoch_train_loss=3.0880553899535195
Epoch 1314
Epoch 1314 :: Batch 0/1
Batch Loss = 3.087251038650725
1314, epoch_train_loss=3.087251038650725
Epoch 1315
Epoch 1315 :: Batch 0/1
Batch Loss = 3.0843981859799303
1315, epoch_train_loss=3.0843981859799303
Epoch 1316
Epoch 1316 :: Batch 0/1
Batch Loss = 3.0835617823927506
1316, epoch_train_loss=3.0835617823927506
Epoch 1317
Epoch 1317 :: Batch 0/1
Batch Loss = 3.0892825945581075
1317, epoch_train_loss=3.0892825945581075
Epoch 1318
Epoch 1318 :: Batch 0/1
Batch Loss = 3.0952054760977283
1318, epoch_train_loss=3.0952054760977283
Epoch 1319
Epoch 1319 :: Batch 0/1
Batch Loss = 3.1200748485245517
1319, epoch_train_loss=3.1200748485245517
Epoch 1320
Epoch 1320 :: Batch 0/1
Batch Loss = 3.110168050307732
1320, epoch_train_loss=3.110168050307732
Epoch 1321
Epoch 1321 :: Batch 0/1
Batch Loss = 3.141805875562467
1321, epoch_train_loss=3.141805875562467
Epoch 1322
Epoch 1322 :: Batch 0/1
Batch Loss = 3.081675818645423
1322, epoch_train_loss=3.081675818645423
Epoch 1323
Epoch 1323 :: Batch 0/1
Batch Loss = 3.114428250734919
1323, epoch_train_loss=3.114428250734919
Epoch 1324
Epoch 1324 :: Batch 0/1
Batch Loss = 3.2487384116642715
1324, epoch_train_loss=3.2487384116642715
Epoch 1325
Epoch 1325 :: Batch 0/1
Batch Loss = 3.2198990771935527
1325, epoch_train_loss=3.2198990771935527
Epoch 1326
Epoch 1326 :: Batch 0/1
Batch Loss = 3.1165207963211063
1326, epoch_train_loss=3.1165207963211063
Epoch 1327
Epoch 1327 :: Batch 0/1
Batch Loss = 3.10854451390845
1327, epoch_train_loss=3.10854451390845
Epoch 1328
Epoch 1328 :: Batch 0/1
Batch Loss = 3.156612728135568
1328, epoch_train_loss=3.156612728135568
Epoch 1329
Epoch 1329 :: Batch 0/1
Batch Loss = 3.1390941731447586
1329, epoch_train_loss=3.1390941731447586
Epoch 1330
Epoch 1330 :: Batch 0/1
Batch Loss = 3.1624258041316513
1330, epoch_train_loss=3.1624258041316513
Epoch 1331
Epoch 1331 :: Batch 0/1
Batch Loss = 3.1377187823251305
1331, epoch_train_loss=3.1377187823251305
Epoch 1332
Epoch 1332 :: Batch 0/1
Batch Loss = 3.153013462228058
1332, epoch_train_loss=3.153013462228058
Epoch 1333
Epoch 1333 :: Batch 0/1
Batch Loss = 3.111108094779334
1333, epoch_train_loss=3.111108094779334
Epoch 1334
Epoch 1334 :: Batch 0/1
Batch Loss = 3.175934197285331
1334, epoch_train_loss=3.175934197285331
Epoch 1335
Epoch 1335 :: Batch 0/1
Batch Loss = 3.122616958868486
1335, epoch_train_loss=3.122616958868486
Epoch 1336
Epoch 1336 :: Batch 0/1
Batch Loss = 3.168988671443494
1336, epoch_train_loss=3.168988671443494
Epoch 1337
Epoch 1337 :: Batch 0/1
Batch Loss = 3.115722352951717
1337, epoch_train_loss=3.115722352951717
Epoch 1338
Epoch 1338 :: Batch 0/1
Batch Loss = 3.1438328013353196
1338, epoch_train_loss=3.1438328013353196
Epoch 1339
Epoch 1339 :: Batch 0/1
Batch Loss = 3.1062681547008566
1339, epoch_train_loss=3.1062681547008566
Epoch 1340
Epoch 1340 :: Batch 0/1
Batch Loss = 3.1171781135745107
1340, epoch_train_loss=3.1171781135745107
Epoch 1341
Epoch 1341 :: Batch 0/1
Batch Loss = 3.126087708329429
1341, epoch_train_loss=3.126087708329429
Epoch 1342
Epoch 1342 :: Batch 0/1
Batch Loss = 3.0980204052962432
1342, epoch_train_loss=3.0980204052962432
Epoch 1343
Epoch 1343 :: Batch 0/1
Batch Loss = 3.1305888903112753
1343, epoch_train_loss=3.1305888903112753
Epoch 1344
Epoch 1344 :: Batch 0/1
Batch Loss = 3.0967706236689234
1344, epoch_train_loss=3.0967706236689234
Epoch 1345
Epoch 1345 :: Batch 0/1
Batch Loss = 3.1176225844406575
1345, epoch_train_loss=3.1176225844406575
Epoch 1346
Epoch 1346 :: Batch 0/1
Batch Loss = 3.100743552377515
1346, epoch_train_loss=3.100743552377515
Epoch 1347
Epoch 1347 :: Batch 0/1
Batch Loss = 3.1111910221963086
1347, epoch_train_loss=3.1111910221963086
Epoch 1348
Epoch 1348 :: Batch 0/1
Batch Loss = 3.0967233295406182
1348, epoch_train_loss=3.0967233295406182
Epoch 1349
Epoch 1349 :: Batch 0/1
Batch Loss = 3.104129167910783
1349, epoch_train_loss=3.104129167910783
Epoch 1350
Epoch 1350 :: Batch 0/1
Batch Loss = 3.098325689142902
1350, epoch_train_loss=3.098325689142902
Epoch 1351
Epoch 1351 :: Batch 0/1
Batch Loss = 3.1002909211748353
1351, epoch_train_loss=3.1002909211748353
Epoch 1352
Epoch 1352 :: Batch 0/1
Batch Loss = 3.0916096737970853
1352, epoch_train_loss=3.0916096737970853
Epoch 1353
Epoch 1353 :: Batch 0/1
Batch Loss = 3.096685143589497
1353, epoch_train_loss=3.096685143589497
Epoch 1354
Epoch 1354 :: Batch 0/1
Batch Loss = 3.089880699988813
1354, epoch_train_loss=3.089880699988813
Epoch 1355
Epoch 1355 :: Batch 0/1
Batch Loss = 3.0955678076576225
1355, epoch_train_loss=3.0955678076576225
Epoch 1356
Epoch 1356 :: Batch 0/1
Batch Loss = 3.0856933581783785
1356, epoch_train_loss=3.0856933581783785
Epoch 1357
Epoch 1357 :: Batch 0/1
Batch Loss = 3.0895722025928736
1357, epoch_train_loss=3.0895722025928736
Epoch 1358
Epoch 1358 :: Batch 0/1
Batch Loss = 3.0846071852764494
1358, epoch_train_loss=3.0846071852764494
Epoch 1359
Epoch 1359 :: Batch 0/1
Batch Loss = 3.0834020681884837
1359, epoch_train_loss=3.0834020681884837
Epoch 1360
Epoch 1360 :: Batch 0/1
Batch Loss = 3.0855081709973655
1360, epoch_train_loss=3.0855081709973655
Epoch 1361
Epoch 1361 :: Batch 0/1
Batch Loss = 3.0794994034457055
1361, epoch_train_loss=3.0794994034457055
Epoch 1362
Epoch 1362 :: Batch 0/1
Batch Loss = 3.0827857109293193
1362, epoch_train_loss=3.0827857109293193
Epoch 1363
Epoch 1363 :: Batch 0/1
Batch Loss = 3.0847574336891967
1363, epoch_train_loss=3.0847574336891967
Epoch 1364
Epoch 1364 :: Batch 0/1
Batch Loss = 3.0764281915232914
1364, epoch_train_loss=3.0764281915232914
Epoch 1365
Epoch 1365 :: Batch 0/1
Batch Loss = 3.0793434386092478
1365, epoch_train_loss=3.0793434386092478
Epoch 1366
Epoch 1366 :: Batch 0/1
Batch Loss = 3.087855818549234
1366, epoch_train_loss=3.087855818549234
Epoch 1367
Epoch 1367 :: Batch 0/1
Batch Loss = 3.0784136258162933
1367, epoch_train_loss=3.0784136258162933
Epoch 1368
Epoch 1368 :: Batch 0/1
Batch Loss = 3.072873972627749
1368, epoch_train_loss=3.072873972627749
Epoch 1369
Epoch 1369 :: Batch 0/1
Batch Loss = 3.0717377182229155
1369, epoch_train_loss=3.0717377182229155
Epoch 1370
Epoch 1370 :: Batch 0/1
Batch Loss = 3.074823446809631
1370, epoch_train_loss=3.074823446809631
Epoch 1371
Epoch 1371 :: Batch 0/1
Batch Loss = 3.0855960005895056
1371, epoch_train_loss=3.0855960005895056
Epoch 1372
Epoch 1372 :: Batch 0/1
Batch Loss = 3.093519987941648
1372, epoch_train_loss=3.093519987941648
Epoch 1373
Epoch 1373 :: Batch 0/1
Batch Loss = 3.1346410700714293
1373, epoch_train_loss=3.1346410700714293
Epoch 1374
Epoch 1374 :: Batch 0/1
Batch Loss = 3.072525118259043
1374, epoch_train_loss=3.072525118259043
Epoch 1375
Epoch 1375 :: Batch 0/1
Batch Loss = 3.0846082359128197
1375, epoch_train_loss=3.0846082359128197
Epoch 1376
Epoch 1376 :: Batch 0/1
Batch Loss = 3.1729073170853237
1376, epoch_train_loss=3.1729073170853237
Epoch 1377
Epoch 1377 :: Batch 0/1
Batch Loss = 3.072317990381326
1377, epoch_train_loss=3.072317990381326
Epoch 1378
Epoch 1378 :: Batch 0/1
Batch Loss = 3.3731686579570335
1378, epoch_train_loss=3.3731686579570335
Epoch 1379
Epoch 1379 :: Batch 0/1
Batch Loss = 3.390467258518589
1379, epoch_train_loss=3.390467258518589
Epoch 1380
Epoch 1380 :: Batch 0/1
Batch Loss = 3.54633777658401
1380, epoch_train_loss=3.54633777658401
Epoch 1381
Epoch 1381 :: Batch 0/1
Batch Loss = 3.5265274826135697
1381, epoch_train_loss=3.5265274826135697
Epoch 1382
Epoch 1382 :: Batch 0/1
Batch Loss = 3.5309336676963894
1382, epoch_train_loss=3.5309336676963894
Epoch 1383
Epoch 1383 :: Batch 0/1
Batch Loss = 3.5384351300693653
1383, epoch_train_loss=3.5384351300693653
Epoch 1384
Epoch 1384 :: Batch 0/1
Batch Loss = 3.521367573296315
1384, epoch_train_loss=3.521367573296315
Epoch 1385
Epoch 1385 :: Batch 0/1
Batch Loss = 3.486278157316588
1385, epoch_train_loss=3.486278157316588
Epoch 1386
Epoch 1386 :: Batch 0/1
Batch Loss = 3.4249029826754773
1386, epoch_train_loss=3.4249029826754773
Epoch 1387
Epoch 1387 :: Batch 0/1
Batch Loss = 3.4342449852423154
1387, epoch_train_loss=3.4342449852423154
Epoch 1388
Epoch 1388 :: Batch 0/1
Batch Loss = 3.444869967159702
1388, epoch_train_loss=3.444869967159702
Epoch 1389
Epoch 1389 :: Batch 0/1
Batch Loss = 3.3947515186319555
1389, epoch_train_loss=3.3947515186319555
Epoch 1390
Epoch 1390 :: Batch 0/1
Batch Loss = 3.368593091352289
1390, epoch_train_loss=3.368593091352289
Epoch 1391
Epoch 1391 :: Batch 0/1
Batch Loss = 3.3769667677792197
1391, epoch_train_loss=3.3769667677792197
Epoch 1392
Epoch 1392 :: Batch 0/1
Batch Loss = 3.3537019583363263
1392, epoch_train_loss=3.3537019583363263
Epoch 1393
Epoch 1393 :: Batch 0/1
Batch Loss = 3.335680611712134
1393, epoch_train_loss=3.335680611712134
Epoch 1394
Epoch 1394 :: Batch 0/1
Batch Loss = 3.301242109426777
1394, epoch_train_loss=3.301242109426777
Epoch 1395
Epoch 1395 :: Batch 0/1
Batch Loss = 3.314445013451327
1395, epoch_train_loss=3.314445013451327
Epoch 1396
Epoch 1396 :: Batch 0/1
Batch Loss = 3.2753589733048862
1396, epoch_train_loss=3.2753589733048862
Epoch 1397
Epoch 1397 :: Batch 0/1
Batch Loss = 3.273254800243555
1397, epoch_train_loss=3.273254800243555
Epoch 1398
Epoch 1398 :: Batch 0/1
Batch Loss = 3.2956313071068792
1398, epoch_train_loss=3.2956313071068792
Epoch 1399
Epoch 1399 :: Batch 0/1
Batch Loss = 3.278744634879053
1399, epoch_train_loss=3.278744634879053
Epoch 1400
Epoch 1400 :: Batch 0/1
Batch Loss = 3.2408316181656516
1400, epoch_train_loss=3.2408316181656516
Epoch 1401
Epoch 1401 :: Batch 0/1
Batch Loss = 3.2436925118224753
1401, epoch_train_loss=3.2436925118224753
Epoch 1402
Epoch 1402 :: Batch 0/1
Batch Loss = 3.2336380780905425
1402, epoch_train_loss=3.2336380780905425
Epoch 1403
Epoch 1403 :: Batch 0/1
Batch Loss = 3.1975727714557447
1403, epoch_train_loss=3.1975727714557447
Epoch 1404
Epoch 1404 :: Batch 0/1
Batch Loss = 3.1911210260685166
1404, epoch_train_loss=3.1911210260685166
Epoch 1405
Epoch 1405 :: Batch 0/1
Batch Loss = 3.180331302147794
1405, epoch_train_loss=3.180331302147794
Epoch 1406
Epoch 1406 :: Batch 0/1
Batch Loss = 3.1524165982702503
1406, epoch_train_loss=3.1524165982702503
Epoch 1407
Epoch 1407 :: Batch 0/1
Batch Loss = 3.145016160237099
1407, epoch_train_loss=3.145016160237099
Epoch 1408
Epoch 1408 :: Batch 0/1
Batch Loss = 3.1415796031125196
1408, epoch_train_loss=3.1415796031125196
Epoch 1409
Epoch 1409 :: Batch 0/1
Batch Loss = 3.137891474437223
1409, epoch_train_loss=3.137891474437223
Epoch 1410
Epoch 1410 :: Batch 0/1
Batch Loss = 3.128162971834584
1410, epoch_train_loss=3.128162971834584
Epoch 1411
Epoch 1411 :: Batch 0/1
Batch Loss = 3.1298632782898594
1411, epoch_train_loss=3.1298632782898594
Epoch 1412
Epoch 1412 :: Batch 0/1
Batch Loss = 3.1443070661740653
1412, epoch_train_loss=3.1443070661740653
Epoch 1413
Epoch 1413 :: Batch 0/1
Batch Loss = 3.2025406560619527
1413, epoch_train_loss=3.2025406560619527
Epoch 1414
Epoch 1414 :: Batch 0/1
Batch Loss = 3.322083406253259
1414, epoch_train_loss=3.322083406253259
Epoch 1415
Epoch 1415 :: Batch 0/1
Batch Loss = 3.4456480189805947
1415, epoch_train_loss=3.4456480189805947
Epoch 1416
Epoch 1416 :: Batch 0/1
Batch Loss = 3.1260365265459407
1416, epoch_train_loss=3.1260365265459407
Epoch 1417
Epoch 1417 :: Batch 0/1
Batch Loss = 3.390877045501431
1417, epoch_train_loss=3.390877045501431
Epoch 1418
Epoch 1418 :: Batch 0/1
Batch Loss = 3.137160553932848
1418, epoch_train_loss=3.137160553932848
Epoch 1419
Epoch 1419 :: Batch 0/1
Batch Loss = 3.2172991997421754
1419, epoch_train_loss=3.2172991997421754
Epoch 1420
Epoch 1420 :: Batch 0/1
Batch Loss = 3.2290723623026443
1420, epoch_train_loss=3.2290723623026443
Epoch 1421
Epoch 1421 :: Batch 0/1
Batch Loss = 3.248416893991707
1421, epoch_train_loss=3.248416893991707
Epoch 1422
Epoch 1422 :: Batch 0/1
Batch Loss = 3.2163969444283405
1422, epoch_train_loss=3.2163969444283405
Epoch 1423
Epoch 1423 :: Batch 0/1
Batch Loss = 3.182548268464173
1423, epoch_train_loss=3.182548268464173
Epoch 1424
Epoch 1424 :: Batch 0/1
Batch Loss = 3.203012785023465
1424, epoch_train_loss=3.203012785023465
Epoch 1425
Epoch 1425 :: Batch 0/1
Batch Loss = 3.1999407337602537
1425, epoch_train_loss=3.1999407337602537
Epoch 1426
Epoch 1426 :: Batch 0/1
Batch Loss = 3.154230217998265
1426, epoch_train_loss=3.154230217998265
Epoch 1427
Epoch 1427 :: Batch 0/1
Batch Loss = 3.137416081382031
1427, epoch_train_loss=3.137416081382031
Epoch 1428
Epoch 1428 :: Batch 0/1
Batch Loss = 3.147871140664414
1428, epoch_train_loss=3.147871140664414
Epoch 1429
Epoch 1429 :: Batch 0/1
Batch Loss = 3.155475570693611
1429, epoch_train_loss=3.155475570693611
Epoch 1430
Epoch 1430 :: Batch 0/1
Batch Loss = 3.143637408709453
1430, epoch_train_loss=3.143637408709453
Epoch 1431
Epoch 1431 :: Batch 0/1
Batch Loss = 3.1286666348348127
1431, epoch_train_loss=3.1286666348348127
Epoch 1432
Epoch 1432 :: Batch 0/1
Batch Loss = 3.1253437243544924
1432, epoch_train_loss=3.1253437243544924
Epoch 1433
Epoch 1433 :: Batch 0/1
Batch Loss = 3.1333929752789746
1433, epoch_train_loss=3.1333929752789746
Epoch 1434
Epoch 1434 :: Batch 0/1
Batch Loss = 3.1351628073098783
1434, epoch_train_loss=3.1351628073098783
Epoch 1435
Epoch 1435 :: Batch 0/1
Batch Loss = 3.123080712621569
1435, epoch_train_loss=3.123080712621569
Epoch 1436
Epoch 1436 :: Batch 0/1
Batch Loss = 3.119432103280137
1436, epoch_train_loss=3.119432103280137
Epoch 1437
Epoch 1437 :: Batch 0/1
Batch Loss = 3.125978055361063
1437, epoch_train_loss=3.125978055361063
Epoch 1438
Epoch 1438 :: Batch 0/1
Batch Loss = 3.1280519500406294
1438, epoch_train_loss=3.1280519500406294
Epoch 1439
Epoch 1439 :: Batch 0/1
Batch Loss = 3.1199675156113615
1439, epoch_train_loss=3.1199675156113615
Epoch 1440
Epoch 1440 :: Batch 0/1
Batch Loss = 3.114814289907641
1440, epoch_train_loss=3.114814289907641
Epoch 1441
Epoch 1441 :: Batch 0/1
Batch Loss = 3.1185846478378605
1441, epoch_train_loss=3.1185846478378605
Epoch 1442
Epoch 1442 :: Batch 0/1
Batch Loss = 3.1213574454990094
1442, epoch_train_loss=3.1213574454990094
Epoch 1443
Epoch 1443 :: Batch 0/1
Batch Loss = 3.118393006664902
1443, epoch_train_loss=3.118393006664902
Epoch 1444
Epoch 1444 :: Batch 0/1
Batch Loss = 3.1154458280098782
1444, epoch_train_loss=3.1154458280098782
Epoch 1445
Epoch 1445 :: Batch 0/1
Batch Loss = 3.115955325212527
1445, epoch_train_loss=3.115955325212527
Epoch 1446
Epoch 1446 :: Batch 0/1
Batch Loss = 3.1163856980442994
1446, epoch_train_loss=3.1163856980442994
Epoch 1447
Epoch 1447 :: Batch 0/1
Batch Loss = 3.115879372070465
1447, epoch_train_loss=3.115879372070465
Epoch 1448
Epoch 1448 :: Batch 0/1
Batch Loss = 3.1156529845075602
1448, epoch_train_loss=3.1156529845075602
Epoch 1449
Epoch 1449 :: Batch 0/1
Batch Loss = 3.1135544743263903
1449, epoch_train_loss=3.1135544743263903
Epoch 1450
Epoch 1450 :: Batch 0/1
Batch Loss = 3.11293520484284
1450, epoch_train_loss=3.11293520484284
Epoch 1451
Epoch 1451 :: Batch 0/1
Batch Loss = 3.113836821038838
1451, epoch_train_loss=3.113836821038838
Epoch 1452
Epoch 1452 :: Batch 0/1
Batch Loss = 3.114362025212679
1452, epoch_train_loss=3.114362025212679
Epoch 1453
Epoch 1453 :: Batch 0/1
Batch Loss = 3.1131061496418053
1453, epoch_train_loss=3.1131061496418053
Epoch 1454
Epoch 1454 :: Batch 0/1
Batch Loss = 3.1115654449160486
1454, epoch_train_loss=3.1115654449160486
Epoch 1455
Epoch 1455 :: Batch 0/1
Batch Loss = 3.1117608455433445
1455, epoch_train_loss=3.1117608455433445
Epoch 1456
Epoch 1456 :: Batch 0/1
Batch Loss = 3.112569654656542
1456, epoch_train_loss=3.112569654656542
Epoch 1457
Epoch 1457 :: Batch 0/1
Batch Loss = 3.111710685928824
1457, epoch_train_loss=3.111710685928824
Epoch 1458
Epoch 1458 :: Batch 0/1
Batch Loss = 3.1110405671928443
1458, epoch_train_loss=3.1110405671928443
Epoch 1459
Epoch 1459 :: Batch 0/1
Batch Loss = 3.111105812150824
1459, epoch_train_loss=3.111105812150824
Epoch 1460
Epoch 1460 :: Batch 0/1
Batch Loss = 3.1109173927600686
1460, epoch_train_loss=3.1109173927600686
Epoch 1461
Epoch 1461 :: Batch 0/1
Batch Loss = 3.1107279578758273
1461, epoch_train_loss=3.1107279578758273
Epoch 1462
Epoch 1462 :: Batch 0/1
Batch Loss = 3.110533755091962
1462, epoch_train_loss=3.110533755091962
Epoch 1463
Epoch 1463 :: Batch 0/1
Batch Loss = 3.1100590939191353
1463, epoch_train_loss=3.1100590939191353
Epoch 1464
Epoch 1464 :: Batch 0/1
Batch Loss = 3.109821222822004
1464, epoch_train_loss=3.109821222822004
Epoch 1465
Epoch 1465 :: Batch 0/1
Batch Loss = 3.1100472181028107
1465, epoch_train_loss=3.1100472181028107
Epoch 1466
Epoch 1466 :: Batch 0/1
Batch Loss = 3.109933567720143
1466, epoch_train_loss=3.109933567720143
Epoch 1467
Epoch 1467 :: Batch 0/1
Batch Loss = 3.109359687961638
1467, epoch_train_loss=3.109359687961638
Epoch 1468
Epoch 1468 :: Batch 0/1
Batch Loss = 3.109196123776468
1468, epoch_train_loss=3.109196123776468
Epoch 1469
Epoch 1469 :: Batch 0/1
Batch Loss = 3.1093336581923103
1469, epoch_train_loss=3.1093336581923103
Epoch 1470
Epoch 1470 :: Batch 0/1
Batch Loss = 3.109174399006109
1470, epoch_train_loss=3.109174399006109
Epoch 1471
Epoch 1471 :: Batch 0/1
Batch Loss = 3.1088512545185294
1471, epoch_train_loss=3.1088512545185294
Epoch 1472
Epoch 1472 :: Batch 0/1
Batch Loss = 3.108658669470896
1472, epoch_train_loss=3.108658669470896
Epoch 1473
Epoch 1473 :: Batch 0/1
Batch Loss = 3.1085545486899573
1473, epoch_train_loss=3.1085545486899573
Epoch 1474
Epoch 1474 :: Batch 0/1
Batch Loss = 3.1084526971384374
1474, epoch_train_loss=3.1084526971384374
Epoch 1475
Epoch 1475 :: Batch 0/1
Batch Loss = 3.108366332075509
1475, epoch_train_loss=3.108366332075509
Epoch 1476
Epoch 1476 :: Batch 0/1
Batch Loss = 3.108137860702353
1476, epoch_train_loss=3.108137860702353
Epoch 1477
Epoch 1477 :: Batch 0/1
Batch Loss = 3.107923660590828
1477, epoch_train_loss=3.107923660590828
Epoch 1478
Epoch 1478 :: Batch 0/1
Batch Loss = 3.1078717902731388
1478, epoch_train_loss=3.1078717902731388
Epoch 1479
Epoch 1479 :: Batch 0/1
Batch Loss = 3.1078204717439117
1479, epoch_train_loss=3.1078204717439117
Epoch 1480
Epoch 1480 :: Batch 0/1
Batch Loss = 3.1076322562607395
1480, epoch_train_loss=3.1076322562607395
Epoch 1481
Epoch 1481 :: Batch 0/1
Batch Loss = 3.1074419483000066
1481, epoch_train_loss=3.1074419483000066
Epoch 1482
Epoch 1482 :: Batch 0/1
Batch Loss = 3.1073575646900053
1482, epoch_train_loss=3.1073575646900053
Epoch 1483
Epoch 1483 :: Batch 0/1
Batch Loss = 3.1072517645725193
1483, epoch_train_loss=3.1072517645725193
Epoch 1484
Epoch 1484 :: Batch 0/1
Batch Loss = 3.1071059155983978
1484, epoch_train_loss=3.1071059155983978
Epoch 1485
Epoch 1485 :: Batch 0/1
Batch Loss = 3.106978488684413
1485, epoch_train_loss=3.106978488684413
Epoch 1486
Epoch 1486 :: Batch 0/1
Batch Loss = 3.1068548096761366
1486, epoch_train_loss=3.1068548096761366
Epoch 1487
Epoch 1487 :: Batch 0/1
Batch Loss = 3.106726942747823
1487, epoch_train_loss=3.106726942747823
Epoch 1488
Epoch 1488 :: Batch 0/1
Batch Loss = 3.1066235308859524
1488, epoch_train_loss=3.1066235308859524
Epoch 1489
Epoch 1489 :: Batch 0/1
Batch Loss = 3.1065057174045725
1489, epoch_train_loss=3.1065057174045725
Epoch 1490
Epoch 1490 :: Batch 0/1
Batch Loss = 3.106363281536054
1490, epoch_train_loss=3.106363281536054
Epoch 1491
Epoch 1491 :: Batch 0/1
Batch Loss = 3.106260913866945
1491, epoch_train_loss=3.106260913866945
Epoch 1492
Epoch 1492 :: Batch 0/1
Batch Loss = 3.106173198804355
1492, epoch_train_loss=3.106173198804355
Epoch 1493
Epoch 1493 :: Batch 0/1
Batch Loss = 3.106048408494483
1493, epoch_train_loss=3.106048408494483
Epoch 1494
Epoch 1494 :: Batch 0/1
Batch Loss = 3.1059193843588013
1494, epoch_train_loss=3.1059193843588013
Epoch 1495
Epoch 1495 :: Batch 0/1
Batch Loss = 3.1058199155863777
1495, epoch_train_loss=3.1058199155863777
Epoch 1496
Epoch 1496 :: Batch 0/1
Batch Loss = 3.1057170054786036
1496, epoch_train_loss=3.1057170054786036
Epoch 1497
Epoch 1497 :: Batch 0/1
Batch Loss = 3.1056057804872266
1497, epoch_train_loss=3.1056057804872266
Epoch 1498
Epoch 1498 :: Batch 0/1
Batch Loss = 3.105497387356423
1498, epoch_train_loss=3.105497387356423
Epoch 1499
Epoch 1499 :: Batch 0/1
Batch Loss = 3.105385534027718
1499, epoch_train_loss=3.105385534027718
Epoch 1500
Epoch 1500 :: Batch 0/1
Batch Loss = 3.105281400323509
1500, epoch_train_loss=3.105281400323509
Epoch 1501
Epoch 1501 :: Batch 0/1
Batch Loss = 3.105188113339887
1501, epoch_train_loss=3.105188113339887
Epoch 1502
Epoch 1502 :: Batch 0/1
Batch Loss = 3.105085028175471
1502, epoch_train_loss=3.105085028175471
Epoch 1503
Epoch 1503 :: Batch 0/1
Batch Loss = 3.1049734085916323
1503, epoch_train_loss=3.1049734085916323
Epoch 1504
Epoch 1504 :: Batch 0/1
Batch Loss = 3.1048756150812475
1504, epoch_train_loss=3.1048756150812475
Epoch 1505
Epoch 1505 :: Batch 0/1
Batch Loss = 3.1047823592431185
1505, epoch_train_loss=3.1047823592431185
Epoch 1506
Epoch 1506 :: Batch 0/1
Batch Loss = 3.1046825184763125
1506, epoch_train_loss=3.1046825184763125
Epoch 1507
Epoch 1507 :: Batch 0/1
Batch Loss = 3.104582972368348
1507, epoch_train_loss=3.104582972368348
Epoch 1508
Epoch 1508 :: Batch 0/1
Batch Loss = 3.1044870724199276
1508, epoch_train_loss=3.1044870724199276
Epoch 1509
Epoch 1509 :: Batch 0/1
Batch Loss = 3.104391147756912
1509, epoch_train_loss=3.104391147756912
Epoch 1510
Epoch 1510 :: Batch 0/1
Batch Loss = 3.104297189894711
1510, epoch_train_loss=3.104297189894711
Epoch 1511
Epoch 1511 :: Batch 0/1
Batch Loss = 3.1042043084942357
1511, epoch_train_loss=3.1042043084942357
Epoch 1512
Epoch 1512 :: Batch 0/1
Batch Loss = 3.1041091597764794
1512, epoch_train_loss=3.1041091597764794
Epoch 1513
Epoch 1513 :: Batch 0/1
Batch Loss = 3.1040174414005195
1513, epoch_train_loss=3.1040174414005195
Epoch 1514
Epoch 1514 :: Batch 0/1
Batch Loss = 3.1039291368771793
1514, epoch_train_loss=3.1039291368771793
Epoch 1515
Epoch 1515 :: Batch 0/1
Batch Loss = 3.1038381305239984
1515, epoch_train_loss=3.1038381305239984
Epoch 1516
Epoch 1516 :: Batch 0/1
Batch Loss = 3.1037462653442525
1516, epoch_train_loss=3.1037462653442525
Epoch 1517
Epoch 1517 :: Batch 0/1
Batch Loss = 3.1036584028965404
1517, epoch_train_loss=3.1036584028965404
Epoch 1518
Epoch 1518 :: Batch 0/1
Batch Loss = 3.1035712223632164
1518, epoch_train_loss=3.1035712223632164
Epoch 1519
Epoch 1519 :: Batch 0/1
Batch Loss = 3.103483393243999
1519, epoch_train_loss=3.103483393243999
Epoch 1520
Epoch 1520 :: Batch 0/1
Batch Loss = 3.1033964366163724
1520, epoch_train_loss=3.1033964366163724
Epoch 1521
Epoch 1521 :: Batch 0/1
Batch Loss = 3.1033097672576955
1521, epoch_train_loss=3.1033097672576955
Epoch 1522
Epoch 1522 :: Batch 0/1
Batch Loss = 3.1032243011084133
1522, epoch_train_loss=3.1032243011084133
Epoch 1523
Epoch 1523 :: Batch 0/1
Batch Loss = 3.1031404989177185
1523, epoch_train_loss=3.1031404989177185
Epoch 1524
Epoch 1524 :: Batch 0/1
Batch Loss = 3.1030561550013913
1524, epoch_train_loss=3.1030561550013913
Epoch 1525
Epoch 1525 :: Batch 0/1
Batch Loss = 3.1029716111210535
1525, epoch_train_loss=3.1029716111210535
Epoch 1526
Epoch 1526 :: Batch 0/1
Batch Loss = 3.1028890410726646
1526, epoch_train_loss=3.1028890410726646
Epoch 1527
Epoch 1527 :: Batch 0/1
Batch Loss = 3.10280708933798
1527, epoch_train_loss=3.10280708933798
Epoch 1528
Epoch 1528 :: Batch 0/1
Batch Loss = 3.1027249307032014
1528, epoch_train_loss=3.1027249307032014
Epoch 1529
Epoch 1529 :: Batch 0/1
Batch Loss = 3.1026434698308116
1529, epoch_train_loss=3.1026434698308116
Epoch 1530
Epoch 1530 :: Batch 0/1
Batch Loss = 3.102562811751706
1530, epoch_train_loss=3.102562811751706
Epoch 1531
Epoch 1531 :: Batch 0/1
Batch Loss = 3.1024826540838757
1531, epoch_train_loss=3.1024826540838757
Epoch 1532
Epoch 1532 :: Batch 0/1
Batch Loss = 3.102403268305739
1532, epoch_train_loss=3.102403268305739
Epoch 1533
Epoch 1533 :: Batch 0/1
Batch Loss = 3.1023241976031937
1533, epoch_train_loss=3.1023241976031937
Epoch 1534
Epoch 1534 :: Batch 0/1
Batch Loss = 3.102245493450789
1534, epoch_train_loss=3.102245493450789
Epoch 1535
Epoch 1535 :: Batch 0/1
Batch Loss = 3.1021677902089033
1535, epoch_train_loss=3.1021677902089033
Epoch 1536
Epoch 1536 :: Batch 0/1
Batch Loss = 3.1020906992058137
1536, epoch_train_loss=3.1020906992058137
Epoch 1537
Epoch 1537 :: Batch 0/1
Batch Loss = 3.102013775231685
1537, epoch_train_loss=3.102013775231685
Epoch 1538
Epoch 1538 :: Batch 0/1
Batch Loss = 3.1019375594732304
1538, epoch_train_loss=3.1019375594732304
Epoch 1539
Epoch 1539 :: Batch 0/1
Batch Loss = 3.10186214892639
1539, epoch_train_loss=3.10186214892639
Epoch 1540
Epoch 1540 :: Batch 0/1
Batch Loss = 3.1017871670629797
1540, epoch_train_loss=3.1017871670629797
Epoch 1541
Epoch 1541 :: Batch 0/1
Batch Loss = 3.1017127590418023
1541, epoch_train_loss=3.1017127590418023
Epoch 1542
Epoch 1542 :: Batch 0/1
Batch Loss = 3.101638900399273
1542, epoch_train_loss=3.101638900399273
Epoch 1543
Epoch 1543 :: Batch 0/1
Batch Loss = 3.101565608197864
1543, epoch_train_loss=3.101565608197864
Epoch 1544
Epoch 1544 :: Batch 0/1
Batch Loss = 3.1014930671020062
1544, epoch_train_loss=3.1014930671020062
Epoch 1545
Epoch 1545 :: Batch 0/1
Batch Loss = 3.101421074505603
1545, epoch_train_loss=3.101421074505603
Epoch 1546
Epoch 1546 :: Batch 0/1
Batch Loss = 3.1013494895222067
1546, epoch_train_loss=3.1013494895222067
Epoch 1547
Epoch 1547 :: Batch 0/1
Batch Loss = 3.101278609691417
1547, epoch_train_loss=3.101278609691417
Epoch 1548
Epoch 1548 :: Batch 0/1
Batch Loss = 3.1012083739520504
1548, epoch_train_loss=3.1012083739520504
Epoch 1549
Epoch 1549 :: Batch 0/1
Batch Loss = 3.1011386473371565
1549, epoch_train_loss=3.1011386473371565
Epoch 1550
Epoch 1550 :: Batch 0/1
Batch Loss = 3.1010695026607493
1550, epoch_train_loss=3.1010695026607493
Epoch 1551
Epoch 1551 :: Batch 0/1
Batch Loss = 3.1010009449321094
1551, epoch_train_loss=3.1010009449321094
Epoch 1552
Epoch 1552 :: Batch 0/1
Batch Loss = 3.1009329590281203
1552, epoch_train_loss=3.1009329590281203
Epoch 1553
Epoch 1553 :: Batch 0/1
Batch Loss = 3.1008655944141013
1553, epoch_train_loss=3.1008655944141013
Epoch 1554
Epoch 1554 :: Batch 0/1
Batch Loss = 3.1007987627682407
1554, epoch_train_loss=3.1007987627682407
Epoch 1555
Epoch 1555 :: Batch 0/1
Batch Loss = 3.10073246346299
1555, epoch_train_loss=3.10073246346299
Epoch 1556
Epoch 1556 :: Batch 0/1
Batch Loss = 3.10066677439428
1556, epoch_train_loss=3.10066677439428
Epoch 1557
Epoch 1557 :: Batch 0/1
Batch Loss = 3.1006016341260385
1557, epoch_train_loss=3.1006016341260385
Epoch 1558
Epoch 1558 :: Batch 0/1
Batch Loss = 3.1005369903260434
1558, epoch_train_loss=3.1005369903260434
Epoch 1559
Epoch 1559 :: Batch 0/1
Batch Loss = 3.1004728988239125
1559, epoch_train_loss=3.1004728988239125
Epoch 1560
Epoch 1560 :: Batch 0/1
Batch Loss = 3.1004093439818554
1560, epoch_train_loss=3.1004093439818554
Epoch 1561
Epoch 1561 :: Batch 0/1
Batch Loss = 3.100346305729125
1561, epoch_train_loss=3.100346305729125
Epoch 1562
Epoch 1562 :: Batch 0/1
Batch Loss = 3.1002837826992873
1562, epoch_train_loss=3.1002837826992873
Epoch 1563
Epoch 1563 :: Batch 0/1
Batch Loss = 3.1002217375583663
1563, epoch_train_loss=3.1002217375583663
Epoch 1564
Epoch 1564 :: Batch 0/1
Batch Loss = 3.1001601852716254
1564, epoch_train_loss=3.1001601852716254
Epoch 1565
Epoch 1565 :: Batch 0/1
Batch Loss = 3.100099136954246
1565, epoch_train_loss=3.100099136954246
Epoch 1566
Epoch 1566 :: Batch 0/1
Batch Loss = 3.1000385404163238
1566, epoch_train_loss=3.1000385404163238
Epoch 1567
Epoch 1567 :: Batch 0/1
Batch Loss = 3.0999783920190214
1567, epoch_train_loss=3.0999783920190214
Epoch 1568
Epoch 1568 :: Batch 0/1
Batch Loss = 3.09991870263403
1568, epoch_train_loss=3.09991870263403
Epoch 1569
Epoch 1569 :: Batch 0/1
Batch Loss = 3.099859450364652
1569, epoch_train_loss=3.099859450364652
Epoch 1570
Epoch 1570 :: Batch 0/1
Batch Loss = 3.099800630147752
1570, epoch_train_loss=3.099800630147752
Epoch 1571
Epoch 1571 :: Batch 0/1
Batch Loss = 3.0997422308485167
1571, epoch_train_loss=3.0997422308485167
Epoch 1572
Epoch 1572 :: Batch 0/1
Batch Loss = 3.099684236789982
1572, epoch_train_loss=3.099684236789982
Epoch 1573
Epoch 1573 :: Batch 0/1
Batch Loss = 3.0996266501457352
1573, epoch_train_loss=3.0996266501457352
Epoch 1574
Epoch 1574 :: Batch 0/1
Batch Loss = 3.099569457024937
1574, epoch_train_loss=3.099569457024937
Epoch 1575
Epoch 1575 :: Batch 0/1
Batch Loss = 3.0995126395470476
1575, epoch_train_loss=3.0995126395470476
Epoch 1576
Epoch 1576 :: Batch 0/1
Batch Loss = 3.0994562010213667
1576, epoch_train_loss=3.0994562010213667
Epoch 1577
Epoch 1577 :: Batch 0/1
Batch Loss = 3.099400133526123
1577, epoch_train_loss=3.099400133526123
Epoch 1578
Epoch 1578 :: Batch 0/1
Batch Loss = 3.0993444219479143
1578, epoch_train_loss=3.0993444219479143
Epoch 1579
Epoch 1579 :: Batch 0/1
Batch Loss = 3.099289062703547
1579, epoch_train_loss=3.099289062703547
Epoch 1580
Epoch 1580 :: Batch 0/1
Batch Loss = 3.099234047377035
1580, epoch_train_loss=3.099234047377035
Epoch 1581
Epoch 1581 :: Batch 0/1
Batch Loss = 3.0991793685831124
1581, epoch_train_loss=3.0991793685831124
Epoch 1582
Epoch 1582 :: Batch 0/1
Batch Loss = 3.0991250199059404
1582, epoch_train_loss=3.0991250199059404
Epoch 1583
Epoch 1583 :: Batch 0/1
Batch Loss = 3.099070985566431
1583, epoch_train_loss=3.099070985566431
Epoch 1584
Epoch 1584 :: Batch 0/1
Batch Loss = 3.0990172584479354
1584, epoch_train_loss=3.0990172584479354
Epoch 1585
Epoch 1585 :: Batch 0/1
Batch Loss = 3.0989638330975766
1585, epoch_train_loss=3.0989638330975766
Epoch 1586
Epoch 1586 :: Batch 0/1
Batch Loss = 3.0989106941461633
1586, epoch_train_loss=3.0989106941461633
Epoch 1587
Epoch 1587 :: Batch 0/1
Batch Loss = 3.098857829052725
1587, epoch_train_loss=3.098857829052725
Epoch 1588
Epoch 1588 :: Batch 0/1
Batch Loss = 3.09880522458114
1588, epoch_train_loss=3.09880522458114
Epoch 1589
Epoch 1589 :: Batch 0/1
Batch Loss = 3.0987528650562215
1589, epoch_train_loss=3.0987528650562215
Epoch 1590
Epoch 1590 :: Batch 0/1
Batch Loss = 3.0987007356036145
1590, epoch_train_loss=3.0987007356036145
Epoch 1591
Epoch 1591 :: Batch 0/1
Batch Loss = 3.0986488170180726
1591, epoch_train_loss=3.0986488170180726
Epoch 1592
Epoch 1592 :: Batch 0/1
Batch Loss = 3.0985970897899486
1592, epoch_train_loss=3.0985970897899486
Epoch 1593
Epoch 1593 :: Batch 0/1
Batch Loss = 3.098545537130741
1593, epoch_train_loss=3.098545537130741
Epoch 1594
Epoch 1594 :: Batch 0/1
Batch Loss = 3.0984941383575597
1594, epoch_train_loss=3.0984941383575597
Epoch 1595
Epoch 1595 :: Batch 0/1
Batch Loss = 3.0984428721098976
1595, epoch_train_loss=3.0984428721098976
Epoch 1596
Epoch 1596 :: Batch 0/1
Batch Loss = 3.0983917181708893
1596, epoch_train_loss=3.0983917181708893
Epoch 1597
Epoch 1597 :: Batch 0/1
Batch Loss = 3.0983406545662135
1597, epoch_train_loss=3.0983406545662135
Epoch 1598
Epoch 1598 :: Batch 0/1
Batch Loss = 3.0982896598121545
1598, epoch_train_loss=3.0982896598121545
Epoch 1599
Epoch 1599 :: Batch 0/1
Batch Loss = 3.0982387127857858
1599, epoch_train_loss=3.0982387127857858
Epoch 1600
Epoch 1600 :: Batch 0/1
Batch Loss = 3.0981877921185843
1600, epoch_train_loss=3.0981877921185843
Epoch 1601
Epoch 1601 :: Batch 0/1
Batch Loss = 3.098136878775749
1601, epoch_train_loss=3.098136878775749
Epoch 1602
Epoch 1602 :: Batch 0/1
Batch Loss = 3.0980859528263736
1602, epoch_train_loss=3.0980859528263736
Epoch 1603
Epoch 1603 :: Batch 0/1
Batch Loss = 3.098034994507215
1603, epoch_train_loss=3.098034994507215
Epoch 1604
Epoch 1604 :: Batch 0/1
Batch Loss = 3.097983986196485
1604, epoch_train_loss=3.097983986196485
Epoch 1605
Epoch 1605 :: Batch 0/1
Batch Loss = 3.097932910561821
1605, epoch_train_loss=3.097932910561821
Epoch 1606
Epoch 1606 :: Batch 0/1
Batch Loss = 3.0978817508058305
1606, epoch_train_loss=3.0978817508058305
Epoch 1607
Epoch 1607 :: Batch 0/1
Batch Loss = 3.0978304910152548
1607, epoch_train_loss=3.0978304910152548
Epoch 1608
Epoch 1608 :: Batch 0/1
Batch Loss = 3.0977791160164805
1608, epoch_train_loss=3.0977791160164805
Epoch 1609
Epoch 1609 :: Batch 0/1
Batch Loss = 3.0977276124774926
1609, epoch_train_loss=3.0977276124774926
Epoch 1610
Epoch 1610 :: Batch 0/1
Batch Loss = 3.0976759675633696
1610, epoch_train_loss=3.0976759675633696
Epoch 1611
Epoch 1611 :: Batch 0/1
Batch Loss = 3.0976241686755204
1611, epoch_train_loss=3.0976241686755204
Epoch 1612
Epoch 1612 :: Batch 0/1
Batch Loss = 3.0975722044558216
1612, epoch_train_loss=3.0975722044558216
Epoch 1613
Epoch 1613 :: Batch 0/1
Batch Loss = 3.097520064099003
1613, epoch_train_loss=3.097520064099003
Epoch 1614
Epoch 1614 :: Batch 0/1
Batch Loss = 3.0974677372897386
1614, epoch_train_loss=3.0974677372897386
Epoch 1615
Epoch 1615 :: Batch 0/1
Batch Loss = 3.0974152145888425
1615, epoch_train_loss=3.0974152145888425
Epoch 1616
Epoch 1616 :: Batch 0/1
Batch Loss = 3.097362487104453
1616, epoch_train_loss=3.097362487104453
Epoch 1617
Epoch 1617 :: Batch 0/1
Batch Loss = 3.0973095467181926
1617, epoch_train_loss=3.0973095467181926
Epoch 1618
Epoch 1618 :: Batch 0/1
Batch Loss = 3.0972563856418116
1618, epoch_train_loss=3.0972563856418116
Epoch 1619
Epoch 1619 :: Batch 0/1
Batch Loss = 3.097202996340041
1619, epoch_train_loss=3.097202996340041
Epoch 1620
Epoch 1620 :: Batch 0/1
Batch Loss = 3.0971493721482624
1620, epoch_train_loss=3.0971493721482624
Epoch 1621
Epoch 1621 :: Batch 0/1
Batch Loss = 3.0970955066748123
1621, epoch_train_loss=3.0970955066748123
Epoch 1622
Epoch 1622 :: Batch 0/1
Batch Loss = 3.0970413937634733
1622, epoch_train_loss=3.0970413937634733
Epoch 1623
Epoch 1623 :: Batch 0/1
Batch Loss = 3.0969870277076788
1623, epoch_train_loss=3.0969870277076788
Epoch 1624
Epoch 1624 :: Batch 0/1
Batch Loss = 3.096932403176457
1624, epoch_train_loss=3.096932403176457
Epoch 1625
Epoch 1625 :: Batch 0/1
Batch Loss = 3.096877515330041
1625, epoch_train_loss=3.096877515330041
Epoch 1626
Epoch 1626 :: Batch 0/1
Batch Loss = 3.096822359605318
1626, epoch_train_loss=3.096822359605318
Epoch 1627
Epoch 1627 :: Batch 0/1
Batch Loss = 3.096766931755758
1627, epoch_train_loss=3.096766931755758
Epoch 1628
Epoch 1628 :: Batch 0/1
Batch Loss = 3.0967112279523312
1628, epoch_train_loss=3.0967112279523312
Epoch 1629
Epoch 1629 :: Batch 0/1
Batch Loss = 3.0966552445735944
1629, epoch_train_loss=3.0966552445735944
Epoch 1630
Epoch 1630 :: Batch 0/1
Batch Loss = 3.0965989783377
1630, epoch_train_loss=3.0965989783377
Epoch 1631
Epoch 1631 :: Batch 0/1
Batch Loss = 3.096542426469672
1631, epoch_train_loss=3.096542426469672
Epoch 1632
Epoch 1632 :: Batch 0/1
Batch Loss = 3.096485586530567
1632, epoch_train_loss=3.096485586530567
Epoch 1633
Epoch 1633 :: Batch 0/1
Batch Loss = 3.0964284564140026
1633, epoch_train_loss=3.0964284564140026
Epoch 1634
Epoch 1634 :: Batch 0/1
Batch Loss = 3.0963710343082953
1634, epoch_train_loss=3.0963710343082953
Epoch 1635
Epoch 1635 :: Batch 0/1
Batch Loss = 3.0963133188128364
1635, epoch_train_loss=3.0963133188128364
Epoch 1636
Epoch 1636 :: Batch 0/1
Batch Loss = 3.09625530895701
1636, epoch_train_loss=3.09625530895701
Epoch 1637
Epoch 1637 :: Batch 0/1
Batch Loss = 3.0961970040813522
1637, epoch_train_loss=3.0961970040813522
Epoch 1638
Epoch 1638 :: Batch 0/1
Batch Loss = 3.096138403897405
1638, epoch_train_loss=3.096138403897405
Epoch 1639
Epoch 1639 :: Batch 0/1
Batch Loss = 3.0960795084821093
1639, epoch_train_loss=3.0960795084821093
Epoch 1640
Epoch 1640 :: Batch 0/1
Batch Loss = 3.0960203182736326
1640, epoch_train_loss=3.0960203182736326
Epoch 1641
Epoch 1641 :: Batch 0/1
Batch Loss = 3.095960834136903
1641, epoch_train_loss=3.095960834136903
Epoch 1642
Epoch 1642 :: Batch 0/1
Batch Loss = 3.0959010572903325
1642, epoch_train_loss=3.0959010572903325
Epoch 1643
Epoch 1643 :: Batch 0/1
Batch Loss = 3.0958409892738517
1643, epoch_train_loss=3.0958409892738517
Epoch 1644
Epoch 1644 :: Batch 0/1
Batch Loss = 3.0957806319397863
1644, epoch_train_loss=3.0957806319397863
Epoch 1645
Epoch 1645 :: Batch 0/1
Batch Loss = 3.095719987481746
1645, epoch_train_loss=3.095719987481746
Epoch 1646
Epoch 1646 :: Batch 0/1
Batch Loss = 3.095659058468909
1646, epoch_train_loss=3.095659058468909
Epoch 1647
Epoch 1647 :: Batch 0/1
Batch Loss = 3.0955978477835386
1647, epoch_train_loss=3.0955978477835386
Epoch 1648
Epoch 1648 :: Batch 0/1
Batch Loss = 3.09553635860555
1648, epoch_train_loss=3.09553635860555
Epoch 1649
Epoch 1649 :: Batch 0/1
Batch Loss = 3.0954745943931683
1649, epoch_train_loss=3.0954745943931683
Epoch 1650
Epoch 1650 :: Batch 0/1
Batch Loss = 3.0954125588583823
1650, epoch_train_loss=3.0954125588583823
Epoch 1651
Epoch 1651 :: Batch 0/1
Batch Loss = 3.0953502559707275
1651, epoch_train_loss=3.0953502559707275
Epoch 1652
Epoch 1652 :: Batch 0/1
Batch Loss = 3.095287689951567
1652, epoch_train_loss=3.095287689951567
Epoch 1653
Epoch 1653 :: Batch 0/1
Batch Loss = 3.095224865201967
1653, epoch_train_loss=3.095224865201967
Epoch 1654
Epoch 1654 :: Batch 0/1
Batch Loss = 3.0951617863110568
1654, epoch_train_loss=3.0951617863110568
Epoch 1655
Epoch 1655 :: Batch 0/1
Batch Loss = 3.0950984580430068
1655, epoch_train_loss=3.0950984580430068
Epoch 1656
Epoch 1656 :: Batch 0/1
Batch Loss = 3.0950348853565894
1656, epoch_train_loss=3.0950348853565894
Epoch 1657
Epoch 1657 :: Batch 0/1
Batch Loss = 3.0949710733364255
1657, epoch_train_loss=3.0949710733364255
Epoch 1658
Epoch 1658 :: Batch 0/1
Batch Loss = 3.0949070271626598
1658, epoch_train_loss=3.0949070271626598
Epoch 1659
Epoch 1659 :: Batch 0/1
Batch Loss = 3.0948427520875224
1659, epoch_train_loss=3.0948427520875224
Epoch 1660
Epoch 1660 :: Batch 0/1
Batch Loss = 3.094778253418607
1660, epoch_train_loss=3.094778253418607
Epoch 1661
Epoch 1661 :: Batch 0/1
Batch Loss = 3.094713536521299
1661, epoch_train_loss=3.094713536521299
Epoch 1662
Epoch 1662 :: Batch 0/1
Batch Loss = 3.094648606777184
1662, epoch_train_loss=3.094648606777184
Epoch 1663
Epoch 1663 :: Batch 0/1
Batch Loss = 3.094583469581516
1663, epoch_train_loss=3.094583469581516
Epoch 1664
Epoch 1664 :: Batch 0/1
Batch Loss = 3.094518130291707
1664, epoch_train_loss=3.094518130291707
Epoch 1665
Epoch 1665 :: Batch 0/1
Batch Loss = 3.0944525942136245
1665, epoch_train_loss=3.0944525942136245
Epoch 1666
Epoch 1666 :: Batch 0/1
Batch Loss = 3.0943868666110808
1666, epoch_train_loss=3.0943868666110808
Epoch 1667
Epoch 1667 :: Batch 0/1
Batch Loss = 3.0943209526683173
1667, epoch_train_loss=3.0943209526683173
Epoch 1668
Epoch 1668 :: Batch 0/1
Batch Loss = 3.0942548574428765
1668, epoch_train_loss=3.0942548574428765
Epoch 1669
Epoch 1669 :: Batch 0/1
Batch Loss = 3.0941885858809535
1669, epoch_train_loss=3.0941885858809535
Epoch 1670
Epoch 1670 :: Batch 0/1
Batch Loss = 3.094122142793428
1670, epoch_train_loss=3.094122142793428
Epoch 1671
Epoch 1671 :: Batch 0/1
Batch Loss = 3.0940555328565242
1671, epoch_train_loss=3.0940555328565242
Epoch 1672
Epoch 1672 :: Batch 0/1
Batch Loss = 3.09398876059157
1672, epoch_train_loss=3.09398876059157
Epoch 1673
Epoch 1673 :: Batch 0/1
Batch Loss = 3.0939218303310727
1673, epoch_train_loss=3.0939218303310727
Epoch 1674
Epoch 1674 :: Batch 0/1
Batch Loss = 3.0938547462214263
1674, epoch_train_loss=3.0938547462214263
Epoch 1675
Epoch 1675 :: Batch 0/1
Batch Loss = 3.09378751220005
1675, epoch_train_loss=3.09378751220005
Epoch 1676
Epoch 1676 :: Batch 0/1
Batch Loss = 3.0937201320115157
1676, epoch_train_loss=3.0937201320115157
Epoch 1677
Epoch 1677 :: Batch 0/1
Batch Loss = 3.093652609183401
1677, epoch_train_loss=3.093652609183401
Epoch 1678
Epoch 1678 :: Batch 0/1
Batch Loss = 3.093584947013725
1678, epoch_train_loss=3.093584947013725
Epoch 1679
Epoch 1679 :: Batch 0/1
Batch Loss = 3.093517148577367
1679, epoch_train_loss=3.093517148577367
Epoch 1680
Epoch 1680 :: Batch 0/1
Batch Loss = 3.093449216702568
1680, epoch_train_loss=3.093449216702568
Epoch 1681
Epoch 1681 :: Batch 0/1
Batch Loss = 3.093381154000246
1681, epoch_train_loss=3.093381154000246
Epoch 1682
Epoch 1682 :: Batch 0/1
Batch Loss = 3.093312962820548
1682, epoch_train_loss=3.093312962820548
Epoch 1683
Epoch 1683 :: Batch 0/1
Batch Loss = 3.093244645270782
1683, epoch_train_loss=3.093244645270782
Epoch 1684
Epoch 1684 :: Batch 0/1
Batch Loss = 3.093176203208649
1684, epoch_train_loss=3.093176203208649
Epoch 1685
Epoch 1685 :: Batch 0/1
Batch Loss = 3.0931076382492777
1685, epoch_train_loss=3.0931076382492777
Epoch 1686
Epoch 1686 :: Batch 0/1
Batch Loss = 3.093038951763646
1686, epoch_train_loss=3.093038951763646
Epoch 1687
Epoch 1687 :: Batch 0/1
Batch Loss = 3.0929701448695037
1687, epoch_train_loss=3.0929701448695037
Epoch 1688
Epoch 1688 :: Batch 0/1
Batch Loss = 3.0929012184327846
1688, epoch_train_loss=3.0929012184327846
Epoch 1689
Epoch 1689 :: Batch 0/1
Batch Loss = 3.0928321730746315
1689, epoch_train_loss=3.0928321730746315
Epoch 1690
Epoch 1690 :: Batch 0/1
Batch Loss = 3.092763009173018
1690, epoch_train_loss=3.092763009173018
Epoch 1691
Epoch 1691 :: Batch 0/1
Batch Loss = 3.0926937268637986
1691, epoch_train_loss=3.0926937268637986
Epoch 1692
Epoch 1692 :: Batch 0/1
Batch Loss = 3.0926243260385964
1692, epoch_train_loss=3.0926243260385964
Epoch 1693
Epoch 1693 :: Batch 0/1
Batch Loss = 3.0925548063681827
1693, epoch_train_loss=3.0925548063681827
Epoch 1694
Epoch 1694 :: Batch 0/1
Batch Loss = 3.0924851672686327
1694, epoch_train_loss=3.0924851672686327
Epoch 1695
Epoch 1695 :: Batch 0/1
Batch Loss = 3.0924154079454333
1695, epoch_train_loss=3.0924154079454333
Epoch 1696
Epoch 1696 :: Batch 0/1
Batch Loss = 3.092345527370533
1696, epoch_train_loss=3.092345527370533
Epoch 1697
Epoch 1697 :: Batch 0/1
Batch Loss = 3.092275524291294
1697, epoch_train_loss=3.092275524291294
Epoch 1698
Epoch 1698 :: Batch 0/1
Batch Loss = 3.092205397239239
1698, epoch_train_loss=3.092205397239239
Epoch 1699
Epoch 1699 :: Batch 0/1
Batch Loss = 3.0921351445299354
1699, epoch_train_loss=3.0921351445299354
Epoch 1700
Epoch 1700 :: Batch 0/1
Batch Loss = 3.092064764283726
1700, epoch_train_loss=3.092064764283726
Epoch 1701
Epoch 1701 :: Batch 0/1
Batch Loss = 3.0919942544083128
1701, epoch_train_loss=3.0919942544083128
Epoch 1702
Epoch 1702 :: Batch 0/1
Batch Loss = 3.0919236126245675
1702, epoch_train_loss=3.0919236126245675
Epoch 1703
Epoch 1703 :: Batch 0/1
Batch Loss = 3.0918528364553084
1703, epoch_train_loss=3.0918528364553084
Epoch 1704
Epoch 1704 :: Batch 0/1
Batch Loss = 3.091781923232774
1704, epoch_train_loss=3.091781923232774
Epoch 1705
Epoch 1705 :: Batch 0/1
Batch Loss = 3.0917108701192864
1705, epoch_train_loss=3.0917108701192864
Epoch 1706
Epoch 1706 :: Batch 0/1
Batch Loss = 3.0916396740896808
1706, epoch_train_loss=3.0916396740896808
Epoch 1707
Epoch 1707 :: Batch 0/1
Batch Loss = 3.091568331961814
1707, epoch_train_loss=3.091568331961814
Epoch 1708
Epoch 1708 :: Batch 0/1
Batch Loss = 3.091496840384809
1708, epoch_train_loss=3.091496840384809
Epoch 1709
Epoch 1709 :: Batch 0/1
Batch Loss = 3.0914251958440695
1709, epoch_train_loss=3.0914251958440695
Epoch 1710
Epoch 1710 :: Batch 0/1
Batch Loss = 3.0913533946842415
1710, epoch_train_loss=3.0913533946842415
Epoch 1711
Epoch 1711 :: Batch 0/1
Batch Loss = 3.0912814330906633
1711, epoch_train_loss=3.0912814330906633
Epoch 1712
Epoch 1712 :: Batch 0/1
Batch Loss = 3.0912093071126914
1712, epoch_train_loss=3.0912093071126914
Epoch 1713
Epoch 1713 :: Batch 0/1
Batch Loss = 3.0911370126701874
1713, epoch_train_loss=3.0911370126701874
Epoch 1714
Epoch 1714 :: Batch 0/1
Batch Loss = 3.0910645455399846
1714, epoch_train_loss=3.0910645455399846
Epoch 1715
Epoch 1715 :: Batch 0/1
Batch Loss = 3.0909919013783296
1715, epoch_train_loss=3.0909919013783296
Epoch 1716
Epoch 1716 :: Batch 0/1
Batch Loss = 3.0909190757263727
1716, epoch_train_loss=3.0909190757263727
Epoch 1717
Epoch 1717 :: Batch 0/1
Batch Loss = 3.090846063996595
1717, epoch_train_loss=3.090846063996595
Epoch 1718
Epoch 1718 :: Batch 0/1
Batch Loss = 3.090772861494961
1718, epoch_train_loss=3.090772861494961
Epoch 1719
Epoch 1719 :: Batch 0/1
Batch Loss = 3.0906994634249796
1719, epoch_train_loss=3.0906994634249796
Epoch 1720
Epoch 1720 :: Batch 0/1
Batch Loss = 3.090625864873308
1720, epoch_train_loss=3.090625864873308
Epoch 1721
Epoch 1721 :: Batch 0/1
Batch Loss = 3.090552060831554
1721, epoch_train_loss=3.090552060831554
Epoch 1722
Epoch 1722 :: Batch 0/1
Batch Loss = 3.0904780461932777
1722, epoch_train_loss=3.0904780461932777
Epoch 1723
Epoch 1723 :: Batch 0/1
Batch Loss = 3.0904038157623086
1723, epoch_train_loss=3.0904038157623086
Epoch 1724
Epoch 1724 :: Batch 0/1
Batch Loss = 3.090329364232234
1724, epoch_train_loss=3.090329364232234
Epoch 1725
Epoch 1725 :: Batch 0/1
Batch Loss = 3.0902546862298927
1725, epoch_train_loss=3.0902546862298927
Epoch 1726
Epoch 1726 :: Batch 0/1
Batch Loss = 3.0901797762753143
1726, epoch_train_loss=3.0901797762753143
Epoch 1727
Epoch 1727 :: Batch 0/1
Batch Loss = 3.090104628807071
1727, epoch_train_loss=3.090104628807071
Epoch 1728
Epoch 1728 :: Batch 0/1
Batch Loss = 3.090029238176854
1728, epoch_train_loss=3.090029238176854
Epoch 1729
Epoch 1729 :: Batch 0/1
Batch Loss = 3.0899535986504
1729, epoch_train_loss=3.0899535986504
Epoch 1730
Epoch 1730 :: Batch 0/1
Batch Loss = 3.0898777044076025
1730, epoch_train_loss=3.0898777044076025
Epoch 1731
Epoch 1731 :: Batch 0/1
Batch Loss = 3.089801549541932
1731, epoch_train_loss=3.089801549541932
Epoch 1732
Epoch 1732 :: Batch 0/1
Batch Loss = 3.089725128060386
1732, epoch_train_loss=3.089725128060386
Epoch 1733
Epoch 1733 :: Batch 0/1
Batch Loss = 3.0896484338824695
1733, epoch_train_loss=3.0896484338824695
Epoch 1734
Epoch 1734 :: Batch 0/1
Batch Loss = 3.0895714608386395
1734, epoch_train_loss=3.0895714608386395
Epoch 1735
Epoch 1735 :: Batch 0/1
Batch Loss = 3.0894942026684675
1735, epoch_train_loss=3.0894942026684675
Epoch 1736
Epoch 1736 :: Batch 0/1
Batch Loss = 3.089416653012193
1736, epoch_train_loss=3.089416653012193
Epoch 1737
Epoch 1737 :: Batch 0/1
Batch Loss = 3.089338805427614
1737, epoch_train_loss=3.089338805427614
Epoch 1738
Epoch 1738 :: Batch 0/1
Batch Loss = 3.089260653369047
1738, epoch_train_loss=3.089260653369047
Epoch 1739
Epoch 1739 :: Batch 0/1
Batch Loss = 3.089182190190491
1739, epoch_train_loss=3.089182190190491
Epoch 1740
Epoch 1740 :: Batch 0/1
Batch Loss = 3.0891034091427594
1740, epoch_train_loss=3.0891034091427594
Epoch 1741
Epoch 1741 :: Batch 0/1
Batch Loss = 3.0890243033640994
1741, epoch_train_loss=3.0890243033640994
Epoch 1742
Epoch 1742 :: Batch 0/1
Batch Loss = 3.0889448658962806
1742, epoch_train_loss=3.0889448658962806
Epoch 1743
Epoch 1743 :: Batch 0/1
Batch Loss = 3.0888650896624146
1743, epoch_train_loss=3.0888650896624146
Epoch 1744
Epoch 1744 :: Batch 0/1
Batch Loss = 3.0887849674702865
1744, epoch_train_loss=3.0887849674702865
Epoch 1745
Epoch 1745 :: Batch 0/1
Batch Loss = 3.0887044920035693
1745, epoch_train_loss=3.0887044920035693
Epoch 1746
Epoch 1746 :: Batch 0/1
Batch Loss = 3.088623655838906
1746, epoch_train_loss=3.088623655838906
Epoch 1747
Epoch 1747 :: Batch 0/1
Batch Loss = 3.0885424514178825
1747, epoch_train_loss=3.0885424514178825
Epoch 1748
Epoch 1748 :: Batch 0/1
Batch Loss = 3.088460871070996
1748, epoch_train_loss=3.088460871070996
Epoch 1749
Epoch 1749 :: Batch 0/1
Batch Loss = 3.0883789069907617
1749, epoch_train_loss=3.0883789069907617
Epoch 1750
Epoch 1750 :: Batch 0/1
Batch Loss = 3.088296551258034
1750, epoch_train_loss=3.088296551258034
Epoch 1751
Epoch 1751 :: Batch 0/1
Batch Loss = 3.08821379582315
1751, epoch_train_loss=3.08821379582315
Epoch 1752
Epoch 1752 :: Batch 0/1
Batch Loss = 3.0881306325070836
1752, epoch_train_loss=3.0881306325070836
Epoch 1753
Epoch 1753 :: Batch 0/1
Batch Loss = 3.088047053024044
1753, epoch_train_loss=3.088047053024044
Epoch 1754
Epoch 1754 :: Batch 0/1
Batch Loss = 3.08796304895879
1754, epoch_train_loss=3.08796304895879
Epoch 1755
Epoch 1755 :: Batch 0/1
Batch Loss = 3.0878786117919064
1755, epoch_train_loss=3.0878786117919064
Epoch 1756
Epoch 1756 :: Batch 0/1
Batch Loss = 3.087793732899541
1756, epoch_train_loss=3.087793732899541
Epoch 1757
Epoch 1757 :: Batch 0/1
Batch Loss = 3.0877084035684255
1757, epoch_train_loss=3.0877084035684255
Epoch 1758
Epoch 1758 :: Batch 0/1
Batch Loss = 3.0876226149856008
1758, epoch_train_loss=3.0876226149856008
Epoch 1759
Epoch 1759 :: Batch 0/1
Batch Loss = 3.087536358277181
1759, epoch_train_loss=3.087536358277181
Epoch 1760
Epoch 1760 :: Batch 0/1
Batch Loss = 3.0874496244938134
1760, epoch_train_loss=3.0874496244938134
Epoch 1761
Epoch 1761 :: Batch 0/1
Batch Loss = 3.087362404652723
1761, epoch_train_loss=3.087362404652723
Epoch 1762
Epoch 1762 :: Batch 0/1
Batch Loss = 3.0872746897271894
1762, epoch_train_loss=3.0872746897271894
Epoch 1763
Epoch 1763 :: Batch 0/1
Batch Loss = 3.087186470685627
1763, epoch_train_loss=3.087186470685627
Epoch 1764
Epoch 1764 :: Batch 0/1
Batch Loss = 3.0870977385050407
1764, epoch_train_loss=3.0870977385050407
Epoch 1765
Epoch 1765 :: Batch 0/1
Batch Loss = 3.0870084841983876
1765, epoch_train_loss=3.0870084841983876
Epoch 1766
Epoch 1766 :: Batch 0/1
Batch Loss = 3.086918698817072
1766, epoch_train_loss=3.086918698817072
Epoch 1767
Epoch 1767 :: Batch 0/1
Batch Loss = 3.0868283734944253
1767, epoch_train_loss=3.0868283734944253
Epoch 1768
Epoch 1768 :: Batch 0/1
Batch Loss = 3.08673749946596
1768, epoch_train_loss=3.08673749946596
Epoch 1769
Epoch 1769 :: Batch 0/1
Batch Loss = 3.086646068089913
1769, epoch_train_loss=3.086646068089913
Epoch 1770
Epoch 1770 :: Batch 0/1
Batch Loss = 3.086554070879766
1770, epoch_train_loss=3.086554070879766
Epoch 1771
Epoch 1771 :: Batch 0/1
Batch Loss = 3.0864614995174673
1771, epoch_train_loss=3.0864614995174673
Epoch 1772
Epoch 1772 :: Batch 0/1
Batch Loss = 3.0863683458978413
1772, epoch_train_loss=3.0863683458978413
Epoch 1773
Epoch 1773 :: Batch 0/1
Batch Loss = 3.08627460211616
1773, epoch_train_loss=3.08627460211616
Epoch 1774
Epoch 1774 :: Batch 0/1
Batch Loss = 3.0861802605533946
1774, epoch_train_loss=3.0861802605533946
Epoch 1775
Epoch 1775 :: Batch 0/1
Batch Loss = 3.086085313781287
1775, epoch_train_loss=3.086085313781287
Epoch 1776
Epoch 1776 :: Batch 0/1
Batch Loss = 3.0859897547960573
1776, epoch_train_loss=3.0859897547960573
Epoch 1777
Epoch 1777 :: Batch 0/1
Batch Loss = 3.0858935766313422
1777, epoch_train_loss=3.0858935766313422
Epoch 1778
Epoch 1778 :: Batch 0/1
Batch Loss = 3.085796773195921
1778, epoch_train_loss=3.085796773195921
Epoch 1779
Epoch 1779 :: Batch 0/1
Batch Loss = 3.085699337860188
1779, epoch_train_loss=3.085699337860188
Epoch 1780
Epoch 1780 :: Batch 0/1
Batch Loss = 3.0856012672836193
1780, epoch_train_loss=3.0856012672836193
Epoch 1781
Epoch 1781 :: Batch 0/1
Batch Loss = 3.085502557979316
1781, epoch_train_loss=3.085502557979316
Epoch 1782
Epoch 1782 :: Batch 0/1
Batch Loss = 3.0854032346467894
1782, epoch_train_loss=3.0854032346467894
Epoch 1783
Epoch 1783 :: Batch 0/1
Batch Loss = 3.085303394666425
1783, epoch_train_loss=3.085303394666425
Epoch 1784
Epoch 1784 :: Batch 0/1
Batch Loss = 3.0852036523294655
1784, epoch_train_loss=3.0852036523294655
Epoch 1785
Epoch 1785 :: Batch 0/1
Batch Loss = 3.0851070395474642
1785, epoch_train_loss=3.0851070395474642
Epoch 1786
Epoch 1786 :: Batch 0/1
Batch Loss = 3.0850307984589653
1786, epoch_train_loss=3.0850307984589653
Epoch 1787
Epoch 1787 :: Batch 0/1
Batch Loss = 3.085070604833515
1787, epoch_train_loss=3.085070604833515
Epoch 1788
Epoch 1788 :: Batch 0/1
Batch Loss = 3.08580369426206
1788, epoch_train_loss=3.08580369426206
Epoch 1789
Epoch 1789 :: Batch 0/1
Batch Loss = 3.0903838550602667
1789, epoch_train_loss=3.0903838550602667
Epoch 1790
Epoch 1790 :: Batch 0/1
Batch Loss = 3.11619176730683
1790, epoch_train_loss=3.11619176730683
Epoch 1791
Epoch 1791 :: Batch 0/1
Batch Loss = 3.1830793904659442
1791, epoch_train_loss=3.1830793904659442
Epoch 1792
Epoch 1792 :: Batch 0/1
Batch Loss = 3.2577304120410284
1792, epoch_train_loss=3.2577304120410284
Epoch 1793
Epoch 1793 :: Batch 0/1
Batch Loss = 3.1457267859322684
1793, epoch_train_loss=3.1457267859322684
Epoch 1794
Epoch 1794 :: Batch 0/1
Batch Loss = 3.1086210349654873
1794, epoch_train_loss=3.1086210349654873
Epoch 1795
Epoch 1795 :: Batch 0/1
Batch Loss = 3.1725311912872556
1795, epoch_train_loss=3.1725311912872556
Epoch 1796
Epoch 1796 :: Batch 0/1
Batch Loss = 3.1048850910927976
1796, epoch_train_loss=3.1048850910927976
Epoch 1797
Epoch 1797 :: Batch 0/1
Batch Loss = 3.1078961153002864
1797, epoch_train_loss=3.1078961153002864
Epoch 1798
Epoch 1798 :: Batch 0/1
Batch Loss = 3.1436705817730943
1798, epoch_train_loss=3.1436705817730943
Epoch 1799
Epoch 1799 :: Batch 0/1
Batch Loss = 3.0902789650906173
1799, epoch_train_loss=3.0902789650906173
Epoch 1800
Epoch 1800 :: Batch 0/1
Batch Loss = 3.1285110081776195
1800, epoch_train_loss=3.1285110081776195
Epoch 1801
Epoch 1801 :: Batch 0/1
Batch Loss = 3.110505130376311
1801, epoch_train_loss=3.110505130376311
Epoch 1802
Epoch 1802 :: Batch 0/1
Batch Loss = 3.100785644412625
1802, epoch_train_loss=3.100785644412625
Epoch 1803
Epoch 1803 :: Batch 0/1
Batch Loss = 3.1188596295576363
1803, epoch_train_loss=3.1188596295576363
Epoch 1804
Epoch 1804 :: Batch 0/1
Batch Loss = 3.090685330063521
1804, epoch_train_loss=3.090685330063521
Epoch 1805
Epoch 1805 :: Batch 0/1
Batch Loss = 3.114910487138329
1805, epoch_train_loss=3.114910487138329
Epoch 1806
Epoch 1806 :: Batch 0/1
Batch Loss = 3.0909553347907646
1806, epoch_train_loss=3.0909553347907646
Epoch 1807
Epoch 1807 :: Batch 0/1
Batch Loss = 3.105068566147664
1807, epoch_train_loss=3.105068566147664
Epoch 1808
Epoch 1808 :: Batch 0/1
Batch Loss = 3.0970672454000825
1808, epoch_train_loss=3.0970672454000825
Epoch 1809
Epoch 1809 :: Batch 0/1
Batch Loss = 3.095771398396094
1809, epoch_train_loss=3.095771398396094
Epoch 1810
Epoch 1810 :: Batch 0/1
Batch Loss = 3.100342570446161
1810, epoch_train_loss=3.100342570446161
Epoch 1811
Epoch 1811 :: Batch 0/1
Batch Loss = 3.0888041244089823
1811, epoch_train_loss=3.0888041244089823
Epoch 1812
Epoch 1812 :: Batch 0/1
Batch Loss = 3.09951658273925
1812, epoch_train_loss=3.09951658273925
Epoch 1813
Epoch 1813 :: Batch 0/1
Batch Loss = 3.087990713796421
1813, epoch_train_loss=3.087990713796421
Epoch 1814
Epoch 1814 :: Batch 0/1
Batch Loss = 3.096204386949215
1814, epoch_train_loss=3.096204386949215
Epoch 1815
Epoch 1815 :: Batch 0/1
Batch Loss = 3.089945639253666
1815, epoch_train_loss=3.089945639253666
Epoch 1816
Epoch 1816 :: Batch 0/1
Batch Loss = 3.0903641946673037
1816, epoch_train_loss=3.0903641946673037
Epoch 1817
Epoch 1817 :: Batch 0/1
Batch Loss = 3.0923285897105517
1817, epoch_train_loss=3.0923285897105517
Epoch 1818
Epoch 1818 :: Batch 0/1
Batch Loss = 3.086439564583068
1818, epoch_train_loss=3.086439564583068
Epoch 1819
Epoch 1819 :: Batch 0/1
Batch Loss = 3.0920350651758466
1819, epoch_train_loss=3.0920350651758466
Epoch 1820
Epoch 1820 :: Batch 0/1
Batch Loss = 3.086196552818383
1820, epoch_train_loss=3.086196552818383
Epoch 1821
Epoch 1821 :: Batch 0/1
Batch Loss = 3.0892896134987025
1821, epoch_train_loss=3.0892896134987025
Epoch 1822
Epoch 1822 :: Batch 0/1
Batch Loss = 3.0879157210529447
1822, epoch_train_loss=3.0879157210529447
Epoch 1823
Epoch 1823 :: Batch 0/1
Batch Loss = 3.086504723386561
1823, epoch_train_loss=3.086504723386561
Epoch 1824
Epoch 1824 :: Batch 0/1
Batch Loss = 3.088851794285329
1824, epoch_train_loss=3.088851794285329
Epoch 1825
Epoch 1825 :: Batch 0/1
Batch Loss = 3.085292274528261
1825, epoch_train_loss=3.085292274528261
Epoch 1826
Epoch 1826 :: Batch 0/1
Batch Loss = 3.0878371034313385
1826, epoch_train_loss=3.0878371034313385
Epoch 1827
Epoch 1827 :: Batch 0/1
Batch Loss = 3.086088386110379
1827, epoch_train_loss=3.086088386110379
Epoch 1828
Epoch 1828 :: Batch 0/1
Batch Loss = 3.086011163931027
1828, epoch_train_loss=3.086011163931027
Epoch 1829
Epoch 1829 :: Batch 0/1
Batch Loss = 3.0868901501748334
1829, epoch_train_loss=3.0868901501748334
Epoch 1830
Epoch 1830 :: Batch 0/1
Batch Loss = 3.08488190466427
1830, epoch_train_loss=3.08488190466427
Epoch 1831
Epoch 1831 :: Batch 0/1
Batch Loss = 3.086446264278327
1831, epoch_train_loss=3.086446264278327
Epoch 1832
Epoch 1832 :: Batch 0/1
Batch Loss = 3.085168367615068
1832, epoch_train_loss=3.085168367615068
Epoch 1833
Epoch 1833 :: Batch 0/1
Batch Loss = 3.0852477729619254
1833, epoch_train_loss=3.0852477729619254
Epoch 1834
Epoch 1834 :: Batch 0/1
Batch Loss = 3.0856765755255378
1834, epoch_train_loss=3.0856765755255378
Epoch 1835
Epoch 1835 :: Batch 0/1
Batch Loss = 3.0844586893642627
1835, epoch_train_loss=3.0844586893642627
Epoch 1836
Epoch 1836 :: Batch 0/1
Batch Loss = 3.0853405415331165
1836, epoch_train_loss=3.0853405415331165
Epoch 1837
Epoch 1837 :: Batch 0/1
Batch Loss = 3.0845977377481737
1837, epoch_train_loss=3.0845977377481737
Epoch 1838
Epoch 1838 :: Batch 0/1
Batch Loss = 3.084404333005136
1838, epoch_train_loss=3.084404333005136
Epoch 1839
Epoch 1839 :: Batch 0/1
Batch Loss = 3.0847716704182826
1839, epoch_train_loss=3.0847716704182826
Epoch 1840
Epoch 1840 :: Batch 0/1
Batch Loss = 3.0839454114259968
1840, epoch_train_loss=3.0839454114259968
Epoch 1841
Epoch 1841 :: Batch 0/1
Batch Loss = 3.0843165448298957
1841, epoch_train_loss=3.0843165448298957
Epoch 1842
Epoch 1842 :: Batch 0/1
Batch Loss = 3.0840255336397013
1842, epoch_train_loss=3.0840255336397013
Epoch 1843
Epoch 1843 :: Batch 0/1
Batch Loss = 3.083648988569664
1843, epoch_train_loss=3.083648988569664
Epoch 1844
Epoch 1844 :: Batch 0/1
Batch Loss = 3.0839479340870484
1844, epoch_train_loss=3.0839479340870484
Epoch 1845
Epoch 1845 :: Batch 0/1
Batch Loss = 3.083444823782402
1845, epoch_train_loss=3.083444823782402
Epoch 1846
Epoch 1846 :: Batch 0/1
Batch Loss = 3.0834402434672965
1846, epoch_train_loss=3.0834402434672965
Epoch 1847
Epoch 1847 :: Batch 0/1
Batch Loss = 3.0834634554844356
1847, epoch_train_loss=3.0834634554844356
Epoch 1848
Epoch 1848 :: Batch 0/1
Batch Loss = 3.083043067396274
1848, epoch_train_loss=3.083043067396274
Epoch 1849
Epoch 1849 :: Batch 0/1
Batch Loss = 3.0831740962296053
1849, epoch_train_loss=3.0831740962296053
Epoch 1850
Epoch 1850 :: Batch 0/1
Batch Loss = 3.0829756595394455
1850, epoch_train_loss=3.0829756595394455
Epoch 1851
Epoch 1851 :: Batch 0/1
Batch Loss = 3.082739415066937
1851, epoch_train_loss=3.082739415066937
Epoch 1852
Epoch 1852 :: Batch 0/1
Batch Loss = 3.082823064573505
1852, epoch_train_loss=3.082823064573505
Epoch 1853
Epoch 1853 :: Batch 0/1
Batch Loss = 3.082550080664909
1853, epoch_train_loss=3.082550080664909
Epoch 1854
Epoch 1854 :: Batch 0/1
Batch Loss = 3.0824509915773235
1854, epoch_train_loss=3.0824509915773235
Epoch 1855
Epoch 1855 :: Batch 0/1
Batch Loss = 3.082442826192595
1855, epoch_train_loss=3.082442826192595
Epoch 1856
Epoch 1856 :: Batch 0/1
Batch Loss = 3.08218446614605
1856, epoch_train_loss=3.08218446614605
Epoch 1857
Epoch 1857 :: Batch 0/1
Batch Loss = 3.082137254307232
1857, epoch_train_loss=3.082137254307232
Epoch 1858
Epoch 1858 :: Batch 0/1
Batch Loss = 3.0820592334698977
1858, epoch_train_loss=3.0820592334698977
Epoch 1859
Epoch 1859 :: Batch 0/1
Batch Loss = 3.08184262242735
1859, epoch_train_loss=3.08184262242735
Epoch 1860
Epoch 1860 :: Batch 0/1
Batch Loss = 3.081805077767445
1860, epoch_train_loss=3.081805077767445
Epoch 1861
Epoch 1861 :: Batch 0/1
Batch Loss = 3.081683253684231
1861, epoch_train_loss=3.081683253684231
Epoch 1862
Epoch 1862 :: Batch 0/1
Batch Loss = 3.08150581123099
1862, epoch_train_loss=3.08150581123099
Epoch 1863
Epoch 1863 :: Batch 0/1
Batch Loss = 3.0814509327388158
1863, epoch_train_loss=3.0814509327388158
Epoch 1864
Epoch 1864 :: Batch 0/1
Batch Loss = 3.0813172995641533
1864, epoch_train_loss=3.0813172995641533
Epoch 1865
Epoch 1865 :: Batch 0/1
Batch Loss = 3.0811581363458997
1865, epoch_train_loss=3.0811581363458997
Epoch 1866
Epoch 1866 :: Batch 0/1
Batch Loss = 3.081086317125498
1866, epoch_train_loss=3.081086317125498
Epoch 1867
Epoch 1867 :: Batch 0/1
Batch Loss = 3.0809551674537934
1867, epoch_train_loss=3.0809551674537934
Epoch 1868
Epoch 1868 :: Batch 0/1
Batch Loss = 3.0808036039853106
1868, epoch_train_loss=3.0808036039853106
Epoch 1869
Epoch 1869 :: Batch 0/1
Batch Loss = 3.0807189978896594
1869, epoch_train_loss=3.0807189978896594
Epoch 1870
Epoch 1870 :: Batch 0/1
Batch Loss = 3.0805899340100846
1870, epoch_train_loss=3.0805899340100846
Epoch 1871
Epoch 1871 :: Batch 0/1
Batch Loss = 3.080442269042331
1871, epoch_train_loss=3.080442269042331
Epoch 1872
Epoch 1872 :: Batch 0/1
Batch Loss = 3.080344736334757
1872, epoch_train_loss=3.080344736334757
Epoch 1873
Epoch 1873 :: Batch 0/1
Batch Loss = 3.0802203709014684
1873, epoch_train_loss=3.0802203709014684
Epoch 1874
Epoch 1874 :: Batch 0/1
Batch Loss = 3.080075158505702
1874, epoch_train_loss=3.080075158505702
Epoch 1875
Epoch 1875 :: Batch 0/1
Batch Loss = 3.079964556819305
1875, epoch_train_loss=3.079964556819305
Epoch 1876
Epoch 1876 :: Batch 0/1
Batch Loss = 3.0798456615812486
1876, epoch_train_loss=3.0798456615812486
Epoch 1877
Epoch 1877 :: Batch 0/1
Batch Loss = 3.079702092657336
1877, epoch_train_loss=3.079702092657336
Epoch 1878
Epoch 1878 :: Batch 0/1
Batch Loss = 3.079578737465807
1878, epoch_train_loss=3.079578737465807
Epoch 1879
Epoch 1879 :: Batch 0/1
Batch Loss = 3.079461795268032
1879, epoch_train_loss=3.079461795268032
Epoch 1880
Epoch 1880 :: Batch 0/1
Batch Loss = 3.0793216741021996
1880, epoch_train_loss=3.0793216741021996
Epoch 1881
Epoch 1881 :: Batch 0/1
Batch Loss = 3.079188212194653
1881, epoch_train_loss=3.079188212194653
Epoch 1882
Epoch 1882 :: Batch 0/1
Batch Loss = 3.07906680966661
1882, epoch_train_loss=3.07906680966661
Epoch 1883
Epoch 1883 :: Batch 0/1
Batch Loss = 3.0789329102211784
1883, epoch_train_loss=3.0789329102211784
Epoch 1884
Epoch 1884 :: Batch 0/1
Batch Loss = 3.078791999748582
1884, epoch_train_loss=3.078791999748582
Epoch 1885
Epoch 1885 :: Batch 0/1
Batch Loss = 3.078662356085674
1885, epoch_train_loss=3.078662356085674
Epoch 1886
Epoch 1886 :: Batch 0/1
Batch Loss = 3.078531827752541
1886, epoch_train_loss=3.078531827752541
Epoch 1887
Epoch 1887 :: Batch 0/1
Batch Loss = 3.0783901064896018
1887, epoch_train_loss=3.0783901064896018
Epoch 1888
Epoch 1888 :: Batch 0/1
Batch Loss = 3.078250817347536
1888, epoch_train_loss=3.078250817347536
Epoch 1889
Epoch 1889 :: Batch 0/1
Batch Loss = 3.078117309322047
1889, epoch_train_loss=3.078117309322047
Epoch 1890
Epoch 1890 :: Batch 0/1
Batch Loss = 3.0779784670840775
1890, epoch_train_loss=3.0779784670840775
Epoch 1891
Epoch 1891 :: Batch 0/1
Batch Loss = 3.0778336523230116
1891, epoch_train_loss=3.0778336523230116
Epoch 1892
Epoch 1892 :: Batch 0/1
Batch Loss = 3.0776919917560845
1892, epoch_train_loss=3.0776919917560845
Epoch 1893
Epoch 1893 :: Batch 0/1
Batch Loss = 3.0775525477035073
1893, epoch_train_loss=3.0775525477035073
Epoch 1894
Epoch 1894 :: Batch 0/1
Batch Loss = 3.0774080758477265
1894, epoch_train_loss=3.0774080758477265
Epoch 1895
Epoch 1895 :: Batch 0/1
Batch Loss = 3.0772600919348507
1895, epoch_train_loss=3.0772600919348507
Epoch 1896
Epoch 1896 :: Batch 0/1
Batch Loss = 3.077113843616727
1896, epoch_train_loss=3.077113843616727
Epoch 1897
Epoch 1897 :: Batch 0/1
Batch Loss = 3.0769685440060988
1897, epoch_train_loss=3.0769685440060988
Epoch 1898
Epoch 1898 :: Batch 0/1
Batch Loss = 3.076819547588788
1898, epoch_train_loss=3.076819547588788
Epoch 1899
Epoch 1899 :: Batch 0/1
Batch Loss = 3.076667536955894
1899, epoch_train_loss=3.076667536955894
Epoch 1900
Epoch 1900 :: Batch 0/1
Batch Loss = 3.076515794224071
1900, epoch_train_loss=3.076515794224071
Epoch 1901
Epoch 1901 :: Batch 0/1
Batch Loss = 3.0763644652664595
1901, epoch_train_loss=3.0763644652664595
Epoch 1902
Epoch 1902 :: Batch 0/1
Batch Loss = 3.0762110371113276
1902, epoch_train_loss=3.0762110371113276
Epoch 1903
Epoch 1903 :: Batch 0/1
Batch Loss = 3.076054511620368
1903, epoch_train_loss=3.076054511620368
Epoch 1904
Epoch 1904 :: Batch 0/1
Batch Loss = 3.0758968363625137
1904, epoch_train_loss=3.0758968363625137
Epoch 1905
Epoch 1905 :: Batch 0/1
Batch Loss = 3.075739022726679
1905, epoch_train_loss=3.075739022726679
Epoch 1906
Epoch 1906 :: Batch 0/1
Batch Loss = 3.0755802674614987
1906, epoch_train_loss=3.0755802674614987
Epoch 1907
Epoch 1907 :: Batch 0/1
Batch Loss = 3.0754191898791032
1907, epoch_train_loss=3.0754191898791032
Epoch 1908
Epoch 1908 :: Batch 0/1
Batch Loss = 3.0752558781496258
1908, epoch_train_loss=3.0752558781496258
Epoch 1909
Epoch 1909 :: Batch 0/1
Batch Loss = 3.075091258361707
1909, epoch_train_loss=3.075091258361707
Epoch 1910
Epoch 1910 :: Batch 0/1
Batch Loss = 3.0749258137364084
1910, epoch_train_loss=3.0749258137364084
Epoch 1911
Epoch 1911 :: Batch 0/1
Batch Loss = 3.0747590844252404
1911, epoch_train_loss=3.0747590844252404
Epoch 1912
Epoch 1912 :: Batch 0/1
Batch Loss = 3.074590449149475
1912, epoch_train_loss=3.074590449149475
Epoch 1913
Epoch 1913 :: Batch 0/1
Batch Loss = 3.0744196749969386
1913, epoch_train_loss=3.0744196749969386
Epoch 1914
Epoch 1914 :: Batch 0/1
Batch Loss = 3.074247066440819
1914, epoch_train_loss=3.074247066440819
Epoch 1915
Epoch 1915 :: Batch 0/1
Batch Loss = 3.0740729772864794
1915, epoch_train_loss=3.0740729772864794
Epoch 1916
Epoch 1916 :: Batch 0/1
Batch Loss = 3.0738974679202933
1916, epoch_train_loss=3.0738974679202933
Epoch 1917
Epoch 1917 :: Batch 0/1
Batch Loss = 3.0737203538829867
1917, epoch_train_loss=3.0737203538829867
Epoch 1918
Epoch 1918 :: Batch 0/1
Batch Loss = 3.0735412909172353
1918, epoch_train_loss=3.0735412909172353
Epoch 1919
Epoch 1919 :: Batch 0/1
Batch Loss = 3.0733601957457073
1919, epoch_train_loss=3.0733601957457073
Epoch 1920
Epoch 1920 :: Batch 0/1
Batch Loss = 3.073177008749366
1920, epoch_train_loss=3.073177008749366
Epoch 1921
Epoch 1921 :: Batch 0/1
Batch Loss = 3.072991844777186
1921, epoch_train_loss=3.072991844777186
Epoch 1922
Epoch 1922 :: Batch 0/1
Batch Loss = 3.0728047036890587
1922, epoch_train_loss=3.0728047036890587
Epoch 1923
Epoch 1923 :: Batch 0/1
Batch Loss = 3.072615631818127
1923, epoch_train_loss=3.072615631818127
Epoch 1924
Epoch 1924 :: Batch 0/1
Batch Loss = 3.072424559044346
1924, epoch_train_loss=3.072424559044346
Epoch 1925
Epoch 1925 :: Batch 0/1
Batch Loss = 3.072231417484071
1925, epoch_train_loss=3.072231417484071
Epoch 1926
Epoch 1926 :: Batch 0/1
Batch Loss = 3.0720361247121737
1926, epoch_train_loss=3.0720361247121737
Epoch 1927
Epoch 1927 :: Batch 0/1
Batch Loss = 3.07183861451542
1927, epoch_train_loss=3.07183861451542
Epoch 1928
Epoch 1928 :: Batch 0/1
Batch Loss = 3.071638863128338
1928, epoch_train_loss=3.071638863128338
Epoch 1929
Epoch 1929 :: Batch 0/1
Batch Loss = 3.0714368109832955
1929, epoch_train_loss=3.0714368109832955
Epoch 1930
Epoch 1930 :: Batch 0/1
Batch Loss = 3.0712324981239885
1930, epoch_train_loss=3.0712324981239885
Epoch 1931
Epoch 1931 :: Batch 0/1
Batch Loss = 3.071025955826087
1931, epoch_train_loss=3.071025955826087
Epoch 1932
Epoch 1932 :: Batch 0/1
Batch Loss = 3.0708174032047086
1932, epoch_train_loss=3.0708174032047086
Epoch 1933
Epoch 1933 :: Batch 0/1
Batch Loss = 3.0706072694700155
1933, epoch_train_loss=3.0706072694700155
Epoch 1934
Epoch 1934 :: Batch 0/1
Batch Loss = 3.070396649105433
1934, epoch_train_loss=3.070396649105433
Epoch 1935
Epoch 1935 :: Batch 0/1
Batch Loss = 3.070188155572278
1935, epoch_train_loss=3.070188155572278
Epoch 1936
Epoch 1936 :: Batch 0/1
Batch Loss = 3.069987972523958
1936, epoch_train_loss=3.069987972523958
Epoch 1937
Epoch 1937 :: Batch 0/1
Batch Loss = 3.069812360198827
1937, epoch_train_loss=3.069812360198827
Epoch 1938
Epoch 1938 :: Batch 0/1
Batch Loss = 3.0697016497970977
1938, epoch_train_loss=3.0697016497970977
Epoch 1939
Epoch 1939 :: Batch 0/1
Batch Loss = 3.0697718622477113
1939, epoch_train_loss=3.0697718622477113
Epoch 1940
Epoch 1940 :: Batch 0/1
Batch Loss = 3.0703238000850654
1940, epoch_train_loss=3.0703238000850654
Epoch 1941
Epoch 1941 :: Batch 0/1
Batch Loss = 3.0723134424085905
1941, epoch_train_loss=3.0723134424085905
Epoch 1942
Epoch 1942 :: Batch 0/1
Batch Loss = 3.0781077904237333
1942, epoch_train_loss=3.0781077904237333
Epoch 1943
Epoch 1943 :: Batch 0/1
Batch Loss = 3.095836244939712
1943, epoch_train_loss=3.095836244939712
Epoch 1944
Epoch 1944 :: Batch 0/1
Batch Loss = 3.1353701681208954
1944, epoch_train_loss=3.1353701681208954
Epoch 1945
Epoch 1945 :: Batch 0/1
Batch Loss = 3.2238863118505243
1945, epoch_train_loss=3.2238863118505243
Epoch 1946
Epoch 1946 :: Batch 0/1
Batch Loss = 3.2124733086491513
1946, epoch_train_loss=3.2124733086491513
Epoch 1947
Epoch 1947 :: Batch 0/1
Batch Loss = 3.1378521428443404
1947, epoch_train_loss=3.1378521428443404
Epoch 1948
Epoch 1948 :: Batch 0/1
Batch Loss = 3.077379943470875
1948, epoch_train_loss=3.077379943470875
Epoch 1949
Epoch 1949 :: Batch 0/1
Batch Loss = 3.16778485740647
1949, epoch_train_loss=3.16778485740647
Epoch 1950
Epoch 1950 :: Batch 0/1
Batch Loss = 3.200970758942364
1950, epoch_train_loss=3.200970758942364
Epoch 1951
Epoch 1951 :: Batch 0/1
Batch Loss = 3.093042466994525
1951, epoch_train_loss=3.093042466994525
Epoch 1952
Epoch 1952 :: Batch 0/1
Batch Loss = 3.2171867105220073
1952, epoch_train_loss=3.2171867105220073
Epoch 1953
Epoch 1953 :: Batch 0/1
Batch Loss = 3.162532729933082
1953, epoch_train_loss=3.162532729933082
Epoch 1954
Epoch 1954 :: Batch 0/1
Batch Loss = 3.118950126574144
1954, epoch_train_loss=3.118950126574144
Epoch 1955
Epoch 1955 :: Batch 0/1
Batch Loss = 3.170572746341841
1955, epoch_train_loss=3.170572746341841
Epoch 1956
Epoch 1956 :: Batch 0/1
Batch Loss = 3.0970549431586805
1956, epoch_train_loss=3.0970549431586805
Epoch 1957
Epoch 1957 :: Batch 0/1
Batch Loss = 3.1543528746217118
1957, epoch_train_loss=3.1543528746217118
Epoch 1958
Epoch 1958 :: Batch 0/1
Batch Loss = 3.0855685354542555
1958, epoch_train_loss=3.0855685354542555
Epoch 1959
Epoch 1959 :: Batch 0/1
Batch Loss = 3.1439588612745437
1959, epoch_train_loss=3.1439588612745437
Epoch 1960
Epoch 1960 :: Batch 0/1
Batch Loss = 3.086383238269157
1960, epoch_train_loss=3.086383238269157
Epoch 1961
Epoch 1961 :: Batch 0/1
Batch Loss = 3.1164362175081384
1961, epoch_train_loss=3.1164362175081384
Epoch 1962
Epoch 1962 :: Batch 0/1
Batch Loss = 3.092831464268301
1962, epoch_train_loss=3.092831464268301
Epoch 1963
Epoch 1963 :: Batch 0/1
Batch Loss = 3.0916099087574667
1963, epoch_train_loss=3.0916099087574667
Epoch 1964
Epoch 1964 :: Batch 0/1
Batch Loss = 3.1072249568562866
1964, epoch_train_loss=3.1072249568562866
Epoch 1965
Epoch 1965 :: Batch 0/1
Batch Loss = 3.0857291969809366
1965, epoch_train_loss=3.0857291969809366
Epoch 1966
Epoch 1966 :: Batch 0/1
Batch Loss = 3.1007550733747093
1966, epoch_train_loss=3.1007550733747093
Epoch 1967
Epoch 1967 :: Batch 0/1
Batch Loss = 3.0837938082325325
1967, epoch_train_loss=3.0837938082325325
Epoch 1968
Epoch 1968 :: Batch 0/1
Batch Loss = 3.082854910677022
1968, epoch_train_loss=3.082854910677022
Epoch 1969
Epoch 1969 :: Batch 0/1
Batch Loss = 3.0876101484159877
1969, epoch_train_loss=3.0876101484159877
Epoch 1970
Epoch 1970 :: Batch 0/1
Batch Loss = 3.0758309420990133
1970, epoch_train_loss=3.0758309420990133
Epoch 1971
Epoch 1971 :: Batch 0/1
Batch Loss = 3.0845756568121816
1971, epoch_train_loss=3.0845756568121816
Epoch 1972
Epoch 1972 :: Batch 0/1
Batch Loss = 3.075558069489972
1972, epoch_train_loss=3.075558069489972
Epoch 1973
Epoch 1973 :: Batch 0/1
Batch Loss = 3.076477295284232
1973, epoch_train_loss=3.076477295284232
Epoch 1974
Epoch 1974 :: Batch 0/1
Batch Loss = 3.0800519937591213
1974, epoch_train_loss=3.0800519937591213
Epoch 1975
Epoch 1975 :: Batch 0/1
Batch Loss = 3.0737307888900234
1975, epoch_train_loss=3.0737307888900234
Epoch 1976
Epoch 1976 :: Batch 0/1
Batch Loss = 3.0770670997454035
1976, epoch_train_loss=3.0770670997454035
Epoch 1977
Epoch 1977 :: Batch 0/1
Batch Loss = 3.0717701841113487
1977, epoch_train_loss=3.0717701841113487
Epoch 1978
Epoch 1978 :: Batch 0/1
Batch Loss = 3.0725993462612955
1978, epoch_train_loss=3.0725993462612955
Epoch 1979
Epoch 1979 :: Batch 0/1
Batch Loss = 3.0727746772644027
1979, epoch_train_loss=3.0727746772644027
Epoch 1980
Epoch 1980 :: Batch 0/1
Batch Loss = 3.0703130647087407
1980, epoch_train_loss=3.0703130647087407
Epoch 1981
Epoch 1981 :: Batch 0/1
Batch Loss = 3.0731195516879595
1981, epoch_train_loss=3.0731195516879595
Epoch 1982
Epoch 1982 :: Batch 0/1
Batch Loss = 3.06805755706562
1982, epoch_train_loss=3.06805755706562
Epoch 1983
Epoch 1983 :: Batch 0/1
Batch Loss = 3.0709730918642038
1983, epoch_train_loss=3.0709730918642038
Epoch 1984
Epoch 1984 :: Batch 0/1
Batch Loss = 3.0696700104485326
1984, epoch_train_loss=3.0696700104485326
Epoch 1985
Epoch 1985 :: Batch 0/1
Batch Loss = 3.067593923086518
1985, epoch_train_loss=3.067593923086518
Epoch 1986
Epoch 1986 :: Batch 0/1
Batch Loss = 3.0704681884240816
1986, epoch_train_loss=3.0704681884240816
Epoch 1987
Epoch 1987 :: Batch 0/1
Batch Loss = 3.0658069914326056
1987, epoch_train_loss=3.0658069914326056
Epoch 1988
Epoch 1988 :: Batch 0/1
Batch Loss = 3.0677982044575414
1988, epoch_train_loss=3.0677982044575414
Epoch 1989
Epoch 1989 :: Batch 0/1
Batch Loss = 3.0667330013108547
1989, epoch_train_loss=3.0667330013108547
Epoch 1990
Epoch 1990 :: Batch 0/1
Batch Loss = 3.0650556568115213
1990, epoch_train_loss=3.0650556568115213
Epoch 1991
Epoch 1991 :: Batch 0/1
Batch Loss = 3.066516674111139
1991, epoch_train_loss=3.066516674111139
Epoch 1992
Epoch 1992 :: Batch 0/1
Batch Loss = 3.0645938338859047
1992, epoch_train_loss=3.0645938338859047
Epoch 1993
Epoch 1993 :: Batch 0/1
Batch Loss = 3.0648047505867764
1993, epoch_train_loss=3.0648047505867764
Epoch 1994
Epoch 1994 :: Batch 0/1
Batch Loss = 3.0643187440591046
1994, epoch_train_loss=3.0643187440591046
Epoch 1995
Epoch 1995 :: Batch 0/1
Batch Loss = 3.063876879986393
1995, epoch_train_loss=3.063876879986393
Epoch 1996
Epoch 1996 :: Batch 0/1
Batch Loss = 3.0631719028023716
1996, epoch_train_loss=3.0631719028023716
Epoch 1997
Epoch 1997 :: Batch 0/1
Batch Loss = 3.0632339126330574
1997, epoch_train_loss=3.0632339126330574
Epoch 1998
Epoch 1998 :: Batch 0/1
Batch Loss = 3.062503648905848
1998, epoch_train_loss=3.062503648905848
Epoch 1999
Epoch 1999 :: Batch 0/1
Batch Loss = 3.0618904549479193
1999, epoch_train_loss=3.0618904549479193
Epoch 2000
Epoch 2000 :: Batch 0/1
Batch Loss = 3.0622317012160956
2000, epoch_train_loss=3.0622317012160956
Epoch 2001
Epoch 2001 :: Batch 0/1
Batch Loss = 3.0609063423238805
2001, epoch_train_loss=3.0609063423238805
Epoch 2002
Epoch 2002 :: Batch 0/1
Batch Loss = 3.0609811025739044
2002, epoch_train_loss=3.0609811025739044
Epoch 2003
Epoch 2003 :: Batch 0/1
Batch Loss = 3.060554969483783
2003, epoch_train_loss=3.060554969483783
Epoch 2004
Epoch 2004 :: Batch 0/1
Batch Loss = 3.0600695193193754
2004, epoch_train_loss=3.0600695193193754
Epoch 2005
Epoch 2005 :: Batch 0/1
Batch Loss = 3.059565261677107
2005, epoch_train_loss=3.059565261677107
Epoch 2006
Epoch 2006 :: Batch 0/1
Batch Loss = 3.0594082974472614
2006, epoch_train_loss=3.0594082974472614
Epoch 2007
Epoch 2007 :: Batch 0/1
Batch Loss = 3.0589663158585467
2007, epoch_train_loss=3.0589663158585467
Epoch 2008
Epoch 2008 :: Batch 0/1
Batch Loss = 3.058285364631469
2008, epoch_train_loss=3.058285364631469
Epoch 2009
Epoch 2009 :: Batch 0/1
Batch Loss = 3.058233004506764
2009, epoch_train_loss=3.058233004506764
Epoch 2010
Epoch 2010 :: Batch 0/1
Batch Loss = 3.057676119082517
2010, epoch_train_loss=3.057676119082517
Epoch 2011
Epoch 2011 :: Batch 0/1
Batch Loss = 3.0572870641684147
2011, epoch_train_loss=3.0572870641684147
Epoch 2012
Epoch 2012 :: Batch 0/1
Batch Loss = 3.0566985887731604
2012, epoch_train_loss=3.0566985887731604
Epoch 2013
Epoch 2013 :: Batch 0/1
Batch Loss = 3.0565500372012187
2013, epoch_train_loss=3.0565500372012187
Epoch 2014
Epoch 2014 :: Batch 0/1
Batch Loss = 3.056071067930893
2014, epoch_train_loss=3.056071067930893
Epoch 2015
Epoch 2015 :: Batch 0/1
Batch Loss = 3.055596095745208
2015, epoch_train_loss=3.055596095745208
Epoch 2016
Epoch 2016 :: Batch 0/1
Batch Loss = 3.0551221453675126
2016, epoch_train_loss=3.0551221453675126
Epoch 2017
Epoch 2017 :: Batch 0/1
Batch Loss = 3.0547990023802507
2017, epoch_train_loss=3.0547990023802507
Epoch 2018
Epoch 2018 :: Batch 0/1
Batch Loss = 3.0544272759398683
2018, epoch_train_loss=3.0544272759398683
Epoch 2019
Epoch 2019 :: Batch 0/1
Batch Loss = 3.053923295315381
2019, epoch_train_loss=3.053923295315381
Epoch 2020
Epoch 2020 :: Batch 0/1
Batch Loss = 3.053443166130547
2020, epoch_train_loss=3.053443166130547
Epoch 2021
Epoch 2021 :: Batch 0/1
Batch Loss = 3.052973885829493
2021, epoch_train_loss=3.052973885829493
Epoch 2022
Epoch 2022 :: Batch 0/1
Batch Loss = 3.0526283549748
2022, epoch_train_loss=3.0526283549748
Epoch 2023
Epoch 2023 :: Batch 0/1
Batch Loss = 3.0521769958020504
2023, epoch_train_loss=3.0521769958020504
Epoch 2024
Epoch 2024 :: Batch 0/1
Batch Loss = 3.051744121216331
2024, epoch_train_loss=3.051744121216331
Epoch 2025
Epoch 2025 :: Batch 0/1
Batch Loss = 3.0512050645280806
2025, epoch_train_loss=3.0512050645280806
Epoch 2026
Epoch 2026 :: Batch 0/1
Batch Loss = 3.0507431366399436
2026, epoch_train_loss=3.0507431366399436
Epoch 2027
Epoch 2027 :: Batch 0/1
Batch Loss = 3.0502533759972454
2027, epoch_train_loss=3.0502533759972454
Epoch 2028
Epoch 2028 :: Batch 0/1
Batch Loss = 3.04983615789197
2028, epoch_train_loss=3.04983615789197
Epoch 2029
Epoch 2029 :: Batch 0/1
Batch Loss = 3.049377057989102
2029, epoch_train_loss=3.049377057989102
Epoch 2030
Epoch 2030 :: Batch 0/1
Batch Loss = 3.048943888829991
2030, epoch_train_loss=3.048943888829991
Epoch 2031
Epoch 2031 :: Batch 0/1
Batch Loss = 3.0484722476672745
2031, epoch_train_loss=3.0484722476672745
Epoch 2032
Epoch 2032 :: Batch 0/1
Batch Loss = 3.048005249488214
2032, epoch_train_loss=3.048005249488214
Epoch 2033
Epoch 2033 :: Batch 0/1
Batch Loss = 3.047532445489551
2033, epoch_train_loss=3.047532445489551
Epoch 2034
Epoch 2034 :: Batch 0/1
Batch Loss = 3.047072071934683
2034, epoch_train_loss=3.047072071934683
Epoch 2035
Epoch 2035 :: Batch 0/1
Batch Loss = 3.046682977528525
2035, epoch_train_loss=3.046682977528525
Epoch 2036
Epoch 2036 :: Batch 0/1
Batch Loss = 3.0463451358190126
2036, epoch_train_loss=3.0463451358190126
Epoch 2037
Epoch 2037 :: Batch 0/1
Batch Loss = 3.046329511822881
2037, epoch_train_loss=3.046329511822881
Epoch 2038
Epoch 2038 :: Batch 0/1
Batch Loss = 3.0467159945626605
2038, epoch_train_loss=3.0467159945626605
Epoch 2039
Epoch 2039 :: Batch 0/1
Batch Loss = 3.049141238973814
2039, epoch_train_loss=3.049141238973814
Epoch 2040
Epoch 2040 :: Batch 0/1
Batch Loss = 3.0530289683862737
2040, epoch_train_loss=3.0530289683862737
Epoch 2041
Epoch 2041 :: Batch 0/1
Batch Loss = 3.0702157066640194
2041, epoch_train_loss=3.0702157066640194
Epoch 2042
Epoch 2042 :: Batch 0/1
Batch Loss = 3.072105409593887
2042, epoch_train_loss=3.072105409593887
Epoch 2043
Epoch 2043 :: Batch 0/1
Batch Loss = 3.0998218980657084
2043, epoch_train_loss=3.0998218980657084
Epoch 2044
Epoch 2044 :: Batch 0/1
Batch Loss = 3.054908992314514
2044, epoch_train_loss=3.054908992314514
Epoch 2045
Epoch 2045 :: Batch 0/1
Batch Loss = 3.0512523968002387
2045, epoch_train_loss=3.0512523968002387
Epoch 2046
Epoch 2046 :: Batch 0/1
Batch Loss = 3.0867332473082856
2046, epoch_train_loss=3.0867332473082856
Epoch 2047
Epoch 2047 :: Batch 0/1
Batch Loss = 3.0852590898442895
2047, epoch_train_loss=3.0852590898442895
Epoch 2048
Epoch 2048 :: Batch 0/1
Batch Loss = 3.0857091121134697
2048, epoch_train_loss=3.0857091121134697
Epoch 2049
Epoch 2049 :: Batch 0/1
Batch Loss = 3.050613922047291
2049, epoch_train_loss=3.050613922047291
Epoch 2050
Epoch 2050 :: Batch 0/1
Batch Loss = 3.0438477882711825
2050, epoch_train_loss=3.0438477882711825
Epoch 2051
Epoch 2051 :: Batch 0/1
Batch Loss = 3.0525187300991248
2051, epoch_train_loss=3.0525187300991248
Epoch 2052
Epoch 2052 :: Batch 0/1
Batch Loss = 3.061692524134039
2052, epoch_train_loss=3.061692524134039
Epoch 2053
Epoch 2053 :: Batch 0/1
Batch Loss = 3.0909121923248786
2053, epoch_train_loss=3.0909121923248786
Epoch 2054
Epoch 2054 :: Batch 0/1
Batch Loss = 3.0575134216831947
2054, epoch_train_loss=3.0575134216831947
Epoch 2055
Epoch 2055 :: Batch 0/1
Batch Loss = 3.043412283591773
2055, epoch_train_loss=3.043412283591773
Epoch 2056
Epoch 2056 :: Batch 0/1
Batch Loss = 3.0392123930032877
2056, epoch_train_loss=3.0392123930032877
Epoch 2057
Epoch 2057 :: Batch 0/1
Batch Loss = 3.047836449352128
2057, epoch_train_loss=3.047836449352128
Epoch 2058
Epoch 2058 :: Batch 0/1
Batch Loss = 3.066730224763739
2058, epoch_train_loss=3.066730224763739
Epoch 2059
Epoch 2059 :: Batch 0/1
Batch Loss = 3.078242163601599
2059, epoch_train_loss=3.078242163601599
Epoch 2060
Epoch 2060 :: Batch 0/1
Batch Loss = 3.140918358183288
2060, epoch_train_loss=3.140918358183288
Epoch 2061
Epoch 2061 :: Batch 0/1
Batch Loss = 3.056499795113318
2061, epoch_train_loss=3.056499795113318
Epoch 2062
Epoch 2062 :: Batch 0/1
Batch Loss = 3.1694998604820515
2062, epoch_train_loss=3.1694998604820515
Epoch 2063
Epoch 2063 :: Batch 0/1
Batch Loss = 3.2090358050202537
2063, epoch_train_loss=3.2090358050202537
Epoch 2064
Epoch 2064 :: Batch 0/1
Batch Loss = 3.2164499637761743
2064, epoch_train_loss=3.2164499637761743
Epoch 2065
Epoch 2065 :: Batch 0/1
Batch Loss = 3.147700440288132
2065, epoch_train_loss=3.147700440288132
Epoch 2066
Epoch 2066 :: Batch 0/1
Batch Loss = 3.2105998396715756
2066, epoch_train_loss=3.2105998396715756
Epoch 2067
Epoch 2067 :: Batch 0/1
Batch Loss = 3.100759986046903
2067, epoch_train_loss=3.100759986046903
Epoch 2068
Epoch 2068 :: Batch 0/1
Batch Loss = 3.0951800261807234
2068, epoch_train_loss=3.0951800261807234
Epoch 2069
Epoch 2069 :: Batch 0/1
Batch Loss = 3.1053484621158085
2069, epoch_train_loss=3.1053484621158085
Epoch 2070
Epoch 2070 :: Batch 0/1
Batch Loss = 3.1430042097311413
2070, epoch_train_loss=3.1430042097311413
Epoch 2071
Epoch 2071 :: Batch 0/1
Batch Loss = 3.1642331376453234
2071, epoch_train_loss=3.1642331376453234
Epoch 2072
Epoch 2072 :: Batch 0/1
Batch Loss = 3.2846250759364146
2072, epoch_train_loss=3.2846250759364146
Epoch 2073
Epoch 2073 :: Batch 0/1
Batch Loss = 3.230916913296608
2073, epoch_train_loss=3.230916913296608
Epoch 2074
Epoch 2074 :: Batch 0/1
Batch Loss = 3.2565780939000466
2074, epoch_train_loss=3.2565780939000466
Epoch 2075
Epoch 2075 :: Batch 0/1
Batch Loss = 3.3104534935759666
2075, epoch_train_loss=3.3104534935759666
Epoch 2076
Epoch 2076 :: Batch 0/1
Batch Loss = 3.2619629403592514
2076, epoch_train_loss=3.2619629403592514
Epoch 2077
Epoch 2077 :: Batch 0/1
Batch Loss = 3.197189305797951
2077, epoch_train_loss=3.197189305797951
Epoch 2078
Epoch 2078 :: Batch 0/1
Batch Loss = 3.19968891183447
2078, epoch_train_loss=3.19968891183447
Epoch 2079
Epoch 2079 :: Batch 0/1
Batch Loss = 3.1912925072983644
2079, epoch_train_loss=3.1912925072983644
Epoch 2080
Epoch 2080 :: Batch 0/1
Batch Loss = 3.142613880521055
2080, epoch_train_loss=3.142613880521055
Epoch 2081
Epoch 2081 :: Batch 0/1
Batch Loss = 3.1342180921523175
2081, epoch_train_loss=3.1342180921523175
Epoch 2082
Epoch 2082 :: Batch 0/1
Batch Loss = 3.1583085210552273
2082, epoch_train_loss=3.1583085210552273
Epoch 2083
Epoch 2083 :: Batch 0/1
Batch Loss = 3.130363512411725
2083, epoch_train_loss=3.130363512411725
Epoch 2084
Epoch 2084 :: Batch 0/1
Batch Loss = 3.123689367151704
2084, epoch_train_loss=3.123689367151704
Epoch 2085
Epoch 2085 :: Batch 0/1
Batch Loss = 3.1272493515047537
2085, epoch_train_loss=3.1272493515047537
Epoch 2086
Epoch 2086 :: Batch 0/1
Batch Loss = 3.0962874948292223
2086, epoch_train_loss=3.0962874948292223
Epoch 2087
Epoch 2087 :: Batch 0/1
Batch Loss = 3.074372735279283
2087, epoch_train_loss=3.074372735279283
Epoch 2088
Epoch 2088 :: Batch 0/1
Batch Loss = 3.077656361477288
2088, epoch_train_loss=3.077656361477288
Epoch 2089
Epoch 2089 :: Batch 0/1
Batch Loss = 3.0847503103166947
2089, epoch_train_loss=3.0847503103166947
Epoch 2090
Epoch 2090 :: Batch 0/1
Batch Loss = 3.0905207718926175
2090, epoch_train_loss=3.0905207718926175
Epoch 2091
Epoch 2091 :: Batch 0/1
Batch Loss = 3.072494061586811
2091, epoch_train_loss=3.072494061586811
Epoch 2092
Epoch 2092 :: Batch 0/1
Batch Loss = 3.0872834812751986
2092, epoch_train_loss=3.0872834812751986
Epoch 2093
Epoch 2093 :: Batch 0/1
Batch Loss = 3.067828634805493
2093, epoch_train_loss=3.067828634805493
Epoch 2094
Epoch 2094 :: Batch 0/1
Batch Loss = 3.0854559178316308
2094, epoch_train_loss=3.0854559178316308
Epoch 2095
Epoch 2095 :: Batch 0/1
Batch Loss = 3.094158298842706
2095, epoch_train_loss=3.094158298842706
Epoch 2096
Epoch 2096 :: Batch 0/1
Batch Loss = 3.0581943332156936
2096, epoch_train_loss=3.0581943332156936
Epoch 2097
Epoch 2097 :: Batch 0/1
Batch Loss = 3.0945058532743603
2097, epoch_train_loss=3.0945058532743603
Epoch 2098
Epoch 2098 :: Batch 0/1
Batch Loss = 3.1167639387426638
2098, epoch_train_loss=3.1167639387426638
Epoch 2099
Epoch 2099 :: Batch 0/1
Batch Loss = 3.0677350387218567
2099, epoch_train_loss=3.0677350387218567
Epoch 2100
Epoch 2100 :: Batch 0/1
Batch Loss = 3.132030196721148
2100, epoch_train_loss=3.132030196721148
Epoch 2101
Epoch 2101 :: Batch 0/1
Batch Loss = 3.063523960008561
2101, epoch_train_loss=3.063523960008561
Epoch 2102
Epoch 2102 :: Batch 0/1
Batch Loss = 3.0961977673294996
2102, epoch_train_loss=3.0961977673294996
Epoch 2103
Epoch 2103 :: Batch 0/1
Batch Loss = 3.0612542564816256
2103, epoch_train_loss=3.0612542564816256
Epoch 2104
Epoch 2104 :: Batch 0/1
Batch Loss = 3.096983320307911
2104, epoch_train_loss=3.096983320307911
Epoch 2105
Epoch 2105 :: Batch 0/1
Batch Loss = 3.0603951294355562
2105, epoch_train_loss=3.0603951294355562
Epoch 2106
Epoch 2106 :: Batch 0/1
Batch Loss = 3.084576114508045
2106, epoch_train_loss=3.084576114508045
Epoch 2107
Epoch 2107 :: Batch 0/1
Batch Loss = 3.0587564575907185
2107, epoch_train_loss=3.0587564575907185
Epoch 2108
Epoch 2108 :: Batch 0/1
Batch Loss = 3.084612651016566
2108, epoch_train_loss=3.084612651016566
Epoch 2109
Epoch 2109 :: Batch 0/1
Batch Loss = 3.0513193514623547
2109, epoch_train_loss=3.0513193514623547
Epoch 2110
Epoch 2110 :: Batch 0/1
Batch Loss = 3.0707493833511936
2110, epoch_train_loss=3.0707493833511936
Epoch 2111
Epoch 2111 :: Batch 0/1
Batch Loss = 3.049511237618349
2111, epoch_train_loss=3.049511237618349
Epoch 2112
Epoch 2112 :: Batch 0/1
Batch Loss = 3.05848802007574
2112, epoch_train_loss=3.05848802007574
Epoch 2113
Epoch 2113 :: Batch 0/1
Batch Loss = 3.059473756632054
2113, epoch_train_loss=3.059473756632054
Epoch 2114
Epoch 2114 :: Batch 0/1
Batch Loss = 3.048018346265565
2114, epoch_train_loss=3.048018346265565
Epoch 2115
Epoch 2115 :: Batch 0/1
Batch Loss = 3.0671005868054073
2115, epoch_train_loss=3.0671005868054073
Epoch 2116
Epoch 2116 :: Batch 0/1
Batch Loss = 3.0524264496519353
2116, epoch_train_loss=3.0524264496519353
Epoch 2117
Epoch 2117 :: Batch 0/1
Batch Loss = 3.051442273421989
2117, epoch_train_loss=3.051442273421989
Epoch 2118
Epoch 2118 :: Batch 0/1
Batch Loss = 3.0635382131309363
2118, epoch_train_loss=3.0635382131309363
Epoch 2119
Epoch 2119 :: Batch 0/1
Batch Loss = 3.0485570000714306
2119, epoch_train_loss=3.0485570000714306
Epoch 2120
Epoch 2120 :: Batch 0/1
Batch Loss = 3.046632475028088
2120, epoch_train_loss=3.046632475028088
Epoch 2121
Epoch 2121 :: Batch 0/1
Batch Loss = 3.0576842032538023
2121, epoch_train_loss=3.0576842032538023
Epoch 2122
Epoch 2122 :: Batch 0/1
Batch Loss = 3.048217335971288
2122, epoch_train_loss=3.048217335971288
Epoch 2123
Epoch 2123 :: Batch 0/1
Batch Loss = 3.041208519667556
2123, epoch_train_loss=3.041208519667556
Epoch 2124
Epoch 2124 :: Batch 0/1
Batch Loss = 3.0576492419611365
2124, epoch_train_loss=3.0576492419611365
Epoch 2125
Epoch 2125 :: Batch 0/1
Batch Loss = 3.056450618333516
2125, epoch_train_loss=3.056450618333516
Epoch 2126
Epoch 2126 :: Batch 0/1
Batch Loss = 3.039448953751719
2126, epoch_train_loss=3.039448953751719
Epoch 2127
Epoch 2127 :: Batch 0/1
Batch Loss = 3.0676796969851856
2127, epoch_train_loss=3.0676796969851856
Epoch 2128
Epoch 2128 :: Batch 0/1
Batch Loss = 3.066495078694997
2128, epoch_train_loss=3.066495078694997
Epoch 2129
Epoch 2129 :: Batch 0/1
Batch Loss = 3.0460883154901484
2129, epoch_train_loss=3.0460883154901484
Epoch 2130
Epoch 2130 :: Batch 0/1
Batch Loss = 3.084777357608359
2130, epoch_train_loss=3.084777357608359
Epoch 2131
Epoch 2131 :: Batch 0/1
Batch Loss = 3.0568224553840935
2131, epoch_train_loss=3.0568224553840935
Epoch 2132
Epoch 2132 :: Batch 0/1
Batch Loss = 3.0600538979827436
2132, epoch_train_loss=3.0600538979827436
Epoch 2133
Epoch 2133 :: Batch 0/1
Batch Loss = 3.059154896702651
2133, epoch_train_loss=3.059154896702651
Epoch 2134
Epoch 2134 :: Batch 0/1
Batch Loss = 3.0381689108825762
2134, epoch_train_loss=3.0381689108825762
Epoch 2135
Epoch 2135 :: Batch 0/1
Batch Loss = 3.05491781107098
2135, epoch_train_loss=3.05491781107098
Epoch 2136
Epoch 2136 :: Batch 0/1
Batch Loss = 3.0342305274701467
2136, epoch_train_loss=3.0342305274701467
Epoch 2137
Epoch 2137 :: Batch 0/1
Batch Loss = 3.0549058396159756
2137, epoch_train_loss=3.0549058396159756
Epoch 2138
Epoch 2138 :: Batch 0/1
Batch Loss = 3.056559380179578
2138, epoch_train_loss=3.056559380179578
Epoch 2139
Epoch 2139 :: Batch 0/1
Batch Loss = 3.0363103446556687
2139, epoch_train_loss=3.0363103446556687
Epoch 2140
Epoch 2140 :: Batch 0/1
Batch Loss = 3.0736817941331824
2140, epoch_train_loss=3.0736817941331824
Epoch 2141
Epoch 2141 :: Batch 0/1
Batch Loss = 3.066534381079936
2141, epoch_train_loss=3.066534381079936
Epoch 2142
Epoch 2142 :: Batch 0/1
Batch Loss = 3.0460283474438032
2142, epoch_train_loss=3.0460283474438032
Epoch 2143
Epoch 2143 :: Batch 0/1
Batch Loss = 3.089857295355875
2143, epoch_train_loss=3.089857295355875
Epoch 2144
Epoch 2144 :: Batch 0/1
Batch Loss = 3.053582286694339
2144, epoch_train_loss=3.053582286694339
Epoch 2145
Epoch 2145 :: Batch 0/1
Batch Loss = 3.0593390848676068
2145, epoch_train_loss=3.0593390848676068
Epoch 2146
Epoch 2146 :: Batch 0/1
Batch Loss = 3.056374012595685
2146, epoch_train_loss=3.056374012595685
Epoch 2147
Epoch 2147 :: Batch 0/1
Batch Loss = 3.032249790719463
2147, epoch_train_loss=3.032249790719463
Epoch 2148
Epoch 2148 :: Batch 0/1
Batch Loss = 3.051090162496624
2148, epoch_train_loss=3.051090162496624
Epoch 2149
Epoch 2149 :: Batch 0/1
Batch Loss = 3.0291141021082204
2149, epoch_train_loss=3.0291141021082204
Epoch 2150
Epoch 2150 :: Batch 0/1
Batch Loss = 3.0433519817365697
2150, epoch_train_loss=3.0433519817365697
Epoch 2151
Epoch 2151 :: Batch 0/1
Batch Loss = 3.056095422122056
2151, epoch_train_loss=3.056095422122056
Epoch 2152
Epoch 2152 :: Batch 0/1
Batch Loss = 3.0276292803759532
2152, epoch_train_loss=3.0276292803759532
Epoch 2153
Epoch 2153 :: Batch 0/1
Batch Loss = 3.0649814350432663
2153, epoch_train_loss=3.0649814350432663
Epoch 2154
Epoch 2154 :: Batch 0/1
Batch Loss = 3.0759110676714236
2154, epoch_train_loss=3.0759110676714236
Epoch 2155
Epoch 2155 :: Batch 0/1
Batch Loss = 3.0395285506953855
2155, epoch_train_loss=3.0395285506953855
Epoch 2156
Epoch 2156 :: Batch 0/1
Batch Loss = 3.10702249983247
2156, epoch_train_loss=3.10702249983247
Epoch 2157
Epoch 2157 :: Batch 0/1
Batch Loss = 3.0625859272811926
2157, epoch_train_loss=3.0625859272811926
Epoch 2158
Epoch 2158 :: Batch 0/1
Batch Loss = 3.0730700345311606
2158, epoch_train_loss=3.0730700345311606
Epoch 2159
Epoch 2159 :: Batch 0/1
Batch Loss = 3.050178404576958
2159, epoch_train_loss=3.050178404576958
Epoch 2160
Epoch 2160 :: Batch 0/1
Batch Loss = 3.053550522765787
2160, epoch_train_loss=3.053550522765787
Epoch 2161
Epoch 2161 :: Batch 0/1
Batch Loss = 3.0599673804029535
2161, epoch_train_loss=3.0599673804029535
Epoch 2162
Epoch 2162 :: Batch 0/1
Batch Loss = 3.044221911569445
2162, epoch_train_loss=3.044221911569445
Epoch 2163
Epoch 2163 :: Batch 0/1
Batch Loss = 3.07469631116193
2163, epoch_train_loss=3.07469631116193
Epoch 2164
Epoch 2164 :: Batch 0/1
Batch Loss = 3.030142690902419
2164, epoch_train_loss=3.030142690902419
Epoch 2165
Epoch 2165 :: Batch 0/1
Batch Loss = 3.0491971091290626
2165, epoch_train_loss=3.0491971091290626
Epoch 2166
Epoch 2166 :: Batch 0/1
Batch Loss = 3.035456019829956
2166, epoch_train_loss=3.035456019829956
Epoch 2167
Epoch 2167 :: Batch 0/1
Batch Loss = 3.027929284774016
2167, epoch_train_loss=3.027929284774016
Epoch 2168
Epoch 2168 :: Batch 0/1
Batch Loss = 3.0430841806101836
2168, epoch_train_loss=3.0430841806101836
Epoch 2169
Epoch 2169 :: Batch 0/1
Batch Loss = 3.0229767537985044
2169, epoch_train_loss=3.0229767537985044
Epoch 2170
Epoch 2170 :: Batch 0/1
Batch Loss = 3.037971389126358
2170, epoch_train_loss=3.037971389126358
Epoch 2171
Epoch 2171 :: Batch 0/1
Batch Loss = 3.04433896759953
2171, epoch_train_loss=3.04433896759953
Epoch 2172
Epoch 2172 :: Batch 0/1
Batch Loss = 3.0210546570983987
2172, epoch_train_loss=3.0210546570983987
Epoch 2173
Epoch 2173 :: Batch 0/1
Batch Loss = 3.0536983057132074
2173, epoch_train_loss=3.0536983057132074
Epoch 2174
Epoch 2174 :: Batch 0/1
Batch Loss = 3.0657039336057528
2174, epoch_train_loss=3.0657039336057528
Epoch 2175
Epoch 2175 :: Batch 0/1
Batch Loss = 3.025498860354554
2175, epoch_train_loss=3.025498860354554
Epoch 2176
Epoch 2176 :: Batch 0/1
Batch Loss = 3.1027075714426826
2176, epoch_train_loss=3.1027075714426826
Epoch 2177
Epoch 2177 :: Batch 0/1
Batch Loss = 3.0875181313749978
2177, epoch_train_loss=3.0875181313749978
Epoch 2178
Epoch 2178 :: Batch 0/1
Batch Loss = 3.076001182166369
2178, epoch_train_loss=3.076001182166369
Epoch 2179
Epoch 2179 :: Batch 0/1
Batch Loss = 3.067417075292067
2179, epoch_train_loss=3.067417075292067
Epoch 2180
Epoch 2180 :: Batch 0/1
Batch Loss = 3.0448816942021315
2180, epoch_train_loss=3.0448816942021315
Epoch 2181
Epoch 2181 :: Batch 0/1
Batch Loss = 3.0658925364695016
2181, epoch_train_loss=3.0658925364695016
Epoch 2182
Epoch 2182 :: Batch 0/1
Batch Loss = 3.043228070102411
2182, epoch_train_loss=3.043228070102411
Epoch 2183
Epoch 2183 :: Batch 0/1
Batch Loss = 3.0749237178634403
2183, epoch_train_loss=3.0749237178634403
Epoch 2184
Epoch 2184 :: Batch 0/1
Batch Loss = 3.022232717076307
2184, epoch_train_loss=3.022232717076307
Epoch 2185
Epoch 2185 :: Batch 0/1
Batch Loss = 3.05829801607891
2185, epoch_train_loss=3.05829801607891
Epoch 2186
Epoch 2186 :: Batch 0/1
Batch Loss = 3.0221152974590573
2186, epoch_train_loss=3.0221152974590573
Epoch 2187
Epoch 2187 :: Batch 0/1
Batch Loss = 3.0476900271202134
2187, epoch_train_loss=3.0476900271202134
Epoch 2188
Epoch 2188 :: Batch 0/1
Batch Loss = 3.0418533066109825
2188, epoch_train_loss=3.0418533066109825
Epoch 2189
Epoch 2189 :: Batch 0/1
Batch Loss = 3.024734331839249
2189, epoch_train_loss=3.024734331839249
Epoch 2190
Epoch 2190 :: Batch 0/1
Batch Loss = 3.0560535296838767
2190, epoch_train_loss=3.0560535296838767
Epoch 2191
Epoch 2191 :: Batch 0/1
Batch Loss = 3.0253090829077895
2191, epoch_train_loss=3.0253090829077895
Epoch 2192
Epoch 2192 :: Batch 0/1
Batch Loss = 3.0306798116067535
2192, epoch_train_loss=3.0306798116067535
Epoch 2193
Epoch 2193 :: Batch 0/1
Batch Loss = 3.0362737801062103
2193, epoch_train_loss=3.0362737801062103
Epoch 2194
Epoch 2194 :: Batch 0/1
Batch Loss = 3.0142234320170127
2194, epoch_train_loss=3.0142234320170127
Epoch 2195
Epoch 2195 :: Batch 0/1
Batch Loss = 3.022839955917044
2195, epoch_train_loss=3.022839955917044
Epoch 2196
Epoch 2196 :: Batch 0/1
Batch Loss = 3.024903308326139
2196, epoch_train_loss=3.024903308326139
Epoch 2197
Epoch 2197 :: Batch 0/1
Batch Loss = 3.014028474531179
2197, epoch_train_loss=3.014028474531179
Epoch 2198
Epoch 2198 :: Batch 0/1
Batch Loss = 3.0109683610134685
2198, epoch_train_loss=3.0109683610134685
Epoch 2199
Epoch 2199 :: Batch 0/1
Batch Loss = 3.020182839175617
2199, epoch_train_loss=3.020182839175617
Epoch 2200
Epoch 2200 :: Batch 0/1
Batch Loss = 3.025756990659805
2200, epoch_train_loss=3.025756990659805
Epoch 2201
Epoch 2201 :: Batch 0/1
Batch Loss = 3.0086941746820037
2201, epoch_train_loss=3.0086941746820037
Epoch 2202
Epoch 2202 :: Batch 0/1
Batch Loss = 3.0103556793285065
2202, epoch_train_loss=3.0103556793285065
Epoch 2203
Epoch 2203 :: Batch 0/1
Batch Loss = 3.023410737791958
2203, epoch_train_loss=3.023410737791958
Epoch 2204
Epoch 2204 :: Batch 0/1
Batch Loss = 3.0110990646169946
2204, epoch_train_loss=3.0110990646169946
Epoch 2205
Epoch 2205 :: Batch 0/1
Batch Loss = 3.0044886543959977
2205, epoch_train_loss=3.0044886543959977
Epoch 2206
Epoch 2206 :: Batch 0/1
Batch Loss = 3.0084432862917225
2206, epoch_train_loss=3.0084432862917225
Epoch 2207
Epoch 2207 :: Batch 0/1
Batch Loss = 3.011233164960175
2207, epoch_train_loss=3.011233164960175
Epoch 2208
Epoch 2208 :: Batch 0/1
Batch Loss = 3.012715073401863
2208, epoch_train_loss=3.012715073401863
Epoch 2209
Epoch 2209 :: Batch 0/1
Batch Loss = 3.0047016025011364
2209, epoch_train_loss=3.0047016025011364
Epoch 2210
Epoch 2210 :: Batch 0/1
Batch Loss = 3.001182161162063
2210, epoch_train_loss=3.001182161162063
Epoch 2211
Epoch 2211 :: Batch 0/1
Batch Loss = 3.002207282919562
2211, epoch_train_loss=3.002207282919562
Epoch 2212
Epoch 2212 :: Batch 0/1
Batch Loss = 3.0053853076963395
2212, epoch_train_loss=3.0053853076963395
Epoch 2213
Epoch 2213 :: Batch 0/1
Batch Loss = 3.012459642352589
2213, epoch_train_loss=3.012459642352589
Epoch 2214
Epoch 2214 :: Batch 0/1
Batch Loss = 3.0099119420296105
2214, epoch_train_loss=3.0099119420296105
Epoch 2215
Epoch 2215 :: Batch 0/1
Batch Loss = 3.012980758170782
2215, epoch_train_loss=3.012980758170782
Epoch 2216
Epoch 2216 :: Batch 0/1
Batch Loss = 3.0041791095665404
2216, epoch_train_loss=3.0041791095665404
Epoch 2217
Epoch 2217 :: Batch 0/1
Batch Loss = 3.000127260895917
2217, epoch_train_loss=3.000127260895917
Epoch 2218
Epoch 2218 :: Batch 0/1
Batch Loss = 2.997111754321408
2218, epoch_train_loss=2.997111754321408
Epoch 2219
Epoch 2219 :: Batch 0/1
Batch Loss = 2.9961026877385053
2219, epoch_train_loss=2.9961026877385053
Epoch 2220
Epoch 2220 :: Batch 0/1
Batch Loss = 2.9961157552902677
2220, epoch_train_loss=2.9961157552902677
Epoch 2221
Epoch 2221 :: Batch 0/1
Batch Loss = 2.997825618548275
2221, epoch_train_loss=2.997825618548275
Epoch 2222
Epoch 2222 :: Batch 0/1
Batch Loss = 3.0053447491649834
2222, epoch_train_loss=3.0053447491649834
Epoch 2223
Epoch 2223 :: Batch 0/1
Batch Loss = 3.016965716110275
2223, epoch_train_loss=3.016965716110275
Epoch 2224
Epoch 2224 :: Batch 0/1
Batch Loss = 3.069538138460769
2224, epoch_train_loss=3.069538138460769
Epoch 2225
Epoch 2225 :: Batch 0/1
Batch Loss = 3.006602430531248
2225, epoch_train_loss=3.006602430531248
Epoch 2226
Epoch 2226 :: Batch 0/1
Batch Loss = 2.9966654265114148
2226, epoch_train_loss=2.9966654265114148
Epoch 2227
Epoch 2227 :: Batch 0/1
Batch Loss = 3.0150212053433845
2227, epoch_train_loss=3.0150212053433845
Epoch 2228
Epoch 2228 :: Batch 0/1
Batch Loss = 3.02718547545065
2228, epoch_train_loss=3.02718547545065
Epoch 2229
Epoch 2229 :: Batch 0/1
Batch Loss = 3.073362242587098
2229, epoch_train_loss=3.073362242587098
Epoch 2230
Epoch 2230 :: Batch 0/1
Batch Loss = 2.9966104965763636
2230, epoch_train_loss=2.9966104965763636
Epoch 2231
Epoch 2231 :: Batch 0/1
Batch Loss = 3.049978431860567
2231, epoch_train_loss=3.049978431860567
Epoch 2232
Epoch 2232 :: Batch 0/1
Batch Loss = 3.1706790069406954
2232, epoch_train_loss=3.1706790069406954
Epoch 2233
Epoch 2233 :: Batch 0/1
Batch Loss = 3.090559809824126
2233, epoch_train_loss=3.090559809824126
Epoch 2234
Epoch 2234 :: Batch 0/1
Batch Loss = 3.1898400733938983
2234, epoch_train_loss=3.1898400733938983
Epoch 2235
Epoch 2235 :: Batch 0/1
Batch Loss = 3.048461515925897
2235, epoch_train_loss=3.048461515925897
Epoch 2236
Epoch 2236 :: Batch 0/1
Batch Loss = 3.121285181466649
2236, epoch_train_loss=3.121285181466649
Epoch 2237
Epoch 2237 :: Batch 0/1
Batch Loss = 3.05558317881489
2237, epoch_train_loss=3.05558317881489
Epoch 2238
Epoch 2238 :: Batch 0/1
Batch Loss = 3.091085436616582
2238, epoch_train_loss=3.091085436616582
Epoch 2239
Epoch 2239 :: Batch 0/1
Batch Loss = 3.074796071696953
2239, epoch_train_loss=3.074796071696953
Epoch 2240
Epoch 2240 :: Batch 0/1
Batch Loss = 3.059424828552285
2240, epoch_train_loss=3.059424828552285
Epoch 2241
Epoch 2241 :: Batch 0/1
Batch Loss = 3.059444265744832
2241, epoch_train_loss=3.059444265744832
Epoch 2242
Epoch 2242 :: Batch 0/1
Batch Loss = 3.0586548743461903
2242, epoch_train_loss=3.0586548743461903
Epoch 2243
Epoch 2243 :: Batch 0/1
Batch Loss = 3.0682255982312276
2243, epoch_train_loss=3.0682255982312276
Epoch 2244
Epoch 2244 :: Batch 0/1
Batch Loss = 3.0318817922783405
2244, epoch_train_loss=3.0318817922783405
Epoch 2245
Epoch 2245 :: Batch 0/1
Batch Loss = 3.0550704919842553
2245, epoch_train_loss=3.0550704919842553
Epoch 2246
Epoch 2246 :: Batch 0/1
Batch Loss = 3.0387996020886683
2246, epoch_train_loss=3.0387996020886683
Epoch 2247
Epoch 2247 :: Batch 0/1
Batch Loss = 3.0338161059816557
2247, epoch_train_loss=3.0338161059816557
Epoch 2248
Epoch 2248 :: Batch 0/1
Batch Loss = 3.0386717477201923
2248, epoch_train_loss=3.0386717477201923
Epoch 2249
Epoch 2249 :: Batch 0/1
Batch Loss = 3.014377232006394
2249, epoch_train_loss=3.014377232006394
Epoch 2250
Epoch 2250 :: Batch 0/1
Batch Loss = 3.050430111280119
2250, epoch_train_loss=3.050430111280119
Epoch 2251
Epoch 2251 :: Batch 0/1
Batch Loss = 3.0452737392377687
2251, epoch_train_loss=3.0452737392377687
Epoch 2252
Epoch 2252 :: Batch 0/1
Batch Loss = 3.0178998178753913
2252, epoch_train_loss=3.0178998178753913
Epoch 2253
Epoch 2253 :: Batch 0/1
Batch Loss = 3.0277979202243874
2253, epoch_train_loss=3.0277979202243874
Epoch 2254
Epoch 2254 :: Batch 0/1
Batch Loss = 3.0558655480278394
2254, epoch_train_loss=3.0558655480278394
Epoch 2255
Epoch 2255 :: Batch 0/1
Batch Loss = 3.0082484796773663
2255, epoch_train_loss=3.0082484796773663
Epoch 2256
Epoch 2256 :: Batch 0/1
Batch Loss = 3.017882652900637
2256, epoch_train_loss=3.017882652900637
Epoch 2257
Epoch 2257 :: Batch 0/1
Batch Loss = 3.052849215196116
2257, epoch_train_loss=3.052849215196116
Epoch 2258
Epoch 2258 :: Batch 0/1
Batch Loss = 2.9992827280070777
2258, epoch_train_loss=2.9992827280070777
Epoch 2259
Epoch 2259 :: Batch 0/1
Batch Loss = 3.052213360348145
2259, epoch_train_loss=3.052213360348145
Epoch 2260
Epoch 2260 :: Batch 0/1
Batch Loss = 3.077552470040505
2260, epoch_train_loss=3.077552470040505
Epoch 2261
Epoch 2261 :: Batch 0/1
Batch Loss = 3.020683704513923
2261, epoch_train_loss=3.020683704513923
Epoch 2262
Epoch 2262 :: Batch 0/1
Batch Loss = 3.1237091111492186
2262, epoch_train_loss=3.1237091111492186
Epoch 2263
Epoch 2263 :: Batch 0/1
Batch Loss = 3.0206697365374757
2263, epoch_train_loss=3.0206697365374757
Epoch 2264
Epoch 2264 :: Batch 0/1
Batch Loss = 3.063523458465
2264, epoch_train_loss=3.063523458465
Epoch 2265
Epoch 2265 :: Batch 0/1
Batch Loss = 3.020214432853823
2265, epoch_train_loss=3.020214432853823
Epoch 2266
Epoch 2266 :: Batch 0/1
Batch Loss = 3.0480537019505585
2266, epoch_train_loss=3.0480537019505585
Epoch 2267
Epoch 2267 :: Batch 0/1
Batch Loss = 3.018193608654161
2267, epoch_train_loss=3.018193608654161
Epoch 2268
Epoch 2268 :: Batch 0/1
Batch Loss = 3.0256710091307086
2268, epoch_train_loss=3.0256710091307086
Epoch 2269
Epoch 2269 :: Batch 0/1
Batch Loss = 3.0333163963514513
2269, epoch_train_loss=3.0333163963514513
Epoch 2270
Epoch 2270 :: Batch 0/1
Batch Loss = 2.99934354902905
2270, epoch_train_loss=2.99934354902905
Epoch 2271
Epoch 2271 :: Batch 0/1
Batch Loss = 3.026843256398935
2271, epoch_train_loss=3.026843256398935
Epoch 2272
Epoch 2272 :: Batch 0/1
Batch Loss = 3.0188906097825856
2272, epoch_train_loss=3.0188906097825856
Epoch 2273
Epoch 2273 :: Batch 0/1
Batch Loss = 2.9967325847294113
2273, epoch_train_loss=2.9967325847294113
Epoch 2274
Epoch 2274 :: Batch 0/1
Batch Loss = 3.003966408729742
2274, epoch_train_loss=3.003966408729742
Epoch 2275
Epoch 2275 :: Batch 0/1
Batch Loss = 3.017798319500268
2275, epoch_train_loss=3.017798319500268
Epoch 2276
Epoch 2276 :: Batch 0/1
Batch Loss = 3.0159664872470966
2276, epoch_train_loss=3.0159664872470966
Epoch 2277
Epoch 2277 :: Batch 0/1
Batch Loss = 2.9963176372123232
2277, epoch_train_loss=2.9963176372123232
Epoch 2278
Epoch 2278 :: Batch 0/1
Batch Loss = 2.9983721182651006
2278, epoch_train_loss=2.9983721182651006
Epoch 2279
Epoch 2279 :: Batch 0/1
Batch Loss = 3.0155133649062495
2279, epoch_train_loss=3.0155133649062495
Epoch 2280
Epoch 2280 :: Batch 0/1
Batch Loss = 3.0123524047804535
2280, epoch_train_loss=3.0123524047804535
Epoch 2281
Epoch 2281 :: Batch 0/1
Batch Loss = 3.0042408809262335
2281, epoch_train_loss=3.0042408809262335
Epoch 2282
Epoch 2282 :: Batch 0/1
Batch Loss = 2.9923138556257105
2282, epoch_train_loss=2.9923138556257105
Epoch 2283
Epoch 2283 :: Batch 0/1
Batch Loss = 2.9915258650836987
2283, epoch_train_loss=2.9915258650836987
Epoch 2284
Epoch 2284 :: Batch 0/1
Batch Loss = 2.999080906207227
2284, epoch_train_loss=2.999080906207227
Epoch 2285
Epoch 2285 :: Batch 0/1
Batch Loss = 3.0021174261062984
2285, epoch_train_loss=3.0021174261062984
Epoch 2286
Epoch 2286 :: Batch 0/1
Batch Loss = 3.0025872118294523
2286, epoch_train_loss=3.0025872118294523
Epoch 2287
Epoch 2287 :: Batch 0/1
Batch Loss = 2.993341991662158
2287, epoch_train_loss=2.993341991662158
Epoch 2288
Epoch 2288 :: Batch 0/1
Batch Loss = 2.9887793338232305
2288, epoch_train_loss=2.9887793338232305
Epoch 2289
Epoch 2289 :: Batch 0/1
Batch Loss = 2.9889953440514105
2289, epoch_train_loss=2.9889953440514105
Epoch 2290
Epoch 2290 :: Batch 0/1
Batch Loss = 2.9928606418682477
2290, epoch_train_loss=2.9928606418682477
Epoch 2291
Epoch 2291 :: Batch 0/1
Batch Loss = 3.0025672598987208
2291, epoch_train_loss=3.0025672598987208
Epoch 2292
Epoch 2292 :: Batch 0/1
Batch Loss = 3.0079780917936083
2292, epoch_train_loss=3.0079780917936083
Epoch 2293
Epoch 2293 :: Batch 0/1
Batch Loss = 3.0315724778490973
2293, epoch_train_loss=3.0315724778490973
Epoch 2294
Epoch 2294 :: Batch 0/1
Batch Loss = 3.0046880006603414
2294, epoch_train_loss=3.0046880006603414
Epoch 2295
Epoch 2295 :: Batch 0/1
Batch Loss = 2.9963502360887637
2295, epoch_train_loss=2.9963502360887637
Epoch 2296
Epoch 2296 :: Batch 0/1
Batch Loss = 2.9884550249638426
2296, epoch_train_loss=2.9884550249638426
Epoch 2297
Epoch 2297 :: Batch 0/1
Batch Loss = 2.9857317680201008
2297, epoch_train_loss=2.9857317680201008
Epoch 2298
Epoch 2298 :: Batch 0/1
Batch Loss = 2.9852118297099444
2298, epoch_train_loss=2.9852118297099444
Epoch 2299
Epoch 2299 :: Batch 0/1
Batch Loss = 2.988262151579654
2299, epoch_train_loss=2.988262151579654
Epoch 2300
Epoch 2300 :: Batch 0/1
Batch Loss = 3.0024477287126925
2300, epoch_train_loss=3.0024477287126925
Epoch 2301
Epoch 2301 :: Batch 0/1
Batch Loss = 3.028175385512081
2301, epoch_train_loss=3.028175385512081
Epoch 2302
Epoch 2302 :: Batch 0/1
Batch Loss = 3.1338128477648026
2302, epoch_train_loss=3.1338128477648026
Epoch 2303
Epoch 2303 :: Batch 0/1
Batch Loss = 2.989841822973427
2303, epoch_train_loss=2.989841822973427
Epoch 2304
Epoch 2304 :: Batch 0/1
Batch Loss = 3.2132698237907804
2304, epoch_train_loss=3.2132698237907804
Epoch 2305
Epoch 2305 :: Batch 0/1
Batch Loss = 3.2195413553422196
2305, epoch_train_loss=3.2195413553422196
Epoch 2306
Epoch 2306 :: Batch 0/1
Batch Loss = 3.234721922995575
2306, epoch_train_loss=3.234721922995575
Epoch 2307
Epoch 2307 :: Batch 0/1
Batch Loss = 3.216970533272126
2307, epoch_train_loss=3.216970533272126
Epoch 2308
Epoch 2308 :: Batch 0/1
Batch Loss = 3.2526136535208874
2308, epoch_train_loss=3.2526136535208874
Epoch 2309
Epoch 2309 :: Batch 0/1
Batch Loss = 3.2132060807836242
2309, epoch_train_loss=3.2132060807836242
Epoch 2310
Epoch 2310 :: Batch 0/1
Batch Loss = 3.1975900892070612
2310, epoch_train_loss=3.1975900892070612
Epoch 2311
Epoch 2311 :: Batch 0/1
Batch Loss = 3.200319547036091
2311, epoch_train_loss=3.200319547036091
Epoch 2312
Epoch 2312 :: Batch 0/1
Batch Loss = 3.1958942770583145
2312, epoch_train_loss=3.1958942770583145
Epoch 2313
Epoch 2313 :: Batch 0/1
Batch Loss = 3.1855238921217444
2313, epoch_train_loss=3.1855238921217444
Epoch 2314
Epoch 2314 :: Batch 0/1
Batch Loss = 3.1640815224359513
2314, epoch_train_loss=3.1640815224359513
Epoch 2315
Epoch 2315 :: Batch 0/1
Batch Loss = 3.1239961536308747
2315, epoch_train_loss=3.1239961536308747
Epoch 2316
Epoch 2316 :: Batch 0/1
Batch Loss = 3.1012383675831625
2316, epoch_train_loss=3.1012383675831625
Epoch 2317
Epoch 2317 :: Batch 0/1
Batch Loss = 3.0883068054766243
2317, epoch_train_loss=3.0883068054766243
Epoch 2318
Epoch 2318 :: Batch 0/1
Batch Loss = 3.073830651067919
2318, epoch_train_loss=3.073830651067919
Epoch 2319
Epoch 2319 :: Batch 0/1
Batch Loss = 3.034850156403907
2319, epoch_train_loss=3.034850156403907
Epoch 2320
Epoch 2320 :: Batch 0/1
Batch Loss = 3.0897883730346423
2320, epoch_train_loss=3.0897883730346423
Epoch 2321
Epoch 2321 :: Batch 0/1
Batch Loss = 3.0567138133462666
2321, epoch_train_loss=3.0567138133462666
Epoch 2322
Epoch 2322 :: Batch 0/1
Batch Loss = 3.029560519883216
2322, epoch_train_loss=3.029560519883216
Epoch 2323
Epoch 2323 :: Batch 0/1
Batch Loss = 3.0568582996500915
2323, epoch_train_loss=3.0568582996500915
Epoch 2324
Epoch 2324 :: Batch 0/1
Batch Loss = 3.1140968275676255
2324, epoch_train_loss=3.1140968275676255
Epoch 2325
Epoch 2325 :: Batch 0/1
Batch Loss = 3.167936916824303
2325, epoch_train_loss=3.167936916824303
Epoch 2326
Epoch 2326 :: Batch 0/1
Batch Loss = 3.034890190205924
2326, epoch_train_loss=3.034890190205924
Epoch 2327
Epoch 2327 :: Batch 0/1
Batch Loss = 3.227648164604693
2327, epoch_train_loss=3.227648164604693
Epoch 2328
Epoch 2328 :: Batch 0/1
Batch Loss = 3.0980205493675985
2328, epoch_train_loss=3.0980205493675985
Epoch 2329
Epoch 2329 :: Batch 0/1
Batch Loss = 3.1628467840508687
2329, epoch_train_loss=3.1628467840508687
Epoch 2330
Epoch 2330 :: Batch 0/1
Batch Loss = 3.084709227718816
2330, epoch_train_loss=3.084709227718816
Epoch 2331
Epoch 2331 :: Batch 0/1
Batch Loss = 3.100087977612231
2331, epoch_train_loss=3.100087977612231
Epoch 2332
Epoch 2332 :: Batch 0/1
Batch Loss = 3.123259529071969
2332, epoch_train_loss=3.123259529071969
Epoch 2333
Epoch 2333 :: Batch 0/1
Batch Loss = 3.0816819333265175
2333, epoch_train_loss=3.0816819333265175
Epoch 2334
Epoch 2334 :: Batch 0/1
Batch Loss = 3.058278115037004
2334, epoch_train_loss=3.058278115037004
Epoch 2335
Epoch 2335 :: Batch 0/1
Batch Loss = 3.1008525372099087
2335, epoch_train_loss=3.1008525372099087
Epoch 2336
Epoch 2336 :: Batch 0/1
Batch Loss = 3.0606286303815344
2336, epoch_train_loss=3.0606286303815344
Epoch 2337
Epoch 2337 :: Batch 0/1
Batch Loss = 3.0773228980070004
2337, epoch_train_loss=3.0773228980070004
Epoch 2338
Epoch 2338 :: Batch 0/1
Batch Loss = 3.029775687151822
2338, epoch_train_loss=3.029775687151822
Epoch 2339
Epoch 2339 :: Batch 0/1
Batch Loss = 3.0514228145906785
2339, epoch_train_loss=3.0514228145906785
Epoch 2340
Epoch 2340 :: Batch 0/1
Batch Loss = 3.0165283364812066
2340, epoch_train_loss=3.0165283364812066
Epoch 2341
Epoch 2341 :: Batch 0/1
Batch Loss = 3.0514700478977095
2341, epoch_train_loss=3.0514700478977095
Epoch 2342
Epoch 2342 :: Batch 0/1
Batch Loss = 3.0172409640873346
2342, epoch_train_loss=3.0172409640873346
Epoch 2343
Epoch 2343 :: Batch 0/1
Batch Loss = 3.0462684895599255
2343, epoch_train_loss=3.0462684895599255
Epoch 2344
Epoch 2344 :: Batch 0/1
Batch Loss = 3.0186793043096447
2344, epoch_train_loss=3.0186793043096447
Epoch 2345
Epoch 2345 :: Batch 0/1
Batch Loss = 3.037114006812418
2345, epoch_train_loss=3.037114006812418
Epoch 2346
Epoch 2346 :: Batch 0/1
Batch Loss = 3.0345727261974944
2346, epoch_train_loss=3.0345727261974944
Epoch 2347
Epoch 2347 :: Batch 0/1
Batch Loss = 3.0124029509031307
2347, epoch_train_loss=3.0124029509031307
Epoch 2348
Epoch 2348 :: Batch 0/1
Batch Loss = 3.0509367728537593
2348, epoch_train_loss=3.0509367728537593
Epoch 2349
Epoch 2349 :: Batch 0/1
Batch Loss = 3.0436283124202266
2349, epoch_train_loss=3.0436283124202266
Epoch 2350
Epoch 2350 :: Batch 0/1
Batch Loss = 3.0078414858370395
2350, epoch_train_loss=3.0078414858370395
Epoch 2351
Epoch 2351 :: Batch 0/1
Batch Loss = 3.070292177324185
2351, epoch_train_loss=3.070292177324185
Epoch 2352
Epoch 2352 :: Batch 0/1
Batch Loss = 3.0453466339743107
2352, epoch_train_loss=3.0453466339743107
Epoch 2353
Epoch 2353 :: Batch 0/1
Batch Loss = 3.015079818002644
2353, epoch_train_loss=3.015079818002644
Epoch 2354
Epoch 2354 :: Batch 0/1
Batch Loss = 3.062017118704693
2354, epoch_train_loss=3.062017118704693
Epoch 2355
Epoch 2355 :: Batch 0/1
Batch Loss = 3.0056455690775237
2355, epoch_train_loss=3.0056455690775237
Epoch 2356
Epoch 2356 :: Batch 0/1
Batch Loss = 3.0430930298631518
2356, epoch_train_loss=3.0430930298631518
Epoch 2357
Epoch 2357 :: Batch 0/1
Batch Loss = 3.0124058875408632
2357, epoch_train_loss=3.0124058875408632
Epoch 2358
Epoch 2358 :: Batch 0/1
Batch Loss = 3.033875235371923
2358, epoch_train_loss=3.033875235371923
Epoch 2359
Epoch 2359 :: Batch 0/1
Batch Loss = 3.0144014257158696
2359, epoch_train_loss=3.0144014257158696
Epoch 2360
Epoch 2360 :: Batch 0/1
Batch Loss = 3.0218729286494614
2360, epoch_train_loss=3.0218729286494614
Epoch 2361
Epoch 2361 :: Batch 0/1
Batch Loss = 3.0119374884536256
2361, epoch_train_loss=3.0119374884536256
Epoch 2362
Epoch 2362 :: Batch 0/1
Batch Loss = 3.0211178846471554
2362, epoch_train_loss=3.0211178846471554
Epoch 2363
Epoch 2363 :: Batch 0/1
Batch Loss = 3.0005248416083625
2363, epoch_train_loss=3.0005248416083625
Epoch 2364
Epoch 2364 :: Batch 0/1
Batch Loss = 3.0210355031438763
2364, epoch_train_loss=3.0210355031438763
Epoch 2365
Epoch 2365 :: Batch 0/1
Batch Loss = 2.992851863743994
2365, epoch_train_loss=2.992851863743994
Epoch 2366
Epoch 2366 :: Batch 0/1
Batch Loss = 3.012553204194706
2366, epoch_train_loss=3.012553204194706
Epoch 2367
Epoch 2367 :: Batch 0/1
Batch Loss = 3.015562342106355
2367, epoch_train_loss=3.015562342106355
Epoch 2368
Epoch 2368 :: Batch 0/1
Batch Loss = 2.994719970479409
2368, epoch_train_loss=2.994719970479409
Epoch 2369
Epoch 2369 :: Batch 0/1
Batch Loss = 3.0304733199031633
2369, epoch_train_loss=3.0304733199031633
Epoch 2370
Epoch 2370 :: Batch 0/1
Batch Loss = 3.0274523990716893
2370, epoch_train_loss=3.0274523990716893
Epoch 2371
Epoch 2371 :: Batch 0/1
Batch Loss = 2.9971150943279374
2371, epoch_train_loss=2.9971150943279374
Epoch 2372
Epoch 2372 :: Batch 0/1
Batch Loss = 3.06065111093964
2372, epoch_train_loss=3.06065111093964
Epoch 2373
Epoch 2373 :: Batch 0/1
Batch Loss = 3.050488950587975
2373, epoch_train_loss=3.050488950587975
Epoch 2374
Epoch 2374 :: Batch 0/1
Batch Loss = 3.0132645464644847
2374, epoch_train_loss=3.0132645464644847
Epoch 2375
Epoch 2375 :: Batch 0/1
Batch Loss = 3.090098426811299
2375, epoch_train_loss=3.090098426811299
Epoch 2376
Epoch 2376 :: Batch 0/1
Batch Loss = 3.0019403320547324
2376, epoch_train_loss=3.0019403320547324
Epoch 2377
Epoch 2377 :: Batch 0/1
Batch Loss = 3.0394933811968263
2377, epoch_train_loss=3.0394933811968263
Epoch 2378
Epoch 2378 :: Batch 0/1
Batch Loss = 3.001121307587963
2378, epoch_train_loss=3.001121307587963
Epoch 2379
Epoch 2379 :: Batch 0/1
Batch Loss = 3.0318462742352774
2379, epoch_train_loss=3.0318462742352774
Epoch 2380
Epoch 2380 :: Batch 0/1
Batch Loss = 3.000977737298096
2380, epoch_train_loss=3.000977737298096
Epoch 2381
Epoch 2381 :: Batch 0/1
Batch Loss = 3.0250059760191417
2381, epoch_train_loss=3.0250059760191417
Epoch 2382
Epoch 2382 :: Batch 0/1
Batch Loss = 2.9972233691101575
2382, epoch_train_loss=2.9972233691101575
Epoch 2383
Epoch 2383 :: Batch 0/1
Batch Loss = 3.0200785320820325
2383, epoch_train_loss=3.0200785320820325
Epoch 2384
Epoch 2384 :: Batch 0/1
Batch Loss = 2.9952037418101116
2384, epoch_train_loss=2.9952037418101116
Epoch 2385
Epoch 2385 :: Batch 0/1
Batch Loss = 3.011007238318365
2385, epoch_train_loss=3.011007238318365
Epoch 2386
Epoch 2386 :: Batch 0/1
Batch Loss = 2.996318268425441
2386, epoch_train_loss=2.996318268425441
Epoch 2387
Epoch 2387 :: Batch 0/1
Batch Loss = 2.9957281580512656
2387, epoch_train_loss=2.9957281580512656
Epoch 2388
Epoch 2388 :: Batch 0/1
Batch Loss = 3.0057841400619143
2388, epoch_train_loss=3.0057841400619143
Epoch 2389
Epoch 2389 :: Batch 0/1
Batch Loss = 2.987679410129487
2389, epoch_train_loss=2.987679410129487
Epoch 2390
Epoch 2390 :: Batch 0/1
Batch Loss = 3.0016084787108746
2390, epoch_train_loss=3.0016084787108746
Epoch 2391
Epoch 2391 :: Batch 0/1
Batch Loss = 3.0069041911648693
2391, epoch_train_loss=3.0069041911648693
Epoch 2392
Epoch 2392 :: Batch 0/1
Batch Loss = 2.9861598741591413
2392, epoch_train_loss=2.9861598741591413
Epoch 2393
Epoch 2393 :: Batch 0/1
Batch Loss = 3.0055090078655495
2393, epoch_train_loss=3.0055090078655495
Epoch 2394
Epoch 2394 :: Batch 0/1
Batch Loss = 3.0180739434682593
2394, epoch_train_loss=3.0180739434682593
Epoch 2395
Epoch 2395 :: Batch 0/1
Batch Loss = 2.984457122806011
2395, epoch_train_loss=2.984457122806011
Epoch 2396
Epoch 2396 :: Batch 0/1
Batch Loss = 3.011646557821955
2396, epoch_train_loss=3.011646557821955
Epoch 2397
Epoch 2397 :: Batch 0/1
Batch Loss = 3.0371951774382
2397, epoch_train_loss=3.0371951774382
Epoch 2398
Epoch 2398 :: Batch 0/1
Batch Loss = 2.9843047822834103
2398, epoch_train_loss=2.9843047822834103
Epoch 2399
Epoch 2399 :: Batch 0/1
Batch Loss = 3.052848263463362
2399, epoch_train_loss=3.052848263463362
Epoch 2400
Epoch 2400 :: Batch 0/1
Batch Loss = 3.0562790651204903
2400, epoch_train_loss=3.0562790651204903
Epoch 2401
Epoch 2401 :: Batch 0/1
Batch Loss = 3.013622802116589
2401, epoch_train_loss=3.013622802116589
Epoch 2402
Epoch 2402 :: Batch 0/1
Batch Loss = 3.0839365808904398
2402, epoch_train_loss=3.0839365808904398
Epoch 2403
Epoch 2403 :: Batch 0/1
Batch Loss = 2.993186550690119
2403, epoch_train_loss=2.993186550690119
Epoch 2404
Epoch 2404 :: Batch 0/1
Batch Loss = 3.051487835387515
2404, epoch_train_loss=3.051487835387515
Epoch 2405
Epoch 2405 :: Batch 0/1
Batch Loss = 3.000191846596917
2405, epoch_train_loss=3.000191846596917
Epoch 2406
Epoch 2406 :: Batch 0/1
Batch Loss = 3.0401358746136604
2406, epoch_train_loss=3.0401358746136604
Epoch 2407
Epoch 2407 :: Batch 0/1
Batch Loss = 3.0078838282874467
2407, epoch_train_loss=3.0078838282874467
Epoch 2408
Epoch 2408 :: Batch 0/1
Batch Loss = 3.024364830767574
2408, epoch_train_loss=3.024364830767574
Epoch 2409
Epoch 2409 :: Batch 0/1
Batch Loss = 3.006594411411158
2409, epoch_train_loss=3.006594411411158
Epoch 2410
Epoch 2410 :: Batch 0/1
Batch Loss = 3.0192785013999526
2410, epoch_train_loss=3.0192785013999526
Epoch 2411
Epoch 2411 :: Batch 0/1
Batch Loss = 3.004683984939363
2411, epoch_train_loss=3.004683984939363
Epoch 2412
Epoch 2412 :: Batch 0/1
Batch Loss = 3.014980177377746
2412, epoch_train_loss=3.014980177377746
Epoch 2413
Epoch 2413 :: Batch 0/1
Batch Loss = 2.994827267806722
2413, epoch_train_loss=2.994827267806722
Epoch 2414
Epoch 2414 :: Batch 0/1
Batch Loss = 3.019201440128109
2414, epoch_train_loss=3.019201440128109
Epoch 2415
Epoch 2415 :: Batch 0/1
Batch Loss = 2.989227009423516
2415, epoch_train_loss=2.989227009423516
Epoch 2416
Epoch 2416 :: Batch 0/1
Batch Loss = 3.016488803754443
2416, epoch_train_loss=3.016488803754443
Epoch 2417
Epoch 2417 :: Batch 0/1
Batch Loss = 2.987953796180181
2417, epoch_train_loss=2.987953796180181
Epoch 2418
Epoch 2418 :: Batch 0/1
Batch Loss = 3.0038787702718786
2418, epoch_train_loss=3.0038787702718786
Epoch 2419
Epoch 2419 :: Batch 0/1
Batch Loss = 3.0010113507579703
2419, epoch_train_loss=3.0010113507579703
Epoch 2420
Epoch 2420 :: Batch 0/1
Batch Loss = 2.9859858787532043
2420, epoch_train_loss=2.9859858787532043
Epoch 2421
Epoch 2421 :: Batch 0/1
Batch Loss = 3.007520631957938
2421, epoch_train_loss=3.007520631957938
Epoch 2422
Epoch 2422 :: Batch 0/1
Batch Loss = 2.991336433857082
2422, epoch_train_loss=2.991336433857082
Epoch 2423
Epoch 2423 :: Batch 0/1
Batch Loss = 2.984758952251604
2423, epoch_train_loss=2.984758952251604
Epoch 2424
Epoch 2424 :: Batch 0/1
Batch Loss = 3.0012341504068663
2424, epoch_train_loss=3.0012341504068663
Epoch 2425
Epoch 2425 :: Batch 0/1
Batch Loss = 2.9873224598017183
2425, epoch_train_loss=2.9873224598017183
Epoch 2426
Epoch 2426 :: Batch 0/1
Batch Loss = 2.982098439085546
2426, epoch_train_loss=2.982098439085546
Epoch 2427
Epoch 2427 :: Batch 0/1
Batch Loss = 2.9967148944527726
2427, epoch_train_loss=2.9967148944527726
Epoch 2428
Epoch 2428 :: Batch 0/1
Batch Loss = 2.9909808149449413
2428, epoch_train_loss=2.9909808149449413
Epoch 2429
Epoch 2429 :: Batch 0/1
Batch Loss = 2.9789952255825836
2429, epoch_train_loss=2.9789952255825836
Epoch 2430
Epoch 2430 :: Batch 0/1
Batch Loss = 2.9943521980802883
2430, epoch_train_loss=2.9943521980802883
Epoch 2431
Epoch 2431 :: Batch 0/1
Batch Loss = 2.9991556009436398
2431, epoch_train_loss=2.9991556009436398
Epoch 2432
Epoch 2432 :: Batch 0/1
Batch Loss = 2.9783831212403107
2432, epoch_train_loss=2.9783831212403107
Epoch 2433
Epoch 2433 :: Batch 0/1
Batch Loss = 2.9904714463782853
2433, epoch_train_loss=2.9904714463782853
Epoch 2434
Epoch 2434 :: Batch 0/1
Batch Loss = 3.0092609064117157
2434, epoch_train_loss=3.0092609064117157
Epoch 2435
Epoch 2435 :: Batch 0/1
Batch Loss = 2.9801102606301604
2435, epoch_train_loss=2.9801102606301604
Epoch 2436
Epoch 2436 :: Batch 0/1
Batch Loss = 2.9846713385306813
2436, epoch_train_loss=2.9846713385306813
Epoch 2437
Epoch 2437 :: Batch 0/1
Batch Loss = 3.008580007062328
2437, epoch_train_loss=3.008580007062328
Epoch 2438
Epoch 2438 :: Batch 0/1
Batch Loss = 2.983529119056767
2438, epoch_train_loss=2.983529119056767
Epoch 2439
Epoch 2439 :: Batch 0/1
Batch Loss = 2.976986668577335
2439, epoch_train_loss=2.976986668577335
Epoch 2440
Epoch 2440 :: Batch 0/1
Batch Loss = 2.990579376855046
2440, epoch_train_loss=2.990579376855046
Epoch 2441
Epoch 2441 :: Batch 0/1
Batch Loss = 2.9900424887529087
2441, epoch_train_loss=2.9900424887529087
Epoch 2442
Epoch 2442 :: Batch 0/1
Batch Loss = 2.9845481875875173
2442, epoch_train_loss=2.9845481875875173
Epoch 2443
Epoch 2443 :: Batch 0/1
Batch Loss = 2.9753850349427733
2443, epoch_train_loss=2.9753850349427733
Epoch 2444
Epoch 2444 :: Batch 0/1
Batch Loss = 2.976813513612598
2444, epoch_train_loss=2.976813513612598
Epoch 2445
Epoch 2445 :: Batch 0/1
Batch Loss = 2.9857814616174507
2445, epoch_train_loss=2.9857814616174507
Epoch 2446
Epoch 2446 :: Batch 0/1
Batch Loss = 2.986132393424963
2446, epoch_train_loss=2.986132393424963
Epoch 2447
Epoch 2447 :: Batch 0/1
Batch Loss = 2.984688492280026
2447, epoch_train_loss=2.984688492280026
Epoch 2448
Epoch 2448 :: Batch 0/1
Batch Loss = 2.9762741414908676
2448, epoch_train_loss=2.9762741414908676
Epoch 2449
Epoch 2449 :: Batch 0/1
Batch Loss = 2.9737013427550836
2449, epoch_train_loss=2.9737013427550836
Epoch 2450
Epoch 2450 :: Batch 0/1
Batch Loss = 2.976369043639139
2450, epoch_train_loss=2.976369043639139
Epoch 2451
Epoch 2451 :: Batch 0/1
Batch Loss = 2.9799418596508422
2451, epoch_train_loss=2.9799418596508422
Epoch 2452
Epoch 2452 :: Batch 0/1
Batch Loss = 2.983016668488862
2452, epoch_train_loss=2.983016668488862
Epoch 2453
Epoch 2453 :: Batch 0/1
Batch Loss = 2.9788255216235417
2453, epoch_train_loss=2.9788255216235417
Epoch 2454
Epoch 2454 :: Batch 0/1
Batch Loss = 2.975635426300276
2454, epoch_train_loss=2.975635426300276
Epoch 2455
Epoch 2455 :: Batch 0/1
Batch Loss = 2.972905504978627
2455, epoch_train_loss=2.972905504978627
Epoch 2456
Epoch 2456 :: Batch 0/1
Batch Loss = 2.9723681545154728
2456, epoch_train_loss=2.9723681545154728
Epoch 2457
Epoch 2457 :: Batch 0/1
Batch Loss = 2.9735749934448177
2457, epoch_train_loss=2.9735749934448177
Epoch 2458
Epoch 2458 :: Batch 0/1
Batch Loss = 2.9757830511773427
2458, epoch_train_loss=2.9757830511773427
Epoch 2459
Epoch 2459 :: Batch 0/1
Batch Loss = 2.980189012187932
2459, epoch_train_loss=2.980189012187932
Epoch 2460
Epoch 2460 :: Batch 0/1
Batch Loss = 2.982152128061384
2460, epoch_train_loss=2.982152128061384
Epoch 2461
Epoch 2461 :: Batch 0/1
Batch Loss = 2.988816544405569
2461, epoch_train_loss=2.988816544405569
Epoch 2462
Epoch 2462 :: Batch 0/1
Batch Loss = 2.9843442748363187
2462, epoch_train_loss=2.9843442748363187
Epoch 2463
Epoch 2463 :: Batch 0/1
Batch Loss = 2.9848615970066423
2463, epoch_train_loss=2.9848615970066423
Epoch 2464
Epoch 2464 :: Batch 0/1
Batch Loss = 2.9779760392519217
2464, epoch_train_loss=2.9779760392519217
Epoch 2465
Epoch 2465 :: Batch 0/1
Batch Loss = 2.974679501944313
2465, epoch_train_loss=2.974679501944313
Epoch 2466
Epoch 2466 :: Batch 0/1
Batch Loss = 2.9718681071984787
2466, epoch_train_loss=2.9718681071984787
Epoch 2467
Epoch 2467 :: Batch 0/1
Batch Loss = 2.970695589842057
2467, epoch_train_loss=2.970695589842057
Epoch 2468
Epoch 2468 :: Batch 0/1
Batch Loss = 2.970674941230517
2468, epoch_train_loss=2.970674941230517
Epoch 2469
Epoch 2469 :: Batch 0/1
Batch Loss = 2.9715865917016395
2469, epoch_train_loss=2.9715865917016395
Epoch 2470
Epoch 2470 :: Batch 0/1
Batch Loss = 2.9742281342818657
2470, epoch_train_loss=2.9742281342818657
Epoch 2471
Epoch 2471 :: Batch 0/1
Batch Loss = 2.978938574097307
2471, epoch_train_loss=2.978938574097307
Epoch 2472
Epoch 2472 :: Batch 0/1
Batch Loss = 2.9939754269005956
2472, epoch_train_loss=2.9939754269005956
Epoch 2473
Epoch 2473 :: Batch 0/1
Batch Loss = 2.999611222938948
2473, epoch_train_loss=2.999611222938948
Epoch 2474
Epoch 2474 :: Batch 0/1
Batch Loss = 3.024661399137519
2474, epoch_train_loss=3.024661399137519
Epoch 2475
Epoch 2475 :: Batch 0/1
Batch Loss = 2.9835720068889593
2475, epoch_train_loss=2.9835720068889593
Epoch 2476
Epoch 2476 :: Batch 0/1
Batch Loss = 2.971215992397652
2476, epoch_train_loss=2.971215992397652
Epoch 2477
Epoch 2477 :: Batch 0/1
Batch Loss = 2.9792212427673355
2477, epoch_train_loss=2.9792212427673355
Epoch 2478
Epoch 2478 :: Batch 0/1
Batch Loss = 2.99177452312598
2478, epoch_train_loss=2.99177452312598
Epoch 2479
Epoch 2479 :: Batch 0/1
Batch Loss = 3.0052230508741897
2479, epoch_train_loss=3.0052230508741897
Epoch 2480
Epoch 2480 :: Batch 0/1
Batch Loss = 2.9805953350826373
2480, epoch_train_loss=2.9805953350826373
Epoch 2481
Epoch 2481 :: Batch 0/1
Batch Loss = 2.9706757300865663
2481, epoch_train_loss=2.9706757300865663
Epoch 2482
Epoch 2482 :: Batch 0/1
Batch Loss = 2.97645365252193
2482, epoch_train_loss=2.97645365252193
Epoch 2483
Epoch 2483 :: Batch 0/1
Batch Loss = 2.987540884680912
2483, epoch_train_loss=2.987540884680912
Epoch 2484
Epoch 2484 :: Batch 0/1
Batch Loss = 2.99492779052589
2484, epoch_train_loss=2.99492779052589
Epoch 2485
Epoch 2485 :: Batch 0/1
Batch Loss = 2.9792135380187386
2485, epoch_train_loss=2.9792135380187386
Epoch 2486
Epoch 2486 :: Batch 0/1
Batch Loss = 2.970728676990036
2486, epoch_train_loss=2.970728676990036
Epoch 2487
Epoch 2487 :: Batch 0/1
Batch Loss = 2.973042366845978
2487, epoch_train_loss=2.973042366845978
Epoch 2488
Epoch 2488 :: Batch 0/1
Batch Loss = 2.9803943813116875
2488, epoch_train_loss=2.9803943813116875
Epoch 2489
Epoch 2489 :: Batch 0/1
Batch Loss = 2.9844765542187743
2489, epoch_train_loss=2.9844765542187743
Epoch 2490
Epoch 2490 :: Batch 0/1
Batch Loss = 2.976496530203235
2490, epoch_train_loss=2.976496530203235
Epoch 2491
Epoch 2491 :: Batch 0/1
Batch Loss = 2.9704900085325656
2491, epoch_train_loss=2.9704900085325656
Epoch 2492
Epoch 2492 :: Batch 0/1
Batch Loss = 2.969693704857018
2492, epoch_train_loss=2.969693704857018
Epoch 2493
Epoch 2493 :: Batch 0/1
Batch Loss = 2.9732272901800902
2493, epoch_train_loss=2.9732272901800902
Epoch 2494
Epoch 2494 :: Batch 0/1
Batch Loss = 2.9789314410277505
2494, epoch_train_loss=2.9789314410277505
Epoch 2495
Epoch 2495 :: Batch 0/1
Batch Loss = 2.9804259495992915
2495, epoch_train_loss=2.9804259495992915
Epoch 2496
Epoch 2496 :: Batch 0/1
Batch Loss = 2.9810871439671898
2496, epoch_train_loss=2.9810871439671898
Epoch 2497
Epoch 2497 :: Batch 0/1
Batch Loss = 2.9758584590981854
2497, epoch_train_loss=2.9758584590981854
Epoch 2498
Epoch 2498 :: Batch 0/1
Batch Loss = 2.9716845240978813
2498, epoch_train_loss=2.9716845240978813
Epoch 2499
Epoch 2499 :: Batch 0/1
Batch Loss = 2.9684784806123794
2499, epoch_train_loss=2.9684784806123794
