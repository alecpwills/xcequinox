/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting "B3LYP_WITH_VWN5 = True" in pyscf_conf.py
  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802e680> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802e680> in UKS object of <class 'pyscf.dft.uks.UKS'>
WARNING: External module "mldftdat" required for non-local descriptor use.
<pyscf.gto.mole.Mole object at 0x7ffeb802e680> [['P', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb802caf0> [['N', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb802e740> [['H', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb802e8c0> [['Li', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb802e350> [['O', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb802ead0> [['Cl', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb802f8b0> [['Al', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb802d480> [['S', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb802ec80> [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb802e2f0> [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb802e4a0> [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb802e1a0> [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb802d9c0> [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb802ddb0> [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb802ce50> [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb802cd30> [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb802cb50> [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb802d030> [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb802f2e0> [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb802c070> [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb802dae0> [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb802f850> [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb802f550> [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb802e320> [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb802ee30> [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]] 5
<pyscf.gto.mole.Mole object at 0x7ffeb802dd20> [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb802fdc0> [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]] 5
mol:  [['P', array([0., 0., 0.])]]
converged SCF energy = -341.104145992717  <S^2> = 3.7502984  2S+1 = 4.0001492
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802caf0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802caf0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-3.48053232e-03 -9.22981806e-04 -2.09507924e-03 ... -1.11294850e+01
 -1.11294850e+01 -1.11294850e+01] = SCAN,
rho_a.shape=(6, 6320), rho_b.shape=(6, 6320)
fxc_a.shape=(6320,), fxc_b.shape=(6320,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12640), fxc.shape=(12640,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(2, 6320, 15)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12640, 15)
rho_filt.shape=(12640,)
mol:  [['N', array([0., 0., 0.])]]
converged SCF energy = -54.5289742046674  <S^2> = 3.7524945  2S+1 = 4.0012471
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802e740> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802e740> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5016, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.10256797e-03 -5.98013179e-04 -6.71209617e-05 ... -5.03581543e+00
 -5.03581543e+00 -5.03581543e+00] = SCAN,
rho_a.shape=(6, 5016), rho_b.shape=(6, 5016)
fxc_a.shape=(5016,), fxc_b.shape=(5016,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 10032), fxc.shape=(10032,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5016, 22)
rho shape (4, 4, 2, 5016)
rho_filt shape: (5016,)
get descriptors tdrho.shape=(2, 5016, 15)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(10032, 15)
rho_filt.shape=(10032,)
mol:  [['H', array([0., 0., 0.])]]
converged SCF energy = -0.499812984008539  <S^2> = 0.75  2S+1 = 2
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802e8c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802e8c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 7, 7)
ao.shape (10, 2440, 7)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.59173730e-03 -7.67300285e-04 -6.09330303e-05 ... -7.27736608e-01
 -7.27736608e-01 -7.27736608e-01] = SCAN,
rho_a.shape=(6, 2440), rho_b.shape=(6, 2440)
fxc_a.shape=(2440,), fxc_b.shape=(2440,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 2440), fxc.shape=(2440,)
get_data, dm shape = (2, 7, 7)
ao_eval.shape=(4, 2440, 7)
rho shape (4, 4, 2, 2440)
rho_filt shape: (2440,)
get descriptors tdrho.shape=(2, 2440, 15)
mol:  [['Li', array([0., 0., 0.])]]
converged SCF energy = -7.46006188627842  <S^2> = 0.75000049  2S+1 = 2.0000005
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802e350> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802e350> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 4592, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-6.71507910e-03 -1.45299376e-03 -1.45299376e-03 ... -1.46930969e-02
 -2.05021258e+00 -2.05021258e+00] = SCAN,
rho_a.shape=(6, 4592), rho_b.shape=(6, 4592)
fxc_a.shape=(4592,), fxc_b.shape=(4592,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 9184), fxc.shape=(9184,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 4592, 22)
rho shape (4, 4, 2, 4592)
rho_filt shape: (4592,)
get descriptors tdrho.shape=(2, 4592, 15)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(9184, 15)
rho_filt.shape=(9184,)
mol:  [['O', array([0., 0., 0.])]]
converged SCF energy = -75.0033774512088  <S^2> = 2.0027452  2S+1 = 3.0018296
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802ead0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802ead0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5040, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-7.08362723e-04 -1.23159784e-04 -6.18173540e-06 ... -5.78388652e+00
 -5.78388652e+00 -5.78388652e+00] = SCAN,
rho_a.shape=(6, 5040), rho_b.shape=(6, 5040)
fxc_a.shape=(5040,), fxc_b.shape=(5040,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 10080), fxc.shape=(10080,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5040, 22)
rho shape (4, 4, 2, 5040)
rho_filt shape: (5040,)
get descriptors tdrho.shape=(2, 5040, 15)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(10080, 15)
rho_filt.shape=(10080,)
mol:  [['Cl', array([0., 0., 0.])]]
converged SCF energy = -459.957577122153  <S^2> = 0.7516194  2S+1 = 2.0016187
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802f8b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802f8b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6152, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.70217241e-04 -1.00412833e-03 -3.55938008e-04 ... -1.26646370e+01
 -1.26646370e+01 -1.26646370e+01] = SCAN,
rho_a.shape=(6, 6152), rho_b.shape=(6, 6152)
fxc_a.shape=(6152,), fxc_b.shape=(6152,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12304), fxc.shape=(12304,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6152, 30)
rho shape (4, 4, 2, 6152)
rho_filt shape: (6152,)
get descriptors tdrho.shape=(2, 6152, 15)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12304, 15)
rho_filt.shape=(12304,)
mol:  [['Al', array([0., 0., 0.])]]
converged SCF energy = -242.226560989229  <S^2> = 0.75226414  2S+1 = 2.0022629
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802d480> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802d480> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6088, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.39564124e-02 -8.69586045e-03 -4.30171315e-03 ... -1.39781083e-04
 -1.04899399e-03 -7.75352742e-05] = SCAN,
rho_a.shape=(6, 6088), rho_b.shape=(6, 6088)
fxc_a.shape=(6088,), fxc_b.shape=(6088,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12176), fxc.shape=(12176,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6088, 30)
rho shape (4, 4, 2, 6088)
rho_filt shape: (6088,)
get descriptors tdrho.shape=(2, 6088, 15)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12176, 15)
rho_filt.shape=(12176,)
mol:  [['S', array([0., 0., 0.])]]
converged SCF energy = -397.938786809801  <S^2> = 2.0022329  2S+1 = 3.0014882
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802ec80> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802ec80> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.22130556e-03 -9.10168990e-04 -9.93795884e-04 ... -1.18982463e+01
 -1.18982463e+01 -1.18982463e+01] = SCAN,
rho_a.shape=(6, 6320), rho_b.shape=(6, 6320)
fxc_a.shape=(6320,), fxc_b.shape=(6320,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12640), fxc.shape=(12640,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(2, 6320, 15)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12640, 15)
rho_filt.shape=(12640,)
mol:  [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]]
converged SCF energy = -1.16580491182912  <S^2> = -4.4408921e-16  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802e2f0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802e2f0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 14, 14)
ao.shape (10, 4776, 14)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.31556415e-04 -9.72662915e-06 -3.66768501e-04 ... -5.54165573e-01
 -5.54165573e-01 -5.54165573e-01] = SCAN,
rho_a.shape=(6, 4776), rho_b.shape=(6, 4776)
fxc_a.shape=(4776,), fxc_b.shape=(4776,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 4776), fxc.shape=(4776,)
get_data, dm shape = (2, 14, 14)
ao_eval.shape=(4, 4776, 14)
rho shape (4, 4, 2, 4776)
rho_filt shape: (4776,)
get descriptors tdrho.shape=(2, 4776, 15)
mol:  [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]]
converged SCF energy = -109.439263799566  <S^2> = 1.3322676e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802e4a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802e4a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9848, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-9.68469910e-05 -9.84742277e-04 -2.59676148e-04 ... -2.39626668e-05
 -2.39626668e-05 -9.68469910e-05] = SCAN,
rho_a.shape=(6, 9848), rho_b.shape=(6, 9848)
fxc_a.shape=(9848,), fxc_b.shape=(9848,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9848), fxc.shape=(9848,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9848, 44)
rho shape (4, 4, 2, 9848)
rho_filt shape: (9848,)
get descriptors tdrho.shape=(2, 9848, 15)
mol:  [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]]
converged SCF energy = -107.339357395734  <S^2> = 8.8817842e-15  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802e1a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802e1a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9752, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.04987750e-03 -6.68953858e-04 -8.57556270e-04 ... -1.07485583e-03
 -8.01425698e-01 -8.01425698e-01] = SCAN,
rho_a.shape=(6, 9752), rho_b.shape=(6, 9752)
fxc_a.shape=(9752,), fxc_b.shape=(9752,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9752), fxc.shape=(9752,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9752, 44)
rho shape (4, 4, 2, 9752)
rho_filt shape: (9752,)
get descriptors tdrho.shape=(2, 9752, 15)
mol:  [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]]
converged SCF energy = -93.3377924465129  <S^2> = 4.0072923e-10  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802d9c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802d9c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 12256, 51)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.97917285e-04 -2.54412366e-05 -3.15182243e-05 ... -6.37386500e-01
 -6.37386500e-01 -6.37386500e-01] = SCAN,
rho_a.shape=(6, 12256), rho_b.shape=(6, 12256)
fxc_a.shape=(12256,), fxc_b.shape=(12256,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12256), fxc.shape=(12256,)
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 12256, 51)
rho shape (4, 4, 2, 12256)
rho_filt shape: (12256,)
get descriptors tdrho.shape=(2, 12256, 15)
mol:  [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]]
converged SCF energy = -188.456965322844  <S^2> = 2.1316282e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802ddb0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802ddb0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 66, 66)
ao.shape (10, 14920, 66)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.50217115e-04 -2.07520066e-04 -9.23619896e-04 ... -2.74295208e-06
 -4.27559894e+00 -4.27559894e+00] = SCAN,
rho_a.shape=(6, 14920), rho_b.shape=(6, 14920)
fxc_a.shape=(14920,), fxc_b.shape=(14920,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 14920), fxc.shape=(14920,)
get_data, dm shape = (2, 66, 66)
ao_eval.shape=(4, 14920, 66)
rho shape (4, 4, 2, 14920)
rho_filt shape: (14920,)
get descriptors tdrho.shape=(2, 14920, 15)
mol:  [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]]
converged SCF energy = -920.00560888896  <S^2> = 4.9737992e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802ce50> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802ce50> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12208, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00155834 -0.00091688 -0.00215831 ... -0.00091688 -0.41618507
 -0.41618507] = SCAN,
rho_a.shape=(6, 12208), rho_b.shape=(6, 12208)
fxc_a.shape=(12208,), fxc_b.shape=(12208,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12208), fxc.shape=(12208,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12208, 60)
rho shape (4, 4, 2, 12208)
rho_filt shape: (12208,)
get descriptors tdrho.shape=(2, 12208, 15)
mol:  [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]]
converged SCF energy = -199.394370591172  <S^2> = 1.2612134e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802cd30> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802cd30> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9824, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.92948614e-04 -1.95198688e-05 -1.16699802e-03 ... -4.89378340e-01
 -4.89378340e-01 -4.89378340e-01] = SCAN,
rho_a.shape=(6, 9824), rho_b.shape=(6, 9824)
fxc_a.shape=(9824,), fxc_b.shape=(9824,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9824), fxc.shape=(9824,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9824, 44)
rho shape (4, 4, 2, 9824)
rho_filt shape: (9824,)
get descriptors tdrho.shape=(2, 9824, 15)
mol:  [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]]
converged SCF energy = -150.214894518297  <S^2> = 1.0018599  2S+1 = 2.2377309
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802cb50> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802cb50> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9912, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-6.70678771e-04 -1.18855892e-04 -6.10662494e-06 ... -6.59150577e-01
 -6.59150577e-01 -6.59150577e-01] = SCAN,
rho_a.shape=(6, 9912), rho_b.shape=(6, 9912)
fxc_a.shape=(9912,), fxc_b.shape=(9912,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9912), fxc.shape=(9912,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9912, 44)
rho shape (4, 4, 2, 9912)
rho_filt shape: (9912,)
get descriptors tdrho.shape=(2, 9912, 15)
mol:  [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]]
converged SCF energy = -77.2435048346373  <S^2> = 1.4210855e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802d030> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802d030> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 58, 58)
ao.shape (10, 15208, 58)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.83270456e-05 -8.83270456e-05 -9.75839850e-04 ... -3.46719667e-05
 -3.31708644e-05 -3.31708644e-05] = SCAN,
rho_a.shape=(6, 15208), rho_b.shape=(6, 15208)
fxc_a.shape=(15208,), fxc_b.shape=(15208,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 15208), fxc.shape=(15208,)
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15208, 58)
rho shape (4, 4, 2, 15208)
rho_filt shape: (15208,)
get descriptors tdrho.shape=(2, 15208, 15)
mol:  [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]]
converged SCF energy = -113.221335689652  <S^2> = 6.5547567e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802f2e0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802f2e0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 10040, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-5.37000578e-04 -8.55494549e-04 -2.46853288e-03 ... -7.34251999e-01
 -7.34251999e-01 -7.34251999e-01] = SCAN,
rho_a.shape=(6, 10040), rho_b.shape=(6, 10040)
fxc_a.shape=(10040,), fxc_b.shape=(10040,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 10040), fxc.shape=(10040,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 10040, 44)
rho shape (4, 4, 2, 10040)
rho_filt shape: (10040,)
get descriptors tdrho.shape=(2, 10040, 15)
mol:  [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]]
converged SCF energy = -460.624592374078  <S^2> = 6.2172489e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802c070> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802c070> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 37, 37)
ao.shape (10, 8552, 37)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.38161177e-04 -1.81188367e-05 -2.37300299e-05 ... -2.83738108e+00
 -2.83738108e+00 -2.83738108e+00] = SCAN,
rho_a.shape=(6, 8552), rho_b.shape=(6, 8552)
fxc_a.shape=(8552,), fxc_b.shape=(8552,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 8552), fxc.shape=(8552,)
get_data, dm shape = (2, 37, 37)
ao_eval.shape=(4, 8552, 37)
rho shape (4, 4, 2, 8552)
rho_filt shape: (8552,)
get descriptors tdrho.shape=(2, 8552, 15)
mol:  [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]]
converged SCF energy = -8.04458854018922  <S^2> = 7.9047879e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802dae0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802dae0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 29, 29)
ao.shape (10, 6936, 29)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00113445 -0.00118624 -0.00118624 ... -0.48434639 -0.48434639
 -0.48434639] = SCAN,
rho_a.shape=(6, 6936), rho_b.shape=(6, 6936)
fxc_a.shape=(6936,), fxc_b.shape=(6936,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 6936), fxc.shape=(6936,)
get_data, dm shape = (2, 29, 29)
ao_eval.shape=(4, 6936, 29)
rho shape (4, 4, 2, 6936)
rho_filt shape: (6936,)
get descriptors tdrho.shape=(2, 6936, 15)
mol:  [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]]
converged SCF energy = -324.340512506578  <S^2> = 1.5864643e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802f850> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802f850> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 11536, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00297935 -0.00297935 -0.00407089 ... -0.00297935 -0.00297935
 -0.00407089] = SCAN,
rho_a.shape=(6, 11536), rho_b.shape=(6, 11536)
fxc_a.shape=(11536,), fxc_b.shape=(11536,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 11536), fxc.shape=(11536,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 11536, 60)
rho shape (4, 4, 2, 11536)
rho_filt shape: (11536,)
get descriptors tdrho.shape=(2, 11536, 15)
mol:  [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]]
converged SCF energy = -1622.57507845814  <S^2> = 8.31335e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802f550> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802f550> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 120, 120)
ao.shape (10, 24512, 120)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.61400996e-04 -4.90484900e-04 -2.56451718e-03 ... -9.59296113e+00
 -9.59296113e+00 -9.59296113e+00] = SCAN,
rho_a.shape=(6, 24512), rho_b.shape=(6, 24512)
fxc_a.shape=(24512,), fxc_b.shape=(24512,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 24512), fxc.shape=(24512,)
get_data, dm shape = (2, 120, 120)
ao_eval.shape=(4, 24512, 120)
rho shape (4, 4, 2, 24512)
rho_filt shape: (24512,)
get descriptors tdrho.shape=(2, 24512, 15)
mol:  [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]]
converged SCF energy = -342.979728469575  <S^2> = 2.5394797e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802e320> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802e320> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 13096, 51)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.28637920e-03 -4.32383300e-04 -3.74057272e-05 ... -1.91722770e+00
 -1.91722770e+00 -1.91722770e+00] = SCAN,
rho_a.shape=(6, 13096), rho_b.shape=(6, 13096)
fxc_a.shape=(13096,), fxc_b.shape=(13096,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 13096), fxc.shape=(13096,)
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 13096, 51)
rho shape (4, 4, 2, 13096)
rho_filt shape: (13096,)
get descriptors tdrho.shape=(2, 13096, 15)
mol:  [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]]
converged SCF energy = -578.565336626331  <S^2> = 1.0034707  2S+1 = 2.2391701
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802ee30> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802ee30> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12384, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.59951545e-04 -2.60672288e-04 -2.59277799e-04 ... -3.86943697e-01
 -3.86943697e-01 -3.86943697e-01] = SCAN,
rho_a.shape=(6, 12384), rho_b.shape=(6, 12384)
fxc_a.shape=(12384,), fxc_b.shape=(12384,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12384), fxc.shape=(12384,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12384, 60)
rho shape (4, 4, 2, 12384)
rho_filt shape: (12384,)
get descriptors tdrho.shape=(2, 12384, 15)
mol:  [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]]
converged SCF energy = -40.4598214864077  <S^2> = 3.1885605e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802dd20> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802dd20> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 50, 50)
ao.shape (10, 13936, 50)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.68439986e-04 -2.42462569e-04 -1.69927031e-05 ... -2.55230307e-05
 -2.55230307e-05 -2.55230307e-05] = SCAN,
rho_a.shape=(6, 13936), rho_b.shape=(6, 13936)
fxc_a.shape=(13936,), fxc_b.shape=(13936,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 13936), fxc.shape=(13936,)
get_data, dm shape = (2, 50, 50)
ao_eval.shape=(4, 13936, 50)
rho shape (4, 4, 2, 13936)
rho_filt shape: (13936,)
get descriptors tdrho.shape=(2, 13936, 15)
mol:  [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]]
converged SCF energy = -39.0756147483504  <S^2> = 6.1972649e-12  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb802fdc0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb802fdc0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 36, 36)
ao.shape (10, 9656, 36)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-7.67688751e-04 -4.57393214e-05 -2.02834191e-04 ... -1.14928924e+00
 -1.14928924e+00 -1.14928924e+00] = SCAN,
rho_a.shape=(6, 9656), rho_b.shape=(6, 9656)
fxc_a.shape=(9656,), fxc_b.shape=(9656,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9656), fxc.shape=(9656,)
get_data, dm shape = (2, 36, 36)
ao_eval.shape=(4, 9656, 36)
rho shape (4, 4, 2, 9656)
rho_filt shape: (9656,)
get descriptors tdrho.shape=(2, 9656, 15)
mol:  [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]]
converged SCF energy = -291.719272437819  <S^2> = 1.315037e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
New DM shape: (2, 58, 58)
ao.shape (10, 15256, 58)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.33850535e-04 -2.34903029e-04 -1.75623665e-05 ... -1.92891112e-05
 -1.92891112e-05 -1.92891112e-05] = SCAN,
rho_a.shape=(6, 15256), rho_b.shape=(6, 15256)
fxc_a.shape=(15256,), fxc_b.shape=(15256,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 15256), fxc.shape=(15256,)
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15256, 58)
rho shape (4, 4, 2, 15256)
rho_filt shape: (15256,)
get descriptors tdrho.shape=(2, 15256, 15)
localnet.spin_scaling: concatenating the data
first data shape = (10940, 15)
concatenated: tdrho.shape=(271719, 15)
PRE NAN FILT: tFxc.shape=(271719,), tdrho.shape=(271719, 15)
nan_filt_rho.shape=(271719,)
nan_filt_fxc.shape=(271719,)
tFxc.shape=(271719,), tdrho.shape=(271719, 15)
inp[0].shape = (271719, 15)
Epoch 0
Epoch 0 :: Batch 0/1
Batch Loss = 68545.19586576002
0, epoch_train_loss=68545.19586576002
Epoch 1
Epoch 1 :: Batch 0/1
Batch Loss = 1099979120.247849
1, epoch_train_loss=1099979120.247849
Epoch 2
Epoch 2 :: Batch 0/1
Batch Loss = 20846047.544015013
2, epoch_train_loss=20846047.544015013
Epoch 3
Epoch 3 :: Batch 0/1
Batch Loss = 37552277.50978654
3, epoch_train_loss=37552277.50978654
Epoch 4
Epoch 4 :: Batch 0/1
Batch Loss = 19650842.766405094
4, epoch_train_loss=19650842.766405094
Epoch 5
Epoch 5 :: Batch 0/1
Batch Loss = 6853089.7562551005
5, epoch_train_loss=6853089.7562551005
Epoch 6
Epoch 6 :: Batch 0/1
Batch Loss = 43990.78300337571
6, epoch_train_loss=43990.78300337571
Epoch 7
Epoch 7 :: Batch 0/1
Batch Loss = 684065.1899510405
7, epoch_train_loss=684065.1899510405
Epoch 8
Epoch 8 :: Batch 0/1
Batch Loss = 1719051.2879120235
8, epoch_train_loss=1719051.2879120235
Epoch 9
Epoch 9 :: Batch 0/1
Batch Loss = 881852.8977181717
9, epoch_train_loss=881852.8977181717
Epoch 10
Epoch 10 :: Batch 0/1
Batch Loss = 2840863.315076747
10, epoch_train_loss=2840863.315076747
Epoch 11
Epoch 11 :: Batch 0/1
Batch Loss = 80434.9315663787
11, epoch_train_loss=80434.9315663787
Epoch 12
Epoch 12 :: Batch 0/1
Batch Loss = 455264.5412772914
12, epoch_train_loss=455264.5412772914
Epoch 13
Epoch 13 :: Batch 0/1
Batch Loss = 448643.32234372426
13, epoch_train_loss=448643.32234372426
Epoch 14
Epoch 14 :: Batch 0/1
Batch Loss = 337341.5371944519
14, epoch_train_loss=337341.5371944519
Epoch 15
Epoch 15 :: Batch 0/1
Batch Loss = 183807.41873269135
15, epoch_train_loss=183807.41873269135
Epoch 16
Epoch 16 :: Batch 0/1
Batch Loss = 83577.06976967587
16, epoch_train_loss=83577.06976967587
Epoch 17
Epoch 17 :: Batch 0/1
Batch Loss = 23878.48292315991
17, epoch_train_loss=23878.48292315991
Epoch 18
Epoch 18 :: Batch 0/1
Batch Loss = 321.58326814281685
18, epoch_train_loss=321.58326814281685
Epoch 19
Epoch 19 :: Batch 0/1
Batch Loss = 12.966134399031976
19, epoch_train_loss=12.966134399031976
Epoch 20
Epoch 20 :: Batch 0/1
Batch Loss = 1.3265029931881853
20, epoch_train_loss=1.3265029931881853
Epoch 21
Epoch 21 :: Batch 0/1
Batch Loss = 1.4002594736137601
21, epoch_train_loss=1.4002594736137601
Epoch 22
Epoch 22 :: Batch 0/1
Batch Loss = 1.3613849412746102
22, epoch_train_loss=1.3613849412746102
Epoch 23
Epoch 23 :: Batch 0/1
Batch Loss = 1.456599238346084
23, epoch_train_loss=1.456599238346084
Epoch 24
Epoch 24 :: Batch 0/1
Batch Loss = 1.4598073314041153
24, epoch_train_loss=1.4598073314041153
Epoch 25
Epoch 25 :: Batch 0/1
Batch Loss = 1.4351131260418286
25, epoch_train_loss=1.4351131260418286
Epoch 26
Epoch 26 :: Batch 0/1
Batch Loss = 1.470252248920372
26, epoch_train_loss=1.470252248920372
Epoch 27
Epoch 27 :: Batch 0/1
Batch Loss = 1.4711539807633656
27, epoch_train_loss=1.4711539807633656
Epoch 28
Epoch 28 :: Batch 0/1
Batch Loss = 1.5513238138468681
28, epoch_train_loss=1.5513238138468681
Epoch 29
Epoch 29 :: Batch 0/1
Batch Loss = 1.4314256935073006
29, epoch_train_loss=1.4314256935073006
Epoch 30
Epoch 30 :: Batch 0/1
Batch Loss = 4.603274794753906
30, epoch_train_loss=4.603274794753906
Epoch 31
Epoch 31 :: Batch 0/1
Batch Loss = 175.76745520658093
31, epoch_train_loss=175.76745520658093
Epoch 32
Epoch 32 :: Batch 0/1
Batch Loss = 1.4694140722114915
32, epoch_train_loss=1.4694140722114915
Epoch 33
Epoch 33 :: Batch 0/1
Batch Loss = 1.479717956236247
33, epoch_train_loss=1.479717956236247
Epoch 34
Epoch 34 :: Batch 0/1
Batch Loss = 1.4740471868729252
34, epoch_train_loss=1.4740471868729252
Epoch 35
Epoch 35 :: Batch 0/1
Batch Loss = 1.4525701321175049
35, epoch_train_loss=1.4525701321175049
Epoch 36
Epoch 36 :: Batch 0/1
Batch Loss = 1.43743738853391
36, epoch_train_loss=1.43743738853391
Epoch 37
Epoch 37 :: Batch 0/1
Batch Loss = 1.4317480530567033
37, epoch_train_loss=1.4317480530567033
Epoch 38
Epoch 38 :: Batch 0/1
Batch Loss = 1.418272960933289
38, epoch_train_loss=1.418272960933289
Epoch 39
Epoch 39 :: Batch 0/1
Batch Loss = 1.3946156071849336
39, epoch_train_loss=1.3946156071849336
Epoch 40
Epoch 40 :: Batch 0/1
Batch Loss = 1.3728610352194435
40, epoch_train_loss=1.3728610352194435
Epoch 41
Epoch 41 :: Batch 0/1
Batch Loss = 3.3733032528924394
41, epoch_train_loss=3.3733032528924394
Epoch 42
Epoch 42 :: Batch 0/1
Batch Loss = 1.650429727460614
42, epoch_train_loss=1.650429727460614
Epoch 43
Epoch 43 :: Batch 0/1
Batch Loss = 2.232292918366002
43, epoch_train_loss=2.232292918366002
Epoch 44
Epoch 44 :: Batch 0/1
Batch Loss = 2.229032860533508
44, epoch_train_loss=2.229032860533508
Epoch 45
Epoch 45 :: Batch 0/1
Batch Loss = 1.8521394669447109
45, epoch_train_loss=1.8521394669447109
Epoch 46
Epoch 46 :: Batch 0/1
Batch Loss = 1.4276232923638879
46, epoch_train_loss=1.4276232923638879
Epoch 47
Epoch 47 :: Batch 0/1
Batch Loss = 1.2746561946488588
47, epoch_train_loss=1.2746561946488588
Epoch 48
Epoch 48 :: Batch 0/1
Batch Loss = 1.4750971883249033
48, epoch_train_loss=1.4750971883249033
Epoch 49
Epoch 49 :: Batch 0/1
Batch Loss = 1.7179084867723524
49, epoch_train_loss=1.7179084867723524
Epoch 50
Epoch 50 :: Batch 0/1
Batch Loss = 1.3825518018385736
50, epoch_train_loss=1.3825518018385736
Epoch 51
Epoch 51 :: Batch 0/1
Batch Loss = 1.854259600139774
51, epoch_train_loss=1.854259600139774
Epoch 52
Epoch 52 :: Batch 0/1
Batch Loss = 1.6480871270718422
52, epoch_train_loss=1.6480871270718422
Epoch 53
Epoch 53 :: Batch 0/1
Batch Loss = 1.3240496005179456
53, epoch_train_loss=1.3240496005179456
Epoch 54
Epoch 54 :: Batch 0/1
Batch Loss = 1.1917971688248372
54, epoch_train_loss=1.1917971688248372
Epoch 55
Epoch 55 :: Batch 0/1
Batch Loss = 1.272756986403936
55, epoch_train_loss=1.272756986403936
Epoch 56
Epoch 56 :: Batch 0/1
Batch Loss = 1.414660451840911
56, epoch_train_loss=1.414660451840911
Epoch 57
Epoch 57 :: Batch 0/1
Batch Loss = 1.4732207716650318
57, epoch_train_loss=1.4732207716650318
Epoch 58
Epoch 58 :: Batch 0/1
Batch Loss = 1.4022024999288893
58, epoch_train_loss=1.4022024999288893
Epoch 59
Epoch 59 :: Batch 0/1
Batch Loss = 1.2502356821936855
59, epoch_train_loss=1.2502356821936855
Epoch 60
Epoch 60 :: Batch 0/1
Batch Loss = 1.1182278171292177
60, epoch_train_loss=1.1182278171292177
Epoch 61
Epoch 61 :: Batch 0/1
Batch Loss = 1.0926194348240197
61, epoch_train_loss=1.0926194348240197
Epoch 62
Epoch 62 :: Batch 0/1
Batch Loss = 1.162725070503097
62, epoch_train_loss=1.162725070503097
Epoch 63
Epoch 63 :: Batch 0/1
Batch Loss = 1.2199556092315278
63, epoch_train_loss=1.2199556092315278
Epoch 64
Epoch 64 :: Batch 0/1
Batch Loss = 1.176239505417801
64, epoch_train_loss=1.176239505417801
Epoch 65
Epoch 65 :: Batch 0/1
Batch Loss = 1.0673762647126415
65, epoch_train_loss=1.0673762647126415
Epoch 66
Epoch 66 :: Batch 0/1
Batch Loss = 0.9920933026537855
66, epoch_train_loss=0.9920933026537855
Epoch 67
Epoch 67 :: Batch 0/1
Batch Loss = 0.9928211364419717
67, epoch_train_loss=0.9928211364419717
Epoch 68
Epoch 68 :: Batch 0/1
Batch Loss = 1.030672740798662
68, epoch_train_loss=1.030672740798662
Epoch 69
Epoch 69 :: Batch 0/1
Batch Loss = 1.041689900451841
69, epoch_train_loss=1.041689900451841
Epoch 70
Epoch 70 :: Batch 0/1
Batch Loss = 0.9987775874696976
70, epoch_train_loss=0.9987775874696976
Epoch 71
Epoch 71 :: Batch 0/1
Batch Loss = 0.9286988738598514
71, epoch_train_loss=0.9286988738598514
Epoch 72
Epoch 72 :: Batch 0/1
Batch Loss = 0.8833868018682097
72, epoch_train_loss=0.8833868018682097
Epoch 73
Epoch 73 :: Batch 0/1
Batch Loss = 0.8859268307064475
73, epoch_train_loss=0.8859268307064475
Epoch 74
Epoch 74 :: Batch 0/1
Batch Loss = 0.9028527095197223
74, epoch_train_loss=0.9028527095197223
Epoch 75
Epoch 75 :: Batch 0/1
Batch Loss = 79.97247736777028
75, epoch_train_loss=79.97247736777028
Epoch 76
Epoch 76 :: Batch 0/1
Batch Loss = 0.8384470736736459
76, epoch_train_loss=0.8384470736736459
Epoch 77
Epoch 77 :: Batch 0/1
Batch Loss = 0.8029762814172741
77, epoch_train_loss=0.8029762814172741
Epoch 78
Epoch 78 :: Batch 0/1
Batch Loss = 0.8062328527548641
78, epoch_train_loss=0.8062328527548641
Epoch 79
Epoch 79 :: Batch 0/1
Batch Loss = 0.8211559312724955
79, epoch_train_loss=0.8211559312724955
Epoch 80
Epoch 80 :: Batch 0/1
Batch Loss = 0.8148521344528471
80, epoch_train_loss=0.8148521344528471
Epoch 81
Epoch 81 :: Batch 0/1
Batch Loss = 0.7856554198983076
81, epoch_train_loss=0.7856554198983076
Epoch 82
Epoch 82 :: Batch 0/1
Batch Loss = 0.7655344781067493
82, epoch_train_loss=0.7655344781067493
Epoch 83
Epoch 83 :: Batch 0/1
Batch Loss = 0.7706681058055694
83, epoch_train_loss=0.7706681058055694
Epoch 84
Epoch 84 :: Batch 0/1
Batch Loss = 0.7764258695706177
84, epoch_train_loss=0.7764258695706177
Epoch 85
Epoch 85 :: Batch 0/1
Batch Loss = 0.7595091478663204
85, epoch_train_loss=0.7595091478663204
Epoch 86
Epoch 86 :: Batch 0/1
Batch Loss = 0.7333902177736783
86, epoch_train_loss=0.7333902177736783
Epoch 87
Epoch 87 :: Batch 0/1
Batch Loss = 0.723713469161862
87, epoch_train_loss=0.723713469161862
Epoch 88
Epoch 88 :: Batch 0/1
Batch Loss = 0.7257449064379183
88, epoch_train_loss=0.7257449064379183
Epoch 89
Epoch 89 :: Batch 0/1
Batch Loss = 0.7150822687664947
89, epoch_train_loss=0.7150822687664947
Epoch 90
Epoch 90 :: Batch 0/1
Batch Loss = 0.6920389249253188
90, epoch_train_loss=0.6920389249253188
Epoch 91
Epoch 91 :: Batch 0/1
Batch Loss = 0.6765336920360302
91, epoch_train_loss=0.6765336920360302
Epoch 92
Epoch 92 :: Batch 0/1
Batch Loss = 0.6721329799163375
92, epoch_train_loss=0.6721329799163375
Epoch 93
Epoch 93 :: Batch 0/1
Batch Loss = 0.6606366841129714
93, epoch_train_loss=0.6606366841129714
Epoch 94
Epoch 94 :: Batch 0/1
Batch Loss = 0.6387103395917312
94, epoch_train_loss=0.6387103395917312
Epoch 95
Epoch 95 :: Batch 0/1
Batch Loss = 0.622729156759351
95, epoch_train_loss=0.622729156759351
Epoch 96
Epoch 96 :: Batch 0/1
Batch Loss = 0.6146907120050479
96, epoch_train_loss=0.6146907120050479
Epoch 97
Epoch 97 :: Batch 0/1
Batch Loss = 0.6011844471914021
97, epoch_train_loss=0.6011844471914021
Epoch 98
Epoch 98 :: Batch 0/1
Batch Loss = 0.5808520581577167
98, epoch_train_loss=0.5808520581577167
Epoch 99
Epoch 99 :: Batch 0/1
Batch Loss = 0.5660718776385654
99, epoch_train_loss=0.5660718776385654
Epoch 100
Epoch 100 :: Batch 0/1
Batch Loss = 0.5561884127634681
100, epoch_train_loss=0.5561884127634681
Epoch 101
Epoch 101 :: Batch 0/1
Batch Loss = 0.540533833221549
101, epoch_train_loss=0.540533833221549
Epoch 102
Epoch 102 :: Batch 0/1
Batch Loss = 0.5224188101718251
102, epoch_train_loss=0.5224188101718251
Epoch 103
Epoch 103 :: Batch 0/1
Batch Loss = 0.5102209808008024
103, epoch_train_loss=0.5102209808008024
Epoch 104
Epoch 104 :: Batch 0/1
Batch Loss = 0.49856242518684746
104, epoch_train_loss=0.49856242518684746
Epoch 105
Epoch 105 :: Batch 0/1
Batch Loss = 0.4818100825568505
105, epoch_train_loss=0.4818100825568505
Epoch 106
Epoch 106 :: Batch 0/1
Batch Loss = 0.46705053636545035
106, epoch_train_loss=0.46705053636545035
Epoch 107
Epoch 107 :: Batch 0/1
Batch Loss = 0.45607852185690506
107, epoch_train_loss=0.45607852185690506
Epoch 108
Epoch 108 :: Batch 0/1
Batch Loss = 0.4419030670661225
108, epoch_train_loss=0.4419030670661225
Epoch 109
Epoch 109 :: Batch 0/1
Batch Loss = 0.4267895474950783
109, epoch_train_loss=0.4267895474950783
Epoch 110
Epoch 110 :: Batch 0/1
Batch Loss = 0.4155717938975717
110, epoch_train_loss=0.4155717938975717
Epoch 111
Epoch 111 :: Batch 0/1
Batch Loss = 0.4032016444568332
111, epoch_train_loss=0.4032016444568332
Epoch 112
Epoch 112 :: Batch 0/1
Batch Loss = 0.3888584833866506
112, epoch_train_loss=0.3888584833866506
Epoch 113
Epoch 113 :: Batch 0/1
Batch Loss = 0.3775919819910136
113, epoch_train_loss=0.3775919819910136
Epoch 114
Epoch 114 :: Batch 0/1
Batch Loss = 0.3663023880094236
114, epoch_train_loss=0.3663023880094236
Epoch 115
Epoch 115 :: Batch 0/1
Batch Loss = 0.35339136889430056
115, epoch_train_loss=0.35339136889430056
Epoch 116
Epoch 116 :: Batch 0/1
Batch Loss = 0.3429656769789783
116, epoch_train_loss=0.3429656769789783
Epoch 117
Epoch 117 :: Batch 0/1
Batch Loss = 0.3325146226473088
117, epoch_train_loss=0.3325146226473088
Epoch 118
Epoch 118 :: Batch 0/1
Batch Loss = 0.32097701334775136
118, epoch_train_loss=0.32097701334775136
Epoch 119
Epoch 119 :: Batch 0/1
Batch Loss = 0.3115996204457277
119, epoch_train_loss=0.3115996204457277
Epoch 120
Epoch 120 :: Batch 0/1
Batch Loss = 0.30173545488226755
120, epoch_train_loss=0.30173545488226755
Epoch 121
Epoch 121 :: Batch 0/1
Batch Loss = 0.291665760847419
121, epoch_train_loss=0.291665760847419
Epoch 122
Epoch 122 :: Batch 0/1
Batch Loss = 0.28343980823889564
122, epoch_train_loss=0.28343980823889564
Epoch 123
Epoch 123 :: Batch 0/1
Batch Loss = 0.2743656369668593
123, epoch_train_loss=0.2743656369668593
Epoch 124
Epoch 124 :: Batch 0/1
Batch Loss = 0.26600692034036283
124, epoch_train_loss=0.26600692034036283
Epoch 125
Epoch 125 :: Batch 0/1
Batch Loss = 0.2585589984376459
125, epoch_train_loss=0.2585589984376459
Epoch 126
Epoch 126 :: Batch 0/1
Batch Loss = 0.25063833729204305
126, epoch_train_loss=0.25063833729204305
Epoch 127
Epoch 127 :: Batch 0/1
Batch Loss = 0.2440735199483813
127, epoch_train_loss=0.2440735199483813
Epoch 128
Epoch 128 :: Batch 0/1
Batch Loss = 0.23719721436552052
128, epoch_train_loss=0.23719721436552052
Epoch 129
Epoch 129 :: Batch 0/1
Batch Loss = 0.23091878926968798
129, epoch_train_loss=0.23091878926968798
Epoch 130
Epoch 130 :: Batch 0/1
Batch Loss = 0.22525290825355915
130, epoch_train_loss=0.22525290825355915
Epoch 131
Epoch 131 :: Batch 0/1
Batch Loss = 0.2193936809235803
131, epoch_train_loss=0.2193936809235803
Epoch 132
Epoch 132 :: Batch 0/1
Batch Loss = 0.21456682674508396
132, epoch_train_loss=0.21456682674508396
Epoch 133
Epoch 133 :: Batch 0/1
Batch Loss = 0.20932320403491492
133, epoch_train_loss=0.20932320403491492
Epoch 134
Epoch 134 :: Batch 0/1
Batch Loss = 0.2049019852709274
134, epoch_train_loss=0.2049019852709274
Epoch 135
Epoch 135 :: Batch 0/1
Batch Loss = 0.2004407157052098
135, epoch_train_loss=0.2004407157052098
Epoch 136
Epoch 136 :: Batch 0/1
Batch Loss = 0.19635365646145528
136, epoch_train_loss=0.19635365646145528
Epoch 137
Epoch 137 :: Batch 0/1
Batch Loss = 0.1925922502866246
137, epoch_train_loss=0.1925922502866246
Epoch 138
Epoch 138 :: Batch 0/1
Batch Loss = 0.18881145181707512
138, epoch_train_loss=0.18881145181707512
Epoch 139
Epoch 139 :: Batch 0/1
Batch Loss = 0.18554669984209451
139, epoch_train_loss=0.18554669984209451
Epoch 140
Epoch 140 :: Batch 0/1
Batch Loss = 0.18210258866666715
140, epoch_train_loss=0.18210258866666715
Epoch 141
Epoch 141 :: Batch 0/1
Batch Loss = 0.1791783760135564
141, epoch_train_loss=0.1791783760135564
Epoch 142
Epoch 142 :: Batch 0/1
Batch Loss = 0.17604385514236734
142, epoch_train_loss=0.17604385514236734
Epoch 143
Epoch 143 :: Batch 0/1
Batch Loss = 0.17333502390599237
143, epoch_train_loss=0.17333502390599237
Epoch 144
Epoch 144 :: Batch 0/1
Batch Loss = 0.17046574941538617
144, epoch_train_loss=0.17046574941538617
Epoch 145
Epoch 145 :: Batch 0/1
Batch Loss = 0.167934512056858
145, epoch_train_loss=0.167934512056858
Epoch 146
Epoch 146 :: Batch 0/1
Batch Loss = 0.16531876733053136
146, epoch_train_loss=0.16531876733053136
Epoch 147
Epoch 147 :: Batch 0/1
Batch Loss = 0.16299430045077518
147, epoch_train_loss=0.16299430045077518
Epoch 148
Epoch 148 :: Batch 0/1
Batch Loss = 0.16060598713221375
148, epoch_train_loss=0.16060598713221375
Epoch 149
Epoch 149 :: Batch 0/1
Batch Loss = 0.1584552876221671
149, epoch_train_loss=0.1584552876221671
Epoch 150
Epoch 150 :: Batch 0/1
Batch Loss = 0.15623326924612832
150, epoch_train_loss=0.15623326924612832
Epoch 151
Epoch 151 :: Batch 0/1
Batch Loss = 0.15424880880606182
151, epoch_train_loss=0.15424880880606182
Epoch 152
Epoch 152 :: Batch 0/1
Batch Loss = 0.1522194106805223
152, epoch_train_loss=0.1522194106805223
Epoch 153
Epoch 153 :: Batch 0/1
Batch Loss = 0.1503897676199676
153, epoch_train_loss=0.1503897676199676
Epoch 154
Epoch 154 :: Batch 0/1
Batch Loss = 0.14851739215604676
154, epoch_train_loss=0.14851739215604676
Epoch 155
Epoch 155 :: Batch 0/1
Batch Loss = 0.14677982112470514
155, epoch_train_loss=0.14677982112470514
Epoch 156
Epoch 156 :: Batch 0/1
Batch Loss = 0.1450250603599733
156, epoch_train_loss=0.1450250603599733
Epoch 157
Epoch 157 :: Batch 0/1
Batch Loss = 0.14337498211615077
157, epoch_train_loss=0.14337498211615077
Epoch 158
Epoch 158 :: Batch 0/1
Batch Loss = 0.1417318396402752
158, epoch_train_loss=0.1417318396402752
Epoch 159
Epoch 159 :: Batch 0/1
Batch Loss = 0.14015811985360604
159, epoch_train_loss=0.14015811985360604
Epoch 160
Epoch 160 :: Batch 0/1
Batch Loss = 0.1386063571157863
160, epoch_train_loss=0.1386063571157863
Epoch 161
Epoch 161 :: Batch 0/1
Batch Loss = 0.13706417532142243
161, epoch_train_loss=0.13706417532142243
Epoch 162
Epoch 162 :: Batch 0/1
Batch Loss = 0.1355500085405984
162, epoch_train_loss=0.1355500085405984
Epoch 163
Epoch 163 :: Batch 0/1
Batch Loss = 0.13399934230455843
163, epoch_train_loss=0.13399934230455843
Epoch 164
Epoch 164 :: Batch 0/1
Batch Loss = 0.132504870976817
164, epoch_train_loss=0.132504870976817
Epoch 165
Epoch 165 :: Batch 0/1
Batch Loss = 0.13101641546233242
165, epoch_train_loss=0.13101641546233242
Epoch 166
Epoch 166 :: Batch 0/1
Batch Loss = 0.12957232799382074
166, epoch_train_loss=0.12957232799382074
Epoch 167
Epoch 167 :: Batch 0/1
Batch Loss = 0.12815464046354857
167, epoch_train_loss=0.12815464046354857
Epoch 168
Epoch 168 :: Batch 0/1
Batch Loss = 0.12682887129304274
168, epoch_train_loss=0.12682887129304274
Epoch 169
Epoch 169 :: Batch 0/1
Batch Loss = 0.12560309895120597
169, epoch_train_loss=0.12560309895120597
Epoch 170
Epoch 170 :: Batch 0/1
Batch Loss = 0.12447527856378868
170, epoch_train_loss=0.12447527856378868
Epoch 171
Epoch 171 :: Batch 0/1
Batch Loss = 0.12343630269268516
171, epoch_train_loss=0.12343630269268516
Epoch 172
Epoch 172 :: Batch 0/1
Batch Loss = 0.12238279846097878
172, epoch_train_loss=0.12238279846097878
Epoch 173
Epoch 173 :: Batch 0/1
Batch Loss = 0.12126213294368521
173, epoch_train_loss=0.12126213294368521
Epoch 174
Epoch 174 :: Batch 0/1
Batch Loss = 0.11948133121173386
174, epoch_train_loss=0.11948133121173386
Epoch 175
Epoch 175 :: Batch 0/1
Batch Loss = 0.11865357841071564
175, epoch_train_loss=0.11865357841071564
Epoch 176
Epoch 176 :: Batch 0/1
Batch Loss = 0.11849265896752137
176, epoch_train_loss=0.11849265896752137
Epoch 177
Epoch 177 :: Batch 0/1
Batch Loss = 0.11689664145977513
177, epoch_train_loss=0.11689664145977513
Epoch 178
Epoch 178 :: Batch 0/1
Batch Loss = 0.114739593014193
178, epoch_train_loss=0.114739593014193
Epoch 179
Epoch 179 :: Batch 0/1
Batch Loss = 0.11428500472476241
179, epoch_train_loss=0.11428500472476241
Epoch 180
Epoch 180 :: Batch 0/1
Batch Loss = 0.12473375557160118
180, epoch_train_loss=0.12473375557160118
Epoch 181
Epoch 181 :: Batch 0/1
Batch Loss = 0.19498923747529195
181, epoch_train_loss=0.19498923747529195
Epoch 182
Epoch 182 :: Batch 0/1
Batch Loss = 0.18891342505584727
182, epoch_train_loss=0.18891342505584727
Epoch 183
Epoch 183 :: Batch 0/1
Batch Loss = 0.2884449668901011
183, epoch_train_loss=0.2884449668901011
Epoch 184
Epoch 184 :: Batch 0/1
Batch Loss = 0.7375166914204437
184, epoch_train_loss=0.7375166914204437
Epoch 185
Epoch 185 :: Batch 0/1
Batch Loss = 0.16972894026840063
185, epoch_train_loss=0.16972894026840063
Epoch 186
Epoch 186 :: Batch 0/1
Batch Loss = 0.9072757819757291
186, epoch_train_loss=0.9072757819757291
Epoch 187
Epoch 187 :: Batch 0/1
Batch Loss = 0.41649412786294504
187, epoch_train_loss=0.41649412786294504
Epoch 188
Epoch 188 :: Batch 0/1
Batch Loss = 0.7222116088576795
188, epoch_train_loss=0.7222116088576795
Epoch 189
Epoch 189 :: Batch 0/1
Batch Loss = 0.16051400836681917
189, epoch_train_loss=0.16051400836681917
Epoch 190
Epoch 190 :: Batch 0/1
Batch Loss = 0.6866852871082721
190, epoch_train_loss=0.6866852871082721
Epoch 191
Epoch 191 :: Batch 0/1
Batch Loss = 0.17213471934160907
191, epoch_train_loss=0.17213471934160907
Epoch 192
Epoch 192 :: Batch 0/1
Batch Loss = 0.3828202754057609
192, epoch_train_loss=0.3828202754057609
Epoch 193
Epoch 193 :: Batch 0/1
Batch Loss = 0.301216139156376
193, epoch_train_loss=0.301216139156376
Epoch 194
Epoch 194 :: Batch 0/1
Batch Loss = 0.6696363494707064
194, epoch_train_loss=0.6696363494707064
Epoch 195
Epoch 195 :: Batch 0/1
Batch Loss = 0.29749487584498563
195, epoch_train_loss=0.29749487584498563
Epoch 196
Epoch 196 :: Batch 0/1
Batch Loss = 0.34690634739477977
196, epoch_train_loss=0.34690634739477977
Epoch 197
Epoch 197 :: Batch 0/1
Batch Loss = 0.15397323836301013
197, epoch_train_loss=0.15397323836301013
Epoch 198
Epoch 198 :: Batch 0/1
Batch Loss = 0.39956593454772904
198, epoch_train_loss=0.39956593454772904
Epoch 199
Epoch 199 :: Batch 0/1
Batch Loss = 0.20705087395347146
199, epoch_train_loss=0.20705087395347146
Epoch 200
Epoch 200 :: Batch 0/1
Batch Loss = 0.24485790486767128
200, epoch_train_loss=0.24485790486767128
Epoch 201
Epoch 201 :: Batch 0/1
Batch Loss = 0.3237215450232614
201, epoch_train_loss=0.3237215450232614
Epoch 202
Epoch 202 :: Batch 0/1
Batch Loss = 0.14935936852563303
202, epoch_train_loss=0.14935936852563303
Epoch 203
Epoch 203 :: Batch 0/1
Batch Loss = 0.2778198149590552
203, epoch_train_loss=0.2778198149590552
Epoch 204
Epoch 204 :: Batch 0/1
Batch Loss = 0.1888534248709536
204, epoch_train_loss=0.1888534248709536
Epoch 205
Epoch 205 :: Batch 0/1
Batch Loss = 0.17136074270936155
205, epoch_train_loss=0.17136074270936155
Epoch 206
Epoch 206 :: Batch 0/1
Batch Loss = 0.23301659838099736
206, epoch_train_loss=0.23301659838099736
Epoch 207
Epoch 207 :: Batch 0/1
Batch Loss = 0.12618211927710193
207, epoch_train_loss=0.12618211927710193
Epoch 208
Epoch 208 :: Batch 0/1
Batch Loss = 0.19546041148721133
208, epoch_train_loss=0.19546041148721133
Epoch 209
Epoch 209 :: Batch 0/1
Batch Loss = 0.15042265832330912
209, epoch_train_loss=0.15042265832330912
Epoch 210
Epoch 210 :: Batch 0/1
Batch Loss = 0.1415442643553549
210, epoch_train_loss=0.1415442643553549
Epoch 211
Epoch 211 :: Batch 0/1
Batch Loss = 0.177927641949637
211, epoch_train_loss=0.177927641949637
Epoch 212
Epoch 212 :: Batch 0/1
Batch Loss = 0.11288029593673111
212, epoch_train_loss=0.11288029593673111
Epoch 213
Epoch 213 :: Batch 0/1
Batch Loss = 0.16253457957619283
213, epoch_train_loss=0.16253457957619283
Epoch 214
Epoch 214 :: Batch 0/1
Batch Loss = 0.11754417944107315
214, epoch_train_loss=0.11754417944107315
Epoch 215
Epoch 215 :: Batch 0/1
Batch Loss = 0.12660915263329106
215, epoch_train_loss=0.12660915263329106
Epoch 216
Epoch 216 :: Batch 0/1
Batch Loss = 0.13194499178116625
216, epoch_train_loss=0.13194499178116625
Epoch 217
Epoch 217 :: Batch 0/1
Batch Loss = 0.10239309182382549
217, epoch_train_loss=0.10239309182382549
Epoch 218
Epoch 218 :: Batch 0/1
Batch Loss = 0.13220637204799293
218, epoch_train_loss=0.13220637204799293
Epoch 219
Epoch 219 :: Batch 0/1
Batch Loss = 0.09593978706284964
219, epoch_train_loss=0.09593978706284964
Epoch 220
Epoch 220 :: Batch 0/1
Batch Loss = 0.12061478679065395
220, epoch_train_loss=0.12061478679065395
Epoch 221
Epoch 221 :: Batch 0/1
Batch Loss = 0.10004950128649852
221, epoch_train_loss=0.10004950128649852
Epoch 222
Epoch 222 :: Batch 0/1
Batch Loss = 0.10402365725282628
222, epoch_train_loss=0.10402365725282628
Epoch 223
Epoch 223 :: Batch 0/1
Batch Loss = 0.10120577394526954
223, epoch_train_loss=0.10120577394526954
Epoch 224
Epoch 224 :: Batch 0/1
Batch Loss = 0.09226486612506962
224, epoch_train_loss=0.09226486612506962
Epoch 225
Epoch 225 :: Batch 0/1
Batch Loss = 0.1003473819070151
225, epoch_train_loss=0.1003473819070151
Epoch 226
Epoch 226 :: Batch 0/1
Batch Loss = 0.08340490472666526
226, epoch_train_loss=0.08340490472666526
Epoch 227
Epoch 227 :: Batch 0/1
Batch Loss = 0.09645644822342865
227, epoch_train_loss=0.09645644822342865
Epoch 228
Epoch 228 :: Batch 0/1
Batch Loss = 0.08003649131260732
228, epoch_train_loss=0.08003649131260732
Epoch 229
Epoch 229 :: Batch 0/1
Batch Loss = 0.09035724516305621
229, epoch_train_loss=0.09035724516305621
Epoch 230
Epoch 230 :: Batch 0/1
Batch Loss = 0.07950783014662256
230, epoch_train_loss=0.07950783014662256
Epoch 231
Epoch 231 :: Batch 0/1
Batch Loss = 0.08527641888623289
231, epoch_train_loss=0.08527641888623289
Epoch 232
Epoch 232 :: Batch 0/1
Batch Loss = 0.07828512534122463
232, epoch_train_loss=0.07828512534122463
Epoch 233
Epoch 233 :: Batch 0/1
Batch Loss = 0.0804405024089674
233, epoch_train_loss=0.0804405024089674
Epoch 234
Epoch 234 :: Batch 0/1
Batch Loss = 0.07751537627271869
234, epoch_train_loss=0.07751537627271869
Epoch 235
Epoch 235 :: Batch 0/1
Batch Loss = 0.07601534688078829
235, epoch_train_loss=0.07601534688078829
Epoch 236
Epoch 236 :: Batch 0/1
Batch Loss = 0.07568963958466268
236, epoch_train_loss=0.07568963958466268
Epoch 237
Epoch 237 :: Batch 0/1
Batch Loss = 0.07310299894830063
237, epoch_train_loss=0.07310299894830063
Epoch 238
Epoch 238 :: Batch 0/1
Batch Loss = 0.07376911311952312
238, epoch_train_loss=0.07376911311952312
Epoch 239
Epoch 239 :: Batch 0/1
Batch Loss = 0.07062506783944887
239, epoch_train_loss=0.07062506783944887
Epoch 240
Epoch 240 :: Batch 0/1
Batch Loss = 0.07207955712940764
240, epoch_train_loss=0.07207955712940764
Epoch 241
Epoch 241 :: Batch 0/1
Batch Loss = 0.06901718321729879
241, epoch_train_loss=0.06901718321729879
Epoch 242
Epoch 242 :: Batch 0/1
Batch Loss = 0.07023334044140149
242, epoch_train_loss=0.07023334044140149
Epoch 243
Epoch 243 :: Batch 0/1
Batch Loss = 0.06751994887900988
243, epoch_train_loss=0.06751994887900988
Epoch 244
Epoch 244 :: Batch 0/1
Batch Loss = 0.06828394805633575
244, epoch_train_loss=0.06828394805633575
Epoch 245
Epoch 245 :: Batch 0/1
Batch Loss = 0.06618351021428144
245, epoch_train_loss=0.06618351021428144
Epoch 246
Epoch 246 :: Batch 0/1
Batch Loss = 0.0664609010476542
246, epoch_train_loss=0.0664609010476542
Epoch 247
Epoch 247 :: Batch 0/1
Batch Loss = 0.06513404405424632
247, epoch_train_loss=0.06513404405424632
Epoch 248
Epoch 248 :: Batch 0/1
Batch Loss = 0.06479736904667664
248, epoch_train_loss=0.06479736904667664
Epoch 249
Epoch 249 :: Batch 0/1
Batch Loss = 0.06437110699342986
249, epoch_train_loss=0.06437110699342986
Epoch 250
Epoch 250 :: Batch 0/1
Batch Loss = 0.06341464799659824
250, epoch_train_loss=0.06341464799659824
Epoch 251
Epoch 251 :: Batch 0/1
Batch Loss = 0.06348484349298707
251, epoch_train_loss=0.06348484349298707
Epoch 252
Epoch 252 :: Batch 0/1
Batch Loss = 0.062140810845420794
252, epoch_train_loss=0.062140810845420794
Epoch 253
Epoch 253 :: Batch 0/1
Batch Loss = 0.0625646445570813
253, epoch_train_loss=0.0625646445570813
Epoch 254
Epoch 254 :: Batch 0/1
Batch Loss = 0.06100946816672895
254, epoch_train_loss=0.06100946816672895
Epoch 255
Epoch 255 :: Batch 0/1
Batch Loss = 0.06157658456120237
255, epoch_train_loss=0.06157658456120237
Epoch 256
Epoch 256 :: Batch 0/1
Batch Loss = 0.06013324991335395
256, epoch_train_loss=0.06013324991335395
Epoch 257
Epoch 257 :: Batch 0/1
Batch Loss = 0.06049631134800922
257, epoch_train_loss=0.06049631134800922
Epoch 258
Epoch 258 :: Batch 0/1
Batch Loss = 0.059429354284153725
258, epoch_train_loss=0.059429354284153725
Epoch 259
Epoch 259 :: Batch 0/1
Batch Loss = 0.05944041164461747
259, epoch_train_loss=0.05944041164461747
Epoch 260
Epoch 260 :: Batch 0/1
Batch Loss = 0.05881714720577365
260, epoch_train_loss=0.05881714720577365
Epoch 261
Epoch 261 :: Batch 0/1
Batch Loss = 0.058416267528861664
261, epoch_train_loss=0.058416267528861664
Epoch 262
Epoch 262 :: Batch 0/1
Batch Loss = 0.05820198303456429
262, epoch_train_loss=0.05820198303456429
Epoch 263
Epoch 263 :: Batch 0/1
Batch Loss = 0.0574878098610789
263, epoch_train_loss=0.0574878098610789
Epoch 264
Epoch 264 :: Batch 0/1
Batch Loss = 0.05748548499887531
264, epoch_train_loss=0.05748548499887531
Epoch 265
Epoch 265 :: Batch 0/1
Batch Loss = 0.05672204396138644
265, epoch_train_loss=0.05672204396138644
Epoch 266
Epoch 266 :: Batch 0/1
Batch Loss = 0.05669675605606112
266, epoch_train_loss=0.05669675605606112
Epoch 267
Epoch 267 :: Batch 0/1
Batch Loss = 0.05609954492225162
267, epoch_train_loss=0.05609954492225162
Epoch 268
Epoch 268 :: Batch 0/1
Batch Loss = 0.05589244989084861
268, epoch_train_loss=0.05589244989084861
Epoch 269
Epoch 269 :: Batch 0/1
Batch Loss = 0.05553197140586811
269, epoch_train_loss=0.05553197140586811
Epoch 270
Epoch 270 :: Batch 0/1
Batch Loss = 0.05511832609393721
270, epoch_train_loss=0.05511832609393721
Epoch 271
Epoch 271 :: Batch 0/1
Batch Loss = 0.05492603380413016
271, epoch_train_loss=0.05492603380413016
Epoch 272
Epoch 272 :: Batch 0/1
Batch Loss = 0.05441674387105492
272, epoch_train_loss=0.05441674387105492
Epoch 273
Epoch 273 :: Batch 0/1
Batch Loss = 0.05426524466265136
273, epoch_train_loss=0.05426524466265136
Epoch 274
Epoch 274 :: Batch 0/1
Batch Loss = 0.053810104298673805
274, epoch_train_loss=0.053810104298673805
Epoch 275
Epoch 275 :: Batch 0/1
Batch Loss = 0.053580227391141474
275, epoch_train_loss=0.053580227391141474
Epoch 276
Epoch 276 :: Batch 0/1
Batch Loss = 0.05325611178044782
276, epoch_train_loss=0.05325611178044782
Epoch 277
Epoch 277 :: Batch 0/1
Batch Loss = 0.052917611393170035
277, epoch_train_loss=0.052917611393170035
Epoch 278
Epoch 278 :: Batch 0/1
Batch Loss = 0.05269050520063411
278, epoch_train_loss=0.05269050520063411
Epoch 279
Epoch 279 :: Batch 0/1
Batch Loss = 0.052309016529621055
279, epoch_train_loss=0.052309016529621055
Epoch 280
Epoch 280 :: Batch 0/1
Batch Loss = 0.05209883813899608
280, epoch_train_loss=0.05209883813899608
Epoch 281
Epoch 281 :: Batch 0/1
Batch Loss = 0.051751039230463876
281, epoch_train_loss=0.051751039230463876
Epoch 282
Epoch 282 :: Batch 0/1
Batch Loss = 0.051498994889731774
282, epoch_train_loss=0.051498994889731774
Epoch 283
Epoch 283 :: Batch 0/1
Batch Loss = 0.051220809122720555
283, epoch_train_loss=0.051220809122720555
Epoch 284
Epoch 284 :: Batch 0/1
Batch Loss = 0.0509165585268349
284, epoch_train_loss=0.0509165585268349
Epoch 285
Epoch 285 :: Batch 0/1
Batch Loss = 0.050687036599047636
285, epoch_train_loss=0.050687036599047636
Epoch 286
Epoch 286 :: Batch 0/1
Batch Loss = 0.050371663090524876
286, epoch_train_loss=0.050371663090524876
Epoch 287
Epoch 287 :: Batch 0/1
Batch Loss = 0.05014007839268371
287, epoch_train_loss=0.05014007839268371
Epoch 288
Epoch 288 :: Batch 0/1
Batch Loss = 0.04985410089035121
288, epoch_train_loss=0.04985410089035121
Epoch 289
Epoch 289 :: Batch 0/1
Batch Loss = 0.04959056349838248
289, epoch_train_loss=0.04959056349838248
Epoch 290
Epoch 290 :: Batch 0/1
Batch Loss = 0.04934048563543503
290, epoch_train_loss=0.04934048563543503
Epoch 291
Epoch 291 :: Batch 0/1
Batch Loss = 0.049055978956713674
291, epoch_train_loss=0.049055978956713674
Epoch 292
Epoch 292 :: Batch 0/1
Batch Loss = 0.04881990686924319
292, epoch_train_loss=0.04881990686924319
Epoch 293
Epoch 293 :: Batch 0/1
Batch Loss = 0.04854384598352895
293, epoch_train_loss=0.04854384598352895
Epoch 294
Epoch 294 :: Batch 0/1
Batch Loss = 0.048299252829762364
294, epoch_train_loss=0.048299252829762364
Epoch 295
Epoch 295 :: Batch 0/1
Batch Loss = 0.04804574284640021
295, epoch_train_loss=0.04804574284640021
Epoch 296
Epoch 296 :: Batch 0/1
Batch Loss = 0.04778734940437204
296, epoch_train_loss=0.04778734940437204
Epoch 297
Epoch 297 :: Batch 0/1
Batch Loss = 0.04755089800796218
297, epoch_train_loss=0.04755089800796218
Epoch 298
Epoch 298 :: Batch 0/1
Batch Loss = 0.04729118332744918
298, epoch_train_loss=0.04729118332744918
Epoch 299
Epoch 299 :: Batch 0/1
Batch Loss = 0.0470566188727515
299, epoch_train_loss=0.0470566188727515
Epoch 300
Epoch 300 :: Batch 0/1
Batch Loss = 0.046810361188613196
300, epoch_train_loss=0.046810361188613196
Epoch 301
Epoch 301 :: Batch 0/1
Batch Loss = 0.046569428984826924
301, epoch_train_loss=0.046569428984826924
Epoch 302
Epoch 302 :: Batch 0/1
Batch Loss = 0.04633747942640512
302, epoch_train_loss=0.04633747942640512
Epoch 303
Epoch 303 :: Batch 0/1
Batch Loss = 0.046094328798868094
303, epoch_train_loss=0.046094328798868094
Epoch 304
Epoch 304 :: Batch 0/1
Batch Loss = 0.04586721456148212
304, epoch_train_loss=0.04586721456148212
Epoch 305
Epoch 305 :: Batch 0/1
Batch Loss = 0.04563128169131387
305, epoch_train_loss=0.04563128169131387
Epoch 306
Epoch 306 :: Batch 0/1
Batch Loss = 0.04540190400156083
306, epoch_train_loss=0.04540190400156083
Epoch 307
Epoch 307 :: Batch 0/1
Batch Loss = 0.04517653061082745
307, epoch_train_loss=0.04517653061082745
Epoch 308
Epoch 308 :: Batch 0/1
Batch Loss = 0.04494674155294651
308, epoch_train_loss=0.04494674155294651
Epoch 309
Epoch 309 :: Batch 0/1
Batch Loss = 0.04472692005648636
309, epoch_train_loss=0.04472692005648636
Epoch 310
Epoch 310 :: Batch 0/1
Batch Loss = 0.044502032530684715
310, epoch_train_loss=0.044502032530684715
Epoch 311
Epoch 311 :: Batch 0/1
Batch Loss = 0.044282250110892256
311, epoch_train_loss=0.044282250110892256
Epoch 312
Epoch 312 :: Batch 0/1
Batch Loss = 0.04406453277656674
312, epoch_train_loss=0.04406453277656674
Epoch 313
Epoch 313 :: Batch 0/1
Batch Loss = 0.04384497507402977
313, epoch_train_loss=0.04384497507402977
Epoch 314
Epoch 314 :: Batch 0/1
Batch Loss = 0.04363173494836361
314, epoch_train_loss=0.04363173494836361
Epoch 315
Epoch 315 :: Batch 0/1
Batch Loss = 0.043415971431746775
315, epoch_train_loss=0.043415971431746775
Epoch 316
Epoch 316 :: Batch 0/1
Batch Loss = 0.04320372526061671
316, epoch_train_loss=0.04320372526061671
Epoch 317
Epoch 317 :: Batch 0/1
Batch Loss = 0.04299327829065551
317, epoch_train_loss=0.04299327829065551
Epoch 318
Epoch 318 :: Batch 0/1
Batch Loss = 0.04278200296595969
318, epoch_train_loss=0.04278200296595969
Epoch 319
Epoch 319 :: Batch 0/1
Batch Loss = 0.04257509684426487
319, epoch_train_loss=0.04257509684426487
Epoch 320
Epoch 320 :: Batch 0/1
Batch Loss = 0.04236745011180096
320, epoch_train_loss=0.04236745011180096
Epoch 321
Epoch 321 :: Batch 0/1
Batch Loss = 0.042162275707101696
321, epoch_train_loss=0.042162275707101696
Epoch 322
Epoch 322 :: Batch 0/1
Batch Loss = 0.041959408609111876
322, epoch_train_loss=0.041959408609111876
Epoch 323
Epoch 323 :: Batch 0/1
Batch Loss = 0.04175661216508492
323, epoch_train_loss=0.04175661216508492
Epoch 324
Epoch 324 :: Batch 0/1
Batch Loss = 0.04155730481652606
324, epoch_train_loss=0.04155730481652606
Epoch 325
Epoch 325 :: Batch 0/1
Batch Loss = 0.0413586917093865
325, epoch_train_loss=0.0413586917093865
Epoch 326
Epoch 326 :: Batch 0/1
Batch Loss = 0.041161952443087464
326, epoch_train_loss=0.041161952443087464
Epoch 327
Epoch 327 :: Batch 0/1
Batch Loss = 0.04096784140689676
327, epoch_train_loss=0.04096784140689676
Epoch 328
Epoch 328 :: Batch 0/1
Batch Loss = 0.04077441311745162
328, epoch_train_loss=0.04077441311745162
Epoch 329
Epoch 329 :: Batch 0/1
Batch Loss = 0.040583694948350096
329, epoch_train_loss=0.040583694948350096
Epoch 330
Epoch 330 :: Batch 0/1
Batch Loss = 0.04039449404952059
330, epoch_train_loss=0.04039449404952059
Epoch 331
Epoch 331 :: Batch 0/1
Batch Loss = 0.04020671015223814
331, epoch_train_loss=0.04020671015223814
Epoch 332
Epoch 332 :: Batch 0/1
Batch Loss = 0.040021422977071994
332, epoch_train_loss=0.040021422977071994
Epoch 333
Epoch 333 :: Batch 0/1
Batch Loss = 0.03983727137129472
333, epoch_train_loss=0.03983727137129472
Epoch 334
Epoch 334 :: Batch 0/1
Batch Loss = 0.03965515371038255
334, epoch_train_loss=0.03965515371038255
Epoch 335
Epoch 335 :: Batch 0/1
Batch Loss = 0.03947496689377313
335, epoch_train_loss=0.03947496689377313
Epoch 336
Epoch 336 :: Batch 0/1
Batch Loss = 0.039296050805749695
336, epoch_train_loss=0.039296050805749695
Epoch 337
Epoch 337 :: Batch 0/1
Batch Loss = 0.03911926156856564
337, epoch_train_loss=0.03911926156856564
Epoch 338
Epoch 338 :: Batch 0/1
Batch Loss = 0.03894394912685386
338, epoch_train_loss=0.03894394912685386
Epoch 339
Epoch 339 :: Batch 0/1
Batch Loss = 0.03877015174299905
339, epoch_train_loss=0.03877015174299905
Epoch 340
Epoch 340 :: Batch 0/1
Batch Loss = 0.0385982632353171
340, epoch_train_loss=0.0385982632353171
Epoch 341
Epoch 341 :: Batch 0/1
Batch Loss = 0.03842767409796621
341, epoch_train_loss=0.03842767409796621
Epoch 342
Epoch 342 :: Batch 0/1
Batch Loss = 0.03825873513548282
342, epoch_train_loss=0.03825873513548282
Epoch 343
Epoch 343 :: Batch 0/1
Batch Loss = 0.03809141710768063
343, epoch_train_loss=0.03809141710768063
Epoch 344
Epoch 344 :: Batch 0/1
Batch Loss = 0.037925380021656754
344, epoch_train_loss=0.037925380021656754
Epoch 345
Epoch 345 :: Batch 0/1
Batch Loss = 0.037760979692344085
345, epoch_train_loss=0.037760979692344085
Epoch 346
Epoch 346 :: Batch 0/1
Batch Loss = 0.0375979944406594
346, epoch_train_loss=0.0375979944406594
Epoch 347
Epoch 347 :: Batch 0/1
Batch Loss = 0.03743634105768889
347, epoch_train_loss=0.03743634105768889
Epoch 348
Epoch 348 :: Batch 0/1
Batch Loss = 0.03727624253508093
348, epoch_train_loss=0.03727624253508093
Epoch 349
Epoch 349 :: Batch 0/1
Batch Loss = 0.03711744937353608
349, epoch_train_loss=0.03711744937353608
Epoch 350
Epoch 350 :: Batch 0/1
Batch Loss = 0.03696001647865253
350, epoch_train_loss=0.03696001647865253
Epoch 351
Epoch 351 :: Batch 0/1
Batch Loss = 0.03680402726488167
351, epoch_train_loss=0.03680402726488167
Epoch 352
Epoch 352 :: Batch 0/1
Batch Loss = 0.03664928737729933
352, epoch_train_loss=0.03664928737729933
Epoch 353
Epoch 353 :: Batch 0/1
Batch Loss = 0.036495895843287975
353, epoch_train_loss=0.036495895843287975
Epoch 354
Epoch 354 :: Batch 0/1
Batch Loss = 0.036343846867352265
354, epoch_train_loss=0.036343846867352265
Epoch 355
Epoch 355 :: Batch 0/1
Batch Loss = 0.03619301748659274
355, epoch_train_loss=0.03619301748659274
Epoch 356
Epoch 356 :: Batch 0/1
Batch Loss = 0.03604350042968
356, epoch_train_loss=0.03604350042968
Epoch 357
Epoch 357 :: Batch 0/1
Batch Loss = 0.03589524954075579
357, epoch_train_loss=0.03589524954075579
Epoch 358
Epoch 358 :: Batch 0/1
Batch Loss = 0.03574819791190302
358, epoch_train_loss=0.03574819791190302
Epoch 359
Epoch 359 :: Batch 0/1
Batch Loss = 0.035602416619519364
359, epoch_train_loss=0.035602416619519364
Epoch 360
Epoch 360 :: Batch 0/1
Batch Loss = 0.035457850818914
360, epoch_train_loss=0.035457850818914
Epoch 361
Epoch 361 :: Batch 0/1
Batch Loss = 0.035314468990474135
361, epoch_train_loss=0.035314468990474135
Epoch 362
Epoch 362 :: Batch 0/1
Batch Loss = 0.03517231911310487
362, epoch_train_loss=0.03517231911310487
Epoch 363
Epoch 363 :: Batch 0/1
Batch Loss = 0.035031348656659744
363, epoch_train_loss=0.035031348656659744
Epoch 364
Epoch 364 :: Batch 0/1
Batch Loss = 0.03489154448515659
364, epoch_train_loss=0.03489154448515659
Epoch 365
Epoch 365 :: Batch 0/1
Batch Loss = 0.03475293706683707
365, epoch_train_loss=0.03475293706683707
Epoch 366
Epoch 366 :: Batch 0/1
Batch Loss = 0.034615482588575296
366, epoch_train_loss=0.034615482588575296
Epoch 367
Epoch 367 :: Batch 0/1
Batch Loss = 0.03447917847484962
367, epoch_train_loss=0.03447917847484962
Epoch 368
Epoch 368 :: Batch 0/1
Batch Loss = 0.03434404376282456
368, epoch_train_loss=0.03434404376282456
Epoch 369
Epoch 369 :: Batch 0/1
Batch Loss = 0.03421004474686462
369, epoch_train_loss=0.03421004474686462
Epoch 370
Epoch 370 :: Batch 0/1
Batch Loss = 0.034077183787117486
370, epoch_train_loss=0.034077183787117486
Epoch 371
Epoch 371 :: Batch 0/1
Batch Loss = 0.03394547318842886
371, epoch_train_loss=0.03394547318842886
Epoch 372
Epoch 372 :: Batch 0/1
Batch Loss = 0.03381488917789224
372, epoch_train_loss=0.03381488917789224
Epoch 373
Epoch 373 :: Batch 0/1
Batch Loss = 0.03368543578998087
373, epoch_train_loss=0.03368543578998087
Epoch 374
Epoch 374 :: Batch 0/1
Batch Loss = 0.03355712221660379
374, epoch_train_loss=0.03355712221660379
Epoch 375
Epoch 375 :: Batch 0/1
Batch Loss = 0.03342993250390316
375, epoch_train_loss=0.03342993250390316
Epoch 376
Epoch 376 :: Batch 0/1
Batch Loss = 0.03330387008618522
376, epoch_train_loss=0.03330387008618522
Epoch 377
Epoch 377 :: Batch 0/1
Batch Loss = 0.03317894243413414
377, epoch_train_loss=0.03317894243413414
Epoch 378
Epoch 378 :: Batch 0/1
Batch Loss = 0.03305513842730964
378, epoch_train_loss=0.03305513842730964
Epoch 379
Epoch 379 :: Batch 0/1
Batch Loss = 0.0329324601261314
379, epoch_train_loss=0.0329324601261314
Epoch 380
Epoch 380 :: Batch 0/1
Batch Loss = 0.03281091393606081
380, epoch_train_loss=0.03281091393606081
Epoch 381
Epoch 381 :: Batch 0/1
Batch Loss = 0.0326904919966816
381, epoch_train_loss=0.0326904919966816
Epoch 382
Epoch 382 :: Batch 0/1
Batch Loss = 0.03257119510735557
382, epoch_train_loss=0.03257119510735557
Epoch 383
Epoch 383 :: Batch 0/1
Batch Loss = 0.032453028225828205
383, epoch_train_loss=0.032453028225828205
Epoch 384
Epoch 384 :: Batch 0/1
Batch Loss = 0.032335985539712696
384, epoch_train_loss=0.032335985539712696
Epoch 385
Epoch 385 :: Batch 0/1
Batch Loss = 0.03222006625069686
385, epoch_train_loss=0.03222006625069686
Epoch 386
Epoch 386 :: Batch 0/1
Batch Loss = 0.03210527345940572
386, epoch_train_loss=0.03210527345940572
Epoch 387
Epoch 387 :: Batch 0/1
Batch Loss = 0.03199160246666234
387, epoch_train_loss=0.03199160246666234
Epoch 388
Epoch 388 :: Batch 0/1
Batch Loss = 0.031879050383365616
388, epoch_train_loss=0.031879050383365616
Epoch 389
Epoch 389 :: Batch 0/1
Batch Loss = 0.031767617937199684
389, epoch_train_loss=0.031767617937199684
Epoch 390
Epoch 390 :: Batch 0/1
Batch Loss = 0.03165730040604854
390, epoch_train_loss=0.03165730040604854
Epoch 391
Epoch 391 :: Batch 0/1
Batch Loss = 0.031548092557265296
391, epoch_train_loss=0.031548092557265296
Epoch 392
Epoch 392 :: Batch 0/1
Batch Loss = 0.03143999244122271
392, epoch_train_loss=0.03143999244122271
Epoch 393
Epoch 393 :: Batch 0/1
Batch Loss = 0.031332994473754386
393, epoch_train_loss=0.031332994473754386
Epoch 394
Epoch 394 :: Batch 0/1
Batch Loss = 0.03122709136816551
394, epoch_train_loss=0.03122709136816551
Epoch 395
Epoch 395 :: Batch 0/1
Batch Loss = 0.031122278324566075
395, epoch_train_loss=0.031122278324566075
Epoch 396
Epoch 396 :: Batch 0/1
Batch Loss = 0.03101854852786055
396, epoch_train_loss=0.03101854852786055
Epoch 397
Epoch 397 :: Batch 0/1
Batch Loss = 0.03091589298065037
397, epoch_train_loss=0.03091589298065037
Epoch 398
Epoch 398 :: Batch 0/1
Batch Loss = 0.03081430414838687
398, epoch_train_loss=0.03081430414838687
Epoch 399
Epoch 399 :: Batch 0/1
Batch Loss = 0.030713773877124588
399, epoch_train_loss=0.030713773877124588
Epoch 400
Epoch 400 :: Batch 0/1
Batch Loss = 0.030614291860709186
400, epoch_train_loss=0.030614291860709186
Epoch 401
Epoch 401 :: Batch 0/1
Batch Loss = 0.030515848357060938
401, epoch_train_loss=0.030515848357060938
Epoch 402
Epoch 402 :: Batch 0/1
Batch Loss = 0.030418433887096617
402, epoch_train_loss=0.030418433887096617
Epoch 403
Epoch 403 :: Batch 0/1
Batch Loss = 0.030322037248922972
403, epoch_train_loss=0.030322037248922972
Epoch 404
Epoch 404 :: Batch 0/1
Batch Loss = 0.030226647183627265
404, epoch_train_loss=0.030226647183627265
Epoch 405
Epoch 405 :: Batch 0/1
Batch Loss = 0.03013225304956334
405, epoch_train_loss=0.03013225304956334
Epoch 406
Epoch 406 :: Batch 0/1
Batch Loss = 0.030038843278084666
406, epoch_train_loss=0.030038843278084666
Epoch 407
Epoch 407 :: Batch 0/1
Batch Loss = 0.029946405840568867
407, epoch_train_loss=0.029946405840568867
Epoch 408
Epoch 408 :: Batch 0/1
Batch Loss = 0.02985492934165324
408, epoch_train_loss=0.02985492934165324
Epoch 409
Epoch 409 :: Batch 0/1
Batch Loss = 0.029764402226518293
409, epoch_train_loss=0.029764402226518293
Epoch 410
Epoch 410 :: Batch 0/1
Batch Loss = 0.029674812448039815
410, epoch_train_loss=0.029674812448039815
Epoch 411
Epoch 411 :: Batch 0/1
Batch Loss = 0.02958614838938237
411, epoch_train_loss=0.02958614838938237
Epoch 412
Epoch 412 :: Batch 0/1
Batch Loss = 0.029498398782049486
412, epoch_train_loss=0.029498398782049486
Epoch 413
Epoch 413 :: Batch 0/1
Batch Loss = 0.029411552094577898
413, epoch_train_loss=0.029411552094577898
Epoch 414
Epoch 414 :: Batch 0/1
Batch Loss = 0.029325597009053857
414, epoch_train_loss=0.029325597009053857
Epoch 415
Epoch 415 :: Batch 0/1
Batch Loss = 0.02924052274586488
415, epoch_train_loss=0.02924052274586488
Epoch 416
Epoch 416 :: Batch 0/1
Batch Loss = 0.029156318593789123
416, epoch_train_loss=0.029156318593789123
Epoch 417
Epoch 417 :: Batch 0/1
Batch Loss = 0.029072973872051818
417, epoch_train_loss=0.029072973872051818
Epoch 418
Epoch 418 :: Batch 0/1
Batch Loss = 0.02899047839450996
418, epoch_train_loss=0.02899047839450996
Epoch 419
Epoch 419 :: Batch 0/1
Batch Loss = 0.028908822270540313
419, epoch_train_loss=0.028908822270540313
Epoch 420
Epoch 420 :: Batch 0/1
Batch Loss = 0.028827995660643594
420, epoch_train_loss=0.028827995660643594
Epoch 421
Epoch 421 :: Batch 0/1
Batch Loss = 0.02874798899173398
421, epoch_train_loss=0.02874798899173398
Epoch 422
Epoch 422 :: Batch 0/1
Batch Loss = 0.028668793035483458
422, epoch_train_loss=0.028668793035483458
Epoch 423
Epoch 423 :: Batch 0/1
Batch Loss = 0.028590398697875306
423, epoch_train_loss=0.028590398697875306
Epoch 424
Epoch 424 :: Batch 0/1
Batch Loss = 0.028512796979965505
424, epoch_train_loss=0.028512796979965505
Epoch 425
Epoch 425 :: Batch 0/1
Batch Loss = 0.028435979114180863
425, epoch_train_loss=0.028435979114180863
Epoch 426
Epoch 426 :: Batch 0/1
Batch Loss = 0.028359936506949936
426, epoch_train_loss=0.028359936506949936
Epoch 427
Epoch 427 :: Batch 0/1
Batch Loss = 0.028284660580386034
427, epoch_train_loss=0.028284660580386034
Epoch 428
Epoch 428 :: Batch 0/1
Batch Loss = 0.02821014282794157
428, epoch_train_loss=0.02821014282794157
Epoch 429
Epoch 429 :: Batch 0/1
Batch Loss = 0.028136374875046753
429, epoch_train_loss=0.028136374875046753
Epoch 430
Epoch 430 :: Batch 0/1
Batch Loss = 0.028063348358264652
430, epoch_train_loss=0.028063348358264652
Epoch 431
Epoch 431 :: Batch 0/1
Batch Loss = 0.02799105486590462
431, epoch_train_loss=0.02799105486590462
Epoch 432
Epoch 432 :: Batch 0/1
Batch Loss = 0.027919486031186352
432, epoch_train_loss=0.027919486031186352
Epoch 433
Epoch 433 :: Batch 0/1
Batch Loss = 0.027848633485006675
433, epoch_train_loss=0.027848633485006675
Epoch 434
Epoch 434 :: Batch 0/1
Batch Loss = 0.027778488794651352
434, epoch_train_loss=0.027778488794651352
Epoch 435
Epoch 435 :: Batch 0/1
Batch Loss = 0.027709043488526017
435, epoch_train_loss=0.027709043488526017
Epoch 436
Epoch 436 :: Batch 0/1
Batch Loss = 0.027640289063413304
436, epoch_train_loss=0.027640289063413304
Epoch 437
Epoch 437 :: Batch 0/1
Batch Loss = 0.02757221699035238
437, epoch_train_loss=0.02757221699035238
Epoch 438
Epoch 438 :: Batch 0/1
Batch Loss = 0.027504818664767443
438, epoch_train_loss=0.027504818664767443
Epoch 439
Epoch 439 :: Batch 0/1
Batch Loss = 0.027438085427464794
439, epoch_train_loss=0.027438085427464794
Epoch 440
Epoch 440 :: Batch 0/1
Batch Loss = 0.027372008606211685
440, epoch_train_loss=0.027372008606211685
Epoch 441
Epoch 441 :: Batch 0/1
Batch Loss = 0.027306579492589567
441, epoch_train_loss=0.027306579492589567
Epoch 442
Epoch 442 :: Batch 0/1
Batch Loss = 0.0272417893211882
442, epoch_train_loss=0.0272417893211882
Epoch 443
Epoch 443 :: Batch 0/1
Batch Loss = 0.027177629322150145
443, epoch_train_loss=0.027177629322150145
Epoch 444
Epoch 444 :: Batch 0/1
Batch Loss = 0.027114090713344616
444, epoch_train_loss=0.027114090713344616
Epoch 445
Epoch 445 :: Batch 0/1
Batch Loss = 0.027051164695367995
445, epoch_train_loss=0.027051164695367995
Epoch 446
Epoch 446 :: Batch 0/1
Batch Loss = 0.026988842469951018
446, epoch_train_loss=0.026988842469951018
Epoch 447
Epoch 447 :: Batch 0/1
Batch Loss = 0.02692711523380345
447, epoch_train_loss=0.02692711523380345
Epoch 448
Epoch 448 :: Batch 0/1
Batch Loss = 0.026865974207370434
448, epoch_train_loss=0.026865974207370434
Epoch 449
Epoch 449 :: Batch 0/1
Batch Loss = 0.026805410631379794
449, epoch_train_loss=0.026805410631379794
Epoch 450
Epoch 450 :: Batch 0/1
Batch Loss = 0.026745415763691834
450, epoch_train_loss=0.026745415763691834
Epoch 451
Epoch 451 :: Batch 0/1
Batch Loss = 0.026685980928397184
451, epoch_train_loss=0.026685980928397184
Epoch 452
Epoch 452 :: Batch 0/1
Batch Loss = 0.02662709751638198
452, epoch_train_loss=0.02662709751638198
Epoch 453
Epoch 453 :: Batch 0/1
Batch Loss = 0.026568757028188388
453, epoch_train_loss=0.026568757028188388
Epoch 454
Epoch 454 :: Batch 0/1
Batch Loss = 0.026510951105648724
454, epoch_train_loss=0.026510951105648724
Epoch 455
Epoch 455 :: Batch 0/1
Batch Loss = 0.026453671600386447
455, epoch_train_loss=0.026453671600386447
Epoch 456
Epoch 456 :: Batch 0/1
Batch Loss = 0.026396910663103824
456, epoch_train_loss=0.026396910663103824
Epoch 457
Epoch 457 :: Batch 0/1
Batch Loss = 0.026340660806010087
457, epoch_train_loss=0.026340660806010087
Epoch 458
Epoch 458 :: Batch 0/1
Batch Loss = 0.026284915021159014
458, epoch_train_loss=0.026284915021159014
Epoch 459
Epoch 459 :: Batch 0/1
Batch Loss = 0.02622966689612423
459, epoch_train_loss=0.02622966689612423
Epoch 460
Epoch 460 :: Batch 0/1
Batch Loss = 0.026174910671529095
460, epoch_train_loss=0.026174910671529095
Epoch 461
Epoch 461 :: Batch 0/1
Batch Loss = 0.02612064132099145
461, epoch_train_loss=0.02612064132099145
Epoch 462
Epoch 462 :: Batch 0/1
Batch Loss = 0.026066854511365953
462, epoch_train_loss=0.026066854511365953
Epoch 463
Epoch 463 :: Batch 0/1
Batch Loss = 0.026013546490378684
463, epoch_train_loss=0.026013546490378684
Epoch 464
Epoch 464 :: Batch 0/1
Batch Loss = 0.02596071385085847
464, epoch_train_loss=0.02596071385085847
Epoch 465
Epoch 465 :: Batch 0/1
Batch Loss = 0.02590835318623392
465, epoch_train_loss=0.02590835318623392
Epoch 466
Epoch 466 :: Batch 0/1
Batch Loss = 0.02585646066466791
466, epoch_train_loss=0.02585646066466791
Epoch 467
Epoch 467 :: Batch 0/1
Batch Loss = 0.02580503161699412
467, epoch_train_loss=0.02580503161699412
Epoch 468
Epoch 468 :: Batch 0/1
Batch Loss = 0.02575406013437957
468, epoch_train_loss=0.02575406013437957
Epoch 469
Epoch 469 :: Batch 0/1
Batch Loss = 0.025703538840576735
469, epoch_train_loss=0.025703538840576735
Epoch 470
Epoch 470 :: Batch 0/1
Batch Loss = 0.02565345878607632
470, epoch_train_loss=0.02565345878607632
Epoch 471
Epoch 471 :: Batch 0/1
Batch Loss = 0.02560380942392062
471, epoch_train_loss=0.02560380942392062
Epoch 472
Epoch 472 :: Batch 0/1
Batch Loss = 0.02555457859053052
472, epoch_train_loss=0.02555457859053052
Epoch 473
Epoch 473 :: Batch 0/1
Batch Loss = 0.02550575240624894
473, epoch_train_loss=0.02550575240624894
Epoch 474
Epoch 474 :: Batch 0/1
Batch Loss = 0.025457315008093236
474, epoch_train_loss=0.025457315008093236
Epoch 475
Epoch 475 :: Batch 0/1
Batch Loss = 0.025409248234990946
475, epoch_train_loss=0.025409248234990946
Epoch 476
Epoch 476 :: Batch 0/1
Batch Loss = 0.02536153160720038
476, epoch_train_loss=0.02536153160720038
Epoch 477
Epoch 477 :: Batch 0/1
Batch Loss = 0.02531414357359112
477, epoch_train_loss=0.02531414357359112
Epoch 478
Epoch 478 :: Batch 0/1
Batch Loss = 0.02526706594387518
478, epoch_train_loss=0.02526706594387518
Epoch 479
Epoch 479 :: Batch 0/1
Batch Loss = 0.025220294946767693
479, epoch_train_loss=0.025220294946767693
Epoch 480
Epoch 480 :: Batch 0/1
Batch Loss = 0.025173862047654334
480, epoch_train_loss=0.025173862047654334
Epoch 481
Epoch 481 :: Batch 0/1
Batch Loss = 0.025127858782765734
481, epoch_train_loss=0.025127858782765734
Epoch 482
Epoch 482 :: Batch 0/1
Batch Loss = 0.025082432601990244
482, epoch_train_loss=0.025082432601990244
Epoch 483
Epoch 483 :: Batch 0/1
Batch Loss = 0.025037695295326935
483, epoch_train_loss=0.025037695295326935
Epoch 484
Epoch 484 :: Batch 0/1
Batch Loss = 0.024993563256944098
484, epoch_train_loss=0.024993563256944098
Epoch 485
Epoch 485 :: Batch 0/1
Batch Loss = 0.024949743721303493
485, epoch_train_loss=0.024949743721303493
Epoch 486
Epoch 486 :: Batch 0/1
Batch Loss = 0.024906000045493155
486, epoch_train_loss=0.024906000045493155
Epoch 487
Epoch 487 :: Batch 0/1
Batch Loss = 0.024862369642300812
487, epoch_train_loss=0.024862369642300812
Epoch 488
Epoch 488 :: Batch 0/1
Batch Loss = 0.02481904859843155
488, epoch_train_loss=0.02481904859843155
Epoch 489
Epoch 489 :: Batch 0/1
Batch Loss = 0.024776200963792152
489, epoch_train_loss=0.024776200963792152
Epoch 490
Epoch 490 :: Batch 0/1
Batch Loss = 0.02473389719925881
490, epoch_train_loss=0.02473389719925881
Epoch 491
Epoch 491 :: Batch 0/1
Batch Loss = 0.02469206119955765
491, epoch_train_loss=0.02469206119955765
Epoch 492
Epoch 492 :: Batch 0/1
Batch Loss = 0.02465049754594633
492, epoch_train_loss=0.02465049754594633
Epoch 493
Epoch 493 :: Batch 0/1
Batch Loss = 0.024609089343931054
493, epoch_train_loss=0.024609089343931054
Epoch 494
Epoch 494 :: Batch 0/1
Batch Loss = 0.02456789562289797
494, epoch_train_loss=0.02456789562289797
Epoch 495
Epoch 495 :: Batch 0/1
Batch Loss = 0.02452701246805563
495, epoch_train_loss=0.02452701246805563
Epoch 496
Epoch 496 :: Batch 0/1
Batch Loss = 0.024486473329857913
496, epoch_train_loss=0.024486473329857913
Epoch 497
Epoch 497 :: Batch 0/1
Batch Loss = 0.02444630163371265
497, epoch_train_loss=0.02444630163371265
Epoch 498
Epoch 498 :: Batch 0/1
Batch Loss = 0.024406525011476964
498, epoch_train_loss=0.024406525011476964
Epoch 499
Epoch 499 :: Batch 0/1
Batch Loss = 0.024367104713126118
499, epoch_train_loss=0.024367104713126118
Epoch 500
Epoch 500 :: Batch 0/1
Batch Loss = 0.0243279490890438
500, epoch_train_loss=0.0243279490890438
Epoch 501
Epoch 501 :: Batch 0/1
Batch Loss = 0.02428902852929032
501, epoch_train_loss=0.02428902852929032
Epoch 502
Epoch 502 :: Batch 0/1
Batch Loss = 0.024250398953731718
502, epoch_train_loss=0.024250398953731718
Epoch 503
Epoch 503 :: Batch 0/1
Batch Loss = 0.024212103474208994
503, epoch_train_loss=0.024212103474208994
Epoch 504
Epoch 504 :: Batch 0/1
Batch Loss = 0.024174125459324586
504, epoch_train_loss=0.024174125459324586
Epoch 505
Epoch 505 :: Batch 0/1
Batch Loss = 0.02413643672603075
505, epoch_train_loss=0.02413643672603075
Epoch 506
Epoch 506 :: Batch 0/1
Batch Loss = 0.024099028907038765
506, epoch_train_loss=0.024099028907038765
Epoch 507
Epoch 507 :: Batch 0/1
Batch Loss = 0.02406189417991596
507, epoch_train_loss=0.02406189417991596
Epoch 508
Epoch 508 :: Batch 0/1
Batch Loss = 0.02402502048199046
508, epoch_train_loss=0.02402502048199046
Epoch 509
Epoch 509 :: Batch 0/1
Batch Loss = 0.02398841602515753
509, epoch_train_loss=0.02398841602515753
Epoch 510
Epoch 510 :: Batch 0/1
Batch Loss = 0.023952106243164507
510, epoch_train_loss=0.023952106243164507
Epoch 511
Epoch 511 :: Batch 0/1
Batch Loss = 0.023916099705194538
511, epoch_train_loss=0.023916099705194538
Epoch 512
Epoch 512 :: Batch 0/1
Batch Loss = 0.02388037768516758
512, epoch_train_loss=0.02388037768516758
Epoch 513
Epoch 513 :: Batch 0/1
Batch Loss = 0.023844918061876483
513, epoch_train_loss=0.023844918061876483
Epoch 514
Epoch 514 :: Batch 0/1
Batch Loss = 0.023809713388521253
514, epoch_train_loss=0.023809713388521253
Epoch 515
Epoch 515 :: Batch 0/1
Batch Loss = 0.02377476698209951
515, epoch_train_loss=0.02377476698209951
Epoch 516
Epoch 516 :: Batch 0/1
Batch Loss = 0.023740083958583603
516, epoch_train_loss=0.023740083958583603
Epoch 517
Epoch 517 :: Batch 0/1
Batch Loss = 0.02370566823399074
517, epoch_train_loss=0.02370566823399074
Epoch 518
Epoch 518 :: Batch 0/1
Batch Loss = 0.02367151868917303
518, epoch_train_loss=0.02367151868917303
Epoch 519
Epoch 519 :: Batch 0/1
Batch Loss = 0.023637625726355777
519, epoch_train_loss=0.023637625726355777
Epoch 520
Epoch 520 :: Batch 0/1
Batch Loss = 0.02360397713675823
520, epoch_train_loss=0.02360397713675823
Epoch 521
Epoch 521 :: Batch 0/1
Batch Loss = 0.023570569146393643
521, epoch_train_loss=0.023570569146393643
Epoch 522
Epoch 522 :: Batch 0/1
Batch Loss = 0.023537407323608123
522, epoch_train_loss=0.023537407323608123
Epoch 523
Epoch 523 :: Batch 0/1
Batch Loss = 0.023504496306490517
523, epoch_train_loss=0.023504496306490517
Epoch 524
Epoch 524 :: Batch 0/1
Batch Loss = 0.023471832908042677
524, epoch_train_loss=0.023471832908042677
Epoch 525
Epoch 525 :: Batch 0/1
Batch Loss = 0.02343941007490587
525, epoch_train_loss=0.02343941007490587
Epoch 526
Epoch 526 :: Batch 0/1
Batch Loss = 0.023407222546624088
526, epoch_train_loss=0.023407222546624088
Epoch 527
Epoch 527 :: Batch 0/1
Batch Loss = 0.023375267400982305
527, epoch_train_loss=0.023375267400982305
Epoch 528
Epoch 528 :: Batch 0/1
Batch Loss = 0.023343543088560032
528, epoch_train_loss=0.023343543088560032
Epoch 529
Epoch 529 :: Batch 0/1
Batch Loss = 0.023312049593094197
529, epoch_train_loss=0.023312049593094197
Epoch 530
Epoch 530 :: Batch 0/1
Batch Loss = 0.023280786366772572
530, epoch_train_loss=0.023280786366772572
Epoch 531
Epoch 531 :: Batch 0/1
Batch Loss = 0.02324974925690077
531, epoch_train_loss=0.02324974925690077
Epoch 532
Epoch 532 :: Batch 0/1
Batch Loss = 0.023218932027902203
532, epoch_train_loss=0.023218932027902203
Epoch 533
Epoch 533 :: Batch 0/1
Batch Loss = 0.02318833113672561
533, epoch_train_loss=0.02318833113672561
Epoch 534
Epoch 534 :: Batch 0/1
Batch Loss = 0.023157946495877022
534, epoch_train_loss=0.023157946495877022
Epoch 535
Epoch 535 :: Batch 0/1
Batch Loss = 0.023127777915743112
535, epoch_train_loss=0.023127777915743112
Epoch 536
Epoch 536 :: Batch 0/1
Batch Loss = 0.023097823102984683
536, epoch_train_loss=0.023097823102984683
Epoch 537
Epoch 537 :: Batch 0/1
Batch Loss = 0.02306807861949017
537, epoch_train_loss=0.02306807861949017
Epoch 538
Epoch 538 :: Batch 0/1
Batch Loss = 0.02303854086430652
538, epoch_train_loss=0.02303854086430652
Epoch 539
Epoch 539 :: Batch 0/1
Batch Loss = 0.023009206532890892
539, epoch_train_loss=0.023009206532890892
Epoch 540
Epoch 540 :: Batch 0/1
Batch Loss = 0.02298007353153892
540, epoch_train_loss=0.02298007353153892
Epoch 541
Epoch 541 :: Batch 0/1
Batch Loss = 0.02295114096613791
541, epoch_train_loss=0.02295114096613791
Epoch 542
Epoch 542 :: Batch 0/1
Batch Loss = 0.022922407396008905
542, epoch_train_loss=0.022922407396008905
Epoch 543
Epoch 543 :: Batch 0/1
Batch Loss = 0.022893869739176176
543, epoch_train_loss=0.022893869739176176
Epoch 544
Epoch 544 :: Batch 0/1
Batch Loss = 0.022865524443780722
544, epoch_train_loss=0.022865524443780722
Epoch 545
Epoch 545 :: Batch 0/1
Batch Loss = 0.02283736890188744
545, epoch_train_loss=0.02283736890188744
Epoch 546
Epoch 546 :: Batch 0/1
Batch Loss = 0.02280940131384879
546, epoch_train_loss=0.02280940131384879
Epoch 547
Epoch 547 :: Batch 0/1
Batch Loss = 0.02278161986016547
547, epoch_train_loss=0.02278161986016547
Epoch 548
Epoch 548 :: Batch 0/1
Batch Loss = 0.02275402248668588
548, epoch_train_loss=0.02275402248668588
Epoch 549
Epoch 549 :: Batch 0/1
Batch Loss = 0.0227266069519231
549, epoch_train_loss=0.0227266069519231
Epoch 550
Epoch 550 :: Batch 0/1
Batch Loss = 0.02269937080558984
550, epoch_train_loss=0.02269937080558984
Epoch 551
Epoch 551 :: Batch 0/1
Batch Loss = 0.022672311677772557
551, epoch_train_loss=0.022672311677772557
Epoch 552
Epoch 552 :: Batch 0/1
Batch Loss = 0.02264542768609245
552, epoch_train_loss=0.02264542768609245
Epoch 553
Epoch 553 :: Batch 0/1
Batch Loss = 0.022618717183241245
553, epoch_train_loss=0.022618717183241245
Epoch 554
Epoch 554 :: Batch 0/1
Batch Loss = 0.02259217818229128
554, epoch_train_loss=0.02259217818229128
Epoch 555
Epoch 555 :: Batch 0/1
Batch Loss = 0.022565808368951844
555, epoch_train_loss=0.022565808368951844
Epoch 556
Epoch 556 :: Batch 0/1
Batch Loss = 0.022539605613808692
556, epoch_train_loss=0.022539605613808692
Epoch 557
Epoch 557 :: Batch 0/1
Batch Loss = 0.022513568141834295
557, epoch_train_loss=0.022513568141834295
Epoch 558
Epoch 558 :: Batch 0/1
Batch Loss = 0.02248769428444289
558, epoch_train_loss=0.02248769428444289
Epoch 559
Epoch 559 :: Batch 0/1
Batch Loss = 0.02246198232924297
559, epoch_train_loss=0.02246198232924297
Epoch 560
Epoch 560 :: Batch 0/1
Batch Loss = 0.022436430492836196
560, epoch_train_loss=0.022436430492836196
Epoch 561
Epoch 561 :: Batch 0/1
Batch Loss = 0.022411036903353394
561, epoch_train_loss=0.022411036903353394
Epoch 562
Epoch 562 :: Batch 0/1
Batch Loss = 0.022385799719694774
562, epoch_train_loss=0.022385799719694774
Epoch 563
Epoch 563 :: Batch 0/1
Batch Loss = 0.02236071733861551
563, epoch_train_loss=0.02236071733861551
Epoch 564
Epoch 564 :: Batch 0/1
Batch Loss = 0.022335788365347106
564, epoch_train_loss=0.022335788365347106
Epoch 565
Epoch 565 :: Batch 0/1
Batch Loss = 0.02231101136590565
565, epoch_train_loss=0.02231101136590565
Epoch 566
Epoch 566 :: Batch 0/1
Batch Loss = 0.022286384768035895
566, epoch_train_loss=0.022286384768035895
Epoch 567
Epoch 567 :: Batch 0/1
Batch Loss = 0.022261907013128558
567, epoch_train_loss=0.022261907013128558
Epoch 568
Epoch 568 :: Batch 0/1
Batch Loss = 0.022237576653003105
568, epoch_train_loss=0.022237576653003105
Epoch 569
Epoch 569 :: Batch 0/1
Batch Loss = 0.022213392326693376
569, epoch_train_loss=0.022213392326693376
Epoch 570
Epoch 570 :: Batch 0/1
Batch Loss = 0.022189352737762975
570, epoch_train_loss=0.022189352737762975
Epoch 571
Epoch 571 :: Batch 0/1
Batch Loss = 0.022165456622863698
571, epoch_train_loss=0.022165456622863698
Epoch 572
Epoch 572 :: Batch 0/1
Batch Loss = 0.022141702707360083
572, epoch_train_loss=0.022141702707360083
Epoch 573
Epoch 573 :: Batch 0/1
Batch Loss = 0.022118089715575166
573, epoch_train_loss=0.022118089715575166
Epoch 574
Epoch 574 :: Batch 0/1
Batch Loss = 0.022094616446559245
574, epoch_train_loss=0.022094616446559245
Epoch 575
Epoch 575 :: Batch 0/1
Batch Loss = 0.02207128178303844
575, epoch_train_loss=0.02207128178303844
Epoch 576
Epoch 576 :: Batch 0/1
Batch Loss = 0.02204808462084849
576, epoch_train_loss=0.02204808462084849
Epoch 577
Epoch 577 :: Batch 0/1
Batch Loss = 0.022025023835238612
577, epoch_train_loss=0.022025023835238612
Epoch 578
Epoch 578 :: Batch 0/1
Batch Loss = 0.022002098317894472
578, epoch_train_loss=0.022002098317894472
Epoch 579
Epoch 579 :: Batch 0/1
Batch Loss = 0.021979307012455852
579, epoch_train_loss=0.021979307012455852
Epoch 580
Epoch 580 :: Batch 0/1
Batch Loss = 0.021956648883722795
580, epoch_train_loss=0.021956648883722795
Epoch 581
Epoch 581 :: Batch 0/1
Batch Loss = 0.02193412292956482
581, epoch_train_loss=0.02193412292956482
Epoch 582
Epoch 582 :: Batch 0/1
Batch Loss = 0.021911728138331008
582, epoch_train_loss=0.021911728138331008
Epoch 583
Epoch 583 :: Batch 0/1
Batch Loss = 0.02188946350624689
583, epoch_train_loss=0.02188946350624689
Epoch 584
Epoch 584 :: Batch 0/1
Batch Loss = 0.021867327994259812
584, epoch_train_loss=0.021867327994259812
Epoch 585
Epoch 585 :: Batch 0/1
Batch Loss = 0.021845320638490113
585, epoch_train_loss=0.021845320638490113
Epoch 586
Epoch 586 :: Batch 0/1
Batch Loss = 0.021823440459208247
586, epoch_train_loss=0.021823440459208247
Epoch 587
Epoch 587 :: Batch 0/1
Batch Loss = 0.021801686598371214
587, epoch_train_loss=0.021801686598371214
Epoch 588
Epoch 588 :: Batch 0/1
Batch Loss = 0.02178005816012385
588, epoch_train_loss=0.02178005816012385
Epoch 589
Epoch 589 :: Batch 0/1
Batch Loss = 0.021758554656061718
589, epoch_train_loss=0.021758554656061718
Epoch 590
Epoch 590 :: Batch 0/1
Batch Loss = 0.021737175893506475
590, epoch_train_loss=0.021737175893506475
Epoch 591
Epoch 591 :: Batch 0/1
Batch Loss = 0.021715923384330538
591, epoch_train_loss=0.021715923384330538
Epoch 592
Epoch 592 :: Batch 0/1
Batch Loss = 0.02169480120001382
592, epoch_train_loss=0.02169480120001382
Epoch 593
Epoch 593 :: Batch 0/1
Batch Loss = 0.021673821811279786
593, epoch_train_loss=0.021673821811279786
Epoch 594
Epoch 594 :: Batch 0/1
Batch Loss = 0.021653014571490322
594, epoch_train_loss=0.021653014571490322
Epoch 595
Epoch 595 :: Batch 0/1
Batch Loss = 0.021632455693288782
595, epoch_train_loss=0.021632455693288782
Epoch 596
Epoch 596 :: Batch 0/1
Batch Loss = 0.02161232833811672
596, epoch_train_loss=0.02161232833811672
Epoch 597
Epoch 597 :: Batch 0/1
Batch Loss = 0.021593099930553864
597, epoch_train_loss=0.021593099930553864
Epoch 598
Epoch 598 :: Batch 0/1
Batch Loss = 0.02157592143322989
598, epoch_train_loss=0.02157592143322989
Epoch 599
Epoch 599 :: Batch 0/1
Batch Loss = 0.02156378482021813
599, epoch_train_loss=0.02156378482021813
Epoch 600
Epoch 600 :: Batch 0/1
Batch Loss = 0.021564145536995965
600, epoch_train_loss=0.021564145536995965
Epoch 601
Epoch 601 :: Batch 0/1
Batch Loss = 0.021597169744407434
601, epoch_train_loss=0.021597169744407434
Epoch 602
Epoch 602 :: Batch 0/1
Batch Loss = 0.021712514023380983
602, epoch_train_loss=0.021712514023380983
Epoch 603
Epoch 603 :: Batch 0/1
Batch Loss = 0.022054819140280017
603, epoch_train_loss=0.022054819140280017
Epoch 604
Epoch 604 :: Batch 0/1
Batch Loss = 0.022957954627140124
604, epoch_train_loss=0.022957954627140124
Epoch 605
Epoch 605 :: Batch 0/1
Batch Loss = 0.02555272339468041
605, epoch_train_loss=0.02555272339468041
Epoch 606
Epoch 606 :: Batch 0/1
Batch Loss = 0.032010247828599604
606, epoch_train_loss=0.032010247828599604
Epoch 607
Epoch 607 :: Batch 0/1
Batch Loss = 0.051990517659621455
607, epoch_train_loss=0.051990517659621455
Epoch 608
Epoch 608 :: Batch 0/1
Batch Loss = 0.0968149973947656
608, epoch_train_loss=0.0968149973947656
Epoch 609
Epoch 609 :: Batch 0/1
Batch Loss = 0.22405471883295425
609, epoch_train_loss=0.22405471883295425
Epoch 610
Epoch 610 :: Batch 0/1
Batch Loss = 0.37173062555208586
610, epoch_train_loss=0.37173062555208586
Epoch 611
Epoch 611 :: Batch 0/1
Batch Loss = 0.5347935517616675
611, epoch_train_loss=0.5347935517616675
Epoch 612
Epoch 612 :: Batch 0/1
Batch Loss = 0.189149669218756
612, epoch_train_loss=0.189149669218756
Epoch 613
Epoch 613 :: Batch 0/1
Batch Loss = 0.04304440772682109
613, epoch_train_loss=0.04304440772682109
Epoch 614
Epoch 614 :: Batch 0/1
Batch Loss = 0.2401075335074447
614, epoch_train_loss=0.2401075335074447
Epoch 615
Epoch 615 :: Batch 0/1
Batch Loss = 0.12075889966282449
615, epoch_train_loss=0.12075889966282449
Epoch 616
Epoch 616 :: Batch 0/1
Batch Loss = 0.08419349949474726
616, epoch_train_loss=0.08419349949474726
Epoch 617
Epoch 617 :: Batch 0/1
Batch Loss = 0.16608830211647801
617, epoch_train_loss=0.16608830211647801
Epoch 618
Epoch 618 :: Batch 0/1
Batch Loss = 0.048138942500324015
618, epoch_train_loss=0.048138942500324015
Epoch 619
Epoch 619 :: Batch 0/1
Batch Loss = 0.14070684503907777
619, epoch_train_loss=0.14070684503907777
Epoch 620
Epoch 620 :: Batch 0/1
Batch Loss = 0.0647922821578013
620, epoch_train_loss=0.0647922821578013
Epoch 621
Epoch 621 :: Batch 0/1
Batch Loss = 0.08379973346951386
621, epoch_train_loss=0.08379973346951386
Epoch 622
Epoch 622 :: Batch 0/1
Batch Loss = 0.09119969944454755
622, epoch_train_loss=0.09119969944454755
Epoch 623
Epoch 623 :: Batch 0/1
Batch Loss = 0.039430265363384086
623, epoch_train_loss=0.039430265363384086
Epoch 624
Epoch 624 :: Batch 0/1
Batch Loss = 0.08658093039214987
624, epoch_train_loss=0.08658093039214987
Epoch 625
Epoch 625 :: Batch 0/1
Batch Loss = 0.03444934508122992
625, epoch_train_loss=0.03444934508122992
Epoch 626
Epoch 626 :: Batch 0/1
Batch Loss = 0.06701831098272515
626, epoch_train_loss=0.06701831098272515
Epoch 627
Epoch 627 :: Batch 0/1
Batch Loss = 0.052660646235747295
627, epoch_train_loss=0.052660646235747295
Epoch 628
Epoch 628 :: Batch 0/1
Batch Loss = 0.04212078453294796
628, epoch_train_loss=0.04212078453294796
Epoch 629
Epoch 629 :: Batch 0/1
Batch Loss = 0.0640776804846293
629, epoch_train_loss=0.0640776804846293
Epoch 630
Epoch 630 :: Batch 0/1
Batch Loss = 0.03130262964309164
630, epoch_train_loss=0.03130262964309164
Epoch 631
Epoch 631 :: Batch 0/1
Batch Loss = 0.053924930048368264
631, epoch_train_loss=0.053924930048368264
Epoch 632
Epoch 632 :: Batch 0/1
Batch Loss = 0.03964618782899242
632, epoch_train_loss=0.03964618782899242
Epoch 633
Epoch 633 :: Batch 0/1
Batch Loss = 0.0358837624881884
633, epoch_train_loss=0.0358837624881884
Epoch 634
Epoch 634 :: Batch 0/1
Batch Loss = 0.04873024283165739
634, epoch_train_loss=0.04873024283165739
Epoch 635
Epoch 635 :: Batch 0/1
Batch Loss = 0.029132111252724767
635, epoch_train_loss=0.029132111252724767
Epoch 636
Epoch 636 :: Batch 0/1
Batch Loss = 0.04372430407706148
636, epoch_train_loss=0.04372430407706148
Epoch 637
Epoch 637 :: Batch 0/1
Batch Loss = 0.03657315807406138
637, epoch_train_loss=0.03657315807406138
Epoch 638
Epoch 638 :: Batch 0/1
Batch Loss = 0.030621510728995847
638, epoch_train_loss=0.030621510728995847
Epoch 639
Epoch 639 :: Batch 0/1
Batch Loss = 0.041682761596092195
639, epoch_train_loss=0.041682761596092195
Epoch 640
Epoch 640 :: Batch 0/1
Batch Loss = 0.02833035018733458
640, epoch_train_loss=0.02833035018733458
Epoch 641
Epoch 641 :: Batch 0/1
Batch Loss = 0.03439111509516492
641, epoch_train_loss=0.03439111509516492
Epoch 642
Epoch 642 :: Batch 0/1
Batch Loss = 0.03419336993750046
642, epoch_train_loss=0.03419336993750046
Epoch 643
Epoch 643 :: Batch 0/1
Batch Loss = 0.02728072500541673
643, epoch_train_loss=0.02728072500541673
Epoch 644
Epoch 644 :: Batch 0/1
Batch Loss = 0.034505473470342954
644, epoch_train_loss=0.034505473470342954
Epoch 645
Epoch 645 :: Batch 0/1
Batch Loss = 0.028438316674158203
645, epoch_train_loss=0.028438316674158203
Epoch 646
Epoch 646 :: Batch 0/1
Batch Loss = 0.029024588820915367
646, epoch_train_loss=0.029024588820915367
Epoch 647
Epoch 647 :: Batch 0/1
Batch Loss = 0.031673922740651214
647, epoch_train_loss=0.031673922740651214
Epoch 648
Epoch 648 :: Batch 0/1
Batch Loss = 0.026551466875991175
648, epoch_train_loss=0.026551466875991175
Epoch 649
Epoch 649 :: Batch 0/1
Batch Loss = 0.02978816620250012
649, epoch_train_loss=0.02978816620250012
Epoch 650
Epoch 650 :: Batch 0/1
Batch Loss = 0.028986550805924802
650, epoch_train_loss=0.028986550805924802
Epoch 651
Epoch 651 :: Batch 0/1
Batch Loss = 0.02624456091497538
651, epoch_train_loss=0.02624456091497538
Epoch 652
Epoch 652 :: Batch 0/1
Batch Loss = 0.029587141621248123
652, epoch_train_loss=0.029587141621248123
Epoch 653
Epoch 653 :: Batch 0/1
Batch Loss = 0.026584451410797863
653, epoch_train_loss=0.026584451410797863
Epoch 654
Epoch 654 :: Batch 0/1
Batch Loss = 0.02666571221998002
654, epoch_train_loss=0.02666571221998002
Epoch 655
Epoch 655 :: Batch 0/1
Batch Loss = 0.027893967762590247
655, epoch_train_loss=0.027893967762590247
Epoch 656
Epoch 656 :: Batch 0/1
Batch Loss = 0.025397200795633696
656, epoch_train_loss=0.025397200795633696
Epoch 657
Epoch 657 :: Batch 0/1
Batch Loss = 0.026474200375414446
657, epoch_train_loss=0.026474200375414446
Epoch 658
Epoch 658 :: Batch 0/1
Batch Loss = 0.026267557325763743
658, epoch_train_loss=0.026267557325763743
Epoch 659
Epoch 659 :: Batch 0/1
Batch Loss = 0.024686605101838333
659, epoch_train_loss=0.024686605101838333
Epoch 660
Epoch 660 :: Batch 0/1
Batch Loss = 0.025947746089719514
660, epoch_train_loss=0.025947746089719514
Epoch 661
Epoch 661 :: Batch 0/1
Batch Loss = 0.024753384024440443
661, epoch_train_loss=0.024753384024440443
Epoch 662
Epoch 662 :: Batch 0/1
Batch Loss = 0.024374696978812165
662, epoch_train_loss=0.024374696978812165
Epoch 663
Epoch 663 :: Batch 0/1
Batch Loss = 0.024947624710692208
663, epoch_train_loss=0.024947624710692208
Epoch 664
Epoch 664 :: Batch 0/1
Batch Loss = 0.02376568706164303
664, epoch_train_loss=0.02376568706164303
Epoch 665
Epoch 665 :: Batch 0/1
Batch Loss = 0.023917084747141177
665, epoch_train_loss=0.023917084747141177
Epoch 666
Epoch 666 :: Batch 0/1
Batch Loss = 0.024002523901367822
666, epoch_train_loss=0.024002523901367822
Epoch 667
Epoch 667 :: Batch 0/1
Batch Loss = 0.023091666751013203
667, epoch_train_loss=0.023091666751013203
Epoch 668
Epoch 668 :: Batch 0/1
Batch Loss = 0.02349558997162623
668, epoch_train_loss=0.02349558997162623
Epoch 669
Epoch 669 :: Batch 0/1
Batch Loss = 0.023234615709406974
669, epoch_train_loss=0.023234615709406974
Epoch 670
Epoch 670 :: Batch 0/1
Batch Loss = 0.022713511162491932
670, epoch_train_loss=0.022713511162491932
Epoch 671
Epoch 671 :: Batch 0/1
Batch Loss = 0.023036753367779335
671, epoch_train_loss=0.023036753367779335
Epoch 672
Epoch 672 :: Batch 0/1
Batch Loss = 0.022710006210373515
672, epoch_train_loss=0.022710006210373515
Epoch 673
Epoch 673 :: Batch 0/1
Batch Loss = 0.02235930480583796
673, epoch_train_loss=0.02235930480583796
Epoch 674
Epoch 674 :: Batch 0/1
Batch Loss = 0.022592915839857362
674, epoch_train_loss=0.022592915839857362
Epoch 675
Epoch 675 :: Batch 0/1
Batch Loss = 0.022237339443593032
675, epoch_train_loss=0.022237339443593032
Epoch 676
Epoch 676 :: Batch 0/1
Batch Loss = 0.021996706766393113
676, epoch_train_loss=0.021996706766393113
Epoch 677
Epoch 677 :: Batch 0/1
Batch Loss = 0.022138430621368128
677, epoch_train_loss=0.022138430621368128
Epoch 678
Epoch 678 :: Batch 0/1
Batch Loss = 0.02189680307539482
678, epoch_train_loss=0.02189680307539482
Epoch 679
Epoch 679 :: Batch 0/1
Batch Loss = 0.021691489302632018
679, epoch_train_loss=0.021691489302632018
Epoch 680
Epoch 680 :: Batch 0/1
Batch Loss = 0.021827317520060976
680, epoch_train_loss=0.021827317520060976
Epoch 681
Epoch 681 :: Batch 0/1
Batch Loss = 0.021684566427470218
681, epoch_train_loss=0.021684566427470218
Epoch 682
Epoch 682 :: Batch 0/1
Batch Loss = 0.02150016591328555
682, epoch_train_loss=0.02150016591328555
Epoch 683
Epoch 683 :: Batch 0/1
Batch Loss = 0.021569841689855714
683, epoch_train_loss=0.021569841689855714
Epoch 684
Epoch 684 :: Batch 0/1
Batch Loss = 0.021528197154596633
684, epoch_train_loss=0.021528197154596633
Epoch 685
Epoch 685 :: Batch 0/1
Batch Loss = 0.021344277984814978
685, epoch_train_loss=0.021344277984814978
Epoch 686
Epoch 686 :: Batch 0/1
Batch Loss = 0.02135991057263839
686, epoch_train_loss=0.02135991057263839
Epoch 687
Epoch 687 :: Batch 0/1
Batch Loss = 0.021369595635974646
687, epoch_train_loss=0.021369595635974646
Epoch 688
Epoch 688 :: Batch 0/1
Batch Loss = 0.021239317936000864
688, epoch_train_loss=0.021239317936000864
Epoch 689
Epoch 689 :: Batch 0/1
Batch Loss = 0.02118745972539026
689, epoch_train_loss=0.02118745972539026
Epoch 690
Epoch 690 :: Batch 0/1
Batch Loss = 0.0212226725805743
690, epoch_train_loss=0.0212226725805743
Epoch 691
Epoch 691 :: Batch 0/1
Batch Loss = 0.021145773766188668
691, epoch_train_loss=0.021145773766188668
Epoch 692
Epoch 692 :: Batch 0/1
Batch Loss = 0.021068601315731628
692, epoch_train_loss=0.021068601315731628
Epoch 693
Epoch 693 :: Batch 0/1
Batch Loss = 0.021079308667247616
693, epoch_train_loss=0.021079308667247616
Epoch 694
Epoch 694 :: Batch 0/1
Batch Loss = 0.021061663307058588
694, epoch_train_loss=0.021061663307058588
Epoch 695
Epoch 695 :: Batch 0/1
Batch Loss = 0.02098169609001201
695, epoch_train_loss=0.02098169609001201
Epoch 696
Epoch 696 :: Batch 0/1
Batch Loss = 0.020958615148529158
696, epoch_train_loss=0.020958615148529158
Epoch 697
Epoch 697 :: Batch 0/1
Batch Loss = 0.020957535378353983
697, epoch_train_loss=0.020957535378353983
Epoch 698
Epoch 698 :: Batch 0/1
Batch Loss = 0.020906408372446107
698, epoch_train_loss=0.020906408372446107
Epoch 699
Epoch 699 :: Batch 0/1
Batch Loss = 0.020851278615785063
699, epoch_train_loss=0.020851278615785063
Epoch 700
Epoch 700 :: Batch 0/1
Batch Loss = 0.020841703564910843
700, epoch_train_loss=0.020841703564910843
Epoch 701
Epoch 701 :: Batch 0/1
Batch Loss = 0.02081695987001212
701, epoch_train_loss=0.02081695987001212
Epoch 702
Epoch 702 :: Batch 0/1
Batch Loss = 0.020763587095859647
702, epoch_train_loss=0.020763587095859647
Epoch 703
Epoch 703 :: Batch 0/1
Batch Loss = 0.020730610275609036
703, epoch_train_loss=0.020730610275609036
Epoch 704
Epoch 704 :: Batch 0/1
Batch Loss = 0.0207178648795452
704, epoch_train_loss=0.0207178648795452
Epoch 705
Epoch 705 :: Batch 0/1
Batch Loss = 0.020683690924535637
705, epoch_train_loss=0.020683690924535637
Epoch 706
Epoch 706 :: Batch 0/1
Batch Loss = 0.020641000898477816
706, epoch_train_loss=0.020641000898477816
Epoch 707
Epoch 707 :: Batch 0/1
Batch Loss = 0.02061864127023871
707, epoch_train_loss=0.02061864127023871
Epoch 708
Epoch 708 :: Batch 0/1
Batch Loss = 0.020599617042461823
708, epoch_train_loss=0.020599617042461823
Epoch 709
Epoch 709 :: Batch 0/1
Batch Loss = 0.02056438332035436
709, epoch_train_loss=0.02056438332035436
Epoch 710
Epoch 710 :: Batch 0/1
Batch Loss = 0.020529683945534977
710, epoch_train_loss=0.020529683945534977
Epoch 711
Epoch 711 :: Batch 0/1
Batch Loss = 0.020509189317317204
711, epoch_train_loss=0.020509189317317204
Epoch 712
Epoch 712 :: Batch 0/1
Batch Loss = 0.020486105450481164
712, epoch_train_loss=0.020486105450481164
Epoch 713
Epoch 713 :: Batch 0/1
Batch Loss = 0.02045354981382387
713, epoch_train_loss=0.02045354981382387
Epoch 714
Epoch 714 :: Batch 0/1
Batch Loss = 0.020424417735138332
714, epoch_train_loss=0.020424417735138332
Epoch 715
Epoch 715 :: Batch 0/1
Batch Loss = 0.020404181707839177
715, epoch_train_loss=0.020404181707839177
Epoch 716
Epoch 716 :: Batch 0/1
Batch Loss = 0.02038016073918583
716, epoch_train_loss=0.02038016073918583
Epoch 717
Epoch 717 :: Batch 0/1
Batch Loss = 0.02035077830172387
717, epoch_train_loss=0.02035077830172387
Epoch 718
Epoch 718 :: Batch 0/1
Batch Loss = 0.020324722019194705
718, epoch_train_loss=0.020324722019194705
Epoch 719
Epoch 719 :: Batch 0/1
Batch Loss = 0.020304119799542882
719, epoch_train_loss=0.020304119799542882
Epoch 720
Epoch 720 :: Batch 0/1
Batch Loss = 0.020280496532150912
720, epoch_train_loss=0.020280496532150912
Epoch 721
Epoch 721 :: Batch 0/1
Batch Loss = 0.02025363171467588
721, epoch_train_loss=0.02025363171467588
Epoch 722
Epoch 722 :: Batch 0/1
Batch Loss = 0.020229113636691547
722, epoch_train_loss=0.020229113636691547
Epoch 723
Epoch 723 :: Batch 0/1
Batch Loss = 0.020207925435595717
723, epoch_train_loss=0.020207925435595717
Epoch 724
Epoch 724 :: Batch 0/1
Batch Loss = 0.020184799798012822
724, epoch_train_loss=0.020184799798012822
Epoch 725
Epoch 725 :: Batch 0/1
Batch Loss = 0.020159457667183533
725, epoch_train_loss=0.020159457667183533
Epoch 726
Epoch 726 :: Batch 0/1
Batch Loss = 0.0201356398657238
726, epoch_train_loss=0.0201356398657238
Epoch 727
Epoch 727 :: Batch 0/1
Batch Loss = 0.020114195562319248
727, epoch_train_loss=0.020114195562319248
Epoch 728
Epoch 728 :: Batch 0/1
Batch Loss = 0.02009202835202173
728, epoch_train_loss=0.02009202835202173
Epoch 729
Epoch 729 :: Batch 0/1
Batch Loss = 0.020068221329945863
729, epoch_train_loss=0.020068221329945863
Epoch 730
Epoch 730 :: Batch 0/1
Batch Loss = 0.020045101221324836
730, epoch_train_loss=0.020045101221324836
Epoch 731
Epoch 731 :: Batch 0/1
Batch Loss = 0.020023607681627417
731, epoch_train_loss=0.020023607681627417
Epoch 732
Epoch 732 :: Batch 0/1
Batch Loss = 0.020002199737912348
732, epoch_train_loss=0.020002199737912348
Epoch 733
Epoch 733 :: Batch 0/1
Batch Loss = 0.01997962637216662
733, epoch_train_loss=0.01997962637216662
Epoch 734
Epoch 734 :: Batch 0/1
Batch Loss = 0.019957075795405975
734, epoch_train_loss=0.019957075795405975
Epoch 735
Epoch 735 :: Batch 0/1
Batch Loss = 0.01993554854089283
735, epoch_train_loss=0.01993554854089283
Epoch 736
Epoch 736 :: Batch 0/1
Batch Loss = 0.01991462552439573
736, epoch_train_loss=0.01991462552439573
Epoch 737
Epoch 737 :: Batch 0/1
Batch Loss = 0.019893130869059156
737, epoch_train_loss=0.019893130869059156
Epoch 738
Epoch 738 :: Batch 0/1
Batch Loss = 0.01987130164120407
738, epoch_train_loss=0.01987130164120407
Epoch 739
Epoch 739 :: Batch 0/1
Batch Loss = 0.019849913530762892
739, epoch_train_loss=0.019849913530762892
Epoch 740
Epoch 740 :: Batch 0/1
Batch Loss = 0.019829249360735976
740, epoch_train_loss=0.019829249360735976
Epoch 741
Epoch 741 :: Batch 0/1
Batch Loss = 0.01980865049199745
741, epoch_train_loss=0.01980865049199745
Epoch 742
Epoch 742 :: Batch 0/1
Batch Loss = 0.019787760161687325
742, epoch_train_loss=0.019787760161687325
Epoch 743
Epoch 743 :: Batch 0/1
Batch Loss = 0.019766884645932438
743, epoch_train_loss=0.019766884645932438
Epoch 744
Epoch 744 :: Batch 0/1
Batch Loss = 0.01974647669751973
744, epoch_train_loss=0.01974647669751973
Epoch 745
Epoch 745 :: Batch 0/1
Batch Loss = 0.019726496891071487
745, epoch_train_loss=0.019726496891071487
Epoch 746
Epoch 746 :: Batch 0/1
Batch Loss = 0.01970655984085333
746, epoch_train_loss=0.01970655984085333
Epoch 747
Epoch 747 :: Batch 0/1
Batch Loss = 0.019686551086650954
747, epoch_train_loss=0.019686551086650954
Epoch 748
Epoch 748 :: Batch 0/1
Batch Loss = 0.019666668851328385
748, epoch_train_loss=0.019666668851328385
Epoch 749
Epoch 749 :: Batch 0/1
Batch Loss = 0.01964717789638952
749, epoch_train_loss=0.01964717789638952
Epoch 750
Epoch 750 :: Batch 0/1
Batch Loss = 0.019628006649790964
750, epoch_train_loss=0.019628006649790964
Epoch 751
Epoch 751 :: Batch 0/1
Batch Loss = 0.019608985232814586
751, epoch_train_loss=0.019608985232814586
Epoch 752
Epoch 752 :: Batch 0/1
Batch Loss = 0.019590014956374507
752, epoch_train_loss=0.019590014956374507
Epoch 753
Epoch 753 :: Batch 0/1
Batch Loss = 0.01957123511898669
753, epoch_train_loss=0.01957123511898669
Epoch 754
Epoch 754 :: Batch 0/1
Batch Loss = 0.019552770101033703
754, epoch_train_loss=0.019552770101033703
Epoch 755
Epoch 755 :: Batch 0/1
Batch Loss = 0.019534626209422357
755, epoch_train_loss=0.019534626209422357
Epoch 756
Epoch 756 :: Batch 0/1
Batch Loss = 0.01951668961204044
756, epoch_train_loss=0.01951668961204044
Epoch 757
Epoch 757 :: Batch 0/1
Batch Loss = 0.019498893734130447
757, epoch_train_loss=0.019498893734130447
Epoch 758
Epoch 758 :: Batch 0/1
Batch Loss = 0.01948128331744379
758, epoch_train_loss=0.01948128331744379
Epoch 759
Epoch 759 :: Batch 0/1
Batch Loss = 0.019463936029681113
759, epoch_train_loss=0.019463936029681113
Epoch 760
Epoch 760 :: Batch 0/1
Batch Loss = 0.019446883296593876
760, epoch_train_loss=0.019446883296593876
Epoch 761
Epoch 761 :: Batch 0/1
Batch Loss = 0.01943006442058227
761, epoch_train_loss=0.01943006442058227
Epoch 762
Epoch 762 :: Batch 0/1
Batch Loss = 0.019413431736694575
762, epoch_train_loss=0.019413431736694575
Epoch 763
Epoch 763 :: Batch 0/1
Batch Loss = 0.01939697148293487
763, epoch_train_loss=0.01939697148293487
Epoch 764
Epoch 764 :: Batch 0/1
Batch Loss = 0.01938072982988531
764, epoch_train_loss=0.01938072982988531
Epoch 765
Epoch 765 :: Batch 0/1
Batch Loss = 0.019364727397087833
765, epoch_train_loss=0.019364727397087833
Epoch 766
Epoch 766 :: Batch 0/1
Batch Loss = 0.01934894963554019
766, epoch_train_loss=0.01934894963554019
Epoch 767
Epoch 767 :: Batch 0/1
Batch Loss = 0.019333354454928814
767, epoch_train_loss=0.019333354454928814
Epoch 768
Epoch 768 :: Batch 0/1
Batch Loss = 0.01931791556783976
768, epoch_train_loss=0.01931791556783976
Epoch 769
Epoch 769 :: Batch 0/1
Batch Loss = 0.0193026409434724
769, epoch_train_loss=0.0193026409434724
Epoch 770
Epoch 770 :: Batch 0/1
Batch Loss = 0.019287543057723372
770, epoch_train_loss=0.019287543057723372
Epoch 771
Epoch 771 :: Batch 0/1
Batch Loss = 0.01927262386582332
771, epoch_train_loss=0.01927262386582332
Epoch 772
Epoch 772 :: Batch 0/1
Batch Loss = 0.019257858232220686
772, epoch_train_loss=0.019257858232220686
Epoch 773
Epoch 773 :: Batch 0/1
Batch Loss = 0.019243223796723243
773, epoch_train_loss=0.019243223796723243
Epoch 774
Epoch 774 :: Batch 0/1
Batch Loss = 0.019228708444382892
774, epoch_train_loss=0.019228708444382892
Epoch 775
Epoch 775 :: Batch 0/1
Batch Loss = 0.019214317418516997
775, epoch_train_loss=0.019214317418516997
Epoch 776
Epoch 776 :: Batch 0/1
Batch Loss = 0.01920005384609689
776, epoch_train_loss=0.01920005384609689
Epoch 777
Epoch 777 :: Batch 0/1
Batch Loss = 0.01918591091121302
777, epoch_train_loss=0.01918591091121302
Epoch 778
Epoch 778 :: Batch 0/1
Batch Loss = 0.01917187351510565
778, epoch_train_loss=0.01917187351510565
Epoch 779
Epoch 779 :: Batch 0/1
Batch Loss = 0.019157927561999043
779, epoch_train_loss=0.019157927561999043
Epoch 780
Epoch 780 :: Batch 0/1
Batch Loss = 0.019144069501315933
780, epoch_train_loss=0.019144069501315933
Epoch 781
Epoch 781 :: Batch 0/1
Batch Loss = 0.019130299619903993
781, epoch_train_loss=0.019130299619903993
Epoch 782
Epoch 782 :: Batch 0/1
Batch Loss = 0.019116619047527313
782, epoch_train_loss=0.019116619047527313
Epoch 783
Epoch 783 :: Batch 0/1
Batch Loss = 0.019103021426824907
783, epoch_train_loss=0.019103021426824907
Epoch 784
Epoch 784 :: Batch 0/1
Batch Loss = 0.019089498740486505
784, epoch_train_loss=0.019089498740486505
Epoch 785
Epoch 785 :: Batch 0/1
Batch Loss = 0.019076044012056204
785, epoch_train_loss=0.019076044012056204
Epoch 786
Epoch 786 :: Batch 0/1
Batch Loss = 0.019062654968127323
786, epoch_train_loss=0.019062654968127323
Epoch 787
Epoch 787 :: Batch 0/1
Batch Loss = 0.019049332457834182
787, epoch_train_loss=0.019049332457834182
Epoch 788
Epoch 788 :: Batch 0/1
Batch Loss = 0.019036075830552893
788, epoch_train_loss=0.019036075830552893
Epoch 789
Epoch 789 :: Batch 0/1
Batch Loss = 0.019022882732581042
789, epoch_train_loss=0.019022882732581042
Epoch 790
Epoch 790 :: Batch 0/1
Batch Loss = 0.019009748321759043
790, epoch_train_loss=0.019009748321759043
Epoch 791
Epoch 791 :: Batch 0/1
Batch Loss = 0.01899666931532536
791, epoch_train_loss=0.01899666931532536
Epoch 792
Epoch 792 :: Batch 0/1
Batch Loss = 0.018983643983113554
792, epoch_train_loss=0.018983643983113554
Epoch 793
Epoch 793 :: Batch 0/1
Batch Loss = 0.018970672379307517
793, epoch_train_loss=0.018970672379307517
Epoch 794
Epoch 794 :: Batch 0/1
Batch Loss = 0.018957754502555175
794, epoch_train_loss=0.018957754502555175
Epoch 795
Epoch 795 :: Batch 0/1
Batch Loss = 0.018944889097658114
795, epoch_train_loss=0.018944889097658114
Epoch 796
Epoch 796 :: Batch 0/1
Batch Loss = 0.018932074232186617
796, epoch_train_loss=0.018932074232186617
Epoch 797
Epoch 797 :: Batch 0/1
Batch Loss = 0.01891930764594705
797, epoch_train_loss=0.01891930764594705
Epoch 798
Epoch 798 :: Batch 0/1
Batch Loss = 0.01890658817265488
798, epoch_train_loss=0.01890658817265488
Epoch 799
Epoch 799 :: Batch 0/1
Batch Loss = 0.018893915286174436
799, epoch_train_loss=0.018893915286174436
Epoch 800
Epoch 800 :: Batch 0/1
Batch Loss = 0.018881288922881018
800, epoch_train_loss=0.018881288922881018
Epoch 801
Epoch 801 :: Batch 0/1
Batch Loss = 0.01886870869886147
801, epoch_train_loss=0.01886870869886147
Epoch 802
Epoch 802 :: Batch 0/1
Batch Loss = 0.018856173696270725
802, epoch_train_loss=0.018856173696270725
Epoch 803
Epoch 803 :: Batch 0/1
Batch Loss = 0.018843682769566834
803, epoch_train_loss=0.018843682769566834
Epoch 804
Epoch 804 :: Batch 0/1
Batch Loss = 0.018831234771292588
804, epoch_train_loss=0.018831234771292588
Epoch 805
Epoch 805 :: Batch 0/1
Batch Loss = 0.018818829007397887
805, epoch_train_loss=0.018818829007397887
Epoch 806
Epoch 806 :: Batch 0/1
Batch Loss = 0.01880646506549721
806, epoch_train_loss=0.01880646506549721
Epoch 807
Epoch 807 :: Batch 0/1
Batch Loss = 0.018794142693471787
807, epoch_train_loss=0.018794142693471787
Epoch 808
Epoch 808 :: Batch 0/1
Batch Loss = 0.01878186155404896
808, epoch_train_loss=0.01878186155404896
Epoch 809
Epoch 809 :: Batch 0/1
Batch Loss = 0.01876962109487425
809, epoch_train_loss=0.01876962109487425
Epoch 810
Epoch 810 :: Batch 0/1
Batch Loss = 0.018757420669959272
810, epoch_train_loss=0.018757420669959272
Epoch 811
Epoch 811 :: Batch 0/1
Batch Loss = 0.018745259589860983
811, epoch_train_loss=0.018745259589860983
Epoch 812
Epoch 812 :: Batch 0/1
Batch Loss = 0.01873313730792455
812, epoch_train_loss=0.01873313730792455
Epoch 813
Epoch 813 :: Batch 0/1
Batch Loss = 0.01872105341746649
813, epoch_train_loss=0.01872105341746649
Epoch 814
Epoch 814 :: Batch 0/1
Batch Loss = 0.018709007573630042
814, epoch_train_loss=0.018709007573630042
Epoch 815
Epoch 815 :: Batch 0/1
Batch Loss = 0.018696999499070555
815, epoch_train_loss=0.018696999499070555
Epoch 816
Epoch 816 :: Batch 0/1
Batch Loss = 0.01868502881540125
816, epoch_train_loss=0.01868502881540125
Epoch 817
Epoch 817 :: Batch 0/1
Batch Loss = 0.0186730951375468
817, epoch_train_loss=0.0186730951375468
Epoch 818
Epoch 818 :: Batch 0/1
Batch Loss = 0.018661198003900113
818, epoch_train_loss=0.018661198003900113
Epoch 819
Epoch 819 :: Batch 0/1
Batch Loss = 0.018649336977761995
819, epoch_train_loss=0.018649336977761995
Epoch 820
Epoch 820 :: Batch 0/1
Batch Loss = 0.018637511667492914
820, epoch_train_loss=0.018637511667492914
Epoch 821
Epoch 821 :: Batch 0/1
Batch Loss = 0.01862572168123623
821, epoch_train_loss=0.01862572168123623
Epoch 822
Epoch 822 :: Batch 0/1
Batch Loss = 0.018613966726690096
822, epoch_train_loss=0.018613966726690096
Epoch 823
Epoch 823 :: Batch 0/1
Batch Loss = 0.018602246452648873
823, epoch_train_loss=0.018602246452648873
Epoch 824
Epoch 824 :: Batch 0/1
Batch Loss = 0.018590560579951895
824, epoch_train_loss=0.018590560579951895
Epoch 825
Epoch 825 :: Batch 0/1
Batch Loss = 0.018578908755852476
825, epoch_train_loss=0.018578908755852476
Epoch 826
Epoch 826 :: Batch 0/1
Batch Loss = 0.01856729065645318
826, epoch_train_loss=0.01856729065645318
Epoch 827
Epoch 827 :: Batch 0/1
Batch Loss = 0.018555705938130457
827, epoch_train_loss=0.018555705938130457
Epoch 828
Epoch 828 :: Batch 0/1
Batch Loss = 0.018544154242729306
828, epoch_train_loss=0.018544154242729306
Epoch 829
Epoch 829 :: Batch 0/1
Batch Loss = 0.018532635254790087
829, epoch_train_loss=0.018532635254790087
Epoch 830
Epoch 830 :: Batch 0/1
Batch Loss = 0.01852114862083339
830, epoch_train_loss=0.01852114862083339
Epoch 831
Epoch 831 :: Batch 0/1
Batch Loss = 0.018509694053092596
831, epoch_train_loss=0.018509694053092596
Epoch 832
Epoch 832 :: Batch 0/1
Batch Loss = 0.01849827121862198
832, epoch_train_loss=0.01849827121862198
Epoch 833
Epoch 833 :: Batch 0/1
Batch Loss = 0.01848687984004544
833, epoch_train_loss=0.01848687984004544
Epoch 834
Epoch 834 :: Batch 0/1
Batch Loss = 0.018475519600984546
834, epoch_train_loss=0.018475519600984546
Epoch 835
Epoch 835 :: Batch 0/1
Batch Loss = 0.018464190217544314
835, epoch_train_loss=0.018464190217544314
Epoch 836
Epoch 836 :: Batch 0/1
Batch Loss = 0.018452891385565592
836, epoch_train_loss=0.018452891385565592
Epoch 837
Epoch 837 :: Batch 0/1
Batch Loss = 0.018441622817319314
837, epoch_train_loss=0.018441622817319314
Epoch 838
Epoch 838 :: Batch 0/1
Batch Loss = 0.018430384216699007
838, epoch_train_loss=0.018430384216699007
Epoch 839
Epoch 839 :: Batch 0/1
Batch Loss = 0.018419175296035734
839, epoch_train_loss=0.018419175296035734
Epoch 840
Epoch 840 :: Batch 0/1
Batch Loss = 0.01840799577036399
840, epoch_train_loss=0.01840799577036399
Epoch 841
Epoch 841 :: Batch 0/1
Batch Loss = 0.01839684535385443
841, epoch_train_loss=0.01839684535385443
Epoch 842
Epoch 842 :: Batch 0/1
Batch Loss = 0.018385723773445245
842, epoch_train_loss=0.018385723773445245
Epoch 843
Epoch 843 :: Batch 0/1
Batch Loss = 0.018374630745551696
843, epoch_train_loss=0.018374630745551696
Epoch 844
Epoch 844 :: Batch 0/1
Batch Loss = 0.018363566012448405
844, epoch_train_loss=0.018363566012448405
Epoch 845
Epoch 845 :: Batch 0/1
Batch Loss = 0.01835252929146292
845, epoch_train_loss=0.01835252929146292
Epoch 846
Epoch 846 :: Batch 0/1
Batch Loss = 0.0183415203353323
846, epoch_train_loss=0.0183415203353323
Epoch 847
Epoch 847 :: Batch 0/1
Batch Loss = 0.01833053886609826
847, epoch_train_loss=0.01833053886609826
Epoch 848
Epoch 848 :: Batch 0/1
Batch Loss = 0.018319584642903904
848, epoch_train_loss=0.018319584642903904
Epoch 849
Epoch 849 :: Batch 0/1
Batch Loss = 0.01830865739246894
849, epoch_train_loss=0.01830865739246894
Epoch 850
Epoch 850 :: Batch 0/1
Batch Loss = 0.018297756881453884
850, epoch_train_loss=0.018297756881453884
Epoch 851
Epoch 851 :: Batch 0/1
Batch Loss = 0.018286882841842877
851, epoch_train_loss=0.018286882841842877
Epoch 852
Epoch 852 :: Batch 0/1
Batch Loss = 0.018276035047095747
852, epoch_train_loss=0.018276035047095747
Epoch 853
Epoch 853 :: Batch 0/1
Batch Loss = 0.01826521323393828
853, epoch_train_loss=0.01826521323393828
Epoch 854
Epoch 854 :: Batch 0/1
Batch Loss = 0.018254417184062025
854, epoch_train_loss=0.018254417184062025
Epoch 855
Epoch 855 :: Batch 0/1
Batch Loss = 0.01824364663688657
855, epoch_train_loss=0.01824364663688657
Epoch 856
Epoch 856 :: Batch 0/1
Batch Loss = 0.018232901387075933
856, epoch_train_loss=0.018232901387075933
Epoch 857
Epoch 857 :: Batch 0/1
Batch Loss = 0.0182221811770937
857, epoch_train_loss=0.0182221811770937
Epoch 858
Epoch 858 :: Batch 0/1
Batch Loss = 0.018211485820977424
858, epoch_train_loss=0.018211485820977424
Epoch 859
Epoch 859 :: Batch 0/1
Batch Loss = 0.018200815065174913
859, epoch_train_loss=0.018200815065174913
Epoch 860
Epoch 860 :: Batch 0/1
Batch Loss = 0.018190168758637066
860, epoch_train_loss=0.018190168758637066
Epoch 861
Epoch 861 :: Batch 0/1
Batch Loss = 0.01817954666501472
861, epoch_train_loss=0.01817954666501472
Epoch 862
Epoch 862 :: Batch 0/1
Batch Loss = 0.01816894870342287
862, epoch_train_loss=0.01816894870342287
Epoch 863
Epoch 863 :: Batch 0/1
Batch Loss = 0.01815837469657784
863, epoch_train_loss=0.01815837469657784
Epoch 864
Epoch 864 :: Batch 0/1
Batch Loss = 0.01814782473358782
864, epoch_train_loss=0.01814782473358782
Epoch 865
Epoch 865 :: Batch 0/1
Batch Loss = 0.018137298827116816
865, epoch_train_loss=0.018137298827116816
Epoch 866
Epoch 866 :: Batch 0/1
Batch Loss = 0.01812679752301134
866, epoch_train_loss=0.01812679752301134
Epoch 867
Epoch 867 :: Batch 0/1
Batch Loss = 0.018116321461545615
867, epoch_train_loss=0.018116321461545615
Epoch 868
Epoch 868 :: Batch 0/1
Batch Loss = 0.018105872526768485
868, epoch_train_loss=0.018105872526768485
Epoch 869
Epoch 869 :: Batch 0/1
Batch Loss = 0.01809545345316818
869, epoch_train_loss=0.01809545345316818
Epoch 870
Epoch 870 :: Batch 0/1
Batch Loss = 0.01808507033969387
870, epoch_train_loss=0.01808507033969387
Epoch 871
Epoch 871 :: Batch 0/1
Batch Loss = 0.018074733059111732
871, epoch_train_loss=0.018074733059111732
Epoch 872
Epoch 872 :: Batch 0/1
Batch Loss = 0.01806446183455874
872, epoch_train_loss=0.01806446183455874
Epoch 873
Epoch 873 :: Batch 0/1
Batch Loss = 0.01805429164797883
873, epoch_train_loss=0.01805429164797883
Epoch 874
Epoch 874 :: Batch 0/1
Batch Loss = 0.01804429267138816
874, epoch_train_loss=0.01804429267138816
Epoch 875
Epoch 875 :: Batch 0/1
Batch Loss = 0.018034591260981062
875, epoch_train_loss=0.018034591260981062
Epoch 876
Epoch 876 :: Batch 0/1
Batch Loss = 0.01802544341882386
876, epoch_train_loss=0.01802544341882386
Epoch 877
Epoch 877 :: Batch 0/1
Batch Loss = 0.01801731963709876
877, epoch_train_loss=0.01801731963709876
Epoch 878
Epoch 878 :: Batch 0/1
Batch Loss = 0.018011202592918996
878, epoch_train_loss=0.018011202592918996
Epoch 879
Epoch 879 :: Batch 0/1
Batch Loss = 0.018008899983550077
879, epoch_train_loss=0.018008899983550077
Epoch 880
Epoch 880 :: Batch 0/1
Batch Loss = 0.018014387574240058
880, epoch_train_loss=0.018014387574240058
Epoch 881
Epoch 881 :: Batch 0/1
Batch Loss = 0.01803478986052478
881, epoch_train_loss=0.01803478986052478
Epoch 882
Epoch 882 :: Batch 0/1
Batch Loss = 0.018087139700923214
882, epoch_train_loss=0.018087139700923214
Epoch 883
Epoch 883 :: Batch 0/1
Batch Loss = 0.018199780638095794
883, epoch_train_loss=0.018199780638095794
Epoch 884
Epoch 884 :: Batch 0/1
Batch Loss = 0.01845061403942947
884, epoch_train_loss=0.01845061403942947
Epoch 885
Epoch 885 :: Batch 0/1
Batch Loss = 0.01894889219423331
885, epoch_train_loss=0.01894889219423331
Epoch 886
Epoch 886 :: Batch 0/1
Batch Loss = 0.020077642942319552
886, epoch_train_loss=0.020077642942319552
Epoch 887
Epoch 887 :: Batch 0/1
Batch Loss = 0.022200264166280258
887, epoch_train_loss=0.022200264166280258
Epoch 888
Epoch 888 :: Batch 0/1
Batch Loss = 0.02730280508090868
888, epoch_train_loss=0.02730280508090868
Epoch 889
Epoch 889 :: Batch 0/1
Batch Loss = 0.03610230877495152
889, epoch_train_loss=0.03610230877495152
Epoch 890
Epoch 890 :: Batch 0/1
Batch Loss = 0.058151512030422604
890, epoch_train_loss=0.058151512030422604
Epoch 891
Epoch 891 :: Batch 0/1
Batch Loss = 0.0901419002300571
891, epoch_train_loss=0.0901419002300571
Epoch 892
Epoch 892 :: Batch 0/1
Batch Loss = 0.1609253265788944
892, epoch_train_loss=0.1609253265788944
Epoch 893
Epoch 893 :: Batch 0/1
Batch Loss = 0.20688096494031388
893, epoch_train_loss=0.20688096494031388
Epoch 894
Epoch 894 :: Batch 0/1
Batch Loss = 0.24988097961316083
894, epoch_train_loss=0.24988097961316083
Epoch 895
Epoch 895 :: Batch 0/1
Batch Loss = 0.13103203563792817
895, epoch_train_loss=0.13103203563792817
Epoch 896
Epoch 896 :: Batch 0/1
Batch Loss = 0.030609543579075537
896, epoch_train_loss=0.030609543579075537
Epoch 897
Epoch 897 :: Batch 0/1
Batch Loss = 0.05720556319758593
897, epoch_train_loss=0.05720556319758593
Epoch 898
Epoch 898 :: Batch 0/1
Batch Loss = 0.11668716867813576
898, epoch_train_loss=0.11668716867813576
Epoch 899
Epoch 899 :: Batch 0/1
Batch Loss = 0.0725114313796015
899, epoch_train_loss=0.0725114313796015
Epoch 900
Epoch 900 :: Batch 0/1
Batch Loss = 0.025168482107793197
900, epoch_train_loss=0.025168482107793197
Epoch 901
Epoch 901 :: Batch 0/1
Batch Loss = 0.06691227318817254
901, epoch_train_loss=0.06691227318817254
Epoch 902
Epoch 902 :: Batch 0/1
Batch Loss = 0.08883781564507125
902, epoch_train_loss=0.08883781564507125
Epoch 903
Epoch 903 :: Batch 0/1
Batch Loss = 0.03305000295916914
903, epoch_train_loss=0.03305000295916914
Epoch 904
Epoch 904 :: Batch 0/1
Batch Loss = 0.029173691141542127
904, epoch_train_loss=0.029173691141542127
Epoch 905
Epoch 905 :: Batch 0/1
Batch Loss = 0.06578510796455132
905, epoch_train_loss=0.06578510796455132
Epoch 906
Epoch 906 :: Batch 0/1
Batch Loss = 0.04694820128701145
906, epoch_train_loss=0.04694820128701145
Epoch 907
Epoch 907 :: Batch 0/1
Batch Loss = 0.020987551389242454
907, epoch_train_loss=0.020987551389242454
Epoch 908
Epoch 908 :: Batch 0/1
Batch Loss = 0.033020072943314355
908, epoch_train_loss=0.033020072943314355
Epoch 909
Epoch 909 :: Batch 0/1
Batch Loss = 0.04361643077829516
909, epoch_train_loss=0.04361643077829516
Epoch 910
Epoch 910 :: Batch 0/1
Batch Loss = 0.02826268471410996
910, epoch_train_loss=0.02826268471410996
Epoch 911
Epoch 911 :: Batch 0/1
Batch Loss = 0.022582200743018657
911, epoch_train_loss=0.022582200743018657
Epoch 912
Epoch 912 :: Batch 0/1
Batch Loss = 0.0357341056440197
912, epoch_train_loss=0.0357341056440197
Epoch 913
Epoch 913 :: Batch 0/1
Batch Loss = 0.03381485072923102
913, epoch_train_loss=0.03381485072923102
Epoch 914
Epoch 914 :: Batch 0/1
Batch Loss = 0.021043865020321156
914, epoch_train_loss=0.021043865020321156
Epoch 915
Epoch 915 :: Batch 0/1
Batch Loss = 0.028253469336664755
915, epoch_train_loss=0.028253469336664755
Epoch 916
Epoch 916 :: Batch 0/1
Batch Loss = 0.0327860816261495
916, epoch_train_loss=0.0327860816261495
Epoch 917
Epoch 917 :: Batch 0/1
Batch Loss = 0.021895422645521332
917, epoch_train_loss=0.021895422645521332
Epoch 918
Epoch 918 :: Batch 0/1
Batch Loss = 0.022195592842315183
918, epoch_train_loss=0.022195592842315183
Epoch 919
Epoch 919 :: Batch 0/1
Batch Loss = 0.02963505905782514
919, epoch_train_loss=0.02963505905782514
Epoch 920
Epoch 920 :: Batch 0/1
Batch Loss = 0.023095668267541277
920, epoch_train_loss=0.023095668267541277
Epoch 921
Epoch 921 :: Batch 0/1
Batch Loss = 0.019149550853660827
921, epoch_train_loss=0.019149550853660827
Epoch 922
Epoch 922 :: Batch 0/1
Batch Loss = 0.02403063698521911
922, epoch_train_loss=0.02403063698521911
Epoch 923
Epoch 923 :: Batch 0/1
Batch Loss = 0.02475589847802207
923, epoch_train_loss=0.02475589847802207
Epoch 924
Epoch 924 :: Batch 0/1
Batch Loss = 0.019953498115605418
924, epoch_train_loss=0.019953498115605418
Epoch 925
Epoch 925 :: Batch 0/1
Batch Loss = 0.019951063121554237
925, epoch_train_loss=0.019951063121554237
Epoch 926
Epoch 926 :: Batch 0/1
Batch Loss = 0.023386584867848233
926, epoch_train_loss=0.023386584867848233
Epoch 927
Epoch 927 :: Batch 0/1
Batch Loss = 0.021641888610653484
927, epoch_train_loss=0.021641888610653484
Epoch 928
Epoch 928 :: Batch 0/1
Batch Loss = 0.018767982986893878
928, epoch_train_loss=0.018767982986893878
Epoch 929
Epoch 929 :: Batch 0/1
Batch Loss = 0.020219821035133277
929, epoch_train_loss=0.020219821035133277
Epoch 930
Epoch 930 :: Batch 0/1
Batch Loss = 0.021708893858667904
930, epoch_train_loss=0.021708893858667904
Epoch 931
Epoch 931 :: Batch 0/1
Batch Loss = 0.01969593027139119
931, epoch_train_loss=0.01969593027139119
Epoch 932
Epoch 932 :: Batch 0/1
Batch Loss = 0.01818288351963421
932, epoch_train_loss=0.01818288351963421
Epoch 933
Epoch 933 :: Batch 0/1
Batch Loss = 0.019654865171175307
933, epoch_train_loss=0.019654865171175307
Epoch 934
Epoch 934 :: Batch 0/1
Batch Loss = 0.020472860597292173
934, epoch_train_loss=0.020472860597292173
Epoch 935
Epoch 935 :: Batch 0/1
Batch Loss = 0.019230567722624183
935, epoch_train_loss=0.019230567722624183
Epoch 936
Epoch 936 :: Batch 0/1
Batch Loss = 0.01809802554243058
936, epoch_train_loss=0.01809802554243058
Epoch 937
Epoch 937 :: Batch 0/1
Batch Loss = 0.018824550139701586
937, epoch_train_loss=0.018824550139701586
Epoch 938
Epoch 938 :: Batch 0/1
Batch Loss = 0.01972753804037367
938, epoch_train_loss=0.01972753804037367
Epoch 939
Epoch 939 :: Batch 0/1
Batch Loss = 0.019042948484486198
939, epoch_train_loss=0.019042948484486198
Epoch 940
Epoch 940 :: Batch 0/1
Batch Loss = 0.018078834535262663
940, epoch_train_loss=0.018078834535262663
Epoch 941
Epoch 941 :: Batch 0/1
Batch Loss = 0.01811670317874097
941, epoch_train_loss=0.01811670317874097
Epoch 942
Epoch 942 :: Batch 0/1
Batch Loss = 0.018807628383349116
942, epoch_train_loss=0.018807628383349116
Epoch 943
Epoch 943 :: Batch 0/1
Batch Loss = 0.018802622654077592
943, epoch_train_loss=0.018802622654077592
Epoch 944
Epoch 944 :: Batch 0/1
Batch Loss = 0.018117737173551706
944, epoch_train_loss=0.018117737173551706
Epoch 945
Epoch 945 :: Batch 0/1
Batch Loss = 0.017774264014721767
945, epoch_train_loss=0.017774264014721767
Epoch 946
Epoch 946 :: Batch 0/1
Batch Loss = 0.018096535086597473
946, epoch_train_loss=0.018096535086597473
Epoch 947
Epoch 947 :: Batch 0/1
Batch Loss = 0.018423928591472866
947, epoch_train_loss=0.018423928591472866
Epoch 948
Epoch 948 :: Batch 0/1
Batch Loss = 0.018191780516720234
948, epoch_train_loss=0.018191780516720234
Epoch 949
Epoch 949 :: Batch 0/1
Batch Loss = 0.017786924361088818
949, epoch_train_loss=0.017786924361088818
Epoch 950
Epoch 950 :: Batch 0/1
Batch Loss = 0.01772309957542678
950, epoch_train_loss=0.01772309957542678
Epoch 951
Epoch 951 :: Batch 0/1
Batch Loss = 0.017970716414272198
951, epoch_train_loss=0.017970716414272198
Epoch 952
Epoch 952 :: Batch 0/1
Batch Loss = 0.01806231094967853
952, epoch_train_loss=0.01806231094967853
Epoch 953
Epoch 953 :: Batch 0/1
Batch Loss = 0.017838554638344533
953, epoch_train_loss=0.017838554638344533
Epoch 954
Epoch 954 :: Batch 0/1
Batch Loss = 0.017611620203602484
954, epoch_train_loss=0.017611620203602484
Epoch 955
Epoch 955 :: Batch 0/1
Batch Loss = 0.01761669003644867
955, epoch_train_loss=0.01761669003644867
Epoch 956
Epoch 956 :: Batch 0/1
Batch Loss = 0.0177674698096722
956, epoch_train_loss=0.0177674698096722
Epoch 957
Epoch 957 :: Batch 0/1
Batch Loss = 0.017781472372568906
957, epoch_train_loss=0.017781472372568906
Epoch 958
Epoch 958 :: Batch 0/1
Batch Loss = 0.01763153224708571
958, epoch_train_loss=0.01763153224708571
Epoch 959
Epoch 959 :: Batch 0/1
Batch Loss = 0.0175108436000263
959, epoch_train_loss=0.0175108436000263
Epoch 960
Epoch 960 :: Batch 0/1
Batch Loss = 0.017534147461603852
960, epoch_train_loss=0.017534147461603852
Epoch 961
Epoch 961 :: Batch 0/1
Batch Loss = 0.017618398679677565
961, epoch_train_loss=0.017618398679677565
Epoch 962
Epoch 962 :: Batch 0/1
Batch Loss = 0.017596856075331128
962, epoch_train_loss=0.017596856075331128
Epoch 963
Epoch 963 :: Batch 0/1
Batch Loss = 0.017492822738068285
963, epoch_train_loss=0.017492822738068285
Epoch 964
Epoch 964 :: Batch 0/1
Batch Loss = 0.017426188447812085
964, epoch_train_loss=0.017426188447812085
Epoch 965
Epoch 965 :: Batch 0/1
Batch Loss = 0.01744663877734431
965, epoch_train_loss=0.01744663877734431
Epoch 966
Epoch 966 :: Batch 0/1
Batch Loss = 0.017487154914103156
966, epoch_train_loss=0.017487154914103156
Epoch 967
Epoch 967 :: Batch 0/1
Batch Loss = 0.017458744471777293
967, epoch_train_loss=0.017458744471777293
Epoch 968
Epoch 968 :: Batch 0/1
Batch Loss = 0.017391405987991604
968, epoch_train_loss=0.017391405987991604
Epoch 969
Epoch 969 :: Batch 0/1
Batch Loss = 0.01735311120672678
969, epoch_train_loss=0.01735311120672678
Epoch 970
Epoch 970 :: Batch 0/1
Batch Loss = 0.017366233473146614
970, epoch_train_loss=0.017366233473146614
Epoch 971
Epoch 971 :: Batch 0/1
Batch Loss = 0.017384825295862882
971, epoch_train_loss=0.017384825295862882
Epoch 972
Epoch 972 :: Batch 0/1
Batch Loss = 0.017360786079280675
972, epoch_train_loss=0.017360786079280675
Epoch 973
Epoch 973 :: Batch 0/1
Batch Loss = 0.017315445307393186
973, epoch_train_loss=0.017315445307393186
Epoch 974
Epoch 974 :: Batch 0/1
Batch Loss = 0.017287557372191238
974, epoch_train_loss=0.017287557372191238
Epoch 975
Epoch 975 :: Batch 0/1
Batch Loss = 0.01728986761655695
975, epoch_train_loss=0.01728986761655695
Epoch 976
Epoch 976 :: Batch 0/1
Batch Loss = 0.0172944741272773
976, epoch_train_loss=0.0172944741272773
Epoch 977
Epoch 977 :: Batch 0/1
Batch Loss = 0.01727496781953295
977, epoch_train_loss=0.01727496781953295
Epoch 978
Epoch 978 :: Batch 0/1
Batch Loss = 0.017243176199505703
978, epoch_train_loss=0.017243176199505703
Epoch 979
Epoch 979 :: Batch 0/1
Batch Loss = 0.017221067021198903
979, epoch_train_loss=0.017221067021198903
Epoch 980
Epoch 980 :: Batch 0/1
Batch Loss = 0.01721727873216279
980, epoch_train_loss=0.01721727873216279
Epoch 981
Epoch 981 :: Batch 0/1
Batch Loss = 0.017215694757208654
981, epoch_train_loss=0.017215694757208654
Epoch 982
Epoch 982 :: Batch 0/1
Batch Loss = 0.017200647545943106
982, epoch_train_loss=0.017200647545943106
Epoch 983
Epoch 983 :: Batch 0/1
Batch Loss = 0.017177081816433223
983, epoch_train_loss=0.017177081816433223
Epoch 984
Epoch 984 :: Batch 0/1
Batch Loss = 0.017157440047924882
984, epoch_train_loss=0.017157440047924882
Epoch 985
Epoch 985 :: Batch 0/1
Batch Loss = 0.01714861304558896
985, epoch_train_loss=0.01714861304558896
Epoch 986
Epoch 986 :: Batch 0/1
Batch Loss = 0.01714261029465955
986, epoch_train_loss=0.01714261029465955
Epoch 987
Epoch 987 :: Batch 0/1
Batch Loss = 0.01713003852504739
987, epoch_train_loss=0.01713003852504739
Epoch 988
Epoch 988 :: Batch 0/1
Batch Loss = 0.017111537224277065
988, epoch_train_loss=0.017111537224277065
Epoch 989
Epoch 989 :: Batch 0/1
Batch Loss = 0.017093494198293268
989, epoch_train_loss=0.017093494198293268
Epoch 990
Epoch 990 :: Batch 0/1
Batch Loss = 0.017081587719586833
990, epoch_train_loss=0.017081587719586833
Epoch 991
Epoch 991 :: Batch 0/1
Batch Loss = 0.017073128709636905
991, epoch_train_loss=0.017073128709636905
Epoch 992
Epoch 992 :: Batch 0/1
Batch Loss = 0.01706257595356853
992, epoch_train_loss=0.01706257595356853
Epoch 993
Epoch 993 :: Batch 0/1
Batch Loss = 0.017048056387555174
993, epoch_train_loss=0.017048056387555174
Epoch 994
Epoch 994 :: Batch 0/1
Batch Loss = 0.01703175899522998
994, epoch_train_loss=0.01703175899522998
Epoch 995
Epoch 995 :: Batch 0/1
Batch Loss = 0.017017884671945423
995, epoch_train_loss=0.017017884671945423
Epoch 996
Epoch 996 :: Batch 0/1
Batch Loss = 0.017006922209629283
996, epoch_train_loss=0.017006922209629283
Epoch 997
Epoch 997 :: Batch 0/1
Batch Loss = 0.016996531124054352
997, epoch_train_loss=0.016996531124054352
Epoch 998
Epoch 998 :: Batch 0/1
Batch Loss = 0.01698444576244409
998, epoch_train_loss=0.01698444576244409
Epoch 999
Epoch 999 :: Batch 0/1
Batch Loss = 0.016970249757286157
999, epoch_train_loss=0.016970249757286157
Epoch 1000
Epoch 1000 :: Batch 0/1
Batch Loss = 0.016956050590827495
1000, epoch_train_loss=0.016956050590827495
Epoch 1001
Epoch 1001 :: Batch 0/1
Batch Loss = 0.016943324961258853
1001, epoch_train_loss=0.016943324961258853
Epoch 1002
Epoch 1002 :: Batch 0/1
Batch Loss = 0.0169320217795241
1002, epoch_train_loss=0.0169320217795241
Epoch 1003
Epoch 1003 :: Batch 0/1
Batch Loss = 0.016920920058637295
1003, epoch_train_loss=0.016920920058637295
Epoch 1004
Epoch 1004 :: Batch 0/1
Batch Loss = 0.016908710767660366
1004, epoch_train_loss=0.016908710767660366
Epoch 1005
Epoch 1005 :: Batch 0/1
Batch Loss = 0.016895577894368016
1005, epoch_train_loss=0.016895577894368016
Epoch 1006
Epoch 1006 :: Batch 0/1
Batch Loss = 0.016882351826239837
1006, epoch_train_loss=0.016882351826239837
Epoch 1007
Epoch 1007 :: Batch 0/1
Batch Loss = 0.016869834203111674
1007, epoch_train_loss=0.016869834203111674
Epoch 1008
Epoch 1008 :: Batch 0/1
Batch Loss = 0.016858147353682136
1008, epoch_train_loss=0.016858147353682136
Epoch 1009
Epoch 1009 :: Batch 0/1
Batch Loss = 0.016846604067047785
1009, epoch_train_loss=0.016846604067047785
Epoch 1010
Epoch 1010 :: Batch 0/1
Batch Loss = 0.016834683246056508
1010, epoch_train_loss=0.016834683246056508
Epoch 1011
Epoch 1011 :: Batch 0/1
Batch Loss = 0.01682223341669248
1011, epoch_train_loss=0.01682223341669248
Epoch 1012
Epoch 1012 :: Batch 0/1
Batch Loss = 0.016809576411041423
1012, epoch_train_loss=0.016809576411041423
Epoch 1013
Epoch 1013 :: Batch 0/1
Batch Loss = 0.01679720374671006
1013, epoch_train_loss=0.01679720374671006
Epoch 1014
Epoch 1014 :: Batch 0/1
Batch Loss = 0.016785238978532388
1014, epoch_train_loss=0.016785238978532388
Epoch 1015
Epoch 1015 :: Batch 0/1
Batch Loss = 0.01677355455409465
1015, epoch_train_loss=0.01677355455409465
Epoch 1016
Epoch 1016 :: Batch 0/1
Batch Loss = 0.01676184469237074
1016, epoch_train_loss=0.01676184469237074
Epoch 1017
Epoch 1017 :: Batch 0/1
Batch Loss = 0.016749883037015172
1017, epoch_train_loss=0.016749883037015172
Epoch 1018
Epoch 1018 :: Batch 0/1
Batch Loss = 0.016737724754199323
1018, epoch_train_loss=0.016737724754199323
Epoch 1019
Epoch 1019 :: Batch 0/1
Batch Loss = 0.01672552664538123
1019, epoch_train_loss=0.01672552664538123
Epoch 1020
Epoch 1020 :: Batch 0/1
Batch Loss = 0.01671347453099025
1020, epoch_train_loss=0.01671347453099025
Epoch 1021
Epoch 1021 :: Batch 0/1
Batch Loss = 0.01670164421623916
1021, epoch_train_loss=0.01670164421623916
Epoch 1022
Epoch 1022 :: Batch 0/1
Batch Loss = 0.016689943059922406
1022, epoch_train_loss=0.016689943059922406
Epoch 1023
Epoch 1023 :: Batch 0/1
Batch Loss = 0.016678264236927497
1023, epoch_train_loss=0.016678264236927497
Epoch 1024
Epoch 1024 :: Batch 0/1
Batch Loss = 0.016666509145915883
1024, epoch_train_loss=0.016666509145915883
Epoch 1025
Epoch 1025 :: Batch 0/1
Batch Loss = 0.016654660075242377
1025, epoch_train_loss=0.016654660075242377
Epoch 1026
Epoch 1026 :: Batch 0/1
Batch Loss = 0.01664279043324299
1026, epoch_train_loss=0.01664279043324299
Epoch 1027
Epoch 1027 :: Batch 0/1
Batch Loss = 0.016630959877014
1027, epoch_train_loss=0.016630959877014
Epoch 1028
Epoch 1028 :: Batch 0/1
Batch Loss = 0.016619224485699798
1028, epoch_train_loss=0.016619224485699798
Epoch 1029
Epoch 1029 :: Batch 0/1
Batch Loss = 0.016607586015397195
1029, epoch_train_loss=0.016607586015397195
Epoch 1030
Epoch 1030 :: Batch 0/1
Batch Loss = 0.016596000814708457
1030, epoch_train_loss=0.016596000814708457
Epoch 1031
Epoch 1031 :: Batch 0/1
Batch Loss = 0.01658443472464889
1031, epoch_train_loss=0.01658443472464889
Epoch 1032
Epoch 1032 :: Batch 0/1
Batch Loss = 0.0165728520040826
1032, epoch_train_loss=0.0165728520040826
Epoch 1033
Epoch 1033 :: Batch 0/1
Batch Loss = 0.016561250320015715
1033, epoch_train_loss=0.016561250320015715
Epoch 1034
Epoch 1034 :: Batch 0/1
Batch Loss = 0.016549651864867152
1034, epoch_train_loss=0.016549651864867152
Epoch 1035
Epoch 1035 :: Batch 0/1
Batch Loss = 0.016538077381151886
1035, epoch_train_loss=0.016538077381151886
Epoch 1036
Epoch 1036 :: Batch 0/1
Batch Loss = 0.016526550338892165
1036, epoch_train_loss=0.016526550338892165
Epoch 1037
Epoch 1037 :: Batch 0/1
Batch Loss = 0.016515077763055695
1037, epoch_train_loss=0.016515077763055695
Epoch 1038
Epoch 1038 :: Batch 0/1
Batch Loss = 0.01650364999329567
1038, epoch_train_loss=0.01650364999329567
Epoch 1039
Epoch 1039 :: Batch 0/1
Batch Loss = 0.01649225770219312
1039, epoch_train_loss=0.01649225770219312
Epoch 1040
Epoch 1040 :: Batch 0/1
Batch Loss = 0.016480885993776666
1040, epoch_train_loss=0.016480885993776666
Epoch 1041
Epoch 1041 :: Batch 0/1
Batch Loss = 0.01646952656134101
1041, epoch_train_loss=0.01646952656134101
Epoch 1042
Epoch 1042 :: Batch 0/1
Batch Loss = 0.01645818131405748
1042, epoch_train_loss=0.01645818131405748
Epoch 1043
Epoch 1043 :: Batch 0/1
Batch Loss = 0.016446851007619747
1043, epoch_train_loss=0.016446851007619747
Epoch 1044
Epoch 1044 :: Batch 0/1
Batch Loss = 0.016435544179309786
1044, epoch_train_loss=0.016435544179309786
Epoch 1045
Epoch 1045 :: Batch 0/1
Batch Loss = 0.01642426703353478
1045, epoch_train_loss=0.01642426703353478
Epoch 1046
Epoch 1046 :: Batch 0/1
Batch Loss = 0.016413022618371735
1046, epoch_train_loss=0.016413022618371735
Epoch 1047
Epoch 1047 :: Batch 0/1
Batch Loss = 0.01640181403644169
1047, epoch_train_loss=0.01640181403644169
Epoch 1048
Epoch 1048 :: Batch 0/1
Batch Loss = 0.016390639662051006
1048, epoch_train_loss=0.016390639662051006
Epoch 1049
Epoch 1049 :: Batch 0/1
Batch Loss = 0.016379496634335508
1049, epoch_train_loss=0.016379496634335508
Epoch 1050
Epoch 1050 :: Batch 0/1
Batch Loss = 0.01636838321101612
1050, epoch_train_loss=0.01636838321101612
Epoch 1051
Epoch 1051 :: Batch 0/1
Batch Loss = 0.016357296383170957
1051, epoch_train_loss=0.016357296383170957
Epoch 1052
Epoch 1052 :: Batch 0/1
Batch Loss = 0.016346234945795724
1052, epoch_train_loss=0.016346234945795724
Epoch 1053
Epoch 1053 :: Batch 0/1
Batch Loss = 0.01633519938530639
1053, epoch_train_loss=0.01633519938530639
Epoch 1054
Epoch 1054 :: Batch 0/1
Batch Loss = 0.01632418888933787
1054, epoch_train_loss=0.01632418888933787
Epoch 1055
Epoch 1055 :: Batch 0/1
Batch Loss = 0.016313205113209547
1055, epoch_train_loss=0.016313205113209547
Epoch 1056
Epoch 1056 :: Batch 0/1
Batch Loss = 0.01630224870332118
1056, epoch_train_loss=0.01630224870332118
Epoch 1057
Epoch 1057 :: Batch 0/1
Batch Loss = 0.01629132043770165
1057, epoch_train_loss=0.01629132043770165
Epoch 1058
Epoch 1058 :: Batch 0/1
Batch Loss = 0.22070396779403825
1058, epoch_train_loss=0.22070396779403825
Epoch 1059
Epoch 1059 :: Batch 0/1
Batch Loss = 3.726225809454726
1059, epoch_train_loss=3.726225809454726
Epoch 1060
Epoch 1060 :: Batch 0/1
Batch Loss = 2.518989394261895
1060, epoch_train_loss=2.518989394261895
Epoch 1061
Epoch 1061 :: Batch 0/1
Batch Loss = 1.3458645283740611
1061, epoch_train_loss=1.3458645283740611
Epoch 1062
Epoch 1062 :: Batch 0/1
Batch Loss = 2.1096593189641832
1062, epoch_train_loss=2.1096593189641832
Epoch 1063
Epoch 1063 :: Batch 0/1
Batch Loss = 1.6095324289660817
1063, epoch_train_loss=1.6095324289660817
Epoch 1064
Epoch 1064 :: Batch 0/1
Batch Loss = 2.5745627268881526
1064, epoch_train_loss=2.5745627268881526
Epoch 1065
Epoch 1065 :: Batch 0/1
Batch Loss = 1.1364526462768607
1065, epoch_train_loss=1.1364526462768607
Epoch 1066
Epoch 1066 :: Batch 0/1
Batch Loss = 1.3352553402839722
1066, epoch_train_loss=1.3352553402839722
Epoch 1067
Epoch 1067 :: Batch 0/1
Batch Loss = 1.3056769777508084
1067, epoch_train_loss=1.3056769777508084
Epoch 1068
Epoch 1068 :: Batch 0/1
Batch Loss = 0.6400982260898433
1068, epoch_train_loss=0.6400982260898433
Epoch 1069
Epoch 1069 :: Batch 0/1
Batch Loss = 1.2713720746964707
1069, epoch_train_loss=1.2713720746964707
Epoch 1070
Epoch 1070 :: Batch 0/1
Batch Loss = 1.14399347823491
1070, epoch_train_loss=1.14399347823491
Epoch 1071
Epoch 1071 :: Batch 0/1
Batch Loss = 0.4902913870609856
1071, epoch_train_loss=0.4902913870609856
Epoch 1072
Epoch 1072 :: Batch 0/1
Batch Loss = 0.8484999520283907
1072, epoch_train_loss=0.8484999520283907
Epoch 1073
Epoch 1073 :: Batch 0/1
Batch Loss = 0.7478863553605373
1073, epoch_train_loss=0.7478863553605373
Epoch 1074
Epoch 1074 :: Batch 0/1
Batch Loss = 0.43108716199636155
1074, epoch_train_loss=0.43108716199636155
Epoch 1075
Epoch 1075 :: Batch 0/1
Batch Loss = 2797.5640714223396
1075, epoch_train_loss=2797.5640714223396
Epoch 1076
Epoch 1076 :: Batch 0/1
Batch Loss = 0.5484514642906484
1076, epoch_train_loss=0.5484514642906484
Epoch 1077
Epoch 1077 :: Batch 0/1
Batch Loss = 0.24502827308269318
1077, epoch_train_loss=0.24502827308269318
Epoch 1078
Epoch 1078 :: Batch 0/1
Batch Loss = 0.603341715224625
1078, epoch_train_loss=0.603341715224625
Epoch 1079
Epoch 1079 :: Batch 0/1
Batch Loss = 0.26762601155802657
1079, epoch_train_loss=0.26762601155802657
Epoch 1080
Epoch 1080 :: Batch 0/1
Batch Loss = 0.3443941469181448
1080, epoch_train_loss=0.3443941469181448
Epoch 1081
Epoch 1081 :: Batch 0/1
Batch Loss = 0.4592736535875555
1081, epoch_train_loss=0.4592736535875555
Epoch 1082
Epoch 1082 :: Batch 0/1
Batch Loss = 0.19868208558231282
1082, epoch_train_loss=0.19868208558231282
Epoch 1083
Epoch 1083 :: Batch 0/1
Batch Loss = 0.3834903357310602
1083, epoch_train_loss=0.3834903357310602
Epoch 1084
Epoch 1084 :: Batch 0/1
Batch Loss = 0.27019792737312337
1084, epoch_train_loss=0.27019792737312337
Epoch 1085
Epoch 1085 :: Batch 0/1
Batch Loss = 0.24079317942302
1085, epoch_train_loss=0.24079317942302
Epoch 1086
Epoch 1086 :: Batch 0/1
Batch Loss = 0.35077080016001305
1086, epoch_train_loss=0.35077080016001305
Epoch 1087
Epoch 1087 :: Batch 0/1
Batch Loss = 0.18272779580744408
1087, epoch_train_loss=0.18272779580744408
Epoch 1088
Epoch 1088 :: Batch 0/1
Batch Loss = 0.29216132793513566
1088, epoch_train_loss=0.29216132793513566
Epoch 1089
Epoch 1089 :: Batch 0/1
Batch Loss = 0.23038945887278137
1089, epoch_train_loss=0.23038945887278137
Epoch 1090
Epoch 1090 :: Batch 0/1
Batch Loss = 0.19271383095326813
1090, epoch_train_loss=0.19271383095326813
Epoch 1091
Epoch 1091 :: Batch 0/1
Batch Loss = 0.26903744133378166
1091, epoch_train_loss=0.26903744133378166
Epoch 1092
Epoch 1092 :: Batch 0/1
Batch Loss = 0.1712071976042057
1092, epoch_train_loss=0.1712071976042057
Epoch 1093
Epoch 1093 :: Batch 0/1
Batch Loss = 0.211526979104134
1093, epoch_train_loss=0.211526979104134
Epoch 1094
Epoch 1094 :: Batch 0/1
Batch Loss = 0.21224849667177667
1094, epoch_train_loss=0.21224849667177667
Epoch 1095
Epoch 1095 :: Batch 0/1
Batch Loss = 0.16015762646480322
1095, epoch_train_loss=0.16015762646480322
Epoch 1096
Epoch 1096 :: Batch 0/1
Batch Loss = 0.20914993569704243
1096, epoch_train_loss=0.20914993569704243
Epoch 1097
Epoch 1097 :: Batch 0/1
Batch Loss = 0.16677922052248348
1097, epoch_train_loss=0.16677922052248348
Epoch 1098
Epoch 1098 :: Batch 0/1
Batch Loss = 0.1673821299262232
1098, epoch_train_loss=0.1673821299262232
Epoch 1099
Epoch 1099 :: Batch 0/1
Batch Loss = 0.18153913310698358
1099, epoch_train_loss=0.18153913310698358
Epoch 1100
Epoch 1100 :: Batch 0/1
Batch Loss = 0.14813514876820277
1100, epoch_train_loss=0.14813514876820277
Epoch 1101
Epoch 1101 :: Batch 0/1
Batch Loss = 0.16947584125762793
1101, epoch_train_loss=0.16947584125762793
Epoch 1102
Epoch 1102 :: Batch 0/1
Batch Loss = 0.14816853263501095
1102, epoch_train_loss=0.14816853263501095
Epoch 1103
Epoch 1103 :: Batch 0/1
Batch Loss = 0.13672965903576192
1103, epoch_train_loss=0.13672965903576192
Epoch 1104
Epoch 1104 :: Batch 0/1
Batch Loss = 0.15722285655690177
1104, epoch_train_loss=0.15722285655690177
Epoch 1105
Epoch 1105 :: Batch 0/1
Batch Loss = 0.12874501576073252
1105, epoch_train_loss=0.12874501576073252
Epoch 1106
Epoch 1106 :: Batch 0/1
Batch Loss = 0.14136024170968114
1106, epoch_train_loss=0.14136024170968114
Epoch 1107
Epoch 1107 :: Batch 0/1
Batch Loss = 0.13593773675030887
1107, epoch_train_loss=0.13593773675030887
Epoch 1108
Epoch 1108 :: Batch 0/1
Batch Loss = 0.12844223369926638
1108, epoch_train_loss=0.12844223369926638
Epoch 1109
Epoch 1109 :: Batch 0/1
Batch Loss = 0.13682157470405776
1109, epoch_train_loss=0.13682157470405776
Epoch 1110
Epoch 1110 :: Batch 0/1
Batch Loss = 0.12257799218691155
1110, epoch_train_loss=0.12257799218691155
Epoch 1111
Epoch 1111 :: Batch 0/1
Batch Loss = 0.12949570922598533
1111, epoch_train_loss=0.12949570922598533
Epoch 1112
Epoch 1112 :: Batch 0/1
Batch Loss = 0.12360809200380857
1112, epoch_train_loss=0.12360809200380857
Epoch 1113
Epoch 1113 :: Batch 0/1
Batch Loss = 0.11912582490609777
1113, epoch_train_loss=0.11912582490609777
Epoch 1114
Epoch 1114 :: Batch 0/1
Batch Loss = 0.1220495604351361
1114, epoch_train_loss=0.1220495604351361
Epoch 1115
Epoch 1115 :: Batch 0/1
Batch Loss = 0.11177984748850746
1115, epoch_train_loss=0.11177984748850746
Epoch 1116
Epoch 1116 :: Batch 0/1
Batch Loss = 0.11523377632997454
1116, epoch_train_loss=0.11523377632997454
Epoch 1117
Epoch 1117 :: Batch 0/1
Batch Loss = 0.10795768426546139
1117, epoch_train_loss=0.10795768426546139
Epoch 1118
Epoch 1118 :: Batch 0/1
Batch Loss = 0.10795616436203605
1118, epoch_train_loss=0.10795616436203605
Epoch 1119
Epoch 1119 :: Batch 0/1
Batch Loss = 0.10850030527815474
1119, epoch_train_loss=0.10850030527815474
Epoch 1120
Epoch 1120 :: Batch 0/1
Batch Loss = 0.10256594405560028
1120, epoch_train_loss=0.10256594405560028
Epoch 1121
Epoch 1121 :: Batch 0/1
Batch Loss = 0.10554514283925918
1121, epoch_train_loss=0.10554514283925918
Epoch 1122
Epoch 1122 :: Batch 0/1
Batch Loss = 0.10049977171007864
1122, epoch_train_loss=0.10049977171007864
Epoch 1123
Epoch 1123 :: Batch 0/1
Batch Loss = 0.10131510700365795
1123, epoch_train_loss=0.10131510700365795
Epoch 1124
Epoch 1124 :: Batch 0/1
Batch Loss = 0.09945435629712622
1124, epoch_train_loss=0.09945435629712622
Epoch 1125
Epoch 1125 :: Batch 0/1
Batch Loss = 0.09704290641640123
1125, epoch_train_loss=0.09704290641640123
Epoch 1126
Epoch 1126 :: Batch 0/1
Batch Loss = 0.0962678045193987
1126, epoch_train_loss=0.0962678045193987
Epoch 1127
Epoch 1127 :: Batch 0/1
Batch Loss = 0.09207950062690226
1127, epoch_train_loss=0.09207950062690226
Epoch 1128
Epoch 1128 :: Batch 0/1
Batch Loss = 0.09292652320783683
1128, epoch_train_loss=0.09292652320783683
Epoch 1129
Epoch 1129 :: Batch 0/1
Batch Loss = 0.08900693741216859
1129, epoch_train_loss=0.08900693741216859
Epoch 1130
Epoch 1130 :: Batch 0/1
Batch Loss = 0.08935916220981756
1130, epoch_train_loss=0.08935916220981756
Epoch 1131
Epoch 1131 :: Batch 0/1
Batch Loss = 0.08624622850127676
1131, epoch_train_loss=0.08624622850127676
Epoch 1132
Epoch 1132 :: Batch 0/1
Batch Loss = 0.08467018872547992
1132, epoch_train_loss=0.08467018872547992
Epoch 1133
Epoch 1133 :: Batch 0/1
Batch Loss = 0.08345342149080631
1133, epoch_train_loss=0.08345342149080631
Epoch 1134
Epoch 1134 :: Batch 0/1
Batch Loss = 0.08083124985058282
1134, epoch_train_loss=0.08083124985058282
Epoch 1135
Epoch 1135 :: Batch 0/1
Batch Loss = 0.07980524934900234
1135, epoch_train_loss=0.07980524934900234
Epoch 1136
Epoch 1136 :: Batch 0/1
Batch Loss = 0.07736178311743289
1136, epoch_train_loss=0.07736178311743289
Epoch 1137
Epoch 1137 :: Batch 0/1
Batch Loss = 0.07623227645095695
1137, epoch_train_loss=0.07623227645095695
Epoch 1138
Epoch 1138 :: Batch 0/1
Batch Loss = 0.07345975091937283
1138, epoch_train_loss=0.07345975091937283
Epoch 1139
Epoch 1139 :: Batch 0/1
Batch Loss = 0.07180749777326217
1139, epoch_train_loss=0.07180749777326217
Epoch 1140
Epoch 1140 :: Batch 0/1
Batch Loss = 0.06912220807665657
1140, epoch_train_loss=0.06912220807665657
Epoch 1141
Epoch 1141 :: Batch 0/1
Batch Loss = 0.06701093037243203
1141, epoch_train_loss=0.06701093037243203
Epoch 1142
Epoch 1142 :: Batch 0/1
Batch Loss = 0.06552331244145093
1142, epoch_train_loss=0.06552331244145093
Epoch 1143
Epoch 1143 :: Batch 0/1
Batch Loss = 0.06378015414161618
1143, epoch_train_loss=0.06378015414161618
Epoch 1144
Epoch 1144 :: Batch 0/1
Batch Loss = 0.06293914218421193
1144, epoch_train_loss=0.06293914218421193
Epoch 1145
Epoch 1145 :: Batch 0/1
Batch Loss = 0.06116261142532537
1145, epoch_train_loss=0.06116261142532537
Epoch 1146
Epoch 1146 :: Batch 0/1
Batch Loss = 0.06073105980590017
1146, epoch_train_loss=0.06073105980590017
Epoch 1147
Epoch 1147 :: Batch 0/1
Batch Loss = 0.059772558902000096
1147, epoch_train_loss=0.059772558902000096
Epoch 1148
Epoch 1148 :: Batch 0/1
Batch Loss = 0.05940525841727062
1148, epoch_train_loss=0.05940525841727062
Epoch 1149
Epoch 1149 :: Batch 0/1
Batch Loss = 0.05791777596233904
1149, epoch_train_loss=0.05791777596233904
Epoch 1150
Epoch 1150 :: Batch 0/1
Batch Loss = 0.05731436583454073
1150, epoch_train_loss=0.05731436583454073
Epoch 1151
Epoch 1151 :: Batch 0/1
Batch Loss = 0.05665309447137372
1151, epoch_train_loss=0.05665309447137372
Epoch 1152
Epoch 1152 :: Batch 0/1
Batch Loss = 0.05706746799102495
1152, epoch_train_loss=0.05706746799102495
Epoch 1153
Epoch 1153 :: Batch 0/1
Batch Loss = 0.05699627451435784
1153, epoch_train_loss=0.05699627451435784
Epoch 1154
Epoch 1154 :: Batch 0/1
Batch Loss = 0.057225446669708344
1154, epoch_train_loss=0.057225446669708344
Epoch 1155
Epoch 1155 :: Batch 0/1
Batch Loss = 0.05664843993287546
1155, epoch_train_loss=0.05664843993287546
Epoch 1156
Epoch 1156 :: Batch 0/1
Batch Loss = 0.05622166924053443
1156, epoch_train_loss=0.05622166924053443
Epoch 1157
Epoch 1157 :: Batch 0/1
Batch Loss = 0.05570007808242564
1157, epoch_train_loss=0.05570007808242564
Epoch 1158
Epoch 1158 :: Batch 0/1
Batch Loss = 0.0554282157533484
1158, epoch_train_loss=0.0554282157533484
Epoch 1159
Epoch 1159 :: Batch 0/1
Batch Loss = 0.05535591357676107
1159, epoch_train_loss=0.05535591357676107
Epoch 1160
Epoch 1160 :: Batch 0/1
Batch Loss = 0.05497204657765736
1160, epoch_train_loss=0.05497204657765736
Epoch 1161
Epoch 1161 :: Batch 0/1
Batch Loss = 0.054677610069444126
1161, epoch_train_loss=0.054677610069444126
Epoch 1162
Epoch 1162 :: Batch 0/1
Batch Loss = 0.054027555911388025
1162, epoch_train_loss=0.054027555911388025
Epoch 1163
Epoch 1163 :: Batch 0/1
Batch Loss = 0.053698318090256186
1163, epoch_train_loss=0.053698318090256186
Epoch 1164
Epoch 1164 :: Batch 0/1
Batch Loss = 0.05324048347868759
1164, epoch_train_loss=0.05324048347868759
Epoch 1165
Epoch 1165 :: Batch 0/1
Batch Loss = 0.053117601432106726
1165, epoch_train_loss=0.053117601432106726
Epoch 1166
Epoch 1166 :: Batch 0/1
Batch Loss = 0.05299304189269252
1166, epoch_train_loss=0.05299304189269252
Epoch 1167
Epoch 1167 :: Batch 0/1
Batch Loss = 0.052851406485752865
1167, epoch_train_loss=0.052851406485752865
Epoch 1168
Epoch 1168 :: Batch 0/1
Batch Loss = 0.05272961180658106
1168, epoch_train_loss=0.05272961180658106
Epoch 1169
Epoch 1169 :: Batch 0/1
Batch Loss = 0.05259388718769585
1169, epoch_train_loss=0.05259388718769585
Epoch 1170
Epoch 1170 :: Batch 0/1
Batch Loss = 0.05259176406368327
1170, epoch_train_loss=0.05259176406368327
Epoch 1171
Epoch 1171 :: Batch 0/1
Batch Loss = 0.052450000582888186
1171, epoch_train_loss=0.052450000582888186
Epoch 1172
Epoch 1172 :: Batch 0/1
Batch Loss = 0.052322640256240414
1172, epoch_train_loss=0.052322640256240414
Epoch 1173
Epoch 1173 :: Batch 0/1
Batch Loss = 0.05202413985684391
1173, epoch_train_loss=0.05202413985684391
Epoch 1174
Epoch 1174 :: Batch 0/1
Batch Loss = 0.05178529225918675
1174, epoch_train_loss=0.05178529225918675
Epoch 1175
Epoch 1175 :: Batch 0/1
Batch Loss = 0.05154457266672147
1175, epoch_train_loss=0.05154457266672147
Epoch 1176
Epoch 1176 :: Batch 0/1
Batch Loss = 0.051345768472539174
1176, epoch_train_loss=0.051345768472539174
Epoch 1177
Epoch 1177 :: Batch 0/1
Batch Loss = 0.05116608281083308
1177, epoch_train_loss=0.05116608281083308
Epoch 1178
Epoch 1178 :: Batch 0/1
Batch Loss = 0.05098171122853081
1178, epoch_train_loss=0.05098171122853081
Epoch 1179
Epoch 1179 :: Batch 0/1
Batch Loss = 0.05089141706881221
1179, epoch_train_loss=0.05089141706881221
Epoch 1180
Epoch 1180 :: Batch 0/1
Batch Loss = 0.05078700916641554
1180, epoch_train_loss=0.05078700916641554
Epoch 1181
Epoch 1181 :: Batch 0/1
Batch Loss = 0.050713789657974295
1181, epoch_train_loss=0.050713789657974295
Epoch 1182
Epoch 1182 :: Batch 0/1
Batch Loss = 0.05056513070865762
1182, epoch_train_loss=0.05056513070865762
Epoch 1183
Epoch 1183 :: Batch 0/1
Batch Loss = 0.050392309432882886
1183, epoch_train_loss=0.050392309432882886
Epoch 1184
Epoch 1184 :: Batch 0/1
Batch Loss = 0.05020468000431055
1184, epoch_train_loss=0.05020468000431055
Epoch 1185
Epoch 1185 :: Batch 0/1
Batch Loss = 0.05013807661161316
1185, epoch_train_loss=0.05013807661161316
Epoch 1186
Epoch 1186 :: Batch 0/1
Batch Loss = 0.050022521492890645
1186, epoch_train_loss=0.050022521492890645
Epoch 1187
Epoch 1187 :: Batch 0/1
Batch Loss = 0.049822829523768444
1187, epoch_train_loss=0.049822829523768444
Epoch 1188
Epoch 1188 :: Batch 0/1
Batch Loss = 0.04971932040330738
1188, epoch_train_loss=0.04971932040330738
Epoch 1189
Epoch 1189 :: Batch 0/1
Batch Loss = 0.04964734785507899
1189, epoch_train_loss=0.04964734785507899
Epoch 1190
Epoch 1190 :: Batch 0/1
Batch Loss = 0.04958576485186202
1190, epoch_train_loss=0.04958576485186202
Epoch 1191
Epoch 1191 :: Batch 0/1
Batch Loss = 0.04950360649642856
1191, epoch_train_loss=0.04950360649642856
Epoch 1192
Epoch 1192 :: Batch 0/1
Batch Loss = 0.04941148800051946
1192, epoch_train_loss=0.04941148800051946
Epoch 1193
Epoch 1193 :: Batch 0/1
Batch Loss = 0.049298358820938824
1193, epoch_train_loss=0.049298358820938824
Epoch 1194
Epoch 1194 :: Batch 0/1
Batch Loss = 0.04917368590974991
1194, epoch_train_loss=0.04917368590974991
Epoch 1195
Epoch 1195 :: Batch 0/1
Batch Loss = 0.04906103240670236
1195, epoch_train_loss=0.04906103240670236
Epoch 1196
Epoch 1196 :: Batch 0/1
Batch Loss = 0.0489542411883285
1196, epoch_train_loss=0.0489542411883285
Epoch 1197
Epoch 1197 :: Batch 0/1
Batch Loss = 0.04886590789154015
1197, epoch_train_loss=0.04886590789154015
Epoch 1198
Epoch 1198 :: Batch 0/1
Batch Loss = 0.048806085643250015
1198, epoch_train_loss=0.048806085643250015
Epoch 1199
Epoch 1199 :: Batch 0/1
Batch Loss = 0.04873476890649619
1199, epoch_train_loss=0.04873476890649619
Epoch 1200
Epoch 1200 :: Batch 0/1
Batch Loss = 0.04863167133521686
1200, epoch_train_loss=0.04863167133521686
Epoch 1201
Epoch 1201 :: Batch 0/1
Batch Loss = 0.048547051464714996
1201, epoch_train_loss=0.048547051464714996
Epoch 1202
Epoch 1202 :: Batch 0/1
Batch Loss = 0.048481431412706666
1202, epoch_train_loss=0.048481431412706666
Epoch 1203
Epoch 1203 :: Batch 0/1
Batch Loss = 0.04841340727599063
1203, epoch_train_loss=0.04841340727599063
Epoch 1204
Epoch 1204 :: Batch 0/1
Batch Loss = 0.04833507979513906
1204, epoch_train_loss=0.04833507979513906
Epoch 1205
Epoch 1205 :: Batch 0/1
Batch Loss = 0.048252643512936774
1205, epoch_train_loss=0.048252643512936774
Epoch 1206
Epoch 1206 :: Batch 0/1
Batch Loss = 0.04817242888557013
1206, epoch_train_loss=0.04817242888557013
Epoch 1207
Epoch 1207 :: Batch 0/1
Batch Loss = 0.048093133599031085
1207, epoch_train_loss=0.048093133599031085
Epoch 1208
Epoch 1208 :: Batch 0/1
Batch Loss = 0.048016937494483235
1208, epoch_train_loss=0.048016937494483235
Epoch 1209
Epoch 1209 :: Batch 0/1
Batch Loss = 0.04794621403520005
1209, epoch_train_loss=0.04794621403520005
Epoch 1210
Epoch 1210 :: Batch 0/1
Batch Loss = 0.04787574230318308
1210, epoch_train_loss=0.04787574230318308
Epoch 1211
Epoch 1211 :: Batch 0/1
Batch Loss = 0.04779147798792263
1211, epoch_train_loss=0.04779147798792263
Epoch 1212
Epoch 1212 :: Batch 0/1
Batch Loss = 0.047689453759635875
1212, epoch_train_loss=0.047689453759635875
Epoch 1213
Epoch 1213 :: Batch 0/1
Batch Loss = 0.24060498047539447
1213, epoch_train_loss=0.24060498047539447
Epoch 1214
Epoch 1214 :: Batch 0/1
Batch Loss = 0.9137535306098469
1214, epoch_train_loss=0.9137535306098469
Epoch 1215
Epoch 1215 :: Batch 0/1
Batch Loss = 0.05470801593087976
1215, epoch_train_loss=0.05470801593087976
Epoch 1216
Epoch 1216 :: Batch 0/1
Batch Loss = 0.7669662077225872
1216, epoch_train_loss=0.7669662077225872
Epoch 1217
Epoch 1217 :: Batch 0/1
Batch Loss = 0.09658071505026483
1217, epoch_train_loss=0.09658071505026483
Epoch 1218
Epoch 1218 :: Batch 0/1
Batch Loss = 0.561605112709737
1218, epoch_train_loss=0.561605112709737
Epoch 1219
Epoch 1219 :: Batch 0/1
Batch Loss = 0.10873626685758608
1219, epoch_train_loss=0.10873626685758608
Epoch 1220
Epoch 1220 :: Batch 0/1
Batch Loss = 0.3561962691415847
1220, epoch_train_loss=0.3561962691415847
Epoch 1221
Epoch 1221 :: Batch 0/1
Batch Loss = 0.21174657433528243
1221, epoch_train_loss=0.21174657433528243
Epoch 1222
Epoch 1222 :: Batch 0/1
Batch Loss = 0.15244395355580978
1222, epoch_train_loss=0.15244395355580978
Epoch 1223
Epoch 1223 :: Batch 0/1
Batch Loss = 0.3296289985697524
1223, epoch_train_loss=0.3296289985697524
Epoch 1224
Epoch 1224 :: Batch 0/1
Batch Loss = 0.1407812270448204
1224, epoch_train_loss=0.1407812270448204
Epoch 1225
Epoch 1225 :: Batch 0/1
Batch Loss = 0.1736119576309442
1225, epoch_train_loss=0.1736119576309442
Epoch 1226
Epoch 1226 :: Batch 0/1
Batch Loss = 0.250528216204872
1226, epoch_train_loss=0.250528216204872
Epoch 1227
Epoch 1227 :: Batch 0/1
Batch Loss = 0.0961500623836686
1227, epoch_train_loss=0.0961500623836686
Epoch 1228
Epoch 1228 :: Batch 0/1
Batch Loss = 0.19002475720368048
1228, epoch_train_loss=0.19002475720368048
Epoch 1229
Epoch 1229 :: Batch 0/1
Batch Loss = 0.19140765824228387
1229, epoch_train_loss=0.19140765824228387
Epoch 1230
Epoch 1230 :: Batch 0/1
Batch Loss = 0.09542768349936408
1230, epoch_train_loss=0.09542768349936408
Epoch 1231
Epoch 1231 :: Batch 0/1
Batch Loss = 0.16945309942918638
1231, epoch_train_loss=0.16945309942918638
Epoch 1232
Epoch 1232 :: Batch 0/1
Batch Loss = 0.1353435271490723
1232, epoch_train_loss=0.1353435271490723
Epoch 1233
Epoch 1233 :: Batch 0/1
Batch Loss = 0.0999699426455232
1233, epoch_train_loss=0.0999699426455232
Epoch 1234
Epoch 1234 :: Batch 0/1
Batch Loss = 0.15702564715873638
1234, epoch_train_loss=0.15702564715873638
Epoch 1235
Epoch 1235 :: Batch 0/1
Batch Loss = 0.10845628894123924
1235, epoch_train_loss=0.10845628894123924
Epoch 1236
Epoch 1236 :: Batch 0/1
Batch Loss = 0.09877496232858837
1236, epoch_train_loss=0.09877496232858837
Epoch 1237
Epoch 1237 :: Batch 0/1
Batch Loss = 0.13530789378800048
1237, epoch_train_loss=0.13530789378800048
Epoch 1238
Epoch 1238 :: Batch 0/1
Batch Loss = 0.08439650370623941
1238, epoch_train_loss=0.08439650370623941
Epoch 1239
Epoch 1239 :: Batch 0/1
Batch Loss = 0.11254790399816966
1239, epoch_train_loss=0.11254790399816966
Epoch 1240
Epoch 1240 :: Batch 0/1
Batch Loss = 0.10743329305297805
1240, epoch_train_loss=0.10743329305297805
Epoch 1241
Epoch 1241 :: Batch 0/1
Batch Loss = 0.07754408726683325
1241, epoch_train_loss=0.07754408726683325
Epoch 1242
Epoch 1242 :: Batch 0/1
Batch Loss = 0.10635563788305057
1242, epoch_train_loss=0.10635563788305057
Epoch 1243
Epoch 1243 :: Batch 0/1
Batch Loss = 0.0801838391719786
1243, epoch_train_loss=0.0801838391719786
Epoch 1244
Epoch 1244 :: Batch 0/1
Batch Loss = 0.07686961995104553
1244, epoch_train_loss=0.07686961995104553
Epoch 1245
Epoch 1245 :: Batch 0/1
Batch Loss = 0.08721388742986223
1245, epoch_train_loss=0.08721388742986223
Epoch 1246
Epoch 1246 :: Batch 0/1
Batch Loss = 0.07034128903018115
1246, epoch_train_loss=0.07034128903018115
Epoch 1247
Epoch 1247 :: Batch 0/1
Batch Loss = 0.07669797003970526
1247, epoch_train_loss=0.07669797003970526
Epoch 1248
Epoch 1248 :: Batch 0/1
Batch Loss = 0.08122691654333342
1248, epoch_train_loss=0.08122691654333342
Epoch 1249
Epoch 1249 :: Batch 0/1
Batch Loss = 0.06845015656820116
1249, epoch_train_loss=0.06845015656820116
Epoch 1250
Epoch 1250 :: Batch 0/1
Batch Loss = 0.07137497018115398
1250, epoch_train_loss=0.07137497018115398
Epoch 1251
Epoch 1251 :: Batch 0/1
Batch Loss = 0.05904234006296672
1251, epoch_train_loss=0.05904234006296672
Epoch 1252
Epoch 1252 :: Batch 0/1
Batch Loss = 0.06510712780597214
1252, epoch_train_loss=0.06510712780597214
Epoch 1253
Epoch 1253 :: Batch 0/1
Batch Loss = 0.06146361746512663
1253, epoch_train_loss=0.06146361746512663
Epoch 1254
Epoch 1254 :: Batch 0/1
Batch Loss = 0.05265390339124962
1254, epoch_train_loss=0.05265390339124962
Epoch 1255
Epoch 1255 :: Batch 0/1
Batch Loss = 0.06212205226099639
1255, epoch_train_loss=0.06212205226099639
Epoch 1256
Epoch 1256 :: Batch 0/1
Batch Loss = 0.05544485835831682
1256, epoch_train_loss=0.05544485835831682
Epoch 1257
Epoch 1257 :: Batch 0/1
Batch Loss = 0.05140705140493876
1257, epoch_train_loss=0.05140705140493876
Epoch 1258
Epoch 1258 :: Batch 0/1
Batch Loss = 0.05084563109913958
1258, epoch_train_loss=0.05084563109913958
Epoch 1259
Epoch 1259 :: Batch 0/1
Batch Loss = 0.05209347398232308
1259, epoch_train_loss=0.05209347398232308
Epoch 1260
Epoch 1260 :: Batch 0/1
Batch Loss = 0.05084166265206681
1260, epoch_train_loss=0.05084166265206681
Epoch 1261
Epoch 1261 :: Batch 0/1
Batch Loss = 0.044664649167458825
1261, epoch_train_loss=0.044664649167458825
Epoch 1262
Epoch 1262 :: Batch 0/1
Batch Loss = 0.0503527029279788
1262, epoch_train_loss=0.0503527029279788
Epoch 1263
Epoch 1263 :: Batch 0/1
Batch Loss = 0.04714996526654568
1263, epoch_train_loss=0.04714996526654568
Epoch 1264
Epoch 1264 :: Batch 0/1
Batch Loss = 0.04564828605809838
1264, epoch_train_loss=0.04564828605809838
Epoch 1265
Epoch 1265 :: Batch 0/1
Batch Loss = 0.04648627552081758
1265, epoch_train_loss=0.04648627552081758
Epoch 1266
Epoch 1266 :: Batch 0/1
Batch Loss = 0.04648648638001201
1266, epoch_train_loss=0.04648648638001201
Epoch 1267
Epoch 1267 :: Batch 0/1
Batch Loss = 0.04563174759335977
1267, epoch_train_loss=0.04563174759335977
Epoch 1268
Epoch 1268 :: Batch 0/1
Batch Loss = 0.04301713027477585
1268, epoch_train_loss=0.04301713027477585
Epoch 1269
Epoch 1269 :: Batch 0/1
Batch Loss = 0.04634961964047186
1269, epoch_train_loss=0.04634961964047186
Epoch 1270
Epoch 1270 :: Batch 0/1
Batch Loss = 0.04416070609182495
1270, epoch_train_loss=0.04416070609182495
Epoch 1271
Epoch 1271 :: Batch 0/1
Batch Loss = 0.04387630635749137
1271, epoch_train_loss=0.04387630635749137
Epoch 1272
Epoch 1272 :: Batch 0/1
Batch Loss = 0.043288714212984795
1272, epoch_train_loss=0.043288714212984795
Epoch 1273
Epoch 1273 :: Batch 0/1
Batch Loss = 0.04417380253352246
1273, epoch_train_loss=0.04417380253352246
Epoch 1274
Epoch 1274 :: Batch 0/1
Batch Loss = 0.04222173287183595
1274, epoch_train_loss=0.04222173287183595
Epoch 1275
Epoch 1275 :: Batch 0/1
Batch Loss = 0.043371163404406665
1275, epoch_train_loss=0.043371163404406665
Epoch 1276
Epoch 1276 :: Batch 0/1
Batch Loss = 0.04304625212491126
1276, epoch_train_loss=0.04304625212491126
Epoch 1277
Epoch 1277 :: Batch 0/1
Batch Loss = 0.04224797372074584
1277, epoch_train_loss=0.04224797372074584
Epoch 1278
Epoch 1278 :: Batch 0/1
Batch Loss = 0.04243401688400922
1278, epoch_train_loss=0.04243401688400922
Epoch 1279
Epoch 1279 :: Batch 0/1
Batch Loss = 0.04248714610178916
1279, epoch_train_loss=0.04248714610178916
Epoch 1280
Epoch 1280 :: Batch 0/1
Batch Loss = 0.04215394617170494
1280, epoch_train_loss=0.04215394617170494
Epoch 1281
Epoch 1281 :: Batch 0/1
Batch Loss = 0.04203873790720291
1281, epoch_train_loss=0.04203873790720291
Epoch 1282
Epoch 1282 :: Batch 0/1
Batch Loss = 0.042487740454389904
1282, epoch_train_loss=0.042487740454389904
Epoch 1283
Epoch 1283 :: Batch 0/1
Batch Loss = 0.04169991916022716
1283, epoch_train_loss=0.04169991916022716
Epoch 1284
Epoch 1284 :: Batch 0/1
Batch Loss = 0.041928295799566834
1284, epoch_train_loss=0.041928295799566834
Epoch 1285
Epoch 1285 :: Batch 0/1
Batch Loss = 0.04176655514236814
1285, epoch_train_loss=0.04176655514236814
Epoch 1286
Epoch 1286 :: Batch 0/1
Batch Loss = 0.041850052737026126
1286, epoch_train_loss=0.041850052737026126
Epoch 1287
Epoch 1287 :: Batch 0/1
Batch Loss = 0.04124752268695648
1287, epoch_train_loss=0.04124752268695648
Epoch 1288
Epoch 1288 :: Batch 0/1
Batch Loss = 0.041539893930602624
1288, epoch_train_loss=0.041539893930602624
Epoch 1289
Epoch 1289 :: Batch 0/1
Batch Loss = 0.041182872900317996
1289, epoch_train_loss=0.041182872900317996
Epoch 1290
Epoch 1290 :: Batch 0/1
Batch Loss = 0.04107089505149279
1290, epoch_train_loss=0.04107089505149279
Epoch 1291
Epoch 1291 :: Batch 0/1
Batch Loss = 0.041165866416092135
1291, epoch_train_loss=0.041165866416092135
Epoch 1292
Epoch 1292 :: Batch 0/1
Batch Loss = 0.04100266595938025
1292, epoch_train_loss=0.04100266595938025
Epoch 1293
Epoch 1293 :: Batch 0/1
Batch Loss = 0.040924133518235574
1293, epoch_train_loss=0.040924133518235574
Epoch 1294
Epoch 1294 :: Batch 0/1
Batch Loss = 0.04101708325986956
1294, epoch_train_loss=0.04101708325986956
Epoch 1295
Epoch 1295 :: Batch 0/1
Batch Loss = 0.04077928705782844
1295, epoch_train_loss=0.04077928705782844
Epoch 1296
Epoch 1296 :: Batch 0/1
Batch Loss = 0.04089074851055689
1296, epoch_train_loss=0.04089074851055689
Epoch 1297
Epoch 1297 :: Batch 0/1
Batch Loss = 0.04080935455185459
1297, epoch_train_loss=0.04080935455185459
Epoch 1298
Epoch 1298 :: Batch 0/1
Batch Loss = 0.040620048868425115
1298, epoch_train_loss=0.040620048868425115
Epoch 1299
Epoch 1299 :: Batch 0/1
Batch Loss = 0.04069321673242678
1299, epoch_train_loss=0.04069321673242678
Epoch 1300
Epoch 1300 :: Batch 0/1
Batch Loss = 0.04049289085266354
1300, epoch_train_loss=0.04049289085266354
Epoch 1301
Epoch 1301 :: Batch 0/1
Batch Loss = 0.04050809188851799
1301, epoch_train_loss=0.04050809188851799
Epoch 1302
Epoch 1302 :: Batch 0/1
Batch Loss = 0.04042366762288017
1302, epoch_train_loss=0.04042366762288017
Epoch 1303
Epoch 1303 :: Batch 0/1
Batch Loss = 0.040361778601798175
1303, epoch_train_loss=0.040361778601798175
Epoch 1304
Epoch 1304 :: Batch 0/1
Batch Loss = 0.04033146975338252
1304, epoch_train_loss=0.04033146975338252
Epoch 1305
Epoch 1305 :: Batch 0/1
Batch Loss = 0.040281877940596504
1305, epoch_train_loss=0.040281877940596504
Epoch 1306
Epoch 1306 :: Batch 0/1
Batch Loss = 0.04022707646992101
1306, epoch_train_loss=0.04022707646992101
Epoch 1307
Epoch 1307 :: Batch 0/1
Batch Loss = 0.04021685460536974
1307, epoch_train_loss=0.04021685460536974
Epoch 1308
Epoch 1308 :: Batch 0/1
Batch Loss = 0.0400977442291412
1308, epoch_train_loss=0.0400977442291412
Epoch 1309
Epoch 1309 :: Batch 0/1
Batch Loss = 0.040101369940077736
1309, epoch_train_loss=0.040101369940077736
Epoch 1310
Epoch 1310 :: Batch 0/1
Batch Loss = 0.039987955660207825
1310, epoch_train_loss=0.039987955660207825
Epoch 1311
Epoch 1311 :: Batch 0/1
Batch Loss = 0.03998061966939375
1311, epoch_train_loss=0.03998061966939375
Epoch 1312
Epoch 1312 :: Batch 0/1
Batch Loss = 0.039914488397828005
1312, epoch_train_loss=0.039914488397828005
Epoch 1313
Epoch 1313 :: Batch 0/1
Batch Loss = 0.03987247209971629
1313, epoch_train_loss=0.03987247209971629
Epoch 1314
Epoch 1314 :: Batch 0/1
Batch Loss = 0.03984079948079992
1314, epoch_train_loss=0.03984079948079992
Epoch 1315
Epoch 1315 :: Batch 0/1
Batch Loss = 0.03978172034972048
1315, epoch_train_loss=0.03978172034972048
Epoch 1316
Epoch 1316 :: Batch 0/1
Batch Loss = 0.039755383776259305
1316, epoch_train_loss=0.039755383776259305
Epoch 1317
Epoch 1317 :: Batch 0/1
Batch Loss = 0.03970167264641065
1317, epoch_train_loss=0.03970167264641065
Epoch 1318
Epoch 1318 :: Batch 0/1
Batch Loss = 0.039655535947523614
1318, epoch_train_loss=0.039655535947523614
Epoch 1319
Epoch 1319 :: Batch 0/1
Batch Loss = 0.03961509666846661
1319, epoch_train_loss=0.03961509666846661
Epoch 1320
Epoch 1320 :: Batch 0/1
Batch Loss = 0.03956173875811671
1320, epoch_train_loss=0.03956173875811671
Epoch 1321
Epoch 1321 :: Batch 0/1
Batch Loss = 0.03953008357568119
1321, epoch_train_loss=0.03953008357568119
Epoch 1322
Epoch 1322 :: Batch 0/1
Batch Loss = 0.039484416001778815
1322, epoch_train_loss=0.039484416001778815
Epoch 1323
Epoch 1323 :: Batch 0/1
Batch Loss = 0.03944447144883339
1323, epoch_train_loss=0.03944447144883339
Epoch 1324
Epoch 1324 :: Batch 0/1
Batch Loss = 0.03940879901082099
1324, epoch_train_loss=0.03940879901082099
Epoch 1325
Epoch 1325 :: Batch 0/1
Batch Loss = 0.039359063285377534
1325, epoch_train_loss=0.039359063285377534
Epoch 1326
Epoch 1326 :: Batch 0/1
Batch Loss = 0.03932882330543874
1326, epoch_train_loss=0.03932882330543874
Epoch 1327
Epoch 1327 :: Batch 0/1
Batch Loss = 0.039278040513514446
1327, epoch_train_loss=0.039278040513514446
Epoch 1328
Epoch 1328 :: Batch 0/1
Batch Loss = 0.03924347581950586
1328, epoch_train_loss=0.03924347581950586
Epoch 1329
Epoch 1329 :: Batch 0/1
Batch Loss = 0.03919860344801895
1329, epoch_train_loss=0.03919860344801895
Epoch 1330
Epoch 1330 :: Batch 0/1
Batch Loss = 0.03915933118155867
1330, epoch_train_loss=0.03915933118155867
Epoch 1331
Epoch 1331 :: Batch 0/1
Batch Loss = 0.03912139562196537
1331, epoch_train_loss=0.03912139562196537
Epoch 1332
Epoch 1332 :: Batch 0/1
Batch Loss = 0.039081231777529445
1332, epoch_train_loss=0.039081231777529445
Epoch 1333
Epoch 1333 :: Batch 0/1
Batch Loss = 0.039042920612219305
1333, epoch_train_loss=0.039042920612219305
Epoch 1334
Epoch 1334 :: Batch 0/1
Batch Loss = 0.03900440097633965
1334, epoch_train_loss=0.03900440097633965
Epoch 1335
Epoch 1335 :: Batch 0/1
Batch Loss = 0.03896288311887529
1335, epoch_train_loss=0.03896288311887529
Epoch 1336
Epoch 1336 :: Batch 0/1
Batch Loss = 0.03892712121593779
1336, epoch_train_loss=0.03892712121593779
Epoch 1337
Epoch 1337 :: Batch 0/1
Batch Loss = 0.03888456790448307
1337, epoch_train_loss=0.03888456790448307
Epoch 1338
Epoch 1338 :: Batch 0/1
Batch Loss = 0.03884857944327777
1338, epoch_train_loss=0.03884857944327777
Epoch 1339
Epoch 1339 :: Batch 0/1
Batch Loss = 0.03880800642928834
1339, epoch_train_loss=0.03880800642928834
Epoch 1340
Epoch 1340 :: Batch 0/1
Batch Loss = 0.03877065544343335
1340, epoch_train_loss=0.03877065544343335
Epoch 1341
Epoch 1341 :: Batch 0/1
Batch Loss = 0.038733188162906884
1341, epoch_train_loss=0.038733188162906884
Epoch 1342
Epoch 1342 :: Batch 0/1
Batch Loss = 0.03869513212399631
1342, epoch_train_loss=0.03869513212399631
Epoch 1343
Epoch 1343 :: Batch 0/1
Batch Loss = 0.038658302853598246
1343, epoch_train_loss=0.038658302853598246
Epoch 1344
Epoch 1344 :: Batch 0/1
Batch Loss = 0.038620769080174935
1344, epoch_train_loss=0.038620769080174935
Epoch 1345
Epoch 1345 :: Batch 0/1
Batch Loss = 0.03858328781595205
1345, epoch_train_loss=0.03858328781595205
Epoch 1346
Epoch 1346 :: Batch 0/1
Batch Loss = 0.03854709972138722
1346, epoch_train_loss=0.03854709972138722
Epoch 1347
Epoch 1347 :: Batch 0/1
Batch Loss = 0.0385096049327475
1347, epoch_train_loss=0.0385096049327475
Epoch 1348
Epoch 1348 :: Batch 0/1
Batch Loss = 0.038473807033847
1348, epoch_train_loss=0.038473807033847
Epoch 1349
Epoch 1349 :: Batch 0/1
Batch Loss = 0.038437337875988666
1349, epoch_train_loss=0.038437337875988666
Epoch 1350
Epoch 1350 :: Batch 0/1
Batch Loss = 0.03840137585266657
1350, epoch_train_loss=0.03840137585266657
Epoch 1351
Epoch 1351 :: Batch 0/1
Batch Loss = 0.03836625396220254
1351, epoch_train_loss=0.03836625396220254
Epoch 1352
Epoch 1352 :: Batch 0/1
Batch Loss = 0.03833041930321305
1352, epoch_train_loss=0.03833041930321305
Epoch 1353
Epoch 1353 :: Batch 0/1
Batch Loss = 0.03829566331983145
1353, epoch_train_loss=0.03829566331983145
Epoch 1354
Epoch 1354 :: Batch 0/1
Batch Loss = 0.03826046632715406
1354, epoch_train_loss=0.03826046632715406
Epoch 1355
Epoch 1355 :: Batch 0/1
Batch Loss = 0.03822569343590285
1355, epoch_train_loss=0.03822569343590285
Epoch 1356
Epoch 1356 :: Batch 0/1
Batch Loss = 0.038191343411153864
1356, epoch_train_loss=0.038191343411153864
Epoch 1357
Epoch 1357 :: Batch 0/1
Batch Loss = 0.03815695860164624
1357, epoch_train_loss=0.03815695860164624
Epoch 1358
Epoch 1358 :: Batch 0/1
Batch Loss = 0.03812300822517051
1358, epoch_train_loss=0.03812300822517051
Epoch 1359
Epoch 1359 :: Batch 0/1
Batch Loss = 0.03808939478902058
1359, epoch_train_loss=0.03808939478902058
Epoch 1360
Epoch 1360 :: Batch 0/1
Batch Loss = 0.03805575161103304
1360, epoch_train_loss=0.03805575161103304
Epoch 1361
Epoch 1361 :: Batch 0/1
Batch Loss = 0.038022873546213046
1361, epoch_train_loss=0.038022873546213046
Epoch 1362
Epoch 1362 :: Batch 0/1
Batch Loss = 0.03798977542699641
1362, epoch_train_loss=0.03798977542699641
Epoch 1363
Epoch 1363 :: Batch 0/1
Batch Loss = 0.03795726386118261
1363, epoch_train_loss=0.03795726386118261
Epoch 1364
Epoch 1364 :: Batch 0/1
Batch Loss = 0.03792485095867595
1364, epoch_train_loss=0.03792485095867595
Epoch 1365
Epoch 1365 :: Batch 0/1
Batch Loss = 0.037892736042566384
1365, epoch_train_loss=0.037892736042566384
Epoch 1366
Epoch 1366 :: Batch 0/1
Batch Loss = 0.037860967128536704
1366, epoch_train_loss=0.037860967128536704
Epoch 1367
Epoch 1367 :: Batch 0/1
Batch Loss = 0.037829486946830373
1367, epoch_train_loss=0.037829486946830373
Epoch 1368
Epoch 1368 :: Batch 0/1
Batch Loss = 0.03779822176582873
1368, epoch_train_loss=0.03779822176582873
Epoch 1369
Epoch 1369 :: Batch 0/1
Batch Loss = 0.03776743693560782
1369, epoch_train_loss=0.03776743693560782
Epoch 1370
Epoch 1370 :: Batch 0/1
Batch Loss = 0.03773676762055087
1370, epoch_train_loss=0.03773676762055087
Epoch 1371
Epoch 1371 :: Batch 0/1
Batch Loss = 0.037706599457020526
1371, epoch_train_loss=0.037706599457020526
Epoch 1372
Epoch 1372 :: Batch 0/1
Batch Loss = 0.03767662763620718
1372, epoch_train_loss=0.03767662763620718
Epoch 1373
Epoch 1373 :: Batch 0/1
Batch Loss = 0.03764699115080638
1373, epoch_train_loss=0.03764699115080638
Epoch 1374
Epoch 1374 :: Batch 0/1
Batch Loss = 0.03761768010073791
1374, epoch_train_loss=0.03761768010073791
Epoch 1375
Epoch 1375 :: Batch 0/1
Batch Loss = 0.037588658443727684
1375, epoch_train_loss=0.037588658443727684
Epoch 1376
Epoch 1376 :: Batch 0/1
Batch Loss = 0.0375599461296542
1376, epoch_train_loss=0.0375599461296542
Epoch 1377
Epoch 1377 :: Batch 0/1
Batch Loss = 0.037531576021397506
1377, epoch_train_loss=0.037531576021397506
Epoch 1378
Epoch 1378 :: Batch 0/1
Batch Loss = 0.03750342804289183
1378, epoch_train_loss=0.03750342804289183
Epoch 1379
Epoch 1379 :: Batch 0/1
Batch Loss = 0.037475635662813034
1379, epoch_train_loss=0.037475635662813034
Epoch 1380
Epoch 1380 :: Batch 0/1
Batch Loss = 0.037448072587990315
1380, epoch_train_loss=0.037448072587990315
Epoch 1381
Epoch 1381 :: Batch 0/1
Batch Loss = 0.03742079041167207
1381, epoch_train_loss=0.03742079041167207
Epoch 1382
Epoch 1382 :: Batch 0/1
Batch Loss = 0.037393766646642165
1382, epoch_train_loss=0.037393766646642165
Epoch 1383
Epoch 1383 :: Batch 0/1
Batch Loss = 0.037366956317053654
1383, epoch_train_loss=0.037366956317053654
Epoch 1384
Epoch 1384 :: Batch 0/1
Batch Loss = 0.037340382608300735
1384, epoch_train_loss=0.037340382608300735
Epoch 1385
Epoch 1385 :: Batch 0/1
Batch Loss = 0.037314027671302534
1385, epoch_train_loss=0.037314027671302534
Epoch 1386
Epoch 1386 :: Batch 0/1
Batch Loss = 0.03728785920869741
1386, epoch_train_loss=0.03728785920869741
Epoch 1387
Epoch 1387 :: Batch 0/1
Batch Loss = 0.037261904572964154
1387, epoch_train_loss=0.037261904572964154
Epoch 1388
Epoch 1388 :: Batch 0/1
Batch Loss = 0.03723611340202823
1388, epoch_train_loss=0.03723611340202823
Epoch 1389
Epoch 1389 :: Batch 0/1
Batch Loss = 0.03721049693000068
1389, epoch_train_loss=0.03721049693000068
Epoch 1390
Epoch 1390 :: Batch 0/1
Batch Loss = 0.037185052711376725
1390, epoch_train_loss=0.037185052711376725
Epoch 1391
Epoch 1391 :: Batch 0/1
Batch Loss = 0.037159752130678694
1391, epoch_train_loss=0.037159752130678694
Epoch 1392
Epoch 1392 :: Batch 0/1
Batch Loss = 0.0371346124314862
1392, epoch_train_loss=0.0371346124314862
Epoch 1393
Epoch 1393 :: Batch 0/1
Batch Loss = 0.03710961454651475
1393, epoch_train_loss=0.03710961454651475
Epoch 1394
Epoch 1394 :: Batch 0/1
Batch Loss = 0.037084756915901844
1394, epoch_train_loss=0.037084756915901844
Epoch 1395
Epoch 1395 :: Batch 0/1
Batch Loss = 0.037060049159341434
1395, epoch_train_loss=0.037060049159341434
Epoch 1396
Epoch 1396 :: Batch 0/1
Batch Loss = 0.037035479769763444
1396, epoch_train_loss=0.037035479769763444
Epoch 1397
Epoch 1397 :: Batch 0/1
Batch Loss = 0.03701105258985017
1397, epoch_train_loss=0.03701105258985017
Epoch 1398
Epoch 1398 :: Batch 0/1
Batch Loss = 0.036986772930349784
1398, epoch_train_loss=0.036986772930349784
Epoch 1399
Epoch 1399 :: Batch 0/1
Batch Loss = 0.0369626288958221
1399, epoch_train_loss=0.0369626288958221
Epoch 1400
Epoch 1400 :: Batch 0/1
Batch Loss = 0.03693863461305788
1400, epoch_train_loss=0.03693863461305788
Epoch 1401
Epoch 1401 :: Batch 0/1
Batch Loss = 0.03691478109937342
1401, epoch_train_loss=0.03691478109937342
Epoch 1402
Epoch 1402 :: Batch 0/1
Batch Loss = 0.03689107032000024
1402, epoch_train_loss=0.03689107032000024
Epoch 1403
Epoch 1403 :: Batch 0/1
Batch Loss = 0.036867503309685086
1403, epoch_train_loss=0.036867503309685086
Epoch 1404
Epoch 1404 :: Batch 0/1
Batch Loss = 0.03684407627272162
1404, epoch_train_loss=0.03684407627272162
Epoch 1405
Epoch 1405 :: Batch 0/1
Batch Loss = 0.036820787351813086
1405, epoch_train_loss=0.036820787351813086
Epoch 1406
Epoch 1406 :: Batch 0/1
Batch Loss = 0.03679763811633278
1406, epoch_train_loss=0.03679763811633278
Epoch 1407
Epoch 1407 :: Batch 0/1
Batch Loss = 0.036774618136765184
1407, epoch_train_loss=0.036774618136765184
Epoch 1408
Epoch 1408 :: Batch 0/1
Batch Loss = 0.03675172964225189
1408, epoch_train_loss=0.03675172964225189
Epoch 1409
Epoch 1409 :: Batch 0/1
Batch Loss = 0.03672896317971771
1409, epoch_train_loss=0.03672896317971771
Epoch 1410
Epoch 1410 :: Batch 0/1
Batch Loss = 0.03670631417140648
1410, epoch_train_loss=0.03670631417140648
Epoch 1411
Epoch 1411 :: Batch 0/1
Batch Loss = 0.036683776103956706
1411, epoch_train_loss=0.036683776103956706
Epoch 1412
Epoch 1412 :: Batch 0/1
Batch Loss = 0.03666134039367118
1412, epoch_train_loss=0.03666134039367118
Epoch 1413
Epoch 1413 :: Batch 0/1
Batch Loss = 0.036638997131524055
1413, epoch_train_loss=0.036638997131524055
Epoch 1414
Epoch 1414 :: Batch 0/1
Batch Loss = 0.03661673865647605
1414, epoch_train_loss=0.03661673865647605
Epoch 1415
Epoch 1415 :: Batch 0/1
Batch Loss = 0.036594550055387746
1415, epoch_train_loss=0.036594550055387746
Epoch 1416
Epoch 1416 :: Batch 0/1
Batch Loss = 0.03657242103202311
1416, epoch_train_loss=0.03657242103202311
Epoch 1417
Epoch 1417 :: Batch 0/1
Batch Loss = 0.03655033459466992
1417, epoch_train_loss=0.03655033459466992
Epoch 1418
Epoch 1418 :: Batch 0/1
Batch Loss = 0.03652827386554477
1418, epoch_train_loss=0.03652827386554477
Epoch 1419
Epoch 1419 :: Batch 0/1
Batch Loss = 0.036506219201677674
1419, epoch_train_loss=0.036506219201677674
Epoch 1420
Epoch 1420 :: Batch 0/1
Batch Loss = 0.03648414797491502
1420, epoch_train_loss=0.03648414797491502
Epoch 1421
Epoch 1421 :: Batch 0/1
Batch Loss = 0.036462033818173256
1421, epoch_train_loss=0.036462033818173256
Epoch 1422
Epoch 1422 :: Batch 0/1
Batch Loss = 0.03643984929718714
1422, epoch_train_loss=0.03643984929718714
Epoch 1423
Epoch 1423 :: Batch 0/1
Batch Loss = 0.036417561369515745
1423, epoch_train_loss=0.036417561369515745
Epoch 1424
Epoch 1424 :: Batch 0/1
Batch Loss = 0.03639513757851264
1424, epoch_train_loss=0.03639513757851264
Epoch 1425
Epoch 1425 :: Batch 0/1
Batch Loss = 0.03637254195705777
1425, epoch_train_loss=0.03637254195705777
Epoch 1426
Epoch 1426 :: Batch 0/1
Batch Loss = 0.036349739932863175
1426, epoch_train_loss=0.036349739932863175
Epoch 1427
Epoch 1427 :: Batch 0/1
Batch Loss = 0.03632669993832249
1427, epoch_train_loss=0.03632669993832249
Epoch 1428
Epoch 1428 :: Batch 0/1
Batch Loss = 0.03630339638057375
1428, epoch_train_loss=0.03630339638057375
Epoch 1429
Epoch 1429 :: Batch 0/1
Batch Loss = 0.03627981223134748
1429, epoch_train_loss=0.03627981223134748
Epoch 1430
Epoch 1430 :: Batch 0/1
Batch Loss = 0.0362559428300785
1430, epoch_train_loss=0.0362559428300785
Epoch 1431
Epoch 1431 :: Batch 0/1
Batch Loss = 0.03623179668623913
1431, epoch_train_loss=0.03623179668623913
Epoch 1432
Epoch 1432 :: Batch 0/1
Batch Loss = 0.036207399630145604
1432, epoch_train_loss=0.036207399630145604
Epoch 1433
Epoch 1433 :: Batch 0/1
Batch Loss = 0.03618279167859681
1433, epoch_train_loss=0.03618279167859681
Epoch 1434
Epoch 1434 :: Batch 0/1
Batch Loss = 0.036158026805079534
1434, epoch_train_loss=0.036158026805079534
Epoch 1435
Epoch 1435 :: Batch 0/1
Batch Loss = 0.03613316875233402
1435, epoch_train_loss=0.03613316875233402
Epoch 1436
Epoch 1436 :: Batch 0/1
Batch Loss = 0.03610828879047263
1436, epoch_train_loss=0.03610828879047263
Epoch 1437
Epoch 1437 :: Batch 0/1
Batch Loss = 0.03608346219873008
1437, epoch_train_loss=0.03608346219873008
Epoch 1438
Epoch 1438 :: Batch 0/1
Batch Loss = 0.03605876286523458
1438, epoch_train_loss=0.03605876286523458
Epoch 1439
Epoch 1439 :: Batch 0/1
Batch Loss = 0.03603425907743177
1439, epoch_train_loss=0.03603425907743177
Epoch 1440
Epoch 1440 :: Batch 0/1
Batch Loss = 0.03601000933967767
1440, epoch_train_loss=0.03601000933967767
Epoch 1441
Epoch 1441 :: Batch 0/1
Batch Loss = 0.035986059889655884
1441, epoch_train_loss=0.035986059889655884
Epoch 1442
Epoch 1442 :: Batch 0/1
Batch Loss = 0.03596244238478714
1442, epoch_train_loss=0.03596244238478714
Epoch 1443
Epoch 1443 :: Batch 0/1
Batch Loss = 0.03593917294901389
1443, epoch_train_loss=0.03593917294901389
Epoch 1444
Epoch 1444 :: Batch 0/1
Batch Loss = 0.035916253818649324
1444, epoch_train_loss=0.035916253818649324
Epoch 1445
Epoch 1445 :: Batch 0/1
Batch Loss = 0.03589367533592282
1445, epoch_train_loss=0.03589367533592282
Epoch 1446
Epoch 1446 :: Batch 0/1
Batch Loss = 0.03587141932626576
1446, epoch_train_loss=0.03587141932626576
Epoch 1447
Epoch 1447 :: Batch 0/1
Batch Loss = 0.03584946130131727
1447, epoch_train_loss=0.03584946130131727
Epoch 1448
Epoch 1448 :: Batch 0/1
Batch Loss = 0.03582777354325046
1448, epoch_train_loss=0.03582777354325046
Epoch 1449
Epoch 1449 :: Batch 0/1
Batch Loss = 0.0358063267452778
1449, epoch_train_loss=0.0358063267452778
Epoch 1450
Epoch 1450 :: Batch 0/1
Batch Loss = 0.035785091733191436
1450, epoch_train_loss=0.035785091733191436
Epoch 1451
Epoch 1451 :: Batch 0/1
Batch Loss = 0.035764040449121126
1451, epoch_train_loss=0.035764040449121126
Epoch 1452
Epoch 1452 :: Batch 0/1
Batch Loss = 0.0357431468781585
1452, epoch_train_loss=0.0357431468781585
Epoch 1453
Epoch 1453 :: Batch 0/1
Batch Loss = 0.03572238775326655
1453, epoch_train_loss=0.03572238775326655
Epoch 1454
Epoch 1454 :: Batch 0/1
Batch Loss = 0.035701742754402375
1454, epoch_train_loss=0.035701742754402375
Epoch 1455
Epoch 1455 :: Batch 0/1
Batch Loss = 0.035681194843919344
1455, epoch_train_loss=0.035681194843919344
Epoch 1456
Epoch 1456 :: Batch 0/1
Batch Loss = 0.0356607299929818
1456, epoch_train_loss=0.0356607299929818
Epoch 1457
Epoch 1457 :: Batch 0/1
Batch Loss = 0.03564033706240901
1457, epoch_train_loss=0.03564033706240901
Epoch 1458
Epoch 1458 :: Batch 0/1
Batch Loss = 0.035620007212782724
1458, epoch_train_loss=0.035620007212782724
Epoch 1459
Epoch 1459 :: Batch 0/1
Batch Loss = 0.03559973350861912
1459, epoch_train_loss=0.03559973350861912
Epoch 1460
Epoch 1460 :: Batch 0/1
Batch Loss = 0.035579510366227834
1460, epoch_train_loss=0.035579510366227834
Epoch 1461
Epoch 1461 :: Batch 0/1
Batch Loss = 0.03555933310831858
1461, epoch_train_loss=0.03555933310831858
Epoch 1462
Epoch 1462 :: Batch 0/1
Batch Loss = 0.03553919754828951
1462, epoch_train_loss=0.03553919754828951
Epoch 1463
Epoch 1463 :: Batch 0/1
Batch Loss = 0.035519099528882186
1463, epoch_train_loss=0.035519099528882186
Epoch 1464
Epoch 1464 :: Batch 0/1
Batch Loss = 0.035499034501844176
1464, epoch_train_loss=0.035499034501844176
Epoch 1465
Epoch 1465 :: Batch 0/1
Batch Loss = 0.03547899706458419
1465, epoch_train_loss=0.03547899706458419
Epoch 1466
Epoch 1466 :: Batch 0/1
Batch Loss = 0.0354589805204547
1466, epoch_train_loss=0.0354589805204547
Epoch 1467
Epoch 1467 :: Batch 0/1
Batch Loss = 0.03543897644313818
1467, epoch_train_loss=0.03543897644313818
Epoch 1468
Epoch 1468 :: Batch 0/1
Batch Loss = 0.03541897410735686
1468, epoch_train_loss=0.03541897410735686
Epoch 1469
Epoch 1469 :: Batch 0/1
Batch Loss = 0.03539895994995935
1469, epoch_train_loss=0.03539895994995935
Epoch 1470
Epoch 1470 :: Batch 0/1
Batch Loss = 0.03537891683867076
1470, epoch_train_loss=0.03537891683867076
Epoch 1471
Epoch 1471 :: Batch 0/1
Batch Loss = 0.03535882329506061
1471, epoch_train_loss=0.03535882329506061
Epoch 1472
Epoch 1472 :: Batch 0/1
Batch Loss = 0.03533865230445314
1472, epoch_train_loss=0.03533865230445314
Epoch 1473
Epoch 1473 :: Batch 0/1
Batch Loss = 0.0353183697734111
1473, epoch_train_loss=0.0353183697734111
Epoch 1474
Epoch 1474 :: Batch 0/1
Batch Loss = 0.0352979323921502
1474, epoch_train_loss=0.0352979323921502
Epoch 1475
Epoch 1475 :: Batch 0/1
Batch Loss = 0.03527728479260499
1475, epoch_train_loss=0.03527728479260499
Epoch 1476
Epoch 1476 :: Batch 0/1
Batch Loss = 0.035256355644760436
1476, epoch_train_loss=0.035256355644760436
Epoch 1477
Epoch 1477 :: Batch 0/1
Batch Loss = 0.03523505237233676
1477, epoch_train_loss=0.03523505237233676
Epoch 1478
Epoch 1478 :: Batch 0/1
Batch Loss = 0.035213254239335516
1478, epoch_train_loss=0.035213254239335516
Epoch 1479
Epoch 1479 :: Batch 0/1
Batch Loss = 0.03519080388482647
1479, epoch_train_loss=0.03519080388482647
Epoch 1480
Epoch 1480 :: Batch 0/1
Batch Loss = 0.035167498014831274
1480, epoch_train_loss=0.035167498014831274
Epoch 1481
Epoch 1481 :: Batch 0/1
Batch Loss = 0.03514307984711548
1481, epoch_train_loss=0.03514307984711548
Epoch 1482
Epoch 1482 :: Batch 0/1
Batch Loss = 0.03511723909664818
1482, epoch_train_loss=0.03511723909664818
Epoch 1483
Epoch 1483 :: Batch 0/1
Batch Loss = 0.035089629945176386
1483, epoch_train_loss=0.035089629945176386
Epoch 1484
Epoch 1484 :: Batch 0/1
Batch Loss = 0.03505991932053477
1484, epoch_train_loss=0.03505991932053477
Epoch 1485
Epoch 1485 :: Batch 0/1
Batch Loss = 0.03502786811667808
1485, epoch_train_loss=0.03502786811667808
Epoch 1486
Epoch 1486 :: Batch 0/1
Batch Loss = 0.0349934185170605
1486, epoch_train_loss=0.0349934185170605
Epoch 1487
Epoch 1487 :: Batch 0/1
Batch Loss = 0.0349567286839744
1487, epoch_train_loss=0.0349567286839744
Epoch 1488
Epoch 1488 :: Batch 0/1
Batch Loss = 0.03491810780623566
1488, epoch_train_loss=0.03491810780623566
Epoch 1489
Epoch 1489 :: Batch 0/1
Batch Loss = 0.03487788002754381
1489, epoch_train_loss=0.03487788002754381
Epoch 1490
Epoch 1490 :: Batch 0/1
Batch Loss = 0.03483628943991152
1490, epoch_train_loss=0.03483628943991152
Epoch 1491
Epoch 1491 :: Batch 0/1
Batch Loss = 0.03479354344085406
1491, epoch_train_loss=0.03479354344085406
Epoch 1492
Epoch 1492 :: Batch 0/1
Batch Loss = 0.03474995708971779
1492, epoch_train_loss=0.03474995708971779
Epoch 1493
Epoch 1493 :: Batch 0/1
Batch Loss = 0.03470605521022397
1493, epoch_train_loss=0.03470605521022397
Epoch 1494
Epoch 1494 :: Batch 0/1
Batch Loss = 0.03466252091139759
1494, epoch_train_loss=0.03466252091139759
Epoch 1495
Epoch 1495 :: Batch 0/1
Batch Loss = 0.03462000452352485
1495, epoch_train_loss=0.03462000452352485
Epoch 1496
Epoch 1496 :: Batch 0/1
Batch Loss = 0.034578914845500235
1496, epoch_train_loss=0.034578914845500235
Epoch 1497
Epoch 1497 :: Batch 0/1
Batch Loss = 0.03453932405625618
1497, epoch_train_loss=0.03453932405625618
Epoch 1498
Epoch 1498 :: Batch 0/1
Batch Loss = 0.034501030252828684
1498, epoch_train_loss=0.034501030252828684
Epoch 1499
Epoch 1499 :: Batch 0/1
Batch Loss = 0.03446371397622994
1499, epoch_train_loss=0.03446371397622994
Epoch 1500
Epoch 1500 :: Batch 0/1
Batch Loss = 0.03442708226638911
1500, epoch_train_loss=0.03442708226638911
Epoch 1501
Epoch 1501 :: Batch 0/1
Batch Loss = 0.03439093291216047
1501, epoch_train_loss=0.03439093291216047
Epoch 1502
Epoch 1502 :: Batch 0/1
Batch Loss = 0.034355144518730715
1502, epoch_train_loss=0.034355144518730715
Epoch 1503
Epoch 1503 :: Batch 0/1
Batch Loss = 0.03431963950467525
1503, epoch_train_loss=0.03431963950467525
Epoch 1504
Epoch 1504 :: Batch 0/1
Batch Loss = 0.03428435803241308
1504, epoch_train_loss=0.03428435803241308
Epoch 1505
Epoch 1505 :: Batch 0/1
Batch Loss = 0.03424924970732012
1505, epoch_train_loss=0.03424924970732012
Epoch 1506
Epoch 1506 :: Batch 0/1
Batch Loss = 0.034214273391746826
1506, epoch_train_loss=0.034214273391746826
Epoch 1507
Epoch 1507 :: Batch 0/1
Batch Loss = 0.03417939827917611
1507, epoch_train_loss=0.03417939827917611
Epoch 1508
Epoch 1508 :: Batch 0/1
Batch Loss = 0.034144606325188585
1508, epoch_train_loss=0.034144606325188585
Epoch 1509
Epoch 1509 :: Batch 0/1
Batch Loss = 0.034109898833620846
1509, epoch_train_loss=0.034109898833620846
Epoch 1510
Epoch 1510 :: Batch 0/1
Batch Loss = 0.034075306598226676
1510, epoch_train_loss=0.034075306598226676
Epoch 1511
Epoch 1511 :: Batch 0/1
Batch Loss = 0.03404089748385808
1511, epoch_train_loss=0.03404089748385808
Epoch 1512
Epoch 1512 :: Batch 0/1
Batch Loss = 0.034006774122424206
1512, epoch_train_loss=0.034006774122424206
Epoch 1513
Epoch 1513 :: Batch 0/1
Batch Loss = 0.033973059448595286
1513, epoch_train_loss=0.033973059448595286
Epoch 1514
Epoch 1514 :: Batch 0/1
Batch Loss = 0.033939874808502433
1514, epoch_train_loss=0.033939874808502433
Epoch 1515
Epoch 1515 :: Batch 0/1
Batch Loss = 0.03390731943197082
1515, epoch_train_loss=0.03390731943197082
Epoch 1516
Epoch 1516 :: Batch 0/1
Batch Loss = 0.03387545896848896
1516, epoch_train_loss=0.03387545896848896
Epoch 1517
Epoch 1517 :: Batch 0/1
Batch Loss = 0.033844325538722214
1517, epoch_train_loss=0.033844325538722214
Epoch 1518
Epoch 1518 :: Batch 0/1
Batch Loss = 0.03381392545590324
1518, epoch_train_loss=0.03381392545590324
Epoch 1519
Epoch 1519 :: Batch 0/1
Batch Loss = 0.03378424761871614
1519, epoch_train_loss=0.03378424761871614
Epoch 1520
Epoch 1520 :: Batch 0/1
Batch Loss = 0.033755267453324855
1520, epoch_train_loss=0.033755267453324855
Epoch 1521
Epoch 1521 :: Batch 0/1
Batch Loss = 0.03372694648497012
1521, epoch_train_loss=0.03372694648497012
Epoch 1522
Epoch 1522 :: Batch 0/1
Batch Loss = 0.03369923180547961
1522, epoch_train_loss=0.03369923180547961
Epoch 1523
Epoch 1523 :: Batch 0/1
Batch Loss = 0.033672059538933596
1523, epoch_train_loss=0.033672059538933596
Epoch 1524
Epoch 1524 :: Batch 0/1
Batch Loss = 0.03364536266634593
1524, epoch_train_loss=0.03364536266634593
Epoch 1525
Epoch 1525 :: Batch 0/1
Batch Loss = 0.03361907976691077
1525, epoch_train_loss=0.03361907976691077
Epoch 1526
Epoch 1526 :: Batch 0/1
Batch Loss = 0.033593160283438846
1526, epoch_train_loss=0.033593160283438846
Epoch 1527
Epoch 1527 :: Batch 0/1
Batch Loss = 0.033567564396558364
1527, epoch_train_loss=0.033567564396558364
Epoch 1528
Epoch 1528 :: Batch 0/1
Batch Loss = 0.03354225909664951
1528, epoch_train_loss=0.03354225909664951
Epoch 1529
Epoch 1529 :: Batch 0/1
Batch Loss = 0.0335172138028379
1529, epoch_train_loss=0.0335172138028379
Epoch 1530
Epoch 1530 :: Batch 0/1
Batch Loss = 0.03349239813453719
1530, epoch_train_loss=0.03349239813453719
Epoch 1531
Epoch 1531 :: Batch 0/1
Batch Loss = 0.033467782217809314
1531, epoch_train_loss=0.033467782217809314
Epoch 1532
Epoch 1532 :: Batch 0/1
Batch Loss = 0.03344333814891202
1532, epoch_train_loss=0.03344333814891202
Epoch 1533
Epoch 1533 :: Batch 0/1
Batch Loss = 0.033419040886925946
1533, epoch_train_loss=0.033419040886925946
Epoch 1534
Epoch 1534 :: Batch 0/1
Batch Loss = 0.03339486777063617
1534, epoch_train_loss=0.03339486777063617
Epoch 1535
Epoch 1535 :: Batch 0/1
Batch Loss = 0.033370797199694324
1535, epoch_train_loss=0.033370797199694324
Epoch 1536
Epoch 1536 :: Batch 0/1
Batch Loss = 0.03334680759759707
1536, epoch_train_loss=0.03334680759759707
Epoch 1537
Epoch 1537 :: Batch 0/1
Batch Loss = 0.033322877511053034
1537, epoch_train_loss=0.033322877511053034
Epoch 1538
Epoch 1538 :: Batch 0/1
Batch Loss = 0.03329898679572996
1538, epoch_train_loss=0.03329898679572996
Epoch 1539
Epoch 1539 :: Batch 0/1
Batch Loss = 0.03327511807643531
1539, epoch_train_loss=0.03327511807643531
Epoch 1540
Epoch 1540 :: Batch 0/1
Batch Loss = 0.03325125758824814
1540, epoch_train_loss=0.03325125758824814
Epoch 1541
Epoch 1541 :: Batch 0/1
Batch Loss = 0.033227394934649074
1541, epoch_train_loss=0.033227394934649074
Epoch 1542
Epoch 1542 :: Batch 0/1
Batch Loss = 0.03320352195835948
1542, epoch_train_loss=0.03320352195835948
Epoch 1543
Epoch 1543 :: Batch 0/1
Batch Loss = 0.03317963131054797
1543, epoch_train_loss=0.03317963131054797
Epoch 1544
Epoch 1544 :: Batch 0/1
Batch Loss = 0.033155715246991915
1544, epoch_train_loss=0.033155715246991915
Epoch 1545
Epoch 1545 :: Batch 0/1
Batch Loss = 0.033131764902505745
1545, epoch_train_loss=0.033131764902505745
Epoch 1546
Epoch 1546 :: Batch 0/1
Batch Loss = 0.03310776998429229
1546, epoch_train_loss=0.03310776998429229
Epoch 1547
Epoch 1547 :: Batch 0/1
Batch Loss = 0.03308371860543259
1547, epoch_train_loss=0.03308371860543259
Epoch 1548
Epoch 1548 :: Batch 0/1
Batch Loss = 0.03305959702181731
1548, epoch_train_loss=0.03305959702181731
Epoch 1549
Epoch 1549 :: Batch 0/1
Batch Loss = 0.033035389221258
1549, epoch_train_loss=0.033035389221258
Epoch 1550
Epoch 1550 :: Batch 0/1
Batch Loss = 0.03301107646604222
1550, epoch_train_loss=0.03301107646604222
Epoch 1551
Epoch 1551 :: Batch 0/1
Batch Loss = 0.03298663699863536
1551, epoch_train_loss=0.03298663699863536
Epoch 1552
Epoch 1552 :: Batch 0/1
Batch Loss = 0.032962046034805216
1552, epoch_train_loss=0.032962046034805216
Epoch 1553
Epoch 1553 :: Batch 0/1
Batch Loss = 0.032937276073250636
1553, epoch_train_loss=0.032937276073250636
Epoch 1554
Epoch 1554 :: Batch 0/1
Batch Loss = 0.03291229737152557
1554, epoch_train_loss=0.03291229737152557
Epoch 1555
Epoch 1555 :: Batch 0/1
Batch Loss = 0.03288707847262463
1555, epoch_train_loss=0.03288707847262463
Epoch 1556
Epoch 1556 :: Batch 0/1
Batch Loss = 0.032861586726593936
1556, epoch_train_loss=0.032861586726593936
Epoch 1557
Epoch 1557 :: Batch 0/1
Batch Loss = 0.03283578884457363
1557, epoch_train_loss=0.03283578884457363
Epoch 1558
Epoch 1558 :: Batch 0/1
Batch Loss = 0.03280965158010988
1558, epoch_train_loss=0.03280965158010988
Epoch 1559
Epoch 1559 :: Batch 0/1
Batch Loss = 0.032783142707487105
1559, epoch_train_loss=0.032783142707487105
Epoch 1560
Epoch 1560 :: Batch 0/1
Batch Loss = 0.03275623236171515
1560, epoch_train_loss=0.03275623236171515
Epoch 1561
Epoch 1561 :: Batch 0/1
Batch Loss = 0.03272889479761046
1561, epoch_train_loss=0.03272889479761046
Epoch 1562
Epoch 1562 :: Batch 0/1
Batch Loss = 0.03270111051433569
1562, epoch_train_loss=0.03270111051433569
Epoch 1563
Epoch 1563 :: Batch 0/1
Batch Loss = 0.03267286868797999
1563, epoch_train_loss=0.03267286868797999
Epoch 1564
Epoch 1564 :: Batch 0/1
Batch Loss = 0.032644169758833504
1564, epoch_train_loss=0.032644169758833504
Epoch 1565
Epoch 1565 :: Batch 0/1
Batch Loss = 0.03261502800382901
1565, epoch_train_loss=0.03261502800382901
Epoch 1566
Epoch 1566 :: Batch 0/1
Batch Loss = 0.0325854737237741
1566, epoch_train_loss=0.0325854737237741
Epoch 1567
Epoch 1567 :: Batch 0/1
Batch Loss = 0.032555554615314015
1567, epoch_train_loss=0.032555554615314015
Epoch 1568
Epoch 1568 :: Batch 0/1
Batch Loss = 0.03252533577084476
1568, epoch_train_loss=0.03252533577084476
Epoch 1569
Epoch 1569 :: Batch 0/1
Batch Loss = 0.03249489784974468
1569, epoch_train_loss=0.03249489784974468
Epoch 1570
Epoch 1570 :: Batch 0/1
Batch Loss = 0.032464333286664614
1570, epoch_train_loss=0.032464333286664614
Epoch 1571
Epoch 1571 :: Batch 0/1
Batch Loss = 0.03243374084795356
1571, epoch_train_loss=0.03243374084795356
Epoch 1572
Epoch 1572 :: Batch 0/1
Batch Loss = 0.03240321930445347
1572, epoch_train_loss=0.03240321930445347
Epoch 1573
Epoch 1573 :: Batch 0/1
Batch Loss = 0.03237286122751446
1573, epoch_train_loss=0.03237286122751446
Epoch 1574
Epoch 1574 :: Batch 0/1
Batch Loss = 0.03234274777682914
1574, epoch_train_loss=0.03234274777682914
Epoch 1575
Epoch 1575 :: Batch 0/1
Batch Loss = 0.0323129449853043
1575, epoch_train_loss=0.0323129449853043
Epoch 1576
Epoch 1576 :: Batch 0/1
Batch Loss = 0.032283501689469814
1576, epoch_train_loss=0.032283501689469814
Epoch 1577
Epoch 1577 :: Batch 0/1
Batch Loss = 0.03225444905200127
1577, epoch_train_loss=0.03225444905200127
Epoch 1578
Epoch 1578 :: Batch 0/1
Batch Loss = 0.03222580159427517
1578, epoch_train_loss=0.03222580159427517
Epoch 1579
Epoch 1579 :: Batch 0/1
Batch Loss = 0.032197559625331953
1579, epoch_train_loss=0.032197559625331953
Epoch 1580
Epoch 1580 :: Batch 0/1
Batch Loss = 0.032169712706258555
1580, epoch_train_loss=0.032169712706258555
Epoch 1581
Epoch 1581 :: Batch 0/1
Batch Loss = 0.032142243566706585
1581, epoch_train_loss=0.032142243566706585
Epoch 1582
Epoch 1582 :: Batch 0/1
Batch Loss = 0.03211513170908042
1582, epoch_train_loss=0.03211513170908042
Epoch 1583
Epoch 1583 :: Batch 0/1
Batch Loss = 0.03208835604856544
1583, epoch_train_loss=0.03208835604856544
Epoch 1584
Epoch 1584 :: Batch 0/1
Batch Loss = 0.03206189637486947
1584, epoch_train_loss=0.03206189637486947
Epoch 1585
Epoch 1585 :: Batch 0/1
Batch Loss = 0.032035733795148034
1585, epoch_train_loss=0.032035733795148034
Epoch 1586
Epoch 1586 :: Batch 0/1
Batch Loss = 0.032009850609874164
1586, epoch_train_loss=0.032009850609874164
Epoch 1587
Epoch 1587 :: Batch 0/1
Batch Loss = 0.03198423004145621
1587, epoch_train_loss=0.03198423004145621
Epoch 1588
Epoch 1588 :: Batch 0/1
Batch Loss = 0.03195885607595215
1588, epoch_train_loss=0.03195885607595215
Epoch 1589
Epoch 1589 :: Batch 0/1
Batch Loss = 0.03193371342526619
1589, epoch_train_loss=0.03193371342526619
Epoch 1590
Epoch 1590 :: Batch 0/1
Batch Loss = 0.03190878752577714
1590, epoch_train_loss=0.03190878752577714
Epoch 1591
Epoch 1591 :: Batch 0/1
Batch Loss = 0.03188406451636689
1591, epoch_train_loss=0.03188406451636689
Epoch 1592
Epoch 1592 :: Batch 0/1
Batch Loss = 0.03185953116989736
1592, epoch_train_loss=0.03185953116989736
Epoch 1593
Epoch 1593 :: Batch 0/1
Batch Loss = 0.031835174838407476
1593, epoch_train_loss=0.031835174838407476
Epoch 1594
Epoch 1594 :: Batch 0/1
Batch Loss = 0.03181098344779007
1594, epoch_train_loss=0.03181098344779007
Epoch 1595
Epoch 1595 :: Batch 0/1
Batch Loss = 0.03178694556559976
1595, epoch_train_loss=0.03178694556559976
Epoch 1596
Epoch 1596 :: Batch 0/1
Batch Loss = 0.031763050482207934
1596, epoch_train_loss=0.031763050482207934
Epoch 1597
Epoch 1597 :: Batch 0/1
Batch Loss = 0.03173928826211768
1597, epoch_train_loss=0.03173928826211768
Epoch 1598
Epoch 1598 :: Batch 0/1
Batch Loss = 0.03171564973411088
1598, epoch_train_loss=0.03171564973411088
Epoch 1599
Epoch 1599 :: Batch 0/1
Batch Loss = 0.03169212643074704
1599, epoch_train_loss=0.03169212643074704
Epoch 1600
Epoch 1600 :: Batch 0/1
Batch Loss = 0.03166871051714302
1600, epoch_train_loss=0.03166871051714302
Epoch 1601
Epoch 1601 :: Batch 0/1
Batch Loss = 0.03164539475227705
1601, epoch_train_loss=0.03164539475227705
Epoch 1602
Epoch 1602 :: Batch 0/1
Batch Loss = 0.031622172505966925
1602, epoch_train_loss=0.031622172505966925
Epoch 1603
Epoch 1603 :: Batch 0/1
Batch Loss = 0.03159903782666457
1603, epoch_train_loss=0.03159903782666457
Epoch 1604
Epoch 1604 :: Batch 0/1
Batch Loss = 0.031575985535150286
1604, epoch_train_loss=0.031575985535150286
Epoch 1605
Epoch 1605 :: Batch 0/1
Batch Loss = 0.031553011314665154
1605, epoch_train_loss=0.031553011314665154
Epoch 1606
Epoch 1606 :: Batch 0/1
Batch Loss = 0.03153011177629579
1606, epoch_train_loss=0.03153011177629579
Epoch 1607
Epoch 1607 :: Batch 0/1
Batch Loss = 0.0315072844907399
1607, epoch_train_loss=0.0315072844907399
Epoch 1608
Epoch 1608 :: Batch 0/1
Batch Loss = 0.03148452798445167
1608, epoch_train_loss=0.03148452798445167
Epoch 1609
Epoch 1609 :: Batch 0/1
Batch Loss = 0.03146184171197792
1609, epoch_train_loss=0.03146184171197792
Epoch 1610
Epoch 1610 :: Batch 0/1
Batch Loss = 0.031439225988406046
1610, epoch_train_loss=0.031439225988406046
Epoch 1611
Epoch 1611 :: Batch 0/1
Batch Loss = 0.031416681901274085
1611, epoch_train_loss=0.031416681901274085
Epoch 1612
Epoch 1612 :: Batch 0/1
Batch Loss = 0.03139421119384836
1612, epoch_train_loss=0.03139421119384836
Epoch 1613
Epoch 1613 :: Batch 0/1
Batch Loss = 0.03137181613745975
1613, epoch_train_loss=0.03137181613745975
Epoch 1614
Epoch 1614 :: Batch 0/1
Batch Loss = 0.03134949938411515
1614, epoch_train_loss=0.03134949938411515
Epoch 1615
Epoch 1615 :: Batch 0/1
Batch Loss = 0.03132726383843369
1615, epoch_train_loss=0.03132726383843369
Epoch 1616
Epoch 1616 :: Batch 0/1
Batch Loss = 0.031305112532388234
1616, epoch_train_loss=0.031305112532388234
Epoch 1617
Epoch 1617 :: Batch 0/1
Batch Loss = 0.03128304852853518
1617, epoch_train_loss=0.03128304852853518
Epoch 1618
Epoch 1618 :: Batch 0/1
Batch Loss = 0.031261074844021026
1618, epoch_train_loss=0.031261074844021026
Epoch 1619
Epoch 1619 :: Batch 0/1
Batch Loss = 0.031239194393063094
1619, epoch_train_loss=0.031239194393063094
Epoch 1620
Epoch 1620 :: Batch 0/1
Batch Loss = 0.031217409951325843
1620, epoch_train_loss=0.031217409951325843
Epoch 1621
Epoch 1621 :: Batch 0/1
Batch Loss = 0.03119572413040866
1621, epoch_train_loss=0.03119572413040866
Epoch 1622
Epoch 1622 :: Batch 0/1
Batch Loss = 0.03117413935878382
1622, epoch_train_loss=0.03117413935878382
Epoch 1623
Epoch 1623 :: Batch 0/1
Batch Loss = 0.0311526578847988
1623, epoch_train_loss=0.0311526578847988
Epoch 1624
Epoch 1624 :: Batch 0/1
Batch Loss = 0.031131281773832068
1624, epoch_train_loss=0.031131281773832068
Epoch 1625
Epoch 1625 :: Batch 0/1
Batch Loss = 0.031110012924304327
1625, epoch_train_loss=0.031110012924304327
Epoch 1626
Epoch 1626 :: Batch 0/1
Batch Loss = 0.031088853071658606
1626, epoch_train_loss=0.031088853071658606
Epoch 1627
Epoch 1627 :: Batch 0/1
Batch Loss = 0.03106780380418123
1627, epoch_train_loss=0.03106780380418123
Epoch 1628
Epoch 1628 :: Batch 0/1
Batch Loss = 0.031046866561081833
1628, epoch_train_loss=0.031046866561081833
Epoch 1629
Epoch 1629 :: Batch 0/1
Batch Loss = 0.031026042637099693
1629, epoch_train_loss=0.031026042637099693
Epoch 1630
Epoch 1630 :: Batch 0/1
Batch Loss = 0.031005333175308965
1630, epoch_train_loss=0.031005333175308965
Epoch 1631
Epoch 1631 :: Batch 0/1
Batch Loss = 0.030984739159254034
1631, epoch_train_loss=0.030984739159254034
Epoch 1632
Epoch 1632 :: Batch 0/1
Batch Loss = 0.03096426139583018
1632, epoch_train_loss=0.03096426139583018
Epoch 1633
Epoch 1633 :: Batch 0/1
Batch Loss = 0.030943900506543724
1633, epoch_train_loss=0.030943900506543724
Epoch 1634
Epoch 1634 :: Batch 0/1
Batch Loss = 0.030923656911647664
1634, epoch_train_loss=0.030923656911647664
Epoch 1635
Epoch 1635 :: Batch 0/1
Batch Loss = 0.03090353081795674
1635, epoch_train_loss=0.03090353081795674
Epoch 1636
Epoch 1636 :: Batch 0/1
Batch Loss = 0.03088352220172988
1636, epoch_train_loss=0.03088352220172988
Epoch 1637
Epoch 1637 :: Batch 0/1
Batch Loss = 0.030863630803914025
1637, epoch_train_loss=0.030863630803914025
Epoch 1638
Epoch 1638 :: Batch 0/1
Batch Loss = 0.03084385612279955
1638, epoch_train_loss=0.03084385612279955
Epoch 1639
Epoch 1639 :: Batch 0/1
Batch Loss = 0.03082419741295522
1639, epoch_train_loss=0.03082419741295522
Epoch 1640
Epoch 1640 :: Batch 0/1
Batch Loss = 0.030804653688973442
1640, epoch_train_loss=0.030804653688973442
Epoch 1641
Epoch 1641 :: Batch 0/1
Batch Loss = 0.030785223735033902
1641, epoch_train_loss=0.030785223735033902
Epoch 1642
Epoch 1642 :: Batch 0/1
Batch Loss = 0.030765906111471127
1642, epoch_train_loss=0.030765906111471127
Epoch 1643
Epoch 1643 :: Batch 0/1
Batch Loss = 0.030746699171015846
1643, epoch_train_loss=0.030746699171015846
Epoch 1644
Epoch 1644 :: Batch 0/1
Batch Loss = 0.03072760107548181
1644, epoch_train_loss=0.03072760107548181
Epoch 1645
Epoch 1645 :: Batch 0/1
Batch Loss = 0.030708609805719284
1645, epoch_train_loss=0.030708609805719284
Epoch 1646
Epoch 1646 :: Batch 0/1
Batch Loss = 0.030689723170168003
1646, epoch_train_loss=0.030689723170168003
Epoch 1647
Epoch 1647 :: Batch 0/1
Batch Loss = 0.03067093881720254
1647, epoch_train_loss=0.03067093881720254
Epoch 1648
Epoch 1648 :: Batch 0/1
Batch Loss = 0.030652254244361205
1648, epoch_train_loss=0.030652254244361205
Epoch 1649
Epoch 1649 :: Batch 0/1
Batch Loss = 0.030633666796346465
1649, epoch_train_loss=0.030633666796346465
Epoch 1650
Epoch 1650 :: Batch 0/1
Batch Loss = 0.03061517366462124
1650, epoch_train_loss=0.03061517366462124
Epoch 1651
Epoch 1651 :: Batch 0/1
Batch Loss = 0.030596771878927037
1651, epoch_train_loss=0.030596771878927037
Epoch 1652
Epoch 1652 :: Batch 0/1
Batch Loss = 0.030578458300359372
1652, epoch_train_loss=0.030578458300359372
Epoch 1653
Epoch 1653 :: Batch 0/1
Batch Loss = 0.03056022959171359
1653, epoch_train_loss=0.03056022959171359
Epoch 1654
Epoch 1654 :: Batch 0/1
Batch Loss = 0.030542082200565344
1654, epoch_train_loss=0.030542082200565344
Epoch 1655
Epoch 1655 :: Batch 0/1
Batch Loss = 0.030524012316624865
1655, epoch_train_loss=0.030524012316624865
Epoch 1656
Epoch 1656 :: Batch 0/1
Batch Loss = 0.030506015823981093
1656, epoch_train_loss=0.030506015823981093
Epoch 1657
Epoch 1657 :: Batch 0/1
Batch Loss = 0.030488088233937966
1657, epoch_train_loss=0.030488088233937966
Epoch 1658
Epoch 1658 :: Batch 0/1
Batch Loss = 0.03047022460124805
1658, epoch_train_loss=0.03047022460124805
Epoch 1659
Epoch 1659 :: Batch 0/1
Batch Loss = 0.030452419401244123
1659, epoch_train_loss=0.030452419401244123
Epoch 1660
Epoch 1660 :: Batch 0/1
Batch Loss = 0.030434666369262897
1660, epoch_train_loss=0.030434666369262897
Epoch 1661
Epoch 1661 :: Batch 0/1
Batch Loss = 0.030416958285737334
1661, epoch_train_loss=0.030416958285737334
Epoch 1662
Epoch 1662 :: Batch 0/1
Batch Loss = 0.03039928666868815
1662, epoch_train_loss=0.03039928666868815
Epoch 1663
Epoch 1663 :: Batch 0/1
Batch Loss = 0.03038164135905613
1663, epoch_train_loss=0.03038164135905613
Epoch 1664
Epoch 1664 :: Batch 0/1
Batch Loss = 0.030364009923471894
1664, epoch_train_loss=0.030364009923471894
Epoch 1665
Epoch 1665 :: Batch 0/1
Batch Loss = 0.030346376805059916
1665, epoch_train_loss=0.030346376805059916
Epoch 1666
Epoch 1666 :: Batch 0/1
Batch Loss = 0.030328722089371176
1666, epoch_train_loss=0.030328722089371176
Epoch 1667
Epoch 1667 :: Batch 0/1
Batch Loss = 0.03031101966841825
1667, epoch_train_loss=0.03031101966841825
Epoch 1668
Epoch 1668 :: Batch 0/1
Batch Loss = 0.03029323445350908
1668, epoch_train_loss=0.03029323445350908
Epoch 1669
Epoch 1669 :: Batch 0/1
Batch Loss = 0.03027531806469819
1669, epoch_train_loss=0.03027531806469819
Epoch 1670
Epoch 1670 :: Batch 0/1
Batch Loss = 0.030257201962552834
1670, epoch_train_loss=0.030257201962552834
Epoch 1671
Epoch 1671 :: Batch 0/1
Batch Loss = 0.030238786248818043
1671, epoch_train_loss=0.030238786248818043
Epoch 1672
Epoch 1672 :: Batch 0/1
Batch Loss = 0.03021992086560349
1672, epoch_train_loss=0.03021992086560349
Epoch 1673
Epoch 1673 :: Batch 0/1
Batch Loss = 0.03020037316577905
1673, epoch_train_loss=0.03020037316577905
Epoch 1674
Epoch 1674 :: Batch 0/1
Batch Loss = 0.030179770602225868
1674, epoch_train_loss=0.030179770602225868
Epoch 1675
Epoch 1675 :: Batch 0/1
Batch Loss = 0.03015749773036349
1675, epoch_train_loss=0.03015749773036349
Epoch 1676
Epoch 1676 :: Batch 0/1
Batch Loss = 0.0301325119569128
1676, epoch_train_loss=0.0301325119569128
Epoch 1677
Epoch 1677 :: Batch 0/1
Batch Loss = 0.030103032260211514
1677, epoch_train_loss=0.030103032260211514
Epoch 1678
Epoch 1678 :: Batch 0/1
Batch Loss = 0.030066118030081967
1678, epoch_train_loss=0.030066118030081967
Epoch 1679
Epoch 1679 :: Batch 0/1
Batch Loss = 0.030017570141101787
1679, epoch_train_loss=0.030017570141101787
Epoch 1680
Epoch 1680 :: Batch 0/1
Batch Loss = 0.02995390080233216
1680, epoch_train_loss=0.02995390080233216
Epoch 1681
Epoch 1681 :: Batch 0/1
Batch Loss = 0.029878551359939217
1681, epoch_train_loss=0.029878551359939217
Epoch 1682
Epoch 1682 :: Batch 0/1
Batch Loss = 0.029803070888454478
1682, epoch_train_loss=0.029803070888454478
Epoch 1683
Epoch 1683 :: Batch 0/1
Batch Loss = 0.029725690475746615
1683, epoch_train_loss=0.029725690475746615
Epoch 1684
Epoch 1684 :: Batch 0/1
Batch Loss = 0.02963325973878528
1684, epoch_train_loss=0.02963325973878528
Epoch 1685
Epoch 1685 :: Batch 0/1
Batch Loss = 0.029542442640504363
1685, epoch_train_loss=0.029542442640504363
Epoch 1686
Epoch 1686 :: Batch 0/1
Batch Loss = 0.029474488392979555
1686, epoch_train_loss=0.029474488392979555
Epoch 1687
Epoch 1687 :: Batch 0/1
Batch Loss = 0.029415902204690307
1687, epoch_train_loss=0.029415902204690307
Epoch 1688
Epoch 1688 :: Batch 0/1
Batch Loss = 0.029357508169173308
1688, epoch_train_loss=0.029357508169173308
Epoch 1689
Epoch 1689 :: Batch 0/1
Batch Loss = 0.02931636027519106
1689, epoch_train_loss=0.02931636027519106
Epoch 1690
Epoch 1690 :: Batch 0/1
Batch Loss = 0.029292039650245422
1690, epoch_train_loss=0.029292039650245422
Epoch 1691
Epoch 1691 :: Batch 0/1
Batch Loss = 0.02926840102129514
1691, epoch_train_loss=0.02926840102129514
Epoch 1692
Epoch 1692 :: Batch 0/1
Batch Loss = 0.029248083876695884
1692, epoch_train_loss=0.029248083876695884
Epoch 1693
Epoch 1693 :: Batch 0/1
Batch Loss = 0.029236102522553636
1693, epoch_train_loss=0.029236102522553636
Epoch 1694
Epoch 1694 :: Batch 0/1
Batch Loss = 0.02922134415351493
1694, epoch_train_loss=0.02922134415351493
Epoch 1695
Epoch 1695 :: Batch 0/1
Batch Loss = 0.029199207444091286
1695, epoch_train_loss=0.029199207444091286
Epoch 1696
Epoch 1696 :: Batch 0/1
Batch Loss = 0.0291779823253653
1696, epoch_train_loss=0.0291779823253653
Epoch 1697
Epoch 1697 :: Batch 0/1
Batch Loss = 0.02915749555463059
1697, epoch_train_loss=0.02915749555463059
Epoch 1698
Epoch 1698 :: Batch 0/1
Batch Loss = 0.02913115406954865
1698, epoch_train_loss=0.02913115406954865
Epoch 1699
Epoch 1699 :: Batch 0/1
Batch Loss = 0.029101261002693737
1699, epoch_train_loss=0.029101261002693737
Epoch 1700
Epoch 1700 :: Batch 0/1
Batch Loss = 0.0290721141243666
1700, epoch_train_loss=0.0290721141243666
Epoch 1701
Epoch 1701 :: Batch 0/1
Batch Loss = 0.029040812226065898
1701, epoch_train_loss=0.029040812226065898
Epoch 1702
Epoch 1702 :: Batch 0/1
Batch Loss = 0.029006599428690532
1702, epoch_train_loss=0.029006599428690532
Epoch 1703
Epoch 1703 :: Batch 0/1
Batch Loss = 0.02897438863685451
1703, epoch_train_loss=0.02897438863685451
Epoch 1704
Epoch 1704 :: Batch 0/1
Batch Loss = 0.028944816260479195
1704, epoch_train_loss=0.028944816260479195
Epoch 1705
Epoch 1705 :: Batch 0/1
Batch Loss = 0.028914774826838907
1705, epoch_train_loss=0.028914774826838907
Epoch 1706
Epoch 1706 :: Batch 0/1
Batch Loss = 0.028885409975024968
1706, epoch_train_loss=0.028885409975024968
Epoch 1707
Epoch 1707 :: Batch 0/1
Batch Loss = 0.02885892394940717
1707, epoch_train_loss=0.02885892394940717
Epoch 1708
Epoch 1708 :: Batch 0/1
Batch Loss = 0.028833620443317366
1708, epoch_train_loss=0.028833620443317366
Epoch 1709
Epoch 1709 :: Batch 0/1
Batch Loss = 0.028808674487627296
1709, epoch_train_loss=0.028808674487627296
Epoch 1710
Epoch 1710 :: Batch 0/1
Batch Loss = 0.028785848950401448
1710, epoch_train_loss=0.028785848950401448
Epoch 1711
Epoch 1711 :: Batch 0/1
Batch Loss = 0.028764543513637855
1711, epoch_train_loss=0.028764543513637855
Epoch 1712
Epoch 1712 :: Batch 0/1
Batch Loss = 0.028742792687796642
1712, epoch_train_loss=0.028742792687796642
Epoch 1713
Epoch 1713 :: Batch 0/1
Batch Loss = 0.02872117582884882
1713, epoch_train_loss=0.02872117582884882
Epoch 1714
Epoch 1714 :: Batch 0/1
Batch Loss = 0.028700475033846005
1714, epoch_train_loss=0.028700475033846005
Epoch 1715
Epoch 1715 :: Batch 0/1
Batch Loss = 0.028679570311875527
1715, epoch_train_loss=0.028679570311875527
Epoch 1716
Epoch 1716 :: Batch 0/1
Batch Loss = 0.028658209795277348
1716, epoch_train_loss=0.028658209795277348
Epoch 1717
Epoch 1717 :: Batch 0/1
Batch Loss = 0.028637270045857696
1717, epoch_train_loss=0.028637270045857696
Epoch 1718
Epoch 1718 :: Batch 0/1
Batch Loss = 0.028616349839472373
1718, epoch_train_loss=0.028616349839472373
Epoch 1719
Epoch 1719 :: Batch 0/1
Batch Loss = 0.028594818234296344
1719, epoch_train_loss=0.028594818234296344
Epoch 1720
Epoch 1720 :: Batch 0/1
Batch Loss = 0.028573342146197105
1720, epoch_train_loss=0.028573342146197105
Epoch 1721
Epoch 1721 :: Batch 0/1
Batch Loss = 0.028552302801514703
1721, epoch_train_loss=0.028552302801514703
Epoch 1722
Epoch 1722 :: Batch 0/1
Batch Loss = 0.02853120972947403
1722, epoch_train_loss=0.02853120972947403
Epoch 1723
Epoch 1723 :: Batch 0/1
Batch Loss = 0.028510156988946392
1723, epoch_train_loss=0.028510156988946392
Epoch 1724
Epoch 1724 :: Batch 0/1
Batch Loss = 0.02848956887725601
1724, epoch_train_loss=0.02848956887725601
Epoch 1725
Epoch 1725 :: Batch 0/1
Batch Loss = 0.02846919058911317
1725, epoch_train_loss=0.02846919058911317
Epoch 1726
Epoch 1726 :: Batch 0/1
Batch Loss = 0.028448821058322218
1726, epoch_train_loss=0.028448821058322218
Epoch 1727
Epoch 1727 :: Batch 0/1
Batch Loss = 0.028428805885121967
1727, epoch_train_loss=0.028428805885121967
Epoch 1728
Epoch 1728 :: Batch 0/1
Batch Loss = 0.028409203669353428
1728, epoch_train_loss=0.028409203669353428
Epoch 1729
Epoch 1729 :: Batch 0/1
Batch Loss = 0.028389722397423784
1729, epoch_train_loss=0.028389722397423784
Epoch 1730
Epoch 1730 :: Batch 0/1
Batch Loss = 0.028370401146143646
1730, epoch_train_loss=0.028370401146143646
Epoch 1731
Epoch 1731 :: Batch 0/1
Batch Loss = 0.028351376223080798
1731, epoch_train_loss=0.028351376223080798
Epoch 1732
Epoch 1732 :: Batch 0/1
Batch Loss = 0.02833248011629379
1732, epoch_train_loss=0.02833248011629379
Epoch 1733
Epoch 1733 :: Batch 0/1
Batch Loss = 0.028313634268549628
1733, epoch_train_loss=0.028313634268549628
Epoch 1734
Epoch 1734 :: Batch 0/1
Batch Loss = 0.028294989476758535
1734, epoch_train_loss=0.028294989476758535
Epoch 1735
Epoch 1735 :: Batch 0/1
Batch Loss = 0.02827652105193098
1735, epoch_train_loss=0.02827652105193098
Epoch 1736
Epoch 1736 :: Batch 0/1
Batch Loss = 0.028258077366521146
1736, epoch_train_loss=0.028258077366521146
Epoch 1737
Epoch 1737 :: Batch 0/1
Batch Loss = 0.028239695044315494
1737, epoch_train_loss=0.028239695044315494
Epoch 1738
Epoch 1738 :: Batch 0/1
Batch Loss = 0.028221442793477484
1738, epoch_train_loss=0.028221442793477484
Epoch 1739
Epoch 1739 :: Batch 0/1
Batch Loss = 0.02820324864837606
1739, epoch_train_loss=0.02820324864837606
Epoch 1740
Epoch 1740 :: Batch 0/1
Batch Loss = 0.02818509608944276
1740, epoch_train_loss=0.02818509608944276
Epoch 1741
Epoch 1741 :: Batch 0/1
Batch Loss = 0.028167061050536575
1741, epoch_train_loss=0.028167061050536575
Epoch 1742
Epoch 1742 :: Batch 0/1
Batch Loss = 0.028149125172422573
1742, epoch_train_loss=0.028149125172422573
Epoch 1743
Epoch 1743 :: Batch 0/1
Batch Loss = 0.02813122959975199
1743, epoch_train_loss=0.02813122959975199
Epoch 1744
Epoch 1744 :: Batch 0/1
Batch Loss = 0.028113413418992903
1744, epoch_train_loss=0.028113413418992903
Epoch 1745
Epoch 1745 :: Batch 0/1
Batch Loss = 0.02809571529402295
1745, epoch_train_loss=0.02809571529402295
Epoch 1746
Epoch 1746 :: Batch 0/1
Batch Loss = 0.02807810048595547
1746, epoch_train_loss=0.02807810048595547
Epoch 1747
Epoch 1747 :: Batch 0/1
Batch Loss = 0.028060562910252045
1747, epoch_train_loss=0.028060562910252045
Epoch 1748
Epoch 1748 :: Batch 0/1
Batch Loss = 0.028043133362461413
1748, epoch_train_loss=0.028043133362461413
Epoch 1749
Epoch 1749 :: Batch 0/1
Batch Loss = 0.028025795420883123
1749, epoch_train_loss=0.028025795420883123
Epoch 1750
Epoch 1750 :: Batch 0/1
Batch Loss = 0.028008522620833365
1750, epoch_train_loss=0.028008522620833365
Epoch 1751
Epoch 1751 :: Batch 0/1
Batch Loss = 0.02799133448020427
1751, epoch_train_loss=0.02799133448020427
Epoch 1752
Epoch 1752 :: Batch 0/1
Batch Loss = 0.02797424006669887
1752, epoch_train_loss=0.02797424006669887
Epoch 1753
Epoch 1753 :: Batch 0/1
Batch Loss = 0.027957213446249067
1753, epoch_train_loss=0.027957213446249067
Epoch 1754
Epoch 1754 :: Batch 0/1
Batch Loss = 0.027940248309654084
1754, epoch_train_loss=0.027940248309654084
Epoch 1755
Epoch 1755 :: Batch 0/1
Batch Loss = 0.027923355869646706
1755, epoch_train_loss=0.027923355869646706
Epoch 1756
Epoch 1756 :: Batch 0/1
Batch Loss = 0.027906525590897346
1756, epoch_train_loss=0.027906525590897346
Epoch 1757
Epoch 1757 :: Batch 0/1
Batch Loss = 0.027889746490268912
1757, epoch_train_loss=0.027889746490268912
Epoch 1758
Epoch 1758 :: Batch 0/1
Batch Loss = 0.027873028426588658
1758, epoch_train_loss=0.027873028426588658
Epoch 1759
Epoch 1759 :: Batch 0/1
Batch Loss = 0.027856372818888128
1759, epoch_train_loss=0.027856372818888128
Epoch 1760
Epoch 1760 :: Batch 0/1
Batch Loss = 0.027839766967917536
1760, epoch_train_loss=0.027839766967917536
Epoch 1761
Epoch 1761 :: Batch 0/1
Batch Loss = 0.02782321102672943
1761, epoch_train_loss=0.02782321102672943
Epoch 1762
Epoch 1762 :: Batch 0/1
Batch Loss = 0.02780671233063351
1762, epoch_train_loss=0.02780671233063351
Epoch 1763
Epoch 1763 :: Batch 0/1
Batch Loss = 0.027790266669880995
1763, epoch_train_loss=0.027790266669880995
Epoch 1764
Epoch 1764 :: Batch 0/1
Batch Loss = 0.02777387006308854
1764, epoch_train_loss=0.02777387006308854
Epoch 1765
Epoch 1765 :: Batch 0/1
Batch Loss = 0.027757527047143946
1765, epoch_train_loss=0.027757527047143946
Epoch 1766
Epoch 1766 :: Batch 0/1
Batch Loss = 0.02774123669449329
1766, epoch_train_loss=0.02774123669449329
Epoch 1767
Epoch 1767 :: Batch 0/1
Batch Loss = 0.02772499266090334
1767, epoch_train_loss=0.02772499266090334
Epoch 1768
Epoch 1768 :: Batch 0/1
Batch Loss = 0.027708795519476175
1768, epoch_train_loss=0.027708795519476175
Epoch 1769
Epoch 1769 :: Batch 0/1
Batch Loss = 0.027692647669899734
1769, epoch_train_loss=0.027692647669899734
Epoch 1770
Epoch 1770 :: Batch 0/1
Batch Loss = 0.027676545108689953
1770, epoch_train_loss=0.027676545108689953
Epoch 1771
Epoch 1771 :: Batch 0/1
Batch Loss = 0.027660484491805165
1771, epoch_train_loss=0.027660484491805165
Epoch 1772
Epoch 1772 :: Batch 0/1
Batch Loss = 0.027644466556365245
1772, epoch_train_loss=0.027644466556365245
Epoch 1773
Epoch 1773 :: Batch 0/1
Batch Loss = 0.027628489429267145
1773, epoch_train_loss=0.027628489429267145
Epoch 1774
Epoch 1774 :: Batch 0/1
Batch Loss = 0.027612549450390017
1774, epoch_train_loss=0.027612549450390017
Epoch 1775
Epoch 1775 :: Batch 0/1
Batch Loss = 0.027596646411101426
1775, epoch_train_loss=0.027596646411101426
Epoch 1776
Epoch 1776 :: Batch 0/1
Batch Loss = 0.027580780272314084
1776, epoch_train_loss=0.027580780272314084
Epoch 1777
Epoch 1777 :: Batch 0/1
Batch Loss = 0.027564947986066193
1777, epoch_train_loss=0.027564947986066193
Epoch 1778
Epoch 1778 :: Batch 0/1
Batch Loss = 0.027549147501182064
1778, epoch_train_loss=0.027549147501182064
Epoch 1779
Epoch 1779 :: Batch 0/1
Batch Loss = 0.02753337884400225
1779, epoch_train_loss=0.02753337884400225
Epoch 1780
Epoch 1780 :: Batch 0/1
Batch Loss = 0.02751764065086741
1780, epoch_train_loss=0.02751764065086741
Epoch 1781
Epoch 1781 :: Batch 0/1
Batch Loss = 0.02750193078775865
1781, epoch_train_loss=0.02750193078775865
Epoch 1782
Epoch 1782 :: Batch 0/1
Batch Loss = 0.027486248617164602
1782, epoch_train_loss=0.027486248617164602
Epoch 1783
Epoch 1783 :: Batch 0/1
Batch Loss = 0.027470593269527204
1783, epoch_train_loss=0.027470593269527204
Epoch 1784
Epoch 1784 :: Batch 0/1
Batch Loss = 0.027454962563063734
1784, epoch_train_loss=0.027454962563063734
Epoch 1785
Epoch 1785 :: Batch 0/1
Batch Loss = 0.027439355042338565
1785, epoch_train_loss=0.027439355042338565
Epoch 1786
Epoch 1786 :: Batch 0/1
Batch Loss = 0.02742377007069453
1786, epoch_train_loss=0.02742377007069453
Epoch 1787
Epoch 1787 :: Batch 0/1
Batch Loss = 0.027408206092286745
1787, epoch_train_loss=0.027408206092286745
Epoch 1788
Epoch 1788 :: Batch 0/1
Batch Loss = 0.027392661194720183
1788, epoch_train_loss=0.027392661194720183
Epoch 1789
Epoch 1789 :: Batch 0/1
Batch Loss = 0.027377134199932485
1789, epoch_train_loss=0.027377134199932485
Epoch 1790
Epoch 1790 :: Batch 0/1
Batch Loss = 0.027361623782500878
1790, epoch_train_loss=0.027361623782500878
Epoch 1791
Epoch 1791 :: Batch 0/1
Batch Loss = 0.027346128098393624
1791, epoch_train_loss=0.027346128098393624
Epoch 1792
Epoch 1792 :: Batch 0/1
Batch Loss = 0.027330645718685678
1792, epoch_train_loss=0.027330645718685678
Epoch 1793
Epoch 1793 :: Batch 0/1
Batch Loss = 0.027315175488650967
1793, epoch_train_loss=0.027315175488650967
Epoch 1794
Epoch 1794 :: Batch 0/1
Batch Loss = 0.02729971576190475
1794, epoch_train_loss=0.02729971576190475
Epoch 1795
Epoch 1795 :: Batch 0/1
Batch Loss = 0.02728426483379493
1795, epoch_train_loss=0.02728426483379493
Epoch 1796
Epoch 1796 :: Batch 0/1
Batch Loss = 0.027268821412777436
1796, epoch_train_loss=0.027268821412777436
Epoch 1797
Epoch 1797 :: Batch 0/1
Batch Loss = 0.0272533841140667
1797, epoch_train_loss=0.0272533841140667
Epoch 1798
Epoch 1798 :: Batch 0/1
Batch Loss = 0.027237951314025836
1798, epoch_train_loss=0.027237951314025836
Epoch 1799
Epoch 1799 :: Batch 0/1
Batch Loss = 0.027222521585970175
1799, epoch_train_loss=0.027222521585970175
Epoch 1800
Epoch 1800 :: Batch 0/1
Batch Loss = 0.027207093584874012
1800, epoch_train_loss=0.027207093584874012
Epoch 1801
Epoch 1801 :: Batch 0/1
Batch Loss = 0.027191665740576798
1801, epoch_train_loss=0.027191665740576798
Epoch 1802
Epoch 1802 :: Batch 0/1
Batch Loss = 0.027176236518697442
1802, epoch_train_loss=0.027176236518697442
Epoch 1803
Epoch 1803 :: Batch 0/1
Batch Loss = 0.027160804582830288
1803, epoch_train_loss=0.027160804582830288
Epoch 1804
Epoch 1804 :: Batch 0/1
Batch Loss = 0.0271453685140377
1804, epoch_train_loss=0.0271453685140377
Epoch 1805
Epoch 1805 :: Batch 0/1
Batch Loss = 0.02712992678578934
1805, epoch_train_loss=0.02712992678578934
Epoch 1806
Epoch 1806 :: Batch 0/1
Batch Loss = 0.027114477989146877
1806, epoch_train_loss=0.027114477989146877
Epoch 1807
Epoch 1807 :: Batch 0/1
Batch Loss = 0.027099020764502765
1807, epoch_train_loss=0.027099020764502765
Epoch 1808
Epoch 1808 :: Batch 0/1
Batch Loss = 0.02708355367549029
1808, epoch_train_loss=0.02708355367549029
Epoch 1809
Epoch 1809 :: Batch 0/1
Batch Loss = 0.02706807533921851
1809, epoch_train_loss=0.02706807533921851
Epoch 1810
Epoch 1810 :: Batch 0/1
Batch Loss = 0.02705258447501059
1810, epoch_train_loss=0.02705258447501059
Epoch 1811
Epoch 1811 :: Batch 0/1
Batch Loss = 0.027037079772032513
1811, epoch_train_loss=0.027037079772032513
Epoch 1812
Epoch 1812 :: Batch 0/1
Batch Loss = 0.0270215599116378
1812, epoch_train_loss=0.0270215599116378
Epoch 1813
Epoch 1813 :: Batch 0/1
Batch Loss = 0.027006023679132487
1813, epoch_train_loss=0.027006023679132487
Epoch 1814
Epoch 1814 :: Batch 0/1
Batch Loss = 0.026990469914004483
1814, epoch_train_loss=0.026990469914004483
Epoch 1815
Epoch 1815 :: Batch 0/1
Batch Loss = 0.026974897454431882
1815, epoch_train_loss=0.026974897454431882
Epoch 1816
Epoch 1816 :: Batch 0/1
Batch Loss = 0.026959305202985992
1816, epoch_train_loss=0.026959305202985992
Epoch 1817
Epoch 1817 :: Batch 0/1
Batch Loss = 0.026943692147324467
1817, epoch_train_loss=0.026943692147324467
Epoch 1818
Epoch 1818 :: Batch 0/1
Batch Loss = 0.026928057307500544
1818, epoch_train_loss=0.026928057307500544
Epoch 1819
Epoch 1819 :: Batch 0/1
Batch Loss = 0.026912399758154373
1819, epoch_train_loss=0.026912399758154373
Epoch 1820
Epoch 1820 :: Batch 0/1
Batch Loss = 0.026896718673044377
1820, epoch_train_loss=0.026896718673044377
Epoch 1821
Epoch 1821 :: Batch 0/1
Batch Loss = 0.026881013301360904
1821, epoch_train_loss=0.026881013301360904
Epoch 1822
Epoch 1822 :: Batch 0/1
Batch Loss = 0.02686528294544433
1822, epoch_train_loss=0.02686528294544433
Epoch 1823
Epoch 1823 :: Batch 0/1
Batch Loss = 0.02684952700510146
1823, epoch_train_loss=0.02684952700510146
Epoch 1824
Epoch 1824 :: Batch 0/1
Batch Loss = 0.02683374498622242
1824, epoch_train_loss=0.02683374498622242
Epoch 1825
Epoch 1825 :: Batch 0/1
Batch Loss = 0.026817936476257083
1825, epoch_train_loss=0.026817936476257083
Epoch 1826
Epoch 1826 :: Batch 0/1
Batch Loss = 0.026802101166247806
1826, epoch_train_loss=0.026802101166247806
Epoch 1827
Epoch 1827 :: Batch 0/1
Batch Loss = 0.02678623886341655
1827, epoch_train_loss=0.02678623886341655
Epoch 1828
Epoch 1828 :: Batch 0/1
Batch Loss = 0.026770349485766703
1828, epoch_train_loss=0.026770349485766703
Epoch 1829
Epoch 1829 :: Batch 0/1
Batch Loss = 0.026754433056559182
1829, epoch_train_loss=0.026754433056559182
Epoch 1830
Epoch 1830 :: Batch 0/1
Batch Loss = 0.026738489728287882
1830, epoch_train_loss=0.026738489728287882
Epoch 1831
Epoch 1831 :: Batch 0/1
Batch Loss = 0.026722519787824372
1831, epoch_train_loss=0.026722519787824372
Epoch 1832
Epoch 1832 :: Batch 0/1
Batch Loss = 0.026706523641870095
1832, epoch_train_loss=0.026706523641870095
Epoch 1833
Epoch 1833 :: Batch 0/1
Batch Loss = 0.02669050182748204
1833, epoch_train_loss=0.02669050182748204
Epoch 1834
Epoch 1834 :: Batch 0/1
Batch Loss = 0.026674455025530116
1834, epoch_train_loss=0.026674455025530116
Epoch 1835
Epoch 1835 :: Batch 0/1
Batch Loss = 0.02665838404978987
1835, epoch_train_loss=0.02665838404978987
Epoch 1836
Epoch 1836 :: Batch 0/1
Batch Loss = 0.026642289847936797
1836, epoch_train_loss=0.026642289847936797
Epoch 1837
Epoch 1837 :: Batch 0/1
Batch Loss = 0.026626173513284234
1837, epoch_train_loss=0.026626173513284234
Epoch 1838
Epoch 1838 :: Batch 0/1
Batch Loss = 0.02661003628103168
1838, epoch_train_loss=0.02661003628103168
Epoch 1839
Epoch 1839 :: Batch 0/1
Batch Loss = 0.026593879517318727
1839, epoch_train_loss=0.026593879517318727
Epoch 1840
Epoch 1840 :: Batch 0/1
Batch Loss = 0.026577704727608873
1840, epoch_train_loss=0.026577704727608873
Epoch 1841
Epoch 1841 :: Batch 0/1
Batch Loss = 0.026561513552760527
1841, epoch_train_loss=0.026561513552760527
Epoch 1842
Epoch 1842 :: Batch 0/1
Batch Loss = 0.026545307760577324
1842, epoch_train_loss=0.026545307760577324
Epoch 1843
Epoch 1843 :: Batch 0/1
Batch Loss = 0.026529089245514472
1843, epoch_train_loss=0.026529089245514472
Epoch 1844
Epoch 1844 :: Batch 0/1
Batch Loss = 0.026512860021678547
1844, epoch_train_loss=0.026512860021678547
Epoch 1845
Epoch 1845 :: Batch 0/1
Batch Loss = 0.0264966222181467
1845, epoch_train_loss=0.0264966222181467
Epoch 1846
Epoch 1846 :: Batch 0/1
Batch Loss = 0.026480378068282455
1846, epoch_train_loss=0.026480378068282455
Epoch 1847
Epoch 1847 :: Batch 0/1
Batch Loss = 0.02646412990443328
1847, epoch_train_loss=0.02646412990443328
Epoch 1848
Epoch 1848 :: Batch 0/1
Batch Loss = 0.026447880150768672
1848, epoch_train_loss=0.026447880150768672
Epoch 1849
Epoch 1849 :: Batch 0/1
Batch Loss = 0.026431631307016954
1849, epoch_train_loss=0.026431631307016954
Epoch 1850
Epoch 1850 :: Batch 0/1
Batch Loss = 0.026415385943898844
1850, epoch_train_loss=0.026415385943898844
Epoch 1851
Epoch 1851 :: Batch 0/1
Batch Loss = 0.02639914668861659
1851, epoch_train_loss=0.02639914668861659
Epoch 1852
Epoch 1852 :: Batch 0/1
Batch Loss = 0.026382916216015004
1852, epoch_train_loss=0.026382916216015004
Epoch 1853
Epoch 1853 :: Batch 0/1
Batch Loss = 0.026366697234642033
1853, epoch_train_loss=0.026366697234642033
Epoch 1854
Epoch 1854 :: Batch 0/1
Batch Loss = 0.026350492475935367
1854, epoch_train_loss=0.026350492475935367
Epoch 1855
Epoch 1855 :: Batch 0/1
Batch Loss = 0.02633430468177389
1855, epoch_train_loss=0.02633430468177389
Epoch 1856
Epoch 1856 :: Batch 0/1
Batch Loss = 0.02631813659104071
1856, epoch_train_loss=0.02631813659104071
Epoch 1857
Epoch 1857 :: Batch 0/1
Batch Loss = 0.026301990928088777
1857, epoch_train_loss=0.026301990928088777
Epoch 1858
Epoch 1858 :: Batch 0/1
Batch Loss = 0.026285870391485056
1858, epoch_train_loss=0.026285870391485056
Epoch 1859
Epoch 1859 :: Batch 0/1
Batch Loss = 0.02626977764287849
1859, epoch_train_loss=0.02626977764287849
Epoch 1860
Epoch 1860 :: Batch 0/1
Batch Loss = 0.0262537152903797
1860, epoch_train_loss=0.0262537152903797
Epoch 1861
Epoch 1861 :: Batch 0/1
Batch Loss = 0.02623768588765253
1861, epoch_train_loss=0.02623768588765253
Epoch 1862
Epoch 1862 :: Batch 0/1
Batch Loss = 0.02622169191346428
1862, epoch_train_loss=0.02622169191346428
Epoch 1863
Epoch 1863 :: Batch 0/1
Batch Loss = 0.0262057357711543
1863, epoch_train_loss=0.0262057357711543
Epoch 1864
Epoch 1864 :: Batch 0/1
Batch Loss = 0.02618981977557139
1864, epoch_train_loss=0.02618981977557139
Epoch 1865
Epoch 1865 :: Batch 0/1
Batch Loss = 0.026173946147171716
1865, epoch_train_loss=0.026173946147171716
Epoch 1866
Epoch 1866 :: Batch 0/1
Batch Loss = 0.026158117005129197
1866, epoch_train_loss=0.026158117005129197
Epoch 1867
Epoch 1867 :: Batch 0/1
Batch Loss = 0.026142334361855604
1867, epoch_train_loss=0.026142334361855604
Epoch 1868
Epoch 1868 :: Batch 0/1
Batch Loss = 0.026126600118882282
1868, epoch_train_loss=0.026126600118882282
Epoch 1869
Epoch 1869 :: Batch 0/1
Batch Loss = 0.026110916062033283
1869, epoch_train_loss=0.026110916062033283
Epoch 1870
Epoch 1870 :: Batch 0/1
Batch Loss = 0.02609528386385645
1870, epoch_train_loss=0.02609528386385645
Epoch 1871
Epoch 1871 :: Batch 0/1
Batch Loss = 0.02607970507351447
1871, epoch_train_loss=0.02607970507351447
Epoch 1872
Epoch 1872 :: Batch 0/1
Batch Loss = 0.02606418112509529
1872, epoch_train_loss=0.02606418112509529
Epoch 1873
Epoch 1873 :: Batch 0/1
Batch Loss = 0.02604871333143923
1873, epoch_train_loss=0.02604871333143923
Epoch 1874
Epoch 1874 :: Batch 0/1
Batch Loss = 0.026033302890343744
1874, epoch_train_loss=0.026033302890343744
Epoch 1875
Epoch 1875 :: Batch 0/1
Batch Loss = 0.026017950882862337
1875, epoch_train_loss=0.026017950882862337
Epoch 1876
Epoch 1876 :: Batch 0/1
Batch Loss = 0.02600265827579643
1876, epoch_train_loss=0.02600265827579643
Epoch 1877
Epoch 1877 :: Batch 0/1
Batch Loss = 0.02598742592850579
1877, epoch_train_loss=0.02598742592850579
Epoch 1878
Epoch 1878 :: Batch 0/1
Batch Loss = 0.02597225459178433
1878, epoch_train_loss=0.02597225459178433
Epoch 1879
Epoch 1879 :: Batch 0/1
Batch Loss = 0.02595714491688149
1879, epoch_train_loss=0.02595714491688149
Epoch 1880
Epoch 1880 :: Batch 0/1
Batch Loss = 0.025942097456188983
1880, epoch_train_loss=0.025942097456188983
Epoch 1881
Epoch 1881 :: Batch 0/1
Batch Loss = 0.025927112668024773
1881, epoch_train_loss=0.025927112668024773
Epoch 1882
Epoch 1882 :: Batch 0/1
Batch Loss = 0.025912190925193083
1882, epoch_train_loss=0.025912190925193083
Epoch 1883
Epoch 1883 :: Batch 0/1
Batch Loss = 0.02589733251507329
1883, epoch_train_loss=0.02589733251507329
Epoch 1884
Epoch 1884 :: Batch 0/1
Batch Loss = 0.02588253764943807
1884, epoch_train_loss=0.02588253764943807
Epoch 1885
Epoch 1885 :: Batch 0/1
Batch Loss = 0.025867806464699987
1885, epoch_train_loss=0.025867806464699987
Epoch 1886
Epoch 1886 :: Batch 0/1
Batch Loss = 0.025853139030371022
1886, epoch_train_loss=0.025853139030371022
Epoch 1887
Epoch 1887 :: Batch 0/1
Batch Loss = 0.025838535353736647
1887, epoch_train_loss=0.025838535353736647
Epoch 1888
Epoch 1888 :: Batch 0/1
Batch Loss = 0.025823995382059043
1888, epoch_train_loss=0.025823995382059043
Epoch 1889
Epoch 1889 :: Batch 0/1
Batch Loss = 0.02580951900576255
1889, epoch_train_loss=0.02580951900576255
Epoch 1890
Epoch 1890 :: Batch 0/1
Batch Loss = 0.025795106069466896
1890, epoch_train_loss=0.025795106069466896
Epoch 1891
Epoch 1891 :: Batch 0/1
Batch Loss = 0.025780756368728258
1891, epoch_train_loss=0.025780756368728258
Epoch 1892
Epoch 1892 :: Batch 0/1
Batch Loss = 0.025766469653573276
1892, epoch_train_loss=0.025766469653573276
Epoch 1893
Epoch 1893 :: Batch 0/1
Batch Loss = 0.025752245638549522
1893, epoch_train_loss=0.025752245638549522
Epoch 1894
Epoch 1894 :: Batch 0/1
Batch Loss = 0.025738083997454905
1894, epoch_train_loss=0.025738083997454905
Epoch 1895
Epoch 1895 :: Batch 0/1
Batch Loss = 0.02572398437047568
1895, epoch_train_loss=0.02572398437047568
Epoch 1896
Epoch 1896 :: Batch 0/1
Batch Loss = 0.02570994636523658
1896, epoch_train_loss=0.02570994636523658
Epoch 1897
Epoch 1897 :: Batch 0/1
Batch Loss = 0.025695969559811217
1897, epoch_train_loss=0.025695969559811217
Epoch 1898
Epoch 1898 :: Batch 0/1
Batch Loss = 0.025682053500961362
1898, epoch_train_loss=0.025682053500961362
Epoch 1899
Epoch 1899 :: Batch 0/1
Batch Loss = 0.02566819770777816
1899, epoch_train_loss=0.02566819770777816
Epoch 1900
Epoch 1900 :: Batch 0/1
Batch Loss = 0.02565440167611424
1900, epoch_train_loss=0.02565440167611424
Epoch 1901
Epoch 1901 :: Batch 0/1
Batch Loss = 0.025640664872761596
1901, epoch_train_loss=0.025640664872761596
Epoch 1902
Epoch 1902 :: Batch 0/1
Batch Loss = 0.02562698674062821
1902, epoch_train_loss=0.02562698674062821
Epoch 1903
Epoch 1903 :: Batch 0/1
Batch Loss = 0.025613366698138712
1903, epoch_train_loss=0.025613366698138712
Epoch 1904
Epoch 1904 :: Batch 0/1
Batch Loss = 0.025599804139607704
1904, epoch_train_loss=0.025599804139607704
Epoch 1905
Epoch 1905 :: Batch 0/1
Batch Loss = 0.02558629843550811
1905, epoch_train_loss=0.02558629843550811
Epoch 1906
Epoch 1906 :: Batch 0/1
Batch Loss = 0.02557284893264855
1906, epoch_train_loss=0.02557284893264855
Epoch 1907
Epoch 1907 :: Batch 0/1
Batch Loss = 0.02555945495428135
1907, epoch_train_loss=0.02555945495428135
Epoch 1908
Epoch 1908 :: Batch 0/1
Batch Loss = 0.02554611580017762
1908, epoch_train_loss=0.02554611580017762
Epoch 1909
Epoch 1909 :: Batch 0/1
Batch Loss = 0.02553283074669358
1909, epoch_train_loss=0.02553283074669358
Epoch 1910
Epoch 1910 :: Batch 0/1
Batch Loss = 0.025519599046843186
1910, epoch_train_loss=0.025519599046843186
Epoch 1911
Epoch 1911 :: Batch 0/1
Batch Loss = 0.02550641992933769
1911, epoch_train_loss=0.02550641992933769
Epoch 1912
Epoch 1912 :: Batch 0/1
Batch Loss = 0.025493292601936682
1912, epoch_train_loss=0.025493292601936682
Epoch 1913
Epoch 1913 :: Batch 0/1
Batch Loss = 0.025480216248468992
1913, epoch_train_loss=0.025480216248468992
Epoch 1914
Epoch 1914 :: Batch 0/1
Batch Loss = 0.02546719003019779
1914, epoch_train_loss=0.02546719003019779
Epoch 1915
Epoch 1915 :: Batch 0/1
Batch Loss = 0.02545421308621214
1915, epoch_train_loss=0.02545421308621214
Epoch 1916
Epoch 1916 :: Batch 0/1
Batch Loss = 0.02544128453288246
1916, epoch_train_loss=0.02544128453288246
Epoch 1917
Epoch 1917 :: Batch 0/1
Batch Loss = 0.0254284034676016
1917, epoch_train_loss=0.0254284034676016
Epoch 1918
Epoch 1918 :: Batch 0/1
Batch Loss = 0.025415568965338104
1918, epoch_train_loss=0.025415568965338104
Epoch 1919
Epoch 1919 :: Batch 0/1
Batch Loss = 0.025402780083652456
1919, epoch_train_loss=0.025402780083652456
Epoch 1920
Epoch 1920 :: Batch 0/1
Batch Loss = 0.02539003586056108
1920, epoch_train_loss=0.02539003586056108
Epoch 1921
Epoch 1921 :: Batch 0/1
Batch Loss = 0.0253773353166864
1921, epoch_train_loss=0.0253773353166864
Epoch 1922
Epoch 1922 :: Batch 0/1
Batch Loss = 0.025364677455477304
1922, epoch_train_loss=0.025364677455477304
Epoch 1923
Epoch 1923 :: Batch 0/1
Batch Loss = 0.025352061266630595
1923, epoch_train_loss=0.025352061266630595
Epoch 1924
Epoch 1924 :: Batch 0/1
Batch Loss = 0.02533948572757343
1924, epoch_train_loss=0.02533948572757343
Epoch 1925
Epoch 1925 :: Batch 0/1
Batch Loss = 0.025326949802038666
1925, epoch_train_loss=0.025326949802038666
Epoch 1926
Epoch 1926 :: Batch 0/1
Batch Loss = 0.025314452445873478
1926, epoch_train_loss=0.025314452445873478
Epoch 1927
Epoch 1927 :: Batch 0/1
Batch Loss = 0.025301992605858525
1927, epoch_train_loss=0.025301992605858525
Epoch 1928
Epoch 1928 :: Batch 0/1
Batch Loss = 0.025289569221699414
1928, epoch_train_loss=0.025289569221699414
Epoch 1929
Epoch 1929 :: Batch 0/1
Batch Loss = 0.025277181231127453
1929, epoch_train_loss=0.025277181231127453
Epoch 1930
Epoch 1930 :: Batch 0/1
Batch Loss = 0.02526482756808166
1930, epoch_train_loss=0.02526482756808166
Epoch 1931
Epoch 1931 :: Batch 0/1
Batch Loss = 0.025252507168015297
1931, epoch_train_loss=0.025252507168015297
Epoch 1932
Epoch 1932 :: Batch 0/1
Batch Loss = 0.025240218969288478
1932, epoch_train_loss=0.025240218969288478
Epoch 1933
Epoch 1933 :: Batch 0/1
Batch Loss = 0.025227961916649465
1933, epoch_train_loss=0.025227961916649465
Epoch 1934
Epoch 1934 :: Batch 0/1
Batch Loss = 0.025215734960842548
1934, epoch_train_loss=0.025215734960842548
Epoch 1935
Epoch 1935 :: Batch 0/1
Batch Loss = 0.02520353706425025
1935, epoch_train_loss=0.02520353706425025
Epoch 1936
Epoch 1936 :: Batch 0/1
Batch Loss = 0.02519136720363134
1936, epoch_train_loss=0.02519136720363134
Epoch 1937
Epoch 1937 :: Batch 0/1
Batch Loss = 0.025179224369996808
1937, epoch_train_loss=0.025179224369996808
Epoch 1938
Epoch 1938 :: Batch 0/1
Batch Loss = 0.025167107574466597
1938, epoch_train_loss=0.025167107574466597
Epoch 1939
Epoch 1939 :: Batch 0/1
Batch Loss = 0.025155015850257027
1939, epoch_train_loss=0.025155015850257027
Epoch 1940
Epoch 1940 :: Batch 0/1
Batch Loss = 0.02514294825669445
1940, epoch_train_loss=0.02514294825669445
Epoch 1941
Epoch 1941 :: Batch 0/1
Batch Loss = 0.025130903879388405
1941, epoch_train_loss=0.025130903879388405
Epoch 1942
Epoch 1942 :: Batch 0/1
Batch Loss = 0.025118881836317733
1942, epoch_train_loss=0.025118881836317733
Epoch 1943
Epoch 1943 :: Batch 0/1
Batch Loss = 0.025106881280059126
1943, epoch_train_loss=0.025106881280059126
Epoch 1944
Epoch 1944 :: Batch 0/1
Batch Loss = 0.025094901401036953
1944, epoch_train_loss=0.025094901401036953
Epoch 1945
Epoch 1945 :: Batch 0/1
Batch Loss = 0.025082941430808737
1945, epoch_train_loss=0.025082941430808737
Epoch 1946
Epoch 1946 :: Batch 0/1
Batch Loss = 0.02507100064537654
1946, epoch_train_loss=0.02507100064537654
Epoch 1947
Epoch 1947 :: Batch 0/1
Batch Loss = 0.025059078368513975
1947, epoch_train_loss=0.025059078368513975
Epoch 1948
Epoch 1948 :: Batch 0/1
Batch Loss = 0.025047173975096558
1948, epoch_train_loss=0.025047173975096558
Epoch 1949
Epoch 1949 :: Batch 0/1
Batch Loss = 0.025035286894421856
1949, epoch_train_loss=0.025035286894421856
Epoch 1950
Epoch 1950 :: Batch 0/1
Batch Loss = 0.025023416613504913
1950, epoch_train_loss=0.025023416613504913
Epoch 1951
Epoch 1951 :: Batch 0/1
Batch Loss = 0.02501156268033152
1951, epoch_train_loss=0.02501156268033152
Epoch 1952
Epoch 1952 :: Batch 0/1
Batch Loss = 0.02499972470705022
1952, epoch_train_loss=0.02499972470705022
Epoch 1953
Epoch 1953 :: Batch 0/1
Batch Loss = 0.0249879023730819
1953, epoch_train_loss=0.0249879023730819
Epoch 1954
Epoch 1954 :: Batch 0/1
Batch Loss = 0.024976095428123665
1954, epoch_train_loss=0.024976095428123665
Epoch 1955
Epoch 1955 :: Batch 0/1
Batch Loss = 0.024964303694068515
1955, epoch_train_loss=0.024964303694068515
Epoch 1956
Epoch 1956 :: Batch 0/1
Batch Loss = 0.02495252707058204
1956, epoch_train_loss=0.02495252707058204
Epoch 1957
Epoch 1957 :: Batch 0/1
Batch Loss = 0.02494076553476807
1957, epoch_train_loss=0.02494076553476807
Epoch 1958
Epoch 1958 :: Batch 0/1
Batch Loss = 0.02492901914443636
1958, epoch_train_loss=0.02492901914443636
Epoch 1959
Epoch 1959 :: Batch 0/1
Batch Loss = 0.024917288039218853
1959, epoch_train_loss=0.024917288039218853
Epoch 1960
Epoch 1960 :: Batch 0/1
Batch Loss = 0.024905572445200554
1960, epoch_train_loss=0.024905572445200554
Epoch 1961
Epoch 1961 :: Batch 0/1
Batch Loss = 0.02489387267353965
1961, epoch_train_loss=0.02489387267353965
Epoch 1962
Epoch 1962 :: Batch 0/1
Batch Loss = 0.024882189121594796
1962, epoch_train_loss=0.024882189121594796
Epoch 1963
Epoch 1963 :: Batch 0/1
Batch Loss = 0.024870522276501394
1963, epoch_train_loss=0.024870522276501394
Epoch 1964
Epoch 1964 :: Batch 0/1
Batch Loss = 0.02485887271265957
1964, epoch_train_loss=0.02485887271265957
Epoch 1965
Epoch 1965 :: Batch 0/1
Batch Loss = 0.024847241092562124
1965, epoch_train_loss=0.024847241092562124
Epoch 1966
Epoch 1966 :: Batch 0/1
Batch Loss = 0.024835628165261688
1966, epoch_train_loss=0.024835628165261688
Epoch 1967
Epoch 1967 :: Batch 0/1
Batch Loss = 0.024824034768095016
1967, epoch_train_loss=0.024824034768095016
Epoch 1968
Epoch 1968 :: Batch 0/1
Batch Loss = 0.024812461821304203
1968, epoch_train_loss=0.024812461821304203
Epoch 1969
Epoch 1969 :: Batch 0/1
Batch Loss = 0.024800910328688648
1969, epoch_train_loss=0.024800910328688648
Epoch 1970
Epoch 1970 :: Batch 0/1
Batch Loss = 0.024789381374871702
1970, epoch_train_loss=0.024789381374871702
Epoch 1971
Epoch 1971 :: Batch 0/1
Batch Loss = 0.02477787611920573
1971, epoch_train_loss=0.02477787611920573
Epoch 1972
Epoch 1972 :: Batch 0/1
Batch Loss = 0.02476639579563561
1972, epoch_train_loss=0.02476639579563561
Epoch 1973
Epoch 1973 :: Batch 0/1
Batch Loss = 0.024754941705462805
1973, epoch_train_loss=0.024754941705462805
Epoch 1974
Epoch 1974 :: Batch 0/1
Batch Loss = 0.024743515212356394
1974, epoch_train_loss=0.024743515212356394
Epoch 1975
Epoch 1975 :: Batch 0/1
Batch Loss = 0.024732117738665505
1975, epoch_train_loss=0.024732117738665505
Epoch 1976
Epoch 1976 :: Batch 0/1
Batch Loss = 0.02472075075842914
1976, epoch_train_loss=0.02472075075842914
Epoch 1977
Epoch 1977 :: Batch 0/1
Batch Loss = 0.024709415790829678
1977, epoch_train_loss=0.024709415790829678
Epoch 1978
Epoch 1978 :: Batch 0/1
Batch Loss = 0.02469811439411434
1978, epoch_train_loss=0.02469811439411434
Epoch 1979
Epoch 1979 :: Batch 0/1
Batch Loss = 0.024686848156369137
1979, epoch_train_loss=0.024686848156369137
Epoch 1980
Epoch 1980 :: Batch 0/1
Batch Loss = 0.024675618687797683
1980, epoch_train_loss=0.024675618687797683
Epoch 1981
Epoch 1981 :: Batch 0/1
Batch Loss = 0.024664427614529548
1981, epoch_train_loss=0.024664427614529548
Epoch 1982
Epoch 1982 :: Batch 0/1
Batch Loss = 0.024653276569473453
1982, epoch_train_loss=0.024653276569473453
Epoch 1983
Epoch 1983 :: Batch 0/1
Batch Loss = 0.024642167183943606
1983, epoch_train_loss=0.024642167183943606
Epoch 1984
Epoch 1984 :: Batch 0/1
Batch Loss = 0.02463110108011458
1984, epoch_train_loss=0.02463110108011458
Epoch 1985
Epoch 1985 :: Batch 0/1
Batch Loss = 0.024620079859903672
1985, epoch_train_loss=0.024620079859903672
Epoch 1986
Epoch 1986 :: Batch 0/1
Batch Loss = 0.024609105099334276
1986, epoch_train_loss=0.024609105099334276
Epoch 1987
Epoch 1987 :: Batch 0/1
Batch Loss = 0.024598178339477018
1987, epoch_train_loss=0.024598178339477018
Epoch 1988
Epoch 1988 :: Batch 0/1
Batch Loss = 0.02458730107854685
1988, epoch_train_loss=0.02458730107854685
Epoch 1989
Epoch 1989 :: Batch 0/1
Batch Loss = 0.024576474764333042
1989, epoch_train_loss=0.024576474764333042
Epoch 1990
Epoch 1990 :: Batch 0/1
Batch Loss = 0.024565700787036515
1990, epoch_train_loss=0.024565700787036515
Epoch 1991
Epoch 1991 :: Batch 0/1
Batch Loss = 0.024554980472583445
1991, epoch_train_loss=0.024554980472583445
Epoch 1992
Epoch 1992 :: Batch 0/1
Batch Loss = 0.024544315076478283
1992, epoch_train_loss=0.024544315076478283
Epoch 1993
Epoch 1993 :: Batch 0/1
Batch Loss = 0.02453370577825188
1993, epoch_train_loss=0.02453370577825188
Epoch 1994
Epoch 1994 :: Batch 0/1
Batch Loss = 0.02452315367655233
1994, epoch_train_loss=0.02452315367655233
Epoch 1995
Epoch 1995 :: Batch 0/1
Batch Loss = 0.02451265978406883
1995, epoch_train_loss=0.02451265978406883
Epoch 1996
Epoch 1996 :: Batch 0/1
Batch Loss = 0.024502225027412165
1996, epoch_train_loss=0.024502225027412165
Epoch 1997
Epoch 1997 :: Batch 0/1
Batch Loss = 0.02449185023922729
1997, epoch_train_loss=0.02449185023922729
Epoch 1998
Epoch 1998 :: Batch 0/1
Batch Loss = 0.02448153615871608
1998, epoch_train_loss=0.02448153615871608
Epoch 1999
Epoch 1999 :: Batch 0/1
Batch Loss = 0.024471283431988993
1999, epoch_train_loss=0.024471283431988993
Epoch 2000
Epoch 2000 :: Batch 0/1
Batch Loss = 0.02446109260896927
2000, epoch_train_loss=0.02446109260896927
Epoch 2001
Epoch 2001 :: Batch 0/1
Batch Loss = 0.024450964144368332
2001, epoch_train_loss=0.024450964144368332
Epoch 2002
Epoch 2002 :: Batch 0/1
Batch Loss = 0.024440898397688312
2002, epoch_train_loss=0.024440898397688312
Epoch 2003
Epoch 2003 :: Batch 0/1
Batch Loss = 0.02443089563794632
2003, epoch_train_loss=0.02443089563794632
Epoch 2004
Epoch 2004 :: Batch 0/1
Batch Loss = 0.024420956040047005
2004, epoch_train_loss=0.024420956040047005
Epoch 2005
Epoch 2005 :: Batch 0/1
Batch Loss = 0.024411079692298325
2005, epoch_train_loss=0.024411079692298325
Epoch 2006
Epoch 2006 :: Batch 0/1
Batch Loss = 0.02440126659949846
2006, epoch_train_loss=0.02440126659949846
Epoch 2007
Epoch 2007 :: Batch 0/1
Batch Loss = 0.024391516681739053
2007, epoch_train_loss=0.024391516681739053
Epoch 2008
Epoch 2008 :: Batch 0/1
Batch Loss = 0.024381829783172996
2008, epoch_train_loss=0.024381829783172996
Epoch 2009
Epoch 2009 :: Batch 0/1
Batch Loss = 0.024372205675490068
2009, epoch_train_loss=0.024372205675490068
Epoch 2010
Epoch 2010 :: Batch 0/1
Batch Loss = 0.024362644060915137
2010, epoch_train_loss=0.024362644060915137
Epoch 2011
Epoch 2011 :: Batch 0/1
Batch Loss = 0.02435314457700707
2011, epoch_train_loss=0.02435314457700707
Epoch 2012
Epoch 2012 :: Batch 0/1
Batch Loss = 0.024343706803895696
2012, epoch_train_loss=0.024343706803895696
Epoch 2013
Epoch 2013 :: Batch 0/1
Batch Loss = 0.024334330266215124
2013, epoch_train_loss=0.024334330266215124
Epoch 2014
Epoch 2014 :: Batch 0/1
Batch Loss = 0.024325014441214
2014, epoch_train_loss=0.024325014441214
Epoch 2015
Epoch 2015 :: Batch 0/1
Batch Loss = 0.024315758760750022
2015, epoch_train_loss=0.024315758760750022
Epoch 2016
Epoch 2016 :: Batch 0/1
Batch Loss = 0.0243065626185407
2016, epoch_train_loss=0.0243065626185407
Epoch 2017
Epoch 2017 :: Batch 0/1
Batch Loss = 0.02429742537501117
2017, epoch_train_loss=0.02429742537501117
Epoch 2018
Epoch 2018 :: Batch 0/1
Batch Loss = 0.02428834635979631
2018, epoch_train_loss=0.02428834635979631
Epoch 2019
Epoch 2019 :: Batch 0/1
Batch Loss = 0.02427932487925792
2019, epoch_train_loss=0.02427932487925792
Epoch 2020
Epoch 2020 :: Batch 0/1
Batch Loss = 0.0242703602178902
2020, epoch_train_loss=0.0242703602178902
Epoch 2021
Epoch 2021 :: Batch 0/1
Batch Loss = 0.02426145164464934
2021, epoch_train_loss=0.02426145164464934
Epoch 2022
Epoch 2022 :: Batch 0/1
Batch Loss = 0.024252598416811738
2022, epoch_train_loss=0.024252598416811738
Epoch 2023
Epoch 2023 :: Batch 0/1
Batch Loss = 0.02424379978143702
2023, epoch_train_loss=0.02424379978143702
Epoch 2024
Epoch 2024 :: Batch 0/1
Batch Loss = 0.024235054981591564
2024, epoch_train_loss=0.024235054981591564
Epoch 2025
Epoch 2025 :: Batch 0/1
Batch Loss = 0.024226363255866766
2025, epoch_train_loss=0.024226363255866766
Epoch 2026
Epoch 2026 :: Batch 0/1
Batch Loss = 0.02421772384613453
2026, epoch_train_loss=0.02421772384613453
Epoch 2027
Epoch 2027 :: Batch 0/1
Batch Loss = 0.024209135995127112
2027, epoch_train_loss=0.024209135995127112
Epoch 2028
Epoch 2028 :: Batch 0/1
Batch Loss = 0.024200598951513556
2028, epoch_train_loss=0.024200598951513556
Epoch 2029
Epoch 2029 :: Batch 0/1
Batch Loss = 0.02419211197118376
2029, epoch_train_loss=0.02419211197118376
Epoch 2030
Epoch 2030 :: Batch 0/1
Batch Loss = 0.024183674318984053
2030, epoch_train_loss=0.024183674318984053
Epoch 2031
Epoch 2031 :: Batch 0/1
Batch Loss = 0.024175285270889017
2031, epoch_train_loss=0.024175285270889017
Epoch 2032
Epoch 2032 :: Batch 0/1
Batch Loss = 0.024166944112543033
2032, epoch_train_loss=0.024166944112543033
Epoch 2033
Epoch 2033 :: Batch 0/1
Batch Loss = 0.024158650145061362
2033, epoch_train_loss=0.024158650145061362
Epoch 2034
Epoch 2034 :: Batch 0/1
Batch Loss = 0.024150402680459294
2034, epoch_train_loss=0.024150402680459294
Epoch 2035
Epoch 2035 :: Batch 0/1
Batch Loss = 0.024142201047693806
2035, epoch_train_loss=0.024142201047693806
Epoch 2036
Epoch 2036 :: Batch 0/1
Batch Loss = 0.024134044587796662
2036, epoch_train_loss=0.024134044587796662
Epoch 2037
Epoch 2037 :: Batch 0/1
Batch Loss = 0.02412593265953173
2037, epoch_train_loss=0.02412593265953173
Epoch 2038
Epoch 2038 :: Batch 0/1
Batch Loss = 0.02411786463429374
2038, epoch_train_loss=0.02411786463429374
Epoch 2039
Epoch 2039 :: Batch 0/1
Batch Loss = 0.024109839900795446
2039, epoch_train_loss=0.024109839900795446
Epoch 2040
Epoch 2040 :: Batch 0/1
Batch Loss = 0.024101857862394924
2040, epoch_train_loss=0.024101857862394924
Epoch 2041
Epoch 2041 :: Batch 0/1
Batch Loss = 0.02409391793762324
2041, epoch_train_loss=0.02409391793762324
Epoch 2042
Epoch 2042 :: Batch 0/1
Batch Loss = 0.024086019559972426
2042, epoch_train_loss=0.024086019559972426
Epoch 2043
Epoch 2043 :: Batch 0/1
Batch Loss = 0.02407816217761564
2043, epoch_train_loss=0.02407816217761564
Epoch 2044
Epoch 2044 :: Batch 0/1
Batch Loss = 0.02407034525307147
2044, epoch_train_loss=0.02407034525307147
Epoch 2045
Epoch 2045 :: Batch 0/1
Batch Loss = 0.024062568262192812
2045, epoch_train_loss=0.024062568262192812
Epoch 2046
Epoch 2046 :: Batch 0/1
Batch Loss = 0.02405483069564739
2046, epoch_train_loss=0.02405483069564739
Epoch 2047
Epoch 2047 :: Batch 0/1
Batch Loss = 0.02404713205595047
2047, epoch_train_loss=0.02404713205595047
Epoch 2048
Epoch 2048 :: Batch 0/1
Batch Loss = 0.024039471860138908
2048, epoch_train_loss=0.024039471860138908
Epoch 2049
Epoch 2049 :: Batch 0/1
Batch Loss = 0.024031849634932056
2049, epoch_train_loss=0.024031849634932056
Epoch 2050
Epoch 2050 :: Batch 0/1
Batch Loss = 0.02402426492061735
2050, epoch_train_loss=0.02402426492061735
Epoch 2051
Epoch 2051 :: Batch 0/1
Batch Loss = 0.02401671726808604
2051, epoch_train_loss=0.02401671726808604
Epoch 2052
Epoch 2052 :: Batch 0/1
Batch Loss = 0.024009206238377646
2052, epoch_train_loss=0.024009206238377646
Epoch 2053
Epoch 2053 :: Batch 0/1
Batch Loss = 0.0240017314040516
2053, epoch_train_loss=0.0240017314040516
Epoch 2054
Epoch 2054 :: Batch 0/1
Batch Loss = 0.023994292346299465
2054, epoch_train_loss=0.023994292346299465
Epoch 2055
Epoch 2055 :: Batch 0/1
Batch Loss = 0.023986888656939764
2055, epoch_train_loss=0.023986888656939764
Epoch 2056
Epoch 2056 :: Batch 0/1
Batch Loss = 0.023979519936182365
2056, epoch_train_loss=0.023979519936182365
Epoch 2057
Epoch 2057 :: Batch 0/1
Batch Loss = 0.023972185792242817
2057, epoch_train_loss=0.023972185792242817
Epoch 2058
Epoch 2058 :: Batch 0/1
Batch Loss = 0.023964885842159767
2058, epoch_train_loss=0.023964885842159767
Epoch 2059
Epoch 2059 :: Batch 0/1
Batch Loss = 0.02395761971142232
2059, epoch_train_loss=0.02395761971142232
Epoch 2060
Epoch 2060 :: Batch 0/1
Batch Loss = 0.02395038703185719
2060, epoch_train_loss=0.02395038703185719
Epoch 2061
Epoch 2061 :: Batch 0/1
Batch Loss = 0.02394318744366937
2061, epoch_train_loss=0.02394318744366937
Epoch 2062
Epoch 2062 :: Batch 0/1
Batch Loss = 0.02393602059279098
2062, epoch_train_loss=0.02393602059279098
Epoch 2063
Epoch 2063 :: Batch 0/1
Batch Loss = 0.023928886132360015
2063, epoch_train_loss=0.023928886132360015
Epoch 2064
Epoch 2064 :: Batch 0/1
Batch Loss = 0.023921783722444894
2064, epoch_train_loss=0.023921783722444894
Epoch 2065
Epoch 2065 :: Batch 0/1
Batch Loss = 0.023914713028065512
2065, epoch_train_loss=0.023914713028065512
Epoch 2066
Epoch 2066 :: Batch 0/1
Batch Loss = 0.02390767372070112
2066, epoch_train_loss=0.02390767372070112
Epoch 2067
Epoch 2067 :: Batch 0/1
Batch Loss = 0.023900665478065526
2067, epoch_train_loss=0.023900665478065526
Epoch 2068
Epoch 2068 :: Batch 0/1
Batch Loss = 0.02389368798219857
2068, epoch_train_loss=0.02389368798219857
Epoch 2069
Epoch 2069 :: Batch 0/1
Batch Loss = 0.02388674092099607
2069, epoch_train_loss=0.02388674092099607
Epoch 2070
Epoch 2070 :: Batch 0/1
Batch Loss = 0.023879823987465975
2070, epoch_train_loss=0.023879823987465975
Epoch 2071
Epoch 2071 :: Batch 0/1
Batch Loss = 0.023872936880128395
2071, epoch_train_loss=0.023872936880128395
Epoch 2072
Epoch 2072 :: Batch 0/1
Batch Loss = 0.02386607930063043
2072, epoch_train_loss=0.02386607930063043
Epoch 2073
Epoch 2073 :: Batch 0/1
Batch Loss = 0.023859250957521602
2073, epoch_train_loss=0.023859250957521602
Epoch 2074
Epoch 2074 :: Batch 0/1
Batch Loss = 0.0238524515622193
2074, epoch_train_loss=0.0238524515622193
Epoch 2075
Epoch 2075 :: Batch 0/1
Batch Loss = 0.023845680831120784
2075, epoch_train_loss=0.023845680831120784
Epoch 2076
Epoch 2076 :: Batch 0/1
Batch Loss = 0.02383893848493453
2076, epoch_train_loss=0.02383893848493453
Epoch 2077
Epoch 2077 :: Batch 0/1
Batch Loss = 0.023832224248577137
2077, epoch_train_loss=0.023832224248577137
Epoch 2078
Epoch 2078 :: Batch 0/1
Batch Loss = 0.023825537851076935
2078, epoch_train_loss=0.023825537851076935
Epoch 2079
Epoch 2079 :: Batch 0/1
Batch Loss = 0.02381887902548402
2079, epoch_train_loss=0.02381887902548402
Epoch 2080
Epoch 2080 :: Batch 0/1
Batch Loss = 0.02381224750878591
2080, epoch_train_loss=0.02381224750878591
Epoch 2081
Epoch 2081 :: Batch 0/1
Batch Loss = 0.023805643041828604
2081, epoch_train_loss=0.023805643041828604
Epoch 2082
Epoch 2082 :: Batch 0/1
Batch Loss = 0.023799065369242235
2082, epoch_train_loss=0.023799065369242235
Epoch 2083
Epoch 2083 :: Batch 0/1
Batch Loss = 0.023792514239371426
2083, epoch_train_loss=0.023792514239371426
Epoch 2084
Epoch 2084 :: Batch 0/1
Batch Loss = 0.023785989403679455
2084, epoch_train_loss=0.023785989403679455
Epoch 2085
Epoch 2085 :: Batch 0/1
Batch Loss = 0.023779490618279497
2085, epoch_train_loss=0.023779490618279497
Epoch 2086
Epoch 2086 :: Batch 0/1
Batch Loss = 0.02377301764280515
2086, epoch_train_loss=0.02377301764280515
Epoch 2087
Epoch 2087 :: Batch 0/1
Batch Loss = 0.023766570239307356
2087, epoch_train_loss=0.023766570239307356
Epoch 2088
Epoch 2088 :: Batch 0/1
Batch Loss = 0.023760148173263947
2088, epoch_train_loss=0.023760148173263947
Epoch 2089
Epoch 2089 :: Batch 0/1
Batch Loss = 0.023753751214568873
2089, epoch_train_loss=0.023753751214568873
Epoch 2090
Epoch 2090 :: Batch 0/1
Batch Loss = 0.023747379135394996
2090, epoch_train_loss=0.023747379135394996
Epoch 2091
Epoch 2091 :: Batch 0/1
Batch Loss = 0.023741031712229646
2091, epoch_train_loss=0.023741031712229646
Epoch 2092
Epoch 2092 :: Batch 0/1
Batch Loss = 0.02373470872427148
2092, epoch_train_loss=0.02373470872427148
Epoch 2093
Epoch 2093 :: Batch 0/1
Batch Loss = 0.023728409953909273
2093, epoch_train_loss=0.023728409953909273
Epoch 2094
Epoch 2094 :: Batch 0/1
Batch Loss = 0.02372213518617176
2094, epoch_train_loss=0.02372213518617176
Epoch 2095
Epoch 2095 :: Batch 0/1
Batch Loss = 0.023715884210221434
2095, epoch_train_loss=0.023715884210221434
Epoch 2096
Epoch 2096 :: Batch 0/1
Batch Loss = 0.02370965681727494
2096, epoch_train_loss=0.02370965681727494
Epoch 2097
Epoch 2097 :: Batch 0/1
Batch Loss = 0.023703452802600798
2097, epoch_train_loss=0.023703452802600798
Epoch 2098
Epoch 2098 :: Batch 0/1
Batch Loss = 0.023697271963959515
2098, epoch_train_loss=0.023697271963959515
Epoch 2099
Epoch 2099 :: Batch 0/1
Batch Loss = 0.023691114102077967
2099, epoch_train_loss=0.023691114102077967
Epoch 2100
Epoch 2100 :: Batch 0/1
Batch Loss = 0.023684979020116733
2100, epoch_train_loss=0.023684979020116733
Epoch 2101
Epoch 2101 :: Batch 0/1
Batch Loss = 0.023678866524638877
2101, epoch_train_loss=0.023678866524638877
Epoch 2102
Epoch 2102 :: Batch 0/1
Batch Loss = 0.02367277642556871
2102, epoch_train_loss=0.02367277642556871
Epoch 2103
Epoch 2103 :: Batch 0/1
Batch Loss = 0.023666708534671177
2103, epoch_train_loss=0.023666708534671177
Epoch 2104
Epoch 2104 :: Batch 0/1
Batch Loss = 0.02366066266750212
2104, epoch_train_loss=0.02366066266750212
Epoch 2105
Epoch 2105 :: Batch 0/1
Batch Loss = 0.023654638641401025
2105, epoch_train_loss=0.023654638641401025
Epoch 2106
Epoch 2106 :: Batch 0/1
Batch Loss = 0.023648636277427325
2106, epoch_train_loss=0.023648636277427325
Epoch 2107
Epoch 2107 :: Batch 0/1
Batch Loss = 0.02364265539836734
2107, epoch_train_loss=0.02364265539836734
Epoch 2108
Epoch 2108 :: Batch 0/1
Batch Loss = 0.023636695830172045
2108, epoch_train_loss=0.023636695830172045
Epoch 2109
Epoch 2109 :: Batch 0/1
Batch Loss = 0.02363075740191712
2109, epoch_train_loss=0.02363075740191712
Epoch 2110
Epoch 2110 :: Batch 0/1
Batch Loss = 0.0236248399443193
2110, epoch_train_loss=0.0236248399443193
Epoch 2111
Epoch 2111 :: Batch 0/1
Batch Loss = 0.02361894329115812
2111, epoch_train_loss=0.02361894329115812
Epoch 2112
Epoch 2112 :: Batch 0/1
Batch Loss = 0.023613067278757525
2112, epoch_train_loss=0.023613067278757525
Epoch 2113
Epoch 2113 :: Batch 0/1
Batch Loss = 0.02360721174595279
2113, epoch_train_loss=0.02360721174595279
Epoch 2114
Epoch 2114 :: Batch 0/1
Batch Loss = 0.023601376534057084
2114, epoch_train_loss=0.023601376534057084
Epoch 2115
Epoch 2115 :: Batch 0/1
Batch Loss = 0.023595561487300576
2115, epoch_train_loss=0.023595561487300576
Epoch 2116
Epoch 2116 :: Batch 0/1
Batch Loss = 0.023589766451374516
2116, epoch_train_loss=0.023589766451374516
Epoch 2117
Epoch 2117 :: Batch 0/1
Batch Loss = 0.02358399127482063
2117, epoch_train_loss=0.02358399127482063
Epoch 2118
Epoch 2118 :: Batch 0/1
Batch Loss = 0.023578235808520313
2118, epoch_train_loss=0.023578235808520313
Epoch 2119
Epoch 2119 :: Batch 0/1
Batch Loss = 0.02357249990565894
2119, epoch_train_loss=0.02357249990565894
Epoch 2120
Epoch 2120 :: Batch 0/1
Batch Loss = 0.02356678342168951
2120, epoch_train_loss=0.02356678342168951
Epoch 2121
Epoch 2121 :: Batch 0/1
Batch Loss = 0.023561086214295972
2121, epoch_train_loss=0.023561086214295972
Epoch 2122
Epoch 2122 :: Batch 0/1
Batch Loss = 0.023555408143355997
2122, epoch_train_loss=0.023555408143355997
Epoch 2123
Epoch 2123 :: Batch 0/1
Batch Loss = 0.023549749070903153
2123, epoch_train_loss=0.023549749070903153
Epoch 2124
Epoch 2124 :: Batch 0/1
Batch Loss = 0.02354410886108879
2124, epoch_train_loss=0.02354410886108879
Epoch 2125
Epoch 2125 :: Batch 0/1
Batch Loss = 0.023538487380143264
2125, epoch_train_loss=0.023538487380143264
Epoch 2126
Epoch 2126 :: Batch 0/1
Batch Loss = 0.02353288449633689
2126, epoch_train_loss=0.02353288449633689
Epoch 2127
Epoch 2127 :: Batch 0/1
Batch Loss = 0.023527300079940326
2127, epoch_train_loss=0.023527300079940326
Epoch 2128
Epoch 2128 :: Batch 0/1
Batch Loss = 0.023521734003184653
2128, epoch_train_loss=0.023521734003184653
Epoch 2129
Epoch 2129 :: Batch 0/1
Batch Loss = 0.023516186140221048
2129, epoch_train_loss=0.023516186140221048
Epoch 2130
Epoch 2130 :: Batch 0/1
Batch Loss = 0.023510656367080114
2130, epoch_train_loss=0.023510656367080114
Epoch 2131
Epoch 2131 :: Batch 0/1
Batch Loss = 0.02350514456118224
2131, epoch_train_loss=0.02350514456118224
Epoch 2132
Epoch 2132 :: Batch 0/1
Batch Loss = 0.023499650602644785
2132, epoch_train_loss=0.023499650602644785
Epoch 2133
Epoch 2133 :: Batch 0/1
Batch Loss = 0.02349417437288984
2133, epoch_train_loss=0.02349417437288984
Epoch 2134
Epoch 2134 :: Batch 0/1
Batch Loss = 0.0234887157550536
2134, epoch_train_loss=0.0234887157550536
Epoch 2135
Epoch 2135 :: Batch 0/1
Batch Loss = 0.023483274633944576
2135, epoch_train_loss=0.023483274633944576
Epoch 2136
Epoch 2136 :: Batch 0/1
Batch Loss = 0.023477850896001615
2136, epoch_train_loss=0.023477850896001615
Epoch 2137
Epoch 2137 :: Batch 0/1
Batch Loss = 0.023472444428811766
2137, epoch_train_loss=0.023472444428811766
Epoch 2138
Epoch 2138 :: Batch 0/1
Batch Loss = 0.023467055122391157
2138, epoch_train_loss=0.023467055122391157
Epoch 2139
Epoch 2139 :: Batch 0/1
Batch Loss = 0.023461682867817888
2139, epoch_train_loss=0.023461682867817888
Epoch 2140
Epoch 2140 :: Batch 0/1
Batch Loss = 0.0234563275571965
2140, epoch_train_loss=0.0234563275571965
Epoch 2141
Epoch 2141 :: Batch 0/1
Batch Loss = 0.02345098908492669
2141, epoch_train_loss=0.02345098908492669
Epoch 2142
Epoch 2142 :: Batch 0/1
Batch Loss = 0.023445667345915595
2142, epoch_train_loss=0.023445667345915595
Epoch 2143
Epoch 2143 :: Batch 0/1
Batch Loss = 0.02344036223684502
2143, epoch_train_loss=0.02344036223684502
Epoch 2144
Epoch 2144 :: Batch 0/1
Batch Loss = 0.023435073656124142
2144, epoch_train_loss=0.023435073656124142
Epoch 2145
Epoch 2145 :: Batch 0/1
Batch Loss = 0.023429801502983648
2145, epoch_train_loss=0.023429801502983648
Epoch 2146
Epoch 2146 :: Batch 0/1
Batch Loss = 0.02342454567786954
2146, epoch_train_loss=0.02342454567786954
Epoch 2147
Epoch 2147 :: Batch 0/1
Batch Loss = 0.023419306081976128
2147, epoch_train_loss=0.023419306081976128
Epoch 2148
Epoch 2148 :: Batch 0/1
Batch Loss = 0.023414082618063613
2148, epoch_train_loss=0.023414082618063613
Epoch 2149
Epoch 2149 :: Batch 0/1
Batch Loss = 0.023408875189988604
2149, epoch_train_loss=0.023408875189988604
Epoch 2150
Epoch 2150 :: Batch 0/1
Batch Loss = 0.023403683703088533
2150, epoch_train_loss=0.023403683703088533
Epoch 2151
Epoch 2151 :: Batch 0/1
Batch Loss = 0.02339850806287259
2151, epoch_train_loss=0.02339850806287259
Epoch 2152
Epoch 2152 :: Batch 0/1
Batch Loss = 0.02339334817667779
2152, epoch_train_loss=0.02339334817667779
Epoch 2153
Epoch 2153 :: Batch 0/1
Batch Loss = 0.02338820395194382
2153, epoch_train_loss=0.02338820395194382
Epoch 2154
Epoch 2154 :: Batch 0/1
Batch Loss = 0.02338307529744239
2154, epoch_train_loss=0.02338307529744239
Epoch 2155
Epoch 2155 :: Batch 0/1
Batch Loss = 0.023377962122819416
2155, epoch_train_loss=0.023377962122819416
Epoch 2156
Epoch 2156 :: Batch 0/1
Batch Loss = 0.02337286433856044
2156, epoch_train_loss=0.02337286433856044
Epoch 2157
Epoch 2157 :: Batch 0/1
Batch Loss = 0.023367781856371124
2157, epoch_train_loss=0.023367781856371124
Epoch 2158
Epoch 2158 :: Batch 0/1
Batch Loss = 0.023362714588312548
2158, epoch_train_loss=0.023362714588312548
Epoch 2159
Epoch 2159 :: Batch 0/1
Batch Loss = 0.023357662446774913
2159, epoch_train_loss=0.023357662446774913
Epoch 2160
Epoch 2160 :: Batch 0/1
Batch Loss = 0.023352625344862505
2160, epoch_train_loss=0.023352625344862505
Epoch 2161
Epoch 2161 :: Batch 0/1
Batch Loss = 0.02334760319718217
2161, epoch_train_loss=0.02334760319718217
Epoch 2162
Epoch 2162 :: Batch 0/1
Batch Loss = 0.02334259591898655
2162, epoch_train_loss=0.02334259591898655
Epoch 2163
Epoch 2163 :: Batch 0/1
Batch Loss = 0.023337603425331827
2163, epoch_train_loss=0.023337603425331827
Epoch 2164
Epoch 2164 :: Batch 0/1
Batch Loss = 0.02333262563227869
2164, epoch_train_loss=0.02333262563227869
Epoch 2165
Epoch 2165 :: Batch 0/1
Batch Loss = 0.02332766245645598
2165, epoch_train_loss=0.02332766245645598
Epoch 2166
Epoch 2166 :: Batch 0/1
Batch Loss = 0.023322713815035305
2166, epoch_train_loss=0.023322713815035305
Epoch 2167
Epoch 2167 :: Batch 0/1
Batch Loss = 0.02331777962570687
2167, epoch_train_loss=0.02331777962570687
Epoch 2168
Epoch 2168 :: Batch 0/1
Batch Loss = 0.023312859806656143
2168, epoch_train_loss=0.023312859806656143
Epoch 2169
Epoch 2169 :: Batch 0/1
Batch Loss = 0.02330795427654174
2169, epoch_train_loss=0.02330795427654174
Epoch 2170
Epoch 2170 :: Batch 0/1
Batch Loss = 0.023303062954474426
2170, epoch_train_loss=0.023303062954474426
Epoch 2171
Epoch 2171 :: Batch 0/1
Batch Loss = 0.023298185759996936
2171, epoch_train_loss=0.023298185759996936
Epoch 2172
Epoch 2172 :: Batch 0/1
Batch Loss = 0.023293322613065198
2172, epoch_train_loss=0.023293322613065198
Epoch 2173
Epoch 2173 :: Batch 0/1
Batch Loss = 0.023288473433634913
2173, epoch_train_loss=0.023288473433634913
Epoch 2174
Epoch 2174 :: Batch 0/1
Batch Loss = 0.023283638143227505
2174, epoch_train_loss=0.023283638143227505
Epoch 2175
Epoch 2175 :: Batch 0/1
Batch Loss = 0.023278816662146113
2175, epoch_train_loss=0.023278816662146113
Epoch 2176
Epoch 2176 :: Batch 0/1
Batch Loss = 0.02327400891222664
2176, epoch_train_loss=0.02327400891222664
Epoch 2177
Epoch 2177 :: Batch 0/1
Batch Loss = 0.023269214815244608
2177, epoch_train_loss=0.023269214815244608
Epoch 2178
Epoch 2178 :: Batch 0/1
Batch Loss = 0.023264434293297782
2178, epoch_train_loss=0.023264434293297782
Epoch 2179
Epoch 2179 :: Batch 0/1
Batch Loss = 0.023259667268405827
2179, epoch_train_loss=0.023259667268405827
Epoch 2180
Epoch 2180 :: Batch 0/1
Batch Loss = 0.0232549136636684
2180, epoch_train_loss=0.0232549136636684
Epoch 2181
Epoch 2181 :: Batch 0/1
Batch Loss = 0.023250173402085537
2181, epoch_train_loss=0.023250173402085537
Epoch 2182
Epoch 2182 :: Batch 0/1
Batch Loss = 0.023245446406554383
2182, epoch_train_loss=0.023245446406554383
Epoch 2183
Epoch 2183 :: Batch 0/1
Batch Loss = 0.023240732601020435
2183, epoch_train_loss=0.023240732601020435
Epoch 2184
Epoch 2184 :: Batch 0/1
Batch Loss = 0.023236031909310853
2184, epoch_train_loss=0.023236031909310853
Epoch 2185
Epoch 2185 :: Batch 0/1
Batch Loss = 0.02323134425513375
2185, epoch_train_loss=0.02323134425513375
Epoch 2186
Epoch 2186 :: Batch 0/1
Batch Loss = 0.023226669563223105
2186, epoch_train_loss=0.023226669563223105
Epoch 2187
Epoch 2187 :: Batch 0/1
Batch Loss = 0.023222007757803684
2187, epoch_train_loss=0.023222007757803684
Epoch 2188
Epoch 2188 :: Batch 0/1
Batch Loss = 0.023217358764115636
2188, epoch_train_loss=0.023217358764115636
Epoch 2189
Epoch 2189 :: Batch 0/1
Batch Loss = 0.023212722507267945
2189, epoch_train_loss=0.023212722507267945
Epoch 2190
Epoch 2190 :: Batch 0/1
Batch Loss = 0.023208098912242188
2190, epoch_train_loss=0.023208098912242188
Epoch 2191
Epoch 2191 :: Batch 0/1
Batch Loss = 0.023203487904649742
2191, epoch_train_loss=0.023203487904649742
Epoch 2192
Epoch 2192 :: Batch 0/1
Batch Loss = 0.023198889410727796
2192, epoch_train_loss=0.023198889410727796
Epoch 2193
Epoch 2193 :: Batch 0/1
Batch Loss = 0.02319430335621286
2193, epoch_train_loss=0.02319430335621286
Epoch 2194
Epoch 2194 :: Batch 0/1
Batch Loss = 0.023189729667845677
2194, epoch_train_loss=0.023189729667845677
Epoch 2195
Epoch 2195 :: Batch 0/1
Batch Loss = 0.023185168271875167
2195, epoch_train_loss=0.023185168271875167
Epoch 2196
Epoch 2196 :: Batch 0/1
Batch Loss = 0.023180619095185304
2196, epoch_train_loss=0.023180619095185304
Epoch 2197
Epoch 2197 :: Batch 0/1
Batch Loss = 0.023176082065294604
2197, epoch_train_loss=0.023176082065294604
Epoch 2198
Epoch 2198 :: Batch 0/1
Batch Loss = 0.023171557109247922
2198, epoch_train_loss=0.023171557109247922
Epoch 2199
Epoch 2199 :: Batch 0/1
Batch Loss = 0.02316704415473618
2199, epoch_train_loss=0.02316704415473618
Epoch 2200
Epoch 2200 :: Batch 0/1
Batch Loss = 0.02316254312973015
2200, epoch_train_loss=0.02316254312973015
Epoch 2201
Epoch 2201 :: Batch 0/1
Batch Loss = 0.023158053962853333
2201, epoch_train_loss=0.023158053962853333
Epoch 2202
Epoch 2202 :: Batch 0/1
Batch Loss = 0.023153576582287436
2202, epoch_train_loss=0.023153576582287436
Epoch 2203
Epoch 2203 :: Batch 0/1
Batch Loss = 0.02314911091688247
2203, epoch_train_loss=0.02314911091688247
Epoch 2204
Epoch 2204 :: Batch 0/1
Batch Loss = 0.023144656895796232
2204, epoch_train_loss=0.023144656895796232
Epoch 2205
Epoch 2205 :: Batch 0/1
Batch Loss = 0.0231402144485024
2205, epoch_train_loss=0.0231402144485024
Epoch 2206
Epoch 2206 :: Batch 0/1
Batch Loss = 0.023135783505160778
2206, epoch_train_loss=0.023135783505160778
Epoch 2207
Epoch 2207 :: Batch 0/1
Batch Loss = 0.023131363995177748
2207, epoch_train_loss=0.023131363995177748
Epoch 2208
Epoch 2208 :: Batch 0/1
Batch Loss = 0.023126955849748634
2208, epoch_train_loss=0.023126955849748634
Epoch 2209
Epoch 2209 :: Batch 0/1
Batch Loss = 0.023122558998976625
2209, epoch_train_loss=0.023122558998976625
Epoch 2210
Epoch 2210 :: Batch 0/1
Batch Loss = 0.023118173374765294
2210, epoch_train_loss=0.023118173374765294
Epoch 2211
Epoch 2211 :: Batch 0/1
Batch Loss = 0.02311379890795251
2211, epoch_train_loss=0.02311379890795251
Epoch 2212
Epoch 2212 :: Batch 0/1
Batch Loss = 0.023109435531188893
2212, epoch_train_loss=0.023109435531188893
Epoch 2213
Epoch 2213 :: Batch 0/1
Batch Loss = 0.023105083176086408
2213, epoch_train_loss=0.023105083176086408
Epoch 2214
Epoch 2214 :: Batch 0/1
Batch Loss = 0.023100741776082793
2214, epoch_train_loss=0.023100741776082793
Epoch 2215
Epoch 2215 :: Batch 0/1
Batch Loss = 0.023096411263604618
2215, epoch_train_loss=0.023096411263604618
Epoch 2216
Epoch 2216 :: Batch 0/1
Batch Loss = 0.02309209157256466
2216, epoch_train_loss=0.02309209157256466
Epoch 2217
Epoch 2217 :: Batch 0/1
Batch Loss = 0.02308778263695174
2217, epoch_train_loss=0.02308778263695174
Epoch 2218
Epoch 2218 :: Batch 0/1
Batch Loss = 0.023083484391196226
2218, epoch_train_loss=0.023083484391196226
Epoch 2219
Epoch 2219 :: Batch 0/1
Batch Loss = 0.023079196770180496
2219, epoch_train_loss=0.023079196770180496
Epoch 2220
Epoch 2220 :: Batch 0/1
Batch Loss = 0.023074919709249114
2220, epoch_train_loss=0.023074919709249114
Epoch 2221
Epoch 2221 :: Batch 0/1
Batch Loss = 0.023070653144219196
2221, epoch_train_loss=0.023070653144219196
Epoch 2222
Epoch 2222 :: Batch 0/1
Batch Loss = 0.02306639701104263
2222, epoch_train_loss=0.02306639701104263
Epoch 2223
Epoch 2223 :: Batch 0/1
Batch Loss = 0.023062151246861208
2223, epoch_train_loss=0.023062151246861208
Epoch 2224
Epoch 2224 :: Batch 0/1
Batch Loss = 0.02305791578862394
2224, epoch_train_loss=0.02305791578862394
Epoch 2225
Epoch 2225 :: Batch 0/1
Batch Loss = 0.023053690574486186
2225, epoch_train_loss=0.023053690574486186
Epoch 2226
Epoch 2226 :: Batch 0/1
Batch Loss = 0.023049475542777875
2226, epoch_train_loss=0.023049475542777875
Epoch 2227
Epoch 2227 :: Batch 0/1
Batch Loss = 0.02304527063236056
2227, epoch_train_loss=0.02304527063236056
Epoch 2228
Epoch 2228 :: Batch 0/1
Batch Loss = 0.02304107578263671
2228, epoch_train_loss=0.02304107578263671
Epoch 2229
Epoch 2229 :: Batch 0/1
Batch Loss = 0.0230368909332166
2229, epoch_train_loss=0.0230368909332166
Epoch 2230
Epoch 2230 :: Batch 0/1
Batch Loss = 0.023032716024955005
2230, epoch_train_loss=0.023032716024955005
Epoch 2231
Epoch 2231 :: Batch 0/1
Batch Loss = 0.023028550998590105
2231, epoch_train_loss=0.023028550998590105
Epoch 2232
Epoch 2232 :: Batch 0/1
Batch Loss = 0.023024395795778488
2232, epoch_train_loss=0.023024395795778488
Epoch 2233
Epoch 2233 :: Batch 0/1
Batch Loss = 0.023020250359099645
2233, epoch_train_loss=0.023020250359099645
Epoch 2234
Epoch 2234 :: Batch 0/1
Batch Loss = 0.023016114631045404
2234, epoch_train_loss=0.023016114631045404
Epoch 2235
Epoch 2235 :: Batch 0/1
Batch Loss = 0.023011988555384475
2235, epoch_train_loss=0.023011988555384475
Epoch 2236
Epoch 2236 :: Batch 0/1
Batch Loss = 0.023007872076152416
2236, epoch_train_loss=0.023007872076152416
Epoch 2237
Epoch 2237 :: Batch 0/1
Batch Loss = 0.02300376513766208
2237, epoch_train_loss=0.02300376513766208
Epoch 2238
Epoch 2238 :: Batch 0/1
Batch Loss = 0.022999667685184088
2238, epoch_train_loss=0.022999667685184088
Epoch 2239
Epoch 2239 :: Batch 0/1
Batch Loss = 0.02299557966494944
2239, epoch_train_loss=0.02299557966494944
Epoch 2240
Epoch 2240 :: Batch 0/1
Batch Loss = 0.02299150102315104
2240, epoch_train_loss=0.02299150102315104
Epoch 2241
Epoch 2241 :: Batch 0/1
Batch Loss = 0.022987431706954845
2241, epoch_train_loss=0.022987431706954845
Epoch 2242
Epoch 2242 :: Batch 0/1
Batch Loss = 0.022983371664501637
2242, epoch_train_loss=0.022983371664501637
Epoch 2243
Epoch 2243 :: Batch 0/1
Batch Loss = 0.02297932084391412
2243, epoch_train_loss=0.02297932084391412
Epoch 2244
Epoch 2244 :: Batch 0/1
Batch Loss = 0.022975279194300268
2244, epoch_train_loss=0.022975279194300268
Epoch 2245
Epoch 2245 :: Batch 0/1
Batch Loss = 0.0229712466657541
2245, epoch_train_loss=0.0229712466657541
Epoch 2246
Epoch 2246 :: Batch 0/1
Batch Loss = 0.022967223208039025
2246, epoch_train_loss=0.022967223208039025
Epoch 2247
Epoch 2247 :: Batch 0/1
Batch Loss = 0.02296320877290089
2247, epoch_train_loss=0.02296320877290089
Epoch 2248
Epoch 2248 :: Batch 0/1
Batch Loss = 0.022959203311763036
2248, epoch_train_loss=0.022959203311763036
Epoch 2249
Epoch 2249 :: Batch 0/1
Batch Loss = 0.02295520677704716
2249, epoch_train_loss=0.02295520677704716
Epoch 2250
Epoch 2250 :: Batch 0/1
Batch Loss = 0.02295121912184594
2250, epoch_train_loss=0.02295121912184594
Epoch 2251
Epoch 2251 :: Batch 0/1
Batch Loss = 0.022947240299924787
2251, epoch_train_loss=0.022947240299924787
Epoch 2252
Epoch 2252 :: Batch 0/1
Batch Loss = 0.02294327026604835
2252, epoch_train_loss=0.02294327026604835
Epoch 2253
Epoch 2253 :: Batch 0/1
Batch Loss = 0.022939308974681293
2253, epoch_train_loss=0.022939308974681293
Epoch 2254
Epoch 2254 :: Batch 0/1
Batch Loss = 0.02293535638194013
2254, epoch_train_loss=0.02293535638194013
Epoch 2255
Epoch 2255 :: Batch 0/1
Batch Loss = 0.02293141244461524
2255, epoch_train_loss=0.02293141244461524
Epoch 2256
Epoch 2256 :: Batch 0/1
Batch Loss = 0.022927477119526554
2256, epoch_train_loss=0.022927477119526554
Epoch 2257
Epoch 2257 :: Batch 0/1
Batch Loss = 0.022923550364172565
2257, epoch_train_loss=0.022923550364172565
Epoch 2258
Epoch 2258 :: Batch 0/1
Batch Loss = 0.022919632137371113
2258, epoch_train_loss=0.022919632137371113
Epoch 2259
Epoch 2259 :: Batch 0/1
Batch Loss = 0.022915722398291635
2259, epoch_train_loss=0.022915722398291635
Epoch 2260
Epoch 2260 :: Batch 0/1
Batch Loss = 0.022911821107095737
2260, epoch_train_loss=0.022911821107095737
Epoch 2261
Epoch 2261 :: Batch 0/1
Batch Loss = 0.0229079282236572
2261, epoch_train_loss=0.0229079282236572
Epoch 2262
Epoch 2262 :: Batch 0/1
Batch Loss = 0.022904043709477715
2262, epoch_train_loss=0.022904043709477715
Epoch 2263
Epoch 2263 :: Batch 0/1
Batch Loss = 0.022900167526405474
2263, epoch_train_loss=0.022900167526405474
Epoch 2264
Epoch 2264 :: Batch 0/1
Batch Loss = 0.02289629963695213
2264, epoch_train_loss=0.02289629963695213
Epoch 2265
Epoch 2265 :: Batch 0/1
Batch Loss = 0.02289244000428968
2265, epoch_train_loss=0.02289244000428968
Epoch 2266
Epoch 2266 :: Batch 0/1
Batch Loss = 0.022888588591931623
2266, epoch_train_loss=0.022888588591931623
Epoch 2267
Epoch 2267 :: Batch 0/1
Batch Loss = 0.022884745364676534
2267, epoch_train_loss=0.022884745364676534
Epoch 2268
Epoch 2268 :: Batch 0/1
Batch Loss = 0.02288091028765557
2268, epoch_train_loss=0.02288091028765557
Epoch 2269
Epoch 2269 :: Batch 0/1
Batch Loss = 0.022877083326331135
2269, epoch_train_loss=0.022877083326331135
Epoch 2270
Epoch 2270 :: Batch 0/1
Batch Loss = 0.02287326444743372
2270, epoch_train_loss=0.02287326444743372
Epoch 2271
Epoch 2271 :: Batch 0/1
Batch Loss = 0.022869453618014395
2271, epoch_train_loss=0.022869453618014395
Epoch 2272
Epoch 2272 :: Batch 0/1
Batch Loss = 0.02286565080544267
2272, epoch_train_loss=0.02286565080544267
Epoch 2273
Epoch 2273 :: Batch 0/1
Batch Loss = 0.02286185597833664
2273, epoch_train_loss=0.02286185597833664
Epoch 2274
Epoch 2274 :: Batch 0/1
Batch Loss = 0.022858069105310307
2274, epoch_train_loss=0.022858069105310307
Epoch 2275
Epoch 2275 :: Batch 0/1
Batch Loss = 0.02285429015621202
2275, epoch_train_loss=0.02285429015621202
Epoch 2276
Epoch 2276 :: Batch 0/1
Batch Loss = 0.02285051910118538
2276, epoch_train_loss=0.02285051910118538
Epoch 2277
Epoch 2277 :: Batch 0/1
Batch Loss = 0.022846755910665744
2277, epoch_train_loss=0.022846755910665744
Epoch 2278
Epoch 2278 :: Batch 0/1
Batch Loss = 0.02284300055629971
2278, epoch_train_loss=0.02284300055629971
Epoch 2279
Epoch 2279 :: Batch 0/1
Batch Loss = 0.0228392530097039
2279, epoch_train_loss=0.0228392530097039
Epoch 2280
Epoch 2280 :: Batch 0/1
Batch Loss = 0.02283551324338321
2280, epoch_train_loss=0.02283551324338321
Epoch 2281
Epoch 2281 :: Batch 0/1
Batch Loss = 0.022831781230720546
2281, epoch_train_loss=0.022831781230720546
Epoch 2282
Epoch 2282 :: Batch 0/1
Batch Loss = 0.02282805694505077
2282, epoch_train_loss=0.02282805694505077
Epoch 2283
Epoch 2283 :: Batch 0/1
Batch Loss = 0.022824340360877254
2283, epoch_train_loss=0.022824340360877254
Epoch 2284
Epoch 2284 :: Batch 0/1
Batch Loss = 0.022820631452642244
2284, epoch_train_loss=0.022820631452642244
Epoch 2285
Epoch 2285 :: Batch 0/1
Batch Loss = 0.02281693019563477
2285, epoch_train_loss=0.02281693019563477
Epoch 2286
Epoch 2286 :: Batch 0/1
Batch Loss = 0.022813236565979552
2286, epoch_train_loss=0.022813236565979552
Epoch 2287
Epoch 2287 :: Batch 0/1
Batch Loss = 0.02280955053971935
2287, epoch_train_loss=0.02280955053971935
Epoch 2288
Epoch 2288 :: Batch 0/1
Batch Loss = 0.0228058720937168
2288, epoch_train_loss=0.0228058720937168
Epoch 2289
Epoch 2289 :: Batch 0/1
Batch Loss = 0.022802201205642903
2289, epoch_train_loss=0.022802201205642903
Epoch 2290
Epoch 2290 :: Batch 0/1
Batch Loss = 0.022798537853064436
2290, epoch_train_loss=0.022798537853064436
Epoch 2291
Epoch 2291 :: Batch 0/1
Batch Loss = 0.02279488201433986
2291, epoch_train_loss=0.02279488201433986
Epoch 2292
Epoch 2292 :: Batch 0/1
Batch Loss = 0.022791233668308323
2292, epoch_train_loss=0.022791233668308323
Epoch 2293
Epoch 2293 :: Batch 0/1
Batch Loss = 0.022787592794280522
2293, epoch_train_loss=0.022787592794280522
Epoch 2294
Epoch 2294 :: Batch 0/1
Batch Loss = 0.02278395937202938
2294, epoch_train_loss=0.02278395937202938
Epoch 2295
Epoch 2295 :: Batch 0/1
Batch Loss = 0.022780333381780694
2295, epoch_train_loss=0.022780333381780694
Epoch 2296
Epoch 2296 :: Batch 0/1
Batch Loss = 0.02277671480420357
2296, epoch_train_loss=0.02277671480420357
Epoch 2297
Epoch 2297 :: Batch 0/1
Batch Loss = 0.02277310362040086
2297, epoch_train_loss=0.02277310362040086
Epoch 2298
Epoch 2298 :: Batch 0/1
Batch Loss = 0.022769499811899428
2298, epoch_train_loss=0.022769499811899428
Epoch 2299
Epoch 2299 :: Batch 0/1
Batch Loss = 0.022765903360640327
2299, epoch_train_loss=0.022765903360640327
Epoch 2300
Epoch 2300 :: Batch 0/1
Batch Loss = 0.02276231424896893
2300, epoch_train_loss=0.02276231424896893
Epoch 2301
Epoch 2301 :: Batch 0/1
Batch Loss = 0.02275873245962492
2301, epoch_train_loss=0.02275873245962492
Epoch 2302
Epoch 2302 :: Batch 0/1
Batch Loss = 0.022755157975732215
2302, epoch_train_loss=0.022755157975732215
Epoch 2303
Epoch 2303 :: Batch 0/1
Batch Loss = 0.022751590780788776
2303, epoch_train_loss=0.022751590780788776
Epoch 2304
Epoch 2304 :: Batch 0/1
Batch Loss = 0.022748030858656416
2304, epoch_train_loss=0.022748030858656416
Epoch 2305
Epoch 2305 :: Batch 0/1
Batch Loss = 0.0227444781935504
2305, epoch_train_loss=0.0227444781935504
Epoch 2306
Epoch 2306 :: Batch 0/1
Batch Loss = 0.022740932770029126
2306, epoch_train_loss=0.022740932770029126
Epoch 2307
Epoch 2307 :: Batch 0/1
Batch Loss = 0.02273739457298353
2307, epoch_train_loss=0.02273739457298353
Epoch 2308
Epoch 2308 :: Batch 0/1
Batch Loss = 0.022733863587626635
2308, epoch_train_loss=0.022733863587626635
Epoch 2309
Epoch 2309 :: Batch 0/1
Batch Loss = 0.022730339799482854
2309, epoch_train_loss=0.022730339799482854
Epoch 2310
Epoch 2310 :: Batch 0/1
Batch Loss = 0.022726823194377275
2310, epoch_train_loss=0.022726823194377275
Epoch 2311
Epoch 2311 :: Batch 0/1
Batch Loss = 0.02272331375813693
2311, epoch_train_loss=0.02272331375813693
Epoch 2312
Epoch 2312 :: Batch 0/1
Batch Loss = 0.022719811477444864
2312, epoch_train_loss=0.022719811477444864
Epoch 2313
Epoch 2313 :: Batch 0/1
Batch Loss = 0.02271631633896329
2313, epoch_train_loss=0.02271631633896329
Epoch 2314
Epoch 2314 :: Batch 0/1
Batch Loss = 0.02271282832932532
2314, epoch_train_loss=0.02271282832932532
Epoch 2315
Epoch 2315 :: Batch 0/1
Batch Loss = 0.022709347435983605
2315, epoch_train_loss=0.022709347435983605
Epoch 2316
Epoch 2316 :: Batch 0/1
Batch Loss = 0.022705873646053407
2316, epoch_train_loss=0.022705873646053407
Epoch 2317
Epoch 2317 :: Batch 0/1
Batch Loss = 0.022702406947444905
2317, epoch_train_loss=0.022702406947444905
Epoch 2318
Epoch 2318 :: Batch 0/1
Batch Loss = 0.022698947327710696
2318, epoch_train_loss=0.022698947327710696
Epoch 2319
Epoch 2319 :: Batch 0/1
Batch Loss = 0.02269549477517337
2319, epoch_train_loss=0.02269549477517337
Epoch 2320
Epoch 2320 :: Batch 0/1
Batch Loss = 0.022692049278060282
2320, epoch_train_loss=0.022692049278060282
Epoch 2321
Epoch 2321 :: Batch 0/1
Batch Loss = 0.022688610824776963
2321, epoch_train_loss=0.022688610824776963
Epoch 2322
Epoch 2322 :: Batch 0/1
Batch Loss = 0.022685179403613902
2322, epoch_train_loss=0.022685179403613902
Epoch 2323
Epoch 2323 :: Batch 0/1
Batch Loss = 0.022681755003299803
2323, epoch_train_loss=0.022681755003299803
Epoch 2324
Epoch 2324 :: Batch 0/1
Batch Loss = 0.02267833761298736
2324, epoch_train_loss=0.02267833761298736
Epoch 2325
Epoch 2325 :: Batch 0/1
Batch Loss = 0.0226749272213987
2325, epoch_train_loss=0.0226749272213987
Epoch 2326
Epoch 2326 :: Batch 0/1
Batch Loss = 0.02267152381765729
2326, epoch_train_loss=0.02267152381765729
Epoch 2327
Epoch 2327 :: Batch 0/1
Batch Loss = 0.02266812739127341
2327, epoch_train_loss=0.02266812739127341
Epoch 2328
Epoch 2328 :: Batch 0/1
Batch Loss = 0.022664737931572854
2328, epoch_train_loss=0.022664737931572854
Epoch 2329
Epoch 2329 :: Batch 0/1
Batch Loss = 0.022661355427687626
2329, epoch_train_loss=0.022661355427687626
Epoch 2330
Epoch 2330 :: Batch 0/1
Batch Loss = 0.022657979869100897
2330, epoch_train_loss=0.022657979869100897
Epoch 2331
Epoch 2331 :: Batch 0/1
Batch Loss = 0.022654611245355616
2331, epoch_train_loss=0.022654611245355616
Epoch 2332
Epoch 2332 :: Batch 0/1
Batch Loss = 0.022651249546042394
2332, epoch_train_loss=0.022651249546042394
Epoch 2333
Epoch 2333 :: Batch 0/1
Batch Loss = 0.022647894761062798
2333, epoch_train_loss=0.022647894761062798
Epoch 2334
Epoch 2334 :: Batch 0/1
Batch Loss = 0.022644546879789524
2334, epoch_train_loss=0.022644546879789524
Epoch 2335
Epoch 2335 :: Batch 0/1
Batch Loss = 0.022641205891882583
2335, epoch_train_loss=0.022641205891882583
Epoch 2336
Epoch 2336 :: Batch 0/1
Batch Loss = 0.022637871787000507
2336, epoch_train_loss=0.022637871787000507
Epoch 2337
Epoch 2337 :: Batch 0/1
Batch Loss = 0.022634544554788003
2337, epoch_train_loss=0.022634544554788003
Epoch 2338
Epoch 2338 :: Batch 0/1
Batch Loss = 0.02263122418513627
2338, epoch_train_loss=0.02263122418513627
Epoch 2339
Epoch 2339 :: Batch 0/1
Batch Loss = 0.022627910667351373
2339, epoch_train_loss=0.022627910667351373
Epoch 2340
Epoch 2340 :: Batch 0/1
Batch Loss = 0.02262460399096197
2340, epoch_train_loss=0.02262460399096197
Epoch 2341
Epoch 2341 :: Batch 0/1
Batch Loss = 0.022621304145433167
2341, epoch_train_loss=0.022621304145433167
Epoch 2342
Epoch 2342 :: Batch 0/1
Batch Loss = 0.022618011120154095
2342, epoch_train_loss=0.022618011120154095
Epoch 2343
Epoch 2343 :: Batch 0/1
Batch Loss = 0.022614724904425475
2343, epoch_train_loss=0.022614724904425475
Epoch 2344
Epoch 2344 :: Batch 0/1
Batch Loss = 0.02261144548744718
2344, epoch_train_loss=0.02261144548744718
Epoch 2345
Epoch 2345 :: Batch 0/1
Batch Loss = 0.022608172858305853
2345, epoch_train_loss=0.022608172858305853
Epoch 2346
Epoch 2346 :: Batch 0/1
Batch Loss = 0.02260490700596256
2346, epoch_train_loss=0.02260490700596256
Epoch 2347
Epoch 2347 :: Batch 0/1
Batch Loss = 0.022601647919240415
2347, epoch_train_loss=0.022601647919240415
Epoch 2348
Epoch 2348 :: Batch 0/1
Batch Loss = 0.022598395586812394
2348, epoch_train_loss=0.022598395586812394
Epoch 2349
Epoch 2349 :: Batch 0/1
Batch Loss = 0.02259514999718909
2349, epoch_train_loss=0.02259514999718909
Epoch 2350
Epoch 2350 :: Batch 0/1
Batch Loss = 0.022591911138706577
2350, epoch_train_loss=0.022591911138706577
Epoch 2351
Epoch 2351 :: Batch 0/1
Batch Loss = 0.02258867899951441
2351, epoch_train_loss=0.02258867899951441
Epoch 2352
Epoch 2352 :: Batch 0/1
Batch Loss = 0.022585453567298557
2352, epoch_train_loss=0.022585453567298557
Epoch 2353
Epoch 2353 :: Batch 0/1
Batch Loss = 0.022582234830065863
2353, epoch_train_loss=0.022582234830065863
Epoch 2354
Epoch 2354 :: Batch 0/1
Batch Loss = 0.02257902277533522
2354, epoch_train_loss=0.02257902277533522
Epoch 2355
Epoch 2355 :: Batch 0/1
Batch Loss = 0.022575817390391963
2355, epoch_train_loss=0.022575817390391963
Epoch 2356
Epoch 2356 :: Batch 0/1
Batch Loss = 0.0225726186620135
2356, epoch_train_loss=0.0225726186620135
Epoch 2357
Epoch 2357 :: Batch 0/1
Batch Loss = 0.022569426577248027
2357, epoch_train_loss=0.022569426577248027
Epoch 2358
Epoch 2358 :: Batch 0/1
Batch Loss = 0.022566241122612647
2358, epoch_train_loss=0.022566241122612647
Epoch 2359
Epoch 2359 :: Batch 0/1
Batch Loss = 0.02256306228408506
2359, epoch_train_loss=0.02256306228408506
Epoch 2360
Epoch 2360 :: Batch 0/1
Batch Loss = 0.02255989004787798
2360, epoch_train_loss=0.02255989004787798
Epoch 2361
Epoch 2361 :: Batch 0/1
Batch Loss = 0.022556724399642784
2361, epoch_train_loss=0.022556724399642784
Epoch 2362
Epoch 2362 :: Batch 0/1
Batch Loss = 0.02255356532446187
2362, epoch_train_loss=0.02255356532446187
Epoch 2363
Epoch 2363 :: Batch 0/1
Batch Loss = 0.022550412807618988
2363, epoch_train_loss=0.022550412807618988
Epoch 2364
Epoch 2364 :: Batch 0/1
Batch Loss = 0.022547266833549764
2364, epoch_train_loss=0.022547266833549764
Epoch 2365
Epoch 2365 :: Batch 0/1
Batch Loss = 0.02254412738687013
2365, epoch_train_loss=0.02254412738687013
Epoch 2366
Epoch 2366 :: Batch 0/1
Batch Loss = 0.02254099445158935
2366, epoch_train_loss=0.02254099445158935
Epoch 2367
Epoch 2367 :: Batch 0/1
Batch Loss = 0.022537868011103707
2367, epoch_train_loss=0.022537868011103707
Epoch 2368
Epoch 2368 :: Batch 0/1
Batch Loss = 0.02253474804896058
2368, epoch_train_loss=0.02253474804896058
Epoch 2369
Epoch 2369 :: Batch 0/1
Batch Loss = 0.02253163454782099
2369, epoch_train_loss=0.02253163454782099
Epoch 2370
Epoch 2370 :: Batch 0/1
Batch Loss = 0.02252852749022369
2370, epoch_train_loss=0.02252852749022369
Epoch 2371
Epoch 2371 :: Batch 0/1
Batch Loss = 0.022525426858575427
2371, epoch_train_loss=0.022525426858575427
Epoch 2372
Epoch 2372 :: Batch 0/1
Batch Loss = 0.0225223326343779
2372, epoch_train_loss=0.0225223326343779
Epoch 2373
Epoch 2373 :: Batch 0/1
Batch Loss = 0.02251924479924236
2373, epoch_train_loss=0.02251924479924236
Epoch 2374
Epoch 2374 :: Batch 0/1
Batch Loss = 0.02251616333386451
2374, epoch_train_loss=0.02251616333386451
Epoch 2375
Epoch 2375 :: Batch 0/1
Batch Loss = 0.022513088218782753
2375, epoch_train_loss=0.022513088218782753
Epoch 2376
Epoch 2376 :: Batch 0/1
Batch Loss = 0.022510019434370657
2376, epoch_train_loss=0.022510019434370657
Epoch 2377
Epoch 2377 :: Batch 0/1
Batch Loss = 0.022506956959821606
2377, epoch_train_loss=0.022506956959821606
Epoch 2378
Epoch 2378 :: Batch 0/1
Batch Loss = 0.02250390077491227
2378, epoch_train_loss=0.02250390077491227
Epoch 2379
Epoch 2379 :: Batch 0/1
Batch Loss = 0.022500850858232186
2379, epoch_train_loss=0.022500850858232186
Epoch 2380
Epoch 2380 :: Batch 0/1
Batch Loss = 0.02249780718843996
2380, epoch_train_loss=0.02249780718843996
Epoch 2381
Epoch 2381 :: Batch 0/1
Batch Loss = 0.022494769743005592
2381, epoch_train_loss=0.022494769743005592
Epoch 2382
Epoch 2382 :: Batch 0/1
Batch Loss = 0.022491738499961597
2382, epoch_train_loss=0.022491738499961597
Epoch 2383
Epoch 2383 :: Batch 0/1
Batch Loss = 0.022488713436148635
2383, epoch_train_loss=0.022488713436148635
Epoch 2384
Epoch 2384 :: Batch 0/1
Batch Loss = 0.022485694528215
2384, epoch_train_loss=0.022485694528215
Epoch 2385
Epoch 2385 :: Batch 0/1
Batch Loss = 0.02248268175236528
2385, epoch_train_loss=0.02248268175236528
Epoch 2386
Epoch 2386 :: Batch 0/1
Batch Loss = 0.022479675084359736
2386, epoch_train_loss=0.022479675084359736
Epoch 2387
Epoch 2387 :: Batch 0/1
Batch Loss = 0.02247667449951435
2387, epoch_train_loss=0.02247667449951435
Epoch 2388
Epoch 2388 :: Batch 0/1
Batch Loss = 0.02247367997270106
2388, epoch_train_loss=0.02247367997270106
Epoch 2389
Epoch 2389 :: Batch 0/1
Batch Loss = 0.022470691478348712
2389, epoch_train_loss=0.022470691478348712
Epoch 2390
Epoch 2390 :: Batch 0/1
Batch Loss = 0.02246770899044433
2390, epoch_train_loss=0.02246770899044433
Epoch 2391
Epoch 2391 :: Batch 0/1
Batch Loss = 0.022464732482534878
2391, epoch_train_loss=0.022464732482534878
Epoch 2392
Epoch 2392 :: Batch 0/1
Batch Loss = 0.02246176192772947
2392, epoch_train_loss=0.02246176192772947
Epoch 2393
Epoch 2393 :: Batch 0/1
Batch Loss = 0.02245879729870199
2393, epoch_train_loss=0.02245879729870199
Epoch 2394
Epoch 2394 :: Batch 0/1
Batch Loss = 0.022455838567694165
2394, epoch_train_loss=0.022455838567694165
Epoch 2395
Epoch 2395 :: Batch 0/1
Batch Loss = 0.022452885706519
2395, epoch_train_loss=0.022452885706519
Epoch 2396
Epoch 2396 :: Batch 0/1
Batch Loss = 0.02244993868656465
2396, epoch_train_loss=0.02244993868656465
Epoch 2397
Epoch 2397 :: Batch 0/1
Batch Loss = 0.022446997478556413
2397, epoch_train_loss=0.022446997478556413
Epoch 2398
Epoch 2398 :: Batch 0/1
Batch Loss = 0.022444062053288787
2398, epoch_train_loss=0.022444062053288787
Epoch 2399
Epoch 2399 :: Batch 0/1
Batch Loss = 0.022441132380902208
2399, epoch_train_loss=0.022441132380902208
Epoch 2400
Epoch 2400 :: Batch 0/1
Batch Loss = 0.022438208431131236
2400, epoch_train_loss=0.022438208431131236
Epoch 2401
Epoch 2401 :: Batch 0/1
Batch Loss = 0.022435290173069746
2401, epoch_train_loss=0.022435290173069746
Epoch 2402
Epoch 2402 :: Batch 0/1
Batch Loss = 0.02243237757589875
2402, epoch_train_loss=0.02243237757589875
Epoch 2403
Epoch 2403 :: Batch 0/1
Batch Loss = 0.02242947060816979
2403, epoch_train_loss=0.02242947060816979
Epoch 2404
Epoch 2404 :: Batch 0/1
Batch Loss = 0.02242656923781338
2404, epoch_train_loss=0.02242656923781338
Epoch 2405
Epoch 2405 :: Batch 0/1
Batch Loss = 0.022423673432863448
2405, epoch_train_loss=0.022423673432863448
Epoch 2406
Epoch 2406 :: Batch 0/1
Batch Loss = 0.022420783160507364
2406, epoch_train_loss=0.022420783160507364
Epoch 2407
Epoch 2407 :: Batch 0/1
Batch Loss = 0.02241789838804781
2407, epoch_train_loss=0.02241789838804781
Epoch 2408
Epoch 2408 :: Batch 0/1
Batch Loss = 0.0224150190821939
2408, epoch_train_loss=0.0224150190821939
Epoch 2409
Epoch 2409 :: Batch 0/1
Batch Loss = 0.022412145209070446
2409, epoch_train_loss=0.022412145209070446
Epoch 2410
Epoch 2410 :: Batch 0/1
Batch Loss = 0.022409276734936482
2410, epoch_train_loss=0.022409276734936482
Epoch 2411
Epoch 2411 :: Batch 0/1
Batch Loss = 0.02240641362524464
2411, epoch_train_loss=0.02240641362524464
Epoch 2412
Epoch 2412 :: Batch 0/1
Batch Loss = 0.02240355584559474
2412, epoch_train_loss=0.02240355584559474
Epoch 2413
Epoch 2413 :: Batch 0/1
Batch Loss = 0.02240070336103182
2413, epoch_train_loss=0.02240070336103182
Epoch 2414
Epoch 2414 :: Batch 0/1
Batch Loss = 0.022397856135820817
2414, epoch_train_loss=0.022397856135820817
Epoch 2415
Epoch 2415 :: Batch 0/1
Batch Loss = 0.022395014134863455
2415, epoch_train_loss=0.022395014134863455
Epoch 2416
Epoch 2416 :: Batch 0/1
Batch Loss = 0.022392177322294517
2416, epoch_train_loss=0.022392177322294517
Epoch 2417
Epoch 2417 :: Batch 0/1
Batch Loss = 0.022389345661726394
2417, epoch_train_loss=0.022389345661726394
Epoch 2418
Epoch 2418 :: Batch 0/1
Batch Loss = 0.022386519116723954
2418, epoch_train_loss=0.022386519116723954
Epoch 2419
Epoch 2419 :: Batch 0/1
Batch Loss = 0.022383697650809723
2419, epoch_train_loss=0.022383697650809723
Epoch 2420
Epoch 2420 :: Batch 0/1
Batch Loss = 0.022380881226771725
2420, epoch_train_loss=0.022380881226771725
Epoch 2421
Epoch 2421 :: Batch 0/1
Batch Loss = 0.022378069807369543
2421, epoch_train_loss=0.022378069807369543
Epoch 2422
Epoch 2422 :: Batch 0/1
Batch Loss = 0.022375263355338227
2422, epoch_train_loss=0.022375263355338227
Epoch 2423
Epoch 2423 :: Batch 0/1
Batch Loss = 0.02237246183246891
2423, epoch_train_loss=0.02237246183246891
Epoch 2424
Epoch 2424 :: Batch 0/1
Batch Loss = 0.022369665201233683
2424, epoch_train_loss=0.022369665201233683
Epoch 2425
Epoch 2425 :: Batch 0/1
Batch Loss = 0.02236687342317296
2425, epoch_train_loss=0.02236687342317296
Epoch 2426
Epoch 2426 :: Batch 0/1
Batch Loss = 0.02236408645982404
2426, epoch_train_loss=0.02236408645982404
Epoch 2427
Epoch 2427 :: Batch 0/1
Batch Loss = 0.022361304272496337
2427, epoch_train_loss=0.022361304272496337
Epoch 2428
Epoch 2428 :: Batch 0/1
Batch Loss = 0.022358526822273135
2428, epoch_train_loss=0.022358526822273135
Epoch 2429
Epoch 2429 :: Batch 0/1
Batch Loss = 0.022355754070247542
2429, epoch_train_loss=0.022355754070247542
Epoch 2430
Epoch 2430 :: Batch 0/1
Batch Loss = 0.02235298597660636
2430, epoch_train_loss=0.02235298597660636
Epoch 2431
Epoch 2431 :: Batch 0/1
Batch Loss = 0.022350222502012025
2431, epoch_train_loss=0.022350222502012025
Epoch 2432
Epoch 2432 :: Batch 0/1
Batch Loss = 0.0223474636069091
2432, epoch_train_loss=0.0223474636069091
Epoch 2433
Epoch 2433 :: Batch 0/1
Batch Loss = 0.022344709251082638
2433, epoch_train_loss=0.022344709251082638
Epoch 2434
Epoch 2434 :: Batch 0/1
Batch Loss = 0.022341959394102204
2434, epoch_train_loss=0.022341959394102204
Epoch 2435
Epoch 2435 :: Batch 0/1
Batch Loss = 0.022339213995795706
2435, epoch_train_loss=0.022339213995795706
Epoch 2436
Epoch 2436 :: Batch 0/1
Batch Loss = 0.022336473015767428
2436, epoch_train_loss=0.022336473015767428
Epoch 2437
Epoch 2437 :: Batch 0/1
Batch Loss = 0.022333736412982887
2437, epoch_train_loss=0.022333736412982887
Epoch 2438
Epoch 2438 :: Batch 0/1
Batch Loss = 0.02233100414617349
2438, epoch_train_loss=0.02233100414617349
Epoch 2439
Epoch 2439 :: Batch 0/1
Batch Loss = 0.022328276174353883
2439, epoch_train_loss=0.022328276174353883
Epoch 2440
Epoch 2440 :: Batch 0/1
Batch Loss = 0.022325552456053027
2440, epoch_train_loss=0.022325552456053027
Epoch 2441
Epoch 2441 :: Batch 0/1
Batch Loss = 0.02232283294966783
2441, epoch_train_loss=0.02232283294966783
Epoch 2442
Epoch 2442 :: Batch 0/1
Batch Loss = 0.022320117613297063
2442, epoch_train_loss=0.022320117613297063
Epoch 2443
Epoch 2443 :: Batch 0/1
Batch Loss = 0.02231740640476764
2443, epoch_train_loss=0.02231740640476764
Epoch 2444
Epoch 2444 :: Batch 0/1
Batch Loss = 0.022314699282011435
2444, epoch_train_loss=0.022314699282011435
Epoch 2445
Epoch 2445 :: Batch 0/1
Batch Loss = 0.022311996202902685
2445, epoch_train_loss=0.022311996202902685
Epoch 2446
Epoch 2446 :: Batch 0/1
Batch Loss = 0.022309297124987932
2446, epoch_train_loss=0.022309297124987932
Epoch 2447
Epoch 2447 :: Batch 0/1
Batch Loss = 0.022306602006591528
2447, epoch_train_loss=0.022306602006591528
Epoch 2448
Epoch 2448 :: Batch 0/1
Batch Loss = 0.022303910805897765
2448, epoch_train_loss=0.022303910805897765
Epoch 2449
Epoch 2449 :: Batch 0/1
Batch Loss = 0.022301223484024312
2449, epoch_train_loss=0.022301223484024312
Epoch 2450
Epoch 2450 :: Batch 0/1
Batch Loss = 0.022298540004656465
2450, epoch_train_loss=0.022298540004656465
Epoch 2451
Epoch 2451 :: Batch 0/1
Batch Loss = 0.022295860340656833
2451, epoch_train_loss=0.022295860340656833
Epoch 2452
Epoch 2452 :: Batch 0/1
Batch Loss = 0.02229318447746797
2452, epoch_train_loss=0.02229318447746797
Epoch 2453
Epoch 2453 :: Batch 0/1
Batch Loss = 0.022290512434489384
2453, epoch_train_loss=0.022290512434489384
Epoch 2454
Epoch 2454 :: Batch 0/1
Batch Loss = 0.02228784428769672
2454, epoch_train_loss=0.02228784428769672
Epoch 2455
Epoch 2455 :: Batch 0/1
Batch Loss = 0.02228518024303942
2455, epoch_train_loss=0.02228518024303942
Epoch 2456
Epoch 2456 :: Batch 0/1
Batch Loss = 0.02228252074508459
2456, epoch_train_loss=0.02228252074508459
Epoch 2457
Epoch 2457 :: Batch 0/1
Batch Loss = 0.022279866755522276
2457, epoch_train_loss=0.022279866755522276
Epoch 2458
Epoch 2458 :: Batch 0/1
Batch Loss = 0.022277220237275634
2458, epoch_train_loss=0.022277220237275634
Epoch 2459
Epoch 2459 :: Batch 0/1
Batch Loss = 0.02227458526190806
2459, epoch_train_loss=0.02227458526190806
Epoch 2460
Epoch 2460 :: Batch 0/1
Batch Loss = 0.02227197012693271
2460, epoch_train_loss=0.02227197012693271
Epoch 2461
Epoch 2461 :: Batch 0/1
Batch Loss = 0.022269391923079883
2461, epoch_train_loss=0.022269391923079883
Epoch 2462
Epoch 2462 :: Batch 0/1
Batch Loss = 0.022266885766618952
2462, epoch_train_loss=0.022266885766618952
Epoch 2463
Epoch 2463 :: Batch 0/1
Batch Loss = 0.022264524131385376
2463, epoch_train_loss=0.022264524131385376
Epoch 2464
Epoch 2464 :: Batch 0/1
Batch Loss = 0.022262457462120615
2464, epoch_train_loss=0.022262457462120615
Epoch 2465
Epoch 2465 :: Batch 0/1
Batch Loss = 0.02226099743009006
2465, epoch_train_loss=0.02226099743009006
Epoch 2466
Epoch 2466 :: Batch 0/1
Batch Loss = 0.022260798314869542
2466, epoch_train_loss=0.022260798314869542
Epoch 2467
Epoch 2467 :: Batch 0/1
Batch Loss = 0.022263218804901192
2467, epoch_train_loss=0.022263218804901192
Epoch 2468
Epoch 2468 :: Batch 0/1
Batch Loss = 0.022271146142864063
2468, epoch_train_loss=0.022271146142864063
Epoch 2469
Epoch 2469 :: Batch 0/1
Batch Loss = 0.022290559408194823
2469, epoch_train_loss=0.022290559408194823
Epoch 2470
Epoch 2470 :: Batch 0/1
Batch Loss = 0.022334295494256754
2470, epoch_train_loss=0.022334295494256754
Epoch 2471
Epoch 2471 :: Batch 0/1
Batch Loss = 0.022428509983306
2471, epoch_train_loss=0.022428509983306
Epoch 2472
Epoch 2472 :: Batch 0/1
Batch Loss = 0.022628395321891405
2472, epoch_train_loss=0.022628395321891405
Epoch 2473
Epoch 2473 :: Batch 0/1
Batch Loss = 0.023041115152005975
2473, epoch_train_loss=0.023041115152005975
Epoch 2474
Epoch 2474 :: Batch 0/1
Batch Loss = 0.02384341988032703
2474, epoch_train_loss=0.02384341988032703
Epoch 2475
Epoch 2475 :: Batch 0/1
Batch Loss = 0.025371014740996418
2475, epoch_train_loss=0.025371014740996418
Epoch 2476
Epoch 2476 :: Batch 0/1
Batch Loss = 0.027609393268936012
2476, epoch_train_loss=0.027609393268936012
Epoch 2477
Epoch 2477 :: Batch 0/1
Batch Loss = 0.031376061805565784
2477, epoch_train_loss=0.031376061805565784
Epoch 2478
Epoch 2478 :: Batch 0/1
Batch Loss = 0.03675999690783097
2478, epoch_train_loss=0.03675999690783097
Epoch 2479
Epoch 2479 :: Batch 0/1
Batch Loss = 0.04310224326181092
2479, epoch_train_loss=0.04310224326181092
Epoch 2480
Epoch 2480 :: Batch 0/1
Batch Loss = 0.05019113177373663
2480, epoch_train_loss=0.05019113177373663
Epoch 2481
Epoch 2481 :: Batch 0/1
Batch Loss = 0.052557796709281804
2481, epoch_train_loss=0.052557796709281804
Epoch 2482
Epoch 2482 :: Batch 0/1
Batch Loss = 0.050789975135383233
2482, epoch_train_loss=0.050789975135383233
Epoch 2483
Epoch 2483 :: Batch 0/1
Batch Loss = 0.03992067715364966
2483, epoch_train_loss=0.03992067715364966
Epoch 2484
Epoch 2484 :: Batch 0/1
Batch Loss = 0.028676967830777472
2484, epoch_train_loss=0.028676967830777472
Epoch 2485
Epoch 2485 :: Batch 0/1
Batch Loss = 0.022822597230937026
2485, epoch_train_loss=0.022822597230937026
Epoch 2486
Epoch 2486 :: Batch 0/1
Batch Loss = 0.025293603105070218
2486, epoch_train_loss=0.025293603105070218
Epoch 2487
Epoch 2487 :: Batch 0/1
Batch Loss = 0.03148822971359888
2487, epoch_train_loss=0.03148822971359888
Epoch 2488
Epoch 2488 :: Batch 0/1
Batch Loss = 0.03308827561599173
2488, epoch_train_loss=0.03308827561599173
Epoch 2489
Epoch 2489 :: Batch 0/1
Batch Loss = 0.02828246481140224
2489, epoch_train_loss=0.02828246481140224
Epoch 2490
Epoch 2490 :: Batch 0/1
Batch Loss = 0.023823081184225728
2490, epoch_train_loss=0.023823081184225728
Epoch 2491
Epoch 2491 :: Batch 0/1
Batch Loss = 0.024325438844213638
2491, epoch_train_loss=0.024325438844213638
Epoch 2492
Epoch 2492 :: Batch 0/1
Batch Loss = 0.029125584033729675
2492, epoch_train_loss=0.029125584033729675
Epoch 2493
Epoch 2493 :: Batch 0/1
Batch Loss = 0.029497290601382665
2493, epoch_train_loss=0.029497290601382665
Epoch 2494
Epoch 2494 :: Batch 0/1
Batch Loss = 0.02671349951656324
2494, epoch_train_loss=0.02671349951656324
Epoch 2495
Epoch 2495 :: Batch 0/1
Batch Loss = 0.023704272974888613
2495, epoch_train_loss=0.023704272974888613
Epoch 2496
Epoch 2496 :: Batch 0/1
Batch Loss = 0.02328116466511044
2496, epoch_train_loss=0.02328116466511044
Epoch 2497
Epoch 2497 :: Batch 0/1
Batch Loss = 0.026172711052791325
2497, epoch_train_loss=0.026172711052791325
Epoch 2498
Epoch 2498 :: Batch 0/1
Batch Loss = 0.027286769188847832
2498, epoch_train_loss=0.027286769188847832
Epoch 2499
Epoch 2499 :: Batch 0/1
Batch Loss = 0.027250567261208947
2499, epoch_train_loss=0.027250567261208947
