/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting "B3LYP_WITH_VWN5 = True" in pyscf_conf.py
  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0302b60> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0302b60> in UKS object of <class 'pyscf.dft.uks.UKS'>
WARNING: External module "mldftdat" required for non-local descriptor use.
<pyscf.gto.mole.Mole object at 0x7ffeb0302b60> [['P', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb03006d0> [['N', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb0300fa0> [['H', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb0302e30> [['Li', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb0301900> [['O', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb0302530> [['Cl', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb0301690> [['Al', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb0300580> [['S', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeb0300ca0> [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb0301390> [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb0302ef0> [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb0302770> [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb03024a0> [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb03008b0> [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb00a4160> [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb00a4b20> [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb00a6c20> [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb00a6c50> [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb00a79d0> [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb00a7580> [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb00a72b0> [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb00a6590> [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb00a5690> [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeb00a5f90> [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeb00a4700> [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]] 5
<pyscf.gto.mole.Mole object at 0x7ffeb02d0820> [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeb02d3580> [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]] 5
mol:  [['P', array([0., 0., 0.])]]
converged SCF energy = -341.104145992717  <S^2> = 3.7502984  2S+1 = 4.0001492
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb03006d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb03006d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-3.47389956e-03 -8.82676818e-04 -2.08411238e-03 ... -1.11301603e+01
 -1.11301603e+01 -1.11301603e+01] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 16)
mol:  [['N', array([0., 0., 0.])]]
converged SCF energy = -54.5289742046674  <S^2> = 3.7524945  2S+1 = 4.0012471
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0300fa0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0300fa0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5016, 22)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.07670570e-03 -5.92340671e-04 -6.66573372e-05 ... -5.03679786e+00
 -5.03679786e+00 -5.03679786e+00] = SCAN,
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5016, 22)
rho shape (4, 4, 2, 5016)
rho_filt shape: (5016,)
get descriptors tdrho.shape=(5016, 16)
mol:  [['H', array([0., 0., 0.])]]
converged SCF energy = -0.499812984008539  <S^2> = 0.75  2S+1 = 2
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0302e30> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0302e30> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 7, 7)
ao.shape (10, 2440, 7)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.59173730e-03 -7.67300285e-04 -6.09330303e-05 ... -7.27736608e-01
 -7.27736608e-01 -7.27736608e-01] = SCAN,
get_data, dm shape = (2, 7, 7)
ao_eval.shape=(4, 2440, 7)
rho shape (4, 4, 2, 2440)
rho_filt shape: (2440,)
get descriptors tdrho.shape=(2440, 16)
mol:  [['Li', array([0., 0., 0.])]]
converged SCF energy = -7.46006188627842  <S^2> = 0.75000049  2S+1 = 2.0000005
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0301900> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0301900> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 4592, 22)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-6.71503005e-03 -1.44519923e-03 -1.44519923e-03 ... -1.46899070e-02
 -2.03947707e+00 -2.03947707e+00] = SCAN,
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 4592, 22)
rho shape (4, 4, 2, 4592)
rho_filt shape: (4592,)
get descriptors tdrho.shape=(4592, 16)
mol:  [['O', array([0., 0., 0.])]]
converged SCF energy = -75.0033774438863  <S^2> = 2.0027452  2S+1 = 3.0018296
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0302530> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0302530> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5040, 22)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-7.48511638e-04 -1.24324024e-04 -6.20777460e-06 ... -5.78449376e+00
 -5.78449376e+00 -5.78449376e+00] = SCAN,
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5040, 22)
rho shape (4, 4, 2, 5040)
rho_filt shape: (5040,)
get descriptors tdrho.shape=(5040, 16)
mol:  [['Cl', array([0., 0., 0.])]]
converged SCF energy = -459.95757712413  <S^2> = 0.75161941  2S+1 = 2.0016188
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0301690> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0301690> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6152, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-5.81963844e-04 -9.06839735e-04 -3.08241277e-04 ... -1.26648275e+01
 -1.26648275e+01 -1.26648275e+01] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6152, 30)
rho shape (4, 4, 2, 6152)
rho_filt shape: (6152,)
get descriptors tdrho.shape=(6152, 16)
mol:  [['Al', array([0., 0., 0.])]]
converged SCF energy = -242.226560989234  <S^2> = 0.75226414  2S+1 = 2.0022629
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0300580> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0300580> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6088, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.33089483e-02 -8.48878485e-03 -4.25204071e-03 ... -1.37643420e-04
 -1.02964531e-03 -7.41775908e-05] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6088, 30)
rho shape (4, 4, 2, 6088)
rho_filt shape: (6088,)
get descriptors tdrho.shape=(6088, 16)
mol:  [['S', array([0., 0., 0.])]]
converged SCF energy = -397.938786807052  <S^2> = 2.0022329  2S+1 = 3.0014882
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0300ca0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0300ca0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.59289362e-03 -7.51450947e-04 -9.06608871e-04 ... -1.18986567e+01
 -1.18986567e+01 -1.18986567e+01] = SCAN,
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(6320, 16)
mol:  [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]]
converged SCF energy = -1.16580491182912  <S^2> = 4.4408921e-16  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0301390> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0301390> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 14, 14)
ao.shape (10, 4776, 14)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.31557088e-04 -9.73828620e-06 -3.66768667e-04 ... -5.54165574e-01
 -5.54165574e-01 -5.54165574e-01] = SCAN,
get_data, dm shape = (2, 14, 14)
ao_eval.shape=(4, 4776, 14)
rho shape (4, 4, 2, 4776)
rho_filt shape: (4776,)
get descriptors tdrho.shape=(4776, 16)
mol:  [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]]
converged SCF energy = -109.439263799566  <S^2> = 1.9539925e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0302ef0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0302ef0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9848, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-9.68474977e-05 -9.84742592e-04 -2.59676393e-04 ... -2.39645778e-05
 -2.39645778e-05 -9.68474977e-05] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9848, 44)
rho shape (4, 4, 2, 9848)
rho_filt shape: (9848,)
get descriptors tdrho.shape=(9848, 16)
mol:  [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]]
converged SCF energy = -107.339357395734  <S^2> = 7.1054274e-15  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb0302770> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb0302770> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9752, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.04987770e-03 -6.68954111e-04 -8.57556562e-04 ... -1.07485605e-03
 -8.01425702e-01 -8.01425702e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9752, 44)
rho shape (4, 4, 2, 9752)
rho_filt shape: (9752,)
get descriptors tdrho.shape=(9752, 16)
mol:  [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]]
converged SCF energy = -93.337792446513  <S^2> = 4.0072923e-10  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb03024a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb03024a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 12256, 51)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.97917639e-04 -2.54437615e-05 -3.15202008e-05 ... -6.37386388e-01
 -6.37386388e-01 -6.37386388e-01] = SCAN,
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 12256, 51)
rho shape (4, 4, 2, 12256)
rho_filt shape: (12256,)
get descriptors tdrho.shape=(12256, 16)
mol:  [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]]
converged SCF energy = -188.456965322843  <S^2> = 1.5987212e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb03008b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb03008b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 66, 66)
ao.shape (10, 14920, 66)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.50217343e-04 -2.07520331e-04 -9.23619961e-04 ... -2.76182455e-06
 -4.27559894e+00 -4.27559894e+00] = SCAN,
get_data, dm shape = (2, 66, 66)
ao_eval.shape=(4, 14920, 66)
rho shape (4, 4, 2, 14920)
rho_filt shape: (14920,)
get descriptors tdrho.shape=(14920, 16)
mol:  [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]]
converged SCF energy = -920.005608888958  <S^2> = 5.0448534e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb00a4160> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb00a4160> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12208, 60)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-0.00155834 -0.00091688 -0.00215831 ... -0.00091688 -0.41618506
 -0.41618506] = SCAN,
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12208, 60)
rho shape (4, 4, 2, 12208)
rho_filt shape: (12208,)
get descriptors tdrho.shape=(12208, 16)
mol:  [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]]
converged SCF energy = -199.394370591172  <S^2> = 1.2967405e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb00a4b20> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb00a4b20> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9824, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.92948752e-04 -1.95215890e-05 -1.16699780e-03 ... -4.89378326e-01
 -4.89378326e-01 -4.89378326e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9824, 44)
rho shape (4, 4, 2, 9824)
rho_filt shape: (9824,)
get descriptors tdrho.shape=(9824, 16)
mol:  [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]]
converged SCF energy = -150.214894533261  <S^2> = 1.0018599  2S+1 = 2.2377309
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb00a6c20> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb00a6c20> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9912, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-6.91772546e-04 -1.23036460e-04 -6.34270046e-06 ... -6.59150605e-01
 -6.59150605e-01 -6.59150605e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9912, 44)
rho shape (4, 4, 2, 9912)
rho_filt shape: (9912,)
get descriptors tdrho.shape=(9912, 16)
mol:  [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]]
converged SCF energy = -77.2435048346374  <S^2> = 9.7699626e-15  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb00a6c50> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb00a6c50> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 58, 58)
ao.shape (10, 15208, 58)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-8.83278187e-05 -8.83278187e-05 -9.75839793e-04 ... -3.46740731e-05
 -3.31729009e-05 -3.31729009e-05] = SCAN,
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15208, 58)
rho shape (4, 4, 2, 15208)
rho_filt shape: (15208,)
get descriptors tdrho.shape=(15208, 16)
mol:  [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]]
converged SCF energy = -113.221335689652  <S^2> = 6.5547567e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb00a79d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb00a79d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 10040, 44)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-5.37000596e-04 -8.55494373e-04 -2.46853248e-03 ... -7.34251993e-01
 -7.34251993e-01 -7.34251993e-01] = SCAN,
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 10040, 44)
rho shape (4, 4, 2, 10040)
rho_filt shape: (10040,)
get descriptors tdrho.shape=(10040, 16)
mol:  [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]]
converged SCF energy = -460.624592374077  <S^2> = 7.283063e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb00a7580> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb00a7580> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 37, 37)
ao.shape (10, 8552, 37)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-2.38161478e-04 -1.81223966e-05 -2.37327566e-05 ... -2.83738108e+00
 -2.83738108e+00 -2.83738108e+00] = SCAN,
get_data, dm shape = (2, 37, 37)
ao_eval.shape=(4, 8552, 37)
rho shape (4, 4, 2, 8552)
rho_filt shape: (8552,)
get descriptors tdrho.shape=(8552, 16)
mol:  [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]]
converged SCF energy = -8.04458854018922  <S^2> = 7.7937656e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb00a72b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb00a72b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 29, 29)
ao.shape (10, 6936, 29)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-0.00113445 -0.00118624 -0.00118624 ... -0.48434639 -0.48434639
 -0.48434639] = SCAN,
get_data, dm shape = (2, 29, 29)
ao_eval.shape=(4, 6936, 29)
rho shape (4, 4, 2, 6936)
rho_filt shape: (6936,)
get descriptors tdrho.shape=(6936, 16)
mol:  [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]]
converged SCF energy = -324.340512506578  <S^2> = 1.5855761e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb00a6590> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb00a6590> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 11536, 60)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-0.00297936 -0.00297936 -0.00407091 ... -0.00297936 -0.00297936
 -0.00407091] = SCAN,
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 11536, 60)
rho shape (4, 4, 2, 11536)
rho_filt shape: (11536,)
get descriptors tdrho.shape=(11536, 16)
mol:  [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]]
converged SCF energy = -1622.57507845814  <S^2> = 8.2422957e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb00a5690> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb00a5690> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 120, 120)
ao.shape (10, 24512, 120)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.61401455e-04 -4.90485117e-04 -2.56451688e-03 ... -9.59296114e+00
 -9.59296114e+00 -9.59296114e+00] = SCAN,
get_data, dm shape = (2, 120, 120)
ao_eval.shape=(4, 24512, 120)
rho shape (4, 4, 2, 24512)
rho_filt shape: (24512,)
get descriptors tdrho.shape=(24512, 16)
mol:  [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]]
converged SCF energy = -342.979728469575  <S^2> = 2.5393021e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb00a5f90> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb00a5f90> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 13096, 51)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.28637187e-03 -4.32380890e-04 -3.74072638e-05 ... -1.91722763e+00
 -1.91722763e+00 -1.91722763e+00] = SCAN,
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 13096, 51)
rho shape (4, 4, 2, 13096)
rho_filt shape: (13096,)
get descriptors tdrho.shape=(13096, 16)
mol:  [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]]
converged SCF energy = -578.565336641349  <S^2> = 1.0034707  2S+1 = 2.2391701
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb00a4700> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb00a4700> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12384, 60)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-1.60118913e-04 -2.60152326e-04 -2.60145797e-04 ... -3.86943856e-01
 -3.86943856e-01 -3.86943856e-01] = SCAN,
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12384, 60)
rho shape (4, 4, 2, 12384)
rho_filt shape: (12384,)
get descriptors tdrho.shape=(12384, 16)
mol:  [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]]
converged SCF energy = -40.4598214864075  <S^2> = 3.1974423e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb02d0820> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb02d0820> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 50, 50)
ao.shape (10, 13936, 50)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-8.68439856e-04 -2.42462783e-04 -1.69965237e-05 ... -2.55256081e-05
 -2.55256081e-05 -2.55256081e-05] = SCAN,
get_data, dm shape = (2, 50, 50)
ao_eval.shape=(4, 13936, 50)
rho shape (4, 4, 2, 13936)
rho_filt shape: (13936,)
get descriptors tdrho.shape=(13936, 16)
mol:  [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]]
converged SCF energy = -39.0756147483504  <S^2> = 6.1994854e-12  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeb02d3580> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeb02d3580> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 36, 36)
ao.shape (10, 9656, 36)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-7.67691257e-04 -4.57409182e-05 -2.02835243e-04 ... -1.14928928e+00
 -1.14928928e+00 -1.14928928e+00] = SCAN,
get_data, dm shape = (2, 36, 36)
ao_eval.shape=(4, 9656, 36)
rho shape (4, 4, 2, 9656)
rho_filt shape: (9656,)
get descriptors tdrho.shape=(9656, 16)
mol:  [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]]
converged SCF energy = -291.719272437818  <S^2> = 1.3155699e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
New DM shape: (2, 58, 58)
ao.shape (10, 15256, 58)
Exchange contribution only
SCAN,
no spin scaling
exc with xc_func = [-8.33847724e-04 -2.34902391e-04 -1.75660753e-05 ... -1.92925750e-05
 -1.92925750e-05 -1.92925750e-05] = SCAN,
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15256, 58)
rho shape (4, 4, 2, 15256)
rho_filt shape: (15256,)
get descriptors tdrho.shape=(15256, 16)
PRE NAN FILT: tFxc.shape=(237021,), tdrho.shape=(237021, 16)
nan_filt_rho.shape=(237021,)
nan_filt_fxc.shape=(237021,)
tFxc.shape=(237021,), tdrho.shape=(237021, 16)
inp[0].shape = (237021, 16)
Epoch 0
Epoch 0 :: Batch 0/1
Batch Loss = 948345.9307054718
0, epoch_train_loss=948345.9307054718
Epoch 1
Epoch 1 :: Batch 0/1
Batch Loss = 195434780.0766344
1, epoch_train_loss=195434780.0766344
Epoch 2
Epoch 2 :: Batch 0/1
Batch Loss = 1025850.2795476998
2, epoch_train_loss=1025850.2795476998
Epoch 3
Epoch 3 :: Batch 0/1
Batch Loss = 3465183.9635517616
3, epoch_train_loss=3465183.9635517616
Epoch 4
Epoch 4 :: Batch 0/1
Batch Loss = 1199258.2530317523
4, epoch_train_loss=1199258.2530317523
Epoch 5
Epoch 5 :: Batch 0/1
Batch Loss = 20151.15847826155
5, epoch_train_loss=20151.15847826155
Epoch 6
Epoch 6 :: Batch 0/1
Batch Loss = 489267.042108189
6, epoch_train_loss=489267.042108189
Epoch 7
Epoch 7 :: Batch 0/1
Batch Loss = 239103.97742016215
7, epoch_train_loss=239103.97742016215
Epoch 8
Epoch 8 :: Batch 0/1
Batch Loss = 6338.903719167407
8, epoch_train_loss=6338.903719167407
Epoch 9
Epoch 9 :: Batch 0/1
Batch Loss = 316537.8251132697
9, epoch_train_loss=316537.8251132697
Epoch 10
Epoch 10 :: Batch 0/1
Batch Loss = 238371.0024466091
10, epoch_train_loss=238371.0024466091
Epoch 11
Epoch 11 :: Batch 0/1
Batch Loss = 3078.4310029064454
11, epoch_train_loss=3078.4310029064454
Epoch 12
Epoch 12 :: Batch 0/1
Batch Loss = 139691.63698100514
12, epoch_train_loss=139691.63698100514
Epoch 13
Epoch 13 :: Batch 0/1
Batch Loss = 206202.34663829062
13, epoch_train_loss=206202.34663829062
Epoch 14
Epoch 14 :: Batch 0/1
Batch Loss = 40627.151785628404
14, epoch_train_loss=40627.151785628404
Epoch 15
Epoch 15 :: Batch 0/1
Batch Loss = 24323.978769579568
15, epoch_train_loss=24323.978769579568
Epoch 16
Epoch 16 :: Batch 0/1
Batch Loss = 156782.53540405838
16, epoch_train_loss=156782.53540405838
Epoch 17
Epoch 17 :: Batch 0/1
Batch Loss = 110462.28436017521
17, epoch_train_loss=110462.28436017521
Epoch 18
Epoch 18 :: Batch 0/1
Batch Loss = 1826.3885013312336
18, epoch_train_loss=1826.3885013312336
Epoch 19
Epoch 19 :: Batch 0/1
Batch Loss = 149031.1362740771
19, epoch_train_loss=149031.1362740771
Epoch 20
Epoch 20 :: Batch 0/1
Batch Loss = 44406.713993938756
20, epoch_train_loss=44406.713993938756
Epoch 21
Epoch 21 :: Batch 0/1
Batch Loss = 124.2951630801103
21, epoch_train_loss=124.2951630801103
Epoch 22
Epoch 22 :: Batch 0/1
Batch Loss = 25350.61795811554
22, epoch_train_loss=25350.61795811554
Epoch 23
Epoch 23 :: Batch 0/1
Batch Loss = 35580.44080260917
23, epoch_train_loss=35580.44080260917
Epoch 24
Epoch 24 :: Batch 0/1
Batch Loss = 14226.055018143832
24, epoch_train_loss=14226.055018143832
Epoch 25
Epoch 25 :: Batch 0/1
Batch Loss = 28.081132745451605
25, epoch_train_loss=28.081132745451605
Epoch 26
Epoch 26 :: Batch 0/1
Batch Loss = 11999.18164341706
26, epoch_train_loss=11999.18164341706
Epoch 27
Epoch 27 :: Batch 0/1
Batch Loss = 20600.886626169977
27, epoch_train_loss=20600.886626169977
Epoch 28
Epoch 28 :: Batch 0/1
Batch Loss = 4763.719956228747
28, epoch_train_loss=4763.719956228747
Epoch 29
Epoch 29 :: Batch 0/1
Batch Loss = 159601.95819635122
29, epoch_train_loss=159601.95819635122
Epoch 30
Epoch 30 :: Batch 0/1
Batch Loss = 266476.4807880246
30, epoch_train_loss=266476.4807880246
Epoch 31
Epoch 31 :: Batch 0/1
Batch Loss = 110141.7663468405
31, epoch_train_loss=110141.7663468405
Epoch 32
Epoch 32 :: Batch 0/1
Batch Loss = 10272.411496191415
32, epoch_train_loss=10272.411496191415
Epoch 33
Epoch 33 :: Batch 0/1
Batch Loss = 195882.26791943112
33, epoch_train_loss=195882.26791943112
Epoch 34
Epoch 34 :: Batch 0/1
Batch Loss = 125173.24637645084
34, epoch_train_loss=125173.24637645084
Epoch 35
Epoch 35 :: Batch 0/1
Batch Loss = 202.67832443914915
35, epoch_train_loss=202.67832443914915
Epoch 36
Epoch 36 :: Batch 0/1
Batch Loss = 107144.88111443404
36, epoch_train_loss=107144.88111443404
Epoch 37
Epoch 37 :: Batch 0/1
Batch Loss = 132527.20612626418
37, epoch_train_loss=132527.20612626418
Epoch 38
Epoch 38 :: Batch 0/1
Batch Loss = 21021.24722834454
38, epoch_train_loss=21021.24722834454
Epoch 39
Epoch 39 :: Batch 0/1
Batch Loss = 17768.83334751158
39, epoch_train_loss=17768.83334751158
Epoch 40
Epoch 40 :: Batch 0/1
Batch Loss = 72725.03964918798
40, epoch_train_loss=72725.03964918798
Epoch 41
Epoch 41 :: Batch 0/1
Batch Loss = 53759.83565036699
41, epoch_train_loss=53759.83565036699
Epoch 42
Epoch 42 :: Batch 0/1
Batch Loss = 9908.34880734348
42, epoch_train_loss=9908.34880734348
Epoch 43
Epoch 43 :: Batch 0/1
Batch Loss = 3077.7808484474263
43, epoch_train_loss=3077.7808484474263
Epoch 44
Epoch 44 :: Batch 0/1
Batch Loss = 28371.997089167267
44, epoch_train_loss=28371.997089167267
Epoch 45
Epoch 45 :: Batch 0/1
Batch Loss = 41507.90979693575
45, epoch_train_loss=41507.90979693575
Epoch 46
Epoch 46 :: Batch 0/1
Batch Loss = 25908.962943145594
46, epoch_train_loss=25908.962943145594
Epoch 47
Epoch 47 :: Batch 0/1
Batch Loss = 4491.902761588399
47, epoch_train_loss=4491.902761588399
Epoch 48
Epoch 48 :: Batch 0/1
Batch Loss = 1517.5807005901813
48, epoch_train_loss=1517.5807005901813
Epoch 49
Epoch 49 :: Batch 0/1
Batch Loss = 14223.912091365914
49, epoch_train_loss=14223.912091365914
Epoch 50
Epoch 50 :: Batch 0/1
Batch Loss = 22509.730329587717
50, epoch_train_loss=22509.730329587717
Epoch 51
Epoch 51 :: Batch 0/1
Batch Loss = 16286.98609504705
51, epoch_train_loss=16286.98609504705
Epoch 52
Epoch 52 :: Batch 0/1
Batch Loss = 4489.249867700858
52, epoch_train_loss=4489.249867700858
Epoch 53
Epoch 53 :: Batch 0/1
Batch Loss = 54.747300464488916
53, epoch_train_loss=54.747300464488916
Epoch 54
Epoch 54 :: Batch 0/1
Batch Loss = 4952.316435753548
54, epoch_train_loss=4952.316435753548
Epoch 55
Epoch 55 :: Batch 0/1
Batch Loss = 11082.069339747733
55, epoch_train_loss=11082.069339747733
Epoch 56
Epoch 56 :: Batch 0/1
Batch Loss = 11122.208964909121
56, epoch_train_loss=11122.208964909121
Epoch 57
Epoch 57 :: Batch 0/1
Batch Loss = 5672.393016810875
57, epoch_train_loss=5672.393016810875
Epoch 58
Epoch 58 :: Batch 0/1
Batch Loss = 734.8616158726917
58, epoch_train_loss=734.8616158726917
Epoch 59
Epoch 59 :: Batch 0/1
Batch Loss = 560.6300354143394
59, epoch_train_loss=560.6300354143394
Epoch 60
Epoch 60 :: Batch 0/1
Batch Loss = 3876.060689423383
60, epoch_train_loss=3876.060689423383
Epoch 61
Epoch 61 :: Batch 0/1
Batch Loss = 6295.415861767492
61, epoch_train_loss=6295.415861767492
Epoch 62
Epoch 62 :: Batch 0/1
Batch Loss = 5265.151064641797
62, epoch_train_loss=5265.151064641797
Epoch 63
Epoch 63 :: Batch 0/1
Batch Loss = 2168.4936685421994
63, epoch_train_loss=2168.4936685421994
Epoch 64
Epoch 64 :: Batch 0/1
Batch Loss = 116.93973615477273
64, epoch_train_loss=116.93973615477273
Epoch 65
Epoch 65 :: Batch 0/1
Batch Loss = 588.489628717186
65, epoch_train_loss=588.489628717186
Epoch 66
Epoch 66 :: Batch 0/1
Batch Loss = 2397.221900083931
66, epoch_train_loss=2397.221900083931
Epoch 67
Epoch 67 :: Batch 0/1
Batch Loss = 3378.447631583282
67, epoch_train_loss=3378.447631583282
Epoch 68
Epoch 68 :: Batch 0/1
Batch Loss = 2601.548817932775
68, epoch_train_loss=2601.548817932775
Epoch 69
Epoch 69 :: Batch 0/1
Batch Loss = 973.3193356218261
69, epoch_train_loss=973.3193356218261
Epoch 70
Epoch 70 :: Batch 0/1
Batch Loss = 28.837468457352283
70, epoch_train_loss=28.837468457352283
Epoch 71
Epoch 71 :: Batch 0/1
Batch Loss = 387.87324352916335
71, epoch_train_loss=387.87324352916335
Epoch 72
Epoch 72 :: Batch 0/1
Batch Loss = 1352.62795485037
72, epoch_train_loss=1352.62795485037
Epoch 73
Epoch 73 :: Batch 0/1
Batch Loss = 1802.9259984208777
73, epoch_train_loss=1802.9259984208777
Epoch 74
Epoch 74 :: Batch 0/1
Batch Loss = 1322.184962973027
74, epoch_train_loss=1322.184962973027
Epoch 75
Epoch 75 :: Batch 0/1
Batch Loss = 453.60337883579126
75, epoch_train_loss=453.60337883579126
Epoch 76
Epoch 76 :: Batch 0/1
Batch Loss = 6.4303075266988685
76, epoch_train_loss=6.4303075266988685
Epoch 77
Epoch 77 :: Batch 0/1
Batch Loss = 249.15609345549214
77, epoch_train_loss=249.15609345549214
Epoch 78
Epoch 78 :: Batch 0/1
Batch Loss = 764.191647990343
78, epoch_train_loss=764.191647990343
Epoch 79
Epoch 79 :: Batch 0/1
Batch Loss = 966.1493081308611
79, epoch_train_loss=966.1493081308611
Epoch 80
Epoch 80 :: Batch 0/1
Batch Loss = 676.212947458365
80, epoch_train_loss=676.212947458365
Epoch 81
Epoch 81 :: Batch 0/1
Batch Loss = 212.47731551772554
81, epoch_train_loss=212.47731551772554
Epoch 82
Epoch 82 :: Batch 0/1
Batch Loss = 1.9447802035056434
82, epoch_train_loss=1.9447802035056434
Epoch 83
Epoch 83 :: Batch 0/1
Batch Loss = 160.24015989024016
83, epoch_train_loss=160.24015989024016
Epoch 84
Epoch 84 :: Batch 0/1
Batch Loss = 437.4150797797438
84, epoch_train_loss=437.4150797797438
Epoch 85
Epoch 85 :: Batch 0/1
Batch Loss = 518.9743991489462
85, epoch_train_loss=518.9743991489462
Epoch 86
Epoch 86 :: Batch 0/1
Batch Loss = 336.524163551822
86, epoch_train_loss=336.524163551822
Epoch 87
Epoch 87 :: Batch 0/1
Batch Loss = 88.88838574132075
87, epoch_train_loss=88.88838574132075
Epoch 88
Epoch 88 :: Batch 0/1
Batch Loss = 3.298189781394758
88, epoch_train_loss=3.298189781394758
Epoch 89
Epoch 89 :: Batch 0/1
Batch Loss = 112.23452252696738
89, epoch_train_loss=112.23452252696738
Epoch 90
Epoch 90 :: Batch 0/1
Batch Loss = 257.4065676151006
90, epoch_train_loss=257.4065676151006
Epoch 91
Epoch 91 :: Batch 0/1
Batch Loss = 276.12024333722337
91, epoch_train_loss=276.12024333722337
Epoch 92
Epoch 92 :: Batch 0/1
Batch Loss = 158.84637366268706
92, epoch_train_loss=158.84637366268706
Epoch 93
Epoch 93 :: Batch 0/1
Batch Loss = 31.184130454419037
93, epoch_train_loss=31.184130454419037
Epoch 94
Epoch 94 :: Batch 0/1
Batch Loss = 7.716486736096677
94, epoch_train_loss=7.716486736096677
Epoch 95
Epoch 95 :: Batch 0/1
Batch Loss = 81.76035606570996
95, epoch_train_loss=81.76035606570996
Epoch 96
Epoch 96 :: Batch 0/1
Batch Loss = 152.9036789725876
96, epoch_train_loss=152.9036789725876
Epoch 97
Epoch 97 :: Batch 0/1
Batch Loss = 142.4264002227953
97, epoch_train_loss=142.4264002227953
Epoch 98
Epoch 98 :: Batch 0/1
Batch Loss = 67.58751546556017
98, epoch_train_loss=67.58751546556017
Epoch 99
Epoch 99 :: Batch 0/1
Batch Loss = 7.4791352967904
99, epoch_train_loss=7.4791352967904
Epoch 100
Epoch 100 :: Batch 0/1
Batch Loss = 12.964546664017787
100, epoch_train_loss=12.964546664017787
Epoch 101
Epoch 101 :: Batch 0/1
Batch Loss = 60.620060713843436
101, epoch_train_loss=60.620060713843436
Epoch 102
Epoch 102 :: Batch 0/1
Batch Loss = 89.38016981221594
102, epoch_train_loss=89.38016981221594
Epoch 103
Epoch 103 :: Batch 0/1
Batch Loss = 68.72956818663575
103, epoch_train_loss=68.72956818663575
Epoch 104
Epoch 104 :: Batch 0/1
Batch Loss = 24.20721230436052
104, epoch_train_loss=24.20721230436052
Epoch 105
Epoch 105 :: Batch 0/1
Batch Loss = 1.5489064018105605
105, epoch_train_loss=1.5489064018105605
Epoch 106
Epoch 106 :: Batch 0/1
Batch Loss = 16.380250934701092
106, epoch_train_loss=16.380250934701092
Epoch 107
Epoch 107 :: Batch 0/1
Batch Loss = 43.35764179049081
107, epoch_train_loss=43.35764179049081
Epoch 108
Epoch 108 :: Batch 0/1
Batch Loss = 49.41778548510161
108, epoch_train_loss=49.41778548510161
Epoch 109
Epoch 109 :: Batch 0/1
Batch Loss = 29.37923637329522
109, epoch_train_loss=29.37923637329522
Epoch 110
Epoch 110 :: Batch 0/1
Batch Loss = 6.429227509839315
110, epoch_train_loss=6.429227509839315
Epoch 111
Epoch 111 :: Batch 0/1
Batch Loss = 2.8886011057276284
111, epoch_train_loss=2.8886011057276284
Epoch 112
Epoch 112 :: Batch 0/1
Batch Loss = 16.92571304879709
112, epoch_train_loss=16.92571304879709
Epoch 113
Epoch 113 :: Batch 0/1
Batch Loss = 28.745903429719007
113, epoch_train_loss=28.745903429719007
Epoch 114
Epoch 114 :: Batch 0/1
Batch Loss = 24.61411079857207
114, epoch_train_loss=24.61411079857207
Epoch 115
Epoch 115 :: Batch 0/1
Batch Loss = 10.333550022840432
115, epoch_train_loss=10.333550022840432
Epoch 116
Epoch 116 :: Batch 0/1
Batch Loss = 1.6241392064142792
116, epoch_train_loss=1.6241392064142792
Epoch 117
Epoch 117 :: Batch 0/1
Batch Loss = 5.570735425007186
117, epoch_train_loss=5.570735425007186
Epoch 118
Epoch 118 :: Batch 0/1
Batch Loss = 14.529552016946072
118, epoch_train_loss=14.529552016946072
Epoch 119
Epoch 119 :: Batch 0/1
Batch Loss = 16.91699767507957
119, epoch_train_loss=16.91699767507957
Epoch 120
Epoch 120 :: Batch 0/1
Batch Loss = 10.435675027380078
120, epoch_train_loss=10.435675027380078
Epoch 121
Epoch 121 :: Batch 0/1
Batch Loss = 2.9321290547454772
121, epoch_train_loss=2.9321290547454772
Epoch 122
Epoch 122 :: Batch 0/1
Batch Loss = 2.0375294543785123
122, epoch_train_loss=2.0375294543785123
Epoch 123
Epoch 123 :: Batch 0/1
Batch Loss = 6.849147030796192
123, epoch_train_loss=6.849147030796192
Epoch 124
Epoch 124 :: Batch 0/1
Batch Loss = 10.42993405658191
124, epoch_train_loss=10.42993405658191
Epoch 125
Epoch 125 :: Batch 0/1
Batch Loss = 8.502288962040002
125, epoch_train_loss=8.502288962040002
Epoch 126
Epoch 126 :: Batch 0/1
Batch Loss = 3.705892809287624
126, epoch_train_loss=3.705892809287624
Epoch 127
Epoch 127 :: Batch 0/1
Batch Loss = 1.4415268813674011
127, epoch_train_loss=1.4415268813674011
Epoch 128
Epoch 128 :: Batch 0/1
Batch Loss = 3.3624202039790565
128, epoch_train_loss=3.3624202039790565
Epoch 129
Epoch 129 :: Batch 0/1
Batch Loss = 6.183349907236096
129, epoch_train_loss=6.183349907236096
Epoch 130
Epoch 130 :: Batch 0/1
Batch Loss = 6.232504528240285
130, epoch_train_loss=6.232504528240285
Epoch 131
Epoch 131 :: Batch 0/1
Batch Loss = 3.6561831564752114
131, epoch_train_loss=3.6561831564752114
Epoch 132
Epoch 132 :: Batch 0/1
Batch Loss = 1.5753930593838745
132, epoch_train_loss=1.5753930593838745
Epoch 133
Epoch 133 :: Batch 0/1
Batch Loss = 2.012590977933451
133, epoch_train_loss=2.012590977933451
Epoch 134
Epoch 134 :: Batch 0/1
Batch Loss = 3.7826977345878405
134, epoch_train_loss=3.7826977345878405
Epoch 135
Epoch 135 :: Batch 0/1
Batch Loss = 4.4128881479412945
135, epoch_train_loss=4.4128881479412945
Epoch 136
Epoch 136 :: Batch 0/1
Batch Loss = 3.202879822903407
136, epoch_train_loss=3.202879822903407
Epoch 137
Epoch 137 :: Batch 0/1
Batch Loss = 1.7149677122350895
137, epoch_train_loss=1.7149677122350895
Epoch 138
Epoch 138 :: Batch 0/1
Batch Loss = 1.567237098419909
138, epoch_train_loss=1.567237098419909
Epoch 139
Epoch 139 :: Batch 0/1
Batch Loss = 2.5478518813642244
139, epoch_train_loss=2.5478518813642244
Epoch 140
Epoch 140 :: Batch 0/1
Batch Loss = 3.1843345058846886
140, epoch_train_loss=3.1843345058846886
Epoch 141
Epoch 141 :: Batch 0/1
Batch Loss = 2.6833634654339678
141, epoch_train_loss=2.6833634654339678
Epoch 142
Epoch 142 :: Batch 0/1
Batch Loss = 1.7377042615259155
142, epoch_train_loss=1.7377042615259155
Epoch 143
Epoch 143 :: Batch 0/1
Batch Loss = 1.4472979142757698
143, epoch_train_loss=1.4472979142757698
Epoch 144
Epoch 144 :: Batch 0/1
Batch Loss = 1.950183325022915
144, epoch_train_loss=1.950183325022915
Epoch 145
Epoch 145 :: Batch 0/1
Batch Loss = 2.429373714270729
145, epoch_train_loss=2.429373714270729
Epoch 146
Epoch 146 :: Batch 0/1
Batch Loss = 2.2505022949328386
146, epoch_train_loss=2.2505022949328386
Epoch 147
Epoch 147 :: Batch 0/1
Batch Loss = 1.6837989047376274
147, epoch_train_loss=1.6837989047376274
Epoch 148
Epoch 148 :: Batch 0/1
Batch Loss = 1.4198393484016327
148, epoch_train_loss=1.4198393484016327
Epoch 149
Epoch 149 :: Batch 0/1
Batch Loss = 1.6665408264515535
149, epoch_train_loss=1.6665408264515535
Epoch 150
Epoch 150 :: Batch 0/1
Batch Loss = 1.9845731129835842
150, epoch_train_loss=1.9845731129835842
Epoch 151
Epoch 151 :: Batch 0/1
Batch Loss = 1.9321509863340447
151, epoch_train_loss=1.9321509863340447
Epoch 152
Epoch 152 :: Batch 0/1
Batch Loss = 1.6001913464162691
152, epoch_train_loss=1.6001913464162691
Epoch 153
Epoch 153 :: Batch 0/1
Batch Loss = 1.4069542063325473
153, epoch_train_loss=1.4069542063325473
Epoch 154
Epoch 154 :: Batch 0/1
Batch Loss = 1.5293453788885196
154, epoch_train_loss=1.5293453788885196
Epoch 155
Epoch 155 :: Batch 0/1
Batch Loss = 1.7295308406049121
155, epoch_train_loss=1.7295308406049121
Epoch 156
Epoch 156 :: Batch 0/1
Batch Loss = 1.7205579342607609
156, epoch_train_loss=1.7205579342607609
Epoch 157
Epoch 157 :: Batch 0/1
Batch Loss = 1.527523479879777
157, epoch_train_loss=1.527523479879777
Epoch 158
Epoch 158 :: Batch 0/1
Batch Loss = 1.4004265824519824
158, epoch_train_loss=1.4004265824519824
Epoch 159
Epoch 159 :: Batch 0/1
Batch Loss = 1.4644795107994935
159, epoch_train_loss=1.4644795107994935
Epoch 160
Epoch 160 :: Batch 0/1
Batch Loss = 1.586875127210701
160, epoch_train_loss=1.586875127210701
Epoch 161
Epoch 161 :: Batch 0/1
Batch Loss = 1.5879951112042572
161, epoch_train_loss=1.5879951112042572
Epoch 162
Epoch 162 :: Batch 0/1
Batch Loss = 1.473754765433643
162, epoch_train_loss=1.473754765433643
Epoch 163
Epoch 163 :: Batch 0/1
Batch Loss = 1.3947886921603778
163, epoch_train_loss=1.3947886921603778
Epoch 164
Epoch 164 :: Batch 0/1
Batch Loss = 1.431285799144805
164, epoch_train_loss=1.431285799144805
Epoch 165
Epoch 165 :: Batch 0/1
Batch Loss = 1.50456258224398
165, epoch_train_loss=1.50456258224398
Epoch 166
Epoch 166 :: Batch 0/1
Batch Loss = 1.5047859626299225
166, epoch_train_loss=1.5047859626299225
Epoch 167
Epoch 167 :: Batch 0/1
Batch Loss = 1.4355048600212945
167, epoch_train_loss=1.4355048600212945
Epoch 168
Epoch 168 :: Batch 0/1
Batch Loss = 1.3886991849185968
168, epoch_train_loss=1.3886991849185968
Epoch 169
Epoch 169 :: Batch 0/1
Batch Loss = 1.4117102568097069
169, epoch_train_loss=1.4117102568097069
Epoch 170
Epoch 170 :: Batch 0/1
Batch Loss = 1.4547356829321096
170, epoch_train_loss=1.4547356829321096
Epoch 171
Epoch 171 :: Batch 0/1
Batch Loss = 1.4519888192862687
171, epoch_train_loss=1.4519888192862687
Epoch 172
Epoch 172 :: Batch 0/1
Batch Loss = 1.4089915994478623
172, epoch_train_loss=1.4089915994478623
Epoch 173
Epoch 173 :: Batch 0/1
Batch Loss = 1.3825903534129789
173, epoch_train_loss=1.3825903534129789
Epoch 174
Epoch 174 :: Batch 0/1
Batch Loss = 1.3982764257167883
174, epoch_train_loss=1.3982764257167883
Epoch 175
Epoch 175 :: Batch 0/1
Batch Loss = 1.4227843948411105
175, epoch_train_loss=1.4227843948411105
Epoch 176
Epoch 176 :: Batch 0/1
Batch Loss = 1.4178187642148485
176, epoch_train_loss=1.4178187642148485
Epoch 177
Epoch 177 :: Batch 0/1
Batch Loss = 1.3907762760574252
177, epoch_train_loss=1.3907762760574252
Epoch 178
Epoch 178 :: Batch 0/1
Batch Loss = 1.3768313413234006
178, epoch_train_loss=1.3768313413234006
Epoch 179
Epoch 179 :: Batch 0/1
Batch Loss = 1.3878690058285734
179, epoch_train_loss=1.3878690058285734
Epoch 180
Epoch 180 :: Batch 0/1
Batch Loss = 1.4010595215782886
180, epoch_train_loss=1.4010595215782886
Epoch 181
Epoch 181 :: Batch 0/1
Batch Loss = 1.3951324044337696
181, epoch_train_loss=1.3951324044337696
Epoch 182
Epoch 182 :: Batch 0/1
Batch Loss = 1.3781365773396685
182, epoch_train_loss=1.3781365773396685
Epoch 183
Epoch 183 :: Batch 0/1
Batch Loss = 1.3714854890071388
183, epoch_train_loss=1.3714854890071388
Epoch 184
Epoch 184 :: Batch 0/1
Batch Loss = 1.3791301585497533
184, epoch_train_loss=1.3791301585497533
Epoch 185
Epoch 185 :: Batch 0/1
Batch Loss = 1.3854445551320553
185, epoch_train_loss=1.3854445551320553
Epoch 186
Epoch 186 :: Batch 0/1
Batch Loss = 1.379572353075692
186, epoch_train_loss=1.379572353075692
Epoch 187
Epoch 187 :: Batch 0/1
Batch Loss = 1.3690634338629974
187, epoch_train_loss=1.3690634338629974
Epoch 188
Epoch 188 :: Batch 0/1
Batch Loss = 1.3664192256643364
188, epoch_train_loss=1.3664192256643364
Epoch 189
Epoch 189 :: Batch 0/1
Batch Loss = 1.3713926156125333
189, epoch_train_loss=1.3713926156125333
Epoch 190
Epoch 190 :: Batch 0/1
Batch Loss = 1.373614527133829
190, epoch_train_loss=1.373614527133829
Epoch 191
Epoch 191 :: Batch 0/1
Batch Loss = 1.3684137900709066
191, epoch_train_loss=1.3684137900709066
Epoch 192
Epoch 192 :: Batch 0/1
Batch Loss = 1.36210047324817
192, epoch_train_loss=1.36210047324817
Epoch 193
Epoch 193 :: Batch 0/1
Batch Loss = 1.3614095703069777
193, epoch_train_loss=1.3614095703069777
Epoch 194
Epoch 194 :: Batch 0/1
Batch Loss = 1.3642559600677064
194, epoch_train_loss=1.3642559600677064
Epoch 195
Epoch 195 :: Batch 0/1
Batch Loss = 1.3641675924023395
195, epoch_train_loss=1.3641675924023395
Epoch 196
Epoch 196 :: Batch 0/1
Batch Loss = 1.3599203715945083
196, epoch_train_loss=1.3599203715945083
Epoch 197
Epoch 197 :: Batch 0/1
Batch Loss = 1.3562597511765568
197, epoch_train_loss=1.3562597511765568
Epoch 198
Epoch 198 :: Batch 0/1
Batch Loss = 1.3562965869814845
198, epoch_train_loss=1.3562965869814845
Epoch 199
Epoch 199 :: Batch 0/1
Batch Loss = 1.357515701618448
199, epoch_train_loss=1.357515701618448
Epoch 200
Epoch 200 :: Batch 0/1
Batch Loss = 1.3562629460181301
200, epoch_train_loss=1.3562629460181301
Epoch 201
Epoch 201 :: Batch 0/1
Batch Loss = 1.3530036102189391
201, epoch_train_loss=1.3530036102189391
Epoch 202
Epoch 202 :: Batch 0/1
Batch Loss = 1.350926773424137
202, epoch_train_loss=1.350926773424137
Epoch 203
Epoch 203 :: Batch 0/1
Batch Loss = 1.3510155225167564
203, epoch_train_loss=1.3510155225167564
Epoch 204
Epoch 204 :: Batch 0/1
Batch Loss = 1.3510779374597082
204, epoch_train_loss=1.3510779374597082
Epoch 205
Epoch 205 :: Batch 0/1
Batch Loss = 1.349385817437582
205, epoch_train_loss=1.349385817437582
Epoch 206
Epoch 206 :: Batch 0/1
Batch Loss = 1.3470069175420594
206, epoch_train_loss=1.3470069175420594
Epoch 207
Epoch 207 :: Batch 0/1
Batch Loss = 1.3457976464933326
207, epoch_train_loss=1.3457976464933326
Epoch 208
Epoch 208 :: Batch 0/1
Batch Loss = 1.3456469884655629
208, epoch_train_loss=1.3456469884655629
Epoch 209
Epoch 209 :: Batch 0/1
Batch Loss = 1.3449858816130018
209, epoch_train_loss=1.3449858816130018
Epoch 210
Epoch 210 :: Batch 0/1
Batch Loss = 1.3432663966887974
210, epoch_train_loss=1.3432663966887974
Epoch 211
Epoch 211 :: Batch 0/1
Batch Loss = 1.3415510412441836
211, epoch_train_loss=1.3415510412441836
Epoch 212
Epoch 212 :: Batch 0/1
Batch Loss = 1.340711146466684
212, epoch_train_loss=1.340711146466684
Epoch 213
Epoch 213 :: Batch 0/1
Batch Loss = 1.3402212037208225
213, epoch_train_loss=1.3402212037208225
Epoch 214
Epoch 214 :: Batch 0/1
Batch Loss = 1.3391610690261955
214, epoch_train_loss=1.3391610690261955
Epoch 215
Epoch 215 :: Batch 0/1
Batch Loss = 1.3376015746949221
215, epoch_train_loss=1.3376015746949221
Epoch 216
Epoch 216 :: Batch 0/1
Batch Loss = 1.336322093982238
216, epoch_train_loss=1.336322093982238
Epoch 217
Epoch 217 :: Batch 0/1
Batch Loss = 1.3355624492353861
217, epoch_train_loss=1.3355624492353861
Epoch 218
Epoch 218 :: Batch 0/1
Batch Loss = 1.334775522939295
218, epoch_train_loss=1.334775522939295
Epoch 219
Epoch 219 :: Batch 0/1
Batch Loss = 1.333558539711087
219, epoch_train_loss=1.333558539711087
Epoch 220
Epoch 220 :: Batch 0/1
Batch Loss = 1.332208099002233
220, epoch_train_loss=1.332208099002233
Epoch 221
Epoch 221 :: Batch 0/1
Batch Loss = 1.3311629403692014
221, epoch_train_loss=1.3311629403692014
Epoch 222
Epoch 222 :: Batch 0/1
Batch Loss = 1.3303410521788943
222, epoch_train_loss=1.3303410521788943
Epoch 223
Epoch 223 :: Batch 0/1
Batch Loss = 1.3293533538813094
223, epoch_train_loss=1.3293533538813094
Epoch 224
Epoch 224 :: Batch 0/1
Batch Loss = 1.3281261815207777
224, epoch_train_loss=1.3281261815207777
Epoch 225
Epoch 225 :: Batch 0/1
Batch Loss = 1.3269510186110696
225, epoch_train_loss=1.3269510186110696
Epoch 226
Epoch 226 :: Batch 0/1
Batch Loss = 1.3259910868098534
226, epoch_train_loss=1.3259910868098534
Epoch 227
Epoch 227 :: Batch 0/1
Batch Loss = 1.3250650759512541
227, epoch_train_loss=1.3250650759512541
Epoch 228
Epoch 228 :: Batch 0/1
Batch Loss = 1.3239761175084583
228, epoch_train_loss=1.3239761175084583
Epoch 229
Epoch 229 :: Batch 0/1
Batch Loss = 1.322804268668439
229, epoch_train_loss=1.322804268668439
Epoch 230
Epoch 230 :: Batch 0/1
Batch Loss = 1.321737627927012
230, epoch_train_loss=1.321737627927012
Epoch 231
Epoch 231 :: Batch 0/1
Batch Loss = 1.3207742617169538
231, epoch_train_loss=1.3207742617169538
Epoch 232
Epoch 232 :: Batch 0/1
Batch Loss = 1.3197595404068811
232, epoch_train_loss=1.3197595404068811
Epoch 233
Epoch 233 :: Batch 0/1
Batch Loss = 1.318643410634913
233, epoch_train_loss=1.318643410634913
Epoch 234
Epoch 234 :: Batch 0/1
Batch Loss = 1.317535594834125
234, epoch_train_loss=1.317535594834125
Epoch 235
Epoch 235 :: Batch 0/1
Batch Loss = 1.3165126586683207
235, epoch_train_loss=1.3165126586683207
Epoch 236
Epoch 236 :: Batch 0/1
Batch Loss = 1.3155095140446005
236, epoch_train_loss=1.3155095140446005
Epoch 237
Epoch 237 :: Batch 0/1
Batch Loss = 1.3144425710014604
237, epoch_train_loss=1.3144425710014604
Epoch 238
Epoch 238 :: Batch 0/1
Batch Loss = 1.3133383617918877
238, epoch_train_loss=1.3133383617918877
Epoch 239
Epoch 239 :: Batch 0/1
Batch Loss = 1.3122734145253845
239, epoch_train_loss=1.3122734145253845
Epoch 240
Epoch 240 :: Batch 0/1
Batch Loss = 1.3112497402557641
240, epoch_train_loss=1.3112497402557641
Epoch 241
Epoch 241 :: Batch 0/1
Batch Loss = 1.3102052907866524
241, epoch_train_loss=1.3102052907866524
Epoch 242
Epoch 242 :: Batch 0/1
Batch Loss = 1.3091193554144802
242, epoch_train_loss=1.3091193554144802
Epoch 243
Epoch 243 :: Batch 0/1
Batch Loss = 1.308036248706804
243, epoch_train_loss=1.308036248706804
Epoch 244
Epoch 244 :: Batch 0/1
Batch Loss = 1.3069860215443623
244, epoch_train_loss=1.3069860215443623
Epoch 245
Epoch 245 :: Batch 0/1
Batch Loss = 1.3059412364330445
245, epoch_train_loss=1.3059412364330445
Epoch 246
Epoch 246 :: Batch 0/1
Batch Loss = 1.3048691798057521
246, epoch_train_loss=1.3048691798057521
Epoch 247
Epoch 247 :: Batch 0/1
Batch Loss = 1.3037826897051925
247, epoch_train_loss=1.3037826897051925
Epoch 248
Epoch 248 :: Batch 0/1
Batch Loss = 1.3027121409290856
248, epoch_train_loss=1.3027121409290856
Epoch 249
Epoch 249 :: Batch 0/1
Batch Loss = 1.301655678385652
249, epoch_train_loss=1.301655678385652
Epoch 250
Epoch 250 :: Batch 0/1
Batch Loss = 1.3005879856500495
250, epoch_train_loss=1.3005879856500495
Epoch 251
Epoch 251 :: Batch 0/1
Batch Loss = 1.2995035671334978
251, epoch_train_loss=1.2995035671334978
Epoch 252
Epoch 252 :: Batch 0/1
Batch Loss = 1.2984215268627153
252, epoch_train_loss=1.2984215268627153
Epoch 253
Epoch 253 :: Batch 0/1
Batch Loss = 1.2973514770983137
253, epoch_train_loss=1.2973514770983137
Epoch 254
Epoch 254 :: Batch 0/1
Batch Loss = 1.2962805186250255
254, epoch_train_loss=1.2962805186250255
Epoch 255
Epoch 255 :: Batch 0/1
Batch Loss = 1.295197482971929
255, epoch_train_loss=1.295197482971929
Epoch 256
Epoch 256 :: Batch 0/1
Batch Loss = 1.2941101397534491
256, epoch_train_loss=1.2941101397534491
Epoch 257
Epoch 257 :: Batch 0/1
Batch Loss = 1.2930298477336475
257, epoch_train_loss=1.2930298477336475
Epoch 258
Epoch 258 :: Batch 0/1
Batch Loss = 1.2919532607779274
258, epoch_train_loss=1.2919532607779274
Epoch 259
Epoch 259 :: Batch 0/1
Batch Loss = 1.290870215851997
259, epoch_train_loss=1.290870215851997
Epoch 260
Epoch 260 :: Batch 0/1
Batch Loss = 1.2897807885049164
260, epoch_train_loss=1.2897807885049164
Epoch 261
Epoch 261 :: Batch 0/1
Batch Loss = 1.2886931498895997
261, epoch_train_loss=1.2886931498895997
Epoch 262
Epoch 262 :: Batch 0/1
Batch Loss = 1.287609077279703
262, epoch_train_loss=1.287609077279703
Epoch 263
Epoch 263 :: Batch 0/1
Batch Loss = 1.2865221656068868
263, epoch_train_loss=1.2865221656068868
Epoch 264
Epoch 264 :: Batch 0/1
Batch Loss = 1.285429317125055
264, epoch_train_loss=1.285429317125055
Epoch 265
Epoch 265 :: Batch 0/1
Batch Loss = 1.2843424972450155
265, epoch_train_loss=1.2843424972450155
Epoch 266
Epoch 266 :: Batch 0/1
Batch Loss = 1.283247999461231
266, epoch_train_loss=1.283247999461231
Epoch 267
Epoch 267 :: Batch 0/1
Batch Loss = 1.2821535983794778
267, epoch_train_loss=1.2821535983794778
Epoch 268
Epoch 268 :: Batch 0/1
Batch Loss = 1.2810616197957239
268, epoch_train_loss=1.2810616197957239
Epoch 269
Epoch 269 :: Batch 0/1
Batch Loss = 1.279968512181322
269, epoch_train_loss=1.279968512181322
Epoch 270
Epoch 270 :: Batch 0/1
Batch Loss = 1.2788768319291113
270, epoch_train_loss=1.2788768319291113
Epoch 271
Epoch 271 :: Batch 0/1
Batch Loss = 1.2777853776419115
271, epoch_train_loss=1.2777853776419115
Epoch 272
Epoch 272 :: Batch 0/1
Batch Loss = 1.2766907525226545
272, epoch_train_loss=1.2766907525226545
Epoch 273
Epoch 273 :: Batch 0/1
Batch Loss = 1.2755923144847605
273, epoch_train_loss=1.2755923144847605
Epoch 274
Epoch 274 :: Batch 0/1
Batch Loss = 1.2744921714609343
274, epoch_train_loss=1.2744921714609343
Epoch 275
Epoch 275 :: Batch 0/1
Batch Loss = 1.2733972547011856
275, epoch_train_loss=1.2733972547011856
Epoch 276
Epoch 276 :: Batch 0/1
Batch Loss = 1.2723064512032602
276, epoch_train_loss=1.2723064512032602
Epoch 277
Epoch 277 :: Batch 0/1
Batch Loss = 1.2712096466213398
277, epoch_train_loss=1.2712096466213398
Epoch 278
Epoch 278 :: Batch 0/1
Batch Loss = 1.2701222414717597
278, epoch_train_loss=1.2701222414717597
Epoch 279
Epoch 279 :: Batch 0/1
Batch Loss = 1.2690328863079046
279, epoch_train_loss=1.2690328863079046
Epoch 280
Epoch 280 :: Batch 0/1
Batch Loss = 1.2679398321960982
280, epoch_train_loss=1.2679398321960982
Epoch 281
Epoch 281 :: Batch 0/1
Batch Loss = 1.2668482584543026
281, epoch_train_loss=1.2668482584543026
Epoch 282
Epoch 282 :: Batch 0/1
Batch Loss = 1.2657661580773212
282, epoch_train_loss=1.2657661580773212
Epoch 283
Epoch 283 :: Batch 0/1
Batch Loss = 1.2646797024810423
283, epoch_train_loss=1.2646797024810423
Epoch 284
Epoch 284 :: Batch 0/1
Batch Loss = 1.2636018909180338
284, epoch_train_loss=1.2636018909180338
Epoch 285
Epoch 285 :: Batch 0/1
Batch Loss = 1.2625160678180314
285, epoch_train_loss=1.2625160678180314
Epoch 286
Epoch 286 :: Batch 0/1
Batch Loss = 1.2614320869315343
286, epoch_train_loss=1.2614320869315343
Epoch 287
Epoch 287 :: Batch 0/1
Batch Loss = 1.2603534939802297
287, epoch_train_loss=1.2603534939802297
Epoch 288
Epoch 288 :: Batch 0/1
Batch Loss = 1.2592798366877065
288, epoch_train_loss=1.2592798366877065
Epoch 289
Epoch 289 :: Batch 0/1
Batch Loss = 1.2582058254448751
289, epoch_train_loss=1.2582058254448751
Epoch 290
Epoch 290 :: Batch 0/1
Batch Loss = 1.2571261674251737
290, epoch_train_loss=1.2571261674251737
Epoch 291
Epoch 291 :: Batch 0/1
Batch Loss = 1.256060576680393
291, epoch_train_loss=1.256060576680393
Epoch 292
Epoch 292 :: Batch 0/1
Batch Loss = 1.2549903148700188
292, epoch_train_loss=1.2549903148700188
Epoch 293
Epoch 293 :: Batch 0/1
Batch Loss = 1.2539238106483894
293, epoch_train_loss=1.2539238106483894
Epoch 294
Epoch 294 :: Batch 0/1
Batch Loss = 1.2528529932470187
294, epoch_train_loss=1.2528529932470187
Epoch 295
Epoch 295 :: Batch 0/1
Batch Loss = 1.2517880491412106
295, epoch_train_loss=1.2517880491412106
Epoch 296
Epoch 296 :: Batch 0/1
Batch Loss = 1.2507302021416329
296, epoch_train_loss=1.2507302021416329
Epoch 297
Epoch 297 :: Batch 0/1
Batch Loss = 1.249665479679289
297, epoch_train_loss=1.249665479679289
Epoch 298
Epoch 298 :: Batch 0/1
Batch Loss = 1.2486158176464206
298, epoch_train_loss=1.2486158176464206
Epoch 299
Epoch 299 :: Batch 0/1
Batch Loss = 1.2475523832596946
299, epoch_train_loss=1.2475523832596946
Epoch 300
Epoch 300 :: Batch 0/1
Batch Loss = 1.246511536999383
300, epoch_train_loss=1.246511536999383
Epoch 301
Epoch 301 :: Batch 0/1
Batch Loss = 1.2454582380577526
301, epoch_train_loss=1.2454582380577526
Epoch 302
Epoch 302 :: Batch 0/1
Batch Loss = 1.2443934537198071
302, epoch_train_loss=1.2443934537198071
Epoch 303
Epoch 303 :: Batch 0/1
Batch Loss = 1.2433462573336613
303, epoch_train_loss=1.2433462573336613
Epoch 304
Epoch 304 :: Batch 0/1
Batch Loss = 1.242305722725185
304, epoch_train_loss=1.242305722725185
Epoch 305
Epoch 305 :: Batch 0/1
Batch Loss = 1.2412542364788746
305, epoch_train_loss=1.2412542364788746
Epoch 306
Epoch 306 :: Batch 0/1
Batch Loss = 1.2402085870827346
306, epoch_train_loss=1.2402085870827346
Epoch 307
Epoch 307 :: Batch 0/1
Batch Loss = 1.2391648403744369
307, epoch_train_loss=1.2391648403744369
Epoch 308
Epoch 308 :: Batch 0/1
Batch Loss = 1.2380508453564543
308, epoch_train_loss=1.2380508453564543
Epoch 309
Epoch 309 :: Batch 0/1
Batch Loss = 1.2370848095063096
309, epoch_train_loss=1.2370848095063096
Epoch 310
Epoch 310 :: Batch 0/1
Batch Loss = 1.236171614514512
310, epoch_train_loss=1.236171614514512
Epoch 311
Epoch 311 :: Batch 0/1
Batch Loss = 1.2352157208459709
311, epoch_train_loss=1.2352157208459709
Epoch 312
Epoch 312 :: Batch 0/1
Batch Loss = 1.2341996757831126
312, epoch_train_loss=1.2341996757831126
Epoch 313
Epoch 313 :: Batch 0/1
Batch Loss = 1.2331245268799336
313, epoch_train_loss=1.2331245268799336
Epoch 314
Epoch 314 :: Batch 0/1
Batch Loss = 1.2319980695450792
314, epoch_train_loss=1.2319980695450792
Epoch 315
Epoch 315 :: Batch 0/1
Batch Loss = 1.2307671274251017
315, epoch_train_loss=1.2307671274251017
Epoch 316
Epoch 316 :: Batch 0/1
Batch Loss = 1.2301175969824412
316, epoch_train_loss=1.2301175969824412
Epoch 317
Epoch 317 :: Batch 0/1
Batch Loss = 1.2288120073888644
317, epoch_train_loss=1.2288120073888644
Epoch 318
Epoch 318 :: Batch 0/1
Batch Loss = 1.2278906321995802
318, epoch_train_loss=1.2278906321995802
Epoch 319
Epoch 319 :: Batch 0/1
Batch Loss = 1.2270288080980043
319, epoch_train_loss=1.2270288080980043
Epoch 320
Epoch 320 :: Batch 0/1
Batch Loss = 1.226093130529501
320, epoch_train_loss=1.226093130529501
Epoch 321
Epoch 321 :: Batch 0/1
Batch Loss = 1.2250778785133467
321, epoch_train_loss=1.2250778785133467
Epoch 322
Epoch 322 :: Batch 0/1
Batch Loss = 1.2239889997807087
322, epoch_train_loss=1.2239889997807087
Epoch 323
Epoch 323 :: Batch 0/1
Batch Loss = 1.2228415303867246
323, epoch_train_loss=1.2228415303867246
Epoch 324
Epoch 324 :: Batch 0/1
Batch Loss = 1.2216543983981267
324, epoch_train_loss=1.2216543983981267
Epoch 325
Epoch 325 :: Batch 0/1
Batch Loss = 1.2208631518465383
325, epoch_train_loss=1.2208631518465383
Epoch 326
Epoch 326 :: Batch 0/1
Batch Loss = 1.2196327589718168
326, epoch_train_loss=1.2196327589718168
Epoch 327
Epoch 327 :: Batch 0/1
Batch Loss = 1.2187953168167942
327, epoch_train_loss=1.2187953168167942
Epoch 328
Epoch 328 :: Batch 0/1
Batch Loss = 1.2178797924761942
328, epoch_train_loss=1.2178797924761942
Epoch 329
Epoch 329 :: Batch 0/1
Batch Loss = 1.2168998991951168
329, epoch_train_loss=1.2168998991951168
Epoch 330
Epoch 330 :: Batch 0/1
Batch Loss = 1.215838085174371
330, epoch_train_loss=1.215838085174371
Epoch 331
Epoch 331 :: Batch 0/1
Batch Loss = 1.2146420019632667
331, epoch_train_loss=1.2146420019632667
Epoch 332
Epoch 332 :: Batch 0/1
Batch Loss = 1.2139666572243824
332, epoch_train_loss=1.2139666572243824
Epoch 333
Epoch 333 :: Batch 0/1
Batch Loss = 1.212673855754683
333, epoch_train_loss=1.212673855754683
Epoch 334
Epoch 334 :: Batch 0/1
Batch Loss = 1.2118855918584879
334, epoch_train_loss=1.2118855918584879
Epoch 335
Epoch 335 :: Batch 0/1
Batch Loss = 1.2110102740668114
335, epoch_train_loss=1.2110102740668114
Epoch 336
Epoch 336 :: Batch 0/1
Batch Loss = 1.2100265093073415
336, epoch_train_loss=1.2100265093073415
Epoch 337
Epoch 337 :: Batch 0/1
Batch Loss = 1.2089402017220425
337, epoch_train_loss=1.2089402017220425
Epoch 338
Epoch 338 :: Batch 0/1
Batch Loss = 1.2077318683533607
338, epoch_train_loss=1.2077318683533607
Epoch 339
Epoch 339 :: Batch 0/1
Batch Loss = 1.2070359431174216
339, epoch_train_loss=1.2070359431174216
Epoch 340
Epoch 340 :: Batch 0/1
Batch Loss = 1.2057785448467275
340, epoch_train_loss=1.2057785448467275
Epoch 341
Epoch 341 :: Batch 0/1
Batch Loss = 1.205007236240273
341, epoch_train_loss=1.205007236240273
Epoch 342
Epoch 342 :: Batch 0/1
Batch Loss = 1.2040958408172808
342, epoch_train_loss=1.2040958408172808
Epoch 343
Epoch 343 :: Batch 0/1
Batch Loss = 1.203075584529179
343, epoch_train_loss=1.203075584529179
Epoch 344
Epoch 344 :: Batch 0/1
Batch Loss = 1.2018990475095295
344, epoch_train_loss=1.2018990475095295
Epoch 345
Epoch 345 :: Batch 0/1
Batch Loss = 1.2012139647637365
345, epoch_train_loss=1.2012139647637365
Epoch 346
Epoch 346 :: Batch 0/1
Batch Loss = 1.1999747648098933
346, epoch_train_loss=1.1999747648098933
Epoch 347
Epoch 347 :: Batch 0/1
Batch Loss = 1.1992264247620492
347, epoch_train_loss=1.1992264247620492
Epoch 348
Epoch 348 :: Batch 0/1
Batch Loss = 1.1983581098911682
348, epoch_train_loss=1.1983581098911682
Epoch 349
Epoch 349 :: Batch 0/1
Batch Loss = 1.1973545067499525
349, epoch_train_loss=1.1973545067499525
Epoch 350
Epoch 350 :: Batch 0/1
Batch Loss = 1.196163843516428
350, epoch_train_loss=1.196163843516428
Epoch 351
Epoch 351 :: Batch 0/1
Batch Loss = 1.1959739148317858
351, epoch_train_loss=1.1959739148317858
Epoch 352
Epoch 352 :: Batch 0/1
Batch Loss = 1.1944325677253618
352, epoch_train_loss=1.1944325677253618
Epoch 353
Epoch 353 :: Batch 0/1
Batch Loss = 1.1936094328698457
353, epoch_train_loss=1.1936094328698457
Epoch 354
Epoch 354 :: Batch 0/1
Batch Loss = 1.1930024513492596
354, epoch_train_loss=1.1930024513492596
Epoch 355
Epoch 355 :: Batch 0/1
Batch Loss = 1.1921788784686045
355, epoch_train_loss=1.1921788784686045
Epoch 356
Epoch 356 :: Batch 0/1
Batch Loss = 1.1911221636782574
356, epoch_train_loss=1.1911221636782574
Epoch 357
Epoch 357 :: Batch 0/1
Batch Loss = 1.1898795411884135
357, epoch_train_loss=1.1898795411884135
Epoch 358
Epoch 358 :: Batch 0/1
Batch Loss = 1.1885209099855132
358, epoch_train_loss=1.1885209099855132
Epoch 359
Epoch 359 :: Batch 0/1
Batch Loss = 1.1882732969857017
359, epoch_train_loss=1.1882732969857017
Epoch 360
Epoch 360 :: Batch 0/1
Batch Loss = 1.186694174811074
360, epoch_train_loss=1.186694174811074
Epoch 361
Epoch 361 :: Batch 0/1
Batch Loss = 1.1860829213110036
361, epoch_train_loss=1.1860829213110036
Epoch 362
Epoch 362 :: Batch 0/1
Batch Loss = 1.185402408610569
362, epoch_train_loss=1.185402408610569
Epoch 363
Epoch 363 :: Batch 0/1
Batch Loss = 1.184504449908914
363, epoch_train_loss=1.184504449908914
Epoch 364
Epoch 364 :: Batch 0/1
Batch Loss = 1.1833947878142717
364, epoch_train_loss=1.1833947878142717
Epoch 365
Epoch 365 :: Batch 0/1
Batch Loss = 1.182186834898088
365, epoch_train_loss=1.182186834898088
Epoch 366
Epoch 366 :: Batch 0/1
Batch Loss = 1.181085185692559
366, epoch_train_loss=1.181085185692559
Epoch 367
Epoch 367 :: Batch 0/1
Batch Loss = 1.1801755474477031
367, epoch_train_loss=1.1801755474477031
Epoch 368
Epoch 368 :: Batch 0/1
Batch Loss = 1.1793894333360464
368, epoch_train_loss=1.1793894333360464
Epoch 369
Epoch 369 :: Batch 0/1
Batch Loss = 1.1783014190509606
369, epoch_train_loss=1.1783014190509606
Epoch 370
Epoch 370 :: Batch 0/1
Batch Loss = 1.177351475128096
370, epoch_train_loss=1.177351475128096
Epoch 371
Epoch 371 :: Batch 0/1
Batch Loss = 1.176600286218592
371, epoch_train_loss=1.176600286218592
Epoch 372
Epoch 372 :: Batch 0/1
Batch Loss = 1.175856516360825
372, epoch_train_loss=1.175856516360825
Epoch 373
Epoch 373 :: Batch 0/1
Batch Loss = 1.1747114694080394
373, epoch_train_loss=1.1747114694080394
Epoch 374
Epoch 374 :: Batch 0/1
Batch Loss = 1.1739050371686706
374, epoch_train_loss=1.1739050371686706
Epoch 375
Epoch 375 :: Batch 0/1
Batch Loss = 1.1731897842667385
375, epoch_train_loss=1.1731897842667385
Epoch 376
Epoch 376 :: Batch 0/1
Batch Loss = 1.1722690721084372
376, epoch_train_loss=1.1722690721084372
Epoch 377
Epoch 377 :: Batch 0/1
Batch Loss = 1.1711560549195774
377, epoch_train_loss=1.1711560549195774
Epoch 378
Epoch 378 :: Batch 0/1
Batch Loss = 1.1699964621651646
378, epoch_train_loss=1.1699964621651646
Epoch 379
Epoch 379 :: Batch 0/1
Batch Loss = 1.1692435526895721
379, epoch_train_loss=1.1692435526895721
Epoch 380
Epoch 380 :: Batch 0/1
Batch Loss = 1.168144013000495
380, epoch_train_loss=1.168144013000495
Epoch 381
Epoch 381 :: Batch 0/1
Batch Loss = 1.1674382365298324
381, epoch_train_loss=1.1674382365298324
Epoch 382
Epoch 382 :: Batch 0/1
Batch Loss = 1.166517906635393
382, epoch_train_loss=1.166517906635393
Epoch 383
Epoch 383 :: Batch 0/1
Batch Loss = 1.1656186594068292
383, epoch_train_loss=1.1656186594068292
Epoch 384
Epoch 384 :: Batch 0/1
Batch Loss = 1.1645196209064461
384, epoch_train_loss=1.1645196209064461
Epoch 385
Epoch 385 :: Batch 0/1
Batch Loss = 1.1656423591262615
385, epoch_train_loss=1.1656423591262615
Epoch 386
Epoch 386 :: Batch 0/1
Batch Loss = 1.1631017146498812
386, epoch_train_loss=1.1631017146498812
Epoch 387
Epoch 387 :: Batch 0/1
Batch Loss = 1.1623196131717248
387, epoch_train_loss=1.1623196131717248
Epoch 388
Epoch 388 :: Batch 0/1
Batch Loss = 1.162106724158691
388, epoch_train_loss=1.162106724158691
Epoch 389
Epoch 389 :: Batch 0/1
Batch Loss = 1.1615267503035056
389, epoch_train_loss=1.1615267503035056
Epoch 390
Epoch 390 :: Batch 0/1
Batch Loss = 1.1605069433109987
390, epoch_train_loss=1.1605069433109987
Epoch 391
Epoch 391 :: Batch 0/1
Batch Loss = 1.159124233576213
391, epoch_train_loss=1.159124233576213
Epoch 392
Epoch 392 :: Batch 0/1
Batch Loss = 1.1575703084574183
392, epoch_train_loss=1.1575703084574183
Epoch 393
Epoch 393 :: Batch 0/1
Batch Loss = 1.1568588123352714
393, epoch_train_loss=1.1568588123352714
Epoch 394
Epoch 394 :: Batch 0/1
Batch Loss = 1.1561485818178532
394, epoch_train_loss=1.1561485818178532
Epoch 395
Epoch 395 :: Batch 0/1
Batch Loss = 1.1596800936054836
395, epoch_train_loss=1.1596800936054836
Epoch 396
Epoch 396 :: Batch 0/1
Batch Loss = 1.1543034070465605
396, epoch_train_loss=1.1543034070465605
Epoch 397
Epoch 397 :: Batch 0/1
Batch Loss = 1.1536449743266919
397, epoch_train_loss=1.1536449743266919
Epoch 398
Epoch 398 :: Batch 0/1
Batch Loss = 1.1527025661970023
398, epoch_train_loss=1.1527025661970023
Epoch 399
Epoch 399 :: Batch 0/1
Batch Loss = 1.151531635561158
399, epoch_train_loss=1.151531635561158
Epoch 400
Epoch 400 :: Batch 0/1
Batch Loss = 1.1502035290846517
400, epoch_train_loss=1.1502035290846517
Epoch 401
Epoch 401 :: Batch 0/1
Batch Loss = 1.15029824973057
401, epoch_train_loss=1.15029824973057
Epoch 402
Epoch 402 :: Batch 0/1
Batch Loss = 1.148543760900994
402, epoch_train_loss=1.148543760900994
Epoch 403
Epoch 403 :: Batch 0/1
Batch Loss = 1.1479843078641399
403, epoch_train_loss=1.1479843078641399
Epoch 404
Epoch 404 :: Batch 0/1
Batch Loss = 1.1474510178700614
404, epoch_train_loss=1.1474510178700614
Epoch 405
Epoch 405 :: Batch 0/1
Batch Loss = 1.1466539327346532
405, epoch_train_loss=1.1466539327346532
Epoch 406
Epoch 406 :: Batch 0/1
Batch Loss = 1.1455905834985736
406, epoch_train_loss=1.1455905834985736
Epoch 407
Epoch 407 :: Batch 0/1
Batch Loss = 1.1443389783426343
407, epoch_train_loss=1.1443389783426343
Epoch 408
Epoch 408 :: Batch 0/1
Batch Loss = 1.1433222707787534
408, epoch_train_loss=1.1433222707787534
Epoch 409
Epoch 409 :: Batch 0/1
Batch Loss = 1.142668896565482
409, epoch_train_loss=1.142668896565482
Epoch 410
Epoch 410 :: Batch 0/1
Batch Loss = 1.1414859697347077
410, epoch_train_loss=1.1414859697347077
Epoch 411
Epoch 411 :: Batch 0/1
Batch Loss = 1.142320641900165
411, epoch_train_loss=1.142320641900165
Epoch 412
Epoch 412 :: Batch 0/1
Batch Loss = 1.140035179270052
412, epoch_train_loss=1.140035179270052
Epoch 413
Epoch 413 :: Batch 0/1
Batch Loss = 1.1392152188779203
413, epoch_train_loss=1.1392152188779203
Epoch 414
Epoch 414 :: Batch 0/1
Batch Loss = 1.138950534562507
414, epoch_train_loss=1.138950534562507
Epoch 415
Epoch 415 :: Batch 0/1
Batch Loss = 1.1383611981464183
415, epoch_train_loss=1.1383611981464183
Epoch 416
Epoch 416 :: Batch 0/1
Batch Loss = 1.1373620522097818
416, epoch_train_loss=1.1373620522097818
Epoch 417
Epoch 417 :: Batch 0/1
Batch Loss = 1.136037400266293
417, epoch_train_loss=1.136037400266293
Epoch 418
Epoch 418 :: Batch 0/1
Batch Loss = 1.134568979509274
418, epoch_train_loss=1.134568979509274
Epoch 419
Epoch 419 :: Batch 0/1
Batch Loss = 1.1339409427321225
419, epoch_train_loss=1.1339409427321225
Epoch 420
Epoch 420 :: Batch 0/1
Batch Loss = 1.1331077826944247
420, epoch_train_loss=1.1331077826944247
Epoch 421
Epoch 421 :: Batch 0/1
Batch Loss = 1.1318417135740617
421, epoch_train_loss=1.1318417135740617
Epoch 422
Epoch 422 :: Batch 0/1
Batch Loss = 1.1313343464044592
422, epoch_train_loss=1.1313343464044592
Epoch 423
Epoch 423 :: Batch 0/1
Batch Loss = 1.1306119698806392
423, epoch_train_loss=1.1306119698806392
Epoch 424
Epoch 424 :: Batch 0/1
Batch Loss = 1.1296138066135906
424, epoch_train_loss=1.1296138066135906
Epoch 425
Epoch 425 :: Batch 0/1
Batch Loss = 1.128406738462711
425, epoch_train_loss=1.128406738462711
Epoch 426
Epoch 426 :: Batch 0/1
Batch Loss = 1.1272193823142407
426, epoch_train_loss=1.1272193823142407
Epoch 427
Epoch 427 :: Batch 0/1
Batch Loss = 1.1267195918428379
427, epoch_train_loss=1.1267195918428379
Epoch 428
Epoch 428 :: Batch 0/1
Batch Loss = 1.1254023571752723
428, epoch_train_loss=1.1254023571752723
Epoch 429
Epoch 429 :: Batch 0/1
Batch Loss = 1.1247616579252244
429, epoch_train_loss=1.1247616579252244
Epoch 430
Epoch 430 :: Batch 0/1
Batch Loss = 1.1239364563532626
430, epoch_train_loss=1.1239364563532626
Epoch 431
Epoch 431 :: Batch 0/1
Batch Loss = 1.1229716781845882
431, epoch_train_loss=1.1229716781845882
Epoch 432
Epoch 432 :: Batch 0/1
Batch Loss = 1.1218546279046346
432, epoch_train_loss=1.1218546279046346
Epoch 433
Epoch 433 :: Batch 0/1
Batch Loss = 1.1212119686604654
433, epoch_train_loss=1.1212119686604654
Epoch 434
Epoch 434 :: Batch 0/1
Batch Loss = 1.1200890788735618
434, epoch_train_loss=1.1200890788735618
Epoch 435
Epoch 435 :: Batch 0/1
Batch Loss = 1.1194258865672335
435, epoch_train_loss=1.1194258865672335
Epoch 436
Epoch 436 :: Batch 0/1
Batch Loss = 1.1186417091069614
436, epoch_train_loss=1.1186417091069614
Epoch 437
Epoch 437 :: Batch 0/1
Batch Loss = 1.1176997236684063
437, epoch_train_loss=1.1176997236684063
Epoch 438
Epoch 438 :: Batch 0/1
Batch Loss = 1.116562683117565
438, epoch_train_loss=1.116562683117565
Epoch 439
Epoch 439 :: Batch 0/1
Batch Loss = 1.1160873625595822
439, epoch_train_loss=1.1160873625595822
Epoch 440
Epoch 440 :: Batch 0/1
Batch Loss = 1.1148535504157686
440, epoch_train_loss=1.1148535504157686
Epoch 441
Epoch 441 :: Batch 0/1
Batch Loss = 1.1141786500072337
441, epoch_train_loss=1.1141786500072337
Epoch 442
Epoch 442 :: Batch 0/1
Batch Loss = 1.1135034299689384
442, epoch_train_loss=1.1135034299689384
Epoch 443
Epoch 443 :: Batch 0/1
Batch Loss = 1.1125832137664642
443, epoch_train_loss=1.1125832137664642
Epoch 444
Epoch 444 :: Batch 0/1
Batch Loss = 1.1114760567421638
444, epoch_train_loss=1.1114760567421638
Epoch 445
Epoch 445 :: Batch 0/1
Batch Loss = 1.1104009693947559
445, epoch_train_loss=1.1104009693947559
Epoch 446
Epoch 446 :: Batch 0/1
Batch Loss = 1.1096189269736119
446, epoch_train_loss=1.1096189269736119
Epoch 447
Epoch 447 :: Batch 0/1
Batch Loss = 1.1086200002033002
447, epoch_train_loss=1.1086200002033002
Epoch 448
Epoch 448 :: Batch 0/1
Batch Loss = 1.1077696115335771
448, epoch_train_loss=1.1077696115335771
Epoch 449
Epoch 449 :: Batch 0/1
Batch Loss = 1.1081916798994584
449, epoch_train_loss=1.1081916798994584
Epoch 450
Epoch 450 :: Batch 0/1
Batch Loss = 1.1063763833373792
450, epoch_train_loss=1.1063763833373792
Epoch 451
Epoch 451 :: Batch 0/1
Batch Loss = 1.1054884934013016
451, epoch_train_loss=1.1054884934013016
Epoch 452
Epoch 452 :: Batch 0/1
Batch Loss = 1.1051861275583572
452, epoch_train_loss=1.1051861275583572
Epoch 453
Epoch 453 :: Batch 0/1
Batch Loss = 1.1045010026242756
453, epoch_train_loss=1.1045010026242756
Epoch 454
Epoch 454 :: Batch 0/1
Batch Loss = 1.1034061899587053
454, epoch_train_loss=1.1034061899587053
Epoch 455
Epoch 455 :: Batch 0/1
Batch Loss = 1.1020093451208761
455, epoch_train_loss=1.1020093451208761
Epoch 456
Epoch 456 :: Batch 0/1
Batch Loss = 1.1006760314949067
456, epoch_train_loss=1.1006760314949067
Epoch 457
Epoch 457 :: Batch 0/1
Batch Loss = 1.1005307967675453
457, epoch_train_loss=1.1005307967675453
Epoch 458
Epoch 458 :: Batch 0/1
Batch Loss = 1.0988722781806564
458, epoch_train_loss=1.0988722781806564
Epoch 459
Epoch 459 :: Batch 0/1
Batch Loss = 1.0983620469087179
459, epoch_train_loss=1.0983620469087179
Epoch 460
Epoch 460 :: Batch 0/1
Batch Loss = 1.0976777841799792
460, epoch_train_loss=1.0976777841799792
Epoch 461
Epoch 461 :: Batch 0/1
Batch Loss = 1.0967278399456313
461, epoch_train_loss=1.0967278399456313
Epoch 462
Epoch 462 :: Batch 0/1
Batch Loss = 1.0956001914154916
462, epoch_train_loss=1.0956001914154916
Epoch 463
Epoch 463 :: Batch 0/1
Batch Loss = 1.0945296161470215
463, epoch_train_loss=1.0945296161470215
Epoch 464
Epoch 464 :: Batch 0/1
Batch Loss = 1.093804190366974
464, epoch_train_loss=1.093804190366974
Epoch 465
Epoch 465 :: Batch 0/1
Batch Loss = 1.0927176059538084
465, epoch_train_loss=1.0927176059538084
Epoch 466
Epoch 466 :: Batch 0/1
Batch Loss = 1.0920233186892734
466, epoch_train_loss=1.0920233186892734
Epoch 467
Epoch 467 :: Batch 0/1
Batch Loss = 1.0910804096113433
467, epoch_train_loss=1.0910804096113433
Epoch 468
Epoch 468 :: Batch 0/1
Batch Loss = 1.0901402699079925
468, epoch_train_loss=1.0901402699079925
Epoch 469
Epoch 469 :: Batch 0/1
Batch Loss = 1.0893758573021457
469, epoch_train_loss=1.0893758573021457
Epoch 470
Epoch 470 :: Batch 0/1
Batch Loss = 1.088589139292386
470, epoch_train_loss=1.088589139292386
Epoch 471
Epoch 471 :: Batch 0/1
Batch Loss = 1.0877120150186452
471, epoch_train_loss=1.0877120150186452
Epoch 472
Epoch 472 :: Batch 0/1
Batch Loss = 1.086702613637432
472, epoch_train_loss=1.086702613637432
Epoch 473
Epoch 473 :: Batch 0/1
Batch Loss = 1.0855633943815257
473, epoch_train_loss=1.0855633943815257
Epoch 474
Epoch 474 :: Batch 0/1
Batch Loss = 1.0848756451135393
474, epoch_train_loss=1.0848756451135393
Epoch 475
Epoch 475 :: Batch 0/1
Batch Loss = 1.0837867306784714
475, epoch_train_loss=1.0837867306784714
Epoch 476
Epoch 476 :: Batch 0/1
Batch Loss = 1.083139139081162
476, epoch_train_loss=1.083139139081162
Epoch 477
Epoch 477 :: Batch 0/1
Batch Loss = 1.0824288074983717
477, epoch_train_loss=1.0824288074983717
Epoch 478
Epoch 478 :: Batch 0/1
Batch Loss = 1.0815722315272636
478, epoch_train_loss=1.0815722315272636
Epoch 479
Epoch 479 :: Batch 0/1
Batch Loss = 1.0805343315318443
479, epoch_train_loss=1.0805343315318443
Epoch 480
Epoch 480 :: Batch 0/1
Batch Loss = 1.0793301794081018
480, epoch_train_loss=1.0793301794081018
Epoch 481
Epoch 481 :: Batch 0/1
Batch Loss = 1.0788267772031568
481, epoch_train_loss=1.0788267772031568
Epoch 482
Epoch 482 :: Batch 0/1
Batch Loss = 1.0775791934878387
482, epoch_train_loss=1.0775791934878387
Epoch 483
Epoch 483 :: Batch 0/1
Batch Loss = 1.0769531526885092
483, epoch_train_loss=1.0769531526885092
Epoch 484
Epoch 484 :: Batch 0/1
Batch Loss = 1.0762083251154047
484, epoch_train_loss=1.0762083251154047
Epoch 485
Epoch 485 :: Batch 0/1
Batch Loss = 1.075287503573609
485, epoch_train_loss=1.075287503573609
Epoch 486
Epoch 486 :: Batch 0/1
Batch Loss = 1.0742446675803088
486, epoch_train_loss=1.0742446675803088
Epoch 487
Epoch 487 :: Batch 0/1
Batch Loss = 1.0731036459999947
487, epoch_train_loss=1.0731036459999947
Epoch 488
Epoch 488 :: Batch 0/1
Batch Loss = 1.0724148621416696
488, epoch_train_loss=1.0724148621416696
Epoch 489
Epoch 489 :: Batch 0/1
Batch Loss = 1.0713337656481465
489, epoch_train_loss=1.0713337656481465
Epoch 490
Epoch 490 :: Batch 0/1
Batch Loss = 1.070684214185384
490, epoch_train_loss=1.070684214185384
Epoch 491
Epoch 491 :: Batch 0/1
Batch Loss = 1.0699768819721716
491, epoch_train_loss=1.0699768819721716
Epoch 492
Epoch 492 :: Batch 0/1
Batch Loss = 1.0691239702334712
492, epoch_train_loss=1.0691239702334712
Epoch 493
Epoch 493 :: Batch 0/1
Batch Loss = 1.0680913384032802
493, epoch_train_loss=1.0680913384032802
Epoch 494
Epoch 494 :: Batch 0/1
Batch Loss = 1.066871661455661
494, epoch_train_loss=1.066871661455661
Epoch 495
Epoch 495 :: Batch 0/1
Batch Loss = 1.066517121357838
495, epoch_train_loss=1.066517121357838
Epoch 496
Epoch 496 :: Batch 0/1
Batch Loss = 1.0651450568824137
496, epoch_train_loss=1.0651450568824137
Epoch 497
Epoch 497 :: Batch 0/1
Batch Loss = 1.064490563247323
497, epoch_train_loss=1.064490563247323
Epoch 498
Epoch 498 :: Batch 0/1
Batch Loss = 1.063814036575445
498, epoch_train_loss=1.063814036575445
Epoch 499
Epoch 499 :: Batch 0/1
Batch Loss = 1.0629266482448694
499, epoch_train_loss=1.0629266482448694
Epoch 500
Epoch 500 :: Batch 0/1
Batch Loss = 1.0618359065422591
500, epoch_train_loss=1.0618359065422591
Epoch 501
Epoch 501 :: Batch 0/1
Batch Loss = 1.0605733238919408
501, epoch_train_loss=1.0605733238919408
Epoch 502
Epoch 502 :: Batch 0/1
Batch Loss = 1.0603327259043862
502, epoch_train_loss=1.0603327259043862
Epoch 503
Epoch 503 :: Batch 0/1
Batch Loss = 1.0588565682075193
503, epoch_train_loss=1.0588565682075193
Epoch 504
Epoch 504 :: Batch 0/1
Batch Loss = 1.0581604483722802
504, epoch_train_loss=1.0581604483722802
Epoch 505
Epoch 505 :: Batch 0/1
Batch Loss = 1.0575136141832517
505, epoch_train_loss=1.0575136141832517
Epoch 506
Epoch 506 :: Batch 0/1
Batch Loss = 1.0566410583022754
506, epoch_train_loss=1.0566410583022754
Epoch 507
Epoch 507 :: Batch 0/1
Batch Loss = 1.0555192495592733
507, epoch_train_loss=1.0555192495592733
Epoch 508
Epoch 508 :: Batch 0/1
Batch Loss = 1.0543633034468574
508, epoch_train_loss=1.0543633034468574
Epoch 509
Epoch 509 :: Batch 0/1
Batch Loss = 1.0547340758547505
509, epoch_train_loss=1.0547340758547505
Epoch 510
Epoch 510 :: Batch 0/1
Batch Loss = 1.0536825278774538
510, epoch_train_loss=1.0536825278774538
Epoch 511
Epoch 511 :: Batch 0/1
Batch Loss = 1.0515789926278638
511, epoch_train_loss=1.0515789926278638
Epoch 512
Epoch 512 :: Batch 0/1
Batch Loss = 1.0513070528374646
512, epoch_train_loss=1.0513070528374646
Epoch 513
Epoch 513 :: Batch 0/1
Batch Loss = 1.0509063064738577
513, epoch_train_loss=1.0509063064738577
Epoch 514
Epoch 514 :: Batch 0/1
Batch Loss = 1.0500247111949588
514, epoch_train_loss=1.0500247111949588
Epoch 515
Epoch 515 :: Batch 0/1
Batch Loss = 1.0486813155465782
515, epoch_train_loss=1.0486813155465782
Epoch 516
Epoch 516 :: Batch 0/1
Batch Loss = 1.0470122371785897
516, epoch_train_loss=1.0470122371785897
Epoch 517
Epoch 517 :: Batch 0/1
Batch Loss = 1.045489021245843
517, epoch_train_loss=1.045489021245843
Epoch 518
Epoch 518 :: Batch 0/1
Batch Loss = 1.106030393346716
518, epoch_train_loss=1.106030393346716
Epoch 519
Epoch 519 :: Batch 0/1
Batch Loss = 1.057959373934051
519, epoch_train_loss=1.057959373934051
Epoch 520
Epoch 520 :: Batch 0/1
Batch Loss = 1.0734729389551234
520, epoch_train_loss=1.0734729389551234
Epoch 521
Epoch 521 :: Batch 0/1
Batch Loss = 1.0756419360614358
521, epoch_train_loss=1.0756419360614358
Epoch 522
Epoch 522 :: Batch 0/1
Batch Loss = 1364.2115679063697
522, epoch_train_loss=1364.2115679063697
Epoch 523
Epoch 523 :: Batch 0/1
Batch Loss = 663.5371324343167
523, epoch_train_loss=663.5371324343167
Epoch 524
Epoch 524 :: Batch 0/1
Batch Loss = 941.70275499499
524, epoch_train_loss=941.70275499499
Epoch 525
Epoch 525 :: Batch 0/1
Batch Loss = 179.0029099181776
525, epoch_train_loss=179.0029099181776
Epoch 526
Epoch 526 :: Batch 0/1
Batch Loss = 137.46155488574524
526, epoch_train_loss=137.46155488574524
Epoch 527
Epoch 527 :: Batch 0/1
Batch Loss = 673.1114585927137
527, epoch_train_loss=673.1114585927137
Epoch 528
Epoch 528 :: Batch 0/1
Batch Loss = 419.78831609728576
528, epoch_train_loss=419.78831609728576
Epoch 529
Epoch 529 :: Batch 0/1
Batch Loss = 1.2998071410720125
529, epoch_train_loss=1.2998071410720125
Epoch 530
Epoch 530 :: Batch 0/1
Batch Loss = 16574.177546657054
530, epoch_train_loss=16574.177546657054
Epoch 531
Epoch 531 :: Batch 0/1
Batch Loss = 565.4180511502178
531, epoch_train_loss=565.4180511502178
Epoch 532
Epoch 532 :: Batch 0/1
Batch Loss = 33.154997702074795
532, epoch_train_loss=33.154997702074795
Epoch 533
Epoch 533 :: Batch 0/1
Batch Loss = 390.5262767017463
533, epoch_train_loss=390.5262767017463
Epoch 534
Epoch 534 :: Batch 0/1
Batch Loss = 773.7638488563623
534, epoch_train_loss=773.7638488563623
Epoch 535
Epoch 535 :: Batch 0/1
Batch Loss = 175.46826619740267
535, epoch_train_loss=175.46826619740267
Epoch 536
Epoch 536 :: Batch 0/1
Batch Loss = 123.5109717708227
536, epoch_train_loss=123.5109717708227
Epoch 537
Epoch 537 :: Batch 0/1
Batch Loss = 584.462870687039
537, epoch_train_loss=584.462870687039
Epoch 538
Epoch 538 :: Batch 0/1
Batch Loss = 254.37267603149343
538, epoch_train_loss=254.37267603149343
Epoch 539
Epoch 539 :: Batch 0/1
Batch Loss = 29.43157750155803
539, epoch_train_loss=29.43157750155803
Epoch 540
Epoch 540 :: Batch 0/1
Batch Loss = 398.06337097305044
540, epoch_train_loss=398.06337097305044
Epoch 541
Epoch 541 :: Batch 0/1
Batch Loss = 254.54049242966656
541, epoch_train_loss=254.54049242966656
Epoch 542
Epoch 542 :: Batch 0/1
Batch Loss = 5.809940750878399
542, epoch_train_loss=5.809940750878399
Epoch 543
Epoch 543 :: Batch 0/1
Batch Loss = 269.12581610662113
543, epoch_train_loss=269.12581610662113
Epoch 544
Epoch 544 :: Batch 0/1
Batch Loss = 219.62880825886367
544, epoch_train_loss=219.62880825886367
Epoch 545
Epoch 545 :: Batch 0/1
Batch Loss = 2.768129616492254
545, epoch_train_loss=2.768129616492254
Epoch 546
Epoch 546 :: Batch 0/1
Batch Loss = 184.92366644773506
546, epoch_train_loss=184.92366644773506
Epoch 547
Epoch 547 :: Batch 0/1
Batch Loss = 175.05029457142146
547, epoch_train_loss=175.05029457142146
Epoch 548
Epoch 548 :: Batch 0/1
Batch Loss = 3.8390382879414573
548, epoch_train_loss=3.8390382879414573
Epoch 549
Epoch 549 :: Batch 0/1
Batch Loss = 134.69059525927764
549, epoch_train_loss=134.69059525927764
Epoch 550
Epoch 550 :: Batch 0/1
Batch Loss = 139.25964686706843
550, epoch_train_loss=139.25964686706843
Epoch 551
Epoch 551 :: Batch 0/1
Batch Loss = 6.492802163454382
551, epoch_train_loss=6.492802163454382
Epoch 552
Epoch 552 :: Batch 0/1
Batch Loss = 96.36365090674325
552, epoch_train_loss=96.36365090674325
Epoch 553
Epoch 553 :: Batch 0/1
Batch Loss = 98.80169114962416
553, epoch_train_loss=98.80169114962416
Epoch 554
Epoch 554 :: Batch 0/1
Batch Loss = 2.65111285865953
554, epoch_train_loss=2.65111285865953
Epoch 555
Epoch 555 :: Batch 0/1
Batch Loss = 71.55408737496074
555, epoch_train_loss=71.55408737496074
Epoch 556
Epoch 556 :: Batch 0/1
Batch Loss = 71.34635788162018
556, epoch_train_loss=71.34635788162018
Epoch 557
Epoch 557 :: Batch 0/1
Batch Loss = 1.8810385256996076
557, epoch_train_loss=1.8810385256996076
Epoch 558
Epoch 558 :: Batch 0/1
Batch Loss = 54.44277664915118
558, epoch_train_loss=54.44277664915118
Epoch 559
Epoch 559 :: Batch 0/1
Batch Loss = 50.5882554188302
559, epoch_train_loss=50.5882554188302
Epoch 560
Epoch 560 :: Batch 0/1
Batch Loss = 1.483071395994258
560, epoch_train_loss=1.483071395994258
Epoch 561
Epoch 561 :: Batch 0/1
Batch Loss = 42.003261156483276
561, epoch_train_loss=42.003261156483276
Epoch 562
Epoch 562 :: Batch 0/1
Batch Loss = 35.4694253990155
562, epoch_train_loss=35.4694253990155
Epoch 563
Epoch 563 :: Batch 0/1
Batch Loss = 104.74155051676969
563, epoch_train_loss=104.74155051676969
Epoch 564
Epoch 564 :: Batch 0/1
Batch Loss = 32.79895275393549
564, epoch_train_loss=32.79895275393549
Epoch 565
Epoch 565 :: Batch 0/1
Batch Loss = 25.82816905917964
565, epoch_train_loss=25.82816905917964
Epoch 566
Epoch 566 :: Batch 0/1
Batch Loss = 2.057853491067137
566, epoch_train_loss=2.057853491067137
Epoch 567
Epoch 567 :: Batch 0/1
Batch Loss = 25.60097853782198
567, epoch_train_loss=25.60097853782198
Epoch 568
Epoch 568 :: Batch 0/1
Batch Loss = 16.48837356072222
568, epoch_train_loss=16.48837356072222
Epoch 569
Epoch 569 :: Batch 0/1
Batch Loss = 2.3771601950453953
569, epoch_train_loss=2.3771601950453953
Epoch 570
Epoch 570 :: Batch 0/1
Batch Loss = 20.346349930497826
570, epoch_train_loss=20.346349930497826
Epoch 571
Epoch 571 :: Batch 0/1
Batch Loss = 11.116842416415732
571, epoch_train_loss=11.116842416415732
Epoch 572
Epoch 572 :: Batch 0/1
Batch Loss = 2.462307623339425
572, epoch_train_loss=2.462307623339425
Epoch 573
Epoch 573 :: Batch 0/1
Batch Loss = 15.560186124214082
573, epoch_train_loss=15.560186124214082
Epoch 574
Epoch 574 :: Batch 0/1
Batch Loss = 7.1576376065366505
574, epoch_train_loss=7.1576376065366505
Epoch 575
Epoch 575 :: Batch 0/1
Batch Loss = 2.6863545491167558
575, epoch_train_loss=2.6863545491167558
Epoch 576
Epoch 576 :: Batch 0/1
Batch Loss = 12.03013541958767
576, epoch_train_loss=12.03013541958767
Epoch 577
Epoch 577 :: Batch 0/1
Batch Loss = 4.675364794782145
577, epoch_train_loss=4.675364794782145
Epoch 578
Epoch 578 :: Batch 0/1
Batch Loss = 2.8850864594951546
578, epoch_train_loss=2.8850864594951546
Epoch 579
Epoch 579 :: Batch 0/1
Batch Loss = 9.259573472851935
579, epoch_train_loss=9.259573472851935
Epoch 580
Epoch 580 :: Batch 0/1
Batch Loss = 3.094063362268844
580, epoch_train_loss=3.094063362268844
Epoch 581
Epoch 581 :: Batch 0/1
Batch Loss = 2.9833511384151183
581, epoch_train_loss=2.9833511384151183
Epoch 582
Epoch 582 :: Batch 0/1
Batch Loss = 7.085894096389189
582, epoch_train_loss=7.085894096389189
Epoch 583
Epoch 583 :: Batch 0/1
Batch Loss = 2.1373813757436073
583, epoch_train_loss=2.1373813757436073
Epoch 584
Epoch 584 :: Batch 0/1
Batch Loss = 2.979394820858988
584, epoch_train_loss=2.979394820858988
Epoch 585
Epoch 585 :: Batch 0/1
Batch Loss = 5.402385383872196
585, epoch_train_loss=5.402385383872196
Epoch 586
Epoch 586 :: Batch 0/1
Batch Loss = 1.5975942383073718
586, epoch_train_loss=1.5975942383073718
Epoch 587
Epoch 587 :: Batch 0/1
Batch Loss = 2.891451129328561
587, epoch_train_loss=2.891451129328561
Epoch 588
Epoch 588 :: Batch 0/1
Batch Loss = 4.117997925067689
588, epoch_train_loss=4.117997925067689
Epoch 589
Epoch 589 :: Batch 0/1
Batch Loss = 1.3282421607765496
589, epoch_train_loss=1.3282421607765496
Epoch 590
Epoch 590 :: Batch 0/1
Batch Loss = 2.7379770880917853
590, epoch_train_loss=2.7379770880917853
Epoch 591
Epoch 591 :: Batch 0/1
Batch Loss = 3.158409631183786
591, epoch_train_loss=3.158409631183786
Epoch 592
Epoch 592 :: Batch 0/1
Batch Loss = 1.2227089031920808
592, epoch_train_loss=1.2227089031920808
Epoch 593
Epoch 593 :: Batch 0/1
Batch Loss = 2.543183045436252
593, epoch_train_loss=2.543183045436252
Epoch 594
Epoch 594 :: Batch 0/1
Batch Loss = 2.4599121463788287
594, epoch_train_loss=2.4599121463788287
Epoch 595
Epoch 595 :: Batch 0/1
Batch Loss = 1.208739447975982
595, epoch_train_loss=1.208739447975982
Epoch 596
Epoch 596 :: Batch 0/1
Batch Loss = 2.328130684515825
596, epoch_train_loss=2.328130684515825
Epoch 597
Epoch 597 :: Batch 0/1
Batch Loss = 1.9676609918777368
597, epoch_train_loss=1.9676609918777368
Epoch 598
Epoch 598 :: Batch 0/1
Batch Loss = 1.23771498078485
598, epoch_train_loss=1.23771498078485
Epoch 599
Epoch 599 :: Batch 0/1
Batch Loss = 2.111382308042155
599, epoch_train_loss=2.111382308042155
Epoch 600
Epoch 600 :: Batch 0/1
Batch Loss = 1.6347188136433868
600, epoch_train_loss=1.6347188136433868
Epoch 601
Epoch 601 :: Batch 0/1
Batch Loss = 1.278921903465698
601, epoch_train_loss=1.278921903465698
Epoch 602
Epoch 602 :: Batch 0/1
Batch Loss = 1.9069209664384685
602, epoch_train_loss=1.9069209664384685
Epoch 603
Epoch 603 :: Batch 0/1
Batch Loss = 1.4208262298938734
603, epoch_train_loss=1.4208262298938734
Epoch 604
Epoch 604 :: Batch 0/1
Batch Loss = 1.3149171364156047
604, epoch_train_loss=1.3149171364156047
Epoch 605
Epoch 605 :: Batch 0/1
Batch Loss = 1.7244196453886766
605, epoch_train_loss=1.7244196453886766
Epoch 606
Epoch 606 :: Batch 0/1
Batch Loss = 1.2928619289300296
606, epoch_train_loss=1.2928619289300296
Epoch 607
Epoch 607 :: Batch 0/1
Batch Loss = 1.3372531362541622
607, epoch_train_loss=1.3372531362541622
Epoch 608
Epoch 608 :: Batch 0/1
Batch Loss = 1.5695648683419863
608, epoch_train_loss=1.5695648683419863
Epoch 609
Epoch 609 :: Batch 0/1
Batch Loss = 1.2238334876120547
609, epoch_train_loss=1.2238334876120547
Epoch 610
Epoch 610 :: Batch 0/1
Batch Loss = 1.343910247244722
610, epoch_train_loss=1.343910247244722
Epoch 611
Epoch 611 :: Batch 0/1
Batch Loss = 1.4444697360324248
611, epoch_train_loss=1.4444697360324248
Epoch 612
Epoch 612 :: Batch 0/1
Batch Loss = 1.1928079318955536
612, epoch_train_loss=1.1928079318955536
Epoch 613
Epoch 613 :: Batch 0/1
Batch Loss = 1.336473492205424
613, epoch_train_loss=1.336473492205424
Epoch 614
Epoch 614 :: Batch 0/1
Batch Loss = 1.3484414471642565
614, epoch_train_loss=1.3484414471642565
Epoch 615
Epoch 615 :: Batch 0/1
Batch Loss = 1.1840025247334927
615, epoch_train_loss=1.1840025247334927
Epoch 616
Epoch 616 :: Batch 0/1
Batch Loss = 1.3185493225006217
616, epoch_train_loss=1.3185493225006217
Epoch 617
Epoch 617 :: Batch 0/1
Batch Loss = 1.2786991420091782
617, epoch_train_loss=1.2786991420091782
Epoch 618
Epoch 618 :: Batch 0/1
Batch Loss = 1.1862313949025067
618, epoch_train_loss=1.1862313949025067
Epoch 619
Epoch 619 :: Batch 0/1
Batch Loss = 1.2943167512861633
619, epoch_train_loss=1.2943167512861633
Epoch 620
Epoch 620 :: Batch 0/1
Batch Loss = 1.2311607600714543
620, epoch_train_loss=1.2311607600714543
Epoch 621
Epoch 621 :: Batch 0/1
Batch Loss = 1.1921111287652066
621, epoch_train_loss=1.1921111287652066
Epoch 622
Epoch 622 :: Batch 0/1
Batch Loss = 1.2677221800364957
622, epoch_train_loss=1.2677221800364957
Epoch 623
Epoch 623 :: Batch 0/1
Batch Loss = 1.2011755738635808
623, epoch_train_loss=1.2011755738635808
Epoch 624
Epoch 624 :: Batch 0/1
Batch Loss = 1.1972880814738318
624, epoch_train_loss=1.1972880814738318
Epoch 625
Epoch 625 :: Batch 0/1
Batch Loss = 1.2419464391301176
625, epoch_train_loss=1.2419464391301176
Epoch 626
Epoch 626 :: Batch 0/1
Batch Loss = 1.1840806178998127
626, epoch_train_loss=1.1840806178998127
Epoch 627
Epoch 627 :: Batch 0/1
Batch Loss = 1.1996973530133166
627, epoch_train_loss=1.1996973530133166
Epoch 628
Epoch 628 :: Batch 0/1
Batch Loss = 1.2191769267697565
628, epoch_train_loss=1.2191769267697565
Epoch 629
Epoch 629 :: Batch 0/1
Batch Loss = 1.175664993273245
629, epoch_train_loss=1.175664993273245
Epoch 630
Epoch 630 :: Batch 0/1
Batch Loss = 1.198836416653747
630, epoch_train_loss=1.198836416653747
Epoch 631
Epoch 631 :: Batch 0/1
Batch Loss = 1.2005998997088416
631, epoch_train_loss=1.2005998997088416
Epoch 632
Epoch 632 :: Batch 0/1
Batch Loss = 1.172429450089674
632, epoch_train_loss=1.172429450089674
Epoch 633
Epoch 633 :: Batch 0/1
Batch Loss = 1.19516906169496
633, epoch_train_loss=1.19516906169496
Epoch 634
Epoch 634 :: Batch 0/1
Batch Loss = 1.1865306223100853
634, epoch_train_loss=1.1865306223100853
Epoch 635
Epoch 635 :: Batch 0/1
Batch Loss = 1.1717247813013763
635, epoch_train_loss=1.1717247813013763
Epoch 636
Epoch 636 :: Batch 0/1
Batch Loss = 1.1896190479061572
636, epoch_train_loss=1.1896190479061572
Epoch 637
Epoch 637 :: Batch 0/1
Batch Loss = 1.176630481058131
637, epoch_train_loss=1.176630481058131
Epoch 638
Epoch 638 :: Batch 0/1
Batch Loss = 1.1717479572439193
638, epoch_train_loss=1.1717479572439193
Epoch 639
Epoch 639 :: Batch 0/1
Batch Loss = 1.1832070844571454
639, epoch_train_loss=1.1832070844571454
Epoch 640
Epoch 640 :: Batch 0/1
Batch Loss = 1.170153687199472
640, epoch_train_loss=1.170153687199472
Epoch 641
Epoch 641 :: Batch 0/1
Batch Loss = 1.1714458169252358
641, epoch_train_loss=1.1714458169252358
Epoch 642
Epoch 642 :: Batch 0/1
Batch Loss = 1.176810936545193
642, epoch_train_loss=1.176810936545193
Epoch 643
Epoch 643 :: Batch 0/1
Batch Loss = 1.1661766728427319
643, epoch_train_loss=1.1661766728427319
Epoch 644
Epoch 644 :: Batch 0/1
Batch Loss = 1.170360908880463
644, epoch_train_loss=1.170360908880463
Epoch 645
Epoch 645 :: Batch 0/1
Batch Loss = 1.1710426661908222
645, epoch_train_loss=1.1710426661908222
Epoch 646
Epoch 646 :: Batch 0/1
Batch Loss = 1.1637886047524086
646, epoch_train_loss=1.1637886047524086
Epoch 647
Epoch 647 :: Batch 0/1
Batch Loss = 1.168449923497897
647, epoch_train_loss=1.168449923497897
Epoch 648
Epoch 648 :: Batch 0/1
Batch Loss = 1.1662200174739605
648, epoch_train_loss=1.1662200174739605
Epoch 649
Epoch 649 :: Batch 0/1
Batch Loss = 1.1622197666037406
649, epoch_train_loss=1.1622197666037406
Epoch 650
Epoch 650 :: Batch 0/1
Batch Loss = 1.1659106634795833
650, epoch_train_loss=1.1659106634795833
Epoch 651
Epoch 651 :: Batch 0/1
Batch Loss = 1.162402967853665
651, epoch_train_loss=1.162402967853665
Epoch 652
Epoch 652 :: Batch 0/1
Batch Loss = 1.1609092835809829
652, epoch_train_loss=1.1609092835809829
Epoch 653
Epoch 653 :: Batch 0/1
Batch Loss = 1.1630351258656968
653, epoch_train_loss=1.1630351258656968
Epoch 654
Epoch 654 :: Batch 0/1
Batch Loss = 1.159467074676867
654, epoch_train_loss=1.159467074676867
Epoch 655
Epoch 655 :: Batch 0/1
Batch Loss = 1.159517090702495
655, epoch_train_loss=1.159517090702495
Epoch 656
Epoch 656 :: Batch 0/1
Batch Loss = 1.160103702382704
656, epoch_train_loss=1.160103702382704
Epoch 657
Epoch 657 :: Batch 0/1
Batch Loss = 1.1571882219937997
657, epoch_train_loss=1.1571882219937997
Epoch 658
Epoch 658 :: Batch 0/1
Batch Loss = 1.1578947949579608
658, epoch_train_loss=1.1578947949579608
Epoch 659
Epoch 659 :: Batch 0/1
Batch Loss = 1.1573230764621836
659, epoch_train_loss=1.1573230764621836
Epoch 660
Epoch 660 :: Batch 0/1
Batch Loss = 1.1553183323533154
660, epoch_train_loss=1.1553183323533154
Epoch 661
Epoch 661 :: Batch 0/1
Batch Loss = 1.1560332140912675
661, epoch_train_loss=1.1560332140912675
Epoch 662
Epoch 662 :: Batch 0/1
Batch Loss = 1.1548036422006704
662, epoch_train_loss=1.1548036422006704
Epoch 663
Epoch 663 :: Batch 0/1
Batch Loss = 1.1536409297508792
663, epoch_train_loss=1.1536409297508792
Epoch 664
Epoch 664 :: Batch 0/1
Batch Loss = 1.154003204195935
664, epoch_train_loss=1.154003204195935
Epoch 665
Epoch 665 :: Batch 0/1
Batch Loss = 1.1525675000618312
665, epoch_train_loss=1.1525675000618312
Epoch 666
Epoch 666 :: Batch 0/1
Batch Loss = 1.1520021327207377
666, epoch_train_loss=1.1520021327207377
Epoch 667
Epoch 667 :: Batch 0/1
Batch Loss = 1.1519035518177054
667, epoch_train_loss=1.1519035518177054
Epoch 668
Epoch 668 :: Batch 0/1
Batch Loss = 1.1505736870567018
668, epoch_train_loss=1.1505736870567018
Epoch 669
Epoch 669 :: Batch 0/1
Batch Loss = 1.1503165903876016
669, epoch_train_loss=1.1503165903876016
Epoch 670
Epoch 670 :: Batch 0/1
Batch Loss = 1.149821185281249
670, epoch_train_loss=1.149821185281249
Epoch 671
Epoch 671 :: Batch 0/1
Batch Loss = 1.1487479247343655
671, epoch_train_loss=1.1487479247343655
Epoch 672
Epoch 672 :: Batch 0/1
Batch Loss = 1.1485571500175014
672, epoch_train_loss=1.1485571500175014
Epoch 673
Epoch 673 :: Batch 0/1
Batch Loss = 1.1478122444211172
673, epoch_train_loss=1.1478122444211172
Epoch 674
Epoch 674 :: Batch 0/1
Batch Loss = 1.1470128829872301
674, epoch_train_loss=1.1470128829872301
Epoch 675
Epoch 675 :: Batch 0/1
Batch Loss = 1.1467370034479456
675, epoch_train_loss=1.1467370034479456
Epoch 676
Epoch 676 :: Batch 0/1
Batch Loss = 1.14589806892906
676, epoch_train_loss=1.14589806892906
Epoch 677
Epoch 677 :: Batch 0/1
Batch Loss = 1.1453069031348695
677, epoch_train_loss=1.1453069031348695
Epoch 678
Epoch 678 :: Batch 0/1
Batch Loss = 1.144887604952968
678, epoch_train_loss=1.144887604952968
Epoch 679
Epoch 679 :: Batch 0/1
Batch Loss = 1.1440711443651268
679, epoch_train_loss=1.1440711443651268
Epoch 680
Epoch 680 :: Batch 0/1
Batch Loss = 1.1435920753961428
680, epoch_train_loss=1.1435920753961428
Epoch 681
Epoch 681 :: Batch 0/1
Batch Loss = 1.1430408368998257
681, epoch_train_loss=1.1430408368998257
Epoch 682
Epoch 682 :: Batch 0/1
Batch Loss = 1.1423068952269402
682, epoch_train_loss=1.1423068952269402
Epoch 683
Epoch 683 :: Batch 0/1
Batch Loss = 1.1418535053435812
683, epoch_train_loss=1.1418535053435812
Epoch 684
Epoch 684 :: Batch 0/1
Batch Loss = 1.1412183306123191
684, epoch_train_loss=1.1412183306123191
Epoch 685
Epoch 685 :: Batch 0/1
Batch Loss = 1.1405761174761857
685, epoch_train_loss=1.1405761174761857
Epoch 686
Epoch 686 :: Batch 0/1
Batch Loss = 1.1400929911520683
686, epoch_train_loss=1.1400929911520683
Epoch 687
Epoch 687 :: Batch 0/1
Batch Loss = 1.1394280762779816
687, epoch_train_loss=1.1394280762779816
Epoch 688
Epoch 688 :: Batch 0/1
Batch Loss = 1.138854267543668
688, epoch_train_loss=1.138854267543668
Epoch 689
Epoch 689 :: Batch 0/1
Batch Loss = 1.1383206393187375
689, epoch_train_loss=1.1383206393187375
Epoch 690
Epoch 690 :: Batch 0/1
Batch Loss = 1.1376665161804036
690, epoch_train_loss=1.1376665161804036
Epoch 691
Epoch 691 :: Batch 0/1
Batch Loss = 1.137126084124709
691, epoch_train_loss=1.137126084124709
Epoch 692
Epoch 692 :: Batch 0/1
Batch Loss = 1.1365474782162595
692, epoch_train_loss=1.1365474782162595
Epoch 693
Epoch 693 :: Batch 0/1
Batch Loss = 1.135923458062527
693, epoch_train_loss=1.135923458062527
Epoch 694
Epoch 694 :: Batch 0/1
Batch Loss = 1.1353857483657712
694, epoch_train_loss=1.1353857483657712
Epoch 695
Epoch 695 :: Batch 0/1
Batch Loss = 1.1347807491833133
695, epoch_train_loss=1.1347807491833133
Epoch 696
Epoch 696 :: Batch 0/1
Batch Loss = 1.1341873526333552
696, epoch_train_loss=1.1341873526333552
Epoch 697
Epoch 697 :: Batch 0/1
Batch Loss = 1.1336344786691508
697, epoch_train_loss=1.1336344786691508
Epoch 698
Epoch 698 :: Batch 0/1
Batch Loss = 1.1330229907357685
698, epoch_train_loss=1.1330229907357685
Epoch 699
Epoch 699 :: Batch 0/1
Batch Loss = 1.1324499672346842
699, epoch_train_loss=1.1324499672346842
Epoch 700
Epoch 700 :: Batch 0/1
Batch Loss = 1.1318776702714017
700, epoch_train_loss=1.1318776702714017
Epoch 701
Epoch 701 :: Batch 0/1
Batch Loss = 1.1312731685870179
701, epoch_train_loss=1.1312731685870179
Epoch 702
Epoch 702 :: Batch 0/1
Batch Loss = 1.13070701875786
702, epoch_train_loss=1.13070701875786
Epoch 703
Epoch 703 :: Batch 0/1
Batch Loss = 1.1301194530211014
703, epoch_train_loss=1.1301194530211014
Epoch 704
Epoch 704 :: Batch 0/1
Batch Loss = 1.1295262107149697
704, epoch_train_loss=1.1295262107149697
Epoch 705
Epoch 705 :: Batch 0/1
Batch Loss = 1.128956477014005
705, epoch_train_loss=1.128956477014005
Epoch 706
Epoch 706 :: Batch 0/1
Batch Loss = 1.1283617466152527
706, epoch_train_loss=1.1283617466152527
Epoch 707
Epoch 707 :: Batch 0/1
Batch Loss = 1.1277781038236865
707, epoch_train_loss=1.1277781038236865
Epoch 708
Epoch 708 :: Batch 0/1
Batch Loss = 1.127200737612871
708, epoch_train_loss=1.127200737612871
Epoch 709
Epoch 709 :: Batch 0/1
Batch Loss = 1.1266065574110788
709, epoch_train_loss=1.1266065574110788
Epoch 710
Epoch 710 :: Batch 0/1
Batch Loss = 1.1260282326966997
710, epoch_train_loss=1.1260282326966997
Epoch 711
Epoch 711 :: Batch 0/1
Batch Loss = 1.12544443170926
711, epoch_train_loss=1.12544443170926
Epoch 712
Epoch 712 :: Batch 0/1
Batch Loss = 1.1248553247170856
712, epoch_train_loss=1.1248553247170856
Epoch 713
Epoch 713 :: Batch 0/1
Batch Loss = 1.1242783167150245
713, epoch_train_loss=1.1242783167150245
Epoch 714
Epoch 714 :: Batch 0/1
Batch Loss = 1.1236920113815403
714, epoch_train_loss=1.1236920113815403
Epoch 715
Epoch 715 :: Batch 0/1
Batch Loss = 1.1231092052985538
715, epoch_train_loss=1.1231092052985538
Epoch 716
Epoch 716 :: Batch 0/1
Batch Loss = 1.1225317207250762
716, epoch_train_loss=1.1225317207250762
Epoch 717
Epoch 717 :: Batch 0/1
Batch Loss = 1.1219472881381618
717, epoch_train_loss=1.1219472881381618
Epoch 718
Epoch 718 :: Batch 0/1
Batch Loss = 1.1213701026540255
718, epoch_train_loss=1.1213701026540255
Epoch 719
Epoch 719 :: Batch 0/1
Batch Loss = 1.1207927618993978
719, epoch_train_loss=1.1207927618993978
Epoch 720
Epoch 720 :: Batch 0/1
Batch Loss = 1.120213328079674
720, epoch_train_loss=1.120213328079674
Epoch 721
Epoch 721 :: Batch 0/1
Batch Loss = 1.1196405202770932
721, epoch_train_loss=1.1196405202770932
Epoch 722
Epoch 722 :: Batch 0/1
Batch Loss = 1.1190652339310252
722, epoch_train_loss=1.1190652339310252
Epoch 723
Epoch 723 :: Batch 0/1
Batch Loss = 1.1184920433857042
723, epoch_train_loss=1.1184920433857042
Epoch 724
Epoch 724 :: Batch 0/1
Batch Loss = 1.1179228314474279
724, epoch_train_loss=1.1179228314474279
Epoch 725
Epoch 725 :: Batch 0/1
Batch Loss = 1.1173518211847955
725, epoch_train_loss=1.1173518211847955
Epoch 726
Epoch 726 :: Batch 0/1
Batch Loss = 1.1167851591937057
726, epoch_train_loss=1.1167851591937057
Epoch 727
Epoch 727 :: Batch 0/1
Batch Loss = 1.1162202853760066
727, epoch_train_loss=1.1162202853760066
Epoch 728
Epoch 728 :: Batch 0/1
Batch Loss = 1.115656026015079
728, epoch_train_loss=1.115656026015079
Epoch 729
Epoch 729 :: Batch 0/1
Batch Loss = 1.1150965497754852
729, epoch_train_loss=1.1150965497754852
Epoch 730
Epoch 730 :: Batch 0/1
Batch Loss = 1.1145381111933343
730, epoch_train_loss=1.1145381111933343
Epoch 731
Epoch 731 :: Batch 0/1
Batch Loss = 1.1139825824164886
731, epoch_train_loss=1.1139825824164886
Epoch 732
Epoch 732 :: Batch 0/1
Batch Loss = 1.1134310475266116
732, epoch_train_loss=1.1134310475266116
Epoch 733
Epoch 733 :: Batch 0/1
Batch Loss = 1.1128809483093935
733, epoch_train_loss=1.1128809483093935
Epoch 734
Epoch 734 :: Batch 0/1
Batch Loss = 1.1123349295473715
734, epoch_train_loss=1.1123349295473715
Epoch 735
Epoch 735 :: Batch 0/1
Batch Loss = 1.1117919172696322
735, epoch_train_loss=1.1117919172696322
Epoch 736
Epoch 736 :: Batch 0/1
Batch Loss = 1.1112513139954785
736, epoch_train_loss=1.1112513139954785
Epoch 737
Epoch 737 :: Batch 0/1
Batch Loss = 1.1107148996316951
737, epoch_train_loss=1.1107148996316951
Epoch 738
Epoch 738 :: Batch 0/1
Batch Loss = 1.1101810006223887
738, epoch_train_loss=1.1101810006223887
Epoch 739
Epoch 739 :: Batch 0/1
Batch Loss = 1.1096504055287728
739, epoch_train_loss=1.1096504055287728
Epoch 740
Epoch 740 :: Batch 0/1
Batch Loss = 1.1091235299382702
740, epoch_train_loss=1.1091235299382702
Epoch 741
Epoch 741 :: Batch 0/1
Batch Loss = 1.1085992506334938
741, epoch_train_loss=1.1085992506334938
Epoch 742
Epoch 742 :: Batch 0/1
Batch Loss = 1.1080787089364914
742, epoch_train_loss=1.1080787089364914
Epoch 743
Epoch 743 :: Batch 0/1
Batch Loss = 1.1075613747936706
743, epoch_train_loss=1.1075613747936706
Epoch 744
Epoch 744 :: Batch 0/1
Batch Loss = 1.1070470258700038
744, epoch_train_loss=1.1070470258700038
Epoch 745
Epoch 745 :: Batch 0/1
Batch Loss = 1.1065363973977798
745, epoch_train_loss=1.1065363973977798
Epoch 746
Epoch 746 :: Batch 0/1
Batch Loss = 1.106028737969852
746, epoch_train_loss=1.106028737969852
Epoch 747
Epoch 747 :: Batch 0/1
Batch Loss = 1.1055244362810135
747, epoch_train_loss=1.1055244362810135
Epoch 748
Epoch 748 :: Batch 0/1
Batch Loss = 1.1050236064959185
748, epoch_train_loss=1.1050236064959185
Epoch 749
Epoch 749 :: Batch 0/1
Batch Loss = 1.1045257797251002
749, epoch_train_loss=1.1045257797251002
Epoch 750
Epoch 750 :: Batch 0/1
Batch Loss = 1.1040314445220016
750, epoch_train_loss=1.1040314445220016
Epoch 751
Epoch 751 :: Batch 0/1
Batch Loss = 1.1035402903209197
751, epoch_train_loss=1.1035402903209197
Epoch 752
Epoch 752 :: Batch 0/1
Batch Loss = 1.103052230514384
752, epoch_train_loss=1.103052230514384
Epoch 753
Epoch 753 :: Batch 0/1
Batch Loss = 1.1025674939358745
753, epoch_train_loss=1.1025674939358745
Epoch 754
Epoch 754 :: Batch 0/1
Batch Loss = 1.1020856747515249
754, epoch_train_loss=1.1020856747515249
Epoch 755
Epoch 755 :: Batch 0/1
Batch Loss = 1.1016069010603367
755, epoch_train_loss=1.1016069010603367
Epoch 756
Epoch 756 :: Batch 0/1
Batch Loss = 1.1011310904207796
756, epoch_train_loss=1.1011310904207796
Epoch 757
Epoch 757 :: Batch 0/1
Batch Loss = 1.1006579852975154
757, epoch_train_loss=1.1006579852975154
Epoch 758
Epoch 758 :: Batch 0/1
Batch Loss = 1.1001877234979338
758, epoch_train_loss=1.1001877234979338
Epoch 759
Epoch 759 :: Batch 0/1
Batch Loss = 1.0997200728642955
759, epoch_train_loss=1.0997200728642955
Epoch 760
Epoch 760 :: Batch 0/1
Batch Loss = 1.0992549780211236
760, epoch_train_loss=1.0992549780211236
Epoch 761
Epoch 761 :: Batch 0/1
Batch Loss = 1.098792466377249
761, epoch_train_loss=1.098792466377249
Epoch 762
Epoch 762 :: Batch 0/1
Batch Loss = 1.0983323256132735
762, epoch_train_loss=1.0983323256132735
Epoch 763
Epoch 763 :: Batch 0/1
Batch Loss = 1.097874599336074
763, epoch_train_loss=1.097874599336074
Epoch 764
Epoch 764 :: Batch 0/1
Batch Loss = 1.0974191903690438
764, epoch_train_loss=1.0974191903690438
Epoch 765
Epoch 765 :: Batch 0/1
Batch Loss = 1.0969659766169357
765, epoch_train_loss=1.0969659766169357
Epoch 766
Epoch 766 :: Batch 0/1
Batch Loss = 1.0965149778333019
766, epoch_train_loss=1.0965149778333019
Epoch 767
Epoch 767 :: Batch 0/1
Batch Loss = 1.0960660416165162
767, epoch_train_loss=1.0960660416165162
Epoch 768
Epoch 768 :: Batch 0/1
Batch Loss = 1.0956191264858144
768, epoch_train_loss=1.0956191264858144
Epoch 769
Epoch 769 :: Batch 0/1
Batch Loss = 1.095174186512808
769, epoch_train_loss=1.095174186512808
Epoch 770
Epoch 770 :: Batch 0/1
Batch Loss = 1.0947310993386086
770, epoch_train_loss=1.0947310993386086
Epoch 771
Epoch 771 :: Batch 0/1
Batch Loss = 1.094289859567132
771, epoch_train_loss=1.094289859567132
Epoch 772
Epoch 772 :: Batch 0/1
Batch Loss = 1.0938503828681234
772, epoch_train_loss=1.0938503828681234
Epoch 773
Epoch 773 :: Batch 0/1
Batch Loss = 1.093412610547392
773, epoch_train_loss=1.093412610547392
Epoch 774
Epoch 774 :: Batch 0/1
Batch Loss = 1.0929765287533921
774, epoch_train_loss=1.0929765287533921
Epoch 775
Epoch 775 :: Batch 0/1
Batch Loss = 1.0925420566747153
775, epoch_train_loss=1.0925420566747153
Epoch 776
Epoch 776 :: Batch 0/1
Batch Loss = 1.0921091786029211
776, epoch_train_loss=1.0921091786029211
Epoch 777
Epoch 777 :: Batch 0/1
Batch Loss = 1.091677857035734
777, epoch_train_loss=1.091677857035734
Epoch 778
Epoch 778 :: Batch 0/1
Batch Loss = 1.0912480411554557
778, epoch_train_loss=1.0912480411554557
Epoch 779
Epoch 779 :: Batch 0/1
Batch Loss = 1.0908197261832688
779, epoch_train_loss=1.0908197261832688
Epoch 780
Epoch 780 :: Batch 0/1
Batch Loss = 1.090392867265089
780, epoch_train_loss=1.090392867265089
Epoch 781
Epoch 781 :: Batch 0/1
Batch Loss = 1.0899674471089476
781, epoch_train_loss=1.0899674471089476
Epoch 782
Epoch 782 :: Batch 0/1
Batch Loss = 1.0895434548797942
782, epoch_train_loss=1.0895434548797942
Epoch 783
Epoch 783 :: Batch 0/1
Batch Loss = 1.089120858759764
783, epoch_train_loss=1.089120858759764
Epoch 784
Epoch 784 :: Batch 0/1
Batch Loss = 1.0886996584218847
784, epoch_train_loss=1.0886996584218847
Epoch 785
Epoch 785 :: Batch 0/1
Batch Loss = 1.0882798344426277
785, epoch_train_loss=1.0882798344426277
Epoch 786
Epoch 786 :: Batch 0/1
Batch Loss = 1.0878613717340448
786, epoch_train_loss=1.0878613717340448
Epoch 787
Epoch 787 :: Batch 0/1
Batch Loss = 1.0874442669082949
787, epoch_train_loss=1.0874442669082949
Epoch 788
Epoch 788 :: Batch 0/1
Batch Loss = 1.0870284978221192
788, epoch_train_loss=1.0870284978221192
Epoch 789
Epoch 789 :: Batch 0/1
Batch Loss = 1.0866140578444463
789, epoch_train_loss=1.0866140578444463
Epoch 790
Epoch 790 :: Batch 0/1
Batch Loss = 1.0862009338459508
790, epoch_train_loss=1.0862009338459508
Epoch 791
Epoch 791 :: Batch 0/1
Batch Loss = 1.0857891084135298
791, epoch_train_loss=1.0857891084135298
Epoch 792
Epoch 792 :: Batch 0/1
Batch Loss = 1.0853785751003606
792, epoch_train_loss=1.0853785751003606
Epoch 793
Epoch 793 :: Batch 0/1
Batch Loss = 1.0849693161914775
793, epoch_train_loss=1.0849693161914775
Epoch 794
Epoch 794 :: Batch 0/1
Batch Loss = 1.0845613206958509
794, epoch_train_loss=1.0845613206958509
Epoch 795
Epoch 795 :: Batch 0/1
Batch Loss = 1.0841545777263217
795, epoch_train_loss=1.0841545777263217
Epoch 796
Epoch 796 :: Batch 0/1
Batch Loss = 1.0837490705263015
796, epoch_train_loss=1.0837490705263015
Epoch 797
Epoch 797 :: Batch 0/1
Batch Loss = 1.0833447894758237
797, epoch_train_loss=1.0833447894758237
Epoch 798
Epoch 798 :: Batch 0/1
Batch Loss = 1.082941718662659
798, epoch_train_loss=1.082941718662659
Epoch 799
Epoch 799 :: Batch 0/1
Batch Loss = 1.0825398433351363
799, epoch_train_loss=1.0825398433351363
Epoch 800
Epoch 800 :: Batch 0/1
Batch Loss = 1.0821391504616333
800, epoch_train_loss=1.0821391504616333
Epoch 801
Epoch 801 :: Batch 0/1
Batch Loss = 1.0817396220191255
801, epoch_train_loss=1.0817396220191255
Epoch 802
Epoch 802 :: Batch 0/1
Batch Loss = 1.0813412440506653
802, epoch_train_loss=1.0813412440506653
Epoch 803
Epoch 803 :: Batch 0/1
Batch Loss = 1.0809440000597015
803, epoch_train_loss=1.0809440000597015
Epoch 804
Epoch 804 :: Batch 0/1
Batch Loss = 1.0805478732612885
804, epoch_train_loss=1.0805478732612885
Epoch 805
Epoch 805 :: Batch 0/1
Batch Loss = 1.080152849155391
805, epoch_train_loss=1.080152849155391
Epoch 806
Epoch 806 :: Batch 0/1
Batch Loss = 1.0797589104421523
806, epoch_train_loss=1.0797589104421523
Epoch 807
Epoch 807 :: Batch 0/1
Batch Loss = 1.0793660420892004
807, epoch_train_loss=1.0793660420892004
Epoch 808
Epoch 808 :: Batch 0/1
Batch Loss = 1.078974228505017
808, epoch_train_loss=1.078974228505017
Epoch 809
Epoch 809 :: Batch 0/1
Batch Loss = 1.0785834532932863
809, epoch_train_loss=1.0785834532932863
Epoch 810
Epoch 810 :: Batch 0/1
Batch Loss = 1.0781937018691987
810, epoch_train_loss=1.0781937018691987
Epoch 811
Epoch 811 :: Batch 0/1
Batch Loss = 1.077804957950325
811, epoch_train_loss=1.077804957950325
Epoch 812
Epoch 812 :: Batch 0/1
Batch Loss = 1.0774172063236966
812, epoch_train_loss=1.0774172063236966
Epoch 813
Epoch 813 :: Batch 0/1
Batch Loss = 1.077030431949232
813, epoch_train_loss=1.077030431949232
Epoch 814
Epoch 814 :: Batch 0/1
Batch Loss = 1.076644619007361
814, epoch_train_loss=1.076644619007361
Epoch 815
Epoch 815 :: Batch 0/1
Batch Loss = 1.0762597530573181
815, epoch_train_loss=1.0762597530573181
Epoch 816
Epoch 816 :: Batch 0/1
Batch Loss = 1.0758758190344697
816, epoch_train_loss=1.0758758190344697
Epoch 817
Epoch 817 :: Batch 0/1
Batch Loss = 1.07549280229143
817, epoch_train_loss=1.07549280229143
Epoch 818
Epoch 818 :: Batch 0/1
Batch Loss = 1.0751106887806658
818, epoch_train_loss=1.0751106887806658
Epoch 819
Epoch 819 :: Batch 0/1
Batch Loss = 1.074729464044839
819, epoch_train_loss=1.074729464044839
Epoch 820
Epoch 820 :: Batch 0/1
Batch Loss = 1.0743491144772905
820, epoch_train_loss=1.0743491144772905
Epoch 821
Epoch 821 :: Batch 0/1
Batch Loss = 1.073969626313603
821, epoch_train_loss=1.073969626313603
Epoch 822
Epoch 822 :: Batch 0/1
Batch Loss = 1.0735909859596677
822, epoch_train_loss=1.0735909859596677
Epoch 823
Epoch 823 :: Batch 0/1
Batch Loss = 1.0732131803495273
823, epoch_train_loss=1.0732131803495273
Epoch 824
Epoch 824 :: Batch 0/1
Batch Loss = 1.0728361962178405
824, epoch_train_loss=1.0728361962178405
Epoch 825
Epoch 825 :: Batch 0/1
Batch Loss = 1.0724600208727324
825, epoch_train_loss=1.0724600208727324
Epoch 826
Epoch 826 :: Batch 0/1
Batch Loss = 1.0720846417769938
826, epoch_train_loss=1.0720846417769938
Epoch 827
Epoch 827 :: Batch 0/1
Batch Loss = 1.0717100467033607
827, epoch_train_loss=1.0717100467033607
Epoch 828
Epoch 828 :: Batch 0/1
Batch Loss = 1.0713362241377271
828, epoch_train_loss=1.0713362241377271
Epoch 829
Epoch 829 :: Batch 0/1
Batch Loss = 1.0709631627678706
829, epoch_train_loss=1.0709631627678706
Epoch 830
Epoch 830 :: Batch 0/1
Batch Loss = 1.0705908520930008
830, epoch_train_loss=1.0705908520930008
Epoch 831
Epoch 831 :: Batch 0/1
Batch Loss = 1.0702192821157235
831, epoch_train_loss=1.0702192821157235
Epoch 832
Epoch 832 :: Batch 0/1
Batch Loss = 1.0698484432083588
832, epoch_train_loss=1.0698484432083588
Epoch 833
Epoch 833 :: Batch 0/1
Batch Loss = 1.0694783261499126
833, epoch_train_loss=1.0694783261499126
Epoch 834
Epoch 834 :: Batch 0/1
Batch Loss = 1.0691089213760432
834, epoch_train_loss=1.0691089213760432
Epoch 835
Epoch 835 :: Batch 0/1
Batch Loss = 1.0687402190837576
835, epoch_train_loss=1.0687402190837576
Epoch 836
Epoch 836 :: Batch 0/1
Batch Loss = 1.068372208578729
836, epoch_train_loss=1.068372208578729
Epoch 837
Epoch 837 :: Batch 0/1
Batch Loss = 1.0680048782993803
837, epoch_train_loss=1.0680048782993803
Epoch 838
Epoch 838 :: Batch 0/1
Batch Loss = 1.0676382158873585
838, epoch_train_loss=1.0676382158873585
Epoch 839
Epoch 839 :: Batch 0/1
Batch Loss = 1.0672722086724167
839, epoch_train_loss=1.0672722086724167
Epoch 840
Epoch 840 :: Batch 0/1
Batch Loss = 1.0669068442454475
840, epoch_train_loss=1.0669068442454475
Epoch 841
Epoch 841 :: Batch 0/1
Batch Loss = 1.0665421112595623
841, epoch_train_loss=1.0665421112595623
Epoch 842
Epoch 842 :: Batch 0/1
Batch Loss = 1.0661779994973928
842, epoch_train_loss=1.0661779994973928
Epoch 843
Epoch 843 :: Batch 0/1
Batch Loss = 1.0658145001647847
843, epoch_train_loss=1.0658145001647847
Epoch 844
Epoch 844 :: Batch 0/1
Batch Loss = 1.0654516053848455
844, epoch_train_loss=1.0654516053848455
Epoch 845
Epoch 845 :: Batch 0/1
Batch Loss = 1.0650893079293504
845, epoch_train_loss=1.0650893079293504
Epoch 846
Epoch 846 :: Batch 0/1
Batch Loss = 1.0647276008830266
846, epoch_train_loss=1.0647276008830266
Epoch 847
Epoch 847 :: Batch 0/1
Batch Loss = 1.0643664773956596
847, epoch_train_loss=1.0643664773956596
Epoch 848
Epoch 848 :: Batch 0/1
Batch Loss = 1.064005930654396
848, epoch_train_loss=1.064005930654396
Epoch 849
Epoch 849 :: Batch 0/1
Batch Loss = 1.0636459537676666
849, epoch_train_loss=1.0636459537676666
Epoch 850
Epoch 850 :: Batch 0/1
Batch Loss = 1.063286539905742
850, epoch_train_loss=1.063286539905742
Epoch 851
Epoch 851 :: Batch 0/1
Batch Loss = 1.0629276822159288
851, epoch_train_loss=1.0629276822159288
Epoch 852
Epoch 852 :: Batch 0/1
Batch Loss = 1.0625693739322073
852, epoch_train_loss=1.0625693739322073
Epoch 853
Epoch 853 :: Batch 0/1
Batch Loss = 1.062211608353667
853, epoch_train_loss=1.062211608353667
Epoch 854
Epoch 854 :: Batch 0/1
Batch Loss = 1.0618543788162123
854, epoch_train_loss=1.0618543788162123
Epoch 855
Epoch 855 :: Batch 0/1
Batch Loss = 1.0614976788012473
855, epoch_train_loss=1.0614976788012473
Epoch 856
Epoch 856 :: Batch 0/1
Batch Loss = 1.0611415018265604
856, epoch_train_loss=1.0611415018265604
Epoch 857
Epoch 857 :: Batch 0/1
Batch Loss = 1.0607858415371372
857, epoch_train_loss=1.0607858415371372
Epoch 858
Epoch 858 :: Batch 0/1
Batch Loss = 1.0604306916469952
858, epoch_train_loss=1.0604306916469952
Epoch 859
Epoch 859 :: Batch 0/1
Batch Loss = 1.0600760459149599
859, epoch_train_loss=1.0600760459149599
Epoch 860
Epoch 860 :: Batch 0/1
Batch Loss = 1.0597218982373662
860, epoch_train_loss=1.0597218982373662
Epoch 861
Epoch 861 :: Batch 0/1
Batch Loss = 1.0593682425380895
861, epoch_train_loss=1.0593682425380895
Epoch 862
Epoch 862 :: Batch 0/1
Batch Loss = 1.059015072837483
862, epoch_train_loss=1.059015072837483
Epoch 863
Epoch 863 :: Batch 0/1
Batch Loss = 1.0586623832653836
863, epoch_train_loss=1.0586623832653836
Epoch 864
Epoch 864 :: Batch 0/1
Batch Loss = 1.0583101679629752
864, epoch_train_loss=1.0583101679629752
Epoch 865
Epoch 865 :: Batch 0/1
Batch Loss = 1.0579584211950703
865, epoch_train_loss=1.0579584211950703
Epoch 866
Epoch 866 :: Batch 0/1
Batch Loss = 1.0576071372379576
866, epoch_train_loss=1.0576071372379576
Epoch 867
Epoch 867 :: Batch 0/1
Batch Loss = 1.057256310449651
867, epoch_train_loss=1.057256310449651
Epoch 868
Epoch 868 :: Batch 0/1
Batch Loss = 1.0569059352471466
868, epoch_train_loss=1.0569059352471466
Epoch 869
Epoch 869 :: Batch 0/1
Batch Loss = 1.056556006120429
869, epoch_train_loss=1.056556006120429
Epoch 870
Epoch 870 :: Batch 0/1
Batch Loss = 1.0562065175507698
870, epoch_train_loss=1.0562065175507698
Epoch 871
Epoch 871 :: Batch 0/1
Batch Loss = 1.0558574640871448
871, epoch_train_loss=1.0558574640871448
Epoch 872
Epoch 872 :: Batch 0/1
Batch Loss = 1.0555088403104071
872, epoch_train_loss=1.0555088403104071
Epoch 873
Epoch 873 :: Batch 0/1
Batch Loss = 1.0551606408607053
873, epoch_train_loss=1.0551606408607053
Epoch 874
Epoch 874 :: Batch 0/1
Batch Loss = 1.0548128603202247
874, epoch_train_loss=1.0548128603202247
Epoch 875
Epoch 875 :: Batch 0/1
Batch Loss = 1.0544654933990598
875, epoch_train_loss=1.0544654933990598
Epoch 876
Epoch 876 :: Batch 0/1
Batch Loss = 1.05411853474221
876, epoch_train_loss=1.05411853474221
Epoch 877
Epoch 877 :: Batch 0/1
Batch Loss = 1.0537719790299236
877, epoch_train_loss=1.0537719790299236
Epoch 878
Epoch 878 :: Batch 0/1
Batch Loss = 1.0534258209491125
878, epoch_train_loss=1.0534258209491125
Epoch 879
Epoch 879 :: Batch 0/1
Batch Loss = 1.053080055185554
879, epoch_train_loss=1.053080055185554
Epoch 880
Epoch 880 :: Batch 0/1
Batch Loss = 1.0527346764209535
880, epoch_train_loss=1.0527346764209535
Epoch 881
Epoch 881 :: Batch 0/1
Batch Loss = 1.0523896793281053
881, epoch_train_loss=1.0523896793281053
Epoch 882
Epoch 882 :: Batch 0/1
Batch Loss = 1.0520450585653394
882, epoch_train_loss=1.0520450585653394
Epoch 883
Epoch 883 :: Batch 0/1
Batch Loss = 1.051700808774012
883, epoch_train_loss=1.051700808774012
Epoch 884
Epoch 884 :: Batch 0/1
Batch Loss = 1.0513569245724943
884, epoch_train_loss=1.0513569245724943
Epoch 885
Epoch 885 :: Batch 0/1
Batch Loss = 1.0510134005532243
885, epoch_train_loss=1.0510134005532243
Epoch 886
Epoch 886 :: Batch 0/1
Batch Loss = 1.050670231251794
886, epoch_train_loss=1.050670231251794
Epoch 887
Epoch 887 :: Batch 0/1
Batch Loss = 1.0503274112218035
887, epoch_train_loss=1.0503274112218035
Epoch 888
Epoch 888 :: Batch 0/1
Batch Loss = 1.049984934952009
888, epoch_train_loss=1.049984934952009
Epoch 889
Epoch 889 :: Batch 0/1
Batch Loss = 1.0496427968872841
889, epoch_train_loss=1.0496427968872841
Epoch 890
Epoch 890 :: Batch 0/1
Batch Loss = 1.0493009914246207
890, epoch_train_loss=1.0493009914246207
Epoch 891
Epoch 891 :: Batch 0/1
Batch Loss = 1.0489595128813114
891, epoch_train_loss=1.0489595128813114
Epoch 892
Epoch 892 :: Batch 0/1
Batch Loss = 1.0486183555683977
892, epoch_train_loss=1.0486183555683977
Epoch 893
Epoch 893 :: Batch 0/1
Batch Loss = 1.0482775137062235
893, epoch_train_loss=1.0482775137062235
Epoch 894
Epoch 894 :: Batch 0/1
Batch Loss = 1.0479369814180437
894, epoch_train_loss=1.0479369814180437
Epoch 895
Epoch 895 :: Batch 0/1
Batch Loss = 1.047596752803842
895, epoch_train_loss=1.047596752803842
Epoch 896
Epoch 896 :: Batch 0/1
Batch Loss = 1.0472568218561051
896, epoch_train_loss=1.0472568218561051
Epoch 897
Epoch 897 :: Batch 0/1
Batch Loss = 1.0469171824561072
897, epoch_train_loss=1.0469171824561072
Epoch 898
Epoch 898 :: Batch 0/1
Batch Loss = 1.0465778284246663
898, epoch_train_loss=1.0465778284246663
Epoch 899
Epoch 899 :: Batch 0/1
Batch Loss = 1.0462387535219935
899, epoch_train_loss=1.0462387535219935
Epoch 900
Epoch 900 :: Batch 0/1
Batch Loss = 1.045899951398792
900, epoch_train_loss=1.045899951398792
Epoch 901
Epoch 901 :: Batch 0/1
Batch Loss = 1.0455614156027329
901, epoch_train_loss=1.0455614156027329
Epoch 902
Epoch 902 :: Batch 0/1
Batch Loss = 1.0452231396673046
902, epoch_train_loss=1.0452231396673046
Epoch 903
Epoch 903 :: Batch 0/1
Batch Loss = 1.0448851170215767
903, epoch_train_loss=1.0448851170215767
Epoch 904
Epoch 904 :: Batch 0/1
Batch Loss = 1.0445473411129496
904, epoch_train_loss=1.0445473411129496
Epoch 905
Epoch 905 :: Batch 0/1
Batch Loss = 1.0442098053251119
905, epoch_train_loss=1.0442098053251119
Epoch 906
Epoch 906 :: Batch 0/1
Batch Loss = 1.0438725030818197
906, epoch_train_loss=1.0438725030818197
Epoch 907
Epoch 907 :: Batch 0/1
Batch Loss = 1.0435354278485478
907, epoch_train_loss=1.0435354278485478
Epoch 908
Epoch 908 :: Batch 0/1
Batch Loss = 1.0431985731865452
908, epoch_train_loss=1.0431985731865452
Epoch 909
Epoch 909 :: Batch 0/1
Batch Loss = 1.0428619327001305
909, epoch_train_loss=1.0428619327001305
Epoch 910
Epoch 910 :: Batch 0/1
Batch Loss = 1.0425255001344895
910, epoch_train_loss=1.0425255001344895
Epoch 911
Epoch 911 :: Batch 0/1
Batch Loss = 1.0421892693849952
911, epoch_train_loss=1.0421892693849952
Epoch 912
Epoch 912 :: Batch 0/1
Batch Loss = 1.0418532344148468
912, epoch_train_loss=1.0418532344148468
Epoch 913
Epoch 913 :: Batch 0/1
Batch Loss = 1.0415173893116156
913, epoch_train_loss=1.0415173893116156
Epoch 914
Epoch 914 :: Batch 0/1
Batch Loss = 1.0411817282470877
914, epoch_train_loss=1.0411817282470877
Epoch 915
Epoch 915 :: Batch 0/1
Batch Loss = 1.040846245341413
915, epoch_train_loss=1.040846245341413
Epoch 916
Epoch 916 :: Batch 0/1
Batch Loss = 1.0405109346677484
916, epoch_train_loss=1.0405109346677484
Epoch 917
Epoch 917 :: Batch 0/1
Batch Loss = 1.0401757901428952
917, epoch_train_loss=1.0401757901428952
Epoch 918
Epoch 918 :: Batch 0/1
Batch Loss = 1.0398408054400279
918, epoch_train_loss=1.0398408054400279
Epoch 919
Epoch 919 :: Batch 0/1
Batch Loss = 1.039505973904907
919, epoch_train_loss=1.039505973904907
Epoch 920
Epoch 920 :: Batch 0/1
Batch Loss = 1.039171288483218
920, epoch_train_loss=1.039171288483218
Epoch 921
Epoch 921 :: Batch 0/1
Batch Loss = 1.0388367416663158
921, epoch_train_loss=1.0388367416663158
Epoch 922
Epoch 922 :: Batch 0/1
Batch Loss = 1.0385023254621233
922, epoch_train_loss=1.0385023254621233
Epoch 923
Epoch 923 :: Batch 0/1
Batch Loss = 1.0381680313961448
923, epoch_train_loss=1.0381680313961448
Epoch 924
Epoch 924 :: Batch 0/1
Batch Loss = 1.0378338505453124
924, epoch_train_loss=1.0378338505453124
Epoch 925
Epoch 925 :: Batch 0/1
Batch Loss = 1.0374997736039608
925, epoch_train_loss=1.0374997736039608
Epoch 926
Epoch 926 :: Batch 0/1
Batch Loss = 1.0371657909774326
926, epoch_train_loss=1.0371657909774326
Epoch 927
Epoch 927 :: Batch 0/1
Batch Loss = 1.0368318928953888
927, epoch_train_loss=1.0368318928953888
Epoch 928
Epoch 928 :: Batch 0/1
Batch Loss = 1.0364980695342563
928, epoch_train_loss=1.0364980695342563
Epoch 929
Epoch 929 :: Batch 0/1
Batch Loss = 1.036164311111733
929, epoch_train_loss=1.036164311111733
Epoch 930
Epoch 930 :: Batch 0/1
Batch Loss = 1.0358306080719832
930, epoch_train_loss=1.0358306080719832
Epoch 931
Epoch 931 :: Batch 0/1
Batch Loss = 1.035496951095227
931, epoch_train_loss=1.035496951095227
Epoch 932
Epoch 932 :: Batch 0/1
Batch Loss = 1.0351633311593809
932, epoch_train_loss=1.0351633311593809
Epoch 933
Epoch 933 :: Batch 0/1
Batch Loss = 1.034829739652157
933, epoch_train_loss=1.034829739652157
Epoch 934
Epoch 934 :: Batch 0/1
Batch Loss = 1.034496168302863
934, epoch_train_loss=1.034496168302863
Epoch 935
Epoch 935 :: Batch 0/1
Batch Loss = 1.0341626091699847
935, epoch_train_loss=1.0341626091699847
Epoch 936
Epoch 936 :: Batch 0/1
Batch Loss = 1.0338290546895597
936, epoch_train_loss=1.0338290546895597
Epoch 937
Epoch 937 :: Batch 0/1
Batch Loss = 1.0334954975575292
937, epoch_train_loss=1.0334954975575292
Epoch 938
Epoch 938 :: Batch 0/1
Batch Loss = 1.0331619307094306
938, epoch_train_loss=1.0331619307094306
Epoch 939
Epoch 939 :: Batch 0/1
Batch Loss = 1.0328283472469906
939, epoch_train_loss=1.0328283472469906
Epoch 940
Epoch 940 :: Batch 0/1
Batch Loss = 1.0324947404699203
940, epoch_train_loss=1.0324947404699203
Epoch 941
Epoch 941 :: Batch 0/1
Batch Loss = 1.032161103731211
941, epoch_train_loss=1.032161103731211
Epoch 942
Epoch 942 :: Batch 0/1
Batch Loss = 1.031827430504558
942, epoch_train_loss=1.031827430504558
Epoch 943
Epoch 943 :: Batch 0/1
Batch Loss = 1.0314937142512335
943, epoch_train_loss=1.0314937142512335
Epoch 944
Epoch 944 :: Batch 0/1
Batch Loss = 1.0311599484737877
944, epoch_train_loss=1.0311599484737877
Epoch 945
Epoch 945 :: Batch 0/1
Batch Loss = 1.0308261266978858
945, epoch_train_loss=1.0308261266978858
Epoch 946
Epoch 946 :: Batch 0/1
Batch Loss = 1.0304922424074907
946, epoch_train_loss=1.0304922424074907
Epoch 947
Epoch 947 :: Batch 0/1
Batch Loss = 1.0301582890360486
947, epoch_train_loss=1.0301582890360486
Epoch 948
Epoch 948 :: Batch 0/1
Batch Loss = 1.02982426001297
948, epoch_train_loss=1.02982426001297
Epoch 949
Epoch 949 :: Batch 0/1
Batch Loss = 1.0294901487612342
949, epoch_train_loss=1.0294901487612342
Epoch 950
Epoch 950 :: Batch 0/1
Batch Loss = 1.0291559486193802
950, epoch_train_loss=1.0291559486193802
Epoch 951
Epoch 951 :: Batch 0/1
Batch Loss = 1.0288216529199898
951, epoch_train_loss=1.0288216529199898
Epoch 952
Epoch 952 :: Batch 0/1
Batch Loss = 1.0284872549912205
952, epoch_train_loss=1.0284872549912205
Epoch 953
Epoch 953 :: Batch 0/1
Batch Loss = 1.0281527480807453
953, epoch_train_loss=1.0281527480807453
Epoch 954
Epoch 954 :: Batch 0/1
Batch Loss = 1.0278181254348588
954, epoch_train_loss=1.0278181254348588
Epoch 955
Epoch 955 :: Batch 0/1
Batch Loss = 1.027483380273694
955, epoch_train_loss=1.027483380273694
Epoch 956
Epoch 956 :: Batch 0/1
Batch Loss = 1.0271485057921055
956, epoch_train_loss=1.0271485057921055
Epoch 957
Epoch 957 :: Batch 0/1
Batch Loss = 1.0268134951866117
957, epoch_train_loss=1.0268134951866117
Epoch 958
Epoch 958 :: Batch 0/1
Batch Loss = 1.0264783415792416
958, epoch_train_loss=1.0264783415792416
Epoch 959
Epoch 959 :: Batch 0/1
Batch Loss = 1.0261430380986876
959, epoch_train_loss=1.0261430380986876
Epoch 960
Epoch 960 :: Batch 0/1
Batch Loss = 1.025807577834622
960, epoch_train_loss=1.025807577834622
Epoch 961
Epoch 961 :: Batch 0/1
Batch Loss = 1.0254719539524135
961, epoch_train_loss=1.0254719539524135
Epoch 962
Epoch 962 :: Batch 0/1
Batch Loss = 1.025136159554107
962, epoch_train_loss=1.025136159554107
Epoch 963
Epoch 963 :: Batch 0/1
Batch Loss = 1.0248001878066066
963, epoch_train_loss=1.0248001878066066
Epoch 964
Epoch 964 :: Batch 0/1
Batch Loss = 1.0244640319471114
964, epoch_train_loss=1.0244640319471114
Epoch 965
Epoch 965 :: Batch 0/1
Batch Loss = 1.0241276853194452
965, epoch_train_loss=1.0241276853194452
Epoch 966
Epoch 966 :: Batch 0/1
Batch Loss = 1.023791141409819
966, epoch_train_loss=1.023791141409819
Epoch 967
Epoch 967 :: Batch 0/1
Batch Loss = 1.0234543938728866
967, epoch_train_loss=1.0234543938728866
Epoch 968
Epoch 968 :: Batch 0/1
Batch Loss = 1.0231174365099158
968, epoch_train_loss=1.0231174365099158
Epoch 969
Epoch 969 :: Batch 0/1
Batch Loss = 1.022780263343563
969, epoch_train_loss=1.022780263343563
Epoch 970
Epoch 970 :: Batch 0/1
Batch Loss = 1.0224428683639457
970, epoch_train_loss=1.0224428683639457
Epoch 971
Epoch 971 :: Batch 0/1
Batch Loss = 1.0221052456017936
971, epoch_train_loss=1.0221052456017936
Epoch 972
Epoch 972 :: Batch 0/1
Batch Loss = 1.0217673888910552
972, epoch_train_loss=1.0217673888910552
Epoch 973
Epoch 973 :: Batch 0/1
Batch Loss = 1.0214292917643182
973, epoch_train_loss=1.0214292917643182
Epoch 974
Epoch 974 :: Batch 0/1
Batch Loss = 1.021090947324563
974, epoch_train_loss=1.021090947324563
Epoch 975
Epoch 975 :: Batch 0/1
Batch Loss = 1.020752348268608
975, epoch_train_loss=1.020752348268608
Epoch 976
Epoch 976 :: Batch 0/1
Batch Loss = 1.0204134868038952
976, epoch_train_loss=1.0204134868038952
Epoch 977
Epoch 977 :: Batch 0/1
Batch Loss = 1.0200743546847673
977, epoch_train_loss=1.0200743546847673
Epoch 978
Epoch 978 :: Batch 0/1
Batch Loss = 1.0197349433511635
978, epoch_train_loss=1.0197349433511635
Epoch 979
Epoch 979 :: Batch 0/1
Batch Loss = 1.01939524391522
979, epoch_train_loss=1.01939524391522
Epoch 980
Epoch 980 :: Batch 0/1
Batch Loss = 1.0190552472191252
980, epoch_train_loss=1.0190552472191252
Epoch 981
Epoch 981 :: Batch 0/1
Batch Loss = 1.018714943958088
981, epoch_train_loss=1.018714943958088
Epoch 982
Epoch 982 :: Batch 0/1
Batch Loss = 1.018374324629946
982, epoch_train_loss=1.018374324629946
Epoch 983
Epoch 983 :: Batch 0/1
Batch Loss = 1.0180333795503997
983, epoch_train_loss=1.0180333795503997
Epoch 984
Epoch 984 :: Batch 0/1
Batch Loss = 1.0176920989374634
984, epoch_train_loss=1.0176920989374634
Epoch 985
Epoch 985 :: Batch 0/1
Batch Loss = 1.0173504728038574
985, epoch_train_loss=1.0173504728038574
Epoch 986
Epoch 986 :: Batch 0/1
Batch Loss = 1.017008491030243
986, epoch_train_loss=1.017008491030243
Epoch 987
Epoch 987 :: Batch 0/1
Batch Loss = 1.0166661433571564
987, epoch_train_loss=1.0166661433571564
Epoch 988
Epoch 988 :: Batch 0/1
Batch Loss = 1.0163234192967179
988, epoch_train_loss=1.0163234192967179
Epoch 989
Epoch 989 :: Batch 0/1
Batch Loss = 1.0159803082308472
989, epoch_train_loss=1.0159803082308472
Epoch 990
Epoch 990 :: Batch 0/1
Batch Loss = 1.0156367992982065
990, epoch_train_loss=1.0156367992982065
Epoch 991
Epoch 991 :: Batch 0/1
Batch Loss = 1.0152928814688496
991, epoch_train_loss=1.0152928814688496
Epoch 992
Epoch 992 :: Batch 0/1
Batch Loss = 1.014948543540822
992, epoch_train_loss=1.014948543540822
Epoch 993
Epoch 993 :: Batch 0/1
Batch Loss = 1.01460377405795
993, epoch_train_loss=1.01460377405795
Epoch 994
Epoch 994 :: Batch 0/1
Batch Loss = 1.0142585614158088
994, epoch_train_loss=1.0142585614158088
Epoch 995
Epoch 995 :: Batch 0/1
Batch Loss = 1.013912893727988
995, epoch_train_loss=1.013912893727988
Epoch 996
Epoch 996 :: Batch 0/1
Batch Loss = 1.0135667590146147
996, epoch_train_loss=1.0135667590146147
Epoch 997
Epoch 997 :: Batch 0/1
Batch Loss = 1.0132201450165674
997, epoch_train_loss=1.0132201450165674
Epoch 998
Epoch 998 :: Batch 0/1
Batch Loss = 1.012873039305862
998, epoch_train_loss=1.012873039305862
Epoch 999
Epoch 999 :: Batch 0/1
Batch Loss = 1.0125254292637678
999, epoch_train_loss=1.0125254292637678
Epoch 1000
Epoch 1000 :: Batch 0/1
Batch Loss = 1.012177302088406
1000, epoch_train_loss=1.012177302088406
Epoch 1001
Epoch 1001 :: Batch 0/1
Batch Loss = 1.0118286448331748
1001, epoch_train_loss=1.0118286448331748
Epoch 1002
Epoch 1002 :: Batch 0/1
Batch Loss = 1.0114794443148616
1002, epoch_train_loss=1.0114794443148616
Epoch 1003
Epoch 1003 :: Batch 0/1
Batch Loss = 1.0111296873258282
1003, epoch_train_loss=1.0111296873258282
Epoch 1004
Epoch 1004 :: Batch 0/1
Batch Loss = 1.0107793604474513
1004, epoch_train_loss=1.0107793604474513
Epoch 1005
Epoch 1005 :: Batch 0/1
Batch Loss = 1.0104284503063279
1005, epoch_train_loss=1.0104284503063279
Epoch 1006
Epoch 1006 :: Batch 0/1
Batch Loss = 1.0100769434054608
1006, epoch_train_loss=1.0100769434054608
Epoch 1007
Epoch 1007 :: Batch 0/1
Batch Loss = 1.009724826401327
1007, epoch_train_loss=1.009724826401327
Epoch 1008
Epoch 1008 :: Batch 0/1
Batch Loss = 1.0093720859555553
1008, epoch_train_loss=1.0093720859555553
Epoch 1009
Epoch 1009 :: Batch 0/1
Batch Loss = 1.0090187090070655
1009, epoch_train_loss=1.0090187090070655
Epoch 1010
Epoch 1010 :: Batch 0/1
Batch Loss = 1.0086646827543244
1010, epoch_train_loss=1.0086646827543244
Epoch 1011
Epoch 1011 :: Batch 0/1
Batch Loss = 1.008309994784373
1011, epoch_train_loss=1.008309994784373
Epoch 1012
Epoch 1012 :: Batch 0/1
Batch Loss = 1.0079546331837481
1012, epoch_train_loss=1.0079546331837481
Epoch 1013
Epoch 1013 :: Batch 0/1
Batch Loss = 1.0075985866574022
1013, epoch_train_loss=1.0075985866574022
Epoch 1014
Epoch 1014 :: Batch 0/1
Batch Loss = 1.0072418446540514
1014, epoch_train_loss=1.0072418446540514
Epoch 1015
Epoch 1015 :: Batch 0/1
Batch Loss = 1.0068843974683208
1015, epoch_train_loss=1.0068843974683208
Epoch 1016
Epoch 1016 :: Batch 0/1
Batch Loss = 1.0065262364562222
1016, epoch_train_loss=1.0065262364562222
Epoch 1017
Epoch 1017 :: Batch 0/1
Batch Loss = 1.006167354056181
1017, epoch_train_loss=1.006167354056181
Epoch 1018
Epoch 1018 :: Batch 0/1
Batch Loss = 1.0058077440572806
1018, epoch_train_loss=1.0058077440572806
Epoch 1019
Epoch 1019 :: Batch 0/1
Batch Loss = 1.0054474015269248
1019, epoch_train_loss=1.0054474015269248
Epoch 1020
Epoch 1020 :: Batch 0/1
Batch Loss = 1.00508632311959
1020, epoch_train_loss=1.00508632311959
Epoch 1021
Epoch 1021 :: Batch 0/1
Batch Loss = 1.0047245070653783
1021, epoch_train_loss=1.0047245070653783
Epoch 1022
Epoch 1022 :: Batch 0/1
Batch Loss = 1.004361953253175
1022, epoch_train_loss=1.004361953253175
Epoch 1023
Epoch 1023 :: Batch 0/1
Batch Loss = 1.0039986633778375
1023, epoch_train_loss=1.0039986633778375
Epoch 1024
Epoch 1024 :: Batch 0/1
Batch Loss = 1.0036346408658345
1024, epoch_train_loss=1.0036346408658345
Epoch 1025
Epoch 1025 :: Batch 0/1
Batch Loss = 1.0032698909982771
1025, epoch_train_loss=1.0032698909982771
Epoch 1026
Epoch 1026 :: Batch 0/1
Batch Loss = 1.0029044207763267
1026, epoch_train_loss=1.0029044207763267
Epoch 1027
Epoch 1027 :: Batch 0/1
Batch Loss = 1.0025382389498916
1027, epoch_train_loss=1.0025382389498916
Epoch 1028
Epoch 1028 :: Batch 0/1
Batch Loss = 1.0021713559243062
1028, epoch_train_loss=1.0021713559243062
Epoch 1029
Epoch 1029 :: Batch 0/1
Batch Loss = 1.0018037835705131
1029, epoch_train_loss=1.0018037835705131
Epoch 1030
Epoch 1030 :: Batch 0/1
Batch Loss = 1.0014355350513624
1030, epoch_train_loss=1.0014355350513624
Epoch 1031
Epoch 1031 :: Batch 0/1
Batch Loss = 1.0010666246644642
1031, epoch_train_loss=1.0010666246644642
Epoch 1032
Epoch 1032 :: Batch 0/1
Batch Loss = 1.000697067588215
1032, epoch_train_loss=1.000697067588215
Epoch 1033
Epoch 1033 :: Batch 0/1
Batch Loss = 1.0003268795036968
1033, epoch_train_loss=1.0003268795036968
Epoch 1034
Epoch 1034 :: Batch 0/1
Batch Loss = 0.9999560763545207
1034, epoch_train_loss=0.9999560763545207
Epoch 1035
Epoch 1035 :: Batch 0/1
Batch Loss = 0.999584673991648
1035, epoch_train_loss=0.999584673991648
Epoch 1036
Epoch 1036 :: Batch 0/1
Batch Loss = 0.999212687708863
1036, epoch_train_loss=0.999212687708863
Epoch 1037
Epoch 1037 :: Batch 0/1
Batch Loss = 0.9988401319368165
1037, epoch_train_loss=0.9988401319368165
Epoch 1038
Epoch 1038 :: Batch 0/1
Batch Loss = 0.9984670198457057
1038, epoch_train_loss=0.9984670198457057
Epoch 1039
Epoch 1039 :: Batch 0/1
Batch Loss = 0.9980933628372526
1039, epoch_train_loss=0.9980933628372526
Epoch 1040
Epoch 1040 :: Batch 0/1
Batch Loss = 0.9977191703726437
1040, epoch_train_loss=0.9977191703726437
Epoch 1041
Epoch 1041 :: Batch 0/1
Batch Loss = 0.99734444942256
1041, epoch_train_loss=0.99734444942256
Epoch 1042
Epoch 1042 :: Batch 0/1
Batch Loss = 0.9969692042764897
1042, epoch_train_loss=0.9969692042764897
Epoch 1043
Epoch 1043 :: Batch 0/1
Batch Loss = 0.9965934362573232
1043, epoch_train_loss=0.9965934362573232
Epoch 1044
Epoch 1044 :: Batch 0/1
Batch Loss = 0.9962171435248808
1044, epoch_train_loss=0.9962171435248808
Epoch 1045
Epoch 1045 :: Batch 0/1
Batch Loss = 0.9958403209763032
1045, epoch_train_loss=0.9958403209763032
Epoch 1046
Epoch 1046 :: Batch 0/1
Batch Loss = 0.9954629600731699
1046, epoch_train_loss=0.9954629600731699
Epoch 1047
Epoch 1047 :: Batch 0/1
Batch Loss = 0.9950850490384678
1047, epoch_train_loss=0.9950850490384678
Epoch 1048
Epoch 1048 :: Batch 0/1
Batch Loss = 0.9947065728390523
1048, epoch_train_loss=0.9947065728390523
Epoch 1049
Epoch 1049 :: Batch 0/1
Batch Loss = 0.9943275134226623
1049, epoch_train_loss=0.9943275134226623
Epoch 1050
Epoch 1050 :: Batch 0/1
Batch Loss = 0.9939478498559189
1050, epoch_train_loss=0.9939478498559189
Epoch 1051
Epoch 1051 :: Batch 0/1
Batch Loss = 0.9935675588337695
1051, epoch_train_loss=0.9935675588337695
Epoch 1052
Epoch 1052 :: Batch 0/1
Batch Loss = 0.993186614970426
1052, epoch_train_loss=0.993186614970426
Epoch 1053
Epoch 1053 :: Batch 0/1
Batch Loss = 0.9928049913151523
1053, epoch_train_loss=0.9928049913151523
Epoch 1054
Epoch 1054 :: Batch 0/1
Batch Loss = 0.9924226599194136
1054, epoch_train_loss=0.9924226599194136
Epoch 1055
Epoch 1055 :: Batch 0/1
Batch Loss = 0.9920395924607668
1055, epoch_train_loss=0.9920395924607668
Epoch 1056
Epoch 1056 :: Batch 0/1
Batch Loss = 0.9916557610799244
1056, epoch_train_loss=0.9916557610799244
Epoch 1057
Epoch 1057 :: Batch 0/1
Batch Loss = 0.9912711391407831
1057, epoch_train_loss=0.9912711391407831
Epoch 1058
Epoch 1058 :: Batch 0/1
Batch Loss = 0.990885702215609
1058, epoch_train_loss=0.990885702215609
Epoch 1059
Epoch 1059 :: Batch 0/1
Batch Loss = 0.9904994291435646
1059, epoch_train_loss=0.9904994291435646
Epoch 1060
Epoch 1060 :: Batch 0/1
Batch Loss = 0.9901123031452083
1060, epoch_train_loss=0.9901123031452083
Epoch 1061
Epoch 1061 :: Batch 0/1
Batch Loss = 0.9897243131052206
1061, epoch_train_loss=0.9897243131052206
Epoch 1062
Epoch 1062 :: Batch 0/1
Batch Loss = 0.9893354546248436
1062, epoch_train_loss=0.9893354546248436
Epoch 1063
Epoch 1063 :: Batch 0/1
Batch Loss = 0.9889457311937221
1063, epoch_train_loss=0.9889457311937221
Epoch 1064
Epoch 1064 :: Batch 0/1
Batch Loss = 0.9885551548888145
1064, epoch_train_loss=0.9885551548888145
Epoch 1065
Epoch 1065 :: Batch 0/1
Batch Loss = 0.9881637467600854
1065, epoch_train_loss=0.9881637467600854
Epoch 1066
Epoch 1066 :: Batch 0/1
Batch Loss = 0.9877715365299381
1066, epoch_train_loss=0.9877715365299381
Epoch 1067
Epoch 1067 :: Batch 0/1
Batch Loss = 0.9873785615724407
1067, epoch_train_loss=0.9873785615724407
Epoch 1068
Epoch 1068 :: Batch 0/1
Batch Loss = 0.9869848648732447
1068, epoch_train_loss=0.9869848648732447
Epoch 1069
Epoch 1069 :: Batch 0/1
Batch Loss = 0.9865904918695151
1069, epoch_train_loss=0.9865904918695151
Epoch 1070
Epoch 1070 :: Batch 0/1
Batch Loss = 0.9861954865205527
1070, epoch_train_loss=0.9861954865205527
Epoch 1071
Epoch 1071 :: Batch 0/1
Batch Loss = 0.9857998863288489
1071, epoch_train_loss=0.9857998863288489
Epoch 1072
Epoch 1072 :: Batch 0/1
Batch Loss = 0.985403717116383
1072, epoch_train_loss=0.985403717116383
Epoch 1073
Epoch 1073 :: Batch 0/1
Batch Loss = 0.9850069874796183
1073, epoch_train_loss=0.9850069874796183
Epoch 1074
Epoch 1074 :: Batch 0/1
Batch Loss = 0.9846096838675363
1074, epoch_train_loss=0.9846096838675363
Epoch 1075
Epoch 1075 :: Batch 0/1
Batch Loss = 0.9842117661847984
1075, epoch_train_loss=0.9842117661847984
Epoch 1076
Epoch 1076 :: Batch 0/1
Batch Loss = 0.9838131646193022
1076, epoch_train_loss=0.9838131646193022
Epoch 1077
Epoch 1077 :: Batch 0/1
Batch Loss = 0.9834137773388604
1077, epoch_train_loss=0.9834137773388604
Epoch 1078
Epoch 1078 :: Batch 0/1
Batch Loss = 0.9830134688843027
1078, epoch_train_loss=0.9830134688843027
Epoch 1079
Epoch 1079 :: Batch 0/1
Batch Loss = 0.9826120685968645
1079, epoch_train_loss=0.9826120685968645
Epoch 1080
Epoch 1080 :: Batch 0/1
Batch Loss = 0.9822093679114592
1080, epoch_train_loss=0.9822093679114592
Epoch 1081
Epoch 1081 :: Batch 0/1
Batch Loss = 0.9818051153884634
1081, epoch_train_loss=0.9818051153884634
Epoch 1082
Epoch 1082 :: Batch 0/1
Batch Loss = 0.9813990070323708
1082, epoch_train_loss=0.9813990070323708
Epoch 1083
Epoch 1083 :: Batch 0/1
Batch Loss = 0.9809906685668952
1083, epoch_train_loss=0.9809906685668952
Epoch 1084
Epoch 1084 :: Batch 0/1
Batch Loss = 0.9805796231668201
1084, epoch_train_loss=0.9805796231668201
Epoch 1085
Epoch 1085 :: Batch 0/1
Batch Loss = 0.9801652332005535
1085, epoch_train_loss=0.9801652332005535
Epoch 1086
Epoch 1086 :: Batch 0/1
Batch Loss = 0.9797465937379529
1086, epoch_train_loss=0.9797465937379529
Epoch 1087
Epoch 1087 :: Batch 0/1
Batch Loss = 0.9793223378683853
1087, epoch_train_loss=0.9793223378683853
Epoch 1088
Epoch 1088 :: Batch 0/1
Batch Loss = 0.9788902821929985
1088, epoch_train_loss=0.9788902821929985
Epoch 1089
Epoch 1089 :: Batch 0/1
Batch Loss = 0.9784467886972626
1089, epoch_train_loss=0.9784467886972626
Epoch 1090
Epoch 1090 :: Batch 0/1
Batch Loss = 0.9779856464545326
1090, epoch_train_loss=0.9779856464545326
Epoch 1091
Epoch 1091 :: Batch 0/1
Batch Loss = 0.9774962673288182
1091, epoch_train_loss=0.9774962673288182
Epoch 1092
Epoch 1092 :: Batch 0/1
Batch Loss = 0.9769616348079131
1092, epoch_train_loss=0.9769616348079131
Epoch 1093
Epoch 1093 :: Batch 0/1
Batch Loss = 0.9763603891999115
1093, epoch_train_loss=0.9763603891999115
Epoch 1094
Epoch 1094 :: Batch 0/1
Batch Loss = 0.9756870333173441
1094, epoch_train_loss=0.9756870333173441
Epoch 1095
Epoch 1095 :: Batch 0/1
Batch Loss = 0.9749578313374851
1095, epoch_train_loss=0.9749578313374851
Epoch 1096
Epoch 1096 :: Batch 0/1
Batch Loss = 0.9738967240407562
1096, epoch_train_loss=0.9738967240407562
Epoch 1097
Epoch 1097 :: Batch 0/1
Batch Loss = 0.9697446246926748
1097, epoch_train_loss=0.9697446246926748
Epoch 1098
Epoch 1098 :: Batch 0/1
Batch Loss = 1.029514647851286
1098, epoch_train_loss=1.029514647851286
Epoch 1099
Epoch 1099 :: Batch 0/1
Batch Loss = 0.9683522527025834
1099, epoch_train_loss=0.9683522527025834
Epoch 1100
Epoch 1100 :: Batch 0/1
Batch Loss = 0.9716158172346949
1100, epoch_train_loss=0.9716158172346949
Epoch 1101
Epoch 1101 :: Batch 0/1
Batch Loss = 0.971573420298209
1101, epoch_train_loss=0.971573420298209
Epoch 1102
Epoch 1102 :: Batch 0/1
Batch Loss = 0.9714365144009637
1102, epoch_train_loss=0.9714365144009637
Epoch 1103
Epoch 1103 :: Batch 0/1
Batch Loss = 0.9712602343127396
1103, epoch_train_loss=0.9712602343127396
Epoch 1104
Epoch 1104 :: Batch 0/1
Batch Loss = 0.9710289234135764
1104, epoch_train_loss=0.9710289234135764
Epoch 1105
Epoch 1105 :: Batch 0/1
Batch Loss = 0.9707779651646319
1105, epoch_train_loss=0.9707779651646319
Epoch 1106
Epoch 1106 :: Batch 0/1
Batch Loss = 0.9705304604089302
1106, epoch_train_loss=0.9705304604089302
Epoch 1107
Epoch 1107 :: Batch 0/1
Batch Loss = 0.9702917994269892
1107, epoch_train_loss=0.9702917994269892
Epoch 1108
Epoch 1108 :: Batch 0/1
Batch Loss = 0.9700488772553785
1108, epoch_train_loss=0.9700488772553785
Epoch 1109
Epoch 1109 :: Batch 0/1
Batch Loss = 0.9697733519356992
1109, epoch_train_loss=0.9697733519356992
Epoch 1110
Epoch 1110 :: Batch 0/1
Batch Loss = 0.9694399246260484
1110, epoch_train_loss=0.9694399246260484
Epoch 1111
Epoch 1111 :: Batch 0/1
Batch Loss = 0.969040948304964
1111, epoch_train_loss=0.969040948304964
Epoch 1112
Epoch 1112 :: Batch 0/1
Batch Loss = 0.9685916288751171
1112, epoch_train_loss=0.9685916288751171
Epoch 1113
Epoch 1113 :: Batch 0/1
Batch Loss = 0.9681183610977374
1113, epoch_train_loss=0.9681183610977374
Epoch 1114
Epoch 1114 :: Batch 0/1
Batch Loss = 0.967643291830784
1114, epoch_train_loss=0.967643291830784
Epoch 1115
Epoch 1115 :: Batch 0/1
Batch Loss = 0.9671756878165972
1115, epoch_train_loss=0.9671756878165972
Epoch 1116
Epoch 1116 :: Batch 0/1
Batch Loss = 0.9667128777424643
1116, epoch_train_loss=0.9667128777424643
Epoch 1117
Epoch 1117 :: Batch 0/1
Batch Loss = 0.9662485432780266
1117, epoch_train_loss=0.9662485432780266
Epoch 1118
Epoch 1118 :: Batch 0/1
Batch Loss = 0.9657791039000166
1118, epoch_train_loss=0.9657791039000166
Epoch 1119
Epoch 1119 :: Batch 0/1
Batch Loss = 0.965307580078081
1119, epoch_train_loss=0.965307580078081
Epoch 1120
Epoch 1120 :: Batch 0/1
Batch Loss = 0.9648409361814361
1120, epoch_train_loss=0.9648409361814361
Epoch 1121
Epoch 1121 :: Batch 0/1
Batch Loss = 0.9643857154432356
1121, epoch_train_loss=0.9643857154432356
Epoch 1122
Epoch 1122 :: Batch 0/1
Batch Loss = 0.9639430117746359
1122, epoch_train_loss=0.9639430117746359
Epoch 1123
Epoch 1123 :: Batch 0/1
Batch Loss = 0.9635074819478846
1123, epoch_train_loss=0.9635074819478846
Epoch 1124
Epoch 1124 :: Batch 0/1
Batch Loss = 0.9630703776960232
1124, epoch_train_loss=0.9630703776960232
Epoch 1125
Epoch 1125 :: Batch 0/1
Batch Loss = 0.9626244357284208
1125, epoch_train_loss=0.9626244357284208
Epoch 1126
Epoch 1126 :: Batch 0/1
Batch Loss = 0.9621675648229001
1126, epoch_train_loss=0.9621675648229001
Epoch 1127
Epoch 1127 :: Batch 0/1
Batch Loss = 0.9617024632500653
1127, epoch_train_loss=0.9617024632500653
Epoch 1128
Epoch 1128 :: Batch 0/1
Batch Loss = 0.9612339447531548
1128, epoch_train_loss=0.9612339447531548
Epoch 1129
Epoch 1129 :: Batch 0/1
Batch Loss = 0.9607651368185567
1129, epoch_train_loss=0.9607651368185567
Epoch 1130
Epoch 1130 :: Batch 0/1
Batch Loss = 0.9602960455597754
1130, epoch_train_loss=0.9602960455597754
Epoch 1131
Epoch 1131 :: Batch 0/1
Batch Loss = 0.9598242214889682
1131, epoch_train_loss=0.9598242214889682
Epoch 1132
Epoch 1132 :: Batch 0/1
Batch Loss = 0.9593472512314933
1132, epoch_train_loss=0.9593472512314933
Epoch 1133
Epoch 1133 :: Batch 0/1
Batch Loss = 0.9588646753065089
1133, epoch_train_loss=0.9588646753065089
Epoch 1134
Epoch 1134 :: Batch 0/1
Batch Loss = 0.9583783068632589
1134, epoch_train_loss=0.9583783068632589
Epoch 1135
Epoch 1135 :: Batch 0/1
Batch Loss = 0.9578908708108472
1135, epoch_train_loss=0.9578908708108472
Epoch 1136
Epoch 1136 :: Batch 0/1
Batch Loss = 0.957403898559002
1136, epoch_train_loss=0.957403898559002
Epoch 1137
Epoch 1137 :: Batch 0/1
Batch Loss = 0.956916645817732
1137, epoch_train_loss=0.956916645817732
Epoch 1138
Epoch 1138 :: Batch 0/1
Batch Loss = 0.9564264893106879
1138, epoch_train_loss=0.9564264893106879
Epoch 1139
Epoch 1139 :: Batch 0/1
Batch Loss = 0.9559307802976981
1139, epoch_train_loss=0.9559307802976981
Epoch 1140
Epoch 1140 :: Batch 0/1
Batch Loss = 0.955428323036194
1140, epoch_train_loss=0.955428323036194
Epoch 1141
Epoch 1141 :: Batch 0/1
Batch Loss = 0.9549196493219346
1141, epoch_train_loss=0.9549196493219346
Epoch 1142
Epoch 1142 :: Batch 0/1
Batch Loss = 0.9544058364033037
1142, epoch_train_loss=0.9544058364033037
Epoch 1143
Epoch 1143 :: Batch 0/1
Batch Loss = 0.9538871836965651
1143, epoch_train_loss=0.9538871836965651
Epoch 1144
Epoch 1144 :: Batch 0/1
Batch Loss = 0.9533626851777031
1144, epoch_train_loss=0.9533626851777031
Epoch 1145
Epoch 1145 :: Batch 0/1
Batch Loss = 0.9528305095869667
1145, epoch_train_loss=0.9528305095869667
Epoch 1146
Epoch 1146 :: Batch 0/1
Batch Loss = 0.952288867577577
1146, epoch_train_loss=0.952288867577577
Epoch 1147
Epoch 1147 :: Batch 0/1
Batch Loss = 0.9517363889056889
1147, epoch_train_loss=0.9517363889056889
Epoch 1148
Epoch 1148 :: Batch 0/1
Batch Loss = 0.9511717642876357
1148, epoch_train_loss=0.9511717642876357
Epoch 1149
Epoch 1149 :: Batch 0/1
Batch Loss = 0.9505927122704919
1149, epoch_train_loss=0.9505927122704919
Epoch 1150
Epoch 1150 :: Batch 0/1
Batch Loss = 0.9499946218523843
1150, epoch_train_loss=0.9499946218523843
Epoch 1151
Epoch 1151 :: Batch 0/1
Batch Loss = 0.9493671649411004
1151, epoch_train_loss=0.9493671649411004
Epoch 1152
Epoch 1152 :: Batch 0/1
Batch Loss = 0.9486793748165271
1152, epoch_train_loss=0.9486793748165271
Epoch 1153
Epoch 1153 :: Batch 0/1
Batch Loss = 0.9478849428650054
1153, epoch_train_loss=0.9478849428650054
Epoch 1154
Epoch 1154 :: Batch 0/1
Batch Loss = 0.9468759198443102
1154, epoch_train_loss=0.9468759198443102
Epoch 1155
Epoch 1155 :: Batch 0/1
Batch Loss = 848.7158544575902
1155, epoch_train_loss=848.7158544575902
Epoch 1156
Epoch 1156 :: Batch 0/1
Batch Loss = 1639.0235257921
1156, epoch_train_loss=1639.0235257921
Epoch 1157
Epoch 1157 :: Batch 0/1
Batch Loss = 121.09701461438988
1157, epoch_train_loss=121.09701461438988
Epoch 1158
Epoch 1158 :: Batch 0/1
Batch Loss = 1118.9777076915057
1158, epoch_train_loss=1118.9777076915057
Epoch 1159
Epoch 1159 :: Batch 0/1
Batch Loss = 357.5121191000192
1159, epoch_train_loss=357.5121191000192
Epoch 1160
Epoch 1160 :: Batch 0/1
Batch Loss = 614.8264418478241
1160, epoch_train_loss=614.8264418478241
Epoch 1161
Epoch 1161 :: Batch 0/1
Batch Loss = 575.7270853889089
1161, epoch_train_loss=575.7270853889089
Epoch 1162
Epoch 1162 :: Batch 0/1
Batch Loss = 236.24804995744037
1162, epoch_train_loss=236.24804995744037
Epoch 1163
Epoch 1163 :: Batch 0/1
Batch Loss = 675.7595329875128
1163, epoch_train_loss=675.7595329875128
Epoch 1164
Epoch 1164 :: Batch 0/1
Batch Loss = 39.164394973374776
1164, epoch_train_loss=39.164394973374776
Epoch 1165
Epoch 1165 :: Batch 0/1
Batch Loss = 633.2774078325355
1165, epoch_train_loss=633.2774078325355
Epoch 1166
Epoch 1166 :: Batch 0/1
Batch Loss = 5.50823326226961
1166, epoch_train_loss=5.50823326226961
Epoch 1167
Epoch 1167 :: Batch 0/1
Batch Loss = 482.4821075103087
1167, epoch_train_loss=482.4821075103087
Epoch 1168
Epoch 1168 :: Batch 0/1
Batch Loss = 76.43408504873278
1168, epoch_train_loss=76.43408504873278
Epoch 1169
Epoch 1169 :: Batch 0/1
Batch Loss = 288.71672336192586
1169, epoch_train_loss=288.71672336192586
Epoch 1170
Epoch 1170 :: Batch 0/1
Batch Loss = 175.82057468923475
1170, epoch_train_loss=175.82057468923475
Epoch 1171
Epoch 1171 :: Batch 0/1
Batch Loss = 121.4062677661997
1171, epoch_train_loss=121.4062677661997
Epoch 1172
Epoch 1172 :: Batch 0/1
Batch Loss = 239.24680757499502
1172, epoch_train_loss=239.24680757499502
Epoch 1173
Epoch 1173 :: Batch 0/1
Batch Loss = 24.267904253506863
1173, epoch_train_loss=24.267904253506863
Epoch 1174
Epoch 1174 :: Batch 0/1
Batch Loss = 239.9004247319274
1174, epoch_train_loss=239.9004247319274
Epoch 1175
Epoch 1175 :: Batch 0/1
Batch Loss = 1.6255850029946581
1175, epoch_train_loss=1.6255850029946581
Epoch 1176
Epoch 1176 :: Batch 0/1
Batch Loss = 186.60632523946563
1176, epoch_train_loss=186.60632523946563
Epoch 1177
Epoch 1177 :: Batch 0/1
Batch Loss = 29.166496723653083
1177, epoch_train_loss=29.166496723653083
Epoch 1178
Epoch 1178 :: Batch 0/1
Batch Loss = 110.07095372870339
1178, epoch_train_loss=110.07095372870339
Epoch 1179
Epoch 1179 :: Batch 0/1
Batch Loss = 70.54794606750617
1179, epoch_train_loss=70.54794606750617
Epoch 1180
Epoch 1180 :: Batch 0/1
Batch Loss = 43.64885404438187
1180, epoch_train_loss=43.64885404438187
Epoch 1181
Epoch 1181 :: Batch 0/1
Batch Loss = 95.91915614767476
1181, epoch_train_loss=95.91915614767476
Epoch 1182
Epoch 1182 :: Batch 0/1
Batch Loss = 7.3340045456905845
1182, epoch_train_loss=7.3340045456905845
Epoch 1183
Epoch 1183 :: Batch 0/1
Batch Loss = 93.80381779930538
1183, epoch_train_loss=93.80381779930538
Epoch 1184
Epoch 1184 :: Batch 0/1
Batch Loss = 2.097653170753252
1184, epoch_train_loss=2.097653170753252
Epoch 1185
Epoch 1185 :: Batch 0/1
Batch Loss = 69.80856314013025
1185, epoch_train_loss=69.80856314013025
Epoch 1186
Epoch 1186 :: Batch 0/1
Batch Loss = 15.748918603106318
1186, epoch_train_loss=15.748918603106318
Epoch 1187
Epoch 1187 :: Batch 0/1
Batch Loss = 38.452690225612265
1187, epoch_train_loss=38.452690225612265
Epoch 1188
Epoch 1188 :: Batch 0/1
Batch Loss = 32.332319046409246
1188, epoch_train_loss=32.332319046409246
Epoch 1189
Epoch 1189 :: Batch 0/1
Batch Loss = 13.556191975873833
1189, epoch_train_loss=13.556191975873833
Epoch 1190
Epoch 1190 :: Batch 0/1
Batch Loss = 40.42273911853381
1190, epoch_train_loss=40.42273911853381
Epoch 1191
Epoch 1191 :: Batch 0/1
Batch Loss = 1.94624375961074
1191, epoch_train_loss=1.94624375961074
Epoch 1192
Epoch 1192 :: Batch 0/1
Batch Loss = 36.99742392306804
1192, epoch_train_loss=36.99742392306804
Epoch 1193
Epoch 1193 :: Batch 0/1
Batch Loss = 2.375768250225553
1193, epoch_train_loss=2.375768250225553
Epoch 1194
Epoch 1194 :: Batch 0/1
Batch Loss = 25.807407419581843
1194, epoch_train_loss=25.807407419581843
Epoch 1195
Epoch 1195 :: Batch 0/1
Batch Loss = 8.882338949234386
1195, epoch_train_loss=8.882338949234386
Epoch 1196
Epoch 1196 :: Batch 0/1
Batch Loss = 13.224737873167388
1196, epoch_train_loss=13.224737873167388
Epoch 1197
Epoch 1197 :: Batch 0/1
Batch Loss = 15.078841633926523
1197, epoch_train_loss=15.078841633926523
Epoch 1198
Epoch 1198 :: Batch 0/1
Batch Loss = 4.338213930135678
1198, epoch_train_loss=4.338213930135678
Epoch 1199
Epoch 1199 :: Batch 0/1
Batch Loss = 17.15699693675704
1199, epoch_train_loss=17.15699693675704
Epoch 1200
Epoch 1200 :: Batch 0/1
Batch Loss = 1.0040216431057416
1200, epoch_train_loss=1.0040216431057416
Epoch 1201
Epoch 1201 :: Batch 0/1
Batch Loss = 14.749453797352743
1201, epoch_train_loss=14.749453797352743
Epoch 1202
Epoch 1202 :: Batch 0/1
Batch Loss = 2.0400654999679797
1202, epoch_train_loss=2.0400654999679797
Epoch 1203
Epoch 1203 :: Batch 0/1
Batch Loss = 9.833618538692642
1203, epoch_train_loss=9.833618538692642
Epoch 1204
Epoch 1204 :: Batch 0/1
Batch Loss = 4.835938004088927
1204, epoch_train_loss=4.835938004088927
Epoch 1205
Epoch 1205 :: Batch 0/1
Batch Loss = 4.956657083988297
1205, epoch_train_loss=4.956657083988297
Epoch 1206
Epoch 1206 :: Batch 0/1
Batch Loss = 7.025878779552638
1206, epoch_train_loss=7.025878779552638
Epoch 1207
Epoch 1207 :: Batch 0/1
Batch Loss = 1.851867485709217
1207, epoch_train_loss=1.851867485709217
Epoch 1208
Epoch 1208 :: Batch 0/1
Batch Loss = 7.439787165857183
1208, epoch_train_loss=7.439787165857183
Epoch 1209
Epoch 1209 :: Batch 0/1
Batch Loss = 0.9315174241968527
1209, epoch_train_loss=0.9315174241968527
Epoch 1210
Epoch 1210 :: Batch 0/1
Batch Loss = 6.196263615337689
1210, epoch_train_loss=6.196263615337689
Epoch 1211
Epoch 1211 :: Batch 0/1
Batch Loss = 1.549274137109958
1211, epoch_train_loss=1.549274137109958
Epoch 1212
Epoch 1212 :: Batch 0/1
Batch Loss = 4.169533643299661
1212, epoch_train_loss=4.169533643299661
Epoch 1213
Epoch 1213 :: Batch 0/1
Batch Loss = 2.663086524506372
1213, epoch_train_loss=2.663086524506372
Epoch 1214
Epoch 1214 :: Batch 0/1
Batch Loss = 2.3145592812549065
1214, epoch_train_loss=2.3145592812549065
Epoch 1215
Epoch 1215 :: Batch 0/1
Batch Loss = 3.4301382577925583
1215, epoch_train_loss=3.4301382577925583
Epoch 1216
Epoch 1216 :: Batch 0/1
Batch Loss = 1.2087977686541083
1216, epoch_train_loss=1.2087977686541083
Epoch 1217
Epoch 1217 :: Batch 0/1
Batch Loss = 3.492288337315669
1217, epoch_train_loss=3.492288337315669
Epoch 1218
Epoch 1218 :: Batch 0/1
Batch Loss = 0.9315747025510471
1218, epoch_train_loss=0.9315747025510471
Epoch 1219
Epoch 1219 :: Batch 0/1
Batch Loss = 2.9518710243935193
1219, epoch_train_loss=2.9518710243935193
Epoch 1220
Epoch 1220 :: Batch 0/1
Batch Loss = 1.202149752027097
1220, epoch_train_loss=1.202149752027097
Epoch 1221
Epoch 1221 :: Batch 0/1
Batch Loss = 2.1545813783708803
1221, epoch_train_loss=2.1545813783708803
Epoch 1222
Epoch 1222 :: Batch 0/1
Batch Loss = 1.627771673230537
1222, epoch_train_loss=1.627771673230537
Epoch 1223
Epoch 1223 :: Batch 0/1
Batch Loss = 1.4483796395566275
1223, epoch_train_loss=1.4483796395566275
Epoch 1224
Epoch 1224 :: Batch 0/1
Batch Loss = 1.9079413739223219
1224, epoch_train_loss=1.9079413739223219
Epoch 1225
Epoch 1225 :: Batch 0/1
Batch Loss = 1.0324401544897568
1225, epoch_train_loss=1.0324401544897568
Epoch 1226
Epoch 1226 :: Batch 0/1
Batch Loss = 1.923028638143531
1226, epoch_train_loss=1.923028638143531
Epoch 1227
Epoch 1227 :: Batch 0/1
Batch Loss = 0.9260160639463874
1227, epoch_train_loss=0.9260160639463874
Epoch 1228
Epoch 1228 :: Batch 0/1
Batch Loss = 1.7161469155524396
1228, epoch_train_loss=1.7161469155524396
Epoch 1229
Epoch 1229 :: Batch 0/1
Batch Loss = 1.0234158486100695
1229, epoch_train_loss=1.0234158486100695
Epoch 1230
Epoch 1230 :: Batch 0/1
Batch Loss = 1.4138176043877255
1230, epoch_train_loss=1.4138176043877255
Epoch 1231
Epoch 1231 :: Batch 0/1
Batch Loss = 1.1816586026564357
1231, epoch_train_loss=1.1816586026564357
Epoch 1232
Epoch 1232 :: Batch 0/1
Batch Loss = 1.1420272548034107
1232, epoch_train_loss=1.1420272548034107
Epoch 1233
Epoch 1233 :: Batch 0/1
Batch Loss = 1.2905670581573068
1233, epoch_train_loss=1.2905670581573068
Epoch 1234
Epoch 1234 :: Batch 0/1
Batch Loss = 0.9749842460000535
1234, epoch_train_loss=0.9749842460000535
Epoch 1235
Epoch 1235 :: Batch 0/1
Batch Loss = 1.3047389621588645
1235, epoch_train_loss=1.3047389621588645
Epoch 1236
Epoch 1236 :: Batch 0/1
Batch Loss = 0.9220002270086318
1236, epoch_train_loss=0.9220002270086318
Epoch 1237
Epoch 1237 :: Batch 0/1
Batch Loss = 1.235718969988368
1237, epoch_train_loss=1.235718969988368
Epoch 1238
Epoch 1238 :: Batch 0/1
Batch Loss = 0.9490425173016876
1238, epoch_train_loss=0.9490425173016876
Epoch 1239
Epoch 1239 :: Batch 0/1
Batch Loss = 1.1258583029292184
1239, epoch_train_loss=1.1258583029292184
Epoch 1240
Epoch 1240 :: Batch 0/1
Batch Loss = 1.0053608876763378
1240, epoch_train_loss=1.0053608876763378
Epoch 1241
Epoch 1241 :: Batch 0/1
Batch Loss = 1.0209265823757276
1241, epoch_train_loss=1.0209265823757276
Epoch 1242
Epoch 1242 :: Batch 0/1
Batch Loss = 1.0498249105117567
1242, epoch_train_loss=1.0498249105117567
Epoch 1243
Epoch 1243 :: Batch 0/1
Batch Loss = 0.9500554998622004
1243, epoch_train_loss=0.9500554998622004
Epoch 1244
Epoch 1244 :: Batch 0/1
Batch Loss = 1.062315722682472
1244, epoch_train_loss=1.062315722682472
Epoch 1245
Epoch 1245 :: Batch 0/1
Batch Loss = 0.9206311171140844
1245, epoch_train_loss=0.9206311171140844
Epoch 1246
Epoch 1246 :: Batch 0/1
Batch Loss = 1.0433353157528775
1246, epoch_train_loss=1.0433353157528775
Epoch 1247
Epoch 1247 :: Batch 0/1
Batch Loss = 0.9232042007468619
1247, epoch_train_loss=0.9232042007468619
Epoch 1248
Epoch 1248 :: Batch 0/1
Batch Loss = 1.0059459349447193
1248, epoch_train_loss=1.0059459349447193
Epoch 1249
Epoch 1249 :: Batch 0/1
Batch Loss = 0.941001755410697
1249, epoch_train_loss=0.941001755410697
Epoch 1250
Epoch 1250 :: Batch 0/1
Batch Loss = 0.9660932434561866
1250, epoch_train_loss=0.9660932434561866
Epoch 1251
Epoch 1251 :: Batch 0/1
Batch Loss = 0.9585972121096337
1251, epoch_train_loss=0.9585972121096337
Epoch 1252
Epoch 1252 :: Batch 0/1
Batch Loss = 0.9355611517745884
1252, epoch_train_loss=0.9355611517745884
Epoch 1253
Epoch 1253 :: Batch 0/1
Batch Loss = 0.9668509991381697
1253, epoch_train_loss=0.9668509991381697
Epoch 1254
Epoch 1254 :: Batch 0/1
Batch Loss = 0.919121718491489
1254, epoch_train_loss=0.919121718491489
Epoch 1255
Epoch 1255 :: Batch 0/1
Batch Loss = 0.9637606550186825
1255, epoch_train_loss=0.9637606550186825
Epoch 1256
Epoch 1256 :: Batch 0/1
Batch Loss = 0.915306389812927
1256, epoch_train_loss=0.915306389812927
Epoch 1257
Epoch 1257 :: Batch 0/1
Batch Loss = 0.9524318249747328
1257, epoch_train_loss=0.9524318249747328
Epoch 1258
Epoch 1258 :: Batch 0/1
Batch Loss = 0.9191525624985697
1258, epoch_train_loss=0.9191525624985697
Epoch 1259
Epoch 1259 :: Batch 0/1
Batch Loss = 0.9380226246614294
1259, epoch_train_loss=0.9380226246614294
Epoch 1260
Epoch 1260 :: Batch 0/1
Batch Loss = 0.925153105148086
1260, epoch_train_loss=0.925153105148086
Epoch 1261
Epoch 1261 :: Batch 0/1
Batch Loss = 0.9251383347649415
1261, epoch_train_loss=0.9251383347649415
Epoch 1262
Epoch 1262 :: Batch 0/1
Batch Loss = 0.9292977609199105
1262, epoch_train_loss=0.9292977609199105
Epoch 1263
Epoch 1263 :: Batch 0/1
Batch Loss = 0.9164088621193814
1263, epoch_train_loss=0.9164088621193814
Epoch 1264
Epoch 1264 :: Batch 0/1
Batch Loss = 0.9298426782616762
1264, epoch_train_loss=0.9298426782616762
Epoch 1265
Epoch 1265 :: Batch 0/1
Batch Loss = 0.9122897571935141
1265, epoch_train_loss=0.9122897571935141
Epoch 1266
Epoch 1266 :: Batch 0/1
Batch Loss = 0.927019867847901
1266, epoch_train_loss=0.927019867847901
Epoch 1267
Epoch 1267 :: Batch 0/1
Batch Loss = 0.9116859089950589
1267, epoch_train_loss=0.9116859089950589
Epoch 1268
Epoch 1268 :: Batch 0/1
Batch Loss = 0.922199978695962
1268, epoch_train_loss=0.922199978695962
Epoch 1269
Epoch 1269 :: Batch 0/1
Batch Loss = 0.9128613600571557
1269, epoch_train_loss=0.9128613600571557
Epoch 1270
Epoch 1270 :: Batch 0/1
Batch Loss = 0.9169980634234637
1270, epoch_train_loss=0.9169980634234637
Epoch 1271
Epoch 1271 :: Batch 0/1
Batch Loss = 0.9142233700126554
1271, epoch_train_loss=0.9142233700126554
Epoch 1272
Epoch 1272 :: Batch 0/1
Batch Loss = 0.9126405411291236
1272, epoch_train_loss=0.9126405411291236
Epoch 1273
Epoch 1273 :: Batch 0/1
Batch Loss = 0.914766961418773
1273, epoch_train_loss=0.914766961418773
Epoch 1274
Epoch 1274 :: Batch 0/1
Batch Loss = 0.9097022108759324
1274, epoch_train_loss=0.9097022108759324
Epoch 1275
Epoch 1275 :: Batch 0/1
Batch Loss = 0.9141631566617809
1275, epoch_train_loss=0.9141631566617809
Epoch 1276
Epoch 1276 :: Batch 0/1
Batch Loss = 0.9081551937479289
1276, epoch_train_loss=0.9081551937479289
Epoch 1277
Epoch 1277 :: Batch 0/1
Batch Loss = 0.9126013865691412
1277, epoch_train_loss=0.9126013865691412
Epoch 1278
Epoch 1278 :: Batch 0/1
Batch Loss = 0.907592039395628
1278, epoch_train_loss=0.907592039395628
Epoch 1279
Epoch 1279 :: Batch 0/1
Batch Loss = 0.9105297970068565
1279, epoch_train_loss=0.9105297970068565
Epoch 1280
Epoch 1280 :: Batch 0/1
Batch Loss = 0.907485539962607
1280, epoch_train_loss=0.907485539962607
Epoch 1281
Epoch 1281 :: Batch 0/1
Batch Loss = 0.9084149577700721
1281, epoch_train_loss=0.9084149577700721
Epoch 1282
Epoch 1282 :: Batch 0/1
Batch Loss = 0.9073881479900878
1282, epoch_train_loss=0.9073881479900878
Epoch 1283
Epoch 1283 :: Batch 0/1
Batch Loss = 0.906588578676147
1283, epoch_train_loss=0.906588578676147
Epoch 1284
Epoch 1284 :: Batch 0/1
Batch Loss = 0.9070322806896236
1284, epoch_train_loss=0.9070322806896236
Epoch 1285
Epoch 1285 :: Batch 0/1
Batch Loss = 0.9051943285879612
1285, epoch_train_loss=0.9051943285879612
Epoch 1286
Epoch 1286 :: Batch 0/1
Batch Loss = 0.906339227505468
1286, epoch_train_loss=0.906339227505468
Epoch 1287
Epoch 1287 :: Batch 0/1
Batch Loss = 0.9042120528678157
1287, epoch_train_loss=0.9042120528678157
Epoch 1288
Epoch 1288 :: Batch 0/1
Batch Loss = 0.9053688482702511
1288, epoch_train_loss=0.9053688482702511
Epoch 1289
Epoch 1289 :: Batch 0/1
Batch Loss = 0.9035221221351492
1289, epoch_train_loss=0.9035221221351492
Epoch 1290
Epoch 1290 :: Batch 0/1
Batch Loss = 0.9042482137426167
1290, epoch_train_loss=0.9042482137426167
Epoch 1291
Epoch 1291 :: Batch 0/1
Batch Loss = 0.9029748058740525
1291, epoch_train_loss=0.9029748058740525
Epoch 1292
Epoch 1292 :: Batch 0/1
Batch Loss = 0.9031083283927117
1292, epoch_train_loss=0.9031083283927117
Epoch 1293
Epoch 1293 :: Batch 0/1
Batch Loss = 0.9024423723005477
1293, epoch_train_loss=0.9024423723005477
Epoch 1294
Epoch 1294 :: Batch 0/1
Batch Loss = 0.9020441494298063
1294, epoch_train_loss=0.9020441494298063
Epoch 1295
Epoch 1295 :: Batch 0/1
Batch Loss = 0.9018453790789512
1295, epoch_train_loss=0.9018453790789512
Epoch 1296
Epoch 1296 :: Batch 0/1
Batch Loss = 0.9011003792401875
1296, epoch_train_loss=0.9011003792401875
Epoch 1297
Epoch 1297 :: Batch 0/1
Batch Loss = 0.9011555324927956
1297, epoch_train_loss=0.9011555324927956
Epoch 1298
Epoch 1298 :: Batch 0/1
Batch Loss = 0.9002769869558295
1298, epoch_train_loss=0.9002769869558295
Epoch 1299
Epoch 1299 :: Batch 0/1
Batch Loss = 0.9003834531803198
1299, epoch_train_loss=0.9003834531803198
Epoch 1300
Epoch 1300 :: Batch 0/1
Batch Loss = 0.8995451491378441
1300, epoch_train_loss=0.8995451491378441
Epoch 1301
Epoch 1301 :: Batch 0/1
Batch Loss = 0.8995605622054964
1301, epoch_train_loss=0.8995605622054964
Epoch 1302
Epoch 1302 :: Batch 0/1
Batch Loss = 0.8988651267306588
1302, epoch_train_loss=0.8988651267306588
Epoch 1303
Epoch 1303 :: Batch 0/1
Batch Loss = 0.8987223740439251
1303, epoch_train_loss=0.8987223740439251
Epoch 1304
Epoch 1304 :: Batch 0/1
Batch Loss = 0.8982003522127067
1304, epoch_train_loss=0.8982003522127067
Epoch 1305
Epoch 1305 :: Batch 0/1
Batch Loss = 0.8978971897482257
1305, epoch_train_loss=0.8978971897482257
Epoch 1306
Epoch 1306 :: Batch 0/1
Batch Loss = 0.8975253748967843
1306, epoch_train_loss=0.8975253748967843
Epoch 1307
Epoch 1307 :: Batch 0/1
Batch Loss = 0.8971010996405636
1307, epoch_train_loss=0.8971010996405636
Epoch 1308
Epoch 1308 :: Batch 0/1
Batch Loss = 0.8968279239816099
1308, epoch_train_loss=0.8968279239816099
Epoch 1309
Epoch 1309 :: Batch 0/1
Batch Loss = 0.8963381498056183
1309, epoch_train_loss=0.8963381498056183
Epoch 1310
Epoch 1310 :: Batch 0/1
Batch Loss = 0.8961068146268345
1310, epoch_train_loss=0.8961068146268345
Epoch 1311
Epoch 1311 :: Batch 0/1
Batch Loss = 0.8956035996918215
1311, epoch_train_loss=0.8956035996918215
Epoch 1312
Epoch 1312 :: Batch 0/1
Batch Loss = 0.8953678698004832
1312, epoch_train_loss=0.8953678698004832
Epoch 1313
Epoch 1313 :: Batch 0/1
Batch Loss = 0.894888197989276
1313, epoch_train_loss=0.894888197989276
Epoch 1314
Epoch 1314 :: Batch 0/1
Batch Loss = 0.8946196690085931
1314, epoch_train_loss=0.8946196690085931
Epoch 1315
Epoch 1315 :: Batch 0/1
Batch Loss = 0.8941819812893524
1315, epoch_train_loss=0.8941819812893524
Epoch 1316
Epoch 1316 :: Batch 0/1
Batch Loss = 0.8938702666042472
1316, epoch_train_loss=0.8938702666042472
Epoch 1317
Epoch 1317 :: Batch 0/1
Batch Loss = 0.893476810130114
1317, epoch_train_loss=0.893476810130114
Epoch 1318
Epoch 1318 :: Batch 0/1
Batch Loss = 0.8931253177333102
1318, epoch_train_loss=0.8931253177333102
Epoch 1319
Epoch 1319 :: Batch 0/1
Batch Loss = 0.8927674982925539
1319, epoch_train_loss=0.8927674982925539
Epoch 1320
Epoch 1320 :: Batch 0/1
Batch Loss = 0.8923875163137088
1320, epoch_train_loss=0.8923875163137088
Epoch 1321
Epoch 1321 :: Batch 0/1
Batch Loss = 0.892051805764945
1321, epoch_train_loss=0.892051805764945
Epoch 1322
Epoch 1322 :: Batch 0/1
Batch Loss = 0.8916569666171256
1322, epoch_train_loss=0.8916569666171256
Epoch 1323
Epoch 1323 :: Batch 0/1
Batch Loss = 0.8913297521843486
1323, epoch_train_loss=0.8913297521843486
Epoch 1324
Epoch 1324 :: Batch 0/1
Batch Loss = 0.8909320606335464
1324, epoch_train_loss=0.8909320606335464
Epoch 1325
Epoch 1325 :: Batch 0/1
Batch Loss = 0.8906026839124174
1325, epoch_train_loss=0.8906026839124174
Epoch 1326
Epoch 1326 :: Batch 0/1
Batch Loss = 0.8902102993486141
1326, epoch_train_loss=0.8902102993486141
Epoch 1327
Epoch 1327 :: Batch 0/1
Batch Loss = 0.8898720415406051
1327, epoch_train_loss=0.8898720415406051
Epoch 1328
Epoch 1328 :: Batch 0/1
Batch Loss = 0.8894885860850754
1328, epoch_train_loss=0.8894885860850754
Epoch 1329
Epoch 1329 :: Batch 0/1
Batch Loss = 0.8891386090511121
1329, epoch_train_loss=0.8891386090511121
Epoch 1330
Epoch 1330 :: Batch 0/1
Batch Loss = 0.8887643931664282
1330, epoch_train_loss=0.8887643931664282
Epoch 1331
Epoch 1331 :: Batch 0/1
Batch Loss = 0.8884034562733147
1331, epoch_train_loss=0.8884034562733147
Epoch 1332
Epoch 1332 :: Batch 0/1
Batch Loss = 0.8880368116049252
1332, epoch_train_loss=0.8880368116049252
Epoch 1333
Epoch 1333 :: Batch 0/1
Batch Loss = 0.8876673725968955
1333, epoch_train_loss=0.8876673725968955
Epoch 1334
Epoch 1334 :: Batch 0/1
Batch Loss = 0.8873052129564221
1334, epoch_train_loss=0.8873052129564221
Epoch 1335
Epoch 1335 :: Batch 0/1
Batch Loss = 0.8869301981545405
1335, epoch_train_loss=0.8869301981545405
Epoch 1336
Epoch 1336 :: Batch 0/1
Batch Loss = 0.8865693993942416
1336, epoch_train_loss=0.8865693993942416
Epoch 1337
Epoch 1337 :: Batch 0/1
Batch Loss = 0.886191600428788
1337, epoch_train_loss=0.886191600428788
Epoch 1338
Epoch 1338 :: Batch 0/1
Batch Loss = 0.8858297056249709
1338, epoch_train_loss=0.8858297056249709
Epoch 1339
Epoch 1339 :: Batch 0/1
Batch Loss = 0.8854513017900337
1339, epoch_train_loss=0.8854513017900337
Epoch 1340
Epoch 1340 :: Batch 0/1
Batch Loss = 0.8850868316081447
1340, epoch_train_loss=0.8850868316081447
Epoch 1341
Epoch 1341 :: Batch 0/1
Batch Loss = 0.8847092614422843
1341, epoch_train_loss=0.8847092614422843
Epoch 1342
Epoch 1342 :: Batch 0/1
Batch Loss = 0.8843417303040356
1342, epoch_train_loss=0.8843417303040356
Epoch 1343
Epoch 1343 :: Batch 0/1
Batch Loss = 0.8839657608821903
1343, epoch_train_loss=0.8839657608821903
Epoch 1344
Epoch 1344 :: Batch 0/1
Batch Loss = 0.8835954660002525
1344, epoch_train_loss=0.8835954660002525
Epoch 1345
Epoch 1345 :: Batch 0/1
Batch Loss = 0.8832213299721138
1345, epoch_train_loss=0.8832213299721138
Epoch 1346
Epoch 1346 :: Batch 0/1
Batch Loss = 0.8828490156723718
1346, epoch_train_loss=0.8828490156723718
Epoch 1347
Epoch 1347 :: Batch 0/1
Batch Loss = 0.8824765517041061
1347, epoch_train_loss=0.8824765517041061
Epoch 1348
Epoch 1348 :: Batch 0/1
Batch Loss = 0.8821030751174149
1348, epoch_train_loss=0.8821030751174149
Epoch 1349
Epoch 1349 :: Batch 0/1
Batch Loss = 0.8817318701448379
1349, epoch_train_loss=0.8817318701448379
Epoch 1350
Epoch 1350 :: Batch 0/1
Batch Loss = 0.8813579705349206
1350, epoch_train_loss=0.8813579705349206
Epoch 1351
Epoch 1351 :: Batch 0/1
Batch Loss = 0.8809875108262848
1351, epoch_train_loss=0.8809875108262848
Epoch 1352
Epoch 1352 :: Batch 0/1
Batch Loss = 0.8806137139131293
1352, epoch_train_loss=0.8806137139131293
Epoch 1353
Epoch 1353 :: Batch 0/1
Batch Loss = 0.8802435267021282
1353, epoch_train_loss=0.8802435267021282
Epoch 1354
Epoch 1354 :: Batch 0/1
Batch Loss = 0.8798701528816436
1354, epoch_train_loss=0.8798701528816436
Epoch 1355
Epoch 1355 :: Batch 0/1
Batch Loss = 0.8794999087098833
1355, epoch_train_loss=0.8794999087098833
Epoch 1356
Epoch 1356 :: Batch 0/1
Batch Loss = 0.8791271292121842
1356, epoch_train_loss=0.8791271292121842
Epoch 1357
Epoch 1357 :: Batch 0/1
Batch Loss = 0.8787566826101493
1357, epoch_train_loss=0.8787566826101493
Epoch 1358
Epoch 1358 :: Batch 0/1
Batch Loss = 0.8783845741991461
1358, epoch_train_loss=0.8783845741991461
Epoch 1359
Epoch 1359 :: Batch 0/1
Batch Loss = 0.8780139398605102
1359, epoch_train_loss=0.8780139398605102
Epoch 1360
Epoch 1360 :: Batch 0/1
Batch Loss = 0.8776425172377917
1360, epoch_train_loss=0.8776425172377917
Epoch 1361
Epoch 1361 :: Batch 0/1
Batch Loss = 0.8772718174454284
1361, epoch_train_loss=0.8772718174454284
Epoch 1362
Epoch 1362 :: Batch 0/1
Batch Loss = 0.8769010706330568
1362, epoch_train_loss=0.8769010706330568
Epoch 1363
Epoch 1363 :: Batch 0/1
Batch Loss = 0.8765304968829256
1363, epoch_train_loss=0.8765304968829256
Epoch 1364
Epoch 1364 :: Batch 0/1
Batch Loss = 0.8761604030503146
1364, epoch_train_loss=0.8761604030503146
Epoch 1365
Epoch 1365 :: Batch 0/1
Batch Loss = 0.8757900899712786
1365, epoch_train_loss=0.8757900899712786
Epoch 1366
Epoch 1366 :: Batch 0/1
Batch Loss = 0.87542044713358
1366, epoch_train_loss=0.87542044713358
Epoch 1367
Epoch 1367 :: Batch 0/1
Batch Loss = 0.8750502193850501
1367, epoch_train_loss=0.8750502193850501
Epoch 1368
Epoch 1368 :: Batch 0/1
Batch Loss = 0.8746805211023586
1368, epoch_train_loss=0.8746805211023586
Epoch 1369
Epoch 1369 :: Batch 0/1
Batch Loss = 0.8743099516365656
1369, epoch_train_loss=0.8743099516365656
Epoch 1370
Epoch 1370 :: Batch 0/1
Batch Loss = 0.8739396577998667
1370, epoch_train_loss=0.8739396577998667
Epoch 1371
Epoch 1371 :: Batch 0/1
Batch Loss = 0.873568384038467
1371, epoch_train_loss=0.873568384038467
Epoch 1372
Epoch 1372 :: Batch 0/1
Batch Loss = 0.8731971311857605
1372, epoch_train_loss=0.8731971311857605
Epoch 1373
Epoch 1373 :: Batch 0/1
Batch Loss = 0.8728248792436362
1373, epoch_train_loss=0.8728248792436362
Epoch 1374
Epoch 1374 :: Batch 0/1
Batch Loss = 0.8724523253127667
1374, epoch_train_loss=0.8724523253127667
Epoch 1375
Epoch 1375 :: Batch 0/1
Batch Loss = 0.8720786690856175
1375, epoch_train_loss=0.8720786690856175
Epoch 1376
Epoch 1376 :: Batch 0/1
Batch Loss = 0.8717042398707142
1376, epoch_train_loss=0.8717042398707142
Epoch 1377
Epoch 1377 :: Batch 0/1
Batch Loss = 0.8713284265205482
1377, epoch_train_loss=0.8713284265205482
Epoch 1378
Epoch 1378 :: Batch 0/1
Batch Loss = 0.8709512110778143
1378, epoch_train_loss=0.8709512110778143
Epoch 1379
Epoch 1379 :: Batch 0/1
Batch Loss = 0.8705721110212373
1379, epoch_train_loss=0.8705721110212373
Epoch 1380
Epoch 1380 :: Batch 0/1
Batch Loss = 0.8701907784330668
1380, epoch_train_loss=0.8701907784330668
Epoch 1381
Epoch 1381 :: Batch 0/1
Batch Loss = 0.8698067311130908
1381, epoch_train_loss=0.8698067311130908
Epoch 1382
Epoch 1382 :: Batch 0/1
Batch Loss = 0.8694192559514725
1382, epoch_train_loss=0.8694192559514725
Epoch 1383
Epoch 1383 :: Batch 0/1
Batch Loss = 0.8690276815560525
1383, epoch_train_loss=0.8690276815560525
Epoch 1384
Epoch 1384 :: Batch 0/1
Batch Loss = 0.8686308407436711
1384, epoch_train_loss=0.8686308407436711
Epoch 1385
Epoch 1385 :: Batch 0/1
Batch Loss = 0.868227672941041
1385, epoch_train_loss=0.868227672941041
Epoch 1386
Epoch 1386 :: Batch 0/1
Batch Loss = 0.8678165153755787
1386, epoch_train_loss=0.8678165153755787
Epoch 1387
Epoch 1387 :: Batch 0/1
Batch Loss = 0.8673959506855966
1387, epoch_train_loss=0.8673959506855966
Epoch 1388
Epoch 1388 :: Batch 0/1
Batch Loss = 0.866964275856024
1388, epoch_train_loss=0.866964275856024
Epoch 1389
Epoch 1389 :: Batch 0/1
Batch Loss = 0.8665206979495979
1389, epoch_train_loss=0.8665206979495979
Epoch 1390
Epoch 1390 :: Batch 0/1
Batch Loss = 0.8660652787666826
1390, epoch_train_loss=0.8660652787666826
Epoch 1391
Epoch 1391 :: Batch 0/1
Batch Loss = 0.8656002331171166
1391, epoch_train_loss=0.8656002331171166
Epoch 1392
Epoch 1392 :: Batch 0/1
Batch Loss = 0.8651295290901432
1392, epoch_train_loss=0.8651295290901432
Epoch 1393
Epoch 1393 :: Batch 0/1
Batch Loss = 0.8646585842716523
1393, epoch_train_loss=0.8646585842716523
Epoch 1394
Epoch 1394 :: Batch 0/1
Batch Loss = 0.8641920899480687
1394, epoch_train_loss=0.8641920899480687
Epoch 1395
Epoch 1395 :: Batch 0/1
Batch Loss = 0.8637327554072415
1395, epoch_train_loss=0.8637327554072415
Epoch 1396
Epoch 1396 :: Batch 0/1
Batch Loss = 0.8632808029488304
1396, epoch_train_loss=0.8632808029488304
Epoch 1397
Epoch 1397 :: Batch 0/1
Batch Loss = 0.8628352702011122
1397, epoch_train_loss=0.8628352702011122
Epoch 1398
Epoch 1398 :: Batch 0/1
Batch Loss = 0.8623950133124786
1398, epoch_train_loss=0.8623950133124786
Epoch 1399
Epoch 1399 :: Batch 0/1
Batch Loss = 0.8619592472558288
1399, epoch_train_loss=0.8619592472558288
Epoch 1400
Epoch 1400 :: Batch 0/1
Batch Loss = 0.8615274177603356
1400, epoch_train_loss=0.8615274177603356
Epoch 1401
Epoch 1401 :: Batch 0/1
Batch Loss = 0.8610990613022914
1401, epoch_train_loss=0.8610990613022914
Epoch 1402
Epoch 1402 :: Batch 0/1
Batch Loss = 0.8606737457807974
1402, epoch_train_loss=0.8606737457807974
Epoch 1403
Epoch 1403 :: Batch 0/1
Batch Loss = 0.8602510082978057
1403, epoch_train_loss=0.8602510082978057
Epoch 1404
Epoch 1404 :: Batch 0/1
Batch Loss = 0.8598304217351407
1404, epoch_train_loss=0.8598304217351407
Epoch 1405
Epoch 1405 :: Batch 0/1
Batch Loss = 0.8594115355963178
1405, epoch_train_loss=0.8594115355963178
Epoch 1406
Epoch 1406 :: Batch 0/1
Batch Loss = 0.8589939763460128
1406, epoch_train_loss=0.8589939763460128
Epoch 1407
Epoch 1407 :: Batch 0/1
Batch Loss = 0.8585773693836426
1407, epoch_train_loss=0.8585773693836426
Epoch 1408
Epoch 1408 :: Batch 0/1
Batch Loss = 0.8581614290094651
1408, epoch_train_loss=0.8581614290094651
Epoch 1409
Epoch 1409 :: Batch 0/1
Batch Loss = 0.8577458669242609
1409, epoch_train_loss=0.8577458669242609
Epoch 1410
Epoch 1410 :: Batch 0/1
Batch Loss = 0.857330473927927
1410, epoch_train_loss=0.857330473927927
Epoch 1411
Epoch 1411 :: Batch 0/1
Batch Loss = 0.8569150447361956
1411, epoch_train_loss=0.8569150447361956
Epoch 1412
Epoch 1412 :: Batch 0/1
Batch Loss = 0.8564994571693818
1412, epoch_train_loss=0.8564994571693818
Epoch 1413
Epoch 1413 :: Batch 0/1
Batch Loss = 0.8560836137032258
1413, epoch_train_loss=0.8560836137032258
Epoch 1414
Epoch 1414 :: Batch 0/1
Batch Loss = 0.8556675010533304
1414, epoch_train_loss=0.8556675010533304
Epoch 1415
Epoch 1415 :: Batch 0/1
Batch Loss = 0.855251137390957
1415, epoch_train_loss=0.855251137390957
Epoch 1416
Epoch 1416 :: Batch 0/1
Batch Loss = 0.8548346068699594
1416, epoch_train_loss=0.8548346068699594
Epoch 1417
Epoch 1417 :: Batch 0/1
Batch Loss = 0.8544180131749354
1417, epoch_train_loss=0.8544180131749354
Epoch 1418
Epoch 1418 :: Batch 0/1
Batch Loss = 0.8540014942878347
1418, epoch_train_loss=0.8540014942878347
Epoch 1419
Epoch 1419 :: Batch 0/1
Batch Loss = 0.8535851849745697
1419, epoch_train_loss=0.8535851849745697
Epoch 1420
Epoch 1420 :: Batch 0/1
Batch Loss = 0.853169219403669
1420, epoch_train_loss=0.853169219403669
Epoch 1421
Epoch 1421 :: Batch 0/1
Batch Loss = 0.852753706031755
1421, epoch_train_loss=0.852753706031755
Epoch 1422
Epoch 1422 :: Batch 0/1
Batch Loss = 0.8523387256847581
1422, epoch_train_loss=0.8523387256847581
Epoch 1423
Epoch 1423 :: Batch 0/1
Batch Loss = 0.8519243197761747
1423, epoch_train_loss=0.8519243197761747
Epoch 1424
Epoch 1424 :: Batch 0/1
Batch Loss = 0.8515104877943409
1424, epoch_train_loss=0.8515104877943409
Epoch 1425
Epoch 1425 :: Batch 0/1
Batch Loss = 0.8510971869564831
1425, epoch_train_loss=0.8510971869564831
Epoch 1426
Epoch 1426 :: Batch 0/1
Batch Loss = 0.8506843318333193
1426, epoch_train_loss=0.8506843318333193
Epoch 1427
Epoch 1427 :: Batch 0/1
Batch Loss = 0.8502718035323947
1427, epoch_train_loss=0.8502718035323947
Epoch 1428
Epoch 1428 :: Batch 0/1
Batch Loss = 0.8498594509522145
1428, epoch_train_loss=0.8498594509522145
Epoch 1429
Epoch 1429 :: Batch 0/1
Batch Loss = 0.8494471025869459
1429, epoch_train_loss=0.8494471025869459
Epoch 1430
Epoch 1430 :: Batch 0/1
Batch Loss = 0.8490345630902015
1430, epoch_train_loss=0.8490345630902015
Epoch 1431
Epoch 1431 :: Batch 0/1
Batch Loss = 0.8486216185639172
1431, epoch_train_loss=0.8486216185639172
Epoch 1432
Epoch 1432 :: Batch 0/1
Batch Loss = 0.8482080230068216
1432, epoch_train_loss=0.8482080230068216
Epoch 1433
Epoch 1433 :: Batch 0/1
Batch Loss = 0.8477934923527526
1433, epoch_train_loss=0.8477934923527526
Epoch 1434
Epoch 1434 :: Batch 0/1
Batch Loss = 0.8473776799037542
1434, epoch_train_loss=0.8473776799037542
Epoch 1435
Epoch 1435 :: Batch 0/1
Batch Loss = 0.8469601595793919
1435, epoch_train_loss=0.8469601595793919
Epoch 1436
Epoch 1436 :: Batch 0/1
Batch Loss = 0.8465403956939334
1436, epoch_train_loss=0.8465403956939334
Epoch 1437
Epoch 1437 :: Batch 0/1
Batch Loss = 0.8461177287630879
1437, epoch_train_loss=0.8461177287630879
Epoch 1438
Epoch 1438 :: Batch 0/1
Batch Loss = 0.8456913719901034
1438, epoch_train_loss=0.8456913719901034
Epoch 1439
Epoch 1439 :: Batch 0/1
Batch Loss = 0.8452604665198602
1439, epoch_train_loss=0.8452604665198602
Epoch 1440
Epoch 1440 :: Batch 0/1
Batch Loss = 0.8448242251063242
1440, epoch_train_loss=0.8448242251063242
Epoch 1441
Epoch 1441 :: Batch 0/1
Batch Loss = 0.8443822287806033
1441, epoch_train_loss=0.8443822287806033
Epoch 1442
Epoch 1442 :: Batch 0/1
Batch Loss = 0.8439348590902462
1442, epoch_train_loss=0.8439348590902462
Epoch 1443
Epoch 1443 :: Batch 0/1
Batch Loss = 0.8434837207639961
1443, epoch_train_loss=0.8434837207639961
Epoch 1444
Epoch 1444 :: Batch 0/1
Batch Loss = 0.8430316911573676
1444, epoch_train_loss=0.8430316911573676
Epoch 1445
Epoch 1445 :: Batch 0/1
Batch Loss = 0.8425822833859687
1445, epoch_train_loss=0.8425822833859687
Epoch 1446
Epoch 1446 :: Batch 0/1
Batch Loss = 0.8421384904491953
1446, epoch_train_loss=0.8421384904491953
Epoch 1447
Epoch 1447 :: Batch 0/1
Batch Loss = 0.8417018523857394
1447, epoch_train_loss=0.8417018523857394
Epoch 1448
Epoch 1448 :: Batch 0/1
Batch Loss = 0.8412723338977879
1448, epoch_train_loss=0.8412723338977879
Epoch 1449
Epoch 1449 :: Batch 0/1
Batch Loss = 0.8408488726284956
1449, epoch_train_loss=0.8408488726284956
Epoch 1450
Epoch 1450 :: Batch 0/1
Batch Loss = 0.8404300398422656
1450, epoch_train_loss=0.8404300398422656
Epoch 1451
Epoch 1451 :: Batch 0/1
Batch Loss = 0.8400144578526022
1451, epoch_train_loss=0.8400144578526022
Epoch 1452
Epoch 1452 :: Batch 0/1
Batch Loss = 0.8396009563370387
1452, epoch_train_loss=0.8396009563370387
Epoch 1453
Epoch 1453 :: Batch 0/1
Batch Loss = 0.8391885950941652
1453, epoch_train_loss=0.8391885950941652
Epoch 1454
Epoch 1454 :: Batch 0/1
Batch Loss = 0.8387766542181138
1454, epoch_train_loss=0.8387766542181138
Epoch 1455
Epoch 1455 :: Batch 0/1
Batch Loss = 0.838364627410992
1455, epoch_train_loss=0.838364627410992
Epoch 1456
Epoch 1456 :: Batch 0/1
Batch Loss = 0.8379522128071875
1456, epoch_train_loss=0.8379522128071875
Epoch 1457
Epoch 1457 :: Batch 0/1
Batch Loss = 0.8375392880932476
1457, epoch_train_loss=0.8375392880932476
Epoch 1458
Epoch 1458 :: Batch 0/1
Batch Loss = 0.8371258686168467
1458, epoch_train_loss=0.8371258686168467
Epoch 1459
Epoch 1459 :: Batch 0/1
Batch Loss = 0.8367120571346968
1459, epoch_train_loss=0.8367120571346968
Epoch 1460
Epoch 1460 :: Batch 0/1
Batch Loss = 0.8362979977769697
1460, epoch_train_loss=0.8362979977769697
Epoch 1461
Epoch 1461 :: Batch 0/1
Batch Loss = 0.8358838413648221
1461, epoch_train_loss=0.8358838413648221
Epoch 1462
Epoch 1462 :: Batch 0/1
Batch Loss = 0.8354697245675832
1462, epoch_train_loss=0.8354697245675832
Epoch 1463
Epoch 1463 :: Batch 0/1
Batch Loss = 0.8350557593135425
1463, epoch_train_loss=0.8350557593135425
Epoch 1464
Epoch 1464 :: Batch 0/1
Batch Loss = 0.8346420294581453
1464, epoch_train_loss=0.8346420294581453
Epoch 1465
Epoch 1465 :: Batch 0/1
Batch Loss = 0.8342285905859048
1465, epoch_train_loss=0.8342285905859048
Epoch 1466
Epoch 1466 :: Batch 0/1
Batch Loss = 0.8338154717945099
1466, epoch_train_loss=0.8338154717945099
Epoch 1467
Epoch 1467 :: Batch 0/1
Batch Loss = 0.8334026783915954
1467, epoch_train_loss=0.8334026783915954
Epoch 1468
Epoch 1468 :: Batch 0/1
Batch Loss = 0.8329901957711646
1468, epoch_train_loss=0.8329901957711646
Epoch 1469
Epoch 1469 :: Batch 0/1
Batch Loss = 0.8325779939761754
1469, epoch_train_loss=0.8325779939761754
Epoch 1470
Epoch 1470 :: Batch 0/1
Batch Loss = 0.8321660327848005
1470, epoch_train_loss=0.8321660327848005
Epoch 1471
Epoch 1471 :: Batch 0/1
Batch Loss = 0.8317542669820313
1471, epoch_train_loss=0.8317542669820313
Epoch 1472
Epoch 1472 :: Batch 0/1
Batch Loss = 0.8313426514637253
1472, epoch_train_loss=0.8313426514637253
Epoch 1473
Epoch 1473 :: Batch 0/1
Batch Loss = 0.8309311456009562
1473, epoch_train_loss=0.8309311456009562
Epoch 1474
Epoch 1474 :: Batch 0/1
Batch Loss = 0.8305197163722052
1474, epoch_train_loss=0.8305197163722052
Epoch 1475
Epoch 1475 :: Batch 0/1
Batch Loss = 0.8301083401573968
1475, epoch_train_loss=0.8301083401573968
Epoch 1476
Epoch 1476 :: Batch 0/1
Batch Loss = 0.8296970030218707
1476, epoch_train_loss=0.8296970030218707
Epoch 1477
Epoch 1477 :: Batch 0/1
Batch Loss = 0.8292856997719577
1477, epoch_train_loss=0.8292856997719577
Epoch 1478
Epoch 1478 :: Batch 0/1
Batch Loss = 0.8288744319604539
1478, epoch_train_loss=0.8288744319604539
Epoch 1479
Epoch 1479 :: Batch 0/1
Batch Loss = 0.8284632057787461
1479, epoch_train_loss=0.8284632057787461
Epoch 1480
Epoch 1480 :: Batch 0/1
Batch Loss = 0.82805202961175
1480, epoch_train_loss=0.82805202961175
Epoch 1481
Epoch 1481 :: Batch 0/1
Batch Loss = 0.8276409118203095
1481, epoch_train_loss=0.8276409118203095
Epoch 1482
Epoch 1482 :: Batch 0/1
Batch Loss = 0.8272298589634614
1482, epoch_train_loss=0.8272298589634614
Epoch 1483
Epoch 1483 :: Batch 0/1
Batch Loss = 0.826818874651838
1483, epoch_train_loss=0.826818874651838
Epoch 1484
Epoch 1484 :: Batch 0/1
Batch Loss = 0.826407958928476
1484, epoch_train_loss=0.826407958928476
Epoch 1485
Epoch 1485 :: Batch 0/1
Batch Loss = 0.8259971082671225
1485, epoch_train_loss=0.8259971082671225
Epoch 1486
Epoch 1486 :: Batch 0/1
Batch Loss = 0.8255863161657134
1486, epoch_train_loss=0.8255863161657134
Epoch 1487
Epoch 1487 :: Batch 0/1
Batch Loss = 0.8251755737706697
1487, epoch_train_loss=0.8251755737706697
Epoch 1488
Epoch 1488 :: Batch 0/1
Batch Loss = 0.8247648707666786
1488, epoch_train_loss=0.8247648707666786
Epoch 1489
Epoch 1489 :: Batch 0/1
Batch Loss = 0.8243541963085441
1489, epoch_train_loss=0.8243541963085441
Epoch 1490
Epoch 1490 :: Batch 0/1
Batch Loss = 0.8239435396421151
1490, epoch_train_loss=0.8239435396421151
Epoch 1491
Epoch 1491 :: Batch 0/1
Batch Loss = 0.823532890553432
1491, epoch_train_loss=0.823532890553432
Epoch 1492
Epoch 1492 :: Batch 0/1
Batch Loss = 0.8231222396801673
1492, epoch_train_loss=0.8231222396801673
Epoch 1493
Epoch 1493 :: Batch 0/1
Batch Loss = 0.8227115783764261
1493, epoch_train_loss=0.8227115783764261
Epoch 1494
Epoch 1494 :: Batch 0/1
Batch Loss = 0.8223008987320265
1494, epoch_train_loss=0.8223008987320265
Epoch 1495
Epoch 1495 :: Batch 0/1
Batch Loss = 0.8218901934817257
1495, epoch_train_loss=0.8218901934817257
Epoch 1496
Epoch 1496 :: Batch 0/1
Batch Loss = 0.821479455878336
1496, epoch_train_loss=0.821479455878336
Epoch 1497
Epoch 1497 :: Batch 0/1
Batch Loss = 0.8210686798264315
1497, epoch_train_loss=0.8210686798264315
Epoch 1498
Epoch 1498 :: Batch 0/1
Batch Loss = 0.8206578597769187
1498, epoch_train_loss=0.8206578597769187
Epoch 1499
Epoch 1499 :: Batch 0/1
Batch Loss = 0.8202469908163214
1499, epoch_train_loss=0.8202469908163214
Epoch 1500
Epoch 1500 :: Batch 0/1
Batch Loss = 0.8198360686097992
1500, epoch_train_loss=0.8198360686097992
Epoch 1501
Epoch 1501 :: Batch 0/1
Batch Loss = 0.8194250891689054
1501, epoch_train_loss=0.8194250891689054
Epoch 1502
Epoch 1502 :: Batch 0/1
Batch Loss = 0.8190140487828522
1502, epoch_train_loss=0.8190140487828522
Epoch 1503
Epoch 1503 :: Batch 0/1
Batch Loss = 0.8186029436383002
1503, epoch_train_loss=0.8186029436383002
Epoch 1504
Epoch 1504 :: Batch 0/1
Batch Loss = 0.8181917696577891
1504, epoch_train_loss=0.8181917696577891
Epoch 1505
Epoch 1505 :: Batch 0/1
Batch Loss = 0.8177805222234134
1505, epoch_train_loss=0.8177805222234134
Epoch 1506
Epoch 1506 :: Batch 0/1
Batch Loss = 0.8173691959703624
1506, epoch_train_loss=0.8173691959703624
Epoch 1507
Epoch 1507 :: Batch 0/1
Batch Loss = 0.8169577846460833
1507, epoch_train_loss=0.8169577846460833
Epoch 1508
Epoch 1508 :: Batch 0/1
Batch Loss = 0.8165462808615066
1508, epoch_train_loss=0.8165462808615066
Epoch 1509
Epoch 1509 :: Batch 0/1
Batch Loss = 0.816134676033568
1509, epoch_train_loss=0.816134676033568
Epoch 1510
Epoch 1510 :: Batch 0/1
Batch Loss = 0.8157229601587749
1510, epoch_train_loss=0.8157229601587749
Epoch 1511
Epoch 1511 :: Batch 0/1
Batch Loss = 0.815311121528767
1511, epoch_train_loss=0.815311121528767
Epoch 1512
Epoch 1512 :: Batch 0/1
Batch Loss = 0.8148991462896944
1512, epoch_train_loss=0.8148991462896944
Epoch 1513
Epoch 1513 :: Batch 0/1
Batch Loss = 0.8144870177901894
1513, epoch_train_loss=0.8144870177901894
Epoch 1514
Epoch 1514 :: Batch 0/1
Batch Loss = 0.8140747154045845
1514, epoch_train_loss=0.8140747154045845
Epoch 1515
Epoch 1515 :: Batch 0/1
Batch Loss = 0.8136622131801293
1515, epoch_train_loss=0.8136622131801293
Epoch 1516
Epoch 1516 :: Batch 0/1
Batch Loss = 0.8132494771849256
1516, epoch_train_loss=0.8132494771849256
Epoch 1517
Epoch 1517 :: Batch 0/1
Batch Loss = 0.8128364620927687
1517, epoch_train_loss=0.8128364620927687
Epoch 1518
Epoch 1518 :: Batch 0/1
Batch Loss = 0.8124231054163147
1518, epoch_train_loss=0.8124231054163147
Epoch 1519
Epoch 1519 :: Batch 0/1
Batch Loss = 0.8120093186678781
1519, epoch_train_loss=0.8120093186678781
Epoch 1520
Epoch 1520 :: Batch 0/1
Batch Loss = 0.8115949731217447
1520, epoch_train_loss=0.8115949731217447
Epoch 1521
Epoch 1521 :: Batch 0/1
Batch Loss = 0.811179876523287
1521, epoch_train_loss=0.811179876523287
Epoch 1522
Epoch 1522 :: Batch 0/1
Batch Loss = 0.810763734305182
1522, epoch_train_loss=0.810763734305182
Epoch 1523
Epoch 1523 :: Batch 0/1
Batch Loss = 0.8103460844735944
1523, epoch_train_loss=0.8103460844735944
Epoch 1524
Epoch 1524 :: Batch 0/1
Batch Loss = 0.8099261877628684
1524, epoch_train_loss=0.8099261877628684
Epoch 1525
Epoch 1525 :: Batch 0/1
Batch Loss = 0.8095028473000224
1525, epoch_train_loss=0.8095028473000224
Epoch 1526
Epoch 1526 :: Batch 0/1
Batch Loss = 0.80907413418449
1526, epoch_train_loss=0.80907413418449
Epoch 1527
Epoch 1527 :: Batch 0/1
Batch Loss = 0.8086370552368014
1527, epoch_train_loss=0.8086370552368014
Epoch 1528
Epoch 1528 :: Batch 0/1
Batch Loss = 0.8081874445842183
1528, epoch_train_loss=0.8081874445842183
Epoch 1529
Epoch 1529 :: Batch 0/1
Batch Loss = 0.8077209524810559
1529, epoch_train_loss=0.8077209524810559
Epoch 1530
Epoch 1530 :: Batch 0/1
Batch Loss = 0.8072364164196046
1530, epoch_train_loss=0.8072364164196046
Epoch 1531
Epoch 1531 :: Batch 0/1
Batch Loss = 0.806740749287721
1531, epoch_train_loss=0.806740749287721
Epoch 1532
Epoch 1532 :: Batch 0/1
Batch Loss = 0.8062507256053759
1532, epoch_train_loss=0.8062507256053759
Epoch 1533
Epoch 1533 :: Batch 0/1
Batch Loss = 0.8057897432558497
1533, epoch_train_loss=0.8057897432558497
Epoch 1534
Epoch 1534 :: Batch 0/1
Batch Loss = 0.8053739000112434
1534, epoch_train_loss=0.8053739000112434
Epoch 1535
Epoch 1535 :: Batch 0/1
Batch Loss = 0.804990819768754
1535, epoch_train_loss=0.804990819768754
Epoch 1536
Epoch 1536 :: Batch 0/1
Batch Loss = 0.8046090038540656
1536, epoch_train_loss=0.8046090038540656
Epoch 1537
Epoch 1537 :: Batch 0/1
Batch Loss = 0.8042046244875162
1537, epoch_train_loss=0.8042046244875162
Epoch 1538
Epoch 1538 :: Batch 0/1
Batch Loss = 0.8037703434583786
1538, epoch_train_loss=0.8037703434583786
Epoch 1539
Epoch 1539 :: Batch 0/1
Batch Loss = 0.8033121803375846
1539, epoch_train_loss=0.8033121803375846
Epoch 1540
Epoch 1540 :: Batch 0/1
Batch Loss = 0.8028451809660234
1540, epoch_train_loss=0.8028451809660234
Epoch 1541
Epoch 1541 :: Batch 0/1
Batch Loss = 0.8023875134192902
1541, epoch_train_loss=0.8023875134192902
Epoch 1542
Epoch 1542 :: Batch 0/1
Batch Loss = 0.8019508279384995
1542, epoch_train_loss=0.8019508279384995
Epoch 1543
Epoch 1543 :: Batch 0/1
Batch Loss = 0.8015332094500172
1543, epoch_train_loss=0.8015332094500172
Epoch 1544
Epoch 1544 :: Batch 0/1
Batch Loss = 0.8011231990452624
1544, epoch_train_loss=0.8011231990452624
Epoch 1545
Epoch 1545 :: Batch 0/1
Batch Loss = 0.8007092699804036
1545, epoch_train_loss=0.8007092699804036
Epoch 1546
Epoch 1546 :: Batch 0/1
Batch Loss = 0.8002850546082145
1546, epoch_train_loss=0.8002850546082145
Epoch 1547
Epoch 1547 :: Batch 0/1
Batch Loss = 0.7998498657870305
1547, epoch_train_loss=0.7998498657870305
Epoch 1548
Epoch 1548 :: Batch 0/1
Batch Loss = 0.7994075051896742
1548, epoch_train_loss=0.7994075051896742
Epoch 1549
Epoch 1549 :: Batch 0/1
Batch Loss = 0.798964478791013
1549, epoch_train_loss=0.798964478791013
Epoch 1550
Epoch 1550 :: Batch 0/1
Batch Loss = 0.7985272880780017
1550, epoch_train_loss=0.7985272880780017
Epoch 1551
Epoch 1551 :: Batch 0/1
Batch Loss = 0.7980990306486264
1551, epoch_train_loss=0.7980990306486264
Epoch 1552
Epoch 1552 :: Batch 0/1
Batch Loss = 0.7976775264984218
1552, epoch_train_loss=0.7976775264984218
Epoch 1553
Epoch 1553 :: Batch 0/1
Batch Loss = 0.7972570826460271
1553, epoch_train_loss=0.7972570826460271
Epoch 1554
Epoch 1554 :: Batch 0/1
Batch Loss = 0.7968324555726508
1554, epoch_train_loss=0.7968324555726508
Epoch 1555
Epoch 1555 :: Batch 0/1
Batch Loss = 0.796401705759232
1555, epoch_train_loss=0.796401705759232
Epoch 1556
Epoch 1556 :: Batch 0/1
Batch Loss = 0.795966542474826
1556, epoch_train_loss=0.795966542474826
Epoch 1557
Epoch 1557 :: Batch 0/1
Batch Loss = 0.7955307576620841
1557, epoch_train_loss=0.7955307576620841
Epoch 1558
Epoch 1558 :: Batch 0/1
Batch Loss = 0.7950978844453913
1558, epoch_train_loss=0.7950978844453913
Epoch 1559
Epoch 1559 :: Batch 0/1
Batch Loss = 0.7946693065760599
1559, epoch_train_loss=0.7946693065760599
Epoch 1560
Epoch 1560 :: Batch 0/1
Batch Loss = 0.794243846074429
1560, epoch_train_loss=0.794243846074429
Epoch 1561
Epoch 1561 :: Batch 0/1
Batch Loss = 0.7938188910175447
1561, epoch_train_loss=0.7938188910175447
Epoch 1562
Epoch 1562 :: Batch 0/1
Batch Loss = 0.7933920773430139
1562, epoch_train_loss=0.7933920773430139
Epoch 1563
Epoch 1563 :: Batch 0/1
Batch Loss = 0.792962457703788
1563, epoch_train_loss=0.792962457703788
Epoch 1564
Epoch 1564 :: Batch 0/1
Batch Loss = 0.7925307169948241
1564, epoch_train_loss=0.7925307169948241
Epoch 1565
Epoch 1565 :: Batch 0/1
Batch Loss = 0.7920985490644845
1565, epoch_train_loss=0.7920985490644845
Epoch 1566
Epoch 1566 :: Batch 0/1
Batch Loss = 0.7916676067021926
1566, epoch_train_loss=0.7916676067021926
Epoch 1567
Epoch 1567 :: Batch 0/1
Batch Loss = 0.7912385981586955
1567, epoch_train_loss=0.7912385981586955
Epoch 1568
Epoch 1568 :: Batch 0/1
Batch Loss = 0.7908110324831483
1568, epoch_train_loss=0.7908110324831483
Epoch 1569
Epoch 1569 :: Batch 0/1
Batch Loss = 0.7903837001642021
1569, epoch_train_loss=0.7903837001642021
Epoch 1570
Epoch 1570 :: Batch 0/1
Batch Loss = 0.7899554843384768
1570, epoch_train_loss=0.7899554843384768
Epoch 1571
Epoch 1571 :: Batch 0/1
Batch Loss = 0.7895259594849404
1571, epoch_train_loss=0.7895259594849404
Epoch 1572
Epoch 1572 :: Batch 0/1
Batch Loss = 0.7890954872483719
1572, epoch_train_loss=0.7890954872483719
Epoch 1573
Epoch 1573 :: Batch 0/1
Batch Loss = 0.7886648662190832
1573, epoch_train_loss=0.7886648662190832
Epoch 1574
Epoch 1574 :: Batch 0/1
Batch Loss = 0.7882348104664831
1574, epoch_train_loss=0.7882348104664831
Epoch 1575
Epoch 1575 :: Batch 0/1
Batch Loss = 0.7878055680335903
1575, epoch_train_loss=0.7878055680335903
Epoch 1576
Epoch 1576 :: Batch 0/1
Batch Loss = 0.7873768651487452
1576, epoch_train_loss=0.7873768651487452
Epoch 1577
Epoch 1577 :: Batch 0/1
Batch Loss = 0.786948151671037
1577, epoch_train_loss=0.786948151671037
Epoch 1578
Epoch 1578 :: Batch 0/1
Batch Loss = 0.786518957638072
1578, epoch_train_loss=0.786518957638072
Epoch 1579
Epoch 1579 :: Batch 0/1
Batch Loss = 0.7860891430993995
1579, epoch_train_loss=0.7860891430993995
Epoch 1580
Epoch 1580 :: Batch 0/1
Batch Loss = 0.7856589213169194
1580, epoch_train_loss=0.7856589213169194
Epoch 1581
Epoch 1581 :: Batch 0/1
Batch Loss = 0.785228679950902
1581, epoch_train_loss=0.785228679950902
Epoch 1582
Epoch 1582 :: Batch 0/1
Batch Loss = 0.78479873200403
1582, epoch_train_loss=0.78479873200403
Epoch 1583
Epoch 1583 :: Batch 0/1
Batch Loss = 0.7843691498103634
1583, epoch_train_loss=0.7843691498103634
Epoch 1584
Epoch 1584 :: Batch 0/1
Batch Loss = 0.7839397664358387
1584, epoch_train_loss=0.7839397664358387
Epoch 1585
Epoch 1585 :: Batch 0/1
Batch Loss = 0.783510315342235
1585, epoch_train_loss=0.783510315342235
Epoch 1586
Epoch 1586 :: Batch 0/1
Batch Loss = 0.7830806019884413
1586, epoch_train_loss=0.7830806019884413
Epoch 1587
Epoch 1587 :: Batch 0/1
Batch Loss = 0.7826506025787798
1587, epoch_train_loss=0.7826506025787798
Epoch 1588
Epoch 1588 :: Batch 0/1
Batch Loss = 0.7822204482937046
1588, epoch_train_loss=0.7822204482937046
Epoch 1589
Epoch 1589 :: Batch 0/1
Batch Loss = 0.7817903245923872
1589, epoch_train_loss=0.7817903245923872
Epoch 1590
Epoch 1590 :: Batch 0/1
Batch Loss = 0.7813603568641222
1590, epoch_train_loss=0.7813603568641222
Epoch 1591
Epoch 1591 :: Batch 0/1
Batch Loss = 0.7809305502664894
1591, epoch_train_loss=0.7809305502664894
Epoch 1592
Epoch 1592 :: Batch 0/1
Batch Loss = 0.7805008099901031
1592, epoch_train_loss=0.7805008099901031
Epoch 1593
Epoch 1593 :: Batch 0/1
Batch Loss = 0.7800710172094987
1593, epoch_train_loss=0.7800710172094987
Epoch 1594
Epoch 1594 :: Batch 0/1
Batch Loss = 0.779641105762136
1594, epoch_train_loss=0.779641105762136
Epoch 1595
Epoch 1595 :: Batch 0/1
Batch Loss = 0.7792110933974267
1595, epoch_train_loss=0.7792110933974267
Epoch 1596
Epoch 1596 :: Batch 0/1
Batch Loss = 0.778781056608253
1596, epoch_train_loss=0.778781056608253
Epoch 1597
Epoch 1597 :: Batch 0/1
Batch Loss = 0.7783510750211927
1597, epoch_train_loss=0.7783510750211927
Epoch 1598
Epoch 1598 :: Batch 0/1
Batch Loss = 0.7779211849728968
1598, epoch_train_loss=0.7779211849728968
Epoch 1599
Epoch 1599 :: Batch 0/1
Batch Loss = 0.7774913684413955
1599, epoch_train_loss=0.7774913684413955
Epoch 1600
Epoch 1600 :: Batch 0/1
Batch Loss = 0.7770615765944173
1600, epoch_train_loss=0.7770615765944173
Epoch 1601
Epoch 1601 :: Batch 0/1
Batch Loss = 0.7766317666996406
1601, epoch_train_loss=0.7766317666996406
Epoch 1602
Epoch 1602 :: Batch 0/1
Batch Loss = 0.7762019279903258
1602, epoch_train_loss=0.7762019279903258
Epoch 1603
Epoch 1603 :: Batch 0/1
Batch Loss = 0.7757720837221955
1603, epoch_train_loss=0.7757720837221955
Epoch 1604
Epoch 1604 :: Batch 0/1
Batch Loss = 0.7753422728363623
1604, epoch_train_loss=0.7753422728363623
Epoch 1605
Epoch 1605 :: Batch 0/1
Batch Loss = 0.7749125255844742
1605, epoch_train_loss=0.7749125255844742
Epoch 1606
Epoch 1606 :: Batch 0/1
Batch Loss = 0.7744828484226242
1606, epoch_train_loss=0.7744828484226242
Epoch 1607
Epoch 1607 :: Batch 0/1
Batch Loss = 0.7740532255779269
1607, epoch_train_loss=0.7740532255779269
Epoch 1608
Epoch 1608 :: Batch 0/1
Batch Loss = 0.7736236335779689
1608, epoch_train_loss=0.7736236335779689
Epoch 1609
Epoch 1609 :: Batch 0/1
Batch Loss = 0.7731940579450792
1609, epoch_train_loss=0.7731940579450792
Epoch 1610
Epoch 1610 :: Batch 0/1
Batch Loss = 0.7727645012768114
1610, epoch_train_loss=0.7727645012768114
Epoch 1611
Epoch 1611 :: Batch 0/1
Batch Loss = 0.7723349795939662
1611, epoch_train_loss=0.7723349795939662
Epoch 1612
Epoch 1612 :: Batch 0/1
Batch Loss = 0.7719055112525911
1612, epoch_train_loss=0.7719055112525911
Epoch 1613
Epoch 1613 :: Batch 0/1
Batch Loss = 0.7714761067787678
1613, epoch_train_loss=0.7714761067787678
Epoch 1614
Epoch 1614 :: Batch 0/1
Batch Loss = 0.7710467652413737
1614, epoch_train_loss=0.7710467652413737
Epoch 1615
Epoch 1615 :: Batch 0/1
Batch Loss = 0.7706174783002204
1615, epoch_train_loss=0.7706174783002204
Epoch 1616
Epoch 1616 :: Batch 0/1
Batch Loss = 0.7701882375209078
1616, epoch_train_loss=0.7701882375209078
Epoch 1617
Epoch 1617 :: Batch 0/1
Batch Loss = 0.7697590402659732
1617, epoch_train_loss=0.7697590402659732
Epoch 1618
Epoch 1618 :: Batch 0/1
Batch Loss = 0.7693298909396475
1618, epoch_train_loss=0.7693298909396475
Epoch 1619
Epoch 1619 :: Batch 0/1
Batch Loss = 0.7689007978089036
1619, epoch_train_loss=0.7689007978089036
Epoch 1620
Epoch 1620 :: Batch 0/1
Batch Loss = 0.7684717681927602
1620, epoch_train_loss=0.7684717681927602
Epoch 1621
Epoch 1621 :: Batch 0/1
Batch Loss = 0.7680428049574592
1621, epoch_train_loss=0.7680428049574592
Epoch 1622
Epoch 1622 :: Batch 0/1
Batch Loss = 0.7676139060208022
1622, epoch_train_loss=0.7676139060208022
Epoch 1623
Epoch 1623 :: Batch 0/1
Batch Loss = 0.7671850668526033
1623, epoch_train_loss=0.7671850668526033
Epoch 1624
Epoch 1624 :: Batch 0/1
Batch Loss = 0.7667562836038317
1624, epoch_train_loss=0.7667562836038317
Epoch 1625
Epoch 1625 :: Batch 0/1
Batch Loss = 0.7663275554095952
1625, epoch_train_loss=0.7663275554095952
Epoch 1626
Epoch 1626 :: Batch 0/1
Batch Loss = 0.7658988843572619
1626, epoch_train_loss=0.7658988843572619
Epoch 1627
Epoch 1627 :: Batch 0/1
Batch Loss = 0.7654702740835445
1627, epoch_train_loss=0.7654702740835445
Epoch 1628
Epoch 1628 :: Batch 0/1
Batch Loss = 0.7650417276997857
1628, epoch_train_loss=0.7650417276997857
Epoch 1629
Epoch 1629 :: Batch 0/1
Batch Loss = 0.7646132467202542
1629, epoch_train_loss=0.7646132467202542
Epoch 1630
Epoch 1630 :: Batch 0/1
Batch Loss = 0.7641848310526006
1630, epoch_train_loss=0.7641848310526006
Epoch 1631
Epoch 1631 :: Batch 0/1
Batch Loss = 0.7637564799028641
1631, epoch_train_loss=0.7637564799028641
Epoch 1632
Epoch 1632 :: Batch 0/1
Batch Loss = 0.7633281926508927
1632, epoch_train_loss=0.7633281926508927
Epoch 1633
Epoch 1633 :: Batch 0/1
Batch Loss = 0.7628999694207017
1633, epoch_train_loss=0.7628999694207017
Epoch 1634
Epoch 1634 :: Batch 0/1
Batch Loss = 0.7624718108148518
1634, epoch_train_loss=0.7624718108148518
Epoch 1635
Epoch 1635 :: Batch 0/1
Batch Loss = 0.7620437172511623
1635, epoch_train_loss=0.7620437172511623
Epoch 1636
Epoch 1636 :: Batch 0/1
Batch Loss = 0.7616156881238718
1636, epoch_train_loss=0.7616156881238718
Epoch 1637
Epoch 1637 :: Batch 0/1
Batch Loss = 0.761187721476539
1637, epoch_train_loss=0.761187721476539
Epoch 1638
Epoch 1638 :: Batch 0/1
Batch Loss = 0.7607598139440905
1638, epoch_train_loss=0.7607598139440905
Epoch 1639
Epoch 1639 :: Batch 0/1
Batch Loss = 0.7603319609907194
1639, epoch_train_loss=0.7603319609907194
Epoch 1640
Epoch 1640 :: Batch 0/1
Batch Loss = 0.759904157153777
1640, epoch_train_loss=0.759904157153777
Epoch 1641
Epoch 1641 :: Batch 0/1
Batch Loss = 0.7594763961181977
1641, epoch_train_loss=0.7594763961181977
Epoch 1642
Epoch 1642 :: Batch 0/1
Batch Loss = 0.7590486703802125
1642, epoch_train_loss=0.7590486703802125
Epoch 1643
Epoch 1643 :: Batch 0/1
Batch Loss = 0.7586209708704795
1643, epoch_train_loss=0.7586209708704795
Epoch 1644
Epoch 1644 :: Batch 0/1
Batch Loss = 0.7581932864886747
1644, epoch_train_loss=0.7581932864886747
Epoch 1645
Epoch 1645 :: Batch 0/1
Batch Loss = 0.7577656035124659
1645, epoch_train_loss=0.7577656035124659
Epoch 1646
Epoch 1646 :: Batch 0/1
Batch Loss = 0.7573379050485037
1646, epoch_train_loss=0.7573379050485037
Epoch 1647
Epoch 1647 :: Batch 0/1
Batch Loss = 0.7569101706307027
1647, epoch_train_loss=0.7569101706307027
Epoch 1648
Epoch 1648 :: Batch 0/1
Batch Loss = 0.7564823757744302
1648, epoch_train_loss=0.7564823757744302
Epoch 1649
Epoch 1649 :: Batch 0/1
Batch Loss = 0.7560544912785733
1649, epoch_train_loss=0.7560544912785733
Epoch 1650
Epoch 1650 :: Batch 0/1
Batch Loss = 0.7556264825842604
1650, epoch_train_loss=0.7556264825842604
Epoch 1651
Epoch 1651 :: Batch 0/1
Batch Loss = 0.7551983090221305
1651, epoch_train_loss=0.7551983090221305
Epoch 1652
Epoch 1652 :: Batch 0/1
Batch Loss = 0.7547699232391487
1652, epoch_train_loss=0.7547699232391487
Epoch 1653
Epoch 1653 :: Batch 0/1
Batch Loss = 0.7543412703032614
1653, epoch_train_loss=0.7543412703032614
Epoch 1654
Epoch 1654 :: Batch 0/1
Batch Loss = 0.7539122873642257
1654, epoch_train_loss=0.7539122873642257
Epoch 1655
Epoch 1655 :: Batch 0/1
Batch Loss = 0.7534829030097512
1655, epoch_train_loss=0.7534829030097512
Epoch 1656
Epoch 1656 :: Batch 0/1
Batch Loss = 0.7530530368148488
1656, epoch_train_loss=0.7530530368148488
Epoch 1657
Epoch 1657 :: Batch 0/1
Batch Loss = 0.7526225987244967
1657, epoch_train_loss=0.7526225987244967
Epoch 1658
Epoch 1658 :: Batch 0/1
Batch Loss = 0.7521914882347798
1658, epoch_train_loss=0.7521914882347798
Epoch 1659
Epoch 1659 :: Batch 0/1
Batch Loss = 0.7517595926035107
1659, epoch_train_loss=0.7517595926035107
Epoch 1660
Epoch 1660 :: Batch 0/1
Batch Loss = 0.7513267837262313
1660, epoch_train_loss=0.7513267837262313
Epoch 1661
Epoch 1661 :: Batch 0/1
Batch Loss = 0.7508929127215178
1661, epoch_train_loss=0.7508929127215178
Epoch 1662
Epoch 1662 :: Batch 0/1
Batch Loss = 0.7504578006404499
1662, epoch_train_loss=0.7504578006404499
Epoch 1663
Epoch 1663 :: Batch 0/1
Batch Loss = 0.7500212243201793
1663, epoch_train_loss=0.7500212243201793
Epoch 1664
Epoch 1664 :: Batch 0/1
Batch Loss = 0.7495828950770145
1664, epoch_train_loss=0.7495828950770145
Epoch 1665
Epoch 1665 :: Batch 0/1
Batch Loss = 0.7491424283403637
1665, epoch_train_loss=0.7491424283403637
Epoch 1666
Epoch 1666 :: Batch 0/1
Batch Loss = 0.7486993011742032
1666, epoch_train_loss=0.7486993011742032
Epoch 1667
Epoch 1667 :: Batch 0/1
Batch Loss = 0.7482527936734227
1667, epoch_train_loss=0.7482527936734227
Epoch 1668
Epoch 1668 :: Batch 0/1
Batch Loss = 0.747801910958566
1668, epoch_train_loss=0.747801910958566
Epoch 1669
Epoch 1669 :: Batch 0/1
Batch Loss = 0.7473452900664146
1669, epoch_train_loss=0.7473452900664146
Epoch 1670
Epoch 1670 :: Batch 0/1
Batch Loss = 0.74688112207822
1670, epoch_train_loss=0.74688112207822
Epoch 1671
Epoch 1671 :: Batch 0/1
Batch Loss = 0.7464071960017142
1671, epoch_train_loss=0.7464071960017142
Epoch 1672
Epoch 1672 :: Batch 0/1
Batch Loss = 0.745921330388957
1672, epoch_train_loss=0.745921330388957
Epoch 1673
Epoch 1673 :: Batch 0/1
Batch Loss = 0.745422653019291
1673, epoch_train_loss=0.745422653019291
Epoch 1674
Epoch 1674 :: Batch 0/1
Batch Loss = 0.7449139456621233
1674, epoch_train_loss=0.7449139456621233
Epoch 1675
Epoch 1675 :: Batch 0/1
Batch Loss = 0.7444035577607458
1675, epoch_train_loss=0.7444035577607458
Epoch 1676
Epoch 1676 :: Batch 0/1
Batch Loss = 0.7439030044344438
1676, epoch_train_loss=0.7439030044344438
Epoch 1677
Epoch 1677 :: Batch 0/1
Batch Loss = 0.7434192880169401
1677, epoch_train_loss=0.7434192880169401
Epoch 1678
Epoch 1678 :: Batch 0/1
Batch Loss = 0.7429495075499786
1678, epoch_train_loss=0.7429495075499786
Epoch 1679
Epoch 1679 :: Batch 0/1
Batch Loss = 0.7424840064309123
1679, epoch_train_loss=0.7424840064309123
Epoch 1680
Epoch 1680 :: Batch 0/1
Batch Loss = 0.7420135884383565
1680, epoch_train_loss=0.7420135884383565
Epoch 1681
Epoch 1681 :: Batch 0/1
Batch Loss = 0.741534527254071
1681, epoch_train_loss=0.741534527254071
Epoch 1682
Epoch 1682 :: Batch 0/1
Batch Loss = 0.7410502171884825
1682, epoch_train_loss=0.7410502171884825
Epoch 1683
Epoch 1683 :: Batch 0/1
Batch Loss = 0.7405702363996524
1683, epoch_train_loss=0.7405702363996524
Epoch 1684
Epoch 1684 :: Batch 0/1
Batch Loss = 0.740105902226564
1684, epoch_train_loss=0.740105902226564
Epoch 1685
Epoch 1685 :: Batch 0/1
Batch Loss = 0.7396615623318273
1685, epoch_train_loss=0.7396615623318273
Epoch 1686
Epoch 1686 :: Batch 0/1
Batch Loss = 0.7392295623103973
1686, epoch_train_loss=0.7392295623103973
Epoch 1687
Epoch 1687 :: Batch 0/1
Batch Loss = 0.738798262040112
1687, epoch_train_loss=0.738798262040112
Epoch 1688
Epoch 1688 :: Batch 0/1
Batch Loss = 0.7383621828688273
1688, epoch_train_loss=0.7383621828688273
Epoch 1689
Epoch 1689 :: Batch 0/1
Batch Loss = 0.7379224282061624
1689, epoch_train_loss=0.7379224282061624
Epoch 1690
Epoch 1690 :: Batch 0/1
Batch Loss = 0.7374823605963678
1690, epoch_train_loss=0.7374823605963678
Epoch 1691
Epoch 1691 :: Batch 0/1
Batch Loss = 0.7370446144818232
1691, epoch_train_loss=0.7370446144818232
Epoch 1692
Epoch 1692 :: Batch 0/1
Batch Loss = 0.736610216642356
1692, epoch_train_loss=0.736610216642356
Epoch 1693
Epoch 1693 :: Batch 0/1
Batch Loss = 0.7361788343430855
1693, epoch_train_loss=0.7361788343430855
Epoch 1694
Epoch 1694 :: Batch 0/1
Batch Loss = 0.7357493492628133
1694, epoch_train_loss=0.7357493492628133
Epoch 1695
Epoch 1695 :: Batch 0/1
Batch Loss = 0.7353203965984176
1695, epoch_train_loss=0.7353203965984176
Epoch 1696
Epoch 1696 :: Batch 0/1
Batch Loss = 0.7348907836242747
1696, epoch_train_loss=0.7348907836242747
Epoch 1697
Epoch 1697 :: Batch 0/1
Batch Loss = 0.734459770096528
1697, epoch_train_loss=0.734459770096528
Epoch 1698
Epoch 1698 :: Batch 0/1
Batch Loss = 0.7340271737945734
1698, epoch_train_loss=0.7340271737945734
Epoch 1699
Epoch 1699 :: Batch 0/1
Batch Loss = 0.7335932853864472
1699, epoch_train_loss=0.7335932853864472
Epoch 1700
Epoch 1700 :: Batch 0/1
Batch Loss = 0.733158651265382
1700, epoch_train_loss=0.733158651265382
Epoch 1701
Epoch 1701 :: Batch 0/1
Batch Loss = 0.7327238402355696
1701, epoch_train_loss=0.7327238402355696
Epoch 1702
Epoch 1702 :: Batch 0/1
Batch Loss = 0.7322892888733494
1702, epoch_train_loss=0.7322892888733494
Epoch 1703
Epoch 1703 :: Batch 0/1
Batch Loss = 0.7318552393962425
1703, epoch_train_loss=0.7318552393962425
Epoch 1704
Epoch 1704 :: Batch 0/1
Batch Loss = 0.731421728707717
1704, epoch_train_loss=0.731421728707717
Epoch 1705
Epoch 1705 :: Batch 0/1
Batch Loss = 0.7309885985768686
1705, epoch_train_loss=0.7309885985768686
Epoch 1706
Epoch 1706 :: Batch 0/1
Batch Loss = 0.7305555405123244
1706, epoch_train_loss=0.7305555405123244
Epoch 1707
Epoch 1707 :: Batch 0/1
Batch Loss = 0.7301222027988642
1707, epoch_train_loss=0.7301222027988642
Epoch 1708
Epoch 1708 :: Batch 0/1
Batch Loss = 0.7296883462234793
1708, epoch_train_loss=0.7296883462234793
Epoch 1709
Epoch 1709 :: Batch 0/1
Batch Loss = 0.7292539813264032
1709, epoch_train_loss=0.7292539813264032
Epoch 1710
Epoch 1710 :: Batch 0/1
Batch Loss = 0.7288194042406395
1710, epoch_train_loss=0.7288194042406395
Epoch 1711
Epoch 1711 :: Batch 0/1
Batch Loss = 0.7283850879660919
1711, epoch_train_loss=0.7283850879660919
Epoch 1712
Epoch 1712 :: Batch 0/1
Batch Loss = 0.7279514632951042
1712, epoch_train_loss=0.7279514632951042
Epoch 1713
Epoch 1713 :: Batch 0/1
Batch Loss = 0.7275187012776982
1713, epoch_train_loss=0.7275187012776982
Epoch 1714
Epoch 1714 :: Batch 0/1
Batch Loss = 0.7270866331826742
1714, epoch_train_loss=0.7270866331826742
Epoch 1715
Epoch 1715 :: Batch 0/1
Batch Loss = 0.7266548668599496
1715, epoch_train_loss=0.7266548668599496
Epoch 1716
Epoch 1716 :: Batch 0/1
Batch Loss = 0.726223023289278
1716, epoch_train_loss=0.726223023289278
Epoch 1717
Epoch 1717 :: Batch 0/1
Batch Loss = 0.7257909396399674
1717, epoch_train_loss=0.7257909396399674
Epoch 1718
Epoch 1718 :: Batch 0/1
Batch Loss = 0.7253587231995619
1718, epoch_train_loss=0.7253587231995619
Epoch 1719
Epoch 1719 :: Batch 0/1
Batch Loss = 0.7249266476506591
1719, epoch_train_loss=0.7249266476506591
Epoch 1720
Epoch 1720 :: Batch 0/1
Batch Loss = 0.724494976934964
1720, epoch_train_loss=0.724494976934964
Epoch 1721
Epoch 1721 :: Batch 0/1
Batch Loss = 0.7240638278817855
1721, epoch_train_loss=0.7240638278817855
Epoch 1722
Epoch 1722 :: Batch 0/1
Batch Loss = 0.7236331391758093
1722, epoch_train_loss=0.7236331391758093
Epoch 1723
Epoch 1723 :: Batch 0/1
Batch Loss = 0.7232027430264788
1723, epoch_train_loss=0.7232027430264788
Epoch 1724
Epoch 1724 :: Batch 0/1
Batch Loss = 0.722772482188679
1724, epoch_train_loss=0.722772482188679
Epoch 1725
Epoch 1725 :: Batch 0/1
Batch Loss = 0.7223423038666722
1725, epoch_train_loss=0.7223423038666722
Epoch 1726
Epoch 1726 :: Batch 0/1
Batch Loss = 0.7219122842190793
1726, epoch_train_loss=0.7219122842190793
Epoch 1727
Epoch 1727 :: Batch 0/1
Batch Loss = 0.721482580691413
1727, epoch_train_loss=0.721482580691413
Epoch 1728
Epoch 1728 :: Batch 0/1
Batch Loss = 0.7210533479902591
1728, epoch_train_loss=0.7210533479902591
Epoch 1729
Epoch 1729 :: Batch 0/1
Batch Loss = 0.7206246703830098
1729, epoch_train_loss=0.7206246703830098
Epoch 1730
Epoch 1730 :: Batch 0/1
Batch Loss = 0.7201965443509312
1730, epoch_train_loss=0.7201965443509312
Epoch 1731
Epoch 1731 :: Batch 0/1
Batch Loss = 0.7197689086241019
1731, epoch_train_loss=0.7197689086241019
Epoch 1732
Epoch 1732 :: Batch 0/1
Batch Loss = 0.7193416926481976
1732, epoch_train_loss=0.7193416926481976
Epoch 1733
Epoch 1733 :: Batch 0/1
Batch Loss = 0.7189148537239108
1733, epoch_train_loss=0.7189148537239108
Epoch 1734
Epoch 1734 :: Batch 0/1
Batch Loss = 0.7184883881237274
1734, epoch_train_loss=0.7184883881237274
Epoch 1735
Epoch 1735 :: Batch 0/1
Batch Loss = 0.7180623175469225
1735, epoch_train_loss=0.7180623175469225
Epoch 1736
Epoch 1736 :: Batch 0/1
Batch Loss = 0.7176366628610277
1736, epoch_train_loss=0.7176366628610277
Epoch 1737
Epoch 1737 :: Batch 0/1
Batch Loss = 0.717211421314041
1737, epoch_train_loss=0.717211421314041
Epoch 1738
Epoch 1738 :: Batch 0/1
Batch Loss = 0.7167865589632473
1738, epoch_train_loss=0.7167865589632473
Epoch 1739
Epoch 1739 :: Batch 0/1
Batch Loss = 0.7163620203102476
1739, epoch_train_loss=0.7163620203102476
Epoch 1740
Epoch 1740 :: Batch 0/1
Batch Loss = 0.7159377466771703
1740, epoch_train_loss=0.7159377466771703
Epoch 1741
Epoch 1741 :: Batch 0/1
Batch Loss = 0.715513692358308
1741, epoch_train_loss=0.715513692358308
Epoch 1742
Epoch 1742 :: Batch 0/1
Batch Loss = 0.7150898320337878
1742, epoch_train_loss=0.7150898320337878
Epoch 1743
Epoch 1743 :: Batch 0/1
Batch Loss = 0.7146661582588818
1743, epoch_train_loss=0.7146661582588818
Epoch 1744
Epoch 1744 :: Batch 0/1
Batch Loss = 0.7142426724410957
1744, epoch_train_loss=0.7142426724410957
Epoch 1745
Epoch 1745 :: Batch 0/1
Batch Loss = 0.7138193742819157
1745, epoch_train_loss=0.7138193742819157
Epoch 1746
Epoch 1746 :: Batch 0/1
Batch Loss = 0.7133962553551874
1746, epoch_train_loss=0.7133962553551874
Epoch 1747
Epoch 1747 :: Batch 0/1
Batch Loss = 0.7129732993938025
1747, epoch_train_loss=0.7129732993938025
Epoch 1748
Epoch 1748 :: Batch 0/1
Batch Loss = 0.7125504883945956
1748, epoch_train_loss=0.7125504883945956
Epoch 1749
Epoch 1749 :: Batch 0/1
Batch Loss = 0.7121278096391893
1749, epoch_train_loss=0.7121278096391893
Epoch 1750
Epoch 1750 :: Batch 0/1
Batch Loss = 0.7117052600837872
1750, epoch_train_loss=0.7117052600837872
Epoch 1751
Epoch 1751 :: Batch 0/1
Batch Loss = 0.7112828458596753
1751, epoch_train_loss=0.7112828458596753
Epoch 1752
Epoch 1752 :: Batch 0/1
Batch Loss = 0.7108605779014028
1752, epoch_train_loss=0.7108605779014028
Epoch 1753
Epoch 1753 :: Batch 0/1
Batch Loss = 0.7104384660780152
1753, epoch_train_loss=0.7104384660780152
Epoch 1754
Epoch 1754 :: Batch 0/1
Batch Loss = 0.7100165145769121
1754, epoch_train_loss=0.7100165145769121
Epoch 1755
Epoch 1755 :: Batch 0/1
Batch Loss = 0.7095947212850089
1755, epoch_train_loss=0.7095947212850089
Epoch 1756
Epoch 1756 :: Batch 0/1
Batch Loss = 0.7091730807464064
1756, epoch_train_loss=0.7091730807464064
Epoch 1757
Epoch 1757 :: Batch 0/1
Batch Loss = 0.7087515885078841
1757, epoch_train_loss=0.7087515885078841
Epoch 1758
Epoch 1758 :: Batch 0/1
Batch Loss = 0.7083302436925615
1758, epoch_train_loss=0.7083302436925615
Epoch 1759
Epoch 1759 :: Batch 0/1
Batch Loss = 0.7079090487003875
1759, epoch_train_loss=0.7079090487003875
Epoch 1760
Epoch 1760 :: Batch 0/1
Batch Loss = 0.7074880062233009
1760, epoch_train_loss=0.7074880062233009
Epoch 1761
Epoch 1761 :: Batch 0/1
Batch Loss = 0.7070671161680915
1761, epoch_train_loss=0.7070671161680915
Epoch 1762
Epoch 1762 :: Batch 0/1
Batch Loss = 0.7066463739711346
1762, epoch_train_loss=0.7066463739711346
Epoch 1763
Epoch 1763 :: Batch 0/1
Batch Loss = 0.7062257716540342
1763, epoch_train_loss=0.7062257716540342
Epoch 1764
Epoch 1764 :: Batch 0/1
Batch Loss = 0.7058053004667937
1764, epoch_train_loss=0.7058053004667937
Epoch 1765
Epoch 1765 :: Batch 0/1
Batch Loss = 0.7053849534789355
1765, epoch_train_loss=0.7053849534789355
Epoch 1766
Epoch 1766 :: Batch 0/1
Batch Loss = 0.7049647263275934
1766, epoch_train_loss=0.7049647263275934
Epoch 1767
Epoch 1767 :: Batch 0/1
Batch Loss = 0.7045446162177809
1767, epoch_train_loss=0.7045446162177809
Epoch 1768
Epoch 1768 :: Batch 0/1
Batch Loss = 0.7041246199348399
1768, epoch_train_loss=0.7041246199348399
Epoch 1769
Epoch 1769 :: Batch 0/1
Batch Loss = 0.7037047324849474
1769, epoch_train_loss=0.7037047324849474
Epoch 1770
Epoch 1770 :: Batch 0/1
Batch Loss = 0.7032849470961974
1770, epoch_train_loss=0.7032849470961974
Epoch 1771
Epoch 1771 :: Batch 0/1
Batch Loss = 0.7028652563464858
1771, epoch_train_loss=0.7028652563464858
Epoch 1772
Epoch 1772 :: Batch 0/1
Batch Loss = 0.702445653566798
1772, epoch_train_loss=0.702445653566798
Epoch 1773
Epoch 1773 :: Batch 0/1
Batch Loss = 0.7020261336936747
1773, epoch_train_loss=0.7020261336936747
Epoch 1774
Epoch 1774 :: Batch 0/1
Batch Loss = 0.7016066933133622
1774, epoch_train_loss=0.7016066933133622
Epoch 1775
Epoch 1775 :: Batch 0/1
Batch Loss = 0.7011873303048318
1775, epoch_train_loss=0.7011873303048318
Epoch 1776
Epoch 1776 :: Batch 0/1
Batch Loss = 0.7007680433276346
1776, epoch_train_loss=0.7007680433276346
Epoch 1777
Epoch 1777 :: Batch 0/1
Batch Loss = 0.7003488301365071
1777, epoch_train_loss=0.7003488301365071
Epoch 1778
Epoch 1778 :: Batch 0/1
Batch Loss = 0.6999296835955128
1778, epoch_train_loss=0.6999296835955128
Epoch 1779
Epoch 1779 :: Batch 0/1
Batch Loss = 0.6995105881571958
1779, epoch_train_loss=0.6995105881571958
Epoch 1780
Epoch 1780 :: Batch 0/1
Batch Loss = 0.699091523530599
1780, epoch_train_loss=0.699091523530599
Epoch 1781
Epoch 1781 :: Batch 0/1
Batch Loss = 0.6986724760480566
1781, epoch_train_loss=0.6986724760480566
Epoch 1782
Epoch 1782 :: Batch 0/1
Batch Loss = 0.6982534465145487
1782, epoch_train_loss=0.6982534465145487
Epoch 1783
Epoch 1783 :: Batch 0/1
Batch Loss = 0.6978344454471688
1783, epoch_train_loss=0.6978344454471688
Epoch 1784
Epoch 1784 :: Batch 0/1
Batch Loss = 0.6974154813247242
1784, epoch_train_loss=0.6974154813247242
Epoch 1785
Epoch 1785 :: Batch 0/1
Batch Loss = 0.6969965549738211
1785, epoch_train_loss=0.6969965549738211
Epoch 1786
Epoch 1786 :: Batch 0/1
Batch Loss = 0.6965776623932259
1786, epoch_train_loss=0.6965776623932259
Epoch 1787
Epoch 1787 :: Batch 0/1
Batch Loss = 0.6961587989053141
1787, epoch_train_loss=0.6961587989053141
Epoch 1788
Epoch 1788 :: Batch 0/1
Batch Loss = 0.69573996077393
1788, epoch_train_loss=0.69573996077393
Epoch 1789
Epoch 1789 :: Batch 0/1
Batch Loss = 0.6953211449267478
1789, epoch_train_loss=0.6953211449267478
Epoch 1790
Epoch 1790 :: Batch 0/1
Batch Loss = 0.6949023481494404
1790, epoch_train_loss=0.6949023481494404
Epoch 1791
Epoch 1791 :: Batch 0/1
Batch Loss = 0.6944835670021099
1791, epoch_train_loss=0.6944835670021099
Epoch 1792
Epoch 1792 :: Batch 0/1
Batch Loss = 0.6940647978896429
1792, epoch_train_loss=0.6940647978896429
Epoch 1793
Epoch 1793 :: Batch 0/1
Batch Loss = 0.6936460373534741
1793, epoch_train_loss=0.6936460373534741
Epoch 1794
Epoch 1794 :: Batch 0/1
Batch Loss = 0.6932272821546118
1794, epoch_train_loss=0.6932272821546118
Epoch 1795
Epoch 1795 :: Batch 0/1
Batch Loss = 0.6928085292752761
1795, epoch_train_loss=0.6928085292752761
Epoch 1796
Epoch 1796 :: Batch 0/1
Batch Loss = 0.6923897757857121
1796, epoch_train_loss=0.6923897757857121
Epoch 1797
Epoch 1797 :: Batch 0/1
Batch Loss = 0.6919710186817427
1797, epoch_train_loss=0.6919710186817427
Epoch 1798
Epoch 1798 :: Batch 0/1
Batch Loss = 0.6915522550420675
1798, epoch_train_loss=0.6915522550420675
Epoch 1799
Epoch 1799 :: Batch 0/1
Batch Loss = 0.6911334821417341
1799, epoch_train_loss=0.6911334821417341
Epoch 1800
Epoch 1800 :: Batch 0/1
Batch Loss = 0.6907146974638914
1800, epoch_train_loss=0.6907146974638914
Epoch 1801
Epoch 1801 :: Batch 0/1
Batch Loss = 0.6902958987041474
1801, epoch_train_loss=0.6902958987041474
Epoch 1802
Epoch 1802 :: Batch 0/1
Batch Loss = 0.6898770837968954
1802, epoch_train_loss=0.6898770837968954
Epoch 1803
Epoch 1803 :: Batch 0/1
Batch Loss = 0.6894582508524931
1803, epoch_train_loss=0.6894582508524931
Epoch 1804
Epoch 1804 :: Batch 0/1
Batch Loss = 0.6890393981736773
1804, epoch_train_loss=0.6890393981736773
Epoch 1805
Epoch 1805 :: Batch 0/1
Batch Loss = 0.6886205243428273
1805, epoch_train_loss=0.6886205243428273
Epoch 1806
Epoch 1806 :: Batch 0/1
Batch Loss = 0.68820162821158
1806, epoch_train_loss=0.68820162821158
Epoch 1807
Epoch 1807 :: Batch 0/1
Batch Loss = 0.6877827089324093
1807, epoch_train_loss=0.6877827089324093
Epoch 1808
Epoch 1808 :: Batch 0/1
Batch Loss = 0.6873637660291888
1808, epoch_train_loss=0.6873637660291888
Epoch 1809
Epoch 1809 :: Batch 0/1
Batch Loss = 0.6869447993586693
1809, epoch_train_loss=0.6869447993586693
Epoch 1810
Epoch 1810 :: Batch 0/1
Batch Loss = 0.6865258091209314
1810, epoch_train_loss=0.6865258091209314
Epoch 1811
Epoch 1811 :: Batch 0/1
Batch Loss = 0.6861067958923524
1811, epoch_train_loss=0.6861067958923524
Epoch 1812
Epoch 1812 :: Batch 0/1
Batch Loss = 0.6856877606687639
1812, epoch_train_loss=0.6856877606687639
Epoch 1813
Epoch 1813 :: Batch 0/1
Batch Loss = 0.6852687048997065
1813, epoch_train_loss=0.6852687048997065
Epoch 1814
Epoch 1814 :: Batch 0/1
Batch Loss = 0.6848496305031655
1814, epoch_train_loss=0.6848496305031655
Epoch 1815
Epoch 1815 :: Batch 0/1
Batch Loss = 0.6844305398982629
1815, epoch_train_loss=0.6844305398982629
Epoch 1816
Epoch 1816 :: Batch 0/1
Batch Loss = 0.6840114359003819
1816, epoch_train_loss=0.6840114359003819
Epoch 1817
Epoch 1817 :: Batch 0/1
Batch Loss = 0.6835923217935614
1817, epoch_train_loss=0.6835923217935614
Epoch 1818
Epoch 1818 :: Batch 0/1
Batch Loss = 0.683173201414231
1818, epoch_train_loss=0.683173201414231
Epoch 1819
Epoch 1819 :: Batch 0/1
Batch Loss = 0.6827540791040032
1819, epoch_train_loss=0.6827540791040032
Epoch 1820
Epoch 1820 :: Batch 0/1
Batch Loss = 0.6823349596213815
1820, epoch_train_loss=0.6823349596213815
Epoch 1821
Epoch 1821 :: Batch 0/1
Batch Loss = 0.6819158483427606
1821, epoch_train_loss=0.6819158483427606
Epoch 1822
Epoch 1822 :: Batch 0/1
Batch Loss = 0.6814967511144845
1822, epoch_train_loss=0.6814967511144845
Epoch 1823
Epoch 1823 :: Batch 0/1
Batch Loss = 0.6810776742688275
1823, epoch_train_loss=0.6810776742688275
Epoch 1824
Epoch 1824 :: Batch 0/1
Batch Loss = 0.680658624608337
1824, epoch_train_loss=0.680658624608337
Epoch 1825
Epoch 1825 :: Batch 0/1
Batch Loss = 0.6802396093600848
1825, epoch_train_loss=0.6802396093600848
Epoch 1826
Epoch 1826 :: Batch 0/1
Batch Loss = 0.6798206362985701
1826, epoch_train_loss=0.6798206362985701
Epoch 1827
Epoch 1827 :: Batch 0/1
Batch Loss = 0.679401713493269
1827, epoch_train_loss=0.679401713493269
Epoch 1828
Epoch 1828 :: Batch 0/1
Batch Loss = 0.6789828495203934
1828, epoch_train_loss=0.6789828495203934
Epoch 1829
Epoch 1829 :: Batch 0/1
Batch Loss = 0.6785640532976639
1829, epoch_train_loss=0.6785640532976639
Epoch 1830
Epoch 1830 :: Batch 0/1
Batch Loss = 0.6781453340512498
1830, epoch_train_loss=0.6781453340512498
Epoch 1831
Epoch 1831 :: Batch 0/1
Batch Loss = 0.6777267013842625
1831, epoch_train_loss=0.6777267013842625
Epoch 1832
Epoch 1832 :: Batch 0/1
Batch Loss = 0.677308165144154
1832, epoch_train_loss=0.677308165144154
Epoch 1833
Epoch 1833 :: Batch 0/1
Batch Loss = 0.6768897353901039
1833, epoch_train_loss=0.6768897353901039
Epoch 1834
Epoch 1834 :: Batch 0/1
Batch Loss = 0.676471422457703
1834, epoch_train_loss=0.676471422457703
Epoch 1835
Epoch 1835 :: Batch 0/1
Batch Loss = 0.6760532368169989
1835, epoch_train_loss=0.6760532368169989
Epoch 1836
Epoch 1836 :: Batch 0/1
Batch Loss = 0.675635189027972
1836, epoch_train_loss=0.675635189027972
Epoch 1837
Epoch 1837 :: Batch 0/1
Batch Loss = 0.6752172897618762
1837, epoch_train_loss=0.6752172897618762
Epoch 1838
Epoch 1838 :: Batch 0/1
Batch Loss = 0.6747995497559558
1838, epoch_train_loss=0.6747995497559558
Epoch 1839
Epoch 1839 :: Batch 0/1
Batch Loss = 0.6743819796691753
1839, epoch_train_loss=0.6743819796691753
Epoch 1840
Epoch 1840 :: Batch 0/1
Batch Loss = 0.673964590175152
1840, epoch_train_loss=0.673964590175152
Epoch 1841
Epoch 1841 :: Batch 0/1
Batch Loss = 0.6735473918217624
1841, epoch_train_loss=0.6735473918217624
Epoch 1842
Epoch 1842 :: Batch 0/1
Batch Loss = 0.6731303949941598
1842, epoch_train_loss=0.6731303949941598
Epoch 1843
Epoch 1843 :: Batch 0/1
Batch Loss = 0.6727136099819337
1843, epoch_train_loss=0.6727136099819337
Epoch 1844
Epoch 1844 :: Batch 0/1
Batch Loss = 0.6722970468156733
1844, epoch_train_loss=0.6722970468156733
Epoch 1845
Epoch 1845 :: Batch 0/1
Batch Loss = 0.6718807153427158
1845, epoch_train_loss=0.6718807153427158
Epoch 1846
Epoch 1846 :: Batch 0/1
Batch Loss = 0.6714646251722112
1846, epoch_train_loss=0.6714646251722112
Epoch 1847
Epoch 1847 :: Batch 0/1
Batch Loss = 0.6710487856564136
1847, epoch_train_loss=0.6710487856564136
Epoch 1848
Epoch 1848 :: Batch 0/1
Batch Loss = 0.6706332059076676
1848, epoch_train_loss=0.6706332059076676
Epoch 1849
Epoch 1849 :: Batch 0/1
Batch Loss = 0.6702178946841882
1849, epoch_train_loss=0.6702178946841882
Epoch 1850
Epoch 1850 :: Batch 0/1
Batch Loss = 0.6698028605133991
1850, epoch_train_loss=0.6698028605133991
Epoch 1851
Epoch 1851 :: Batch 0/1
Batch Loss = 0.6693881115518633
1851, epoch_train_loss=0.6693881115518633
Epoch 1852
Epoch 1852 :: Batch 0/1
Batch Loss = 0.6689736556835858
1852, epoch_train_loss=0.6689736556835858
Epoch 1853
Epoch 1853 :: Batch 0/1
Batch Loss = 0.6685595004887304
1853, epoch_train_loss=0.6685595004887304
Epoch 1854
Epoch 1854 :: Batch 0/1
Batch Loss = 0.6681456532495461
1854, epoch_train_loss=0.6681456532495461
Epoch 1855
Epoch 1855 :: Batch 0/1
Batch Loss = 0.6677321209598256
1855, epoch_train_loss=0.6677321209598256
Epoch 1856
Epoch 1856 :: Batch 0/1
Batch Loss = 0.6673189103377719
1856, epoch_train_loss=0.6673189103377719
Epoch 1857
Epoch 1857 :: Batch 0/1
Batch Loss = 0.6669060278420033
1857, epoch_train_loss=0.6669060278420033
Epoch 1858
Epoch 1858 :: Batch 0/1
Batch Loss = 0.6664934796900599
1858, epoch_train_loss=0.6664934796900599
Epoch 1859
Epoch 1859 :: Batch 0/1
Batch Loss = 0.6660812719115737
1859, epoch_train_loss=0.6660812719115737
Epoch 1860
Epoch 1860 :: Batch 0/1
Batch Loss = 0.6656694102358407
1860, epoch_train_loss=0.6656694102358407
Epoch 1861
Epoch 1861 :: Batch 0/1
Batch Loss = 0.6652579003438656
1861, epoch_train_loss=0.6652579003438656
Epoch 1862
Epoch 1862 :: Batch 0/1
Batch Loss = 0.66484674762163
1862, epoch_train_loss=0.66484674762163
Epoch 1863
Epoch 1863 :: Batch 0/1
Batch Loss = 0.6644359574094333
1863, epoch_train_loss=0.6644359574094333
Epoch 1864
Epoch 1864 :: Batch 0/1
Batch Loss = 0.6640255348851165
1864, epoch_train_loss=0.6640255348851165
Epoch 1865
Epoch 1865 :: Batch 0/1
Batch Loss = 0.6636154851113126
1865, epoch_train_loss=0.6636154851113126
Epoch 1866
Epoch 1866 :: Batch 0/1
Batch Loss = 0.6632058130474217
1866, epoch_train_loss=0.6632058130474217
Epoch 1867
Epoch 1867 :: Batch 0/1
Batch Loss = 0.6627965235589897
1867, epoch_train_loss=0.6627965235589897
Epoch 1868
Epoch 1868 :: Batch 0/1
Batch Loss = 0.6623876214240699
1868, epoch_train_loss=0.6623876214240699
Epoch 1869
Epoch 1869 :: Batch 0/1
Batch Loss = 0.6619791113033422
1869, epoch_train_loss=0.6619791113033422
Epoch 1870
Epoch 1870 :: Batch 0/1
Batch Loss = 0.6615709978711527
1870, epoch_train_loss=0.6615709978711527
Epoch 1871
Epoch 1871 :: Batch 0/1
Batch Loss = 0.6611632855808411
1871, epoch_train_loss=0.6611632855808411
Epoch 1872
Epoch 1872 :: Batch 0/1
Batch Loss = 0.6607559788867339
1872, epoch_train_loss=0.6607559788867339
Epoch 1873
Epoch 1873 :: Batch 0/1
Batch Loss = 0.6603490820677341
1873, epoch_train_loss=0.6603490820677341
Epoch 1874
Epoch 1874 :: Batch 0/1
Batch Loss = 0.6599425993429535
1874, epoch_train_loss=0.6599425993429535
Epoch 1875
Epoch 1875 :: Batch 0/1
Batch Loss = 0.6595365347545326
1875, epoch_train_loss=0.6595365347545326
Epoch 1876
Epoch 1876 :: Batch 0/1
Batch Loss = 0.6591308921465652
1876, epoch_train_loss=0.6591308921465652
Epoch 1877
Epoch 1877 :: Batch 0/1
Batch Loss = 0.658725675240104
1877, epoch_train_loss=0.658725675240104
Epoch 1878
Epoch 1878 :: Batch 0/1
Batch Loss = 0.658320887478101
1878, epoch_train_loss=0.658320887478101
Epoch 1879
Epoch 1879 :: Batch 0/1
Batch Loss = 0.6579165321311818
1879, epoch_train_loss=0.6579165321311818
Epoch 1880
Epoch 1880 :: Batch 0/1
Batch Loss = 0.6575126121749361
1880, epoch_train_loss=0.6575126121749361
Epoch 1881
Epoch 1881 :: Batch 0/1
Batch Loss = 0.6571091302660997
1881, epoch_train_loss=0.6571091302660997
Epoch 1882
Epoch 1882 :: Batch 0/1
Batch Loss = 0.6567060888175523
1882, epoch_train_loss=0.6567060888175523
Epoch 1883
Epoch 1883 :: Batch 0/1
Batch Loss = 0.6563034898475878
1883, epoch_train_loss=0.6563034898475878
Epoch 1884
Epoch 1884 :: Batch 0/1
Batch Loss = 0.655901335091681
1884, epoch_train_loss=0.655901335091681
Epoch 1885
Epoch 1885 :: Batch 0/1
Batch Loss = 0.6554996258573904
1885, epoch_train_loss=0.6554996258573904
Epoch 1886
Epoch 1886 :: Batch 0/1
Batch Loss = 0.655098363109776
1886, epoch_train_loss=0.655098363109776
Epoch 1887
Epoch 1887 :: Batch 0/1
Batch Loss = 0.6546975474630609
1887, epoch_train_loss=0.6546975474630609
Epoch 1888
Epoch 1888 :: Batch 0/1
Batch Loss = 0.6542971791118012
1888, epoch_train_loss=0.6542971791118012
Epoch 1889
Epoch 1889 :: Batch 0/1
Batch Loss = 0.6538972577988792
1889, epoch_train_loss=0.6538972577988792
Epoch 1890
Epoch 1890 :: Batch 0/1
Batch Loss = 0.6534977830132157
1890, epoch_train_loss=0.6534977830132157
Epoch 1891
Epoch 1891 :: Batch 0/1
Batch Loss = 0.6530987538041773
1891, epoch_train_loss=0.6530987538041773
Epoch 1892
Epoch 1892 :: Batch 0/1
Batch Loss = 0.6527001687934858
1892, epoch_train_loss=0.6527001687934858
Epoch 1893
Epoch 1893 :: Batch 0/1
Batch Loss = 0.6523020263827339
1893, epoch_train_loss=0.6523020263827339
Epoch 1894
Epoch 1894 :: Batch 0/1
Batch Loss = 0.6519043245454171
1894, epoch_train_loss=0.6519043245454171
Epoch 1895
Epoch 1895 :: Batch 0/1
Batch Loss = 0.6515070609746111
1895, epoch_train_loss=0.6515070609746111
Epoch 1896
Epoch 1896 :: Batch 0/1
Batch Loss = 0.6511102330712721
1896, epoch_train_loss=0.6511102330712721
Epoch 1897
Epoch 1897 :: Batch 0/1
Batch Loss = 0.6507138379974875
1897, epoch_train_loss=0.6507138379974875
Epoch 1898
Epoch 1898 :: Batch 0/1
Batch Loss = 0.6503178726021497
1898, epoch_train_loss=0.6503178726021497
Epoch 1899
Epoch 1899 :: Batch 0/1
Batch Loss = 0.6499223335065587
1899, epoch_train_loss=0.6499223335065587
Epoch 1900
Epoch 1900 :: Batch 0/1
Batch Loss = 0.6495272172209828
1900, epoch_train_loss=0.6495272172209828
Epoch 1901
Epoch 1901 :: Batch 0/1
Batch Loss = 0.6491325199733424
1901, epoch_train_loss=0.6491325199733424
Epoch 1902
Epoch 1902 :: Batch 0/1
Batch Loss = 0.6487389582340294
1902, epoch_train_loss=0.6487389582340294
Epoch 1903
Epoch 1903 :: Batch 0/1
Batch Loss = 0.8168832077699923
1903, epoch_train_loss=0.8168832077699923
Epoch 1904
Epoch 1904 :: Batch 0/1
Batch Loss = 0.6480548713266387
1904, epoch_train_loss=0.6480548713266387
Epoch 1905
Epoch 1905 :: Batch 0/1
Batch Loss = 0.6478963741314859
1905, epoch_train_loss=0.6478963741314859
Epoch 1906
Epoch 1906 :: Batch 0/1
Batch Loss = 0.647693313653167
1906, epoch_train_loss=0.647693313653167
Epoch 1907
Epoch 1907 :: Batch 0/1
Batch Loss = 0.6474465326651241
1907, epoch_train_loss=0.6474465326651241
Epoch 1908
Epoch 1908 :: Batch 0/1
Batch Loss = 0.6471698929338843
1908, epoch_train_loss=0.6471698929338843
Epoch 1909
Epoch 1909 :: Batch 0/1
Batch Loss = 0.6468700130224229
1909, epoch_train_loss=0.6468700130224229
Epoch 1910
Epoch 1910 :: Batch 0/1
Batch Loss = 0.6465568383771286
1910, epoch_train_loss=0.6465568383771286
Epoch 1911
Epoch 1911 :: Batch 0/1
Batch Loss = 0.6462319855366359
1911, epoch_train_loss=0.6462319855366359
Epoch 1912
Epoch 1912 :: Batch 0/1
Batch Loss = 0.6458914797416819
1912, epoch_train_loss=0.6458914797416819
Epoch 1913
Epoch 1913 :: Batch 0/1
Batch Loss = 0.6455370240729783
1913, epoch_train_loss=0.6455370240729783
Epoch 1914
Epoch 1914 :: Batch 0/1
Batch Loss = 0.6451713450976417
1914, epoch_train_loss=0.6451713450976417
Epoch 1915
Epoch 1915 :: Batch 0/1
Batch Loss = 0.6447915813321806
1915, epoch_train_loss=0.6447915813321806
Epoch 1916
Epoch 1916 :: Batch 0/1
Batch Loss = 0.6443965205066166
1916, epoch_train_loss=0.6443965205066166
Epoch 1917
Epoch 1917 :: Batch 0/1
Batch Loss = 0.6439918409143296
1917, epoch_train_loss=0.6439918409143296
Epoch 1918
Epoch 1918 :: Batch 0/1
Batch Loss = 0.6435813770472433
1918, epoch_train_loss=0.6435813770472433
Epoch 1919
Epoch 1919 :: Batch 0/1
Batch Loss = 0.643161920591761
1919, epoch_train_loss=0.643161920591761
Epoch 1920
Epoch 1920 :: Batch 0/1
Batch Loss = 0.642735105265893
1920, epoch_train_loss=0.642735105265893
Epoch 1921
Epoch 1921 :: Batch 0/1
Batch Loss = 0.6423161285568104
1921, epoch_train_loss=0.6423161285568104
Epoch 1922
Epoch 1922 :: Batch 0/1
Batch Loss = 0.6419158667324474
1922, epoch_train_loss=0.6419158667324474
Epoch 1923
Epoch 1923 :: Batch 0/1
Batch Loss = 0.6415218116608555
1923, epoch_train_loss=0.6415218116608555
Epoch 1924
Epoch 1924 :: Batch 0/1
Batch Loss = 0.6411181798537955
1924, epoch_train_loss=0.6411181798537955
Epoch 1925
Epoch 1925 :: Batch 0/1
Batch Loss = 0.6407067880884447
1925, epoch_train_loss=0.6407067880884447
Epoch 1926
Epoch 1926 :: Batch 0/1
Batch Loss = 0.6402974687256117
1926, epoch_train_loss=0.6402974687256117
Epoch 1927
Epoch 1927 :: Batch 0/1
Batch Loss = 0.6398975687156732
1927, epoch_train_loss=0.6398975687156732
Epoch 1928
Epoch 1928 :: Batch 0/1
Batch Loss = 0.6395068933108535
1928, epoch_train_loss=0.6395068933108535
Epoch 1929
Epoch 1929 :: Batch 0/1
Batch Loss = 0.6391183615206639
1929, epoch_train_loss=0.6391183615206639
Epoch 1930
Epoch 1930 :: Batch 0/1
Batch Loss = 0.6387262136472056
1930, epoch_train_loss=0.6387262136472056
Epoch 1931
Epoch 1931 :: Batch 0/1
Batch Loss = 0.6383321981616863
1931, epoch_train_loss=0.6383321981616863
Epoch 1932
Epoch 1932 :: Batch 0/1
Batch Loss = 0.637941926760299
1932, epoch_train_loss=0.637941926760299
Epoch 1933
Epoch 1933 :: Batch 0/1
Batch Loss = 0.6375561993464177
1933, epoch_train_loss=0.6375561993464177
Epoch 1934
Epoch 1934 :: Batch 0/1
Batch Loss = 0.6371699368293009
1934, epoch_train_loss=0.6371699368293009
Epoch 1935
Epoch 1935 :: Batch 0/1
Batch Loss = 0.6367797324689084
1935, epoch_train_loss=0.6367797324689084
Epoch 1936
Epoch 1936 :: Batch 0/1
Batch Loss = 0.6363868588242593
1936, epoch_train_loss=0.6363868588242593
Epoch 1937
Epoch 1937 :: Batch 0/1
Batch Loss = 0.6359928379604908
1937, epoch_train_loss=0.6359928379604908
Epoch 1938
Epoch 1938 :: Batch 0/1
Batch Loss = 0.6355987346490781
1938, epoch_train_loss=0.6355987346490781
Epoch 1939
Epoch 1939 :: Batch 0/1
Batch Loss = 0.6352072151264363
1939, epoch_train_loss=0.6352072151264363
Epoch 1940
Epoch 1940 :: Batch 0/1
Batch Loss = 0.6348195708336588
1940, epoch_train_loss=0.6348195708336588
Epoch 1941
Epoch 1941 :: Batch 0/1
Batch Loss = 0.6344332169827439
1941, epoch_train_loss=0.6344332169827439
Epoch 1942
Epoch 1942 :: Batch 0/1
Batch Loss = 0.6340451955524617
1942, epoch_train_loss=0.6340451955524617
Epoch 1943
Epoch 1943 :: Batch 0/1
Batch Loss = 0.6336551794268257
1943, epoch_train_loss=0.6336551794268257
Epoch 1944
Epoch 1944 :: Batch 0/1
Batch Loss = 0.6332639546489641
1944, epoch_train_loss=0.6332639546489641
Epoch 1945
Epoch 1945 :: Batch 0/1
Batch Loss = 0.6328720610166506
1945, epoch_train_loss=0.6328720610166506
Epoch 1946
Epoch 1946 :: Batch 0/1
Batch Loss = 0.6324806241265095
1946, epoch_train_loss=0.6324806241265095
Epoch 1947
Epoch 1947 :: Batch 0/1
Batch Loss = 0.6320909578886615
1947, epoch_train_loss=0.6320909578886615
Epoch 1948
Epoch 1948 :: Batch 0/1
Batch Loss = 0.6317029799829559
1948, epoch_train_loss=0.6317029799829559
Epoch 1949
Epoch 1949 :: Batch 0/1
Batch Loss = 0.6313156916113833
1949, epoch_train_loss=0.6313156916113833
Epoch 1950
Epoch 1950 :: Batch 0/1
Batch Loss = 0.630928618813556
1950, epoch_train_loss=0.630928618813556
Epoch 1951
Epoch 1951 :: Batch 0/1
Batch Loss = 0.6305415764290181
1951, epoch_train_loss=0.6305415764290181
Epoch 1952
Epoch 1952 :: Batch 0/1
Batch Loss = 0.6301541247562048
1952, epoch_train_loss=0.6301541247562048
Epoch 1953
Epoch 1953 :: Batch 0/1
Batch Loss = 0.6297661480867637
1953, epoch_train_loss=0.6297661480867637
Epoch 1954
Epoch 1954 :: Batch 0/1
Batch Loss = 0.6293780429315922
1954, epoch_train_loss=0.6293780429315922
Epoch 1955
Epoch 1955 :: Batch 0/1
Batch Loss = 0.6289900423355934
1955, epoch_train_loss=0.6289900423355934
Epoch 1956
Epoch 1956 :: Batch 0/1
Batch Loss = 0.6286021514465137
1956, epoch_train_loss=0.6286021514465137
Epoch 1957
Epoch 1957 :: Batch 0/1
Batch Loss = 0.6282146212839876
1957, epoch_train_loss=0.6282146212839876
Epoch 1958
Epoch 1958 :: Batch 0/1
Batch Loss = 0.6278277279831174
1958, epoch_train_loss=0.6278277279831174
Epoch 1959
Epoch 1959 :: Batch 0/1
Batch Loss = 0.6274412686102274
1959, epoch_train_loss=0.6274412686102274
Epoch 1960
Epoch 1960 :: Batch 0/1
Batch Loss = 0.6270548157762501
1960, epoch_train_loss=0.6270548157762501
Epoch 1961
Epoch 1961 :: Batch 0/1
Batch Loss = 0.6266682044136475
1961, epoch_train_loss=0.6266682044136475
Epoch 1962
Epoch 1962 :: Batch 0/1
Batch Loss = 0.6262814765300333
1962, epoch_train_loss=0.6262814765300333
Epoch 1963
Epoch 1963 :: Batch 0/1
Batch Loss = 0.6258947475943804
1963, epoch_train_loss=0.6258947475943804
Epoch 1964
Epoch 1964 :: Batch 0/1
Batch Loss = 0.6255082797944489
1964, epoch_train_loss=0.6255082797944489
Epoch 1965
Epoch 1965 :: Batch 0/1
Batch Loss = 0.6251223048514105
1965, epoch_train_loss=0.6251223048514105
Epoch 1966
Epoch 1966 :: Batch 0/1
Batch Loss = 0.6247367335995224
1966, epoch_train_loss=0.6247367335995224
Epoch 1967
Epoch 1967 :: Batch 0/1
Batch Loss = 0.6243512951955358
1967, epoch_train_loss=0.6243512951955358
Epoch 1968
Epoch 1968 :: Batch 0/1
Batch Loss = 0.6239658642489538
1968, epoch_train_loss=0.6239658642489538
Epoch 1969
Epoch 1969 :: Batch 0/1
Batch Loss = 0.6235804673294397
1969, epoch_train_loss=0.6235804673294397
Epoch 1970
Epoch 1970 :: Batch 0/1
Batch Loss = 0.6231951414564179
1970, epoch_train_loss=0.6231951414564179
Epoch 1971
Epoch 1971 :: Batch 0/1
Batch Loss = 0.6228099465328415
1971, epoch_train_loss=0.6228099465328415
Epoch 1972
Epoch 1972 :: Batch 0/1
Batch Loss = 0.6224249672191621
1972, epoch_train_loss=0.6224249672191621
Epoch 1973
Epoch 1973 :: Batch 0/1
Batch Loss = 0.6220402092446066
1973, epoch_train_loss=0.6220402092446066
Epoch 1974
Epoch 1974 :: Batch 0/1
Batch Loss = 0.6216556072184553
1974, epoch_train_loss=0.6216556072184553
Epoch 1975
Epoch 1975 :: Batch 0/1
Batch Loss = 0.6212711343010007
1975, epoch_train_loss=0.6212711343010007
Epoch 1976
Epoch 1976 :: Batch 0/1
Batch Loss = 0.6208867923982424
1976, epoch_train_loss=0.6208867923982424
Epoch 1977
Epoch 1977 :: Batch 0/1
Batch Loss = 0.6205025389580977
1977, epoch_train_loss=0.6205025389580977
Epoch 1978
Epoch 1978 :: Batch 0/1
Batch Loss = 0.6201183241646033
1978, epoch_train_loss=0.6201183241646033
Epoch 1979
Epoch 1979 :: Batch 0/1
Batch Loss = 0.6197341585561299
1979, epoch_train_loss=0.6197341585561299
Epoch 1980
Epoch 1980 :: Batch 0/1
Batch Loss = 0.6193500853786645
1980, epoch_train_loss=0.6193500853786645
Epoch 1981
Epoch 1981 :: Batch 0/1
Batch Loss = 0.6189661292927231
1981, epoch_train_loss=0.6189661292927231
Epoch 1982
Epoch 1982 :: Batch 0/1
Batch Loss = 0.6185822989234869
1982, epoch_train_loss=0.6185822989234869
Epoch 1983
Epoch 1983 :: Batch 0/1
Batch Loss = 0.6181985921890438
1983, epoch_train_loss=0.6181985921890438
Epoch 1984
Epoch 1984 :: Batch 0/1
Batch Loss = 0.6178149829151639
1984, epoch_train_loss=0.6178149829151639
Epoch 1985
Epoch 1985 :: Batch 0/1
Batch Loss = 0.6174314398400832
1985, epoch_train_loss=0.6174314398400832
Epoch 1986
Epoch 1986 :: Batch 0/1
Batch Loss = 0.6170479631697733
1986, epoch_train_loss=0.6170479631697733
Epoch 1987
Epoch 1987 :: Batch 0/1
Batch Loss = 0.616664577190361
1987, epoch_train_loss=0.616664577190361
Epoch 1988
Epoch 1988 :: Batch 0/1
Batch Loss = 0.6162812981092358
1988, epoch_train_loss=0.6162812981092358
Epoch 1989
Epoch 1989 :: Batch 0/1
Batch Loss = 0.6158981303580977
1989, epoch_train_loss=0.6158981303580977
Epoch 1990
Epoch 1990 :: Batch 0/1
Batch Loss = 0.6155150786531413
1990, epoch_train_loss=0.6155150786531413
Epoch 1991
Epoch 1991 :: Batch 0/1
Batch Loss = 0.6151321463766641
1991, epoch_train_loss=0.6151321463766641
Epoch 1992
Epoch 1992 :: Batch 0/1
Batch Loss = 0.6147493361018571
1992, epoch_train_loss=0.6147493361018571
Epoch 1993
Epoch 1993 :: Batch 0/1
Batch Loss = 0.6143666629792935
1993, epoch_train_loss=0.6143666629792935
Epoch 1994
Epoch 1994 :: Batch 0/1
Batch Loss = 0.6139841568498196
1994, epoch_train_loss=0.6139841568498196
Epoch 1995
Epoch 1995 :: Batch 0/1
Batch Loss = 0.6136018467005752
1995, epoch_train_loss=0.6136018467005752
Epoch 1996
Epoch 1996 :: Batch 0/1
Batch Loss = 0.6132197532352037
1996, epoch_train_loss=0.6132197532352037
Epoch 1997
Epoch 1997 :: Batch 0/1
Batch Loss = 0.6128378977209924
1997, epoch_train_loss=0.6128378977209924
Epoch 1998
Epoch 1998 :: Batch 0/1
Batch Loss = 0.612456309410272
1998, epoch_train_loss=0.612456309410272
Epoch 1999
Epoch 1999 :: Batch 0/1
Batch Loss = 0.6120750227410492
1999, epoch_train_loss=0.6120750227410492
Epoch 2000
Epoch 2000 :: Batch 0/1
Batch Loss = 0.6116940728257717
2000, epoch_train_loss=0.6116940728257717
Epoch 2001
Epoch 2001 :: Batch 0/1
Batch Loss = 0.6113134931670996
2001, epoch_train_loss=0.6113134931670996
Epoch 2002
Epoch 2002 :: Batch 0/1
Batch Loss = 0.6109333121994519
2002, epoch_train_loss=0.6109333121994519
Epoch 2003
Epoch 2003 :: Batch 0/1
Batch Loss = 0.6105535509917988
2003, epoch_train_loss=0.6105535509917988
Epoch 2004
Epoch 2004 :: Batch 0/1
Batch Loss = 0.6101742251022251
2004, epoch_train_loss=0.6101742251022251
Epoch 2005
Epoch 2005 :: Batch 0/1
Batch Loss = 0.6097953473224298
2005, epoch_train_loss=0.6097953473224298
Epoch 2006
Epoch 2006 :: Batch 0/1
Batch Loss = 0.6094169258579663
2006, epoch_train_loss=0.6094169258579663
Epoch 2007
Epoch 2007 :: Batch 0/1
Batch Loss = 0.6090389609020689
2007, epoch_train_loss=0.6090389609020689
Epoch 2008
Epoch 2008 :: Batch 0/1
Batch Loss = 0.6086614453323775
2008, epoch_train_loss=0.6086614453323775
Epoch 2009
Epoch 2009 :: Batch 0/1
Batch Loss = 0.6082843680253582
2009, epoch_train_loss=0.6082843680253582
Epoch 2010
Epoch 2010 :: Batch 0/1
Batch Loss = 0.6079077160220548
2010, epoch_train_loss=0.6079077160220548
Epoch 2011
Epoch 2011 :: Batch 0/1
Batch Loss = 0.6075314753013954
2011, epoch_train_loss=0.6075314753013954
Epoch 2012
Epoch 2012 :: Batch 0/1
Batch Loss = 0.6071556322594612
2012, epoch_train_loss=0.6071556322594612
Epoch 2013
Epoch 2013 :: Batch 0/1
Batch Loss = 0.6067801754707665
2013, epoch_train_loss=0.6067801754707665
Epoch 2014
Epoch 2014 :: Batch 0/1
Batch Loss = 0.6064050964768779
2014, epoch_train_loss=0.6064050964768779
Epoch 2015
Epoch 2015 :: Batch 0/1
Batch Loss = 0.6060303891115614
2015, epoch_train_loss=0.6060303891115614
Epoch 2016
Epoch 2016 :: Batch 0/1
Batch Loss = 0.6056560492124917
2016, epoch_train_loss=0.6056560492124917
Epoch 2017
Epoch 2017 :: Batch 0/1
Batch Loss = 0.6052820746622753
2017, epoch_train_loss=0.6052820746622753
Epoch 2018
Epoch 2018 :: Batch 0/1
Batch Loss = 0.6049084648924491
2018, epoch_train_loss=0.6049084648924491
Epoch 2019
Epoch 2019 :: Batch 0/1
Batch Loss = 0.6045352200902027
2019, epoch_train_loss=0.6045352200902027
Epoch 2020
Epoch 2020 :: Batch 0/1
Batch Loss = 0.6041623404571348
2020, epoch_train_loss=0.6041623404571348
Epoch 2021
Epoch 2021 :: Batch 0/1
Batch Loss = 0.6037898261013273
2021, epoch_train_loss=0.6037898261013273
Epoch 2022
Epoch 2022 :: Batch 0/1
Batch Loss = 0.6034176765627686
2022, epoch_train_loss=0.6034176765627686
Epoch 2023
Epoch 2023 :: Batch 0/1
Batch Loss = 0.6030458898081215
2023, epoch_train_loss=0.6030458898081215
Epoch 2024
Epoch 2024 :: Batch 0/1
Batch Loss = 0.6026744617379849
2024, epoch_train_loss=0.6026744617379849
Epoch 2025
Epoch 2025 :: Batch 0/1
Batch Loss = 0.6023033862615209
2025, epoch_train_loss=0.6023033862615209
Epoch 2026
Epoch 2026 :: Batch 0/1
Batch Loss = 0.6019326562379878
2026, epoch_train_loss=0.6019326562379878
Epoch 2027
Epoch 2027 :: Batch 0/1
Batch Loss = 0.6015622639051538
2027, epoch_train_loss=0.6015622639051538
Epoch 2028
Epoch 2028 :: Batch 0/1
Batch Loss = 0.601192201351468
2028, epoch_train_loss=0.601192201351468
Epoch 2029
Epoch 2029 :: Batch 0/1
Batch Loss = 0.6008224606495024
2029, epoch_train_loss=0.6008224606495024
Epoch 2030
Epoch 2030 :: Batch 0/1
Batch Loss = 0.6004530339375012
2030, epoch_train_loss=0.6004530339375012
Epoch 2031
Epoch 2031 :: Batch 0/1
Batch Loss = 0.6000839135728245
2031, epoch_train_loss=0.6000839135728245
Epoch 2032
Epoch 2032 :: Batch 0/1
Batch Loss = 0.5997150922892045
2032, epoch_train_loss=0.5997150922892045
Epoch 2033
Epoch 2033 :: Batch 0/1
Batch Loss = 0.5993465637405013
2033, epoch_train_loss=0.5993465637405013
Epoch 2034
Epoch 2034 :: Batch 0/1
Batch Loss = 0.5989783224061673
2034, epoch_train_loss=0.5989783224061673
Epoch 2035
Epoch 2035 :: Batch 0/1
Batch Loss = 0.5986103637218481
2035, epoch_train_loss=0.5986103637218481
Epoch 2036
Epoch 2036 :: Batch 0/1
Batch Loss = 0.5982426836714493
2036, epoch_train_loss=0.5982426836714493
Epoch 2037
Epoch 2037 :: Batch 0/1
Batch Loss = 0.5978752789462972
2037, epoch_train_loss=0.5978752789462972
Epoch 2038
Epoch 2038 :: Batch 0/1
Batch Loss = 0.5975081466515528
2038, epoch_train_loss=0.5975081466515528
Epoch 2039
Epoch 2039 :: Batch 0/1
Batch Loss = 0.5971412843918956
2039, epoch_train_loss=0.5971412843918956
Epoch 2040
Epoch 2040 :: Batch 0/1
Batch Loss = 0.5967746900008661
2040, epoch_train_loss=0.5967746900008661
Epoch 2041
Epoch 2041 :: Batch 0/1
Batch Loss = 0.5964083614583139
2041, epoch_train_loss=0.5964083614583139
Epoch 2042
Epoch 2042 :: Batch 0/1
Batch Loss = 0.5960422967988951
2042, epoch_train_loss=0.5960422967988951
Epoch 2043
Epoch 2043 :: Batch 0/1
Batch Loss = 0.5956764940180731
2043, epoch_train_loss=0.5956764940180731
Epoch 2044
Epoch 2044 :: Batch 0/1
Batch Loss = 0.5953109510236413
2044, epoch_train_loss=0.5953109510236413
Epoch 2045
Epoch 2045 :: Batch 0/1
Batch Loss = 0.5949456656367986
2045, epoch_train_loss=0.5949456656367986
Epoch 2046
Epoch 2046 :: Batch 0/1
Batch Loss = 0.5945806357359862
2046, epoch_train_loss=0.5945806357359862
Epoch 2047
Epoch 2047 :: Batch 0/1
Batch Loss = 0.5942158591569645
2047, epoch_train_loss=0.5942158591569645
Epoch 2048
Epoch 2048 :: Batch 0/1
Batch Loss = 0.593851333846558
2048, epoch_train_loss=0.593851333846558
Epoch 2049
Epoch 2049 :: Batch 0/1
Batch Loss = 0.5934870576967248
2049, epoch_train_loss=0.5934870576967248
Epoch 2050
Epoch 2050 :: Batch 0/1
Batch Loss = 0.5931230288151188
2050, epoch_train_loss=0.5931230288151188
Epoch 2051
Epoch 2051 :: Batch 0/1
Batch Loss = 0.5927592454461438
2051, epoch_train_loss=0.5927592454461438
Epoch 2052
Epoch 2052 :: Batch 0/1
Batch Loss = 0.5923957059834504
2052, epoch_train_loss=0.5923957059834504
Epoch 2053
Epoch 2053 :: Batch 0/1
Batch Loss = 0.5920324090729445
2053, epoch_train_loss=0.5920324090729445
Epoch 2054
Epoch 2054 :: Batch 0/1
Batch Loss = 0.591669353515895
2054, epoch_train_loss=0.591669353515895
Epoch 2055
Epoch 2055 :: Batch 0/1
Batch Loss = 0.5913065383950431
2055, epoch_train_loss=0.5913065383950431
Epoch 2056
Epoch 2056 :: Batch 0/1
Batch Loss = 0.5909439629740597
2056, epoch_train_loss=0.5909439629740597
Epoch 2057
Epoch 2057 :: Batch 0/1
Batch Loss = 0.5905816266737758
2057, epoch_train_loss=0.5905816266737758
Epoch 2058
Epoch 2058 :: Batch 0/1
Batch Loss = 0.5902195290992526
2058, epoch_train_loss=0.5902195290992526
Epoch 2059
Epoch 2059 :: Batch 0/1
Batch Loss = 0.58985767000378
2059, epoch_train_loss=0.58985767000378
Epoch 2060
Epoch 2060 :: Batch 0/1
Batch Loss = 0.5894960491754938
2060, epoch_train_loss=0.5894960491754938
Epoch 2061
Epoch 2061 :: Batch 0/1
Batch Loss = 0.5891346665497879
2061, epoch_train_loss=0.5891346665497879
Epoch 2062
Epoch 2062 :: Batch 0/1
Batch Loss = 0.588773522093877
2062, epoch_train_loss=0.588773522093877
Epoch 2063
Epoch 2063 :: Batch 0/1
Batch Loss = 0.5884126158869132
2063, epoch_train_loss=0.5884126158869132
Epoch 2064
Epoch 2064 :: Batch 0/1
Batch Loss = 0.5880519481044243
2064, epoch_train_loss=0.5880519481044243
Epoch 2065
Epoch 2065 :: Batch 0/1
Batch Loss = 0.5876915189216709
2065, epoch_train_loss=0.5876915189216709
Epoch 2066
Epoch 2066 :: Batch 0/1
Batch Loss = 0.5873313286025637
2066, epoch_train_loss=0.5873313286025637
Epoch 2067
Epoch 2067 :: Batch 0/1
Batch Loss = 0.5869713775018209
2067, epoch_train_loss=0.5869713775018209
Epoch 2068
Epoch 2068 :: Batch 0/1
Batch Loss = 0.5866116659763142
2068, epoch_train_loss=0.5866116659763142
Epoch 2069
Epoch 2069 :: Batch 0/1
Batch Loss = 0.5862521944705553
2069, epoch_train_loss=0.5862521944705553
Epoch 2070
Epoch 2070 :: Batch 0/1
Batch Loss = 0.5858929634831519
2070, epoch_train_loss=0.5858929634831519
Epoch 2071
Epoch 2071 :: Batch 0/1
Batch Loss = 0.5855339735871037
2071, epoch_train_loss=0.5855339735871037
Epoch 2072
Epoch 2072 :: Batch 0/1
Batch Loss = 0.5851752253010525
2072, epoch_train_loss=0.5851752253010525
Epoch 2073
Epoch 2073 :: Batch 0/1
Batch Loss = 0.5848167192815164
2073, epoch_train_loss=0.5848167192815164
Epoch 2074
Epoch 2074 :: Batch 0/1
Batch Loss = 0.5844584561063617
2074, epoch_train_loss=0.5844584561063617
Epoch 2075
Epoch 2075 :: Batch 0/1
Batch Loss = 0.5841004363761945
2075, epoch_train_loss=0.5841004363761945
Epoch 2076
Epoch 2076 :: Batch 0/1
Batch Loss = 0.5837426606683194
2076, epoch_train_loss=0.5837426606683194
Epoch 2077
Epoch 2077 :: Batch 0/1
Batch Loss = 0.5833851295214966
2077, epoch_train_loss=0.5833851295214966
Epoch 2078
Epoch 2078 :: Batch 0/1
Batch Loss = 0.5830278434226953
2078, epoch_train_loss=0.5830278434226953
Epoch 2079
Epoch 2079 :: Batch 0/1
Batch Loss = 0.5826708027942359
2079, epoch_train_loss=0.5826708027942359
Epoch 2080
Epoch 2080 :: Batch 0/1
Batch Loss = 0.582314007981688
2080, epoch_train_loss=0.582314007981688
Epoch 2081
Epoch 2081 :: Batch 0/1
Batch Loss = 0.5819574592442179
2081, epoch_train_loss=0.5819574592442179
Epoch 2082
Epoch 2082 :: Batch 0/1
Batch Loss = 0.581601156747353
2082, epoch_train_loss=0.581601156747353
Epoch 2083
Epoch 2083 :: Batch 0/1
Batch Loss = 0.5812451005568992
2083, epoch_train_loss=0.5812451005568992
Epoch 2084
Epoch 2084 :: Batch 0/1
Batch Loss = 0.580889290604982
2084, epoch_train_loss=0.580889290604982
Epoch 2085
Epoch 2085 :: Batch 0/1
Batch Loss = 0.5805337267734879
2085, epoch_train_loss=0.5805337267734879
Epoch 2086
Epoch 2086 :: Batch 0/1
Batch Loss = 0.5801784088340791
2086, epoch_train_loss=0.5801784088340791
Epoch 2087
Epoch 2087 :: Batch 0/1
Batch Loss = 0.579823336389054
2087, epoch_train_loss=0.579823336389054
Epoch 2088
Epoch 2088 :: Batch 0/1
Batch Loss = 0.5794685089287079
2088, epoch_train_loss=0.5794685089287079
Epoch 2089
Epoch 2089 :: Batch 0/1
Batch Loss = 0.5791139258898248
2089, epoch_train_loss=0.5791139258898248
Epoch 2090
Epoch 2090 :: Batch 0/1
Batch Loss = 0.578759586541887
2090, epoch_train_loss=0.578759586541887
Epoch 2091
Epoch 2091 :: Batch 0/1
Batch Loss = 0.5784054901039936
2091, epoch_train_loss=0.5784054901039936
Epoch 2092
Epoch 2092 :: Batch 0/1
Batch Loss = 0.5780516356599289
2092, epoch_train_loss=0.5780516356599289
Epoch 2093
Epoch 2093 :: Batch 0/1
Batch Loss = 0.5776980221892221
2093, epoch_train_loss=0.5776980221892221
Epoch 2094
Epoch 2094 :: Batch 0/1
Batch Loss = 0.5773446485413977
2094, epoch_train_loss=0.5773446485413977
Epoch 2095
Epoch 2095 :: Batch 0/1
Batch Loss = 0.5769915135257431
2095, epoch_train_loss=0.5769915135257431
Epoch 2096
Epoch 2096 :: Batch 0/1
Batch Loss = 0.5766386158003236
2096, epoch_train_loss=0.5766386158003236
Epoch 2097
Epoch 2097 :: Batch 0/1
Batch Loss = 0.5762859539913726
2097, epoch_train_loss=0.5762859539913726
Epoch 2098
Epoch 2098 :: Batch 0/1
Batch Loss = 0.5759335266117503
2098, epoch_train_loss=0.5759335266117503
Epoch 2099
Epoch 2099 :: Batch 0/1
Batch Loss = 0.5755813320942695
2099, epoch_train_loss=0.5755813320942695
Epoch 2100
Epoch 2100 :: Batch 0/1
Batch Loss = 0.5752293687677815
2100, epoch_train_loss=0.5752293687677815
Epoch 2101
Epoch 2101 :: Batch 0/1
Batch Loss = 0.574877634919502
2101, epoch_train_loss=0.574877634919502
Epoch 2102
Epoch 2102 :: Batch 0/1
Batch Loss = 0.5745261287999969
2102, epoch_train_loss=0.5745261287999969
Epoch 2103
Epoch 2103 :: Batch 0/1
Batch Loss = 0.5741748485421939
2103, epoch_train_loss=0.5741748485421939
Epoch 2104
Epoch 2104 :: Batch 0/1
Batch Loss = 0.5738237922803904
2104, epoch_train_loss=0.5738237922803904
Epoch 2105
Epoch 2105 :: Batch 0/1
Batch Loss = 0.5734729580403423
2105, epoch_train_loss=0.5734729580403423
Epoch 2106
Epoch 2106 :: Batch 0/1
Batch Loss = 0.5731223438577892
2106, epoch_train_loss=0.5731223438577892
Epoch 2107
Epoch 2107 :: Batch 0/1
Batch Loss = 0.572771947668176
2107, epoch_train_loss=0.572771947668176
Epoch 2108
Epoch 2108 :: Batch 0/1
Batch Loss = 0.57242176739596
2108, epoch_train_loss=0.57242176739596
Epoch 2109
Epoch 2109 :: Batch 0/1
Batch Loss = 0.5720718009579112
2109, epoch_train_loss=0.5720718009579112
Epoch 2110
Epoch 2110 :: Batch 0/1
Batch Loss = 0.571722046180688
2110, epoch_train_loss=0.571722046180688
Epoch 2111
Epoch 2111 :: Batch 0/1
Batch Loss = 0.5713725008890238
2111, epoch_train_loss=0.5713725008890238
Epoch 2112
Epoch 2112 :: Batch 0/1
Batch Loss = 0.5710231628795782
2112, epoch_train_loss=0.5710231628795782
Epoch 2113
Epoch 2113 :: Batch 0/1
Batch Loss = 0.5706740299229702
2113, epoch_train_loss=0.5706740299229702
Epoch 2114
Epoch 2114 :: Batch 0/1
Batch Loss = 0.5703250997653851
2114, epoch_train_loss=0.5703250997653851
Epoch 2115
Epoch 2115 :: Batch 0/1
Batch Loss = 0.5699763701580876
2115, epoch_train_loss=0.5699763701580876
Epoch 2116
Epoch 2116 :: Batch 0/1
Batch Loss = 0.5696278387728831
2116, epoch_train_loss=0.5696278387728831
Epoch 2117
Epoch 2117 :: Batch 0/1
Batch Loss = 0.5692795032874919
2117, epoch_train_loss=0.5692795032874919
Epoch 2118
Epoch 2118 :: Batch 0/1
Batch Loss = 0.5689313613566591
2118, epoch_train_loss=0.5689313613566591
Epoch 2119
Epoch 2119 :: Batch 0/1
Batch Loss = 0.568583410610935
2119, epoch_train_loss=0.568583410610935
Epoch 2120
Epoch 2120 :: Batch 0/1
Batch Loss = 0.5682356486546005
2120, epoch_train_loss=0.5682356486546005
Epoch 2121
Epoch 2121 :: Batch 0/1
Batch Loss = 0.5678880730626125
2121, epoch_train_loss=0.5678880730626125
Epoch 2122
Epoch 2122 :: Batch 0/1
Batch Loss = 0.5675406813763947
2122, epoch_train_loss=0.5675406813763947
Epoch 2123
Epoch 2123 :: Batch 0/1
Batch Loss = 0.5671934710982591
2123, epoch_train_loss=0.5671934710982591
Epoch 2124
Epoch 2124 :: Batch 0/1
Batch Loss = 0.5668464396842015
2124, epoch_train_loss=0.5668464396842015
Epoch 2125
Epoch 2125 :: Batch 0/1
Batch Loss = 0.5664995845347216
2125, epoch_train_loss=0.5664995845347216
Epoch 2126
Epoch 2126 :: Batch 0/1
Batch Loss = 0.5661529029832432
2126, epoch_train_loss=0.5661529029832432
Epoch 2127
Epoch 2127 :: Batch 0/1
Batch Loss = 0.5658063922816398
2127, epoch_train_loss=0.5658063922816398
Epoch 2128
Epoch 2128 :: Batch 0/1
Batch Loss = 0.5654600495822933
2128, epoch_train_loss=0.5654600495822933
Epoch 2129
Epoch 2129 :: Batch 0/1
Batch Loss = 0.5651138719159431
2129, epoch_train_loss=0.5651138719159431
Epoch 2130
Epoch 2130 :: Batch 0/1
Batch Loss = 0.5647678561643891
2130, epoch_train_loss=0.5647678561643891
Epoch 2131
Epoch 2131 :: Batch 0/1
Batch Loss = 0.5644219989987196
2131, epoch_train_loss=0.5644219989987196
Epoch 2132
Epoch 2132 :: Batch 0/1
Batch Loss = 0.564076296922528
2132, epoch_train_loss=0.564076296922528
Epoch 2133
Epoch 2133 :: Batch 0/1
Batch Loss = 0.5637307461364803
2133, epoch_train_loss=0.5637307461364803
Epoch 2134
Epoch 2134 :: Batch 0/1
Batch Loss = 0.5633853425038605
2134, epoch_train_loss=0.5633853425038605
Epoch 2135
Epoch 2135 :: Batch 0/1
Batch Loss = 0.5630400814733009
2135, epoch_train_loss=0.5630400814733009
Epoch 2136
Epoch 2136 :: Batch 0/1
Batch Loss = 0.5626949579833166
2136, epoch_train_loss=0.5626949579833166
Epoch 2137
Epoch 2137 :: Batch 0/1
Batch Loss = 0.5623499663160063
2137, epoch_train_loss=0.5623499663160063
Epoch 2138
Epoch 2138 :: Batch 0/1
Batch Loss = 0.5620051000347148
2138, epoch_train_loss=0.5620051000347148
Epoch 2139
Epoch 2139 :: Batch 0/1
Batch Loss = 0.5616603517171231
2139, epoch_train_loss=0.5616603517171231
Epoch 2140
Epoch 2140 :: Batch 0/1
Batch Loss = 0.5613157127275121
2140, epoch_train_loss=0.5613157127275121
Epoch 2141
Epoch 2141 :: Batch 0/1
Batch Loss = 0.5609711730159938
2141, epoch_train_loss=0.5609711730159938
Epoch 2142
Epoch 2142 :: Batch 0/1
Batch Loss = 0.5606267206485047
2142, epoch_train_loss=0.5606267206485047
Epoch 2143
Epoch 2143 :: Batch 0/1
Batch Loss = 0.5602823414413676
2143, epoch_train_loss=0.5602823414413676
Epoch 2144
Epoch 2144 :: Batch 0/1
Batch Loss = 0.559938018394879
2144, epoch_train_loss=0.559938018394879
Epoch 2145
Epoch 2145 :: Batch 0/1
Batch Loss = 0.5595937309234609
2145, epoch_train_loss=0.5595937309234609
Epoch 2146
Epoch 2146 :: Batch 0/1
Batch Loss = 0.5592494539852564
2146, epoch_train_loss=0.5592494539852564
Epoch 2147
Epoch 2147 :: Batch 0/1
Batch Loss = 0.5589051569265424
2147, epoch_train_loss=0.5589051569265424
Epoch 2148
Epoch 2148 :: Batch 0/1
Batch Loss = 0.5585608021319517
2148, epoch_train_loss=0.5585608021319517
Epoch 2149
Epoch 2149 :: Batch 0/1
Batch Loss = 0.558216343262793
2149, epoch_train_loss=0.558216343262793
Epoch 2150
Epoch 2150 :: Batch 0/1
Batch Loss = 0.5578717231862071
2150, epoch_train_loss=0.5578717231862071
Epoch 2151
Epoch 2151 :: Batch 0/1
Batch Loss = 0.5575268714238792
2151, epoch_train_loss=0.5575268714238792
Epoch 2152
Epoch 2152 :: Batch 0/1
Batch Loss = 0.557181701504119
2152, epoch_train_loss=0.557181701504119
Epoch 2153
Epoch 2153 :: Batch 0/1
Batch Loss = 0.5568361080203825
2153, epoch_train_loss=0.5568361080203825
Epoch 2154
Epoch 2154 :: Batch 0/1
Batch Loss = 0.5564899643066374
2154, epoch_train_loss=0.5564899643066374
Epoch 2155
Epoch 2155 :: Batch 0/1
Batch Loss = 0.5561431213326973
2155, epoch_train_loss=0.5561431213326973
Epoch 2156
Epoch 2156 :: Batch 0/1
Batch Loss = 0.5557954094809656
2156, epoch_train_loss=0.5557954094809656
Epoch 2157
Epoch 2157 :: Batch 0/1
Batch Loss = 0.5554466454565409
2157, epoch_train_loss=0.5554466454565409
Epoch 2158
Epoch 2158 :: Batch 0/1
Batch Loss = 0.5550966470527816
2158, epoch_train_loss=0.5550966470527816
Epoch 2159
Epoch 2159 :: Batch 0/1
Batch Loss = 0.5547452585535543
2159, epoch_train_loss=0.5547452585535543
Epoch 2160
Epoch 2160 :: Batch 0/1
Batch Loss = 0.554392387357798
2160, epoch_train_loss=0.554392387357798
Epoch 2161
Epoch 2161 :: Batch 0/1
Batch Loss = 0.5540380475101765
2161, epoch_train_loss=0.5540380475101765
Epoch 2162
Epoch 2162 :: Batch 0/1
Batch Loss = 0.5536823980087523
2162, epoch_train_loss=0.5536823980087523
Epoch 2163
Epoch 2163 :: Batch 0/1
Batch Loss = 0.55332575690135
2163, epoch_train_loss=0.55332575690135
Epoch 2164
Epoch 2164 :: Batch 0/1
Batch Loss = 0.5529685724908108
2164, epoch_train_loss=0.5529685724908108
Epoch 2165
Epoch 2165 :: Batch 0/1
Batch Loss = 0.5526113463685741
2165, epoch_train_loss=0.5526113463685741
Epoch 2166
Epoch 2166 :: Batch 0/1
Batch Loss = 0.5522545278438309
2166, epoch_train_loss=0.5522545278438309
Epoch 2167
Epoch 2167 :: Batch 0/1
Batch Loss = 0.5518984191013679
2167, epoch_train_loss=0.5518984191013679
Epoch 2168
Epoch 2168 :: Batch 0/1
Batch Loss = 0.5515431294817189
2168, epoch_train_loss=0.5515431294817189
Epoch 2169
Epoch 2169 :: Batch 0/1
Batch Loss = 0.5511885942935563
2169, epoch_train_loss=0.5511885942935563
Epoch 2170
Epoch 2170 :: Batch 0/1
Batch Loss = 0.5508346443911231
2170, epoch_train_loss=0.5508346443911231
Epoch 2171
Epoch 2171 :: Batch 0/1
Batch Loss = 0.5504810946088611
2171, epoch_train_loss=0.5504810946088611
Epoch 2172
Epoch 2172 :: Batch 0/1
Batch Loss = 0.550127817915322
2172, epoch_train_loss=0.550127817915322
Epoch 2173
Epoch 2173 :: Batch 0/1
Batch Loss = 0.5497747836525367
2173, epoch_train_loss=0.5497747836525367
Epoch 2174
Epoch 2174 :: Batch 0/1
Batch Loss = 0.5494220554729554
2174, epoch_train_loss=0.5494220554729554
Epoch 2175
Epoch 2175 :: Batch 0/1
Batch Loss = 0.5490697594654415
2175, epoch_train_loss=0.5490697594654415
Epoch 2176
Epoch 2176 :: Batch 0/1
Batch Loss = 0.5487180421422235
2176, epoch_train_loss=0.5487180421422235
Epoch 2177
Epoch 2177 :: Batch 0/1
Batch Loss = 0.5483670343215431
2177, epoch_train_loss=0.5483670343215431
Epoch 2178
Epoch 2178 :: Batch 0/1
Batch Loss = 0.5480168288971349
2178, epoch_train_loss=0.5480168288971349
Epoch 2179
Epoch 2179 :: Batch 0/1
Batch Loss = 0.5476674710820358
2179, epoch_train_loss=0.5476674710820358
Epoch 2180
Epoch 2180 :: Batch 0/1
Batch Loss = 0.5473189573617292
2180, epoch_train_loss=0.5473189573617292
Epoch 2181
Epoch 2181 :: Batch 0/1
Batch Loss = 0.5469712409049662
2181, epoch_train_loss=0.5469712409049662
Epoch 2182
Epoch 2182 :: Batch 0/1
Batch Loss = 0.5466242429785254
2182, epoch_train_loss=0.5466242429785254
Epoch 2183
Epoch 2183 :: Batch 0/1
Batch Loss = 0.546277867714424
2183, epoch_train_loss=0.546277867714424
Epoch 2184
Epoch 2184 :: Batch 0/1
Batch Loss = 0.5459320149260974
2184, epoch_train_loss=0.5459320149260974
Epoch 2185
Epoch 2185 :: Batch 0/1
Batch Loss = 0.5455865868840093
2185, epoch_train_loss=0.5455865868840093
Epoch 2186
Epoch 2186 :: Batch 0/1
Batch Loss = 0.545241488419332
2186, epoch_train_loss=0.545241488419332
Epoch 2187
Epoch 2187 :: Batch 0/1
Batch Loss = 0.5448966228682837
2187, epoch_train_loss=0.5448966228682837
Epoch 2188
Epoch 2188 :: Batch 0/1
Batch Loss = 0.5445518874895707
2188, epoch_train_loss=0.5445518874895707
Epoch 2189
Epoch 2189 :: Batch 0/1
Batch Loss = 0.5442071690838258
2189, epoch_train_loss=0.5442071690838258
Epoch 2190
Epoch 2190 :: Batch 0/1
Batch Loss = 0.543862339456728
2190, epoch_train_loss=0.543862339456728
Epoch 2191
Epoch 2191 :: Batch 0/1
Batch Loss = 0.5435172501073293
2191, epoch_train_loss=0.5435172501073293
Epoch 2192
Epoch 2192 :: Batch 0/1
Batch Loss = 0.5431717262269664
2192, epoch_train_loss=0.5431717262269664
Epoch 2193
Epoch 2193 :: Batch 0/1
Batch Loss = 0.5428255600821242
2193, epoch_train_loss=0.5428255600821242
Epoch 2194
Epoch 2194 :: Batch 0/1
Batch Loss = 0.5424785036453849
2194, epoch_train_loss=0.5424785036453849
Epoch 2195
Epoch 2195 :: Batch 0/1
Batch Loss = 0.5421302605400056
2195, epoch_train_loss=0.5421302605400056
Epoch 2196
Epoch 2196 :: Batch 0/1
Batch Loss = 0.5417804811625757
2196, epoch_train_loss=0.5417804811625757
Epoch 2197
Epoch 2197 :: Batch 0/1
Batch Loss = 0.541428767879304
2197, epoch_train_loss=0.541428767879304
Epoch 2198
Epoch 2198 :: Batch 0/1
Batch Loss = 0.5410747003882431
2198, epoch_train_loss=0.5410747003882431
Epoch 2199
Epoch 2199 :: Batch 0/1
Batch Loss = 0.5407178921185346
2199, epoch_train_loss=0.5407178921185346
Epoch 2200
Epoch 2200 :: Batch 0/1
Batch Loss = 0.5403580827290951
2200, epoch_train_loss=0.5403580827290951
Epoch 2201
Epoch 2201 :: Batch 0/1
Batch Loss = 0.5399952555778796
2201, epoch_train_loss=0.5399952555778796
Epoch 2202
Epoch 2202 :: Batch 0/1
Batch Loss = 0.5396297392094921
2202, epoch_train_loss=0.5396297392094921
Epoch 2203
Epoch 2203 :: Batch 0/1
Batch Loss = 0.5392622258126833
2203, epoch_train_loss=0.5392622258126833
Epoch 2204
Epoch 2204 :: Batch 0/1
Batch Loss = 0.5388936511032694
2204, epoch_train_loss=0.5388936511032694
Epoch 2205
Epoch 2205 :: Batch 0/1
Batch Loss = 0.5385249533503126
2205, epoch_train_loss=0.5385249533503126
Epoch 2206
Epoch 2206 :: Batch 0/1
Batch Loss = 0.5381568196535707
2206, epoch_train_loss=0.5381568196535707
Epoch 2207
Epoch 2207 :: Batch 0/1
Batch Loss = 0.5377895472281626
2207, epoch_train_loss=0.5377895472281626
Epoch 2208
Epoch 2208 :: Batch 0/1
Batch Loss = 0.5374230702628985
2208, epoch_train_loss=0.5374230702628985
Epoch 2209
Epoch 2209 :: Batch 0/1
Batch Loss = 0.5370571021951466
2209, epoch_train_loss=0.5370571021951466
Epoch 2210
Epoch 2210 :: Batch 0/1
Batch Loss = 0.5366913050401522
2210, epoch_train_loss=0.5366913050401522
Epoch 2211
Epoch 2211 :: Batch 0/1
Batch Loss = 0.5363254181840802
2211, epoch_train_loss=0.5363254181840802
Epoch 2212
Epoch 2212 :: Batch 0/1
Batch Loss = 0.5359593230093377
2212, epoch_train_loss=0.5359593230093377
Epoch 2213
Epoch 2213 :: Batch 0/1
Batch Loss = 0.535593048910587
2213, epoch_train_loss=0.535593048910587
Epoch 2214
Epoch 2214 :: Batch 0/1
Batch Loss = 0.5352267423512855
2214, epoch_train_loss=0.5352267423512855
Epoch 2215
Epoch 2215 :: Batch 0/1
Batch Loss = 0.5348606200516559
2215, epoch_train_loss=0.5348606200516559
Epoch 2216
Epoch 2216 :: Batch 0/1
Batch Loss = 0.5344949236058726
2216, epoch_train_loss=0.5344949236058726
Epoch 2217
Epoch 2217 :: Batch 0/1
Batch Loss = 0.5341298837832735
2217, epoch_train_loss=0.5341298837832735
Epoch 2218
Epoch 2218 :: Batch 0/1
Batch Loss = 0.5337656982445883
2218, epoch_train_loss=0.5337656982445883
Epoch 2219
Epoch 2219 :: Batch 0/1
Batch Loss = 0.533402522051421
2219, epoch_train_loss=0.533402522051421
Epoch 2220
Epoch 2220 :: Batch 0/1
Batch Loss = 0.5330404684224576
2220, epoch_train_loss=0.5330404684224576
Epoch 2221
Epoch 2221 :: Batch 0/1
Batch Loss = 0.5326796162335408
2221, epoch_train_loss=0.5326796162335408
Epoch 2222
Epoch 2222 :: Batch 0/1
Batch Loss = 0.5323200206463371
2222, epoch_train_loss=0.5323200206463371
Epoch 2223
Epoch 2223 :: Batch 0/1
Batch Loss = 0.5319617237356736
2223, epoch_train_loss=0.5319617237356736
Epoch 2224
Epoch 2224 :: Batch 0/1
Batch Loss = 0.5316047621767912
2224, epoch_train_loss=0.5316047621767912
Epoch 2225
Epoch 2225 :: Batch 0/1
Batch Loss = 0.5312491710520817
2225, epoch_train_loss=0.5312491710520817
Epoch 2226
Epoch 2226 :: Batch 0/1
Batch Loss = 0.5308949835087459
2226, epoch_train_loss=0.5308949835087459
Epoch 2227
Epoch 2227 :: Batch 0/1
Batch Loss = 0.5305422279304572
2227, epoch_train_loss=0.5305422279304572
Epoch 2228
Epoch 2228 :: Batch 0/1
Batch Loss = 0.5301909241938895
2228, epoch_train_loss=0.5301909241938895
Epoch 2229
Epoch 2229 :: Batch 0/1
Batch Loss = 0.5298410804767526
2229, epoch_train_loss=0.5298410804767526
Epoch 2230
Epoch 2230 :: Batch 0/1
Batch Loss = 0.5294926917085137
2230, epoch_train_loss=0.5294926917085137
Epoch 2231
Epoch 2231 :: Batch 0/1
Batch Loss = 0.529145739719346
2231, epoch_train_loss=0.529145739719346
Epoch 2232
Epoch 2232 :: Batch 0/1
Batch Loss = 0.5288001952363371
2232, epoch_train_loss=0.5288001952363371
Epoch 2233
Epoch 2233 :: Batch 0/1
Batch Loss = 0.5284560205904987
2233, epoch_train_loss=0.5284560205904987
Epoch 2234
Epoch 2234 :: Batch 0/1
Batch Loss = 0.528113172077024
2234, epoch_train_loss=0.528113172077024
Epoch 2235
Epoch 2235 :: Batch 0/1
Batch Loss = 0.5277716016802715
2235, epoch_train_loss=0.5277716016802715
Epoch 2236
Epoch 2236 :: Batch 0/1
Batch Loss = 0.5274312580112583
2236, epoch_train_loss=0.5274312580112583
Epoch 2237
Epoch 2237 :: Batch 0/1
Batch Loss = 0.5270920873107384
2237, epoch_train_loss=0.5270920873107384
Epoch 2238
Epoch 2238 :: Batch 0/1
Batch Loss = 0.5267540346997167
2238, epoch_train_loss=0.5267540346997167
Epoch 2239
Epoch 2239 :: Batch 0/1
Batch Loss = 0.5264170453245172
2239, epoch_train_loss=0.5264170453245172
Epoch 2240
Epoch 2240 :: Batch 0/1
Batch Loss = 0.5260810650978117
2240, epoch_train_loss=0.5260810650978117
Epoch 2241
Epoch 2241 :: Batch 0/1
Batch Loss = 0.5257460412922347
2241, epoch_train_loss=0.5257460412922347
Epoch 2242
Epoch 2242 :: Batch 0/1
Batch Loss = 0.5254119229520317
2242, epoch_train_loss=0.5254119229520317
Epoch 2243
Epoch 2243 :: Batch 0/1
Batch Loss = 0.5250786612884127
2243, epoch_train_loss=0.5250786612884127
Epoch 2244
Epoch 2244 :: Batch 0/1
Batch Loss = 0.5247462102830472
2244, epoch_train_loss=0.5247462102830472
Epoch 2245
Epoch 2245 :: Batch 0/1
Batch Loss = 0.5244145271368491
2245, epoch_train_loss=0.5244145271368491
Epoch 2246
Epoch 2246 :: Batch 0/1
Batch Loss = 0.5240835724717312
2246, epoch_train_loss=0.5240835724717312
Epoch 2247
Epoch 2247 :: Batch 0/1
Batch Loss = 0.5237533106712091
2247, epoch_train_loss=0.5237533106712091
Epoch 2248
Epoch 2248 :: Batch 0/1
Batch Loss = 0.5234237096409737
2248, epoch_train_loss=0.5234237096409737
Epoch 2249
Epoch 2249 :: Batch 0/1
Batch Loss = 0.5230947406638125
2249, epoch_train_loss=0.5230947406638125
Epoch 2250
Epoch 2250 :: Batch 0/1
Batch Loss = 0.5227663780414501
2250, epoch_train_loss=0.5227663780414501
Epoch 2251
Epoch 2251 :: Batch 0/1
Batch Loss = 0.522438598863956
2251, epoch_train_loss=0.522438598863956
Epoch 2252
Epoch 2252 :: Batch 0/1
Batch Loss = 0.5221113828889298
2252, epoch_train_loss=0.5221113828889298
Epoch 2253
Epoch 2253 :: Batch 0/1
Batch Loss = 0.5217847118655993
2253, epoch_train_loss=0.5217847118655993
Epoch 2254
Epoch 2254 :: Batch 0/1
Batch Loss = 0.5214585673602716
2254, epoch_train_loss=0.5214585673602716
Epoch 2255
Epoch 2255 :: Batch 0/1
Batch Loss = 0.5211329238230519
2255, epoch_train_loss=0.5211329238230519
Epoch 2256
Epoch 2256 :: Batch 0/1
Batch Loss = 0.5208077311183397
2256, epoch_train_loss=0.5208077311183397
Epoch 2257
Epoch 2257 :: Batch 0/1
Batch Loss = 0.520482867930271
2257, epoch_train_loss=0.520482867930271
Epoch 2258
Epoch 2258 :: Batch 0/1
Batch Loss = 0.5201580239505978
2258, epoch_train_loss=0.5201580239505978
Epoch 2259
Epoch 2259 :: Batch 0/1
Batch Loss = 0.5198325365435479
2259, epoch_train_loss=0.5198325365435479
Epoch 2260
Epoch 2260 :: Batch 0/1
Batch Loss = 0.5195056138162417
2260, epoch_train_loss=0.5195056138162417
Epoch 2261
Epoch 2261 :: Batch 0/1
Batch Loss = 0.5191783814313552
2261, epoch_train_loss=0.5191783814313552
Epoch 2262
Epoch 2262 :: Batch 0/1
Batch Loss = 0.5188561401872441
2262, epoch_train_loss=0.5188561401872441
Epoch 2263
Epoch 2263 :: Batch 0/1
Batch Loss = 0.5185394294459239
2263, epoch_train_loss=0.5185394294459239
Epoch 2264
Epoch 2264 :: Batch 0/1
Batch Loss = 0.5182231575261588
2264, epoch_train_loss=0.5182231575261588
Epoch 2265
Epoch 2265 :: Batch 0/1
Batch Loss = 0.5179058109642246
2265, epoch_train_loss=0.5179058109642246
Epoch 2266
Epoch 2266 :: Batch 0/1
Batch Loss = 0.5175877944306824
2266, epoch_train_loss=0.5175877944306824
Epoch 2267
Epoch 2267 :: Batch 0/1
Batch Loss = 0.517269530191504
2267, epoch_train_loss=0.517269530191504
Epoch 2268
Epoch 2268 :: Batch 0/1
Batch Loss = 0.516951231321983
2268, epoch_train_loss=0.516951231321983
Epoch 2269
Epoch 2269 :: Batch 0/1
Batch Loss = 0.516632969860322
2269, epoch_train_loss=0.516632969860322
Epoch 2270
Epoch 2270 :: Batch 0/1
Batch Loss = 0.5163147247178718
2270, epoch_train_loss=0.5163147247178718
Epoch 2271
Epoch 2271 :: Batch 0/1
Batch Loss = 0.5159963899952293
2271, epoch_train_loss=0.5159963899952293
Epoch 2272
Epoch 2272 :: Batch 0/1
Batch Loss = 0.515677794420936
2272, epoch_train_loss=0.515677794420936
Epoch 2273
Epoch 2273 :: Batch 0/1
Batch Loss = 0.5153588338146604
2273, epoch_train_loss=0.5153588338146604
Epoch 2274
Epoch 2274 :: Batch 0/1
Batch Loss = 0.5150398855833264
2274, epoch_train_loss=0.5150398855833264
Epoch 2275
Epoch 2275 :: Batch 0/1
Batch Loss = 0.5147222508511552
2275, epoch_train_loss=0.5147222508511552
Epoch 2276
Epoch 2276 :: Batch 0/1
Batch Loss = 0.5144070304812889
2276, epoch_train_loss=0.5144070304812889
Epoch 2277
Epoch 2277 :: Batch 0/1
Batch Loss = 0.5140932870030962
2277, epoch_train_loss=0.5140932870030962
Epoch 2278
Epoch 2278 :: Batch 0/1
Batch Loss = 0.513779358209053
2278, epoch_train_loss=0.513779358209053
Epoch 2279
Epoch 2279 :: Batch 0/1
Batch Loss = 0.5134645025314906
2279, epoch_train_loss=0.5134645025314906
Epoch 2280
Epoch 2280 :: Batch 0/1
Batch Loss = 0.5131487722297924
2280, epoch_train_loss=0.5131487722297924
Epoch 2281
Epoch 2281 :: Batch 0/1
Batch Loss = 0.5128327360741686
2281, epoch_train_loss=0.5128327360741686
Epoch 2282
Epoch 2282 :: Batch 0/1
Batch Loss = 0.51251733642842
2282, epoch_train_loss=0.51251733642842
Epoch 2283
Epoch 2283 :: Batch 0/1
Batch Loss = 0.5122032380122352
2283, epoch_train_loss=0.5122032380122352
Epoch 2284
Epoch 2284 :: Batch 0/1
Batch Loss = 0.5118901786315281
2284, epoch_train_loss=0.5118901786315281
Epoch 2285
Epoch 2285 :: Batch 0/1
Batch Loss = 0.5115774281336855
2285, epoch_train_loss=0.5115774281336855
Epoch 2286
Epoch 2286 :: Batch 0/1
Batch Loss = 0.5112645025354867
2286, epoch_train_loss=0.5112645025354867
Epoch 2287
Epoch 2287 :: Batch 0/1
Batch Loss = 0.5109512809877585
2287, epoch_train_loss=0.5109512809877585
Epoch 2288
Epoch 2288 :: Batch 0/1
Batch Loss = 0.5106378912155504
2288, epoch_train_loss=0.5106378912155504
Epoch 2289
Epoch 2289 :: Batch 0/1
Batch Loss = 0.5103246400714945
2289, epoch_train_loss=0.5103246400714945
Epoch 2290
Epoch 2290 :: Batch 0/1
Batch Loss = 0.5100118989807448
2290, epoch_train_loss=0.5100118989807448
Epoch 2291
Epoch 2291 :: Batch 0/1
Batch Loss = 0.5096998487774147
2291, epoch_train_loss=0.5096998487774147
Epoch 2292
Epoch 2292 :: Batch 0/1
Batch Loss = 0.5093883080484515
2292, epoch_train_loss=0.5093883080484515
Epoch 2293
Epoch 2293 :: Batch 0/1
Batch Loss = 0.5090769095960548
2293, epoch_train_loss=0.5090769095960548
Epoch 2294
Epoch 2294 :: Batch 0/1
Batch Loss = 0.5087654122642885
2294, epoch_train_loss=0.5087654122642885
Epoch 2295
Epoch 2295 :: Batch 0/1
Batch Loss = 0.5084538342115097
2295, epoch_train_loss=0.5084538342115097
Epoch 2296
Epoch 2296 :: Batch 0/1
Batch Loss = 0.5081423851436218
2296, epoch_train_loss=0.5081423851436218
Epoch 2297
Epoch 2297 :: Batch 0/1
Batch Loss = 0.5078312878661544
2297, epoch_train_loss=0.5078312878661544
Epoch 2298
Epoch 2298 :: Batch 0/1
Batch Loss = 0.5075206100438482
2298, epoch_train_loss=0.5075206100438482
Epoch 2299
Epoch 2299 :: Batch 0/1
Batch Loss = 0.5072102440044591
2299, epoch_train_loss=0.5072102440044591
Epoch 2300
Epoch 2300 :: Batch 0/1
Batch Loss = 0.5069000255715148
2300, epoch_train_loss=0.5069000255715148
Epoch 2301
Epoch 2301 :: Batch 0/1
Batch Loss = 0.5065898498210402
2301, epoch_train_loss=0.5065898498210402
Epoch 2302
Epoch 2302 :: Batch 0/1
Batch Loss = 0.5062797095061892
2302, epoch_train_loss=0.5062797095061892
Epoch 2303
Epoch 2303 :: Batch 0/1
Batch Loss = 0.5059696741786691
2303, epoch_train_loss=0.5059696741786691
Epoch 2304
Epoch 2304 :: Batch 0/1
Batch Loss = 0.5056598347518348
2304, epoch_train_loss=0.5056598347518348
Epoch 2305
Epoch 2305 :: Batch 0/1
Batch Loss = 0.5053502355196718
2305, epoch_train_loss=0.5053502355196718
Epoch 2306
Epoch 2306 :: Batch 0/1
Batch Loss = 0.5050408375091737
2306, epoch_train_loss=0.5050408375091737
Epoch 2307
Epoch 2307 :: Batch 0/1
Batch Loss = 0.5047315467293871
2307, epoch_train_loss=0.5047315467293871
Epoch 2308
Epoch 2308 :: Batch 0/1
Batch Loss = 0.5044222808219972
2308, epoch_train_loss=0.5044222808219972
Epoch 2309
Epoch 2309 :: Batch 0/1
Batch Loss = 0.5041130143823783
2309, epoch_train_loss=0.5041130143823783
Epoch 2310
Epoch 2310 :: Batch 0/1
Batch Loss = 0.5038037729997973
2310, epoch_train_loss=0.5038037729997973
Epoch 2311
Epoch 2311 :: Batch 0/1
Batch Loss = 0.503494590563721
2311, epoch_train_loss=0.503494590563721
Epoch 2312
Epoch 2312 :: Batch 0/1
Batch Loss = 0.5031854665843734
2312, epoch_train_loss=0.5031854665843734
Epoch 2313
Epoch 2313 :: Batch 0/1
Batch Loss = 0.5028763535915002
2313, epoch_train_loss=0.5028763535915002
Epoch 2314
Epoch 2314 :: Batch 0/1
Batch Loss = 0.5025671766123875
2314, epoch_train_loss=0.5025671766123875
Epoch 2315
Epoch 2315 :: Batch 0/1
Batch Loss = 0.5022578619457725
2315, epoch_train_loss=0.5022578619457725
Epoch 2316
Epoch 2316 :: Batch 0/1
Batch Loss = 0.5019483528271428
2316, epoch_train_loss=0.5019483528271428
Epoch 2317
Epoch 2317 :: Batch 0/1
Batch Loss = 0.5016386052468879
2317, epoch_train_loss=0.5016386052468879
Epoch 2318
Epoch 2318 :: Batch 0/1
Batch Loss = 0.5013285685566528
2318, epoch_train_loss=0.5013285685566528
Epoch 2319
Epoch 2319 :: Batch 0/1
Batch Loss = 0.5010181629001506
2319, epoch_train_loss=0.5010181629001506
Epoch 2320
Epoch 2320 :: Batch 0/1
Batch Loss = 0.5007072655246532
2320, epoch_train_loss=0.5007072655246532
Epoch 2321
Epoch 2321 :: Batch 0/1
Batch Loss = 0.5003957107304356
2321, epoch_train_loss=0.5003957107304356
Epoch 2322
Epoch 2322 :: Batch 0/1
Batch Loss = 0.5000832955941327
2322, epoch_train_loss=0.5000832955941327
Epoch 2323
Epoch 2323 :: Batch 0/1
Batch Loss = 0.499769778285845
2323, epoch_train_loss=0.499769778285845
Epoch 2324
Epoch 2324 :: Batch 0/1
Batch Loss = 0.4994548602969187
2324, epoch_train_loss=0.4994548602969187
Epoch 2325
Epoch 2325 :: Batch 0/1
Batch Loss = 0.49913815312548154
2325, epoch_train_loss=0.49913815312548154
Epoch 2326
Epoch 2326 :: Batch 0/1
Batch Loss = 0.4988191359868982
2326, epoch_train_loss=0.4988191359868982
Epoch 2327
Epoch 2327 :: Batch 0/1
Batch Loss = 0.49849710931208613
2327, epoch_train_loss=0.49849710931208613
Epoch 2328
Epoch 2328 :: Batch 0/1
Batch Loss = 0.4981711428886889
2328, epoch_train_loss=0.4981711428886889
Epoch 2329
Epoch 2329 :: Batch 0/1
Batch Loss = 0.49784001313390513
2329, epoch_train_loss=0.49784001313390513
Epoch 2330
Epoch 2330 :: Batch 0/1
Batch Loss = 0.49750212700514973
2330, epoch_train_loss=0.49750212700514973
Epoch 2331
Epoch 2331 :: Batch 0/1
Batch Loss = 0.49715544318480603
2331, epoch_train_loss=0.49715544318480603
Epoch 2332
Epoch 2332 :: Batch 0/1
Batch Loss = 0.49679742537340055
2332, epoch_train_loss=0.49679742537340055
Epoch 2333
Epoch 2333 :: Batch 0/1
Batch Loss = 0.4964250932542047
2333, epoch_train_loss=0.4964250932542047
Epoch 2334
Epoch 2334 :: Batch 0/1
Batch Loss = 0.49603524672386007
2334, epoch_train_loss=0.49603524672386007
Epoch 2335
Epoch 2335 :: Batch 0/1
Batch Loss = 0.4956248297423229
2335, epoch_train_loss=0.4956248297423229
Epoch 2336
Epoch 2336 :: Batch 0/1
Batch Loss = 0.495190968628605
2336, epoch_train_loss=0.495190968628605
Epoch 2337
Epoch 2337 :: Batch 0/1
Batch Loss = 0.49472936608937607
2337, epoch_train_loss=0.49472936608937607
Epoch 2338
Epoch 2338 :: Batch 0/1
Batch Loss = 0.4942289968850585
2338, epoch_train_loss=0.4942289968850585
Epoch 2339
Epoch 2339 :: Batch 0/1
Batch Loss = 0.4936615581382134
2339, epoch_train_loss=0.4936615581382134
Epoch 2340
Epoch 2340 :: Batch 0/1
Batch Loss = 0.49296764611444044
2340, epoch_train_loss=0.49296764611444044
Epoch 2341
Epoch 2341 :: Batch 0/1
Batch Loss = 0.4920595370583521
2341, epoch_train_loss=0.4920595370583521
Epoch 2342
Epoch 2342 :: Batch 0/1
Batch Loss = 0.4908952481046486
2342, epoch_train_loss=0.4908952481046486
Epoch 2343
Epoch 2343 :: Batch 0/1
Batch Loss = 0.48953255932973594
2343, epoch_train_loss=0.48953255932973594
Epoch 2344
Epoch 2344 :: Batch 0/1
Batch Loss = 0.4878304632115839
2344, epoch_train_loss=0.4878304632115839
Epoch 2345
Epoch 2345 :: Batch 0/1
Batch Loss = 0.48629396129901087
2345, epoch_train_loss=0.48629396129901087
Epoch 2346
Epoch 2346 :: Batch 0/1
Batch Loss = 0.4859808751691351
2346, epoch_train_loss=0.4859808751691351
Epoch 2347
Epoch 2347 :: Batch 0/1
Batch Loss = 0.48643396488952334
2347, epoch_train_loss=0.48643396488952334
Epoch 2348
Epoch 2348 :: Batch 0/1
Batch Loss = 0.4867115877804318
2348, epoch_train_loss=0.4867115877804318
Epoch 2349
Epoch 2349 :: Batch 0/1
Batch Loss = 0.48642739709331145
2349, epoch_train_loss=0.48642739709331145
Epoch 2350
Epoch 2350 :: Batch 0/1
Batch Loss = 0.4856062907372263
2350, epoch_train_loss=0.4856062907372263
Epoch 2351
Epoch 2351 :: Batch 0/1
Batch Loss = 0.4845188546423074
2351, epoch_train_loss=0.4845188546423074
Epoch 2352
Epoch 2352 :: Batch 0/1
Batch Loss = 0.4832791484947191
2352, epoch_train_loss=0.4832791484947191
Epoch 2353
Epoch 2353 :: Batch 0/1
Batch Loss = 0.4823152046557923
2353, epoch_train_loss=0.4823152046557923
Epoch 2354
Epoch 2354 :: Batch 0/1
Batch Loss = 0.48205990211722605
2354, epoch_train_loss=0.48205990211722605
Epoch 2355
Epoch 2355 :: Batch 0/1
Batch Loss = 0.4821022272423426
2355, epoch_train_loss=0.4821022272423426
Epoch 2356
Epoch 2356 :: Batch 0/1
Batch Loss = 0.4819626233536019
2356, epoch_train_loss=0.4819626233536019
Epoch 2357
Epoch 2357 :: Batch 0/1
Batch Loss = 0.4814984988914252
2357, epoch_train_loss=0.4814984988914252
Epoch 2358
Epoch 2358 :: Batch 0/1
Batch Loss = 0.4808170853987788
2358, epoch_train_loss=0.4808170853987788
Epoch 2359
Epoch 2359 :: Batch 0/1
Batch Loss = 0.4801207996712383
2359, epoch_train_loss=0.4801207996712383
Epoch 2360
Epoch 2360 :: Batch 0/1
Batch Loss = 0.4794909005568938
2360, epoch_train_loss=0.4794909005568938
Epoch 2361
Epoch 2361 :: Batch 0/1
Batch Loss = 0.47900929878701015
2361, epoch_train_loss=0.47900929878701015
Epoch 2362
Epoch 2362 :: Batch 0/1
Batch Loss = 0.47876324440606555
2362, epoch_train_loss=0.47876324440606555
Epoch 2363
Epoch 2363 :: Batch 0/1
Batch Loss = 0.47861975242681964
2363, epoch_train_loss=0.47861975242681964
Epoch 2364
Epoch 2364 :: Batch 0/1
Batch Loss = 0.4783645748495336
2364, epoch_train_loss=0.4783645748495336
Epoch 2365
Epoch 2365 :: Batch 0/1
Batch Loss = 0.4779527270485941
2365, epoch_train_loss=0.4779527270485941
Epoch 2366
Epoch 2366 :: Batch 0/1
Batch Loss = 0.47745153559986403
2366, epoch_train_loss=0.47745153559986403
Epoch 2367
Epoch 2367 :: Batch 0/1
Batch Loss = 0.4769773038655667
2367, epoch_train_loss=0.4769773038655667
Epoch 2368
Epoch 2368 :: Batch 0/1
Batch Loss = 0.4766090259305642
2368, epoch_train_loss=0.4766090259305642
Epoch 2369
Epoch 2369 :: Batch 0/1
Batch Loss = 0.4763110526162381
2369, epoch_train_loss=0.4763110526162381
Epoch 2370
Epoch 2370 :: Batch 0/1
Batch Loss = 0.47607488687485044
2370, epoch_train_loss=0.47607488687485044
Epoch 2371
Epoch 2371 :: Batch 0/1
Batch Loss = 0.4758325976399181
2371, epoch_train_loss=0.4758325976399181
Epoch 2372
Epoch 2372 :: Batch 0/1
Batch Loss = 0.47551854016853806
2372, epoch_train_loss=0.47551854016853806
Epoch 2373
Epoch 2373 :: Batch 0/1
Batch Loss = 0.475154860457199
2373, epoch_train_loss=0.475154860457199
Epoch 2374
Epoch 2374 :: Batch 0/1
Batch Loss = 0.47477903313065845
2374, epoch_train_loss=0.47477903313065845
Epoch 2375
Epoch 2375 :: Batch 0/1
Batch Loss = 0.4744366187912678
2375, epoch_train_loss=0.4744366187912678
Epoch 2376
Epoch 2376 :: Batch 0/1
Batch Loss = 0.47414258295387796
2376, epoch_train_loss=0.47414258295387796
Epoch 2377
Epoch 2377 :: Batch 0/1
Batch Loss = 0.4738767549098944
2377, epoch_train_loss=0.4738767549098944
Epoch 2378
Epoch 2378 :: Batch 0/1
Batch Loss = 0.47362240712971754
2378, epoch_train_loss=0.47362240712971754
Epoch 2379
Epoch 2379 :: Batch 0/1
Batch Loss = 0.4733548870419699
2379, epoch_train_loss=0.4733548870419699
Epoch 2380
Epoch 2380 :: Batch 0/1
Batch Loss = 0.47305204278330065
2380, epoch_train_loss=0.47305204278330065
Epoch 2381
Epoch 2381 :: Batch 0/1
Batch Loss = 0.47272877545807596
2381, epoch_train_loss=0.47272877545807596
Epoch 2382
Epoch 2382 :: Batch 0/1
Batch Loss = 0.47241362554069055
2382, epoch_train_loss=0.47241362554069055
Epoch 2383
Epoch 2383 :: Batch 0/1
Batch Loss = 0.4721242720298267
2383, epoch_train_loss=0.4721242720298267
Epoch 2384
Epoch 2384 :: Batch 0/1
Batch Loss = 0.4718579756887659
2384, epoch_train_loss=0.4718579756887659
Epoch 2385
Epoch 2385 :: Batch 0/1
Batch Loss = 0.4715931564572601
2385, epoch_train_loss=0.4715931564572601
Epoch 2386
Epoch 2386 :: Batch 0/1
Batch Loss = 0.4713205135333633
2386, epoch_train_loss=0.4713205135333633
Epoch 2387
Epoch 2387 :: Batch 0/1
Batch Loss = 0.4710399261287654
2387, epoch_train_loss=0.4710399261287654
Epoch 2388
Epoch 2388 :: Batch 0/1
Batch Loss = 0.4707490195141335
2388, epoch_train_loss=0.4707490195141335
Epoch 2389
Epoch 2389 :: Batch 0/1
Batch Loss = 0.4704546261069707
2389, epoch_train_loss=0.4704546261069707
Epoch 2390
Epoch 2390 :: Batch 0/1
Batch Loss = 0.4701680200345446
2390, epoch_train_loss=0.4701680200345446
Epoch 2391
Epoch 2391 :: Batch 0/1
Batch Loss = 0.46989439798263605
2391, epoch_train_loss=0.46989439798263605
Epoch 2392
Epoch 2392 :: Batch 0/1
Batch Loss = 0.46962934178769006
2392, epoch_train_loss=0.46962934178769006
Epoch 2393
Epoch 2393 :: Batch 0/1
Batch Loss = 0.46936079131245745
2393, epoch_train_loss=0.46936079131245745
Epoch 2394
Epoch 2394 :: Batch 0/1
Batch Loss = 0.46908356716721783
2394, epoch_train_loss=0.46908356716721783
Epoch 2395
Epoch 2395 :: Batch 0/1
Batch Loss = 0.46880291791017964
2395, epoch_train_loss=0.46880291791017964
Epoch 2396
Epoch 2396 :: Batch 0/1
Batch Loss = 0.4685231519400016
2396, epoch_train_loss=0.4685231519400016
Epoch 2397
Epoch 2397 :: Batch 0/1
Batch Loss = 0.46824647885530957
2397, epoch_train_loss=0.46824647885530957
Epoch 2398
Epoch 2398 :: Batch 0/1
Batch Loss = 0.46797441138812024
2398, epoch_train_loss=0.46797441138812024
Epoch 2399
Epoch 2399 :: Batch 0/1
Batch Loss = 0.4677058648937364
2399, epoch_train_loss=0.4677058648937364
Epoch 2400
Epoch 2400 :: Batch 0/1
Batch Loss = 0.4674379733580016
2400, epoch_train_loss=0.4674379733580016
Epoch 2401
Epoch 2401 :: Batch 0/1
Batch Loss = 0.46716696764146576
2401, epoch_train_loss=0.46716696764146576
Epoch 2402
Epoch 2402 :: Batch 0/1
Batch Loss = 0.4668917756467676
2402, epoch_train_loss=0.4668917756467676
Epoch 2403
Epoch 2403 :: Batch 0/1
Batch Loss = 0.4666161053794037
2403, epoch_train_loss=0.4666161053794037
Epoch 2404
Epoch 2404 :: Batch 0/1
Batch Loss = 0.4663431560347604
2404, epoch_train_loss=0.4663431560347604
Epoch 2405
Epoch 2405 :: Batch 0/1
Batch Loss = 0.46607282524294313
2405, epoch_train_loss=0.46607282524294313
Epoch 2406
Epoch 2406 :: Batch 0/1
Batch Loss = 0.4658036294886369
2406, epoch_train_loss=0.4658036294886369
Epoch 2407
Epoch 2407 :: Batch 0/1
Batch Loss = 0.4655337236729873
2407, epoch_train_loss=0.4655337236729873
Epoch 2408
Epoch 2408 :: Batch 0/1
Batch Loss = 0.4652623361501541
2408, epoch_train_loss=0.4652623361501541
Epoch 2409
Epoch 2409 :: Batch 0/1
Batch Loss = 0.46498967107497063
2409, epoch_train_loss=0.46498967107497063
Epoch 2410
Epoch 2410 :: Batch 0/1
Batch Loss = 0.4647163182328032
2410, epoch_train_loss=0.4647163182328032
Epoch 2411
Epoch 2411 :: Batch 0/1
Batch Loss = 0.46444380286445547
2411, epoch_train_loss=0.46444380286445547
Epoch 2412
Epoch 2412 :: Batch 0/1
Batch Loss = 0.46417284636667105
2412, epoch_train_loss=0.46417284636667105
Epoch 2413
Epoch 2413 :: Batch 0/1
Batch Loss = 0.4639025743778256
2413, epoch_train_loss=0.4639025743778256
Epoch 2414
Epoch 2414 :: Batch 0/1
Batch Loss = 0.4636319002358139
2414, epoch_train_loss=0.4636319002358139
Epoch 2415
Epoch 2415 :: Batch 0/1
Batch Loss = 0.4633601707785313
2415, epoch_train_loss=0.4633601707785313
Epoch 2416
Epoch 2416 :: Batch 0/1
Batch Loss = 0.4630877562270965
2416, epoch_train_loss=0.4630877562270965
Epoch 2417
Epoch 2417 :: Batch 0/1
Batch Loss = 0.46281548700065717
2417, epoch_train_loss=0.46281548700065717
Epoch 2418
Epoch 2418 :: Batch 0/1
Batch Loss = 0.46254373551639005
2418, epoch_train_loss=0.46254373551639005
Epoch 2419
Epoch 2419 :: Batch 0/1
Batch Loss = 0.4622726419607284
2419, epoch_train_loss=0.4622726419607284
Epoch 2420
Epoch 2420 :: Batch 0/1
Batch Loss = 0.4620019039186807
2420, epoch_train_loss=0.4620019039186807
Epoch 2421
Epoch 2421 :: Batch 0/1
Batch Loss = 0.4617310039529862
2421, epoch_train_loss=0.4617310039529862
Epoch 2422
Epoch 2422 :: Batch 0/1
Batch Loss = 0.46145976779121806
2422, epoch_train_loss=0.46145976779121806
Epoch 2423
Epoch 2423 :: Batch 0/1
Batch Loss = 0.46118828535988665
2423, epoch_train_loss=0.46118828535988665
Epoch 2424
Epoch 2424 :: Batch 0/1
Batch Loss = 0.4609169291454851
2424, epoch_train_loss=0.4609169291454851
Epoch 2425
Epoch 2425 :: Batch 0/1
Batch Loss = 0.46064601457687854
2425, epoch_train_loss=0.46064601457687854
Epoch 2426
Epoch 2426 :: Batch 0/1
Batch Loss = 0.46037549163492375
2426, epoch_train_loss=0.46037549163492375
Epoch 2427
Epoch 2427 :: Batch 0/1
Batch Loss = 0.4601051771425995
2427, epoch_train_loss=0.4601051771425995
Epoch 2428
Epoch 2428 :: Batch 0/1
Batch Loss = 0.45983483220591653
2428, epoch_train_loss=0.45983483220591653
Epoch 2429
Epoch 2429 :: Batch 0/1
Batch Loss = 0.4595643689767026
2429, epoch_train_loss=0.4595643689767026
Epoch 2430
Epoch 2430 :: Batch 0/1
Batch Loss = 0.4592939362663282
2430, epoch_train_loss=0.4592939362663282
Epoch 2431
Epoch 2431 :: Batch 0/1
Batch Loss = 0.45902367280686013
2431, epoch_train_loss=0.45902367280686013
Epoch 2432
Epoch 2432 :: Batch 0/1
Batch Loss = 0.4587536697860029
2432, epoch_train_loss=0.4587536697860029
Epoch 2433
Epoch 2433 :: Batch 0/1
Batch Loss = 0.4584839152154786
2433, epoch_train_loss=0.4584839152154786
Epoch 2434
Epoch 2434 :: Batch 0/1
Batch Loss = 0.4582143157500598
2434, epoch_train_loss=0.4582143157500598
Epoch 2435
Epoch 2435 :: Batch 0/1
Batch Loss = 0.4579448085218306
2435, epoch_train_loss=0.4579448085218306
Epoch 2436
Epoch 2436 :: Batch 0/1
Batch Loss = 0.45767537704435385
2436, epoch_train_loss=0.45767537704435385
Epoch 2437
Epoch 2437 :: Batch 0/1
Batch Loss = 0.4574087423512058
2437, epoch_train_loss=0.4574087423512058
Epoch 2438
Epoch 2438 :: Batch 0/1
Batch Loss = 0.4571808571511631
2438, epoch_train_loss=0.4571808571511631
Epoch 2439
Epoch 2439 :: Batch 0/1
Batch Loss = 0.457006118786518
2439, epoch_train_loss=0.457006118786518
Epoch 2440
Epoch 2440 :: Batch 0/1
Batch Loss = 0.4567209754661452
2440, epoch_train_loss=0.4567209754661452
Epoch 2441
Epoch 2441 :: Batch 0/1
Batch Loss = 0.456446094996109
2441, epoch_train_loss=0.456446094996109
Epoch 2442
Epoch 2442 :: Batch 0/1
Batch Loss = 0.4562281895855555
2442, epoch_train_loss=0.4562281895855555
Epoch 2443
Epoch 2443 :: Batch 0/1
Batch Loss = 0.4560149210821826
2443, epoch_train_loss=0.4560149210821826
Epoch 2444
Epoch 2444 :: Batch 0/1
Batch Loss = 0.45579340689210124
2444, epoch_train_loss=0.45579340689210124
Epoch 2445
Epoch 2445 :: Batch 0/1
Batch Loss = 0.4555610842435582
2445, epoch_train_loss=0.4555610842435582
Epoch 2446
Epoch 2446 :: Batch 0/1
Batch Loss = 0.45531774277799814
2446, epoch_train_loss=0.45531774277799814
Epoch 2447
Epoch 2447 :: Batch 0/1
Batch Loss = 0.45506541375641807
2447, epoch_train_loss=0.45506541375641807
Epoch 2448
Epoch 2448 :: Batch 0/1
Batch Loss = 0.45480942623068565
2448, epoch_train_loss=0.45480942623068565
Epoch 2449
Epoch 2449 :: Batch 0/1
Batch Loss = 0.454563733443795
2449, epoch_train_loss=0.454563733443795
Epoch 2450
Epoch 2450 :: Batch 0/1
Batch Loss = 0.4543468206351298
2450, epoch_train_loss=0.4543468206351298
Epoch 2451
Epoch 2451 :: Batch 0/1
Batch Loss = 0.4541183994625634
2451, epoch_train_loss=0.4541183994625634
Epoch 2452
Epoch 2452 :: Batch 0/1
Batch Loss = 0.4538613862912293
2452, epoch_train_loss=0.4538613862912293
Epoch 2453
Epoch 2453 :: Batch 0/1
Batch Loss = 0.4536227408409401
2453, epoch_train_loss=0.4536227408409401
Epoch 2454
Epoch 2454 :: Batch 0/1
Batch Loss = 0.45339670841329677
2454, epoch_train_loss=0.45339670841329677
Epoch 2455
Epoch 2455 :: Batch 0/1
Batch Loss = 0.4531700756236886
2455, epoch_train_loss=0.4531700756236886
Epoch 2456
Epoch 2456 :: Batch 0/1
Batch Loss = 0.4529396398602026
2456, epoch_train_loss=0.4529396398602026
Epoch 2457
Epoch 2457 :: Batch 0/1
Batch Loss = 0.45270562224030997
2457, epoch_train_loss=0.45270562224030997
Epoch 2458
Epoch 2458 :: Batch 0/1
Batch Loss = 0.4524694066698853
2458, epoch_train_loss=0.4524694066698853
Epoch 2459
Epoch 2459 :: Batch 0/1
Batch Loss = 0.4522332569982472
2459, epoch_train_loss=0.4522332569982472
Epoch 2460
Epoch 2460 :: Batch 0/1
Batch Loss = 0.4520009095247841
2460, epoch_train_loss=0.4520009095247841
Epoch 2461
Epoch 2461 :: Batch 0/1
Batch Loss = 0.4517745538833602
2461, epoch_train_loss=0.4517745538833602
Epoch 2462
Epoch 2462 :: Batch 0/1
Batch Loss = 0.4515466603276232
2462, epoch_train_loss=0.4515466603276232
Epoch 2463
Epoch 2463 :: Batch 0/1
Batch Loss = 0.45131149748642246
2463, epoch_train_loss=0.45131149748642246
Epoch 2464
Epoch 2464 :: Batch 0/1
Batch Loss = 0.4510772527420874
2464, epoch_train_loss=0.4510772527420874
Epoch 2465
Epoch 2465 :: Batch 0/1
Batch Loss = 0.4508478631685533
2465, epoch_train_loss=0.4508478631685533
Epoch 2466
Epoch 2466 :: Batch 0/1
Batch Loss = 0.4506202125117353
2466, epoch_train_loss=0.4506202125117353
Epoch 2467
Epoch 2467 :: Batch 0/1
Batch Loss = 0.45039157460691664
2467, epoch_train_loss=0.45039157460691664
Epoch 2468
Epoch 2468 :: Batch 0/1
Batch Loss = 0.4501612748383363
2468, epoch_train_loss=0.4501612748383363
Epoch 2469
Epoch 2469 :: Batch 0/1
Batch Loss = 0.4499302317555988
2469, epoch_train_loss=0.4499302317555988
Epoch 2470
Epoch 2470 :: Batch 0/1
Batch Loss = 0.44970051835948893
2470, epoch_train_loss=0.44970051835948893
Epoch 2471
Epoch 2471 :: Batch 0/1
Batch Loss = 0.4494737513629359
2471, epoch_train_loss=0.4494737513629359
Epoch 2472
Epoch 2472 :: Batch 0/1
Batch Loss = 0.4492478601359069
2472, epoch_train_loss=0.4492478601359069
Epoch 2473
Epoch 2473 :: Batch 0/1
Batch Loss = 0.4490195832324096
2473, epoch_train_loss=0.4490195832324096
Epoch 2474
Epoch 2474 :: Batch 0/1
Batch Loss = 0.44879074125455104
2474, epoch_train_loss=0.44879074125455104
Epoch 2475
Epoch 2475 :: Batch 0/1
Batch Loss = 0.448564010438449
2475, epoch_train_loss=0.448564010438449
Epoch 2476
Epoch 2476 :: Batch 0/1
Batch Loss = 0.44833858575406926
2476, epoch_train_loss=0.44833858575406926
Epoch 2477
Epoch 2477 :: Batch 0/1
Batch Loss = 0.44811272206680197
2477, epoch_train_loss=0.44811272206680197
Epoch 2478
Epoch 2478 :: Batch 0/1
Batch Loss = 0.4478859169402852
2478, epoch_train_loss=0.4478859169402852
Epoch 2479
Epoch 2479 :: Batch 0/1
Batch Loss = 0.44765901039280165
2479, epoch_train_loss=0.44765901039280165
Epoch 2480
Epoch 2480 :: Batch 0/1
Batch Loss = 0.4474331962679967
2480, epoch_train_loss=0.4474331962679967
Epoch 2481
Epoch 2481 :: Batch 0/1
Batch Loss = 0.4472085152071613
2481, epoch_train_loss=0.4472085152071613
Epoch 2482
Epoch 2482 :: Batch 0/1
Batch Loss = 0.44698365810722646
2482, epoch_train_loss=0.44698365810722646
Epoch 2483
Epoch 2483 :: Batch 0/1
Batch Loss = 0.44675815659756657
2483, epoch_train_loss=0.44675815659756657
Epoch 2484
Epoch 2484 :: Batch 0/1
Batch Loss = 0.44653302959497615
2484, epoch_train_loss=0.44653302959497615
Epoch 2485
Epoch 2485 :: Batch 0/1
Batch Loss = 0.44630883890245854
2485, epoch_train_loss=0.44630883890245854
Epoch 2486
Epoch 2486 :: Batch 0/1
Batch Loss = 0.4460851209192806
2486, epoch_train_loss=0.4460851209192806
Epoch 2487
Epoch 2487 :: Batch 0/1
Batch Loss = 0.4458613059459418
2487, epoch_train_loss=0.4458613059459418
Epoch 2488
Epoch 2488 :: Batch 0/1
Batch Loss = 0.44563734370667313
2488, epoch_train_loss=0.44563734370667313
Epoch 2489
Epoch 2489 :: Batch 0/1
Batch Loss = 0.44541363962846336
2489, epoch_train_loss=0.44541363962846336
Epoch 2490
Epoch 2490 :: Batch 0/1
Batch Loss = 0.4451905117641484
2490, epoch_train_loss=0.4451905117641484
Epoch 2491
Epoch 2491 :: Batch 0/1
Batch Loss = 0.44496774428254277
2491, epoch_train_loss=0.44496774428254277
Epoch 2492
Epoch 2492 :: Batch 0/1
Batch Loss = 0.4447449098390958
2492, epoch_train_loss=0.4447449098390958
Epoch 2493
Epoch 2493 :: Batch 0/1
Batch Loss = 0.4445220313921381
2493, epoch_train_loss=0.4445220313921381
Epoch 2494
Epoch 2494 :: Batch 0/1
Batch Loss = 0.4442994515894364
2494, epoch_train_loss=0.4442994515894364
Epoch 2495
Epoch 2495 :: Batch 0/1
Batch Loss = 0.44407726128138797
2495, epoch_train_loss=0.44407726128138797
Epoch 2496
Epoch 2496 :: Batch 0/1
Batch Loss = 0.44385525405157333
2496, epoch_train_loss=0.44385525405157333
Epoch 2497
Epoch 2497 :: Batch 0/1
Batch Loss = 0.44363325937618303
2497, epoch_train_loss=0.44363325937618303
Epoch 2498
Epoch 2498 :: Batch 0/1
Batch Loss = 0.44341133526908555
2498, epoch_train_loss=0.44341133526908555
Epoch 2499
Epoch 2499 :: Batch 0/1
Batch Loss = 0.4431896540584056
2499, epoch_train_loss=0.4431896540584056
