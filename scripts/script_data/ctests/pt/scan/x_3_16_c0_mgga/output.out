/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting "B3LYP_WITH_VWN5 = True" in pyscf_conf.py
  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e40a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e40a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
<pyscf.gto.mole.Mole object at 0x7ffeac1e40a0> [['P', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac1e4970> [['N', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac1e4c40> [['H', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac1e7520> [['Li', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac1e5c30> [['O', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac1e6050> [['Cl', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac1e63b0> [['Al', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac1e5720> [['S', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac1e62c0> [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac1e7a30> [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeac1e7cd0> [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeac1e7370> [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac1e4820> [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac1e4cd0> [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac1e4160> [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeac1e52a0> [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac1e66b0> [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac1e49a0> [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac1e7160> [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac1e6ef0> [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeac1e65c0> [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeac1e6560> [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac074d90> [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]] 5
<pyscf.gto.mole.Mole object at 0x7ffeac075810> [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeac076320> [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]] 5
mol:  [['P', array([0., 0., 0.])]]
converged SCF energy = -341.104145992717  <S^2> = 3.7502984  2S+1 = 4.0001492
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e4970> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e4970> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-3.48053232e-03 -9.22981806e-04 -2.09507924e-03 ... -1.11294850e+01
 -1.11294850e+01 -1.11294850e+01] = SCAN,
rho_a.shape=(6, 6320), rho_b.shape=(6, 6320)
fxc_a.shape=(6320,), fxc_b.shape=(6320,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12640), fxc.shape=(12640,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(2, 6320, 3)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12640, 3)
rho_filt.shape=(12640,)
mol:  [['N', array([0., 0., 0.])]]
converged SCF energy = -54.5289742046675  <S^2> = 3.7524945  2S+1 = 4.0012471
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e4c40> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e4c40> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5016, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.10256797e-03 -5.98013179e-04 -6.71209617e-05 ... -5.03581543e+00
 -5.03581543e+00 -5.03581543e+00] = SCAN,
rho_a.shape=(6, 5016), rho_b.shape=(6, 5016)
fxc_a.shape=(5016,), fxc_b.shape=(5016,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 10032), fxc.shape=(10032,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5016, 22)
rho shape (4, 4, 2, 5016)
rho_filt shape: (5016,)
get descriptors tdrho.shape=(2, 5016, 3)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(10032, 3)
rho_filt.shape=(10032,)
mol:  [['H', array([0., 0., 0.])]]
converged SCF energy = -0.499812984008539  <S^2> = 0.75  2S+1 = 2
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e7520> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e7520> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 7, 7)
ao.shape (10, 2440, 7)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.59173730e-03 -7.67300285e-04 -6.09330303e-05 ... -7.27736608e-01
 -7.27736608e-01 -7.27736608e-01] = SCAN,
rho_a.shape=(6, 2440), rho_b.shape=(6, 2440)
fxc_a.shape=(2440,), fxc_b.shape=(2440,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 2440), fxc.shape=(2440,)
get_data, dm shape = (2, 7, 7)
ao_eval.shape=(4, 2440, 7)
rho shape (4, 4, 2, 2440)
rho_filt shape: (2440,)
get descriptors tdrho.shape=(2, 2440, 3)
mol:  [['Li', array([0., 0., 0.])]]
converged SCF energy = -7.46006188627842  <S^2> = 0.75000049  2S+1 = 2.0000005
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e5c30> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e5c30> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 4592, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-6.71507910e-03 -1.45299376e-03 -1.45299376e-03 ... -1.46930969e-02
 -2.05021258e+00 -2.05021258e+00] = SCAN,
rho_a.shape=(6, 4592), rho_b.shape=(6, 4592)
fxc_a.shape=(4592,), fxc_b.shape=(4592,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 9184), fxc.shape=(9184,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 4592, 22)
rho shape (4, 4, 2, 4592)
rho_filt shape: (4592,)
get descriptors tdrho.shape=(2, 4592, 3)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(9184, 3)
rho_filt.shape=(9184,)
mol:  [['O', array([0., 0., 0.])]]
converged SCF energy = -75.003380292217  <S^2> = 2.0027446  2S+1 = 3.0018292
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e6050> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e6050> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5040, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-7.50862872e-04 -1.30793603e-04 -6.57493534e-06 ... -5.78388650e+00
 -5.78388650e+00 -5.78388650e+00] = SCAN,
rho_a.shape=(6, 5040), rho_b.shape=(6, 5040)
fxc_a.shape=(5040,), fxc_b.shape=(5040,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 10080), fxc.shape=(10080,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5040, 22)
rho shape (4, 4, 2, 5040)
rho_filt shape: (5040,)
get descriptors tdrho.shape=(2, 5040, 3)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(10080, 3)
rho_filt.shape=(10080,)
mol:  [['Cl', array([0., 0., 0.])]]
converged SCF energy = -459.957577121336  <S^2> = 0.75161941  2S+1 = 2.0016188
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e63b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e63b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6152, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-7.49158483e-04 -9.79939847e-04 -3.45846905e-04 ... -1.26646370e+01
 -1.26646370e+01 -1.26646370e+01] = SCAN,
rho_a.shape=(6, 6152), rho_b.shape=(6, 6152)
fxc_a.shape=(6152,), fxc_b.shape=(6152,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12304), fxc.shape=(12304,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6152, 30)
rho shape (4, 4, 2, 6152)
rho_filt shape: (6152,)
get descriptors tdrho.shape=(2, 6152, 3)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12304, 3)
rho_filt.shape=(12304,)
mol:  [['Al', array([0., 0., 0.])]]
converged SCF energy = -242.226560989246  <S^2> = 0.75226414  2S+1 = 2.0022629
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e5720> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e5720> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6088, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.39565697e-02 -8.69596418e-03 -4.30176653e-03 ... -1.39782503e-04
 -1.04898436e-03 -7.75345721e-05] = SCAN,
rho_a.shape=(6, 6088), rho_b.shape=(6, 6088)
fxc_a.shape=(6088,), fxc_b.shape=(6088,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12176), fxc.shape=(12176,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6088, 30)
rho shape (4, 4, 2, 6088)
rho_filt shape: (6088,)
get descriptors tdrho.shape=(2, 6088, 3)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12176, 3)
rho_filt.shape=(12176,)
mol:  [['S', array([0., 0., 0.])]]
converged SCF energy = -397.938786814395  <S^2> = 2.0022329  2S+1 = 3.0014882
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e62c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e62c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.38675245e-03 -8.23622005e-04 -9.79799418e-04 ... -1.18982463e+01
 -1.18982463e+01 -1.18982463e+01] = SCAN,
rho_a.shape=(6, 6320), rho_b.shape=(6, 6320)
fxc_a.shape=(6320,), fxc_b.shape=(6320,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12640), fxc.shape=(12640,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(2, 6320, 3)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12640, 3)
rho_filt.shape=(12640,)
mol:  [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]]
converged SCF energy = -107.339357395734  <S^2> = 1.7763568e-15  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e7a30> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e7a30> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9752, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.04987750e-03 -6.68953858e-04 -8.57556270e-04 ... -1.07485583e-03
 -8.01425698e-01 -8.01425698e-01] = SCAN,
rho_a.shape=(6, 9752), rho_b.shape=(6, 9752)
fxc_a.shape=(9752,), fxc_b.shape=(9752,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9752), fxc.shape=(9752,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9752, 44)
rho shape (4, 4, 2, 9752)
rho_filt shape: (9752,)
get descriptors tdrho.shape=(2, 9752, 3)
mol:  [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]]
converged SCF energy = -93.3377924465131  <S^2> = 4.0073189e-10  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e7cd0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e7cd0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 12256, 51)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.97917285e-04 -2.54412366e-05 -3.15182243e-05 ... -6.37386500e-01
 -6.37386500e-01 -6.37386500e-01] = SCAN,
rho_a.shape=(6, 12256), rho_b.shape=(6, 12256)
fxc_a.shape=(12256,), fxc_b.shape=(12256,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12256), fxc.shape=(12256,)
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 12256, 51)
rho shape (4, 4, 2, 12256)
rho_filt shape: (12256,)
get descriptors tdrho.shape=(2, 12256, 3)
mol:  [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]]
converged SCF energy = -188.456965322843  <S^2> = 1.9539925e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e7370> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e7370> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 66, 66)
ao.shape (10, 14920, 66)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.50217115e-04 -2.07520066e-04 -9.23619896e-04 ... -2.74295208e-06
 -4.27559894e+00 -4.27559894e+00] = SCAN,
rho_a.shape=(6, 14920), rho_b.shape=(6, 14920)
fxc_a.shape=(14920,), fxc_b.shape=(14920,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 14920), fxc.shape=(14920,)
get_data, dm shape = (2, 66, 66)
ao_eval.shape=(4, 14920, 66)
rho shape (4, 4, 2, 14920)
rho_filt shape: (14920,)
get descriptors tdrho.shape=(2, 14920, 3)
mol:  [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]]
converged SCF energy = -920.005608888959  <S^2> = 4.938272e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e4820> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e4820> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12208, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00155834 -0.00091688 -0.00215831 ... -0.00091688 -0.41618507
 -0.41618507] = SCAN,
rho_a.shape=(6, 12208), rho_b.shape=(6, 12208)
fxc_a.shape=(12208,), fxc_b.shape=(12208,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12208), fxc.shape=(12208,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12208, 60)
rho shape (4, 4, 2, 12208)
rho_filt shape: (12208,)
get descriptors tdrho.shape=(2, 12208, 3)
mol:  [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]]
converged SCF energy = -199.394370591172  <S^2> = 1.2079227e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e4cd0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e4cd0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9824, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.92948614e-04 -1.95198688e-05 -1.16699802e-03 ... -4.89378340e-01
 -4.89378340e-01 -4.89378340e-01] = SCAN,
rho_a.shape=(6, 9824), rho_b.shape=(6, 9824)
fxc_a.shape=(9824,), fxc_b.shape=(9824,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9824), fxc.shape=(9824,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9824, 44)
rho shape (4, 4, 2, 9824)
rho_filt shape: (9824,)
get descriptors tdrho.shape=(2, 9824, 3)
mol:  [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]]
converged SCF energy = -150.214894502658  <S^2> = 1.0018599  2S+1 = 2.2377309
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e4160> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e4160> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9912, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.02062734e-04 -1.43137772e-04 -7.39292322e-06 ... -6.59150624e-01
 -6.59150624e-01 -6.59150624e-01] = SCAN,
rho_a.shape=(6, 9912), rho_b.shape=(6, 9912)
fxc_a.shape=(9912,), fxc_b.shape=(9912,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9912), fxc.shape=(9912,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9912, 44)
rho shape (4, 4, 2, 9912)
rho_filt shape: (9912,)
get descriptors tdrho.shape=(2, 9912, 3)
mol:  [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]]
converged SCF energy = -77.2435048346373  <S^2> = 8.8817842e-15  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e52a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e52a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 58, 58)
ao.shape (10, 15208, 58)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.83270456e-05 -8.83270456e-05 -9.75839850e-04 ... -3.46719667e-05
 -3.31708644e-05 -3.31708644e-05] = SCAN,
rho_a.shape=(6, 15208), rho_b.shape=(6, 15208)
fxc_a.shape=(15208,), fxc_b.shape=(15208,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 15208), fxc.shape=(15208,)
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15208, 58)
rho shape (4, 4, 2, 15208)
rho_filt shape: (15208,)
get descriptors tdrho.shape=(2, 15208, 3)
mol:  [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]]
converged SCF energy = -113.221335689652  <S^2> = 6.5902839e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e66b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e66b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 10040, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-5.37000578e-04 -8.55494549e-04 -2.46853288e-03 ... -7.34251999e-01
 -7.34251999e-01 -7.34251999e-01] = SCAN,
rho_a.shape=(6, 10040), rho_b.shape=(6, 10040)
fxc_a.shape=(10040,), fxc_b.shape=(10040,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 10040), fxc.shape=(10040,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 10040, 44)
rho shape (4, 4, 2, 10040)
rho_filt shape: (10040,)
get descriptors tdrho.shape=(2, 10040, 3)
mol:  [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]]
converged SCF energy = -460.624592374077  <S^2> = 6.5725203e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e49a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e49a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 37, 37)
ao.shape (10, 8552, 37)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.38161177e-04 -1.81188367e-05 -2.37300299e-05 ... -2.83738108e+00
 -2.83738108e+00 -2.83738108e+00] = SCAN,
rho_a.shape=(6, 8552), rho_b.shape=(6, 8552)
fxc_a.shape=(8552,), fxc_b.shape=(8552,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 8552), fxc.shape=(8552,)
get_data, dm shape = (2, 37, 37)
ao_eval.shape=(4, 8552, 37)
rho shape (4, 4, 2, 8552)
rho_filt shape: (8552,)
get descriptors tdrho.shape=(2, 8552, 3)
mol:  [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]]
converged SCF energy = -8.04458854018922  <S^2> = 7.7937656e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e7160> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e7160> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 29, 29)
ao.shape (10, 6936, 29)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00113445 -0.00118624 -0.00118624 ... -0.48434639 -0.48434639
 -0.48434639] = SCAN,
rho_a.shape=(6, 6936), rho_b.shape=(6, 6936)
fxc_a.shape=(6936,), fxc_b.shape=(6936,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 6936), fxc.shape=(6936,)
get_data, dm shape = (2, 29, 29)
ao_eval.shape=(4, 6936, 29)
rho shape (4, 4, 2, 6936)
rho_filt shape: (6936,)
get descriptors tdrho.shape=(2, 6936, 3)
mol:  [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]]
converged SCF energy = -324.340512506578  <S^2> = 1.5862867e-11  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e6ef0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e6ef0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 11536, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00297935 -0.00297935 -0.00407089 ... -0.00297935 -0.00297935
 -0.00407089] = SCAN,
rho_a.shape=(6, 11536), rho_b.shape=(6, 11536)
fxc_a.shape=(11536,), fxc_b.shape=(11536,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 11536), fxc.shape=(11536,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 11536, 60)
rho shape (4, 4, 2, 11536)
rho_filt shape: (11536,)
get descriptors tdrho.shape=(2, 11536, 3)
mol:  [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]]
converged SCF energy = -1622.57507845814  <S^2> = 8.31335e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e65c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e65c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 120, 120)
ao.shape (10, 24512, 120)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.61400996e-04 -4.90484900e-04 -2.56451718e-03 ... -9.59296113e+00
 -9.59296113e+00 -9.59296113e+00] = SCAN,
rho_a.shape=(6, 24512), rho_b.shape=(6, 24512)
fxc_a.shape=(24512,), fxc_b.shape=(24512,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 24512), fxc.shape=(24512,)
get_data, dm shape = (2, 120, 120)
ao_eval.shape=(4, 24512, 120)
rho shape (4, 4, 2, 24512)
rho_filt shape: (24512,)
get descriptors tdrho.shape=(2, 24512, 3)
mol:  [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]]
converged SCF energy = -342.979728469575  <S^2> = 2.5394797e-11  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac1e6560> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac1e6560> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 13096, 51)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.28637920e-03 -4.32383300e-04 -3.74057272e-05 ... -1.91722770e+00
 -1.91722770e+00 -1.91722770e+00] = SCAN,
rho_a.shape=(6, 13096), rho_b.shape=(6, 13096)
fxc_a.shape=(13096,), fxc_b.shape=(13096,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 13096), fxc.shape=(13096,)
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 13096, 51)
rho shape (4, 4, 2, 13096)
rho_filt shape: (13096,)
get descriptors tdrho.shape=(2, 13096, 3)
mol:  [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]]
converged SCF energy = -578.565336180875  <S^2> = 1.0034705  2S+1 = 2.2391699
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac074d90> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac074d90> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12384, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.59570403e-04 -2.59078681e-04 -2.60105975e-04 ... -3.86943992e-01
 -3.86943992e-01 -3.86943992e-01] = SCAN,
rho_a.shape=(6, 12384), rho_b.shape=(6, 12384)
fxc_a.shape=(12384,), fxc_b.shape=(12384,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12384), fxc.shape=(12384,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12384, 60)
rho shape (4, 4, 2, 12384)
rho_filt shape: (12384,)
get descriptors tdrho.shape=(2, 12384, 3)
mol:  [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]]
converged SCF energy = -40.4598214864077  <S^2> = 3.1885605e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac075810> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac075810> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 50, 50)
ao.shape (10, 13936, 50)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.68439986e-04 -2.42462569e-04 -1.69927031e-05 ... -2.55230307e-05
 -2.55230307e-05 -2.55230307e-05] = SCAN,
rho_a.shape=(6, 13936), rho_b.shape=(6, 13936)
fxc_a.shape=(13936,), fxc_b.shape=(13936,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 13936), fxc.shape=(13936,)
get_data, dm shape = (2, 50, 50)
ao_eval.shape=(4, 13936, 50)
rho shape (4, 4, 2, 13936)
rho_filt shape: (13936,)
get descriptors tdrho.shape=(2, 13936, 3)
mol:  [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]]
converged SCF energy = -39.0756147483504  <S^2> = 6.2030381e-12  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac076320> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac076320> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 36, 36)
ao.shape (10, 9656, 36)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-7.67688751e-04 -4.57393214e-05 -2.02834191e-04 ... -1.14928924e+00
 -1.14928924e+00 -1.14928924e+00] = SCAN,
rho_a.shape=(6, 9656), rho_b.shape=(6, 9656)
fxc_a.shape=(9656,), fxc_b.shape=(9656,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9656), fxc.shape=(9656,)
get_data, dm shape = (2, 36, 36)
ao_eval.shape=(4, 9656, 36)
rho shape (4, 4, 2, 9656)
rho_filt shape: (9656,)
get descriptors tdrho.shape=(2, 9656, 3)
mol:  [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]]
converged SCF energy = -291.719272437819  <S^2> = 1.3159251e-11  2S+1 = 1
New DM shape: (2, 58, 58)
ao.shape (10, 15256, 58)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.33850535e-04 -2.34903029e-04 -1.75623665e-05 ... -1.92891112e-05
 -1.92891112e-05 -1.92891112e-05] = SCAN,
rho_a.shape=(6, 15256), rho_b.shape=(6, 15256)
fxc_a.shape=(15256,), fxc_b.shape=(15256,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 15256), fxc.shape=(15256,)
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15256, 58)
rho shape (4, 4, 2, 15256)
rho_filt shape: (15256,)
get descriptors tdrho.shape=(2, 15256, 3)
localnet.spin_scaling: concatenating the data
first data shape = (10940, 3)
concatenated: tdrho.shape=(258861, 3)
PRE NAN FILT: tFxc.shape=(258861,), tdrho.shape=(258861, 3)
nan_filt_rho.shape=(258861,)
nan_filt_fxc.shape=(258861,)
tFxc.shape=(258861,), tdrho.shape=(258861, 3)
inp[0].shape = (258861, 2)
Epoch 0
Epoch 0 :: Batch 0/1
Batch Loss = 5.73475350470164
0, epoch_train_loss=5.73475350470164
Epoch 1
Epoch 1 :: Batch 0/1
Batch Loss = 5.330913041877925
1, epoch_train_loss=5.330913041877925
Epoch 2
Epoch 2 :: Batch 0/1
Batch Loss = 4.9662714303410525
2, epoch_train_loss=4.9662714303410525
Epoch 3
Epoch 3 :: Batch 0/1
Batch Loss = 4.557016748272407
3, epoch_train_loss=4.557016748272407
Epoch 4
Epoch 4 :: Batch 0/1
Batch Loss = 4.246443393719767
4, epoch_train_loss=4.246443393719767
Epoch 5
Epoch 5 :: Batch 0/1
Batch Loss = 3.94940575860691
5, epoch_train_loss=3.94940575860691
Epoch 6
Epoch 6 :: Batch 0/1
Batch Loss = 3.7433513425643996
6, epoch_train_loss=3.7433513425643996
Epoch 7
Epoch 7 :: Batch 0/1
Batch Loss = 3.465980495657705
7, epoch_train_loss=3.465980495657705
Epoch 8
Epoch 8 :: Batch 0/1
Batch Loss = 3.2737919808728324
8, epoch_train_loss=3.2737919808728324
Epoch 9
Epoch 9 :: Batch 0/1
Batch Loss = 3.0965959033800328
9, epoch_train_loss=3.0965959033800328
Epoch 10
Epoch 10 :: Batch 0/1
Batch Loss = 2.927705863861653
10, epoch_train_loss=2.927705863861653
Epoch 11
Epoch 11 :: Batch 0/1
Batch Loss = 2.906122895539271
11, epoch_train_loss=2.906122895539271
Epoch 12
Epoch 12 :: Batch 0/1
Batch Loss = 3.100389061146763
12, epoch_train_loss=3.100389061146763
Epoch 13
Epoch 13 :: Batch 0/1
Batch Loss = 2.7887619708334412
13, epoch_train_loss=2.7887619708334412
Epoch 14
Epoch 14 :: Batch 0/1
Batch Loss = 2.8877442322353577
14, epoch_train_loss=2.8877442322353577
Epoch 15
Epoch 15 :: Batch 0/1
Batch Loss = 2.8814229985540205
15, epoch_train_loss=2.8814229985540205
Epoch 16
Epoch 16 :: Batch 0/1
Batch Loss = 2.812984727220899
16, epoch_train_loss=2.812984727220899
Epoch 17
Epoch 17 :: Batch 0/1
Batch Loss = 2.7982902548846083
17, epoch_train_loss=2.7982902548846083
Epoch 18
Epoch 18 :: Batch 0/1
Batch Loss = 2.7429167618389156
18, epoch_train_loss=2.7429167618389156
Epoch 19
Epoch 19 :: Batch 0/1
Batch Loss = 2.7496579385980673
19, epoch_train_loss=2.7496579385980673
Epoch 20
Epoch 20 :: Batch 0/1
Batch Loss = 2.761619808414324
20, epoch_train_loss=2.761619808414324
Epoch 21
Epoch 21 :: Batch 0/1
Batch Loss = 2.6777702116800897
21, epoch_train_loss=2.6777702116800897
Epoch 22
Epoch 22 :: Batch 0/1
Batch Loss = 2.7517194182158167
22, epoch_train_loss=2.7517194182158167
Epoch 23
Epoch 23 :: Batch 0/1
Batch Loss = 2.6647456689081976
23, epoch_train_loss=2.6647456689081976
Epoch 24
Epoch 24 :: Batch 0/1
Batch Loss = 2.718431580757487
24, epoch_train_loss=2.718431580757487
Epoch 25
Epoch 25 :: Batch 0/1
Batch Loss = 2.68983463707488
25, epoch_train_loss=2.68983463707488
Epoch 26
Epoch 26 :: Batch 0/1
Batch Loss = 2.671724244645197
26, epoch_train_loss=2.671724244645197
Epoch 27
Epoch 27 :: Batch 0/1
Batch Loss = 2.706565014214981
27, epoch_train_loss=2.706565014214981
Epoch 28
Epoch 28 :: Batch 0/1
Batch Loss = 2.6601398081571284
28, epoch_train_loss=2.6601398081571284
Epoch 29
Epoch 29 :: Batch 0/1
Batch Loss = 2.6899137818226784
29, epoch_train_loss=2.6899137818226784
Epoch 30
Epoch 30 :: Batch 0/1
Batch Loss = 2.6845022192728303
30, epoch_train_loss=2.6845022192728303
Epoch 31
Epoch 31 :: Batch 0/1
Batch Loss = 2.6640850783577075
31, epoch_train_loss=2.6640850783577075
Epoch 32
Epoch 32 :: Batch 0/1
Batch Loss = 2.6878775785821425
32, epoch_train_loss=2.6878775785821425
Epoch 33
Epoch 33 :: Batch 0/1
Batch Loss = 2.663407007342888
33, epoch_train_loss=2.663407007342888
Epoch 34
Epoch 34 :: Batch 0/1
Batch Loss = 2.669624230525829
34, epoch_train_loss=2.669624230525829
Epoch 35
Epoch 35 :: Batch 0/1
Batch Loss = 2.676035885558806
35, epoch_train_loss=2.676035885558806
Epoch 36
Epoch 36 :: Batch 0/1
Batch Loss = 2.6559131129683364
36, epoch_train_loss=2.6559131129683364
Epoch 37
Epoch 37 :: Batch 0/1
Batch Loss = 2.6687199715366803
37, epoch_train_loss=2.6687199715366803
Epoch 38
Epoch 38 :: Batch 0/1
Batch Loss = 2.661251790018244
38, epoch_train_loss=2.661251790018244
Epoch 39
Epoch 39 :: Batch 0/1
Batch Loss = 2.656435806030896
39, epoch_train_loss=2.656435806030896
Epoch 40
Epoch 40 :: Batch 0/1
Batch Loss = 2.6641575804213953
40, epoch_train_loss=2.6641575804213953
Epoch 41
Epoch 41 :: Batch 0/1
Batch Loss = 2.651912946996664
41, epoch_train_loss=2.651912946996664
Epoch 42
Epoch 42 :: Batch 0/1
Batch Loss = 2.6585465261019925
42, epoch_train_loss=2.6585465261019925
Epoch 43
Epoch 43 :: Batch 0/1
Batch Loss = 2.655838348979181
43, epoch_train_loss=2.655838348979181
Epoch 44
Epoch 44 :: Batch 0/1
Batch Loss = 2.6510231806813747
44, epoch_train_loss=2.6510231806813747
Epoch 45
Epoch 45 :: Batch 0/1
Batch Loss = 2.656502093162539
45, epoch_train_loss=2.656502093162539
Epoch 46
Epoch 46 :: Batch 0/1
Batch Loss = 2.648802082103137
46, epoch_train_loss=2.648802082103137
Epoch 47
Epoch 47 :: Batch 0/1
Batch Loss = 2.6531600316954913
47, epoch_train_loss=2.6531600316954913
Epoch 48
Epoch 48 :: Batch 0/1
Batch Loss = 2.6493259302784993
48, epoch_train_loss=2.6493259302784993
Epoch 49
Epoch 49 :: Batch 0/1
Batch Loss = 2.648347760428887
49, epoch_train_loss=2.648347760428887
Epoch 50
Epoch 50 :: Batch 0/1
Batch Loss = 2.650217839013214
50, epoch_train_loss=2.650217839013214
Epoch 51
Epoch 51 :: Batch 0/1
Batch Loss = 2.6457611541672486
51, epoch_train_loss=2.6457611541672486
Epoch 52
Epoch 52 :: Batch 0/1
Batch Loss = 2.64918668074281
52, epoch_train_loss=2.64918668074281
Epoch 53
Epoch 53 :: Batch 0/1
Batch Loss = 2.6448127231293737
53, epoch_train_loss=2.6448127231293737
Epoch 54
Epoch 54 :: Batch 0/1
Batch Loss = 2.647318313751731
54, epoch_train_loss=2.647318313751731
Epoch 55
Epoch 55 :: Batch 0/1
Batch Loss = 2.6449520681492245
55, epoch_train_loss=2.6449520681492245
Epoch 56
Epoch 56 :: Batch 0/1
Batch Loss = 2.645239612435395
56, epoch_train_loss=2.645239612435395
Epoch 57
Epoch 57 :: Batch 0/1
Batch Loss = 2.6446166771248243
57, epoch_train_loss=2.6446166771248243
Epoch 58
Epoch 58 :: Batch 0/1
Batch Loss = 2.6437271248079295
58, epoch_train_loss=2.6437271248079295
Epoch 59
Epoch 59 :: Batch 0/1
Batch Loss = 2.644174895420548
59, epoch_train_loss=2.644174895420548
Epoch 60
Epoch 60 :: Batch 0/1
Batch Loss = 2.6425247754663572
60, epoch_train_loss=2.6425247754663572
Epoch 61
Epoch 61 :: Batch 0/1
Batch Loss = 2.6435206359527874
61, epoch_train_loss=2.6435206359527874
Epoch 62
Epoch 62 :: Batch 0/1
Batch Loss = 2.641875679900198
62, epoch_train_loss=2.641875679900198
Epoch 63
Epoch 63 :: Batch 0/1
Batch Loss = 2.642780438895325
63, epoch_train_loss=2.642780438895325
Epoch 64
Epoch 64 :: Batch 0/1
Batch Loss = 2.6411799716733255
64, epoch_train_loss=2.6411799716733255
Epoch 65
Epoch 65 :: Batch 0/1
Batch Loss = 2.6420565498301363
65, epoch_train_loss=2.6420565498301363
Epoch 66
Epoch 66 :: Batch 0/1
Batch Loss = 2.6407443856917885
66, epoch_train_loss=2.6407443856917885
Epoch 67
Epoch 67 :: Batch 0/1
Batch Loss = 2.641312107809829
67, epoch_train_loss=2.641312107809829
Epoch 68
Epoch 68 :: Batch 0/1
Batch Loss = 2.640250058204933
68, epoch_train_loss=2.640250058204933
Epoch 69
Epoch 69 :: Batch 0/1
Batch Loss = 2.6406018261535524
69, epoch_train_loss=2.6406018261535524
Epoch 70
Epoch 70 :: Batch 0/1
Batch Loss = 2.6398521209715073
70, epoch_train_loss=2.6398521209715073
Epoch 71
Epoch 71 :: Batch 0/1
Batch Loss = 2.639829700263722
71, epoch_train_loss=2.639829700263722
Epoch 72
Epoch 72 :: Batch 0/1
Batch Loss = 2.639504772500691
72, epoch_train_loss=2.639504772500691
Epoch 73
Epoch 73 :: Batch 0/1
Batch Loss = 2.639138270741803
73, epoch_train_loss=2.639138270741803
Epoch 74
Epoch 74 :: Batch 0/1
Batch Loss = 2.6391655292067826
74, epoch_train_loss=2.6391655292067826
Epoch 75
Epoch 75 :: Batch 0/1
Batch Loss = 2.638546573573461
75, epoch_train_loss=2.638546573573461
Epoch 76
Epoch 76 :: Batch 0/1
Batch Loss = 2.6387806593555054
76, epoch_train_loss=2.6387806593555054
Epoch 77
Epoch 77 :: Batch 0/1
Batch Loss = 2.6380922324775047
77, epoch_train_loss=2.6380922324775047
Epoch 78
Epoch 78 :: Batch 0/1
Batch Loss = 2.6382414187009764
78, epoch_train_loss=2.6382414187009764
Epoch 79
Epoch 79 :: Batch 0/1
Batch Loss = 2.6377834263334115
79, epoch_train_loss=2.6377834263334115
Epoch 80
Epoch 80 :: Batch 0/1
Batch Loss = 2.637581204040401
80, epoch_train_loss=2.637581204040401
Epoch 81
Epoch 81 :: Batch 0/1
Batch Loss = 2.637527261383242
81, epoch_train_loss=2.637527261383242
Epoch 82
Epoch 82 :: Batch 0/1
Batch Loss = 2.6370576662131526
82, epoch_train_loss=2.6370576662131526
Epoch 83
Epoch 83 :: Batch 0/1
Batch Loss = 2.63713625280803
83, epoch_train_loss=2.63713625280803
Epoch 84
Epoch 84 :: Batch 0/1
Batch Loss = 2.6367784626700237
84, epoch_train_loss=2.6367784626700237
Epoch 85
Epoch 85 :: Batch 0/1
Batch Loss = 2.6365885634823707
85, epoch_train_loss=2.6365885634823707
Epoch 86
Epoch 86 :: Batch 0/1
Batch Loss = 2.636547420786019
86, epoch_train_loss=2.636547420786019
Epoch 87
Epoch 87 :: Batch 0/1
Batch Loss = 2.6361764606866287
87, epoch_train_loss=2.6361764606866287
Epoch 88
Epoch 88 :: Batch 0/1
Batch Loss = 2.6361145886060657
88, epoch_train_loss=2.6361145886060657
Epoch 89
Epoch 89 :: Batch 0/1
Batch Loss = 2.6359573887492895
89, epoch_train_loss=2.6359573887492895
Epoch 90
Epoch 90 :: Batch 0/1
Batch Loss = 2.635680742861152
90, epoch_train_loss=2.635680742861152
Epoch 91
Epoch 91 :: Batch 0/1
Batch Loss = 2.6356317418627504
91, epoch_train_loss=2.6356317418627504
Epoch 92
Epoch 92 :: Batch 0/1
Batch Loss = 2.6354642204572825
92, epoch_train_loss=2.6354642204572825
Epoch 93
Epoch 93 :: Batch 0/1
Batch Loss = 2.6352255638458337
93, epoch_train_loss=2.6352255638458337
Epoch 94
Epoch 94 :: Batch 0/1
Batch Loss = 2.6351642037219225
94, epoch_train_loss=2.6351642037219225
Epoch 95
Epoch 95 :: Batch 0/1
Batch Loss = 2.6350020331683903
95, epoch_train_loss=2.6350020331683903
Epoch 96
Epoch 96 :: Batch 0/1
Batch Loss = 2.6347960728803463
96, epoch_train_loss=2.6347960728803463
Epoch 97
Epoch 97 :: Batch 0/1
Batch Loss = 2.634706200541707
97, epoch_train_loss=2.634706200541707
Epoch 98
Epoch 98 :: Batch 0/1
Batch Loss = 2.6345905063153907
98, epoch_train_loss=2.6345905063153907
Epoch 99
Epoch 99 :: Batch 0/1
Batch Loss = 2.6343960162732345
99, epoch_train_loss=2.6343960162732345
Epoch 100
Epoch 100 :: Batch 0/1
Batch Loss = 2.634273697529655
100, epoch_train_loss=2.634273697529655
Epoch 101
Epoch 101 :: Batch 0/1
Batch Loss = 2.634182435918184
101, epoch_train_loss=2.634182435918184
Epoch 102
Epoch 102 :: Batch 0/1
Batch Loss = 2.6340263707354388
102, epoch_train_loss=2.6340263707354388
Epoch 103
Epoch 103 :: Batch 0/1
Batch Loss = 2.6338679993637344
103, epoch_train_loss=2.6338679993637344
Epoch 104
Epoch 104 :: Batch 0/1
Batch Loss = 2.6337621564517812
104, epoch_train_loss=2.6337621564517812
Epoch 105
Epoch 105 :: Batch 0/1
Batch Loss = 2.6336606833263385
105, epoch_train_loss=2.6336606833263385
Epoch 106
Epoch 106 :: Batch 0/1
Batch Loss = 2.6335142813933854
106, epoch_train_loss=2.6335142813933854
Epoch 107
Epoch 107 :: Batch 0/1
Batch Loss = 2.6333641690937846
107, epoch_train_loss=2.6333641690937846
Epoch 108
Epoch 108 :: Batch 0/1
Batch Loss = 2.6332507115031447
108, epoch_train_loss=2.6332507115031447
Epoch 109
Epoch 109 :: Batch 0/1
Batch Loss = 2.63314599949021
109, epoch_train_loss=2.63314599949021
Epoch 110
Epoch 110 :: Batch 0/1
Batch Loss = 2.6330186266344855
110, epoch_train_loss=2.6330186266344855
Epoch 111
Epoch 111 :: Batch 0/1
Batch Loss = 2.632875971626157
111, epoch_train_loss=2.632875971626157
Epoch 112
Epoch 112 :: Batch 0/1
Batch Loss = 2.6327406013971255
112, epoch_train_loss=2.6327406013971255
Epoch 113
Epoch 113 :: Batch 0/1
Batch Loss = 2.632620507964653
113, epoch_train_loss=2.632620507964653
Epoch 114
Epoch 114 :: Batch 0/1
Batch Loss = 2.63250666175368
114, epoch_train_loss=2.63250666175368
Epoch 115
Epoch 115 :: Batch 0/1
Batch Loss = 2.632385543639459
115, epoch_train_loss=2.632385543639459
Epoch 116
Epoch 116 :: Batch 0/1
Batch Loss = 2.632251868438028
116, epoch_train_loss=2.632251868438028
Epoch 117
Epoch 117 :: Batch 0/1
Batch Loss = 2.632112047868872
117, epoch_train_loss=2.632112047868872
Epoch 118
Epoch 118 :: Batch 0/1
Batch Loss = 2.6319730498080496
118, epoch_train_loss=2.6319730498080496
Epoch 119
Epoch 119 :: Batch 0/1
Batch Loss = 2.6318383769637452
119, epoch_train_loss=2.6318383769637452
Epoch 120
Epoch 120 :: Batch 0/1
Batch Loss = 2.6317074622311716
120, epoch_train_loss=2.6317074622311716
Epoch 121
Epoch 121 :: Batch 0/1
Batch Loss = 2.6315780944346154
121, epoch_train_loss=2.6315780944346154
Epoch 122
Epoch 122 :: Batch 0/1
Batch Loss = 2.631448794854229
122, epoch_train_loss=2.631448794854229
Epoch 123
Epoch 123 :: Batch 0/1
Batch Loss = 2.6313186204328716
123, epoch_train_loss=2.6313186204328716
Epoch 124
Epoch 124 :: Batch 0/1
Batch Loss = 2.6311898342145397
124, epoch_train_loss=2.6311898342145397
Epoch 125
Epoch 125 :: Batch 0/1
Batch Loss = 2.6310661058735723
125, epoch_train_loss=2.6310661058735723
Epoch 126
Epoch 126 :: Batch 0/1
Batch Loss = 2.630961003313334
126, epoch_train_loss=2.630961003313334
Epoch 127
Epoch 127 :: Batch 0/1
Batch Loss = 2.6309073231914497
127, epoch_train_loss=2.6309073231914497
Epoch 128
Epoch 128 :: Batch 0/1
Batch Loss = 2.63101490504645
128, epoch_train_loss=2.63101490504645
Epoch 129
Epoch 129 :: Batch 0/1
Batch Loss = 2.631462871988629
129, epoch_train_loss=2.631462871988629
Epoch 130
Epoch 130 :: Batch 0/1
Batch Loss = 2.6319780763937315
130, epoch_train_loss=2.6319780763937315
Epoch 131
Epoch 131 :: Batch 0/1
Batch Loss = 2.6322368627617423
131, epoch_train_loss=2.6322368627617423
Epoch 132
Epoch 132 :: Batch 0/1
Batch Loss = 2.634305830176073
132, epoch_train_loss=2.634305830176073
Epoch 133
Epoch 133 :: Batch 0/1
Batch Loss = 2.6408426479228027
133, epoch_train_loss=2.6408426479228027
Epoch 134
Epoch 134 :: Batch 0/1
Batch Loss = 2.6589325883378128
134, epoch_train_loss=2.6589325883378128
Epoch 135
Epoch 135 :: Batch 0/1
Batch Loss = 2.689495160529408
135, epoch_train_loss=2.689495160529408
Epoch 136
Epoch 136 :: Batch 0/1
Batch Loss = 2.765034005350659
136, epoch_train_loss=2.765034005350659
Epoch 137
Epoch 137 :: Batch 0/1
Batch Loss = 2.7084814324204785
137, epoch_train_loss=2.7084814324204785
Epoch 138
Epoch 138 :: Batch 0/1
Batch Loss = 2.6525655637049996
138, epoch_train_loss=2.6525655637049996
Epoch 139
Epoch 139 :: Batch 0/1
Batch Loss = 2.634014627623742
139, epoch_train_loss=2.634014627623742
Epoch 140
Epoch 140 :: Batch 0/1
Batch Loss = 2.671055034966037
140, epoch_train_loss=2.671055034966037
Epoch 141
Epoch 141 :: Batch 0/1
Batch Loss = 2.663996403217707
141, epoch_train_loss=2.663996403217707
Epoch 142
Epoch 142 :: Batch 0/1
Batch Loss = 2.6321778417137054
142, epoch_train_loss=2.6321778417137054
Epoch 143
Epoch 143 :: Batch 0/1
Batch Loss = 2.667885467053387
143, epoch_train_loss=2.667885467053387
Epoch 144
Epoch 144 :: Batch 0/1
Batch Loss = 2.647493034021778
144, epoch_train_loss=2.647493034021778
Epoch 145
Epoch 145 :: Batch 0/1
Batch Loss = 2.640699003837947
145, epoch_train_loss=2.640699003837947
Epoch 146
Epoch 146 :: Batch 0/1
Batch Loss = 2.6569637511792767
146, epoch_train_loss=2.6569637511792767
Epoch 147
Epoch 147 :: Batch 0/1
Batch Loss = 2.631193328103836
147, epoch_train_loss=2.631193328103836
Epoch 148
Epoch 148 :: Batch 0/1
Batch Loss = 2.648548627475781
148, epoch_train_loss=2.648548627475781
Epoch 149
Epoch 149 :: Batch 0/1
Batch Loss = 2.6331017815311935
149, epoch_train_loss=2.6331017815311935
Epoch 150
Epoch 150 :: Batch 0/1
Batch Loss = 2.6410300406011356
150, epoch_train_loss=2.6410300406011356
Epoch 151
Epoch 151 :: Batch 0/1
Batch Loss = 2.6370640508477665
151, epoch_train_loss=2.6370640508477665
Epoch 152
Epoch 152 :: Batch 0/1
Batch Loss = 2.6350720145984985
152, epoch_train_loss=2.6350720145984985
Epoch 153
Epoch 153 :: Batch 0/1
Batch Loss = 2.6380563536059345
153, epoch_train_loss=2.6380563536059345
Epoch 154
Epoch 154 :: Batch 0/1
Batch Loss = 2.631059432162231
154, epoch_train_loss=2.631059432162231
Epoch 155
Epoch 155 :: Batch 0/1
Batch Loss = 2.637252835058399
155, epoch_train_loss=2.637252835058399
Epoch 156
Epoch 156 :: Batch 0/1
Batch Loss = 2.6298511832400293
156, epoch_train_loss=2.6298511832400293
Epoch 157
Epoch 157 :: Batch 0/1
Batch Loss = 2.6357831203021034
157, epoch_train_loss=2.6357831203021034
Epoch 158
Epoch 158 :: Batch 0/1
Batch Loss = 2.6296008262617394
158, epoch_train_loss=2.6296008262617394
Epoch 159
Epoch 159 :: Batch 0/1
Batch Loss = 2.633028396467112
159, epoch_train_loss=2.633028396467112
Epoch 160
Epoch 160 :: Batch 0/1
Batch Loss = 2.6297750479299262
160, epoch_train_loss=2.6297750479299262
Epoch 161
Epoch 161 :: Batch 0/1
Batch Loss = 2.6304250698450113
161, epoch_train_loss=2.6304250698450113
Epoch 162
Epoch 162 :: Batch 0/1
Batch Loss = 2.630179924640508
162, epoch_train_loss=2.630179924640508
Epoch 163
Epoch 163 :: Batch 0/1
Batch Loss = 2.62827063697525
163, epoch_train_loss=2.62827063697525
Epoch 164
Epoch 164 :: Batch 0/1
Batch Loss = 2.6300974316508534
164, epoch_train_loss=2.6300974316508534
Epoch 165
Epoch 165 :: Batch 0/1
Batch Loss = 2.6266590249249835
165, epoch_train_loss=2.6266590249249835
Epoch 166
Epoch 166 :: Batch 0/1
Batch Loss = 2.629254766900151
166, epoch_train_loss=2.629254766900151
Epoch 167
Epoch 167 :: Batch 0/1
Batch Loss = 2.625971754215559
167, epoch_train_loss=2.625971754215559
Epoch 168
Epoch 168 :: Batch 0/1
Batch Loss = 2.6279142913950753
168, epoch_train_loss=2.6279142913950753
Epoch 169
Epoch 169 :: Batch 0/1
Batch Loss = 2.6259429093092352
169, epoch_train_loss=2.6259429093092352
Epoch 170
Epoch 170 :: Batch 0/1
Batch Loss = 2.626106749356153
170, epoch_train_loss=2.626106749356153
Epoch 171
Epoch 171 :: Batch 0/1
Batch Loss = 2.625965253582696
171, epoch_train_loss=2.625965253582696
Epoch 172
Epoch 172 :: Batch 0/1
Batch Loss = 2.624572766246035
172, epoch_train_loss=2.624572766246035
Epoch 173
Epoch 173 :: Batch 0/1
Batch Loss = 2.6253732302243122
173, epoch_train_loss=2.6253732302243122
Epoch 174
Epoch 174 :: Batch 0/1
Batch Loss = 2.6239254280393336
174, epoch_train_loss=2.6239254280393336
Epoch 175
Epoch 175 :: Batch 0/1
Batch Loss = 2.6239767528894142
175, epoch_train_loss=2.6239767528894142
Epoch 176
Epoch 176 :: Batch 0/1
Batch Loss = 2.623888442139571
176, epoch_train_loss=2.623888442139571
Epoch 177
Epoch 177 :: Batch 0/1
Batch Loss = 2.6225828770558595
177, epoch_train_loss=2.6225828770558595
Epoch 178
Epoch 178 :: Batch 0/1
Batch Loss = 2.6232123570366
178, epoch_train_loss=2.6232123570366
Epoch 179
Epoch 179 :: Batch 0/1
Batch Loss = 2.6222262096425446
179, epoch_train_loss=2.6222262096425446
Epoch 180
Epoch 180 :: Batch 0/1
Batch Loss = 2.6216803769602333
180, epoch_train_loss=2.6216803769602333
Epoch 181
Epoch 181 :: Batch 0/1
Batch Loss = 2.621890948530041
181, epoch_train_loss=2.621890948530041
Epoch 182
Epoch 182 :: Batch 0/1
Batch Loss = 2.621016289699546
182, epoch_train_loss=2.621016289699546
Epoch 183
Epoch 183 :: Batch 0/1
Batch Loss = 2.6204398474401764
183, epoch_train_loss=2.6204398474401764
Epoch 184
Epoch 184 :: Batch 0/1
Batch Loss = 2.6206237871803393
184, epoch_train_loss=2.6206237871803393
Epoch 185
Epoch 185 :: Batch 0/1
Batch Loss = 2.61985510856912
185, epoch_train_loss=2.61985510856912
Epoch 186
Epoch 186 :: Batch 0/1
Batch Loss = 2.6191693461042598
186, epoch_train_loss=2.6191693461042598
Epoch 187
Epoch 187 :: Batch 0/1
Batch Loss = 2.619086808549665
187, epoch_train_loss=2.619086808549665
Epoch 188
Epoch 188 :: Batch 0/1
Batch Loss = 2.6188497335251553
188, epoch_train_loss=2.6188497335251553
Epoch 189
Epoch 189 :: Batch 0/1
Batch Loss = 2.6179793765008057
189, epoch_train_loss=2.6179793765008057
Epoch 190
Epoch 190 :: Batch 0/1
Batch Loss = 2.6174072950469918
190, epoch_train_loss=2.6174072950469918
Epoch 191
Epoch 191 :: Batch 0/1
Batch Loss = 2.6171792586638656
191, epoch_train_loss=2.6171792586638656
Epoch 192
Epoch 192 :: Batch 0/1
Batch Loss = 2.616964825649606
192, epoch_train_loss=2.616964825649606
Epoch 193
Epoch 193 :: Batch 0/1
Batch Loss = 2.6164735983035774
193, epoch_train_loss=2.6164735983035774
Epoch 194
Epoch 194 :: Batch 0/1
Batch Loss = 2.615772478875422
194, epoch_train_loss=2.615772478875422
Epoch 195
Epoch 195 :: Batch 0/1
Batch Loss = 2.615073944716282
195, epoch_train_loss=2.615073944716282
Epoch 196
Epoch 196 :: Batch 0/1
Batch Loss = 2.6143740530339605
196, epoch_train_loss=2.6143740530339605
Epoch 197
Epoch 197 :: Batch 0/1
Batch Loss = 2.6137549517002743
197, epoch_train_loss=2.6137549517002743
Epoch 198
Epoch 198 :: Batch 0/1
Batch Loss = 2.6131042008162275
198, epoch_train_loss=2.6131042008162275
Epoch 199
Epoch 199 :: Batch 0/1
Batch Loss = 2.612485120026359
199, epoch_train_loss=2.612485120026359
Epoch 200
Epoch 200 :: Batch 0/1
Batch Loss = 2.611894327796082
200, epoch_train_loss=2.611894327796082
Epoch 201
Epoch 201 :: Batch 0/1
Batch Loss = 2.61157878268455
201, epoch_train_loss=2.61157878268455
Epoch 202
Epoch 202 :: Batch 0/1
Batch Loss = 2.6128680301374057
202, epoch_train_loss=2.6128680301374057
Epoch 203
Epoch 203 :: Batch 0/1
Batch Loss = 2.6229130742475415
203, epoch_train_loss=2.6229130742475415
Epoch 204
Epoch 204 :: Batch 0/1
Batch Loss = 2.671552348538442
204, epoch_train_loss=2.671552348538442
Epoch 205
Epoch 205 :: Batch 0/1
Batch Loss = 2.821797186354473
205, epoch_train_loss=2.821797186354473
Epoch 206
Epoch 206 :: Batch 0/1
Batch Loss = 2.781767238176511
206, epoch_train_loss=2.781767238176511
Epoch 207
Epoch 207 :: Batch 0/1
Batch Loss = 2.674169152240632
207, epoch_train_loss=2.674169152240632
Epoch 208
Epoch 208 :: Batch 0/1
Batch Loss = 2.6302255878147216
208, epoch_train_loss=2.6302255878147216
Epoch 209
Epoch 209 :: Batch 0/1
Batch Loss = 2.704510834748593
209, epoch_train_loss=2.704510834748593
Epoch 210
Epoch 210 :: Batch 0/1
Batch Loss = 2.6236057498142036
210, epoch_train_loss=2.6236057498142036
Epoch 211
Epoch 211 :: Batch 0/1
Batch Loss = 2.6652532222441048
211, epoch_train_loss=2.6652532222441048
Epoch 212
Epoch 212 :: Batch 0/1
Batch Loss = 2.6215007268641375
212, epoch_train_loss=2.6215007268641375
Epoch 213
Epoch 213 :: Batch 0/1
Batch Loss = 2.6522399212269496
213, epoch_train_loss=2.6522399212269496
Epoch 214
Epoch 214 :: Batch 0/1
Batch Loss = 2.625177119023756
214, epoch_train_loss=2.625177119023756
Epoch 215
Epoch 215 :: Batch 0/1
Batch Loss = 2.6485940631536145
215, epoch_train_loss=2.6485940631536145
Epoch 216
Epoch 216 :: Batch 0/1
Batch Loss = 2.6183056144033574
216, epoch_train_loss=2.6183056144033574
Epoch 217
Epoch 217 :: Batch 0/1
Batch Loss = 2.6440566545912527
217, epoch_train_loss=2.6440566545912527
Epoch 218
Epoch 218 :: Batch 0/1
Batch Loss = 2.6170224185911652
218, epoch_train_loss=2.6170224185911652
Epoch 219
Epoch 219 :: Batch 0/1
Batch Loss = 2.635337397690582
219, epoch_train_loss=2.635337397690582
Epoch 220
Epoch 220 :: Batch 0/1
Batch Loss = 2.619303253593926
220, epoch_train_loss=2.619303253593926
Epoch 221
Epoch 221 :: Batch 0/1
Batch Loss = 2.627024216320614
221, epoch_train_loss=2.627024216320614
Epoch 222
Epoch 222 :: Batch 0/1
Batch Loss = 2.619985683417335
222, epoch_train_loss=2.619985683417335
Epoch 223
Epoch 223 :: Batch 0/1
Batch Loss = 2.6207902921711903
223, epoch_train_loss=2.6207902921711903
Epoch 224
Epoch 224 :: Batch 0/1
Batch Loss = 2.621484737772555
224, epoch_train_loss=2.621484737772555
Epoch 225
Epoch 225 :: Batch 0/1
Batch Loss = 2.615094530159498
225, epoch_train_loss=2.615094530159498
Epoch 226
Epoch 226 :: Batch 0/1
Batch Loss = 2.61975994509055
226, epoch_train_loss=2.61975994509055
Epoch 227
Epoch 227 :: Batch 0/1
Batch Loss = 2.61202168657993
227, epoch_train_loss=2.61202168657993
Epoch 228
Epoch 228 :: Batch 0/1
Batch Loss = 2.617143288013551
228, epoch_train_loss=2.617143288013551
Epoch 229
Epoch 229 :: Batch 0/1
Batch Loss = 2.6086449796785476
229, epoch_train_loss=2.6086449796785476
Epoch 230
Epoch 230 :: Batch 0/1
Batch Loss = 2.61403277212851
230, epoch_train_loss=2.61403277212851
Epoch 231
Epoch 231 :: Batch 0/1
Batch Loss = 2.6064864487656556
231, epoch_train_loss=2.6064864487656556
Epoch 232
Epoch 232 :: Batch 0/1
Batch Loss = 2.6107130329892057
232, epoch_train_loss=2.6107130329892057
Epoch 233
Epoch 233 :: Batch 0/1
Batch Loss = 2.6044504331790117
233, epoch_train_loss=2.6044504331790117
Epoch 234
Epoch 234 :: Batch 0/1
Batch Loss = 2.607613517980483
234, epoch_train_loss=2.607613517980483
Epoch 235
Epoch 235 :: Batch 0/1
Batch Loss = 2.6030270788503675
235, epoch_train_loss=2.6030270788503675
Epoch 236
Epoch 236 :: Batch 0/1
Batch Loss = 2.6041291032012843
236, epoch_train_loss=2.6041291032012843
Epoch 237
Epoch 237 :: Batch 0/1
Batch Loss = 2.601848914050951
237, epoch_train_loss=2.601848914050951
Epoch 238
Epoch 238 :: Batch 0/1
Batch Loss = 2.6000060852342965
238, epoch_train_loss=2.6000060852342965
Epoch 239
Epoch 239 :: Batch 0/1
Batch Loss = 2.6007549749800662
239, epoch_train_loss=2.6007549749800662
Epoch 240
Epoch 240 :: Batch 0/1
Batch Loss = 2.5962957981241654
240, epoch_train_loss=2.5962957981241654
Epoch 241
Epoch 241 :: Batch 0/1
Batch Loss = 2.598272089486554
241, epoch_train_loss=2.598272089486554
Epoch 242
Epoch 242 :: Batch 0/1
Batch Loss = 2.59462216037106
242, epoch_train_loss=2.59462216037106
Epoch 243
Epoch 243 :: Batch 0/1
Batch Loss = 2.593205974035662
243, epoch_train_loss=2.593205974035662
Epoch 244
Epoch 244 :: Batch 0/1
Batch Loss = 2.593923131873184
244, epoch_train_loss=2.593923131873184
Epoch 245
Epoch 245 :: Batch 0/1
Batch Loss = 2.590111987805976
245, epoch_train_loss=2.590111987805976
Epoch 246
Epoch 246 :: Batch 0/1
Batch Loss = 2.5889176763783324
246, epoch_train_loss=2.5889176763783324
Epoch 247
Epoch 247 :: Batch 0/1
Batch Loss = 2.5894285009924745
247, epoch_train_loss=2.5894285009924745
Epoch 248
Epoch 248 :: Batch 0/1
Batch Loss = 2.5867982224726647
248, epoch_train_loss=2.5867982224726647
Epoch 249
Epoch 249 :: Batch 0/1
Batch Loss = 2.5837815672961635
249, epoch_train_loss=2.5837815672961635
Epoch 250
Epoch 250 :: Batch 0/1
Batch Loss = 2.5833782569369217
250, epoch_train_loss=2.5833782569369217
Epoch 251
Epoch 251 :: Batch 0/1
Batch Loss = 2.583604023844039
251, epoch_train_loss=2.583604023844039
Epoch 252
Epoch 252 :: Batch 0/1
Batch Loss = 2.5822209441021915
252, epoch_train_loss=2.5822209441021915
Epoch 253
Epoch 253 :: Batch 0/1
Batch Loss = 2.580607309438392
253, epoch_train_loss=2.580607309438392
Epoch 254
Epoch 254 :: Batch 0/1
Batch Loss = 2.5807420750266266
254, epoch_train_loss=2.5807420750266266
Epoch 255
Epoch 255 :: Batch 0/1
Batch Loss = 2.583325297739279
255, epoch_train_loss=2.583325297739279
Epoch 256
Epoch 256 :: Batch 0/1
Batch Loss = 2.5778972437313596
256, epoch_train_loss=2.5778972437313596
Epoch 257
Epoch 257 :: Batch 0/1
Batch Loss = 2.5725486197891456
257, epoch_train_loss=2.5725486197891456
Epoch 258
Epoch 258 :: Batch 0/1
Batch Loss = 2.5750236417457018
258, epoch_train_loss=2.5750236417457018
Epoch 259
Epoch 259 :: Batch 0/1
Batch Loss = 2.58307959234086
259, epoch_train_loss=2.58307959234086
Epoch 260
Epoch 260 :: Batch 0/1
Batch Loss = 2.6254149617092466
260, epoch_train_loss=2.6254149617092466
Epoch 261
Epoch 261 :: Batch 0/1
Batch Loss = 2.81367897202441
261, epoch_train_loss=2.81367897202441
Epoch 262
Epoch 262 :: Batch 0/1
Batch Loss = 2.9032002318696835
262, epoch_train_loss=2.9032002318696835
Epoch 263
Epoch 263 :: Batch 0/1
Batch Loss = 2.995830664813578
263, epoch_train_loss=2.995830664813578
Epoch 264
Epoch 264 :: Batch 0/1
Batch Loss = 2.616053121516496
264, epoch_train_loss=2.616053121516496
Epoch 265
Epoch 265 :: Batch 0/1
Batch Loss = 3.077623947079022
265, epoch_train_loss=3.077623947079022
Epoch 266
Epoch 266 :: Batch 0/1
Batch Loss = 2.6382122843117983
266, epoch_train_loss=2.6382122843117983
Epoch 267
Epoch 267 :: Batch 0/1
Batch Loss = 2.887987559681757
267, epoch_train_loss=2.887987559681757
Epoch 268
Epoch 268 :: Batch 0/1
Batch Loss = 2.6725681158047636
268, epoch_train_loss=2.6725681158047636
Epoch 269
Epoch 269 :: Batch 0/1
Batch Loss = 2.6619112512757894
269, epoch_train_loss=2.6619112512757894
Epoch 270
Epoch 270 :: Batch 0/1
Batch Loss = 2.7504215920425272
270, epoch_train_loss=2.7504215920425272
Epoch 271
Epoch 271 :: Batch 0/1
Batch Loss = 2.627481330636398
271, epoch_train_loss=2.627481330636398
Epoch 272
Epoch 272 :: Batch 0/1
Batch Loss = 2.7283600031729076
272, epoch_train_loss=2.7283600031729076
Epoch 273
Epoch 273 :: Batch 0/1
Batch Loss = 2.6956147145222698
273, epoch_train_loss=2.6956147145222698
Epoch 274
Epoch 274 :: Batch 0/1
Batch Loss = 2.6195448893782483
274, epoch_train_loss=2.6195448893782483
Epoch 275
Epoch 275 :: Batch 0/1
Batch Loss = 2.6585424040565293
275, epoch_train_loss=2.6585424040565293
Epoch 276
Epoch 276 :: Batch 0/1
Batch Loss = 2.6431025775152657
276, epoch_train_loss=2.6431025775152657
Epoch 277
Epoch 277 :: Batch 0/1
Batch Loss = 2.6123907698242776
277, epoch_train_loss=2.6123907698242776
Epoch 278
Epoch 278 :: Batch 0/1
Batch Loss = 2.6473499535488534
278, epoch_train_loss=2.6473499535488534
Epoch 279
Epoch 279 :: Batch 0/1
Batch Loss = 2.6416167983934056
279, epoch_train_loss=2.6416167983934056
Epoch 280
Epoch 280 :: Batch 0/1
Batch Loss = 2.6059332355104834
280, epoch_train_loss=2.6059332355104834
Epoch 281
Epoch 281 :: Batch 0/1
Batch Loss = 2.6236791939930555
281, epoch_train_loss=2.6236791939930555
Epoch 282
Epoch 282 :: Batch 0/1
Batch Loss = 2.6286310570101294
282, epoch_train_loss=2.6286310570101294
Epoch 283
Epoch 283 :: Batch 0/1
Batch Loss = 2.6013170762527054
283, epoch_train_loss=2.6013170762527054
Epoch 284
Epoch 284 :: Batch 0/1
Batch Loss = 2.613603977671048
284, epoch_train_loss=2.613603977671048
Epoch 285
Epoch 285 :: Batch 0/1
Batch Loss = 2.620163819732541
285, epoch_train_loss=2.620163819732541
Epoch 286
Epoch 286 :: Batch 0/1
Batch Loss = 2.6009038598483136
286, epoch_train_loss=2.6009038598483136
Epoch 287
Epoch 287 :: Batch 0/1
Batch Loss = 2.602752757845822
287, epoch_train_loss=2.602752757845822
Epoch 288
Epoch 288 :: Batch 0/1
Batch Loss = 2.6115593163177144
288, epoch_train_loss=2.6115593163177144
Epoch 289
Epoch 289 :: Batch 0/1
Batch Loss = 2.595879914625743
289, epoch_train_loss=2.595879914625743
Epoch 290
Epoch 290 :: Batch 0/1
Batch Loss = 2.5966221439469486
290, epoch_train_loss=2.5966221439469486
Epoch 291
Epoch 291 :: Batch 0/1
Batch Loss = 2.602908656254886
291, epoch_train_loss=2.602908656254886
Epoch 292
Epoch 292 :: Batch 0/1
Batch Loss = 2.5906751586098435
292, epoch_train_loss=2.5906751586098435
Epoch 293
Epoch 293 :: Batch 0/1
Batch Loss = 2.5900004661435743
293, epoch_train_loss=2.5900004661435743
Epoch 294
Epoch 294 :: Batch 0/1
Batch Loss = 2.593797802658783
294, epoch_train_loss=2.593797802658783
Epoch 295
Epoch 295 :: Batch 0/1
Batch Loss = 2.5829912636133927
295, epoch_train_loss=2.5829912636133927
Epoch 296
Epoch 296 :: Batch 0/1
Batch Loss = 2.5857086224800576
296, epoch_train_loss=2.5857086224800576
Epoch 297
Epoch 297 :: Batch 0/1
Batch Loss = 2.585206450210481
297, epoch_train_loss=2.585206450210481
Epoch 298
Epoch 298 :: Batch 0/1
Batch Loss = 2.5773055225637065
298, epoch_train_loss=2.5773055225637065
Epoch 299
Epoch 299 :: Batch 0/1
Batch Loss = 2.581284832008792
299, epoch_train_loss=2.581284832008792
Epoch 300
Epoch 300 :: Batch 0/1
Batch Loss = 2.5761796999457394
300, epoch_train_loss=2.5761796999457394
Epoch 301
Epoch 301 :: Batch 0/1
Batch Loss = 2.574104012695228
301, epoch_train_loss=2.574104012695228
Epoch 302
Epoch 302 :: Batch 0/1
Batch Loss = 2.5754350028248525
302, epoch_train_loss=2.5754350028248525
Epoch 303
Epoch 303 :: Batch 0/1
Batch Loss = 2.5691380841532028
303, epoch_train_loss=2.5691380841532028
Epoch 304
Epoch 304 :: Batch 0/1
Batch Loss = 2.5712619752461623
304, epoch_train_loss=2.5712619752461623
Epoch 305
Epoch 305 :: Batch 0/1
Batch Loss = 2.566620230460389
305, epoch_train_loss=2.566620230460389
Epoch 306
Epoch 306 :: Batch 0/1
Batch Loss = 2.5661181156080253
306, epoch_train_loss=2.5661181156080253
Epoch 307
Epoch 307 :: Batch 0/1
Batch Loss = 2.5645267430422414
307, epoch_train_loss=2.5645267430422414
Epoch 308
Epoch 308 :: Batch 0/1
Batch Loss = 2.561444811587035
308, epoch_train_loss=2.561444811587035
Epoch 309
Epoch 309 :: Batch 0/1
Batch Loss = 2.5616627089084734
309, epoch_train_loss=2.5616627089084734
Epoch 310
Epoch 310 :: Batch 0/1
Batch Loss = 2.557942402171837
310, epoch_train_loss=2.557942402171837
Epoch 311
Epoch 311 :: Batch 0/1
Batch Loss = 2.5585450358183492
311, epoch_train_loss=2.5585450358183492
Epoch 312
Epoch 312 :: Batch 0/1
Batch Loss = 2.554714103541984
312, epoch_train_loss=2.554714103541984
Epoch 313
Epoch 313 :: Batch 0/1
Batch Loss = 2.555206176782292
313, epoch_train_loss=2.555206176782292
Epoch 314
Epoch 314 :: Batch 0/1
Batch Loss = 2.55178972512135
314, epoch_train_loss=2.55178972512135
Epoch 315
Epoch 315 :: Batch 0/1
Batch Loss = 2.5519374756209157
315, epoch_train_loss=2.5519374756209157
Epoch 316
Epoch 316 :: Batch 0/1
Batch Loss = 2.5489166097359885
316, epoch_train_loss=2.5489166097359885
Epoch 317
Epoch 317 :: Batch 0/1
Batch Loss = 2.5487607924954703
317, epoch_train_loss=2.5487607924954703
Epoch 318
Epoch 318 :: Batch 0/1
Batch Loss = 2.5462184620891386
318, epoch_train_loss=2.5462184620891386
Epoch 319
Epoch 319 :: Batch 0/1
Batch Loss = 2.5455698758547785
319, epoch_train_loss=2.5455698758547785
Epoch 320
Epoch 320 :: Batch 0/1
Batch Loss = 2.5437649115186383
320, epoch_train_loss=2.5437649115186383
Epoch 321
Epoch 321 :: Batch 0/1
Batch Loss = 2.5422718607453816
321, epoch_train_loss=2.5422718607453816
Epoch 322
Epoch 322 :: Batch 0/1
Batch Loss = 2.5414956865846876
322, epoch_train_loss=2.5414956865846876
Epoch 323
Epoch 323 :: Batch 0/1
Batch Loss = 2.539331120515503
323, epoch_train_loss=2.539331120515503
Epoch 324
Epoch 324 :: Batch 0/1
Batch Loss = 2.5389738283790653
324, epoch_train_loss=2.5389738283790653
Epoch 325
Epoch 325 :: Batch 0/1
Batch Loss = 2.537141357510125
325, epoch_train_loss=2.537141357510125
Epoch 326
Epoch 326 :: Batch 0/1
Batch Loss = 2.535929058330979
326, epoch_train_loss=2.535929058330979
Epoch 327
Epoch 327 :: Batch 0/1
Batch Loss = 2.535187705472891
327, epoch_train_loss=2.535187705472891
Epoch 328
Epoch 328 :: Batch 0/1
Batch Loss = 2.5333553908609105
328, epoch_train_loss=2.5333553908609105
Epoch 329
Epoch 329 :: Batch 0/1
Batch Loss = 2.5324493151062133
329, epoch_train_loss=2.5324493151062133
Epoch 330
Epoch 330 :: Batch 0/1
Batch Loss = 2.531563003350941
330, epoch_train_loss=2.531563003350941
Epoch 331
Epoch 331 :: Batch 0/1
Batch Loss = 2.5299953610155743
331, epoch_train_loss=2.5299953610155743
Epoch 332
Epoch 332 :: Batch 0/1
Batch Loss = 2.528914718660258
332, epoch_train_loss=2.528914718660258
Epoch 333
Epoch 333 :: Batch 0/1
Batch Loss = 2.528194293381925
333, epoch_train_loss=2.528194293381925
Epoch 334
Epoch 334 :: Batch 0/1
Batch Loss = 2.527002313468556
334, epoch_train_loss=2.527002313468556
Epoch 335
Epoch 335 :: Batch 0/1
Batch Loss = 2.5256111591985997
335, epoch_train_loss=2.5256111591985997
Epoch 336
Epoch 336 :: Batch 0/1
Batch Loss = 2.524734983928746
336, epoch_train_loss=2.524734983928746
Epoch 337
Epoch 337 :: Batch 0/1
Batch Loss = 2.5240287907626686
337, epoch_train_loss=2.5240287907626686
Epoch 338
Epoch 338 :: Batch 0/1
Batch Loss = 2.523049887438035
338, epoch_train_loss=2.523049887438035
Epoch 339
Epoch 339 :: Batch 0/1
Batch Loss = 2.521887563483896
339, epoch_train_loss=2.521887563483896
Epoch 340
Epoch 340 :: Batch 0/1
Batch Loss = 2.520753484362552
340, epoch_train_loss=2.520753484362552
Epoch 341
Epoch 341 :: Batch 0/1
Batch Loss = 2.5197641566098468
341, epoch_train_loss=2.5197641566098468
Epoch 342
Epoch 342 :: Batch 0/1
Batch Loss = 2.518944511187284
342, epoch_train_loss=2.518944511187284
Epoch 343
Epoch 343 :: Batch 0/1
Batch Loss = 2.5182454622539967
343, epoch_train_loss=2.5182454622539967
Epoch 344
Epoch 344 :: Batch 0/1
Batch Loss = 2.5177408391793015
344, epoch_train_loss=2.5177408391793015
Epoch 345
Epoch 345 :: Batch 0/1
Batch Loss = 2.5176550943787657
345, epoch_train_loss=2.5176550943787657
Epoch 346
Epoch 346 :: Batch 0/1
Batch Loss = 2.5190355582651875
346, epoch_train_loss=2.5190355582651875
Epoch 347
Epoch 347 :: Batch 0/1
Batch Loss = 2.5240355792750595
347, epoch_train_loss=2.5240355792750595
Epoch 348
Epoch 348 :: Batch 0/1
Batch Loss = 2.5455549120912044
348, epoch_train_loss=2.5455549120912044
Epoch 349
Epoch 349 :: Batch 0/1
Batch Loss = 2.595048667073967
349, epoch_train_loss=2.595048667073967
Epoch 350
Epoch 350 :: Batch 0/1
Batch Loss = 2.7931073444055
350, epoch_train_loss=2.7931073444055
Epoch 351
Epoch 351 :: Batch 0/1
Batch Loss = 2.593179403127474
351, epoch_train_loss=2.593179403127474
Epoch 352
Epoch 352 :: Batch 0/1
Batch Loss = 2.5255658342222422
352, epoch_train_loss=2.5255658342222422
Epoch 353
Epoch 353 :: Batch 0/1
Batch Loss = 2.5204467293219417
353, epoch_train_loss=2.5204467293219417
Epoch 354
Epoch 354 :: Batch 0/1
Batch Loss = 2.561024805778794
354, epoch_train_loss=2.561024805778794
Epoch 355
Epoch 355 :: Batch 0/1
Batch Loss = 2.6125498063049366
355, epoch_train_loss=2.6125498063049366
Epoch 356
Epoch 356 :: Batch 0/1
Batch Loss = 2.528329230831153
356, epoch_train_loss=2.528329230831153
Epoch 357
Epoch 357 :: Batch 0/1
Batch Loss = 2.52627287168527
357, epoch_train_loss=2.52627287168527
Epoch 358
Epoch 358 :: Batch 0/1
Batch Loss = 2.5793912723887433
358, epoch_train_loss=2.5793912723887433
Epoch 359
Epoch 359 :: Batch 0/1
Batch Loss = 2.531984375039214
359, epoch_train_loss=2.531984375039214
Epoch 360
Epoch 360 :: Batch 0/1
Batch Loss = 2.5145484843355446
360, epoch_train_loss=2.5145484843355446
Epoch 361
Epoch 361 :: Batch 0/1
Batch Loss = 2.541863479802114
361, epoch_train_loss=2.541863479802114
Epoch 362
Epoch 362 :: Batch 0/1
Batch Loss = 2.5262663987119547
362, epoch_train_loss=2.5262663987119547
Epoch 363
Epoch 363 :: Batch 0/1
Batch Loss = 2.511651211777296
363, epoch_train_loss=2.511651211777296
Epoch 364
Epoch 364 :: Batch 0/1
Batch Loss = 2.523362982057444
364, epoch_train_loss=2.523362982057444
Epoch 365
Epoch 365 :: Batch 0/1
Batch Loss = 2.5213859281094684
365, epoch_train_loss=2.5213859281094684
Epoch 366
Epoch 366 :: Batch 0/1
Batch Loss = 2.5110328904180523
366, epoch_train_loss=2.5110328904180523
Epoch 367
Epoch 367 :: Batch 0/1
Batch Loss = 2.5137304828858125
367, epoch_train_loss=2.5137304828858125
Epoch 368
Epoch 368 :: Batch 0/1
Batch Loss = 2.5175163054148557
368, epoch_train_loss=2.5175163054148557
Epoch 369
Epoch 369 :: Batch 0/1
Batch Loss = 2.511969405240489
369, epoch_train_loss=2.511969405240489
Epoch 370
Epoch 370 :: Batch 0/1
Batch Loss = 2.508856385755694
370, epoch_train_loss=2.508856385755694
Epoch 371
Epoch 371 :: Batch 0/1
Batch Loss = 2.5133943675900734
371, epoch_train_loss=2.5133943675900734
Epoch 372
Epoch 372 :: Batch 0/1
Batch Loss = 2.5128821111846738
372, epoch_train_loss=2.5128821111846738
Epoch 373
Epoch 373 :: Batch 0/1
Batch Loss = 2.5069053351417754
373, epoch_train_loss=2.5069053351417754
Epoch 374
Epoch 374 :: Batch 0/1
Batch Loss = 2.5086054369416213
374, epoch_train_loss=2.5086054369416213
Epoch 375
Epoch 375 :: Batch 0/1
Batch Loss = 2.511650041978692
375, epoch_train_loss=2.511650041978692
Epoch 376
Epoch 376 :: Batch 0/1
Batch Loss = 2.5069433208022094
376, epoch_train_loss=2.5069433208022094
Epoch 377
Epoch 377 :: Batch 0/1
Batch Loss = 2.5040274664713262
377, epoch_train_loss=2.5040274664713262
Epoch 378
Epoch 378 :: Batch 0/1
Batch Loss = 2.5061485128349443
378, epoch_train_loss=2.5061485128349443
Epoch 379
Epoch 379 :: Batch 0/1
Batch Loss = 2.506783492894805
379, epoch_train_loss=2.506783492894805
Epoch 380
Epoch 380 :: Batch 0/1
Batch Loss = 2.504344152215038
380, epoch_train_loss=2.504344152215038
Epoch 381
Epoch 381 :: Batch 0/1
Batch Loss = 2.50140419698215
381, epoch_train_loss=2.50140419698215
Epoch 382
Epoch 382 :: Batch 0/1
Batch Loss = 2.5011030852038343
382, epoch_train_loss=2.5011030852038343
Epoch 383
Epoch 383 :: Batch 0/1
Batch Loss = 2.502340657998294
383, epoch_train_loss=2.502340657998294
Epoch 384
Epoch 384 :: Batch 0/1
Batch Loss = 2.502360078411431
384, epoch_train_loss=2.502360078411431
Epoch 385
Epoch 385 :: Batch 0/1
Batch Loss = 2.501139241541005
385, epoch_train_loss=2.501139241541005
Epoch 386
Epoch 386 :: Batch 0/1
Batch Loss = 2.498823515612614
386, epoch_train_loss=2.498823515612614
Epoch 387
Epoch 387 :: Batch 0/1
Batch Loss = 2.496798765466308
387, epoch_train_loss=2.496798765466308
Epoch 388
Epoch 388 :: Batch 0/1
Batch Loss = 2.495181332079361
388, epoch_train_loss=2.495181332079361
Epoch 389
Epoch 389 :: Batch 0/1
Batch Loss = 2.493848118763313
389, epoch_train_loss=2.493848118763313
Epoch 390
Epoch 390 :: Batch 0/1
Batch Loss = 2.492680247662222
390, epoch_train_loss=2.492680247662222
Epoch 391
Epoch 391 :: Batch 0/1
Batch Loss = 2.491590227542799
391, epoch_train_loss=2.491590227542799
Epoch 392
Epoch 392 :: Batch 0/1
Batch Loss = 2.4907007712321327
392, epoch_train_loss=2.4907007712321327
Epoch 393
Epoch 393 :: Batch 0/1
Batch Loss = 2.490847837678744
393, epoch_train_loss=2.490847837678744
Epoch 394
Epoch 394 :: Batch 0/1
Batch Loss = 2.496679838628578
394, epoch_train_loss=2.496679838628578
Epoch 395
Epoch 395 :: Batch 0/1
Batch Loss = 2.532703816482202
395, epoch_train_loss=2.532703816482202
Epoch 396
Epoch 396 :: Batch 0/1
Batch Loss = 2.743643760400544
396, epoch_train_loss=2.743643760400544
Epoch 397
Epoch 397 :: Batch 0/1
Batch Loss = 2.8174703896190074
397, epoch_train_loss=2.8174703896190074
Epoch 398
Epoch 398 :: Batch 0/1
Batch Loss = 3.124498877308732
398, epoch_train_loss=3.124498877308732
Epoch 399
Epoch 399 :: Batch 0/1
Batch Loss = 2.723884534591278
399, epoch_train_loss=2.723884534591278
Epoch 400
Epoch 400 :: Batch 0/1
Batch Loss = 3.5408866403888224
400, epoch_train_loss=3.5408866403888224
Epoch 401
Epoch 401 :: Batch 0/1
Batch Loss = 2.92047201318094
401, epoch_train_loss=2.92047201318094
Epoch 402
Epoch 402 :: Batch 0/1
Batch Loss = 3.309889725701295
402, epoch_train_loss=3.309889725701295
Epoch 403
Epoch 403 :: Batch 0/1
Batch Loss = 3.0668392695417017
403, epoch_train_loss=3.0668392695417017
Epoch 404
Epoch 404 :: Batch 0/1
Batch Loss = 2.720948312197844
404, epoch_train_loss=2.720948312197844
Epoch 405
Epoch 405 :: Batch 0/1
Batch Loss = 2.8347292533696438
405, epoch_train_loss=2.8347292533696438
Epoch 406
Epoch 406 :: Batch 0/1
Batch Loss = 2.885765560159867
406, epoch_train_loss=2.885765560159867
Epoch 407
Epoch 407 :: Batch 0/1
Batch Loss = 2.640584063480382
407, epoch_train_loss=2.640584063480382
Epoch 408
Epoch 408 :: Batch 0/1
Batch Loss = 2.7184990176149695
408, epoch_train_loss=2.7184990176149695
Epoch 409
Epoch 409 :: Batch 0/1
Batch Loss = 2.8469682274301977
409, epoch_train_loss=2.8469682274301977
Epoch 410
Epoch 410 :: Batch 0/1
Batch Loss = 3.166186786637525
410, epoch_train_loss=3.166186786637525
Epoch 411
Epoch 411 :: Batch 0/1
Batch Loss = 2.7138645801115726
411, epoch_train_loss=2.7138645801115726
Epoch 412
Epoch 412 :: Batch 0/1
Batch Loss = 2.739967265039403
412, epoch_train_loss=2.739967265039403
Epoch 413
Epoch 413 :: Batch 0/1
Batch Loss = 2.7888717892655484
413, epoch_train_loss=2.7888717892655484
Epoch 414
Epoch 414 :: Batch 0/1
Batch Loss = 2.761015382967762
414, epoch_train_loss=2.761015382967762
Epoch 415
Epoch 415 :: Batch 0/1
Batch Loss = 2.687438014117038
415, epoch_train_loss=2.687438014117038
Epoch 416
Epoch 416 :: Batch 0/1
Batch Loss = 2.733527163529142
416, epoch_train_loss=2.733527163529142
Epoch 417
Epoch 417 :: Batch 0/1
Batch Loss = 2.7429299485312435
417, epoch_train_loss=2.7429299485312435
Epoch 418
Epoch 418 :: Batch 0/1
Batch Loss = 2.741900848531765
418, epoch_train_loss=2.741900848531765
Epoch 419
Epoch 419 :: Batch 0/1
Batch Loss = 2.6947326468669295
419, epoch_train_loss=2.6947326468669295
Epoch 420
Epoch 420 :: Batch 0/1
Batch Loss = 2.6673271065791
420, epoch_train_loss=2.6673271065791
Epoch 421
Epoch 421 :: Batch 0/1
Batch Loss = 2.6741684950251963
421, epoch_train_loss=2.6741684950251963
Epoch 422
Epoch 422 :: Batch 0/1
Batch Loss = 2.6906277768480282
422, epoch_train_loss=2.6906277768480282
Epoch 423
Epoch 423 :: Batch 0/1
Batch Loss = 2.6628517325972965
423, epoch_train_loss=2.6628517325972965
Epoch 424
Epoch 424 :: Batch 0/1
Batch Loss = 2.6340439594989347
424, epoch_train_loss=2.6340439594989347
Epoch 425
Epoch 425 :: Batch 0/1
Batch Loss = 2.636030173971648
425, epoch_train_loss=2.636030173971648
Epoch 426
Epoch 426 :: Batch 0/1
Batch Loss = 2.644461676604363
426, epoch_train_loss=2.644461676604363
Epoch 427
Epoch 427 :: Batch 0/1
Batch Loss = 2.634777821489473
427, epoch_train_loss=2.634777821489473
Epoch 428
Epoch 428 :: Batch 0/1
Batch Loss = 2.611860074491744
428, epoch_train_loss=2.611860074491744
Epoch 429
Epoch 429 :: Batch 0/1
Batch Loss = 2.606941054373161
429, epoch_train_loss=2.606941054373161
Epoch 430
Epoch 430 :: Batch 0/1
Batch Loss = 2.6088739225678013
430, epoch_train_loss=2.6088739225678013
Epoch 431
Epoch 431 :: Batch 0/1
Batch Loss = 2.609557699638682
431, epoch_train_loss=2.609557699638682
Epoch 432
Epoch 432 :: Batch 0/1
Batch Loss = 2.5924757599018453
432, epoch_train_loss=2.5924757599018453
Epoch 433
Epoch 433 :: Batch 0/1
Batch Loss = 2.5916948880173347
433, epoch_train_loss=2.5916948880173347
Epoch 434
Epoch 434 :: Batch 0/1
Batch Loss = 2.5972288281864078
434, epoch_train_loss=2.5972288281864078
Epoch 435
Epoch 435 :: Batch 0/1
Batch Loss = 2.59941353113408
435, epoch_train_loss=2.59941353113408
Epoch 436
Epoch 436 :: Batch 0/1
Batch Loss = 2.590808543659217
436, epoch_train_loss=2.590808543659217
Epoch 437
Epoch 437 :: Batch 0/1
Batch Loss = 2.580251484905909
437, epoch_train_loss=2.580251484905909
Epoch 438
Epoch 438 :: Batch 0/1
Batch Loss = 2.5841448426327855
438, epoch_train_loss=2.5841448426327855
Epoch 439
Epoch 439 :: Batch 0/1
Batch Loss = 2.5819103070095606
439, epoch_train_loss=2.5819103070095606
Epoch 440
Epoch 440 :: Batch 0/1
Batch Loss = 2.5735483718290872
440, epoch_train_loss=2.5735483718290872
Epoch 441
Epoch 441 :: Batch 0/1
Batch Loss = 2.585202672925283
441, epoch_train_loss=2.585202672925283
Epoch 442
Epoch 442 :: Batch 0/1
Batch Loss = 2.576855720305295
442, epoch_train_loss=2.576855720305295
Epoch 443
Epoch 443 :: Batch 0/1
Batch Loss = 2.574200578334791
443, epoch_train_loss=2.574200578334791
Epoch 444
Epoch 444 :: Batch 0/1
Batch Loss = 2.5822830257309723
444, epoch_train_loss=2.5822830257309723
Epoch 445
Epoch 445 :: Batch 0/1
Batch Loss = 2.5653593601428986
445, epoch_train_loss=2.5653593601428986
Epoch 446
Epoch 446 :: Batch 0/1
Batch Loss = 2.570527636470379
446, epoch_train_loss=2.570527636470379
Epoch 447
Epoch 447 :: Batch 0/1
Batch Loss = 2.5660341474129535
447, epoch_train_loss=2.5660341474129535
Epoch 448
Epoch 448 :: Batch 0/1
Batch Loss = 2.562377664682514
448, epoch_train_loss=2.562377664682514
Epoch 449
Epoch 449 :: Batch 0/1
Batch Loss = 2.564716522487867
449, epoch_train_loss=2.564716522487867
Epoch 450
Epoch 450 :: Batch 0/1
Batch Loss = 2.5612124071631546
450, epoch_train_loss=2.5612124071631546
Epoch 451
Epoch 451 :: Batch 0/1
Batch Loss = 2.557193695239205
451, epoch_train_loss=2.557193695239205
Epoch 452
Epoch 452 :: Batch 0/1
Batch Loss = 2.5554697401791016
452, epoch_train_loss=2.5554697401791016
Epoch 453
Epoch 453 :: Batch 0/1
Batch Loss = 2.5549060894690836
453, epoch_train_loss=2.5549060894690836
Epoch 454
Epoch 454 :: Batch 0/1
Batch Loss = 2.548379026307634
454, epoch_train_loss=2.548379026307634
Epoch 455
Epoch 455 :: Batch 0/1
Batch Loss = 2.546883165242989
455, epoch_train_loss=2.546883165242989
Epoch 456
Epoch 456 :: Batch 0/1
Batch Loss = 2.5477874154818885
456, epoch_train_loss=2.5477874154818885
Epoch 457
Epoch 457 :: Batch 0/1
Batch Loss = 2.5489368347045223
457, epoch_train_loss=2.5489368347045223
Epoch 458
Epoch 458 :: Batch 0/1
Batch Loss = 2.5447440244264024
458, epoch_train_loss=2.5447440244264024
Epoch 459
Epoch 459 :: Batch 0/1
Batch Loss = 2.549086537052139
459, epoch_train_loss=2.549086537052139
Epoch 460
Epoch 460 :: Batch 0/1
Batch Loss = 2.55162439196376
460, epoch_train_loss=2.55162439196376
Epoch 461
Epoch 461 :: Batch 0/1
Batch Loss = 2.5559024097978753
461, epoch_train_loss=2.5559024097978753
Epoch 462
Epoch 462 :: Batch 0/1
Batch Loss = 2.5609241408188903
462, epoch_train_loss=2.5609241408188903
Epoch 463
Epoch 463 :: Batch 0/1
Batch Loss = 2.5446253323931884
463, epoch_train_loss=2.5446253323931884
Epoch 464
Epoch 464 :: Batch 0/1
Batch Loss = 2.5306997663305437
464, epoch_train_loss=2.5306997663305437
Epoch 465
Epoch 465 :: Batch 0/1
Batch Loss = 2.5461127093579257
465, epoch_train_loss=2.5461127093579257
Epoch 466
Epoch 466 :: Batch 0/1
Batch Loss = 2.530771473968613
466, epoch_train_loss=2.530771473968613
Epoch 467
Epoch 467 :: Batch 0/1
Batch Loss = 2.5264838576213
467, epoch_train_loss=2.5264838576213
Epoch 468
Epoch 468 :: Batch 0/1
Batch Loss = 2.5255732466500462
468, epoch_train_loss=2.5255732466500462
Epoch 469
Epoch 469 :: Batch 0/1
Batch Loss = 2.5243573386060816
469, epoch_train_loss=2.5243573386060816
Epoch 470
Epoch 470 :: Batch 0/1
Batch Loss = 2.521906659894165
470, epoch_train_loss=2.521906659894165
Epoch 471
Epoch 471 :: Batch 0/1
Batch Loss = 2.521469592786684
471, epoch_train_loss=2.521469592786684
Epoch 472
Epoch 472 :: Batch 0/1
Batch Loss = 2.5202437135853737
472, epoch_train_loss=2.5202437135853737
Epoch 473
Epoch 473 :: Batch 0/1
Batch Loss = 2.5175775243066387
473, epoch_train_loss=2.5175775243066387
Epoch 474
Epoch 474 :: Batch 0/1
Batch Loss = 2.515126127104727
474, epoch_train_loss=2.515126127104727
Epoch 475
Epoch 475 :: Batch 0/1
Batch Loss = 2.5142882102094934
475, epoch_train_loss=2.5142882102094934
Epoch 476
Epoch 476 :: Batch 0/1
Batch Loss = 2.5122160259700026
476, epoch_train_loss=2.5122160259700026
Epoch 477
Epoch 477 :: Batch 0/1
Batch Loss = 2.511445953089875
477, epoch_train_loss=2.511445953089875
Epoch 478
Epoch 478 :: Batch 0/1
Batch Loss = 2.5099061995487437
478, epoch_train_loss=2.5099061995487437
Epoch 479
Epoch 479 :: Batch 0/1
Batch Loss = 2.507738826959918
479, epoch_train_loss=2.507738826959918
Epoch 480
Epoch 480 :: Batch 0/1
Batch Loss = 2.5069213702677917
480, epoch_train_loss=2.5069213702677917
Epoch 481
Epoch 481 :: Batch 0/1
Batch Loss = 2.5053803445167175
481, epoch_train_loss=2.5053803445167175
Epoch 482
Epoch 482 :: Batch 0/1
Batch Loss = 2.504635779419851
482, epoch_train_loss=2.504635779419851
Epoch 483
Epoch 483 :: Batch 0/1
Batch Loss = 2.504370184844259
483, epoch_train_loss=2.504370184844259
Epoch 484
Epoch 484 :: Batch 0/1
Batch Loss = 2.503141730653606
484, epoch_train_loss=2.503141730653606
Epoch 485
Epoch 485 :: Batch 0/1
Batch Loss = 2.502713551036024
485, epoch_train_loss=2.502713551036024
Epoch 486
Epoch 486 :: Batch 0/1
Batch Loss = 2.5028484454108866
486, epoch_train_loss=2.5028484454108866
Epoch 487
Epoch 487 :: Batch 0/1
Batch Loss = 2.5022713223603
487, epoch_train_loss=2.5022713223603
Epoch 488
Epoch 488 :: Batch 0/1
Batch Loss = 2.501428031091176
488, epoch_train_loss=2.501428031091176
Epoch 489
Epoch 489 :: Batch 0/1
Batch Loss = 2.500904224245734
489, epoch_train_loss=2.500904224245734
Epoch 490
Epoch 490 :: Batch 0/1
Batch Loss = 2.500563977480887
490, epoch_train_loss=2.500563977480887
Epoch 491
Epoch 491 :: Batch 0/1
Batch Loss = 2.4996125796803503
491, epoch_train_loss=2.4996125796803503
Epoch 492
Epoch 492 :: Batch 0/1
Batch Loss = 2.4985874956966465
492, epoch_train_loss=2.4985874956966465
Epoch 493
Epoch 493 :: Batch 0/1
Batch Loss = 2.498031714776334
493, epoch_train_loss=2.498031714776334
Epoch 494
Epoch 494 :: Batch 0/1
Batch Loss = 2.4974590760020483
494, epoch_train_loss=2.4974590760020483
Epoch 495
Epoch 495 :: Batch 0/1
Batch Loss = 2.4967089081183245
495, epoch_train_loss=2.4967089081183245
Epoch 496
Epoch 496 :: Batch 0/1
Batch Loss = 2.495829050526685
496, epoch_train_loss=2.495829050526685
Epoch 497
Epoch 497 :: Batch 0/1
Batch Loss = 2.4951449159610153
497, epoch_train_loss=2.4951449159610153
Epoch 498
Epoch 498 :: Batch 0/1
Batch Loss = 2.4945241198337205
498, epoch_train_loss=2.4945241198337205
Epoch 499
Epoch 499 :: Batch 0/1
Batch Loss = 2.493841834994686
499, epoch_train_loss=2.493841834994686
Epoch 500
Epoch 500 :: Batch 0/1
Batch Loss = 2.4930468577360565
500, epoch_train_loss=2.4930468577360565
Epoch 501
Epoch 501 :: Batch 0/1
Batch Loss = 2.492283555032076
501, epoch_train_loss=2.492283555032076
Epoch 502
Epoch 502 :: Batch 0/1
Batch Loss = 2.4915678237910255
502, epoch_train_loss=2.4915678237910255
Epoch 503
Epoch 503 :: Batch 0/1
Batch Loss = 2.490837684464792
503, epoch_train_loss=2.490837684464792
Epoch 504
Epoch 504 :: Batch 0/1
Batch Loss = 2.4901497284477987
504, epoch_train_loss=2.4901497284477987
Epoch 505
Epoch 505 :: Batch 0/1
Batch Loss = 2.489484798962896
505, epoch_train_loss=2.489484798962896
Epoch 506
Epoch 506 :: Batch 0/1
Batch Loss = 2.4888208289316025
506, epoch_train_loss=2.4888208289316025
Epoch 507
Epoch 507 :: Batch 0/1
Batch Loss = 2.4880571744317206
507, epoch_train_loss=2.4880571744317206
Epoch 508
Epoch 508 :: Batch 0/1
Batch Loss = 2.4872953882382984
508, epoch_train_loss=2.4872953882382984
Epoch 509
Epoch 509 :: Batch 0/1
Batch Loss = 2.4865437454666726
509, epoch_train_loss=2.4865437454666726
Epoch 510
Epoch 510 :: Batch 0/1
Batch Loss = 2.4857707673776948
510, epoch_train_loss=2.4857707673776948
Epoch 511
Epoch 511 :: Batch 0/1
Batch Loss = 2.485001861222959
511, epoch_train_loss=2.485001861222959
Epoch 512
Epoch 512 :: Batch 0/1
Batch Loss = 2.4842745675593934
512, epoch_train_loss=2.4842745675593934
Epoch 513
Epoch 513 :: Batch 0/1
Batch Loss = 2.483639628063133
513, epoch_train_loss=2.483639628063133
Epoch 514
Epoch 514 :: Batch 0/1
Batch Loss = 2.4832631805217265
514, epoch_train_loss=2.4832631805217265
Epoch 515
Epoch 515 :: Batch 0/1
Batch Loss = 2.483636720276045
515, epoch_train_loss=2.483636720276045
Epoch 516
Epoch 516 :: Batch 0/1
Batch Loss = 2.4862321139446295
516, epoch_train_loss=2.4862321139446295
Epoch 517
Epoch 517 :: Batch 0/1
Batch Loss = 2.493994229874718
517, epoch_train_loss=2.493994229874718
Epoch 518
Epoch 518 :: Batch 0/1
Batch Loss = 2.518078411611918
518, epoch_train_loss=2.518078411611918
Epoch 519
Epoch 519 :: Batch 0/1
Batch Loss = 2.5614730105967913
519, epoch_train_loss=2.5614730105967913
Epoch 520
Epoch 520 :: Batch 0/1
Batch Loss = 2.6567630075475375
520, epoch_train_loss=2.6567630075475375
Epoch 521
Epoch 521 :: Batch 0/1
Batch Loss = 2.588188029611987
521, epoch_train_loss=2.588188029611987
Epoch 522
Epoch 522 :: Batch 0/1
Batch Loss = 2.5108099082951285
522, epoch_train_loss=2.5108099082951285
Epoch 523
Epoch 523 :: Batch 0/1
Batch Loss = 2.4832869631996055
523, epoch_train_loss=2.4832869631996055
Epoch 524
Epoch 524 :: Batch 0/1
Batch Loss = 2.5350315927635547
524, epoch_train_loss=2.5350315927635547
Epoch 525
Epoch 525 :: Batch 0/1
Batch Loss = 2.513600876935783
525, epoch_train_loss=2.513600876935783
Epoch 526
Epoch 526 :: Batch 0/1
Batch Loss = 2.4842472455127727
526, epoch_train_loss=2.4842472455127727
Epoch 527
Epoch 527 :: Batch 0/1
Batch Loss = 2.526966091244806
527, epoch_train_loss=2.526966091244806
Epoch 528
Epoch 528 :: Batch 0/1
Batch Loss = 2.485269288037478
528, epoch_train_loss=2.485269288037478
Epoch 529
Epoch 529 :: Batch 0/1
Batch Loss = 2.495557952290874
529, epoch_train_loss=2.495557952290874
Epoch 530
Epoch 530 :: Batch 0/1
Batch Loss = 2.4938231936702486
530, epoch_train_loss=2.4938231936702486
Epoch 531
Epoch 531 :: Batch 0/1
Batch Loss = 2.4736540949972117
531, epoch_train_loss=2.4736540949972117
Epoch 532
Epoch 532 :: Batch 0/1
Batch Loss = 2.491731991569759
532, epoch_train_loss=2.491731991569759
Epoch 533
Epoch 533 :: Batch 0/1
Batch Loss = 2.4673088697645174
533, epoch_train_loss=2.4673088697645174
Epoch 534
Epoch 534 :: Batch 0/1
Batch Loss = 2.4841828407373545
534, epoch_train_loss=2.4841828407373545
Epoch 535
Epoch 535 :: Batch 0/1
Batch Loss = 2.468931105186117
535, epoch_train_loss=2.468931105186117
Epoch 536
Epoch 536 :: Batch 0/1
Batch Loss = 2.4698588357843976
536, epoch_train_loss=2.4698588357843976
Epoch 537
Epoch 537 :: Batch 0/1
Batch Loss = 2.4705785495908805
537, epoch_train_loss=2.4705785495908805
Epoch 538
Epoch 538 :: Batch 0/1
Batch Loss = 2.457101050293081
538, epoch_train_loss=2.457101050293081
Epoch 539
Epoch 539 :: Batch 0/1
Batch Loss = 2.465961485719318
539, epoch_train_loss=2.465961485719318
Epoch 540
Epoch 540 :: Batch 0/1
Batch Loss = 2.452309761617582
540, epoch_train_loss=2.452309761617582
Epoch 541
Epoch 541 :: Batch 0/1
Batch Loss = 2.454571558801789
541, epoch_train_loss=2.454571558801789
Epoch 542
Epoch 542 :: Batch 0/1
Batch Loss = 2.451875620686125
542, epoch_train_loss=2.451875620686125
Epoch 543
Epoch 543 :: Batch 0/1
Batch Loss = 2.4418048486833035
543, epoch_train_loss=2.4418048486833035
Epoch 544
Epoch 544 :: Batch 0/1
Batch Loss = 2.445928677656139
544, epoch_train_loss=2.445928677656139
Epoch 545
Epoch 545 :: Batch 0/1
Batch Loss = 2.4358337061110222
545, epoch_train_loss=2.4358337061110222
Epoch 546
Epoch 546 :: Batch 0/1
Batch Loss = 2.4311166745937283
546, epoch_train_loss=2.4311166745937283
Epoch 547
Epoch 547 :: Batch 0/1
Batch Loss = 2.4304056550862896
547, epoch_train_loss=2.4304056550862896
Epoch 548
Epoch 548 :: Batch 0/1
Batch Loss = 2.4191762460701938
548, epoch_train_loss=2.4191762460701938
Epoch 549
Epoch 549 :: Batch 0/1
Batch Loss = 2.415863513696918
549, epoch_train_loss=2.415863513696918
Epoch 550
Epoch 550 :: Batch 0/1
Batch Loss = 2.4109654390593875
550, epoch_train_loss=2.4109654390593875
Epoch 551
Epoch 551 :: Batch 0/1
Batch Loss = 2.400333951817945
551, epoch_train_loss=2.400333951817945
Epoch 552
Epoch 552 :: Batch 0/1
Batch Loss = 2.3959374766601758
552, epoch_train_loss=2.3959374766601758
Epoch 553
Epoch 553 :: Batch 0/1
Batch Loss = 2.3897455730745065
553, epoch_train_loss=2.3897455730745065
Epoch 554
Epoch 554 :: Batch 0/1
Batch Loss = 2.379594599057326
554, epoch_train_loss=2.379594599057326
Epoch 555
Epoch 555 :: Batch 0/1
Batch Loss = 2.3730678977784723
555, epoch_train_loss=2.3730678977784723
Epoch 556
Epoch 556 :: Batch 0/1
Batch Loss = 2.366045342743096
556, epoch_train_loss=2.366045342743096
Epoch 557
Epoch 557 :: Batch 0/1
Batch Loss = 2.3551368174966716
557, epoch_train_loss=2.3551368174966716
Epoch 558
Epoch 558 :: Batch 0/1
Batch Loss = 2.344867119181112
558, epoch_train_loss=2.344867119181112
Epoch 559
Epoch 559 :: Batch 0/1
Batch Loss = 2.3364192621685285
559, epoch_train_loss=2.3364192621685285
Epoch 560
Epoch 560 :: Batch 0/1
Batch Loss = 2.329625897814868
560, epoch_train_loss=2.329625897814868
Epoch 561
Epoch 561 :: Batch 0/1
Batch Loss = 2.3231287337295417
561, epoch_train_loss=2.3231287337295417
Epoch 562
Epoch 562 :: Batch 0/1
Batch Loss = 2.317278386737552
562, epoch_train_loss=2.317278386737552
Epoch 563
Epoch 563 :: Batch 0/1
Batch Loss = 2.31750421666664
563, epoch_train_loss=2.31750421666664
Epoch 564
Epoch 564 :: Batch 0/1
Batch Loss = 2.34778378020784
564, epoch_train_loss=2.34778378020784
Epoch 565
Epoch 565 :: Batch 0/1
Batch Loss = 2.4785552232531938
565, epoch_train_loss=2.4785552232531938
Epoch 566
Epoch 566 :: Batch 0/1
Batch Loss = 2.7983219423683408
566, epoch_train_loss=2.7983219423683408
Epoch 567
Epoch 567 :: Batch 0/1
Batch Loss = 2.587682780824958
567, epoch_train_loss=2.587682780824958
Epoch 568
Epoch 568 :: Batch 0/1
Batch Loss = 2.404980402333826
568, epoch_train_loss=2.404980402333826
Epoch 569
Epoch 569 :: Batch 0/1
Batch Loss = 2.678269237653735
569, epoch_train_loss=2.678269237653735
Epoch 570
Epoch 570 :: Batch 0/1
Batch Loss = 2.3315324288385777
570, epoch_train_loss=2.3315324288385777
Epoch 571
Epoch 571 :: Batch 0/1
Batch Loss = 2.530321591404825
571, epoch_train_loss=2.530321591404825
Epoch 572
Epoch 572 :: Batch 0/1
Batch Loss = 2.363404122084067
572, epoch_train_loss=2.363404122084067
Epoch 573
Epoch 573 :: Batch 0/1
Batch Loss = 2.4786525597743245
573, epoch_train_loss=2.4786525597743245
Epoch 574
Epoch 574 :: Batch 0/1
Batch Loss = 2.3918180065449284
574, epoch_train_loss=2.3918180065449284
Epoch 575
Epoch 575 :: Batch 0/1
Batch Loss = 2.3549046224029424
575, epoch_train_loss=2.3549046224029424
Epoch 576
Epoch 576 :: Batch 0/1
Batch Loss = 2.4201615152256775
576, epoch_train_loss=2.4201615152256775
Epoch 577
Epoch 577 :: Batch 0/1
Batch Loss = 2.3285043375819723
577, epoch_train_loss=2.3285043375819723
Epoch 578
Epoch 578 :: Batch 0/1
Batch Loss = 2.360044418428933
578, epoch_train_loss=2.360044418428933
Epoch 579
Epoch 579 :: Batch 0/1
Batch Loss = 2.3603909616377297
579, epoch_train_loss=2.3603909616377297
Epoch 580
Epoch 580 :: Batch 0/1
Batch Loss = 2.2958275056818493
580, epoch_train_loss=2.2958275056818493
Epoch 581
Epoch 581 :: Batch 0/1
Batch Loss = 2.3067629787374107
581, epoch_train_loss=2.3067629787374107
Epoch 582
Epoch 582 :: Batch 0/1
Batch Loss = 2.3079098682506993
582, epoch_train_loss=2.3079098682506993
Epoch 583
Epoch 583 :: Batch 0/1
Batch Loss = 2.2669071562498666
583, epoch_train_loss=2.2669071562498666
Epoch 584
Epoch 584 :: Batch 0/1
Batch Loss = 2.2801364319734883
584, epoch_train_loss=2.2801364319734883
Epoch 585
Epoch 585 :: Batch 0/1
Batch Loss = 2.2884339834097194
585, epoch_train_loss=2.2884339834097194
Epoch 586
Epoch 586 :: Batch 0/1
Batch Loss = 2.257360843941269
586, epoch_train_loss=2.257360843941269
Epoch 587
Epoch 587 :: Batch 0/1
Batch Loss = 2.2514273945365226
587, epoch_train_loss=2.2514273945365226
Epoch 588
Epoch 588 :: Batch 0/1
Batch Loss = 2.255035666413275
588, epoch_train_loss=2.255035666413275
Epoch 589
Epoch 589 :: Batch 0/1
Batch Loss = 2.2479838715993825
589, epoch_train_loss=2.2479838715993825
Epoch 590
Epoch 590 :: Batch 0/1
Batch Loss = 2.246273878309568
590, epoch_train_loss=2.246273878309568
Epoch 591
Epoch 591 :: Batch 0/1
Batch Loss = 2.2680203560189196
591, epoch_train_loss=2.2680203560189196
Epoch 592
Epoch 592 :: Batch 0/1
Batch Loss = 2.3113102761979736
592, epoch_train_loss=2.3113102761979736
Epoch 593
Epoch 593 :: Batch 0/1
Batch Loss = 2.6692364575534886
593, epoch_train_loss=2.6692364575534886
Epoch 594
Epoch 594 :: Batch 0/1
Batch Loss = 2.555678458220792
594, epoch_train_loss=2.555678458220792
Epoch 595
Epoch 595 :: Batch 0/1
Batch Loss = 2.586851819902278
595, epoch_train_loss=2.586851819902278
Epoch 596
Epoch 596 :: Batch 0/1
Batch Loss = 2.6139590382394426
596, epoch_train_loss=2.6139590382394426
Epoch 597
Epoch 597 :: Batch 0/1
Batch Loss = 2.7123926355432784
597, epoch_train_loss=2.7123926355432784
Epoch 598
Epoch 598 :: Batch 0/1
Batch Loss = 2.499106697170738
598, epoch_train_loss=2.499106697170738
Epoch 599
Epoch 599 :: Batch 0/1
Batch Loss = 2.54157571223145
599, epoch_train_loss=2.54157571223145
Epoch 600
Epoch 600 :: Batch 0/1
Batch Loss = 2.425024892894833
600, epoch_train_loss=2.425024892894833
Epoch 601
Epoch 601 :: Batch 0/1
Batch Loss = 2.562477179585626
601, epoch_train_loss=2.562477179585626
Epoch 602
Epoch 602 :: Batch 0/1
Batch Loss = 2.4393746679430004
602, epoch_train_loss=2.4393746679430004
Epoch 603
Epoch 603 :: Batch 0/1
Batch Loss = 2.356005747412139
603, epoch_train_loss=2.356005747412139
Epoch 604
Epoch 604 :: Batch 0/1
Batch Loss = 2.45541934646771
604, epoch_train_loss=2.45541934646771
Epoch 605
Epoch 605 :: Batch 0/1
Batch Loss = 2.3666682701796975
605, epoch_train_loss=2.3666682701796975
Epoch 606
Epoch 606 :: Batch 0/1
Batch Loss = 2.350196126930516
606, epoch_train_loss=2.350196126930516
Epoch 607
Epoch 607 :: Batch 0/1
Batch Loss = 2.3955509161771964
607, epoch_train_loss=2.3955509161771964
Epoch 608
Epoch 608 :: Batch 0/1
Batch Loss = 2.3449316293249676
608, epoch_train_loss=2.3449316293249676
Epoch 609
Epoch 609 :: Batch 0/1
Batch Loss = 2.292402812101978
609, epoch_train_loss=2.292402812101978
Epoch 610
Epoch 610 :: Batch 0/1
Batch Loss = 2.3056493674122303
610, epoch_train_loss=2.3056493674122303
Epoch 611
Epoch 611 :: Batch 0/1
Batch Loss = 2.3317838290876436
611, epoch_train_loss=2.3317838290876436
Epoch 612
Epoch 612 :: Batch 0/1
Batch Loss = 2.31863903259231
612, epoch_train_loss=2.31863903259231
Epoch 613
Epoch 613 :: Batch 0/1
Batch Loss = 2.2728185721955168
613, epoch_train_loss=2.2728185721955168
Epoch 614
Epoch 614 :: Batch 0/1
Batch Loss = 2.27749116670011
614, epoch_train_loss=2.27749116670011
Epoch 615
Epoch 615 :: Batch 0/1
Batch Loss = 2.280491378369424
615, epoch_train_loss=2.280491378369424
Epoch 616
Epoch 616 :: Batch 0/1
Batch Loss = 2.2833435587878044
616, epoch_train_loss=2.2833435587878044
Epoch 617
Epoch 617 :: Batch 0/1
Batch Loss = 2.253286612931686
617, epoch_train_loss=2.253286612931686
Epoch 618
Epoch 618 :: Batch 0/1
Batch Loss = 2.2580180748632754
618, epoch_train_loss=2.2580180748632754
Epoch 619
Epoch 619 :: Batch 0/1
Batch Loss = 2.254084359672756
619, epoch_train_loss=2.254084359672756
Epoch 620
Epoch 620 :: Batch 0/1
Batch Loss = 2.230784428730994
620, epoch_train_loss=2.230784428730994
Epoch 621
Epoch 621 :: Batch 0/1
Batch Loss = 2.2485633960009896
621, epoch_train_loss=2.2485633960009896
Epoch 622
Epoch 622 :: Batch 0/1
Batch Loss = 2.2376316716844067
622, epoch_train_loss=2.2376316716844067
Epoch 623
Epoch 623 :: Batch 0/1
Batch Loss = 2.2210553517730176
623, epoch_train_loss=2.2210553517730176
Epoch 624
Epoch 624 :: Batch 0/1
Batch Loss = 2.2254802470123387
624, epoch_train_loss=2.2254802470123387
Epoch 625
Epoch 625 :: Batch 0/1
Batch Loss = 2.2237446029137886
625, epoch_train_loss=2.2237446029137886
Epoch 626
Epoch 626 :: Batch 0/1
Batch Loss = 2.214350145736623
626, epoch_train_loss=2.214350145736623
Epoch 627
Epoch 627 :: Batch 0/1
Batch Loss = 2.2074703277915253
627, epoch_train_loss=2.2074703277915253
Epoch 628
Epoch 628 :: Batch 0/1
Batch Loss = 2.207604420845175
628, epoch_train_loss=2.207604420845175
Epoch 629
Epoch 629 :: Batch 0/1
Batch Loss = 2.2229421192210324
629, epoch_train_loss=2.2229421192210324
Epoch 630
Epoch 630 :: Batch 0/1
Batch Loss = 2.2458981118485184
630, epoch_train_loss=2.2458981118485184
Epoch 631
Epoch 631 :: Batch 0/1
Batch Loss = 2.3430141168447913
631, epoch_train_loss=2.3430141168447913
Epoch 632
Epoch 632 :: Batch 0/1
Batch Loss = 2.2750949526721054
632, epoch_train_loss=2.2750949526721054
Epoch 633
Epoch 633 :: Batch 0/1
Batch Loss = 2.2588154898093347
633, epoch_train_loss=2.2588154898093347
Epoch 634
Epoch 634 :: Batch 0/1
Batch Loss = 2.189108948141712
634, epoch_train_loss=2.189108948141712
Epoch 635
Epoch 635 :: Batch 0/1
Batch Loss = 2.176397368211363
635, epoch_train_loss=2.176397368211363
Epoch 636
Epoch 636 :: Batch 0/1
Batch Loss = 2.213166866267248
636, epoch_train_loss=2.213166866267248
Epoch 637
Epoch 637 :: Batch 0/1
Batch Loss = 2.2756239227947685
637, epoch_train_loss=2.2756239227947685
Epoch 638
Epoch 638 :: Batch 0/1
Batch Loss = 2.4334598538546857
638, epoch_train_loss=2.4334598538546857
Epoch 639
Epoch 639 :: Batch 0/1
Batch Loss = 2.1931352176617938
639, epoch_train_loss=2.1931352176617938
Epoch 640
Epoch 640 :: Batch 0/1
Batch Loss = 2.318132559029417
640, epoch_train_loss=2.318132559029417
Epoch 641
Epoch 641 :: Batch 0/1
Batch Loss = 2.4879874687811143
641, epoch_train_loss=2.4879874687811143
Epoch 642
Epoch 642 :: Batch 0/1
Batch Loss = 2.255381994741561
642, epoch_train_loss=2.255381994741561
Epoch 643
Epoch 643 :: Batch 0/1
Batch Loss = 2.7738502364256545
643, epoch_train_loss=2.7738502364256545
Epoch 644
Epoch 644 :: Batch 0/1
Batch Loss = 2.405303962817192
644, epoch_train_loss=2.405303962817192
Epoch 645
Epoch 645 :: Batch 0/1
Batch Loss = 2.580589566365183
645, epoch_train_loss=2.580589566365183
Epoch 646
Epoch 646 :: Batch 0/1
Batch Loss = 2.3182966436261045
646, epoch_train_loss=2.3182966436261045
Epoch 647
Epoch 647 :: Batch 0/1
Batch Loss = 2.405781237969926
647, epoch_train_loss=2.405781237969926
Epoch 648
Epoch 648 :: Batch 0/1
Batch Loss = 2.3987634949713144
648, epoch_train_loss=2.3987634949713144
Epoch 649
Epoch 649 :: Batch 0/1
Batch Loss = 2.252117143227249
649, epoch_train_loss=2.252117143227249
Epoch 650
Epoch 650 :: Batch 0/1
Batch Loss = 2.406475583275151
650, epoch_train_loss=2.406475583275151
Epoch 651
Epoch 651 :: Batch 0/1
Batch Loss = 2.273959047428163
651, epoch_train_loss=2.273959047428163
Epoch 652
Epoch 652 :: Batch 0/1
Batch Loss = 2.3020799011745403
652, epoch_train_loss=2.3020799011745403
Epoch 653
Epoch 653 :: Batch 0/1
Batch Loss = 2.346735777617119
653, epoch_train_loss=2.346735777617119
Epoch 654
Epoch 654 :: Batch 0/1
Batch Loss = 2.2496353184548177
654, epoch_train_loss=2.2496353184548177
Epoch 655
Epoch 655 :: Batch 0/1
Batch Loss = 2.3085548406682417
655, epoch_train_loss=2.3085548406682417
Epoch 656
Epoch 656 :: Batch 0/1
Batch Loss = 2.296663344567907
656, epoch_train_loss=2.296663344567907
Epoch 657
Epoch 657 :: Batch 0/1
Batch Loss = 2.256591471582458
657, epoch_train_loss=2.256591471582458
Epoch 658
Epoch 658 :: Batch 0/1
Batch Loss = 2.261606487814154
658, epoch_train_loss=2.261606487814154
Epoch 659
Epoch 659 :: Batch 0/1
Batch Loss = 2.2638845197616844
659, epoch_train_loss=2.2638845197616844
Epoch 660
Epoch 660 :: Batch 0/1
Batch Loss = 2.2273279390571834
660, epoch_train_loss=2.2273279390571834
Epoch 661
Epoch 661 :: Batch 0/1
Batch Loss = 2.2585826033644913
661, epoch_train_loss=2.2585826033644913
Epoch 662
Epoch 662 :: Batch 0/1
Batch Loss = 2.222189134540599
662, epoch_train_loss=2.222189134540599
Epoch 663
Epoch 663 :: Batch 0/1
Batch Loss = 2.212828396617063
663, epoch_train_loss=2.212828396617063
Epoch 664
Epoch 664 :: Batch 0/1
Batch Loss = 2.231350996063418
664, epoch_train_loss=2.231350996063418
Epoch 665
Epoch 665 :: Batch 0/1
Batch Loss = 2.200865040246543
665, epoch_train_loss=2.200865040246543
Epoch 666
Epoch 666 :: Batch 0/1
Batch Loss = 2.2048378034782448
666, epoch_train_loss=2.2048378034782448
Epoch 667
Epoch 667 :: Batch 0/1
Batch Loss = 2.2031027366887126
667, epoch_train_loss=2.2031027366887126
Epoch 668
Epoch 668 :: Batch 0/1
Batch Loss = 2.1798702992095844
668, epoch_train_loss=2.1798702992095844
Epoch 669
Epoch 669 :: Batch 0/1
Batch Loss = 2.1914319865455023
669, epoch_train_loss=2.1914319865455023
Epoch 670
Epoch 670 :: Batch 0/1
Batch Loss = 2.17467932793408
670, epoch_train_loss=2.17467932793408
Epoch 671
Epoch 671 :: Batch 0/1
Batch Loss = 2.181200617634868
671, epoch_train_loss=2.181200617634868
Epoch 672
Epoch 672 :: Batch 0/1
Batch Loss = 2.159573704446592
672, epoch_train_loss=2.159573704446592
Epoch 673
Epoch 673 :: Batch 0/1
Batch Loss = 2.17199743222307
673, epoch_train_loss=2.17199743222307
Epoch 674
Epoch 674 :: Batch 0/1
Batch Loss = 2.1543138932299732
674, epoch_train_loss=2.1543138932299732
Epoch 675
Epoch 675 :: Batch 0/1
Batch Loss = 2.157733430581705
675, epoch_train_loss=2.157733430581705
Epoch 676
Epoch 676 :: Batch 0/1
Batch Loss = 2.1464429205435565
676, epoch_train_loss=2.1464429205435565
Epoch 677
Epoch 677 :: Batch 0/1
Batch Loss = 2.151588082231878
677, epoch_train_loss=2.151588082231878
Epoch 678
Epoch 678 :: Batch 0/1
Batch Loss = 2.1405586216816794
678, epoch_train_loss=2.1405586216816794
Epoch 679
Epoch 679 :: Batch 0/1
Batch Loss = 2.1402510721219254
679, epoch_train_loss=2.1402510721219254
Epoch 680
Epoch 680 :: Batch 0/1
Batch Loss = 2.1391740709127114
680, epoch_train_loss=2.1391740709127114
Epoch 681
Epoch 681 :: Batch 0/1
Batch Loss = 2.127479339800707
681, epoch_train_loss=2.127479339800707
Epoch 682
Epoch 682 :: Batch 0/1
Batch Loss = 2.1313678869810513
682, epoch_train_loss=2.1313678869810513
Epoch 683
Epoch 683 :: Batch 0/1
Batch Loss = 2.125722707581633
683, epoch_train_loss=2.125722707581633
Epoch 684
Epoch 684 :: Batch 0/1
Batch Loss = 2.117653397410682
684, epoch_train_loss=2.117653397410682
Epoch 685
Epoch 685 :: Batch 0/1
Batch Loss = 2.1201809365458297
685, epoch_train_loss=2.1201809365458297
Epoch 686
Epoch 686 :: Batch 0/1
Batch Loss = 2.117892765106165
686, epoch_train_loss=2.117892765106165
Epoch 687
Epoch 687 :: Batch 0/1
Batch Loss = 2.110051645396177
687, epoch_train_loss=2.110051645396177
Epoch 688
Epoch 688 :: Batch 0/1
Batch Loss = 2.105294522938392
688, epoch_train_loss=2.105294522938392
Epoch 689
Epoch 689 :: Batch 0/1
Batch Loss = 2.1065882949278225
689, epoch_train_loss=2.1065882949278225
Epoch 690
Epoch 690 :: Batch 0/1
Batch Loss = 2.1071514107129143
690, epoch_train_loss=2.1071514107129143
Epoch 691
Epoch 691 :: Batch 0/1
Batch Loss = 2.101267366088585
691, epoch_train_loss=2.101267366088585
Epoch 692
Epoch 692 :: Batch 0/1
Batch Loss = 2.0945193230617125
692, epoch_train_loss=2.0945193230617125
Epoch 693
Epoch 693 :: Batch 0/1
Batch Loss = 2.0896080302064433
693, epoch_train_loss=2.0896080302064433
Epoch 694
Epoch 694 :: Batch 0/1
Batch Loss = 2.086285056954481
694, epoch_train_loss=2.086285056954481
Epoch 695
Epoch 695 :: Batch 0/1
Batch Loss = 2.085344821529892
695, epoch_train_loss=2.085344821529892
Epoch 696
Epoch 696 :: Batch 0/1
Batch Loss = 2.0870894734424033
696, epoch_train_loss=2.0870894734424033
Epoch 697
Epoch 697 :: Batch 0/1
Batch Loss = 2.094951390443082
697, epoch_train_loss=2.094951390443082
Epoch 698
Epoch 698 :: Batch 0/1
Batch Loss = 2.1135992236681793
698, epoch_train_loss=2.1135992236681793
Epoch 699
Epoch 699 :: Batch 0/1
Batch Loss = 2.1617033675494515
699, epoch_train_loss=2.1617033675494515
Epoch 700
Epoch 700 :: Batch 0/1
Batch Loss = 2.1872968546598863
700, epoch_train_loss=2.1872968546598863
Epoch 701
Epoch 701 :: Batch 0/1
Batch Loss = 2.2261722340767953
701, epoch_train_loss=2.2261722340767953
Epoch 702
Epoch 702 :: Batch 0/1
Batch Loss = 2.121173041523417
702, epoch_train_loss=2.121173041523417
Epoch 703
Epoch 703 :: Batch 0/1
Batch Loss = 2.0652941215747704
703, epoch_train_loss=2.0652941215747704
Epoch 704
Epoch 704 :: Batch 0/1
Batch Loss = 2.09436924264104
704, epoch_train_loss=2.09436924264104
Epoch 705
Epoch 705 :: Batch 0/1
Batch Loss = 2.1532118488403316
705, epoch_train_loss=2.1532118488403316
Epoch 706
Epoch 706 :: Batch 0/1
Batch Loss = 2.1808383120485675
706, epoch_train_loss=2.1808383120485675
Epoch 707
Epoch 707 :: Batch 0/1
Batch Loss = 2.0901834028677073
707, epoch_train_loss=2.0901834028677073
Epoch 708
Epoch 708 :: Batch 0/1
Batch Loss = 2.0836017455086724
708, epoch_train_loss=2.0836017455086724
Epoch 709
Epoch 709 :: Batch 0/1
Batch Loss = 2.1619720416918073
709, epoch_train_loss=2.1619720416918073
Epoch 710
Epoch 710 :: Batch 0/1
Batch Loss = 2.135449955553629
710, epoch_train_loss=2.135449955553629
Epoch 711
Epoch 711 :: Batch 0/1
Batch Loss = 2.0767013031753314
711, epoch_train_loss=2.0767013031753314
Epoch 712
Epoch 712 :: Batch 0/1
Batch Loss = 2.05769422799292
712, epoch_train_loss=2.05769422799292
Epoch 713
Epoch 713 :: Batch 0/1
Batch Loss = 2.0993128025868653
713, epoch_train_loss=2.0993128025868653
Epoch 714
Epoch 714 :: Batch 0/1
Batch Loss = 2.1372441621524914
714, epoch_train_loss=2.1372441621524914
Epoch 715
Epoch 715 :: Batch 0/1
Batch Loss = 2.0791646691366608
715, epoch_train_loss=2.0791646691366608
Epoch 716
Epoch 716 :: Batch 0/1
Batch Loss = 2.0463555111237137
716, epoch_train_loss=2.0463555111237137
Epoch 717
Epoch 717 :: Batch 0/1
Batch Loss = 2.0609310869723703
717, epoch_train_loss=2.0609310869723703
Epoch 718
Epoch 718 :: Batch 0/1
Batch Loss = 2.096345641413145
718, epoch_train_loss=2.096345641413145
Epoch 719
Epoch 719 :: Batch 0/1
Batch Loss = 2.1307966869168737
719, epoch_train_loss=2.1307966869168737
Epoch 720
Epoch 720 :: Batch 0/1
Batch Loss = 2.103726098050236
720, epoch_train_loss=2.103726098050236
Epoch 721
Epoch 721 :: Batch 0/1
Batch Loss = 2.0616711216368624
721, epoch_train_loss=2.0616711216368624
Epoch 722
Epoch 722 :: Batch 0/1
Batch Loss = 2.030676633288035
722, epoch_train_loss=2.030676633288035
Epoch 723
Epoch 723 :: Batch 0/1
Batch Loss = 2.083470449173649
723, epoch_train_loss=2.083470449173649
Epoch 724
Epoch 724 :: Batch 0/1
Batch Loss = 2.2076189453818915
724, epoch_train_loss=2.2076189453818915
Epoch 725
Epoch 725 :: Batch 0/1
Batch Loss = 2.299537060396686
725, epoch_train_loss=2.299537060396686
Epoch 726
Epoch 726 :: Batch 0/1
Batch Loss = 2.3153604010433564
726, epoch_train_loss=2.3153604010433564
Epoch 727
Epoch 727 :: Batch 0/1
Batch Loss = 2.135224907519042
727, epoch_train_loss=2.135224907519042
Epoch 728
Epoch 728 :: Batch 0/1
Batch Loss = 2.4715882895278094
728, epoch_train_loss=2.4715882895278094
Epoch 729
Epoch 729 :: Batch 0/1
Batch Loss = 2.7093834368549
729, epoch_train_loss=2.7093834368549
Epoch 730
Epoch 730 :: Batch 0/1
Batch Loss = 2.3045124059793753
730, epoch_train_loss=2.3045124059793753
Epoch 731
Epoch 731 :: Batch 0/1
Batch Loss = 2.5283874000996325
731, epoch_train_loss=2.5283874000996325
Epoch 732
Epoch 732 :: Batch 0/1
Batch Loss = 2.6806292220234114
732, epoch_train_loss=2.6806292220234114
Epoch 733
Epoch 733 :: Batch 0/1
Batch Loss = 2.2686082862663213
733, epoch_train_loss=2.2686082862663213
Epoch 734
Epoch 734 :: Batch 0/1
Batch Loss = 2.534837708146585
734, epoch_train_loss=2.534837708146585
Epoch 735
Epoch 735 :: Batch 0/1
Batch Loss = 3.0605553904142675
735, epoch_train_loss=3.0605553904142675
Epoch 736
Epoch 736 :: Batch 0/1
Batch Loss = 2.1830724066385794
736, epoch_train_loss=2.1830724066385794
Epoch 737
Epoch 737 :: Batch 0/1
Batch Loss = 2.676778943803853
737, epoch_train_loss=2.676778943803853
Epoch 738
Epoch 738 :: Batch 0/1
Batch Loss = 2.5879880143689102
738, epoch_train_loss=2.5879880143689102
Epoch 739
Epoch 739 :: Batch 0/1
Batch Loss = 2.5573460075314354
739, epoch_train_loss=2.5573460075314354
Epoch 740
Epoch 740 :: Batch 0/1
Batch Loss = 2.4756548823577837
740, epoch_train_loss=2.4756548823577837
Epoch 741
Epoch 741 :: Batch 0/1
Batch Loss = 2.3295433479390906
741, epoch_train_loss=2.3295433479390906
Epoch 742
Epoch 742 :: Batch 0/1
Batch Loss = 2.2733252766227703
742, epoch_train_loss=2.2733252766227703
Epoch 743
Epoch 743 :: Batch 0/1
Batch Loss = 2.275186806530914
743, epoch_train_loss=2.275186806530914
Epoch 744
Epoch 744 :: Batch 0/1
Batch Loss = 2.2281416905870755
744, epoch_train_loss=2.2281416905870755
Epoch 745
Epoch 745 :: Batch 0/1
Batch Loss = 2.28667498293548
745, epoch_train_loss=2.28667498293548
Epoch 746
Epoch 746 :: Batch 0/1
Batch Loss = 2.2063823085128154
746, epoch_train_loss=2.2063823085128154
Epoch 747
Epoch 747 :: Batch 0/1
Batch Loss = 2.2778192462375593
747, epoch_train_loss=2.2778192462375593
Epoch 748
Epoch 748 :: Batch 0/1
Batch Loss = 2.208349363143755
748, epoch_train_loss=2.208349363143755
Epoch 749
Epoch 749 :: Batch 0/1
Batch Loss = 2.2447180394313064
749, epoch_train_loss=2.2447180394313064
Epoch 750
Epoch 750 :: Batch 0/1
Batch Loss = 2.2140937828612657
750, epoch_train_loss=2.2140937828612657
Epoch 751
Epoch 751 :: Batch 0/1
Batch Loss = 2.1775877618650425
751, epoch_train_loss=2.1775877618650425
Epoch 752
Epoch 752 :: Batch 0/1
Batch Loss = 2.2142412622015595
752, epoch_train_loss=2.2142412622015595
Epoch 753
Epoch 753 :: Batch 0/1
Batch Loss = 2.190576397803465
753, epoch_train_loss=2.190576397803465
Epoch 754
Epoch 754 :: Batch 0/1
Batch Loss = 2.2003263624108658
754, epoch_train_loss=2.2003263624108658
Epoch 755
Epoch 755 :: Batch 0/1
Batch Loss = 2.167064395383839
755, epoch_train_loss=2.167064395383839
Epoch 756
Epoch 756 :: Batch 0/1
Batch Loss = 2.147844369648215
756, epoch_train_loss=2.147844369648215
Epoch 757
Epoch 757 :: Batch 0/1
Batch Loss = 2.1688683803821096
757, epoch_train_loss=2.1688683803821096
Epoch 758
Epoch 758 :: Batch 0/1
Batch Loss = 2.141329960777523
758, epoch_train_loss=2.141329960777523
Epoch 759
Epoch 759 :: Batch 0/1
Batch Loss = 2.1393968706307844
759, epoch_train_loss=2.1393968706307844
Epoch 760
Epoch 760 :: Batch 0/1
Batch Loss = 2.1238682358675014
760, epoch_train_loss=2.1238682358675014
Epoch 761
Epoch 761 :: Batch 0/1
Batch Loss = 2.142207651363182
761, epoch_train_loss=2.142207651363182
Epoch 762
Epoch 762 :: Batch 0/1
Batch Loss = 2.124822210586762
762, epoch_train_loss=2.124822210586762
Epoch 763
Epoch 763 :: Batch 0/1
Batch Loss = 2.1182799286516016
763, epoch_train_loss=2.1182799286516016
Epoch 764
Epoch 764 :: Batch 0/1
Batch Loss = 2.1180681195831617
764, epoch_train_loss=2.1180681195831617
Epoch 765
Epoch 765 :: Batch 0/1
Batch Loss = 2.1184433597235164
765, epoch_train_loss=2.1184433597235164
Epoch 766
Epoch 766 :: Batch 0/1
Batch Loss = 2.106414457151468
766, epoch_train_loss=2.106414457151468
Epoch 767
Epoch 767 :: Batch 0/1
Batch Loss = 2.0995447840684336
767, epoch_train_loss=2.0995447840684336
Epoch 768
Epoch 768 :: Batch 0/1
Batch Loss = 2.097742362257232
768, epoch_train_loss=2.097742362257232
Epoch 769
Epoch 769 :: Batch 0/1
Batch Loss = 2.096756861771959
769, epoch_train_loss=2.096756861771959
Epoch 770
Epoch 770 :: Batch 0/1
Batch Loss = 2.086437823295837
770, epoch_train_loss=2.086437823295837
Epoch 771
Epoch 771 :: Batch 0/1
Batch Loss = 2.0845621637939917
771, epoch_train_loss=2.0845621637939917
Epoch 772
Epoch 772 :: Batch 0/1
Batch Loss = 2.078046864428548
772, epoch_train_loss=2.078046864428548
Epoch 773
Epoch 773 :: Batch 0/1
Batch Loss = 2.0810034325835187
773, epoch_train_loss=2.0810034325835187
Epoch 774
Epoch 774 :: Batch 0/1
Batch Loss = 2.0716428583064674
774, epoch_train_loss=2.0716428583064674
Epoch 775
Epoch 775 :: Batch 0/1
Batch Loss = 2.0662728912580905
775, epoch_train_loss=2.0662728912580905
Epoch 776
Epoch 776 :: Batch 0/1
Batch Loss = 2.066791062974775
776, epoch_train_loss=2.066791062974775
Epoch 777
Epoch 777 :: Batch 0/1
Batch Loss = 2.059823704205651
777, epoch_train_loss=2.059823704205651
Epoch 778
Epoch 778 :: Batch 0/1
Batch Loss = 2.058712405881201
778, epoch_train_loss=2.058712405881201
Epoch 779
Epoch 779 :: Batch 0/1
Batch Loss = 2.0519312670528564
779, epoch_train_loss=2.0519312670528564
Epoch 780
Epoch 780 :: Batch 0/1
Batch Loss = 2.05057519171238
780, epoch_train_loss=2.05057519171238
Epoch 781
Epoch 781 :: Batch 0/1
Batch Loss = 2.049460383801526
781, epoch_train_loss=2.049460383801526
Epoch 782
Epoch 782 :: Batch 0/1
Batch Loss = 2.044021107127168
782, epoch_train_loss=2.044021107127168
Epoch 783
Epoch 783 :: Batch 0/1
Batch Loss = 2.039521331340732
783, epoch_train_loss=2.039521331340732
Epoch 784
Epoch 784 :: Batch 0/1
Batch Loss = 2.0380670916501953
784, epoch_train_loss=2.0380670916501953
Epoch 785
Epoch 785 :: Batch 0/1
Batch Loss = 2.0337308410206285
785, epoch_train_loss=2.0337308410206285
Epoch 786
Epoch 786 :: Batch 0/1
Batch Loss = 2.0307484380365675
786, epoch_train_loss=2.0307484380365675
Epoch 787
Epoch 787 :: Batch 0/1
Batch Loss = 2.03317494259476
787, epoch_train_loss=2.03317494259476
Epoch 788
Epoch 788 :: Batch 0/1
Batch Loss = 2.038356520764989
788, epoch_train_loss=2.038356520764989
Epoch 789
Epoch 789 :: Batch 0/1
Batch Loss = 2.021673979516685
789, epoch_train_loss=2.021673979516685
Epoch 790
Epoch 790 :: Batch 0/1
Batch Loss = 2.021797557624235
790, epoch_train_loss=2.021797557624235
Epoch 791
Epoch 791 :: Batch 0/1
Batch Loss = 2.0175321889687483
791, epoch_train_loss=2.0175321889687483
Epoch 792
Epoch 792 :: Batch 0/1
Batch Loss = 2.0150187447269565
792, epoch_train_loss=2.0150187447269565
Epoch 793
Epoch 793 :: Batch 0/1
Batch Loss = 2.0109061518272187
793, epoch_train_loss=2.0109061518272187
Epoch 794
Epoch 794 :: Batch 0/1
Batch Loss = 2.0062989355300895
794, epoch_train_loss=2.0062989355300895
Epoch 795
Epoch 795 :: Batch 0/1
Batch Loss = 2.005996129049352
795, epoch_train_loss=2.005996129049352
Epoch 796
Epoch 796 :: Batch 0/1
Batch Loss = 2.002937978923575
796, epoch_train_loss=2.002937978923575
Epoch 797
Epoch 797 :: Batch 0/1
Batch Loss = 2.005719643914194
797, epoch_train_loss=2.005719643914194
Epoch 798
Epoch 798 :: Batch 0/1
Batch Loss = 2.0312952732000884
798, epoch_train_loss=2.0312952732000884
Epoch 799
Epoch 799 :: Batch 0/1
Batch Loss = 2.134157866833906
799, epoch_train_loss=2.134157866833906
Epoch 800
Epoch 800 :: Batch 0/1
Batch Loss = 2.663940501598431
800, epoch_train_loss=2.663940501598431
Epoch 801
Epoch 801 :: Batch 0/1
Batch Loss = 2.0354540977974986
801, epoch_train_loss=2.0354540977974986
Epoch 802
Epoch 802 :: Batch 0/1
Batch Loss = 2.77487417967201
802, epoch_train_loss=2.77487417967201
Epoch 803
Epoch 803 :: Batch 0/1
Batch Loss = 3.258071252054804
803, epoch_train_loss=3.258071252054804
Epoch 804
Epoch 804 :: Batch 0/1
Batch Loss = 3.529951511231795
804, epoch_train_loss=3.529951511231795
Epoch 805
Epoch 805 :: Batch 0/1
Batch Loss = 2.806468276583241
805, epoch_train_loss=2.806468276583241
Epoch 806
Epoch 806 :: Batch 0/1
Batch Loss = 2.782341503650461
806, epoch_train_loss=2.782341503650461
Epoch 807
Epoch 807 :: Batch 0/1
Batch Loss = 2.620898787774489
807, epoch_train_loss=2.620898787774489
Epoch 808
Epoch 808 :: Batch 0/1
Batch Loss = 3.3301770319582196
808, epoch_train_loss=3.3301770319582196
Epoch 809
Epoch 809 :: Batch 0/1
Batch Loss = 4.551355128861078
809, epoch_train_loss=4.551355128861078
Epoch 810
Epoch 810 :: Batch 0/1
Batch Loss = 3.690968016180147
810, epoch_train_loss=3.690968016180147
Epoch 811
Epoch 811 :: Batch 0/1
Batch Loss = 3.6426247342971494
811, epoch_train_loss=3.6426247342971494
Epoch 812
Epoch 812 :: Batch 0/1
Batch Loss = 3.2283763429007606
812, epoch_train_loss=3.2283763429007606
Epoch 813
Epoch 813 :: Batch 0/1
Batch Loss = 2.8393119831091904
813, epoch_train_loss=2.8393119831091904
Epoch 814
Epoch 814 :: Batch 0/1
Batch Loss = 2.859522880284268
814, epoch_train_loss=2.859522880284268
Epoch 815
Epoch 815 :: Batch 0/1
Batch Loss = 2.7662045372604505
815, epoch_train_loss=2.7662045372604505
Epoch 816
Epoch 816 :: Batch 0/1
Batch Loss = 2.867772211517805
816, epoch_train_loss=2.867772211517805
Epoch 817
Epoch 817 :: Batch 0/1
Batch Loss = 2.76913707559307
817, epoch_train_loss=2.76913707559307
Epoch 818
Epoch 818 :: Batch 0/1
Batch Loss = 2.8467767418616403
818, epoch_train_loss=2.8467767418616403
Epoch 819
Epoch 819 :: Batch 0/1
Batch Loss = 2.761533609024781
819, epoch_train_loss=2.761533609024781
Epoch 820
Epoch 820 :: Batch 0/1
Batch Loss = 2.8087220377920232
820, epoch_train_loss=2.8087220377920232
Epoch 821
Epoch 821 :: Batch 0/1
Batch Loss = 2.7055505337911367
821, epoch_train_loss=2.7055505337911367
Epoch 822
Epoch 822 :: Batch 0/1
Batch Loss = 2.7340503341622773
822, epoch_train_loss=2.7340503341622773
Epoch 823
Epoch 823 :: Batch 0/1
Batch Loss = 2.6572482614175787
823, epoch_train_loss=2.6572482614175787
Epoch 824
Epoch 824 :: Batch 0/1
Batch Loss = 2.6920654553945766
824, epoch_train_loss=2.6920654553945766
Epoch 825
Epoch 825 :: Batch 0/1
Batch Loss = 2.639226654379483
825, epoch_train_loss=2.639226654379483
Epoch 826
Epoch 826 :: Batch 0/1
Batch Loss = 2.6818446833607283
826, epoch_train_loss=2.6818446833607283
Epoch 827
Epoch 827 :: Batch 0/1
Batch Loss = 2.6475235260341683
827, epoch_train_loss=2.6475235260341683
Epoch 828
Epoch 828 :: Batch 0/1
Batch Loss = 2.6868786387009727
828, epoch_train_loss=2.6868786387009727
Epoch 829
Epoch 829 :: Batch 0/1
Batch Loss = 2.6490198735981183
829, epoch_train_loss=2.6490198735981183
Epoch 830
Epoch 830 :: Batch 0/1
Batch Loss = 2.6708839052847484
830, epoch_train_loss=2.6708839052847484
Epoch 831
Epoch 831 :: Batch 0/1
Batch Loss = 2.633115539584201
831, epoch_train_loss=2.633115539584201
Epoch 832
Epoch 832 :: Batch 0/1
Batch Loss = 2.6437702590559664
832, epoch_train_loss=2.6437702590559664
Epoch 833
Epoch 833 :: Batch 0/1
Batch Loss = 2.6136335616628776
833, epoch_train_loss=2.6136335616628776
Epoch 834
Epoch 834 :: Batch 0/1
Batch Loss = 2.608044554885254
834, epoch_train_loss=2.608044554885254
Epoch 835
Epoch 835 :: Batch 0/1
Batch Loss = 2.600791639273489
835, epoch_train_loss=2.600791639273489
Epoch 836
Epoch 836 :: Batch 0/1
Batch Loss = 2.5865741906525708
836, epoch_train_loss=2.5865741906525708
Epoch 837
Epoch 837 :: Batch 0/1
Batch Loss = 2.599965725107878
837, epoch_train_loss=2.599965725107878
Epoch 838
Epoch 838 :: Batch 0/1
Batch Loss = 2.582245160832905
838, epoch_train_loss=2.582245160832905
Epoch 839
Epoch 839 :: Batch 0/1
Batch Loss = 2.5916334258196736
839, epoch_train_loss=2.5916334258196736
Epoch 840
Epoch 840 :: Batch 0/1
Batch Loss = 2.577832658012625
840, epoch_train_loss=2.577832658012625
Epoch 841
Epoch 841 :: Batch 0/1
Batch Loss = 2.5672726901051193
841, epoch_train_loss=2.5672726901051193
Epoch 842
Epoch 842 :: Batch 0/1
Batch Loss = 2.5706112863323503
842, epoch_train_loss=2.5706112863323503
Epoch 843
Epoch 843 :: Batch 0/1
Batch Loss = 2.555397461123379
843, epoch_train_loss=2.555397461123379
Epoch 844
Epoch 844 :: Batch 0/1
Batch Loss = 2.550136348644566
844, epoch_train_loss=2.550136348644566
Epoch 845
Epoch 845 :: Batch 0/1
Batch Loss = 2.5553669802724785
845, epoch_train_loss=2.5553669802724785
Epoch 846
Epoch 846 :: Batch 0/1
Batch Loss = 2.5451582461612885
846, epoch_train_loss=2.5451582461612885
Epoch 847
Epoch 847 :: Batch 0/1
Batch Loss = 2.5351084118673444
847, epoch_train_loss=2.5351084118673444
Epoch 848
Epoch 848 :: Batch 0/1
Batch Loss = 2.5354396999491455
848, epoch_train_loss=2.5354396999491455
Epoch 849
Epoch 849 :: Batch 0/1
Batch Loss = 2.5333580866818894
849, epoch_train_loss=2.5333580866818894
Epoch 850
Epoch 850 :: Batch 0/1
Batch Loss = 2.527294675035612
850, epoch_train_loss=2.527294675035612
Epoch 851
Epoch 851 :: Batch 0/1
Batch Loss = 2.5133195443542307
851, epoch_train_loss=2.5133195443542307
Epoch 852
Epoch 852 :: Batch 0/1
Batch Loss = 2.50986077580605
852, epoch_train_loss=2.50986077580605
Epoch 853
Epoch 853 :: Batch 0/1
Batch Loss = 2.5111383000284975
853, epoch_train_loss=2.5111383000284975
Epoch 854
Epoch 854 :: Batch 0/1
Batch Loss = 2.512468752273995
854, epoch_train_loss=2.512468752273995
Epoch 855
Epoch 855 :: Batch 0/1
Batch Loss = 2.517978447065122
855, epoch_train_loss=2.517978447065122
Epoch 856
Epoch 856 :: Batch 0/1
Batch Loss = 2.5068321672379335
856, epoch_train_loss=2.5068321672379335
Epoch 857
Epoch 857 :: Batch 0/1
Batch Loss = 2.5013176245792854
857, epoch_train_loss=2.5013176245792854
Epoch 858
Epoch 858 :: Batch 0/1
Batch Loss = 2.4890443744324324
858, epoch_train_loss=2.4890443744324324
Epoch 859
Epoch 859 :: Batch 0/1
Batch Loss = 2.4806388270946536
859, epoch_train_loss=2.4806388270946536
Epoch 860
Epoch 860 :: Batch 0/1
Batch Loss = 2.4618924832588194
860, epoch_train_loss=2.4618924832588194
Epoch 861
Epoch 861 :: Batch 0/1
Batch Loss = 2.4489920168833814
861, epoch_train_loss=2.4489920168833814
Epoch 862
Epoch 862 :: Batch 0/1
Batch Loss = 2.442301079842332
862, epoch_train_loss=2.442301079842332
Epoch 863
Epoch 863 :: Batch 0/1
Batch Loss = 2.446215076562024
863, epoch_train_loss=2.446215076562024
Epoch 864
Epoch 864 :: Batch 0/1
Batch Loss = 2.464022175172205
864, epoch_train_loss=2.464022175172205
Epoch 865
Epoch 865 :: Batch 0/1
Batch Loss = 2.499924419276265
865, epoch_train_loss=2.499924419276265
Epoch 866
Epoch 866 :: Batch 0/1
Batch Loss = 2.5807923994983697
866, epoch_train_loss=2.5807923994983697
Epoch 867
Epoch 867 :: Batch 0/1
Batch Loss = 2.509095564013712
867, epoch_train_loss=2.509095564013712
Epoch 868
Epoch 868 :: Batch 0/1
Batch Loss = 2.4229726510503675
868, epoch_train_loss=2.4229726510503675
Epoch 869
Epoch 869 :: Batch 0/1
Batch Loss = 2.5066401730286447
869, epoch_train_loss=2.5066401730286447
Epoch 870
Epoch 870 :: Batch 0/1
Batch Loss = 2.55051398343498
870, epoch_train_loss=2.55051398343498
Epoch 871
Epoch 871 :: Batch 0/1
Batch Loss = 2.508193294230083
871, epoch_train_loss=2.508193294230083
Epoch 872
Epoch 872 :: Batch 0/1
Batch Loss = 2.417106024103071
872, epoch_train_loss=2.417106024103071
Epoch 873
Epoch 873 :: Batch 0/1
Batch Loss = 2.624407756205005
873, epoch_train_loss=2.624407756205005
Epoch 874
Epoch 874 :: Batch 0/1
Batch Loss = 2.603396652939018
874, epoch_train_loss=2.603396652939018
Epoch 875
Epoch 875 :: Batch 0/1
Batch Loss = 2.5851381210984865
875, epoch_train_loss=2.5851381210984865
Epoch 876
Epoch 876 :: Batch 0/1
Batch Loss = 2.4593256177850233
876, epoch_train_loss=2.4593256177850233
Epoch 877
Epoch 877 :: Batch 0/1
Batch Loss = 2.5846802399133506
877, epoch_train_loss=2.5846802399133506
Epoch 878
Epoch 878 :: Batch 0/1
Batch Loss = 2.4871106792429667
878, epoch_train_loss=2.4871106792429667
Epoch 879
Epoch 879 :: Batch 0/1
Batch Loss = 2.4679177059113218
879, epoch_train_loss=2.4679177059113218
Epoch 880
Epoch 880 :: Batch 0/1
Batch Loss = 2.52737068484291
880, epoch_train_loss=2.52737068484291
Epoch 881
Epoch 881 :: Batch 0/1
Batch Loss = 2.439398369355419
881, epoch_train_loss=2.439398369355419
Epoch 882
Epoch 882 :: Batch 0/1
Batch Loss = 2.5144778190011396
882, epoch_train_loss=2.5144778190011396
Epoch 883
Epoch 883 :: Batch 0/1
Batch Loss = 2.4282711173427862
883, epoch_train_loss=2.4282711173427862
Epoch 884
Epoch 884 :: Batch 0/1
Batch Loss = 2.397896729224404
884, epoch_train_loss=2.397896729224404
Epoch 885
Epoch 885 :: Batch 0/1
Batch Loss = 2.4261030075121686
885, epoch_train_loss=2.4261030075121686
Epoch 886
Epoch 886 :: Batch 0/1
Batch Loss = 2.433434835841261
886, epoch_train_loss=2.433434835841261
Epoch 887
Epoch 887 :: Batch 0/1
Batch Loss = 2.435808638680897
887, epoch_train_loss=2.435808638680897
Epoch 888
Epoch 888 :: Batch 0/1
Batch Loss = 2.378761722352994
888, epoch_train_loss=2.378761722352994
Epoch 889
Epoch 889 :: Batch 0/1
Batch Loss = 2.397951116634232
889, epoch_train_loss=2.397951116634232
Epoch 890
Epoch 890 :: Batch 0/1
Batch Loss = 2.383186225125343
890, epoch_train_loss=2.383186225125343
Epoch 891
Epoch 891 :: Batch 0/1
Batch Loss = 2.422233947464571
891, epoch_train_loss=2.422233947464571
Epoch 892
Epoch 892 :: Batch 0/1
Batch Loss = 2.3902546616525693
892, epoch_train_loss=2.3902546616525693
Epoch 893
Epoch 893 :: Batch 0/1
Batch Loss = 2.382242579962127
893, epoch_train_loss=2.382242579962127
Epoch 894
Epoch 894 :: Batch 0/1
Batch Loss = 2.368179564791609
894, epoch_train_loss=2.368179564791609
Epoch 895
Epoch 895 :: Batch 0/1
Batch Loss = 2.3615296061944413
895, epoch_train_loss=2.3615296061944413
Epoch 896
Epoch 896 :: Batch 0/1
Batch Loss = 2.3811247453848186
896, epoch_train_loss=2.3811247453848186
Epoch 897
Epoch 897 :: Batch 0/1
Batch Loss = 2.3592670180932185
897, epoch_train_loss=2.3592670180932185
Epoch 898
Epoch 898 :: Batch 0/1
Batch Loss = 2.362300323548894
898, epoch_train_loss=2.362300323548894
Epoch 899
Epoch 899 :: Batch 0/1
Batch Loss = 2.341081730066014
899, epoch_train_loss=2.341081730066014
Epoch 900
Epoch 900 :: Batch 0/1
Batch Loss = 2.3513036483647496
900, epoch_train_loss=2.3513036483647496
Epoch 901
Epoch 901 :: Batch 0/1
Batch Loss = 2.349827362186565
901, epoch_train_loss=2.349827362186565
Epoch 902
Epoch 902 :: Batch 0/1
Batch Loss = 2.3358769497762903
902, epoch_train_loss=2.3358769497762903
Epoch 903
Epoch 903 :: Batch 0/1
Batch Loss = 2.332517743241665
903, epoch_train_loss=2.332517743241665
Epoch 904
Epoch 904 :: Batch 0/1
Batch Loss = 2.3273732457260103
904, epoch_train_loss=2.3273732457260103
Epoch 905
Epoch 905 :: Batch 0/1
Batch Loss = 2.3246991936919437
905, epoch_train_loss=2.3246991936919437
Epoch 906
Epoch 906 :: Batch 0/1
Batch Loss = 2.3244836618947002
906, epoch_train_loss=2.3244836618947002
Epoch 907
Epoch 907 :: Batch 0/1
Batch Loss = 2.3133508453172094
907, epoch_train_loss=2.3133508453172094
Epoch 908
Epoch 908 :: Batch 0/1
Batch Loss = 2.310363766625911
908, epoch_train_loss=2.310363766625911
Epoch 909
Epoch 909 :: Batch 0/1
Batch Loss = 2.3114151953816764
909, epoch_train_loss=2.3114151953816764
Epoch 910
Epoch 910 :: Batch 0/1
Batch Loss = 2.3074998902408375
910, epoch_train_loss=2.3074998902408375
Epoch 911
Epoch 911 :: Batch 0/1
Batch Loss = 2.2974052807921903
911, epoch_train_loss=2.2974052807921903
Epoch 912
Epoch 912 :: Batch 0/1
Batch Loss = 2.2990252971832836
912, epoch_train_loss=2.2990252971832836
Epoch 913
Epoch 913 :: Batch 0/1
Batch Loss = 2.2954665905123126
913, epoch_train_loss=2.2954665905123126
Epoch 914
Epoch 914 :: Batch 0/1
Batch Loss = 2.29316174420267
914, epoch_train_loss=2.29316174420267
Epoch 915
Epoch 915 :: Batch 0/1
Batch Loss = 2.2863805041372505
915, epoch_train_loss=2.2863805041372505
Epoch 916
Epoch 916 :: Batch 0/1
Batch Loss = 2.2825533275923675
916, epoch_train_loss=2.2825533275923675
Epoch 917
Epoch 917 :: Batch 0/1
Batch Loss = 2.284781455131115
917, epoch_train_loss=2.284781455131115
Epoch 918
Epoch 918 :: Batch 0/1
Batch Loss = 2.2806738473955
918, epoch_train_loss=2.2806738473955
Epoch 919
Epoch 919 :: Batch 0/1
Batch Loss = 2.279394321020087
919, epoch_train_loss=2.279394321020087
Epoch 920
Epoch 920 :: Batch 0/1
Batch Loss = 2.279166605414073
920, epoch_train_loss=2.279166605414073
Epoch 921
Epoch 921 :: Batch 0/1
Batch Loss = 2.2762720728210506
921, epoch_train_loss=2.2762720728210506
Epoch 922
Epoch 922 :: Batch 0/1
Batch Loss = 2.274839483735963
922, epoch_train_loss=2.274839483735963
Epoch 923
Epoch 923 :: Batch 0/1
Batch Loss = 2.283278997207552
923, epoch_train_loss=2.283278997207552
Epoch 924
Epoch 924 :: Batch 0/1
Batch Loss = 2.2982970298332575
924, epoch_train_loss=2.2982970298332575
Epoch 925
Epoch 925 :: Batch 0/1
Batch Loss = 2.3722514825176524
925, epoch_train_loss=2.3722514825176524
Epoch 926
Epoch 926 :: Batch 0/1
Batch Loss = 2.4076742594027385
926, epoch_train_loss=2.4076742594027385
Epoch 927
Epoch 927 :: Batch 0/1
Batch Loss = 2.5960427942321878
927, epoch_train_loss=2.5960427942321878
Epoch 928
Epoch 928 :: Batch 0/1
Batch Loss = 2.2702679689113703
928, epoch_train_loss=2.2702679689113703
Epoch 929
Epoch 929 :: Batch 0/1
Batch Loss = 2.9304422008393813
929, epoch_train_loss=2.9304422008393813
Epoch 930
Epoch 930 :: Batch 0/1
Batch Loss = 3.01795383590398
930, epoch_train_loss=3.01795383590398
Epoch 931
Epoch 931 :: Batch 0/1
Batch Loss = 3.1289917760589825
931, epoch_train_loss=3.1289917760589825
Epoch 932
Epoch 932 :: Batch 0/1
Batch Loss = 4.069701634378358
932, epoch_train_loss=4.069701634378358
Epoch 933
Epoch 933 :: Batch 0/1
Batch Loss = 3.924722494792458
933, epoch_train_loss=3.924722494792458
Epoch 934
Epoch 934 :: Batch 0/1
Batch Loss = 3.879510210532037
934, epoch_train_loss=3.879510210532037
Epoch 935
Epoch 935 :: Batch 0/1
Batch Loss = 3.5436573293807516
935, epoch_train_loss=3.5436573293807516
Epoch 936
Epoch 936 :: Batch 0/1
Batch Loss = 3.612270289679331
936, epoch_train_loss=3.612270289679331
Epoch 937
Epoch 937 :: Batch 0/1
Batch Loss = 3.202634200349423
937, epoch_train_loss=3.202634200349423
Epoch 938
Epoch 938 :: Batch 0/1
Batch Loss = 3.324237412362614
938, epoch_train_loss=3.324237412362614
Epoch 939
Epoch 939 :: Batch 0/1
Batch Loss = 3.27716725102151
939, epoch_train_loss=3.27716725102151
Epoch 940
Epoch 940 :: Batch 0/1
Batch Loss = 2.9392017794489864
940, epoch_train_loss=2.9392017794489864
Epoch 941
Epoch 941 :: Batch 0/1
Batch Loss = 2.8036539605103967
941, epoch_train_loss=2.8036539605103967
Epoch 942
Epoch 942 :: Batch 0/1
Batch Loss = 3.168191011621232
942, epoch_train_loss=3.168191011621232
Epoch 943
Epoch 943 :: Batch 0/1
Batch Loss = 2.844300644590819
943, epoch_train_loss=2.844300644590819
Epoch 944
Epoch 944 :: Batch 0/1
Batch Loss = 2.9080239125997327
944, epoch_train_loss=2.9080239125997327
Epoch 945
Epoch 945 :: Batch 0/1
Batch Loss = 3.1186398736416705
945, epoch_train_loss=3.1186398736416705
Epoch 946
Epoch 946 :: Batch 0/1
Batch Loss = 2.8956283118896353
946, epoch_train_loss=2.8956283118896353
Epoch 947
Epoch 947 :: Batch 0/1
Batch Loss = 2.918816844911153
947, epoch_train_loss=2.918816844911153
Epoch 948
Epoch 948 :: Batch 0/1
Batch Loss = 3.0670497017225484
948, epoch_train_loss=3.0670497017225484
Epoch 949
Epoch 949 :: Batch 0/1
Batch Loss = 2.8353287504938187
949, epoch_train_loss=2.8353287504938187
Epoch 950
Epoch 950 :: Batch 0/1
Batch Loss = 2.988895228029815
950, epoch_train_loss=2.988895228029815
Epoch 951
Epoch 951 :: Batch 0/1
Batch Loss = 2.960256214381861
951, epoch_train_loss=2.960256214381861
Epoch 952
Epoch 952 :: Batch 0/1
Batch Loss = 2.8231799202298906
952, epoch_train_loss=2.8231799202298906
Epoch 953
Epoch 953 :: Batch 0/1
Batch Loss = 2.9579694485852808
953, epoch_train_loss=2.9579694485852808
Epoch 954
Epoch 954 :: Batch 0/1
Batch Loss = 2.837828462057594
954, epoch_train_loss=2.837828462057594
Epoch 955
Epoch 955 :: Batch 0/1
Batch Loss = 2.823345179263001
955, epoch_train_loss=2.823345179263001
Epoch 956
Epoch 956 :: Batch 0/1
Batch Loss = 2.8790026881362794
956, epoch_train_loss=2.8790026881362794
Epoch 957
Epoch 957 :: Batch 0/1
Batch Loss = 2.771984780185557
957, epoch_train_loss=2.771984780185557
Epoch 958
Epoch 958 :: Batch 0/1
Batch Loss = 2.788077155907601
958, epoch_train_loss=2.788077155907601
Epoch 959
Epoch 959 :: Batch 0/1
Batch Loss = 2.8031994554141884
959, epoch_train_loss=2.8031994554141884
Epoch 960
Epoch 960 :: Batch 0/1
Batch Loss = 2.726175954338167
960, epoch_train_loss=2.726175954338167
Epoch 961
Epoch 961 :: Batch 0/1
Batch Loss = 2.756016363212724
961, epoch_train_loss=2.756016363212724
Epoch 962
Epoch 962 :: Batch 0/1
Batch Loss = 2.753557720276554
962, epoch_train_loss=2.753557720276554
Epoch 963
Epoch 963 :: Batch 0/1
Batch Loss = 2.7035349144270326
963, epoch_train_loss=2.7035349144270326
Epoch 964
Epoch 964 :: Batch 0/1
Batch Loss = 2.7272058488655953
964, epoch_train_loss=2.7272058488655953
Epoch 965
Epoch 965 :: Batch 0/1
Batch Loss = 2.725565461039
965, epoch_train_loss=2.725565461039
Epoch 966
Epoch 966 :: Batch 0/1
Batch Loss = 2.693542907438426
966, epoch_train_loss=2.693542907438426
Epoch 967
Epoch 967 :: Batch 0/1
Batch Loss = 2.71475353220129
967, epoch_train_loss=2.71475353220129
Epoch 968
Epoch 968 :: Batch 0/1
Batch Loss = 2.7153363617420894
968, epoch_train_loss=2.7153363617420894
Epoch 969
Epoch 969 :: Batch 0/1
Batch Loss = 2.6938793306668063
969, epoch_train_loss=2.6938793306668063
Epoch 970
Epoch 970 :: Batch 0/1
Batch Loss = 2.708885369345709
970, epoch_train_loss=2.708885369345709
Epoch 971
Epoch 971 :: Batch 0/1
Batch Loss = 2.7123013501305464
971, epoch_train_loss=2.7123013501305464
Epoch 972
Epoch 972 :: Batch 0/1
Batch Loss = 2.6969066619339124
972, epoch_train_loss=2.6969066619339124
Epoch 973
Epoch 973 :: Batch 0/1
Batch Loss = 2.7078615139017286
973, epoch_train_loss=2.7078615139017286
Epoch 974
Epoch 974 :: Batch 0/1
Batch Loss = 2.7111814176209896
974, epoch_train_loss=2.7111814176209896
Epoch 975
Epoch 975 :: Batch 0/1
Batch Loss = 2.6993927109537146
975, epoch_train_loss=2.6993927109537146
Epoch 976
Epoch 976 :: Batch 0/1
Batch Loss = 2.705142158255342
976, epoch_train_loss=2.705142158255342
Epoch 977
Epoch 977 :: Batch 0/1
Batch Loss = 2.7082968675228387
977, epoch_train_loss=2.7082968675228387
Epoch 978
Epoch 978 :: Batch 0/1
Batch Loss = 2.698514136352998
978, epoch_train_loss=2.698514136352998
Epoch 979
Epoch 979 :: Batch 0/1
Batch Loss = 2.701602075817916
979, epoch_train_loss=2.701602075817916
Epoch 980
Epoch 980 :: Batch 0/1
Batch Loss = 2.703563152200771
980, epoch_train_loss=2.703563152200771
Epoch 981
Epoch 981 :: Batch 0/1
Batch Loss = 2.6959400220068828
981, epoch_train_loss=2.6959400220068828
Epoch 982
Epoch 982 :: Batch 0/1
Batch Loss = 2.6967701514372227
982, epoch_train_loss=2.6967701514372227
Epoch 983
Epoch 983 :: Batch 0/1
Batch Loss = 2.698533050014903
983, epoch_train_loss=2.698533050014903
Epoch 984
Epoch 984 :: Batch 0/1
Batch Loss = 2.6925682857466455
984, epoch_train_loss=2.6925682857466455
Epoch 985
Epoch 985 :: Batch 0/1
Batch Loss = 2.6927978166147177
985, epoch_train_loss=2.6927978166147177
Epoch 986
Epoch 986 :: Batch 0/1
Batch Loss = 2.6941546373167404
986, epoch_train_loss=2.6941546373167404
Epoch 987
Epoch 987 :: Batch 0/1
Batch Loss = 2.689967548124239
987, epoch_train_loss=2.689967548124239
Epoch 988
Epoch 988 :: Batch 0/1
Batch Loss = 2.6898536915903115
988, epoch_train_loss=2.6898536915903115
Epoch 989
Epoch 989 :: Batch 0/1
Batch Loss = 2.6912041762966754
989, epoch_train_loss=2.6912041762966754
Epoch 990
Epoch 990 :: Batch 0/1
Batch Loss = 2.688216251239456
990, epoch_train_loss=2.688216251239456
Epoch 991
Epoch 991 :: Batch 0/1
Batch Loss = 2.688223005228652
991, epoch_train_loss=2.688223005228652
Epoch 992
Epoch 992 :: Batch 0/1
Batch Loss = 2.6893360036393927
992, epoch_train_loss=2.6893360036393927
Epoch 993
Epoch 993 :: Batch 0/1
Batch Loss = 2.6872969719687196
993, epoch_train_loss=2.6872969719687196
Epoch 994
Epoch 994 :: Batch 0/1
Batch Loss = 2.6873387124823904
994, epoch_train_loss=2.6873387124823904
Epoch 995
Epoch 995 :: Batch 0/1
Batch Loss = 2.6882484270481943
995, epoch_train_loss=2.6882484270481943
Epoch 996
Epoch 996 :: Batch 0/1
Batch Loss = 2.6867400326693236
996, epoch_train_loss=2.6867400326693236
Epoch 997
Epoch 997 :: Batch 0/1
Batch Loss = 2.6868453458754322
997, epoch_train_loss=2.6868453458754322
Epoch 998
Epoch 998 :: Batch 0/1
Batch Loss = 2.687438727101679
998, epoch_train_loss=2.687438727101679
Epoch 999
Epoch 999 :: Batch 0/1
Batch Loss = 2.6862883803686866
999, epoch_train_loss=2.6862883803686866
Epoch 1000
Epoch 1000 :: Batch 0/1
Batch Loss = 2.686353933232739
1000, epoch_train_loss=2.686353933232739
Epoch 1001
Epoch 1001 :: Batch 0/1
Batch Loss = 2.6866823001659252
1001, epoch_train_loss=2.6866823001659252
Epoch 1002
Epoch 1002 :: Batch 0/1
Batch Loss = 2.6857302559393172
1002, epoch_train_loss=2.6857302559393172
Epoch 1003
Epoch 1003 :: Batch 0/1
Batch Loss = 2.6857724578995894
1003, epoch_train_loss=2.6857724578995894
Epoch 1004
Epoch 1004 :: Batch 0/1
Batch Loss = 2.685864147662776
1004, epoch_train_loss=2.685864147662776
Epoch 1005
Epoch 1005 :: Batch 0/1
Batch Loss = 2.685081088737438
1005, epoch_train_loss=2.685081088737438
Epoch 1006
Epoch 1006 :: Batch 0/1
Batch Loss = 2.6850924061415684
1006, epoch_train_loss=2.6850924061415684
Epoch 1007
Epoch 1007 :: Batch 0/1
Batch Loss = 2.685035928278542
1007, epoch_train_loss=2.685035928278542
Epoch 1008
Epoch 1008 :: Batch 0/1
Batch Loss = 2.6844025581719
1008, epoch_train_loss=2.6844025581719
Epoch 1009
Epoch 1009 :: Batch 0/1
Batch Loss = 2.6844174895761648
1009, epoch_train_loss=2.6844174895761648
Epoch 1010
Epoch 1010 :: Batch 0/1
Batch Loss = 2.684275056026241
1010, epoch_train_loss=2.684275056026241
Epoch 1011
Epoch 1011 :: Batch 0/1
Batch Loss = 2.683793994967334
1011, epoch_train_loss=2.683793994967334
Epoch 1012
Epoch 1012 :: Batch 0/1
Batch Loss = 2.6838186841140836
1012, epoch_train_loss=2.6838186841140836
Epoch 1013
Epoch 1013 :: Batch 0/1
Batch Loss = 2.6836428072393366
1013, epoch_train_loss=2.6836428072393366
Epoch 1014
Epoch 1014 :: Batch 0/1
Batch Loss = 2.68330011390315
1014, epoch_train_loss=2.68330011390315
Epoch 1015
Epoch 1015 :: Batch 0/1
Batch Loss = 2.6833314503490224
1015, epoch_train_loss=2.6833314503490224
Epoch 1016
Epoch 1016 :: Batch 0/1
Batch Loss = 2.6831463560397
1016, epoch_train_loss=2.6831463560397
Epoch 1017
Epoch 1017 :: Batch 0/1
Batch Loss = 2.682914037303613
1017, epoch_train_loss=2.682914037303613
Epoch 1018
Epoch 1018 :: Batch 0/1
Batch Loss = 2.6829353951543635
1018, epoch_train_loss=2.6829353951543635
Epoch 1019
Epoch 1019 :: Batch 0/1
Batch Loss = 2.6827500107868927
1019, epoch_train_loss=2.6827500107868927
Epoch 1020
Epoch 1020 :: Batch 0/1
Batch Loss = 2.6825944410032356
1020, epoch_train_loss=2.6825944410032356
Epoch 1021
Epoch 1021 :: Batch 0/1
Batch Loss = 2.6825871176913445
1021, epoch_train_loss=2.6825871176913445
Epoch 1022
Epoch 1022 :: Batch 0/1
Batch Loss = 2.68240587621724
1022, epoch_train_loss=2.68240587621724
Epoch 1023
Epoch 1023 :: Batch 0/1
Batch Loss = 2.6822957103246967
1023, epoch_train_loss=2.6822957103246967
Epoch 1024
Epoch 1024 :: Batch 0/1
Batch Loss = 2.682253645320551
1024, epoch_train_loss=2.682253645320551
Epoch 1025
Epoch 1025 :: Batch 0/1
Batch Loss = 2.6820805247790953
1025, epoch_train_loss=2.6820805247790953
Epoch 1026
Epoch 1026 :: Batch 0/1
Batch Loss = 2.681993913100806
1026, epoch_train_loss=2.681993913100806
Epoch 1027
Epoch 1027 :: Batch 0/1
Batch Loss = 2.681919041385829
1027, epoch_train_loss=2.681919041385829
Epoch 1028
Epoch 1028 :: Batch 0/1
Batch Loss = 2.6817619923724347
1028, epoch_train_loss=2.6817619923724347
Epoch 1029
Epoch 1029 :: Batch 0/1
Batch Loss = 2.6816873232374956
1029, epoch_train_loss=2.6816873232374956
Epoch 1030
Epoch 1030 :: Batch 0/1
Batch Loss = 2.681591644379069
1030, epoch_train_loss=2.681591644379069
Epoch 1031
Epoch 1031 :: Batch 0/1
Batch Loss = 2.6814553577357043
1031, epoch_train_loss=2.6814553577357043
Epoch 1032
Epoch 1032 :: Batch 0/1
Batch Loss = 2.6813852620118914
1032, epoch_train_loss=2.6813852620118914
Epoch 1033
Epoch 1033 :: Batch 0/1
Batch Loss = 2.6812806070313115
1033, epoch_train_loss=2.6812806070313115
Epoch 1034
Epoch 1034 :: Batch 0/1
Batch Loss = 2.681167679908455
1034, epoch_train_loss=2.681167679908455
Epoch 1035
Epoch 1035 :: Batch 0/1
Batch Loss = 2.68109961655084
1035, epoch_train_loss=2.68109961655084
Epoch 1036
Epoch 1036 :: Batch 0/1
Batch Loss = 2.680995498831842
1036, epoch_train_loss=2.680995498831842
Epoch 1037
Epoch 1037 :: Batch 0/1
Batch Loss = 2.6809021648590345
1037, epoch_train_loss=2.6809021648590345
Epoch 1038
Epoch 1038 :: Batch 0/1
Batch Loss = 2.680832823290647
1038, epoch_train_loss=2.680832823290647
Epoch 1039
Epoch 1039 :: Batch 0/1
Batch Loss = 2.6807347569500903
1039, epoch_train_loss=2.6807347569500903
Epoch 1040
Epoch 1040 :: Batch 0/1
Batch Loss = 2.680656133447168
1040, epoch_train_loss=2.680656133447168
Epoch 1041
Epoch 1041 :: Batch 0/1
Batch Loss = 2.6805847556274007
1041, epoch_train_loss=2.6805847556274007
Epoch 1042
Epoch 1042 :: Batch 0/1
Batch Loss = 2.680494208899853
1042, epoch_train_loss=2.680494208899853
Epoch 1043
Epoch 1043 :: Batch 0/1
Batch Loss = 2.6804235763864668
1043, epoch_train_loss=2.6804235763864668
Epoch 1044
Epoch 1044 :: Batch 0/1
Batch Loss = 2.68034973520416
1044, epoch_train_loss=2.68034973520416
Epoch 1045
Epoch 1045 :: Batch 0/1
Batch Loss = 2.6802670137288978
1045, epoch_train_loss=2.6802670137288978
Epoch 1046
Epoch 1046 :: Batch 0/1
Batch Loss = 2.6802002618983916
1046, epoch_train_loss=2.6802002618983916
Epoch 1047
Epoch 1047 :: Batch 0/1
Batch Loss = 2.6801252651230576
1047, epoch_train_loss=2.6801252651230576
Epoch 1048
Epoch 1048 :: Batch 0/1
Batch Loss = 2.6800490714175575
1048, epoch_train_loss=2.6800490714175575
Epoch 1049
Epoch 1049 :: Batch 0/1
Batch Loss = 2.679983211929666
1049, epoch_train_loss=2.679983211929666
Epoch 1050
Epoch 1050 :: Batch 0/1
Batch Loss = 2.6799089216281033
1050, epoch_train_loss=2.6799089216281033
Epoch 1051
Epoch 1051 :: Batch 0/1
Batch Loss = 2.6798384884132185
1051, epoch_train_loss=2.6798384884132185
Epoch 1052
Epoch 1052 :: Batch 0/1
Batch Loss = 2.679772999516659
1052, epoch_train_loss=2.679772999516659
Epoch 1053
Epoch 1053 :: Batch 0/1
Batch Loss = 2.6797011085013067
1053, epoch_train_loss=2.6797011085013067
Epoch 1054
Epoch 1054 :: Batch 0/1
Batch Loss = 2.679635012180417
1054, epoch_train_loss=2.679635012180417
Epoch 1055
Epoch 1055 :: Batch 0/1
Batch Loss = 2.6795699699597666
1055, epoch_train_loss=2.6795699699597666
Epoch 1056
Epoch 1056 :: Batch 0/1
Batch Loss = 2.6795017312521243
1056, epoch_train_loss=2.6795017312521243
Epoch 1057
Epoch 1057 :: Batch 0/1
Batch Loss = 2.6794391002690627
1057, epoch_train_loss=2.6794391002690627
Epoch 1058
Epoch 1058 :: Batch 0/1
Batch Loss = 2.679375193148873
1058, epoch_train_loss=2.679375193148873
Epoch 1059
Epoch 1059 :: Batch 0/1
Batch Loss = 2.6793107162149514
1059, epoch_train_loss=2.6793107162149514
Epoch 1060
Epoch 1060 :: Batch 0/1
Batch Loss = 2.679250365921468
1060, epoch_train_loss=2.679250365921468
Epoch 1061
Epoch 1061 :: Batch 0/1
Batch Loss = 2.679188092394908
1061, epoch_train_loss=2.679188092394908
Epoch 1062
Epoch 1062 :: Batch 0/1
Batch Loss = 2.6791271096798344
1062, epoch_train_loss=2.6791271096798344
Epoch 1063
Epoch 1063 :: Batch 0/1
Batch Loss = 2.6790684314814506
1063, epoch_train_loss=2.6790684314814506
Epoch 1064
Epoch 1064 :: Batch 0/1
Batch Loss = 2.679008066612026
1064, epoch_train_loss=2.679008066612026
Epoch 1065
Epoch 1065 :: Batch 0/1
Batch Loss = 2.678949721701304
1065, epoch_train_loss=2.678949721701304
Epoch 1066
Epoch 1066 :: Batch 0/1
Batch Loss = 2.678892224251107
1066, epoch_train_loss=2.678892224251107
Epoch 1067
Epoch 1067 :: Batch 0/1
Batch Loss = 2.678833924870785
1067, epoch_train_loss=2.678833924870785
Epoch 1068
Epoch 1068 :: Batch 0/1
Batch Loss = 2.6787776568042876
1068, epoch_train_loss=2.6787776568042876
Epoch 1069
Epoch 1069 :: Batch 0/1
Batch Loss = 2.678721312494356
1069, epoch_train_loss=2.678721312494356
Epoch 1070
Epoch 1070 :: Batch 0/1
Batch Loss = 2.6786649758199754
1070, epoch_train_loss=2.6786649758199754
Epoch 1071
Epoch 1071 :: Batch 0/1
Batch Loss = 2.678610218715965
1071, epoch_train_loss=2.678610218715965
Epoch 1072
Epoch 1072 :: Batch 0/1
Batch Loss = 2.6785551320043592
1072, epoch_train_loss=2.6785551320043592
Epoch 1073
Epoch 1073 :: Batch 0/1
Batch Loss = 2.6785007091219293
1073, epoch_train_loss=2.6785007091219293
Epoch 1074
Epoch 1074 :: Batch 0/1
Batch Loss = 2.6784472877647336
1074, epoch_train_loss=2.6784472877647336
Epoch 1075
Epoch 1075 :: Batch 0/1
Batch Loss = 2.678393599812388
1075, epoch_train_loss=2.678393599812388
Epoch 1076
Epoch 1076 :: Batch 0/1
Batch Loss = 2.6783408339999526
1076, epoch_train_loss=2.6783408339999526
Epoch 1077
Epoch 1077 :: Batch 0/1
Batch Loss = 2.6782886062125373
1077, epoch_train_loss=2.6782886062125373
Epoch 1078
Epoch 1078 :: Batch 0/1
Batch Loss = 2.678236417682891
1078, epoch_train_loss=2.678236417682891
Epoch 1079
Epoch 1079 :: Batch 0/1
Batch Loss = 2.678185152299434
1079, epoch_train_loss=2.678185152299434
Epoch 1080
Epoch 1080 :: Batch 0/1
Batch Loss = 2.678134107924784
1080, epoch_train_loss=2.678134107924784
Epoch 1081
Epoch 1081 :: Batch 0/1
Batch Loss = 2.678083344943094
1081, epoch_train_loss=2.678083344943094
Epoch 1082
Epoch 1082 :: Batch 0/1
Batch Loss = 2.6780333266877983
1082, epoch_train_loss=2.6780333266877983
Epoch 1083
Epoch 1083 :: Batch 0/1
Batch Loss = 2.677983438211317
1083, epoch_train_loss=2.677983438211317
Epoch 1084
Epoch 1084 :: Batch 0/1
Batch Loss = 2.6779340172481785
1084, epoch_train_loss=2.6779340172481785
Epoch 1085
Epoch 1085 :: Batch 0/1
Batch Loss = 2.6778851154455743
1085, epoch_train_loss=2.6778851154455743
Epoch 1086
Epoch 1086 :: Batch 0/1
Batch Loss = 2.677836342380372
1086, epoch_train_loss=2.677836342380372
Epoch 1087
Epoch 1087 :: Batch 0/1
Batch Loss = 2.6777880812355823
1087, epoch_train_loss=2.6777880812355823
Epoch 1088
Epoch 1088 :: Batch 0/1
Batch Loss = 2.677740169630909
1088, epoch_train_loss=2.677740169630909
Epoch 1089
Epoch 1089 :: Batch 0/1
Batch Loss = 2.6776924782718834
1089, epoch_train_loss=2.6776924782718834
Epoch 1090
Epoch 1090 :: Batch 0/1
Batch Loss = 2.6776452699106588
1090, epoch_train_loss=2.6776452699106588
Epoch 1091
Epoch 1091 :: Batch 0/1
Batch Loss = 2.6775982984309743
1091, epoch_train_loss=2.6775982984309743
Epoch 1092
Epoch 1092 :: Batch 0/1
Batch Loss = 2.6775516125697156
1092, epoch_train_loss=2.6775516125697156
Epoch 1093
Epoch 1093 :: Batch 0/1
Batch Loss = 2.6775053342729818
1093, epoch_train_loss=2.6775053342729818
Epoch 1094
Epoch 1094 :: Batch 0/1
Batch Loss = 2.67745926928039
1094, epoch_train_loss=2.67745926928039
Epoch 1095
Epoch 1095 :: Batch 0/1
Batch Loss = 2.677413541184556
1095, epoch_train_loss=2.677413541184556
Epoch 1096
Epoch 1096 :: Batch 0/1
Batch Loss = 2.6773681375790694
1096, epoch_train_loss=2.6773681375790694
Epoch 1097
Epoch 1097 :: Batch 0/1
Batch Loss = 2.677322947604188
1097, epoch_train_loss=2.677322947604188
Epoch 1098
Epoch 1098 :: Batch 0/1
Batch Loss = 2.67727809559935
1098, epoch_train_loss=2.67727809559935
Epoch 1099
Epoch 1099 :: Batch 0/1
Batch Loss = 2.6772335117295616
1099, epoch_train_loss=2.6772335117295616
Epoch 1100
Epoch 1100 :: Batch 0/1
Batch Loss = 2.6771891714965057
1100, epoch_train_loss=2.6771891714965057
Epoch 1101
Epoch 1101 :: Batch 0/1
Batch Loss = 2.677145147082966
1101, epoch_train_loss=2.677145147082966
Epoch 1102
Epoch 1102 :: Batch 0/1
Batch Loss = 2.6771013513580653
1102, epoch_train_loss=2.6771013513580653
Epoch 1103
Epoch 1103 :: Batch 0/1
Batch Loss = 2.677057812405694
1103, epoch_train_loss=2.677057812405694
Epoch 1104
Epoch 1104 :: Batch 0/1
Batch Loss = 2.6770145550371556
1104, epoch_train_loss=2.6770145550371556
Epoch 1105
Epoch 1105 :: Batch 0/1
Batch Loss = 2.676971519185932
1105, epoch_train_loss=2.676971519185932
Epoch 1106
Epoch 1106 :: Batch 0/1
Batch Loss = 2.6769287483516653
1106, epoch_train_loss=2.6769287483516653
Epoch 1107
Epoch 1107 :: Batch 0/1
Batch Loss = 2.6768862250990826
1107, epoch_train_loss=2.6768862250990826
Epoch 1108
Epoch 1108 :: Batch 0/1
Batch Loss = 2.6768439206534325
1108, epoch_train_loss=2.6768439206534325
Epoch 1109
Epoch 1109 :: Batch 0/1
Batch Loss = 2.676801873304585
1109, epoch_train_loss=2.676801873304585
Epoch 1110
Epoch 1110 :: Batch 0/1
Batch Loss = 2.676760054152199
1110, epoch_train_loss=2.676760054152199
Epoch 1111
Epoch 1111 :: Batch 0/1
Batch Loss = 2.6767184624053497
1111, epoch_train_loss=2.6767184624053497
Epoch 1112
Epoch 1112 :: Batch 0/1
Batch Loss = 2.6766771151206434
1112, epoch_train_loss=2.6766771151206434
Epoch 1113
Epoch 1113 :: Batch 0/1
Batch Loss = 2.676635982864582
1113, epoch_train_loss=2.676635982864582
Epoch 1114
Epoch 1114 :: Batch 0/1
Batch Loss = 2.6765950795308013
1114, epoch_train_loss=2.6765950795308013
Epoch 1115
Epoch 1115 :: Batch 0/1
Batch Loss = 2.676554407660874
1115, epoch_train_loss=2.676554407660874
Epoch 1116
Epoch 1116 :: Batch 0/1
Batch Loss = 2.6765139504980158
1116, epoch_train_loss=2.6765139504980158
Epoch 1117
Epoch 1117 :: Batch 0/1
Batch Loss = 2.6764737229125344
1117, epoch_train_loss=2.6764737229125344
Epoch 1118
Epoch 1118 :: Batch 0/1
Batch Loss = 2.676433714957891
1118, epoch_train_loss=2.676433714957891
Epoch 1119
Epoch 1119 :: Batch 0/1
Batch Loss = 2.676393920952041
1119, epoch_train_loss=2.676393920952041
Epoch 1120
Epoch 1120 :: Batch 0/1
Batch Loss = 2.6763543521016073
1120, epoch_train_loss=2.6763543521016073
Epoch 1121
Epoch 1121 :: Batch 0/1
Batch Loss = 2.676314997642583
1121, epoch_train_loss=2.676314997642583
Epoch 1122
Epoch 1122 :: Batch 0/1
Batch Loss = 2.676275859946136
1122, epoch_train_loss=2.676275859946136
Epoch 1123
Epoch 1123 :: Batch 0/1
Batch Loss = 2.676236941805003
1123, epoch_train_loss=2.676236941805003
Epoch 1124
Epoch 1124 :: Batch 0/1
Batch Loss = 2.676198233953802
1124, epoch_train_loss=2.676198233953802
Epoch 1125
Epoch 1125 :: Batch 0/1
Batch Loss = 2.6761597422268584
1125, epoch_train_loss=2.6761597422268584
Epoch 1126
Epoch 1126 :: Batch 0/1
Batch Loss = 2.676121465407604
1126, epoch_train_loss=2.676121465407604
Epoch 1127
Epoch 1127 :: Batch 0/1
Batch Loss = 2.6760833993108912
1127, epoch_train_loss=2.6760833993108912
Epoch 1128
Epoch 1128 :: Batch 0/1
Batch Loss = 2.676045548411523
1128, epoch_train_loss=2.676045548411523
Epoch 1129
Epoch 1129 :: Batch 0/1
Batch Loss = 2.676007908077126
1129, epoch_train_loss=2.676007908077126
Epoch 1130
Epoch 1130 :: Batch 0/1
Batch Loss = 2.6759704777344018
1130, epoch_train_loss=2.6759704777344018
Epoch 1131
Epoch 1131 :: Batch 0/1
Batch Loss = 2.675933260138109
1131, epoch_train_loss=2.675933260138109
Epoch 1132
Epoch 1132 :: Batch 0/1
Batch Loss = 2.67589625143909
1132, epoch_train_loss=2.67589625143909
Epoch 1133
Epoch 1133 :: Batch 0/1
Batch Loss = 2.6758594529376927
1133, epoch_train_loss=2.6758594529376927
Epoch 1134
Epoch 1134 :: Batch 0/1
Batch Loss = 2.6758228641958652
1134, epoch_train_loss=2.6758228641958652
Epoch 1135
Epoch 1135 :: Batch 0/1
Batch Loss = 2.675786482247848
1135, epoch_train_loss=2.675786482247848
Epoch 1136
Epoch 1136 :: Batch 0/1
Batch Loss = 2.6757503088904167
1136, epoch_train_loss=2.6757503088904167
Epoch 1137
Epoch 1137 :: Batch 0/1
Batch Loss = 2.6757143426484093
1137, epoch_train_loss=2.6757143426484093
Epoch 1138
Epoch 1138 :: Batch 0/1
Batch Loss = 2.6756785821751192
1138, epoch_train_loss=2.6756785821751192
Epoch 1139
Epoch 1139 :: Batch 0/1
Batch Loss = 2.6756430280219323
1139, epoch_train_loss=2.6756430280219323
Epoch 1140
Epoch 1140 :: Batch 0/1
Batch Loss = 2.6756076776907127
1140, epoch_train_loss=2.6756076776907127
Epoch 1141
Epoch 1141 :: Batch 0/1
Batch Loss = 2.6755725307001748
1141, epoch_train_loss=2.6755725307001748
Epoch 1142
Epoch 1142 :: Batch 0/1
Batch Loss = 2.675537586755878
1142, epoch_train_loss=2.675537586755878
Epoch 1143
Epoch 1143 :: Batch 0/1
Batch Loss = 2.675502843740898
1143, epoch_train_loss=2.675502843740898
Epoch 1144
Epoch 1144 :: Batch 0/1
Batch Loss = 2.6754683012150893
1144, epoch_train_loss=2.6754683012150893
Epoch 1145
Epoch 1145 :: Batch 0/1
Batch Loss = 2.6754339575661334
1145, epoch_train_loss=2.6754339575661334
Epoch 1146
Epoch 1146 :: Batch 0/1
Batch Loss = 2.675399810869717
1146, epoch_train_loss=2.675399810869717
Epoch 1147
Epoch 1147 :: Batch 0/1
Batch Loss = 2.6753658604638546
1147, epoch_train_loss=2.6753658604638546
Epoch 1148
Epoch 1148 :: Batch 0/1
Batch Loss = 2.6753321043740907
1148, epoch_train_loss=2.6753321043740907
Epoch 1149
Epoch 1149 :: Batch 0/1
Batch Loss = 2.675298540946736
1149, epoch_train_loss=2.675298540946736
Epoch 1150
Epoch 1150 :: Batch 0/1
Batch Loss = 2.675265168714683
1150, epoch_train_loss=2.675265168714683
Epoch 1151
Epoch 1151 :: Batch 0/1
Batch Loss = 2.6752319852664237
1151, epoch_train_loss=2.6752319852664237
Epoch 1152
Epoch 1152 :: Batch 0/1
Batch Loss = 2.675198988961662
1152, epoch_train_loss=2.675198988961662
Epoch 1153
Epoch 1153 :: Batch 0/1
Batch Loss = 2.6751661778786597
1153, epoch_train_loss=2.6751661778786597
Epoch 1154
Epoch 1154 :: Batch 0/1
Batch Loss = 2.6751335496250106
1154, epoch_train_loss=2.6751335496250106
Epoch 1155
Epoch 1155 :: Batch 0/1
Batch Loss = 2.6751011022396534
1155, epoch_train_loss=2.6751011022396534
Epoch 1156
Epoch 1156 :: Batch 0/1
Batch Loss = 2.675068833197158
1156, epoch_train_loss=2.675068833197158
Epoch 1157
Epoch 1157 :: Batch 0/1
Batch Loss = 2.675036740015851
1157, epoch_train_loss=2.675036740015851
Epoch 1158
Epoch 1158 :: Batch 0/1
Batch Loss = 2.6750048204548733
1158, epoch_train_loss=2.6750048204548733
Epoch 1159
Epoch 1159 :: Batch 0/1
Batch Loss = 2.674973071787299
1159, epoch_train_loss=2.674973071787299
Epoch 1160
Epoch 1160 :: Batch 0/1
Batch Loss = 2.67494149142428
1160, epoch_train_loss=2.67494149142428
Epoch 1161
Epoch 1161 :: Batch 0/1
Batch Loss = 2.67491007667303
1161, epoch_train_loss=2.67491007667303
Epoch 1162
Epoch 1162 :: Batch 0/1
Batch Loss = 2.674878824575035
1162, epoch_train_loss=2.674878824575035
Epoch 1163
Epoch 1163 :: Batch 0/1
Batch Loss = 2.67484773240278
1163, epoch_train_loss=2.67484773240278
Epoch 1164
Epoch 1164 :: Batch 0/1
Batch Loss = 2.6748167972396577
1164, epoch_train_loss=2.6748167972396577
Epoch 1165
Epoch 1165 :: Batch 0/1
Batch Loss = 2.6747860160499406
1165, epoch_train_loss=2.6747860160499406
Epoch 1166
Epoch 1166 :: Batch 0/1
Batch Loss = 2.674755385881227
1166, epoch_train_loss=2.674755385881227
Epoch 1167
Epoch 1167 :: Batch 0/1
Batch Loss = 2.6747249035542984
1167, epoch_train_loss=2.6747249035542984
Epoch 1168
Epoch 1168 :: Batch 0/1
Batch Loss = 2.674694565959189
1168, epoch_train_loss=2.674694565959189
Epoch 1169
Epoch 1169 :: Batch 0/1
Batch Loss = 2.6746643699996393
1169, epoch_train_loss=2.6746643699996393
Epoch 1170
Epoch 1170 :: Batch 0/1
Batch Loss = 2.6746343124278416
1170, epoch_train_loss=2.6746343124278416
Epoch 1171
Epoch 1171 :: Batch 0/1
Batch Loss = 2.674604390054861
1171, epoch_train_loss=2.674604390054861
Epoch 1172
Epoch 1172 :: Batch 0/1
Batch Loss = 2.674574599620901
1172, epoch_train_loss=2.674574599620901
Epoch 1173
Epoch 1173 :: Batch 0/1
Batch Loss = 2.6745449378186654
1173, epoch_train_loss=2.6745449378186654
Epoch 1174
Epoch 1174 :: Batch 0/1
Batch Loss = 2.6745154014227084
1174, epoch_train_loss=2.6745154014227084
Epoch 1175
Epoch 1175 :: Batch 0/1
Batch Loss = 2.674485987136508
1175, epoch_train_loss=2.674485987136508
Epoch 1176
Epoch 1176 :: Batch 0/1
Batch Loss = 2.6744566916650463
1176, epoch_train_loss=2.6744566916650463
Epoch 1177
Epoch 1177 :: Batch 0/1
Batch Loss = 2.674427511738182
1177, epoch_train_loss=2.674427511738182
Epoch 1178
Epoch 1178 :: Batch 0/1
Batch Loss = 2.674398444039499
1178, epoch_train_loss=2.674398444039499
Epoch 1179
Epoch 1179 :: Batch 0/1
Batch Loss = 2.67436948531235
1179, epoch_train_loss=2.67436948531235
Epoch 1180
Epoch 1180 :: Batch 0/1
Batch Loss = 2.6743406323171746
1180, epoch_train_loss=2.6743406323171746
Epoch 1181
Epoch 1181 :: Batch 0/1
Batch Loss = 2.6743118818008322
1181, epoch_train_loss=2.6743118818008322
Epoch 1182
Epoch 1182 :: Batch 0/1
Batch Loss = 2.6742832305663535
1182, epoch_train_loss=2.6742832305663535
Epoch 1183
Epoch 1183 :: Batch 0/1
Batch Loss = 2.674254675417247
1183, epoch_train_loss=2.674254675417247
Epoch 1184
Epoch 1184 :: Batch 0/1
Batch Loss = 2.6742262131918855
1184, epoch_train_loss=2.6742262131918855
Epoch 1185
Epoch 1185 :: Batch 0/1
Batch Loss = 2.6741978407873788
1185, epoch_train_loss=2.6741978407873788
Epoch 1186
Epoch 1186 :: Batch 0/1
Batch Loss = 2.674169555120289
1186, epoch_train_loss=2.674169555120289
Epoch 1187
Epoch 1187 :: Batch 0/1
Batch Loss = 2.6741413531542033
1187, epoch_train_loss=2.6741413531542033
Epoch 1188
Epoch 1188 :: Batch 0/1
Batch Loss = 2.6741132318953977
1188, epoch_train_loss=2.6741132318953977
Epoch 1189
Epoch 1189 :: Batch 0/1
Batch Loss = 2.6740851883888563
1189, epoch_train_loss=2.6740851883888563
Epoch 1190
Epoch 1190 :: Batch 0/1
Batch Loss = 2.674057219744308
1190, epoch_train_loss=2.674057219744308
Epoch 1191
Epoch 1191 :: Batch 0/1
Batch Loss = 2.674029323123586
1191, epoch_train_loss=2.674029323123586
Epoch 1192
Epoch 1192 :: Batch 0/1
Batch Loss = 2.6740014957344824
1192, epoch_train_loss=2.6740014957344824
Epoch 1193
Epoch 1193 :: Batch 0/1
Batch Loss = 2.673973734846315
1193, epoch_train_loss=2.673973734846315
Epoch 1194
Epoch 1194 :: Batch 0/1
Batch Loss = 2.6739460377811213
1194, epoch_train_loss=2.6739460377811213
Epoch 1195
Epoch 1195 :: Batch 0/1
Batch Loss = 2.6739184019216586
1195, epoch_train_loss=2.6739184019216586
Epoch 1196
Epoch 1196 :: Batch 0/1
Batch Loss = 2.6738908247218913
1196, epoch_train_loss=2.6738908247218913
Epoch 1197
Epoch 1197 :: Batch 0/1
Batch Loss = 2.673863303688042
1197, epoch_train_loss=2.673863303688042
Epoch 1198
Epoch 1198 :: Batch 0/1
Batch Loss = 2.6738358363884944
1198, epoch_train_loss=2.6738358363884944
Epoch 1199
Epoch 1199 :: Batch 0/1
Batch Loss = 2.673808420461391
1199, epoch_train_loss=2.673808420461391
Epoch 1200
Epoch 1200 :: Batch 0/1
Batch Loss = 2.6737810536024154
1200, epoch_train_loss=2.6737810536024154
Epoch 1201
Epoch 1201 :: Batch 0/1
Batch Loss = 2.67375373357607
1201, epoch_train_loss=2.67375373357607
Epoch 1202
Epoch 1202 :: Batch 0/1
Batch Loss = 2.673726458210624
1202, epoch_train_loss=2.673726458210624
Epoch 1203
Epoch 1203 :: Batch 0/1
Batch Loss = 2.673699225400067
1203, epoch_train_loss=2.673699225400067
Epoch 1204
Epoch 1204 :: Batch 0/1
Batch Loss = 2.673672033099563
1204, epoch_train_loss=2.673672033099563
Epoch 1205
Epoch 1205 :: Batch 0/1
Batch Loss = 2.6736448793271594
1205, epoch_train_loss=2.6736448793271594
Epoch 1206
Epoch 1206 :: Batch 0/1
Batch Loss = 2.6736177621695436
1206, epoch_train_loss=2.6736177621695436
Epoch 1207
Epoch 1207 :: Batch 0/1
Batch Loss = 2.673590679770843
1207, epoch_train_loss=2.673590679770843
Epoch 1208
Epoch 1208 :: Batch 0/1
Batch Loss = 2.6735636303435273
1208, epoch_train_loss=2.6735636303435273
Epoch 1209
Epoch 1209 :: Batch 0/1
Batch Loss = 2.6735366121559454
1209, epoch_train_loss=2.6735366121559454
Epoch 1210
Epoch 1210 :: Batch 0/1
Batch Loss = 2.6735096235372193
1210, epoch_train_loss=2.6735096235372193
Epoch 1211
Epoch 1211 :: Batch 0/1
Batch Loss = 2.6734826628757493
1211, epoch_train_loss=2.6734826628757493
Epoch 1212
Epoch 1212 :: Batch 0/1
Batch Loss = 2.6734557286199268
1212, epoch_train_loss=2.6734557286199268
Epoch 1213
Epoch 1213 :: Batch 0/1
Batch Loss = 2.6734288192756495
1213, epoch_train_loss=2.6734288192756495
Epoch 1214
Epoch 1214 :: Batch 0/1
Batch Loss = 2.673401933397734
1214, epoch_train_loss=2.673401933397734
Epoch 1215
Epoch 1215 :: Batch 0/1
Batch Loss = 2.673375069600996
1215, epoch_train_loss=2.673375069600996
Epoch 1216
Epoch 1216 :: Batch 0/1
Batch Loss = 2.6733482265518416
1216, epoch_train_loss=2.6733482265518416
Epoch 1217
Epoch 1217 :: Batch 0/1
Batch Loss = 2.6733214029647736
1217, epoch_train_loss=2.6733214029647736
Epoch 1218
Epoch 1218 :: Batch 0/1
Batch Loss = 2.673294597603667
1218, epoch_train_loss=2.673294597603667
Epoch 1219
Epoch 1219 :: Batch 0/1
Batch Loss = 2.6732678092837157
1219, epoch_train_loss=2.6732678092837157
Epoch 1220
Epoch 1220 :: Batch 0/1
Batch Loss = 2.6732410368642268
1220, epoch_train_loss=2.6732410368642268
Epoch 1221
Epoch 1221 :: Batch 0/1
Batch Loss = 2.6732142792482936
1221, epoch_train_loss=2.6732142792482936
Epoch 1222
Epoch 1222 :: Batch 0/1
Batch Loss = 2.6731875353822474
1222, epoch_train_loss=2.6731875353822474
Epoch 1223
Epoch 1223 :: Batch 0/1
Batch Loss = 2.6731608042520874
1223, epoch_train_loss=2.6731608042520874
Epoch 1224
Epoch 1224 :: Batch 0/1
Batch Loss = 2.6731340848869842
1224, epoch_train_loss=2.6731340848869842
Epoch 1225
Epoch 1225 :: Batch 0/1
Batch Loss = 2.6731073763517226
1225, epoch_train_loss=2.6731073763517226
Epoch 1226
Epoch 1226 :: Batch 0/1
Batch Loss = 2.6730806777468845
1226, epoch_train_loss=2.6730806777468845
Epoch 1227
Epoch 1227 :: Batch 0/1
Batch Loss = 2.673053988207424
1227, epoch_train_loss=2.673053988207424
Epoch 1228
Epoch 1228 :: Batch 0/1
Batch Loss = 2.6730273068996375
1228, epoch_train_loss=2.6730273068996375
Epoch 1229
Epoch 1229 :: Batch 0/1
Batch Loss = 2.6730006330251586
1229, epoch_train_loss=2.6730006330251586
Epoch 1230
Epoch 1230 :: Batch 0/1
Batch Loss = 2.672973965812958
1230, epoch_train_loss=2.672973965812958
Epoch 1231
Epoch 1231 :: Batch 0/1
Batch Loss = 2.6729473045199597
1231, epoch_train_loss=2.6729473045199597
Epoch 1232
Epoch 1232 :: Batch 0/1
Batch Loss = 2.672920648429439
1232, epoch_train_loss=2.672920648429439
Epoch 1233
Epoch 1233 :: Batch 0/1
Batch Loss = 2.6728939968479937
1233, epoch_train_loss=2.6728939968479937
Epoch 1234
Epoch 1234 :: Batch 0/1
Batch Loss = 2.6728673491081674
1234, epoch_train_loss=2.6728673491081674
Epoch 1235
Epoch 1235 :: Batch 0/1
Batch Loss = 2.672840704566662
1235, epoch_train_loss=2.672840704566662
Epoch 1236
Epoch 1236 :: Batch 0/1
Batch Loss = 2.6728140625967853
1236, epoch_train_loss=2.6728140625967853
Epoch 1237
Epoch 1237 :: Batch 0/1
Batch Loss = 2.6727874225954245
1237, epoch_train_loss=2.6727874225954245
Epoch 1238
Epoch 1238 :: Batch 0/1
Batch Loss = 2.6727607839757868
1238, epoch_train_loss=2.6727607839757868
Epoch 1239
Epoch 1239 :: Batch 0/1
Batch Loss = 2.672734146166361
1239, epoch_train_loss=2.672734146166361
Epoch 1240
Epoch 1240 :: Batch 0/1
Batch Loss = 2.6727075086157135
1240, epoch_train_loss=2.6727075086157135
Epoch 1241
Epoch 1241 :: Batch 0/1
Batch Loss = 2.672680870782944
1241, epoch_train_loss=2.672680870782944
Epoch 1242
Epoch 1242 :: Batch 0/1
Batch Loss = 2.6726542321428757
1242, epoch_train_loss=2.6726542321428757
Epoch 1243
Epoch 1243 :: Batch 0/1
Batch Loss = 2.6726275921830482
1243, epoch_train_loss=2.6726275921830482
Epoch 1244
Epoch 1244 :: Batch 0/1
Batch Loss = 2.672600950405076
1244, epoch_train_loss=2.672600950405076
Epoch 1245
Epoch 1245 :: Batch 0/1
Batch Loss = 2.6725743063173413
1245, epoch_train_loss=2.6725743063173413
Epoch 1246
Epoch 1246 :: Batch 0/1
Batch Loss = 2.672547659440238
1246, epoch_train_loss=2.672547659440238
Epoch 1247
Epoch 1247 :: Batch 0/1
Batch Loss = 2.6725210093053526
1247, epoch_train_loss=2.6725210093053526
Epoch 1248
Epoch 1248 :: Batch 0/1
Batch Loss = 2.6724943554484546
1248, epoch_train_loss=2.6724943554484546
Epoch 1249
Epoch 1249 :: Batch 0/1
Batch Loss = 2.6724676974152213
1249, epoch_train_loss=2.6724676974152213
Epoch 1250
Epoch 1250 :: Batch 0/1
Batch Loss = 2.672441034758505
1250, epoch_train_loss=2.672441034758505
Epoch 1251
Epoch 1251 :: Batch 0/1
Batch Loss = 2.6724143670396447
1251, epoch_train_loss=2.6724143670396447
Epoch 1252
Epoch 1252 :: Batch 0/1
Batch Loss = 2.672387693821562
1252, epoch_train_loss=2.672387693821562
Epoch 1253
Epoch 1253 :: Batch 0/1
Batch Loss = 2.672361014674504
1253, epoch_train_loss=2.672361014674504
Epoch 1254
Epoch 1254 :: Batch 0/1
Batch Loss = 2.6723343291735646
1254, epoch_train_loss=2.6723343291735646
Epoch 1255
Epoch 1255 :: Batch 0/1
Batch Loss = 2.6723076368983167
1255, epoch_train_loss=2.6723076368983167
Epoch 1256
Epoch 1256 :: Batch 0/1
Batch Loss = 2.672280937432304
1256, epoch_train_loss=2.672280937432304
Epoch 1257
Epoch 1257 :: Batch 0/1
Batch Loss = 2.6722542303625842
1257, epoch_train_loss=2.6722542303625842
Epoch 1258
Epoch 1258 :: Batch 0/1
Batch Loss = 2.6722275152793715
1258, epoch_train_loss=2.6722275152793715
Epoch 1259
Epoch 1259 :: Batch 0/1
Batch Loss = 2.672200791775787
1259, epoch_train_loss=2.672200791775787
Epoch 1260
Epoch 1260 :: Batch 0/1
Batch Loss = 2.672174059447674
1260, epoch_train_loss=2.672174059447674
Epoch 1261
Epoch 1261 :: Batch 0/1
Batch Loss = 2.672147317893327
1261, epoch_train_loss=2.672147317893327
Epoch 1262
Epoch 1262 :: Batch 0/1
Batch Loss = 2.6721205667131933
1262, epoch_train_loss=2.6721205667131933
Epoch 1263
Epoch 1263 :: Batch 0/1
Batch Loss = 2.6720938055096464
1263, epoch_train_loss=2.6720938055096464
Epoch 1264
Epoch 1264 :: Batch 0/1
Batch Loss = 2.6720670338868535
1264, epoch_train_loss=2.6720670338868535
Epoch 1265
Epoch 1265 :: Batch 0/1
Batch Loss = 2.6720402514507042
1265, epoch_train_loss=2.6720402514507042
Epoch 1266
Epoch 1266 :: Batch 0/1
Batch Loss = 2.672013457806606
1266, epoch_train_loss=2.672013457806606
Epoch 1267
Epoch 1267 :: Batch 0/1
Batch Loss = 2.67198665256564
1267, epoch_train_loss=2.67198665256564
Epoch 1268
Epoch 1268 :: Batch 0/1
Batch Loss = 2.6719598353381198
1268, epoch_train_loss=2.6719598353381198
Epoch 1269
Epoch 1269 :: Batch 0/1
Batch Loss = 2.67193300573564
1269, epoch_train_loss=2.67193300573564
Epoch 1270
Epoch 1270 :: Batch 0/1
Batch Loss = 2.6719061633689707
1270, epoch_train_loss=2.6719061633689707
Epoch 1271
Epoch 1271 :: Batch 0/1
Batch Loss = 2.6718793078543834
1271, epoch_train_loss=2.6718793078543834
Epoch 1272
Epoch 1272 :: Batch 0/1
Batch Loss = 2.6718524388051716
1272, epoch_train_loss=2.6718524388051716
Epoch 1273
Epoch 1273 :: Batch 0/1
Batch Loss = 2.6718255558400292
1273, epoch_train_loss=2.6718255558400292
Epoch 1274
Epoch 1274 :: Batch 0/1
Batch Loss = 2.6717986585767117
1274, epoch_train_loss=2.6717986585767117
Epoch 1275
Epoch 1275 :: Batch 0/1
Batch Loss = 2.671771746634182
1275, epoch_train_loss=2.671771746634182
Epoch 1276
Epoch 1276 :: Batch 0/1
Batch Loss = 2.6717448196326736
1276, epoch_train_loss=2.6717448196326736
Epoch 1277
Epoch 1277 :: Batch 0/1
Batch Loss = 2.6717178771915906
1277, epoch_train_loss=2.6717178771915906
Epoch 1278
Epoch 1278 :: Batch 0/1
Batch Loss = 2.6716909189358446
1278, epoch_train_loss=2.6716909189358446
Epoch 1279
Epoch 1279 :: Batch 0/1
Batch Loss = 2.6716639444874017
1279, epoch_train_loss=2.6716639444874017
Epoch 1280
Epoch 1280 :: Batch 0/1
Batch Loss = 2.6716369534716815
1280, epoch_train_loss=2.6716369534716815
Epoch 1281
Epoch 1281 :: Batch 0/1
Batch Loss = 2.6716099455176354
1281, epoch_train_loss=2.6716099455176354
Epoch 1282
Epoch 1282 :: Batch 0/1
Batch Loss = 2.671582920251437
1282, epoch_train_loss=2.671582920251437
Epoch 1283
Epoch 1283 :: Batch 0/1
Batch Loss = 2.67155587730498
1283, epoch_train_loss=2.67155587730498
Epoch 1284
Epoch 1284 :: Batch 0/1
Batch Loss = 2.671528816307403
1284, epoch_train_loss=2.671528816307403
Epoch 1285
Epoch 1285 :: Batch 0/1
Batch Loss = 2.671501736893628
1285, epoch_train_loss=2.671501736893628
Epoch 1286
Epoch 1286 :: Batch 0/1
Batch Loss = 2.6714746386959205
1286, epoch_train_loss=2.6714746386959205
Epoch 1287
Epoch 1287 :: Batch 0/1
Batch Loss = 2.6714475213503204
1287, epoch_train_loss=2.6714475213503204
Epoch 1288
Epoch 1288 :: Batch 0/1
Batch Loss = 2.6714203844945392
1288, epoch_train_loss=2.6714203844945392
Epoch 1289
Epoch 1289 :: Batch 0/1
Batch Loss = 2.671393227770101
1289, epoch_train_loss=2.671393227770101
Epoch 1290
Epoch 1290 :: Batch 0/1
Batch Loss = 2.6713660508159602
1290, epoch_train_loss=2.6713660508159602
Epoch 1291
Epoch 1291 :: Batch 0/1
Batch Loss = 2.6713388532749374
1291, epoch_train_loss=2.6713388532749374
Epoch 1292
Epoch 1292 :: Batch 0/1
Batch Loss = 2.671311634791625
1292, epoch_train_loss=2.671311634791625
Epoch 1293
Epoch 1293 :: Batch 0/1
Batch Loss = 2.6712843950123983
1293, epoch_train_loss=2.6712843950123983
Epoch 1294
Epoch 1294 :: Batch 0/1
Batch Loss = 2.6712571335875497
1294, epoch_train_loss=2.6712571335875497
Epoch 1295
Epoch 1295 :: Batch 0/1
Batch Loss = 2.671229850164851
1295, epoch_train_loss=2.671229850164851
Epoch 1296
Epoch 1296 :: Batch 0/1
Batch Loss = 2.6712025443959817
1296, epoch_train_loss=2.6712025443959817
Epoch 1297
Epoch 1297 :: Batch 0/1
Batch Loss = 2.671175215934399
1297, epoch_train_loss=2.671175215934399
Epoch 1298
Epoch 1298 :: Batch 0/1
Batch Loss = 2.6711478644353264
1298, epoch_train_loss=2.6711478644353264
Epoch 1299
Epoch 1299 :: Batch 0/1
Batch Loss = 2.6711204895578726
1299, epoch_train_loss=2.6711204895578726
Epoch 1300
Epoch 1300 :: Batch 0/1
Batch Loss = 2.6710930909563846
1300, epoch_train_loss=2.6710930909563846
Epoch 1301
Epoch 1301 :: Batch 0/1
Batch Loss = 2.671065668293328
1301, epoch_train_loss=2.671065668293328
Epoch 1302
Epoch 1302 :: Batch 0/1
Batch Loss = 2.671038221230652
1302, epoch_train_loss=2.671038221230652
Epoch 1303
Epoch 1303 :: Batch 0/1
Batch Loss = 2.6710107494340614
1303, epoch_train_loss=2.6710107494340614
Epoch 1304
Epoch 1304 :: Batch 0/1
Batch Loss = 2.670983252564323
1304, epoch_train_loss=2.670983252564323
Epoch 1305
Epoch 1305 :: Batch 0/1
Batch Loss = 2.6709557302901503
1305, epoch_train_loss=2.6709557302901503
Epoch 1306
Epoch 1306 :: Batch 0/1
Batch Loss = 2.670928182279499
1306, epoch_train_loss=2.670928182279499
Epoch 1307
Epoch 1307 :: Batch 0/1
Batch Loss = 2.670900608201662
1307, epoch_train_loss=2.670900608201662
Epoch 1308
Epoch 1308 :: Batch 0/1
Batch Loss = 2.670873007727199
1308, epoch_train_loss=2.670873007727199
Epoch 1309
Epoch 1309 :: Batch 0/1
Batch Loss = 2.670845380525683
1309, epoch_train_loss=2.670845380525683
Epoch 1310
Epoch 1310 :: Batch 0/1
Batch Loss = 2.670817726274283
1310, epoch_train_loss=2.670817726274283
Epoch 1311
Epoch 1311 :: Batch 0/1
Batch Loss = 2.670790044642457
1311, epoch_train_loss=2.670790044642457
Epoch 1312
Epoch 1312 :: Batch 0/1
Batch Loss = 2.670762335304889
1312, epoch_train_loss=2.670762335304889
Epoch 1313
Epoch 1313 :: Batch 0/1
Batch Loss = 2.6707345979414057
1313, epoch_train_loss=2.6707345979414057
Epoch 1314
Epoch 1314 :: Batch 0/1
Batch Loss = 2.670706832223776
1314, epoch_train_loss=2.670706832223776
Epoch 1315
Epoch 1315 :: Batch 0/1
Batch Loss = 2.670679037828669
1315, epoch_train_loss=2.670679037828669
Epoch 1316
Epoch 1316 :: Batch 0/1
Batch Loss = 2.670651214437533
1316, epoch_train_loss=2.670651214437533
Epoch 1317
Epoch 1317 :: Batch 0/1
Batch Loss = 2.670623361723336
1317, epoch_train_loss=2.670623361723336
Epoch 1318
Epoch 1318 :: Batch 0/1
Batch Loss = 2.6705954793635605
1318, epoch_train_loss=2.6705954793635605
Epoch 1319
Epoch 1319 :: Batch 0/1
Batch Loss = 2.670567567040073
1319, epoch_train_loss=2.670567567040073
Epoch 1320
Epoch 1320 :: Batch 0/1
Batch Loss = 2.670539624425801
1320, epoch_train_loss=2.670539624425801
Epoch 1321
Epoch 1321 :: Batch 0/1
Batch Loss = 2.670511651197736
1321, epoch_train_loss=2.670511651197736
Epoch 1322
Epoch 1322 :: Batch 0/1
Batch Loss = 2.6704836470345863
1322, epoch_train_loss=2.6704836470345863
Epoch 1323
Epoch 1323 :: Batch 0/1
Batch Loss = 2.67045561161
1323, epoch_train_loss=2.67045561161
Epoch 1324
Epoch 1324 :: Batch 0/1
Batch Loss = 2.670427544601213
1324, epoch_train_loss=2.670427544601213
Epoch 1325
Epoch 1325 :: Batch 0/1
Batch Loss = 2.6703994456800357
1325, epoch_train_loss=2.6703994456800357
Epoch 1326
Epoch 1326 :: Batch 0/1
Batch Loss = 2.6703713145192896
1326, epoch_train_loss=2.6703713145192896
Epoch 1327
Epoch 1327 :: Batch 0/1
Batch Loss = 2.670343150792615
1327, epoch_train_loss=2.670343150792615
Epoch 1328
Epoch 1328 :: Batch 0/1
Batch Loss = 2.6703149541676345
1328, epoch_train_loss=2.6703149541676345
Epoch 1329
Epoch 1329 :: Batch 0/1
Batch Loss = 2.6702867243146122
1329, epoch_train_loss=2.6702867243146122
Epoch 1330
Epoch 1330 :: Batch 0/1
Batch Loss = 2.6702584608973683
1330, epoch_train_loss=2.6702584608973683
Epoch 1331
Epoch 1331 :: Batch 0/1
Batch Loss = 2.6702301635797197
1331, epoch_train_loss=2.6702301635797197
Epoch 1332
Epoch 1332 :: Batch 0/1
Batch Loss = 2.670201832025263
1332, epoch_train_loss=2.670201832025263
Epoch 1333
Epoch 1333 :: Batch 0/1
Batch Loss = 2.670173465888227
1333, epoch_train_loss=2.670173465888227
Epoch 1334
Epoch 1334 :: Batch 0/1
Batch Loss = 2.670145064828853
1334, epoch_train_loss=2.670145064828853
Epoch 1335
Epoch 1335 :: Batch 0/1
Batch Loss = 2.670116628497534
1335, epoch_train_loss=2.670116628497534
Epoch 1336
Epoch 1336 :: Batch 0/1
Batch Loss = 2.6700881565457304
1336, epoch_train_loss=2.6700881565457304
Epoch 1337
Epoch 1337 :: Batch 0/1
Batch Loss = 2.670059648614516
1337, epoch_train_loss=2.670059648614516
Epoch 1338
Epoch 1338 :: Batch 0/1
Batch Loss = 2.6700311043500093
1338, epoch_train_loss=2.6700311043500093
Epoch 1339
Epoch 1339 :: Batch 0/1
Batch Loss = 2.670002523387402
1339, epoch_train_loss=2.670002523387402
Epoch 1340
Epoch 1340 :: Batch 0/1
Batch Loss = 2.6699739053596616
1340, epoch_train_loss=2.6699739053596616
Epoch 1341
Epoch 1341 :: Batch 0/1
Batch Loss = 2.6699452498949983
1341, epoch_train_loss=2.6699452498949983
Epoch 1342
Epoch 1342 :: Batch 0/1
Batch Loss = 2.669916556616574
1342, epoch_train_loss=2.669916556616574
Epoch 1343
Epoch 1343 :: Batch 0/1
Batch Loss = 2.6698878251422
1343, epoch_train_loss=2.6698878251422
Epoch 1344
Epoch 1344 :: Batch 0/1
Batch Loss = 2.6698590550840344
1344, epoch_train_loss=2.6698590550840344
Epoch 1345
Epoch 1345 :: Batch 0/1
Batch Loss = 2.669830246048265
1345, epoch_train_loss=2.669830246048265
Epoch 1346
Epoch 1346 :: Batch 0/1
Batch Loss = 2.6698013976347954
1346, epoch_train_loss=2.6698013976347954
Epoch 1347
Epoch 1347 :: Batch 0/1
Batch Loss = 2.6697725094369065
1347, epoch_train_loss=2.6697725094369065
Epoch 1348
Epoch 1348 :: Batch 0/1
Batch Loss = 2.6697435810409256
1348, epoch_train_loss=2.6697435810409256
Epoch 1349
Epoch 1349 :: Batch 0/1
Batch Loss = 2.6697146120236104
1349, epoch_train_loss=2.6697146120236104
Epoch 1350
Epoch 1350 :: Batch 0/1
Batch Loss = 2.669685601960932
1350, epoch_train_loss=2.669685601960932
Epoch 1351
Epoch 1351 :: Batch 0/1
Batch Loss = 2.669656550411751
1351, epoch_train_loss=2.669656550411751
Epoch 1352
Epoch 1352 :: Batch 0/1
Batch Loss = 2.669627456933468
1352, epoch_train_loss=2.669627456933468
Epoch 1353
Epoch 1353 :: Batch 0/1
Batch Loss = 2.669598321070244
1353, epoch_train_loss=2.669598321070244
Epoch 1354
Epoch 1354 :: Batch 0/1
Batch Loss = 2.6695691423641468
1354, epoch_train_loss=2.6695691423641468
Epoch 1355
Epoch 1355 :: Batch 0/1
Batch Loss = 2.669539920338761
1355, epoch_train_loss=2.669539920338761
Epoch 1356
Epoch 1356 :: Batch 0/1
Batch Loss = 2.6695106545149865
1356, epoch_train_loss=2.6695106545149865
Epoch 1357
Epoch 1357 :: Batch 0/1
Batch Loss = 2.6694813444015804
1357, epoch_train_loss=2.6694813444015804
Epoch 1358
Epoch 1358 :: Batch 0/1
Batch Loss = 2.6694519894949322
1358, epoch_train_loss=2.6694519894949322
Epoch 1359
Epoch 1359 :: Batch 0/1
Batch Loss = 2.669422589285833
1359, epoch_train_loss=2.669422589285833
Epoch 1360
Epoch 1360 :: Batch 0/1
Batch Loss = 2.6693931432501263
1360, epoch_train_loss=2.6693931432501263
Epoch 1361
Epoch 1361 :: Batch 0/1
Batch Loss = 2.669363650857969
1361, epoch_train_loss=2.669363650857969
Epoch 1362
Epoch 1362 :: Batch 0/1
Batch Loss = 2.6693341115669567
1362, epoch_train_loss=2.6693341115669567
Epoch 1363
Epoch 1363 :: Batch 0/1
Batch Loss = 2.6693045248223135
1363, epoch_train_loss=2.6693045248223135
Epoch 1364
Epoch 1364 :: Batch 0/1
Batch Loss = 2.6692748900642105
1364, epoch_train_loss=2.6692748900642105
Epoch 1365
Epoch 1365 :: Batch 0/1
Batch Loss = 2.669245206718929
1365, epoch_train_loss=2.669245206718929
Epoch 1366
Epoch 1366 :: Batch 0/1
Batch Loss = 2.6692154742088423
1366, epoch_train_loss=2.6692154742088423
Epoch 1367
Epoch 1367 :: Batch 0/1
Batch Loss = 2.669185691943878
1367, epoch_train_loss=2.669185691943878
Epoch 1368
Epoch 1368 :: Batch 0/1
Batch Loss = 2.669155859329542
1368, epoch_train_loss=2.669155859329542
Epoch 1369
Epoch 1369 :: Batch 0/1
Batch Loss = 2.669125975768117
1369, epoch_train_loss=2.669125975768117
Epoch 1370
Epoch 1370 :: Batch 0/1
Batch Loss = 2.669096040655324
1370, epoch_train_loss=2.669096040655324
Epoch 1371
Epoch 1371 :: Batch 0/1
Batch Loss = 2.6690660533794572
1371, epoch_train_loss=2.6690660533794572
Epoch 1372
Epoch 1372 :: Batch 0/1
Batch Loss = 2.6690360133372377
1372, epoch_train_loss=2.6690360133372377
Epoch 1373
Epoch 1373 :: Batch 0/1
Batch Loss = 2.6690059199214127
1373, epoch_train_loss=2.6690059199214127
Epoch 1374
Epoch 1374 :: Batch 0/1
Batch Loss = 2.6689757725225522
1374, epoch_train_loss=2.6689757725225522
Epoch 1375
Epoch 1375 :: Batch 0/1
Batch Loss = 2.6689455705451546
1375, epoch_train_loss=2.6689455705451546
Epoch 1376
Epoch 1376 :: Batch 0/1
Batch Loss = 2.6689153133927905
1376, epoch_train_loss=2.6689153133927905
Epoch 1377
Epoch 1377 :: Batch 0/1
Batch Loss = 2.6688850004792535
1377, epoch_train_loss=2.6688850004792535
Epoch 1378
Epoch 1378 :: Batch 0/1
Batch Loss = 2.6688546312275827
1378, epoch_train_loss=2.6688546312275827
Epoch 1379
Epoch 1379 :: Batch 0/1
Batch Loss = 2.668824205073509
1379, epoch_train_loss=2.668824205073509
Epoch 1380
Epoch 1380 :: Batch 0/1
Batch Loss = 2.668793721458818
1380, epoch_train_loss=2.668793721458818
Epoch 1381
Epoch 1381 :: Batch 0/1
Batch Loss = 2.6687631798384404
1381, epoch_train_loss=2.6687631798384404
Epoch 1382
Epoch 1382 :: Batch 0/1
Batch Loss = 2.6687325796771137
1382, epoch_train_loss=2.6687325796771137
Epoch 1383
Epoch 1383 :: Batch 0/1
Batch Loss = 2.6687019204473446
1383, epoch_train_loss=2.6687019204473446
Epoch 1384
Epoch 1384 :: Batch 0/1
Batch Loss = 2.668671201625998
1384, epoch_train_loss=2.668671201625998
Epoch 1385
Epoch 1385 :: Batch 0/1
Batch Loss = 2.6686404226892058
1385, epoch_train_loss=2.6686404226892058
Epoch 1386
Epoch 1386 :: Batch 0/1
Batch Loss = 2.6686095831052254
1386, epoch_train_loss=2.6686095831052254
Epoch 1387
Epoch 1387 :: Batch 0/1
Batch Loss = 2.66857868232735
1387, epoch_train_loss=2.66857868232735
Epoch 1388
Epoch 1388 :: Batch 0/1
Batch Loss = 2.668547719771922
1388, epoch_train_loss=2.668547719771922
Epoch 1389
Epoch 1389 :: Batch 0/1
Batch Loss = 2.6685166948178343
1389, epoch_train_loss=2.6685166948178343
Epoch 1390
Epoch 1390 :: Batch 0/1
Batch Loss = 2.668485606778738
1390, epoch_train_loss=2.668485606778738
Epoch 1391
Epoch 1391 :: Batch 0/1
Batch Loss = 2.6684544548847855
1391, epoch_train_loss=2.6684544548847855
Epoch 1392
Epoch 1392 :: Batch 0/1
Batch Loss = 2.668423238259902
1392, epoch_train_loss=2.668423238259902
Epoch 1393
Epoch 1393 :: Batch 0/1
Batch Loss = 2.668391955895567
1393, epoch_train_loss=2.668391955895567
Epoch 1394
Epoch 1394 :: Batch 0/1
Batch Loss = 2.668360606634837
1394, epoch_train_loss=2.668360606634837
Epoch 1395
Epoch 1395 :: Batch 0/1
Batch Loss = 2.668329189143645
1395, epoch_train_loss=2.668329189143645
Epoch 1396
Epoch 1396 :: Batch 0/1
Batch Loss = 2.6682977018958636
1396, epoch_train_loss=2.6682977018958636
Epoch 1397
Epoch 1397 :: Batch 0/1
Batch Loss = 2.6682661431615067
1397, epoch_train_loss=2.6682661431615067
Epoch 1398
Epoch 1398 :: Batch 0/1
Batch Loss = 2.6682345109993597
1398, epoch_train_loss=2.6682345109993597
Epoch 1399
Epoch 1399 :: Batch 0/1
Batch Loss = 2.6682028032671528
1399, epoch_train_loss=2.6682028032671528
Epoch 1400
Epoch 1400 :: Batch 0/1
Batch Loss = 2.6681710176241467
1400, epoch_train_loss=2.6681710176241467
Epoch 1401
Epoch 1401 :: Batch 0/1
Batch Loss = 2.668139151547421
1401, epoch_train_loss=2.668139151547421
Epoch 1402
Epoch 1402 :: Batch 0/1
Batch Loss = 2.6681072023601105
1402, epoch_train_loss=2.6681072023601105
Epoch 1403
Epoch 1403 :: Batch 0/1
Batch Loss = 2.668075167246455
1403, epoch_train_loss=2.668075167246455
Epoch 1404
Epoch 1404 :: Batch 0/1
Batch Loss = 2.668043043273578
1404, epoch_train_loss=2.668043043273578
Epoch 1405
Epoch 1405 :: Batch 0/1
Batch Loss = 2.668010827417527
1405, epoch_train_loss=2.668010827417527
Epoch 1406
Epoch 1406 :: Batch 0/1
Batch Loss = 2.6679785165659977
1406, epoch_train_loss=2.6679785165659977
Epoch 1407
Epoch 1407 :: Batch 0/1
Batch Loss = 2.6679461075341595
1407, epoch_train_loss=2.6679461075341595
Epoch 1408
Epoch 1408 :: Batch 0/1
Batch Loss = 2.667913597051803
1408, epoch_train_loss=2.667913597051803
Epoch 1409
Epoch 1409 :: Batch 0/1
Batch Loss = 2.6678809817619995
1409, epoch_train_loss=2.6678809817619995
Epoch 1410
Epoch 1410 :: Batch 0/1
Batch Loss = 2.6678482581925542
1410, epoch_train_loss=2.6678482581925542
Epoch 1411
Epoch 1411 :: Batch 0/1
Batch Loss = 2.6678154227254907
1411, epoch_train_loss=2.6678154227254907
Epoch 1412
Epoch 1412 :: Batch 0/1
Batch Loss = 2.667782471561568
1412, epoch_train_loss=2.667782471561568
Epoch 1413
Epoch 1413 :: Batch 0/1
Batch Loss = 2.667749400665923
1413, epoch_train_loss=2.667749400665923
Epoch 1414
Epoch 1414 :: Batch 0/1
Batch Loss = 2.66771620570366
1414, epoch_train_loss=2.66771620570366
Epoch 1415
Epoch 1415 :: Batch 0/1
Batch Loss = 2.667682881963428
1415, epoch_train_loss=2.667682881963428
Epoch 1416
Epoch 1416 :: Batch 0/1
Batch Loss = 2.6676494242535327
1416, epoch_train_loss=2.6676494242535327
Epoch 1417
Epoch 1417 :: Batch 0/1
Batch Loss = 2.6676158267976247
1417, epoch_train_loss=2.6676158267976247
Epoch 1418
Epoch 1418 :: Batch 0/1
Batch Loss = 2.6675820830903194
1418, epoch_train_loss=2.6675820830903194
Epoch 1419
Epoch 1419 :: Batch 0/1
Batch Loss = 2.6675481857569574
1419, epoch_train_loss=2.6675481857569574
Epoch 1420
Epoch 1420 :: Batch 0/1
Batch Loss = 2.667514126403027
1420, epoch_train_loss=2.667514126403027
Epoch 1421
Epoch 1421 :: Batch 0/1
Batch Loss = 2.667479895503766
1421, epoch_train_loss=2.667479895503766
Epoch 1422
Epoch 1422 :: Batch 0/1
Batch Loss = 2.6674454823890414
1422, epoch_train_loss=2.6674454823890414
Epoch 1423
Epoch 1423 :: Batch 0/1
Batch Loss = 2.6674108754851615
1423, epoch_train_loss=2.6674108754851615
Epoch 1424
Epoch 1424 :: Batch 0/1
Batch Loss = 2.667376063021279
1424, epoch_train_loss=2.667376063021279
Epoch 1425
Epoch 1425 :: Batch 0/1
Batch Loss = 2.6673410345843496
1425, epoch_train_loss=2.6673410345843496
Epoch 1426
Epoch 1426 :: Batch 0/1
Batch Loss = 2.6673057840213144
1426, epoch_train_loss=2.6673057840213144
Epoch 1427
Epoch 1427 :: Batch 0/1
Batch Loss = 2.667270314173466
1427, epoch_train_loss=2.667270314173466
Epoch 1428
Epoch 1428 :: Batch 0/1
Batch Loss = 2.6672346434390404
1428, epoch_train_loss=2.6672346434390404
Epoch 1429
Epoch 1429 :: Batch 0/1
Batch Loss = 2.667198812696571
1429, epoch_train_loss=2.667198812696571
Epoch 1430
Epoch 1430 :: Batch 0/1
Batch Loss = 2.6671628884136416
1430, epoch_train_loss=2.6671628884136416
Epoch 1431
Epoch 1431 :: Batch 0/1
Batch Loss = 2.6671269549478427
1431, epoch_train_loss=2.6671269549478427
Epoch 1432
Epoch 1432 :: Batch 0/1
Batch Loss = 2.6670910899412315
1432, epoch_train_loss=2.6670910899412315
Epoch 1433
Epoch 1433 :: Batch 0/1
Batch Loss = 2.6670553260674787
1433, epoch_train_loss=2.6670553260674787
Epoch 1434
Epoch 1434 :: Batch 0/1
Batch Loss = 2.6670196182592774
1434, epoch_train_loss=2.6670196182592774
Epoch 1435
Epoch 1435 :: Batch 0/1
Batch Loss = 2.666983843301389
1435, epoch_train_loss=2.666983843301389
Epoch 1436
Epoch 1436 :: Batch 0/1
Batch Loss = 2.666947843056553
1436, epoch_train_loss=2.666947843056553
Epoch 1437
Epoch 1437 :: Batch 0/1
Batch Loss = 2.6669114910750094
1437, epoch_train_loss=2.6669114910750094
Epoch 1438
Epoch 1438 :: Batch 0/1
Batch Loss = 2.6668747433789433
1438, epoch_train_loss=2.6668747433789433
Epoch 1439
Epoch 1439 :: Batch 0/1
Batch Loss = 2.6668376452446623
1439, epoch_train_loss=2.6668376452446623
Epoch 1440
Epoch 1440 :: Batch 0/1
Batch Loss = 2.6668002957895482
1440, epoch_train_loss=2.6668002957895482
Epoch 1441
Epoch 1441 :: Batch 0/1
Batch Loss = 2.66676279642163
1441, epoch_train_loss=2.66676279642163
Epoch 1442
Epoch 1442 :: Batch 0/1
Batch Loss = 2.6667252124050056
1442, epoch_train_loss=2.6667252124050056
Epoch 1443
Epoch 1443 :: Batch 0/1
Batch Loss = 2.6666875615655217
1443, epoch_train_loss=2.6666875615655217
Epoch 1444
Epoch 1444 :: Batch 0/1
Batch Loss = 2.666649825276677
1444, epoch_train_loss=2.666649825276677
Epoch 1445
Epoch 1445 :: Batch 0/1
Batch Loss = 2.666611967786821
1445, epoch_train_loss=2.666611967786821
Epoch 1446
Epoch 1446 :: Batch 0/1
Batch Loss = 2.666573952545089
1446, epoch_train_loss=2.666573952545089
Epoch 1447
Epoch 1447 :: Batch 0/1
Batch Loss = 2.666535751292093
1447, epoch_train_loss=2.666535751292093
Epoch 1448
Epoch 1448 :: Batch 0/1
Batch Loss = 2.666497346975082
1448, epoch_train_loss=2.666497346975082
Epoch 1449
Epoch 1449 :: Batch 0/1
Batch Loss = 2.6664587331721608
1449, epoch_train_loss=2.6664587331721608
Epoch 1450
Epoch 1450 :: Batch 0/1
Batch Loss = 2.6664199122664165
1450, epoch_train_loss=2.6664199122664165
Epoch 1451
Epoch 1451 :: Batch 0/1
Batch Loss = 2.666380893430526
1451, epoch_train_loss=2.666380893430526
Epoch 1452
Epoch 1452 :: Batch 0/1
Batch Loss = 2.6663416907373625
1452, epoch_train_loss=2.6663416907373625
Epoch 1453
Epoch 1453 :: Batch 0/1
Batch Loss = 2.6663023212172385
1453, epoch_train_loss=2.6663023212172385
Epoch 1454
Epoch 1454 :: Batch 0/1
Batch Loss = 2.6662628026585056
1454, epoch_train_loss=2.6662628026585056
Epoch 1455
Epoch 1455 :: Batch 0/1
Batch Loss = 2.6662231511656542
1455, epoch_train_loss=2.6662231511656542
Epoch 1456
Epoch 1456 :: Batch 0/1
Batch Loss = 2.6661833788289613
1456, epoch_train_loss=2.6661833788289613
Epoch 1457
Epoch 1457 :: Batch 0/1
Batch Loss = 2.6661434921526697
1457, epoch_train_loss=2.6661434921526697
Epoch 1458
Epoch 1458 :: Batch 0/1
Batch Loss = 2.6661034919492996
1458, epoch_train_loss=2.6661034919492996
Epoch 1459
Epoch 1459 :: Batch 0/1
Batch Loss = 2.666063375005136
1459, epoch_train_loss=2.666063375005136
Epoch 1460
Epoch 1460 :: Batch 0/1
Batch Loss = 2.6660231370989274
1460, epoch_train_loss=2.6660231370989274
Epoch 1461
Epoch 1461 :: Batch 0/1
Batch Loss = 2.6659827763214627
1461, epoch_train_loss=2.6659827763214627
Epoch 1462
Epoch 1462 :: Batch 0/1
Batch Loss = 2.6659422954307845
1462, epoch_train_loss=2.6659422954307845
Epoch 1463
Epoch 1463 :: Batch 0/1
Batch Loss = 2.6659017023896867
1463, epoch_train_loss=2.6659017023896867
Epoch 1464
Epoch 1464 :: Batch 0/1
Batch Loss = 2.6658610090802317
1464, epoch_train_loss=2.6658610090802317
Epoch 1465
Epoch 1465 :: Batch 0/1
Batch Loss = 2.665820228923814
1465, epoch_train_loss=2.665820228923814
Epoch 1466
Epoch 1466 :: Batch 0/1
Batch Loss = 2.6657793744192038
1466, epoch_train_loss=2.6657793744192038
Epoch 1467
Epoch 1467 :: Batch 0/1
Batch Loss = 2.6657384554503487
1467, epoch_train_loss=2.6657384554503487
Epoch 1468
Epoch 1468 :: Batch 0/1
Batch Loss = 2.665697478652888
1468, epoch_train_loss=2.665697478652888
Epoch 1469
Epoch 1469 :: Batch 0/1
Batch Loss = 2.66565644772617
1469, epoch_train_loss=2.66565644772617
Epoch 1470
Epoch 1470 :: Batch 0/1
Batch Loss = 2.6656153642352054
1470, epoch_train_loss=2.6656153642352054
Epoch 1471
Epoch 1471 :: Batch 0/1
Batch Loss = 2.665574228547794
1471, epoch_train_loss=2.665574228547794
Epoch 1472
Epoch 1472 :: Batch 0/1
Batch Loss = 2.6655330405637137
1472, epoch_train_loss=2.6655330405637137
Epoch 1473
Epoch 1473 :: Batch 0/1
Batch Loss = 2.66549180014475
1473, epoch_train_loss=2.66549180014475
Epoch 1474
Epoch 1474 :: Batch 0/1
Batch Loss = 2.66545050718699
1474, epoch_train_loss=2.66545050718699
Epoch 1475
Epoch 1475 :: Batch 0/1
Batch Loss = 2.6654091614630055
1475, epoch_train_loss=2.6654091614630055
Epoch 1476
Epoch 1476 :: Batch 0/1
Batch Loss = 2.665367762270725
1476, epoch_train_loss=2.665367762270725
Epoch 1477
Epoch 1477 :: Batch 0/1
Batch Loss = 2.665326308022414
1477, epoch_train_loss=2.665326308022414
Epoch 1478
Epoch 1478 :: Batch 0/1
Batch Loss = 2.665284795809478
1478, epoch_train_loss=2.665284795809478
Epoch 1479
Epoch 1479 :: Batch 0/1
Batch Loss = 2.6652432211085646
1479, epoch_train_loss=2.6652432211085646
Epoch 1480
Epoch 1480 :: Batch 0/1
Batch Loss = 2.6652015776126117
1480, epoch_train_loss=2.6652015776126117
Epoch 1481
Epoch 1481 :: Batch 0/1
Batch Loss = 2.665159857273232
1481, epoch_train_loss=2.665159857273232
Epoch 1482
Epoch 1482 :: Batch 0/1
Batch Loss = 2.665118050521755
1482, epoch_train_loss=2.665118050521755
Epoch 1483
Epoch 1483 :: Batch 0/1
Batch Loss = 2.6650761466346484
1483, epoch_train_loss=2.6650761466346484
Epoch 1484
Epoch 1484 :: Batch 0/1
Batch Loss = 2.665034134125686
1484, epoch_train_loss=2.665034134125686
Epoch 1485
Epoch 1485 :: Batch 0/1
Batch Loss = 2.664992001106024
1485, epoch_train_loss=2.664992001106024
Epoch 1486
Epoch 1486 :: Batch 0/1
Batch Loss = 2.6649497355482543
1486, epoch_train_loss=2.6649497355482543
Epoch 1487
Epoch 1487 :: Batch 0/1
Batch Loss = 2.6649073254036657
1487, epoch_train_loss=2.6649073254036657
Epoch 1488
Epoch 1488 :: Batch 0/1
Batch Loss = 2.664864758626508
1488, epoch_train_loss=2.664864758626508
Epoch 1489
Epoch 1489 :: Batch 0/1
Batch Loss = 2.664822023158168
1489, epoch_train_loss=2.664822023158168
Epoch 1490
Epoch 1490 :: Batch 0/1
Batch Loss = 2.6647791068985165
1490, epoch_train_loss=2.6647791068985165
Epoch 1491
Epoch 1491 :: Batch 0/1
Batch Loss = 2.6647359977359546
1491, epoch_train_loss=2.6647359977359546
Epoch 1492
Epoch 1492 :: Batch 0/1
Batch Loss = 2.6646926836508387
1492, epoch_train_loss=2.6646926836508387
Epoch 1493
Epoch 1493 :: Batch 0/1
Batch Loss = 2.664649152846876
1493, epoch_train_loss=2.664649152846876
Epoch 1494
Epoch 1494 :: Batch 0/1
Batch Loss = 2.664605393930796
1494, epoch_train_loss=2.664605393930796
Epoch 1495
Epoch 1495 :: Batch 0/1
Batch Loss = 2.6645613960739714
1495, epoch_train_loss=2.6645613960739714
Epoch 1496
Epoch 1496 :: Batch 0/1
Batch Loss = 2.6645171491283826
1496, epoch_train_loss=2.6645171491283826
Epoch 1497
Epoch 1497 :: Batch 0/1
Batch Loss = 2.664472643715074
1497, epoch_train_loss=2.664472643715074
Epoch 1498
Epoch 1498 :: Batch 0/1
Batch Loss = 2.664427871227764
1498, epoch_train_loss=2.664427871227764
Epoch 1499
Epoch 1499 :: Batch 0/1
Batch Loss = 2.6643828238094467
1499, epoch_train_loss=2.6643828238094467
Epoch 1500
Epoch 1500 :: Batch 0/1
Batch Loss = 2.6643374942792173
1500, epoch_train_loss=2.6643374942792173
Epoch 1501
Epoch 1501 :: Batch 0/1
Batch Loss = 2.6642918760244196
1501, epoch_train_loss=2.6642918760244196
Epoch 1502
Epoch 1502 :: Batch 0/1
Batch Loss = 2.6642459629093493
1502, epoch_train_loss=2.6642459629093493
Epoch 1503
Epoch 1503 :: Batch 0/1
Batch Loss = 2.664199749158721
1503, epoch_train_loss=2.664199749158721
Epoch 1504
Epoch 1504 :: Batch 0/1
Batch Loss = 2.6641532292772716
1504, epoch_train_loss=2.6641532292772716
Epoch 1505
Epoch 1505 :: Batch 0/1
Batch Loss = 2.664106397967426
1505, epoch_train_loss=2.664106397967426
Epoch 1506
Epoch 1506 :: Batch 0/1
Batch Loss = 2.664059250060202
1506, epoch_train_loss=2.664059250060202
Epoch 1507
Epoch 1507 :: Batch 0/1
Batch Loss = 2.6640117804540817
1507, epoch_train_loss=2.6640117804540817
Epoch 1508
Epoch 1508 :: Batch 0/1
Batch Loss = 2.663963984037599
1508, epoch_train_loss=2.663963984037599
Epoch 1509
Epoch 1509 :: Batch 0/1
Batch Loss = 2.6639158556299734
1509, epoch_train_loss=2.6639158556299734
Epoch 1510
Epoch 1510 :: Batch 0/1
Batch Loss = 2.663867389902569
1510, epoch_train_loss=2.663867389902569
Epoch 1511
Epoch 1511 :: Batch 0/1
Batch Loss = 2.6638185813034094
1511, epoch_train_loss=2.6638185813034094
Epoch 1512
Epoch 1512 :: Batch 0/1
Batch Loss = 2.663769423986698
1512, epoch_train_loss=2.663769423986698
Epoch 1513
Epoch 1513 :: Batch 0/1
Batch Loss = 2.66371991175721
1513, epoch_train_loss=2.66371991175721
Epoch 1514
Epoch 1514 :: Batch 0/1
Batch Loss = 2.6636700380105007
1514, epoch_train_loss=2.6636700380105007
Epoch 1515
Epoch 1515 :: Batch 0/1
Batch Loss = 2.6636197957338976
1515, epoch_train_loss=2.6636197957338976
Epoch 1516
Epoch 1516 :: Batch 0/1
Batch Loss = 2.6635691774651185
1516, epoch_train_loss=2.6635691774651185
Epoch 1517
Epoch 1517 :: Batch 0/1
Batch Loss = 2.6635181753243935
1517, epoch_train_loss=2.6635181753243935
Epoch 1518
Epoch 1518 :: Batch 0/1
Batch Loss = 2.663466781012831
1518, epoch_train_loss=2.663466781012831
Epoch 1519
Epoch 1519 :: Batch 0/1
Batch Loss = 2.6634149858365626
1519, epoch_train_loss=2.6634149858365626
Epoch 1520
Epoch 1520 :: Batch 0/1
Batch Loss = 2.663362780729278
1520, epoch_train_loss=2.663362780729278
Epoch 1521
Epoch 1521 :: Batch 0/1
Batch Loss = 2.6633101562747394
1521, epoch_train_loss=2.6633101562747394
Epoch 1522
Epoch 1522 :: Batch 0/1
Batch Loss = 2.6632571027238257
1522, epoch_train_loss=2.6632571027238257
Epoch 1523
Epoch 1523 :: Batch 0/1
Batch Loss = 2.663203610026859
1523, epoch_train_loss=2.663203610026859
Epoch 1524
Epoch 1524 :: Batch 0/1
Batch Loss = 2.6631496678394404
1524, epoch_train_loss=2.6631496678394404
Epoch 1525
Epoch 1525 :: Batch 0/1
Batch Loss = 2.66309526554396
1525, epoch_train_loss=2.66309526554396
Epoch 1526
Epoch 1526 :: Batch 0/1
Batch Loss = 2.663040392265899
1526, epoch_train_loss=2.663040392265899
Epoch 1527
Epoch 1527 :: Batch 0/1
Batch Loss = 2.662985036884255
1527, epoch_train_loss=2.662985036884255
Epoch 1528
Epoch 1528 :: Batch 0/1
Batch Loss = 2.6629291880572046
1528, epoch_train_loss=2.6629291880572046
Epoch 1529
Epoch 1529 :: Batch 0/1
Batch Loss = 2.662872834213623
1529, epoch_train_loss=2.662872834213623
Epoch 1530
Epoch 1530 :: Batch 0/1
Batch Loss = 2.6628159635754645
1530, epoch_train_loss=2.6628159635754645
Epoch 1531
Epoch 1531 :: Batch 0/1
Batch Loss = 2.6627585641438642
1531, epoch_train_loss=2.6627585641438642
Epoch 1532
Epoch 1532 :: Batch 0/1
Batch Loss = 2.662700623692965
1532, epoch_train_loss=2.662700623692965
Epoch 1533
Epoch 1533 :: Batch 0/1
Batch Loss = 2.6626421297726073
1533, epoch_train_loss=2.6626421297726073
Epoch 1534
Epoch 1534 :: Batch 0/1
Batch Loss = 2.6625830696746084
1534, epoch_train_loss=2.6625830696746084
Epoch 1535
Epoch 1535 :: Batch 0/1
Batch Loss = 2.6625234304329193
1535, epoch_train_loss=2.6625234304329193
Epoch 1536
Epoch 1536 :: Batch 0/1
Batch Loss = 2.6624631987893776
1536, epoch_train_loss=2.6624631987893776
Epoch 1537
Epoch 1537 :: Batch 0/1
Batch Loss = 2.6624023611724974
1537, epoch_train_loss=2.6624023611724974
Epoch 1538
Epoch 1538 :: Batch 0/1
Batch Loss = 2.6623409036911254
1538, epoch_train_loss=2.6623409036911254
Epoch 1539
Epoch 1539 :: Batch 0/1
Batch Loss = 2.6622788120951606
1539, epoch_train_loss=2.6622788120951606
Epoch 1540
Epoch 1540 :: Batch 0/1
Batch Loss = 2.66221607177138
1540, epoch_train_loss=2.66221607177138
Epoch 1541
Epoch 1541 :: Batch 0/1
Batch Loss = 2.662152667726428
1541, epoch_train_loss=2.662152667726428
Epoch 1542
Epoch 1542 :: Batch 0/1
Batch Loss = 2.662088584556117
1542, epoch_train_loss=2.662088584556117
Epoch 1543
Epoch 1543 :: Batch 0/1
Batch Loss = 2.6620238064464674
1543, epoch_train_loss=2.6620238064464674
Epoch 1544
Epoch 1544 :: Batch 0/1
Batch Loss = 2.6619583171617784
1544, epoch_train_loss=2.6619583171617784
Epoch 1545
Epoch 1545 :: Batch 0/1
Batch Loss = 2.661892100018695
1545, epoch_train_loss=2.661892100018695
Epoch 1546
Epoch 1546 :: Batch 0/1
Batch Loss = 2.661825137893123
1546, epoch_train_loss=2.661825137893123
Epoch 1547
Epoch 1547 :: Batch 0/1
Batch Loss = 2.661757413213625
1547, epoch_train_loss=2.661757413213625
Epoch 1548
Epoch 1548 :: Batch 0/1
Batch Loss = 2.6616889079399386
1548, epoch_train_loss=2.6616889079399386
Epoch 1549
Epoch 1549 :: Batch 0/1
Batch Loss = 2.661619603574708
1549, epoch_train_loss=2.661619603574708
Epoch 1550
Epoch 1550 :: Batch 0/1
Batch Loss = 2.6615494811545606
1550, epoch_train_loss=2.6615494811545606
Epoch 1551
Epoch 1551 :: Batch 0/1
Batch Loss = 2.6614785212522825
1551, epoch_train_loss=2.6614785212522825
Epoch 1552
Epoch 1552 :: Batch 0/1
Batch Loss = 2.6614067039503837
1552, epoch_train_loss=2.6614067039503837
Epoch 1553
Epoch 1553 :: Batch 0/1
Batch Loss = 2.661334008870243
1553, epoch_train_loss=2.661334008870243
Epoch 1554
Epoch 1554 :: Batch 0/1
Batch Loss = 2.6612604151496133
1554, epoch_train_loss=2.6612604151496133
Epoch 1555
Epoch 1555 :: Batch 0/1
Batch Loss = 2.661185901423548
1555, epoch_train_loss=2.661185901423548
Epoch 1556
Epoch 1556 :: Batch 0/1
Batch Loss = 2.6611104458271533
1556, epoch_train_loss=2.6611104458271533
Epoch 1557
Epoch 1557 :: Batch 0/1
Batch Loss = 2.6610340260039176
1557, epoch_train_loss=2.6610340260039176
Epoch 1558
Epoch 1558 :: Batch 0/1
Batch Loss = 2.6609566190586835
1558, epoch_train_loss=2.6609566190586835
Epoch 1559
Epoch 1559 :: Batch 0/1
Batch Loss = 2.660878201569689
1559, epoch_train_loss=2.660878201569689
Epoch 1560
Epoch 1560 :: Batch 0/1
Batch Loss = 2.6607987495637926
1560, epoch_train_loss=2.6607987495637926
Epoch 1561
Epoch 1561 :: Batch 0/1
Batch Loss = 2.6607182385226595
1561, epoch_train_loss=2.6607182385226595
Epoch 1562
Epoch 1562 :: Batch 0/1
Batch Loss = 2.660636643351372
1562, epoch_train_loss=2.660636643351372
Epoch 1563
Epoch 1563 :: Batch 0/1
Batch Loss = 2.6605539383789476
1563, epoch_train_loss=2.6605539383789476
Epoch 1564
Epoch 1564 :: Batch 0/1
Batch Loss = 2.6604700973202675
1564, epoch_train_loss=2.6604700973202675
Epoch 1565
Epoch 1565 :: Batch 0/1
Batch Loss = 2.660385093304547
1565, epoch_train_loss=2.660385093304547
Epoch 1566
Epoch 1566 :: Batch 0/1
Batch Loss = 2.6602988988389926
1566, epoch_train_loss=2.6602988988389926
Epoch 1567
Epoch 1567 :: Batch 0/1
Batch Loss = 2.660211485806243
1567, epoch_train_loss=2.660211485806243
Epoch 1568
Epoch 1568 :: Batch 0/1
Batch Loss = 2.660122825456245
1568, epoch_train_loss=2.660122825456245
Epoch 1569
Epoch 1569 :: Batch 0/1
Batch Loss = 2.660032888391917
1569, epoch_train_loss=2.660032888391917
Epoch 1570
Epoch 1570 :: Batch 0/1
Batch Loss = 2.659941644576969
1570, epoch_train_loss=2.659941644576969
Epoch 1571
Epoch 1571 :: Batch 0/1
Batch Loss = 2.659849063330865
1571, epoch_train_loss=2.659849063330865
Epoch 1572
Epoch 1572 :: Batch 0/1
Batch Loss = 2.659755113309469
1572, epoch_train_loss=2.659755113309469
Epoch 1573
Epoch 1573 :: Batch 0/1
Batch Loss = 2.6596597625078253
1573, epoch_train_loss=2.6596597625078253
Epoch 1574
Epoch 1574 :: Batch 0/1
Batch Loss = 2.659562978256197
1574, epoch_train_loss=2.659562978256197
Epoch 1575
Epoch 1575 :: Batch 0/1
Batch Loss = 2.6594647272085497
1575, epoch_train_loss=2.6594647272085497
Epoch 1576
Epoch 1576 :: Batch 0/1
Batch Loss = 2.6593649753545168
1576, epoch_train_loss=2.6593649753545168
Epoch 1577
Epoch 1577 :: Batch 0/1
Batch Loss = 2.659263688016734
1577, epoch_train_loss=2.659263688016734
Epoch 1578
Epoch 1578 :: Batch 0/1
Batch Loss = 2.6591608298238363
1578, epoch_train_loss=2.6591608298238363
Epoch 1579
Epoch 1579 :: Batch 0/1
Batch Loss = 2.659056364740093
1579, epoch_train_loss=2.659056364740093
Epoch 1580
Epoch 1580 :: Batch 0/1
Batch Loss = 2.65895025603897
1580, epoch_train_loss=2.65895025603897
Epoch 1581
Epoch 1581 :: Batch 0/1
Batch Loss = 2.6588424663009045
1581, epoch_train_loss=2.6588424663009045
Epoch 1582
Epoch 1582 :: Batch 0/1
Batch Loss = 2.6587329574379086
1582, epoch_train_loss=2.6587329574379086
Epoch 1583
Epoch 1583 :: Batch 0/1
Batch Loss = 2.6586216906590017
1583, epoch_train_loss=2.6586216906590017
Epoch 1584
Epoch 1584 :: Batch 0/1
Batch Loss = 2.6585086264966074
1584, epoch_train_loss=2.6585086264966074
Epoch 1585
Epoch 1585 :: Batch 0/1
Batch Loss = 2.6583937247995015
1585, epoch_train_loss=2.6583937247995015
Epoch 1586
Epoch 1586 :: Batch 0/1
Batch Loss = 2.658276944744627
1586, epoch_train_loss=2.658276944744627
Epoch 1587
Epoch 1587 :: Batch 0/1
Batch Loss = 2.658158244812846
1587, epoch_train_loss=2.658158244812846
Epoch 1588
Epoch 1588 :: Batch 0/1
Batch Loss = 2.6580375828210667
1588, epoch_train_loss=2.6580375828210667
Epoch 1589
Epoch 1589 :: Batch 0/1
Batch Loss = 2.657914915928632
1589, epoch_train_loss=2.657914915928632
Epoch 1590
Epoch 1590 :: Batch 0/1
Batch Loss = 2.657790200614987
1590, epoch_train_loss=2.657790200614987
Epoch 1591
Epoch 1591 :: Batch 0/1
Batch Loss = 2.6576633927165885
1591, epoch_train_loss=2.6576633927165885
Epoch 1592
Epoch 1592 :: Batch 0/1
Batch Loss = 2.6575344474265057
1592, epoch_train_loss=2.6575344474265057
Epoch 1593
Epoch 1593 :: Batch 0/1
Batch Loss = 2.657403319315292
1593, epoch_train_loss=2.657403319315292
Epoch 1594
Epoch 1594 :: Batch 0/1
Batch Loss = 2.6572699623114553
1594, epoch_train_loss=2.6572699623114553
Epoch 1595
Epoch 1595 :: Batch 0/1
Batch Loss = 2.6571343297453263
1595, epoch_train_loss=2.6571343297453263
Epoch 1596
Epoch 1596 :: Batch 0/1
Batch Loss = 2.6569963743531373
1596, epoch_train_loss=2.6569963743531373
Epoch 1597
Epoch 1597 :: Batch 0/1
Batch Loss = 2.6568560482927523
1597, epoch_train_loss=2.6568560482927523
Epoch 1598
Epoch 1598 :: Batch 0/1
Batch Loss = 2.6567133031608816
1598, epoch_train_loss=2.6567133031608816
Epoch 1599
Epoch 1599 :: Batch 0/1
Batch Loss = 2.6565680900118385
1599, epoch_train_loss=2.6565680900118385
Epoch 1600
Epoch 1600 :: Batch 0/1
Batch Loss = 2.6564203593779068
1600, epoch_train_loss=2.6564203593779068
Epoch 1601
Epoch 1601 :: Batch 0/1
Batch Loss = 2.656270061291364
1601, epoch_train_loss=2.656270061291364
Epoch 1602
Epoch 1602 :: Batch 0/1
Batch Loss = 2.656117145308189
1602, epoch_train_loss=2.656117145308189
Epoch 1603
Epoch 1603 :: Batch 0/1
Batch Loss = 2.655961560533485
1603, epoch_train_loss=2.655961560533485
Epoch 1604
Epoch 1604 :: Batch 0/1
Batch Loss = 2.6558032556486033
1604, epoch_train_loss=2.6558032556486033
Epoch 1605
Epoch 1605 :: Batch 0/1
Batch Loss = 2.655642178939948
1605, epoch_train_loss=2.655642178939948
Epoch 1606
Epoch 1606 :: Batch 0/1
Batch Loss = 2.6554782783294346
1606, epoch_train_loss=2.6554782783294346
Epoch 1607
Epoch 1607 :: Batch 0/1
Batch Loss = 2.6553115014065383
1607, epoch_train_loss=2.6553115014065383
Epoch 1608
Epoch 1608 :: Batch 0/1
Batch Loss = 2.6551417954483014
1608, epoch_train_loss=2.6551417954483014
Epoch 1609
Epoch 1609 :: Batch 0/1
Batch Loss = 2.6549691074947392
1609, epoch_train_loss=2.6549691074947392
Epoch 1610
Epoch 1610 :: Batch 0/1
Batch Loss = 2.6547933843452696
1610, epoch_train_loss=2.6547933843452696
Epoch 1611
Epoch 1611 :: Batch 0/1
Batch Loss = 2.654614572609716
1611, epoch_train_loss=2.654614572609716
Epoch 1612
Epoch 1612 :: Batch 0/1
Batch Loss = 2.654432618732793
1612, epoch_train_loss=2.654432618732793
Epoch 1613
Epoch 1613 :: Batch 0/1
Batch Loss = 2.6542474690776343
1613, epoch_train_loss=2.6542474690776343
Epoch 1614
Epoch 1614 :: Batch 0/1
Batch Loss = 2.6540590699092053
1614, epoch_train_loss=2.6540590699092053
Epoch 1615
Epoch 1615 :: Batch 0/1
Batch Loss = 2.6538673674958027
1615, epoch_train_loss=2.6538673674958027
Epoch 1616
Epoch 1616 :: Batch 0/1
Batch Loss = 2.65367230810814
1616, epoch_train_loss=2.65367230810814
Epoch 1617
Epoch 1617 :: Batch 0/1
Batch Loss = 2.65347383807853
1617, epoch_train_loss=2.65347383807853
Epoch 1618
Epoch 1618 :: Batch 0/1
Batch Loss = 2.653271903846271
1618, epoch_train_loss=2.653271903846271
Epoch 1619
Epoch 1619 :: Batch 0/1
Batch Loss = 2.6530664519874065
1619, epoch_train_loss=2.6530664519874065
Epoch 1620
Epoch 1620 :: Batch 0/1
Batch Loss = 2.6528574292938054
1620, epoch_train_loss=2.6528574292938054
Epoch 1621
Epoch 1621 :: Batch 0/1
Batch Loss = 2.652644782821966
1621, epoch_train_loss=2.652644782821966
Epoch 1622
Epoch 1622 :: Batch 0/1
Batch Loss = 2.6524284598904413
1622, epoch_train_loss=2.6524284598904413
Epoch 1623
Epoch 1623 :: Batch 0/1
Batch Loss = 2.652208408196042
1623, epoch_train_loss=2.652208408196042
Epoch 1624
Epoch 1624 :: Batch 0/1
Batch Loss = 2.651984575793492
1624, epoch_train_loss=2.651984575793492
Epoch 1625
Epoch 1625 :: Batch 0/1
Batch Loss = 2.6517569112143042
1625, epoch_train_loss=2.6517569112143042
Epoch 1626
Epoch 1626 :: Batch 0/1
Batch Loss = 2.651525363444041
1626, epoch_train_loss=2.651525363444041
Epoch 1627
Epoch 1627 :: Batch 0/1
Batch Loss = 2.6512898820432915
1627, epoch_train_loss=2.6512898820432915
Epoch 1628
Epoch 1628 :: Batch 0/1
Batch Loss = 2.651050417121801
1628, epoch_train_loss=2.651050417121801
Epoch 1629
Epoch 1629 :: Batch 0/1
Batch Loss = 2.6508069194414334
1629, epoch_train_loss=2.6508069194414334
Epoch 1630
Epoch 1630 :: Batch 0/1
Batch Loss = 2.650559340444383
1630, epoch_train_loss=2.650559340444383
Epoch 1631
Epoch 1631 :: Batch 0/1
Batch Loss = 2.6503076323189423
1631, epoch_train_loss=2.6503076323189423
Epoch 1632
Epoch 1632 :: Batch 0/1
Batch Loss = 2.6500517479840378
1632, epoch_train_loss=2.6500517479840378
Epoch 1633
Epoch 1633 :: Batch 0/1
Batch Loss = 2.649791641191808
1633, epoch_train_loss=2.649791641191808
Epoch 1634
Epoch 1634 :: Batch 0/1
Batch Loss = 2.649527266549529
1634, epoch_train_loss=2.649527266549529
Epoch 1635
Epoch 1635 :: Batch 0/1
Batch Loss = 2.6492585795809718
1635, epoch_train_loss=2.6492585795809718
Epoch 1636
Epoch 1636 :: Batch 0/1
Batch Loss = 2.6489855367003226
1636, epoch_train_loss=2.6489855367003226
Epoch 1637
Epoch 1637 :: Batch 0/1
Batch Loss = 2.648708095311467
1637, epoch_train_loss=2.648708095311467
Epoch 1638
Epoch 1638 :: Batch 0/1
Batch Loss = 2.648426213820472
1638, epoch_train_loss=2.648426213820472
Epoch 1639
Epoch 1639 :: Batch 0/1
Batch Loss = 2.648139851666579
1639, epoch_train_loss=2.648139851666579
Epoch 1640
Epoch 1640 :: Batch 0/1
Batch Loss = 2.6478489693505214
1640, epoch_train_loss=2.6478489693505214
Epoch 1641
Epoch 1641 :: Batch 0/1
Batch Loss = 2.6475535284836837
1641, epoch_train_loss=2.6475535284836837
Epoch 1642
Epoch 1642 :: Batch 0/1
Batch Loss = 2.6472534917402033
1642, epoch_train_loss=2.6472534917402033
Epoch 1643
Epoch 1643 :: Batch 0/1
Batch Loss = 2.646948822921855
1643, epoch_train_loss=2.646948822921855
Epoch 1644
Epoch 1644 :: Batch 0/1
Batch Loss = 2.646639487023721
1644, epoch_train_loss=2.646639487023721
Epoch 1645
Epoch 1645 :: Batch 0/1
Batch Loss = 2.6463254502000924
1645, epoch_train_loss=2.6463254502000924
Epoch 1646
Epoch 1646 :: Batch 0/1
Batch Loss = 2.6460066797228676
1646, epoch_train_loss=2.6460066797228676
Epoch 1647
Epoch 1647 :: Batch 0/1
Batch Loss = 2.6456831440358615
1647, epoch_train_loss=2.6456831440358615
Epoch 1648
Epoch 1648 :: Batch 0/1
Batch Loss = 2.645354812809708
1648, epoch_train_loss=2.645354812809708
Epoch 1649
Epoch 1649 :: Batch 0/1
Batch Loss = 2.645021656864044
1649, epoch_train_loss=2.645021656864044
Epoch 1650
Epoch 1650 :: Batch 0/1
Batch Loss = 2.6446836481879985
1650, epoch_train_loss=2.6446836481879985
Epoch 1651
Epoch 1651 :: Batch 0/1
Batch Loss = 2.644340759904245
1651, epoch_train_loss=2.644340759904245
Epoch 1652
Epoch 1652 :: Batch 0/1
Batch Loss = 2.6439929663665107
1652, epoch_train_loss=2.6439929663665107
Epoch 1653
Epoch 1653 :: Batch 0/1
Batch Loss = 2.643640242951452
1653, epoch_train_loss=2.643640242951452
Epoch 1654
Epoch 1654 :: Batch 0/1
Batch Loss = 2.6432825662327533
1654, epoch_train_loss=2.6432825662327533
Epoch 1655
Epoch 1655 :: Batch 0/1
Batch Loss = 2.6429199138474853
1655, epoch_train_loss=2.6429199138474853
Epoch 1656
Epoch 1656 :: Batch 0/1
Batch Loss = 2.642552264497509
1656, epoch_train_loss=2.642552264497509
Epoch 1657
Epoch 1657 :: Batch 0/1
Batch Loss = 2.6421795978900127
1657, epoch_train_loss=2.6421795978900127
Epoch 1658
Epoch 1658 :: Batch 0/1
Batch Loss = 2.641801894793567
1658, epoch_train_loss=2.641801894793567
Epoch 1659
Epoch 1659 :: Batch 0/1
Batch Loss = 2.641419136913795
1659, epoch_train_loss=2.641419136913795
Epoch 1660
Epoch 1660 :: Batch 0/1
Batch Loss = 2.6410313068525872
1660, epoch_train_loss=2.6410313068525872
Epoch 1661
Epoch 1661 :: Batch 0/1
Batch Loss = 2.6406383881588535
1661, epoch_train_loss=2.6406383881588535
Epoch 1662
Epoch 1662 :: Batch 0/1
Batch Loss = 2.6402403651602078
1662, epoch_train_loss=2.6402403651602078
Epoch 1663
Epoch 1663 :: Batch 0/1
Batch Loss = 2.6398372230414866
1663, epoch_train_loss=2.6398372230414866
Epoch 1664
Epoch 1664 :: Batch 0/1
Batch Loss = 2.6394289477010875
1664, epoch_train_loss=2.6394289477010875
Epoch 1665
Epoch 1665 :: Batch 0/1
Batch Loss = 2.6390155256977867
1665, epoch_train_loss=2.6390155256977867
Epoch 1666
Epoch 1666 :: Batch 0/1
Batch Loss = 2.6385969442626735
1666, epoch_train_loss=2.6385969442626735
Epoch 1667
Epoch 1667 :: Batch 0/1
Batch Loss = 2.6381731912460697
1667, epoch_train_loss=2.6381731912460697
Epoch 1668
Epoch 1668 :: Batch 0/1
Batch Loss = 2.6377442549599297
1668, epoch_train_loss=2.6377442549599297
Epoch 1669
Epoch 1669 :: Batch 0/1
Batch Loss = 2.6373101242555417
1669, epoch_train_loss=2.6373101242555417
Epoch 1670
Epoch 1670 :: Batch 0/1
Batch Loss = 2.636870788327813
1670, epoch_train_loss=2.636870788327813
Epoch 1671
Epoch 1671 :: Batch 0/1
Batch Loss = 2.6364262367588336
1671, epoch_train_loss=2.6364262367588336
Epoch 1672
Epoch 1672 :: Batch 0/1
Batch Loss = 2.635976459459643
1672, epoch_train_loss=2.635976459459643
Epoch 1673
Epoch 1673 :: Batch 0/1
Batch Loss = 2.6355214465022905
1673, epoch_train_loss=2.6355214465022905
Epoch 1674
Epoch 1674 :: Batch 0/1
Batch Loss = 2.6350611882028714
1674, epoch_train_loss=2.6350611882028714
Epoch 1675
Epoch 1675 :: Batch 0/1
Batch Loss = 2.634595674915502
1675, epoch_train_loss=2.634595674915502
Epoch 1676
Epoch 1676 :: Batch 0/1
Batch Loss = 2.634124897081226
1676, epoch_train_loss=2.634124897081226
Epoch 1677
Epoch 1677 :: Batch 0/1
Batch Loss = 2.633648845131713
1677, epoch_train_loss=2.633648845131713
Epoch 1678
Epoch 1678 :: Batch 0/1
Batch Loss = 2.6331675094297546
1678, epoch_train_loss=2.6331675094297546
Epoch 1679
Epoch 1679 :: Batch 0/1
Batch Loss = 2.632680880249455
1679, epoch_train_loss=2.632680880249455
Epoch 1680
Epoch 1680 :: Batch 0/1
Batch Loss = 2.6321889475625464
1680, epoch_train_loss=2.6321889475625464
Epoch 1681
Epoch 1681 :: Batch 0/1
Batch Loss = 2.6316917012545225
1681, epoch_train_loss=2.6316917012545225
Epoch 1682
Epoch 1682 :: Batch 0/1
Batch Loss = 2.631189130794563
1682, epoch_train_loss=2.631189130794563
Epoch 1683
Epoch 1683 :: Batch 0/1
Batch Loss = 2.630681225337836
1683, epoch_train_loss=2.630681225337836
Epoch 1684
Epoch 1684 :: Batch 0/1
Batch Loss = 2.630167973632832
1684, epoch_train_loss=2.630167973632832
Epoch 1685
Epoch 1685 :: Batch 0/1
Batch Loss = 2.629649363969229
1685, epoch_train_loss=2.629649363969229
Epoch 1686
Epoch 1686 :: Batch 0/1
Batch Loss = 2.629125384127104
1686, epoch_train_loss=2.629125384127104
Epoch 1687
Epoch 1687 :: Batch 0/1
Batch Loss = 2.6285960213275263
1687, epoch_train_loss=2.6285960213275263
Epoch 1688
Epoch 1688 :: Batch 0/1
Batch Loss = 2.6280612621845183
1688, epoch_train_loss=2.6280612621845183
Epoch 1689
Epoch 1689 :: Batch 0/1
Batch Loss = 2.627521092658396
1689, epoch_train_loss=2.627521092658396
Epoch 1690
Epoch 1690 :: Batch 0/1
Batch Loss = 2.6269754980104247
1690, epoch_train_loss=2.6269754980104247
Epoch 1691
Epoch 1691 :: Batch 0/1
Batch Loss = 2.6264244627587745
1691, epoch_train_loss=2.6264244627587745
Epoch 1692
Epoch 1692 :: Batch 0/1
Batch Loss = 2.625867970591075
1692, epoch_train_loss=2.625867970591075
Epoch 1693
Epoch 1693 :: Batch 0/1
Batch Loss = 2.625306004500878
1693, epoch_train_loss=2.625306004500878
Epoch 1694
Epoch 1694 :: Batch 0/1
Batch Loss = 2.624738546434584
1694, epoch_train_loss=2.624738546434584
Epoch 1695
Epoch 1695 :: Batch 0/1
Batch Loss = 2.624165577565175
1695, epoch_train_loss=2.624165577565175
Epoch 1696
Epoch 1696 :: Batch 0/1
Batch Loss = 2.623587078073759
1696, epoch_train_loss=2.623587078073759
Epoch 1697
Epoch 1697 :: Batch 0/1
Batch Loss = 2.62300302715609
1697, epoch_train_loss=2.62300302715609
Epoch 1698
Epoch 1698 :: Batch 0/1
Batch Loss = 2.6224134029847974
1698, epoch_train_loss=2.6224134029847974
Epoch 1699
Epoch 1699 :: Batch 0/1
Batch Loss = 2.6218181826242932
1699, epoch_train_loss=2.6218181826242932
Epoch 1700
Epoch 1700 :: Batch 0/1
Batch Loss = 2.6212173421360996
1700, epoch_train_loss=2.6212173421360996
Epoch 1701
Epoch 1701 :: Batch 0/1
Batch Loss = 2.620610856351445
1701, epoch_train_loss=2.620610856351445
Epoch 1702
Epoch 1702 :: Batch 0/1
Batch Loss = 2.619998699026426
1702, epoch_train_loss=2.619998699026426
Epoch 1703
Epoch 1703 :: Batch 0/1
Batch Loss = 2.6193808426603216
1703, epoch_train_loss=2.6193808426603216
Epoch 1704
Epoch 1704 :: Batch 0/1
Batch Loss = 2.61875725845603
1704, epoch_train_loss=2.61875725845603
Epoch 1705
Epoch 1705 :: Batch 0/1
Batch Loss = 2.6181279164314004
1705, epoch_train_loss=2.6181279164314004
Epoch 1706
Epoch 1706 :: Batch 0/1
Batch Loss = 2.617492785181223
1706, epoch_train_loss=2.617492785181223
Epoch 1707
Epoch 1707 :: Batch 0/1
Batch Loss = 2.616851832039391
1707, epoch_train_loss=2.616851832039391
Epoch 1708
Epoch 1708 :: Batch 0/1
Batch Loss = 2.6162050228874913
1708, epoch_train_loss=2.6162050228874913
Epoch 1709
Epoch 1709 :: Batch 0/1
Batch Loss = 2.6155523221115926
1709, epoch_train_loss=2.6155523221115926
Epoch 1710
Epoch 1710 :: Batch 0/1
Batch Loss = 2.614893692716214
1710, epoch_train_loss=2.614893692716214
Epoch 1711
Epoch 1711 :: Batch 0/1
Batch Loss = 2.6142290960716394
1711, epoch_train_loss=2.6142290960716394
Epoch 1712
Epoch 1712 :: Batch 0/1
Batch Loss = 2.61355849202534
1712, epoch_train_loss=2.61355849202534
Epoch 1713
Epoch 1713 :: Batch 0/1
Batch Loss = 2.612881838856462
1713, epoch_train_loss=2.612881838856462
Epoch 1714
Epoch 1714 :: Batch 0/1
Batch Loss = 2.612199093063564
1714, epoch_train_loss=2.612199093063564
Epoch 1715
Epoch 1715 :: Batch 0/1
Batch Loss = 2.6115102094726823
1715, epoch_train_loss=2.6115102094726823
Epoch 1716
Epoch 1716 :: Batch 0/1
Batch Loss = 2.610815141182706
1716, epoch_train_loss=2.610815141182706
Epoch 1717
Epoch 1717 :: Batch 0/1
Batch Loss = 2.610113839337738
1717, epoch_train_loss=2.610113839337738
Epoch 1718
Epoch 1718 :: Batch 0/1
Batch Loss = 2.609406253226664
1718, epoch_train_loss=2.609406253226664
Epoch 1719
Epoch 1719 :: Batch 0/1
Batch Loss = 2.6086923302134326
1719, epoch_train_loss=2.6086923302134326
Epoch 1720
Epoch 1720 :: Batch 0/1
Batch Loss = 2.6079720154873103
1720, epoch_train_loss=2.6079720154873103
Epoch 1721
Epoch 1721 :: Batch 0/1
Batch Loss = 2.607245252146788
1721, epoch_train_loss=2.607245252146788
Epoch 1722
Epoch 1722 :: Batch 0/1
Batch Loss = 2.606511981048143
1722, epoch_train_loss=2.606511981048143
Epoch 1723
Epoch 1723 :: Batch 0/1
Batch Loss = 2.6057721407603305
1723, epoch_train_loss=2.6057721407603305
Epoch 1724
Epoch 1724 :: Batch 0/1
Batch Loss = 2.6050256672128596
1724, epoch_train_loss=2.6050256672128596
Epoch 1725
Epoch 1725 :: Batch 0/1
Batch Loss = 2.6042724939852557
1725, epoch_train_loss=2.6042724939852557
Epoch 1726
Epoch 1726 :: Batch 0/1
Batch Loss = 2.603512551751496
1726, epoch_train_loss=2.603512551751496
Epoch 1727
Epoch 1727 :: Batch 0/1
Batch Loss = 2.602745768367321
1727, epoch_train_loss=2.602745768367321
Epoch 1728
Epoch 1728 :: Batch 0/1
Batch Loss = 2.6019720686455377
1728, epoch_train_loss=2.6019720686455377
Epoch 1729
Epoch 1729 :: Batch 0/1
Batch Loss = 2.6011913741766532
1729, epoch_train_loss=2.6011913741766532
Epoch 1730
Epoch 1730 :: Batch 0/1
Batch Loss = 2.6004036031346374
1730, epoch_train_loss=2.6004036031346374
Epoch 1731
Epoch 1731 :: Batch 0/1
Batch Loss = 2.599608670067259
1731, epoch_train_loss=2.599608670067259
Epoch 1732
Epoch 1732 :: Batch 0/1
Batch Loss = 2.5988064856704147
1732, epoch_train_loss=2.5988064856704147
Epoch 1733
Epoch 1733 :: Batch 0/1
Batch Loss = 2.597996956545857
1733, epoch_train_loss=2.597996956545857
Epoch 1734
Epoch 1734 :: Batch 0/1
Batch Loss = 2.597179984941615
1734, epoch_train_loss=2.597179984941615
Epoch 1735
Epoch 1735 :: Batch 0/1
Batch Loss = 2.596355468474367
1735, epoch_train_loss=2.596355468474367
Epoch 1736
Epoch 1736 :: Batch 0/1
Batch Loss = 2.5955232997660733
1736, epoch_train_loss=2.5955232997660733
Epoch 1737
Epoch 1737 :: Batch 0/1
Batch Loss = 2.5946833663270663
1737, epoch_train_loss=2.5946833663270663
Epoch 1738
Epoch 1738 :: Batch 0/1
Batch Loss = 2.593835550020216
1738, epoch_train_loss=2.593835550020216
Epoch 1739
Epoch 1739 :: Batch 0/1
Batch Loss = 2.5929797267674743
1739, epoch_train_loss=2.5929797267674743
Epoch 1740
Epoch 1740 :: Batch 0/1
Batch Loss = 2.59211576616739
1740, epoch_train_loss=2.59211576616739
Epoch 1741
Epoch 1741 :: Batch 0/1
Batch Loss = 2.591243531018294
1741, epoch_train_loss=2.591243531018294
Epoch 1742
Epoch 1742 :: Batch 0/1
Batch Loss = 2.590362877095405
1742, epoch_train_loss=2.590362877095405
Epoch 1743
Epoch 1743 :: Batch 0/1
Batch Loss = 2.589473652484576
1743, epoch_train_loss=2.589473652484576
Epoch 1744
Epoch 1744 :: Batch 0/1
Batch Loss = 2.5885756971663056
1744, epoch_train_loss=2.5885756971663056
Epoch 1745
Epoch 1745 :: Batch 0/1
Batch Loss = 2.587668842430573
1745, epoch_train_loss=2.587668842430573
Epoch 1746
Epoch 1746 :: Batch 0/1
Batch Loss = 2.5867529105546985
1746, epoch_train_loss=2.5867529105546985
Epoch 1747
Epoch 1747 :: Batch 0/1
Batch Loss = 2.585827713946271
1747, epoch_train_loss=2.585827713946271
Epoch 1748
Epoch 1748 :: Batch 0/1
Batch Loss = 2.584893054844676
1748, epoch_train_loss=2.584893054844676
Epoch 1749
Epoch 1749 :: Batch 0/1
Batch Loss = 2.5839487244087405
1749, epoch_train_loss=2.5839487244087405
Epoch 1750
Epoch 1750 :: Batch 0/1
Batch Loss = 2.5829945023801044
1750, epoch_train_loss=2.5829945023801044
Epoch 1751
Epoch 1751 :: Batch 0/1
Batch Loss = 2.5820301562059806
1751, epoch_train_loss=2.5820301562059806
Epoch 1752
Epoch 1752 :: Batch 0/1
Batch Loss = 2.581055440374463
1752, epoch_train_loss=2.581055440374463
Epoch 1753
Epoch 1753 :: Batch 0/1
Batch Loss = 2.5800700959875056
1753, epoch_train_loss=2.5800700959875056
Epoch 1754
Epoch 1754 :: Batch 0/1
Batch Loss = 2.5790738498003973
1754, epoch_train_loss=2.5790738498003973
Epoch 1755
Epoch 1755 :: Batch 0/1
Batch Loss = 2.5780664138378255
1755, epoch_train_loss=2.5780664138378255
Epoch 1756
Epoch 1756 :: Batch 0/1
Batch Loss = 2.5770474847473146
1756, epoch_train_loss=2.5770474847473146
Epoch 1757
Epoch 1757 :: Batch 0/1
Batch Loss = 2.576016743390299
1757, epoch_train_loss=2.576016743390299
Epoch 1758
Epoch 1758 :: Batch 0/1
Batch Loss = 2.5749738542130456
1758, epoch_train_loss=2.5749738542130456
Epoch 1759
Epoch 1759 :: Batch 0/1
Batch Loss = 2.5739184653456197
1759, epoch_train_loss=2.5739184653456197
Epoch 1760
Epoch 1760 :: Batch 0/1
Batch Loss = 2.572850208234859
1760, epoch_train_loss=2.572850208234859
Epoch 1761
Epoch 1761 :: Batch 0/1
Batch Loss = 2.571768698207121
1761, epoch_train_loss=2.571768698207121
Epoch 1762
Epoch 1762 :: Batch 0/1
Batch Loss = 2.570673534673948
1762, epoch_train_loss=2.570673534673948
Epoch 1763
Epoch 1763 :: Batch 0/1
Batch Loss = 2.569564302338869
1763, epoch_train_loss=2.569564302338869
Epoch 1764
Epoch 1764 :: Batch 0/1
Batch Loss = 2.5684405725476878
1764, epoch_train_loss=2.5684405725476878
Epoch 1765
Epoch 1765 :: Batch 0/1
Batch Loss = 2.567301905394989
1765, epoch_train_loss=2.567301905394989
Epoch 1766
Epoch 1766 :: Batch 0/1
Batch Loss = 2.5661478521416363
1766, epoch_train_loss=2.5661478521416363
Epoch 1767
Epoch 1767 :: Batch 0/1
Batch Loss = 2.5649779589468964
1767, epoch_train_loss=2.5649779589468964
Epoch 1768
Epoch 1768 :: Batch 0/1
Batch Loss = 2.563791771096534
1768, epoch_train_loss=2.563791771096534
Epoch 1769
Epoch 1769 :: Batch 0/1
Batch Loss = 2.5625888382442703
1769, epoch_train_loss=2.5625888382442703
Epoch 1770
Epoch 1770 :: Batch 0/1
Batch Loss = 2.5613687206175144
1770, epoch_train_loss=2.5613687206175144
Epoch 1771
Epoch 1771 :: Batch 0/1
Batch Loss = 2.5601309961743315
1771, epoch_train_loss=2.5601309961743315
Epoch 1772
Epoch 1772 :: Batch 0/1
Batch Loss = 2.5588752686978635
1772, epoch_train_loss=2.5588752686978635
Epoch 1773
Epoch 1773 :: Batch 0/1
Batch Loss = 2.5576011766050373
1773, epoch_train_loss=2.5576011766050373
Epoch 1774
Epoch 1774 :: Batch 0/1
Batch Loss = 2.5563084029738303
1774, epoch_train_loss=2.5563084029738303
Epoch 1775
Epoch 1775 :: Batch 0/1
Batch Loss = 2.5549966856903956
1775, epoch_train_loss=2.5549966856903956
Epoch 1776
Epoch 1776 :: Batch 0/1
Batch Loss = 2.553665861561439
1776, epoch_train_loss=2.553665861561439
Epoch 1777
Epoch 1777 :: Batch 0/1
Batch Loss = 2.552316429201642
1777, epoch_train_loss=2.552316429201642
Epoch 1778
Epoch 1778 :: Batch 0/1
Batch Loss = 2.5509634908812067
1778, epoch_train_loss=2.5509634908812067
Epoch 1779
Epoch 1779 :: Batch 0/1
Batch Loss = 2.549978421957279
1779, epoch_train_loss=2.549978421957279
Epoch 1780
Epoch 1780 :: Batch 0/1
Batch Loss = 2.560656256048941
1780, epoch_train_loss=2.560656256048941
Epoch 1781
Epoch 1781 :: Batch 0/1
Batch Loss = 2.822234850883922
1781, epoch_train_loss=2.822234850883922
Epoch 1782
Epoch 1782 :: Batch 0/1
Batch Loss = 2.8777565402281358
1782, epoch_train_loss=2.8777565402281358
Epoch 1783
Epoch 1783 :: Batch 0/1
Batch Loss = 4.6127605570639645
1783, epoch_train_loss=4.6127605570639645
Epoch 1784
Epoch 1784 :: Batch 0/1
Batch Loss = 4.0789291711107705
1784, epoch_train_loss=4.0789291711107705
Epoch 1785
Epoch 1785 :: Batch 0/1
Batch Loss = 4.12289485909271
1785, epoch_train_loss=4.12289485909271
Epoch 1786
Epoch 1786 :: Batch 0/1
Batch Loss = 2.8590266629365564
1786, epoch_train_loss=2.8590266629365564
Epoch 1787
Epoch 1787 :: Batch 0/1
Batch Loss = 6.99274453781888
1787, epoch_train_loss=6.99274453781888
Epoch 1788
Epoch 1788 :: Batch 0/1
Batch Loss = 4.23979716861063
1788, epoch_train_loss=4.23979716861063
Epoch 1789
Epoch 1789 :: Batch 0/1
Batch Loss = 4.812617482919488
1789, epoch_train_loss=4.812617482919488
Epoch 1790
Epoch 1790 :: Batch 0/1
Batch Loss = 4.807656104002542
1790, epoch_train_loss=4.807656104002542
Epoch 1791
Epoch 1791 :: Batch 0/1
Batch Loss = 4.781190499159623
1791, epoch_train_loss=4.781190499159623
Epoch 1792
Epoch 1792 :: Batch 0/1
Batch Loss = 4.740212683823309
1792, epoch_train_loss=4.740212683823309
Epoch 1793
Epoch 1793 :: Batch 0/1
Batch Loss = 7.046892442344103
1793, epoch_train_loss=7.046892442344103
Epoch 1794
Epoch 1794 :: Batch 0/1
Batch Loss = 4.64477805048296
1794, epoch_train_loss=4.64477805048296
Epoch 1795
Epoch 1795 :: Batch 0/1
Batch Loss = 4.608975113483259
1795, epoch_train_loss=4.608975113483259
Epoch 1796
Epoch 1796 :: Batch 0/1
Batch Loss = 4.588989437015688
1796, epoch_train_loss=4.588989437015688
Epoch 1797
Epoch 1797 :: Batch 0/1
Batch Loss = 4.584182039021836
1797, epoch_train_loss=4.584182039021836
Epoch 1798
Epoch 1798 :: Batch 0/1
Batch Loss = 4.591357129127367
1798, epoch_train_loss=4.591357129127367
Epoch 1799
Epoch 1799 :: Batch 0/1
Batch Loss = 4.605776657767887
1799, epoch_train_loss=4.605776657767887
Epoch 1800
Epoch 1800 :: Batch 0/1
Batch Loss = 4.622319050026118
1800, epoch_train_loss=4.622319050026118
Epoch 1801
Epoch 1801 :: Batch 0/1
Batch Loss = 4.636532778307258
1801, epoch_train_loss=4.636532778307258
Epoch 1802
Epoch 1802 :: Batch 0/1
Batch Loss = 4.645370376024802
1802, epoch_train_loss=4.645370376024802
Epoch 1803
Epoch 1803 :: Batch 0/1
Batch Loss = 4.647494681456712
1803, epoch_train_loss=4.647494681456712
Epoch 1804
Epoch 1804 :: Batch 0/1
Batch Loss = 4.643177400235116
1804, epoch_train_loss=4.643177400235116
Epoch 1805
Epoch 1805 :: Batch 0/1
Batch Loss = 4.633904830942356
1805, epoch_train_loss=4.633904830942356
Epoch 1806
Epoch 1806 :: Batch 0/1
Batch Loss = 4.6218413322111696
1806, epoch_train_loss=4.6218413322111696
Epoch 1807
Epoch 1807 :: Batch 0/1
Batch Loss = 4.609287289226398
1807, epoch_train_loss=4.609287289226398
Epoch 1808
Epoch 1808 :: Batch 0/1
Batch Loss = 4.598229756006463
1808, epoch_train_loss=4.598229756006463
Epoch 1809
Epoch 1809 :: Batch 0/1
Batch Loss = 4.590041400173607
1809, epoch_train_loss=4.590041400173607
Epoch 1810
Epoch 1810 :: Batch 0/1
Batch Loss = 4.5853467795278675
1810, epoch_train_loss=4.5853467795278675
Epoch 1811
Epoch 1811 :: Batch 0/1
Batch Loss = 4.5840460570095996
1811, epoch_train_loss=4.5840460570095996
Epoch 1812
Epoch 1812 :: Batch 0/1
Batch Loss = 4.585464118749294
1812, epoch_train_loss=4.585464118749294
Epoch 1813
Epoch 1813 :: Batch 0/1
Batch Loss = 4.588577747142645
1813, epoch_train_loss=4.588577747142645
Epoch 1814
Epoch 1814 :: Batch 0/1
Batch Loss = 4.5922666718123155
1814, epoch_train_loss=4.5922666718123155
Epoch 1815
Epoch 1815 :: Batch 0/1
Batch Loss = 4.595537619093739
1815, epoch_train_loss=4.595537619093739
Epoch 1816
Epoch 1816 :: Batch 0/1
Batch Loss = 4.597683440617961
1816, epoch_train_loss=4.597683440617961
Epoch 1817
Epoch 1817 :: Batch 0/1
Batch Loss = 4.598358589680725
1817, epoch_train_loss=4.598358589680725
Epoch 1818
Epoch 1818 :: Batch 0/1
Batch Loss = 4.597572334285329
1818, epoch_train_loss=4.597572334285329
Epoch 1819
Epoch 1819 :: Batch 0/1
Batch Loss = 4.595617114413605
1819, epoch_train_loss=4.595617114413605
Epoch 1820
Epoch 1820 :: Batch 0/1
Batch Loss = 4.592958471729891
1820, epoch_train_loss=4.592958471729891
Epoch 1821
Epoch 1821 :: Batch 0/1
Batch Loss = 4.59011469263447
1821, epoch_train_loss=4.59011469263447
Epoch 1822
Epoch 1822 :: Batch 0/1
Batch Loss = 4.587550254634046
1822, epoch_train_loss=4.587550254634046
Epoch 1823
Epoch 1823 :: Batch 0/1
Batch Loss = 4.585599686401516
1823, epoch_train_loss=4.585599686401516
Epoch 1824
Epoch 1824 :: Batch 0/1
Batch Loss = 4.584429715336607
1824, epoch_train_loss=4.584429715336607
Epoch 1825
Epoch 1825 :: Batch 0/1
Batch Loss = 4.58403929924885
1825, epoch_train_loss=4.58403929924885
Epoch 1826
Epoch 1826 :: Batch 0/1
Batch Loss = 4.584290500252802
1826, epoch_train_loss=4.584290500252802
Epoch 1827
Epoch 1827 :: Batch 0/1
Batch Loss = 4.584958902424877
1827, epoch_train_loss=4.584958902424877
Epoch 1828
Epoch 1828 :: Batch 0/1
Batch Loss = 4.58579072607915
1828, epoch_train_loss=4.58579072607915
Epoch 1829
Epoch 1829 :: Batch 0/1
Batch Loss = 4.586554828799004
1829, epoch_train_loss=4.586554828799004
Epoch 1830
Epoch 1830 :: Batch 0/1
Batch Loss = 4.5870808461069155
1830, epoch_train_loss=4.5870808461069155
Epoch 1831
Epoch 1831 :: Batch 0/1
Batch Loss = 4.587278918396159
1831, epoch_train_loss=4.587278918396159
Epoch 1832
Epoch 1832 :: Batch 0/1
Batch Loss = 4.587140766614368
1832, epoch_train_loss=4.587140766614368
Epoch 1833
Epoch 1833 :: Batch 0/1
Batch Loss = 4.586725436660664
1833, epoch_train_loss=4.586725436660664
Epoch 1834
Epoch 1834 :: Batch 0/1
Batch Loss = 4.586135276644765
1834, epoch_train_loss=4.586135276644765
Epoch 1835
Epoch 1835 :: Batch 0/1
Batch Loss = 4.585488463607008
1835, epoch_train_loss=4.585488463607008
Epoch 1836
Epoch 1836 :: Batch 0/1
Batch Loss = 4.584893821686849
1836, epoch_train_loss=4.584893821686849
Epoch 1837
Epoch 1837 :: Batch 0/1
Batch Loss = 4.584432150929451
1837, epoch_train_loss=4.584432150929451
Epoch 1838
Epoch 1838 :: Batch 0/1
Batch Loss = 4.584146285806352
1838, epoch_train_loss=4.584146285806352
Epoch 1839
Epoch 1839 :: Batch 0/1
Batch Loss = 4.584040080555278
1839, epoch_train_loss=4.584040080555278
Epoch 1840
Epoch 1840 :: Batch 0/1
Batch Loss = 4.5840848392384315
1840, epoch_train_loss=4.5840848392384315
Epoch 1841
Epoch 1841 :: Batch 0/1
Batch Loss = 4.584230609144484
1841, epoch_train_loss=4.584230609144484
Epoch 1842
Epoch 1842 :: Batch 0/1
Batch Loss = 4.58441933278063
1842, epoch_train_loss=4.58441933278063
Epoch 1843
Epoch 1843 :: Batch 0/1
Batch Loss = 4.584597062218452
1843, epoch_train_loss=4.584597062218452
Epoch 1844
Epoch 1844 :: Batch 0/1
Batch Loss = 4.584723131564721
1844, epoch_train_loss=4.584723131564721
Epoch 1845
Epoch 1845 :: Batch 0/1
Batch Loss = 4.584775141794538
1845, epoch_train_loss=4.584775141794538
Epoch 1846
Epoch 1846 :: Batch 0/1
Batch Loss = 4.584749609598348
1846, epoch_train_loss=4.584749609598348
Epoch 1847
Epoch 1847 :: Batch 0/1
Batch Loss = 4.584658971126871
1847, epoch_train_loss=4.584658971126871
Epoch 1848
Epoch 1848 :: Batch 0/1
Batch Loss = 4.584526184020216
1848, epoch_train_loss=4.584526184020216
Epoch 1849
Epoch 1849 :: Batch 0/1
Batch Loss = 4.584378386353832
1849, epoch_train_loss=4.584378386353832
Epoch 1850
Epoch 1850 :: Batch 0/1
Batch Loss = 4.584240973216108
1850, epoch_train_loss=4.584240973216108
Epoch 1851
Epoch 1851 :: Batch 0/1
Batch Loss = 4.584133117932557
1851, epoch_train_loss=4.584133117932557
Epoch 1852
Epoch 1852 :: Batch 0/1
Batch Loss = 4.584065303218717
1852, epoch_train_loss=4.584065303218717
Epoch 1853
Epoch 1853 :: Batch 0/1
Batch Loss = 4.584038946576873
1853, epoch_train_loss=4.584038946576873
Epoch 1854
Epoch 1854 :: Batch 0/1
Batch Loss = 4.5840477977359395
1854, epoch_train_loss=4.5840477977359395
Epoch 1855
Epoch 1855 :: Batch 0/1
Batch Loss = 4.58408051480275
1855, epoch_train_loss=4.58408051480275
Epoch 1856
Epoch 1856 :: Batch 0/1
Batch Loss = 4.584123715782935
1856, epoch_train_loss=4.584123715782935
Epoch 1857
Epoch 1857 :: Batch 0/1
Batch Loss = 4.584164844056071
1857, epoch_train_loss=4.584164844056071
Epoch 1858
Epoch 1858 :: Batch 0/1
Batch Loss = 4.5841943445154865
1858, epoch_train_loss=4.5841943445154865
Epoch 1859
Epoch 1859 :: Batch 0/1
Batch Loss = 4.584206870358394
1859, epoch_train_loss=4.584206870358394
Epoch 1860
Epoch 1860 :: Batch 0/1
Batch Loss = 4.584201474930105
1860, epoch_train_loss=4.584201474930105
Epoch 1861
Epoch 1861 :: Batch 0/1
Batch Loss = 4.58418094234389
1861, epoch_train_loss=4.58418094234389
Epoch 1862
Epoch 1862 :: Batch 0/1
Batch Loss = 4.584150543856474
1862, epoch_train_loss=4.584150543856474
Epoch 1863
Epoch 1863 :: Batch 0/1
Batch Loss = 4.584116561342183
1863, epoch_train_loss=4.584116561342183
Epoch 1864
Epoch 1864 :: Batch 0/1
Batch Loss = 4.584084899171094
1864, epoch_train_loss=4.584084899171094
Epoch 1865
Epoch 1865 :: Batch 0/1
Batch Loss = 4.584060029147036
1865, epoch_train_loss=4.584060029147036
Epoch 1866
Epoch 1866 :: Batch 0/1
Batch Loss = 4.584044404730354
1866, epoch_train_loss=4.584044404730354
Epoch 1867
Epoch 1867 :: Batch 0/1
Batch Loss = 4.584038366973059
1867, epoch_train_loss=4.584038366973059
Epoch 1868
Epoch 1868 :: Batch 0/1
Batch Loss = 4.5840404677682125
1868, epoch_train_loss=4.5840404677682125
Epoch 1869
Epoch 1869 :: Batch 0/1
Batch Loss = 4.5840480712308755
1869, epoch_train_loss=4.5840480712308755
Epoch 1870
Epoch 1870 :: Batch 0/1
Batch Loss = 4.5840580675276845
1870, epoch_train_loss=4.5840580675276845
Epoch 1871
Epoch 1871 :: Batch 0/1
Batch Loss = 4.584067543179794
1871, epoch_train_loss=4.584067543179794
Epoch 1872
Epoch 1872 :: Batch 0/1
Batch Loss = 4.584074289177593
1872, epoch_train_loss=4.584074289177593
Epoch 1873
Epoch 1873 :: Batch 0/1
Batch Loss = 4.584077081094881
1873, epoch_train_loss=4.584077081094881
Epoch 1874
Epoch 1874 :: Batch 0/1
Batch Loss = 4.584075720789187
1874, epoch_train_loss=4.584075720789187
Epoch 1875
Epoch 1875 :: Batch 0/1
Batch Loss = 4.584070876369035
1875, epoch_train_loss=4.584070876369035
Epoch 1876
Epoch 1876 :: Batch 0/1
Batch Loss = 4.584063788453346
1876, epoch_train_loss=4.584063788453346
Epoch 1877
Epoch 1877 :: Batch 0/1
Batch Loss = 4.584055923388103
1877, epoch_train_loss=4.584055923388103
Epoch 1878
Epoch 1878 :: Batch 0/1
Batch Loss = 4.584048649065994
1878, epoch_train_loss=4.584048649065994
Epoch 1879
Epoch 1879 :: Batch 0/1
Batch Loss = 4.584042990596074
1879, epoch_train_loss=4.584042990596074
Epoch 1880
Epoch 1880 :: Batch 0/1
Batch Loss = 4.584039497218624
1880, epoch_train_loss=4.584039497218624
Epoch 1881
Epoch 1881 :: Batch 0/1
Batch Loss = 4.584038224897261
1881, epoch_train_loss=4.584038224897261
Epoch 1882
Epoch 1882 :: Batch 0/1
Batch Loss = 4.584038816216975
1882, epoch_train_loss=4.584038816216975
Epoch 1883
Epoch 1883 :: Batch 0/1
Batch Loss = 4.584040644159683
1883, epoch_train_loss=4.584040644159683
Epoch 1884
Epoch 1884 :: Batch 0/1
Batch Loss = 4.584042980427878
1884, epoch_train_loss=4.584042980427878
Epoch 1885
Epoch 1885 :: Batch 0/1
Batch Loss = 4.584045151660386
1885, epoch_train_loss=4.584045151660386
Epoch 1886
Epoch 1886 :: Batch 0/1
Batch Loss = 4.584046656033284
1886, epoch_train_loss=4.584046656033284
Epoch 1887
Epoch 1887 :: Batch 0/1
Batch Loss = 4.5840472254248175
1887, epoch_train_loss=4.5840472254248175
Epoch 1888
Epoch 1888 :: Batch 0/1
Batch Loss = 4.584046831464931
1888, epoch_train_loss=4.584046831464931
Epoch 1889
Epoch 1889 :: Batch 0/1
Batch Loss = 4.584045644804595
1889, epoch_train_loss=4.584045644804595
Epoch 1890
Epoch 1890 :: Batch 0/1
Batch Loss = 4.584043964125185
1890, epoch_train_loss=4.584043964125185
Epoch 1891
Epoch 1891 :: Batch 0/1
Batch Loss = 4.5840421340975
1891, epoch_train_loss=4.5840421340975
Epoch 1892
Epoch 1892 :: Batch 0/1
Batch Loss = 4.584040470026501
1892, epoch_train_loss=4.584040470026501
Epoch 1893
Epoch 1893 :: Batch 0/1
Batch Loss = 4.5840392023292145
1893, epoch_train_loss=4.5840392023292145
Epoch 1894
Epoch 1894 :: Batch 0/1
Batch Loss = 4.584038447743823
1894, epoch_train_loss=4.584038447743823
Epoch 1895
Epoch 1895 :: Batch 0/1
Batch Loss = 4.5840382077728155
1895, epoch_train_loss=4.5840382077728155
Epoch 1896
Epoch 1896 :: Batch 0/1
Batch Loss = 4.584038389552777
1896, epoch_train_loss=4.584038389552777
Epoch 1897
Epoch 1897 :: Batch 0/1
Batch Loss = 4.584038840949749
1897, epoch_train_loss=4.584038840949749
Epoch 1898
Epoch 1898 :: Batch 0/1
Batch Loss = 4.584039390484748
1898, epoch_train_loss=4.584039390484748
Epoch 1899
Epoch 1899 :: Batch 0/1
Batch Loss = 4.584039883522653
1899, epoch_train_loss=4.584039883522653
Epoch 1900
Epoch 1900 :: Batch 0/1
Batch Loss = 4.584040208474684
1900, epoch_train_loss=4.584040208474684
Epoch 1901
Epoch 1901 :: Batch 0/1
Batch Loss = 4.584040309849244
1901, epoch_train_loss=4.584040309849244
Epoch 1902
Epoch 1902 :: Batch 0/1
Batch Loss = 4.584040188101404
1902, epoch_train_loss=4.584040188101404
Epoch 1903
Epoch 1903 :: Batch 0/1
Batch Loss = 4.584039888778407
1903, epoch_train_loss=4.584039888778407
Epoch 1904
Epoch 1904 :: Batch 0/1
Batch Loss = 4.584039485049381
1904, epoch_train_loss=4.584039485049381
Epoch 1905
Epoch 1905 :: Batch 0/1
Batch Loss = 4.584039058216892
1905, epoch_train_loss=4.584039058216892
Epoch 1906
Epoch 1906 :: Batch 0/1
Batch Loss = 4.584038680338006
1906, epoch_train_loss=4.584038680338006
Epoch 1907
Epoch 1907 :: Batch 0/1
Batch Loss = 4.584038401905753
1907, epoch_train_loss=4.584038401905753
Epoch 1908
Epoch 1908 :: Batch 0/1
Batch Loss = 4.584038246016254
1908, epoch_train_loss=4.584038246016254
Epoch 1909
Epoch 1909 :: Batch 0/1
Batch Loss = 4.584038208930755
1909, epoch_train_loss=4.584038208930755
Epoch 1910
Epoch 1910 :: Batch 0/1
Batch Loss = 4.584038265725666
1910, epoch_train_loss=4.584038265725666
Epoch 1911
Epoch 1911 :: Batch 0/1
Batch Loss = 4.5840383789862456
1911, epoch_train_loss=4.5840383789862456
Epoch 1912
Epoch 1912 :: Batch 0/1
Batch Loss = 4.58403850829377
1912, epoch_train_loss=4.58403850829377
Epoch 1913
Epoch 1913 :: Batch 0/1
Batch Loss = 4.5840386185232305
1913, epoch_train_loss=4.5840386185232305
Epoch 1914
Epoch 1914 :: Batch 0/1
Batch Loss = 4.5840386855687365
1914, epoch_train_loss=4.5840386855687365
Epoch 1915
Epoch 1915 :: Batch 0/1
Batch Loss = 4.584038698869587
1915, epoch_train_loss=4.584038698869587
Epoch 1916
Epoch 1916 :: Batch 0/1
Batch Loss = 4.584038660846132
1916, epoch_train_loss=4.584038660846132
Epoch 1917
Epoch 1917 :: Batch 0/1
Batch Loss = 4.584038583932405
1917, epoch_train_loss=4.584038583932405
Epoch 1918
Epoch 1918 :: Batch 0/1
Batch Loss = 4.584038486229739
1918, epoch_train_loss=4.584038486229739
Epoch 1919
Epoch 1919 :: Batch 0/1
Batch Loss = 4.584038386881641
1919, epoch_train_loss=4.584038386881641
Epoch 1920
Epoch 1920 :: Batch 0/1
Batch Loss = 4.584038302118746
1920, epoch_train_loss=4.584038302118746
Epoch 1921
Epoch 1921 :: Batch 0/1
Batch Loss = 4.584038242615638
1921, epoch_train_loss=4.584038242615638
Epoch 1922
Epoch 1922 :: Batch 0/1
Batch Loss = 4.584038212426835
1922, epoch_train_loss=4.584038212426835
Epoch 1923
Epoch 1923 :: Batch 0/1
Batch Loss = 4.584038209412714
1923, epoch_train_loss=4.584038209412714
Epoch 1924
Epoch 1924 :: Batch 0/1
Batch Loss = 4.584038226793336
1924, epoch_train_loss=4.584038226793336
Epoch 1925
Epoch 1925 :: Batch 0/1
Batch Loss = 4.584038255316498
1925, epoch_train_loss=4.584038255316498
Epoch 1926
Epoch 1926 :: Batch 0/1
Batch Loss = 4.584038285502875
1926, epoch_train_loss=4.584038285502875
Epoch 1927
Epoch 1927 :: Batch 0/1
Batch Loss = 4.584038309516381
1927, epoch_train_loss=4.584038309516381
Epoch 1928
Epoch 1928 :: Batch 0/1
Batch Loss = 4.584038322365302
1928, epoch_train_loss=4.584038322365302
Epoch 1929
Epoch 1929 :: Batch 0/1
Batch Loss = 4.584038322325142
1929, epoch_train_loss=4.584038322325142
Epoch 1930
Epoch 1930 :: Batch 0/1
Batch Loss = 4.58403831064623
1930, epoch_train_loss=4.58403831064623
Epoch 1931
Epoch 1931 :: Batch 0/1
Batch Loss = 4.584038290736852
1931, epoch_train_loss=4.584038290736852
Epoch 1932
Epoch 1932 :: Batch 0/1
Batch Loss = 4.58403826707944
1932, epoch_train_loss=4.58403826707944
Epoch 1933
Epoch 1933 :: Batch 0/1
Batch Loss = 4.584038244141354
1933, epoch_train_loss=4.584038244141354
Epoch 1934
Epoch 1934 :: Batch 0/1
Batch Loss = 4.584038225494045
1934, epoch_train_loss=4.584038225494045
Epoch 1935
Epoch 1935 :: Batch 0/1
Batch Loss = 4.584038213273743
1935, epoch_train_loss=4.584038213273743
Epoch 1936
Epoch 1936 :: Batch 0/1
Batch Loss = 4.584038208025146
1936, epoch_train_loss=4.584038208025146
Epoch 1937
Epoch 1937 :: Batch 0/1
Batch Loss = 4.5840382088869465
1937, epoch_train_loss=4.5840382088869465
Epoch 1938
Epoch 1938 :: Batch 0/1
Batch Loss = 4.584038214018778
1938, epoch_train_loss=4.584038214018778
Epoch 1939
Epoch 1939 :: Batch 0/1
Batch Loss = 4.584038221140659
1939, epoch_train_loss=4.584038221140659
Epoch 1940
Epoch 1940 :: Batch 0/1
Batch Loss = 4.584038228058025
1940, epoch_train_loss=4.584038228058025
Epoch 1941
Epoch 1941 :: Batch 0/1
Batch Loss = 4.584038233072012
1941, epoch_train_loss=4.584038233072012
Epoch 1942
Epoch 1942 :: Batch 0/1
Batch Loss = 4.584038235215853
1942, epoch_train_loss=4.584038235215853
Epoch 1943
Epoch 1943 :: Batch 0/1
Batch Loss = 4.5840382343035
1943, epoch_train_loss=4.5840382343035
Epoch 1944
Epoch 1944 :: Batch 0/1
Batch Loss = 4.584038230816006
1944, epoch_train_loss=4.584038230816006
Epoch 1945
Epoch 1945 :: Batch 0/1
Batch Loss = 4.5840382256783405
1945, epoch_train_loss=4.5840382256783405
Epoch 1946
Epoch 1946 :: Batch 0/1
Batch Loss = 4.584038219991024
1946, epoch_train_loss=4.584038219991024
Epoch 1947
Epoch 1947 :: Batch 0/1
Batch Loss = 4.58403821477782
1947, epoch_train_loss=4.58403821477782
Epoch 1948
Epoch 1948 :: Batch 0/1
Batch Loss = 4.5840382107961375
1948, epoch_train_loss=4.5840382107961375
Epoch 1949
Epoch 1949 :: Batch 0/1
Batch Loss = 4.584038208435738
1949, epoch_train_loss=4.584038208435738
Epoch 1950
Epoch 1950 :: Batch 0/1
Batch Loss = 4.584038207709152
1950, epoch_train_loss=4.584038207709152
Epoch 1951
Epoch 1951 :: Batch 0/1
Batch Loss = 4.584038208318527
1951, epoch_train_loss=4.584038208318527
Epoch 1952
Epoch 1952 :: Batch 0/1
Batch Loss = 4.58403820977137
1952, epoch_train_loss=4.58403820977137
Epoch 1953
Epoch 1953 :: Batch 0/1
Batch Loss = 4.584038211513159
1953, epoch_train_loss=4.584038211513159
Epoch 1954
Epoch 1954 :: Batch 0/1
Batch Loss = 4.5840382130474895
1954, epoch_train_loss=4.5840382130474895
Epoch 1955
Epoch 1955 :: Batch 0/1
Batch Loss = 4.584038214022275
1955, epoch_train_loss=4.584038214022275
Epoch 1956
Epoch 1956 :: Batch 0/1
Batch Loss = 4.584038214271345
1956, epoch_train_loss=4.584038214271345
Epoch 1957
Epoch 1957 :: Batch 0/1
Batch Loss = 4.584038213811522
1957, epoch_train_loss=4.584038213811522
Epoch 1958
Epoch 1958 :: Batch 0/1
Batch Loss = 4.584038212804087
1958, epoch_train_loss=4.584038212804087
Epoch 1959
Epoch 1959 :: Batch 0/1
Batch Loss = 4.584038211494923
1959, epoch_train_loss=4.584038211494923
Epoch 1960
Epoch 1960 :: Batch 0/1
Batch Loss = 4.5840382101492
1960, epoch_train_loss=4.5840382101492
Epoch 1961
Epoch 1961 :: Batch 0/1
Batch Loss = 4.584038208994527
1961, epoch_train_loss=4.584038208994527
Epoch 1962
Epoch 1962 :: Batch 0/1
Batch Loss = 4.584038208182308
1962, epoch_train_loss=4.584038208182308
Epoch 1963
Epoch 1963 :: Batch 0/1
Batch Loss = 4.584038207771525
1963, epoch_train_loss=4.584038207771525
Epoch 1964
Epoch 1964 :: Batch 0/1
Batch Loss = 4.584038207733968
1964, epoch_train_loss=4.584038207733968
Epoch 1965
Epoch 1965 :: Batch 0/1
Batch Loss = 4.584038207975877
1965, epoch_train_loss=4.584038207975877
Epoch 1966
Epoch 1966 :: Batch 0/1
Batch Loss = 4.5840382083685824
1966, epoch_train_loss=4.5840382083685824
Epoch 1967
Epoch 1967 :: Batch 0/1
Batch Loss = 4.58403820878039
1967, epoch_train_loss=4.58403820878039
Epoch 1968
Epoch 1968 :: Batch 0/1
Batch Loss = 4.584038209103111
1968, epoch_train_loss=4.584038209103111
Epoch 1969
Epoch 1969 :: Batch 0/1
Batch Loss = 4.584038209268959
1969, epoch_train_loss=4.584038209268959
Epoch 1970
Epoch 1970 :: Batch 0/1
Batch Loss = 4.584038209256245
1970, epoch_train_loss=4.584038209256245
Epoch 1971
Epoch 1971 :: Batch 0/1
Batch Loss = 4.584038209084836
1971, epoch_train_loss=4.584038209084836
Epoch 1972
Epoch 1972 :: Batch 0/1
Batch Loss = 4.584038208804193
1972, epoch_train_loss=4.584038208804193
Epoch 1973
Epoch 1973 :: Batch 0/1
Batch Loss = 4.584038208477757
1973, epoch_train_loss=4.584038208477757
Epoch 1974
Epoch 1974 :: Batch 0/1
Batch Loss = 4.584038208167498
1974, epoch_train_loss=4.584038208167498
Epoch 1975
Epoch 1975 :: Batch 0/1
Batch Loss = 4.5840382079216635
1975, epoch_train_loss=4.5840382079216635
Epoch 1976
Epoch 1976 :: Batch 0/1
Batch Loss = 4.584038207767588
1976, epoch_train_loss=4.584038207767588
Epoch 1977
Epoch 1977 :: Batch 0/1
Batch Loss = 4.584038207710043
1977, epoch_train_loss=4.584038207710043
Epoch 1978
Epoch 1978 :: Batch 0/1
Batch Loss = 4.584038207734436
1978, epoch_train_loss=4.584038207734436
Epoch 1979
Epoch 1979 :: Batch 0/1
Batch Loss = 4.584038207813282
1979, epoch_train_loss=4.584038207813282
Epoch 1980
Epoch 1980 :: Batch 0/1
Batch Loss = 4.584038207914064
1980, epoch_train_loss=4.584038207914064
Epoch 1981
Epoch 1981 :: Batch 0/1
Batch Loss = 4.584038208006631
1981, epoch_train_loss=4.584038208006631
Epoch 1982
Epoch 1982 :: Batch 0/1
Batch Loss = 4.58403820806874
1982, epoch_train_loss=4.58403820806874
Epoch 1983
Epoch 1983 :: Batch 0/1
Batch Loss = 4.58403820808898
1983, epoch_train_loss=4.58403820808898
Epoch 1984
Epoch 1984 :: Batch 0/1
Batch Loss = 4.584038208066977
1984, epoch_train_loss=4.584038208066977
Epoch 1985
Epoch 1985 :: Batch 0/1
Batch Loss = 4.584038208011341
1985, epoch_train_loss=4.584038208011341
Epoch 1986
Epoch 1986 :: Batch 0/1
Batch Loss = 4.584038207936208
1986, epoch_train_loss=4.584038207936208
Epoch 1987
Epoch 1987 :: Batch 0/1
Batch Loss = 4.584038207857318
1987, epoch_train_loss=4.584038207857318
Epoch 1988
Epoch 1988 :: Batch 0/1
Batch Loss = 4.5840382077885025
1988, epoch_train_loss=4.5840382077885025
Epoch 1989
Epoch 1989 :: Batch 0/1
Batch Loss = 4.5840382077392405
1989, epoch_train_loss=4.5840382077392405
Epoch 1990
Epoch 1990 :: Batch 0/1
Batch Loss = 4.5840382077135216
1990, epoch_train_loss=4.5840382077135216
Epoch 1991
Epoch 1991 :: Batch 0/1
Batch Loss = 4.584038207710063
1991, epoch_train_loss=4.584038207710063
Epoch 1992
Epoch 1992 :: Batch 0/1
Batch Loss = 4.58403820772353
1992, epoch_train_loss=4.58403820772353
Epoch 1993
Epoch 1993 :: Batch 0/1
Batch Loss = 4.584038207746359
1993, epoch_train_loss=4.584038207746359
Epoch 1994
Epoch 1994 :: Batch 0/1
Batch Loss = 4.58403820777068
1994, epoch_train_loss=4.58403820777068
Epoch 1995
Epoch 1995 :: Batch 0/1
Batch Loss = 4.584038207789959
1995, epoch_train_loss=4.584038207789959
Epoch 1996
Epoch 1996 :: Batch 0/1
Batch Loss = 4.584038207800042
1996, epoch_train_loss=4.584038207800042
Epoch 1997
Epoch 1997 :: Batch 0/1
Batch Loss = 4.584038207799541
1997, epoch_train_loss=4.584038207799541
Epoch 1998
Epoch 1998 :: Batch 0/1
Batch Loss = 4.584038207789582
1998, epoch_train_loss=4.584038207789582
Epoch 1999
Epoch 1999 :: Batch 0/1
Batch Loss = 4.584038207773091
1999, epoch_train_loss=4.584038207773091
Epoch 2000
Epoch 2000 :: Batch 0/1
Batch Loss = 4.584038207753856
2000, epoch_train_loss=4.584038207753856
Epoch 2001
Epoch 2001 :: Batch 0/1
Batch Loss = 4.584038207735587
2001, epoch_train_loss=4.584038207735587
Epoch 2002
Epoch 2002 :: Batch 0/1
Batch Loss = 4.584038207721166
2002, epoch_train_loss=4.584038207721166
Epoch 2003
Epoch 2003 :: Batch 0/1
Batch Loss = 4.5840382077122195
2003, epoch_train_loss=4.5840382077122195
Epoch 2004
Epoch 2004 :: Batch 0/1
Batch Loss = 4.584038207709009
2004, epoch_train_loss=4.584038207709009
Epoch 2005
Epoch 2005 :: Batch 0/1
Batch Loss = 4.584038207710632
2005, epoch_train_loss=4.584038207710632
Epoch 2006
Epoch 2006 :: Batch 0/1
Batch Loss = 4.584038207715426
2006, epoch_train_loss=4.584038207715426
Epoch 2007
Epoch 2007 :: Batch 0/1
Batch Loss = 4.584038207721433
2007, epoch_train_loss=4.584038207721433
Epoch 2008
Epoch 2008 :: Batch 0/1
Batch Loss = 4.584038207726854
2008, epoch_train_loss=4.584038207726854
Epoch 2009
Epoch 2009 :: Batch 0/1
Batch Loss = 4.584038207730391
2009, epoch_train_loss=4.584038207730391
Epoch 2010
Epoch 2010 :: Batch 0/1
Batch Loss = 4.584038207731395
2010, epoch_train_loss=4.584038207731395
Epoch 2011
Epoch 2011 :: Batch 0/1
Batch Loss = 4.584038207729892
2011, epoch_train_loss=4.584038207729892
Epoch 2012
Epoch 2012 :: Batch 0/1
Batch Loss = 4.584038207726435
2012, epoch_train_loss=4.584038207726435
Epoch 2013
Epoch 2013 :: Batch 0/1
Batch Loss = 4.584038207721895
2013, epoch_train_loss=4.584038207721895
Epoch 2014
Epoch 2014 :: Batch 0/1
Batch Loss = 4.584038207717223
2014, epoch_train_loss=4.584038207717223
Epoch 2015
Epoch 2015 :: Batch 0/1
Batch Loss = 4.58403820771323
2015, epoch_train_loss=4.58403820771323
Epoch 2016
Epoch 2016 :: Batch 0/1
Batch Loss = 4.584038207710459
2016, epoch_train_loss=4.584038207710459
Epoch 2017
Epoch 2017 :: Batch 0/1
Batch Loss = 4.58403820770911
2017, epoch_train_loss=4.58403820770911
Epoch 2018
Epoch 2018 :: Batch 0/1
Batch Loss = 4.584038207709069
2018, epoch_train_loss=4.584038207709069
Epoch 2019
Epoch 2019 :: Batch 0/1
Batch Loss = 4.58403820770999
2019, epoch_train_loss=4.58403820770999
Epoch 2020
Epoch 2020 :: Batch 0/1
Batch Loss = 4.584038207711403
2020, epoch_train_loss=4.584038207711403
Epoch 2021
Epoch 2021 :: Batch 0/1
Batch Loss = 4.584038207712838
2021, epoch_train_loss=4.584038207712838
Epoch 2022
Epoch 2022 :: Batch 0/1
Batch Loss = 4.584038207713914
2022, epoch_train_loss=4.584038207713914
Epoch 2023
Epoch 2023 :: Batch 0/1
Batch Loss = 4.584038207714409
2023, epoch_train_loss=4.584038207714409
Epoch 2024
Epoch 2024 :: Batch 0/1
Batch Loss = 4.584038207714264
2024, epoch_train_loss=4.584038207714264
Epoch 2025
Epoch 2025 :: Batch 0/1
Batch Loss = 4.584038207713571
2025, epoch_train_loss=4.584038207713571
Epoch 2026
Epoch 2026 :: Batch 0/1
Batch Loss = 4.584038207712526
2026, epoch_train_loss=4.584038207712526
Epoch 2027
Epoch 2027 :: Batch 0/1
Batch Loss = 4.584038207711361
2027, epoch_train_loss=4.584038207711361
Epoch 2028
Epoch 2028 :: Batch 0/1
Batch Loss = 4.584038207710296
2028, epoch_train_loss=4.584038207710296
Epoch 2029
Epoch 2029 :: Batch 0/1
Batch Loss = 4.584038207709491
2029, epoch_train_loss=4.584038207709491
Epoch 2030
Epoch 2030 :: Batch 0/1
Batch Loss = 4.584038207709032
2030, epoch_train_loss=4.584038207709032
Epoch 2031
Epoch 2031 :: Batch 0/1
Batch Loss = 4.584038207708914
2031, epoch_train_loss=4.584038207708914
Epoch 2032
Epoch 2032 :: Batch 0/1
Batch Loss = 4.584038207709068
2032, epoch_train_loss=4.584038207709068
Epoch 2033
Epoch 2033 :: Batch 0/1
Batch Loss = 4.584038207709387
2033, epoch_train_loss=4.584038207709387
Epoch 2034
Epoch 2034 :: Batch 0/1
Batch Loss = 4.5840382077097495
2034, epoch_train_loss=4.5840382077097495
Epoch 2035
Epoch 2035 :: Batch 0/1
Batch Loss = 4.584038207710052
2035, epoch_train_loss=4.584038207710052
Epoch 2036
Epoch 2036 :: Batch 0/1
Batch Loss = 4.5840382077102255
2036, epoch_train_loss=4.5840382077102255
Epoch 2037
Epoch 2037 :: Batch 0/1
Batch Loss = 4.58403820771024
2037, epoch_train_loss=4.58403820771024
Epoch 2038
Epoch 2038 :: Batch 0/1
Batch Loss = 4.584038207710107
2038, epoch_train_loss=4.584038207710107
Epoch 2039
Epoch 2039 :: Batch 0/1
Batch Loss = 4.584038207709869
2039, epoch_train_loss=4.584038207709869
Epoch 2040
Epoch 2040 :: Batch 0/1
Batch Loss = 4.5840382077095825
2040, epoch_train_loss=4.5840382077095825
Epoch 2041
Epoch 2041 :: Batch 0/1
Batch Loss = 4.5840382077093045
2041, epoch_train_loss=4.5840382077093045
Epoch 2042
Epoch 2042 :: Batch 0/1
Batch Loss = 4.58403820770908
2042, epoch_train_loss=4.58403820770908
Epoch 2043
Epoch 2043 :: Batch 0/1
Batch Loss = 4.584038207708938
2043, epoch_train_loss=4.584038207708938
Epoch 2044
Epoch 2044 :: Batch 0/1
Batch Loss = 4.584038207708881
2044, epoch_train_loss=4.584038207708881
Epoch 2045
Epoch 2045 :: Batch 0/1
Batch Loss = 4.5840382077088995
2045, epoch_train_loss=4.5840382077088995
Epoch 2046
Epoch 2046 :: Batch 0/1
Batch Loss = 4.584038207708968
2046, epoch_train_loss=4.584038207708968
Epoch 2047
Epoch 2047 :: Batch 0/1
Batch Loss = 4.584038207709056
2047, epoch_train_loss=4.584038207709056
Epoch 2048
Epoch 2048 :: Batch 0/1
Batch Loss = 4.584038207709136
2048, epoch_train_loss=4.584038207709136
Epoch 2049
Epoch 2049 :: Batch 0/1
Batch Loss = 4.584038207709187
2049, epoch_train_loss=4.584038207709187
Epoch 2050
Epoch 2050 :: Batch 0/1
Batch Loss = 4.584038207709201
2050, epoch_train_loss=4.584038207709201
Epoch 2051
Epoch 2051 :: Batch 0/1
Batch Loss = 4.584038207709176
2051, epoch_train_loss=4.584038207709176
Epoch 2052
Epoch 2052 :: Batch 0/1
Batch Loss = 4.584038207709121
2052, epoch_train_loss=4.584038207709121
Epoch 2053
Epoch 2053 :: Batch 0/1
Batch Loss = 4.584038207709049
2053, epoch_train_loss=4.584038207709049
Epoch 2054
Epoch 2054 :: Batch 0/1
Batch Loss = 4.584038207708976
2054, epoch_train_loss=4.584038207708976
Epoch 2055
Epoch 2055 :: Batch 0/1
Batch Loss = 4.584038207708914
2055, epoch_train_loss=4.584038207708914
Epoch 2056
Epoch 2056 :: Batch 0/1
Batch Loss = 4.584038207708871
2056, epoch_train_loss=4.584038207708871
Epoch 2057
Epoch 2057 :: Batch 0/1
Batch Loss = 4.584038207708849
2057, epoch_train_loss=4.584038207708849
Epoch 2058
Epoch 2058 :: Batch 0/1
Batch Loss = 4.584038207708849
2058, epoch_train_loss=4.584038207708849
Epoch 2059
Epoch 2059 :: Batch 0/1
Batch Loss = 4.584038207708861
2059, epoch_train_loss=4.584038207708861
Epoch 2060
Epoch 2060 :: Batch 0/1
Batch Loss = 4.584038207708882
2060, epoch_train_loss=4.584038207708882
Epoch 2061
Epoch 2061 :: Batch 0/1
Batch Loss = 4.5840382077088995
2061, epoch_train_loss=4.5840382077088995
Epoch 2062
Epoch 2062 :: Batch 0/1
Batch Loss = 4.584038207708913
2062, epoch_train_loss=4.584038207708913
Epoch 2063
Epoch 2063 :: Batch 0/1
Batch Loss = 4.584038207708917
2063, epoch_train_loss=4.584038207708917
Epoch 2064
Epoch 2064 :: Batch 0/1
Batch Loss = 4.58403820770891
2064, epoch_train_loss=4.58403820770891
Epoch 2065
Epoch 2065 :: Batch 0/1
Batch Loss = 4.584038207708897
2065, epoch_train_loss=4.584038207708897
Epoch 2066
Epoch 2066 :: Batch 0/1
Batch Loss = 4.584038207708878
2066, epoch_train_loss=4.584038207708878
Epoch 2067
Epoch 2067 :: Batch 0/1
Batch Loss = 4.584038207708857
2067, epoch_train_loss=4.584038207708857
Epoch 2068
Epoch 2068 :: Batch 0/1
Batch Loss = 4.58403820770884
2068, epoch_train_loss=4.58403820770884
Epoch 2069
Epoch 2069 :: Batch 0/1
Batch Loss = 4.584038207708826
2069, epoch_train_loss=4.584038207708826
Epoch 2070
Epoch 2070 :: Batch 0/1
Batch Loss = 4.584038207708818
2070, epoch_train_loss=4.584038207708818
Epoch 2071
Epoch 2071 :: Batch 0/1
Batch Loss = 4.584038207708814
2071, epoch_train_loss=4.584038207708814
Epoch 2072
Epoch 2072 :: Batch 0/1
Batch Loss = 4.584038207708815
2072, epoch_train_loss=4.584038207708815
Epoch 2073
Epoch 2073 :: Batch 0/1
Batch Loss = 4.584038207708819
2073, epoch_train_loss=4.584038207708819
Epoch 2074
Epoch 2074 :: Batch 0/1
Batch Loss = 4.584038207708821
2074, epoch_train_loss=4.584038207708821
Epoch 2075
Epoch 2075 :: Batch 0/1
Batch Loss = 4.584038207708823
2075, epoch_train_loss=4.584038207708823
Epoch 2076
Epoch 2076 :: Batch 0/1
Batch Loss = 4.584038207708822
2076, epoch_train_loss=4.584038207708822
Epoch 2077
Epoch 2077 :: Batch 0/1
Batch Loss = 4.5840382077088195
2077, epoch_train_loss=4.5840382077088195
Epoch 2078
Epoch 2078 :: Batch 0/1
Batch Loss = 4.584038207708815
2078, epoch_train_loss=4.584038207708815
Epoch 2079
Epoch 2079 :: Batch 0/1
Batch Loss = 4.584038207708808
2079, epoch_train_loss=4.584038207708808
Epoch 2080
Epoch 2080 :: Batch 0/1
Batch Loss = 4.584038207708801
2080, epoch_train_loss=4.584038207708801
Epoch 2081
Epoch 2081 :: Batch 0/1
Batch Loss = 4.5840382077087956
2081, epoch_train_loss=4.5840382077087956
Epoch 2082
Epoch 2082 :: Batch 0/1
Batch Loss = 4.584038207708789
2082, epoch_train_loss=4.584038207708789
Epoch 2083
Epoch 2083 :: Batch 0/1
Batch Loss = 4.584038207708785
2083, epoch_train_loss=4.584038207708785
Epoch 2084
Epoch 2084 :: Batch 0/1
Batch Loss = 4.584038207708781
2084, epoch_train_loss=4.584038207708781
Epoch 2085
Epoch 2085 :: Batch 0/1
Batch Loss = 4.5840382077087805
2085, epoch_train_loss=4.5840382077087805
Epoch 2086
Epoch 2086 :: Batch 0/1
Batch Loss = 4.584038207708779
2086, epoch_train_loss=4.584038207708779
Epoch 2087
Epoch 2087 :: Batch 0/1
Batch Loss = 4.584038207708778
2087, epoch_train_loss=4.584038207708778
Epoch 2088
Epoch 2088 :: Batch 0/1
Batch Loss = 4.584038207708777
2088, epoch_train_loss=4.584038207708777
Epoch 2089
Epoch 2089 :: Batch 0/1
Batch Loss = 4.584038207708774
2089, epoch_train_loss=4.584038207708774
Epoch 2090
Epoch 2090 :: Batch 0/1
Batch Loss = 4.584038207708772
2090, epoch_train_loss=4.584038207708772
Epoch 2091
Epoch 2091 :: Batch 0/1
Batch Loss = 4.584038207708769
2091, epoch_train_loss=4.584038207708769
Epoch 2092
Epoch 2092 :: Batch 0/1
Batch Loss = 4.584038207708766
2092, epoch_train_loss=4.584038207708766
Epoch 2093
Epoch 2093 :: Batch 0/1
Batch Loss = 4.584038207708762
2093, epoch_train_loss=4.584038207708762
Epoch 2094
Epoch 2094 :: Batch 0/1
Batch Loss = 4.584038207708759
2094, epoch_train_loss=4.584038207708759
Epoch 2095
Epoch 2095 :: Batch 0/1
Batch Loss = 4.584038207708756
2095, epoch_train_loss=4.584038207708756
Epoch 2096
Epoch 2096 :: Batch 0/1
Batch Loss = 4.584038207708752
2096, epoch_train_loss=4.584038207708752
Epoch 2097
Epoch 2097 :: Batch 0/1
Batch Loss = 4.584038207708749
2097, epoch_train_loss=4.584038207708749
Epoch 2098
Epoch 2098 :: Batch 0/1
Batch Loss = 4.584038207708748
2098, epoch_train_loss=4.584038207708748
Epoch 2099
Epoch 2099 :: Batch 0/1
Batch Loss = 4.584038207708745
2099, epoch_train_loss=4.584038207708745
Epoch 2100
Epoch 2100 :: Batch 0/1
Batch Loss = 4.584038207708743
2100, epoch_train_loss=4.584038207708743
Epoch 2101
Epoch 2101 :: Batch 0/1
Batch Loss = 4.584038207708741
2101, epoch_train_loss=4.584038207708741
Epoch 2102
Epoch 2102 :: Batch 0/1
Batch Loss = 4.584038207708739
2102, epoch_train_loss=4.584038207708739
Epoch 2103
Epoch 2103 :: Batch 0/1
Batch Loss = 4.584038207708737
2103, epoch_train_loss=4.584038207708737
Epoch 2104
Epoch 2104 :: Batch 0/1
Batch Loss = 4.584038207708733
2104, epoch_train_loss=4.584038207708733
Epoch 2105
Epoch 2105 :: Batch 0/1
Batch Loss = 4.584038207708731
2105, epoch_train_loss=4.584038207708731
Epoch 2106
Epoch 2106 :: Batch 0/1
Batch Loss = 4.584038207708729
2106, epoch_train_loss=4.584038207708729
Epoch 2107
Epoch 2107 :: Batch 0/1
Batch Loss = 4.584038207708725
2107, epoch_train_loss=4.584038207708725
Epoch 2108
Epoch 2108 :: Batch 0/1
Batch Loss = 4.584038207708722
2108, epoch_train_loss=4.584038207708722
Epoch 2109
Epoch 2109 :: Batch 0/1
Batch Loss = 4.584038207708719
2109, epoch_train_loss=4.584038207708719
Epoch 2110
Epoch 2110 :: Batch 0/1
Batch Loss = 4.584038207708717
2110, epoch_train_loss=4.584038207708717
Epoch 2111
Epoch 2111 :: Batch 0/1
Batch Loss = 4.584038207708714
2111, epoch_train_loss=4.584038207708714
Epoch 2112
Epoch 2112 :: Batch 0/1
Batch Loss = 4.584038207708712
2112, epoch_train_loss=4.584038207708712
Epoch 2113
Epoch 2113 :: Batch 0/1
Batch Loss = 4.58403820770871
2113, epoch_train_loss=4.58403820770871
Epoch 2114
Epoch 2114 :: Batch 0/1
Batch Loss = 4.584038207708708
2114, epoch_train_loss=4.584038207708708
Epoch 2115
Epoch 2115 :: Batch 0/1
Batch Loss = 4.584038207708705
2115, epoch_train_loss=4.584038207708705
Epoch 2116
Epoch 2116 :: Batch 0/1
Batch Loss = 4.584038207708702
2116, epoch_train_loss=4.584038207708702
Epoch 2117
Epoch 2117 :: Batch 0/1
Batch Loss = 4.5840382077087
2117, epoch_train_loss=4.5840382077087
Epoch 2118
Epoch 2118 :: Batch 0/1
Batch Loss = 4.584038207708698
2118, epoch_train_loss=4.584038207708698
Epoch 2119
Epoch 2119 :: Batch 0/1
Batch Loss = 4.584038207708696
2119, epoch_train_loss=4.584038207708696
Epoch 2120
Epoch 2120 :: Batch 0/1
Batch Loss = 4.5840382077086925
2120, epoch_train_loss=4.5840382077086925
Epoch 2121
Epoch 2121 :: Batch 0/1
Batch Loss = 4.58403820770869
2121, epoch_train_loss=4.58403820770869
Epoch 2122
Epoch 2122 :: Batch 0/1
Batch Loss = 4.584038207708687
2122, epoch_train_loss=4.584038207708687
Epoch 2123
Epoch 2123 :: Batch 0/1
Batch Loss = 4.5840382077086845
2123, epoch_train_loss=4.5840382077086845
Epoch 2124
Epoch 2124 :: Batch 0/1
Batch Loss = 4.584038207708682
2124, epoch_train_loss=4.584038207708682
Epoch 2125
Epoch 2125 :: Batch 0/1
Batch Loss = 4.58403820770868
2125, epoch_train_loss=4.58403820770868
Epoch 2126
Epoch 2126 :: Batch 0/1
Batch Loss = 4.584038207708677
2126, epoch_train_loss=4.584038207708677
Epoch 2127
Epoch 2127 :: Batch 0/1
Batch Loss = 4.584038207708675
2127, epoch_train_loss=4.584038207708675
Epoch 2128
Epoch 2128 :: Batch 0/1
Batch Loss = 4.584038207708673
2128, epoch_train_loss=4.584038207708673
Epoch 2129
Epoch 2129 :: Batch 0/1
Batch Loss = 4.584038207708669
2129, epoch_train_loss=4.584038207708669
Epoch 2130
Epoch 2130 :: Batch 0/1
Batch Loss = 4.584038207708668
2130, epoch_train_loss=4.584038207708668
Epoch 2131
Epoch 2131 :: Batch 0/1
Batch Loss = 4.584038207708664
2131, epoch_train_loss=4.584038207708664
Epoch 2132
Epoch 2132 :: Batch 0/1
Batch Loss = 4.584038207708662
2132, epoch_train_loss=4.584038207708662
Epoch 2133
Epoch 2133 :: Batch 0/1
Batch Loss = 4.5840382077086606
2133, epoch_train_loss=4.5840382077086606
Epoch 2134
Epoch 2134 :: Batch 0/1
Batch Loss = 4.584038207708657
2134, epoch_train_loss=4.584038207708657
Epoch 2135
Epoch 2135 :: Batch 0/1
Batch Loss = 4.584038207708655
2135, epoch_train_loss=4.584038207708655
Epoch 2136
Epoch 2136 :: Batch 0/1
Batch Loss = 4.584038207708652
2136, epoch_train_loss=4.584038207708652
Epoch 2137
Epoch 2137 :: Batch 0/1
Batch Loss = 4.584038207708649
2137, epoch_train_loss=4.584038207708649
Epoch 2138
Epoch 2138 :: Batch 0/1
Batch Loss = 4.584038207708647
2138, epoch_train_loss=4.584038207708647
Epoch 2139
Epoch 2139 :: Batch 0/1
Batch Loss = 4.584038207708644
2139, epoch_train_loss=4.584038207708644
Epoch 2140
Epoch 2140 :: Batch 0/1
Batch Loss = 4.584038207708642
2140, epoch_train_loss=4.584038207708642
Epoch 2141
Epoch 2141 :: Batch 0/1
Batch Loss = 4.584038207708639
2141, epoch_train_loss=4.584038207708639
Epoch 2142
Epoch 2142 :: Batch 0/1
Batch Loss = 4.584038207708637
2142, epoch_train_loss=4.584038207708637
Epoch 2143
Epoch 2143 :: Batch 0/1
Batch Loss = 4.584038207708634
2143, epoch_train_loss=4.584038207708634
Epoch 2144
Epoch 2144 :: Batch 0/1
Batch Loss = 4.584038207708631
2144, epoch_train_loss=4.584038207708631
Epoch 2145
Epoch 2145 :: Batch 0/1
Batch Loss = 4.5840382077086295
2145, epoch_train_loss=4.5840382077086295
Epoch 2146
Epoch 2146 :: Batch 0/1
Batch Loss = 4.584038207708626
2146, epoch_train_loss=4.584038207708626
Epoch 2147
Epoch 2147 :: Batch 0/1
Batch Loss = 4.584038207708624
2147, epoch_train_loss=4.584038207708624
Epoch 2148
Epoch 2148 :: Batch 0/1
Batch Loss = 4.5840382077086215
2148, epoch_train_loss=4.5840382077086215
Epoch 2149
Epoch 2149 :: Batch 0/1
Batch Loss = 4.584038207708619
2149, epoch_train_loss=4.584038207708619
Epoch 2150
Epoch 2150 :: Batch 0/1
Batch Loss = 4.584038207708616
2150, epoch_train_loss=4.584038207708616
Epoch 2151
Epoch 2151 :: Batch 0/1
Batch Loss = 4.584038207708614
2151, epoch_train_loss=4.584038207708614
Epoch 2152
Epoch 2152 :: Batch 0/1
Batch Loss = 4.584038207708612
2152, epoch_train_loss=4.584038207708612
Epoch 2153
Epoch 2153 :: Batch 0/1
Batch Loss = 4.584038207708608
2153, epoch_train_loss=4.584038207708608
Epoch 2154
Epoch 2154 :: Batch 0/1
Batch Loss = 4.584038207708606
2154, epoch_train_loss=4.584038207708606
Epoch 2155
Epoch 2155 :: Batch 0/1
Batch Loss = 4.584038207708604
2155, epoch_train_loss=4.584038207708604
Epoch 2156
Epoch 2156 :: Batch 0/1
Batch Loss = 4.584038207708601
2156, epoch_train_loss=4.584038207708601
Epoch 2157
Epoch 2157 :: Batch 0/1
Batch Loss = 4.584038207708599
2157, epoch_train_loss=4.584038207708599
Epoch 2158
Epoch 2158 :: Batch 0/1
Batch Loss = 4.584038207708597
2158, epoch_train_loss=4.584038207708597
Epoch 2159
Epoch 2159 :: Batch 0/1
Batch Loss = 4.584038207708593
2159, epoch_train_loss=4.584038207708593
Epoch 2160
Epoch 2160 :: Batch 0/1
Batch Loss = 4.58403820770859
2160, epoch_train_loss=4.58403820770859
Epoch 2161
Epoch 2161 :: Batch 0/1
Batch Loss = 4.584038207708589
2161, epoch_train_loss=4.584038207708589
Epoch 2162
Epoch 2162 :: Batch 0/1
Batch Loss = 4.584038207708586
2162, epoch_train_loss=4.584038207708586
Epoch 2163
Epoch 2163 :: Batch 0/1
Batch Loss = 4.584038207708583
2163, epoch_train_loss=4.584038207708583
Epoch 2164
Epoch 2164 :: Batch 0/1
Batch Loss = 4.58403820770858
2164, epoch_train_loss=4.58403820770858
Epoch 2165
Epoch 2165 :: Batch 0/1
Batch Loss = 4.584038207708577
2165, epoch_train_loss=4.584038207708577
Epoch 2166
Epoch 2166 :: Batch 0/1
Batch Loss = 4.584038207708575
2166, epoch_train_loss=4.584038207708575
Epoch 2167
Epoch 2167 :: Batch 0/1
Batch Loss = 4.584038207708573
2167, epoch_train_loss=4.584038207708573
Epoch 2168
Epoch 2168 :: Batch 0/1
Batch Loss = 4.58403820770857
2168, epoch_train_loss=4.58403820770857
Epoch 2169
Epoch 2169 :: Batch 0/1
Batch Loss = 4.584038207708567
2169, epoch_train_loss=4.584038207708567
Epoch 2170
Epoch 2170 :: Batch 0/1
Batch Loss = 4.584038207708565
2170, epoch_train_loss=4.584038207708565
Epoch 2171
Epoch 2171 :: Batch 0/1
Batch Loss = 4.584038207708562
2171, epoch_train_loss=4.584038207708562
Epoch 2172
Epoch 2172 :: Batch 0/1
Batch Loss = 4.584038207708559
2172, epoch_train_loss=4.584038207708559
Epoch 2173
Epoch 2173 :: Batch 0/1
Batch Loss = 4.5840382077085575
2173, epoch_train_loss=4.5840382077085575
Epoch 2174
Epoch 2174 :: Batch 0/1
Batch Loss = 4.584038207708555
2174, epoch_train_loss=4.584038207708555
Epoch 2175
Epoch 2175 :: Batch 0/1
Batch Loss = 4.584038207708552
2175, epoch_train_loss=4.584038207708552
Epoch 2176
Epoch 2176 :: Batch 0/1
Batch Loss = 4.5840382077085495
2176, epoch_train_loss=4.5840382077085495
Epoch 2177
Epoch 2177 :: Batch 0/1
Batch Loss = 4.584038207708547
2177, epoch_train_loss=4.584038207708547
Epoch 2178
Epoch 2178 :: Batch 0/1
Batch Loss = 4.584038207708544
2178, epoch_train_loss=4.584038207708544
Epoch 2179
Epoch 2179 :: Batch 0/1
Batch Loss = 4.5840382077085415
2179, epoch_train_loss=4.5840382077085415
Epoch 2180
Epoch 2180 :: Batch 0/1
Batch Loss = 4.58403820770854
2180, epoch_train_loss=4.58403820770854
Epoch 2181
Epoch 2181 :: Batch 0/1
Batch Loss = 4.584038207708537
2181, epoch_train_loss=4.584038207708537
Epoch 2182
Epoch 2182 :: Batch 0/1
Batch Loss = 4.584038207708534
2182, epoch_train_loss=4.584038207708534
Epoch 2183
Epoch 2183 :: Batch 0/1
Batch Loss = 4.584038207708532
2183, epoch_train_loss=4.584038207708532
Epoch 2184
Epoch 2184 :: Batch 0/1
Batch Loss = 4.584038207708529
2184, epoch_train_loss=4.584038207708529
Epoch 2185
Epoch 2185 :: Batch 0/1
Batch Loss = 4.584038207708526
2185, epoch_train_loss=4.584038207708526
Epoch 2186
Epoch 2186 :: Batch 0/1
Batch Loss = 4.584038207708524
2186, epoch_train_loss=4.584038207708524
Epoch 2187
Epoch 2187 :: Batch 0/1
Batch Loss = 4.584038207708521
2187, epoch_train_loss=4.584038207708521
Epoch 2188
Epoch 2188 :: Batch 0/1
Batch Loss = 4.584038207708518
2188, epoch_train_loss=4.584038207708518
Epoch 2189
Epoch 2189 :: Batch 0/1
Batch Loss = 4.584038207708517
2189, epoch_train_loss=4.584038207708517
Epoch 2190
Epoch 2190 :: Batch 0/1
Batch Loss = 4.584038207708513
2190, epoch_train_loss=4.584038207708513
Epoch 2191
Epoch 2191 :: Batch 0/1
Batch Loss = 4.584038207708511
2191, epoch_train_loss=4.584038207708511
Epoch 2192
Epoch 2192 :: Batch 0/1
Batch Loss = 4.584038207708507
2192, epoch_train_loss=4.584038207708507
Epoch 2193
Epoch 2193 :: Batch 0/1
Batch Loss = 4.584038207708505
2193, epoch_train_loss=4.584038207708505
Epoch 2194
Epoch 2194 :: Batch 0/1
Batch Loss = 4.5840382077085025
2194, epoch_train_loss=4.5840382077085025
Epoch 2195
Epoch 2195 :: Batch 0/1
Batch Loss = 4.5840382077085
2195, epoch_train_loss=4.5840382077085
Epoch 2196
Epoch 2196 :: Batch 0/1
Batch Loss = 4.584038207708498
2196, epoch_train_loss=4.584038207708498
Epoch 2197
Epoch 2197 :: Batch 0/1
Batch Loss = 4.5840382077084945
2197, epoch_train_loss=4.5840382077084945
Epoch 2198
Epoch 2198 :: Batch 0/1
Batch Loss = 4.584038207708493
2198, epoch_train_loss=4.584038207708493
Epoch 2199
Epoch 2199 :: Batch 0/1
Batch Loss = 4.584038207708489
2199, epoch_train_loss=4.584038207708489
Epoch 2200
Epoch 2200 :: Batch 0/1
Batch Loss = 4.584038207708487
2200, epoch_train_loss=4.584038207708487
Epoch 2201
Epoch 2201 :: Batch 0/1
Batch Loss = 4.584038207708485
2201, epoch_train_loss=4.584038207708485
Epoch 2202
Epoch 2202 :: Batch 0/1
Batch Loss = 4.584038207708482
2202, epoch_train_loss=4.584038207708482
Epoch 2203
Epoch 2203 :: Batch 0/1
Batch Loss = 4.584038207708479
2203, epoch_train_loss=4.584038207708479
Epoch 2204
Epoch 2204 :: Batch 0/1
Batch Loss = 4.584038207708477
2204, epoch_train_loss=4.584038207708477
Epoch 2205
Epoch 2205 :: Batch 0/1
Batch Loss = 4.584038207708474
2205, epoch_train_loss=4.584038207708474
Epoch 2206
Epoch 2206 :: Batch 0/1
Batch Loss = 4.584038207708471
2206, epoch_train_loss=4.584038207708471
Epoch 2207
Epoch 2207 :: Batch 0/1
Batch Loss = 4.584038207708469
2207, epoch_train_loss=4.584038207708469
Epoch 2208
Epoch 2208 :: Batch 0/1
Batch Loss = 4.584038207708466
2208, epoch_train_loss=4.584038207708466
Epoch 2209
Epoch 2209 :: Batch 0/1
Batch Loss = 4.584038207708464
2209, epoch_train_loss=4.584038207708464
Epoch 2210
Epoch 2210 :: Batch 0/1
Batch Loss = 4.584038207708461
2210, epoch_train_loss=4.584038207708461
Epoch 2211
Epoch 2211 :: Batch 0/1
Batch Loss = 4.584038207708459
2211, epoch_train_loss=4.584038207708459
Epoch 2212
Epoch 2212 :: Batch 0/1
Batch Loss = 4.584038207708455
2212, epoch_train_loss=4.584038207708455
Epoch 2213
Epoch 2213 :: Batch 0/1
Batch Loss = 4.584038207708454
2213, epoch_train_loss=4.584038207708454
Epoch 2214
Epoch 2214 :: Batch 0/1
Batch Loss = 4.58403820770845
2214, epoch_train_loss=4.58403820770845
Epoch 2215
Epoch 2215 :: Batch 0/1
Batch Loss = 4.584038207708448
2215, epoch_train_loss=4.584038207708448
Epoch 2216
Epoch 2216 :: Batch 0/1
Batch Loss = 4.584038207708446
2216, epoch_train_loss=4.584038207708446
Epoch 2217
Epoch 2217 :: Batch 0/1
Batch Loss = 4.584038207708443
2217, epoch_train_loss=4.584038207708443
Epoch 2218
Epoch 2218 :: Batch 0/1
Batch Loss = 4.58403820770844
2218, epoch_train_loss=4.58403820770844
Epoch 2219
Epoch 2219 :: Batch 0/1
Batch Loss = 4.584038207708437
2219, epoch_train_loss=4.584038207708437
Epoch 2220
Epoch 2220 :: Batch 0/1
Batch Loss = 4.584038207708434
2220, epoch_train_loss=4.584038207708434
Epoch 2221
Epoch 2221 :: Batch 0/1
Batch Loss = 4.584038207708431
2221, epoch_train_loss=4.584038207708431
Epoch 2222
Epoch 2222 :: Batch 0/1
Batch Loss = 4.584038207708429
2222, epoch_train_loss=4.584038207708429
Epoch 2223
Epoch 2223 :: Batch 0/1
Batch Loss = 4.584038207708426
2223, epoch_train_loss=4.584038207708426
Epoch 2224
Epoch 2224 :: Batch 0/1
Batch Loss = 4.584038207708424
2224, epoch_train_loss=4.584038207708424
Epoch 2225
Epoch 2225 :: Batch 0/1
Batch Loss = 4.584038207708421
2225, epoch_train_loss=4.584038207708421
Epoch 2226
Epoch 2226 :: Batch 0/1
Batch Loss = 4.584038207708419
2226, epoch_train_loss=4.584038207708419
Epoch 2227
Epoch 2227 :: Batch 0/1
Batch Loss = 4.584038207708415
2227, epoch_train_loss=4.584038207708415
Epoch 2228
Epoch 2228 :: Batch 0/1
Batch Loss = 4.584038207708414
2228, epoch_train_loss=4.584038207708414
Epoch 2229
Epoch 2229 :: Batch 0/1
Batch Loss = 4.58403820770841
2229, epoch_train_loss=4.58403820770841
Epoch 2230
Epoch 2230 :: Batch 0/1
Batch Loss = 4.584038207708408
2230, epoch_train_loss=4.584038207708408
Epoch 2231
Epoch 2231 :: Batch 0/1
Batch Loss = 4.584038207708405
2231, epoch_train_loss=4.584038207708405
Epoch 2232
Epoch 2232 :: Batch 0/1
Batch Loss = 4.584038207708403
2232, epoch_train_loss=4.584038207708403
Epoch 2233
Epoch 2233 :: Batch 0/1
Batch Loss = 4.5840382077084
2233, epoch_train_loss=4.5840382077084
Epoch 2234
Epoch 2234 :: Batch 0/1
Batch Loss = 4.584038207708398
2234, epoch_train_loss=4.584038207708398
Epoch 2235
Epoch 2235 :: Batch 0/1
Batch Loss = 4.584038207708395
2235, epoch_train_loss=4.584038207708395
Epoch 2236
Epoch 2236 :: Batch 0/1
Batch Loss = 4.584038207708392
2236, epoch_train_loss=4.584038207708392
Epoch 2237
Epoch 2237 :: Batch 0/1
Batch Loss = 4.58403820770839
2237, epoch_train_loss=4.58403820770839
Epoch 2238
Epoch 2238 :: Batch 0/1
Batch Loss = 4.584038207708387
2238, epoch_train_loss=4.584038207708387
Epoch 2239
Epoch 2239 :: Batch 0/1
Batch Loss = 4.584038207708384
2239, epoch_train_loss=4.584038207708384
Epoch 2240
Epoch 2240 :: Batch 0/1
Batch Loss = 4.584038207708382
2240, epoch_train_loss=4.584038207708382
Epoch 2241
Epoch 2241 :: Batch 0/1
Batch Loss = 4.584038207708379
2241, epoch_train_loss=4.584038207708379
Epoch 2242
Epoch 2242 :: Batch 0/1
Batch Loss = 4.584038207708376
2242, epoch_train_loss=4.584038207708376
Epoch 2243
Epoch 2243 :: Batch 0/1
Batch Loss = 4.584038207708374
2243, epoch_train_loss=4.584038207708374
Epoch 2244
Epoch 2244 :: Batch 0/1
Batch Loss = 4.584038207708371
2244, epoch_train_loss=4.584038207708371
Epoch 2245
Epoch 2245 :: Batch 0/1
Batch Loss = 4.5840382077083675
2245, epoch_train_loss=4.5840382077083675
Epoch 2246
Epoch 2246 :: Batch 0/1
Batch Loss = 4.584038207708365
2246, epoch_train_loss=4.584038207708365
Epoch 2247
Epoch 2247 :: Batch 0/1
Batch Loss = 4.584038207708363
2247, epoch_train_loss=4.584038207708363
Epoch 2248
Epoch 2248 :: Batch 0/1
Batch Loss = 4.5840382077083595
2248, epoch_train_loss=4.5840382077083595
Epoch 2249
Epoch 2249 :: Batch 0/1
Batch Loss = 4.584038207708358
2249, epoch_train_loss=4.584038207708358
Epoch 2250
Epoch 2250 :: Batch 0/1
Batch Loss = 4.584038207708354
2250, epoch_train_loss=4.584038207708354
Epoch 2251
Epoch 2251 :: Batch 0/1
Batch Loss = 4.5840382077083515
2251, epoch_train_loss=4.5840382077083515
Epoch 2252
Epoch 2252 :: Batch 0/1
Batch Loss = 4.584038207708349
2252, epoch_train_loss=4.584038207708349
Epoch 2253
Epoch 2253 :: Batch 0/1
Batch Loss = 4.584038207708346
2253, epoch_train_loss=4.584038207708346
Epoch 2254
Epoch 2254 :: Batch 0/1
Batch Loss = 4.5840382077083435
2254, epoch_train_loss=4.5840382077083435
Epoch 2255
Epoch 2255 :: Batch 0/1
Batch Loss = 4.584038207708342
2255, epoch_train_loss=4.584038207708342
Epoch 2256
Epoch 2256 :: Batch 0/1
Batch Loss = 4.584038207708338
2256, epoch_train_loss=4.584038207708338
Epoch 2257
Epoch 2257 :: Batch 0/1
Batch Loss = 4.5840382077083355
2257, epoch_train_loss=4.5840382077083355
Epoch 2258
Epoch 2258 :: Batch 0/1
Batch Loss = 4.584038207708333
2258, epoch_train_loss=4.584038207708333
Epoch 2259
Epoch 2259 :: Batch 0/1
Batch Loss = 4.58403820770833
2259, epoch_train_loss=4.58403820770833
Epoch 2260
Epoch 2260 :: Batch 0/1
Batch Loss = 4.584038207708328
2260, epoch_train_loss=4.584038207708328
Epoch 2261
Epoch 2261 :: Batch 0/1
Batch Loss = 4.584038207708326
2261, epoch_train_loss=4.584038207708326
Epoch 2262
Epoch 2262 :: Batch 0/1
Batch Loss = 4.584038207708322
2262, epoch_train_loss=4.584038207708322
Epoch 2263
Epoch 2263 :: Batch 0/1
Batch Loss = 4.58403820770832
2263, epoch_train_loss=4.58403820770832
Epoch 2264
Epoch 2264 :: Batch 0/1
Batch Loss = 4.584038207708318
2264, epoch_train_loss=4.584038207708318
Epoch 2265
Epoch 2265 :: Batch 0/1
Batch Loss = 4.584038207708314
2265, epoch_train_loss=4.584038207708314
Epoch 2266
Epoch 2266 :: Batch 0/1
Batch Loss = 4.5840382077083115
2266, epoch_train_loss=4.5840382077083115
Epoch 2267
Epoch 2267 :: Batch 0/1
Batch Loss = 4.58403820770831
2267, epoch_train_loss=4.58403820770831
Epoch 2268
Epoch 2268 :: Batch 0/1
Batch Loss = 4.584038207708306
2268, epoch_train_loss=4.584038207708306
Epoch 2269
Epoch 2269 :: Batch 0/1
Batch Loss = 4.584038207708304
2269, epoch_train_loss=4.584038207708304
Epoch 2270
Epoch 2270 :: Batch 0/1
Batch Loss = 4.584038207708301
2270, epoch_train_loss=4.584038207708301
Epoch 2271
Epoch 2271 :: Batch 0/1
Batch Loss = 4.584038207708297
2271, epoch_train_loss=4.584038207708297
Epoch 2272
Epoch 2272 :: Batch 0/1
Batch Loss = 4.584038207708295
2272, epoch_train_loss=4.584038207708295
Epoch 2273
Epoch 2273 :: Batch 0/1
Batch Loss = 4.584038207708292
2273, epoch_train_loss=4.584038207708292
Epoch 2274
Epoch 2274 :: Batch 0/1
Batch Loss = 4.584038207708289
2274, epoch_train_loss=4.584038207708289
Epoch 2275
Epoch 2275 :: Batch 0/1
Batch Loss = 4.5840382077082875
2275, epoch_train_loss=4.5840382077082875
Epoch 2276
Epoch 2276 :: Batch 0/1
Batch Loss = 4.584038207708284
2276, epoch_train_loss=4.584038207708284
Epoch 2277
Epoch 2277 :: Batch 0/1
Batch Loss = 4.584038207708281
2277, epoch_train_loss=4.584038207708281
Epoch 2278
Epoch 2278 :: Batch 0/1
Batch Loss = 4.584038207708279
2278, epoch_train_loss=4.584038207708279
Epoch 2279
Epoch 2279 :: Batch 0/1
Batch Loss = 4.584038207708277
2279, epoch_train_loss=4.584038207708277
Epoch 2280
Epoch 2280 :: Batch 0/1
Batch Loss = 4.584038207708273
2280, epoch_train_loss=4.584038207708273
Epoch 2281
Epoch 2281 :: Batch 0/1
Batch Loss = 4.5840382077082715
2281, epoch_train_loss=4.5840382077082715
Epoch 2282
Epoch 2282 :: Batch 0/1
Batch Loss = 4.584038207708268
2282, epoch_train_loss=4.584038207708268
Epoch 2283
Epoch 2283 :: Batch 0/1
Batch Loss = 4.584038207708265
2283, epoch_train_loss=4.584038207708265
Epoch 2284
Epoch 2284 :: Batch 0/1
Batch Loss = 4.584038207708263
2284, epoch_train_loss=4.584038207708263
Epoch 2285
Epoch 2285 :: Batch 0/1
Batch Loss = 4.58403820770826
2285, epoch_train_loss=4.58403820770826
Epoch 2286
Epoch 2286 :: Batch 0/1
Batch Loss = 4.584038207708257
2286, epoch_train_loss=4.584038207708257
Epoch 2287
Epoch 2287 :: Batch 0/1
Batch Loss = 4.584038207708255
2287, epoch_train_loss=4.584038207708255
Epoch 2288
Epoch 2288 :: Batch 0/1
Batch Loss = 4.584038207708252
2288, epoch_train_loss=4.584038207708252
Epoch 2289
Epoch 2289 :: Batch 0/1
Batch Loss = 4.584038207708249
2289, epoch_train_loss=4.584038207708249
Epoch 2290
Epoch 2290 :: Batch 0/1
Batch Loss = 4.584038207708247
2290, epoch_train_loss=4.584038207708247
Epoch 2291
Epoch 2291 :: Batch 0/1
Batch Loss = 4.584038207708244
2291, epoch_train_loss=4.584038207708244
Epoch 2292
Epoch 2292 :: Batch 0/1
Batch Loss = 4.584038207708241
2292, epoch_train_loss=4.584038207708241
Epoch 2293
Epoch 2293 :: Batch 0/1
Batch Loss = 4.584038207708239
2293, epoch_train_loss=4.584038207708239
Epoch 2294
Epoch 2294 :: Batch 0/1
Batch Loss = 4.584038207708236
2294, epoch_train_loss=4.584038207708236
Epoch 2295
Epoch 2295 :: Batch 0/1
Batch Loss = 4.584038207708233
2295, epoch_train_loss=4.584038207708233
Epoch 2296
Epoch 2296 :: Batch 0/1
Batch Loss = 4.584038207708231
2296, epoch_train_loss=4.584038207708231
Epoch 2297
Epoch 2297 :: Batch 0/1
Batch Loss = 4.584038207708228
2297, epoch_train_loss=4.584038207708228
Epoch 2298
Epoch 2298 :: Batch 0/1
Batch Loss = 4.584038207708224
2298, epoch_train_loss=4.584038207708224
Epoch 2299
Epoch 2299 :: Batch 0/1
Batch Loss = 4.584038207708221
2299, epoch_train_loss=4.584038207708221
Epoch 2300
Epoch 2300 :: Batch 0/1
Batch Loss = 4.584038207708219
2300, epoch_train_loss=4.584038207708219
Epoch 2301
Epoch 2301 :: Batch 0/1
Batch Loss = 4.584038207708216
2301, epoch_train_loss=4.584038207708216
Epoch 2302
Epoch 2302 :: Batch 0/1
Batch Loss = 4.584038207708214
2302, epoch_train_loss=4.584038207708214
Epoch 2303
Epoch 2303 :: Batch 0/1
Batch Loss = 4.58403820770821
2303, epoch_train_loss=4.58403820770821
Epoch 2304
Epoch 2304 :: Batch 0/1
Batch Loss = 4.584038207708208
2304, epoch_train_loss=4.584038207708208
Epoch 2305
Epoch 2305 :: Batch 0/1
Batch Loss = 4.584038207708205
2305, epoch_train_loss=4.584038207708205
Epoch 2306
Epoch 2306 :: Batch 0/1
Batch Loss = 4.584038207708202
2306, epoch_train_loss=4.584038207708202
Epoch 2307
Epoch 2307 :: Batch 0/1
Batch Loss = 4.5840382077082
2307, epoch_train_loss=4.5840382077082
Epoch 2308
Epoch 2308 :: Batch 0/1
Batch Loss = 4.584038207708197
2308, epoch_train_loss=4.584038207708197
Epoch 2309
Epoch 2309 :: Batch 0/1
Batch Loss = 4.584038207708194
2309, epoch_train_loss=4.584038207708194
Epoch 2310
Epoch 2310 :: Batch 0/1
Batch Loss = 4.584038207708191
2310, epoch_train_loss=4.584038207708191
Epoch 2311
Epoch 2311 :: Batch 0/1
Batch Loss = 4.584038207708189
2311, epoch_train_loss=4.584038207708189
Epoch 2312
Epoch 2312 :: Batch 0/1
Batch Loss = 4.584038207708185
2312, epoch_train_loss=4.584038207708185
Epoch 2313
Epoch 2313 :: Batch 0/1
Batch Loss = 4.584038207708184
2313, epoch_train_loss=4.584038207708184
Epoch 2314
Epoch 2314 :: Batch 0/1
Batch Loss = 4.58403820770818
2314, epoch_train_loss=4.58403820770818
Epoch 2315
Epoch 2315 :: Batch 0/1
Batch Loss = 4.584038207708177
2315, epoch_train_loss=4.584038207708177
Epoch 2316
Epoch 2316 :: Batch 0/1
Batch Loss = 4.584038207708175
2316, epoch_train_loss=4.584038207708175
Epoch 2317
Epoch 2317 :: Batch 0/1
Batch Loss = 4.584038207708172
2317, epoch_train_loss=4.584038207708172
Epoch 2318
Epoch 2318 :: Batch 0/1
Batch Loss = 4.584038207708169
2318, epoch_train_loss=4.584038207708169
Epoch 2319
Epoch 2319 :: Batch 0/1
Batch Loss = 4.584038207708168
2319, epoch_train_loss=4.584038207708168
Epoch 2320
Epoch 2320 :: Batch 0/1
Batch Loss = 4.584038207708164
2320, epoch_train_loss=4.584038207708164
Epoch 2321
Epoch 2321 :: Batch 0/1
Batch Loss = 4.584038207708161
2321, epoch_train_loss=4.584038207708161
Epoch 2322
Epoch 2322 :: Batch 0/1
Batch Loss = 4.584038207708159
2322, epoch_train_loss=4.584038207708159
Epoch 2323
Epoch 2323 :: Batch 0/1
Batch Loss = 4.584038207708155
2323, epoch_train_loss=4.584038207708155
Epoch 2324
Epoch 2324 :: Batch 0/1
Batch Loss = 4.5840382077081525
2324, epoch_train_loss=4.5840382077081525
Epoch 2325
Epoch 2325 :: Batch 0/1
Batch Loss = 4.584038207708149
2325, epoch_train_loss=4.584038207708149
Epoch 2326
Epoch 2326 :: Batch 0/1
Batch Loss = 4.584038207708146
2326, epoch_train_loss=4.584038207708146
Epoch 2327
Epoch 2327 :: Batch 0/1
Batch Loss = 4.584038207708144
2327, epoch_train_loss=4.584038207708144
Epoch 2328
Epoch 2328 :: Batch 0/1
Batch Loss = 4.584038207708141
2328, epoch_train_loss=4.584038207708141
Epoch 2329
Epoch 2329 :: Batch 0/1
Batch Loss = 4.584038207708138
2329, epoch_train_loss=4.584038207708138
Epoch 2330
Epoch 2330 :: Batch 0/1
Batch Loss = 4.584038207708136
2330, epoch_train_loss=4.584038207708136
Epoch 2331
Epoch 2331 :: Batch 0/1
Batch Loss = 4.584038207708133
2331, epoch_train_loss=4.584038207708133
Epoch 2332
Epoch 2332 :: Batch 0/1
Batch Loss = 4.58403820770813
2332, epoch_train_loss=4.58403820770813
Epoch 2333
Epoch 2333 :: Batch 0/1
Batch Loss = 4.584038207708128
2333, epoch_train_loss=4.584038207708128
Epoch 2334
Epoch 2334 :: Batch 0/1
Batch Loss = 4.584038207708125
2334, epoch_train_loss=4.584038207708125
Epoch 2335
Epoch 2335 :: Batch 0/1
Batch Loss = 4.584038207708122
2335, epoch_train_loss=4.584038207708122
Epoch 2336
Epoch 2336 :: Batch 0/1
Batch Loss = 4.584038207708119
2336, epoch_train_loss=4.584038207708119
Epoch 2337
Epoch 2337 :: Batch 0/1
Batch Loss = 4.584038207708116
2337, epoch_train_loss=4.584038207708116
Epoch 2338
Epoch 2338 :: Batch 0/1
Batch Loss = 4.584038207708113
2338, epoch_train_loss=4.584038207708113
Epoch 2339
Epoch 2339 :: Batch 0/1
Batch Loss = 4.584038207708111
2339, epoch_train_loss=4.584038207708111
Epoch 2340
Epoch 2340 :: Batch 0/1
Batch Loss = 4.584038207708108
2340, epoch_train_loss=4.584038207708108
Epoch 2341
Epoch 2341 :: Batch 0/1
Batch Loss = 4.5840382077081046
2341, epoch_train_loss=4.5840382077081046
Epoch 2342
Epoch 2342 :: Batch 0/1
Batch Loss = 4.584038207708103
2342, epoch_train_loss=4.584038207708103
Epoch 2343
Epoch 2343 :: Batch 0/1
Batch Loss = 4.584038207708099
2343, epoch_train_loss=4.584038207708099
Epoch 2344
Epoch 2344 :: Batch 0/1
Batch Loss = 4.584038207708097
2344, epoch_train_loss=4.584038207708097
Epoch 2345
Epoch 2345 :: Batch 0/1
Batch Loss = 4.584038207708094
2345, epoch_train_loss=4.584038207708094
Epoch 2346
Epoch 2346 :: Batch 0/1
Batch Loss = 4.584038207708091
2346, epoch_train_loss=4.584038207708091
Epoch 2347
Epoch 2347 :: Batch 0/1
Batch Loss = 4.584038207708089
2347, epoch_train_loss=4.584038207708089
Epoch 2348
Epoch 2348 :: Batch 0/1
Batch Loss = 4.584038207708086
2348, epoch_train_loss=4.584038207708086
Epoch 2349
Epoch 2349 :: Batch 0/1
Batch Loss = 4.584038207708082
2349, epoch_train_loss=4.584038207708082
Epoch 2350
Epoch 2350 :: Batch 0/1
Batch Loss = 4.584038207708079
2350, epoch_train_loss=4.584038207708079
Epoch 2351
Epoch 2351 :: Batch 0/1
Batch Loss = 4.584038207708077
2351, epoch_train_loss=4.584038207708077
Epoch 2352
Epoch 2352 :: Batch 0/1
Batch Loss = 4.584038207708074
2352, epoch_train_loss=4.584038207708074
Epoch 2353
Epoch 2353 :: Batch 0/1
Batch Loss = 4.584038207708072
2353, epoch_train_loss=4.584038207708072
Epoch 2354
Epoch 2354 :: Batch 0/1
Batch Loss = 4.584038207708068
2354, epoch_train_loss=4.584038207708068
Epoch 2355
Epoch 2355 :: Batch 0/1
Batch Loss = 4.5840382077080655
2355, epoch_train_loss=4.5840382077080655
Epoch 2356
Epoch 2356 :: Batch 0/1
Batch Loss = 4.584038207708063
2356, epoch_train_loss=4.584038207708063
Epoch 2357
Epoch 2357 :: Batch 0/1
Batch Loss = 4.584038207708059
2357, epoch_train_loss=4.584038207708059
Epoch 2358
Epoch 2358 :: Batch 0/1
Batch Loss = 4.5840382077080575
2358, epoch_train_loss=4.5840382077080575
Epoch 2359
Epoch 2359 :: Batch 0/1
Batch Loss = 4.584038207708055
2359, epoch_train_loss=4.584038207708055
Epoch 2360
Epoch 2360 :: Batch 0/1
Batch Loss = 4.584038207708052
2360, epoch_train_loss=4.584038207708052
Epoch 2361
Epoch 2361 :: Batch 0/1
Batch Loss = 4.584038207708049
2361, epoch_train_loss=4.584038207708049
Epoch 2362
Epoch 2362 :: Batch 0/1
Batch Loss = 4.584038207708046
2362, epoch_train_loss=4.584038207708046
Epoch 2363
Epoch 2363 :: Batch 0/1
Batch Loss = 4.584038207708043
2363, epoch_train_loss=4.584038207708043
Epoch 2364
Epoch 2364 :: Batch 0/1
Batch Loss = 4.5840382077080415
2364, epoch_train_loss=4.5840382077080415
Epoch 2365
Epoch 2365 :: Batch 0/1
Batch Loss = 4.584038207708037
2365, epoch_train_loss=4.584038207708037
Epoch 2366
Epoch 2366 :: Batch 0/1
Batch Loss = 4.584038207708035
2366, epoch_train_loss=4.584038207708035
Epoch 2367
Epoch 2367 :: Batch 0/1
Batch Loss = 4.584038207708032
2367, epoch_train_loss=4.584038207708032
Epoch 2368
Epoch 2368 :: Batch 0/1
Batch Loss = 4.584038207708029
2368, epoch_train_loss=4.584038207708029
Epoch 2369
Epoch 2369 :: Batch 0/1
Batch Loss = 4.584038207708026
2369, epoch_train_loss=4.584038207708026
Epoch 2370
Epoch 2370 :: Batch 0/1
Batch Loss = 4.584038207708024
2370, epoch_train_loss=4.584038207708024
Epoch 2371
Epoch 2371 :: Batch 0/1
Batch Loss = 4.584038207708021
2371, epoch_train_loss=4.584038207708021
Epoch 2372
Epoch 2372 :: Batch 0/1
Batch Loss = 4.5840382077080175
2372, epoch_train_loss=4.5840382077080175
Epoch 2373
Epoch 2373 :: Batch 0/1
Batch Loss = 4.584038207708015
2373, epoch_train_loss=4.584038207708015
Epoch 2374
Epoch 2374 :: Batch 0/1
Batch Loss = 4.584038207708011
2374, epoch_train_loss=4.584038207708011
Epoch 2375
Epoch 2375 :: Batch 0/1
Batch Loss = 4.584038207708009
2375, epoch_train_loss=4.584038207708009
Epoch 2376
Epoch 2376 :: Batch 0/1
Batch Loss = 4.584038207708006
2376, epoch_train_loss=4.584038207708006
Epoch 2377
Epoch 2377 :: Batch 0/1
Batch Loss = 4.584038207708003
2377, epoch_train_loss=4.584038207708003
Epoch 2378
Epoch 2378 :: Batch 0/1
Batch Loss = 4.584038207708
2378, epoch_train_loss=4.584038207708
Epoch 2379
Epoch 2379 :: Batch 0/1
Batch Loss = 4.584038207707998
2379, epoch_train_loss=4.584038207707998
Epoch 2380
Epoch 2380 :: Batch 0/1
Batch Loss = 4.584038207707994
2380, epoch_train_loss=4.584038207707994
Epoch 2381
Epoch 2381 :: Batch 0/1
Batch Loss = 4.584038207707992
2381, epoch_train_loss=4.584038207707992
Epoch 2382
Epoch 2382 :: Batch 0/1
Batch Loss = 4.584038207707989
2382, epoch_train_loss=4.584038207707989
Epoch 2383
Epoch 2383 :: Batch 0/1
Batch Loss = 4.5840382077079855
2383, epoch_train_loss=4.5840382077079855
Epoch 2384
Epoch 2384 :: Batch 0/1
Batch Loss = 4.584038207707984
2384, epoch_train_loss=4.584038207707984
Epoch 2385
Epoch 2385 :: Batch 0/1
Batch Loss = 4.58403820770798
2385, epoch_train_loss=4.58403820770798
Epoch 2386
Epoch 2386 :: Batch 0/1
Batch Loss = 4.584038207707978
2386, epoch_train_loss=4.584038207707978
Epoch 2387
Epoch 2387 :: Batch 0/1
Batch Loss = 4.584038207707975
2387, epoch_train_loss=4.584038207707975
Epoch 2388
Epoch 2388 :: Batch 0/1
Batch Loss = 4.584038207707972
2388, epoch_train_loss=4.584038207707972
Epoch 2389
Epoch 2389 :: Batch 0/1
Batch Loss = 4.5840382077079695
2389, epoch_train_loss=4.5840382077079695
Epoch 2390
Epoch 2390 :: Batch 0/1
Batch Loss = 4.584038207707967
2390, epoch_train_loss=4.584038207707967
Epoch 2391
Epoch 2391 :: Batch 0/1
Batch Loss = 4.584038207707963
2391, epoch_train_loss=4.584038207707963
Epoch 2392
Epoch 2392 :: Batch 0/1
Batch Loss = 4.584038207707961
2392, epoch_train_loss=4.584038207707961
Epoch 2393
Epoch 2393 :: Batch 0/1
Batch Loss = 4.584038207707958
2393, epoch_train_loss=4.584038207707958
Epoch 2394
Epoch 2394 :: Batch 0/1
Batch Loss = 4.584038207707955
2394, epoch_train_loss=4.584038207707955
Epoch 2395
Epoch 2395 :: Batch 0/1
Batch Loss = 4.584038207707952
2395, epoch_train_loss=4.584038207707952
Epoch 2396
Epoch 2396 :: Batch 0/1
Batch Loss = 4.58403820770795
2396, epoch_train_loss=4.58403820770795
Epoch 2397
Epoch 2397 :: Batch 0/1
Batch Loss = 4.5840382077079465
2397, epoch_train_loss=4.5840382077079465
Epoch 2398
Epoch 2398 :: Batch 0/1
Batch Loss = 4.584038207707944
2398, epoch_train_loss=4.584038207707944
Epoch 2399
Epoch 2399 :: Batch 0/1
Batch Loss = 4.58403820770794
2399, epoch_train_loss=4.58403820770794
Epoch 2400
Epoch 2400 :: Batch 0/1
Batch Loss = 4.584038207707937
2400, epoch_train_loss=4.584038207707937
Epoch 2401
Epoch 2401 :: Batch 0/1
Batch Loss = 4.584038207707934
2401, epoch_train_loss=4.584038207707934
Epoch 2402
Epoch 2402 :: Batch 0/1
Batch Loss = 4.584038207707931
2402, epoch_train_loss=4.584038207707931
Epoch 2403
Epoch 2403 :: Batch 0/1
Batch Loss = 4.584038207707929
2403, epoch_train_loss=4.584038207707929
Epoch 2404
Epoch 2404 :: Batch 0/1
Batch Loss = 4.584038207707926
2404, epoch_train_loss=4.584038207707926
Epoch 2405
Epoch 2405 :: Batch 0/1
Batch Loss = 4.5840382077079225
2405, epoch_train_loss=4.5840382077079225
Epoch 2406
Epoch 2406 :: Batch 0/1
Batch Loss = 4.58403820770792
2406, epoch_train_loss=4.58403820770792
Epoch 2407
Epoch 2407 :: Batch 0/1
Batch Loss = 4.584038207707917
2407, epoch_train_loss=4.584038207707917
Epoch 2408
Epoch 2408 :: Batch 0/1
Batch Loss = 4.584038207707914
2408, epoch_train_loss=4.584038207707914
Epoch 2409
Epoch 2409 :: Batch 0/1
Batch Loss = 4.584038207707912
2409, epoch_train_loss=4.584038207707912
Epoch 2410
Epoch 2410 :: Batch 0/1
Batch Loss = 4.584038207707908
2410, epoch_train_loss=4.584038207707908
Epoch 2411
Epoch 2411 :: Batch 0/1
Batch Loss = 4.584038207707906
2411, epoch_train_loss=4.584038207707906
Epoch 2412
Epoch 2412 :: Batch 0/1
Batch Loss = 4.584038207707903
2412, epoch_train_loss=4.584038207707903
Epoch 2413
Epoch 2413 :: Batch 0/1
Batch Loss = 4.584038207707899
2413, epoch_train_loss=4.584038207707899
Epoch 2414
Epoch 2414 :: Batch 0/1
Batch Loss = 4.584038207707897
2414, epoch_train_loss=4.584038207707897
Epoch 2415
Epoch 2415 :: Batch 0/1
Batch Loss = 4.584038207707894
2415, epoch_train_loss=4.584038207707894
Epoch 2416
Epoch 2416 :: Batch 0/1
Batch Loss = 4.584038207707891
2416, epoch_train_loss=4.584038207707891
Epoch 2417
Epoch 2417 :: Batch 0/1
Batch Loss = 4.584038207707889
2417, epoch_train_loss=4.584038207707889
Epoch 2418
Epoch 2418 :: Batch 0/1
Batch Loss = 4.584038207707886
2418, epoch_train_loss=4.584038207707886
Epoch 2419
Epoch 2419 :: Batch 0/1
Batch Loss = 4.584038207707882
2419, epoch_train_loss=4.584038207707882
Epoch 2420
Epoch 2420 :: Batch 0/1
Batch Loss = 4.58403820770788
2420, epoch_train_loss=4.58403820770788
Epoch 2421
Epoch 2421 :: Batch 0/1
Batch Loss = 4.584038207707876
2421, epoch_train_loss=4.584038207707876
Epoch 2422
Epoch 2422 :: Batch 0/1
Batch Loss = 4.584038207707874
2422, epoch_train_loss=4.584038207707874
Epoch 2423
Epoch 2423 :: Batch 0/1
Batch Loss = 4.58403820770787
2423, epoch_train_loss=4.58403820770787
Epoch 2424
Epoch 2424 :: Batch 0/1
Batch Loss = 4.5840382077078665
2424, epoch_train_loss=4.5840382077078665
Epoch 2425
Epoch 2425 :: Batch 0/1
Batch Loss = 4.584038207707865
2425, epoch_train_loss=4.584038207707865
Epoch 2426
Epoch 2426 :: Batch 0/1
Batch Loss = 4.584038207707861
2426, epoch_train_loss=4.584038207707861
Epoch 2427
Epoch 2427 :: Batch 0/1
Batch Loss = 4.5840382077078585
2427, epoch_train_loss=4.5840382077078585
Epoch 2428
Epoch 2428 :: Batch 0/1
Batch Loss = 4.584038207707856
2428, epoch_train_loss=4.584038207707856
Epoch 2429
Epoch 2429 :: Batch 0/1
Batch Loss = 4.584038207707853
2429, epoch_train_loss=4.584038207707853
Epoch 2430
Epoch 2430 :: Batch 0/1
Batch Loss = 4.5840382077078505
2430, epoch_train_loss=4.5840382077078505
Epoch 2431
Epoch 2431 :: Batch 0/1
Batch Loss = 4.584038207707847
2431, epoch_train_loss=4.584038207707847
Epoch 2432
Epoch 2432 :: Batch 0/1
Batch Loss = 4.584038207707843
2432, epoch_train_loss=4.584038207707843
Epoch 2433
Epoch 2433 :: Batch 0/1
Batch Loss = 4.584038207707842
2433, epoch_train_loss=4.584038207707842
Epoch 2434
Epoch 2434 :: Batch 0/1
Batch Loss = 4.584038207707839
2434, epoch_train_loss=4.584038207707839
Epoch 2435
Epoch 2435 :: Batch 0/1
Batch Loss = 4.584038207707835
2435, epoch_train_loss=4.584038207707835
Epoch 2436
Epoch 2436 :: Batch 0/1
Batch Loss = 4.584038207707833
2436, epoch_train_loss=4.584038207707833
Epoch 2437
Epoch 2437 :: Batch 0/1
Batch Loss = 4.58403820770783
2437, epoch_train_loss=4.58403820770783
Epoch 2438
Epoch 2438 :: Batch 0/1
Batch Loss = 4.584038207707827
2438, epoch_train_loss=4.584038207707827
Epoch 2439
Epoch 2439 :: Batch 0/1
Batch Loss = 4.584038207707824
2439, epoch_train_loss=4.584038207707824
Epoch 2440
Epoch 2440 :: Batch 0/1
Batch Loss = 4.584038207707821
2440, epoch_train_loss=4.584038207707821
Epoch 2441
Epoch 2441 :: Batch 0/1
Batch Loss = 4.584038207707819
2441, epoch_train_loss=4.584038207707819
Epoch 2442
Epoch 2442 :: Batch 0/1
Batch Loss = 4.584038207707816
2442, epoch_train_loss=4.584038207707816
Epoch 2443
Epoch 2443 :: Batch 0/1
Batch Loss = 4.584038207707812
2443, epoch_train_loss=4.584038207707812
Epoch 2444
Epoch 2444 :: Batch 0/1
Batch Loss = 4.58403820770781
2444, epoch_train_loss=4.58403820770781
Epoch 2445
Epoch 2445 :: Batch 0/1
Batch Loss = 4.584038207707806
2445, epoch_train_loss=4.584038207707806
Epoch 2446
Epoch 2446 :: Batch 0/1
Batch Loss = 4.5840382077078035
2446, epoch_train_loss=4.5840382077078035
Epoch 2447
Epoch 2447 :: Batch 0/1
Batch Loss = 4.5840382077078
2447, epoch_train_loss=4.5840382077078
Epoch 2448
Epoch 2448 :: Batch 0/1
Batch Loss = 4.584038207707797
2448, epoch_train_loss=4.584038207707797
Epoch 2449
Epoch 2449 :: Batch 0/1
Batch Loss = 4.584038207707794
2449, epoch_train_loss=4.584038207707794
Epoch 2450
Epoch 2450 :: Batch 0/1
Batch Loss = 4.584038207707791
2450, epoch_train_loss=4.584038207707791
Epoch 2451
Epoch 2451 :: Batch 0/1
Batch Loss = 4.584038207707788
2451, epoch_train_loss=4.584038207707788
Epoch 2452
Epoch 2452 :: Batch 0/1
Batch Loss = 4.584038207707786
2452, epoch_train_loss=4.584038207707786
Epoch 2453
Epoch 2453 :: Batch 0/1
Batch Loss = 4.584038207707782
2453, epoch_train_loss=4.584038207707782
Epoch 2454
Epoch 2454 :: Batch 0/1
Batch Loss = 4.5840382077077795
2454, epoch_train_loss=4.5840382077077795
Epoch 2455
Epoch 2455 :: Batch 0/1
Batch Loss = 4.584038207707777
2455, epoch_train_loss=4.584038207707777
Epoch 2456
Epoch 2456 :: Batch 0/1
Batch Loss = 4.584038207707773
2456, epoch_train_loss=4.584038207707773
Epoch 2457
Epoch 2457 :: Batch 0/1
Batch Loss = 4.584038207707771
2457, epoch_train_loss=4.584038207707771
Epoch 2458
Epoch 2458 :: Batch 0/1
Batch Loss = 4.584038207707768
2458, epoch_train_loss=4.584038207707768
Epoch 2459
Epoch 2459 :: Batch 0/1
Batch Loss = 4.584038207707764
2459, epoch_train_loss=4.584038207707764
Epoch 2460
Epoch 2460 :: Batch 0/1
Batch Loss = 4.584038207707762
2460, epoch_train_loss=4.584038207707762
Epoch 2461
Epoch 2461 :: Batch 0/1
Batch Loss = 4.584038207707759
2461, epoch_train_loss=4.584038207707759
Epoch 2462
Epoch 2462 :: Batch 0/1
Batch Loss = 4.5840382077077555
2462, epoch_train_loss=4.5840382077077555
Epoch 2463
Epoch 2463 :: Batch 0/1
Batch Loss = 4.584038207707753
2463, epoch_train_loss=4.584038207707753
Epoch 2464
Epoch 2464 :: Batch 0/1
Batch Loss = 4.58403820770775
2464, epoch_train_loss=4.58403820770775
Epoch 2465
Epoch 2465 :: Batch 0/1
Batch Loss = 4.5840382077077475
2465, epoch_train_loss=4.5840382077077475
Epoch 2466
Epoch 2466 :: Batch 0/1
Batch Loss = 4.584038207707745
2466, epoch_train_loss=4.584038207707745
Epoch 2467
Epoch 2467 :: Batch 0/1
Batch Loss = 4.584038207707741
2467, epoch_train_loss=4.584038207707741
Epoch 2468
Epoch 2468 :: Batch 0/1
Batch Loss = 4.584038207707738
2468, epoch_train_loss=4.584038207707738
Epoch 2469
Epoch 2469 :: Batch 0/1
Batch Loss = 4.584038207707736
2469, epoch_train_loss=4.584038207707736
Epoch 2470
Epoch 2470 :: Batch 0/1
Batch Loss = 4.584038207707733
2470, epoch_train_loss=4.584038207707733
Epoch 2471
Epoch 2471 :: Batch 0/1
Batch Loss = 4.584038207707729
2471, epoch_train_loss=4.584038207707729
Epoch 2472
Epoch 2472 :: Batch 0/1
Batch Loss = 4.584038207707726
2472, epoch_train_loss=4.584038207707726
Epoch 2473
Epoch 2473 :: Batch 0/1
Batch Loss = 4.584038207707723
2473, epoch_train_loss=4.584038207707723
Epoch 2474
Epoch 2474 :: Batch 0/1
Batch Loss = 4.58403820770772
2474, epoch_train_loss=4.58403820770772
Epoch 2475
Epoch 2475 :: Batch 0/1
Batch Loss = 4.584038207707717
2475, epoch_train_loss=4.584038207707717
Epoch 2476
Epoch 2476 :: Batch 0/1
Batch Loss = 4.584038207707714
2476, epoch_train_loss=4.584038207707714
Epoch 2477
Epoch 2477 :: Batch 0/1
Batch Loss = 4.584038207707711
2477, epoch_train_loss=4.584038207707711
Epoch 2478
Epoch 2478 :: Batch 0/1
Batch Loss = 4.584038207707708
2478, epoch_train_loss=4.584038207707708
Epoch 2479
Epoch 2479 :: Batch 0/1
Batch Loss = 4.584038207707705
2479, epoch_train_loss=4.584038207707705
Epoch 2480
Epoch 2480 :: Batch 0/1
Batch Loss = 4.584038207707702
2480, epoch_train_loss=4.584038207707702
Epoch 2481
Epoch 2481 :: Batch 0/1
Batch Loss = 4.5840382077076995
2481, epoch_train_loss=4.5840382077076995
Epoch 2482
Epoch 2482 :: Batch 0/1
Batch Loss = 4.584038207707696
2482, epoch_train_loss=4.584038207707696
Epoch 2483
Epoch 2483 :: Batch 0/1
Batch Loss = 4.584038207707693
2483, epoch_train_loss=4.584038207707693
Epoch 2484
Epoch 2484 :: Batch 0/1
Batch Loss = 4.584038207707691
2484, epoch_train_loss=4.584038207707691
Epoch 2485
Epoch 2485 :: Batch 0/1
Batch Loss = 4.584038207707687
2485, epoch_train_loss=4.584038207707687
Epoch 2486
Epoch 2486 :: Batch 0/1
Batch Loss = 4.584038207707684
2486, epoch_train_loss=4.584038207707684
Epoch 2487
Epoch 2487 :: Batch 0/1
Batch Loss = 4.584038207707682
2487, epoch_train_loss=4.584038207707682
Epoch 2488
Epoch 2488 :: Batch 0/1
Batch Loss = 4.584038207707678
2488, epoch_train_loss=4.584038207707678
Epoch 2489
Epoch 2489 :: Batch 0/1
Batch Loss = 4.584038207707676
2489, epoch_train_loss=4.584038207707676
Epoch 2490
Epoch 2490 :: Batch 0/1
Batch Loss = 4.584038207707673
2490, epoch_train_loss=4.584038207707673
Epoch 2491
Epoch 2491 :: Batch 0/1
Batch Loss = 4.584038207707669
2491, epoch_train_loss=4.584038207707669
Epoch 2492
Epoch 2492 :: Batch 0/1
Batch Loss = 4.584038207707668
2492, epoch_train_loss=4.584038207707668
Epoch 2493
Epoch 2493 :: Batch 0/1
Batch Loss = 4.584038207707664
2493, epoch_train_loss=4.584038207707664
Epoch 2494
Epoch 2494 :: Batch 0/1
Batch Loss = 4.5840382077076605
2494, epoch_train_loss=4.5840382077076605
Epoch 2495
Epoch 2495 :: Batch 0/1
Batch Loss = 4.584038207707657
2495, epoch_train_loss=4.584038207707657
Epoch 2496
Epoch 2496 :: Batch 0/1
Batch Loss = 4.584038207707653
2496, epoch_train_loss=4.584038207707653
Epoch 2497
Epoch 2497 :: Batch 0/1
Batch Loss = 4.584038207707652
2497, epoch_train_loss=4.584038207707652
Epoch 2498
Epoch 2498 :: Batch 0/1
Batch Loss = 4.584038207707648
2498, epoch_train_loss=4.584038207707648
Epoch 2499
Epoch 2499 :: Batch 0/1
Batch Loss = 4.5840382077076445
2499, epoch_train_loss=4.5840382077076445
