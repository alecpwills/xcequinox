/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting "B3LYP_WITH_VWN5 = True" in pyscf_conf.py
  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820f340> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820f340> in UKS object of <class 'pyscf.dft.uks.UKS'>
<pyscf.gto.mole.Mole object at 0x7ffec820f340> [['P', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec820d2a0> [['N', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec820fbe0> [['H', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec820d480> [['Li', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec820e6e0> [['O', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec820cc70> [['Cl', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec820fb50> [['Al', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec820d0c0> [['S', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffec820c250> [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec820e350> [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec820d690> [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec820ef80> [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]] 3
<pyscf.gto.mole.Mole object at 0x7ffec820d450> [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]] 3
<pyscf.gto.mole.Mole object at 0x7ffec820fb80> [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec820ed40> [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec820c1c0> [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec820e4a0> [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]] 4
<pyscf.gto.mole.Mole object at 0x7ffec820f8b0> [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec820e4d0> [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec820fa60> [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec820cbb0> [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec820ff10> [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]] 4
<pyscf.gto.mole.Mole object at 0x7ffec820dd80> [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]] 4
<pyscf.gto.mole.Mole object at 0x7ffec820dae0> [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]] 2
<pyscf.gto.mole.Mole object at 0x7ffec820c700> [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]] 5
<pyscf.gto.mole.Mole object at 0x7ffec820cc10> [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]] 3
<pyscf.gto.mole.Mole object at 0x7ffec820dc30> [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]] 5
mol:  [['P', array([0., 0., 0.])]]
converged SCF energy = -341.104145992717  <S^2> = 3.7502984  2S+1 = 4.0001492
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820d2a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820d2a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-3.48053232e-03 -9.22981806e-04 -2.09507924e-03 ... -1.11294850e+01
 -1.11294850e+01 -1.11294850e+01] = SCAN,
rho_a.shape=(6, 6320), rho_b.shape=(6, 6320)
fxc_a.shape=(6320,), fxc_b.shape=(6320,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12640), fxc.shape=(12640,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(2, 6320, 2)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12640, 2)
rho_filt.shape=(12640,)
mol:  [['N', array([0., 0., 0.])]]
converged SCF energy = -54.5289742046675  <S^2> = 3.7524945  2S+1 = 4.0012471
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820fbe0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820fbe0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5016, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.10256797e-03 -5.98013179e-04 -6.71209617e-05 ... -5.03581543e+00
 -5.03581543e+00 -5.03581543e+00] = SCAN,
rho_a.shape=(6, 5016), rho_b.shape=(6, 5016)
fxc_a.shape=(5016,), fxc_b.shape=(5016,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 10032), fxc.shape=(10032,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5016, 22)
rho shape (4, 4, 2, 5016)
rho_filt shape: (5016,)
get descriptors tdrho.shape=(2, 5016, 2)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(10032, 2)
rho_filt.shape=(10032,)
mol:  [['H', array([0., 0., 0.])]]
converged SCF energy = -0.499812984008539  <S^2> = 0.75  2S+1 = 2
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820d480> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820d480> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 7, 7)
ao.shape (10, 2440, 7)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.59173730e-03 -7.67300285e-04 -6.09330303e-05 ... -7.27736608e-01
 -7.27736608e-01 -7.27736608e-01] = SCAN,
rho_a.shape=(6, 2440), rho_b.shape=(6, 2440)
fxc_a.shape=(2440,), fxc_b.shape=(2440,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 2440), fxc.shape=(2440,)
get_data, dm shape = (2, 7, 7)
ao_eval.shape=(4, 2440, 7)
rho shape (4, 4, 2, 2440)
rho_filt shape: (2440,)
get descriptors tdrho.shape=(2, 2440, 2)
mol:  [['Li', array([0., 0., 0.])]]
converged SCF energy = -7.46006188627842  <S^2> = 0.75000049  2S+1 = 2.0000005
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820e6e0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820e6e0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 4592, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-6.71507910e-03 -1.45299376e-03 -1.45299376e-03 ... -1.46930969e-02
 -2.05021258e+00 -2.05021258e+00] = SCAN,
rho_a.shape=(6, 4592), rho_b.shape=(6, 4592)
fxc_a.shape=(4592,), fxc_b.shape=(4592,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 9184), fxc.shape=(9184,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 4592, 22)
rho shape (4, 4, 2, 4592)
rho_filt shape: (4592,)
get descriptors tdrho.shape=(2, 4592, 2)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(9184, 2)
rho_filt.shape=(9184,)
mol:  [['O', array([0., 0., 0.])]]
converged SCF energy = -75.003383488575  <S^2> = 2.0027437  2S+1 = 3.0018286
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820cc70> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820cc70> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5040, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.10003615e-04 -1.41439129e-04 -7.12423491e-06 ... -5.78388625e+00
 -5.78388625e+00 -5.78388625e+00] = SCAN,
rho_a.shape=(6, 5040), rho_b.shape=(6, 5040)
fxc_a.shape=(5040,), fxc_b.shape=(5040,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 10080), fxc.shape=(10080,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5040, 22)
rho shape (4, 4, 2, 5040)
rho_filt shape: (5040,)
get descriptors tdrho.shape=(2, 5040, 2)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(10080, 2)
rho_filt.shape=(10080,)
mol:  [['Cl', array([0., 0., 0.])]]
converged SCF energy = -459.957577120834  <S^2> = 0.75161941  2S+1 = 2.0016188
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820fb50> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820fb50> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6152, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.04138993e-03 -1.04126500e-03 -3.70769624e-04 ... -1.26646370e+01
 -1.26646370e+01 -1.26646370e+01] = SCAN,
rho_a.shape=(6, 6152), rho_b.shape=(6, 6152)
fxc_a.shape=(6152,), fxc_b.shape=(6152,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12304), fxc.shape=(12304,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6152, 30)
rho shape (4, 4, 2, 6152)
rho_filt shape: (6152,)
get descriptors tdrho.shape=(2, 6152, 2)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12304, 2)
rho_filt.shape=(12304,)
mol:  [['Al', array([0., 0., 0.])]]
converged SCF energy = -242.22656098921  <S^2> = 0.75226414  2S+1 = 2.0022629
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820d0c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820d0c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6088, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.39521696e-02 -8.69306060e-03 -4.30027045e-03 ... -1.39756529e-04
 -1.04859832e-03 -7.75066292e-05] = SCAN,
rho_a.shape=(6, 6088), rho_b.shape=(6, 6088)
fxc_a.shape=(6088,), fxc_b.shape=(6088,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12176), fxc.shape=(12176,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6088, 30)
rho shape (4, 4, 2, 6088)
rho_filt shape: (6088,)
get descriptors tdrho.shape=(2, 6088, 2)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12176, 2)
rho_filt.shape=(12176,)
mol:  [['S', array([0., 0., 0.])]]
converged SCF energy = -397.938786807398  <S^2> = 2.0022329  2S+1 = 3.0014882
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820c250> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820c250> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.23872019e-03 -9.01909626e-04 -9.92388620e-04 ... -1.18982463e+01
 -1.18982463e+01 -1.18982463e+01] = SCAN,
rho_a.shape=(6, 6320), rho_b.shape=(6, 6320)
fxc_a.shape=(6320,), fxc_b.shape=(6320,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12640), fxc.shape=(12640,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(2, 6320, 2)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12640, 2)
rho_filt.shape=(12640,)
mol:  [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]]
converged SCF energy = -1.16580491182912  <S^2> = -4.4408921e-16  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820e350> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820e350> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 14, 14)
ao.shape (10, 4776, 14)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.31556415e-04 -9.72662915e-06 -3.66768501e-04 ... -5.54165573e-01
 -5.54165573e-01 -5.54165573e-01] = SCAN,
rho_a.shape=(6, 4776), rho_b.shape=(6, 4776)
fxc_a.shape=(4776,), fxc_b.shape=(4776,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 4776), fxc.shape=(4776,)
get_data, dm shape = (2, 14, 14)
ao_eval.shape=(4, 4776, 14)
rho shape (4, 4, 2, 4776)
rho_filt shape: (4776,)
get descriptors tdrho.shape=(2, 4776, 2)
mol:  [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]]
converged SCF energy = -109.439263799566  <S^2> = 1.5987212e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820d690> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820d690> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9848, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-9.68469910e-05 -9.84742277e-04 -2.59676148e-04 ... -2.39626668e-05
 -2.39626668e-05 -9.68469910e-05] = SCAN,
rho_a.shape=(6, 9848), rho_b.shape=(6, 9848)
fxc_a.shape=(9848,), fxc_b.shape=(9848,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9848), fxc.shape=(9848,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9848, 44)
rho shape (4, 4, 2, 9848)
rho_filt shape: (9848,)
get descriptors tdrho.shape=(2, 9848, 2)
mol:  [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]]
converged SCF energy = -107.339357395734  <S^2> = 4.4408921e-15  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820ef80> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820ef80> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9752, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.04987750e-03 -6.68953858e-04 -8.57556270e-04 ... -1.07485583e-03
 -8.01425698e-01 -8.01425698e-01] = SCAN,
rho_a.shape=(6, 9752), rho_b.shape=(6, 9752)
fxc_a.shape=(9752,), fxc_b.shape=(9752,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9752), fxc.shape=(9752,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9752, 44)
rho shape (4, 4, 2, 9752)
rho_filt shape: (9752,)
get descriptors tdrho.shape=(2, 9752, 2)
mol:  [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]]
converged SCF energy = -93.3377924465129  <S^2> = 4.0072479e-10  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820d450> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820d450> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 12256, 51)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.97917285e-04 -2.54412366e-05 -3.15182243e-05 ... -6.37386500e-01
 -6.37386500e-01 -6.37386500e-01] = SCAN,
rho_a.shape=(6, 12256), rho_b.shape=(6, 12256)
fxc_a.shape=(12256,), fxc_b.shape=(12256,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12256), fxc.shape=(12256,)
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 12256, 51)
rho shape (4, 4, 2, 12256)
rho_filt shape: (12256,)
get descriptors tdrho.shape=(2, 12256, 2)
mol:  [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]]
converged SCF energy = -188.456965322843  <S^2> = 1.0658141e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820fb80> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820fb80> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 66, 66)
ao.shape (10, 14920, 66)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.50217115e-04 -2.07520066e-04 -9.23619896e-04 ... -2.74295208e-06
 -4.27559894e+00 -4.27559894e+00] = SCAN,
rho_a.shape=(6, 14920), rho_b.shape=(6, 14920)
fxc_a.shape=(14920,), fxc_b.shape=(14920,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 14920), fxc.shape=(14920,)
get_data, dm shape = (2, 66, 66)
ao_eval.shape=(4, 14920, 66)
rho shape (4, 4, 2, 14920)
rho_filt shape: (14920,)
get descriptors tdrho.shape=(2, 14920, 2)
mol:  [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]]
converged SCF energy = -920.005608888959  <S^2> = 4.9737992e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820ed40> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820ed40> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12208, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00155834 -0.00091688 -0.00215831 ... -0.00091688 -0.41618507
 -0.41618507] = SCAN,
rho_a.shape=(6, 12208), rho_b.shape=(6, 12208)
fxc_a.shape=(12208,), fxc_b.shape=(12208,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12208), fxc.shape=(12208,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12208, 60)
rho shape (4, 4, 2, 12208)
rho_filt shape: (12208,)
get descriptors tdrho.shape=(2, 12208, 2)
mol:  [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]]
converged SCF energy = -199.394370591172  <S^2> = 1.1546319e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820c1c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820c1c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9824, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.92948614e-04 -1.95198688e-05 -1.16699802e-03 ... -4.89378340e-01
 -4.89378340e-01 -4.89378340e-01] = SCAN,
rho_a.shape=(6, 9824), rho_b.shape=(6, 9824)
fxc_a.shape=(9824,), fxc_b.shape=(9824,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9824), fxc.shape=(9824,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9824, 44)
rho shape (4, 4, 2, 9824)
rho_filt shape: (9824,)
get descriptors tdrho.shape=(2, 9824, 2)
mol:  [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]]
converged SCF energy = -150.214894405799  <S^2> = 1.0018598  2S+1 = 2.2377308
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820e4a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820e4a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9912, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-3.12999834e-04 -5.28727802e-05 -2.62764780e-06 ... -6.59150659e-01
 -6.59150659e-01 -6.59150659e-01] = SCAN,
rho_a.shape=(6, 9912), rho_b.shape=(6, 9912)
fxc_a.shape=(9912,), fxc_b.shape=(9912,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9912), fxc.shape=(9912,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9912, 44)
rho shape (4, 4, 2, 9912)
rho_filt shape: (9912,)
get descriptors tdrho.shape=(2, 9912, 2)
mol:  [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]]
converged SCF energy = -77.2435048346373  <S^2> = 8.8817842e-15  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820f8b0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820f8b0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 58, 58)
ao.shape (10, 15208, 58)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.83270456e-05 -8.83270456e-05 -9.75839850e-04 ... -3.46719667e-05
 -3.31708644e-05 -3.31708644e-05] = SCAN,
rho_a.shape=(6, 15208), rho_b.shape=(6, 15208)
fxc_a.shape=(15208,), fxc_b.shape=(15208,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 15208), fxc.shape=(15208,)
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15208, 58)
rho shape (4, 4, 2, 15208)
rho_filt shape: (15208,)
get descriptors tdrho.shape=(2, 15208, 2)
mol:  [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]]
converged SCF energy = -113.221335689652  <S^2> = 6.5814021e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820e4d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820e4d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 10040, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-5.37000578e-04 -8.55494549e-04 -2.46853288e-03 ... -7.34251999e-01
 -7.34251999e-01 -7.34251999e-01] = SCAN,
rho_a.shape=(6, 10040), rho_b.shape=(6, 10040)
fxc_a.shape=(10040,), fxc_b.shape=(10040,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 10040), fxc.shape=(10040,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 10040, 44)
rho shape (4, 4, 2, 10040)
rho_filt shape: (10040,)
get descriptors tdrho.shape=(2, 10040, 2)
mol:  [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]]
converged SCF energy = -460.624592374078  <S^2> = 7.283063e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820fa60> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820fa60> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 37, 37)
ao.shape (10, 8552, 37)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.38161177e-04 -1.81188367e-05 -2.37300299e-05 ... -2.83738108e+00
 -2.83738108e+00 -2.83738108e+00] = SCAN,
rho_a.shape=(6, 8552), rho_b.shape=(6, 8552)
fxc_a.shape=(8552,), fxc_b.shape=(8552,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 8552), fxc.shape=(8552,)
get_data, dm shape = (2, 37, 37)
ao_eval.shape=(4, 8552, 37)
rho shape (4, 4, 2, 8552)
rho_filt shape: (8552,)
get descriptors tdrho.shape=(2, 8552, 2)
mol:  [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]]
converged SCF energy = -8.04458854018923  <S^2> = 7.5051076e-14  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820cbb0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820cbb0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 29, 29)
ao.shape (10, 6936, 29)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00113445 -0.00118624 -0.00118624 ... -0.48434639 -0.48434639
 -0.48434639] = SCAN,
rho_a.shape=(6, 6936), rho_b.shape=(6, 6936)
fxc_a.shape=(6936,), fxc_b.shape=(6936,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 6936), fxc.shape=(6936,)
get_data, dm shape = (2, 29, 29)
ao_eval.shape=(4, 6936, 29)
rho shape (4, 4, 2, 6936)
rho_filt shape: (6936,)
get descriptors tdrho.shape=(2, 6936, 2)
mol:  [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]]
converged SCF energy = -324.340512506579  <S^2> = 1.5853985e-11  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820ff10> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820ff10> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 11536, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00297935 -0.00297935 -0.00407089 ... -0.00297935 -0.00297935
 -0.00407089] = SCAN,
rho_a.shape=(6, 11536), rho_b.shape=(6, 11536)
fxc_a.shape=(11536,), fxc_b.shape=(11536,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 11536), fxc.shape=(11536,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 11536, 60)
rho shape (4, 4, 2, 11536)
rho_filt shape: (11536,)
get descriptors tdrho.shape=(2, 11536, 2)
mol:  [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]]
converged SCF energy = -1622.57507845814  <S^2> = 8.3844043e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820dd80> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820dd80> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 120, 120)
ao.shape (10, 24512, 120)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.61400996e-04 -4.90484900e-04 -2.56451718e-03 ... -9.59296113e+00
 -9.59296113e+00 -9.59296113e+00] = SCAN,
rho_a.shape=(6, 24512), rho_b.shape=(6, 24512)
fxc_a.shape=(24512,), fxc_b.shape=(24512,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 24512), fxc.shape=(24512,)
get_data, dm shape = (2, 120, 120)
ao_eval.shape=(4, 24512, 120)
rho shape (4, 4, 2, 24512)
rho_filt shape: (24512,)
get descriptors tdrho.shape=(2, 24512, 2)
mol:  [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]]
converged SCF energy = -342.979728469574  <S^2> = 2.5387692e-11  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820dae0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820dae0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 13096, 51)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.28637920e-03 -4.32383300e-04 -3.74057272e-05 ... -1.91722770e+00
 -1.91722770e+00 -1.91722770e+00] = SCAN,
rho_a.shape=(6, 13096), rho_b.shape=(6, 13096)
fxc_a.shape=(13096,), fxc_b.shape=(13096,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 13096), fxc.shape=(13096,)
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 13096, 51)
rho shape (4, 4, 2, 13096)
rho_filt shape: (13096,)
get descriptors tdrho.shape=(2, 13096, 2)
mol:  [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]]
converged SCF energy = -578.565335952491  <S^2> = 1.0034705  2S+1 = 2.2391699
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820c700> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820c700> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12384, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.59396665e-04 -2.59124431e-04 -2.59711371e-04 ... -3.86944059e-01
 -3.86944059e-01 -3.86944059e-01] = SCAN,
rho_a.shape=(6, 12384), rho_b.shape=(6, 12384)
fxc_a.shape=(12384,), fxc_b.shape=(12384,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12384), fxc.shape=(12384,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12384, 60)
rho shape (4, 4, 2, 12384)
rho_filt shape: (12384,)
get descriptors tdrho.shape=(2, 12384, 2)
mol:  [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]]
converged SCF energy = -40.4598214864077  <S^2> = 3.1885605e-13  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820cc10> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820cc10> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 50, 50)
ao.shape (10, 13936, 50)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.68439986e-04 -2.42462569e-04 -1.69927031e-05 ... -2.55230307e-05
 -2.55230307e-05 -2.55230307e-05] = SCAN,
rho_a.shape=(6, 13936), rho_b.shape=(6, 13936)
fxc_a.shape=(13936,), fxc_b.shape=(13936,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 13936), fxc.shape=(13936,)
get_data, dm shape = (2, 50, 50)
ao_eval.shape=(4, 13936, 50)
rho shape (4, 4, 2, 13936)
rho_filt shape: (13936,)
get descriptors tdrho.shape=(2, 13936, 2)
mol:  [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]]
converged SCF energy = -39.0756147483504  <S^2> = 6.2030381e-12  2S+1 = 1
Warning: <pyscf.gto.mole.Mole object at 0x7ffec820dc30> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffec820dc30> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 36, 36)
ao.shape (10, 9656, 36)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-7.67688751e-04 -4.57393214e-05 -2.02834191e-04 ... -1.14928924e+00
 -1.14928924e+00 -1.14928924e+00] = SCAN,
rho_a.shape=(6, 9656), rho_b.shape=(6, 9656)
fxc_a.shape=(9656,), fxc_b.shape=(9656,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9656), fxc.shape=(9656,)
get_data, dm shape = (2, 36, 36)
ao_eval.shape=(4, 9656, 36)
rho shape (4, 4, 2, 9656)
rho_filt shape: (9656,)
get descriptors tdrho.shape=(2, 9656, 2)
mol:  [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]]
converged SCF energy = -291.719272437819  <S^2> = 1.3145041e-11  2S+1 = 1
New DM shape: (2, 58, 58)
ao.shape (10, 15256, 58)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.33850535e-04 -2.34903029e-04 -1.75623665e-05 ... -1.92891112e-05
 -1.92891112e-05 -1.92891112e-05] = SCAN,
rho_a.shape=(6, 15256), rho_b.shape=(6, 15256)
fxc_a.shape=(15256,), fxc_b.shape=(15256,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 15256), fxc.shape=(15256,)
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15256, 58)
rho shape (4, 4, 2, 15256)
rho_filt shape: (15256,)
get descriptors tdrho.shape=(2, 15256, 2)
localnet.spin_scaling: concatenating the data
first data shape = (10940, 2)
concatenated: tdrho.shape=(271699, 2)
PRE NAN FILT: tFxc.shape=(271699,), tdrho.shape=(271699, 2)
nan_filt_rho.shape=(271699,)
nan_filt_fxc.shape=(271699,)
tFxc.shape=(271699,), tdrho.shape=(271699, 2)
inp[0].shape = (271699, 1)
Epoch 0
Epoch 0 :: Batch 0/1
Batch Loss = 5.567271014159502
0, epoch_train_loss=5.567271014159502
Epoch 1
Epoch 1 :: Batch 0/1
Batch Loss = 5.033966692050044
1, epoch_train_loss=5.033966692050044
Epoch 2
Epoch 2 :: Batch 0/1
Batch Loss = 4.554753529649199
2, epoch_train_loss=4.554753529649199
Epoch 3
Epoch 3 :: Batch 0/1
Batch Loss = 4.201209685588494
3, epoch_train_loss=4.201209685588494
Epoch 4
Epoch 4 :: Batch 0/1
Batch Loss = 3.91174199659232
4, epoch_train_loss=3.91174199659232
Epoch 5
Epoch 5 :: Batch 0/1
Batch Loss = 3.8392053985037813
5, epoch_train_loss=3.8392053985037813
Epoch 6
Epoch 6 :: Batch 0/1
Batch Loss = 3.6700268683554174
6, epoch_train_loss=3.6700268683554174
Epoch 7
Epoch 7 :: Batch 0/1
Batch Loss = 3.561274822676202
7, epoch_train_loss=3.561274822676202
Epoch 8
Epoch 8 :: Batch 0/1
Batch Loss = 3.5457740908368667
8, epoch_train_loss=3.5457740908368667
Epoch 9
Epoch 9 :: Batch 0/1
Batch Loss = 3.552604378458118
9, epoch_train_loss=3.552604378458118
Epoch 10
Epoch 10 :: Batch 0/1
Batch Loss = 3.5511483854107406
10, epoch_train_loss=3.5511483854107406
Epoch 11
Epoch 11 :: Batch 0/1
Batch Loss = 3.5276234272632303
11, epoch_train_loss=3.5276234272632303
Epoch 12
Epoch 12 :: Batch 0/1
Batch Loss = 3.510823663669471
12, epoch_train_loss=3.510823663669471
Epoch 13
Epoch 13 :: Batch 0/1
Batch Loss = 3.506294761161934
13, epoch_train_loss=3.506294761161934
Epoch 14
Epoch 14 :: Batch 0/1
Batch Loss = 3.517204787808884
14, epoch_train_loss=3.517204787808884
Epoch 15
Epoch 15 :: Batch 0/1
Batch Loss = 3.527530427345127
15, epoch_train_loss=3.527530427345127
Epoch 16
Epoch 16 :: Batch 0/1
Batch Loss = 3.5107639840218514
16, epoch_train_loss=3.5107639840218514
Epoch 17
Epoch 17 :: Batch 0/1
Batch Loss = 3.4962301986078956
17, epoch_train_loss=3.4962301986078956
Epoch 18
Epoch 18 :: Batch 0/1
Batch Loss = 3.4984574884542505
18, epoch_train_loss=3.4984574884542505
Epoch 19
Epoch 19 :: Batch 0/1
Batch Loss = 3.4913129785402073
19, epoch_train_loss=3.4913129785402073
Epoch 20
Epoch 20 :: Batch 0/1
Batch Loss = 3.4973915087019445
20, epoch_train_loss=3.4973915087019445
Epoch 21
Epoch 21 :: Batch 0/1
Batch Loss = 3.4955498708507537
21, epoch_train_loss=3.4955498708507537
Epoch 22
Epoch 22 :: Batch 0/1
Batch Loss = 3.493021032700775
22, epoch_train_loss=3.493021032700775
Epoch 23
Epoch 23 :: Batch 0/1
Batch Loss = 3.4865046086062903
23, epoch_train_loss=3.4865046086062903
Epoch 24
Epoch 24 :: Batch 0/1
Batch Loss = 3.481423997727751
24, epoch_train_loss=3.481423997727751
Epoch 25
Epoch 25 :: Batch 0/1
Batch Loss = 3.4775037345901882
25, epoch_train_loss=3.4775037345901882
Epoch 26
Epoch 26 :: Batch 0/1
Batch Loss = 3.475930720519389
26, epoch_train_loss=3.475930720519389
Epoch 27
Epoch 27 :: Batch 0/1
Batch Loss = 3.4756331377055747
27, epoch_train_loss=3.4756331377055747
Epoch 28
Epoch 28 :: Batch 0/1
Batch Loss = 3.4756282837262997
28, epoch_train_loss=3.4756282837262997
Epoch 29
Epoch 29 :: Batch 0/1
Batch Loss = 3.4760846646568364
29, epoch_train_loss=3.4760846646568364
Epoch 30
Epoch 30 :: Batch 0/1
Batch Loss = 3.475261516220646
30, epoch_train_loss=3.475261516220646
Epoch 31
Epoch 31 :: Batch 0/1
Batch Loss = 3.474786216922938
31, epoch_train_loss=3.474786216922938
Epoch 32
Epoch 32 :: Batch 0/1
Batch Loss = 3.4732294098153074
32, epoch_train_loss=3.4732294098153074
Epoch 33
Epoch 33 :: Batch 0/1
Batch Loss = 3.472554342275369
33, epoch_train_loss=3.472554342275369
Epoch 34
Epoch 34 :: Batch 0/1
Batch Loss = 3.4716903944039585
34, epoch_train_loss=3.4716903944039585
Epoch 35
Epoch 35 :: Batch 0/1
Batch Loss = 3.471560242713435
35, epoch_train_loss=3.471560242713435
Epoch 36
Epoch 36 :: Batch 0/1
Batch Loss = 3.471644013257491
36, epoch_train_loss=3.471644013257491
Epoch 37
Epoch 37 :: Batch 0/1
Batch Loss = 3.471567506746978
37, epoch_train_loss=3.471567506746978
Epoch 38
Epoch 38 :: Batch 0/1
Batch Loss = 3.4716663597791864
38, epoch_train_loss=3.4716663597791864
Epoch 39
Epoch 39 :: Batch 0/1
Batch Loss = 3.4708508562283638
39, epoch_train_loss=3.4708508562283638
Epoch 40
Epoch 40 :: Batch 0/1
Batch Loss = 3.470485801512792
40, epoch_train_loss=3.470485801512792
Epoch 41
Epoch 41 :: Batch 0/1
Batch Loss = 3.469273158967615
41, epoch_train_loss=3.469273158967615
Epoch 42
Epoch 42 :: Batch 0/1
Batch Loss = 3.4689114884276417
42, epoch_train_loss=3.4689114884276417
Epoch 43
Epoch 43 :: Batch 0/1
Batch Loss = 3.4679939455910507
43, epoch_train_loss=3.4679939455910507
Epoch 44
Epoch 44 :: Batch 0/1
Batch Loss = 3.4679838249288446
44, epoch_train_loss=3.4679838249288446
Epoch 45
Epoch 45 :: Batch 0/1
Batch Loss = 3.46746728204302
45, epoch_train_loss=3.46746728204302
Epoch 46
Epoch 46 :: Batch 0/1
Batch Loss = 3.467445122697242
46, epoch_train_loss=3.467445122697242
Epoch 47
Epoch 47 :: Batch 0/1
Batch Loss = 3.4669208661448883
47, epoch_train_loss=3.4669208661448883
Epoch 48
Epoch 48 :: Batch 0/1
Batch Loss = 3.4665612914175243
48, epoch_train_loss=3.4665612914175243
Epoch 49
Epoch 49 :: Batch 0/1
Batch Loss = 3.466007915290197
49, epoch_train_loss=3.466007915290197
Epoch 50
Epoch 50 :: Batch 0/1
Batch Loss = 3.465496369084703
50, epoch_train_loss=3.465496369084703
Epoch 51
Epoch 51 :: Batch 0/1
Batch Loss = 3.465162745495587
51, epoch_train_loss=3.465162745495587
Epoch 52
Epoch 52 :: Batch 0/1
Batch Loss = 3.4647633237882633
52, epoch_train_loss=3.4647633237882633
Epoch 53
Epoch 53 :: Batch 0/1
Batch Loss = 3.4646593348434065
53, epoch_train_loss=3.4646593348434065
Epoch 54
Epoch 54 :: Batch 0/1
Batch Loss = 3.464299194065655
54, epoch_train_loss=3.464299194065655
Epoch 55
Epoch 55 :: Batch 0/1
Batch Loss = 3.464174314401578
55, epoch_train_loss=3.464174314401578
Epoch 56
Epoch 56 :: Batch 0/1
Batch Loss = 3.4637219709849214
56, epoch_train_loss=3.4637219709849214
Epoch 57
Epoch 57 :: Batch 0/1
Batch Loss = 3.4635155734034826
57, epoch_train_loss=3.4635155734034826
Epoch 58
Epoch 58 :: Batch 0/1
Batch Loss = 3.4631175317793432
58, epoch_train_loss=3.4631175317793432
Epoch 59
Epoch 59 :: Batch 0/1
Batch Loss = 3.4629657930651856
59, epoch_train_loss=3.4629657930651856
Epoch 60
Epoch 60 :: Batch 0/1
Batch Loss = 3.4627618328833485
60, epoch_train_loss=3.4627618328833485
Epoch 61
Epoch 61 :: Batch 0/1
Batch Loss = 3.4626234555237825
61, epoch_train_loss=3.4626234555237825
Epoch 62
Epoch 62 :: Batch 0/1
Batch Loss = 3.4624998059592778
62, epoch_train_loss=3.4624998059592778
Epoch 63
Epoch 63 :: Batch 0/1
Batch Loss = 3.4622557058491483
63, epoch_train_loss=3.4622557058491483
Epoch 64
Epoch 64 :: Batch 0/1
Batch Loss = 3.462112457819477
64, epoch_train_loss=3.462112457819477
Epoch 65
Epoch 65 :: Batch 0/1
Batch Loss = 3.461860884081048
65, epoch_train_loss=3.461860884081048
Epoch 66
Epoch 66 :: Batch 0/1
Batch Loss = 3.4617382626621693
66, epoch_train_loss=3.4617382626621693
Epoch 67
Epoch 67 :: Batch 0/1
Batch Loss = 3.4616013685634965
67, epoch_train_loss=3.4616013685634965
Epoch 68
Epoch 68 :: Batch 0/1
Batch Loss = 3.4614851920372494
68, epoch_train_loss=3.4614851920372494
Epoch 69
Epoch 69 :: Batch 0/1
Batch Loss = 3.4614080516751913
69, epoch_train_loss=3.4614080516751913
Epoch 70
Epoch 70 :: Batch 0/1
Batch Loss = 3.461259638644584
70, epoch_train_loss=3.461259638644584
Epoch 71
Epoch 71 :: Batch 0/1
Batch Loss = 3.4611755889711113
71, epoch_train_loss=3.4611755889711113
Epoch 72
Epoch 72 :: Batch 0/1
Batch Loss = 3.4610562927610373
72, epoch_train_loss=3.4610562927610373
Epoch 73
Epoch 73 :: Batch 0/1
Batch Loss = 3.4609711670679975
73, epoch_train_loss=3.4609711670679975
Epoch 74
Epoch 74 :: Batch 0/1
Batch Loss = 3.46091707963073
74, epoch_train_loss=3.46091707963073
Epoch 75
Epoch 75 :: Batch 0/1
Batch Loss = 3.460830629730286
75, epoch_train_loss=3.460830629730286
Epoch 76
Epoch 76 :: Batch 0/1
Batch Loss = 3.46078150471407
76, epoch_train_loss=3.46078150471407
Epoch 77
Epoch 77 :: Batch 0/1
Batch Loss = 3.4607080909987245
77, epoch_train_loss=3.4607080909987245
Epoch 78
Epoch 78 :: Batch 0/1
Batch Loss = 3.4606404281841265
78, epoch_train_loss=3.4606404281841265
Epoch 79
Epoch 79 :: Batch 0/1
Batch Loss = 3.460604613477986
79, epoch_train_loss=3.460604613477986
Epoch 80
Epoch 80 :: Batch 0/1
Batch Loss = 3.460553356457929
80, epoch_train_loss=3.460553356457929
Epoch 81
Epoch 81 :: Batch 0/1
Batch Loss = 3.4605245300368215
81, epoch_train_loss=3.4605245300368215
Epoch 82
Epoch 82 :: Batch 0/1
Batch Loss = 3.460489518887231
82, epoch_train_loss=3.460489518887231
Epoch 83
Epoch 83 :: Batch 0/1
Batch Loss = 3.460437679615914
83, epoch_train_loss=3.460437679615914
Epoch 84
Epoch 84 :: Batch 0/1
Batch Loss = 3.460401527251782
84, epoch_train_loss=3.460401527251782
Epoch 85
Epoch 85 :: Batch 0/1
Batch Loss = 3.4603572762075423
85, epoch_train_loss=3.4603572762075423
Epoch 86
Epoch 86 :: Batch 0/1
Batch Loss = 3.4603197175524487
86, epoch_train_loss=3.4603197175524487
Epoch 87
Epoch 87 :: Batch 0/1
Batch Loss = 3.460295332615819
87, epoch_train_loss=3.460295332615819
Epoch 88
Epoch 88 :: Batch 0/1
Batch Loss = 3.4602598549984624
88, epoch_train_loss=3.4602598549984624
Epoch 89
Epoch 89 :: Batch 0/1
Batch Loss = 3.460227633003775
89, epoch_train_loss=3.460227633003775
Epoch 90
Epoch 90 :: Batch 0/1
Batch Loss = 3.460199637202744
90, epoch_train_loss=3.460199637202744
Epoch 91
Epoch 91 :: Batch 0/1
Batch Loss = 3.460165979585223
91, epoch_train_loss=3.460165979585223
Epoch 92
Epoch 92 :: Batch 0/1
Batch Loss = 3.4601406518833513
92, epoch_train_loss=3.4601406518833513
Epoch 93
Epoch 93 :: Batch 0/1
Batch Loss = 3.4601193266504815
93, epoch_train_loss=3.4601193266504815
Epoch 94
Epoch 94 :: Batch 0/1
Batch Loss = 3.460092062707344
94, epoch_train_loss=3.460092062707344
Epoch 95
Epoch 95 :: Batch 0/1
Batch Loss = 3.460068057938978
95, epoch_train_loss=3.460068057938978
Epoch 96
Epoch 96 :: Batch 0/1
Batch Loss = 3.4600461447032917
96, epoch_train_loss=3.4600461447032917
Epoch 97
Epoch 97 :: Batch 0/1
Batch Loss = 3.4600221813308902
97, epoch_train_loss=3.4600221813308902
Epoch 98
Epoch 98 :: Batch 0/1
Batch Loss = 3.460003203250659
98, epoch_train_loss=3.460003203250659
Epoch 99
Epoch 99 :: Batch 0/1
Batch Loss = 3.459986854267819
99, epoch_train_loss=3.459986854267819
Epoch 100
Epoch 100 :: Batch 0/1
Batch Loss = 3.459966780050675
100, epoch_train_loss=3.459966780050675
Epoch 101
Epoch 101 :: Batch 0/1
Batch Loss = 3.459947048948622
101, epoch_train_loss=3.459947048948622
Epoch 102
Epoch 102 :: Batch 0/1
Batch Loss = 3.4599295337492575
102, epoch_train_loss=3.4599295337492575
Epoch 103
Epoch 103 :: Batch 0/1
Batch Loss = 3.459911272838145
103, epoch_train_loss=3.459911272838145
Epoch 104
Epoch 104 :: Batch 0/1
Batch Loss = 3.4598941845340594
104, epoch_train_loss=3.4598941845340594
Epoch 105
Epoch 105 :: Batch 0/1
Batch Loss = 3.4598795661487616
105, epoch_train_loss=3.4598795661487616
Epoch 106
Epoch 106 :: Batch 0/1
Batch Loss = 3.459864003707069
106, epoch_train_loss=3.459864003707069
Epoch 107
Epoch 107 :: Batch 0/1
Batch Loss = 3.459847403411579
107, epoch_train_loss=3.459847403411579
Epoch 108
Epoch 108 :: Batch 0/1
Batch Loss = 3.459832503308471
108, epoch_train_loss=3.459832503308471
Epoch 109
Epoch 109 :: Batch 0/1
Batch Loss = 3.4598186231412162
109, epoch_train_loss=3.4598186231412162
Epoch 110
Epoch 110 :: Batch 0/1
Batch Loss = 3.4598042081428044
110, epoch_train_loss=3.4598042081428044
Epoch 111
Epoch 111 :: Batch 0/1
Batch Loss = 3.4597900939150477
111, epoch_train_loss=3.4597900939150477
Epoch 112
Epoch 112 :: Batch 0/1
Batch Loss = 3.4597766593649366
112, epoch_train_loss=3.4597766593649366
Epoch 113
Epoch 113 :: Batch 0/1
Batch Loss = 3.459762880214806
113, epoch_train_loss=3.459762880214806
Epoch 114
Epoch 114 :: Batch 0/1
Batch Loss = 3.4597490093713184
114, epoch_train_loss=3.4597490093713184
Epoch 115
Epoch 115 :: Batch 0/1
Batch Loss = 3.4597360116232383
115, epoch_train_loss=3.4597360116232383
Epoch 116
Epoch 116 :: Batch 0/1
Batch Loss = 3.4597232991340543
116, epoch_train_loss=3.4597232991340543
Epoch 117
Epoch 117 :: Batch 0/1
Batch Loss = 3.4597098438981866
117, epoch_train_loss=3.4597098438981866
Epoch 118
Epoch 118 :: Batch 0/1
Batch Loss = 3.459696071503007
118, epoch_train_loss=3.459696071503007
Epoch 119
Epoch 119 :: Batch 0/1
Batch Loss = 3.459682766480857
119, epoch_train_loss=3.459682766480857
Epoch 120
Epoch 120 :: Batch 0/1
Batch Loss = 3.4596697384519737
120, epoch_train_loss=3.4596697384519737
Epoch 121
Epoch 121 :: Batch 0/1
Batch Loss = 3.459656540799046
121, epoch_train_loss=3.459656540799046
Epoch 122
Epoch 122 :: Batch 0/1
Batch Loss = 3.4596432567242887
122, epoch_train_loss=3.4596432567242887
Epoch 123
Epoch 123 :: Batch 0/1
Batch Loss = 3.4596301335589783
123, epoch_train_loss=3.4596301335589783
Epoch 124
Epoch 124 :: Batch 0/1
Batch Loss = 3.459617069428556
124, epoch_train_loss=3.459617069428556
Epoch 125
Epoch 125 :: Batch 0/1
Batch Loss = 3.4596039282951168
125, epoch_train_loss=3.4596039282951168
Epoch 126
Epoch 126 :: Batch 0/1
Batch Loss = 3.459590845546122
126, epoch_train_loss=3.459590845546122
Epoch 127
Epoch 127 :: Batch 0/1
Batch Loss = 3.459577935160129
127, epoch_train_loss=3.459577935160129
Epoch 128
Epoch 128 :: Batch 0/1
Batch Loss = 3.459565084078558
128, epoch_train_loss=3.459565084078558
Epoch 129
Epoch 129 :: Batch 0/1
Batch Loss = 3.459552166889497
129, epoch_train_loss=3.459552166889497
Epoch 130
Epoch 130 :: Batch 0/1
Batch Loss = 3.4595392402222607
130, epoch_train_loss=3.4595392402222607
Epoch 131
Epoch 131 :: Batch 0/1
Batch Loss = 3.4595264448354195
131, epoch_train_loss=3.4595264448354195
Epoch 132
Epoch 132 :: Batch 0/1
Batch Loss = 3.4595137630937987
132, epoch_train_loss=3.4595137630937987
Epoch 133
Epoch 133 :: Batch 0/1
Batch Loss = 3.459501065823662
133, epoch_train_loss=3.459501065823662
Epoch 134
Epoch 134 :: Batch 0/1
Batch Loss = 3.4594883051194145
134, epoch_train_loss=3.4594883051194145
Epoch 135
Epoch 135 :: Batch 0/1
Batch Loss = 3.4594755568382913
135, epoch_train_loss=3.4594755568382913
Epoch 136
Epoch 136 :: Batch 0/1
Batch Loss = 3.4594629109840165
136, epoch_train_loss=3.4594629109840165
Epoch 137
Epoch 137 :: Batch 0/1
Batch Loss = 3.459450354397928
137, epoch_train_loss=3.459450354397928
Epoch 138
Epoch 138 :: Batch 0/1
Batch Loss = 3.4594378089514377
138, epoch_train_loss=3.4594378089514377
Epoch 139
Epoch 139 :: Batch 0/1
Batch Loss = 3.459425233807301
139, epoch_train_loss=3.459425233807301
Epoch 140
Epoch 140 :: Batch 0/1
Batch Loss = 3.459412649100169
140, epoch_train_loss=3.459412649100169
Epoch 141
Epoch 141 :: Batch 0/1
Batch Loss = 3.4594000956092272
141, epoch_train_loss=3.4594000956092272
Epoch 142
Epoch 142 :: Batch 0/1
Batch Loss = 3.4593875768309004
142, epoch_train_loss=3.4593875768309004
Epoch 143
Epoch 143 :: Batch 0/1
Batch Loss = 3.4593750717263707
143, epoch_train_loss=3.4593750717263707
Epoch 144
Epoch 144 :: Batch 0/1
Batch Loss = 3.4593625637134915
144, epoch_train_loss=3.4593625637134915
Epoch 145
Epoch 145 :: Batch 0/1
Batch Loss = 3.459350048764141
145, epoch_train_loss=3.459350048764141
Epoch 146
Epoch 146 :: Batch 0/1
Batch Loss = 3.459337522773098
146, epoch_train_loss=3.459337522773098
Epoch 147
Epoch 147 :: Batch 0/1
Batch Loss = 3.4593249738638683
147, epoch_train_loss=3.4593249738638683
Epoch 148
Epoch 148 :: Batch 0/1
Batch Loss = 3.459312397813187
148, epoch_train_loss=3.459312397813187
Epoch 149
Epoch 149 :: Batch 0/1
Batch Loss = 3.4592998039486527
149, epoch_train_loss=3.4592998039486527
Epoch 150
Epoch 150 :: Batch 0/1
Batch Loss = 3.459287204409581
150, epoch_train_loss=3.459287204409581
Epoch 151
Epoch 151 :: Batch 0/1
Batch Loss = 3.459274596140076
151, epoch_train_loss=3.459274596140076
Epoch 152
Epoch 152 :: Batch 0/1
Batch Loss = 3.4592619610024706
152, epoch_train_loss=3.4592619610024706
Epoch 153
Epoch 153 :: Batch 0/1
Batch Loss = 3.4592492847601033
153, epoch_train_loss=3.4592492847601033
Epoch 154
Epoch 154 :: Batch 0/1
Batch Loss = 3.4592365654841215
154, epoch_train_loss=3.4592365654841215
Epoch 155
Epoch 155 :: Batch 0/1
Batch Loss = 3.4592238075664214
155, epoch_train_loss=3.4592238075664214
Epoch 156
Epoch 156 :: Batch 0/1
Batch Loss = 3.4592110086096888
156, epoch_train_loss=3.4592110086096888
Epoch 157
Epoch 157 :: Batch 0/1
Batch Loss = 3.4591981596183126
157, epoch_train_loss=3.4591981596183126
Epoch 158
Epoch 158 :: Batch 0/1
Batch Loss = 3.4591852541893346
158, epoch_train_loss=3.4591852541893346
Epoch 159
Epoch 159 :: Batch 0/1
Batch Loss = 3.4591722921713637
159, epoch_train_loss=3.4591722921713637
Epoch 160
Epoch 160 :: Batch 0/1
Batch Loss = 3.459159273327129
160, epoch_train_loss=3.459159273327129
Epoch 161
Epoch 161 :: Batch 0/1
Batch Loss = 3.4591461916463566
161, epoch_train_loss=3.4591461916463566
Epoch 162
Epoch 162 :: Batch 0/1
Batch Loss = 3.45913303837556
162, epoch_train_loss=3.45913303837556
Epoch 163
Epoch 163 :: Batch 0/1
Batch Loss = 3.459119808253561
163, epoch_train_loss=3.459119808253561
Epoch 164
Epoch 164 :: Batch 0/1
Batch Loss = 3.4591065002771235
164, epoch_train_loss=3.4591065002771235
Epoch 165
Epoch 165 :: Batch 0/1
Batch Loss = 3.4590931117117916
165, epoch_train_loss=3.4590931117117916
Epoch 166
Epoch 166 :: Batch 0/1
Batch Loss = 3.4590796365441507
166, epoch_train_loss=3.4590796365441507
Epoch 167
Epoch 167 :: Batch 0/1
Batch Loss = 3.459066068248173
167, epoch_train_loss=3.459066068248173
Epoch 168
Epoch 168 :: Batch 0/1
Batch Loss = 3.4590524057811067
168, epoch_train_loss=3.4590524057811067
Epoch 169
Epoch 169 :: Batch 0/1
Batch Loss = 3.459038653519477
169, epoch_train_loss=3.459038653519477
Epoch 170
Epoch 170 :: Batch 0/1
Batch Loss = 3.4590248252101214
170, epoch_train_loss=3.4590248252101214
Epoch 171
Epoch 171 :: Batch 0/1
Batch Loss = 3.4590109545830607
171, epoch_train_loss=3.4590109545830607
Epoch 172
Epoch 172 :: Batch 0/1
Batch Loss = 3.4589971326968474
172, epoch_train_loss=3.4589971326968474
Epoch 173
Epoch 173 :: Batch 0/1
Batch Loss = 3.4589835845014227
173, epoch_train_loss=3.4589835845014227
Epoch 174
Epoch 174 :: Batch 0/1
Batch Loss = 3.4589708641347254
174, epoch_train_loss=3.4589708641347254
Epoch 175
Epoch 175 :: Batch 0/1
Batch Loss = 3.4589603989776503
175, epoch_train_loss=3.4589603989776503
Epoch 176
Epoch 176 :: Batch 0/1
Batch Loss = 3.4589557256225225
176, epoch_train_loss=3.4589557256225225
Epoch 177
Epoch 177 :: Batch 0/1
Batch Loss = 3.4589668116177363
177, epoch_train_loss=3.4589668116177363
Epoch 178
Epoch 178 :: Batch 0/1
Batch Loss = 3.459018050369182
178, epoch_train_loss=3.459018050369182
Epoch 179
Epoch 179 :: Batch 0/1
Batch Loss = 3.459187025737627
179, epoch_train_loss=3.459187025737627
Epoch 180
Epoch 180 :: Batch 0/1
Batch Loss = 3.459644265595714
180, epoch_train_loss=3.459644265595714
Epoch 181
Epoch 181 :: Batch 0/1
Batch Loss = 3.4610452126059674
181, epoch_train_loss=3.4610452126059674
Epoch 182
Epoch 182 :: Batch 0/1
Batch Loss = 3.4641701211886575
182, epoch_train_loss=3.4641701211886575
Epoch 183
Epoch 183 :: Batch 0/1
Batch Loss = 3.4734483304315193
183, epoch_train_loss=3.4734483304315193
Epoch 184
Epoch 184 :: Batch 0/1
Batch Loss = 3.478973730635806
184, epoch_train_loss=3.478973730635806
Epoch 185
Epoch 185 :: Batch 0/1
Batch Loss = 3.486949680434764
185, epoch_train_loss=3.486949680434764
Epoch 186
Epoch 186 :: Batch 0/1
Batch Loss = 3.463068474279467
186, epoch_train_loss=3.463068474279467
Epoch 187
Epoch 187 :: Batch 0/1
Batch Loss = 3.466079140776727
187, epoch_train_loss=3.466079140776727
Epoch 188
Epoch 188 :: Batch 0/1
Batch Loss = 3.4806431665329236
188, epoch_train_loss=3.4806431665329236
Epoch 189
Epoch 189 :: Batch 0/1
Batch Loss = 3.4614767257644585
189, epoch_train_loss=3.4614767257644585
Epoch 190
Epoch 190 :: Batch 0/1
Batch Loss = 3.4660002008930197
190, epoch_train_loss=3.4660002008930197
Epoch 191
Epoch 191 :: Batch 0/1
Batch Loss = 3.478406173274009
191, epoch_train_loss=3.478406173274009
Epoch 192
Epoch 192 :: Batch 0/1
Batch Loss = 3.4599018888888353
192, epoch_train_loss=3.4599018888888353
Epoch 193
Epoch 193 :: Batch 0/1
Batch Loss = 3.4695978000548973
193, epoch_train_loss=3.4695978000548973
Epoch 194
Epoch 194 :: Batch 0/1
Batch Loss = 3.474939509849418
194, epoch_train_loss=3.474939509849418
Epoch 195
Epoch 195 :: Batch 0/1
Batch Loss = 3.4600636230653414
195, epoch_train_loss=3.4600636230653414
Epoch 196
Epoch 196 :: Batch 0/1
Batch Loss = 3.4779222062919932
196, epoch_train_loss=3.4779222062919932
Epoch 197
Epoch 197 :: Batch 0/1
Batch Loss = 3.4681550887591897
197, epoch_train_loss=3.4681550887591897
Epoch 198
Epoch 198 :: Batch 0/1
Batch Loss = 3.4650766448166506
198, epoch_train_loss=3.4650766448166506
Epoch 199
Epoch 199 :: Batch 0/1
Batch Loss = 3.474242462804398
199, epoch_train_loss=3.474242462804398
Epoch 200
Epoch 200 :: Batch 0/1
Batch Loss = 3.4594360665076462
200, epoch_train_loss=3.4594360665076462
Epoch 201
Epoch 201 :: Batch 0/1
Batch Loss = 3.467549230104572
201, epoch_train_loss=3.467549230104572
Epoch 202
Epoch 202 :: Batch 0/1
Batch Loss = 3.4615668377908246
202, epoch_train_loss=3.4615668377908246
Epoch 203
Epoch 203 :: Batch 0/1
Batch Loss = 3.4619641782575408
203, epoch_train_loss=3.4619641782575408
Epoch 204
Epoch 204 :: Batch 0/1
Batch Loss = 3.4644045219705726
204, epoch_train_loss=3.4644045219705726
Epoch 205
Epoch 205 :: Batch 0/1
Batch Loss = 3.4595814618248246
205, epoch_train_loss=3.4595814618248246
Epoch 206
Epoch 206 :: Batch 0/1
Batch Loss = 3.465163470466433
206, epoch_train_loss=3.465163470466433
Epoch 207
Epoch 207 :: Batch 0/1
Batch Loss = 3.4589088491185698
207, epoch_train_loss=3.4589088491185698
Epoch 208
Epoch 208 :: Batch 0/1
Batch Loss = 3.462759045331886
208, epoch_train_loss=3.462759045331886
Epoch 209
Epoch 209 :: Batch 0/1
Batch Loss = 3.4594926955574254
209, epoch_train_loss=3.4594926955574254
Epoch 210
Epoch 210 :: Batch 0/1
Batch Loss = 3.4607822187939674
210, epoch_train_loss=3.4607822187939674
Epoch 211
Epoch 211 :: Batch 0/1
Batch Loss = 3.4607303375262357
211, epoch_train_loss=3.4607303375262357
Epoch 212
Epoch 212 :: Batch 0/1
Batch Loss = 3.459379630270226
212, epoch_train_loss=3.459379630270226
Epoch 213
Epoch 213 :: Batch 0/1
Batch Loss = 3.4611345409181355
213, epoch_train_loss=3.4611345409181355
Epoch 214
Epoch 214 :: Batch 0/1
Batch Loss = 3.4585099501492573
214, epoch_train_loss=3.4585099501492573
Epoch 215
Epoch 215 :: Batch 0/1
Batch Loss = 3.46061795985847
215, epoch_train_loss=3.46061795985847
Epoch 216
Epoch 216 :: Batch 0/1
Batch Loss = 3.4584586012199723
216, epoch_train_loss=3.4584586012199723
Epoch 217
Epoch 217 :: Batch 0/1
Batch Loss = 3.460094722792887
217, epoch_train_loss=3.460094722792887
Epoch 218
Epoch 218 :: Batch 0/1
Batch Loss = 3.4588277123297293
218, epoch_train_loss=3.4588277123297293
Epoch 219
Epoch 219 :: Batch 0/1
Batch Loss = 3.459255301590659
219, epoch_train_loss=3.459255301590659
Epoch 220
Epoch 220 :: Batch 0/1
Batch Loss = 3.459122476911904
220, epoch_train_loss=3.459122476911904
Epoch 221
Epoch 221 :: Batch 0/1
Batch Loss = 3.4585500710324615
221, epoch_train_loss=3.4585500710324615
Epoch 222
Epoch 222 :: Batch 0/1
Batch Loss = 3.4592342944769925
222, epoch_train_loss=3.4592342944769925
Epoch 223
Epoch 223 :: Batch 0/1
Batch Loss = 3.45826649692898
223, epoch_train_loss=3.45826649692898
Epoch 224
Epoch 224 :: Batch 0/1
Batch Loss = 3.4591386413551914
224, epoch_train_loss=3.4591386413551914
Epoch 225
Epoch 225 :: Batch 0/1
Batch Loss = 3.458229981705903
225, epoch_train_loss=3.458229981705903
Epoch 226
Epoch 226 :: Batch 0/1
Batch Loss = 3.4587469339453345
226, epoch_train_loss=3.4587469339453345
Epoch 227
Epoch 227 :: Batch 0/1
Batch Loss = 3.4583110280765905
227, epoch_train_loss=3.4583110280765905
Epoch 228
Epoch 228 :: Batch 0/1
Batch Loss = 3.458359016599062
228, epoch_train_loss=3.458359016599062
Epoch 229
Epoch 229 :: Batch 0/1
Batch Loss = 3.458415245308936
229, epoch_train_loss=3.458415245308936
Epoch 230
Epoch 230 :: Batch 0/1
Batch Loss = 3.4580717199048308
230, epoch_train_loss=3.4580717199048308
Epoch 231
Epoch 231 :: Batch 0/1
Batch Loss = 3.458390541017801
231, epoch_train_loss=3.458390541017801
Epoch 232
Epoch 232 :: Batch 0/1
Batch Loss = 3.4579143703373463
232, epoch_train_loss=3.4579143703373463
Epoch 233
Epoch 233 :: Batch 0/1
Batch Loss = 3.458202977462501
233, epoch_train_loss=3.458202977462501
Epoch 234
Epoch 234 :: Batch 0/1
Batch Loss = 3.457872303213631
234, epoch_train_loss=3.457872303213631
Epoch 235
Epoch 235 :: Batch 0/1
Batch Loss = 3.4579644706362815
235, epoch_train_loss=3.4579644706362815
Epoch 236
Epoch 236 :: Batch 0/1
Batch Loss = 3.457868334395533
236, epoch_train_loss=3.457868334395533
Epoch 237
Epoch 237 :: Batch 0/1
Batch Loss = 3.4577285963598783
237, epoch_train_loss=3.4577285963598783
Epoch 238
Epoch 238 :: Batch 0/1
Batch Loss = 3.4578107643127654
238, epoch_train_loss=3.4578107643127654
Epoch 239
Epoch 239 :: Batch 0/1
Batch Loss = 3.457550529430111
239, epoch_train_loss=3.457550529430111
Epoch 240
Epoch 240 :: Batch 0/1
Batch Loss = 3.457654909761742
240, epoch_train_loss=3.457654909761742
Epoch 241
Epoch 241 :: Batch 0/1
Batch Loss = 3.457440979619785
241, epoch_train_loss=3.457440979619785
Epoch 242
Epoch 242 :: Batch 0/1
Batch Loss = 3.4574446299345314
242, epoch_train_loss=3.4574446299345314
Epoch 243
Epoch 243 :: Batch 0/1
Batch Loss = 3.457347915973784
243, epoch_train_loss=3.457347915973784
Epoch 244
Epoch 244 :: Batch 0/1
Batch Loss = 3.457217730982399
244, epoch_train_loss=3.457217730982399
Epoch 245
Epoch 245 :: Batch 0/1
Batch Loss = 3.457210336294239
245, epoch_train_loss=3.457210336294239
Epoch 246
Epoch 246 :: Batch 0/1
Batch Loss = 3.4570159052895035
246, epoch_train_loss=3.4570159052895035
Epoch 247
Epoch 247 :: Batch 0/1
Batch Loss = 3.4569958447555518
247, epoch_train_loss=3.4569958447555518
Epoch 248
Epoch 248 :: Batch 0/1
Batch Loss = 3.456834813103662
248, epoch_train_loss=3.456834813103662
Epoch 249
Epoch 249 :: Batch 0/1
Batch Loss = 3.4567302862029092
249, epoch_train_loss=3.4567302862029092
Epoch 250
Epoch 250 :: Batch 0/1
Batch Loss = 3.4566296916280947
250, epoch_train_loss=3.4566296916280947
Epoch 251
Epoch 251 :: Batch 0/1
Batch Loss = 3.456447322003528
251, epoch_train_loss=3.456447322003528
Epoch 252
Epoch 252 :: Batch 0/1
Batch Loss = 3.4563563675194233
252, epoch_train_loss=3.4563563675194233
Epoch 253
Epoch 253 :: Batch 0/1
Batch Loss = 3.4561572410046573
253, epoch_train_loss=3.4561572410046573
Epoch 254
Epoch 254 :: Batch 0/1
Batch Loss = 3.456002194748305
254, epoch_train_loss=3.456002194748305
Epoch 255
Epoch 255 :: Batch 0/1
Batch Loss = 3.455826236848909
255, epoch_train_loss=3.455826236848909
Epoch 256
Epoch 256 :: Batch 0/1
Batch Loss = 3.4555922773356986
256, epoch_train_loss=3.4555922773356986
Epoch 257
Epoch 257 :: Batch 0/1
Batch Loss = 3.4553997452597573
257, epoch_train_loss=3.4553997452597573
Epoch 258
Epoch 258 :: Batch 0/1
Batch Loss = 3.4551304912678367
258, epoch_train_loss=3.4551304912678367
Epoch 259
Epoch 259 :: Batch 0/1
Batch Loss = 3.4548584419630988
259, epoch_train_loss=3.4548584419630988
Epoch 260
Epoch 260 :: Batch 0/1
Batch Loss = 3.4545670154883257
260, epoch_train_loss=3.4545670154883257
Epoch 261
Epoch 261 :: Batch 0/1
Batch Loss = 3.45420320446073
261, epoch_train_loss=3.45420320446073
Epoch 262
Epoch 262 :: Batch 0/1
Batch Loss = 3.453834960762408
262, epoch_train_loss=3.453834960762408
Epoch 263
Epoch 263 :: Batch 0/1
Batch Loss = 3.4534007297251605
263, epoch_train_loss=3.4534007297251605
Epoch 264
Epoch 264 :: Batch 0/1
Batch Loss = 3.45289522701535
264, epoch_train_loss=3.45289522701535
Epoch 265
Epoch 265 :: Batch 0/1
Batch Loss = 3.452349499952116
265, epoch_train_loss=3.452349499952116
Epoch 266
Epoch 266 :: Batch 0/1
Batch Loss = 3.4517007174233965
266, epoch_train_loss=3.4517007174233965
Epoch 267
Epoch 267 :: Batch 0/1
Batch Loss = 3.4509469911403623
267, epoch_train_loss=3.4509469911403623
Epoch 268
Epoch 268 :: Batch 0/1
Batch Loss = 3.4500944493695767
268, epoch_train_loss=3.4500944493695767
Epoch 269
Epoch 269 :: Batch 0/1
Batch Loss = 3.44908192268674
269, epoch_train_loss=3.44908192268674
Epoch 270
Epoch 270 :: Batch 0/1
Batch Loss = 3.447878626928705
270, epoch_train_loss=3.447878626928705
Epoch 271
Epoch 271 :: Batch 0/1
Batch Loss = 3.4464644315057336
271, epoch_train_loss=3.4464644315057336
Epoch 272
Epoch 272 :: Batch 0/1
Batch Loss = 3.444775142595779
272, epoch_train_loss=3.444775142595779
Epoch 273
Epoch 273 :: Batch 0/1
Batch Loss = 3.442728233769181
273, epoch_train_loss=3.442728233769181
Epoch 274
Epoch 274 :: Batch 0/1
Batch Loss = 3.440222301225743
274, epoch_train_loss=3.440222301225743
Epoch 275
Epoch 275 :: Batch 0/1
Batch Loss = 3.4371470211549107
275, epoch_train_loss=3.4371470211549107
Epoch 276
Epoch 276 :: Batch 0/1
Batch Loss = 3.4333309292665772
276, epoch_train_loss=3.4333309292665772
Epoch 277
Epoch 277 :: Batch 0/1
Batch Loss = 3.428573324913708
277, epoch_train_loss=3.428573324913708
Epoch 278
Epoch 278 :: Batch 0/1
Batch Loss = 3.422621409422181
278, epoch_train_loss=3.422621409422181
Epoch 279
Epoch 279 :: Batch 0/1
Batch Loss = 3.41575327586203
279, epoch_train_loss=3.41575327586203
Epoch 280
Epoch 280 :: Batch 0/1
Batch Loss = 3.4166722147670785
280, epoch_train_loss=3.4166722147670785
Epoch 281
Epoch 281 :: Batch 0/1
Batch Loss = 3.5556427794925516
281, epoch_train_loss=3.5556427794925516
Epoch 282
Epoch 282 :: Batch 0/1
Batch Loss = 3.6656401405365266
282, epoch_train_loss=3.6656401405365266
Epoch 283
Epoch 283 :: Batch 0/1
Batch Loss = 3.6199032887908302
283, epoch_train_loss=3.6199032887908302
Epoch 284
Epoch 284 :: Batch 0/1
Batch Loss = 3.702628175869159
284, epoch_train_loss=3.702628175869159
Epoch 285
Epoch 285 :: Batch 0/1
Batch Loss = 3.5515863508125074
285, epoch_train_loss=3.5515863508125074
Epoch 286
Epoch 286 :: Batch 0/1
Batch Loss = 3.543764819114883
286, epoch_train_loss=3.543764819114883
Epoch 287
Epoch 287 :: Batch 0/1
Batch Loss = 3.568089190762147
287, epoch_train_loss=3.568089190762147
Epoch 288
Epoch 288 :: Batch 0/1
Batch Loss = 3.520447707041251
288, epoch_train_loss=3.520447707041251
Epoch 289
Epoch 289 :: Batch 0/1
Batch Loss = 3.5846313217171657
289, epoch_train_loss=3.5846313217171657
Epoch 290
Epoch 290 :: Batch 0/1
Batch Loss = 3.4874343903122926
290, epoch_train_loss=3.4874343903122926
Epoch 291
Epoch 291 :: Batch 0/1
Batch Loss = 3.592418740795372
291, epoch_train_loss=3.592418740795372
Epoch 292
Epoch 292 :: Batch 0/1
Batch Loss = 3.5428851038581324
292, epoch_train_loss=3.5428851038581324
Epoch 293
Epoch 293 :: Batch 0/1
Batch Loss = 3.5468919640444225
293, epoch_train_loss=3.5468919640444225
Epoch 294
Epoch 294 :: Batch 0/1
Batch Loss = 3.510869418177905
294, epoch_train_loss=3.510869418177905
Epoch 295
Epoch 295 :: Batch 0/1
Batch Loss = 3.4892850018784913
295, epoch_train_loss=3.4892850018784913
Epoch 296
Epoch 296 :: Batch 0/1
Batch Loss = 3.5187599758583277
296, epoch_train_loss=3.5187599758583277
Epoch 297
Epoch 297 :: Batch 0/1
Batch Loss = 3.4712389425608974
297, epoch_train_loss=3.4712389425608974
Epoch 298
Epoch 298 :: Batch 0/1
Batch Loss = 3.503350726470524
298, epoch_train_loss=3.503350726470524
Epoch 299
Epoch 299 :: Batch 0/1
Batch Loss = 3.4871462721049546
299, epoch_train_loss=3.4871462721049546
Epoch 300
Epoch 300 :: Batch 0/1
Batch Loss = 3.4713383598938132
300, epoch_train_loss=3.4713383598938132
Epoch 301
Epoch 301 :: Batch 0/1
Batch Loss = 3.4911845165415634
301, epoch_train_loss=3.4911845165415634
Epoch 302
Epoch 302 :: Batch 0/1
Batch Loss = 3.4739619237047648
302, epoch_train_loss=3.4739619237047648
Epoch 303
Epoch 303 :: Batch 0/1
Batch Loss = 3.4712638116998438
303, epoch_train_loss=3.4712638116998438
Epoch 304
Epoch 304 :: Batch 0/1
Batch Loss = 3.475987054672457
304, epoch_train_loss=3.475987054672457
Epoch 305
Epoch 305 :: Batch 0/1
Batch Loss = 3.4697386781162156
305, epoch_train_loss=3.4697386781162156
Epoch 306
Epoch 306 :: Batch 0/1
Batch Loss = 3.4716211464740496
306, epoch_train_loss=3.4716211464740496
Epoch 307
Epoch 307 :: Batch 0/1
Batch Loss = 3.464737812231614
307, epoch_train_loss=3.464737812231614
Epoch 308
Epoch 308 :: Batch 0/1
Batch Loss = 3.4640899349196563
308, epoch_train_loss=3.4640899349196563
Epoch 309
Epoch 309 :: Batch 0/1
Batch Loss = 3.4711496238137385
309, epoch_train_loss=3.4711496238137385
Epoch 310
Epoch 310 :: Batch 0/1
Batch Loss = 3.4637961313634733
310, epoch_train_loss=3.4637961313634733
Epoch 311
Epoch 311 :: Batch 0/1
Batch Loss = 3.464426378923939
311, epoch_train_loss=3.464426378923939
Epoch 312
Epoch 312 :: Batch 0/1
Batch Loss = 3.4627489820337067
312, epoch_train_loss=3.4627489820337067
Epoch 313
Epoch 313 :: Batch 0/1
Batch Loss = 3.4631811566976674
313, epoch_train_loss=3.4631811566976674
Epoch 314
Epoch 314 :: Batch 0/1
Batch Loss = 3.4649404003958426
314, epoch_train_loss=3.4649404003958426
Epoch 315
Epoch 315 :: Batch 0/1
Batch Loss = 3.4608383405610303
315, epoch_train_loss=3.4608383405610303
Epoch 316
Epoch 316 :: Batch 0/1
Batch Loss = 3.461188724756344
316, epoch_train_loss=3.461188724756344
Epoch 317
Epoch 317 :: Batch 0/1
Batch Loss = 3.461496035158385
317, epoch_train_loss=3.461496035158385
Epoch 318
Epoch 318 :: Batch 0/1
Batch Loss = 3.460444828872667
318, epoch_train_loss=3.460444828872667
Epoch 319
Epoch 319 :: Batch 0/1
Batch Loss = 3.4618906444282946
319, epoch_train_loss=3.4618906444282946
Epoch 320
Epoch 320 :: Batch 0/1
Batch Loss = 3.459725049470188
320, epoch_train_loss=3.459725049470188
Epoch 321
Epoch 321 :: Batch 0/1
Batch Loss = 3.459725452881173
321, epoch_train_loss=3.459725452881173
Epoch 322
Epoch 322 :: Batch 0/1
Batch Loss = 3.4599820214720136
322, epoch_train_loss=3.4599820214720136
Epoch 323
Epoch 323 :: Batch 0/1
Batch Loss = 3.4596551126591173
323, epoch_train_loss=3.4596551126591173
Epoch 324
Epoch 324 :: Batch 0/1
Batch Loss = 3.460305763838902
324, epoch_train_loss=3.460305763838902
Epoch 325
Epoch 325 :: Batch 0/1
Batch Loss = 3.459025517526149
325, epoch_train_loss=3.459025517526149
Epoch 326
Epoch 326 :: Batch 0/1
Batch Loss = 3.458869589855678
326, epoch_train_loss=3.458869589855678
Epoch 327
Epoch 327 :: Batch 0/1
Batch Loss = 3.4589573779845106
327, epoch_train_loss=3.4589573779845106
Epoch 328
Epoch 328 :: Batch 0/1
Batch Loss = 3.458659324179138
328, epoch_train_loss=3.458659324179138
Epoch 329
Epoch 329 :: Batch 0/1
Batch Loss = 3.4589786931440174
329, epoch_train_loss=3.4589786931440174
Epoch 330
Epoch 330 :: Batch 0/1
Batch Loss = 3.458081273811359
330, epoch_train_loss=3.458081273811359
Epoch 331
Epoch 331 :: Batch 0/1
Batch Loss = 3.458034953487186
331, epoch_train_loss=3.458034953487186
Epoch 332
Epoch 332 :: Batch 0/1
Batch Loss = 3.4580386064817947
332, epoch_train_loss=3.4580386064817947
Epoch 333
Epoch 333 :: Batch 0/1
Batch Loss = 3.457887307271145
333, epoch_train_loss=3.457887307271145
Epoch 334
Epoch 334 :: Batch 0/1
Batch Loss = 3.4579550615344883
334, epoch_train_loss=3.4579550615344883
Epoch 335
Epoch 335 :: Batch 0/1
Batch Loss = 3.4573084217121224
335, epoch_train_loss=3.4573084217121224
Epoch 336
Epoch 336 :: Batch 0/1
Batch Loss = 3.4572817006999648
336, epoch_train_loss=3.4572817006999648
Epoch 337
Epoch 337 :: Batch 0/1
Batch Loss = 3.4571369330977433
337, epoch_train_loss=3.4571369330977433
Epoch 338
Epoch 338 :: Batch 0/1
Batch Loss = 3.4569969947060577
338, epoch_train_loss=3.4569969947060577
Epoch 339
Epoch 339 :: Batch 0/1
Batch Loss = 3.45685427849759
339, epoch_train_loss=3.45685427849759
Epoch 340
Epoch 340 :: Batch 0/1
Batch Loss = 3.4564020493416505
340, epoch_train_loss=3.4564020493416505
Epoch 341
Epoch 341 :: Batch 0/1
Batch Loss = 3.456385321863815
341, epoch_train_loss=3.456385321863815
Epoch 342
Epoch 342 :: Batch 0/1
Batch Loss = 3.456175755543016
342, epoch_train_loss=3.456175755543016
Epoch 343
Epoch 343 :: Batch 0/1
Batch Loss = 3.4560349321774466
343, epoch_train_loss=3.4560349321774466
Epoch 344
Epoch 344 :: Batch 0/1
Batch Loss = 3.4558174284731633
344, epoch_train_loss=3.4558174284731633
Epoch 345
Epoch 345 :: Batch 0/1
Batch Loss = 3.4555058164685
345, epoch_train_loss=3.4555058164685
Epoch 346
Epoch 346 :: Batch 0/1
Batch Loss = 3.455415699615327
346, epoch_train_loss=3.455415699615327
Epoch 347
Epoch 347 :: Batch 0/1
Batch Loss = 3.455155932577544
347, epoch_train_loss=3.455155932577544
Epoch 348
Epoch 348 :: Batch 0/1
Batch Loss = 3.4549808426674224
348, epoch_train_loss=3.4549808426674224
Epoch 349
Epoch 349 :: Batch 0/1
Batch Loss = 3.454709961879707
349, epoch_train_loss=3.454709961879707
Epoch 350
Epoch 350 :: Batch 0/1
Batch Loss = 3.4544587488428813
350, epoch_train_loss=3.4544587488428813
Epoch 351
Epoch 351 :: Batch 0/1
Batch Loss = 3.454287556424604
351, epoch_train_loss=3.454287556424604
Epoch 352
Epoch 352 :: Batch 0/1
Batch Loss = 3.454006123962227
352, epoch_train_loss=3.454006123962227
Epoch 353
Epoch 353 :: Batch 0/1
Batch Loss = 3.4537916338006327
353, epoch_train_loss=3.4537916338006327
Epoch 354
Epoch 354 :: Batch 0/1
Batch Loss = 3.4534987930619487
354, epoch_train_loss=3.4534987930619487
Epoch 355
Epoch 355 :: Batch 0/1
Batch Loss = 3.4532560762040894
355, epoch_train_loss=3.4532560762040894
Epoch 356
Epoch 356 :: Batch 0/1
Batch Loss = 3.4529872975962803
356, epoch_train_loss=3.4529872975962803
Epoch 357
Epoch 357 :: Batch 0/1
Batch Loss = 3.452676829347256
357, epoch_train_loss=3.452676829347256
Epoch 358
Epoch 358 :: Batch 0/1
Batch Loss = 3.4524001747469963
358, epoch_train_loss=3.4524001747469963
Epoch 359
Epoch 359 :: Batch 0/1
Batch Loss = 3.4520762088095927
359, epoch_train_loss=3.4520762088095927
Epoch 360
Epoch 360 :: Batch 0/1
Batch Loss = 3.4517849654246566
360, epoch_train_loss=3.4517849654246566
Epoch 361
Epoch 361 :: Batch 0/1
Batch Loss = 3.4514362305136994
361, epoch_train_loss=3.4514362305136994
Epoch 362
Epoch 362 :: Batch 0/1
Batch Loss = 3.4510938050675506
362, epoch_train_loss=3.4510938050675506
Epoch 363
Epoch 363 :: Batch 0/1
Batch Loss = 3.450747340099263
363, epoch_train_loss=3.450747340099263
Epoch 364
Epoch 364 :: Batch 0/1
Batch Loss = 3.450376190484644
364, epoch_train_loss=3.450376190484644
Epoch 365
Epoch 365 :: Batch 0/1
Batch Loss = 3.449998726104063
365, epoch_train_loss=3.449998726104063
Epoch 366
Epoch 366 :: Batch 0/1
Batch Loss = 3.4495777377776995
366, epoch_train_loss=3.4495777377776995
Epoch 367
Epoch 367 :: Batch 0/1
Batch Loss = 3.4491715140226047
367, epoch_train_loss=3.4491715140226047
Epoch 368
Epoch 368 :: Batch 0/1
Batch Loss = 3.4487359070599526
368, epoch_train_loss=3.4487359070599526
Epoch 369
Epoch 369 :: Batch 0/1
Batch Loss = 3.448285747199912
369, epoch_train_loss=3.448285747199912
Epoch 370
Epoch 370 :: Batch 0/1
Batch Loss = 3.4478072683215295
370, epoch_train_loss=3.4478072683215295
Epoch 371
Epoch 371 :: Batch 0/1
Batch Loss = 3.4473092249715696
371, epoch_train_loss=3.4473092249715696
Epoch 372
Epoch 372 :: Batch 0/1
Batch Loss = 3.4468004720993863
372, epoch_train_loss=3.4468004720993863
Epoch 373
Epoch 373 :: Batch 0/1
Batch Loss = 3.446253708652938
373, epoch_train_loss=3.446253708652938
Epoch 374
Epoch 374 :: Batch 0/1
Batch Loss = 3.4456888035342366
374, epoch_train_loss=3.4456888035342366
Epoch 375
Epoch 375 :: Batch 0/1
Batch Loss = 3.4450915186777022
375, epoch_train_loss=3.4450915186777022
Epoch 376
Epoch 376 :: Batch 0/1
Batch Loss = 3.4444759675750665
376, epoch_train_loss=3.4444759675750665
Epoch 377
Epoch 377 :: Batch 0/1
Batch Loss = 3.4438231524295753
377, epoch_train_loss=3.4438231524295753
Epoch 378
Epoch 378 :: Batch 0/1
Batch Loss = 3.443140176768581
378, epoch_train_loss=3.443140176768581
Epoch 379
Epoch 379 :: Batch 0/1
Batch Loss = 3.4424270709153126
379, epoch_train_loss=3.4424270709153126
Epoch 380
Epoch 380 :: Batch 0/1
Batch Loss = 3.4416758354596158
380, epoch_train_loss=3.4416758354596158
Epoch 381
Epoch 381 :: Batch 0/1
Batch Loss = 3.4408884724372677
381, epoch_train_loss=3.4408884724372677
Epoch 382
Epoch 382 :: Batch 0/1
Batch Loss = 3.4400585828121524
382, epoch_train_loss=3.4400585828121524
Epoch 383
Epoch 383 :: Batch 0/1
Batch Loss = 3.4391939980402118
383, epoch_train_loss=3.4391939980402118
Epoch 384
Epoch 384 :: Batch 0/1
Batch Loss = 3.4382798681732605
384, epoch_train_loss=3.4382798681732605
Epoch 385
Epoch 385 :: Batch 0/1
Batch Loss = 3.4373204031800513
385, epoch_train_loss=3.4373204031800513
Epoch 386
Epoch 386 :: Batch 0/1
Batch Loss = 3.4363111215888438
386, epoch_train_loss=3.4363111215888438
Epoch 387
Epoch 387 :: Batch 0/1
Batch Loss = 3.435251981157757
387, epoch_train_loss=3.435251981157757
Epoch 388
Epoch 388 :: Batch 0/1
Batch Loss = 3.4341352394156486
388, epoch_train_loss=3.4341352394156486
Epoch 389
Epoch 389 :: Batch 0/1
Batch Loss = 3.4329611818522894
389, epoch_train_loss=3.4329611818522894
Epoch 390
Epoch 390 :: Batch 0/1
Batch Loss = 3.431727180857247
390, epoch_train_loss=3.431727180857247
Epoch 391
Epoch 391 :: Batch 0/1
Batch Loss = 3.430427447790271
391, epoch_train_loss=3.430427447790271
Epoch 392
Epoch 392 :: Batch 0/1
Batch Loss = 3.4290617511776826
392, epoch_train_loss=3.4290617511776826
Epoch 393
Epoch 393 :: Batch 0/1
Batch Loss = 3.4276250083373205
393, epoch_train_loss=3.4276250083373205
Epoch 394
Epoch 394 :: Batch 0/1
Batch Loss = 3.426114304688921
394, epoch_train_loss=3.426114304688921
Epoch 395
Epoch 395 :: Batch 0/1
Batch Loss = 3.4245245178831496
395, epoch_train_loss=3.4245245178831496
Epoch 396
Epoch 396 :: Batch 0/1
Batch Loss = 3.4228555012467914
396, epoch_train_loss=3.4228555012467914
Epoch 397
Epoch 397 :: Batch 0/1
Batch Loss = 3.4210991671619517
397, epoch_train_loss=3.4210991671619517
Epoch 398
Epoch 398 :: Batch 0/1
Batch Loss = 3.419254488360622
398, epoch_train_loss=3.419254488360622
Epoch 399
Epoch 399 :: Batch 0/1
Batch Loss = 3.4173162777262984
399, epoch_train_loss=3.4173162777262984
Epoch 400
Epoch 400 :: Batch 0/1
Batch Loss = 3.415280682133457
400, epoch_train_loss=3.415280682133457
Epoch 401
Epoch 401 :: Batch 0/1
Batch Loss = 3.4131426750741074
401, epoch_train_loss=3.4131426750741074
Epoch 402
Epoch 402 :: Batch 0/1
Batch Loss = 3.4108966675770787
402, epoch_train_loss=3.4108966675770787
Epoch 403
Epoch 403 :: Batch 0/1
Batch Loss = 3.408533068370355
403, epoch_train_loss=3.408533068370355
Epoch 404
Epoch 404 :: Batch 0/1
Batch Loss = 3.4060441177020597
404, epoch_train_loss=3.4060441177020597
Epoch 405
Epoch 405 :: Batch 0/1
Batch Loss = 3.4034629718502147
405, epoch_train_loss=3.4034629718502147
Epoch 406
Epoch 406 :: Batch 0/1
Batch Loss = 3.4008464229932707
406, epoch_train_loss=3.4008464229932707
Epoch 407
Epoch 407 :: Batch 0/1
Batch Loss = 3.3980699278330553
407, epoch_train_loss=3.3980699278330553
Epoch 408
Epoch 408 :: Batch 0/1
Batch Loss = 3.3950956363284903
408, epoch_train_loss=3.3950956363284903
Epoch 409
Epoch 409 :: Batch 0/1
Batch Loss = 3.3921177322219487
409, epoch_train_loss=3.3921177322219487
Epoch 410
Epoch 410 :: Batch 0/1
Batch Loss = 3.388909632836247
410, epoch_train_loss=3.388909632836247
Epoch 411
Epoch 411 :: Batch 0/1
Batch Loss = 3.385665681062479
411, epoch_train_loss=3.385665681062479
Epoch 412
Epoch 412 :: Batch 0/1
Batch Loss = 3.382212078481077
412, epoch_train_loss=3.382212078481077
Epoch 413
Epoch 413 :: Batch 0/1
Batch Loss = 3.3786655602322915
413, epoch_train_loss=3.3786655602322915
Epoch 414
Epoch 414 :: Batch 0/1
Batch Loss = 3.374997927988175
414, epoch_train_loss=3.374997927988175
Epoch 415
Epoch 415 :: Batch 0/1
Batch Loss = 3.3712018934842445
415, epoch_train_loss=3.3712018934842445
Epoch 416
Epoch 416 :: Batch 0/1
Batch Loss = 3.3677226830575946
416, epoch_train_loss=3.3677226830575946
Epoch 417
Epoch 417 :: Batch 0/1
Batch Loss = 3.3636196383624735
417, epoch_train_loss=3.3636196383624735
Epoch 418
Epoch 418 :: Batch 0/1
Batch Loss = 3.359720412440052
418, epoch_train_loss=3.359720412440052
Epoch 419
Epoch 419 :: Batch 0/1
Batch Loss = 3.3550562689550008
419, epoch_train_loss=3.3550562689550008
Epoch 420
Epoch 420 :: Batch 0/1
Batch Loss = 3.3508716602074395
420, epoch_train_loss=3.3508716602074395
Epoch 421
Epoch 421 :: Batch 0/1
Batch Loss = 3.3469737375629594
421, epoch_train_loss=3.3469737375629594
Epoch 422
Epoch 422 :: Batch 0/1
Batch Loss = 3.348653400513251
422, epoch_train_loss=3.348653400513251
Epoch 423
Epoch 423 :: Batch 0/1
Batch Loss = 3.361744227606192
423, epoch_train_loss=3.361744227606192
Epoch 424
Epoch 424 :: Batch 0/1
Batch Loss = 3.3689380704151874
424, epoch_train_loss=3.3689380704151874
Epoch 425
Epoch 425 :: Batch 0/1
Batch Loss = 3.363661467417886
425, epoch_train_loss=3.363661467417886
Epoch 426
Epoch 426 :: Batch 0/1
Batch Loss = 3.3981984176557645
426, epoch_train_loss=3.3981984176557645
Epoch 427
Epoch 427 :: Batch 0/1
Batch Loss = 3.347704831607871
427, epoch_train_loss=3.347704831607871
Epoch 428
Epoch 428 :: Batch 0/1
Batch Loss = 3.418601649116677
428, epoch_train_loss=3.418601649116677
Epoch 429
Epoch 429 :: Batch 0/1
Batch Loss = 3.4726425869927016
429, epoch_train_loss=3.4726425869927016
Epoch 430
Epoch 430 :: Batch 0/1
Batch Loss = 3.396517144434203
430, epoch_train_loss=3.396517144434203
Epoch 431
Epoch 431 :: Batch 0/1
Batch Loss = 3.484557272459629
431, epoch_train_loss=3.484557272459629
Epoch 432
Epoch 432 :: Batch 0/1
Batch Loss = 3.3306523426226393
432, epoch_train_loss=3.3306523426226393
Epoch 433
Epoch 433 :: Batch 0/1
Batch Loss = 3.3804875616594012
433, epoch_train_loss=3.3804875616594012
Epoch 434
Epoch 434 :: Batch 0/1
Batch Loss = 3.305652393248711
434, epoch_train_loss=3.305652393248711
Epoch 435
Epoch 435 :: Batch 0/1
Batch Loss = 3.3459641544955034
435, epoch_train_loss=3.3459641544955034
Epoch 436
Epoch 436 :: Batch 0/1
Batch Loss = 3.330218845315353
436, epoch_train_loss=3.330218845315353
Epoch 437
Epoch 437 :: Batch 0/1
Batch Loss = 3.303872278665384
437, epoch_train_loss=3.303872278665384
Epoch 438
Epoch 438 :: Batch 0/1
Batch Loss = 3.3251575457369476
438, epoch_train_loss=3.3251575457369476
Epoch 439
Epoch 439 :: Batch 0/1
Batch Loss = 3.26419903903666
439, epoch_train_loss=3.26419903903666
Epoch 440
Epoch 440 :: Batch 0/1
Batch Loss = 3.3048996036247056
440, epoch_train_loss=3.3048996036247056
Epoch 441
Epoch 441 :: Batch 0/1
Batch Loss = 3.2480714842743246
441, epoch_train_loss=3.2480714842743246
Epoch 442
Epoch 442 :: Batch 0/1
Batch Loss = 3.2564199125670594
442, epoch_train_loss=3.2564199125670594
Epoch 443
Epoch 443 :: Batch 0/1
Batch Loss = 3.2523890608258386
443, epoch_train_loss=3.2523890608258386
Epoch 444
Epoch 444 :: Batch 0/1
Batch Loss = 3.212996494463951
444, epoch_train_loss=3.212996494463951
Epoch 445
Epoch 445 :: Batch 0/1
Batch Loss = 3.2258086241641415
445, epoch_train_loss=3.2258086241641415
Epoch 446
Epoch 446 :: Batch 0/1
Batch Loss = 3.196810452843141
446, epoch_train_loss=3.196810452843141
Epoch 447
Epoch 447 :: Batch 0/1
Batch Loss = 3.1842547352958688
447, epoch_train_loss=3.1842547352958688
Epoch 448
Epoch 448 :: Batch 0/1
Batch Loss = 3.1946101158115505
448, epoch_train_loss=3.1946101158115505
Epoch 449
Epoch 449 :: Batch 0/1
Batch Loss = 3.1704373943313895
449, epoch_train_loss=3.1704373943313895
Epoch 450
Epoch 450 :: Batch 0/1
Batch Loss = 3.16325523727404
450, epoch_train_loss=3.16325523727404
Epoch 451
Epoch 451 :: Batch 0/1
Batch Loss = 3.1663254366935476
451, epoch_train_loss=3.1663254366935476
Epoch 452
Epoch 452 :: Batch 0/1
Batch Loss = 3.169293611339685
452, epoch_train_loss=3.169293611339685
Epoch 453
Epoch 453 :: Batch 0/1
Batch Loss = 3.1857750141677315
453, epoch_train_loss=3.1857750141677315
Epoch 454
Epoch 454 :: Batch 0/1
Batch Loss = 3.1937095627267422
454, epoch_train_loss=3.1937095627267422
Epoch 455
Epoch 455 :: Batch 0/1
Batch Loss = 3.236709477578313
455, epoch_train_loss=3.236709477578313
Epoch 456
Epoch 456 :: Batch 0/1
Batch Loss = 3.2362895914201286
456, epoch_train_loss=3.2362895914201286
Epoch 457
Epoch 457 :: Batch 0/1
Batch Loss = 3.2109096469443803
457, epoch_train_loss=3.2109096469443803
Epoch 458
Epoch 458 :: Batch 0/1
Batch Loss = 3.148729697825042
458, epoch_train_loss=3.148729697825042
Epoch 459
Epoch 459 :: Batch 0/1
Batch Loss = 3.2159935111938123
459, epoch_train_loss=3.2159935111938123
Epoch 460
Epoch 460 :: Batch 0/1
Batch Loss = 3.2299628063759083
460, epoch_train_loss=3.2299628063759083
Epoch 461
Epoch 461 :: Batch 0/1
Batch Loss = 3.164252309217977
461, epoch_train_loss=3.164252309217977
Epoch 462
Epoch 462 :: Batch 0/1
Batch Loss = 3.2726022840052194
462, epoch_train_loss=3.2726022840052194
Epoch 463
Epoch 463 :: Batch 0/1
Batch Loss = 3.1687988547076316
463, epoch_train_loss=3.1687988547076316
Epoch 464
Epoch 464 :: Batch 0/1
Batch Loss = 3.1938190670531807
464, epoch_train_loss=3.1938190670531807
Epoch 465
Epoch 465 :: Batch 0/1
Batch Loss = 3.191657978678836
465, epoch_train_loss=3.191657978678836
Epoch 466
Epoch 466 :: Batch 0/1
Batch Loss = 3.1502359330452028
466, epoch_train_loss=3.1502359330452028
Epoch 467
Epoch 467 :: Batch 0/1
Batch Loss = 3.191165286073919
467, epoch_train_loss=3.191165286073919
Epoch 468
Epoch 468 :: Batch 0/1
Batch Loss = 3.1467859950704193
468, epoch_train_loss=3.1467859950704193
Epoch 469
Epoch 469 :: Batch 0/1
Batch Loss = 3.191618195968966
469, epoch_train_loss=3.191618195968966
Epoch 470
Epoch 470 :: Batch 0/1
Batch Loss = 3.147830089659755
470, epoch_train_loss=3.147830089659755
Epoch 471
Epoch 471 :: Batch 0/1
Batch Loss = 3.1732238015007748
471, epoch_train_loss=3.1732238015007748
Epoch 472
Epoch 472 :: Batch 0/1
Batch Loss = 3.1458494196603746
472, epoch_train_loss=3.1458494196603746
Epoch 473
Epoch 473 :: Batch 0/1
Batch Loss = 3.163838488082161
473, epoch_train_loss=3.163838488082161
Epoch 474
Epoch 474 :: Batch 0/1
Batch Loss = 3.1512762098427034
474, epoch_train_loss=3.1512762098427034
Epoch 475
Epoch 475 :: Batch 0/1
Batch Loss = 3.1532092289244296
475, epoch_train_loss=3.1532092289244296
Epoch 476
Epoch 476 :: Batch 0/1
Batch Loss = 3.1518207421444067
476, epoch_train_loss=3.1518207421444067
Epoch 477
Epoch 477 :: Batch 0/1
Batch Loss = 3.1414642895372755
477, epoch_train_loss=3.1414642895372755
Epoch 478
Epoch 478 :: Batch 0/1
Batch Loss = 3.1527363797489856
478, epoch_train_loss=3.1527363797489856
Epoch 479
Epoch 479 :: Batch 0/1
Batch Loss = 3.1356497362381495
479, epoch_train_loss=3.1356497362381495
Epoch 480
Epoch 480 :: Batch 0/1
Batch Loss = 3.1500409702390506
480, epoch_train_loss=3.1500409702390506
Epoch 481
Epoch 481 :: Batch 0/1
Batch Loss = 3.1356018847446894
481, epoch_train_loss=3.1356018847446894
Epoch 482
Epoch 482 :: Batch 0/1
Batch Loss = 3.139878126349864
482, epoch_train_loss=3.139878126349864
Epoch 483
Epoch 483 :: Batch 0/1
Batch Loss = 3.1389170057440516
483, epoch_train_loss=3.1389170057440516
Epoch 484
Epoch 484 :: Batch 0/1
Batch Loss = 3.1308301835374084
484, epoch_train_loss=3.1308301835374084
Epoch 485
Epoch 485 :: Batch 0/1
Batch Loss = 3.1382255951696774
485, epoch_train_loss=3.1382255951696774
Epoch 486
Epoch 486 :: Batch 0/1
Batch Loss = 3.128788535336591
486, epoch_train_loss=3.128788535336591
Epoch 487
Epoch 487 :: Batch 0/1
Batch Loss = 3.1309280093743808
487, epoch_train_loss=3.1309280093743808
Epoch 488
Epoch 488 :: Batch 0/1
Batch Loss = 3.1317549910527873
488, epoch_train_loss=3.1317549910527873
Epoch 489
Epoch 489 :: Batch 0/1
Batch Loss = 3.1244441930084585
489, epoch_train_loss=3.1244441930084585
Epoch 490
Epoch 490 :: Batch 0/1
Batch Loss = 3.1288909789073864
490, epoch_train_loss=3.1288909789073864
Epoch 491
Epoch 491 :: Batch 0/1
Batch Loss = 3.1258646532492445
491, epoch_train_loss=3.1258646532492445
Epoch 492
Epoch 492 :: Batch 0/1
Batch Loss = 3.121988104618956
492, epoch_train_loss=3.121988104618956
Epoch 493
Epoch 493 :: Batch 0/1
Batch Loss = 3.1254480920855503
493, epoch_train_loss=3.1254480920855503
Epoch 494
Epoch 494 :: Batch 0/1
Batch Loss = 3.1218195674454976
494, epoch_train_loss=3.1218195674454976
Epoch 495
Epoch 495 :: Batch 0/1
Batch Loss = 3.1200553480110536
495, epoch_train_loss=3.1200553480110536
Epoch 496
Epoch 496 :: Batch 0/1
Batch Loss = 3.122221958093827
496, epoch_train_loss=3.122221958093827
Epoch 497
Epoch 497 :: Batch 0/1
Batch Loss = 3.1191184146511985
497, epoch_train_loss=3.1191184146511985
Epoch 498
Epoch 498 :: Batch 0/1
Batch Loss = 3.1181580703474148
498, epoch_train_loss=3.1181580703474148
Epoch 499
Epoch 499 :: Batch 0/1
Batch Loss = 3.1196436241496706
499, epoch_train_loss=3.1196436241496706
Epoch 500
Epoch 500 :: Batch 0/1
Batch Loss = 3.117263364430537
500, epoch_train_loss=3.117263364430537
Epoch 501
Epoch 501 :: Batch 0/1
Batch Loss = 3.1164735107516517
501, epoch_train_loss=3.1164735107516517
Epoch 502
Epoch 502 :: Batch 0/1
Batch Loss = 3.117639921693506
502, epoch_train_loss=3.117639921693506
Epoch 503
Epoch 503 :: Batch 0/1
Batch Loss = 3.1160334925118724
503, epoch_train_loss=3.1160334925118724
Epoch 504
Epoch 504 :: Batch 0/1
Batch Loss = 3.115162409626759
504, epoch_train_loss=3.115162409626759
Epoch 505
Epoch 505 :: Batch 0/1
Batch Loss = 3.1159633796175195
505, epoch_train_loss=3.1159633796175195
Epoch 506
Epoch 506 :: Batch 0/1
Batch Loss = 3.115164825862669
506, epoch_train_loss=3.115164825862669
Epoch 507
Epoch 507 :: Batch 0/1
Batch Loss = 3.114218598079701
507, epoch_train_loss=3.114218598079701
Epoch 508
Epoch 508 :: Batch 0/1
Batch Loss = 3.1147562059688743
508, epoch_train_loss=3.1147562059688743
Epoch 509
Epoch 509 :: Batch 0/1
Batch Loss = 3.1144616452564153
509, epoch_train_loss=3.1144616452564153
Epoch 510
Epoch 510 :: Batch 0/1
Batch Loss = 3.1136035248688403
510, epoch_train_loss=3.1136035248688403
Epoch 511
Epoch 511 :: Batch 0/1
Batch Loss = 3.113807894201838
511, epoch_train_loss=3.113807894201838
Epoch 512
Epoch 512 :: Batch 0/1
Batch Loss = 3.113864750274634
512, epoch_train_loss=3.113864750274634
Epoch 513
Epoch 513 :: Batch 0/1
Batch Loss = 3.113252209394331
513, epoch_train_loss=3.113252209394331
Epoch 514
Epoch 514 :: Batch 0/1
Batch Loss = 3.1130872726887837
514, epoch_train_loss=3.1130872726887837
Epoch 515
Epoch 515 :: Batch 0/1
Batch Loss = 3.113286931615556
515, epoch_train_loss=3.113286931615556
Epoch 516
Epoch 516 :: Batch 0/1
Batch Loss = 3.1129950036472294
516, epoch_train_loss=3.1129950036472294
Epoch 517
Epoch 517 :: Batch 0/1
Batch Loss = 3.11259377637575
517, epoch_train_loss=3.11259377637575
Epoch 518
Epoch 518 :: Batch 0/1
Batch Loss = 3.112688297273908
518, epoch_train_loss=3.112688297273908
Epoch 519
Epoch 519 :: Batch 0/1
Batch Loss = 3.1126711792822808
519, epoch_train_loss=3.1126711792822808
Epoch 520
Epoch 520 :: Batch 0/1
Batch Loss = 3.1122820925589907
520, epoch_train_loss=3.1122820925589907
Epoch 521
Epoch 521 :: Batch 0/1
Batch Loss = 3.112138873231316
521, epoch_train_loss=3.112138873231316
Epoch 522
Epoch 522 :: Batch 0/1
Batch Loss = 3.112187017693199
522, epoch_train_loss=3.112187017693199
Epoch 523
Epoch 523 :: Batch 0/1
Batch Loss = 3.112001028334292
523, epoch_train_loss=3.112001028334292
Epoch 524
Epoch 524 :: Batch 0/1
Batch Loss = 3.11173113233487
524, epoch_train_loss=3.11173113233487
Epoch 525
Epoch 525 :: Batch 0/1
Batch Loss = 3.1116271389534247
525, epoch_train_loss=3.1116271389534247
Epoch 526
Epoch 526 :: Batch 0/1
Batch Loss = 3.111594659773695
526, epoch_train_loss=3.111594659773695
Epoch 527
Epoch 527 :: Batch 0/1
Batch Loss = 3.1114301803631843
527, epoch_train_loss=3.1114301803631843
Epoch 528
Epoch 528 :: Batch 0/1
Batch Loss = 3.1111845310034307
528, epoch_train_loss=3.1111845310034307
Epoch 529
Epoch 529 :: Batch 0/1
Batch Loss = 3.1110679235462593
529, epoch_train_loss=3.1110679235462593
Epoch 530
Epoch 530 :: Batch 0/1
Batch Loss = 3.111017163509144
530, epoch_train_loss=3.111017163509144
Epoch 531
Epoch 531 :: Batch 0/1
Batch Loss = 3.1108555698435487
531, epoch_train_loss=3.1108555698435487
Epoch 532
Epoch 532 :: Batch 0/1
Batch Loss = 3.110641436189192
532, epoch_train_loss=3.110641436189192
Epoch 533
Epoch 533 :: Batch 0/1
Batch Loss = 3.1105015787699952
533, epoch_train_loss=3.1105015787699952
Epoch 534
Epoch 534 :: Batch 0/1
Batch Loss = 3.1104188688146834
534, epoch_train_loss=3.1104188688146834
Epoch 535
Epoch 535 :: Batch 0/1
Batch Loss = 3.110296296259706
535, epoch_train_loss=3.110296296259706
Epoch 536
Epoch 536 :: Batch 0/1
Batch Loss = 3.1101184796051227
536, epoch_train_loss=3.1101184796051227
Epoch 537
Epoch 537 :: Batch 0/1
Batch Loss = 3.1099517039975115
537, epoch_train_loss=3.1099517039975115
Epoch 538
Epoch 538 :: Batch 0/1
Batch Loss = 3.109828719974228
538, epoch_train_loss=3.109828719974228
Epoch 539
Epoch 539 :: Batch 0/1
Batch Loss = 3.1097261713806255
539, epoch_train_loss=3.1097261713806255
Epoch 540
Epoch 540 :: Batch 0/1
Batch Loss = 3.1095986491837255
540, epoch_train_loss=3.1095986491837255
Epoch 541
Epoch 541 :: Batch 0/1
Batch Loss = 3.1094378346272857
541, epoch_train_loss=3.1094378346272857
Epoch 542
Epoch 542 :: Batch 0/1
Batch Loss = 3.1092832895094538
542, epoch_train_loss=3.1092832895094538
Epoch 543
Epoch 543 :: Batch 0/1
Batch Loss = 3.1091574477349044
543, epoch_train_loss=3.1091574477349044
Epoch 544
Epoch 544 :: Batch 0/1
Batch Loss = 3.1090467888404096
544, epoch_train_loss=3.1090467888404096
Epoch 545
Epoch 545 :: Batch 0/1
Batch Loss = 3.1089271312102715
545, epoch_train_loss=3.1089271312102715
Epoch 546
Epoch 546 :: Batch 0/1
Batch Loss = 3.108790633919357
546, epoch_train_loss=3.108790633919357
Epoch 547
Epoch 547 :: Batch 0/1
Batch Loss = 3.1086470819956915
547, epoch_train_loss=3.1086470819956915
Epoch 548
Epoch 548 :: Batch 0/1
Batch Loss = 3.108506408076427
548, epoch_train_loss=3.108506408076427
Epoch 549
Epoch 549 :: Batch 0/1
Batch Loss = 3.1083765801172705
549, epoch_train_loss=3.1083765801172705
Epoch 550
Epoch 550 :: Batch 0/1
Batch Loss = 3.108256035001029
550, epoch_train_loss=3.108256035001029
Epoch 551
Epoch 551 :: Batch 0/1
Batch Loss = 3.108138432491011
551, epoch_train_loss=3.108138432491011
Epoch 552
Epoch 552 :: Batch 0/1
Batch Loss = 3.108017395242236
552, epoch_train_loss=3.108017395242236
Epoch 553
Epoch 553 :: Batch 0/1
Batch Loss = 3.1078887144932783
553, epoch_train_loss=3.1078887144932783
Epoch 554
Epoch 554 :: Batch 0/1
Batch Loss = 3.107756278517123
554, epoch_train_loss=3.107756278517123
Epoch 555
Epoch 555 :: Batch 0/1
Batch Loss = 3.1076220892143387
555, epoch_train_loss=3.1076220892143387
Epoch 556
Epoch 556 :: Batch 0/1
Batch Loss = 3.1074884169623243
556, epoch_train_loss=3.1074884169623243
Epoch 557
Epoch 557 :: Batch 0/1
Batch Loss = 3.1073552992191704
557, epoch_train_loss=3.1073552992191704
Epoch 558
Epoch 558 :: Batch 0/1
Batch Loss = 3.1072235863870383
558, epoch_train_loss=3.1072235863870383
Epoch 559
Epoch 559 :: Batch 0/1
Batch Loss = 3.1070935012248566
559, epoch_train_loss=3.1070935012248566
Epoch 560
Epoch 560 :: Batch 0/1
Batch Loss = 3.106964295605083
560, epoch_train_loss=3.106964295605083
Epoch 561
Epoch 561 :: Batch 0/1
Batch Loss = 3.1068356246437707
561, epoch_train_loss=3.1068356246437707
Epoch 562
Epoch 562 :: Batch 0/1
Batch Loss = 3.1067069538223056
562, epoch_train_loss=3.1067069538223056
Epoch 563
Epoch 563 :: Batch 0/1
Batch Loss = 3.106578836837561
563, epoch_train_loss=3.106578836837561
Epoch 564
Epoch 564 :: Batch 0/1
Batch Loss = 3.10645241041714
564, epoch_train_loss=3.10645241041714
Epoch 565
Epoch 565 :: Batch 0/1
Batch Loss = 3.106329984707955
565, epoch_train_loss=3.106329984707955
Epoch 566
Epoch 566 :: Batch 0/1
Batch Loss = 3.1062154476594994
566, epoch_train_loss=3.1062154476594994
Epoch 567
Epoch 567 :: Batch 0/1
Batch Loss = 3.106120562400054
567, epoch_train_loss=3.106120562400054
Epoch 568
Epoch 568 :: Batch 0/1
Batch Loss = 3.1060655977962823
568, epoch_train_loss=3.1060655977962823
Epoch 569
Epoch 569 :: Batch 0/1
Batch Loss = 3.1061223395115105
569, epoch_train_loss=3.1061223395115105
Epoch 570
Epoch 570 :: Batch 0/1
Batch Loss = 3.1063968496485344
570, epoch_train_loss=3.1063968496485344
Epoch 571
Epoch 571 :: Batch 0/1
Batch Loss = 3.107379103404943
571, epoch_train_loss=3.107379103404943
Epoch 572
Epoch 572 :: Batch 0/1
Batch Loss = 3.1094477097510085
572, epoch_train_loss=3.1094477097510085
Epoch 573
Epoch 573 :: Batch 0/1
Batch Loss = 3.115985328547973
573, epoch_train_loss=3.115985328547973
Epoch 574
Epoch 574 :: Batch 0/1
Batch Loss = 3.123920904817779
574, epoch_train_loss=3.123920904817779
Epoch 575
Epoch 575 :: Batch 0/1
Batch Loss = 3.149356628281913
575, epoch_train_loss=3.149356628281913
Epoch 576
Epoch 576 :: Batch 0/1
Batch Loss = 3.141474047375501
576, epoch_train_loss=3.141474047375501
Epoch 577
Epoch 577 :: Batch 0/1
Batch Loss = 3.143719730814791
577, epoch_train_loss=3.143719730814791
Epoch 578
Epoch 578 :: Batch 0/1
Batch Loss = 3.1110626760493902
578, epoch_train_loss=3.1110626760493902
Epoch 579
Epoch 579 :: Batch 0/1
Batch Loss = 3.1166101905996966
579, epoch_train_loss=3.1166101905996966
Epoch 580
Epoch 580 :: Batch 0/1
Batch Loss = 3.1507446130376278
580, epoch_train_loss=3.1507446130376278
Epoch 581
Epoch 581 :: Batch 0/1
Batch Loss = 3.1340310623906946
581, epoch_train_loss=3.1340310623906946
Epoch 582
Epoch 582 :: Batch 0/1
Batch Loss = 3.125277669937686
582, epoch_train_loss=3.125277669937686
Epoch 583
Epoch 583 :: Batch 0/1
Batch Loss = 3.1065130838901722
583, epoch_train_loss=3.1065130838901722
Epoch 584
Epoch 584 :: Batch 0/1
Batch Loss = 3.1100607135185756
584, epoch_train_loss=3.1100607135185756
Epoch 585
Epoch 585 :: Batch 0/1
Batch Loss = 3.123946672225293
585, epoch_train_loss=3.123946672225293
Epoch 586
Epoch 586 :: Batch 0/1
Batch Loss = 3.1242784295492667
586, epoch_train_loss=3.1242784295492667
Epoch 587
Epoch 587 :: Batch 0/1
Batch Loss = 3.132997293466753
587, epoch_train_loss=3.132997293466753
Epoch 588
Epoch 588 :: Batch 0/1
Batch Loss = 3.107677301608162
588, epoch_train_loss=3.107677301608162
Epoch 589
Epoch 589 :: Batch 0/1
Batch Loss = 3.1132492783529924
589, epoch_train_loss=3.1132492783529924
Epoch 590
Epoch 590 :: Batch 0/1
Batch Loss = 3.1306160368334965
590, epoch_train_loss=3.1306160368334965
Epoch 591
Epoch 591 :: Batch 0/1
Batch Loss = 3.111773765134122
591, epoch_train_loss=3.111773765134122
Epoch 592
Epoch 592 :: Batch 0/1
Batch Loss = 3.1080397883839543
592, epoch_train_loss=3.1080397883839543
Epoch 593
Epoch 593 :: Batch 0/1
Batch Loss = 3.1121323252113693
593, epoch_train_loss=3.1121323252113693
Epoch 594
Epoch 594 :: Batch 0/1
Batch Loss = 3.122473347561251
594, epoch_train_loss=3.122473347561251
Epoch 595
Epoch 595 :: Batch 0/1
Batch Loss = 3.123325944452702
595, epoch_train_loss=3.123325944452702
Epoch 596
Epoch 596 :: Batch 0/1
Batch Loss = 3.109562921016429
596, epoch_train_loss=3.109562921016429
Epoch 597
Epoch 597 :: Batch 0/1
Batch Loss = 3.107590679723637
597, epoch_train_loss=3.107590679723637
Epoch 598
Epoch 598 :: Batch 0/1
Batch Loss = 3.10911476911707
598, epoch_train_loss=3.10911476911707
Epoch 599
Epoch 599 :: Batch 0/1
Batch Loss = 3.1135148920255937
599, epoch_train_loss=3.1135148920255937
Epoch 600
Epoch 600 :: Batch 0/1
Batch Loss = 3.1090192863422463
600, epoch_train_loss=3.1090192863422463
Epoch 601
Epoch 601 :: Batch 0/1
Batch Loss = 3.1058542904710125
601, epoch_train_loss=3.1058542904710125
Epoch 602
Epoch 602 :: Batch 0/1
Batch Loss = 3.1050536872242733
602, epoch_train_loss=3.1050536872242733
Epoch 603
Epoch 603 :: Batch 0/1
Batch Loss = 3.110568313595242
603, epoch_train_loss=3.110568313595242
Epoch 604
Epoch 604 :: Batch 0/1
Batch Loss = 3.1096505391228098
604, epoch_train_loss=3.1096505391228098
Epoch 605
Epoch 605 :: Batch 0/1
Batch Loss = 3.107019623050346
605, epoch_train_loss=3.107019623050346
Epoch 606
Epoch 606 :: Batch 0/1
Batch Loss = 3.103445944040049
606, epoch_train_loss=3.103445944040049
Epoch 607
Epoch 607 :: Batch 0/1
Batch Loss = 3.103458914112929
607, epoch_train_loss=3.103458914112929
Epoch 608
Epoch 608 :: Batch 0/1
Batch Loss = 3.1067033833930915
608, epoch_train_loss=3.1067033833930915
Epoch 609
Epoch 609 :: Batch 0/1
Batch Loss = 3.105488941788442
609, epoch_train_loss=3.105488941788442
Epoch 610
Epoch 610 :: Batch 0/1
Batch Loss = 3.1042297686009
610, epoch_train_loss=3.1042297686009
Epoch 611
Epoch 611 :: Batch 0/1
Batch Loss = 3.101829781972063
611, epoch_train_loss=3.101829781972063
Epoch 612
Epoch 612 :: Batch 0/1
Batch Loss = 3.1028065396698623
612, epoch_train_loss=3.1028065396698623
Epoch 613
Epoch 613 :: Batch 0/1
Batch Loss = 3.1035700774485013
613, epoch_train_loss=3.1035700774485013
Epoch 614
Epoch 614 :: Batch 0/1
Batch Loss = 3.104538355267055
614, epoch_train_loss=3.104538355267055
Epoch 615
Epoch 615 :: Batch 0/1
Batch Loss = 3.1038176601936085
615, epoch_train_loss=3.1038176601936085
Epoch 616
Epoch 616 :: Batch 0/1
Batch Loss = 3.1026839413307066
616, epoch_train_loss=3.1026839413307066
Epoch 617
Epoch 617 :: Batch 0/1
Batch Loss = 3.101053813362522
617, epoch_train_loss=3.101053813362522
Epoch 618
Epoch 618 :: Batch 0/1
Batch Loss = 3.101183674577364
618, epoch_train_loss=3.101183674577364
Epoch 619
Epoch 619 :: Batch 0/1
Batch Loss = 3.1013705767427204
619, epoch_train_loss=3.1013705767427204
Epoch 620
Epoch 620 :: Batch 0/1
Batch Loss = 3.1023159909666
620, epoch_train_loss=3.1023159909666
Epoch 621
Epoch 621 :: Batch 0/1
Batch Loss = 3.10194952133075
621, epoch_train_loss=3.10194952133075
Epoch 622
Epoch 622 :: Batch 0/1
Batch Loss = 3.1018197439515074
622, epoch_train_loss=3.1018197439515074
Epoch 623
Epoch 623 :: Batch 0/1
Batch Loss = 3.1009362136601926
623, epoch_train_loss=3.1009362136601926
Epoch 624
Epoch 624 :: Batch 0/1
Batch Loss = 3.10037381334259
624, epoch_train_loss=3.10037381334259
Epoch 625
Epoch 625 :: Batch 0/1
Batch Loss = 3.0998461550910377
625, epoch_train_loss=3.0998461550910377
Epoch 626
Epoch 626 :: Batch 0/1
Batch Loss = 3.0996095572512274
626, epoch_train_loss=3.0996095572512274
Epoch 627
Epoch 627 :: Batch 0/1
Batch Loss = 3.0996310361470343
627, epoch_train_loss=3.0996310361470343
Epoch 628
Epoch 628 :: Batch 0/1
Batch Loss = 3.099669655927873
628, epoch_train_loss=3.099669655927873
Epoch 629
Epoch 629 :: Batch 0/1
Batch Loss = 3.099926766455531
629, epoch_train_loss=3.099926766455531
Epoch 630
Epoch 630 :: Batch 0/1
Batch Loss = 3.0999855758819437
630, epoch_train_loss=3.0999855758819437
Epoch 631
Epoch 631 :: Batch 0/1
Batch Loss = 3.1004380277013253
631, epoch_train_loss=3.1004380277013253
Epoch 632
Epoch 632 :: Batch 0/1
Batch Loss = 3.1004639614825114
632, epoch_train_loss=3.1004639614825114
Epoch 633
Epoch 633 :: Batch 0/1
Batch Loss = 3.10116335983065
633, epoch_train_loss=3.10116335983065
Epoch 634
Epoch 634 :: Batch 0/1
Batch Loss = 3.101311583023089
634, epoch_train_loss=3.101311583023089
Epoch 635
Epoch 635 :: Batch 0/1
Batch Loss = 3.1026346122491155
635, epoch_train_loss=3.1026346122491155
Epoch 636
Epoch 636 :: Batch 0/1
Batch Loss = 3.102693926390722
636, epoch_train_loss=3.102693926390722
Epoch 637
Epoch 637 :: Batch 0/1
Batch Loss = 3.1047714962200956
637, epoch_train_loss=3.1047714962200956
Epoch 638
Epoch 638 :: Batch 0/1
Batch Loss = 3.1045799691908496
638, epoch_train_loss=3.1045799691908496
Epoch 639
Epoch 639 :: Batch 0/1
Batch Loss = 3.10752055853084
639, epoch_train_loss=3.10752055853084
Epoch 640
Epoch 640 :: Batch 0/1
Batch Loss = 3.106036201897745
640, epoch_train_loss=3.106036201897745
Epoch 641
Epoch 641 :: Batch 0/1
Batch Loss = 3.108723070227682
641, epoch_train_loss=3.108723070227682
Epoch 642
Epoch 642 :: Batch 0/1
Batch Loss = 3.105642873659449
642, epoch_train_loss=3.105642873659449
Epoch 643
Epoch 643 :: Batch 0/1
Batch Loss = 3.1061523082763745
643, epoch_train_loss=3.1061523082763745
Epoch 644
Epoch 644 :: Batch 0/1
Batch Loss = 3.102587144439794
644, epoch_train_loss=3.102587144439794
Epoch 645
Epoch 645 :: Batch 0/1
Batch Loss = 3.1016398026250225
645, epoch_train_loss=3.1016398026250225
Epoch 646
Epoch 646 :: Batch 0/1
Batch Loss = 3.099704903090031
646, epoch_train_loss=3.099704903090031
Epoch 647
Epoch 647 :: Batch 0/1
Batch Loss = 3.099292410775467
647, epoch_train_loss=3.099292410775467
Epoch 648
Epoch 648 :: Batch 0/1
Batch Loss = 3.098808946772817
648, epoch_train_loss=3.098808946772817
Epoch 649
Epoch 649 :: Batch 0/1
Batch Loss = 3.099524890676224
649, epoch_train_loss=3.099524890676224
Epoch 650
Epoch 650 :: Batch 0/1
Batch Loss = 3.1000447257609993
650, epoch_train_loss=3.1000447257609993
Epoch 651
Epoch 651 :: Batch 0/1
Batch Loss = 3.1027844586874265
651, epoch_train_loss=3.1027844586874265
Epoch 652
Epoch 652 :: Batch 0/1
Batch Loss = 3.103807281420193
652, epoch_train_loss=3.103807281420193
Epoch 653
Epoch 653 :: Batch 0/1
Batch Loss = 3.1091691914624753
653, epoch_train_loss=3.1091691914624753
Epoch 654
Epoch 654 :: Batch 0/1
Batch Loss = 3.1067618832341775
654, epoch_train_loss=3.1067618832341775
Epoch 655
Epoch 655 :: Batch 0/1
Batch Loss = 3.109622948914963
655, epoch_train_loss=3.109622948914963
Epoch 656
Epoch 656 :: Batch 0/1
Batch Loss = 3.1035204073521196
656, epoch_train_loss=3.1035204073521196
Epoch 657
Epoch 657 :: Batch 0/1
Batch Loss = 3.101524927053267
657, epoch_train_loss=3.101524927053267
Epoch 658
Epoch 658 :: Batch 0/1
Batch Loss = 3.098086916596279
658, epoch_train_loss=3.098086916596279
Epoch 659
Epoch 659 :: Batch 0/1
Batch Loss = 3.0967992737708467
659, epoch_train_loss=3.0967992737708467
Epoch 660
Epoch 660 :: Batch 0/1
Batch Loss = 3.0959513461240213
660, epoch_train_loss=3.0959513461240213
Epoch 661
Epoch 661 :: Batch 0/1
Batch Loss = 3.0956926833847205
661, epoch_train_loss=3.0956926833847205
Epoch 662
Epoch 662 :: Batch 0/1
Batch Loss = 3.095579472268434
662, epoch_train_loss=3.095579472268434
Epoch 663
Epoch 663 :: Batch 0/1
Batch Loss = 3.095943235618037
663, epoch_train_loss=3.095943235618037
Epoch 664
Epoch 664 :: Batch 0/1
Batch Loss = 3.09648800633233
664, epoch_train_loss=3.09648800633233
Epoch 665
Epoch 665 :: Batch 0/1
Batch Loss = 3.0982834977750913
665, epoch_train_loss=3.0982834977750913
Epoch 666
Epoch 666 :: Batch 0/1
Batch Loss = 3.1000503092446197
666, epoch_train_loss=3.1000503092446197
Epoch 667
Epoch 667 :: Batch 0/1
Batch Loss = 3.1067132659018957
667, epoch_train_loss=3.1067132659018957
Epoch 668
Epoch 668 :: Batch 0/1
Batch Loss = 3.1093412986584985
668, epoch_train_loss=3.1093412986584985
Epoch 669
Epoch 669 :: Batch 0/1
Batch Loss = 3.1256202190198104
669, epoch_train_loss=3.1256202190198104
Epoch 670
Epoch 670 :: Batch 0/1
Batch Loss = 3.111743831646098
670, epoch_train_loss=3.111743831646098
Epoch 671
Epoch 671 :: Batch 0/1
Batch Loss = 3.1076062645156526
671, epoch_train_loss=3.1076062645156526
Epoch 672
Epoch 672 :: Batch 0/1
Batch Loss = 3.0962278326969184
672, epoch_train_loss=3.0962278326969184
Epoch 673
Epoch 673 :: Batch 0/1
Batch Loss = 3.0938239670710663
673, epoch_train_loss=3.0938239670710663
Epoch 674
Epoch 674 :: Batch 0/1
Batch Loss = 3.0997394613605764
674, epoch_train_loss=3.0997394613605764
Epoch 675
Epoch 675 :: Batch 0/1
Batch Loss = 3.112591080331852
675, epoch_train_loss=3.112591080331852
Epoch 676
Epoch 676 :: Batch 0/1
Batch Loss = 3.1567193403128755
676, epoch_train_loss=3.1567193403128755
Epoch 677
Epoch 677 :: Batch 0/1
Batch Loss = 3.11937947517108
677, epoch_train_loss=3.11937947517108
Epoch 678
Epoch 678 :: Batch 0/1
Batch Loss = 3.103879661412074
678, epoch_train_loss=3.103879661412074
Epoch 679
Epoch 679 :: Batch 0/1
Batch Loss = 3.0998213946218205
679, epoch_train_loss=3.0998213946218205
Epoch 680
Epoch 680 :: Batch 0/1
Batch Loss = 3.114151249957158
680, epoch_train_loss=3.114151249957158
Epoch 681
Epoch 681 :: Batch 0/1
Batch Loss = 3.1444413254641033
681, epoch_train_loss=3.1444413254641033
Epoch 682
Epoch 682 :: Batch 0/1
Batch Loss = 3.112998402349506
682, epoch_train_loss=3.112998402349506
Epoch 683
Epoch 683 :: Batch 0/1
Batch Loss = 3.1038282391102703
683, epoch_train_loss=3.1038282391102703
Epoch 684
Epoch 684 :: Batch 0/1
Batch Loss = 3.0936518658311103
684, epoch_train_loss=3.0936518658311103
Epoch 685
Epoch 685 :: Batch 0/1
Batch Loss = 3.0995434660533236
685, epoch_train_loss=3.0995434660533236
Epoch 686
Epoch 686 :: Batch 0/1
Batch Loss = 3.112509493370141
686, epoch_train_loss=3.112509493370141
Epoch 687
Epoch 687 :: Batch 0/1
Batch Loss = 3.1013325622842784
687, epoch_train_loss=3.1013325622842784
Epoch 688
Epoch 688 :: Batch 0/1
Batch Loss = 3.0953487614807007
688, epoch_train_loss=3.0953487614807007
Epoch 689
Epoch 689 :: Batch 0/1
Batch Loss = 3.0935157244852602
689, epoch_train_loss=3.0935157244852602
Epoch 690
Epoch 690 :: Batch 0/1
Batch Loss = 3.099568932066651
690, epoch_train_loss=3.099568932066651
Epoch 691
Epoch 691 :: Batch 0/1
Batch Loss = 3.106885588000578
691, epoch_train_loss=3.106885588000578
Epoch 692
Epoch 692 :: Batch 0/1
Batch Loss = 3.102945497156075
692, epoch_train_loss=3.102945497156075
Epoch 693
Epoch 693 :: Batch 0/1
Batch Loss = 3.1011337188276524
693, epoch_train_loss=3.1011337188276524
Epoch 694
Epoch 694 :: Batch 0/1
Batch Loss = 3.093182860743924
694, epoch_train_loss=3.093182860743924
Epoch 695
Epoch 695 :: Batch 0/1
Batch Loss = 3.094895642538236
695, epoch_train_loss=3.094895642538236
Epoch 696
Epoch 696 :: Batch 0/1
Batch Loss = 3.099963058392613
696, epoch_train_loss=3.099963058392613
Epoch 697
Epoch 697 :: Batch 0/1
Batch Loss = 3.100949990787553
697, epoch_train_loss=3.100949990787553
Epoch 698
Epoch 698 :: Batch 0/1
Batch Loss = 3.1003113059544227
698, epoch_train_loss=3.1003113059544227
Epoch 699
Epoch 699 :: Batch 0/1
Batch Loss = 3.0950591784040795
699, epoch_train_loss=3.0950591784040795
Epoch 700
Epoch 700 :: Batch 0/1
Batch Loss = 3.0925602944919066
700, epoch_train_loss=3.0925602944919066
Epoch 701
Epoch 701 :: Batch 0/1
Batch Loss = 3.09076774253838
701, epoch_train_loss=3.09076774253838
Epoch 702
Epoch 702 :: Batch 0/1
Batch Loss = 3.0929663880186253
702, epoch_train_loss=3.0929663880186253
Epoch 703
Epoch 703 :: Batch 0/1
Batch Loss = 3.0953511161070657
703, epoch_train_loss=3.0953511161070657
Epoch 704
Epoch 704 :: Batch 0/1
Batch Loss = 3.096556114262444
704, epoch_train_loss=3.096556114262444
Epoch 705
Epoch 705 :: Batch 0/1
Batch Loss = 3.0972294877924735
705, epoch_train_loss=3.0972294877924735
Epoch 706
Epoch 706 :: Batch 0/1
Batch Loss = 3.0941448343478335
706, epoch_train_loss=3.0941448343478335
Epoch 707
Epoch 707 :: Batch 0/1
Batch Loss = 3.092213500690371
707, epoch_train_loss=3.092213500690371
Epoch 708
Epoch 708 :: Batch 0/1
Batch Loss = 3.089986128911655
708, epoch_train_loss=3.089986128911655
Epoch 709
Epoch 709 :: Batch 0/1
Batch Loss = 3.090189532568369
709, epoch_train_loss=3.090189532568369
Epoch 710
Epoch 710 :: Batch 0/1
Batch Loss = 3.090468640914838
710, epoch_train_loss=3.090468640914838
Epoch 711
Epoch 711 :: Batch 0/1
Batch Loss = 3.0924452039491372
711, epoch_train_loss=3.0924452039491372
Epoch 712
Epoch 712 :: Batch 0/1
Batch Loss = 3.0953652751130014
712, epoch_train_loss=3.0953652751130014
Epoch 713
Epoch 713 :: Batch 0/1
Batch Loss = 3.096513038988608
713, epoch_train_loss=3.096513038988608
Epoch 714
Epoch 714 :: Batch 0/1
Batch Loss = 3.101195303782174
714, epoch_train_loss=3.101195303782174
Epoch 715
Epoch 715 :: Batch 0/1
Batch Loss = 3.0962674661806107
715, epoch_train_loss=3.0962674661806107
Epoch 716
Epoch 716 :: Batch 0/1
Batch Loss = 3.094865227915794
716, epoch_train_loss=3.094865227915794
Epoch 717
Epoch 717 :: Batch 0/1
Batch Loss = 3.090958068369141
717, epoch_train_loss=3.090958068369141
Epoch 718
Epoch 718 :: Batch 0/1
Batch Loss = 3.089834203522338
718, epoch_train_loss=3.089834203522338
Epoch 719
Epoch 719 :: Batch 0/1
Batch Loss = 3.0885763747060464
719, epoch_train_loss=3.0885763747060464
Epoch 720
Epoch 720 :: Batch 0/1
Batch Loss = 3.0883782541864058
720, epoch_train_loss=3.0883782541864058
Epoch 721
Epoch 721 :: Batch 0/1
Batch Loss = 3.08818105918135
721, epoch_train_loss=3.08818105918135
Epoch 722
Epoch 722 :: Batch 0/1
Batch Loss = 3.088091893268652
722, epoch_train_loss=3.088091893268652
Epoch 723
Epoch 723 :: Batch 0/1
Batch Loss = 3.0885283389999656
723, epoch_train_loss=3.0885283389999656
Epoch 724
Epoch 724 :: Batch 0/1
Batch Loss = 3.0888566360113763
724, epoch_train_loss=3.0888566360113763
Epoch 725
Epoch 725 :: Batch 0/1
Batch Loss = 3.0904770297092137
725, epoch_train_loss=3.0904770297092137
Epoch 726
Epoch 726 :: Batch 0/1
Batch Loss = 3.091910372422253
726, epoch_train_loss=3.091910372422253
Epoch 727
Epoch 727 :: Batch 0/1
Batch Loss = 3.098084308000486
727, epoch_train_loss=3.098084308000486
Epoch 728
Epoch 728 :: Batch 0/1
Batch Loss = 3.1016815096423764
728, epoch_train_loss=3.1016815096423764
Epoch 729
Epoch 729 :: Batch 0/1
Batch Loss = 3.1198295580217903
729, epoch_train_loss=3.1198295580217903
Epoch 730
Epoch 730 :: Batch 0/1
Batch Loss = 3.102227631524312
730, epoch_train_loss=3.102227631524312
Epoch 731
Epoch 731 :: Batch 0/1
Batch Loss = 3.096113677257765
731, epoch_train_loss=3.096113677257765
Epoch 732
Epoch 732 :: Batch 0/1
Batch Loss = 3.087896274833011
732, epoch_train_loss=3.087896274833011
Epoch 733
Epoch 733 :: Batch 0/1
Batch Loss = 3.0878128670149785
733, epoch_train_loss=3.0878128670149785
Epoch 734
Epoch 734 :: Batch 0/1
Batch Loss = 3.096128644014073
734, epoch_train_loss=3.096128644014073
Epoch 735
Epoch 735 :: Batch 0/1
Batch Loss = 3.1086705656174822
735, epoch_train_loss=3.1086705656174822
Epoch 736
Epoch 736 :: Batch 0/1
Batch Loss = 3.154213859869428
736, epoch_train_loss=3.154213859869428
Epoch 737
Epoch 737 :: Batch 0/1
Batch Loss = 3.0990691395474363
737, epoch_train_loss=3.0990691395474363
Epoch 738
Epoch 738 :: Batch 0/1
Batch Loss = 3.100125712723996
738, epoch_train_loss=3.100125712723996
Epoch 739
Epoch 739 :: Batch 0/1
Batch Loss = 3.133788622683012
739, epoch_train_loss=3.133788622683012
Epoch 740
Epoch 740 :: Batch 0/1
Batch Loss = 3.0958266735547983
740, epoch_train_loss=3.0958266735547983
Epoch 741
Epoch 741 :: Batch 0/1
Batch Loss = 3.098781110570194
741, epoch_train_loss=3.098781110570194
Epoch 742
Epoch 742 :: Batch 0/1
Batch Loss = 3.150939674002243
742, epoch_train_loss=3.150939674002243
Epoch 743
Epoch 743 :: Batch 0/1
Batch Loss = 3.1097570394054186
743, epoch_train_loss=3.1097570394054186
Epoch 744
Epoch 744 :: Batch 0/1
Batch Loss = 3.0886235772887156
744, epoch_train_loss=3.0886235772887156
Epoch 745
Epoch 745 :: Batch 0/1
Batch Loss = 3.0969644802054557
745, epoch_train_loss=3.0969644802054557
Epoch 746
Epoch 746 :: Batch 0/1
Batch Loss = 3.1024894591074688
746, epoch_train_loss=3.1024894591074688
Epoch 747
Epoch 747 :: Batch 0/1
Batch Loss = 3.111076824508473
747, epoch_train_loss=3.111076824508473
Epoch 748
Epoch 748 :: Batch 0/1
Batch Loss = 3.088728045603238
748, epoch_train_loss=3.088728045603238
Epoch 749
Epoch 749 :: Batch 0/1
Batch Loss = 3.0981450661096783
749, epoch_train_loss=3.0981450661096783
Epoch 750
Epoch 750 :: Batch 0/1
Batch Loss = 3.1292461898445008
750, epoch_train_loss=3.1292461898445008
Epoch 751
Epoch 751 :: Batch 0/1
Batch Loss = 3.0920035799966556
751, epoch_train_loss=3.0920035799966556
Epoch 752
Epoch 752 :: Batch 0/1
Batch Loss = 3.1043262204879323
752, epoch_train_loss=3.1043262204879323
Epoch 753
Epoch 753 :: Batch 0/1
Batch Loss = 3.1483299853840876
753, epoch_train_loss=3.1483299853840876
Epoch 754
Epoch 754 :: Batch 0/1
Batch Loss = 3.0919641296046714
754, epoch_train_loss=3.0919641296046714
Epoch 755
Epoch 755 :: Batch 0/1
Batch Loss = 3.161129534887185
755, epoch_train_loss=3.161129534887185
Epoch 756
Epoch 756 :: Batch 0/1
Batch Loss = 3.2713081862766273
756, epoch_train_loss=3.2713081862766273
Epoch 757
Epoch 757 :: Batch 0/1
Batch Loss = 3.2160409416676847
757, epoch_train_loss=3.2160409416676847
Epoch 758
Epoch 758 :: Batch 0/1
Batch Loss = 3.1808107406400783
758, epoch_train_loss=3.1808107406400783
Epoch 759
Epoch 759 :: Batch 0/1
Batch Loss = 3.2649206179652266
759, epoch_train_loss=3.2649206179652266
Epoch 760
Epoch 760 :: Batch 0/1
Batch Loss = 3.2340378048749385
760, epoch_train_loss=3.2340378048749385
Epoch 761
Epoch 761 :: Batch 0/1
Batch Loss = 3.291882611806031
761, epoch_train_loss=3.291882611806031
Epoch 762
Epoch 762 :: Batch 0/1
Batch Loss = 3.196544042346138
762, epoch_train_loss=3.196544042346138
Epoch 763
Epoch 763 :: Batch 0/1
Batch Loss = 3.262882462702711
763, epoch_train_loss=3.262882462702711
Epoch 764
Epoch 764 :: Batch 0/1
Batch Loss = 3.1626015361720334
764, epoch_train_loss=3.1626015361720334
Epoch 765
Epoch 765 :: Batch 0/1
Batch Loss = 3.2199925588076814
765, epoch_train_loss=3.2199925588076814
Epoch 766
Epoch 766 :: Batch 0/1
Batch Loss = 3.1233449844715877
766, epoch_train_loss=3.1233449844715877
Epoch 767
Epoch 767 :: Batch 0/1
Batch Loss = 3.2578112779733814
767, epoch_train_loss=3.2578112779733814
Epoch 768
Epoch 768 :: Batch 0/1
Batch Loss = 3.148762588979647
768, epoch_train_loss=3.148762588979647
Epoch 769
Epoch 769 :: Batch 0/1
Batch Loss = 3.2365741230787215
769, epoch_train_loss=3.2365741230787215
Epoch 770
Epoch 770 :: Batch 0/1
Batch Loss = 3.1216000088375693
770, epoch_train_loss=3.1216000088375693
Epoch 771
Epoch 771 :: Batch 0/1
Batch Loss = 3.226569412277497
771, epoch_train_loss=3.226569412277497
Epoch 772
Epoch 772 :: Batch 0/1
Batch Loss = 3.1220858589759755
772, epoch_train_loss=3.1220858589759755
Epoch 773
Epoch 773 :: Batch 0/1
Batch Loss = 3.178384036892448
773, epoch_train_loss=3.178384036892448
Epoch 774
Epoch 774 :: Batch 0/1
Batch Loss = 3.1299022032414383
774, epoch_train_loss=3.1299022032414383
Epoch 775
Epoch 775 :: Batch 0/1
Batch Loss = 3.1584742628158398
775, epoch_train_loss=3.1584742628158398
Epoch 776
Epoch 776 :: Batch 0/1
Batch Loss = 3.100679683590397
776, epoch_train_loss=3.100679683590397
Epoch 777
Epoch 777 :: Batch 0/1
Batch Loss = 3.125500250099099
777, epoch_train_loss=3.125500250099099
Epoch 778
Epoch 778 :: Batch 0/1
Batch Loss = 3.1090484555663194
778, epoch_train_loss=3.1090484555663194
Epoch 779
Epoch 779 :: Batch 0/1
Batch Loss = 3.123798758260294
779, epoch_train_loss=3.123798758260294
Epoch 780
Epoch 780 :: Batch 0/1
Batch Loss = 3.0957894281363267
780, epoch_train_loss=3.0957894281363267
Epoch 781
Epoch 781 :: Batch 0/1
Batch Loss = 3.1109430373800206
781, epoch_train_loss=3.1109430373800206
Epoch 782
Epoch 782 :: Batch 0/1
Batch Loss = 3.0964644094165736
782, epoch_train_loss=3.0964644094165736
Epoch 783
Epoch 783 :: Batch 0/1
Batch Loss = 3.113811145925805
783, epoch_train_loss=3.113811145925805
Epoch 784
Epoch 784 :: Batch 0/1
Batch Loss = 3.094221111901352
784, epoch_train_loss=3.094221111901352
Epoch 785
Epoch 785 :: Batch 0/1
Batch Loss = 3.104203408814312
785, epoch_train_loss=3.104203408814312
Epoch 786
Epoch 786 :: Batch 0/1
Batch Loss = 3.092222972847484
786, epoch_train_loss=3.092222972847484
Epoch 787
Epoch 787 :: Batch 0/1
Batch Loss = 3.101313552309936
787, epoch_train_loss=3.101313552309936
Epoch 788
Epoch 788 :: Batch 0/1
Batch Loss = 3.0949714710940643
788, epoch_train_loss=3.0949714710940643
Epoch 789
Epoch 789 :: Batch 0/1
Batch Loss = 3.0971964979067383
789, epoch_train_loss=3.0971964979067383
Epoch 790
Epoch 790 :: Batch 0/1
Batch Loss = 3.0937406247630372
790, epoch_train_loss=3.0937406247630372
Epoch 791
Epoch 791 :: Batch 0/1
Batch Loss = 3.0906678987421636
791, epoch_train_loss=3.0906678987421636
Epoch 792
Epoch 792 :: Batch 0/1
Batch Loss = 3.095650110729188
792, epoch_train_loss=3.095650110729188
Epoch 793
Epoch 793 :: Batch 0/1
Batch Loss = 3.0890098514287665
793, epoch_train_loss=3.0890098514287665
Epoch 794
Epoch 794 :: Batch 0/1
Batch Loss = 3.096382223606265
794, epoch_train_loss=3.096382223606265
Epoch 795
Epoch 795 :: Batch 0/1
Batch Loss = 3.088807015587095
795, epoch_train_loss=3.088807015587095
Epoch 796
Epoch 796 :: Batch 0/1
Batch Loss = 3.0918662999026143
796, epoch_train_loss=3.0918662999026143
Epoch 797
Epoch 797 :: Batch 0/1
Batch Loss = 3.0897494983647165
797, epoch_train_loss=3.0897494983647165
Epoch 798
Epoch 798 :: Batch 0/1
Batch Loss = 3.0883778326584364
798, epoch_train_loss=3.0883778326584364
Epoch 799
Epoch 799 :: Batch 0/1
Batch Loss = 3.090420204792442
799, epoch_train_loss=3.090420204792442
Epoch 800
Epoch 800 :: Batch 0/1
Batch Loss = 3.0865858506515784
800, epoch_train_loss=3.0865858506515784
Epoch 801
Epoch 801 :: Batch 0/1
Batch Loss = 3.091235218023588
801, epoch_train_loss=3.091235218023588
Epoch 802
Epoch 802 :: Batch 0/1
Batch Loss = 3.0872938855076373
802, epoch_train_loss=3.0872938855076373
Epoch 803
Epoch 803 :: Batch 0/1
Batch Loss = 3.087670342766677
803, epoch_train_loss=3.087670342766677
Epoch 804
Epoch 804 :: Batch 0/1
Batch Loss = 3.0887066926951654
804, epoch_train_loss=3.0887066926951654
Epoch 805
Epoch 805 :: Batch 0/1
Batch Loss = 3.0856819789558325
805, epoch_train_loss=3.0856819789558325
Epoch 806
Epoch 806 :: Batch 0/1
Batch Loss = 3.0875486667300494
806, epoch_train_loss=3.0875486667300494
Epoch 807
Epoch 807 :: Batch 0/1
Batch Loss = 3.0855925002176496
807, epoch_train_loss=3.0855925002176496
Epoch 808
Epoch 808 :: Batch 0/1
Batch Loss = 3.086365631898314
808, epoch_train_loss=3.086365631898314
Epoch 809
Epoch 809 :: Batch 0/1
Batch Loss = 3.0863938908808857
809, epoch_train_loss=3.0863938908808857
Epoch 810
Epoch 810 :: Batch 0/1
Batch Loss = 3.0848631961305877
810, epoch_train_loss=3.0848631961305877
Epoch 811
Epoch 811 :: Batch 0/1
Batch Loss = 3.086355169866022
811, epoch_train_loss=3.086355169866022
Epoch 812
Epoch 812 :: Batch 0/1
Batch Loss = 3.084966425505767
812, epoch_train_loss=3.084966425505767
Epoch 813
Epoch 813 :: Batch 0/1
Batch Loss = 3.08490250902716
813, epoch_train_loss=3.08490250902716
Epoch 814
Epoch 814 :: Batch 0/1
Batch Loss = 3.0851162165187196
814, epoch_train_loss=3.0851162165187196
Epoch 815
Epoch 815 :: Batch 0/1
Batch Loss = 3.0839880948559486
815, epoch_train_loss=3.0839880948559486
Epoch 816
Epoch 816 :: Batch 0/1
Batch Loss = 3.084609656240889
816, epoch_train_loss=3.084609656240889
Epoch 817
Epoch 817 :: Batch 0/1
Batch Loss = 3.0838485671149805
817, epoch_train_loss=3.0838485671149805
Epoch 818
Epoch 818 :: Batch 0/1
Batch Loss = 3.0835867129807055
818, epoch_train_loss=3.0835867129807055
Epoch 819
Epoch 819 :: Batch 0/1
Batch Loss = 3.083895824237894
819, epoch_train_loss=3.083895824237894
Epoch 820
Epoch 820 :: Batch 0/1
Batch Loss = 3.0831074498422173
820, epoch_train_loss=3.0831074498422173
Epoch 821
Epoch 821 :: Batch 0/1
Batch Loss = 3.0833678683207215
821, epoch_train_loss=3.0833678683207215
Epoch 822
Epoch 822 :: Batch 0/1
Batch Loss = 3.0830949817931805
822, epoch_train_loss=3.0830949817931805
Epoch 823
Epoch 823 :: Batch 0/1
Batch Loss = 3.0826583629744637
823, epoch_train_loss=3.0826583629744637
Epoch 824
Epoch 824 :: Batch 0/1
Batch Loss = 3.082926422397272
824, epoch_train_loss=3.082926422397272
Epoch 825
Epoch 825 :: Batch 0/1
Batch Loss = 3.0823945763305782
825, epoch_train_loss=3.0823945763305782
Epoch 826
Epoch 826 :: Batch 0/1
Batch Loss = 3.082275522736173
826, epoch_train_loss=3.082275522736173
Epoch 827
Epoch 827 :: Batch 0/1
Batch Loss = 3.082318911749374
827, epoch_train_loss=3.082318911749374
Epoch 828
Epoch 828 :: Batch 0/1
Batch Loss = 3.081831033008338
828, epoch_train_loss=3.081831033008338
Epoch 829
Epoch 829 :: Batch 0/1
Batch Loss = 3.081845489511437
829, epoch_train_loss=3.081845489511437
Epoch 830
Epoch 830 :: Batch 0/1
Batch Loss = 3.0816925009879372
830, epoch_train_loss=3.0816925009879372
Epoch 831
Epoch 831 :: Batch 0/1
Batch Loss = 3.0813543523618026
831, epoch_train_loss=3.0813543523618026
Epoch 832
Epoch 832 :: Batch 0/1
Batch Loss = 3.0813681521756515
832, epoch_train_loss=3.0813681521756515
Epoch 833
Epoch 833 :: Batch 0/1
Batch Loss = 3.0811203771865254
833, epoch_train_loss=3.0811203771865254
Epoch 834
Epoch 834 :: Batch 0/1
Batch Loss = 3.0809071604634144
834, epoch_train_loss=3.0809071604634144
Epoch 835
Epoch 835 :: Batch 0/1
Batch Loss = 3.0808693396263322
835, epoch_train_loss=3.0808693396263322
Epoch 836
Epoch 836 :: Batch 0/1
Batch Loss = 3.0806031529189357
836, epoch_train_loss=3.0806031529189357
Epoch 837
Epoch 837 :: Batch 0/1
Batch Loss = 3.080455428264154
837, epoch_train_loss=3.080455428264154
Epoch 838
Epoch 838 :: Batch 0/1
Batch Loss = 3.0803697162304027
838, epoch_train_loss=3.0803697162304027
Epoch 839
Epoch 839 :: Batch 0/1
Batch Loss = 3.080120483743855
839, epoch_train_loss=3.080120483743855
Epoch 840
Epoch 840 :: Batch 0/1
Batch Loss = 3.079988965101949
840, epoch_train_loss=3.079988965101949
Epoch 841
Epoch 841 :: Batch 0/1
Batch Loss = 3.07986858560511
841, epoch_train_loss=3.07986858560511
Epoch 842
Epoch 842 :: Batch 0/1
Batch Loss = 3.079647130052056
842, epoch_train_loss=3.079647130052056
Epoch 843
Epoch 843 :: Batch 0/1
Batch Loss = 3.0795195538873292
843, epoch_train_loss=3.0795195538873292
Epoch 844
Epoch 844 :: Batch 0/1
Batch Loss = 3.0793775424597
844, epoch_train_loss=3.0793775424597
Epoch 845
Epoch 845 :: Batch 0/1
Batch Loss = 3.0791727713230346
845, epoch_train_loss=3.0791727713230346
Epoch 846
Epoch 846 :: Batch 0/1
Batch Loss = 3.07904241824949
846, epoch_train_loss=3.07904241824949
Epoch 847
Epoch 847 :: Batch 0/1
Batch Loss = 3.0788966870996455
847, epoch_train_loss=3.0788966870996455
Epoch 848
Epoch 848 :: Batch 0/1
Batch Loss = 3.0787005320183165
848, epoch_train_loss=3.0787005320183165
Epoch 849
Epoch 849 :: Batch 0/1
Batch Loss = 3.0785601295922027
849, epoch_train_loss=3.0785601295922027
Epoch 850
Epoch 850 :: Batch 0/1
Batch Loss = 3.0784184035963973
850, epoch_train_loss=3.0784184035963973
Epoch 851
Epoch 851 :: Batch 0/1
Batch Loss = 3.0782305003136208
851, epoch_train_loss=3.0782305003136208
Epoch 852
Epoch 852 :: Batch 0/1
Batch Loss = 3.078078789588158
852, epoch_train_loss=3.078078789588158
Epoch 853
Epoch 853 :: Batch 0/1
Batch Loss = 3.077940536772111
853, epoch_train_loss=3.077940536772111
Epoch 854
Epoch 854 :: Batch 0/1
Batch Loss = 3.0777622256334234
854, epoch_train_loss=3.0777622256334234
Epoch 855
Epoch 855 :: Batch 0/1
Batch Loss = 3.0775994015743287
855, epoch_train_loss=3.0775994015743287
Epoch 856
Epoch 856 :: Batch 0/1
Batch Loss = 3.077460158121718
856, epoch_train_loss=3.077460158121718
Epoch 857
Epoch 857 :: Batch 0/1
Batch Loss = 3.0772937551067896
857, epoch_train_loss=3.0772937551067896
Epoch 858
Epoch 858 :: Batch 0/1
Batch Loss = 3.0771244957927992
858, epoch_train_loss=3.0771244957927992
Epoch 859
Epoch 859 :: Batch 0/1
Batch Loss = 3.076977960873928
859, epoch_train_loss=3.076977960873928
Epoch 860
Epoch 860 :: Batch 0/1
Batch Loss = 3.076822414917957
860, epoch_train_loss=3.076822414917957
Epoch 861
Epoch 861 :: Batch 0/1
Batch Loss = 3.0766536785744227
861, epoch_train_loss=3.0766536785744227
Epoch 862
Epoch 862 :: Batch 0/1
Batch Loss = 3.0764962779971134
862, epoch_train_loss=3.0764962779971134
Epoch 863
Epoch 863 :: Batch 0/1
Batch Loss = 3.0763456472757307
863, epoch_train_loss=3.0763456472757307
Epoch 864
Epoch 864 :: Batch 0/1
Batch Loss = 3.0761836373421643
864, epoch_train_loss=3.0761836373421643
Epoch 865
Epoch 865 :: Batch 0/1
Batch Loss = 3.076018943021365
865, epoch_train_loss=3.076018943021365
Epoch 866
Epoch 866 :: Batch 0/1
Batch Loss = 3.075864682305824
866, epoch_train_loss=3.075864682305824
Epoch 867
Epoch 867 :: Batch 0/1
Batch Loss = 3.0757090242500498
867, epoch_train_loss=3.0757090242500498
Epoch 868
Epoch 868 :: Batch 0/1
Batch Loss = 3.0755454583138735
868, epoch_train_loss=3.0755454583138735
Epoch 869
Epoch 869 :: Batch 0/1
Batch Loss = 3.075384074929182
869, epoch_train_loss=3.075384074929182
Epoch 870
Epoch 870 :: Batch 0/1
Batch Loss = 3.07522791241244
870, epoch_train_loss=3.07522791241244
Epoch 871
Epoch 871 :: Batch 0/1
Batch Loss = 3.0750698789096265
871, epoch_train_loss=3.0750698789096265
Epoch 872
Epoch 872 :: Batch 0/1
Batch Loss = 3.074907103991694
872, epoch_train_loss=3.074907103991694
Epoch 873
Epoch 873 :: Batch 0/1
Batch Loss = 3.074745612856482
873, epoch_train_loss=3.074745612856482
Epoch 874
Epoch 874 :: Batch 0/1
Batch Loss = 3.0745876112005743
874, epoch_train_loss=3.0745876112005743
Epoch 875
Epoch 875 :: Batch 0/1
Batch Loss = 3.0744287341772822
875, epoch_train_loss=3.0744287341772822
Epoch 876
Epoch 876 :: Batch 0/1
Batch Loss = 3.0742662423728646
876, epoch_train_loss=3.0742662423728646
Epoch 877
Epoch 877 :: Batch 0/1
Batch Loss = 3.0741037016590664
877, epoch_train_loss=3.0741037016590664
Epoch 878
Epoch 878 :: Batch 0/1
Batch Loss = 3.073943963227038
878, epoch_train_loss=3.073943963227038
Epoch 879
Epoch 879 :: Batch 0/1
Batch Loss = 3.073784024961076
879, epoch_train_loss=3.073784024961076
Epoch 880
Epoch 880 :: Batch 0/1
Batch Loss = 3.073621855340609
880, epoch_train_loss=3.073621855340609
Epoch 881
Epoch 881 :: Batch 0/1
Batch Loss = 3.073458641064729
881, epoch_train_loss=3.073458641064729
Epoch 882
Epoch 882 :: Batch 0/1
Batch Loss = 3.0732965408602344
882, epoch_train_loss=3.0732965408602344
Epoch 883
Epoch 883 :: Batch 0/1
Batch Loss = 3.0731351134361766
883, epoch_train_loss=3.0731351134361766
Epoch 884
Epoch 884 :: Batch 0/1
Batch Loss = 3.072973155573442
884, epoch_train_loss=3.072973155573442
Epoch 885
Epoch 885 :: Batch 0/1
Batch Loss = 3.0728097360519397
885, epoch_train_loss=3.0728097360519397
Epoch 886
Epoch 886 :: Batch 0/1
Batch Loss = 3.072645707385745
886, epoch_train_loss=3.072645707385745
Epoch 887
Epoch 887 :: Batch 0/1
Batch Loss = 3.0724821808539784
887, epoch_train_loss=3.0724821808539784
Epoch 888
Epoch 888 :: Batch 0/1
Batch Loss = 3.0723190710641806
888, epoch_train_loss=3.0723190710641806
Epoch 889
Epoch 889 :: Batch 0/1
Batch Loss = 3.072155447961369
889, epoch_train_loss=3.072155447961369
Epoch 890
Epoch 890 :: Batch 0/1
Batch Loss = 3.0719910338438257
890, epoch_train_loss=3.0719910338438257
Epoch 891
Epoch 891 :: Batch 0/1
Batch Loss = 3.0718259268260706
891, epoch_train_loss=3.0718259268260706
Epoch 892
Epoch 892 :: Batch 0/1
Batch Loss = 3.0716605764545797
892, epoch_train_loss=3.0716605764545797
Epoch 893
Epoch 893 :: Batch 0/1
Batch Loss = 3.071495325837393
893, epoch_train_loss=3.071495325837393
Epoch 894
Epoch 894 :: Batch 0/1
Batch Loss = 3.071330096977074
894, epoch_train_loss=3.071330096977074
Epoch 895
Epoch 895 :: Batch 0/1
Batch Loss = 3.071164427412157
895, epoch_train_loss=3.071164427412157
Epoch 896
Epoch 896 :: Batch 0/1
Batch Loss = 3.070998276945766
896, epoch_train_loss=3.070998276945766
Epoch 897
Epoch 897 :: Batch 0/1
Batch Loss = 3.070831536582686
897, epoch_train_loss=3.070831536582686
Epoch 898
Epoch 898 :: Batch 0/1
Batch Loss = 3.070664363636198
898, epoch_train_loss=3.070664363636198
Epoch 899
Epoch 899 :: Batch 0/1
Batch Loss = 3.0704968949184366
899, epoch_train_loss=3.0704968949184366
Epoch 900
Epoch 900 :: Batch 0/1
Batch Loss = 3.0703292322884312
900, epoch_train_loss=3.0703292322884312
Epoch 901
Epoch 901 :: Batch 0/1
Batch Loss = 3.070161311176457
901, epoch_train_loss=3.070161311176457
Epoch 902
Epoch 902 :: Batch 0/1
Batch Loss = 3.06999312329344
902, epoch_train_loss=3.06999312329344
Epoch 903
Epoch 903 :: Batch 0/1
Batch Loss = 3.069824644003825
903, epoch_train_loss=3.069824644003825
Epoch 904
Epoch 904 :: Batch 0/1
Batch Loss = 3.0696557652089735
904, epoch_train_loss=3.0696557652089735
Epoch 905
Epoch 905 :: Batch 0/1
Batch Loss = 3.069486559460464
905, epoch_train_loss=3.069486559460464
Epoch 906
Epoch 906 :: Batch 0/1
Batch Loss = 3.0693169761501937
906, epoch_train_loss=3.0693169761501937
Epoch 907
Epoch 907 :: Batch 0/1
Batch Loss = 3.0691470650699686
907, epoch_train_loss=3.0691470650699686
Epoch 908
Epoch 908 :: Batch 0/1
Batch Loss = 3.0689768814557628
908, epoch_train_loss=3.0689768814557628
Epoch 909
Epoch 909 :: Batch 0/1
Batch Loss = 3.0688065059028156
909, epoch_train_loss=3.0688065059028156
Epoch 910
Epoch 910 :: Batch 0/1
Batch Loss = 3.068636072661238
910, epoch_train_loss=3.068636072661238
Epoch 911
Epoch 911 :: Batch 0/1
Batch Loss = 3.068465935486609
911, epoch_train_loss=3.068465935486609
Epoch 912
Epoch 912 :: Batch 0/1
Batch Loss = 3.06829673598379
912, epoch_train_loss=3.06829673598379
Epoch 913
Epoch 913 :: Batch 0/1
Batch Loss = 3.068129828715219
913, epoch_train_loss=3.068129828715219
Epoch 914
Epoch 914 :: Batch 0/1
Batch Loss = 3.067968789950159
914, epoch_train_loss=3.067968789950159
Epoch 915
Epoch 915 :: Batch 0/1
Batch Loss = 3.0678201364298805
915, epoch_train_loss=3.0678201364298805
Epoch 916
Epoch 916 :: Batch 0/1
Batch Loss = 3.0677057711974376
916, epoch_train_loss=3.0677057711974376
Epoch 917
Epoch 917 :: Batch 0/1
Batch Loss = 3.067660517135525
917, epoch_train_loss=3.067660517135525
Epoch 918
Epoch 918 :: Batch 0/1
Batch Loss = 3.067843923336699
918, epoch_train_loss=3.067843923336699
Epoch 919
Epoch 919 :: Batch 0/1
Batch Loss = 3.0684204617733792
919, epoch_train_loss=3.0684204617733792
Epoch 920
Epoch 920 :: Batch 0/1
Batch Loss = 3.070760180537539
920, epoch_train_loss=3.070760180537539
Epoch 921
Epoch 921 :: Batch 0/1
Batch Loss = 3.0744235720239566
921, epoch_train_loss=3.0744235720239566
Epoch 922
Epoch 922 :: Batch 0/1
Batch Loss = 3.090779836026448
922, epoch_train_loss=3.090779836026448
Epoch 923
Epoch 923 :: Batch 0/1
Batch Loss = 3.087210843544558
923, epoch_train_loss=3.087210843544558
Epoch 924
Epoch 924 :: Batch 0/1
Batch Loss = 3.1072488726769394
924, epoch_train_loss=3.1072488726769394
Epoch 925
Epoch 925 :: Batch 0/1
Batch Loss = 3.0754901755143575
925, epoch_train_loss=3.0754901755143575
Epoch 926
Epoch 926 :: Batch 0/1
Batch Loss = 3.067419743869107
926, epoch_train_loss=3.067419743869107
Epoch 927
Epoch 927 :: Batch 0/1
Batch Loss = 3.081042420652094
927, epoch_train_loss=3.081042420652094
Epoch 928
Epoch 928 :: Batch 0/1
Batch Loss = 3.0958581849644844
928, epoch_train_loss=3.0958581849644844
Epoch 929
Epoch 929 :: Batch 0/1
Batch Loss = 3.164924122973243
929, epoch_train_loss=3.164924122973243
Epoch 930
Epoch 930 :: Batch 0/1
Batch Loss = 3.0770014542680384
930, epoch_train_loss=3.0770014542680384
Epoch 931
Epoch 931 :: Batch 0/1
Batch Loss = 3.3518204787860784
931, epoch_train_loss=3.3518204787860784
Epoch 932
Epoch 932 :: Batch 0/1
Batch Loss = 3.475409124407612
932, epoch_train_loss=3.475409124407612
Epoch 933
Epoch 933 :: Batch 0/1
Batch Loss = 3.445073171765956
933, epoch_train_loss=3.445073171765956
Epoch 934
Epoch 934 :: Batch 0/1
Batch Loss = 3.3810446108525607
934, epoch_train_loss=3.3810446108525607
Epoch 935
Epoch 935 :: Batch 0/1
Batch Loss = 3.354914093460198
935, epoch_train_loss=3.354914093460198
Epoch 936
Epoch 936 :: Batch 0/1
Batch Loss = 3.358293421246916
936, epoch_train_loss=3.358293421246916
Epoch 937
Epoch 937 :: Batch 0/1
Batch Loss = 3.355798484648521
937, epoch_train_loss=3.355798484648521
Epoch 938
Epoch 938 :: Batch 0/1
Batch Loss = 3.3353880266984737
938, epoch_train_loss=3.3353880266984737
Epoch 939
Epoch 939 :: Batch 0/1
Batch Loss = 3.2994681091207654
939, epoch_train_loss=3.2994681091207654
Epoch 940
Epoch 940 :: Batch 0/1
Batch Loss = 3.290113658845807
940, epoch_train_loss=3.290113658845807
Epoch 941
Epoch 941 :: Batch 0/1
Batch Loss = 3.2635783779802088
941, epoch_train_loss=3.2635783779802088
Epoch 942
Epoch 942 :: Batch 0/1
Batch Loss = 3.263916860598218
942, epoch_train_loss=3.263916860598218
Epoch 943
Epoch 943 :: Batch 0/1
Batch Loss = 3.2361825229119283
943, epoch_train_loss=3.2361825229119283
Epoch 944
Epoch 944 :: Batch 0/1
Batch Loss = 3.219036816107707
944, epoch_train_loss=3.219036816107707
Epoch 945
Epoch 945 :: Batch 0/1
Batch Loss = 3.239325941517015
945, epoch_train_loss=3.239325941517015
Epoch 946
Epoch 946 :: Batch 0/1
Batch Loss = 3.261379221117852
946, epoch_train_loss=3.261379221117852
Epoch 947
Epoch 947 :: Batch 0/1
Batch Loss = 3.204252164797122
947, epoch_train_loss=3.204252164797122
Epoch 948
Epoch 948 :: Batch 0/1
Batch Loss = 3.390816466228615
948, epoch_train_loss=3.390816466228615
Epoch 949
Epoch 949 :: Batch 0/1
Batch Loss = 3.461241213845184
949, epoch_train_loss=3.461241213845184
Epoch 950
Epoch 950 :: Batch 0/1
Batch Loss = 3.5497383117280426
950, epoch_train_loss=3.5497383117280426
Epoch 951
Epoch 951 :: Batch 0/1
Batch Loss = 3.3287569962682024
951, epoch_train_loss=3.3287569962682024
Epoch 952
Epoch 952 :: Batch 0/1
Batch Loss = 3.304651037918439
952, epoch_train_loss=3.304651037918439
Epoch 953
Epoch 953 :: Batch 0/1
Batch Loss = 3.2698001427903716
953, epoch_train_loss=3.2698001427903716
Epoch 954
Epoch 954 :: Batch 0/1
Batch Loss = 3.278775030740857
954, epoch_train_loss=3.278775030740857
Epoch 955
Epoch 955 :: Batch 0/1
Batch Loss = 3.225817396092813
955, epoch_train_loss=3.225817396092813
Epoch 956
Epoch 956 :: Batch 0/1
Batch Loss = 3.2824189504514436
956, epoch_train_loss=3.2824189504514436
Epoch 957
Epoch 957 :: Batch 0/1
Batch Loss = 3.2425199179912383
957, epoch_train_loss=3.2425199179912383
Epoch 958
Epoch 958 :: Batch 0/1
Batch Loss = 3.2704483043069863
958, epoch_train_loss=3.2704483043069863
Epoch 959
Epoch 959 :: Batch 0/1
Batch Loss = 3.233246734420666
959, epoch_train_loss=3.233246734420666
Epoch 960
Epoch 960 :: Batch 0/1
Batch Loss = 3.2619912363308265
960, epoch_train_loss=3.2619912363308265
Epoch 961
Epoch 961 :: Batch 0/1
Batch Loss = 3.2347918460342875
961, epoch_train_loss=3.2347918460342875
Epoch 962
Epoch 962 :: Batch 0/1
Batch Loss = 3.23445642799792
962, epoch_train_loss=3.23445642799792
Epoch 963
Epoch 963 :: Batch 0/1
Batch Loss = 3.2435016207608833
963, epoch_train_loss=3.2435016207608833
Epoch 964
Epoch 964 :: Batch 0/1
Batch Loss = 3.243809608294352
964, epoch_train_loss=3.243809608294352
Epoch 965
Epoch 965 :: Batch 0/1
Batch Loss = 3.231557999853378
965, epoch_train_loss=3.231557999853378
Epoch 966
Epoch 966 :: Batch 0/1
Batch Loss = 3.2154418825622324
966, epoch_train_loss=3.2154418825622324
Epoch 967
Epoch 967 :: Batch 0/1
Batch Loss = 3.2291163373607956
967, epoch_train_loss=3.2291163373607956
Epoch 968
Epoch 968 :: Batch 0/1
Batch Loss = 3.217238854475335
968, epoch_train_loss=3.217238854475335
Epoch 969
Epoch 969 :: Batch 0/1
Batch Loss = 3.213915079435348
969, epoch_train_loss=3.213915079435348
Epoch 970
Epoch 970 :: Batch 0/1
Batch Loss = 3.2039306191775783
970, epoch_train_loss=3.2039306191775783
Epoch 971
Epoch 971 :: Batch 0/1
Batch Loss = 3.2108455473120823
971, epoch_train_loss=3.2108455473120823
Epoch 972
Epoch 972 :: Batch 0/1
Batch Loss = 3.195912105761637
972, epoch_train_loss=3.195912105761637
Epoch 973
Epoch 973 :: Batch 0/1
Batch Loss = 3.197044228923691
973, epoch_train_loss=3.197044228923691
Epoch 974
Epoch 974 :: Batch 0/1
Batch Loss = 3.1974854106258306
974, epoch_train_loss=3.1974854106258306
Epoch 975
Epoch 975 :: Batch 0/1
Batch Loss = 3.1973703963761495
975, epoch_train_loss=3.1973703963761495
Epoch 976
Epoch 976 :: Batch 0/1
Batch Loss = 3.19313612948639
976, epoch_train_loss=3.19313612948639
Epoch 977
Epoch 977 :: Batch 0/1
Batch Loss = 3.1940892985369507
977, epoch_train_loss=3.1940892985369507
Epoch 978
Epoch 978 :: Batch 0/1
Batch Loss = 3.1950081699849253
978, epoch_train_loss=3.1950081699849253
Epoch 979
Epoch 979 :: Batch 0/1
Batch Loss = 3.191404089974157
979, epoch_train_loss=3.191404089974157
Epoch 980
Epoch 980 :: Batch 0/1
Batch Loss = 3.188654663203393
980, epoch_train_loss=3.188654663203393
Epoch 981
Epoch 981 :: Batch 0/1
Batch Loss = 3.1886870849242794
981, epoch_train_loss=3.1886870849242794
Epoch 982
Epoch 982 :: Batch 0/1
Batch Loss = 3.188713768330895
982, epoch_train_loss=3.188713768330895
Epoch 983
Epoch 983 :: Batch 0/1
Batch Loss = 3.186559591103321
983, epoch_train_loss=3.186559591103321
Epoch 984
Epoch 984 :: Batch 0/1
Batch Loss = 3.187773426012395
984, epoch_train_loss=3.187773426012395
Epoch 985
Epoch 985 :: Batch 0/1
Batch Loss = 3.1864743735789878
985, epoch_train_loss=3.1864743735789878
Epoch 986
Epoch 986 :: Batch 0/1
Batch Loss = 3.1850239634387
986, epoch_train_loss=3.1850239634387
Epoch 987
Epoch 987 :: Batch 0/1
Batch Loss = 3.185864549184055
987, epoch_train_loss=3.185864549184055
Epoch 988
Epoch 988 :: Batch 0/1
Batch Loss = 3.1826134175739145
988, epoch_train_loss=3.1826134175739145
Epoch 989
Epoch 989 :: Batch 0/1
Batch Loss = 3.1815003417795915
989, epoch_train_loss=3.1815003417795915
Epoch 990
Epoch 990 :: Batch 0/1
Batch Loss = 3.181145346472763
990, epoch_train_loss=3.181145346472763
Epoch 991
Epoch 991 :: Batch 0/1
Batch Loss = 3.179266373946308
991, epoch_train_loss=3.179266373946308
Epoch 992
Epoch 992 :: Batch 0/1
Batch Loss = 3.1790485628969307
992, epoch_train_loss=3.1790485628969307
Epoch 993
Epoch 993 :: Batch 0/1
Batch Loss = 3.1778210898850827
993, epoch_train_loss=3.1778210898850827
Epoch 994
Epoch 994 :: Batch 0/1
Batch Loss = 3.1767536739228794
994, epoch_train_loss=3.1767536739228794
Epoch 995
Epoch 995 :: Batch 0/1
Batch Loss = 3.1768458974908906
995, epoch_train_loss=3.1768458974908906
Epoch 996
Epoch 996 :: Batch 0/1
Batch Loss = 3.1759589250545894
996, epoch_train_loss=3.1759589250545894
Epoch 997
Epoch 997 :: Batch 0/1
Batch Loss = 3.1747710161944704
997, epoch_train_loss=3.1747710161944704
Epoch 998
Epoch 998 :: Batch 0/1
Batch Loss = 3.174074955826755
998, epoch_train_loss=3.174074955826755
Epoch 999
Epoch 999 :: Batch 0/1
Batch Loss = 3.173079365477706
999, epoch_train_loss=3.173079365477706
Epoch 1000
Epoch 1000 :: Batch 0/1
Batch Loss = 3.1727440024498827
1000, epoch_train_loss=3.1727440024498827
Epoch 1001
Epoch 1001 :: Batch 0/1
Batch Loss = 3.1722719585860717
1001, epoch_train_loss=3.1722719585860717
Epoch 1002
Epoch 1002 :: Batch 0/1
Batch Loss = 3.171346955533847
1002, epoch_train_loss=3.171346955533847
Epoch 1003
Epoch 1003 :: Batch 0/1
Batch Loss = 3.170991733250367
1003, epoch_train_loss=3.170991733250367
Epoch 1004
Epoch 1004 :: Batch 0/1
Batch Loss = 3.1702748970829866
1004, epoch_train_loss=3.1702748970829866
Epoch 1005
Epoch 1005 :: Batch 0/1
Batch Loss = 3.169806558400292
1005, epoch_train_loss=3.169806558400292
Epoch 1006
Epoch 1006 :: Batch 0/1
Batch Loss = 3.169267023372512
1006, epoch_train_loss=3.169267023372512
Epoch 1007
Epoch 1007 :: Batch 0/1
Batch Loss = 3.1683053010899163
1007, epoch_train_loss=3.1683053010899163
Epoch 1008
Epoch 1008 :: Batch 0/1
Batch Loss = 3.167877965866028
1008, epoch_train_loss=3.167877965866028
Epoch 1009
Epoch 1009 :: Batch 0/1
Batch Loss = 3.1671089348683923
1009, epoch_train_loss=3.1671089348683923
Epoch 1010
Epoch 1010 :: Batch 0/1
Batch Loss = 3.1666016173560934
1010, epoch_train_loss=3.1666016173560934
Epoch 1011
Epoch 1011 :: Batch 0/1
Batch Loss = 3.1660465400867834
1011, epoch_train_loss=3.1660465400867834
Epoch 1012
Epoch 1012 :: Batch 0/1
Batch Loss = 3.165517124535239
1012, epoch_train_loss=3.165517124535239
Epoch 1013
Epoch 1013 :: Batch 0/1
Batch Loss = 3.1650163074398256
1013, epoch_train_loss=3.1650163074398256
Epoch 1014
Epoch 1014 :: Batch 0/1
Batch Loss = 3.1644096806745456
1014, epoch_train_loss=3.1644096806745456
Epoch 1015
Epoch 1015 :: Batch 0/1
Batch Loss = 3.164059710384782
1015, epoch_train_loss=3.164059710384782
Epoch 1016
Epoch 1016 :: Batch 0/1
Batch Loss = 3.163541773135235
1016, epoch_train_loss=3.163541773135235
Epoch 1017
Epoch 1017 :: Batch 0/1
Batch Loss = 3.163177394226988
1017, epoch_train_loss=3.163177394226988
Epoch 1018
Epoch 1018 :: Batch 0/1
Batch Loss = 3.1626199434395623
1018, epoch_train_loss=3.1626199434395623
Epoch 1019
Epoch 1019 :: Batch 0/1
Batch Loss = 3.162170954935187
1019, epoch_train_loss=3.162170954935187
Epoch 1020
Epoch 1020 :: Batch 0/1
Batch Loss = 3.161667526049853
1020, epoch_train_loss=3.161667526049853
Epoch 1021
Epoch 1021 :: Batch 0/1
Batch Loss = 3.1612666611135185
1021, epoch_train_loss=3.1612666611135185
Epoch 1022
Epoch 1022 :: Batch 0/1
Batch Loss = 3.1608396302383523
1022, epoch_train_loss=3.1608396302383523
Epoch 1023
Epoch 1023 :: Batch 0/1
Batch Loss = 3.160405871399853
1023, epoch_train_loss=3.160405871399853
Epoch 1024
Epoch 1024 :: Batch 0/1
Batch Loss = 3.1599528943770547
1024, epoch_train_loss=3.1599528943770547
Epoch 1025
Epoch 1025 :: Batch 0/1
Batch Loss = 3.1594970119219274
1025, epoch_train_loss=3.1594970119219274
Epoch 1026
Epoch 1026 :: Batch 0/1
Batch Loss = 3.1590480084900308
1026, epoch_train_loss=3.1590480084900308
Epoch 1027
Epoch 1027 :: Batch 0/1
Batch Loss = 3.158569256570545
1027, epoch_train_loss=3.158569256570545
Epoch 1028
Epoch 1028 :: Batch 0/1
Batch Loss = 3.1581140929147518
1028, epoch_train_loss=3.1581140929147518
Epoch 1029
Epoch 1029 :: Batch 0/1
Batch Loss = 3.1576545981200317
1029, epoch_train_loss=3.1576545981200317
Epoch 1030
Epoch 1030 :: Batch 0/1
Batch Loss = 3.1572186339259694
1030, epoch_train_loss=3.1572186339259694
Epoch 1031
Epoch 1031 :: Batch 0/1
Batch Loss = 3.1567818416331233
1031, epoch_train_loss=3.1567818416331233
Epoch 1032
Epoch 1032 :: Batch 0/1
Batch Loss = 3.1563661873101703
1032, epoch_train_loss=3.1563661873101703
Epoch 1033
Epoch 1033 :: Batch 0/1
Batch Loss = 3.1559559247413698
1033, epoch_train_loss=3.1559559247413698
Epoch 1034
Epoch 1034 :: Batch 0/1
Batch Loss = 3.155556853399855
1034, epoch_train_loss=3.155556853399855
Epoch 1035
Epoch 1035 :: Batch 0/1
Batch Loss = 3.1551626434086035
1035, epoch_train_loss=3.1551626434086035
Epoch 1036
Epoch 1036 :: Batch 0/1
Batch Loss = 3.154769573242515
1036, epoch_train_loss=3.154769573242515
Epoch 1037
Epoch 1037 :: Batch 0/1
Batch Loss = 3.1543993058273188
1037, epoch_train_loss=3.1543993058273188
Epoch 1038
Epoch 1038 :: Batch 0/1
Batch Loss = 3.154040056292191
1038, epoch_train_loss=3.154040056292191
Epoch 1039
Epoch 1039 :: Batch 0/1
Batch Loss = 3.1537327493790017
1039, epoch_train_loss=3.1537327493790017
Epoch 1040
Epoch 1040 :: Batch 0/1
Batch Loss = 3.153474162453398
1040, epoch_train_loss=3.153474162453398
Epoch 1041
Epoch 1041 :: Batch 0/1
Batch Loss = 3.1534142761823776
1041, epoch_train_loss=3.1534142761823776
Epoch 1042
Epoch 1042 :: Batch 0/1
Batch Loss = 3.1536591185150034
1042, epoch_train_loss=3.1536591185150034
Epoch 1043
Epoch 1043 :: Batch 0/1
Batch Loss = 3.155027863693777
1043, epoch_train_loss=3.155027863693777
Epoch 1044
Epoch 1044 :: Batch 0/1
Batch Loss = 3.157801514574105
1044, epoch_train_loss=3.157801514574105
Epoch 1045
Epoch 1045 :: Batch 0/1
Batch Loss = 3.1665433357441617
1045, epoch_train_loss=3.1665433357441617
Epoch 1046
Epoch 1046 :: Batch 0/1
Batch Loss = 3.1675021473315543
1046, epoch_train_loss=3.1675021473315543
Epoch 1047
Epoch 1047 :: Batch 0/1
Batch Loss = 3.1749366790306226
1047, epoch_train_loss=3.1749366790306226
Epoch 1048
Epoch 1048 :: Batch 0/1
Batch Loss = 3.1543595766905295
1048, epoch_train_loss=3.1543595766905295
Epoch 1049
Epoch 1049 :: Batch 0/1
Batch Loss = 3.1523266828333627
1049, epoch_train_loss=3.1523266828333627
Epoch 1050
Epoch 1050 :: Batch 0/1
Batch Loss = 3.163757708149272
1050, epoch_train_loss=3.163757708149272
Epoch 1051
Epoch 1051 :: Batch 0/1
Batch Loss = 3.1562619132355967
1051, epoch_train_loss=3.1562619132355967
Epoch 1052
Epoch 1052 :: Batch 0/1
Batch Loss = 3.149977393886372
1052, epoch_train_loss=3.149977393886372
Epoch 1053
Epoch 1053 :: Batch 0/1
Batch Loss = 3.152681965131018
1053, epoch_train_loss=3.152681965131018
Epoch 1054
Epoch 1054 :: Batch 0/1
Batch Loss = 3.1545181391653943
1054, epoch_train_loss=3.1545181391653943
Epoch 1055
Epoch 1055 :: Batch 0/1
Batch Loss = 3.1515365276831777
1055, epoch_train_loss=3.1515365276831777
Epoch 1056
Epoch 1056 :: Batch 0/1
Batch Loss = 3.1489034030507357
1056, epoch_train_loss=3.1489034030507357
Epoch 1057
Epoch 1057 :: Batch 0/1
Batch Loss = 3.1516869614554244
1057, epoch_train_loss=3.1516869614554244
Epoch 1058
Epoch 1058 :: Batch 0/1
Batch Loss = 3.152890151104454
1058, epoch_train_loss=3.152890151104454
Epoch 1059
Epoch 1059 :: Batch 0/1
Batch Loss = 3.1485526274302025
1059, epoch_train_loss=3.1485526274302025
Epoch 1060
Epoch 1060 :: Batch 0/1
Batch Loss = 3.148651117806339
1060, epoch_train_loss=3.148651117806339
Epoch 1061
Epoch 1061 :: Batch 0/1
Batch Loss = 3.151531880354595
1061, epoch_train_loss=3.151531880354595
Epoch 1062
Epoch 1062 :: Batch 0/1
Batch Loss = 3.1494946909652763
1062, epoch_train_loss=3.1494946909652763
Epoch 1063
Epoch 1063 :: Batch 0/1
Batch Loss = 3.146912274879499
1063, epoch_train_loss=3.146912274879499
Epoch 1064
Epoch 1064 :: Batch 0/1
Batch Loss = 3.1476192489040784
1064, epoch_train_loss=3.1476192489040784
Epoch 1065
Epoch 1065 :: Batch 0/1
Batch Loss = 3.1489243217430536
1065, epoch_train_loss=3.1489243217430536
Epoch 1066
Epoch 1066 :: Batch 0/1
Batch Loss = 3.148528955605674
1066, epoch_train_loss=3.148528955605674
Epoch 1067
Epoch 1067 :: Batch 0/1
Batch Loss = 3.146257105299256
1067, epoch_train_loss=3.146257105299256
Epoch 1068
Epoch 1068 :: Batch 0/1
Batch Loss = 3.146071041040677
1068, epoch_train_loss=3.146071041040677
Epoch 1069
Epoch 1069 :: Batch 0/1
Batch Loss = 3.1473996116865877
1069, epoch_train_loss=3.1473996116865877
Epoch 1070
Epoch 1070 :: Batch 0/1
Batch Loss = 3.1470317756318966
1070, epoch_train_loss=3.1470317756318966
Epoch 1071
Epoch 1071 :: Batch 0/1
Batch Loss = 3.1458626675027648
1071, epoch_train_loss=3.1458626675027648
Epoch 1072
Epoch 1072 :: Batch 0/1
Batch Loss = 3.144837672826941
1072, epoch_train_loss=3.144837672826941
Epoch 1073
Epoch 1073 :: Batch 0/1
Batch Loss = 3.145056670917766
1073, epoch_train_loss=3.145056670917766
Epoch 1074
Epoch 1074 :: Batch 0/1
Batch Loss = 3.145678434339993
1074, epoch_train_loss=3.145678434339993
Epoch 1075
Epoch 1075 :: Batch 0/1
Batch Loss = 3.1452643288362383
1075, epoch_train_loss=3.1452643288362383
Epoch 1076
Epoch 1076 :: Batch 0/1
Batch Loss = 3.144447992934654
1076, epoch_train_loss=3.144447992934654
Epoch 1077
Epoch 1077 :: Batch 0/1
Batch Loss = 3.14380797388795
1077, epoch_train_loss=3.14380797388795
Epoch 1078
Epoch 1078 :: Batch 0/1
Batch Loss = 3.143867148349524
1078, epoch_train_loss=3.143867148349524
Epoch 1079
Epoch 1079 :: Batch 0/1
Batch Loss = 3.1442214824658423
1079, epoch_train_loss=3.1442214824658423
Epoch 1080
Epoch 1080 :: Batch 0/1
Batch Loss = 3.1439733129674385
1080, epoch_train_loss=3.1439733129674385
Epoch 1081
Epoch 1081 :: Batch 0/1
Batch Loss = 3.1434608074321586
1081, epoch_train_loss=3.1434608074321586
Epoch 1082
Epoch 1082 :: Batch 0/1
Batch Loss = 3.1429336973737216
1082, epoch_train_loss=3.1429336973737216
Epoch 1083
Epoch 1083 :: Batch 0/1
Batch Loss = 3.1427373568952954
1083, epoch_train_loss=3.1427373568952954
Epoch 1084
Epoch 1084 :: Batch 0/1
Batch Loss = 3.1428303958216084
1084, epoch_train_loss=3.1428303958216084
Epoch 1085
Epoch 1085 :: Batch 0/1
Batch Loss = 3.1428751309103284
1085, epoch_train_loss=3.1428751309103284
Epoch 1086
Epoch 1086 :: Batch 0/1
Batch Loss = 3.142756404361275
1086, epoch_train_loss=3.142756404361275
Epoch 1087
Epoch 1087 :: Batch 0/1
Batch Loss = 3.1423879187276245
1087, epoch_train_loss=3.1423879187276245
Epoch 1088
Epoch 1088 :: Batch 0/1
Batch Loss = 3.1420433098152643
1088, epoch_train_loss=3.1420433098152643
Epoch 1089
Epoch 1089 :: Batch 0/1
Batch Loss = 3.141719842408993
1089, epoch_train_loss=3.141719842408993
Epoch 1090
Epoch 1090 :: Batch 0/1
Batch Loss = 3.1415150942065355
1090, epoch_train_loss=3.1415150942065355
Epoch 1091
Epoch 1091 :: Batch 0/1
Batch Loss = 3.1414234856610745
1091, epoch_train_loss=3.1414234856610745
Epoch 1092
Epoch 1092 :: Batch 0/1
Batch Loss = 3.1413858627915974
1092, epoch_train_loss=3.1413858627915974
Epoch 1093
Epoch 1093 :: Batch 0/1
Batch Loss = 3.141403108784516
1093, epoch_train_loss=3.141403108784516
Epoch 1094
Epoch 1094 :: Batch 0/1
Batch Loss = 3.141407959536372
1094, epoch_train_loss=3.141407959536372
Epoch 1095
Epoch 1095 :: Batch 0/1
Batch Loss = 3.141540539422743
1095, epoch_train_loss=3.141540539422743
Epoch 1096
Epoch 1096 :: Batch 0/1
Batch Loss = 3.1415849926761337
1096, epoch_train_loss=3.1415849926761337
Epoch 1097
Epoch 1097 :: Batch 0/1
Batch Loss = 3.1419661536701544
1097, epoch_train_loss=3.1419661536701544
Epoch 1098
Epoch 1098 :: Batch 0/1
Batch Loss = 3.1420676505340976
1098, epoch_train_loss=3.1420676505340976
Epoch 1099
Epoch 1099 :: Batch 0/1
Batch Loss = 3.142928137172961
1099, epoch_train_loss=3.142928137172961
Epoch 1100
Epoch 1100 :: Batch 0/1
Batch Loss = 3.1428291410533045
1100, epoch_train_loss=3.1428291410533045
Epoch 1101
Epoch 1101 :: Batch 0/1
Batch Loss = 3.1438386248760404
1101, epoch_train_loss=3.1438386248760404
Epoch 1102
Epoch 1102 :: Batch 0/1
Batch Loss = 3.1429442067388393
1102, epoch_train_loss=3.1429442067388393
Epoch 1103
Epoch 1103 :: Batch 0/1
Batch Loss = 3.1427836780278904
1103, epoch_train_loss=3.1427836780278904
Epoch 1104
Epoch 1104 :: Batch 0/1
Batch Loss = 3.1413054526913564
1104, epoch_train_loss=3.1413054526913564
Epoch 1105
Epoch 1105 :: Batch 0/1
Batch Loss = 3.140219318069232
1105, epoch_train_loss=3.140219318069232
Epoch 1106
Epoch 1106 :: Batch 0/1
Batch Loss = 3.1394711054715256
1106, epoch_train_loss=3.1394711054715256
Epoch 1107
Epoch 1107 :: Batch 0/1
Batch Loss = 3.1393386436290953
1107, epoch_train_loss=3.1393386436290953
Epoch 1108
Epoch 1108 :: Batch 0/1
Batch Loss = 3.139673470833246
1108, epoch_train_loss=3.139673470833246
Epoch 1109
Epoch 1109 :: Batch 0/1
Batch Loss = 3.1400617755144635
1109, epoch_train_loss=3.1400617755144635
Epoch 1110
Epoch 1110 :: Batch 0/1
Batch Loss = 3.1404485014901016
1110, epoch_train_loss=3.1404485014901016
Epoch 1111
Epoch 1111 :: Batch 0/1
Batch Loss = 3.1403178817104527
1111, epoch_train_loss=3.1403178817104527
Epoch 1112
Epoch 1112 :: Batch 0/1
Batch Loss = 3.1401587234353414
1112, epoch_train_loss=3.1401587234353414
Epoch 1113
Epoch 1113 :: Batch 0/1
Batch Loss = 3.1396413556022127
1113, epoch_train_loss=3.1396413556022127
Epoch 1114
Epoch 1114 :: Batch 0/1
Batch Loss = 3.139233052760299
1114, epoch_train_loss=3.139233052760299
Epoch 1115
Epoch 1115 :: Batch 0/1
Batch Loss = 3.1387777286912555
1115, epoch_train_loss=3.1387777286912555
Epoch 1116
Epoch 1116 :: Batch 0/1
Batch Loss = 3.1384579081733137
1116, epoch_train_loss=3.1384579081733137
Epoch 1117
Epoch 1117 :: Batch 0/1
Batch Loss = 3.138186657478477
1117, epoch_train_loss=3.138186657478477
Epoch 1118
Epoch 1118 :: Batch 0/1
Batch Loss = 3.1379883128999824
1118, epoch_train_loss=3.1379883128999824
Epoch 1119
Epoch 1119 :: Batch 0/1
Batch Loss = 3.1378216131694643
1119, epoch_train_loss=3.1378216131694643
Epoch 1120
Epoch 1120 :: Batch 0/1
Batch Loss = 3.137680663063434
1120, epoch_train_loss=3.137680663063434
Epoch 1121
Epoch 1121 :: Batch 0/1
Batch Loss = 3.1375514997962743
1121, epoch_train_loss=3.1375514997962743
Epoch 1122
Epoch 1122 :: Batch 0/1
Batch Loss = 3.137430005048995
1122, epoch_train_loss=3.137430005048995
Epoch 1123
Epoch 1123 :: Batch 0/1
Batch Loss = 3.1373143291937713
1123, epoch_train_loss=3.1373143291937713
Epoch 1124
Epoch 1124 :: Batch 0/1
Batch Loss = 3.1372059959970944
1124, epoch_train_loss=3.1372059959970944
Epoch 1125
Epoch 1125 :: Batch 0/1
Batch Loss = 3.13710978769809
1125, epoch_train_loss=3.13710978769809
Epoch 1126
Epoch 1126 :: Batch 0/1
Batch Loss = 3.137039019513557
1126, epoch_train_loss=3.137039019513557
Epoch 1127
Epoch 1127 :: Batch 0/1
Batch Loss = 3.137027557479439
1127, epoch_train_loss=3.137027557479439
Epoch 1128
Epoch 1128 :: Batch 0/1
Batch Loss = 3.137167720206938
1128, epoch_train_loss=3.137167720206938
Epoch 1129
Epoch 1129 :: Batch 0/1
Batch Loss = 3.137701820516639
1129, epoch_train_loss=3.137701820516639
Epoch 1130
Epoch 1130 :: Batch 0/1
Batch Loss = 3.139311072219223
1130, epoch_train_loss=3.139311072219223
Epoch 1131
Epoch 1131 :: Batch 0/1
Batch Loss = 3.1439191596459675
1131, epoch_train_loss=3.1439191596459675
Epoch 1132
Epoch 1132 :: Batch 0/1
Batch Loss = 3.1563672647310725
1132, epoch_train_loss=3.1563672647310725
Epoch 1133
Epoch 1133 :: Batch 0/1
Batch Loss = 3.1880137928624066
1133, epoch_train_loss=3.1880137928624066
Epoch 1134
Epoch 1134 :: Batch 0/1
Batch Loss = 3.2359952324062227
1134, epoch_train_loss=3.2359952324062227
Epoch 1135
Epoch 1135 :: Batch 0/1
Batch Loss = 3.2531199559295305
1135, epoch_train_loss=3.2531199559295305
Epoch 1136
Epoch 1136 :: Batch 0/1
Batch Loss = 3.1902315446950227
1136, epoch_train_loss=3.1902315446950227
Epoch 1137
Epoch 1137 :: Batch 0/1
Batch Loss = 3.1626259524414317
1137, epoch_train_loss=3.1626259524414317
Epoch 1138
Epoch 1138 :: Batch 0/1
Batch Loss = 3.2389488252729546
1138, epoch_train_loss=3.2389488252729546
Epoch 1139
Epoch 1139 :: Batch 0/1
Batch Loss = 3.198891577039611
1139, epoch_train_loss=3.198891577039611
Epoch 1140
Epoch 1140 :: Batch 0/1
Batch Loss = 3.2770976859935055
1140, epoch_train_loss=3.2770976859935055
Epoch 1141
Epoch 1141 :: Batch 0/1
Batch Loss = 3.203093600533282
1141, epoch_train_loss=3.203093600533282
Epoch 1142
Epoch 1142 :: Batch 0/1
Batch Loss = 3.347440551331589
1142, epoch_train_loss=3.347440551331589
Epoch 1143
Epoch 1143 :: Batch 0/1
Batch Loss = 3.465905449237418
1143, epoch_train_loss=3.465905449237418
Epoch 1144
Epoch 1144 :: Batch 0/1
Batch Loss = 3.7886766043292193
1144, epoch_train_loss=3.7886766043292193
Epoch 1145
Epoch 1145 :: Batch 0/1
Batch Loss = 3.3746774026955215
1145, epoch_train_loss=3.3746774026955215
Epoch 1146
Epoch 1146 :: Batch 0/1
Batch Loss = 3.493896900251812
1146, epoch_train_loss=3.493896900251812
Epoch 1147
Epoch 1147 :: Batch 0/1
Batch Loss = 3.404263161662812
1147, epoch_train_loss=3.404263161662812
Epoch 1148
Epoch 1148 :: Batch 0/1
Batch Loss = 3.5139112547411915
1148, epoch_train_loss=3.5139112547411915
Epoch 1149
Epoch 1149 :: Batch 0/1
Batch Loss = 3.332921025424062
1149, epoch_train_loss=3.332921025424062
Epoch 1150
Epoch 1150 :: Batch 0/1
Batch Loss = 3.430780796088761
1150, epoch_train_loss=3.430780796088761
Epoch 1151
Epoch 1151 :: Batch 0/1
Batch Loss = 3.307090426212196
1151, epoch_train_loss=3.307090426212196
Epoch 1152
Epoch 1152 :: Batch 0/1
Batch Loss = 3.316074888483301
1152, epoch_train_loss=3.316074888483301
Epoch 1153
Epoch 1153 :: Batch 0/1
Batch Loss = 3.3064560569436034
1153, epoch_train_loss=3.3064560569436034
Epoch 1154
Epoch 1154 :: Batch 0/1
Batch Loss = 3.249365178514675
1154, epoch_train_loss=3.249365178514675
Epoch 1155
Epoch 1155 :: Batch 0/1
Batch Loss = 3.300859018176818
1155, epoch_train_loss=3.300859018176818
Epoch 1156
Epoch 1156 :: Batch 0/1
Batch Loss = 3.220052186365071
1156, epoch_train_loss=3.220052186365071
Epoch 1157
Epoch 1157 :: Batch 0/1
Batch Loss = 3.301471225176823
1157, epoch_train_loss=3.301471225176823
Epoch 1158
Epoch 1158 :: Batch 0/1
Batch Loss = 3.1987379631249846
1158, epoch_train_loss=3.1987379631249846
Epoch 1159
Epoch 1159 :: Batch 0/1
Batch Loss = 3.225921187457191
1159, epoch_train_loss=3.225921187457191
Epoch 1160
Epoch 1160 :: Batch 0/1
Batch Loss = 3.2197491453340796
1160, epoch_train_loss=3.2197491453340796
Epoch 1161
Epoch 1161 :: Batch 0/1
Batch Loss = 3.2015390001675623
1161, epoch_train_loss=3.2015390001675623
Epoch 1162
Epoch 1162 :: Batch 0/1
Batch Loss = 3.1866140029097614
1162, epoch_train_loss=3.1866140029097614
Epoch 1163
Epoch 1163 :: Batch 0/1
Batch Loss = 3.171657121722577
1163, epoch_train_loss=3.171657121722577
Epoch 1164
Epoch 1164 :: Batch 0/1
Batch Loss = 3.17842615681795
1164, epoch_train_loss=3.17842615681795
Epoch 1165
Epoch 1165 :: Batch 0/1
Batch Loss = 3.1745901185727563
1165, epoch_train_loss=3.1745901185727563
Epoch 1166
Epoch 1166 :: Batch 0/1
Batch Loss = 3.168437600676051
1166, epoch_train_loss=3.168437600676051
Epoch 1167
Epoch 1167 :: Batch 0/1
Batch Loss = 3.1630346019023046
1167, epoch_train_loss=3.1630346019023046
Epoch 1168
Epoch 1168 :: Batch 0/1
Batch Loss = 3.164119840054021
1168, epoch_train_loss=3.164119840054021
Epoch 1169
Epoch 1169 :: Batch 0/1
Batch Loss = 3.1665425839926082
1169, epoch_train_loss=3.1665425839926082
Epoch 1170
Epoch 1170 :: Batch 0/1
Batch Loss = 3.153193401905777
1170, epoch_train_loss=3.153193401905777
Epoch 1171
Epoch 1171 :: Batch 0/1
Batch Loss = 3.1571947672631477
1171, epoch_train_loss=3.1571947672631477
Epoch 1172
Epoch 1172 :: Batch 0/1
Batch Loss = 3.1578358753638276
1172, epoch_train_loss=3.1578358753638276
Epoch 1173
Epoch 1173 :: Batch 0/1
Batch Loss = 3.1578180014560258
1173, epoch_train_loss=3.1578180014560258
Epoch 1174
Epoch 1174 :: Batch 0/1
Batch Loss = 3.1489487017713964
1174, epoch_train_loss=3.1489487017713964
Epoch 1175
Epoch 1175 :: Batch 0/1
Batch Loss = 3.1424787976575925
1175, epoch_train_loss=3.1424787976575925
Epoch 1176
Epoch 1176 :: Batch 0/1
Batch Loss = 3.147882707750554
1176, epoch_train_loss=3.147882707750554
Epoch 1177
Epoch 1177 :: Batch 0/1
Batch Loss = 3.1416920291994974
1177, epoch_train_loss=3.1416920291994974
Epoch 1178
Epoch 1178 :: Batch 0/1
Batch Loss = 3.134441376654926
1178, epoch_train_loss=3.134441376654926
Epoch 1179
Epoch 1179 :: Batch 0/1
Batch Loss = 3.1342998933606974
1179, epoch_train_loss=3.1342998933606974
Epoch 1180
Epoch 1180 :: Batch 0/1
Batch Loss = 3.1341878016638645
1180, epoch_train_loss=3.1341878016638645
Epoch 1181
Epoch 1181 :: Batch 0/1
Batch Loss = 3.123991063650859
1181, epoch_train_loss=3.123991063650859
Epoch 1182
Epoch 1182 :: Batch 0/1
Batch Loss = 3.128140627418681
1182, epoch_train_loss=3.128140627418681
Epoch 1183
Epoch 1183 :: Batch 0/1
Batch Loss = 3.1204145291270273
1183, epoch_train_loss=3.1204145291270273
Epoch 1184
Epoch 1184 :: Batch 0/1
Batch Loss = 3.121274362233333
1184, epoch_train_loss=3.121274362233333
Epoch 1185
Epoch 1185 :: Batch 0/1
Batch Loss = 3.1166476007681094
1185, epoch_train_loss=3.1166476007681094
Epoch 1186
Epoch 1186 :: Batch 0/1
Batch Loss = 3.1163848790962585
1186, epoch_train_loss=3.1163848790962585
Epoch 1187
Epoch 1187 :: Batch 0/1
Batch Loss = 3.1132704156452107
1187, epoch_train_loss=3.1132704156452107
Epoch 1188
Epoch 1188 :: Batch 0/1
Batch Loss = 3.1124380546277126
1188, epoch_train_loss=3.1124380546277126
Epoch 1189
Epoch 1189 :: Batch 0/1
Batch Loss = 3.10974065184284
1189, epoch_train_loss=3.10974065184284
Epoch 1190
Epoch 1190 :: Batch 0/1
Batch Loss = 3.1103261515572087
1190, epoch_train_loss=3.1103261515572087
Epoch 1191
Epoch 1191 :: Batch 0/1
Batch Loss = 3.10551431324493
1191, epoch_train_loss=3.10551431324493
Epoch 1192
Epoch 1192 :: Batch 0/1
Batch Loss = 3.108951331864334
1192, epoch_train_loss=3.108951331864334
Epoch 1193
Epoch 1193 :: Batch 0/1
Batch Loss = 3.1028371402880452
1193, epoch_train_loss=3.1028371402880452
Epoch 1194
Epoch 1194 :: Batch 0/1
Batch Loss = 3.1052092898082635
1194, epoch_train_loss=3.1052092898082635
Epoch 1195
Epoch 1195 :: Batch 0/1
Batch Loss = 3.1034098231997898
1195, epoch_train_loss=3.1034098231997898
Epoch 1196
Epoch 1196 :: Batch 0/1
Batch Loss = 3.0998787518794106
1196, epoch_train_loss=3.0998787518794106
Epoch 1197
Epoch 1197 :: Batch 0/1
Batch Loss = 3.1024146300926096
1197, epoch_train_loss=3.1024146300926096
Epoch 1198
Epoch 1198 :: Batch 0/1
Batch Loss = 3.099690141976436
1198, epoch_train_loss=3.099690141976436
Epoch 1199
Epoch 1199 :: Batch 0/1
Batch Loss = 3.0967071658742307
1199, epoch_train_loss=3.0967071658742307
Epoch 1200
Epoch 1200 :: Batch 0/1
Batch Loss = 3.098284648039422
1200, epoch_train_loss=3.098284648039422
Epoch 1201
Epoch 1201 :: Batch 0/1
Batch Loss = 3.0981082236561597
1201, epoch_train_loss=3.0981082236561597
Epoch 1202
Epoch 1202 :: Batch 0/1
Batch Loss = 3.093576344277685
1202, epoch_train_loss=3.093576344277685
Epoch 1203
Epoch 1203 :: Batch 0/1
Batch Loss = 3.0927828700400473
1203, epoch_train_loss=3.0927828700400473
Epoch 1204
Epoch 1204 :: Batch 0/1
Batch Loss = 3.095066396320565
1204, epoch_train_loss=3.095066396320565
Epoch 1205
Epoch 1205 :: Batch 0/1
Batch Loss = 3.093535369188553
1205, epoch_train_loss=3.093535369188553
Epoch 1206
Epoch 1206 :: Batch 0/1
Batch Loss = 3.090898892008944
1206, epoch_train_loss=3.090898892008944
Epoch 1207
Epoch 1207 :: Batch 0/1
Batch Loss = 3.0882522513279436
1207, epoch_train_loss=3.0882522513279436
Epoch 1208
Epoch 1208 :: Batch 0/1
Batch Loss = 3.0873064148123395
1208, epoch_train_loss=3.0873064148123395
Epoch 1209
Epoch 1209 :: Batch 0/1
Batch Loss = 3.0881356678544085
1209, epoch_train_loss=3.0881356678544085
Epoch 1210
Epoch 1210 :: Batch 0/1
Batch Loss = 3.0895976219490002
1210, epoch_train_loss=3.0895976219490002
Epoch 1211
Epoch 1211 :: Batch 0/1
Batch Loss = 3.094217550755813
1211, epoch_train_loss=3.094217550755813
Epoch 1212
Epoch 1212 :: Batch 0/1
Batch Loss = 3.0983205846346915
1212, epoch_train_loss=3.0983205846346915
Epoch 1213
Epoch 1213 :: Batch 0/1
Batch Loss = 3.1126120965357025
1213, epoch_train_loss=3.1126120965357025
Epoch 1214
Epoch 1214 :: Batch 0/1
Batch Loss = 3.1049106353328955
1214, epoch_train_loss=3.1049106353328955
Epoch 1215
Epoch 1215 :: Batch 0/1
Batch Loss = 3.1097165941392677
1215, epoch_train_loss=3.1097165941392677
Epoch 1216
Epoch 1216 :: Batch 0/1
Batch Loss = 3.0951250320424233
1216, epoch_train_loss=3.0951250320424233
Epoch 1217
Epoch 1217 :: Batch 0/1
Batch Loss = 3.091573675838486
1217, epoch_train_loss=3.091573675838486
Epoch 1218
Epoch 1218 :: Batch 0/1
Batch Loss = 3.0872407717451376
1218, epoch_train_loss=3.0872407717451376
Epoch 1219
Epoch 1219 :: Batch 0/1
Batch Loss = 3.0887908670639477
1219, epoch_train_loss=3.0887908670639477
Epoch 1220
Epoch 1220 :: Batch 0/1
Batch Loss = 3.0906362283591644
1220, epoch_train_loss=3.0906362283591644
Epoch 1221
Epoch 1221 :: Batch 0/1
Batch Loss = 3.1047345661228474
1221, epoch_train_loss=3.1047345661228474
Epoch 1222
Epoch 1222 :: Batch 0/1
Batch Loss = 3.1074605862404554
1222, epoch_train_loss=3.1074605862404554
Epoch 1223
Epoch 1223 :: Batch 0/1
Batch Loss = 3.135524901188844
1223, epoch_train_loss=3.135524901188844
Epoch 1224
Epoch 1224 :: Batch 0/1
Batch Loss = 3.088500893136785
1224, epoch_train_loss=3.088500893136785
Epoch 1225
Epoch 1225 :: Batch 0/1
Batch Loss = 3.0768413552683396
1225, epoch_train_loss=3.0768413552683396
Epoch 1226
Epoch 1226 :: Batch 0/1
Batch Loss = 3.080191046367396
1226, epoch_train_loss=3.080191046367396
Epoch 1227
Epoch 1227 :: Batch 0/1
Batch Loss = 3.0971550387915316
1227, epoch_train_loss=3.0971550387915316
Epoch 1228
Epoch 1228 :: Batch 0/1
Batch Loss = 3.1478070215528287
1228, epoch_train_loss=3.1478070215528287
Epoch 1229
Epoch 1229 :: Batch 0/1
Batch Loss = 3.082255431575423
1229, epoch_train_loss=3.082255431575423
Epoch 1230
Epoch 1230 :: Batch 0/1
Batch Loss = 3.081498268305355
1230, epoch_train_loss=3.081498268305355
Epoch 1231
Epoch 1231 :: Batch 0/1
Batch Loss = 3.1310948625212602
1231, epoch_train_loss=3.1310948625212602
Epoch 1232
Epoch 1232 :: Batch 0/1
Batch Loss = 3.1022898254146574
1232, epoch_train_loss=3.1022898254146574
Epoch 1233
Epoch 1233 :: Batch 0/1
Batch Loss = 3.098888777331856
1233, epoch_train_loss=3.098888777331856
Epoch 1234
Epoch 1234 :: Batch 0/1
Batch Loss = 3.0823207759496247
1234, epoch_train_loss=3.0823207759496247
Epoch 1235
Epoch 1235 :: Batch 0/1
Batch Loss = 3.07811734311627
1235, epoch_train_loss=3.07811734311627
Epoch 1236
Epoch 1236 :: Batch 0/1
Batch Loss = 3.0758371991603486
1236, epoch_train_loss=3.0758371991603486
Epoch 1237
Epoch 1237 :: Batch 0/1
Batch Loss = 3.0747951135184084
1237, epoch_train_loss=3.0747951135184084
Epoch 1238
Epoch 1238 :: Batch 0/1
Batch Loss = 3.0749750098385737
1238, epoch_train_loss=3.0749750098385737
Epoch 1239
Epoch 1239 :: Batch 0/1
Batch Loss = 3.078267948982475
1239, epoch_train_loss=3.078267948982475
Epoch 1240
Epoch 1240 :: Batch 0/1
Batch Loss = 3.0850927576124145
1240, epoch_train_loss=3.0850927576124145
Epoch 1241
Epoch 1241 :: Batch 0/1
Batch Loss = 3.1149491431706764
1241, epoch_train_loss=3.1149491431706764
Epoch 1242
Epoch 1242 :: Batch 0/1
Batch Loss = 3.1023061078435483
1242, epoch_train_loss=3.1023061078435483
Epoch 1243
Epoch 1243 :: Batch 0/1
Batch Loss = 3.128062589732211
1243, epoch_train_loss=3.128062589732211
Epoch 1244
Epoch 1244 :: Batch 0/1
Batch Loss = 3.0792988696325048
1244, epoch_train_loss=3.0792988696325048
Epoch 1245
Epoch 1245 :: Batch 0/1
Batch Loss = 3.069994909876205
1245, epoch_train_loss=3.069994909876205
Epoch 1246
Epoch 1246 :: Batch 0/1
Batch Loss = 3.075610282773503
1246, epoch_train_loss=3.075610282773503
Epoch 1247
Epoch 1247 :: Batch 0/1
Batch Loss = 3.098160657770485
1247, epoch_train_loss=3.098160657770485
Epoch 1248
Epoch 1248 :: Batch 0/1
Batch Loss = 3.172582432703096
1248, epoch_train_loss=3.172582432703096
Epoch 1249
Epoch 1249 :: Batch 0/1
Batch Loss = 3.074901473057192
1249, epoch_train_loss=3.074901473057192
Epoch 1250
Epoch 1250 :: Batch 0/1
Batch Loss = 3.334230110960085
1250, epoch_train_loss=3.334230110960085
Epoch 1251
Epoch 1251 :: Batch 0/1
Batch Loss = 3.300147559257097
1251, epoch_train_loss=3.300147559257097
Epoch 1252
Epoch 1252 :: Batch 0/1
Batch Loss = 3.3766301189039813
1252, epoch_train_loss=3.3766301189039813
Epoch 1253
Epoch 1253 :: Batch 0/1
Batch Loss = 3.4242346352158153
1253, epoch_train_loss=3.4242346352158153
Epoch 1254
Epoch 1254 :: Batch 0/1
Batch Loss = 3.7026173548070442
1254, epoch_train_loss=3.7026173548070442
Epoch 1255
Epoch 1255 :: Batch 0/1
Batch Loss = 3.7114721727732656
1255, epoch_train_loss=3.7114721727732656
Epoch 1256
Epoch 1256 :: Batch 0/1
Batch Loss = 3.4461515742489075
1256, epoch_train_loss=3.4461515742489075
Epoch 1257
Epoch 1257 :: Batch 0/1
Batch Loss = 3.555902489504899
1257, epoch_train_loss=3.555902489504899
Epoch 1258
Epoch 1258 :: Batch 0/1
Batch Loss = 3.342206718017408
1258, epoch_train_loss=3.342206718017408
Epoch 1259
Epoch 1259 :: Batch 0/1
Batch Loss = 3.4398275025176437
1259, epoch_train_loss=3.4398275025176437
Epoch 1260
Epoch 1260 :: Batch 0/1
Batch Loss = 3.3176698748221587
1260, epoch_train_loss=3.3176698748221587
Epoch 1261
Epoch 1261 :: Batch 0/1
Batch Loss = 3.336977553677505
1261, epoch_train_loss=3.336977553677505
Epoch 1262
Epoch 1262 :: Batch 0/1
Batch Loss = 3.3480883402343586
1262, epoch_train_loss=3.3480883402343586
Epoch 1263
Epoch 1263 :: Batch 0/1
Batch Loss = 3.316420139906445
1263, epoch_train_loss=3.316420139906445
Epoch 1264
Epoch 1264 :: Batch 0/1
Batch Loss = 3.2816812100364503
1264, epoch_train_loss=3.2816812100364503
Epoch 1265
Epoch 1265 :: Batch 0/1
Batch Loss = 3.270799594225154
1265, epoch_train_loss=3.270799594225154
Epoch 1266
Epoch 1266 :: Batch 0/1
Batch Loss = 3.2374289263774427
1266, epoch_train_loss=3.2374289263774427
Epoch 1267
Epoch 1267 :: Batch 0/1
Batch Loss = 3.2659885507368283
1267, epoch_train_loss=3.2659885507368283
Epoch 1268
Epoch 1268 :: Batch 0/1
Batch Loss = 3.196421854313649
1268, epoch_train_loss=3.196421854313649
Epoch 1269
Epoch 1269 :: Batch 0/1
Batch Loss = 3.238344660656857
1269, epoch_train_loss=3.238344660656857
Epoch 1270
Epoch 1270 :: Batch 0/1
Batch Loss = 3.2339652388998297
1270, epoch_train_loss=3.2339652388998297
Epoch 1271
Epoch 1271 :: Batch 0/1
Batch Loss = 3.212063152315401
1271, epoch_train_loss=3.212063152315401
Epoch 1272
Epoch 1272 :: Batch 0/1
Batch Loss = 3.211724300396411
1272, epoch_train_loss=3.211724300396411
Epoch 1273
Epoch 1273 :: Batch 0/1
Batch Loss = 3.198460230482375
1273, epoch_train_loss=3.198460230482375
Epoch 1274
Epoch 1274 :: Batch 0/1
Batch Loss = 3.211040915676205
1274, epoch_train_loss=3.211040915676205
Epoch 1275
Epoch 1275 :: Batch 0/1
Batch Loss = 3.1805030546076316
1275, epoch_train_loss=3.1805030546076316
Epoch 1276
Epoch 1276 :: Batch 0/1
Batch Loss = 3.2023271687280914
1276, epoch_train_loss=3.2023271687280914
Epoch 1277
Epoch 1277 :: Batch 0/1
Batch Loss = 3.20128172685121
1277, epoch_train_loss=3.20128172685121
Epoch 1278
Epoch 1278 :: Batch 0/1
Batch Loss = 3.1770199094755256
1278, epoch_train_loss=3.1770199094755256
Epoch 1279
Epoch 1279 :: Batch 0/1
Batch Loss = 3.187077890900957
1279, epoch_train_loss=3.187077890900957
Epoch 1280
Epoch 1280 :: Batch 0/1
Batch Loss = 3.1857159280514344
1280, epoch_train_loss=3.1857159280514344
Epoch 1281
Epoch 1281 :: Batch 0/1
Batch Loss = 3.1768687781474823
1281, epoch_train_loss=3.1768687781474823
Epoch 1282
Epoch 1282 :: Batch 0/1
Batch Loss = 3.1743349276559267
1282, epoch_train_loss=3.1743349276559267
Epoch 1283
Epoch 1283 :: Batch 0/1
Batch Loss = 3.1754166542040587
1283, epoch_train_loss=3.1754166542040587
Epoch 1284
Epoch 1284 :: Batch 0/1
Batch Loss = 3.1780442901281267
1284, epoch_train_loss=3.1780442901281267
Epoch 1285
Epoch 1285 :: Batch 0/1
Batch Loss = 3.1650876418960525
1285, epoch_train_loss=3.1650876418960525
Epoch 1286
Epoch 1286 :: Batch 0/1
Batch Loss = 3.1675101259243204
1286, epoch_train_loss=3.1675101259243204
Epoch 1287
Epoch 1287 :: Batch 0/1
Batch Loss = 3.168492646213124
1287, epoch_train_loss=3.168492646213124
Epoch 1288
Epoch 1288 :: Batch 0/1
Batch Loss = 3.165839776642005
1288, epoch_train_loss=3.165839776642005
Epoch 1289
Epoch 1289 :: Batch 0/1
Batch Loss = 3.161682511650266
1289, epoch_train_loss=3.161682511650266
Epoch 1290
Epoch 1290 :: Batch 0/1
Batch Loss = 3.160274689353282
1290, epoch_train_loss=3.160274689353282
Epoch 1291
Epoch 1291 :: Batch 0/1
Batch Loss = 3.1634297053373572
1291, epoch_train_loss=3.1634297053373572
Epoch 1292
Epoch 1292 :: Batch 0/1
Batch Loss = 3.1584317340151777
1292, epoch_train_loss=3.1584317340151777
Epoch 1293
Epoch 1293 :: Batch 0/1
Batch Loss = 3.157947865118902
1293, epoch_train_loss=3.157947865118902
Epoch 1294
Epoch 1294 :: Batch 0/1
Batch Loss = 3.156275775353002
1294, epoch_train_loss=3.156275775353002
Epoch 1295
Epoch 1295 :: Batch 0/1
Batch Loss = 3.157753695626135
1295, epoch_train_loss=3.157753695626135
Epoch 1296
Epoch 1296 :: Batch 0/1
Batch Loss = 3.156084486821747
1296, epoch_train_loss=3.156084486821747
Epoch 1297
Epoch 1297 :: Batch 0/1
Batch Loss = 3.1534850382209507
1297, epoch_train_loss=3.1534850382209507
Epoch 1298
Epoch 1298 :: Batch 0/1
Batch Loss = 3.1545415713025995
1298, epoch_train_loss=3.1545415713025995
Epoch 1299
Epoch 1299 :: Batch 0/1
Batch Loss = 3.153180821108449
1299, epoch_train_loss=3.153180821108449
Epoch 1300
Epoch 1300 :: Batch 0/1
Batch Loss = 3.1533532590835667
1300, epoch_train_loss=3.1533532590835667
Epoch 1301
Epoch 1301 :: Batch 0/1
Batch Loss = 3.1509279714451863
1301, epoch_train_loss=3.1509279714451863
Epoch 1302
Epoch 1302 :: Batch 0/1
Batch Loss = 3.151409520765094
1302, epoch_train_loss=3.151409520765094
Epoch 1303
Epoch 1303 :: Batch 0/1
Batch Loss = 3.150586527423798
1303, epoch_train_loss=3.150586527423798
Epoch 1304
Epoch 1304 :: Batch 0/1
Batch Loss = 3.1503541230943353
1304, epoch_train_loss=3.1503541230943353
Epoch 1305
Epoch 1305 :: Batch 0/1
Batch Loss = 3.149083641762012
1305, epoch_train_loss=3.149083641762012
Epoch 1306
Epoch 1306 :: Batch 0/1
Batch Loss = 3.148648487827548
1306, epoch_train_loss=3.148648487827548
Epoch 1307
Epoch 1307 :: Batch 0/1
Batch Loss = 3.148442906351144
1307, epoch_train_loss=3.148442906351144
Epoch 1308
Epoch 1308 :: Batch 0/1
Batch Loss = 3.147692012723502
1308, epoch_train_loss=3.147692012723502
Epoch 1309
Epoch 1309 :: Batch 0/1
Batch Loss = 3.147391127087048
1309, epoch_train_loss=3.147391127087048
Epoch 1310
Epoch 1310 :: Batch 0/1
Batch Loss = 3.146608649837929
1310, epoch_train_loss=3.146608649837929
Epoch 1311
Epoch 1311 :: Batch 0/1
Batch Loss = 3.1466558655847026
1311, epoch_train_loss=3.1466558655847026
Epoch 1312
Epoch 1312 :: Batch 0/1
Batch Loss = 3.1458951759107117
1312, epoch_train_loss=3.1458951759107117
Epoch 1313
Epoch 1313 :: Batch 0/1
Batch Loss = 3.1457637457016303
1313, epoch_train_loss=3.1457637457016303
Epoch 1314
Epoch 1314 :: Batch 0/1
Batch Loss = 3.145053743278634
1314, epoch_train_loss=3.145053743278634
Epoch 1315
Epoch 1315 :: Batch 0/1
Batch Loss = 3.145014984998964
1315, epoch_train_loss=3.145014984998964
Epoch 1316
Epoch 1316 :: Batch 0/1
Batch Loss = 3.144370433419383
1316, epoch_train_loss=3.144370433419383
Epoch 1317
Epoch 1317 :: Batch 0/1
Batch Loss = 3.1442855878662264
1317, epoch_train_loss=3.1442855878662264
Epoch 1318
Epoch 1318 :: Batch 0/1
Batch Loss = 3.1437601019111185
1318, epoch_train_loss=3.1437601019111185
Epoch 1319
Epoch 1319 :: Batch 0/1
Batch Loss = 3.1436254292017787
1319, epoch_train_loss=3.1436254292017787
Epoch 1320
Epoch 1320 :: Batch 0/1
Batch Loss = 3.143111556974366
1320, epoch_train_loss=3.143111556974366
Epoch 1321
Epoch 1321 :: Batch 0/1
Batch Loss = 3.1429614110197797
1321, epoch_train_loss=3.1429614110197797
Epoch 1322
Epoch 1322 :: Batch 0/1
Batch Loss = 3.142561431143354
1322, epoch_train_loss=3.142561431143354
Epoch 1323
Epoch 1323 :: Batch 0/1
Batch Loss = 3.1423798558037164
1323, epoch_train_loss=3.1423798558037164
Epoch 1324
Epoch 1324 :: Batch 0/1
Batch Loss = 3.1419825943505795
1324, epoch_train_loss=3.1419825943505795
Epoch 1325
Epoch 1325 :: Batch 0/1
Batch Loss = 3.141840490022673
1325, epoch_train_loss=3.141840490022673
Epoch 1326
Epoch 1326 :: Batch 0/1
Batch Loss = 3.1415400761737664
1326, epoch_train_loss=3.1415400761737664
Epoch 1327
Epoch 1327 :: Batch 0/1
Batch Loss = 3.1413590454420652
1327, epoch_train_loss=3.1413590454420652
Epoch 1328
Epoch 1328 :: Batch 0/1
Batch Loss = 3.141024037944675
1328, epoch_train_loss=3.141024037944675
Epoch 1329
Epoch 1329 :: Batch 0/1
Batch Loss = 3.140876885542979
1329, epoch_train_loss=3.140876885542979
Epoch 1330
Epoch 1330 :: Batch 0/1
Batch Loss = 3.140626179398937
1330, epoch_train_loss=3.140626179398937
Epoch 1331
Epoch 1331 :: Batch 0/1
Batch Loss = 3.1404620252667175
1331, epoch_train_loss=3.1404620252667175
Epoch 1332
Epoch 1332 :: Batch 0/1
Batch Loss = 3.1401836461784423
1332, epoch_train_loss=3.1401836461784423
Epoch 1333
Epoch 1333 :: Batch 0/1
Batch Loss = 3.14005086274317
1333, epoch_train_loss=3.14005086274317
Epoch 1334
Epoch 1334 :: Batch 0/1
Batch Loss = 3.1398321568167185
1334, epoch_train_loss=3.1398321568167185
Epoch 1335
Epoch 1335 :: Batch 0/1
Batch Loss = 3.139675948448861
1335, epoch_train_loss=3.139675948448861
Epoch 1336
Epoch 1336 :: Batch 0/1
Batch Loss = 3.139432922114008
1336, epoch_train_loss=3.139432922114008
Epoch 1337
Epoch 1337 :: Batch 0/1
Batch Loss = 3.1392985892511653
1337, epoch_train_loss=3.1392985892511653
Epoch 1338
Epoch 1338 :: Batch 0/1
Batch Loss = 3.139110925520557
1338, epoch_train_loss=3.139110925520557
Epoch 1339
Epoch 1339 :: Batch 0/1
Batch Loss = 3.1389611814925664
1339, epoch_train_loss=3.1389611814925664
Epoch 1340
Epoch 1340 :: Batch 0/1
Batch Loss = 3.1387643190820733
1340, epoch_train_loss=3.1387643190820733
Epoch 1341
Epoch 1341 :: Batch 0/1
Batch Loss = 3.1386288640320505
1341, epoch_train_loss=3.1386288640320505
Epoch 1342
Epoch 1342 :: Batch 0/1
Batch Loss = 3.13847106292812
1342, epoch_train_loss=3.13847106292812
Epoch 1343
Epoch 1343 :: Batch 0/1
Batch Loss = 3.1383192887015134
1343, epoch_train_loss=3.1383192887015134
Epoch 1344
Epoch 1344 :: Batch 0/1
Batch Loss = 3.138159317508124
1344, epoch_train_loss=3.138159317508124
Epoch 1345
Epoch 1345 :: Batch 0/1
Batch Loss = 3.138020842374877
1345, epoch_train_loss=3.138020842374877
Epoch 1346
Epoch 1346 :: Batch 0/1
Batch Loss = 3.1378906344160793
1346, epoch_train_loss=3.1378906344160793
Epoch 1347
Epoch 1347 :: Batch 0/1
Batch Loss = 3.1377417812545985
1347, epoch_train_loss=3.1377417812545985
Epoch 1348
Epoch 1348 :: Batch 0/1
Batch Loss = 3.1376098803577843
1348, epoch_train_loss=3.1376098803577843
Epoch 1349
Epoch 1349 :: Batch 0/1
Batch Loss = 3.137472955265771
1349, epoch_train_loss=3.137472955265771
Epoch 1350
Epoch 1350 :: Batch 0/1
Batch Loss = 3.1373560925214874
1350, epoch_train_loss=3.1373560925214874
Epoch 1351
Epoch 1351 :: Batch 0/1
Batch Loss = 3.137216415288537
1351, epoch_train_loss=3.137216415288537
Epoch 1352
Epoch 1352 :: Batch 0/1
Batch Loss = 3.137096969998795
1352, epoch_train_loss=3.137096969998795
Epoch 1353
Epoch 1353 :: Batch 0/1
Batch Loss = 3.1369722921026932
1353, epoch_train_loss=3.1369722921026932
Epoch 1354
Epoch 1354 :: Batch 0/1
Batch Loss = 3.1368588368688393
1354, epoch_train_loss=3.1368588368688393
Epoch 1355
Epoch 1355 :: Batch 0/1
Batch Loss = 3.1367364806429534
1355, epoch_train_loss=3.1367364806429534
Epoch 1356
Epoch 1356 :: Batch 0/1
Batch Loss = 3.136620393428529
1356, epoch_train_loss=3.136620393428529
Epoch 1357
Epoch 1357 :: Batch 0/1
Batch Loss = 3.136510546761584
1357, epoch_train_loss=3.136510546761584
Epoch 1358
Epoch 1358 :: Batch 0/1
Batch Loss = 3.1363980594418304
1358, epoch_train_loss=3.1363980594418304
Epoch 1359
Epoch 1359 :: Batch 0/1
Batch Loss = 3.1362907705615695
1359, epoch_train_loss=3.1362907705615695
Epoch 1360
Epoch 1360 :: Batch 0/1
Batch Loss = 3.1361801010232746
1360, epoch_train_loss=3.1361801010232746
Epoch 1361
Epoch 1361 :: Batch 0/1
Batch Loss = 3.136079340528676
1361, epoch_train_loss=3.136079340528676
Epoch 1362
Epoch 1362 :: Batch 0/1
Batch Loss = 3.1359734959037544
1362, epoch_train_loss=3.1359734959037544
Epoch 1363
Epoch 1363 :: Batch 0/1
Batch Loss = 3.135872372793711
1363, epoch_train_loss=3.135872372793711
Epoch 1364
Epoch 1364 :: Batch 0/1
Batch Loss = 3.1357715225963037
1364, epoch_train_loss=3.1357715225963037
Epoch 1365
Epoch 1365 :: Batch 0/1
Batch Loss = 3.135673359653197
1365, epoch_train_loss=3.135673359653197
Epoch 1366
Epoch 1366 :: Batch 0/1
Batch Loss = 3.1355772601010687
1366, epoch_train_loss=3.1355772601010687
Epoch 1367
Epoch 1367 :: Batch 0/1
Batch Loss = 3.135479702262681
1367, epoch_train_loss=3.135479702262681
Epoch 1368
Epoch 1368 :: Batch 0/1
Batch Loss = 3.1353869786949633
1368, epoch_train_loss=3.1353869786949633
Epoch 1369
Epoch 1369 :: Batch 0/1
Batch Loss = 3.1352928345366506
1369, epoch_train_loss=3.1352928345366506
Epoch 1370
Epoch 1370 :: Batch 0/1
Batch Loss = 3.1352019177654853
1370, epoch_train_loss=3.1352019177654853
Epoch 1371
Epoch 1371 :: Batch 0/1
Batch Loss = 3.135111008534144
1371, epoch_train_loss=3.135111008534144
Epoch 1372
Epoch 1372 :: Batch 0/1
Batch Loss = 3.1350218649855472
1372, epoch_train_loss=3.1350218649855472
Epoch 1373
Epoch 1373 :: Batch 0/1
Batch Loss = 3.1349345065233516
1373, epoch_train_loss=3.1349345065233516
Epoch 1374
Epoch 1374 :: Batch 0/1
Batch Loss = 3.1348468079273695
1374, epoch_train_loss=3.1348468079273695
Epoch 1375
Epoch 1375 :: Batch 0/1
Batch Loss = 3.13476181189765
1375, epoch_train_loss=3.13476181189765
Epoch 1376
Epoch 1376 :: Batch 0/1
Batch Loss = 3.1346765197111406
1376, epoch_train_loss=3.1346765197111406
Epoch 1377
Epoch 1377 :: Batch 0/1
Batch Loss = 3.1345929745915044
1377, epoch_train_loss=3.1345929745915044
Epoch 1378
Epoch 1378 :: Batch 0/1
Batch Loss = 3.134510229734724
1378, epoch_train_loss=3.134510229734724
Epoch 1379
Epoch 1379 :: Batch 0/1
Batch Loss = 3.1344282433720116
1379, epoch_train_loss=3.1344282433720116
Epoch 1380
Epoch 1380 :: Batch 0/1
Batch Loss = 3.1343476930657492
1380, epoch_train_loss=3.1343476930657492
Epoch 1381
Epoch 1381 :: Batch 0/1
Batch Loss = 3.134267014052551
1381, epoch_train_loss=3.134267014052551
Epoch 1382
Epoch 1382 :: Batch 0/1
Batch Loss = 3.1341879723795065
1382, epoch_train_loss=3.1341879723795065
Epoch 1383
Epoch 1383 :: Batch 0/1
Batch Loss = 3.1341093769545774
1383, epoch_train_loss=3.1341093769545774
Epoch 1384
Epoch 1384 :: Batch 0/1
Batch Loss = 3.134031342838332
1384, epoch_train_loss=3.134031342838332
Epoch 1385
Epoch 1385 :: Batch 0/1
Batch Loss = 3.1339543072320835
1385, epoch_train_loss=3.1339543072320835
Epoch 1386
Epoch 1386 :: Batch 0/1
Batch Loss = 3.1338776337372414
1386, epoch_train_loss=3.1338776337372414
Epoch 1387
Epoch 1387 :: Batch 0/1
Batch Loss = 3.1338019633839176
1387, epoch_train_loss=3.1338019633839176
Epoch 1388
Epoch 1388 :: Batch 0/1
Batch Loss = 3.1337264173127495
1388, epoch_train_loss=3.1337264173127495
Epoch 1389
Epoch 1389 :: Batch 0/1
Batch Loss = 3.1336514705131266
1389, epoch_train_loss=3.1336514705131266
Epoch 1390
Epoch 1390 :: Batch 0/1
Batch Loss = 3.1335772942463964
1390, epoch_train_loss=3.1335772942463964
Epoch 1391
Epoch 1391 :: Batch 0/1
Batch Loss = 3.1335031833722606
1391, epoch_train_loss=3.1335031833722606
Epoch 1392
Epoch 1392 :: Batch 0/1
Batch Loss = 3.133429617537064
1392, epoch_train_loss=3.133429617537064
Epoch 1393
Epoch 1393 :: Batch 0/1
Batch Loss = 3.1333563136511624
1393, epoch_train_loss=3.1333563136511624
Epoch 1394
Epoch 1394 :: Batch 0/1
Batch Loss = 3.1332832951461245
1394, epoch_train_loss=3.1332832951461245
Epoch 1395
Epoch 1395 :: Batch 0/1
Batch Loss = 3.1332105484335737
1395, epoch_train_loss=3.1332105484335737
Epoch 1396
Epoch 1396 :: Batch 0/1
Batch Loss = 3.133137751608733
1396, epoch_train_loss=3.133137751608733
Epoch 1397
Epoch 1397 :: Batch 0/1
Batch Loss = 3.133065236445159
1397, epoch_train_loss=3.133065236445159
Epoch 1398
Epoch 1398 :: Batch 0/1
Batch Loss = 3.132992731639223
1398, epoch_train_loss=3.132992731639223
Epoch 1399
Epoch 1399 :: Batch 0/1
Batch Loss = 3.1329200811470934
1399, epoch_train_loss=3.1329200811470934
Epoch 1400
Epoch 1400 :: Batch 0/1
Batch Loss = 3.1328474261634076
1400, epoch_train_loss=3.1328474261634076
Epoch 1401
Epoch 1401 :: Batch 0/1
Batch Loss = 3.132774522711923
1401, epoch_train_loss=3.132774522711923
Epoch 1402
Epoch 1402 :: Batch 0/1
Batch Loss = 3.1327014048701742
1402, epoch_train_loss=3.1327014048701742
Epoch 1403
Epoch 1403 :: Batch 0/1
Batch Loss = 3.13262797197457
1403, epoch_train_loss=3.13262797197457
Epoch 1404
Epoch 1404 :: Batch 0/1
Batch Loss = 3.132554063686913
1404, epoch_train_loss=3.132554063686913
Epoch 1405
Epoch 1405 :: Batch 0/1
Batch Loss = 3.132479773796452
1405, epoch_train_loss=3.132479773796452
Epoch 1406
Epoch 1406 :: Batch 0/1
Batch Loss = 3.132404925129881
1406, epoch_train_loss=3.132404925129881
Epoch 1407
Epoch 1407 :: Batch 0/1
Batch Loss = 3.132329429604632
1407, epoch_train_loss=3.132329429604632
Epoch 1408
Epoch 1408 :: Batch 0/1
Batch Loss = 3.132253350757305
1408, epoch_train_loss=3.132253350757305
Epoch 1409
Epoch 1409 :: Batch 0/1
Batch Loss = 3.1321765826203514
1409, epoch_train_loss=3.1321765826203514
Epoch 1410
Epoch 1410 :: Batch 0/1
Batch Loss = 3.1320991265834985
1410, epoch_train_loss=3.1320991265834985
Epoch 1411
Epoch 1411 :: Batch 0/1
Batch Loss = 3.132021021445823
1411, epoch_train_loss=3.132021021445823
Epoch 1412
Epoch 1412 :: Batch 0/1
Batch Loss = 3.1319422720998276
1412, epoch_train_loss=3.1319422720998276
Epoch 1413
Epoch 1413 :: Batch 0/1
Batch Loss = 3.131862993812677
1413, epoch_train_loss=3.131862993812677
Epoch 1414
Epoch 1414 :: Batch 0/1
Batch Loss = 3.131783301503418
1414, epoch_train_loss=3.131783301503418
Epoch 1415
Epoch 1415 :: Batch 0/1
Batch Loss = 3.1317033645536094
1415, epoch_train_loss=3.1317033645536094
Epoch 1416
Epoch 1416 :: Batch 0/1
Batch Loss = 3.1316234576251847
1416, epoch_train_loss=3.1316234576251847
Epoch 1417
Epoch 1417 :: Batch 0/1
Batch Loss = 3.131543843015063
1417, epoch_train_loss=3.131543843015063
Epoch 1418
Epoch 1418 :: Batch 0/1
Batch Loss = 3.131464838258806
1418, epoch_train_loss=3.131464838258806
Epoch 1419
Epoch 1419 :: Batch 0/1
Batch Loss = 3.131386814491891
1419, epoch_train_loss=3.131386814491891
Epoch 1420
Epoch 1420 :: Batch 0/1
Batch Loss = 3.131310099431064
1420, epoch_train_loss=3.131310099431064
Epoch 1421
Epoch 1421 :: Batch 0/1
Batch Loss = 3.1312350224633674
1421, epoch_train_loss=3.1312350224633674
Epoch 1422
Epoch 1422 :: Batch 0/1
Batch Loss = 3.1311618821726395
1422, epoch_train_loss=3.1311618821726395
Epoch 1423
Epoch 1423 :: Batch 0/1
Batch Loss = 3.1310908479382844
1423, epoch_train_loss=3.1310908479382844
Epoch 1424
Epoch 1424 :: Batch 0/1
Batch Loss = 3.1310219799636987
1424, epoch_train_loss=3.1310219799636987
Epoch 1425
Epoch 1425 :: Batch 0/1
Batch Loss = 3.130955220622549
1425, epoch_train_loss=3.130955220622549
Epoch 1426
Epoch 1426 :: Batch 0/1
Batch Loss = 3.130890349338839
1426, epoch_train_loss=3.130890349338839
Epoch 1427
Epoch 1427 :: Batch 0/1
Batch Loss = 3.130827039101004
1427, epoch_train_loss=3.130827039101004
Epoch 1428
Epoch 1428 :: Batch 0/1
Batch Loss = 3.1307649026207347
1428, epoch_train_loss=3.1307649026207347
Epoch 1429
Epoch 1429 :: Batch 0/1
Batch Loss = 3.1307035027248937
1429, epoch_train_loss=3.1307035027248937
Epoch 1430
Epoch 1430 :: Batch 0/1
Batch Loss = 3.130642426528047
1430, epoch_train_loss=3.130642426528047
Epoch 1431
Epoch 1431 :: Batch 0/1
Batch Loss = 3.1305813362208657
1431, epoch_train_loss=3.1305813362208657
Epoch 1432
Epoch 1432 :: Batch 0/1
Batch Loss = 3.130519989405301
1432, epoch_train_loss=3.130519989405301
Epoch 1433
Epoch 1433 :: Batch 0/1
Batch Loss = 3.130458274239974
1433, epoch_train_loss=3.130458274239974
Epoch 1434
Epoch 1434 :: Batch 0/1
Batch Loss = 3.1303961871420283
1434, epoch_train_loss=3.1303961871420283
Epoch 1435
Epoch 1435 :: Batch 0/1
Batch Loss = 3.130333797282583
1435, epoch_train_loss=3.130333797282583
Epoch 1436
Epoch 1436 :: Batch 0/1
Batch Loss = 3.1302712318852426
1436, epoch_train_loss=3.1302712318852426
Epoch 1437
Epoch 1437 :: Batch 0/1
Batch Loss = 3.1302086291837052
1437, epoch_train_loss=3.1302086291837052
Epoch 1438
Epoch 1438 :: Batch 0/1
Batch Loss = 3.1301461083849453
1438, epoch_train_loss=3.1301461083849453
Epoch 1439
Epoch 1439 :: Batch 0/1
Batch Loss = 3.1300837732628213
1439, epoch_train_loss=3.1300837732628213
Epoch 1440
Epoch 1440 :: Batch 0/1
Batch Loss = 3.1300217009345372
1440, epoch_train_loss=3.1300217009345372
Epoch 1441
Epoch 1441 :: Batch 0/1
Batch Loss = 3.129959939247658
1441, epoch_train_loss=3.129959939247658
Epoch 1442
Epoch 1442 :: Batch 0/1
Batch Loss = 3.129898523922461
1442, epoch_train_loss=3.129898523922461
Epoch 1443
Epoch 1443 :: Batch 0/1
Batch Loss = 3.12983748326919
1443, epoch_train_loss=3.12983748326919
Epoch 1444
Epoch 1444 :: Batch 0/1
Batch Loss = 3.1297768370842634
1444, epoch_train_loss=3.1297768370842634
Epoch 1445
Epoch 1445 :: Batch 0/1
Batch Loss = 3.1297166003990706
1445, epoch_train_loss=3.1297166003990706
Epoch 1446
Epoch 1446 :: Batch 0/1
Batch Loss = 3.129656780919307
1446, epoch_train_loss=3.129656780919307
Epoch 1447
Epoch 1447 :: Batch 0/1
Batch Loss = 3.1295973735756863
1447, epoch_train_loss=3.1295973735756863
Epoch 1448
Epoch 1448 :: Batch 0/1
Batch Loss = 3.129538357239167
1448, epoch_train_loss=3.129538357239167
Epoch 1449
Epoch 1449 :: Batch 0/1
Batch Loss = 3.1294796948038544
1449, epoch_train_loss=3.1294796948038544
Epoch 1450
Epoch 1450 :: Batch 0/1
Batch Loss = 3.129421337529311
1450, epoch_train_loss=3.129421337529311
Epoch 1451
Epoch 1451 :: Batch 0/1
Batch Loss = 3.1293632281818966
1451, epoch_train_loss=3.1293632281818966
Epoch 1452
Epoch 1452 :: Batch 0/1
Batch Loss = 3.129305309100961
1452, epoch_train_loss=3.129305309100961
Epoch 1453
Epoch 1453 :: Batch 0/1
Batch Loss = 3.129247532543355
1453, epoch_train_loss=3.129247532543355
Epoch 1454
Epoch 1454 :: Batch 0/1
Batch Loss = 3.1291898624434165
1454, epoch_train_loss=3.1291898624434165
Epoch 1455
Epoch 1455 :: Batch 0/1
Batch Loss = 3.129132278110207
1455, epoch_train_loss=3.129132278110207
Epoch 1456
Epoch 1456 :: Batch 0/1
Batch Loss = 3.129074777722535
1456, epoch_train_loss=3.129074777722535
Epoch 1457
Epoch 1457 :: Batch 0/1
Batch Loss = 3.1290173719927155
1457, epoch_train_loss=3.1290173719927155
Epoch 1458
Epoch 1458 :: Batch 0/1
Batch Loss = 3.1289600799440915
1458, epoch_train_loss=3.1289600799440915
Epoch 1459
Epoch 1459 :: Batch 0/1
Batch Loss = 3.128902926338028
1459, epoch_train_loss=3.128902926338028
Epoch 1460
Epoch 1460 :: Batch 0/1
Batch Loss = 3.128845934286783
1460, epoch_train_loss=3.128845934286783
Epoch 1461
Epoch 1461 :: Batch 0/1
Batch Loss = 3.128789121278856
1461, epoch_train_loss=3.128789121278856
Epoch 1462
Epoch 1462 :: Batch 0/1
Batch Loss = 3.1287324981744367
1462, epoch_train_loss=3.1287324981744367
Epoch 1463
Epoch 1463 :: Batch 0/1
Batch Loss = 3.1286760676712335
1463, epoch_train_loss=3.1286760676712335
Epoch 1464
Epoch 1464 :: Batch 0/1
Batch Loss = 3.1286198252517634
1464, epoch_train_loss=3.1286198252517634
Epoch 1465
Epoch 1465 :: Batch 0/1
Batch Loss = 3.128563760615914
1465, epoch_train_loss=3.128563760615914
Epoch 1466
Epoch 1466 :: Batch 0/1
Batch Loss = 3.128507859565151
1466, epoch_train_loss=3.128507859565151
Epoch 1467
Epoch 1467 :: Batch 0/1
Batch Loss = 3.128452106917468
1467, epoch_train_loss=3.128452106917468
Epoch 1468
Epoch 1468 :: Batch 0/1
Batch Loss = 3.1283964869751206
1468, epoch_train_loss=3.1283964869751206
Epoch 1469
Epoch 1469 :: Batch 0/1
Batch Loss = 3.1283409846415515
1469, epoch_train_loss=3.1283409846415515
Epoch 1470
Epoch 1470 :: Batch 0/1
Batch Loss = 3.1282855872221225
1470, epoch_train_loss=3.1282855872221225
Epoch 1471
Epoch 1471 :: Batch 0/1
Batch Loss = 3.128230283423505
1471, epoch_train_loss=3.128230283423505
Epoch 1472
Epoch 1472 :: Batch 0/1
Batch Loss = 3.128175063528249
1472, epoch_train_loss=3.128175063528249
Epoch 1473
Epoch 1473 :: Batch 0/1
Batch Loss = 3.128119920047486
1473, epoch_train_loss=3.128119920047486
Epoch 1474
Epoch 1474 :: Batch 0/1
Batch Loss = 3.1280648466537193
1474, epoch_train_loss=3.1280648466537193
Epoch 1475
Epoch 1475 :: Batch 0/1
Batch Loss = 3.12800983852485
1475, epoch_train_loss=3.12800983852485
Epoch 1476
Epoch 1476 :: Batch 0/1
Batch Loss = 3.127954891983045
1476, epoch_train_loss=3.127954891983045
Epoch 1477
Epoch 1477 :: Batch 0/1
Batch Loss = 3.1279000039161673
1477, epoch_train_loss=3.1279000039161673
Epoch 1478
Epoch 1478 :: Batch 0/1
Batch Loss = 3.1278451722461993
1478, epoch_train_loss=3.1278451722461993
Epoch 1479
Epoch 1479 :: Batch 0/1
Batch Loss = 3.127790394951637
1479, epoch_train_loss=3.127790394951637
Epoch 1480
Epoch 1480 :: Batch 0/1
Batch Loss = 3.1277356697609178
1480, epoch_train_loss=3.1277356697609178
Epoch 1481
Epoch 1481 :: Batch 0/1
Batch Loss = 3.127680994271289
1481, epoch_train_loss=3.127680994271289
Epoch 1482
Epoch 1482 :: Batch 0/1
Batch Loss = 3.12762636495611
1482, epoch_train_loss=3.12762636495611
Epoch 1483
Epoch 1483 :: Batch 0/1
Batch Loss = 3.1275717774451657
1483, epoch_train_loss=3.1275717774451657
Epoch 1484
Epoch 1484 :: Batch 0/1
Batch Loss = 3.1275172264035573
1484, epoch_train_loss=3.1275172264035573
Epoch 1485
Epoch 1485 :: Batch 0/1
Batch Loss = 3.1274627053427713
1485, epoch_train_loss=3.1274627053427713
Epoch 1486
Epoch 1486 :: Batch 0/1
Batch Loss = 3.1274082074295997
1486, epoch_train_loss=3.1274082074295997
Epoch 1487
Epoch 1487 :: Batch 0/1
Batch Loss = 3.1273537252890855
1487, epoch_train_loss=3.1273537252890855
Epoch 1488
Epoch 1488 :: Batch 0/1
Batch Loss = 3.1272992513969378
1488, epoch_train_loss=3.1272992513969378
Epoch 1489
Epoch 1489 :: Batch 0/1
Batch Loss = 3.1272447786430626
1489, epoch_train_loss=3.1272447786430626
Epoch 1490
Epoch 1490 :: Batch 0/1
Batch Loss = 3.1271903001080443
1490, epoch_train_loss=3.1271903001080443
Epoch 1491
Epoch 1491 :: Batch 0/1
Batch Loss = 3.1271358095111235
1491, epoch_train_loss=3.1271358095111235
Epoch 1492
Epoch 1492 :: Batch 0/1
Batch Loss = 3.1270813010630594
1492, epoch_train_loss=3.1270813010630594
Epoch 1493
Epoch 1493 :: Batch 0/1
Batch Loss = 3.1270267692730003
1493, epoch_train_loss=3.1270267692730003
Epoch 1494
Epoch 1494 :: Batch 0/1
Batch Loss = 3.1269722090883945
1494, epoch_train_loss=3.1269722090883945
Epoch 1495
Epoch 1495 :: Batch 0/1
Batch Loss = 3.126917615353176
1495, epoch_train_loss=3.126917615353176
Epoch 1496
Epoch 1496 :: Batch 0/1
Batch Loss = 3.1268629828153336
1496, epoch_train_loss=3.1268629828153336
Epoch 1497
Epoch 1497 :: Batch 0/1
Batch Loss = 3.126808305994487
1497, epoch_train_loss=3.126808305994487
Epoch 1498
Epoch 1498 :: Batch 0/1
Batch Loss = 3.1267535789123024
1498, epoch_train_loss=3.1267535789123024
Epoch 1499
Epoch 1499 :: Batch 0/1
Batch Loss = 3.126698795347478
1499, epoch_train_loss=3.126698795347478
Epoch 1500
Epoch 1500 :: Batch 0/1
Batch Loss = 3.1266439486244293
1500, epoch_train_loss=3.1266439486244293
Epoch 1501
Epoch 1501 :: Batch 0/1
Batch Loss = 3.126589031760315
1501, epoch_train_loss=3.126589031760315
Epoch 1502
Epoch 1502 :: Batch 0/1
Batch Loss = 3.126534037615989
1502, epoch_train_loss=3.126534037615989
Epoch 1503
Epoch 1503 :: Batch 0/1
Batch Loss = 3.1264789588022874
1503, epoch_train_loss=3.1264789588022874
Epoch 1504
Epoch 1504 :: Batch 0/1
Batch Loss = 3.126423787980826
1504, epoch_train_loss=3.126423787980826
Epoch 1505
Epoch 1505 :: Batch 0/1
Batch Loss = 3.1263685177439107
1505, epoch_train_loss=3.1263685177439107
Epoch 1506
Epoch 1506 :: Batch 0/1
Batch Loss = 3.126313140714451
1506, epoch_train_loss=3.126313140714451
Epoch 1507
Epoch 1507 :: Batch 0/1
Batch Loss = 3.126257649600961
1507, epoch_train_loss=3.126257649600961
Epoch 1508
Epoch 1508 :: Batch 0/1
Batch Loss = 3.1262020370289285
1508, epoch_train_loss=3.1262020370289285
Epoch 1509
Epoch 1509 :: Batch 0/1
Batch Loss = 3.126146295697769
1509, epoch_train_loss=3.126146295697769
Epoch 1510
Epoch 1510 :: Batch 0/1
Batch Loss = 3.1260904181941984
1510, epoch_train_loss=3.1260904181941984
Epoch 1511
Epoch 1511 :: Batch 0/1
Batch Loss = 3.1260343970316633
1511, epoch_train_loss=3.1260343970316633
Epoch 1512
Epoch 1512 :: Batch 0/1
Batch Loss = 3.125978224645129
1512, epoch_train_loss=3.125978224645129
Epoch 1513
Epoch 1513 :: Batch 0/1
Batch Loss = 3.1259218932741453
1513, epoch_train_loss=3.1259218932741453
Epoch 1514
Epoch 1514 :: Batch 0/1
Batch Loss = 3.1258653950593946
1514, epoch_train_loss=3.1258653950593946
Epoch 1515
Epoch 1515 :: Batch 0/1
Batch Loss = 3.1258087219335935
1515, epoch_train_loss=3.1258087219335935
Epoch 1516
Epoch 1516 :: Batch 0/1
Batch Loss = 3.1257518656745584
1516, epoch_train_loss=3.1257518656745584
Epoch 1517
Epoch 1517 :: Batch 0/1
Batch Loss = 3.125694817920024
1517, epoch_train_loss=3.125694817920024
Epoch 1518
Epoch 1518 :: Batch 0/1
Batch Loss = 3.1256375701262282
1518, epoch_train_loss=3.1256375701262282
Epoch 1519
Epoch 1519 :: Batch 0/1
Batch Loss = 3.1255801136767465
1519, epoch_train_loss=3.1255801136767465
Epoch 1520
Epoch 1520 :: Batch 0/1
Batch Loss = 3.1255224397917316
1520, epoch_train_loss=3.1255224397917316
Epoch 1521
Epoch 1521 :: Batch 0/1
Batch Loss = 3.1254645396436485
1521, epoch_train_loss=3.1254645396436485
Epoch 1522
Epoch 1522 :: Batch 0/1
Batch Loss = 3.12540640429869
1522, epoch_train_loss=3.12540640429869
Epoch 1523
Epoch 1523 :: Batch 0/1
Batch Loss = 3.125348024763144
1523, epoch_train_loss=3.125348024763144
Epoch 1524
Epoch 1524 :: Batch 0/1
Batch Loss = 3.1252893919941283
1524, epoch_train_loss=3.1252893919941283
Epoch 1525
Epoch 1525 :: Batch 0/1
Batch Loss = 3.125230496843916
1525, epoch_train_loss=3.125230496843916
Epoch 1526
Epoch 1526 :: Batch 0/1
Batch Loss = 3.1251713301293282
1526, epoch_train_loss=3.1251713301293282
Epoch 1527
Epoch 1527 :: Batch 0/1
Batch Loss = 3.1251118825364745
1527, epoch_train_loss=3.1251118825364745
Epoch 1528
Epoch 1528 :: Batch 0/1
Batch Loss = 3.1250521446961956
1528, epoch_train_loss=3.1250521446961956
Epoch 1529
Epoch 1529 :: Batch 0/1
Batch Loss = 3.1249921071183397
1529, epoch_train_loss=3.1249921071183397
Epoch 1530
Epoch 1530 :: Batch 0/1
Batch Loss = 3.124931760230366
1530, epoch_train_loss=3.124931760230366
Epoch 1531
Epoch 1531 :: Batch 0/1
Batch Loss = 3.124871094386369
1531, epoch_train_loss=3.124871094386369
Epoch 1532
Epoch 1532 :: Batch 0/1
Batch Loss = 3.1248100998415533
1532, epoch_train_loss=3.1248100998415533
Epoch 1533
Epoch 1533 :: Batch 0/1
Batch Loss = 3.12474876683677
1533, epoch_train_loss=3.12474876683677
Epoch 1534
Epoch 1534 :: Batch 0/1
Batch Loss = 3.1246870855340583
1534, epoch_train_loss=3.1246870855340583
Epoch 1535
Epoch 1535 :: Batch 0/1
Batch Loss = 3.1246250461123006
1535, epoch_train_loss=3.1246250461123006
Epoch 1536
Epoch 1536 :: Batch 0/1
Batch Loss = 3.124562638712479
1536, epoch_train_loss=3.124562638712479
Epoch 1537
Epoch 1537 :: Batch 0/1
Batch Loss = 3.124499853484335
1537, epoch_train_loss=3.124499853484335
Epoch 1538
Epoch 1538 :: Batch 0/1
Batch Loss = 3.1244366805830963
1538, epoch_train_loss=3.1244366805830963
Epoch 1539
Epoch 1539 :: Batch 0/1
Batch Loss = 3.1243731101566468
1539, epoch_train_loss=3.1243731101566468
Epoch 1540
Epoch 1540 :: Batch 0/1
Batch Loss = 3.1243091323729364
1540, epoch_train_loss=3.1243091323729364
Epoch 1541
Epoch 1541 :: Batch 0/1
Batch Loss = 3.1242447373985263
1541, epoch_train_loss=3.1242447373985263
Epoch 1542
Epoch 1542 :: Batch 0/1
Batch Loss = 3.1241799154242025
1542, epoch_train_loss=3.1241799154242025
Epoch 1543
Epoch 1543 :: Batch 0/1
Batch Loss = 3.1241146566453897
1543, epoch_train_loss=3.1241146566453897
Epoch 1544
Epoch 1544 :: Batch 0/1
Batch Loss = 3.1240489513216367
1544, epoch_train_loss=3.1240489513216367
Epoch 1545
Epoch 1545 :: Batch 0/1
Batch Loss = 3.1239827897226013
1545, epoch_train_loss=3.1239827897226013
Epoch 1546
Epoch 1546 :: Batch 0/1
Batch Loss = 3.1239161622091243
1546, epoch_train_loss=3.1239161622091243
Epoch 1547
Epoch 1547 :: Batch 0/1
Batch Loss = 3.123849059194608
1547, epoch_train_loss=3.123849059194608
Epoch 1548
Epoch 1548 :: Batch 0/1
Batch Loss = 3.1237814711912484
1548, epoch_train_loss=3.1237814711912484
Epoch 1549
Epoch 1549 :: Batch 0/1
Batch Loss = 3.123713388785049
1549, epoch_train_loss=3.123713388785049
Epoch 1550
Epoch 1550 :: Batch 0/1
Batch Loss = 3.1236448026868864
1550, epoch_train_loss=3.1236448026868864
Epoch 1551
Epoch 1551 :: Batch 0/1
Batch Loss = 3.12357570369335
1551, epoch_train_loss=3.12357570369335
Epoch 1552
Epoch 1552 :: Batch 0/1
Batch Loss = 3.1235060827214096
1552, epoch_train_loss=3.1235060827214096
Epoch 1553
Epoch 1553 :: Batch 0/1
Batch Loss = 3.123435930790982
1553, epoch_train_loss=3.123435930790982
Epoch 1554
Epoch 1554 :: Batch 0/1
Batch Loss = 3.123365239075185
1554, epoch_train_loss=3.123365239075185
Epoch 1555
Epoch 1555 :: Batch 0/1
Batch Loss = 3.1232939988253308
1555, epoch_train_loss=3.1232939988253308
Epoch 1556
Epoch 1556 :: Batch 0/1
Batch Loss = 3.123222201471441
1556, epoch_train_loss=3.123222201471441
Epoch 1557
Epoch 1557 :: Batch 0/1
Batch Loss = 3.123149838545868
1557, epoch_train_loss=3.123149838545868
Epoch 1558
Epoch 1558 :: Batch 0/1
Batch Loss = 3.1230769017813
1558, epoch_train_loss=3.1230769017813
Epoch 1559
Epoch 1559 :: Batch 0/1
Batch Loss = 3.1230033829768598
1559, epoch_train_loss=3.1230033829768598
Epoch 1560
Epoch 1560 :: Batch 0/1
Batch Loss = 3.122929274200989
1560, epoch_train_loss=3.122929274200989
Epoch 1561
Epoch 1561 :: Batch 0/1
Batch Loss = 3.1228545675275936
1561, epoch_train_loss=3.1228545675275936
Epoch 1562
Epoch 1562 :: Batch 0/1
Batch Loss = 3.122779255402862
1562, epoch_train_loss=3.122779255402862
Epoch 1563
Epoch 1563 :: Batch 0/1
Batch Loss = 3.122703330124878
1563, epoch_train_loss=3.122703330124878
Epoch 1564
Epoch 1564 :: Batch 0/1
Batch Loss = 3.122626784568325
1564, epoch_train_loss=3.122626784568325
Epoch 1565
Epoch 1565 :: Batch 0/1
Batch Loss = 3.1225496111607276
1565, epoch_train_loss=3.1225496111607276
Epoch 1566
Epoch 1566 :: Batch 0/1
Batch Loss = 3.1224718034258356
1566, epoch_train_loss=3.1224718034258356
Epoch 1567
Epoch 1567 :: Batch 0/1
Batch Loss = 3.122393353783978
1567, epoch_train_loss=3.122393353783978
Epoch 1568
Epoch 1568 :: Batch 0/1
Batch Loss = 3.122314257098384
1568, epoch_train_loss=3.122314257098384
Epoch 1569
Epoch 1569 :: Batch 0/1
Batch Loss = 3.1222345060082692
1569, epoch_train_loss=3.1222345060082692
Epoch 1570
Epoch 1570 :: Batch 0/1
Batch Loss = 3.1221540998987813
1570, epoch_train_loss=3.1221540998987813
Epoch 1571
Epoch 1571 :: Batch 0/1
Batch Loss = 3.1220730358916735
1571, epoch_train_loss=3.1220730358916735
Epoch 1572
Epoch 1572 :: Batch 0/1
Batch Loss = 3.121991336201541
1572, epoch_train_loss=3.121991336201541
Epoch 1573
Epoch 1573 :: Batch 0/1
Batch Loss = 3.1219090409408037
1573, epoch_train_loss=3.1219090409408037
Epoch 1574
Epoch 1574 :: Batch 0/1
Batch Loss = 3.121826322267799
1574, epoch_train_loss=3.121826322267799
Epoch 1575
Epoch 1575 :: Batch 0/1
Batch Loss = 3.1217435891445433
1575, epoch_train_loss=3.1217435891445433
Epoch 1576
Epoch 1576 :: Batch 0/1
Batch Loss = 3.121662174571041
1576, epoch_train_loss=3.121662174571041
Epoch 1577
Epoch 1577 :: Batch 0/1
Batch Loss = 3.1215856919461964
1577, epoch_train_loss=3.1215856919461964
Epoch 1578
Epoch 1578 :: Batch 0/1
Batch Loss = 3.1215255850676145
1578, epoch_train_loss=3.1215255850676145
Epoch 1579
Epoch 1579 :: Batch 0/1
Batch Loss = 3.1215146316254163
1579, epoch_train_loss=3.1215146316254163
Epoch 1580
Epoch 1580 :: Batch 0/1
Batch Loss = 3.1216620142521894
1580, epoch_train_loss=3.1216620142521894
Epoch 1581
Epoch 1581 :: Batch 0/1
Batch Loss = 3.1222755008263543
1581, epoch_train_loss=3.1222755008263543
Epoch 1582
Epoch 1582 :: Batch 0/1
Batch Loss = 3.124515320867963
1582, epoch_train_loss=3.124515320867963
Epoch 1583
Epoch 1583 :: Batch 0/1
Batch Loss = 3.131056880587155
1583, epoch_train_loss=3.131056880587155
Epoch 1584
Epoch 1584 :: Batch 0/1
Batch Loss = 3.154857080530827
1584, epoch_train_loss=3.154857080530827
Epoch 1585
Epoch 1585 :: Batch 0/1
Batch Loss = 3.198039790441136
1585, epoch_train_loss=3.198039790441136
Epoch 1586
Epoch 1586 :: Batch 0/1
Batch Loss = 3.34052290731311
1586, epoch_train_loss=3.34052290731311
Epoch 1587
Epoch 1587 :: Batch 0/1
Batch Loss = 3.212568149292798
1587, epoch_train_loss=3.212568149292798
Epoch 1588
Epoch 1588 :: Batch 0/1
Batch Loss = 3.145165117088352
1588, epoch_train_loss=3.145165117088352
Epoch 1589
Epoch 1589 :: Batch 0/1
Batch Loss = 3.1287085514224526
1589, epoch_train_loss=3.1287085514224526
Epoch 1590
Epoch 1590 :: Batch 0/1
Batch Loss = 3.1761350410685365
1590, epoch_train_loss=3.1761350410685365
Epoch 1591
Epoch 1591 :: Batch 0/1
Batch Loss = 3.2092114772992613
1591, epoch_train_loss=3.2092114772992613
Epoch 1592
Epoch 1592 :: Batch 0/1
Batch Loss = 3.1270594675927934
1592, epoch_train_loss=3.1270594675927934
Epoch 1593
Epoch 1593 :: Batch 0/1
Batch Loss = 3.1835125143458067
1593, epoch_train_loss=3.1835125143458067
Epoch 1594
Epoch 1594 :: Batch 0/1
Batch Loss = 3.242284738453131
1594, epoch_train_loss=3.242284738453131
Epoch 1595
Epoch 1595 :: Batch 0/1
Batch Loss = 3.122665754456007
1595, epoch_train_loss=3.122665754456007
Epoch 1596
Epoch 1596 :: Batch 0/1
Batch Loss = 3.2531449251533777
1596, epoch_train_loss=3.2531449251533777
Epoch 1597
Epoch 1597 :: Batch 0/1
Batch Loss = 3.238878096735285
1597, epoch_train_loss=3.238878096735285
Epoch 1598
Epoch 1598 :: Batch 0/1
Batch Loss = 3.180606034436449
1598, epoch_train_loss=3.180606034436449
Epoch 1599
Epoch 1599 :: Batch 0/1
Batch Loss = 3.2740743404279673
1599, epoch_train_loss=3.2740743404279673
Epoch 1600
Epoch 1600 :: Batch 0/1
Batch Loss = 3.1309665802604245
1600, epoch_train_loss=3.1309665802604245
Epoch 1601
Epoch 1601 :: Batch 0/1
Batch Loss = 3.2034477603969305
1601, epoch_train_loss=3.2034477603969305
Epoch 1602
Epoch 1602 :: Batch 0/1
Batch Loss = 3.1256749853455577
1602, epoch_train_loss=3.1256749853455577
Epoch 1603
Epoch 1603 :: Batch 0/1
Batch Loss = 3.192667828784536
1603, epoch_train_loss=3.192667828784536
Epoch 1604
Epoch 1604 :: Batch 0/1
Batch Loss = 3.1453825325832296
1604, epoch_train_loss=3.1453825325832296
Epoch 1605
Epoch 1605 :: Batch 0/1
Batch Loss = 3.169499013289619
1605, epoch_train_loss=3.169499013289619
Epoch 1606
Epoch 1606 :: Batch 0/1
Batch Loss = 3.134625358112834
1606, epoch_train_loss=3.134625358112834
Epoch 1607
Epoch 1607 :: Batch 0/1
Batch Loss = 3.1591063126504686
1607, epoch_train_loss=3.1591063126504686
Epoch 1608
Epoch 1608 :: Batch 0/1
Batch Loss = 3.137170881641693
1608, epoch_train_loss=3.137170881641693
Epoch 1609
Epoch 1609 :: Batch 0/1
Batch Loss = 3.154642952730106
1609, epoch_train_loss=3.154642952730106
Epoch 1610
Epoch 1610 :: Batch 0/1
Batch Loss = 3.127619125533726
1610, epoch_train_loss=3.127619125533726
Epoch 1611
Epoch 1611 :: Batch 0/1
Batch Loss = 3.152923805258112
1611, epoch_train_loss=3.152923805258112
Epoch 1612
Epoch 1612 :: Batch 0/1
Batch Loss = 3.126551698680039
1612, epoch_train_loss=3.126551698680039
Epoch 1613
Epoch 1613 :: Batch 0/1
Batch Loss = 3.1474431049804816
1613, epoch_train_loss=3.1474431049804816
Epoch 1614
Epoch 1614 :: Batch 0/1
Batch Loss = 3.123745457517303
1614, epoch_train_loss=3.123745457517303
Epoch 1615
Epoch 1615 :: Batch 0/1
Batch Loss = 3.1446199204974064
1615, epoch_train_loss=3.1446199204974064
Epoch 1616
Epoch 1616 :: Batch 0/1
Batch Loss = 3.124219578408214
1616, epoch_train_loss=3.124219578408214
Epoch 1617
Epoch 1617 :: Batch 0/1
Batch Loss = 3.1382356772058517
1617, epoch_train_loss=3.1382356772058517
Epoch 1618
Epoch 1618 :: Batch 0/1
Batch Loss = 3.1264272586259008
1618, epoch_train_loss=3.1264272586259008
Epoch 1619
Epoch 1619 :: Batch 0/1
Batch Loss = 3.1332844035995433
1619, epoch_train_loss=3.1332844035995433
Epoch 1620
Epoch 1620 :: Batch 0/1
Batch Loss = 3.1273703096655656
1620, epoch_train_loss=3.1273703096655656
Epoch 1621
Epoch 1621 :: Batch 0/1
Batch Loss = 3.129119530395189
1621, epoch_train_loss=3.129119530395189
Epoch 1622
Epoch 1622 :: Batch 0/1
Batch Loss = 3.129757386991352
1622, epoch_train_loss=3.129757386991352
Epoch 1623
Epoch 1623 :: Batch 0/1
Batch Loss = 3.125126181499514
1623, epoch_train_loss=3.125126181499514
Epoch 1624
Epoch 1624 :: Batch 0/1
Batch Loss = 3.1301010671912466
1624, epoch_train_loss=3.1301010671912466
Epoch 1625
Epoch 1625 :: Batch 0/1
Batch Loss = 3.1237295041893383
1625, epoch_train_loss=3.1237295041893383
Epoch 1626
Epoch 1626 :: Batch 0/1
Batch Loss = 3.1291339043348474
1626, epoch_train_loss=3.1291339043348474
Epoch 1627
Epoch 1627 :: Batch 0/1
Batch Loss = 3.123247009314844
1627, epoch_train_loss=3.123247009314844
Epoch 1628
Epoch 1628 :: Batch 0/1
Batch Loss = 3.1279091632277938
1628, epoch_train_loss=3.1279091632277938
Epoch 1629
Epoch 1629 :: Batch 0/1
Batch Loss = 3.1233578075231625
1629, epoch_train_loss=3.1233578075231625
Epoch 1630
Epoch 1630 :: Batch 0/1
Batch Loss = 3.1260280822171653
1630, epoch_train_loss=3.1260280822171653
Epoch 1631
Epoch 1631 :: Batch 0/1
Batch Loss = 3.123959717459018
1631, epoch_train_loss=3.123959717459018
Epoch 1632
Epoch 1632 :: Batch 0/1
Batch Loss = 3.124375198529556
1632, epoch_train_loss=3.124375198529556
Epoch 1633
Epoch 1633 :: Batch 0/1
Batch Loss = 3.1241865618777322
1633, epoch_train_loss=3.1241865618777322
Epoch 1634
Epoch 1634 :: Batch 0/1
Batch Loss = 3.1232578522515557
1634, epoch_train_loss=3.1232578522515557
Epoch 1635
Epoch 1635 :: Batch 0/1
Batch Loss = 3.124190607151375
1635, epoch_train_loss=3.124190607151375
Epoch 1636
Epoch 1636 :: Batch 0/1
Batch Loss = 3.1223259544251496
1636, epoch_train_loss=3.1223259544251496
Epoch 1637
Epoch 1637 :: Batch 0/1
Batch Loss = 3.1238951506486843
1637, epoch_train_loss=3.1238951506486843
Epoch 1638
Epoch 1638 :: Batch 0/1
Batch Loss = 3.1219013985371245
1638, epoch_train_loss=3.1219013985371245
Epoch 1639
Epoch 1639 :: Batch 0/1
Batch Loss = 3.123348049665957
1639, epoch_train_loss=3.123348049665957
Epoch 1640
Epoch 1640 :: Batch 0/1
Batch Loss = 3.12165306984247
1640, epoch_train_loss=3.12165306984247
Epoch 1641
Epoch 1641 :: Batch 0/1
Batch Loss = 3.122756024704912
1641, epoch_train_loss=3.122756024704912
Epoch 1642
Epoch 1642 :: Batch 0/1
Batch Loss = 3.121446088440967
1642, epoch_train_loss=3.121446088440967
Epoch 1643
Epoch 1643 :: Batch 0/1
Batch Loss = 3.1221320400423793
1643, epoch_train_loss=3.1221320400423793
Epoch 1644
Epoch 1644 :: Batch 0/1
Batch Loss = 3.1213165704450314
1644, epoch_train_loss=3.1213165704450314
Epoch 1645
Epoch 1645 :: Batch 0/1
Batch Loss = 3.1215813208288923
1645, epoch_train_loss=3.1215813208288923
Epoch 1646
Epoch 1646 :: Batch 0/1
Batch Loss = 3.121112785829388
1646, epoch_train_loss=3.121112785829388
Epoch 1647
Epoch 1647 :: Batch 0/1
Batch Loss = 3.121093246244645
1647, epoch_train_loss=3.121093246244645
Epoch 1648
Epoch 1648 :: Batch 0/1
Batch Loss = 3.120885321591438
1648, epoch_train_loss=3.120885321591438
Epoch 1649
Epoch 1649 :: Batch 0/1
Batch Loss = 3.120661654311361
1649, epoch_train_loss=3.120661654311361
Epoch 1650
Epoch 1650 :: Batch 0/1
Batch Loss = 3.12061964208541
1650, epoch_train_loss=3.12061964208541
Epoch 1651
Epoch 1651 :: Batch 0/1
Batch Loss = 3.1202965919051877
1651, epoch_train_loss=3.1202965919051877
Epoch 1652
Epoch 1652 :: Batch 0/1
Batch Loss = 3.120326848171607
1652, epoch_train_loss=3.120326848171607
Epoch 1653
Epoch 1653 :: Batch 0/1
Batch Loss = 3.119944382311575
1653, epoch_train_loss=3.119944382311575
Epoch 1654
Epoch 1654 :: Batch 0/1
Batch Loss = 3.1200096874799175
1654, epoch_train_loss=3.1200096874799175
Epoch 1655
Epoch 1655 :: Batch 0/1
Batch Loss = 3.1196298342694893
1655, epoch_train_loss=3.1196298342694893
Epoch 1656
Epoch 1656 :: Batch 0/1
Batch Loss = 3.119689222361333
1656, epoch_train_loss=3.119689222361333
Epoch 1657
Epoch 1657 :: Batch 0/1
Batch Loss = 3.119317692814264
1657, epoch_train_loss=3.119317692814264
Epoch 1658
Epoch 1658 :: Batch 0/1
Batch Loss = 3.1193592769125256
1658, epoch_train_loss=3.1193592769125256
Epoch 1659
Epoch 1659 :: Batch 0/1
Batch Loss = 3.1190127653898605
1659, epoch_train_loss=3.1190127653898605
Epoch 1660
Epoch 1660 :: Batch 0/1
Batch Loss = 3.119022882578045
1660, epoch_train_loss=3.119022882578045
Epoch 1661
Epoch 1661 :: Batch 0/1
Batch Loss = 3.1187055911643533
1661, epoch_train_loss=3.1187055911643533
Epoch 1662
Epoch 1662 :: Batch 0/1
Batch Loss = 3.118688589597308
1662, epoch_train_loss=3.118688589597308
Epoch 1663
Epoch 1663 :: Batch 0/1
Batch Loss = 3.1183933790738116
1663, epoch_train_loss=3.1183933790738116
Epoch 1664
Epoch 1664 :: Batch 0/1
Batch Loss = 3.118345727175971
1664, epoch_train_loss=3.118345727175971
Epoch 1665
Epoch 1665 :: Batch 0/1
Batch Loss = 3.1180720100816397
1665, epoch_train_loss=3.1180720100816397
Epoch 1666
Epoch 1666 :: Batch 0/1
Batch Loss = 3.117998852211621
1666, epoch_train_loss=3.117998852211621
Epoch 1667
Epoch 1667 :: Batch 0/1
Batch Loss = 3.1177450148893677
1667, epoch_train_loss=3.1177450148893677
Epoch 1668
Epoch 1668 :: Batch 0/1
Batch Loss = 3.117644491959044
1668, epoch_train_loss=3.117644491959044
Epoch 1669
Epoch 1669 :: Batch 0/1
Batch Loss = 3.117403899263913
1669, epoch_train_loss=3.117403899263913
Epoch 1670
Epoch 1670 :: Batch 0/1
Batch Loss = 3.1172785974576223
1670, epoch_train_loss=3.1172785974576223
Epoch 1671
Epoch 1671 :: Batch 0/1
Batch Loss = 3.1170513271848743
1671, epoch_train_loss=3.1170513271848743
Epoch 1672
Epoch 1672 :: Batch 0/1
Batch Loss = 3.116901918427536
1672, epoch_train_loss=3.116901918427536
Epoch 1673
Epoch 1673 :: Batch 0/1
Batch Loss = 3.1166844655733357
1673, epoch_train_loss=3.1166844655733357
Epoch 1674
Epoch 1674 :: Batch 0/1
Batch Loss = 3.1165117066313237
1674, epoch_train_loss=3.1165117066313237
Epoch 1675
Epoch 1675 :: Batch 0/1
Batch Loss = 3.1163017310677326
1675, epoch_train_loss=3.1163017310677326
Epoch 1676
Epoch 1676 :: Batch 0/1
Batch Loss = 3.116107964657981
1676, epoch_train_loss=3.116107964657981
Epoch 1677
Epoch 1677 :: Batch 0/1
Batch Loss = 3.1159025174705897
1677, epoch_train_loss=3.1159025174705897
Epoch 1678
Epoch 1678 :: Batch 0/1
Batch Loss = 3.115689911762
1678, epoch_train_loss=3.115689911762
Epoch 1679
Epoch 1679 :: Batch 0/1
Batch Loss = 3.1154879744574595
1679, epoch_train_loss=3.1154879744574595
Epoch 1680
Epoch 1680 :: Batch 0/1
Batch Loss = 3.1152596247422233
1680, epoch_train_loss=3.1152596247422233
Epoch 1681
Epoch 1681 :: Batch 0/1
Batch Loss = 3.1150573153930745
1681, epoch_train_loss=3.1150573153930745
Epoch 1682
Epoch 1682 :: Batch 0/1
Batch Loss = 3.114819286128377
1682, epoch_train_loss=3.114819286128377
Epoch 1683
Epoch 1683 :: Batch 0/1
Batch Loss = 3.114615640570282
1683, epoch_train_loss=3.114615640570282
Epoch 1684
Epoch 1684 :: Batch 0/1
Batch Loss = 3.114373890230317
1684, epoch_train_loss=3.114373890230317
Epoch 1685
Epoch 1685 :: Batch 0/1
Batch Loss = 3.1141661210392173
1685, epoch_train_loss=3.1141661210392173
Epoch 1686
Epoch 1686 :: Batch 0/1
Batch Loss = 3.1139280122280955
1686, epoch_train_loss=3.1139280122280955
Epoch 1687
Epoch 1687 :: Batch 0/1
Batch Loss = 3.1137167139951383
1687, epoch_train_loss=3.1137167139951383
Epoch 1688
Epoch 1688 :: Batch 0/1
Batch Loss = 3.1134881892291095
1688, epoch_train_loss=3.1134881892291095
Epoch 1689
Epoch 1689 :: Batch 0/1
Batch Loss = 3.113274466172283
1689, epoch_train_loss=3.113274466172283
Epoch 1690
Epoch 1690 :: Batch 0/1
Batch Loss = 3.113060754572893
1690, epoch_train_loss=3.113060754572893
Epoch 1691
Epoch 1691 :: Batch 0/1
Batch Loss = 3.1128490140479084
1691, epoch_train_loss=3.1128490140479084
Epoch 1692
Epoch 1692 :: Batch 0/1
Batch Loss = 3.1126505603250028
1692, epoch_train_loss=3.1126505603250028
Epoch 1693
Epoch 1693 :: Batch 0/1
Batch Loss = 3.112447019612402
1693, epoch_train_loss=3.112447019612402
Epoch 1694
Epoch 1694 :: Batch 0/1
Batch Loss = 3.1122634754002982
1694, epoch_train_loss=3.1122634754002982
Epoch 1695
Epoch 1695 :: Batch 0/1
Batch Loss = 3.1120739951464236
1695, epoch_train_loss=3.1120739951464236
Epoch 1696
Epoch 1696 :: Batch 0/1
Batch Loss = 3.111902286953102
1696, epoch_train_loss=3.111902286953102
Epoch 1697
Epoch 1697 :: Batch 0/1
Batch Loss = 3.1117312119045266
1697, epoch_train_loss=3.1117312119045266
Epoch 1698
Epoch 1698 :: Batch 0/1
Batch Loss = 3.111569665843311
1698, epoch_train_loss=3.111569665843311
Epoch 1699
Epoch 1699 :: Batch 0/1
Batch Loss = 3.1114163530277357
1699, epoch_train_loss=3.1114163530277357
Epoch 1700
Epoch 1700 :: Batch 0/1
Batch Loss = 3.1112647730612277
1700, epoch_train_loss=3.1112647730612277
Epoch 1701
Epoch 1701 :: Batch 0/1
Batch Loss = 3.111125241954619
1701, epoch_train_loss=3.111125241954619
Epoch 1702
Epoch 1702 :: Batch 0/1
Batch Loss = 3.1109842564374874
1702, epoch_train_loss=3.1109842564374874
Epoch 1703
Epoch 1703 :: Batch 0/1
Batch Loss = 3.11085234696406
1703, epoch_train_loss=3.11085234696406
Epoch 1704
Epoch 1704 :: Batch 0/1
Batch Loss = 3.1107218092005966
1704, epoch_train_loss=3.1107218092005966
Epoch 1705
Epoch 1705 :: Batch 0/1
Batch Loss = 3.1105935182952558
1705, epoch_train_loss=3.1105935182952558
Epoch 1706
Epoch 1706 :: Batch 0/1
Batch Loss = 3.1104697843745663
1706, epoch_train_loss=3.1104697843745663
Epoch 1707
Epoch 1707 :: Batch 0/1
Batch Loss = 3.110343940189292
1707, epoch_train_loss=3.110343940189292
Epoch 1708
Epoch 1708 :: Batch 0/1
Batch Loss = 3.1102221659588714
1708, epoch_train_loss=3.1102221659588714
Epoch 1709
Epoch 1709 :: Batch 0/1
Batch Loss = 3.1100986800955797
1709, epoch_train_loss=3.1100986800955797
Epoch 1710
Epoch 1710 :: Batch 0/1
Batch Loss = 3.1099753201536324
1710, epoch_train_loss=3.1099753201536324
Epoch 1711
Epoch 1711 :: Batch 0/1
Batch Loss = 3.109852877064715
1711, epoch_train_loss=3.109852877064715
Epoch 1712
Epoch 1712 :: Batch 0/1
Batch Loss = 3.1097276048433384
1712, epoch_train_loss=3.1097276048433384
Epoch 1713
Epoch 1713 :: Batch 0/1
Batch Loss = 3.109603444908093
1713, epoch_train_loss=3.109603444908093
Epoch 1714
Epoch 1714 :: Batch 0/1
Batch Loss = 3.109477432068486
1714, epoch_train_loss=3.109477432068486
Epoch 1715
Epoch 1715 :: Batch 0/1
Batch Loss = 3.1093502438976492
1715, epoch_train_loss=3.1093502438976492
Epoch 1716
Epoch 1716 :: Batch 0/1
Batch Loss = 3.109223284203286
1716, epoch_train_loss=3.109223284203286
Epoch 1717
Epoch 1717 :: Batch 0/1
Batch Loss = 3.1090941515539927
1717, epoch_train_loss=3.1090941515539927
Epoch 1718
Epoch 1718 :: Batch 0/1
Batch Loss = 3.1089651603072865
1718, epoch_train_loss=3.1089651603072865
Epoch 1719
Epoch 1719 :: Batch 0/1
Batch Loss = 3.1088354547834562
1719, epoch_train_loss=3.1088354547834562
Epoch 1720
Epoch 1720 :: Batch 0/1
Batch Loss = 3.1087045825905815
1720, epoch_train_loss=3.1087045825905815
Epoch 1721
Epoch 1721 :: Batch 0/1
Batch Loss = 3.1085741757242014
1721, epoch_train_loss=3.1085741757242014
Epoch 1722
Epoch 1722 :: Batch 0/1
Batch Loss = 3.1084428282790393
1722, epoch_train_loss=3.1084428282790393
Epoch 1723
Epoch 1723 :: Batch 0/1
Batch Loss = 3.108311238662466
1723, epoch_train_loss=3.108311238662466
Epoch 1724
Epoch 1724 :: Batch 0/1
Batch Loss = 3.108179951882572
1724, epoch_train_loss=3.108179951882572
Epoch 1725
Epoch 1725 :: Batch 0/1
Batch Loss = 3.1080479288507727
1725, epoch_train_loss=3.1080479288507727
Epoch 1726
Epoch 1726 :: Batch 0/1
Batch Loss = 3.107916098081296
1726, epoch_train_loss=3.107916098081296
Epoch 1727
Epoch 1727 :: Batch 0/1
Batch Loss = 3.107784316468997
1727, epoch_train_loss=3.107784316468997
Epoch 1728
Epoch 1728 :: Batch 0/1
Batch Loss = 3.1076520199641924
1728, epoch_train_loss=3.1076520199641924
Epoch 1729
Epoch 1729 :: Batch 0/1
Batch Loss = 3.107519948696493
1729, epoch_train_loss=3.107519948696493
Epoch 1730
Epoch 1730 :: Batch 0/1
Batch Loss = 3.107387707044582
1730, epoch_train_loss=3.107387707044582
Epoch 1731
Epoch 1731 :: Batch 0/1
Batch Loss = 3.107255045478854
1731, epoch_train_loss=3.107255045478854
Epoch 1732
Epoch 1732 :: Batch 0/1
Batch Loss = 3.107122441079384
1732, epoch_train_loss=3.107122441079384
Epoch 1733
Epoch 1733 :: Batch 0/1
Batch Loss = 3.1069894906156756
1733, epoch_train_loss=3.1069894906156756
Epoch 1734
Epoch 1734 :: Batch 0/1
Batch Loss = 3.1068560826716407
1734, epoch_train_loss=3.1068560826716407
Epoch 1735
Epoch 1735 :: Batch 0/1
Batch Loss = 3.1067225238689313
1735, epoch_train_loss=3.1067225238689313
Epoch 1736
Epoch 1736 :: Batch 0/1
Batch Loss = 3.106588480846885
1736, epoch_train_loss=3.106588480846885
Epoch 1737
Epoch 1737 :: Batch 0/1
Batch Loss = 3.106453885331891
1737, epoch_train_loss=3.106453885331891
Epoch 1738
Epoch 1738 :: Batch 0/1
Batch Loss = 3.106318946394522
1738, epoch_train_loss=3.106318946394522
Epoch 1739
Epoch 1739 :: Batch 0/1
Batch Loss = 3.106183427087756
1739, epoch_train_loss=3.106183427087756
Epoch 1740
Epoch 1740 :: Batch 0/1
Batch Loss = 3.1060472541519535
1740, epoch_train_loss=3.1060472541519535
Epoch 1741
Epoch 1741 :: Batch 0/1
Batch Loss = 3.1059105926142707
1741, epoch_train_loss=3.1059105926142707
Epoch 1742
Epoch 1742 :: Batch 0/1
Batch Loss = 3.1057732906046853
1742, epoch_train_loss=3.1057732906046853
Epoch 1743
Epoch 1743 :: Batch 0/1
Batch Loss = 3.10563525869359
1743, epoch_train_loss=3.10563525869359
Epoch 1744
Epoch 1744 :: Batch 0/1
Batch Loss = 3.1054966306492275
1744, epoch_train_loss=3.1054966306492275
Epoch 1745
Epoch 1745 :: Batch 0/1
Batch Loss = 3.1053573337206255
1745, epoch_train_loss=3.1053573337206255
Epoch 1746
Epoch 1746 :: Batch 0/1
Batch Loss = 3.1052172654689496
1746, epoch_train_loss=3.1052172654689496
Epoch 1747
Epoch 1747 :: Batch 0/1
Batch Loss = 3.1050765239225124
1747, epoch_train_loss=3.1050765239225124
Epoch 1748
Epoch 1748 :: Batch 0/1
Batch Loss = 3.104935098668727
1748, epoch_train_loss=3.104935098668727
Epoch 1749
Epoch 1749 :: Batch 0/1
Batch Loss = 3.104792892466324
1749, epoch_train_loss=3.104792892466324
Epoch 1750
Epoch 1750 :: Batch 0/1
Batch Loss = 3.104649954557477
1750, epoch_train_loss=3.104649954557477
Epoch 1751
Epoch 1751 :: Batch 0/1
Batch Loss = 3.104506313917662
1751, epoch_train_loss=3.104506313917662
Epoch 1752
Epoch 1752 :: Batch 0/1
Batch Loss = 3.104361905112854
1752, epoch_train_loss=3.104361905112854
Epoch 1753
Epoch 1753 :: Batch 0/1
Batch Loss = 3.1042167256548376
1753, epoch_train_loss=3.1042167256548376
Epoch 1754
Epoch 1754 :: Batch 0/1
Batch Loss = 3.104070804477997
1754, epoch_train_loss=3.104070804477997
Epoch 1755
Epoch 1755 :: Batch 0/1
Batch Loss = 3.103924128066313
1755, epoch_train_loss=3.103924128066313
Epoch 1756
Epoch 1756 :: Batch 0/1
Batch Loss = 3.103776666485569
1756, epoch_train_loss=3.103776666485569
Epoch 1757
Epoch 1757 :: Batch 0/1
Batch Loss = 3.10362841417203
1757, epoch_train_loss=3.10362841417203
Epoch 1758
Epoch 1758 :: Batch 0/1
Batch Loss = 3.103479393109926
1758, epoch_train_loss=3.103479393109926
Epoch 1759
Epoch 1759 :: Batch 0/1
Batch Loss = 3.103329583177122
1759, epoch_train_loss=3.103329583177122
Epoch 1760
Epoch 1760 :: Batch 0/1
Batch Loss = 3.1031789488464447
1760, epoch_train_loss=3.1031789488464447
Epoch 1761
Epoch 1761 :: Batch 0/1
Batch Loss = 3.1030275055362253
1761, epoch_train_loss=3.1030275055362253
Epoch 1762
Epoch 1762 :: Batch 0/1
Batch Loss = 3.1028752509294395
1762, epoch_train_loss=3.1028752509294395
Epoch 1763
Epoch 1763 :: Batch 0/1
Batch Loss = 3.1027221605804054
1763, epoch_train_loss=3.1027221605804054
Epoch 1764
Epoch 1764 :: Batch 0/1
Batch Loss = 3.10256822397179
1764, epoch_train_loss=3.10256822397179
Epoch 1765
Epoch 1765 :: Batch 0/1
Batch Loss = 3.102413429399459
1765, epoch_train_loss=3.102413429399459
Epoch 1766
Epoch 1766 :: Batch 0/1
Batch Loss = 3.1022577800420734
1766, epoch_train_loss=3.1022577800420734
Epoch 1767
Epoch 1767 :: Batch 0/1
Batch Loss = 3.1021012631074645
1767, epoch_train_loss=3.1021012631074645
Epoch 1768
Epoch 1768 :: Batch 0/1
Batch Loss = 3.1019438539114716
1768, epoch_train_loss=3.1019438539114716
Epoch 1769
Epoch 1769 :: Batch 0/1
Batch Loss = 3.101785552870687
1769, epoch_train_loss=3.101785552870687
Epoch 1770
Epoch 1770 :: Batch 0/1
Batch Loss = 3.10162635010955
1770, epoch_train_loss=3.10162635010955
Epoch 1771
Epoch 1771 :: Batch 0/1
Batch Loss = 3.1014662389070096
1771, epoch_train_loss=3.1014662389070096
Epoch 1772
Epoch 1772 :: Batch 0/1
Batch Loss = 3.1013052093575704
1772, epoch_train_loss=3.1013052093575704
Epoch 1773
Epoch 1773 :: Batch 0/1
Batch Loss = 3.101143243150837
1773, epoch_train_loss=3.101143243150837
Epoch 1774
Epoch 1774 :: Batch 0/1
Batch Loss = 3.1009803404484586
1774, epoch_train_loss=3.1009803404484586
Epoch 1775
Epoch 1775 :: Batch 0/1
Batch Loss = 3.1008164900740938
1775, epoch_train_loss=3.1008164900740938
Epoch 1776
Epoch 1776 :: Batch 0/1
Batch Loss = 3.1006516861805347
1776, epoch_train_loss=3.1006516861805347
Epoch 1777
Epoch 1777 :: Batch 0/1
Batch Loss = 3.1004859212350295
1777, epoch_train_loss=3.1004859212350295
Epoch 1778
Epoch 1778 :: Batch 0/1
Batch Loss = 3.100319180432851
1778, epoch_train_loss=3.100319180432851
Epoch 1779
Epoch 1779 :: Batch 0/1
Batch Loss = 3.100151461598128
1779, epoch_train_loss=3.100151461598128
Epoch 1780
Epoch 1780 :: Batch 0/1
Batch Loss = 3.0999827514645304
1780, epoch_train_loss=3.0999827514645304
Epoch 1781
Epoch 1781 :: Batch 0/1
Batch Loss = 3.099813047607843
1781, epoch_train_loss=3.099813047607843
Epoch 1782
Epoch 1782 :: Batch 0/1
Batch Loss = 3.099642340086694
1782, epoch_train_loss=3.099642340086694
Epoch 1783
Epoch 1783 :: Batch 0/1
Batch Loss = 3.0994706196738906
1783, epoch_train_loss=3.0994706196738906
Epoch 1784
Epoch 1784 :: Batch 0/1
Batch Loss = 3.0992978804749325
1784, epoch_train_loss=3.0992978804749325
Epoch 1785
Epoch 1785 :: Batch 0/1
Batch Loss = 3.099124109248279
1785, epoch_train_loss=3.099124109248279
Epoch 1786
Epoch 1786 :: Batch 0/1
Batch Loss = 3.0989493029883626
1786, epoch_train_loss=3.0989493029883626
Epoch 1787
Epoch 1787 :: Batch 0/1
Batch Loss = 3.098773447074167
1787, epoch_train_loss=3.098773447074167
Epoch 1788
Epoch 1788 :: Batch 0/1
Batch Loss = 3.0985965389987653
1788, epoch_train_loss=3.0985965389987653
Epoch 1789
Epoch 1789 :: Batch 0/1
Batch Loss = 3.0984185646686098
1789, epoch_train_loss=3.0984185646686098
Epoch 1790
Epoch 1790 :: Batch 0/1
Batch Loss = 3.098239520030223
1790, epoch_train_loss=3.098239520030223
Epoch 1791
Epoch 1791 :: Batch 0/1
Batch Loss = 3.0980593919185333
1791, epoch_train_loss=3.0980593919185333
Epoch 1792
Epoch 1792 :: Batch 0/1
Batch Loss = 3.097878175120212
1792, epoch_train_loss=3.097878175120212
Epoch 1793
Epoch 1793 :: Batch 0/1
Batch Loss = 3.0976958572218005
1793, epoch_train_loss=3.0976958572218005
Epoch 1794
Epoch 1794 :: Batch 0/1
Batch Loss = 3.097512432162365
1794, epoch_train_loss=3.097512432162365
Epoch 1795
Epoch 1795 :: Batch 0/1
Batch Loss = 3.0973278877512613
1795, epoch_train_loss=3.0973278877512613
Epoch 1796
Epoch 1796 :: Batch 0/1
Batch Loss = 3.0971422186277886
1796, epoch_train_loss=3.0971422186277886
Epoch 1797
Epoch 1797 :: Batch 0/1
Batch Loss = 3.096955411467377
1797, epoch_train_loss=3.096955411467377
Epoch 1798
Epoch 1798 :: Batch 0/1
Batch Loss = 3.0967674635050533
1798, epoch_train_loss=3.0967674635050533
Epoch 1799
Epoch 1799 :: Batch 0/1
Batch Loss = 3.096578359400582
1799, epoch_train_loss=3.096578359400582
Epoch 1800
Epoch 1800 :: Batch 0/1
Batch Loss = 3.0963881058865352
1800, epoch_train_loss=3.0963881058865352
Epoch 1801
Epoch 1801 :: Batch 0/1
Batch Loss = 3.0961966879773954
1801, epoch_train_loss=3.0961966879773954
Epoch 1802
Epoch 1802 :: Batch 0/1
Batch Loss = 3.096004155986714
1802, epoch_train_loss=3.096004155986714
Epoch 1803
Epoch 1803 :: Batch 0/1
Batch Loss = 3.095810552034136
1803, epoch_train_loss=3.095810552034136
Epoch 1804
Epoch 1804 :: Batch 0/1
Batch Loss = 3.095616221702028
1804, epoch_train_loss=3.095616221702028
Epoch 1805
Epoch 1805 :: Batch 0/1
Batch Loss = 3.095421926887069
1805, epoch_train_loss=3.095421926887069
Epoch 1806
Epoch 1806 :: Batch 0/1
Batch Loss = 3.0952307659952
1806, epoch_train_loss=3.0952307659952
Epoch 1807
Epoch 1807 :: Batch 0/1
Batch Loss = 3.095052082155313
1807, epoch_train_loss=3.095052082155313
Epoch 1808
Epoch 1808 :: Batch 0/1
Batch Loss = 3.094920967252302
1808, epoch_train_loss=3.094920967252302
Epoch 1809
Epoch 1809 :: Batch 0/1
Batch Loss = 3.0949576676523236
1809, epoch_train_loss=3.0949576676523236
Epoch 1810
Epoch 1810 :: Batch 0/1
Batch Loss = 3.095639385410191
1810, epoch_train_loss=3.095639385410191
Epoch 1811
Epoch 1811 :: Batch 0/1
Batch Loss = 3.0986470950091722
1811, epoch_train_loss=3.0986470950091722
Epoch 1812
Epoch 1812 :: Batch 0/1
Batch Loss = 3.111505340569465
1812, epoch_train_loss=3.111505340569465
Epoch 1813
Epoch 1813 :: Batch 0/1
Batch Loss = 3.154712539035171
1813, epoch_train_loss=3.154712539035171
Epoch 1814
Epoch 1814 :: Batch 0/1
Batch Loss = 3.325860632742594
1814, epoch_train_loss=3.325860632742594
Epoch 1815
Epoch 1815 :: Batch 0/1
Batch Loss = 3.4112721110809265
1815, epoch_train_loss=3.4112721110809265
Epoch 1816
Epoch 1816 :: Batch 0/1
Batch Loss = 3.4890645861270175
1816, epoch_train_loss=3.4890645861270175
Epoch 1817
Epoch 1817 :: Batch 0/1
Batch Loss = 3.1227471306017285
1817, epoch_train_loss=3.1227471306017285
Epoch 1818
Epoch 1818 :: Batch 0/1
Batch Loss = 3.5822620893914165
1818, epoch_train_loss=3.5822620893914165
Epoch 1819
Epoch 1819 :: Batch 0/1
Batch Loss = 3.419337754338576
1819, epoch_train_loss=3.419337754338576
Epoch 1820
Epoch 1820 :: Batch 0/1
Batch Loss = 3.457194996150315
1820, epoch_train_loss=3.457194996150315
Epoch 1821
Epoch 1821 :: Batch 0/1
Batch Loss = 3.1840997580204267
1821, epoch_train_loss=3.1840997580204267
Epoch 1822
Epoch 1822 :: Batch 0/1
Batch Loss = 3.47967941395305
1822, epoch_train_loss=3.47967941395305
Epoch 1823
Epoch 1823 :: Batch 0/1
Batch Loss = 3.167584540752423
1823, epoch_train_loss=3.167584540752423
Epoch 1824
Epoch 1824 :: Batch 0/1
Batch Loss = 3.3353152722525876
1824, epoch_train_loss=3.3353152722525876
Epoch 1825
Epoch 1825 :: Batch 0/1
Batch Loss = 3.2629435609189903
1825, epoch_train_loss=3.2629435609189903
Epoch 1826
Epoch 1826 :: Batch 0/1
Batch Loss = 3.1844685955038776
1826, epoch_train_loss=3.1844685955038776
Epoch 1827
Epoch 1827 :: Batch 0/1
Batch Loss = 3.2511165977026737
1827, epoch_train_loss=3.2511165977026737
Epoch 1828
Epoch 1828 :: Batch 0/1
Batch Loss = 3.2008910981775984
1828, epoch_train_loss=3.2008910981775984
Epoch 1829
Epoch 1829 :: Batch 0/1
Batch Loss = 3.16963817655164
1829, epoch_train_loss=3.16963817655164
Epoch 1830
Epoch 1830 :: Batch 0/1
Batch Loss = 3.220139727963335
1830, epoch_train_loss=3.220139727963335
Epoch 1831
Epoch 1831 :: Batch 0/1
Batch Loss = 3.1940285645315796
1831, epoch_train_loss=3.1940285645315796
Epoch 1832
Epoch 1832 :: Batch 0/1
Batch Loss = 3.136718161018664
1832, epoch_train_loss=3.136718161018664
Epoch 1833
Epoch 1833 :: Batch 0/1
Batch Loss = 3.1936563754586804
1833, epoch_train_loss=3.1936563754586804
Epoch 1834
Epoch 1834 :: Batch 0/1
Batch Loss = 3.163963068492573
1834, epoch_train_loss=3.163963068492573
Epoch 1835
Epoch 1835 :: Batch 0/1
Batch Loss = 3.1225991969260147
1835, epoch_train_loss=3.1225991969260147
Epoch 1836
Epoch 1836 :: Batch 0/1
Batch Loss = 3.1665263446785765
1836, epoch_train_loss=3.1665263446785765
Epoch 1837
Epoch 1837 :: Batch 0/1
Batch Loss = 3.1571572675401143
1837, epoch_train_loss=3.1571572675401143
Epoch 1838
Epoch 1838 :: Batch 0/1
Batch Loss = 3.116551022730402
1838, epoch_train_loss=3.116551022730402
Epoch 1839
Epoch 1839 :: Batch 0/1
Batch Loss = 3.1425687179614585
1839, epoch_train_loss=3.1425687179614585
Epoch 1840
Epoch 1840 :: Batch 0/1
Batch Loss = 3.1456765256156785
1840, epoch_train_loss=3.1456765256156785
Epoch 1841
Epoch 1841 :: Batch 0/1
Batch Loss = 3.1111179890803426
1841, epoch_train_loss=3.1111179890803426
Epoch 1842
Epoch 1842 :: Batch 0/1
Batch Loss = 3.1263454748228243
1842, epoch_train_loss=3.1263454748228243
Epoch 1843
Epoch 1843 :: Batch 0/1
Batch Loss = 3.134604278977269
1843, epoch_train_loss=3.134604278977269
Epoch 1844
Epoch 1844 :: Batch 0/1
Batch Loss = 3.116555136839413
1844, epoch_train_loss=3.116555136839413
Epoch 1845
Epoch 1845 :: Batch 0/1
Batch Loss = 3.116722228952204
1845, epoch_train_loss=3.116722228952204
Epoch 1846
Epoch 1846 :: Batch 0/1
Batch Loss = 3.128233671019238
1846, epoch_train_loss=3.128233671019238
Epoch 1847
Epoch 1847 :: Batch 0/1
Batch Loss = 3.1158846408381753
1847, epoch_train_loss=3.1158846408381753
Epoch 1848
Epoch 1848 :: Batch 0/1
Batch Loss = 3.1109509920059204
1848, epoch_train_loss=3.1109509920059204
Epoch 1849
Epoch 1849 :: Batch 0/1
Batch Loss = 3.120584789900008
1849, epoch_train_loss=3.120584789900008
Epoch 1850
Epoch 1850 :: Batch 0/1
Batch Loss = 3.1157971945338025
1850, epoch_train_loss=3.1157971945338025
Epoch 1851
Epoch 1851 :: Batch 0/1
Batch Loss = 3.1107562898582857
1851, epoch_train_loss=3.1107562898582857
Epoch 1852
Epoch 1852 :: Batch 0/1
Batch Loss = 3.1156695155640457
1852, epoch_train_loss=3.1156695155640457
Epoch 1853
Epoch 1853 :: Batch 0/1
Batch Loss = 3.1134738839835947
1853, epoch_train_loss=3.1134738839835947
Epoch 1854
Epoch 1854 :: Batch 0/1
Batch Loss = 3.10881282812958
1854, epoch_train_loss=3.10881282812958
Epoch 1855
Epoch 1855 :: Batch 0/1
Batch Loss = 3.1093259352844256
1855, epoch_train_loss=3.1093259352844256
Epoch 1856
Epoch 1856 :: Batch 0/1
Batch Loss = 3.1104218039747877
1856, epoch_train_loss=3.1104218039747877
Epoch 1857
Epoch 1857 :: Batch 0/1
Batch Loss = 3.1069574362879178
1857, epoch_train_loss=3.1069574362879178
Epoch 1858
Epoch 1858 :: Batch 0/1
Batch Loss = 3.106618419176882
1858, epoch_train_loss=3.106618419176882
Epoch 1859
Epoch 1859 :: Batch 0/1
Batch Loss = 3.107827270662368
1859, epoch_train_loss=3.107827270662368
Epoch 1860
Epoch 1860 :: Batch 0/1
Batch Loss = 3.1050268075459115
1860, epoch_train_loss=3.1050268075459115
Epoch 1861
Epoch 1861 :: Batch 0/1
Batch Loss = 3.1043410957639206
1861, epoch_train_loss=3.1043410957639206
Epoch 1862
Epoch 1862 :: Batch 0/1
Batch Loss = 3.1044818158450265
1862, epoch_train_loss=3.1044818158450265
Epoch 1863
Epoch 1863 :: Batch 0/1
Batch Loss = 3.1035797576125903
1863, epoch_train_loss=3.1035797576125903
Epoch 1864
Epoch 1864 :: Batch 0/1
Batch Loss = 3.102481428707322
1864, epoch_train_loss=3.102481428707322
Epoch 1865
Epoch 1865 :: Batch 0/1
Batch Loss = 3.1030630561425694
1865, epoch_train_loss=3.1030630561425694
Epoch 1866
Epoch 1866 :: Batch 0/1
Batch Loss = 3.102322495812954
1866, epoch_train_loss=3.102322495812954
Epoch 1867
Epoch 1867 :: Batch 0/1
Batch Loss = 3.1013281253614693
1867, epoch_train_loss=3.1013281253614693
Epoch 1868
Epoch 1868 :: Batch 0/1
Batch Loss = 3.1014452215813577
1868, epoch_train_loss=3.1014452215813577
Epoch 1869
Epoch 1869 :: Batch 0/1
Batch Loss = 3.101199397874504
1869, epoch_train_loss=3.101199397874504
Epoch 1870
Epoch 1870 :: Batch 0/1
Batch Loss = 3.1004661679033045
1870, epoch_train_loss=3.1004661679033045
Epoch 1871
Epoch 1871 :: Batch 0/1
Batch Loss = 3.1002385311641154
1871, epoch_train_loss=3.1002385311641154
Epoch 1872
Epoch 1872 :: Batch 0/1
Batch Loss = 3.10030007714699
1872, epoch_train_loss=3.10030007714699
Epoch 1873
Epoch 1873 :: Batch 0/1
Batch Loss = 3.09944460527349
1873, epoch_train_loss=3.09944460527349
Epoch 1874
Epoch 1874 :: Batch 0/1
Batch Loss = 3.099240391125648
1874, epoch_train_loss=3.099240391125648
Epoch 1875
Epoch 1875 :: Batch 0/1
Batch Loss = 3.0991504468736366
1875, epoch_train_loss=3.0991504468736366
Epoch 1876
Epoch 1876 :: Batch 0/1
Batch Loss = 3.098662370272777
1876, epoch_train_loss=3.098662370272777
Epoch 1877
Epoch 1877 :: Batch 0/1
Batch Loss = 3.0981280957759565
1877, epoch_train_loss=3.0981280957759565
Epoch 1878
Epoch 1878 :: Batch 0/1
Batch Loss = 3.0981051012627945
1878, epoch_train_loss=3.0981051012627945
Epoch 1879
Epoch 1879 :: Batch 0/1
Batch Loss = 3.097551272485967
1879, epoch_train_loss=3.097551272485967
Epoch 1880
Epoch 1880 :: Batch 0/1
Batch Loss = 3.0971151533685326
1880, epoch_train_loss=3.0971151533685326
Epoch 1881
Epoch 1881 :: Batch 0/1
Batch Loss = 3.096988717293618
1881, epoch_train_loss=3.096988717293618
Epoch 1882
Epoch 1882 :: Batch 0/1
Batch Loss = 3.0967066712436697
1882, epoch_train_loss=3.0967066712436697
Epoch 1883
Epoch 1883 :: Batch 0/1
Batch Loss = 3.096140428527618
1883, epoch_train_loss=3.096140428527618
Epoch 1884
Epoch 1884 :: Batch 0/1
Batch Loss = 3.0959913581496705
1884, epoch_train_loss=3.0959913581496705
Epoch 1885
Epoch 1885 :: Batch 0/1
Batch Loss = 3.095663744789882
1885, epoch_train_loss=3.095663744789882
Epoch 1886
Epoch 1886 :: Batch 0/1
Batch Loss = 3.0952117948070414
1886, epoch_train_loss=3.0952117948070414
Epoch 1887
Epoch 1887 :: Batch 0/1
Batch Loss = 3.0949932968386418
1887, epoch_train_loss=3.0949932968386418
Epoch 1888
Epoch 1888 :: Batch 0/1
Batch Loss = 3.0948119516337522
1888, epoch_train_loss=3.0948119516337522
Epoch 1889
Epoch 1889 :: Batch 0/1
Batch Loss = 3.094349332797992
1889, epoch_train_loss=3.094349332797992
Epoch 1890
Epoch 1890 :: Batch 0/1
Batch Loss = 3.094103270490626
1890, epoch_train_loss=3.094103270490626
Epoch 1891
Epoch 1891 :: Batch 0/1
Batch Loss = 3.0938646884227614
1891, epoch_train_loss=3.0938646884227614
Epoch 1892
Epoch 1892 :: Batch 0/1
Batch Loss = 3.09347642309759
1892, epoch_train_loss=3.09347642309759
Epoch 1893
Epoch 1893 :: Batch 0/1
Batch Loss = 3.093211475448782
1893, epoch_train_loss=3.093211475448782
Epoch 1894
Epoch 1894 :: Batch 0/1
Batch Loss = 3.0930131044381373
1894, epoch_train_loss=3.0930131044381373
Epoch 1895
Epoch 1895 :: Batch 0/1
Batch Loss = 3.0926529605007875
1895, epoch_train_loss=3.0926529605007875
Epoch 1896
Epoch 1896 :: Batch 0/1
Batch Loss = 3.0923473901815433
1896, epoch_train_loss=3.0923473901815433
Epoch 1897
Epoch 1897 :: Batch 0/1
Batch Loss = 3.0921209113317003
1897, epoch_train_loss=3.0921209113317003
Epoch 1898
Epoch 1898 :: Batch 0/1
Batch Loss = 3.0917655395362846
1898, epoch_train_loss=3.0917655395362846
Epoch 1899
Epoch 1899 :: Batch 0/1
Batch Loss = 3.091477362406664
1899, epoch_train_loss=3.091477362406664
Epoch 1900
Epoch 1900 :: Batch 0/1
Batch Loss = 3.091231957399081
1900, epoch_train_loss=3.091231957399081
Epoch 1901
Epoch 1901 :: Batch 0/1
Batch Loss = 3.090912172739765
1901, epoch_train_loss=3.090912172739765
Epoch 1902
Epoch 1902 :: Batch 0/1
Batch Loss = 3.0905971407203032
1902, epoch_train_loss=3.0905971407203032
Epoch 1903
Epoch 1903 :: Batch 0/1
Batch Loss = 3.0903332843725524
1903, epoch_train_loss=3.0903332843725524
Epoch 1904
Epoch 1904 :: Batch 0/1
Batch Loss = 3.090016399590317
1904, epoch_train_loss=3.090016399590317
Epoch 1905
Epoch 1905 :: Batch 0/1
Batch Loss = 3.089704453823605
1905, epoch_train_loss=3.089704453823605
Epoch 1906
Epoch 1906 :: Batch 0/1
Batch Loss = 3.0894378857851876
1906, epoch_train_loss=3.0894378857851876
Epoch 1907
Epoch 1907 :: Batch 0/1
Batch Loss = 3.0891221120726287
1907, epoch_train_loss=3.0891221120726287
Epoch 1908
Epoch 1908 :: Batch 0/1
Batch Loss = 3.0888125533918735
1908, epoch_train_loss=3.0888125533918735
Epoch 1909
Epoch 1909 :: Batch 0/1
Batch Loss = 3.0885250442757224
1909, epoch_train_loss=3.0885250442757224
Epoch 1910
Epoch 1910 :: Batch 0/1
Batch Loss = 3.088213179810521
1910, epoch_train_loss=3.088213179810521
Epoch 1911
Epoch 1911 :: Batch 0/1
Batch Loss = 3.0879069418730234
1911, epoch_train_loss=3.0879069418730234
Epoch 1912
Epoch 1912 :: Batch 0/1
Batch Loss = 3.087610385676298
1912, epoch_train_loss=3.087610385676298
Epoch 1913
Epoch 1913 :: Batch 0/1
Batch Loss = 3.0873027376548707
1913, epoch_train_loss=3.0873027376548707
Epoch 1914
Epoch 1914 :: Batch 0/1
Batch Loss = 3.086991916760284
1914, epoch_train_loss=3.086991916760284
Epoch 1915
Epoch 1915 :: Batch 0/1
Batch Loss = 3.08668636910411
1915, epoch_train_loss=3.08668636910411
Epoch 1916
Epoch 1916 :: Batch 0/1
Batch Loss = 3.0863783158151774
1916, epoch_train_loss=3.0863783158151774
Epoch 1917
Epoch 1917 :: Batch 0/1
Batch Loss = 3.0860652678661133
1917, epoch_train_loss=3.0860652678661133
Epoch 1918
Epoch 1918 :: Batch 0/1
Batch Loss = 3.085754819493515
1918, epoch_train_loss=3.085754819493515
Epoch 1919
Epoch 1919 :: Batch 0/1
Batch Loss = 3.085443106708847
1919, epoch_train_loss=3.085443106708847
Epoch 1920
Epoch 1920 :: Batch 0/1
Batch Loss = 3.085125433523278
1920, epoch_train_loss=3.085125433523278
Epoch 1921
Epoch 1921 :: Batch 0/1
Batch Loss = 3.08480916560017
1921, epoch_train_loss=3.08480916560017
Epoch 1922
Epoch 1922 :: Batch 0/1
Batch Loss = 3.0844942076665345
1922, epoch_train_loss=3.0844942076665345
Epoch 1923
Epoch 1923 :: Batch 0/1
Batch Loss = 3.084173762679679
1923, epoch_train_loss=3.084173762679679
Epoch 1924
Epoch 1924 :: Batch 0/1
Batch Loss = 3.0838525051781085
1924, epoch_train_loss=3.0838525051781085
Epoch 1925
Epoch 1925 :: Batch 0/1
Batch Loss = 3.083532578185674
1925, epoch_train_loss=3.083532578185674
Epoch 1926
Epoch 1926 :: Batch 0/1
Batch Loss = 3.08320727984859
1926, epoch_train_loss=3.08320727984859
Epoch 1927
Epoch 1927 :: Batch 0/1
Batch Loss = 3.082880830978707
1927, epoch_train_loss=3.082880830978707
Epoch 1928
Epoch 1928 :: Batch 0/1
Batch Loss = 3.082556858875202
1928, epoch_train_loss=3.082556858875202
Epoch 1929
Epoch 1929 :: Batch 0/1
Batch Loss = 3.08222799201872
1929, epoch_train_loss=3.08222799201872
Epoch 1930
Epoch 1930 :: Batch 0/1
Batch Loss = 3.0818969639769835
1930, epoch_train_loss=3.0818969639769835
Epoch 1931
Epoch 1931 :: Batch 0/1
Batch Loss = 3.0815675978213144
1931, epoch_train_loss=3.0815675978213144
Epoch 1932
Epoch 1932 :: Batch 0/1
Batch Loss = 3.0812340651532395
1932, epoch_train_loss=3.0812340651532395
Epoch 1933
Epoch 1933 :: Batch 0/1
Batch Loss = 3.0808988031345867
1933, epoch_train_loss=3.0808988031345867
Epoch 1934
Epoch 1934 :: Batch 0/1
Batch Loss = 3.0805646356612386
1934, epoch_train_loss=3.0805646356612386
Epoch 1935
Epoch 1935 :: Batch 0/1
Batch Loss = 3.0802270037823636
1935, epoch_train_loss=3.0802270037823636
Epoch 1936
Epoch 1936 :: Batch 0/1
Batch Loss = 3.0798877946327323
1936, epoch_train_loss=3.0798877946327323
Epoch 1937
Epoch 1937 :: Batch 0/1
Batch Loss = 3.079548082578963
1937, epoch_train_loss=3.079548082578963
Epoch 1938
Epoch 1938 :: Batch 0/1
Batch Loss = 3.0792051447609516
1938, epoch_train_loss=3.0792051447609516
Epoch 1939
Epoch 1939 :: Batch 0/1
Batch Loss = 3.0788615958776093
1939, epoch_train_loss=3.0788615958776093
Epoch 1940
Epoch 1940 :: Batch 0/1
Batch Loss = 3.078517351300191
1940, epoch_train_loss=3.078517351300191
Epoch 1941
Epoch 1941 :: Batch 0/1
Batch Loss = 3.0781698915109517
1941, epoch_train_loss=3.0781698915109517
Epoch 1942
Epoch 1942 :: Batch 0/1
Batch Loss = 3.077821361920247
1942, epoch_train_loss=3.077821361920247
Epoch 1943
Epoch 1943 :: Batch 0/1
Batch Loss = 3.0774717827340647
1943, epoch_train_loss=3.0774717827340647
Epoch 1944
Epoch 1944 :: Batch 0/1
Batch Loss = 3.0771197484608854
1944, epoch_train_loss=3.0771197484608854
Epoch 1945
Epoch 1945 :: Batch 0/1
Batch Loss = 3.0767666590490834
1945, epoch_train_loss=3.0767666590490834
Epoch 1946
Epoch 1946 :: Batch 0/1
Batch Loss = 3.076412033590417
1946, epoch_train_loss=3.076412033590417
Epoch 1947
Epoch 1947 :: Batch 0/1
Batch Loss = 3.0760550624849956
1947, epoch_train_loss=3.0760550624849956
Epoch 1948
Epoch 1948 :: Batch 0/1
Batch Loss = 3.0756968962213147
1948, epoch_train_loss=3.0756968962213147
Epoch 1949
Epoch 1949 :: Batch 0/1
Batch Loss = 3.075337039267355
1949, epoch_train_loss=3.075337039267355
Epoch 1950
Epoch 1950 :: Batch 0/1
Batch Loss = 3.074975301398143
1950, epoch_train_loss=3.074975301398143
Epoch 1951
Epoch 1951 :: Batch 0/1
Batch Loss = 3.0746122283896127
1951, epoch_train_loss=3.0746122283896127
Epoch 1952
Epoch 1952 :: Batch 0/1
Batch Loss = 3.074247441433551
1952, epoch_train_loss=3.074247441433551
Epoch 1953
Epoch 1953 :: Batch 0/1
Batch Loss = 3.07388095043477
1953, epoch_train_loss=3.07388095043477
Epoch 1954
Epoch 1954 :: Batch 0/1
Batch Loss = 3.0735128976757355
1954, epoch_train_loss=3.0735128976757355
Epoch 1955
Epoch 1955 :: Batch 0/1
Batch Loss = 3.073143398802113
1955, epoch_train_loss=3.073143398802113
Epoch 1956
Epoch 1956 :: Batch 0/1
Batch Loss = 3.072772440456621
1956, epoch_train_loss=3.072772440456621
Epoch 1957
Epoch 1957 :: Batch 0/1
Batch Loss = 3.072400913539248
1957, epoch_train_loss=3.072400913539248
Epoch 1958
Epoch 1958 :: Batch 0/1
Batch Loss = 3.0720300878022457
1958, epoch_train_loss=3.0720300878022457
Epoch 1959
Epoch 1959 :: Batch 0/1
Batch Loss = 3.071665154372641
1959, epoch_train_loss=3.071665154372641
Epoch 1960
Epoch 1960 :: Batch 0/1
Batch Loss = 3.071320785294938
1960, epoch_train_loss=3.071320785294938
Epoch 1961
Epoch 1961 :: Batch 0/1
Batch Loss = 3.071048258975827
1961, epoch_train_loss=3.071048258975827
Epoch 1962
Epoch 1962 :: Batch 0/1
Batch Loss = 3.0710331647453346
1962, epoch_train_loss=3.0710331647453346
Epoch 1963
Epoch 1963 :: Batch 0/1
Batch Loss = 3.0718880139186653
1963, epoch_train_loss=3.0718880139186653
Epoch 1964
Epoch 1964 :: Batch 0/1
Batch Loss = 3.0761483785660726
1964, epoch_train_loss=3.0761483785660726
Epoch 1965
Epoch 1965 :: Batch 0/1
Batch Loss = 3.087398197224351
1965, epoch_train_loss=3.087398197224351
Epoch 1966
Epoch 1966 :: Batch 0/1
Batch Loss = 3.1137468778949455
1966, epoch_train_loss=3.1137468778949455
Epoch 1967
Epoch 1967 :: Batch 0/1
Batch Loss = 3.077856463675071
1967, epoch_train_loss=3.077856463675071
Epoch 1968
Epoch 1968 :: Batch 0/1
Batch Loss = 3.06941493400693
1968, epoch_train_loss=3.06941493400693
Epoch 1969
Epoch 1969 :: Batch 0/1
Batch Loss = 3.0726656972192994
1969, epoch_train_loss=3.0726656972192994
Epoch 1970
Epoch 1970 :: Batch 0/1
Batch Loss = 3.087145178444931
1970, epoch_train_loss=3.087145178444931
Epoch 1971
Epoch 1971 :: Batch 0/1
Batch Loss = 3.1191324274055554
1971, epoch_train_loss=3.1191324274055554
Epoch 1972
Epoch 1972 :: Batch 0/1
Batch Loss = 3.0695289290307697
1972, epoch_train_loss=3.0695289290307697
Epoch 1973
Epoch 1973 :: Batch 0/1
Batch Loss = 3.0888657326134434
1973, epoch_train_loss=3.0888657326134434
Epoch 1974
Epoch 1974 :: Batch 0/1
Batch Loss = 3.1493572758798147
1974, epoch_train_loss=3.1493572758798147
Epoch 1975
Epoch 1975 :: Batch 0/1
Batch Loss = 3.099628943824174
1975, epoch_train_loss=3.099628943824174
Epoch 1976
Epoch 1976 :: Batch 0/1
Batch Loss = 3.273269353849612
1976, epoch_train_loss=3.273269353849612
Epoch 1977
Epoch 1977 :: Batch 0/1
Batch Loss = 3.1964862281878137
1977, epoch_train_loss=3.1964862281878137
Epoch 1978
Epoch 1978 :: Batch 0/1
Batch Loss = 3.2664602631548916
1978, epoch_train_loss=3.2664602631548916
Epoch 1979
Epoch 1979 :: Batch 0/1
Batch Loss = 3.2553016262435035
1979, epoch_train_loss=3.2553016262435035
Epoch 1980
Epoch 1980 :: Batch 0/1
Batch Loss = 3.2173262222911165
1980, epoch_train_loss=3.2173262222911165
Epoch 1981
Epoch 1981 :: Batch 0/1
Batch Loss = 3.186291017668358
1981, epoch_train_loss=3.186291017668358
Epoch 1982
Epoch 1982 :: Batch 0/1
Batch Loss = 3.191714298797409
1982, epoch_train_loss=3.191714298797409
Epoch 1983
Epoch 1983 :: Batch 0/1
Batch Loss = 3.2106642766606606
1983, epoch_train_loss=3.2106642766606606
Epoch 1984
Epoch 1984 :: Batch 0/1
Batch Loss = 3.217694150270862
1984, epoch_train_loss=3.217694150270862
Epoch 1985
Epoch 1985 :: Batch 0/1
Batch Loss = 3.208666168633702
1985, epoch_train_loss=3.208666168633702
Epoch 1986
Epoch 1986 :: Batch 0/1
Batch Loss = 3.195086687615416
1986, epoch_train_loss=3.195086687615416
Epoch 1987
Epoch 1987 :: Batch 0/1
Batch Loss = 3.196295412195062
1987, epoch_train_loss=3.196295412195062
Epoch 1988
Epoch 1988 :: Batch 0/1
Batch Loss = 3.2026318989255005
1988, epoch_train_loss=3.2026318989255005
Epoch 1989
Epoch 1989 :: Batch 0/1
Batch Loss = 3.194402006828934
1989, epoch_train_loss=3.194402006828934
Epoch 1990
Epoch 1990 :: Batch 0/1
Batch Loss = 3.1729946605612085
1990, epoch_train_loss=3.1729946605612085
Epoch 1991
Epoch 1991 :: Batch 0/1
Batch Loss = 3.15432207862032
1991, epoch_train_loss=3.15432207862032
Epoch 1992
Epoch 1992 :: Batch 0/1
Batch Loss = 3.154830834924407
1992, epoch_train_loss=3.154830834924407
Epoch 1993
Epoch 1993 :: Batch 0/1
Batch Loss = 3.16108517227761
1993, epoch_train_loss=3.16108517227761
Epoch 1994
Epoch 1994 :: Batch 0/1
Batch Loss = 3.1524282373377686
1994, epoch_train_loss=3.1524282373377686
Epoch 1995
Epoch 1995 :: Batch 0/1
Batch Loss = 3.138786430263145
1995, epoch_train_loss=3.138786430263145
Epoch 1996
Epoch 1996 :: Batch 0/1
Batch Loss = 3.1378415165591593
1996, epoch_train_loss=3.1378415165591593
Epoch 1997
Epoch 1997 :: Batch 0/1
Batch Loss = 3.1439423630326173
1997, epoch_train_loss=3.1439423630326173
Epoch 1998
Epoch 1998 :: Batch 0/1
Batch Loss = 3.143767460171549
1998, epoch_train_loss=3.143767460171549
Epoch 1999
Epoch 1999 :: Batch 0/1
Batch Loss = 3.1421809632589626
1999, epoch_train_loss=3.1421809632589626
Epoch 2000
Epoch 2000 :: Batch 0/1
Batch Loss = 3.1445399794556215
2000, epoch_train_loss=3.1445399794556215
Epoch 2001
Epoch 2001 :: Batch 0/1
Batch Loss = 3.149292093589645
2001, epoch_train_loss=3.149292093589645
Epoch 2002
Epoch 2002 :: Batch 0/1
Batch Loss = 3.148894475739479
2002, epoch_train_loss=3.148894475739479
Epoch 2003
Epoch 2003 :: Batch 0/1
Batch Loss = 3.1428503776749714
2003, epoch_train_loss=3.1428503776749714
Epoch 2004
Epoch 2004 :: Batch 0/1
Batch Loss = 3.141001803666704
2004, epoch_train_loss=3.141001803666704
Epoch 2005
Epoch 2005 :: Batch 0/1
Batch Loss = 3.1432952239074305
2005, epoch_train_loss=3.1432952239074305
Epoch 2006
Epoch 2006 :: Batch 0/1
Batch Loss = 3.140648991388782
2006, epoch_train_loss=3.140648991388782
Epoch 2007
Epoch 2007 :: Batch 0/1
Batch Loss = 3.1362860517292357
2007, epoch_train_loss=3.1362860517292357
Epoch 2008
Epoch 2008 :: Batch 0/1
Batch Loss = 3.1352786463924325
2008, epoch_train_loss=3.1352786463924325
Epoch 2009
Epoch 2009 :: Batch 0/1
Batch Loss = 3.1357428987643465
2009, epoch_train_loss=3.1357428987643465
Epoch 2010
Epoch 2010 :: Batch 0/1
Batch Loss = 3.1343717786674476
2010, epoch_train_loss=3.1343717786674476
Epoch 2011
Epoch 2011 :: Batch 0/1
Batch Loss = 3.132521852825488
2011, epoch_train_loss=3.132521852825488
Epoch 2012
Epoch 2012 :: Batch 0/1
Batch Loss = 3.1332382084592494
2012, epoch_train_loss=3.1332382084592494
Epoch 2013
Epoch 2013 :: Batch 0/1
Batch Loss = 3.134743518285255
2013, epoch_train_loss=3.134743518285255
Epoch 2014
Epoch 2014 :: Batch 0/1
Batch Loss = 3.1343703431836247
2014, epoch_train_loss=3.1343703431836247
Epoch 2015
Epoch 2015 :: Batch 0/1
Batch Loss = 3.133742668970757
2015, epoch_train_loss=3.133742668970757
Epoch 2016
Epoch 2016 :: Batch 0/1
Batch Loss = 3.133230720712498
2016, epoch_train_loss=3.133230720712498
Epoch 2017
Epoch 2017 :: Batch 0/1
Batch Loss = 3.1337432760042545
2017, epoch_train_loss=3.1337432760042545
Epoch 2018
Epoch 2018 :: Batch 0/1
Batch Loss = 3.1338130467286818
2018, epoch_train_loss=3.1338130467286818
Epoch 2019
Epoch 2019 :: Batch 0/1
Batch Loss = 3.132768512562301
2019, epoch_train_loss=3.132768512562301
Epoch 2020
Epoch 2020 :: Batch 0/1
Batch Loss = 3.1322093100096393
2020, epoch_train_loss=3.1322093100096393
Epoch 2021
Epoch 2021 :: Batch 0/1
Batch Loss = 3.1321226195246994
2021, epoch_train_loss=3.1321226195246994
Epoch 2022
Epoch 2022 :: Batch 0/1
Batch Loss = 3.131610062910503
2022, epoch_train_loss=3.131610062910503
Epoch 2023
Epoch 2023 :: Batch 0/1
Batch Loss = 3.1306704297361283
2023, epoch_train_loss=3.1306704297361283
Epoch 2024
Epoch 2024 :: Batch 0/1
Batch Loss = 3.130071433122291
2024, epoch_train_loss=3.130071433122291
Epoch 2025
Epoch 2025 :: Batch 0/1
Batch Loss = 3.1303717729242444
2025, epoch_train_loss=3.1303717729242444
Epoch 2026
Epoch 2026 :: Batch 0/1
Batch Loss = 3.13020192182831
2026, epoch_train_loss=3.13020192182831
Epoch 2027
Epoch 2027 :: Batch 0/1
Batch Loss = 3.1296553544228676
2027, epoch_train_loss=3.1296553544228676
Epoch 2028
Epoch 2028 :: Batch 0/1
Batch Loss = 3.129478978352112
2028, epoch_train_loss=3.129478978352112
Epoch 2029
Epoch 2029 :: Batch 0/1
Batch Loss = 3.1295401467029715
2029, epoch_train_loss=3.1295401467029715
Epoch 2030
Epoch 2030 :: Batch 0/1
Batch Loss = 3.1294383342730967
2030, epoch_train_loss=3.1294383342730967
Epoch 2031
Epoch 2031 :: Batch 0/1
Batch Loss = 3.1291853713991533
2031, epoch_train_loss=3.1291853713991533
Epoch 2032
Epoch 2032 :: Batch 0/1
Batch Loss = 3.129021102148361
2032, epoch_train_loss=3.129021102148361
Epoch 2033
Epoch 2033 :: Batch 0/1
Batch Loss = 3.1290271771297733
2033, epoch_train_loss=3.1290271771297733
Epoch 2034
Epoch 2034 :: Batch 0/1
Batch Loss = 3.1287759743122927
2034, epoch_train_loss=3.1287759743122927
Epoch 2035
Epoch 2035 :: Batch 0/1
Batch Loss = 3.1284570628167865
2035, epoch_train_loss=3.1284570628167865
Epoch 2036
Epoch 2036 :: Batch 0/1
Batch Loss = 3.128194042955026
2036, epoch_train_loss=3.128194042955026
Epoch 2037
Epoch 2037 :: Batch 0/1
Batch Loss = 3.1280216565653918
2037, epoch_train_loss=3.1280216565653918
Epoch 2038
Epoch 2038 :: Batch 0/1
Batch Loss = 3.1278854897390604
2038, epoch_train_loss=3.1278854897390604
Epoch 2039
Epoch 2039 :: Batch 0/1
Batch Loss = 3.1276255813531777
2039, epoch_train_loss=3.1276255813531777
Epoch 2040
Epoch 2040 :: Batch 0/1
Batch Loss = 3.127414720077811
2040, epoch_train_loss=3.127414720077811
Epoch 2041
Epoch 2041 :: Batch 0/1
Batch Loss = 3.1273084474387085
2041, epoch_train_loss=3.1273084474387085
Epoch 2042
Epoch 2042 :: Batch 0/1
Batch Loss = 3.1271769496174113
2042, epoch_train_loss=3.1271769496174113
Epoch 2043
Epoch 2043 :: Batch 0/1
Batch Loss = 3.1270058120666486
2043, epoch_train_loss=3.1270058120666486
Epoch 2044
Epoch 2044 :: Batch 0/1
Batch Loss = 3.126857465325502
2044, epoch_train_loss=3.126857465325502
Epoch 2045
Epoch 2045 :: Batch 0/1
Batch Loss = 3.126779535760393
2045, epoch_train_loss=3.126779535760393
Epoch 2046
Epoch 2046 :: Batch 0/1
Batch Loss = 3.126677707125997
2046, epoch_train_loss=3.126677707125997
Epoch 2047
Epoch 2047 :: Batch 0/1
Batch Loss = 3.1264792325880846
2047, epoch_train_loss=3.1264792325880846
Epoch 2048
Epoch 2048 :: Batch 0/1
Batch Loss = 3.1263289086152293
2048, epoch_train_loss=3.1263289086152293
Epoch 2049
Epoch 2049 :: Batch 0/1
Batch Loss = 3.1262106109689864
2049, epoch_train_loss=3.1262106109689864
Epoch 2050
Epoch 2050 :: Batch 0/1
Batch Loss = 3.126071869817987
2050, epoch_train_loss=3.126071869817987
Epoch 2051
Epoch 2051 :: Batch 0/1
Batch Loss = 3.125903375340311
2051, epoch_train_loss=3.125903375340311
Epoch 2052
Epoch 2052 :: Batch 0/1
Batch Loss = 3.125750539800956
2052, epoch_train_loss=3.125750539800956
Epoch 2053
Epoch 2053 :: Batch 0/1
Batch Loss = 3.1256283197705836
2053, epoch_train_loss=3.1256283197705836
Epoch 2054
Epoch 2054 :: Batch 0/1
Batch Loss = 3.125488208798823
2054, epoch_train_loss=3.125488208798823
Epoch 2055
Epoch 2055 :: Batch 0/1
Batch Loss = 3.125336226672977
2055, epoch_train_loss=3.125336226672977
Epoch 2056
Epoch 2056 :: Batch 0/1
Batch Loss = 3.125212858921142
2056, epoch_train_loss=3.125212858921142
Epoch 2057
Epoch 2057 :: Batch 0/1
Batch Loss = 3.1250995447239793
2057, epoch_train_loss=3.1250995447239793
Epoch 2058
Epoch 2058 :: Batch 0/1
Batch Loss = 3.1249811426625778
2058, epoch_train_loss=3.1249811426625778
Epoch 2059
Epoch 2059 :: Batch 0/1
Batch Loss = 3.12485196405784
2059, epoch_train_loss=3.12485196405784
Epoch 2060
Epoch 2060 :: Batch 0/1
Batch Loss = 3.1247249380157163
2060, epoch_train_loss=3.1247249380157163
Epoch 2061
Epoch 2061 :: Batch 0/1
Batch Loss = 3.1246107423895295
2061, epoch_train_loss=3.1246107423895295
Epoch 2062
Epoch 2062 :: Batch 0/1
Batch Loss = 3.1244875644632746
2062, epoch_train_loss=3.1244875644632746
Epoch 2063
Epoch 2063 :: Batch 0/1
Batch Loss = 3.1243612405847614
2063, epoch_train_loss=3.1243612405847614
Epoch 2064
Epoch 2064 :: Batch 0/1
Batch Loss = 3.1242368869639394
2064, epoch_train_loss=3.1242368869639394
Epoch 2065
Epoch 2065 :: Batch 0/1
Batch Loss = 3.1241173038543377
2065, epoch_train_loss=3.1241173038543377
Epoch 2066
Epoch 2066 :: Batch 0/1
Batch Loss = 3.1239956766629837
2066, epoch_train_loss=3.1239956766629837
Epoch 2067
Epoch 2067 :: Batch 0/1
Batch Loss = 3.1238681408034865
2067, epoch_train_loss=3.1238681408034865
Epoch 2068
Epoch 2068 :: Batch 0/1
Batch Loss = 3.1237477226707324
2068, epoch_train_loss=3.1237477226707324
Epoch 2069
Epoch 2069 :: Batch 0/1
Batch Loss = 3.123634838541762
2069, epoch_train_loss=3.123634838541762
Epoch 2070
Epoch 2070 :: Batch 0/1
Batch Loss = 3.1235181991773917
2070, epoch_train_loss=3.1235181991773917
Epoch 2071
Epoch 2071 :: Batch 0/1
Batch Loss = 3.1233996580196908
2071, epoch_train_loss=3.1233996580196908
Epoch 2072
Epoch 2072 :: Batch 0/1
Batch Loss = 3.123284924627719
2072, epoch_train_loss=3.123284924627719
Epoch 2073
Epoch 2073 :: Batch 0/1
Batch Loss = 3.123173275789276
2073, epoch_train_loss=3.123173275789276
Epoch 2074
Epoch 2074 :: Batch 0/1
Batch Loss = 3.1230593269528315
2074, epoch_train_loss=3.1230593269528315
Epoch 2075
Epoch 2075 :: Batch 0/1
Batch Loss = 3.1229431907254357
2075, epoch_train_loss=3.1229431907254357
Epoch 2076
Epoch 2076 :: Batch 0/1
Batch Loss = 3.122831459593146
2076, epoch_train_loss=3.122831459593146
Epoch 2077
Epoch 2077 :: Batch 0/1
Batch Loss = 3.122719422817585
2077, epoch_train_loss=3.122719422817585
Epoch 2078
Epoch 2078 :: Batch 0/1
Batch Loss = 3.1226050669503986
2078, epoch_train_loss=3.1226050669503986
Epoch 2079
Epoch 2079 :: Batch 0/1
Batch Loss = 3.1224912946621166
2079, epoch_train_loss=3.1224912946621166
Epoch 2080
Epoch 2080 :: Batch 0/1
Batch Loss = 3.122379622932343
2080, epoch_train_loss=3.122379622932343
Epoch 2081
Epoch 2081 :: Batch 0/1
Batch Loss = 3.122269275577311
2081, epoch_train_loss=3.122269275577311
Epoch 2082
Epoch 2082 :: Batch 0/1
Batch Loss = 3.122157826942275
2082, epoch_train_loss=3.122157826942275
Epoch 2083
Epoch 2083 :: Batch 0/1
Batch Loss = 3.122047222670004
2083, epoch_train_loss=3.122047222670004
Epoch 2084
Epoch 2084 :: Batch 0/1
Batch Loss = 3.1219381475377816
2084, epoch_train_loss=3.1219381475377816
Epoch 2085
Epoch 2085 :: Batch 0/1
Batch Loss = 3.1218292820552143
2085, epoch_train_loss=3.1218292820552143
Epoch 2086
Epoch 2086 :: Batch 0/1
Batch Loss = 3.121720546650768
2086, epoch_train_loss=3.121720546650768
Epoch 2087
Epoch 2087 :: Batch 0/1
Batch Loss = 3.121612355420266
2087, epoch_train_loss=3.121612355420266
Epoch 2088
Epoch 2088 :: Batch 0/1
Batch Loss = 3.121504926532252
2088, epoch_train_loss=3.121504926532252
Epoch 2089
Epoch 2089 :: Batch 0/1
Batch Loss = 3.1213976708628515
2089, epoch_train_loss=3.1213976708628515
Epoch 2090
Epoch 2090 :: Batch 0/1
Batch Loss = 3.121289899464756
2090, epoch_train_loss=3.121289899464756
Epoch 2091
Epoch 2091 :: Batch 0/1
Batch Loss = 3.1211824851218832
2091, epoch_train_loss=3.1211824851218832
Epoch 2092
Epoch 2092 :: Batch 0/1
Batch Loss = 3.1210758099300464
2092, epoch_train_loss=3.1210758099300464
Epoch 2093
Epoch 2093 :: Batch 0/1
Batch Loss = 3.120969315562418
2093, epoch_train_loss=3.120969315562418
Epoch 2094
Epoch 2094 :: Batch 0/1
Batch Loss = 3.120862887129829
2094, epoch_train_loss=3.120862887129829
Epoch 2095
Epoch 2095 :: Batch 0/1
Batch Loss = 3.120756683165139
2095, epoch_train_loss=3.120756683165139
Epoch 2096
Epoch 2096 :: Batch 0/1
Batch Loss = 3.120651104640524
2096, epoch_train_loss=3.120651104640524
Epoch 2097
Epoch 2097 :: Batch 0/1
Batch Loss = 3.1205455547607484
2097, epoch_train_loss=3.1205455547607484
Epoch 2098
Epoch 2098 :: Batch 0/1
Batch Loss = 3.120439874903073
2098, epoch_train_loss=3.120439874903073
Epoch 2099
Epoch 2099 :: Batch 0/1
Batch Loss = 3.1203346166610864
2099, epoch_train_loss=3.1203346166610864
Epoch 2100
Epoch 2100 :: Batch 0/1
Batch Loss = 3.1202294867567493
2100, epoch_train_loss=3.1202294867567493
Epoch 2101
Epoch 2101 :: Batch 0/1
Batch Loss = 3.120124173255738
2101, epoch_train_loss=3.120124173255738
Epoch 2102
Epoch 2102 :: Batch 0/1
Batch Loss = 3.120018624982539
2102, epoch_train_loss=3.120018624982539
Epoch 2103
Epoch 2103 :: Batch 0/1
Batch Loss = 3.1199131035886682
2103, epoch_train_loss=3.1199131035886682
Epoch 2104
Epoch 2104 :: Batch 0/1
Batch Loss = 3.1198074915781593
2104, epoch_train_loss=3.1198074915781593
Epoch 2105
Epoch 2105 :: Batch 0/1
Batch Loss = 3.119701423736072
2105, epoch_train_loss=3.119701423736072
Epoch 2106
Epoch 2106 :: Batch 0/1
Batch Loss = 3.119595069829328
2106, epoch_train_loss=3.119595069829328
Epoch 2107
Epoch 2107 :: Batch 0/1
Batch Loss = 3.119488432572521
2107, epoch_train_loss=3.119488432572521
Epoch 2108
Epoch 2108 :: Batch 0/1
Batch Loss = 3.119381307392096
2108, epoch_train_loss=3.119381307392096
Epoch 2109
Epoch 2109 :: Batch 0/1
Batch Loss = 3.1192735815773047
2109, epoch_train_loss=3.1192735815773047
Epoch 2110
Epoch 2110 :: Batch 0/1
Batch Loss = 3.119165240420121
2110, epoch_train_loss=3.119165240420121
Epoch 2111
Epoch 2111 :: Batch 0/1
Batch Loss = 3.1190562203085315
2111, epoch_train_loss=3.1190562203085315
Epoch 2112
Epoch 2112 :: Batch 0/1
Batch Loss = 3.1189462569468445
2112, epoch_train_loss=3.1189462569468445
Epoch 2113
Epoch 2113 :: Batch 0/1
Batch Loss = 3.118835188328703
2113, epoch_train_loss=3.118835188328703
Epoch 2114
Epoch 2114 :: Batch 0/1
Batch Loss = 3.1187229528479956
2114, epoch_train_loss=3.1187229528479956
Epoch 2115
Epoch 2115 :: Batch 0/1
Batch Loss = 3.118609377129195
2115, epoch_train_loss=3.118609377129195
Epoch 2116
Epoch 2116 :: Batch 0/1
Batch Loss = 3.118494246248643
2116, epoch_train_loss=3.118494246248643
Epoch 2117
Epoch 2117 :: Batch 0/1
Batch Loss = 3.1183773607254723
2117, epoch_train_loss=3.1183773607254723
Epoch 2118
Epoch 2118 :: Batch 0/1
Batch Loss = 3.1182585284523734
2118, epoch_train_loss=3.1182585284523734
Epoch 2119
Epoch 2119 :: Batch 0/1
Batch Loss = 3.118137488527756
2119, epoch_train_loss=3.118137488527756
Epoch 2120
Epoch 2120 :: Batch 0/1
Batch Loss = 3.118013902902136
2120, epoch_train_loss=3.118013902902136
Epoch 2121
Epoch 2121 :: Batch 0/1
Batch Loss = 3.117887471368447
2121, epoch_train_loss=3.117887471368447
Epoch 2122
Epoch 2122 :: Batch 0/1
Batch Loss = 3.1177578727420565
2122, epoch_train_loss=3.1177578727420565
Epoch 2123
Epoch 2123 :: Batch 0/1
Batch Loss = 3.11762467316632
2123, epoch_train_loss=3.11762467316632
Epoch 2124
Epoch 2124 :: Batch 0/1
Batch Loss = 3.117487385634844
2124, epoch_train_loss=3.117487385634844
Epoch 2125
Epoch 2125 :: Batch 0/1
Batch Loss = 3.117345489853886
2125, epoch_train_loss=3.117345489853886
Epoch 2126
Epoch 2126 :: Batch 0/1
Batch Loss = 3.11719839437733
2126, epoch_train_loss=3.11719839437733
Epoch 2127
Epoch 2127 :: Batch 0/1
Batch Loss = 3.1170453763509194
2127, epoch_train_loss=3.1170453763509194
Epoch 2128
Epoch 2128 :: Batch 0/1
Batch Loss = 3.116885621216828
2128, epoch_train_loss=3.116885621216828
Epoch 2129
Epoch 2129 :: Batch 0/1
Batch Loss = 3.1167182441733305
2129, epoch_train_loss=3.1167182441733305
Epoch 2130
Epoch 2130 :: Batch 0/1
Batch Loss = 3.116542183981892
2130, epoch_train_loss=3.116542183981892
Epoch 2131
Epoch 2131 :: Batch 0/1
Batch Loss = 3.116356199575399
2131, epoch_train_loss=3.116356199575399
Epoch 2132
Epoch 2132 :: Batch 0/1
Batch Loss = 3.1161588861933547
2132, epoch_train_loss=3.1161588861933547
Epoch 2133
Epoch 2133 :: Batch 0/1
Batch Loss = 3.1159486055694687
2133, epoch_train_loss=3.1159486055694687
Epoch 2134
Epoch 2134 :: Batch 0/1
Batch Loss = 3.1157234272678433
2134, epoch_train_loss=3.1157234272678433
Epoch 2135
Epoch 2135 :: Batch 0/1
Batch Loss = 3.1154811191678946
2135, epoch_train_loss=3.1154811191678946
Epoch 2136
Epoch 2136 :: Batch 0/1
Batch Loss = 3.11521915018131
2136, epoch_train_loss=3.11521915018131
Epoch 2137
Epoch 2137 :: Batch 0/1
Batch Loss = 3.1149346152674884
2137, epoch_train_loss=3.1149346152674884
Epoch 2138
Epoch 2138 :: Batch 0/1
Batch Loss = 3.1146242063216416
2138, epoch_train_loss=3.1146242063216416
Epoch 2139
Epoch 2139 :: Batch 0/1
Batch Loss = 3.1142842498651175
2139, epoch_train_loss=3.1142842498651175
Epoch 2140
Epoch 2140 :: Batch 0/1
Batch Loss = 3.113910716820493
2140, epoch_train_loss=3.113910716820493
Epoch 2141
Epoch 2141 :: Batch 0/1
Batch Loss = 3.113499322076157
2141, epoch_train_loss=3.113499322076157
Epoch 2142
Epoch 2142 :: Batch 0/1
Batch Loss = 3.1130457644840743
2142, epoch_train_loss=3.1130457644840743
Epoch 2143
Epoch 2143 :: Batch 0/1
Batch Loss = 3.112546063161312
2143, epoch_train_loss=3.112546063161312
Epoch 2144
Epoch 2144 :: Batch 0/1
Batch Loss = 3.1119969971924153
2144, epoch_train_loss=3.1119969971924153
Epoch 2145
Epoch 2145 :: Batch 0/1
Batch Loss = 3.1113967012593133
2145, epoch_train_loss=3.1113967012593133
Epoch 2146
Epoch 2146 :: Batch 0/1
Batch Loss = 3.1107453604691266
2146, epoch_train_loss=3.1107453604691266
Epoch 2147
Epoch 2147 :: Batch 0/1
Batch Loss = 3.110045824854163
2147, epoch_train_loss=3.110045824854163
Epoch 2148
Epoch 2148 :: Batch 0/1
Batch Loss = 3.1093040855116287
2148, epoch_train_loss=3.1093040855116287
Epoch 2149
Epoch 2149 :: Batch 0/1
Batch Loss = 3.108529463356214
2149, epoch_train_loss=3.108529463356214
Epoch 2150
Epoch 2150 :: Batch 0/1
Batch Loss = 3.1077342494050026
2150, epoch_train_loss=3.1077342494050026
Epoch 2151
Epoch 2151 :: Batch 0/1
Batch Loss = 3.1069328108328396
2151, epoch_train_loss=3.1069328108328396
Epoch 2152
Epoch 2152 :: Batch 0/1
Batch Loss = 3.106140375189946
2152, epoch_train_loss=3.106140375189946
Epoch 2153
Epoch 2153 :: Batch 0/1
Batch Loss = 3.105371724602251
2153, epoch_train_loss=3.105371724602251
Epoch 2154
Epoch 2154 :: Batch 0/1
Batch Loss = 3.1046400927103477
2154, epoch_train_loss=3.1046400927103477
Epoch 2155
Epoch 2155 :: Batch 0/1
Batch Loss = 3.10395645121886
2155, epoch_train_loss=3.10395645121886
Epoch 2156
Epoch 2156 :: Batch 0/1
Batch Loss = 3.1033291089516535
2156, epoch_train_loss=3.1033291089516535
Epoch 2157
Epoch 2157 :: Batch 0/1
Batch Loss = 3.102763536751943
2157, epoch_train_loss=3.102763536751943
Epoch 2158
Epoch 2158 :: Batch 0/1
Batch Loss = 3.102262410987698
2158, epoch_train_loss=3.102262410987698
Epoch 2159
Epoch 2159 :: Batch 0/1
Batch Loss = 3.1018257529835855
2159, epoch_train_loss=3.1018257529835855
Epoch 2160
Epoch 2160 :: Batch 0/1
Batch Loss = 3.1014510710667453
2160, epoch_train_loss=3.1014510710667453
Epoch 2161
Epoch 2161 :: Batch 0/1
Batch Loss = 3.1011335539714633
2161, epoch_train_loss=3.1011335539714633
Epoch 2162
Epoch 2162 :: Batch 0/1
Batch Loss = 3.1008663945134614
2162, epoch_train_loss=3.1008663945134614
Epoch 2163
Epoch 2163 :: Batch 0/1
Batch Loss = 3.10064129550557
2163, epoch_train_loss=3.10064129550557
Epoch 2164
Epoch 2164 :: Batch 0/1
Batch Loss = 3.1004491414643764
2164, epoch_train_loss=3.1004491414643764
Epoch 2165
Epoch 2165 :: Batch 0/1
Batch Loss = 3.100280747910896
2165, epoch_train_loss=3.100280747910896
Epoch 2166
Epoch 2166 :: Batch 0/1
Batch Loss = 3.1001275470617666
2166, epoch_train_loss=3.1001275470617666
Epoch 2167
Epoch 2167 :: Batch 0/1
Batch Loss = 3.0999820992872436
2167, epoch_train_loss=3.0999820992872436
Epoch 2168
Epoch 2168 :: Batch 0/1
Batch Loss = 3.099838386982251
2168, epoch_train_loss=3.099838386982251
Epoch 2169
Epoch 2169 :: Batch 0/1
Batch Loss = 3.0996918745781885
2169, epoch_train_loss=3.0996918745781885
Epoch 2170
Epoch 2170 :: Batch 0/1
Batch Loss = 3.099539373584259
2170, epoch_train_loss=3.099539373584259
Epoch 2171
Epoch 2171 :: Batch 0/1
Batch Loss = 3.0993787988383983
2171, epoch_train_loss=3.0993787988383983
Epoch 2172
Epoch 2172 :: Batch 0/1
Batch Loss = 3.0992089040181714
2172, epoch_train_loss=3.0992089040181714
Epoch 2173
Epoch 2173 :: Batch 0/1
Batch Loss = 3.0990290609566564
2173, epoch_train_loss=3.0990290609566564
Epoch 2174
Epoch 2174 :: Batch 0/1
Batch Loss = 3.098839112968592
2174, epoch_train_loss=3.098839112968592
Epoch 2175
Epoch 2175 :: Batch 0/1
Batch Loss = 3.098639290275241
2175, epoch_train_loss=3.098639290275241
Epoch 2176
Epoch 2176 :: Batch 0/1
Batch Loss = 3.0984301468439313
2176, epoch_train_loss=3.0984301468439313
Epoch 2177
Epoch 2177 :: Batch 0/1
Batch Loss = 3.098212484403691
2177, epoch_train_loss=3.098212484403691
Epoch 2178
Epoch 2178 :: Batch 0/1
Batch Loss = 3.0979872422866053
2178, epoch_train_loss=3.0979872422866053
Epoch 2179
Epoch 2179 :: Batch 0/1
Batch Loss = 3.0977553519967604
2179, epoch_train_loss=3.0977553519967604
Epoch 2180
Epoch 2180 :: Batch 0/1
Batch Loss = 3.097517576339294
2180, epoch_train_loss=3.097517576339294
Epoch 2181
Epoch 2181 :: Batch 0/1
Batch Loss = 3.097274356081905
2181, epoch_train_loss=3.097274356081905
Epoch 2182
Epoch 2182 :: Batch 0/1
Batch Loss = 3.09702568595755
2182, epoch_train_loss=3.09702568595755
Epoch 2183
Epoch 2183 :: Batch 0/1
Batch Loss = 3.0967710274926694
2183, epoch_train_loss=3.0967710274926694
Epoch 2184
Epoch 2184 :: Batch 0/1
Batch Loss = 3.0965092489202437
2184, epoch_train_loss=3.0965092489202437
Epoch 2185
Epoch 2185 :: Batch 0/1
Batch Loss = 3.0962385677144066
2185, epoch_train_loss=3.0962385677144066
Epoch 2186
Epoch 2186 :: Batch 0/1
Batch Loss = 3.095956462557106
2186, epoch_train_loss=3.095956462557106
Epoch 2187
Epoch 2187 :: Batch 0/1
Batch Loss = 3.0956595213470885
2187, epoch_train_loss=3.0956595213470885
Epoch 2188
Epoch 2188 :: Batch 0/1
Batch Loss = 3.095343193749286
2188, epoch_train_loss=3.095343193749286
Epoch 2189
Epoch 2189 :: Batch 0/1
Batch Loss = 3.0950014185988746
2189, epoch_train_loss=3.0950014185988746
Epoch 2190
Epoch 2190 :: Batch 0/1
Batch Loss = 3.0946260934303607
2190, epoch_train_loss=3.0946260934303607
Epoch 2191
Epoch 2191 :: Batch 0/1
Batch Loss = 3.0942063452502606
2191, epoch_train_loss=3.0942063452502606
Epoch 2192
Epoch 2192 :: Batch 0/1
Batch Loss = 3.0937275612993687
2192, epoch_train_loss=3.0937275612993687
Epoch 2193
Epoch 2193 :: Batch 0/1
Batch Loss = 3.0931701746314406
2193, epoch_train_loss=3.0931701746314406
Epoch 2194
Epoch 2194 :: Batch 0/1
Batch Loss = 3.092508327943934
2194, epoch_train_loss=3.092508327943934
Epoch 2195
Epoch 2195 :: Batch 0/1
Batch Loss = 3.091708866463782
2195, epoch_train_loss=3.091708866463782
Epoch 2196
Epoch 2196 :: Batch 0/1
Batch Loss = 3.0907317653290716
2196, epoch_train_loss=3.0907317653290716
Epoch 2197
Epoch 2197 :: Batch 0/1
Batch Loss = 3.0895340220823124
2197, epoch_train_loss=3.0895340220823124
Epoch 2198
Epoch 2198 :: Batch 0/1
Batch Loss = 3.0880795410561888
2198, epoch_train_loss=3.0880795410561888
Epoch 2199
Epoch 2199 :: Batch 0/1
Batch Loss = 3.0863558830761586
2199, epoch_train_loss=3.0863558830761586
Epoch 2200
Epoch 2200 :: Batch 0/1
Batch Loss = 3.084393777886724
2200, epoch_train_loss=3.084393777886724
Epoch 2201
Epoch 2201 :: Batch 0/1
Batch Loss = 3.0822791444938846
2201, epoch_train_loss=3.0822791444938846
Epoch 2202
Epoch 2202 :: Batch 0/1
Batch Loss = 3.080145464261938
2202, epoch_train_loss=3.080145464261938
Epoch 2203
Epoch 2203 :: Batch 0/1
Batch Loss = 3.0781436204258887
2203, epoch_train_loss=3.0781436204258887
Epoch 2204
Epoch 2204 :: Batch 0/1
Batch Loss = 3.0763999003778837
2204, epoch_train_loss=3.0763999003778837
Epoch 2205
Epoch 2205 :: Batch 0/1
Batch Loss = 3.0749758184512346
2205, epoch_train_loss=3.0749758184512346
Epoch 2206
Epoch 2206 :: Batch 0/1
Batch Loss = 3.073843554831985
2206, epoch_train_loss=3.073843554831985
Epoch 2207
Epoch 2207 :: Batch 0/1
Batch Loss = 3.072887237851339
2207, epoch_train_loss=3.072887237851339
Epoch 2208
Epoch 2208 :: Batch 0/1
Batch Loss = 3.0719321203927796
2208, epoch_train_loss=3.0719321203927796
Epoch 2209
Epoch 2209 :: Batch 0/1
Batch Loss = 3.0707969462320794
2209, epoch_train_loss=3.0707969462320794
Epoch 2210
Epoch 2210 :: Batch 0/1
Batch Loss = 3.069351079366571
2210, epoch_train_loss=3.069351079366571
Epoch 2211
Epoch 2211 :: Batch 0/1
Batch Loss = 3.067559516812744
2211, epoch_train_loss=3.067559516812744
Epoch 2212
Epoch 2212 :: Batch 0/1
Batch Loss = 3.0654946750055743
2212, epoch_train_loss=3.0654946750055743
Epoch 2213
Epoch 2213 :: Batch 0/1
Batch Loss = 3.0633373270904563
2213, epoch_train_loss=3.0633373270904563
Epoch 2214
Epoch 2214 :: Batch 0/1
Batch Loss = 3.0613647897928313
2214, epoch_train_loss=3.0613647897928313
Epoch 2215
Epoch 2215 :: Batch 0/1
Batch Loss = 3.0607855038962914
2215, epoch_train_loss=3.0607855038962914
Epoch 2216
Epoch 2216 :: Batch 0/1
Batch Loss = 3.0757013428368136
2216, epoch_train_loss=3.0757013428368136
Epoch 2217
Epoch 2217 :: Batch 0/1
Batch Loss = 3.200267461054871
2217, epoch_train_loss=3.200267461054871
Epoch 2218
Epoch 2218 :: Batch 0/1
Batch Loss = 3.3077906396539056
2218, epoch_train_loss=3.3077906396539056
Epoch 2219
Epoch 2219 :: Batch 0/1
Batch Loss = 3.3736696143852276
2219, epoch_train_loss=3.3736696143852276
Epoch 2220
Epoch 2220 :: Batch 0/1
Batch Loss = 3.2373806025901324
2220, epoch_train_loss=3.2373806025901324
Epoch 2221
Epoch 2221 :: Batch 0/1
Batch Loss = 3.357675969248286
2221, epoch_train_loss=3.357675969248286
Epoch 2222
Epoch 2222 :: Batch 0/1
Batch Loss = 3.444666297214622
2222, epoch_train_loss=3.444666297214622
Epoch 2223
Epoch 2223 :: Batch 0/1
Batch Loss = 3.198119294593726
2223, epoch_train_loss=3.198119294593726
Epoch 2224
Epoch 2224 :: Batch 0/1
Batch Loss = 3.3887431790195457
2224, epoch_train_loss=3.3887431790195457
Epoch 2225
Epoch 2225 :: Batch 0/1
Batch Loss = 3.356520670653451
2225, epoch_train_loss=3.356520670653451
Epoch 2226
Epoch 2226 :: Batch 0/1
Batch Loss = 3.290759528895685
2226, epoch_train_loss=3.290759528895685
Epoch 2227
Epoch 2227 :: Batch 0/1
Batch Loss = 3.2832679296276037
2227, epoch_train_loss=3.2832679296276037
Epoch 2228
Epoch 2228 :: Batch 0/1
Batch Loss = 3.210779655936573
2228, epoch_train_loss=3.210779655936573
Epoch 2229
Epoch 2229 :: Batch 0/1
Batch Loss = 3.1731426859539855
2229, epoch_train_loss=3.1731426859539855
Epoch 2230
Epoch 2230 :: Batch 0/1
Batch Loss = 3.3127371241581502
2230, epoch_train_loss=3.3127371241581502
Epoch 2231
Epoch 2231 :: Batch 0/1
Batch Loss = 3.141207055374597
2231, epoch_train_loss=3.141207055374597
Epoch 2232
Epoch 2232 :: Batch 0/1
Batch Loss = 3.2802931639472694
2232, epoch_train_loss=3.2802931639472694
Epoch 2233
Epoch 2233 :: Batch 0/1
Batch Loss = 3.1955521272370335
2233, epoch_train_loss=3.1955521272370335
Epoch 2234
Epoch 2234 :: Batch 0/1
Batch Loss = 3.223795938947367
2234, epoch_train_loss=3.223795938947367
Epoch 2235
Epoch 2235 :: Batch 0/1
Batch Loss = 3.202364419522577
2235, epoch_train_loss=3.202364419522577
Epoch 2236
Epoch 2236 :: Batch 0/1
Batch Loss = 3.1450210069629487
2236, epoch_train_loss=3.1450210069629487
Epoch 2237
Epoch 2237 :: Batch 0/1
Batch Loss = 3.1978930204149285
2237, epoch_train_loss=3.1978930204149285
Epoch 2238
Epoch 2238 :: Batch 0/1
Batch Loss = 3.189133077829482
2238, epoch_train_loss=3.189133077829482
Epoch 2239
Epoch 2239 :: Batch 0/1
Batch Loss = 3.1543695814808608
2239, epoch_train_loss=3.1543695814808608
Epoch 2240
Epoch 2240 :: Batch 0/1
Batch Loss = 3.1580363119555526
2240, epoch_train_loss=3.1580363119555526
Epoch 2241
Epoch 2241 :: Batch 0/1
Batch Loss = 3.149404218438592
2241, epoch_train_loss=3.149404218438592
Epoch 2242
Epoch 2242 :: Batch 0/1
Batch Loss = 3.148722863058253
2242, epoch_train_loss=3.148722863058253
Epoch 2243
Epoch 2243 :: Batch 0/1
Batch Loss = 3.1576959652256407
2243, epoch_train_loss=3.1576959652256407
Epoch 2244
Epoch 2244 :: Batch 0/1
Batch Loss = 3.1342439037869694
2244, epoch_train_loss=3.1342439037869694
Epoch 2245
Epoch 2245 :: Batch 0/1
Batch Loss = 3.1513871360806953
2245, epoch_train_loss=3.1513871360806953
Epoch 2246
Epoch 2246 :: Batch 0/1
Batch Loss = 3.1192796749390435
2246, epoch_train_loss=3.1192796749390435
Epoch 2247
Epoch 2247 :: Batch 0/1
Batch Loss = 3.1335967643867804
2247, epoch_train_loss=3.1335967643867804
Epoch 2248
Epoch 2248 :: Batch 0/1
Batch Loss = 3.1238422165935438
2248, epoch_train_loss=3.1238422165935438
Epoch 2249
Epoch 2249 :: Batch 0/1
Batch Loss = 3.1202639462504305
2249, epoch_train_loss=3.1202639462504305
Epoch 2250
Epoch 2250 :: Batch 0/1
Batch Loss = 3.1316557035872643
2250, epoch_train_loss=3.1316557035872643
Epoch 2251
Epoch 2251 :: Batch 0/1
Batch Loss = 3.104127965747286
2251, epoch_train_loss=3.104127965747286
Epoch 2252
Epoch 2252 :: Batch 0/1
Batch Loss = 3.1105520509547215
2252, epoch_train_loss=3.1105520509547215
Epoch 2253
Epoch 2253 :: Batch 0/1
Batch Loss = 3.099392836247803
2253, epoch_train_loss=3.099392836247803
Epoch 2254
Epoch 2254 :: Batch 0/1
Batch Loss = 3.1055319754130113
2254, epoch_train_loss=3.1055319754130113
Epoch 2255
Epoch 2255 :: Batch 0/1
Batch Loss = 3.0937490078349543
2255, epoch_train_loss=3.0937490078349543
Epoch 2256
Epoch 2256 :: Batch 0/1
Batch Loss = 3.0973531943439503
2256, epoch_train_loss=3.0973531943439503
Epoch 2257
Epoch 2257 :: Batch 0/1
Batch Loss = 3.0930332163021155
2257, epoch_train_loss=3.0930332163021155
Epoch 2258
Epoch 2258 :: Batch 0/1
Batch Loss = 3.0843645773346684
2258, epoch_train_loss=3.0843645773346684
Epoch 2259
Epoch 2259 :: Batch 0/1
Batch Loss = 3.0867557930663962
2259, epoch_train_loss=3.0867557930663962
Epoch 2260
Epoch 2260 :: Batch 0/1
Batch Loss = 3.0832935111941233
2260, epoch_train_loss=3.0832935111941233
Epoch 2261
Epoch 2261 :: Batch 0/1
Batch Loss = 3.0862123912509087
2261, epoch_train_loss=3.0862123912509087
Epoch 2262
Epoch 2262 :: Batch 0/1
Batch Loss = 3.0817736464499808
2262, epoch_train_loss=3.0817736464499808
Epoch 2263
Epoch 2263 :: Batch 0/1
Batch Loss = 3.0827436854016628
2263, epoch_train_loss=3.0827436854016628
Epoch 2264
Epoch 2264 :: Batch 0/1
Batch Loss = 3.08222354012761
2264, epoch_train_loss=3.08222354012761
Epoch 2265
Epoch 2265 :: Batch 0/1
Batch Loss = 3.0758931268509535
2265, epoch_train_loss=3.0758931268509535
Epoch 2266
Epoch 2266 :: Batch 0/1
Batch Loss = 3.0782862267797193
2266, epoch_train_loss=3.0782862267797193
Epoch 2267
Epoch 2267 :: Batch 0/1
Batch Loss = 3.0771591614514264
2267, epoch_train_loss=3.0771591614514264
Epoch 2268
Epoch 2268 :: Batch 0/1
Batch Loss = 3.0737520088137917
2268, epoch_train_loss=3.0737520088137917
Epoch 2269
Epoch 2269 :: Batch 0/1
Batch Loss = 3.0754777399090294
2269, epoch_train_loss=3.0754777399090294
Epoch 2270
Epoch 2270 :: Batch 0/1
Batch Loss = 3.0747990876865816
2270, epoch_train_loss=3.0747990876865816
Epoch 2271
Epoch 2271 :: Batch 0/1
Batch Loss = 3.071318607102003
2271, epoch_train_loss=3.071318607102003
Epoch 2272
Epoch 2272 :: Batch 0/1
Batch Loss = 3.0704981970027463
2272, epoch_train_loss=3.0704981970027463
Epoch 2273
Epoch 2273 :: Batch 0/1
Batch Loss = 3.0709374957963806
2273, epoch_train_loss=3.0709374957963806
Epoch 2274
Epoch 2274 :: Batch 0/1
Batch Loss = 3.0701879888547614
2274, epoch_train_loss=3.0701879888547614
Epoch 2275
Epoch 2275 :: Batch 0/1
Batch Loss = 3.0683468531982636
2275, epoch_train_loss=3.0683468531982636
Epoch 2276
Epoch 2276 :: Batch 0/1
Batch Loss = 3.0673337500889444
2276, epoch_train_loss=3.0673337500889444
Epoch 2277
Epoch 2277 :: Batch 0/1
Batch Loss = 3.067329605728954
2277, epoch_train_loss=3.067329605728954
Epoch 2278
Epoch 2278 :: Batch 0/1
Batch Loss = 3.067004071560954
2278, epoch_train_loss=3.067004071560954
Epoch 2279
Epoch 2279 :: Batch 0/1
Batch Loss = 3.0657568614842825
2279, epoch_train_loss=3.0657568614842825
Epoch 2280
Epoch 2280 :: Batch 0/1
Batch Loss = 3.063781423045002
2280, epoch_train_loss=3.063781423045002
Epoch 2281
Epoch 2281 :: Batch 0/1
Batch Loss = 3.0623612684705868
2281, epoch_train_loss=3.0623612684705868
Epoch 2282
Epoch 2282 :: Batch 0/1
Batch Loss = 3.0610487826930086
2282, epoch_train_loss=3.0610487826930086
Epoch 2283
Epoch 2283 :: Batch 0/1
Batch Loss = 3.059953008020878
2283, epoch_train_loss=3.059953008020878
Epoch 2284
Epoch 2284 :: Batch 0/1
Batch Loss = 3.0598483928193883
2284, epoch_train_loss=3.0598483928193883
Epoch 2285
Epoch 2285 :: Batch 0/1
Batch Loss = 3.0638653073212847
2285, epoch_train_loss=3.0638653073212847
Epoch 2286
Epoch 2286 :: Batch 0/1
Batch Loss = 3.078205907919191
2286, epoch_train_loss=3.078205907919191
Epoch 2287
Epoch 2287 :: Batch 0/1
Batch Loss = 3.119108871784223
2287, epoch_train_loss=3.119108871784223
Epoch 2288
Epoch 2288 :: Batch 0/1
Batch Loss = 3.0557302408129376
2288, epoch_train_loss=3.0557302408129376
Epoch 2289
Epoch 2289 :: Batch 0/1
Batch Loss = 3.1074152454960164
2289, epoch_train_loss=3.1074152454960164
Epoch 2290
Epoch 2290 :: Batch 0/1
Batch Loss = 3.1658816723195624
2290, epoch_train_loss=3.1658816723195624
Epoch 2291
Epoch 2291 :: Batch 0/1
Batch Loss = 3.1637075192390123
2291, epoch_train_loss=3.1637075192390123
Epoch 2292
Epoch 2292 :: Batch 0/1
Batch Loss = 3.072656174799363
2292, epoch_train_loss=3.072656174799363
Epoch 2293
Epoch 2293 :: Batch 0/1
Batch Loss = 3.081743136209352
2293, epoch_train_loss=3.081743136209352
Epoch 2294
Epoch 2294 :: Batch 0/1
Batch Loss = 3.1479526760645236
2294, epoch_train_loss=3.1479526760645236
Epoch 2295
Epoch 2295 :: Batch 0/1
Batch Loss = 3.1416623212639894
2295, epoch_train_loss=3.1416623212639894
Epoch 2296
Epoch 2296 :: Batch 0/1
Batch Loss = 3.079906532075567
2296, epoch_train_loss=3.079906532075567
Epoch 2297
Epoch 2297 :: Batch 0/1
Batch Loss = 3.0770054518754724
2297, epoch_train_loss=3.0770054518754724
Epoch 2298
Epoch 2298 :: Batch 0/1
Batch Loss = 3.1203064370576366
2298, epoch_train_loss=3.1203064370576366
Epoch 2299
Epoch 2299 :: Batch 0/1
Batch Loss = 3.0929661360260954
2299, epoch_train_loss=3.0929661360260954
Epoch 2300
Epoch 2300 :: Batch 0/1
Batch Loss = 3.165020222639849
2300, epoch_train_loss=3.165020222639849
Epoch 2301
Epoch 2301 :: Batch 0/1
Batch Loss = 3.066299841133985
2301, epoch_train_loss=3.066299841133985
Epoch 2302
Epoch 2302 :: Batch 0/1
Batch Loss = 3.1004217610778246
2302, epoch_train_loss=3.1004217610778246
Epoch 2303
Epoch 2303 :: Batch 0/1
Batch Loss = 3.070294867289288
2303, epoch_train_loss=3.070294867289288
Epoch 2304
Epoch 2304 :: Batch 0/1
Batch Loss = 3.0665158932993943
2304, epoch_train_loss=3.0665158932993943
Epoch 2305
Epoch 2305 :: Batch 0/1
Batch Loss = 3.0886401540642634
2305, epoch_train_loss=3.0886401540642634
Epoch 2306
Epoch 2306 :: Batch 0/1
Batch Loss = 3.0611560953407784
2306, epoch_train_loss=3.0611560953407784
Epoch 2307
Epoch 2307 :: Batch 0/1
Batch Loss = 3.075198800656904
2307, epoch_train_loss=3.075198800656904
Epoch 2308
Epoch 2308 :: Batch 0/1
Batch Loss = 3.1005758443226044
2308, epoch_train_loss=3.1005758443226044
Epoch 2309
Epoch 2309 :: Batch 0/1
Batch Loss = 3.058520795317607
2309, epoch_train_loss=3.058520795317607
Epoch 2310
Epoch 2310 :: Batch 0/1
Batch Loss = 3.1188212289211785
2310, epoch_train_loss=3.1188212289211785
Epoch 2311
Epoch 2311 :: Batch 0/1
Batch Loss = 3.119478938170595
2311, epoch_train_loss=3.119478938170595
Epoch 2312
Epoch 2312 :: Batch 0/1
Batch Loss = 3.092451539212258
2312, epoch_train_loss=3.092451539212258
Epoch 2313
Epoch 2313 :: Batch 0/1
Batch Loss = 3.193610950375515
2313, epoch_train_loss=3.193610950375515
Epoch 2314
Epoch 2314 :: Batch 0/1
Batch Loss = 3.074122489192108
2314, epoch_train_loss=3.074122489192108
Epoch 2315
Epoch 2315 :: Batch 0/1
Batch Loss = 3.091041479655129
2315, epoch_train_loss=3.091041479655129
Epoch 2316
Epoch 2316 :: Batch 0/1
Batch Loss = 3.1092644690903124
2316, epoch_train_loss=3.1092644690903124
Epoch 2317
Epoch 2317 :: Batch 0/1
Batch Loss = 3.0701197487649723
2317, epoch_train_loss=3.0701197487649723
Epoch 2318
Epoch 2318 :: Batch 0/1
Batch Loss = 3.0609704973782605
2318, epoch_train_loss=3.0609704973782605
Epoch 2319
Epoch 2319 :: Batch 0/1
Batch Loss = 3.104727954916574
2319, epoch_train_loss=3.104727954916574
Epoch 2320
Epoch 2320 :: Batch 0/1
Batch Loss = 3.097493093645643
2320, epoch_train_loss=3.097493093645643
Epoch 2321
Epoch 2321 :: Batch 0/1
Batch Loss = 3.071963052813831
2321, epoch_train_loss=3.071963052813831
Epoch 2322
Epoch 2322 :: Batch 0/1
Batch Loss = 3.1600979721211018
2322, epoch_train_loss=3.1600979721211018
Epoch 2323
Epoch 2323 :: Batch 0/1
Batch Loss = 3.0649962634026022
2323, epoch_train_loss=3.0649962634026022
Epoch 2324
Epoch 2324 :: Batch 0/1
Batch Loss = 3.0817890063329885
2324, epoch_train_loss=3.0817890063329885
Epoch 2325
Epoch 2325 :: Batch 0/1
Batch Loss = 3.103971947202364
2325, epoch_train_loss=3.103971947202364
Epoch 2326
Epoch 2326 :: Batch 0/1
Batch Loss = 3.061198960584682
2326, epoch_train_loss=3.061198960584682
Epoch 2327
Epoch 2327 :: Batch 0/1
Batch Loss = 3.059378841722125
2327, epoch_train_loss=3.059378841722125
Epoch 2328
Epoch 2328 :: Batch 0/1
Batch Loss = 3.092138979282939
2328, epoch_train_loss=3.092138979282939
Epoch 2329
Epoch 2329 :: Batch 0/1
Batch Loss = 3.078938651759787
2329, epoch_train_loss=3.078938651759787
Epoch 2330
Epoch 2330 :: Batch 0/1
Batch Loss = 3.0517401812646314
2330, epoch_train_loss=3.0517401812646314
Epoch 2331
Epoch 2331 :: Batch 0/1
Batch Loss = 3.085688374547696
2331, epoch_train_loss=3.085688374547696
Epoch 2332
Epoch 2332 :: Batch 0/1
Batch Loss = 3.0967209663326862
2332, epoch_train_loss=3.0967209663326862
Epoch 2333
Epoch 2333 :: Batch 0/1
Batch Loss = 3.0504333350464488
2333, epoch_train_loss=3.0504333350464488
Epoch 2334
Epoch 2334 :: Batch 0/1
Batch Loss = 3.1098604898685567
2334, epoch_train_loss=3.1098604898685567
Epoch 2335
Epoch 2335 :: Batch 0/1
Batch Loss = 3.099338041322951
2335, epoch_train_loss=3.099338041322951
Epoch 2336
Epoch 2336 :: Batch 0/1
Batch Loss = 3.0590116883751786
2336, epoch_train_loss=3.0590116883751786
Epoch 2337
Epoch 2337 :: Batch 0/1
Batch Loss = 3.1682944176186068
2337, epoch_train_loss=3.1682944176186068
Epoch 2338
Epoch 2338 :: Batch 0/1
Batch Loss = 3.11759440000256
2338, epoch_train_loss=3.11759440000256
Epoch 2339
Epoch 2339 :: Batch 0/1
Batch Loss = 3.126898787096084
2339, epoch_train_loss=3.126898787096084
Epoch 2340
Epoch 2340 :: Batch 0/1
Batch Loss = 3.0822415561135945
2340, epoch_train_loss=3.0822415561135945
Epoch 2341
Epoch 2341 :: Batch 0/1
Batch Loss = 3.054954781190276
2341, epoch_train_loss=3.054954781190276
Epoch 2342
Epoch 2342 :: Batch 0/1
Batch Loss = 3.1026555334445027
2342, epoch_train_loss=3.1026555334445027
Epoch 2343
Epoch 2343 :: Batch 0/1
Batch Loss = 3.051459101494579
2343, epoch_train_loss=3.051459101494579
Epoch 2344
Epoch 2344 :: Batch 0/1
Batch Loss = 3.1196358989764628
2344, epoch_train_loss=3.1196358989764628
Epoch 2345
Epoch 2345 :: Batch 0/1
Batch Loss = 3.084649832902726
2345, epoch_train_loss=3.084649832902726
Epoch 2346
Epoch 2346 :: Batch 0/1
Batch Loss = 3.0651112211999765
2346, epoch_train_loss=3.0651112211999765
Epoch 2347
Epoch 2347 :: Batch 0/1
Batch Loss = 3.1428237071263925
2347, epoch_train_loss=3.1428237071263925
Epoch 2348
Epoch 2348 :: Batch 0/1
Batch Loss = 3.0599360684378887
2348, epoch_train_loss=3.0599360684378887
Epoch 2349
Epoch 2349 :: Batch 0/1
Batch Loss = 3.0699938295479847
2349, epoch_train_loss=3.0699938295479847
Epoch 2350
Epoch 2350 :: Batch 0/1
Batch Loss = 3.1102981847222626
2350, epoch_train_loss=3.1102981847222626
Epoch 2351
Epoch 2351 :: Batch 0/1
Batch Loss = 3.0534959433021602
2351, epoch_train_loss=3.0534959433021602
Epoch 2352
Epoch 2352 :: Batch 0/1
Batch Loss = 3.0616703727258177
2352, epoch_train_loss=3.0616703727258177
Epoch 2353
Epoch 2353 :: Batch 0/1
Batch Loss = 3.094158894885753
2353, epoch_train_loss=3.094158894885753
Epoch 2354
Epoch 2354 :: Batch 0/1
Batch Loss = 3.0588651812569414
2354, epoch_train_loss=3.0588651812569414
Epoch 2355
Epoch 2355 :: Batch 0/1
Batch Loss = 3.0503127091890856
2355, epoch_train_loss=3.0503127091890856
Epoch 2356
Epoch 2356 :: Batch 0/1
Batch Loss = 3.080414345752212
2356, epoch_train_loss=3.080414345752212
Epoch 2357
Epoch 2357 :: Batch 0/1
Batch Loss = 3.070621426207533
2357, epoch_train_loss=3.070621426207533
Epoch 2358
Epoch 2358 :: Batch 0/1
Batch Loss = 3.0460253768508947
2358, epoch_train_loss=3.0460253768508947
Epoch 2359
Epoch 2359 :: Batch 0/1
Batch Loss = 3.0672757542820857
2359, epoch_train_loss=3.0672757542820857
Epoch 2360
Epoch 2360 :: Batch 0/1
Batch Loss = 3.07893750252017
2360, epoch_train_loss=3.07893750252017
Epoch 2361
Epoch 2361 :: Batch 0/1
Batch Loss = 3.046286997886481
2361, epoch_train_loss=3.046286997886481
Epoch 2362
Epoch 2362 :: Batch 0/1
Batch Loss = 3.061336328215837
2362, epoch_train_loss=3.061336328215837
Epoch 2363
Epoch 2363 :: Batch 0/1
Batch Loss = 3.081430499002815
2363, epoch_train_loss=3.081430499002815
Epoch 2364
Epoch 2364 :: Batch 0/1
Batch Loss = 3.0463075229469396
2364, epoch_train_loss=3.0463075229469396
Epoch 2365
Epoch 2365 :: Batch 0/1
Batch Loss = 3.0615150818840964
2365, epoch_train_loss=3.0615150818840964
Epoch 2366
Epoch 2366 :: Batch 0/1
Batch Loss = 3.0829866271419224
2366, epoch_train_loss=3.0829866271419224
Epoch 2367
Epoch 2367 :: Batch 0/1
Batch Loss = 3.045304361051634
2367, epoch_train_loss=3.045304361051634
Epoch 2368
Epoch 2368 :: Batch 0/1
Batch Loss = 3.066348667917083
2368, epoch_train_loss=3.066348667917083
Epoch 2369
Epoch 2369 :: Batch 0/1
Batch Loss = 3.085013969786914
2369, epoch_train_loss=3.085013969786914
Epoch 2370
Epoch 2370 :: Batch 0/1
Batch Loss = 3.0439171437837897
2370, epoch_train_loss=3.0439171437837897
Epoch 2371
Epoch 2371 :: Batch 0/1
Batch Loss = 3.0740486253099886
2371, epoch_train_loss=3.0740486253099886
Epoch 2372
Epoch 2372 :: Batch 0/1
Batch Loss = 3.0846395822151913
2372, epoch_train_loss=3.0846395822151913
Epoch 2373
Epoch 2373 :: Batch 0/1
Batch Loss = 3.043097399050613
2373, epoch_train_loss=3.043097399050613
Epoch 2374
Epoch 2374 :: Batch 0/1
Batch Loss = 3.0800783884549365
2374, epoch_train_loss=3.0800783884549365
Epoch 2375
Epoch 2375 :: Batch 0/1
Batch Loss = 3.081775379118305
2375, epoch_train_loss=3.081775379118305
Epoch 2376
Epoch 2376 :: Batch 0/1
Batch Loss = 3.0426235240425297
2376, epoch_train_loss=3.0426235240425297
Epoch 2377
Epoch 2377 :: Batch 0/1
Batch Loss = 3.081281555976581
2377, epoch_train_loss=3.081281555976581
Epoch 2378
Epoch 2378 :: Batch 0/1
Batch Loss = 3.078975158538922
2378, epoch_train_loss=3.078975158538922
Epoch 2379
Epoch 2379 :: Batch 0/1
Batch Loss = 3.0421731588037004
2379, epoch_train_loss=3.0421731588037004
Epoch 2380
Epoch 2380 :: Batch 0/1
Batch Loss = 3.079779208357378
2380, epoch_train_loss=3.079779208357378
Epoch 2381
Epoch 2381 :: Batch 0/1
Batch Loss = 3.0773824543773496
2381, epoch_train_loss=3.0773824543773496
Epoch 2382
Epoch 2382 :: Batch 0/1
Batch Loss = 3.041562485114464
2382, epoch_train_loss=3.041562485114464
Epoch 2383
Epoch 2383 :: Batch 0/1
Batch Loss = 3.0760047801513095
2383, epoch_train_loss=3.0760047801513095
Epoch 2384
Epoch 2384 :: Batch 0/1
Batch Loss = 3.0771141187640576
2384, epoch_train_loss=3.0771141187640576
Epoch 2385
Epoch 2385 :: Batch 0/1
Batch Loss = 3.040999962114098
2385, epoch_train_loss=3.040999962114098
Epoch 2386
Epoch 2386 :: Batch 0/1
Batch Loss = 3.0711409725630996
2386, epoch_train_loss=3.0711409725630996
Epoch 2387
Epoch 2387 :: Batch 0/1
Batch Loss = 3.0779138286613197
2387, epoch_train_loss=3.0779138286613197
Epoch 2388
Epoch 2388 :: Batch 0/1
Batch Loss = 3.0406422521163345
2388, epoch_train_loss=3.0406422521163345
Epoch 2389
Epoch 2389 :: Batch 0/1
Batch Loss = 3.0660529776289818
2389, epoch_train_loss=3.0660529776289818
Epoch 2390
Epoch 2390 :: Batch 0/1
Batch Loss = 3.0789581579405207
2390, epoch_train_loss=3.0789581579405207
Epoch 2391
Epoch 2391 :: Batch 0/1
Batch Loss = 3.0405335050189866
2391, epoch_train_loss=3.0405335050189866
Epoch 2392
Epoch 2392 :: Batch 0/1
Batch Loss = 3.06136967889287
2392, epoch_train_loss=3.06136967889287
Epoch 2393
Epoch 2393 :: Batch 0/1
Batch Loss = 3.0795871151385477
2393, epoch_train_loss=3.0795871151385477
Epoch 2394
Epoch 2394 :: Batch 0/1
Batch Loss = 3.040865849809927
2394, epoch_train_loss=3.040865849809927
Epoch 2395
Epoch 2395 :: Batch 0/1
Batch Loss = 3.0555257330628733
2395, epoch_train_loss=3.0555257330628733
Epoch 2396
Epoch 2396 :: Batch 0/1
Batch Loss = 3.078886766054583
2396, epoch_train_loss=3.078886766054583
Epoch 2397
Epoch 2397 :: Batch 0/1
Batch Loss = 3.042562850832384
2397, epoch_train_loss=3.042562850832384
Epoch 2398
Epoch 2398 :: Batch 0/1
Batch Loss = 3.0472264063231975
2398, epoch_train_loss=3.0472264063231975
Epoch 2399
Epoch 2399 :: Batch 0/1
Batch Loss = 3.073536263571458
2399, epoch_train_loss=3.073536263571458
Epoch 2400
Epoch 2400 :: Batch 0/1
Batch Loss = 3.048979854098616
2400, epoch_train_loss=3.048979854098616
Epoch 2401
Epoch 2401 :: Batch 0/1
Batch Loss = 3.0386549186955705
2401, epoch_train_loss=3.0386549186955705
Epoch 2402
Epoch 2402 :: Batch 0/1
Batch Loss = 3.054753374271787
2402, epoch_train_loss=3.054753374271787
Epoch 2403
Epoch 2403 :: Batch 0/1
Batch Loss = 3.0617093914807003
2403, epoch_train_loss=3.0617093914807003
Epoch 2404
Epoch 2404 :: Batch 0/1
Batch Loss = 3.0437604838641974
2404, epoch_train_loss=3.0437604838641974
Epoch 2405
Epoch 2405 :: Batch 0/1
Batch Loss = 3.0376135068231447
2405, epoch_train_loss=3.0376135068231447
Epoch 2406
Epoch 2406 :: Batch 0/1
Batch Loss = 3.0470928479355193
2406, epoch_train_loss=3.0470928479355193
Epoch 2407
Epoch 2407 :: Batch 0/1
Batch Loss = 3.055709837433187
2407, epoch_train_loss=3.055709837433187
Epoch 2408
Epoch 2408 :: Batch 0/1
Batch Loss = 3.047132389563436
2408, epoch_train_loss=3.047132389563436
Epoch 2409
Epoch 2409 :: Batch 0/1
Batch Loss = 3.0375247375813177
2409, epoch_train_loss=3.0375247375813177
Epoch 2410
Epoch 2410 :: Batch 0/1
Batch Loss = 3.0375475885041157
2410, epoch_train_loss=3.0375475885041157
Epoch 2411
Epoch 2411 :: Batch 0/1
Batch Loss = 3.044609925533767
2411, epoch_train_loss=3.044609925533767
Epoch 2412
Epoch 2412 :: Batch 0/1
Batch Loss = 3.0493704751143786
2412, epoch_train_loss=3.0493704751143786
Epoch 2413
Epoch 2413 :: Batch 0/1
Batch Loss = 3.045437842305723
2413, epoch_train_loss=3.045437842305723
Epoch 2414
Epoch 2414 :: Batch 0/1
Batch Loss = 3.0382225242210796
2414, epoch_train_loss=3.0382225242210796
Epoch 2415
Epoch 2415 :: Batch 0/1
Batch Loss = 3.0351439700389666
2415, epoch_train_loss=3.0351439700389666
Epoch 2416
Epoch 2416 :: Batch 0/1
Batch Loss = 3.0362280715385395
2416, epoch_train_loss=3.0362280715385395
Epoch 2417
Epoch 2417 :: Batch 0/1
Batch Loss = 3.0400352586197688
2417, epoch_train_loss=3.0400352586197688
Epoch 2418
Epoch 2418 :: Batch 0/1
Batch Loss = 3.0442361357283936
2418, epoch_train_loss=3.0442361357283936
Epoch 2419
Epoch 2419 :: Batch 0/1
Batch Loss = 3.0439209523607955
2419, epoch_train_loss=3.0439209523607955
Epoch 2420
Epoch 2420 :: Batch 0/1
Batch Loss = 3.0408865765621544
2420, epoch_train_loss=3.0408865765621544
Epoch 2421
Epoch 2421 :: Batch 0/1
Batch Loss = 3.0369159181958363
2421, epoch_train_loss=3.0369159181958363
Epoch 2422
Epoch 2422 :: Batch 0/1
Batch Loss = 3.034526251859071
2422, epoch_train_loss=3.034526251859071
Epoch 2423
Epoch 2423 :: Batch 0/1
Batch Loss = 3.033345998013543
2423, epoch_train_loss=3.033345998013543
Epoch 2424
Epoch 2424 :: Batch 0/1
Batch Loss = 3.0329542939057736
2424, epoch_train_loss=3.0329542939057736
Epoch 2425
Epoch 2425 :: Batch 0/1
Batch Loss = 3.0331053304028472
2425, epoch_train_loss=3.0331053304028472
Epoch 2426
Epoch 2426 :: Batch 0/1
Batch Loss = 3.033836526234562
2426, epoch_train_loss=3.033836526234562
Epoch 2427
Epoch 2427 :: Batch 0/1
Batch Loss = 3.0355475253128312
2427, epoch_train_loss=3.0355475253128312
Epoch 2428
Epoch 2428 :: Batch 0/1
Batch Loss = 3.038348047419418
2428, epoch_train_loss=3.038348047419418
Epoch 2429
Epoch 2429 :: Batch 0/1
Batch Loss = 3.0430074651396986
2429, epoch_train_loss=3.0430074651396986
Epoch 2430
Epoch 2430 :: Batch 0/1
Batch Loss = 3.0451312855374355
2430, epoch_train_loss=3.0451312855374355
Epoch 2431
Epoch 2431 :: Batch 0/1
Batch Loss = 3.0462835438868576
2431, epoch_train_loss=3.0462835438868576
Epoch 2432
Epoch 2432 :: Batch 0/1
Batch Loss = 3.0417547496317083
2432, epoch_train_loss=3.0417547496317083
Epoch 2433
Epoch 2433 :: Batch 0/1
Batch Loss = 3.039133809265133
2433, epoch_train_loss=3.039133809265133
Epoch 2434
Epoch 2434 :: Batch 0/1
Batch Loss = 3.036828394312786
2434, epoch_train_loss=3.036828394312786
Epoch 2435
Epoch 2435 :: Batch 0/1
Batch Loss = 3.0369816731894224
2435, epoch_train_loss=3.0369816731894224
Epoch 2436
Epoch 2436 :: Batch 0/1
Batch Loss = 3.0379426862505254
2436, epoch_train_loss=3.0379426862505254
Epoch 2437
Epoch 2437 :: Batch 0/1
Batch Loss = 3.0417994476506425
2437, epoch_train_loss=3.0417994476506425
Epoch 2438
Epoch 2438 :: Batch 0/1
Batch Loss = 3.0441459691536705
2438, epoch_train_loss=3.0441459691536705
Epoch 2439
Epoch 2439 :: Batch 0/1
Batch Loss = 3.0479700471959523
2439, epoch_train_loss=3.0479700471959523
Epoch 2440
Epoch 2440 :: Batch 0/1
Batch Loss = 3.0436795134082253
2440, epoch_train_loss=3.0436795134082253
Epoch 2441
Epoch 2441 :: Batch 0/1
Batch Loss = 3.041926128363018
2441, epoch_train_loss=3.041926128363018
Epoch 2442
Epoch 2442 :: Batch 0/1
Batch Loss = 3.0385981157624156
2442, epoch_train_loss=3.0385981157624156
Epoch 2443
Epoch 2443 :: Batch 0/1
Batch Loss = 3.0392827323973797
2443, epoch_train_loss=3.0392827323973797
Epoch 2444
Epoch 2444 :: Batch 0/1
Batch Loss = 3.0397961512169003
2444, epoch_train_loss=3.0397961512169003
Epoch 2445
Epoch 2445 :: Batch 0/1
Batch Loss = 3.0443547711988455
2445, epoch_train_loss=3.0443547711988455
Epoch 2446
Epoch 2446 :: Batch 0/1
Batch Loss = 3.044170792986063
2446, epoch_train_loss=3.044170792986063
Epoch 2447
Epoch 2447 :: Batch 0/1
Batch Loss = 3.046689690214388
2447, epoch_train_loss=3.046689690214388
Epoch 2448
Epoch 2448 :: Batch 0/1
Batch Loss = 3.0414980205912485
2448, epoch_train_loss=3.0414980205912485
Epoch 2449
Epoch 2449 :: Batch 0/1
Batch Loss = 3.040907110339353
2449, epoch_train_loss=3.040907110339353
Epoch 2450
Epoch 2450 :: Batch 0/1
Batch Loss = 3.0385226823457483
2450, epoch_train_loss=3.0385226823457483
Epoch 2451
Epoch 2451 :: Batch 0/1
Batch Loss = 3.0411541731947915
2451, epoch_train_loss=3.0411541731947915
Epoch 2452
Epoch 2452 :: Batch 0/1
Batch Loss = 3.0412507394203154
2452, epoch_train_loss=3.0412507394203154
Epoch 2453
Epoch 2453 :: Batch 0/1
Batch Loss = 3.04597945097045
2453, epoch_train_loss=3.04597945097045
Epoch 2454
Epoch 2454 :: Batch 0/1
Batch Loss = 3.042315788052358
2454, epoch_train_loss=3.042315788052358
Epoch 2455
Epoch 2455 :: Batch 0/1
Batch Loss = 3.043247993079912
2455, epoch_train_loss=3.043247993079912
Epoch 2456
Epoch 2456 :: Batch 0/1
Batch Loss = 3.03888033548169
2456, epoch_train_loss=3.03888033548169
Epoch 2457
Epoch 2457 :: Batch 0/1
Batch Loss = 3.040270539502048
2457, epoch_train_loss=3.040270539502048
Epoch 2458
Epoch 2458 :: Batch 0/1
Batch Loss = 3.0388985626542304
2458, epoch_train_loss=3.0388985626542304
Epoch 2459
Epoch 2459 :: Batch 0/1
Batch Loss = 3.0431989216695143
2459, epoch_train_loss=3.0431989216695143
Epoch 2460
Epoch 2460 :: Batch 0/1
Batch Loss = 3.041039529508045
2460, epoch_train_loss=3.041039529508045
Epoch 2461
Epoch 2461 :: Batch 0/1
Batch Loss = 3.044125440469007
2461, epoch_train_loss=3.044125440469007
Epoch 2462
Epoch 2462 :: Batch 0/1
Batch Loss = 3.0391803305400735
2462, epoch_train_loss=3.0391803305400735
Epoch 2463
Epoch 2463 :: Batch 0/1
Batch Loss = 3.04039788225734
2463, epoch_train_loss=3.04039788225734
Epoch 2464
Epoch 2464 :: Batch 0/1
Batch Loss = 3.037518122330385
2464, epoch_train_loss=3.037518122330385
Epoch 2465
Epoch 2465 :: Batch 0/1
Batch Loss = 3.0409498692932257
2465, epoch_train_loss=3.0409498692932257
Epoch 2466
Epoch 2466 :: Batch 0/1
Batch Loss = 3.039065938556368
2466, epoch_train_loss=3.039065938556368
Epoch 2467
Epoch 2467 :: Batch 0/1
Batch Loss = 3.043293687475381
2467, epoch_train_loss=3.043293687475381
Epoch 2468
Epoch 2468 :: Batch 0/1
Batch Loss = 3.03878225385687
2468, epoch_train_loss=3.03878225385687
Epoch 2469
Epoch 2469 :: Batch 0/1
Batch Loss = 3.040762889515517
2469, epoch_train_loss=3.040762889515517
Epoch 2470
Epoch 2470 :: Batch 0/1
Batch Loss = 3.0367584210990337
2470, epoch_train_loss=3.0367584210990337
Epoch 2471
Epoch 2471 :: Batch 0/1
Batch Loss = 3.0396026174524895
2471, epoch_train_loss=3.0396026174524895
Epoch 2472
Epoch 2472 :: Batch 0/1
Batch Loss = 3.037202612261122
2472, epoch_train_loss=3.037202612261122
Epoch 2473
Epoch 2473 :: Batch 0/1
Batch Loss = 3.0415993881909644
2473, epoch_train_loss=3.0415993881909644
Epoch 2474
Epoch 2474 :: Batch 0/1
Batch Loss = 3.037728485158763
2474, epoch_train_loss=3.037728485158763
Epoch 2475
Epoch 2475 :: Batch 0/1
Batch Loss = 3.0407715333087646
2475, epoch_train_loss=3.0407715333087646
Epoch 2476
Epoch 2476 :: Batch 0/1
Batch Loss = 3.0361490824248385
2476, epoch_train_loss=3.0361490824248385
Epoch 2477
Epoch 2477 :: Batch 0/1
Batch Loss = 3.0388375585144107
2477, epoch_train_loss=3.0388375585144107
Epoch 2478
Epoch 2478 :: Batch 0/1
Batch Loss = 3.035678895690137
2478, epoch_train_loss=3.035678895690137
Epoch 2479
Epoch 2479 :: Batch 0/1
Batch Loss = 3.0398465309142773
2479, epoch_train_loss=3.0398465309142773
Epoch 2480
Epoch 2480 :: Batch 0/1
Batch Loss = 3.03632042777867
2480, epoch_train_loss=3.03632042777867
Epoch 2481
Epoch 2481 :: Batch 0/1
Batch Loss = 3.040198557637046
2481, epoch_train_loss=3.040198557637046
Epoch 2482
Epoch 2482 :: Batch 0/1
Batch Loss = 3.035400394780027
2482, epoch_train_loss=3.035400394780027
Epoch 2483
Epoch 2483 :: Batch 0/1
Batch Loss = 3.0383307004351203
2483, epoch_train_loss=3.0383307004351203
Epoch 2484
Epoch 2484 :: Batch 0/1
Batch Loss = 3.0344597566571174
2484, epoch_train_loss=3.0344597566571174
Epoch 2485
Epoch 2485 :: Batch 0/1
Batch Loss = 3.0383856032929093
2485, epoch_train_loss=3.0383856032929093
Epoch 2486
Epoch 2486 :: Batch 0/1
Batch Loss = 3.034834105256578
2486, epoch_train_loss=3.034834105256578
Epoch 2487
Epoch 2487 :: Batch 0/1
Batch Loss = 3.0391746212188595
2487, epoch_train_loss=3.0391746212188595
Epoch 2488
Epoch 2488 :: Batch 0/1
Batch Loss = 3.0344350663384874
2488, epoch_train_loss=3.0344350663384874
Epoch 2489
Epoch 2489 :: Batch 0/1
Batch Loss = 3.037839124417453
2489, epoch_train_loss=3.037839124417453
Epoch 2490
Epoch 2490 :: Batch 0/1
Batch Loss = 3.0334084539705013
2490, epoch_train_loss=3.0334084539705013
Epoch 2491
Epoch 2491 :: Batch 0/1
Batch Loss = 3.03723357995965
2491, epoch_train_loss=3.03723357995965
Epoch 2492
Epoch 2492 :: Batch 0/1
Batch Loss = 3.0334111864879767
2492, epoch_train_loss=3.0334111864879767
Epoch 2493
Epoch 2493 :: Batch 0/1
Batch Loss = 3.037942108006114
2493, epoch_train_loss=3.037942108006114
Epoch 2494
Epoch 2494 :: Batch 0/1
Batch Loss = 3.033296557228655
2494, epoch_train_loss=3.033296557228655
Epoch 2495
Epoch 2495 :: Batch 0/1
Batch Loss = 3.0372154316822773
2495, epoch_train_loss=3.0372154316822773
Epoch 2496
Epoch 2496 :: Batch 0/1
Batch Loss = 3.0324021860175274
2496, epoch_train_loss=3.0324021860175274
Epoch 2497
Epoch 2497 :: Batch 0/1
Batch Loss = 3.036289874364374
2497, epoch_train_loss=3.036289874364374
Epoch 2498
Epoch 2498 :: Batch 0/1
Batch Loss = 3.0320970926692135
2498, epoch_train_loss=3.0320970926692135
Epoch 2499
Epoch 2499 :: Batch 0/1
Batch Loss = 3.036690217227669
2499, epoch_train_loss=3.036690217227669
