/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/pyscf/dft/libxc.py:772: UserWarning: Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, the same to the B3LYP functional in Gaussian and ORCA (issue 1480). To restore the VWN5 definition, you can put the setting "B3LYP_WITH_VWN5 = True" in pyscf_conf.py
  warnings.warn('Since PySCF-2.3, B3LYP (and B3P86) are changed to the VWN-RPA variant, '
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0a92d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0a92d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
WARNING: External module "mldftdat" required for non-local descriptor use.
<pyscf.gto.mole.Mole object at 0x7ffeac0a92d0> [['P', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac0a9ab0> [['N', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac0a9cc0> [['H', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac0a9b40> [['Li', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac0a9bd0> [['O', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac0a9e70> [['Cl', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac0aa080> [['Al', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac0a9e10> [['S', array([0., 0., 0.])]] 1
<pyscf.gto.mole.Mole object at 0x7ffeac0aa470> [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0aa4a0> [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0aa5c0> [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0aa8f0> [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeac0aab00> [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeac0aa890> [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0aada0> [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0aadd0> [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0aaa40> [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeac0ab070> [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0aaf80> [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0aaef0> [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0ab220> [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0ab430> [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeac0ab1c0> [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]] 4
<pyscf.gto.mole.Mole object at 0x7ffeac0ab6d0> [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]] 2
<pyscf.gto.mole.Mole object at 0x7ffeac0ab370> [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]] 5
<pyscf.gto.mole.Mole object at 0x7ffeac0ab820> [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]] 3
<pyscf.gto.mole.Mole object at 0x7ffeac0abb50> [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]] 5
mol:  [['P', array([0., 0., 0.])]]
converged SCF energy = -341.104145992717  <S^2> = 3.7502984  2S+1 = 4.0001492
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0a9ab0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0a9ab0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-3.48053232e-03 -9.22981806e-04 -2.09507924e-03 ... -1.11294850e+01
 -1.11294850e+01 -1.11294850e+01] = SCAN,
rho_a.shape=(6, 6320), rho_b.shape=(6, 6320)
fxc_a.shape=(6320,), fxc_b.shape=(6320,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12640), fxc.shape=(12640,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(2, 6320, 15)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12640, 15)
rho_filt.shape=(12640,)
mol:  [['N', array([0., 0., 0.])]]
converged SCF energy = -54.5289742046674  <S^2> = 3.7524945  2S+1 = 4.0012471
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0a9cc0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0a9cc0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5016, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.10256797e-03 -5.98013179e-04 -6.71209617e-05 ... -5.03581543e+00
 -5.03581543e+00 -5.03581543e+00] = SCAN,
rho_a.shape=(6, 5016), rho_b.shape=(6, 5016)
fxc_a.shape=(5016,), fxc_b.shape=(5016,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 10032), fxc.shape=(10032,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5016, 22)
rho shape (4, 4, 2, 5016)
rho_filt shape: (5016,)
get descriptors tdrho.shape=(2, 5016, 15)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(10032, 15)
rho_filt.shape=(10032,)
mol:  [['H', array([0., 0., 0.])]]
converged SCF energy = -0.499812984008539  <S^2> = 0.75  2S+1 = 2
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0a9b40> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0a9b40> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 7, 7)
ao.shape (10, 2440, 7)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.59173730e-03 -7.67300285e-04 -6.09330303e-05 ... -7.27736608e-01
 -7.27736608e-01 -7.27736608e-01] = SCAN,
rho_a.shape=(6, 2440), rho_b.shape=(6, 2440)
fxc_a.shape=(2440,), fxc_b.shape=(2440,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 2440), fxc.shape=(2440,)
get_data, dm shape = (2, 7, 7)
ao_eval.shape=(4, 2440, 7)
rho shape (4, 4, 2, 2440)
rho_filt shape: (2440,)
get descriptors tdrho.shape=(2, 2440, 15)
mol:  [['Li', array([0., 0., 0.])]]
converged SCF energy = -7.46006188627842  <S^2> = 0.75000049  2S+1 = 2.0000005
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0a9bd0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0a9bd0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 4592, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-6.71507910e-03 -1.45299376e-03 -1.45299376e-03 ... -1.46930969e-02
 -2.05021258e+00 -2.05021258e+00] = SCAN,
rho_a.shape=(6, 4592), rho_b.shape=(6, 4592)
fxc_a.shape=(4592,), fxc_b.shape=(4592,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 9184), fxc.shape=(9184,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 4592, 22)
rho shape (4, 4, 2, 4592)
rho_filt shape: (4592,)
get descriptors tdrho.shape=(2, 4592, 15)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(9184, 15)
rho_filt.shape=(9184,)
mol:  [['O', array([0., 0., 0.])]]
SCF not converged.
SCF energy = -75.0033774338371 after 50 cycles  <S^2> = 2.0027403  2S+1 = 3.0018263
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0a9e70> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0a9e70> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 22, 22)
ao.shape (10, 5040, 22)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-7.08461346e-04 -1.23177614e-04 -6.18265752e-06 ... -5.78388659e+00
 -5.78388659e+00 -5.78388659e+00] = SCAN,
rho_a.shape=(6, 5040), rho_b.shape=(6, 5040)
fxc_a.shape=(5040,), fxc_b.shape=(5040,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 10080), fxc.shape=(10080,)
get_data, dm shape = (2, 22, 22)
ao_eval.shape=(4, 5040, 22)
rho shape (4, 4, 2, 5040)
rho_filt shape: (5040,)
get descriptors tdrho.shape=(2, 5040, 15)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(10080, 15)
rho_filt.shape=(10080,)
mol:  [['Cl', array([0., 0., 0.])]]
converged SCF energy = -459.957577121563  <S^2> = 0.7516194  2S+1 = 2.0016187
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0aa080> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0aa080> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6152, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-5.92787549e-04 -9.51826916e-04 -3.33326666e-04 ... -1.26646370e+01
 -1.26646370e+01 -1.26646370e+01] = SCAN,
rho_a.shape=(6, 6152), rho_b.shape=(6, 6152)
fxc_a.shape=(6152,), fxc_b.shape=(6152,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12304), fxc.shape=(12304,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6152, 30)
rho shape (4, 4, 2, 6152)
rho_filt shape: (6152,)
get descriptors tdrho.shape=(2, 6152, 15)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12304, 15)
rho_filt.shape=(12304,)
mol:  [['Al', array([0., 0., 0.])]]
converged SCF energy = -242.226560990731  <S^2> = 0.75226414  2S+1 = 2.0022629
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0a9e10> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0a9e10> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6088, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.02415743 -0.01542239 -0.00781769 ... -0.0001393  -0.00172046
 -0.00012585] = SCAN,
rho_a.shape=(6, 6088), rho_b.shape=(6, 6088)
fxc_a.shape=(6088,), fxc_b.shape=(6088,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12176), fxc.shape=(12176,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6088, 30)
rho shape (4, 4, 2, 6088)
rho_filt shape: (6088,)
get descriptors tdrho.shape=(2, 6088, 15)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12176, 15)
rho_filt.shape=(12176,)
mol:  [['S', array([0., 0., 0.])]]
converged SCF energy = -397.938786806904  <S^2> = 2.0022329  2S+1 = 3.0014882
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0aa470> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0aa470> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 30, 30)
ao.shape (10, 6320, 30)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.43071587e-03 -7.97253589e-04 -9.75867133e-04 ... -1.18982463e+01
 -1.18982463e+01 -1.18982463e+01] = SCAN,
rho_a.shape=(6, 6320), rho_b.shape=(6, 6320)
fxc_a.shape=(6320,), fxc_b.shape=(6320,)
mol.spin != 0 and sum(mol.nelec) > 1
rho.shape=(6, 12640), fxc.shape=(12640,)
get_data, dm shape = (2, 30, 30)
ao_eval.shape=(4, 6320, 30)
rho shape (4, 4, 2, 6320)
rho_filt shape: (6320,)
get descriptors tdrho.shape=(2, 6320, 15)
mol.spin != 0 and sum(mol.nelec) > 1
concatenating spin channels along axis=0
tdrho.shape=(12640, 15)
rho_filt.shape=(12640,)
mol:  [['H', array([0.      , 0.      , 0.371395])], ['H', array([ 0.      ,  0.      , -0.371395])]]
converged SCF energy = -1.16580491182912  <S^2> = -8.8817842e-16  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0aa4a0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0aa4a0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 14, 14)
ao.shape (10, 4776, 14)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.31556415e-04 -9.72662915e-06 -3.66768501e-04 ... -5.54165573e-01
 -5.54165573e-01 -5.54165573e-01] = SCAN,
rho_a.shape=(6, 4776), rho_b.shape=(6, 4776)
fxc_a.shape=(4776,), fxc_b.shape=(4776,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 4776), fxc.shape=(4776,)
get_data, dm shape = (2, 14, 14)
ao_eval.shape=(4, 4776, 14)
rho shape (4, 4, 2, 4776)
rho_filt shape: (4776,)
get descriptors tdrho.shape=(2, 4776, 15)
mol:  [['N', array([0.      , 0.      , 0.549396])], ['N', array([ 0.      ,  0.      , -0.549396])]]
converged SCF energy = -109.439263799566  <S^2> = 1.5099033e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0aa5c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0aa5c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9848, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-9.68469910e-05 -9.84742277e-04 -2.59676148e-04 ... -2.39626668e-05
 -2.39626668e-05 -9.68469910e-05] = SCAN,
rho_a.shape=(6, 9848), rho_b.shape=(6, 9848)
fxc_a.shape=(9848,), fxc_b.shape=(9848,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9848), fxc.shape=(9848,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9848, 44)
rho shape (4, 4, 2, 9848)
rho_filt shape: (9848,)
get descriptors tdrho.shape=(2, 9848, 15)
mol:  [['Li', array([ 0.      ,  0.      , -1.172697])], ['F', array([0.      , 0.      , 0.390899])]]
converged SCF energy = -107.339357395734  <S^2> = 1.7763568e-15  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0aa8f0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0aa8f0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9752, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.04987750e-03 -6.68953858e-04 -8.57556270e-04 ... -1.07485583e-03
 -8.01425698e-01 -8.01425698e-01] = SCAN,
rho_a.shape=(6, 9752), rho_b.shape=(6, 9752)
fxc_a.shape=(9752,), fxc_b.shape=(9752,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9752), fxc.shape=(9752,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9752, 44)
rho shape (4, 4, 2, 9752)
rho_filt shape: (9752,)
get descriptors tdrho.shape=(2, 9752, 15)
mol:  [['C', array([ 0.      ,  0.      , -0.499686])], ['N', array([0.      , 0.      , 0.652056])], ['H', array([ 0.        ,  0.        , -1.56627401])]]
converged SCF energy = -93.337792446513  <S^2> = 4.0072834e-10  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0aab00> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0aab00> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 12256, 51)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.97917285e-04 -2.54412366e-05 -3.15182243e-05 ... -6.37386500e-01
 -6.37386500e-01 -6.37386500e-01] = SCAN,
rho_a.shape=(6, 12256), rho_b.shape=(6, 12256)
fxc_a.shape=(12256,), fxc_b.shape=(12256,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12256), fxc.shape=(12256,)
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 12256, 51)
rho shape (4, 4, 2, 12256)
rho_filt shape: (12256,)
get descriptors tdrho.shape=(2, 12256, 15)
mol:  [['C', array([0., 0., 0.])], ['O', array([0.      , 0.      , 1.162879])], ['O', array([ 0.      ,  0.      , -1.162879])]]
converged SCF energy = -188.456965322844  <S^2> = 2.1316282e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0aa890> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0aa890> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 66, 66)
ao.shape (10, 14920, 66)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.50217115e-04 -2.07520066e-04 -9.23619896e-04 ... -2.74295208e-06
 -4.27559894e+00 -4.27559894e+00] = SCAN,
rho_a.shape=(6, 14920), rho_b.shape=(6, 14920)
fxc_a.shape=(14920,), fxc_b.shape=(14920,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 14920), fxc.shape=(14920,)
get_data, dm shape = (2, 66, 66)
ao_eval.shape=(4, 14920, 66)
rho shape (4, 4, 2, 14920)
rho_filt shape: (14920,)
get descriptors tdrho.shape=(2, 14920, 15)
mol:  [['Cl', array([0.      , 0.      , 1.008241])], ['Cl', array([ 0.      ,  0.      , -1.008241])]]
converged SCF energy = -920.005608888959  <S^2> = 4.9027449e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0aada0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0aada0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12208, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00155834 -0.00091688 -0.00215831 ... -0.00091688 -0.41618507
 -0.41618507] = SCAN,
rho_a.shape=(6, 12208), rho_b.shape=(6, 12208)
fxc_a.shape=(12208,), fxc_b.shape=(12208,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12208), fxc.shape=(12208,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12208, 60)
rho shape (4, 4, 2, 12208)
rho_filt shape: (12208,)
get descriptors tdrho.shape=(2, 12208, 15)
mol:  [['F', array([0.      , 0.      , 0.693963])], ['F', array([ 0.      ,  0.      , -0.693963])]]
converged SCF energy = -199.394370591172  <S^2> = 1.2612134e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0aadd0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0aadd0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9824, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.92948614e-04 -1.95198688e-05 -1.16699802e-03 ... -4.89378340e-01
 -4.89378340e-01 -4.89378340e-01] = SCAN,
rho_a.shape=(6, 9824), rho_b.shape=(6, 9824)
fxc_a.shape=(9824,), fxc_b.shape=(9824,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9824), fxc.shape=(9824,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9824, 44)
rho shape (4, 4, 2, 9824)
rho_filt shape: (9824,)
get descriptors tdrho.shape=(2, 9824, 15)
mol:  [['O', array([0.      , 0.      , 0.603195])], ['O', array([ 0.      ,  0.      , -0.603195])]]
converged SCF energy = -150.214894490397  <S^2> = 1.0018599  2S+1 = 2.2377309
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0aaa40> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0aaa40> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 9912, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.22200160e-04 -1.46866652e-04 -7.59096596e-06 ... -6.59150638e-01
 -6.59150638e-01 -6.59150638e-01] = SCAN,
rho_a.shape=(6, 9912), rho_b.shape=(6, 9912)
fxc_a.shape=(9912,), fxc_b.shape=(9912,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9912), fxc.shape=(9912,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 9912, 44)
rho shape (4, 4, 2, 9912)
rho_filt shape: (9912,)
get descriptors tdrho.shape=(2, 9912, 15)
mol:  [['C', array([0.      , 0.      , 0.599454])], ['C', array([ 0.      ,  0.      , -0.599454])], ['H', array([ 0.        ,  0.        , -1.66162301])], ['H', array([0.        , 0.        , 1.66162301])]]
converged SCF energy = -77.2435048346372  <S^2> = 7.9936058e-15  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0ab070> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0ab070> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 58, 58)
ao.shape (10, 15208, 58)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.83270456e-05 -8.83270456e-05 -9.75839850e-04 ... -3.46719667e-05
 -3.31708644e-05 -3.31708644e-05] = SCAN,
rho_a.shape=(6, 15208), rho_b.shape=(6, 15208)
fxc_a.shape=(15208,), fxc_b.shape=(15208,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 15208), fxc.shape=(15208,)
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15208, 58)
rho shape (4, 4, 2, 15208)
rho_filt shape: (15208,)
get descriptors tdrho.shape=(2, 15208, 15)
mol:  [['O', array([0.      , 0.      , 0.484676])], ['C', array([ 0.      ,  0.      , -0.646235])]]
converged SCF energy = -113.221335689652  <S^2> = 6.5991657e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0aaf80> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0aaf80> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 44, 44)
ao.shape (10, 10040, 44)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-5.37000578e-04 -8.55494549e-04 -2.46853288e-03 ... -7.34251999e-01
 -7.34251999e-01 -7.34251999e-01] = SCAN,
rho_a.shape=(6, 10040), rho_b.shape=(6, 10040)
fxc_a.shape=(10040,), fxc_b.shape=(10040,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 10040), fxc.shape=(10040,)
get_data, dm shape = (2, 44, 44)
ao_eval.shape=(4, 10040, 44)
rho shape (4, 4, 2, 10040)
rho_filt shape: (10040,)
get descriptors tdrho.shape=(2, 10040, 15)
mol:  [['Cl', array([0.      , 0.      , 0.071315])], ['H', array([ 0.      ,  0.      , -1.212358])]]
converged SCF energy = -460.624592374078  <S^2> = 6.5725203e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0aaef0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0aaef0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 37, 37)
ao.shape (10, 8552, 37)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-2.38161177e-04 -1.81188367e-05 -2.37300299e-05 ... -2.83738108e+00
 -2.83738108e+00 -2.83738108e+00] = SCAN,
rho_a.shape=(6, 8552), rho_b.shape=(6, 8552)
fxc_a.shape=(8552,), fxc_b.shape=(8552,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 8552), fxc.shape=(8552,)
get_data, dm shape = (2, 37, 37)
ao_eval.shape=(4, 8552, 37)
rho shape (4, 4, 2, 8552)
rho_filt shape: (8552,)
get descriptors tdrho.shape=(2, 8552, 15)
mol:  [['Li', array([0.      , 0.      , 0.403632])], ['H', array([ 0.      ,  0.      , -1.210897])]]
converged SCF energy = -8.04458854018922  <S^2> = 7.6827433e-14  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0ab220> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0ab220> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 29, 29)
ao.shape (10, 6936, 29)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00113445 -0.00118624 -0.00118624 ... -0.48434639 -0.48434639
 -0.48434639] = SCAN,
rho_a.shape=(6, 6936), rho_b.shape=(6, 6936)
fxc_a.shape=(6936,), fxc_b.shape=(6936,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 6936), fxc.shape=(6936,)
get_data, dm shape = (2, 29, 29)
ao_eval.shape=(4, 6936, 29)
rho shape (4, 4, 2, 6936)
rho_filt shape: (6936,)
get descriptors tdrho.shape=(2, 6936, 15)
mol:  [['Na', array([0.        , 0.        , 1.50747901])], ['Na', array([ 0.        ,  0.        , -1.50747901])]]
converged SCF energy = -324.340512506578  <S^2> = 1.5864643e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0ab430> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0ab430> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 11536, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-0.00297935 -0.00297935 -0.00407089 ... -0.00297935 -0.00297935
 -0.00407089] = SCAN,
rho_a.shape=(6, 11536), rho_b.shape=(6, 11536)
fxc_a.shape=(11536,), fxc_b.shape=(11536,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 11536), fxc.shape=(11536,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 11536, 60)
rho shape (4, 4, 2, 11536)
rho_filt shape: (11536,)
get descriptors tdrho.shape=(2, 11536, 15)
mol:  [['Al', array([0., 0., 0.])], ['Cl', array([0.        , 2.08019101, 0.        ])], ['Cl', array([ 1.80149801, -1.040095  ,  0.        ])], ['Cl', array([-1.80149801, -1.040095  ,  0.        ])]]
converged SCF energy = -1622.57507845814  <S^2> = 8.31335e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0ab1c0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0ab1c0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 120, 120)
ao.shape (10, 24512, 120)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.61400996e-04 -4.90484900e-04 -2.56451718e-03 ... -9.59296113e+00
 -9.59296113e+00 -9.59296113e+00] = SCAN,
rho_a.shape=(6, 24512), rho_b.shape=(6, 24512)
fxc_a.shape=(24512,), fxc_b.shape=(24512,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 24512), fxc.shape=(24512,)
get_data, dm shape = (2, 120, 120)
ao_eval.shape=(4, 24512, 120)
rho shape (4, 4, 2, 24512)
rho_filt shape: (24512,)
get descriptors tdrho.shape=(2, 24512, 15)
mol:  [['P', array([0.      , 0.      , 0.128906])], ['H', array([ 0.      ,  1.19333 , -0.644531])], ['H', array([ 1.033455, -0.596665, -0.644531])], ['H', array([-1.033455, -0.596665, -0.644531])]]
converged SCF energy = -342.979728469576  <S^2> = 2.5391245e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0ab6d0> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0ab6d0> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 51, 51)
ao.shape (10, 13096, 51)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.28637920e-03 -4.32383300e-04 -3.74057272e-05 ... -1.91722770e+00
 -1.91722770e+00 -1.91722770e+00] = SCAN,
rho_a.shape=(6, 13096), rho_b.shape=(6, 13096)
fxc_a.shape=(13096,), fxc_b.shape=(13096,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 13096), fxc.shape=(13096,)
get_data, dm shape = (2, 51, 51)
ao_eval.shape=(4, 13096, 51)
rho shape (4, 4, 2, 13096)
rho_filt shape: (13096,)
get descriptors tdrho.shape=(2, 13096, 15)
mol:  [['Si', array([0.      , 0.      , 1.135214])], ['Si', array([ 0.      ,  0.      , -1.135214])]]
converged SCF energy = -578.565336680792  <S^2> = 1.0034708  2S+1 = 2.2391702
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0ab370> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0ab370> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 60, 60)
ao.shape (10, 12384, 60)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-1.60030106e-04 -2.60763242e-04 -2.59345009e-04 ... -3.86943856e-01
 -3.86943856e-01 -3.86943856e-01] = SCAN,
rho_a.shape=(6, 12384), rho_b.shape=(6, 12384)
fxc_a.shape=(12384,), fxc_b.shape=(12384,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 12384), fxc.shape=(12384,)
get_data, dm shape = (2, 60, 60)
ao_eval.shape=(4, 12384, 60)
rho shape (4, 4, 2, 12384)
rho_filt shape: (12384,)
get descriptors tdrho.shape=(2, 12384, 15)
mol:  [['C', array([0., 0., 0.])], ['H', array([0.630382, 0.630382, 0.630382])], ['H', array([-0.630382, -0.630382,  0.630382])], ['H', array([ 0.630382, -0.630382, -0.630382])], ['H', array([-0.630382,  0.630382, -0.630382])]]
converged SCF energy = -40.4598214864075  <S^2> = 3.2152059e-13  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0ab820> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0ab820> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 50, 50)
ao.shape (10, 13936, 50)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.68439986e-04 -2.42462569e-04 -1.69927031e-05 ... -2.55230307e-05
 -2.55230307e-05 -2.55230307e-05] = SCAN,
rho_a.shape=(6, 13936), rho_b.shape=(6, 13936)
fxc_a.shape=(13936,), fxc_b.shape=(13936,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 13936), fxc.shape=(13936,)
get_data, dm shape = (2, 50, 50)
ao_eval.shape=(4, 13936, 50)
rho shape (4, 4, 2, 13936)
rho_filt shape: (13936,)
get descriptors tdrho.shape=(2, 13936, 15)
mol:  [['C', array([0.      , 0.      , 0.179918])], ['H', array([ 0.      ,  0.855475, -0.539754])], ['H', array([ 0.      , -0.855475, -0.539754])]]
converged SCF energy = -39.0756147483504  <S^2> = 6.1985972e-12  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
Warning: <pyscf.gto.mole.Mole object at 0x7ffeac0abb50> must be initialized before calling SCF.
Initialize <pyscf.gto.mole.Mole object at 0x7ffeac0abb50> in UKS object of <class 'pyscf.dft.uks.UKS'>
New DM shape: (2, 36, 36)
ao.shape (10, 9656, 36)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-7.67688751e-04 -4.57393214e-05 -2.02834191e-04 ... -1.14928924e+00
 -1.14928924e+00 -1.14928924e+00] = SCAN,
rho_a.shape=(6, 9656), rho_b.shape=(6, 9656)
fxc_a.shape=(9656,), fxc_b.shape=(9656,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 9656), fxc.shape=(9656,)
get_data, dm shape = (2, 36, 36)
ao_eval.shape=(4, 9656, 36)
rho shape (4, 4, 2, 9656)
rho_filt shape: (9656,)
get descriptors tdrho.shape=(2, 9656, 15)
mol:  [['Si', array([0., 0., 0.])], ['H', array([0.855876, 0.855876, 0.855876])], ['H', array([-0.855876, -0.855876,  0.855876])], ['H', array([-0.855876,  0.855876, -0.855876])], ['H', array([ 0.855876, -0.855876, -0.855876])]]
converged SCF energy = -291.719272437818  <S^2> = 1.3148593e-11  2S+1 = 1
/gpfs/home/jofranklin/.conda/envs/pyscfad/lib/python3.10/site-packages/jax/_src/ops/scatter.py:96: FutureWarning: scatter inputs have incompatible types: cannot safely cast value from dtype=int64 to dtype=int32 with jax_numpy_dtype_promotion='standard'. In future JAX releases this will result in an error.
  warnings.warn(
New DM shape: (2, 58, 58)
ao.shape (10, 15256, 58)
Exchange contribution only
SCAN,
spin scaling
fxc with xc_func = [-8.33850535e-04 -2.34903029e-04 -1.75623665e-05 ... -1.92891112e-05
 -1.92891112e-05 -1.92891112e-05] = SCAN,
rho_a.shape=(6, 15256), rho_b.shape=(6, 15256)
fxc_a.shape=(15256,), fxc_b.shape=(15256,)
NOT (mol.spin != 0 and sum(mol.nelec) > 1)
rho.shape=(6, 15256), fxc.shape=(15256,)
get_data, dm shape = (2, 58, 58)
ao_eval.shape=(4, 15256, 58)
rho shape (4, 4, 2, 15256)
rho_filt shape: (15256,)
get descriptors tdrho.shape=(2, 15256, 15)
localnet.spin_scaling: concatenating the data
first data shape = (10940, 15)
concatenated: tdrho.shape=(271711, 15)
PRE NAN FILT: tFxc.shape=(271711,), tdrho.shape=(271711, 15)
nan_filt_rho.shape=(271711,)
nan_filt_fxc.shape=(271711,)
tFxc.shape=(271711,), tdrho.shape=(271711, 15)
inp[0].shape = (271711, 15)
Epoch 0
Epoch 0 :: Batch 0/1
Batch Loss = 3699743.2558365576
0, epoch_train_loss=3699743.2558365576
Epoch 1
Epoch 1 :: Batch 0/1
Batch Loss = 1177019.576781283
1, epoch_train_loss=1177019.576781283
Epoch 2
Epoch 2 :: Batch 0/1
Batch Loss = 40306.67906831451
2, epoch_train_loss=40306.67906831451
Epoch 3
Epoch 3 :: Batch 0/1
Batch Loss = 121.68337334688559
3, epoch_train_loss=121.68337334688559
Epoch 4
Epoch 4 :: Batch 0/1
Batch Loss = 4.9895876559841605
4, epoch_train_loss=4.9895876559841605
Epoch 5
Epoch 5 :: Batch 0/1
Batch Loss = 131749.7147468941
5, epoch_train_loss=131749.7147468941
Epoch 6
Epoch 6 :: Batch 0/1
Batch Loss = 4.892774506721112
6, epoch_train_loss=4.892774506721112
Epoch 7
Epoch 7 :: Batch 0/1
Batch Loss = 4.887172154077316
7, epoch_train_loss=4.887172154077316
Epoch 8
Epoch 8 :: Batch 0/1
Batch Loss = 4.87666419893181
8, epoch_train_loss=4.87666419893181
Epoch 9
Epoch 9 :: Batch 0/1
Batch Loss = 4.86456249666527
9, epoch_train_loss=4.86456249666527
Epoch 10
Epoch 10 :: Batch 0/1
Batch Loss = 6.116837298646306
10, epoch_train_loss=6.116837298646306
Epoch 11
Epoch 11 :: Batch 0/1
Batch Loss = 4.84949714390274
11, epoch_train_loss=4.84949714390274
Epoch 12
Epoch 12 :: Batch 0/1
Batch Loss = 1225.160344363706
12, epoch_train_loss=1225.160344363706
Epoch 13
Epoch 13 :: Batch 0/1
Batch Loss = 4.841874421135796
13, epoch_train_loss=4.841874421135796
Epoch 14
Epoch 14 :: Batch 0/1
Batch Loss = 95746.56003026351
14, epoch_train_loss=95746.56003026351
Epoch 15
Epoch 15 :: Batch 0/1
Batch Loss = 4.829492153071134
15, epoch_train_loss=4.829492153071134
Epoch 16
Epoch 16 :: Batch 0/1
Batch Loss = 4.822976854624891
16, epoch_train_loss=4.822976854624891
Epoch 17
Epoch 17 :: Batch 0/1
Batch Loss = 4.815536275442883
17, epoch_train_loss=4.815536275442883
Epoch 18
Epoch 18 :: Batch 0/1
Batch Loss = 4.807145731448691
18, epoch_train_loss=4.807145731448691
Epoch 19
Epoch 19 :: Batch 0/1
Batch Loss = 4.798322996860483
19, epoch_train_loss=4.798322996860483
Epoch 20
Epoch 20 :: Batch 0/1
Batch Loss = 4.793805317699544
20, epoch_train_loss=4.793805317699544
Epoch 21
Epoch 21 :: Batch 0/1
Batch Loss = 4.786190435160839
21, epoch_train_loss=4.786190435160839
Epoch 22
Epoch 22 :: Batch 0/1
Batch Loss = 4.778255030039568
22, epoch_train_loss=4.778255030039568
Epoch 23
Epoch 23 :: Batch 0/1
Batch Loss = 4.772361173939514
23, epoch_train_loss=4.772361173939514
Epoch 24
Epoch 24 :: Batch 0/1
Batch Loss = 4.766833326460552
24, epoch_train_loss=4.766833326460552
Epoch 25
Epoch 25 :: Batch 0/1
Batch Loss = 4.760734841743288
25, epoch_train_loss=4.760734841743288
Epoch 26
Epoch 26 :: Batch 0/1
Batch Loss = 4.753627268624399
26, epoch_train_loss=4.753627268624399
Epoch 27
Epoch 27 :: Batch 0/1
Batch Loss = 4.746353344981553
27, epoch_train_loss=4.746353344981553
Epoch 28
Epoch 28 :: Batch 0/1
Batch Loss = 4.743257944321003
28, epoch_train_loss=4.743257944321003
Epoch 29
Epoch 29 :: Batch 0/1
Batch Loss = 4.735489577097866
29, epoch_train_loss=4.735489577097866
Epoch 30
Epoch 30 :: Batch 0/1
Batch Loss = 4.729648354120268
30, epoch_train_loss=4.729648354120268
Epoch 31
Epoch 31 :: Batch 0/1
Batch Loss = 4.725362132601068
31, epoch_train_loss=4.725362132601068
Epoch 32
Epoch 32 :: Batch 0/1
Batch Loss = 4.7194884674146325
32, epoch_train_loss=4.7194884674146325
Epoch 33
Epoch 33 :: Batch 0/1
Batch Loss = 4.712513275293906
33, epoch_train_loss=4.712513275293906
Epoch 34
Epoch 34 :: Batch 0/1
Batch Loss = 4.709370743204591
34, epoch_train_loss=4.709370743204591
Epoch 35
Epoch 35 :: Batch 0/1
Batch Loss = 4.701593312332152
35, epoch_train_loss=4.701593312332152
Epoch 36
Epoch 36 :: Batch 0/1
Batch Loss = 4.697139220087985
36, epoch_train_loss=4.697139220087985
Epoch 37
Epoch 37 :: Batch 0/1
Batch Loss = 4.691830451286655
37, epoch_train_loss=4.691830451286655
Epoch 38
Epoch 38 :: Batch 0/1
Batch Loss = 4.684869332205356
38, epoch_train_loss=4.684869332205356
Epoch 39
Epoch 39 :: Batch 0/1
Batch Loss = 4.682295015545981
39, epoch_train_loss=4.682295015545981
Epoch 40
Epoch 40 :: Batch 0/1
Batch Loss = 4.676177567518402
40, epoch_train_loss=4.676177567518402
Epoch 41
Epoch 41 :: Batch 0/1
Batch Loss = 4.6729896460553535
41, epoch_train_loss=4.6729896460553535
Epoch 42
Epoch 42 :: Batch 0/1
Batch Loss = 4.66545385536515
42, epoch_train_loss=4.66545385536515
Epoch 43
Epoch 43 :: Batch 0/1
Batch Loss = 400.89839694540206
43, epoch_train_loss=400.89839694540206
Epoch 44
Epoch 44 :: Batch 0/1
Batch Loss = 4.683487253900072
44, epoch_train_loss=4.683487253900072
Epoch 45
Epoch 45 :: Batch 0/1
Batch Loss = 4.691822317199304
45, epoch_train_loss=4.691822317199304
Epoch 46
Epoch 46 :: Batch 0/1
Batch Loss = 4.69116320609
46, epoch_train_loss=4.69116320609
Epoch 47
Epoch 47 :: Batch 0/1
Batch Loss = 4.688645158884864
47, epoch_train_loss=4.688645158884864
Epoch 48
Epoch 48 :: Batch 0/1
Batch Loss = 4.685625130220342
48, epoch_train_loss=4.685625130220342
Epoch 49
Epoch 49 :: Batch 0/1
Batch Loss = 4.682452340049806
49, epoch_train_loss=4.682452340049806
Epoch 50
Epoch 50 :: Batch 0/1
Batch Loss = 4.679242109174091
50, epoch_train_loss=4.679242109174091
Epoch 51
Epoch 51 :: Batch 0/1
Batch Loss = 4.6760370907223185
51, epoch_train_loss=4.6760370907223185
Epoch 52
Epoch 52 :: Batch 0/1
Batch Loss = 4.672853878420872
52, epoch_train_loss=4.672853878420872
Epoch 53
Epoch 53 :: Batch 0/1
Batch Loss = 4.669698848279109
53, epoch_train_loss=4.669698848279109
Epoch 54
Epoch 54 :: Batch 0/1
Batch Loss = 4.66657419035305
54, epoch_train_loss=4.66657419035305
Epoch 55
Epoch 55 :: Batch 0/1
Batch Loss = 4.663480264481601
55, epoch_train_loss=4.663480264481601
Epoch 56
Epoch 56 :: Batch 0/1
Batch Loss = 4.660416627249632
56, epoch_train_loss=4.660416627249632
Epoch 57
Epoch 57 :: Batch 0/1
Batch Loss = 4.6573825079198015
57, epoch_train_loss=4.6573825079198015
Epoch 58
Epoch 58 :: Batch 0/1
Batch Loss = 4.654377035996442
58, epoch_train_loss=4.654377035996442
Epoch 59
Epoch 59 :: Batch 0/1
Batch Loss = 4.651399348544531
59, epoch_train_loss=4.651399348544531
Epoch 60
Epoch 60 :: Batch 0/1
Batch Loss = 4.6484486415001305
60, epoch_train_loss=4.6484486415001305
Epoch 61
Epoch 61 :: Batch 0/1
Batch Loss = 4.645524190327415
61, epoch_train_loss=4.645524190327415
Epoch 62
Epoch 62 :: Batch 0/1
Batch Loss = 4.642625357165707
62, epoch_train_loss=4.642625357165707
Epoch 63
Epoch 63 :: Batch 0/1
Batch Loss = 4.639751588370885
63, epoch_train_loss=4.639751588370885
Epoch 64
Epoch 64 :: Batch 0/1
Batch Loss = 4.636902410646138
64, epoch_train_loss=4.636902410646138
Epoch 65
Epoch 65 :: Batch 0/1
Batch Loss = 4.634077424282984
65, epoch_train_loss=4.634077424282984
Epoch 66
Epoch 66 :: Batch 0/1
Batch Loss = 4.631276296926799
66, epoch_train_loss=4.631276296926799
Epoch 67
Epoch 67 :: Batch 0/1
Batch Loss = 4.628498756081895
67, epoch_train_loss=4.628498756081895
Epoch 68
Epoch 68 :: Batch 0/1
Batch Loss = 4.625744582750696
68, epoch_train_loss=4.625744582750696
Epoch 69
Epoch 69 :: Batch 0/1
Batch Loss = 4.623013605274793
69, epoch_train_loss=4.623013605274793
Epoch 70
Epoch 70 :: Batch 0/1
Batch Loss = 4.620305693441498
70, epoch_train_loss=4.620305693441498
Epoch 71
Epoch 71 :: Batch 0/1
Batch Loss = 4.617620753874326
71, epoch_train_loss=4.617620753874326
Epoch 72
Epoch 72 :: Batch 0/1
Batch Loss = 4.61495872466934
72, epoch_train_loss=4.61495872466934
Epoch 73
Epoch 73 :: Batch 0/1
Batch Loss = 4.612319571088243
73, epoch_train_loss=4.612319571088243
Epoch 74
Epoch 74 :: Batch 0/1
Batch Loss = 4.609703282243602
74, epoch_train_loss=4.609703282243602
Epoch 75
Epoch 75 :: Batch 0/1
Batch Loss = 4.607109866975548
75, epoch_train_loss=4.607109866975548
Epoch 76
Epoch 76 :: Batch 0/1
Batch Loss = 4.604539350678355
76, epoch_train_loss=4.604539350678355
Epoch 77
Epoch 77 :: Batch 0/1
Batch Loss = 4.60199177282303
77, epoch_train_loss=4.60199177282303
Epoch 78
Epoch 78 :: Batch 0/1
Batch Loss = 4.59946718437408
78, epoch_train_loss=4.59946718437408
Epoch 79
Epoch 79 :: Batch 0/1
Batch Loss = 4.596965645097942
79, epoch_train_loss=4.596965645097942
Epoch 80
Epoch 80 :: Batch 0/1
Batch Loss = 4.594487221499701
80, epoch_train_loss=4.594487221499701
Epoch 81
Epoch 81 :: Batch 0/1
Batch Loss = 4.592031985525195
81, epoch_train_loss=4.592031985525195
Epoch 82
Epoch 82 :: Batch 0/1
Batch Loss = 4.589600012165606
82, epoch_train_loss=4.589600012165606
Epoch 83
Epoch 83 :: Batch 0/1
Batch Loss = 4.58719137871426
83, epoch_train_loss=4.58719137871426
Epoch 84
Epoch 84 :: Batch 0/1
Batch Loss = 4.584806162727892
84, epoch_train_loss=4.584806162727892
Epoch 85
Epoch 85 :: Batch 0/1
Batch Loss = 4.582444441405293
85, epoch_train_loss=4.582444441405293
Epoch 86
Epoch 86 :: Batch 0/1
Batch Loss = 4.5801062903746725
86, epoch_train_loss=4.5801062903746725
Epoch 87
Epoch 87 :: Batch 0/1
Batch Loss = 4.577791782957005
87, epoch_train_loss=4.577791782957005
Epoch 88
Epoch 88 :: Batch 0/1
Batch Loss = 4.575500988848555
88, epoch_train_loss=4.575500988848555
Epoch 89
Epoch 89 :: Batch 0/1
Batch Loss = 4.573233973945361
89, epoch_train_loss=4.573233973945361
Epoch 90
Epoch 90 :: Batch 0/1
Batch Loss = 4.570990799727533
90, epoch_train_loss=4.570990799727533
Epoch 91
Epoch 91 :: Batch 0/1
Batch Loss = 4.568771522230804
91, epoch_train_loss=4.568771522230804
Epoch 92
Epoch 92 :: Batch 0/1
Batch Loss = 4.566576192102611
92, epoch_train_loss=4.566576192102611
Epoch 93
Epoch 93 :: Batch 0/1
Batch Loss = 4.564404854049064
93, epoch_train_loss=4.564404854049064
Epoch 94
Epoch 94 :: Batch 0/1
Batch Loss = 4.56225754667238
94, epoch_train_loss=4.56225754667238
Epoch 95
Epoch 95 :: Batch 0/1
Batch Loss = 4.560134301721767
95, epoch_train_loss=4.560134301721767
Epoch 96
Epoch 96 :: Batch 0/1
Batch Loss = 4.55803514436145
96, epoch_train_loss=4.55803514436145
Epoch 97
Epoch 97 :: Batch 0/1
Batch Loss = 4.555960092835617
97, epoch_train_loss=4.555960092835617
Epoch 98
Epoch 98 :: Batch 0/1
Batch Loss = 4.553909158336713
98, epoch_train_loss=4.553909158336713
Epoch 99
Epoch 99 :: Batch 0/1
Batch Loss = 4.551882344910234
99, epoch_train_loss=4.551882344910234
Epoch 100
Epoch 100 :: Batch 0/1
Batch Loss = 4.549879649392426
100, epoch_train_loss=4.549879649392426
Epoch 101
Epoch 101 :: Batch 0/1
Batch Loss = 4.547901061377526
101, epoch_train_loss=4.547901061377526
Epoch 102
Epoch 102 :: Batch 0/1
Batch Loss = 4.545946563211524
102, epoch_train_loss=4.545946563211524
Epoch 103
Epoch 103 :: Batch 0/1
Batch Loss = 4.544016130009697
103, epoch_train_loss=4.544016130009697
Epoch 104
Epoch 104 :: Batch 0/1
Batch Loss = 4.542109729695386
104, epoch_train_loss=4.542109729695386
Epoch 105
Epoch 105 :: Batch 0/1
Batch Loss = 4.5402273229178025
105, epoch_train_loss=4.5402273229178025
Epoch 106
Epoch 106 :: Batch 0/1
Batch Loss = 4.53836886354964
106, epoch_train_loss=4.53836886354964
Epoch 107
Epoch 107 :: Batch 0/1
Batch Loss = 4.536534298350615
107, epoch_train_loss=4.536534298350615
Epoch 108
Epoch 108 :: Batch 0/1
Batch Loss = 4.5347235672133595
108, epoch_train_loss=4.5347235672133595
Epoch 109
Epoch 109 :: Batch 0/1
Batch Loss = 4.53293660314512
109, epoch_train_loss=4.53293660314512
Epoch 110
Epoch 110 :: Batch 0/1
Batch Loss = 4.531173332794246
110, epoch_train_loss=4.531173332794246
Epoch 111
Epoch 111 :: Batch 0/1
Batch Loss = 4.529433676179276
111, epoch_train_loss=4.529433676179276
Epoch 112
Epoch 112 :: Batch 0/1
Batch Loss = 4.5277175468387645
112, epoch_train_loss=4.5277175468387645
Epoch 113
Epoch 113 :: Batch 0/1
Batch Loss = 4.526024852365363
113, epoch_train_loss=4.526024852365363
Epoch 114
Epoch 114 :: Batch 0/1
Batch Loss = 4.524355494170964
114, epoch_train_loss=4.524355494170964
Epoch 115
Epoch 115 :: Batch 0/1
Batch Loss = 4.522709367776056
115, epoch_train_loss=4.522709367776056
Epoch 116
Epoch 116 :: Batch 0/1
Batch Loss = 4.521086362972524
116, epoch_train_loss=4.521086362972524
Epoch 117
Epoch 117 :: Batch 0/1
Batch Loss = 4.5194863638707545
117, epoch_train_loss=4.5194863638707545
Epoch 118
Epoch 118 :: Batch 0/1
Batch Loss = 4.51790924931092
118, epoch_train_loss=4.51790924931092
Epoch 119
Epoch 119 :: Batch 0/1
Batch Loss = 4.516354893025662
119, epoch_train_loss=4.516354893025662
Epoch 120
Epoch 120 :: Batch 0/1
Batch Loss = 4.514823163462554
120, epoch_train_loss=4.514823163462554
Epoch 121
Epoch 121 :: Batch 0/1
Batch Loss = 4.513313924421911
121, epoch_train_loss=4.513313924421911
Epoch 122
Epoch 122 :: Batch 0/1
Batch Loss = 4.511827034772371
122, epoch_train_loss=4.511827034772371
Epoch 123
Epoch 123 :: Batch 0/1
Batch Loss = 4.51036234907584
123, epoch_train_loss=4.51036234907584
Epoch 124
Epoch 124 :: Batch 0/1
Batch Loss = 4.508919717316236
124, epoch_train_loss=4.508919717316236
Epoch 125
Epoch 125 :: Batch 0/1
Batch Loss = 4.507498985510222
125, epoch_train_loss=4.507498985510222
Epoch 126
Epoch 126 :: Batch 0/1
Batch Loss = 4.506099995447593
126, epoch_train_loss=4.506099995447593
Epoch 127
Epoch 127 :: Batch 0/1
Batch Loss = 4.504722585184327
127, epoch_train_loss=4.504722585184327
Epoch 128
Epoch 128 :: Batch 0/1
Batch Loss = 4.503366589103352
128, epoch_train_loss=4.503366589103352
Epoch 129
Epoch 129 :: Batch 0/1
Batch Loss = 4.502031838181285
129, epoch_train_loss=4.502031838181285
Epoch 130
Epoch 130 :: Batch 0/1
Batch Loss = 4.500718159853525
130, epoch_train_loss=4.500718159853525
Epoch 131
Epoch 131 :: Batch 0/1
Batch Loss = 4.499425378480874
131, epoch_train_loss=4.499425378480874
Epoch 132
Epoch 132 :: Batch 0/1
Batch Loss = 4.498153315408967
132, epoch_train_loss=4.498153315408967
Epoch 133
Epoch 133 :: Batch 0/1
Batch Loss = 4.496901789127217
133, epoch_train_loss=4.496901789127217
Epoch 134
Epoch 134 :: Batch 0/1
Batch Loss = 4.49567061551685
134, epoch_train_loss=4.49567061551685
Epoch 135
Epoch 135 :: Batch 0/1
Batch Loss = 4.494459607727495
135, epoch_train_loss=4.494459607727495
Epoch 136
Epoch 136 :: Batch 0/1
Batch Loss = 4.493268576608118
136, epoch_train_loss=4.493268576608118
Epoch 137
Epoch 137 :: Batch 0/1
Batch Loss = 4.492097330761357
137, epoch_train_loss=4.492097330761357
Epoch 138
Epoch 138 :: Batch 0/1
Batch Loss = 4.49094567668905
138, epoch_train_loss=4.49094567668905
Epoch 139
Epoch 139 :: Batch 0/1
Batch Loss = 4.489813418934865
139, epoch_train_loss=4.489813418934865
Epoch 140
Epoch 140 :: Batch 0/1
Batch Loss = 4.488700360223965
140, epoch_train_loss=4.488700360223965
Epoch 141
Epoch 141 :: Batch 0/1
Batch Loss = 4.4876063015996435
141, epoch_train_loss=4.4876063015996435
Epoch 142
Epoch 142 :: Batch 0/1
Batch Loss = 4.48653104255695
142, epoch_train_loss=4.48653104255695
Epoch 143
Epoch 143 :: Batch 0/1
Batch Loss = 4.485474381173226
143, epoch_train_loss=4.485474381173226
Epoch 144
Epoch 144 :: Batch 0/1
Batch Loss = 4.484436114235552
144, epoch_train_loss=4.484436114235552
Epoch 145
Epoch 145 :: Batch 0/1
Batch Loss = 4.483416037289216
145, epoch_train_loss=4.483416037289216
Epoch 146
Epoch 146 :: Batch 0/1
Batch Loss = 4.482413945063419
146, epoch_train_loss=4.482413945063419
Epoch 147
Epoch 147 :: Batch 0/1
Batch Loss = 4.481429631058183
147, epoch_train_loss=4.481429631058183
Epoch 148
Epoch 148 :: Batch 0/1
Batch Loss = 4.480462888187401
148, epoch_train_loss=4.480462888187401
Epoch 149
Epoch 149 :: Batch 0/1
Batch Loss = 4.479513508590012
149, epoch_train_loss=4.479513508590012
Epoch 150
Epoch 150 :: Batch 0/1
Batch Loss = 4.478581283814207
150, epoch_train_loss=4.478581283814207
Epoch 151
Epoch 151 :: Batch 0/1
Batch Loss = 4.477666004855102
151, epoch_train_loss=4.477666004855102
Epoch 152
Epoch 152 :: Batch 0/1
Batch Loss = 4.4767674624638305
152, epoch_train_loss=4.4767674624638305
Epoch 153
Epoch 153 :: Batch 0/1
Batch Loss = 4.475885447038781
153, epoch_train_loss=4.475885447038781
Epoch 154
Epoch 154 :: Batch 0/1
Batch Loss = 4.475019748727991
154, epoch_train_loss=4.475019748727991
Epoch 155
Epoch 155 :: Batch 0/1
Batch Loss = 4.47417015771806
155, epoch_train_loss=4.47417015771806
Epoch 156
Epoch 156 :: Batch 0/1
Batch Loss = 4.47333646412763
156, epoch_train_loss=4.47333646412763
Epoch 157
Epoch 157 :: Batch 0/1
Batch Loss = 4.472518458100666
157, epoch_train_loss=4.472518458100666
Epoch 158
Epoch 158 :: Batch 0/1
Batch Loss = 4.471715930075818
158, epoch_train_loss=4.471715930075818
Epoch 159
Epoch 159 :: Batch 0/1
Batch Loss = 4.4709286706819595
159, epoch_train_loss=4.4709286706819595
Epoch 160
Epoch 160 :: Batch 0/1
Batch Loss = 4.470156470822563
160, epoch_train_loss=4.470156470822563
Epoch 161
Epoch 161 :: Batch 0/1
Batch Loss = 4.469399121926165
161, epoch_train_loss=4.469399121926165
Epoch 162
Epoch 162 :: Batch 0/1
Batch Loss = 4.468656415788482
162, epoch_train_loss=4.468656415788482
Epoch 163
Epoch 163 :: Batch 0/1
Batch Loss = 4.467928144815687
163, epoch_train_loss=4.467928144815687
Epoch 164
Epoch 164 :: Batch 0/1
Batch Loss = 4.467214102087251
164, epoch_train_loss=4.467214102087251
Epoch 165
Epoch 165 :: Batch 0/1
Batch Loss = 4.466514081259858
165, epoch_train_loss=4.466514081259858
Epoch 166
Epoch 166 :: Batch 0/1
Batch Loss = 4.465827876843641
166, epoch_train_loss=4.465827876843641
Epoch 167
Epoch 167 :: Batch 0/1
Batch Loss = 4.465155284052039
167, epoch_train_loss=4.465155284052039
Epoch 168
Epoch 168 :: Batch 0/1
Batch Loss = 4.464496099015171
168, epoch_train_loss=4.464496099015171
Epoch 169
Epoch 169 :: Batch 0/1
Batch Loss = 4.4638501187812425
169, epoch_train_loss=4.4638501187812425
Epoch 170
Epoch 170 :: Batch 0/1
Batch Loss = 4.463217141367929
170, epoch_train_loss=4.463217141367929
Epoch 171
Epoch 171 :: Batch 0/1
Batch Loss = 4.46259696585721
171, epoch_train_loss=4.46259696585721
Epoch 172
Epoch 172 :: Batch 0/1
Batch Loss = 4.461989392301248
172, epoch_train_loss=4.461989392301248
Epoch 173
Epoch 173 :: Batch 0/1
Batch Loss = 4.461394221951459
173, epoch_train_loss=4.461394221951459
Epoch 174
Epoch 174 :: Batch 0/1
Batch Loss = 4.460811257073428
174, epoch_train_loss=4.460811257073428
Epoch 175
Epoch 175 :: Batch 0/1
Batch Loss = 4.460240301294615
175, epoch_train_loss=4.460240301294615
Epoch 176
Epoch 176 :: Batch 0/1
Batch Loss = 4.45968115933135
176, epoch_train_loss=4.45968115933135
Epoch 177
Epoch 177 :: Batch 0/1
Batch Loss = 4.459133637196088
177, epoch_train_loss=4.459133637196088
Epoch 178
Epoch 178 :: Batch 0/1
Batch Loss = 4.4585975421826625
178, epoch_train_loss=4.4585975421826625
Epoch 179
Epoch 179 :: Batch 0/1
Batch Loss = 4.458072682892553
179, epoch_train_loss=4.458072682892553
Epoch 180
Epoch 180 :: Batch 0/1
Batch Loss = 4.457558869258286
180, epoch_train_loss=4.457558869258286
Epoch 181
Epoch 181 :: Batch 0/1
Batch Loss = 4.457055912563865
181, epoch_train_loss=4.457055912563865
Epoch 182
Epoch 182 :: Batch 0/1
Batch Loss = 4.456563625462233
182, epoch_train_loss=4.456563625462233
Epoch 183
Epoch 183 :: Batch 0/1
Batch Loss = 4.456081821989771
183, epoch_train_loss=4.456081821989771
Epoch 184
Epoch 184 :: Batch 0/1
Batch Loss = 4.455610317577759
184, epoch_train_loss=4.455610317577759
Epoch 185
Epoch 185 :: Batch 0/1
Batch Loss = 4.45514892906087
185, epoch_train_loss=4.45514892906087
Epoch 186
Epoch 186 :: Batch 0/1
Batch Loss = 4.454697474649044
186, epoch_train_loss=4.454697474649044
Epoch 187
Epoch 187 :: Batch 0/1
Batch Loss = 4.454255774064982
187, epoch_train_loss=4.454255774064982
Epoch 188
Epoch 188 :: Batch 0/1
Batch Loss = 4.453823648308607
188, epoch_train_loss=4.453823648308607
Epoch 189
Epoch 189 :: Batch 0/1
Batch Loss = 4.453400919887492
189, epoch_train_loss=4.453400919887492
Epoch 190
Epoch 190 :: Batch 0/1
Batch Loss = 4.452987412647591
190, epoch_train_loss=4.452987412647591
Epoch 191
Epoch 191 :: Batch 0/1
Batch Loss = 4.452582951892066
191, epoch_train_loss=4.452582951892066
Epoch 192
Epoch 192 :: Batch 0/1
Batch Loss = 4.452187364276713
192, epoch_train_loss=4.452187364276713
Epoch 193
Epoch 193 :: Batch 0/1
Batch Loss = 4.451800477830184
193, epoch_train_loss=4.451800477830184
Epoch 194
Epoch 194 :: Batch 0/1
Batch Loss = 4.451422121941133
194, epoch_train_loss=4.451422121941133
Epoch 195
Epoch 195 :: Batch 0/1
Batch Loss = 4.451052127317193
195, epoch_train_loss=4.451052127317193
Epoch 196
Epoch 196 :: Batch 0/1
Batch Loss = 4.450690326055162
196, epoch_train_loss=4.450690326055162
Epoch 197
Epoch 197 :: Batch 0/1
Batch Loss = 4.450336551517235
197, epoch_train_loss=4.450336551517235
Epoch 198
Epoch 198 :: Batch 0/1
Batch Loss = 4.449990638428177
198, epoch_train_loss=4.449990638428177
Epoch 199
Epoch 199 :: Batch 0/1
Batch Loss = 4.44965242278506
199, epoch_train_loss=4.44965242278506
Epoch 200
Epoch 200 :: Batch 0/1
Batch Loss = 4.449321741853111
200, epoch_train_loss=4.449321741853111
Epoch 201
Epoch 201 :: Batch 0/1
Batch Loss = 4.448998434236416
201, epoch_train_loss=4.448998434236416
Epoch 202
Epoch 202 :: Batch 0/1
Batch Loss = 4.448682339776929
202, epoch_train_loss=4.448682339776929
Epoch 203
Epoch 203 :: Batch 0/1
Batch Loss = 4.448373299628092
203, epoch_train_loss=4.448373299628092
Epoch 204
Epoch 204 :: Batch 0/1
Batch Loss = 4.448071156251457
204, epoch_train_loss=4.448071156251457
Epoch 205
Epoch 205 :: Batch 0/1
Batch Loss = 4.447775753343916
205, epoch_train_loss=4.447775753343916
Epoch 206
Epoch 206 :: Batch 0/1
Batch Loss = 4.447486935895094
206, epoch_train_loss=4.447486935895094
Epoch 207
Epoch 207 :: Batch 0/1
Batch Loss = 4.447204550161415
207, epoch_train_loss=4.447204550161415
Epoch 208
Epoch 208 :: Batch 0/1
Batch Loss = 4.446928443562451
208, epoch_train_loss=4.446928443562451
Epoch 209
Epoch 209 :: Batch 0/1
Batch Loss = 4.446658464687463
209, epoch_train_loss=4.446658464687463
Epoch 210
Epoch 210 :: Batch 0/1
Batch Loss = 4.446394463219175
210, epoch_train_loss=4.446394463219175
Epoch 211
Epoch 211 :: Batch 0/1
Batch Loss = 4.4461362897946675
211, epoch_train_loss=4.4461362897946675
Epoch 212
Epoch 212 :: Batch 0/1
Batch Loss = 4.445883796004712
212, epoch_train_loss=4.445883796004712
Epoch 213
Epoch 213 :: Batch 0/1
Batch Loss = 4.445636834389424
213, epoch_train_loss=4.445636834389424
Epoch 214
Epoch 214 :: Batch 0/1
Batch Loss = 4.445395258497372
214, epoch_train_loss=4.445395258497372
Epoch 215
Epoch 215 :: Batch 0/1
Batch Loss = 4.445158923227805
215, epoch_train_loss=4.445158923227805
Epoch 216
Epoch 216 :: Batch 0/1
Batch Loss = 4.444927685437433
216, epoch_train_loss=4.444927685437433
Epoch 217
Epoch 217 :: Batch 0/1
Batch Loss = 4.4447014046557545
217, epoch_train_loss=4.4447014046557545
Epoch 218
Epoch 218 :: Batch 0/1
Batch Loss = 4.444479944142862
218, epoch_train_loss=4.444479944142862
Epoch 219
Epoch 219 :: Batch 0/1
Batch Loss = 4.444263171717993
219, epoch_train_loss=4.444263171717993
Epoch 220
Epoch 220 :: Batch 0/1
Batch Loss = 4.444050959886836
220, epoch_train_loss=4.444050959886836
Epoch 221
Epoch 221 :: Batch 0/1
Batch Loss = 4.443843184424148
221, epoch_train_loss=4.443843184424148
Epoch 222
Epoch 222 :: Batch 0/1
Batch Loss = 4.443639720583453
222, epoch_train_loss=4.443639720583453
Epoch 223
Epoch 223 :: Batch 0/1
Batch Loss = 4.443440436658681
223, epoch_train_loss=4.443440436658681
Epoch 224
Epoch 224 :: Batch 0/1
Batch Loss = 4.4432451859523345
224, epoch_train_loss=4.4432451859523345
Epoch 225
Epoch 225 :: Batch 0/1
Batch Loss = 4.443053800070992
225, epoch_train_loss=4.443053800070992
Epoch 226
Epoch 226 :: Batch 0/1
Batch Loss = 4.442866087831968
226, epoch_train_loss=4.442866087831968
Epoch 227
Epoch 227 :: Batch 0/1
Batch Loss = 4.442681843304963
227, epoch_train_loss=4.442681843304963
Epoch 228
Epoch 228 :: Batch 0/1
Batch Loss = 4.442500862789821
228, epoch_train_loss=4.442500862789821
Epoch 229
Epoch 229 :: Batch 0/1
Batch Loss = 4.442322964755201
229, epoch_train_loss=4.442322964755201
Epoch 230
Epoch 230 :: Batch 0/1
Batch Loss = 4.442148003387589
230, epoch_train_loss=4.442148003387589
Epoch 231
Epoch 231 :: Batch 0/1
Batch Loss = 4.441975867978488
231, epoch_train_loss=4.441975867978488
Epoch 232
Epoch 232 :: Batch 0/1
Batch Loss = 4.44180646757217
232, epoch_train_loss=4.44180646757217
Epoch 233
Epoch 233 :: Batch 0/1
Batch Loss = 4.441639708280145
233, epoch_train_loss=4.441639708280145
Epoch 234
Epoch 234 :: Batch 0/1
Batch Loss = 4.441475473721809
234, epoch_train_loss=4.441475473721809
Epoch 235
Epoch 235 :: Batch 0/1
Batch Loss = 4.44131361615577
235, epoch_train_loss=4.44131361615577
Epoch 236
Epoch 236 :: Batch 0/1
Batch Loss = 4.441153959292187
236, epoch_train_loss=4.441153959292187
Epoch 237
Epoch 237 :: Batch 0/1
Batch Loss = 4.440996308883688
237, epoch_train_loss=4.440996308883688
Epoch 238
Epoch 238 :: Batch 0/1
Batch Loss = 4.440840465911558
238, epoch_train_loss=4.440840465911558
Epoch 239
Epoch 239 :: Batch 0/1
Batch Loss = 4.440686238463662
239, epoch_train_loss=4.440686238463662
Epoch 240
Epoch 240 :: Batch 0/1
Batch Loss = 4.440533450516759
240, epoch_train_loss=4.440533450516759
Epoch 241
Epoch 241 :: Batch 0/1
Batch Loss = 4.440381947416707
241, epoch_train_loss=4.440381947416707
Epoch 242
Epoch 242 :: Batch 0/1
Batch Loss = 4.440231598265224
242, epoch_train_loss=4.440231598265224
Epoch 243
Epoch 243 :: Batch 0/1
Batch Loss = 4.440082296055193
243, epoch_train_loss=4.440082296055193
Epoch 244
Epoch 244 :: Batch 0/1
Batch Loss = 4.439933956679308
244, epoch_train_loss=4.439933956679308
Epoch 245
Epoch 245 :: Batch 0/1
Batch Loss = 4.4397865182864065
245, epoch_train_loss=4.4397865182864065
Epoch 246
Epoch 246 :: Batch 0/1
Batch Loss = 4.43963994287609
246, epoch_train_loss=4.43963994287609
Epoch 247
Epoch 247 :: Batch 0/1
Batch Loss = 4.439494221113482
247, epoch_train_loss=4.439494221113482
Epoch 248
Epoch 248 :: Batch 0/1
Batch Loss = 4.439349483007543
248, epoch_train_loss=4.439349483007543
Epoch 249
Epoch 249 :: Batch 0/1
Batch Loss = 4.4392052746697015
249, epoch_train_loss=4.4392052746697015
Epoch 250
Epoch 250 :: Batch 0/1
Batch Loss = 4.439063698448056
250, epoch_train_loss=4.439063698448056
Epoch 251
Epoch 251 :: Batch 0/1
Batch Loss = 4.438910450426525
251, epoch_train_loss=4.438910450426525
Epoch 252
Epoch 252 :: Batch 0/1
Batch Loss = 4.438784000941762
252, epoch_train_loss=4.438784000941762
Epoch 253
Epoch 253 :: Batch 0/1
Batch Loss = 4.4386388626038125
253, epoch_train_loss=4.4386388626038125
Epoch 254
Epoch 254 :: Batch 0/1
Batch Loss = 4.4385155877439715
254, epoch_train_loss=4.4385155877439715
Epoch 255
Epoch 255 :: Batch 0/1
Batch Loss = 4.438373041515556
255, epoch_train_loss=4.438373041515556
Epoch 256
Epoch 256 :: Batch 0/1
Batch Loss = 4.438247177053621
256, epoch_train_loss=4.438247177053621
Epoch 257
Epoch 257 :: Batch 0/1
Batch Loss = 4.438115685502971
257, epoch_train_loss=4.438115685502971
Epoch 258
Epoch 258 :: Batch 0/1
Batch Loss = 4.4379916103136665
258, epoch_train_loss=4.4379916103136665
Epoch 259
Epoch 259 :: Batch 0/1
Batch Loss = 4.437872047583913
259, epoch_train_loss=4.437872047583913
Epoch 260
Epoch 260 :: Batch 0/1
Batch Loss = 4.437745632373382
260, epoch_train_loss=4.437745632373382
Epoch 261
Epoch 261 :: Batch 0/1
Batch Loss = 4.437637479861076
261, epoch_train_loss=4.437637479861076
Epoch 262
Epoch 262 :: Batch 0/1
Batch Loss = 4.437512232374354
262, epoch_train_loss=4.437512232374354
Epoch 263
Epoch 263 :: Batch 0/1
Batch Loss = 4.437408139503713
263, epoch_train_loss=4.437408139503713
Epoch 264
Epoch 264 :: Batch 0/1
Batch Loss = 4.437287960260676
264, epoch_train_loss=4.437287960260676
Epoch 265
Epoch 265 :: Batch 0/1
Batch Loss = 4.437183905262787
265, epoch_train_loss=4.437183905262787
Epoch 266
Epoch 266 :: Batch 0/1
Batch Loss = 4.437070766043373
266, epoch_train_loss=4.437070766043373
Epoch 267
Epoch 267 :: Batch 0/1
Batch Loss = 4.4369659573630145
267, epoch_train_loss=4.4369659573630145
Epoch 268
Epoch 268 :: Batch 0/1
Batch Loss = 4.436859421399644
268, epoch_train_loss=4.436859421399644
Epoch 269
Epoch 269 :: Batch 0/1
Batch Loss = 4.436753573816491
269, epoch_train_loss=4.436753573816491
Epoch 270
Epoch 270 :: Batch 0/1
Batch Loss = 4.436651697828218
270, epoch_train_loss=4.436651697828218
Epoch 271
Epoch 271 :: Batch 0/1
Batch Loss = 4.436545929898802
271, epoch_train_loss=4.436545929898802
Epoch 272
Epoch 272 :: Batch 0/1
Batch Loss = 4.436446183592307
272, epoch_train_loss=4.436446183592307
Epoch 273
Epoch 273 :: Batch 0/1
Batch Loss = 4.436341317569456
273, epoch_train_loss=4.436341317569456
Epoch 274
Epoch 274 :: Batch 0/1
Batch Loss = 4.436242533791655
274, epoch_train_loss=4.436242533791655
Epoch 275
Epoch 275 :: Batch 0/1
Batch Loss = 4.436138441328772
275, epoch_train_loss=4.436138441328772
Epoch 276
Epoch 276 :: Batch 0/1
Batch Loss = 4.436039350707352
276, epoch_train_loss=4.436039350707352
Epoch 277
Epoch 277 :: Batch 0/1
Batch Loss = 4.43593640339637
277, epoch_train_loss=4.43593640339637
Epoch 278
Epoch 278 :: Batch 0/1
Batch Loss = 4.435836340618204
278, epoch_train_loss=4.435836340618204
Epoch 279
Epoch 279 :: Batch 0/1
Batch Loss = 4.435734843403367
279, epoch_train_loss=4.435734843403367
Epoch 280
Epoch 280 :: Batch 0/1
Batch Loss = 4.435632616757823
280, epoch_train_loss=4.435632616757823
Epoch 281
Epoch 281 :: Batch 0/1
Batch Loss = 4.435532808600994
281, epoch_train_loss=4.435532808600994
Epoch 282
Epoch 282 :: Batch 0/1
Batch Loss = 4.435428602086908
282, epoch_train_loss=4.435428602086908
Epoch 283
Epoch 283 :: Batch 0/1
Batch Loss = 4.435327694415846
283, epoch_train_loss=4.435327694415846
Epoch 284
Epoch 284 :: Batch 0/1
Batch Loss = 4.4352251720162235
284, epoch_train_loss=4.4352251720162235
Epoch 285
Epoch 285 :: Batch 0/1
Batch Loss = 4.435117834627955
285, epoch_train_loss=4.435117834627955
Epoch 286
Epoch 286 :: Batch 0/1
Batch Loss = 4.435009942647144
286, epoch_train_loss=4.435009942647144
Epoch 287
Epoch 287 :: Batch 0/1
Batch Loss = 4.43490211827096
287, epoch_train_loss=4.43490211827096
Epoch 288
Epoch 288 :: Batch 0/1
Batch Loss = 4.434791337233564
288, epoch_train_loss=4.434791337233564
Epoch 289
Epoch 289 :: Batch 0/1
Batch Loss = 4.4346742502936465
289, epoch_train_loss=4.4346742502936465
Epoch 290
Epoch 290 :: Batch 0/1
Batch Loss = 4.43455389451239
290, epoch_train_loss=4.43455389451239
Epoch 291
Epoch 291 :: Batch 0/1
Batch Loss = 4.434426856576574
291, epoch_train_loss=4.434426856576574
Epoch 292
Epoch 292 :: Batch 0/1
Batch Loss = 4.434296149748185
292, epoch_train_loss=4.434296149748185
Epoch 293
Epoch 293 :: Batch 0/1
Batch Loss = 4.434157911534018
293, epoch_train_loss=4.434157911534018
Epoch 294
Epoch 294 :: Batch 0/1
Batch Loss = 4.434010470069025
294, epoch_train_loss=4.434010470069025
Epoch 295
Epoch 295 :: Batch 0/1
Batch Loss = 4.433853165335989
295, epoch_train_loss=4.433853165335989
Epoch 296
Epoch 296 :: Batch 0/1
Batch Loss = 4.43367634767434
296, epoch_train_loss=4.43367634767434
Epoch 297
Epoch 297 :: Batch 0/1
Batch Loss = 4.4334922418989
297, epoch_train_loss=4.4334922418989
Epoch 298
Epoch 298 :: Batch 0/1
Batch Loss = 4.433298132448219
298, epoch_train_loss=4.433298132448219
Epoch 299
Epoch 299 :: Batch 0/1
Batch Loss = 4.433137517368497
299, epoch_train_loss=4.433137517368497
Epoch 300
Epoch 300 :: Batch 0/1
Batch Loss = 4.432901007975127
300, epoch_train_loss=4.432901007975127
Epoch 301
Epoch 301 :: Batch 0/1
Batch Loss = 4.43269953641908
301, epoch_train_loss=4.43269953641908
Epoch 302
Epoch 302 :: Batch 0/1
Batch Loss = 4.4324731633250165
302, epoch_train_loss=4.4324731633250165
Epoch 303
Epoch 303 :: Batch 0/1
Batch Loss = 4.432223055328396
303, epoch_train_loss=4.432223055328396
Epoch 304
Epoch 304 :: Batch 0/1
Batch Loss = 4.431955376663027
304, epoch_train_loss=4.431955376663027
Epoch 305
Epoch 305 :: Batch 0/1
Batch Loss = 4.4316793556458975
305, epoch_train_loss=4.4316793556458975
Epoch 306
Epoch 306 :: Batch 0/1
Batch Loss = 4.431405990565526
306, epoch_train_loss=4.431405990565526
Epoch 307
Epoch 307 :: Batch 0/1
Batch Loss = 4.431144290040514
307, epoch_train_loss=4.431144290040514
Epoch 308
Epoch 308 :: Batch 0/1
Batch Loss = 4.430893826369256
308, epoch_train_loss=4.430893826369256
Epoch 309
Epoch 309 :: Batch 0/1
Batch Loss = 4.430650107892807
309, epoch_train_loss=4.430650107892807
Epoch 310
Epoch 310 :: Batch 0/1
Batch Loss = 4.430431518175165
310, epoch_train_loss=4.430431518175165
Epoch 311
Epoch 311 :: Batch 0/1
Batch Loss = 4.430232489761166
311, epoch_train_loss=4.430232489761166
Epoch 312
Epoch 312 :: Batch 0/1
Batch Loss = 4.4299805003333015
312, epoch_train_loss=4.4299805003333015
Epoch 313
Epoch 313 :: Batch 0/1
Batch Loss = 4.429663262876607
313, epoch_train_loss=4.429663262876607
Epoch 314
Epoch 314 :: Batch 0/1
Batch Loss = 4.429288717274844
314, epoch_train_loss=4.429288717274844
Epoch 315
Epoch 315 :: Batch 0/1
Batch Loss = 4.428854259461915
315, epoch_train_loss=4.428854259461915
Epoch 316
Epoch 316 :: Batch 0/1
Batch Loss = 4.4284009541672305
316, epoch_train_loss=4.4284009541672305
Epoch 317
Epoch 317 :: Batch 0/1
Batch Loss = 4.427959218257796
317, epoch_train_loss=4.427959218257796
Epoch 318
Epoch 318 :: Batch 0/1
Batch Loss = 4.427492100758616
318, epoch_train_loss=4.427492100758616
Epoch 319
Epoch 319 :: Batch 0/1
Batch Loss = 4.4270102996418075
319, epoch_train_loss=4.4270102996418075
Epoch 320
Epoch 320 :: Batch 0/1
Batch Loss = 4.426457590063054
320, epoch_train_loss=4.426457590063054
Epoch 321
Epoch 321 :: Batch 0/1
Batch Loss = 4.425787037658988
321, epoch_train_loss=4.425787037658988
Epoch 322
Epoch 322 :: Batch 0/1
Batch Loss = 20.928004518204375
322, epoch_train_loss=20.928004518204375
Epoch 323
Epoch 323 :: Batch 0/1
Batch Loss = 4.4270418960989115
323, epoch_train_loss=4.4270418960989115
Epoch 324
Epoch 324 :: Batch 0/1
Batch Loss = 4.42567431078055
324, epoch_train_loss=4.42567431078055
Epoch 325
Epoch 325 :: Batch 0/1
Batch Loss = 4.425859953179911
325, epoch_train_loss=4.425859953179911
Epoch 326
Epoch 326 :: Batch 0/1
Batch Loss = 4.424780595749624
326, epoch_train_loss=4.424780595749624
Epoch 327
Epoch 327 :: Batch 0/1
Batch Loss = 4.421934498709171
327, epoch_train_loss=4.421934498709171
Epoch 328
Epoch 328 :: Batch 0/1
Batch Loss = 4.417568339605395
328, epoch_train_loss=4.417568339605395
Epoch 329
Epoch 329 :: Batch 0/1
Batch Loss = 4.415706969377575
329, epoch_train_loss=4.415706969377575
Epoch 330
Epoch 330 :: Batch 0/1
Batch Loss = 4.421974545363781
330, epoch_train_loss=4.421974545363781
Epoch 331
Epoch 331 :: Batch 0/1
Batch Loss = 4.416493480926442
331, epoch_train_loss=4.416493480926442
Epoch 332
Epoch 332 :: Batch 0/1
Batch Loss = 4.421696179205323
332, epoch_train_loss=4.421696179205323
Epoch 333
Epoch 333 :: Batch 0/1
Batch Loss = 4.422143000241407
333, epoch_train_loss=4.422143000241407
Epoch 334
Epoch 334 :: Batch 0/1
Batch Loss = 4.418060317788332
334, epoch_train_loss=4.418060317788332
Epoch 335
Epoch 335 :: Batch 0/1
Batch Loss = 4.4184101254551535
335, epoch_train_loss=4.4184101254551535
Epoch 336
Epoch 336 :: Batch 0/1
Batch Loss = 4.4181358815307785
336, epoch_train_loss=4.4181358815307785
Epoch 337
Epoch 337 :: Batch 0/1
Batch Loss = 4.416608999796563
337, epoch_train_loss=4.416608999796563
Epoch 338
Epoch 338 :: Batch 0/1
Batch Loss = 4.418739578060846
338, epoch_train_loss=4.418739578060846
Epoch 339
Epoch 339 :: Batch 0/1
Batch Loss = 4.41695009041379
339, epoch_train_loss=4.41695009041379
Epoch 340
Epoch 340 :: Batch 0/1
Batch Loss = 4.413557207851824
340, epoch_train_loss=4.413557207851824
Epoch 341
Epoch 341 :: Batch 0/1
Batch Loss = 4.417854421881183
341, epoch_train_loss=4.417854421881183
Epoch 342
Epoch 342 :: Batch 0/1
Batch Loss = 4.413319434165096
342, epoch_train_loss=4.413319434165096
Epoch 343
Epoch 343 :: Batch 0/1
Batch Loss = 4.415582162405307
343, epoch_train_loss=4.415582162405307
Epoch 344
Epoch 344 :: Batch 0/1
Batch Loss = 4.415498352368168
344, epoch_train_loss=4.415498352368168
Epoch 345
Epoch 345 :: Batch 0/1
Batch Loss = 4.413004304925689
345, epoch_train_loss=4.413004304925689
Epoch 346
Epoch 346 :: Batch 0/1
Batch Loss = 4.4138138857481595
346, epoch_train_loss=4.4138138857481595
Epoch 347
Epoch 347 :: Batch 0/1
Batch Loss = 4.412382112105437
347, epoch_train_loss=4.412382112105437
Epoch 348
Epoch 348 :: Batch 0/1
Batch Loss = 4.411696262858261
348, epoch_train_loss=4.411696262858261
Epoch 349
Epoch 349 :: Batch 0/1
Batch Loss = 4.412099385195506
349, epoch_train_loss=4.412099385195506
Epoch 350
Epoch 350 :: Batch 0/1
Batch Loss = 4.410030886834343
350, epoch_train_loss=4.410030886834343
Epoch 351
Epoch 351 :: Batch 0/1
Batch Loss = 4.41016289509871
351, epoch_train_loss=4.41016289509871
Epoch 352
Epoch 352 :: Batch 0/1
Batch Loss = 4.408266024879288
352, epoch_train_loss=4.408266024879288
Epoch 353
Epoch 353 :: Batch 0/1
Batch Loss = 4.4080330871980165
353, epoch_train_loss=4.4080330871980165
Epoch 354
Epoch 354 :: Batch 0/1
Batch Loss = 4.407017533045034
354, epoch_train_loss=4.407017533045034
Epoch 355
Epoch 355 :: Batch 0/1
Batch Loss = 4.405756597882425
355, epoch_train_loss=4.405756597882425
Epoch 356
Epoch 356 :: Batch 0/1
Batch Loss = 4.404909314455715
356, epoch_train_loss=4.404909314455715
Epoch 357
Epoch 357 :: Batch 0/1
Batch Loss = 4.404123866405383
357, epoch_train_loss=4.404123866405383
Epoch 358
Epoch 358 :: Batch 0/1
Batch Loss = 4.4029658130888105
358, epoch_train_loss=4.4029658130888105
Epoch 359
Epoch 359 :: Batch 0/1
Batch Loss = 4.402243765873807
359, epoch_train_loss=4.402243765873807
Epoch 360
Epoch 360 :: Batch 0/1
Batch Loss = 4.400532219660672
360, epoch_train_loss=4.400532219660672
Epoch 361
Epoch 361 :: Batch 0/1
Batch Loss = 4.399987669240419
361, epoch_train_loss=4.399987669240419
Epoch 362
Epoch 362 :: Batch 0/1
Batch Loss = 4.398565177708195
362, epoch_train_loss=4.398565177708195
Epoch 363
Epoch 363 :: Batch 0/1
Batch Loss = 4.3975123102654745
363, epoch_train_loss=4.3975123102654745
Epoch 364
Epoch 364 :: Batch 0/1
Batch Loss = 4.3978641822784965
364, epoch_train_loss=4.3978641822784965
Epoch 365
Epoch 365 :: Batch 0/1
Batch Loss = 4.855563571877567
365, epoch_train_loss=4.855563571877567
Epoch 366
Epoch 366 :: Batch 0/1
Batch Loss = 4.435540499144627
366, epoch_train_loss=4.435540499144627
Epoch 367
Epoch 367 :: Batch 0/1
Batch Loss = 4.437296277437395
367, epoch_train_loss=4.437296277437395
Epoch 368
Epoch 368 :: Batch 0/1
Batch Loss = 4.4374622762690334
368, epoch_train_loss=4.4374622762690334
Epoch 369
Epoch 369 :: Batch 0/1
Batch Loss = 4.437492403192177
369, epoch_train_loss=4.437492403192177
Epoch 370
Epoch 370 :: Batch 0/1
Batch Loss = 4.437503302503435
370, epoch_train_loss=4.437503302503435
Epoch 371
Epoch 371 :: Batch 0/1
Batch Loss = 4.4375085327759125
371, epoch_train_loss=4.4375085327759125
Epoch 372
Epoch 372 :: Batch 0/1
Batch Loss = 4.43751223318477
372, epoch_train_loss=4.43751223318477
Epoch 373
Epoch 373 :: Batch 0/1
Batch Loss = 4.437515759528019
373, epoch_train_loss=4.437515759528019
Epoch 374
Epoch 374 :: Batch 0/1
Batch Loss = 4.4375196166869015
374, epoch_train_loss=4.4375196166869015
Epoch 375
Epoch 375 :: Batch 0/1
Batch Loss = 4.4375241450174014
375, epoch_train_loss=4.4375241450174014
Epoch 376
Epoch 376 :: Batch 0/1
Batch Loss = 4.437529653940997
376, epoch_train_loss=4.437529653940997
Epoch 377
Epoch 377 :: Batch 0/1
Batch Loss = 4.437536346411799
377, epoch_train_loss=4.437536346411799
Epoch 378
Epoch 378 :: Batch 0/1
Batch Loss = 4.437544094688751
378, epoch_train_loss=4.437544094688751
Epoch 379
Epoch 379 :: Batch 0/1
Batch Loss = 4.437552152669181
379, epoch_train_loss=4.437552152669181
Epoch 380
Epoch 380 :: Batch 0/1
Batch Loss = 4.437559052014162
380, epoch_train_loss=4.437559052014162
Epoch 381
Epoch 381 :: Batch 0/1
Batch Loss = 4.437562963451778
381, epoch_train_loss=4.437562963451778
Epoch 382
Epoch 382 :: Batch 0/1
Batch Loss = 4.437562558449885
382, epoch_train_loss=4.437562558449885
Epoch 383
Epoch 383 :: Batch 0/1
Batch Loss = 4.437557751987264
383, epoch_train_loss=4.437557751987264
Epoch 384
Epoch 384 :: Batch 0/1
Batch Loss = 4.437549705922859
384, epoch_train_loss=4.437549705922859
Epoch 385
Epoch 385 :: Batch 0/1
Batch Loss = 4.437540125655149
385, epoch_train_loss=4.437540125655149
Epoch 386
Epoch 386 :: Batch 0/1
Batch Loss = 4.437530464794513
386, epoch_train_loss=4.437530464794513
Epoch 387
Epoch 387 :: Batch 0/1
Batch Loss = 4.437521558856186
387, epoch_train_loss=4.437521558856186
Epoch 388
Epoch 388 :: Batch 0/1
Batch Loss = 4.437513693534635
388, epoch_train_loss=4.437513693534635
Epoch 389
Epoch 389 :: Batch 0/1
Batch Loss = 4.4375068347416855
389, epoch_train_loss=4.4375068347416855
Epoch 390
Epoch 390 :: Batch 0/1
Batch Loss = 4.437500822510164
390, epoch_train_loss=4.437500822510164
Epoch 391
Epoch 391 :: Batch 0/1
Batch Loss = 4.437495479265988
391, epoch_train_loss=4.437495479265988
Epoch 392
Epoch 392 :: Batch 0/1
Batch Loss = 4.437490653500361
392, epoch_train_loss=4.437490653500361
Epoch 393
Epoch 393 :: Batch 0/1
Batch Loss = 4.437486229341856
393, epoch_train_loss=4.437486229341856
Epoch 394
Epoch 394 :: Batch 0/1
Batch Loss = 4.437482122516377
394, epoch_train_loss=4.437482122516377
Epoch 395
Epoch 395 :: Batch 0/1
Batch Loss = 4.437478272897505
395, epoch_train_loss=4.437478272897505
Epoch 396
Epoch 396 :: Batch 0/1
Batch Loss = 4.437474637623296
396, epoch_train_loss=4.437474637623296
Epoch 397
Epoch 397 :: Batch 0/1
Batch Loss = 4.437471185821958
397, epoch_train_loss=4.437471185821958
Epoch 398
Epoch 398 :: Batch 0/1
Batch Loss = 4.437467894878108
398, epoch_train_loss=4.437467894878108
Epoch 399
Epoch 399 :: Batch 0/1
Batch Loss = 4.437464747880498
399, epoch_train_loss=4.437464747880498
Epoch 400
Epoch 400 :: Batch 0/1
Batch Loss = 4.437461731898636
400, epoch_train_loss=4.437461731898636
Epoch 401
Epoch 401 :: Batch 0/1
Batch Loss = 4.437458836816298
401, epoch_train_loss=4.437458836816298
Epoch 402
Epoch 402 :: Batch 0/1
Batch Loss = 4.437456054535627
402, epoch_train_loss=4.437456054535627
Epoch 403
Epoch 403 :: Batch 0/1
Batch Loss = 4.437453378424925
403, epoch_train_loss=4.437453378424925
Epoch 404
Epoch 404 :: Batch 0/1
Batch Loss = 4.437450802932095
404, epoch_train_loss=4.437450802932095
Epoch 405
Epoch 405 :: Batch 0/1
Batch Loss = 4.437448323308749
405, epoch_train_loss=4.437448323308749
Epoch 406
Epoch 406 :: Batch 0/1
Batch Loss = 4.43744593541073
406, epoch_train_loss=4.43744593541073
Epoch 407
Epoch 407 :: Batch 0/1
Batch Loss = 4.437443635552867
407, epoch_train_loss=4.437443635552867
Epoch 408
Epoch 408 :: Batch 0/1
Batch Loss = 4.437441420402571
408, epoch_train_loss=4.437441420402571
Epoch 409
Epoch 409 :: Batch 0/1
Batch Loss = 4.437439286899845
409, epoch_train_loss=4.437439286899845
Epoch 410
Epoch 410 :: Batch 0/1
Batch Loss = 4.437437232198936
410, epoch_train_loss=4.437437232198936
Epoch 411
Epoch 411 :: Batch 0/1
Batch Loss = 4.437435253623594
411, epoch_train_loss=4.437435253623594
Epoch 412
Epoch 412 :: Batch 0/1
Batch Loss = 4.437433348634586
412, epoch_train_loss=4.437433348634586
Epoch 413
Epoch 413 :: Batch 0/1
Batch Loss = 4.437431514804909
413, epoch_train_loss=4.437431514804909
Epoch 414
Epoch 414 :: Batch 0/1
Batch Loss = 4.437429749801429
414, epoch_train_loss=4.437429749801429
Epoch 415
Epoch 415 :: Batch 0/1
Batch Loss = 4.43742805137125
415, epoch_train_loss=4.43742805137125
Epoch 416
Epoch 416 :: Batch 0/1
Batch Loss = 4.437426417331767
416, epoch_train_loss=4.437426417331767
Epoch 417
Epoch 417 :: Batch 0/1
Batch Loss = 4.437424845562907
417, epoch_train_loss=4.437424845562907
Epoch 418
Epoch 418 :: Batch 0/1
Batch Loss = 4.437423334002148
418, epoch_train_loss=4.437423334002148
Epoch 419
Epoch 419 :: Batch 0/1
Batch Loss = 4.437421880640527
419, epoch_train_loss=4.437421880640527
Epoch 420
Epoch 420 :: Batch 0/1
Batch Loss = 4.437420483519999
420, epoch_train_loss=4.437420483519999
Epoch 421
Epoch 421 :: Batch 0/1
Batch Loss = 4.437419140731187
421, epoch_train_loss=4.437419140731187
Epoch 422
Epoch 422 :: Batch 0/1
Batch Loss = 4.437417850412375
422, epoch_train_loss=4.437417850412375
Epoch 423
Epoch 423 :: Batch 0/1
Batch Loss = 4.4374166107484765
423, epoch_train_loss=4.4374166107484765
Epoch 424
Epoch 424 :: Batch 0/1
Batch Loss = 4.437415419970388
424, epoch_train_loss=4.437415419970388
Epoch 425
Epoch 425 :: Batch 0/1
Batch Loss = 4.437414276354545
425, epoch_train_loss=4.437414276354545
Epoch 426
Epoch 426 :: Batch 0/1
Batch Loss = 4.437413178222582
426, epoch_train_loss=4.437413178222582
Epoch 427
Epoch 427 :: Batch 0/1
Batch Loss = 4.43741212394104
427, epoch_train_loss=4.43741212394104
Epoch 428
Epoch 428 :: Batch 0/1
Batch Loss = 4.437411111921103
428, epoch_train_loss=4.437411111921103
Epoch 429
Epoch 429 :: Batch 0/1
Batch Loss = 4.437410140618242
429, epoch_train_loss=4.437410140618242
Epoch 430
Epoch 430 :: Batch 0/1
Batch Loss = 4.437409208532134
430, epoch_train_loss=4.437409208532134
Epoch 431
Epoch 431 :: Batch 0/1
Batch Loss = 4.437408314206066
431, epoch_train_loss=4.437408314206066
Epoch 432
Epoch 432 :: Batch 0/1
Batch Loss = 4.437407456226615
432, epoch_train_loss=4.437407456226615
Epoch 433
Epoch 433 :: Batch 0/1
Batch Loss = 4.437406633223181
433, epoch_train_loss=4.437406633223181
Epoch 434
Epoch 434 :: Batch 0/1
Batch Loss = 4.437405843867416
434, epoch_train_loss=4.437405843867416
Epoch 435
Epoch 435 :: Batch 0/1
Batch Loss = 4.4374050868728245
435, epoch_train_loss=4.4374050868728245
Epoch 436
Epoch 436 :: Batch 0/1
Batch Loss = 4.437404360993962
436, epoch_train_loss=4.437404360993962
Epoch 437
Epoch 437 :: Batch 0/1
Batch Loss = 4.4374036650258155
437, epoch_train_loss=4.4374036650258155
Epoch 438
Epoch 438 :: Batch 0/1
Batch Loss = 4.4374029978030975
438, epoch_train_loss=4.4374029978030975
Epoch 439
Epoch 439 :: Batch 0/1
Batch Loss = 4.437402358199483
439, epoch_train_loss=4.437402358199483
Epoch 440
Epoch 440 :: Batch 0/1
Batch Loss = 4.437401745126779
440, epoch_train_loss=4.437401745126779
Epoch 441
Epoch 441 :: Batch 0/1
Batch Loss = 4.437401157534238
441, epoch_train_loss=4.437401157534238
Epoch 442
Epoch 442 :: Batch 0/1
Batch Loss = 4.437400594407534
442, epoch_train_loss=4.437400594407534
Epoch 443
Epoch 443 :: Batch 0/1
Batch Loss = 4.43740005476804
443, epoch_train_loss=4.43740005476804
Epoch 444
Epoch 444 :: Batch 0/1
Batch Loss = 4.43739953767193
444, epoch_train_loss=4.43739953767193
Epoch 445
Epoch 445 :: Batch 0/1
Batch Loss = 4.4373990422091785
445, epoch_train_loss=4.4373990422091785
Epoch 446
Epoch 446 :: Batch 0/1
Batch Loss = 4.437398567502815
446, epoch_train_loss=4.437398567502815
Epoch 447
Epoch 447 :: Batch 0/1
Batch Loss = 4.437398112707907
447, epoch_train_loss=4.437398112707907
Epoch 448
Epoch 448 :: Batch 0/1
Batch Loss = 4.437397677010659
448, epoch_train_loss=4.437397677010659
Epoch 449
Epoch 449 :: Batch 0/1
Batch Loss = 4.437397259627616
449, epoch_train_loss=4.437397259627616
Epoch 450
Epoch 450 :: Batch 0/1
Batch Loss = 4.437396859804633
450, epoch_train_loss=4.437396859804633
Epoch 451
Epoch 451 :: Batch 0/1
Batch Loss = 4.437396476816083
451, epoch_train_loss=4.437396476816083
Epoch 452
Epoch 452 :: Batch 0/1
Batch Loss = 4.437396109963936
452, epoch_train_loss=4.437396109963936
Epoch 453
Epoch 453 :: Batch 0/1
Batch Loss = 4.437395758576928
453, epoch_train_loss=4.437395758576928
Epoch 454
Epoch 454 :: Batch 0/1
Batch Loss = 4.437395422009611
454, epoch_train_loss=4.437395422009611
Epoch 455
Epoch 455 :: Batch 0/1
Batch Loss = 4.437395099641593
455, epoch_train_loss=4.437395099641593
Epoch 456
Epoch 456 :: Batch 0/1
Batch Loss = 4.437394790876712
456, epoch_train_loss=4.437394790876712
Epoch 457
Epoch 457 :: Batch 0/1
Batch Loss = 4.437394495142139
457, epoch_train_loss=4.437394495142139
Epoch 458
Epoch 458 :: Batch 0/1
Batch Loss = 4.437394211887664
458, epoch_train_loss=4.437394211887664
Epoch 459
Epoch 459 :: Batch 0/1
Batch Loss = 4.4373939405849105
459, epoch_train_loss=4.4373939405849105
Epoch 460
Epoch 460 :: Batch 0/1
Batch Loss = 4.437393680726489
460, epoch_train_loss=4.437393680726489
Epoch 461
Epoch 461 :: Batch 0/1
Batch Loss = 4.437393431825381
461, epoch_train_loss=4.437393431825381
Epoch 462
Epoch 462 :: Batch 0/1
Batch Loss = 4.437393193414129
462, epoch_train_loss=4.437393193414129
Epoch 463
Epoch 463 :: Batch 0/1
Batch Loss = 4.437392965044179
463, epoch_train_loss=4.437392965044179
Epoch 464
Epoch 464 :: Batch 0/1
Batch Loss = 4.437392746285128
464, epoch_train_loss=4.437392746285128
Epoch 465
Epoch 465 :: Batch 0/1
Batch Loss = 4.437392536724121
465, epoch_train_loss=4.437392536724121
Epoch 466
Epoch 466 :: Batch 0/1
Batch Loss = 4.437392335965171
466, epoch_train_loss=4.437392335965171
Epoch 467
Epoch 467 :: Batch 0/1
Batch Loss = 4.437392143628585
467, epoch_train_loss=4.437392143628585
Epoch 468
Epoch 468 :: Batch 0/1
Batch Loss = 4.437391959350264
468, epoch_train_loss=4.437391959350264
Epoch 469
Epoch 469 :: Batch 0/1
Batch Loss = 4.437391782781166
469, epoch_train_loss=4.437391782781166
Epoch 470
Epoch 470 :: Batch 0/1
Batch Loss = 4.437391613586754
470, epoch_train_loss=4.437391613586754
Epoch 471
Epoch 471 :: Batch 0/1
Batch Loss = 4.437391451446413
471, epoch_train_loss=4.437391451446413
Epoch 472
Epoch 472 :: Batch 0/1
Batch Loss = 4.437391296052881
472, epoch_train_loss=4.437391296052881
Epoch 473
Epoch 473 :: Batch 0/1
Batch Loss = 4.4373911471118035
473, epoch_train_loss=4.4373911471118035
Epoch 474
Epoch 474 :: Batch 0/1
Batch Loss = 4.437391004341183
474, epoch_train_loss=4.437391004341183
Epoch 475
Epoch 475 :: Batch 0/1
Batch Loss = 4.4373908674709055
475, epoch_train_loss=4.4373908674709055
Epoch 476
Epoch 476 :: Batch 0/1
Batch Loss = 4.437390736242265
476, epoch_train_loss=4.437390736242265
Epoch 477
Epoch 477 :: Batch 0/1
Batch Loss = 4.437390610407544
477, epoch_train_loss=4.437390610407544
Epoch 478
Epoch 478 :: Batch 0/1
Batch Loss = 4.437390489729542
478, epoch_train_loss=4.437390489729542
Epoch 479
Epoch 479 :: Batch 0/1
Batch Loss = 4.43739037398116
479, epoch_train_loss=4.43739037398116
Epoch 480
Epoch 480 :: Batch 0/1
Batch Loss = 4.4373902629450255
480, epoch_train_loss=4.4373902629450255
Epoch 481
Epoch 481 :: Batch 0/1
Batch Loss = 4.4373901564130644
481, epoch_train_loss=4.4373901564130644
Epoch 482
Epoch 482 :: Batch 0/1
Batch Loss = 4.437390054186137
482, epoch_train_loss=4.437390054186137
Epoch 483
Epoch 483 :: Batch 0/1
Batch Loss = 4.437389956073694
483, epoch_train_loss=4.437389956073694
Epoch 484
Epoch 484 :: Batch 0/1
Batch Loss = 4.437389861893401
484, epoch_train_loss=4.437389861893401
Epoch 485
Epoch 485 :: Batch 0/1
Batch Loss = 4.437389771470806
485, epoch_train_loss=4.437389771470806
Epoch 486
Epoch 486 :: Batch 0/1
Batch Loss = 4.437389684639038
486, epoch_train_loss=4.437389684639038
Epoch 487
Epoch 487 :: Batch 0/1
Batch Loss = 4.437389601238468
487, epoch_train_loss=4.437389601238468
Epoch 488
Epoch 488 :: Batch 0/1
Batch Loss = 4.437389521116422
488, epoch_train_loss=4.437389521116422
Epoch 489
Epoch 489 :: Batch 0/1
Batch Loss = 4.437389444126901
489, epoch_train_loss=4.437389444126901
Epoch 490
Epoch 490 :: Batch 0/1
Batch Loss = 4.437389370130279
490, epoch_train_loss=4.437389370130279
Epoch 491
Epoch 491 :: Batch 0/1
Batch Loss = 4.437389298993067
491, epoch_train_loss=4.437389298993067
Epoch 492
Epoch 492 :: Batch 0/1
Batch Loss = 4.437389230587626
492, epoch_train_loss=4.437389230587626
Epoch 493
Epoch 493 :: Batch 0/1
Batch Loss = 4.437389164791945
493, epoch_train_loss=4.437389164791945
Epoch 494
Epoch 494 :: Batch 0/1
Batch Loss = 4.437389101489393
494, epoch_train_loss=4.437389101489393
Epoch 495
Epoch 495 :: Batch 0/1
Batch Loss = 4.437389040568492
495, epoch_train_loss=4.437389040568492
Epoch 496
Epoch 496 :: Batch 0/1
Batch Loss = 4.437388981922703
496, epoch_train_loss=4.437388981922703
Epoch 497
Epoch 497 :: Batch 0/1
Batch Loss = 4.437388925450201
497, epoch_train_loss=4.437388925450201
Epoch 498
Epoch 498 :: Batch 0/1
Batch Loss = 4.437388871053706
498, epoch_train_loss=4.437388871053706
Epoch 499
Epoch 499 :: Batch 0/1
Batch Loss = 4.437388818640253
499, epoch_train_loss=4.437388818640253
Epoch 500
Epoch 500 :: Batch 0/1
Batch Loss = 4.4373887681210284
500, epoch_train_loss=4.4373887681210284
Epoch 501
Epoch 501 :: Batch 0/1
Batch Loss = 4.437388719411187
501, epoch_train_loss=4.437388719411187
Epoch 502
Epoch 502 :: Batch 0/1
Batch Loss = 4.4373886724296785
502, epoch_train_loss=4.4373886724296785
Epoch 503
Epoch 503 :: Batch 0/1
Batch Loss = 4.437388627099089
503, epoch_train_loss=4.437388627099089
Epoch 504
Epoch 504 :: Batch 0/1
Batch Loss = 4.43738858334547
504, epoch_train_loss=4.43738858334547
Epoch 505
Epoch 505 :: Batch 0/1
Batch Loss = 4.43738854109821
505, epoch_train_loss=4.43738854109821
Epoch 506
Epoch 506 :: Batch 0/1
Batch Loss = 4.437388500289866
506, epoch_train_loss=4.437388500289866
Epoch 507
Epoch 507 :: Batch 0/1
Batch Loss = 4.437388460856044
507, epoch_train_loss=4.437388460856044
Epoch 508
Epoch 508 :: Batch 0/1
Batch Loss = 4.437388422735252
508, epoch_train_loss=4.437388422735252
Epoch 509
Epoch 509 :: Batch 0/1
Batch Loss = 4.437388385868787
509, epoch_train_loss=4.437388385868787
Epoch 510
Epoch 510 :: Batch 0/1
Batch Loss = 4.437388350200589
510, epoch_train_loss=4.437388350200589
Epoch 511
Epoch 511 :: Batch 0/1
Batch Loss = 4.437388315677153
511, epoch_train_loss=4.437388315677153
Epoch 512
Epoch 512 :: Batch 0/1
Batch Loss = 4.437388282247398
512, epoch_train_loss=4.437388282247398
Epoch 513
Epoch 513 :: Batch 0/1
Batch Loss = 4.437388249862564
513, epoch_train_loss=4.437388249862564
Epoch 514
Epoch 514 :: Batch 0/1
Batch Loss = 4.437388218476106
514, epoch_train_loss=4.437388218476106
Epoch 515
Epoch 515 :: Batch 0/1
Batch Loss = 4.437388188043607
515, epoch_train_loss=4.437388188043607
Epoch 516
Epoch 516 :: Batch 0/1
Batch Loss = 4.437388158522665
516, epoch_train_loss=4.437388158522665
Epoch 517
Epoch 517 :: Batch 0/1
Batch Loss = 4.437388129872816
517, epoch_train_loss=4.437388129872816
Epoch 518
Epoch 518 :: Batch 0/1
Batch Loss = 4.437388102055447
518, epoch_train_loss=4.437388102055447
Epoch 519
Epoch 519 :: Batch 0/1
Batch Loss = 4.437388075033707
519, epoch_train_loss=4.437388075033707
Epoch 520
Epoch 520 :: Batch 0/1
Batch Loss = 4.437388048772432
520, epoch_train_loss=4.437388048772432
Epoch 521
Epoch 521 :: Batch 0/1
Batch Loss = 4.437388023238065
521, epoch_train_loss=4.437388023238065
Epoch 522
Epoch 522 :: Batch 0/1
Batch Loss = 4.437387998398589
522, epoch_train_loss=4.437387998398589
Epoch 523
Epoch 523 :: Batch 0/1
Batch Loss = 4.4373879742234505
523, epoch_train_loss=4.4373879742234505
Epoch 524
Epoch 524 :: Batch 0/1
Batch Loss = 4.437387950683494
524, epoch_train_loss=4.437387950683494
Epoch 525
Epoch 525 :: Batch 0/1
Batch Loss = 4.437387927750908
525, epoch_train_loss=4.437387927750908
Epoch 526
Epoch 526 :: Batch 0/1
Batch Loss = 4.437387905399154
526, epoch_train_loss=4.437387905399154
Epoch 527
Epoch 527 :: Batch 0/1
Batch Loss = 4.437387883602906
527, epoch_train_loss=4.437387883602906
Epoch 528
Epoch 528 :: Batch 0/1
Batch Loss = 4.437387862338003
528, epoch_train_loss=4.437387862338003
Epoch 529
Epoch 529 :: Batch 0/1
Batch Loss = 4.437387841581393
529, epoch_train_loss=4.437387841581393
Epoch 530
Epoch 530 :: Batch 0/1
Batch Loss = 4.437387821311078
530, epoch_train_loss=4.437387821311078
Epoch 531
Epoch 531 :: Batch 0/1
Batch Loss = 4.4373878015060715
531, epoch_train_loss=4.4373878015060715
Epoch 532
Epoch 532 :: Batch 0/1
Batch Loss = 4.43738778214634
532, epoch_train_loss=4.43738778214634
Epoch 533
Epoch 533 :: Batch 0/1
Batch Loss = 4.4373877632127785
533, epoch_train_loss=4.4373877632127785
Epoch 534
Epoch 534 :: Batch 0/1
Batch Loss = 4.437387744687148
534, epoch_train_loss=4.437387744687148
Epoch 535
Epoch 535 :: Batch 0/1
Batch Loss = 4.437387726552044
535, epoch_train_loss=4.437387726552044
Epoch 536
Epoch 536 :: Batch 0/1
Batch Loss = 4.437387708790856
536, epoch_train_loss=4.437387708790856
Epoch 537
Epoch 537 :: Batch 0/1
Batch Loss = 4.437387691387734
537, epoch_train_loss=4.437387691387734
Epoch 538
Epoch 538 :: Batch 0/1
Batch Loss = 4.4373876743275416
538, epoch_train_loss=4.4373876743275416
Epoch 539
Epoch 539 :: Batch 0/1
Batch Loss = 4.4373876575958375
539, epoch_train_loss=4.4373876575958375
Epoch 540
Epoch 540 :: Batch 0/1
Batch Loss = 4.43738764117883
540, epoch_train_loss=4.43738764117883
Epoch 541
Epoch 541 :: Batch 0/1
Batch Loss = 4.437387625063351
541, epoch_train_loss=4.437387625063351
Epoch 542
Epoch 542 :: Batch 0/1
Batch Loss = 4.437387609236826
542, epoch_train_loss=4.437387609236826
Epoch 543
Epoch 543 :: Batch 0/1
Batch Loss = 4.437387593687247
543, epoch_train_loss=4.437387593687247
Epoch 544
Epoch 544 :: Batch 0/1
Batch Loss = 4.437387578403147
544, epoch_train_loss=4.437387578403147
Epoch 545
Epoch 545 :: Batch 0/1
Batch Loss = 4.43738756337356
545, epoch_train_loss=4.43738756337356
Epoch 546
Epoch 546 :: Batch 0/1
Batch Loss = 4.4373875485880205
546, epoch_train_loss=4.4373875485880205
Epoch 547
Epoch 547 :: Batch 0/1
Batch Loss = 4.43738753403652
547, epoch_train_loss=4.43738753403652
Epoch 548
Epoch 548 :: Batch 0/1
Batch Loss = 4.437387519709489
548, epoch_train_loss=4.437387519709489
Epoch 549
Epoch 549 :: Batch 0/1
Batch Loss = 4.437387505597778
549, epoch_train_loss=4.437387505597778
Epoch 550
Epoch 550 :: Batch 0/1
Batch Loss = 4.437387491692641
550, epoch_train_loss=4.437387491692641
Epoch 551
Epoch 551 :: Batch 0/1
Batch Loss = 4.4373874779857
551, epoch_train_loss=4.4373874779857
Epoch 552
Epoch 552 :: Batch 0/1
Batch Loss = 4.437387464468952
552, epoch_train_loss=4.437387464468952
Epoch 553
Epoch 553 :: Batch 0/1
Batch Loss = 4.437387451134721
553, epoch_train_loss=4.437387451134721
Epoch 554
Epoch 554 :: Batch 0/1
Batch Loss = 4.437387437975662
554, epoch_train_loss=4.437387437975662
Epoch 555
Epoch 555 :: Batch 0/1
Batch Loss = 4.437387424984737
555, epoch_train_loss=4.437387424984737
Epoch 556
Epoch 556 :: Batch 0/1
Batch Loss = 4.437387412155202
556, epoch_train_loss=4.437387412155202
Epoch 557
Epoch 557 :: Batch 0/1
Batch Loss = 4.437387399480584
557, epoch_train_loss=4.437387399480584
Epoch 558
Epoch 558 :: Batch 0/1
Batch Loss = 4.4373873869546765
558, epoch_train_loss=4.4373873869546765
Epoch 559
Epoch 559 :: Batch 0/1
Batch Loss = 4.437387374571519
559, epoch_train_loss=4.437387374571519
Epoch 560
Epoch 560 :: Batch 0/1
Batch Loss = 4.437387362325391
560, epoch_train_loss=4.437387362325391
Epoch 561
Epoch 561 :: Batch 0/1
Batch Loss = 4.437387350210789
561, epoch_train_loss=4.437387350210789
Epoch 562
Epoch 562 :: Batch 0/1
Batch Loss = 4.437387338222421
562, epoch_train_loss=4.437387338222421
Epoch 563
Epoch 563 :: Batch 0/1
Batch Loss = 4.4373873263551955
563, epoch_train_loss=4.4373873263551955
Epoch 564
Epoch 564 :: Batch 0/1
Batch Loss = 4.437387314604208
564, epoch_train_loss=4.437387314604208
Epoch 565
Epoch 565 :: Batch 0/1
Batch Loss = 4.437387302964727
565, epoch_train_loss=4.437387302964727
Epoch 566
Epoch 566 :: Batch 0/1
Batch Loss = 4.437387291432192
566, epoch_train_loss=4.437387291432192
Epoch 567
Epoch 567 :: Batch 0/1
Batch Loss = 4.437387280002197
567, epoch_train_loss=4.437387280002197
Epoch 568
Epoch 568 :: Batch 0/1
Batch Loss = 4.437387268670482
568, epoch_train_loss=4.437387268670482
Epoch 569
Epoch 569 :: Batch 0/1
Batch Loss = 4.437387257432926
569, epoch_train_loss=4.437387257432926
Epoch 570
Epoch 570 :: Batch 0/1
Batch Loss = 4.437387246285536
570, epoch_train_loss=4.437387246285536
Epoch 571
Epoch 571 :: Batch 0/1
Batch Loss = 4.437387235224442
571, epoch_train_loss=4.437387235224442
Epoch 572
Epoch 572 :: Batch 0/1
Batch Loss = 4.437387224245882
572, epoch_train_loss=4.437387224245882
Epoch 573
Epoch 573 :: Batch 0/1
Batch Loss = 4.437387213346202
573, epoch_train_loss=4.437387213346202
Epoch 574
Epoch 574 :: Batch 0/1
Batch Loss = 4.437387202521846
574, epoch_train_loss=4.437387202521846
Epoch 575
Epoch 575 :: Batch 0/1
Batch Loss = 4.437387191769343
575, epoch_train_loss=4.437387191769343
Epoch 576
Epoch 576 :: Batch 0/1
Batch Loss = 4.437387181085306
576, epoch_train_loss=4.437387181085306
Epoch 577
Epoch 577 :: Batch 0/1
Batch Loss = 4.437387170466429
577, epoch_train_loss=4.437387170466429
Epoch 578
Epoch 578 :: Batch 0/1
Batch Loss = 4.437387159909465
578, epoch_train_loss=4.437387159909465
Epoch 579
Epoch 579 :: Batch 0/1
Batch Loss = 4.43738714941124
579, epoch_train_loss=4.43738714941124
Epoch 580
Epoch 580 :: Batch 0/1
Batch Loss = 4.437387138968627
580, epoch_train_loss=4.437387138968627
Epoch 581
Epoch 581 :: Batch 0/1
Batch Loss = 4.437387128578554
581, epoch_train_loss=4.437387128578554
Epoch 582
Epoch 582 :: Batch 0/1
Batch Loss = 4.437387118237993
582, epoch_train_loss=4.437387118237993
Epoch 583
Epoch 583 :: Batch 0/1
Batch Loss = 4.437387107943952
583, epoch_train_loss=4.437387107943952
Epoch 584
Epoch 584 :: Batch 0/1
Batch Loss = 4.437387097693472
584, epoch_train_loss=4.437387097693472
Epoch 585
Epoch 585 :: Batch 0/1
Batch Loss = 4.437387087483621
585, epoch_train_loss=4.437387087483621
Epoch 586
Epoch 586 :: Batch 0/1
Batch Loss = 4.43738707731149
586, epoch_train_loss=4.43738707731149
Epoch 587
Epoch 587 :: Batch 0/1
Batch Loss = 4.4373870671741855
587, epoch_train_loss=4.4373870671741855
Epoch 588
Epoch 588 :: Batch 0/1
Batch Loss = 4.437387057068821
588, epoch_train_loss=4.437387057068821
Epoch 589
Epoch 589 :: Batch 0/1
Batch Loss = 4.4373870469925185
589, epoch_train_loss=4.4373870469925185
Epoch 590
Epoch 590 :: Batch 0/1
Batch Loss = 4.437387036942405
590, epoch_train_loss=4.437387036942405
Epoch 591
Epoch 591 :: Batch 0/1
Batch Loss = 4.4373870269155935
591, epoch_train_loss=4.4373870269155935
Epoch 592
Epoch 592 :: Batch 0/1
Batch Loss = 4.437387016909195
592, epoch_train_loss=4.437387016909195
Epoch 593
Epoch 593 :: Batch 0/1
Batch Loss = 4.437387006920301
593, epoch_train_loss=4.437387006920301
Epoch 594
Epoch 594 :: Batch 0/1
Batch Loss = 4.437386996945988
594, epoch_train_loss=4.437386996945988
Epoch 595
Epoch 595 :: Batch 0/1
Batch Loss = 4.437386986983304
595, epoch_train_loss=4.437386986983304
Epoch 596
Epoch 596 :: Batch 0/1
Batch Loss = 4.437386977029273
596, epoch_train_loss=4.437386977029273
Epoch 597
Epoch 597 :: Batch 0/1
Batch Loss = 4.43738696708088
597, epoch_train_loss=4.43738696708088
Epoch 598
Epoch 598 :: Batch 0/1
Batch Loss = 4.437386957135076
598, epoch_train_loss=4.437386957135076
Epoch 599
Epoch 599 :: Batch 0/1
Batch Loss = 4.437386947188769
599, epoch_train_loss=4.437386947188769
Epoch 600
Epoch 600 :: Batch 0/1
Batch Loss = 4.437386937238818
600, epoch_train_loss=4.437386937238818
Epoch 601
Epoch 601 :: Batch 0/1
Batch Loss = 4.437386927282032
601, epoch_train_loss=4.437386927282032
Epoch 602
Epoch 602 :: Batch 0/1
Batch Loss = 4.437386917315162
602, epoch_train_loss=4.437386917315162
Epoch 603
Epoch 603 :: Batch 0/1
Batch Loss = 4.437386907334902
603, epoch_train_loss=4.437386907334902
Epoch 604
Epoch 604 :: Batch 0/1
Batch Loss = 4.437386897337884
604, epoch_train_loss=4.437386897337884
Epoch 605
Epoch 605 :: Batch 0/1
Batch Loss = 4.437386887320666
605, epoch_train_loss=4.437386887320666
Epoch 606
Epoch 606 :: Batch 0/1
Batch Loss = 4.437386877279739
606, epoch_train_loss=4.437386877279739
Epoch 607
Epoch 607 :: Batch 0/1
Batch Loss = 4.437386867211521
607, epoch_train_loss=4.437386867211521
Epoch 608
Epoch 608 :: Batch 0/1
Batch Loss = 4.437386857112348
608, epoch_train_loss=4.437386857112348
Epoch 609
Epoch 609 :: Batch 0/1
Batch Loss = 4.437386846978478
609, epoch_train_loss=4.437386846978478
Epoch 610
Epoch 610 :: Batch 0/1
Batch Loss = 4.437386836806084
610, epoch_train_loss=4.437386836806084
Epoch 611
Epoch 611 :: Batch 0/1
Batch Loss = 4.437386826591257
611, epoch_train_loss=4.437386826591257
Epoch 612
Epoch 612 :: Batch 0/1
Batch Loss = 4.437386816329997
612, epoch_train_loss=4.437386816329997
Epoch 613
Epoch 613 :: Batch 0/1
Batch Loss = 4.437386806018225
613, epoch_train_loss=4.437386806018225
Epoch 614
Epoch 614 :: Batch 0/1
Batch Loss = 4.4373867956517605
614, epoch_train_loss=4.4373867956517605
Epoch 615
Epoch 615 :: Batch 0/1
Batch Loss = 4.437386785226351
615, epoch_train_loss=4.437386785226351
Epoch 616
Epoch 616 :: Batch 0/1
Batch Loss = 4.437386774737645
616, epoch_train_loss=4.437386774737645
Epoch 617
Epoch 617 :: Batch 0/1
Batch Loss = 4.437386764181214
617, epoch_train_loss=4.437386764181214
Epoch 618
Epoch 618 :: Batch 0/1
Batch Loss = 4.4373867535525475
618, epoch_train_loss=4.4373867535525475
Epoch 619
Epoch 619 :: Batch 0/1
Batch Loss = 4.437386742847059
619, epoch_train_loss=4.437386742847059
Epoch 620
Epoch 620 :: Batch 0/1
Batch Loss = 4.43738673206009
620, epoch_train_loss=4.43738673206009
Epoch 621
Epoch 621 :: Batch 0/1
Batch Loss = 4.437386721186925
621, epoch_train_loss=4.437386721186925
Epoch 622
Epoch 622 :: Batch 0/1
Batch Loss = 4.43738671022279
622, epoch_train_loss=4.43738671022279
Epoch 623
Epoch 623 :: Batch 0/1
Batch Loss = 4.437386699162873
623, epoch_train_loss=4.437386699162873
Epoch 624
Epoch 624 :: Batch 0/1
Batch Loss = 4.4373866880023325
624, epoch_train_loss=4.4373866880023325
Epoch 625
Epoch 625 :: Batch 0/1
Batch Loss = 4.437386676736315
625, epoch_train_loss=4.437386676736315
Epoch 626
Epoch 626 :: Batch 0/1
Batch Loss = 4.437386665359975
626, epoch_train_loss=4.437386665359975
Epoch 627
Epoch 627 :: Batch 0/1
Batch Loss = 4.4373866538684945
627, epoch_train_loss=4.4373866538684945
Epoch 628
Epoch 628 :: Batch 0/1
Batch Loss = 4.437386642257107
628, epoch_train_loss=4.437386642257107
Epoch 629
Epoch 629 :: Batch 0/1
Batch Loss = 4.437386630521131
629, epoch_train_loss=4.437386630521131
Epoch 630
Epoch 630 :: Batch 0/1
Batch Loss = 4.4373866186559985
630, epoch_train_loss=4.4373866186559985
Epoch 631
Epoch 631 :: Batch 0/1
Batch Loss = 4.4373866066572845
631, epoch_train_loss=4.4373866066572845
Epoch 632
Epoch 632 :: Batch 0/1
Batch Loss = 4.437386594520765
632, epoch_train_loss=4.437386594520765
Epoch 633
Epoch 633 :: Batch 0/1
Batch Loss = 4.437386582242445
633, epoch_train_loss=4.437386582242445
Epoch 634
Epoch 634 :: Batch 0/1
Batch Loss = 4.4373865698186155
634, epoch_train_loss=4.4373865698186155
Epoch 635
Epoch 635 :: Batch 0/1
Batch Loss = 4.43738655724591
635, epoch_train_loss=4.43738655724591
Epoch 636
Epoch 636 :: Batch 0/1
Batch Loss = 4.43738654452136
636, epoch_train_loss=4.43738654452136
Epoch 637
Epoch 637 :: Batch 0/1
Batch Loss = 4.437386531642467
637, epoch_train_loss=4.437386531642467
Epoch 638
Epoch 638 :: Batch 0/1
Batch Loss = 4.437386518607266
638, epoch_train_loss=4.437386518607266
Epoch 639
Epoch 639 :: Batch 0/1
Batch Loss = 4.437386505414402
639, epoch_train_loss=4.437386505414402
Epoch 640
Epoch 640 :: Batch 0/1
Batch Loss = 4.437386492063222
640, epoch_train_loss=4.437386492063222
Epoch 641
Epoch 641 :: Batch 0/1
Batch Loss = 4.43738647855384
641, epoch_train_loss=4.43738647855384
Epoch 642
Epoch 642 :: Batch 0/1
Batch Loss = 4.437386464887247
642, epoch_train_loss=4.437386464887247
Epoch 643
Epoch 643 :: Batch 0/1
Batch Loss = 4.437386451065389
643, epoch_train_loss=4.437386451065389
Epoch 644
Epoch 644 :: Batch 0/1
Batch Loss = 4.437386437091269
644, epoch_train_loss=4.437386437091269
Epoch 645
Epoch 645 :: Batch 0/1
Batch Loss = 4.437386422969039
645, epoch_train_loss=4.437386422969039
Epoch 646
Epoch 646 :: Batch 0/1
Batch Loss = 4.437386408704105
646, epoch_train_loss=4.437386408704105
Epoch 647
Epoch 647 :: Batch 0/1
Batch Loss = 4.437386394303209
647, epoch_train_loss=4.437386394303209
Epoch 648
Epoch 648 :: Batch 0/1
Batch Loss = 4.437386379774526
648, epoch_train_loss=4.437386379774526
Epoch 649
Epoch 649 :: Batch 0/1
Batch Loss = 4.437386365127749
649, epoch_train_loss=4.437386365127749
Epoch 650
Epoch 650 :: Batch 0/1
Batch Loss = 4.437386350374161
650, epoch_train_loss=4.437386350374161
Epoch 651
Epoch 651 :: Batch 0/1
Batch Loss = 4.437386335526704
651, epoch_train_loss=4.437386335526704
Epoch 652
Epoch 652 :: Batch 0/1
Batch Loss = 4.437386320600025
652, epoch_train_loss=4.437386320600025
Epoch 653
Epoch 653 :: Batch 0/1
Batch Loss = 4.437386305610512
653, epoch_train_loss=4.437386305610512
Epoch 654
Epoch 654 :: Batch 0/1
Batch Loss = 4.437386290576303
654, epoch_train_loss=4.437386290576303
Epoch 655
Epoch 655 :: Batch 0/1
Batch Loss = 4.437386275517281
655, epoch_train_loss=4.437386275517281
Epoch 656
Epoch 656 :: Batch 0/1
Batch Loss = 4.437386260455026
656, epoch_train_loss=4.437386260455026
Epoch 657
Epoch 657 :: Batch 0/1
Batch Loss = 4.437386245412756
657, epoch_train_loss=4.437386245412756
Epoch 658
Epoch 658 :: Batch 0/1
Batch Loss = 4.437386230415226
658, epoch_train_loss=4.437386230415226
Epoch 659
Epoch 659 :: Batch 0/1
Batch Loss = 4.437386215488588
659, epoch_train_loss=4.437386215488588
Epoch 660
Epoch 660 :: Batch 0/1
Batch Loss = 4.437386200660226
660, epoch_train_loss=4.437386200660226
Epoch 661
Epoch 661 :: Batch 0/1
Batch Loss = 4.43738618595855
661, epoch_train_loss=4.43738618595855
Epoch 662
Epoch 662 :: Batch 0/1
Batch Loss = 4.4373861714127525
662, epoch_train_loss=4.4373861714127525
Epoch 663
Epoch 663 :: Batch 0/1
Batch Loss = 4.43738615705253
663, epoch_train_loss=4.43738615705253
Epoch 664
Epoch 664 :: Batch 0/1
Batch Loss = 4.437386142907783
664, epoch_train_loss=4.437386142907783
Epoch 665
Epoch 665 :: Batch 0/1
Batch Loss = 4.437386129008264
665, epoch_train_loss=4.437386129008264
Epoch 666
Epoch 666 :: Batch 0/1
Batch Loss = 4.437386115383232
666, epoch_train_loss=4.437386115383232
Epoch 667
Epoch 667 :: Batch 0/1
Batch Loss = 4.437386102061064
667, epoch_train_loss=4.437386102061064
Epoch 668
Epoch 668 :: Batch 0/1
Batch Loss = 4.43738608906887
668, epoch_train_loss=4.43738608906887
Epoch 669
Epoch 669 :: Batch 0/1
Batch Loss = 4.437386076432106
669, epoch_train_loss=4.437386076432106
Epoch 670
Epoch 670 :: Batch 0/1
Batch Loss = 4.437386064174186
670, epoch_train_loss=4.437386064174186
Epoch 671
Epoch 671 :: Batch 0/1
Batch Loss = 4.4373860523161115
671, epoch_train_loss=4.4373860523161115
Epoch 672
Epoch 672 :: Batch 0/1
Batch Loss = 4.437386040876136
672, epoch_train_loss=4.437386040876136
Epoch 673
Epoch 673 :: Batch 0/1
Batch Loss = 4.437386029869458
673, epoch_train_loss=4.437386029869458
Epoch 674
Epoch 674 :: Batch 0/1
Batch Loss = 4.437386019307958
674, epoch_train_loss=4.437386019307958
Epoch 675
Epoch 675 :: Batch 0/1
Batch Loss = 4.437386009199985
675, epoch_train_loss=4.437386009199985
Epoch 676
Epoch 676 :: Batch 0/1
Batch Loss = 4.43738599955022
676, epoch_train_loss=4.43738599955022
Epoch 677
Epoch 677 :: Batch 0/1
Batch Loss = 4.437385990359586
677, epoch_train_loss=4.437385990359586
Epoch 678
Epoch 678 :: Batch 0/1
Batch Loss = 4.437385981625241
678, epoch_train_loss=4.437385981625241
Epoch 679
Epoch 679 :: Batch 0/1
Batch Loss = 4.437385973340637
679, epoch_train_loss=4.437385973340637
Epoch 680
Epoch 680 :: Batch 0/1
Batch Loss = 4.437385965495656
680, epoch_train_loss=4.437385965495656
Epoch 681
Epoch 681 :: Batch 0/1
Batch Loss = 4.437385958076806
681, epoch_train_loss=4.437385958076806
Epoch 682
Epoch 682 :: Batch 0/1
Batch Loss = 4.437385951067506
682, epoch_train_loss=4.437385951067506
Epoch 683
Epoch 683 :: Batch 0/1
Batch Loss = 4.437385944448397
683, epoch_train_loss=4.437385944448397
Epoch 684
Epoch 684 :: Batch 0/1
Batch Loss = 4.437385938197727
684, epoch_train_loss=4.437385938197727
Epoch 685
Epoch 685 :: Batch 0/1
Batch Loss = 4.437385932291775
685, epoch_train_loss=4.437385932291775
Epoch 686
Epoch 686 :: Batch 0/1
Batch Loss = 4.437385926705279
686, epoch_train_loss=4.437385926705279
Epoch 687
Epoch 687 :: Batch 0/1
Batch Loss = 4.437385921411918
687, epoch_train_loss=4.437385921411918
Epoch 688
Epoch 688 :: Batch 0/1
Batch Loss = 4.437385916384773
688, epoch_train_loss=4.437385916384773
Epoch 689
Epoch 689 :: Batch 0/1
Batch Loss = 4.437385911596772
689, epoch_train_loss=4.437385911596772
Epoch 690
Epoch 690 :: Batch 0/1
Batch Loss = 4.437385907021142
690, epoch_train_loss=4.437385907021142
Epoch 691
Epoch 691 :: Batch 0/1
Batch Loss = 4.437385902631797
691, epoch_train_loss=4.437385902631797
Epoch 692
Epoch 692 :: Batch 0/1
Batch Loss = 4.437385898403715
692, epoch_train_loss=4.437385898403715
Epoch 693
Epoch 693 :: Batch 0/1
Batch Loss = 4.437385894313237
693, epoch_train_loss=4.437385894313237
Epoch 694
Epoch 694 :: Batch 0/1
Batch Loss = 4.437385890338339
694, epoch_train_loss=4.437385890338339
Epoch 695
Epoch 695 :: Batch 0/1
Batch Loss = 4.437385886458823
695, epoch_train_loss=4.437385886458823
Epoch 696
Epoch 696 :: Batch 0/1
Batch Loss = 4.437385882656463
696, epoch_train_loss=4.437385882656463
Epoch 697
Epoch 697 :: Batch 0/1
Batch Loss = 4.43738587891509
697, epoch_train_loss=4.43738587891509
Epoch 698
Epoch 698 :: Batch 0/1
Batch Loss = 4.437385875220615
698, epoch_train_loss=4.437385875220615
Epoch 699
Epoch 699 :: Batch 0/1
Batch Loss = 4.437385871561011
699, epoch_train_loss=4.437385871561011
Epoch 700
Epoch 700 :: Batch 0/1
Batch Loss = 4.437385867926244
700, epoch_train_loss=4.437385867926244
Epoch 701
Epoch 701 :: Batch 0/1
Batch Loss = 4.437385864308171
701, epoch_train_loss=4.437385864308171
Epoch 702
Epoch 702 :: Batch 0/1
Batch Loss = 4.437385860700394
702, epoch_train_loss=4.437385860700394
Epoch 703
Epoch 703 :: Batch 0/1
Batch Loss = 4.437385857098114
703, epoch_train_loss=4.437385857098114
Epoch 704
Epoch 704 :: Batch 0/1
Batch Loss = 4.437385853497941
704, epoch_train_loss=4.437385853497941
Epoch 705
Epoch 705 :: Batch 0/1
Batch Loss = 4.43738584989772
705, epoch_train_loss=4.43738584989772
Epoch 706
Epoch 706 :: Batch 0/1
Batch Loss = 4.437385846296346
706, epoch_train_loss=4.437385846296346
Epoch 707
Epoch 707 :: Batch 0/1
Batch Loss = 4.437385842693572
707, epoch_train_loss=4.437385842693572
Epoch 708
Epoch 708 :: Batch 0/1
Batch Loss = 4.4373858390898455
708, epoch_train_loss=4.4373858390898455
Epoch 709
Epoch 709 :: Batch 0/1
Batch Loss = 4.4373858354861415
709, epoch_train_loss=4.4373858354861415
Epoch 710
Epoch 710 :: Batch 0/1
Batch Loss = 4.437385831883816
710, epoch_train_loss=4.437385831883816
Epoch 711
Epoch 711 :: Batch 0/1
Batch Loss = 4.43738582828448
711, epoch_train_loss=4.43738582828448
Epoch 712
Epoch 712 :: Batch 0/1
Batch Loss = 4.437385824689882
712, epoch_train_loss=4.437385824689882
Epoch 713
Epoch 713 :: Batch 0/1
Batch Loss = 4.437385821101817
713, epoch_train_loss=4.437385821101817
Epoch 714
Epoch 714 :: Batch 0/1
Batch Loss = 4.437385817522047
714, epoch_train_loss=4.437385817522047
Epoch 715
Epoch 715 :: Batch 0/1
Batch Loss = 4.437385813952248
715, epoch_train_loss=4.437385813952248
Epoch 716
Epoch 716 :: Batch 0/1
Batch Loss = 4.437385810393954
716, epoch_train_loss=4.437385810393954
Epoch 717
Epoch 717 :: Batch 0/1
Batch Loss = 4.437385806848535
717, epoch_train_loss=4.437385806848535
Epoch 718
Epoch 718 :: Batch 0/1
Batch Loss = 4.437385803317177
718, epoch_train_loss=4.437385803317177
Epoch 719
Epoch 719 :: Batch 0/1
Batch Loss = 4.437385799800865
719, epoch_train_loss=4.437385799800865
Epoch 720
Epoch 720 :: Batch 0/1
Batch Loss = 4.437385796300394
720, epoch_train_loss=4.437385796300394
Epoch 721
Epoch 721 :: Batch 0/1
Batch Loss = 4.437385792816369
721, epoch_train_loss=4.437385792816369
Epoch 722
Epoch 722 :: Batch 0/1
Batch Loss = 4.437385789349216
722, epoch_train_loss=4.437385789349216
Epoch 723
Epoch 723 :: Batch 0/1
Batch Loss = 4.437385785899197
723, epoch_train_loss=4.437385785899197
Epoch 724
Epoch 724 :: Batch 0/1
Batch Loss = 4.437385782466433
724, epoch_train_loss=4.437385782466433
Epoch 725
Epoch 725 :: Batch 0/1
Batch Loss = 4.437385779050913
725, epoch_train_loss=4.437385779050913
Epoch 726
Epoch 726 :: Batch 0/1
Batch Loss = 4.437385775652525
726, epoch_train_loss=4.437385775652525
Epoch 727
Epoch 727 :: Batch 0/1
Batch Loss = 4.437385772271065
727, epoch_train_loss=4.437385772271065
Epoch 728
Epoch 728 :: Batch 0/1
Batch Loss = 4.437385768906261
728, epoch_train_loss=4.437385768906261
Epoch 729
Epoch 729 :: Batch 0/1
Batch Loss = 4.437385765557787
729, epoch_train_loss=4.437385765557787
Epoch 730
Epoch 730 :: Batch 0/1
Batch Loss = 4.437385762225284
730, epoch_train_loss=4.437385762225284
Epoch 731
Epoch 731 :: Batch 0/1
Batch Loss = 4.4373857589083645
731, epoch_train_loss=4.4373857589083645
Epoch 732
Epoch 732 :: Batch 0/1
Batch Loss = 4.437385755606633
732, epoch_train_loss=4.437385755606633
Epoch 733
Epoch 733 :: Batch 0/1
Batch Loss = 4.437385752319692
733, epoch_train_loss=4.437385752319692
Epoch 734
Epoch 734 :: Batch 0/1
Batch Loss = 4.437385749047148
734, epoch_train_loss=4.437385749047148
Epoch 735
Epoch 735 :: Batch 0/1
Batch Loss = 4.437385745788626
735, epoch_train_loss=4.437385745788626
Epoch 736
Epoch 736 :: Batch 0/1
Batch Loss = 4.437385742543768
736, epoch_train_loss=4.437385742543768
Epoch 737
Epoch 737 :: Batch 0/1
Batch Loss = 4.437385739312235
737, epoch_train_loss=4.437385739312235
Epoch 738
Epoch 738 :: Batch 0/1
Batch Loss = 4.437385736093719
738, epoch_train_loss=4.437385736093719
Epoch 739
Epoch 739 :: Batch 0/1
Batch Loss = 4.437385732887935
739, epoch_train_loss=4.437385732887935
Epoch 740
Epoch 740 :: Batch 0/1
Batch Loss = 4.437385729694624
740, epoch_train_loss=4.437385729694624
Epoch 741
Epoch 741 :: Batch 0/1
Batch Loss = 4.4373857265135515
741, epoch_train_loss=4.4373857265135515
Epoch 742
Epoch 742 :: Batch 0/1
Batch Loss = 4.437385723344514
742, epoch_train_loss=4.437385723344514
Epoch 743
Epoch 743 :: Batch 0/1
Batch Loss = 4.437385720187327
743, epoch_train_loss=4.437385720187327
Epoch 744
Epoch 744 :: Batch 0/1
Batch Loss = 4.43738571704183
744, epoch_train_loss=4.43738571704183
Epoch 745
Epoch 745 :: Batch 0/1
Batch Loss = 4.437385713907884
745, epoch_train_loss=4.437385713907884
Epoch 746
Epoch 746 :: Batch 0/1
Batch Loss = 4.437385710785363
746, epoch_train_loss=4.437385710785363
Epoch 747
Epoch 747 :: Batch 0/1
Batch Loss = 4.437385707674165
747, epoch_train_loss=4.437385707674165
Epoch 748
Epoch 748 :: Batch 0/1
Batch Loss = 4.437385704574197
748, epoch_train_loss=4.437385704574197
Epoch 749
Epoch 749 :: Batch 0/1
Batch Loss = 4.437385701485377
749, epoch_train_loss=4.437385701485377
Epoch 750
Epoch 750 :: Batch 0/1
Batch Loss = 4.437385698407636
750, epoch_train_loss=4.437385698407636
Epoch 751
Epoch 751 :: Batch 0/1
Batch Loss = 4.437385695340905
751, epoch_train_loss=4.437385695340905
Epoch 752
Epoch 752 :: Batch 0/1
Batch Loss = 4.437385692285134
752, epoch_train_loss=4.437385692285134
Epoch 753
Epoch 753 :: Batch 0/1
Batch Loss = 4.4373856892402666
753, epoch_train_loss=4.4373856892402666
Epoch 754
Epoch 754 :: Batch 0/1
Batch Loss = 4.437385686206257
754, epoch_train_loss=4.437385686206257
Epoch 755
Epoch 755 :: Batch 0/1
Batch Loss = 4.437385683183055
755, epoch_train_loss=4.437385683183055
Epoch 756
Epoch 756 :: Batch 0/1
Batch Loss = 4.437385680170619
756, epoch_train_loss=4.437385680170619
Epoch 757
Epoch 757 :: Batch 0/1
Batch Loss = 4.4373856771689
757, epoch_train_loss=4.4373856771689
Epoch 758
Epoch 758 :: Batch 0/1
Batch Loss = 4.4373856741778575
758, epoch_train_loss=4.4373856741778575
Epoch 759
Epoch 759 :: Batch 0/1
Batch Loss = 4.437385671197444
759, epoch_train_loss=4.437385671197444
Epoch 760
Epoch 760 :: Batch 0/1
Batch Loss = 4.437385668227612
760, epoch_train_loss=4.437385668227612
Epoch 761
Epoch 761 :: Batch 0/1
Batch Loss = 4.437385665268317
761, epoch_train_loss=4.437385665268317
Epoch 762
Epoch 762 :: Batch 0/1
Batch Loss = 4.437385662319506
762, epoch_train_loss=4.437385662319506
Epoch 763
Epoch 763 :: Batch 0/1
Batch Loss = 4.437385659381131
763, epoch_train_loss=4.437385659381131
Epoch 764
Epoch 764 :: Batch 0/1
Batch Loss = 4.4373856564531415
764, epoch_train_loss=4.4373856564531415
Epoch 765
Epoch 765 :: Batch 0/1
Batch Loss = 4.437385653535482
765, epoch_train_loss=4.437385653535482
Epoch 766
Epoch 766 :: Batch 0/1
Batch Loss = 4.437385650628099
766, epoch_train_loss=4.437385650628099
Epoch 767
Epoch 767 :: Batch 0/1
Batch Loss = 4.437385647730937
767, epoch_train_loss=4.437385647730937
Epoch 768
Epoch 768 :: Batch 0/1
Batch Loss = 4.437385644843941
768, epoch_train_loss=4.437385644843941
Epoch 769
Epoch 769 :: Batch 0/1
Batch Loss = 4.437385641967053
769, epoch_train_loss=4.437385641967053
Epoch 770
Epoch 770 :: Batch 0/1
Batch Loss = 4.437385639100216
770, epoch_train_loss=4.437385639100216
Epoch 771
Epoch 771 :: Batch 0/1
Batch Loss = 4.437385636243371
771, epoch_train_loss=4.437385636243371
Epoch 772
Epoch 772 :: Batch 0/1
Batch Loss = 4.4373856333964605
772, epoch_train_loss=4.4373856333964605
Epoch 773
Epoch 773 :: Batch 0/1
Batch Loss = 4.437385630559426
773, epoch_train_loss=4.437385630559426
Epoch 774
Epoch 774 :: Batch 0/1
Batch Loss = 4.43738562773221
774, epoch_train_loss=4.43738562773221
Epoch 775
Epoch 775 :: Batch 0/1
Batch Loss = 4.437385624914753
775, epoch_train_loss=4.437385624914753
Epoch 776
Epoch 776 :: Batch 0/1
Batch Loss = 4.437385622106999
776, epoch_train_loss=4.437385622106999
Epoch 777
Epoch 777 :: Batch 0/1
Batch Loss = 4.437385619308889
777, epoch_train_loss=4.437385619308889
Epoch 778
Epoch 778 :: Batch 0/1
Batch Loss = 4.43738561652037
778, epoch_train_loss=4.43738561652037
Epoch 779
Epoch 779 :: Batch 0/1
Batch Loss = 4.437385613741379
779, epoch_train_loss=4.437385613741379
Epoch 780
Epoch 780 :: Batch 0/1
Batch Loss = 4.437385610971867
780, epoch_train_loss=4.437385610971867
Epoch 781
Epoch 781 :: Batch 0/1
Batch Loss = 4.437385608211773
781, epoch_train_loss=4.437385608211773
Epoch 782
Epoch 782 :: Batch 0/1
Batch Loss = 4.437385605461047
782, epoch_train_loss=4.437385605461047
Epoch 783
Epoch 783 :: Batch 0/1
Batch Loss = 4.437385602719635
783, epoch_train_loss=4.437385602719635
Epoch 784
Epoch 784 :: Batch 0/1
Batch Loss = 4.43738559998748
784, epoch_train_loss=4.43738559998748
Epoch 785
Epoch 785 :: Batch 0/1
Batch Loss = 4.437385597264533
785, epoch_train_loss=4.437385597264533
Epoch 786
Epoch 786 :: Batch 0/1
Batch Loss = 4.437385594550741
786, epoch_train_loss=4.437385594550741
Epoch 787
Epoch 787 :: Batch 0/1
Batch Loss = 4.437385591846054
787, epoch_train_loss=4.437385591846054
Epoch 788
Epoch 788 :: Batch 0/1
Batch Loss = 4.437385589150418
788, epoch_train_loss=4.437385589150418
Epoch 789
Epoch 789 :: Batch 0/1
Batch Loss = 4.437385586463783
789, epoch_train_loss=4.437385586463783
Epoch 790
Epoch 790 :: Batch 0/1
Batch Loss = 4.437385583786104
790, epoch_train_loss=4.437385583786104
Epoch 791
Epoch 791 :: Batch 0/1
Batch Loss = 4.4373855811173275
791, epoch_train_loss=4.4373855811173275
Epoch 792
Epoch 792 :: Batch 0/1
Batch Loss = 4.437385578457405
792, epoch_train_loss=4.437385578457405
Epoch 793
Epoch 793 :: Batch 0/1
Batch Loss = 4.4373855758062914
793, epoch_train_loss=4.4373855758062914
Epoch 794
Epoch 794 :: Batch 0/1
Batch Loss = 4.437385573163936
794, epoch_train_loss=4.437385573163936
Epoch 795
Epoch 795 :: Batch 0/1
Batch Loss = 4.437385570530292
795, epoch_train_loss=4.437385570530292
Epoch 796
Epoch 796 :: Batch 0/1
Batch Loss = 4.437385567905313
796, epoch_train_loss=4.437385567905313
Epoch 797
Epoch 797 :: Batch 0/1
Batch Loss = 4.437385565288951
797, epoch_train_loss=4.437385565288951
Epoch 798
Epoch 798 :: Batch 0/1
Batch Loss = 4.437385562681163
798, epoch_train_loss=4.437385562681163
Epoch 799
Epoch 799 :: Batch 0/1
Batch Loss = 4.4373855600819
799, epoch_train_loss=4.4373855600819
Epoch 800
Epoch 800 :: Batch 0/1
Batch Loss = 4.437385557491117
800, epoch_train_loss=4.437385557491117
Epoch 801
Epoch 801 :: Batch 0/1
Batch Loss = 4.4373855549087695
801, epoch_train_loss=4.4373855549087695
Epoch 802
Epoch 802 :: Batch 0/1
Batch Loss = 4.437385552334812
802, epoch_train_loss=4.437385552334812
Epoch 803
Epoch 803 :: Batch 0/1
Batch Loss = 4.437385549769199
803, epoch_train_loss=4.437385549769199
Epoch 804
Epoch 804 :: Batch 0/1
Batch Loss = 4.437385547211889
804, epoch_train_loss=4.437385547211889
Epoch 805
Epoch 805 :: Batch 0/1
Batch Loss = 4.437385544662833
805, epoch_train_loss=4.437385544662833
Epoch 806
Epoch 806 :: Batch 0/1
Batch Loss = 4.437385542121992
806, epoch_train_loss=4.437385542121992
Epoch 807
Epoch 807 :: Batch 0/1
Batch Loss = 4.437385539589322
807, epoch_train_loss=4.437385539589322
Epoch 808
Epoch 808 :: Batch 0/1
Batch Loss = 4.437385537064777
808, epoch_train_loss=4.437385537064777
Epoch 809
Epoch 809 :: Batch 0/1
Batch Loss = 4.437385534548314
809, epoch_train_loss=4.437385534548314
Epoch 810
Epoch 810 :: Batch 0/1
Batch Loss = 4.437385532039894
810, epoch_train_loss=4.437385532039894
Epoch 811
Epoch 811 :: Batch 0/1
Batch Loss = 4.437385529539473
811, epoch_train_loss=4.437385529539473
Epoch 812
Epoch 812 :: Batch 0/1
Batch Loss = 4.437385527047008
812, epoch_train_loss=4.437385527047008
Epoch 813
Epoch 813 :: Batch 0/1
Batch Loss = 4.437385524562456
813, epoch_train_loss=4.437385524562456
Epoch 814
Epoch 814 :: Batch 0/1
Batch Loss = 4.437385522085778
814, epoch_train_loss=4.437385522085778
Epoch 815
Epoch 815 :: Batch 0/1
Batch Loss = 4.437385519616931
815, epoch_train_loss=4.437385519616931
Epoch 816
Epoch 816 :: Batch 0/1
Batch Loss = 4.437385517155875
816, epoch_train_loss=4.437385517155875
Epoch 817
Epoch 817 :: Batch 0/1
Batch Loss = 4.4373855147025685
817, epoch_train_loss=4.4373855147025685
Epoch 818
Epoch 818 :: Batch 0/1
Batch Loss = 4.437385512256973
818, epoch_train_loss=4.437385512256973
Epoch 819
Epoch 819 :: Batch 0/1
Batch Loss = 4.437385509819046
819, epoch_train_loss=4.437385509819046
Epoch 820
Epoch 820 :: Batch 0/1
Batch Loss = 4.437385507388748
820, epoch_train_loss=4.437385507388748
Epoch 821
Epoch 821 :: Batch 0/1
Batch Loss = 4.43738550496604
821, epoch_train_loss=4.43738550496604
Epoch 822
Epoch 822 :: Batch 0/1
Batch Loss = 4.437385502550882
822, epoch_train_loss=4.437385502550882
Epoch 823
Epoch 823 :: Batch 0/1
Batch Loss = 4.437385500143235
823, epoch_train_loss=4.437385500143235
Epoch 824
Epoch 824 :: Batch 0/1
Batch Loss = 4.437385497743061
824, epoch_train_loss=4.437385497743061
Epoch 825
Epoch 825 :: Batch 0/1
Batch Loss = 4.437385495350321
825, epoch_train_loss=4.437385495350321
Epoch 826
Epoch 826 :: Batch 0/1
Batch Loss = 4.437385492964977
826, epoch_train_loss=4.437385492964977
Epoch 827
Epoch 827 :: Batch 0/1
Batch Loss = 4.4373854905869905
827, epoch_train_loss=4.4373854905869905
Epoch 828
Epoch 828 :: Batch 0/1
Batch Loss = 4.437385488216322
828, epoch_train_loss=4.437385488216322
Epoch 829
Epoch 829 :: Batch 0/1
Batch Loss = 4.437385485852937
829, epoch_train_loss=4.437385485852937
Epoch 830
Epoch 830 :: Batch 0/1
Batch Loss = 4.437385483496796
830, epoch_train_loss=4.437385483496796
Epoch 831
Epoch 831 :: Batch 0/1
Batch Loss = 4.437385481147865
831, epoch_train_loss=4.437385481147865
Epoch 832
Epoch 832 :: Batch 0/1
Batch Loss = 4.437385478806101
832, epoch_train_loss=4.437385478806101
Epoch 833
Epoch 833 :: Batch 0/1
Batch Loss = 4.4373854764714755
833, epoch_train_loss=4.4373854764714755
Epoch 834
Epoch 834 :: Batch 0/1
Batch Loss = 4.437385474143946
834, epoch_train_loss=4.437385474143946
Epoch 835
Epoch 835 :: Batch 0/1
Batch Loss = 4.437385471823479
835, epoch_train_loss=4.437385471823479
Epoch 836
Epoch 836 :: Batch 0/1
Batch Loss = 4.437385469510038
836, epoch_train_loss=4.437385469510038
Epoch 837
Epoch 837 :: Batch 0/1
Batch Loss = 4.437385467203588
837, epoch_train_loss=4.437385467203588
Epoch 838
Epoch 838 :: Batch 0/1
Batch Loss = 4.437385464904093
838, epoch_train_loss=4.437385464904093
Epoch 839
Epoch 839 :: Batch 0/1
Batch Loss = 4.437385462611519
839, epoch_train_loss=4.437385462611519
Epoch 840
Epoch 840 :: Batch 0/1
Batch Loss = 4.437385460325829
840, epoch_train_loss=4.437385460325829
Epoch 841
Epoch 841 :: Batch 0/1
Batch Loss = 4.4373854580469905
841, epoch_train_loss=4.4373854580469905
Epoch 842
Epoch 842 :: Batch 0/1
Batch Loss = 4.437385455774967
842, epoch_train_loss=4.437385455774967
Epoch 843
Epoch 843 :: Batch 0/1
Batch Loss = 4.437385453509727
843, epoch_train_loss=4.437385453509727
Epoch 844
Epoch 844 :: Batch 0/1
Batch Loss = 4.437385451251232
844, epoch_train_loss=4.437385451251232
Epoch 845
Epoch 845 :: Batch 0/1
Batch Loss = 4.437385448999455
845, epoch_train_loss=4.437385448999455
Epoch 846
Epoch 846 :: Batch 0/1
Batch Loss = 4.437385446754356
846, epoch_train_loss=4.437385446754356
Epoch 847
Epoch 847 :: Batch 0/1
Batch Loss = 4.437385444515905
847, epoch_train_loss=4.437385444515905
Epoch 848
Epoch 848 :: Batch 0/1
Batch Loss = 4.437385442284068
848, epoch_train_loss=4.437385442284068
Epoch 849
Epoch 849 :: Batch 0/1
Batch Loss = 4.4373854400588115
849, epoch_train_loss=4.4373854400588115
Epoch 850
Epoch 850 :: Batch 0/1
Batch Loss = 4.437385437840104
850, epoch_train_loss=4.437385437840104
Epoch 851
Epoch 851 :: Batch 0/1
Batch Loss = 4.437385435627911
851, epoch_train_loss=4.437385435627911
Epoch 852
Epoch 852 :: Batch 0/1
Batch Loss = 4.437385433422202
852, epoch_train_loss=4.437385433422202
Epoch 853
Epoch 853 :: Batch 0/1
Batch Loss = 4.437385431222947
853, epoch_train_loss=4.437385431222947
Epoch 854
Epoch 854 :: Batch 0/1
Batch Loss = 4.437385429030109
854, epoch_train_loss=4.437385429030109
Epoch 855
Epoch 855 :: Batch 0/1
Batch Loss = 4.437385426843658
855, epoch_train_loss=4.437385426843658
Epoch 856
Epoch 856 :: Batch 0/1
Batch Loss = 4.437385424663565
856, epoch_train_loss=4.437385424663565
Epoch 857
Epoch 857 :: Batch 0/1
Batch Loss = 4.437385422489797
857, epoch_train_loss=4.437385422489797
Epoch 858
Epoch 858 :: Batch 0/1
Batch Loss = 4.4373854203223235
858, epoch_train_loss=4.4373854203223235
Epoch 859
Epoch 859 :: Batch 0/1
Batch Loss = 4.437385418161112
859, epoch_train_loss=4.437385418161112
Epoch 860
Epoch 860 :: Batch 0/1
Batch Loss = 4.437385416006133
860, epoch_train_loss=4.437385416006133
Epoch 861
Epoch 861 :: Batch 0/1
Batch Loss = 4.437385413857355
861, epoch_train_loss=4.437385413857355
Epoch 862
Epoch 862 :: Batch 0/1
Batch Loss = 4.43738541171475
862, epoch_train_loss=4.43738541171475
Epoch 863
Epoch 863 :: Batch 0/1
Batch Loss = 4.437385409578285
863, epoch_train_loss=4.437385409578285
Epoch 864
Epoch 864 :: Batch 0/1
Batch Loss = 4.437385407447931
864, epoch_train_loss=4.437385407447931
Epoch 865
Epoch 865 :: Batch 0/1
Batch Loss = 4.43738540532366
865, epoch_train_loss=4.43738540532366
Epoch 866
Epoch 866 :: Batch 0/1
Batch Loss = 4.4373854032054405
866, epoch_train_loss=4.4373854032054405
Epoch 867
Epoch 867 :: Batch 0/1
Batch Loss = 4.437385401093243
867, epoch_train_loss=4.437385401093243
Epoch 868
Epoch 868 :: Batch 0/1
Batch Loss = 4.437385398987039
868, epoch_train_loss=4.437385398987039
Epoch 869
Epoch 869 :: Batch 0/1
Batch Loss = 4.4373853968868
869, epoch_train_loss=4.4373853968868
Epoch 870
Epoch 870 :: Batch 0/1
Batch Loss = 4.4373853947924955
870, epoch_train_loss=4.4373853947924955
Epoch 871
Epoch 871 :: Batch 0/1
Batch Loss = 4.437385392704098
871, epoch_train_loss=4.437385392704098
Epoch 872
Epoch 872 :: Batch 0/1
Batch Loss = 4.4373853906215786
872, epoch_train_loss=4.4373853906215786
Epoch 873
Epoch 873 :: Batch 0/1
Batch Loss = 4.437385388544908
873, epoch_train_loss=4.437385388544908
Epoch 874
Epoch 874 :: Batch 0/1
Batch Loss = 4.437385386474061
874, epoch_train_loss=4.437385386474061
Epoch 875
Epoch 875 :: Batch 0/1
Batch Loss = 4.437385384409007
875, epoch_train_loss=4.437385384409007
Epoch 876
Epoch 876 :: Batch 0/1
Batch Loss = 4.437385382349718
876, epoch_train_loss=4.437385382349718
Epoch 877
Epoch 877 :: Batch 0/1
Batch Loss = 4.43738538029617
877, epoch_train_loss=4.43738538029617
Epoch 878
Epoch 878 :: Batch 0/1
Batch Loss = 4.4373853782483295
878, epoch_train_loss=4.4373853782483295
Epoch 879
Epoch 879 :: Batch 0/1
Batch Loss = 4.437385376206173
879, epoch_train_loss=4.437385376206173
Epoch 880
Epoch 880 :: Batch 0/1
Batch Loss = 4.437385374169673
880, epoch_train_loss=4.437385374169673
Epoch 881
Epoch 881 :: Batch 0/1
Batch Loss = 4.437385372138802
881, epoch_train_loss=4.437385372138802
Epoch 882
Epoch 882 :: Batch 0/1
Batch Loss = 4.437385370113534
882, epoch_train_loss=4.437385370113534
Epoch 883
Epoch 883 :: Batch 0/1
Batch Loss = 4.4373853680938415
883, epoch_train_loss=4.4373853680938415
Epoch 884
Epoch 884 :: Batch 0/1
Batch Loss = 4.437385366079698
884, epoch_train_loss=4.437385366079698
Epoch 885
Epoch 885 :: Batch 0/1
Batch Loss = 4.437385364071077
885, epoch_train_loss=4.437385364071077
Epoch 886
Epoch 886 :: Batch 0/1
Batch Loss = 4.437385362067953
886, epoch_train_loss=4.437385362067953
Epoch 887
Epoch 887 :: Batch 0/1
Batch Loss = 4.4373853600703
887, epoch_train_loss=4.4373853600703
Epoch 888
Epoch 888 :: Batch 0/1
Batch Loss = 4.43738535807809
888, epoch_train_loss=4.43738535807809
Epoch 889
Epoch 889 :: Batch 0/1
Batch Loss = 4.4373853560912995
889, epoch_train_loss=4.4373853560912995
Epoch 890
Epoch 890 :: Batch 0/1
Batch Loss = 4.437385354109903
890, epoch_train_loss=4.437385354109903
Epoch 891
Epoch 891 :: Batch 0/1
Batch Loss = 4.437385352133873
891, epoch_train_loss=4.437385352133873
Epoch 892
Epoch 892 :: Batch 0/1
Batch Loss = 4.437385350163187
892, epoch_train_loss=4.437385350163187
Epoch 893
Epoch 893 :: Batch 0/1
Batch Loss = 4.437385348197818
893, epoch_train_loss=4.437385348197818
Epoch 894
Epoch 894 :: Batch 0/1
Batch Loss = 4.4373853462377415
894, epoch_train_loss=4.4373853462377415
Epoch 895
Epoch 895 :: Batch 0/1
Batch Loss = 4.437385344282933
895, epoch_train_loss=4.437385344282933
Epoch 896
Epoch 896 :: Batch 0/1
Batch Loss = 4.437385342333366
896, epoch_train_loss=4.437385342333366
Epoch 897
Epoch 897 :: Batch 0/1
Batch Loss = 4.437385340389018
897, epoch_train_loss=4.437385340389018
Epoch 898
Epoch 898 :: Batch 0/1
Batch Loss = 4.4373853384498645
898, epoch_train_loss=4.4373853384498645
Epoch 899
Epoch 899 :: Batch 0/1
Batch Loss = 4.43738533651588
899, epoch_train_loss=4.43738533651588
Epoch 900
Epoch 900 :: Batch 0/1
Batch Loss = 4.437385334587041
900, epoch_train_loss=4.437385334587041
Epoch 901
Epoch 901 :: Batch 0/1
Batch Loss = 4.437385332663323
901, epoch_train_loss=4.437385332663323
Epoch 902
Epoch 902 :: Batch 0/1
Batch Loss = 4.437385330744703
902, epoch_train_loss=4.437385330744703
Epoch 903
Epoch 903 :: Batch 0/1
Batch Loss = 4.437385328831157
903, epoch_train_loss=4.437385328831157
Epoch 904
Epoch 904 :: Batch 0/1
Batch Loss = 4.43738532692266
904, epoch_train_loss=4.43738532692266
Epoch 905
Epoch 905 :: Batch 0/1
Batch Loss = 4.437385325019193
905, epoch_train_loss=4.437385325019193
Epoch 906
Epoch 906 :: Batch 0/1
Batch Loss = 4.4373853231207265
906, epoch_train_loss=4.4373853231207265
Epoch 907
Epoch 907 :: Batch 0/1
Batch Loss = 4.437385321227242
907, epoch_train_loss=4.437385321227242
Epoch 908
Epoch 908 :: Batch 0/1
Batch Loss = 4.437385319338714
908, epoch_train_loss=4.437385319338714
Epoch 909
Epoch 909 :: Batch 0/1
Batch Loss = 4.437385317455122
909, epoch_train_loss=4.437385317455122
Epoch 910
Epoch 910 :: Batch 0/1
Batch Loss = 4.43738531557644
910, epoch_train_loss=4.43738531557644
Epoch 911
Epoch 911 :: Batch 0/1
Batch Loss = 4.437385313702646
911, epoch_train_loss=4.437385313702646
Epoch 912
Epoch 912 :: Batch 0/1
Batch Loss = 4.43738531183372
912, epoch_train_loss=4.43738531183372
Epoch 913
Epoch 913 :: Batch 0/1
Batch Loss = 4.437385309969638
913, epoch_train_loss=4.437385309969638
Epoch 914
Epoch 914 :: Batch 0/1
Batch Loss = 4.437385308110378
914, epoch_train_loss=4.437385308110378
Epoch 915
Epoch 915 :: Batch 0/1
Batch Loss = 4.4373853062559165
915, epoch_train_loss=4.4373853062559165
Epoch 916
Epoch 916 :: Batch 0/1
Batch Loss = 4.437385304406233
916, epoch_train_loss=4.437385304406233
Epoch 917
Epoch 917 :: Batch 0/1
Batch Loss = 4.437385302561305
917, epoch_train_loss=4.437385302561305
Epoch 918
Epoch 918 :: Batch 0/1
Batch Loss = 4.437385300721112
918, epoch_train_loss=4.437385300721112
Epoch 919
Epoch 919 :: Batch 0/1
Batch Loss = 4.437385298885631
919, epoch_train_loss=4.437385298885631
Epoch 920
Epoch 920 :: Batch 0/1
Batch Loss = 4.437385297054839
920, epoch_train_loss=4.437385297054839
Epoch 921
Epoch 921 :: Batch 0/1
Batch Loss = 4.437385295228718
921, epoch_train_loss=4.437385295228718
Epoch 922
Epoch 922 :: Batch 0/1
Batch Loss = 4.437385293407243
922, epoch_train_loss=4.437385293407243
Epoch 923
Epoch 923 :: Batch 0/1
Batch Loss = 4.437385291590397
923, epoch_train_loss=4.437385291590397
Epoch 924
Epoch 924 :: Batch 0/1
Batch Loss = 4.437385289778155
924, epoch_train_loss=4.437385289778155
Epoch 925
Epoch 925 :: Batch 0/1
Batch Loss = 4.437385287970499
925, epoch_train_loss=4.437385287970499
Epoch 926
Epoch 926 :: Batch 0/1
Batch Loss = 4.437385286167407
926, epoch_train_loss=4.437385286167407
Epoch 927
Epoch 927 :: Batch 0/1
Batch Loss = 4.437385284368857
927, epoch_train_loss=4.437385284368857
Epoch 928
Epoch 928 :: Batch 0/1
Batch Loss = 4.43738528257483
928, epoch_train_loss=4.43738528257483
Epoch 929
Epoch 929 :: Batch 0/1
Batch Loss = 4.437385280785305
929, epoch_train_loss=4.437385280785305
Epoch 930
Epoch 930 :: Batch 0/1
Batch Loss = 4.437385279000264
930, epoch_train_loss=4.437385279000264
Epoch 931
Epoch 931 :: Batch 0/1
Batch Loss = 4.437385277219682
931, epoch_train_loss=4.437385277219682
Epoch 932
Epoch 932 :: Batch 0/1
Batch Loss = 4.4373852754435426
932, epoch_train_loss=4.4373852754435426
Epoch 933
Epoch 933 :: Batch 0/1
Batch Loss = 4.437385273671822
933, epoch_train_loss=4.437385273671822
Epoch 934
Epoch 934 :: Batch 0/1
Batch Loss = 4.437385271904506
934, epoch_train_loss=4.437385271904506
Epoch 935
Epoch 935 :: Batch 0/1
Batch Loss = 4.43738527014157
935, epoch_train_loss=4.43738527014157
Epoch 936
Epoch 936 :: Batch 0/1
Batch Loss = 4.437385268382995
936, epoch_train_loss=4.437385268382995
Epoch 937
Epoch 937 :: Batch 0/1
Batch Loss = 4.4373852666287625
937, epoch_train_loss=4.4373852666287625
Epoch 938
Epoch 938 :: Batch 0/1
Batch Loss = 4.437385264878855
938, epoch_train_loss=4.437385264878855
Epoch 939
Epoch 939 :: Batch 0/1
Batch Loss = 4.437385263133247
939, epoch_train_loss=4.437385263133247
Epoch 940
Epoch 940 :: Batch 0/1
Batch Loss = 4.437385261391924
940, epoch_train_loss=4.437385261391924
Epoch 941
Epoch 941 :: Batch 0/1
Batch Loss = 4.437385259654867
941, epoch_train_loss=4.437385259654867
Epoch 942
Epoch 942 :: Batch 0/1
Batch Loss = 4.437385257922055
942, epoch_train_loss=4.437385257922055
Epoch 943
Epoch 943 :: Batch 0/1
Batch Loss = 4.437385256193471
943, epoch_train_loss=4.437385256193471
Epoch 944
Epoch 944 :: Batch 0/1
Batch Loss = 4.437385254469093
944, epoch_train_loss=4.437385254469093
Epoch 945
Epoch 945 :: Batch 0/1
Batch Loss = 4.4373852527489035
945, epoch_train_loss=4.4373852527489035
Epoch 946
Epoch 946 :: Batch 0/1
Batch Loss = 4.437385251032885
946, epoch_train_loss=4.437385251032885
Epoch 947
Epoch 947 :: Batch 0/1
Batch Loss = 4.437385249321019
947, epoch_train_loss=4.437385249321019
Epoch 948
Epoch 948 :: Batch 0/1
Batch Loss = 4.437385247613285
948, epoch_train_loss=4.437385247613285
Epoch 949
Epoch 949 :: Batch 0/1
Batch Loss = 4.437385245909668
949, epoch_train_loss=4.437385245909668
Epoch 950
Epoch 950 :: Batch 0/1
Batch Loss = 4.437385244210145
950, epoch_train_loss=4.437385244210145
Epoch 951
Epoch 951 :: Batch 0/1
Batch Loss = 4.437385242514702
951, epoch_train_loss=4.437385242514702
Epoch 952
Epoch 952 :: Batch 0/1
Batch Loss = 4.437385240823317
952, epoch_train_loss=4.437385240823317
Epoch 953
Epoch 953 :: Batch 0/1
Batch Loss = 4.437385239135976
953, epoch_train_loss=4.437385239135976
Epoch 954
Epoch 954 :: Batch 0/1
Batch Loss = 4.437385237452659
954, epoch_train_loss=4.437385237452659
Epoch 955
Epoch 955 :: Batch 0/1
Batch Loss = 4.437385235773348
955, epoch_train_loss=4.437385235773348
Epoch 956
Epoch 956 :: Batch 0/1
Batch Loss = 4.437385234098026
956, epoch_train_loss=4.437385234098026
Epoch 957
Epoch 957 :: Batch 0/1
Batch Loss = 4.437385232426673
957, epoch_train_loss=4.437385232426673
Epoch 958
Epoch 958 :: Batch 0/1
Batch Loss = 4.4373852307592765
958, epoch_train_loss=4.4373852307592765
Epoch 959
Epoch 959 :: Batch 0/1
Batch Loss = 4.437385229095815
959, epoch_train_loss=4.437385229095815
Epoch 960
Epoch 960 :: Batch 0/1
Batch Loss = 4.4373852274362715
960, epoch_train_loss=4.4373852274362715
Epoch 961
Epoch 961 :: Batch 0/1
Batch Loss = 4.437385225780631
961, epoch_train_loss=4.437385225780631
Epoch 962
Epoch 962 :: Batch 0/1
Batch Loss = 4.437385224128874
962, epoch_train_loss=4.437385224128874
Epoch 963
Epoch 963 :: Batch 0/1
Batch Loss = 4.437385222480984
963, epoch_train_loss=4.437385222480984
Epoch 964
Epoch 964 :: Batch 0/1
Batch Loss = 4.437385220836943
964, epoch_train_loss=4.437385220836943
Epoch 965
Epoch 965 :: Batch 0/1
Batch Loss = 4.437385219196736
965, epoch_train_loss=4.437385219196736
Epoch 966
Epoch 966 :: Batch 0/1
Batch Loss = 4.437385217560346
966, epoch_train_loss=4.437385217560346
Epoch 967
Epoch 967 :: Batch 0/1
Batch Loss = 4.437385215927755
967, epoch_train_loss=4.437385215927755
Epoch 968
Epoch 968 :: Batch 0/1
Batch Loss = 4.4373852142989465
968, epoch_train_loss=4.4373852142989465
Epoch 969
Epoch 969 :: Batch 0/1
Batch Loss = 4.437385212673904
969, epoch_train_loss=4.437385212673904
Epoch 970
Epoch 970 :: Batch 0/1
Batch Loss = 4.437385211052612
970, epoch_train_loss=4.437385211052612
Epoch 971
Epoch 971 :: Batch 0/1
Batch Loss = 4.437385209435053
971, epoch_train_loss=4.437385209435053
Epoch 972
Epoch 972 :: Batch 0/1
Batch Loss = 4.437385207821212
972, epoch_train_loss=4.437385207821212
Epoch 973
Epoch 973 :: Batch 0/1
Batch Loss = 4.437385206211071
973, epoch_train_loss=4.437385206211071
Epoch 974
Epoch 974 :: Batch 0/1
Batch Loss = 4.437385204604615
974, epoch_train_loss=4.437385204604615
Epoch 975
Epoch 975 :: Batch 0/1
Batch Loss = 4.437385203001829
975, epoch_train_loss=4.437385203001829
Epoch 976
Epoch 976 :: Batch 0/1
Batch Loss = 4.4373852014026935
976, epoch_train_loss=4.4373852014026935
Epoch 977
Epoch 977 :: Batch 0/1
Batch Loss = 4.437385199807194
977, epoch_train_loss=4.437385199807194
Epoch 978
Epoch 978 :: Batch 0/1
Batch Loss = 4.437385198215316
978, epoch_train_loss=4.437385198215316
Epoch 979
Epoch 979 :: Batch 0/1
Batch Loss = 4.437385196627044
979, epoch_train_loss=4.437385196627044
Epoch 980
Epoch 980 :: Batch 0/1
Batch Loss = 4.437385195042362
980, epoch_train_loss=4.437385195042362
Epoch 981
Epoch 981 :: Batch 0/1
Batch Loss = 4.437385193461251
981, epoch_train_loss=4.437385193461251
Epoch 982
Epoch 982 :: Batch 0/1
Batch Loss = 4.437385191883701
982, epoch_train_loss=4.437385191883701
Epoch 983
Epoch 983 :: Batch 0/1
Batch Loss = 4.437385190309692
983, epoch_train_loss=4.437385190309692
Epoch 984
Epoch 984 :: Batch 0/1
Batch Loss = 4.437385188739211
984, epoch_train_loss=4.437385188739211
Epoch 985
Epoch 985 :: Batch 0/1
Batch Loss = 4.437385187172242
985, epoch_train_loss=4.437385187172242
Epoch 986
Epoch 986 :: Batch 0/1
Batch Loss = 4.43738518560877
986, epoch_train_loss=4.43738518560877
Epoch 987
Epoch 987 :: Batch 0/1
Batch Loss = 4.437385184048779
987, epoch_train_loss=4.437385184048779
Epoch 988
Epoch 988 :: Batch 0/1
Batch Loss = 4.437385182492255
988, epoch_train_loss=4.437385182492255
Epoch 989
Epoch 989 :: Batch 0/1
Batch Loss = 4.4373851809391835
989, epoch_train_loss=4.4373851809391835
Epoch 990
Epoch 990 :: Batch 0/1
Batch Loss = 4.437385179389548
990, epoch_train_loss=4.437385179389548
Epoch 991
Epoch 991 :: Batch 0/1
Batch Loss = 4.437385177843334
991, epoch_train_loss=4.437385177843334
Epoch 992
Epoch 992 :: Batch 0/1
Batch Loss = 4.437385176300527
992, epoch_train_loss=4.437385176300527
Epoch 993
Epoch 993 :: Batch 0/1
Batch Loss = 4.437385174761112
993, epoch_train_loss=4.437385174761112
Epoch 994
Epoch 994 :: Batch 0/1
Batch Loss = 4.437385173225076
994, epoch_train_loss=4.437385173225076
Epoch 995
Epoch 995 :: Batch 0/1
Batch Loss = 4.437385171692401
995, epoch_train_loss=4.437385171692401
Epoch 996
Epoch 996 :: Batch 0/1
Batch Loss = 4.437385170163076
996, epoch_train_loss=4.437385170163076
Epoch 997
Epoch 997 :: Batch 0/1
Batch Loss = 4.437385168637085
997, epoch_train_loss=4.437385168637085
Epoch 998
Epoch 998 :: Batch 0/1
Batch Loss = 4.437385167114415
998, epoch_train_loss=4.437385167114415
Epoch 999
Epoch 999 :: Batch 0/1
Batch Loss = 4.437385165595047
999, epoch_train_loss=4.437385165595047
Epoch 1000
Epoch 1000 :: Batch 0/1
Batch Loss = 4.437385164078973
1000, epoch_train_loss=4.437385164078973
Epoch 1001
Epoch 1001 :: Batch 0/1
Batch Loss = 4.437385162566176
1001, epoch_train_loss=4.437385162566176
Epoch 1002
Epoch 1002 :: Batch 0/1
Batch Loss = 4.437385161056642
1002, epoch_train_loss=4.437385161056642
Epoch 1003
Epoch 1003 :: Batch 0/1
Batch Loss = 4.437385159550357
1003, epoch_train_loss=4.437385159550357
Epoch 1004
Epoch 1004 :: Batch 0/1
Batch Loss = 4.437385158047307
1004, epoch_train_loss=4.437385158047307
Epoch 1005
Epoch 1005 :: Batch 0/1
Batch Loss = 4.437385156547478
1005, epoch_train_loss=4.437385156547478
Epoch 1006
Epoch 1006 :: Batch 0/1
Batch Loss = 4.437385155050855
1006, epoch_train_loss=4.437385155050855
Epoch 1007
Epoch 1007 :: Batch 0/1
Batch Loss = 4.437385153557427
1007, epoch_train_loss=4.437385153557427
Epoch 1008
Epoch 1008 :: Batch 0/1
Batch Loss = 4.437385152067179
1008, epoch_train_loss=4.437385152067179
Epoch 1009
Epoch 1009 :: Batch 0/1
Batch Loss = 4.437385150580097
1009, epoch_train_loss=4.437385150580097
Epoch 1010
Epoch 1010 :: Batch 0/1
Batch Loss = 4.437385149096167
1010, epoch_train_loss=4.437385149096167
Epoch 1011
Epoch 1011 :: Batch 0/1
Batch Loss = 4.437385147615378
1011, epoch_train_loss=4.437385147615378
Epoch 1012
Epoch 1012 :: Batch 0/1
Batch Loss = 4.4373851461377125
1012, epoch_train_loss=4.4373851461377125
Epoch 1013
Epoch 1013 :: Batch 0/1
Batch Loss = 4.4373851446631605
1013, epoch_train_loss=4.4373851446631605
Epoch 1014
Epoch 1014 :: Batch 0/1
Batch Loss = 4.437385143191708
1014, epoch_train_loss=4.437385143191708
Epoch 1015
Epoch 1015 :: Batch 0/1
Batch Loss = 4.4373851417233405
1015, epoch_train_loss=4.4373851417233405
Epoch 1016
Epoch 1016 :: Batch 0/1
Batch Loss = 4.437385140258046
1016, epoch_train_loss=4.437385140258046
Epoch 1017
Epoch 1017 :: Batch 0/1
Batch Loss = 4.437385138795811
1017, epoch_train_loss=4.437385138795811
Epoch 1018
Epoch 1018 :: Batch 0/1
Batch Loss = 4.437385137336622
1018, epoch_train_loss=4.437385137336622
Epoch 1019
Epoch 1019 :: Batch 0/1
Batch Loss = 4.437385135880467
1019, epoch_train_loss=4.437385135880467
Epoch 1020
Epoch 1020 :: Batch 0/1
Batch Loss = 4.437385134427332
1020, epoch_train_loss=4.437385134427332
Epoch 1021
Epoch 1021 :: Batch 0/1
Batch Loss = 4.437385132977206
1021, epoch_train_loss=4.437385132977206
Epoch 1022
Epoch 1022 :: Batch 0/1
Batch Loss = 4.437385131530074
1022, epoch_train_loss=4.437385131530074
Epoch 1023
Epoch 1023 :: Batch 0/1
Batch Loss = 4.437385130085923
1023, epoch_train_loss=4.437385130085923
Epoch 1024
Epoch 1024 :: Batch 0/1
Batch Loss = 4.437385128644742
1024, epoch_train_loss=4.437385128644742
Epoch 1025
Epoch 1025 :: Batch 0/1
Batch Loss = 4.437385127206517
1025, epoch_train_loss=4.437385127206517
Epoch 1026
Epoch 1026 :: Batch 0/1
Batch Loss = 4.437385125771237
1026, epoch_train_loss=4.437385125771237
Epoch 1027
Epoch 1027 :: Batch 0/1
Batch Loss = 4.4373851243388875
1027, epoch_train_loss=4.4373851243388875
Epoch 1028
Epoch 1028 :: Batch 0/1
Batch Loss = 4.4373851229094585
1028, epoch_train_loss=4.4373851229094585
Epoch 1029
Epoch 1029 :: Batch 0/1
Batch Loss = 4.437385121482935
1029, epoch_train_loss=4.437385121482935
Epoch 1030
Epoch 1030 :: Batch 0/1
Batch Loss = 4.437385120059306
1030, epoch_train_loss=4.437385120059306
Epoch 1031
Epoch 1031 :: Batch 0/1
Batch Loss = 4.4373851186385584
1031, epoch_train_loss=4.4373851186385584
Epoch 1032
Epoch 1032 :: Batch 0/1
Batch Loss = 4.437385117220682
1032, epoch_train_loss=4.437385117220682
Epoch 1033
Epoch 1033 :: Batch 0/1
Batch Loss = 4.437385115805662
1033, epoch_train_loss=4.437385115805662
Epoch 1034
Epoch 1034 :: Batch 0/1
Batch Loss = 4.437385114393487
1034, epoch_train_loss=4.437385114393487
Epoch 1035
Epoch 1035 :: Batch 0/1
Batch Loss = 4.4373851129841455
1035, epoch_train_loss=4.4373851129841455
Epoch 1036
Epoch 1036 :: Batch 0/1
Batch Loss = 4.437385111577626
1036, epoch_train_loss=4.437385111577626
Epoch 1037
Epoch 1037 :: Batch 0/1
Batch Loss = 4.437385110173914
1037, epoch_train_loss=4.437385110173914
Epoch 1038
Epoch 1038 :: Batch 0/1
Batch Loss = 4.437385108773001
1038, epoch_train_loss=4.437385108773001
Epoch 1039
Epoch 1039 :: Batch 0/1
Batch Loss = 4.4373851073748725
1039, epoch_train_loss=4.4373851073748725
Epoch 1040
Epoch 1040 :: Batch 0/1
Batch Loss = 4.437385105979518
1040, epoch_train_loss=4.437385105979518
Epoch 1041
Epoch 1041 :: Batch 0/1
Batch Loss = 4.437385104586925
1041, epoch_train_loss=4.437385104586925
Epoch 1042
Epoch 1042 :: Batch 0/1
Batch Loss = 4.437385103197082
1042, epoch_train_loss=4.437385103197082
Epoch 1043
Epoch 1043 :: Batch 0/1
Batch Loss = 4.437385101809977
1043, epoch_train_loss=4.437385101809977
Epoch 1044
Epoch 1044 :: Batch 0/1
Batch Loss = 4.437385100425599
1044, epoch_train_loss=4.437385100425599
Epoch 1045
Epoch 1045 :: Batch 0/1
Batch Loss = 4.437385099043937
1045, epoch_train_loss=4.437385099043937
Epoch 1046
Epoch 1046 :: Batch 0/1
Batch Loss = 4.437385097664979
1046, epoch_train_loss=4.437385097664979
Epoch 1047
Epoch 1047 :: Batch 0/1
Batch Loss = 4.437385096288712
1047, epoch_train_loss=4.437385096288712
Epoch 1048
Epoch 1048 :: Batch 0/1
Batch Loss = 4.437385094915126
1048, epoch_train_loss=4.437385094915126
Epoch 1049
Epoch 1049 :: Batch 0/1
Batch Loss = 4.4373850935442105
1049, epoch_train_loss=4.4373850935442105
Epoch 1050
Epoch 1050 :: Batch 0/1
Batch Loss = 4.437385092175952
1050, epoch_train_loss=4.437385092175952
Epoch 1051
Epoch 1051 :: Batch 0/1
Batch Loss = 4.43738509081034
1051, epoch_train_loss=4.43738509081034
Epoch 1052
Epoch 1052 :: Batch 0/1
Batch Loss = 4.437385089447365
1052, epoch_train_loss=4.437385089447365
Epoch 1053
Epoch 1053 :: Batch 0/1
Batch Loss = 4.437385088087014
1053, epoch_train_loss=4.437385088087014
Epoch 1054
Epoch 1054 :: Batch 0/1
Batch Loss = 4.437385086729276
1054, epoch_train_loss=4.437385086729276
Epoch 1055
Epoch 1055 :: Batch 0/1
Batch Loss = 4.437385085374141
1055, epoch_train_loss=4.437385085374141
Epoch 1056
Epoch 1056 :: Batch 0/1
Batch Loss = 4.437385084021596
1056, epoch_train_loss=4.437385084021596
Epoch 1057
Epoch 1057 :: Batch 0/1
Batch Loss = 4.437385082671632
1057, epoch_train_loss=4.437385082671632
Epoch 1058
Epoch 1058 :: Batch 0/1
Batch Loss = 4.437385081324236
1058, epoch_train_loss=4.437385081324236
Epoch 1059
Epoch 1059 :: Batch 0/1
Batch Loss = 4.437385079979401
1059, epoch_train_loss=4.437385079979401
Epoch 1060
Epoch 1060 :: Batch 0/1
Batch Loss = 4.437385078637113
1060, epoch_train_loss=4.437385078637113
Epoch 1061
Epoch 1061 :: Batch 0/1
Batch Loss = 4.43738507729736
1061, epoch_train_loss=4.43738507729736
Epoch 1062
Epoch 1062 :: Batch 0/1
Batch Loss = 4.437385075960135
1062, epoch_train_loss=4.437385075960135
Epoch 1063
Epoch 1063 :: Batch 0/1
Batch Loss = 4.437385074625424
1063, epoch_train_loss=4.437385074625424
Epoch 1064
Epoch 1064 :: Batch 0/1
Batch Loss = 4.437385073293218
1064, epoch_train_loss=4.437385073293218
Epoch 1065
Epoch 1065 :: Batch 0/1
Batch Loss = 4.437385071963506
1065, epoch_train_loss=4.437385071963506
Epoch 1066
Epoch 1066 :: Batch 0/1
Batch Loss = 4.437385070636278
1066, epoch_train_loss=4.437385070636278
Epoch 1067
Epoch 1067 :: Batch 0/1
Batch Loss = 4.4373850693115235
1067, epoch_train_loss=4.4373850693115235
Epoch 1068
Epoch 1068 :: Batch 0/1
Batch Loss = 4.437385067989229
1068, epoch_train_loss=4.437385067989229
Epoch 1069
Epoch 1069 :: Batch 0/1
Batch Loss = 4.437385066669389
1069, epoch_train_loss=4.437385066669389
Epoch 1070
Epoch 1070 :: Batch 0/1
Batch Loss = 4.43738506535199
1070, epoch_train_loss=4.43738506535199
Epoch 1071
Epoch 1071 :: Batch 0/1
Batch Loss = 4.437385064037022
1071, epoch_train_loss=4.437385064037022
Epoch 1072
Epoch 1072 :: Batch 0/1
Batch Loss = 4.437385062724475
1072, epoch_train_loss=4.437385062724475
Epoch 1073
Epoch 1073 :: Batch 0/1
Batch Loss = 4.437385061414339
1073, epoch_train_loss=4.437385061414339
Epoch 1074
Epoch 1074 :: Batch 0/1
Batch Loss = 4.437385060106604
1074, epoch_train_loss=4.437385060106604
Epoch 1075
Epoch 1075 :: Batch 0/1
Batch Loss = 4.437385058801259
1075, epoch_train_loss=4.437385058801259
Epoch 1076
Epoch 1076 :: Batch 0/1
Batch Loss = 4.437385057498294
1076, epoch_train_loss=4.437385057498294
Epoch 1077
Epoch 1077 :: Batch 0/1
Batch Loss = 4.437385056197699
1077, epoch_train_loss=4.437385056197699
Epoch 1078
Epoch 1078 :: Batch 0/1
Batch Loss = 4.437385054899465
1078, epoch_train_loss=4.437385054899465
Epoch 1079
Epoch 1079 :: Batch 0/1
Batch Loss = 4.437385053603579
1079, epoch_train_loss=4.437385053603579
Epoch 1080
Epoch 1080 :: Batch 0/1
Batch Loss = 4.437385052310035
1080, epoch_train_loss=4.437385052310035
Epoch 1081
Epoch 1081 :: Batch 0/1
Batch Loss = 4.437385051018819
1081, epoch_train_loss=4.437385051018819
Epoch 1082
Epoch 1082 :: Batch 0/1
Batch Loss = 4.437385049729926
1082, epoch_train_loss=4.437385049729926
Epoch 1083
Epoch 1083 :: Batch 0/1
Batch Loss = 4.437385048443342
1083, epoch_train_loss=4.437385048443342
Epoch 1084
Epoch 1084 :: Batch 0/1
Batch Loss = 4.437385047159059
1084, epoch_train_loss=4.437385047159059
Epoch 1085
Epoch 1085 :: Batch 0/1
Batch Loss = 4.4373850458770665
1085, epoch_train_loss=4.4373850458770665
Epoch 1086
Epoch 1086 :: Batch 0/1
Batch Loss = 4.437385044597353
1086, epoch_train_loss=4.437385044597353
Epoch 1087
Epoch 1087 :: Batch 0/1
Batch Loss = 4.437385043319915
1087, epoch_train_loss=4.437385043319915
Epoch 1088
Epoch 1088 :: Batch 0/1
Batch Loss = 4.437385042044735
1088, epoch_train_loss=4.437385042044735
Epoch 1089
Epoch 1089 :: Batch 0/1
Batch Loss = 4.437385040771808
1089, epoch_train_loss=4.437385040771808
Epoch 1090
Epoch 1090 :: Batch 0/1
Batch Loss = 4.437385039501124
1090, epoch_train_loss=4.437385039501124
Epoch 1091
Epoch 1091 :: Batch 0/1
Batch Loss = 4.437385038232672
1091, epoch_train_loss=4.437385038232672
Epoch 1092
Epoch 1092 :: Batch 0/1
Batch Loss = 4.437385036966445
1092, epoch_train_loss=4.437385036966445
Epoch 1093
Epoch 1093 :: Batch 0/1
Batch Loss = 4.437385035702432
1093, epoch_train_loss=4.437385035702432
Epoch 1094
Epoch 1094 :: Batch 0/1
Batch Loss = 4.437385034440622
1094, epoch_train_loss=4.437385034440622
Epoch 1095
Epoch 1095 :: Batch 0/1
Batch Loss = 4.437385033181008
1095, epoch_train_loss=4.437385033181008
Epoch 1096
Epoch 1096 :: Batch 0/1
Batch Loss = 4.43738503192358
1096, epoch_train_loss=4.43738503192358
Epoch 1097
Epoch 1097 :: Batch 0/1
Batch Loss = 4.437385030668327
1097, epoch_train_loss=4.437385030668327
Epoch 1098
Epoch 1098 :: Batch 0/1
Batch Loss = 4.4373850294152435
1098, epoch_train_loss=4.4373850294152435
Epoch 1099
Epoch 1099 :: Batch 0/1
Batch Loss = 4.437385028164318
1099, epoch_train_loss=4.437385028164318
Epoch 1100
Epoch 1100 :: Batch 0/1
Batch Loss = 4.437385026915539
1100, epoch_train_loss=4.437385026915539
Epoch 1101
Epoch 1101 :: Batch 0/1
Batch Loss = 4.437385025668901
1101, epoch_train_loss=4.437385025668901
Epoch 1102
Epoch 1102 :: Batch 0/1
Batch Loss = 4.437385024424393
1102, epoch_train_loss=4.437385024424393
Epoch 1103
Epoch 1103 :: Batch 0/1
Batch Loss = 4.437385023182005
1103, epoch_train_loss=4.437385023182005
Epoch 1104
Epoch 1104 :: Batch 0/1
Batch Loss = 4.437385021941731
1104, epoch_train_loss=4.437385021941731
Epoch 1105
Epoch 1105 :: Batch 0/1
Batch Loss = 4.437385020703561
1105, epoch_train_loss=4.437385020703561
Epoch 1106
Epoch 1106 :: Batch 0/1
Batch Loss = 4.437385019467483
1106, epoch_train_loss=4.437385019467483
Epoch 1107
Epoch 1107 :: Batch 0/1
Batch Loss = 4.437385018233492
1107, epoch_train_loss=4.437385018233492
Epoch 1108
Epoch 1108 :: Batch 0/1
Batch Loss = 4.437385017001575
1108, epoch_train_loss=4.437385017001575
Epoch 1109
Epoch 1109 :: Batch 0/1
Batch Loss = 4.437385015771728
1109, epoch_train_loss=4.437385015771728
Epoch 1110
Epoch 1110 :: Batch 0/1
Batch Loss = 4.4373850145439375
1110, epoch_train_loss=4.4373850145439375
Epoch 1111
Epoch 1111 :: Batch 0/1
Batch Loss = 4.437385013318198
1111, epoch_train_loss=4.437385013318198
Epoch 1112
Epoch 1112 :: Batch 0/1
Batch Loss = 4.4373850120944995
1112, epoch_train_loss=4.4373850120944995
Epoch 1113
Epoch 1113 :: Batch 0/1
Batch Loss = 4.437385010872832
1113, epoch_train_loss=4.437385010872832
Epoch 1114
Epoch 1114 :: Batch 0/1
Batch Loss = 4.43738500965319
1114, epoch_train_loss=4.43738500965319
Epoch 1115
Epoch 1115 :: Batch 0/1
Batch Loss = 4.437385008435561
1115, epoch_train_loss=4.437385008435561
Epoch 1116
Epoch 1116 :: Batch 0/1
Batch Loss = 4.437385007219938
1116, epoch_train_loss=4.437385007219938
Epoch 1117
Epoch 1117 :: Batch 0/1
Batch Loss = 4.437385006006312
1117, epoch_train_loss=4.437385006006312
Epoch 1118
Epoch 1118 :: Batch 0/1
Batch Loss = 4.437385004794677
1118, epoch_train_loss=4.437385004794677
Epoch 1119
Epoch 1119 :: Batch 0/1
Batch Loss = 4.437385003585021
1119, epoch_train_loss=4.437385003585021
Epoch 1120
Epoch 1120 :: Batch 0/1
Batch Loss = 4.437385002377336
1120, epoch_train_loss=4.437385002377336
Epoch 1121
Epoch 1121 :: Batch 0/1
Batch Loss = 4.437385001171615
1121, epoch_train_loss=4.437385001171615
Epoch 1122
Epoch 1122 :: Batch 0/1
Batch Loss = 4.437384999967848
1122, epoch_train_loss=4.437384999967848
Epoch 1123
Epoch 1123 :: Batch 0/1
Batch Loss = 4.437384998766028
1123, epoch_train_loss=4.437384998766028
Epoch 1124
Epoch 1124 :: Batch 0/1
Batch Loss = 4.437384997566146
1124, epoch_train_loss=4.437384997566146
Epoch 1125
Epoch 1125 :: Batch 0/1
Batch Loss = 4.437384996368194
1125, epoch_train_loss=4.437384996368194
Epoch 1126
Epoch 1126 :: Batch 0/1
Batch Loss = 4.437384995172161
1126, epoch_train_loss=4.437384995172161
Epoch 1127
Epoch 1127 :: Batch 0/1
Batch Loss = 4.437384993978043
1127, epoch_train_loss=4.437384993978043
Epoch 1128
Epoch 1128 :: Batch 0/1
Batch Loss = 4.437384992785828
1128, epoch_train_loss=4.437384992785828
Epoch 1129
Epoch 1129 :: Batch 0/1
Batch Loss = 4.43738499159551
1129, epoch_train_loss=4.43738499159551
Epoch 1130
Epoch 1130 :: Batch 0/1
Batch Loss = 4.437384990407081
1130, epoch_train_loss=4.437384990407081
Epoch 1131
Epoch 1131 :: Batch 0/1
Batch Loss = 4.437384989220531
1131, epoch_train_loss=4.437384989220531
Epoch 1132
Epoch 1132 :: Batch 0/1
Batch Loss = 4.437384988035852
1132, epoch_train_loss=4.437384988035852
Epoch 1133
Epoch 1133 :: Batch 0/1
Batch Loss = 4.437384986853037
1133, epoch_train_loss=4.437384986853037
Epoch 1134
Epoch 1134 :: Batch 0/1
Batch Loss = 4.437384985672077
1134, epoch_train_loss=4.437384985672077
Epoch 1135
Epoch 1135 :: Batch 0/1
Batch Loss = 4.437384984492965
1135, epoch_train_loss=4.437384984492965
Epoch 1136
Epoch 1136 :: Batch 0/1
Batch Loss = 4.437384983315693
1136, epoch_train_loss=4.437384983315693
Epoch 1137
Epoch 1137 :: Batch 0/1
Batch Loss = 4.43738498214025
1137, epoch_train_loss=4.43738498214025
Epoch 1138
Epoch 1138 :: Batch 0/1
Batch Loss = 4.437384980966632
1138, epoch_train_loss=4.437384980966632
Epoch 1139
Epoch 1139 :: Batch 0/1
Batch Loss = 4.437384979794829
1139, epoch_train_loss=4.437384979794829
Epoch 1140
Epoch 1140 :: Batch 0/1
Batch Loss = 4.437384978624834
1140, epoch_train_loss=4.437384978624834
Epoch 1141
Epoch 1141 :: Batch 0/1
Batch Loss = 4.437384977456637
1141, epoch_train_loss=4.437384977456637
Epoch 1142
Epoch 1142 :: Batch 0/1
Batch Loss = 4.437384976290233
1142, epoch_train_loss=4.437384976290233
Epoch 1143
Epoch 1143 :: Batch 0/1
Batch Loss = 4.43738497512561
1143, epoch_train_loss=4.43738497512561
Epoch 1144
Epoch 1144 :: Batch 0/1
Batch Loss = 4.437384973962766
1144, epoch_train_loss=4.437384973962766
Epoch 1145
Epoch 1145 :: Batch 0/1
Batch Loss = 4.437384972801689
1145, epoch_train_loss=4.437384972801689
Epoch 1146
Epoch 1146 :: Batch 0/1
Batch Loss = 4.4373849716423726
1146, epoch_train_loss=4.4373849716423726
Epoch 1147
Epoch 1147 :: Batch 0/1
Batch Loss = 4.437384970484807
1147, epoch_train_loss=4.437384970484807
Epoch 1148
Epoch 1148 :: Batch 0/1
Batch Loss = 4.437384969328987
1148, epoch_train_loss=4.437384969328987
Epoch 1149
Epoch 1149 :: Batch 0/1
Batch Loss = 4.437384968174905
1149, epoch_train_loss=4.437384968174905
Epoch 1150
Epoch 1150 :: Batch 0/1
Batch Loss = 4.437384967022552
1150, epoch_train_loss=4.437384967022552
Epoch 1151
Epoch 1151 :: Batch 0/1
Batch Loss = 4.43738496587192
1151, epoch_train_loss=4.43738496587192
Epoch 1152
Epoch 1152 :: Batch 0/1
Batch Loss = 4.437384964723003
1152, epoch_train_loss=4.437384964723003
Epoch 1153
Epoch 1153 :: Batch 0/1
Batch Loss = 4.437384963575792
1153, epoch_train_loss=4.437384963575792
Epoch 1154
Epoch 1154 :: Batch 0/1
Batch Loss = 4.43738496243028
1154, epoch_train_loss=4.43738496243028
Epoch 1155
Epoch 1155 :: Batch 0/1
Batch Loss = 4.43738496128646
1155, epoch_train_loss=4.43738496128646
Epoch 1156
Epoch 1156 :: Batch 0/1
Batch Loss = 4.437384960144324
1156, epoch_train_loss=4.437384960144324
Epoch 1157
Epoch 1157 :: Batch 0/1
Batch Loss = 4.437384959003864
1157, epoch_train_loss=4.437384959003864
Epoch 1158
Epoch 1158 :: Batch 0/1
Batch Loss = 4.437384957865072
1158, epoch_train_loss=4.437384957865072
Epoch 1159
Epoch 1159 :: Batch 0/1
Batch Loss = 4.437384956727943
1159, epoch_train_loss=4.437384956727943
Epoch 1160
Epoch 1160 :: Batch 0/1
Batch Loss = 4.437384955592467
1160, epoch_train_loss=4.437384955592467
Epoch 1161
Epoch 1161 :: Batch 0/1
Batch Loss = 4.437384954458639
1161, epoch_train_loss=4.437384954458639
Epoch 1162
Epoch 1162 :: Batch 0/1
Batch Loss = 4.43738495332645
1162, epoch_train_loss=4.43738495332645
Epoch 1163
Epoch 1163 :: Batch 0/1
Batch Loss = 4.437384952195892
1163, epoch_train_loss=4.437384952195892
Epoch 1164
Epoch 1164 :: Batch 0/1
Batch Loss = 4.43738495106696
1164, epoch_train_loss=4.43738495106696
Epoch 1165
Epoch 1165 :: Batch 0/1
Batch Loss = 4.437384949939645
1165, epoch_train_loss=4.437384949939645
Epoch 1166
Epoch 1166 :: Batch 0/1
Batch Loss = 4.43738494881394
1166, epoch_train_loss=4.43738494881394
Epoch 1167
Epoch 1167 :: Batch 0/1
Batch Loss = 4.4373849476898375
1167, epoch_train_loss=4.4373849476898375
Epoch 1168
Epoch 1168 :: Batch 0/1
Batch Loss = 4.437384946567331
1168, epoch_train_loss=4.437384946567331
Epoch 1169
Epoch 1169 :: Batch 0/1
Batch Loss = 4.4373849454464125
1169, epoch_train_loss=4.4373849454464125
Epoch 1170
Epoch 1170 :: Batch 0/1
Batch Loss = 4.437384944327077
1170, epoch_train_loss=4.437384944327077
Epoch 1171
Epoch 1171 :: Batch 0/1
Batch Loss = 4.437384943209314
1171, epoch_train_loss=4.437384943209314
Epoch 1172
Epoch 1172 :: Batch 0/1
Batch Loss = 4.437384942093119
1172, epoch_train_loss=4.437384942093119
Epoch 1173
Epoch 1173 :: Batch 0/1
Batch Loss = 4.437384940978482
1173, epoch_train_loss=4.437384940978482
Epoch 1174
Epoch 1174 :: Batch 0/1
Batch Loss = 4.4373849398653995
1174, epoch_train_loss=4.4373849398653995
Epoch 1175
Epoch 1175 :: Batch 0/1
Batch Loss = 4.437384938753861
1175, epoch_train_loss=4.437384938753861
Epoch 1176
Epoch 1176 :: Batch 0/1
Batch Loss = 4.437384937643864
1176, epoch_train_loss=4.437384937643864
Epoch 1177
Epoch 1177 :: Batch 0/1
Batch Loss = 4.437384936535396
1177, epoch_train_loss=4.437384936535396
Epoch 1178
Epoch 1178 :: Batch 0/1
Batch Loss = 4.4373849354284545
1178, epoch_train_loss=4.4373849354284545
Epoch 1179
Epoch 1179 :: Batch 0/1
Batch Loss = 4.437384934323029
1179, epoch_train_loss=4.437384934323029
Epoch 1180
Epoch 1180 :: Batch 0/1
Batch Loss = 4.437384933219115
1180, epoch_train_loss=4.437384933219115
Epoch 1181
Epoch 1181 :: Batch 0/1
Batch Loss = 4.4373849321167045
1181, epoch_train_loss=4.4373849321167045
Epoch 1182
Epoch 1182 :: Batch 0/1
Batch Loss = 4.437384931015791
1182, epoch_train_loss=4.437384931015791
Epoch 1183
Epoch 1183 :: Batch 0/1
Batch Loss = 4.437384929916369
1183, epoch_train_loss=4.437384929916369
Epoch 1184
Epoch 1184 :: Batch 0/1
Batch Loss = 4.437384928818427
1184, epoch_train_loss=4.437384928818427
Epoch 1185
Epoch 1185 :: Batch 0/1
Batch Loss = 4.437384927721965
1185, epoch_train_loss=4.437384927721965
Epoch 1186
Epoch 1186 :: Batch 0/1
Batch Loss = 4.43738492662697
1186, epoch_train_loss=4.43738492662697
Epoch 1187
Epoch 1187 :: Batch 0/1
Batch Loss = 4.437384925533438
1187, epoch_train_loss=4.437384925533438
Epoch 1188
Epoch 1188 :: Batch 0/1
Batch Loss = 4.437384924441362
1188, epoch_train_loss=4.437384924441362
Epoch 1189
Epoch 1189 :: Batch 0/1
Batch Loss = 4.437384923350735
1189, epoch_train_loss=4.437384923350735
Epoch 1190
Epoch 1190 :: Batch 0/1
Batch Loss = 4.4373849222615505
1190, epoch_train_loss=4.4373849222615505
Epoch 1191
Epoch 1191 :: Batch 0/1
Batch Loss = 4.437384921173803
1191, epoch_train_loss=4.437384921173803
Epoch 1192
Epoch 1192 :: Batch 0/1
Batch Loss = 4.4373849200874815
1192, epoch_train_loss=4.4373849200874815
Epoch 1193
Epoch 1193 :: Batch 0/1
Batch Loss = 4.437384919002584
1193, epoch_train_loss=4.437384919002584
Epoch 1194
Epoch 1194 :: Batch 0/1
Batch Loss = 4.437384917919102
1194, epoch_train_loss=4.437384917919102
Epoch 1195
Epoch 1195 :: Batch 0/1
Batch Loss = 4.437384916837029
1195, epoch_train_loss=4.437384916837029
Epoch 1196
Epoch 1196 :: Batch 0/1
Batch Loss = 4.4373849157563585
1196, epoch_train_loss=4.4373849157563585
Epoch 1197
Epoch 1197 :: Batch 0/1
Batch Loss = 4.437384914677083
1197, epoch_train_loss=4.437384914677083
Epoch 1198
Epoch 1198 :: Batch 0/1
Batch Loss = 4.437384913599198
1198, epoch_train_loss=4.437384913599198
Epoch 1199
Epoch 1199 :: Batch 0/1
Batch Loss = 4.437384912522694
1199, epoch_train_loss=4.437384912522694
Epoch 1200
Epoch 1200 :: Batch 0/1
Batch Loss = 4.437384911447566
1200, epoch_train_loss=4.437384911447566
Epoch 1201
Epoch 1201 :: Batch 0/1
Batch Loss = 4.437384910373809
1201, epoch_train_loss=4.437384910373809
Epoch 1202
Epoch 1202 :: Batch 0/1
Batch Loss = 4.437384909301414
1202, epoch_train_loss=4.437384909301414
Epoch 1203
Epoch 1203 :: Batch 0/1
Batch Loss = 4.437384908230375
1203, epoch_train_loss=4.437384908230375
Epoch 1204
Epoch 1204 :: Batch 0/1
Batch Loss = 4.437384907160687
1204, epoch_train_loss=4.437384907160687
Epoch 1205
Epoch 1205 :: Batch 0/1
Batch Loss = 4.437384906092342
1205, epoch_train_loss=4.437384906092342
Epoch 1206
Epoch 1206 :: Batch 0/1
Batch Loss = 4.4373849050253344
1206, epoch_train_loss=4.4373849050253344
Epoch 1207
Epoch 1207 :: Batch 0/1
Batch Loss = 4.437384903959658
1207, epoch_train_loss=4.437384903959658
Epoch 1208
Epoch 1208 :: Batch 0/1
Batch Loss = 4.437384902895305
1208, epoch_train_loss=4.437384902895305
Epoch 1209
Epoch 1209 :: Batch 0/1
Batch Loss = 4.43738490183227
1209, epoch_train_loss=4.43738490183227
Epoch 1210
Epoch 1210 :: Batch 0/1
Batch Loss = 4.437384900770547
1210, epoch_train_loss=4.437384900770547
Epoch 1211
Epoch 1211 :: Batch 0/1
Batch Loss = 4.437384899710129
1211, epoch_train_loss=4.437384899710129
Epoch 1212
Epoch 1212 :: Batch 0/1
Batch Loss = 4.43738489865101
1212, epoch_train_loss=4.43738489865101
Epoch 1213
Epoch 1213 :: Batch 0/1
Batch Loss = 4.437384897593184
1213, epoch_train_loss=4.437384897593184
Epoch 1214
Epoch 1214 :: Batch 0/1
Batch Loss = 4.437384896536644
1214, epoch_train_loss=4.437384896536644
Epoch 1215
Epoch 1215 :: Batch 0/1
Batch Loss = 4.437384895481384
1215, epoch_train_loss=4.437384895481384
Epoch 1216
Epoch 1216 :: Batch 0/1
Batch Loss = 4.437384894427397
1216, epoch_train_loss=4.437384894427397
Epoch 1217
Epoch 1217 :: Batch 0/1
Batch Loss = 4.437384893374678
1217, epoch_train_loss=4.437384893374678
Epoch 1218
Epoch 1218 :: Batch 0/1
Batch Loss = 4.437384892323222
1218, epoch_train_loss=4.437384892323222
Epoch 1219
Epoch 1219 :: Batch 0/1
Batch Loss = 4.437384891273019
1219, epoch_train_loss=4.437384891273019
Epoch 1220
Epoch 1220 :: Batch 0/1
Batch Loss = 4.437384890224067
1220, epoch_train_loss=4.437384890224067
Epoch 1221
Epoch 1221 :: Batch 0/1
Batch Loss = 4.437384889176355
1221, epoch_train_loss=4.437384889176355
Epoch 1222
Epoch 1222 :: Batch 0/1
Batch Loss = 4.437384888129881
1222, epoch_train_loss=4.437384888129881
Epoch 1223
Epoch 1223 :: Batch 0/1
Batch Loss = 4.4373848870846375
1223, epoch_train_loss=4.4373848870846375
Epoch 1224
Epoch 1224 :: Batch 0/1
Batch Loss = 4.437384886040619
1224, epoch_train_loss=4.437384886040619
Epoch 1225
Epoch 1225 :: Batch 0/1
Batch Loss = 4.437384884997817
1225, epoch_train_loss=4.437384884997817
Epoch 1226
Epoch 1226 :: Batch 0/1
Batch Loss = 4.437384883956229
1226, epoch_train_loss=4.437384883956229
Epoch 1227
Epoch 1227 :: Batch 0/1
Batch Loss = 4.437384882915846
1227, epoch_train_loss=4.437384882915846
Epoch 1228
Epoch 1228 :: Batch 0/1
Batch Loss = 4.437384881876664
1228, epoch_train_loss=4.437384881876664
Epoch 1229
Epoch 1229 :: Batch 0/1
Batch Loss = 4.4373848808386755
1229, epoch_train_loss=4.4373848808386755
Epoch 1230
Epoch 1230 :: Batch 0/1
Batch Loss = 4.437384879801874
1230, epoch_train_loss=4.437384879801874
Epoch 1231
Epoch 1231 :: Batch 0/1
Batch Loss = 4.437384878766255
1231, epoch_train_loss=4.437384878766255
Epoch 1232
Epoch 1232 :: Batch 0/1
Batch Loss = 4.437384877731812
1232, epoch_train_loss=4.437384877731812
Epoch 1233
Epoch 1233 :: Batch 0/1
Batch Loss = 4.437384876698538
1233, epoch_train_loss=4.437384876698538
Epoch 1234
Epoch 1234 :: Batch 0/1
Batch Loss = 4.43738487566643
1234, epoch_train_loss=4.43738487566643
Epoch 1235
Epoch 1235 :: Batch 0/1
Batch Loss = 4.43738487463548
1235, epoch_train_loss=4.43738487463548
Epoch 1236
Epoch 1236 :: Batch 0/1
Batch Loss = 4.43738487360568
1236, epoch_train_loss=4.43738487360568
Epoch 1237
Epoch 1237 :: Batch 0/1
Batch Loss = 4.437384872577029
1237, epoch_train_loss=4.437384872577029
Epoch 1238
Epoch 1238 :: Batch 0/1
Batch Loss = 4.437384871549515
1238, epoch_train_loss=4.437384871549515
Epoch 1239
Epoch 1239 :: Batch 0/1
Batch Loss = 4.437384870523138
1239, epoch_train_loss=4.437384870523138
Epoch 1240
Epoch 1240 :: Batch 0/1
Batch Loss = 4.437384869497888
1240, epoch_train_loss=4.437384869497888
Epoch 1241
Epoch 1241 :: Batch 0/1
Batch Loss = 4.437384868473762
1241, epoch_train_loss=4.437384868473762
Epoch 1242
Epoch 1242 :: Batch 0/1
Batch Loss = 4.437384867450751
1242, epoch_train_loss=4.437384867450751
Epoch 1243
Epoch 1243 :: Batch 0/1
Batch Loss = 4.437384866428853
1243, epoch_train_loss=4.437384866428853
Epoch 1244
Epoch 1244 :: Batch 0/1
Batch Loss = 4.437384865408058
1244, epoch_train_loss=4.437384865408058
Epoch 1245
Epoch 1245 :: Batch 0/1
Batch Loss = 4.437384864388365
1245, epoch_train_loss=4.437384864388365
Epoch 1246
Epoch 1246 :: Batch 0/1
Batch Loss = 4.437384863369765
1246, epoch_train_loss=4.437384863369765
Epoch 1247
Epoch 1247 :: Batch 0/1
Batch Loss = 4.437384862352251
1247, epoch_train_loss=4.437384862352251
Epoch 1248
Epoch 1248 :: Batch 0/1
Batch Loss = 4.437384861335819
1248, epoch_train_loss=4.437384861335819
Epoch 1249
Epoch 1249 :: Batch 0/1
Batch Loss = 4.437384860320464
1249, epoch_train_loss=4.437384860320464
Epoch 1250
Epoch 1250 :: Batch 0/1
Batch Loss = 4.43738485930618
1250, epoch_train_loss=4.43738485930618
Epoch 1251
Epoch 1251 :: Batch 0/1
Batch Loss = 4.437384858292961
1251, epoch_train_loss=4.437384858292961
Epoch 1252
Epoch 1252 :: Batch 0/1
Batch Loss = 4.437384857280801
1252, epoch_train_loss=4.437384857280801
Epoch 1253
Epoch 1253 :: Batch 0/1
Batch Loss = 4.437384856269694
1253, epoch_train_loss=4.437384856269694
Epoch 1254
Epoch 1254 :: Batch 0/1
Batch Loss = 4.437384855259635
1254, epoch_train_loss=4.437384855259635
Epoch 1255
Epoch 1255 :: Batch 0/1
Batch Loss = 4.437384854250618
1255, epoch_train_loss=4.437384854250618
Epoch 1256
Epoch 1256 :: Batch 0/1
Batch Loss = 4.437384853242637
1256, epoch_train_loss=4.437384853242637
Epoch 1257
Epoch 1257 :: Batch 0/1
Batch Loss = 4.437384852235687
1257, epoch_train_loss=4.437384852235687
Epoch 1258
Epoch 1258 :: Batch 0/1
Batch Loss = 4.437384851229762
1258, epoch_train_loss=4.437384851229762
Epoch 1259
Epoch 1259 :: Batch 0/1
Batch Loss = 4.437384850224857
1259, epoch_train_loss=4.437384850224857
Epoch 1260
Epoch 1260 :: Batch 0/1
Batch Loss = 4.437384849220966
1260, epoch_train_loss=4.437384849220966
Epoch 1261
Epoch 1261 :: Batch 0/1
Batch Loss = 4.437384848218084
1261, epoch_train_loss=4.437384848218084
Epoch 1262
Epoch 1262 :: Batch 0/1
Batch Loss = 4.437384847216203
1262, epoch_train_loss=4.437384847216203
Epoch 1263
Epoch 1263 :: Batch 0/1
Batch Loss = 4.43738484621532
1263, epoch_train_loss=4.43738484621532
Epoch 1264
Epoch 1264 :: Batch 0/1
Batch Loss = 4.437384845215429
1264, epoch_train_loss=4.437384845215429
Epoch 1265
Epoch 1265 :: Batch 0/1
Batch Loss = 4.437384844216524
1265, epoch_train_loss=4.437384844216524
Epoch 1266
Epoch 1266 :: Batch 0/1
Batch Loss = 4.437384843218599
1266, epoch_train_loss=4.437384843218599
Epoch 1267
Epoch 1267 :: Batch 0/1
Batch Loss = 4.437384842221649
1267, epoch_train_loss=4.437384842221649
Epoch 1268
Epoch 1268 :: Batch 0/1
Batch Loss = 4.4373848412256685
1268, epoch_train_loss=4.4373848412256685
Epoch 1269
Epoch 1269 :: Batch 0/1
Batch Loss = 4.437384840230654
1269, epoch_train_loss=4.437384840230654
Epoch 1270
Epoch 1270 :: Batch 0/1
Batch Loss = 4.4373848392365955
1270, epoch_train_loss=4.4373848392365955
Epoch 1271
Epoch 1271 :: Batch 0/1
Batch Loss = 4.43738483824349
1271, epoch_train_loss=4.43738483824349
Epoch 1272
Epoch 1272 :: Batch 0/1
Batch Loss = 4.437384837251334
1272, epoch_train_loss=4.437384837251334
Epoch 1273
Epoch 1273 :: Batch 0/1
Batch Loss = 4.43738483626012
1273, epoch_train_loss=4.43738483626012
Epoch 1274
Epoch 1274 :: Batch 0/1
Batch Loss = 4.4373848352698415
1274, epoch_train_loss=4.4373848352698415
Epoch 1275
Epoch 1275 :: Batch 0/1
Batch Loss = 4.437384834280495
1275, epoch_train_loss=4.437384834280495
Epoch 1276
Epoch 1276 :: Batch 0/1
Batch Loss = 4.437384833292075
1276, epoch_train_loss=4.437384833292075
Epoch 1277
Epoch 1277 :: Batch 0/1
Batch Loss = 4.437384832304574
1277, epoch_train_loss=4.437384832304574
Epoch 1278
Epoch 1278 :: Batch 0/1
Batch Loss = 4.437384831317989
1278, epoch_train_loss=4.437384831317989
Epoch 1279
Epoch 1279 :: Batch 0/1
Batch Loss = 4.437384830332314
1279, epoch_train_loss=4.437384830332314
Epoch 1280
Epoch 1280 :: Batch 0/1
Batch Loss = 4.437384829347542
1280, epoch_train_loss=4.437384829347542
Epoch 1281
Epoch 1281 :: Batch 0/1
Batch Loss = 4.437384828363671
1281, epoch_train_loss=4.437384828363671
Epoch 1282
Epoch 1282 :: Batch 0/1
Batch Loss = 4.437384827380693
1282, epoch_train_loss=4.437384827380693
Epoch 1283
Epoch 1283 :: Batch 0/1
Batch Loss = 4.437384826398603
1283, epoch_train_loss=4.437384826398603
Epoch 1284
Epoch 1284 :: Batch 0/1
Batch Loss = 4.437384825417396
1284, epoch_train_loss=4.437384825417396
Epoch 1285
Epoch 1285 :: Batch 0/1
Batch Loss = 4.437384824437066
1285, epoch_train_loss=4.437384824437066
Epoch 1286
Epoch 1286 :: Batch 0/1
Batch Loss = 4.437384823457609
1286, epoch_train_loss=4.437384823457609
Epoch 1287
Epoch 1287 :: Batch 0/1
Batch Loss = 4.43738482247902
1287, epoch_train_loss=4.43738482247902
Epoch 1288
Epoch 1288 :: Batch 0/1
Batch Loss = 4.437384821501292
1288, epoch_train_loss=4.437384821501292
Epoch 1289
Epoch 1289 :: Batch 0/1
Batch Loss = 4.437384820524421
1289, epoch_train_loss=4.437384820524421
Epoch 1290
Epoch 1290 :: Batch 0/1
Batch Loss = 4.4373848195484005
1290, epoch_train_loss=4.4373848195484005
Epoch 1291
Epoch 1291 :: Batch 0/1
Batch Loss = 4.437384818573224
1291, epoch_train_loss=4.437384818573224
Epoch 1292
Epoch 1292 :: Batch 0/1
Batch Loss = 4.4373848175988915
1292, epoch_train_loss=4.4373848175988915
Epoch 1293
Epoch 1293 :: Batch 0/1
Batch Loss = 4.437384816625393
1293, epoch_train_loss=4.437384816625393
Epoch 1294
Epoch 1294 :: Batch 0/1
Batch Loss = 4.4373848156527265
1294, epoch_train_loss=4.4373848156527265
Epoch 1295
Epoch 1295 :: Batch 0/1
Batch Loss = 4.437384814680883
1295, epoch_train_loss=4.437384814680883
Epoch 1296
Epoch 1296 :: Batch 0/1
Batch Loss = 4.437384813709859
1296, epoch_train_loss=4.437384813709859
Epoch 1297
Epoch 1297 :: Batch 0/1
Batch Loss = 4.437384812739651
1297, epoch_train_loss=4.437384812739651
Epoch 1298
Epoch 1298 :: Batch 0/1
Batch Loss = 4.437384811770252
1298, epoch_train_loss=4.437384811770252
Epoch 1299
Epoch 1299 :: Batch 0/1
Batch Loss = 4.437384810801658
1299, epoch_train_loss=4.437384810801658
Epoch 1300
Epoch 1300 :: Batch 0/1
Batch Loss = 4.437384809833862
1300, epoch_train_loss=4.437384809833862
Epoch 1301
Epoch 1301 :: Batch 0/1
Batch Loss = 4.43738480886686
1301, epoch_train_loss=4.43738480886686
Epoch 1302
Epoch 1302 :: Batch 0/1
Batch Loss = 4.437384807900647
1302, epoch_train_loss=4.437384807900647
Epoch 1303
Epoch 1303 :: Batch 0/1
Batch Loss = 4.437384806935218
1303, epoch_train_loss=4.437384806935218
Epoch 1304
Epoch 1304 :: Batch 0/1
Batch Loss = 4.437384805970567
1304, epoch_train_loss=4.437384805970567
Epoch 1305
Epoch 1305 :: Batch 0/1
Batch Loss = 4.43738480500669
1305, epoch_train_loss=4.43738480500669
Epoch 1306
Epoch 1306 :: Batch 0/1
Batch Loss = 4.437384804043582
1306, epoch_train_loss=4.437384804043582
Epoch 1307
Epoch 1307 :: Batch 0/1
Batch Loss = 4.437384803081236
1307, epoch_train_loss=4.437384803081236
Epoch 1308
Epoch 1308 :: Batch 0/1
Batch Loss = 4.437384802119647
1308, epoch_train_loss=4.437384802119647
Epoch 1309
Epoch 1309 :: Batch 0/1
Batch Loss = 4.437384801158813
1309, epoch_train_loss=4.437384801158813
Epoch 1310
Epoch 1310 :: Batch 0/1
Batch Loss = 4.437384800198726
1310, epoch_train_loss=4.437384800198726
Epoch 1311
Epoch 1311 :: Batch 0/1
Batch Loss = 4.437384799239382
1311, epoch_train_loss=4.437384799239382
Epoch 1312
Epoch 1312 :: Batch 0/1
Batch Loss = 4.437384798280774
1312, epoch_train_loss=4.437384798280774
Epoch 1313
Epoch 1313 :: Batch 0/1
Batch Loss = 4.437384797322901
1313, epoch_train_loss=4.437384797322901
Epoch 1314
Epoch 1314 :: Batch 0/1
Batch Loss = 4.4373847963657544
1314, epoch_train_loss=4.4373847963657544
Epoch 1315
Epoch 1315 :: Batch 0/1
Batch Loss = 4.43738479540933
1315, epoch_train_loss=4.43738479540933
Epoch 1316
Epoch 1316 :: Batch 0/1
Batch Loss = 4.437384794453625
1316, epoch_train_loss=4.437384794453625
Epoch 1317
Epoch 1317 :: Batch 0/1
Batch Loss = 4.437384793498631
1317, epoch_train_loss=4.437384793498631
Epoch 1318
Epoch 1318 :: Batch 0/1
Batch Loss = 4.437384792544345
1318, epoch_train_loss=4.437384792544345
Epoch 1319
Epoch 1319 :: Batch 0/1
Batch Loss = 4.437384791590762
1319, epoch_train_loss=4.437384791590762
Epoch 1320
Epoch 1320 :: Batch 0/1
Batch Loss = 4.437384790637874
1320, epoch_train_loss=4.437384790637874
Epoch 1321
Epoch 1321 :: Batch 0/1
Batch Loss = 4.43738478968568
1321, epoch_train_loss=4.43738478968568
Epoch 1322
Epoch 1322 :: Batch 0/1
Batch Loss = 4.437384788734174
1322, epoch_train_loss=4.437384788734174
Epoch 1323
Epoch 1323 :: Batch 0/1
Batch Loss = 4.437384787783349
1323, epoch_train_loss=4.437384787783349
Epoch 1324
Epoch 1324 :: Batch 0/1
Batch Loss = 4.437384786833204
1324, epoch_train_loss=4.437384786833204
Epoch 1325
Epoch 1325 :: Batch 0/1
Batch Loss = 4.437384785883729
1325, epoch_train_loss=4.437384785883729
Epoch 1326
Epoch 1326 :: Batch 0/1
Batch Loss = 4.437384784934924
1326, epoch_train_loss=4.437384784934924
Epoch 1327
Epoch 1327 :: Batch 0/1
Batch Loss = 4.43738478398678
1327, epoch_train_loss=4.43738478398678
Epoch 1328
Epoch 1328 :: Batch 0/1
Batch Loss = 4.437384783039294
1328, epoch_train_loss=4.437384783039294
Epoch 1329
Epoch 1329 :: Batch 0/1
Batch Loss = 4.43738478209246
1329, epoch_train_loss=4.43738478209246
Epoch 1330
Epoch 1330 :: Batch 0/1
Batch Loss = 4.437384781146275
1330, epoch_train_loss=4.437384781146275
Epoch 1331
Epoch 1331 :: Batch 0/1
Batch Loss = 4.437384780200733
1331, epoch_train_loss=4.437384780200733
Epoch 1332
Epoch 1332 :: Batch 0/1
Batch Loss = 4.437384779255828
1332, epoch_train_loss=4.437384779255828
Epoch 1333
Epoch 1333 :: Batch 0/1
Batch Loss = 4.437384778311556
1333, epoch_train_loss=4.437384778311556
Epoch 1334
Epoch 1334 :: Batch 0/1
Batch Loss = 4.437384777367914
1334, epoch_train_loss=4.437384777367914
Epoch 1335
Epoch 1335 :: Batch 0/1
Batch Loss = 4.437384776424892
1335, epoch_train_loss=4.437384776424892
Epoch 1336
Epoch 1336 :: Batch 0/1
Batch Loss = 4.437384775482491
1336, epoch_train_loss=4.437384775482491
Epoch 1337
Epoch 1337 :: Batch 0/1
Batch Loss = 4.437384774540703
1337, epoch_train_loss=4.437384774540703
Epoch 1338
Epoch 1338 :: Batch 0/1
Batch Loss = 4.437384773599522
1338, epoch_train_loss=4.437384773599522
Epoch 1339
Epoch 1339 :: Batch 0/1
Batch Loss = 4.4373847726589455
1339, epoch_train_loss=4.4373847726589455
Epoch 1340
Epoch 1340 :: Batch 0/1
Batch Loss = 4.4373847717189685
1340, epoch_train_loss=4.4373847717189685
Epoch 1341
Epoch 1341 :: Batch 0/1
Batch Loss = 4.437384770779586
1341, epoch_train_loss=4.437384770779586
Epoch 1342
Epoch 1342 :: Batch 0/1
Batch Loss = 4.437384769840791
1342, epoch_train_loss=4.437384769840791
Epoch 1343
Epoch 1343 :: Batch 0/1
Batch Loss = 4.4373847689025805
1343, epoch_train_loss=4.4373847689025805
Epoch 1344
Epoch 1344 :: Batch 0/1
Batch Loss = 4.437384767964949
1344, epoch_train_loss=4.437384767964949
Epoch 1345
Epoch 1345 :: Batch 0/1
Batch Loss = 4.4373847670278925
1345, epoch_train_loss=4.4373847670278925
Epoch 1346
Epoch 1346 :: Batch 0/1
Batch Loss = 4.437384766091407
1346, epoch_train_loss=4.437384766091407
Epoch 1347
Epoch 1347 :: Batch 0/1
Batch Loss = 4.437384765155485
1347, epoch_train_loss=4.437384765155485
Epoch 1348
Epoch 1348 :: Batch 0/1
Batch Loss = 4.437384764220123
1348, epoch_train_loss=4.437384764220123
Epoch 1349
Epoch 1349 :: Batch 0/1
Batch Loss = 4.4373847632853165
1349, epoch_train_loss=4.4373847632853165
Epoch 1350
Epoch 1350 :: Batch 0/1
Batch Loss = 4.43738476235106
1350, epoch_train_loss=4.43738476235106
Epoch 1351
Epoch 1351 :: Batch 0/1
Batch Loss = 4.437384761417349
1351, epoch_train_loss=4.437384761417349
Epoch 1352
Epoch 1352 :: Batch 0/1
Batch Loss = 4.437384760484181
1352, epoch_train_loss=4.437384760484181
Epoch 1353
Epoch 1353 :: Batch 0/1
Batch Loss = 4.437384759551546
1353, epoch_train_loss=4.437384759551546
Epoch 1354
Epoch 1354 :: Batch 0/1
Batch Loss = 4.4373847586194435
1354, epoch_train_loss=4.4373847586194435
Epoch 1355
Epoch 1355 :: Batch 0/1
Batch Loss = 4.437384757687868
1355, epoch_train_loss=4.437384757687868
Epoch 1356
Epoch 1356 :: Batch 0/1
Batch Loss = 4.437384756756813
1356, epoch_train_loss=4.437384756756813
Epoch 1357
Epoch 1357 :: Batch 0/1
Batch Loss = 4.437384755826276
1357, epoch_train_loss=4.437384755826276
Epoch 1358
Epoch 1358 :: Batch 0/1
Batch Loss = 4.437384754896249
1358, epoch_train_loss=4.437384754896249
Epoch 1359
Epoch 1359 :: Batch 0/1
Batch Loss = 4.437384753966732
1359, epoch_train_loss=4.437384753966732
Epoch 1360
Epoch 1360 :: Batch 0/1
Batch Loss = 4.4373847530377155
1360, epoch_train_loss=4.4373847530377155
Epoch 1361
Epoch 1361 :: Batch 0/1
Batch Loss = 4.437384752109198
1361, epoch_train_loss=4.437384752109198
Epoch 1362
Epoch 1362 :: Batch 0/1
Batch Loss = 4.437384751181173
1362, epoch_train_loss=4.437384751181173
Epoch 1363
Epoch 1363 :: Batch 0/1
Batch Loss = 4.437384750253637
1363, epoch_train_loss=4.437384750253637
Epoch 1364
Epoch 1364 :: Batch 0/1
Batch Loss = 4.437384749326584
1364, epoch_train_loss=4.437384749326584
Epoch 1365
Epoch 1365 :: Batch 0/1
Batch Loss = 4.437384748400009
1365, epoch_train_loss=4.437384748400009
Epoch 1366
Epoch 1366 :: Batch 0/1
Batch Loss = 4.437384747473908
1366, epoch_train_loss=4.437384747473908
Epoch 1367
Epoch 1367 :: Batch 0/1
Batch Loss = 4.437384746548277
1367, epoch_train_loss=4.437384746548277
Epoch 1368
Epoch 1368 :: Batch 0/1
Batch Loss = 4.437384745623111
1368, epoch_train_loss=4.437384745623111
Epoch 1369
Epoch 1369 :: Batch 0/1
Batch Loss = 4.437384744698403
1369, epoch_train_loss=4.437384744698403
Epoch 1370
Epoch 1370 :: Batch 0/1
Batch Loss = 4.437384743774152
1370, epoch_train_loss=4.437384743774152
Epoch 1371
Epoch 1371 :: Batch 0/1
Batch Loss = 4.437384742850351
1371, epoch_train_loss=4.437384742850351
Epoch 1372
Epoch 1372 :: Batch 0/1
Batch Loss = 4.437384741926995
1372, epoch_train_loss=4.437384741926995
Epoch 1373
Epoch 1373 :: Batch 0/1
Batch Loss = 4.437384741004081
1373, epoch_train_loss=4.437384741004081
Epoch 1374
Epoch 1374 :: Batch 0/1
Batch Loss = 4.437384740081602
1374, epoch_train_loss=4.437384740081602
Epoch 1375
Epoch 1375 :: Batch 0/1
Batch Loss = 4.437384739159556
1375, epoch_train_loss=4.437384739159556
Epoch 1376
Epoch 1376 :: Batch 0/1
Batch Loss = 4.4373847382379354
1376, epoch_train_loss=4.4373847382379354
Epoch 1377
Epoch 1377 :: Batch 0/1
Batch Loss = 4.437384737316738
1377, epoch_train_loss=4.437384737316738
Epoch 1378
Epoch 1378 :: Batch 0/1
Batch Loss = 4.437384736395956
1378, epoch_train_loss=4.437384736395956
Epoch 1379
Epoch 1379 :: Batch 0/1
Batch Loss = 4.437384735475589
1379, epoch_train_loss=4.437384735475589
Epoch 1380
Epoch 1380 :: Batch 0/1
Batch Loss = 4.437384734555629
1380, epoch_train_loss=4.437384734555629
Epoch 1381
Epoch 1381 :: Batch 0/1
Batch Loss = 4.437384733636073
1381, epoch_train_loss=4.437384733636073
Epoch 1382
Epoch 1382 :: Batch 0/1
Batch Loss = 4.437384732716915
1382, epoch_train_loss=4.437384732716915
Epoch 1383
Epoch 1383 :: Batch 0/1
Batch Loss = 4.437384731798152
1383, epoch_train_loss=4.437384731798152
Epoch 1384
Epoch 1384 :: Batch 0/1
Batch Loss = 4.437384730879777
1384, epoch_train_loss=4.437384730879777
Epoch 1385
Epoch 1385 :: Batch 0/1
Batch Loss = 4.437384729961788
1385, epoch_train_loss=4.437384729961788
Epoch 1386
Epoch 1386 :: Batch 0/1
Batch Loss = 4.437384729044178
1386, epoch_train_loss=4.437384729044178
Epoch 1387
Epoch 1387 :: Batch 0/1
Batch Loss = 4.4373847281269425
1387, epoch_train_loss=4.4373847281269425
Epoch 1388
Epoch 1388 :: Batch 0/1
Batch Loss = 4.4373847272100795
1388, epoch_train_loss=4.4373847272100795
Epoch 1389
Epoch 1389 :: Batch 0/1
Batch Loss = 4.437384726293583
1389, epoch_train_loss=4.437384726293583
Epoch 1390
Epoch 1390 :: Batch 0/1
Batch Loss = 4.437384725377445
1390, epoch_train_loss=4.437384725377445
Epoch 1391
Epoch 1391 :: Batch 0/1
Batch Loss = 4.437384724461665
1391, epoch_train_loss=4.437384724461665
Epoch 1392
Epoch 1392 :: Batch 0/1
Batch Loss = 4.437384723546238
1392, epoch_train_loss=4.437384723546238
Epoch 1393
Epoch 1393 :: Batch 0/1
Batch Loss = 4.4373847226311565
1393, epoch_train_loss=4.4373847226311565
Epoch 1394
Epoch 1394 :: Batch 0/1
Batch Loss = 4.437384721716419
1394, epoch_train_loss=4.437384721716419
Epoch 1395
Epoch 1395 :: Batch 0/1
Batch Loss = 4.43738472080202
1395, epoch_train_loss=4.43738472080202
Epoch 1396
Epoch 1396 :: Batch 0/1
Batch Loss = 4.437384719887953
1396, epoch_train_loss=4.437384719887953
Epoch 1397
Epoch 1397 :: Batch 0/1
Batch Loss = 4.437384718974215
1397, epoch_train_loss=4.437384718974215
Epoch 1398
Epoch 1398 :: Batch 0/1
Batch Loss = 4.437384718060801
1398, epoch_train_loss=4.437384718060801
Epoch 1399
Epoch 1399 :: Batch 0/1
Batch Loss = 4.437384717147707
1399, epoch_train_loss=4.437384717147707
Epoch 1400
Epoch 1400 :: Batch 0/1
Batch Loss = 4.437384716234928
1400, epoch_train_loss=4.437384716234928
Epoch 1401
Epoch 1401 :: Batch 0/1
Batch Loss = 4.437384715322459
1401, epoch_train_loss=4.437384715322459
Epoch 1402
Epoch 1402 :: Batch 0/1
Batch Loss = 4.437384714410294
1402, epoch_train_loss=4.437384714410294
Epoch 1403
Epoch 1403 :: Batch 0/1
Batch Loss = 4.437384713498433
1403, epoch_train_loss=4.437384713498433
Epoch 1404
Epoch 1404 :: Batch 0/1
Batch Loss = 4.437384712586866
1404, epoch_train_loss=4.437384712586866
Epoch 1405
Epoch 1405 :: Batch 0/1
Batch Loss = 4.43738471167559
1405, epoch_train_loss=4.43738471167559
Epoch 1406
Epoch 1406 :: Batch 0/1
Batch Loss = 4.4373847107646025
1406, epoch_train_loss=4.4373847107646025
Epoch 1407
Epoch 1407 :: Batch 0/1
Batch Loss = 4.437384709853897
1407, epoch_train_loss=4.437384709853897
Epoch 1408
Epoch 1408 :: Batch 0/1
Batch Loss = 4.437384708943469
1408, epoch_train_loss=4.437384708943469
Epoch 1409
Epoch 1409 :: Batch 0/1
Batch Loss = 4.4373847080333135
1409, epoch_train_loss=4.4373847080333135
Epoch 1410
Epoch 1410 :: Batch 0/1
Batch Loss = 4.437384707123428
1410, epoch_train_loss=4.437384707123428
Epoch 1411
Epoch 1411 :: Batch 0/1
Batch Loss = 4.437384706213805
1411, epoch_train_loss=4.437384706213805
Epoch 1412
Epoch 1412 :: Batch 0/1
Batch Loss = 4.437384705304442
1412, epoch_train_loss=4.437384705304442
Epoch 1413
Epoch 1413 :: Batch 0/1
Batch Loss = 4.437384704395333
1413, epoch_train_loss=4.437384704395333
Epoch 1414
Epoch 1414 :: Batch 0/1
Batch Loss = 4.437384703486475
1414, epoch_train_loss=4.437384703486475
Epoch 1415
Epoch 1415 :: Batch 0/1
Batch Loss = 4.4373847025778606
1415, epoch_train_loss=4.4373847025778606
Epoch 1416
Epoch 1416 :: Batch 0/1
Batch Loss = 4.437384701669488
1416, epoch_train_loss=4.437384701669488
Epoch 1417
Epoch 1417 :: Batch 0/1
Batch Loss = 4.437384700761353
1417, epoch_train_loss=4.437384700761353
Epoch 1418
Epoch 1418 :: Batch 0/1
Batch Loss = 4.437384699853446
1418, epoch_train_loss=4.437384699853446
Epoch 1419
Epoch 1419 :: Batch 0/1
Batch Loss = 4.437384698945768
1419, epoch_train_loss=4.437384698945768
Epoch 1420
Epoch 1420 :: Batch 0/1
Batch Loss = 4.437384698038311
1420, epoch_train_loss=4.437384698038311
Epoch 1421
Epoch 1421 :: Batch 0/1
Batch Loss = 4.4373846971310735
1421, epoch_train_loss=4.4373846971310735
Epoch 1422
Epoch 1422 :: Batch 0/1
Batch Loss = 4.437384696224048
1422, epoch_train_loss=4.437384696224048
Epoch 1423
Epoch 1423 :: Batch 0/1
Batch Loss = 4.43738469531723
1423, epoch_train_loss=4.43738469531723
Epoch 1424
Epoch 1424 :: Batch 0/1
Batch Loss = 4.437384694410617
1424, epoch_train_loss=4.437384694410617
Epoch 1425
Epoch 1425 :: Batch 0/1
Batch Loss = 4.437384693504203
1425, epoch_train_loss=4.437384693504203
Epoch 1426
Epoch 1426 :: Batch 0/1
Batch Loss = 4.437384692597982
1426, epoch_train_loss=4.437384692597982
Epoch 1427
Epoch 1427 :: Batch 0/1
Batch Loss = 4.437384691691952
1427, epoch_train_loss=4.437384691691952
Epoch 1428
Epoch 1428 :: Batch 0/1
Batch Loss = 4.437384690786107
1428, epoch_train_loss=4.437384690786107
Epoch 1429
Epoch 1429 :: Batch 0/1
Batch Loss = 4.437384689880444
1429, epoch_train_loss=4.437384689880444
Epoch 1430
Epoch 1430 :: Batch 0/1
Batch Loss = 4.437384688974956
1430, epoch_train_loss=4.437384688974956
Epoch 1431
Epoch 1431 :: Batch 0/1
Batch Loss = 4.437384688069639
1431, epoch_train_loss=4.437384688069639
Epoch 1432
Epoch 1432 :: Batch 0/1
Batch Loss = 4.437384687164489
1432, epoch_train_loss=4.437384687164489
Epoch 1433
Epoch 1433 :: Batch 0/1
Batch Loss = 4.437384686259501
1433, epoch_train_loss=4.437384686259501
Epoch 1434
Epoch 1434 :: Batch 0/1
Batch Loss = 4.437384685354671
1434, epoch_train_loss=4.437384685354671
Epoch 1435
Epoch 1435 :: Batch 0/1
Batch Loss = 4.437384684449994
1435, epoch_train_loss=4.437384684449994
Epoch 1436
Epoch 1436 :: Batch 0/1
Batch Loss = 4.4373846835454644
1436, epoch_train_loss=4.4373846835454644
Epoch 1437
Epoch 1437 :: Batch 0/1
Batch Loss = 4.437384682641079
1437, epoch_train_loss=4.437384682641079
Epoch 1438
Epoch 1438 :: Batch 0/1
Batch Loss = 4.437384681736832
1438, epoch_train_loss=4.437384681736832
Epoch 1439
Epoch 1439 :: Batch 0/1
Batch Loss = 4.437384680832721
1439, epoch_train_loss=4.437384680832721
Epoch 1440
Epoch 1440 :: Batch 0/1
Batch Loss = 4.437384679928738
1440, epoch_train_loss=4.437384679928738
Epoch 1441
Epoch 1441 :: Batch 0/1
Batch Loss = 4.4373846790248805
1441, epoch_train_loss=4.4373846790248805
Epoch 1442
Epoch 1442 :: Batch 0/1
Batch Loss = 4.437384678121145
1442, epoch_train_loss=4.437384678121145
Epoch 1443
Epoch 1443 :: Batch 0/1
Batch Loss = 4.437384677217523
1443, epoch_train_loss=4.437384677217523
Epoch 1444
Epoch 1444 :: Batch 0/1
Batch Loss = 4.437384676314014
1444, epoch_train_loss=4.437384676314014
Epoch 1445
Epoch 1445 :: Batch 0/1
Batch Loss = 4.437384675410609
1445, epoch_train_loss=4.437384675410609
Epoch 1446
Epoch 1446 :: Batch 0/1
Batch Loss = 4.437384674507309
1446, epoch_train_loss=4.437384674507309
Epoch 1447
Epoch 1447 :: Batch 0/1
Batch Loss = 4.437384673604106
1447, epoch_train_loss=4.437384673604106
Epoch 1448
Epoch 1448 :: Batch 0/1
Batch Loss = 4.437384672700994
1448, epoch_train_loss=4.437384672700994
Epoch 1449
Epoch 1449 :: Batch 0/1
Batch Loss = 4.437384671797972
1449, epoch_train_loss=4.437384671797972
Epoch 1450
Epoch 1450 :: Batch 0/1
Batch Loss = 4.437384670895031
1450, epoch_train_loss=4.437384670895031
Epoch 1451
Epoch 1451 :: Batch 0/1
Batch Loss = 4.43738466999217
1451, epoch_train_loss=4.43738466999217
Epoch 1452
Epoch 1452 :: Batch 0/1
Batch Loss = 4.437384669089384
1452, epoch_train_loss=4.437384669089384
Epoch 1453
Epoch 1453 :: Batch 0/1
Batch Loss = 4.437384668186666
1453, epoch_train_loss=4.437384668186666
Epoch 1454
Epoch 1454 :: Batch 0/1
Batch Loss = 4.437384667284014
1454, epoch_train_loss=4.437384667284014
Epoch 1455
Epoch 1455 :: Batch 0/1
Batch Loss = 4.43738466638142
1455, epoch_train_loss=4.43738466638142
Epoch 1456
Epoch 1456 :: Batch 0/1
Batch Loss = 4.437384665478884
1456, epoch_train_loss=4.437384665478884
Epoch 1457
Epoch 1457 :: Batch 0/1
Batch Loss = 4.437384664576397
1457, epoch_train_loss=4.437384664576397
Epoch 1458
Epoch 1458 :: Batch 0/1
Batch Loss = 4.437384663673957
1458, epoch_train_loss=4.437384663673957
Epoch 1459
Epoch 1459 :: Batch 0/1
Batch Loss = 4.437384662771559
1459, epoch_train_loss=4.437384662771559
Epoch 1460
Epoch 1460 :: Batch 0/1
Batch Loss = 4.4373846618691974
1460, epoch_train_loss=4.4373846618691974
Epoch 1461
Epoch 1461 :: Batch 0/1
Batch Loss = 4.437384660966868
1461, epoch_train_loss=4.437384660966868
Epoch 1462
Epoch 1462 :: Batch 0/1
Batch Loss = 4.437384660064566
1462, epoch_train_loss=4.437384660064566
Epoch 1463
Epoch 1463 :: Batch 0/1
Batch Loss = 4.437384659162286
1463, epoch_train_loss=4.437384659162286
Epoch 1464
Epoch 1464 :: Batch 0/1
Batch Loss = 4.437384658260025
1464, epoch_train_loss=4.437384658260025
Epoch 1465
Epoch 1465 :: Batch 0/1
Batch Loss = 4.437384657357778
1465, epoch_train_loss=4.437384657357778
Epoch 1466
Epoch 1466 :: Batch 0/1
Batch Loss = 4.4373846564555395
1466, epoch_train_loss=4.4373846564555395
Epoch 1467
Epoch 1467 :: Batch 0/1
Batch Loss = 4.437384655553304
1467, epoch_train_loss=4.437384655553304
Epoch 1468
Epoch 1468 :: Batch 0/1
Batch Loss = 4.437384654651069
1468, epoch_train_loss=4.437384654651069
Epoch 1469
Epoch 1469 :: Batch 0/1
Batch Loss = 4.437384653748829
1469, epoch_train_loss=4.437384653748829
Epoch 1470
Epoch 1470 :: Batch 0/1
Batch Loss = 4.437384652846578
1470, epoch_train_loss=4.437384652846578
Epoch 1471
Epoch 1471 :: Batch 0/1
Batch Loss = 4.4373846519443125
1471, epoch_train_loss=4.4373846519443125
Epoch 1472
Epoch 1472 :: Batch 0/1
Batch Loss = 4.437384651042028
1472, epoch_train_loss=4.437384651042028
Epoch 1473
Epoch 1473 :: Batch 0/1
Batch Loss = 4.43738465013972
1473, epoch_train_loss=4.43738465013972
Epoch 1474
Epoch 1474 :: Batch 0/1
Batch Loss = 4.437384649237382
1474, epoch_train_loss=4.437384649237382
Epoch 1475
Epoch 1475 :: Batch 0/1
Batch Loss = 4.437384648335011
1475, epoch_train_loss=4.437384648335011
Epoch 1476
Epoch 1476 :: Batch 0/1
Batch Loss = 4.437384647432602
1476, epoch_train_loss=4.437384647432602
Epoch 1477
Epoch 1477 :: Batch 0/1
Batch Loss = 4.437384646530149
1477, epoch_train_loss=4.437384646530149
Epoch 1478
Epoch 1478 :: Batch 0/1
Batch Loss = 4.437384645627651
1478, epoch_train_loss=4.437384645627651
Epoch 1479
Epoch 1479 :: Batch 0/1
Batch Loss = 4.437384644725099
1479, epoch_train_loss=4.437384644725099
Epoch 1480
Epoch 1480 :: Batch 0/1
Batch Loss = 4.437384643822489
1480, epoch_train_loss=4.437384643822489
Epoch 1481
Epoch 1481 :: Batch 0/1
Batch Loss = 4.4373846429198185
1481, epoch_train_loss=4.4373846429198185
Epoch 1482
Epoch 1482 :: Batch 0/1
Batch Loss = 4.43738464201708
1482, epoch_train_loss=4.43738464201708
Epoch 1483
Epoch 1483 :: Batch 0/1
Batch Loss = 4.437384641114272
1483, epoch_train_loss=4.437384641114272
Epoch 1484
Epoch 1484 :: Batch 0/1
Batch Loss = 4.437384640211387
1484, epoch_train_loss=4.437384640211387
Epoch 1485
Epoch 1485 :: Batch 0/1
Batch Loss = 4.437384639308421
1485, epoch_train_loss=4.437384639308421
Epoch 1486
Epoch 1486 :: Batch 0/1
Batch Loss = 4.437384638405369
1486, epoch_train_loss=4.437384638405369
Epoch 1487
Epoch 1487 :: Batch 0/1
Batch Loss = 4.437384637502227
1487, epoch_train_loss=4.437384637502227
Epoch 1488
Epoch 1488 :: Batch 0/1
Batch Loss = 4.43738463659899
1488, epoch_train_loss=4.43738463659899
Epoch 1489
Epoch 1489 :: Batch 0/1
Batch Loss = 4.437384635695653
1489, epoch_train_loss=4.437384635695653
Epoch 1490
Epoch 1490 :: Batch 0/1
Batch Loss = 4.4373846347922115
1490, epoch_train_loss=4.4373846347922115
Epoch 1491
Epoch 1491 :: Batch 0/1
Batch Loss = 4.437384633888661
1491, epoch_train_loss=4.437384633888661
Epoch 1492
Epoch 1492 :: Batch 0/1
Batch Loss = 4.437384632984997
1492, epoch_train_loss=4.437384632984997
Epoch 1493
Epoch 1493 :: Batch 0/1
Batch Loss = 4.437384632081211
1493, epoch_train_loss=4.437384632081211
Epoch 1494
Epoch 1494 :: Batch 0/1
Batch Loss = 4.437384631177304
1494, epoch_train_loss=4.437384631177304
Epoch 1495
Epoch 1495 :: Batch 0/1
Batch Loss = 4.437384630273267
1495, epoch_train_loss=4.437384630273267
Epoch 1496
Epoch 1496 :: Batch 0/1
Batch Loss = 4.437384629369097
1496, epoch_train_loss=4.437384629369097
Epoch 1497
Epoch 1497 :: Batch 0/1
Batch Loss = 4.437384628464789
1497, epoch_train_loss=4.437384628464789
Epoch 1498
Epoch 1498 :: Batch 0/1
Batch Loss = 4.437384627560338
1498, epoch_train_loss=4.437384627560338
Epoch 1499
Epoch 1499 :: Batch 0/1
Batch Loss = 4.437384626655739
1499, epoch_train_loss=4.437384626655739
Epoch 1500
Epoch 1500 :: Batch 0/1
Batch Loss = 4.437384625750987
1500, epoch_train_loss=4.437384625750987
Epoch 1501
Epoch 1501 :: Batch 0/1
Batch Loss = 4.4373846248460795
1501, epoch_train_loss=4.4373846248460795
Epoch 1502
Epoch 1502 :: Batch 0/1
Batch Loss = 4.437384623941007
1502, epoch_train_loss=4.437384623941007
Epoch 1503
Epoch 1503 :: Batch 0/1
Batch Loss = 4.4373846230357685
1503, epoch_train_loss=4.4373846230357685
Epoch 1504
Epoch 1504 :: Batch 0/1
Batch Loss = 4.437384622130357
1504, epoch_train_loss=4.437384622130357
Epoch 1505
Epoch 1505 :: Batch 0/1
Batch Loss = 4.43738462122477
1505, epoch_train_loss=4.43738462122477
Epoch 1506
Epoch 1506 :: Batch 0/1
Batch Loss = 4.437384620319001
1506, epoch_train_loss=4.437384620319001
Epoch 1507
Epoch 1507 :: Batch 0/1
Batch Loss = 4.437384619413044
1507, epoch_train_loss=4.437384619413044
Epoch 1508
Epoch 1508 :: Batch 0/1
Batch Loss = 4.437384618506896
1508, epoch_train_loss=4.437384618506896
Epoch 1509
Epoch 1509 :: Batch 0/1
Batch Loss = 4.437384617600553
1509, epoch_train_loss=4.437384617600553
Epoch 1510
Epoch 1510 :: Batch 0/1
Batch Loss = 4.437384616694008
1510, epoch_train_loss=4.437384616694008
Epoch 1511
Epoch 1511 :: Batch 0/1
Batch Loss = 4.4373846157872565
1511, epoch_train_loss=4.4373846157872565
Epoch 1512
Epoch 1512 :: Batch 0/1
Batch Loss = 4.437384614880294
1512, epoch_train_loss=4.437384614880294
Epoch 1513
Epoch 1513 :: Batch 0/1
Batch Loss = 4.437384613973116
1513, epoch_train_loss=4.437384613973116
Epoch 1514
Epoch 1514 :: Batch 0/1
Batch Loss = 4.437384613065716
1514, epoch_train_loss=4.437384613065716
Epoch 1515
Epoch 1515 :: Batch 0/1
Batch Loss = 4.437384612158092
1515, epoch_train_loss=4.437384612158092
Epoch 1516
Epoch 1516 :: Batch 0/1
Batch Loss = 4.437384611250236
1516, epoch_train_loss=4.437384611250236
Epoch 1517
Epoch 1517 :: Batch 0/1
Batch Loss = 4.437384610342144
1517, epoch_train_loss=4.437384610342144
Epoch 1518
Epoch 1518 :: Batch 0/1
Batch Loss = 4.437384609433812
1518, epoch_train_loss=4.437384609433812
Epoch 1519
Epoch 1519 :: Batch 0/1
Batch Loss = 4.437384608525234
1519, epoch_train_loss=4.437384608525234
Epoch 1520
Epoch 1520 :: Batch 0/1
Batch Loss = 4.437384607616407
1520, epoch_train_loss=4.437384607616407
Epoch 1521
Epoch 1521 :: Batch 0/1
Batch Loss = 4.437384606707323
1521, epoch_train_loss=4.437384606707323
Epoch 1522
Epoch 1522 :: Batch 0/1
Batch Loss = 4.4373846057979796
1522, epoch_train_loss=4.4373846057979796
Epoch 1523
Epoch 1523 :: Batch 0/1
Batch Loss = 4.43738460488837
1523, epoch_train_loss=4.43738460488837
Epoch 1524
Epoch 1524 :: Batch 0/1
Batch Loss = 4.437384603978491
1524, epoch_train_loss=4.437384603978491
Epoch 1525
Epoch 1525 :: Batch 0/1
Batch Loss = 4.4373846030683355
1525, epoch_train_loss=4.4373846030683355
Epoch 1526
Epoch 1526 :: Batch 0/1
Batch Loss = 4.437384602157901
1526, epoch_train_loss=4.437384602157901
Epoch 1527
Epoch 1527 :: Batch 0/1
Batch Loss = 4.437384601247179
1527, epoch_train_loss=4.437384601247179
Epoch 1528
Epoch 1528 :: Batch 0/1
Batch Loss = 4.437384600336168
1528, epoch_train_loss=4.437384600336168
Epoch 1529
Epoch 1529 :: Batch 0/1
Batch Loss = 4.4373845994248615
1529, epoch_train_loss=4.4373845994248615
Epoch 1530
Epoch 1530 :: Batch 0/1
Batch Loss = 4.437384598513254
1530, epoch_train_loss=4.437384598513254
Epoch 1531
Epoch 1531 :: Batch 0/1
Batch Loss = 4.4373845976013415
1531, epoch_train_loss=4.4373845976013415
Epoch 1532
Epoch 1532 :: Batch 0/1
Batch Loss = 4.437384596689118
1532, epoch_train_loss=4.437384596689118
Epoch 1533
Epoch 1533 :: Batch 0/1
Batch Loss = 4.43738459577658
1533, epoch_train_loss=4.43738459577658
Epoch 1534
Epoch 1534 :: Batch 0/1
Batch Loss = 4.437384594863721
1534, epoch_train_loss=4.437384594863721
Epoch 1535
Epoch 1535 :: Batch 0/1
Batch Loss = 4.437384593950536
1535, epoch_train_loss=4.437384593950536
Epoch 1536
Epoch 1536 :: Batch 0/1
Batch Loss = 4.437384593037019
1536, epoch_train_loss=4.437384593037019
Epoch 1537
Epoch 1537 :: Batch 0/1
Batch Loss = 4.437384592123167
1537, epoch_train_loss=4.437384592123167
Epoch 1538
Epoch 1538 :: Batch 0/1
Batch Loss = 4.437384591208975
1538, epoch_train_loss=4.437384591208975
Epoch 1539
Epoch 1539 :: Batch 0/1
Batch Loss = 4.437384590294436
1539, epoch_train_loss=4.437384590294436
Epoch 1540
Epoch 1540 :: Batch 0/1
Batch Loss = 4.437384589379546
1540, epoch_train_loss=4.437384589379546
Epoch 1541
Epoch 1541 :: Batch 0/1
Batch Loss = 4.437384588464299
1541, epoch_train_loss=4.437384588464299
Epoch 1542
Epoch 1542 :: Batch 0/1
Batch Loss = 4.437384587548692
1542, epoch_train_loss=4.437384587548692
Epoch 1543
Epoch 1543 :: Batch 0/1
Batch Loss = 4.437384586632717
1543, epoch_train_loss=4.437384586632717
Epoch 1544
Epoch 1544 :: Batch 0/1
Batch Loss = 4.43738458571637
1544, epoch_train_loss=4.43738458571637
Epoch 1545
Epoch 1545 :: Batch 0/1
Batch Loss = 4.437384584799648
1545, epoch_train_loss=4.437384584799648
Epoch 1546
Epoch 1546 :: Batch 0/1
Batch Loss = 4.437384583882541
1546, epoch_train_loss=4.437384583882541
Epoch 1547
Epoch 1547 :: Batch 0/1
Batch Loss = 4.4373845829650485
1547, epoch_train_loss=4.4373845829650485
Epoch 1548
Epoch 1548 :: Batch 0/1
Batch Loss = 4.437384582047164
1548, epoch_train_loss=4.437384582047164
Epoch 1549
Epoch 1549 :: Batch 0/1
Batch Loss = 4.437384581128882
1549, epoch_train_loss=4.437384581128882
Epoch 1550
Epoch 1550 :: Batch 0/1
Batch Loss = 4.437384580210195
1550, epoch_train_loss=4.437384580210195
Epoch 1551
Epoch 1551 :: Batch 0/1
Batch Loss = 4.437384579291101
1551, epoch_train_loss=4.437384579291101
Epoch 1552
Epoch 1552 :: Batch 0/1
Batch Loss = 4.437384578371594
1552, epoch_train_loss=4.437384578371594
Epoch 1553
Epoch 1553 :: Batch 0/1
Batch Loss = 4.4373845774516685
1553, epoch_train_loss=4.4373845774516685
Epoch 1554
Epoch 1554 :: Batch 0/1
Batch Loss = 4.4373845765313185
1554, epoch_train_loss=4.4373845765313185
Epoch 1555
Epoch 1555 :: Batch 0/1
Batch Loss = 4.43738457561054
1555, epoch_train_loss=4.43738457561054
Epoch 1556
Epoch 1556 :: Batch 0/1
Batch Loss = 4.437384574689326
1556, epoch_train_loss=4.437384574689326
Epoch 1557
Epoch 1557 :: Batch 0/1
Batch Loss = 4.437384573767673
1557, epoch_train_loss=4.437384573767673
Epoch 1558
Epoch 1558 :: Batch 0/1
Batch Loss = 4.437384572845576
1558, epoch_train_loss=4.437384572845576
Epoch 1559
Epoch 1559 :: Batch 0/1
Batch Loss = 4.437384571923028
1559, epoch_train_loss=4.437384571923028
Epoch 1560
Epoch 1560 :: Batch 0/1
Batch Loss = 4.437384571000023
1560, epoch_train_loss=4.437384571000023
Epoch 1561
Epoch 1561 :: Batch 0/1
Batch Loss = 4.437384570076559
1561, epoch_train_loss=4.437384570076559
Epoch 1562
Epoch 1562 :: Batch 0/1
Batch Loss = 4.437384569152629
1562, epoch_train_loss=4.437384569152629
Epoch 1563
Epoch 1563 :: Batch 0/1
Batch Loss = 4.437384568228225
1563, epoch_train_loss=4.437384568228225
Epoch 1564
Epoch 1564 :: Batch 0/1
Batch Loss = 4.437384567303346
1564, epoch_train_loss=4.437384567303346
Epoch 1565
Epoch 1565 :: Batch 0/1
Batch Loss = 4.437384566377985
1565, epoch_train_loss=4.437384566377985
Epoch 1566
Epoch 1566 :: Batch 0/1
Batch Loss = 4.437384565452135
1566, epoch_train_loss=4.437384565452135
Epoch 1567
Epoch 1567 :: Batch 0/1
Batch Loss = 4.437384564525793
1567, epoch_train_loss=4.437384564525793
Epoch 1568
Epoch 1568 :: Batch 0/1
Batch Loss = 4.437384563598952
1568, epoch_train_loss=4.437384563598952
Epoch 1569
Epoch 1569 :: Batch 0/1
Batch Loss = 4.437384562671607
1569, epoch_train_loss=4.437384562671607
Epoch 1570
Epoch 1570 :: Batch 0/1
Batch Loss = 4.437384561743753
1570, epoch_train_loss=4.437384561743753
Epoch 1571
Epoch 1571 :: Batch 0/1
Batch Loss = 4.437384560815384
1571, epoch_train_loss=4.437384560815384
Epoch 1572
Epoch 1572 :: Batch 0/1
Batch Loss = 4.437384559886494
1572, epoch_train_loss=4.437384559886494
Epoch 1573
Epoch 1573 :: Batch 0/1
Batch Loss = 4.4373845589570795
1573, epoch_train_loss=4.4373845589570795
Epoch 1574
Epoch 1574 :: Batch 0/1
Batch Loss = 4.437384558027134
1574, epoch_train_loss=4.437384558027134
Epoch 1575
Epoch 1575 :: Batch 0/1
Batch Loss = 4.43738455709665
1575, epoch_train_loss=4.43738455709665
Epoch 1576
Epoch 1576 :: Batch 0/1
Batch Loss = 4.437384556165624
1576, epoch_train_loss=4.437384556165624
Epoch 1577
Epoch 1577 :: Batch 0/1
Batch Loss = 4.437384555234051
1577, epoch_train_loss=4.437384555234051
Epoch 1578
Epoch 1578 :: Batch 0/1
Batch Loss = 4.437384554301926
1578, epoch_train_loss=4.437384554301926
Epoch 1579
Epoch 1579 :: Batch 0/1
Batch Loss = 4.437384553369241
1579, epoch_train_loss=4.437384553369241
Epoch 1580
Epoch 1580 :: Batch 0/1
Batch Loss = 4.437384552435992
1580, epoch_train_loss=4.437384552435992
Epoch 1581
Epoch 1581 :: Batch 0/1
Batch Loss = 4.437384551502172
1581, epoch_train_loss=4.437384551502172
Epoch 1582
Epoch 1582 :: Batch 0/1
Batch Loss = 4.437384550567778
1582, epoch_train_loss=4.437384550567778
Epoch 1583
Epoch 1583 :: Batch 0/1
Batch Loss = 4.437384549632804
1583, epoch_train_loss=4.437384549632804
Epoch 1584
Epoch 1584 :: Batch 0/1
Batch Loss = 4.43738454869724
1584, epoch_train_loss=4.43738454869724
Epoch 1585
Epoch 1585 :: Batch 0/1
Batch Loss = 4.437384547761086
1585, epoch_train_loss=4.437384547761086
Epoch 1586
Epoch 1586 :: Batch 0/1
Batch Loss = 4.437384546824333
1586, epoch_train_loss=4.437384546824333
Epoch 1587
Epoch 1587 :: Batch 0/1
Batch Loss = 4.437384545886977
1587, epoch_train_loss=4.437384545886977
Epoch 1588
Epoch 1588 :: Batch 0/1
Batch Loss = 4.437384544949011
1588, epoch_train_loss=4.437384544949011
Epoch 1589
Epoch 1589 :: Batch 0/1
Batch Loss = 4.437384544010431
1589, epoch_train_loss=4.437384544010431
Epoch 1590
Epoch 1590 :: Batch 0/1
Batch Loss = 4.43738454307123
1590, epoch_train_loss=4.43738454307123
Epoch 1591
Epoch 1591 :: Batch 0/1
Batch Loss = 4.437384542131404
1591, epoch_train_loss=4.437384542131404
Epoch 1592
Epoch 1592 :: Batch 0/1
Batch Loss = 4.437384541190944
1592, epoch_train_loss=4.437384541190944
Epoch 1593
Epoch 1593 :: Batch 0/1
Batch Loss = 4.437384540249848
1593, epoch_train_loss=4.437384540249848
Epoch 1594
Epoch 1594 :: Batch 0/1
Batch Loss = 4.437384539308106
1594, epoch_train_loss=4.437384539308106
Epoch 1595
Epoch 1595 :: Batch 0/1
Batch Loss = 4.437384538365717
1595, epoch_train_loss=4.437384538365717
Epoch 1596
Epoch 1596 :: Batch 0/1
Batch Loss = 4.4373845374226715
1596, epoch_train_loss=4.4373845374226715
Epoch 1597
Epoch 1597 :: Batch 0/1
Batch Loss = 4.437384536478966
1597, epoch_train_loss=4.437384536478966
Epoch 1598
Epoch 1598 :: Batch 0/1
Batch Loss = 4.437384535534594
1598, epoch_train_loss=4.437384535534594
Epoch 1599
Epoch 1599 :: Batch 0/1
Batch Loss = 4.437384534589549
1599, epoch_train_loss=4.437384534589549
Epoch 1600
Epoch 1600 :: Batch 0/1
Batch Loss = 4.437384533643827
1600, epoch_train_loss=4.437384533643827
Epoch 1601
Epoch 1601 :: Batch 0/1
Batch Loss = 4.437384532697419
1601, epoch_train_loss=4.437384532697419
Epoch 1602
Epoch 1602 :: Batch 0/1
Batch Loss = 4.437384531750322
1602, epoch_train_loss=4.437384531750322
Epoch 1603
Epoch 1603 :: Batch 0/1
Batch Loss = 4.437384530802529
1603, epoch_train_loss=4.437384530802529
Epoch 1604
Epoch 1604 :: Batch 0/1
Batch Loss = 4.437384529854033
1604, epoch_train_loss=4.437384529854033
Epoch 1605
Epoch 1605 :: Batch 0/1
Batch Loss = 4.43738452890483
1605, epoch_train_loss=4.43738452890483
Epoch 1606
Epoch 1606 :: Batch 0/1
Batch Loss = 4.437384527954913
1606, epoch_train_loss=4.437384527954913
Epoch 1607
Epoch 1607 :: Batch 0/1
Batch Loss = 4.437384527004278
1607, epoch_train_loss=4.437384527004278
Epoch 1608
Epoch 1608 :: Batch 0/1
Batch Loss = 4.437384526052916
1608, epoch_train_loss=4.437384526052916
Epoch 1609
Epoch 1609 :: Batch 0/1
Batch Loss = 4.437384525100823
1609, epoch_train_loss=4.437384525100823
Epoch 1610
Epoch 1610 :: Batch 0/1
Batch Loss = 4.4373845241479914
1610, epoch_train_loss=4.4373845241479914
Epoch 1611
Epoch 1611 :: Batch 0/1
Batch Loss = 4.437384523194418
1611, epoch_train_loss=4.437384523194418
Epoch 1612
Epoch 1612 :: Batch 0/1
Batch Loss = 4.437384522240093
1612, epoch_train_loss=4.437384522240093
Epoch 1613
Epoch 1613 :: Batch 0/1
Batch Loss = 4.437384521285014
1613, epoch_train_loss=4.437384521285014
Epoch 1614
Epoch 1614 :: Batch 0/1
Batch Loss = 4.437384520329172
1614, epoch_train_loss=4.437384520329172
Epoch 1615
Epoch 1615 :: Batch 0/1
Batch Loss = 4.4373845193725625
1615, epoch_train_loss=4.4373845193725625
Epoch 1616
Epoch 1616 :: Batch 0/1
Batch Loss = 4.43738451841518
1616, epoch_train_loss=4.43738451841518
Epoch 1617
Epoch 1617 :: Batch 0/1
Batch Loss = 4.4373845174570175
1617, epoch_train_loss=4.4373845174570175
Epoch 1618
Epoch 1618 :: Batch 0/1
Batch Loss = 4.437384516498067
1618, epoch_train_loss=4.437384516498067
Epoch 1619
Epoch 1619 :: Batch 0/1
Batch Loss = 4.437384515538327
1619, epoch_train_loss=4.437384515538327
Epoch 1620
Epoch 1620 :: Batch 0/1
Batch Loss = 4.437384514577786
1620, epoch_train_loss=4.437384514577786
Epoch 1621
Epoch 1621 :: Batch 0/1
Batch Loss = 4.4373845136164425
1621, epoch_train_loss=4.4373845136164425
Epoch 1622
Epoch 1622 :: Batch 0/1
Batch Loss = 4.437384512654287
1622, epoch_train_loss=4.437384512654287
Epoch 1623
Epoch 1623 :: Batch 0/1
Batch Loss = 4.437384511691314
1623, epoch_train_loss=4.437384511691314
Epoch 1624
Epoch 1624 :: Batch 0/1
Batch Loss = 4.437384510727518
1624, epoch_train_loss=4.437384510727518
Epoch 1625
Epoch 1625 :: Batch 0/1
Batch Loss = 4.437384509762892
1625, epoch_train_loss=4.437384509762892
Epoch 1626
Epoch 1626 :: Batch 0/1
Batch Loss = 4.4373845087974315
1626, epoch_train_loss=4.4373845087974315
Epoch 1627
Epoch 1627 :: Batch 0/1
Batch Loss = 4.437384507831127
1627, epoch_train_loss=4.437384507831127
Epoch 1628
Epoch 1628 :: Batch 0/1
Batch Loss = 4.4373845068639755
1628, epoch_train_loss=4.4373845068639755
Epoch 1629
Epoch 1629 :: Batch 0/1
Batch Loss = 4.437384505895969
1629, epoch_train_loss=4.437384505895969
Epoch 1630
Epoch 1630 :: Batch 0/1
Batch Loss = 4.437384504927101
1630, epoch_train_loss=4.437384504927101
Epoch 1631
Epoch 1631 :: Batch 0/1
Batch Loss = 4.4373845039573645
1631, epoch_train_loss=4.4373845039573645
Epoch 1632
Epoch 1632 :: Batch 0/1
Batch Loss = 4.437384502986755
1632, epoch_train_loss=4.437384502986755
Epoch 1633
Epoch 1633 :: Batch 0/1
Batch Loss = 4.437384502015266
1633, epoch_train_loss=4.437384502015266
Epoch 1634
Epoch 1634 :: Batch 0/1
Batch Loss = 4.437384501042889
1634, epoch_train_loss=4.437384501042889
Epoch 1635
Epoch 1635 :: Batch 0/1
Batch Loss = 4.437384500069618
1635, epoch_train_loss=4.437384500069618
Epoch 1636
Epoch 1636 :: Batch 0/1
Batch Loss = 4.43738449909545
1636, epoch_train_loss=4.43738449909545
Epoch 1637
Epoch 1637 :: Batch 0/1
Batch Loss = 4.437384498120375
1637, epoch_train_loss=4.437384498120375
Epoch 1638
Epoch 1638 :: Batch 0/1
Batch Loss = 4.437384497144386
1638, epoch_train_loss=4.437384497144386
Epoch 1639
Epoch 1639 :: Batch 0/1
Batch Loss = 4.4373844961674775
1639, epoch_train_loss=4.4373844961674775
Epoch 1640
Epoch 1640 :: Batch 0/1
Batch Loss = 4.437384495189644
1640, epoch_train_loss=4.437384495189644
Epoch 1641
Epoch 1641 :: Batch 0/1
Batch Loss = 4.437384494210879
1641, epoch_train_loss=4.437384494210879
Epoch 1642
Epoch 1642 :: Batch 0/1
Batch Loss = 4.437384493231173
1642, epoch_train_loss=4.437384493231173
Epoch 1643
Epoch 1643 :: Batch 0/1
Batch Loss = 4.437384492250523
1643, epoch_train_loss=4.437384492250523
Epoch 1644
Epoch 1644 :: Batch 0/1
Batch Loss = 4.4373844912689195
1644, epoch_train_loss=4.4373844912689195
Epoch 1645
Epoch 1645 :: Batch 0/1
Batch Loss = 4.437384490286358
1645, epoch_train_loss=4.437384490286358
Epoch 1646
Epoch 1646 :: Batch 0/1
Batch Loss = 4.43738448930283
1646, epoch_train_loss=4.43738448930283
Epoch 1647
Epoch 1647 :: Batch 0/1
Batch Loss = 4.43738448831833
1647, epoch_train_loss=4.43738448831833
Epoch 1648
Epoch 1648 :: Batch 0/1
Batch Loss = 4.437384487332853
1648, epoch_train_loss=4.437384487332853
Epoch 1649
Epoch 1649 :: Batch 0/1
Batch Loss = 4.437384486346387
1649, epoch_train_loss=4.437384486346387
Epoch 1650
Epoch 1650 :: Batch 0/1
Batch Loss = 4.437384485358932
1650, epoch_train_loss=4.437384485358932
Epoch 1651
Epoch 1651 :: Batch 0/1
Batch Loss = 4.437384484370474
1651, epoch_train_loss=4.437384484370474
Epoch 1652
Epoch 1652 :: Batch 0/1
Batch Loss = 4.437384483381012
1652, epoch_train_loss=4.437384483381012
Epoch 1653
Epoch 1653 :: Batch 0/1
Batch Loss = 4.437384482390538
1653, epoch_train_loss=4.437384482390538
Epoch 1654
Epoch 1654 :: Batch 0/1
Batch Loss = 4.437384481399042
1654, epoch_train_loss=4.437384481399042
Epoch 1655
Epoch 1655 :: Batch 0/1
Batch Loss = 4.43738448040652
1655, epoch_train_loss=4.43738448040652
Epoch 1656
Epoch 1656 :: Batch 0/1
Batch Loss = 4.437384479412964
1656, epoch_train_loss=4.437384479412964
Epoch 1657
Epoch 1657 :: Batch 0/1
Batch Loss = 4.437384478418368
1657, epoch_train_loss=4.437384478418368
Epoch 1658
Epoch 1658 :: Batch 0/1
Batch Loss = 4.437384477422724
1658, epoch_train_loss=4.437384477422724
Epoch 1659
Epoch 1659 :: Batch 0/1
Batch Loss = 4.437384476426026
1659, epoch_train_loss=4.437384476426026
Epoch 1660
Epoch 1660 :: Batch 0/1
Batch Loss = 4.437384475428266
1660, epoch_train_loss=4.437384475428266
Epoch 1661
Epoch 1661 :: Batch 0/1
Batch Loss = 4.437384474429437
1661, epoch_train_loss=4.437384474429437
Epoch 1662
Epoch 1662 :: Batch 0/1
Batch Loss = 4.437384473429534
1662, epoch_train_loss=4.437384473429534
Epoch 1663
Epoch 1663 :: Batch 0/1
Batch Loss = 4.437384472428548
1663, epoch_train_loss=4.437384472428548
Epoch 1664
Epoch 1664 :: Batch 0/1
Batch Loss = 4.437384471426472
1664, epoch_train_loss=4.437384471426472
Epoch 1665
Epoch 1665 :: Batch 0/1
Batch Loss = 4.437384470423298
1665, epoch_train_loss=4.437384470423298
Epoch 1666
Epoch 1666 :: Batch 0/1
Batch Loss = 4.437384469419021
1666, epoch_train_loss=4.437384469419021
Epoch 1667
Epoch 1667 :: Batch 0/1
Batch Loss = 4.437384468413632
1667, epoch_train_loss=4.437384468413632
Epoch 1668
Epoch 1668 :: Batch 0/1
Batch Loss = 4.437384467407126
1668, epoch_train_loss=4.437384467407126
Epoch 1669
Epoch 1669 :: Batch 0/1
Batch Loss = 4.437384466399493
1669, epoch_train_loss=4.437384466399493
Epoch 1670
Epoch 1670 :: Batch 0/1
Batch Loss = 4.437384465390727
1670, epoch_train_loss=4.437384465390727
Epoch 1671
Epoch 1671 :: Batch 0/1
Batch Loss = 4.437384464380822
1671, epoch_train_loss=4.437384464380822
Epoch 1672
Epoch 1672 :: Batch 0/1
Batch Loss = 4.437384463369769
1672, epoch_train_loss=4.437384463369769
Epoch 1673
Epoch 1673 :: Batch 0/1
Batch Loss = 4.437384462357561
1673, epoch_train_loss=4.437384462357561
Epoch 1674
Epoch 1674 :: Batch 0/1
Batch Loss = 4.437384461344192
1674, epoch_train_loss=4.437384461344192
Epoch 1675
Epoch 1675 :: Batch 0/1
Batch Loss = 4.437384460329652
1675, epoch_train_loss=4.437384460329652
Epoch 1676
Epoch 1676 :: Batch 0/1
Batch Loss = 4.437384459313935
1676, epoch_train_loss=4.437384459313935
Epoch 1677
Epoch 1677 :: Batch 0/1
Batch Loss = 4.437384458297035
1677, epoch_train_loss=4.437384458297035
Epoch 1678
Epoch 1678 :: Batch 0/1
Batch Loss = 4.437384457278942
1678, epoch_train_loss=4.437384457278942
Epoch 1679
Epoch 1679 :: Batch 0/1
Batch Loss = 4.437384456259649
1679, epoch_train_loss=4.437384456259649
Epoch 1680
Epoch 1680 :: Batch 0/1
Batch Loss = 4.43738445523915
1680, epoch_train_loss=4.43738445523915
Epoch 1681
Epoch 1681 :: Batch 0/1
Batch Loss = 4.437384454217437
1681, epoch_train_loss=4.437384454217437
Epoch 1682
Epoch 1682 :: Batch 0/1
Batch Loss = 4.437384453194502
1682, epoch_train_loss=4.437384453194502
Epoch 1683
Epoch 1683 :: Batch 0/1
Batch Loss = 4.437384452170337
1683, epoch_train_loss=4.437384452170337
Epoch 1684
Epoch 1684 :: Batch 0/1
Batch Loss = 4.4373844511449345
1684, epoch_train_loss=4.4373844511449345
Epoch 1685
Epoch 1685 :: Batch 0/1
Batch Loss = 4.437384450118287
1685, epoch_train_loss=4.437384450118287
Epoch 1686
Epoch 1686 :: Batch 0/1
Batch Loss = 4.437384449090388
1686, epoch_train_loss=4.437384449090388
Epoch 1687
Epoch 1687 :: Batch 0/1
Batch Loss = 4.437384448061229
1687, epoch_train_loss=4.437384448061229
Epoch 1688
Epoch 1688 :: Batch 0/1
Batch Loss = 4.4373844470308
1688, epoch_train_loss=4.4373844470308
Epoch 1689
Epoch 1689 :: Batch 0/1
Batch Loss = 4.437384445999097
1689, epoch_train_loss=4.437384445999097
Epoch 1690
Epoch 1690 :: Batch 0/1
Batch Loss = 4.43738444496611
1690, epoch_train_loss=4.43738444496611
Epoch 1691
Epoch 1691 :: Batch 0/1
Batch Loss = 4.4373844439318315
1691, epoch_train_loss=4.4373844439318315
Epoch 1692
Epoch 1692 :: Batch 0/1
Batch Loss = 4.437384442896253
1692, epoch_train_loss=4.437384442896253
Epoch 1693
Epoch 1693 :: Batch 0/1
Batch Loss = 4.437384441859368
1693, epoch_train_loss=4.437384441859368
Epoch 1694
Epoch 1694 :: Batch 0/1
Batch Loss = 4.437384440821167
1694, epoch_train_loss=4.437384440821167
Epoch 1695
Epoch 1695 :: Batch 0/1
Batch Loss = 4.437384439781644
1695, epoch_train_loss=4.437384439781644
Epoch 1696
Epoch 1696 :: Batch 0/1
Batch Loss = 4.43738443874079
1696, epoch_train_loss=4.43738443874079
Epoch 1697
Epoch 1697 :: Batch 0/1
Batch Loss = 4.437384437698597
1697, epoch_train_loss=4.437384437698597
Epoch 1698
Epoch 1698 :: Batch 0/1
Batch Loss = 4.437384436655056
1698, epoch_train_loss=4.437384436655056
Epoch 1699
Epoch 1699 :: Batch 0/1
Batch Loss = 4.437384435610159
1699, epoch_train_loss=4.437384435610159
Epoch 1700
Epoch 1700 :: Batch 0/1
Batch Loss = 4.4373844345639
1700, epoch_train_loss=4.4373844345639
Epoch 1701
Epoch 1701 :: Batch 0/1
Batch Loss = 4.437384433516269
1701, epoch_train_loss=4.437384433516269
Epoch 1702
Epoch 1702 :: Batch 0/1
Batch Loss = 4.43738443246726
1702, epoch_train_loss=4.43738443246726
Epoch 1703
Epoch 1703 :: Batch 0/1
Batch Loss = 4.437384431416862
1703, epoch_train_loss=4.437384431416862
Epoch 1704
Epoch 1704 :: Batch 0/1
Batch Loss = 4.437384430365067
1704, epoch_train_loss=4.437384430365067
Epoch 1705
Epoch 1705 :: Batch 0/1
Batch Loss = 4.437384429311868
1705, epoch_train_loss=4.437384429311868
Epoch 1706
Epoch 1706 :: Batch 0/1
Batch Loss = 4.437384428257258
1706, epoch_train_loss=4.437384428257258
Epoch 1707
Epoch 1707 :: Batch 0/1
Batch Loss = 4.437384427201224
1707, epoch_train_loss=4.437384427201224
Epoch 1708
Epoch 1708 :: Batch 0/1
Batch Loss = 4.437384426143763
1708, epoch_train_loss=4.437384426143763
Epoch 1709
Epoch 1709 :: Batch 0/1
Batch Loss = 4.437384425084864
1709, epoch_train_loss=4.437384425084864
Epoch 1710
Epoch 1710 :: Batch 0/1
Batch Loss = 4.4373844240245175
1710, epoch_train_loss=4.4373844240245175
Epoch 1711
Epoch 1711 :: Batch 0/1
Batch Loss = 4.437384422962717
1711, epoch_train_loss=4.437384422962717
Epoch 1712
Epoch 1712 :: Batch 0/1
Batch Loss = 4.437384421899454
1712, epoch_train_loss=4.437384421899454
Epoch 1713
Epoch 1713 :: Batch 0/1
Batch Loss = 4.437384420834718
1713, epoch_train_loss=4.437384420834718
Epoch 1714
Epoch 1714 :: Batch 0/1
Batch Loss = 4.437384419768502
1714, epoch_train_loss=4.437384419768502
Epoch 1715
Epoch 1715 :: Batch 0/1
Batch Loss = 4.4373844187007965
1715, epoch_train_loss=4.4373844187007965
Epoch 1716
Epoch 1716 :: Batch 0/1
Batch Loss = 4.437384417631594
1716, epoch_train_loss=4.437384417631594
Epoch 1717
Epoch 1717 :: Batch 0/1
Batch Loss = 4.437384416560886
1717, epoch_train_loss=4.437384416560886
Epoch 1718
Epoch 1718 :: Batch 0/1
Batch Loss = 4.437384415488662
1718, epoch_train_loss=4.437384415488662
Epoch 1719
Epoch 1719 :: Batch 0/1
Batch Loss = 4.437384414414914
1719, epoch_train_loss=4.437384414414914
Epoch 1720
Epoch 1720 :: Batch 0/1
Batch Loss = 4.437384413339633
1720, epoch_train_loss=4.437384413339633
Epoch 1721
Epoch 1721 :: Batch 0/1
Batch Loss = 4.437384412262812
1721, epoch_train_loss=4.437384412262812
Epoch 1722
Epoch 1722 :: Batch 0/1
Batch Loss = 4.43738441118444
1722, epoch_train_loss=4.43738441118444
Epoch 1723
Epoch 1723 :: Batch 0/1
Batch Loss = 4.437384410104508
1723, epoch_train_loss=4.437384410104508
Epoch 1724
Epoch 1724 :: Batch 0/1
Batch Loss = 4.437384409023009
1724, epoch_train_loss=4.437384409023009
Epoch 1725
Epoch 1725 :: Batch 0/1
Batch Loss = 4.437384407939932
1725, epoch_train_loss=4.437384407939932
Epoch 1726
Epoch 1726 :: Batch 0/1
Batch Loss = 4.43738440685527
1726, epoch_train_loss=4.43738440685527
Epoch 1727
Epoch 1727 :: Batch 0/1
Batch Loss = 4.437384405769012
1727, epoch_train_loss=4.437384405769012
Epoch 1728
Epoch 1728 :: Batch 0/1
Batch Loss = 4.43738440468115
1728, epoch_train_loss=4.43738440468115
Epoch 1729
Epoch 1729 :: Batch 0/1
Batch Loss = 4.4373844035916745
1729, epoch_train_loss=4.4373844035916745
Epoch 1730
Epoch 1730 :: Batch 0/1
Batch Loss = 4.437384402500577
1730, epoch_train_loss=4.437384402500577
Epoch 1731
Epoch 1731 :: Batch 0/1
Batch Loss = 4.437384401407845
1731, epoch_train_loss=4.437384401407845
Epoch 1732
Epoch 1732 :: Batch 0/1
Batch Loss = 4.437384400313475
1732, epoch_train_loss=4.437384400313475
Epoch 1733
Epoch 1733 :: Batch 0/1
Batch Loss = 4.4373843992174535
1733, epoch_train_loss=4.4373843992174535
Epoch 1734
Epoch 1734 :: Batch 0/1
Batch Loss = 4.4373843981197725
1734, epoch_train_loss=4.4373843981197725
Epoch 1735
Epoch 1735 :: Batch 0/1
Batch Loss = 4.437384397020422
1735, epoch_train_loss=4.437384397020422
Epoch 1736
Epoch 1736 :: Batch 0/1
Batch Loss = 4.437384395919393
1736, epoch_train_loss=4.437384395919393
Epoch 1737
Epoch 1737 :: Batch 0/1
Batch Loss = 4.437384394816675
1737, epoch_train_loss=4.437384394816675
Epoch 1738
Epoch 1738 :: Batch 0/1
Batch Loss = 4.437384393712261
1738, epoch_train_loss=4.437384393712261
Epoch 1739
Epoch 1739 :: Batch 0/1
Batch Loss = 4.43738439260614
1739, epoch_train_loss=4.43738439260614
Epoch 1740
Epoch 1740 :: Batch 0/1
Batch Loss = 4.437384391498302
1740, epoch_train_loss=4.437384391498302
Epoch 1741
Epoch 1741 :: Batch 0/1
Batch Loss = 4.437384390388738
1741, epoch_train_loss=4.437384390388738
Epoch 1742
Epoch 1742 :: Batch 0/1
Batch Loss = 4.437384389277438
1742, epoch_train_loss=4.437384389277438
Epoch 1743
Epoch 1743 :: Batch 0/1
Batch Loss = 4.437384388164392
1743, epoch_train_loss=4.437384388164392
Epoch 1744
Epoch 1744 :: Batch 0/1
Batch Loss = 4.4373843870495895
1744, epoch_train_loss=4.4373843870495895
Epoch 1745
Epoch 1745 :: Batch 0/1
Batch Loss = 4.437384385933024
1745, epoch_train_loss=4.437384385933024
Epoch 1746
Epoch 1746 :: Batch 0/1
Batch Loss = 4.437384384814683
1746, epoch_train_loss=4.437384384814683
Epoch 1747
Epoch 1747 :: Batch 0/1
Batch Loss = 4.437384383694556
1747, epoch_train_loss=4.437384383694556
Epoch 1748
Epoch 1748 :: Batch 0/1
Batch Loss = 4.437384382572636
1748, epoch_train_loss=4.437384382572636
Epoch 1749
Epoch 1749 :: Batch 0/1
Batch Loss = 4.4373843814489105
1749, epoch_train_loss=4.4373843814489105
Epoch 1750
Epoch 1750 :: Batch 0/1
Batch Loss = 4.43738438032337
1750, epoch_train_loss=4.43738438032337
Epoch 1751
Epoch 1751 :: Batch 0/1
Batch Loss = 4.437384379196003
1751, epoch_train_loss=4.437384379196003
Epoch 1752
Epoch 1752 :: Batch 0/1
Batch Loss = 4.437384378066804
1752, epoch_train_loss=4.437384378066804
Epoch 1753
Epoch 1753 :: Batch 0/1
Batch Loss = 4.437384376935759
1753, epoch_train_loss=4.437384376935759
Epoch 1754
Epoch 1754 :: Batch 0/1
Batch Loss = 4.437384375802857
1754, epoch_train_loss=4.437384375802857
Epoch 1755
Epoch 1755 :: Batch 0/1
Batch Loss = 4.437384374668091
1755, epoch_train_loss=4.437384374668091
Epoch 1756
Epoch 1756 :: Batch 0/1
Batch Loss = 4.437384373531448
1756, epoch_train_loss=4.437384373531448
Epoch 1757
Epoch 1757 :: Batch 0/1
Batch Loss = 4.43738437239292
1757, epoch_train_loss=4.43738437239292
Epoch 1758
Epoch 1758 :: Batch 0/1
Batch Loss = 4.437384371252494
1758, epoch_train_loss=4.437384371252494
Epoch 1759
Epoch 1759 :: Batch 0/1
Batch Loss = 4.437384370110163
1759, epoch_train_loss=4.437384370110163
Epoch 1760
Epoch 1760 :: Batch 0/1
Batch Loss = 4.437384368965912
1760, epoch_train_loss=4.437384368965912
Epoch 1761
Epoch 1761 :: Batch 0/1
Batch Loss = 4.437384367819735
1761, epoch_train_loss=4.437384367819735
Epoch 1762
Epoch 1762 :: Batch 0/1
Batch Loss = 4.437384366671618
1762, epoch_train_loss=4.437384366671618
Epoch 1763
Epoch 1763 :: Batch 0/1
Batch Loss = 4.4373843655215515
1763, epoch_train_loss=4.4373843655215515
Epoch 1764
Epoch 1764 :: Batch 0/1
Batch Loss = 4.437384364369527
1764, epoch_train_loss=4.437384364369527
Epoch 1765
Epoch 1765 :: Batch 0/1
Batch Loss = 4.437384363215529
1765, epoch_train_loss=4.437384363215529
Epoch 1766
Epoch 1766 :: Batch 0/1
Batch Loss = 4.437384362059552
1766, epoch_train_loss=4.437384362059552
Epoch 1767
Epoch 1767 :: Batch 0/1
Batch Loss = 4.437384360901581
1767, epoch_train_loss=4.437384360901581
Epoch 1768
Epoch 1768 :: Batch 0/1
Batch Loss = 4.437384359741608
1768, epoch_train_loss=4.437384359741608
Epoch 1769
Epoch 1769 :: Batch 0/1
Batch Loss = 4.43738435857962
1769, epoch_train_loss=4.43738435857962
Epoch 1770
Epoch 1770 :: Batch 0/1
Batch Loss = 4.437384357415606
1770, epoch_train_loss=4.437384357415606
Epoch 1771
Epoch 1771 :: Batch 0/1
Batch Loss = 4.437384356249557
1771, epoch_train_loss=4.437384356249557
Epoch 1772
Epoch 1772 :: Batch 0/1
Batch Loss = 4.437384355081461
1772, epoch_train_loss=4.437384355081461
Epoch 1773
Epoch 1773 :: Batch 0/1
Batch Loss = 4.437384353911305
1773, epoch_train_loss=4.437384353911305
Epoch 1774
Epoch 1774 :: Batch 0/1
Batch Loss = 4.437384352739081
1774, epoch_train_loss=4.437384352739081
Epoch 1775
Epoch 1775 :: Batch 0/1
Batch Loss = 4.437384351564775
1775, epoch_train_loss=4.437384351564775
Epoch 1776
Epoch 1776 :: Batch 0/1
Batch Loss = 4.437384350388378
1776, epoch_train_loss=4.437384350388378
Epoch 1777
Epoch 1777 :: Batch 0/1
Batch Loss = 4.437384349209876
1777, epoch_train_loss=4.437384349209876
Epoch 1778
Epoch 1778 :: Batch 0/1
Batch Loss = 4.4373843480292585
1778, epoch_train_loss=4.4373843480292585
Epoch 1779
Epoch 1779 :: Batch 0/1
Batch Loss = 4.437384346846517
1779, epoch_train_loss=4.437384346846517
Epoch 1780
Epoch 1780 :: Batch 0/1
Batch Loss = 4.437384345661636
1780, epoch_train_loss=4.437384345661636
Epoch 1781
Epoch 1781 :: Batch 0/1
Batch Loss = 4.437384344474605
1781, epoch_train_loss=4.437384344474605
Epoch 1782
Epoch 1782 :: Batch 0/1
Batch Loss = 4.437384343285414
1782, epoch_train_loss=4.437384343285414
Epoch 1783
Epoch 1783 :: Batch 0/1
Batch Loss = 4.437384342094049
1783, epoch_train_loss=4.437384342094049
Epoch 1784
Epoch 1784 :: Batch 0/1
Batch Loss = 4.4373843409005
1784, epoch_train_loss=4.4373843409005
Epoch 1785
Epoch 1785 :: Batch 0/1
Batch Loss = 4.437384339704753
1785, epoch_train_loss=4.437384339704753
Epoch 1786
Epoch 1786 :: Batch 0/1
Batch Loss = 4.437384338506799
1786, epoch_train_loss=4.437384338506799
Epoch 1787
Epoch 1787 :: Batch 0/1
Batch Loss = 4.437384337306623
1787, epoch_train_loss=4.437384337306623
Epoch 1788
Epoch 1788 :: Batch 0/1
Batch Loss = 4.437384336104217
1788, epoch_train_loss=4.437384336104217
Epoch 1789
Epoch 1789 :: Batch 0/1
Batch Loss = 4.437384334899565
1789, epoch_train_loss=4.437384334899565
Epoch 1790
Epoch 1790 :: Batch 0/1
Batch Loss = 4.437384333692657
1790, epoch_train_loss=4.437384333692657
Epoch 1791
Epoch 1791 :: Batch 0/1
Batch Loss = 4.437384332483481
1791, epoch_train_loss=4.437384332483481
Epoch 1792
Epoch 1792 :: Batch 0/1
Batch Loss = 4.437384331272023
1792, epoch_train_loss=4.437384331272023
Epoch 1793
Epoch 1793 :: Batch 0/1
Batch Loss = 4.437384330058272
1793, epoch_train_loss=4.437384330058272
Epoch 1794
Epoch 1794 :: Batch 0/1
Batch Loss = 4.437384328842216
1794, epoch_train_loss=4.437384328842216
Epoch 1795
Epoch 1795 :: Batch 0/1
Batch Loss = 4.437384327623841
1795, epoch_train_loss=4.437384327623841
Epoch 1796
Epoch 1796 :: Batch 0/1
Batch Loss = 4.437384326403136
1796, epoch_train_loss=4.437384326403136
Epoch 1797
Epoch 1797 :: Batch 0/1
Batch Loss = 4.437384325180089
1797, epoch_train_loss=4.437384325180089
Epoch 1798
Epoch 1798 :: Batch 0/1
Batch Loss = 4.437384323954685
1798, epoch_train_loss=4.437384323954685
Epoch 1799
Epoch 1799 :: Batch 0/1
Batch Loss = 4.437384322726913
1799, epoch_train_loss=4.437384322726913
Epoch 1800
Epoch 1800 :: Batch 0/1
Batch Loss = 4.4373843214967605
1800, epoch_train_loss=4.4373843214967605
Epoch 1801
Epoch 1801 :: Batch 0/1
Batch Loss = 4.437384320264214
1801, epoch_train_loss=4.437384320264214
Epoch 1802
Epoch 1802 :: Batch 0/1
Batch Loss = 4.437384319029261
1802, epoch_train_loss=4.437384319029261
Epoch 1803
Epoch 1803 :: Batch 0/1
Batch Loss = 4.437384317791888
1803, epoch_train_loss=4.437384317791888
Epoch 1804
Epoch 1804 :: Batch 0/1
Batch Loss = 4.437384316552082
1804, epoch_train_loss=4.437384316552082
Epoch 1805
Epoch 1805 :: Batch 0/1
Batch Loss = 4.437384315309831
1805, epoch_train_loss=4.437384315309831
Epoch 1806
Epoch 1806 :: Batch 0/1
Batch Loss = 4.437384314065121
1806, epoch_train_loss=4.437384314065121
Epoch 1807
Epoch 1807 :: Batch 0/1
Batch Loss = 4.43738431281794
1807, epoch_train_loss=4.43738431281794
Epoch 1808
Epoch 1808 :: Batch 0/1
Batch Loss = 4.437384311568272
1808, epoch_train_loss=4.437384311568272
Epoch 1809
Epoch 1809 :: Batch 0/1
Batch Loss = 4.437384310316106
1809, epoch_train_loss=4.437384310316106
Epoch 1810
Epoch 1810 :: Batch 0/1
Batch Loss = 4.437384309061429
1810, epoch_train_loss=4.437384309061429
Epoch 1811
Epoch 1811 :: Batch 0/1
Batch Loss = 4.437384307804224
1811, epoch_train_loss=4.437384307804224
Epoch 1812
Epoch 1812 :: Batch 0/1
Batch Loss = 4.437384306544482
1812, epoch_train_loss=4.437384306544482
Epoch 1813
Epoch 1813 :: Batch 0/1
Batch Loss = 4.437384305282187
1813, epoch_train_loss=4.437384305282187
Epoch 1814
Epoch 1814 :: Batch 0/1
Batch Loss = 4.437384304017326
1814, epoch_train_loss=4.437384304017326
Epoch 1815
Epoch 1815 :: Batch 0/1
Batch Loss = 4.437384302749884
1815, epoch_train_loss=4.437384302749884
Epoch 1816
Epoch 1816 :: Batch 0/1
Batch Loss = 4.437384301479847
1816, epoch_train_loss=4.437384301479847
Epoch 1817
Epoch 1817 :: Batch 0/1
Batch Loss = 4.437384300207204
1817, epoch_train_loss=4.437384300207204
Epoch 1818
Epoch 1818 :: Batch 0/1
Batch Loss = 4.4373842989319385
1818, epoch_train_loss=4.4373842989319385
Epoch 1819
Epoch 1819 :: Batch 0/1
Batch Loss = 4.437384297654036
1819, epoch_train_loss=4.437384297654036
Epoch 1820
Epoch 1820 :: Batch 0/1
Batch Loss = 4.4373842963734855
1820, epoch_train_loss=4.4373842963734855
Epoch 1821
Epoch 1821 :: Batch 0/1
Batch Loss = 4.43738429509027
1821, epoch_train_loss=4.43738429509027
Epoch 1822
Epoch 1822 :: Batch 0/1
Batch Loss = 4.437384293804376
1822, epoch_train_loss=4.437384293804376
Epoch 1823
Epoch 1823 :: Batch 0/1
Batch Loss = 4.437384292515788
1823, epoch_train_loss=4.437384292515788
Epoch 1824
Epoch 1824 :: Batch 0/1
Batch Loss = 4.437384291224495
1824, epoch_train_loss=4.437384291224495
Epoch 1825
Epoch 1825 :: Batch 0/1
Batch Loss = 4.437384289930478
1825, epoch_train_loss=4.437384289930478
Epoch 1826
Epoch 1826 :: Batch 0/1
Batch Loss = 4.437384288633725
1826, epoch_train_loss=4.437384288633725
Epoch 1827
Epoch 1827 :: Batch 0/1
Batch Loss = 4.437384287334221
1827, epoch_train_loss=4.437384287334221
Epoch 1828
Epoch 1828 :: Batch 0/1
Batch Loss = 4.437384286031952
1828, epoch_train_loss=4.437384286031952
Epoch 1829
Epoch 1829 :: Batch 0/1
Batch Loss = 4.437384284726901
1829, epoch_train_loss=4.437384284726901
Epoch 1830
Epoch 1830 :: Batch 0/1
Batch Loss = 4.437384283419055
1830, epoch_train_loss=4.437384283419055
Epoch 1831
Epoch 1831 :: Batch 0/1
Batch Loss = 4.437384282108399
1831, epoch_train_loss=4.437384282108399
Epoch 1832
Epoch 1832 :: Batch 0/1
Batch Loss = 4.437384280794916
1832, epoch_train_loss=4.437384280794916
Epoch 1833
Epoch 1833 :: Batch 0/1
Batch Loss = 4.437384279478594
1833, epoch_train_loss=4.437384279478594
Epoch 1834
Epoch 1834 :: Batch 0/1
Batch Loss = 4.437384278159414
1834, epoch_train_loss=4.437384278159414
Epoch 1835
Epoch 1835 :: Batch 0/1
Batch Loss = 4.437384276837364
1835, epoch_train_loss=4.437384276837364
Epoch 1836
Epoch 1836 :: Batch 0/1
Batch Loss = 4.437384275512427
1836, epoch_train_loss=4.437384275512427
Epoch 1837
Epoch 1837 :: Batch 0/1
Batch Loss = 4.437384274184588
1837, epoch_train_loss=4.437384274184588
Epoch 1838
Epoch 1838 :: Batch 0/1
Batch Loss = 4.437384272853829
1838, epoch_train_loss=4.437384272853829
Epoch 1839
Epoch 1839 :: Batch 0/1
Batch Loss = 4.43738427152014
1839, epoch_train_loss=4.43738427152014
Epoch 1840
Epoch 1840 :: Batch 0/1
Batch Loss = 4.437384270183499
1840, epoch_train_loss=4.437384270183499
Epoch 1841
Epoch 1841 :: Batch 0/1
Batch Loss = 4.437384268843894
1841, epoch_train_loss=4.437384268843894
Epoch 1842
Epoch 1842 :: Batch 0/1
Batch Loss = 4.437384267501308
1842, epoch_train_loss=4.437384267501308
Epoch 1843
Epoch 1843 :: Batch 0/1
Batch Loss = 4.437384266155725
1843, epoch_train_loss=4.437384266155725
Epoch 1844
Epoch 1844 :: Batch 0/1
Batch Loss = 4.437384264807129
1844, epoch_train_loss=4.437384264807129
Epoch 1845
Epoch 1845 :: Batch 0/1
Batch Loss = 4.437384263455504
1845, epoch_train_loss=4.437384263455504
Epoch 1846
Epoch 1846 :: Batch 0/1
Batch Loss = 4.437384262100832
1846, epoch_train_loss=4.437384262100832
Epoch 1847
Epoch 1847 :: Batch 0/1
Batch Loss = 4.4373842607431
1847, epoch_train_loss=4.4373842607431
Epoch 1848
Epoch 1848 :: Batch 0/1
Batch Loss = 4.437384259382288
1848, epoch_train_loss=4.437384259382288
Epoch 1849
Epoch 1849 :: Batch 0/1
Batch Loss = 4.4373842580183815
1849, epoch_train_loss=4.4373842580183815
Epoch 1850
Epoch 1850 :: Batch 0/1
Batch Loss = 4.437384256651364
1850, epoch_train_loss=4.437384256651364
Epoch 1851
Epoch 1851 :: Batch 0/1
Batch Loss = 4.437384255281217
1851, epoch_train_loss=4.437384255281217
Epoch 1852
Epoch 1852 :: Batch 0/1
Batch Loss = 4.437384253907926
1852, epoch_train_loss=4.437384253907926
Epoch 1853
Epoch 1853 :: Batch 0/1
Batch Loss = 4.437384252531473
1853, epoch_train_loss=4.437384252531473
Epoch 1854
Epoch 1854 :: Batch 0/1
Batch Loss = 4.437384251151839
1854, epoch_train_loss=4.437384251151839
Epoch 1855
Epoch 1855 :: Batch 0/1
Batch Loss = 4.437384249769009
1855, epoch_train_loss=4.437384249769009
Epoch 1856
Epoch 1856 :: Batch 0/1
Batch Loss = 4.437384248382966
1856, epoch_train_loss=4.437384248382966
Epoch 1857
Epoch 1857 :: Batch 0/1
Batch Loss = 4.437384246993692
1857, epoch_train_loss=4.437384246993692
Epoch 1858
Epoch 1858 :: Batch 0/1
Batch Loss = 4.43738424560117
1858, epoch_train_loss=4.43738424560117
Epoch 1859
Epoch 1859 :: Batch 0/1
Batch Loss = 4.437384244205382
1859, epoch_train_loss=4.437384244205382
Epoch 1860
Epoch 1860 :: Batch 0/1
Batch Loss = 4.43738424280631
1860, epoch_train_loss=4.43738424280631
Epoch 1861
Epoch 1861 :: Batch 0/1
Batch Loss = 4.4373842414039375
1861, epoch_train_loss=4.4373842414039375
Epoch 1862
Epoch 1862 :: Batch 0/1
Batch Loss = 4.437384239998246
1862, epoch_train_loss=4.437384239998246
Epoch 1863
Epoch 1863 :: Batch 0/1
Batch Loss = 4.4373842385892175
1863, epoch_train_loss=4.4373842385892175
Epoch 1864
Epoch 1864 :: Batch 0/1
Batch Loss = 4.4373842371768335
1864, epoch_train_loss=4.4373842371768335
Epoch 1865
Epoch 1865 :: Batch 0/1
Batch Loss = 4.437384235761076
1865, epoch_train_loss=4.437384235761076
Epoch 1866
Epoch 1866 :: Batch 0/1
Batch Loss = 4.437384234341928
1866, epoch_train_loss=4.437384234341928
Epoch 1867
Epoch 1867 :: Batch 0/1
Batch Loss = 4.437384232919369
1867, epoch_train_loss=4.437384232919369
Epoch 1868
Epoch 1868 :: Batch 0/1
Batch Loss = 4.4373842314933825
1868, epoch_train_loss=4.4373842314933825
Epoch 1869
Epoch 1869 :: Batch 0/1
Batch Loss = 4.437384230063949
1869, epoch_train_loss=4.437384230063949
Epoch 1870
Epoch 1870 :: Batch 0/1
Batch Loss = 4.43738422863105
1870, epoch_train_loss=4.43738422863105
Epoch 1871
Epoch 1871 :: Batch 0/1
Batch Loss = 4.437384227194667
1871, epoch_train_loss=4.437384227194667
Epoch 1872
Epoch 1872 :: Batch 0/1
Batch Loss = 4.437384225754781
1872, epoch_train_loss=4.437384225754781
Epoch 1873
Epoch 1873 :: Batch 0/1
Batch Loss = 4.437384224311372
1873, epoch_train_loss=4.437384224311372
Epoch 1874
Epoch 1874 :: Batch 0/1
Batch Loss = 4.437384222864422
1874, epoch_train_loss=4.437384222864422
Epoch 1875
Epoch 1875 :: Batch 0/1
Batch Loss = 4.437384221413911
1875, epoch_train_loss=4.437384221413911
Epoch 1876
Epoch 1876 :: Batch 0/1
Batch Loss = 4.437384219959821
1876, epoch_train_loss=4.437384219959821
Epoch 1877
Epoch 1877 :: Batch 0/1
Batch Loss = 4.437384218502131
1877, epoch_train_loss=4.437384218502131
Epoch 1878
Epoch 1878 :: Batch 0/1
Batch Loss = 4.4373842170408215
1878, epoch_train_loss=4.4373842170408215
Epoch 1879
Epoch 1879 :: Batch 0/1
Batch Loss = 4.437384215575873
1879, epoch_train_loss=4.437384215575873
Epoch 1880
Epoch 1880 :: Batch 0/1
Batch Loss = 4.437384214107267
1880, epoch_train_loss=4.437384214107267
Epoch 1881
Epoch 1881 :: Batch 0/1
Batch Loss = 4.437384212634981
1881, epoch_train_loss=4.437384212634981
Epoch 1882
Epoch 1882 :: Batch 0/1
Batch Loss = 4.437384211158997
1882, epoch_train_loss=4.437384211158997
Epoch 1883
Epoch 1883 :: Batch 0/1
Batch Loss = 4.437384209679296
1883, epoch_train_loss=4.437384209679296
Epoch 1884
Epoch 1884 :: Batch 0/1
Batch Loss = 4.437384208195853
1884, epoch_train_loss=4.437384208195853
Epoch 1885
Epoch 1885 :: Batch 0/1
Batch Loss = 4.437384206708653
1885, epoch_train_loss=4.437384206708653
Epoch 1886
Epoch 1886 :: Batch 0/1
Batch Loss = 4.437384205217671
1886, epoch_train_loss=4.437384205217671
Epoch 1887
Epoch 1887 :: Batch 0/1
Batch Loss = 4.437384203722889
1887, epoch_train_loss=4.437384203722889
Epoch 1888
Epoch 1888 :: Batch 0/1
Batch Loss = 4.437384202224285
1888, epoch_train_loss=4.437384202224285
Epoch 1889
Epoch 1889 :: Batch 0/1
Batch Loss = 4.437384200721839
1889, epoch_train_loss=4.437384200721839
Epoch 1890
Epoch 1890 :: Batch 0/1
Batch Loss = 4.43738419921553
1890, epoch_train_loss=4.43738419921553
Epoch 1891
Epoch 1891 :: Batch 0/1
Batch Loss = 4.4373841977053345
1891, epoch_train_loss=4.4373841977053345
Epoch 1892
Epoch 1892 :: Batch 0/1
Batch Loss = 4.4373841961912355
1892, epoch_train_loss=4.4373841961912355
Epoch 1893
Epoch 1893 :: Batch 0/1
Batch Loss = 4.437384194673205
1893, epoch_train_loss=4.437384194673205
Epoch 1894
Epoch 1894 :: Batch 0/1
Batch Loss = 4.437384193151229
1894, epoch_train_loss=4.437384193151229
Epoch 1895
Epoch 1895 :: Batch 0/1
Batch Loss = 4.43738419162528
1895, epoch_train_loss=4.43738419162528
Epoch 1896
Epoch 1896 :: Batch 0/1
Batch Loss = 4.437384190095339
1896, epoch_train_loss=4.437384190095339
Epoch 1897
Epoch 1897 :: Batch 0/1
Batch Loss = 4.437384188561384
1897, epoch_train_loss=4.437384188561384
Epoch 1898
Epoch 1898 :: Batch 0/1
Batch Loss = 4.437384187023392
1898, epoch_train_loss=4.437384187023392
Epoch 1899
Epoch 1899 :: Batch 0/1
Batch Loss = 4.437384185481339
1899, epoch_train_loss=4.437384185481339
Epoch 1900
Epoch 1900 :: Batch 0/1
Batch Loss = 4.4373841839352055
1900, epoch_train_loss=4.4373841839352055
Epoch 1901
Epoch 1901 :: Batch 0/1
Batch Loss = 4.437384182384969
1901, epoch_train_loss=4.437384182384969
Epoch 1902
Epoch 1902 :: Batch 0/1
Batch Loss = 4.4373841808306045
1902, epoch_train_loss=4.4373841808306045
Epoch 1903
Epoch 1903 :: Batch 0/1
Batch Loss = 4.43738417927209
1903, epoch_train_loss=4.43738417927209
Epoch 1904
Epoch 1904 :: Batch 0/1
Batch Loss = 4.437384177709405
1904, epoch_train_loss=4.437384177709405
Epoch 1905
Epoch 1905 :: Batch 0/1
Batch Loss = 4.437384176142523
1905, epoch_train_loss=4.437384176142523
Epoch 1906
Epoch 1906 :: Batch 0/1
Batch Loss = 4.437384174571423
1906, epoch_train_loss=4.437384174571423
Epoch 1907
Epoch 1907 :: Batch 0/1
Batch Loss = 4.437384172996079
1907, epoch_train_loss=4.437384172996079
Epoch 1908
Epoch 1908 :: Batch 0/1
Batch Loss = 4.437384171416471
1908, epoch_train_loss=4.437384171416471
Epoch 1909
Epoch 1909 :: Batch 0/1
Batch Loss = 4.437384169832572
1909, epoch_train_loss=4.437384169832572
Epoch 1910
Epoch 1910 :: Batch 0/1
Batch Loss = 4.4373841682443596
1910, epoch_train_loss=4.4373841682443596
Epoch 1911
Epoch 1911 :: Batch 0/1
Batch Loss = 4.4373841666518095
1911, epoch_train_loss=4.4373841666518095
Epoch 1912
Epoch 1912 :: Batch 0/1
Batch Loss = 4.437384165054898
1912, epoch_train_loss=4.437384165054898
Epoch 1913
Epoch 1913 :: Batch 0/1
Batch Loss = 4.4373841634536015
1913, epoch_train_loss=4.4373841634536015
Epoch 1914
Epoch 1914 :: Batch 0/1
Batch Loss = 4.437384161847894
1914, epoch_train_loss=4.437384161847894
Epoch 1915
Epoch 1915 :: Batch 0/1
Batch Loss = 4.437384160237753
1915, epoch_train_loss=4.437384160237753
Epoch 1916
Epoch 1916 :: Batch 0/1
Batch Loss = 4.437384158623148
1916, epoch_train_loss=4.437384158623148
Epoch 1917
Epoch 1917 :: Batch 0/1
Batch Loss = 4.437384157004062
1917, epoch_train_loss=4.437384157004062
Epoch 1918
Epoch 1918 :: Batch 0/1
Batch Loss = 4.437384155380464
1918, epoch_train_loss=4.437384155380464
Epoch 1919
Epoch 1919 :: Batch 0/1
Batch Loss = 4.4373841537523315
1919, epoch_train_loss=4.4373841537523315
Epoch 1920
Epoch 1920 :: Batch 0/1
Batch Loss = 4.437384152119638
1920, epoch_train_loss=4.437384152119638
Epoch 1921
Epoch 1921 :: Batch 0/1
Batch Loss = 4.43738415048236
1921, epoch_train_loss=4.43738415048236
Epoch 1922
Epoch 1922 :: Batch 0/1
Batch Loss = 4.437384148840468
1922, epoch_train_loss=4.437384148840468
Epoch 1923
Epoch 1923 :: Batch 0/1
Batch Loss = 4.437384147193939
1923, epoch_train_loss=4.437384147193939
Epoch 1924
Epoch 1924 :: Batch 0/1
Batch Loss = 4.437384145542745
1924, epoch_train_loss=4.437384145542745
Epoch 1925
Epoch 1925 :: Batch 0/1
Batch Loss = 4.437384143886861
1925, epoch_train_loss=4.437384143886861
Epoch 1926
Epoch 1926 :: Batch 0/1
Batch Loss = 4.437384142226261
1926, epoch_train_loss=4.437384142226261
Epoch 1927
Epoch 1927 :: Batch 0/1
Batch Loss = 4.4373841405609165
1927, epoch_train_loss=4.4373841405609165
Epoch 1928
Epoch 1928 :: Batch 0/1
Batch Loss = 4.4373841388908035
1928, epoch_train_loss=4.4373841388908035
Epoch 1929
Epoch 1929 :: Batch 0/1
Batch Loss = 4.437384137215892
1929, epoch_train_loss=4.437384137215892
Epoch 1930
Epoch 1930 :: Batch 0/1
Batch Loss = 4.437384135536155
1930, epoch_train_loss=4.437384135536155
Epoch 1931
Epoch 1931 :: Batch 0/1
Batch Loss = 4.437384133851569
1931, epoch_train_loss=4.437384133851569
Epoch 1932
Epoch 1932 :: Batch 0/1
Batch Loss = 4.437384132162102
1932, epoch_train_loss=4.437384132162102
Epoch 1933
Epoch 1933 :: Batch 0/1
Batch Loss = 4.437384130467728
1933, epoch_train_loss=4.437384130467728
Epoch 1934
Epoch 1934 :: Batch 0/1
Batch Loss = 4.43738412876842
1934, epoch_train_loss=4.43738412876842
Epoch 1935
Epoch 1935 :: Batch 0/1
Batch Loss = 4.437384127064147
1935, epoch_train_loss=4.437384127064147
Epoch 1936
Epoch 1936 :: Batch 0/1
Batch Loss = 4.437384125354887
1936, epoch_train_loss=4.437384125354887
Epoch 1937
Epoch 1937 :: Batch 0/1
Batch Loss = 4.437384123640605
1937, epoch_train_loss=4.437384123640605
Epoch 1938
Epoch 1938 :: Batch 0/1
Batch Loss = 4.4373841219212755
1938, epoch_train_loss=4.4373841219212755
Epoch 1939
Epoch 1939 :: Batch 0/1
Batch Loss = 4.437384120196868
1939, epoch_train_loss=4.437384120196868
Epoch 1940
Epoch 1940 :: Batch 0/1
Batch Loss = 4.437384118467355
1940, epoch_train_loss=4.437384118467355
Epoch 1941
Epoch 1941 :: Batch 0/1
Batch Loss = 4.437384116732706
1941, epoch_train_loss=4.437384116732706
Epoch 1942
Epoch 1942 :: Batch 0/1
Batch Loss = 4.437384114992893
1942, epoch_train_loss=4.437384114992893
Epoch 1943
Epoch 1943 :: Batch 0/1
Batch Loss = 4.4373841132478855
1943, epoch_train_loss=4.4373841132478855
Epoch 1944
Epoch 1944 :: Batch 0/1
Batch Loss = 4.4373841114976535
1944, epoch_train_loss=4.4373841114976535
Epoch 1945
Epoch 1945 :: Batch 0/1
Batch Loss = 4.437384109742169
1945, epoch_train_loss=4.437384109742169
Epoch 1946
Epoch 1946 :: Batch 0/1
Batch Loss = 4.437384107981398
1946, epoch_train_loss=4.437384107981398
Epoch 1947
Epoch 1947 :: Batch 0/1
Batch Loss = 4.437384106215313
1947, epoch_train_loss=4.437384106215313
Epoch 1948
Epoch 1948 :: Batch 0/1
Batch Loss = 4.437384104443883
1948, epoch_train_loss=4.437384104443883
Epoch 1949
Epoch 1949 :: Batch 0/1
Batch Loss = 4.4373841026670755
1949, epoch_train_loss=4.4373841026670755
Epoch 1950
Epoch 1950 :: Batch 0/1
Batch Loss = 4.437384100884862
1950, epoch_train_loss=4.437384100884862
Epoch 1951
Epoch 1951 :: Batch 0/1
Batch Loss = 4.437384099097209
1951, epoch_train_loss=4.437384099097209
Epoch 1952
Epoch 1952 :: Batch 0/1
Batch Loss = 4.437384097304086
1952, epoch_train_loss=4.437384097304086
Epoch 1953
Epoch 1953 :: Batch 0/1
Batch Loss = 4.437384095505462
1953, epoch_train_loss=4.437384095505462
Epoch 1954
Epoch 1954 :: Batch 0/1
Batch Loss = 4.437384093701303
1954, epoch_train_loss=4.437384093701303
Epoch 1955
Epoch 1955 :: Batch 0/1
Batch Loss = 4.437384091891579
1955, epoch_train_loss=4.437384091891579
Epoch 1956
Epoch 1956 :: Batch 0/1
Batch Loss = 4.437384090076255
1956, epoch_train_loss=4.437384090076255
Epoch 1957
Epoch 1957 :: Batch 0/1
Batch Loss = 4.4373840882553015
1957, epoch_train_loss=4.4373840882553015
Epoch 1958
Epoch 1958 :: Batch 0/1
Batch Loss = 4.437384086428684
1958, epoch_train_loss=4.437384086428684
Epoch 1959
Epoch 1959 :: Batch 0/1
Batch Loss = 4.43738408459637
1959, epoch_train_loss=4.43738408459637
Epoch 1960
Epoch 1960 :: Batch 0/1
Batch Loss = 4.437384082758324
1960, epoch_train_loss=4.437384082758324
Epoch 1961
Epoch 1961 :: Batch 0/1
Batch Loss = 4.437384080914516
1961, epoch_train_loss=4.437384080914516
Epoch 1962
Epoch 1962 :: Batch 0/1
Batch Loss = 4.43738407906491
1962, epoch_train_loss=4.43738407906491
Epoch 1963
Epoch 1963 :: Batch 0/1
Batch Loss = 4.437384077209473
1963, epoch_train_loss=4.437384077209473
Epoch 1964
Epoch 1964 :: Batch 0/1
Batch Loss = 4.43738407534817
1964, epoch_train_loss=4.43738407534817
Epoch 1965
Epoch 1965 :: Batch 0/1
Batch Loss = 4.437384073480968
1965, epoch_train_loss=4.437384073480968
Epoch 1966
Epoch 1966 :: Batch 0/1
Batch Loss = 4.4373840716078305
1966, epoch_train_loss=4.4373840716078305
Epoch 1967
Epoch 1967 :: Batch 0/1
Batch Loss = 4.437384069728722
1967, epoch_train_loss=4.437384069728722
Epoch 1968
Epoch 1968 :: Batch 0/1
Batch Loss = 4.43738406784361
1968, epoch_train_loss=4.43738406784361
Epoch 1969
Epoch 1969 :: Batch 0/1
Batch Loss = 4.4373840659524575
1969, epoch_train_loss=4.4373840659524575
Epoch 1970
Epoch 1970 :: Batch 0/1
Batch Loss = 4.437384064055228
1970, epoch_train_loss=4.437384064055228
Epoch 1971
Epoch 1971 :: Batch 0/1
Batch Loss = 4.437384062151885
1971, epoch_train_loss=4.437384062151885
Epoch 1972
Epoch 1972 :: Batch 0/1
Batch Loss = 4.437384060242396
1972, epoch_train_loss=4.437384060242396
Epoch 1973
Epoch 1973 :: Batch 0/1
Batch Loss = 4.437384058326721
1973, epoch_train_loss=4.437384058326721
Epoch 1974
Epoch 1974 :: Batch 0/1
Batch Loss = 4.437384056404825
1974, epoch_train_loss=4.437384056404825
Epoch 1975
Epoch 1975 :: Batch 0/1
Batch Loss = 4.437384054476668
1975, epoch_train_loss=4.437384054476668
Epoch 1976
Epoch 1976 :: Batch 0/1
Batch Loss = 4.4373840525422175
1976, epoch_train_loss=4.4373840525422175
Epoch 1977
Epoch 1977 :: Batch 0/1
Batch Loss = 4.437384050601432
1977, epoch_train_loss=4.437384050601432
Epoch 1978
Epoch 1978 :: Batch 0/1
Batch Loss = 4.437384048654275
1978, epoch_train_loss=4.437384048654275
Epoch 1979
Epoch 1979 :: Batch 0/1
Batch Loss = 4.43738404670071
1979, epoch_train_loss=4.43738404670071
Epoch 1980
Epoch 1980 :: Batch 0/1
Batch Loss = 4.437384044740695
1980, epoch_train_loss=4.437384044740695
Epoch 1981
Epoch 1981 :: Batch 0/1
Batch Loss = 4.437384042774195
1981, epoch_train_loss=4.437384042774195
Epoch 1982
Epoch 1982 :: Batch 0/1
Batch Loss = 4.437384040801168
1982, epoch_train_loss=4.437384040801168
Epoch 1983
Epoch 1983 :: Batch 0/1
Batch Loss = 4.437384038821578
1983, epoch_train_loss=4.437384038821578
Epoch 1984
Epoch 1984 :: Batch 0/1
Batch Loss = 4.4373840368353825
1984, epoch_train_loss=4.4373840368353825
Epoch 1985
Epoch 1985 :: Batch 0/1
Batch Loss = 4.437384034842544
1985, epoch_train_loss=4.437384034842544
Epoch 1986
Epoch 1986 :: Batch 0/1
Batch Loss = 4.43738403284302
1986, epoch_train_loss=4.43738403284302
Epoch 1987
Epoch 1987 :: Batch 0/1
Batch Loss = 4.437384030836772
1987, epoch_train_loss=4.437384030836772
Epoch 1988
Epoch 1988 :: Batch 0/1
Batch Loss = 4.437384028823757
1988, epoch_train_loss=4.437384028823757
Epoch 1989
Epoch 1989 :: Batch 0/1
Batch Loss = 4.4373840268039375
1989, epoch_train_loss=4.4373840268039375
Epoch 1990
Epoch 1990 :: Batch 0/1
Batch Loss = 4.437384024777269
1990, epoch_train_loss=4.437384024777269
Epoch 1991
Epoch 1991 :: Batch 0/1
Batch Loss = 4.4373840227437125
1991, epoch_train_loss=4.4373840227437125
Epoch 1992
Epoch 1992 :: Batch 0/1
Batch Loss = 4.437384020703225
1992, epoch_train_loss=4.437384020703225
Epoch 1993
Epoch 1993 :: Batch 0/1
Batch Loss = 4.437384018655762
1993, epoch_train_loss=4.437384018655762
Epoch 1994
Epoch 1994 :: Batch 0/1
Batch Loss = 4.437384016601285
1994, epoch_train_loss=4.437384016601285
Epoch 1995
Epoch 1995 :: Batch 0/1
Batch Loss = 4.437384014539748
1995, epoch_train_loss=4.437384014539748
Epoch 1996
Epoch 1996 :: Batch 0/1
Batch Loss = 4.437384012471109
1996, epoch_train_loss=4.437384012471109
Epoch 1997
Epoch 1997 :: Batch 0/1
Batch Loss = 4.437384010395324
1997, epoch_train_loss=4.437384010395324
Epoch 1998
Epoch 1998 :: Batch 0/1
Batch Loss = 4.4373840083123515
1998, epoch_train_loss=4.4373840083123515
Epoch 1999
Epoch 1999 :: Batch 0/1
Batch Loss = 4.4373840062221435
1999, epoch_train_loss=4.4373840062221435
Epoch 2000
Epoch 2000 :: Batch 0/1
Batch Loss = 4.437384004124657
2000, epoch_train_loss=4.437384004124657
Epoch 2001
Epoch 2001 :: Batch 0/1
Batch Loss = 4.437384002019848
2001, epoch_train_loss=4.437384002019848
Epoch 2002
Epoch 2002 :: Batch 0/1
Batch Loss = 4.437383999907672
2002, epoch_train_loss=4.437383999907672
Epoch 2003
Epoch 2003 :: Batch 0/1
Batch Loss = 4.43738399778808
2003, epoch_train_loss=4.43738399778808
Epoch 2004
Epoch 2004 :: Batch 0/1
Batch Loss = 4.437383995661029
2004, epoch_train_loss=4.437383995661029
Epoch 2005
Epoch 2005 :: Batch 0/1
Batch Loss = 4.437383993526473
2005, epoch_train_loss=4.437383993526473
Epoch 2006
Epoch 2006 :: Batch 0/1
Batch Loss = 4.4373839913843645
2006, epoch_train_loss=4.4373839913843645
Epoch 2007
Epoch 2007 :: Batch 0/1
Batch Loss = 4.437383989234656
2007, epoch_train_loss=4.437383989234656
Epoch 2008
Epoch 2008 :: Batch 0/1
Batch Loss = 4.4373839870773
2008, epoch_train_loss=4.4373839870773
Epoch 2009
Epoch 2009 :: Batch 0/1
Batch Loss = 4.437383984912251
2009, epoch_train_loss=4.437383984912251
Epoch 2010
Epoch 2010 :: Batch 0/1
Batch Loss = 4.437383982739458
2010, epoch_train_loss=4.437383982739458
Epoch 2011
Epoch 2011 :: Batch 0/1
Batch Loss = 4.4373839805588755
2011, epoch_train_loss=4.4373839805588755
Epoch 2012
Epoch 2012 :: Batch 0/1
Batch Loss = 4.437383978370452
2012, epoch_train_loss=4.437383978370452
Epoch 2013
Epoch 2013 :: Batch 0/1
Batch Loss = 4.43738397617414
2013, epoch_train_loss=4.43738397617414
Epoch 2014
Epoch 2014 :: Batch 0/1
Batch Loss = 4.43738397396989
2014, epoch_train_loss=4.43738397396989
Epoch 2015
Epoch 2015 :: Batch 0/1
Batch Loss = 4.437383971757652
2015, epoch_train_loss=4.437383971757652
Epoch 2016
Epoch 2016 :: Batch 0/1
Batch Loss = 4.437383969537374
2016, epoch_train_loss=4.437383969537374
Epoch 2017
Epoch 2017 :: Batch 0/1
Batch Loss = 4.437383967309006
2017, epoch_train_loss=4.437383967309006
Epoch 2018
Epoch 2018 :: Batch 0/1
Batch Loss = 4.437383965072499
2018, epoch_train_loss=4.437383965072499
Epoch 2019
Epoch 2019 :: Batch 0/1
Batch Loss = 4.437383962827799
2019, epoch_train_loss=4.437383962827799
Epoch 2020
Epoch 2020 :: Batch 0/1
Batch Loss = 4.437383960574855
2020, epoch_train_loss=4.437383960574855
Epoch 2021
Epoch 2021 :: Batch 0/1
Batch Loss = 4.4373839583136165
2021, epoch_train_loss=4.4373839583136165
Epoch 2022
Epoch 2022 :: Batch 0/1
Batch Loss = 4.437383956044026
2022, epoch_train_loss=4.437383956044026
Epoch 2023
Epoch 2023 :: Batch 0/1
Batch Loss = 4.437383953766035
2023, epoch_train_loss=4.437383953766035
Epoch 2024
Epoch 2024 :: Batch 0/1
Batch Loss = 4.437383951479587
2024, epoch_train_loss=4.437383951479587
Epoch 2025
Epoch 2025 :: Batch 0/1
Batch Loss = 4.437383949184629
2025, epoch_train_loss=4.437383949184629
Epoch 2026
Epoch 2026 :: Batch 0/1
Batch Loss = 4.437383946881107
2026, epoch_train_loss=4.437383946881107
Epoch 2027
Epoch 2027 :: Batch 0/1
Batch Loss = 4.437383944568965
2027, epoch_train_loss=4.437383944568965
Epoch 2028
Epoch 2028 :: Batch 0/1
Batch Loss = 4.437383942248149
2028, epoch_train_loss=4.437383942248149
Epoch 2029
Epoch 2029 :: Batch 0/1
Batch Loss = 4.437383939918602
2029, epoch_train_loss=4.437383939918602
Epoch 2030
Epoch 2030 :: Batch 0/1
Batch Loss = 4.437383937580268
2030, epoch_train_loss=4.437383937580268
Epoch 2031
Epoch 2031 :: Batch 0/1
Batch Loss = 4.4373839352330915
2031, epoch_train_loss=4.4373839352330915
Epoch 2032
Epoch 2032 :: Batch 0/1
Batch Loss = 4.437383932877013
2032, epoch_train_loss=4.437383932877013
Epoch 2033
Epoch 2033 :: Batch 0/1
Batch Loss = 4.437383930511979
2033, epoch_train_loss=4.437383930511979
Epoch 2034
Epoch 2034 :: Batch 0/1
Batch Loss = 4.437383928137926
2034, epoch_train_loss=4.437383928137926
Epoch 2035
Epoch 2035 :: Batch 0/1
Batch Loss = 4.4373839257548
2035, epoch_train_loss=4.4373839257548
Epoch 2036
Epoch 2036 :: Batch 0/1
Batch Loss = 4.437383923362539
2036, epoch_train_loss=4.437383923362539
Epoch 2037
Epoch 2037 :: Batch 0/1
Batch Loss = 4.437383920961085
2037, epoch_train_loss=4.437383920961085
Epoch 2038
Epoch 2038 :: Batch 0/1
Batch Loss = 4.437383918550378
2038, epoch_train_loss=4.437383918550378
Epoch 2039
Epoch 2039 :: Batch 0/1
Batch Loss = 4.437383916130358
2039, epoch_train_loss=4.437383916130358
Epoch 2040
Epoch 2040 :: Batch 0/1
Batch Loss = 4.437383913700962
2040, epoch_train_loss=4.437383913700962
Epoch 2041
Epoch 2041 :: Batch 0/1
Batch Loss = 4.43738391126213
2041, epoch_train_loss=4.43738391126213
Epoch 2042
Epoch 2042 :: Batch 0/1
Batch Loss = 4.437383908813801
2042, epoch_train_loss=4.437383908813801
Epoch 2043
Epoch 2043 :: Batch 0/1
Batch Loss = 4.437383906355909
2043, epoch_train_loss=4.437383906355909
Epoch 2044
Epoch 2044 :: Batch 0/1
Batch Loss = 4.437383903888394
2044, epoch_train_loss=4.437383903888394
Epoch 2045
Epoch 2045 :: Batch 0/1
Batch Loss = 4.437383901411192
2045, epoch_train_loss=4.437383901411192
Epoch 2046
Epoch 2046 :: Batch 0/1
Batch Loss = 4.437383898924239
2046, epoch_train_loss=4.437383898924239
Epoch 2047
Epoch 2047 :: Batch 0/1
Batch Loss = 4.43738389642747
2047, epoch_train_loss=4.43738389642747
Epoch 2048
Epoch 2048 :: Batch 0/1
Batch Loss = 4.43738389392082
2048, epoch_train_loss=4.43738389392082
Epoch 2049
Epoch 2049 :: Batch 0/1
Batch Loss = 4.437383891404223
2049, epoch_train_loss=4.437383891404223
Epoch 2050
Epoch 2050 :: Batch 0/1
Batch Loss = 4.437383888877613
2050, epoch_train_loss=4.437383888877613
Epoch 2051
Epoch 2051 :: Batch 0/1
Batch Loss = 4.437383886340923
2051, epoch_train_loss=4.437383886340923
Epoch 2052
Epoch 2052 :: Batch 0/1
Batch Loss = 4.4373838837940855
2052, epoch_train_loss=4.4373838837940855
Epoch 2053
Epoch 2053 :: Batch 0/1
Batch Loss = 4.437383881237033
2053, epoch_train_loss=4.437383881237033
Epoch 2054
Epoch 2054 :: Batch 0/1
Batch Loss = 4.4373838786696975
2054, epoch_train_loss=4.4373838786696975
Epoch 2055
Epoch 2055 :: Batch 0/1
Batch Loss = 4.437383876092009
2055, epoch_train_loss=4.437383876092009
Epoch 2056
Epoch 2056 :: Batch 0/1
Batch Loss = 4.437383873503898
2056, epoch_train_loss=4.437383873503898
Epoch 2057
Epoch 2057 :: Batch 0/1
Batch Loss = 4.437383870905294
2057, epoch_train_loss=4.437383870905294
Epoch 2058
Epoch 2058 :: Batch 0/1
Batch Loss = 4.437383868296127
2058, epoch_train_loss=4.437383868296127
Epoch 2059
Epoch 2059 :: Batch 0/1
Batch Loss = 4.437383865676326
2059, epoch_train_loss=4.437383865676326
Epoch 2060
Epoch 2060 :: Batch 0/1
Batch Loss = 4.437383863045816
2060, epoch_train_loss=4.437383863045816
Epoch 2061
Epoch 2061 :: Batch 0/1
Batch Loss = 4.437383860404527
2061, epoch_train_loss=4.437383860404527
Epoch 2062
Epoch 2062 :: Batch 0/1
Batch Loss = 4.437383857752385
2062, epoch_train_loss=4.437383857752385
Epoch 2063
Epoch 2063 :: Batch 0/1
Batch Loss = 4.437383855089316
2063, epoch_train_loss=4.437383855089316
Epoch 2064
Epoch 2064 :: Batch 0/1
Batch Loss = 4.437383852415247
2064, epoch_train_loss=4.437383852415247
Epoch 2065
Epoch 2065 :: Batch 0/1
Batch Loss = 4.437383849730098
2065, epoch_train_loss=4.437383849730098
Epoch 2066
Epoch 2066 :: Batch 0/1
Batch Loss = 4.437383847033798
2066, epoch_train_loss=4.437383847033798
Epoch 2067
Epoch 2067 :: Batch 0/1
Batch Loss = 4.437383844326268
2067, epoch_train_loss=4.437383844326268
Epoch 2068
Epoch 2068 :: Batch 0/1
Batch Loss = 4.437383841607431
2068, epoch_train_loss=4.437383841607431
Epoch 2069
Epoch 2069 :: Batch 0/1
Batch Loss = 4.437383838877212
2069, epoch_train_loss=4.437383838877212
Epoch 2070
Epoch 2070 :: Batch 0/1
Batch Loss = 4.437383836135527
2070, epoch_train_loss=4.437383836135527
Epoch 2071
Epoch 2071 :: Batch 0/1
Batch Loss = 4.4373838333823015
2071, epoch_train_loss=4.4373838333823015
Epoch 2072
Epoch 2072 :: Batch 0/1
Batch Loss = 4.437383830617454
2072, epoch_train_loss=4.437383830617454
Epoch 2073
Epoch 2073 :: Batch 0/1
Batch Loss = 4.437383827840902
2073, epoch_train_loss=4.437383827840902
Epoch 2074
Epoch 2074 :: Batch 0/1
Batch Loss = 4.437383825052567
2074, epoch_train_loss=4.437383825052567
Epoch 2075
Epoch 2075 :: Batch 0/1
Batch Loss = 4.4373838222523645
2075, epoch_train_loss=4.4373838222523645
Epoch 2076
Epoch 2076 :: Batch 0/1
Batch Loss = 4.437383819440213
2076, epoch_train_loss=4.437383819440213
Epoch 2077
Epoch 2077 :: Batch 0/1
Batch Loss = 4.4373838166160295
2077, epoch_train_loss=4.4373838166160295
Epoch 2078
Epoch 2078 :: Batch 0/1
Batch Loss = 4.437383813779728
2078, epoch_train_loss=4.437383813779728
Epoch 2079
Epoch 2079 :: Batch 0/1
Batch Loss = 4.437383810931225
2079, epoch_train_loss=4.437383810931225
Epoch 2080
Epoch 2080 :: Batch 0/1
Batch Loss = 4.437383808070433
2080, epoch_train_loss=4.437383808070433
Epoch 2081
Epoch 2081 :: Batch 0/1
Batch Loss = 4.437383805197267
2081, epoch_train_loss=4.437383805197267
Epoch 2082
Epoch 2082 :: Batch 0/1
Batch Loss = 4.437383802311638
2082, epoch_train_loss=4.437383802311638
Epoch 2083
Epoch 2083 :: Batch 0/1
Batch Loss = 4.4373837994134595
2083, epoch_train_loss=4.4373837994134595
Epoch 2084
Epoch 2084 :: Batch 0/1
Batch Loss = 4.437383796502642
2084, epoch_train_loss=4.437383796502642
Epoch 2085
Epoch 2085 :: Batch 0/1
Batch Loss = 4.437383793579095
2085, epoch_train_loss=4.437383793579095
Epoch 2086
Epoch 2086 :: Batch 0/1
Batch Loss = 4.43738379064273
2086, epoch_train_loss=4.43738379064273
Epoch 2087
Epoch 2087 :: Batch 0/1
Batch Loss = 4.437383787693453
2087, epoch_train_loss=4.437383787693453
Epoch 2088
Epoch 2088 :: Batch 0/1
Batch Loss = 4.437383784731173
2088, epoch_train_loss=4.437383784731173
Epoch 2089
Epoch 2089 :: Batch 0/1
Batch Loss = 4.4373837817557975
2089, epoch_train_loss=4.4373837817557975
Epoch 2090
Epoch 2090 :: Batch 0/1
Batch Loss = 4.437383778767231
2090, epoch_train_loss=4.437383778767231
Epoch 2091
Epoch 2091 :: Batch 0/1
Batch Loss = 4.4373837757653805
2091, epoch_train_loss=4.4373837757653805
Epoch 2092
Epoch 2092 :: Batch 0/1
Batch Loss = 4.437383772750149
2092, epoch_train_loss=4.437383772750149
Epoch 2093
Epoch 2093 :: Batch 0/1
Batch Loss = 4.437383769721439
2093, epoch_train_loss=4.437383769721439
Epoch 2094
Epoch 2094 :: Batch 0/1
Batch Loss = 4.437383766679155
2094, epoch_train_loss=4.437383766679155
Epoch 2095
Epoch 2095 :: Batch 0/1
Batch Loss = 4.4373837636232
2095, epoch_train_loss=4.4373837636232
Epoch 2096
Epoch 2096 :: Batch 0/1
Batch Loss = 4.437383760553471
2096, epoch_train_loss=4.437383760553471
Epoch 2097
Epoch 2097 :: Batch 0/1
Batch Loss = 4.4373837574698705
2097, epoch_train_loss=4.4373837574698705
Epoch 2098
Epoch 2098 :: Batch 0/1
Batch Loss = 4.437383754372298
2098, epoch_train_loss=4.437383754372298
Epoch 2099
Epoch 2099 :: Batch 0/1
Batch Loss = 4.437383751260648
2099, epoch_train_loss=4.437383751260648
Epoch 2100
Epoch 2100 :: Batch 0/1
Batch Loss = 4.43738374813482
2100, epoch_train_loss=4.43738374813482
Epoch 2101
Epoch 2101 :: Batch 0/1
Batch Loss = 4.43738374499471
2101, epoch_train_loss=4.43738374499471
Epoch 2102
Epoch 2102 :: Batch 0/1
Batch Loss = 4.437383741840213
2102, epoch_train_loss=4.437383741840213
Epoch 2103
Epoch 2103 :: Batch 0/1
Batch Loss = 4.4373837386712225
2103, epoch_train_loss=4.4373837386712225
Epoch 2104
Epoch 2104 :: Batch 0/1
Batch Loss = 4.437383735487631
2104, epoch_train_loss=4.437383735487631
Epoch 2105
Epoch 2105 :: Batch 0/1
Batch Loss = 4.437383732289333
2105, epoch_train_loss=4.437383732289333
Epoch 2106
Epoch 2106 :: Batch 0/1
Batch Loss = 4.437383729076217
2106, epoch_train_loss=4.437383729076217
Epoch 2107
Epoch 2107 :: Batch 0/1
Batch Loss = 4.437383725848174
2107, epoch_train_loss=4.437383725848174
Epoch 2108
Epoch 2108 :: Batch 0/1
Batch Loss = 4.437383722605094
2108, epoch_train_loss=4.437383722605094
Epoch 2109
Epoch 2109 :: Batch 0/1
Batch Loss = 4.437383719346862
2109, epoch_train_loss=4.437383719346862
Epoch 2110
Epoch 2110 :: Batch 0/1
Batch Loss = 4.4373837160733665
2110, epoch_train_loss=4.4373837160733665
Epoch 2111
Epoch 2111 :: Batch 0/1
Batch Loss = 4.437383712784493
2111, epoch_train_loss=4.437383712784493
Epoch 2112
Epoch 2112 :: Batch 0/1
Batch Loss = 4.4373837094801285
2112, epoch_train_loss=4.4373837094801285
Epoch 2113
Epoch 2113 :: Batch 0/1
Batch Loss = 4.437383706160152
2113, epoch_train_loss=4.437383706160152
Epoch 2114
Epoch 2114 :: Batch 0/1
Batch Loss = 4.437383702824449
2114, epoch_train_loss=4.437383702824449
Epoch 2115
Epoch 2115 :: Batch 0/1
Batch Loss = 4.4373836994729015
2115, epoch_train_loss=4.4373836994729015
Epoch 2116
Epoch 2116 :: Batch 0/1
Batch Loss = 4.437383696105387
2116, epoch_train_loss=4.437383696105387
Epoch 2117
Epoch 2117 :: Batch 0/1
Batch Loss = 4.437383692721786
2117, epoch_train_loss=4.437383692721786
Epoch 2118
Epoch 2118 :: Batch 0/1
Batch Loss = 4.437383689321975
2118, epoch_train_loss=4.437383689321975
Epoch 2119
Epoch 2119 :: Batch 0/1
Batch Loss = 4.437383685905832
2119, epoch_train_loss=4.437383685905832
Epoch 2120
Epoch 2120 :: Batch 0/1
Batch Loss = 4.437383682473231
2120, epoch_train_loss=4.437383682473231
Epoch 2121
Epoch 2121 :: Batch 0/1
Batch Loss = 4.437383679024049
2121, epoch_train_loss=4.437383679024049
Epoch 2122
Epoch 2122 :: Batch 0/1
Batch Loss = 4.437383675558154
2122, epoch_train_loss=4.437383675558154
Epoch 2123
Epoch 2123 :: Batch 0/1
Batch Loss = 4.437383672075421
2123, epoch_train_loss=4.437383672075421
Epoch 2124
Epoch 2124 :: Batch 0/1
Batch Loss = 4.43738366857572
2124, epoch_train_loss=4.43738366857572
Epoch 2125
Epoch 2125 :: Batch 0/1
Batch Loss = 4.43738366505892
2125, epoch_train_loss=4.43738366505892
Epoch 2126
Epoch 2126 :: Batch 0/1
Batch Loss = 4.437383661524888
2126, epoch_train_loss=4.437383661524888
Epoch 2127
Epoch 2127 :: Batch 0/1
Batch Loss = 4.43738365797349
2127, epoch_train_loss=4.43738365797349
Epoch 2128
Epoch 2128 :: Batch 0/1
Batch Loss = 4.437383654404594
2128, epoch_train_loss=4.437383654404594
Epoch 2129
Epoch 2129 :: Batch 0/1
Batch Loss = 4.437383650818061
2129, epoch_train_loss=4.437383650818061
Epoch 2130
Epoch 2130 :: Batch 0/1
Batch Loss = 4.437383647213755
2130, epoch_train_loss=4.437383647213755
Epoch 2131
Epoch 2131 :: Batch 0/1
Batch Loss = 4.437383643591535
2131, epoch_train_loss=4.437383643591535
Epoch 2132
Epoch 2132 :: Batch 0/1
Batch Loss = 4.437383639951261
2132, epoch_train_loss=4.437383639951261
Epoch 2133
Epoch 2133 :: Batch 0/1
Batch Loss = 4.437383636292795
2133, epoch_train_loss=4.437383636292795
Epoch 2134
Epoch 2134 :: Batch 0/1
Batch Loss = 4.437383632615988
2134, epoch_train_loss=4.437383632615988
Epoch 2135
Epoch 2135 :: Batch 0/1
Batch Loss = 4.437383628920698
2135, epoch_train_loss=4.437383628920698
Epoch 2136
Epoch 2136 :: Batch 0/1
Batch Loss = 4.437383625206781
2136, epoch_train_loss=4.437383625206781
Epoch 2137
Epoch 2137 :: Batch 0/1
Batch Loss = 4.437383621474085
2137, epoch_train_loss=4.437383621474085
Epoch 2138
Epoch 2138 :: Batch 0/1
Batch Loss = 4.437383617722464
2138, epoch_train_loss=4.437383617722464
Epoch 2139
Epoch 2139 :: Batch 0/1
Batch Loss = 4.437383613951766
2139, epoch_train_loss=4.437383613951766
Epoch 2140
Epoch 2140 :: Batch 0/1
Batch Loss = 4.437383610161839
2140, epoch_train_loss=4.437383610161839
Epoch 2141
Epoch 2141 :: Batch 0/1
Batch Loss = 4.437383606352528
2141, epoch_train_loss=4.437383606352528
Epoch 2142
Epoch 2142 :: Batch 0/1
Batch Loss = 4.4373836025236795
2142, epoch_train_loss=4.4373836025236795
Epoch 2143
Epoch 2143 :: Batch 0/1
Batch Loss = 4.437383598675135
2143, epoch_train_loss=4.437383598675135
Epoch 2144
Epoch 2144 :: Batch 0/1
Batch Loss = 4.437383594806736
2144, epoch_train_loss=4.437383594806736
Epoch 2145
Epoch 2145 :: Batch 0/1
Batch Loss = 4.437383590918324
2145, epoch_train_loss=4.437383590918324
Epoch 2146
Epoch 2146 :: Batch 0/1
Batch Loss = 4.437383587009735
2146, epoch_train_loss=4.437383587009735
Epoch 2147
Epoch 2147 :: Batch 0/1
Batch Loss = 4.4373835830808055
2147, epoch_train_loss=4.4373835830808055
Epoch 2148
Epoch 2148 :: Batch 0/1
Batch Loss = 4.43738357913137
2148, epoch_train_loss=4.43738357913137
Epoch 2149
Epoch 2149 :: Batch 0/1
Batch Loss = 4.437383575161261
2149, epoch_train_loss=4.437383575161261
Epoch 2150
Epoch 2150 :: Batch 0/1
Batch Loss = 4.437383571170312
2150, epoch_train_loss=4.437383571170312
Epoch 2151
Epoch 2151 :: Batch 0/1
Batch Loss = 4.437383567158349
2151, epoch_train_loss=4.437383567158349
Epoch 2152
Epoch 2152 :: Batch 0/1
Batch Loss = 4.437383563125203
2152, epoch_train_loss=4.437383563125203
Epoch 2153
Epoch 2153 :: Batch 0/1
Batch Loss = 4.437383559070695
2153, epoch_train_loss=4.437383559070695
Epoch 2154
Epoch 2154 :: Batch 0/1
Batch Loss = 4.4373835549946525
2154, epoch_train_loss=4.4373835549946525
Epoch 2155
Epoch 2155 :: Batch 0/1
Batch Loss = 4.437383550896897
2155, epoch_train_loss=4.437383550896897
Epoch 2156
Epoch 2156 :: Batch 0/1
Batch Loss = 4.437383546777245
2156, epoch_train_loss=4.437383546777245
Epoch 2157
Epoch 2157 :: Batch 0/1
Batch Loss = 4.437383542635519
2157, epoch_train_loss=4.437383542635519
Epoch 2158
Epoch 2158 :: Batch 0/1
Batch Loss = 4.437383538471535
2158, epoch_train_loss=4.437383538471535
Epoch 2159
Epoch 2159 :: Batch 0/1
Batch Loss = 4.437383534285105
2159, epoch_train_loss=4.437383534285105
Epoch 2160
Epoch 2160 :: Batch 0/1
Batch Loss = 4.43738353007604
2160, epoch_train_loss=4.43738353007604
Epoch 2161
Epoch 2161 :: Batch 0/1
Batch Loss = 4.437383525844151
2161, epoch_train_loss=4.437383525844151
Epoch 2162
Epoch 2162 :: Batch 0/1
Batch Loss = 4.437383521589248
2162, epoch_train_loss=4.437383521589248
Epoch 2163
Epoch 2163 :: Batch 0/1
Batch Loss = 4.437383517311135
2163, epoch_train_loss=4.437383517311135
Epoch 2164
Epoch 2164 :: Batch 0/1
Batch Loss = 4.437383513009617
2164, epoch_train_loss=4.437383513009617
Epoch 2165
Epoch 2165 :: Batch 0/1
Batch Loss = 4.437383508684493
2165, epoch_train_loss=4.437383508684493
Epoch 2166
Epoch 2166 :: Batch 0/1
Batch Loss = 4.437383504335566
2166, epoch_train_loss=4.437383504335566
Epoch 2167
Epoch 2167 :: Batch 0/1
Batch Loss = 4.437383499962631
2167, epoch_train_loss=4.437383499962631
Epoch 2168
Epoch 2168 :: Batch 0/1
Batch Loss = 4.437383495565482
2168, epoch_train_loss=4.437383495565482
Epoch 2169
Epoch 2169 :: Batch 0/1
Batch Loss = 4.4373834911439145
2169, epoch_train_loss=4.4373834911439145
Epoch 2170
Epoch 2170 :: Batch 0/1
Batch Loss = 4.437383486697716
2170, epoch_train_loss=4.437383486697716
Epoch 2171
Epoch 2171 :: Batch 0/1
Batch Loss = 4.437383482226679
2171, epoch_train_loss=4.437383482226679
Epoch 2172
Epoch 2172 :: Batch 0/1
Batch Loss = 4.437383477730583
2172, epoch_train_loss=4.437383477730583
Epoch 2173
Epoch 2173 :: Batch 0/1
Batch Loss = 4.4373834732092154
2173, epoch_train_loss=4.4373834732092154
Epoch 2174
Epoch 2174 :: Batch 0/1
Batch Loss = 4.437383468662357
2174, epoch_train_loss=4.437383468662357
Epoch 2175
Epoch 2175 :: Batch 0/1
Batch Loss = 4.437383464089785
2175, epoch_train_loss=4.437383464089785
Epoch 2176
Epoch 2176 :: Batch 0/1
Batch Loss = 4.437383459491276
2176, epoch_train_loss=4.437383459491276
Epoch 2177
Epoch 2177 :: Batch 0/1
Batch Loss = 4.437383454866601
2177, epoch_train_loss=4.437383454866601
Epoch 2178
Epoch 2178 :: Batch 0/1
Batch Loss = 4.437383450215534
2178, epoch_train_loss=4.437383450215534
Epoch 2179
Epoch 2179 :: Batch 0/1
Batch Loss = 4.437383445537843
2179, epoch_train_loss=4.437383445537843
Epoch 2180
Epoch 2180 :: Batch 0/1
Batch Loss = 4.4373834408332895
2180, epoch_train_loss=4.4373834408332895
Epoch 2181
Epoch 2181 :: Batch 0/1
Batch Loss = 4.4373834361016415
2181, epoch_train_loss=4.4373834361016415
Epoch 2182
Epoch 2182 :: Batch 0/1
Batch Loss = 4.437383431342656
2182, epoch_train_loss=4.437383431342656
Epoch 2183
Epoch 2183 :: Batch 0/1
Batch Loss = 4.4373834265560905
2183, epoch_train_loss=4.4373834265560905
Epoch 2184
Epoch 2184 :: Batch 0/1
Batch Loss = 4.437383421741702
2184, epoch_train_loss=4.437383421741702
Epoch 2185
Epoch 2185 :: Batch 0/1
Batch Loss = 4.437383416899237
2185, epoch_train_loss=4.437383416899237
Epoch 2186
Epoch 2186 :: Batch 0/1
Batch Loss = 4.437383412028451
2186, epoch_train_loss=4.437383412028451
Epoch 2187
Epoch 2187 :: Batch 0/1
Batch Loss = 4.437383407129086
2187, epoch_train_loss=4.437383407129086
Epoch 2188
Epoch 2188 :: Batch 0/1
Batch Loss = 4.437383402200884
2188, epoch_train_loss=4.437383402200884
Epoch 2189
Epoch 2189 :: Batch 0/1
Batch Loss = 4.437383397243586
2189, epoch_train_loss=4.437383397243586
Epoch 2190
Epoch 2190 :: Batch 0/1
Batch Loss = 4.437383392256931
2190, epoch_train_loss=4.437383392256931
Epoch 2191
Epoch 2191 :: Batch 0/1
Batch Loss = 4.437383387240649
2191, epoch_train_loss=4.437383387240649
Epoch 2192
Epoch 2192 :: Batch 0/1
Batch Loss = 4.437383382194474
2192, epoch_train_loss=4.437383382194474
Epoch 2193
Epoch 2193 :: Batch 0/1
Batch Loss = 4.437383377118131
2193, epoch_train_loss=4.437383377118131
Epoch 2194
Epoch 2194 :: Batch 0/1
Batch Loss = 4.437383372011347
2194, epoch_train_loss=4.437383372011347
Epoch 2195
Epoch 2195 :: Batch 0/1
Batch Loss = 4.4373833668738385
2195, epoch_train_loss=4.4373833668738385
Epoch 2196
Epoch 2196 :: Batch 0/1
Batch Loss = 4.4373833617053275
2196, epoch_train_loss=4.4373833617053275
Epoch 2197
Epoch 2197 :: Batch 0/1
Batch Loss = 4.437383356505524
2197, epoch_train_loss=4.437383356505524
Epoch 2198
Epoch 2198 :: Batch 0/1
Batch Loss = 4.437383351274143
2198, epoch_train_loss=4.437383351274143
Epoch 2199
Epoch 2199 :: Batch 0/1
Batch Loss = 4.437383346010889
2199, epoch_train_loss=4.437383346010889
Epoch 2200
Epoch 2200 :: Batch 0/1
Batch Loss = 4.437383340715468
2200, epoch_train_loss=4.437383340715468
Epoch 2201
Epoch 2201 :: Batch 0/1
Batch Loss = 4.437383335387577
2201, epoch_train_loss=4.437383335387577
Epoch 2202
Epoch 2202 :: Batch 0/1
Batch Loss = 4.437383330026916
2202, epoch_train_loss=4.437383330026916
Epoch 2203
Epoch 2203 :: Batch 0/1
Batch Loss = 4.437383324633175
2203, epoch_train_loss=4.437383324633175
Epoch 2204
Epoch 2204 :: Batch 0/1
Batch Loss = 4.4373833192060435
2204, epoch_train_loss=4.4373833192060435
Epoch 2205
Epoch 2205 :: Batch 0/1
Batch Loss = 4.437383313745209
2205, epoch_train_loss=4.437383313745209
Epoch 2206
Epoch 2206 :: Batch 0/1
Batch Loss = 4.43738330825035
2206, epoch_train_loss=4.43738330825035
Epoch 2207
Epoch 2207 :: Batch 0/1
Batch Loss = 4.437383302721144
2207, epoch_train_loss=4.437383302721144
Epoch 2208
Epoch 2208 :: Batch 0/1
Batch Loss = 4.437383297157266
2208, epoch_train_loss=4.437383297157266
Epoch 2209
Epoch 2209 :: Batch 0/1
Batch Loss = 4.437383291558388
2209, epoch_train_loss=4.437383291558388
Epoch 2210
Epoch 2210 :: Batch 0/1
Batch Loss = 4.43738328592417
2210, epoch_train_loss=4.43738328592417
Epoch 2211
Epoch 2211 :: Batch 0/1
Batch Loss = 4.437383280254275
2211, epoch_train_loss=4.437383280254275
Epoch 2212
Epoch 2212 :: Batch 0/1
Batch Loss = 4.437383274548362
2212, epoch_train_loss=4.437383274548362
Epoch 2213
Epoch 2213 :: Batch 0/1
Batch Loss = 4.437383268806081
2213, epoch_train_loss=4.437383268806081
Epoch 2214
Epoch 2214 :: Batch 0/1
Batch Loss = 4.4373832630270815
2214, epoch_train_loss=4.4373832630270815
Epoch 2215
Epoch 2215 :: Batch 0/1
Batch Loss = 4.437383257211006
2215, epoch_train_loss=4.437383257211006
Epoch 2216
Epoch 2216 :: Batch 0/1
Batch Loss = 4.437383251357496
2216, epoch_train_loss=4.437383251357496
Epoch 2217
Epoch 2217 :: Batch 0/1
Batch Loss = 4.437383245466185
2217, epoch_train_loss=4.437383245466185
Epoch 2218
Epoch 2218 :: Batch 0/1
Batch Loss = 4.437383239536703
2218, epoch_train_loss=4.437383239536703
Epoch 2219
Epoch 2219 :: Batch 0/1
Batch Loss = 4.437383233568673
2219, epoch_train_loss=4.437383233568673
Epoch 2220
Epoch 2220 :: Batch 0/1
Batch Loss = 4.43738322756172
2220, epoch_train_loss=4.43738322756172
Epoch 2221
Epoch 2221 :: Batch 0/1
Batch Loss = 4.437383221515459
2221, epoch_train_loss=4.437383221515459
Epoch 2222
Epoch 2222 :: Batch 0/1
Batch Loss = 4.437383215429496
2222, epoch_train_loss=4.437383215429496
Epoch 2223
Epoch 2223 :: Batch 0/1
Batch Loss = 4.437383209303439
2223, epoch_train_loss=4.437383209303439
Epoch 2224
Epoch 2224 :: Batch 0/1
Batch Loss = 4.437383203136892
2224, epoch_train_loss=4.437383203136892
Epoch 2225
Epoch 2225 :: Batch 0/1
Batch Loss = 4.437383196929446
2225, epoch_train_loss=4.437383196929446
Epoch 2226
Epoch 2226 :: Batch 0/1
Batch Loss = 4.437383190680691
2226, epoch_train_loss=4.437383190680691
Epoch 2227
Epoch 2227 :: Batch 0/1
Batch Loss = 4.437383184390216
2227, epoch_train_loss=4.437383184390216
Epoch 2228
Epoch 2228 :: Batch 0/1
Batch Loss = 4.437383178057596
2228, epoch_train_loss=4.437383178057596
Epoch 2229
Epoch 2229 :: Batch 0/1
Batch Loss = 4.437383171682406
2229, epoch_train_loss=4.437383171682406
Epoch 2230
Epoch 2230 :: Batch 0/1
Batch Loss = 4.437383165264215
2230, epoch_train_loss=4.437383165264215
Epoch 2231
Epoch 2231 :: Batch 0/1
Batch Loss = 4.437383158802586
2231, epoch_train_loss=4.437383158802586
Epoch 2232
Epoch 2232 :: Batch 0/1
Batch Loss = 4.437383152297071
2232, epoch_train_loss=4.437383152297071
Epoch 2233
Epoch 2233 :: Batch 0/1
Batch Loss = 4.437383145747224
2233, epoch_train_loss=4.437383145747224
Epoch 2234
Epoch 2234 :: Batch 0/1
Batch Loss = 4.437383139152588
2234, epoch_train_loss=4.437383139152588
Epoch 2235
Epoch 2235 :: Batch 0/1
Batch Loss = 4.437383132512705
2235, epoch_train_loss=4.437383132512705
Epoch 2236
Epoch 2236 :: Batch 0/1
Batch Loss = 4.437383125827104
2236, epoch_train_loss=4.437383125827104
Epoch 2237
Epoch 2237 :: Batch 0/1
Batch Loss = 4.437383119095311
2237, epoch_train_loss=4.437383119095311
Epoch 2238
Epoch 2238 :: Batch 0/1
Batch Loss = 4.437383112316844
2238, epoch_train_loss=4.437383112316844
Epoch 2239
Epoch 2239 :: Batch 0/1
Batch Loss = 4.437383105491221
2239, epoch_train_loss=4.437383105491221
Epoch 2240
Epoch 2240 :: Batch 0/1
Batch Loss = 4.4373830986179446
2240, epoch_train_loss=4.4373830986179446
Epoch 2241
Epoch 2241 :: Batch 0/1
Batch Loss = 4.437383091696515
2241, epoch_train_loss=4.437383091696515
Epoch 2242
Epoch 2242 :: Batch 0/1
Batch Loss = 4.437383084726425
2242, epoch_train_loss=4.437383084726425
Epoch 2243
Epoch 2243 :: Batch 0/1
Batch Loss = 4.4373830777071595
2243, epoch_train_loss=4.4373830777071595
Epoch 2244
Epoch 2244 :: Batch 0/1
Batch Loss = 4.437383070638198
2244, epoch_train_loss=4.437383070638198
Epoch 2245
Epoch 2245 :: Batch 0/1
Batch Loss = 4.437383063519011
2245, epoch_train_loss=4.437383063519011
Epoch 2246
Epoch 2246 :: Batch 0/1
Batch Loss = 4.437383056349063
2246, epoch_train_loss=4.437383056349063
Epoch 2247
Epoch 2247 :: Batch 0/1
Batch Loss = 4.437383049127811
2247, epoch_train_loss=4.437383049127811
Epoch 2248
Epoch 2248 :: Batch 0/1
Batch Loss = 4.437383041854703
2248, epoch_train_loss=4.437383041854703
Epoch 2249
Epoch 2249 :: Batch 0/1
Batch Loss = 4.437383034529182
2249, epoch_train_loss=4.437383034529182
Epoch 2250
Epoch 2250 :: Batch 0/1
Batch Loss = 4.437383027150676
2250, epoch_train_loss=4.437383027150676
Epoch 2251
Epoch 2251 :: Batch 0/1
Batch Loss = 4.437383019718617
2251, epoch_train_loss=4.437383019718617
Epoch 2252
Epoch 2252 :: Batch 0/1
Batch Loss = 4.437383012232417
2252, epoch_train_loss=4.437383012232417
Epoch 2253
Epoch 2253 :: Batch 0/1
Batch Loss = 4.437383004691488
2253, epoch_train_loss=4.437383004691488
Epoch 2254
Epoch 2254 :: Batch 0/1
Batch Loss = 4.437382997095228
2254, epoch_train_loss=4.437382997095228
Epoch 2255
Epoch 2255 :: Batch 0/1
Batch Loss = 4.43738298944303
2255, epoch_train_loss=4.43738298944303
Epoch 2256
Epoch 2256 :: Batch 0/1
Batch Loss = 4.437382981734277
2256, epoch_train_loss=4.437382981734277
Epoch 2257
Epoch 2257 :: Batch 0/1
Batch Loss = 4.437382973968341
2257, epoch_train_loss=4.437382973968341
Epoch 2258
Epoch 2258 :: Batch 0/1
Batch Loss = 4.437382966144589
2258, epoch_train_loss=4.437382966144589
Epoch 2259
Epoch 2259 :: Batch 0/1
Batch Loss = 4.4373829582623765
2259, epoch_train_loss=4.4373829582623765
Epoch 2260
Epoch 2260 :: Batch 0/1
Batch Loss = 4.43738295032105
2260, epoch_train_loss=4.43738295032105
Epoch 2261
Epoch 2261 :: Batch 0/1
Batch Loss = 4.437382942319946
2261, epoch_train_loss=4.437382942319946
Epoch 2262
Epoch 2262 :: Batch 0/1
Batch Loss = 4.437382934258391
2262, epoch_train_loss=4.437382934258391
Epoch 2263
Epoch 2263 :: Batch 0/1
Batch Loss = 4.437382926135702
2263, epoch_train_loss=4.437382926135702
Epoch 2264
Epoch 2264 :: Batch 0/1
Batch Loss = 4.437382917951187
2264, epoch_train_loss=4.437382917951187
Epoch 2265
Epoch 2265 :: Batch 0/1
Batch Loss = 4.437382909704142
2265, epoch_train_loss=4.437382909704142
Epoch 2266
Epoch 2266 :: Batch 0/1
Batch Loss = 4.4373829013938515
2266, epoch_train_loss=4.4373829013938515
Epoch 2267
Epoch 2267 :: Batch 0/1
Batch Loss = 4.437382893019593
2267, epoch_train_loss=4.437382893019593
Epoch 2268
Epoch 2268 :: Batch 0/1
Batch Loss = 4.437382884580632
2268, epoch_train_loss=4.437382884580632
Epoch 2269
Epoch 2269 :: Batch 0/1
Batch Loss = 4.43738287607622
2269, epoch_train_loss=4.43738287607622
Epoch 2270
Epoch 2270 :: Batch 0/1
Batch Loss = 4.4373828675056
2270, epoch_train_loss=4.4373828675056
Epoch 2271
Epoch 2271 :: Batch 0/1
Batch Loss = 4.437382858868004
2271, epoch_train_loss=4.437382858868004
Epoch 2272
Epoch 2272 :: Batch 0/1
Batch Loss = 4.437382850162649
2272, epoch_train_loss=4.437382850162649
Epoch 2273
Epoch 2273 :: Batch 0/1
Batch Loss = 4.437382841388743
2273, epoch_train_loss=4.437382841388743
Epoch 2274
Epoch 2274 :: Batch 0/1
Batch Loss = 4.437382832545481
2274, epoch_train_loss=4.437382832545481
Epoch 2275
Epoch 2275 :: Batch 0/1
Batch Loss = 4.4373828236320465
2275, epoch_train_loss=4.4373828236320465
Epoch 2276
Epoch 2276 :: Batch 0/1
Batch Loss = 4.437382814647608
2276, epoch_train_loss=4.437382814647608
Epoch 2277
Epoch 2277 :: Batch 0/1
Batch Loss = 4.437382805591324
2277, epoch_train_loss=4.437382805591324
Epoch 2278
Epoch 2278 :: Batch 0/1
Batch Loss = 4.4373827964623365
2278, epoch_train_loss=4.4373827964623365
Epoch 2279
Epoch 2279 :: Batch 0/1
Batch Loss = 4.437382787259778
2279, epoch_train_loss=4.437382787259778
Epoch 2280
Epoch 2280 :: Batch 0/1
Batch Loss = 4.437382777982764
2280, epoch_train_loss=4.437382777982764
Epoch 2281
Epoch 2281 :: Batch 0/1
Batch Loss = 4.437382768630398
2281, epoch_train_loss=4.437382768630398
Epoch 2282
Epoch 2282 :: Batch 0/1
Batch Loss = 4.437382759201769
2282, epoch_train_loss=4.437382759201769
Epoch 2283
Epoch 2283 :: Batch 0/1
Batch Loss = 4.437382749695953
2283, epoch_train_loss=4.437382749695953
Epoch 2284
Epoch 2284 :: Batch 0/1
Batch Loss = 4.437382740112007
2284, epoch_train_loss=4.437382740112007
Epoch 2285
Epoch 2285 :: Batch 0/1
Batch Loss = 4.437382730448976
2285, epoch_train_loss=4.437382730448976
Epoch 2286
Epoch 2286 :: Batch 0/1
Batch Loss = 4.437382720705891
2286, epoch_train_loss=4.437382720705891
Epoch 2287
Epoch 2287 :: Batch 0/1
Batch Loss = 4.437382710881765
2287, epoch_train_loss=4.437382710881765
Epoch 2288
Epoch 2288 :: Batch 0/1
Batch Loss = 4.437382700975595
2288, epoch_train_loss=4.437382700975595
Epoch 2289
Epoch 2289 :: Batch 0/1
Batch Loss = 4.437382690986364
2289, epoch_train_loss=4.437382690986364
Epoch 2290
Epoch 2290 :: Batch 0/1
Batch Loss = 4.437382680913034
2290, epoch_train_loss=4.437382680913034
Epoch 2291
Epoch 2291 :: Batch 0/1
Batch Loss = 4.437382670754556
2291, epoch_train_loss=4.437382670754556
Epoch 2292
Epoch 2292 :: Batch 0/1
Batch Loss = 4.437382660509861
2292, epoch_train_loss=4.437382660509861
Epoch 2293
Epoch 2293 :: Batch 0/1
Batch Loss = 4.437382650177859
2293, epoch_train_loss=4.437382650177859
Epoch 2294
Epoch 2294 :: Batch 0/1
Batch Loss = 4.4373826397574465
2294, epoch_train_loss=4.4373826397574465
Epoch 2295
Epoch 2295 :: Batch 0/1
Batch Loss = 4.437382629247502
2295, epoch_train_loss=4.437382629247502
Epoch 2296
Epoch 2296 :: Batch 0/1
Batch Loss = 4.43738261864688
2296, epoch_train_loss=4.43738261864688
Epoch 2297
Epoch 2297 :: Batch 0/1
Batch Loss = 4.4373826079544205
2297, epoch_train_loss=4.4373826079544205
Epoch 2298
Epoch 2298 :: Batch 0/1
Batch Loss = 4.437382597168946
2298, epoch_train_loss=4.437382597168946
Epoch 2299
Epoch 2299 :: Batch 0/1
Batch Loss = 4.437382586289251
2299, epoch_train_loss=4.437382586289251
Epoch 2300
Epoch 2300 :: Batch 0/1
Batch Loss = 4.43738257531412
2300, epoch_train_loss=4.43738257531412
Epoch 2301
Epoch 2301 :: Batch 0/1
Batch Loss = 4.437382564242307
2301, epoch_train_loss=4.437382564242307
Epoch 2302
Epoch 2302 :: Batch 0/1
Batch Loss = 4.43738255307255
2302, epoch_train_loss=4.43738255307255
Epoch 2303
Epoch 2303 :: Batch 0/1
Batch Loss = 4.437382541803565
2303, epoch_train_loss=4.437382541803565
Epoch 2304
Epoch 2304 :: Batch 0/1
Batch Loss = 4.4373825304340455
2304, epoch_train_loss=4.4373825304340455
Epoch 2305
Epoch 2305 :: Batch 0/1
Batch Loss = 4.437382518962661
2305, epoch_train_loss=4.437382518962661
Epoch 2306
Epoch 2306 :: Batch 0/1
Batch Loss = 4.437382507388061
2306, epoch_train_loss=4.437382507388061
Epoch 2307
Epoch 2307 :: Batch 0/1
Batch Loss = 4.437382495708868
2307, epoch_train_loss=4.437382495708868
Epoch 2308
Epoch 2308 :: Batch 0/1
Batch Loss = 4.437382483923681
2308, epoch_train_loss=4.437382483923681
Epoch 2309
Epoch 2309 :: Batch 0/1
Batch Loss = 4.437382472031079
2309, epoch_train_loss=4.437382472031079
Epoch 2310
Epoch 2310 :: Batch 0/1
Batch Loss = 4.437382460029607
2310, epoch_train_loss=4.437382460029607
Epoch 2311
Epoch 2311 :: Batch 0/1
Batch Loss = 4.437382447917791
2311, epoch_train_loss=4.437382447917791
Epoch 2312
Epoch 2312 :: Batch 0/1
Batch Loss = 4.43738243569413
2312, epoch_train_loss=4.43738243569413
Epoch 2313
Epoch 2313 :: Batch 0/1
Batch Loss = 4.437382423357097
2313, epoch_train_loss=4.437382423357097
Epoch 2314
Epoch 2314 :: Batch 0/1
Batch Loss = 4.437382410905132
2314, epoch_train_loss=4.437382410905132
Epoch 2315
Epoch 2315 :: Batch 0/1
Batch Loss = 4.437382398336651
2315, epoch_train_loss=4.437382398336651
Epoch 2316
Epoch 2316 :: Batch 0/1
Batch Loss = 4.437382385650042
2316, epoch_train_loss=4.437382385650042
Epoch 2317
Epoch 2317 :: Batch 0/1
Batch Loss = 4.437382372843663
2317, epoch_train_loss=4.437382372843663
Epoch 2318
Epoch 2318 :: Batch 0/1
Batch Loss = 4.437382359915841
2318, epoch_train_loss=4.437382359915841
Epoch 2319
Epoch 2319 :: Batch 0/1
Batch Loss = 4.4373823468648705
2319, epoch_train_loss=4.4373823468648705
Epoch 2320
Epoch 2320 :: Batch 0/1
Batch Loss = 4.437382333689022
2320, epoch_train_loss=4.437382333689022
Epoch 2321
Epoch 2321 :: Batch 0/1
Batch Loss = 4.437382320386524
2321, epoch_train_loss=4.437382320386524
Epoch 2322
Epoch 2322 :: Batch 0/1
Batch Loss = 4.437382306955579
2322, epoch_train_loss=4.437382306955579
Epoch 2323
Epoch 2323 :: Batch 0/1
Batch Loss = 4.437382293394352
2323, epoch_train_loss=4.437382293394352
Epoch 2324
Epoch 2324 :: Batch 0/1
Batch Loss = 4.437382279700977
2324, epoch_train_loss=4.437382279700977
Epoch 2325
Epoch 2325 :: Batch 0/1
Batch Loss = 4.437382265873549
2325, epoch_train_loss=4.437382265873549
Epoch 2326
Epoch 2326 :: Batch 0/1
Batch Loss = 4.4373822519101305
2326, epoch_train_loss=4.4373822519101305
Epoch 2327
Epoch 2327 :: Batch 0/1
Batch Loss = 4.437382237808746
2327, epoch_train_loss=4.437382237808746
Epoch 2328
Epoch 2328 :: Batch 0/1
Batch Loss = 4.43738222356738
2328, epoch_train_loss=4.43738222356738
Epoch 2329
Epoch 2329 :: Batch 0/1
Batch Loss = 4.4373822091839825
2329, epoch_train_loss=4.4373822091839825
Epoch 2330
Epoch 2330 :: Batch 0/1
Batch Loss = 4.437382194656458
2330, epoch_train_loss=4.437382194656458
Epoch 2331
Epoch 2331 :: Batch 0/1
Batch Loss = 4.4373821799826745
2331, epoch_train_loss=4.4373821799826745
Epoch 2332
Epoch 2332 :: Batch 0/1
Batch Loss = 4.437382165160459
2332, epoch_train_loss=4.437382165160459
Epoch 2333
Epoch 2333 :: Batch 0/1
Batch Loss = 4.437382150187594
2333, epoch_train_loss=4.437382150187594
Epoch 2334
Epoch 2334 :: Batch 0/1
Batch Loss = 4.437382135061821
2334, epoch_train_loss=4.437382135061821
Epoch 2335
Epoch 2335 :: Batch 0/1
Batch Loss = 4.437382119780832
2335, epoch_train_loss=4.437382119780832
Epoch 2336
Epoch 2336 :: Batch 0/1
Batch Loss = 4.4373821043422765
2336, epoch_train_loss=4.4373821043422765
Epoch 2337
Epoch 2337 :: Batch 0/1
Batch Loss = 4.437382088743759
2337, epoch_train_loss=4.437382088743759
Epoch 2338
Epoch 2338 :: Batch 0/1
Batch Loss = 4.437382072982832
2338, epoch_train_loss=4.437382072982832
Epoch 2339
Epoch 2339 :: Batch 0/1
Batch Loss = 4.437382057057001
2339, epoch_train_loss=4.437382057057001
Epoch 2340
Epoch 2340 :: Batch 0/1
Batch Loss = 4.43738204096372
2340, epoch_train_loss=4.43738204096372
Epoch 2341
Epoch 2341 :: Batch 0/1
Batch Loss = 4.437382024700391
2341, epoch_train_loss=4.437382024700391
Epoch 2342
Epoch 2342 :: Batch 0/1
Batch Loss = 4.437382008264365
2342, epoch_train_loss=4.437382008264365
Epoch 2343
Epoch 2343 :: Batch 0/1
Batch Loss = 4.437381991652935
2343, epoch_train_loss=4.437381991652935
Epoch 2344
Epoch 2344 :: Batch 0/1
Batch Loss = 4.43738197486334
2344, epoch_train_loss=4.43738197486334
Epoch 2345
Epoch 2345 :: Batch 0/1
Batch Loss = 4.4373819578927645
2345, epoch_train_loss=4.4373819578927645
Epoch 2346
Epoch 2346 :: Batch 0/1
Batch Loss = 4.437381940738328
2346, epoch_train_loss=4.437381940738328
Epoch 2347
Epoch 2347 :: Batch 0/1
Batch Loss = 4.437381923397096
2347, epoch_train_loss=4.437381923397096
Epoch 2348
Epoch 2348 :: Batch 0/1
Batch Loss = 4.437381905866069
2348, epoch_train_loss=4.437381905866069
Epoch 2349
Epoch 2349 :: Batch 0/1
Batch Loss = 4.437381888142183
2349, epoch_train_loss=4.437381888142183
Epoch 2350
Epoch 2350 :: Batch 0/1
Batch Loss = 4.43738187022231
2350, epoch_train_loss=4.43738187022231
Epoch 2351
Epoch 2351 :: Batch 0/1
Batch Loss = 4.4373818521032575
2351, epoch_train_loss=4.4373818521032575
Epoch 2352
Epoch 2352 :: Batch 0/1
Batch Loss = 4.4373818337817585
2352, epoch_train_loss=4.4373818337817585
Epoch 2353
Epoch 2353 :: Batch 0/1
Batch Loss = 4.437381815254479
2353, epoch_train_loss=4.437381815254479
Epoch 2354
Epoch 2354 :: Batch 0/1
Batch Loss = 4.437381796518015
2354, epoch_train_loss=4.437381796518015
Epoch 2355
Epoch 2355 :: Batch 0/1
Batch Loss = 4.437381777568885
2355, epoch_train_loss=4.437381777568885
Epoch 2356
Epoch 2356 :: Batch 0/1
Batch Loss = 4.437381758403529
2356, epoch_train_loss=4.437381758403529
Epoch 2357
Epoch 2357 :: Batch 0/1
Batch Loss = 4.437381739018312
2357, epoch_train_loss=4.437381739018312
Epoch 2358
Epoch 2358 :: Batch 0/1
Batch Loss = 4.437381719409519
2358, epoch_train_loss=4.437381719409519
Epoch 2359
Epoch 2359 :: Batch 0/1
Batch Loss = 4.437381699573344
2359, epoch_train_loss=4.437381699573344
Epoch 2360
Epoch 2360 :: Batch 0/1
Batch Loss = 4.437381679505909
2360, epoch_train_loss=4.437381679505909
Epoch 2361
Epoch 2361 :: Batch 0/1
Batch Loss = 4.437381659203235
2361, epoch_train_loss=4.437381659203235
Epoch 2362
Epoch 2362 :: Batch 0/1
Batch Loss = 4.437381638661264
2362, epoch_train_loss=4.437381638661264
Epoch 2363
Epoch 2363 :: Batch 0/1
Batch Loss = 4.437381617875835
2363, epoch_train_loss=4.437381617875835
Epoch 2364
Epoch 2364 :: Batch 0/1
Batch Loss = 4.437381596842699
2364, epoch_train_loss=4.437381596842699
Epoch 2365
Epoch 2365 :: Batch 0/1
Batch Loss = 4.437381575557507
2365, epoch_train_loss=4.437381575557507
Epoch 2366
Epoch 2366 :: Batch 0/1
Batch Loss = 4.437381554015809
2366, epoch_train_loss=4.437381554015809
Epoch 2367
Epoch 2367 :: Batch 0/1
Batch Loss = 4.437381532213046
2367, epoch_train_loss=4.437381532213046
Epoch 2368
Epoch 2368 :: Batch 0/1
Batch Loss = 4.437381510144561
2368, epoch_train_loss=4.437381510144561
Epoch 2369
Epoch 2369 :: Batch 0/1
Batch Loss = 4.43738148780558
2369, epoch_train_loss=4.43738148780558
Epoch 2370
Epoch 2370 :: Batch 0/1
Batch Loss = 4.4373814651912165
2370, epoch_train_loss=4.4373814651912165
Epoch 2371
Epoch 2371 :: Batch 0/1
Batch Loss = 4.437381442296472
2371, epoch_train_loss=4.437381442296472
Epoch 2372
Epoch 2372 :: Batch 0/1
Batch Loss = 4.437381419116222
2372, epoch_train_loss=4.437381419116222
Epoch 2373
Epoch 2373 :: Batch 0/1
Batch Loss = 4.437381395645221
2373, epoch_train_loss=4.437381395645221
Epoch 2374
Epoch 2374 :: Batch 0/1
Batch Loss = 4.4373813718780974
2374, epoch_train_loss=4.4373813718780974
Epoch 2375
Epoch 2375 :: Batch 0/1
Batch Loss = 4.437381347809342
2375, epoch_train_loss=4.437381347809342
Epoch 2376
Epoch 2376 :: Batch 0/1
Batch Loss = 4.437381323433319
2376, epoch_train_loss=4.437381323433319
Epoch 2377
Epoch 2377 :: Batch 0/1
Batch Loss = 4.437381298744242
2377, epoch_train_loss=4.437381298744242
Epoch 2378
Epoch 2378 :: Batch 0/1
Batch Loss = 4.4373812737361895
2378, epoch_train_loss=4.4373812737361895
Epoch 2379
Epoch 2379 :: Batch 0/1
Batch Loss = 4.437381248403088
2379, epoch_train_loss=4.437381248403088
Epoch 2380
Epoch 2380 :: Batch 0/1
Batch Loss = 4.437381222738712
2380, epoch_train_loss=4.437381222738712
Epoch 2381
Epoch 2381 :: Batch 0/1
Batch Loss = 4.437381196736673
2381, epoch_train_loss=4.437381196736673
Epoch 2382
Epoch 2382 :: Batch 0/1
Batch Loss = 4.437381170390427
2382, epoch_train_loss=4.437381170390427
Epoch 2383
Epoch 2383 :: Batch 0/1
Batch Loss = 4.437381143693256
2383, epoch_train_loss=4.437381143693256
Epoch 2384
Epoch 2384 :: Batch 0/1
Batch Loss = 4.43738111663827
2384, epoch_train_loss=4.43738111663827
Epoch 2385
Epoch 2385 :: Batch 0/1
Batch Loss = 4.437381089218397
2385, epoch_train_loss=4.437381089218397
Epoch 2386
Epoch 2386 :: Batch 0/1
Batch Loss = 4.437381061426383
2386, epoch_train_loss=4.437381061426383
Epoch 2387
Epoch 2387 :: Batch 0/1
Batch Loss = 4.437381033254781
2387, epoch_train_loss=4.437381033254781
Epoch 2388
Epoch 2388 :: Batch 0/1
Batch Loss = 4.437381004695945
2388, epoch_train_loss=4.437381004695945
Epoch 2389
Epoch 2389 :: Batch 0/1
Batch Loss = 4.437380975742028
2389, epoch_train_loss=4.437380975742028
Epoch 2390
Epoch 2390 :: Batch 0/1
Batch Loss = 4.437380946384965
2390, epoch_train_loss=4.437380946384965
Epoch 2391
Epoch 2391 :: Batch 0/1
Batch Loss = 4.43738091661648
2391, epoch_train_loss=4.43738091661648
Epoch 2392
Epoch 2392 :: Batch 0/1
Batch Loss = 4.437380886428067
2392, epoch_train_loss=4.437380886428067
Epoch 2393
Epoch 2393 :: Batch 0/1
Batch Loss = 4.437380855810985
2393, epoch_train_loss=4.437380855810985
Epoch 2394
Epoch 2394 :: Batch 0/1
Batch Loss = 4.437380824756254
2394, epoch_train_loss=4.437380824756254
Epoch 2395
Epoch 2395 :: Batch 0/1
Batch Loss = 4.437380793254641
2395, epoch_train_loss=4.437380793254641
Epoch 2396
Epoch 2396 :: Batch 0/1
Batch Loss = 4.437380761296654
2396, epoch_train_loss=4.437380761296654
Epoch 2397
Epoch 2397 :: Batch 0/1
Batch Loss = 4.437380728872534
2397, epoch_train_loss=4.437380728872534
Epoch 2398
Epoch 2398 :: Batch 0/1
Batch Loss = 4.4373806959722435
2398, epoch_train_loss=4.4373806959722435
Epoch 2399
Epoch 2399 :: Batch 0/1
Batch Loss = 4.4373806625854595
2399, epoch_train_loss=4.4373806625854595
Epoch 2400
Epoch 2400 :: Batch 0/1
Batch Loss = 4.437380628701556
2400, epoch_train_loss=4.437380628701556
Epoch 2401
Epoch 2401 :: Batch 0/1
Batch Loss = 4.437380594309599
2401, epoch_train_loss=4.437380594309599
Epoch 2402
Epoch 2402 :: Batch 0/1
Batch Loss = 4.437380559398337
2402, epoch_train_loss=4.437380559398337
Epoch 2403
Epoch 2403 :: Batch 0/1
Batch Loss = 4.437380523956182
2403, epoch_train_loss=4.437380523956182
Epoch 2404
Epoch 2404 :: Batch 0/1
Batch Loss = 4.437380487971201
2404, epoch_train_loss=4.437380487971201
Epoch 2405
Epoch 2405 :: Batch 0/1
Batch Loss = 4.43738045143111
2405, epoch_train_loss=4.43738045143111
Epoch 2406
Epoch 2406 :: Batch 0/1
Batch Loss = 4.437380414323244
2406, epoch_train_loss=4.437380414323244
Epoch 2407
Epoch 2407 :: Batch 0/1
Batch Loss = 4.43738037663456
2407, epoch_train_loss=4.43738037663456
Epoch 2408
Epoch 2408 :: Batch 0/1
Batch Loss = 4.43738033835161
2408, epoch_train_loss=4.43738033835161
Epoch 2409
Epoch 2409 :: Batch 0/1
Batch Loss = 4.4373802994605285
2409, epoch_train_loss=4.4373802994605285
Epoch 2410
Epoch 2410 :: Batch 0/1
Batch Loss = 4.437380259947024
2410, epoch_train_loss=4.437380259947024
Epoch 2411
Epoch 2411 :: Batch 0/1
Batch Loss = 4.437380219796347
2411, epoch_train_loss=4.437380219796347
Epoch 2412
Epoch 2412 :: Batch 0/1
Batch Loss = 4.43738017899329
2412, epoch_train_loss=4.43738017899329
Epoch 2413
Epoch 2413 :: Batch 0/1
Batch Loss = 4.437380137522151
2413, epoch_train_loss=4.437380137522151
Epoch 2414
Epoch 2414 :: Batch 0/1
Batch Loss = 4.437380095366719
2414, epoch_train_loss=4.437380095366719
Epoch 2415
Epoch 2415 :: Batch 0/1
Batch Loss = 4.437380052510271
2415, epoch_train_loss=4.437380052510271
Epoch 2416
Epoch 2416 :: Batch 0/1
Batch Loss = 4.437380008935525
2416, epoch_train_loss=4.437380008935525
Epoch 2417
Epoch 2417 :: Batch 0/1
Batch Loss = 4.437379964624627
2417, epoch_train_loss=4.437379964624627
Epoch 2418
Epoch 2418 :: Batch 0/1
Batch Loss = 4.437379919559129
2418, epoch_train_loss=4.437379919559129
Epoch 2419
Epoch 2419 :: Batch 0/1
Batch Loss = 4.437379873719967
2419, epoch_train_loss=4.437379873719967
Epoch 2420
Epoch 2420 :: Batch 0/1
Batch Loss = 4.437379827087423
2420, epoch_train_loss=4.437379827087423
Epoch 2421
Epoch 2421 :: Batch 0/1
Batch Loss = 4.437379779641106
2421, epoch_train_loss=4.437379779641106
Epoch 2422
Epoch 2422 :: Batch 0/1
Batch Loss = 4.437379731359927
2422, epoch_train_loss=4.437379731359927
Epoch 2423
Epoch 2423 :: Batch 0/1
Batch Loss = 4.437379682222047
2423, epoch_train_loss=4.437379682222047
Epoch 2424
Epoch 2424 :: Batch 0/1
Batch Loss = 4.437379632204875
2424, epoch_train_loss=4.437379632204875
Epoch 2425
Epoch 2425 :: Batch 0/1
Batch Loss = 4.437379581285005
2425, epoch_train_loss=4.437379581285005
Epoch 2426
Epoch 2426 :: Batch 0/1
Batch Loss = 4.437379529438197
2426, epoch_train_loss=4.437379529438197
Epoch 2427
Epoch 2427 :: Batch 0/1
Batch Loss = 4.437379476639333
2427, epoch_train_loss=4.437379476639333
Epoch 2428
Epoch 2428 :: Batch 0/1
Batch Loss = 4.437379422862376
2428, epoch_train_loss=4.437379422862376
Epoch 2429
Epoch 2429 :: Batch 0/1
Batch Loss = 4.4373793680803315
2429, epoch_train_loss=4.4373793680803315
Epoch 2430
Epoch 2430 :: Batch 0/1
Batch Loss = 4.437379312265188
2430, epoch_train_loss=4.437379312265188
Epoch 2431
Epoch 2431 :: Batch 0/1
Batch Loss = 4.4373792553878895
2431, epoch_train_loss=4.4373792553878895
Epoch 2432
Epoch 2432 :: Batch 0/1
Batch Loss = 4.437379197418276
2432, epoch_train_loss=4.437379197418276
Epoch 2433
Epoch 2433 :: Batch 0/1
Batch Loss = 4.437379138325019
2433, epoch_train_loss=4.437379138325019
Epoch 2434
Epoch 2434 :: Batch 0/1
Batch Loss = 4.4373790780755735
2434, epoch_train_loss=4.4373790780755735
Epoch 2435
Epoch 2435 :: Batch 0/1
Batch Loss = 4.437379016636125
2435, epoch_train_loss=4.437379016636125
Epoch 2436
Epoch 2436 :: Batch 0/1
Batch Loss = 4.43737895397152
2436, epoch_train_loss=4.43737895397152
Epoch 2437
Epoch 2437 :: Batch 0/1
Batch Loss = 4.437378890045186
2437, epoch_train_loss=4.437378890045186
Epoch 2438
Epoch 2438 :: Batch 0/1
Batch Loss = 4.437378824819059
2438, epoch_train_loss=4.437378824819059
Epoch 2439
Epoch 2439 :: Batch 0/1
Batch Loss = 4.43737875825354
2439, epoch_train_loss=4.43737875825354
Epoch 2440
Epoch 2440 :: Batch 0/1
Batch Loss = 4.437378690307368
2440, epoch_train_loss=4.437378690307368
Epoch 2441
Epoch 2441 :: Batch 0/1
Batch Loss = 4.437378620937562
2441, epoch_train_loss=4.437378620937562
Epoch 2442
Epoch 2442 :: Batch 0/1
Batch Loss = 4.437378550099312
2442, epoch_train_loss=4.437378550099312
Epoch 2443
Epoch 2443 :: Batch 0/1
Batch Loss = 4.437378477745879
2443, epoch_train_loss=4.437378477745879
Epoch 2444
Epoch 2444 :: Batch 0/1
Batch Loss = 4.437378403828507
2444, epoch_train_loss=4.437378403828507
Epoch 2445
Epoch 2445 :: Batch 0/1
Batch Loss = 4.437378328296289
2445, epoch_train_loss=4.437378328296289
Epoch 2446
Epoch 2446 :: Batch 0/1
Batch Loss = 4.43737825109605
2446, epoch_train_loss=4.43737825109605
Epoch 2447
Epoch 2447 :: Batch 0/1
Batch Loss = 4.437378172172222
2447, epoch_train_loss=4.437378172172222
Epoch 2448
Epoch 2448 :: Batch 0/1
Batch Loss = 4.43737809146669
2448, epoch_train_loss=4.43737809146669
Epoch 2449
Epoch 2449 :: Batch 0/1
Batch Loss = 4.4373780089186585
2449, epoch_train_loss=4.4373780089186585
Epoch 2450
Epoch 2450 :: Batch 0/1
Batch Loss = 4.4373779244644895
2450, epoch_train_loss=4.4373779244644895
Epoch 2451
Epoch 2451 :: Batch 0/1
Batch Loss = 4.437377838037521
2451, epoch_train_loss=4.437377838037521
Epoch 2452
Epoch 2452 :: Batch 0/1
Batch Loss = 4.437377749567877
2452, epoch_train_loss=4.437377749567877
Epoch 2453
Epoch 2453 :: Batch 0/1
Batch Loss = 4.437377658982266
2453, epoch_train_loss=4.437377658982266
Epoch 2454
Epoch 2454 :: Batch 0/1
Batch Loss = 4.437377566203785
2454, epoch_train_loss=4.437377566203785
Epoch 2455
Epoch 2455 :: Batch 0/1
Batch Loss = 4.437377471151655
2455, epoch_train_loss=4.437377471151655
Epoch 2456
Epoch 2456 :: Batch 0/1
Batch Loss = 4.437377373740992
2456, epoch_train_loss=4.437377373740992
Epoch 2457
Epoch 2457 :: Batch 0/1
Batch Loss = 4.4373772738825314
2457, epoch_train_loss=4.4373772738825314
Epoch 2458
Epoch 2458 :: Batch 0/1
Batch Loss = 4.43737717148231
2458, epoch_train_loss=4.43737717148231
Epoch 2459
Epoch 2459 :: Batch 0/1
Batch Loss = 4.437377066441373
2459, epoch_train_loss=4.437377066441373
Epoch 2460
Epoch 2460 :: Batch 0/1
Batch Loss = 4.437376958655397
2460, epoch_train_loss=4.437376958655397
Epoch 2461
Epoch 2461 :: Batch 0/1
Batch Loss = 4.4373768480143365
2461, epoch_train_loss=4.4373768480143365
Epoch 2462
Epoch 2462 :: Batch 0/1
Batch Loss = 4.4373767344019805
2462, epoch_train_loss=4.4373767344019805
Epoch 2463
Epoch 2463 :: Batch 0/1
Batch Loss = 4.437376617695535
2463, epoch_train_loss=4.437376617695535
Epoch 2464
Epoch 2464 :: Batch 0/1
Batch Loss = 4.437376497765094
2464, epoch_train_loss=4.437376497765094
Epoch 2465
Epoch 2465 :: Batch 0/1
Batch Loss = 4.43737637447312
2465, epoch_train_loss=4.43737637447312
Epoch 2466
Epoch 2466 :: Batch 0/1
Batch Loss = 4.437376247673852
2466, epoch_train_loss=4.437376247673852
Epoch 2467
Epoch 2467 :: Batch 0/1
Batch Loss = 4.4373761172126525
2467, epoch_train_loss=4.4373761172126525
Epoch 2468
Epoch 2468 :: Batch 0/1
Batch Loss = 4.437375982925291
2468, epoch_train_loss=4.437375982925291
Epoch 2469
Epoch 2469 :: Batch 0/1
Batch Loss = 4.437375844637168
2469, epoch_train_loss=4.437375844637168
Epoch 2470
Epoch 2470 :: Batch 0/1
Batch Loss = 4.437375702162417
2470, epoch_train_loss=4.437375702162417
Epoch 2471
Epoch 2471 :: Batch 0/1
Batch Loss = 4.43737555530297
2471, epoch_train_loss=4.43737555530297
Epoch 2472
Epoch 2472 :: Batch 0/1
Batch Loss = 4.437375403847487
2472, epoch_train_loss=4.437375403847487
Epoch 2473
Epoch 2473 :: Batch 0/1
Batch Loss = 4.437375247570174
2473, epoch_train_loss=4.437375247570174
Epoch 2474
Epoch 2474 :: Batch 0/1
Batch Loss = 4.437375086229476
2474, epoch_train_loss=4.437375086229476
Epoch 2475
Epoch 2475 :: Batch 0/1
Batch Loss = 4.437374919566574
2475, epoch_train_loss=4.437374919566574
Epoch 2476
Epoch 2476 :: Batch 0/1
Batch Loss = 4.437374747303784
2476, epoch_train_loss=4.437374747303784
Epoch 2477
Epoch 2477 :: Batch 0/1
Batch Loss = 4.437374569142709
2477, epoch_train_loss=4.437374569142709
Epoch 2478
Epoch 2478 :: Batch 0/1
Batch Loss = 4.437374384762169
2478, epoch_train_loss=4.437374384762169
Epoch 2479
Epoch 2479 :: Batch 0/1
Batch Loss = 4.437374193815878
2479, epoch_train_loss=4.437374193815878
Epoch 2480
Epoch 2480 :: Batch 0/1
Batch Loss = 4.437373995929824
2480, epoch_train_loss=4.437373995929824
Epoch 2481
Epoch 2481 :: Batch 0/1
Batch Loss = 4.437373790699288
2481, epoch_train_loss=4.437373790699288
Epoch 2482
Epoch 2482 :: Batch 0/1
Batch Loss = 4.437373577685452
2482, epoch_train_loss=4.437373577685452
Epoch 2483
Epoch 2483 :: Batch 0/1
Batch Loss = 4.437373356411603
2483, epoch_train_loss=4.437373356411603
Epoch 2484
Epoch 2484 :: Batch 0/1
Batch Loss = 4.437373126358679
2484, epoch_train_loss=4.437373126358679
Epoch 2485
Epoch 2485 :: Batch 0/1
Batch Loss = 4.437372886960241
2485, epoch_train_loss=4.437372886960241
Epoch 2486
Epoch 2486 :: Batch 0/1
Batch Loss = 4.437372637596661
2486, epoch_train_loss=4.437372637596661
Epoch 2487
Epoch 2487 :: Batch 0/1
Batch Loss = 4.437372377588399
2487, epoch_train_loss=4.437372377588399
Epoch 2488
Epoch 2488 :: Batch 0/1
Batch Loss = 4.437372106188266
2488, epoch_train_loss=4.437372106188266
Epoch 2489
Epoch 2489 :: Batch 0/1
Batch Loss = 4.43737182257232
2489, epoch_train_loss=4.43737182257232
Epoch 2490
Epoch 2490 :: Batch 0/1
Batch Loss = 4.437371525829274
2490, epoch_train_loss=4.437371525829274
Epoch 2491
Epoch 2491 :: Batch 0/1
Batch Loss = 4.4373712149480555
2491, epoch_train_loss=4.4373712149480555
Epoch 2492
Epoch 2492 :: Batch 0/1
Batch Loss = 4.437370888803125
2492, epoch_train_loss=4.437370888803125
Epoch 2493
Epoch 2493 :: Batch 0/1
Batch Loss = 4.4373705461370285
2493, epoch_train_loss=4.4373705461370285
Epoch 2494
Epoch 2494 :: Batch 0/1
Batch Loss = 4.4373701855397405
2494, epoch_train_loss=4.4373701855397405
Epoch 2495
Epoch 2495 :: Batch 0/1
Batch Loss = 4.437369805423686
2495, epoch_train_loss=4.437369805423686
Epoch 2496
Epoch 2496 :: Batch 0/1
Batch Loss = 4.437369403993913
2496, epoch_train_loss=4.437369403993913
Epoch 2497
Epoch 2497 :: Batch 0/1
Batch Loss = 4.4373689792116755
2497, epoch_train_loss=4.4373689792116755
Epoch 2498
Epoch 2498 :: Batch 0/1
Batch Loss = 4.437368528750294
2498, epoch_train_loss=4.437368528750294
Epoch 2499
Epoch 2499 :: Batch 0/1
Batch Loss = 4.437368049940704
2499, epoch_train_loss=4.437368049940704
